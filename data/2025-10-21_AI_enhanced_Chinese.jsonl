{"id": "2510.15972", "categories": ["cs.CL", "cs.AI", "81P68 (Primary), 68T50, 68T07 (Secondary)", "I.2.7; F.1.2"], "pdf": "https://arxiv.org/pdf/2510.15972", "abs": "https://arxiv.org/abs/2510.15972", "authors": ["Ling Sun", "Peter Sullivan", "Michael Martin", "Yun Zhou"], "title": "Quantum NLP models on Natural Language Inference", "comment": "Accepted, presented, and to appear in the Proceedings of the Quantum\n  AI and NLP 2025 Conference", "summary": "Quantum natural language processing (QNLP) offers a novel approach to\nsemantic modeling by embedding compositional structure directly into quantum\ncircuits. This paper investigates the application of QNLP models to the task of\nNatural Language Inference (NLI), comparing quantum, hybrid, and classical\ntransformer-based models under a constrained few-shot setting. Using the lambeq\nlibrary and the DisCoCat framework, we construct parameterized quantum circuits\nfor sentence pairs and train them for both semantic relatedness and inference\nclassification. To assess efficiency, we introduce a novel\ninformation-theoretic metric, Information Gain per Parameter (IGPP), which\nquantifies learning dynamics independent of model size. Our results demonstrate\nthat quantum models achieve performance comparable to classical baselines while\noperating with dramatically fewer parameters. The Quantum-based models\noutperform randomly initialized transformers in inference and achieve lower\ntest error on relatedness tasks. Moreover, quantum models exhibit significantly\nhigher per-parameter learning efficiency (up to five orders of magnitude more\nthan classical counterparts), highlighting the promise of QNLP in low-resource,\nstructure-sensitive settings. To address circuit-level isolation and promote\nparameter sharing, we also propose a novel cluster-based architecture that\nimproves generalization by tying gate parameters to learned word clusters\nrather than individual tokens.", "AI": {"tldr": "该论文探讨了量子自然语言处理在自然语言推理任务中的应用，表明量子模型在few-shot设置下，可以使用较少的参数实现高效率的语义理解和推理，展示了其在低资源环境中的应用潜力。", "motivation": "研究动机在于探索QNLP模型在NLI任务中的应用，并通过比较量子、混合及经典Transformer模型来评估它们在few-shot设置下的表现。", "method": "该研究使用lambeq库和DisCoCat框架构建用于句子对的参数化量子电路，旨在进行语义相关性和推理分类任务。同时引入了一个新的信息论度量标准IGPP来评估训练效率。", "result": "实验结果显示，量子模型在参数数量大幅减少的情况下仍然能够达到与经典模型相当的性能。量子模型在推理任务上优于随机初始化的Transformer，并且在相关性任务上测试误差更低。量子模型的每个参数学习效率比经典模型更高，最多高出五个数量级。", "conclusion": "该研究展示了QNLP在资源受限和结构敏感环境下的潜力，并提出了一种新的集群架构，通过将门参数与学习的词簇关联起来，而不是单个的词汇单位，以提高泛化能力。"}}
{"id": "2510.16057", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16057", "abs": "https://arxiv.org/abs/2510.16057", "authors": ["Md Kamrul Siam", "Md Jobair Hossain Faruk", "Jerry Q. Cheng", "Huanying Gu"], "title": "Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus", "comment": "7 pages (Accepted to IEEE BHI 2025)", "summary": "This study presents a novel multi-model fusion framework leveraging two\nstate-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance\nthe reliability of chest X-ray interpretation on the CheXpert dataset. From the\nfull CheXpert corpus of 224,316 chest radiographs, we randomly selected 234\nradiologist-annotated studies to evaluate unimodal performance using image-only\nprompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of\n62.8% and 76.9%, respectively. A similarity-based consensus approach, using a\n95% output similarity threshold, improved accuracy to 77.6%. To assess the\nimpact of multimodal inputs, we then generated synthetic clinical notes\nfollowing the MIMIC-CXR template and evaluated a separate subset of 50 randomly\nselected cases paired with both images and synthetic text. On this multimodal\ncohort, performance improved to 84% for ChatGPT and 76% for Claude, while\nconsensus accuracy reached 91.3%. Across both experimental conditions,\nagreement-based fusion consistently outperformed individual models. These\nfindings highlight the utility of integrating complementary modalities and\nusing output-level consensus to improve the trustworthiness and clinical\nutility of AI-assisted radiological diagnosis, offering a practical path to\nreduce diagnostic errors with minimal computational overhead.", "AI": {"tldr": "研究者介绍了一种利用ChatGPT和Claude两种大型语言模型的多模态融合框架，此种框架能够通过结合图像与合成文本提高胸部X光片解读的准确性，将共识后的诊断准确率提升至91.3%。", "motivation": "此研究旨在利用多模态融合框架，通过强化不同输入模态的信息互补性和输出一致性，提高AI辅助放射学诊断的可靠性和临床效用，以降低诊断错误，并尽可能减少计算资源的消耗。", "method": "本研究提出了一个利用两个最先进的大型语言模型（LLMs）——ChatGPT和Claude的多模态融合框架，以提高CheXpert数据集中胸部X光片解读的可靠性。研究首先在224,316张胸部X光片中随机选取了234个放射科医生标注的研究案例，使用仅包含图像的提示评估单一模型的表现；随后，在多模态输入的情况下，生成了符合MIMIC-CXR模板的合成临床笔记，并评估了50个随机选择的样本与图像和合成文本配对后的表现。", "result": "单一模型使用图像提示时，ChatGPT和Claude的诊断准确率分别为62.8%和76.9%。采用95%输出相似性阈值的共识方法可将准确率提高至77.6%。而在多模态输入条件下，两者的准确率分别提高到84%和76%，共识准确率达到了91.3%。", "conclusion": "这些结果表明，将不同的信息模态相结合，并使用输出水平的一致性来提升AI辅助放射性诊断的信任度和临床实用性，为提高诊断准确率并减少诊断错误提供了一种实际的方法。"}}
{"id": "2510.16062", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16062", "abs": "https://arxiv.org/abs/2510.16062", "authors": ["Guiyao Tie", "Zenghui Yuan", "Zeli Zhao", "Chaoran Hu", "Tianhe Gu", "Ruihang Zhang", "Sizhe Zhang", "Junran Wu", "Xiaoyue Tu", "Ming Jin", "Qingsong Wen", "Lixing Chen", "Pan Zhou", "Lichao Sun"], "title": "Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs", "comment": "38 pages, 25 figures, 8 tables", "summary": "Self-correction of large language models (LLMs) emerges as a critical\ncomponent for enhancing their reasoning performance. Although various\nself-correction methods have been proposed, a comprehensive evaluation of these\nmethods remains largely unexplored, and the question of whether LLMs can truly\ncorrect themselves is a matter of significant interest and concern. In this\nstudy, we introduce CorrectBench, a benchmark developed to evaluate the\neffectiveness of self-correction strategies, including intrinsic, external, and\nfine-tuned approaches, across three tasks: commonsense reasoning, mathematical\nreasoning, and code generation. Our findings reveal that: 1) Self-correction\nmethods can improve accuracy, especially for complex reasoning tasks; 2) Mixing\ndifferent self-correction strategies yields further improvements, though it\nreduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited\noptimization under additional self-correction methods and have high time costs.\nInterestingly, a comparatively simple chain-of-thought (CoT) baseline\ndemonstrates competitive accuracy and efficiency. These results underscore the\npotential of self-correction to enhance LLM's reasoning performance while\nhighlighting the ongoing challenge of improving their efficiency. Consequently,\nwe advocate for further research focused on optimizing the balance between\nreasoning capabilities and operational efficiency. Project Page:\nhttps://correctbench.github.io/", "AI": {"tldr": "Evaluation of self-correction strategies for LLMs shows promise but highlights ongoing efficiency challenges, suggesting a need for further optimization.", "motivation": "To evaluate the effectiveness of self-correction methods in LLMs and determine if LLMs can significantly improve their reasoning performance through self-correction.", "method": "Self-correction strategies for LLMs are evaluated using CorrectBench across three tasks: commonsense reasoning, mathematical reasoning, and code generation. The strategies include intrinsic, external, and fine-tuned methods.", "result": "Self-correction improves LLM accuracy, especially in complex reasoning tasks. Combining methods further improves accuracy but reduces efficiency. Some LLMs have limited improvement and high time costs with added self-correction. A simple CoT baseline shows good performance.", "conclusion": "Self-correction holds potential for enhancing LLM reasoning performance but presents challenges in balancing with operational efficiency."}}
{"id": "2510.16079", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16079", "abs": "https://arxiv.org/abs/2510.16079", "authors": ["Rong Wu", "Xiaoman Wang", "Jianbiao Mei", "Pinlong Cai", "Daocheng Fu", "Cheng Yang", "Licheng Wen", "Xuemeng Yang", "Yufan Shen", "Yuxin Wang", "Botian Shi"], "title": "EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle", "comment": null, "summary": "Current Large Language Model (LLM) agents show strong performance in tool\nuse, but lack the crucial capability to systematically learn from their own\nexperiences. While existing frameworks mainly focus on mitigating external\nknowledge gaps, they fail to address a more fundamental limitation: the\ninability to iteratively refine problem-solving strategies. In this work, we\nintroduce EvolveR, a framework designed to enable agent to self-improve through\na complete, closed-loop experience lifecycle. This lifecycle comprises two key\nstages: (1) Offline Self-Distillation, where the agent's interaction\ntrajectories are synthesized into a structured repository of abstract, reusable\nstrategic principles; (2) Online Interaction, where the agent interacts with\ntasks and actively retrieves distilled principles to guide its decision-making,\naccumulating a diverse set of behavioral trajectories. This loop employs a\npolicy reinforcement mechanism to iteratively update the agent based on its\nperformance. We demonstrate the effectiveness of EvolveR on complex multi-hop\nquestion-answering benchmarks, where it achieves superior performance over\nstrong agentic baselines. Our work presents a comprehensive blueprint for\nagents that learn not only from external data but also from the consequences of\ntheir own actions, paving the way for more autonomous and continuously\nimproving systems. Code is available at https://github.com/Edaizi/EvolveR.", "AI": {"tldr": "文章介绍了一个名为EvolveR的框架，该框架通过闭环学习过程，使代理能够从自己的经验中进行自我改进，在复杂基准测试中表现优异。代码在GitHub上可用。", "motivation": "现有的大型语言模型（LLM）表现出强大的工具使用能力，但缺乏至关重要地系统性学习自身经验的能力。虽然现有的框架主要集中在缓解外部知识缺口上，但它们未能解决更基础的局限性：即无法迭代改进问题解决策略。", "method": "本文提出了EvolveR框架，该框架旨在通过完整且闭环的经验生命周期使代理自我改进。这个生命周期包括两个关键阶段：离线自我蒸馏，即合成代理的交互轨迹，以形成一个结构化的、抽象的、可重复使用的战略原则存储库；在线交互，在此阶段，代理与任务互动，主动检索提炼出的原则指导其决策制定，并积累一组多样的行为轨迹。这一循环通过策略强化机制迭代更新代理的性能。", "result": "本文展示了EvolveR在复杂多跳问题解答基准测试中的有效性，其性能优于强大的代理基线。", "conclusion": "研究提出了一个全面的蓝图，供代理不仅从外部数据，也从自身行为后果中学习，为更自主且持续改进的系统铺平道路。"}}
{"id": "2510.15963", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15963", "abs": "https://arxiv.org/abs/2510.15963", "authors": ["Jiani Huang", "Amish Sethi", "Matthew Kuo", "Mayank Keoliya", "Neelay Velingker", "JungHo Jung", "Ser-Nam Lim", "Ziyang Li", "Mayur Naik"], "title": "ESCA: Contextualizing Embodied Agents via Scene-Graph Generation", "comment": "Accepted as a Spotlight Paper at NeurIPS 2025", "summary": "Multi-modal large language models (MLLMs) are making rapid progress toward\ngeneral-purpose embodied agents. However, current training pipelines primarily\nrely on high-level vision-sound-text pairs and lack fine-grained, structured\nalignment between pixel-level visual content and textual semantics. To overcome\nthis challenge, we propose ESCA, a new framework for contextualizing embodied\nagents through structured spatial-temporal understanding. At its core is\nSGClip, a novel CLIP-based, open-domain, and promptable model for generating\nscene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic\nlearning pipeline, which harnesses model-driven self-supervision from\nvideo-caption pairs and structured reasoning, thereby eliminating the need for\nhuman-labeled scene graph annotations. We demonstrate that SGClip supports both\nprompt-based inference and task-specific fine-tuning, excelling in scene graph\ngeneration and action localization benchmarks. ESCA with SGClip consistently\nimproves both open-source and commercial MLLMs, achieving state-of-the-art\nperformance across two embodied environments. Notably, it significantly reduces\nagent perception errors and enables open-source models to surpass proprietary\nbaselines.", "AI": {"tldr": "本文提出ESCA框架，核心为SGClip模型，用于生成场景图并提高具身智能体的性能，展示了在具身环境中的先进表现。", "motivation": "现有的训练流程主要依赖高层次的视觉-声音-文本配对，缺少像素级视觉内容与文本语义之间的细粒度、结构化的对齐。为解决这一挑战，提出了ESCA框架。", "method": "本文提出了ESCA框架，通过结构化时空理解为具身智能体提供上下文。其核心是SGClip，这是一种基于CLIP的新型开放式领域可提示模型，用于生成场景图。SGClip通过对超过87000个开放式领域视频进行训练，并采用了神经符号学习管道，利用视频-字幕对的模型驱动自我监督和结构化推理，从而避免了需要人工标注的场景图注释。", "result": "SGClip在场景图生成和动作定位基准测试中表现出色，ESCA框架显著减少了具身智能体的感知错误，并提升了多模态大型语言模型的整体性能。", "conclusion": "实验表明，ESCA与SGClip能够持续改善开源和商业多模态大型语言模型的表现，在两个具身环境中均达到了最先进的性能。尤其是在减少智能体感知错误和提升开源模型超越专有基线方面有显著改进。"}}
{"id": "2510.16091", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16091", "abs": "https://arxiv.org/abs/2510.16091", "authors": ["Binglan Han", "Anuradha Mathrani", "Teo Susnjak"], "title": "Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification", "comment": null, "summary": "This study quantifies how prompting strategies interact with large language\nmodels (LLMs) to automate the screening stage of systematic literature reviews\n(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,\nGemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types\n(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)\nacross relevance classification and six Level-2 tasks, using accuracy,\nprecision, recall, and F1. Results show pronounced model-prompt interaction\neffects: CoT-few-shot yields the most reliable precision-recall balance;\nzero-shot maximizes recall for high-sensitivity passes; and self-reflection\nunderperforms due to over-inclusivity and instability across models. GPT-4o and\nDeepSeek provide robust overall performance, while GPT-4o-mini performs\ncompetitively at a substantially lower dollar cost. A cost-performance analysis\nfor relevance classification (per 1,000 abstracts) reveals large absolute\ndifferences among model-prompt pairings; GPT-4o-mini remains low-cost across\nprompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer\nattractive F1 at a small incremental cost. We recommend a staged workflow that\n(1) deploys low-cost models with structured prompts for first-pass screening\nand (2) escalates only borderline cases to higher-capacity models. These\nfindings highlight LLMs' uneven but promising potential to automate literature\nscreening. By systematically analyzing prompt-model interactions, we provide a\ncomparative benchmark and practical guidance for task-adaptive LLM deployment.", "AI": {"tldr": "研究了六个LLMs在不同提示类型下的表现，发现CoT-少样本提示最可靠，GPT-4o和DeepSeek有较强的综合性能，GPT-4o-mini成本低且性能具有竞争力。推荐使用低成本模型带结构化提示进行初步筛选，并将边界情况分派给高容量模型。", "motivation": "量化提示策略如何与大型语言模型互动，以自动化系统文献回顾的筛选阶段。", "method": "评估六个大型语言模型(LLMs)在五种提示类型下的表现，包括零样本、少样本、链式思考(CoT)、CoT-少样本、自我反思，针对相关性分类和六个二级任务，使用准确率、精度、召回率和F1分数作为评价指标。", "result": "研究显示模型-提示交互效应显著：CoT-少样本提示在精度-召回率平衡中表现最可靠；零样本提示最大化高敏感性筛选中的召回率；自我反思由于过度包含性和模型间的波动表现较差。GPT-4o和DeepSeek具有较强的综合性能，而GPT-4o-mini在大幅降低成本的同时表现依然具有竞争力。针对相关性分类（每处理1000个摘要），不同模型-提示组合间存在显著的绝对差异，GPT-4o-mini在所有提示下都保持较低的成本，而结构化的提示（CoT/CoT-少样本）应用于GPT-4o-mini上则以微小的成本增量提供有吸引力的F1分数。", "conclusion": "这些发现强调了LLMs不平衡但有前景的潜力，可以用于自动化文献筛选。通过系统分析提示-模型间的相互作用，提供了任务适应型LLM部署的对比基准和实用指南。"}}
{"id": "2510.15991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.15991", "abs": "https://arxiv.org/abs/2510.15991", "authors": ["Huiming Yang"], "title": "CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection", "comment": "13 pages", "summary": "The sparse cross-modality detector offers more advantages than its\ncounterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of\nadaptability for downstream tasks and computational cost savings. However,\nexisting sparse detectors overlook the quality of token representation, leaving\nit with a sub-optimal foreground quality and limited performance. In this\npaper, we identify that the geometric structure preserved and the class\ndistribution are the key to improving the performance of the sparse detector,\nand propose a Sparse Selector (SS). The core module of SS is Ray-Aware\nSupervision (RAS), which preserves rich geometric information during the\ntraining stage, and Class-Balanced Supervision, which adaptively reweights the\nsalience of class semantics, ensuring that tokens associated with small objects\nare retained during token sampling. Thereby, outperforming other sparse\nmulti-modal detectors in the representation of tokens. Additionally, we design\nRay Positional Encoding (Ray PE) to address the distribution differences\nbetween the LiDAR modality and the image. Finally, we integrate the\naforementioned module into an end-to-end sparse multi-modality detector, dubbed\nCrossRay3D. Experiments show that, on the challenging nuScenes benchmark,\nCrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS,\nwhile running 1.84 faster than other leading methods. Moreover, CrossRay3D\ndemonstrates strong robustness even in scenarios where LiDAR or camera data are\npartially or entirely missing.", "AI": {"tldr": "This paper proposes CrossRay3D, an advanced sparse multi-modal detector, which improves token quality and performance while maintaining computational efficiency, outperforming other methods on the nuScenes benchmark.", "motivation": "The goal is to address the shortcomings of existing sparse cross-modality detectors, particularly in terms of token representation quality and overall detector performance.", "method": "The paper proposes a Sparse Selector (SS) that includes two key components: Ray-Aware Supervision (RAS) and Class-Balanced Supervision. It also introduces Ray Positional Encoding (Ray PE) to help with the distribution differences between LiDAR and image modalities.", "result": "The proposed model, CrossRay3D, achieves state-of-the-art performance on the nuScenes benchmark with 72.4 mAP and 74.7 NDS. It runs 1.84 times faster than other leading methods and shows strong robustness in scenarios with partial or complete loss of LiDAR or camera data.", "conclusion": "The inclusion of Ray-Aware Supervision, Class-Balanced Supervision, and Ray Positional Encoding in the proposed framework leads to superior performance in sparse multi-modal detection tasks, demonstrating strong robustness and efficiency."}}
{"id": "2510.16096", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16096", "abs": "https://arxiv.org/abs/2510.16096", "authors": ["Tina Behnia", "Puneesh Deora", "Christos Thrampoulidis"], "title": "Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization", "comment": "28 pages, 15 figures", "summary": "Language models are pretrained on sequences that blend statistical\nregularities (making text fluent) with factual associations between specific\ntokens (knowledge of facts). While recent work suggests that the variability of\ntheir interaction, such as paraphrases of factual associations, critically\ndetermines generalization ability, we lack a systematic analysis of these\nimpacts. This paper introduces a flexible synthetic testbed that combines a\nstatistical stream of generic tokens with an abstract factual stream of\nsource-target token pairs, enabling fine-grained control over their\ninteraction. The design enables the independent control of diversity nature by\nmanipulating stream composition (contextual structure) and the diversity level\nby varying which statistical streams each fact appears in. Through controlled\nexperiments, we find that while higher contextual diversity delays\nin-distribution (ID) factual accuracy, its impact on out-of-distribution (OOD)\nfactual generalization depends critically on contextual structure. In some\ncases, OOD performance follows the same trend as ID, but in others, diversity\nbecomes essential for non-trivial factual recall. Even when low diversity\nprohibits factual recall, optimal diversity levels depend on training duration.\nBeyond factual recall failures, we identify structures where statistical\ngeneralization fails independently, and others where both capabilities degrade.\nThis shows how the interplay between contextual design and diversity level\nimpacts different generalization aspects. Further, through a series of\ncontrolled interventions on the model components, we trace the OOD failures to\ndistinct optimization bottlenecks, highlighting the importance of the embedding\nand unembedding layers. Our synthetic framework allows us to isolate effects\nthat would be confounded in large-scale studies, offering a controlled testbed\nfor future investigations.", "AI": {"tldr": "通过灵活的合成测试床，本文研究了语言模型中统计规律和事实关联的相互作用，发现多样性对泛化能力的影响依赖于上下文结构，也强调了模型组件的重要性。", "motivation": "尽管近期研究表明统计规律和事实关联的可变性对泛化能力至关重要，但我们缺乏对这些影响的系统性分析。", "method": "本文介绍了结合通用标记的统计流与源目标标记对的事实流的灵活合成测试床，用于分析语言模型中的统计规律和事实关联之间的相互作用。通过调整流的组成和统计流中的事实出现情况，可以独立控制多样性特征和多样性水平。", "result": "研究表明，更高的上下文多样性虽然会延迟分布内（ID）事实的准确性，但其对分布外（OOD）事实泛化的影响取决于上下文结构。在某些情况下，多样性对非平凡的事实回忆至关重要，甚至在低多样性情况下也存在最优的多样性水平。", "conclusion": "本文展示了上下文设计和多样性水平之间的相互作用如何影响不同的泛化方面，并通过一系列对模型组件的控制干预，追踪OOD失败的原因并强调了嵌入层和解嵌层的重要性。"}}
{"id": "2510.16017", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16017", "abs": "https://arxiv.org/abs/2510.16017", "authors": ["Ibrahim Sheikh Mohamed", "Abdullah Yahya Abdullah Omaisan"], "title": "InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects", "comment": null, "summary": "Infrastructure in smart cities is increasingly monitored by networks of\nclosed circuit television (CCTV) cameras. Roads, bridges and tunnels develop\ncracks, potholes, and fluid leaks that threaten public safety and require\ntimely repair. Manual inspection is costly and hazardous, and existing\nautomatic systems typically address individual defect types or provide\nunstructured outputs that cannot directly guide maintenance crews. This paper\nproposes a comprehensive pipeline that leverages street CCTV streams for multi\ndefect detection and segmentation using the YOLO family of object detectors and\npasses the detections to a vision language model (VLM) for scene aware\nsummarization. The VLM generates a structured action plan in JSON format that\nincludes incident descriptions, recommended tools, dimensions, repair plans,\nand urgent alerts. We review literature on pothole, crack and leak detection,\nhighlight recent advances in large vision language models such as QwenVL and\nLLaVA, and describe the design of our early prototype. Experimental evaluation\non public datasets and captured CCTV clips demonstrates that the system\naccurately identifies diverse defects and produces coherent summaries. We\nconclude by discussing challenges and directions for scaling the system to city\nwide deployments.", "AI": {"tldr": "本文提出使用CCTV流进行多缺陷检测和分割，结合VLM生成结构化维修建议，改善城市基础设施的监测和维护。", "motivation": "现有的自动系统通常只针对个别缺陷类型或提供无法直接指导维修队伍的非结构化输出。手动检查成本高昂且危险。", "method": "本文提出了一种全面的管道，利用街道CCTV流进行多缺陷检测和分割，采用YOLO对象检测器家族，并将检测结果传递给视觉语言模型（VLM）进行场景感知总结，生成包括事件描述、建议工具、尺寸、修复计划和紧急警报的结构化行动计划。", "result": "实验评估显示，系统能够准确识别多种缺陷并生成连贯的报告。", "conclusion": "讨论了将系统扩展到城市范围部署的挑战和方向。"}}
{"id": "2510.16173", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16173", "abs": "https://arxiv.org/abs/2510.16173", "authors": ["Aria Pessianzadeh", "Naima Sultana", "Hildegarde Van den Bulck", "David Gefen", "Shahin Jabari", "Rezvaneh Rezapour"], "title": "In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions", "comment": null, "summary": "The rise of generative AI (GenAI) has impacted many aspects of human life. As\nthese systems become embedded in everyday practices, understanding public trust\nin them also becomes essential for responsible adoption and governance. Prior\nwork on trust in AI has largely drawn from psychology and human-computer\ninteraction, but there is a lack of computational, large-scale, and\nlongitudinal approaches to measuring trust and distrust in GenAI and large\nlanguage models (LLMs). This paper presents the first computational study of\nTrust and Distrust in GenAI, using a multi-year Reddit dataset (2022--2025)\nspanning 39 subreddits and 197,618 posts. Crowd-sourced annotations of a\nrepresentative sample were combined with classification models to scale\nanalysis. We find that Trust and Distrust are nearly balanced over time, with\nshifts around major model releases. Technical performance and usability\ndominate as dimensions, while personal experience is the most frequent reason\nshaping attitudes. Distinct patterns also emerge across trustors (e.g.,\nexperts, ethicists, general users). Our results provide a methodological\nframework for large-scale Trust analysis and insights into evolving public\nperceptions of GenAI.", "AI": {"tldr": "本文通过多年的Reddit数据研究了公众对生成式AI（GenAI）的信任和不信任，揭示了影响态度的技术维度和个体差异。", "motivation": "由于以往对AI的信任研究大多依赖于心理学和人机交互方法，且缺乏大规模的计算和纵向研究，这篇文章提出了首个对GenAI和大型语言模型的信任和不信任的计算研究。", "method": "使用了多年的Reddit数据集（2022-2025），涵盖了39个子版块和197,618篇帖子，结合众包注释和分类模型进行分析。", "result": "发现信任和不信任的态度在时间上几乎持平，并且在主要模型发布时发生变化。性能和技术的实用性是最主要的考量维度，而个人经验则是影响态度形成的最常见原因。不同对象（如专家、伦理学家、普通用户）也有各自不同的模式。", "conclusion": "研究结果为大规模信任分析提供了一个方法框架，并深入理解了公众对GenAI态度的演变。"}}
{"id": "2510.16036", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16036", "abs": "https://arxiv.org/abs/2510.16036", "authors": ["Zewen Li", "Zitong Yu", "Qilang Ye", "Weicheng Xie", "Wei Zhuo", "Linlin Shen"], "title": "IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection", "comment": "Accepted by IEEE Transactions on Instrumentation and Measurement\n  (TIM)", "summary": "The robust causal capability of Multimodal Large Language Models (MLLMs) hold\nthe potential of detecting defective objects in Industrial Anomaly Detection\n(IAD). However, most traditional IAD methods lack the ability to provide\nmulti-turn human-machine dialogues and detailed descriptions, such as the color\nof objects, the shape of an anomaly, or specific types of anomalies. At the\nsame time, methods based on large pre-trained models have not fully stimulated\nthe ability of large models in anomaly detection tasks. In this paper, we\nexplore the combination of rich text semantics with both image-level and\npixel-level information from images and propose IAD-GPT, a novel paradigm based\non MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate\ndetailed anomaly prompts for specific objects. These specific prompts from the\nlarge language model (LLM) are used to activate the detection and segmentation\nfunctions of the pre-trained visual-language model (i.e., CLIP). To enhance the\nvisual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein\nimage features interact with normal and abnormal text prompts to dynamically\nselect enhancement pathways, which enables language models to focus on specific\naspects of visual data, enhancing their ability to accurately interpret and\nrespond to anomalies within images. Moreover, we design a Multi-Mask Fusion\nmodule to incorporate mask as expert knowledge, which enhances the LLM's\nperception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA\ndatasets demonstrate our state-of-the-art performance on self-supervised and\nfew-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA\ndatasets. The codes are available at\n\\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.", "AI": {"tldr": "本文提出了一种新的基于多模态大规模语言模型IAD-GPT，用于工业异常检测，通过异常提示生成器，文本引导增强器及多掩码融合模块等改进了异常检测和分割的性能。实验表明方法有效。", "motivation": "尽管多模态大规模语言模型在工业异常检测中展现出强大的因果推理能力，但传统的工业异常检测方法缺乏提供多轮人类与机器对话和具体描述的能力。同时，基于大规模预训练模型的方法尚未完全激发这些模型在异常检测任务中的能力。因此，本文致力于丰富文本语义与图像级别、像素级别信息的结合，以提升异常检测的性能。", "method": "文章提出了一种新的基于多模态大规模语言模型（MLLM）进行工业异常检测(IAD)的方法，名为IAD-GPT。通过使用异常提示生成器（Abnormal Prompt Generator, APG）生成特定对象的详细异常提示，从大规模预训练语言模型中获取并激活视觉语言模型（如CLIP）的检测和分割功能。为了增强MLLM的视觉定位能力，提出了文本引导增强器(Text-Guided Enhancer)，使语言模型能够专注于视觉数据的特定方面，提高了对图像中异常的解释和响应准确性。此外，设计了一种多掩码融合模块，通过将掩码作为专家知识纳入，提升了模型对像素级别异常的感知能力。", "result": "在MVTec-AD和VisA数据集上的大量实验证明了该方法在自监督和少量样本异常检测和分割任务中的卓越性能。", "conclusion": "研究展示了通过结合丰富的文本语义与图像信息，使用多模态大规模语言模型进行工业异常检测的有效性。提出的IAD-GPT在自监督和少量样本的情况下，在异常检测和分割任务上表现出色，达到了当前最优水平。"}}
{"id": "2510.16198", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16198", "abs": "https://arxiv.org/abs/2510.16198", "authors": ["Mohamed Gamil", "Abdelrahman Elsayed", "Abdelrahman Lila", "Ahmed Gad", "Hesham Abdelgawad", "Mohamed Aref", "Ahmed Fares"], "title": "EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture", "comment": null, "summary": "Despite recent advances in AI, multimodal culturally diverse datasets are\nstill limited, particularly for regions in the Middle East and Africa. In this\npaper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian\nculture. By designing and running a new data collection pipeline, we collected\nover 3,000 images, covering 313 concepts across landmarks, food, and folklore.\nEach entry in the dataset is manually validated for cultural authenticity and\nmultimodal coherence. EgMM-Corpus aims to provide a reliable resource for\nevaluating and training vision-language models in an Egyptian cultural context.\nWe further evaluate the zero-shot performance of Contrastive Language-Image\nPre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and\n36.4% Top-5 accuracy in classification. These results underscore the existing\ncultural bias in large-scale vision-language models and demonstrate the\nimportance of EgMM-Corpus as a benchmark for developing culturally aware\nmodels.", "AI": {"tldr": "我们介绍了EgMM-Corpus，一个专注于埃及文化的多模态数据集，用于评估和训练视觉语言模型。EgMM-Corpus显示了现有大规模视觉语言模型存在的文化偏差。", "motivation": "尽管人工智能领域取得了进展，但多模态文化多样性数据集仍然有限，特别是在中东和非洲地区。本文旨在引入一个专注于埃及文化的多模态数据集。", "method": "我们设计并执行了一个新的数据收集流程，收集了超过3000张图像，涵盖了313个概念，包括地标、食物和民间传说。每个数据条目都经过手动验证，确保文化真实性和多模态一致性。", "result": "我们评估了在EgMM-Corpus上CLIP的零样本性能，其在分类任务上的Top-1和Top-5准确性分别为21.2%和36.4%。这强调了大规模视觉语言模型中存在的文化偏见。", "conclusion": "EgMM-Corpus作为一个基准数据集，对开发文化感知模型具有重要意义。"}}
{"id": "2510.16070", "categories": ["cs.CV", "cs.AI", "cs.HC", "eess.IV", "H.5.5; H.1.2; I.4.0"], "pdf": "https://arxiv.org/pdf/2510.16070", "abs": "https://arxiv.org/abs/2510.16070", "authors": ["Mahta Khoobi", "Marc Sebastian von der Stueck", "Felix Barajas Ordonez", "Anca-Maria Iancu", "Eric Corban", "Julia Nowak", "Aleksandar Kargaliev", "Valeria Perelygina", "Anna-Sophie Schott", "Daniel Pinto dos Santos", "Christiane Kuhl", "Daniel Truhn", "Sven Nebelung", "Robert Siepmann"], "title": "Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography", "comment": "Preprint version - Under second revision at Radiology (manuscript\n  RAD-25-1348)", "summary": "Structured reporting (SR) and artificial intelligence (AI) may transform how\nradiologists interact with imaging studies. This prospective study (July to\nDecember 2024) evaluated the impact of three reporting modes: free-text (FT),\nstructured reporting (SR), and AI-assisted structured reporting (AI-SR), on\nimage analysis behavior, diagnostic accuracy, efficiency, and user experience.\nFour novice and four non-novice readers (radiologists and medical students)\neach analyzed 35 bedside chest radiographs per session using a customized\nviewer and an eye-tracking system. Outcomes included diagnostic accuracy\n(compared with expert consensus using Cohen's $\\kappa$), reporting time per\nradiograph, eye-tracking metrics, and questionnaire-based user experience.\nStatistical analysis used generalized linear mixed models with Bonferroni\npost-hoc tests with a significance level of ($P \\le .01$). Diagnostic accuracy\nwas similar in FT ($\\kappa = 0.58$) and SR ($\\kappa = 0.60$) but higher in\nAI-SR ($\\kappa = 0.71$, $P < .001$). Reporting times decreased from $88 \\pm 38$\ns (FT) to $37 \\pm 18$ s (SR) and $25 \\pm 9$ s (AI-SR) ($P < .001$). Saccade\ncounts for the radiograph field ($205 \\pm 135$ (FT), $123 \\pm 88$ (SR), $97 \\pm\n58$ (AI-SR)) and total fixation duration for the report field ($11 \\pm 5$ s\n(FT), $5 \\pm 3$ s (SR), $4 \\pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P <\n.001$ each). Novice readers shifted gaze towards the radiograph in SR, while\nnon-novice readers maintained their focus on the radiograph. AI-SR was the\npreferred mode. In conclusion, SR improves efficiency by guiding visual\nattention toward the image, and AI-prefilled SR further enhances diagnostic\naccuracy and user satisfaction.", "AI": {"tldr": "研究显示，使用AI辅助的结构化报告（AI-SR）不仅提高了放射科医生的效率，缩短了报告时间，并提升了诊断准确性、用户体验，显示出比结构化报告（SR）和自由文本（FT）更佳的整体效果。", "motivation": "本研究旨在探讨结构化报告（SR）和人工智能（AI）如何改变放射科医生的影像分析方式，特别是关注SR与AI相结合的模式在提高效率和诊断准确性方面的潜力。", "method": "研究采用前瞻性设计（2024年7月至12月），评估了三种报告模式（自由文本FT、结构化报告SR及AI辅助结构化报告AI-SR）对影像分析行为、诊断准确性、效率及用户体验的影响。四名初学者和四名非初学者（包括放射科医生及医学生）每人分析了35张床边胸部X光片。使用定制的查看器和眼动追踪系统进行分析。关键结果通过与专家共识对比计算的Cohen's $\\\text{kappa}$值衡量，以及报告时间、眼动追踪指标及问卷调查用户的体验进行评估。", "result": "诊断准确性方面，FT组（$\\kappa = 0.58$）和SR组（$\\kappa = 0.60$）相似，但AI-SR组（$\\kappa = 0.71$，$P < .001$）更高。报告时间从FT组的$88 ± 38$秒减少至SR组的$37 ± 18$秒和AI-SR组的$25 ± 9$秒（$P < .001$）。初学者在SR模式下更专注于影像，而非初学者无论在哪种模式下都将目光集中在影像上。AI-SR是首选方式。", "conclusion": "结构化报告通过指导视觉注意力集中于影像提高了效率，而AI预填充的结构化报告进一步增强了诊断准确性和用户体验。"}}
{"id": "2510.16227", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16227", "abs": "https://arxiv.org/abs/2510.16227", "authors": ["Jennifer Hu", "Ethan Gotlieb Wilcox", "Siyuan Song", "Kyle Mahowald", "Roger P. Levy"], "title": "What Can String Probability Tell Us About Grammaticality?", "comment": null, "summary": "What have language models (LMs) learned about grammar? This question remains\nhotly debated, with major ramifications for linguistic theory. However, since\nprobability and grammaticality are distinct notions in linguistics, it is not\nobvious what string probabilities can reveal about an LM's underlying\ngrammatical knowledge. We present a theoretical analysis of the relationship\nbetween grammar, meaning, and string probability, based on simple assumptions\nabout the generative process of corpus data. Our framework makes three\npredictions, which we validate empirically using 280K sentence pairs in English\nand Chinese: (1) correlation between the probability of strings within minimal\npairs, i.e., string pairs with minimal semantic differences; (2) correlation\nbetween models' and humans' deltas within minimal pairs; and (3) poor\nseparation in probability space between unpaired grammatical and ungrammatical\nstrings. Our analyses give theoretical grounding for using probability to learn\nabout LMs' structural knowledge, and suggest directions for future work in LM\ngrammatical evaluation.", "AI": {"tldr": "文章提出理论框架研究语言模型对语法的理解并验证了三个关键假设。", "motivation": "探讨语言模型对语法的理解程度，分析字符串概率如何揭示语言模型的语法知识。", "method": "基于语料库数据生成过程的简单假设，本文提出了一种理论分析框架，用以探讨语法、意义和字符串概率之间的关系。", "result": "通过分析英语和中文的280,000个最小对，并验证了三个预测：最小对字符串的概率间的相关性，模型和人类的最小对差异的相关性，以及无配对语法和不语法字符串在概率空间中的差分程度较低。", "conclusion": "此分析为使用概率了解语言模型的结构知识提供了理论依据，并为未来的语言模型语法评估研究指明了方向。"}}
{"id": "2510.16072", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16072", "abs": "https://arxiv.org/abs/2510.16072", "authors": ["Farjana Yesmin"], "title": "Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation", "comment": "18 pages", "summary": "Machine learning models trained on imbalanced datasets often exhibit\nintersectional biases-systematic errors arising from the interaction of\nmultiple attributes such as object class and environmental conditions. This\npaper presents a data-driven framework for analyzing and mitigating such biases\nin image classification. We introduce the Intersectional Fairness Evaluation\nFramework (IFEF), which combines quantitative fairness metrics with\ninterpretability tools to systematically identify bias patterns in model\npredictions. Building on this analysis, we propose Bias-Weighted Augmentation\n(BWA), a novel data augmentation strategy that adapts transformation\nintensities based on subgroup distribution statistics. Experiments on the Open\nImages V7 dataset with five object classes demonstrate that BWA improves\naccuracy for underrepresented class-environment intersections by up to 24\npercentage points while reducing fairness metric disparities by 35%.\nStatistical analysis across multiple independent runs confirms the significance\nof improvements (p < 0.05). Our methodology provides a replicable approach for\nanalyzing and addressing intersectional biases in image classification systems.", "AI": {"tldr": "本文提出了一个数据驱动的框架来发现和缓解图像分类中由于多个属性交互导致的交叉偏见，提出Biased-Weighted Augmentation策略并证明其有效性。", "motivation": "旨在解决机器学习模型在训练不平衡数据集时出现的交互偏见问题，这些偏见是由多个属性之间的交互作用而产生的，如目标类别和环境条件。", "method": "介绍了一个名为Intersectional Fairness Evaluation Framework (IFEF)的数据驱动框架，用于分析和缓解图像分类中的交叉偏见。该框架结合了定量公平度量和可解释性工具，系统地识别模型预测中的偏见模式。基于此分析，提出了一种新的数据增强策略Bias-Weighted Augmentation (BWA)，该策略根据子组分布统计信息来调整变换强度。", "result": "实验表明，利用Open Images V7数据集在五大目标类别上的测试中，BWA策略能够提高代表性不足的类别-环境交叉的精确度达24个百分点，同时将公平度量差异减少35%。多种独立运行的统计分析证明了改进的显著性（p < 0.05）。", "conclusion": "该方法提供了一种可重复的方法，用于分析和解决图像分类系统中的交叉偏见问题。"}}
{"id": "2510.16257", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16257", "abs": "https://arxiv.org/abs/2510.16257", "authors": ["Chu Fei Luo", "Samuel Dahan", "Xiaodan Zhu"], "title": "Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback", "comment": "Findings of EMNLP 2025, 5 pages", "summary": "As language models have a greater impact on society, it is important to\nensure they are aligned to a diverse range of perspectives and are able to\nreflect nuance in human values. However, the most popular training paradigms\nfor modern language models often assume there is one optimal answer for every\nquery, leading to generic responses and poor alignment. In this work, we aim to\nenhance pluralistic alignment of language models in a low-resource setting with\ntwo methods: pluralistic decoding and model steering. We empirically\ndemonstrate that model steering offers consistent improvement over zero-shot\nand few-shot baselines with only 50 annotated samples. Our proposed methods\ndecrease false positives in several high-stakes tasks such as hate speech\ndetection and misinformation detection, and improves the distributional\nalignment to human values in GlobalOpinionQA. We hope our work highlights the\nimportance of diversity and how language models can be adapted to consider\nnuanced perspectives.", "AI": {"tldr": "Proposed pluralistic decoding and model steering to improve language model alignment and reflect diverse perspectives with minimal training resources.", "motivation": "Modern language models often assume there is one optimal answer for every query, leading to generic responses and poor alignment. The aim is to enhance pluralistic alignment in a low-resource setting.", "method": "Our proposed methods include pluralistic decoding and model steering to enhance pluralistic alignment in a low-resource setting.", "result": "Empirically demonstrated consistent improvement over zero-shot and few-shot baselines with only 50 annotated samples. Decreased false positives in hate speech detection, misinformation detection, and improved distributional alignment to human values in GlobalOpinionQA.", "conclusion": "Highlight the importance of diversity and how language models can be adapted to consider nuanced perspectives."}}
{"id": "2510.16088", "categories": ["cs.CV", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16088", "abs": "https://arxiv.org/abs/2510.16088", "authors": ["Zia Badar"], "title": "Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch", "comment": null, "summary": "Quantization of neural networks provides benefits of inference in less\ncompute and memory requirements. Previous work in quantization lack two\nimportant aspects which this work provides. First almost all previous work in\nquantization used a non-differentiable approach and for learning; the\nderivative is usually set manually in backpropogation which make the learning\nability of algorithm questionable, our approach is not just differentiable, we\nalso provide proof of convergence of our approach to the optimal neural\nnetwork. Second previous work in shift/logrithmic quantization either have\navoided activation quantization along with weight quantization or achieved less\naccuracy. Learning logrithmic quantize values of form $2^n$ requires the\nquantization function can scale to more than 1 bit quantization which is\nanother benifit of our quantization that it provides $n$ bits quantization as\nwell. Our approach when tested with image classification task using imagenet\ndataset, resnet18 and weight quantization only achieves less than 1 percent\naccuracy compared to full precision accuracy while taking only 15 epochs to\ntrain using shift bit quantization and achieves comparable to SOTA approaches\naccuracy in both weight and activation quantization using shift bit\nquantization in 15 training epochs with slightly higher(only higher cpu\ninstructions) inference cost compared to 1 bit quantization(without logrithmic\nquantization) and not requiring any higher precision multiplication.", "AI": {"tldr": "This paper presents a differentiable quantization method for neural networks, which maintains high accuracy with less computational and storage needs, and outperforms previous quantization techniques in terms of both accuracy and training efficiency.", "motivation": "The motivation is to address the limitations of non-differentiable quantization approaches and lack of effective activation quantization while keeping the accuracy high with lower computational and memory needs.", "method": "The paper proposes a differentiable quantization method for neural networks that converges to the optimal network and supports $n$ bits quantization, enabling both weight and activation quantization.", "result": "The method, when applied to an image classification task with the ImageNet dataset and ResNet18, achieved slightly less than 1 percent less accuracy compared to full precision using only weight quantization, and achieved state-of-the-art accuracy with both weight and activation quantization, while requiring only 15 epochs to train.", "conclusion": "The proposed quantization method provides a superior approach to quantize neural networks by ensuring differentiability, supporting convergence proof, and offering flexibility in bits quantization without sacrificing performance."}}
{"id": "2510.16282", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16282", "abs": "https://arxiv.org/abs/2510.16282", "authors": ["Zhaoxuan Tan", "Zixuan Zhang", "Haoyang Wen", "Zheng Li", "Rongzhi Zhang", "Pei Chen", "Fengran Mo", "Zheyuan Liu", "Qingkai Zeng", "Qingyu Yin", "Meng Jiang"], "title": "Instant Personalized Large Language Model Adaptation via Hypernetwork", "comment": null, "summary": "Personalized large language models (LLMs) tailor content to individual\npreferences using user profiles or histories. However, existing\nparameter-efficient fine-tuning (PEFT) methods, such as the\n``One-PEFT-Per-User'' (OPPU) paradigm, require training a separate adapter for\neach user, making them computationally expensive and impractical for real-time\nupdates. We introduce Profile-to-PEFT, a scalable framework that employs a\nhypernetwork, trained end-to-end, to map a user's encoded profile directly to a\nfull set of adapter parameters (e.g., LoRA), eliminating per-user training at\ndeployment. This design enables instant adaptation, generalization to unseen\nusers, and privacy-preserving local deployment. Experimental results\ndemonstrate that our method outperforms both prompt-based personalization and\nOPPU while using substantially fewer computational resources at deployment. The\nframework exhibits strong generalization to out-of-distribution users and\nmaintains robustness across varying user activity levels and different\nembedding backbones. The proposed Profile-to-PEFT framework enables efficient,\nscalable, and adaptive LLM personalization suitable for large-scale\napplications.", "AI": {"tldr": "本文提出 Profile-to-PEFT 框架，使用超网络将用户的编码资料直接映射到适配器参数，实现快速适应、未见用户泛化和隐私保护，且在资源消耗上更高效。", "motivation": "现有的参数高效微调（PEFT）方法，比如为每个用户训练一个适配器的单个PEFT（OPPU）范式，计算成本高且不适用于实时更新。为解决此问题，引入了 Profile-to-PEFT 框架。", "method": "通过使用超网络，Profile-to-PEFT 能够直接将用户的编码资料映射到一组完整的适配器参数（例如 LoRA），从而在部署时无需针对每个用户进行单独训练。", "result": "实验结果显示，本方法在减少部署计算资源需求的同时，不仅超越了基于提示的个性化，也优于OPPU。", "conclusion": "通过 Profile-to-PEFT 框架，LLM 的个性化能够在大规模应用中实现高效、可扩展和适应性强的个性化服务。"}}
{"id": "2510.16115", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16115", "abs": "https://arxiv.org/abs/2510.16115", "authors": ["Jianhan Lin", "Yuchu Qin", "Shuai Gao", "Yikang Rui", "Jie Liu", "Yanjie Lv"], "title": "StripRFNet: A Strip Receptive Field and Shape-Aware Network for Road Damage Detection", "comment": null, "summary": "Well-maintained road networks are crucial for achieving Sustainable\nDevelopment Goal (SDG) 11. Road surface damage not only threatens traffic\nsafety but also hinders sustainable urban development. Accurate detection,\nhowever, remains challenging due to the diverse shapes of damages, the\ndifficulty of capturing slender cracks with high aspect ratios, and the high\nerror rates in small-scale damage recognition. To address these issues, we\npropose StripRFNet, a novel deep neural network comprising three modules: (1) a\nShape Perception Module (SPM) that enhances shape discrimination via large\nseparable kernel attention (LSKA) in multi-scale feature aggregation; (2) a\nStrip Receptive Field Module (SRFM) that employs large strip convolutions and\npooling to capture features of slender cracks; and (3) a Small-Scale\nEnhancement Module (SSEM) that leverages a high-resolution P2 feature map, a\ndedicated detection head, and dynamic upsampling to improve small-object\ndetection. Experiments on the RDD2022 benchmark show that StripRFNet surpasses\nexisting methods. On the Chinese subset, it improves F1-score, mAP50, and\nmAP50:95 by 4.4, 2.9, and 3.4 percentage points over the baseline,\nrespectively. On the full dataset, it achieves the highest F1-score of 80.33%\ncompared with CRDDC'2022 participants and ORDDC'2024 Phase 2 results, while\nmaintaining competitive inference speed. These results demonstrate that\nStripRFNet achieves state-of-the-art accuracy and real-time efficiency,\noffering a promising tool for intelligent road maintenance and sustainable\ninfrastructure management.", "AI": {"tldr": "StripRFNet通过三个特定模块提升道路损伤检测的准确性和实时性，在多个评估指标上超越了现有方法，为道路维护提供了先进的解决方案。", "motivation": "研究背景在于准确识别道路损伤的挑战性，例如损伤多样形状、难以捕捉细长裂纹以及小损伤识别高误差率。", "method": "StripRFNet包含三个模块：SPM增强了形状的多尺度特征聚合的区分度；SRFM捕捉到了细长裂纹的特征；SSEM提升了小孔径目标的检测性能。", "result": "StripRFNet 是一种新型的深度神经网络，旨在提高道路损伤检测的准确性。该网络通过三大模块实现这一目标：1) 形状感知模块（SPM），通过大规模分离内核注意力提升多尺度特征聚合的形状区分度；2) 条带感受野模块（SRFM），使用大条带卷积和池化策略捕捉细长裂纹的特征；3) 小尺度增强模块（SSEM），通过高分辨率 P2 特征图、专业检测头及动态上采样技术提高小目标的检测性能。实验结果表明显RFNet在RDD2022基准上超越现有方法，分别提升F1分数、mAP50和mAP50:95 4.4、2.9和3.4个百分点。在中国的子集上同样取得最佳F1分数80.33%，同时也保持了实时推理速度。总体上，StripRFNet在准确性和实时性上都达到了先进水平，为道路智能维护和可持久基础设施管理提供了一个有希望的工具。", "conclusion": "实验结果证明StripRFNet在RDD2022基准上的性能优越，尤其是在中国的数据子集上的表现更为突出。StripRFNet在提升检测准确性的同时保留了实时性的优势，表现出了可持续基础设施管理的前景。"}}
{"id": "2510.16340", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16340", "abs": "https://arxiv.org/abs/2510.16340", "authors": ["Pratham Singla", "Shivank Garg", "Ayush Singh", "Ishan Garg", "Ketan Suhaas Saichandran"], "title": "Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models", "comment": null, "summary": "Recent advances in post-training techniques have endowed Large Language\nModels (LLMs) with enhanced capabilities for tackling complex, logic-intensive\ntasks through the generation of supplementary planning tokens. This development\nraises a fundamental question: Are these models aware of what they \"learn\" and\n\"think\"? To address this, we define three core competencies: (1) awareness of\nlearned latent policies, (2) generalization of these policies across domains,\nand (3) alignment between internal reasoning traces and final outputs. We\nempirically evaluate these abilities on several tasks, each designed to require\nlearning a distinct policy. Furthermore, we contrast the profiles of models\npost-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization\n(DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate\nthat RL-trained models not only demonstrate greater awareness of their learned\nbehaviors and stronger generalizability to novel, structurally similar tasks\nthan SFT models but also often exhibit weak alignment between their reasoning\ntraces and final outputs, an effect most pronounced in GRPO-trained models.", "AI": {"tldr": "本文探讨了通过后训练技术赋予大型语言模型的自我意识能力，研究发现强化学习训练的模型在认识学习行为和任务泛化能力方面表现较好，但推理过程与最终输出之间却存在弱对齐现象。", "motivation": "最近后训练技术的进步使大型语言模型能够通过生成辅助规划标记来应对复杂的逻辑密集型任务，此背景下提出对模型是否意识到其\"学习\"和\"思考\"的探讨。", "method": "本文通过定义三大核心能力来评估模型的自我意识：1) 对学习到的潜在策略的认识；2) 这些策略在不同领域的泛化能力；3) 内部推理轨迹与最终输出之间的对齐情况。这些能力在设计用于要求学习不同策略的任务上进行实证评估。", "result": "研究发现，通过强化学习训练的模型不仅在对学习行为的认识以及对于结构相似的新任务上的泛化能力方面优于监督微调的模型，还经常表现出推理轨迹与最终输出之间的弱对齐，这种现象在通过组相对策略优化训练的模型中最为明显。", "conclusion": "研究结论表明，虽然强化学习训练的模型（尤其是直接策略优化和组相对策略优化）在自我意识和任务泛化能力上表现较强，但也存在推理过程与最终输出之间不对齐的问题。"}}
{"id": "2510.16118", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16118", "abs": "https://arxiv.org/abs/2510.16118", "authors": ["Nishad Sahu", "Shounak Sural", "Aditya Satish Patil", "Ragunathan", "Rajkumar"], "title": "ObjectTransforms for Uncertainty Quantification and Reduction in Vision-Based Perception for Autonomous Vehicles", "comment": "Accepted at International Conference on Computer Vision (ICCV) 2025\n  Workshops", "summary": "Reliable perception is fundamental for safety critical decision making in\nautonomous driving. Yet, vision based object detector neural networks remain\nvulnerable to uncertainty arising from issues such as data bias and\ndistributional shifts. In this paper, we introduce ObjectTransforms, a\ntechnique for quantifying and reducing uncertainty in vision based object\ndetection through object specific transformations at both training and\ninference times. At training time, ObjectTransforms perform color space\nperturbations on individual objects, improving robustness to lighting and color\nvariations. ObjectTransforms also uses diffusion models to generate realistic,\ndiverse pedestrian instances. At inference time, object perturbations are\napplied to detected objects and the variance of detection scores are used to\nquantify predictive uncertainty in real time. This uncertainty signal is then\nused to filter out false positives and also recover false negatives, improving\nthe overall precision recall curve. Experiments with YOLOv8 on the NuImages 10K\ndataset demonstrate that our method yields notable accuracy improvements and\nuncertainty reduction across all object classes during training, while\npredicting desirably higher uncertainty values for false positives as compared\nto true positives during inference. Our results highlight the potential of\nObjectTransforms as a lightweight yet effective mechanism for reducing and\nquantifying uncertainty in vision-based perception during training and\ninference respectively.", "AI": {"tldr": "本论文引入了ObjectTransforms技术，用于量化和减少基于视觉的对象检测中的不确定性。该技术提高了鲁棒性，增强了精度召回曲线，且在训练和推断阶段表现出较低的不确定性和较高的准确性。", "motivation": "视觉基础的对象检测神经网络容易受到如数据偏差和分布变化等不确定性的影响。我们旨在通过引入ObjectTransforms解决这一问题，提升自动驾驶中关键决策的安全性。", "method": "ObjectTransforms技术通过在训练和推断阶段对特定对象进行变换来量化和减少基于视觉的对象检测中的不确定性。训练阶段，ObjectTransforms在对象上执行颜色空间扰动来提高对光照和颜色变化的鲁棒性，并使用扩散模型生成真实且多样的行人实例。推断阶段，对检测到的对象应用扰动，并使用检测评分的方差来量化实时预测不确定性。", "result": "实验结果显示，相较于基线模型，在NuImages 10K数据集上使用YOLOv8时，ObjectTransforms可以显著提升所有对象类别的训练准确性，减少不确定性，并在推断阶段为假正例预测出比真实正例更高的不确定性值。", "conclusion": "实验结果强调了ObjectTransforms作为一种轻量有效的方法，对于减少和量化基于视觉感知中训练和推断阶段的不确定性具有潜在价值。"}}
{"id": "2510.16359", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16359", "abs": "https://arxiv.org/abs/2510.16359", "authors": ["Utsav Dhanuka", "Soham Poddar", "Saptarshi Ghosh"], "title": "Utilising Large Language Models for Generating Effective Counter Arguments to Anti-Vaccine Tweets", "comment": "14 pages, 1 figure, work done as a part of B.Tech project at IIT\n  Kharagpur", "summary": "In an era where public health is increasingly influenced by information\nshared on social media, combatting vaccine skepticism and misinformation has\nbecome a critical societal goal. Misleading narratives around vaccination have\nspread widely, creating barriers to achieving high immunisation rates and\nundermining trust in health recommendations. While efforts to detect\nmisinformation have made significant progress, the generation of real time\ncounter-arguments tailored to debunk such claims remains an insufficiently\nexplored area. In this work, we explore the capabilities of LLMs to generate\nsound counter-argument rebuttals to vaccine misinformation. Building on prior\nresearch in misinformation debunking, we experiment with various prompting\nstrategies and fine-tuning approaches to optimise counter-argument generation.\nAdditionally, we train classifiers to categorise anti-vaccine tweets into\nmulti-labeled categories such as concerns about vaccine efficacy, side effects,\nand political influences allowing for more context aware rebuttals. Our\nevaluation, conducted through human judgment, LLM based assessments, and\nautomatic metrics, reveals strong alignment across these methods. Our findings\ndemonstrate that integrating label descriptions and structured fine-tuning\nenhances counter-argument effectiveness, offering a promising approach for\nmitigating vaccine misinformation at scale.", "AI": {"tldr": "研究利用大语言模型生成针对疫苗错误信息的有效反驳策略，通过分类和微调方法优化反驳生成，评估表明该方法具有缓解疫苗错误信息的潜力。", "motivation": "当前，社交媒体上疫苗怀疑论和错误信息泛滥，导致免疫接种率低并削弱了公众对健康建议的信任。尽管在检测错误信息方面取得了一定进展，但实时生成针对性反驳的领域仍待进一步探索。", "method": "研究通过各种提示策略和微调方法来优化针对疫苗错误信息的反驳生成，并通过训练分类器将反对疫苗的推文归类到多个类别中，如对疫苗功效、副作用及政治影响的担忧，用以生成更有针对性的反驳。", "result": "研究通过人类判断、基于LLM的评估和自动指标进行的评估显示，这些方法之间存在强烈的一致性。研究发现，结合标签描述和结构化微调可以提高反驳的有效性，为大规模缓解疫苗错误信息提供了一种有前景的方法。", "conclusion": "整合标签描述和结构化微调能够增强针对疫苗错误信息的反驳效果，有助于大规模缓解疫苗错误信息。"}}
{"id": "2510.16134", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16134", "abs": "https://arxiv.org/abs/2510.16134", "authors": ["Chen Kong", "James Fort", "Aria Kang", "Jonathan Wittmer", "Simon Green", "Tianwei Shen", "Yipu Zhao", "Cheng Peng", "Gustavo Solaira", "Andrew Berkovich", "Nikhil Raina", "Vijay Baiyya", "Evgeniy Oleinik", "Eric Huang", "Fan Zhang", "Julian Straub", "Mark Schwesinger", "Luis Pesqueira", "Xiaqing Pan", "Jakob Julian Engel", "Carl Ren", "Mingfei Yan", "Richard Newcombe"], "title": "Aria Gen 2 Pilot Dataset", "comment": null, "summary": "The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset\ncaptured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely\naccess, A2PD is released incrementally with ongoing dataset enhancements. The\ninitial release features Dia'ane, our primary subject, who records her daily\nactivities alongside friends, each equipped with Aria Gen 2 glasses. It\nencompasses five primary scenarios: cleaning, cooking, eating, playing, and\noutdoor walking. In each of the scenarios, we provide comprehensive raw sensor\ndata and output data from various machine perception algorithms. These data\nillustrate the device's ability to perceive the wearer, the surrounding\nenvironment, and interactions between the wearer and the environment, while\nmaintaining robust performance across diverse users and conditions. The A2PD is\npublicly available at projectaria.com, with open-source tools and usage\nexamples provided in Project Aria Tools.", "AI": {"tldr": "Aria Gen 2 Pilot Dataset（A2PD）是一个公开的第一人称多模态数据集，使用Aria Gen 2眼镜捕捉数据，适用于多种场景，具有良好的鲁棒性。", "motivation": "创建Aria Gen 2 Pilot Dataset（A2PD）是为了提供一个及时更新的、公开的、第一人称视角的多模态数据集，以促进相关研究的发展。", "method": "该数据集使用尖端的Aria Gen 2眼镜捕捉第一人称多模式数据，包含了关于日常生活活动的多种传感器数据和机器感知算法输出。通过多个场景，展示了设备感知佩戴者、周围环境及两者之间交互的能力，并具有跨用户和条件的鲁棒性能。", "result": "该数据集的初始版本已发布，并可通过projectaria.com访问。提供了开源工具和使用示例，方便研究者理解和使用数据。", "conclusion": "A2PD展示了尖端技术在捕捉和处理多模态数据方面的潜力，为未来的研究提供了一种宝贵的资源。"}}
{"id": "2510.16363", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16363", "abs": "https://arxiv.org/abs/2510.16363", "authors": ["Nilmadhab Das", "Vishal Vaibhav", "Yash Sunil Choudhary", "V. Vijaya Saradhi", "Ashish Anand"], "title": "End-to-End Argument Mining through Autoregressive Argumentative Structure Prediction", "comment": "Accepted version. To appear in IJCNN 2025", "summary": "Argument Mining (AM) helps in automating the extraction of complex\nargumentative structures such as Argument Components (ACs) like Premise, Claim\netc. and Argumentative Relations (ARs) like Support, Attack etc. in an\nargumentative text. Due to the inherent complexity of reasoning involved with\nthis task, modelling dependencies between ACs and ARs is challenging. Most of\nthe recent approaches formulate this task through a generative paradigm by\nflattening the argumentative structures. In contrast to that, this study\njointly formulates the key tasks of AM in an end-to-end fashion using\nAutoregressive Argumentative Structure Prediction (AASP) framework. The\nproposed AASP framework is based on the autoregressive structure prediction\nframework that has given good performance for several NLP tasks. AASP framework\nmodels the argumentative structures as constrained pre-defined sets of actions\nwith the help of a conditional pre-trained language model. These actions build\nthe argumentative structures step-by-step in an autoregressive manner to\ncapture the flow of argumentative reasoning in an efficient way. Extensive\nexperiments conducted on three standard AM benchmarks demonstrate that AASP\nachieves state-of-theart (SoTA) results across all AM tasks in two benchmarks\nand delivers strong results in one benchmark.", "AI": {"tldr": "本文介绍了一种新的论证挖掘方法AASP框架，基于自回归结构预测，通过一系列预定义动作逐步构建论证结构，提升了论证推理的效果。", "motivation": "由于论证挖掘任务中内在推理的复杂性，建模论证组件和论证关系之间的依赖关系具有挑战性。当前大多数方法通过生成范式来处理该任务，而本文提出了一种新的方法。", "method": "AASP框架使用自回归结构预测技术，基于条件预训练语言模型，通过预定义的一系列动作逐步构建论证结构，以有效捕捉论证推理流程。", "result": "在三个标准论证挖掘基准测试中，AASP框架在两个基准上实现了最先进的结果，在一个基准上也取得了很强的结果。", "conclusion": "实验结果表明，在三个标准论证挖掘基准测试中，AASP框架在两个基准上实现了最先进的结果，在一个基准上也取得了很强的结果。"}}
{"id": "2510.16136", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.16136", "abs": "https://arxiv.org/abs/2510.16136", "authors": ["Sayan Deb Sarkar", "Sinisa Stekovic", "Vincent Lepetit", "Iro Armeni"], "title": "GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer", "comment": "NeurIPS 2025. Project Page: https://sayands.github.io/guideflow3d/", "summary": "Transferring appearance to 3D assets using different representations of the\nappearance object - such as images or text - has garnered interest due to its\nwide range of applications in industries like gaming, augmented reality, and\ndigital content creation. However, state-of-the-art methods still fail when the\ngeometry between the input and appearance objects is significantly different. A\nstraightforward approach is to directly apply a 3D generative model, but we\nshow that this ultimately fails to produce appealing results. Instead, we\npropose a principled approach inspired by universal guidance. Given a\npretrained rectified flow model conditioned on image or text, our training-free\nmethod interacts with the sampling process by periodically adding guidance.\nThis guidance can be modeled as a differentiable loss function, and we\nexperiment with two different types of guidance including part-aware losses for\nappearance and self-similarity. Our experiments show that our approach\nsuccessfully transfers texture and geometric details to the input 3D asset,\noutperforming baselines both qualitatively and quantitatively. We also show\nthat traditional metrics are not suitable for evaluating the task due to their\ninability of focusing on local details and comparing dissimilar inputs, in\nabsence of ground truth data. We thus evaluate appearance transfer quality with\na GPT-based system objectively ranking outputs, ensuring robust and human-like\nassessment, as further confirmed by our user study. Beyond showcased scenarios,\nour method is general and could be extended to different types of diffusion\nmodels and guidance functions.", "AI": {"tldr": "提出一种基于预训练修正流模型并周期性添加引导的方法，以改进外观转移到3D资产的效果，实验结果表明此方法优于基线。", "motivation": "外观转移至具有不同几何结构的3D资产时，现有的最先进方法往往效果不佳。而单纯采用3D生成模型又容易产生不理想的结果。因此，提出这种新的基于引导的方法以改进外观转移的效果。", "method": "我们的方法是基于预训练的修正流模型，在该模型采样过程中周期性地加入引导以改进3D资产的外观转移效果。引导被建模为可微分的损失函数，具体实验了两种类型的引导：针对外观的部件感知损失和自我相似性损失。", "result": "实验表明，这种方法在转移到3D资产的纹理和几何细节上超过基线方法，在定性定量分析中都有出色表现。此外，传统评估指标不适合此任务，我们采用了基于GPT的系统进行结果评估，该系统能更公正且符合人类评价标准。用户研究也进一步确认了这一评估方法的有效性。", "conclusion": "研究表明，我们提出的方法成功转移了纹理和几何细节，且超越了其他方法在定性和定量上的评估。该方法在不同扩散模型和引导函数上的通用性也为未来更多应用场景提供了可能性。"}}
{"id": "2510.16373", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16373", "abs": "https://arxiv.org/abs/2510.16373", "authors": ["Federico Ravenda", "Seyed Ali Bahrainian", "Andrea Raballo", "Antonietta Mira"], "title": "Navigating through the hidden embedding space: steering LLMs to improve mental health assessment", "comment": null, "summary": "The rapid evolution of Large Language Models (LLMs) is transforming AI,\nopening new opportunities in sensitive and high-impact areas such as Mental\nHealth (MH). Yet, despite these advancements, recent evidence reveals that\nsmaller-scale models still struggle to deliver optimal performance in\ndomain-specific applications. In this study, we present a cost-efficient yet\npowerful approach to improve MH assessment capabilities of an LLM, without\nrelying on any computationally intensive techniques. Our lightweight method\nconsists of a linear transformation applied to a specific layer's activations,\nleveraging steering vectors to guide the model's output. Remarkably, this\nintervention enables the model to achieve improved results across two distinct\ntasks: (1) identifying whether a Reddit post is useful for detecting the\npresence or absence of depressive symptoms (relevance prediction task), and (2)\ncompleting a standardized psychological screening questionnaire for depression\nbased on users' Reddit post history (questionnaire completion task). Results\nhighlight the untapped potential of steering mechanisms as computationally\nefficient tools for LLMs' MH domain adaptation.", "AI": {"tldr": "本研究提出了一种轻量级方法，通过线性转换和引导向量来提高大型语言模型在心理健康评估中的效能，证明了这种方法在不使用计算密集型技术的情况下可以优化模型在相关任务中的性能。", "motivation": "尽管大型语言模型的发展正在改变AI领域，提供了在心理健康等敏感且高影响力领域的新机会，但研究证据表明，小型模型尚难以在特定领域应用中实现最佳性能。因此，本研究旨在提出一种成本效益高且有效的改进方法。", "method": "本研究提出了一种轻量级的方法，该方法通过对特定层的激活进行线性转换，并利用引导向量来指导模型的输出，从而在不依赖任何计算密集型技术的情况下提高大型语言模型在心理健康评估方面的能力。", "result": "该干预措施使模型在两个任务中取得了更好的结果：（1）判断Reddit帖子是否有助于检测抑郁症状的存在与否（相关预测任务），（2）基于用户的Reddit帖子历史完成标准化的心理筛查问卷来诊断抑郁症（问卷完成任务）。", "conclusion": "研究结果突显了引导机制作为计算效率工具，在大型语言模型心理健康领域适应中的未开发潜力。"}}
{"id": "2510.16145", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16145", "abs": "https://arxiv.org/abs/2510.16145", "authors": ["Ahmad Arrabi", "Jay hwasung Jung", "J Le", "A Nguyen", "J Reed", "E Stahl", "Nathan Franssen", "Scott Raymond", "Safwan Wshah"], "title": "C-arm Guidance: A Self-supervised Approach To Automated Positioning During Stroke Thrombectomy", "comment": null, "summary": "Thrombectomy is one of the most effective treatments for ischemic stroke, but\nit is resource and personnel-intensive. We propose employing deep learning to\nautomate critical aspects of thrombectomy, thereby enhancing efficiency and\nsafety. In this work, we introduce a self-supervised framework that classifies\nvarious skeletal landmarks using a regression-based pretext task. Our\nexperiments demonstrate that our model outperforms existing methods in both\nregression and classification tasks. Notably, our results indicate that the\npositional pretext task significantly enhances downstream classification\nperformance. Future work will focus on extending this framework toward fully\nautonomous C-arm control, aiming to optimize trajectories from the pelvis to\nthe head during stroke thrombectomy procedures. All code used is available at\nhttps://github.com/AhmadArrabi/C_arm_guidance", "AI": {"tldr": "本文提出了一种使用深度学习的方法来自动化血栓切除术中的关键步骤，提高治疗效率和安全性，取得了显著成果。", "motivation": "由于血栓切除术资源和人力需求量大，本文提出通过自动化关键步骤来提高该治疗方式的效率和安全性。", "method": "使用深度学习和自监督框架，通过回归预设任务来分类不同的骨骼标记点，以此来自动化血栓切除术的关键步骤。", "result": "实验表明，该模型在回归和分类任务上均优于现有方法，并且位置预设任务显著提高了分类性能。", "conclusion": "未来工作的重点是扩展该框架以实现完全自主的C臂控制，优化缺血性脑卒中血栓切除术中从骨盆到头部的轨迹。"}}
{"id": "2510.16380", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16380", "abs": "https://arxiv.org/abs/2510.16380", "authors": ["Yu Ying Chiu", "Michael S. Lee", "Rachel Calcott", "Brandon Handoko", "Paul de Font-Reaulx", "Paula Rodriguez", "Chen Bo Calvin Zhang", "Ziwen Han", "Udari Madhushani Sehwag", "Yash Maurya", "Christina Q Knight", "Harry R. Lloyd", "Florence Bacus", "Mantas Mazeika", "Bing Liu", "Yejin Choi", "Mitchell L Gordon", "Sydney Levine"], "title": "MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes", "comment": "46 pages, 8 figures, 10 tables. Preprint", "summary": "As AI systems progress, we rely more on them to make decisions with us and\nfor us. To ensure that such decisions are aligned with human values, it is\nimperative for us to understand not only what decisions they make but also how\nthey come to those decisions. Reasoning language models, which provide both\nfinal responses and (partially transparent) intermediate thinking traces,\npresent a timely opportunity to study AI procedural reasoning. Unlike math and\ncode problems which often have objectively correct answers, moral dilemmas are\nan excellent testbed for process-focused evaluation because they allow for\nmultiple defensible conclusions. To do so, we present MoReBench: 1,000 moral\nscenarios, each paired with a set of rubric criteria that experts consider\nessential to include (or avoid) when reasoning about the scenarios. MoReBench\ncontains over 23 thousand criteria including identifying moral considerations,\nweighing trade-offs, and giving actionable recommendations to cover cases on AI\nadvising humans moral decisions as well as making moral decisions autonomously.\nSeparately, we curate MoReBench-Theory: 150 examples to test whether AI can\nreason under five major frameworks in normative ethics. Our results show that\nscaling laws and existing benchmarks on math, code, and scientific reasoning\ntasks fail to predict models' abilities to perform moral reasoning. Models also\nshow partiality towards specific moral frameworks (e.g., Benthamite Act\nUtilitarianism and Kantian Deontology), which might be side effects of popular\ntraining paradigms. Together, these benchmarks advance process-focused\nreasoning evaluation towards safer and more transparent AI.", "AI": {"tldr": "该论文提出MoReBench和MoReBench-Theory，用于评估AI在道德困境中的程序性推理能力，强调了透明性和对多种道德框架的理解，发现现有评估方法不能准确预测AI进行道德推理的能力。", "motivation": "随着AI系统的发展，为了使AI决策更好地符合人类价值观，需要对AI决策过程进行透明化和评估，特别是在道德困境中。", "method": "作者创建了MoReBench数据集，包括1000个道德场景和23000多个评估标准，以及MoReBench-Theory数据集，包含150个测试AI是否能在五个主要伦理学框架中进行合理推理的示例。", "result": "实验结果表明，规模扩张和现有评估方法不能准确预测模型在道德推理任务上的表现，模型在某些道德框架上有偏好。", "conclusion": "这些基准测试推动了合理性和透明度在AI推理评估中的应用，有望促使AI更好地为人类道德决策提供建议或自主做出伦理选择。"}}
{"id": "2510.16146", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16146", "abs": "https://arxiv.org/abs/2510.16146", "authors": ["Thanh-Huy Nguyen", "Hoang-Thien Nguyen", "Vi Vu", "Ba-Thinh Lam", "Phat Huynh", "Tianyang Wang", "Xingjian Li", "Ulas Bagci", "Min Xu"], "title": "DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via Decoupled Branch Optimization", "comment": "The paper is under review at CMIG", "summary": "The limited availability of annotated data in medical imaging makes\nsemi-supervised learning increasingly appealing for its ability to learn from\nimperfect supervision. Recently, teacher-student frameworks have gained\npopularity for their training benefits and robust performance. However, jointly\noptimizing the entire network can hinder convergence and stability, especially\nin challenging scenarios. To address this for medical image segmentation, we\npropose DuetMatch, a novel dual-branch semi-supervised framework with\nasynchronous optimization, where each branch optimizes either the encoder or\ndecoder while keeping the other frozen. To improve consistency under noisy\nconditions, we introduce Decoupled Dropout Perturbation, enforcing\nregularization across branches. We also design Pair-wise CutMix Cross-Guidance\nto enhance model diversity by exchanging pseudo-labels through augmented input\npairs. To mitigate confirmation bias from noisy pseudo-labels, we propose\nConsistency Matching, refining labels using stable predictions from frozen\nteacher models. Extensive experiments on benchmark brain MRI segmentation\ndatasets, including ISLES2022 and BraTS, show that DuetMatch consistently\noutperforms state-of-the-art methods, demonstrating its effectiveness and\nrobustness across diverse semi-supervised segmentation scenarios.", "AI": {"tldr": "DuetMatch, a novel semi-supervised learning framework for medical image segmentation, addresses challenges in robustness and convergence with asynchronous optimization techniques and diverse regularization methods.", "motivation": "To overcome the limitations of joint optimization and improve robustness in medical image segmentation under semi-supervised conditions.", "method": "DuetMatch, a dual-branch semi-supervised framework with asynchronous optimization, Decoupled Dropout Perturbation, Pair-wise CutMix Cross-Guidance, and Consistency Matching techniques are introduced.", "result": "DuetMatch outperforms state-of-the-art methods on benchmark brain MRI segmentation datasets (ISLES2022 and BraTS).", "conclusion": "The proposed DuetMatch framework demonstrates superior performance and robustness in semi-supervised medical image segmentation tasks."}}
{"id": "2510.16381", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16381", "abs": "https://arxiv.org/abs/2510.16381", "authors": ["David Peer", "Sebastian Stabinger"], "title": "ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities, yet\ntheir deployment in high-stakes domains is hindered by inherent limitations in\ntrustworthiness, including hallucinations, instability, and a lack of\ntransparency. To address these challenges, we introduce a generic\nneuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The\ncore of our approach lies in decoupling tasks into two distinct phases: Offline\nknowledge ingestion and online task processing. During knowledge ingestion, an\nLLM translates an informal problem specification into a formal, symbolic\nknowledge base. This formal representation is crucial as it can be verified and\nrefined by human experts, ensuring its correctness and alignment with domain\nrequirements. In the subsequent task processing phase, each incoming input is\nencoded into the same formal language. A symbolic decision engine then utilizes\nthis encoded input in conjunction with the formal knowledge base to derive a\nreliable result. Through an extensive evaluation on a complex reasoning task,\nwe demonstrate that a concrete implementation of ATA is competitive with\nstate-of-the-art end-to-end reasoning models in a fully automated setup while\nmaintaining trustworthiness. Crucially, with a human-verified and corrected\nknowledge base, our approach significantly outperforms even larger models,\nwhile exhibiting perfect determinism, enhanced stability against input\nperturbations, and inherent immunity to prompt injection attacks. By generating\ndecisions grounded in symbolic reasoning, ATA offers a practical and\ncontrollable architecture for building the next generation of transparent,\nauditable, and reliable autonomous agents.", "AI": {"tldr": "论文介绍了一个名为ATA的神经符号方法，通过将问题转换成可验证的知识库并结合人类专家调整来提升大型语言模型的可靠性和透明度，实验表明该方法显著提升了模型的信任度和性能。", "motivation": "动机是解决大型语言模型在可靠性和透明度方面的限制，这些限制阻碍了它们在高风险领域中的应用。", "method": "我们的方法提出了一种称为自主可信代理（ATA）的神经符号方法来解决大型语言模型在高风险领域部署中的信任问题。方法包括两个主要阶段：离线知识摄入和在线任务处理。离线阶段中，LLM将非正式的问题规范转换成符号形式的知识库，并由人类专家验证和调整以保证其准确性和符合领域要求。在线阶段处理任务时，将每个输入编码为相同的符号形式，并通过符号决策引擎与知识库结合生成可靠结果。", "result": "实验结果表明，ATA的具体实现可以在全自动设置下与先进的端到端推理模型竞争，同时保持信任度。经过人类验证和修正的知识库使我们方法显著超越更大模型，表现出完美确定性，增强的输入扰动稳定性和对提示注入攻击的内在免疫力。", "conclusion": "结论是，ATA通过符号推理产生决策，提供了一个实用可靠的架构来构建下一代透明、可查核和可靠的自主代理。"}}
{"id": "2510.16160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16160", "abs": "https://arxiv.org/abs/2510.16160", "authors": ["Ahmad Arrabi", "Jay Hwasung Jung", "Jax Luo", "Nathan Franssen", "Scott Raymond", "Safwan Wshah"], "title": "Automated C-Arm Positioning via Conformal Landmark Localization", "comment": null, "summary": "Accurate and reliable C-arm positioning is essential for fluoroscopy-guided\ninterventions. However, clinical workflows rely on manual alignment that\nincreases radiation exposure and procedural delays. In this work, we present a\npipeline that autonomously navigates the C-arm to predefined anatomical\nlandmarks utilizing X-ray images. Given an input X-ray image from an arbitrary\nstarting location on the operating table, the model predicts a 3D displacement\nvector toward each target landmark along the body. To ensure reliable\ndeployment, we capture both aleatoric and epistemic uncertainties in the\nmodel's predictions and further calibrate them using conformal prediction. The\nderived prediction regions are interpreted as 3D confidence regions around the\npredicted landmark locations. The training framework combines a probabilistic\nloss with skeletal pose regularization to encourage anatomically plausible\noutputs. We validate our approach on a synthetic X-ray dataset generated from\nDeepDRR. Results show not only strong localization accuracy across multiple\narchitectures but also well-calibrated prediction bounds. These findings\nhighlight the pipeline's potential as a component in safe and reliable\nautonomous C-arm systems. Code is available at\nhttps://github.com/AhmadArrabi/C_arm_guidance_APAH", "AI": {"tldr": "开发了一种新的C形臂自动导航系统，利用X光图像预测向预定义解剖标志的3D位移矢量，并结合不确定性校准以确保可靠部署，从而提高荧光镜引导手术的安全性和准确性。", "motivation": "减少手动对齐C形臂带来的辐射暴露和程序延迟，提高荧光镜引导手术的准确性和可靠性。", "method": "使用基于X光图像的模型预测向解剖标志的3D位移矢量，结合不确定性校正（both aleatoric and epistemic uncertainties），并通过利用DeepDRR生成的合成X光数据集进行训练。", "result": "方法在多种架构下显示出强大的定位精度和校准的预测边界，表明该方法在安全可靠的自主C形臂系统中的潜力。", "conclusion": "开发的系统在准确性、安全性以及降低辐射暴露方面有潜力，对于自主C形臂系统是一个有益的组成部分。"}}
{"id": "2510.16387", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16387", "abs": "https://arxiv.org/abs/2510.16387", "authors": ["Fu-An Chao", "Bi-Cheng Yan", "Berlin Chen"], "title": "Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment", "comment": null, "summary": "In this paper, we explore the untapped potential of Whisper, a\nwell-established automatic speech recognition (ASR) foundation model, in the\ncontext of L2 spoken language assessment (SLA). Unlike prior studies that\nextrinsically analyze transcriptions produced by Whisper, our approach goes a\nstep further to probe its latent capabilities by extracting acoustic and\nlinguistic features from hidden representations. With only a lightweight\nclassifier being trained on top of Whisper's intermediate and final outputs,\nour method achieves strong performance on the GEPT picture-description dataset,\noutperforming existing cutting-edge baselines, including a multimodal approach.\nFurthermore, by incorporating image and text-prompt information as auxiliary\nrelevance cues, we demonstrate additional performance gains. Finally, we\nconduct an in-depth analysis of Whisper's embeddings, which reveals that, even\nwithout task-specific fine-tuning, the model intrinsically encodes both ordinal\nproficiency patterns and semantic aspects of speech, highlighting its potential\nas a powerful foundation for SLA and other spoken language understanding tasks.", "AI": {"tldr": "The paper explores the use of Whisper, an ASR foundation model, for L2 spoken language assessment (SLA) by extracting acoustic and linguistic features from its hidden representations, achieving strong performance without task-specific fine-tuning.", "motivation": "The motivation behind this study is to understand the inherent capabilities of Whisper in SLA tasks, moving beyond previous extrinsic analysis and showcasing its potential in understanding language proficiency and semantic aspects of speech.", "method": "The method involves extracting features from Whisper's hidden states and training a lightweight classifier on top of these features to assess L2 spoken language proficiency. Image and text-prompt information are used as auxiliary relevance cues.", "result": "The results demonstrate superior performance compared to existing baselines, including a multimodal approach, on the GEPT picture-description dataset. An analysis of Whisper's embeddings shows intrinsic encoding of proficiency patterns and semantic aspects.", "conclusion": "The paper concludes that Whisper exhibits strong potential as a foundational model for SLA and similar tasks without task-specific fine-tuning, highlighting its value in understanding spoken language proficiency and semantic richness."}}
{"id": "2510.16179", "categories": ["cs.CV", "I.4.9"], "pdf": "https://arxiv.org/pdf/2510.16179", "abs": "https://arxiv.org/abs/2510.16179", "authors": ["Xavier Giro-i-Nieto", "Nefeli Andreou", "Anqi Liang", "Manel Baradad", "Francesc Moreno-Noguer", "Aleix Martinez"], "title": "Cost Savings from Automatic Quality Assessment of Generated Images", "comment": null, "summary": "Deep generative models have shown impressive progress in recent years, making\nit possible to produce high quality images with a simple text prompt or a\nreference image. However, state of the art technology does not yet meet the\nquality standards offered by traditional photographic methods. For this reason,\nproduction pipelines that use generated images often include a manual stage of\nimage quality assessment (IQA). This process is slow and expensive, especially\nbecause of the low yield of automatically generated images that pass the\nquality bar. The IQA workload can be reduced by introducing an automatic\npre-filtering stage, that will increase the overall quality of the images sent\nto review and, therefore, reduce the average cost required to obtain a high\nquality image. We present a formula that estimates the cost savings depending\non the precision and pass yield of a generic IQA engine. This formula is\napplied in a use case of background inpainting, showcasing a significant cost\nsaving of 51.61% obtained with a simple AutoML solution.", "AI": {"tldr": "The paper discusses reducing the cost of image quality assessment (IQA) in production pipelines using deep generative models by introducing an automatic pre-filtering stage, resulting in a significant cost saving of 51.61% in a background inpainting use case using a simple AutoML solution.", "motivation": "The motivation behind the research is the inefficiency and high cost of manual IQA in production pipelines using generated images, aiming to improve the overall quality and lower the cost by implementing an automatic pre-filtering system.", "method": "The method involves developing a formula to estimate cost savings from an automatic pre-filtering stage and applying this formula in a background inpainting case study with an AutoML solution.", "result": "The result shows a significant reduction in cost, specifically 51.61%, due to the application of the automatic pre-filtering stage with a simple AutoML solution in the background inpainting use case.", "conclusion": "The conclusion is that automatic pre-filtering can notably reduce the cost and improve the efficiency of image quality assessment in deep generative models by filtering out poorer quality images before they reach the manual assessment stage."}}
{"id": "2510.16439", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16439", "abs": "https://arxiv.org/abs/2510.16439", "authors": ["Syed Rifat Raiyan", "Md Farhan Ishmam", "Abdullah Al Imran", "Mohammad Ali Moni"], "title": "FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution", "comment": null, "summary": "Large language models (LLMs) owe much of their stellar performance to\nexpansive input contexts, yet such verbosity inflates monetary costs, carbon\nfootprint, and inference-time latency. Much of this overhead manifests from the\nredundant low-utility tokens present in typical prompts, as only a fraction of\ntokens typically carries the majority of the semantic weight. We address this\ninefficiency by introducing FrugalPrompt, a novel prompt compression framework\nfor LLMs, which retains only the most semantically significant tokens.\nLeveraging two state-of-the-art token attribution methods, GlobEnc and DecompX,\nwe assign salience scores to every token in an input sequence, rank them to\npreserve the top-k% tokens in their original order, and obtain a sparse\nfrugalized prompt. We evaluate the approach across four NLP tasks: Sentiment\nAnalysis, Commonsense QA, Summarization, and Mathematical Reasoning, using a\nsuite of frontier LLMs. For the first three tasks, a 20% prompt reduction\nincurs only a marginal loss in task performance, demonstrating that\ncontemporary LLMs can reconstruct elided context from high-salience cues. In\ncontrast, performance on mathematical reasoning deteriorates sharply,\nreflecting a stronger dependence on complete token continuity. Further analysis\nwith bottom-k% and random-k% tokens reveals asymmetric performance patterns\nthat may suggest potential task contamination effects, wherein models may\nresort to shallow memorized patterns from pretraining exposure for conventional\nNLP tasks. We posit that our work contributes to a more nuanced understanding\nof LLM behavior in performance-efficiency trade-offs, and delineate the\nboundary between tasks tolerant to contextual sparsity and those requiring\nexhaustive context. Our source code and models are available at:\nhttps://github.com/Starscream-11813/Frugal-ICL", "AI": {"tldr": "论文介绍了FrugalPrompt，一种用于压缩大语言模型(LLMs)的提示框架，通过只保留语义上最重要的标记来减少LLMs的冗余并提高效率。", "motivation": "大语言模型（LLMs）的卓越性能很大程度上归功于其扩展的输入上下文，但这种冗长增加了经济成本、碳足迹和推理时间延迟。", "method": "FrugalPrompt，一种新的提示压缩框架，用于大语言模型，该框架保留了语义上最重要的标记。通过使用两种最先进的标记归因方法（GlobEnc 和 DecompX），为输入序列中的每个标记分配重要性评分，然后对其进行排序，保留前k%的标记以获得稀疏化的简明提示。", "result": "实验在四个NLP任务（情感分析、常识问答、总结和数学推理）上进行，并使用了一系列前沿的LLMs。对于前三个任务，减少20%的提示只导致了轻微的性能下降，表明现代LLMs可以从高相关性线索中重建被删除的上下文。然而，在数学推理任务上的表现急剧下降，反映出对该任务需要完整的标记连续性。此外，使用低重要性和随机性的标记百分比进行进一步分析揭示了不对称的表现模式，可能是由于模型利用了在常规NLP任务上的浅层模式记忆。", "conclusion": "该工作为理解LLM在性能和效率之间的权衡提供了更细致的认识，并界定了哪些任务对上下文的稀疏性具有耐受性，以及哪些任务需要完整上下文。"}}
{"id": "2510.16196", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16196", "abs": "https://arxiv.org/abs/2510.16196", "authors": ["Zheng Huang", "Enpei Zhang", "Yinghao Cai", "Weikang Qiu", "Carl Yang", "Elynn Chen", "Xiang Zhang", "Rex Ying", "Dawei Zhou", "Yujun Yan"], "title": "Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI", "comment": null, "summary": "Understanding how the brain encodes visual information is a central challenge\nin neuroscience and machine learning. A promising approach is to reconstruct\nvisual stimuli, essentially images, from functional Magnetic Resonance Imaging\n(fMRI) signals. This involves two stages: transforming fMRI signals into a\nlatent space and then using a pretrained generative model to reconstruct\nimages. The reconstruction quality depends on how similar the latent space is\nto the structure of neural activity and how well the generative model produces\nimages from that space. Yet, it remains unclear which type of latent space best\nsupports this transformation and how it should be organized to represent visual\nstimuli effectively. We present two key findings. First, fMRI signals are more\nsimilar to the text space of a language model than to either a vision based\nspace or a joint text image space. Second, text representations and the\ngenerative model should be adapted to capture the compositional nature of\nvisual stimuli, including objects, their detailed attributes, and\nrelationships. Building on these insights, we propose PRISM, a model that\nProjects fMRI sIgnals into a Structured text space as an interMediate\nrepresentation for visual stimuli reconstruction. It includes an object centric\ndiffusion module that generates images by composing individual objects to\nreduce object detection errors, and an attribute relationship search module\nthat automatically identifies key attributes and relationships that best align\nwith the neural activity. Extensive experiments on real world datasets\ndemonstrate that our framework outperforms existing methods, achieving up to an\n8% reduction in perceptual loss. These results highlight the importance of\nusing structured text as the intermediate space to bridge fMRI signals and\nimage reconstruction.", "AI": {"tldr": "研究发现，将fMRI信号转换到类似语言模型的文本空间比直接转换到视觉或联合文本图像空间更有效。通过提出将fMRI信号投影到一个结构化文本空间的PRISM模型，实验表明这种方法在视觉刺激重建中优于以往方法。", "motivation": "理解大脑如何编码视觉信息是神经科学和机器学习中的一个中心挑战。研究希望通过从功能磁共振成像(fMRI)信号中重建视觉刺激，如图像，来推进这一理解。这个过程涉及到将fMRI信号转换成潜在空间，然后使用预训练的生成模型来重建图像。", "method": "本研究提出了一个名为PRISM的模型，该模型通过将fMRI信号投影到一个结构化的文本空间中，以此作为视觉刺激重建过程中的中间表示。模型包括一个基于对象的扩散模块，通过组合单独的对象来减少对象检测错误，以及一个属性关系搜索模块，自动识别与神经活动最吻合的关键属性和关系。", "result": "实验结果表明，本研究的方法优于现有的方法，感知损失减少了高达8%。这些结果强调了使用结构化的文本空间作为中间表示形式，来连接fMRI信号和图像重建的重要性。", "conclusion": "研究结论指出，使用结构化的文本空间作为fMRI信号和图像重建之间的桥梁，具有显著的优越性，特别是对于理解大脑如何编码和重建视觉信息方面。"}}
{"id": "2510.16449", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16449", "abs": "https://arxiv.org/abs/2510.16449", "authors": ["Bin Yu", "Xinming Wang", "Shijie Lian", "Haotian Li", "Changti Wu", "Ruina Hu", "Bailing Wang", "Yuliang Wei", "Kai Chen"], "title": "TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model", "comment": "13 pages, 6 figures. Project website:\n  https://zgca-ai4edu.github.io/TrajSelector", "summary": "Large language models (LLMs) have shown remarkable progress in complex\nreasoning tasks, largely enabled by test-time scaling (TTS) paradigms that\nallocate additional compute during inference. Among these, external TTS\n(particularly the Best-of-N selection paradigm) yields scalable performance\nimprovements by selecting from multiple independently generated reasoning\ntrajectories. However, this approach faces key limitations: (i) the high\ncomputational overhead of deploying process reward models, (ii) the\nunderutilization of the LLM's intrinsic latent representations. We introduce\nTrajSelector, an efficient and effective Best-of-N framework that exploit the\nhidden states in the sampler LLM for process-level scoring. A lightweight\nverifier (with only 0.6B parameters) evaluates the quality of step-wise\ntrajectory, and then aggregates these scores to identify the optimal reasoning\ntrajectory. Our framework employs a fully data-driven, end-to-end training\nrecipe that eliminates reliance on massive step-level annotations. Experiential\nresults across five benchmarks demonstrate that TrajSelector delivers\nconsistent performance gains. In Best-of-32 settings, it surpasses majority\nvoting by 4.61% accuracy and outperforms existing process reward models by\n4.31% to 12.21%, all while maintaining lower inference costs.", "AI": {"tldr": "本文引入了TrajSelector，一个有效利用LLM隐状态进行合理过程评分的Best-of-N框架，并在多个基准测试中证明了其高性能和较低的计算成本。", "motivation": "大型语言模型（LLMs）在复杂推理任务中显示出显著的进步，这主要归功于测试时扩展（TTS）范式。然而，当前的TTS方法存在高计算开销和潜在表示利用不足的问题。", "method": "我们介绍了一个名为TrajSelector的有效框架，该框架利用采样器LLM中的隐藏状态进行过程评分。一个轻量级的验证器（仅0.6B参数）评估逐步轨迹的质量，然后聚合这些评分以识别最佳的推理轨迹。框架使用完全基于数据驱动的端到端训练配方，消除了对大量步骤级标注的依赖。", "result": "实验结果表明，在五个基准上，TrajSelector提供了持续的性能改进。在Best-of-32设置下，该框架比多数投票准确率高出4.61%，与现有过程奖励模型相比，提高了4.31%到12.21%，同时保持了较低的推理成本。", "conclusion": "TrajSelector为解决外部TTS方法中的高计算开销和潜在表示利用不足的问题提供了一种高效且有效的解决方案。"}}
{"id": "2510.16207", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16207", "abs": "https://arxiv.org/abs/2510.16207", "authors": ["Mateus Pinto da Silva", "Sabrina P. L. P. Correa", "Hugo N. Oliveira", "Ian M. Nunes", "Jefersson A. dos Santos"], "title": "Data-Centric AI for Tropical Agricultural Mapping: Challenges, Strategies and Scalable Solutions", "comment": "5 pages, 1 figure", "summary": "Mapping agriculture in tropical areas through remote sensing presents unique\nchallenges, including the lack of high-quality annotated data, the elevated\ncosts of labeling, data variability, and regional generalisation. This paper\nadvocates a Data-Centric Artificial Intelligence (DCAI) perspective and\npipeline, emphasizing data quality and curation as key drivers for model\nrobustness and scalability. It reviews and prioritizes techniques such as\nconfident learning, core-set selection, data augmentation, and active learning.\nThe paper highlights the readiness and suitability of 25 distinct strategies in\nlarge-scale agricultural mapping pipelines. The tropical context is of high\ninterest, since high cloudiness, diverse crop calendars, and limited datasets\nlimit traditional model-centric approaches. This tutorial outlines practical\nsolutions as a data-centric approach for curating and training AI models better\nsuited to the dynamic realities of tropical agriculture. Finally, we propose a\npractical pipeline using the 9 most mature and straightforward methods that can\nbe applied to a large-scale tropical agricultural mapping project.", "AI": {"tldr": "本文使用数据导向的人工智能方法应对热带农业遥感制图的挑战，提出并评估了几种关键的数据处理技术，并推荐了一个适合大规模热带农业制图的实用方法。", "motivation": "本文旨在通过数据导向的人工智能（DCAI）方法解决热带地区农业遥感制图面对的独特挑战，如高质量标注数据的缺乏、标注成本高、数据变异性以及区域泛化难题。", "method": "本文优先考虑并审查了诸如自信学习、核心集选择、数据增强和主动学习等关键数据处理技术。", "result": "在大规模农业制图管道中评估了25种不同策略的适用性和成熟度。", "conclusion": "提出了一个实用的管道，采用了9种最成熟和简单的方法，用于大规模热带农业遥感制图项目。"}}
{"id": "2510.16455", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16455", "abs": "https://arxiv.org/abs/2510.16455", "authors": ["Deyi Ji", "Yuekui Yang", "Haiyang Wu", "Shaoping Ma", "Tianrun Chen", "Lanyun Zhu"], "title": "RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning", "comment": "ACL 2025 (Oral, Industry Track)", "summary": "Advertisement (Ad) video violation detection is critical for ensuring\nplatform compliance, but existing methods struggle with precise temporal\ngrounding, noisy annotations, and limited generalization. We propose RAVEN, a\nnovel framework that integrates curriculum reinforcement learning with\nmultimodal large language models (MLLMs) to enhance reasoning and cognitive\ncapabilities for violation detection. RAVEN employs a progressive training\nstrategy, combining precisely and coarsely annotated data, and leverages Group\nRelative Policy Optimization (GRPO) to develop emergent reasoning abilities\nwithout explicit reasoning annotations. Multiple hierarchical sophisticated\nreward mechanism ensures precise temporal grounding and consistent category\nprediction. Experiments on industrial datasets and public benchmarks show that\nRAVEN achieves superior performances in violation category accuracy and\ntemporal interval localization. We also design a pipeline to deploy the RAVEN\non the online Ad services, and online A/B testing further validates its\npractical applicability, with significant improvements in precision and recall.\nRAVEN also demonstrates strong generalization, mitigating the catastrophic\nforgetting issue associated with supervised fine-tuning.", "AI": {"tldr": "文章引入RAVEN框架，通过课程强化学习和多模态大型语言模型相结合的方法，改善了广告视频违规检测中的时间定位精确性、注释噪声问题，并展示了良好的泛化能力。", "motivation": "现有的广告视频违规检测方法在精确时间定位、嘈杂注释和有限泛化能力方面存在挑战。因此，提出了一种结合课程强化学习和多模态大型语言模型的新颖框架RAVEN，以增强违规检测的推理和认知能力。", "method": "RAVEN框架结合了课程强化学习与多模态大型语言模型（MLLMs），通过渐进式训练策略，结合精确和粗糙注释的数据，并利用Group Relative Policy Optimization（GRPO）来发展不需显式推理注释的新兴推理能力。采用多级复杂的奖励机制以确保精确的时间定位和一致的类别预测。", "result": "实验结果显示，RAVEN在工业数据集和公共基准上对违规类别的准确性以及时间间隔定位上的性能表现优异。在线A/B测试进一步验证了其实际应用的有效性，并显著提高了精度和召回率。", "conclusion": "RAVEN具有较强的泛化能力，可以克服与监督微调相关的灾难性遗忘问题。在线A/B测试验证了它的实际应用价值。"}}
{"id": "2510.16209", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16209", "abs": "https://arxiv.org/abs/2510.16209", "authors": ["Nyle Siddiqui", "Rohit Gupta", "Sirnam Swetha", "Mubarak Shah"], "title": "StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales", "comment": null, "summary": "State space models (SSMs) have emerged as a competitive alternative to\ntransformers in various tasks. Their linear complexity and hidden-state\nrecurrence make them particularly attractive for modeling long sequences,\nwhereas attention becomes quadratically expensive. However, current training\nmethods for video understanding are tailored towards transformers and fail to\nfully leverage the unique attributes of SSMs. For example, video models are\noften trained at a fixed resolution and video length to balance the quadratic\nscaling of attention cost against performance. Consequently, these models\nsuffer from degraded performance when evaluated on videos with spatial and\ntemporal resolutions unseen during training; a property we call spatio-temporal\ninflexibility. In the context of action recognition, this severely limits a\nmodel's ability to retain performance across both short- and long-form videos.\nTherefore, we propose a flexible training method that leverages and improves\nthe inherent adaptability of SSMs. Our method samples videos at varying\ntemporal and spatial resolutions during training and dynamically interpolates\nmodel weights to accommodate any spatio-temporal scale. This instills our SSM,\nwhich we call StretchySnake, with spatio-temporal flexibility and enables it to\nseamlessly handle videos ranging from short, fine-grained clips to long,\ncomplex activities. We introduce and compare five different variants of\nflexible training, and identify the most effective strategy for video SSMs. On\nshort-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks,\nStretchySnake outperforms transformer and SSM baselines alike by up to 28%,\nwith strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore,\nour method provides a simple drop-in training recipe that makes video SSMs more\nrobust, resolution-agnostic, and efficient across diverse action recognition\nscenarios.", "AI": {"tldr": "A flexible training method named StretchySnake is introduced for SSMs in video understanding, enhancing their spatio-temporal adaptability and robustness to varying video resolutions, significantly outperforming transformer and SSM baselines.", "motivation": "The motivation stems from the fact that current training methods for video understanding, which are primarily designed for transformers, do not fully leverage the unique capabilities of SSMs, particularly their linear complexity and hidden-state recurrence. This leads to a performance drop when the model is faced with unseen resolutions, a problem this paper aims to solve.", "method": "The method proposes a flexible training approach for State Space Models (SSMs) in video understanding, involving training models on videos of varying spatial and temporal resolutions, and dynamically interpolating model weights to adapt to any spatio-temporal scale.", "result": "The proposed StretchySnake outperforms transformer and traditional SSM baselines by up to 28% on various action recognition benchmarks, showcasing strong adaptability to both short- and long-form videos, and fine-grained actions.", "conclusion": "The flexible training method effectively enhances SSMs' capabilities in video understanding by making them resolution-agnostic and more robust, providing a simple yet powerful solution that outperforms existing models across a range of benchmarks."}}
{"id": "2510.16458", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16458", "abs": "https://arxiv.org/abs/2510.16458", "authors": ["Pingjun Hong", "Beiduo Chen", "Siyao Peng", "Marie-Catherine de Marneffe", "Benjamin Roth", "Barbara Plank"], "title": "Agree, Disagree, Explain: Decomposing Human Label Variation in NLI through the Lens of Explanations", "comment": null, "summary": "Natural Language Inference datasets often exhibit human label variation. To\nbetter understand these variations, explanation-based approaches analyze the\nunderlying reasoning behind annotators' decisions. One such approach is the\nLiTEx taxonomy, which categorizes free-text explanations in English into\nreasoning types. However, previous work applying such taxonomies has focused on\nwithin-label variation: cases where annotators agree on the final NLI label but\nprovide different explanations. In contrast, this paper broadens the scope by\nexamining how annotators may diverge not only in the reasoning type but also in\nthe labeling step. We use explanations as a lens to decompose the reasoning\nprocess underlying NLI annotation and to analyze individual differences. We\napply LiTEx to two NLI English datasets and align annotation variation from\nmultiple aspects: NLI label agreement, explanation similarity, and taxonomy\nagreement, with an additional compounding factor of annotators' selection bias.\nWe observe instances where annotators disagree on the label but provide highly\nsimilar explanations, suggesting that surface-level disagreement may mask\nunderlying agreement in interpretation. Moreover, our analysis reveals\nindividual preferences in explanation strategies and label choices. These\nfindings highlight that agreement in reasoning types better reflects the\nsemantic similarity of free-text explanations than label agreement alone. Our\nfindings underscore the richness of reasoning-based explanations and the need\nfor caution in treating labels as ground truth.", "AI": {"tldr": "本研究通过分析自然语言推理数据集中的注释者差异，发现注释者在解释上的共识比标签一致性更能反映自由文本解释的语义相似性。", "motivation": "过去的工作主要集中在标签内的变化，例如注释者在最终NLI标签上达成一致，但提供的解释不同。本文将视角拓宽，不仅考察注释者在推理类型上的分歧，还考察他们在标注步骤上的分歧。", "method": "通过将LiTEx分类法应用于两个英文NLI数据集，研究从多个角度（包括NLI标签一致性、解释相似性和分类法一致性以及注释者的选取偏差）来对注释者之间的差异进行对齐。", "result": "观察到在标签不一致的情况下，注释者提供高相似度的解释，这表明表面上的分歧可能掩盖了解释背后的共识。此外，分析揭示了注释者在解释策略和标签选择上的个体偏好。", "conclusion": "研究结果强调了基于推理的解释的丰富性，并强调需谨慎对待将标签当作绝对标准的做法。"}}
{"id": "2510.16220", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16220", "abs": "https://arxiv.org/abs/2510.16220", "authors": ["Djamel Eddine Boukhari"], "title": "VM-BeautyNet: A Synergistic Ensemble of Vision Transformer and Mamba for Facial Beauty Prediction", "comment": null, "summary": "Facial Beauty Prediction (FBP) is a complex and challenging computer vision\ntask, aiming to model the subjective and intricate nature of human aesthetic\nperception. While deep learning models, particularly Convolutional Neural\nNetworks (CNNs), have made significant strides, they often struggle to capture\nthe global, holistic facial features that are critical to human judgment.\nVision Transformers (ViT) address this by effectively modeling long-range\nspatial relationships, but their quadratic complexity can be a bottleneck. This\npaper introduces a novel, heterogeneous ensemble architecture,\n\\textbf{VM-BeautyNet}, that synergistically fuses the complementary strengths\nof a Vision Transformer and a Mamba-based Vision model, a recent advancement in\nState-Space Models (SSMs). The ViT backbone excels at capturing global facial\nstructure and symmetry, while the Mamba backbone efficiently models long-range\ndependencies with linear complexity, focusing on sequential features and\ntextures. We evaluate our approach on the benchmark SCUT-FBP5500 dataset. Our\nproposed VM-BeautyNet achieves state-of-the-art performance, with a\n\\textbf{Pearson Correlation (PC) of 0.9212}, a \\textbf{Mean Absolute Error\n(MAE) of 0.2085}, and a \\textbf{Root Mean Square Error (RMSE) of 0.2698}.\nFurthermore, through Grad-CAM visualizations, we provide interpretability\nanalysis that confirms the complementary feature extraction of the two\nbackbones, offering new insights into the model's decision-making process and\npresenting a powerful new architectural paradigm for computational aesthetics.", "AI": {"tldr": "论文提出一种新的混合模型VM-BeautyNet用于面部美感预测，结合了视觉变换器和Mamba模型，实现了全球最佳性能。", "motivation": "面部美感预测是一个复杂的任务，深度学习模型在捕获整体面部特征方面存在困难。为了解决这个问题，论文提出了一种结合两种模型优势的新方法。", "method": "该论文提出了一种新型的异构集成架构 VM-BeautyNet，它将视觉变换器（ViT）和基于Mamba的视觉模型的优点相结合。ViT擅长捕捉面部的整体结构和对称性，而Mamba则在处理长距离依赖和纹理方面更有效，且复杂度更低。", "result": "在基准SCUT-FBP5500数据集上，VM-BeautyNet达到了最先进的性能，取得了Pearson相关系数（PC）0.9212，平均绝对误差（MAE）0.2085以及均方根误差（RMSE）0.2698的成果。", "conclusion": "通过Grad-CAM可视化提供的可解释性分析，证实了两种模型的互补特征提取能力，为计算美学提供了一个强大的新架构范例。"}}
{"id": "2510.16492", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16492", "abs": "https://arxiv.org/abs/2510.16492", "authors": ["Vamshi Krishna Bonagiri", "Ponnurangam Kumaragurum", "Khanh Nguyen", "Benjamin Plaut"], "title": "Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety", "comment": "Reliable ML and Regulatable ML workshops, Neurips 2025", "summary": "As Large Language Model (LLM) agents increasingly operate in complex\nenvironments with real-world consequences, their safety becomes critical. While\nuncertainty quantification is well-studied for single-turn tasks, multi-turn\nagentic scenarios with real-world tool access present unique challenges where\nuncertainties and ambiguities compound, leading to severe or catastrophic risks\nbeyond traditional text generation failures. We propose using \"quitting\" as a\nsimple yet effective behavioral mechanism for LLM agents to recognize and\nwithdraw from situations where they lack confidence. Leveraging the ToolEmu\nframework, we conduct a systematic evaluation of quitting behavior across 12\nstate-of-the-art LLMs. Our results demonstrate a highly favorable\nsafety-helpfulness trade-off: agents prompted to quit with explicit\ninstructions improve safety by an average of +0.39 on a 0-3 scale across all\nmodels (+0.64 for proprietary models), while maintaining a negligible average\ndecrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding\nexplicit quit instructions proves to be a highly effective safety mechanism\nthat can immediately be deployed in existing agent systems, and establishes\nquitting as an effective first-line defense mechanism for autonomous agents in\nhigh-stakes applications.", "AI": {"tldr": "研究了在多轮次交互场景中，通过使大语言模型主体具有退出机制来提高其安全性的方法。结果表明，这种方法在提高安全性的同时，几乎不影响模型的帮助性。", "motivation": "大语言模型代理在复杂环境中操作时，其安全性变得至关重要。然而，对于多轮次代理场景中的不确定性量化仍然是一个挑战。提出了一种简单的退出行为机制，以便代理在缺乏信心时能够认识到并退出这些情况。", "method": "通过使用ToolEmu框架，对12个最先进的大语言模型进行了系统性的退出行为评估。", "result": "结果显示，通过明确定义的退出指令，可以在所有模型的安全性上平均提高+0.39（在专有模型上提高了+0.64），同时在可用性上的平均降低仅为-0.03。", "conclusion": "简单地添加明确的退出指令是一种有效的安全机制，可以立即部署于现有的代理系统，并为高风险应用中的自主代理建立了退出作为一种有效的防御机制。"}}
{"id": "2510.16235", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16235", "abs": "https://arxiv.org/abs/2510.16235", "authors": ["Vishal Manikanden", "Aniketh Bandlamudi", "Daniel Haehn"], "title": "Designing a Convolutional Neural Network for High-Accuracy Oral Cavity Squamous Cell Carcinoma (OCSCC) Detection", "comment": null, "summary": "Oral Cavity Squamous Cell Carcinoma (OCSCC) is the most common type of head\nand neck cancer. Due to the subtle nature of its early stages, deep and hidden\nareas of development, and slow growth, OCSCC often goes undetected, leading to\npreventable deaths. However, properly trained Convolutional Neural Networks\n(CNNs), with their precise image segmentation techniques and ability to apply\nkernel matrices to modify the RGB values of images for accurate image pattern\nrecognition, would be an effective means for early detection of OCSCC. Pairing\nthis neural network with image capturing and processing hardware would allow\nincreased efficacy in OCSCC detection. The aim of our project is to develop a\nConvolutional Neural Network trained to recognize OCSCC, as well as to design a\nphysical hardware system to capture and process detailed images, in order to\ndetermine the image quality required for accurate predictions. A CNN was\ntrained on 4293 training images consisting of benign and malignant tumors, as\nwell as negative samples, and was evaluated for its precision, recall, and Mean\nAverage Precision (mAP) in its predictions of OCSCC. A testing dataset of\nrandomly assorted images of cancerous, non-cancerous, and negative images was\nchosen, and each image was altered to represent 5 common resolutions. This test\ndata set was thoroughly analyzed by the CNN and predictions were scored on the\nbasis of accuracy. The designed enhancement hardware was used to capture\ndetailed images, and its impact was scored. An application was developed to\nfacilitate the testing process and bring open access to the CNN. Images of\nincreasing resolution resulted in higher-accuracy predictions on a logarithmic\nscale, demonstrating the diminishing returns of higher pixel counts.", "AI": {"tldr": "开发了一种用于识别口腔鳞状细胞癌（OCSCC）的卷积神经网络（CNN），结合物理硬件系统提高OCSCC检测效率。研究显示，图像分辨率越高，预测的准确性越高，但在对数尺度上显示出递减的收益。", "motivation": "由于OCSCC在早期阶段难以察觉，导致较高的死亡率。使用经过训练的CNN能有效提高OCSCC的早期检测率，从而降低死亡率。", "method": "使用卷积神经网络（CNN）对口腔鳞状细胞癌（OCSCC）进行识别。CNN通过精确的图像分割技术来修改图像的RGB值，提高图像模式识别的准确性。硬件系统用于图像捕捉和处理，以确定准确预测所需的图像质量。", "result": "使用4293张训练图像（包括良性、恶性肿瘤及阴性样本）训练的CNN，其预测的精度、召回率和均值平均精度（mAP）被评估。通过图像分辨率变换进行测试，发现图像分辨率越高，预测准确性越高，但呈现递减的收益。", "conclusion": "开发了用于识别OCSCC的CNN，并设计了硬件系统来捕捉和处理细节图像。结果显示，更高分辨率的图像能提高预测准确性，但是像素增加到一定程度后收益递减。"}}
{"id": "2510.16499", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16499", "abs": "https://arxiv.org/abs/2510.16499", "authors": ["Michelle Yuan", "Khushbu Pahwa", "Shuaichen Chang", "Mustafa Kaba", "Jiarong Jiang", "Xiaofei Ma", "Yi Zhang", "Monica Sunkara"], "title": "Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection", "comment": "Accepted to NeurIPS 2025 Conference", "summary": "Designing effective agentic systems requires the seamless composition and\nintegration of agents, tools, and models within dynamic and uncertain\nenvironments. Most existing methods rely on static, semantic retrieval\napproaches for tool or agent discovery. However, effective reuse and\ncomposition of existing components remain challenging due to incomplete\ncapability descriptions and the limitations of retrieval methods. Component\nselection suffers because the decisions are not based on capability, cost, and\nreal-time utility. To address these challenges, we introduce a structured,\nautomated framework for agentic system composition that is inspired by the\nknapsack problem. Our framework enables a composer agent to systematically\nidentify, select, and assemble an optimal set of agentic components by jointly\nconsidering performance, budget constraints, and compatibility. By dynamically\ntesting candidate components and modeling their utility in real-time, our\napproach streamlines the assembly of agentic systems and facilitates scalable\nreuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five\nbenchmarking datasets shows that our online-knapsack-based composer\nconsistently lies on the Pareto frontier, achieving higher success rates at\nsignificantly lower component costs compared to our baselines. In the\nsingle-agent setup, the online knapsack composer shows a success rate\nimprovement of up to 31.6% in comparison to the retrieval baselines. In\nmulti-agent systems, the online knapsack composer increases success rate from\n37% to 87% when agents are selected from an agent inventory of 100+ agents. The\nsubstantial performance gap confirms the robust adaptability of our method\nacross diverse domains and budget constraints.", "AI": {"tldr": "本文提出了一种新的代理系统组件组合框架，通过优化选择和组装，提升了系统在不同预算和领域中的表现及成功率，尤其在多代理情况下表现优越。", "motivation": "现有的方法依赖于静态的语义检索来发现工具或代理，这导致了现有组件的有效重用和组合的挑战，因为决策不是基于能力、成本和即时效用。因此，本文旨在解决这些挑战。", "method": "本文提出了一种结构化的自动化框架，用于代理系统的组合，灵感源自背包问题。通过系统性识别、选择和组装最优代理组件集，该框架考虑了性能、预算限制和兼容性。通过动态测试候选组件和实时建模其效用，本文的方法简化了代理系统的设计，并促进了资源的可扩展重用。", "result": "经验评估结果表明，在单代理设置下，相比检索基线，线性背包组合器可以提高高达31.6%的成功率。在多代理系统中，当从包含100+个代理的代理库存中选择代理时，线性背包组合器将成功率从37%提高到了87%。", "conclusion": "实验结果验证了该方法在不同域和预算约束下的强大适应性，证明了在线背包组合器的有效性和优越性。"}}
{"id": "2510.16258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16258", "abs": "https://arxiv.org/abs/2510.16258", "authors": ["Claire McLean", "Makenzie Meendering", "Tristan Swartz", "Orri Gabbay", "Alexandra Olsen", "Rachel Jacobs", "Nicholas Rosen", "Philippe de Bree", "Tony Garcia", "Gadsden Merrill", "Jake Sandakly", "Julia Buffalini", "Neham Jain", "Steven Krenn", "Moneish Kumar", "Dejan Markovic", "Evonne Ng", "Fabian Prada", "Andrew Saba", "Siwei Zhang", "Vasu Agrawal", "Tim Godisart", "Alexander Richard", "Michael Zollhoefer"], "title": "Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset", "comment": null, "summary": "The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of\n500 individual hours of 3D motion data from 439 participants collected in a\nmulti-camera collection stage, amounting to over 54 million frames of tracked\n3D motion. The dataset features a wide range of single-person motion data,\nincluding prompted motions, hand gestures, and locomotion; as well as\nmulti-person behavioral and conversational data like discussions, conversations\nin different emotional states, collaborative activities, and co-living\nscenarios in an apartment-like space. We provide tracked human motion including\nhand tracking and body shape, text annotations, and a separate audio track for\neach participant.", "AI": {"tldr": "Meta引入Embody 3D数据集，包含500小时的3D人体运动数据，旨在为多模态交互研究提供资源。", "motivation": "Meta的Codec Avatars实验室引入了Embody 3D，旨在提供丰富的3D人体运动数据，包括手部追踪、人体形状、文本注释以及每个参与者的单独音频轨道，以推动相关领域的发展。", "method": "Codec Avatars Lab通过多摄像头采集阶段收集了来自439名参与者的数据，生成了名为Embody 3D的多模式数据集，该数据集包含了500小时的3D运动数据，总计超过5400万帧追踪的3D运动数据。", "result": "数据集包含了个人运动数据，如动作指令、手势和移动动作，以及多人行为和会话数据，如讨论、不同情绪状态下的会话、协作活动和公寓环境下共同生活场景。", "conclusion": "Embody 3D数据集的推出，为研究者提供了一个重要的资源，能够支持更深入的人体运动分析和多模态交互的研究和应用开发。"}}
{"id": "2510.16549", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16549", "abs": "https://arxiv.org/abs/2510.16549", "authors": ["Haoxuan Zhang", "Ruochi Li", "Sarthak Shrestha", "Shree Harshini Mamidala", "Revanth Putta", "Arka Krishan Aggarwal", "Ting Xiao", "Junhua Ding", "Haihua Chen"], "title": "ReviewGuard: Enhancing Deficient Peer Review Detection via LLM-Driven Data Augmentation", "comment": null, "summary": "Peer review serves as the gatekeeper of science, yet the surge in submissions\nand widespread adoption of large language models (LLMs) in scholarly evaluation\npresent unprecedented challenges. Recent work has focused on using LLMs to\nimprove review efficiency or generate insightful review content. However,\nunchecked deficient reviews from both human experts and AI systems threaten to\nsystematically undermine the peer review ecosystem and compromise academic\nintegrity. To address this critical issue, we introduce ReviewGuard, an\nautomated system for detecting and categorizing deficient reviews. ReviewGuard\nemploys a comprehensive four-stage LLM-driven framework that: (1) collects ICLR\nand NeurIPS papers with their corresponding reviews from OpenReview; (2)\nannotates review types using GPT-4.1 with human validation; (3) addresses class\nimbalance and data scarcity through LLM-driven synthetic data augmentation,\nproducing a final corpus of 6,634 papers, 24,657 real reviews, and 46,438\nsynthetic reviews; and (4) fine-tunes both encoder-based models and open source\nLLMs. We perform comprehensive feature analysis of the structure and quality of\nthe review text. Compared to sufficient reviews, deficient reviews demonstrate\nlower rating scores, higher self-reported confidence, reduced structural\ncomplexity, and a higher proportion of negative sentiment. AI-generated text\ndetection reveals that, since ChatGPT's emergence, AI-generated reviews have\nincreased dramatically. In the evaluation of deficient review detection models,\nmixed training with synthetic and real review data provides substantial\nenhancements to recall and F1 scores on the binary task. This study presents\nthe first LLM-driven system for detecting deficient peer reviews, providing\nevidence to inform AI governance in peer review while offering valuable\ninsights into human-AI collaboration to maintain academic integrity.", "AI": {"tldr": "为了解决日益增加的论文提交和大型语言模型对审稿系统的影响，研究团队开发了ReviewGuard系统，利用LLM识别不足的审稿评论，证实了其在学术诚信领域的应用前景。", "motivation": "随着提交论文的激增和大型语言模型在学术评价中的广泛使用，审稿质量面临前所未有的挑战。不可控的来自人类专家和AI系统的不足审稿威胁到了审稿生态系统，有损学术诚信，因此提出了ReviewGuard系统来解决这一问题。", "method": "ReviewGuard 使用一个四阶段的LLM驱动框架来检测和分类不足的审稿意见。框架包括收集ICLR和NeurIPS论文及对应的审稿意见，使用GPT-4.1及其人类验证标注审稿类型，通过LLM驱动的数据增强来解决类别不平衡和数据不足问题，最后微调基于编码器的模型和开源的LLM。", "result": "研究表明，相比于充分的审稿意见，不足的审稿意见具有较低的评分、较高的自我评定信心、较低的结构复杂性和更高的负面情绪比例。AI生成的文本检测表明，自ChatGPT出现以来，AI生成的审稿意见明显增加。在不足审稿意见检测模型的评估中，混合使用合成和真实审稿数据训练显著增强了召回率和F1分数。", "conclusion": "这项研究介绍了首个通过LLM驱动的方法来检测不足同行评审意见的系统。这为AI在同行评审中的治理提供了证据，并提供了关于如何促进人与AI的合作以维持学术诚信的宝贵见解。"}}
{"id": "2510.16272", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16272", "abs": "https://arxiv.org/abs/2510.16272", "authors": ["Baicheng Li", "Zike Yan", "Dong Wu", "Hongbin Zha"], "title": "Proactive Scene Decomposition and Reconstruction", "comment": null, "summary": "Human behaviors are the major causes of scene dynamics and inherently contain\nrich cues regarding the dynamics. This paper formalizes a new task of proactive\nscene decomposition and reconstruction, an online approach that leverages\nhuman-object interactions to iteratively disassemble and reconstruct the\nenvironment. By observing these intentional interactions, we can dynamically\nrefine the decomposition and reconstruction process, addressing inherent\nambiguities in static object-level reconstruction. The proposed system\neffectively integrates multiple tasks in dynamic environments such as accurate\ncamera and object pose estimation, instance decomposition, and online map\nupdating, capitalizing on cues from human-object interactions in egocentric\nlive streams for a flexible, progressive alternative to conventional\nobject-level reconstruction methods. Aided by the Gaussian splatting technique,\naccurate and consistent dynamic scene modeling is achieved with photorealistic\nand efficient rendering. The efficacy is validated in multiple real-world\nscenarios with promising advantages.", "AI": {"tldr": "文章提出一种新的任务前瞻场景分解与重构，利用人与物体的互动，迭代地分解和重构环境。这种方法在动态环境中表现出有效性和优势。", "motivation": "文章指出，人类行为是场景动态变化的主要原因，本身就包含了丰富的动态线索。针对传统方法在动态环境中进行物体级别重建存在的固有模糊问题，提出了一个新的任务前瞻场景分解与重构。", "method": "该文提出了一种在线方法，利用人与物体的互动，迭代地分解和重构环境。通过观察这些有意图的互动，可以动态地调整分解与重构流程。系统整合了多个任务，如精确的摄像机和物体姿态估计，实例分解，以及在线地图更新，并使用高斯散射技术实现了准确且一致的动态场景建模。", "result": "该方法在多个现实世界场景中进行了验证，并展示出了显著的优势。", "conclusion": "通过利用人类与物体的互动，文章提出的方法提供了一种比传统物体级别重建更灵活且逐步推进的替代选择。"}}
{"id": "2510.16565", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16565", "abs": "https://arxiv.org/abs/2510.16565", "authors": ["Seungho Cho", "Changgeon Ko", "Eui Jun Hwang", "Junmyeong Lee", "Huije Lee", "Jong C. Park"], "title": "Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models", "comment": "Accepted to CIKM 2025 Workshop on Human Centric AI", "summary": "Large language models (LLMs) are increasingly used across diverse cultural\ncontexts, making accurate cultural understanding essential. Prior evaluations\nhave mostly focused on output-level performance, obscuring the factors that\ndrive differences in responses, while studies using circuit analysis have\ncovered few languages and rarely focused on culture. In this work, we trace\nLLMs' internal cultural understanding mechanisms by measuring activation path\noverlaps when answering semantically equivalent questions under two conditions:\nvarying the target country while fixing the question language, and varying the\nquestion language while fixing the country. We also use same-language country\npairs to disentangle language from cultural aspects. Results show that internal\npaths overlap more for same-language, cross-country questions than for\ncross-language, same-country questions, indicating strong language-specific\npatterns. Notably, the South Korea-North Korea pair exhibits low overlap and\nhigh variability, showing that linguistic similarity does not guarantee aligned\ninternal representation.", "AI": {"tldr": "通过测量LLMs在不同文化情境下回答问题时的激活路径重叠，研究发现语言具体的模式比文化的情境更为显著，但仍存在有文化独特性的情况，特别是在语言相似的情况下。", "motivation": "研究动机在于改进LLMs在跨文化情境下的准确文化理解，因为先前的评估主要集中于输出层面的表现，而忽略了影响不同回应的因素，同时使用电路分析的研究涵盖的语言很少，并且很少关注文化。", "method": "通过测量在两种条件下回答语义等价问题时的激活路径重叠来追踪LLMs的内部文化理解机制：改变目标国家同时固定问题语言，以及改变问题语言同时固定国家。还使用相同语言的国家对来区分语言和文化方面的影响。", "result": "结果显示，对于相同语言、不同国家的问题，内部路径的重叠比对于不同语言、同一国家的问题更多，表明有强烈的语言特定模式。值得注意的是，韩国和朝鲜对展现出低重叠和高变异性，表明语言相似性并不能保证内部表示的一致性。", "conclusion": "LLMs在理解文化时表现出强烈依赖语言的特异性，但语言相似性并不能保证内部文化表示的一致性，特别是在涉及文化差异明显的情景时。"}}
{"id": "2510.16290", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16290", "abs": "https://arxiv.org/abs/2510.16290", "authors": ["Yue Zheng", "Xiufang Shi", "Jiming Chen", "Yuanchao Shu"], "title": "Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models", "comment": null, "summary": "Video anomaly detection (VAD) has rapidly advanced by recent development of\nVision-Language Models (VLMs). While these models offer superior zero-shot\ndetection capabilities, their immense computational cost and unstable visual\ngrounding performance hinder real-time deployment. To overcome these\nchallenges, we introduce Cerberus, a two-stage cascaded system designed for\nefficient yet accurate real-time VAD. Cerberus learns normal behavioral rules\noffline, and combines lightweight filtering with fine-grained VLM reasoning\nduring online inference. The performance gains of Cerberus come from two key\ninnovations: motion mask prompting and rule-based deviation detection. The\nformer directs the VLM's attention to regions relevant to motion, while the\nlatter identifies anomalies as deviations from learned norms rather than\nenumerating possible anomalies. Extensive evaluations on four datasets show\nthat Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a\n151.79$\\times$ speedup, and 97.2\\% accuracy comparable to the state-of-the-art\nVLM-based VAD methods, establishing it as a practical solution for real-time\nvideo analytics.", "AI": {"tldr": "本文介绍了Cerberus，一个用于实时视频异常检测的两阶段级联系统，通过动作掩码提示和基于规则的偏差检测技术，有效提高了计算效率和检测准确性。", "motivation": "项目动机是解决视觉语言模型在视频异常检测中的计算成本高和视觉定位性能不稳定的问题，提高实时部署能力。", "method": "Cerberus采用两阶段级联系统设计，首先离线学习正常行为规则，然后在线推断时结合轻量级过滤和细粒度的VLM推理。系统通过动作掩码提示和基于规则的偏差检测两种创新方法提高性能，前者引导VLM关注与运动相关的区域，后者通过识别与学习规范的偏差来检测异常，而非枚举所有可能的异常。", "result": "实验结果表明，Cerberus在四个数据集上平均实现了57.68 fps的速度，相比其他方法有151.79倍的速度提升，并且实现了97.2%的准确性，这与最先进的VLM基VAD方法相当。", "conclusion": "Cerberus作为一项实用的解决方案，证明了其在实时视频分析中的有效性。"}}
{"id": "2510.16567", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16567", "abs": "https://arxiv.org/abs/2510.16567", "authors": ["Alkis Koudounas", "Moreno La Quatra", "Manuel Giollo", "Sabato Marco Siniscalchi", "Elena Baralis"], "title": "Hallucination Benchmark for Speech Foundation Models", "comment": "Under Review", "summary": "Hallucinations in automatic speech recognition (ASR) systems refer to fluent\nand coherent transcriptions produced by neural ASR models that are completely\nunrelated to the underlying acoustic input (i.e., the speech signal). While\nsimilar to conventional decoding errors in potentially compromising the\nusability of transcriptions for downstream applications, hallucinations can be\nmore detrimental due to their preservation of syntactically and semantically\nplausible structure. This apparent coherence can mislead subsequent processing\nstages and introduce serious risks, particularly in critical domains such as\nhealthcare and law. Conventional evaluation metrics are primarily centered on\nerror-based metrics and fail to distinguish between phonetic inaccuracies and\nhallucinations. Consequently, there is a critical need for new evaluation\nframeworks that can effectively identify and assess models with a heightened\npropensity for generating hallucinated content. To this end, we introduce\nSHALLOW, the first benchmark framework that systematically categorizes and\nquantifies hallucination phenomena in ASR along four complementary axes:\nlexical, phonetic, morphological, and semantic. We define targeted metrics\nwithin each category to produce interpretable profiles of model behavior.\nThrough evaluation across various architectures and speech domains, we have\nfound that SHALLOW metrics correlate strongly with word error rate (WER) when\nrecognition quality is high (i.e., low WER). Still, this correlation weakens\nsubstantially as WER increases. SHALLOW, therefore, captures fine-grained error\npatterns that WER fails to distinguish under degraded and challenging\nconditions. Our framework supports specific diagnosis of model weaknesses and\nprovides feedback for model improvement beyond what aggregate error rates can\noffer.", "AI": {"tldr": "研究提出了SHALLOW框架，该框架系统地分类和量化ASR中的幻觉现象，提供了一套更详细的模型错误分析方法，超出了传统错误率的局限。", "motivation": "当前的自动语音识别（ASR）系统在解码时存在幻觉现象，即完全不相关的流畅且连贯的转录，这可能比传统的解码错误更具危害性。因此，需要新的评估框架来有效识别和评估产生幻觉内容率较高的模型。", "method": "提出了SHALLOW框架，该框架系统地分类和量化ASR中的幻觉现象，涵盖词汇、音素、形态和语义四个互补轴，每个类别内定义了针对性指标以生成可解释的模型行为档案。", "result": "通过SHALLOW评估不同架构和语音领域的模型发现，在高识别质量下，SHALLOW指标与WER有较强的关联，但在WER增加时关联性减弱，显示出捕捉精细错误模式的能力。", "conclusion": "SHALLOW框架在高识别质量下与WER有较强的关联，但在WER增加时关联性减弱，显示出在降级和具有挑战性条件下捕捉WER不能区分的精细错误模式的能力。该框架支持对模型弱点的具体诊断，并提供超出聚合误差率的模型改进反馈。"}}
{"id": "2510.16295", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16295", "abs": "https://arxiv.org/abs/2510.16295", "authors": ["Ryoto Miyamoto", "Xin Fan", "Fuyuko Kido", "Tsuneo Matsumoto", "Hayato Yamana"], "title": "OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models", "comment": null, "summary": "OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in\nevaluating membership inference attacks (MIA) against large vision-language\nmodels (LVLMs). While prior work has reported high attack success rates, our\nanalysis suggests that these results often arise from detecting distributional\nbias introduced during dataset construction rather than from identifying true\nmembership status. To address this issue, we introduce a controlled benchmark\nof 6{,}000 images where the distributions of member and non-member samples are\ncarefully balanced, and ground-truth membership labels are provided across\nthree distinct training stages. Experiments using OpenLVLM-MIA demonstrated\nthat the performance of state-of-the-art MIA methods converged to random chance\nunder unbiased conditions. By offering a transparent and unbiased benchmark,\nOpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and\nprovides a solid foundation for developing stronger privacy-preserving\ntechniques.", "AI": {"tldr": "OpenLVLM-MIA是一个新的、无偏差的基准测试，它揭示了在评估大规模视觉语言模型成员推断攻击时存在分布偏差的问题，并为开发更强的隐私保护技术提供了基础。", "motivation": "该研究的动机在于解决现有方法评估大规模视觉语言模型（LVLM）成员推断攻击时存在的分布偏差问题。", "method": "OpenLVLM-MIA是一个新的基准测试，它包含6,000张图像，其中成员和非成员样本的分布被仔细平衡，并提供了三个不同训练阶段的真实成员标签。", "result": "使用OpenLVLM-MIA的实验结果表明，在无偏差条件下，最先进的成员推断攻击方法的表现收敛于随机机会水平。", "conclusion": "OpenLVLM-MIA澄清了当前MIA研究在LVLM上的局限性，并为开发更强的隐私保护技术提供了坚实的基础。"}}
{"id": "2510.16573", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16573", "abs": "https://arxiv.org/abs/2510.16573", "authors": ["Muhammad Ammar", "Hadiya Murad Hadi", "Usman Majeed Butt"], "title": "AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu", "comment": null, "summary": "Large Language Models (LLMs) are now capable of generating text that closely\nresembles human writing, making them powerful tools for content creation, but\nthis growing ability has also made it harder to tell whether a piece of text\nwas written by a human or by a machine. This challenge becomes even more\nserious for languages like Urdu, where there are very few tools available to\ndetect AI-generated text. To address this gap, we propose a novel AI-generated\ntext detection framework tailored for the Urdu language. A balanced dataset\ncomprising 1,800 humans authored, and 1,800 AI generated texts, sourced from\nmodels such as Gemini, GPT-4o-mini, and Kimi AI was developed. Detailed\nlinguistic and statistical analysis was conducted, focusing on features such as\ncharacter and word counts, vocabulary richness (Type Token Ratio), and N-gram\npatterns, with significance evaluated through t-tests and MannWhitney U tests.\nThree state-of-the-art multilingual transformer models such as\nmdeberta-v3-base, distilbert-base-multilingualcased, and xlm-roberta-base were\nfine-tuned on this dataset. The mDeBERTa-v3-base achieved the highest\nperformance, with an F1-score 91.29 and accuracy of 91.26% on the test set.\nThis research advances efforts in contesting misinformation and academic\nmisconduct in Urdu-speaking communities and contributes to the broader\ndevelopment of NLP tools for low resource languages.", "AI": {"tldr": "本研究提出了一种针对乌尔都语的AI生成文本检测框架，利用了平衡的数据集和多语言转换模型进行微调，mDeBERTa-v3-base模型取得了最佳性能。", "motivation": "乌尔都语缺乏检测AI生成文本的工具，这是一个亟待解决的问题。随着大型语言模型生成文本的能力不断增强，很难辨别文本是由人类还是机器生成，尤其是对于像乌尔都语这样的语言。", "method": "本研究提出了一种针对乌尔都语的AI生成文本检测框架。该框架使用了一个平衡的数据集，包括1800篇人类写作和1800篇由AI生成的文本，这些文本来自像Gemini、GPT-4o-mini和Kimi AI这样的模型。研究进行了详细的语用学和统计分析，重点关注了特征如字符和词语数量、词汇丰富度（类型-词汇率）以及N-gram模式。显著性通过t检验和曼-惠特尼U检验评估。并利用了三个最先进的多语言转换模型：mdeberta-v3-base、distilbert-base-multilingualcased和xlm-roberta-base，对数据集进行了微调。", "result": "通过微调，mDeBERTa-v3-base模型在测试集上达到了最高的F1得分91.29和准确率91.26%。", "conclusion": "本研究通过提出专门针对乌尔都语的AI生成文本检测方法，不仅有助于打击乌尔都语社区中的不实信息和学术不端行为，同时也为低资源语言的NLP工具开发做出了贡献。"}}
{"id": "2510.16319", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16319", "abs": "https://arxiv.org/abs/2510.16319", "authors": ["Rui Yang", "Huining Li", "Yiyi Long", "Xiaojun Wu", "Shengfeng He"], "title": "Stroke2Sketch: Harnessing Stroke Attributes for Training-Free Sketch Generation", "comment": "ICCV 2025", "summary": "Generating sketches guided by reference styles requires precise transfer of\nstroke attributes, such as line thickness, deformation, and texture sparsity,\nwhile preserving semantic structure and content fidelity. To this end, we\npropose Stroke2Sketch, a novel training-free framework that introduces\ncross-image stroke attention, a mechanism embedded within self-attention layers\nto establish fine-grained semantic correspondences and enable accurate stroke\nattribute transfer. This allows our method to adaptively integrate reference\nstroke characteristics into content images while maintaining structural\nintegrity. Additionally, we develop adaptive contrast enhancement and\nsemantic-focused attention to reinforce content preservation and foreground\nemphasis. Stroke2Sketch effectively synthesizes stylistically faithful sketches\nthat closely resemble handcrafted results, outperforming existing methods in\nexpressive stroke control and semantic coherence. Codes are available at\nhttps://github.com/rane7/Stroke2Sketch.", "AI": {"tldr": "我们提出了一个无训练框架Stroke2Sketch，它使用跨图像笔触注意力机制来精确转移笔触属性，同时保留语义结构和内容的忠实度。", "motivation": "为了实现由参考样风格指导的素描生成，需要精确地转移笔触属性，如线粗、变形和纹理稀疏度，同时保留语义结构和内容的忠实度。", "method": "我们提出了Stroke2Sketch，一种无训练框架，它引入了跨图像的笔触注意力机制，该机制嵌入在自注意力层中，用于建立细粒度的语义对应性，从而实现精确的笔触属性转移。此外，我们还开发了自适应对比度增强和语义聚焦注意力，以加强内容的保存和前景的强调。", "result": "实验结果表明，Stroke2Sketch在笔触控制和语义一致性方面优于现有方法，能够根据参考风格生成忠实的素描，且效果几乎与手绘结果无异。", "conclusion": "Stroke2Sketch能够有效地合成忠实于样风格的素描，非常接近于手绘结果，优于现有的方法，在表现笔触控制和语义一致性方面表现出色。"}}
{"id": "2510.16604", "categories": ["cs.CL", "68T50", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.16604", "abs": "https://arxiv.org/abs/2510.16604", "authors": ["Francisco Jose Cortes Delgado", "Eduardo Martinez Gracia", "Rafael Valencia Garcia"], "title": "Fine-tuning of Large Language Models for Constituency Parsing Using a Sequence to Sequence Approach", "comment": "6 pages, 3 figures. Submitted to SEPLN 2023 Conference", "summary": "Recent advances in natural language processing with large neural models have\nopened new possibilities for syntactic analysis based on machine learning. This\nwork explores a novel approach to phrase-structure analysis by fine-tuning\nlarge language models (LLMs) to translate an input sentence into its\ncorresponding syntactic structure. The main objective is to extend the\ncapabilities of MiSintaxis, a tool designed for teaching Spanish syntax.\nSeveral models from the Hugging Face repository were fine-tuned using training\ndata generated from the AnCora-ES corpus, and their performance was evaluated\nusing the F1 score. The results demonstrate high accuracy in phrase-structure\nanalysis and highlight the potential of this methodology.", "AI": {"tldr": "本文通过微调大型语言模型，提出了进行短语结构分析的新方法，并证实了这种方法在提高MiSintaxis工具能力方面的有效性。", "motivation": "旨在扩展MiSintaxis工具的功能，该工具专用于教授西班牙语句法。", "method": "通过微调大型语言模型（LLMs），将输入句子转换为其对应的句法结构，从而探索短语结构分析的新方法。使用的模型来自于Hugging Face仓库，并使用AnCora-ES语料库生成的训练数据进行了微调。", "result": "结果表明，在短语结构分析中表现出了高准确率，并突显了此方法论的潜力。", "conclusion": "实验证明了使用大型神经模型进行句法分析的高效性，并表明该方法有潜力作为教学辅助工具。"}}
{"id": "2510.16320", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16320", "abs": "https://arxiv.org/abs/2510.16320", "authors": ["Wenhao Wang", "Longqi Cai", "Taihong Xiao", "Yuxiao Wang", "Ming-Hsuan Yang"], "title": "Scaling Laws for Deepfake Detection", "comment": null, "summary": "This paper presents a systematic study of scaling laws for the deepfake\ndetection task. Specifically, we analyze the model performance against the\nnumber of real image domains, deepfake generation methods, and training images.\nSince no existing dataset meets the scale requirements for this research, we\nconstruct ScaleDF, the largest dataset to date in this field, which contains\nover 5.8 million real images from 51 different datasets (domains) and more than\n8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we\nobserve power-law scaling similar to that shown in large language models\n(LLMs). Specifically, the average detection error follows a predictable\npower-law decay as either the number of real domains or the number of deepfake\nmethods increases. This key observation not only allows us to forecast the\nnumber of additional real domains or deepfake methods required to reach a\ntarget performance, but also inspires us to counter the evolving deepfake\ntechnology in a data-centric manner. Beyond this, we examine the role of\npre-training and data augmentations in deepfake detection under scaling, as\nwell as the limitations of scaling itself.", "AI": {"tldr": "论文通过构建大规模数据集ScaleDF，分析了深度伪造检测模型的缩放规律，发现在增加真实图像领域或伪造方法数量时，检测错误率遵循幂律衰减规律。这一发现可用于预测达到目标性能所需的数据量，并指导对抗深度伪造技术的策略。此外，论文还探讨了预训练和数据增强在扩展规模下的作用及限制。", "motivation": "由于现有的数据集无法满足研究需求，作者构建了大规模数据集ScaleDF，旨在研究深度伪造检测中模型性能与真实图像领域、伪造方法、训练图像数量之间的关系。", "method": "使用包含超过580万真实图像和880万伪造图像的ScaleDF数据集进行实验，通过增加真实图像领域和伪造方法的数量来观察模型性能的变化。", "result": "发现深度伪造检测中的错误率遵循幂律衰减规律，即错误率随真实图像领域或伪造方法数量增加而按照幂律减少，这一规律类似于大语言模型中的现象。", "conclusion": "论文阐述了通过增加数据量可以预测模型性能提高的目标，为对抗深度伪造技术提供了一种数据为导向的方法，并提出了预训练和数据增强在扩展规模下的局限性。"}}
{"id": "2510.16645", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.16645", "abs": "https://arxiv.org/abs/2510.16645", "authors": ["Zhixuan He", "Yue Feng"], "title": "Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration", "comment": null, "summary": "Large Language Models (LLMs) demonstrate strong performance but often lack\ninterpretable reasoning. This paper introduces the Multi-Agent Collaboration\nFramework for Diverse Thinking Modes (DiMo), which enhances both performance\nand interpretability by simulating a structured debate among four specialized\nLLM agents. Each agent embodies a distinct reasoning paradigm, allowing the\nframework to collaboratively explore diverse cognitive approaches. Through\niterative debate, agents challenge and refine initial responses, yielding more\nrobust conclusions and an explicit, auditable reasoning chain. Across six\nbenchmarks and under a unified open-source setup, DiMo improves accuracy over\nwidely used single-model and debate baselines, with the largest gains on math.\nWe position DiMo as a semantics-aware, Web-native multi-agent framework: it\nmodels human-machine intelligence with LLM agents that produce semantically\ntyped, URL-annotated evidence chains for explanations and user-friendly\ninteractions. Although our experiments use standard reasoning benchmarks, the\nframework is designed to be instantiated over Web corpora and knowledge graphs,\ncombining retrieval-augmented reasoning with structured justifications that\ndownstream systems can inspect and reuse.", "AI": {"tldr": "{\n  \"tldr\": \"本文介绍的Multi-Agent Collaboration Framework for Diverse Thinking Modes (DiMo)框架，通过让四个专门化的语言模型代理之间的结构化辩论，提高了语言模型的表现能力和解释性。\", \n  \"motivation\": \"语言模型虽然在性能上表现出色，但它们往往缺乏可解释的推理能力。本文旨在通过引入新的框架来改善这一状况。\", \n  \"method\": \"DiMo框架通过让四个代表不同推理模式的代理进行辩论，每个代理表示一个特定的推理范式。通过迭代辩论过程，代理们能够挑战和提高原先的回答，提供更强健的结论和明确可审计的推理链。\", \n  \"result\": \"该框架在六个标准上展示了比单一模型和辩论基线更高的准确性，尤其是在数学领域有显著的提高。\", \n  \"conclusion\": \"接下来将DiMo框架定位为一个Web原生多代理框架，利用语义感知语言模型代理生产语义类型的、具有URL注释的证据链，便于解释和用户友好交互，同时结合检索增强的推理和可审查的结构化解释。\", \n  \"tool_calls\": []\n}", "motivation": "{\n  \"tldr\": \"本文介绍的Multi-Agent Collaboration Framework for Diverse Thinking Modes (DiMo)框架，通过让四个专门化的语言模型代理之间的结构化辩论，提高了语言模型的表现能力和解释性。\", \n  \"motivation\": \"语言模型虽然在性能上表现出色，但它们往往缺乏可解释的推理能力。本文旨在通过引入新的框架来改善这一状况。\", \n  \"method\": \"DiMo框架通过让四个代表不同推理模式的代理进行辩论，每个代理表示一个特定的推理范式。通过迭代辩论过程，代理们能够挑战和提高原先的回答，提供更强健的结论和明确可审计的推理链。\", \n  \"result\": \"该框架在六个标准上展示了比单一模型和辩论基线更高的准确性，尤其是在数学领域有显著的提高。\", \n  \"conclusion\": \"接下来将DiMo框架定位为一个Web原生多代理框架，利用语义感知语言模型代理生产语义类型的、具有URL注释的证据链，便于解释和用户友好交互，同时结合检索增强的推理和可审查的结构化解释。\", \n  \"tool_calls\": []\n}", "method": "{\n  \"tldr\": \"本文介绍的Multi-Agent Collaboration Framework for Diverse Thinking Modes (DiMo)框架，通过让四个专门化的语言模型代理之间的结构化辩论，提高了语言模型的表现能力和解释性。\", \n  \"motivation\": \"语言模型虽然在性能上表现出色，但它们往往缺乏可解释的推理能力。本文旨在通过引入新的框架来改善这一状况。\", \n  \"method\": \"DiMo框架通过让四个代表不同推理模式的代理进行辩论，每个代理表示一个特定的推理范式。通过迭代辩论过程，代理们能够挑战和提高原先的回答，提供更强健的结论和明确可审计的推理链。\", \n  \"result\": \"该框架在六个标准上展示了比单一模型和辩论基线更高的准确性，尤其是在数学领域有显著的提高。\", \n  \"conclusion\": \"接下来将DiMo框架定位为一个Web原生多代理框架，利用语义感知语言模型代理生产语义类型的、具有URL注释的证据链，便于解释和用户友好交互，同时结合检索增强的推理和可审查的结构化解释。\", \n  \"tool_calls\": []\n}", "result": "{\n  \"tldr\": \"本文介绍的Multi-Agent Collaboration Framework for Diverse Thinking Modes (DiMo)框架，通过让四个专门化的语言模型代理之间的结构化辩论，提高了语言模型的表现能力和解释性。\", \n  \"motivation\": \"语言模型虽然在性能上表现出色，但它们往往缺乏可解释的推理能力。本文旨在通过引入新的框架来改善这一状况。\", \n  \"method\": \"DiMo框架通过让四个代表不同推理模式的代理进行辩论，每个代理表示一个特定的推理范式。通过迭代辩论过程，代理们能够挑战和提高原先的回答，提供更强健的结论和明确可审计的推理链。\", \n  \"result\": \"该框架在六个标准上展示了比单一模型和辩论基线更高的准确性，尤其是在数学领域有显著的提高。\", \n  \"conclusion\": \"接下来将DiMo框架定位为一个Web原生多代理框架，利用语义感知语言模型代理生产语义类型的、具有URL注释的证据链，便于解释和用户友好交互，同时结合检索增强的推理和可审查的结构化解释。\", \n  \"tool_calls\": []\n}", "conclusion": "{\n  \"tldr\": \"本文介绍的Multi-Agent Collaboration Framework for Diverse Thinking Modes (DiMo)框架，通过让四个专门化的语言模型代理之间的结构化辩论，提高了语言模型的表现能力和解释性。\", \n  \"motivation\": \"语言模型虽然在性能上表现出色，但它们往往缺乏可解释的推理能力。本文旨在通过引入新的框架来改善这一状况。\", \n  \"method\": \"DiMo框架通过让四个代表不同推理模式的代理进行辩论，每个代理表示一个特定的推理范式。通过迭代辩论过程，代理们能够挑战和提高原先的回答，提供更强健的结论和明确可审计的推理链。\", \n  \"result\": \"该框架在六个标准上展示了比单一模型和辩论基线更高的准确性，尤其是在数学领域有显著的提高。\", \n  \"conclusion\": \"接下来将DiMo框架定位为一个Web原生多代理框架，利用语义感知语言模型代理生产语义类型的、具有URL注释的证据链，便于解释和用户友好交互，同时结合检索增强的推理和可审查的结构化解释。\", \n  \"tool_calls\": []\n}"}}
{"id": "2510.16325", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16325", "abs": "https://arxiv.org/abs/2510.16325", "authors": ["Yuyao Zhang", "Yu-Wing Tai"], "title": "Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention", "comment": "22 pages", "summary": "Ultra-high-resolution text-to-image generation demands both fine-grained\ntexture synthesis and globally coherent structure, yet current diffusion models\nremain constrained to sub-$1K \\times 1K$ resolutions due to the prohibitive\nquadratic complexity of attention and the scarcity of native $4K$ training\ndata. We present \\textbf{Scale-DiT}, a new diffusion framework that introduces\nhierarchical local attention with low-resolution global guidance, enabling\nefficient, scalable, and semantically coherent image synthesis at ultra-high\nresolutions. Specifically, high-resolution latents are divided into fixed-size\nlocal windows to reduce attention complexity from quadratic to near-linear,\nwhile a low-resolution latent equipped with scaled positional anchors injects\nglobal semantics. A lightweight LoRA adaptation bridges global and local\npathways during denoising, ensuring consistency across structure and detail. To\nmaximize inference efficiency, we repermute token sequence in Hilbert curve\norder and implement a fused-kernel for skipping masked operations, resulting in\na GPU-friendly design. Extensive experiments demonstrate that Scale-DiT\nachieves more than $2\\times$ faster inference and lower memory usage compared\nto dense attention baselines, while reliably scaling to $4K \\times 4K$\nresolution without requiring additional high-resolution training data. On both\nquantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons,\nScale-DiT delivers superior global coherence and sharper local detail, matching\nor outperforming state-of-the-art methods that rely on native 4K training.\nTaken together, these results highlight hierarchical local attention with\nguided low-resolution anchors as a promising and effective approach for\nadvancing ultra-high-resolution image generation.", "AI": {"tldr": "Scale-DiT是一种新的扩散框架，采用分层局部注意力机制和低分辨率全局引导，实现了高效、可扩展且语义连贯的超高分辨率图像合成。", "motivation": "根据摘要，作者提出Scale-DiT的动机是因为当前扩散模型在超高分辨率（超过1K x 1K）图像生成上受到注意力机制二次复杂度和缺乏原生4K训练数据的限制。他们希望开发出一种新的图像生成方法，能够在不增加额外高分辨率训练数据的情况下扩展到4K分辨率，且保持相对较高的效率。", "method": "通过引入分层局部注意力机制和低分辨率全局引导，Scale-DiT框架解决了现有扩散模型在超高分辨率文本到图像生成上的局限性。该方法将高分辨率潜在特征划分为固定大小的局部窗口，以将注意力复杂度从二次降低到近线性，并用带有缩放位置锚点的低分辨率潜在特征注入全局语义。此外，一种轻量级的LoRA自适应方法在去噪过程中连接全局和局部路径，确保结构和细节之间的连贯性。为了最大化推理效率，该模型重新排列了以希尔伯特曲线顺序排列的标记序列，并实现了融合内核以跳过屏蔽操作，从而获得更适合GPU的设计。", "result": "实验表明，Scale-DiT 在推理速度上比密集注意力基线方法快超过2倍，内存使用更少，并且在不依赖额外的高分辨率数据的情况下可扩展至4K x 4K分辨率。在定量（FID，IS，CLIP Score）和定性比较中，Scale-DiT 表现出了更好的全局连贯性和锐利的局部细节，与依赖原生4K训练数据的最先进技术相比，可匹配或超越其性能。", "conclusion": "这种方法展示了分层局部注意与指导性低分辨率锚点作为推进超高分辨率图像生成的前景和有效途径。通过这种方法，图像生成技术可以超越现有限制，在没有资源密集型高分辨率训练数据集的情况下实现高分辨率和高质量的图像合成。"}}
{"id": "2510.16670", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16670", "abs": "https://arxiv.org/abs/2510.16670", "authors": ["Yiyang Liu", "James C. Liang", "Heng Fan", "Wenhao Yang", "Yiming Cui", "Xiaotian Han", "Lifu Huang", "Dongfang Liu", "Qifan Wang", "Cheng Han"], "title": "All You Need is One: Capsule Prompt Tuning with a Single Vector", "comment": "NeurIPS 2025", "summary": "Prompt-based learning has emerged as a parameter-efficient finetuning (PEFT)\napproach to facilitate Large Language Model (LLM) adaptation to downstream\ntasks by conditioning generation with task-aware guidance. Despite its\nsuccesses, current prompt-based learning methods heavily rely on laborious grid\nsearching for optimal prompt length and typically require considerable number\nof prompts, introducing additional computational burden. Worse yet, our pioneer\nfindings indicate that the task-aware prompt design is inherently limited by\nits absence of instance-aware information, leading to a subtle attention\ninterplay with the input sequence. In contrast, simply incorporating\ninstance-aware information as a part of the guidance can enhance the\nprompt-tuned model performance without additional fine-tuning. Moreover, we\nfind an interesting phenomenon, namely \"attention anchor\", that incorporating\ninstance-aware tokens at the earliest position of the sequence can successfully\npreserve strong attention to critical structural information and exhibit more\nactive attention interaction with all input tokens. In light of our\nobservation, we introduce Capsule Prompt-Tuning (CaPT), an efficient and\neffective solution that leverages off-the-shelf, informative instance semantics\ninto prompt-based learning. Our approach innovatively integrates both\ninstance-aware and task-aware information in a nearly parameter-free manner\n(i.e., one single capsule prompt). Empirical results demonstrate that our\nmethod can exhibit superior performance across various language tasks (e.g.,\n84.03\\% average accuracy on T5-Large), serving as an \"attention anchor,\" while\nenjoying high parameter efficiency (e.g., 0.003\\% of model parameters on\nLlama3.2-1B).", "AI": {"tldr": "提出了Capsule Prompt-Tuning (CaPT)，该方法将任务相关的提示与实例信息结合，不需要额外调优，实验结果显示在多个语言任务中表现出色，参数效率高。", "motivation": "为了解决现有基于提示的学习方法中需要大量手动搜索最优提示长度和需要较多提示的问题，并弥补这些方法中缺失实例信息的不足。", "method": "介绍了Capsule Prompt-Tuning (CaPT)，该方法创新性地将任务相关的提示与实例相关的信息结合，通过在序列早期位置加入实例相关的标记，解决了传统提示方法中缺失实例信息的问题，并且在几乎不增加模型参数的情况下提高了性能。", "result": "实验表明，该方法在多个语言任务中表现优异（如T5-Large平均准确率为84.03%），并且参数效率高（例如Llama3.2-1B模型参数的0.003%）。", "conclusion": "Capsule Prompt-Tuning (CaPT)作为有效且高效的解决方案，将实例信息引入到基于提示的学习中，展示了其在语言任务上的优越性能，并且保持了较高的参数效率。"}}
{"id": "2510.16326", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16326", "abs": "https://arxiv.org/abs/2510.16326", "authors": ["Yi Wei", "Shunpu Tang", "Liang Zhao", "Qiangian Yang"], "title": "DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution", "comment": null, "summary": "Recent advances in diffusion models have driven remarkable progress in image\ngeneration. However, the generation process remains computationally intensive,\nand users often need to iteratively refine prompts to achieve the desired\nresults, further increasing latency and placing a heavy burden on cloud\nresources. To address this challenge, we propose DiffusionX, a cloud-edge\ncollaborative framework for efficient multi-round, prompt-based generation. In\nthis system, a lightweight on-device diffusion model interacts with users by\nrapidly producing preview images, while a high-capacity cloud model performs\nfinal refinements after the prompt is finalized. We further introduce a noise\nlevel predictor that dynamically balances the computation load, optimizing the\ntrade-off between latency and cloud workload. Experiments show that DiffusionX\nreduces average generation time by 15.8% compared with Stable Diffusion v1.5,\nwhile maintaining comparable image quality. Moreover, it is only 0.9% slower\nthan Tiny-SD with significantly improved image quality, thereby demonstrating\nefficiency and scalability with minimal overhead.", "AI": {"tldr": "本文提出了一种云边协作框架DiffusionX，用于高效地进行多轮提示图像生成，通过优化计算负载分布，减少了生成时间并提高了图像质量。", "motivation": "提出DiffusionX的动机在于解决扩散模型生成过程中的计算负担重，以及用户需要多次迭代调整提示以达到所需结果的问题，这些问题增加了延迟并加重了云资源的负担。", "method": "提出了一种名为DiffusionX的云边协作框架，旨在高效地进行多轮提示生成。该系统中，轻量级的设备扩散模型通过快速生成预览图像与用户互动，而在提示最终确定后，高容量的云模型执行最终的细化工作。此外，还引入了一个噪声级别预测器，动态平衡计算负载，优化了延迟和云工作负载之间的权衡。", "result": "实验结果表明，与Stable Diffusion v1.5相比，DiffusionX将平均生成时间减少了15.8%，同时保持了可比较的图像质量。此外，它仅比Tiny-SD慢0.9%，但图像质量有了显著提升。", "conclusion": "结论是DiffusionX展示了一种在减少计算资源消耗的情况下，能够高效生成高质量图像的方法，证明了其有效性和可扩展性。"}}
{"id": "2510.16685", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16685", "abs": "https://arxiv.org/abs/2510.16685", "authors": ["Damin Zhang", "Julia Rayz"], "title": "Temporal Understanding under Deictic Frame of Reference", "comment": "Under review", "summary": "Understanding time is fundamental to human cognition, where temporal\nexperience is often conceptualized through spatial metaphors grounded in\nsensory-motor experience. For example, \"summer is approaching\" parallels \"We\nare approaching the summer\". In such expressions, humans rely on a frame of\nreference (FoR) to interpret meaning relative to a particular viewpoint.\nExtending this concept to time, a temporal frame of reference (t-FoR) defines\nhow temporal relations are perceived relative to an experiencer's moment of\n\"now\". While Large Language Models (LLMs) have shown remarkable advances in\nnatural language understanding, their ability to interpret and reason about\ntime remains limited. In this work, we introduce TUuD (Temporal Understanding\nunder Deictic t-FoR), a framework that evaluates how LLMs interpret time-event\nand event-event relations when the reference point of \"now\" dynamically shifts\nalong a timeline. Following recent work on temporal cognition\n\\cite{li2025other}, LLMs are prompted to rate the similarity between the\ncurrent moment and a target event from 0.00 (completely dissimilar) to 1.00\n(highly similar), where similarity quantifies perceived temporal alignment\nbetween the two points. Our results show that four evaluated LLMs exhibit\nmeasurable adaptation to a deictic t-FoR, with similarity ratings peaking\naround the present and decreasing toward past and future events. The\nadaptation, however, weakens beyond near-term contexts, suggesting that while\nLLMs display partial human-like temporal cognition, their temporal reasoning\nremains sensitive to reference-frame shifts and temporal distance.", "AI": {"tldr": "提出了TUuD（基于指示性时间参照框架的时间理解）来评估语言模型在动态参考时间点下的时间推理能力。结果显示，这些语言模型在短期内容上能部分适应人类的时间认知，但对更远的时间点变化敏感。", "motivation": "尽管大型语言模型在自然语言理解方面取得了显著进展，但它们理解和推理时间的能力仍然有限。为了评估LLMs的时间推理能力，特别是他们如何适应和理解\"现在\"这一参考点在时间线上的动态变化。", "method": "TUuD (Temporal Understanding under Deictic t-FoR)框架评估了LLMs在\"现在\"这一参考点在时间线上动态变化时如何理解和推理时间-事件和事件-事件的关系。LLMs被要求从0.00（完全不相似）到1.00（非常相似）来评定当前时刻与目标事件之间的相似性，以此量化二者之间感知的时间对齐程度。", "result": "评估结果表明，四种被评估的语言模型在当前时刻相似性的评估值会随时间点的变化而呈现出峰值，并在向过去或未来事件延展时减少。这表明虽然语言模型显示了部分人类时间认知，但其对时间点变化和时间距离的适应性仍有限。", "conclusion": "语言模型虽然能部分适应指示性的时态框架，在短期内展现出类似于人类的时间认知，但其对时间点转移和较长距离的时间点敏感。这表明当前的语言模型在时间推理方面仍有改进的空间。"}}
{"id": "2510.16332", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16332", "abs": "https://arxiv.org/abs/2510.16332", "authors": ["Haiyue Sun", "Qingdong He", "Jinlong Peng", "Peng Tang", "Jiangning Zhang", "Junwei Zhu", "Xiaobin Hu", "Shuicheng Yan"], "title": "TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement", "comment": null, "summary": "Autoregressive Model (AR) has shown remarkable success in conditional image\ngeneration. However, these approaches for multiple reference generation\nstruggle with decoupling different reference identities. In this work, we\npropose the TokenAR framework, specifically focused on a simple but effective\ntoken-level enhancement mechanism to address reference identity confusion\nproblem. Such token-level enhancement consists of three parts, 1). Token Index\nEmbedding clusters the tokens index for better representing the same reference\nimages; 2). Instruct Token Injection plays as a role of extra visual feature\ncontainer to inject detailed and complementary priors for reference tokens; 3).\nThe identity-token disentanglement strategy (ITD) explicitly guides the token\nrepresentations toward independently representing the features of each\nidentity.This token-enhancement framework significantly augments the\ncapabilities of existing AR based methods in conditional image generation,\nenabling good identity consistency while preserving high quality background\nreconstruction. Driven by the goal of high-quality and high-diversity in\nmulti-subject generation, we introduce the InstructAR Dataset, the first\nopen-source, large-scale, multi-reference input, open domain image generation\ndataset that includes 28K training pairs, each example has two reference\nsubjects, a relative prompt and a background with mask annotation, curated for\nmultiple reference image generation training and evaluating. Comprehensive\nexperiments validate that our approach surpasses current state-of-the-art\nmodels in multiple reference image generation task. The implementation code and\ndatasets will be made publicly. Codes are available, see\nhttps://github.com/lyrig/TokenAR", "AI": {"tldr": "TokenAR通过强化基于Autoregressive (AR)模型的token处理机制解决多参考生成过程中的身份混淆问题，实现高质量多主体生成。", "motivation": "解决多重参考生成方法中不同身份去耦问题，提升图像生成质量与多样性。", "method": "TokenAR框架通过增强token级别的表示来解决参考身份混合的问题，包括：1) Token Index Embedding聚类tokens索引以便更好地表示同一参考图像；2) Instruct Token Injection作为额外的视觉特征容器，注入详细的互补先验知识以增强参考token；3) 身份-标记解耦策略（ITD）使token表示直接关注独立身份特征。", "result": "实验显示，TokenAR在多参考图像生成任务中超越了目前的最先进模型。", "conclusion": "TokenAR显著增强了基于AR模型的方法在条件图像生成中的能力，可实现良好的身份一致性和高质量背景重建。"}}
{"id": "2510.16686", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16686", "abs": "https://arxiv.org/abs/2510.16686", "authors": ["Wenhang Shi", "Shuqing Bian", "Yiren Chen", "Xinyi Zhang", "Zhe Zhao", "Pengfei Hu", "Wei Lu", "Xiaoyong Du"], "title": "Investigating the Impact of Rationales for LLMs on Natural Language Understanding", "comment": null, "summary": "Chain-of-thought (CoT) rationales, which provide step-by-step reasoning to\nderive final answers, benefit LLMs in both inference and training.\nIncorporating rationales, either by generating them before answering during\ninference, or by placing them before or after the original answers during\ntraining - significantly improves model performance on mathematical, symbolic\nand commonsense reasoning tasks. However, most work focuses on the role of\nrationales in these reasoning tasks, overlooking their potential impact on\nother important tasks like natural language understanding (NLU) tasks. In this\nwork, we raise the question: Can rationales similarly benefit NLU tasks? To\nconduct a systematic exploration, we construct NLURC, a comprehensive and\nhigh-quality NLU dataset collection with rationales, and develop various\nrationale-augmented methods. Through exploring the applicability of these\nmethods on NLU tasks using the dataset, we uncover several potentially\nsurprising findings: (1) CoT inference shifts from hindering NLU performance to\nsurpassing direct label prediction as model size grows, indicating a positive\ncorrelation. (2) Most rationale-augmented training methods perform worse than\nlabel-only training, with one specially designed method consistently achieving\nimprovements. (3) LLMs trained with rationales achieve significant performance\ngains on unseen NLU tasks, rivaling models ten times their size, while\ndelivering interpretability on par with commercial LLMs.", "AI": {"tldr": "本研究构建了一个包含理由的NLURC数据集，并探索了这些理由在自然语言理解（NLU）任务中的应用。研究发现，随着模型规模的增大，CoT推理从影响NLU性能转变为超越直接标签预测，某些特定设计的方法能够在训练中提高性能，并且使用理由训练的LLM在未见过的NLU任务上表现出显著的性能提升，达到比它大十倍模型的性能，同时具有与商业LLM相当的可解释性。", "motivation": "尽管已有研究表明在推理任务中加入理由对模型性能有好处，但大多数研究忽略了它们对NLU任务的影响。因此，本研究旨在探讨在NLU任务中使用理由的可能性及效果。", "method": "Structure", "result": "{\n  \"tldr\": \"本研究构建了一个包含理由的NLURC数据集，并探索了这些理由在自然语言理解（NLU）任务中的应用。研究发现，随着模型规模的增大，CoT推理从影响NLU性能转变为超越直接标签预测，某些特定设计的方法能够在训练中提高性能，并且使用理由训练的LLM在未见过的NLU任务上表现出显著的性能提升，达到比它大十倍模型的性能，同时具有与商业LLM相当的可解释性。\",\n  \"motivation\": \"尽管已有研究表明在推理任务中加入理由对模型性能有好处，但大多数研究忽略了它们对NLU任务的影响。因此，本研究旨在探讨在NLU任务中使用理由的可能性及效果。\",\n  \"method\": \"构建了一个包含理由的NLURC数据集，并提出了几种理由增强的方法来探讨这些方法在NLU任务上的可行性。\",\n  \"result\": \"研究表明CoT推理随模型规模增大对NLU性能的影响发生了从负到正的转变，一些特设计的方法在训练中表现出了性能提升。使用理由进行训练的模型在未见过的NLU任务中表现出色，超越了比它大十倍的模型。\",\n  \"conclusion\": \"理由在NLU任务的训练与预测中有潜在价值，特别是在模型规模较大时能够显著提升性能。然而，不同的理由增强方法表现不同，需要设计专门的方法才能有效提高NLU任务中的模型性能。\")", "conclusion": "理由在NLU任务的训练与预测中有潜在价值，特别是在模型规模较大时能够显著提升性能。然而，不同的理由增强方法表现不同，需要设计专门的方法才能有效提高NLU任务中的模型性能。"}}
{"id": "2510.16333", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16333", "abs": "https://arxiv.org/abs/2510.16333", "authors": ["Junha Song", "Sangdoo Yun", "Dongyoon Han", "Jaegul Choo", "Byeongho Heo"], "title": "RL makes MLLMs see better than SFT", "comment": null, "summary": "A dominant assumption in Multimodal Language Model (MLLM) research is that\nits performance is largely inherited from the LLM backbone, given its immense\nparameter scale and remarkable capabilities. This has created a void in the\nunderstanding of the vision encoder, which determines how MLLMs perceive\nimages. The recent shift in MLLM training paradigms, from Supervised Finetuning\n(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the\nsignificant lack of analysis on how such training reshapes the vision encoder\nas well as the MLLM. To address this, we first investigate the impact of\ntraining strategies on MLLMs, where RL shows a clear advantage over SFT in\nstrongly vision-related VQA benchmarks. Motivated by this, we conduct a\ncritical yet under-explored analysis of the vision encoder of MLLMs through\ndiverse and in-depth experiments, ranging from ImageNet classification and\nsegmentation to gradient visualization. Our results demonstrate that MLLM's\npost-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on\nMLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual\nrepresentations. Specifically, the key finding of our study is that RL produces\nstronger and precisely localized visual representations compared to SFT,\nboosting the ability of the vision encoder for MLLM. We then reframe our\nfindings into a simple recipe for building strong vision encoders for MLLMs,\nPreference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,\na PIVOT-trained vision encoder outperforms even larger and more heavily-trained\ncounterparts, despite requiring less than 1% of the computational cost of\nstandard vision pretraining. This result opens an effective and efficient path\nfor advancing the vision backbones of MLLMs. Project page available at\nhttps://june-page.github.io/pivot/", "AI": {"tldr": "研究对比了强化学习(RL)和监督微调(SFT)对多模态语言模型(MLLM)视觉编码器的影响，发现RL在视觉表征上有明显优势，并提出了PIVOT方法增强视觉编码器。", "motivation": "为了填补对MLLM视觉编码器理解不足的空白，特别是训练策略如何影响视觉编码器以及MLLM。", "method": "通过多样且深入的实验分析了多模态语言模型（MLLM）的视觉编码器，实验范围从ImageNet分类和分割到梯度可视化。", "result": "研究结果表明，RL训练策略产生的视觉表征更强且定位更精确，优于SFT策略，并提出了构建强大视觉编码器的简单方法PIVOT。", "conclusion": "PIVOT训练的视觉编码器不仅优于更大、训练更充足的模型，而且计算成本低于标准视觉预训练的1%，为提高MLLM视觉骨干的有效性和效率开辟了路径。"}}
{"id": "2510.16708", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16708", "abs": "https://arxiv.org/abs/2510.16708", "authors": ["Kailai Yang", "Yan Leng", "Xin Zhang", "Tianlin Zhang", "Paul Thompson", "Bernard Keavney", "Maciej Tomaszewski", "Sophia Ananiadou"], "title": "Natural Language Processing Applications in Cardiology: A Narrative Review", "comment": null, "summary": "Cardiovascular disease has become increasingly prevalent in modern society\nand has a significant effect on global health and well-being. Heart-related\nconditions are intricate, multifaceted disorders, which may be influenced by a\ncombination of genetic predispositions, lifestyle choices, and various\nsocioeconomic and clinical factors. Information regarding these potentially\ncomplex interrelationships is dispersed among diverse types of textual data,\nwhich include patient narratives, medical records, and scientific literature,\namong others. Natural language processing (NLP) techniques have increasingly\nbeen adopted as a powerful means to analyse and make sense of this vast amount\nof unstructured data. This, in turn, can allow healthcare professionals to gain\ndeeper insights into the cardiology field, which has the potential to\nrevolutionize current approaches to the diagnosis, treatment, and prevention of\ncardiac problems. This review provides a detailed overview of NLP research in\ncardiology between 2014 and 2025. We queried six literature databases to find\narticles describing the application of NLP techniques in the context of a range\nof different cardiovascular diseases. Following a rigorous screening process,\nwe identified a total of 265 relevant articles. We analysed each article from\nmultiple dimensions, i.e., NLP paradigm types, cardiology-related task types,\ncardiovascular disease types, and data source types. Our analysis reveals\nconsiderable diversity within each of these dimensions, thus demonstrating the\nconsiderable breadth of NLP research within the field. We also perform a\ntemporal analysis, which illustrates the evolution and changing trends in NLP\nmethods employed over the last decade that we cover. To our knowledge, the\nreview constitutes the most comprehensive overview of NLP research in\ncardiology to date.", "AI": {"tldr": "这篇综述论文对2014至2025年间自然语言处理（NLP）技术在心脏病学中的应用进行了详细分析，涵盖文章筛选、多维度分析及时间趋势分析，显示出NLP在心脏病学中的广泛应用及其趋势。", "motivation": "由于心血管疾病在现代社会中越来越普遍，它对全球健康和福祉产生显著影响。本论文旨在通过综述NLP技术在心脏病学文献中的应用，展示对心脏病研究和治疗的潜在变革影响。", "method": "本论文综述了2014年至2025年间自然语言处理（NLP）技术在心脏病学中的应用研究。通过查询六个文献数据库，收集并筛选了265篇相关的文章。从多个维度，包括NLP范式类型、心脏病学相关的任务类型、心血管疾病类型和数据源类型，对每篇文章进行分析。", "result": "论文分析结果展示了NLP研究在心脏病学领域中显著的多样性。时间趋势分析揭示了过去十年间NLP方法的应用演进和变化趋势。", "conclusion": "论文认为该综述为NLP在心脏病学领域应用提供了迄今为止最为全面的概述，展示了NLP在心脏病研究中多样的应用及其在诊断、治疗和预防心病症方面潜在的变革潜力。"}}
{"id": "2510.16335", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16335", "abs": "https://arxiv.org/abs/2510.16335", "authors": ["Bo Peng", "Jie Lu", "Guangquan Zhang", "Zhen Fang"], "title": "On the Provable Importance of Gradients for Language-Assisted Image Clustering", "comment": "revised and extended version of ICCV2025", "summary": "This paper investigates the recently emerged problem of Language-assisted\nImage Clustering (LaIC), where textual semantics are leveraged to improve the\ndiscriminability of visual representations to facilitate image clustering. Due\nto the unavailability of true class names, one of core challenges of LaIC lies\nin how to filter positive nouns, i.e., those semantically close to the images\nof interest, from unlabeled wild corpus data. Existing filtering strategies are\npredominantly based on the off-the-shelf feature space learned by CLIP;\nhowever, despite being intuitive, these strategies lack a rigorous theoretical\nfoundation. To fill this gap, we propose a novel gradient-based framework,\ntermed as GradNorm, which is theoretically guaranteed and shows strong\nempirical performance. In particular, we measure the positiveness of each noun\nbased on the magnitude of gradients back-propagated from the cross-entropy\nbetween the predicted target distribution and the softmax output.\nTheoretically, we provide a rigorous error bound to quantify the separability\nof positive nouns by GradNorm and prove that GradNorm naturally subsumes\nexisting filtering strategies as extremely special cases of itself.\nEmpirically, extensive experiments show that GradNorm achieves the\nstate-of-the-art clustering performance on various benchmarks.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.16712", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16712", "abs": "https://arxiv.org/abs/2510.16712", "authors": ["Shivam Ratnakar", "Sanjay Raghavendra"], "title": "The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models", "comment": null, "summary": "Integration of Large Language Models with search/retrieval engines has become\nubiquitous, yet these systems harbor a critical vulnerability that undermines\ntheir reliability. We present the first systematic investigation of \"chameleon\nbehavior\" in LLMs: their alarming tendency to shift stances when presented with\ncontradictory questions in multi-turn conversations (especially in\nsearch-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising\n17,770 carefully crafted question-answer pairs across 1,180 multi-turn\nconversations spanning 12 controversial domains, we expose fundamental flaws in\nstate-of-the-art systems. We introduce two theoretically grounded metrics: the\nChameleon Score (0-1) that quantifies stance instability, and Source Re-use\nRate (0-1) that measures knowledge diversity. Our rigorous evaluation of\nLlama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent\nfailures: all models exhibit severe chameleon behavior (scores 0.391-0.511),\nwith GPT-4o-mini showing the worst performance. Crucially, small\nacross-temperature variance (less than 0.004) suggests the effect is not a\nsampling artifact. Our analysis uncovers the mechanism: strong correlations\nbetween source re-use rate and confidence (r=0.627) and stance changes\n(r=0.429) are statistically significant (p less than 0.05), indicating that\nlimited knowledge diversity makes models pathologically deferential to query\nframing. These findings highlight the need for comprehensive consistency\nevaluation before deploying LLMs in healthcare, legal, and financial systems\nwhere maintaining coherent positions across interactions is critical for\nreliable decision support.", "AI": {"tldr": "研究揭示了大型语言模型在多轮对话中处理矛盾问题时出现立场不一致的行为，并提出了Chameleon评分和源重用率两个指标用于评估该问题的严重性。研究结果表明模型在处理矛盾问题时表现不佳，显示出对查询框架的高度依赖。", "motivation": "大语言模型和搜索引擎的集成已经变得非常普遍，但其内部存在严重缺陷，影响了其可靠性。本研究针对大型语言模型在多轮对话的矛盾问题中表现出的立场飘忽不定的问题（特别是搜索赋能的大型语言模型）进行了系统的研究。", "method": "通过构建包含17,770个精心设计的问题回答对和1,180个多轮对话的Chameleon基准数据集，涵盖了12个有争议的领域，本研究首次系统性地探讨了大型语言模型（LLMs）在面对矛盾问题时改变立场的现象（Chameleon行为）。同时引入了两个理论基础的指标：Chameleon评分（0-1）量化立场的不稳定性和源重用率（0-1）衡量知识多样性。", "result": "对Llama-4-Maverick，GPT-4o-mini和Gemini-2.5-Flash的严格评估显示，所有模型都表现出了严重的Chameleon行为（评分0.391-0.511），其中GPT-4o-mini表现最差。此外，较小的跨温度变量（小于0.004）表明这并非是一种采样成品。分析揭示了机制：源重用率与置信度（r=0.627）和立场变化之间存在显著强相关性（r=0.429），且统计显著（p<0.05），这说明知识多样性有限使得模型过分依附于查询框架。", "conclusion": "这些发现表明，在大语言模型的部署之前进行一致性评估是非常必要的，特别是在医疗、法律和金融服务等领域，这时模型必须保持连贯的立场以确保可靠的支持决策。"}}
{"id": "2510.16370", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16370", "abs": "https://arxiv.org/abs/2510.16370", "authors": ["Pulin Li", "Guocheng Wu", "Li Yin", "Yuxin Zheng", "Wei Zhang", "Yanjie Zhou"], "title": "MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization", "comment": "https://github.com/wu33learn/MIRAD", "summary": "Social manufacturing leverages community collaboration and scattered\nresources to realize mass individualization in modern industry. However, this\nparadigm shift also introduces substantial challenges in quality control,\nparticularly in defect detection. The main difficulties stem from three\naspects. First, products often have highly customized configurations. Second,\nproduction typically involves fragmented, small-batch orders. Third, imaging\nenvironments vary considerably across distributed sites. To overcome the\nscarcity of real-world datasets and tailored algorithms, we introduce the Mass\nIndividualization Robust Anomaly Detection (MIRAD) dataset. As the first\nbenchmark explicitly designed for anomaly detection in social manufacturing,\nMIRAD captures three critical dimensions of this domain: (1) diverse\nindividualized products with large intra-class variation, (2) data collected\nfrom six geographically dispersed manufacturing nodes, and (3) substantial\nimaging heterogeneity, including variations in lighting, background, and motion\nconditions. We then conduct extensive evaluations of state-of-the-art (SOTA)\nanomaly detection methods on MIRAD, covering one-class, multi-class, and\nzero-shot approaches. Results show a significant performance drop across all\nmodels compared with conventional benchmarks, highlighting the unresolved\ncomplexities of defect detection in real-world individualized production. By\nbridging industrial requirements and academic research, MIRAD provides a\nrealistic foundation for developing robust quality control solutions essential\nfor Industry 5.0. The dataset is publicly available at\nhttps://github.com/wu33learn/MIRAD.", "AI": {"tldr": "提出了MIRAD数据集，专门用于社会制造中的缺陷检测，评估了多种异常检测方法，显示了这些方法在实际个性化生产中的性能下降。", "motivation": "社会制造利用社区协作和分散资源实现个性化生产，但也带来质量问题，特别是缺陷检测。MIRAD数据集旨在克服数据和算法稀缺，评估和优化这些方法以应对实际生产挑战。", "method": "Social manufacturing利用社区协作和分散资源实现现代工业的个性化生产。然而，这一范式转变也带来了质量控制方面的重大挑战，尤其是缺陷检测。主要困难来自三个方面。首先，产品通常具有高度定制化的配置。其次，生产通常涉及分散的小批量订单。第三，图像环境在分布式站点之间差异很大。为了克服现实世界数据集和定制算法的稀缺性，我们引入了大规模个性化鲁棒异常检测（MIRAD）数据集。作为第一个专门为社会制造中的异常检测设计的基准，MIRAD捕捉了该领域的三个关键维度：（1）多样的个性化产品具有较大的类内变异，（2）数据来自六个地理上分散的制造节点，（3）大量的成像异质性，包括光照、背景和运动条件的变化。然后，我们在MIRAD上对最先进的（SOTA）异常检测方法进行了广泛的评估，涵盖了单类别、多类别和零样本方法。", "result": "对多种异常检测方法进行了评估，结果表明所有模型在MIRAD上的表现都显著下降，这表明在实际大规模个性化生产中缺陷检测仍然存在复杂难题。", "conclusion": "MIRAD数据集为开发符合工业需求的稳健质量控制解决方案提供了现实基础，有助于连接工业需求和学术研究，推动Industry 5.0的发展。"}}
{"id": "2510.16713", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16713", "abs": "https://arxiv.org/abs/2510.16713", "authors": ["Sriharsh Bhyravajjula", "Melanie Walsh", "Anna Preus", "Maria Antoniak"], "title": "so much depends / upon / a whitespace: Why Whitespace Matters for Poets and LLMs", "comment": null, "summary": "Whitespace is a critical component of poetic form, reflecting both adherence\nto standardized forms and rebellion against those forms. Each poem's whitespace\ndistribution reflects the artistic choices of the poet and is an integral\nsemantic and spatial feature of the poem. Yet, despite the popularity of poetry\nas both a long-standing art form and as a generation task for large language\nmodels (LLMs), whitespace has not received sufficient attention from the NLP\ncommunity. Using a corpus of 19k English-language published poems from Poetry\nFoundation, we investigate how 4k poets have used whitespace in their works. We\nrelease a subset of 2.8k public-domain poems with preserved formatting to\nfacilitate further research in this area. We compare whitespace usage in the\npublished poems to (1) 51k LLM-generated poems, and (2) 12k unpublished poems\nposted in an online community. We also explore whitespace usage across time\nperiods, poetic forms, and data sources. Additionally, we find that different\ntext processing methods can result in significantly different representations\nof whitespace in poetry data, motivating us to use these poems and whitespace\npatterns to discuss implications for the processing strategies used to assemble\npretraining datasets for LLMs.", "AI": {"tldr": "研究了诗人如何在他们的作品中使用空白符，以此来理解空白符在诗歌中的重要性，并探讨其对大型语言模型预训练数据集构建的影响。", "motivation": "尽管诗歌作为一种长久以来的艺术形式和大型语言模型（LLMs）的生成任务而广泛流行，但空白符并未受到NLP社区足够的关注。为了解决这一问题，并促进未来的研究，研究者们发布了2.8k首公共领域的保留格式的诗歌。", "method": "通过分析来自Poetry Foundation的19k首英文出版诗歌中4k位诗人在作品中使用空白符的方式，探究空白符在诗歌中的作用。", "result": "比较了19k首出版诗歌与51k个LLMs生成的诗歌以及来自在线社区的12k首未发表诗歌中的空白符使用情况，探讨了不同时期、不同诗歌形式和数据源之间的空白符使用差异。", "conclusion": "这一研究指出不同的文本处理方法会显著影响诗歌数据中空白符的表现，并提出这些诗歌和空白符模式可以用来讨论组装LLMs预训练数据集处理策略的含义。"}}
{"id": "2510.16371", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16371", "abs": "https://arxiv.org/abs/2510.16371", "authors": ["Mohammad Javad Ahmadi", "Iman Gandomi", "Parisa Abdi", "Seyed-Farzad Mohammadi", "Amirhossein Taslimi", "Mehdi Khodaparast", "Hassan Hashemi", "Mahdi Tavakoli", "Hamid D. Taghirad"], "title": "Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning in Surgical Video Analysis", "comment": "20 pages, 11 figures, 11 tables. Data descriptor for the Cataract-LMM\n  benchmark dataset. Source code and dataset are available", "summary": "The development of computer-assisted surgery systems depends on large-scale,\nannotated datasets. Current resources for cataract surgery often lack the\ndiversity and annotation depth needed to train generalizable deep-learning\nmodels. To address this gap, we present a dataset of 3,000 phacoemulsification\ncataract surgery videos from two surgical centers, performed by surgeons with a\nrange of experience levels. This resource is enriched with four annotation\nlayers: temporal surgical phases, instance segmentation of instruments and\nanatomical structures, instrument-tissue interaction tracking, and quantitative\nskill scores based on the established competency rubrics like the ICO-OSCAR.\nThe technical quality of the dataset is supported by a series of benchmarking\nexperiments for key surgical AI tasks, including workflow recognition, scene\nsegmentation, and automated skill assessment. Furthermore, we establish a\ndomain adaptation baseline for the phase recognition task by training a model\non a subset of surgical centers and evaluating its performance on a held-out\ncenter. The dataset and annotations are available in Google Form\n(https://docs.google.com/forms/d/e/1FAIpQLSfmyMAPSTGrIy2sTnz0-TMw08ZagTimRulbAQcWdaPwDy187A/viewform?usp=dialog).", "AI": {"tldr": "A study presents a dataset of 3,000 videos of cataract surgeries with detailed annotations for diverse aspects of surgical procedures, including a domain adaptation baseline for phase recognition task. The dataset is designed to support the training and evaluation of AI models for surgical applications.", "motivation": "The motivation behind this research is to contribute a comprehensive, annotated dataset to the field of computer-assisted surgery, particularly for cataract surgery where current datasets are deemed insufficient in diversity and depth of annotation.", "method": "The paper introduces a dataset of 3,000 phacoemulsification cataract surgery videos with annotations for diverse aspects of surgical procedures. Benchmarking experiments are conducted to assess the efficacy of the dataset in surgical AI tasks, with a particular highlight on phase recognition using domain adaptation techniques.", "result": "Benchmarking experiments validate the dataset's effectiveness in tasks such as workflow recognition, scene segmentation, and automated skill assessment, with domain adaptation providing a valuable baseline for phase recognition.", "conclusion": "The introduction of a large dataset with rich annotations significantly advances the field of AI-assisted cataract surgery, offering a robust testbed for the development and evaluation of surgical AI systems."}}
{"id": "2510.16727", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16727", "abs": "https://arxiv.org/abs/2510.16727", "authors": ["Sanskar Pandey", "Ruhaan Chopra", "Angkul Puniya", "Sohom Pal"], "title": "Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models", "comment": null, "summary": "Large language models internalize a structural trade-off between truthfulness\nand obsequious flattery, emerging from reward optimization that conflates\nhelpfulness with polite submission. This latent bias, known as sycophancy,\nmanifests as a preference for user agreement over principled reasoning. We\nintroduce Beacon, a single-turn forced-choice benchmark that isolates this bias\nindependent of conversational context, enabling precise measurement of the\ntension between factual accuracy and submissive bias. Evaluations across twelve\nstate-of-the-art models reveal that sycophancy decomposes into stable\nlinguistic and affective sub-biases, each scaling with model capacity. We\nfurther propose prompt-level and activation-level interventions that modulate\nthese biases in opposing directions, exposing the internal geometry of\nalignment as a dynamic manifold between truthfulness and socially compliant\njudgment. Beacon reframes sycophancy as a measurable form of normative\nmisgeneralization, providing a reproducible foundation for studying and\nmitigating alignment drift in large-scale generative systems.", "AI": {"tldr": "Beacon 是一种用来评估和测量大型语言模型中奉承偏见的基准方法。", "motivation": "大型语言模型在奖励优化过程中存在一个内在的权衡，即把有用性与礼貌的关系混为一谈，这导致了一种叫做奉承的潜在偏见。", "method": "介绍Beacon，这是一个单回合强制选择基准，可以独立于对话上下文隔离出奉承偏见，精确测量事实准确性与服从偏见之间的紧张关系。", "result": "通过对十二个最先进的模型进行评估，发现奉承可以分解为稳定的语言和情感次偏见，每个次偏见都与模型容量成比例。", "conclusion": "Beacon 将奉承重新定义为一种可量化的规范性过度泛化形式，为研究和缓解大规模生成系统中的对齐偏差提供了可重复的基础。"}}
{"id": "2510.16375", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16375", "abs": "https://arxiv.org/abs/2510.16375", "authors": ["Rishi Raj Sahoo", "Surbhi Saswati Mohanty", "Subhankar Mishra"], "title": "iWatchRoadv2: Pothole Detection, Geospatial Mapping, and Intelligent Road Governance", "comment": "Under review", "summary": "Road potholes pose significant safety hazards and maintenance challenges,\nparticularly on India's diverse and under-maintained road networks. This paper\npresents iWatchRoadv2, a fully automated end-to-end platform for real-time\npothole detection, GPS-based geotagging, and dynamic road health visualization\nusing OpenStreetMap (OSM). We curated a self-annotated dataset of over 7,000\ndashcam frames capturing diverse Indian road conditions, weather patterns, and\nlighting scenarios, which we used to fine-tune the Ultralytics YOLO model for\naccurate pothole detection. The system synchronizes OCR-extracted video\ntimestamps with external GPS logs to precisely geolocate each detected pothole,\nenriching detections with comprehensive metadata, including road segment\nattribution and contractor information managed through an optimized backend\ndatabase. iWatchRoadv2 introduces intelligent governance features that enable\nauthorities to link road segments with contract metadata through a secure login\ninterface. The system automatically sends alerts to contractors and officials\nwhen road health deteriorates, supporting automated accountability and warranty\nenforcement. The intuitive web interface delivers actionable analytics to\nstakeholders and the public, facilitating evidence-driven repair planning,\nbudget allocation, and quality assessment. Our cost-effective and scalable\nsolution streamlines frame processing and storage while supporting seamless\npublic engagement for urban and rural deployments. By automating the complete\npothole monitoring lifecycle, from detection to repair verification,\niWatchRoadv2 enables data-driven smart city management, transparent governance,\nand sustainable improvements in road infrastructure maintenance. The platform\nand live demonstration are accessible at\nhttps://smlab.niser.ac.in/project/iwatchroad.", "AI": {"tldr": "iWatchRoadv2是一个全自动的实时道路坑洞检测、GPS定位和动态道路健康可视化平台，使用OpenStreetMap，针对印度道路条件进行了优化。", "motivation": "印度道路网络复杂且维护不足，道路坑洞带来了显著的安全隐患和维护挑战。", "method": "使用了7000多帧驾驶记录仪拍摄的多样化印度道路条件、天气模式和光照情况的数据集，用于微调Ultralytics YOLO模型进行准确的坑洞检测。系统通过OCR提取的视频时间戳与外部GPS日志同步来精确定位每个检测到的坑洞，并通过优化后的后台数据库来管理路段和承包商信息。", "result": "该系统实现了自动化的坑洞检测、定位和可视化，同时支持与承包商和官员的自动报警，有助于政府加强道路管理和维护，实现透明的治理和道路基础设施的持续改进。", "conclusion": "iWatchRoadv2提供了一个成本效益高、可扩展的解决方案，实现了完整的坑洞监测生命周期的自动化，从检测到维修验证，支持智慧城市管理，提高道路健康管理水平。"}}
{"id": "2510.16761", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16761", "abs": "https://arxiv.org/abs/2510.16761", "authors": ["Yikai Zhang", "Ye Rong", "Siyu Yuan", "Jiangjie Chen", "Jian Xie", "Yanghua Xiao"], "title": "Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games", "comment": null, "summary": "Existing language agents often encounter difficulties in dynamic adversarial\ngames due to poor strategic reasoning. To mitigate this limitation, a promising\napproach is to allow agents to learn from game interactions automatically,\nwithout relying on costly expert-labeled data. Unlike static environments where\nagents receive fixed feedback or rewards, selecting appropriate opponents in\ndynamic adversarial games can significantly impact learning performance.\nHowever, the discussion of opponents in adversarial environments remains an\narea under exploration. In this paper, we propose a Step-level poliCy\nOptimization method through Play-And-Learn, SCO-PAL. Leveraging SCO-PAL, we\nconduct a detailed analysis of opponent selection by setting opponents at\ndifferent levels and find that self-play is the most effective way to improve\nstrategic reasoning in such adversarial environments. Utilizing SCO-PAL with\nself-play, we increase the average win rate against four opponents by\napproximately 30% compared to baselines and achieve a 54.76% win rate against\nGPT-4 in six adversarial games.", "AI": {"tldr": "本文提出了一种新的方法SCO-PAL来改进语言代理在对抗环境中的策略推理能力。通过自我对抗实验，发现该方法比基线方法提高了约30%的平均胜率，并在对抗GPT-4的六场游戏中获得了54.76%的胜率。", "motivation": "解决现有语言代理在动态对抗游戏中因策略推理能力不足导致的问题，通过自动从游戏交互中学习来避免依赖于昂贵的专家标注数据。然而，关于对抗环境中对手选择的讨论仍然是一个探索区域。", "method": "提出了一种名为SCO-PAL的分步策略优化方法，该方法通过玩与学习来进行。通过设置不同级别的对手进行了详细的对手选择分析，并发现自我对抗是提高此类对抗环境中策略推理能力的最有效方式。", "result": "利用SCO-PAL与自我对抗，平均胜率对比基线增加了约30%，对抗GPT-4的六场游戏中获得了54.76%的胜率。", "conclusion": "SCO-PAL方法与自我对抗能够显著提高语言代理在动态对抗环境中的策略推理能力，且能有效提高胜率。"}}
{"id": "2510.16377", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16377", "abs": "https://arxiv.org/abs/2510.16377", "authors": ["Tianhang Cheng", "Albert J. Zhai", "Evan Z. Chen", "Rui Zhou", "Yawen Deng", "Zitong Li", "Kejie Zhao", "Janice Shiu", "Qianyu Zhao", "Yide Xu", "Xinlei Wang", "Yuan Shen", "Sheng Wang", "Lisa Ainsworth", "Kaiyu Guan", "Shenlong Wang"], "title": "Demeter: A Parametric Model of Crop Plant Morphology from the Real World", "comment": "ICCV 2025", "summary": "Learning 3D parametric shape models of objects has gained popularity in\nvision and graphics and has showed broad utility in 3D reconstruction,\ngeneration, understanding, and simulation. While powerful models exist for\nhumans and animals, equally expressive approaches for modeling plants are\nlacking. In this work, we present Demeter, a data-driven parametric model that\nencodes key factors of a plant morphology, including topology, shape,\narticulation, and deformation into a compact learned representation. Unlike\nprevious parametric models, Demeter handles varying shape topology across\nvarious species and models three sources of shape variation: articulation,\nsubcomponent shape variation, and non-rigid deformation. To advance crop plant\nmodeling, we collected a large-scale, ground-truthed dataset from a soybean\nfarm as a testbed. Experiments show that Demeter effectively synthesizes\nshapes, reconstructs structures, and simulates biophysical processes. Code and\ndata is available at https://tianhang-cheng.github.io/Demeter/.", "AI": {"tldr": "介绍了一种名为Demeter的新数据驱动参数化模型，用于编码植物形态的关键因素，并展示了在植物形状合成、结构重构及生物物理模拟方面的应用效果。", "motivation": "尽管对于人类和动物而言存在着强大的模型，但对于植物来说却缺乏同样表现力的建模方法。为了推动农作物建模技术的发展，本研究提出了一个可以灵活处理植物形态特征的新模型。", "method": "本研究提出了一种名为Demeter的数据驱动参数化模型，该模型能够编码植物形态的关键因素，包括拓扑结构、形状、关节运动和非刚性变形，并将这些因素编码进一个紧凑的、学习到的表示中。与之前的方法不同，Demeter 能够处理不同种类植物之间形状拓扑的变化，并且模型可以捕捉到三种形状变异性：关节运动、子组件形状变化以及非刚性变形。", "result": "研究人员从大豆农场收集了大规模的真实数据作为实验的测试平台，实验结果表明，Demeter 模型能有效地合成形状、重构结构，并模拟生物物理过程。", "conclusion": "本研究开发的Demeter参数化模型能够在一定程度上填补植物建模领域的空白，并在合成形状、结构重构以及生物物理过程模拟方面显示出良好的效果。"}}
{"id": "2510.16783", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16783", "abs": "https://arxiv.org/abs/2510.16783", "authors": ["Sheikh Jubair", "Arwa Omayrah", "Amal Alshammari", "Alhanoof Althnian", "Abdulhamed Alothaimen", "Norah A. Alzahrani", "Shahad D. Alzaidi", "Nora Al-Twairesh", "Abdulmohsen Al-Thubaity"], "title": "LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding", "comment": "1 figure, 15 tables, 10 main pages", "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nsophisticated capabilities, including the ability to process and comprehend\nextended contexts. These emergent capabilities necessitate rigorous evaluation\nmethods to effectively assess their performance in long-context understanding.\nIn this paper, we present \\textbf{LC-Eval}, a bilingual, multi-task evaluation\nbenchmark designed to evaluate long-context understanding in English and\nArabic, targeting context lengths ranging from 4k to over 128k tokens. LC-Eval\nintroduces four novel and challenging tasks: multi-document question answering,\nbilingual question answering, claim verification within a paragraph, and\nmultiple-choice questions based on long contexts. These tasks are designed to\nassess LLMs' abilities in deep reasoning, document comprehension, information\ntracing, and bilingual information extraction and understanding. The benchmark\nincludes datasets in both Arabic and English for each task, allowing for a\ncomparative analysis of their performance across different text genres.\nEvaluations were conducted on both open-weight and closed LLMs, with results\nindicating that LC-Eval presents significant challenges. Even high-performing\nmodels, such as GPT-4o, struggled with certain tasks, highlighting the\ncomplexity and rigor of the benchmark.", "AI": {"tldr": "本文介绍了一种名为LC-Eval的双语多任务评估基准，旨在评估英语和阿拉伯语的长文本理解性能。通过四个新设计的任务，LC-Eval显示了即使是顶级语言模型也面临挑战。", "motivation": "随着大型语言模型处理和理解长文本内容能力的提升，研究者需要更严格的评估方法来评判这些模型在长文本理解上的表现。", "method": "该研究提出了一种名为LC-Eval的新型评估基准，用于评估长文本理解能力。LC-Eval涵盖了四类任务：多文档问答、双语问答、段落内的主张验证及基于长上下文的多选题。这些任务旨在评估语言模型在深度推理、文档理解、信息追溯及双语信息提取与理解方面的能力。", "result": "评估表明，即使是像GPT-4o这样表现优异的模型，在某些任务上也表现出了挑战，这说明LC-Eval具有一定的复杂性和严谨性。", "conclusion": "LC-Eval提供了一种有效的手段来评估语言模型在长文本理解能力上的性能，尤其是在涉及深度推理、信息追溯和多语言理解的任务上。"}}
{"id": "2510.16396", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16396", "abs": "https://arxiv.org/abs/2510.16396", "authors": ["Yeh Keng Hao", "Hsu Tzu Wei", "Sun Min"], "title": "SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation", "comment": "Accepted to AICCC 2025", "summary": "With the increasing ubiquity of AR/VR devices, the deployment of deep\nlearning models on edge devices has become a critical challenge. These devices\nrequire real-time inference, low power consumption, and minimal latency. Many\nframework designers face the conundrum of balancing efficiency and performance.\nWe design a light framework that adopts an encoder-decoder architecture and\nintroduces several key contributions aimed at improving both efficiency and\naccuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the\ninherent sparsity in hand pose images, achieving a 42% end-to-end efficiency\nimprovement. Moreover, we propose our SPLite decoder. This new architecture\nsignificantly boosts the decoding process's frame rate by 3.1x on the Raspberry\nPi 5, while maintaining accuracy on par. To further optimize performance, we\napply quantization-aware training, reducing memory usage while preserving\naccuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on\nFreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5\nCPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on\ncompound benchmark datasets, demonstrating comparable accuracy to\nstate-of-the-art approaches while significantly enhancing computational\nefficiency.", "AI": {"tldr": "The paper presents a light framework using sparse convolution, SPLite decoder, and quantization-aware training to achieve real-time and low-latency inference on AR/VR devices, demonstrating 2.98x speed-up on Raspberry Pi 5 without losing accuracy.", "motivation": "The challenge of deploying deep learning models on AR/VR devices, which require real-time inference, low power consumption, and low latency, motivates the design of an efficient and accurate model in this paper.", "method": "We design a light framework with an encoder-decoder architecture. Sparse convolution is applied on a ResNet-18 backbone to handle the sparsity in hand pose images, leading to a 42% efficiency improvement. The SPLite decoder is introduced to enhance the decoding process's frame rate by 3.1x. Additionally, quantization-aware training is used to decrease memory usage without sacrificing accuracy.", "result": "The proposed system achieves a 2.98x speed-up on a Raspberry Pi 5 CPU. It maintains comparable accuracy to the state-of-the-art methods but significantly enhances computational efficiency, as shown on the FreiHAND dataset.", "conclusion": "This work introduces an efficient and accurate deep learning framework for AR/VR devices. By improving both the efficiency and accuracy, the proposed system overcomes the challenge of deploying deep learning models on edge devices with real-time requirements."}}
{"id": "2510.16797", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16797", "abs": "https://arxiv.org/abs/2510.16797", "authors": ["Vera Pavlova", "Mohammed Makhlouf"], "title": "MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning", "comment": null, "summary": "We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain\nContrastive learning), a multi-stage framework for domain adaptation of\nsentence embedding models that incorporates joint domain-specific masked\nsupervision. Our approach addresses the challenges of adapting large-scale\ngeneral-domain sentence embedding models to specialized domains. By jointly\noptimizing masked language modeling (MLM) and contrastive objectives within a\nunified training pipeline, our method enables effective learning of\ndomain-relevant representations while preserving the robust semantic\ndiscrimination properties of the original model. We empirically validate our\napproach on both high-resource and low-resource domains, achieving improvements\nup to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong\ngeneral-domain baselines. Comprehensive ablation studies further demonstrate\nthe effectiveness of each component, highlighting the importance of balanced\njoint supervision and staged adaptation.", "AI": {"tldr": "本论文引入了MOSAIC方法，通过结合掩码语言模型和对比目标的联合优化来实现句嵌入模型的领域适应，并在多个资源环境和领域中展示了相比通用领域基线模型的性能提升。", "motivation": "该论文旨在解决将在大规模通用领域句嵌入模型适应到专业领域中所面临的挑战。", "method": "MOSAIC方法结合了联合领域特定的掩码监督进行多阶段框架的领域适应。通过在同一训练管道中联合优化掩码语言模型（MLM）和对比目标，该方法可以有效地学习领域相关的表示，同时保留原始模型的鲁棒语义区分特性。", "result": "实验验证表明，MOSAIC方法在多个资源环境下均能改善结果，在NDCG@10指标上比强基线模型最高提高了13.4%。此外，详尽的消融研究进一步表明各个组件的有效性，特别是平衡联合监督和阶段性适应的重要性。", "conclusion": "MOSAIC方法对于领域适应的句嵌入模型是有效的，可以在保持鲁棒语义区分特性的同时学习到相关的领域表示。"}}
{"id": "2510.16410", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16410", "abs": "https://arxiv.org/abs/2510.16410", "authors": ["Changyue Shi", "Minghao Chen", "Yiping Mao", "Chuxiao Yang", "Xinyuan Hu", "Jiajun Ding", "Zhou Yu"], "title": "REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting", "comment": null, "summary": "Bridging the gap between complex human instructions and precise 3D object\ngrounding remains a significant challenge in vision and robotics. Existing 3D\nsegmentation methods often struggle to interpret ambiguous, reasoning-based\ninstructions, while 2D vision-language models that excel at such reasoning lack\nintrinsic 3D spatial understanding. In this paper, we introduce REALM, an\ninnovative MLLM-agent framework that enables open-world reasoning-based\nsegmentation without requiring extensive 3D-specific post-training. We perform\nsegmentation directly on 3D Gaussian Splatting representations, capitalizing on\ntheir ability to render photorealistic novel views that are highly suitable for\nMLLM comprehension. As directly feeding one or more rendered views to the MLLM\ncan lead to high sensitivity to viewpoint selection, we propose a novel\nGlobal-to-Local Spatial Grounding strategy. Specifically, multiple global views\nare first fed into the MLLM agent in parallel for coarse-level localization,\naggregating responses to robustly identify the target object. Then, several\nclose-up novel views of the object are synthesized to perform fine-grained\nlocal segmentation, yielding accurate and consistent 3D masks. Extensive\nexperiments show that REALM achieves remarkable performance in interpreting\nboth explicit and implicit instructions across LERF, 3D-OVS, and our newly\nintroduced REALM3D benchmarks. Furthermore, our agent framework seamlessly\nsupports a range of 3D interaction tasks, including object removal,\nreplacement, and style transfer, demonstrating its practical utility and\nversatility. Project page: https://ChangyueShi.github.io/REALM.", "AI": {"tldr": "该研究提出了REALM框架，旨在解决3D对象定位问题，通过结合2D视觉语言模型和3D高斯散射表示，以及提出新的全局到局部空间定位策略，提升了在不同指令下的3D目标分割性能。", "motivation": "目前的3D分割方法难以应对模糊的、基于推理的指令，而2D视觉语言模型虽然在这样的推理上表现优异，但缺乏内禀的3D空间理解能力。", "method": "引入了一个称为REALM的新式MLLM（多模式语言-视觉模型）-agent框架，可以在开放世界环境下进行基于推理的分割，同时避免需要大量的3D特定后训练。利用3D高斯散射表示进行分割，并引入了全局到局部的空间定位策略以提高分割准确性。", "result": "实验表明，REALM在LERF、3D-OVS和新引入的REALM3D基准测试中，对明确和隐含指令的解释都表现出了出色的能力。", "conclusion": "该研究展示了一个能理解和执行复杂人类指令的系统框架，适用于3D交互任务，如对象移除、替换、样式转换等。"}}
{"id": "2510.16815", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16815", "abs": "https://arxiv.org/abs/2510.16815", "authors": ["Hans Hergen Lehmann", "Jae Hee Lee", "Steven Schockaert", "Stefan Wermter"], "title": "Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities", "comment": "33 pages, 20 figures. Submitted ACL ARR 2025 October (under review)", "summary": "Large Language Models (LLMs) are increasingly used for knowledge-based\nreasoning tasks, yet understanding when they rely on genuine knowledge versus\nsuperficial heuristics remains challenging. We investigate this question\nthrough entity comparison tasks by asking models to compare entities along\nnumerical attributes (e.g., ``Which river is longer, the Danube or the\nNile?''), which offer clear ground truth for systematic analysis. Despite\nhaving sufficient numerical knowledge to answer correctly, LLMs frequently make\npredictions that contradict this knowledge. We identify three heuristic biases\nthat strongly influence model predictions: entity popularity, mention order,\nand semantic co-occurrence. For smaller models, a simple logistic regression\nusing only these surface cues predicts model choices more accurately than the\nmodel's own numerical predictions, suggesting heuristics largely override\nprincipled reasoning. Crucially, we find that larger models (32B parameters)\nselectively rely on numerical knowledge when it is more reliable, while smaller\nmodels (7--8B parameters) show no such discrimination, which explains why\nlarger models outperform smaller ones even when the smaller models possess more\naccurate knowledge. Chain-of-thought prompting steers all models towards using\nthe numerical features across all model sizes.", "AI": {"tldr": "本文研究了大型语言模型如何在数值知识和启发式偏见之间做出选择。发现较大型的模型能够在数值知识更可靠时优先依赖这种知识，而较小的模型则依赖启发式偏见。使用chain-of-thought提示可以改进所有模型的数值属性利用。", "motivation": "研究的动机在于探讨LLMs在知识推理任务中如何以及何时依赖真实的数值知识，而不是依赖于表面的启发式方法。", "method": "通过实体比较任务研究大型语言模型（LLMs）是否依赖真实的知识或只是表面的启发式方法。具体来说，任务要求模型比较实体的数值属性（例如，“多瑙河和尼罗河，哪条河更长？”），这类问题可以提供明确的ground truth以进行系统的分析。", "result": "研究揭示了三种强烈影响模型预测的启发式偏差：实体流行度、提及顺序、语义共现。对于较小的模型，仅使用这些表面线索的简单逻辑回归模型就能比这些小模型自身的数值预测更准确地预测模型的选择，说明启发式方法很多时候覆盖了基于原则的推理。然而，较大的模型在数值知识更可靠的情况下，优先依赖数值知识。", "conclusion": "研究得出的结论是：较大的模型（32B参数）在数值知识更可靠时，会更依赖数值知识而非启发式偏见；而较小的模型（7-8B参数）则没有这种区分能力。因此，即使小型模型拥有更准确的知识，大型模型依然能更好地表现。此外，通过chain-of-thought提示法，可以引导所有模型使用数值属性进行推理。"}}
{"id": "2510.16416", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16416", "abs": "https://arxiv.org/abs/2510.16416", "authors": ["Xiaojun Guo", "Runyu Zhou", "Yifei Wang", "Qi Zhang", "Chenheng Zhang", "Stefanie Jegelka", "Xiaohan Wang", "Jiajun Chai", "Guojun Yin", "Wei Lin", "Yisen Wang"], "title": "SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning", "comment": null, "summary": "Vision-language models (VLMs) have shown remarkable abilities by integrating\nlarge language models with visual inputs. However, they often fail to utilize\nvisual evidence adequately, either depending on linguistic priors in\nvision-centric tasks or resorting to textual shortcuts during reasoning.\nAlthough reinforcement learning (RL) can align models with desired behaviors,\nits application to VLMs has been hindered by the lack of scalable and reliable\nreward mechanisms. To overcome this challenge, we propose SSL4RL, a novel\nframework that leverages self-supervised learning (SSL) tasks as a source of\nverifiable rewards for RL-based fine-tuning. Our approach reformulates SSL\nobjectives-such as predicting image rotation or reconstructing masked\npatches-into dense, automatic reward signals, eliminating the need for human\npreference data or unreliable AI evaluators. Experiments show that SSL4RL\nsubstantially improves performance on both vision-centric and vision-language\nreasoning benchmarks. Furthermore, through systematic ablations, we identify\nkey factors-such as task difficulty, model scale, and semantic alignment with\nthe target domain-that influence the effectiveness of SSL4RL tasks, offering\nnew design principles for future work. We also demonstrate the framework's\ngenerality by applying it to graph learning, where it yields significant gains.\nSSL4RL establishes a versatile and effective paradigm for aligning multimodal\nmodels using verifiable, self-supervised objectives.", "AI": {"tldr": "SSL4RL框架解决了强化学习在视觉语言模型中应用的难题，利用自监督学习任务作为可靠奖励来源，获得了出色的性能提升。", "motivation": "视觉语言模型虽然能够将大型语言模型与视觉输入结合展现出强大的能力，但仍经常会由于依赖语言先验或在推理中依赖文本捷径而未能充分利用视觉证据。尽管强化学习可以将模型对齐到期望的行为，但由于缺乏可扩展的和可靠的奖励机制，其在视觉语言模型中的应用受到阻碍。", "method": "SSL4RL框架通过使用自监督学习任务作为强化学习中可靠的奖励机制来源，解决了强化学习在视觉语言模型中的应用难题。具体来说，它将自监督学习目标（如预测图像旋转或重建遮罩的补丁）转化为密集的自动奖励信号，避免了对人工偏好数据或不可靠的AI评估者的依赖。", "result": "实验结果表明，SSL4RL在视觉主导任务和视觉语言推理基准上显著提高了性能。通过系统的消融研究，我们还确定了关键影响因素，如任务难度、模型规模以及与目标领域的语义对齐，为未来的工作提供了新的设计原则。进一步证明了该框架的通用性，将其应用于图学习中也取得了显著的增益。", "conclusion": "SSL4RL建立了一个灵活有效的范式，使用可验证的自监督目标来对齐多模态模型。"}}
