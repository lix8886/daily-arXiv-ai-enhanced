{"id": "2508.05722", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05722", "abs": "https://arxiv.org/abs/2508.05722", "authors": ["Rania Al-Sabbagh"], "title": "PEACH: A sentence-aligned Parallel English-Arabic Corpus for Healthcare", "comment": null, "summary": "This paper introduces PEACH, a sentence-aligned parallel English-Arabic\ncorpus of healthcare texts encompassing patient information leaflets and\neducational materials. The corpus contains 51,671 parallel sentences, totaling\napproximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths\nvary between 9.52 and 11.83 words on average. As a manually aligned corpus,\nPEACH is a gold-standard corpus, aiding researchers in contrastive linguistics,\ntranslation studies, and natural language processing. It can be used to derive\nbilingual lexicons, adapt large language models for domain-specific machine\ntranslation, evaluate user perceptions of machine translation in healthcare,\nassess patient information leaflets and educational materials' readability and\nlay-friendliness, and as an educational resource in translation studies. PEACH\nis publicly accessible.", "AI": {"tldr": "Introduces PEACH, a manually aligned English-Arabic healthcare text corpus, useful for language studies and machine translation, publicly available.", "motivation": "To provide a gold-standard corpus for researchers in contrastive linguistics, translation studies, and natural language processing, enabling various evaluations and adaptations, such as bilingual lexicon creation and machine translation.", "method": "Introduces PEACH, a manually aligned parallel English-Arabic healthcare text corpus consisting of patient information and educational materials.", "result": "The corpus contains 51,671 parallel sentences, totaling approximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths vary between 9.52 and 11.83 words on average.", "conclusion": "PEACH is a valuable resource for deriving bilingual lexicons, adapting language models for healthcare-specific machine translation, assessing patient information readability, and as an educational tool in translation studies, and is publicly accessible."}}
{"id": "2508.05775", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.05775", "abs": "https://arxiv.org/abs/2508.05775", "authors": ["Chi Zhang", "Changjia Zhu", "Junjie Xiong", "Xiaoran Xu", "Lingyao Li", "Yao Liu", "Zhuo Lu"], "title": "Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation", "comment": null, "summary": "Large Language Models (LLMs) have revolutionized content creation across\ndigital platforms, offering unprecedented capabilities in natural language\ngeneration and understanding. These models enable beneficial applications such\nas content generation, question and answering (Q&A), programming, and code\nreasoning. Meanwhile, they also pose serious risks by inadvertently or\nintentionally producing toxic, offensive, or biased content. This dual role of\nLLMs, both as powerful tools for solving real-world problems and as potential\nsources of harmful language, presents a pressing sociotechnical challenge. In\nthis survey, we systematically review recent studies spanning unintentional\ntoxicity, adversarial jailbreaking attacks, and content moderation techniques.\nWe propose a unified taxonomy of LLM-related harms and defenses, analyze\nemerging multimodal and LLM-assisted jailbreak strategies, and assess\nmitigation efforts, including reinforcement learning with human feedback\n(RLHF), prompt engineering, and safety alignment. Our synthesis highlights the\nevolving landscape of LLM safety, identifies limitations in current evaluation\nmethodologies, and outlines future research directions to guide the development\nof robust and ethically aligned language technologies.", "AI": {"tldr": "文章回顾了大型语言模型（LLMs）的毒性问题，提出了一种LLM相关损害与防御的统一分类法，分析了缓解策略，并指出了未来的研究方向。", "motivation": "鉴于大型语言模型（LLM）在解决实际问题方面的强大功能及其潜在的语言危害，文章旨在探索这一两面性的社会技术挑战，并提出了未来的研究方向，以引导开发稳健且符合伦理的语言技术。", "method": "文章通过系统地回顾近期研究，涵盖了无意产生的毒性、对抗性越狱攻击和内容监管技术。作者提出了一个LLM相关的损害和防御的统一分类法，并分析了新兴的多模态和辅助越狱策略，以及评估了包括带有人类反馈的强化学习（RLHF）、提示工程和安全对齐在内的缓解措施。", "result": "文章提出了一种统一的分类法，分析了新兴的策略和评估了缓解措施，强调了当前评估方法的局限性。这为理解和解决LLM安全问题提供了框架。", "conclusion": "文章突出了LLM安全的不断变化的格局，指出现有评估方法的局限，并概述了未来的研究方向，以指导开发稳健和伦理上一致的语言技术。"}}
{"id": "2508.05782", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05782", "abs": "https://arxiv.org/abs/2508.05782", "authors": ["Xiangyan Chen", "Yufeng Li", "Yujian Gan", "Arkaitz Zubiaga", "Matthew Purver"], "title": "FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification", "comment": null, "summary": "Large Language Models (LLMs) are known to produce hallucinations - factually\nincorrect or fabricated information - which poses significant challenges for\nmany Natural Language Processing (NLP) applications, such as dialogue systems.\nAs a result, detecting hallucinations has become a critical area of research.\nCurrent approaches to hallucination detection in dialogue systems primarily\nfocus on verifying the factual consistency of generated responses. However,\nthese responses often contain a mix of accurate, inaccurate or unverifiable\nfacts, making one factual label overly simplistic and coarse-grained. In this\npaper, we introduce a benchmark, FineDialFact, for fine-grained dialogue fact\nverification, which involves verifying atomic facts extracted from dialogue\nresponses. To support this, we construct a dataset based on publicly available\ndialogue datasets and evaluate it using various baseline methods. Experimental\nresults demonstrate that methods incorporating Chain-of-Thought (CoT) reasoning\ncan enhance performance in dialogue fact verification. Despite this, the best\nF1-score achieved on the HybriDialogue, an open-domain dialogue dataset, is\nonly 0.75, indicating that the benchmark remains a challenging task for future\nresearch. Our dataset and code will be public on GitHub.", "AI": {"tldr": "A NEW BENCHMARK FOR FINE-GRAINED DIALOGUE FACT VERIFICATION: This paper presents FineDialFact, a benchmark for verifying atomic facts within dialogue responses, aiming to improve the detection of hallucinations in large language models with more sophisticated methods.", "motivation": "ADDRESSING HALLUCINATIONS IN DIALOGUE SYSTEMS: The motivation is to tackle the challenge of hallucinations - factually incorrect or fabricated information - produced by large language models, particularly in dialogue systems, where current detection methods are simplistic.", "method": "STANCE DETECTION AND FINE-GRAINED DIALOGUE FACT VERIFICATION: The paper introduces a new benchmark named FineDialFact for fine-grained dialogue fact verification, which involves verifying atomic facts extracted from dialogue responses. It constructs a dataset based on existing dialogue datasets.", "result": "METHODS INCORPORATING CHAIN-OF-THOUGHT REASONING IMPROVE PERFORMANCE: Experimental results on HybriDialogue, an open-domain dialogue dataset, show that methods incorporating Chain-of-Thought (CoT) reasoning can enhance performance in dialogue fact verification, but the highest F1-score achieved is only 0.75, indicating the difficulty of the task.", "conclusion": "FINE-DIALOGUE VERIFICATION REMAINS A CHALLENGE: Although Chain-of-Thought methods improve the performance of dialogue fact verification, the best F1-score achieved is only 0.75, showing that the task is still a significant challenge for future research. The dataset and code will be made publicly available on GitHub."}}
{"id": "2508.05803", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.05803", "abs": "https://arxiv.org/abs/2508.05803", "authors": ["Abishek Thamma", "Micha Heilbron"], "title": "Human-like fleeting memory improves language learning but impairs reading time prediction in transformer language models", "comment": null, "summary": "Human memory is fleeting. As words are processed, the exact wordforms that\nmake up incoming sentences are rapidly lost. Cognitive scientists have long\nbelieved that this limitation of memory may, paradoxically, help in learning\nlanguage - an idea supported by classic connectionist modelling work. The rise\nof Transformers appears to challenge this idea, as these models can learn\nlanguage effectively, despite lacking memory limitations or other architectural\nrecency biases. Here, we investigate the hypothesized benefit of fleeting\nmemory for language learning in tightly controlled experiments on transformer\nlanguage models. Training transformers with and without fleeting memory on a\ndevelopmentally realistic training set, we find that fleeting memory\nconsistently improves language learning (as quantified by both overall language\nmodelling performance and targeted syntactic evaluation) but, unexpectedly,\nimpairs surprisal-based prediction of human reading times. Interestingly,\nfollow up analyses revealed that this discrepancy - better language modeling,\nyet worse reading time prediction - could not be accounted for by prior\nexplanations of why better language models sometimes fit human reading time\nworse. Together, these results support a benefit of memory limitations on\nneural network language learning - but not on predicting behavior.", "AI": {"tldr": "研究发现记忆的短暂性有助于Transformer语言模型学习语言，但在预测人类阅读时间行为上的表现不如预期。", "motivation": "探索记忆的短暂性是否像传统认知科学模型所支持的那样能促进语言学习，尽管像Transformer这样的现代模型能够有效学习语言，即便没有记忆限制或其他架构上的时间偏差。", "method": "通过在具有和不具有短暂记忆的Transformer语言模型上进行严格控制的实验来研究记忆的短暂性对语言学习的假设益处。", "result": "实验结果表明记忆的短暂性在语言学习中有持续的优点，提高了整体语言建模性能和目标句法评估性能，但意外地减弱了对人类阅读时间的预测能力。", "conclusion": "这些结果支持神经网络语言学习中记忆限制的益处，但对于预测行为并没有帮助。"}}
{"id": "2508.05689", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05689", "abs": "https://arxiv.org/abs/2508.05689", "authors": ["Jinjia Peng", "Zeze Tao", "Huibing Wang", "Meng Wang", "Yang Wang"], "title": "Boosting Adversarial Transferability via Residual Perturbation Attack", "comment": "Accepted to ieee/cvf international conference on computer vision\n  (ICCV2025)", "summary": "Deep neural networks are susceptible to adversarial examples while suffering\nfrom incorrect predictions via imperceptible perturbations. Transfer-based\nattacks create adversarial examples for surrogate models and transfer these\nexamples to target models under black-box scenarios. Recent studies reveal that\nadversarial examples in flat loss landscapes exhibit superior transferability\nto alleviate overfitting on surrogate models. However, the prior arts overlook\nthe influence of perturbation directions, resulting in limited transferability.\nIn this paper, we propose a novel attack method, named Residual Perturbation\nAttack (ResPA), relying on the residual gradient as the perturbation direction\nto guide the adversarial examples toward the flat regions of the loss function.\nSpecifically, ResPA conducts an exponential moving average on the input\ngradients to obtain the first moment as the reference gradient, which\nencompasses the direction of historical gradients. Instead of heavily relying\non the local flatness that stems from the current gradients as the perturbation\ndirection, ResPA further considers the residual between the current gradient\nand the reference gradient to capture the changes in the global perturbation\ndirection. The experimental results demonstrate the better transferability of\nResPA than the existing typical transfer-based attack methods, while the\ntransferability can be further improved by combining ResPA with the current\ninput transformation methods. The code is available at\nhttps://github.com/ZezeTao/ResPA.", "AI": {"tldr": "文章提出ResPA攻击方法，以提高对抗样本的转移能力。通过引入输入梯度的指数加权平均值和梯度残差，增强了对全局扰动方向变化的捕捉，实验展示了更高的转移效果。", "motivation": "对抗样本在对抗性攻击下的转移能力受到忽视，特别是对抗样本在平坦损失景观中的转移能力更能减轻对替代模型的过拟合，但现有方法忽略了扰动方向对转移能力的影响，因此需提出新的攻击方法。", "method": "文章提出了一种名为Residual Perturbation Attack (ResPA) 的攻击方法，利用残差梯度作为扰动方向，指引对抗样本朝损失函数的平坦区移动。该方法使用输入梯度的指数加权平均值作为参考梯度，结合当前梯度与参考梯度之间的残差，捕捉全局扰动方向的变化。", "result": "<tool_call>", "conclusion": "实验结果表明，ResPA在对抗样本的转移能力方面优于现有典型的对抗性转移攻击方法，结合当前输入变换方法可进一步提升其转移能力。"}}
{"id": "2508.05830", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.05830", "abs": "https://arxiv.org/abs/2508.05830", "authors": ["Tong Li", "Rasiq Hussain", "Mehak Gupta", "Joshua R. Oltmanns"], "title": "\"Mirror\" Language AI Models of Depression are Criterion-Contaminated", "comment": "39 pages, 9 figures", "summary": "A growing number of studies show near-perfect LLM language-based prediction\nof depression assessment scores (up to R2 of .70). However, many develop these\nmodels directly from language responses to depression assessments. These\n\"Mirror models\" suffer from \"criterion contamination\", which arises when a\npredicted score depends in part on the predictors themselves. This causes\nartificial effect size inflation which reduces model generalizability. The\npresent study compares the performance of Mirror models versus \"Non-Mirror\nmodels\", which are developed from language that does not mirror the assessment\nthey are developed to predict. N = 110 research participants completed two\ndifferent interviews: structured diagnostic and life history interviews. GPT-4,\nGPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic\ninterview depression scores from the two transcripts separately. Mirror models\n(using structured diagnostic data) showed very large effect sizes (e.g., R2 =\n.80). As expected, NonMirror models (using life history data) demonstrated\nsmaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror\nand Non-Mirror model-predicted structured interview depression scores were\ncorrelated with self-reported depression symptoms, Mirror and NonMirror\nperformed the same (e.g., r = ~.54), indicating that Mirror models contain bias\nperhaps due to criterion contamination. Topic modeling identified clusters\nacross Mirror and Non-Mirror models, as well as between true-positive and\nfalse-positive predictions. In this head-to-head comparison study, Mirror\nlanguage AI models of depression showed artificially inflated effect sizes and\nless generalizability. As language AI models for depression continue to evolve,\nincorporating Non-Mirror models may identify interpretable, and generalizable\nsemantic features that have unique utility in real-world psychological\nassessment.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.05732", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05732", "abs": "https://arxiv.org/abs/2508.05732", "authors": ["Pinxuan Li", "Bing Cao", "Changqing Zhang", "Qinghua Hu"], "title": "Generalized Few-Shot Out-of-Distribution Detection", "comment": null, "summary": "Few-shot Out-of-Distribution (OOD) detection has emerged as a critical\nresearch direction in machine learning for practical deployment. Most existing\nFew-shot OOD detection methods suffer from insufficient generalization\ncapability for the open world. Due to the few-shot learning paradigm, the OOD\ndetection ability is often overfit to the limited training data itself, thus\ndegrading the performance on generalized data and performing inconsistently\nacross different scenarios. To address this challenge, we proposed a\nGeneralized Few-shot OOD Detection (GOOD) framework, which empowers the general\nknowledge of the OOD detection model with an auxiliary General Knowledge Model\n(GKM), instead of directly learning from few-shot data. We proceed to reveal\nthe few-shot OOD detection from a generalization perspective and theoretically\nderive the Generality-Specificity balance (GS-balance) for OOD detection, which\nprovably reduces the upper bound of generalization error with a general\nknowledge model. Accordingly, we propose a Knowledge Dynamic Embedding (KDE)\nmechanism to adaptively modulate the guidance of general knowledge. KDE\ndynamically aligns the output distributions of the OOD detection model to the\ngeneral knowledge model based on the Generalized Belief (G-Belief) of GKM,\nthereby boosting the GS-balance. Experiments on real-world OOD benchmarks\ndemonstrate our superiority. Codes will be available.", "AI": {"tldr": "本文提出了一种新的少样本OOD检测框架GOOD，在开放世界中提高了泛化能力。通过引入辅助通用知识模型和知识动态嵌入机制，优化了广度-特异性平衡，从而改进检测性能。", "motivation": "现有的少样本OOD检测方法通常泛化能力不足，尤其是在开放世界中，这限制了它们在不同场景下的表现。为了克服这一挑战，提出了这项研究。", "method": "我们提出了一种名为Generalized Few-shot OOD Detection (GOOD) 的框架，通过引入一个辅助的通用知识模型（GKM）来增强OOD检测模型的泛化能力，而不是直接从少量数据中学习。我们还提出了一种知识动态嵌入（KDE）机制，通过基于GKM的广义信念（G-Belief）动态调整输出分布，从而优化广度-特异性平衡（GS-balance）。", "result": "实验结果表明，该方法在现实世界的OOD基准上取得了优越的性能。", "conclusion": "GOOD框架通过引入辅助通用知识模型和使用知识动态嵌入机制，能够在少样本情况下提高OOD检测的泛化性能并激发新的研究方向。"}}
{"id": "2508.05843", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05843", "abs": "https://arxiv.org/abs/2508.05843", "authors": ["Miles Gilberti", "Shane Storks", "Huteng Dai"], "title": "Discovering Properties of Inflectional Morphology in Neural Emergent Communication", "comment": null, "summary": "Emergent communication (EmCom) with deep neural network-based agents promises\nto yield insights into the nature of human language, but remains focused\nprimarily on a few subfield-specific goals and metrics that prioritize\ncommunication schemes which represent attributes with unique characters\none-to-one and compose them syntactically. We thus reinterpret a common EmCom\nsetting, the attribute-value reconstruction game, by imposing a\nsmall-vocabulary constraint to simulate double articulation, and formulating a\nnovel setting analogous to naturalistic inflectional morphology (enabling\nmeaningful comparison to natural language communication schemes). We develop\nnew metrics and explore variations of this game motivated by real properties of\ninflectional morphology: concatenativity and fusionality. Through our\nexperiments, we discover that simulated phonological constraints encourage\nconcatenative morphology, and emergent languages replicate the tendency of\nnatural languages to fuse grammatical attributes.", "AI": {"tldr": "本文对新兴通信中属性-值重构游戏施加小词汇量限制，模拟自然语言的形态特性，发现音系限制会引导句法组合，并且产生的语言有语法属性融合的趋势。", "motivation": "本文旨在重新解释Emergent communication（EmCom）中的一个常见设置，通过施加小词汇量限制以及模拟自然语言中的词形变化机制，深入探讨在特定约束下Agent之间通信方式的新形态，并将之与自然语言中的交际机制进行比较。", "method": "我们通过重新解释常见的EmCom设置，即属性-值重构游戏，来施加一个小词汇量限制，从而模拟双重构词，并形成与自然衍生形态学类比的新设置。这一新设置有助于与自然语言交流方案进行有意义的比较。我们开发了新的度量，并探索了由实际的衍生形态学属性所激发的游戏变体，包括句法组合性和融合性。", "result": "实验发现，模拟的音系限制鼓励了句法组合形态，而产生的语言也呈现了自然语言将语法属性融合的趋势。", "conclusion": "该研究展示了在特定约束条件下，通过模拟自然语言的特性和使用新度量方法可以带来对新兴通信模式的新见解，特别是其如何反映自然语言中语法属性的融合趋势。"}}
{"id": "2508.05755", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05755", "abs": "https://arxiv.org/abs/2508.05755", "authors": ["Agnieszka Polowczyk", "Alicja Polowczyk", "Dawid Malarz", "Artur Kasymov", "Marcin Mazur", "Jacek Tabor", "Przemysław Spurek"], "title": "UnGuide: Learning to Forget with LoRA-Guided Diffusion Models", "comment": null, "summary": "Recent advances in large-scale text-to-image diffusion models have heightened\nconcerns about their potential misuse, especially in generating harmful or\nmisleading content. This underscores the urgent need for effective machine\nunlearning, i.e., removing specific knowledge or concepts from pretrained\nmodels without compromising overall performance. One possible approach is\nLow-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models\nfor targeted unlearning. However, LoRA often inadvertently alters unrelated\ncontent, leading to diminished image fidelity and realism. To address this\nlimitation, we introduce UnGuide -- a novel approach which incorporates\nUnGuidance, a dynamic inference mechanism that leverages Classifier-Free\nGuidance (CFG) to exert precise control over the unlearning process. UnGuide\nmodulates the guidance scale based on the stability of a few first steps of\ndenoising processes, enabling selective unlearning by LoRA adapter. For prompts\ncontaining the erased concept, the LoRA module predominates and is\ncounterbalanced by the base model; for unrelated prompts, the base model\ngoverns generation, preserving content fidelity. Empirical results demonstrate\nthat UnGuide achieves controlled concept removal and retains the expressive\npower of diffusion models, outperforming existing LoRA-based methods in both\nobject erasure and explicit content removal tasks.", "AI": {"tldr": "本研究提出了UnGuide方法，以解决利用LoRA进行机器无学习时出现的与目标无关内容质量下降的问题。该方法通过动态调节指导尺度，实现了选择性无学习。", "motivation": "大型文本到图像扩散模型的最新进展提高了对这些模型可能被误用的关注，特别是在生成有害或误导性内容方面。这凸显了有效机器无学习的迫切需求，即从预训练模型中移除特定知识或概念而不影响整体性能。", "method": "UnGuide方法利用UnGuidance，这是一种动态推理机制，通过无分类器指导（CFG）来精确控制无学习过程。UnGuide可以根据去噪过程的前几个步骤的稳定性来调节指导尺度，实现通过LoRA适配器的选择性无学习。", "result": "实验结果表明，UnGuide能够实现受控的概念删除，并保持扩散模型的表达能力，其在物体擦除和显式内容删除任务中优于现有的基于LoRA的方法。", "conclusion": "UnGuide方法在保持扩散模型表达能力的同时，能够实现对特定概念的有效删除，并优于现有的LoRA方法。"}}
{"id": "2508.05880", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05880", "abs": "https://arxiv.org/abs/2508.05880", "authors": ["Sree Bhattacharyya", "Lucas Craig", "Tharun Dilliraj", "Jia Li", "James Z. Wang"], "title": "Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models", "comment": null, "summary": "Affective Computing has been established as a crucial field of inquiry to\nadvance the holistic development of Artificial Intelligence (AI) systems.\nFoundation models -- especially Large Language Models (LLMs) -- have been\nevaluated, trained, or instruction-tuned in several past works, to become\nbetter predictors or generators of emotion. Most of these studies, however,\napproach emotion-related tasks in a supervised manner, assessing or training\nthe capabilities of LLMs using discrete emotion labels associated with stimuli\n(e.g., text, images, video, audio). Evaluation studies, in particular, have\noften been limited to standard and superficial emotion-related tasks, such as\nthe recognition of evoked or expressed emotions. In this paper, we move beyond\nsurface-level emotion tasks to investigate how LLMs reason about emotions\nthrough cognitive dimensions. Drawing from cognitive appraisal theory, we\nexamine whether LLMs produce coherent and plausible cognitive reasoning when\nreasoning about emotionally charged stimuli. We introduce a large-scale\nbenchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal\ncognitive structures implicitly used by LLMs for emotional reasoning. Through a\nplethora of evaluation experiments and analysis, we seek to answer: (a) Are\nmodels more likely to implicitly rely on specific cognitive appraisal\ndimensions?, (b) What cognitive dimensions are important for characterizing\nspecific emotions?, and, (c) Can the internal representations of different\nemotion categories in LLMs be interpreted through cognitive appraisal\ndimensions? Our results and analyses reveal diverse reasoning patterns across\ndifferent LLMs. Our benchmark and code will be made publicly available.", "AI": {"tldr": "本文通过认知维度调查大型语言模型(LLMs)对情绪的推理能力，并引入了一个名为CoRE的大规模基准测试，以评估LLMs在情绪推理中隐含使用的认知结构。", "motivation": "大多数关于情绪预测或生成的研究采用监督方法进行情绪相关的任务，该论文意图超越表面情绪任务，通过认知维度来研究LMMs对情绪的推理能力。", "method": "基于认知评价理论，通过评估实验分析LLMs在处理情绪相关刺激时产生的认知推理是否一致和可信。引入了一个名为CoRE的大规模基准测试。", "result": "研究揭示了不同LLMs在情绪推理中表现出多样化的推理模式。", "conclusion": "通过大量的评价实验和分析，论文探讨了模型是否更倾向于依赖特定的认知评价维度进行情绪推理以及这些认知维度对于描述不同情绪的重要性，并尝试解释不同情绪类别在LMMs中的内部表示。"}}
{"id": "2508.05769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05769", "abs": "https://arxiv.org/abs/2508.05769", "authors": ["Seyed Hadi Seyed", "Ayberk Cansever", "David Hart"], "title": "Improving Masked Style Transfer using Blended Partial Convolution", "comment": null, "summary": "Artistic style transfer has long been possible with the advancements of\nconvolution- and transformer-based neural networks. Most algorithms apply the\nartistic style transfer to the whole image, but individual users may only need\nto apply a style transfer to a specific region in the image. The standard\npractice is to simply mask the image after the stylization. This work shows\nthat this approach tends to improperly capture the style features in the region\nof interest. We propose a partial-convolution-based style transfer network that\naccurately applies the style features exclusively to the region of interest.\nAdditionally, we present network-internal blending techniques that account for\nimperfections in the region selection. We show that this visually and\nquantitatively improves stylization using examples from the SA-1B dataset. Code\nis publicly available at https://github.com/davidmhart/StyleTransferMasked.", "AI": {"tldr": "本文提出了一种基于部分卷积的风格迁移网络，可以准确地对图像中的指定区域应用艺术风格转换，提高了风格化的效果。", "motivation": "大多数算法将艺术风格迁移应用于整个图像，但个别用户可能只需对图像中的特定区域应用风格转换。标准的做法是在风格化之后简单地屏蔽图像。这种方法往往不能很好地捕获感兴趣区域的风格特征。", "method": "提出了一种基于部分卷积的风格迁移网络，该网络可以准确地将风格特征仅应用于图像中的感兴趣区域。此外，还介绍了网络内部的融合技术，以解决区域选择中的不完美问题。", "result": "实验表明，这种技术在视觉和定量上都改进了样式转换，使用了SA-1B数据集中的示例来展示。", "conclusion": "研究表明，该方法在进行特定区域的艺术风格迁移时有效，提高了视觉和定量的满意度。代码可在https://github.com/davidmhart/StyleTransferMasked公开获取。"}}
{"id": "2508.05909", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05909", "abs": "https://arxiv.org/abs/2508.05909", "authors": ["Zhanghao Hu", "Qinglin Zhu", "Siya Qi", "Yulan He", "Hanqi Yan", "Lin Gui"], "title": "Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation", "comment": null, "summary": "Large Language Models (LLMs) have shown improved generation performance\nthrough retrieval-augmented generation (RAG) following the retriever-reader\nparadigm, which supplements model inputs with externally retrieved knowledge.\nHowever, prior work often evaluates RAG holistically, assessing the retriever\nand reader jointly, making it difficult to isolate the true contribution of\nretrieval, particularly given the prompt sensitivity of LLMs used as readers.\nWe introduce Spectrum Projection Score (SPS), a lightweight, supervision-free\nmetric that allows the reader to gauge the semantic alignment of a retrieved\nsummary with its hidden representation by comparing the area formed by\ngenerated tokens from the summary, and the principal directions of subspace in\nthe reader and to measure the relevance. Building on SPS we present xCompress,\nan inference time controller framework that dynamically samples, ranks, and\ncompresses retrieval summary candidates. Extensive experiments on five QA\nbenchmarks with four open source LLMs show that SPS not only enhances\nperformance across a range of tasks but also provides a principled perspective\non the interaction between retrieval and generation.", "AI": {"tldr": "研究介绍了SPS和xCompress，以改进大型语言模型使用检索增强生成的效果，并通过大量实验展示了它们在多个问答基准和开放源代码大语言模型中的有效性。", "motivation": "先前的工作通常通过评估检索器和阅读器的综合效果来评估RAG，这使得很难孤立地评估检索的真正贡献，特别是考虑到作为阅读器的大型语言模型对提示的敏感性。", "method": "我们介绍了Spectrum Projection Score (SPS)，这是一种轻量级且无需监督的度量方法，允许读取器通过比较摘要生成的标记形成的区域与读者子空间的主要方向来评估检索摘要的语义一致性。基于SPS，我们提出了xCompress，这是一个可以在推理时动态采样、排序和压缩检索摘要候选的框架。", "result": "广泛实验表明，在五个问答基准和四种开源大语言模型上的应用显示，SPS不仅在一系列任务中提升了性能，而且还提供了一个有关检索和生成之间交互的结构化视角。", "conclusion": "SPS和xCompress框架能够提升基于检索增强生成的大型语言模型在各种任务中的表现，并提供了一个系统化的方法来理解检索和生成之间的相互作用。"}}
{"id": "2508.05772", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05772", "abs": "https://arxiv.org/abs/2508.05772", "authors": ["Can Zhao", "Pengfei Guo", "Dong Yang", "Yucheng Tang", "Yufan He", "Benjamin Simon", "Mason Belue", "Stephanie Harmon", "Baris Turkbey", "Daguang Xu"], "title": "MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss", "comment": null, "summary": "Medical image synthesis is an important topic for both clinical and research\napplications. Recently, diffusion models have become a leading approach in this\narea. Despite their strengths, many existing methods struggle with (1) limited\ngeneralizability that only work for specific body regions or voxel spacings,\n(2) slow inference, which is a common issue for diffusion models, and (3) weak\nalignment with input conditions, which is a critical issue for medical imaging.\nMAISI, a previously proposed framework, addresses generalizability issues but\nstill suffers from slow inference and limited condition consistency. In this\nwork, we present MAISI-v2, the first accelerated 3D medical image synthesis\nframework that integrates rectified flow to enable fast and high quality\ngeneration. To further enhance condition fidelity, we introduce a novel\nregion-specific contrastive loss to enhance the sensitivity to region of\ninterest. Our experiments show that MAISI-v2 can achieve SOTA image quality\nwith $33 \\times$ acceleration for latent diffusion model. We also conducted a\ndownstream segmentation experiment to show that the synthetic images can be\nused for data augmentation. We release our code, training details, model\nweights, and a GUI demo to facilitate reproducibility and promote further\ndevelopment within the community.", "AI": {"tldr": "MAISI-v2 是首个加速的3D医学图像合成框架，它结合了校正流来实现快速和高质量的生成，并通过引入区域特定对比损失来进一步增强条件保真度。实验表明，与之前的模型相比，它在潜扩散模型方面实现了33倍的加速，并且达到了当前最佳的图像质量。", "motivation": "解决现有方法中存在的局限性，包括生成能力的局限性、推理速度慢以及与输入条件的对齐性较差这些问题。", "method": "MAISI-v2 采用校正流加速生成过程，并使用区域特定对比损失来增强对感兴趣区域的敏感性。", "result": "实验显示，MAISI-v2 在潜扩散模型中实现了33倍加速，达到了当前最佳的图像质量，并且能够用于数据增强的下游分割实验中。", "conclusion": "MAISI-v2 提供了快速高质量的3D医学图像生成，代码、训练细节、模型权重和GUI演示都已公开，促进了社区内的可重复性和进一步发展。"}}
{"id": "2508.05938", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; K.4"], "pdf": "https://arxiv.org/pdf/2508.05938", "abs": "https://arxiv.org/abs/2508.05938", "authors": ["Rafal Kocielnik", "Min Kim", "Penphob", "Boonyarungsrit", "Fereshteh Soltani", "Deshawn Sambrano", "Animashree Anandkumar", "R. Michael Alvarez"], "title": "Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale", "comment": "9 pages, 4 figures, 4 tables", "summary": "Detecting prosociality in text--communication intended to affirm, support, or\nimprove others' behavior--is a novel and increasingly important challenge for\ntrust and safety systems. Unlike toxic content detection, prosociality lacks\nwell-established definitions and labeled data, requiring new approaches to both\nannotation and deployment. We present a practical, three-stage pipeline that\nenables scalable, high-precision prosocial content classification while\nminimizing human labeling effort and inference costs. First, we identify the\nbest LLM-based labeling strategy using a small seed set of human-labeled\nexamples. We then introduce a human-AI refinement loop, where annotators review\nhigh-disagreement cases between GPT-4 and humans to iteratively clarify and\nexpand the task definition-a critical step for emerging annotation tasks like\nprosociality. This process results in improved label quality and definition\nalignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train\na two-stage inference system: a lightweight classifier handles high-confidence\npredictions, while only $\\sim$35\\% of ambiguous instances are escalated to\nGPT-4o. This architecture reduces inference costs by $\\sim$70% while achieving\nhigh precision ($\\sim$0.90). Our pipeline demonstrates how targeted human-AI\ninteraction, careful task formulation, and deployment-aware architecture design\ncan unlock scalable solutions for novel responsible AI tasks.", "AI": {"tldr": "本文提出了一种三阶段流水线，设计优化了亲社会内容分类，实现高精度分类和成本节省的一种AI解决方案。方法包括最佳标注策略确定、人工-人工智能相互调整以及合成大规模标签进行系统训练。", "motivation": "检测亲社会内容--旨在肯定、支持或改善他人行为的交流--是针对信任和安全系统的新兴且日益重要的挑战。这种方法相比于有毒内容检测，缺乏明确的定义和标注数据，因此需要新的注释和部署方法。", "method": "本文提出了一种实用的、三阶段的流水线设计，以实现大规模的高精度亲社会内容分类，同时最小化人工标注和推理成本：1. 使用少量的人工标注实例作为种子集，以确定最佳的基于LLM的标注策略；2. 采用人工-人工智能调整循环，在高分歧的案例中进行人工审核，以逐步细化任务定义；3. 合成10000个高质量标签，使用GPT-4训练了一个两阶段推理系统，其中轻量级分类器处理高置信度预测，只有35%的模棱两可的案例提交给GPT-4处理。", "result": "该流水线减少推理成本约70%，并实现了高精度约0.90，证明了这种方法在减少成本和提高亲社会内容检测效率方面的有效性。", "conclusion": "本文展示了针对亲社会内容检测这一新兴任务，有针对性的人工智能互动、精心的任务制作设计和部署友好架构如何达成可扩展的解决方案。"}}
{"id": "2508.05783", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05783", "abs": "https://arxiv.org/abs/2508.05783", "authors": ["Mengyu Li", "Guoyao Shen", "Chad W. Farris", "Xin Zhang"], "title": "Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks", "comment": "30 pages, 8 figures, 7 tables", "summary": "Machine learning using transformers has shown great potential in medical\nimaging, but its real-world applicability remains limited due to the scarcity\nof annotated data. In this study, we propose a practical framework for the\nfew-shot deployment of pretrained MRI transformers in diverse brain imaging\ntasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a\nlarge-scale, multi-cohort brain MRI dataset comprising over 31 million slices,\nwe obtain highly transferable latent representations that generalize well\nacross tasks and datasets. For high-level tasks such as classification, a\nfrozen MAE encoder combined with a lightweight linear head achieves\nstate-of-the-art accuracy in MRI sequence identification with minimal\nsupervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a\nhybrid architecture that fuses multiscale CNN features with pretrained MAE\nembeddings. This model consistently outperforms other strong baselines in both\nskull stripping and multi-class anatomical segmentation under data-limited\nconditions. With extensive quantitative and qualitative evaluations, our\nframework demonstrates efficiency, stability, and scalability, suggesting its\nsuitability for low-resource clinical environments and broader neuroimaging\napplications.", "AI": {"tldr": "本文提出了一种用于有限数据条件下在多样化的脑成像任务中部署预训练MRI变压器的实用框架。该框架通过MAE预训练策略获得了良好的泛化性能，展示了在资源有限的环境中应用的潜力。", "motivation": "尽管基于变压器的机器学习在医学成像领域显示出了巨大的潜力，但标注数据的稀缺限制了其在实际应用中的适用性。本研究旨在提出一个实用框架，用于在多样化的脑成像任务中有限数据条件下的预训练MRI变压器的部署。", "method": "通过在包含超过3100万个切片的大型多队列脑部MRI数据集上使用掩码自动编码器（MAE）预训练策略，我们获得了在任务和数据集之间具有良好泛化能力的高度迁移性潜在表示。对于像分类这样的高级任务，冻结的MAE编码器与轻量级线性头相结合，在基于最小监督的MRI序列识别中达到了最先进的准确性。对于像分割这样的低级任务，我们提出了一种融合多尺度CNN特征与预训练MAE嵌入的混合架构MAE-FUnet。该模型在数据有限的情况下，相对于其他强大的基线模型，在颅骨剥离和多类解剖分割中均表现出了更高的性能。", "result": "通过广泛的定量和定性评估，我们的框架展现了效率、稳定性和可扩展性，表明其适用于资源有限的临床环境和更广泛的神经成像应用。", "conclusion": "本研究证明了在有限的数据条件下，我们的方法能够有效且稳定地应用于各种神经成像任务中，并展示出了广泛的适用性，尤其是在资源有限的临床环境中。"}}
{"id": "2508.05987", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05987", "abs": "https://arxiv.org/abs/2508.05987", "authors": ["Chunyun Zhang", "Hongyan Zhao", "Chaoran Cui", "Qilong Song", "Zhiqing Lu", "Shuai Gong", "Kailin Liu"], "title": "Adversarial Topic-aware Prompt-tuning for Cross-topic Automated Essay Scoring", "comment": null, "summary": "Cross-topic automated essay scoring (AES) aims to develop a transferable\nmodel capable of effectively evaluating essays on a target topic. A significant\nchallenge in this domain arises from the inherent discrepancies between topics.\nWhile existing methods predominantly focus on extracting topic-shared features\nthrough distribution alignment of source and target topics, they often neglect\ntopic-specific features, limiting their ability to assess critical traits such\nas topic adherence. To address this limitation, we propose an Adversarial\nTOpic-aware Prompt-tuning (ATOP), a novel method that jointly learns\ntopic-shared and topic-specific features to improve cross-topic AES. ATOP\nachieves this by optimizing a learnable topic-aware prompt--comprising both\nshared and specific components--to elicit relevant knowledge from pre-trained\nlanguage models (PLMs). To enhance the robustness of topic-shared prompt\nlearning and mitigate feature scale sensitivity introduced by topic alignment,\nwe incorporate adversarial training within a unified regression and\nclassification framework. In addition, we employ a neighbor-based classifier to\nmodel the local structure of essay representations and generate pseudo-labels\nfor target-topic essays. These pseudo-labels are then used to guide the\nsupervised learning of topic-specific prompts tailored to the target topic.\nExtensive experiments on the publicly available ASAP++ dataset demonstrate that\nATOP significantly outperforms existing state-of-the-art methods in both\nholistic and multi-trait essay scoring. The implementation of our method is\npublicly available at: https://anonymous.4open.science/r/ATOP-A271.", "AI": {"tldr": "提出一种新型方法ATOP，通过优化主题感知提示来提高跨主题自动作文评分的性能。实验表明，该方法在ASAP++数据集上表现优异，超越了现有最先进方法。", "motivation": "面对跨主题自动作文评分中的主题差异挑战，大多数现有方法通过源主题和目标主题的分布对齐来提取共享特征，却忽视了特定主题特征，这限制了它们评估诸如主题一致性等关键特征的能力。为了解决这一限制，提出了ATOP方法。", "method": "ATOP方法通过优化可学习的主题感知提示，该提示包括共享和特定主题的组成部分，以从预训练语言模型中提取相关知识，从而解决现有方法忽视特定主题特征的问题。此外，为了提升共享提示学习的鲁棒性并减少因主题对齐引起的特征尺度敏感性，该方法引入对抗训练，并采用基于邻居的分类器生成伪标签来指导目标主题的特定提示监督学习。", "result": "ATOP方法在公共可用的ASAP++数据集上的广泛实验表明，该方法在整体和多特征文章评分中大幅超越现有最先进方法。", "conclusion": "实验结果证明了ATOP在跨主题自动作文评分中的有效性，展示了其比现有方法更优越的性能，特别是在整体和多特征作文评分方面的表现。"}}
{"id": "2508.05813", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05813", "abs": "https://arxiv.org/abs/2508.05813", "authors": ["Raphael Du Sablon", "David Hart"], "title": "Optimization-Free Style Transfer for 3D Gaussian Splats", "comment": null, "summary": "The task of style transfer for 3D Gaussian splats has been explored in many\nprevious works, but these require reconstructing or fine-tuning the splat while\nincorporating style information or optimizing a feature extraction network on\nthe splat representation. We propose a reconstruction- and optimization-free\napproach to stylizing 3D Gaussian splats. This is done by generating a graph\nstructure across the implicit surface of the splat representation. A\nfeed-forward, surface-based stylization method is then used and interpolated\nback to the individual splats in the scene. This allows for any style image and\n3D Gaussian splat to be used without any additional training or optimization.\nThis also allows for fast stylization of splats, achieving speeds under 2\nminutes even on consumer-grade hardware. We demonstrate the quality results\nthis approach achieves and compare to other 3D Gaussian splat style transfer\nmethods. Code is publicly available at\nhttps://github.com/davidmhart/FastSplatStyler.", "AI": {"tldr": "本研究提出了一种无需重建和优化的3D高斯splat样式转移方法，通过生成splat表示隐含表面的图结构，使用前馈表面样化方法并插值回场景中的个别splat，实现了快速样化，无需额外训练或优化，可在普通硬件上达到2分钟内完成。", "motivation": "之前的不少工作都探索了3D高斯splat的样式转移任务，但这些方法都需要重建或调整splat以整合样式信息或优化特征提取网络。作者希望通过避免此过程来提供更高效的方法。", "method": "研究提出了一种无需重建和优化的3D高斯splat样式转移方法。首先生成splat表示隐含表面的图结构，然后使用前馈表面样化方法，并将样式化结果插值回至场景中的个别splat。", "result": "该方法允许使用任何样式图像和3D高斯splat而不需额外训练或优化。研究展示了这种样式转移方法的质量，并与其他方法进行了比较。", "conclusion": "实验表明，该方法在普通硬件上可以实现高质量和平面化，速度可快达2分钟内完成。结果证实了研究提出的无重建与优化样化方法的效率与实用性。"}}
{"id": "2508.06016", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06016", "abs": "https://arxiv.org/abs/2508.06016", "authors": ["Sagar Gandhi", "Vishal Gandhi"], "title": "Crisp Attention: Regularizing Transformers via Structured Sparsity", "comment": null, "summary": "The quadratic computational cost of the self-attention mechanism is a primary\nchallenge in scaling Transformer models. While attention sparsity is widely\nstudied as a technique to improve computational efficiency, it is almost\nuniversally assumed to come at the cost of model accuracy. In this paper, we\nreport a surprising counter-example to this common wisdom. By introducing\nstructured, post-hoc sparsity to the attention mechanism of a DistilBERT model\nduring fine-tuning on the SST-2 sentiment analysis task, we find that model\naccuracy improves significantly. Our model with 80\\% attention sparsity\nachieves a validation accuracy of 91.59\\%, a 0.97\\% absolute improvement over\nthe dense baseline. We hypothesize that this phenomenon is due to sparsity\nacting as a powerful implicit regularizer, preventing the model from\noverfitting by forcing it to make predictions with a more constrained and\nrobust set of features. Our work recasts attention sparsity not just as a tool\nfor computational efficiency, but as a potential method for improving the\ngeneralization and performance of Transformer models.", "AI": {"tldr": "研究发现，结构化后处理注意力稀疏性不仅提高了计算效率，还在SST-2情绪分析任务中提升了模型的准确性，挑战了普遍认为稀疏化会降低模型准确性的观点。", "motivation": "本文旨在探索一种新的方法，以解决自注意力机制带来的二次复杂度计算问题，并且挑战稀疏性必须以牺牲模型准确性为代价的普遍假设。", "method": "本研究通过在DistilBERT模型中引入结构化后处理稀疏性，尤其是在SST-2情绪分析任务的微调过程中，验证了稀疏性对模型准确性的提升作用。", "result": "实验结果显示，随着80%注意力稀疏性的加入，模型在验证数据集上的准确率达到了91.59%，相比于密集基线模型，绝对准确率提高了0.97%。", "conclusion": "研究认为稀疏性可以作为一种强大的隐式正则化器，通过限制模型使用更受约束且稳健的一组特征进行预测，从而防止过拟合，同时将注意力稀疏性重新定位为一种不仅有助于计算效率提升，而且能改善Transformer模型泛化性能和表现的方法。"}}
{"id": "2508.05819", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05819", "abs": "https://arxiv.org/abs/2508.05819", "authors": ["Jong-Ik Park", "Carlee Joe-Wong", "Gary K. Fedder"], "title": "MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown Camera Poses", "comment": null, "summary": "Neural Radiance Fields (NeRF) methods excel at 3D reconstruction from\nmultiple 2D images, even those taken with unknown camera poses. However, they\nstill miss the fine-detailed structures that matter in industrial inspection,\ne.g., detecting sub-micron defects on a production line or analyzing chips with\nScanning Electron Microscopy (SEM). In these scenarios, the sensor resolution\nis fixed and compute budgets are tight, so the only way to expose fine\nstructure is to add zoom-in images; yet, this breaks the multi-view consistency\nthat pose-free NeRF training relies on. We propose Multi-Zoom Enhanced NeRF\n(MZEN), the first NeRF framework that natively handles multi-zoom image sets.\nMZEN (i) augments the pin-hole camera model with an explicit, learnable zoom\nscalar that scales the focal length, and (ii) introduces a novel pose strategy:\nwide-field images are solved first to establish a global metric frame, and\nzoom-in images are then pose-primed to the nearest wide-field counterpart via a\nzoom-consistent crop-and-match procedure before joint refinement. Across eight\nforward-facing scenes$\\unicode{x2013}$synthetic TCAD models, real SEM of\nmicro-structures, and BLEFF objects$\\unicode{x2013}$MZEN consistently\noutperforms pose-free baselines and even high-resolution variants, boosting\nPSNR by up to $28 \\%$, SSIM by $10 \\%$, and reducing LPIPS by up to $222 \\%$.\nMZEN, therefore, extends NeRF to real-world factory settings, preserving global\naccuracy while capturing the micron-level details essential for industrial\ninspection.", "AI": {"tldr": "The paper presents MZEN, a method that enhances NeRF for capturing fine-detailed structures in industrial inspection by handling multi-zoom image sets, improving reconstruction quality by up to 28% in PSNR and reducing perceptual difference by 222%.", "motivation": "Standard NeRF methods struggle to capture fine-detailed structures important in industrial settings due to the incompatibility with multi-zoom images, which breaks multi-view consistency. This motivates the development of MZEN to handle such scenarios.", "method": "The paper introduces Multi-Zoom Enhanced NeRF (MZEN), which handles multi-zoom image sets by adding a learnable zoom scalar to the pin-hole camera model and employing a pose strategy where wide-field images establish a global metric frame, and zoom-in images are aligned through a crop-and-match procedure.", "result": "MZEN outperforms pose-free baselines and high-resolution variants across eight forward-facing scenes, improving PSNR by up to 28%, SSIM by 10%, and reducing LPIPS by up to 222%.", "conclusion": "MZEN effectively extends the capabilities of NeRF to handle the strict requirements of industrial inspections, preserving global accuracy while improving the capture of micron-level details."}}
{"id": "2508.06026", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06026", "abs": "https://arxiv.org/abs/2508.06026", "authors": ["Yidong Wang", "Xin Wang", "Cunxiang Wang", "Junfeng Fang", "Qiufeng Wang", "Jianing Chu", "Xuran Meng", "Shuxun Yang", "Libo Qin", "Yue Zhang", "Wei Ye", "Shikun Zhang"], "title": "Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future", "comment": "12 pages, 5 figures", "summary": "Self-Rewarding Language Models propose an architecture in which the Large\nLanguage Models(LLMs) both generates responses and evaluates its own outputs\nvia LLM-as-a-Judge prompting, dynamically improving its generative capabilities\nthrough iterative Direct Preference Optimization (DPO). However, our analysis\nreveals a critical limitation in existing Self-Rewarding paradigms: the\nsynchronized improvement of chosen and rejected responses progressively narrows\nthe representational difference between contrasting samples, undermining\neffective preference learning. We propose \\textbf{Temporal Self-Rewarding\nLanguage Models} that strategically coordinate past, present, and future model\ngenerations to sustain learning signals. Our dual-phase framework introduces:\n(1) \\textit{Anchored Rejection} - fixing rejected responses using the past\ninitial model's outputs and (2) \\textit{Future-Guided Chosen} - dynamically\ncurating chosen samples using next-generation model predictions. Extensive\nexperiments across three model families (Llama, Qwen, Mistral) and different\nmodel sizes (Llama3B/8B/70B) demonstrate significant improvements when trained\nwith our method compared to Self-Rewarding using same computation resources.\nFor example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our\nmethod, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our\nmethod also demonstrates superior out-of-distribution generalization across\nmathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code\ngeneration (HumanEval) tasks, even though we do not specifically collect such\ntraining data.", "AI": {"tldr": "Introduces Temporal Self-Rewarding Language Models to address limitations in self-rewarding architectures, demonstrating significant performance improvements and better generalization capabilities.", "motivation": "To address the limitation in existing Self-Rewarding paradigms where the synchronization of improvement undermines effective preference learning.", "method": "Temporal Self-Rewarding Language Models that strategically coordinate past, present, and future model generations to sustain learning signals through two phases: Anchored Rejection and Future-Guided Chosen.", "result": "Significant improvements demonstrated in win rates and out-of-distribution generalization across various tasks compared to Self-Rewarding baselines.", "conclusion": "The proposed method enhances LLM generative capabilities, showcasing superior performance and generalization without requiring task-specific training data."}}
{"id": "2508.05829", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05829", "abs": "https://arxiv.org/abs/2508.05829", "authors": ["Guoping Xu", "Hua-Chieh Shao", "You Zhang"], "title": "TSMS-SAM2: Multi-scale Temporal Sampling Augmentation and Memory-Splitting Pruning for Promptable Video Object Segmentation and Tracking in Surgical Scenarios", "comment": "23 pages, 5 figures", "summary": "Promptable video object segmentation and tracking (VOST) has seen significant\nadvances with the emergence of foundation models like Segment Anything Model 2\n(SAM2); however, their application in surgical video analysis remains\nchallenging due to complex motion dynamics and the redundancy of memory that\nimpedes effective learning. In this work, we propose TSMS-SAM2, a novel\nframework that enhances promptable VOST in surgical videos by addressing\nchallenges of rapid object motion and memory redundancy in SAM2. TSMS-SAM2\nintroduces two key strategies: multi-temporal-scale video sampling augmentation\nto improve robustness against motion variability, and a memory splitting and\npruning mechanism that organizes and filters past frame features for more\nefficient and accurate segmentation. Evaluated on EndoVis2017 and EndoVis2018\ndatasets, TSMS-SAM2 achieved the highest mean Dice scores of 95.24 and 86.73,\nrespectively, outperforming prior SAM-based and task-specific methods.\nExtensive ablation studies confirm the effectiveness of multiscale temporal\naugmentation and memory splitting, highlighting the framework's potential for\nrobust, efficient segmentation in complex surgical scenarios. Our source code\nwill be available at https://github.com/apple1986/TSMS-SAM2.", "AI": {"tldr": "本文提出了TSMS-SAM2框架以改善手术视频中的可提示VOST，并通过实验验证了其在EndoVis2017和EndoVis2018数据集上的优越性能。", "motivation": "本工作的动机在于解决SAM2在手术视频分析中的应用难题，特别是处理快速物体运动和内存冗余的问题。", "method": "TSMS-SAM2框架通过引入多时间尺度视频采样增强策略以提高对运动变化的鲁棒性，并采用记忆分割和修剪机制来组织和过滤过去的帧特征以提高效率和准确性。", "result": "在EndoVis2017和EndoVis2018数据集上，TSMS-SAM2分别实现了95.24和86.73的最高平均dice分，优于之前的SAM基于方法和任务特定的方法。", "conclusion": "TSMS-SAM2框架展示了在复杂手术场景中实现高效、准确分割的潜力。"}}
{"id": "2508.06030", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06030", "abs": "https://arxiv.org/abs/2508.06030", "authors": ["Kartik Sharma", "Yiqiao Jin", "Rakshit Trivedi", "Srijan Kumar"], "title": "Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings", "comment": null, "summary": "Large language models (LLMs) acquire knowledge across diverse domains such as\nscience, history, and geography encountered during generative pre-training.\nHowever, due to their stochasticity, it is difficult to predict what LLMs have\nacquired. Prior work has developed different ways to probe this knowledge by\ninvestigating the hidden representations, crafting specific task prompts,\ncurating representative samples, and estimating their uncertainty. However,\nthese methods require making forward passes through the underlying model to\nprobe the LLM's knowledge about a specific fact, making them computationally\nexpensive and time-consuming. To bridge this gap, we propose $\\textbf{PEEK}$ or\n$\\textbf{P}$roxy $\\textbf{E}$mbeddings to $\\textbf{E}$stimate\n$\\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models\nthat effectively encode factual knowledge as text or graphs as proxies for\nLLMs. First, we identify a training set of facts known by LLMs through various\nprobing strategies and then adapt embedding models to predict the LLM outputs\nwith a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived\ndatasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict\nLLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find\nthat sentence embedding models are more suitable than graph embeddings to\npredict LLM knowledge, shedding light on the underlying representation of the\nfactual landscape. Thus, we believe that knowledge-adapted embeddings can be\nused to identify knowledge gaps in LLMs at scale and can provide deeper\ninsights into LLMs' internal inductive bias. The code and data are made\navailable at https://github.com/claws-lab/peek.", "AI": {"tldr": "文章提出了一种方法（PEEK），利用预先训练的嵌入模型作为代理，以更高效的方式估计大型语言模型（LLMs）的知识，可以准确率高达90%，并指出句子嵌入模型更适用于此任务。", "motivation": "鉴于现有的探查LLMs知识的方法计算成本高且耗时，本文旨在提出一种更高效的方法来估计LLMs的知识。", "method": "提出了一种名为PEEK的方法，通过预先训练的嵌入模型来估计大型语言模型（LLMs）的知识。该方法包括识别LLMs已知的事实以及适应嵌入模型以预测LLMs的输出。", "result": "在3个维基百科衍生数据集上、4个LLMs和7个嵌入模型的综合评估证明了嵌入模型可以预测LLMs知识，准确率高达90%，并发现句子嵌入模型比图嵌入模型更适合预测LLMs的知识。", "conclusion": "实验表明，嵌入模型可以以高达90%的准确率预测LLMs的知识。此外，结果还表明，句子嵌入模型比图嵌入模型更适合预测LLMs的知识。因此，这些嵌入模型可以用作大规模识别LLMs知识差距的工具，并提供关于LLMs内在归纳偏好的更深入洞见。"}}
{"id": "2508.05851", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05851", "abs": "https://arxiv.org/abs/2508.05851", "authors": ["Ka-Wai Yung", "Felix J. S. Bragman", "Jialang Xu", "Imanol Luengo", "Danail Stoyanov", "Evangelos B. Mazomenos"], "title": "Temporal Cluster Assignment for Efficient Real-Time Video Segmentation", "comment": null, "summary": "Vision Transformers have substantially advanced the capabilities of\nsegmentation models across both image and video domains. Among them, the Swin\nTransformer stands out for its ability to capture hierarchical, multi-scale\nrepresentations, making it a popular backbone for segmentation in videos.\nHowever, despite its window-attention scheme, it still incurs a high\ncomputational cost, especially in larger variants commonly used for dense\nprediction in videos. This remains a major bottleneck for real-time,\nresource-constrained applications. Whilst token reduction methods have been\nproposed to alleviate this, the window-based attention mechanism of Swin\nrequires a fixed number of tokens per window, limiting the applicability of\nconventional pruning techniques. Meanwhile, training-free token clustering\napproaches have shown promise in image segmentation while maintaining window\nconsistency. Nevertheless, they fail to exploit temporal redundancy, missing a\nkey opportunity to further optimize video segmentation performance. We\nintroduce Temporal Cluster Assignment (TCA), a lightweight and effective,\nfine-tuning-free strategy that enhances token clustering by leveraging temporal\ncoherence across frames. Instead of indiscriminately dropping redundant tokens,\nTCA refines token clusters using temporal correlations, thereby retaining\nfine-grained details while significantly reducing computation. Extensive\nevaluations on YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and a private surgical\nvideo dataset show that TCA consistently boosts the accuracy-speed trade-off of\nexisting clustering-based methods. Our results demonstrate that TCA generalizes\ncompetently across both natural and domain-specific videos.", "AI": {"tldr": "介绍了TCA方法，能够在减少计算量的同时提升视频分割的准确性，适用于自然和特定领域的视频。", "motivation": "旨在解决视频分割中的计算成本高问题，现有方法在视频扩展上遇到瓶颈，无法利用时间冗余进一步优化视频分割效果。", "method": "引入了Temporal Cluster Assignment (TCA)，一种轻量级且有效的策略，无需微调即可通过利用帧间的时序相干性来改进基于聚类的方法，从而在减少计算量的同时保留细粒度细节。", "result": "在YouTube-VIS 2019，YouTube-VIS 2021，OVIS和私人外科视频数据集上的评估表明，TCA可以显著提高现有基于聚类方法的准确性和速度之间的平衡。", "conclusion": "TCA展示了在不同视频数据集上的强大泛化能力，证明其在视频分割任务中具有较高效率和准确性。"}}
{"id": "2508.06046", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06046", "abs": "https://arxiv.org/abs/2508.06046", "authors": ["Xinda Wang", "Zhengxu Hou", "Yangshijie Zhang", "Bingren Yan", "Zhibo Yang", "Xingsheng Zhang", "Luxi Xing", "Qiang Zhou", "Chen Zhang"], "title": "EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation", "comment": null, "summary": "Although the effectiveness of Large Language Models (LLMs) as judges\n(LLM-as-a-judge) has been validated, their performance remains limited in\nopen-ended tasks, particularly in story evaluation. Accurate story evaluation\nis crucial not only for assisting human quality judgment but also for providing\nkey signals to guide story generation. However, existing methods face a\ndilemma: prompt engineering for closed-source models suffers from poor\nadaptability, while fine-tuning approaches for open-source models lack the\nrigorous reasoning capabilities essential for story evaluation. To address\nthis, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework.\nGrounded in pairwise comparison, the framework first self-synthesizes\nscore-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To\nensure data quality, these raw CoTs undergo a self-filtering process, utilizing\nmulti-agents to guarantee their logical rigor and robustness. Finally, the\nevaluator trained on the refined data is deployed as a reward model to guide\nthe story generation task. Experimental results demonstrate that our framework\nachieves state-of-the-art (SOTA) performance on three evaluation benchmarks\nincluding StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward\nmodel, it significantly enhances the quality of generated stories, thereby\nfully validating the superiority of our self-evolving approach.", "AI": {"tldr": "This paper addresses the challenge of accurate story evaluation by proposing EvolvR, a self-evolving adaptive framework, which demonstrates superior performance and enhances story generation quality.", "motivation": "To overcome the limitations of existing methods in story evaluation using LLMs, which either suffer from poor adaptability or lack rigorous reasoning capabilities.", "method": "Self-Evolving Pairwise Reasoning (EvolvR) framework, which involves self-synthesizing score-aligned Chain-of-Thought (CoT) data via a multi-persona strategy, followed by a self-filtering process using multi-agents, and then deploying the refined evaluator as a reward model.", "result": "The proposed framework achieves state-of-the-art performance on evaluation benchmarks including StoryER, HANNA, and OpenMEVA, and significantly enhances the quality of generated stories when used as a reward model.", "conclusion": "The self-evolving approach proposed in this paper, EvolvR, proves to be effective in story evaluation and significantly improves the quality of generated stories when used as a reward model."}}
{"id": "2508.05852", "categories": ["cs.CV", "I.5.4"], "pdf": "https://arxiv.org/pdf/2508.05852", "abs": "https://arxiv.org/abs/2508.05852", "authors": ["Kaiser Hamid", "Khandakar Ashrafi Akbar", "Nade Liang"], "title": "VISTA: Vision-Language Imitation of Situational Thinking and Attention for Human-Like Driver Focus in Dynamic Environments", "comment": null, "summary": "Driver visual attention prediction is a critical task in autonomous driving\nand human-computer interaction (HCI) research. Most prior studies focus on\nestimating attention allocation at a single moment in time, typically using\nstatic RGB images such as driving scene pictures. In this work, we propose a\nvision-language framework that models the changing landscape of drivers' gaze\nthrough natural language, using few-shot and zero-shot learning on single RGB\nimages. We curate and refine high-quality captions from the BDD-A dataset using\nhuman-in-the-loop feedback, then fine-tune LLaVA to align visual perception\nwith attention-centric scene understanding. Our approach integrates both\nlow-level cues and top-down context (e.g., route semantics, risk anticipation),\nenabling language-based descriptions of gaze behavior. We evaluate performance\nacross training regimes (few shot, and one-shot) and introduce domain-specific\nmetrics for semantic alignment and response diversity. Results show that our\nfine-tuned model outperforms general-purpose VLMs in attention shift detection\nand interpretability. To our knowledge, this is among the first attempts to\ngenerate driver visual attention allocation and shifting predictions in natural\nlanguage, offering a new direction for explainable AI in autonomous driving.\nOur approach provides a foundation for downstream tasks such as behavior\nforecasting, human-AI teaming, and multi-agent coordination.", "AI": {"tldr": "研究通过视觉-语言框架整合低级视觉信息和高层次上下文，基于少量样本和零样本学习，利用自然语言预测驾驶员的视觉注意力变化，优于现有技术。", "motivation": "大多数先前的研究仅关注于使用静态RGB图像在某一时刻评估注意力分配，而本研究旨在通过自然语言描述驾驶员视觉注意力的变化，提供了一个自动驾驶和人机交互领域的新方向，以实现可解释的人工智能。", "method": "本研究提出了一种基于视觉语言的框架，该框架使用少量样本和零样本学习在单一RGB图像上预测驾驶员视觉注意力的变化。通过人类参与的反馈，从BDD-A数据集中整理和优化了高质量的标注，并对LLaVA进行了微调，使其视觉感知与关注场景理解相一致。该方法整合了低级线索和高层次的上下文（如路径语义、风险预期），允许用语言描述注视行为。", "result": "在多种训练算法（少量样本和一次样本）评估中，我们的微调模型优于普通视觉语言模型，在注意力转移检测和可解释性方面表现出色。", "conclusion": "研究提出的方法为后续的行为预测、人机协作和多代理协调等任务奠定了基础，开创新的研究方向。"}}
{"id": "2508.06094", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06094", "abs": "https://arxiv.org/abs/2508.06094", "authors": ["Morris Alper", "Moran Yanuka", "Raja Giryes", "Gašper Beguš"], "title": "ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline", "comment": "Project page: https://conlangcrafter.github.io", "summary": "Constructed languages (conlangs) such as Esperanto and Quenya have played\ndiverse roles in art, philosophy, and international communication. Meanwhile,\nlarge-scale foundation models have revolutionized creative generation in text,\nimages, and beyond. In this work, we leverage modern LLMs as computational\ncreativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a\nmulti-hop pipeline that decomposes language design into modular stages --\nphonology, morphology, syntax, lexicon generation, and translation. At each\nstage, our method leverages LLMs' meta-linguistic reasoning capabilities,\ninjecting randomness to encourage diversity and leveraging self-refinement\nfeedback to encourage consistency in the emerging language description. We\nevaluate ConlangCrafter on metrics measuring coherence and typological\ndiversity, demonstrating its ability to produce coherent and varied conlangs\nwithout human linguistic expertise.", "AI": {"tldr": "This paper presents ConlangCrafter, a system using large language models to create constructed languages in a modular and consistent manner, highlighting the potential of AI in computational creativity.", "motivation": "The motivation behind this research is to explore the use of LLMs for computational creativity, specifically in the creation of constructed languages (conlangs) without requiring human linguistic expertise.", "method": "The paper introduces ConlangCrafter, a pipeline for creating constructed languages using modern large language models (LLMs). This multi-hop process breaks down language design into stages such as phonology, morphology, syntax, and lexicon generation. The method incorporates randomness for diversity and a feedback loop to ensure consistency.", "result": "ConlangCrafter was evaluated based on metrics related to coherence and typological diversity, showing that it can produce conlangs that are both coherent and varied.", "conclusion": "The paper concludes that modern LLMs can be effectively used as tools for the automated creation of new languages, demonstrating the potential of computational creativity in language design."}}
{"id": "2508.05857", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05857", "abs": "https://arxiv.org/abs/2508.05857", "authors": ["Qiaomu Miao", "Vivek Raju Golani", "Jingyi Xu", "Progga Paromita Dutta", "Minh Hoai", "Dimitris Samaras"], "title": "Multi-view Gaze Target Estimation", "comment": "Accepted to ICCV 2025", "summary": "This paper presents a method that utilizes multiple camera views for the gaze\ntarget estimation (GTE) task. The approach integrates information from\ndifferent camera views to improve accuracy and expand applicability, addressing\nlimitations in existing single-view methods that face challenges such as face\nocclusion, target ambiguity, and out-of-view targets. Our method processes a\npair of camera views as input, incorporating a Head Information Aggregation\n(HIA) module for leveraging head information from both views for more accurate\ngaze estimation, an Uncertainty-based Gaze Selection (UGS) for identifying the\nmost reliable gaze output, and an Epipolar-based Scene Attention (ESA) module\nfor cross-view background information sharing. This approach significantly\noutperforms single-view baselines, especially when the second camera provides a\nclear view of the person's face. Additionally, our method can estimate the gaze\ntarget in the first view using the image of the person in the second view only,\na capability not possessed by single-view GTE methods. Furthermore, the paper\nintroduces a multi-view dataset for developing and evaluating multi-view GTE\nmethods. Data and code are available at\nhttps://www3.cs.stonybrook.edu/~cvl/multiview_gte.html", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.06103", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.06103", "abs": "https://arxiv.org/abs/2508.06103", "authors": ["Mohamed Basem", "Islam Oshallah", "Ali Hamdi", "Ammar Mohammed"], "title": "Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs", "comment": "6 pages , 2 figures , Accepted in IMSA 2025,Egypt ,\n  https://imsa.msa.edu.eg/", "summary": "This paper presents two effective approaches for Extractive Question\nAnswering (QA) on the Quran. It addresses challenges related to complex\nlanguage, unique terminology, and deep meaning in the text. The second uses\nfew-shot prompting with instruction-tuned large language models such as Gemini\nand DeepSeek. A specialized Arabic prompt framework is developed for span\nextraction. A strong post-processing system integrates subword alignment,\noverlap suppression, and semantic filtering. This improves precision and\nreduces hallucinations. Evaluations show that large language models with Arabic\ninstructions outperform traditional fine-tuned models. The best configuration\nachieves a pAP10 score of 0.637. The results confirm that prompt-based\ninstruction tuning is effective for low-resource, semantically rich QA tasks.", "AI": {"tldr": "The paper introduces two methods for improving extractive QA on the Quran using instruction-tuned large language models, achieving a pAP10 score of 0.637.", "motivation": "The motivation is to tackle the challenges of complex language, unique terminology, and deep meaning in the Quranic text for extractive QA.", "method": "The paper employs a specialized Arabic prompt framework for span extraction, and a robust post-processing system for subword alignment, overlap suppression, and semantic filtering.", "result": "The evaluation shows that models with Arabic instructions perform better than fine-tuned models, with the highest pAP10 score of 0.637.", "conclusion": "The conclusion is that prompt-based instruction tuning can effectively enhance performance in extractive QA tasks for low-resource, semantically rich texts like the Quran."}}
{"id": "2508.05898", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05898", "abs": "https://arxiv.org/abs/2508.05898", "authors": ["Hamidreza Dastmalchi", "Aijun An", "Ali cheraghian"], "title": "ETTA: Efficient Test-Time Adaptation for Vision-Language Models through Dynamic Embedding Updates", "comment": "BMVC2025", "summary": "Pretrained vision-language models (VLMs) like CLIP show strong zero-shot\nperformance but struggle with generalization under distribution shifts.\nTest-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test\ndata in new domains. While some TTA methods rely on prompt-tuning,\ntraining-free cache-based approaches are preferred for efficiency. However,\ncurrent cache-based TTA models store only a limited set of high-confidence\nsamples, restricting the decision boundary to these samples and ignoring the\ninfluence of other incoming test data. To address this, we propose Efficient\nTest-Time Adaptation (ETTA), introducing a Recursive Updating module that\nintegrates all incoming test samples, progressively refining the decision\nboundary. This strategy mimics an unbounded cache, dynamically updating\ncontextual embeddings for improved accuracy with minimal memory and\ncomputational overhead. ETTA also includes an Adaptive Ensemble module to\nreduce prompt dependency in image-to-text scores by dynamically selecting\noptimal prompts for each class. Furthermore, ETTA adaptively combines scores\nfrom both modules based on confidence levels, leveraging their complementary\nstrengths. Extensive experiments on two benchmarks confirm that ETTA surpasses\nthe state-of-the-art TTA models in computational complexity and accuracy,\nsetting a new standard for effective, efficient test-time adaptation. The code\nhas been released at https://github.com/hamidreza-dastmalchi/ETTA.", "AI": {"tldr": "本文提出了一个高效的测试时适应方法（ETTA），通过递归更新所有测试样本并自适应地结合模块得分，来改进预训练的视觉语言模型在新环境下的决策边界，提高其准确性和适应性。实验结果表明，ETTA在计算复杂度和准确性上优于同类方法。", "motivation": "预训练的视觉语言模型（VLMs）在零样本情况下表现出色，但面对分布变化时表现不佳。为解决这一问题，本文旨在改进基于缓存的测试时适应方法，以提高适应性和准确性。", "method": "本文提出了一种高效的测试时适应方法（ETTA），通过递归更新模块整合所有测试样本，逐步细化决策边界，并采用自适应集成模块减少对提示的依赖，提高图像文本匹配的准确率。此外，ETTA还根据置信度水平自适应地结合两个模块的得分，充分利用它们的互补优势。", "result": "实验结果表明，ETTA在计算复杂度和准确性方面超越了现有的测试时适应模型，确立了新的效果和效率标准。", "conclusion": "本研究表明，通过引入递归更新模块和自适应集成模块，可以在保持低计算和内存开销的情况下提高预训练视觉语言模型在新环境中的性能。"}}
{"id": "2508.06105", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06105", "abs": "https://arxiv.org/abs/2508.06105", "authors": ["Shengyuan Chen", "Chuang Zhou", "Zheng Yuan", "Qinggang Zhang", "Zeyang Cui", "Hao Chen", "Yilin Xiao", "Jiannong Cao", "Xiao Huang"], "title": "You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures", "comment": null, "summary": "Large language models (LLMs) often suffer from hallucination, generating\nfactually incorrect statements when handling questions beyond their knowledge\nand perception. Retrieval-augmented generation (RAG) addresses this by\nretrieving query-relevant contexts from knowledge bases to support LLM\nreasoning. Recent advances leverage pre-constructed graphs to capture the\nrelational connections among distributed documents, showing remarkable\nperformance in complex tasks. However, existing Graph-based RAG (GraphRAG)\nmethods rely on a costly process to transform the corpus into a graph,\nintroducing overwhelming token cost and update latency. Moreover, real-world\nqueries vary in type and complexity, requiring different logic structures for\naccurate reasoning. The pre-built graph may not align with these required\nstructures, resulting in ineffective knowledge retrieval. To this end, we\npropose a \\textbf{\\underline{Logic}}-aware\n\\textbf{\\underline{R}}etrieval-\\textbf{\\underline{A}}ugmented\n\\textbf{\\underline{G}}eneration framework (\\textbf{LogicRAG}) that dynamically\nextracts reasoning structures at inference time to guide adaptive retrieval\nwithout any pre-built graph. LogicRAG begins by decomposing the input query\ninto a set of subproblems and constructing a directed acyclic graph (DAG) to\nmodel the logical dependencies among them. To support coherent multi-step\nreasoning, LogicRAG then linearizes the graph using topological sort, so that\nsubproblems can be addressed in a logically consistent order. Besides, LogicRAG\napplies graph pruning to reduce redundant retrieval and uses context pruning to\nfilter irrelevant context, significantly reducing the overall token cost.\nExtensive experiments demonstrate that LogicRAG achieves both superior\nperformance and efficiency compared to state-of-the-art baselines.", "AI": {"tldr": "提出了一种名为LogicRAG的框架，通过动态构建推理结构，改进了现有基于图的检索增强生成方法的效率和效果。", "motivation": "针对现有基于图的检索增强生成（GraphRAG）方法将语料库转换成图的过程成本高昂、引入了沉重的Token成本和更新延迟的问题，提出了新的LogicRAG框架。通过动态建模逻辑依赖关系，而不是依赖于预先构建的图，该框架旨在提高知识检索的有效性和效率。", "method": "提出了一种名为LogicRAG的框架，该框架在推理时动态提取推理结构，指导自适应检索，无需预先构建图。LogicRAG首先将输入查询分解成一组子问题，并构建有向无环图（DAG）来建模它们之间的逻辑依赖关系。为了支持连贯的多步推理，LogicRAG接着使用拓扑排序线性化图，使得子问题可以在逻辑一致的顺序中得到解决。此外，LogicRAG通过图修剪减少冗余检索，并通过上下文修剪过滤无关上下文，显著减少总体Token成本。", "result": "通过广泛实验表明，LogicRAG在性能和效率方面超过了现有的先进基线。", "conclusion": "LogicRAG框架通过动态结构提取和自适应检索提供了优于现有GraphRAG方法的性能和效率。"}}
{"id": "2508.05899", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.05899", "abs": "https://arxiv.org/abs/2508.05899", "authors": ["Zixuan Bian", "Ruohan Ren", "Yue Yang", "Chris Callison-Burch"], "title": "HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing", "comment": null, "summary": "3D scene generation plays a crucial role in gaming, artistic creation,\nvirtual reality and many other domains. However, current 3D scene design still\nrelies heavily on extensive manual effort from creators, and existing automated\nmethods struggle to generate open-domain scenes or support flexible editing. As\na result, generating 3D worlds directly from text has garnered increasing\nattention. In this paper, we introduce HOLODECK 2.0, an advanced\nvision-language-guided framework for 3D world generation with support for\ninteractive scene editing based on human feedback. HOLODECK 2.0 can generate\ndiverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and\ncyberpunk styles) that exhibit high semantic fidelity to fine-grained input\ndescriptions, suitable for both indoor and open-domain environments. HOLODECK\n2.0 leverages vision-language models (VLMs) to identify and parse the objects\nrequired in a scene and generates corresponding high-quality assets via\nstate-of-the-art 3D generative models. It then iteratively applies spatial\nconstraints derived from the VLMs to achieve semantically coherent and\nphysically plausible layouts. Human evaluations and CLIP-based assessments\ndemonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely\naligned with detailed textual descriptions, consistently outperforming\nbaselines across indoor and open-domain scenarios. Additionally, we provide\nediting capabilities that flexibly adapt to human feedback, supporting layout\nrefinement and style-consistent object edits. Finally, we present a practical\napplication of HOLODECK 2.0 in procedural game modeling, generating visually\nrich and immersive environments, potentially boosting efficiency.", "AI": {"tldr": "HOLODECK 2.0通过先进的视觉语言模型和3D生成模型灵活生成高保真3D场景，支持基于文本描述的交互式编辑，适用于游戏等领域。", "motivation": "HOLODECK 2.0旨在解决现有3D场景设计过于依赖手工操作的问题，提供一种能从文本直接生成多样化和风格丰富的3D场景的先进框架。", "method": "HOLODECK 2.0利用视觉语言模型（VLMs）解析所需物体，并使用先进的3D生成模型创建高质量资产。然后迭代地应用来自VLMs的空间约束以获得语义连贯和物理上可信的布局。", "result": "人类评估和CLIP评分显示，HOLODECK 2.0能有效生成与详细文本描述一致的高质量场景，并在室内和开放领域场景中均优于基线模型。", "conclusion": "HOLODECK 2.0不仅能够在多个风格（如现实、卡通、动漫和赛博朋克）中生成高保真的3D场景，还具备灵活适应人类反馈的编辑能力，在程序化游戏建模中的实际应用也预示着能极大地提升效率。"}}
{"id": "2508.06124", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06124", "abs": "https://arxiv.org/abs/2508.06124", "authors": ["Sayantan Adak", "Pratyush Chatterjee", "Somnath Banerjee", "Rima Hazra", "Somak Aditya", "Animesh Mukherjee"], "title": "AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models", "comment": null, "summary": "Present day LLMs face the challenge of managing affordance-based safety\nrisks-situations where outputs inadvertently facilitate harmful actions due to\noverlooked logical implications. Traditional safety solutions, such as scalar\noutcome-based reward models, parameter tuning, or heuristic decoding\nstrategies, lack the granularity and proactive nature needed to reliably detect\nand intervene during subtle yet crucial reasoning steps. Addressing this\nfundamental gap, we introduce AURA, an innovative, multi-layered framework\ncentered around Process Reward Models (PRMs), providing comprehensive, step\nlevel evaluations across logical coherence and safety-awareness. Our framework\nseamlessly combines introspective self-critique, fine-grained PRM assessments,\nand adaptive safety-aware decoding to dynamically and proactively guide models\ntoward safer reasoning trajectories. Empirical evidence clearly demonstrates\nthat this approach significantly surpasses existing methods, significantly\nimproving the logical integrity and affordance-sensitive safety of model\noutputs. This research represents a pivotal step toward safer, more\nresponsible, and contextually aware AI, setting a new benchmark for\nalignment-sensitive applications.", "AI": {"tldr": "研究提出AURA框架，专注于改善大型语言模型在安全性和逻辑连贯性方面的表现，通过多层设计实现了更高级别的安全保障。", "motivation": "鉴于现有的大模型（LLMs）在管理安全风险方面面临挑战，尤其是由于未被发现的逻辑影响而导致输出无意中促成有害行为的风险。传统安全解决方案（如标量结果奖励模型、参数调整或启发式解码策略）在检测和干预微妙但关键推理步骤时缺乏细节性和前瞻性。", "method": "介绍了一种名为AURA的多层框架，该框架围绕过程奖励模型（PRMs）构建，能够对逻辑连贯性和安全性进行细致的逐级评估。该框架将内省式自我批判、精细的PRM评估以及适应性的安全意识解码无缝结合，旨在动态地、前瞻性地引导模型走向更安全的推理路径。", "result": "实验证明，该方法在改善模型输出的逻辑完整性和对行为敏感度的安全性方面显著超越现有方法。", "conclusion": "这项研究代表了向更安全、更负责任和更有上下文意识的人工智能迈进的关键一步，为对齐敏感应用设定了新标准。"}}
{"id": "2508.05903", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05903", "abs": "https://arxiv.org/abs/2508.05903", "authors": ["Lang Nie", "Yuan Mei", "Kang Liao", "Yunqiu Xu", "Chunyu Lin", "Bin Xiao"], "title": "Robust Image Stitching with Optimal Plane", "comment": "* Equal contribution", "summary": "We present \\textit{RopStitch}, an unsupervised deep image stitching framework\nwith both robustness and naturalness. To ensure the robustness of\n\\textit{RopStitch}, we propose to incorporate the universal prior of content\nperception into the image stitching model by a dual-branch architecture. It\nseparately captures coarse and fine features and integrates them to achieve\nhighly generalizable performance across diverse unseen real-world scenes.\nConcretely, the dual-branch model consists of a pretrained branch to capture\nsemantically invariant representations and a learnable branch to extract\nfine-grained discriminative features, which are then merged into a whole by a\ncontrollable factor at the correlation level. Besides, considering that content\nalignment and structural preservation are often contradictory to each other, we\npropose a concept of virtual optimal planes to relieve this conflict. To this\nend, we model this problem as a process of estimating homography decomposition\ncoefficients, and design an iterative coefficient predictor and minimal\nsemantic distortion constraint to identify the optimal plane. This scheme is\nfinally incorporated into \\textit{RopStitch} by warping both views onto the\noptimal plane bidirectionally. Extensive experiments across various datasets\ndemonstrate that \\textit{RopStitch} significantly outperforms existing methods,\nparticularly in scene robustness and content naturalness. The code is available\nat {\\color{red}https://github.com/MmelodYy/RopStitch}.", "AI": {"tldr": "本文提出了一个名为\\textit{RopStitch}的无监督深度图像拼接框架，通过双分支架构结合内容和结构信息，解决了鲁棒性和自然性问题，展现出与现有方法相比的优势。", "motivation": "研究的初衷是开发一种无监督的深度图像拼接框架，既能增强鲁棒性，又能保持自然外观。", "method": "本文提出了一个名为\\textit{RopStitch}的无监督深度图像拼接框架，该框架具有鲁棒性和自然性。研究通过一个双分支架构将内容感知的通用先验融入图像拼接模型中，分别捕捉粗略和精细的特征并整合，以实现高度通用化的性能。具体而言，这个双分支模型由一个预训练分支组成，用于捕获语义不变表示，以及一个可学习分支，用来提取精细的辨别特征，并在关联层面通过一个可控因素合并这两个特征。进一步地，考虑到内容对齐和结构保持通常相互矛盾，提出了一种虚拟最优平面的概念来缓解这一矛盾。为此，研究将这个问题建模为一个估计仿射变换分解系数的过程，并设计了一个迭代系数预测器和最小语义扭曲约束条件，以识别最优平面。最终，这种方案以双向拍摄的方式被整合进入\\textit{RopStitch}框架中。", "result": "实验表明，提出的\\textit{RopStitch}方法在多个数据集上显著优于现有方法，特别是在场景鲁棒性和内容自然性上。", "conclusion": "\\textit{RopStitch}框架通过创新的方法实现了在图像拼接上更鲁棒和更自然的效果，相较于现有方法具有显著优势。"}}
{"id": "2508.06135", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06135", "abs": "https://arxiv.org/abs/2508.06135", "authors": ["Lingyuan Liu", "Mengxiang Zhang"], "title": "Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models", "comment": null, "summary": "Knowledge Distillation (KD) is a fundamental technique for compressing large\nlanguage models (LLMs) into compact, efficient student models. However,\nexisting white-box KD methods mainly focus on balancing ground truth and\nstudent-generated responses while overlooking two critical factors: training\ndata quality and student-model compatibility. To address these limitations, we\npropose Selective Reflection Distillation (SRD), a novel data curation\nframework that leverages reflections from student models to systematically\nrefine training data. SRD dynamically evaluates and selects prompt-response\npairs by comparing ground truth data with student model outputs, selectively\ncurating high-quality, student-compatible training instances through automated\nranking based on difficulty. Furthermore, after selecting the training data, a\ncurriculum scheduling strategy is employed to incrementally introduce these\ncurated subsets into the distillation process at fixed intervals. As a\nplug-and-play enhancement, SRD consistently improves distillation outcomes\nacross diverse white-box KD approaches and model architectures, as well as\ndecreases computational cost significantly during KD training. Experiments on a\nrange of language model benchmarks demonstrate SRD's consistent improvements in\ndistilled model performance, as well as a reduction in training runtime by up\nto 39%, under diverse KD methods and model families. Notably, SRD operates as a\nplug-and-play module, enhancing sample efficiency without modifying underlying\nKD algorithms. Our findings highlight that data quality and compatibility are\npivotal to effective and efficient distillation of LLMs, and SRD provides a\nprincipled framework to achieve both. This work advances the understanding of\ndata-centric factors in KD and offers practical insights for enhancing the\ncapability and efficiency of compressed LLMs.", "AI": {"tldr": "SRD 是一种通过学生模型反馈来优化训练数据的方法，从而提高知识蒸馏的效果和效率。实验表明，SRD 能在多种蒸馏方法和模型架构上提高性能，并显著减少训练时间。", "motivation": "现有的白盒知识蒸馏方法主要集中在平衡真实数据和学生生成数据，但忽略了训练数据质量和学生模型兼容性的关键因素。SRD旨在解决这些问题。", "method": "Structure", "result": "{\n  \"tldr\": \"SRD 提出了一种通过学生模型反馈来优化训练数据的方法，从而提高知识蒸馏的效果和效率。实验表明，SRD 能在多种蒸馏方法和模型架构上提高性能，并显著减少训练时间。\",\n  \"motivation\": \"现有的白盒知识蒸馏方法主要集中在平衡真实数据和学生生成数据，但忽略了训练数据质量和学生模型兼容性的关键因素。SRD旨在解决这些问题。\",\n  \"method\": \"SRD 采用反射机制，评估并选择优质训练数据，并通过课程调度策略逐步引入这些数据进行蒸馏。\",\n  \"result\": \"实验结果表明，SRD 能提高蒸馏模型的性能并减少训练时间可达39%，可以在不同蒸馏方法和模型上作为插件使用。\",\n  \"conclusion\": \"SRD 是一种高效的数据优化框架，能够提高语言模型蒸馏的质量和效率，无需修改原始知识蒸馏算法。研究结果表明数据质量与兼容性对于有效蒸馏至关重要。\"}\n}\n", "conclusion": "SRD 是一种高效的数据优化框架，能够提高语言模型蒸馏的质量和效率，无需修改原始知识蒸馏算法。研究结果表明数据质量与兼容性对于有效蒸馏至关重要。"}}
{"id": "2508.05907", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05907", "abs": "https://arxiv.org/abs/2508.05907", "authors": ["Ilya Chugunov"], "title": "Neural Field Representations of Mobile Computational Photography", "comment": "PhD thesis", "summary": "Over the past two decades, mobile imaging has experienced a profound\ntransformation, with cell phones rapidly eclipsing all other forms of digital\nphotography in popularity. Today's cell phones are equipped with a diverse\nrange of imaging technologies - laser depth ranging, multi-focal camera arrays,\nand split-pixel sensors - alongside non-visual sensors such as gyroscopes,\naccelerometers, and magnetometers. This, combined with on-board integrated\nchips for image and signal processing, makes the cell phone a versatile\npocket-sized computational imaging platform. Parallel to this, we have seen in\nrecent years how neural fields - small neural networks trained to map\ncontinuous spatial input coordinates to output signals - enable the\nreconstruction of complex scenes without explicit data representations such as\npixel arrays or point clouds. In this thesis, I demonstrate how carefully\ndesigned neural field models can compactly represent complex geometry and\nlighting effects. Enabling applications such as depth estimation, layer\nseparation, and image stitching directly from collected in-the-wild mobile\nphotography data. These methods outperform state-of-the-art approaches without\nrelying on complex pre-processing steps, labeled ground truth data, or machine\nlearning priors. Instead, they leverage well-constructed, self-regularized\nmodels that tackle challenging inverse problems through stochastic gradient\ndescent, fitting directly to raw measurements from a smartphone.", "AI": {"tldr": "This paper explores using neural field models for advanced imaging tasks on smartphones, outperforming existing methods without complex requirements.", "motivation": "The motivation behind the research is to leverage the advanced imaging technologies and sensors in modern smartphones to enhance computational imaging capabilities using minimal resources and without relying on traditional machine learning techniques.", "method": "The paper uses carefully designed neural field models to represent complex geometry and lighting effects. These models are used for applications like depth estimation, layer separation, and image stitching using data directly from mobile photography.", "result": "The methods proposed in the paper outperform state-of-the-art approaches without requiring complex preprocessing, labeled data, or machine learning priors.", "conclusion": "Well-constructed, self-regularized neural field models can effectively solve challenging inverse problems in mobile imaging by fitting directly to raw smartphone measurements, significantly improving the capabilities of mobile photography."}}
{"id": "2508.06149", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06149", "abs": "https://arxiv.org/abs/2508.06149", "authors": ["Gunhee Cho", "Yun-Gyung Cheong"], "title": "Scaling Personality Control in LLMs with Big Five Scaler Prompts", "comment": null, "summary": "We present Big5-Scaler, a prompt-based framework for conditioning large\nlanguage models (LLMs) with controllable Big Five personality traits. By\nembedding numeric trait values into natural language prompts, our method\nenables fine-grained personality control without additional training. We\nevaluate Big5-Scaler across trait expression, dialogue generation, and human\ntrait imitation tasks. Results show that it induces consistent and\ndistinguishable personality traits across models, with performance varying by\nprompt type and scale. Our analysis highlights the effectiveness of concise\nprompts and lower trait intensities, providing a efficient approach for\nbuilding personality-aware dialogue agents.", "AI": {"tldr": "Big5-Scaler框架通过自然语言提示实现了大型语言模型的性格控制，无需额外训练，展现了在性格表达、对话生成和人类性格模仿方面的有效性。", "motivation": "该研究旨在提供一种能够有效地让大型语言模型具备可控制性格特质的方法，以提高对话系统的多样性和自然性。", "method": "通过将数值性格特质嵌入到自然语言提示中，该研究提出了无需额外训练即可实现对大型语言模型的精细性格控制的框架。", "result": "该框架通过将数值性格特质嵌入到自然语言提示中，实现了在不进行额外训练的情况下对大型语言模型进行精细的性格控制。评估表明，它能够在不同模型中诱导出一致且可区分的性格特质，且表现因提示类型和尺度而异。研究表明，简洁的提示和较低的性格强度提供了构建有性格意识的对话代理的高效方法。", "conclusion": "Big5-Scaler框架为构建性格化的对话代理提供了一种高效且有效的方法，特别是在使用简洁的提示和较低的性格强度时。"}}
{"id": "2508.05922", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05922", "abs": "https://arxiv.org/abs/2508.05922", "authors": ["Sri Ramana Saketh Vasanthawada", "Pengkun Liu", "Pingbo Tang"], "title": "Enhancing Construction Site Analysis and Understanding with 3D Segmentation", "comment": null, "summary": "Monitoring construction progress is crucial yet resource-intensive, prompting\nthe exploration of computer-vision-based methodologies for enhanced efficiency\nand scalability. Traditional data acquisition methods, primarily focusing on\nindoor environments, falter in construction site's complex, cluttered, and\ndynamically changing conditions. This paper critically evaluates the\napplication of two advanced 3D segmentation methods, Segment Anything Model\n(SAM) and Mask3D, in challenging outdoor and indoor conditions. Trained\ninitially on indoor datasets, both models' adaptability and performance are\nassessed in real-world construction settings, highlighting the gap in current\nsegmentation approaches due to the absence of benchmarks for outdoor scenarios.\nThrough a comparative analysis, this study not only showcases the relative\neffectiveness of SAM and Mask3D but also addresses the critical need for\ntailored segmentation workflows capable of extracting actionable insights from\nconstruction site data, thereby advancing the field towards more automated and\nprecise monitoring techniques.", "AI": {"tldr": "论文评估了两种3D分割方法在建筑工地中的效果，并提出需要适合室外场景的分割方法来提高施工进度的自动监测技术。", "motivation": "鉴于传统的数据采集方法在复杂、杂乱和不断变化的建筑工地环境中表现不佳，该论文旨在探索基于计算机视觉的方法来提高监测施工进度的效率和适用性。", "method": "该论文探讨了两种先进的3D分割方法——Segment Anything Model (SAM) 和 Mask3D 在复杂且不断变化的建筑工地环境中的适用性和性能。", "result": "该研究通过比较分析，展示了SAM和Mask3D在建筑工地中的相对有效性，并指出了在室外场景中目前分割方法的不足。", "conclusion": "该论文强调了需要针对建筑工地数据定制分割工作流，以便提取有价值的信息，推动该领域向更自动化的方向发展。"}}
{"id": "2508.06155", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06155", "abs": "https://arxiv.org/abs/2508.06155", "authors": ["Renhan Zhang", "Lian Lian", "Zhen Qi", "Guiran Liu"], "title": "Semantic and Structural Analysis of Implicit Biases in Large Language Models: An Interpretable Approach", "comment": null, "summary": "This paper addresses the issue of implicit stereotypes that may arise during\nthe generation process of large language models. It proposes an interpretable\nbias detection method aimed at identifying hidden social biases in model\noutputs, especially those semantic tendencies that are not easily captured\nthrough explicit linguistic features. The method combines nested semantic\nrepresentation with a contextual contrast mechanism. It extracts latent bias\nfeatures from the vector space structure of model outputs. Using attention\nweight perturbation, it analyzes the model's sensitivity to specific social\nattribute terms, thereby revealing the semantic pathways through which bias is\nformed. To validate the effectiveness of the method, this study uses the\nStereoSet dataset, which covers multiple stereotype dimensions including\ngender, profession, religion, and race. The evaluation focuses on several key\nmetrics, such as bias detection accuracy, semantic consistency, and contextual\nsensitivity. Experimental results show that the proposed method achieves strong\ndetection performance across various dimensions. It can accurately identify\nbias differences between semantically similar texts while maintaining high\nsemantic alignment and output stability. The method also demonstrates high\ninterpretability in its structural design. It helps uncover the internal bias\nassociation mechanisms within language models. This provides a more transparent\nand reliable technical foundation for bias detection. The approach is suitable\nfor real-world applications where high trustworthiness of generated content is\nrequired.", "AI": {"tldr": "This paper introduces an interpretable bias detection method for identifying hidden social biases in large language model outputs. It uses nested semantic representation and contextual contrast to analyze model sensitivity towards social attributes, demonstrating strong detection performance validated with the StereoSet dataset.", "motivation": "The motivation is to address the issue of implicit stereotypes that may arise during the generation process of large language models by proposing an interpretable bias detection method.", "method": "The method combines nested semantic representation with a contextual contrast mechanism to extract latent bias features from the vector space structure of model outputs. Using attention weight perturbation, it analyzes the model's sensitivity to specific social attribute terms, thereby revealing the semantic pathways through which bias is formed.", "result": "Experimental results show that the proposed method achieves strong detection performance across various dimensions. It can accurately identify bias differences between semantically similar texts while maintaining high semantic alignment and output stability.", "conclusion": "The method provides a transparent and reliable technical foundation for bias detection, suitable for real-world applications where high trustworthiness of generated content is required."}}
{"id": "2508.05950", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05950", "abs": "https://arxiv.org/abs/2508.05950", "authors": ["Yanxing Liang", "Yinghui Wang", "Jinlong Yang", "Wei Li"], "title": "A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image", "comment": null, "summary": "The lack of spatial dimensional information remains a challenge in normal\nestimation from a single image. Recent diffusion-based methods have\ndemonstrated significant potential in 2D-to-3D implicit mapping, they rely on\ndata-driven statistical priors and miss the explicit modeling of light-surface\ninteraction, leading to multi-view normal direction conflicts. Moreover, the\ndiscrete sampling mechanism of diffusion models causes gradient discontinuity\nin differentiable rendering reconstruction modules, preventing 3D geometric\nerrors from being backpropagated to the normal generation network, thereby\nforcing existing methods to depend on dense normal annotations. This paper\nproposes SINGAD, a novel Self-supervised framework from a single Image for\nNormal estimation via 3D GAussian splatting guided Diffusion. By integrating\nphysics-driven light-interaction modeling and a differentiable rendering-based\nreprojection strategy, our framework directly converts 3D geometric errors into\nnormal optimization signals, solving the challenges of multi-view geometric\ninconsistency and data dependency. Specifically, the framework constructs a\nlight-interaction-driven 3DGS reparameterization model to generate multi-scale\ngeometric features consistent with light transport principles, ensuring\nmulti-view normal consistency. A cross-domain feature fusion module is designed\nwithin a conditional diffusion model, embedding geometric priors to constrain\nnormal generation while maintaining accurate geometric error propagation.\nFurthermore, a differentiable 3D reprojection loss strategy is introduced for\nself-supervised optimization that minimizes geometric error between the\nreconstructed and input image, eliminating dependence on annotated normal\ndatasets. Quantitative evaluations on the Google Scanned Objects dataset\ndemonstrate that our method outperforms state-of-the-art approaches across\nmultiple metrics.", "AI": {"tldr": "SINGAD is a self-supervised normal estimation framework that addresses the limitations of current diffusion-based methods by incorporating physics-driven light interaction and differentiable rendering, leading to improved performance and multi-view consistency without annotated data.", "motivation": "The motivation is to overcome the limitations of recent diffusion-based methods for normal estimation, such as the dependency on dense annotations and the issue of multi-view normal direction conflicts caused by their reliance on statistical priors.", "method": "The paper proposes SINGAD, a self-supervised framework for normal estimation from a single image. It uses a physics-driven 3D geometric representation and differentiable rendering for reprojection. The framework includes a light-interaction-driven 3D Gaussian splatting model and a cross-domain feature fusion module within a conditional diffusion model to generate normals. The method minimizes geometric errors without relying on dense normal annotations.", "result": "Quantitative evaluations on the Google Scanned Objects dataset show that SINGAD outperforms state-of-the-art approaches across multiple metrics.", "conclusion": "The framework successfully integrates physics-based modeling and differentiable rendering to directly optimize normals based on geometric errors, eliminating the need for dense normal annotations and improving consistency across multi-views."}}
{"id": "2508.06163", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06163", "abs": "https://arxiv.org/abs/2508.06163", "authors": ["Yingfeng Luo", "Dingyang Lin", "Junxin Wang", "Ziqiang Xu", "Kaiyan Chang", "Tong Zheng", "Bei Li", "Anxiang Ma", "Tong Xiao", "Zhengtao Yu", "Jingbo Zhu"], "title": "One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging", "comment": "Under review", "summary": "Model merging has emerged as a compelling data-free paradigm for multi-task\nlearning, enabling the fusion of multiple fine-tuned models into a single,\npowerful entity. A key technique in merging methods is sparsification, which\nprunes redundant parameters from task vectors to mitigate interference.\nHowever, prevailing approaches employ a ``one-size-fits-all'' strategy,\napplying a uniform sparsity ratio that overlooks the inherent structural and\nstatistical heterogeneity of model parameters. This often leads to a suboptimal\ntrade-off, where critical parameters are inadvertently pruned while less useful\nones are retained. To address this limitation, we introduce \\textbf{TADrop}\n(\\textbf{T}ensor-wise \\textbf{A}daptive \\textbf{Drop}), an adaptive\nsparsification strategy that respects this heterogeneity. Instead of a global\nratio, TADrop assigns a tailored sparsity level to each parameter tensor based\non its distributional properties. The core intuition is that tensors with\ndenser, more redundant distributions can be pruned aggressively, while sparser,\nmore critical ones are preserved. As a simple and plug-and-play module, we\nvalidate TADrop by integrating it with foundational, classic, and SOTA merging\nmethods. Extensive experiments across diverse tasks (vision, language, and\nmultimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and\nsignificantly boosts their performance. For instance, when enhancing a leading\nmerging method, it achieves an average performance gain of 2.0\\% across 8\nViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter\ninterference by tailoring sparsification to the model's structure, offering a\nnew baseline for high-performance model merging.", "AI": {"tldr": "TADrop, a tensor-wise adaptive sparsification method, is proposed to improve multi-task learning by addressing the limitations of uniform sparsity ratios in model merging.", "motivation": "To overcome suboptimal parameter pruning caused by a \"one-size-fits-all\" sparsity strategy, which often results in retaining less useful parameters and pruning crucial ones.", "method": "TADrop assigns customized sparsity levels to each parameter tensor based on the tensors' distributional properties, allowing for more effective parameter pruning.", "result": "Experiments on various tasks and models show consistent and significant performance improvement when TADrop is applied, for example, an average gain of 2.0% across 8 ViT-B/32 tasks.", "conclusion": "TADrop boosts model performance by tailoring sparsification to the structural properties of the model, setting a new baseline for effective model merging in multi-task learning."}}
{"id": "2508.05954", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05954", "abs": "https://arxiv.org/abs/2508.05954", "authors": ["Han Lin", "Jaemin Cho", "Amir Zadeh", "Chuan Li", "Mohit Bansal"], "title": "Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents", "comment": "Project Page: https://bifrost-1.github.io", "summary": "There is growing interest in integrating high-fidelity visual synthesis\ncapabilities into large language models (LLMs) without compromising their\nstrong reasoning capabilities. Existing methods that directly train LLMs or\nbridge LLMs and diffusion models usually suffer from costly training since the\nbackbone LLMs have not seen image representations during pretraining. We\npresent Bifrost-1, a unified framework that bridges pretrained multimodal LLMs\n(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent\nvariables, which are natively aligned with the MLLM's CLIP visual encoder.\nThese patch-level image embeddings are integrated into the diffusion model with\na lightweight adaptation of its ControlNet. To retain the original multimodal\nreasoning capabilities of MLLMs, we equip the MLLM with a visual generation\nbranch initialized from the original MLLM parameters when predicting the\npatch-level image embeddings. By seamlessly integrating pretrained MLLMs and\ndiffusion models with patch-level CLIP latents, our framework enables\nhigh-fidelity controllable image generation with significant training\nefficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or\nbetter performance than previous methods in terms of visual fidelity and\nmultimodal understanding, with substantially lower compute during training. We\nalso provide comprehensive ablation studies showing the effectiveness of our\ndesign choices.", "AI": {"tldr": "Bifrost-1框架通过使用CLIP的patch-level图像嵌入作为潜在变量，将预训练的多模态语言模型和扩散模型结合，实现了高保真且计算成本低的图像生成。", "motivation": "该研究的动机是为了整合高保真视觉合成能力到大型语言模型中，同时不牺牲它们的强推理能力。现有的直接训练LLMs或是连接LLMs和扩散模型的方法通常需要高昂的训练成本，因为基础的LLMs在预训练期间并未接触过图像表示。", "method": "通过使用CLIP的patch-level图像嵌入作为潜在变量，Bifrost-1框架将预训练的多模态语言模型（MLLMs）和扩散模型相结合。此外，为了保留MLLMs的多模态推理能力，在预测patch-level图像嵌入时，会为MLLM添加一个初始化为原始MLLM参数的视觉生成分支。", "result": "实验表明，Bifrost-1在视觉保真度和多模态理解方面达到了与先前方法相当或更好的表现，并且在训练过程中计算量显著降低。", "conclusion": "通过无缝集成预训练的MLLMs和扩散模型，并使用CLIP的patch-level潜在变量，Bifrost-1框架实现了高保真可控的图像生成，并且具有显著的训练效率优势。"}}
{"id": "2508.06165", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06165", "abs": "https://arxiv.org/abs/2508.06165", "authors": ["Weitao Li", "Boran Xiang", "Xiaolong Wang", "Zhinan Gou", "Weizhi Ma", "Yang Liu"], "title": "UR$^2$: Unify RAG and Reasoning through Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable capabilities through two\ncomplementary paradigms: Retrieval-Augmented Generation (RAG), which enhances\nknowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),\nwhich optimizes complex reasoning abilities. However, these two capabilities\nare often developed in isolation, and existing efforts to unify them remain\nnarrow in scope-typically limited to open-domain QA with fixed retrieval\nsettings and task-specific assumptions. This lack of integration constrains\ngeneralization and limits the applicability of RAG-RL methods to broader\ndomains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a\ngeneral framework that unifies retrieval and reasoning through reinforcement\nlearning. UR2 introduces two key contributions: a difficulty-aware curriculum\ntraining that selectively invokes retrieval only for challenging problems, and\na hybrid knowledge access strategy combining domain-specific offline corpora\nwith LLM-generated summaries. These components are designed to enable dynamic\ncoordination between retrieval and reasoning, improving adaptability across a\ndiverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,\nand mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B\nand LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods,\nachieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several\nbenchmarks. We have released all code, models, and data at\nhttps://github.com/Tsinghua-dhy/UR2.", "AI": {"tldr": "UR2 framework unifies RAG and RLVR methods through advanced training techniques and hybrid knowledge access, achieving superior performance across diverse tasks.", "motivation": "Address the limitations of isolated development and limited scope of Retrieval-Augmented Generation (RAG) and Reinforcement Learning from Verifiable Rewards (RLVR) methods, aiming to enhance generalization and broaden applicability.", "method": "Unified RAG and Reasoning (UR2) framework, incorporating difficulty-aware curriculum training and a hybrid knowledge access strategy that integrates domain-specific offline corpora with LLM-generated summaries.", "result": "UR2 demonstrates significant performance improvements over existing RAG and RL methods across a variety of tasks and achieves comparable results to GPT-4o-mini and GPT-4.1-mini.", "conclusion": "UR2 effectively integrates retrieval and reasoning under a unified framework, showcasing superior adaptability and performance across multiple domains and tasks."}}
{"id": "2508.05976", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05976", "abs": "https://arxiv.org/abs/2508.05976", "authors": ["Zhihao Zhu", "Yifan Zheng", "Siyu Pan", "Yaohui Jin", "Yao Mu"], "title": "PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation", "comment": "Accepted to ICCV 2025. 8 pages main paper, 8 figures, plus\n  supplementary material", "summary": "The fragmentation between high-level task semantics and low-level geometric\nfeatures remains a persistent challenge in robotic manipulation. While\nvision-language models (VLMs) have shown promise in generating affordance-aware\nvisual representations, the lack of semantic grounding in canonical spaces and\nreliance on manual annotations severely limit their ability to capture dynamic\nsemantic-affordance relationships. To address these, we propose Primitive-Aware\nSemantic Grounding (PASG), a closed-loop framework that introduces: (1)\nAutomatic primitive extraction through geometric feature aggregation, enabling\ncross-category detection of keypoints and axes; (2) VLM-driven semantic\nanchoring that dynamically couples geometric primitives with functional\naffordances and task-relevant description; (3) A spatial-semantic reasoning\nbenchmark and a fine-tuned VLM (Qwen2.5VL-PA). We demonstrate PASG's\neffectiveness in practical robotic manipulation tasks across diverse scenarios,\nachieving performance comparable to manual annotations. PASG achieves a\nfiner-grained semantic-affordance understanding of objects, establishing a\nunified paradigm for bridging geometric primitives with task semantics in\nrobotic manipulation.", "AI": {"tldr": "本文引入PASG框架解决机器人操作中高层次任务语义和低层次几何特征之间的割裂问题，通过自动提取几何特征并将其与视觉语言模型相结合，取得了与手动注释相当的表现。", "motivation": "当前机器人操作面临的一个持续挑战是高层次任务语义与低层次几何特征之间的割裂。视觉-语言模型虽然展示了生成感知意识的视觉表示的潜力，但其在标准空间中的语义接地不足和对手动注释的依赖限制了它们捕捉动态语义-适用性关系的能力。", "method": "本文提出了Primitive-Aware Semantic Grounding (PASG)，这是一个闭环框架，包括：1) 通过几何特征聚合进行自动原语提取，能够跨类别检测关键点和轴；2) 基于视觉语言模型的语义锚定，动态地将几何原语与功能性适用性以及任务相关描述结合；3) 一个空间-语义推理基准和一个微调过的视觉语言模型Qwen2.5VL-PA。", "result": "实验中，PASG在各种场景下的实际机器人操作任务中表现有效，其性能可与手动注释相媲美。", "conclusion": "PASG实现了对物体更细粒度的语义-适用性理解，建立了将几何原语与机器人操作中的任务语义相联系的统一范式。"}}
{"id": "2508.06167", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06167", "abs": "https://arxiv.org/abs/2508.06167", "authors": ["Vít Gvoždiak"], "title": "Pragmatics beyond humans: meaning, communication, and LLMs", "comment": null, "summary": "The paper reconceptualizes pragmatics not as a subordinate, third dimension\nof meaning, but as a dynamic interface through which language operates as a\nsocially embedded tool for action. With the emergence of large language models\n(LLMs) in communicative contexts, this understanding needs to be further\nrefined and methodologically reconsidered. The first section challenges the\ntraditional semiotic trichotomy, arguing that connectionist LLM architectures\ndestabilize established hierarchies of meaning, and proposes the Human-Machine\nCommunication (HMC) framework as a more suitable alternative. The second\nsection examines the tension between human-centred pragmatic theories and the\nmachine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics\ncontinue to dominate, it relies on human-specific assumptions ill-suited to\npredictive systems like LLMs. Probabilistic pragmatics, particularly the\nRational Speech Act framework, offers a more compatible teleology by focusing\non optimization rather than truth-evaluation. The third section addresses the\nissue of substitutionalism in three forms - generalizing, linguistic, and\ncommunicative - highlighting the anthropomorphic biases that distort LLM\nevaluation and obscure the role of human communicative subjects. Finally, the\npaper introduces the concept of context frustration to describe the paradox of\nincreased contextual input paired with a collapse in contextual understanding,\nemphasizing how users are compelled to co-construct pragmatic conditions both\nfor the model and themselves. These arguments suggest that pragmatic theory may\nneed to be adjusted or expanded to better account for communication involving\ngenerative AI.", "AI": {"tldr": "文章重新定义了语用学，认为其应该作为一种动态接口，通过这种接口语言可以作为嵌入社会中的行动工具。文章强调了传统语用学理论在面对大语言模型时的局限性，并提出了一些新的理论框架和概念来适应这一变化。", "motivation": "随着大语言模型在交流领域的出现，传统的语义三元分类法需要进一步调整和方法上的重新考虑。文章认为，传统的基于Grice的语用学在解释类似于LLM的预测系统时，存在人类特有的假设，这些假设并不适合。", "method": "文章提出了一个新的框架——人机沟通（HMC）框架，并探讨了人类中心的语用理论和以机器为中心的大语言模型（LLMs）之间的紧张关系。此外，文章分析了三种形式的替代主义，并介绍了'语境挫折'的概念，以更加全面地解释涉及生成AI的交流理论。", "result": "文章揭示了传统的语用理论在面对大语言模型时的不适用性，并且引入了其他更兼容的方法来解释人与机器之间的交流，同时指出了未来研究的方向。", "conclusion": "文章的结论是，传统的语用学理论可能需要调整或扩展，以便更好地解释包含生成AI的交流。提出了一个新的概念“语境挫折”来描述语境输入量增加与语境理解下降之间的悖论。"}}
{"id": "2508.05982", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05982", "abs": "https://arxiv.org/abs/2508.05982", "authors": ["Qingyang Liu", "Bingjie Gao", "Weiheng Huang", "Jun Zhang", "Zhongqian Sun", "Yang Wei", "Zelin Peng", "Qianli Ma", "Shuai Yang", "Zhaohe Liao", "Haonan Zhao", "Li Niu"], "title": "AnimateScene: Camera-controllable Animation in Any Scene", "comment": null, "summary": "3D scene reconstruction and 4D human animation have seen rapid progress and\nbroad adoption in recent years. However, seamlessly integrating reconstructed\nscenes with 4D human animation to produce visually engaging results remains\nchallenging. One key difficulty lies in placing the human at the correct\nlocation and scale within the scene while avoiding unrealistic\ninterpenetration. Another challenge is that the human and the background may\nexhibit different lighting and style, leading to unrealistic composites. In\naddition, appealing character motion videos are often accompanied by camera\nmovements, which means that the viewpoints need to be reconstructed along a\nspecified trajectory. We present AnimateScene, which addresses the above issues\nin a unified framework. First, we design an accurate placement module that\nautomatically determines a plausible 3D position for the human and prevents any\ninterpenetration within the scene during motion. Second, we propose a\ntraining-free style alignment method that adapts the 4D human representation to\nmatch the background's lighting and style, achieving coherent visual\nintegration. Finally, we design a joint post-reconstruction method for both the\n4D human and the 3D scene that allows camera trajectories to be inserted,\nenabling the final rendered video to feature visually appealing camera\nmovements. Extensive experiments show that AnimateScene generates dynamic scene\nvideos with high geometric detail and spatiotemporal coherence across various\ncamera and action combinations.", "AI": {"tldr": "本文提出了AnimateScene框架，通过准确的定位模块、无需训练的样式对齐方法和可以插入摄像机轨迹的后重建方法解决了3D场景重建与4D人体动画的无缝融合问题。", "motivation": "虽然3D场景重建和4D人体动画近年来取得了快速发展，然而将两者无缝集成以产生视觉上吸引人的结果仍然具有挑战性。主要困难包括将人体放置在正确的场景位置和规模以避免不现实的穿模，以及背景和动画在光照和风格上的差异导致的不真实组合问题。", "method": "AnimateScene提出了一种综合框架来解决人体动画与3D场景重建无缝集成的挑战。该方法包括三个主要部分：1) 设计了一个准确的定位模块，自动确定人体在场景中的合理3D位置并防止动画过程中出现穿模。2) 提出了一种无需训练的样式对齐方法，使得4D人体表示与背景的光照和风格相匹配，从而实现视觉上的连贯整合。3) 设计了一个联合后重建方法，用于4D人体和3D场景，允许插入摄像机轨迹，从而使得最终渲染的视频具有视觉上吸引人的摄像机运动。", "result": "实验表明，AnimateScene生成的动态场景视频在几何细节和各种摄像机及动作组合下的时空连贯性方面都表现出色。", "conclusion": "AnimateScene提供了一种解决这些集成挑战的全面解决方案，使得生成的动态场景视频具有高度的细节和连贯性。"}}
{"id": "2508.06178", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06178", "abs": "https://arxiv.org/abs/2508.06178", "authors": ["Hugo Abonizio", "Thales Almeida", "Roberto Lotufo", "Rodrigo Nogueira"], "title": "Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime", "comment": null, "summary": "Large language models (LLMs) often require vast amounts of text to\neffectively acquire new knowledge. While continuing pre-training on large\ncorpora or employing retrieval-augmented generation (RAG) has proven\nsuccessful, updating an LLM with only a few thousand or million tokens remains\nchallenging. In this work, we investigate the task of injecting small,\nunstructured information into LLMs and its relation to the catastrophic\nforgetting phenomenon. We use a dataset of recent news -- ensuring no overlap\nwith the model's pre-training data -- to evaluate the knowledge acquisition by\nprobing the model with question-answer pairs related the learned information.\nStarting from a continued pre-training baseline, we explored different\naugmentation algorithms to generate synthetic data to improve the knowledge\nacquisition capabilities. Our experiments show that simply continuing\npre-training on limited data yields modest improvements, whereas exposing the\nmodel to diverse textual variations significantly improves the learning of new\nfacts -- particularly with methods that induce greater variability through\ndiverse prompting. Furthermore, we shed light on the forgetting phenomenon in\nsmall-data regimes, illustrating the delicate balance between learning new\ncontent and retaining existing capabilities. We also confirm the sensitivity of\nRAG-based approaches for knowledge injection, which often lead to greater\ndegradation on control datasets compared to parametric methods. Finally, we\ndemonstrate that models can generate effective synthetic training data\nthemselves, suggesting a pathway toward self-improving model updates. All code\nand generated data used in our experiments are publicly available, providing a\nresource for studying efficient knowledge injection in LLMs with limited data\nat https://github.com/hugoabonizio/knowledge-injection-methods.", "AI": {"tldr": "The paper explores methods to inject small, unstructured information into large language models (LLMs), focusing on the challenge of learning new facts and retaining existing knowledge. Experiments show that diverse textual variations aid in learning new facts without significant loss of older knowledge.", "motivation": "The goal is to explore effective methods to update large language models with only a few thousand or million tokens, which remains a challenge, and to examine the balance between learning new content and retaining existing capabilities.", "method": "We investigate the task of injecting a small amount of unstructured information into large language models and its relation to the catastrophic forgetting phenomenon. We experimented with various augmentation algorithms to generate synthetic data to improve knowledge acquisition.", "result": "Experiments show that continuing pre-training on limited data yields modest improvements, while diverse textual variations, particularly those that induce greater variability, significantly improve the learning of new facts. RAG-based approaches were found to be sensitive and cause more degradation on control datasets compared to parametric methods.", "conclusion": "The research concludes that large language models can generate effective synthetic training data themselves, offering a path toward self-improving model updates. The findings contribute to more efficient knowledge injection in LLMs with limited data."}}
{"id": "2508.05989", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05989", "abs": "https://arxiv.org/abs/2508.05989", "authors": ["Younjoon Chung", "Hyoungseob Park", "Patrick Rim", "Xiaoran Zhang", "Jihe He", "Ziyao Zeng", "Safa Cicek", "Byung-Woo Hong", "James S. Duncan", "Alex Wong"], "title": "ETA: Energy-based Test-time Adaptation for Depth Completion", "comment": null, "summary": "We propose a method for test-time adaptation of pretrained depth completion\nmodels. Depth completion models, trained on some ``source'' data, often predict\nerroneous outputs when transferred to ``target'' data captured in novel\nenvironmental conditions due to a covariate shift. The crux of our method lies\nin quantifying the likelihood of depth predictions belonging to the source data\ndistribution. The challenge is in the lack of access to out-of-distribution\n(target) data prior to deployment. Hence, rather than making assumptions\nregarding the target distribution, we utilize adversarial perturbations as a\nmechanism to explore the data space. This enables us to train an energy model\nthat scores local regions of depth predictions as in- or out-of-distribution.\nWe update the parameters of pretrained depth completion models at test time to\nminimize energy, effectively aligning test-time predictions to those of the\nsource distribution. We call our method ``Energy-based Test-time Adaptation'',\nor ETA for short. We evaluate our method across three indoor and three outdoor\ndatasets, where ETA improve over the previous state-of-the-art method by an\naverage of 6.94% for outdoors and 10.23% for indoors. Project Page:\nhttps://fuzzythecat.github.io/eta.", "AI": {"tldr": "通过对抗扰动训练能量模型，实现测试时深度预测模型的自适应调整，提高在新环境下的预测准确性。", "motivation": "解决预训练深度完成模型在新环境数据下因协变量变化导致预测不准的问题。", "method": "提出Energy-based Test-time Adaptation (ETA)方法，利用对抗扰动探索数据空间，训练能量模型对预测区域评分。", "result": "在三个室内和三个室外数据集上，平均比之前的最先进技术提高了6.94%和10.23%。", "conclusion": "对抗扰动和能量模型的有效结合，能在测试时调整模型，提高其适应不同环境的能力。"}}
{"id": "2508.06186", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06186", "abs": "https://arxiv.org/abs/2508.06186", "authors": ["Ali Sarabadani", "Maryam Abdollahi Shamami", "Hamidreza Sadeghsalehi", "Borhan Asadi", "Saba Hesaraki"], "title": "DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration", "comment": null, "summary": "Large Language Models (LLMs) have grown exponentially since the release of\nChatGPT. These models have gained attention due to their robust performance on\nvarious tasks, including language processing tasks. These models achieve\nunderstanding and comprehension of tasks by training billions of parameters.\nThe development of these models is a transformative force in enhancing natural\nlanguage understanding and has taken a significant step towards artificial\ngeneral intelligence (AGI). In this study, we aim to present the DKG-LLM\nframework. The DKG-LLM framework introduces a groundbreaking approach to\nmedical diagnosis and personalized treatment recommendations by integrating a\ndynamic knowledge graph (DKG) with the Grok 3 large language model. Using the\nAdaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data\n(including clinical reports and PubMed articles) and patient records\ndynamically generate a knowledge graph consisting of 15,964 nodes in 13\ndistinct types (e.g., diseases, symptoms, treatments, patient profiles) and\n127,392 edges in 26 relationship types (e.g., causal, therapeutic,\nassociation). ASFA utilizes advanced probabilistic models, Bayesian inference,\nand graph optimization to extract semantic information, dynamically updating\nthe graph with approximately 150 new nodes and edges in each data category\nwhile maintaining scalability with up to 987,654 edges. Real-world datasets,\nincluding MIMIC-III and PubMed, were utilized to evaluate the proposed\narchitecture. The evaluation results show that DKG-LLM achieves a diagnostic\naccuracy of 84.19%. The model also has a treatment recommendation accuracy of\n89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and\ntransformative tool that handles noisy data and complex multi-symptom diseases,\nalong with feedback-based learning from physician input.", "AI": {"tldr": "本文介绍了一种集成动态知识图与大型语言模型的框架（DKG-LLM），通过高级算法自动处理医疗数据，以提高诊断与个性化治疗推荐的准确性，评估结果显示该框架在临床实践中表现出色。", "motivation": "此框架旨在通过先进的自适应语义融合算法（ASFA）从异构医疗数据中提取语义信息，动态产生包含多种类型节点和关系的知识图谱，从而提高医疗诊断和治疗的准确性。", "method": "本文提出了DKG-LLM框架，该框架通过集成动态知识图（DKG）与Grok 3大型语言模型，以期实现医疗诊断和个性化治疗建议的新方法。", "result": "现实世界的数据集（如MIMIC-III和PubMed）评估显示，DKG-LLM在诊断准确率上达到了84.19%，治疗建议准确率为89.63%，语义覆盖率为93.48%。", "conclusion": "DKG-LLM是一个可靠且具有变革性的工具，能够处理嘈杂的数据和复杂的多症状疾病，并支持基于医生反馈的学习。"}}
{"id": "2508.05990", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05990", "abs": "https://arxiv.org/abs/2508.05990", "authors": ["Haichao Wang", "Xinyue Xi", "Jiangtao Wen", "Yuxing Han"], "title": "Fast Motion Estimation and Context-Aware Refinement for Efficient Bayer-Domain Video Vision", "comment": null, "summary": "The efficiency of video computer vision system remains a challenging task due\nto the high temporal redundancy inside a video. Existing works have been\nproposed for efficient vision computer vision. However, they do not fully\nreduce the temporal redundancy and neglect the front end computation overhead.\nIn this paper, we propose an efficient video computer vision system. First,\nimage signal processor is removed and Bayer-format data is directly fed into\nvideo computer vision models, thus saving the front end computation. Second,\ninstead of optical flow models and video codecs, a fast block matching-based\nmotion estimation algorithm is proposed specifically for efficient video\ncomputer vision, with a MV refinement module. To correct the error,\ncontext-aware block refinement network is introduced to refine regions with\nlarge error. To further balance the accuracy and efficiency, a frame selection\nstrategy is employed. Experiments on multiple video computer vision tasks\ndemonstrate that our method achieves significant acceleration with slight\nperformance loss.", "AI": {"tldr": "This paper presents an efficient video computer vision system that improves upon existing methods by directly using Bayer-format data, employing a fast motion estimation algorithm, and using a context-aware refinement network to achieve significant acceleration with minor performance loss.", "motivation": "The motivation behind this paper is to address the challenge of reducing temporal redundancy and the front end computation overhead in video computer vision systems that has not been sufficiently addressed by existing works.", "method": "The paper proposes an efficient video computer vision system by removing the image signal processor and using Bayer-format data input. It introduces a fast block matching-based motion estimation algorithm with a MV refinement module and a context-aware block refinement network to correct errors. The system also employs a frame selection strategy to balance accuracy and efficiency.", "result": "The experimental results show that the proposed method achieves significant acceleration while maintaining a high level of performance in multiple video computer vision tasks.", "conclusion": "The conclusion of the paper is that the proposed system effectively reduces temporal redundancy and front end computation overhead, achieving faster processing with only slight performance loss as demonstrated by experiments on multiple video computer vision tasks."}}
{"id": "2508.06194", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06194", "abs": "https://arxiv.org/abs/2508.06194", "authors": ["Lai Jiang", "Yuekang Li", "Xiaohan Zhang", "Youtao Ding", "Li Pan"], "title": "Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation", "comment": null, "summary": "Precise jailbreak evaluation is vital for LLM red teaming and jailbreak\nresearch. Current approaches employ binary classification ( e.g., string\nmatching, toxic text classifiers, LLM-driven methods), yielding only \"yes/no\"\nlabels without quantifying harm intensity. Existing multi-dimensional\nframeworks ( e.g., Security Violation, Relative Truthfulness, Informativeness)\napply uniform evaluation criteria across scenarios, resulting in\nscenario-specific mismatches--for instance, \"Relative Truthfulness\" is\nirrelevant to \"hate speech\"--which compromise evaluation precision. To tackle\nthese limitations, we introduce SceneJailEval, with key contributions: (1) A\ngroundbreaking scenario-adaptive multi-dimensional framework for jailbreak\nevaluation, overcoming the critical \"one-size-fits-all\" constraint of existing\nmulti-dimensional methods, and featuring strong extensibility to flexibly adapt\nto customized or emerging scenarios. (2) A comprehensive 14-scenario dataset\nwith diverse jailbreak variants and regional cases, filling the long-standing\ngap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3)\nSceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on\nour full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over\nprior SOTA), surpassing accuracy limits of existing evaluation methods in\nheterogeneous scenarios and confirming its advantage.", "AI": {"tldr": "提出了一个名为SceneJailEval的场景适应型越狱评估框架，克服了现有方法的“一刀切”问题，并在多个数据集上取得了优于现有方法的结果。", "motivation": "精确的越狱评估对于LLM红队行动和越狱研究至关重要。当前方法仅提供二元分类结果，且多维框架在应用统一评价标准时存在场景特异性错配问题，影响评价精度。", "method": "介绍了一种突破性的场景自适应多维框架SceneJailEval，克服了现有方法‘一刀切’的限制，并具有强大的扩展性，能够灵活适应定制或新兴场景。", "result": "SceneJailEval在全场景数据集上达到了最新的F1分数0.917（比之前的最佳方法高出6%）和JBB上的0.995（比之前的最佳方法高出3%），超越了现有评估方法在异构场景中的准确性限制。", "conclusion": "SceneJailEval通过提供一个场景适应型多维框架，在提高了评估精度的同时，成功弥补了一个高质量、全面基准的长期空白。"}}
{"id": "2508.05991", "categories": ["cs.CV", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.05991", "abs": "https://arxiv.org/abs/2508.05991", "authors": ["Juewen Hu", "Yexin Li", "Jiulin Li", "Shuo Chen", "Pring Wong"], "title": "ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge", "comment": null, "summary": "Emotion recognition plays a vital role in enhancing human-computer\ninteraction. In this study, we tackle the MER-SEMI challenge of the MER2025\ncompetition by proposing a novel multimodal emotion recognition framework. To\naddress the issue of data scarcity, we leverage large-scale pre-trained models\nto extract informative features from visual, audio, and textual modalities.\nSpecifically, for the visual modality, we design a dual-branch visual encoder\nthat captures both global frame-level features and localized facial\nrepresentations. For the textual modality, we introduce a context-enriched\nmethod that employs large language models to enrich emotional cues within the\ninput text. To effectively integrate these multimodal features, we propose a\nfusion strategy comprising two key components, i.e., self-attention mechanisms\nfor dynamic modality weighting, and residual connections to preserve original\nrepresentations. Beyond architectural design, we further refine noisy labels in\nthe training set by a multi-source labeling strategy. Our approach achieves a\nsubstantial performance improvement over the official baseline on the\nMER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to\n78.63%, thereby validating the effectiveness of the proposed framework.", "AI": {"tldr": "本研究针对MER2025竞赛中的MER-SEMI挑战，通过多模态情感识别框架改进了情感识别性能，框架包括了多层次特征提取、多模态融合和标签优化策略。", "motivation": "本研究旨在解决情绪识别中的数据稀缺问题，增强人机交互体验，特别是在MER2025竞赛的MER-SEMI挑战中。", "method": "我们提出了一个新颖的多模态情感识别框架，利用大规模预训练模型从视觉、音频和文本模态中提取有用特征。视觉模态上，设计了双分支视觉编码器捕获全局帧级特征和面部局部表示；文本模态上，引入了一种基于大语言模型的上下文丰富方法来增强文本中的情感线索。我们提出了一种集成了自注意力机制和残差连接的融合策略来整合这些多模态特征。此外，我们还采用多源标注策略来优化训练集中的噪声标签。", "result": "我们的方法在MER2025-SEMI数据集上显著提高了官方基线的性能，加权F值达到87.49%，相比基线的78.63%有了明显提升。", "conclusion": "实验结果表明，我们提出的情感识别框架在MER2025-SEMI数据集上取得的成绩显著优于官方基线，证明了框架的有效性。"}}
{"id": "2508.06196", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06196", "abs": "https://arxiv.org/abs/2508.06196", "authors": ["Nizi Nazar", "Ehsaneddin Asgari"], "title": "EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations", "comment": null, "summary": "Emotional Intelligence (EI) is a critical yet underexplored dimension in the\ndevelopment of human-aligned LLMs. To address this gap, we introduce a unified,\npsychologically grounded four-layer taxonomy of EI tailored for large language\nmodels (LLMs), encompassing emotional tracking, cause inference, appraisal, and\nemotionally appropriate response generation. Building on this framework, we\npresent EICAP-Bench, a novel MCQ style multi-turn benchmark designed to\nevaluate EI capabilities in open-source LLMs across diverse linguistic and\ncultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma\n(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,\nidentifying Qwen2.5-Instruct as the strongest baseline. To assess the potential\nfor enhancing EI capabilities, we fine-tune both Qwen2.5-Base and\nQwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,\ninstruction-tuned dialogue dataset, in both English and Arabic. Our statistical\nanalysis reveals that among the five EI layers, only the Appraisal layer shows\nsignificant improvement through UC-based fine-tuning. These findings highlight\nthe limitations of existing pretraining and instruction-tuning paradigms in\nequipping LLMs with deeper emotional reasoning and underscore the need for\ntargeted data and modeling strategies for comprehensive EI alignment.", "AI": {"tldr": "本文提出了一个情感智能(EI)的四层分类框架，并开发了一个新的基准测试EICAP-Bench，用于评估大型语言模型的情感智能。基于UltraChat数据集的微调结果显示只有评价层有显著提升，突出了现有方法的局限性和改进数据与模型策略的需求。", "motivation": "文章旨在通过引入一个新框架和测试基准来填补大型语言模型情感智能维度上的研究空白。", "method": "本文提出了一个以心理学为基础的四层情感智能(EI)分类框架，适用于大型语言模型，涵盖情绪跟踪、原因推断、评价和生成情感上合适的响应。基于这个框架，开发了一个新的多回合多项选择题基准测试EICAP-Bench，用于评估不同语言和文化背景下开源大型语言模型的情感智能能力。同时还使用UltraChat数据集对Qwen2.5模型进行了微调。", "result": "在EICAP-Bench基准测试中，发现Qwen2.5-Instruct模型的表现最好。统计分析表明，情感智能的五个层级中，只有评价层在基于UltraChat的数据集上进行微调后有显著的提升。", "conclusion": "研究结果揭示了现有预训练和指令调优范式在赋予大型语言模型更深层次的情感推理方面的局限性，并强调了需要有针对性的数据和建模策略来实现全面的情感智能对齐。"}}
{"id": "2508.05994", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05994", "abs": "https://arxiv.org/abs/2508.05994", "authors": ["Huadong Wu", "Yi Fu", "Yunhao Li", "Yuan Gao", "Kang Du"], "title": "EvoMakeup: High-Fidelity and Controllable Makeup Editing with MakeupQuad", "comment": null, "summary": "Facial makeup editing aims to realistically transfer makeup from a reference\nto a target face. Existing methods often produce low-quality results with\ncoarse makeup details and struggle to preserve both identity and makeup\nfidelity, mainly due to the lack of structured paired data -- where source and\nresult share identity, and reference and result share identical makeup. To\naddress this, we introduce MakeupQuad, a large-scale, high-quality dataset with\nnon-makeup faces, references, edited results, and textual makeup descriptions.\nBuilding on this, we propose EvoMakeup, a unified training framework that\nmitigates image degradation during multi-stage distillation, enabling iterative\nimprovement of both data and model quality. Although trained solely on\nsynthetic data, EvoMakeup generalizes well and outperforms prior methods on\nreal-world benchmarks. It supports high-fidelity, controllable, multi-task\nmakeup editing -- including full-face and partial reference-based editing, as\nwell as text-driven makeup editing -- within a single model. Experimental\nresults demonstrate that our method achieves superior makeup fidelity and\nidentity preservation, effectively balancing both aspects. Code and dataset\nwill be released upon acceptance.", "AI": {"tldr": "本文引入MakeupQuad数据集和EvoMakeup框架，解决面部妆容编辑的身份保持和化妆真实性的难题，实现高质量的妆容编辑。", "motivation": "动机在于解决现有方法中由于缺乏结构化的配对数据而导致的低质量结果、妆容细节粗糙以及难以同时保持身份和妆容真实性的问题。", "method": "我们的方法引入了MakeupQuad，这是一个大规模、高质量的数据集，包含无妆脸、参考妆容、编辑结果和文本妆容描述。我们还提出了EvoMakeup，这是一个统一的训练框架，通过多阶段蒸馏减少图像退化，实现数据和模型质量的迭代改进。", "result": "尽管仅在合成数据上进行训练，EvoMakeup在现实世界基准测试中表现出色，支持高保真度、可控的多任务化妆编辑，包括全脸和部分参考编辑以及基于文本的化妆编辑。", "conclusion": "实验结果表明，我们提出的方法在保持身份的同时，实现了妆容的真实性和控制性，优于先前的方法。"}}
{"id": "2508.06204", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06204", "abs": "https://arxiv.org/abs/2508.06204", "authors": ["Richard Willats", "Josh Pennington", "Aravind Mohan", "Bertie Vidgen"], "title": "Classification is a RAG problem: A case study on hate speech detection", "comment": null, "summary": "Robust content moderation requires classification systems that can quickly\nadapt to evolving policies without costly retraining. We present classification\nusing Retrieval-Augmented Generation (RAG), which shifts traditional\nclassification tasks from determining the correct category in accordance with\npre-trained parameters to evaluating content in relation to contextual\nknowledge retrieved at inference. In hate speech detection, this transforms the\ntask from \"is this hate speech?\" to \"does this violate the hate speech policy?\"\n  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates\nthis approach and offers three key advantages: (1) robust classification\naccuracy comparable to leading commercial systems, (2) inherent explainability\nvia retrieved policy segments, and (3) dynamic policy updates without model\nretraining. Through three experiments, we demonstrate strong baseline\nperformance and show that the system can apply fine-grained policy control by\ncorrectly adjusting protection for specific identity groups without requiring\nretraining or compromising overall performance. These findings establish that\nRAG can transform classification into a more flexible, transparent, and\nadaptable process for content moderation and wider classification problems.", "AI": {"tldr": "本文提出了一种使用检索增强生成（RAG）方法的分类系统，该系统在不重新训练模型的情况下能够根据不同政策进行调整，并展示了其灵活性和准确性的优势。", "motivation": "该方法旨在让分类系统能够快速适应不断变化的政策，而不需要昂贵的重新训练过程，特别是在仇恨言论检测中更加灵活地应用政策。", "method": "本文提出了一种使用检索增强生成（RAG）技术的分类方法，通过检索获取上下文知识，在推理阶段对内容进行评估，而非依赖预训练参数确定正确类别。", "result": "实验证明，该系统在不重新训练的情况下能够精准地调整特定身份群体的保护措施，并且整体性能未受影响。", "conclusion": "这些发现表明，RAG可以将分类过程变成一个更加灵活、透明和适应性强的方法，不仅适用于内容审核，还适用于更广泛的分类问题。"}}
{"id": "2508.06009", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06009", "abs": "https://arxiv.org/abs/2508.06009", "authors": ["Jun Feng", "Zixin Wang", "Zhentao Zhang", "Yue Guo", "Zhihan Zhou", "Xiuyi Chen", "Zhenyang Li", "Dawei Yin"], "title": "MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models", "comment": "29 pages, 16 figures", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in visual mathematical reasoning across various existing\nbenchmarks. However, these benchmarks are predominantly based on clean or\nprocessed multimodal inputs, without incorporating the images provided by\nreal-world Kindergarten through 12th grade (K-12) educational users. To address\nthis gap, we introduce MathReal, a meticulously curated dataset comprising\n2,000 mathematical questions with images captured by handheld mobile devices in\nauthentic scenarios. Each question is an image, containing the question text\nand visual element. We systematically classify the real images into three\nprimary categories: image quality degradation, perspective variation, and\nirrelevant content interference, which are further delineated into 14\nsubcategories. Additionally, MathReal spans five core knowledge and ability\ncategories, which encompass three question types and are divided into three\ndifficulty levels. To comprehensively evaluate the multimodal mathematical\nreasoning abilities of state-of-the-art MLLMs in real-world scenarios, we\ndesign six experimental settings that enable a systematic analysis of their\nperformance. Through extensive experimentation, we find that the\nproblem-solving abilities of existing MLLMs are significantly challenged in\nrealistic educational contexts. Based on this, we conduct a thorough analysis\nof their performance and error patterns, providing insights into their\nrecognition, comprehension, and reasoning capabilities, and outlining\ndirections for future improvements. Data and code:\nhttps://github.com/junfeng0288/MathReal.", "AI": {"tldr": "该研究指出现有MLLMs在处理K-12教育环境中真实场景的数学问题图象任务时面临挑战，为解决这一问题，创建了MathReal数据集来评估模型性能。", "motivation": "该研究旨在填补现有基准测试中对多模态输入质量欠缺、未考虑K-12教育用户实际使用场景的空白。通过创建MathReal数据集，研究人员试图评估和分析大型语言模型在教育现实应用中的数学推理能力。", "method": "研究人员构建了一个名为MathReal的数据集，包括2,000个数学问题，这些问题的图片是通过手持设备在实际场景中拍摄的。他们将图像质量下降、透视变化和无关内容干扰分为14个子类别，并且根据难度和问题类型设置实验来研究模型的性能。", "result": "研究指出现有的多模态大语言模型（MLLMs）在处理由手持设备拍摄的真实场景数学问题图片时表现出色，但这些模型在实际K-12教育环境中效果不佳。为此，研究人员创建了MathReal数据集，以评估这些模型在真实场景中的数学推理能力。实验显示，现有模型在处理这些问题时存在局限性，研究同时提供了关于其性能分析和未来改进方向的见解。", "conclusion": "基于大量的实验，研究人员发现现有的MLLMs在真实的教育环境中的问题解决能力受到显著挑战，提出了一套详细的性能分析，总结了模型错误模式，以便为未来改进计划提供方向。"}}
{"id": "2508.06220", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06220", "abs": "https://arxiv.org/abs/2508.06220", "authors": ["Keummin Ka", "Junhyeong Park", "Jahyun Jeon", "Youngjae Yu"], "title": "InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?", "comment": "14 pages, 9 figures", "summary": "Recent advances in Vision-Language Models (VLMs) have demonstrated impressive\ncapabilities in perception and reasoning. However, the ability to perform\ncausal inference -- a core aspect of human cognition -- remains underexplored,\nparticularly in multimodal settings. In this study, we introduce InfoCausalQA,\na novel benchmark designed to evaluate causal reasoning grounded in\ninfographics that combine structured visual data with textual context. The\nbenchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning\nbased on inferred numerical trends, while Task 2 targets semantic causal\nreasoning involving five types of causal relations: cause, effect,\nintervention, counterfactual, and temporal. We manually collected 494\ninfographic-text pairs from four public sources and used GPT-4o to generate\n1,482 high-quality multiple-choice QA pairs. These questions were then\ncarefully revised by humans to ensure they cannot be answered based on\nsurface-level cues alone but instead require genuine visual grounding. Our\nexperimental results reveal that current VLMs exhibit limited capability in\ncomputational reasoning and even more pronounced limitations in semantic causal\nreasoning. Their significantly lower performance compared to humans indicates a\nsubstantial gap in leveraging infographic-based information for causal\ninference. Through InfoCausalQA, we highlight the need for advancing the causal\nreasoning abilities of multimodal AI systems.", "AI": {"tldr": "InfoCausalQA是一套基于信息图表的因果推理评估基准，用于测试视觉-语言模型的因果推理能力，显示出这些模型在此方面与人类之间存在显著差距。", "motivation": "尽管视觉-语言模型（VLMs）在感知和推理方面取得了显著进展，但因其在因果推理能力上的不足，特别是在多模式情境下，此次研究旨在填补这一空白。", "method": "介绍了一个名为InfoCausalQA的新基准，用于评估基于信息图表的因果推理能力。此基准包含两个任务：任务1基于推断的数值趋势进行定量因果推理；任务2涉及五种类型的因果关系：原因、结果、干预、反事实和时间关系。", "result": "实验结果显示，当前的VLMs在计算推理和语义因果推理方面的能力都存在局限，其性能远低于人类的表现。", "conclusion": "通过InfoCausalQA，研究指出提升多模态AI系统因果推理能力的重要性。"}}
{"id": "2508.06014", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06014", "abs": "https://arxiv.org/abs/2508.06014", "authors": ["Minsu Kim", "Subin Jeon", "In Cho", "Mijin Yoo", "Seon Joo Kim"], "title": "ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors", "comment": "10 pages, 6 Figures, ICCV 2025", "summary": "Recent advances in novel view synthesis (NVS) have enabled real-time\nrendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle\nwith artifacts and missing regions when rendering from viewpoints that deviate\nfrom the training trajectory, limiting seamless scene exploration. To address\nthis, we propose a 3DGS-based pipeline that generates additional training views\nto enhance reconstruction. We introduce an information-gain-driven virtual\ncamera placement strategy to maximize scene coverage, followed by video\ndiffusion priors to refine rendered results. Fine-tuning 3D Gaussians with\nthese enhanced views significantly improves reconstruction quality. To evaluate\nour method, we present Wild-Explore, a benchmark designed for challenging scene\nexploration. Experiments demonstrate that our approach outperforms existing\n3DGS-based methods, enabling high-quality, artifact-free rendering from\narbitrary viewpoints.\n  https://exploregs.github.io", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.06277", "categories": ["cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.06277", "abs": "https://arxiv.org/abs/2508.06277", "authors": ["Theresa Pekarek Rosin", "Burak Can Kaplan", "Stefan Wermter"], "title": "Large Language Model Data Generation for Enhanced Intent Recognition in German Speech", "comment": "11 pages, 3 figures, accepted at KONVENS 2025", "summary": "Intent recognition (IR) for speech commands is essential for artificial\nintelligence (AI) assistant systems; however, most existing approaches are\nlimited to short commands and are predominantly developed for English. This\npaper addresses these limitations by focusing on IR from speech by elderly\nGerman speakers. We propose a novel approach that combines an adapted Whisper\nASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based\nlanguage models trained on synthetic text datasets generated by three\nwell-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To\nevaluate the robustness of our approach, we generate synthetic speech with a\ntext-to-speech model and conduct extensive cross-dataset testing. Our results\nshow that synthetic LLM-generated data significantly boosts classification\nperformance and robustness to different speaking styles and unseen vocabulary.\nNotably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the\nmuch larger ChatGPT (175B) in dataset quality for German intent recognition.\nOur approach demonstrates that generative AI can effectively bridge data gaps\nin low-resource domains. We provide detailed documentation of our data\ngeneration and training process to ensure transparency and reproducibility.", "AI": {"tldr": "本研究针对德国老年人士的语音意图识别问题，结合了Whisper ASR模型和三个大型语言模型生成的合成文本数据集，并证明了使用合成数据能够提高分类性能和鲁棒性。", "motivation": "为了克服现有意图识别（IR）方法中针对短命令的限制，并且主要针对英语的限制，本研究专注于德国老年人的语音意图识别问题。", "method": "本研究提出了一种新型方法，结合了针对老年德语演讲者适应的Whisper ASR模型，以及基于Transformer的语言模型，这些模型是通过三个著名的大型语言模型（LLM）：LeoLM、Llama3和ChatGPT生成的合成文本数据集训练的。", "result": "研究结果表明，使用由LLM生成的合成数据显著增强了分类性能和对不同说话风格及未见过词汇的鲁棒性。特别是，发现130亿参数的领域特定模型LeoLM在德国意图识别的数据质量上超过了1750亿参数的ChatGPT。", "conclusion": "该方法证明了生成式AI可以有效地填补低资源领域的数据缺口。研究提供了详细的数据生成和训练过程文档，以确保透明和可重复性。"}}
{"id": "2508.06021", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06021", "abs": "https://arxiv.org/abs/2508.06021", "authors": ["Utku Ozbulak", "Michaela Cohrs", "Hristo L. Svilenov", "Joris Vankerschaver", "Wesley De Neve"], "title": "Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis", "comment": null, "summary": "Sub-visible particle analysis using flow imaging microscopy combined with\ndeep learning has proven effective in identifying particle types, enabling the\ndistinction of harmless components such as silicone oil from protein particles.\nHowever, the scarcity of available data and severe imbalance between particle\ntypes within datasets remain substantial hurdles when applying multi-class\nclassifiers to such problems, often forcing researchers to rely on less\neffective methods. The aforementioned issue is particularly challenging for\nparticle types that appear unintentionally and in lower numbers, such as\nsilicone oil and air bubbles, as opposed to protein particles, where obtaining\nlarge numbers of images through controlled settings is comparatively\nstraightforward. In this work, we develop a state-of-the-art diffusion model to\naddress data imbalance by generating high-fidelity images that can augment\ntraining datasets, enabling the effective training of multi-class deep neural\nnetworks. We validate this approach by demonstrating that the generated samples\nclosely resemble real particle images in terms of visual quality and structure.\nTo assess the effectiveness of using diffusion-generated images in training\ndatasets, we conduct large-scale experiments on a validation dataset comprising\n500,000 protein particle images and demonstrate that this approach improves\nclassification performance with no negligible downside. Finally, to promote\nopen research and reproducibility, we publicly release both our diffusion\nmodels and the trained multi-class deep neural network classifiers, along with\na straightforward interface for easy integration into future studies, at\nhttps://github.com/utkuozbulak/svp-generative-ai.", "AI": {"tldr": "本文提出了一种基于扩散模型生成高质量子可见粒子图像的方法，解决训练数据集中的类别不平衡问题，提高了多分类深度学习模型的性能。", "motivation": "为了克服子可见粒子分析中数据稀缺和类别不平衡的问题，特别是在无意中出现的且数量较少的粒子类型（如硅油和气泡）的分类问题。", "method": "我们开发了一种基于扩散模型的状态-of-the-art方法来生成高保真图像，以解决数据不平衡问题并增强训练数据集，从而有效地训练多类深度神经网络分类器。", "result": "通过在含有500,000张蛋白质粒子图像的验证数据集上进行的大规模实验，验证了使用扩散模型生成的图像作为训练数据时，分类性能得到了明显提升。", "conclusion": "我们提出的方法通过生成真实粒子图像，在视觉质量和结构上都有很好的匹配，能够在多类分类问题中有效利用生成的数据来改善分类器的表现，并且我们已经公开了使用的扩散模型和训练的多类深度神经网络分类器。"}}
{"id": "2508.06309", "categories": ["cs.CL", "math.PR"], "pdf": "https://arxiv.org/pdf/2508.06309", "abs": "https://arxiv.org/abs/2508.06309", "authors": ["Ruichong Zhang"], "title": "Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC", "comment": null, "summary": "In recent years, concerns about intellectual property (IP) in large language\nmodels (LLMs) have grown significantly. Plagiarizing other LLMs (through direct\nweight copying, upcycling, pruning, or continual pretraining) and claiming\nauthorship without properly attributing to the original license, is a serious\nmisconduct that can lead to significant financial and reputational harm to the\noriginal developers. However, existing methods for detecting LLM plagiarism\nfall short in key areas. They fail to accurately reconstruct weight\ncorrespondences, lack the ability to compute statistical significance measures\nsuch as $p$-values, and may mistakenly flag models trained on similar data as\nbeing related. To address these limitations, we propose Matrix-Driven Instant\nReview (MDIR), a novel method that leverages matrix analysis and Large\nDeviation Theory. MDIR achieves accurate reconstruction of weight\nrelationships, provides rigorous $p$-value estimation, and focuses exclusively\non weight similarity without requiring full model inference. Experimental\nresults demonstrate that MDIR reliably detects plagiarism even after extensive\ntransformations, such as random permutations and continual pretraining with\ntrillions of tokens. Moreover, all detections can be performed on a single PC\nwithin an hour, making MDIR both efficient and accessible.", "AI": {"tldr": "Introduces MDIR to overcome limitations of existing LLM plagiarism detection methods by providing accurate weight correspondence, $p$-value estimation, and focusing on weight similarity alone.", "motivation": "Existing methods for detecting LLM plagiarism have limitations such as failing to accurately reconstruct weight correspondences and lack of statistical significance measures.", "method": "MDIR, a novel method that leverages matrix analysis and Large Deviation Theory to detect plagiarism in LLMs.", "result": "MDIR reliably detects plagiarism after extensive transformations and can be performed on a single PC within an hour.", "conclusion": "MDIR is efficient, accessible, and overcomes the limitations of current plagiarism detection methods in LLMs."}}
{"id": "2508.06032", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06032", "abs": "https://arxiv.org/abs/2508.06032", "authors": ["Kiran Chhatre", "Christopher Peters", "Srikrishna Karanam"], "title": "Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts", "comment": "16 pages, 11 figures", "summary": "Existing methods for human parsing into body parts and clothing often use\nfixed mask categories with broad labels that obscure fine-grained clothing\ntypes. Recent open-vocabulary segmentation approaches leverage pretrained\ntext-to-image (T2I) diffusion model features for strong zero-shot transfer, but\ntypically group entire humans into a single person category, failing to\ndistinguish diverse clothing or detailed body parts. To address this, we\npropose Spectrum, a unified network for part-level pixel parsing (body parts\nand clothing) and instance-level grouping. While diffusion-based\nopen-vocabulary models generalize well across tasks, their internal\nrepresentations are not specialized for detailed human parsing. We observe\nthat, unlike diffusion models with broad representations, image-driven 3D\ntexture generators maintain faithful correspondence to input images, enabling\nstronger representations for parsing diverse clothing and body parts. Spectrum\nintroduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model --\nobtained by fine-tuning a T2I model on 3D human texture maps -- for improved\nalignment with body parts and clothing. From an input image, we extract\nhuman-part internal features via the I2Tx diffusion model and generate\nsemantically valid masks aligned to diverse clothing categories through\nprompt-guided grounding. Once trained, Spectrum produces semantic segmentation\nmaps for every visible body part and clothing category, ignoring standalone\ngarments or irrelevant objects, for any number of humans in the scene. We\nconduct extensive cross-dataset experiments -- separately assessing body parts,\nclothing parts, unseen clothing categories, and full-body masks -- and\ndemonstrate that Spectrum consistently outperforms baseline methods in\nprompt-based segmentation.", "AI": {"tldr": "Spectrum是一种统一网络，用于解析级别的像素解析（身体部位和衣物）和实例级分组，通过微调一个图像到纹理扩散模型来改进对身体部位和衣物的解析，可以生成语义分割图，准确分割身体和衣物。", "motivation": "现有的人体解析方法通常使用固定且标签粗略的掩模类别，难以解析细粒度的衣物类型。而开放词汇分割方法虽能零样本迁移，但未能区分多样化的衣物或详细的身体部分。因此，提出Spectrum方法来解决这些问题。", "method": "Spectrum方法通过对一个图像到纹理（I2Tx）扩散模型进行微调来改进身体部位和衣物的解析，该模型是在3D人体纹理图上对文本到图像（T2I）模型进行微调得到的。通过从输入图像中提取人体部位内部特征，并通过提示引导进行语义有效的掩模生成，以对多样化的衣物分类进行对齐。", "result": "在广泛的跨数据集实验中，Spectrum方法对身体部位、衣物部分、未见过的衣物类别以及全身掩模的评估显示，它在基于提示的分割任务中始终优于基线方法。", "conclusion": "Spectrum方法展示了在多个人体解析任务中，通过调整和利用3D纹理生成模型的内部表示，可以提高对细粒度人体分割的性能，优于现有方法。"}}
{"id": "2508.06345", "categories": ["cs.CL", "cs.AI", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06345", "abs": "https://arxiv.org/abs/2508.06345", "authors": ["Yanbin Wei", "Jiangyue Yan", "Chun Kang", "Yang Chen", "Hua Liu", "James T. Kwok", "Yu Zhang"], "title": "Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering", "comment": null, "summary": "Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities\nin diverse domain question-answering (QA) tasks, including graph QA that\ninvolves complex graph topologies. However, most current approaches use only a\nsingle type of graph representation, namely Topology Representation Form (TRF),\nsuch as prompt-unified text descriptions or style-fixed visual styles. Those\n\"one-size-fits-all\" approaches fail to consider the specific preferences of\ndifferent models or tasks, often leading to incorrect or overly long responses.\nTo address this, we first analyze the characteristics and weaknesses of\nexisting TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to\nzero-shot graph QA. We then introduce a new metric, Graph Response Efficiency\n(GRE), which measures the balance between the performance and the brevity in\ngraph QA. Built on these, we develop the DynamicTRF framework, which aims to\nimprove both the accuracy and conciseness of graph QA. To be specific,\nDynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based\non their GRE scores, to probe the question-specific TRF preferences. Then it\ntrains a TRF router on the TRFP dataset, to adaptively assign the best TRF from\n$F_{ZS}$ for each question during the inference. Extensive experiments across 7\nin-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show\nthat DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms\nof accuracy", "AI": {"tldr": "通过动态调整图表示形式，该论文提出了一种提高大模型在零样本图问答任务中准确性和简洁性的框架。", "motivation": "论文指出现有方法普遍使用单一的图表示形式，忽略了模型或任务的具体偏好，从而导致回答错误或冗长的问题。", "method": "提出了多种零样本图问答的图表示形式$F_{ZS}$，引入了一种新的评估指标Graph Response Efficiency (GRE)，开发了DynamicTRF框架，通过训练一个图表示路由器以自适应选择最佳图表示形式。", "result": "在7个领域的算法图问答任务中和2个跨领域的下游任务中，DynamicTRF显著提升了大模型零样本图问答的准确性。", "conclusion": "通过自适应适合不同问题的图表示形式，DynamicTRF在提高零样本图问答的准确性和简洁性方面表现出显著的优势。"}}
{"id": "2508.06033", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06033", "abs": "https://arxiv.org/abs/2508.06033", "authors": ["Yiming Gong", "Zhen Zhu", "Minjia Zhang"], "title": "InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow", "comment": "ICCV 2025", "summary": "We propose a fast text-guided image editing method called InstantEdit based\non the RectifiedFlow framework, which is structured as a few-step editing\nprocess that preserves critical content while following closely to textual\ninstructions. Our approach leverages the straight sampling trajectories of\nRectifiedFlow by introducing a specialized inversion strategy called PerRFI. To\nmaintain consistent while editable results for RectifiedFlow model, we further\npropose a novel regeneration method, Inversion Latent Injection, which\neffectively reuses latent information obtained during inversion to facilitate\nmore coherent and detailed regeneration. Additionally, we propose a\nDisentangled Prompt Guidance technique to balance editability with detail\npreservation, and integrate a Canny-conditioned ControlNet to incorporate\nstructural cues and suppress artifacts. Evaluation on the PIE image editing\ndataset demonstrates that InstantEdit is not only fast but also achieves better\nqualitative and quantitative results compared to state-of-the-art few-step\nediting methods.", "AI": {"tldr": "A novel, fast, and effective text-guided image editing method named InstantEdit is proposed, leveraging RectifiedFlow framework, specialized inversion and regeneration strategies, and additional techniques for better detail preservation.", "motivation": "The motivation behind InstantEdit is to create a method that can quickly edit images based on textual instructions while maintaining the quality and critical content of the original image. This aims to improve upon current few-step editing methods.", "method": "We propose a method called InstantEdit, which utilizes the RectifiedFlow framework for fast text-guided image editing. The method introduces a specialized inversion strategy named PerRFI and a regeneration method, Inversion Latent Injection, to ensure consistent and detailed regeneration. Additionally, it integrates Disentangled Prompt Guidance and a Canny-conditioned ControlNet to balance editability and preserve details.", "result": "Evaluation on the PIE image editing dataset showed that InstantEdit not only achieves faster editing but also delivers superior qualitative and quantitative results compared to the state-of-the-art few-step editing methods.", "conclusion": "The proposed InstantEdit method effectively combines rapid text-guided editing with high-quality results, outperforming existing few-step image editing techniques."}}
{"id": "2508.06360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06360", "abs": "https://arxiv.org/abs/2508.06360", "authors": ["Aisha Saeid", "Anu Sabu", "Girish A. Koushik", "Ferrante Neri", "Diptesh Kanojia"], "title": "Cyberbullying Detection via Aggression-Enhanced Prompting", "comment": "Accepted to RANLP 2025", "summary": "Detecting cyberbullying on social media remains a critical challenge due to\nits subtle and varied expressions. This study investigates whether integrating\naggression detection as an auxiliary task within a unified training framework\ncan enhance the generalisation and performance of large language models (LLMs)\nin cyberbullying detection. Experiments are conducted on five aggression\ndatasets and one cyberbullying dataset using instruction-tuned LLMs. We\nevaluated multiple strategies: zero-shot, few-shot, independent LoRA\nfine-tuning, and multi-task learning (MTL). Given the inconsistent results of\nMTL, we propose an enriched prompt pipeline approach in which aggression\npredictions are embedded into cyberbullying detection prompts to provide\ncontextual augmentation. Preliminary results show that the enriched prompt\npipeline consistently outperforms standard LoRA fine-tuning, indicating that\naggression-informed context significantly boosts cyberbullying detection. This\nstudy highlights the potential of auxiliary tasks, such as aggression\ndetection, to improve the generalisation of LLMs for safety-critical\napplications on social networks.", "AI": {"tldr": "研究探讨了将攻击性言论检测作为辅助任务，整合到统一训练框架中，以提高大型语言模型（LLMs）在社交媒体上的网络欺凌检测性能。实验表明，在几种测试策略中，嵌入了攻击性预测信息的增强型提示流水线方法表现最佳。这表明带有攻击性信息的上下文能显著提升网络欺凌的检测效果。", "motivation": "社交媒体上网络欺凌的检测一直是个重要且挑战性的问题，因为网络欺凌表现形式微妙且各异。研究试图通过加入攻击性言论检测作为辅助任务来提高大型语言模型在该检测任务上的表现和泛化能力。", "method": "研究通过零样本、少量样本、独立LoRA微调和多任务学习（MTL）等策略，在包含五个攻击性言论数据集和一个网络欺凌数据集上测试了指令调优后的LLMs性能。最终提出了嵌入了攻击性预测信息的增强型提示流水线方法。", "result": "", "conclusion": "初步结果显示，带攻击性信息的增强型提示流水线方法比标准LoRA微调方法表现更好。这表明辅助任务，如攻击性言论检测，对于改善LLMs在社交媒体上的安全关键应用的泛化能力具有潜在价值。"}}
{"id": "2508.06036", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06036", "abs": "https://arxiv.org/abs/2508.06036", "authors": ["Jun Xie", "Yingjian Zhu", "Feng Chen", "Zhenghao Zhang", "Xiaohui Fan", "Hongzhu Yi", "Xinming Wang", "Chen Yu", "Yue Bi", "Zhaoran Zhao", "Xiongjun Guan", "Zhepeng Wang"], "title": "More Is Better: A MoE-Based Emotion Recognition Framework with Human Preference Alignment", "comment": null, "summary": "In this paper, we present our solution for the semi-supervised learning track\n(MER-SEMI) in MER2025. We propose a comprehensive framework, grounded in the\nprinciple that \"more is better,\" to construct a robust Mixture of Experts (MoE)\nemotion recognition system. Our approach integrates a diverse range of input\nmodalities as independent experts, including novel signals such as knowledge\nfrom large Vision-Language Models (VLMs) and temporal Action Unit (AU)\ninformation. To effectively utilize unlabeled data, we introduce a\nconsensus-based pseudo-labeling strategy, generating high-quality labels from\nthe agreement between a baseline model and Gemini, which are then used in a\ntwo-stage training paradigm. Finally, we employ a multi-expert voting ensemble\ncombined with a rule-based re-ranking process to correct prediction bias and\nbetter align the outputs with human preferences. Evaluated on the MER2025-SEMI\nchallenge dataset, our method achieves an F1-score of 0.8772 on the test set,\nranking 2nd in the track. Our code is available at\nhttps://github.com/zhuyjan/MER2025-MRAC25.", "AI": {"tldr": "本文提出了一种基于“量多质优”原则的专家混合情绪识别系统，整合了多种输入模态，通过基于共识的伪标签策略和两阶段训练范式，提高了情绪识别的准确率，在MER2025挑战中取得第二名的成绩。", "motivation": "本文旨在解决MER2025的半监督学习问题，提出一个更加全面的框架，以创建一种更加鲁棒的专家混合情绪识别系统。", "method": "我们提出了一种基于“量多质优”原则的专家混合系统框架，整合了多种输入模态，包括来自大型视觉语言模型(VLMs)的知识和时间动作单元(AU)信息。我们还引入了一种基于共识的伪标签策略来有效利用未标记的数据，并采用了两阶段训练范式。最后，使用多专家投票集成结合基于规则的重新排序过程以校正预测偏差并更好地与人类偏好对齐。", "result": "在MER2025-SEMI挑战数据集上，我们的方法在测试集上实现了0.8772的F1得分，排名第二。", "conclusion": "实验结果证明了我们提出的方法的有效性，其代码已在GitHub上公开。"}}
{"id": "2508.06374", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06374", "abs": "https://arxiv.org/abs/2508.06374", "authors": ["Anubhav Jangra", "Bahareh Sarrafzadeh", "Adrian de Wynter", "Silviu Cucerzan", "Sujay Kumar Jauhar"], "title": "Evaluating Style-Personalized Text Generation: Challenges and Directions", "comment": null, "summary": "While prior research has built tools and benchmarks towards style\npersonalized text generation, there has been limited exploration of evaluation\nin low-resource author style personalized text generation space. Through this\nwork, we question the effectiveness of the widely adopted evaluation metrics\nlike BLEU and ROUGE, and explore other evaluation paradigms such as style\nembeddings and LLM-as-judge to holistically evaluate the style personalized\ntext generation task. We evaluate these metrics and their ensembles using our\nstyle discrimination benchmark, that spans eight writing tasks, and evaluates\nacross three settings, domain discrimination, authorship attribution, and LLM\npersonalized vs non-personalized discrimination. We provide conclusive evidence\nto adopt ensemble of diverse evaluation metrics to effectively evaluate style\npersonalized text generation.", "AI": {"tldr": "研究提出了新的评估方法，采用多样化评估指标组合，以更好地评估低资源条件下的风格个性化文本生成任务。", "motivation": "此研究质疑广泛采用的评价指标如BLEU和ROUGE在低资源作者风格个性化文本生成任务中的有效性，探索其他评价范式如风格嵌入和LLM评分者。", "method": "通过构建风格识别基准，作者评估了多种评价指标及其组合，该基准涵盖了八个写作任务，并在三个设置下进行评估：领域识别、作者归属和LLM个性化与非个性化对比。", "result": "研究发现，采用多样化的评估指标组合可以更有效地评估风格个性化的文本生成任务。", "conclusion": "结论是，为了有效评估风格个性化的文本生成，应采用多样化的评估指标组合。"}}
{"id": "2508.06038", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06038", "abs": "https://arxiv.org/abs/2508.06038", "authors": ["Huanyu Wang", "Jushi Kai", "Haoli Bai", "Lu Hou", "Bo Jiang", "Ziwei He", "Zhouhan Lin"], "title": "Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models", "comment": "12 pages, 4 figures", "summary": "Vision-Language Models (VLMs) typically replace the predefined image\nplaceholder token (<image>) in textual instructions with visual features from\nan image encoder, forming the input to a backbone Large Language Model (LLM).\nHowever, the large number of vision tokens significantly increases the context\nlength, leading to high computational overhead and inference latency. While\nprevious efforts mitigate this by selecting only important visual features or\nleveraging learnable queries to reduce token count, they often compromise\nperformance or introduce substantial extra costs. In response, we propose\nFourier-VLM, a simple yet efficient method that compresses visual\nrepresentations in the frequency domain. Our approach is motivated by the\nobservation that vision features output from the vision encoder exhibit\nconcentrated energy in low-frequency components. Leveraging this, we apply a\nlow-pass filter to the vision features using a two-dimentional Discrete Cosine\nTransform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier\nTransform (FFT) operator with a time complexity of $\\mathcal{O}(n\\log n)$,\nminimizing the extra computational cost while introducing no additional\nparameters. Extensive experiments across various image-based benchmarks\ndemonstrate that Fourier-VLM achieves competitive performance with strong\ngeneralizability across both LLaVA and Qwen-VL architectures. Crucially, it\nreduce inference FLOPs by up to 83.8% and boots generation speed by 31.2%\ncompared to LLaVA-v1.5, highlighting the superior efficiency and practicality.", "AI": {"tldr": "Fourier-VLM 通过离散余弦变换在频域压缩视觉特征，实现了模型的高效性和高泛化能力，同时减少了推理 FLOPs 并提高了生成效率。", "motivation": "Fourier-VLM 的动机在于解决传统视觉-语言模型中的视觉标记过多使上下文长度增加的问题，从而导致计算开销大和延迟高的问题。过去的方法通过选择重要的视觉特征或使用可学习查询来减少标记数量，但这会导致性能下降或引入额外的成本。而 Fourier-VLM 通过频域内的变换方法，有效解决了这些问题。", "method": "Fourier-VLM 提出了一种简单而高效的方法，通过使用二维离散余弦变换（DCT）在频域中压缩视觉表示。这种方法利用了视觉编码器输出的视觉特征在低频分量中的集中能量。通过低通滤波，该方法不会增加额外的参数，并且计算效率很高，时间复杂度为 $\\mathcal{O}(n\\log n)$。", "result": "实验表明，Fourier-VLM 在各种基于图像的基准测试中展示了较强的泛化能力，并与 LLava 和 Qwen-VL 架构兼容。尤其相较于 LLaVA-v1.5，它减少了 83.8% 的推理 FLOPs 并将生成速度提高了 31.2%。", "conclusion": "Fourier-VLM 方法在不降低性能的前提下，显著减少了视觉-语言模型中的计算复杂度和推理延迟，提高了生成效率。"}}
{"id": "2508.06388", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06388", "abs": "https://arxiv.org/abs/2508.06388", "authors": ["Lanlan Qiu", "Xiao Pu", "Yeqi Feng", "Tianxing He"], "title": "LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing", "comment": "21 pages, 17 figures, 3 tables", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nrole-playing conversations and providing emotional support as separate research\ndirections. However, there remains a significant research gap in combining\nthese capabilities to enable emotionally supportive interactions with virtual\ncharacters. To address this research gap, we focus on anime characters as a\ncase study because of their well-defined personalities and large fan bases.\nThis choice enables us to effectively evaluate how well LLMs can provide\nemotional support while maintaining specific character traits. We introduce\nChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We\nfirst thoughtfully select 20 top-tier characters from popular anime communities\nand design 60 emotion-centric real-world scenario questions. Then, we execute a\nnationwide selection process to identify 40 Chinese anime enthusiasts with\nprofound knowledge of specific characters and extensive experience in\nrole-playing. Next, we systematically collect two rounds of dialogue data from\n10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP\nperformance of LLMs, we design a user experience-oriented evaluation system\nfeaturing 9 fine-grained metrics across three dimensions: basic dialogue,\nrole-playing and emotional support, along with an overall metric for response\ndiversity. In total, the dataset comprises 2,400 human-written and 24,000\nLLM-generated answers, supported by over 132,000 human annotations.\nExperimental results show that top-performing LLMs surpass human fans in\nrole-playing and emotional support, while humans still lead in response\ndiversity. We hope this work can provide valuable resources and insights for\nfuture research on optimizing LLMs in ESRP. Our datasets are available at\nhttps://github.com/LanlanQiu/ChatAnime.", "AI": {"tldr": "null", "motivation": "null", "method": "null", "result": "{\"tldr\": \"\\u8be5\\u7406\\u8bba\\u672c\\u9020\\u5efa\\u4e86\\u4e00\\u4e2a\\u547d\\u540d\\u4e3aChatAnime\\u7684\\u6570\\u636e\\u9762\\uff0c\\u8be5\\u6570\\u636e\\u9762\\u67092400\\u4e2a\\u4eba\\u5de5\\u5199\\u56de\\u7b54\\u548c24000\\u4e2a\\u6709\\u6548\\u7684\\u5927\\u578b\\u8bed\\u6587\\u6a21\\u578b\\u56de\\u7b54\\uff0c\\u7528\\u4e8e\\u4e25\\u88c1\\u8f93\\u51fa\\u6709\\u6548\\u7684\\u547d\\u540d\\u89d2\\u8272\\u548c\\u611b\\u60c5\\u652f\\u6491\\u7684\\u5bf9\\u活应用的动漫角色和情感支持。\", \"motivation\": \"\\u7406\\u8bba\\u7684\\u52a0\\u5165\\u529b\\u662f\\u5728LLMs中补充上情感支持和角色扮演的能力，目前在动漫角色扮演的情感支持虚拟交互上有研究空缺。\", \"method\": \"\\u4ece20\\u4e2a\\u5148\\u5148\\u7ea7\\u7684\\u7f8e\\u79c0\\u89d2\\u8272\\u4e2d\\u9009\\u62e9\\u51fa\\u76f8\\u5f53\\u8840\\u7528\\u7684\\u5143\\u7d20\\u53ee\\u9762\\uff0c\\u5e76\\u5728\\u5143\\u7d20\\u89d2\\u8272\\u548c\\u5b66\\u8003\\u8005\\u4e4b\\u95f4\\u8fdb\\u884c\\u5bf9\\u8baf\\u540e\\uff0c \\u6536\\u96c6\\u4e86两轮的对话数据。\", \"result\": \"\\u5b9e\\u9a8c\\u6570\\u636e\\u4e2d\\uff0c\\u4e0a\\u4e00\\u7ea7\\u6a21\\u578b\\u7684\\u5927\\u578b\\u8bed\\u6587\\u6a21\\u578b\\uff08LLM\\uff09\\u5728\\u89d2\\u8272\\u5e38\\u4e0e\\u611b\\u60c5\\u652f\\u6491\\u4e0a\\u8d85\\u8d8a\\u4eba\\u7c7b\\u5a21\\u4e60\\uff0c\\u4f46\\u4eba\\u7c7b\\u5148\\u6765\\u5728\\u5bf9\\u8baf\\u591a\\u6a21\\u578b\\u6709\\u4e00\\u5b9a\\u7684\\u4f18\\u52bf。\", \"conclusion\": \"\\u8be5\\u5de5\\u4f5c\\u4ece\\u8003\\u7a76\\u89d2\\u8272\\u5143\\u7d20\\u4e2d\\u9009\\u62e9\\uff0c\\u53ea\\u7b97\\u662f\\u4e25\\u88c1\\u5e0c\\u5145\\u7684LNMs\\u5e02\\u573a\\uff0c\\u5e76\\u8868\\u660e\\u4e86\\u6b64\\u65b9\\u6cd5\\u5728ESRP环方面的表现。\"}", "conclusion": "null"}}
{"id": "2508.06044", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06044", "abs": "https://arxiv.org/abs/2508.06044", "authors": ["Huimin Wu", "Xiaojian Ma", "Haozhe Zhao", "Yanpeng Zhao", "Qing Li"], "title": "NEP: Autoregressive Image Editing via Next Editing Token Prediction", "comment": "The project page is: https://nep-bigai.github.io/", "summary": "Text-guided image editing involves modifying a source image based on a\nlanguage instruction and, typically, requires changes to only small local\nregions. However, existing approaches generate the entire target image rather\nthan selectively regenerate only the intended editing areas. This results in\n(1) unnecessary computational costs and (2) a bias toward reconstructing\nnon-editing regions, which compromises the quality of the intended edits. To\nresolve these limitations, we propose to formulate image editing as Next\nEditing-token Prediction (NEP) based on autoregressive image generation, where\nonly regions that need to be edited are regenerated, thus avoiding unintended\nmodification to the non-editing areas. To enable any-region editing, we propose\nto pre-train an any-order autoregressive text-to-image (T2I) model. Once\ntrained, it is capable of zero-shot image editing and can be easily adapted to\nNEP for image editing, which achieves a new state-of-the-art on widely used\nimage editing benchmarks. Moreover, our model naturally supports test-time\nscaling (TTS) through iteratively refining its generation in a zero-shot\nmanner. The project page is: https://nep-bigai.github.io/", "AI": {"tldr": "为了改进文本引导的图像编辑方法，我们提出了NEP方法以及一个预先训练的T2I模型，实现了零样本的图像编辑，并在编辑基准上达到了新的最先进水平。", "motivation": "现有的方法通常会生成整个目标图像，而不是选择性地重新生成仅有的编辑区域。这不仅导致了不必要的计算成本，还使得非编辑区域的重建偏向影响了所需编辑的质量。", "method": "我们提出将图像编辑问题形式化为基于自回归图像生成的下一次编辑标记预测（NEP），这种方法只重新生成需要编辑的区域，避免了非编辑区域的无意修改。为此，我们提出了预训练一个可以任意顺序生成的自回归文本到图像（T2I）模型。一旦训练完成，它就能进行零样本图像编辑，并可以轻易地适应用于NEP的图像编辑任务。", "result": "我们的模型在广泛使用的图像编辑基准上实现了新的最先进水平。此外，我们的模型自然支持测试时放缩（TTS），通过零样本的方式迭代地改进生成效果。", "conclusion": "通过NEP方法和预先训练的T2I模型，我们解决了现有方法在图像编辑中的计算成本高和编辑质量差的问题。"}}
{"id": "2508.06418", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06418", "abs": "https://arxiv.org/abs/2508.06418", "authors": ["Haoran Shi", "Hongwei Yao", "Shuo Shao", "Shaopeng Jiao", "Ziqi Peng", "Zhan Qin", "Cong Wang"], "title": "Quantifying Conversation Drift in MCP via Latent Polytope", "comment": null, "summary": "The Model Context Protocol (MCP) enhances large language models (LLMs) by\nintegrating external tools, enabling dynamic aggregation of real-time data to\nimprove task execution. However, its non-isolated execution context introduces\ncritical security and privacy risks. In particular, adversarially crafted\ncontent can induce tool poisoning or indirect prompt injection, leading to\nconversation hijacking, misinformation propagation, or data exfiltration.\nExisting defenses, such as rule-based filters or LLM-driven detection, remain\ninadequate due to their reliance on static signatures, computational\ninefficiency, and inability to quantify conversational hijacking. To address\nthese limitations, we propose SecMCP, a secure framework that detects and\nquantifies conversation drift, deviations in latent space trajectories induced\nby adversarial external knowledge. By modeling LLM activation vectors within a\nlatent polytope space, SecMCP identifies anomalous shifts in conversational\ndynamics, enabling proactive detection of hijacking, misleading, and data\nexfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3,\nVicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA),\ndemonstrating robust detection with AUROC scores exceeding 0.915 while\nmaintaining system usability. Our contributions include a systematic\ncategorization of MCP security threats, a novel latent polytope-based\nmethodology for quantifying conversation drift, and empirical validation of\nSecMCP's efficacy.", "AI": {"tldr": "本文提出了SecMCP以解决MCP实施中引入的安全和隐私风险，通过潜在多面体空间中的模型检测异常活动，有效识别和量化可控的对话偏移。", "motivation": "增强型模型上下文协议（MCP）通过集成外部工具增强了大语言模型（LLMs），这引入了重要的安全和隐私风险。为了解决工具中毒或间接提示注入等问题，提出了SecMCP框架。", "method": "通过建模大语言模型（LLMs）的激活向量到潜在多面体空间，SecMCP能够识别在对话动态中由敌对的外部知识引起的异常偏移，从而实现对对话劫持、误导和数据泄露的主动检测。", "result": "在三个最先进的LLMs（Llama3, Vicuna, Mistral）上对SecMCP进行了评估，结果表明其检测效果稳健，AUROC得分超过0.915，同时保持了系统可用性。", "conclusion": "该研究贡献包括对MCP安全威胁的系统分类、一种新的基于潜在多面体的方法来量化对话偏离，以及SecMCP的有效性实证验证。"}}
{"id": "2508.06051", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06051", "abs": "https://arxiv.org/abs/2508.06051", "authors": ["Linhan Cao", "Wei Sun", "Weixia Zhang", "Xiangyang Zhu", "Jun Jia", "Kaiwei Zhang", "Dandan Zhu", "Guangtao Zhai", "Xiongkuo Min"], "title": "VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning", "comment": null, "summary": "Video quality assessment (VQA) aims to objectively quantify perceptual\nquality degradation in alignment with human visual perception. Despite recent\nadvances, existing VQA models still suffer from two critical limitations:\n\\textit{poor generalization to out-of-distribution (OOD) videos} and\n\\textit{limited explainability}, which restrict their applicability in\nreal-world scenarios. To address these challenges, we propose\n\\textbf{VQAThinker}, a reasoning-based VQA framework that leverages large\nmultimodal models (LMMs) with reinforcement learning to jointly model video\nquality understanding and scoring, emulating human perceptual decision-making.\nSpecifically, we adopt group relative policy optimization (GRPO), a rule-guided\nreinforcement learning algorithm that enables reasoning over video quality\nunder score-level supervision, and introduce three VQA-specific rewards: (1) a\n\\textbf{bell-shaped regression reward} that increases rapidly as the prediction\nerror decreases and becomes progressively less sensitive near the ground truth;\n(2) a \\textbf{pairwise ranking reward} that guides the model to correctly\ndetermine the relative quality between video pairs; and (3) a \\textbf{temporal\nconsistency reward} that encourages the model to prefer temporally coherent\nvideos over their perturbed counterparts. Extensive experiments demonstrate\nthat VQAThinker achieves state-of-the-art performance on both in-domain and OOD\nVQA benchmarks, showing strong generalization for video quality scoring.\nFurthermore, evaluations on video quality understanding tasks validate its\nsuperiority in distortion attribution and quality description compared to\nexisting explainable VQA models and LMMs. These findings demonstrate that\nreinforcement learning offers an effective pathway toward building\ngeneralizable and explainable VQA models solely with score-level supervision.", "AI": {"tldr": "提出VQAThinker，一种基于推理的视频质量评估框架，通过大规模多模态模型和强化学习解决现有VQA模型在面对分布外视频时的泛化问题和可解释性问题。", "motivation": "解决现有VQA模型在面对分布外视频时的泛化能力和可解释性较差的问题。", "method": "采用组相对策略优化（GRPO）算法，引入了三个特定的VQA奖励：钟形回归奖励，成对排名奖励，时间一致性奖励。", "result": "实验表明，VQAThinker在领域内和领域外的VQA基准测试中实现了最先进的性能，显示了对视频质量评分的强大泛化能力。除此之外，该模型在视频质量理解和归因方面优于现有的可解释VQA模型和LMMs。", "conclusion": "强化学习为构建仅依靠评分水平监督的泛化性和可解释性的VQA模型提供了一种有效的途径。"}}
{"id": "2508.06433", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06433", "abs": "https://arxiv.org/abs/2508.06433", "authors": ["Runnan Fang", "Yuan Liang", "Xiaobin Wang", "Jialong Wu", "Shuofei Qiao", "Pengjun Xie", "Fei Huang", "Huajun Chen", "Ningyu Zhang"], "title": "Memp: Exploring Agent Procedural Memory", "comment": "Work in progress", "summary": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they\nsuffer from brittle procedural memory that is manually engineered or entangled\nin static parameters. In this work, we investigate strategies to endow agents\nwith a learnable, updatable, and lifelong procedural memory. We propose Memp\nthat distills past agent trajectories into both fine-grained, step-by-step\ninstructions and higher-level, script-like abstractions, and explore the impact\nof different strategies for Build, Retrieval, and Update of procedural memory.\nCoupled with a dynamic regimen that continuously updates, corrects, and\ndeprecates its contents, this repository evolves in lockstep with new\nexperience. Empirical evaluation on TravelPlanner and ALFWorld shows that as\nthe memory repository is refined, agents achieve steadily higher success rates\nand greater efficiency on analogous tasks. Moreover, procedural memory built\nfrom a stronger model retains its value: migrating the procedural memory to a\nweaker model yields substantial performance gains.", "AI": {"tldr": "本文提出了Memp方法，用于改进代理的记忆存储方式，通过提炼步骤指令和高层脚本，提高了代理在不同任务中的成功率和效率，且该记忆模型可以迁移到其他模型上，带来性能的提升。", "motivation": "大型语言模型（LLMs）代理在多种任务中表现出色，但由于其固有的程序记忆通常是手动编程或是静态参数的一部分，因此存在脆弱的问题。本文旨在研究如何赋予代理一种可学习、可更新的终身程序记忆。", "method": "我们提出了一种名为Memp的方法，它可以将过去的代理轨迹提炼成详细的分步指令和高层次的脚本抽象形式，并探讨了不同构建、检索和更新过程记忆策略的影响。", "result": "实验证明，在TravelPlanner和ALFWorld上的代理随着过程记忆库的不断优化获得了更高的成功率和更优的效率。此外，由更强模型生成的过程记忆迁移到较弱模型后，也能带来性能的显著提升。", "conclusion": "实验结果表明，随着记忆库的不断优化，代理的成功率和效率也在不断提高。此外，一个更强大的模型生成的过程记忆迁移到一个较弱的模型上，仍能显著提高性能。"}}
{"id": "2508.06055", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.06055", "abs": "https://arxiv.org/abs/2508.06055", "authors": ["Wonjung Park", "Suhyun Ahn", "Jinah Park"], "title": "LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study on Alzheimer's disease, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing", "comment": null, "summary": "Lateral ventricle (LV) shape analysis holds promise as a biomarker for\nneurological diseases; however, challenges remain due to substantial shape\nvariability across individuals and segmentation difficulties arising from\nlimited MRI resolution. We introduce LV-Net, a novel framework for producing\nindividualized 3D LV meshes from brain MRI by deforming an anatomy-aware joint\nLV-hippocampus template mesh. By incorporating anatomical relationships\nembedded within the joint template, LV-Net reduces boundary segmentation\nartifacts and improves reconstruction robustness. In addition, by classifying\nthe vertices of the template mesh based on their anatomical adjacency, our\nmethod enhances point correspondence across subjects, leading to more accurate\nLV shape statistics. We demonstrate that LV-Net achieves superior\nreconstruction accuracy, even in the presence of segmentation imperfections,\nand delivers more reliable shape descriptors across diverse datasets. Finally,\nwe apply LV-Net to Alzheimer's disease analysis, identifying LV subregions that\nshow significantly associations with the disease relative to cognitively normal\ncontrols. The codes for LV shape modeling are available at\nhttps://github.com/PWonjung/LV_Shape_Modeling.", "AI": {"tldr": "提出LV-Net，一种新的框架，用于生成个性化的3D侧脑室网格，表现出较高的重建精度并能在各种数据集中提供可靠的形状描述符。该技术还识别出色的与阿尔茨海默病有关的侧脑室亚区域。", "motivation": "侧脑室形状分析在作为神经系统疾病生物标志物方面展现出潜力，但由于个体间形状变异性和MRI分辨率限制导致的分割困难，仍存在挑战。", "method": "引入了LV-Net，这是一种新的框架，用于从脑部MRI中生成个性化的3D侧脑室网格。通过改变一个解剖结构感知的联合侧脑室-海马体模板网格，该方法减少了边界分割伪影并提高了重建鲁棒性。通过根据模板网格顶点的解剖相邻性对其进行分类来增强点对应关系，这提高了跨受试者的侧脑室形状统计的准确性。", "result": "LV-Net 在存在分割不完美的情况下实现了优越的重建精度，并在各种数据集中提供了更可靠的形状描述符。", "conclusion": "LV-Net 被应用于阿尔茨海默病的分析，识别出与正常认知对照组相比较显示出显著关联的侧脑室亚区域。这些结果表明LV-Net在侧脑室分析中具有实用价值。"}}
{"id": "2508.06435", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06435", "abs": "https://arxiv.org/abs/2508.06435", "authors": ["Andrea Nasuto", "Stefano Maria Iacus", "Francisco Rowe", "Devika Jain"], "title": "Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages", "comment": null, "summary": "Large language models (LLMs) are transforming social-science research by\nenabling scalable, precise analysis. Their adaptability raises the question of\nwhether knowledge acquired through fine-tuning in a few languages can transfer\nto unseen languages that only appeared during pre-training. To examine this, we\nfine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or\nmultilingual data sets to classify immigration-related tweets from X/Twitter\nacross 13 languages, a domain characterised by polarised, culturally specific\ndiscourse. We evaluate whether minimal language-specific fine-tuning enables\ncross-lingual topic detection and whether adding targeted languages corrects\npre-training biases. Results show that LLMs fine-tuned in one or two languages\ncan reliably classify immigration-related content in unseen languages. However,\nidentifying whether a tweet expresses a pro- or anti-immigration stance\nbenefits from multilingual fine-tuning. Pre-training bias favours dominant\nlanguages, but even minimal exposure to under-represented languages during\nfine-tuning (as little as $9.62\\times10^{-11}$ of the original pre-training\ntoken volume) yields significant gains. These findings challenge the assumption\nthat cross-lingual mastery requires extensive multilingual training: limited\nlanguage coverage suffices for topic-level generalisation, and structural\nbiases can be corrected with lightweight interventions. By releasing\n4-bit-quantised, LoRA fine-tuned models, we provide an open-source,\nreproducible alternative to proprietary LLMs that delivers 35 times faster\ninference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,\nenabling scalable, inclusive research.", "AI": {"tldr": "研究使用轻量级LLaMA模型对跨越13种语言的移民相关推文进行分类，表明即使是少量的语言特化微调也足以在未见过的语言中进行主题检测，但对于立场识别，多语言微调更为有效。", "motivation": "探讨轻量级语言模型是否能在几种语言的微调后，转移到仅在预训练期间见过的其他语言，在移民相关的极化文化特定的讨论中进行分类。测试少量语言特定微调是否能实现跨语言主题检测，并修正预训练偏差。", "method": "使用轻量级LLaMA 3.2-3B模型对单语言、双语言或多语言数据集进行微调，以分类来自X/Twitter的移民相关推文，覆盖13种语言。评估少量语言特定微调对跨语言主题检测的影响，以及覆盖更多语言是否能修正预训练偏差。", "result": "仅在一种或两种语言上微调的模型能够可靠地分类未见过语言的移民相关内容，但多语言微调对于区分移民立场更为有效。即使是极小的量的语言暴露（仅预训练令牌总量的$9.62\times10^{-11}$），也能显著改善以代表不足的语言。", "conclusion": "研究结果挑战了必须大量多语言训练才能实现跨语言掌握的假设，表明有限的语言覆盖足以实现主题层面的泛化，且采用轻量级干预可以纠正结构性偏差。通过发布轻量量化、LoRA微调模型，提出一种开源且成本低廉的替代方案，加速大规模、包容性研究。"}}
{"id": "2508.06057", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06057", "abs": "https://arxiv.org/abs/2508.06057", "authors": ["Mojtaba Valipour", "Kelly Zheng", "James Lowman", "Spencer Szabados", "Mike Gartner", "Bobby Braswell"], "title": "AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?", "comment": "Accepted in IGARSS 2025!", "summary": "Artificial General Intelligence (AGI) is closer than ever to becoming a\nreality, sparking widespread enthusiasm in the research community to collect\nand work with various modalities, including text, image, video, and audio.\nDespite recent efforts, satellite spectral imagery, as an additional modality,\nhas yet to receive the attention it deserves. This area presents unique\nchallenges, but also holds great promise in advancing the capabilities of AGI\nin understanding the natural world. In this paper, we argue why Earth\nObservation data is useful for an intelligent model, and then we review\nexisting benchmarks and highlight their limitations in evaluating the\ngeneralization ability of foundation models in this domain. This paper\nemphasizes the need for a more comprehensive benchmark to evaluate earth\nobservation models. To facilitate this, we propose a comprehensive set of tasks\nthat a benchmark should encompass to effectively assess a model's ability to\nunderstand and interact with Earth observation data.", "AI": {"tldr": "论文讨论了地球观测数据对AGI的重要性，指出现有的评估基准测试的不足，并提出了一套全面的任务集合以改进评估标准。", "motivation": "此论文的动机在于强调地球观测数据在增强AGI理解自然世界能力中的重要性。考虑到卫星光谱图像作为一种独特而未充分利用的模态，作者提出了一套全面的基准测试需求，以解决当前基准测试的局限性。", "method": "在论文中，作者首先阐述了为什么地球观测数据对于智能模型是有用的。接着，他们回顾了现有的基准测试，并指出了这些基准在评估基础模型在地球观测域中的泛化能力方面的局限性。为了促进这一领域的发展，作者提出了一套全面的任务集合，用于评估模型理解和处理地球观测数据的能力。", "result": "研究结果提出了改进地球观测模型评估的全面任务集合，强调了现有方法的局限性。", "conclusion": "结论认为，开发一个能够全面评估地球观测数据理解和处理能力的基准测试对于推进AGI在理解自然世界的整体性能具有重要意义。"}}
{"id": "2508.06445", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06445", "abs": "https://arxiv.org/abs/2508.06445", "authors": ["Abolfazl Ansari", "Delvin Ce Zhang", "Nafis Irtiza Tripto", "Dongwon Lee"], "title": "Echoes of Automation: The Increasing Use of LLMs in Newsmaking", "comment": "To appear in 18th International Conference on Social Computing,\n  Behavioral-Cultural Modeling, & Prediction and Behavior Representation in\n  Modeling and Simulation, and to be published in the Springer LNCS series", "summary": "The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns\nfor journalistic integrity and authorship. This study examines AI-generated\ncontent across over 40,000 news articles from major, local, and college news\nmedia, in various media formats. Using three advanced AI-text detectors (e.g.,\nBinoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of\nGenAI use in recent years, especially in local and college news. Sentence-level\nanalysis reveals LLMs are often used in the introduction of news, while\nconclusions usually written manually. Linguistic analysis shows GenAI boosts\nword richness and readability but lowers formality, leading to more uniform\nwriting styles, particularly in local media.", "AI": {"tldr": "研究发现近年来，AI生成的内容在新闻领域中的使用显著增加，尤其影响地方和大学新闻，提升了词汇丰富度和可读性但降低了正式性，改变了地方媒体的写作风格。", "motivation": "鉴于生成式人工智能（特别是LLMs）的快速崛起提出了对新闻诚信和作者身份的担忧，本研究旨在探讨其在新闻行业应用中的影响。", "method": "本研究通过对超过40,000篇来自主流、地方和大学新闻媒体的文章分析，使用了三种先进的AI文本检测工具（如Binoculars、Fast-Detect GPT和GPTZero），来检测文本是否由AI生成。", "result": "研究发现近年来GenAI的使用显著增加，特别是在地方和大学新闻中。句子级别的分析显示，LLMs经常被用于新闻的开头部分，而结论部分则通常是手工编写。此外，语言分析表明，GenAI提高了文本的词汇丰富度和可读性，但降低了文本的正式性，导致了更统一的写作风格，特别是在地方媒体中。", "conclusion": "研究表明，GenAI在改善新闻语言的丰富度和可读性方面起到了积极作用，但也带来了风格上的变化，特别是降低了文本的正式程度。这提醒我们，在新闻业中使用生成式AI技术时需谨慎，以保障新闻的诚信和多样性。"}}
{"id": "2508.06058", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06058", "abs": "https://arxiv.org/abs/2508.06058", "authors": ["Shiyang Zhou", "Haijin Zeng", "Yunfan Lu", "Yongyong Chen", "Jie Liu", "Jingyong Su"], "title": "Lightweight Quad Bayer HybridEVS Demosaicing via State Space Augmented Cross-Attention", "comment": null, "summary": "Event cameras like the Hybrid Event-based Vision Sensor (HybridEVS) camera\ncapture brightness changes as asynchronous \"events\" instead of frames, offering\nadvanced application on mobile photography. However, challenges arise from\ncombining a Quad Bayer Color Filter Array (CFA) sensor with event pixels\nlacking color information, resulting in aliasing and artifacts on the\ndemosaicing process before downstream application. Current methods struggle to\naddress these issues, especially on resource-limited mobile devices. In\nresponse, we introduce \\textbf{TSANet}, a lightweight \\textbf{T}wo-stage\nnetwork via \\textbf{S}tate space augmented cross-\\textbf{A}ttention, which can\nhandle event pixels inpainting and demosaicing separately, leveraging the\nbenefits of dividing complex tasks into manageable subtasks. Furthermore, we\nintroduce a lightweight Cross-Swin State Block that uniquely utilizes\npositional prior for demosaicing and enhances global dependencies through the\nstate space model with linear complexity. In summary, TSANet demonstrates\nexcellent demosaicing performance on both simulated and real data of HybridEVS\nwhile maintaining a lightweight model, averaging better results than the\nprevious state-of-the-art method DemosaicFormer across seven diverse datasets\nin both PSNR and SSIM, while respectively reducing parameter and computation\ncosts by $1.86\\times$ and $3.29\\times$. Our approach presents new possibilities\nfor efficient image demosaicing on mobile devices. Code is available in the\nsupplementary materials.", "AI": {"tldr": "研究人员开发了TSANet，一种轻量级设计的两阶段网络，对于处理基于事件相机（如HybridEVS）的数据在去马赛克过程中表现优越，能够提升移动摄影应用上的表现，同时显著降低了计算复杂度。", "motivation": "解决结合Quad Bayer彩色滤光片阵列（CFA）传感器和缺乏颜色信息的事件像素所带来的时间混叠和伪影问题，特别是在资源受限的移动设备上。", "method": "TSANet, 一种通过状态空间增强交叉注意力的两阶段网络，该方法能够独立处理事件像素的插补和去马赛克，使得复杂任务能够被划分为可以管理的子任务。此外，引入了一种轻量级的交叉Swin状态块，该块利用位置先验信息进行去马赛克，并通过状态空间模型增强全局依赖性，具有线性复杂度。", "result": "TSANet 在HybridEVS的模拟和真实数据上展示了出色的去马赛克性能，与当前的最先进方法DemosaicFormer相比，在七个不同的数据集上，TSANet在PSNR和SSIM上平均有更好的结果，同时将参数和计算成本分别减少了1.86倍和3.29倍。", "conclusion": "该方法为移动设备上高效的图像去马赛克打开了新的可能性。代码在补充材料中提供。"}}
{"id": "2508.06447", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06447", "abs": "https://arxiv.org/abs/2508.06447", "authors": ["Lingkun Long", "Rubing Yang", "Yushi Huang", "Desheng Hui", "Ao Zhou", "Jianlei Yang"], "title": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning", "comment": null, "summary": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance.", "AI": {"tldr": "SlimInfer introduces a dynamic token-pruning mechanism in the inference process of Large Language Models, significantly improving speed and efficiency without affecting performance.", "motivation": "To address the high computational demands for long-context inference in LLMs, where current methods still process the full set of hidden states, leading to inefficiency.", "method": "SlimInfer, a framework that prunes less critical prompt tokens during the forward pass in the inference of Large Language Models to enhance efficiency.", "result": "Achieved up to 2.53x speedup in time-to-first-token (TTFT) and 1.88x reduction in end-to-end latency for LLaMA3.1-8B-Instruct on a single RTX 4090, without performance loss.", "conclusion": "SlimInfer effectively accelerates LLM inference by dynamic fine-grained pruning of hidden state tokens, reducing memory usage and I/O costs without compromising performance."}}
{"id": "2508.06063", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06063", "abs": "https://arxiv.org/abs/2508.06063", "authors": ["Chao Hao", "Zitong Yu", "Xin Liu", "Yuhao Wang", "Weicheng Xie", "Jingang Shi", "Huanjing Yue", "Jingyu Yang"], "title": "Distribution-Specific Learning for Joint Salient and Camouflaged Object Detection", "comment": null, "summary": "Salient object detection (SOD) and camouflaged object detection (COD) are two\nclosely related but distinct computer vision tasks. Although both are\nclass-agnostic segmentation tasks that map from RGB space to binary space, the\nformer aims to identify the most salient objects in the image, while the latter\nfocuses on detecting perfectly camouflaged objects that blend into the\nbackground in the image. These two tasks exhibit strong contradictory\nattributes. Previous works have mostly believed that joint learning of these\ntwo tasks would confuse the network, reducing its performance on both tasks.\nHowever, here we present an opposite perspective: with the correct approach to\nlearning, the network can simultaneously possess the capability to find both\nsalient and camouflaged objects, allowing both tasks to benefit from joint\nlearning. We propose SCJoint, a joint learning scheme for SOD and COD tasks,\nassuming that the decoding processes of SOD and COD have different distribution\ncharacteristics. The key to our method is to learn the respective means and\nvariances of the decoding processes for both tasks by inserting a minimal\namount of task-specific learnable parameters within a fully shared network\nstructure, thereby decoupling the contradictory attributes of the two tasks at\na minimal cost. Furthermore, we propose a saliency-based sampling strategy\n(SBSS) to sample the training set of the SOD task to balance the training set\nsizes of the two tasks. In addition, SBSS improves the training set quality and\nshortens the training time. Based on the proposed SCJoint and SBSS, we train a\npowerful generalist network, named JoNet, which has the ability to\nsimultaneously capture both ``salient\" and ``camouflaged\". Extensive\nexperiments demonstrate the competitive performance and effectiveness of our\nproposed method. The code is available at https://github.com/linuxsino/JoNet.", "AI": {"tldr": "提出了一种名为SCJoint的新方法，可以实现显著物体检测和伪装物体检测任务的联合学习，并通过基于显著性的采样策略提高训练效率和质量。", "motivation": "显著物体检测和伪装物体检测是两种相关但不同的计算机视觉任务。尽管两者都是无关类别的分割任务，但前者旨在识别图像中最显著的物体，后者则专注于检测那些融入背景中的完美伪装物体。这两个任务表现出强烈的相反属性。以往的工作普遍认为同时学习这两个任务会混淆网络，降低其在两个任务上的表现。然而，本文提出了相反的观点：使用正确的学习方法，网络可以同时具备识别显著对象和伪装对象的能力，使得两个任务可以从联合学习中受益。", "method": "提出了一种名为SCJoint的联合学习方案来同时处理显著物体检测和伪装物体检测任务。该方法假设两任务的解码过程具有不同的分布特性，通过在网络中插入少量任务特定的可训练参数来学习两任务的解码过程各自的均值和方差，从而以极小的代价解耦两个任务的相互矛盾的属性。此外，还提出了一种基于显著性的采样策略(SBSS)来对显著物体检测任务的训练集进行采样，平衡两个任务训练集的规模，SBSS还提高了训练集的质量并缩短了训练时间。基于SCJoint和SBSS，训练了一个名为JoNet的强大通用网络，该网络具有同时捕获“显著”和“伪装”的能力。", "result": "广泛的实验结果展示了所提出方法的竞争力和有效性。", "conclusion": "实验表明，本文提出的方法表现出了强大的性能和有效性。代码已公开。"}}
{"id": "2508.06471", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06471", "abs": "https://arxiv.org/abs/2508.06471", "authors": ["GLM-4. 5 Team", ":", "Aohan Zeng", "Xin Lv", "Qinkai Zheng", "Zhenyu Hou", "Bin Chen", "Chengxing Xie", "Cunxiang Wang", "Da Yin", "Hao Zeng", "Jiajie Zhang", "Kedong Wang", "Lucen Zhong", "Mingdao Liu", "Rui Lu", "Shulin Cao", "Xiaohan Zhang", "Xuancheng Huang", "Yao Wei", "Yean Cheng", "Yifan An", "Yilin Niu", "Yuanhao Wen", "Yushi Bai", "Zhengxiao Du", "Zihan Wang", "Zilin Zhu", "Bohan Zhang", "Bosi Wen", "Bowen Wu", "Bowen Xu", "Can Huang", "Casey Zhao", "Changpeng Cai", "Chao Yu", "Chen Li", "Chendi Ge", "Chenghua Huang", "Chenhui Zhang", "Chenxi Xu", "Chenzheng Zhu", "Chuang Li", "Congfeng Yin", "Daoyan Lin", "Dayong Yang", "Dazhi Jiang", "Ding Ai", "Erle Zhu", "Fei Wang", "Gengzheng Pan", "Guo Wang", "Hailong Sun", "Haitao Li", "Haiyang Li", "Haiyi Hu", "Hanyu Zhang", "Hao Peng", "Hao Tai", "Haoke Zhang", "Haoran Wang", "Haoyu Yang", "He Liu", "He Zhao", "Hongwei Liu", "Hongxi Yan", "Huan Liu", "Huilong Chen", "Ji Li", "Jiajing Zhao", "Jiamin Ren", "Jian Jiao", "Jiani Zhao", "Jianyang Yan", "Jiaqi Wang", "Jiayi Gui", "Jiayue Zhao", "Jie Liu", "Jijie Li", "Jing Li", "Jing Lu", "Jingsen Wang", "Jingwei Yuan", "Jingxuan Li", "Jingzhao Du", "Jinhua Du", "Jinxin Liu", "Junkai Zhi", "Junli Gao", "Ke Wang", "Lekang Yang", "Liang Xu", "Lin Fan", "Lindong Wu", "Lintao Ding", "Lu Wang", "Man Zhang", "Minghao Li", "Minghuan Xu", "Mingming Zhao", "Mingshu Zhai", "Pengfan Du", "Qian Dong", "Shangde Lei", "Shangqing Tu", "Shangtong Yang", "Shaoyou Lu", "Shijie Li", "Shuang Li", "Shuang-Li", "Shuxun Yang", "Sibo Yi", "Tianshu Yu", "Wei Tian", "Weihan Wang", "Wenbo Yu", "Weng Lam Tam", "Wenjie Liang", "Wentao Liu", "Xiao Wang", "Xiaohan Jia", "Xiaotao Gu", "Xiaoying Ling", "Xin Wang", "Xing Fan", "Xingru Pan", "Xinyuan Zhang", "Xinze Zhang", "Xiuqing Fu", "Xunkai Zhang", "Yabo Xu", "Yandong Wu", "Yida Lu", "Yidong Wang", "Yilin Zhou", "Yiming Pan", "Ying Zhang", "Yingli Wang", "Yingru Li", "Yinpei Su", "Yipeng Geng", "Yitong Zhu", "Yongkun Yang", "Yuhang Li", "Yuhao Wu", "Yujiang Li", "Yunan Liu", "Yunqing Wang", "Yuntao Li", "Yuxuan Zhang", "Zezhen Liu", "Zhen Yang", "Zhengda Zhou", "Zhongpei Qiao", "Zhuoer Feng", "Zhuorui Liu", "Zichen Zhang", "Zihan Wang", "Zijun Yao", "Zikang Wang", "Ziqiang Liu", "Ziwei Chai", "Zixuan Li", "Zuodong Zhao", "Wenguang Chen", "Jidong Zhai", "Bin Xu", "Minlie Huang", "Hongning Wang", "Juanzi Li", "Yuxiao Dong", "Jie Tang"], "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models", "comment": null, "summary": "We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language\nmodel with 355B total parameters and 32B activated parameters, featuring a\nhybrid reasoning method that supports both thinking and direct response modes.\nThrough multi-stage training on 23T tokens and comprehensive post-training with\nexpert model iteration and reinforcement learning, GLM-4.5 achieves strong\nperformance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on\nTAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer\nparameters than several competitors, GLM-4.5 ranks 3rd overall among all\nevaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B\nparameters) and a compact version, GLM-4.5-Air (106B parameters), to advance\nresearch in reasoning and agentic AI systems. Code, models, and more\ninformation are available at https://github.com/zai-org/GLM-4.5.", "AI": {"tldr": "GLM-4.5是一种具有355B总参数和32B激活参数的开放源代码混合专家（MoE）大型语言模型，在较少的参数下实现了代理、推理和编码任务的高水平性能。", "motivation": "开发一种具有开放源代码的混合专家（MoE）大语言模型，旨在支持思考和直接响应模式的混合推理方法，从而在代理能力和推理任务中取得高水平的表现。", "method": "通过多阶段训练23T标记，并通过专家模型迭代和强化学习进行综合后期训练，GLM-4.5实现了在代理、推理和编码（ARC）任务中的强大性能。", "result": "GLM-4.5在TAU-Bench上获得70.1%的成绩，在AIME 24上获得91.0%的成绩，在SWE-bench Verified上获得64.2%的成绩。在所有评估模型中，GLM-4.5总体排名第三，在代理基准测试中排名第二。", "conclusion": "研究发布了具有355B参数的GLM-4.5和一个紧凑版本GLM-4.5-Air（106B参数），以推进在代理和推理AI系统领域的研究。"}}
{"id": "2508.06072", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06072", "abs": "https://arxiv.org/abs/2508.06072", "authors": ["Zijian Chen", "Lirong Deng", "Zhengyu Chen", "Kaiwei Zhang", "Qi Jia", "Yuan Tian", "Yucheng Zhu", "Guangtao Zhai"], "title": "Can Large Models Fool the Eye? A New Turing Test for Biological Animation", "comment": "24 pages, 10 figures", "summary": "Evaluating the abilities of large models and manifesting their gaps are\nchallenging. Current benchmarks adopt either ground-truth-based score-form\nevaluation on static datasets or indistinct textual chatbot-style human\npreferences collection, which may not provide users with immediate, intuitive,\nand perceptible feedback on performance differences. In this paper, we\nintroduce BioMotion Arena, a novel framework for evaluating large language\nmodels (LLMs) and multimodal large language models (MLLMs) via visual\nanimation. Our methodology draws inspiration from the inherent visual\nperception of motion patterns characteristic of living organisms that utilizes\npoint-light source imaging to amplify the performance discrepancies between\nmodels. Specifically, we employ a pairwise comparison evaluation and collect\nmore than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion\nvariants. Data analyses show that the crowd-sourced human votes are in good\nagreement with those of expert raters, demonstrating the superiority of our\nBioMotion Arena in offering discriminative feedback. We also find that over\n90\\% of evaluated models, including the cutting-edge open-source InternVL3 and\nproprietary Claude-4 series, fail to produce fundamental humanoid point-light\ngroups, much less smooth and biologically plausible motions. This enables\nBioMotion Arena to serve as a challenging benchmark for performance\nvisualization and a flexible evaluation framework without restrictions on\nground-truth.", "AI": {"tldr": "介绍了一个名为BioMotion Arena的新框架，通过视觉动画来评估大型语言模型和多模态大型语言模型，并发现超过90%的评估模型在生成人形点光源动作方面存在严重不足，证明BioMotion Arena是一个有效且灵活的评估框架。", "motivation": "目前的评估基准要么基于静态数据集上的ground-truth得分评估，要么通过模糊的文本聊天机器人式的用户偏好收集，无法为用户提供直接、直观且可感知的性能差异反馈。本论文旨在解决这一问题。", "method": "引入了BioMotion Arena，一个通过视觉动画评估大型语言模型(LLMs)和多模态大型语言模型(MLLMs)的新框架。此方法借鉴了生物运动的视觉感知特点，使用点光源成像来放大不同模型的性能差异。具体来说，采用成对比较的方法对53个主流LLMs和MLLMs的90种生物运动变体进行了评估，收集了超过45000个投票。", "result": "数据分析表明，众包的人类投票与专家评分者的结果有很好的一致性，证明了BioMotion Arena在提供有区分度的反馈方面的优越性。结果显示超过90%的评估模型无法生成基本的人形点光源群，更不用说流畅且具有生物合理性的运动。", "conclusion": "BioMotion Arena可以作为性能可视化的有挑战性的基准，并作为一个不受ground-truth限制的灵活评估框架。"}}
{"id": "2508.06475", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06475", "abs": "https://arxiv.org/abs/2508.06475", "authors": ["Guimin Hu", "Daniel Hershcovich", "Hasti Seifi"], "title": "HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning", "comment": null, "summary": "Haptic captioning is the task of generating natural language descriptions\nfrom haptic signals, such as vibrations, for use in virtual reality,\naccessibility, and rehabilitation applications. While previous multimodal\nresearch has focused primarily on vision and audio, haptic signals for the\nsense of touch remain underexplored. To address this gap, we formalize the\nhaptic captioning task and propose HapticLLaMA, a multimodal sensory language\nmodel that interprets vibration signals into descriptions in a given sensory,\nemotional, or associative category. We investigate two types of haptic\ntokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that\nconvert haptic signals into sequences of discrete units, enabling their\nintegration with the LLaMA model. HapticLLaMA is trained in two stages: (1)\nsupervised fine-tuning using the LLaMA architecture with LoRA-based adaptation,\nand (2) fine-tuning via reinforcement learning from human feedback (RLHF). We\nassess HapticLLaMA's captioning performance using both automated n-gram metrics\nand human evaluation. HapticLLaMA demonstrates strong capability in\ninterpreting haptic vibration signals, achieving a METEOR score of 59.98 and a\nBLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated\ncaptions received human ratings above 3.5 on a 7-point scale, with RLHF\nyielding a 10% improvement in the overall rating distribution, indicating\nstronger alignment with human haptic perception. These findings highlight the\npotential of large language models to process and adapt to sensory data.", "AI": {"tldr": "本文提出了HapticLLaMA模型，通过振动信号生成触觉描述，研究了两种触觉分词器，并通过监督微调和人类反馈增强学习进行了模型训练。", "motivation": "本文旨在填补现有研究中对触摸感官信号的研究空白，提出了触觉字幕生成的任务，即从触觉信号中生成自然语言描述，这类研究可应用于虚拟现实、无障碍、康复等领域。", "method": "本文提出了HapticLLaMA，一种多模态感官语言模型，用于将振动信号转化为描述。研究了两种触觉分词器，一种是基于频率的分词器，另一种是基于EnCodec的分词器，用于将触觉信号转化为离散的序列单元。HapticLLaMA的训练分两阶段进行：（1）用LoRA进行监督微调，（2）通过从人类反馈中进行增强学习的再训练。", "result": "HapticLLaMA模型在触觉字幕生成方面表现良好，显示出强大的振动信号解释能力，取得59.98的METEOR评分和32.06的BLEU-4评分。人工评级结果显示61%以上的生成字幕评分高于3.5（满分7分），人类反馈增强学习显著提高了与人类触觉感知的匹配度，评分分布提高了10%。", "conclusion": "这项研究突出了大型语言模型处理和适应感官数据的潜力。通过实验验证，该模型能够有效地生成触觉信号描述，展现出良好的性能。"}}
{"id": "2508.06076", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06076", "abs": "https://arxiv.org/abs/2508.06076", "authors": ["Michael Wehrli", "Alicia Durrer", "Paul Friedrich", "Sidaty El Hadramy", "Edwin Li", "Luana Brahaj", "Carol C. Hasler", "Philippe C. Cattin"], "title": "Towards MR-Based Trochleoplasty Planning", "comment": "Accepted at MICCAI COLAS Workshop 2025. Code:\n  https://wehrlimi.github.io/sr-3d-planning/", "summary": "To treat Trochlear Dysplasia (TD), current approaches rely mainly on\nlow-resolution clinical Magnetic Resonance (MR) scans and surgical intuition.\nThe surgeries are planned based on surgeons experience, have limited adoption\nof minimally invasive techniques, and lead to inconsistent outcomes. We propose\na pipeline that generates super-resolved, patient-specific 3D pseudo-healthy\ntarget morphologies from conventional clinical MR scans. First, we compute an\nisotropic super-resolved MR volume using an Implicit Neural Representation\n(INR). Next, we segment femur, tibia, patella, and fibula with a multi-label\ncustom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to\ngenerate pseudo-healthy target morphologies of the trochlear region. In\ncontrast to prior work producing pseudo-healthy low-resolution 3D MR images,\nour approach enables the generation of sub-millimeter resolved 3D shapes\ncompatible for pre- and intraoperative use. These can serve as preoperative\nblueprints for reshaping the femoral groove while preserving the native patella\narticulation. Furthermore, and in contrast to other work, we do not require a\nCT for our pipeline - reducing the amount of radiation. We evaluated our\napproach on 25 TD patients and could show that our target morphologies\nsignificantly improve the sulcus angle (SA) and trochlear groove depth (TGD).\nThe code and interactive visualization are available at\nhttps://wehrlimi.github.io/sr-3d-planning/.", "AI": {"tldr": "本文提出了一种基于临床MRI生成超分辨率3D目标形态的方法，用于提升Trochlear Dysplasia的治疗效果，显著改善了凹槽角度和深度。", "motivation": "目前针对Trochlear Dysplasia (TD)的治疗方法主要依赖低分辨率的临床MRI扫描和外科直觉，手术计划基于外科医生的经验，微创技术的应用有限，并导致结果不一致。本研究旨在提高这些手术的精度和一致性。", "method": "本研究提出了一种基于临床MR扫描生成超分辨率、个性化3D伪健康目标形态的流程。首先，使用隐式神经表示(INR)计算等向性超分辨率MR体积。然后，使用多标签定制训练的网络分割股骨、胫骨、髌骨和腓骨。最后，训练一个Wavelet扩散模型(WDM)生成膝关节凹槽区域的伪健康目标形态。", "result": "该研究在25例TD患者中进行了评估，结果表明其生成的目标形态显著改善了凹槽角度（SA）和膝关节凹槽深度（TGD）。", "conclusion": "本研究提出的方法为TD患者提供了一种无需CT即可生成亚毫米级别3D形态的解决方案，这在术前和术中都具有重要的应用价值，并有助于减少辐射暴露。"}}
{"id": "2508.06482", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06482", "abs": "https://arxiv.org/abs/2508.06482", "authors": ["Yilun Hua", "Evan Wang", "Yoav Artzi"], "title": "Post-training for Efficient Communication via Convention Formation", "comment": "Accepted to COLM 2025", "summary": "Humans communicate with increasing efficiency in multi-turn interactions, by\nadapting their language and forming ad-hoc conventions. In contrast, prior work\nshows that LLMs do not naturally show this behavior. We develop a post-training\nprocess to develop this ability through targeted fine-tuning on heuristically\nidentified demonstrations of convention formation. We evaluate with two new\nbenchmarks focused on this capability. First, we design a focused,\ncognitively-motivated interaction benchmark that consistently elicits strong\nconvention formation trends in humans. Second, we create a new\ndocument-grounded reference completion task that reflects in-the-wild\nconvention formation behavior. Our studies show significantly improved\nconvention formation abilities in post-trained LLMs across the two evaluation\nmethods.", "AI": {"tldr": "研究通过后训练过程改进了大语言模型在多轮交流中形成惯例的能力，并通过两个新基准测试证明了这一提升。", "motivation": "人类在多轮沟通中通过适应语言和形成临时惯例来提高交流效率，但研究表明，大语言模型（LLMs）并不能自然地表现出这种行为。", "method": "我们开发了一种后训练过程，通过针对识别出的惯例形成演示进行精细调整，来培养这一能力。", "result": "我们通过两种新的评估方法证明了后训练语言模型在惯例形成方面的能力有显著提升。", "conclusion": "研究表明，在通过特定的精细调整后，大语言模型在形成惯例方面的能力得到了显著提高。"}}
{"id": "2508.06080", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06080", "abs": "https://arxiv.org/abs/2508.06080", "authors": ["Bin Xia", "Jiyang Liu", "Yuechen Zhang", "Bohao Peng", "Ruihang Chu", "Yitong Wang", "Xinglong Wu", "Bei Yu", "Jiaya Jia"], "title": "DreamVE: Unified Instruction-based Image and Video Editing", "comment": null, "summary": "Instruction-based editing holds vast potential due to its simple and\nefficient interactive editing format. However, instruction-based editing,\nparticularly for video, has been constrained by limited training data,\nhindering its practical application. To this end, we introduce DreamVE, a\nunified model for instruction-based image and video editing. Specifically, We\npropose a two-stage training strategy: first image editing, then video editing.\nThis offers two main benefits: (1) Image data scales more easily, and models\nare more efficient to train, providing useful priors for faster and better\nvideo editing training. (2) Unifying image and video generation is natural and\naligns with current trends. Moreover, we present comprehensive training data\nsynthesis pipelines, including collage-based and generative model-based data\nsynthesis. The collage-based data synthesis combines foreground objects and\nbackgrounds to generate diverse editing data, such as object manipulation,\nbackground changes, and text modifications. It can easily generate billions of\naccurate, consistent, realistic, and diverse editing pairs. We pretrain DreamVE\non extensive collage-based data to achieve strong performance in key editing\ntypes and enhance generalization and transfer capabilities. However,\ncollage-based data lacks some attribute editing cases, leading to a relative\ndrop in performance. In contrast, the generative model-based pipeline, despite\nbeing hard to scale up, offers flexibility in handling attribute editing cases.\nTherefore, we use generative model-based data to further fine-tune DreamVE.\nBesides, we design an efficient and powerful editing framework for DreamVE. We\nbuild on the SOTA T2V model and use a token concatenation with early drop\napproach to inject source image guidance, ensuring strong consistency and\neditability. The codes and models will be released.", "AI": {"tldr": "为了克服指令驱动的视频编辑技术所面临的训练数据不足的问题，我们介绍了一个名为DreamVE的统一模型，能够实现基于指令的图像和视频编辑。", "motivation": "指令驱动的编辑技术由于其简单和高效的交互编辑格式有着巨大的潜力，但在视频编辑领域，由于训练数据的限制，其实际应用受到了阻碍。我们的目标是提出一个解决这一问题的方案。", "method": "我们提出了一个两级训练策略：首先进行图像编辑，然后进行视频编辑。这种策略使模型能够利用图像数据规模更大、训练更高效的优点，从而提供有用先验，强化视频编辑的训练。此外，我们提出了全面的训练数据合成流水线，包括拼贴数据合成和生成模型数据合成。", "result": "采用了拼贴数据合成技术对模型进行预训练后，DreamVE在一个关键编辑类型中表现良好，概括能力得到增强。同时，利用生成模型数据进一步微调DreamVE，以克服拼贴数据在处理属性修改方面的不足。", "conclusion": "DreamVE作为一个综合的图像和视频编辑模型，通过两级训练策略充分利用图像数据和视频数据的特性，结合多种数据合成流水线，取得了很好的编辑效果，并且提高了泛化和迁移能力。"}}
{"id": "2508.06082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06082", "abs": "https://arxiv.org/abs/2508.06082", "authors": ["Yanxiao Sun", "Jiafu Wu", "Yun Cao", "Chengming Xu", "Yabiao Wang", "Weijian Cao", "Donghao Luo", "Chengjie Wang", "Yanwei Fu"], "title": "SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment", "comment": null, "summary": "Diffusion-based or flow-based models have achieved significant progress in\nvideo synthesis but require multiple iterative sampling steps, which incurs\nsubstantial computational overhead. While many distillation methods that are\nsolely based on trajectory-preserving or distribution-matching have been\ndeveloped to accelerate video generation models, these approaches often suffer\nfrom performance breakdown or increased artifacts under few-step settings. To\naddress these limitations, we propose \\textbf{\\emph{SwiftVideo}}, a unified and\nstable distillation framework that combines the advantages of\ntrajectory-preserving and distribution-matching strategies. Our approach\nintroduces continuous-time consistency distillation to ensure precise\npreservation of ODE trajectories. Subsequently, we propose a dual-perspective\nalignment that includes distribution alignment between synthetic and real data\nalong with trajectory alignment across different inference steps. Our method\nmaintains high-quality video generation while substantially reducing the number\nof inference steps. Quantitative evaluations on the OpenVid-1M benchmark\ndemonstrate that our method significantly outperforms existing approaches in\nfew-step video generation.", "AI": {"tldr": "本文提出了SwiftVideo框架，旨在解决现有视频生成模型计算开销大的问题，并提出一系列改进方法，使得在减少推理步骤的情况下仍能维持高质量的视频生成效果。", "motivation": "扩散模型或流模型在视频合成方面取得了显著进展，但需要多次迭代采样步骤，导致计算开销大。现有的蒸馏方法仅基于轨迹保持或分布匹配策略来加速视频生成模型，这些方法在少量采样步骤下经常出现性能下降或增加伪影的问题。", "method": "SwiftVideo 是一个结合了轨迹保持和分布匹配优势的统一且稳定的蒸馏框架。它引入了连续时间一致性蒸馏以确保精确的ODE轨迹保留，并提出了双视角对齐，其中包括合成数据和真实数据间的分布对齐以及不同推理步骤间的轨迹对齐。", "result": "在OpenVid-1M基准上的定量评估表明，该方法在少步视频生成方面显著优于现有方法，同时大幅减少了推理步骤。", "conclusion": "SwiftVideo框架通过结合连续时间一致性蒸馏和双视角对齐策略，能够在减少视频生成的推理步骤的同时，不仅维持了高质量的视频生成，还显著优于现有方法。"}}
{"id": "2508.06492", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06492", "abs": "https://arxiv.org/abs/2508.06492", "authors": ["Yuwei Yang", "Zeyu Zhang", "Yunzhong Hou", "Zhuowan Li", "Gaowen Liu", "Ali Payani", "Yuan-Sen Ting", "Liang Zheng"], "title": "Effective Training Data Synthesis for Improving MLLM Chart Understanding", "comment": "Accepted by ICCV 2025 (poster). 26 pages, 17 figures", "summary": "Being able to effectively read scientific plots, or chart understanding, is a\ncentral part toward building effective agents for science. However, existing\nmultimodal large language models (MLLMs), especially open-source ones, are\nstill falling behind with a typical success rate of 30%-50% on challenging\nbenchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are\noften restricted by their inadequate similarity to the real charts, which could\ncompromise model training and performance on complex real-world charts. In this\nstudy, we show that modularizing chart generation and diversifying visual\ndetails improves chart understanding capabilities. In particular, we design a\nfive-step data synthesis pipeline, where we separate data and function creation\nfor single plot generation, condition the generation of later subplots on\nearlier ones for multi-subplot figures, visually diversify the generated\nfigures, filter out low quality data, and finally generate the question-answer\n(QA) pairs with GPT-4o. This approach allows us to streamline the generation of\nfine-tuning datasets and introduce the effective chart dataset (ECD), which\ncontains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring\n250+ chart type combinations with high visual complexity. We show that ECD\nconsistently improves the performance of various MLLMs on a range of real-world\nand synthetic test sets. Code, data and models are available at:\nhttps://github.com/yuweiyang-anu/ECD.", "AI": {"tldr": "本文提出了一个五步数据合成流水线来生成有效图表数据集(ECD)，该数据集包括10k+图表和300k+ QA对，可用于训练多模态语言模型，显著提升了模型对科学图表的理解能力。", "motivation": "现有的多模态大型语言模型(MLLMs)在理解和分析科学图表方面表现不佳，成功率只有30%-50%。过时的合成图表与真实的科学图表不相似，这限制了模型在真实图表上的训练及表现。", "method": "我们设计了一个五步数据合成流水线，包括单个图表生成时的数据和函数创建分离，在多子图图形生成时依据先前子图生成后续子图，视觉多样化生成图形，过滤低质量数据，最后使用GPT-4生成问题-答案(QA)对。", "result": "我们引入了有效图表数据集(ECD)，包含10k+图表和超过300k的QA对，涵盖了25个主题和250种图表类型，具有高视觉复杂度。实验显示，ECD可以提升各种MLLMs在真实和合成测试集上的性能。", "conclusion": "通过模块化图表生成和多样化视觉细节，可以提高多模态大型语言模型在科学图表理解上的表现。"}}
{"id": "2508.06084", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06084", "abs": "https://arxiv.org/abs/2508.06084", "authors": ["Weichen Zhang", "Zhui Zhu", "Ningbo Li", "Kebin Liu", "Yunhao Liu"], "title": "AdaptInfer: Adaptive Token Pruning for Vision-Language Model Inference with Dynamical Text Guidance", "comment": null, "summary": "Vision-language models (VLMs) have achieved impressive performance on\nmultimodal reasoning tasks such as visual question answering (VQA), but their\ninference cost remains a significant challenge due to the large number of\nvision tokens processed during the prefill stage. Existing pruning methods\noften rely on directly using the attention patterns or static text prompt\nguidance, failing to exploit the dynamic internal signals generated during\ninference. To address these issues, we propose AdaptInfer, a plug-and-play\nframework for adaptive vision token pruning in VLMs. First, we introduce a\nfine-grained, dynamic text-guided pruning mechanism that reuses layer-wise\ntext-to-text attention maps to construct soft priors over text-token\nimportance, allowing more informed scoring of vision tokens at each stage.\nSecond, we perform an offline analysis of cross-modal attention shifts and\nidentify consistent inflection locations in inference, which inspire us to\npropose a more principled and efficient pruning schedule. Our method is\nlightweight and plug-and-play, also generalizable across multi-modal tasks.\nExperimental results have verified the effectiveness of the proposed method.\nFor example, it reduces CUDA latency by 61.3\\% while maintaining an average\naccuracy of 92.9\\% on vanilla LLaVA-1.5-7B. Under the same token budget,\nAdaptInfer surpasses SOTA in accuracy.", "AI": {"tldr": "AdaptInfer proposes a dynamic text-guided mechanism for adaptive vision token pruning in visual-language models, reducing inference cost significantly without sacrificing accuracy.", "motivation": "To reduce the inference cost of Vision-Language Models (VLMs) by improving upon static pruning methods through dynamic, evidence-driven token management.", "method": "First, introduce a fine-grained, dynamic text-guided pruning mechanism for vision tokens in VLMs, which utilizes layer-wise text-to-text attention maps to inform pruning decisions. Second, conduct offline analysis of cross-modal attention shifts to establish a more efficient pruning schedule.", "result": "The proposed method reduces CUDA latency by 61.3% while maintaining 92.9% accuracy on the LLaVA-1.5-7B model, outperforming existing state-of-the-art techniques under the same token budget.", "conclusion": "AdaptInfer is an effective, lightweight, and generalizable solution for reducing the computational cost associated with the inference process in visual-language models, delivering a significant performance improvement in terms of latency reduction while preserving high accuracy."}}
{"id": "2508.06092", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06092", "abs": "https://arxiv.org/abs/2508.06092", "authors": ["Yachun Mi", "Yu Li", "Yanting Li", "Shixin Sun", "Chen Hui", "Tong Zhang", "Yuanyuan Liu", "Chenyue Song", "Shaohui Liu"], "title": "Q-CLIP: Unleashing the Power of Vision-Language Models for Video Quality Assessment through Unified Cross-Modal Adaptation", "comment": null, "summary": "Accurate and efficient Video Quality Assessment (VQA) has long been a key\nresearch challenge. Current mainstream VQA methods typically improve\nperformance by pretraining on large-scale classification datasets (e.g.,\nImageNet, Kinetics-400), followed by fine-tuning on VQA datasets. However, this\nstrategy presents two significant challenges: (1) merely transferring semantic\nknowledge learned from pretraining is insufficient for VQA, as video quality\ndepends on multiple factors (e.g., semantics, distortion, motion, aesthetics);\n(2) pretraining on large-scale datasets demands enormous computational\nresources, often dozens or even hundreds of times greater than training\ndirectly on VQA datasets. Recently, Vision-Language Models (VLMs) have shown\nremarkable generalization capabilities across a wide range of visual tasks, and\nhave begun to demonstrate promising potential in quality assessment. In this\nwork, we propose Q-CLIP, the first fully VLMs-based framework for VQA. Q-CLIP\nenhances both visual and textual representations through a Shared Cross-Modal\nAdapter (SCMA), which contains only a minimal number of trainable parameters\nand is the only component that requires training. This design significantly\nreduces computational cost. In addition, we introduce a set of five learnable\nquality-level prompts to guide the VLMs in perceiving subtle quality\nvariations, thereby further enhancing the model's sensitivity to video quality.\nFurthermore, we investigate the impact of different frame sampling strategies\non VQA performance, and find that frame-difference-based sampling leads to\nbetter generalization performance across datasets. Extensive experiments\ndemonstrate that Q-CLIP exhibits excellent performance on several VQA datasets.", "AI": {"tldr": "新型Q-CLIP框架基于视觉语言模型，解决了传统视频质量评估方法在成本和效果上的局限性，经实验验证该方法在多个数据集上表现出色。", "motivation": "面临视频质量评估研究中现有的基于大规模预训练的挑战，如计算资源消耗巨大且仅依赖于语义知识不足，该研究旨在开发一种更高效、更具成本效益的VQA方法。", "method": "提出Q-CLIP，利用视觉语言模型并通过共享跨模态适配器（SCMA）来最小化计算资源需求。此外，使用质量级别提示来优化模型对于视频质量变化的感知能力。", "result": "该论文提出了一种名为Q-CLIP的新框架，该框架基于视觉语言模型（VLMs），用于视频质量评估（VQA）。与当前通过预训练大规模分类数据集然后在VQA数据集上微调的方法相比，Q-CLIP通过共享跨模态适配器（SCMA）增强视觉和文本表示，并使用少量可训练参数，显著减少了计算成本。此外，引入了五个可学习的质量级别提示来帮助模型感知细微的质量变化。实验表明，Q-CLIP在多个VQA数据集中表现出色。", "conclusion": "研究表明，在视频质量评估中使用基于视觉语言模型的方法（如Q-CLIP）不仅能够提高评估效果，还能显著减少计算资源需求，新的帧采样策略也进一步提升了模型性能的泛化能力。"}}
{"id": "2508.06093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06093", "abs": "https://arxiv.org/abs/2508.06093", "authors": ["Chen Zhu", "Buzhen Huang", "Zijing Wu", "Binghui Zuo", "Yangang Wang"], "title": "E-React: Towards Emotionally Controlled Synthesis of Human Reactions", "comment": null, "summary": "Emotion serves as an essential component in daily human interactions.\nExisting human motion generation frameworks do not consider the impact of\nemotions, which reduces naturalness and limits their application in interactive\ntasks, such as human reaction synthesis. In this work, we introduce a novel\ntask: generating diverse reaction motions in response to different emotional\ncues. However, learning emotion representation from limited motion data and\nincorporating it into a motion generation framework remains a challenging\nproblem. To address the above obstacles, we introduce a semi-supervised emotion\nprior in an actor-reactor diffusion model to facilitate emotion-driven reaction\nsynthesis. Specifically, based on the observation that motion clips within a\nshort sequence tend to share the same emotion, we first devise a\nsemi-supervised learning framework to train an emotion prior. With this prior,\nwe further train an actor-reactor diffusion model to generate reactions by\nconsidering both spatial interaction and emotional response. Finally, given a\nmotion sequence of an actor, our approach can generate realistic reactions\nunder various emotional conditions. Experimental results demonstrate that our\nmodel outperforms existing reaction generation methods. The code and data will\nbe made publicly available at https://ereact.github.io/", "AI": {"tldr": "本文提出了一种利用半监督情感先验，在演员-反应者扩散模型基础上生成情感驱动反应的新方法，克服了现有运动生成框架的局限性。", "motivation": "现有运动生成框架未考虑情感的影响，导致生成的运动不自然且在交互任务中的应用受限。本文旨在通过引入情感因素来改善这一点。", "method": "本文介绍了一种新的任务：根据不同的情感线索生成多样化的反应动作。为应对从有限的运动数据学习情感表达并将其融入运动生成框架的挑战，本文提出了一种在行为者-反应者扩散模型中引入半监督情感先验的方法，以促进情感驱动的反应合成。通过观察短序列内的运动片段共享相似的情感，本文开发了一种半监督学习框架来训练情感先验。基于这一先验，进一步训练模型综合考虑空间交互和情感反应来生成反应动作。", "result": "实验结果表明，该模型在反应生成方面优于现有方法。", "conclusion": "通过半监督学习框架和情感驱动的反应合成方法，本文的方法能够生成更自然、真实的反应动作，并且在多种情感条件下表现出色。"}}
{"id": "2508.06101", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06101", "abs": "https://arxiv.org/abs/2508.06101", "authors": ["Yachun Mi", "Xingyang He", "Shixin Sun", "Yu Li", "Yanting Li", "Zhixuan Li", "Jian Jin", "Chen Hui", "Shaohui Liu"], "title": "UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization", "comment": null, "summary": "In the digital age, advanced image editing tools pose a serious threat to the\nintegrity of visual content, making image forgery detection and localization a\nkey research focus. Most existing Image Manipulation Localization (IML) methods\nrely on discriminative learning and require large, high-quality annotated\ndatasets. However, current datasets lack sufficient scale and diversity,\nlimiting model performance in real-world scenarios. To overcome this, recent\nstudies have explored Constrained IML (CIML), which generates pixel-level\nannotations through algorithmic supervision. However, existing CIML approaches\noften depend on complex multi-stage pipelines, making the annotation process\ninefficient. In this work, we propose a novel generative framework based on\ndiffusion models, named UGD-IML, which for the first time unifies both IML and\nCIML tasks within a single framework. By learning the underlying data\ndistribution, generative diffusion models inherently reduce the reliance on\nlarge-scale labeled datasets, allowing our approach to perform effectively even\nunder limited data conditions. In addition, by leveraging a class embedding\nmechanism and a parameter-sharing design, our model seamlessly switches between\nIML and CIML modes without extra components or training overhead. Furthermore,\nthe end-to-end design enables our model to avoid cumbersome steps in the data\nannotation process. Extensive experimental results on multiple datasets\ndemonstrate that UGD-IML outperforms the SOTA methods by an average of 9.66 and\n4.36 in terms of F1 metrics for IML and CIML tasks, respectively. Moreover, the\nproposed method also excels in uncertainty estimation, visualization and\nrobustness.", "AI": {"tldr": "我们提出了一个新的基于扩散模型的生成框架UGD-IML，统一了IML和CIML任务，在多个数据集上表现出色，显著超越现有方法。", "motivation": "现代图像编辑工具对视觉内容的完整性构成威胁，而现有图像篡改检测和定位方法依赖于大量高质量标注，但当前数据集规模和多样性不足，限制了模型在实际场景中的表现。为克服这些不足，我们提出了一种新的图像篡改定位方法。", "method": "我们提出了一种基于扩散模型的生成框架，名为UGD-IML，首次将IML和CIML任务统一在单一框架中。通过学习基本数据分布，该生成模型降低了对大规模标注数据的依赖，并在有限数据条件下表现良好。此外，利用类别嵌入机制和参数共享设计，该模型能在IML和CIML之间无缝切换，无需额外组件或训练开销。端到端的设计使该模型在数据标注过程中避免复杂步骤。", "result": "针对多个数据集进行的大量实验表明，UGD-IML显著超越当前最佳方法，在F1指标上分别超过了9.66和4.36，同时在不确定性估计、可视化和模型鲁棒性方面表现优异。", "conclusion": "实验结果显示，UGD-IML在IML和CIML任务上的F1指标分别超过了现有最佳方法9.66和4.36。此外，该方法在不确定性估计、可视化和鲁棒性方面的表现也优于现有方法。"}}
{"id": "2508.06104", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06104", "abs": "https://arxiv.org/abs/2508.06104", "authors": ["Gui Zou", "Chaofan Gan", "Chern Hong Lim", "Supavadee Aramvith", "Weiyao Lin"], "title": "MCA: 2D-3D Retrieval with Noisy Labels via Multi-level Adaptive Correction and Alignment", "comment": "ICMEW 2025", "summary": "With the increasing availability of 2D and 3D data, significant advancements\nhave been made in the field of cross-modal retrieval. Nevertheless, the\nexistence of imperfect annotations presents considerable challenges, demanding\nrobust solutions for 2D-3D cross-modal retrieval in the presence of noisy label\nconditions. Existing methods generally address the issue of noise by dividing\nsamples independently within each modality, making them susceptible to\noverfitting on corrupted labels. To address these issues, we propose a robust\n2D-3D \\textbf{M}ulti-level cross-modal adaptive \\textbf{C}orrection and\n\\textbf{A}lignment framework (MCA). Specifically, we introduce a Multimodal\nJoint label Correction (MJC) mechanism that leverages multimodal historical\nself-predictions to jointly model the modality prediction consistency, enabling\nreliable label refinement. Additionally, we propose a Multi-level Adaptive\nAlignment (MAA) strategy to effectively enhance cross-modal feature semantics\nand discrimination across different levels. Extensive experiments demonstrate\nthe superiority of our method, MCA, which achieves state-of-the-art performance\non both conventional and realistic noisy 3D benchmarks, highlighting its\ngenerality and effectiveness.", "AI": {"tldr": "本文提出了一个鲁棒的2D-3D跨模态自适应校正和对齐框架（MCA），通过多模态联合标签校正（MJC）机制实现可靠的标签修正，并通过多层级自适应对齐（MAA）策略提升跨模态特征语义和区分度。实验表明MCA在有噪声的3D数据上表现出色。", "motivation": "随着2D和3D数据的日益丰富，跨模态检索方面有了显著的进步，但不完美标注的存在提出了挑战，特别是在噪声标签条件下的2D-3D跨模态检索。现有的方法通常通过在每个模态内独立分割样本解决噪声问题，容易对受损标签过拟合。", "method": "我们提出了一种鲁棒的2D-3D跨模态自适应校正和对齐框架（MCA），包括多模态联合标签校正（MJC）机制和多层级自适应对齐（MAA）策略。MJC机制通过多模态历史自预测来联合建模模态预测一致性，实现可靠的标签修正。MAA策略有效提升不同层级上的跨模态特征语义和区分度。", "result": "大量实验表明，我们提出的方法MCA在常规和实际噪声3D基准测试中均达到了最先进的性能，凸显了它的通用性和有效性。", "conclusion": "MCA框架在这项工作中证明了其作为一个有效且通用的解决方案，适用于2D-3D跨模态检索中的噪声标签问题。"}}
{"id": "2508.06107", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06107", "abs": "https://arxiv.org/abs/2508.06107", "authors": ["Shree Mitra", "Ritabrata Chakraborty", "Nilkanta Sahu"], "title": "Mask & Match: Learning to Recognize Handwritten Math with Self-Supervised Attention", "comment": null, "summary": "Recognizing handwritten mathematical expressions (HMER) is a challenging task\ndue to the inherent two-dimensional structure, varying symbol scales, and\ncomplex spatial relationships among symbols. In this paper, we present a\nself-supervised learning (SSL) framework for HMER that eliminates the need for\nexpensive labeled data. Our approach begins by pretraining an image encoder\nusing a combination of global and local contrastive loss, enabling the model to\nlearn both holistic and fine-grained representations. A key contribution of\nthis work is a novel self-supervised attention network, which is trained using\na progressive spatial masking strategy. This attention mechanism is designed to\nlearn semantically meaningful focus regions, such as operators, exponents, and\nnested mathematical notation, without requiring any supervision. The\nprogressive masking curriculum encourages the network to become increasingly\nrobust to missing or occluded visual information, ultimately improving\nstructural understanding. Our complete pipeline consists of (1) self-supervised\npretraining of the encoder, (2) self-supervised attention learning, and (3)\nsupervised fine-tuning with a transformer decoder to generate LATEX sequences.\nExtensive experiments on CROHME benchmarks demonstrate that our method\noutperforms existing SSL and fully supervised baselines, validating the\neffectiveness of our progressive attention mechanism in enhancing HMER\nperformance. Our codebase can be found here.", "AI": {"tldr": "研究提出了一种用于手写数学表达式识别的自我监督学习框架，该框架无需昂贵的标注数据，且在多个实验中显示出优越性。", "motivation": "由于手写数学表达式识别任务具有二维结构、符号尺寸变化和复杂的符号间空间关系等挑战，该研究旨在提出一种自我监督学习框架，以减少昂贵的标注数据需求。", "method": "我们的方法包括三个步骤：(1) 使用全局和局部对比损失进行图像编码器的自监督预训练，(2) 使用逐渐空间掩码策略训练自监督注意力网络，以学习有意义的聚焦区域，(3) 使用带Transformer解码器的监督微调生成LATEX序列。", "result": "在CROHME基准测试中，我们的方法优于现有的自我监督和全监督基线，验证了渐进式注意机制在提高手写数学表达式识别性能中的有效性。", "conclusion": "本研究通过自监督预训练和注意机制成功提高了手写数学表达式的识别效果，并证明渐进式注意机制的有效性。"}}
{"id": "2508.06109", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06109", "abs": "https://arxiv.org/abs/2508.06109", "authors": ["Zhibo Zhu", "Renyu Huang", "Lei He"], "title": "FMCE-Net++: Feature Map Convergence Evaluation and Training", "comment": null, "summary": "Deep Neural Networks (DNNs) face interpretability challenges due to their\nopaque internal representations. While Feature Map Convergence Evaluation\n(FMCE) quantifies module-level convergence via Feature Map Convergence Scores\n(FMCS), it lacks experimental validation and closed-loop integration. To\naddress this limitation, we propose FMCE-Net++, a novel training framework that\nintegrates a pretrained, frozen FMCE-Net as an auxiliary head. This module\ngenerates FMCS predictions, which, combined with task labels, jointly supervise\nbackbone optimization through a Representation Auxiliary Loss. The RAL\ndynamically balances the primary classification loss and feature convergence\noptimization via a tunable \\Representation Abstraction Factor. Extensive\nexperiments conducted on MNIST, CIFAR-10, FashionMNIST, and CIFAR-100\ndemonstrate that FMCE-Net++ consistently enhances model performance without\narchitectural modifications or additional data. Key experimental outcomes\ninclude accuracy gains of $+1.16$ pp (ResNet-50/CIFAR-10) and $+1.08$ pp\n(ShuffleNet v2/CIFAR-100), validating that FMCE-Net++ can effectively elevate\nstate-of-the-art performance ceilings.", "AI": {"tldr": "该研究通过FMCE-Net++框架，利用特征图收敛分数(FMCS)辅助评估机制，增强了模型的训练效果，提高了测试数据集上的准确率，没有对模型结构或数据量进行变更。", "motivation": "论文旨在解决深度神经网络(DNNs)由于其不透明的内部表示而面临的可解释性挑战，特别指出现有的FMCE方法缺乏实验验证和闭环集成，从而提出改进方法。", "method": "该论文提出了一种新的训练框架FMCE-Net++，它将预训练好的FMCE-Net作为辅助头集成，生成的特征图收敛分数(FMCS)预测与任务标签一起监督主干优化，通过表示辅助损失(RAL)和可调的“表示抽象因子”动态平衡主要的分类损失和特征收敛优化。", "result": "在MNIST、CIFAR-10、FashionMNIST和CIFAR-100数据集上的实验表明，FMCE-Net++能够持续提高模型性能，而无需对架构进行修改或增加额外的数据。特别是在ResNet-50和CIFAR-10上的准确率提高了1.16个百分点，在ShuffleNet v2和CIFAR-100上的准确率提高了1.08个百分点。", "conclusion": "实验结果证明了FMCE-Net++能够有效地提升当前最先进的性能上限。"}}
{"id": "2508.06113", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06113", "abs": "https://arxiv.org/abs/2508.06113", "authors": ["Jian Wang", "Chaokang Jiang", "Haitao Xu"], "title": "GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving", "comment": "7 pages, 4 figures", "summary": "Diffusion-based models are redefining the state-of-the-art in end-to-end\nautonomous driving, yet their performance is increasingly hampered by a\nreliance on transformer-based fusion. These architectures face fundamental\nlimitations: quadratic computational complexity restricts the use of\nhigh-resolution features, and a lack of spatial priors prevents them from\neffectively modeling the inherent structure of Bird's Eye View (BEV)\nrepresentations. This paper introduces GMF-Drive (Gated Mamba Fusion for\nDriving), an end-to-end framework that overcomes these challenges through two\nprincipled innovations. First, we supersede the information-limited\nhistogram-based LiDAR representation with a geometrically-augmented pillar\nformat encoding shape descriptors and statistical features, preserving critical\n3D geometric details. Second, we propose a novel hierarchical gated mamba\nfusion (GM-Fusion) architecture that substitutes an expensive transformer with\na highly efficient, spatially-aware state-space model (SSM). Our core BEV-SSM\nleverages directional sequencing and adaptive fusion mechanisms to capture\nlong-range dependencies with linear complexity, while explicitly respecting the\nunique spatial properties of the driving scene. Extensive experiments on the\nchallenging NAVSIM benchmark demonstrate that GMF-Drive achieves a new\nstate-of-the-art performance, significantly outperforming DiffusionDrive.\nComprehensive ablation studies validate the efficacy of each component,\ndemonstrating that task-specific SSMs can surpass a general-purpose transformer\nin both performance and efficiency for autonomous driving.", "AI": {"tldr": "本文介绍了 GMF-Drive，提出了一种新的模型框架，通过增强的几何和高效的空间感知状态空间模型，克服了现有基于扩散模型中 Transformer 融合的局限性，实现了自动驾驶的新一代性能。", "motivation": "尽管基于扩散的模型正在重塑端到端自动驾驶的前沿，但它们的性能越来越受到基于 Transformer 融合的限制。Transformer 架构面临根本性的限制，如二次计算复杂度限制了高分辨率特征的使用，以及缺乏空间先验，阻碍了 Bird's Eye View (BEV) 表示的有效建模。", "method": "GMF-Drive 提出了一种端到端的框架，解决了依赖于 Transformer 融合的局限性。第一，用几何增强的柱体格式替换了信息量较低的基于直方图的 LiDAR 表示，保留了关键的 3D 几何细节。第二，设计了一种新颖的层次化门控 Mamba 融合架构，用高效的空间感知状态空间模型替换了昂贵的 Transformer。", "result": "在具有挑战性的 NAVSIM 数据集上进行的广泛实验表明，GMF-Drive 达到了新的最先进的性能，显著超越了 DiffusionDrive。详细的消融研究验证了每个组件的有效性。", "conclusion": "针对自动驾驶任务，特异性 SSM 在性能和效率上都比通用 Transformer 更占优势。"}}
{"id": "2508.06115", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06115", "abs": "https://arxiv.org/abs/2508.06115", "authors": ["Weichen Zhang", "Kebin Liu", "Fan Dang", "Zhui Zhu", "Xikai Sun", "Yunhao Liu"], "title": "SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation", "comment": null, "summary": "Semantic segmentation in open-vocabulary scenarios presents significant\nchallenges due to the wide range and granularity of semantic categories.\nExisting weakly-supervised methods often rely on category-specific supervision\nand ill-suited feature construction methods for contrastive learning, leading\nto semantic misalignment and poor performance. In this work, we propose a novel\nweakly-supervised approach, SynSeg, to address the challenges. SynSeg performs\nMulti-Category Contrastive Learning (MCCL) as a stronger training signal with a\nnew feature reconstruction framework named Feature Synergy Structure (FSS).\nSpecifically, MCCL strategy robustly combines both intra- and inter-category\nalignment and separation in order to make the model learn the knowledge of\ncorrelations from different categories within the same image. Moreover, FSS\nreconstructs discriminative features for contrastive learning through prior\nfusion and semantic-activation-map enhancement, effectively avoiding the\nforeground bias introduced by the visual encoder. In general, SynSeg\neffectively improves the abilities in semantic localization and discrimination\nunder weak supervision. Extensive experiments on benchmarks demonstrate that\nour method outperforms state-of-the-art (SOTA) performance. For instance,\nSynSeg achieves higher accuracy than SOTA baselines by 4.5\\% on VOC, 8.9\\% on\nContext, 2.6\\% on Object and 2.0\\% on City.", "AI": {"tldr": "提出了SynSeg方法，通过多类别对比学习和特征协同结构，增强了弱监督下语义分割任务的表现，在多个数据集上超过了现有最优方法。", "motivation": "针对开放词汇场景中语义分割的挑战，特别是现有弱监督方法效果不佳的问题，提出了SynSeg，试图解决语义不对齐和表现力差的难题。", "method": "SynSeg采用多类别对比学习(MCCL)作为训练信号，并使用特征协同结构(FSS)框架来重新构建对比学习中的判别特征，改善了视觉编码器带来的前景偏置问题。", "result": "", "conclusion": "SynSeg在弱监督条件下显著提升了语义定位和区分能力，并在多个基准测试中超越了现有最佳方法的性能。"}}
{"id": "2508.06122", "categories": ["cs.CV", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2508.06122", "abs": "https://arxiv.org/abs/2508.06122", "authors": ["Ting-Shuo Yo", "Shih-Hao Su", "Chien-Ming Wu", "Wei-Ting Chen", "Jung-Lien Chu", "Chiao-Wei Chang", "Hung-Chi Kuo"], "title": "Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events", "comment": "37 pages, 6 figures, 3 tables", "summary": "This study applied representation learning algorithms to satellite images and\nevaluated the learned latent spaces with classifications of various weather\nevents. The algorithms investigated include the classical linear\ntransformation, i.e., principal component analysis (PCA), state-of-the-art deep\nlearning method, i.e., convolutional autoencoder (CAE), and a residual network\npre-trained with large image datasets (PT). The experiment results indicated\nthat the latent space learned by CAE consistently showed higher threat scores\nfor all classification tasks. The classifications with PCA yielded high hit\nrates but also high false-alarm rates. In addition, the PT performed\nexceptionally well at recognizing tropical cyclones but was inferior in other\ntasks. Further experiments suggested that representations learned from\nhigher-resolution datasets are superior in all classification tasks for\ndeep-learning algorithms, i.e., CAE and PT. We also found that smaller latent\nspace sizes had minor impact on the classification task's hit rate. Still, a\nlatent space dimension smaller than 128 caused a significantly higher false\nalarm rate. Though the CAE can learn latent spaces effectively and efficiently,\nthe interpretation of the learned representation lacks direct connections to\nphysical attributions. Therefore, developing a physics-informed version of CAE\ncan be a promising outlook for the current work.", "AI": {"tldr": "研究应用几种表示学习算法于卫星图像的天气事件分类上，实验显示，CAE取得了最佳的整体表现，而PT在热带气旋识别中表现突出。进一步的研究指出从高分辨率数据学习表示更好，且潜空间的大小会影响误报率。建议未来开发具备物理信息的CAE。", "motivation": "目的是探索用表示学习算法来提高卫星图像上天气事件分类的准确性，并分析不同算法的性能差异以及影响算法表现的因素。", "method": "应用了表示学习算法到卫星图像上，并评估了学习到的潜空间在各种天气事件分类上的表现。研究的算法包括经典的线性变换方法PCA，先进的深度学习方法卷积自编码器CAE，以及在大规模图像数据集上预训练的残差网络PT。", "result": "实验结果表明，CAE学习到的潜空间在所有分类任务中的威胁评分最高。PCA分类有着高击中率但也有高误报率。PT在识别热带气旋方面表现出色，但在其他任务上的表现较差。进一步的实验表明，从高分辨率数据集中学习到的表示在深度学习算法，即CAE和PT的分类任务上表现出色。我们还发现，相对较小的潜空间尺寸对分类任务的击中率影响不大，但是潜空间维度小于128时会导致误报率显著升高。", "conclusion": "虽然CAE能够有效且高效地学习潜空间，但学习到的表示与物理属性之间缺乏直接联系。因此，开发具有物理信息的CAE版本是未来工作的有希望的方向。"}}
{"id": "2508.06125", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06125", "abs": "https://arxiv.org/abs/2508.06125", "authors": ["Lin Zhang", "Xianfang Zeng", "Kangcong Li", "Gang Yu", "Tao Chen"], "title": "SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning", "comment": "ICCV 2025", "summary": "We propose SC-Captioner, a reinforcement learning framework that enables the\nself-correcting capability of image caption models. Our crucial technique lies\nin the design of the reward function to incentivize accurate caption\ncorrections. Specifically, the predicted and reference captions are decomposed\ninto object, attribute, and relation sets using scene-graph parsing algorithms.\nWe calculate the set difference between sets of initial and self-corrected\ncaptions to identify added and removed elements. These elements are matched\nagainst the reference sets to calculate correctness bonuses for accurate\nrefinements and mistake punishments for wrong additions and removals, thereby\nforming the final reward. For image caption quality assessment, we propose a\nset of metrics refined from CAPTURE that alleviate its incomplete precision\nevaluation and inefficient relation matching problems. Furthermore, we collect\na fine-grained annotated image caption dataset, RefinedCaps, consisting of 6.5K\ndiverse images from COCO dataset. Experiments show that applying SC-Captioner\non large visual-language models can generate better image captions across\nvarious scenarios, significantly outperforming the direct preference\noptimization training strategy.", "AI": {"tldr": "本文提出SC-Captioner框架，用于增强图像描述模型的自我修正能力，通过精心设计的奖励函数和改进的评估指标提高描述质量，并展示其实验效果优于直接偏好优化策略。", "motivation": "本文旨在提出一种新的方法来改善图像描述模型的自我修正能力，通过设计精妙的奖励函数和改进的评估指标来提升图像描述的质量。", "method": "本文提出了一种名为SC-Captioner的强化学习框架，该框架实现了图像描述模型的自我修正能力。框架的关键技术在于设计奖励函数来激励准确的描述修正。具体来说，预测的和参考的描述通过场景图解析算法被分解成对象、属性和关系三部分。通过计算初始描述和自我修正描述之间的集合差异，识别出新增和删除的元素，并将这些元素与参考集相匹配，从而计算出正确的修正奖励和错误增删的惩罚。", "result": "实验表明，SC-Captioner框架提高了图像描述质量，尤其在应用到大型视觉语言模型时表现出色，显著优于直接偏好优化训练策略。", "conclusion": "研究表明，本研究提出的SC-Captioner框架在改善图像描述质量和增加自我修正能力方面具有显著效果。"}}
{"id": "2508.06127", "categories": ["cs.CV", "I.4.9"], "pdf": "https://arxiv.org/pdf/2508.06127", "abs": "https://arxiv.org/abs/2508.06127", "authors": ["Yi Qin", "Rui Wang", "Tao Huang", "Tong Xiao", "Liping Jing"], "title": "SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures", "comment": "8 pages,recived by ICCV2025", "summary": "While the Segment Anything Model (SAM) transforms interactive segmentation\nwith zero-shot abilities, its inherent vulnerabilities present a single-point\nrisk, potentially leading to the failure of numerous downstream applications.\nProactively evaluating these transferable vulnerabilities is thus imperative.\nPrior adversarial attacks on SAM often present limited transferability due to\ninsufficient exploration of common weakness across domains. To address this, we\npropose Vertex-Refining Simplicial Complex Attack (VeSCA), a novel method that\nleverages only the encoder of SAM for generating transferable adversarial\nexamples. Specifically, it achieves this by explicitly characterizing the\nshared vulnerable regions between SAM and downstream models through a\nparametric simplicial complex. Our goal is to identify such complexes within\nadversarially potent regions by iterative vertex-wise refinement. A lightweight\ndomain re-adaptation strategy is introduced to bridge domain divergence using\nminimal reference data during the initialization of simplicial complex.\nUltimately, VeSCA generates consistently transferable adversarial examples\nthrough random simplicial complex sampling. Extensive experiments demonstrate\nthat VeSCA achieves performance improved by 12.7% compared to state-of-the-art\nmethods across three downstream model categories across five domain-specific\ndatasets. Our findings further highlight the downstream model risks posed by\nSAM's vulnerabilities and emphasize the urgency of developing more robust\nfoundation models.", "AI": {"tldr": "VeSCA, a method that uses the encoder of the Segment Anything Model (SAM) to generate transferable adversarial examples by characterizing shared vulnerable regions through a parametric simplicial complex, demonstrates improved performance by 12.7% over existing methods.", "motivation": "To evaluate the transferable vulnerabilities of SAM that could cause the failure of downstream applications, and to address the limited transferability of previous adversarial attacks on SAM.", "method": "Proposes VeSCA, which generates transferable adversarial examples by characterizing shared vulnerable regions using the encoder of SAM through a parametric simplicial complex.", "result": "Experiments show VeSCA achieves a performance improvement of 12.7% over state-of-the-art methods across different downstream models and datasets.", "conclusion": "Highlights the significant risks to downstream models due to SAM's inherent vulnerabilities and underscores the need for developing more robust models."}}
{"id": "2508.06136", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06136", "abs": "https://arxiv.org/abs/2508.06136", "authors": ["YoungChan Choi", "HengFei Wang", "YiHua Cheng", "Boeun Kim", "Hyung Jin Chang", "YoungGeun Choi", "Sang-Il Choi"], "title": "Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation", "comment": "9 pages, 5 figures, ACM Multimeida 2025 accepted", "summary": "We propose a novel 3D gaze redirection framework that leverages an explicit\n3D eyeball structure. Existing gaze redirection methods are typically based on\nneural radiance fields, which employ implicit neural representations via volume\nrendering. Unlike these NeRF-based approaches, where the rotation and\ntranslation of 3D representations are not explicitly modeled, we introduce a\ndedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian\nSplatting (3DGS). Our method generates photorealistic images that faithfully\nreproduce the desired gaze direction by explicitly rotating and translating the\n3D eyeball structure. In addition, we propose an adaptive deformation module\nthat enables the replication of subtle muscle movements around the eyes.\nThrough experiments conducted on the ETH-XGaze dataset, we demonstrate that our\nframework is capable of generating diverse novel gaze images, achieving\nsuperior image quality and gaze estimation accuracy compared to previous\nstate-of-the-art methods.", "AI": {"tldr": "提出了一种新颖的基于3D眼球结构的注视方向重定向框架，该方法生成高度逼真的图像并显式地旋转和移动眼球结构，优于现有的方法。", "motivation": "现有的注视方向重定向方法通常基于神经辐射场，该方法使用隐式神经表示通过体积渲染实现。这些方法中3D表示的旋转和平移没有被明确建模。", "method": "引入了专门的3D眼球结构，使用3D Gaussian Splatting（3DGS）来表示眼球，并通过显式旋转和移动3D眼球结构来生成逼真的图像以忠实再现期望的注视方向。还提出了一种自适应变形模块，能够复制眼睛周围的细微肌肉运动。", "result": "通过对ETH-XGaze数据集进行实验，证明了该框架能够生成多样化的新注视图像，并且在图像质量和注视估计准确性方面优于现有的方法。", "conclusion": "实验证明，该框架能够在生成不同注视方向的图像时，保持高质量和准确的注视估计，优于现有的state-of-the-art方法。"}}
{"id": "2508.06139", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06139", "abs": "https://arxiv.org/abs/2508.06139", "authors": ["Shaohua Pan", "Xinyu Yi", "Yan Zhou", "Weihua Jian", "Yuan Zhang", "Pengfei Wan", "Feng Xu"], "title": "DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera", "comment": null, "summary": "Combining sparse IMUs and a monocular camera is a new promising setting to\nperform real-time human motion capture. This paper proposes a diffusion-based\nsolution to learn human motion priors and fuse the two modalities of signals\ntogether seamlessly in a unified framework. By delicately considering the\ncharacteristics of the two signals, the sequential visual information is\nconsidered as a whole and transformed into a condition embedding, while the\ninertial measurement is concatenated with the noisy body pose frame by frame to\nconstruct a sequential input for the diffusion model. Firstly, we observe that\nthe visual information may be unavailable in some frames due to occlusions or\nsubjects moving out of the camera view. Thus incorporating the sequential\nvisual features as a whole to get a single feature embedding is robust to the\noccasional degenerations of visual information in those frames. On the other\nhand, the IMU measurements are robust to occlusions and always stable when\nsignal transmission has no problem. So incorporating them frame-wisely could\nbetter explore the temporal information for the system. Experiments have\ndemonstrated the effectiveness of the system design and its state-of-the-art\nperformance in pose estimation compared with the previous works. Our codes are\navailable for research at https://shaohua-pan.github.io/diffcap-page.", "AI": {"tldr": "该论文提出了一种基于扩散模型的方法，结合稀疏IMU和单目相机信息进行实时人体动作捕捉，实验表明该系统设计有效，并在姿态估计方面优于先前的工作。", "motivation": "该论文的动机在于利用稀疏IMU和单目相机相结合的新型且有潜力的设置来进行实时人体动作捕捉。通过结合两种信号的特点，提高在视觉信息偶尔退化时的鲁棒性及利用IMU测量在遮挡情况下的稳定性。", "method": "该论文提出了一种基于扩散模型的解决方案，用于学习人体动作先验并将两种信号模态无缝融合到一个统一的框架中。视觉信息被作为一个整体考虑并转化为条件嵌入，而惯性测量则逐帧与有噪声的身体姿态拼接，以构建扩散模型的序列输入。", "result": "实验已经验证了系统设计的有效性，并且与先前的工作相比，在姿态估计方面达到了最先进的性能。", "conclusion": "本研究提出了一种结合视觉和惯性测量信号的人体动作捕捉框架，实现了在实时场景下的高效和高准确度的动作捕捉，展现了该方法在动作捕捉领域的潜力。"}}
{"id": "2508.06142", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06142", "abs": "https://arxiv.org/abs/2508.06142", "authors": ["Hanqing Wang", "Yuan Tian", "Mingyu Liu", "Zhenhao Zhang", "Xiangyang Zhu"], "title": "SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models", "comment": null, "summary": "In the rapidly evolving landscape of Multimodal Large Language Models\n(MLLMs), the safety concerns of their outputs have earned significant\nattention. Although numerous datasets have been proposed, they may become\noutdated with MLLM advancements and are susceptible to data contamination\nissues. To address these problems, we propose \\textbf{SDEval}, the\n\\textit{first} safety dynamic evaluation framework to controllably adjust the\ndistribution and complexity of safety benchmarks. Specifically, SDEval mainly\nadopts three dynamic strategies: text, image, and text-image dynamics to\ngenerate new samples from original benchmarks. We first explore the individual\neffects of text and image dynamics on model safety. Then, we find that\ninjecting text dynamics into images can further impact safety, and conversely,\ninjecting image dynamics into text also leads to safety risks. SDEval is\ngeneral enough to be applied to various existing safety and even capability\nbenchmarks. Experiments across safety benchmarks, MLLMGuard and VLSBench, and\ncapability benchmarks, MMBench and MMVet, show that SDEval significantly\ninfluences safety evaluation, mitigates data contamination, and exposes safety\nlimitations of MLLMs. Code is available at https://github.com/hq-King/SDEval", "AI": {"tldr": "提出SDEval框架，动态调整多模态大语言模型的安全性基准，实验证明其有效。", "motivation": "现有的多模态大语言模型的安全性关注日益增加，但是现有的数据集随着模型的进步可能变得过时，且易受数据污染。为了应对这些问题，提出了SDEval框架。", "method": "SDEval采用三种动态策略：文本动态、图像动态以及图文动态策略，来生成新的安全基准样本。首先，研究了文本和图像动态对模型安全性的独立影响，然后探讨了文本动态注入图像与图像动态注入文本对安全性的进一步影响。", "result": "实验证明SDEval显著影响安全性评估，缓解数据污染，并揭示了MLLMs的安全限制。", "conclusion": "SDEval具有通用性，能应用于多个现有的安全性和能力基准，显示出在安全评估中的重要作用。"}}
{"id": "2508.06146", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06146", "abs": "https://arxiv.org/abs/2508.06146", "authors": ["Yuchen Guan", "Chong Sun", "Canmiao Fu", "Zhipeng Huang", "Chun Yuan", "Chen Li"], "title": "Text-guided Visual Prompt DINO for Generic Segmentation", "comment": null, "summary": "Recent advancements in multimodal vision models have highlighted limitations\nin late-stage feature fusion and suboptimal query selection for hybrid prompts\nopen-world segmentation, alongside constraints from caption-derived\nvocabularies. To address these challenges, we propose Prompt-DINO, a\ntext-guided visual Prompt DINO framework featuring three key innovations.\nFirst, we introduce an early fusion mechanism that unifies text/visual prompts\nand backbone features at the initial encoding stage, enabling deeper\ncross-modal interactions to resolve semantic ambiguities. Second, we design\norder-aligned query selection for DETR-based architectures, explicitly\noptimizing the structural alignment between text and visual queries during\ndecoding to enhance semantic-spatial consistency. Third, we develop a\ngenerative data engine powered by the Recognize Anything via Prompting (RAP)\nmodel, which synthesizes 0.5B diverse training instances through a dual-path\ncross-verification pipeline, reducing label noise by 80.5% compared to\nconventional approaches. Extensive experiments demonstrate that Prompt-DINO\nachieves state-of-the-art performance on open-world detection benchmarks while\nsignificantly expanding semantic coverage beyond fixed-vocabulary constraints.\nOur work establishes a new paradigm for scalable multimodal detection and data\ngeneration in open-world scenarios. Data&Code are available at\nhttps://github.com/WeChatCV/WeVisionOne.", "AI": {"tldr": "本文提出了Prompt-DINO框架，解决多模态视觉模型中的几个关键问题，并在开放式场景检测中展示出优越性能。", "motivation": "为了解决多模态视觉模型中后期特征融合的局限性、混合提示开放式分割中的次优查询选择和由标题导出的词汇约束，从而提高开放式场景中的多模态检测和数据生成能力。", "method": "提出了Prompt-DINO框架，该框架包括三种关键技术：1.早期融合机制，2.结构对齐的query选择，3.基于提示生成的0.5B数据合成引擎。", "result": "实验结果表明，Prompt-DINO在开放式检测基准测试中达到了最先进的性能，并且显著扩大了语义覆盖范围，超越了固定词汇约束。", "conclusion": "这项工作为开放式场景中的可扩展多模态检测和数据生成建立了新的范式。"}}
{"id": "2508.06147", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06147", "abs": "https://arxiv.org/abs/2508.06147", "authors": ["Xuanyu Liu", "Bonan An"], "title": "DSConv: Dynamic Splitting Convolution for Pansharpening", "comment": null, "summary": "Aiming to obtain a high-resolution image, pansharpening involves the fusion\nof a multi-spectral image (MS) and a panchromatic image (PAN), the low-level\nvision task remaining significant and challenging in contemporary research.\nMost existing approaches rely predominantly on standard convolutions, few\nmaking the effort to adaptive convolutions, which are effective owing to the\ninter-pixel correlations of remote sensing images. In this paper, we propose a\nnovel strategy for dynamically splitting convolution kernels in conjunction\nwith attention, selecting positions of interest, and splitting the original\nconvolution kernel into multiple smaller kernels, named DSConv. The proposed\nDSConv more effectively extracts features of different positions within the\nreceptive field, enhancing the network's generalization, optimization, and\nfeature representation capabilities. Furthermore, we innovate and enrich\nconcepts of dynamic splitting convolution and provide a novel network\narchitecture for pansharpening capable of achieving the tasks more efficiently,\nbuilding upon this methodology. Adequate fair experiments illustrate the\neffectiveness and the state-of-the-art performance attained by\nDSConv.Comprehensive and rigorous discussions proved the superiority and\noptimal usage conditions of DSConv.", "AI": {"tldr": "本文提出了一种名为DSConv的新策略，通过动态分割卷积核并结合注意力机制，以提高多光谱图像和全色图像融合的效果，从而获得高分辨率图像。实验表明，该方法在遥感图像融合任务中表现出色。", "motivation": "传统的泛锐化方法主要依赖标准卷积，很少采用自适应卷积。考虑到遥感图像像素间的相关性，作者提出了一种新的策略，希望能在低层次视觉任务中，特别是在多光谱图像和全色图像融合方面，取得更好的效果。", "method": "本文提出的方法被称为DSConv，它结合了动态分割卷积核和注意力机制，可以在感受野内不同位置更有效地提取特征。这是通过选择感兴趣的位置，并将原始卷积核拆分成多个更小的核来实现的。", "result": "实验表明，提出的方法在泛锐化任务中表现优异，超过了当前的其他方法的性能。", "conclusion": "本文提出了一种创新的卷积策略DSConv，该策略通过动态分割卷积核和结合注意力机制，有效改善了网络的泛化、优化和特征表示能力。"}}
{"id": "2508.06152", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06152", "abs": "https://arxiv.org/abs/2508.06152", "authors": ["Kaiyuan Jiang", "Ruoxi Sun", "Ying Cao", "Yuqi Xu", "Xinran Zhang", "Junyan Guo", "ChengSheng Deng"], "title": "VISTAR:A User-Centric and Role-Driven Benchmark for Text-to-Image Evaluation", "comment": "17 pages,8 figures", "summary": "We present VISTAR, a user-centric, multi-dimensional benchmark for\ntext-to-image (T2I) evaluation that addresses the limitations of existing\nmetrics. VISTAR introduces a two-tier hybrid paradigm: it employs\ndeterministic, scriptable metrics for physically quantifiable attributes (e.g.,\ntext rendering, lighting) and a novel Hierarchical Weighted P/N Questioning\n(HWPQ) scheme that uses constrained vision-language models to assess abstract\nsemantics (e.g., style fusion, cultural fidelity). Grounded in a Delphi study\nwith 120 experts, we defined seven user roles and nine evaluation angles to\nconstruct the benchmark, which comprises 2,845 prompts validated by over 15,000\nhuman pairwise comparisons. Our metrics achieve high human alignment (>75%),\nwith the HWPQ scheme reaching 85.9% accuracy on abstract semantics,\nsignificantly outperforming VQA baselines. Comprehensive evaluation of\nstate-of-the-art models reveals no universal champion, as role-weighted scores\nreorder rankings and provide actionable guidance for domain-specific\ndeployment. All resources are publicly released to foster reproducible T2I\nassessment.", "AI": {"tldr": "VISTAR是一个用户为中心的多维文本到图像（T2I）评估基准，采用两级混合范式评估特定和抽象属性，包含2845个验证提示和超过15000个人类比较，显示了高的人类一致性，并揭示了没有单一的最好的模型。", "motivation": "VISTAR旨在解决现有T2I指标的局限性，通过结合确定性评估和基于约束的视觉语言模型来评估抽象语义，从而提供一个全方位的评估框架。", "method": "VISTAR采用了一种两级混合范式来评估文本到图像(T2I)的性能。第一阶段使用确定性的可脚本化的指标来评估可量化属性（如文本渲染、照明等）。第二阶段引入了一种新的层次加权P/N问答（HWPQ）方案来评估抽象语义（如风格融合、文化保真度）。", "result": "VISTAR达到了大于75%的人类一致性，特别是HWPQ方案对抽象语义的准确性达到了85.9%，超过了VQA基线，没有模型能在所有角色加权分数中胜出。", "conclusion": "VISTAR通过引入新的评估方法，提供了一个用户为中心的多维T2I评估框架，旨在促进可重复的T2I性能评估，所有资源都是公开的。"}}
{"id": "2508.06157", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06157", "abs": "https://arxiv.org/abs/2508.06157", "authors": ["Xiaoxiao Yang", "Meiliang Liu", "Yunfang Xu", "Zijin Li", "Zhengye Si", "Xinyue Yang", "Zhiwen Zhao"], "title": "An Interpretable Multi-Plane Fusion Framework With Kolmogorov-Arnold Network Guided Attention Enhancement for Alzheimer's Disease Diagnosis", "comment": null, "summary": "Alzheimer's disease (AD) is a progressive neurodegenerative disorder that\nseverely impairs cognitive function and quality of life. Timely intervention in\nAD relies heavily on early and precise diagnosis, which remains challenging due\nto the complex and subtle structural changes in the brain. Most existing deep\nlearning methods focus only on a single plane of structural magnetic resonance\nimaging (sMRI) and struggle to accurately capture the complex and nonlinear\nrelationships among pathological regions of the brain, thus limiting their\nability to precisely identify atrophic features. To overcome these limitations,\nwe propose an innovative framework, MPF-KANSC, which integrates multi-plane\nfusion (MPF) for combining features from the coronal, sagittal, and axial\nplanes, and a Kolmogorov-Arnold Network-guided spatial-channel attention\nmechanism (KANSC) to more effectively learn and represent sMRI atrophy\nfeatures. Specifically, the proposed model enables parallel feature extraction\nfrom multiple anatomical planes, thus capturing more comprehensive structural\ninformation. The KANSC attention mechanism further leverages a more flexible\nand accurate nonlinear function approximation technique, facilitating precise\nidentification and localization of disease-related abnormalities. Experiments\non the ADNI dataset confirm that the proposed MPF-KANSC achieves superior\nperformance in AD diagnosis. Moreover, our findings provide new evidence of\nright-lateralized asymmetry in subcortical structural changes during AD\nprogression, highlighting the model's promising interpretability.", "AI": {"tldr": "本文提出了一种名为MPF-KANSC的框架，用以改进现有的深度学习方法，以实现更准确的阿尔茨海默病诊断。实验结果表明，该模型在AD诊断方面具有优越性能，同时提供新的亚皮层结构变化不对称性的证据，强调了模型的解释性。", "motivation": "现有的大多数深度学习方法只关注sMRI的单一平面，无法准确捕捉脑部病理区域之间的复杂非线性关系，限制了识别萎缩特征的能力。因此，该研究旨在解决这一局限性，实现更准确的阿尔茨海默病(AD)诊断。", "method": "提出了一种创新框架MPF-KANSC，该框架结合了多平面融合(MPF)和Kolmogorov-Arnold网络引导的空间-通道注意力机制(KANSC)，以更有效地学习和表示sMRI萎缩特征。MPF可以使模型并行抽取多个解剖平面中的特征，而KANSC注意力机制则进一步采用了一种更为灵活和准确的非线性函数近似技术，有助于疾病相关异常的精确识别和定位。", "result": "在ADNI数据集上的实验表明，提出的方法MPF-KANSC在AD诊断方面具有优越的性能。另外，研究发现提供了AD进展期间右侧化不对称性的新证据。", "conclusion": "综上所述，MPF-KANSC框架通过结合MPF和KANSC注意力机制提高了AD诊断的准确性，并为理解AD进展期间亚皮层结构的右侧化不对称性提供了新的见解。"}}
{"id": "2508.06160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06160", "abs": "https://arxiv.org/abs/2508.06160", "authors": ["Zhenbang Du", "Yonggan Fu", "Lifu Wang", "Jiayi Qian", "Xiao Luo", "Yingyan", "Lin"], "title": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment", "comment": "Accepted by ICCV 2025", "summary": "Diffusion models have shown remarkable success across generative tasks, yet\ntheir high computational demands challenge deployment on resource-limited\nplatforms. This paper investigates a critical question for compute-optimal\ndiffusion model deployment: Under a post-training setting without fine-tuning,\nis it more effective to reduce the number of denoising steps or to use a\ncheaper per-step inference? Intuitively, reducing the number of denoising steps\nincreases the variability of the distributions across steps, making the model\nmore sensitive to compression. In contrast, keeping more denoising steps makes\nthe differences smaller, preserving redundancy, and making post-training\ncompression more feasible. To systematically examine this, we propose PostDiff,\na training-free framework for accelerating pre-trained diffusion models by\nreducing redundancy at both the input level and module level in a post-training\nmanner. At the input level, we propose a mixed-resolution denoising scheme\nbased on the insight that reducing generation resolution in early denoising\nsteps can enhance low-frequency components and improve final generation\nfidelity. At the module level, we employ a hybrid module caching strategy to\nreuse computations across denoising steps. Extensive experiments and ablation\nstudies demonstrate that (1) PostDiff can significantly improve the\nfidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to\nboost efficiency while maintaining decent generation fidelity, reducing\nper-step inference cost is often more effective than reducing the number of\ndenoising steps. Our code is available at\nhttps://github.com/GATECH-EIC/PostDiff.", "AI": {"tldr": "研究发现，通过PostDiff框架，在资源有限的平台部署扩散模型时，降低每步骤推断成本比减少去噪步骤数量更有效。", "motivation": "探究在不进行微调的情况下，减少去噪步骤数量还是降低每步骤推断成本更有效的方法，以优化扩散模型的部署。", "method": "PostDiff框架通过减少预训练扩散模型中的冗余来加速模型，包括在输入层面提出混合分辨率去噪方案，以及在模块层面采用混合模块缓存策略。", "result": "实验表明PostDiff显著提高了现有最佳扩散模型的保真度和效率之间的平衡。同时，在保持良好生成保真度的情况下，降低每步骤推断成本通常比减少去噪步骤数量更为有效。", "conclusion": "PostDiff框架能够在保持或提高生成保真度的同时，大幅提高扩散模型的效率。"}}
{"id": "2508.06169", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06169", "abs": "https://arxiv.org/abs/2508.06169", "authors": ["Wenpeng Xing", "Jie Chen", "Zaifeng Yang", "Changting Lin", "Jianfeng Dong", "Chaochao Chen", "Xun Zhou", "Meng Han"], "title": "UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting", "comment": null, "summary": "Underwater 3D scene reconstruction faces severe challenges from light\nabsorption, scattering, and turbidity, which degrade geometry and color\nfidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF\nextensions such as SeaThru-NeRF incorporate physics-based models, their MLP\nreliance limits efficiency and spatial resolution in hazy environments. We\nintroduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for\nrobust underwater reconstruction. Key innovations include: (1) a plug-and-play\nlearnable underwater image formation module using voxel-based regression for\nspatially varying attenuation and backscatter; and (2) a Physics-Aware\nUncertainty Pruning (PAUP) branch that adaptively removes noisy floating\nGaussians via uncertainty scoring, ensuring artifact-free geometry. The\npipeline operates in training and rendering stages. During training, noisy\nGaussians are optimized end-to-end with underwater parameters, guided by PAUP\npruning and scattering modeling. In rendering, refined Gaussians produce clean\nUnattenuated Radiance Images (URIs) free from media effects, while learned\nphysics enable realistic Underwater Images (UWIs) with accurate light\ntransport. Experiments on SeaThru-NeRF and UWBundle datasets show superior\nperformance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on\nSeaThru-NeRF, with ~65% reduction in floating artifacts.", "AI": {"tldr": "The paper presents UW-3DGS, an innovative 3D scene reconstruction method for underwater environments, which effectively handles challenges such as light absorption and scattering, achieving superior results on datasets.", "motivation": "to address the challenges of light absorption, scattering, and turbidity in traditional underwater 3D scene reconstruction methods like Neural Radiance Fields (NeRF), which degrade geometry and color fidelity.", "method": "introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for robust underwater reconstruction, including a plug-and-play learnable underwater image formation module using voxel-based regression for spatially varying attenuation and backscatter, and a Physics-Aware Uncertainty Pruning (PAUP) branch for artifact-free geometry.", "result": "experiments on SeaThru-NeRF and UWBundle datasets show superior performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on SeaThru-NeRF, with a ~65% reduction in floating artifacts.", "conclusion": "the UW-3DGS framework achieves robust underwater 3D scene reconstruction with improved geometry and color fidelity, effectively addressing the limitations of previous methods in hazy underwater environments."}}
{"id": "2508.06170", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06170", "abs": "https://arxiv.org/abs/2508.06170", "authors": ["Ojonugwa Oluwafemi Ejiga Peter", "Akingbola Oluwapemiisin", "Amalahu Chetachi", "Adeniran Opeyemi", "Fahmi Khalifa", "Md Mahmudur Rahman"], "title": "Synthetic Data-Driven Multi-Architecture Framework for Automated Polyp Segmentation Through Integrated Detection and Mask Generation", "comment": null, "summary": "Colonoscopy is a vital tool for the early diagnosis of colorectal cancer,\nwhich is one of the main causes of cancer-related mortality globally; hence, it\nis deemed an essential technique for the prevention and early detection of\ncolorectal cancer. The research introduces a unique multidirectional\narchitectural framework to automate polyp detection within colonoscopy images\nwhile helping resolve limited healthcare dataset sizes and annotation\ncomplexities. The research implements a comprehensive system that delivers\nsynthetic data generation through Stable Diffusion enhancements together with\ndetection and segmentation algorithms. This detection approach combines Faster\nR-CNN for initial object localization while the Segment Anything Model (SAM)\nrefines the segmentation masks. The faster R-CNN detection algorithm achieved a\nrecall of 93.08% combined with a precision of 88.97% and an F1 score of\n90.98%.SAM is then used to generate the image mask. The research evaluated five\nstate-of-the-art segmentation models that included U-Net, PSPNet, FPN, LinkNet,\nand MANet using ResNet34 as a base model. The results demonstrate the superior\nperformance of FPN with the highest scores of PSNR (7.205893) and SSIM\n(0.492381), while UNet excels in recall (84.85%) and LinkNet shows balanced\nperformance in IoU (64.20%) and Dice score (77.53%).", "AI": {"tldr": "The research introduces a multidirectional architectural framework for automating polyp detection in colonoscopy images, using synthetic data generation and a combination of Faster R-CNN for localization and the Segment Anything Model (SAM) for segmentation refinement, achieving high accuracy and recall rates in various models evaluated.", "motivation": "The motivation is to improve the early detection of colorectal cancer through better polyp detection during colonoscopies, overcoming challenges of limited healthcare data and annotation complexities.", "method": "The method utilizes a comprehensive system that generates synthetic data with Stable Diffusion, along with detection and segmentation algorithms. It combines Faster R-CNN for object localization and SAM for refining segmentation masks.", "result": "The study demonstrates that Faster R-CNN has a recall of 93.08% and precision of 88.97%, while SAM is used to generate image masks. Among several evaluated segmentation models (U-Net, PSPNet, FPN, LinkNet, and MANet), FPN showed the best performance.", "conclusion": "The research concludes that the proposed multidirectional framework, combining Faster R-CNN and SAM, effectively enhances polyp detection accuracy and complemented by the evaluation of different segmentation models, shows promising results for improving colorectal cancer early detection through colonoscopies."}}
{"id": "2508.06177", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06177", "abs": "https://arxiv.org/abs/2508.06177", "authors": ["Dominik Brämer", "Diana Kleingarn", "Oliver Urbann"], "title": "Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor", "comment": "Accepted at 28th RoboCup International Symposium, Salvador, Brasil", "summary": "Accurate localization represents a fundamental challenge in\n  robotic navigation. Traditional methodologies, such as Lidar or QR-code based\nsystems, suffer from inherent scalability and adaptability con straints,\nparticularly in complex environments. In this work, we propose\n  an innovative localization framework that harnesses flooring characteris tics\nby employing graph-based representations and Graph Convolutional\n  Networks (GCNs). Our method uses graphs to represent floor features,\n  which helps localize the robot more accurately (0.64cm error) and more\n  efficiently than comparing individual image features. Additionally, this\n  approach successfully addresses the kidnapped robot problem in every\n  frame without requiring complex filtering processes. These advancements\n  open up new possibilities for robotic navigation in diverse environments.", "AI": {"tldr": "本文提出了一种创新的基于地面特征的机器人定位框架，采用图结构和图卷积网络，克服了传统方法的局限性，实现了更准确和有效的定位。", "motivation": "传统的定位方法如激光雷达或二维码系统在复杂环境中存在扩展性和适应性限制，作者提出了一种基于地面特征的创新定位框架以解决这些问题。", "method": "该框架利用图结构表示地板特征，并使用图卷积网络（GCN）进行建模。这种方法通过用图来表示地板上的特征，进行更准确和有效的定位，而无需复杂的滤波过程。", "result": "提出的框架能更准确地（误差0.64厘米）定位机器人，并解决“被绑架机器人问题”中的定位挑战。", "conclusion": "此方法为多样环境中的机器人导航开辟了新的可能性。"}}
{"id": "2508.06189", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06189", "abs": "https://arxiv.org/abs/2508.06189", "authors": ["Cheng Liu", "Daou Zhang", "Tingxu Liu", "Yuhan Wang", "Jinyang Chen", "Yuexuan Li", "Xinying Xiao", "Chenbo Xin", "Ziru Wang", "Weichao Wu"], "title": "MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration", "comment": null, "summary": "With the acceleration of urbanization, criminal behavior in public scenes\nposes an increasingly serious threat to social security. Traditional anomaly\ndetection methods based on feature recognition struggle to capture high-level\nbehavioral semantics from historical information, while generative approaches\nbased on Large Language Models (LLMs) often fail to meet real-time\nrequirements. To address these challenges, we propose MA-CBP, a criminal\nbehavior prediction framework based on multi-agent asynchronous collaboration.\nThis framework transforms real-time video streams into frame-level semantic\ndescriptions, constructs causally consistent historical summaries, and fuses\nadjacent image frames to perform joint reasoning over long- and short-term\ncontexts. The resulting behavioral decisions include key elements such as event\nsubjects, locations, and causes, enabling early warning of potential criminal\nactivity. In addition, we construct a high-quality criminal behavior dataset\nthat provides multi-scale language supervision, including frame-level,\nsummary-level, and event-level semantic annotations. Experimental results\ndemonstrate that our method achieves superior performance on multiple datasets\nand offers a promising solution for risk warning in urban public safety\nscenarios.", "AI": {"tldr": "研究提出MA-CBP框架，通过多智能体异步协作实现实时犯罪行为预测，解决了传统方法的问题，并通过实验验证了其有效性。", "motivation": "由于城市化进程的加快，传统基于特征识别的异常行为检测方法难以从历史信息中捕获高级行为语义，而基于大型语言模型（LLMs）的方法又常常无法满足实时要求。因此，该文旨在解决这些挑战。", "method": "该论文提出了一种基于多智能体异步协作的犯罪行为预测框架（MA-CBP），将实时视频流转换为基于帧的语义描述，构建因果一致的历史摘要，并融合相邻图像帧以进行长时间和短时间上下文的联合推理。", "result": "实验结果表明，该方法在多个数据集上表现出色，并为城市公共安全场景的风险预警提供了一个有前景的解决方案。", "conclusion": "该研究通过提出MA-CBP框架，有效地结合了实时语义描述与因果历史记录，显著提升了犯罪行为预测的准确性和实时性。"}}
{"id": "2508.06191", "categories": ["cs.CV", "68T45, 92C55", "I.4.6; I.5.4; J.3"], "pdf": "https://arxiv.org/pdf/2508.06191", "abs": "https://arxiv.org/abs/2508.06191", "authors": ["Ruixiang Tang", "Jianglong Qin", "Mingda Zhang", "Yan Song", "Yi Wu", "Wei Wu"], "title": "A Semantic Segmentation Algorithm for Pleural Effusion Based on DBIF-AUNet", "comment": "12 pages, 6 figures, 2 tables", "summary": "Pleural effusion semantic segmentation can significantly enhance the accuracy\nand timeliness of clinical diagnosis and treatment by precisely identifying\ndisease severity and lesion areas. Currently, semantic segmentation of pleural\neffusion CT images faces multiple challenges. These include similar gray levels\nbetween effusion and surrounding tissues, blurred edges, and variable\nmorphology. Existing methods often struggle with diverse image variations and\ncomplex edges, primarily because direct feature concatenation causes semantic\ngaps. To address these challenges, we propose the Dual-Branch Interactive\nFusion Attention model (DBIF-AUNet). This model constructs a densely nested\nskip-connection network and innovatively refines the Dual-Domain Feature\nDisentanglement module (DDFD). The DDFD module orthogonally decouples the\nfunctions of dual-domain modules to achieve multi-scale feature complementarity\nand enhance characteristics at different levels. Concurrently, we design a\nBranch Interaction Attention Fusion module (BIAF) that works synergistically\nwith the DDFD. This module dynamically weights and fuses global, local, and\nfrequency band features, thereby improving segmentation robustness.\nFurthermore, we implement a nested deep supervision mechanism with hierarchical\nadaptive hybrid loss to effectively address class imbalance. Through validation\non 1,622 pleural effusion CT images from Southwest Hospital, DBIF-AUNet\nachieved IoU and Dice scores of 80.1% and 89.0% respectively. These results\noutperform state-of-the-art medical image segmentation models U-Net++ and\nSwin-UNet by 5.7%/2.7% and 2.2%/1.5% respectively, demonstrating significant\noptimization in segmentation accuracy for complex pleural effusion CT images.", "AI": {"tldr": "提出了DBIF-AUNet模型，用于改善胸腔积液CT图像的语义分割。该模型通过DDFD和BIAF模块实现多尺度特征互补和动态特征融合，嵌套深度监督机制进一步优化了分割效果。与U-Net++和Swin-UNet相比，DBIF-AUNet在IoU和Dice分数方面有显著提高。", "motivation": "胸腔积液语义分割可以显著提高临床诊断和治疗的准确性和及时性，通过精确识别疾病严重程度和病变区域。目前，胸腔积液CT图像的语义分割面临着多种挑战。这些挑战包括积液和周围组织之间的灰度相似、边缘模糊以及形态变异。现有方法通常难以处理图像变异和复杂边缘，主要是由于直接特征级联造成了语义差距。为了解决这些问题，提出了本方法。", "method": "提出了双分支交互融合注意力模型(DBIF-AUNet)。该模型构建了一个密集嵌套跳跃连接网络，并创新性地优化了双域特征解耦模块（DDFD）。DDFD模块正交解耦了双域模块的功能，以实现多尺度特征互补并增强不同层次的特征。同时设计了分支交互注意力融合模块(BIAF)，与DDFD模块协同工作。该模块动态权重融合全局、局部和频率带特征，从而提高分割健壮性。此外，我们实现了一种嵌套深度监督机制，采用层次自适应混合损失来有效解决类别不平衡问题。", "result": "在西南医院的1,622张胸腔积液CT图像上进行验证，DBIF-AUNet实现了IoU和Dice分数分别为80.1%和89.0%。这些结果优于最先进的医学图像分割模型U-Net++和Swin-UNet，分别高出5.7%/2.7%和2.2%/1.5%，展示了在复杂胸腔积液CT图像分割中准确性方面的显著优化。", "conclusion": "该研究提出了DBIF-AUNet模型，能够更好地解决胸腔积液CT图像语义分割中的挑战，提高了准确性和鲁棒性。该模型在实际数据集上的测试表明，其性能优于现有的其他先进模型，并有效地处理了类别不平衡的问题。"}}
{"id": "2508.06202", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06202", "abs": "https://arxiv.org/abs/2508.06202", "authors": ["Chang Che", "Ziqi Wang", "Pengwan Yang", "Qi Wang", "Hui Ma", "Zenglin Shi"], "title": "LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning", "comment": null, "summary": "Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language\nModels (MLLMs) to incrementally learn new tasks over time. However, this\nprocess is challenged by catastrophic forgetting, where performance on\npreviously learned tasks deteriorates as the model adapts to new ones. A common\napproach to mitigate forgetting is architecture expansion, which introduces\ntask-specific modules to prevent interference. Yet, existing methods often\nexpand entire layers for each task, leading to significant parameter overhead\nand poor scalability. To overcome these issues, we introduce LoRA in LoRA\n(LiLoRA), a highly efficient architecture expansion method tailored for CVIT in\nMLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy,\napplies an additional low-rank decomposition to matrix B to minimize\ntask-specific parameters, and incorporates a cosine-regularized stability loss\nto preserve consistency in shared representations over time. Extensive\nexperiments on a diverse CVIT benchmark show that LiLoRA consistently achieves\nsuperior performance in sequential task learning while significantly improving\nparameter efficiency compared to existing approaches.", "AI": {"tldr": "我们引入了LiLoRA以解决MLLMs在CVIT过程中由于架构扩展而导致的参数开销大和可扩展性差的问题，结果表明LiLoRA在保持性能的同时提高了参数效率。", "motivation": "Continual Visual Instruction Tuning (CVIT)使多模态大型语言模型（MLLMs）能够随着时间的推移逐步学习新任务。然而，这个过程会受到灾难性遗忘的挑战，即模型在适应新任务时，之前任务的性能会退化。现有的方法通常会对每个任务扩展整个层，导致显著的参数开销，并影响可扩展性。", "method": "通过引入LiLoRA（LoRA in LoRA），我们提出了一种专门针对MLLM中CVIT的高度有效的架构扩展方法。LiLoRA在任务之间共享LoRA矩阵A以减少冗余，对矩阵B应用额外的低秩分解以最小化任务特定参数，并引入了余弦正则化稳定损失以保持共享表示随时间的一致性。", "result": "实验结果表明，LiLoRA在一系列针对不同任务的CVIT基准实验中，在保持性能的同时，相对于现有方法显著提高了参数效率。", "conclusion": "在一系列针对不同任务的CVIT基准实验表明，LiLoRA在顺序任务学习中始终实现了优越的性能，同时与现有方法相比，显著提高了参数效率。"}}
{"id": "2508.06203", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06203", "abs": "https://arxiv.org/abs/2508.06203", "authors": ["Zhaopeng Gu", "Bingke Zhu", "Guibo Zhu", "Yingying Chen", "Wei Ge", "Ming Tang", "Jinqiao Wang"], "title": "AnomalyMoE: Towards a Language-free Generalist Model for Unified Visual Anomaly Detection", "comment": null, "summary": "Anomaly detection is a critical task across numerous domains and modalities,\nyet existing methods are often highly specialized, limiting their\ngeneralizability. These specialized models, tailored for specific anomaly types\nlike textural defects or logical errors, typically exhibit limited performance\nwhen deployed outside their designated contexts. To overcome this limitation,\nwe propose AnomalyMoE, a novel and universal anomaly detection framework based\non a Mixture-of-Experts (MoE) architecture. Our key insight is to decompose the\ncomplex anomaly detection problem into three distinct semantic hierarchies:\nlocal structural anomalies, component-level semantic anomalies, and global\nlogical anomalies. AnomalyMoE correspondingly employs three dedicated expert\nnetworks at the patch, component, and global levels, and is specialized in\nreconstructing features and identifying deviations at its designated semantic\nlevel. This hierarchical design allows a single model to concurrently\nunderstand and detect a wide spectrum of anomalies. Furthermore, we introduce\nan Expert Information Repulsion (EIR) module to promote expert diversity and an\nExpert Selection Balancing (ESB) module to ensure the comprehensive utilization\nof all experts. Experiments on 8 challenging datasets spanning industrial\nimaging, 3D point clouds, medical imaging, video surveillance, and logical\nanomaly detection demonstrate that AnomalyMoE establishes new state-of-the-art\nperformance, significantly outperforming specialized methods in their\nrespective domains.", "AI": {"tldr": "提出AnomalyMoE框架，通过三层语义划分和对应专家网络处理，实现了跨领域异常检测的通用性，实验表明其在多种类型的数据集上表现最优。", "motivation": "传统的异常检测方法往往高度专业化，限制了通用性。这些特定模型在它们的指定背景之外部署时通常表现不佳，因此需要一种能够广泛适用于不同类型的异常检测的通用方法。", "method": "本文提出了AnomalyMoE，一种基于专家混合模型的通用异常检测框架。它将复杂的异常检测问题分解为三个语义层次：局部结构异常、组件级别语义异常和全局逻辑异常，并针对性地配置三个专家网络分别检测这三个层次的异常。此外，引入了专家信息排斥（EIR）模块与专家选择均衡（ESB）模块来提高专家多样性与均衡性。", "result": "实验结果表明，AnomalyMoE在8个包含工业成像、三维点云、医学成像、视频监控和逻辑异常检测等领域的测试数据集上取得了新的最先进性能，并显著超越了各自领域中的专门方法。", "conclusion": "AnomalyMoE通过专门化于重建特征和识别指定语义级别的偏差，并使用分层设计使单一模型能并行理解和检测广泛类型的异常。该框架展示了在多个领域超越现有专门化方法的能力。"}}
{"id": "2508.06205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06205", "abs": "https://arxiv.org/abs/2508.06205", "authors": ["Ruiyan Wang", "Lin Zuo", "Zonghao Lin", "Qiang Wang", "Zhengxue Cheng", "Rong Xie", "Jun Ling", "Li Song"], "title": "PA-HOI: A Physics-Aware Human and Object Interaction Dataset", "comment": null, "summary": "The Human-Object Interaction (HOI) task explores the dynamic interactions\nbetween humans and objects in physical environments, providing essential\nbiomechanical and cognitive-behavioral foundations for fields such as robotics,\nvirtual reality, and human-computer interaction. However, existing HOI data\nsets focus on details of affordance, often neglecting the influence of physical\nproperties of objects on human long-term motion. To bridge this gap, we\nintroduce the PA-HOI Motion Capture dataset, which highlights the impact of\nobjects' physical attributes on human motion dynamics, including human posture,\nmoving velocity, and other motion characteristics. The dataset comprises 562\nmotion sequences of human-object interactions, with each sequence performed by\nsubjects of different genders interacting with 35 3D objects that vary in size,\nshape, and weight. This dataset stands out by significantly extending the scope\nof existing ones for understanding how the physical attributes of different\nobjects influence human posture, speed, motion scale, and interacting\nstrategies. We further demonstrate the applicability of the PA-HOI dataset by\nintegrating it with existing motion generation methods, validating its capacity\nto transfer realistic physical awareness.", "AI": {"tldr": "The paper presents the PA-HOI Motion Capture dataset, a significant contribution to HOI research by emphasizing the role of object physical attributes on human motion characteristics, further validated with motion generation methods.", "motivation": "The motivation behind this paper is to address the limitation of existing Human-Object Interaction (HOI) datasets, which focus on affordance details but neglect the influence of object physical properties on long-term human motion.", "method": "The paper introduces the PA-HOI Motion Capture dataset, which includes 562 motion sequences of human-object interactions with a focus on the impact of objects' physical attributes (size, shape, weight) on human motion dynamics, such as posture, velocity, and motion characteristics.", "result": "The dataset created stands out by significantly extending the scope of existing ones, providing a deeper understanding into how different object physical attributes affect human posture, speed, motion scale, and interacting strategies.", "conclusion": "The paper concludes by demonstrating the applicability of the PA-HOI dataset with existing motion generation methods, confirming its potential to transfer realistic physical awareness."}}
{"id": "2508.06218", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06218", "abs": "https://arxiv.org/abs/2508.06218", "authors": ["Zhiyan Bo", "Laura C. Coates", "Bartlomiej W. Papiez"], "title": "Interpretable Rheumatoid Arthritis Scoring via Anatomy-aware Multiple Instance Learning", "comment": "Accepted by MICCAI AMAI Workshop 2025", "summary": "The Sharp/van der Heijde (SvdH) score has been widely used in clinical trials\nto quantify radiographic damage in Rheumatoid Arthritis (RA), but its\ncomplexity has limited its adoption in routine clinical practice. To address\nthe inefficiency of manual scoring, this work proposes a two-stage pipeline for\ninterpretable image-level SvdH score prediction using dual-hand radiographs.\nOur approach extracts disease-relevant image regions and integrates them using\nattention-based multiple instance learning to generate image-level features for\nprediction. We propose two region extraction schemes: 1) sampling image tiles\nmost likely to contain abnormalities, and 2) cropping patches containing\ndisease-relevant joints. With Scheme 2, our best individual score prediction\nmodel achieved a Pearson's correlation coefficient (PCC) of 0.943 and a root\nmean squared error (RMSE) of 15.73. Ensemble learning further boosted\nprediction accuracy, yielding a PCC of 0.945 and RMSE of 15.57, achieving\nstate-of-the-art performance that is comparable to that of experienced\nradiologists (PCC = 0.97, RMSE = 18.75). Finally, our pipeline effectively\nidentified and made decisions based on anatomical structures which clinicians\nconsider relevant to RA progression.", "AI": {"tldr": "该研究提出了一种基于X光片用于预测Sharpe/van der Heijde评分的两阶段管道，比手动评分更高效，且性能与专业放射科医生相当。", "motivation": "由于SvdH评分系统的复杂性，其在临床常规实践中普及受限，因此该研究旨在提高评分效率，为临床提供更高效的评分方法。", "method": "该研究提出了一种用于预测Sharpe/van der Heijde(SvdH)评分的双阶段管道，通过使用双手X光片进行可解释的图像级SvdH评分预测。方法包括抽样可能包含异常的图像块或裁剪包含疾病相关关节的补丁，然后使用基于注意力的多实例学习将它们整合成图像级特征来预测SvdH评分。", "result": "该管道实现了与经验丰富的放射科医生相当的准确性，产生的最佳个体评分预测模型取得了0.943的皮尔逊相关系数(PCC)和15.73的均方根误差(RMSE)。集成学习进一步提高了预测准确性，达到了0.945的PCC和15.57的RMSE。", "conclusion": "研究的管道能有效定位与RA进展相关的解剖结构，并做出决策，显示出与经验丰富的放射科医生相当的预测准确性，达到了最先进的性能。"}}
{"id": "2508.06224", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06224", "abs": "https://arxiv.org/abs/2508.06224", "authors": ["Guoyu Zhou", "Jing Zhang", "Yi Yan", "Hui Zhang", "Li Zhuo"], "title": "TEFormer: Texture-Aware and Edge-Guided Transformer for Semantic Segmentation of Urban Remote Sensing Images", "comment": "Submitted to GRSL", "summary": "Semantic segmentation of urban remote sensing images (URSIs) is crucial for\napplications such as urban planning and environmental monitoring. However,\ngeospatial objects often exhibit subtle texture differences and similar spatial\nstructures, which can easily lead to semantic ambiguity and misclassification.\nMoreover, challenges such as irregular object shapes, blurred boundaries, and\noverlapping spatial distributions of semantic objects contribute to complex and\ndiverse edge morphologies, further complicating accurate segmentation. To\ntackle these issues, we propose a texture-aware and edge-guided Transformer\n(TEFormer) that integrates texture awareness and edge-guidance mechanisms for\nsemantic segmentation of URSIs. In the encoder, a texture-aware module (TaM) is\ndesigned to capture fine-grained texture differences between visually similar\ncategories to enhance semantic discrimination. Then, an edge-guided tri-branch\ndecoder (Eg3Head) is constructed to preserve local edges and details for\nmultiscale context-awareness. Finally, an edge-guided feature fusion module\n(EgFFM) is to fuse contextual and detail information with edge information to\nrealize refined semantic segmentation. Extensive experiments show that TEFormer\nachieves mIoU of 88.57%, 81.46%, and 53.55% on the Potsdam, Vaihingen, and\nLoveDA datasets, respectively, shows the effectiveness in URSI semantic\nsegmentation.", "AI": {"tldr": "本文介绍了TEFormer模型，该模型通过引入纹理感知和边缘引导机制，有效解决了城市遥感图像语义分割中存在的纹理差异和空间结构复杂性问题，实验结果表明该模型在多个数据集上具有良好的分割性能。", "motivation": "城市遥感图像中的地理对象常表现出微小的纹理差异和相似的空间结构，这可能导致语义歧义和误分类。此外，不规则的对象形状、模糊的边界以及语义对象的空间分布重叠等挑战导致边缘形态复杂多样，进一步加大了精确分割的难度。本文正是为了解决这些问题而设计的TEFormer模型。", "method": "本文提出了一种名为TEFormer的纹理感知边缘引导Transformer模型，该模型整合了纹理感知模块和边缘引导机制，旨在解决城市遥感图像中地物细小纹理差异和类似空间结构导致的语义歧义及误分类问题。模型的编码器部分设计了纹理感知模块来捕捉视觉上相似类别的细微纹理差异，增强语义区分；解码器部分构建了边缘引导三分支解码器来保持局部边缘和细节；最后，引入了边缘引导特征融合模块来融合上下文信息和细节信息以实现精细化语义分割。", "result": "实验表明，TEFormer在Potsdam、Vaihingen和LoveDA数据集上的mIoU分别达到了88.57%、81.46%和53.55%，显示了其在城市遥感图像语义分割中的有效性。", "conclusion": "实验结果证明，通过纹理感知和边缘引导机制，TEFormer模型能够有效提高城市遥感图像的语义分割性能，并在多个数据集上取得了显著效果。"}}
{"id": "2508.06227", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06227", "abs": "https://arxiv.org/abs/2508.06227", "authors": ["Md Sazidur Rahman", "David Cabecinhas", "Ricard Marxer"], "title": "Depth Jitter: Seeing through the Depth", "comment": null, "summary": "Depth information is essential in computer vision, particularly in underwater\nimaging, robotics, and autonomous navigation. However, conventional\naugmentation techniques overlook depth aware transformations, limiting model\nrobustness in real world depth variations. In this paper, we introduce\nDepth-Jitter, a novel depth-based augmentation technique that simulates natural\ndepth variations to improve generalization. Our approach applies adaptive depth\noffsetting, guided by depth variance thresholds, to generate synthetic depth\nperturbations while preserving structural integrity. We evaluate Depth-Jitter\non two benchmark datasets, FathomNet and UTDAC2020 demonstrating its impact on\nmodel stability under diverse depth conditions. Extensive experiments compare\nDepth-Jitter against traditional augmentation strategies such as ColorJitter,\nanalyzing performance across varying learning rates, encoders, and loss\nfunctions. While Depth-Jitter does not always outperform conventional methods\nin absolute performance, it consistently enhances model stability and\ngeneralization in depth-sensitive environments. These findings highlight the\npotential of depth-aware augmentation for real-world applications and provide a\nfoundation for further research into depth-based learning strategies. The\nproposed technique is publicly available to support advancements in depth-aware\naugmentation. The code is publicly available on\n\\href{https://github.com/mim-team/Depth-Jitter}{github}.", "AI": {"tldr": "The paper introduces Depth-Jitter, a novel depth-based augmentation technique that improves the robustness of models in handling depth variations by generating synthetic depth perturbations. Evaluations show it enhances model stability and generalization in depth-sensitive applications, introducing depth-aware augmentation to enhance real-world performance.", "motivation": "The motivation is to address the limitation of conventional augmentation techniques in considering depth information, which are crucial for applications like underwater imaging, robotics, and autonomous navigation.", "method": "Depth-Jitter applies adaptive depth offsetting to simulate natural depth variations. The offsetting is guided by depth variance thresholds and aims to preserve the structure while creating depth perturbations.", "result": "The authors evaluated Depth-Jitter on FathomNet and UTDAC2020 datasets and demonstrated that it can enhance model stability and generalization compared to traditional augmentation methods like ColorJitter.", "conclusion": "Depth-Jitter consistently improves model stability in depth-sensitive scenarios, even if not always outperforming traditional methods in absolute performance. The research promotes depth-aware augmentation as a promising direction for enhancing models in depth-focused applications."}}
{"id": "2508.06228", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06228", "abs": "https://arxiv.org/abs/2508.06228", "authors": ["Daniel Feijoo", "Paula Garrido-Mellado", "Jaesung Rim", "Alvaro Garcia", "Marcos V. Conde"], "title": "Towards Unified Image Deblurring using a Mixture-of-Experts Decoder", "comment": "Preprint. Under review", "summary": "Image deblurring, removing blurring artifacts from images, is a fundamental\ntask in computational photography and low-level computer vision. Existing\napproaches focus on specialized solutions tailored to particular blur types,\nthus, these solutions lack generalization. This limitation in current methods\nimplies requiring multiple models to cover several blur types, which is not\npractical in many real scenarios. In this paper, we introduce the first\nall-in-one deblurring method capable of efficiently restoring images affected\nby diverse blur degradations, including global motion, local motion, blur in\nlow-light conditions, and defocus blur. We propose a mixture-of-experts (MoE)\ndecoding module, which dynamically routes image features based on the\nrecognized blur degradation, enabling precise and efficient restoration in an\nend-to-end manner. Our unified approach not only achieves performance\ncomparable to dedicated task-specific models, but also demonstrates remarkable\nrobustness and generalization capabilities on unseen blur degradation\nscenarios.", "AI": {"tldr": "This paper introduces the first all-in-one image deblurring method that can efficiently restore images affected by various blur types using a mixture-of-experts (MoE) decoding module.", "motivation": "Current methods for image deblurring lack generalization due to their specialization for particular blur types, leading to impractical solutions in many real scenarios. This paper aims to provide an all-in-one solution to address this issue.", "method": "We propose a mixture-of-experts (MoE) decoding module that dynamically routes image features based on the recognized blur degradation, facilitating precise and efficient restoration for various blur types.", "result": "The method achieves performance comparable to dedicated task-specific models while exhibiting remarkable robustness and generalization capabilities for unseen blur degradation scenarios.", "conclusion": "Our unified approach demonstrates the ability to perform comparably to specialized models while showing robustness and generalization across different types of blur degradations."}}
{"id": "2508.06248", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06248", "abs": "https://arxiv.org/abs/2508.06248", "authors": ["Andrii Yermakov", "Jan Cech", "Jiri Matas", "Mario Fritz"], "title": "Deepfake Detection that Generalizes Across Benchmarks", "comment": null, "summary": "The generalization of deepfake detectors to unseen manipulation techniques\nremains a challenge for practical deployment. Although many approaches adapt\nfoundation models by introducing significant architectural complexity, this\nwork demonstrates that robust generalization is achievable through a\nparameter-efficient adaptation of a pre-trained CLIP vision encoder. The\nproposed method, LNCLIP-DF, fine-tunes only the Layer Normalization parameters\n(0.03% of the total) and enhances generalization by enforcing a hyperspherical\nfeature manifold using L2 normalization and latent space augmentations.\n  We conducted an extensive evaluation on 13 benchmark datasets spanning from\n2019 to 2025. The proposed method achieves state-of-the-art performance,\noutperforming more complex, recent approaches in average cross-dataset AUROC.\nOur analysis yields two primary findings for the field: 1) training on paired\nreal-fake data from the same source video is essential for mitigating shortcut\nlearning and improving generalization, and 2) detection difficulty on academic\ndatasets has not strictly increased over time, with models trained on older,\ndiverse datasets showing strong generalization capabilities.\n  This work delivers a computationally efficient and reproducible method,\nproving that state-of-the-art generalization is attainable by making targeted,\nminimal changes to a pre-trained CLIP model. The code will be made publicly\navailable upon acceptance.", "AI": {"tldr": "该研究通过仅微调预训练CLIP模型的少量参数，提出了一个高效的深度伪造检测方法LNCLIP-DF，实现了跨数据集的高性能，表明了高效的模型训练策略和多样化的训练数据对提高模型泛化能力的重要性。", "motivation": "尽管许多方法通过引入显著的架构复杂性来适应基础模型，但本研究旨在证明，通过对预训练模型进行参数高效适应，也可以实现强大的泛化能力，以应对深度伪造检测器在未见过的操作技术中表现不佳的问题。", "method": "LNCLIP-DF方法通过仅微调预训练CLIP视觉编码器中的Layer Normalization参数（占总参数的0.03%），并使用L2正则化和潜在空间增强技术强制执行超球面特征流形，从而增强泛化能力。", "result": "该方法在涵盖从2019年到2025年的13个基准数据集上进行了广泛的评估，取得了最先进的性能，平均跨数据集AUROC优于更加复杂、近期的方法。", "conclusion": "本研究提出了一个计算效率高且可复现的方法，证明了通过对预训练的CLIP模型进行有针对性的、最小的改动就能达到最先进的泛化性。此外，研究还指出，训练过程中使用来自相同源视频的真实假数据对减少捷径学习和提高泛化性至关重要，而学术数据集上的检测难度并没有随着时间的推移而显著增加。"}}
{"id": "2508.06256", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06256", "abs": "https://arxiv.org/abs/2508.06256", "authors": ["Barış Büyüktaş", "Jonas Klotz", "Begüm Demir"], "title": "FedX: Explanation-Guided Pruning for Communication-Efficient Federated Learning in Remote Sensing", "comment": null, "summary": "Federated learning (FL) enables the collaborative training of deep neural\nnetworks across decentralized data archives (i.e., clients), where each client\nstores data locally and only shares model updates with a central server. This\nmakes FL a suitable learning paradigm for remote sensing (RS) image\nclassification tasks, where data centralization may be restricted due to legal\nand privacy constraints. However, a key challenge in applying FL to RS tasks is\nthe communication overhead caused by the frequent exchange of large model\nupdates between clients and the central server. To address this issue, in this\npaper we propose a novel strategy (denoted as FedX) that uses\nexplanation-guided pruning to reduce communication overhead by minimizing the\nsize of the transmitted models without compromising performance. FedX leverages\nbackpropagation-based explanation methods to estimate the task-specific\nimportance of model components and prunes the least relevant ones at the\ncentral server. The resulting sparse global model is then sent to clients,\nsubstantially reducing communication overhead. We evaluate FedX on multi-label\nscene classification using the BigEarthNet-S2 dataset and single-label scene\nclassification using the EuroSAT dataset. Experimental results show the success\nof FedX in significantly reducing the number of shared model parameters while\nenhancing the generalization capability of the global model, compared to both\nunpruned model and state-of-the-art pruning methods. The code of FedX will be\navailable at https://git.tu-berlin.de/rsim/FedX.", "AI": {"tldr": "本论文提出了一种减少联邦学习通信开销的新策略FedX，通过剪枝技术减少模型更新的大小而不会损害性能，实验表明其有效改善了模型泛化能力。", "motivation": "联邦学习在实现分散数据归档（即客户端）中深度神经网络的协作训练方面具有潜力，但通信开销是将联邦学习应用于遥感图像分类任务的主要挑战，因为每个客户端本地存储数据并且仅与中央服务器共享模型更新。", "method": "Fedx使用解释指导剪枝策略来减少联邦学习中客户端与中央服务器之间频繁交换大型模型更新造成的通信开销，通过最小化发送模型的大小而不影响性能来实现。Fedx在中央服务器处利用基于反向传播的解释方法来估计模型组件的任务特定重要性，并剪枝最不相关的组件。", "result": "实验结果表明，与未剪枝模型和最先进的剪枝方法相比，Fedx在显著减少共享模型参数数量的同时，增强了全局模型的泛化能力。", "conclusion": "论文结论表明，基于解释引导剪枝的FedX策略在减少通信开销同时提高了模型性能，是一个有效的解决方案。"}}
{"id": "2508.06258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06258", "abs": "https://arxiv.org/abs/2508.06258", "authors": ["Byunghyun Ko", "Anning Tian", "Jeongkyu Lee"], "title": "XAG-Net: A Cross-Slice Attention and Skip Gating Network for 2.5D Femur MRI Segmentation", "comment": "Accepted at the 2025 International Conference on Artificial\n  Intelligence, Computer, Data Sciences and Applications (ACDSA). This is the\n  preprint version of the paper", "summary": "Accurate segmentation of femur structures from Magnetic Resonance Imaging\n(MRI) is critical for orthopedic diagnosis and surgical planning but remains\nchallenging due to the limitations of existing 2D and 3D deep learning-based\nsegmentation approaches. In this study, we propose XAG-Net, a novel 2.5D\nU-Net-based architecture that incorporates pixel-wise cross-slice attention\n(CSA) and skip attention gating (AG) mechanisms to enhance inter-slice\ncontextual modeling and intra-slice feature refinement. Unlike previous\nCSA-based models, XAG-Net applies pixel-wise softmax attention across adjacent\nslices at each spatial location for fine-grained inter-slice modeling.\nExtensive evaluations demonstrate that XAG-Net surpasses baseline 2D, 2.5D, and\n3D U-Net models in femur segmentation accuracy while maintaining computational\nefficiency. Ablation studies further validate the critical role of the CSA and\nAG modules, establishing XAG-Net as a promising framework for efficient and\naccurate femur MRI segmentation.", "AI": {"tldr": "本文提出了一种用于股骨MRI结构分割的2.5D U-Net架构XAG-Net，该架构使用了像素级CSA和AG机制，在提高准确性的同时保持了计算效率。", "motivation": "准确的股骨结构分割对于骨科诊断和手术规划至关重要，但由于现有的2D和3D深度学习分割方法的限制，它仍然是一个挑战。", "method": "XAG-Net, 一种基于2.5D U-Net的新型架构，融合了像素级跨层注意力机制（CSA）和跳越注意力门控机制（AG），以增强跨层上下文建模和层内特征细化。", "result": "广泛的评估表明，XAG-Net在股骨分割准确性方面超过了基线的2D、2.5D和3D U-Net模型，同时保持了计算效率。", "conclusion": "剪枝研究表明CSA和AG模块的关键作用，确立了XAG-Net作为一种高效和准确的股骨MRI分割框架的前景。"}}
{"id": "2508.06259", "categories": ["cs.CV", "cs.AI", "I.2.10"], "pdf": "https://arxiv.org/pdf/2508.06259", "abs": "https://arxiv.org/abs/2508.06259", "authors": ["Zhangquan Chen", "Ruihui Zhao", "Chuwei Luo", "Mingze Sun", "Xinlei Yu", "Yangyang Kang", "Ruqi Huang"], "title": "SIFThinker: Spatially-Aware Image Focus for Visual Reasoning", "comment": "15 pages, 13 figures", "summary": "Current multimodal large language models (MLLMs) still face significant\nchallenges in complex visual tasks (e.g., spatial understanding, fine-grained\nperception). Prior methods have tried to incorporate visual reasoning, however,\nthey fail to leverage attention correction with spatial cues to iteratively\nrefine their focus on prompt-relevant regions. In this paper, we introduce\nSIFThinker, a spatially-aware \"think-with-images\" framework that mimics human\nvisual perception. Specifically, SIFThinker enables attention correcting and\nimage region focusing by interleaving depth-enhanced bounding boxes and natural\nlanguage. Our contributions are twofold: First, we introduce a\nreverse-expansion-forward-inference strategy that facilitates the generation of\ninterleaved image-text chains of thought for process-level supervision, which\nin turn leads to the construction of the SIF-50K dataset. Besides, we propose\nGRPO-SIF, a reinforced training paradigm that integrates depth-informed visual\ngrounding into a unified reasoning pipeline, teaching the model to dynamically\ncorrect and focus on prompt-relevant regions. Extensive experiments demonstrate\nthat SIFThinker outperforms state-of-the-art methods in spatial understanding\nand fine-grained visual perception, while maintaining strong general\ncapabilities, highlighting the effectiveness of our method.", "AI": {"tldr": "本文介绍了一种名为 SIFThinker 的框架，它通过深度增强的边界框和自然语言的交替来提升多模态大语言模型在复杂视觉任务中的表现，特别是在空间理解和细粒度视觉感知上。", "motivation": "当前的多模态大语言模型在处理复杂的视觉任务时依然面临挑战，例如空间理解和细粒度感知。现有的方法未能充分利用空间线索进行注意力修正从而迭代地专注于与提示相关的区域。", "method": "SIFThinker, 一种具有空间感知能力的 '用图像思考' 框架，该框架通过在增强深度的边界框和自然语言之间交替，提升了对提示相关区域的注意力修正和图像区域聚焦能力。", "result": "实验结果表明，SIFThinker 在空间理解和细粒度视觉感知上优于最先进的方法，并且保持了强大的通用能力，证明了该方法的有效性。", "conclusion": "提出的方法在具体任务上展示了显著的性能提升，表明空间感知和图像区域聚焦在多模态模型中的重要性。"}}
{"id": "2508.06317", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06317", "abs": "https://arxiv.org/abs/2508.06317", "authors": ["Jian Hu", "Zixu Cheng", "Shaogang Gong", "Isabel Guan", "Jianye Hao", "Jun Wang", "Kun Shao"], "title": "Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding", "comment": null, "summary": "Video Temporal Grounding (TG) aims to temporally locate video segments\nmatching a natural language description (a query) in a long video. While\nVision-Language Models (VLMs) are effective at holistic semantic matching, they\noften struggle with fine-grained temporal localisation. Recently, Group\nRelative Policy Optimisation (GRPO) reformulates the inference process as a\nreinforcement learning task, enabling fine-grained grounding and achieving\nstrong in-domain performance. However, GRPO relies on labelled data, making it\nunsuitable in unlabelled domains. Moreover, because videos are large and\nexpensive to store and process, performing full-scale adaptation introduces\nprohibitive latency and computational overhead, making it impractical for\nreal-time deployment. To overcome both problems, we introduce a Data-Efficient\nUnlabelled Cross-domain Temporal Grounding method, from which a model is first\ntrained on a labelled source domain, then adapted to a target domain using only\na small number of unlabelled videos from the target domain. This approach\neliminates the need for target annotation and keeps both computational and\nstorage overhead low enough to run in real time. Specifically, we introduce.\nUncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain\nknowledge transfer in learning video temporal grounding without target labels.\nURPA generates multiple candidate predictions using GRPO rollouts, averages\nthem to form a pseudo label, and estimates confidence from the variance across\nthese rollouts. This confidence then weights the training rewards, guiding the\nmodel to focus on reliable supervision. Experiments on three datasets across\nsix cross-domain settings show that URPA generalises well using only a few\nunlabelled target videos. Codes will be released once published.", "AI": {"tldr": "提出了一种高效利用少量未标注目标域视频进行视频时间定位的新方法URPA，克服了GRPO方法需要大量标注数据、计算和存储开销大的问题，实现了跨域知识迁移和实时部署。", "motivation": "改进视频时间定位技术，以应对现有方法如GRPO标注数据依赖且计算存储成本高的问题，提高跨域适应性和实时性能。", "method": "利用不确定性量化的策略回放适应方法（URPA）来无需标注目标域标签进行视频时间定位的训练，通过生成多组候选预测并计算平均值形成伪标签，同时信心评分用于奖励训练，使模型能专注于可靠的监督。", "result": "实验覆盖三个数据集及六个跨域环境，结果证明URPA能够利用少量未标注的目标域视频取得良好的跨域泛化效果。", "conclusion": "提出的方法有效减少了模型对标注数据的依赖，同时在计算存储成本方面更低，实现了快速且适应性强的视频时间定位能力。"}}
{"id": "2508.06318", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06318", "abs": "https://arxiv.org/abs/2508.06318", "authors": ["Giacomo D'Amicantonio", "Snehashis Majhi", "Quan Kong", "Lorenzo Garattoni", "Gianpiero Francesca", "François Bremond", "Egor Bondarev"], "title": "Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection", "comment": null, "summary": "Video Anomaly Detection (VAD) is a challenging task due to the variability of\nanomalous events and the limited availability of labeled data. Under the\nWeakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided\nduring training, while predictions are made at the frame level. Although\nstate-of-the-art models perform well on simple anomalies (e.g., explosions),\nthey struggle with complex real-world events (e.g., shoplifting). This\ndifficulty stems from two key issues: (1) the inability of current models to\naddress the diversity of anomaly types, as they process all categories with a\nshared model, overlooking category-specific features; and (2) the weak\nsupervision signal, which lacks precise temporal information, limiting the\nability to capture nuanced anomalous patterns blended with normal events. To\naddress these challenges, we propose Gaussian Splatting-guided Mixture of\nExperts (GS-MoE), a novel framework that employs a set of expert models, each\nspecialized in capturing specific anomaly types. These experts are guided by a\ntemporal Gaussian splatting loss, enabling the model to leverage temporal\nconsistency and enhance weak supervision. The Gaussian splatting approach\nencourages a more precise and comprehensive representation of anomalies by\nfocusing on temporal segments most likely to contain abnormal events. The\npredictions from these specialized experts are integrated through a\nmixture-of-experts mechanism to model complex relationships across diverse\nanomaly patterns. Our approach achieves state-of-the-art performance, with a\n91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on\nXD-Violence and MSAD datasets. By leveraging category-specific expertise and\ntemporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.", "AI": {"tldr": "本论文提出GS-MoE框架，通过多个专家模型和时间高斯插补损失函数，解决了现有模型在视频异常检测任务中面临的问题，实现了类别特定异常的精准检测，并在多个数据集上取得了优异性能。", "motivation": "当前模型在处理多种异常事件类型时，由于采用单一模型处理所有类别，忽略类别特定特征，以及弱监督信号缺乏精确时间信息的限制，它们无法很好地捕捉与正常事件混合的细微异常模式。", "method": "GS-MoE框架使用一套专家模型，每个模型专门针对特定类型的异常事件。这些专家模型通过时间高斯插补损失函数的引导，增强了弱监督学习中对时间一致性和异常模式的捕捉能力。高斯插补方法专注于最有可能包含异常事件的时间段，使得模型可以更精准、全面地表示异常事件。预测结果通过专家混合机制综合，以建模跨多样异常模式的复杂关系。", "result": "该方法在UCF-Crime数据集上达到了91.58%的AUC，并在XD-Violence和MSAD数据集上表现出优越的结果。", "conclusion": "通过利用类别特定的专长和时间指导，GS-MoE为弱监督下的视频异常检测任务设定了新的基准。"}}
{"id": "2508.06327", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06327", "abs": "https://arxiv.org/abs/2508.06327", "authors": ["Xin Ci Wong", "Duygu Sarikaya", "Kieran Zucker", "Marc De Kamps", "Nishant Ravikumar"], "title": "Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?", "comment": "ICONIP 2025", "summary": "Magnetic resonance (MR) imaging, including cardiac MR, is prone to domain\nshift due to variations in imaging devices and acquisition protocols. This\nchallenge limits the deployment of trained AI models in real-world scenarios,\nwhere performance degrades on unseen domains. Traditional solutions involve\nincreasing the size of the dataset through ad-hoc image augmentation or\nadditional online training/transfer learning, which have several limitations.\nSynthetic data offers a promising alternative, but anatomical/structural\nconsistency constraints limit the effectiveness of generative models in\ncreating image-label pairs. To address this, we propose a diffusion model (DM)\ntrained on a source domain that generates synthetic cardiac MR images that\nresemble a given reference. The synthetic data maintains spatial and structural\nfidelity, ensuring similarity to the source domain and compatibility with the\nsegmentation mask. We assess the utility of our generative approach in\nmulti-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net and\nvanilla U-Net segmentation networks. We explore domain generalisation, where,\ndomain-invariant segmentation models are trained on synthetic source domain\ndata, and domain adaptation, where, we shift target domain data towards the\nsource domain using the DM. Both strategies significantly improved segmentation\nperformance on data from an unseen target domain, in terms of surface-based\nmetrics (Welch's t-test, p < 0.01), compared to training segmentation models on\nreal data alone. The proposed method ameliorates the need for transfer learning\nor online training to address domain shift challenges in cardiac MR image\nanalysis, especially useful in data-scarce settings.", "AI": {"tldr": "提出了一种扩散模型来生成用于心脏MRI合成图像，改善了域移问题，提高了在未见域的有效性。", "motivation": "磁共振成像（MRI）包括心脏MRI，由于成像设备和采集协议的差异而容易出现领域漂移。这限制了训练的人工智能模型在现实场景中的部署，并且性能在未见过的领域中下降。", "method": "我们提出了一种扩散模型（DM），该模型在源域上训练，以生成类似于给定参考的心脏磁共振成像（MRI）合成图像。这些合成图像在空间和结构上保持保真度，确保与源域的相似性，并与分割掩码兼容。", "result": "我们在多中心心脏MRI分割中评估了我们生成方法的效用，使用2D nnU-Net、3D nnU-Net和vanilla U-Net分割网络。我们的方法在目标域上的分割性能（基于Welch's t-test, p < 0.01）显著优于仅训练真实数据的分割模型。我们探索了领域泛化和领域适应两种策略，其中领域不变的分割模型在合成源域数据上训练，或将目标域数据转换为源域数据。", "conclusion": "本研究提出的方法在没有额外转移学习或在线训练的情况下改善了心脏MRI图像分析中的领域迁移挑战，尤其是在数据稀缺的情境中非常有用。"}}
{"id": "2508.06335", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06335", "abs": "https://arxiv.org/abs/2508.06335", "authors": ["Patrick Takenaka", "Johannes Maucher", "Marco F. Huber"], "title": "ViPro-2: Unsupervised State Estimation via Integrated Dynamics for Guiding Video Prediction", "comment": "Published in 2025 International Joint Conference on Neural Networks\n  (IJCNN)", "summary": "Predicting future video frames is a challenging task with many downstream\napplications. Previous work has shown that procedural knowledge enables deep\nmodels for complex dynamical settings, however their model ViPro assumed a\ngiven ground truth initial symbolic state. We show that this approach led to\nthe model learning a shortcut that does not actually connect the observed\nenvironment with the predicted symbolic state, resulting in the inability to\nestimate states given an observation if previous states are noisy. In this\nwork, we add several improvements to ViPro that enables the model to correctly\ninfer states from observations without providing a full ground truth state in\nthe beginning. We show that this is possible in an unsupervised manner, and\nextend the original Orbits dataset with a 3D variant to close the gap to real\nworld scenarios.", "AI": {"tldr": "改进了ViPro模型，实现了从观测数据中无监督地推断状态，并扩展了数据集以接近真实场景。", "motivation": "先前工作的模型依赖于给定的真值初始符号状态，这导致模型仅学习了一个捷径，即没有真正将观察到的环境与预测的符号状态相联系，因此无法在状态有噪音的情况下估计状态。", "method": "通过对ViPro模型进行改进，新方法使得模型可以仅根据观测数据正确推断状态，而无需初始提供完整的真值状态。同时，扩展了Orbits数据集，增加了一个3D变体来缩小与现实场景之间的差距。", "result": "通过改进，新的模型可以正确地从观测数据中推断状态，无需提供完整的真值状态，并且在扩展的数据集上验证了这一点。", "conclusion": "改进的方法证明了在没有提供全真值初始状态的情况下，可以通过观测直接正确推断状态的可能性，这为预测未来视频帧打开了新的可能性。"}}
{"id": "2508.06342", "categories": ["cs.CV", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.06342", "abs": "https://arxiv.org/abs/2508.06342", "authors": ["Kieran Elrod", "Katherine Flanigan", "Mario Bergés"], "title": "Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities", "comment": null, "summary": "Designing socially active streets has long been a goal of urban planning, yet\nexisting quantitative research largely measures pedestrian volume rather than\nthe quality of social interactions. We hypothesize that street view imagery --\nan inexpensive data source with global coverage -- contains latent social\ninformation that can be extracted and interpreted through established social\nscience theory. As a proof of concept, we analyzed 2,998 street view images\nfrom 15 cities using a multimodal large language model guided by Mehta's\ntaxonomy of passive, fleeting, and enduring sociability -- one illustrative\nexample of a theory grounded in urban design that could be substituted or\ncomplemented by other sociological frameworks. We then used linear regression\nmodels, controlling for factors like weather, time of day, and pedestrian\ncounts, to test whether the inferred sociability measures correlate with\ncity-level place attachment scores from the World Values Survey and with\nenvironmental predictors (e.g., green, sky, and water view indices) derived\nfrom individual street view images. Results aligned with long-standing urban\nplanning theory: the sky view index was associated with all three sociability\ntypes, the green view index predicted enduring sociability, and place\nattachment was positively associated with fleeting sociability. These results\nprovide preliminary evidence that street view images can be used to infer\nrelationships between specific types of social interactions and built\nenvironment variables. Further research could establish street view imagery as\na scalable, privacy-preserving tool for studying urban sociability, enabling\ncross-cultural theory testing and evidence-based design of socially vibrant\ncities.", "AI": {"tldr": "通过街景图像分析城市的社交互动，发现天际线视图指数与所有三种社会性类型相关，绿化视图指数预测持久社会性，场所依恋与短暂社会性呈正相关。", "motivation": "作者认为街景图像包含可被提取和解释的潜在社会信息，旨在证明街景图像可以成为研究城市社会互动的可扩展且保护隐私的工具。现有定量研究多数关注行人数量，而不是社会互动的质量。", "method": "本文使用多元大语言模型分析了来自15个城市的2,998张街景图像中的社会互动，基于Mehta的社会性分类进行指导。然后通过多元线性回归模型检验了推测的社会性度量与来自世界价值观调查的城市层面的场所依恋得分以及从单个街景图像中提取的环境预测指标之间的关系。", "result": "研究结果表明天际线视图指数与所有三种社会性类型相关，绿化视图指数预测持久社会性，场所依恋与短暂社会性呈正相关。", "conclusion": "研究表明街景图像能够用于推断特定类型社会互动与建设环境变量之间的关系，这为城市社会性研究提供了一种可扩展且保护隐私的方法，有助于跨文化交流理论测试和以证据为基础的城市设计。"}}
{"id": "2508.06350", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06350", "abs": "https://arxiv.org/abs/2508.06350", "authors": ["Yingxian Chen", "Jiahui Liu", "Ruifan Di", "Yanwei Li", "Chirui Chang", "Shizhen Zhao", "Wilton W. T. Fok", "Xiaojuan Qi", "Yik-Chung Wu"], "title": "Aligning Effective Tokens with Video Anomaly in Large Language Models", "comment": null, "summary": "Understanding abnormal events in videos is a vital and challenging task that\nhas garnered significant attention in a wide range of applications. Although\ncurrent video understanding Multi-modal Large Language Models (MLLMs) are\ncapable of analyzing general videos, they often struggle to handle anomalies\ndue to the spatial and temporal sparsity of abnormal events, where the\nredundant information always leads to suboptimal outcomes. To address these\nchallenges, exploiting the representation and generalization capabilities of\nVison Language Models (VLMs) and Large Language Models (LLMs), we propose\nVA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in\nvarious videos. Our approach efficiently aligns effective tokens between visual\nencoders and LLMs through two key proposed modules: Spatial Effective Token\nSelection (SETS) and Temporal Effective Token Generation (TETG). These modules\nenable our model to effectively capture and analyze both spatial and temporal\ninformation associated with abnormal events, resulting in more accurate\nresponses and interactions. Furthermore, we construct an instruction-following\ndataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a\ncross-domain evaluation benchmark based on XD-Violence dataset. Our proposed\nmethod outperforms existing state-of-the-art methods on various benchmarks.", "AI": {"tldr": "本文提出了VA-GPT模型，利用视觉语言模型和大型语言模型的表示和泛化能力，通过SETS和TETG模块解决了异常事件视频理解中的时空信息捕捉问题，提高了模型在异常事件检测上的性能。", "motivation": "本文旨在解决现有视频理解多模态大型语言模型在处理异常事件时因异常事件的时空稀疏性和冗余信息导致结果次优的问题。", "method": "本文提出了VA-GPT，一种用于总结和定位各种视频中异常事件的新型多模态大型语言模型。该方法通过两个关键模块，空间有效令牌选择（SETS）和时间有效令牌生成（TETG），实现了视觉编码器和大型语言模型之间的有效令牌对齐，从而能够更有效地捕捉和分析与异常事件相关的时空信息。", "result": "本文的方法在多个基准测试上优于现有的最先进的方法。", "conclusion": "本文通过提出VA-GPT模型，有效解决了视频理解中异常事件的时空信息捕捉和处理问题，并通过特定的指令跟随数据集和跨领域评估基准进一步验证了这一方法的有效性。"}}
{"id": "2508.06351", "categories": ["cs.CV", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.06351", "abs": "https://arxiv.org/abs/2508.06351", "authors": ["Olakunle S. Abawonse", "Günay Doğan"], "title": "An Implemention of Two-Phase Image Segmentation using the Split Bregman Method", "comment": "15 pages", "summary": "In this paper, we describe an implementation of the two-phase image\nsegmentation algorithm proposed by Goldstein, Bresson, Osher in\n\\cite{gold:bre}. This algorithm partitions the domain of a given 2d image into\nforeground and background regions, and each pixel of the image is assigned\nmembership to one of these two regions. The underlying assumption for the\nsegmentation model is that the pixel values of the input image can be\nsummarized by two distinct average values, and that the region boundaries are\nsmooth. Accordingly, the model is defined as an energy in which the variable is\na region membership function to assign pixels to either region, originally\nproposed by Chan and Vese in \\cite{chan:vese}. This energy is the sum of image\ndata terms in the regions and a length penalty for region boundaries.\nGoldstein, Bresson, Osher modify the energy of Chan-Vese in \\cite{gold:bre} so\nthat their new energy can be minimized efficiently using the split Bregman\nmethod to produce an equivalent two-phase segmentation. We provide a detailed\nimplementation of this method \\cite{gold:bre}, and document its performance\nwith several images over a range of algorithm parameters.", "AI": {"tldr": "这篇论文描述了Goldstein, Bresson, Osher提出的两阶段图像分割算法的实现，该算法通过分割Bregman方法高效地划分图像为前景和背景区域。", "motivation": "论文动机在于实现一种能够将图像分割为前景和背景的算法，特别是当像素值可以用两个不同的平均值来描述，且区域边界是平滑的情况下。", "method": "该论文采用Goldstein, Bresson, Osher改进的两阶段图像分割算法，该算法基于Chan和Vese提出的能量函数，并通过分割的Bregman方法实现高效的最小化。", "result": "作者详细实现了这一方法，并通过一系列图像在不同算法参数范围内的表现来验证其性能。", "conclusion": "这项研究展示了改进的两阶段图像分割算法的有效性和实用性，特别是在区域边界平滑假设的情况下。"}}
{"id": "2508.06357", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06357", "abs": "https://arxiv.org/abs/2508.06357", "authors": ["Aman Bhatta", "Maria Dhakal", "Michael C. King", "Kevin W. Bowyer"], "title": "Are you In or Out (of gallery)? Wisdom from the Same-Identity Crowd", "comment": null, "summary": "A central problem in one-to-many facial identification is that the person in\nthe probe image may or may not have enrolled image(s) in the gallery; that is,\nmay be In-gallery or Out-of-gallery. Past approaches to detect when a rank-one\nresult is Out-of-gallery have mostly focused on finding a suitable threshold on\nthe similarity score. We take a new approach, using the additional enrolled\nimages of the identity with the rank-one result to predict if the rank-one\nresult is In-gallery / Out-of-gallery. Given a gallery of identities and\nimages, we generate In-gallery and Out-of-gallery training data by extracting\nthe ranks of additional enrolled images corresponding to the rank-one identity.\nWe then train a classifier to utilize this feature vector to predict whether a\nrank-one result is In-gallery or Out-of-gallery. Using two different datasets\nand four different matchers, we present experimental results showing that our\napproach is viable for mugshot quality probe images, and also, importantly, for\nprobes degraded by blur, reduced resolution, atmospheric turbulence and\nsunglasses. We also analyze results across demographic groups, and show that\nIn-gallery / Out-of-gallery classification accuracy is similar across\ndemographics. Our approach has the potential to provide an objective estimate\nof whether a one-to-many facial identification is Out-of-gallery, and thereby\nto reduce false positive identifications, wrongful arrests, and wasted\ninvestigative time. Interestingly, comparing the results of older deep\nCNN-based face matchers with newer ones suggests that the effectiveness of our\nOut-of-gallery detection approach emerges only with matchers trained using\nadvanced margin-based loss functions.", "AI": {"tldr": "本研究提出了一种新的方法，通过利用身份额外登记图像来预测一比一结果是否为馆内或馆外，从而改进传统的馆外检测方法。", "motivation": "传统的馆外检测方法主要集中在设定相似度分数的阈值上。这种方法忽略了利用额外的登记图像来进行更精确预测的可能。为了减少误识别、错误逮捕和浪费的调查时间，本研究旨在提供一种客观的馆外检测方法。", "method": "本研究提出了一种利用身份额外登记图像的方法来预测一比多面部识别中的一比一结果是否是馆内/馆外检测。通过提取与一比一身份对应的额外登记图像的排名，生成训练数据。然后训练一个分类器，利用特征向量来预测一比一结果是馆内还是馆外。", "result": "通过两个不同的数据集和四种不同的匹配器验证了该方法的有效性，适用于质量不高或有降质因素的探头图像，并能在不同人群中保持相似的预测精度。", "conclusion": "实验结果显示，该方法对于质量不同的探头图像均有效，且在不同人群中馆内/馆外分类精度相似。同时证明，只有使用先进边际损失函数训练的面部识别匹配器，本方法的效果才会有所提升。"}}
{"id": "2508.06382", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06382", "abs": "https://arxiv.org/abs/2508.06382", "authors": ["Xiangyu Wu", "Feng Yu", "Yang Yang", "Jianfeng Lu"], "title": "Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning", "comment": "Accepted for publication at ACMMM 2025", "summary": "The integration of prompt tuning with multimodal learning has shown\nsignificant generalization abilities for various downstream tasks. Despite\nadvancements, existing methods heavily depend on massive modality-specific\nlabeled data (e.g., video, audio, and image), or are customized for a single\nmodality. In this study, we present Text as Any-Modality by Consistent Prompt\nTuning (TaAM-CPT), a scalable approach for constructing a general\nrepresentation model toward unlimited modalities using solely text data.\nTaAM-CPT comprises modality prompt pools, text construction, and\nmodality-aligned text encoders from pre-trained models, which allows for\nextending new modalities by simply adding prompt pools and modality-aligned\ntext encoders. To harmonize the learning across different modalities, TaAM-CPT\ndesigns intra- and inter-modal learning objectives, which can capture category\ndetails within modalities while maintaining semantic consistency across\ndifferent modalities. Benefiting from its scalable architecture and pre-trained\nmodels, TaAM-CPT can be seamlessly extended to accommodate unlimited\nmodalities. Remarkably, without any modality-specific labeled data, TaAM-CPT\nachieves leading results on diverse datasets spanning various modalities,\nincluding video classification, image classification, and audio classification.\nThe code is available at https://github.com/Jinx630/TaAM-CPT.", "AI": {"tldr": "本文提出了TaAM-CPT方法，利用文本数据构建通用的多模态模型，可扩展到任意模态，不依赖大规模模态特定标注数据。", "motivation": "现有的模态学习方法高度依赖大量的模态特定标注数据，或为单一模态定制。TaAM-CPT旨在提供一个可扩展的方法，能够只使用文本数据构建面向无限模态的通用模型，解决现有方法的局限性。", "method": "TaAM-CPT采用一致性的prompt调优方法，利用单一文本数据构建面向无限模态的通用表示模型。该方法包括模态prompt池、文本构建和模态一致的文本编码器从预训练模型中获取信息。通过增加prompt池和模态一致的文本编码器来扩展新的模态。其设计的模态内和模态间学习目标有助于在不同模态之间保持语义一致性。", "result": "TaAM-CPT在不使用任何模态特定标注数据的情况下，在视频分类、图像分类和音频分类等多个模态的数据集上取得了领先结果。", "conclusion": "TaAM-CPT证明了可以通过仅使用文本数据构建泛模态的先进模型，展示了在多个模态任务上的推广能力及领先性能。"}}
{"id": "2508.06392", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06392", "abs": "https://arxiv.org/abs/2508.06392", "authors": ["Wenbin Teng", "Gonglin Chen", "Haiwei Chen", "Yajie Zhao"], "title": "FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation", "comment": null, "summary": "Recent progress in 3D reconstruction has enabled realistic 3D models from\ndense image captures, yet challenges persist with sparse views, often leading\nto artifacts in unseen areas. Recent works leverage Video Diffusion Models\n(VDMs) to generate dense observations, filling the gaps when only sparse views\nare available for 3D reconstruction tasks. A significant limitation of these\nmethods is their slow sampling speed when using VDMs. In this paper, we present\nFVGen, a novel framework that addresses this challenge by enabling fast novel\nview synthesis using VDMs in as few as four sampling steps. We propose a novel\nvideo diffusion model distillation method that distills a multi-step denoising\nteacher model into a few-step denoising student model using Generative\nAdversarial Networks (GANs) and softened reverse KL-divergence minimization.\nExtensive experiments on real-world datasets show that, compared to previous\nworks, our framework generates the same number of novel views with similar (or\neven better) visual quality while reducing sampling time by more than 90%.\nFVGen significantly improves time efficiency for downstream reconstruction\ntasks, particularly when working with sparse input views (more than 2) where\npre-trained VDMs need to be run multiple times to achieve better spatial\ncoverage.", "AI": {"tldr": "本文介绍了一种新型框架FVGen，它通过生成对抗网络（GAN）和软KL散度最小化的视频扩散模型提炼方法，实现快速新视图合成，显著提高了在稀疏视图情况下下游重建任务的时间效率。", "motivation": "尽管3D重建方面取得了进展，但在使用稀疏视图时仍然存在挑战，导致未见区域出现瑕疵。此前的工作通过视频扩散模型（VDM）生成稠密观察来填充这些缝隙，但这些方法在采样速度上存在局限性。", "method": "我们提出了一种名为FVGen的新框架，使用生成对抗网络（GAN）和软化的逆KL散度最小化方法，将多步去噪教师模型提炼为少步去噪学生模型，从而实现使用视频扩散模型进行快速新视图合成。", "result": "实验证明，与之前的工作相比，我们的框架能够在生成相同数量新视图的同时，将采样时间减少90%以上，并且保持或提升了视觉质量。", "conclusion": "FVGen框架在处理稀疏输入视图的3D重建任务中，显著提升了时间效率，解决了预训练VDM在较少视图下需要多次运行的问题。"}}
{"id": "2508.06407", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.06407", "abs": "https://arxiv.org/abs/2508.06407", "authors": ["Ch Muhammad Awais", "Marco Reggiannini", "Davide Moroni", "Oktay Karakus"], "title": "A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery", "comment": null, "summary": "High-resolution imagery plays a critical role in improving the performance of\nvisual recognition tasks such as classification, detection, and segmentation.\nIn many domains, including remote sensing and surveillance, low-resolution\nimages can limit the accuracy of automated analysis. To address this,\nsuper-resolution (SR) techniques have been widely adopted to attempt to\nreconstruct high-resolution images from low-resolution inputs. Related\ntraditional approaches focus solely on enhancing image quality based on\npixel-level metrics, leaving the relationship between super-resolved image\nfidelity and downstream classification performance largely underexplored. This\nraises a key question: can integrating classification objectives directly into\nthe super-resolution process further improve classification accuracy? In this\npaper, we try to respond to this question by investigating the relationship\nbetween super-resolution and classification through the deployment of a\nspecialised algorithmic strategy. We propose a novel methodology that increases\nthe resolution of synthetic aperture radar imagery by optimising loss functions\nthat account for both image quality and classification performance. Our\napproach improves image quality, as measured by scientifically ascertained\nimage quality indicators, while also enhancing classification accuracy.", "AI": {"tldr": "本文提出了一种新的超分算法，该算法在提升图像质量的同时，也改善了分类准确性，特别是在合成孔径雷达图像上。", "motivation": "本文的动机在于探索将分类目标直接整合到超分辨率过程中是否可以进一步提高分类准确性。由于传统的超分辨率方法主要侧重于在像素级别上提升图像质量，而忽略了这种提升与下游分类任务准确性之间的关系，因此这项研究就显得尤为重要。", "method": "本文提出了一种新的算法策略，通过优化损失函数来同时优化图像质量和分类性能，这种损失函数在提升合成孔径雷达图像分辨率的同时考虑了分类准确性。", "result": "通过对论文摘要的分析，本文研究了超分辨率技术和分类之间的关系，并提出了一种新的方法，该方法通过优化损失函数来同时提升图像质量和分类准确性。特别地，该算法应用于合成孔径雷达图像，成功增强了低分辨率图像的分辨率，并提升了分类任务的性能。实验结果验证了该方法的有效性，即在提高图像质量的同时，也改善了分类准确性。", "conclusion": "结论表明，通过综合优化图像质量和分类性能，本文提出的方法能够提升超分辨率图像的忠实度并增加分类任务的准确性。实验结果验证了该方法的有效性。"}}
{"id": "2508.06420", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06420", "abs": "https://arxiv.org/abs/2508.06420", "authors": ["Ch Muhammad Awais", "Marco Reggiannini", "Davide Moroni", "Oktay Karakus"], "title": "Feature-Space Oversampling for Addressing Class Imbalance in SAR Ship Classification", "comment": "Accepted and presented at IGARSS", "summary": "SAR ship classification faces the challenge of long-tailed datasets, which\ncomplicates the classification of underrepresented classes. Oversampling\nmethods have proven effective in addressing class imbalance in optical data. In\nthis paper, we evaluated the effect of oversampling in the feature space for\nSAR ship classification. We propose two novel algorithms inspired by the\nMajor-to-minor (M2m) method M2m$_f$, M2m$_u$. The algorithms are tested on two\npublic datasets, OpenSARShip (6 classes) and FuSARShip (9 classes), using three\nstate-of-the-art models as feature extractors: ViT, VGG16, and ResNet50.\nAdditionally, we also analyzed the impact of oversampling methods on different\nclass sizes. The results demonstrated the effectiveness of our novel methods\nover the original M2m and baselines, with an average F1-score increase of 8.82%\nfor FuSARShip and 4.44% for OpenSARShip.", "AI": {"tldr": "本文介绍两种新的过采样算法，旨在解决SAR图像中船隻分类的类别不平衡问题，实验表明新方法相比原始方法和基线方法具有显著的性能提升。", "motivation": "由于SAR图像中船隻分类是长尾分布数据集中的一个挑战，特别是对于少数类别的船只，传统方法不易解决这种类别不平衡问题。因此，本文旨在探索特征空间中的过采样方法，以改善少数类别的船只分类效果。", "method": "本文提出了两种受Major-to-minor方法启发的新算法M2m$_f$和M2m$_u$，用于处理SAR图像中的船只分类问题，特别是在类别分布不均的情况下。", "result": "实验结果显示，与原始的M2m方法和基线方法相比，新方法在FuSARShip数据集上的平均F1分数提高8.82%，在OpenSARShip数据集上提高4.44%。", "conclusion": "实验结果表明，特征空间中的过采样方法能够有效地提升SAR船只分类任务中少数类别的识别率。"}}
{"id": "2508.06429", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06429", "abs": "https://arxiv.org/abs/2508.06429", "authors": ["Guido Manni", "Clemente Lauretti", "Loredana Zollo", "Paolo Soda"], "title": "SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation", "comment": null, "summary": "Deep learning has revolutionized medical imaging, but its effectiveness is\nseverely limited by insufficient labeled training data. This paper introduces a\nnovel GAN-based semi-supervised learning framework specifically designed for\nlow labeled-data regimes, evaluated across settings with 5 to 50 labeled\nsamples per class. Our approach integrates three specialized neural networks --\na generator for class-conditioned image translation, a discriminator for\nauthenticity assessment and classification, and a dedicated classifier --\nwithin a three-phase training framework. The method alternates between\nsupervised training on limited labeled data and unsupervised learning that\nleverages abundant unlabeled images through image-to-image translation rather\nthan generation from noise. We employ ensemble-based pseudo-labeling that\ncombines confidence-weighted predictions from the discriminator and classifier\nwith temporal consistency through exponential moving averaging, enabling\nreliable label estimation for unlabeled data. Comprehensive evaluation across\neleven MedMNIST datasets demonstrates that our approach achieves statistically\nsignificant improvements over six state-of-the-art GAN-based semi-supervised\nmethods, with particularly strong performance in the extreme 5-shot setting\nwhere the scarcity of labeled data is most challenging. The framework maintains\nits superiority across all evaluated settings (5, 10, 20, and 50 shots per\nclass). Our approach offers a practical solution for medical imaging\napplications where annotation costs are prohibitive, enabling robust\nclassification performance even with minimal labeled data. Code is available at\nhttps://github.com/GuidoManni/SPARSE.", "AI": {"tldr": "The paper presents a novel GAN-based semi-supervised learning framework designed for scenarios with very limited labeled data, which outperforms state-of-the-art methods in handling low data regimes, especially in the 5-shot setting across 11 MedMNIST datasets.", "motivation": "The motivation for this work is the challenge presented by the scarcity of labeled training data in medical imaging, which limits the effectiveness of deep learning approaches.", "method": "This paper introduces a GAN-based semi-supervised learning framework consisting of three neural networks: a generator for class-conditioned image translation, a discriminator for assessing image authenticity and classification, and a dedicated classifier. The framework employs a three-phase training process, combining supervised training on limited labeled data with unsupervised learning using abundant unlabeled images through image-to-image translation. An ensemble-based pseudo-labeling method that uses confidence-weighted predictions and temporal consistency is utilized for improved label estimation of unlabeled data.", "result": "The framework demonstrated statistically significant improvements compared to six state-of-the-art GAN-based semi-supervised methods, particularly in the 5-shot setting. It maintains its superior performance for a range of labeled data amounts (5, 10, 20, and 50 shots per class).", "conclusion": "The research concludes that the proposed approach is a practical solution for medical imaging applications with high annotation costs, enabling robust classification even with minimal labeled data."}}
{"id": "2508.06430", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06430", "abs": "https://arxiv.org/abs/2508.06430", "authors": ["Om Patil", "Jinesh Modi", "Suryabha Mukhopadhyay", "Meghaditya Giri", "Chhavi Malhotra"], "title": "MotionSwap", "comment": "8 pages, 7 figures, 5 tables. This is a student research submission\n  from BITS Pilani, Hyderabad Campus. Our implementation enhances SimSwap with\n  attention modules and dynamic training strategies", "summary": "Face swapping technology has gained significant attention in both academic\nresearch and commercial applications. This paper presents our implementation\nand enhancement of SimSwap, an efficient framework for high fidelity face\nswapping. We introduce several improvements to the original model, including\nthe integration of self and cross-attention mechanisms in the generator\narchitecture, dynamic loss weighting, and cosine annealing learning rate\nscheduling. These enhancements lead to significant improvements in identity\npreservation, attribute consistency, and overall visual quality.\n  Our experimental results, spanning 400,000 training iterations, demonstrate\nprogressive improvements in generator and discriminator performance. The\nenhanced model achieves better identity similarity, lower FID scores, and\nvisibly superior qualitative results compared to the baseline. Ablation studies\nconfirm the importance of each architectural and training improvement. We\nconclude by identifying key future directions, such as integrating StyleGAN3,\nimproving lip synchronization, incorporating 3D facial modeling, and\nintroducing temporal consistency for video-based applications.", "AI": {"tldr": "本文改进了SimSwap模型，加入了注意力机制和优化的训练策略，显著提升了脸部识别和整体视觉效果。", "motivation": "人脸交换技术在学术研究和商业应用中受到了广泛关注。本文旨在改进现有的SimSwap模型，以提高身份保持性、属性一致性和整体视觉效果。", "method": "本文通过引入自注意力和交叉注意力机制、动态损失加权和余弦退火学习率调度等改进措施，增强了SimSwap这一高效的高质量人脸交换框架。", "result": "实验结果表明，在400,000次训练迭代后，增强模型在身份相似性和FID评分方面取得了优于基线模型的结果，并且在视觉表现上也更为出色。", "conclusion": "文章最后指出了未来的研究方向，例如整合StyleGAN3、增强唇部同步效果、加入3D面部建模，并实现视频应用中的时间一致性。"}}
{"id": "2508.06434", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06434", "abs": "https://arxiv.org/abs/2508.06434", "authors": ["Shengzhu Yang", "Jiawei Du", "Shuai Lu", "Weihang Zhang", "Ningli Wang", "Huiqi Li"], "title": "CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment", "comment": null, "summary": "Large-scale natural image-text datasets, especially those automatically\ncollected from the web, often suffer from loose semantic alignment due to weak\nsupervision, while medical datasets tend to have high cross-modal correlation\nbut low content diversity. These properties pose a common challenge for\ncontrastive language-image pretraining (CLIP): they hinder the model's ability\nto learn robust and generalizable representations. In this work, we propose\nCLIPin, a unified non-contrastive plug-in that can be seamlessly integrated\ninto CLIP-style architectures to improve multimodal semantic alignment,\nproviding stronger supervision and enhancing alignment robustness. Furthermore,\ntwo shared pre-projectors are designed for image and text modalities\nrespectively to facilitate the integration of contrastive and non-contrastive\nlearning in a parameter-compromise manner. Extensive experiments on diverse\ndownstream tasks demonstrate the effectiveness and generality of CLIPin as a\nplug-and-play component compatible with various contrastive frameworks. Code is\navailable at https://github.com/T6Yang/CLIPin.", "AI": {"tldr": "Structure", "motivation": "Structure", "method": "Structure", "result": "{\n  \"tldr\": \"本文提出了一种名为CLIPin的统一非对比插件，旨在改善对比语言-图像预训练模型的跨模态语义对齐问题。通过设计共享的预投影器，CLIPin可以灵活地结合对比学习和非对比学习，并在多种下游任务中表现出色。\",\n  \"motivation\": \"大规模自然图像-文本数据集通常存在语义对齐不准确的问题，而医学数据集虽然语义相关性高但内容多样性低，这些特性限制了CLIP模型学习出稳健且通用的表征能力。\",\n  \"method\": \"提出了一个名为CLIPin的非对比插件，可以整合到CLIP风格的架构中，增强多模态语义对齐。同时设计了两个共享的预投影器，分别用于图像和文本模态，以促进对比学习与非对比学习的整合。\",\n  \"result\": \"在多种下游任务上的实验结果展示了CLIPin作为一种插件组件的有效性和普遍适用性，兼容多种对比框架。\",\n  \"conclusion\": \"CLIPin适用于各种对比性结构，优化了语义对齐，增强了学习的稳健性和泛化能力。表明了非对比学习方法在提升对比学习基础上的重要价值。代码开源，可供进一步研究。\")", "conclusion": "Structure"}}
