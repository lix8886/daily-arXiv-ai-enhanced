<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 7]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.OH](#cs.OH) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [References Improve LLM Alignment in Non-Verifiable Domains](https://arxiv.org/abs/2602.16802)
*Kejian Shi,Yixin Liu,Peifeng Wang,Alexander R. Fabbri,Shafiq Joty,Arman Cohan*

Main category: cs.CL

> 研究展示，通过利用参考输出提升LLM评测器的性能，可以实现有效的LLM对齐和自我改进，其性能优于直接参考输出的微调和无参考的自我改进，接近与训练强微调奖励模型相当的表现。

<details>
  <summary>Details</summary>

**Motivation:** 尽管强化学习通过可验证奖励（RLVR）在推理任务中展示了强大的效果，但它不能直接应用于缺少真实验证器的不可验证领域，比如LLM对齐。本研究旨在通过引入参考引导的LLM评测器来填补这一空白，尝试将其作为软“验证器”。

**Method:** 我们设计了评估协议，利用参考输出增强LLM评测器，以优化LLM对齐。具体的评测包含两个方面：使用前沿模型的参考输出提升较弱的LLM评测器的准确性；通过高质量参考（如人工撰写）增强强LLM评测器的性能。此外，我们还展示了高质量参考在对齐微调中的作用，利用带有参考的LLM作为评判进一步自我改进。

**Result:** 实验结果显示，参考引导的方法显著提高了较弱的LLM评测器的准确性；即便是较强的LLM评测器，也可以通过高质量参考进一步提升。在带有参考的自我改进中，模型表现优于直接使用参考输出的简单微调，甚至与训练ArmoRM这样的强微调奖励模型相当。使用Llama-3-8B-Instruct模型，在AlpacaEval上表现73.1%，Arena-Hard上58.7%，平均绝对增益+20.2/+17.1点，与简单微调和无参考自我改进相比，性能分别提高了+5.3/+3.6点。使用Qwen2.5-7B模型的结果也很类似。

**Conclusion:** 这些结果表明，利用参考引导的LLM评测器有潜力在不可验证领域实现有效的LLM后期训练。

**Abstract:** While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft "verifiers". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.

</details>


### [2] [Evaluating Monolingual and Multilingual Large Language Models for Greek Question Answering: The DemosQA Benchmark](https://arxiv.org/abs/2602.16811)
*Charalampos Mastrokostas,Nikolaos Giarelis,Nikos Karacapilidis*

Main category: cs.CL

> 该研究解决了针对希腊语问题回答任务中，大语言模型存在的数据偏见问题，并提出了名为DemosQA的新数据集和一个高效的LLM评估框架，同时对11个单一和多语言LLM在6个希腊语QA数据集上进行了全面评估。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大型语言模型在处理低资源语言时存在数据偏见，尤其是在训练数据上偏向流行语言或依赖于从高资源语言向低资源语言迁移学习，导致了对社会、文化和历史背景的误解。

**Method:** Structure

**Result:** {
  "tldr": "该研究旨在解决针对希腊语问题回答任务中，大型语言模型的数据偏见问题，并提出了名为DemosQA的新数据集和一个高效的LLM评估框架，同时对11个单一语言和多语言的LLM在6个人工整理的希腊语QA数据集上进行了全面评估。",
  "motivation": "针对现有大型语言模型在处理低资源语言时存在的数据偏见问题，特别是其训练数据偏向于少数几种流行语言或依赖于从高资源语言向低资源语言的迁移学习，可能导致对社会、文化和历史方面产生误解。",
  "method": "开发了一种名为DemosQA的新型数据集，利用社交网络用户提出的问题以及同行评审的答案，以及一个适应各种QA数据集和语言的内存高效的LLM评估框架，并使用三种不同的提示策略评估。",
  "result": "对11个单一语言和多语言的LLM在6个人工整理的希腊语QA数据集上进行了全面评估。",
  "conclusion": "该研究为解决希腊语的QA问题提供了一些重要的见解和工具，包括一个全新的数据集和一个高效的评估框架，将有助于推进针对希腊语以及其他低资源语言的语言模型的开发和应用。")
}

**Conclusion:** 研究为解决希腊语QA问题提供了新见解和工具，包括一个全新的数据集和评估框架，推动了低资源语言语言模型的开发。

**Abstract:** Recent advancements in Natural Language Processing and Deep Learning have enabled the development of Large Language Models (LLMs), which have significantly advanced the state-of-the-art across a wide range of tasks, including Question Answering (QA). Despite these advancements, research on LLMs has primarily targeted high-resourced languages (e.g., English), and only recently has attention shifted toward multilingual models. However, these models demonstrate a training data bias towards a small number of popular languages or rely on transfer learning from high- to under-resourced languages; this may lead to a misrepresentation of social, cultural, and historical aspects. To address this challenge, monolingual LLMs have been developed for under-resourced languages; however, their effectiveness remains less studied when compared to multilingual counterparts on language-specific tasks. In this study, we address this research gap in Greek QA by contributing: (i) DemosQA, a novel dataset, which is constructed using social media user questions and community-reviewed answers to better capture the Greek social and cultural zeitgeist; (ii) a memory-efficient LLM evaluation framework adaptable to diverse QA datasets and languages; and (iii) an extensive evaluation of 11 monolingual and multilingual LLMs on 6 human-curated Greek QA datasets using 3 different prompting strategies. We release our code and data to facilitate reproducibility.

</details>


### [3] [One-step Language Modeling via Continuous Denoising](https://arxiv.org/abs/2602.16813)
*Chanhyuk Lee,Jaehoon Yoo,Manan Agarwal,Sheel Shah,Jerry Huang,Aditi Raghunathan,Seunghoon Hong,Nicholas M. Boffi,Jinwoo Kim*

Main category: cs.CL

> 一种新的基于流式连续去噪的语言模型（FLM）和蒸馏流图语言模型（FMLM），在生成质量和速度上优于离散扩散模型

<details>
  <summary>Details</summary>

**Motivation:** 解决离散扩散模型在几步范围内样本质量急剧下降的问题，这些模型未能实现快速生成的承诺。流式连续去噪方法可以实现在质量和速度上超越离散扩散模型的生成

**Method:** 通过重新审视离散模式下的流的基础，构建了一个基于流的语言模型（FLM），该模型执行一维独热词元编码的欧几里得去噪。模型可以通过交叉熵目标预测干净的数据，其中引入了一种简单的重新参数化时间的方法以极大地提高训练的稳定性和生成质量。通过将FLM提炼到其相关流图中，获得了一个能够在几步内完成生成的蒸馏流图语言模型（FMLM）

**Result:** 在LM1B和OWT语言数据集上，FLM的生成质量匹配了最先进的离散扩散模型。FMLM在几乎所有的几步语言模型中都表现得更好，一步生成的质量超过了其他模型的8步质量

**Conclusion:** 这项研究质疑离散扩散过程在离散模式下的生成建模中是必要的这一普遍假设，并为大的规模上的快速流式语言建模铺平了道路

**Abstract:** Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings. We show that the model can be trained by predicting the clean data via a cross entropy objective, where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation. On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities, and paves the way toward accelerated flow-based language modeling at scale. Code is available at https://github.com/david3684/flm.

</details>


### [4] [Claim Automation using Large Language Model](https://arxiv.org/abs/2602.16836)
*Zhengda Mo,Zhiyu Quan,Eli O'Donohue,Kaiwen Zhong*

Main category: cs.CL

> 本文通过特定领域微调的大规模语言模型，在保险索赔处理中生成纠正行动建议，表现优于通用LLMs，证明了其在数据敏感领域的应用潜力。

<details>
  <summary>Details</summary>

**Motivation:** 虽然大规模语言模型（LLMs）在通用语言任务上表现优异，但它们在受监管且数据敏感领域（如保险）中的部署仍相当有限。该研究旨在利用数百万人次的历史保修索赔，缓解这一问题。

**Method:** 本文提出了一种本地部署的、具备治理意识的语言模型组件，该组件能够从非结构化的索赔叙述中生成结构化的纠正行动建议。研究通过使用低秩适应（LoRA）技术对预训练的大规模语言模型进行微调，将模型应用于索赔处理管道中的初始决策模块，以此来加速索赔调整员的决策过程。

**Result:** 通过对多维度评价框架的评估（结合了自动化语义相似度度量与人工评价），研究结果表明，基于特定领域的微调显著优于商用通用和基于提示的LLMs，在评估案例中的80%左右，模型输出与真实世界的纠正行动接近一致。

**Conclusion:** 研究不仅从理论上，也从实证上提供了证据，表明领域自适应微调能够使模型输出分布与实际运营数据更紧密对齐，证明了其成为保险应用中可靠、可管理的构建模块的潜力。

**Abstract:** While Large Language Models (LLMs) have achieved strong performance on general-purpose language tasks, their deployment in regulated and data-sensitive domains, including insurance, remains limited. Leveraging millions of historical warranty claims, we propose a locally deployed governance-aware language modeling component that generates structured corrective-action recommendations from unstructured claim narratives. We fine-tune pretrained LLMs using Low-Rank Adaptation (LoRA), scoping the model to an initial decision module within the claim processing pipeline to speed up claim adjusters' decisions. We assess this module using a multi-dimensional evaluation framework that combines automated semantic similarity metrics with human evaluation, enabling a rigorous examination of both practical utility and predictive accuracy. Our results show that domain-specific fine-tuning substantially outperforms commercial general-purpose and prompt-based LLMs, with approximately 80% of the evaluated cases achieving near-identical matches to ground-truth corrective actions. Overall, this study provides both theoretical and empirical evidence to prove that domain-adaptive fine-tuning can align model output distributions more closely with real-world operational data, demonstrating its promise as a reliable and governable building block for insurance applications.

</details>


### [5] [BanglaSummEval: Reference-Free Factual Consistency Evaluation for Bangla Summarization](https://arxiv.org/abs/2602.16843)
*Ahmed Rafid,Rumman Adib,Fariya Ahmed,Ajwad Abrar,Mohammed Saidul Islam*

Main category: cs.CL

> 介绍了一种无参考、基于问答的评估框架BanglaSummEval，用于检测孟加拉语文本中的事实一致性。该框架使用一个训练过的多语言模型进行问题生成和答案抽取，能够有效评估摘要的事实准确性和内容覆盖率。

<details>
  <summary>Details</summary>

**Motivation:** 评估事实一致性在可靠文本摘要中是至关重要的，尤其是在医疗保健和新闻等高风险领域。然而，大多数现有的评估指标忽视了孟加拉语，这是一种广泛使用的但资源不足的语言，并且常常依赖参考摘要。

**Method:** 所提出的BanglaSummEval是一个无参考、基于问题回答的框架，用于评估孟加拉语摘要中的事实一致性。通过从源文档和摘要中自动生成问题和答案来评估事实准确性和内容覆盖率。使用单个多语言指令调整的语言模型来处理问题生成、问题回答、候选答案抽取和问题权重分配。为了捕获超出表面重叠的语义一致性，使用BERTScore-Recall进行答案比较。

**Result:** 在教育和医疗领域的300个人工编写的摘要上验证了BanglaSummEval，演示了与专家人类判断的强相关性（皮尔逊$r = 0.694$，斯皮尔曼$\rho = 0.763$）。

**Conclusion:** BanglaSummEval提供了一种实践和透明的解决方案，用于低资源语言环境下的事实一致性评估。

**Abstract:** Evaluating factual consistency is essential for reliable text summarization, particularly in high-stakes domains such as healthcare and news. However, most existing evaluation metrics overlook Bangla, a widely spoken yet under-resourced language, and often depend on reference summaries. We introduce BanglaSummEval, a reference-free, question-answering-based framework for evaluating factual consistency in Bangla summarization. The proposed method assesses both factual accuracy and content coverage through automatically generated questions and answers derived from the source document and the summary. A single multilingual instruction-tuned language model handles question generation, question answering, candidate answer extraction, and question importance weighting. This unified design reduces system complexity and computational cost. To capture semantic consistency beyond surface-level overlap, we use BERTScore-Recall for answer comparison. We validate BanglaSummEval on 300 human-written summaries from educational and medical domains, demonstrating strong correlation with expert human judgments (Pearson's $r = 0.694$, Spearman's $ρ= 0.763$). By providing interpretable, step-wise diagnostics alongside reliable evaluation scores, BanglaSummEval offers a practical and transparent solution for factual consistency evaluation in low-resource language settings.

</details>


### [6] [Meenz bleibt Meenz, but Large Language Models Do Not Speak Its Dialect](https://arxiv.org/abs/2602.16852)
*Minh Duc Bui,Manuel Mager,Peter Herbert Kann,Katharina von der Wense*

Main category: cs.CL

> 本文探讨了使用自然语言处理技术来帮助保存和复兴美因茨方言Meenzerisch的可能性，介绍了数字化字典的创建和使用该字典进行实验的结果，指出了大型语言模型在生成定义和单词方面的不足，并强调需要更多的资源和研究来改善这一情况。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在通过自然语言处理技术，探讨保存和复兴美因茨方言Meenzerisch的可能性，因为该方言正面临消失的危机。同时，这也是首次有NLP研究专注于美因茨方言。

**Method:** 研究人员基于现有的资源（Schramm, 1966）创建了一个包含2,351个单词及其标准德语解释的数字化字典。使用该字典测试各种大型语言模型生成定义和方言单词的能力，同时尝试了少量样本学习和从训练集提取规则的方法以提高模型的准确率。

**Result:** 研究表明，尽管经过少量样本学习和提取规则后模型的准确率有所提高，但仍然低于10%。这表明，大型语言模型在处理稀有方言时存在困难。

**Conclusion:** 本文的研究结果表明，当前的大型语言模型还不足以有效地处理美因茨方言，需要更多的资源和更集中的研究努力来改善这一状况。

**Abstract:** Meenzerisch, the dialect spoken in the German city of Mainz, is also the traditional language of the Mainz carnival, a yearly celebration well known throughout Germany. However, Meenzerisch is on the verge of dying out-a fate it shares with many other German dialects. Natural language processing (NLP) has the potential to help with the preservation and revival efforts of languages and dialects. However, so far no NLP research has looked at Meenzerisch. This work presents the first research in the field of NLP that is explicitly focused on the dialect of Mainz. We introduce a digital dictionary-an NLP-ready dataset derived from an existing resource (Schramm, 1966)-to support researchers in modeling and benchmarking the language. It contains 2,351 words in the dialect paired with their meanings described in Standard German. We then use this dataset to answer the following research questions: (1) Can state-of-the-art large language models (LLMs) generate definitions for dialect words? (2) Can LLMs generate words in Meenzerisch, given their definitions? Our experiments show that LLMs can do neither: the best model for definitions reaches only 6.27% accuracy and the best word generation model's accuracy is 1.51%. We then conduct two additional experiments in order to see if accuracy is improved by few-shot learning and by extracting rules from the training set, which are then passed to the LLM. While those approaches are able to improve the results, accuracy remains below 10%. This highlights that additional resources and an intensification of research efforts focused on German dialects are desperately needed.

</details>


### [7] [ConvApparel: A Benchmark Dataset and Validation Framework for User Simulators in Conversational Recommenders](https://arxiv.org/abs/2602.16938)
*Ofer Meshi,Krisztian Balog,Sally Goldman,Avi Caciularu,Guy Tennenholtz,Jihwan Jeong,Amir Globerson,Craig Boutilier*

Main category: cs.CL

> 本文介绍了ConvApparel数据集，提出了一种综合验证框架，实验显示数据驱动的用户模拟器在反事实验证中表现出更强的泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决基于大语言模型的用户模拟器与真实世界交互之间的'现实差距'问题，希望通过提供更接近真实的用户体验来改进对话AI系统。

**Method:** 本文介绍了ConvApparel数据集及其独特的双代理数据收集协议，该协议使用“好”和“坏”的推荐者来捕获用户经验的广泛范围，并结合了用户满意度的第一人称注释。还提出了一种综合验证框架，结合统计对齐、类人评分和反事实验证来测试泛化能力。

**Result:** 实验表明，所有模拟器之间存在显著的现实差距。然而，数据驱动的模拟器在反事实验证中表现更好，尤其是在应对未见过的行为方面更真实地适应，这表明它们体现了更健壮的用户模型。

**Conclusion:** 虽然仍存在现实差距，但数据驱动的用户模拟器在应对未见过的行为时表现出了更真实的适应能力，表明它们是当前更健壮但不完美的用户模型。

**Abstract:** The promise of LLM-based user simulators to improve conversational AI is hindered by a critical "realism gap," leading to systems that are optimized for simulated interactions, but may fail to perform well in the real world. We introduce ConvApparel, a new dataset of human-AI conversations designed to address this gap. Its unique dual-agent data collection protocol -- using both "good" and "bad" recommenders -- enables counterfactual validation by capturing a wide spectrum of user experiences, enriched with first-person annotations of user satisfaction. We propose a comprehensive validation framework that combines statistical alignment, a human-likeness score, and counterfactual validation to test for generalization. Our experiments reveal a significant realism gap across all simulators. However, the framework also shows that data-driven simulators outperform a prompted baseline, particularly in counterfactual validation where they adapt more realistically to unseen behaviors, suggesting they embody more robust, if imperfect, user models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [Three-dimensional Damage Visualization of Civil Structures via Gaussian Splatting-enabled Digital Twins](https://arxiv.org/abs/2602.16713)
*Shuo Wang,Shuo Wang,Xin Nie,Yasutaka Narazaki,Thomas Matiki,Billie F. Spencer*

Main category: cs.CV

> 本文提出了一种用于土木基础设施3D损坏可视化的GS启用数字孪生方法。

<details>
  <summary>Details</summary>

**Motivation:** 近年来，在土木基础设施检查领域，前沿的进步强调需要精确的三维（3D）损坏可视化于数字孪生系统中，超越传统的基于2D图像的损坏识别。

**Method:** 该研究提出了一种基于GS的数字孪生方法，用于有效的3D损坏可视化。方法的核心贡献包括：1) 使用GS基3D重建来可视化2D损坏分割结果并减少分割误差；2) 开发多尺度重建策略来平衡效率和损坏细节；3) 实现随着时间推移而发生的数字孪生损坏更新。

**Result:** 通过使用公开的合成数据集对地震后的检查进行测试，提出的方法展示了在土木基础设施数字孪生中实现全面的3D损坏可视化的潜在解决方案。

**Conclusion:** 该方法为土木基础设施的数字孪生系统的3D损坏可视化提供了一个有前景的解决方案。

**Abstract:** Recent advancements in civil infrastructure inspections underscore the need for precise three-dimensional (3D) damage visualization on digital twins, transcending traditional 2D image-based damage identifications. Compared to conventional photogrammetric 3D reconstruction techniques, modern approaches such as Neural Radiance Field (NeRF) and Gaussian Splatting (GS) excel in scene representation, rendering quality, and handling featureless regions. Among them, GS stands out for its efficiency, leveraging discrete anisotropic 3D Gaussians to represent radiance fields, unlike NeRF's continuous implicit model. This study introduces a GS-enabled digital twin method tailored for effective 3D damage visualization. The method's key contributions include: 1) utilizing GS-based 3D reconstruction to visualize 2D damage segmentation results while reducing segmentation errors; 2) developing a multi-scale reconstruction strategy to balance efficiency and damage detail; 3) enabling digital twin updates as damage evolves over time. Demonstrated on an open-source synthetic dataset for post-earthquake inspections, the proposed approach offers a promising solution for comprehensive 3D damage visualization in civil infrastructure digital twins.

</details>


### [9] [Analytic Score Optimization for Multi Dimension Video Quality Assessment](https://arxiv.org/abs/2602.16856)
*Boda Lin,Yongjie Zhu,Wenyu Qin,Meng Wang,Pengfei Wan*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Video Quality Assessment (VQA) is evolving beyond single-number mean opinion score toward richer, multi-faceted evaluations of video content. In this paper, we present a large-scale multi-dimensional VQA dataset UltraVQA that encompasses diverse User-Generated Content~(UGC) annotated across five key quality dimensions: Motion Quality, Motion Amplitude, Aesthetic Quality, Content Quality, and Clarity Quality. Each video in our dataset is scored by over 3 human raters on these dimensions, with fine-grained sub-attribute labels, and accompanied by an explanatory rationale generated by GPT based on the collective human judgments. To better leverage these rich annotations and improve discrete quality score assessment, we introduce Analytic Score Optimization (ASO), a theoretically grounded post-training objective derived for multi-dimensional VQA. By reframing quality assessment as a regularized decision-making process, we obtain a closed-form solution that naturally captures the ordinal nature of human ratings, ensuring alignment with human ranking preferences. In experiments, our method outperforms most baselines including closed-source APIs and open-source models, while also reducing mean absolute error (MAE) in quality prediction. Our work highlights the importance of multi-dimensional, interpretable annotations and reinforcement-based alignment in advancing video quality assessment.

</details>


### [10] [DODO: Discrete OCR Diffusion Models](https://arxiv.org/abs/2602.16872)
*Sean Man,Roy Ganz,Roi Ronen,Shahar Tsiper,Shai Mazor,Niv Nayman*

Main category: cs.CV

> 介绍了DODO，一种利用块离散扩散模型加速OCR任务的新方法，兼顾了精度和速度，实现了比自回归模型快3倍的推理速度。

<details>
  <summary>Details</summary>

**Motivation:** 尽管现代视觉语言模型(VLM)在OCR上实现了较高的准确率，但其自回归解码方法在处理长文档时耗时且效率低下。DODO旨在通过创新的解码方式解决这一问题，提高OCR的速度和效率。

**Method:** 提出了DODO，一种使用块离散扩散模型的新VLM，通过将生成分解成块来减少全局扩散中的同步错误，从而更有效地进行OCR任务。

**Result:** DODO可以达到接近最先进的精度，同时还实现了比自回归基线模型快3倍的推理速度。

**Conclusion:** DODO证明了通过块离散扩散模型能够显著提高OCR任务的效率和速度，为未来研究提供了新的方向。

**Abstract:** Optical Character Recognition (OCR) is a fundamental task for digitizing information, serving as a critical bridge between visual data and textual understanding. While modern Vision-Language Models (VLM) have achieved high accuracy in this domain, they predominantly rely on autoregressive decoding, which becomes computationally expensive and slow for long documents as it requires a sequential forward pass for every generated token. We identify a key opportunity to overcome this bottleneck: unlike open-ended generation, OCR is a highly deterministic task where the visual input strictly dictates a unique output sequence, theoretically enabling efficient, parallel decoding via diffusion models. However, we show that existing masked diffusion models fail to harness this potential; those introduce structural instabilities that are benign in flexible tasks, like captioning, but catastrophic for the rigid, exact-match requirements of OCR. To bridge this gap, we introduce DODO, the first VLM to utilize block discrete diffusion and unlock its speedup potential for OCR. By decomposing generation into blocks, DODO mitigates the synchronization errors of global diffusion. Empirically, our method achieves near state-of-the-art accuracy while enabling up to 3x faster inference compared to autoregressive baselines.

</details>


### [11] [StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation](https://arxiv.org/abs/2602.16915)
*Zeyu Ren,Xiang Li,Yiran Wang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

> 该研究提出了一种名为StereoAdapter-2的技术，通过使用基于选择性状态空间模型的ConvSS2D操作符来提升水下机器人感知中的三维深度估计，相比现有方法有显著提升。

<details>
  <summary>Details</summary>

**Motivation:** 水下立体深度估计面临波长依赖的光衰减、散射和折射等问题，导致严重的域转移。现有的基于GRU的迭代细化方法在处理长距离视差和无纹理水下区域时存在性能限制，因此，需要提出一种更加高效的方法来解决这些问题。

**Method:** 论文提出了一种名为StereoAdapter-2的方法，采用基于选择性状态空间模型的ConvSS2D操作符取代传统的ConvGRU更新器。此外，还构建了一个名为UW-StereoDepth-80K的大规模合成水下立体图像数据集，支持研究的进行。

**Result:** 该论文提出了名为StereoAdapter-2的模型，通过使用基于选择性状态空间模型的ConvSS2D操作符取代传统的ConvGRU更新器，来提高水下立体深度估计的性能。新模型采用四向扫描策略，自然地与视差几何对齐，同时捕获垂直结构一致性，能够在单次更新步骤中实现高效的长距离空间传播。此外，研究人员还构建了一个大规模的合成水下立体图像数据集UW-StereoDepth-80K，以支持这一研究。实验结果表明，该框架在多个水下基准测试中实现了最先进的零样本性能，相较于其他方法，在TartanAir-UW和SQUID数据集上分别提高了17%和7.2%。

**Conclusion:** StereoAdapter-2模型在多个水下基准测试中实现了优秀的零样本性能，并通过真实世界实验进一步验证了其鲁棒性。

**Abstract:** Stereo depth estimation is fundamental to underwater robotic perception, yet suffers from severe domain shifts caused by wavelength-dependent light attenuation, scattering, and refraction. Recent approaches leverage monocular foundation models with GRU-based iterative refinement for underwater adaptation; however, the sequential gating and local convolutional kernels in GRUs necessitate multiple iterations for long-range disparity propagation, limiting performance in large-disparity and textureless underwater regions. In this paper, we propose StereoAdapter-2, which replaces the conventional ConvGRU updater with a novel ConvSS2D operator based on selective state space models. The proposed operator employs a four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation within a single update step at linear computational complexity. Furthermore, we construct UW-StereoDepth-80K, a large-scale synthetic underwater stereo dataset featuring diverse baselines, attenuation coefficients, and scattering parameters through a two-stage generative pipeline combining semantic-aware style transfer and geometry-consistent novel view synthesis. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks with 17% improvement on TartanAir-UW and 7.2% improvment on SQUID, with real-world validation on the BlueROV2 platform demonstrates the robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter-2. Website: https://aigeeksgroup.github.io/StereoAdapter-2.

</details>


<div id='cs.OH'></div>

# cs.OH [[Back]](#toc)

### [12] [A Conceptual Hybrid Framework for Post-Quantum Security: Integrating BB84 QKD, AES, and Bio-inspired Mechanisms](https://arxiv.org/abs/2602.16922)
*Md. Ismiel Hossen Abir*

Main category: cs.OH

> 论文分析了RSA在后量子攻击下的脆弱性，并提出了一种包含经典安全技术和量子技术的混合保护框架。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探讨RSA在经典和量子攻击下的脆弱性，并设计一种新的混合安全框架，以适应后量子时代的数据保护需求。

**Method:** 本研究设计了一个混合安全框架，结合了AES加密（用于经典安全性）、BB84量子密钥分发（用于安全密钥交换和窃听检测）、量子状态比较（用于轻量级认证）和生物启发的免疫系统（用于自适应威胁检测），旨在确保后量子时代的数据保护。该框架是概念性的，没有详细的实现实验验证。

**Result:** RSA易受到Shor算法的攻击，而BB84量子密钥分发可以在理想情况下实现全面的密钥协议，并且能以高度准确性检测窃听行为。通过结合经典和量子安全方法，所提框架提供了一个可扩展且自适应的后量子加密数据保护解决方案。

**Conclusion:** 本文提出了一种将经典安全技术和量子安全方法相结合的概念性框架，提供了一种面向未来的后量子加密数据保护的方法。然而，详细的实现、安全性证明以及广泛的实验验证被认为是未来的工作。

**Abstract:** Quantum computing is a significant risk to classical cryptographic, especially RSA, which depends on the difficulty of factoring large numbers. Classical factorization methods, such as Trial Division and Pollard's Rho, are inefficient for large keys, while Shor's quantum algorithm can break RSA efficiently in polynomial time. This research studies RSA's vulnerabilities under both classical and quantum attacks and designs a hybrid security framework to ensure data protection in the post-quantum era. The conceptual framework combines AES encryption for classical security, BB84 Quantum Key Distribution (QKD) for secure key exchange with eavesdropping detection, quantum state comparison for lightweight authentication, and a bio-inspired immune system for adaptive threat detection. RSA is vulnerable to Shor's algorithm, BB84 achieves full key agreement in ideal conditions, and it detects eavesdropping with high accuracy. The conceptual model includes both classical and quantum security methods, providing a scalable and adaptive solution for Post-Quantum encryption data protection. This work primarily proposes a conceptual framework. Detailed implementation, security proofs, and extensive experimental validation are considered future work.

</details>
