<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 18]
- [cs.CV](#cs.CV) [Total: 15]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning](https://arxiv.org/abs/2507.17842)
*Yimeng Zhang,Tian Wang,Jiri Gesi,Ziyi Wang,Yuxuan Lu,Jiacheng Lin,Sinong Zhan,Vianne Gao,Ruochen Jiao,Junze Liu,Kun Qian,Yuxin Tang,Ran Xue,Houyu Zhang,Qingjun Cui,Yufan Guo,Dakuo Wang*

Main category: cs.CL

> Introduces Shop-R1, a reinforcement learning framework that significantly improves the reasoning capability of LLMs when simulating human behavior in online shopping scenarios.

<details>
  <summary>Details</summary>

**Motivation:** To improve the reasoning ability of LLMs in simulating human behavior in online shopping environments, exceeding the limitations set by current approaches which rely on the reasoning capabilities of the models they utilize.

**Method:** Shop-R1, a reinforcement learning framework for enhancing reasoning ability of LLMs in simulating human behavior in online shopping. It decomposes tasks into rationale generation and action prediction, each with distinct reward signals. For rationale generation, it uses self-supervised learning with internal model signals. For action prediction, it introduces a hierarchical reward structure.

**Result:** The method demonstrates a relative improvement of over 65% compared to baseline approaches.

**Conclusion:** The reinforcement learning framework, Shop-R1, improves LLMs' ability to simulate human-like behavior through a bifurcated task of reasoning and action prediction with specialized reward structures.

**Abstract:** Large Language Models (LLMs) have recently demonstrated strong potential in
generating 'believable human-like' behavior in web environments. Prior work has
explored augmenting training data with LLM-synthesized rationales and applying
supervised fine-tuning (SFT) to enhance reasoning ability, which in turn can
improve downstream action prediction. However, the performance of such
approaches remains inherently bounded by the reasoning capabilities of the
model used to generate the rationales. In this paper, we introduce Shop-R1, a
novel reinforcement learning (RL) framework aimed at enhancing the reasoning
ability of LLMs for simulation of real human behavior in online shopping
environments Specifically, Shop-R1 decomposes the human behavior simulation
task into two stages: rationale generation and action prediction, each guided
by distinct reward signals. For rationale generation, we leverage internal
model signals (e.g., logit distributions) to guide the reasoning process in a
self-supervised manner. For action prediction, we propose a hierarchical reward
structure with difficulty-aware scaling to prevent reward hacking and enable
fine-grained reward assignment. This design evaluates both high-level action
types and the correctness of fine-grained sub-action details (attributes and
values), rewarding outputs proportionally to their difficulty. Experimental
results show that our method achieves a relative improvement of over 65%
compared to the baseline.

</details>


### [2] [Dynamic and Generalizable Process Reward Modeling](https://arxiv.org/abs/2507.17849)
*Zhangyue Yin,Qiushi Sun,Zhiyuan Zeng,Qinyuan Cheng,Xipeng Qiu,Xuanjing Huang*

Main category: cs.CL

> 本文提出了动态通用过程奖励模型（DG-PRM），通过奖励树捕捉细粒度、多维度奖励标准，并采用帕累托优势估计方法处理多方面奖励信号，实验表明该模型在各种基准测试和任务中表现出色，显示出优秀的泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 现有的过程奖励模型依赖于启发式方法，这在跨领域普遍化上表现不佳；尽管有LLM作为评判者的提出，但研究主要集中在反馈结果上，忽视了文本中的有价值指导；静态、宏观评估标准难以适应复杂的过程监督。为了应对这些挑战，本文提出了DG-PRM模型来捕捉细粒度的、多维度的奖励标准。

**Method:** DG-PRM通过奖励树捕捉细粒度、多维度奖励标准，并在每一步根据需要动态选择奖励信号。此外，采用帕累托优势估计来处理多面奖励信号，识别具有区分性的正负样本对。

**Result:** {

**Conclusion:** 实验结果表明，DG-PRM在主流基准测试上取得了显著性能，显著提高了具有密集奖励的任务模型性能，并且在分布外的场景中具有很好的适应性，展示了优秀的泛化能力。

**Abstract:** Process Reward Models (PRMs) are crucial for guiding Large Language Models
(LLMs) in complex scenarios by providing dense reward signals. However,
existing PRMs primarily rely on heuristic approaches, which struggle with
cross-domain generalization. While LLM-as-judge has been proposed to provide
generalized rewards, current research has focused mainly on feedback results,
overlooking the meaningful guidance embedded within the text. Additionally,
static and coarse-grained evaluation criteria struggle to adapt to complex
process supervision. To tackle these challenges, we propose Dynamic and
Generalizable Process Reward Modeling (DG-PRM), which features a reward tree to
capture and store fine-grained, multi-dimensional reward criteria. DG-PRM
dynamically selects reward signals for step-wise reward scoring. To handle
multifaceted reward signals, we pioneeringly adopt Pareto dominance estimation
to identify discriminative positive and negative pairs. Experimental results
show that DG-PRM achieves stunning performance on prevailing benchmarks,
significantly boosting model performance across tasks with dense rewards.
Further analysis reveals that DG-PRM adapts well to out-of-distribution
scenarios, demonstrating exceptional generalizability.

</details>


### [3] [VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL](https://arxiv.org/abs/2507.17896)
*Shubham Mohole,Sainyam Galhotra*

Main category: cs.CL

> VeriMinder is an interactive system designed to help users avoid cognitive biases in data analysis questions by introducing a contextual semantic mapping framework, an operationalized analytical framework, and an LLM-powered prompt generation system. It significantly improved the quality of analysis in user testing.

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the need to help users without a background in statistical analysis to formulate bias-free analytical questions using natural language interfaces to databases.

**Method:** VeriMinder introduces a contextual semantic mapping framework, an analytical framework operationalizing the Hard-to-Vary principle, and an optimized LLM-powered prompt generation system.

**Result:** User testing showed that 82.5% of participants found VeriMinder improved the quality of their analysis, and it outperformed alternative approaches by at least 20% in metrics like concreteness, comprehensiveness, and accuracy.

**Conclusion:** VeriMinder effectively aids users in avoiding cognitive biases during data analysis, with its web application implementation available as open-source software to foster further research and adoption.

**Abstract:** Application systems using natural language interfaces to databases (NLIDBs)
have democratized data analysis. This positive development has also brought
forth an urgent challenge to help users who might use these systems without a
background in statistical analysis to formulate bias-free analytical questions.
Although significant research has focused on text-to-SQL generation accuracy,
addressing cognitive biases in analytical questions remains underexplored. We
present VeriMinder, https://veriminder.ai, an interactive system for detecting
and mitigating such analytical vulnerabilities. Our approach introduces three
key innovations: (1) a contextual semantic mapping framework for biases
relevant to specific analysis contexts (2) an analytical framework that
operationalizes the Hard-to-Vary principle and guides users in systematic data
analysis (3) an optimized LLM-powered system that generates high-quality,
task-specific prompts using a structured process involving multiple candidates,
critic feedback, and self-reflection.
  User testing confirms the merits of our approach. In direct user experience
evaluation, 82.5% participants reported positively impacting the quality of the
analysis. In comparative evaluation, VeriMinder scored significantly higher
than alternative approaches, at least 20% better when considered for metrics of
the analysis's concreteness, comprehensiveness, and accuracy. Our system,
implemented as a web application, is set to help users avoid "wrong question"
vulnerability during data analysis. VeriMinder code base with prompts,
https://reproducibility.link/veriminder, is available as an MIT-licensed
open-source software to facilitate further research and adoption within the
community.

</details>


### [4] [One Whisper to Grade Them All](https://arxiv.org/abs/2507.17918)
*Nhan Phan,Anusha Porwal,Yaroslav Getman,Ekaterina Voskoboinik,Tamás Grósz,Mikko Kurimo*

Main category: cs.CL

> 本文提出了一种基于单个Whisper-small编码器的多部分语音测试评分系统，该系统减少了推理时间，并证明了其在效率和性能上的优越性。

<details>
  <summary>Details</summary>

**Motivation:** 本文的动机是为了开发一种更高效、更快速的语音评估系统，减少计算资源的消耗，并提高在大规模语言学习系统中的实用性。

**Method:** 本文提出了一种高效的一体化多部分第二语言测试自动语音评估(ASA)的方法，利用一个较小的Whisper编码器处理所有四种口语回答，并通过一个轻量级聚合器组合所有信息来预测最终评分。这种方法不需要转录和单部分模型，减少了推理时间，使ASA在大规模计算机辅助语言学习系统中变得实用。

**Result:** 本文所提出的方法达到了0.384的均方根误差(RMSE)，优于文本基线的0.44误差，使用了最多168M参数(约Whisper-small的70%)，并证明了一种新的数据采样策略的效用，在仅使用44.8%的语料库说话者的情况下仍然能达到0.383 RMSE。

**Conclusion:** 该研究证明了提出的自动语音评估系统在减少计算资源消耗和提高大规模语言学习系统实用性方面的有效性，同时表明新的数据采样策略能够在不平衡类别中提高性能和数据效率。

**Abstract:** We present an efficient end-to-end approach for holistic Automatic Speaking
Assessment (ASA) of multi-part second-language tests, developed for the 2025
Speak & Improve Challenge. Our system's main novelty is the ability to process
all four spoken responses with a single Whisper-small encoder, combine all
information via a lightweight aggregator, and predict the final score. This
architecture removes the need for transcription and per-part models, cuts
inference time, and makes ASA practical for large-scale Computer-Assisted
Language Learning systems.
  Our system achieved a Root Mean Squared Error (RMSE) of 0.384, outperforming
the text-based baseline (0.44) while using at most 168M parameters (about 70%
of Whisper-small). Furthermore, we propose a data sampling strategy, allowing
the model to train on only 44.8% of the speakers in the corpus and still reach
0.383 RMSE, demonstrating improved performance on imbalanced classes and strong
data efficiency.

</details>


### [5] [Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text](https://arxiv.org/abs/2507.17944)
*Hulayyil Alshammari,Praveen Rao*

Main category: cs.CL

> 研究评估了六个AI检测工具在检测DeepSeek生成文本方面的有效性，特别是在对抗攻击如人性化改写时的表现。结果显示，QuillBot和Copyleaks在原文检测中表现最优，但在对抗攻击下准确率显著下降。DeepSeek在少量样本提示和链式推理下进行检测模型时显示出较高的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探讨当代AI检测工具在检测新发布的大型语言模型（LLM）——DeepSeek生成的文本能力。鉴于现有研究主要集中在ChatGPT和其它知名LLMs而忽视了DeepSeek，这项工作填补了在这一领域的空白。

**Method:** 本研究通过使用六个常用的AI检测工具（AI文本分类器、内容检测AI、Copyleaks、QuillBot、GPT-2和GPTZero）来检测DeepSeek生成的文本，分析了它们在对抗攻击（包括标准和人性化的改写）中的表现。另外，研究还将DeepSeek作为一种检测器，采用少量样本提示和链式思考推理(CoT)来分类AI和人类撰写的文章。

**Result:** QuillBot和Copyleaks在检测DeepSeek生成的原文和同义改写文本方面表现接近完美，其他工具特别是AI文本分类器和GPT-2则存在明显的一致性问题。最有效的攻击是人性化的处理，导致Copyleaks、QuillBot和GPTZero的准确性分别下降至71%、58%和52%。使用少量样本提示和CoT的DeepSeek检测器，其中五样本提示法的最佳结果只将49个样本之一误分类（AI召回率96%，人类召回率100%）。

**Conclusion:** 研究检视了现有的AI检测工具在对抗攻击下的效能，尤其是在检测DeepSeek生成文本的能力方面。结果表明，尽管一些工具如QuillBot和Copyleaks在检测DeepSeek生成的文本方面表现较好，但它们对人性化的改写攻击较为敏感，准确率显著降低。因此，为了提高检测的准确性，还需要提升现有检测工具的鲁棒性和抗攻击能力。此外，研究还提出，通过少量样本提示和链式思维推理方法，DeepSeek本身在检测任务中也展现出了很高的准确性。

**Abstract:** Large language models (LLMs) have rapidly transformed the creation of written
materials. LLMs have led to questions about writing integrity, thereby driving
the creation of artificial intelligence (AI) detection technologies.
Adversarial attacks, such as standard and humanized paraphrasing, inhibit
detectors' ability to detect machine-generated text. Previous studies have
mainly focused on ChatGPT and other well-known LLMs and have shown varying
accuracy across detectors. However, there is a clear gap in the literature
about DeepSeek, a recently published LLM. Therefore, in this work, we
investigate whether six generally accessible AI detection tools -- AI Text
Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can
consistently recognize text generated by DeepSeek. The detectors were exposed
to the aforementioned adversarial attacks. We also considered DeepSeek as a
detector by performing few-shot prompting and chain-of-thought reasoning (CoT)
for classifying AI and human-written text. We collected 49 human-authored
question-answer pairs from before the LLM era and generated matching responses
using DeepSeek-v3, producing 49 AI-generated samples. Then, we applied
adversarial techniques such as paraphrasing and humanizing to add 196 more
samples. These were used to challenge detector robustness and assess accuracy
impact. While QuillBot and Copyleaks showed near-perfect performance on
original and paraphrased DeepSeek text, others -- particularly AI Text
Classifier and GPT-2 -- showed inconsistent results. The most effective attack
was humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and
52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best
five-shot result misclassifying only one of 49 samples (AI recall 96%, human
recall 100%).

</details>


### [6] [Are LLM Belief Updates Consistent with Bayes' Theorem?](https://arxiv.org/abs/2507.17951)
*Sohaib Imran,Ihor Kendiukhov,Matthew Broerman,Aditya Thomas,Riccardo Campanella,Rob Lamb,Peter M. Atkinson*

Main category: cs.CL

> 研究发现，更大的语言模型在一连串证据下，能够更一致地更新其对命题的信念，符合贝叶斯定理。

<details>
  <summary>Details</summary>

**Motivation:** 研究更大的语言模型是否能够更一致地按照贝叶斯定理更新其对命题的“信念”。

**Method:** 我们制定了一个贝叶斯一致系数（BCC）指标，并生成一个数据集来度量BCC。我们对五个模型家族中的多个仅预训练的语言模型进行了BCC测量，比较了模型参数的数量、训练数据量以及在常见基准测试上的模型得分。

**Result:** 结果表明，更大的语言模型赋值的一致性更高，更符合贝叶斯定理。

**Conclusion:** 这些结果对我们理解和治理大语言模型具有重要意义。

**Abstract:** Do larger and more capable language models learn to update their "beliefs"
about propositions more consistently with Bayes' theorem when presented with
evidence in-context? To test this, we formulate a Bayesian Coherence
Coefficient (BCC) metric and generate a dataset with which to measure the BCC.
We measure BCC for multiple pre-trained-only language models across five model
families, comparing against the number of model parameters, the amount of
training data, and model scores on common benchmarks. Our results provide
evidence for our hypothesis that larger and more capable pre-trained language
models assign credences that are more coherent with Bayes' theorem. These
results have important implications for our understanding and governance of
LLMs.

</details>


### [7] [Natural Language Processing for Tigrinya: Current State and Future Directions](https://arxiv.org/abs/2507.17974)
*Fitsum Gaim,Jong C. Park*

Main category: cs.CL

> 论文系统性回顾了提格雷尼亚语的自然语言处理研究，并指出从规则系统到现代神经架构的发展轨迹。

<details>
  <summary>Details</summary>

**Motivation:** 提格雷尼亚语作为使用广泛的语言之一，在自然语言处理研究中却受到了忽视。这项工作的动机在于总结提格雷尼亚语的NLP研究进展，并指导未来的研究。

**Method:** 此论文通过对超过40项研究的系统性回顾，分析了从2011年到2025年关于提格雷尼亚语自然语言处理的研究状态。

**Result:** 研究表明，提格雷尼亚语自然语言处理的研究已经从基础的规则系统发展到了现代的神经网络模型，并指出资源创设对研究进展的关键作用。

**Conclusion:** 该研究强调提格雷尼亚语的形态学复杂性和资源稀缺性是主要挑战，并提出了包括形态学感知建模和跨语言迁移等有前景的研究方向。

**Abstract:** Despite being spoken by millions of people, Tigrinya remains severely
underrepresented in Natural Language Processing (NLP) research. This work
presents a comprehensive survey of NLP research for Tigrinya, analyzing over 40
studies spanning more than a decade of work from 2011 to 2025. We
systematically review the current state of computational resources, models, and
applications across ten distinct downstream tasks, including morphological
processing, machine translation, speech recognition, and question-answering.
Our analysis reveals a clear trajectory from foundational, rule-based systems
to modern neural architectures, with progress consistently unlocked by resource
creation milestones. We identify key challenges rooted in Tigrinya's
morphological complexity and resource scarcity, while highlighting promising
research directions, including morphology-aware modeling, cross-lingual
transfer, and community-centered resource development. This work serves as both
a comprehensive reference for researchers and a roadmap for advancing Tigrinya
NLP. A curated metadata of the surveyed studies and resources is made publicly
available.\footnote{Tigrinya NLP Anthology:
https://github.com/fgaim/tigrinya-nlp-anthology.

</details>


### [8] [Technical Report of TeleChat2, TeleChat2.5 and T1](https://arxiv.org/abs/2507.18013)
*Zihan Wang,Xinzhang Liu,Yitong Yao,Chao Wang,Yu Zhao,Zhihao Yang,Wenmin Deng,Kaipeng Jia,Jiaxin Peng,Yuyao Huang,Sishi Xiong,Zhuo Jiang,Kaidong Yu,Xiaohui Hu,Fubei Yao,Ruiyu Fang,Zhuoru Jiang,Ruiting Song,Qiyi Xie,Rui Xue,Xuewei He,Yanlei Xue,Zhu Yuan,Zhaoxi Zhang,Zilu Huang,Shiquan Wang,Xin Wang,Hanming Wu,Mingyuan Wang,Xufeng Zhan,Yuhan Sun,Zhaohu Xing,Yuhao Jiang,Bingkai Yang,Shuangyong Song,Yongxiang Li,Zhongjiang He,Xuelong Li*

Main category: cs.CL

> 本文介绍了TeleChat系列的新成员：TeleChat2、TeleChat2.5和T1，通过改进的训练策略，而不是改变模型架构，显著提升了这些模型的性能，尤其是在代码生成和数学推理上。

<details>
  <summary>Details</summary>

**Motivation:** 此论文的动机是为了通过优化训练策略而不是改变模型架构，从而提升语言模型的性能，特别是针对代码生成和数学推理等特定任务。

**Method:** 该系列模型的主要改进在于训练策略的优化，而不是模型结构的改变。TeleChat2通过高质量和多样化的10万亿个token进行预训练，随后进行了监督微调(Supervised Fine-Tuning, SFT)和直接偏好优化(Direct Preference Optimization, DPO)。TeleChat2.5和T1则引入了以特定领域数据为源的持续预训练，并结合强化学习(Reinforcement Learning, RL)，以提升代码生成和数学推理能力。T1模型还能支持复杂的长链式推理(Chain-of-Thought, CoT)，而TeleChat2.5则侧重于加快推理速度。

**Result:** TeleChat2、TeleChat2.5和T1模型在代码生成和数学推理任务上展示了显著的性能提升，特别是在复杂推理任务上。这些模型的性能已经超越了某些私有模型。

**Conclusion:** 这些模型展示了相较于原版TeleChat在推理能力和常规任务性能上的显著提升，特别是在数学和编程任务上的表现超越了包括OpenAI的o1-mini和GPT-4o在内的私有模型。

**Abstract:** We introduce the latest series of TeleChat models: \textbf{TeleChat2},
\textbf{TeleChat2.5}, and \textbf{T1}, offering a significant upgrade over
their predecessor, TeleChat. Despite minimal changes to the model architecture,
the new series achieves substantial performance gains through enhanced training
strategies in both pre-training and post-training stages. The series begins
with \textbf{TeleChat2}, which undergoes pretraining on 10 trillion
high-quality and diverse tokens. This is followed by Supervised Fine-Tuning
(SFT) and Direct Preference Optimization (DPO) to further enhance its
capabilities. \textbf{TeleChat2.5} and \textbf{T1} expand the pipeline by
incorporating a continual pretraining phase with domain-specific datasets,
combined with reinforcement learning (RL) to improve performance in code
generation and mathematical reasoning tasks. The \textbf{T1} variant is
designed for complex reasoning, supporting long Chain-of-Thought (CoT)
reasoning and demonstrating substantial improvements in mathematics and coding.
In contrast, \textbf{TeleChat2.5} prioritizes speed, delivering rapid
inference. Both flagship models of \textbf{T1} and \textbf{TeleChat2.5} are
dense Transformer-based architectures with 115B parameters, showcasing
significant advancements in reasoning and general task performance compared to
the original TeleChat. Notably, \textbf{T1-115B} outperform proprietary models
such as OpenAI's o1-mini and GPT-4o. We publicly release \textbf{TeleChat2},
\textbf{TeleChat2.5} and \textbf{T1}, including post-trained versions with 35B
and 115B parameters, to empower developers and researchers with
state-of-the-art language models tailored for diverse applications.

</details>


### [9] [NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database](https://arxiv.org/abs/2507.18028)
*Weizhi Fei,Hao Shi,Jing Xu,Jingchen Peng,Jiazheng Li,Jingzhao Zhang,Bo Bai,Wei Han,Zhenyuan Chen,Xueyan Niu*

Main category: cs.CL

> NeuralDB, a neural Key-Value database with a gated retrieval module, improves large-scale factual editing of LLMs without compromising their general abilities.

<details>
  <summary>Details</summary>

**Motivation:** To effectively edit large numbers of facts in LLMs without degrading their general abilities or leading to forgetting of edited facts during scale-up.

**Method:** NeuralDB, a framework explicitly representing edited facts as a neural Key-Value database with a non-linear gated retrieval module, to address the issue of preserving general abilities of LLMs during large-scale fact editing.

**Result:** NeuralDB shows superior performance in editing 10,000 facts, excelling in efficacy, generalization, specificity, fluency, and consistency. It also maintains this effectiveness when scaled to 100,000 facts.

**Conclusion:** NeuralDB successfully enhances the capability of large-scale factual editing in LLMs while preserving their general abilities, marking a significant advancement in the field.

**Abstract:** Efficiently editing knowledge stored in large language models (LLMs) enables
model updates without large-scale training. One possible solution is
Locate-and-Edit (L\&E), allowing simultaneous modifications of a massive number
of facts. However, such editing may compromise the general abilities of LLMs
and even result in forgetting edited facts when scaling up to thousands of
edits. In this paper, we model existing linear L\&E methods as querying a
Key-Value (KV) database. From this perspective, we then propose NeuralDB, an
editing framework that explicitly represents the edited facts as a neural KV
database equipped with a non-linear gated retrieval module, % In particular,
our gated module only operates when inference involves the edited facts,
effectively preserving the general abilities of LLMs. Comprehensive experiments
involving the editing of 10,000 facts were conducted on the ZsRE and
CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results
demonstrate that NeuralDB not only excels in editing efficacy, generalization,
specificity, fluency, and consistency, but also preserves overall performance
across six representative text understanding and generation tasks. Further
experiments indicate that NeuralDB maintains its effectiveness even when scaled
to 100,000 facts (\textbf{50x} more than in prior work).

</details>


### [10] [GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs](https://arxiv.org/abs/2507.18043)
*Duy Nguyen,Archiki Prasad,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

> GrAInS是一种针对语言模型和视觉语言模型的推理时间引导方法，通过gradient-based归因方式构建引导向量，实验证明其有效且效率高。

<details>
  <summary>Details</summary>

**Motivation:** 大多数现有的推理时间引导方法依赖于固定的全局干预向量，忽视了个体输入token的因果影响，未能充分利用模型logits的指导性梯度，特别是在多模态设置下，视觉和文本输入贡献不均匀。

**Method:** GrAInS通过对比、基于梯度的归因来识别对输出贡献最大（偏好和非偏好）的token，从而构建方向性引导向量，捕捉从不理想行为到理想行为的语义变化。在推理阶段，GrAInS根据token级别的归因信号调整Transformer层的隐藏激活，并进行归一化处理以保持表征尺度不变。

**Result:** 实证研究表明，GrAInS在TruthfulQA上使用Llama-3.1-8B时准确率提高了13.22%，使用LLaVA-1.6-7B时，在MMHal-Bench上的幻觉率从0.624降低到了0.514，并在SPA-VL上提升了8.11%的对齐获胜率，同时保持了模型的流畅性和通用能力。

**Conclusion:** GrAInS提供了一种轻量级的、对语言模型和视觉语言模型的推理时间引导方法，它能够在不重新训练或提供辅助监督的情况下，实现细粒度、可解释和模块化的模型行为控制。

**Abstract:** Inference-time steering methods offer a lightweight alternative to
fine-tuning large language models (LLMs) and vision-language models (VLMs) by
modifying internal activations at test time without updating model weights.
However, most existing approaches rely on fixed, global intervention vectors,
overlook the causal influence of individual input tokens, and fail to leverage
informative gradients from the model's logits, particularly in multimodal
settings where visual and textual inputs contribute unevenly. To address these
limitations, we introduce GrAInS, an inference-time steering approach that
operates across both language-only and vision-language models and tasks. GrAInS
uses contrastive, gradient-based attribution via Integrated Gradients to
identify the top-k most influential tokens, both positively and negatively
attributed based on their contribution to preferred versus dispreferred
outputs. These tokens are then used to construct directional steering vectors
that capture semantic shifts from undesirable to desirable behavior. During
inference, GrAInS adjusts hidden activations at transformer layers guided by
token-level attribution signals, and normalizes activations to preserve
representational scale. This enables fine-grained, interpretable, and modular
control over model behavior, without retraining or auxiliary supervision.
Empirically, GrAInS consistently outperforms both fine-tuning and existing
steering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using
Llama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514
with LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all
while preserving the model's fluency and general capabilities.

</details>


### [11] [Synthetic Data Generation for Phrase Break Prediction with Large Language Model](https://arxiv.org/abs/2507.18044)
*Hoyeon Lee,Sejung Son,Ye-Eun Kang,Jong-Hwan Kim*

Main category: cs.CL

> 本文探索使用大型语言模型生成合成短语断点注释，以减少人工标注需求，发现该方法在多种语言中有效，表明LLM在语音领域具有巨大潜力。

<details>
  <summary>Details</summary>

**Motivation:** 受到大型语言模型在解决NLP数据挑战方面的成功启发，本文旨在探索使用LLMs生成合成短语断点注释，以减少人工标注的需求，提高语音相关任务的质量。

**Method:** 本文提出的方法是利用大型语言模型（LLMs）生成合成的短语断点注释，以应对传统的文本转语音系统中依赖大量人工标注的问题，并通过与传统注释进行比较来评估这种方法在多种语言中的有效性。

**Result:** 研究结果表明，基于LLMs的合成数据生成有效地解决了短语断点预测中的数据挑战，并凸显了LLMs作为语音领域可行解决方案的潜力。

**Conclusion:** 结论是LLM生成的合成数据在缓解短语断点预测的数据挑战方面有效，并可能成为语音领域的一个可行的解决方案。

**Abstract:** Current approaches to phrase break prediction address crucial prosodic
aspects of text-to-speech systems but heavily rely on vast human annotations
from audio or text, incurring significant manual effort and cost. Inherent
variability in the speech domain, driven by phonetic factors, further
complicates acquiring consistent, high-quality data. Recently, large language
models (LLMs) have shown success in addressing data challenges in NLP by
generating tailored synthetic data while reducing manual annotation needs.
Motivated by this, we explore leveraging LLM to generate synthetic phrase break
annotations, addressing the challenges of both manual annotation and
speech-related tasks by comparing with traditional annotations and assessing
effectiveness across multiple languages. Our findings suggest that LLM-based
synthetic data generation effectively mitigates data challenges in phrase break
prediction and highlights the potential of LLMs as a viable solution for the
speech domain.

</details>


### [12] [Privacy-Preserving Synthetic Review Generation with Diverse Writing Styles Using LLMs](https://arxiv.org/abs/2507.18055)
*Tevin Atwal,Chan Nam Tieu,Yefeng Yuan,Zhan Shi,Yuhong Liu,Liang Cheng*

Main category: cs.CL

> 文章关注文本合成数据的多样性和隐私性评估，发现当前大语言模型生成合成数据的局限性，并提出改进方案。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于大型语言模型生成的合成数据在数据驱动应用中带来的成本效益和可扩展性的机会，以及其多样性和隐私风险尚未得到充分研究的挑战。

**Method:** 提出了一套全面的指标来定量评估由几个最先进的大语言模型生成的合成数据集的多样性和隐私性。这些指标涵盖语言表达、情感和用户视角的多样性，以及再识别风险和风格异常的隐私性。

**Result:** 实验结果显示了这些模型在生成多样且保护隐私的合成数据方面存在显著限制。

**Conclusion:** 基于评估结果，提出了一种基于提示的方法，以增强合成评论的多样性同时保护评论者的隐私。

**Abstract:** The increasing use of synthetic data generated by Large Language Models
(LLMs) presents both opportunities and challenges in data-driven applications.
While synthetic data provides a cost-effective, scalable alternative to
real-world data to facilitate model training, its diversity and privacy risks
remain underexplored. Focusing on text-based synthetic data, we propose a
comprehensive set of metrics to quantitatively assess the diversity (i.e.,
linguistic expression, sentiment, and user perspective), and privacy (i.e.,
re-identification risk and stylistic outliers) of synthetic datasets generated
by several state-of-the-art LLMs. Experiment results reveal significant
limitations in LLMs' capabilities in generating diverse and privacy-preserving
synthetic data. Guided by the evaluation results, a prompt-based approach is
proposed to enhance the diversity of synthetic reviews while preserving
reviewer privacy.

</details>


### [13] [TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios](https://arxiv.org/abs/2507.18061)
*Zehan Li,Hongjie Chen,Yuxin Zhang,Jing Zhou,Xuening Wang,Hang Lv,Mengjie Du,Yaodong Song,Jie Lian,Jian Kang,Jie Li,Yongxiang Li,Zhongjiang He,Xuelong Li*

Main category: cs.CL

> TELEVAL被设计用来评估SLMs在中文对话环境下的表现，针对现有SLMs在自然对话任务中的不足，以期推动相关技术的发展。

<details>
  <summary>Details</summary>

**Motivation:** 大多数现有的基准主要集中在评估SLMs是否能够执行与大型语言模型（LLMs）相当的复杂任务，但往往未能与用户在实际对话场景中的自然互动方式保持一致。为了解决这个问题，提出了TELEVAL。

**Method:** 我们提出了TELEVAL，一个针对中文互动环境下评估语音语言模型（SLMs）作为对话代理有效性的动态基准。TELEVAL定义了三个评估维度：显式语义、副语言和隐式语义、以及系统能力。它采用了与真实世界使用一致的对话格式，并分别评估文本和音频输出。TELEVAL特别关注模型从用户语音中提取隐式线索并适当响应的能力，无需额外指令。

**Result:** 实验表明，尽管最近有所进展，但现有的SLMs在自然对话任务上仍有很大的改进空间。

**Conclusion:** 我们希望TELEVAL能够作为一个以用户为中心的评估框架，直接反映用户体验，并有助于开发更强大的对话导向SLMs。

**Abstract:** Spoken language models (SLMs) have seen rapid progress in recent years, along
with the development of numerous benchmarks for evaluating their performance.
However, most existing benchmarks primarily focus on evaluating whether SLMs
can perform complex tasks comparable to those tackled by large language models
(LLMs), often failing to align with how users naturally interact in real-world
conversational scenarios. In this paper, we propose TELEVAL, a dynamic
benchmark specifically designed to evaluate SLMs' effectiveness as
conversational agents in realistic Chinese interactive settings. TELEVAL
defines three evaluation dimensions: Explicit Semantics, Paralinguistic and
Implicit Semantics, and System Abilities. It adopts a dialogue format
consistent with real-world usage and evaluates text and audio outputs
separately. TELEVAL particularly focuses on the model's ability to extract
implicit cues from user speech and respond appropriately without additional
instructions. Our experiments demonstrate that despite recent progress,
existing SLMs still have considerable room for improvement in natural
conversational tasks. We hope that TELEVAL can serve as a user-centered
evaluation framework that directly reflects the user experience and contributes
to the development of more capable dialogue-oriented SLMs.

</details>


### [14] [Hybrid and Unitary Fine-Tuning of Large Language Models: Methods and Benchmarking under Resource Constraints](https://arxiv.org/abs/2507.18076)
*Haomin Qi,Zihan Dai,Chengbo Huang*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Fine-tuning large language models (LLMs) remains a computational bottleneck
due to their scale and memory demands. This paper presents a comprehensive
evaluation of parameter-efficient fine-tuning (PEFT) techniques, including
LoRA, BOFT, LoRA-GA, and uRNN, and introduces a novel hybrid strategy that
dynamically integrates BOFT's orthogonal stability with LoRA-GA's
gradient-aligned rapid convergence. By computing per-layer adaptive updates
guided by gradient norms, the hybrid method achieves superior convergence
efficiency and generalization across diverse tasks. We also explore, for the
first time, the adaptation of unitary RNN (uRNN) principles to
transformer-based LLMs, enhancing gradient stability through structured unitary
constraints. Empirical evaluations on four benchmarks -- GLUE, GSM8K, MT-Bench,
and HumanEval -- using models ranging from 7B to 405B parameters demonstrate
that our hybrid method consistently outperforms individual PEFT baselines,
approaching full fine-tuning accuracy while reducing resource consumption by up
to 2.1 times in training time and 50 percent in memory usage. These findings
establish the hybrid approach as a practical and scalable fine-tuning solution
for real-world deployment of LLMs under resource constraints.

</details>


### [15] [A New Pair of GloVes](https://arxiv.org/abs/2507.18103)
*Riley Carlson,John Bauer,Christopher D. Manning*

Main category: cs.CL

> 本研究对新近开发的2024年GloVe模型进行了详细记录与评估，结果显示新模型在体现文化与语言变化及特定NER任务上性能提升，弥补了2014年模型在数据记录不详和性能限制上的缺陷。

<details>
  <summary>Details</summary>

**Motivation:** 尽管2014年版的GloVe模型已被广泛使用，鉴于语言和世界的变化，研究者认为更新的模型可能带来更好的效果。此外，2014年模型的数据版本与预处理记录不够详细，新模型进行了详细记录以弥补这一不足。

**Method:** 本论文介绍并评估了2024年的新的英语GloVe（全局向量词表示）模型。实验使用了来自Wikipedia、Gigaword和Dolma子集的数据进行训练。通过词汇比较、直接测试及命名实体识别任务评估了这些新模型。

**Result:** 评估结果显示，2024年的GloVe模型包含了更多的文化与语言相关的词汇，并且在结构任务如类比和相似性上表现相当。在一些新的、时间依赖性较强的NER数据集上，特别是非西方的新闻数据上，新模型的性能有了提升。

**Conclusion:** 2024年更新的GloVe模型不仅捕捉到了最新的语言演变趋势，还改善了对部分数据集的处理性能，特别是在与时间相关的命名实体识别任务上表现得更为出色。

**Abstract:** This report documents, describes, and evaluates new 2024 English GloVe
(Global Vectors for Word Representation) models. While the original GloVe
models built in 2014 have been widely used and found useful, languages and the
world continue to evolve and we thought that current usage could benefit from
updated models. Moreover, the 2014 models were not carefully documented as to
the exact data versions and preprocessing that were used, and we rectify this
by documenting these new models. We trained two sets of word embeddings using
Wikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary
comparison, direct testing, and NER tasks shows that the 2024 vectors
incorporate new culturally and linguistically relevant words, perform
comparably on structural tasks like analogy and similarity, and demonstrate
improved performance on recent, temporally dependent NER datasets such as
non-Western newswire data.

</details>


### [16] [GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness](https://arxiv.org/abs/2507.18119)
*Hongjie Chen,Zehan Li,Yaodong Song,Wenming Deng,Yitong Yao,Yuxin Zhang,Hang Lv,Xuechao Zhu,Jian Kang,Jie Lian,Jie Li,Chao Wang,Shuangyong Song,Yongxiang Li,Zhongjiang He*

Main category: cs.CL

> GOAT-SLM是一个创新的口语语言模型，能够理解和生成表达性强且适应性强的语音，并在多维度评估基准TELEVAL上表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大多数语音语言模型仅将语音视为语言内容的载体，忽视了人类语音中嵌入丰富的副语言和说话人特征信息，如方言、年龄、情感以及非语音发声。这项研究旨在通过引入GOAT-SLM模型，扩展语音语言模型在语义之外的应用。

**Method:** GOAT-SLM采用双模态头部架构，将语义建模与语音实现解耦，以实现鲁棒的语言理解和表达性强且适应性强的语音生成。同时提出了一种模块化的、分阶段的训练策略，以逐步对大规模语音-文本语料库中的语义、副语言和说话人特征信息进行对齐，提高模型效率和灵活性。

**Result:** 实验结果表明，GOAT-SLM在TELEVAL，一个多维评估基准上实现了语义和非语义任务的良好平衡性能，并且在处理情感、方言变化以及年龄敏感互动方面优于现有的开源模型。

**Conclusion:** 此研究强调了在语言模型中考虑超出语言内容建模的重要性和对未来开发自然、适应性强以及具有社会意识的口语系统的推进。

**Abstract:** Recent advances in end-to-end spoken language models (SLMs) have
significantly improved the ability of AI systems to engage in natural spoken
interactions. However, most existing models treat speech merely as a vehicle
for linguistic content, often overlooking the rich paralinguistic and speaker
characteristic cues embedded in human speech, such as dialect, age, emotion,
and non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel
spoken language model with paralinguistic and speaker characteristic awareness,
designed to extend spoken language modeling beyond text semantics. GOAT-SLM
adopts a dual-modality head architecture that decouples linguistic modeling
from acoustic realization, enabling robust language understanding while
supporting expressive and adaptive speech generation. To enhance model
efficiency and versatility, we propose a modular, staged training strategy that
progressively aligns linguistic, paralinguistic, and speaker characteristic
information using large-scale speech-text corpora. Experimental results on
TELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM
achieves well-balanced performance across both semantic and non-semantic tasks,
and outperforms existing open-source models in handling emotion, dialectal
variation, and age-sensitive interactions. This work highlights the importance
of modeling beyond linguistic content and advances the development of more
natural, adaptive, and socially aware spoken language systems.

</details>


### [17] [MathOPEval: A Fine-grained Evaluation Benchmark for Visual Operations of MLLMs in Mathematical Reasoning](https://arxiv.org/abs/2507.18140)
*Xiaoyuan Li,Moxin Li,Wenjie Wang,Rui Men,Yichang Zhang,Fuli Feng,Dayiheng Liu,Junyang Lin*

Main category: cs.CL

> The paper presents a framework for evaluating the effectiveness of Multi-modal Large Language Models (MLLMs) in multi-modal mathematical reasoning, especially in their ability to perform visual operations via code.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to fill the gap in evaluating MLLM's ability to perform accurate visual operations via code in multi-modal mathematical reasoning, as current evaluations focus mainly on text-only reasoning outputs.

**Method:** Content involves introducing a framework that evaluates the capabilities of Multi-modal Large Language Models (MLLMs) in multi-modal mathematical reasoning through two evaluation aspects: Multi-modal Code Generation (MCG) and Multi-modal Code Editing (MCE).

**Result:** The framework evaluates the model's ability to accurately understand and construct visualizations and perform intricate operations on them, using a dataset that covers five popular types of mathematical figures. The results highlight the current models' limitations in these tasks.

**Conclusion:** The experimental results, which involve nine mainstream MLLMs, reveal that existing models still lag significantly behind human performance in performing fine-grained visual operations.

**Abstract:** Recent progress in Multi-modal Large Language Models (MLLMs) has enabled
step-by-step multi-modal mathematical reasoning by performing visual operations
based on the textual instructions. A promising approach uses code as an
intermediate representation to precisely express and manipulate the images in
the reasoning steps. However, existing evaluations focus mainly on text-only
reasoning outputs, leaving the MLLM's ability to perform accurate visual
operations via code largely unexplored. This work takes a first step toward
addressing that gap by evaluating MLLM's code-based capabilities in multi-modal
mathematical reasoning.Specifically, our framework focuses on two key
evaluation aspects: (1) Multi-modal Code Generation (MCG) evaluates the model's
ability to accurately understand and construct visualizations from scratch. (2)
Multi-modal Code Editing (MCE) assesses the model's capacity for fine-grained
operations, which include three types: Deletion, Modification and Annotation.
To evaluate the above tasks, we incorporate a dataset that covers the five most
popular types of mathematical figures, including geometric diagrams, function
plots, and three types of statistical charts, to provide a comprehensive and
effective measurement of existing MLLMs. Our experimental evaluation involves
nine mainstream MLLMs, and the results reveal that existing models still lag
significantly behind human performance in performing fine-grained visual
operations.

</details>


### [18] [HIVMedQA: Benchmarking large language models for HIV medical decision support](https://arxiv.org/abs/2507.18143)
*Gonzalo Cardenal Antolin,Jacques Fellay,Bashkim Jaha,Roger Kouyos,Niko Beerenwinkel,Diane Duroux*

Main category: cs.CL

> 本研究通过HIVMedQA评估框架评估了大型语言模型在HIV管理中的应用，发现Gemini 2.5 Pro表现最佳，且医疗专业型模型并不总是优于通用型模型，模型大小也不能可靠预测性能，强调了定向开发的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 研究大型语言模型在HIV管理中的应用，因为它是一个复杂疾病管理的典型案例，包括多样化的治疗选项、共病症和治疗依从性挑战，亟需评估其在临床环境中的准确性和潜在风险。

**Method:** 本研究通过HIVMedQA评估框架评估了大型语言模型在HIV管理中的表现。该框架包括七个通用型和三个医疗专业型大型语言模型，使用提示工程提升性能，评估维度包括问题理解、推理、知识回忆、偏见、潜在伤害和事实准确性。

**Result:** Gemini 2.5 Pro在大多数维度上表现最佳，而医疗专业型模型并不总是优于通用型模型，模型大小也不是性能的可靠预测因素。推理和问题理解比事实回忆更具挑战性，观察到认知偏见如近期效应和现状偏见。

**Conclusion:** 研究结果表明需要针对开发和评估进行定向改进，以确保大型语言模型在临床护理中的安全和有效性。

**Abstract:** Large language models (LLMs) are emerging as valuable tools to support
clinicians in routine decision-making. HIV management is a compelling use case
due to its complexity, including diverse treatment options, comorbidities, and
adherence challenges. However, integrating LLMs into clinical practice raises
concerns about accuracy, potential harm, and clinician acceptance. Despite
their promise, AI applications in HIV care remain underexplored, and LLM
benchmarking studies are scarce. This study evaluates the current capabilities
of LLMs in HIV management, highlighting their strengths and limitations. We
introduce HIVMedQA, a benchmark designed to assess open-ended medical question
answering in HIV care. The dataset consists of curated, clinically relevant
questions developed with input from an infectious disease physician. We
evaluated seven general-purpose and three medically specialized LLMs, applying
prompt engineering to enhance performance. Our evaluation framework
incorporates both lexical similarity and an LLM-as-a-judge approach, extended
to better reflect clinical relevance. We assessed performance across key
dimensions: question comprehension, reasoning, knowledge recall, bias,
potential harm, and factual accuracy. Results show that Gemini 2.5 Pro
consistently outperformed other models across most dimensions. Notably, two of
the top three models were proprietary. Performance declined as question
complexity increased. Medically fine-tuned models did not always outperform
general-purpose ones, and larger model size was not a reliable predictor of
performance. Reasoning and comprehension were more challenging than factual
recall, and cognitive biases such as recency and status quo were observed.
These findings underscore the need for targeted development and evaluation to
ensure safe, effective LLM integration in clinical care.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [19] [Lumina-mGPT 2.0: Stand-Alone AutoRegressive Image Modeling](https://arxiv.org/abs/2507.17801)
*Yi Xin,Juncheng Yan,Qi Qin,Zhen Li,Dongyang Liu,Shicheng Li,Victor Shea-Jay Huang,Yupeng Zhou,Renrui Zhang,Le Zhuo,Tiancheng Han,Xiaoqing Sun,Siqi Luo,Mengmeng Wang,Bin Fu,Yuewen Cao,Hongsheng Li,Guangtao Zhai,Xiaohong Liu,Yu Qiao,Peng Gao*

Main category: cs.CV

> Lumina-mGPT 2.0 是一个完全从零开始训练的自回归模型，用于高质量图像生成，展现出与顶级扩散模型相匹敌甚至更高的性能。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在重新审视并激活自回归模型在高质量图像生成领域的应用，并提供一个不依赖预训练组件或混合架构的统一生成框架。

**Method:** Lumina-mGPT 2.0 是从零开始训练的独立解码器型自回归模型，支持多种图像生成任务，并在推理时引入了高效的解码策略，如推理时缩放策略和投机性 Jacobi 采样策略，以提升生成质量和速度。

**Result:** Lumina-mGPT 2.0 在标准的文本到图像基准，如 GenEval 和 DPG 上表现出了与现有最先进的扩散模型相匹配甚至超过的表现，并在 Graph200K 多任务基准测试中表现出色。

**Conclusion:** Lumina-mGPT 2.0 成为了一个强大且灵活的基础模型，适用于统一的多模态生成任务。

**Abstract:** We present Lumina-mGPT 2.0, a stand-alone, decoder-only autoregressive model
that revisits and revitalizes the autoregressive paradigm for high-quality
image generation and beyond. Unlike existing approaches that rely on pretrained
components or hybrid architectures, Lumina-mGPT 2.0 is trained entirely from
scratch, enabling unrestricted architectural design and licensing freedom. It
achieves generation quality on par with state-of-the-art diffusion models such
as DALL-E 3 and SANA, while preserving the inherent flexibility and
compositionality of autoregressive modeling. Our unified tokenization scheme
allows the model to seamlessly handle a wide spectrum of tasks-including
subject-driven generation, image editing, controllable synthesis, and dense
prediction-within a single generative framework. To further boost usability, we
incorporate efficient decoding strategies like inference-time scaling and
speculative Jacobi sampling to improve quality and speed, respectively.
Extensive evaluations on standard text-to-image benchmarks (e.g., GenEval, DPG)
demonstrate that Lumina-mGPT 2.0 not only matches but in some cases surpasses
diffusion-based models. Moreover, we confirm its multi-task capabilities on the
Graph200K benchmark, with the native Lumina-mGPT 2.0 performing exceptionally
well. These results position Lumina-mGPT 2.0 as a strong, flexible foundation
model for unified multimodal generation. We have released our training details,
code, and models at https://github.com/Alpha-VLLM/Lumina-mGPT-2.0.

</details>


### [20] [SV3.3B: A Sports Video Understanding Model for Action Recognition](https://arxiv.org/abs/2507.17844)
*Sai Varun Kodathala,Yashwanth Reddy Vutukoori,Rakesh Vunnam*

Main category: cs.CV

> 本文提出了SV3.3B，一种轻量级的33亿参数视频理解模型，改进了体育视频分析过程中对运动员动作的细致理解，该模型速度快、计算需求低，生成的体育描述详细且具有分析深度。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决自动化体育视频分析的挑战，这种方法通常受到计算密集型模型的限制，这种模型需要服务器侧处理，并且缺乏对运动员动作的细致理解。

**Method:** SV3.3B模型结合了新颖的时间运动差异采样与自监督学习，用于有效的设备部署。该方法采用基于DWT-VGG16-LDA的关键帧提取机制，从体育序列中智能识别出16个最具代表性的关键帧，然后通过预先通过掩码去噪目标训练的V-DWT-JEPA2编码器和为体育动作描述生成微调的LLM解码器。

**Result:** 在NSVA篮球数据集的一个子集上进行评估，SV3.3B在传统文本生成指标和体育特定评估标准上均表现出色，优于包括GPT-4o变体在内的更大规模的封闭源模型，同时保持了显著较低的计算要求。

**Conclusion:** 该模型在综合运动分析的关键指标上优于更大规模的模型，演示出生成技术上详细、分析性丰富的体育描述的卓越能力。

**Abstract:** This paper addresses the challenge of automated sports video analysis, which
has traditionally been limited by computationally intensive models requiring
server-side processing and lacking fine-grained understanding of athletic
movements. Current approaches struggle to capture the nuanced biomechanical
transitions essential for meaningful sports analysis, often missing critical
phases like preparation, execution, and follow-through that occur within
seconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B
parameter video understanding model that combines novel temporal motion
difference sampling with self-supervised learning for efficient on-device
deployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction
mechanism that intelligently identifies the 16 most representative frames from
sports sequences, followed by a V-DWT-JEPA2 encoder pretrained through
mask-denoising objectives and an LLM decoder fine-tuned for sports action
description generation. Evaluated on a subset of the NSVA basketball dataset,
SV3.3B achieves superior performance across both traditional text generation
metrics and sports-specific evaluation criteria, outperforming larger
closed-source models including GPT-4o variants while maintaining significantly
lower computational requirements. Our model demonstrates exceptional capability
in generating technically detailed and analytically rich sports descriptions,
achieving 29.2% improvement over GPT-4o in ground truth validation metrics,
with substantial improvements in information density, action complexity, and
measurement precision metrics essential for comprehensive athletic analysis.
Model Available at https://huggingface.co/sportsvision/SV3.3B.

</details>


### [21] [Detail++: Training-Free Detail Enhancer for Text-to-Image Diffusion Models](https://arxiv.org/abs/2507.17853)
*Lifeng Chen,Jiner Wang,Zihao Pan,Beier Zhu,Xiaofeng Yang,Chi Zhang*

Main category: cs.CV

> 提出了一种新的文本到图像生成框架Detail++，该框架通过渐进式细节注入策略来改进复杂提示的处理。实验表明，相较于现有方法，Detail++在处理涉及多个对象和复杂样式条件的场景时表现更佳。

<details>
  <summary>Details</summary>

**Motivation:** 现有的文本到图像生成模型在处理复杂提示时，特别是在涉及多个具有不同属性的对象时，仍面临挑战。为解决这一问题，提出了Detail++框架。

**Method:** 采用了一种称为Detail++的无训练框架，该框架引入了一种新的渐进式细节注入策略来改进文本到图像生成。具体来说，Detail++将复杂的提示分解为一系列简化的子提示，分阶段指导生成过程。通过利用自注意力机制来控制布局，首先确保全局布局，然后进行精确的细化。为了实现属性和对应对象之间的准确绑定，还利用了交叉注意力机制，并在测试时引入了质心对齐损失来降低绑定噪音，增强属性一致性。

**Result:** 实验结果表明，Detail++在涉及多个对象和复杂样式条件的场景中，显著优于现有方法。

**Conclusion:** Detail++通过渐进式细节注入策略改进了文本到图像生成，特别是在处理复杂样式条件和多个对象的场景中，表现优于现有方法。

**Abstract:** Recent advances in text-to-image (T2I) generation have led to impressive
visual results. However, these models still face significant challenges when
handling complex prompt, particularly those involving multiple subjects with
distinct attributes. Inspired by the human drawing process, which first
outlines the composition and then incrementally adds details, we propose
Detail++, a training-free framework that introduces a novel Progressive Detail
Injection (PDI) strategy to address this limitation. Specifically, we decompose
a complex prompt into a sequence of simplified sub-prompts, guiding the
generation process in stages. This staged generation leverages the inherent
layout-controlling capacity of self-attention to first ensure global
composition, followed by precise refinement. To achieve accurate binding
between attributes and corresponding subjects, we exploit cross-attention
mechanisms and further introduce a Centroid Alignment Loss at test time to
reduce binding noise and enhance attribute consistency. Extensive experiments
on T2I-CompBench and a newly constructed style composition benchmark
demonstrate that Detail++ significantly outperforms existing methods,
particularly in scenarios involving multiple objects and complex stylistic
conditions.

</details>


### [22] [FishDet-M: A Unified Large-Scale Benchmark for Robust Fish Detection and CLIP-Guided Model Selection in Diverse Aquatic Visual Domains](https://arxiv.org/abs/2507.17859)
*Muayad Abujabal,Lyes Saad Saoud,Irfan Hussain*

Main category: cs.CV

> FishDet-M is introduced as a comprehensive benchmark and evaluation tool for underwater fish detection, enhancing the accuracy and efficiency of algorithms in diverse aquatic settings.

<details>
  <summary>Details</summary>

**Motivation:** To overcome limitations such as fragmented datasets, inconsistent imaging conditions, and evaluation protocols in underwater fish detection for ecological monitoring, aquaculture automation, and robotic perception.

**Method:** FishDet-M, a large unified benchmark for fish detection, which includes 13 datasets across various aquatic environments. 28 object detection models were evaluated using metrics like mAP, including scale-specific AP and inference profiling.

**Result:** The study highlights varying detection performances among tested models and the trade-offs between accuracy and efficiency. It also introduces a CLIP-based model selection framework for real-time applications.

**Conclusion:** FishDet-M provides a standardized platform for evaluating fish detection models in complex aquatic scenes, with all resources publicly available for further research and development in underwater computer vision and marine intelligence systems.

**Abstract:** Accurate fish detection in underwater imagery is essential for ecological
monitoring, aquaculture automation, and robotic perception. However, practical
deployment remains limited by fragmented datasets, heterogeneous imaging
conditions, and inconsistent evaluation protocols. To address these gaps, we
present \textit{FishDet-M}, the largest unified benchmark for fish detection,
comprising 13 publicly available datasets spanning diverse aquatic environments
including marine, brackish, occluded, and aquarium scenes. All data are
harmonized using COCO-style annotations with both bounding boxes and
segmentation masks, enabling consistent and scalable cross-domain evaluation.
We systematically benchmark 28 contemporary object detection models, covering
the YOLOv8 to YOLOv12 series, R-CNN based detectors, and DETR based models.
Evaluations are conducted using standard metrics including mAP, mAP@50, and
mAP@75, along with scale-specific analyses (AP$_S$, AP$_M$, AP$_L$) and
inference profiling in terms of latency and parameter count. The results
highlight the varying detection performance across models trained on FishDet-M,
as well as the trade-off between accuracy and efficiency across models of
different architectures. To support adaptive deployment, we introduce a
CLIP-based model selection framework that leverages vision-language alignment
to dynamically identify the most semantically appropriate detector for each
input image. This zero-shot selection strategy achieves high performance
without requiring ensemble computation, offering a scalable solution for
real-time applications. FishDet-M establishes a standardized and reproducible
platform for evaluating object detection in complex aquatic scenes. All
datasets, pretrained models, and evaluation tools are publicly available to
facilitate future research in underwater computer vision and intelligent marine
systems.

</details>


### [23] [Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis](https://arxiv.org/abs/2507.17860)
*Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel*

Main category: cs.CV

> 研究利用LightningDiT模型评估公共黑色素瘤分类器的公平性，发现使用逼真合成数据评价公平性很有前景，但需解决训练数据差异带来的挑战。

<details>
  <summary>Details</summary>

**Motivation:** 面对潜在的不公平性和偏见问题，评估和改进这类系统的公平性至关重要。评估数据集需要充分代表不同的个人身份信息（PII）（性别、年龄、种族）和其他少数群体。

**Method:** 利用前沿的生成式AI（GenAI）LightningDiT模型，评估公开可用的黑色素瘤分类器的公平性。

**Result:** 结果表明，使用高度逼真的合成数据进行公平性评估是一个有前途的方向。然而，当我们使用的黑色素瘤检测模型训练数据与合成图像的数据集不一致时，验证公平性变得更加困难。

**Conclusion:** 虽然存在挑战，但我们提议该方法为使用合成数据来衡量和提高医疗影像GenAI系统的公平性提供了一条宝贵的新的途径。

**Abstract:** Recent advancements in Deep Learning and its application on the edge hold
great potential for the revolution of routine screenings for skin cancers like
Melanoma. Along with the anticipated benefits of this technology, potential
dangers arise from unforseen and inherent biases. Thus, assessing and improving
the fairness of such systems is of utmost importance. A key challenge in
fairness assessment is to ensure that the evaluation dataset is sufficiently
representative of different Personal Identifiable Information (PII) (sex, age,
and race) and other minority groups. Against the backdrop of this challenge,
this study leverages the state-of-the-art Generative AI (GenAI) LightningDiT
model to assess the fairness of publicly available melanoma classifiers. The
results suggest that fairness assessment using highly realistic synthetic data
is a promising direction. Yet, our findings indicate that verifying fairness
becomes difficult when the melanoma-detection model used for evaluation is
trained on data that differ from the dataset underpinning the synthetic images.
Nonetheless, we propose that our approach offers a valuable new avenue for
employing synthetic data to gauge and enhance fairness in medical-imaging GenAI
systems.

</details>


### [24] [DiNAT-IR: Exploring Dilated Neighborhood Attention for High-Quality Image Restoration](https://arxiv.org/abs/2507.17892)
*Hanzhou Liu,Binghan Li,Chengkai Liu,Mi Lu*

Main category: cs.CV

> 本文提出DiNAT-IR模型，通过稀疏注意力结合局部注意力模块和通道感知模块，实现了在图像修复任务中高效且高质量的处理。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于Transformer中自注意力机制虽能捕捉长距离依赖性，但由于计算成本高昂，限制了其在高分辨率图像修复任务中的应用。为了平衡效率与质量，本文旨在改进图像修复任务中的注意力机制。

**Method:** 本文提出了一种名为DiNAT-IR的基于Transformer的架构，用于图像修复。该方法结合了稀疏注意力机制和局部注意力模块，通过通道感知模块集成全局上下文信息以改进像素级别的精度。

**Result:** 实验结果显示，DiNAT-IR在多个基准测试中达到了有竞争力的结果，为多种低级计算机视觉问题提供了高质量的解决方案。

**Conclusion:** 通过对通道感知模块的引入，DiNAT-IR提升了图像修复任务中的全局上下文理解和局部细腻性的结合，从而达到了高分辨率图像修复的高效和高质量。

**Abstract:** Transformers, with their self-attention mechanisms for modeling long-range
dependencies, have become a dominant paradigm in image restoration tasks.
However, the high computational cost of self-attention limits scalability to
high-resolution images, making efficiency-quality trade-offs a key research
focus. To address this, Restormer employs channel-wise self-attention, which
computes attention across channels instead of spatial dimensions. While
effective, this approach may overlook localized artifacts that are crucial for
high-quality image restoration. To bridge this gap, we explore Dilated
Neighborhood Attention (DiNA) as a promising alternative, inspired by its
success in high-level vision tasks. DiNA balances global context and local
precision by integrating sliding-window attention with mixed dilation factors,
effectively expanding the receptive field without excessive overhead. However,
our preliminary experiments indicate that directly applying this global-local
design to the classic deblurring task hinders accurate visual restoration,
primarily due to the constrained global context understanding within local
attention. To address this, we introduce a channel-aware module that
complements local attention, effectively integrating global context without
sacrificing pixel-level precision. The proposed DiNAT-IR, a Transformer-based
architecture specifically designed for image restoration, achieves competitive
results across multiple benchmarks, offering a high-quality solution for
diverse low-level computer vision problems.

</details>


### [25] [AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2507.17957)
*Md. Al-Masrur Khan,Durgakant Pushp,Lantao Liu*

Main category: cs.CV

> 本文提出了一种自适应特征精炼（AFR）模块，用于无监督领域适应语义分割（UDA-SS），提高了复杂区域的分割准确性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的UDA-SS方法往往难以平衡细粒度的局部细节与全局上下文信息，导致在复杂区域出现分割错误。

**Method:** AFR模块通过使用来自低分辨率对数的语义先验来增强高分辨率特征，提高分割精度。AFR还集成了捕获细粒度结构并提供重要边界信息的高频成分，改善了物体的边界描绘。AFR通过不确定性驱动的注意力机制平衡局部和全局信息，减少误分类。它的轻量级设计使其能够无缝集成到基于HRDA的UDA方法中。

**Result:** AFR方法在GTA V到Cityscapes的迁移上提升了1.05%的mIoU，在Synthia到Cityscapes的迁移上提升了1.04%的mIoU。

**Conclusion:** AFR模块的集成改进了现有的UDA-SS方法，提高了分割精度，且其轻量级设计便于与其他UDA方法结合使用。

**Abstract:** In Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS), a model is
trained on labeled source domain data (e.g., synthetic images) and adapted to
an unlabeled target domain (e.g., real-world images) without access to target
annotations. Existing UDA-SS methods often struggle to balance fine-grained
local details with global contextual information, leading to segmentation
errors in complex regions. To address this, we introduce the Adaptive Feature
Refinement (AFR) module, which enhances segmentation accuracy by refining
highresolution features using semantic priors from low-resolution logits. AFR
also integrates high-frequency components, which capture fine-grained
structures and provide crucial boundary information, improving object
delineation. Additionally, AFR adaptively balances local and global information
through uncertaintydriven attention, reducing misclassifications. Its
lightweight design allows seamless integration into HRDA-based UDA methods,
leading to state-of-the-art segmentation performance. Our approach improves
existing UDA-SS methods by 1.05% mIoU on GTA V --> Cityscapes and 1.04% mIoU on
Synthia-->Cityscapes. The implementation of our framework is available at:
https://github.com/Masrur02/AFRDA

</details>


### [26] [OPEN: A Benchmark Dataset and Baseline for Older Adult Patient Engagement Recognition in Virtual Rehabilitation Learning Environments](https://arxiv.org/abs/2507.17959)
*Ali Abedi,Sadaf Safa,Tracey J. F. Colella,Shehroz S. Khan*

Main category: cs.CV

> 本文介绍了一个名为OPEN的数据集，用于改善虚拟学习环境中，特别是老年人在远程心脏康复中的参与度识别。OPEN数据集具有独特的情感特征和行为特征标签以及上下文类型指标，为AI驱动的参与度模型提供了一个可扩展的基础。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于填补针对虚拟和远程学习中老年人参与度研究和数据集的不足。传统方法忽视了参与度的上下文相关性和跨会议的纵向特性，而研究准确的虚拟学习小组环境的参与度测量则一直是具有挑战性的。

**Method:** 此论文介绍了OPEN数据集的创建，该数据集用于AI驱动的参与度识别。数据集来源于11位老年人在六周的每周虚拟学习小组会议中的表现，总计超过35小时的数据，是此类最大的数据集。数据集包括面部、手部、身体关节的地标以及从视频中提取的情感和行为特征。标签包括二进制参与状态、情感和行为标签，以及上下文类型指示器。

**Result:** 为了证明数据集的实用性，使用了多种机器学习和深度学习模型训练，达到了高达81%的参与度识别准确率。

**Conclusion:** OPEN数据集为老龄化人群量身定制的参与度建模提供了一个可扩展的基础，并有助于更广泛的参与度识别研究。

**Abstract:** Engagement in virtual learning is essential for participant satisfaction,
performance, and adherence, particularly in online education and virtual
rehabilitation, where interactive communication plays a key role. Yet,
accurately measuring engagement in virtual group settings remains a challenge.
There is increasing interest in using artificial intelligence (AI) for
large-scale, real-world, automated engagement recognition. While engagement has
been widely studied in younger academic populations, research and datasets
focused on older adults in virtual and telehealth learning settings remain
limited. Existing methods often neglect contextual relevance and the
longitudinal nature of engagement across sessions. This paper introduces OPEN
(Older adult Patient ENgagement), a novel dataset supporting AI-driven
engagement recognition. It was collected from eleven older adults participating
in weekly virtual group learning sessions over six weeks as part of cardiac
rehabilitation, producing over 35 hours of data, making it the largest dataset
of its kind. To protect privacy, raw video is withheld; instead, the released
data include facial, hand, and body joint landmarks, along with affective and
behavioral features extracted from video. Annotations include binary engagement
states, affective and behavioral labels, and context-type indicators, such as
whether the instructor addressed the group or an individual. The dataset offers
versions with 5-, 10-, 30-second, and variable-length samples. To demonstrate
utility, multiple machine learning and deep learning models were trained,
achieving engagement recognition accuracy of up to 81 percent. OPEN provides a
scalable foundation for personalized engagement modeling in aging populations
and contributes to broader engagement recognition research.

</details>


### [27] [Bearded Dragon Activity Recognition Pipeline: An AI-Based Approach to Behavioural Monitoring](https://arxiv.org/abs/2507.17987)
*Arsen Yermukan,Pedro Machado,Feliciano Domingos,Isibor Kennedy Ihianle,Jordan J. Bird,Stefano S. K. Kaburu,Samantha J. Ward*

Main category: cs.CV

> 提出了一个自动化的实时视频分析系统，用于检测鬃狮蜥的行为，通过训练YOLO模型，系统能够可靠地检测到部分行为，但也存在一些准确性上的挑战。

<details>
  <summary>Details</summary>

**Motivation:** 传统的鬃狮蜥行为监测方法耗时且容易出错，因此引入了一种自动系统来实现实时视频分析，使得研究效率和数据质量得到显著提升。

**Method:** 使用YOLO对象检测模型来识别鬃狮蜥的关键行为，包括晒太阳和狩猎。训练了五种YOLO变体（v5, v7, v8, v11, v12）在自定义数据集上，该数据集包含1200张鬃狮蜥、加热灯和蟋蟀的图像。通过提取每帧对象的坐标、应用时间插值以及基于规则的逻辑来分类特定行为。

**Result:** YOLOv8s被选为最优模型，因为它的准确性和速度达到了最佳平衡。系统在检测晒太阳行为时表现出很高的可靠性，但狩猎行为的检测准确性较低，主要原因是蟋蟀检测的准确性低（mAP@0.5 = 0.392）。

**Conclusion:** 该自动化系统为在受控环境中监测爬行类行为提供了一种可扩展的解决方案，显著提高了研究效率和数据质量，但是对于较小目标如蟋蟀的检测准确性还有待改善。

**Abstract:** Traditional monitoring of bearded dragon (Pogona Viticeps) behaviour is
time-consuming and prone to errors. This project introduces an automated system
for real-time video analysis, using You Only Look Once (YOLO) object detection
models to identify two key behaviours: basking and hunting. We trained five
YOLO variants (v5, v7, v8, v11, v12) on a custom, publicly available dataset of
1200 images, encompassing bearded dragons (600), heating lamps (500), and
crickets (100). YOLOv8s was selected as the optimal model due to its superior
balance of accuracy (mAP@0.5:0.95 = 0.855) and speed. The system processes
video footage by extracting per-frame object coordinates, applying temporal
interpolation for continuity, and using rule-based logic to classify specific
behaviours. Basking detection proved reliable. However, hunting detection was
less accurate, primarily due to weak cricket detection (mAP@0.5 = 0.392).
Future improvements will focus on enhancing cricket detection through expanded
datasets or specialised small-object detectors. This automated system offers a
scalable solution for monitoring reptile behaviour in controlled environments,
significantly improving research efficiency and data quality.

</details>


### [28] [AG-VPReID.VIR: Bridging Aerial and Ground Platforms for Video-based Visible-Infrared Person Re-ID](https://arxiv.org/abs/2507.17995)
*Huy Nguyen,Kien Nguyen,Akila Pemasiri,Akmal Jahan,Clinton Fookes,Sridha Sridharan*

Main category: cs.CV

> 论文提出了AG-VPReID.VIR数据集以及TCC-VPReID方法，旨在解决人物跨模态再识别问题，特别是在空中-地面视角下的问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有的人物再识别数据集主要关注地面视角，这导致了遮挡、覆盖范围有限和易受障碍物影响的问题。为解决这些问题，该论文引入了一种新的数据集和一种新的方法。

**Method:** 引入了AG-VPReID.VIR，这是第一个空中-地面跨模态视频人物再识别数据集，并提出了TCC-VPReID，一种新的三流架构，旨在解决跨平台和跨模态人物再识别的挑战。

**Result:** 通过风格鲁棒特征学习、基于记忆的跨视图适配和引导时间建模，TCC-VPReID框架在AG-VPReID.VIR数据集上取得了显著的性能提升。

**Conclusion:** 实验表明，AG-VPReID.VIR提出了与现有数据集不同的挑战，并且TCC-VPReID框架在多种评估协议下实现了显著的性能提升。

**Abstract:** Person re-identification (Re-ID) across visible and infrared modalities is
crucial for 24-hour surveillance systems, but existing datasets primarily focus
on ground-level perspectives. While ground-based IR systems offer nighttime
capabilities, they suffer from occlusions, limited coverage, and vulnerability
to obstructions--problems that aerial perspectives uniquely solve. To address
these limitations, we introduce AG-VPReID.VIR, the first aerial-ground
cross-modality video-based person Re-ID dataset. This dataset captures 1,837
identities across 4,861 tracklets (124,855 frames) using both UAV-mounted and
fixed CCTV cameras in RGB and infrared modalities. AG-VPReID.VIR presents
unique challenges including cross-viewpoint variations, modality discrepancies,
and temporal dynamics. Additionally, we propose TCC-VPReID, a novel
three-stream architecture designed to address the joint challenges of
cross-platform and cross-modality person Re-ID. Our approach bridges the domain
gaps between aerial-ground perspectives and RGB-IR modalities, through
style-robust feature learning, memory-based cross-view adaptation, and
intermediary-guided temporal modeling. Experiments show that AG-VPReID.VIR
presents distinctive challenges compared to existing datasets, with our
TCC-VPReID framework achieving significant performance gains across multiple
evaluation protocols. Dataset and code are available at
https://github.com/agvpreid25/AG-VPReID.VIR.

</details>


### [29] [Exploring the interplay of label bias with subgroup size and separability: A case study in mammographic density classification](https://arxiv.org/abs/2507.17996)
*Emma A. M. Stanley,Raghav Mehta,Mélanie Roschewitz,Nils D. Forkert,Ben Glocker*

Main category: cs.CV

> 本研究发现模拟子群标签偏差导致模型学习特征表示发生了显著的变化，这些变化在特征空间中依赖于受标签偏差影响的子群的相对大小和可分离性。当验证集使用有偏的标签时，主要可分离子群的真实阳性率从0.898下降到0.518。

<details>
  <summary>Details</summary>

**Motivation:** 研究规模和可分离性受标签偏见影响的子组如何影响深度学习模型的学习特征和性能。

**Method:** 通过使用EMory BrEast影像数据集（EMBED），在受标签偏见影响的可分离子组（基于成像制造商）或非可分离‘伪子组’上训练深度学习模型，对二元组织密度分类进行了研究。

**Result:** 在模拟子群标签偏见时，模型学习到的特征表示发生了显著变化，这些变化与受标签偏差影响子群的相对大小和可分离性相关。此外，使用带有干净标签或有偏标签的验证集对模型分类阈值的定义有显著影响。

**Conclusion:** 这项研究对理解标签偏差在医学影像AI的子组公平性方面的影响至关重要。

**Abstract:** Systematic mislabelling affecting specific subgroups (i.e., label bias) in
medical imaging datasets represents an understudied issue concerning the
fairness of medical AI systems. In this work, we investigated how size and
separability of subgroups affected by label bias influence the learned features
and performance of a deep learning model. Therefore, we trained deep learning
models for binary tissue density classification using the EMory BrEast imaging
Dataset (EMBED), where label bias affected separable subgroups (based on
imaging manufacturer) or non-separable "pseudo-subgroups". We found that
simulated subgroup label bias led to prominent shifts in the learned feature
representations of the models. Importantly, these shifts within the feature
space were dependent on both the relative size and the separability of the
subgroup affected by label bias. We also observed notable differences in
subgroup performance depending on whether a validation set with clean labels
was used to define the classification threshold for the model. For instance,
with label bias affecting the majority separable subgroup, the true positive
rate for that subgroup fell from 0.898, when the validation set had clean
labels, to 0.518, when the validation set had biased labels. Our work
represents a key contribution toward understanding the consequences of label
bias on subgroup fairness in medical imaging AI.

</details>


### [30] [Registration beyond Points: General Affine Subspace Alignment via Geodesic Distance on Grassmann Manifold](https://arxiv.org/abs/2507.17998)
*Jaeho Shin,Hyeonjae Gil,Junwoo Jang,Maani Ghaffari,Ayoung Kim*

Main category: cs.CV

> 本文首次明确地推导了两个格拉斯曼特征关于刚体转换（$\mathbf{R}$ 和 $\mathbf{t}$）的可优化成本函数。利用高维线性子空间的基础作为成本的显式表示，提出了一个可应用于任何仿射子空间注册问题的可优化成本函数。该方法通过直接最小化测地距离，能够在参数表示的歧义性之外找到一个全局最优解。

<details>
  <summary>Details</summary>

**Motivation:** 虽然仿射格拉斯曼流形在表达直线和平面之间的相近性方面具有理论上的精确度，但现有的方法只能衡量相近性而不能明确给出关于刚体转换的显式距离函数，这限制了它在注册问题中的应用。为了克服这个缺陷，作者们希望开发一种可优化的成本函数，以便于在注册问题中应用仿射子空间。

**Method:** <tool_call>
delete


**Result:** <tool_call>
delete
<tool_call>
delete
<tool_call>
delete
<tool_call>
delete
<tool_call>
delete
<tool_call>
delete
<tool_call>
delete


**Conclusion:** 该文章展示的方法不仅改善了现有解决方案的收敛性，还在多种计算机视觉任务中超过了它们。代码可以在https://github.com/joomeok/GrassmannRegistration上找到。

**Abstract:** Affine Grassmannian has been favored for expressing proximity between lines
and planes due to its theoretical exactness in measuring distances among
features. Despite this advantage, the existing method can only measure the
proximity without yielding the distance as an explicit function of rigid body
transformation. Thus, an optimizable distance function on the manifold has
remained underdeveloped, stifling its application in registration problems.
This paper is the first to explicitly derive an optimizable cost function
between two Grassmannian features with respect to rigid body transformation
($\mathbf{R}$ and $\mathbf{t}$). Specifically, we present a rigorous
mathematical proof demonstrating that the bases of high-dimensional linear
subspaces can serve as an explicit representation of the cost. Finally, we
propose an optimizable cost function based on the transformed bases that can be
applied to the registration problem of any affine subspace. Compared to vector
parameter-based approaches, our method is able to find a globally optimal
solution by directly minimizing the geodesic distance which is agnostic to
representation ambiguity. The resulting cost function and its extension to the
inlier-set maximizing \ac{BnB} solver have been demonstrated to improve the
convergence of existing solutions or outperform them in various computer vision
tasks. The code is available on
https://github.com/joomeok/GrassmannRegistration.

</details>


### [31] [GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures](https://arxiv.org/abs/2507.18009)
*Jake R. Patock,Nicole Catherine Lewis,Kevin McCoy,Christina Gomez,Canling Chen,Lorenzo Luzi*

Main category: cs.CV

> 

<details>
  <summary>Details</summary>

**Motivation:** 

**Method:** 

**Result:** {
  "tldr": "GRR-CoCa, an improved Contrastive Captioner (CoCa) model, incorporates several architectural enhancements previously applied in large language models (LLMs) and shows significant improvements in both pretraining and fine-tuning across vision-language tasks.", 
  "motivation": "The motivation behind this paper is to address the architectural gap between large language models (LLMs) and multimodal models like CoCa, aiming to improve multi-modal model performance by incorporating sophisticated architectural components.", 
  "method": "The researchers introduce GRR-CoCa, which integrates Gaussian error gated linear units, root mean squared normalization, and rotary positional embedding into the textual decoders and the vision transformer (ViT) encoder. They compare GRR-CoCa with a model (Baseline CoCa) that shares the same modified decoders but retains the original CoCa ViT encoder.", 
  "result": "GRR-CoCa showed substantial performance gains over Baseline CoCa across all tested tasks, particularly in pretraining (27.25% in contrastive loss, 3.71% in perplexity, and 7.15% in CoCa loss) and fine-tuning (13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss).", 
  "conclusion": "The conclusion of the paper indicates that GRR-CoCa's enhanced architecture significantly boosts performance and generalization in vision-language domains, showcasing the effectiveness of incorporating advanced architectural components from LLMs in multi-modal models."]}

**Conclusion:** 

**Abstract:** State-of-the-art (SOTA) image and text generation models are multimodal
models that have many similarities to large language models (LLMs). Despite
achieving strong performances, leading foundational multimodal model
architectures frequently lag behind the architectural sophistication of
contemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner
(CoCa) model that incorporates Gaussian error gated linear units, root mean
squared normalization, and rotary positional embedding into the textual
decoders and the vision transformer (ViT) encoder. Each architectural
modification has been shown to improve model performance in LLMs, but has yet
to be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model
with the same modified textual decoders but with CoCa's original ViT encoder.
We used standard pretraining and fine-tuning workflows to benchmark the models
on contrastive and generative tasks. Our GRR-CoCa significantly outperformed
Baseline CoCa on the pretraining dataset and three diverse fine-tuning
datasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in
perplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were
13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We
show that GRR-CoCa's modified architecture improves performance and
generalization across vision-language domains.

</details>


### [32] [Celeb-DF++: A Large-scale Challenging Video DeepFake Benchmark for Generalizable Forensics](https://arxiv.org/abs/2507.18015)
*Yuezun Li,Delong Zhu,Xinjie Cui,Siwei Lyu*

Main category: cs.CV

> Celeb-DF++是一个新的大规模视频DeepFake数据集，覆盖了三种常见伪造类型，并包含22种不同深度伪造方法生成的高精度伪造视频。本文细化了对现有检测方法的局限性调查和数据集复杂性评估。

<details>
  <summary>Details</summary>

**Motivation:** 随着AI技术的快速发展，DeepFake视频在网络上的多样性显著增加。需要一个大且多样化的数据集来发展通用的DeepFake检测方法，以应对各种未见过的伪造类型。现有的大多数数据集规模虽大，但伪造类型有限，不足以支持通用检测方法的开发。

**Method:** 本文基于早期的Celeb-DF数据集创建了一个新的大规模视频DeepFake基准数据集Celeb-DF++。该数据集采用了22种不同的DeepFake生成方法，这些方法在架构、生成流程和目标面部区域上都有所不同，从而覆盖了常见的DeepFake案例。

**Result:** 本文提出了Celeb-DF++数据集，以应对多样化的DeepFake视频检测挑战。该数据集涵盖了三种常见的伪造场景：换脸(FS)、面部重新演绎(FR)和说话面孔(TF)，并使用22种不同的DeepFake生成方法生产高精度伪造视频。通过引入评估协议来衡量24种最新检测方法的泛化能力，突出了现有检测方法的局限性和新数据集的难度。

**Conclusion:** 通过引入评价协议来评估24种最近的检测方法的泛化性，本文揭示了现有识别方法的局限性和数据集的新挑战。因此，Celeb-DF++不仅提供了一个高质量的伪造视频资源，还提供了一个新的测试平台来评估DeepFake检测算法。

**Abstract:** The rapid advancement of AI technologies has significantly increased the
diversity of DeepFake videos circulating online, posing a pressing challenge
for \textit{generalizable forensics}, \ie, detecting a wide range of unseen
DeepFake types using a single model. Addressing this challenge requires
datasets that are not only large-scale but also rich in forgery diversity.
However, most existing datasets, despite their scale, include only a limited
variety of forgery types, making them insufficient for developing generalizable
detection methods. Therefore, we build upon our earlier Celeb-DF dataset and
introduce {Celeb-DF++}, a new large-scale and challenging video DeepFake
benchmark dedicated to the generalizable forensics challenge. Celeb-DF++ covers
three commonly encountered forgery scenarios: Face-swap (FS), Face-reenactment
(FR), and Talking-face (TF). Each scenario contains a substantial number of
high-quality forged videos, generated using a total of 22 various recent
DeepFake methods. These methods differ in terms of architectures, generation
pipelines, and targeted facial regions, covering the most prevalent DeepFake
cases witnessed in the wild. We also introduce evaluation protocols for
measuring the generalizability of 24 recent detection methods, highlighting the
limitations of existing detection methods and the difficulty of our new
dataset.

</details>


### [33] [High-fidelity 3D Gaussian Inpainting: preserving multi-view consistency and photorealistic details](https://arxiv.org/abs/2507.18023)
*Jun Zhou,Dinghao Li,Nannan Li,Mingjie Wang*

Main category: cs.CV

> 研究提出了一个利用稀疏插补视图来重建完整3D场景的新3D高斯插补框架，该框架包含自动Mask细化过程和区域不确定导向优化，以改进隐藏区域的定位和多视角的一致性，实验表明其效果优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 尽管在多视角3D重建和新颖视角合成方面取得了进展，特别是通过神经辐射场（NeRF）和3D高斯点阵（3DGS），但3D场景插补仍然是一个挑战，因为它涉及3D结构的固有不规则性和保持多视角一致性的需求。

**Method:** 我们提出了一个新颖的3D高斯插补框架，通过利用稀疏的插补视图来重建完整的3D场景。该框架包括一个自动的Mask Refinement Process和基于区域的Uncertainty-guided Optimization。具体而言，我们使用高斯场景过滤和反投影等一系列操作来细化插补Mask，增强隐蔽区域的精确定位和真实边界的修复。此外，我们的Uncertainty-guided Fine-grained Optimization策略在训练过程中估计每个区域在多视角图像中的重要性，从而缓解了多视角的不一致问题，并提高了插补结果的精细细节的保真度。

**Result:** 在多样数据集上进行的综合性实验表明，我们的方法在视觉质量和视角一致性方面都优于现有的最先进的方法。

**Conclusion:** 所提出的方法为3D场景插补提供了一个强大而准确的解决方案，改进了多视角一致性和细节的保真度。

**Abstract:** Recent advancements in multi-view 3D reconstruction and novel-view synthesis,
particularly through Neural Radiance Fields (NeRF) and 3D Gaussian Splatting
(3DGS), have greatly enhanced the fidelity and efficiency of 3D content
creation. However, inpainting 3D scenes remains a challenging task due to the
inherent irregularity of 3D structures and the critical need for maintaining
multi-view consistency. In this work, we propose a novel 3D Gaussian inpainting
framework that reconstructs complete 3D scenes by leveraging sparse inpainted
views. Our framework incorporates an automatic Mask Refinement Process and
region-wise Uncertainty-guided Optimization. Specifically, we refine the
inpainting mask using a series of operations, including Gaussian scene
filtering and back-projection, enabling more accurate localization of occluded
regions and realistic boundary restoration. Furthermore, our Uncertainty-guided
Fine-grained Optimization strategy, which estimates the importance of each
region across multi-view images during training, alleviates multi-view
inconsistencies and enhances the fidelity of fine details in the inpainted
results. Comprehensive experiments conducted on diverse datasets demonstrate
that our approach outperforms existing state-of-the-art methods in both visual
quality and view consistency.

</details>
