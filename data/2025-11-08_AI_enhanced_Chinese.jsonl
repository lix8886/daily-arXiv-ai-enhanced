{"id": "2511.03765", "categories": ["cs.CV", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.03765", "abs": "https://arxiv.org/abs/2511.03765", "authors": ["Hyunseok Kwak", "Kyeongwon Lee", "Jae-Jin Lee", "Woojoo Lee"], "title": "LoRA-Edge: Tensor-Train-Assisted LoRA for Practical CNN Fine-Tuning on Edge Devices", "comment": "8 pages, 6 figures, 2 tables, DATE 2026 accepted paper", "summary": "On-device fine-tuning of CNNs is essential to withstand domain shift in edge\napplications such as Human Activity Recognition (HAR), yet full fine-tuning is\ninfeasible under strict memory, compute, and energy budgets. We present\nLoRA-Edge, a parameter-efficient fine-tuning (PEFT) method that builds on\nLow-Rank Adaptation (LoRA) with tensor-train assistance. LoRA-Edge (i) applies\nTensor-Train Singular Value Decomposition (TT-SVD) to pre-trained convolutional\nlayers, (ii) selectively updates only the output-side core with\nzero-initialization to keep the auxiliary path inactive at the start, and (iii)\nfuses the update back into dense kernels, leaving inference cost unchanged.\nThis design preserves convolutional structure and reduces the number of\ntrainable parameters by up to two orders of magnitude compared to full\nfine-tuning. Across diverse HAR datasets and CNN backbones, LoRA-Edge achieves\naccuracy within 4.7% of full fine-tuning while updating at most 1.49% of\nparameters, consistently outperforming prior parameter-efficient baselines\nunder similar budgets. On a Jetson Orin Nano, TT-SVD initialization and\nselective-core training yield 1.4-3.8x faster convergence to target F1.\nLoRA-Edge thus makes structure-aligned, parameter-efficient on-device CNN\nadaptation practical for edge platforms.", "AI": {"tldr": "LoRA-Edge enables efficient on-device fine-tuning of CNNs, significantly reducing the number of trainable parameters and accelerating convergence, making it suitable for edge platforms.", "motivation": "To address the critical need for on-device fine-tuning of CNNs to handle domain shift in edge applications like HAR, under strict resource constraints.", "method": "LoRA-Edge, a parameter-efficient fine-tuning (PEFT) method based on Low-Rank Adaptation (LoRA) and tensor-train assistance, tailored for on-device fine-tuning of CNNs in edge applications.", "result": "Achieves accuracy within 4.7% of full fine-tuning across diverse HAR datasets and CNN backbones while updating at most 1.49% of parameters, with 1.4-3.8x faster convergence compared to prior methods.", "conclusion": "LoRA-Edge proves to be a practical method for on-device CNN adaptation, delivering efficiency in memory, compute, and energy usage."}}
{"id": "2511.03819", "categories": ["cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2511.03819", "abs": "https://arxiv.org/abs/2511.03819", "authors": ["Ozan Kanbertay", "Richard Vogg", "Elif Karakoc", "Peter M. Kappeler", "Claudia Fichtel", "Alexander S. Ecker"], "title": "SILVI: Simple Interface for Labeling Video Interactions", "comment": null, "summary": "Computer vision methods are increasingly used for the automated analysis of\nlarge volumes of video data collected through camera traps, drones, or direct\nobservations of animals in the wild. While recent advances have focused\nprimarily on detecting individual actions, much less work has addressed the\ndetection and annotation of interactions -- a crucial aspect for understanding\nsocial and individualized animal behavior. Existing open-source annotation\ntools support either behavioral labeling without localization of individuals,\nor localization without the capacity to capture interactions. To bridge this\ngap, we present SILVI, an open-source labeling software that integrates both\nfunctionalities. SILVI enables researchers to annotate behaviors and\ninteractions directly within video data, generating structured outputs suitable\nfor training and validating computer vision models. By linking behavioral\necology with computer vision, SILVI facilitates the development of automated\napproaches for fine-grained behavioral analyses. Although developed primarily\nin the context of animal behavior, SILVI could be useful more broadly to\nannotate human interactions in other videos that require extracting dynamic\nscene graphs. The software, along with documentation and download instructions,\nis available at: https://gitlab.gwdg.de/kanbertay/interaction-labelling-app.", "AI": {"tldr": "本文提出了一个名为SILVI的开源标注软件，它可以同时进行行为标注和个体定位，有助于研究人员分析动物的社交和个体行为。", "motivation": "现有的开源标注工具要么只能进行行为标注而无法定位个体，要么只能进行个体定位而无法捕捉互动。本文旨在填补这一空白，支持对动物行为和社会互动的细致分析。", "method": "本文介绍了一种开源标注软件SILVI，它集成了行为标注和个体定位功能，允许研究人员直接在视频数据中注解行为和互动，生成适用于训练和验证计算机视觉模型的结构化输出。", "result": "SILVI有助于连接行为生态学和计算机视觉，促进自动方法发展，以进行细腻的行为分析，不仅可以应用于动物行为，还可广泛应用于需要提取动态场景图的人际互动视频注解中。", "conclusion": "SILVI软件及其文档和下载说明可以在提供的网址获取。"}}
{"id": "2511.03855", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03855", "abs": "https://arxiv.org/abs/2511.03855", "authors": ["Duong Mai", "Lawrence Hall"], "title": "Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets", "comment": "Abstract accepted for oral presentation at SPIE Medical Imaging 2026:\n  Computer-Aided Diagnosis", "summary": "Deep learned (DL) models for image recognition have been shown to fail to\ngeneralize to data from different devices, populations, etc. COVID-19 detection\nfrom Chest X-rays (CXRs), in particular, has been shown to fail to generalize\nto out-of-distribution (OOD) data from new clinical sources not covered in the\ntraining set. This occurs because models learn to exploit shortcuts -\nsource-specific artifacts that do not translate to new distributions - rather\nthan reasonable biomarkers to maximize performance on in-distribution (ID)\ndata. Rendering the models more robust to distribution shifts, our study\ninvestigates the use of fundamental noise injection techniques (Gaussian,\nSpeckle, Poisson, and Salt and Pepper) during training. Our empirical results\ndemonstrate that this technique can significantly reduce the performance gap\nbetween ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results\naveraged over ten random seeds across key metrics such as AUC, F1, accuracy,\nrecall and specificity. Our source code is publicly available at\nhttps://github.com/Duongmai127/Noisy-ood", "AI": {"tldr": "研究展示通过训练时加入噪声注入可以显著减少模型在训练数据分布（ID）和未见数据分布（OOD）上的性能差距，从0.10-0.20降低至0.01-0.06。", "motivation": "深度学习模型在处理来自不同设备或人群的数据时通常表现不佳，特别是在使用胸部X光片（CXR）进行COVID-19检测时，模型往往利用特定源的不通用特征来提升训练集内的表现，而不是学习合理的生物标志物，导致无法很好地泛化到新的临床数据源。", "method": "通过在训练过程中引入基本的噪声注入技术（如高斯噪声、斑点噪声、泊松噪声和椒盐噪声）来增强模型对数据分布变化的鲁棒性。", "result": "实验结果表明，在ID数据和OOD数据上的评估指标（如AUC、F1、准确率、召回率和特异性等）的平均差距可以显著缩小。", "conclusion": "该研究表明，在模型训练过程中引入噪声可以增强其对新的临床数据源的鲁棒性，减少性能差异，提高模型在不同数据源上的可靠性。"}}
{"id": "2511.03882", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.03882", "abs": "https://arxiv.org/abs/2511.03882", "authors": ["Florence Klitzner", "Blanca Inigo", "Benjamin D. Killeen", "Lalithkumar Seenivasan", "Michelle Song", "Axel Krieger", "Mathias Unberath"], "title": "Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures", "comment": null, "summary": "Imitation learning-based robot control policies are enjoying renewed interest\nin video-based robotics. However, it remains unclear whether this approach\napplies to X-ray-guided procedures, such as spine instrumentation. This is\nbecause interpretation of multi-view X-rays is complex. We examine\nopportunities and challenges for imitation policy learning in bi-plane-guided\ncannula insertion. We develop an in silico sandbox for scalable, automated\nsimulation of X-ray-guided spine procedures with a high degree of realism. We\ncurate a dataset of correct trajectories and corresponding bi-planar X-ray\nsequences that emulate the stepwise alignment of providers. We then train\nimitation learning policies for planning and open-loop control that iteratively\nalign a cannula solely based on visual information. This precisely controlled\nsetup offers insights into limitations and capabilities of this method. Our\npolicy succeeded on the first attempt in 68.5% of cases, maintaining safe\nintra-pedicular trajectories across diverse vertebral levels. The policy\ngeneralized to complex anatomy, including fractures, and remained robust to\nvaried initializations. Rollouts on real bi-planar X-rays further suggest that\nthe model can produce plausible trajectories, despite training exclusively in\nsimulation. While these preliminary results are promising, we also identify\nlimitations, especially in entry point precision. Full closed-look control will\nrequire additional considerations around how to provide sufficiently frequent\nfeedback. With more robust priors and domain knowledge, such models may provide\na foundation for future efforts toward lightweight and CT-free robotic\nintra-operative spinal navigation.", "AI": {"tldr": "通过模拟训练模仿学习策略，用于X光引导下的脊柱手术中的套管插入，初步结果显示该策略在大多数情况下可以成功规划和控制，但也存在入口点精度方面的限制。", "motivation": "探索基于模仿学习的策略在X光引导下的脊柱手术中的应用机会与挑战。", "method": "创建X光引导下脊柱手术的模拟环境，收集正确轨迹和对应的双平面X光序列数据，训练模仿学习策略来进行规划和开放环控制。", "result": "策略在68.5%的情况下一次成功完成套管对准，能够在不同椎体级别中保持安全的轨迹。策略还对复杂解剖结构和不同的初始化条件具有鲁棒性。", "conclusion": "初步结果显示该模仿学习策略在X光引导的脊柱手术中具有应用潜力，但仍存在入口点精度的限制，需进一步改进。"}}
{"id": "2511.03738", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03738", "abs": "https://arxiv.org/abs/2511.03738", "authors": ["Pranav Bhandari", "Nicolas Fay", "Sanjeevan Selvaganapathy", "Amitava Datta", "Usman Naseem", "Mehwish Nasim"], "title": "Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs", "comment": null, "summary": "Large Language Models exhibit implicit personalities in their generation, but\nreliably controlling or aligning these traits to meet specific needs remains an\nopen challenge. The need for effective mechanisms for behavioural manipulation\nof the model during generation is a critical gap in the literature that needs\nto be fulfilled. Personality-aware LLMs hold a promising direction towards this\nobjective. However, the relationship between these psychological constructs and\ntheir representations within LLMs remains underexplored and requires further\ninvestigation. Moreover, it is intriguing to understand and study the use of\nthese representations to steer the models' behaviour. We propose a novel\npipeline that extracts hidden state activations from transformer layers using\nthe Big Five Personality Traits (Openness, Conscientiousness, Extraversion,\nAgreeableness and Neuroticism), which is a comprehensive and empirically\nvalidated framework to model human personality applies low-rank subspace\ndiscovery methods, and identifies trait-specific optimal layers across\ndifferent model architectures for robust injection. The resulting\npersonality-aligned directions are then operationalised through a flexible\nsteering framework with dynamic layer selection, enabling precise control of\ntrait expression in LLM outputs. Our findings reveal that personality traits\noccupy a low-rank shared subspace, and that these latent structures can be\ntransformed into actionable mechanisms for effective steering through careful\nperturbations without impacting the fluency, variance and general capabilities,\nhelping to bridge the gap between psychological theory and practical model\nalignment.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.03888", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03888", "abs": "https://arxiv.org/abs/2511.03888", "authors": ["Abdulmumin Sa'ad", "Sulaimon Oyeniyi Adebayo", "Abdul Jabbar Siddiqui"], "title": "Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model", "comment": "8 pages", "summary": "The global waste crisis is escalating, with solid waste generation expected\nto increase by 70% by 2050. Traditional waste collection methods, particularly\nin remote or harsh environments like deserts, are labor-intensive, inefficient,\nand often hazardous. Recent advances in computer vision and deep learning have\nopened the door to automated waste detection systems, yet most research focuses\non urban environments and recyclable materials, overlooking organic and\nhazardous waste and underexplored terrains such as deserts. In this work, we\npropose an enhanced real-time object detection framework based on a pruned,\nlightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT)\nand specialized data augmentation strategies. Using the DroneTrashNet dataset,\nwe demonstrate significant improvements in precision, recall, and mean average\nprecision (mAP), while achieving low latency and compact model size suitable\nfor deployment on resource-constrained aerial drones. Benchmarking our model\nagainst state-of-the-art lightweight YOLO variants further highlights its\noptimal balance of accuracy and efficiency. Our results validate the\neffectiveness of combining data-centric and model-centric enhancements for\nrobust, real-time waste detection in desert environments.", "AI": {"tldr": "本文提出了一种优化的废物检测框架，通过轻量级YOLOv12结合自我对抗训练和数据增强策略，实现了沙漠环境下实时废物检测的高准确性，同时保持低延迟和紧凑的模型大小，适合无人机部署。", "motivation": "传统废物收集方法在偏远或恶劣环境地区（如下沙漠）中具有劳动密集、低效和危险特性。而计算机视觉和深度学习的最新进展为自动废物检测系统打开了大门。然而，大多数研究集中在城市环境和可回收材料上，忽略了有机和危险废物以及未被探索的土地，比如沙漠。本工作旨在填补这一空白。", "method": "我们提出了一种增强的实时目标检测框架，基于剪枝后的轻量级YOLOv12，并结合自我对抗训练（SAT）和专门的数据增强策略。这种方法旨在提高检测精度、召回率和平均精度均值（mAP），同时保持低延迟和紧凑的模型大小，适合资源受限的无人机使用。", "result": "实验中使用DroneTrashNet数据集，表明模型在精度、召回率和平均精度均值（mAP）方面有了显著提升。与现有的轻量级YOLO变体模型进行基准测试，也显示了模型在精度和效率方面的优越平衡。", "conclusion": "我们的研究表明，在沙漠环境中，结合数据和模型的改进可以实现稳健和实时的废物检测，验证了所提方法的有效性。"}}
{"id": "2511.03739", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03739", "abs": "https://arxiv.org/abs/2511.03739", "authors": ["Eugenius Mario Situmorang", "Adila Alfa Krisnadhi", "Ari Wibisono"], "title": "TextualVerifier: Verify TextGrad Step-by-Step", "comment": null, "summary": "TextGrad is a novel approach to text-based automatic differentiation that\nenables composite AI systems to perform optimization without explicit numerical\nequations. However, it currently lacks self-verification mechanisms that ensure\nreasoning validity in text-based decision making. This research introduces\nTextualVerifier, a verification framework that leverages chain-of-thought\nreasoning and majority voting with large language models to address this\nverification gap. TextualVerifier implements a four-stage workflow:\nchain-of-thought decomposition, variant generation, majority voting, and\nconsensus aggregation. It integrates non-invasively with TextGrad at both the\nloss function and optimization result verification stages. Experimental\nevaluation using the Gemini 1.5 Pro model is conducted in two phases: (1)\nstandalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad\non GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically\nsignificant improvements (p < 0.001). In phase one, TextualVerifier improves\nthe validity of reasoning steps by 29 percent. In phase two, integration into\nTextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4\npercent with a moderate overhead of 5.9 LLM calls on average. Further\nevaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92\npercentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively.\nTextualVerifier thus presents the first self-verification framework for\nTextGrad through LLM-based techniques without requiring numerical gradients,\nenabling more reliable reasoning and opening new directions for verification in\ntext-based optimization.", "AI": {"tldr": "本文介绍了一种名为TextualVerifier的验证框架，用于解决TextGrad在基于文本的决策过程中缺乏自我验证的问题，显著提高了系统的可靠性和推理的有效性。", "motivation": "鉴于TextGrad在执行基于文本的优化时缺乏自我验证机制的问题，提出了TextualVerifier框架，利用链式思维和大语言模型的多数投票来解决这一验证差距。", "method": "TextualVerifier采用链式思维分解、变体生成、多数投票和共识聚合四个阶段的流程，与TextGrad无侵入式集成，实现了文本基础的自动微分系统的自验证。", "result": "在PRM800K数据集上独立评估时，TextualVerifier提高了29%的推理步骤的有效性。在与TextGrad集成使用的时间上，分别在GPQA-Diamond、MMLU-ML和MMLU-CP基准上获得了8.08%、10.71%和3.92%的改善。", "conclusion": "TextualVerifier是首个基于LLM技术的TextGrad自验证框架，无需数值梯度，提升了基于文本的优化系统的可靠性和验证的新方向。"}}
{"id": "2511.03891", "categories": ["cs.CV", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.03891", "abs": "https://arxiv.org/abs/2511.03891", "authors": ["Hlali Azzeddine", "Majid Ben Yakhlef", "Soulaiman El Hazzat"], "title": "Improving Diagnostic Performance on Small and Imbalanced Datasets Using Class-Based Input Image Composition", "comment": null, "summary": "Small, imbalanced datasets and poor input image quality can lead to high\nfalse predictions rates with deep learning models. This paper introduces\nClass-Based Image Composition, an approach that allows us to reformulate\ntraining inputs through a fusion of multiple images of the same class into\ncombined visual composites, named Composite Input Images (CoImg). That enhances\nthe intra-class variance and improves the valuable information density per\ntraining sample and increases the ability of the model to distinguish between\nsubtle disease patterns. Our method was evaluated on the Optical Coherence\nTomography Dataset for Image-Based Deep Learning Methods (OCTDL) (Kulyabin et\nal., 2024), which contains 2,064 high-resolution optical coherence tomography\n(OCT) scans of the human retina, representing seven distinct diseases with a\nsignificant class imbalance. We constructed a perfectly class-balanced version\nof this dataset, named Co-OCTDL, where each scan is resented as a 3x1 layout\ncomposite image. To assess the effectiveness of this new representation, we\nconducted a comparative analysis between the original dataset and its variant\nusing a VGG16 model. A fair comparison was ensured by utilizing the identical\nmodel architecture and hyperparameters for all experiments. The proposed\napproach markedly improved diagnostic results.The enhanced Dataset achieved\nnear-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996), compared\nto a baseline model trained on raw dataset. The false prediction rate was also\nsignificantly lower, this demonstrates that the method can producehigh-quality\npredictions even for weak datasets affected by class imbalance or small sample\nsize.", "AI": {"tldr": "通过融合相同类别的多个图像来创建复合输入图像，该方法在处理小规模和类别不平衡的数据集时显著提升了诊断准确性。", "motivation": "解决小规模数据集和类别不平衡问题，降低深度学习模型的误判率。", "method": "通过Class-Based Image Composition技术，生成复合输入图像，增强类内方差和单位训练样本的信息密度。", "result": "在OCTDL数据集上，使用VGG16模型实验，新方法在准确率、F1值和AUC上分别达到99.6%、0.995和0.9996，显著低于误判率。", "conclusion": "该方法能显著提高在小规模和类别不平衡数据集上的诊断准确性。"}}
{"id": "2511.03772", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03772", "abs": "https://arxiv.org/abs/2511.03772", "authors": ["Stergios Chatzikyriakidis", "Dimitris Papadakis", "Sevasti-Ioanna Papaioannou", "Erofili Psaltaki"], "title": "GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning Evaluation", "comment": null, "summary": "We present an extended Greek Dialectal Dataset (GRDD+) 1that complements the\nexisting GRDD dataset with more data from Cretan, Cypriot, Pontic and Northern\nGreek, while we add six new varieties: Greco-Corsican, Griko (Southern Italian\nGreek), Maniot, Heptanesian, Tsakonian, and Katharevusa Greek. The result is a\ndataset with total size 6,374,939 words and 10 varieties. This is the first\ndataset with such variation and size to date. We conduct a number of\nfine-tuning experiments to see the effect of good quality dialectal data on a\nnumber of LLMs. We fine-tune three model architectures (Llama-3-8B,\nLlama-3.1-8B, Krikri-8B) and compare the results to frontier models\n(Claude-3.7-Sonnet, Gemini-2.5, ChatGPT-5).", "AI": {"tldr": "本文扩展了希腊方言数据集（GRDD+），包含10种希腊方言和超过600万个单词，通过微调实验展示了高质量方言数据对大型语言模型的影响。", "motivation": "研究动机是改进现有希腊方言数据集，提高数据集的多样性和规模，并通过微调实验查看高质量方言数据对大型语言模型性能的影响。", "method": "通过创建扩展的希腊方言数据集（GRDD+），并基于此数据集对三种大型语言模型架构进行了微调实验。", "result": "该研究创建了一个扩展的希腊方言数据集（GRDD+），它补充了现有的GRDD数据集，增加了克里特、塞浦路斯、本都和北希腊的更多数据，并添加了六个新的变体：科西嘉希腊语、南意大利希腊语（Griko）、马尼奥特语、海潘尼西亚语、察康尼安语和卡塔雷夫苏斯希腊语。该数据集共有10种变体和6,374,939个单词，是目前为止具有这种多样性和数量的首个数据集。研究者用该数据集对三种模型架构（Llama-3-8B、Llama-3.1-8B、Krikri-8B）进行了微调实验，以观察高质量的方言数据对大规模语言模型的影响，并将这些模型的表现与前沿模型（Claude-3.7-Sonnet、Gemini-2.5、ChatGPT-5）进行比较。", "conclusion": "创建了包含10种希腊语方言、共计超600万单词的GRDD+数据集，并通过实验比较了不同模型在微调后的性能。"}}
{"id": "2511.03912", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03912", "abs": "https://arxiv.org/abs/2511.03912", "authors": ["Nand Kumar Yadav", "Rodrigue Rizk", "William CW Chen", "KC Santosh"], "title": "I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging", "comment": null, "summary": "Unknown anomaly detection in medical imaging remains a fundamental challenge\ndue to the scarcity of labeled anomalies and the high cost of expert\nsupervision. We introduce an unsupervised, oracle-free framework that\nincrementally expands a trusted set of normal samples without any anomaly\nlabels. Starting from a small, verified seed of normal images, our method\nalternates between lightweight adapter updates and uncertainty-gated sample\nadmission. A frozen pretrained vision backbone is augmented with tiny\nconvolutional adapters, ensuring rapid domain adaptation with negligible\ncomputational overhead. Extracted embeddings are stored in a compact coreset\nenabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during\nincremental expansion is enforced by dual probabilistic gates, a sample is\nadmitted into the normal memory only if its distance to the existing coreset\nlies within a calibrated z-score threshold, and its SWAG-based epistemic\nuncertainty remains below a seed-calibrated bound. This mechanism prevents\ndrift and false inclusions without relying on generative reconstruction or\nreplay buffers. Empirically, our system steadily refines the notion of\nnormality as unlabeled data arrive, producing substantial gains over baselines.\nOn COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on\nPneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5,\nROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These\nresults highlight the effectiveness and efficiency of the proposed framework\nfor real-world, label-scarce medical imaging applications.", "AI": {"tldr": "本文介绍了一种无监督的方法，用来在医疗成像中检测未知异常，并且方法有效，尤其适用于数据标签稀缺的场景。", "motivation": "医疗影像中的未知异常检测由于标注异常的稀缺性和专家监督的成本高昂，仍然是一个基本的挑战。", "method": "本文提出了一种无监督且无需专家监督的框架，用于在医疗成像中检测未知异常。该框架从一个小的正常图像种子集开始，通过轻量级适配器更新和不确定性门控样本录取交替进行，逐步扩展可信的正常样本集。适配器与冻结的预训练视觉骨干网络结合使用，确保快速领域适应。通过k-NN评分来检测异常，并采用双概率门控机制确保安全的增量扩展。", "result": "该方法在三个数据集上取得了优于基线模型的结果，在COVID-CXR上的ROC-AUC从0.9489提升到0.9982，F1从0.8048提升到0.9746; 在肺炎CXR上的ROC-AUC从0.6834提升到0.8968; 在Brain MRI ND-5上的ROC-AUC从0.6041提升到0.7269，PR-AUC从0.7539提升到0.8211。", "conclusion": "此研究提出的方法无需任何异常标签即可促使正常样本集的增量扩展，有效地在现实中标签稀缺的医疗成像应用中提升了检测性能。"}}
{"id": "2511.03823", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03823", "abs": "https://arxiv.org/abs/2511.03823", "authors": ["Jan Kocoń", "Maciej Piasecki", "Arkadiusz Janz", "Teddy Ferdinan", "Łukasz Radliński", "Bartłomiej Koptyra", "Marcin Oleksy", "Stanisław Woźniak", "Paweł Walkowiak", "Konrad Wojtasik", "Julia Moska", "Tomasz Naskręt", "Bartosz Walkowiak", "Mateusz Gniewkowski", "Kamil Szyc", "Dawid Motyka", "Dawid Banach", "Jonatan Dalasiński", "Ewa Rudnicka", "Bartłomiej Alberski", "Tomasz Walkowiak", "Aleksander Szczęsny", "Maciej Markiewicz", "Tomasz Bernaś", "Hubert Mazur", "Kamil Żyta", "Mateusz Tykierko", "Grzegorz Chodak", "Tomasz Kajdanowicz", "Przemysław Kazienko", "Agnieszka Karlińska", "Karolina Seweryn", "Anna Kołos", "Maciej Chrabąszcz", "Katarzyna Lorenc", "Aleksandra Krasnodębska", "Artur Wilczek", "Katarzyna Dziewulska", "Paula Betscher", "Zofia Cieślińska", "Katarzyna Kowol", "Daria Mikoś", "Maciej Trzciński", "Dawid Krutul", "Marek Kozłowski", "Sławomir Dadas", "Rafał Poświata", "Michał Perełkiewicz", "Małgorzata Grębowiec", "Maciej Kazuła", "Marcin Białas", "Roman Roszko", "Danuta Roszko", "Jurgita Vaičenonienė", "Andrius Utka", "Paweł Levchuk", "Paweł Kowalski", "Irena Prawdzic-Jankowska", "Maciej Ogrodniczuk", "Monika Borys", "Anna Bulińska", "Wiktoria Gumienna", "Witold Kieraś", "Dorota Komosińska", "Katarzyna Krasnowska-Kieraś", "Łukasz Kobyliński", "Martyna Lewandowska", "Marek Łaziński", "Mikołaj Łątkowski", "Dawid Mastalerz", "Beata Milewicz", "Agnieszka Anna Mykowiecka", "Angelika Peljak-Łapińska", "Sandra Penno", "Zuzanna Przybysz", "Michał Rudolf", "Piotr Rybak", "Karolina Saputa", "Aleksandra Tomaszewska", "Aleksander Wawer", "Marcin Woliński", "Joanna Wołoszyn", "Alina Wróblewska", "Bartosz Żuk", "Filip Żarnecki", "Konrad Kaczyński", "Anna Cichosz", "Zuzanna Deckert", "Monika Garnys", "Izabela Grabarczyk", "Wojciech Janowski", "Sylwia Karasińska", "Aleksandra Kujawiak", "Piotr Misztela", "Maria Szymańska", "Karolina Walkusz", "Igor Siek", "Jakub Kwiatkowski", "Piotr Pęzik"], "title": "PLLuM: A Family of Polish Large Language Models", "comment": "83 pages, 19 figures", "summary": "Large Language Models (LLMs) play a central role in modern artificial\nintelligence, yet their development has been primarily focused on English,\nresulting in limited support for other languages. We present PLLuM (Polish\nLarge Language Model), the largest open-source family of foundation models\ntailored specifically for the Polish language. Developed by a consortium of\nmajor Polish research institutions, PLLuM addresses the need for high-quality,\ntransparent, and culturally relevant language models beyond the English-centric\ncommercial landscape. We describe the development process, including the\nconstruction of a new 140-billion-token Polish text corpus for pre-training, a\n77k custom instructions dataset, and a 100k preference optimization dataset. A\nkey component is a Responsible AI framework that incorporates strict data\ngovernance and a hybrid module for output correction and safety filtering. We\ndetail the models' architecture, training procedures, and alignment techniques\nfor both base and instruction-tuned variants, and demonstrate their utility in\na downstream task within public administration. By releasing these models\npublicly, PLLuM aims to foster open research and strengthen sovereign AI\ntechnologies in Poland.", "AI": {"tldr": "本文介绍了PLLuM，一个面向波兰语的大型开放源模型系列，强调其文化相关性、透明度和高质量，旨在促进开放研究并加强波兰主权AI技术。", "motivation": "大型语言模型在现代人工智能中占据核心地位，但其发展主要集中在英语上，导致了对其他语言支持的不足。PLLuM项目旨在填补这一空缺，倡导开放研究，并加强波兰主权AI技术的发展。", "method": "本文介绍了一种名为PLLuM（波兰大型语言模型）的波兰语专用大型开放源模型系列，由波兰主要研究机构联盟开发。为了满足高质量、透明和文化相关的语言模型需求，文章描述了模型的开发过程，包括构建一个新的1400亿个波兰语标记的预训练文本语料库，一个77K的自定义指令数据集，以及一个100K的偏好优化数据集。此外，该模型还包括一个负责任的AI框架，包含严格的数据治理和输出校正、安全过滤的混合模块。", "result": "该研究详细介绍了模型的架构、训练过程以及针对基础模型和指令调优变体的对齐技术，并展示了其在公共行政领域的下游任务中的实用性。", "conclusion": "通过公开发布这些模型，PLLuM项目希望激励开放研究，并增强波兰的主权AI技术。"}}
{"id": "2511.03943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03943", "abs": "https://arxiv.org/abs/2511.03943", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "Adaptive Temporal Refinement: Continuous Depth Allocation and Distance Regression for Efficient Action Localization", "comment": null, "summary": "Temporal action localization requires precise boundary detection; however,\ncurrent methods apply uniform computation despite significant variations in\ndifficulty across boundaries. We present two complementary contributions.\nFirst, Boundary Distance Regression (BDR) provides information-theoretically\noptimal localization through signed-distance regression rather than\nclassification, achieving 43\\% sharper boundary peaks. BDR retrofits to\nexisting methods with approximately 50 lines of code, yielding consistent 1.8\nto 3.1\\% mAP@0.7 improvements across diverse architectures. Second, Adaptive\nTemporal Refinement (ATR) allocates computation via continuous depth selection\n$\\tau \\in [0,1]$, enabling end-to-end differentiable optimization without\nreinforcement learning. On THUMOS14, ATR achieves 56.5\\% mAP@0.7 at 162G FLOPs,\ncompared to 53.6\\% at 198G for uniform processing, providing a 2.9\\%\nimprovement with 18\\% less compute. Gains scale with boundary heterogeneity,\nshowing 4.2\\% improvement on short actions. Training cost is mitigated via\nknowledge distillation, with lightweight students retaining 99\\% performance at\nbaseline cost. Results are validated across four benchmarks with rigorous\nstatistical testing.", "AI": {"tldr": "研究提出了BDR与ATR两种方法，提升边界检测精度，同时优化计算资源分配，减少计算量并降低训练成本，适用于多种架构。", "motivation": "论文的动机在于现有的方法在处理边界定位时没有考虑到其在不同边界上的难度差异，而应用了统一的计算。这导致边界定位不够精确，因此提出了边界距离回归（BDR）和自适应时序细化（ATR）来提高边界检测的精确性和计算效率。", "method": "该论文提出了两种互补的贡献：首先，边界距离回归（BDR）通过有符号距离回归而不是分类来提供信息理论上最优的定位，实现边界峰值锐利度提高43%。BDR可以应用于现有的方法，仅需大约50行代码，从而在多种架构中实现1.8%到3.1%的mAP@0.7改进。其次，自适应时序细化（ATR）通过连续深度选择τ∈[0,1]分配计算，使得可以直接进行端到端可微优化而无需强化学习。", "result": "在THUMOS14数据集上，ATR达到了56.5%的mAP@0.7，尽管计算量减少了18%。对于短动作来说，改进效果更显著，达到4.2%的提升。通过知识蒸馏，训练成本得到了缓解，轻量级的学生模型在基线成本下可保持99%的性能。这些结果在四个基准上进行了严格的统计测试验证。", "conclusion": "研究展示了边界距离回归（BDR）和自适应时序细化（ATR）的有效性，它们不仅可以在多种架构上提高边界检测的精确度，还可以减少计算量，同时通过知识蒸馏技术降低了训练成本。证明了这两种方法在视频动作定位任务上的实用性和高效性。"}}
{"id": "2511.03827", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03827", "abs": "https://arxiv.org/abs/2511.03827", "authors": ["Mohammad Atif Quamar", "Mohammad Areeb", "Mikhail Kuznetsov", "Muslum Ozgur Ozmen", "Z. Berkay Celik"], "title": "STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models", "comment": "Presented at the 2nd Workshop on Frontiers in Probabilistic\n  Inference: Sampling Meets Learning (NeurIPS 2025)", "summary": "Aligning large language models with human values is crucial for their safe\ndeployment; however, existing methods, such as fine-tuning, are computationally\nexpensive and suboptimal. In contrast, inference-time approaches like Best-of-N\nsampling require practically infeasible computation to achieve optimal\nalignment. We propose STARS: Segment-level Token Alignment with Rejection\nSampling, a decoding-time algorithm that steers model generation by iteratively\nsampling, scoring, and rejecting/accepting short, fixed-size token segments.\nThis allows for early correction of the generation path, significantly\nimproving computational efficiency and boosting alignment quality. Across a\nsuite of six LLMs, we show that STARS outperforms Supervised Fine-Tuning (SFT)\nby up to 14.9 percentage points and Direct Preference Optimization (DPO) by up\nto 4.3 percentage points on win-rates, while remaining highly competitive with\nstrong Best-of-N baselines. Our work establishes granular, reward-guided\nsampling as a generalizable, robust, and efficient alternative to traditional\nfine-tuning and full-sequence ranking methods for aligning LLMs.", "AI": {"tldr": "STARS通过拒绝采样技术改进大型语言模型与人类价值观的对齐，显著提高计算效率和对齐质量。", "motivation": "对齐大型语言模型与人类价值观对于它们的安全部署至关重要；然而，现有方法（如微调）计算量大且效果不佳。相比之下，在推理时间的方法，如最佳N个采样法要求不切实际的计算来实现最佳对齐。", "method": "提出STARS：分段级令牌对齐与拒绝采样，这是一种解码时间算法，通过迭代采样、评分和拒绝/接受固定大小的短令牌段来指导模型生成。", "result": "在六种不同的大语言模型中，STARS在胜率上优于监督微调（SFT）最多14.9个百分点，优于直接偏好优化（DPO）最多4.3个百分点，同时与强大的最佳N个基线保持高度竞争力。", "conclusion": "本研究建立了精细、奖励导向的采样作为传统微调和全序列排序方法的一般化、稳健且高效的替代方案。"}}
{"id": "2511.03950", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03950", "abs": "https://arxiv.org/abs/2511.03950", "authors": ["Zhejia Cai", "Puhua Jiang", "Shiwei Mao", "Hongkun Cao", "Ruqi Huang"], "title": "Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization", "comment": "10 pages", "summary": "Reconstructing real-world objects from multi-view images is essential for\napplications in 3D editing, AR/VR, and digital content creation. Existing\nmethods typically prioritize either geometric accuracy (Multi-View Stereo) or\nphotorealistic rendering (Novel View Synthesis), often decoupling geometry and\nappearance optimization, which hinders downstream editing tasks. This paper\nadvocates an unified treatment on geometry and appearance optimization for\nseamless Gaussian-mesh joint optimization. More specifically, we propose a\nnovel framework that simultaneously optimizes mesh geometry (vertex positions\nand faces) and vertex colors via Gaussian-guided mesh differentiable rendering,\nleveraging photometric consistency from input images and geometric\nregularization from normal and depth maps. The obtained high-quality 3D\nreconstruction can be further exploit in down-stream editing tasks, such as\nrelighting and shape deformation. The code will be publicly available upon\nacceptance.", "AI": {"tldr": "The paper presents an unified framework for optimizing 3D mesh geometry and appearance together, which is significant for downstream editing tasks in 3D content creation.", "motivation": "The motivation behind this work is to overcome the limitation of existing methods which often decouple geometry and appearance optimization, leading to less seamless 3D reconstructions that are not ideal for downstream editing tasks.", "method": "This paper proposes a novel framework to jointly optimize mesh geometry and appearance using Gaussian-guided mesh differentiable rendering.", "result": "High-quality 3D reconstructions are obtained, which can be beneficial for tasks like relighting and shape deformation.", "conclusion": "Joint optimization of geometry and appearance leads to more seamless Gaussian-mesh reconstructions, enhancing the performance in downstream editing tasks."}}
{"id": "2511.03830", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03830", "abs": "https://arxiv.org/abs/2511.03830", "authors": ["Mikołaj Langner", "Jan Eliasz", "Ewa Rudnicka", "Jan Kocoń"], "title": "Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification", "comment": "9 pages, 8 figures", "summary": "We introduce a method for efficient multi-label text classification with\nlarge language models (LLMs), built on reformulating classification tasks as\nsequences of dichotomic (yes/no) decisions. Instead of generating all labels in\na single structured response, each target dimension is queried independently,\nwhich, combined with a prefix caching mechanism, yields substantial efficiency\ngains for short-text inference without loss of accuracy. To demonstrate the\napproach, we focus on affective text analysis, covering 24 dimensions including\nemotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator\nmodel (DeepSeek-V3) provides multiple annotations per text, which are\naggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,\nGemma3-1B). The fine-tuned models show significant improvements over zero-shot\nbaselines, particularly on the dimensions seen during training. Our findings\nsuggest that decomposing multi-label classification into dichotomic queries,\ncombined with distillation and cache-aware inference, offers a scalable and\neffective framework for LLM-based classification. While we validate the method\non affective states, the approach is general and applicable across domains.", "AI": {"tldr": "A scalable and accurate multi-label text classification method using large language models, demonstrating significant improvements over zero-shot baselines, especially effective on seen dimensions during training.", "motivation": "The motivation is to develop an efficient method for multi-label text classification that maintains high accuracy while being scalable, focusing on affective text analysis covering 24 dimensions.", "method": "We introduce a method for efficient multi-label text classification using large language models (LLMs) by reformulating classification tasks into a series of yes/no decisions, querying each target dimension independently, and using a prefix caching mechanism to improve efficiency without losing accuracy.", "result": "The fine-tuned models show significant improvements over zero-shot baselines, especially on seen dimensions during training.", "conclusion": "Our approach of decomposing multi-label classification into dichotomic queries, combined with distillation and cache-aware inference, offers a scalable and effective framework for LLM-based classification, which is generalizable across domains."}}
{"id": "2511.03962", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03962", "abs": "https://arxiv.org/abs/2511.03962", "authors": ["Zhong Chen", "Changfeng Chen"], "title": "A Linear Fractional Transformation Model and Calibration Method for Light Field Camera", "comment": null, "summary": "Accurate calibration of internal parameters is a crucial yet challenging\nprerequisite for 3D reconstruction using light field cameras. In this paper, we\npropose a linear fractional transformation(LFT) parameter $\\alpha$ to decoupled\nthe main lens and micro lens array (MLA). The proposed method includes an\nanalytical solution based on least squares, followed by nonlinear refinement.\nThe method for detecting features from the raw images is also introduced.\nExperimental results on both physical and simulated data have verified the\nperformance of proposed method. Based on proposed model, the simulation of raw\nlight field images becomes faster, which is crucial for data-driven deep\nlearning methods. The corresponding code can be obtained from the author's\nwebsite.", "AI": {"tldr": "This paper introduces a novel calibration method using a linear fractional transformation parameter for light field cameras to improve 3D reconstruction accuracy.", "motivation": "The motivation is to improve the accuracy of the calibration method for internal parameters, which is essential but challenging for 3D reconstruction using light field cameras.", "method": "The paper proposes a linear fractional transformation (LFT) parameter $\\alpha$ to decouple the main lens and micro lens array (MLA) in light field cameras, with an analytical solution using least squares and nonlinear refinement.", "result": "Experiments with both physical and simulated data have validated the effectiveness of the new calibration method and demonstrated its potential for enhancing simulation speeds of raw light field images.", "conclusion": "The new calibration approach allows for more accurate calibration of light field cameras and accelerates the simulation of raw light field images, which is critical for data-driven deep learning methods."}}
{"id": "2511.03880", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.03880", "abs": "https://arxiv.org/abs/2511.03880", "authors": ["Hellina Hailu Nigatu", "Bethelhem Yemane Mamo", "Bontu Fufa Balcha", "Debora Taye Tesfaye", "Elbethel Daniel Zewdie", "Ikram Behiru Nesiru", "Jitu Ewnetu Hailu", "Senait Mengesha Yayo"], "title": "Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered Lens", "comment": "Paper Under Review", "summary": "As low-resourced languages are increasingly incorporated into NLP research,\nthere is an emphasis on collecting large-scale datasets. But in prioritizing\nquantity over quality, we risk 1) building language technologies that perform\npoorly for these languages and 2) producing harmful content that perpetuates\nsocietal biases. In this paper, we investigate the quality of Machine\nTranslation (MT) datasets for three low-resourced languages--Afan Oromo,\nAmharic, and Tigrinya, with a focus on the gender representation in the\ndatasets. Our findings demonstrate that while training data has a large\nrepresentation of political and religious domain text, benchmark datasets are\nfocused on news, health, and sports. We also found a large skew towards the\nmale gender--in names of persons, the grammatical gender of verbs, and in\nstereotypical depictions in the datasets. Further, we found harmful and toxic\ndepictions against women, which were more prominent for the language with the\nlargest amount of data, underscoring that quantity does not guarantee quality.\nWe hope that our work inspires further inquiry into the datasets collected for\nlow-resourced languages and prompts early mitigation of harmful content.\nWARNING: This paper contains discussion of NSFW content that some may find\ndisturbing.", "AI": {"tldr": "研究对三种低资源语言的机器翻译数据集进行了性别表示和质量的评估，发现数据集中存在男性偏向及对女性有害的内容。", "motivation": "探讨低资源语言机器翻译数据集中性别表示和数据质量的问题，旨在提高对低资源语言数据集潜在问题的认识。", "method": "分析了Afan Oromo, Amharic和Tigrinya这三种语言的训练数据和基准数据，重点在于性别表示。", "result": "发现训练数据主要集中在政治和宗教领域，而基准数据集集中于新闻、健康和体育。数据集显示了对男性的显著偏向，并且有对女性的有害描述。", "conclusion": "研究成果强调数量的增长并非质量的保证，鼓励进一步研究低资源语言的数据集，并采取早期措施避免有害内容。"}}
{"id": "2511.03970", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03970", "abs": "https://arxiv.org/abs/2511.03970", "authors": ["Sam Bahrami", "Dylan Campbell"], "title": "Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images", "comment": null, "summary": "Modern scene reconstruction methods are able to accurately recover 3D\nsurfaces that are visible in one or more images. However, this leads to\nincomplete reconstructions, missing all occluded surfaces. While much progress\nhas been made on reconstructing entire objects given partial observations using\ngenerative models, the structural elements of a scene, like the walls, floors\nand ceilings, have received less attention. We argue that these scene elements\nshould be relatively easy to predict, since they are typically planar,\nrepetitive and simple, and so less costly approaches may be suitable. In this\nwork, we present a synthetic dataset -- Room Envelopes -- that facilitates\nprogress on this task by providing a set of RGB images and two associated\npointmaps for each image: one capturing the visible surface and one capturing\nthe first surface once fittings and fixtures are removed, that is, the\nstructural layout. As we show, this enables direct supervision for feed-forward\nmonocular geometry estimators that predict both the first visible surface and\nthe first layout surface. This confers an understanding of the scene's extent,\nas well as the shape and location of its objects.", "AI": {"tldr": "提出'Room Envelopes'合成数据集，包含RGB图像和两个点图，支持直接监督前馈单目几何估计器预测情景可见和布局表面，从而推进场景重建的研究。", "motivation": "现代场景重建方法虽然能准确恢复一个或多个图像中可见的3D表面，但会导致不完全重建，无法捕捉到所有被遮挡的表面。作者认为，由于场景中的元素如墙壁、地板和天花板通常是平面的、重复的且简单，因此这些场景元素应该相对容易预测，可以采用成本较低的方法。", "method": "通过创建一个名为'Room Envelopes'的合成数据集来推进这一任务，该数据集为每个RGB图像提供了两个关联的点图：一个捕获可见表面，另一个捕获移除装饰和装置后的第一个表面，即结构布局。这允许对前馈单目几何估计器进行直接监督，这些估计器可以预测第一个可见表面和第一个布局表面。", "result": "此项工作展示了一个合成数据集，它可以作为直接监督前馈单目几何估计器的基础，这些估计器能够预测第一个可见表面和第一个布局表面。这有助于理解场景的范围，以及其中对象的形状和位置。", "conclusion": "该数据集为预测场景的结构布局提供了直接监督，有助于理解场景的范围及其内对象的形状和位置，为不完全场景重建提供了一种新的研究和解决方法。"}}
{"id": "2511.03900", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03900", "abs": "https://arxiv.org/abs/2511.03900", "authors": ["Manh Nguyen", "Sunil Gupta", "Dai Do", "Hung Le"], "title": "GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation", "comment": null, "summary": "Hallucination mitigation remains a persistent challenge for large language\nmodels (LLMs), even as model scales grow. Existing approaches often rely on\nexternal knowledge sources, such as structured databases or knowledge graphs,\naccessed through prompting or retrieval. However, prompt-based grounding is\nfragile and domain-sensitive, while symbolic knowledge integration incurs heavy\nretrieval and formatting costs. Motivated by knowledge graphs, we introduce\nGraph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds\ngeneration in corpus-derived evidence without retraining. GRAD constructs a\nsparse token transition graph by accumulating next-token logits across a small\nretrieved corpus in a single forward pass. During decoding, graph-retrieved\nlogits are max-normalized and adaptively fused with model logits to favor\nhigh-evidence continuations while preserving fluency. Across three models and a\nrange of question-answering benchmarks spanning intrinsic, extrinsic\nhallucination, and factuality tasks, GRAD consistently surpasses baselines,\nachieving up to 9.7$\\%$ higher intrinsic accuracy, 8.6$\\%$ lower hallucination\nrates, and 6.9$\\%$ greater correctness compared to greedy decoding, while\nattaining the highest truth--informativeness product score among all methods.\nGRAD offers a lightweight, plug-and-play alternative to contrastive decoding\nand knowledge graph augmentation, demonstrating that statistical evidence from\ncorpus-level token transitions can effectively steer generation toward more\ntruthful and verifiable outputs.", "AI": {"tldr": "Introduces GRAD, a method to mitigate hallucinations in large language models by constructing and utilizing a token transition graph during the decoding phase.", "motivation": "To address the issue of hallucination in large language models (LLMs) without the fragility and costs associated with existing methods such as prompt-based grounding and symbolic knowledge integration.", "method": "Graph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds generation in corpus-derived evidence without retraining. GRAD constructs a sparse token transition graph by accumulating next-token logits across a small retrieved corpus in a single forward pass.", "result": "Across different language models and question-answering benchmarks, GRAD consistently surpasses baselines, showing significant improvements in accuracy, reduction in hallucination rates, and factual correctness.", "conclusion": "GRAD provides an effective, lightweight solution to mitigate hallucinations in LLMs, leveraging statistical evidence from corpus-level transitions to improve truthfulness and verifiability of model outputs."}}
{"id": "2511.03988", "categories": ["cs.CV", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2511.03988", "abs": "https://arxiv.org/abs/2511.03988", "authors": ["Wenshuo Qin", "Leyla Isik"], "title": "Simple 3D Pose Features Support Human and Machine Social Scene Understanding", "comment": "28 pages, 6 figures", "summary": "Humans can quickly and effortlessly extract a variety of information about\nothers' social interactions from visual input, ranging from visuospatial cues\nlike whether two people are facing each other to higher-level information. Yet,\nthe computations supporting these abilities remain poorly understood, and\nsocial interaction recognition continues to challenge even the most advanced AI\nvision systems. Here, we hypothesized that humans rely on 3D visuospatial pose\ninformation to make social interaction judgments, which is absent in most AI\nvision models. To test this, we combined state-of-the-art pose and depth\nestimation algorithms to extract 3D joint positions of people in short video\nclips depicting everyday human actions and compared their ability to predict\nhuman social interaction judgments with current AI vision models. Strikingly,\n3D joint positions outperformed most current AI vision models, revealing that\nkey social information is available in explicit body position but not in the\nlearned features of most vision models, including even the layer-wise\nembeddings of the pose models used to extract joint positions. To uncover the\ncritical pose features humans use to make social judgments, we derived a\ncompact set of 3D social pose features describing only the 3D position and\ndirection of faces in the videos. We found that these minimal descriptors\nmatched the predictive strength of the full set of 3D joints and significantly\nimproved the performance of off-the-shelf AI vision models when combined with\ntheir embeddings. Moreover, the degree to which 3D social pose features were\nrepresented in each off-the-shelf AI vision model predicted the model's ability\nto match human social judgments. Together, our findings provide strong evidence\nthat human social scene understanding relies on explicit representations of 3D\npose and can be supported by simple, structured visuospatial primitives.", "AI": {"tldr": "研究发现人类在判断社交互动时依赖3D空间姿态信息，这在大多数AI视觉模型中缺失，提取3D关节位置的数据优于这些模型，简单的3D姿态特征能够显著改善现有视觉模型的表现。", "motivation": "理解人类如何通过视觉输入快速、轻松地从社交互动中提取信息的计算过程，并测试是否人类依赖于3D空间姿态信息来进行社交互动判断。", "method": "结合最先进的姿态和深度估计算法来提取短视频中人物的3D关节位置，并将它们预测人类社交互动判断的能力与当前AI视觉模型进行比较。", "result": "3D关节位置的表现优于大多数现有的AI视觉模型，表明关键的社会信息存在于显式的身体姿势中，而不是现有的学习到的特征。", "conclusion": "研究表明人类理解社交场景依靠显式的3D姿态表达并且可以通过简单的、结构化的视觉-空间基本特征来支持。"}}
{"id": "2511.03908", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03908", "abs": "https://arxiv.org/abs/2511.03908", "authors": ["Alvin Wei Ming Tan", "Ben Prystawski", "Veronica Boyce", "Michael C. Frank"], "title": "Context informs pragmatic interpretation in vision-language models", "comment": "Accepted at CogInterp Workshop, NeurIPS 2025", "summary": "Iterated reference games - in which players repeatedly pick out novel\nreferents using language - present a test case for agents' ability to perform\ncontext-sensitive pragmatic reasoning in multi-turn linguistic environments. We\ntested humans and vision-language models on trials from iterated reference\ngames, varying the given context in terms of amount, order, and relevance.\nWithout relevant context, models were above chance but substantially worse than\nhumans. However, with relevant context, model performance increased\ndramatically over trials. Few-shot reference games with abstract referents\nremain a difficult task for machine learning models.", "AI": {"tldr": "研究显示视觉语言模型在迭代参照游戏中，若无适当上下文，其表现不理想，但适当上下文提供了显著的性能提升，尽管对于抽象参照物的几轮游戏依然困难重重。", "motivation": "此研究旨在评估语言模型处理多轮参照任务的上下文敏感性和语用推理能力，试图揭示当前模型在理解复杂语言环境方面的能力边界。", "method": "本研究通过迭代参照游戏测试人类和视觉语言模型在多轮语言环境中进行语用推理的能力。游戏过程中，玩家反复使用语言挑选新的参照物。研究中改变了给定上下文的数量、顺序和相关性来测试参与者的性能。", "result": "在没有相关上下文的情况下，模型的表现略高于随机水平，但显著低于人类。然而，当提供了相关上下文，模型的表现有了显著的提高。然而，对于使用抽象参照物的几轮参照游戏，机器学习模型仍然面临着较大的挑战。", "conclusion": "虽然视觉语言模型在提供相关上下文的情况下表现有所提升，但与人类相比仍存在差距，特别是在处理抽象参照物的几轮参照游戏中表现不佳，表明在上下文理解方面存在改进空间。"}}
{"id": "2511.03992", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03992", "abs": "https://arxiv.org/abs/2511.03992", "authors": ["Yuwen Tao", "Kanglei Zhou", "Xin Tan", "Yuan Xie"], "title": "CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation", "comment": null, "summary": "Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret\nfree-form language expressions and localize the corresponding 3D regions in\nGaussian fields. While recent advances have introduced cross-modal alignment\nbetween language and 3D geometry, existing pipelines still struggle with\ncross-view consistency due to their reliance on 2D rendered pseudo supervision\nand view specific feature learning. In this work, we present Camera Aware\nReferring Field (CaRF), a fully differentiable framework that operates directly\nin the 3D Gaussian space and achieves multi view consistency. Specifically,\nCaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates\ncamera geometry into Gaussian text interactions to explicitly model view\ndependent variations and enhance geometric reasoning. Building on this, In\nTraining Paired View Supervision (ITPVS) is proposed to align per Gaussian\nlogits across calibrated views during training, effectively mitigating single\nview overfitting and exposing inter view discrepancies for optimization.\nExtensive experiments on three representative benchmarks demonstrate that CaRF\nachieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of\nthe art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively.\nMoreover, this work promotes more reliable and view consistent 3D scene\nunderstanding, with potential benefits for embodied AI, AR/VR interaction, and\nautonomous perception.", "AI": {"tldr": "Camera Aware Referring Field (CaRF) achieves multi-view consistency in 3D Gaussian splatting by incorporating camera geometry into Gaussian text interactions, aligning per Gaussian logits across views during training, and outperforms existing methods.", "motivation": "The motivation behind the method is to address the limitations of existing cross-modal alignment techniques that suffer from cross-view consistency issues due to reliance on 2D rendered pseudo supervision and view-specific feature learning. The goal is to achieve more consistent and reliable 3D scene understanding.", "method": "The method proposes Camera Aware Referring Field (CaRF), a fully differentiable framework that operates in 3D Gaussian space. It includes Gaussian Field Camera Encoding (GFCE), which integrates camera geometry into Gaussian text interactions to explicitly model view-dependent variations. Additionally, In Training Paired View Supervision (ITPVS) is introduced to align per Gaussian logits across calibrated views during training, aiming to mitigate single view overfitting and optimize for inter-view discrepancies.", "result": "Experiments show that the proposed CaRF framework outperforms state-of-the-art methods on the Ref LERF, LERF OVS, and 3D OVS datasets with average mIoU improvements of 16.8%, 4.3%, and 2.0%, respectively.", "conclusion": "The conclusion is that the framework, named CaRF, promotes a more reliable and view-consistent 3D scene understanding, with potential application benefits for embodied AI, AR/VR interaction, and autonomous perception."}}
{"id": "2511.03915", "categories": ["cs.CL", "cs.CY", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.03915", "abs": "https://arxiv.org/abs/2511.03915", "authors": ["Stefano M. Iacus", "Devika Jain", "Andrea Nasuto", "Giuseppe Porro", "Marcello Carammia", "Andrea Vezzulli"], "title": "The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013--2023", "comment": null, "summary": "Quantifying human flourishing, a multidimensional construct including\nhappiness, health, purpose, virtue, relationships, and financial stability, is\ncritical for understanding societal well-being beyond economic indicators.\nExisting measures often lack fine spatial and temporal resolution. Here we\nintroduce the Human Flourishing Geographic Index (HFGI), derived from analyzing\napproximately 2.6 billion geolocated U.S. tweets (2013-2023) using fine-tuned\nlarge language models to classify expressions across 48 indicators aligned with\nHarvard's Global Flourishing Study framework plus attitudes towards migration\nand perception of corruption. The dataset offers monthly and yearly county- and\nstate-level indicators of flourishing-related discourse, validated to confirm\nthat the measures accurately represent the underlying constructs and show\nexpected correlations with established indicators. This resource enables\nmultidisciplinary analyses of well-being, inequality, and social change at\nunprecedented resolution, offering insights into the dynamics of human\nflourishing as reflected in social media discourse across the United States\nover the past decade.", "AI": {"tldr": "该研究使用人类繁荣地理指数(HFGI)通过分析带有地理位置信息的推文来研究人类繁荣，并提供了社会福利、不平等和社会变化的多学科分析。", "motivation": "现有的衡量标准在空间和时间分辨率上往往不足。为了更好地理解超越经济指标的社会福利，该研究量化了包括幸福、健康、目的、美德、关系和财务稳定在内的多维度的人类繁荣。", "method": "该研究通过分析2013年至2023年间大约26亿条带有地理位置信息的美国推文，利用微调后的大型语言模型分类出与哈佛全球繁荣研究框架对齐的48项指标，加上关于移民态度和腐败感知的评估，来进行人类繁荣地理指数(HFGI)的研究。", "result": "该数据集提供了以月度和年度为单位的县和州级别的繁荣相关话语指标，并且验证了这些度量准确地反映了这些潜在变量，并显示出与已建立的指标预期的相关性。", "conclusion": "这项资源以前所未有的分辨率提供了美国过去十年社会媒体话语中人类繁荣动态的深厚见解。"}}
{"id": "2511.03997", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03997", "abs": "https://arxiv.org/abs/2511.03997", "authors": ["Peiyao Wang", "Weining Wang", "Qi Li"], "title": "PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection", "comment": null, "summary": "Recent advances in text-to-video generation have achieved impressive\nperceptual quality, yet generated content often violates fundamental principles\nof physical plausibility - manifesting as implausible object dynamics,\nincoherent interactions, and unrealistic motion patterns. Such failures hinder\nthe deployment of video generation models in embodied AI, robotics, and\nsimulation-intensive domains. To bridge this gap, we propose PhysCorr, a\nunified framework for modeling, evaluating, and optimizing physical consistency\nin video generation. Specifically, we introduce PhysicsRM, the first\ndual-dimensional reward model that quantifies both intra-object stability and\ninter-object interactions. On this foundation, we develop PhyDPO, a novel\ndirect preference optimization pipeline that leverages contrastive feedback and\nphysics-aware reweighting to guide generation toward physically coherent\noutputs. Our approach is model-agnostic and scalable, enabling seamless\nintegration into a wide range of video diffusion and transformer-based\nbackbones. Extensive experiments across multiple benchmarks demonstrate that\nPhysCorr achieves significant improvements in physical realism while preserving\nvisual fidelity and semantic alignment. This work takes a critical step toward\nphysically grounded and trustworthy video generation.", "AI": {"tldr": "本文提出了一种名为PhysCorr的框架，以提升视频生成中的物理一致性，通过PhysicsRM和PhyDPO的结合，改善了视频生成模型中的物理现实性，同时保持了视觉保真度和语义对齐。", "motivation": "现有的文本到视频生成技术在感知质量上取得了显著进步，但生成的内容往往违背了物理合理性的基本原则，如不合理的物体运动、不连贯的交互和不现实的运动模式。这些问题阻碍了视频生成模型在具身AI、机器人学和仿真领域中的应用。为了填补这一差距，提出了PhysCorr框架。", "method": "本文提出了PhysCorr框架，该框架用于建模、评估和优化视频生成中的物理一致性。具体来说，引入了PhysicsRM，这是第一个双重维度的奖励模型，它量化了单个对象的稳定性和对象间的相互作用。在此基础上，开发了PhyDPO，这是一种利用对比反馈和物理感知重权的新型直接偏好优化流水线，引导生成物理上合理的结果。", "result": "实验结果表明，PhysCorr能够在多个基准测试中显著提高物理真实感，同时保持视觉保真度和语义对齐。", "conclusion": "该工作朝着物理现实和可靠的视频生成迈出了关键的一步。"}}
{"id": "2511.03945", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2511.03945", "abs": "https://arxiv.org/abs/2511.03945", "authors": ["Fu-Chun Yang", "Jason Eshraghian"], "title": "Direct Semantic Communication Between Large Language Models via Vector Translation", "comment": "9 pages, 1 figure, 2 tables", "summary": "In multi-agent settings, such as debate, reflection, or tool-calling, large\nlanguage models (LLMs) pass messages as plain tokens, discarding most latent\nsemantics. This constrains information transfer and adds unnecessary\ncomputational overhead. We form a latent bridge via vector translations, which\nuse learned mappings that enable direct semantic exchange between\nrepresentation spaces. A dual-encoder translator trained between Llama-2-7B and\nMistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the\ntranslated vectors at 30 percent blending strength steers the target model's\ngeneration without destabilizing logits. Bidirectional evaluation shows a\n2.01:1 transfer asymmetry, indicating that general-purpose models yield more\ntransferable representations than instruction-tuned variants. This conservative\ninjection preserves computational stability while demonstrating that\ncross-model latent communication is feasible, enabling collaborative AI systems\nthat share meaning rather than tokens.", "AI": {"tldr": "通过矢量翻译形成潜在桥梁，使得Llama-2-7B和Mistral-7B-Instruct之间的表示空间可以进行直接的语义交换，证明跨模型潜在通信的可行性。", "motivation": "在基于多智能体设置的情况下，大型语言模型（LLMs）以纯标记方式传递信息，这限制了信息传输并增加了不必要的计算开销。", "method": "通过矢量转换形成的潜在桥梁，使用学习到的映射，使表示空间之间的语义直接交换成为可能。在Llama-2-7B和Mistral-7B-Instruct模型之间训练了一个双编码器翻译器，平均余弦对齐度为0.538。以30%的融合强度注入翻译后的矢量，可以指导目标模型的生成，而不会使对数概率发生动荡。双向评估显示了2.01:1的传输不对称性，表明通用模型比指令调优变体模型更具可转移的表示形式。", "result": "保守注入保持了计算稳定性，同时证明了跨模型潜在通信的可行性，使得能够共享语义的协作AI系统成为可能。", "conclusion": "跨模型潜在通信是可行的，这使得能够共享语义的协作AI系统成为可能。"}}
{"id": "2511.04008", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04008", "abs": "https://arxiv.org/abs/2511.04008", "authors": ["Mahmoud Soliman", "Omar Abdelaziz", "Ahmed Radwan", "Anand", "Mohamed Shehata"], "title": "GNN-MoE: Context-Aware Patch Routing using GNNs for Parameter-Efficient Domain Generalization", "comment": "6 pages, 3 figures", "summary": "Domain generalization (DG) seeks robust Vision Transformer (ViT) performance\non unseen domains. Efficiently adapting pretrained ViTs for DG is challenging;\nstandard fine-tuning is costly and can impair generalization. We propose\nGNN-MoE, enhancing Parameter-Efficient Fine-Tuning (PEFT) for DG with a\nMixture-of-Experts (MoE) framework using efficient Kronecker adapters. Instead\nof token-based routing, a novel Graph Neural Network (GNN) router (GCN, GAT,\nSAGE) operates on inter-patch graphs to dynamically assign patches to\nspecialized experts. This context-aware GNN routing leverages inter-patch\nrelationships for better adaptation to domain shifts. GNN-MoE achieves\nstate-of-the-art or competitive DG benchmark performance with high parameter\nefficiency, highlighting the utility of graph-based contextual routing for\nrobust, lightweight DG.", "AI": {"tldr": "本研究提出 GNN-MoE 方法，使用 GNN 在补丁间建立关系图，并动态分配补丁到专门化的专家模型中，这种方法在模拟不同领域适应性时表现出色。", "motivation": "研究的动机在于，当前直接微调预训练的 ViT 以适应未知领域是昂贵且可能损害一般化性能的。GNN-MoE 提出了一种更高效、参数更精简的调整方法，以解决这一问题。", "method": "GNN-MoE 方法利用混合专家（MoE）框架，并采用高效的 Kronecker 适配器来增强预训练 Vision Transformer (ViT) 的调整效率。该方法创造性地使用图神经网络（GNN）路由器（如GCN, GAT, SAGE）在补丁间构建关系图，并根据这些关系动态分配补丁到相应的专家模型中。", "result": "GNN-MoE 在域泛化基准测试中表现出色，达到了目前的行业领先水平或具有竞争力的性能，证明了图基背景下路由对于领域泛化问题的实用性和轻量级优势。", "conclusion": "结论是，通过采用基于图的上下文路由机制，GNN-MoE 为 Vision Transformer 提供了一种高效的领域泛化方法，并强调了此方法在参数效率高的同时，依然能够保持出色的模型性能。"}}
{"id": "2511.04020", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04020", "abs": "https://arxiv.org/abs/2511.04020", "authors": ["Shiyin Lin"], "title": "Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises", "comment": null, "summary": "Large Language Models (LLMs) enhanced with retrieval -- commonly referred to\nas Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance\nin knowledge-intensive tasks. However, RAG pipelines often fail when retrieved\nevidence is incomplete, leaving gaps in the reasoning process. In such cases,\n\\emph{abductive inference} -- the process of generating plausible missing\npremises to explain observations -- offers a principled approach to bridge\nthese gaps. In this paper, we propose a framework that integrates abductive\ninference into retrieval-augmented LLMs. Our method detects insufficient\nevidence, generates candidate missing premises, and validates them through\nconsistency and plausibility checks. Experimental results on abductive\nreasoning and multi-hop QA benchmarks show that our approach improves both\nanswer accuracy and reasoning faithfulness. This work highlights abductive\ninference as a promising direction for enhancing the robustness and\nexplainability of RAG systems.", "AI": {"tldr": "此论文提出了一种框架，将演绎推理整合到检索增强的大型语言模型（RAG）中，以提高知识密集型任务的性能，在面对证据不完整时表现尤其出色。", "motivation": "该研究旨在通过引入演绎推理来解决现有检索增强生成模型(RAG)在证据不完整时推理能力受限的问题。", "method": "本研究提出的方法包括检测不足的证据、生成候选的缺失前提，并通过一致性和合理性检查来验证这些候选前提。", "result": "在演绎推理和多跳问答基准测试上的实验结果表明，该方法不仅提高了答案的准确性也增强了推理的可信度。", "conclusion": "该研究强调演绎推理是增强RAG系统鲁棒性和可解释性的有前景的方向。"}}
{"id": "2511.04016", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04016", "abs": "https://arxiv.org/abs/2511.04016", "authors": ["Mahmoud Soliman", "Islam Osman", "Mohamed S. Shehata", "Rasika Rajapakshe"], "title": "MedDChest: A Content-Aware Multimodal Foundational Vision Model for Thoracic Imaging", "comment": "10 pages, 2 figures", "summary": "The performance of vision models in medical imaging is often hindered by the\nprevailing paradigm of fine-tuning backbones pre-trained on out-of-domain\nnatural images. To address this fundamental domain gap, we propose MedDChest, a\nnew foundational Vision Transformer (ViT) model optimized specifically for\nthoracic imaging. We pre-trained MedDChest from scratch on a massive, curated,\nmultimodal dataset of over 1.2 million images, encompassing different\nmodalities including Chest X-ray and Computed Tomography (CT) compiled from 10\npublic sources. A core technical contribution of our work is Guided Random\nResized Crops, a novel content-aware data augmentation strategy that biases\nsampling towards anatomically relevant regions, overcoming the inefficiency of\nstandard cropping techniques on medical scans. We validate our model's\neffectiveness by fine-tuning it on a diverse set of downstream diagnostic\ntasks. Comprehensive experiments empirically demonstrate that MedDChest\nsignificantly outperforms strong, publicly available ImageNet-pretrained\nmodels. By establishing the superiority of large-scale, in-domain pre-training\ncombined with domain-specific data augmentation, MedDChest provides a powerful\nand robust feature extractor that serves as a significantly better starting\npoint for a wide array of thoracic diagnostic tasks. The model weights will be\nmade publicly available to foster future research and applications.", "AI": {"tldr": "提出MedDChest，一种专门针对胸部成像任务进行预训练的Vision Transformer模型，结合了新的数据增强策略，显著提高了医学图像模型的性能。", "motivation": "现有的视觉模型在医学成像性能方面受到阻碍，主要是因为这些模型通常是基于非医学领域图像进行预训练后再微调获得的。为了克服这一领域的根本差异，我们特别提出了MedDChest。", "method": "我们提出了MedDChest，这是一种专门为胸部成像优化的基础视觉变压器模型(ViT)，它从零开始在超过120万张不同模态的胸片图像（包括X光和CT图像）上预训练，这些图像来源于10个公共数据集。提出了一种名为Guided Random Resized Crops的内容感知数据增强策略，它偏向于选择解剖学相关的区域，从而克服了标准裁剪技术在医学图像上的低效性。", "result": "将MedDChest微调后应用于多种下游诊断任务中，实验结果证明其显著优于使用公共ImageNet预训练模型的强大模型。实验还展示了大规模和领域内预训练与领域特定数据增强策略相结合的优越性。", "conclusion": "MedDChest提供了一个强大的特征提取器作为起点，适用于广泛的胸部诊断任务，这为进一步研究和应用程序奠定了坚实的基础。"}}
{"id": "2511.04035", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04035", "abs": "https://arxiv.org/abs/2511.04035", "authors": ["Dongji Gao", "Chenda Liao", "Changliang Liu", "Matthew Wiesner", "Leibny Paola Garcia", "Daniel Povey", "Sanjeev Khudanpur", "Jian Wu"], "title": "WST: Weakly Supervised Transducer for Automatic Speech Recognition", "comment": null, "summary": "The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in\nend-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily\non large-scale, high-quality annotated data, which are often costly and\ndifficult to obtain. To mitigate this reliance, we propose a Weakly Supervised\nTransducer (WST), which integrates a flexible training graph designed to\nrobustly handle errors in the transcripts without requiring additional\nconfidence estimation or auxiliary pre-trained models. Empirical evaluations on\nsynthetic and industrial datasets reveal that WST effectively maintains\nperformance even with transcription error rates of up to 70%, consistently\noutperforming existing Connectionist Temporal Classification (CTC)-based weakly\nsupervised approaches, such as Bypass Temporal Classification (BTC) and\nOmni-Temporal Classification (OTC). These results demonstrate the practical\nutility and robustness of WST in realistic ASR settings. The implementation\nwill be publicly available.", "AI": {"tldr": "本文提出Weakly Supervised Transducer（WST），它能更鲁棒地处理错误的脚本，且无需额外的置信度估计，即使在高错误率转录的情况下也能表现良好。", "motivation": "动机在于减轻RNN-T对大规模、高质量标注数据的依赖，这些数据通常成本高昂且难以获取。", "method": "我们提出了一种名为弱监督转导器（WST）的方法，该方法集成了一个灵活的训练图，可以鲁棒地处理错误的脚本，而无需额外的置信度估计或辅助预训练模型。", "result": "实证评估显示，WST能够有效维持性能，即使转录错误率达到70%，它也始终优于现有的基于连接时序分类（CTC）的弱监督方法，如绕道时间分类（BTC）和全时序分类（OTC）。", "conclusion": "这些结果展示了WST在实际语音识别设置中的实用性和鲁棒性，并将公开实现的代码。"}}
{"id": "2511.04029", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.04029", "abs": "https://arxiv.org/abs/2511.04029", "authors": ["Yihao Luo", "Xianglong He", "Chuanyu Pan", "Yiwen Chen", "Jiaqi Wu", "Yangguang Li", "Wanli Ouyang", "Yuanming Hu", "Guang Yang", "ChoonHwai Yap"], "title": "Near-Lossless 3D Voxel Representation Free from Iso-surface", "comment": null, "summary": "Accurate and efficient voxelized representations of 3D meshes are the\nfoundation of 3D reconstruction and generation. However, existing\nrepresentations based on iso-surface heavily rely on water-tightening or\nrendering optimization, which inevitably compromise geometric fidelity. We\npropose Faithful Contouring, a sparse voxelized representation that supports\n2048+ resolutions for arbitrary meshes, requiring neither converting meshes to\nfield functions nor extracting the isosurface during remeshing. It achieves\nnear-lossless fidelity by preserving sharpness and internal structures, even\nfor challenging cases with complex geometry and topology. The proposed method\nalso shows flexibility for texturing, manipulation, and editing. Beyond\nrepresentation, we design a dual-mode autoencoder for Faithful Contouring,\nenabling scalable and detail-preserving shape reconstruction. Extensive\nexperiments show that Faithful Contouring surpasses existing methods in\naccuracy and efficiency for both representation and reconstruction. For direct\nrepresentation, it achieves distance errors at the $10^{-5}$ level; for mesh\nreconstruction, it yields a 93\\% reduction in Chamfer Distance and a 35\\%\nimprovement in F-score over strong baselines, confirming superior fidelity as a\nrepresentation for 3D learning tasks.", "AI": {"tldr": "提出了一种名为Faithful Contouring的稀疏体素表示方法，该方法无需将网格转换为场函数或在重新网格化时提取等值面，就能实现高保真度的三维网格表示，并优于现有方法。", "motivation": "现有的基于等值面的体素表示方法依赖于加强水密性或渲染优化，这不可避免地会降低几何保真度。", "method": "提出的方法名为Faithful Contouring，是一种稀疏体素表示，支持2048+分辨率的任意网格表示，它可以保持尖锐度和内部结构，即使在具有复杂几何和拓扑的情况下也能实现近无损保真度。此外，该方法还表现出纹理化、操作和编辑的灵活性。", "result": "该方法超越了现有方法，在表示和重建的准确性和效率方面都表现更好。作为直接表示，它实现了10^-5等级的距离误差；作为网格重建，它将Chamfer Distance减少了93%，F-score提升了35%。", "conclusion": "Faithful Contouring方法确认了其作为3D学习任务表示的优越保真度。"}}
{"id": "2511.04070", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04070", "abs": "https://arxiv.org/abs/2511.04070", "authors": ["Shreya Havaldar", "Helen Jin", "Chaehyeon Kim", "Anton Xue", "Weiqiu You", "Marco Gatti", "Bhuvnesh Jain", "Helen Qu", "Daniel A Hashimoto", "Amin Madani", "Rajat Deo", "Sameed Ahmed M. Khatana", "Gary E. Weissman", "Lyle Ungar", "Eric Wong"], "title": "T-FIX: Text-Based Explanations with Features Interpretable to eXperts", "comment": null, "summary": "As LLMs are deployed in knowledge-intensive settings (e.g., surgery,\nastronomy, therapy), users expect not just answers, but also meaningful\nexplanations for those answers. In these settings, users are often domain\nexperts (e.g., doctors, astrophysicists, psychologists) who require\nexplanations that reflect expert-level reasoning. However, current evaluation\nschemes primarily emphasize plausibility or internal faithfulness of the\nexplanation, which fail to capture whether the content of the explanation truly\naligns with expert intuition. We formalize expert alignment as a criterion for\nevaluating explanations with T-FIX, a benchmark spanning seven\nknowledge-intensive domains. In collaboration with domain experts, we develop\nnovel metrics to measure the alignment of LLM explanations with expert\njudgment.", "AI": {"tldr": "本研究开发了名为T-FIX的新基准和度量标准，评估大语言模型生成的解释是否符合知识密集型领域中的专家判断，强调了在专业应用中专家对齐度的重要性。", "motivation": "在诸如外科手术、天文学和心理治疗等知识密集型领域中部署大语言模型（LLM）时，用户不仅需要答案，还希望得到能够体现专家级推理的有意义解释。然而，当前的评估方案主要侧重于解释的表面合理性或内在一致性，未能捕捉到解释内容是否真正符合专家直觉。", "method": "本研究提出T-FIX作为基准，涵盖七个知识密集型领域，旨在衡量大语言模型（LLM）生成的解释与专家直觉的对齐程度。通过与领域专家合作，开发了新的度量标准来评估LLM解释是否符合专家判断。", "result": "开发了新的评估标准来衡量大语言模型生成的解释与专家判断的一致性，但没有具体结果展示。", "conclusion": "通过引入T-FIX基准和开发新的度量方法，本研究提供了一种更为准确的方式来评估大语言模型生成解释的专家对齐度，从而在知识密集型应用中提升用户信任度。"}}
{"id": "2511.04037", "categories": ["cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.04037", "abs": "https://arxiv.org/abs/2511.04037", "authors": ["Arfina Rahman", "Mahesh Banavar"], "title": "A Hybrid Deep Learning Model for Robust Biometric Authentication from Low-Frame-Rate PPG Signals", "comment": "This work has been submitted to IEEE Transactions on Biometrics,\n  Behavior, and Identity Science (TBIOM) for possible publication", "summary": "Photoplethysmography (PPG) signals, which measure changes in blood volume in\nthe skin using light, have recently gained attention in biometric\nauthentication because of their non-invasive acquisition, inherent liveness\ndetection, and suitability for low-cost wearable devices. However, PPG signal\nquality is challenged by motion artifacts, illumination changes, and\ninter-subject physiological variability, making robust feature extraction and\nclassification crucial. This study proposes a lightweight and cost-effective\nbiometric authentication framework based on PPG signals extracted from\nlow-frame-rate fingertip videos. The CFIHSR dataset, comprising PPG recordings\nfrom 46 subjects at a sampling rate of 14 Hz, is employed for evaluation. The\nraw PPG signals undergo a standard preprocessing pipeline involving baseline\ndrift removal, motion artifact suppression using Principal Component Analysis\n(PCA), bandpass filtering, Fourier-based resampling, and amplitude\nnormalization. To generate robust representations, each one-dimensional PPG\nsegment is converted into a two-dimensional time-frequency scalogram via the\nContinuous Wavelet Transform (CWT), effectively capturing transient\ncardiovascular dynamics. We developed a hybrid deep learning model, termed\nCVT-ConvMixer-LSTM, by combining spatial features from the Convolutional Vision\nTransformer (CVT) and ConvMixer branches with temporal features from a Long\nShort-Term Memory network (LSTM). The experimental results on 46 subjects\ndemonstrate an authentication accuracy of 98%, validating the robustness of the\nmodel to noise and variability between subjects. Due to its efficiency,\nscalability, and inherent liveness detection capability, the proposed system is\nwell-suited for real-world mobile and embedded biometric security applications.", "AI": {"tldr": "A robust biometric authentication system using PPG signals from fingertip videos achieves 98% accuracy, addressing challenges like motion artifacts and is ideal for low-cost wearable devices.", "motivation": "The motivation is to leverage the advantages of PPG signals for biometric authentication while addressing challenges like motion artifacts and inter-subject variability, making the system suitable for low-cost wearable devices.", "method": "This paper proposes a lightweight biometric authentication framework using PPG signals from low-frame-rate fingertip videos. A standard preprocessing pipeline and a hybrid deep learning model (CVT-ConvMixer-LSTM) are used to convert raw PPG signals into robust 2D time-frequency scalograms, capturing transient cardiovascular dynamics.", "result": "Experimental results show an authentication accuracy of 98% on a dataset of 46 subjects, demonstrating the model's robustness to noise and subject variability.", "conclusion": "The proposed system is efficient, scalable, and inherently capable of liveness detection, making it suitable for real-world mobile and embedded biometric security applications."}}
{"id": "2511.04072", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04072", "abs": "https://arxiv.org/abs/2511.04072", "authors": ["Xinying Qian", "Ying Zhang", "Yu Zhao", "Baohang Zhou", "Xuhui Sui", "Xiaojie Yuan"], "title": "Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering", "comment": "Submitted to the IEEE for possible publication", "summary": "Temporal Knowledge Graph Question Answering (TKGQA) aims to answer\ntime-sensitive questions by leveraging factual information from Temporal\nKnowledge Graphs (TKGs). While previous studies have employed pre-trained TKG\nembeddings or graph neural networks to inject temporal knowledge, they fail to\nfully understand the complex semantic information of time constraints.\nRecently, Large Language Models (LLMs) have shown remarkable progress,\nbenefiting from their strong semantic understanding and reasoning\ngeneralization capabilities. However, their temporal reasoning ability remains\nlimited. LLMs frequently suffer from hallucination and a lack of knowledge. To\naddress these limitations, we propose the Plan of Knowledge framework with a\ncontrastive temporal retriever, which is named PoK. Specifically, the proposed\nPlan of Knowledge module decomposes a complex temporal question into a sequence\nof sub-objectives from the pre-defined tools, serving as intermediate guidance\nfor reasoning exploration. In parallel, we construct a Temporal Knowledge Store\n(TKS) with a contrastive retrieval framework, enabling the model to selectively\nretrieve semantically and temporally aligned facts from TKGs. By combining\nstructured planning with temporal knowledge retrieval, PoK effectively enhances\nthe interpretability and factual consistency of temporal reasoning. Extensive\nexperiments on four benchmark TKGQA datasets demonstrate that PoK significantly\nimproves the retrieval precision and reasoning accuracy of LLMs, surpassing the\nperformance of the state-of-the-art TKGQA methods by 56.0% at most.", "AI": {"tldr": "提出了一个结合结构化规划与对比式时间知识检索的框架PoK，显著提高了基于大语言模型的时间知识图谱问答系统在时间推理方面的精确性和事实一致性，相较于现有方法最高提升了56.0%的性能。", "motivation": "由于现有方法在理解时间约束的复杂语义信息方面存在不足，同时大语言模型虽然具有强大的语义理解和推理泛化能力，但在时间推理方面的能力有限，且容易出现幻觉和知识缺乏等问题，因此提出PoK框架解决这些问题。", "method": "PoK框架包含一个计划知识模块和一个时间知识存储。计划知识模块通过预定义的工具将复杂的时序问题分解为一系列子目标，提供中间推理指导；时间知识存储部分使用对比式检索框架从时序知识图谱中选择性地检索语义和时间对齐的事实。", "result": "在四个基准时间图谱问答数据集上的实验表明，PoK显著提高了大语言模型的时间检索精度和推理准确性，相对于最先进的时序知识图谱问答方法，性能最多提高了56.0%。", "conclusion": "PoK框架通过结合结构化规划和时间知识检索，有效提升了时序推理的解释性和事实一致性。"}}
{"id": "2511.04078", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04078", "abs": "https://arxiv.org/abs/2511.04078", "authors": ["Zehui Feng", "Chenqi Zhang", "Mingru Wang", "Minuo Wei", "Shiwei Cheng", "Cuntai Guan", "Ting Han"], "title": "Unveiling Deep Semantic Uncertainty Perception for Language-Anchored Multi-modal Vision-Brain Alignment", "comment": "30 pages, 16 figures, under review as a conference paper", "summary": "Unveiling visual semantics from neural signals such as EEG, MEG, and fMRI\nremains a fundamental challenge due to subject variability and the entangled\nnature of visual features. Existing approaches primarily align neural activity\ndirectly with visual embeddings, but visual-only representations often fail to\ncapture latent semantic dimensions, limiting interpretability and deep\nrobustness. To address these limitations, we propose Bratrix, the first\nend-to-end framework to achieve multimodal Language-Anchored Vision-Brain\nalignment. Bratrix decouples visual stimuli into hierarchical visual and\nlinguistic semantic components, and projects both visual and brain\nrepresentations into a shared latent space, enabling the formation of aligned\nvisual-language and brain-language embeddings. To emulate human-like perceptual\nreliability and handle noisy neural signals, Bratrix incorporates a novel\nuncertainty perception module that applies uncertainty-aware weighting during\nalignment. By leveraging learnable language-anchored semantic matrices to\nenhance cross-modal correlations and employing a two-stage training strategy of\nsingle-modality pretraining followed by multimodal fine-tuning, Bratrix-M\nimproves alignment precision. Extensive experiments on EEG, MEG, and fMRI\nbenchmarks demonstrate that Bratrix improves retrieval, reconstruction, and\ncaptioning performance compared to state-of-the-art methods, specifically\nsurpassing 14.3% in 200-way EEG retrieval task. Code and model are available.", "AI": {"tldr": "Bratrix提出了一种多模态语言锚定的视觉-大脑对齐框架，能够更好地解释大脑中的视觉语义，并提升了针对EEG、MEG和fMRI信号的处理性能。", "motivation": "现有的方法主要直接将神经活动与视觉嵌入对齐，但纯视觉表示经常无法捕捉潜在的语义维度，这限制了可解释性和深度鲁棒性。为了解决这些限制，提出了Bratrix框架。", "method": "Bratrix提出了一种多模态语言锚定视觉-大脑对齐的端到端框架。它将视觉刺激分解为分层的视觉和语言语义组件，并将视觉和大脑表示投影到共享的潜在空间中，从而形成对齐的视觉-语言和大脑-语言嵌入。此外，Bratrix引入了一个新颖的不确定性感知模块，以增强在对齐过程中处理嘈杂神经信号的能力。Bratrix-M采用了单模态预训练后进行多模态微调的两阶段训练策略，以提高对齐精度。", "result": "实验表明，Bratrix在EEG、MEG和fMRI基准测试上的检索、重建和描述性能优于最先进的方法，在200路EEG检索任务上超过了最先进的方法14.3%。", "conclusion": "通过引入新颖的不确定性感知模块和采用两阶段训练策略，Bratrix能够更准确地将视觉和语言语义与大脑信号对齐。该框架在多个任务上超出现有方法的表现。"}}
{"id": "2511.04077", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04077", "abs": "https://arxiv.org/abs/2511.04077", "authors": ["Špela Vintar", "Jan Jona Javoršek"], "title": "The truth is no diaper: Human and AI-generated associations to emotional words", "comment": "6 pages, 1 figure. Presented at ICCC'25, Campinas, Brazil", "summary": "Human word associations are a well-known method of gaining insight into the\ninternal mental lexicon, but the responses spontaneously offered by human\nparticipants to word cues are not always predictable as they may be influenced\nby personal experience, emotions or individual cognitive styles. The ability to\nform associative links between seemingly unrelated concepts can be the driving\nmechanisms of creativity. We perform a comparison of the associative behaviour\nof humans compared to large language models. More specifically, we explore\nassociations to emotionally loaded words and try to determine whether large\nlanguage models generate associations in a similar way to humans. We find that\nthe overlap between humans and LLMs is moderate, but also that the associations\nof LLMs tend to amplify the underlying emotional load of the stimulus, and that\nthey tend to be more predictable and less creative than human ones.", "AI": {"tldr": "研究比较了人类和大型语言模型对情感词语的联想反应，发现两者之间有一定的相似性，但大型语言模型生成的联想更可预测，创造性较低。", "motivation": "研究人类与大型语言模型的联想行为差异，特别是在处理带有情感色彩的词汇时的反应，以了解其中的异同。", "method": "通过比较人类与大型语言模型对带有情感词汇的关联反应来进行研究，以确定大型语言模型是否与人类产生类似的关联。", "result": "研究发现，人类与大型语言模型之间的关联重叠度中等，但大型语言模型倾向于放大刺激词的情感负荷，并且它们的关联反应更可预测、创造性较低。", "conclusion": "大型语言模型与人类在处理情感词汇的关联上存在某些相似性，但显示出不同的特征。"}}
{"id": "2511.04083", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04083", "abs": "https://arxiv.org/abs/2511.04083", "authors": ["Abu Hanif Muhammad Syarubany"], "title": "Adversarial and Score-Based CT Denoising: CycleGAN vs Noise2Score", "comment": null, "summary": "We study CT image denoising in the unpaired and self-supervised regimes by\nevaluating two strong, training-data-efficient paradigms: a CycleGAN-based\nresidual translator and a Noise2Score (N2S) score-matching denoiser. Under a\ncommon evaluation protocol, a configuration sweep identifies a simple standard\nU-Net backbone within CycleGAN (lambda_cycle = 30, lambda_iden = 2, ngf = ndf =\n64) as the most reliable setting; we then train it to convergence with a longer\nschedule. The selected CycleGAN improves the noisy input from 34.66 dB / 0.9234\nSSIM to 38.913 dB / 0.971 SSIM and attains an estimated score of 1.9441 and an\nunseen-set (Kaggle leaderboard) score of 1.9343. Noise2Score, while slightly\nbehind in absolute PSNR / SSIM, achieves large gains over very noisy inputs,\nhighlighting its utility when clean pairs are unavailable. Overall, CycleGAN\noffers the strongest final image quality, whereas Noise2Score provides a robust\npair-free alternative with competitive performance. Source code is available at\nhttps://github.com/hanifsyarubany/CT-Scan-Image-Denoising-using-CycleGAN-and-Noise2Score.", "AI": {"tldr": "本研究评估了CycleGAN和Noise2Score在CT图像非配对和自监督去噪中的性能。结果显示CycleGAN提供了较好的图像质量，而Noise2Score则在很大程度上依赖于无噪声输入的情况下提供了一个有效的方法。", "motivation": "研究动机旨在通过两种不同的无成对数据和自监督训练方法提高CT图像的去噪效果。", "method": "我们研究了在非配对和自监督条件下的CT图像去噪，评估了两种高效的数据训练范式：基于CycleGAN的残差转换器和基于Noise2Score的分数匹配去噪器。", "result": "选定的CycleGAN将噪声输入从34.66 dB / 0.9234 SSIM提高到38.913 dB / 0.971 SSIM，并获得了1.9441的估计分数和1.9343的未知集（Kaggle排行榜）评分。Noise2Score在绝对PSNR / SSIM上略逊一筹，但在处理非常噪声输入时表现出显著优势。", "conclusion": "总体上，CycleGAN提供了最强的最终图像质量，而Noise2Score则提供了一个强大的无需成对数据的替代方案，具有竞争力的性能。"}}
{"id": "2511.04079", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04079", "abs": "https://arxiv.org/abs/2511.04079", "authors": ["Eva Prakash", "Maayane Attias", "Pierre Chambon", "Justin Xu", "Steven Truong", "Jean-Benoit Delbrouck", "Tessa Cook", "Curtis Langlotz"], "title": "Improving the Performance of Radiology Report De-identification with Large-Scale Training and Benchmarking Against Cloud Vendor Methods", "comment": "In submission to JAMIA", "summary": "Objective: To enhance automated de-identification of radiology reports by\nscaling transformer-based models through extensive training datasets and\nbenchmarking performance against commercial cloud vendor systems for protected\nhealth information (PHI) detection. Materials and Methods: In this\nretrospective study, we built upon a state-of-the-art, transformer-based, PHI\nde-identification pipeline by fine-tuning on two large annotated radiology\ncorpora from Stanford University, encompassing chest X-ray, chest CT,\nabdomen/pelvis CT, and brain MR reports and introducing an additional PHI\ncategory (AGE) into the architecture. Model performance was evaluated on test\nsets from Stanford and the University of Pennsylvania (Penn) for token-level\nPHI detection. We further assessed (1) the stability of synthetic PHI\ngeneration using a \"hide-in-plain-sight\" method and (2) performance against\ncommercial systems. Precision, recall, and F1 scores were computed across all\nPHI categories. Results: Our model achieved overall F1 scores of 0.973 on the\nPenn dataset and 0.996 on the Stanford dataset, outperforming or maintaining\nthe previous state-of-the-art model performance. Synthetic PHI evaluation\nshowed consistent detectability (overall F1: 0.959 [0.958-0.960]) across 50\nindependently de-identified Penn datasets. Our model outperformed all vendor\nsystems on synthetic Penn reports (overall F1: 0.960 vs. 0.632-0.754).\nDiscussion: Large-scale, multimodal training improved cross-institutional\ngeneralization and robustness. Synthetic PHI generation preserved data utility\nwhile ensuring privacy. Conclusion: A transformer-based de-identification model\ntrained on diverse radiology datasets outperforms prior academic and commercial\nsystems in PHI detection and establishes a new benchmark for secure clinical\ntext processing.", "AI": {"tldr": "研究通过大规模训练数据优化了基于transformer的模型，用于放射学报告的自动化去标识处理，并在基准测试中优于商业云厂商的系统。", "motivation": "提升放射学报告自动化去标识技术，通过扩展训练数据集来优化基于transformer的模型，并对比商业云服务提供商的性能以确定保护健康信息（PHI）检测的准确性。", "method": "本研究在基于transformer的医疗信息去标识流水线的基础上进行微调，利用斯坦福大学的两个大型标注放射学语料库进行训练并增加了一个新的PHI类别（年龄）。同时，使用“隐藏在明处”的方法评估合成PHI生成的稳定性。", "result": "研究模型在宾夕法尼亚大学测试集上的整体F1得分为0.973，在斯坦福大学测试集上的整体F1得分为0.996，优于或保持了之前最先进的模型的表现。在50个独立去标识化的宾夕法尼亚大学的数据集上，合成PHI的检测一致性保持在高水准（0.959）。研究模型优于所有厂商系统。", "conclusion": "大规模、多模态的训练增强了跨机构的推广性和鲁棒性，同时生成的合成PHI保持了数据的实用性并保障了隐私保护。基于transformer的去标识模型在PHI检测上设立了新基准。"}}
{"id": "2511.04084", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04084", "abs": "https://arxiv.org/abs/2511.04084", "authors": ["Nishchal Sapkota", "Haoyan Shi", "Yejia Zhang", "Xianshi Ma", "Bofang Zheng", "Danny Z. Chen"], "title": "When Swin Transformer Meets KANs: An Improved Transformer Architecture for Medical Image Segmentation", "comment": null, "summary": "Medical image segmentation is critical for accurate diagnostics and treatment\nplanning, but remains challenging due to complex anatomical structures and\nlimited annotated training data. CNN-based segmentation methods excel at local\nfeature extraction, but struggle with modeling long-range dependencies.\nTransformers, on the other hand, capture global context more effectively, but\nare inherently data-hungry and computationally expensive. In this work, we\nintroduce UKAST, a U-Net like architecture that integrates rational-function\nbased Kolmogorov-Arnold Networks (KANs) into Swin Transformer encoders. By\nleveraging rational base functions and Group Rational KANs (GR-KANs) from the\nKolmogorov-Arnold Transformer (KAT), our architecture addresses the\ninefficiencies of vanilla spline-based KANs, yielding a more expressive and\ndata-efficient framework with reduced FLOPs and only a very small increase in\nparameter count compared to SwinUNETR. UKAST achieves state-of-the-art\nperformance on four diverse 2D and 3D medical image segmentation benchmarks,\nconsistently surpassing both CNN- and Transformer-based baselines. Notably, it\nattains superior accuracy in data-scarce settings, alleviating the data-hungry\nlimitations of standard Vision Transformers. These results show the potential\nof KAN-enhanced Transformers to advance data-efficient medical image\nsegmentation. Code is available at: https://github.com/nsapkota417/UKAST", "AI": {"tldr": "论文提出UKAST，结合有理函数网络与Transformer，提高医学图像分割的数据效率和精度，特别是在数据稀缺情境下增强超越基线方法的性能。", "motivation": "医学图像分割对于诊断和治疗计划至关重要，但复杂的人体结构和受限的标注训练数据使其成为难题。CNN在局部特征提取中表现出色，但在建模长距离依赖关系方面存在不足；而Transformer尽管能更有效地捕捉全局上下文，但需要大量数据且计算成本高昂。本文旨在解决这些问题。", "method": "本文提出了UKAST架构，该架构整合了基于有理函数的Kolmogorov-Arnold网络(KANs)到Swin Transformer编码器中。通过使用来自Kolmogorov-Arnold Transformer (KAT)的有理基函数和分组有理KANs (GR-KANs)，该架构解决了普通样条基KANs的低效问题，提高了框架的表达能力和数据效率，同时仅微增参数量。", "result": "UKAST在四个不同类型的2D和3D医学图像分割基准上表现出色，持续超越基线的CNN和Transformer方法。特别是在数据稀缺的情况下，其精度优于其他方法，缓解了标准视觉Transformer的数据密集型限制。", "conclusion": "实验结果展示了通过KAN增强的Transformer提升数据高效的医学图像分割的潜力。代码可在https://github.com/nsapkota417/UKAST获取。"}}
{"id": "2511.04103", "categories": ["cs.CL", "cs.AI", "cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04103", "abs": "https://arxiv.org/abs/2511.04103", "authors": ["Moses Charikar", "Chirag Pabbaraju", "Ambuj Tewari"], "title": "A Characterization of List Language Identification in the Limit", "comment": null, "summary": "We study the problem of language identification in the limit, where given a\nsequence of examples from a target language, the goal of the learner is to\noutput a sequence of guesses for the target language such that all the guesses\nbeyond some finite time are correct. Classical results of Gold showed that\nlanguage identification in the limit is impossible for essentially any\ninteresting collection of languages. Later, Angluin gave a precise\ncharacterization of language collections for which this task is possible.\nMotivated by recent positive results for the related problem of language\ngeneration, we revisit the classic language identification problem in the\nsetting where the learner is given the additional power of producing a list of\n$k$ guesses at each time step. The goal is to ensure that beyond some finite\ntime, one of the guesses is correct at each time step.\n  We give an exact characterization of collections of languages that can be\n$k$-list identified in the limit, based on a recursive version of Angluin's\ncharacterization (for language identification with a list of size $1$). This\nfurther leads to a conceptually appealing characterization: A language\ncollection can be $k$-list identified in the limit if and only if the\ncollection can be decomposed into $k$ collections of languages, each of which\ncan be identified in the limit (with a list of size $1$). We also use our\ncharacterization to establish rates for list identification in the statistical\nsetting where the input is drawn as an i.i.d. stream from a distribution\nsupported on some language in the collection. Our results show that if a\ncollection is $k$-list identifiable in the limit, then the collection can be\n$k$-list identified at an exponential rate, and this is best possible. On the\nother hand, if a collection is not $k$-list identifiable in the limit, then it\ncannot be $k$-list identified at any rate that goes to zero.", "AI": {"tldr": "本文研究在学习者具备提出k个猜测能力的情况下，探讨语言识别问题。基于对Angluin结果的递归扩展，精确刻画了哪些语言集合在极限条件下可被k列表识别，并确立了统计设置中的识别速率。", "motivation": "受近期关于语言生成正面结果的启发，本研究重新探讨了经典语言识别问题，特别是给予学习者提出k个猜测的能力，以试图解决Gold早期提出的经典结论，即对于任何有趣的语言集合，语言在极限时的识别是不可能的。", "method": "研究方法是通过引入允许学习者在每个时间步骤提出k个猜测的能力来重新审视经典的语言识别问题。这个方法基于Angluin对语言识别（列表大小为1）的特征化，提出了一个递归版本，以精确刻画哪些语言集合能够以k列表方式识别。此外，还探讨了统计设置中的识别速率问题，即输入是从集合中某个语言上的分布中以i.i.d.方式抽取。", "result": "结果表明，一个语言集合如果能以k列表方式识别，在统计设置中，它也可以以指数级速率进行k列表识别，并且这是最佳的。反之，如果一个集合不能以k列表方式识别，那么它在任何趋向于零的速率下都无法被k列表识别。", "conclusion": "研究得出的结论是，一个语言集合可以以k列表方式在极限条件下识别出来，当且仅当这个集合可以分解为k个子集，每个子集都可以单独在极限条件下识别（列表大小为1）。"}}
{"id": "2511.04112", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04112", "abs": "https://arxiv.org/abs/2511.04112", "authors": ["Biao Liu", "Yuanzhi Liang"], "title": "SpatialLock: Precise Spatial Control in Text-to-Image Synthesis", "comment": "Work in progress", "summary": "Text-to-Image (T2I) synthesis has made significant advancements in recent\nyears, driving applications such as generating datasets automatically. However,\nprecise control over object localization in generated images remains a\nchallenge. Existing methods fail to fully utilize positional information,\nleading to an inadequate understanding of object spatial layouts. To address\nthis issue, we propose SpatialLock, a novel framework that leverages perception\nsignals and grounding information to jointly control the generation of spatial\nlocations. SpatialLock incorporates two components: Position-Engaged Injection\n(PoI) and Position-Guided Learning (PoG). PoI directly integrates spatial\ninformation through an attention layer, encouraging the model to learn the\ngrounding information effectively. PoG employs perception-based supervision to\nfurther refine object localization. Together, these components enable the model\nto generate objects with precise spatial arrangements and improve the visual\nquality of the generated images. Experiments show that SpatialLock sets a new\nstate-of-the-art for precise object positioning, achieving IOU scores above 0.9\nacross multiple datasets.", "AI": {"tldr": "提出SpatialLock框架以解决文本到图像生成中的对象精确定位问题，该框架通过PoI和PoG组件实现空间信息的整合和感知监督，实验结果显示其在多数据集上显著提升了定位精度。", "motivation": "现有方法未能充分利用位置信息，导致对象的空间布局理解不充分，从而难以精确控制生成图像中的对象定位。", "method": "SpatialLock框架，包含Position-Engaged Injection (PoI)和Position-Guided Learning (PoG)，前者通过注意力层直接整合空间信息，后者通过感知监督进一步完善对象定位。", "result": "实验表明，SpatialLock在精确对象定位上达到了新的先进水平，在多个数据集上实现了超过0.9的IOU得分。", "conclusion": "SpatialLock框架能够生成具备精确空间排列的对象，同时还提高了生成图像的视觉质量。"}}
{"id": "2511.04108", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04108", "abs": "https://arxiv.org/abs/2511.04108", "authors": ["Wenmo Qiu", "Saurabh Srivastava"], "title": "Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models", "comment": null, "summary": "Recent work has explored batch prompting as a strategy to amortize inference\ncost in large language models (LLMs). In this paper, we show that batching\noffers an additional, underappreciated benefit: it regularizes model behavior\nduring multi-step reasoning for Large Reasoning Models (LRMs). We conduct a\ncomprehensive study across 13 diverse benchmarks and observe that batching\nimproves accuracy while substantially reducing reasoning token usage, often by\n3x-5x. Through detailed behavioral analysis, we find that batching suppresses\noverthinking, reduces hedging language (e.g., repetitive self-corrections), and\nencourages more decisive answers. Surprisingly, we also observe emergent\ncollective effects in batched inference: models often generalize patterns from\nearlier examples to solve harder ones in the same batch. These findings\nposition batching not just as a throughput optimization, but as a powerful\ninference-time regularizer for more efficient and reliable LLM reasoning.", "AI": {"tldr": "批处理不仅能摊销大型语言模型的推理成本，还能作为一种正则化技术改善推理行为，提高准确性和效率。", "motivation": "论文探讨批处理作为一种策略，不仅可以摊销大型语言模型中的推理成本，还可以作为一种正则化手段来改善多步骤推理行为，提高效率和可靠性。", "method": "通过跨13个不同基准的综合研究，探讨了批处理对大型语言模型推理准确性的影响，并通过详细的分析研究了批处理如何抑制过度思考，减少模棱两可的语言，并鼓励更果断的回答。", "result": "研究发现批处理可以提高模型的准确性，同时显著减少推理令牌的使用量。此外，批处理显示出新兴的集体效应，模型可以将先前例子中的模式泛化以解决同一批次中的更难问题。", "conclusion": "批处理对于大型推理模型不仅是一个吞吐量优化措施，也是提高推理效率和可靠性的重要方法。"}}
{"id": "2511.04117", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04117", "abs": "https://arxiv.org/abs/2511.04117", "authors": ["Yunghee Lee", "Byeonghyun Pak", "Junwha Hong", "Hoseong Kim"], "title": "Tortoise and Hare Guidance: Accelerating Diffusion Model Inference with Multirate Integration", "comment": "21 pages, 8 figures. NeurIPS 2025. Project page:\n  https://yhlee-add.github.io/THG", "summary": "In this paper, we propose Tortoise and Hare Guidance (THG), a training-free\nstrategy that accelerates diffusion sampling while maintaining high-fidelity\ngeneration. We demonstrate that the noise estimate and the additional guidance\nterm exhibit markedly different sensitivity to numerical error by reformulating\nthe classifier-free guidance (CFG) ODE as a multirate system of ODEs. Our\nerror-bound analysis shows that the additional guidance branch is more robust\nto approximation, revealing substantial redundancy that conventional solvers\nfail to exploit. Building on this insight, THG significantly reduces the\ncomputation of the additional guidance: the noise estimate is integrated with\nthe tortoise equation on the original, fine-grained timestep grid, while the\nadditional guidance is integrated with the hare equation only on a coarse grid.\nWe also introduce (i) an error-bound-aware timestep sampler that adaptively\nselects step sizes and (ii) a guidance-scale scheduler that stabilizes large\nextrapolation spans. THG reduces the number of function evaluations (NFE) by up\nto 30% with virtually no loss in generation fidelity ($\\Delta$ImageReward\n$\\leq$ 0.032) and outperforms state-of-the-art CFG-based training-free\naccelerators under identical computation budgets. Our findings highlight the\npotential of multirate formulations for diffusion solvers, paving the way for\nreal-time high-quality image synthesis without any model retraining. The source\ncode is available at https://github.com/yhlee-add/THG.", "AI": {"tldr": "提出Tortoise和Hare Guidance（THG），一种无需训练就能加速扩散采样同时保持高质量生成的策略。通过重新构造Classifier-Free Guidance（CFG）ODE作为多速率ODE系统，揭示了附加指导分支对近似更为鲁棒，减少额外指导的计算，同时也引入了误差界限感知的时间步长采样方法和指导尺度调度器，THG在几乎没有任何生成保真度损失的情况下，最多将函数评估次数减少了30%。", "motivation": "为了在保持高质量生成的前提下加速扩散模型的采样过程。", "method": "通过将Classifier-Free Guidance (CFG) ODE重新构造为一个多速率ODE系统，分析误差界限，揭示附加指导分支的鲁棒性，并提出一种新的多速率加速策略THG，其中噪声估计采用细粒度的时间步长，而附加指导只在粗步长上进行积分，同时引入了误差界限感知的时间步长采样器和指导尺度调度器。", "result": "THG最多可以将函数评估次数（NFE）减少30%，而且在几乎没有任何生成保真度损失的情况下，性能优于现有的CFG加速器。", "conclusion": "THG策略展示了多速率公式化方法在扩散求解器中的潜力，为不用重新训练模型实现高质量图像的实时合成铺平了道路。"}}
{"id": "2511.04120", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04120", "abs": "https://arxiv.org/abs/2511.04120", "authors": ["Xinyuan Li", "Murong Xu", "Wenbiao Tao", "Hanlun Zhu", "Yike Zhao", "Jipeng Zhang", "Yunshi Lan"], "title": "RIDE: Difficulty Evolving Perturbation with Item Response Theory for Mathematical Reasoning", "comment": null, "summary": "Large language models (LLMs) achieve high performance on mathematical\nreasoning, but these results can be inflated by training data leakage or\nsuperficial pattern matching rather than genuine reasoning. To this end, an\nadversarial perturbation-based evaluation is needed to measure true\nmathematical reasoning ability. Current rule-based perturbation methods often\ngenerate ill-posed questions and impede the systematic evaluation of question\ndifficulty and the evolution of benchmarks. To bridge this gap, we propose\nRIDE, a novel adversarial question-rewriting framework that leverages Item\nResponse Theory (IRT) to rigorously measure question difficulty and to generate\nintrinsically more challenging, well-posed variations of mathematical problems.\nWe employ 35 LLMs to simulate students and build a difficulty ranker from their\nresponses. This ranker provides a reward signal during reinforcement learning\nand guides a question-rewriting model to reformulate existing questions across\ndifficulty levels. Applying RIDE to competition-level mathematical benchmarks\nyields perturbed versions that degrade advanced LLM performance, with\nexperiments showing an average 21.73% drop across 26 models, thereby exposing\nlimited robustness in mathematical reasoning and confirming the validity of our\nevaluation approach.", "AI": {"tldr": "提出一种新的对抗性问题重写框架RIDE来严格评估和生成更具有挑战性的数学问题，以此揭示大语言模型在数学推理上的局限。", "motivation": "现有的基于规则的变形方法经常生成不合理的问题，并阻碍了问题难度的系统评估和基准的演化，需要一种新的对抗性评估方法来衡量真正的数学推理能力。", "method": "提出了RIDE框架，该框架使用项目反应理论（IRT）来严格衡量问题难度，并生成更具有挑战性且合理的问题变体。利用35个大语言模型模拟学生，构建了难度评估器，该评估器提供了强化学习过程中的奖励信号，并引导问题重构模型重新设计不同难度等级的问题变体。", "result": "将RIDE应用于竞赛级数学基准测试，生成的变形版本使高级大语言模型的表现下降约21.73%，证明了在数学推理上的脆弱性，并确定了评估方法的有效性。", "conclusion": "实验结果表明，大语言模型在数学推理上存在局限性，特别是在面对经过RIDE框架变形的数学问题时。这显示了RIDE的有效性和评估方法的合理性。"}}
{"id": "2511.04123", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04123", "abs": "https://arxiv.org/abs/2511.04123", "authors": ["Tengjie Li", "Shikui Tu", "Lei Xu"], "title": "Text to Sketch Generation with Multi-Styles", "comment": "Accepted by NeurIPS 2025", "summary": "Recent advances in vision-language models have facilitated progress in sketch\ngeneration. However, existing specialized methods primarily focus on generic\nsynthesis and lack mechanisms for precise control over sketch styles. In this\nwork, we propose a training-free framework based on diffusion models that\nenables explicit style guidance via textual prompts and referenced style\nsketches. Unlike previous style transfer methods that overwrite key and value\nmatrices in self-attention, we incorporate the reference features as auxiliary\ninformation with linear smoothing and leverage a style-content guidance\nmechanism. This design effectively reduces content leakage from reference\nsketches and enhances synthesis quality, especially in cases with low\nstructural similarity between reference and target sketches. Furthermore, we\nextend our framework to support controllable multi-style generation by\nintegrating features from multiple reference sketches, coordinated via a joint\nAdaIN module. Extensive experiments demonstrate that our approach achieves\nhigh-quality sketch generation with accurate style alignment and improved\nflexibility in style control. The official implementation of M3S is available\nat https://github.com/CMACH508/M3S.", "AI": {"tldr": "本文提出了一种基于扩散模型的训练-free框架，该框架通过文本提示和参考样式草图实现明确的样式指导，解决了现有方法缺乏对草图样式的精确控制的问题。实验表明，该方法在高质量的草图生成、准确的样式对齐和样式的灵活性控制方面表现出色。", "motivation": "尽管视觉语言模型的进步促进了草图生成的发展，但现有的专业化方法主要集中在通用合成上，缺乏对草图样式的精确控制的机制。因此，本文动机在于解决这一问题，提供一种新的框架以实现更精细化的样控制。", "method": "Structure", "result": "{\"tldr\": \"本文提出了一种基于扩散模型的训练-free框架，该框架通过文本提示和参考样式草图实现明确的样式指导，解决了现有方法缺乏对草图样式的精确控制的问题。实验表明，该方法在高质量的草图生成、准确的样式对齐和样式的灵活性控制方面表现出色。\", \"motivation\": \"尽管视觉语言模型的进步促进了草图生成的发展，但现有的专业化方法主要集中在通用合成上，缺乏对草图样式的精确控制的机制。因此，本文动机在于解决这一问题，提供一种新的框架以实现更精细化的样控制。\", \"method\": \"提出的方法基于扩散模型，并通过文本提示及参考样式的草图实现样式指导。这一方法并不像之前的样式转换方法那样通过覆盖自注意力中的键和值矩阵来工作，而是将参考特征作为辅助信息，并采用风格和内容引导机制，减少参考草图内容泄漏，提升合成质量。此外，通过整合多个参考草图的特征，利用联合AdaIN模块，使框架支持可控的多种样式生成。\", \"result\": \"实验显示，本方法在高质量草图生成、准确的样式对齐以及样式控制灵活性方面表现出色。\", \"conclusion\": \"该框架通过引入文本提示和参考样式草图，实现了草图生成中的精确风格控制，实验结果表明了其在提升草图质量和控制灵活性方面的优势。\"}", "conclusion": "该框架通过引入文本提示和参考样式草图，实现了草图生成中的精确风格控制，实验结果表明了其在提升草图质量和控制灵活性方面的优势。"}}
{"id": "2511.04139", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.04139", "abs": "https://arxiv.org/abs/2511.04139", "authors": ["Dazhong Chen", "Yi-Cheng Lin", "Yuchen Huang", "Ziwei Gong", "Di Jiang", "Zeying Xie", "Yi R.", "Fung"], "title": "CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese", "comment": null, "summary": "Automatic speech recognition (ASR) is critical for language accessibility,\nyet low-resource Cantonese remains challenging due to limited annotated data,\nsix lexical tones, tone sandhi, and accent variation. Existing ASR models, such\nas Whisper, often suffer from high word error rates. Large audio-language\nmodels (LALMs), in contrast, can leverage broader contextual reasoning but\nstill require explicit tonal and prosodic acoustic cues. We introduce CantoASR,\na collaborative ASR-LALM error correction framework that integrates forced\nalignment for acoustic feature extraction, a LoRA-finetuned Whisper for\nimproved tone discrimination, and an instruction-tuned Qwen-Audio for\nprosody-aware correction. Evaluations on spontaneous Cantonese data show\nsubstantial CER gains over Whisper-Large-V3. These findings suggest that\nintegrating acoustic cues with LALM reasoning provides a scalable strategy for\nlow-resource tonal and dialectal ASR.", "AI": {"tldr": "本文提出了CantoASR方法，利用强制对齐和改进的Whisper与Qwen-Audio相结合，显著提高了低资源粤语的语音识别精度。", "motivation": "项目动机在于解决低资源粤语的自动语音识别问题，特别是涉及有限的标注数据、六个声调、声调变化和发音差异。现有模型如Whisper常常表现出较高的词错误率。", "method": "该研究引入了CantoASR框架，该框架整合了强制对齐的声学特征提取技术，对Whisper进行了LoRA微调以改进语气识别，并使用了经过指示调整的Qwen-Audio进行韵律感知校正。", "result": "研究提出了CantoASR框架，结合了强制对齐的声学特征提取、Whisper的LoRA微调和Qwen-Audio的指示调整，提高了语气和韵律的识别准确率。在粤语口语数据上的测试显示，CantoASR相较于Whisper-Large-V3有显著的字符错误率提升。这表明结合声学线索与大型语言模型推理为低资源声调和方言的语音识别提供了一个可扩展的策略。", "conclusion": "研究表明，在融合声学线索与大型语言模型推理的基础上，这项策略为低资源声调和方言的语音识别问题提供了一种可扩展解决方法。"}}
{"id": "2511.04126", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04126", "abs": "https://arxiv.org/abs/2511.04126", "authors": ["Venkata Manikanta Desu", "Syed Fawaz Ali"], "title": "Automated Tennis Player and Ball Tracking with Court Keypoints Detection (Hawk Eye System)", "comment": "14 pages, 11 figures, planning to submit for a coneference", "summary": "This study presents a complete pipeline for automated tennis match analysis.\nOur framework integrates multiple deep learning models to detect and track\nplayers and the tennis ball in real time, while also identifying court\nkeypoints for spatial reference. Using YOLOv8 for player detection, a\ncustom-trained YOLOv5 model for ball tracking, and a ResNet50-based\narchitecture for court keypoint detection, our system provides detailed\nanalytics including player movement patterns, ball speed, shot accuracy, and\nplayer reaction times. The experimental results demonstrate robust performance\nin varying court conditions and match scenarios. The model outputs an annotated\nvideo along with detailed performance metrics, enabling coaches, broadcasters,\nand players to gain actionable insights into the dynamics of the game.", "AI": {"tldr": "The paper presents a comprehensive pipeline for automated tennis match analysis using deep learning models for player detection, ball tracking, and court keypoint detection, providing valuable game analytics.", "motivation": "The motivation behind the paper is to develop a system that can automatically analyze tennis matches, providing detailed analytics such as player movement, ball speed, shot accuracy, and reaction times, to assist coaches, broadcasters, and players.", "method": "The method involves using YOLOv8 for player detection, a custom-trained YOLOv5 model for ball tracking, and a ResNet50-based architecture for detecting court keypoints to facilitate spatial references.", "result": "The experimental results indicate robust performance under various court conditions and match scenarios, with the system outputting annotated videos and detailed performance metrics.", "conclusion": "The study concludes that the full pipeline for automated tennis analysis can effectively extract meaningful data, offering beneficial insights into the game dynamics for multiple stakeholders."}}
{"id": "2511.04153", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.04153", "abs": "https://arxiv.org/abs/2511.04153", "authors": ["Fahim Ahmed", "Md Mubtasim Ahasan", "Jahir Sadik Monon", "Muntasir Wahed", "M Ashraful Amin", "A K M Mahbubur Rahman", "Amin Ahsan Ali"], "title": "BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation", "comment": null, "summary": "Text-to-SQL systems provide a natural language interface that can enable even\nlaymen to access information stored in databases. However, existing Large\nLanguage Models (LLM) struggle with SQL generation from natural instructions\ndue to large schema sizes and complex reasoning. Prior work often focuses on\ncomplex, somewhat impractical pipelines using flagship models, while smaller,\nefficient models remain overlooked. In this work, we explore three multi-agent\nLLM pipelines, with systematic performance benchmarking across a range of small\nto large open-source models: (1) Multi-agent discussion pipeline, where agents\niteratively critique and refine SQL queries, and a judge synthesizes the final\nanswer; (2) Planner-Coder pipeline, where a thinking model planner generates\nstepwise SQL generation plans and a coder synthesizes queries; and (3)\nCoder-Aggregator pipeline, where multiple coders independently generate SQL\nqueries, and a reasoning agent selects the best query. Experiments on the\nBird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small\nmodel performance, with up to 10.6% increase in Execution Accuracy for\nQwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines,\nthe LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B\nand QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest\nscore of 56.4%. Codes are available at\nhttps://github.com/treeDweller98/bappa-sql.", "AI": {"tldr": "本文通过比较和实验三种多智能体LLM管道，证明了多智能体讨论和特别的LLM推理者-编码者管道可以显著改善小型模型生成SQL查询的性能。", "motivation": "由于大型语言模型在处理自然语言生成SQL查询方面存在较大挑战，尤其是面对大型模式大小和复杂推理时，本文研究了如何通过多智能体管道改进小模型的性能。", "method": "本文探索了三种多智能体LLM管道，包括多智能体讨论管道、规划者-编码者管道和编码者-聚合者管道。多智能体讨论管道涉及智能体迭代地批评和细化SQL查询，由一个法官综合最终答案。规划者-编码者管道由一个思考模型规划者生成逐步SQL生成计划，编码者综合查询。编码者-聚合者管道涉及多个编码者独立生成SQL查询，一个推理智能体选择最佳查询。", "result": "实验表明，多智能体讨论可以提高小型模型的表现，使用Qwen2.5-7b-Instruct在三次讨论后执行准确率提高了10.6%。在所有管道中，LLM推理者-编码者管道表现最好，通过DeepSeek-R1-32B和QwQ-32B规划者将Gemma 3 27B IT的准确率从52.4%提升至56.4%。", "conclusion": "研究表明，多智能体方法，尤其是规划者-编码者管道，能够有效提升小模型生成SQL查询的准确性和性能。"}}
{"id": "2511.04128", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04128", "abs": "https://arxiv.org/abs/2511.04128", "authors": ["Shengyu Tang", "Zeyuan Lu", "Jiazhi Dong", "Changdong Yu", "Xiaoyu Wang", "Yaohui Lyu", "Weihao Xia"], "title": "DMSORT: An efficient parallel maritime multi-object tracking architecture for unmanned vessel platforms", "comment": "Updated version of the Ocean Engineering (Elsevier, 2025) paper with\n  minor corrections", "summary": "Accurate perception of the marine environment through robust multi-object\ntracking (MOT) is essential for ensuring safe vessel navigation and effective\nmaritime surveillance. However, the complicated maritime environment often\ncauses camera motion and subsequent visual degradation, posing significant\nchallenges to MOT. To address this challenge, we propose an efficient\nDual-branch Maritime SORT (DMSORT) method for maritime MOT. The core of the\nframework is a parallel tracker with affine compensation, which incorporates an\nobject detection and re-identification (ReID) branch, along with a dedicated\nbranch for dynamic camera motion estimation. Specifically, a Reversible\nColumnar Detection Network (RCDN) is integrated into the detection module to\nleverage multi-level visual features for robust object detection. Furthermore,\na lightweight Transformer-based appearance extractor (Li-TAE) is designed to\ncapture global contextual information and generate robust appearance features.\nAnother branch decouples platform-induced and target-intrinsic motion by\nconstructing a projective transformation, applying platform-motion compensation\nwithin the Kalman filter, and thereby stabilizing true object trajectories.\nFinally, a clustering-optimized feature fusion module effectively combines\nmotion and appearance cues to ensure identity consistency under noise,\nocclusion, and drift. Extensive evaluations on the Singapore Maritime Dataset\ndemonstrate that DMSORT achieves state-of-the-art performance. Notably, DMSORT\nattains the fastest runtime among existing ReID-based MOT frameworks while\nmaintaining high identity consistency and robustness to jitter and occlusion.\nCode is available at:\nhttps://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-.", "AI": {"tldr": "本文提出了一种双分支海事SORT（DMSORT）方法，通过结合鲁棒对象检测和外观特征提取以及运动补偿，实现了复杂海上环境中多目标跟踪的实时性和鲁棒性。", "motivation": "准确感知复杂的海洋环境对于确保船舶安全航行和有效的海上监视至关重要。复杂的海洋环境引起的摄像机运动和随之而来的视觉退化对多目标跟踪（MOT）构成重大挑战。本文旨在通过提出DMSORT方法解决这一挑战。", "method": "该论文提出了一种双分支海事SORT（DMSORT）方法，用于海事多目标跟踪。该方法的核心是一个带有仿射补偿的并行跟踪器，包含对象检测与重新识别分支，以及一个专门的动态摄像机运动估计分支。具体来说，检测模块中集成了一个可逆柱状检测网络（RCDN），以利用多级视觉特征进行鲁棒对象检测。此外，设计了一种轻量级Transformer外观提取器（Li-TAE），以捕获全局上下文信息并生成鲁棒的外观特征。另一个分支通过构建投影变换，拆分平台诱发的和目标固有的运动，并在卡尔曼滤波器中应用平台运动补偿，从而稳定真实目标轨迹。最后，一个聚类优化特征融合模块有效结合了运动和外观线索，以确保在噪声、遮挡和漂移情况下的身份一致性。", "result": "在新加坡海洋数据集上的广泛评估表明，DMSORT实现了最先进的性能。值得注意的是，在现有的基于ReID的MOT框架中，DMSORT具有最快运行时间，同时保持了高度的身份一致性和对抖动和遮挡的鲁棒性。", "conclusion": "该论文提出的DMSORT方法展示了在海运环境中进行多目标跟踪的有效性，它通过结合仿射补偿、轻量级Transformer外观提取器和聚类优化特征融合，实现了鲁棒的身份跟踪。DMSORT不仅在速度上领先，而且在身份保持和抗遮挡能力方面也有出色表现。"}}
{"id": "2511.04184", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04184", "abs": "https://arxiv.org/abs/2511.04184", "authors": ["Mohammed Musthafa Rafi", "Adarsh Krishnamurthy", "Aditya Balu"], "title": "Trustworthy LLM-Mediated Communication: Evaluating Information Fidelity in LLM as a Communicator (LAAC) Framework in Multiple Application Domains", "comment": "10 pages, 4 figures. Submitted to IEEE DISTILL 2025 (co-located with\n  IEEE TPS 2025)", "summary": "The proliferation of AI-generated content has created an absurd communication\ntheater where senders use LLMs to inflate simple ideas into verbose content,\nrecipients use LLMs to compress them back into summaries, and as a consequence\nneither party engage with authentic content. LAAC (LLM as a Communicator)\nproposes a paradigm shift - positioning LLMs as intelligent communication\nintermediaries that capture the sender's intent through structured dialogue and\nfacilitate genuine knowledge exchange with recipients. Rather than perpetuating\ncycles of AI-generated inflation and compression, LAAC enables authentic\ncommunication across diverse contexts including academic papers, proposals,\nprofessional emails, and cross-platform content generation. However, deploying\nLLMs as trusted communication intermediaries raises critical questions about\ninformation fidelity, consistency, and reliability. This position paper\nsystematically evaluates the trustworthiness requirements for LAAC's deployment\nacross multiple communication domains. We investigate three fundamental\ndimensions: (1) Information Capture Fidelity - accuracy of intent extraction\nduring sender interviews across different communication types, (2)\nReproducibility - consistency of structured knowledge across multiple\ninteraction instances, and (3) Query Response Integrity - reliability of\nrecipient-facing responses without hallucination, source conflation, or\nfabrication. Through controlled experiments spanning multiple LAAC use cases,\nwe assess these trust dimensions using LAAC's multi-agent architecture.\nPreliminary findings reveal measurable trust gaps that must be addressed before\nLAAC can be reliably deployed in high-stakes communication scenarios.", "AI": {"tldr": "LAAC (LLM as a Communicator) aims to solve the problem of AI-generated content exaggeration and compression by becoming an intelligent intermediary, but its deployment raises concerns about information fidelity, consistency, and reliability, which the paper evaluates.", "motivation": "The paper addresses the issue of AI-generated content leading to non-authentic communication and proposes LAAC to act as a trusted communication intermediary.", "method": "The trustworthiness of LAAC is evaluated through three dimensions: information capture fidelity, reproducibility, and query response integrity, assessed through controlled experiments.", "result": "Preliminary findings indicate measurable trust gaps, suggesting that LAAC's reliability needs improvement before it can be deployed in sensitive communication contexts.", "conclusion": "The paper concludes that while LAAC shows promise in genuine communication facilitation, there are significant trustworthiness issues to resolve before it can be fully implemented."}}
{"id": "2511.04137", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04137", "abs": "https://arxiv.org/abs/2511.04137", "authors": ["Yujian Liu", "Ze Wang", "Hao Chen", "Ximeng Sun", "Xiaodong Yu", "Jialian Wu", "Jiang Liu", "Emad Barsoum", "Zicheng Liu", "Shiyu Chang"], "title": "Learning from Online Videos at Inference Time for Computer-Use Agents", "comment": null, "summary": "Computer-use agents can operate computers and automate laborious tasks, but\ndespite recent rapid progress, they still lag behind human users, especially\nwhen tasks require domain-specific procedural knowledge about particular\napplications, platforms, and multi-step workflows. Humans can bridge this gap\nby watching video tutorials: we search, skim, and selectively imitate short\nsegments that match our current subgoal. In this paper, we study how to enable\ncomputer-use agents to learn from online videos at inference time effectively.\nWe propose a framework that retrieves and filters tutorial videos, converts\nthem into structured demonstration trajectories, and dynamically selects\ntrajectories as in-context guidance during execution. Particularly, using a\nVLM, we infer UI actions, segment videos into short subsequences of actions,\nand assign each subsequence a textual objective. At inference time, a two-stage\nselection mechanism dynamically chooses a single trajectory to add in context\nat each step, focusing the agent on the most helpful local guidance for its\nnext decision. Experiments on two widely used benchmarks show that our\nframework consistently outperforms strong base agents and variants that use\nonly textual tutorials or transcripts. Analyses highlight the importance of\ntrajectory segmentation and selection, action filtering, and visual\ninformation, suggesting that abundant online videos can be systematically\ndistilled into actionable guidance that improves computer-use agents at\ninference time. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/video_demo.", "AI": {"tldr": "本文提出了一种使计算机使用代理从在线视频中学习的方法，该方法建立了能够检索与任务相关的在线视频并将其转化为结构化演示轨迹的框架，并在实验中证明了框架的有效性。", "motivation": "人类可以通过观看视频教程，搜索、浏览和有选择地模仿与其当前子目标相匹配的短片段，从而弥补计算机操作代理的任务执行差距。本文旨在研究如何有效地使计算机使用代理从在线视频中学习。", "method": "本文提出了一个框架，该框架能够从在线视频中检索和筛选教程，将视频转化为结构化的演示轨迹，并在执行过程中动态选择轨迹作为情境指导。框架使用视觉语言模型（VLM）来推断用户界面操作，将视频分割成一组短的动作序列，并为目标任务分配相应的文本描述。在推理过程中，采用两阶段选择机制，在每一步动态选择一个轨迹添加到上下文中，从而使代理聚焦于最有助于其下一次决策的帮助信息。", "result": "实验结果表明，本文提出的框架在两个广泛使用的基准测试中始终优于强大的基础代理以及仅使用文本教程或字幕变种的代理。研究表明，轨迹分割和选择、动作筛选以及视觉信息是提高计算机使用代理性能的关键因素。", "conclusion": "分析结果表明，使用丰富的在线视频可以系统地提炼出可以转化为行动指导的信息，从而提高计算机用代理在推理时的性能。实验中充分展示该方法在两个不同基准测试中的强大性能，展现出从视频中抽取指导信息作为上下文的重要作用。"}}
{"id": "2511.04195", "categories": ["cs.CL", "cs.MA", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.04195", "abs": "https://arxiv.org/abs/2511.04195", "authors": ["Nicolò Pagan", "Petter Törnberg", "Christopher A. Bail", "Anikó Hannák", "Christopher Barrie"], "title": "Computational Turing Test Reveals Systematic Differences Between Human and AI Language", "comment": null, "summary": "Large language models (LLMs) are increasingly used in the social sciences to\nsimulate human behavior, based on the assumption that they can generate\nrealistic, human-like text. Yet this assumption remains largely untested.\nExisting validation efforts rely heavily on human-judgment-based evaluations --\ntesting whether humans can distinguish AI from human output -- despite evidence\nthat such judgments are blunt and unreliable. As a result, the field lacks\nrobust tools for assessing the realism of LLM-generated text or for calibrating\nmodels to real-world data. This paper makes two contributions. First, we\nintroduce a computational Turing test: a validation framework that integrates\naggregate metrics (BERT-based detectability and semantic similarity) with\ninterpretable linguistic features (stylistic markers and topical patterns) to\nassess how closely LLMs approximate human language within a given dataset.\nSecond, we systematically compare nine open-weight LLMs across five calibration\nstrategies -- including fine-tuning, stylistic prompting, and context retrieval\n-- benchmarking their ability to reproduce user interactions on X (formerly\nTwitter), Bluesky, and Reddit. Our findings challenge core assumptions in the\nliterature. Even after calibration, LLM outputs remain clearly distinguishable\nfrom human text, particularly in affective tone and emotional expression.\nInstruction-tuned models underperform their base counterparts, and scaling up\nmodel size does not enhance human-likeness. Crucially, we identify a trade-off:\noptimizing for human-likeness often comes at the cost of semantic fidelity, and\nvice versa. These results provide a much-needed scalable framework for\nvalidation and calibration in LLM simulations -- and offer a cautionary note\nabout their current limitations in capturing human communication.", "AI": {"tldr": "对大型语言模型（LLMs）在模拟人类语言和行为时的一系列验证和评估，发现LLMs在情感表达上与人类文本有所区别，微调并不总是提高类人性，且加强人类相似度会降低语义保真度。", "motivation": "现有的验证努力很大程度上依赖于基于人类判断的评估，这些判断被认为是粗糙和不可靠的。因此，领域缺乏评估LLM生成文本逼真度的工具或校准模型到现实数据的方法。", "method": "介绍了一种计算图灵测试：一种验证框架，整合了基于BERT的可检测性、语义相似性的集合指标与可解释的语言特征（如风格标记和主题模式），以评估LLMs在给定数据集中近似人类语言的程度。并且系统地比较了九个开源权重的大型语言模型在五种校准策略（包括微调、风格提示、以及上下文检索）下的表现，评估它们在X（原Twitter）、Bluesky和Reddit上模拟用户互动的能力。", "result": "即使经过校准，LLM输出依然在情感表达方面明显区别于人类文本。指令调优模型的表现不如其基础版本，增加模型大小并不会提高其类人性。研究表明，优化人类相似度往往以牺牲语义保真度为代价，反之亦然。", "conclusion": "这些结果提供了一种必要的可扩展验证和校准框架，用于LLM模拟，并提出了当前模型在捕捉人类交流方面的局限性的警告。"}}
{"id": "2511.04161", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04161", "abs": "https://arxiv.org/abs/2511.04161", "authors": ["Suranjan Goswami", "Abhinav Ravi", "Raja Kolla", "Ali Faraz", "Shaharukh Khan", "Akash", "Chandra Khatri", "Shubham Agarwal"], "title": "Seeing Straight: Document Orientation Detection for Efficient OCR", "comment": null, "summary": "Despite significant advances in document understanding, determining the\ncorrect orientation of scanned or photographed documents remains a critical\npre-processing step in the real world settings. Accurate rotation correction is\nessential for enhancing the performance of downstream tasks such as Optical\nCharacter Recognition (OCR) where misalignment commonly arises due to user\nerrors, particularly incorrect base orientations of the camera during capture.\nIn this study, we first introduce OCR-Rotation-Bench (ORB), a new benchmark for\nevaluating OCR robustness to image rotations, comprising (i) ORB-En, built from\nrotation-transformed structured and free-form English OCR datasets, and (ii)\nORB-Indic, a novel multilingual set spanning 11 Indic mid to low-resource\nlanguages. We also present a fast, robust and lightweight rotation\nclassification pipeline built on the vision encoder of Phi-3.5-Vision model\nwith dynamic image cropping, fine-tuned specifically for 4-class rotation task\nin a standalone fashion. Our method achieves near-perfect 96% and 92% accuracy\non identifying the rotations respectively on both the datasets. Beyond\nclassification, we demonstrate the critical role of our module in boosting OCR\nperformance: closed-source (up to 14%) and open-weights models (up to 4x) in\nthe simulated real-world setting.", "AI": {"tldr": "This paper introduces a new benchmark ORB for evaluating OCR robustness to image rotations and presents a high-performing rotation classification pipeline that increases OCR accuracy by correcting document orientation errors.", "motivation": "The motivation behind this study is to address the persistent challenge of accurately correcting the orientation of scanned or photographed documents, which significantly impacts the performance of downstream tasks, specifically Optical Character Recognition (OCR).", "method": "The paper introduces a fast, robust and lightweight rotation classification pipeline built upon the vision encoder of Phi-3.5-Vision model with dynamic image cropping, fine-tuned for a 4-class rotation task.", "result": "The method achieves 96% and 92% accuracy in identifying rotations on ORB-En and ORB-Indic datasets respectively, improving OCR performance across different models.", "conclusion": "The paper concludes by highlighting the importance of accurate rotation correction in enhancing OCR performance in real-world scenarios."}}
{"id": "2511.04205", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04205", "abs": "https://arxiv.org/abs/2511.04205", "authors": ["Michał Karp", "Anna Kubaszewska", "Magdalena Król", "Robert Król", "Aleksander Smywiński-Pohl", "Mateusz Szymański", "Witold Wydmański"], "title": "LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal", "comment": null, "summary": "This study provides an empirical assessment of whether current large language\nmodels (LLMs) can pass the official qualifying examination for membership in\nPoland's National Appeal Chamber (Krajowa Izba Odwo{\\l}awcza). The authors\nexamine two related ideas: using LLM as actual exam candidates and applying the\n'LLM-as-a-judge' approach, in which model-generated answers are automatically\nevaluated by other models. The paper describes the structure of the exam, which\nincludes a multiple-choice knowledge test on public procurement law and a\nwritten judgment, and presents the hybrid information recovery and extraction\npipeline built to support the models. Several LLMs (including GPT-4.1, Claude 4\nSonnet and Bielik-11B-v2.6) were tested in closed-book and various\nRetrieval-Augmented Generation settings. The results show that although the\nmodels achieved satisfactory scores in the knowledge test, none met the passing\nthreshold in the practical written part, and the evaluations of the\n'LLM-as-a-judge' often diverged from the judgments of the official examining\ncommittee. The authors highlight key limitations: susceptibility to\nhallucinations, incorrect citation of legal provisions, weaknesses in logical\nargumentation, and the need for close collaboration between legal experts and\ntechnical teams. The findings indicate that, despite rapid technological\nprogress, current LLMs cannot yet replace human judges or independent examiners\nin Polish public procurement adjudication.", "AI": {"tldr": "研究表明，尽管技术进步迅速，但现有的大型语言模型（LLMs）在波兰公共采购裁决中还不能替代人类法官或独立考官。", "motivation": "研究动机在于评估当前的大型语言模型（LLMs）是否能够通过波兰国家上诉庭（Krajowa Izba Odwoławcza）的官方资格考试。", "method": "该研究讨论了使用大型语言模型（LLMs）作为波兰国家上诉庭正式资格考试的实际考生，并探讨了“LLM作为法官”的方法，该方法中模型生成的答案将由其他模型自动评估。研究测试了包括GPT-4.1、Claude 4 Sonnet和Bielik-11B-v2.6在内的多个LLM在闭卷和多种检索增强生成设置下的表现。", "result": "研究发现，尽管模型在知识测试中取得了令人满意的成绩，但在实际文字部分均未达到及格线，且“LLM作为法官”的评估结果经常与官方审查委员会的判断不符。主要的限制因素包括生成不实信息、不正确的法律条款引用、逻辑论证不足，以及需要法律专家和技术团队间的紧密合作。", "conclusion": "该研究的结论是，因技术限制如不实信息生成、逻辑论证缺失等问题，当前的LLMs在波兰公共采购裁决中还不能替代人类法官或独立考官。"}}
{"id": "2511.04171", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04171", "abs": "https://arxiv.org/abs/2511.04171", "authors": ["Fatemehzahra Darzi", "Rodrigo Escobar Diaz Guerrero", "Thomas Bocklitz"], "title": "Systematic Evaluation of Preprocessing Techniques for Accurate Image Registration in Digital Pathology", "comment": "14 pages, 7 Figures", "summary": "Image registration refers to the process of spatially aligning two or more\nimages by mapping them into a common coordinate system, so that corresponding\nanatomical or tissue structures are matched across images. In digital\npathology, registration enables direct comparison and integration of\ninformation from different stains or imaging modalities, sup-porting\napplications such as biomarker analysis and tissue reconstruction. Accurate\nregistration of images from different modalities is an essential step in\ndigital pathology. In this study, we investigated how various color\ntransformation techniques affect image registration between hematoxylin and\neosin (H&E) stained images and non-linear multimodal images. We used a dataset\nof 20 tissue sample pairs, with each pair undergoing several preprocessing\nsteps, including different color transformation (CycleGAN, Macenko, Reinhard,\nVahadane), inversion, contrast adjustment, intensity normalization, and\ndenoising. All images were registered using the VALIS registration method,\nwhich first applies rigid registration and then performs non-rigid registration\nin two steps on both low and high-resolution images. Registration performance\nwas evaluated using the relative Target Registration Error (rTRE). We reported\nthe median of median rTRE values (MMrTRE) and the average of median rTRE values\n(AMrTRE) for each method. In addition, we performed a custom point-based\nevaluation using ten manually selected key points. Registration was done\nseparately for two scenarios, using either the original or inverted multimodal\nimages. In both scenarios, CycleGAN color transformation achieved the lowest\nregistration errors, while the other methods showed higher errors. These\nfindings show that applying color transformation before registration improves\nalignment between images from different modalities and supports more reliable\nanalysis in digital pathology.", "AI": {"tldr": "The paper explores how different color transformation techniques affect image registration accuracy between hematoxylin and eosin (H&E) stained images and non-linear multimodal images in digital pathology.", "motivation": "The motivation is to improve the integration and direct comparison of information from different stains or imaging modalities in digital pathology, which is essential for applications such as biomarker analysis and tissue reconstruction.", "method": "Structure", "result": "CycleGAN color transformation achieved the lowest registration errors after applying various color transformations and preprocessing steps to a dataset of 20 tissue sample pairs.", "conclusion": "Applying color transformation before registration improves alignment between images from different modalities, supporting more reliable analysis in digital pathology."}}
{"id": "2511.04228", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6; K.4.1"], "pdf": "https://arxiv.org/pdf/2511.04228", "abs": "https://arxiv.org/abs/2511.04228", "authors": ["Liran Cohen", "Yaniv Nemcovesky", "Avi Mendelson"], "title": "REMIND: Input Loss Landscapes Reveal Residual Memorization in Post-Unlearning LLMs", "comment": "Pre-print version under review", "summary": "Machine unlearning aims to remove the influence of specific training data\nfrom a model without requiring full retraining. This capability is crucial for\nensuring privacy, safety, and regulatory compliance. Therefore, verifying\nwhether a model has truly forgotten target data is essential for maintaining\nreliability and trustworthiness. However, existing evaluation methods often\nassess forgetting at the level of individual inputs. This approach may overlook\nresidual influence present in semantically similar examples. Such influence can\ncompromise privacy and lead to indirect information leakage. We propose REMIND\n(Residual Memorization In Neighborhood Dynamics), a novel evaluation method\naiming to detect the subtle remaining influence of unlearned data and classify\nwhether the data has been effectively forgotten. REMIND analyzes the model's\nloss over small input variations and reveals patterns unnoticed by single-point\nevaluations. We show that unlearned data yield flatter, less steep loss\nlandscapes, while retained or unrelated data exhibit sharper, more volatile\npatterns. REMIND requires only query-based access, outperforms existing methods\nunder similar constraints, and demonstrates robustness across different models,\ndatasets, and paraphrased inputs, making it practical for real-world\ndeployment. By providing a more sensitive and interpretable measure of\nunlearning effectiveness, REMIND provides a reliable framework to assess\nunlearning in language models. As a result, REMIND offers a novel perspective\non memorization and unlearning.", "AI": {"tldr": "The authors of the paper introduce REMIND as a novel evaluation method for assessing machine unlearning, which evaluates models by analyzing the loss landscape around input variations, thereby detecting residual influences of unlearned data that traditional methods might miss.", "motivation": "The motivation is to address the limitations of existing unlearning evaluation methods which assess forgetting at the level of individual inputs, potentially overlooking residual influence in semantically similar examples, which can compromise privacy.", "method": "The paper proposes REMIND, an evaluation method to detect subtle remaining influence of unlearned data and classify if the data has been effectively forgotten by analyzing the model's loss over small input variations.", "result": "REMIND is shown to outperform existing methods under similar constraints by delivering more sensitive and interpretable measures of unlearning effectiveness and demonstrating robustness across different models, datasets, and paraphrased inputs.", "conclusion": "REMIND provides a reliable framework to assess unlearning in language models by offering a sensitive measure and a new perspective on memorization and unlearning."}}
{"id": "2511.04190", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04190", "abs": "https://arxiv.org/abs/2511.04190", "authors": ["Josef Mayr", "Anna Reithmeir", "Maxime Di Folco", "Julia A. Schnabel"], "title": "Covariance Descriptors Meet General Vision Encoders: Riemannian Deep Learning for Medical Image Classification", "comment": "Preprint. Submitted to the IEEE International Symposium on Biomedical\n  Imaging (ISBI) 2026", "summary": "Covariance descriptors capture second-order statistics of image features.\nThey have shown strong performance in general computer vision tasks, but remain\nunderexplored in medical imaging. We investigate their effectiveness for both\nconventional and learning-based medical image classification, with a particular\nfocus on SPDNet, a classification network specifically designed for symmetric\npositive definite (SPD) matrices. We propose constructing covariance\ndescriptors from features extracted by pre-trained general vision encoders\n(GVEs) and comparing them with handcrafted descriptors. Two GVEs - DINOv2 and\nMedSAM - are evaluated across eleven binary and multi-class datasets from the\nMedMNSIT benchmark. Our results show that covariance descriptors derived from\nGVE features consistently outperform those derived from handcrafted features.\nMoreover, SPDNet yields superior performance to state-of-the-art methods when\ncombined with DINOv2 features. Our findings highlight the potential of\ncombining covariance descriptors with powerful pretrained vision encoders for\nmedical image analysis.", "AI": {"tldr": "研究发现协方差描述符在医学图像分类中效果显著，特别是当与预训练视觉编码器结合使用时。实验展示了这种组合在多个医学图像数据集上的优越性能。", "motivation": "协方差描述符在一般计算机视觉任务中展示了强大的性能，但在医学成像中仍然未被广泛研究。本研究旨在探讨协方差描述符在医学图像分类中的效果，特别是在对称正定（SPD）矩阵分类网络中的应用。", "method": "我们研究了协方差描述符在医学图像分类中的有效性，对比了由预训练的一般视觉编码器（GVEs）提取的特征构造的协方差描述符与手工特征构造的协方差描述符。采用了两个GVEs（DINOv2和MedSAM），并在MedMNSIT基准的十一个二分类和多分类数据集上进行评估。", "result": "实验结果显示，由GVE特征构造的协方差描述符在所有数据集中都优于手工特征构造的协方差描述符，并且当与DINOv2特征结合使用时，SPDNet的表现超过了现有的最先进方法。", "conclusion": "研究结果表明，结合协方差描述符与强大的预训练视觉编码器在医学图像分析中具有巨大潜力。"}}
{"id": "2511.04234", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04234", "abs": "https://arxiv.org/abs/2511.04234", "authors": ["Alex Fang", "Thomas Voice", "Ruoming Pang", "Ludwig Schmidt", "Tom Gunter"], "title": "Reusing Pre-Training Data at Test Time is a Compute Multiplier", "comment": null, "summary": "Large language models learn from their vast pre-training corpora, gaining the\nability to solve an ever increasing variety of tasks; yet although researchers\nwork to improve these datasets, there is little effort to understand how\nefficient the pre-training apparatus is at extracting ideas and knowledge from\nthe data. In this work, we use retrieval augmented generation along with\ntest-time compute as a way to quantify how much dataset value was left behind\nby the process of pre-training, and how this changes across scale. We\ndemonstrate that pre-training then retrieving from standard and largely\nopen-sourced datasets results in significant accuracy gains in MMLU, Math-500,\nand SimpleQA, which persist through decontamination. For MMLU we observe that\nretrieval acts as a ~5x compute multiplier versus pre-training alone. We show\nthat these results can be further improved by leveraging additional compute at\ntest time to parse the retrieved context, demonstrating a 10 percentage point\nimprovement on MMLU for the public LLaMA 3.1 8B model. Overall, our results\nsuggest that today's pre-training methods do not make full use of the\ninformation in existing pre-training datasets, leaving significant room for\nprogress.", "AI": {"tldr": "研究表明当前的预训练方法并未充分利用现有的预训练数据集中的信息，还有很大的进步空间。", "motivation": "尽管研究人员努力改进这些数据集，但对于预训练装置从数据中提取想法和知识的效率的研究很少。", "method": "使用检索增强生成及测试时计算量来量化预训练过程从数据中提取想法和知识的效率，并探讨这一效率如何随规模变化。", "result": "实验结果显示，从标准和大多数开源数据集中进行预训练然后检索，可以在MMLU、Math-500和SimpleQA上获得显著的准确性提升，并且这些提升在去污染后仍然存在。", "conclusion": "结果表明，与仅使用预训练相比，检索行为在MMLU上相当于~5倍的计算增益。通过在测试时利用额外的计算量来解析检索到的上下文，可以进一步改善这些结果，例如对于公开的LLaMA 3.1 8B模型在MMLU上可以提高10个百分点。总体来说，预训练方法没有充分利用现有预训练数据集中的信息，这为未来研究留下了很大的提升空间。"}}
{"id": "2511.04192", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04192", "abs": "https://arxiv.org/abs/2511.04192", "authors": ["Hanmo Chen", "Chenghao Xu", "Jiexi Yan", "Cheng Deng"], "title": "AStF: Motion Style Transfer via Adaptive Statistics Fusor", "comment": null, "summary": "Human motion style transfer allows characters to appear less rigidity and\nmore realism with specific style. Traditional arbitrary image style transfer\ntypically process mean and variance which is proved effective. Meanwhile,\nsimilar methods have been adapted for motion style transfer. However, due to\nthe fundamental differences between images and motion, relying on mean and\nvariance is insufficient to fully capture the complex dynamic patterns and\nspatiotemporal coherence properties of motion data. Building upon this, our key\ninsight is to bring two more coefficient, skewness and kurtosis, into the\nanalysis of motion style. Specifically, we propose a novel Adaptive Statistics\nFusor (AStF) which consists of Style Disentanglement Module (SDM) and\nHigh-Order Multi-Statistics Attention (HOS-Attn). We trained our AStF in\nconjunction with a Motion Consistency Regularization (MCR) discriminator.\nExperimental results show that, by providing a more comprehensive model of the\nspatiotemporal statistical patterns inherent in dynamic styles, our proposed\nAStF shows proficiency superiority in motion style transfers over\nstate-of-the-arts. Our code and model are available at\nhttps://github.com/CHMimilanlan/AStF.", "AI": {"tldr": "本文提出自适应统计融合器（AStF），改进传统运动风格转换方法，引入偏度和峰度参数，实现了更优的运动风格转换效果。", "motivation": "传统任意图像风格转换通常处理均值和方差，这一方法已被证明在运动风格转换中是不够的。因此，本文引入了偏度和峰度两个更多参数来分析运动风格。", "method": "本论文提出了一种新的自适应统计融合器（AStF），它由风格分离模块（SDM）和高阶多统计注意（HOS-Attn）组成。AStF在训练时结合了运动一致性正则化（MCR）鉴别器。", "result": "实验结果表明，AStF通过提供更全面的时空统计模式模型，在运动风格转换中优于现有的其他方法。", "conclusion": "此论文创新性地提出AStF，它能更好地捕获运动数据的复杂动态模式和时空连贯性特性，极大地提升了运动风格转换的效果。"}}
{"id": "2511.04248", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04248", "abs": "https://arxiv.org/abs/2511.04248", "authors": ["Salma Mekaoui", "Hiba Sofyan", "Imane Amaaz", "Imane Benchrif", "Arsalane Zarghili", "Ilham Chaker", "Nikola S. Nikolov"], "title": "Efficient Topic Extraction via Graph-Based Labeling: A Lightweight Alternative to Deep Models", "comment": null, "summary": "Extracting topics from text has become an essential task, especially with the\nrapid growth of unstructured textual data. Most existing works rely on highly\ncomputational methods to address this challenge. In this paper, we argue that\nprobabilistic and statistical approaches, such as topic modeling (TM), can\noffer effective alternatives that require fewer computational resources. TM is\na statistical method that automatically discovers topics in large collections\nof unlabeled text; however, it produces topics as distributions of\nrepresentative words, which often lack clear interpretability. Our objective is\nto perform topic labeling by assigning meaningful labels to these sets of\nwords. To achieve this without relying on computationally expensive models, we\npropose a graph-based approach that not only enriches topic words with\nsemantically related terms but also explores the relationships among them. By\nanalyzing these connections within the graph, we derive suitable labels that\naccurately capture each topic's meaning. We present a comparative study between\nour proposed method and several benchmarks, including ChatGPT-3.5, across two\ndifferent datasets. Our method achieved consistently better results than\ntraditional benchmarks in terms of BERTScore and cosine similarity and produced\nresults comparable to ChatGPT-3.5, while remaining computationally efficient.\nFinally, we discuss future directions for topic labeling and highlight\npotential research avenues for enhancing interpretability and automation.", "AI": {"tldr": "The paper proposes a graph-based method for topic labeling that is computationally efficient and effective, surpassing traditional benchmarks and matching results with ChatGPT-3.5.", "motivation": "The motivation is to offer an effective and less computationally demanding alternative to existing topic extraction methods. By proposing a graph-based approach, we aim to achieve better interpretability without relying on complex models.", "method": "Our method involves using a graph-based approach to enrich and label topics derived from topic modeling. This method focuses on finding semantically related terms within topics and analyzing their relationships to derive meaningful labels.", "result": "The proposed method achieved better results than traditional benchmarks as measured by BERTScore and cosine similarity. It also produced results comparable to those generated by ChatGPT-3.5, while being more computationally efficient.", "conclusion": "The paper concludes that the proposed graph-based topic labeling method is effective for improving topic interpretability while remaining computationally efficient. It suggests future research directions for enhancing topic labeling interpretability and automation."}}
{"id": "2511.04255", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04255", "abs": "https://arxiv.org/abs/2511.04255", "authors": ["Marawan Elbatel", "Anbang Wang", "Keyuan Liu", "Kaouther Mouheb", "Enrique Almar-Munoz", "Lizhuo Lin", "Yanqi Yang", "Karim Lekadir", "Xiaomeng Li"], "title": "MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection", "comment": null, "summary": "This paper does not introduce a novel architecture; instead, it revisits a\nfundamental yet overlooked baseline: adapting human-centric foundation models\nfor anatomical landmark detection in medical imaging. While landmark detection\nhas traditionally relied on domain-specific models, the emergence of\nlarge-scale pre-trained vision models presents new opportunities. In this\nstudy, we investigate the adaptation of Sapiens, a human-centric foundation\nmodel designed for pose estimation, to medical imaging through multi-dataset\npretraining, establishing a new state of the art across multiple datasets. Our\nproposed model, MedSapiens, demonstrates that human-centric foundation models,\ninherently optimized for spatial pose localization, provide strong priors for\nanatomical landmark detection, yet this potential has remained largely\nuntapped. We benchmark MedSapiens against existing state-of-the-art models,\nachieving up to 5.26% improvement over generalist models and up to 21.81%\nimprovement over specialist models in the average success detection rate (SDR).\nTo further assess MedSapiens adaptability to novel downstream tasks with few\nannotations, we evaluate its performance in limited-data settings, achieving\n2.69% improvement over the few-shot state of the art in SDR. Code and model\nweights are available at https://github.com/xmed-lab/MedSapiens .", "AI": {"tldr": "研究通过多数据集预训练的方法，将人体为核心基础模型Sapiens适应到医疗影像，创建了MedSapiens，超越了多个现有先进模型的性能。", "motivation": "尽管关键点检测传统上依赖于领域特定的模型，大型预训练视觉模型的出现为关键点检测提供了新的机会，尤其是将人类为中心的基础模型应用于解剖关键点检测的潜力一直未被充分挖掘。", "method": "此研究通过多数据集预训练的方法，将设计用于姿态估计的Sapiens模型适应到医学影像中，形成了一个新的模型：MedSapiens。", "result": "通过对现有最先进模型的基准测试，MedSapiens展示了最高5.26%的平均成功率检测率(SDR)的提升，相比于泛化模型，最高21.81%的提升相比于专科模型。同时，在低数据设置中，它在成功检测率(SDR)方面比少样本状态的最先进方法提高了2.69%。", "conclusion": "MedSapiens的实验结果表明，人类为中心的基础模型，优化的空间姿态定位，为解剖关键点检测提供了强大的先验知识，并展现出在低数据环境下的良好适应性。"}}
{"id": "2511.04256", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04256", "abs": "https://arxiv.org/abs/2511.04256", "authors": ["Kun Yang", "Zikang chen", "Yanmeng Wang", "Zhigen Li"], "title": "SSPO: Subsentence-level Policy Optimization", "comment": null, "summary": "As a significant part of post-training of the Large Language Models (LLMs),\nReinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs'\nreasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative\nPolicy Optimization) and GSPO (Group Sequence Policy Optimization), are\nobserved to suffer from unstable policy updates and low usage of sampling data,\nrespectively. The importance ratio of GRPO is calculated at the token level,\nwhich focuses more on optimizing a single token. This will be easily affected\nby outliers, leading to model training collapse. GSPO proposed the calculation\nof the response level importance ratio, which solves the problem of high\nvariance and training noise accumulation in the calculation of the GRPO\nimportance ratio. However, since all the response tokens share a common\nimportance ratio, extreme values can easily raise or lower the overall mean,\nleading to the entire response being mistakenly discarded, resulting in a\ndecrease in the utilization of sampled data. This paper introduces SSPO, which\napplies sentence-level importance ratio, taking the balance between GRPO and\nGSPO. SSPO not only avoids training collapse and high variance, but also\nprevents the whole response tokens from being abandoned by the clipping\nmechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily\nadjust the clipping bounds, encouraging high-entropy tokens to explore and\nnarrow the clipping range of low-entropy tokens. In particular, SSPO achieves\nan average score of 46.57 across five datasets, surpassing GRPO (43.01) and\nGSPO (44.42), and wins state-of-the-art performance on three datasets. These\nresults highlight SSPO's effectiveness in leveraging generated data by taking\nthe essence of GSPO but rejecting its shortcomings.", "AI": {"tldr": "This paper introduces SSPO, which applies sentence-level importance ratio to avoid the instability of GRPO and GSPO and achieves superior results on multiple datasets.", "motivation": "The motivation of the paper is to address the issues of instability and inefficiency in using sampling data when training LLMs with RLVR algorithms such as GRPO and GSPO.", "method": "The method SSPO uses sentence-level importance ratio to balance the drawbacks of GRPO and GSPO, and applies sentence entropy to PPO-CLIP for clipping bounds adjustment.", "result": "SSPO achieves an average score of 46.57 across five datasets and surpasses both GRPO and GSPO, demonstrating its effectiveness.", "conclusion": "SSPO effectively improves the stability and usage of sampling data in training LLMs, achieving state-of-the-art performance on multiple datasets."}}
{"id": "2511.04260", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04260", "abs": "https://arxiv.org/abs/2511.04260", "authors": ["Claudio Giusti", "Luca Guarnera", "Sebastiano Battiato"], "title": "Proto-LeakNet: Towards Signal-Leak Aware Attribution in Synthetic Human Face Imagery", "comment": "13 pages, 6 figures, 5 tables", "summary": "The growing sophistication of synthetic image and deepfake generation models\nhas turned source attribution and authenticity verification into a critical\nchallenge for modern computer vision systems. Recent studies suggest that\ndiffusion pipelines unintentionally imprint persistent statistical traces,\nknown as signal leaks, within their outputs, particularly in latent\nrepresentations. Building on this observation, we propose Proto-LeakNet, a\nsignal-leak-aware and interpretable attribution framework that integrates\nclosed-set classification with a density-based open-set evaluation on the\nlearned embeddings, enabling analysis of unseen generators without retraining.\nOperating in the latent domain of diffusion models, our method re-simulates\npartial forward diffusion to expose residual generator-specific cues. A\ntemporal attention encoder aggregates multi-step latent features, while a\nfeature-weighted prototype head structures the embedding space and enables\ntransparent attribution. Trained solely on closed data and achieving a Macro\nAUC of 98.13%, Proto-LeakNet learns a latent geometry that remains robust under\npost-processing, surpassing state-of-the-art methods, and achieves strong\nseparability between known and unseen generators. These results demonstrate\nthat modeling signal-leak bias in latent space enables reliable and\ninterpretable AI-image and deepfake forensics. The code for the whole work will\nbe available upon submission.", "AI": {"tldr": "本文提出的一个名为Proto-LeakNet框架，在潜在空间中通过信号泄露识别生成器特性，无需重新训练数据即可分析未见过的生成器，达到高精度和鲁棒性。", "motivation": "自动生成和深度伪造模型的进步导致了现代计算机视觉系统中源归属和真实性验证面临的挑战。本研究的动机在于通过识别这些模型的'信号泄露'来创建一种更加可靠和透明的图像与深度伪造鉴别方法。", "method": "Proto-LeakNet使用了一个时间注意力编码器来聚合多步潜在特征，同时使用了一个特征加权原型头来组织嵌入空间，从而实现透明归因。这种方法依赖于识别扩散模型输出中保留的生成器特异性线索。", "result": "最近的研究表明，扩散模型在生成图像时会无意中留下持久的统计痕迹，称为信号泄露。本文提出了一种名为Proto-LeakNet的新框架，该框架利用信号泄露和解释性归因机制，通过在扩散模型的潜在域中重新模拟部分扩散过程来识别生成器特有的残差线索。这种方法结合了闭集分类和基于密度的开集评估，能够分析未见过的生成器，而无需重新训练。实验结果表明，Proto-LeakNet在闭集数据上的Macro AUC达到了98.13%，并且不因后处理影响其鲁棒性，超越了目前最先进的方法。这表明在潜在空间中建模信号泄露偏见可以实现可靠的AI图像和深度伪造分析。", "conclusion": "实验结果表明，Proto-LeakNet在潜在空间中建模信号泄露偏见具备高精度和鲁棒性，尤其在分析未见过的生成器时能超越现有方法，实现对AI图像和深度伪造的可靠分析。此研究为未来进一步研究指明了方向。"}}
{"id": "2511.04406", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04406", "abs": "https://arxiv.org/abs/2511.04406", "authors": ["Mohammad Amin Ghanizadeh", "Mohammad Javad Dousti"], "title": "Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning", "comment": null, "summary": "Data quality and its effective selection are fundamental to improving the\nperformance of machine translation models, serving as cornerstones for\nachieving robust and reliable translation systems. This paper presents a data\nselection methodology specifically designed for fine-tuning machine translation\nsystems, which leverages the synergy between a learner model and a pre-trained\nreference model to enhance overall training effectiveness. By defining a\nlearnability score, our approach systematically evaluates the utility of data\npoints for training, ensuring that only the most relevant and impactful\nexamples contribute to the fine-tuning process. Furthermore, our method employs\na batch selection strategy which considers interdependencies among data points,\noptimizing the efficiency of the training process while maintaining a focus on\ndata relevance. Experiments on English to Persian and several other language\npairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that\nour method can achieve up to a fivefold improvement in data efficiency compared\nto an iid baseline. Experimental results indicate that our approach improves\ncomputational efficiency by 24 when utilizing cached embeddings, as it requires\nfewer training data points. Additionally, it enhances generalization, resulting\nin superior translation performance compared to random selection method.", "AI": {"tldr": "This paper proposes a data selection methodology that enhances the training effectiveness of machine translation models, leading to improved translation performance and computational efficiency.", "motivation": "The motivation is to improve the performance of machine translation models by effectively selecting high-quality training data, leading to more robust and accurate translation systems.", "method": "Our method evaluates the utility of data points for training in machine translation models using a learnability score. It employs a batch selection strategy considering interdependencies among data points, aiming to enhance training efficiency and effectiveness.", "result": "Experiments show that our method can improve data efficiency up to fivefold compared to a baseline method, and enhance computational efficiency by 24 by utilizing cached embeddings.", "conclusion": "The approach not only achieves superior translation performance compared to random selection but also enhances the generalization of the model, making it a valuable method for fine-tuning machine translation systems."}}
{"id": "2511.04281", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04281", "abs": "https://arxiv.org/abs/2511.04281", "authors": ["Yujie Yang", "Shuang Li", "Jun Ye", "Neng Dong", "Fan Li", "Huafeng Li"], "title": "DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification", "comment": null, "summary": "Video-based Visible-Infrared person re-identification (VVI-ReID) aims to\nretrieve the same pedestrian across visible and infrared modalities from video\nsequences. Existing methods tend to exploit modality-invariant visual features\nbut largely overlook gait features, which are not only modality-invariant but\nalso rich in temporal dynamics, thus limiting their ability to model the\nspatiotemporal consistency essential for cross-modal video matching. To address\nthese challenges, we propose a DINOv2-Driven Gait Representation Learning\n(DinoGRL) framework that leverages the rich visual priors of DINOv2 to learn\ngait features complementary to appearance cues, facilitating robust\nsequence-level representations for cross-modal retrieval. Specifically, we\nintroduce a Semantic-Aware Silhouette and Gait Learning (SASGL) model, which\ngenerates and enhances silhouette representations with general-purpose semantic\npriors from DINOv2 and jointly optimizes them with the ReID objective to\nachieve semantically enriched and task-adaptive gait feature learning.\nFurthermore, we develop a Progressive Bidirectional Multi-Granularity\nEnhancement (PBMGE) module, which progressively refines feature representations\nby enabling bidirectional interactions between gait and appearance streams\nacross multiple spatial granularities, fully leveraging their complementarity\nto enhance global representations with rich local details and produce highly\ndiscriminative features. Extensive experiments on HITSZ-VCM and BUPT datasets\ndemonstrate the superiority of our approach, significantly outperforming\nexisting state-of-the-art methods.", "AI": {"tldr": "提出了DinoGRL框架，利用DINOv2学习步伐特征，引入SASGL和PBMGE以增强步伐和外观流的特征表示，从而提升跨模态视频匹配的表现。", "motivation": "现有方法倾向于利用模态不变的视觉特征，而忽略了模态不变且具有丰富时间动态的步伐特征，这限制了其建模跨模态视频匹配所需的时空一致性的能力。步伐特征不仅模态不变，还具有丰富的时空动态，因此有必要对其加以利用来提升性能。", "method": "通过提出名为DinoGRL的框架，该框架利用DINOv2的丰富视觉先验来学习与外观特征互补的步伐特征，从而实现健壮的跨模态检索。具体而言，引入了语义感知轮廓和步伐学习（SASGL）模型，利用DINOv2的通用语义先验生成和增强轮廓表示，并与ReID目标联合优化，实现语义丰富且任务适应的步伐特征学习。此外，开发了渐进双向多粒度增强（PBMGE）模块，通过使步伐和外观流在多个空间粒度上进行双向交互，逐步优化特征表示，充分利用它们的互补性来提升全局表示的丰富局部细节，并产生高度判别性特征。", "result": "实验结果在HITSZ-VCM和BUPT数据集上显示，该方法显著优于现有最先进的方法。", "conclusion": "该方法通过同步学习步伐特征和支持模态不变的视觉特征，显著提高了跨视频模态行人重新识别任务的性能。"}}
{"id": "2511.04432", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04432", "abs": "https://arxiv.org/abs/2511.04432", "authors": ["Lars Bungum", "Charles Yijia Huang", "Abeer Kashar"], "title": "If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs", "comment": "8 pages, 1 figure, 3 tables, submitted to aconference", "summary": "In this study, we experiment with the ability of LLMs to do temporal\nreasoning. Using a Norwegian book from 1940 containing trivia questions, we\nprompt the LLMs to answer the questions as if it were 1940. We also pose the\nquestions in both English and Norwegian. Correct answers are often presented as\nsentences, and grading is done by means of LLM-as-judge, with sampled checks by\na native speaker. Prompting in English consistently gave better results than in\nNorwegian, an unexpected result. In contrast, using larger LLMs improved\nresults. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families,\nand also the largest available LLM especially crafted for Norwegian.", "AI": {"tldr": "本研究发现 LLM 使用英语答题比挪威语好，而使用更大规模的模型能提高答题结果。", "motivation": "研究旨在探索 LLM 在时序推理方面的潜力，尤其是在特定历史背景下的知识应用。", "method": "该研究使用了一本1940年的挪威 trivia 问题书籍作为数据源，让 LLM 以1940年的视角回答这些问题，并测试了英语和挪威语两种语言下的表现。评估结果通过另一种 LLM 进行评分，并由挪威语母语者进行抽样检查。", "result": "结果显示，使用英语提问比挪威语提问能得到更好的结果，这是一个意外发现。同时，使用更大规模的 LLM 能够改善结果。", "conclusion": "该研究结论强调了语言差异和模型大小对 LLM 在特定任务中性能的影响，展示了 LLM 在复杂推理任务上的潜力。"}}
{"id": "2511.04283", "categories": ["cs.CV", "68T40(Primary)68T45, 68U99 (Secondary)", "I.4.8; I.3.7"], "pdf": "https://arxiv.org/pdf/2511.04283", "abs": "https://arxiv.org/abs/2511.04283", "authors": ["Shiwei Ren", "Tianci Wen", "Yongchun Fang", "Biao Lu"], "title": "FastGS: Training 3D Gaussian Splatting in 100 Seconds", "comment": "Project page: https://fastgs.github.io/", "summary": "The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to\nproperly regulate the number of Gaussians during training, causing redundant\ncomputational time overhead. In this paper, we propose FastGS, a novel, simple,\nand general acceleration framework that fully considers the importance of each\nGaussian based on multi-view consistency, efficiently solving the trade-off\nbetween training time and rendering quality. We innovatively design a\ndensification and pruning strategy based on multi-view consistency, dispensing\nwith the budgeting mechanism. Extensive experiments on Mip-NeRF 360, Tanks &\nTemples, and Deep Blending datasets demonstrate that our method significantly\noutperforms the state-of-the-art methods in training speed, achieving a\n3.32$\\times$ training acceleration and comparable rendering quality compared\nwith DashGaussian on the Mip-NeRF 360 dataset and a 15.45$\\times$ acceleration\ncompared with vanilla 3DGS on the Deep Blending dataset. We demonstrate that\nFastGS exhibits strong generality, delivering 2-7$\\times$ training acceleration\nacross various tasks, including dynamic scene reconstruction, surface\nreconstruction, sparse-view reconstruction, large-scale reconstruction, and\nsimultaneous localization and mapping. The project page is available at\nhttps://fastgs.github.io/", "AI": {"tldr": "FastGS是一种基于多视角一致性的稠化和剪枝策略的新型加速框架，能够显著提升3D高斯散斑方法的训练速度而不影响渲染质量。", "motivation": "当前的3D高斯散斑加速方法未能妥善调控训练过程中的高斯数量，导致冗余的计算时间消耗。为解决这一问题并平衡训练时间与渲染质量，本文作者提出FastGS框架。", "method": "FastGS框架摒弃了预算机制，创新性地提出了基于多视角一致性的稠化和剪枝策略，以调节训练过程中的高斯数量，从而提高计算效率。", "result": "该论文提出了一种名为FastGS的新型加速框架，通过基于多视角一致性的重要性设计稠化和剪枝策略，解决了现有3D高斯散斑加速方法在训练中未合理调控高斯数的问题。实验结果显示，与现有方法相比，FastGS在多个数据集上显著提升了训练速度，同时保持了渲染质量。它表现出强大的通用性，适用于多种任务。", "conclusion": "实验表明，FastGS在Mip-NeRF 360和Deep Blending数据集上分别达到了3.32倍和15.45倍的加速效果，且在多种场景下显示了良好的通用性和高的训练加速率。"}}
{"id": "2511.04476", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04476", "abs": "https://arxiv.org/abs/2511.04476", "authors": ["Fabian Schmidt", "Seyedehmoniba Ravan", "Vladimir Vlassov"], "title": "Probabilistic Textual Time Series Depression Detection", "comment": "14 pages, 8 figures, 4 tables", "summary": "Accurate and interpretable predictions of depression severity are essential\nfor clinical decision support, yet existing models often lack uncertainty\nestimates and temporal modeling. We propose PTTSD, a Probabilistic Textual Time\nSeries Depression Detection framework that predicts PHQ-8 scores from\nutterance-level clinical interviews while modeling uncertainty over time. PTTSD\nincludes sequence-to-sequence and sequence-to-one variants, both combining\nbidirectional LSTMs, self-attention, and residual connections with Gaussian or\nStudent-t output heads trained via negative log-likelihood. Evaluated on E-DAIC\nand DAIC-WOZ, PTTSD achieves state-of-the-art performance among text-only\nsystems (e.g., MAE = 3.85 on E-DAIC, 3.55 on DAIC) and produces well-calibrated\nprediction intervals. Ablations confirm the value of attention and\nprobabilistic modeling, while comparisons with MentalBERT establish generality.\nA three-part calibration analysis and qualitative case studies further\nhighlight the interpretability and clinical relevance of uncertainty-aware\nforecasting.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.04288", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04288", "abs": "https://arxiv.org/abs/2511.04288", "authors": ["Leire Benito-Del-Valle", "Artzai Picón", "Daniel Mugica", "Manuel Ramos", "Eva Portillo", "Javier Romero", "Carlos Javier Jimenez", "Ramón Navarra-Mestre"], "title": "Vision Foundation Models in Agriculture: Toward Domain-Specific Adaptation for Weed Herbicide Trials Assessment", "comment": null, "summary": "Herbicide field trials require accurate identification of plant species and\nassessment of herbicide-induced damage across diverse environments. While\ngeneral-purpose vision foundation models have shown promising results in\ncomplex visual domains, their performance can be limited in agriculture, where\nfine-grained distinctions between species and damage types are critical.\n  In this work, we adapt a general-purpose vision foundation model to herbicide\ntrial characterization. Trained using a self-supervised learning approach on a\nlarge, curated agricultural dataset, the model learns rich and transferable\nrepresentations optimized for herbicide trials images.\n  Our domain-specific model significantly outperforms the best general-purpose\nfoundation model in both species identification (F1 score improvement from 0.91\nto 0.94) and damage classification (from 0.26 to 0.33). Under unseen conditions\n(new locations and other time), it achieves even greater gains (species\nidentification from 0.56 to 0.66; damage classification from 0.17 to 0.27). In\ndomain-shift scenarios, such as drone imagery, it maintains strong performance\n(species classification from 0.49 to 0.60).\n  Additionally, we show that domain-specific pretraining enhances segmentation\naccuracy, particularly in low-annotation regimes. An annotation-efficiency\nanalysis reveals that, under unseen conditions, the domain-specific model\nachieves 5.4% higher F1 score than the general-purpose model, while using 80%\nfewer labeled samples.\n  These results demonstrate the generalization capabilities of domain-specific\nfoundation models and their potential to significantly reduce manual annotation\nefforts, offering a scalable and automated solution for herbicide trial\nanalysis.", "AI": {"tldr": "通过对通用视觉基础模型进行农业领域的自监督学习，提高除草剂试验植物识别及损害评估准确性；领域特定模型改善了物种识别和损害分类并提高了低标注情况下的分割精度。", "motivation": "提升除草剂试验中植物物种识别和除草剂损害评估的准确性。通用视觉基础模型在复杂视觉领域展示出良好的潜力，但在农业领域，需要更细粒度的物种和损害类型区分，因此需要改进。", "method": "将通用视觉基础模型适应于除草剂试验的识别。通过自监督学习方式在大型精心策划的农业数据集上训练模型，使其学习到适用于除草剂试验图像的丰富且可迁移的表示方法。", "result": "领域特定模型在物种识别和损害分类方面显著优于通用基础模型。此外，在数据稀缺情况下，领域特定模型的标注效率也更高。", "conclusion": "领域特定基础模型展示了其泛化能力，可以显著降低人工标注工作量，为除草剂试验分析提供了一种可扩展和自动化的解决方案。"}}
{"id": "2511.04479", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04479", "abs": "https://arxiv.org/abs/2511.04479", "authors": ["Surapon Nonesung", "Teetouch Jaknamon", "Sirinya Chaiophat", "Natapong Nitarach", "Chanakan Wittayasakpan", "Warit Sirichotedumrong", "Adisai Na-Thalang", "Kunat Pipatanakul"], "title": "ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai", "comment": "Accepted at the IJCNLP-AACL 2025 (Main)", "summary": "We present ThaiOCRBench, the first comprehensive benchmark for evaluating\nvision-language models (VLMs) on Thai text-rich visual understanding tasks.\nDespite recent progress in multimodal modeling, existing benchmarks\npredominantly focus on high-resource languages, leaving Thai underrepresented,\nespecially in tasks requiring document structure understanding. ThaiOCRBench\naddresses this gap by offering a diverse, human-annotated dataset comprising\n2,808 samples across 13 task categories. We evaluate a wide range of\nstate-of-the-art VLMs in a zero-shot setting, spanning both proprietary and\nopen-source systems. Results show a significant performance gap, with\nproprietary models (e.g., Gemini 2.5 Pro) outperforming open-source\ncounterparts. Notably, fine-grained text recognition and handwritten content\nextraction exhibit the steepest performance drops among open-source models.\nThrough detailed error analysis, we identify key challenges such as language\nbias, structural mismatch, and hallucinated content. ThaiOCRBench provides a\nstandardized framework for assessing VLMs in low-resource, script-complex\nsettings, and provides actionable insights for improving Thai-language document\nunderstanding.", "AI": {"tldr": "开发了ThaiOCRBench，填补泰语文本密集型视觉理解任务评估的标准，指出专有模型在评估中优于开源系统，并确定了关键挑战及提供改进建议。", "motivation": "尽管多模态建模取得了进展，但现有的基准主要集中在高资源语言上，忽略了泰语等低资源语言，特别是在需要文档结构理解的任务方面。因此，我们构建了ThaiOCRBench来填补这一空白。", "method": "我们提出了ThaiOCRBench，这是第一个针对泰语文本密集型视觉理解任务评估视觉-语言模型（VLMs）的全面基准。此基准由2,808个样本组成，跨越了13个任务类别。", "result": "在零样本设置下，对广泛的最新的VLMs进行了评估，包括专有和开源系统。结果显示出显著的表现差异，专有模型（如Gemini 2.5 Pro）优于开源模型。特别是，细粒度文本识别和手写内容提取方面，开源模型的表现显著下降。", "conclusion": "通过对详细错误的分析，确定了关键挑战，如语言偏差、结构错配和内容生成的不准确性。ThaiOCRBench提供了一个标准化框架，用以评估VLMs在低资源、复杂脚本环境下的性能，并为改进泰文文档理解提供了实践见解。"}}
{"id": "2511.04304", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.04304", "abs": "https://arxiv.org/abs/2511.04304", "authors": ["Robin Spanier", "Thorsten Hoeser", "Claudia Kuenzer"], "title": "Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data", "comment": "14 pages, 9 figures", "summary": "The recent and ongoing expansion of marine infrastructure, including offshore\nwind farms, oil and gas platforms, artificial islands, and aquaculture\nfacilities, highlights the need for effective monitoring systems. The\ndevelopment of robust models for offshore infrastructure detection relies on\ncomprehensive, balanced datasets, but falls short when samples are scarce,\nparticularly for underrepresented object classes, shapes, and sizes. By\ntraining deep learning-based YOLOv10 object detection models with a combination\nof synthetic and real Sentinel-1 satellite imagery acquired in the fourth\nquarter of 2023 from four regions (Caspian Sea, South China Sea, Gulf of\nGuinea, and Coast of Brazil), this study investigates the use of synthetic\ntraining data to enhance model performance. We evaluated this approach by\napplying the model to detect offshore platforms in three unseen regions (Gulf\nof Mexico, North Sea, Persian Gulf) and thereby assess geographic\ntransferability. This region-holdout evaluation demonstrated that the model\ngeneralises beyond the training areas. In total, 3,529 offshore platforms were\ndetected, including 411 in the North Sea, 1,519 in the Gulf of Mexico, and\n1,593 in the Persian Gulf. The model achieved an F1 score of 0.85, which\nimproved to 0.90 upon incorporating synthetic data. We analysed how synthetic\ndata enhances the representation of unbalanced classes and overall model\nperformance, taking a first step toward globally transferable detection of\noffshore infrastructure. This study underscores the importance of balanced\ndatasets and highlights synthetic data generation as an effective strategy to\naddress common challenges in remote sensing, demonstrating the potential of\ndeep learning for scalable, global offshore infrastructure monitoring.", "AI": {"tldr": "研究展示了合成数据在海上基础设施检测中的重要作用，提升了全球海上设施监测的有效性和准确度。", "motivation": "旨在通过合成数据的使用改进针对海上基础设施检测的深度学习模型的性能。", "method": "使用合成和真实的Sentinel-1卫星图像，在四个地区训练YOLOv10目标检测模型，以提升模型性能。", "result": "模型在全球三处未见区域能够有效检测海上平台，共检测到3,529个海上平台，并在包含合成数据后F1分数提高到了0.90。", "conclusion": "强调了平衡数据集的重要性，展示深度学习策略结合合成数据生成，有效应对海上基础设施全球监测的挑战。"}}
{"id": "2511.04491", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04491", "abs": "https://arxiv.org/abs/2511.04491", "authors": ["Nikhil Abhyankar", "Purvi Chaurasia", "Sanchit Kabra", "Ananya Srivastava", "Vivek Gupta", "Chandan K. Reddy"], "title": "RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables", "comment": null, "summary": "Existing tabular reasoning benchmarks mostly test models on small, uniform\ntables, underrepresenting the complexity of real-world data and giving an\nincomplete view of Large Language Models' (LLMs) reasoning abilities. Real\ntables are long, heterogeneous, and domain-specific, mixing structured fields\nwith free text and requiring multi-hop reasoning across thousands of tokens. To\naddress this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from\n2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)\nand ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates\nLLMs jointly across scale, heterogeneity, domain specificity, and reasoning\ncomplexity. Experiments with open-source and proprietary models show that LLMs\nstruggle with heterogeneous schemas and complex multi-hop inference, revealing\npersistent weaknesses in current architectures and prompting strategies.\nRUST-BENCH establishes a challenging new testbed for advancing tabular\nreasoning research.", "AI": {"tldr": "RUST-BENCH是一个大型真实世界表格推理基准，涵盖了科技与体育领域，展示了大语言模型在处理复杂表格推理任务时的不足。", "motivation": "现有的表格推理基准测试大多集中在小而统一的表格上，无法充分反映出大语言模型在处理现实数据时的推理能力。RUST-BENCH旨在填补这一空白。", "method": "提出RUST-BENCH基准测试，包含7966个问题和2031个真实世界的表格，涵盖科学和体育两个领域，用以评估大语言模型在处理异构、领域特定和复杂推理任务时的能力。", "result": "实验显示，现有模型在处理异构模式和复杂多步推理上表现欠佳，这揭示了当前架构的薄弱环节。", "conclusion": "RUST-BENCH为推进表格推理研究提供了具有挑战性的新测试平台。"}}
{"id": "2511.04317", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04317", "abs": "https://arxiv.org/abs/2511.04317", "authors": ["Xiangjun Zhang", "Litong Gong", "Yinglin Zheng", "Yansong Liu", "Wentao Jiang", "Mingyi Xu", "Biao Wang", "Tiezheng Ge", "Ming Zeng"], "title": "RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation", "comment": "17 pages, 16 figures", "summary": "Most text-to-video(T2V) diffusion models depend on pre-trained text encoders\nfor semantic alignment, yet they often fail to maintain video quality when\nprovided with concise prompts rather than well-designed ones. The primary issue\nlies in their limited textual semantics understanding. Moreover, these text\nencoders cannot rephrase prompts online to better align with user intentions,\nwhich limits both the scalability and usability of the models, To address these\nchallenges, we introduce RISE-T2V, which uniquely integrates the processes of\nprompt rephrasing and semantic feature extraction into a single and seamless\nstep instead of two separate steps. RISE-T2V is universal and can be applied to\nvarious pre-trained LLMs and video diffusion models(VDMs), significantly\nenhancing their capabilities for T2V tasks. We propose an innovative module\ncalled the Rephrasing Adapter, enabling diffusion models to utilize text hidden\nstates during the next token prediction of the LLM as a condition for video\ngeneration. By employing a Rephrasing Adapter, the video generation model can\nimplicitly rephrase basic prompts into more comprehensive representations that\nbetter match the user's intent. Furthermore, we leverage the powerful\ncapabilities of LLMs to enable video generation models to accomplish a broader\nrange of T2V tasks. Extensive experiments demonstrate that RISE-T2V is a\nversatile framework applicable to different video diffusion model\narchitectures, significantly enhancing the ability of T2V models to generate\nhigh-quality videos that align with user intent. Visual results are available\non the webpage at https://rise-t2v.github.io.", "AI": {"tldr": "RISE-T2V通过集成提示重述和语义特征提取，增强了文本到视频扩散模型的能力，以生成更高质量且符合用户意图的视频。", "motivation": "当前的文本到视频扩散模型依赖于预训练的文本编码器进行语义对齐，但它们在处理简洁的提示时常常无法保持视频质量，并且这些模型不具备在线重新阐述提示的能力，限制了模型的扩展性和实用性。", "method": "RISE-T2V整合提示重述和语义特征提取为一个单一且无缝的步骤，其关键是Rephrasing Adapter模块，该模块用于扩散模型在LLM的下一个词汇预测中使用文本隐藏状态作为视频生成的条件。", "result": "RISE-T2V是一个多功能框架，适用于不同的视频扩散模型架构，大大提高了T2V模型生成与用户意图高度一致的高质量视频的能力。实验结果表明，引入Rephrasing Adapter可以隐性地将基本的提示扩展成更能准确体现用户意图的表述，并且结合LLMs的强大功能，使视频生成模型能够完成更广泛的文本到视频任务。", "conclusion": "RISE-T2V证明了其作为一个多功能框架，通过引入Rephrasing Adapter和利用LLMs的强大功能，在不同视频扩散模型架构中提高文本到视频生成质量的有效性。"}}
{"id": "2511.04495", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04495", "abs": "https://arxiv.org/abs/2511.04495", "authors": ["Cuong Huynh", "Jie Cao"], "title": "OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation", "comment": "Accepted to TSAR 2025 Workshop at EMNLP2025", "summary": "This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task\n(Alva-Manchego et al., 2025), designed for readability-controlled text\nsimplification using LLM-prompting-based generation. Based on the analysis of\nprompt-based text simplification methods, we discovered an interesting finding\nthat text simplification performance is highly related to the gap between the\nsource CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by\nthis finding, we propose two multi-round simplification methods and generate\nthem via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based\nLLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams.\nLater improvements with MRS-Joint show that taking the LLM simplified\ncandidates as the starting point could further boost the multi-round\nsimplification performance.", "AI": {"tldr": "本文介绍了提交给TSAR-2025共享任务的OUNLP系统，该系统用于通过基于大型语言模型提示生成的简化文本来控制文本可读性。基于对提示基文本简化方法的分析发现，源文本与目标文本的CEFR等级差距与文本简化性能密切关联。因此，文章提出了两种多轮简化方法：基于规则的简化（MRS-Rule）和基于规则与语言模型联合的简化（MRS-Joint），并使用GPT-4o生成。其系统排名20支队伍中的第7位，并通过进一步改进MRS-Joint发现，以语言模型简洁化的候选作为起点能进一步提升多轮简化性能。", "motivation": "基于文本简化性能与CEFR等级之间差距的观察，研究动机在于探索改进文本简化的方法，特别是通过多轮次的简化过程。", "method": "提出了两种简化文本的方法：基于规则的简化（MRS-Rule）和基于规则与语言模型的联合简化（MRS-Joint）。这些方法使用GPT-4o生成简化后的文本。", "result": "OUNLP系统在TSAR-2025共享任务中排名第7，使用MRS-Joint方法的后期改进表明，以语言模型生成的简化作为起点可进一步提升简化效果。", "conclusion": "研究得出文本简化性能高度依赖于源文本与目标文本间的CEFR等级差距的结论，并且基于语言模型的多轮简化方法展示了提升简化解性能的潜力。"}}
{"id": "2511.04334", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04334", "abs": "https://arxiv.org/abs/2511.04334", "authors": ["Saúl Alonso-Monsalve", "Leigh H. Whitehead", "Adam Aurisano", "Lorena Escudero Sanchez"], "title": "Submanifold Sparse Convolutional Networks for Automated 3D Segmentation of Kidneys and Kidney Tumours in Computed Tomography", "comment": "12 pages, 5 figures", "summary": "The accurate delineation of tumours in radiological images like Computed\nTomography is a very specialised and time-consuming task, and currently a\nbottleneck preventing quantitative analyses to be performed routinely in the\nclinical setting. For this reason, developing methods for the automated\nsegmentation of tumours in medical imaging is of the utmost importance and has\ndriven significant efforts in recent years. However, challenges regarding the\nimpracticality of 3D scans, given the large amount of voxels to be analysed,\nusually requires the downsampling of such images or using patches thereof when\napplying traditional convolutional neural networks. To overcome this problem,\nin this paper we propose a new methodology that uses, divided into two stages,\nvoxel sparsification and submanifold sparse convolutional networks. This method\nallows segmentations to be performed with high-resolution inputs and a native\n3D model architecture, obtaining state-of-the-art accuracies while\nsignificantly reducing the computational resources needed in terms of GPU\nmemory and time. We studied the deployment of this methodology in the context\nof Computed Tomography images of renal cancer patients from the KiTS23\nchallenge, and our method achieved results competitive with the challenge\nwinners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7%\nfor tumours + cysts, and 80.3% for tumours alone. Crucially, our method also\noffers significant computational improvements, achieving up to a 60% reduction\nin inference time and up to a 75\\% reduction in VRAM usage compared to an\nequivalent dense architecture, across both CPU and various GPU cards tested.", "AI": {"tldr": "This paper addresses the issue of time-intensive tumour segmentation in 3D medical images by proposing a method that uses voxel sparsification and submanifold sparse convolutions, achieving high accuracy and saving significant computational resources.", "motivation": "The motivation for this paper is to address the challenge of time-consuming and specialized task of tumour delineation in radiological images, especially in 3D scans with a high number of voxels.", "method": "The proposed methodology involves a two-stage process: voxel sparsification and the use of submanifold sparse convolutional networks to reduce the computational load while maintaining high-resolution and accurate 3D segmentations.", "result": "The proposed method achieves Dice similarity coefficients of 95.8% for kidneys + masses, 85.7% for tumours + cysts, and 80.3% for tumours alone in CT images of renal cancer patients. The method also significantly reduces inference time by up to 60% and VRAM usage by up to 75% compared to an equivalent dense architecture.", "conclusion": "The paper concludes that the proposed methodology provides state-of-the-art accuracy while significantly reducing computational resources, making quantitative analyses more feasible in a clinical setting."}}
{"id": "2511.04499", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04499", "abs": "https://arxiv.org/abs/2511.04499", "authors": ["Christos-Nikolaos Zacharopoulos", "Revekka Kyriakoglou"], "title": "Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering", "comment": "Accepted at IJCNLP-AACL 2025", "summary": "As Large Language Models (LLMs) become integral to human-centered\napplications, understanding their personality-like behaviors is increasingly\nimportant for responsible development and deployment. This paper systematically\nevaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to\nassess trait expressions under varying sampling temperatures. We find\nsignificant differences across four of the five personality dimensions, with\nNeuroticism and Extraversion susceptible to temperature adjustments. Further,\nhierarchical clustering reveals distinct model clusters, suggesting that\narchitectural features may predispose certain models toward stable trait\nprofiles. Taken together, these results offer new insights into the emergence\nof personality-like patterns in LLMs and provide a new perspective on model\ntuning, selection, and the ethical governance of AI systems. We share the data\nand code for this analysis here:\nhttps://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1", "AI": {"tldr": "本论文使用BFI-2框架评估了六个大型语言模型的性格特质表现，发现了受温度调整影响的神经质和外向性，并提出模型架构特性可能导致模型倾向于稳定的特性模式。", "motivation": "随着大型语言模型（LLMs）成为人类中心应用的组成部分，了解它们类似于个性的行为对于负责任的发展和部署越来越重要。", "method": "应用了Big Five Inventory-2 (BFI-2) 框架系统地评估了六个大型语言模型，在不同的采样温度下评估其性格特质的表现。", "result": "发现了五个性格维度中的四个存在显著差异，神经质和外向性受温度调整的影响。层次聚类显示了不同的模型聚类，表明架构特性可能导致某些模型倾向于稳定的特性模式。", "conclusion": "这些结果为理解大型语言模型中的性格模式提供了新的见解，并为模型调优、选择以及人工智能系统的道德治理提供了新的视角。"}}
{"id": "2511.04344", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04344", "abs": "https://arxiv.org/abs/2511.04344", "authors": ["Muhammad Annas Shaikh", "Hamza Zaman", "Arbaz Asif"], "title": "Comparative Study of CNN Architectures for Binary Classification of Horses and Motorcycles in the VOC 2008 Dataset", "comment": null, "summary": "This paper presents a comprehensive evaluation of nine convolutional neural\nnetwork architectures for binary classification of horses and motorcycles in\nthe VOC 2008 dataset. We address the significant class imbalance problem by\nimplementing minority-class augmentation techniques. Our experiments compare\nmodern architectures including ResNet-50, ConvNeXt-Tiny, DenseNet-121, and\nVision Transformer across multiple performance metrics. Results demonstrate\nsubstantial performance variations, with ConvNeXt-Tiny achieving the highest\nAverage Precision (AP) of 95.53% for horse detection and 89.12% for motorcycle\ndetection. We observe that data augmentation significantly improves minority\nclass detection, particularly benefiting deeper architectures. This study\nprovides insights into architecture selection for imbalanced binary\nclassification tasks and quantifies the impact of data augmentation strategies\nin mitigating class imbalance issues in object detection.", "AI": {"tldr": "研究了不同卷积神经网络架构在处理不平衡类别二元分类问题时的性能，采用数据增强技术提高少数类检测。", "motivation": "这项研究的动机在于探讨架构选择对不平衡二元分类任务的影响力，并量化数据增强策略在减轻类不平衡问题上的效果。", "method": "本文评估了九种卷积神经网络架构在VOC 2008数据集中对马和摩托车的二元分类问题上的表现，并通过实施少数类增强技术来应对显著的类不平衡问题。", "result": "实验结果显示性能差异显著，其中ConvNeXt-Tiny在马检测中的平均精确度（AP）为95.53%，摩托车检测中的AP为89.12%。数据增强显著提高了少数类检测的性能，尤其是对更深的架构有所帮助。", "conclusion": "本研究为架构选择提供了见解，并且指出了数据增强策略在解决类不平衡问题时的重要性。"}}
{"id": "2511.04502", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04502", "abs": "https://arxiv.org/abs/2511.04502", "authors": ["Joshua Gao", "Quoc Huy Pham", "Subin Varghese", "Silwal Saurav", "Vedhus Hoskere"], "title": "RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific RAG", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is a critical technique for grounding\nLarge Language Models (LLMs) in factual evidence, yet evaluating RAG systems in\nspecialized, safety-critical domains remains a significant challenge. Existing\nevaluation frameworks often rely on heuristic-based metrics that fail to\ncapture domain-specific nuances and other works utilize LLM-as-a-Judge\napproaches that lack validated alignment with human judgment. This paper\nintroduces RAGalyst, an automated, human-aligned agentic framework designed for\nthe rigorous evaluation of domain-specific RAG systems. RAGalyst features an\nagentic pipeline that generates high-quality, synthetic question-answering (QA)\ndatasets from source documents, incorporating an agentic filtering step to\nensure data fidelity. The framework refines two key LLM-as-a-Judge\nmetrics-Answer Correctness and Answerability-using prompt optimization to\nachieve a strong correlation with human annotations. Applying this framework to\nevaluate various RAG components across three distinct domains (military\noperations, cybersecurity, and bridge engineering), we find that performance is\nhighly context-dependent. No single embedding model, LLM, or hyperparameter\nconfiguration proves universally optimal. Additionally, we provide an analysis\non the most common low Answer Correctness reasons in RAG. These findings\nhighlight the necessity of a systematic evaluation framework like RAGalyst,\nwhich empowers practitioners to uncover domain-specific trade-offs and make\ninformed design choices for building reliable and effective RAG systems.\nRAGalyst is available on our Github.", "AI": {"tldr": "本文介绍了一种自动化、人类对齐的代理框架RAGalyst，用于专门领域中RAG系统的严格评估。", "motivation": "现有的评估框架往往依赖于无法捕捉领域特定细微差别的基于启发式的度量方法，或者使用未经验证与人类判断一致性的LLM作为裁判的方法。", "method": "RAGalyst框架包括一个代理管道，从源文件生成高质量的合成问答数据集，并使用代理过滤步骤确保数据保真度。框架通过提示优化来增强两个关键的LLM-as-a-Judge指标，即答案正确性和问题可解答性，以达到与人类标注的高度相关性。", "result": "在军事行动、网络安全和桥梁工程三个领域中，对各种RAG组件进行评估后，发现性能受到高度情景依赖的影响，没有一个嵌入模型、LLM或超参数配置是普遍优化的。", "conclusion": "研究结果表明，性能高度依赖于具体的应用场景，没有单一的嵌入模型、LLM或超参数配置能够普遍适用。RAGalyst框架有助于从业者发现特定领域中的权衡，并做出更明智的设计决策，从而建立可靠的RAG系统。RAGalyst框架已开源。"}}
{"id": "2511.04347", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04347", "abs": "https://arxiv.org/abs/2511.04347", "authors": ["Sanjay Kumar", "Tim Brophy", "Eoin Martino Grua", "Ganesh Sistu", "Valentina Donzella", "Ciaran Eising"], "title": "Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection", "comment": null, "summary": "Accurate 3D object detection is essential for automated vehicles to navigate\nsafely in complex real-world environments. Bird's Eye View (BEV)\nrepresentations, which project multi-sensor data into a top-down spatial\nformat, have emerged as a powerful approach for robust perception. Although\nBEV-based fusion architectures have demonstrated strong performance through\nmultimodal integration, the effects of sensor occlusions, caused by\nenvironmental conditions such as fog, haze, or physical obstructions, on 3D\ndetection accuracy remain underexplored. In this work, we investigate the\nimpact of occlusions on both camera and Light Detection and Ranging (LiDAR)\noutputs using the BEVFusion architecture, evaluated on the nuScenes dataset.\nDetection performance is measured using mean Average Precision (mAP) and the\nnuScenes Detection Score (NDS). Our results show that moderate camera\nocclusions lead to a 41.3% drop in mAP (from 35.6% to 20.9%) when detection is\nbased only on the camera. On the other hand, LiDAR sharply drops in performance\nonly under heavy occlusion, with mAP falling by 47.3% (from 64.7% to 34.1%),\nwith a severe impact on long-range detection. In fused settings, the effect\ndepends on which sensor is occluded: occluding the camera leads to a minor 4.1%\ndrop (from 68.5% to 65.7%), while occluding LiDAR results in a larger 26.8%\ndrop (to 50.1%), revealing the model's stronger reliance on LiDAR for the task\nof 3D object detection. Our results highlight the need for future research into\nocclusion-aware evaluation methods and improved sensor fusion techniques that\ncan maintain detection accuracy in the presence of partial sensor failure or\ndegradation due to adverse environmental conditions.", "AI": {"tldr": "摘要：研究了遮挡条件下基于鸟瞰视图的相机和激光雷达的3D对象检测性能，发现在遮挡情况下，激光雷达比相机更受重影时影响较大，且在传感器融合中激光雷达的作用更为关键。", "motivation": "动机：虽然基于鸟瞰视图（BEV）的多模式融合架构已经表现出强大的性能，但是遮挡对3D检测准确性的影响尚未得到充分研究。这项工作旨在探究遮挡对基于BEV的相机和激光雷达输出的影响。", "method": "方法：使用BEVFusion架构，在nuScenes数据集上评估了相机和激光雷达在不同遮挡条件下的3D对象检测性能。", "result": "结果：中度遮挡导致仅基于相机的检测平均精度（mAP）下降41.3%，而激光雷达的性能在重度遮挡时急剧下降47.3%。在多传感器融合的情况下，遮挡相机对性能影响较小，仅为4.1%，而遮挡激光雷达则影响较大，下降26.8%。", "conclusion": "结论：研究表明，需要进一步研究遮挡感知评估方法和技术，以改善传感器融合技术，确保在部分传感器失效或环境中不利条件导致的性能退化时仍能保持检测精度。"}}
{"id": "2511.04506", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04506", "abs": "https://arxiv.org/abs/2511.04506", "authors": ["Paloma Rabaey", "Jong Hak Moon", "Jung-Oh Lee", "Min Gwan Kim", "Hangyul Yoon", "Thomas Demeester", "Edward Choi"], "title": "Modeling Clinical Uncertainty in Radiology Reports: from Explicit Uncertainty Markers to Implicit Reasoning Pathways", "comment": null, "summary": "Radiology reports are invaluable for clinical decision-making and hold great\npotential for automated analysis when structured into machine-readable formats.\nThese reports often contain uncertainty, which we categorize into two distinct\ntypes: (i) Explicit uncertainty reflects doubt about the presence or absence of\nfindings, conveyed through hedging phrases. These vary in meaning depending on\nthe context, making rule-based systems insufficient to quantify the level of\nuncertainty for specific findings; (ii) Implicit uncertainty arises when\nradiologists omit parts of their reasoning, recording only key findings or\ndiagnoses. Here, it is often unclear whether omitted findings are truly absent\nor simply unmentioned for brevity. We address these challenges with a two-part\nframework. We quantify explicit uncertainty by creating an expert-validated,\nLLM-based reference ranking of common hedging phrases, and mapping each finding\nto a probability value based on this reference. In addition, we model implicit\nuncertainty through an expansion framework that systematically adds\ncharacteristic sub-findings derived from expert-defined diagnostic pathways for\n14 common diagnoses. Using these methods, we release Lunguage++, an expanded,\nuncertainty-aware version of the Lunguage benchmark of fine-grained structured\nradiology reports. This enriched resource enables uncertainty-aware image\nclassification, faithful diagnostic reasoning, and new investigations into the\nclinical impact of diagnostic uncertainty.", "AI": {"tldr": "研究提出了一种框架来处理放射学报告中的显式和隐式不确定性，将不确定性映射到概率，并加入子发现来丰富结构化的放射学报告。", "motivation": "对放射学报告中的不确定性的显式和隐式分类感兴趣，探讨了如何使用机器可读格式进行自动化分析。", "method": "采用了一个两部分框架来量化显式和隐式不确定性。通过专家验证的LLM基参考排名以及映射到概率值，量化了显式不确定性。通过系统地添加从专家定义的14种常见诊断路径中得出的子发现，建立了隐式不确定性模型。", "result": "发布了一个扩展的、能识别不确定性的Lunguage++，这是一个细粒度的结构化放射学报告基准的扩展版本。", "conclusion": "该资源支持识别不确定性的图像分类，可靠的诊断推理，以及诊断不确定性临床影响的新研究。"}}
{"id": "2511.04349", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04349", "abs": "https://arxiv.org/abs/2511.04349", "authors": ["Puneet Mishra", "Martijntje Vollebregt", "Yizhou Ma", "Maria Font-i-Furnols"], "title": "A MATLAB tutorial on deep feature extraction combined with chemometrics for analytical applications", "comment": null, "summary": "Background In analytical chemistry, spatial information about materials is\ncommonly captured through imaging techniques, such as traditional color cameras\nor with advanced hyperspectral cameras and microscopes. However, efficiently\nextracting and analyzing this spatial information for exploratory and\npredictive purposes remains a challenge, especially when using traditional\nchemometric methods. Recent advances in deep learning and artificial\nintelligence have significantly enhanced image processing capabilities,\nenabling the extraction of multiscale deep features that are otherwise\nchallenging to capture with conventional image processing techniques. Despite\nthe wide availability of open-source deep learning models, adoption in\nanalytical chemistry remains limited because of the absence of structured,\nstep-by-step guidance for implementing these models.\n  Results This tutorial aims to bridge this gap by providing a step-by-step\nguide for applying deep learning approaches to extract spatial information from\nimaging data and integrating it with other data sources, such as spectral\ninformation. Importantly, the focus of this work is not on training deep\nlearning models for image processing but on using existing open source models\nto extract deep features from imaging data.\n  Significance The tutorial provides MATLAB code tutorial demonstrations,\nshowcasing the processing of imaging data from various imaging modalities\ncommonly encountered in analytical chemistry. Readers must run the tutorial\nsteps on their own datasets using the codes presented in this tutorial.", "AI": {"tldr": "该教程提供使用开源深度学习模型从成像数据中提取空间信息并加以整合的分步指南，重点不在于训练模型，而在于利用现有模型提取深度特征。", "motivation": "由于缺乏结构化的分步实施指导，尽管有现成的开源深度学习模型，但它们在分析化学中的采用仍受到限制。因此，该论文的目标是通过提供深度学习方法的分步指南来填补这一空白。", "method": "该教程重点不是训练深度学习模型进行图像处理，而是使用现有的开源模型来从图像数据中提取深度特征，其方法包括了分步指导，使用到了MATLAB代码演示，处理各种分析化学中常见的成像模态的成像数据。", "result": "该教程提供了使用MATLAB代码的演示，涵盖从不同常见于分析化学的成像模态中处理图像数据的方法，演示了如何利用深度学习模型从图像中提取深层特征。", "conclusion": "本教程为读者使用自己的数据集独立运行步骤提供了必要条件，通过展示如何将成像数据从各种成像模态中提取出来，并与其它数据源（如光谱信息）进行整合，来强化了分析化学图像数据的空间信息提取功能。"}}
{"id": "2511.04527", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04527", "abs": "https://arxiv.org/abs/2511.04527", "authors": ["Amir Zur", "Atticus Geiger", "Ekdeep Singh Lubana", "Eric Bigelow"], "title": "Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics", "comment": null, "summary": "When a language model generates text, the selection of individual tokens\nmight lead it down very different reasoning paths, making uncertainty difficult\nto quantify. In this work, we consider whether reasoning language models\nrepresent the alternate paths that they could take during generation. To test\nthis hypothesis, we use hidden activations to control and predict a language\nmodel's uncertainty during chain-of-thought reasoning. In our experiments, we\nfind a clear correlation between how uncertain a model is at different tokens,\nand how easily the model can be steered by controlling its activations. This\nsuggests that activation interventions are most effective when there are\nalternate paths available to the model -- in other words, when it has not yet\ncommitted to a particular final answer. We also find that hidden activations\ncan predict a model's future outcome distribution, demonstrating that models\nimplicitly represent the space of possible paths.", "AI": {"tldr": "本文研究了语言模型在生成文本时如何表示其可能采取的其他路径，并发现模型的隐藏激活能用来控制和预测其不确定性，表明模型隐式地表示了可能路径的空间。", "motivation": "语言模型在生成文本时，由于词汇选择的不同，可能导致推理路径的极大差异，使得难以量化不确定性。作者希望探索语言模型是否代表了在生成过程中的其他可能路径。", "method": "本研究通过使用隐藏激活来控制和预测语言模型在连贯思维推理过程中的不确定性。", "result": "研究发现，模型在不同词汇的不确定性与其可被活动干预控制的程度有明显的相关性。此外，隐藏激活能够预测模型对未来结果分布的预测。", "conclusion": "模型在未完全确定最终答案之前，活动干预的效果最为显著，表明模型内在地表示了可能的路径空间。"}}
{"id": "2511.04384", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04384", "abs": "https://arxiv.org/abs/2511.04384", "authors": ["Itbaan Safwan", "Muhammad Annas Shaikh", "Muhammad Haaris", "Ramail Khan", "Muhammad Atif Tahir"], "title": "Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA", "comment": "This is a working paper submitted for Medico 2025: Visual Question\n  Answering (with multimodal explanations) for Gastrointestinal Imaging at\n  MediaEval 2025. 5 pages, 3 figures and 1 table", "summary": "We present a multi-task framework for the MediaEval Medico 2025 challenge,\nleveraging a LoRA-tuned Florence-2 model for simultaneous visual question\nanswering (VQA), explanation generation, and visual grounding. The proposed\nsystem integrates three curated datasets: (1) Kvasir-VQA-x1 for question-answer\nlearning, (2) a synthetically enriched explanation dataset offering structured\nmedical reasoning, and (3) text-to-region pairs linking visual features with\nsegmentation masks. This multi-task setup enables the model to jointly learn\nvisual grounding, reasoning, and interpretation, producing responses that are\nboth accurate and interpretable. Extensive evaluation demonstrates that our\napproach substantially improves over single-task baselines in both answer\naccuracy and visual localization, highlighting the effectiveness of grounded\nmulti-task learning for medical VQA applications.", "AI": {"tldr": "本论文提出了一种针对MediaEval Medico 2025挑战的多任务框架，利用微调后的Florence-2模型同时进行视觉问题回答、解释生成和视觉定位。实验表明，相比单一任务基线模型，该多任务模型在回答准确性和视觉定位性能上均有显著提升。", "motivation": "多任务框架能够同时学习视觉定位、推理和解释，从而生成准确且可解释的回答。这一框架旨在通过利用多个经过精心挑选的数据集，提高医学图像的视觉问题回答的准确性和解释性。", "method": "Structure", "result": "{\n  \"tldr\": \"本论文提出了一种针对MediaEval Medico 2025挑战的多任务框架，利用微调后的Florence-2模型同时进行视觉问题回答、解释生成和视觉定位。实验表明，相比单一任务基线模型，该多任务模型在回答准确性和视觉定位性能上均有显著提升。\",\n  \"motivation\": \"多任务框架能够同时学习视觉定位、推理和解释，从而生成准确且可解释的回答。这一框架旨在通过利用多个经过精心挑选的数据集，提高医学图像的视觉问题回答的准确性和解释性。\",\n  \"method\": \"论文采用了Florence-2模型，并结合了三个不同的数据集：Kvasir-VQA-x1用于问答学习、合成增强的解释数据集提供结构化的医疗推理、以及文本到区域的对来链接视觉特征与分割掩码，以实现问答、解释生成和视觉定位的多任务学习。\",\n  \"result\": \"通过详细的评估，证明了该多任务设置下的模型相比单一任务模型具有显著性能提升，在答案准确性和视觉定位方面都得到了改进，展示了多任务学习方法的有效性。\",\n  \"conclusion\": \"结论表明，通过多任务学习可以提高医学视觉问答任务的性能，这种方法不仅可以提高回答的准确性，还能增强回答的解释能力，对医学应用具有重要价值。这一框架的广泛应用有望进一步推动医学图像分析与解释的发展。\"]}\n", "conclusion": "结论表明，通过多任务学习可以提高医学视觉问答任务的性能，这种方法不仅可以提高回答的准确性，还能增强回答的解释能力，对医学应用具有重要价值。这一框架的广泛应用有望进一步推动医学图像分析与解释的发展。"}}
{"id": "2511.04528", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04528", "abs": "https://arxiv.org/abs/2511.04528", "authors": ["Kaveh Eskandari Miandoab", "Katharine Kowalyshyn", "Kabir Pamnani", "Anesu Gavhera", "Vasanth Sarathy", "Matthias Scheutz"], "title": "IntelliProof: An Argumentation Network-based Conversational Helper for Organized Reflection", "comment": "Accepted for the 40th Annual AAAI Conference on Artificial\n  Intelligence (2026) - Demonstration Track", "summary": "We present IntelliProof, an interactive system for analyzing argumentative\nessays through LLMs. IntelliProof structures an essay as an argumentation\ngraph, where claims are represented as nodes, supporting evidence is attached\nas node properties, and edges encode supporting or attacking relations. Unlike\nexisting automated essay scoring systems, IntelliProof emphasizes the user\nexperience: each relation is initially classified and scored by an LLM, then\nvisualized for enhanced understanding. The system provides justifications for\nclassifications and produces quantitative measures for essay coherence. It\nenables rapid exploration of argumentative quality while retaining human\noversight. In addition, IntelliProof provides a set of tools for a better\nunderstanding of an argumentative essay and its corresponding graph in natural\nlanguage, bridging the gap between the structural semantics of argumentative\nessays and the user's understanding of a given text. A live demo and the system\nare available here to try: \\textbf{https://intelliproof.vercel.app}", "AI": {"tldr": "IntelliProof是一种交互式分析论证性文章的系统，通过构建一个论证图来增强对文章连贯性的理解和评估，同时也为用户提供一套工具来自然语言理解文章和图表。", "motivation": "IntelliProof的目标是提高用户体验，不仅自动评估文章，还提供详尽的结构化解释和对文章连贯性的定量度量，使用户能够快速探索论点质量并保持人工监督。", "method": "IntelliProof通过将文章结构化为论证图来分析论证性文章，图中的主张作为节点，支持证据作为节点属性，边则表示支持或攻击关系。它利用大语言模型对每种关系进行初始分类和评分，并允许用户进行可视化理解和进一步调整。", "result": "该系统能够为论证性文章提供结构化分析，并通过提供用户友好的界面和工具来促进对文章质量的快速探索和理解。", "conclusion": "IntelliProof为用户提供了更好地理解和评估论证性文章的新方法，通过结合大语言模型的自动评分和可视化工具增强用户理解力。"}}
{"id": "2511.04388", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04388", "abs": "https://arxiv.org/abs/2511.04388", "authors": ["Chang Liu", "Juan Li", "Sheng Zhang", "Chang Liu", "Jie Li", "Xu Zhang"], "title": "BoRe-Depth: Self-supervised Monocular Depth Estimation with Boundary Refinement for Embedded Systems", "comment": "8 pages, 5 figures, published to IROS 2025", "summary": "Depth estimation is one of the key technologies for realizing 3D perception\nin unmanned systems. Monocular depth estimation has been widely researched\nbecause of its low-cost advantage, but the existing methods face the challenges\nof poor depth estimation performance and blurred object boundaries on embedded\nsystems. In this paper, we propose a novel monocular depth estimation model,\nBoRe-Depth, which contains only 8.7M parameters. It can accurately estimate\ndepth maps on embedded systems and significantly improves boundary quality.\nFirstly, we design an Enhanced Feature Adaptive Fusion Module (EFAF) which\nadaptively fuses depth features to enhance boundary detail representation.\nSecondly, we integrate semantic knowledge into the encoder to improve the\nobject recognition and boundary perception capabilities. Finally, BoRe-Depth is\ndeployed on NVIDIA Jetson Orin, and runs efficiently at 50.7 FPS. We\ndemonstrate that the proposed model significantly outperforms previous\nlightweight models on multiple challenging datasets, and we provide detailed\nablation studies for the proposed methods. The code is available at\nhttps://github.com/liangxiansheng093/BoRe-Depth.", "AI": {"tldr": "本文提出了一种名为BoRe-Depth的轻量级单目深度估计模型，包含8.7M参数，旨在提高嵌入式系统中的深度估计性能和边界清晰度。模型在NVIDIA Jetson Orin上运行，达到50.7 FPS，优于现有的轻量级模型。", "motivation": "为了克服现有单目深度估计技术在嵌入式系统上的性能不足和边界模糊问题，本文提出了一种新的深度估计模型。", "method": "BoRe-Depth 包括增强特征自适应融合模块（EFAF），自适应融合深度特征以增强边界细节的表示，并在编码器中集成语义知识以提高物体识别和边界感知能力。", "result": "实验表明，BoRe-Depth 在多个具有挑战性的数据集上显著优于其他轻量级模型，并且提供了详细的方法消融研究。", "conclusion": "BoRe-Depth 在保持低参数量和高效率的前提下，改善了深度地图的估计和边界质量，特别是在嵌入式系统中取得了更好的性能。"}}
{"id": "2511.04538", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04538", "abs": "https://arxiv.org/abs/2511.04538", "authors": ["Cyril Vallez", "Alexander Sternfeld", "Andrei Kucharavy", "Ljiljana Dolamic"], "title": "From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting", "comment": null, "summary": "As the role of Large Language Models (LLM)-based coding assistants in\nsoftware development becomes more critical, so does the role of the bugs they\ngenerate in the overall cybersecurity landscape. While a number of LLM code\nsecurity benchmarks have been proposed alongside approaches to improve the\nsecurity of generated code, it remains unclear to what extent they have\nimpacted widely used coding LLMs. Here, we show that even the latest\nopen-weight models are vulnerable in the earliest reported vulnerability\nscenarios in a realistic use setting, suggesting that the safety-functionality\ntrade-off has until now prevented effective patching of vulnerabilities. To\nhelp address this issue, we introduce a new severity metric that reflects the\nrisk posed by an LLM-generated vulnerability, accounting for vulnerability\nseverity, generation chance, and the formulation of the prompt that induces\nvulnerable code generation - Prompt Exposure (PE). To encourage the mitigation\nof the most serious and prevalent vulnerabilities, we use PE to define the\nModel Exposure (ME) score, which indicates the severity and prevalence of\nvulnerabilities a model generates.", "AI": {"tldr": "文章指出即使最新的开放权重模型也会生成已知的网络安全漏洞，且针对这一问题提出了新的度量标准和评分系统来评估这些漏洞的严重性及其分布情况。", "motivation": "随着大型语言模型(Large Language Models)在软件开发中扮演的角色越来越重要，它们生成代码中的漏洞对整体网络安全也日益突出。而研究目前提出的代码安全基准和改进方法对广泛使用的编码LLM模型的影响程度尚不明确，这成为本论文的动力源。", "method": "该论文通过分析最新的开放权重模型在实际使用场景下的漏洞生成情况，引入了一个名为Prompt Exposure (PE)的新度量标准，用于反映由LLM生成的漏洞风险。这个度量标准考虑了漏洞的严重性、生成概率以及激发生成漏洞代码的提示语句的形式。", "result": "该论文主要研究了大型语言模型在代码生成中固有的安全性问题，尤其是这些模型生成的漏洞在网络安全方面的潜在影响。研究发现，即使是最新的开放权重模型，在实际应用中也存在已被识别的漏洞场景。为了缓解此问题，提出了一个漏洞严重度的新度量标准，即Prompt Exposure (PE)，并进一步定义了Model Exposure (ME)分值以评估模型生成漏洞的严重性和普遍性。", "conclusion": "论文提出了一个新的模型漏洞暴露度评分(Model Exposure (ME))，表明了现有模型生成漏洞的严重性和普遍性，并指出这些模型的生成安全性和功能之间的权衡导致了漏洞的有效修补并未实现有效改进。"}}
{"id": "2511.04394", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04394", "abs": "https://arxiv.org/abs/2511.04394", "authors": ["Ke Du", "Yimin Peng", "Chao Gao", "Fan Zhou", "Siqiao Xue"], "title": "DORAEMON: A Unified Library for Visual Object Modeling and Representation Learning at Scale", "comment": "code: https://github.com/wuji3/DORAEMON", "summary": "DORAEMON is an open-source PyTorch library that unifies visual object\nmodeling and representation learning across diverse scales. A single\nYAML-driven workflow covers classification, retrieval and metric learning; more\nthan 1000 pretrained backbones are exposed through a timm-compatible interface,\ntogether with modular losses, augmentations and distributed-training utilities.\nReproducible recipes match or exceed reference results on ImageNet-1K,\nMS-Celeb-1M and Stanford online products, while one-command export to ONNX or\nHuggingFace bridges research and deployment. By consolidating datasets, models,\nand training techniques into one platform, DORAEMON offers a scalable\nfoundation for rapid experimentation in visual recognition and representation\nlearning, enabling efficient transfer of research advances to real-world\napplications. The repository is available at https://github.com/wuji3/DORAEMON.", "AI": {"tldr": "DORAEMON是一个开放源代码的PyTorch库，它统一了视觉对象建模和表示学习，提供了一个高效、可扩展的研究平台。", "motivation": "DORAEMON的动机在于通过集成数据集、模型和训练技术，提供一个统一的实验平台，来加速视觉识别和表示学习的研究，并让这些研究成果能够高效地应用于实际场景中。", "method": "DORAEMON采用了开放源代码的PyTorch库，统一了跨不同尺度的视觉对象建模和表示学习。它提供了一个单一的、由YAML驱动的工作流，涵盖分类、检索和度量学习。此外，它通过与timm兼容的接口提供了超过1000个预训练主干模型，以及模块化的损失函数、增强技术和分布式训练工具。", "result": "实验结果表明，DORAEMON提供的可重复使用的配方在ImageNet-1K、MS-Celeb-1M和Stanford在线产品数据集上能达到或超过参考结果。同时，它还能通过一键导出到ONNX或HuggingFace来连接研究和部署。", "conclusion": "总之，DORAEMON通过提供一个可重复使用的框架，使得在视觉识别和表示学习领域的研究成果能够更容易地转化为实际应用。"}}
{"id": "2511.04560", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04560", "abs": "https://arxiv.org/abs/2511.04560", "authors": ["Sadia Sultana", "Saiyma Sittul Muna", "Mosammat Zannatul Samarukh", "Ajwad Abrar", "Tareque Mohmud Chowdhury"], "title": "BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation Strategies for Bangla Biomedical Question Answering", "comment": "Under Review", "summary": "Developing accurate biomedical Question Answering (QA) systems in\nlow-resource languages remains a major challenge, limiting equitable access to\nreliable medical knowledge. This paper introduces BanglaMedQA and\nBanglaMMedBench, the first large-scale Bangla biomedical Multiple Choice\nQuestion (MCQ) datasets designed to evaluate reasoning and retrieval in medical\nartificial intelligence (AI). The study applies and benchmarks several\nRetrieval-Augmented Generation (RAG) strategies, including Traditional,\nZero-Shot Fallback, Agentic, Iterative Feedback, and Aggregate RAG, combining\ntextbook-based and web retrieval with generative reasoning to improve factual\naccuracy. A key novelty lies in integrating a Bangla medical textbook corpus\nthrough Optical Character Recognition (OCR) and implementing an Agentic RAG\npipeline that dynamically selects between retrieval and reasoning strategies.\nExperimental results show that the Agentic RAG achieved the highest accuracy\n89.54% with openai/gpt-oss-120b, outperforming other configurations and\ndemonstrating superior rationale quality. These findings highlight the\npotential of RAG-based methods to enhance the reliability and accessibility of\nBangla medical QA, establishing a foundation for future research in\nmultilingual medical artificial intelligence.", "AI": {"tldr": "本论文介绍了BanglaMedQA和BanglaMMedBench，这是首个为评估医学AI推理和检索能力而设计的大型孟加拉语生物医学多项选择题数据集，实验表明代理RAG策略在准确性和合理性方面表现最佳。", "motivation": "论文旨在解决在低资源语言中开发精确的生物医学问答系统的挑战，这限制了可靠医疗知识的公平获取。", "method": "该研究应用了几种检索增强生成（RAG）策略，包括传统方法、零样本回退、代理方法、迭代反馈和聚合RAG，目的在于通过结合基于教科书和网络检索与生成性推理来提高事实准确性。特别是，使用光学字符识别（OCR）将孟加拉语医学教科书整合进来，并实现了一种代理RAG流水线，能够动态选择检索与推理策略。", "result": "实验结果显示，代理RAG达到了最高的准确率89.54%，使用openai/gpt-oss-120b模型，优于其他配置，并显示出更好的合理性质量。", "conclusion": "这些发现强调了基于RAG的方法在提升孟加拉语医学问答系统的可靠性和可访问性方面的潜力，为未来在多语言医学人工智能研究奠定了基础。"}}
{"id": "2511.04426", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04426", "abs": "https://arxiv.org/abs/2511.04426", "authors": ["Alan de Aguiar", "Michaella Pereira Andrade", "Charles Morphy D. Santos", "João Paulo Gois"], "title": "HideAndSeg: an AI-based tool with automated prompting for octopus segmentation in natural habitats", "comment": null, "summary": "Analyzing octopuses in their natural habitats is challenging due to their\ncamouflage capability, rapid changes in skin texture and color, non-rigid body\ndeformations, and frequent occlusions, all of which are compounded by variable\nunderwater lighting and turbidity. Addressing the lack of large-scale annotated\ndatasets, this paper introduces HideAndSeg, a novel, minimally supervised\nAI-based tool for segmenting videos of octopuses. It establishes a quantitative\nbaseline for this task. HideAndSeg integrates SAM2 with a custom-trained\nYOLOv11 object detector. First, the user provides point coordinates to generate\nthe initial segmentation masks with SAM2. These masks serve as training data\nfor the YOLO model. After that, our approach fully automates the pipeline by\nproviding a bounding box prompt to SAM2, eliminating the need for further\nmanual intervention. We introduce two unsupervised metrics - temporal\nconsistency $DICE_t$ and new component count $NC_t$ - to quantitatively\nevaluate segmentation quality and guide mask refinement in the absence of\nground-truth data, i.e., real-world information that serves to train, validate,\nand test AI models. Results show that HideAndSeg achieves satisfactory\nperformance, reducing segmentation noise compared to the manually prompted\napproach. Our method can re-identify and segment the octopus even after periods\nof complete occlusion in natural environments, a scenario in which the manually\nprompted model fails. By reducing the need for manual analysis in real-world\nscenarios, this work provides a practical tool that paves the way for more\nefficient behavioral studies of wild cephalopods.", "AI": {"tldr": "该论文提出了一个极小监督的AI工具（HideAndSeg），用于分割章鱼视频，通过结合SAM2和YOLOv11进行自动化处理，能够有效减少需手动分析的工作量，用以提高野生头足类动物行为研究的效率。", "motivation": "该论文旨在解决章鱼在自然栖息地分析的挑战，如其伪装能力、皮肤纹理和颜色的快速变化、非刚性体变形以及频发的遮挡，所有这些因素都进一步受到变化多端的水下光照和浑浊度的影响。", "method": "该论文介绍了一种名为HideAndSeg的新型、极小监督的AI工具，用于分割章鱼视频。HideAndSeg结合了SAM2和一个定制训练的YOLOv11目标检测器。首先，用户提供点坐标来生成初始分割掩模，作为YOLO模型的训练数据。通过提供边界框提示给SAM2，该方法实现了整个流程的自动化。", "result": "HideAndSeg在没有真实世界信息的情况下达到令人满意的性能，减少了分割噪声，并能够在自然环境中的完全遮挡后重新识别和细分章鱼，这是手动提示模型所不能做到的。", "conclusion": "通过减少现实世界场景需要的手动分析工作量，该研究提供了一种实用工具，为更有效地进行野生头足类动物的行为研究铺平道路。"}}
{"id": "2511.04643", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04643", "abs": "https://arxiv.org/abs/2511.04643", "authors": ["Alamgir Munir Qazi", "John P. McCrae", "Jamal Abdul Nasir"], "title": "When retrieval outperforms generation: Dense evidence retrieval for scalable fake news detection", "comment": null, "summary": "The proliferation of misinformation necessitates robust yet computationally\nefficient fact verification systems. While current state-of-the-art approaches\nleverage Large Language Models (LLMs) for generating explanatory rationales,\nthese methods face significant computational barriers and hallucination risks\nin real-world deployments. We present DeReC (Dense Retrieval Classification), a\nlightweight framework that demonstrates how general-purpose text embeddings can\neffectively replace autoregressive LLM-based approaches in fact verification\ntasks. By combining dense retrieval with specialized classification, our system\nachieves better accuracy while being significantly more efficient. DeReC\noutperforms explanation-generating LLMs in efficiency, reducing runtime by 95%\non RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92%\non LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds),\nshowcasing its effectiveness across varying dataset sizes. On the RAWFC\ndataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art\nmethod L-Defense (61.20%). Our results demonstrate that carefully engineered\nretrieval-based systems can match or exceed LLM performance in specialized\ntasks while being significantly more practical for real-world deployment.", "AI": {"tldr": "DeReC凭借密集检索和专门分类，实现在事实验证任务上的高效和准确，优于现有基于LLM的方法。", "motivation": "由于现有基于LLM的方法存在计算负担重和幻觉风险，研究提出了DeReC以实现高效且准确的事实验证。", "method": "DeReC采用密集检索结合专门分类的方法，利用通用文本嵌入来代替自回归LLM方法进行事实验证。", "result": "相比解释生成LLM，DeReC在效率上提高95%和92%，在RAWFC上F1得分为65.58%，超越现有方法L-Defense的61.20%。", "conclusion": "研究表明，精心设计的检索系统在特定任务上可以匹敌甚至超越LLM的表现，并且更适合实际部署。"}}
{"id": "2511.04450", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04450", "abs": "https://arxiv.org/abs/2511.04450", "authors": ["Yaniv Ohayon", "Ofir Itzhak Shahar", "Ohad Ben-Shahar"], "title": "Solving Convex Partition Visual Jigsaw Puzzles", "comment": null, "summary": "Jigsaw puzzle solving requires the rearrangement of unordered pieces into\ntheir original pose in order to reconstruct a coherent whole, often an image,\nand is known to be an intractable problem. While the possible impact of\nautomatic puzzle solvers can be disruptive in various application domains, most\nof the literature has focused on developing solvers for square jigsaw puzzles,\nseverely limiting their practical use. In this work, we significantly expand\nthe types of puzzles handled computationally, focusing on what is known as\nConvex Partitions, a major subset of polygonal puzzles whose pieces are convex.\nWe utilize both geometrical and pictorial compatibilities, introduce a greedy\nsolver, and report several performance measures next to the first benchmark\ndataset of such puzzles.", "AI": {"tldr": "研究扩展了自动拼图求解器的应用范围，处理凸多边形拼图，提出了新的贪婪求解器，并构建了首个基准数据集。", "motivation": "尽管自动拼图求解器在多个应用领域具有潜在的重要影响，但大多数研究集中在方形拼图求解器的开发上，极大地限制了其实际应用。本研究旨在扩展计算处理的拼图类型。", "method": "本研究利用几何和图像兼容性的方法，提出了一种贪婪求解器，并引入了凸多边形拼图的首个基准数据集。", "result": "报告了几种性能衡量指标，并提出了凸多边形拼图的首个基准数据集。", "conclusion": "本研究通过处理凸分区拼图，显著扩展了自动拼图求解器的应用范围。"}}
{"id": "2511.04654", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04654", "abs": "https://arxiv.org/abs/2511.04654", "authors": ["Mohammad Atif Quamar", "Mohammad Areeb"], "title": "Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought Reasoning", "comment": "Presented at the 1st Workshop on Efficient Reasoning (NeurIPS 2025)", "summary": "Chain-of-Thought (CoT) prompting is a key technique for enabling complex\nreasoning in large language models. However, generating full, fixed-length\nrationales is computationally wasteful, inflating both token usage and latency.\nWe introduce LEASH: Logit-Entropy Adaptive Stopping Heuristic, a training-free\ndecoding algorithm that adaptively halts rationale generation. LEASH monitors\ntwo intrinsic signals: the slope of token-level entropy and the improvement in\nthe top-logit margin. It terminates the generation once both signals plateau,\nindicating the model has reached a stable reasoning state. Across four\ninstruction-tuned models on the GSM8K and AQuA-RAT benchmarks, LEASH reduces\naverage token generation by 30--35% and latency by 27%, while incurring a 10\np.p. accuracy drop relative to CoT. LEASH is model-agnostic and requires no\nadditional training or supervision, offering a simple and efficient alternative\nto CoT decoding.", "AI": {"tldr": "本论文提出LEASH算法，它通过监控令牌生成过程中的两个内在信号来自适应地停止生成，从而减少资源消耗，提高推理链生成的效率。", "motivation": "自动生成完整且固定长度的推理链在计算上是浪费资源的，增加了令牌使用量和延迟。我们希望通过LEASH来解决这个问题，使得推理更加高效。", "method": "我们提出了LEASH：一种无训练的解码算法，它能够根据两个内在信号自适应地停止推理链的生成。这两个信号是：逐令牌熵的斜率和顶级令牌边界的改进。当这两个信号趋于平稳时，算法即结束生成，指示模型达到了稳定的推理状态。", "result": "在四个通过指令调整的模型上，LEASH在GSM8K和AQuA-RAT基准测试中的令牌生成平均减少了30-35%，延迟减少了27%，准确率相对CoT下降了10个百分点。", "conclusion": "LEASH是一种无需额外训练或监督就能提高推理链生成效率的方法，为CoT解码提供了一个简单而高效的替代方案。"}}
{"id": "2511.04460", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04460", "abs": "https://arxiv.org/abs/2511.04460", "authors": ["Runqi Qiao", "Qiuna Tan", "Minghan Yang", "Guanting Dong", "Peiqing Yang", "Shiqiang Lang", "Enhui Wan", "Xiaowan Wang", "Yida Xu", "Lan Yang", "Chong Sun", "Chen Li", "Honggang Zhang"], "title": "V-Thinker: Interactive Thinking with Images", "comment": "Working in progress", "summary": "Empowering Large Multimodal Models (LMMs) to deeply integrate image\ninteraction with long-horizon reasoning capabilities remains a long-standing\nchallenge in this field. Recent advances in vision-centric reasoning explore a\npromising \"Thinking with Images\" paradigm for LMMs, marking a shift from\nimage-assisted reasoning to image-interactive thinking. While this milestone\nenables models to focus on fine-grained image regions, progress remains\nconstrained by limited visual tool spaces and task-specific workflow designs.\nTo bridge this gap, we present V-Thinker, a general-purpose multimodal\nreasoning assistant that enables interactive, vision-centric thinking through\nend-to-end reinforcement learning. V-Thinker comprises two key components: (1)\na Data Evolution Flywheel that automatically synthesizes, evolves, and verifies\ninteractive reasoning datasets across three dimensions-diversity, quality, and\ndifficulty; and (2) a Visual Progressive Training Curriculum that first aligns\nperception via point-level supervision, then integrates interactive reasoning\nthrough a two-stage reinforcement learning framework. Furthermore, we introduce\nVTBench, an expert-verified benchmark targeting vision-centric interactive\nreasoning tasks. Extensive experiments demonstrate that V-Thinker consistently\noutperforms strong LMM-based baselines in both general and interactive\nreasoning scenarios, providing valuable insights for advancing\nimage-interactive reasoning applications.", "AI": {"tldr": "本文介绍了V-Thinker，一种旨在通过强化学习来实现互动、视觉中心多模态推理的系统，该系统在各类实验中展示了出色的表现。", "motivation": "该论文旨在克服现有的限制，这些限制包括有限的视觉工具空间和任务特定的工作流程设计。", "method": "该论文提出了V-Thinker，一种通过端到端强化学习实现互动、视觉中心思维的一般多模态推理助手。V-Thinker由两个关键组件组成：数据进化飞轮和视觉渐进训练课程。", "result": "实验表明，V-Thinker在一般的和交互式的推理场景中都优于基于LMM的强基线方法。", "conclusion": "V-Thinker为推动图像互动推理应用的发展提供了有价值的见解。"}}
{"id": "2511.04474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04474", "abs": "https://arxiv.org/abs/2511.04474", "authors": ["Wenwen Li", "Sizhe Wang", "Hyunho Lee", "Chenyan Lu", "Sujit Roy", "Rahul Ramachandran", "Chia-Yu Hsu"], "title": "Landslide Hazard Mapping with Geospatial Foundation Models: Geographical Generalizability, Data Scarcity, and Band Adaptability", "comment": null, "summary": "Landslides cause severe damage to lives, infrastructure, and the environment,\nmaking accurate and timely mapping essential for disaster preparedness and\nresponse. However, conventional deep learning models often struggle when\napplied across different sensors, regions, or under conditions of limited\ntraining data. To address these challenges, we present a three-axis analytical\nframework of sensor, label, and domain for adapting geospatial foundation\nmodels (GeoFMs), focusing on Prithvi-EO-2.0 for landslide mapping. Through a\nseries of experiments, we show that it consistently outperforms task-specific\nCNNs (U-Net, U-Net++), vision transformers (Segformer, SwinV2-B), and other\nGeoFMs (TerraMind, SatMAE). The model, built on global pretraining,\nself-supervision, and adaptable fine-tuning, proved resilient to spectral\nvariation, maintained accuracy under label scarcity, and generalized more\nreliably across diverse datasets and geographic settings. Alongside these\nstrengths, we also highlight remaining challenges such as computational cost\nand the limited availability of reusable AI-ready training data for landslide\nresearch. Overall, our study positions GeoFMs as a step toward more robust and\nscalable approaches for landslide risk reduction and environmental monitoring.", "AI": {"tldr": "本文通过一个涵盖传感器、标签和领域的三轴分析框架来优化地球空间基础模型（GeoFMs），专门应用于Prithvi-EO-2.0进行滑坡制图。该模型在多种实验中表现出色，提升了滑坡预测的准确性和泛化能力，但面临计算成本高等问题。", "motivation": "传统的深度学习模型在应用于不同传感器、区域或训练数据有限的情况下，往往性能不佳。为了应对这些挑战，提出了该方法。", "method": "通过一个涵盖传感器、标签和领域的三轴分析框架来适应地球空间基础模型（GeoFMs），专注于Prithvi-EO-2.0进行滑坡制图。该模型基于全球预训练、自我监督和适应性微调。", "result": "实验结果显示，该模型在对滑坡制图方面，优于特定任务的CNN（U-Net, U-Net++）、视觉变压器（Segformer, SwinV2-B），以及其他GeoFMs（TerraMind, SatMAE）。模型显示了对光谱变异的抵抗力，即使标签稀缺也保持了准确性，并在多种数据集和地理设定间表现出了良好的泛化能力。", "conclusion": "本研究将GeoFMs定位为减少滑坡风险和环境监测更稳健、更可扩展的方法之一，尽管存在计算成本和缺乏可用于滑坡研究的可重复使用的AI训练数据等挑战。"}}
{"id": "2511.04570", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04570", "abs": "https://arxiv.org/abs/2511.04570", "authors": ["Jingqi Tong", "Yurong Mou", "Hangcheng Li", "Mingzhe Li", "Yongzhuo Yang", "Ming Zhang", "Qiguang Chen", "Tianyi Liang", "Xiaomeng Hu", "Yining Zheng", "Xinchi Chen", "Jun Zhao", "Xuanjing Huang", "Xipeng Qiu"], "title": "Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm", "comment": "36 pages, 14 figures", "summary": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly\nimprove the reasoning ability of large language models (LLMs) and Vision\nLanguage Models (VLMs). However, these paradigms have inherent limitations. (1)\nImages capture only single moments and fail to represent dynamic processes or\ncontinuous changes, and (2) The separation of text and vision as distinct\nmodalities, hindering unified multimodal understanding and generation. To\novercome these limitations, we introduce \"Thinking with Video\", a new paradigm\nthat leverages video generation models, such as Sora-2, to bridge visual and\ntextual reasoning in a unified temporal framework. To support this exploration,\nwe developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench\nencompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing\nPuzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our\nevaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,\nSora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even\nsurpasses VLMs on several tasks, such as Eyeballing Games. On text-centric\ntasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.\nFurthermore, we systematically analyse the source of these abilities. We also\nfind that self-consistency and in-context learning can improve Sora-2's\nperformance. In summary, our findings demonstrate that the video generation\nmodel is the potential unified multimodal understanding and generation model,\npositions \"thinking with video\" as a unified multimodal reasoning paradigm.", "AI": {"tldr": "本研究提出“基于视频思考”的新范式，利用视频生成模型（如Sora-2）解决了现有范式在动态过程和多模态理解上的局限性，并通过实验验证了Sora-2在多类型任务上的表现。", "motivation": "此研究的动机是解决“基于文本思考”和“基于图像思考”范式中存在的局限性，包括图像难以表示动态过程或连续变化，以及文本和视觉作为不同模态分离导致的统一多模态理解和生成的障碍。", "method": "本研究提出了“基于视频思考”的新范式，利用视频生成模型（如Sora-2）来统一视觉和文本推理，并在时间框架内进行整合。为此，我们开发了Video Thinking Benchmark（VideoThinkBench），该基准涵盖了两类任务：视觉为中心的任务（如眼力谜题）和文本为中心的任务（如GSM8K、MMMU的子集）。", "result": "评估结果表明，Sora-2在视觉为中心任务中一般可与现有的最先进的VLM模型相媲美，并在某些任务上超过了这些模型，例如眼力游戏。在文本为中心的任务上，Sora-2在MATH任务中达到92%的准确率，在MMMU任务中达到75.53%的准确率。", "conclusion": "研究结果表明，视频生成模型可能成为统一多模态理解和生成的模型，确立了“基于视频思考”作为统一多模态推理范式。"}}
{"id": "2511.04520", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04520", "abs": "https://arxiv.org/abs/2511.04520", "authors": ["Nabyl Quignon", "Baptiste Chopin", "Yaohui Wang", "Antitza Dantcheva"], "title": "THEval. Evaluation Framework for Talking Head Video Generation", "comment": null, "summary": "Video generation has achieved remarkable progress, with generated videos\nincreasingly resembling real ones. However, the rapid advance in generation has\noutpaced the development of adequate evaluation metrics. Currently, the\nassessment of talking head generation primarily relies on limited metrics,\nevaluating general video quality, lip synchronization, and on conducting user\nstudies. Motivated by this, we propose a new evaluation framework comprising 8\nmetrics related to three dimensions (i) quality, (ii) naturalness, and (iii)\nsynchronization. In selecting the metrics, we place emphasis on efficiency, as\nwell as alignment with human preferences. Based on this considerations, we\nstreamline to analyze fine-grained dynamics of head, mouth, and eyebrows, as\nwell as face quality. Our extensive experiments on 85,000 videos generated by\n17 state-of-the-art models suggest that while many algorithms excel in lip\nsynchronization, they face challenges with generating expressiveness and\nartifact-free details. These videos were generated based on a novel real\ndataset, that we have curated, in order to mitigate bias of training data. Our\nproposed benchmark framework is aimed at evaluating the improvement of\ngenerative methods. Original code, dataset and leaderboards will be publicly\nreleased and regularly updated with new methods, in order to reflect progress\nin the field.", "AI": {"tldr": "A new multi-metric framework for evaluating talking head videos is proposed, highlighting the importance of quality, naturalness, and synchronization. The framework addresses the need for more comprehensive evaluation as generative methods advance.", "motivation": "The motivation for this study is the rapid development in video generation technology, which has outpaced the development of proper evaluation metrics. There is a need for a more comprehensive and efficient evaluation framework that can better guide the improvement of generative methods.", "method": "Content covers the background and the rapid progress in video generation, the current evaluation limitation, and introduces a new multi-metric evaluation framework designed for talking head videos focusing on quality, naturalness, and synchronization. The framework is tested on a large number of videos generated by advanced models, highlighting shortcomings in expressiveness and artifact detail.", "result": "The experiments show that many state-of-the-art models perform well in lip synchronization but struggle with generating expressiveness and artifact-free details.", "conclusion": "The new evaluation framework provides a more detailed insight into the performance of talking head video generation methods beyond just lip synchronization. The framework's metrics and the curating of a novel dataset for evaluation aim to push the field forward by providing clear targets for improvement."}}
{"id": "2511.04525", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04525", "abs": "https://arxiv.org/abs/2511.04525", "authors": ["Dimitrios Anastasiou", "Santiago Barbarisi", "Lucy Culshaw", "Jayna Patel", "Evangelos B. Mazomenos", "Imanol Luengo", "Danail Stoyanov"], "title": "Learning from Single Timestamps: Complexity Estimation in Laparoscopic Cholecystectomy", "comment": null, "summary": "Purpose: Accurate assessment of surgical complexity is essential in\nLaparoscopic Cholecystectomy (LC), where severe inflammation is associated with\nlonger operative times and increased risk of postoperative complications. The\nParkland Grading Scale (PGS) provides a clinically validated framework for\nstratifying inflammation severity; however, its automation in surgical videos\nremains largely unexplored, particularly in realistic scenarios where complete\nvideos must be analyzed without prior manual curation. Methods: In this work,\nwe introduce STC-Net, a novel framework for SingleTimestamp-based Complexity\nestimation in LC via the PGS, designed to operate under weak temporal\nsupervision. Unlike prior methods limited to static images or manually trimmed\nclips, STC-Net operates directly on full videos. It jointly performs temporal\nlocalization and grading through a localization, window proposal, and grading\nmodule. We introduce a novel loss formulation combining hard and soft\nlocalization objectives and background-aware grading supervision. Results:\nEvaluated on a private dataset of 1,859 LC videos, STC-Net achieves an accuracy\nof 62.11% and an F1-score of 61.42%, outperforming non-localized baselines by\nover 10% in both metrics and highlighting the effectiveness of weak supervision\nfor surgical complexity assessment. Conclusion: STC-Net demonstrates a scalable\nand effective approach for automated PGS-based surgical complexity estimation\nfrom full LC videos, making it promising for post-operative analysis and\nsurgical training.", "AI": {"tldr": "STC-Net是一个用于基于时间戳的腹腔镜胆囊切除术复杂性评估的框架，该框架能够在较弱的时间监督下从完整手术视频中进行复杂性评估，并在评估人体数据集上显示出优异的性能。", "motivation": "迫切需要有效的自动化手段来评估腹腔镜胆囊切除术中的炎症严重程度，现有的方法无法处理未经手动筛选的完整手术视频。", "method": "STC-Net包括时空定位模块、窗口提议模块和分级模块，通过结合硬定位和软定位目标以及背景感知分级监督损失，来实现对腹腔镜胆囊切除术手术视频中炎症严重程度的自动评估。", "result": "实验结果显示，STC-Net在1,859段LC视频上的准确率为62.11%，F1评分为61.42%，比非定位基础模型在两个指标上都提高了10%以上，证明了这一框架在自动评估手术复杂性方面的有效性。", "conclusion": "STC-Net提出了一种可扩展且有效的框架，能够从完整腹腔镜胆囊切除术视频中自动估计基于PGS的手术复杂性，具有广阔的临床应用前景。"}}
{"id": "2511.04595", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04595", "abs": "https://arxiv.org/abs/2511.04595", "authors": ["Chen Shi", "Shaoshuai Shi", "Xiaoyang Lyu", "Chunyang Liu", "Kehua Sheng", "Bo Zhang", "Li Jiang"], "title": "UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction", "comment": null, "summary": "Feed-forward 3D reconstruction for autonomous driving has advanced rapidly,\nyet existing methods struggle with the joint challenges of sparse,\nnon-overlapping camera views and complex scene dynamics. We present UniSplat, a\ngeneral feed-forward framework that learns robust dynamic scene reconstruction\nthrough unified latent spatio-temporal fusion. UniSplat constructs a 3D latent\nscaffold, a structured representation that captures geometric and semantic\nscene context by leveraging pretrained foundation models. To effectively\nintegrate information across spatial views and temporal frames, we introduce an\nefficient fusion mechanism that operates directly within the 3D scaffold,\nenabling consistent spatio-temporal alignment. To ensure complete and detailed\nreconstructions, we design a dual-branch decoder that generates dynamic-aware\nGaussians from the fused scaffold by combining point-anchored refinement with\nvoxel-based generation, and maintain a persistent memory of static Gaussians to\nenable streaming scene completion beyond current camera coverage. Extensive\nexperiments on real-world datasets demonstrate that UniSplat achieves\nstate-of-the-art performance in novel view synthesis, while providing robust\nand high-quality renderings even for viewpoints outside the original camera\ncoverage.", "AI": {"tldr": "UniSplat是一个能够处理稀疏、非重叠摄像机视图以及复杂动态场景的前馈3D重建框架，通过统一的潜时空融合学习将动态场景进行高精度重建。", "motivation": "尽管前馈3D重建在自动驾驶领域已经取得了迅速发展，但现有方法依然难以应对稀疏、不重叠的摄像机视图和复杂场景动态的联合挑战。因此，提出了UniSplat来解决这些挑战。", "method": "UniSplat方法通过统一的潜在时空融合学习稳健的动态场景重建。该方法首先构建一个3D潜在支架，利用预训练的基础模型捕捉场景的几何和语义上下文。为了有效整合空间视图和时间帧中的信息，引入了一种在3D支架内操作的高效融合机制，实现了一致的空间和时间对齐。设计了双分支解码器，结合点锚定细化与体素生成，从融合的支架中生成动态感知高斯，并保持持久的静态高斯记忆，以实现实时场景补全，超过当前摄像头视图范围。", "result": "实验结果表明，UniSplat在新颖视角合成方面达到了最先进的性能，并且即使对于超出原始摄像头覆盖范围的视角，也提供了强大而高质量的渲染。", "conclusion": "该研究证实了UniSplat在复杂场景下的高效重建能力，特别是在动态场景中，展示了其在自动驾驶领域前馈3D重建方面的优势。"}}
{"id": "2511.04601", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.04601", "abs": "https://arxiv.org/abs/2511.04601", "authors": ["Yicheng Xiao", "Yu Chen", "Haoxuan Ma", "Jiale Hong", "Caorui Li", "Lingxiang Wu", "Haiyun Guo", "Jinqiao Wang"], "title": "PixCLIP: Achieving Fine-grained Visual Language Understanding via Any-granularity Pixel-Text Alignment Learning", "comment": null, "summary": "While the Contrastive Language-Image Pretraining(CLIP) model has achieved\nremarkable success in a variety of downstream vison language understanding\ntasks, enhancing its capability for fine-grained image-text alignment remains\nan active research focus. To this end, most existing works adopt the strategy\nof explicitly increasing the granularity of visual information processing,\ne.g., incorporating visual prompts to guide the model focus on specific local\nregions within the image. Meanwhile, researches on Multimodal Large Language\nModels(MLLMs) have demonstrated that training with long and detailed textual\ndescriptions can effectively improve the model's fine-grained vision-language\nalignment. However, the inherent token length limitation of CLIP's text encoder\nfundamentally limits CLIP to process more granular textual information embedded\nin long text sequences. To synergistically leverage the advantages of enhancing\nboth visual and textual content processing granularity, we propose PixCLIP, a\nnovel framework designed to concurrently accommodate visual prompt inputs and\nprocess lengthy textual descriptions. Specifically, we first establish an\nautomated annotation pipeline capable of generating pixel-level localized,\nlong-form textual descriptions for images. Utilizing this pipeline, we\nconstruct LongGRIT, a high-quality dataset comprising nearly 1.5 million\nsamples. Secondly, we replace CLIP's original text encoder with the LLM and\npropose a three-branch pixel-text alignment learning framework, facilitating\nfine-grained alignment between image regions and corresponding textual\ndescriptions at arbitrary granularity. Experiments demonstrate that PixCLIP\nshowcases breakthroughs in pixel-level interaction and handling long-form\ntexts, achieving state-of-the-art performance.", "AI": {"tldr": "本文提出了PixCLIP框架，用于改进CLIP模型在细粒度图像文本对齐方面的性能。", "motivation": "尽管CLIP模型在多种下游视觉语言理解任务中表现优异，但提高其对细粒度图像文本对齐能力仍然是一个活跃的研究方向。现有研究主要通过增加视觉信息处理的粒度来实现，而关于多模态大型语言模型的研究表明，使用长而详细的文本描述可以有效提升模型的细粒度视觉-语言对齐效果。然而，CLIP文本编码器固有的令牌长度限制阻碍了其处理长文本描述中的细粒度文本信息的能力。", "method": "我们提出了一个名为PixCLIP的新框架，该框架能够同时处理视觉提示输入和长文本描述。我们首先建立了一个自动化注释管道，可以生成像素级的长文本描述。然后，我们用大型语言模型（LLM）替换了CLIP的原始文本编码器，并提出了一个三分支像素-文本对齐学习框架，以实现图像区域和其对应文本描述之间的细粒度对齐。", "result": "实验表明，PixCLIP在像素级别交互和长文本处理方面取得了突破性进展，达到了最先进的性能水平。", "conclusion": "PixCLIP框架通过结合视觉和文本内容处理的粒度增强优势，实现了像素级的交互和长文本处理能力的显著提升，达到了目前最先进的性能水平。"}}
{"id": "2511.04615", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04615", "abs": "https://arxiv.org/abs/2511.04615", "authors": ["Tushar Kataria", "Shikha Dubey", "Mary Bronner", "Jolanta Jedrzkiewicz", "Ben J. Brintz", "Shireen Y. Elhabian", "Beatrice S. Knudsen"], "title": "Building Trust in Virtual Immunohistochemistry: Automated Assessment of Image Quality", "comment": null, "summary": "Deep learning models can generate virtual immunohistochemistry (IHC) stains\nfrom hematoxylin and eosin (H&E) images, offering a scalable and low-cost\nalternative to laboratory IHC. However, reliable evaluation of image quality\nremains a challenge as current texture- and distribution-based metrics quantify\nimage fidelity rather than the accuracy of IHC staining. Here, we introduce an\nautomated and accuracy grounded framework to determine image quality across\nsixteen paired or unpaired image translation models. Using color deconvolution,\nwe generate masks of pixels stained brown (i.e., IHC-positive) as predicted by\neach virtual IHC model. We use the segmented masks of real and virtual IHC to\ncompute stain accuracy metrics (Dice, IoU, Hausdorff distance) that directly\nquantify correct pixel - level labeling without needing expert manual\nannotations. Our results demonstrate that conventional image fidelity metrics,\nincluding Frechet Inception Distance (FID), peak signal-to-noise ratio (PSNR),\nand structural similarity (SSIM), correlate poorly with stain accuracy and\npathologist assessment. Paired models such as PyramidPix2Pix and AdaptiveNCE\nachieve the highest stain accuracy, whereas unpaired diffusion- and GAN-based\nmodels are less reliable in providing accurate IHC positive pixel labels.\nMoreover, whole-slide images (WSI) reveal performance declines that are\ninvisible in patch-based evaluations, emphasizing the need for WSI-level\nbenchmarks. Together, this framework defines a reproducible approach for\nassessing the quality of virtual IHC models, a critical step to accelerate\ntranslation towards routine use by pathologists.", "AI": {"tldr": "研究人员提出了一种自动且基于准确性的框架，用于评估六种配对或非配对图像翻译模型生成的虚拟免疫组化(IHC)染色图像的质量。通过使用颜色分离生成的真实和虚拟IHC图像的分割掩模，他们计算出了可以量化像素级别正确的IHC染色准确性指标。研究发现，传统的图像保真度指标与染色准确性相关性较差，而配对模型如PyramidPix2Pix和AdaptiveNCE在染色准确性上表现最佳。除此之外，全幅图像级别的评估显示了在切片级别的评估下不易发现的性能下降问题。", "motivation": "研究旨在解决深度学习模型生成虚拟IHC染色图像时，传统基于纹理和分布的图像质量评估指标无法准确反映IHC染色准确性的问题。", "method": "通过颜色分离生成真实和虚拟IHC图像中棕色(即阳性)染色像素的掩模，并使用这些掩模计算染色准确性指标(如Dice、IoU、Hausdorff距离)，以直接量化正确的像素级别标签，而无需专家的手动注释。", "result": "研究发现，传统图像保真度指标，如FID、PSNR和SSIM，与染色准确性和病理学家评估的相关性较差。配对模型如PyramidPix2Pix和AdaptiveNCE在染色准确性上表现最佳，而基于扩散和生成对抗网络的非配对模型提供准确的IHC阳性像素标签的可靠性较低。在全幅图像级别的评估中，性能下降的现象更加明显。", "conclusion": "该框架提供了一种可重复的评估虚拟IHC模型质量的方法，这是加速向病理学家的常规应用转化的关键步骤。"}}
{"id": "2511.04628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04628", "abs": "https://arxiv.org/abs/2511.04628", "authors": ["Kylie Cancilla", "Alexander Moore", "Amar Saini", "Carmen Carrano"], "title": "NovisVQ: A Streaming Convolutional Neural Network for No-Reference Opinion-Unaware Frame Quality Assessment", "comment": null, "summary": "Video quality assessment (VQA) is vital for computer vision tasks, but\nexisting approaches face major limitations: full-reference (FR) metrics require\nclean reference videos, and most no-reference (NR) models depend on training on\ncostly human opinion labels. Moreover, most opinion-unaware NR methods are\nimage-based, ignoring temporal context critical for video object detection. In\nthis work, we present a scalable, streaming-based VQA model that is both\nno-reference and opinion-unaware. Our model leverages synthetic degradations of\nthe DAVIS dataset, training a temporal-aware convolutional architecture to\npredict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, without\nreferences at inference. We show that our streaming approach outperforms our\nown image-based baseline by generalizing across diverse degradations,\nunderscoring the value of temporal modeling for scalable VQA in real-world\nvision systems. Additionally, we demonstrate that our model achieves higher\ncorrelation with full-reference metrics compared to BRISQUE, a widely-used\nopinion-aware image quality assessment baseline, validating the effectiveness\nof our temporal, opinion-unaware approach.", "AI": {"tldr": "研究开发了一种基于流的无参考无观感偏见的VQA模型，用于对退化视频的质量进行评估，模型表现优于现有的方法。", "motivation": "尽管视频质量评估对于计算机视觉任务至关重要，但现有的FR（全参考）方法需要清洁的参考视频，大多数NR（无参考）模型依赖于昂贵的人类意见标签进行训练。此外，大多数未考虑观感的NR方法基于图像处理，忽略了对视频目标检测至关重要的时间上下文。", "method": "本研究提出了一种可扩展的、基于流的无参考无主观偏见的视频质量评估（VQA）模型。该模型利用DAVIS数据集中的合成退化数据训练了一个时间感知卷积架构，可以直接从退化的视频中预测全参考指标（LPIPS、PSNR、SSIM），而在推理时不依赖于参考视频。", "result": "研究表明，与基于图像的基线模型相比，该方法对多种退化情况的泛化能力更强，证明了在现实世界的视觉系统中实现可扩展VQA时，时间建模的价值。此外，该模型与全参考指标的相关性也优于广泛使用的基于主观洞察的图像质量评估基准BRISQUE。", "conclusion": "这项工作介绍了一种新的VQA模型，它在广泛的退化情况下表现更好，并展示出与全参考指标和现有方法相比更高的相关性，证明了本研究方法的有效性。"}}
{"id": "2511.04652", "categories": ["cs.CV", "physics.optics"], "pdf": "https://arxiv.org/pdf/2511.04652", "abs": "https://arxiv.org/abs/2511.04652", "authors": ["Mantas Žurauskas", "Tom Bu", "Sanaz Alali", "Beyza Kalkanli", "Derek Shi", "Fernando Alamos", "Gauresh Pandit", "Christopher Mei", "Ali Behrooz", "Ramin Mirjalili", "Dave Stronks", "Alexander Fix", "Dmitri Model"], "title": "Polarization-resolved imaging improves eye tracking", "comment": null, "summary": "Polarization-resolved near-infrared imaging adds a useful optical contrast\nmechanism to eye tracking by measuring the polarization state of light\nreflected by ocular tissues in addition to its intensity. In this paper we\ndemonstrate how this contrast can be used to enable eye tracking. Specifically,\nwe demonstrate that a polarization-enabled eye tracking (PET) system composed\nof a polarization--filter--array camera paired with a linearly polarized\nnear-infrared illuminator can reveal trackable features across the sclera and\ngaze-informative patterns on the cornea, largely absent in intensity-only\nimages. Across a cohort of 346 participants, convolutional neural network based\nmachine learning models trained on data from PET reduced the median\n95th-percentile absolute gaze error by 10--16\\% relative to capacity-matched\nintensity baselines under nominal conditions and in the presence of eyelid\nocclusions, eye-relief changes, and pupil-size variation. These results link\nlight--tissue polarization effects to practical gains in human--computer\ninteraction and position PET as a simple, robust sensing modality for future\nwearable devices.", "AI": {"tldr": "本研究利用偏振光成像技术对眼球进行追踪，实验结果表明使用该技术的眼球追踪系统在多种条件下比传统方法更精准。", "motivation": "为了提高眼球跟踪技术的准确性和鲁棒性，特别是在存在诸如眼睑遮挡、眼距变化和瞳孔大小变化等条件下的表现，本研究旨在利用偏振光特性来增强眼球追踪系统的性能。", "method": "本研究提出了一种名为极化启用的眼球追踪系统（PET），该系统通过结合使用偏振滤镜阵列相机和线性偏振近红外光源，能够在眼睑遮挡、眼距变化和瞳孔大小变化的情况下提取可追踪特征。", "result": "在对346名参与者的数据进行测试时，基于卷积神经网络训练的模型将95%分位数的绝对注视误差相对基线降低了10-16%。", "conclusion": "本研究证实了偏振光特性在眼球追踪中的实际应用价值，并提出这种技术可以作为一种简单且鲁棒的传感方式，适用于未来的可穿戴设备。"}}
{"id": "2511.04655", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04655", "abs": "https://arxiv.org/abs/2511.04655", "authors": ["Ellis Brown", "Jihan Yang", "Shusheng Yang", "Rob Fergus", "Saining Xie"], "title": "Benchmark Designers Should \"Train on the Test Set\" to Expose Exploitable Non-Visual Shortcuts", "comment": "Project page: https://cambrian-mllm.github.io", "summary": "Robust benchmarks are crucial for evaluating Multimodal Large Language Models\n(MLLMs). Yet we find that models can ace many multimodal benchmarks without\nstrong visual understanding, instead exploiting biases, linguistic priors, and\nsuperficial patterns. This is especially problematic for vision-centric\nbenchmarks that are meant to require visual inputs. We adopt a diagnostic\nprinciple for benchmark design: if a benchmark can be gamed, it will be.\nDesigners should therefore try to ``game'' their own benchmarks first, using\ndiagnostic and debiasing procedures to systematically identify and mitigate\nnon-visual biases. Effective diagnosis requires directly ``training on the test\nset'' -- probing the released test set for its intrinsic, exploitable patterns.\n  We operationalize this standard with two components. First, we diagnose\nbenchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology.\nOur primary diagnostic tool involves fine-tuning a powerful Large Language\nModel via $k$-fold cross-validation on exclusively the non-visual, textual\ninputs of the test set to reveal shortcut performance and assign each sample a\nbias score $s(x)$. We complement this with a lightweight Random Forest-based\ndiagnostic operating on hand-crafted features for fast, interpretable auditing.\nSecond, we debias benchmarks by filtering high-bias samples using an\n``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four\nbenchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive\nnon-visual biases. As a case study, we apply our full framework to create\nVSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider\nvision-blind performance gap than the original.", "AI": {"tldr": "本文提出了一种用于评估多模态大规模语言模型(MLLMs)的基准测试设计方法，该方法包括诊断和消除非视觉偏差，以确保模型具有真正的视觉理解能力。", "motivation": "作者发现，模型可以在许多多模态基准测试中表现良好，但这并不意味着它们具有强大的视觉理解力，而是因为它们利用了偏差、语言先验和表面模式。这对需要视觉输入的以视觉为中心的基准测试来说尤其成问题。", "method": "采用诊断原则设计基准测试：如果基准测试可以被操纵，它就会被操纵。为了实现这一标准，作者采用了两种方法。首先，他们通过\"测试集压力测试\"（TsT）方法诊断基准测试的易受攻击性。其次，他们通过\"迭代偏差修剪\"（IBP）程序消减基准测试中的偏差。", "result": "通过对四个基准测试（VSI-Bench、CV-Bench、MMMU和VideoMME）的应用，作者发现普遍存在非视觉偏差，并创建了去偏的VSI-Bench-Debiased版本，其结果展示出更低的非视觉可解性和更广阔的视觉盲性能差距。", "conclusion": "本研究展示了一种有效的诊断和去偏方法，可用于改进多模态基准测试的设计，确保它们能够真实反映模型的视觉理解能力，阻止模型仅通过利用偏差和非视觉信息获得高分。"}}
