<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 8]
- [cs.CV](#cs.CV) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Incentives or Ontology? A Structural Rebuttal to OpenAI's Hallucination Thesis](https://arxiv.org/abs/2512.14801)
*Richard Ackermann,Simeon Emanuilov*

Main category: cs.CL

> The paper argues that hallucination in language models like transformers is not due to misaligned incentives but a result of their structural design. It can only be mitigated through external mechanisms, not internal tuning.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this paper is to challenge the view promoted by OpenAI that hallucinations in language models are caused by misaligned evaluation incentives, proposing instead that they are an architectural inevitability of transformers.

**Method:** Drawing on previous work on structural hallucination and empirical experiments using a Licensing Oracle, the authors argue against the view that hallucination in large language models is due to misaligned evaluation incentives. They aim to show that hallucination is a structural property of the transformer model itself due to its reliance on statistical associations among tokens rather than world-referential structure.

**Result:** Empirical results demonstrate that eliminating hallucination requires external truth-validation and abstention modules, as indicated by the perfect abstention precision of the Licensing Oracle across domains.

**Conclusion:** The conclusion is that hallucination is a structural property of the language models and reliable AI will require hybrid systems that separate linguistic fluency from the responsibility for truth in response.

**Abstract:** OpenAI has recently argued that hallucinations in large language models result primarily from misaligned evaluation incentives that reward confident guessing rather than epistemic humility. On this view, hallucination is a contingent behavioral artifact, remediable through improved benchmarks and reward structures. In this paper, we challenge that interpretation. Drawing on previous work on structural hallucination and empirical experiments using a Licensing Oracle, we argue that hallucination is not an optimization failure but an architectural inevitability of the transformer model.
  Transformers do not represent the world; they model statistical associations among tokens. Their embedding spaces form a pseudo-ontology derived from linguistic co-occurrence rather than world-referential structure. At ontological boundary conditions - regions where training data is sparse or incoherent - the model necessarily interpolates fictional continuations in order to preserve coherence. No incentive mechanism can modify this structural dependence on pattern completion.
  Our empirical results demonstrate that hallucination can only be eliminated through external truth-validation and abstention modules, not through changes to incentives, prompting, or fine-tuning. The Licensing Oracle achieves perfect abstention precision across domains precisely because it supplies grounding that the transformer lacks.
  We conclude that hallucination is a structural property of generative architectures and that reliable AI requires hybrid systems that distinguish linguistic fluency from epistemic responsibility.

</details>


### [2] [T5Gemma 2: Seeing, Reading, and Understanding Longer](https://arxiv.org/abs/2512.14856)
*Biao Zhang,Paul Suganthan,Gaël Liu,Ilya Philippov,Sahil Dua,Ben Hora,Kat Black,Gus Martins,Omar Sanseviero,Shreya Pathak,Cassidy Hardin,Francesco Visin,Jiageng Zhang,Kathleen Kenealy,Qin Yin,Olivier Lacombe,Armand Joulin,Tris Warkentin,Adam Roberts*

Main category: cs.CL

> T5Gemma 2模型支持多语言、多模态和长上下文处理，通过特定方法转化并优化，性能优于其前身。

<details>
  <summary>Details</summary>

**Motivation:** 研发新一代轻量级开放编码器-解码器模型，拥有强大的多语言、多模态和长上下文处理能力。

**Method:** 通过UL2方法将预先训练的仅解码器模型转化为编码器-解码器模型，并扩展到多模态处理。同时提出了两种提高效率的方法：绑定词嵌入和合并注意力模块。

**Result:** 实验表明了适应策略在不同架构和模态中的普适性和编码器-解码器结构在长上下文建模中的独特优势。与Gemma 3相比，T5Gemma 2在预训练和后训练性能上有相似或更好的表现。

**Conclusion:** T5Gemma 2模型的发布为未来的深度学习研究提供了有价值的资源。

**Abstract:** We introduce T5Gemma 2, the next generation of the T5Gemma family of lightweight open encoder-decoder models, featuring strong multilingual, multimodal and long-context capabilities. T5Gemma 2 follows the adaptation recipe (via UL2) in T5Gemma -- adapting a pretrained decoder-only model into an encoder-decoder model, and extends it from text-only regime to multimodal based on the Gemma 3 models. We further propose two methods to improve the efficiency: tied word embedding that shares all embeddings across encoder and decoder, and merged attention that unifies decoder self- and cross-attention into a single joint module. Experiments demonstrate the generality of the adaptation strategy over architectures and modalities as well as the unique strength of the encoder-decoder architecture on long context modeling. Similar to T5Gemma, T5Gemma 2 yields comparable or better pretraining performance and significantly improved post-training performance than its Gemma 3 counterpart. We release the pretrained models (270M-270M, 1B-1B and 4B-4B) to the community for future research.

</details>


### [3] [Integrating Large Language Models and Knowledge Graphs to Capture Political Viewpoints in News Media](https://arxiv.org/abs/2512.14887)
*Massimiliano Fadda,Enrico Motta,Francesco Osborne,Diego Reforgiato Recupero,Angelo Salatino*

Main category: cs.CL

> 本文改进了一个之前提出的方法，用于分析新闻语料中的观点和声明分类，通过微调LLMs和使用Wikidata来提高分类性能，特别是在处理长输入时。

<details>
  <summary>Details</summary>

**Motivation:** 理解和分析新闻源如何通过特定的主题、观点和声音塑造政治和社会讨论对于评估媒体景观是否提供了一个平衡和公正的公共辩论图景至关重要。

**Method:** 通过微调大型语言模型（LLMs）进行观点分类，并通过Wikidata中的相关演员的语义描述丰富声明表示，改进了之前的工作中的管道。

**Result:** 结果表明，虽然这两个机制独立地提高了分类性能，但它们的整合产生了最佳结果，尤其是在处理长输入的LLMs中。

**Conclusion:** 改进的方法证明了在处理复杂的公共辩论时，如英国移民辩论，能够提供更精确的观点和声明分类。

**Abstract:** News sources play a central role in democratic societies by shaping political and social discourse through specific topics, viewpoints and voices. Understanding these dynamics is essential for assessing whether the media landscape offers a balanced and fair account of public debate. In earlier work, we introduced a pipeline that, given a news corpus, i) uses a hybrid human-machine approach to identify the range of viewpoints expressed about a given topic, and ii) classifies relevant claims with respect to the identified viewpoints, defined as sets of semantically and ideologically congruent claims (e.g., positions arguing that immigration positively impacts the UK economy). In this paper, we improve this pipeline by i) fine-tuning Large Language Models (LLMs) for viewpoint classification and ii) enriching claim representations with semantic descriptions of relevant actors drawn from Wikidata. We evaluate our approach against alternative solutions on a benchmark centred on the UK immigration debate. Results show that while both mechanisms independently improve classification performance, their integration yields the best results, particularly when using LLMs capable of processing long inputs.

</details>


### [4] [DrugRAG: Enhancing Pharmacy LLM Performance Through A Novel Retrieval-Augmented Generation Pipeline](https://arxiv.org/abs/2512.14896)
*Houman Kazemzadeh,Kiarash Mokhtari Dizaji,Seyed Reza Tavakoli,Farbod Davoodi,MohammadReza KarimiNejad,Parham Abed Azad,Ali Sabzi,Armin Khosravi,Siavash Ahmadi,Mohammad Hossein Rohban,Glolamali Aminian,Tahereh Javaheri*

Main category: cs.CL

> TLDR: 该论文评估了大型语言模型在药学问题解答上的表现，并开发了一种无需修改模型即可提高准确性的外部知识集成方法，名为DrugRAG，该方法能显著提升模型的准确性。

<details>
  <summary>Details</summary>

**Motivation:** Motivation: 研究的动机是评估大型语言模型在类似药学执照考试的问题解答任务上的性能，并开发一种集成外部知识的方法以提高其准确性。

**Method:** Methods: 研究团队使用包含141个问题的药学数据集对11个不同参数规模（从80亿到700亿以上）的现有大型语言模型进行了基准测试。初步测量了未经修改的每个模型的准确性。随后，他们开发了一个三步检索增强生成（RAG）管道，即DrugRAG，从验证过的来源检索结构化的药物知识，并将基于证据的上下文添加到模型提示中。此管道在模型之外运行，无需对模型架构或参数进行任何更改。

**Result:** Results: 初始准确性范围在46%到92%之间，其中GPT-5（92%）和o3（89%）得分最高。参数少于80亿的模型得分低于50%。DrugRAG提升所有测试模型的准确性，增益范围在7到21个百分点之间（例如，Gemma 3 27B从61%提高至71%，Llama 3.1 8B从46%提高至67%）。

**Conclusion:** Conclusion: 该研究展示了通过DrugRAG集成外部结构化药物知识在不修改基础模型的情况下显著提升大型语言模型在药学任务上的准确性。这种方法为药学导向的人工智能应用程序提供了一个实用的管道，以便利用基于证据的信息。

**Abstract:** Objectives: To evaluate large language model (LLM) performance on pharmacy licensure-style question-answering (QA) tasks and develop an external knowledge integration method to improve their accuracy.
  Methods: We benchmarked eleven existing LLMs with varying parameter sizes (8 billion to 70+ billion) using a 141-question pharmacy dataset. We measured baseline accuracy for each model without modification. We then developed a three-step retrieval-augmented generation (RAG) pipeline, DrugRAG, that retrieves structured drug knowledge from validated sources and augments model prompts with evidence-based context. This pipeline operates externally to the models, requiring no changes to model architecture or parameters.
  Results: Baseline accuracy ranged from 46% to 92%, with GPT-5 (92%) and o3 (89%) achieving the highest scores. Models with fewer than 8 billion parameters scored below 50%. DrugRAG improved accuracy across all tested models, with gains ranging from 7 to 21 percentage points (e.g., Gemma 3 27B: 61% to 71%, Llama 3.1 8B: 46% to 67%) on the 141-item benchmark.
  Conclusion: We demonstrate that external structured drug knowledge integration through DrugRAG measurably improves LLM accuracy on pharmacy tasks without modifying the underlying models. This approach provides a practical pipeline for enhancing pharmacy-focused AI applications with evidence-based information.

</details>


### [5] [Multiscale Aggregated Hierarchical Attention (MAHA): A Game Theoretic and Optimization Driven Approach to Efficient Contextual Modeling in Large Language Models](https://arxiv.org/abs/2512.14925)
*Caner Erden*

Main category: cs.CL

> 提出了一种新型的架构框架 Multiscale Aggregated Hierarchical Attention (MAHA)，通过分层分解和数学上严格的聚合重新制定了注意力机制，解决了现有的稀疏和线性注意力机制在处理长上下文任务时计算复杂性问题。

<details>
  <summary>Details</summary>

**Motivation:** MultiHead SelfAttention (MHSA) 的二次计算复杂性仍然是扩展大型语言模型 (LLMs) 以处理长上下文任务的基本瓶颈。虽然稀疏和线性化的注意力机制试图缓解这个问题，但它们通常会牺牲全局依赖关系的表示，或者无法有效捕捉多尺度的语义细节。

**Method:** Multiscale Aggregated Hierarchical Attention (MAHA)，通过分层分解和严格的数学聚合重新制定了注意力机制。与传统方法不同，MAHA 通过可学习的下采样运算符动态地将输入序列划分为多个尺度。其核心创新在于其聚合策略：将尺度特定注意力矩阵的融合建模为资源分配问题，通过凸优化框架或纳什均衡博弈论方法解决。这确保了在局部细节和全局上下文精度之间的理论上最优平衡。

**Result:** 实验评估表明，MAHA 在可扩展性方面表现优异；经验浮点运算次数分析证实了在序列长度为 4096 的情况下与标准注意相比计算成本降低了 81%。

**Conclusion:** 这项工作架起了优化理论与序列建模之间的桥梁，为下一代 LLMs 提供了一种可扩展的解决方案。

**Abstract:** The quadratic computational complexity of MultiHead SelfAttention (MHSA) remains a fundamental bottleneck in scaling Large Language Models (LLMs) for longcontext tasks. While sparse and linearized attention mechanisms attempt to mitigate this, they often compromise the representation of global dependencies or fail to capture multiscale semantic granularity effectively. In this paper, we propose Multiscale Aggregated Hierarchical Attention (MAHA), a novel architectural framework that reformulates the attention mechanism through hierarchical decomposition and mathematically rigorous aggregation. Unlike conventional approaches that treat token interactions at a single resolution, MAHA dynamically partitions the input sequence into hierarchical scales via learnable downsampling operators. The core innovation lies in its aggregation strategy: we model the fusion of scalespecific attention matrices as a resource allocation problem, solved via a convex optimization framework or a Nash equilibriumbased gametheoretic approach. This ensures a theoretically optimal balance between local nuance and global context fidelity. Implemented within a hybrid dilatedconvolutional transformer backbone, MAHA utilizes differentiable optimization layers to enable endtoend training. Experimental evaluations demonstrate that MAHA achieves superior scalability; empirical FLOPs analysis confirms an 81% reduction in computational cost at a sequence length of 4096 compared to standard attention. This work bridges the gap between optimization theory and sequence modeling, offering a scalable solution for nextgeneration LLMs.

</details>


### [6] [Parameter Efficient Multimodal Instruction Tuning for Romanian Vision Language Models](https://arxiv.org/abs/2512.14926)
*George-Andrei Dima,Dumitru-Clementin Cercel*

Main category: cs.CL

> 本文致力于减少罗马尼亚语多模态NLP资源的差距，通过翻译和扩展数据集，并微调VLMs模型，展示了在视觉问答和图像描述生成任务上的提升。

<details>
  <summary>Details</summary>

**Motivation:** 本文专注于减少罗马尼亚语多模态NLP资源的差距，以促进生成AI的普及。

**Method:** 我们将广为人知的Flickr30k数据集翻译成罗马尼亚语，并进一步扩展用于视觉问题回答，利用开源的大语言模型(LLMs)。我们通过在罗马尼亚语视觉问题回答上微调开源VLMs来展示数据集的实用性。我们选择了来自三个广泛使用的模型家族的VLMs：LLaMA 3.2、LLaVA 1.6 和 Qwen2。对于微调，我们采用了参数高效的LoRA方法。

**Result:** 我们的模型在罗马尼亚语上的视觉问答任务上表现出色，并且在未训练的任务上也有所提升，例如罗马尼亚语图像描述生成。七亿参数的Qwen2-VL-RoVQA在两个任务上均取得最高评分，相比于其原始版本，在BERTScore F1上分别提高了+6.05%和+2.61%。最终，模型显示出显著减少了语法错误，表明在语言理解和罗马尼亚语流畅性方面都有所提升。

**Conclusion:** 研究结果表明，通过语法错误的减少，模型不仅在语言理解上有所改进，而且在罗马尼亚语的流畅性方面也有所提升。

**Abstract:** Focusing on low-resource languages is an essential step toward democratizing generative AI. In this work, we contribute to reducing the multimodal NLP resource gap for Romanian. We translate the widely known Flickr30k dataset into Romanian and further extend it for visual question answering by leveraging open-source LLMs. We demonstrate the usefulness of our datasets by fine-tuning open-source VLMs on Romanian visual question answering. We select VLMs from three widely used model families: LLaMA 3.2, LLaVA 1.6, and Qwen2. For fine-tuning, we employ the parameter-efficient LoRA method. Our models show improved Romanian capabilities in visual QA, as well as on tasks they were not trained on, such as Romanian image description generation. The seven-billion-parameter Qwen2-VL-RoVQA obtains top scores on both tasks, with improvements of +6.05% and +2.61% in BERTScore F1 over its original version. Finally, the models show substantial reductions in grammatical errors compared to their original forms, indicating improvements not only in language understanding but also in Romanian fluency.

</details>


### [7] [Cross-Tokenizer Likelihood Scoring Algorithms for Language Model Distillation](https://arxiv.org/abs/2512.14954)
*Buu Phan,Ashish Khisti,Karen Ullrich*

Main category: cs.CL

> 本研究通过构建跨分词器的可能性评分框架，解决了在不同词汇表之间进行语言模型蒸馏时遇到的问题，尤其在边缘设备部署场景中，能够降低内存占用和提高模型准确性。

<details>
  <summary>Details</summary>

**Motivation:** 解决在知识蒸馏等训练范式中，教师和学生语言模型使用不同分词器时，计算下一个词汇可能性比率的挑战。当边缘设备部署需要较小的词汇表以降低内存开销时，此问题变得更加明显。

**Method:** 通过揭示Byte-Pair Encoding (BPE)算法中隐含的递归结构，并利用该结构建立跨分词器可能性评分的框架，解决了大小不同的词汇表之间的对齐问题。该方法适用于两种情况：学生词汇表是教师模型词汇表的子集，以及一般情况下的任意词汇表。

**Result:** 在学生词汇表是教师模型词汇表子集的情况下，该框架可以计算精确的可能性并提供顺序采样的下一个词汇概率，每个词只需要O(1)的模型评估次数。应用于蒸馏时，该方法可以将Qwen2.5-1.5B模型的内存占用减少高达12%，同时在评估任务中提高基线性能4%。在一般情况下，提出了一种严谨的无损过程，并辅以快速的近似方法，使得大词汇表情况变得实用。这种方法提高了GSM8K数学推理准确率超过2%。

**Conclusion:** 提出的方法解决了不同大小词汇表之间的对齐问题，既适用于学生词汇表为教师模型词汇表子集的情况，也适用于一般情况下的任意词汇表。这一方法显著提高了在数学推理任务上的表现，并降低了模型的内存开销。

**Abstract:** Computing next-token likelihood ratios between two language models (LMs) is a standard task in training paradigms such as knowledge distillation. Since this requires both models to share the same probability space, it becomes challenging when the teacher and student LMs use different tokenizers, for instance, when edge-device deployment necessitates a smaller vocabulary size to lower memory overhead. In this work, we address this vocabulary misalignment problem by uncovering an implicit recursive structure in the commonly deployed Byte-Pair Encoding (BPE) algorithm and utilizing it to create a probabilistic framework for cross-tokenizer likelihood scoring. Our method enables sequence likelihood evaluation for vocabularies different from the teacher model native tokenizer, addressing two specific scenarios: when the student vocabulary is a subset of the teacher vocabulary, and the general case where it is arbitrary. In the subset regime, our framework computes exact likelihoods and provides next-token probabilities for sequential sampling with only O(1) model evaluations per token. When used for distillation, this yields up to a 12% reduction in memory footprint for the Qwen2.5-1.5B model while also improving baseline performance up to 4% on the evaluated tasks. For the general case, we introduce a rigorous lossless procedure that leverages BPE recursive structure, complemented by a fast approximation that keeps large-vocabulary settings practical. Applied to distillation for mathematical reasoning, our approach improves GSM8K accuracy by more than 2% over the current state of the art.

</details>


### [8] [Evaluating Large Language Models on Multimodal Chemistry Olympiad Exams](https://arxiv.org/abs/2512.14989)
*Yiming Cui,Xin Yao,Yuxuan Qin,Xin Li,Shijin Wang,Guoping Hu*

Main category: cs.CL

> 通过对40个多模态LLM进行评估，发现这些模型在处理化学奥林匹克问题时存在挑战，尤其是模态融合能力不足，但chain-of-thought 提示法能够提高模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 研究目的在于评估当前多模态大语言模型在处理复杂的多模态输入（特别是化学问题中的符号图、分子结构和结构化视觉数据）的能力。

**Method:** 系统评估了40个专有和开源多模态大语言模型在化学奥林匹克问题集上的表现，这些问题需要整合视觉和文本进行推理。

**Result:** 研究发现许多模型在模态融合方面存在困难，移除图像有时会提高准确性。chain-of-thought 提示法能够一致地提高准确性和视觉推理能力。

**Conclusion:** 这项研究表明当前多模态大语言模型在科学推理能力上的局限性，提出了提升模型解释性和鲁棒性的策略，并提供了一个及时的基准来衡量特定领域的多模态AI进展。

**Abstract:** Multimodal scientific reasoning remains a significant challenge for large language models (LLMs), particularly in chemistry, where problem-solving relies on symbolic diagrams, molecular structures, and structured visual data. Here, we systematically evaluate 40 proprietary and open-source multimodal LLMs, including GPT-5, o3, Gemini-2.5-Pro, and Qwen2.5-VL, on a curated benchmark of Olympiad-style chemistry questions drawn from over two decades of U.S. National Chemistry Olympiad (USNCO) exams. These questions require integrated visual and textual reasoning across diverse modalities. We find that many models struggle with modality fusion, where in some cases, removing the image even improves accuracy, indicating misalignment in vision-language integration. Chain-of-Thought prompting consistently enhances both accuracy and visual grounding, as demonstrated through ablation studies and occlusion-based interpretability. Our results reveal critical limitations in the scientific reasoning abilities of current MLLMs, providing actionable strategies for developing more robust and interpretable multimodal systems in chemistry. This work provides a timely benchmark for measuring progress in domain-specific multimodal AI and underscores the need for further advances at the intersection of artificial intelligence and scientific reasoning.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [9] [SkyCap: Bitemporal VHR Optical-SAR Quartets for Amplitude Change Detection and Foundation-Model Evaluation](https://arxiv.org/abs/2512.14755)
*Paul Weinmann,Ferdinand Schenck,Martin Šiklar*

Main category: cs.CV

> 研究通过档案匹配和共定位的SkySat光学图像与Capella Space SAR图像构建数据集，使用标签转移技术获取SAR ACD标签，对基础模型进行预训练并进行基准测试，发现在光学变化检测中表现最优的模型并不完全适用于SAR ACD，并且预处理选择对模型性能影响显著。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动力在于光学VHR图像虽然易于解读和标注，但云层会影响数据稳定性；而SAR图像提供了全天候数据采集的能力，但难以标注。因此，研究旨在通过结合两者的优点来提高线性基础设施监控中的变化检测能力。

**Method:** 本研究通过SkySat光学图像与Capella Space SAR图像的档案匹配和共定位构建了SkyCap数据集。并使用光学到SAR的标签转移来获取SAR幅度变化检测(ACD)标签，这并不需要SAR专家标注。研究继续对SARATR-X进行预训练，并与其他光学基础模型在不同预处理选择下的SkyCap数据集上进行基准测试。

**Result:** 在评估的模型中，带有dB+Z-score预处理的MTP(ViT-B+RVSA)表现最佳(F1c = 45.06)，优于直接在Capella数据上进一步预训练的与SAR相关的基础模型。发现预处理与预训练统计的一致性对结果有显著影响，并且在光学变化检测中模型排名并不会完全转移到SAR ACD。

**Conclusion:** 这是首次针对VHR SAR ACD上基础模型的评价，研究表明基础模型的性能高度依赖于预处理的选择，且光学模型在光学变化检测中的表现并不能保证其在SAR ACD中的表现。

**Abstract:** Change detection for linear infrastructure monitoring requires reliable high-resolution data and regular acquisition cadence. Optical very-high-resolution (VHR) imagery is interpretable and straightforward to label, but clouds break this cadence. Synthetic Aperture Radar (SAR) enables all-weather acquisitions, yet is difficult to annotate. We introduce SkyCap, a bitemporal VHR optical-SAR dataset constructed by archive matching and co-registration of (optical) SkySat and Capella Space (SAR) scenes. We utilize optical-to-SAR label transfer to obtain SAR amplitude change detection (ACD) labels without requiring SAR-expert annotations. We perform continued pretraining of SARATR-X on our SAR data and benchmark the resulting SAR-specific foundation models (FMs) together with SARATR-X against optical FMs on SkyCap under different preprocessing choices. Among evaluated models, MTP(ViT-B+RVSA), an optical FM, with dB+Z-score preprocessing attains the best result (F1$_c$ = 45.06), outperforming SAR-specific FMs further pretrained directly on Capella data. We observe strong sensitivity to preprocessing alignment with pretraining statistics, and the ranking of optical models on optical change detection does not transfer one-to-one to SAR ACD. To our knowledge, this is the first evaluation of foundation models on VHR SAR ACD.

</details>


### [10] [SocialNav-MoE: A Mixture-of-Experts Vision Language Model for Socially Compliant Navigation with Reinforcement Fine-Tuning](https://arxiv.org/abs/2512.14757)
*Tomohito Kawabata,Xinyu Zhang,Ling Xiao*

Main category: cs.CV

> 本文提出了一种用于社交合规导航的高效视觉语言模型SocialNav-MoE，并通过SSR增强了其决策能力。实验表明，该模型在导航准确性和效率之间取得了良好的平衡。

<details>
  <summary>Details</summary>

**Motivation:** 现有的机器人导航研究主要集中于安全性，而忽略了社交合规性。大型视觉语言模型计算开销大，不适合资源受限的实时应用场景。

**Method:** 提出了一种名为SocialNav-MoE的高效混合专家视觉语言模型，用于社交合规导航，并采用强化微调（RFT）进行优化。此外，引入了语义相似度奖励（SSR）来提高决策能力。研究了不同小型语言模型类型、路由策略和视觉编码器的效果。

**Result:** 在SNEI数据集上的实验表明，SocialNav-MoE在导航准确性和效率之间取得了优秀的平衡，并且提出的SSR函数优于基于难度等级和字符级别的奖励。

**Conclusion:** SocialNav-MoE在保证导航精度的同时，大大减少了计算开销，使之更适合资源受限的实时导航任务。

**Abstract:** For robots navigating in human-populated environments, safety and social compliance are equally critical, yet prior work has mostly emphasized safety. Socially compliant navigation that accounts for human comfort, social norms, and contextual appropriateness remains underexplored. Vision language models (VLMs) show promise for this task; however, large-scale models incur substantial computational overhead, leading to higher inference latency and energy consumption, which makes them unsuitable for real-time deployment on resource-constrained robotic platforms. To address this issue, we investigate the effectiveness of small VLM and propose SocialNav-MoE, an efficient Mixture-of-Experts vision language model for socially compliant navigation with reinforcement fine-tuning (RFT). We further introduce a semantic similarity reward (SSR) to effectively leverage RFT for enhancing the decision-making capabilities. Additionally, we study the effectiveness of different small language model types (Phi, Qwen, and StableLM), routing strategies, and vision encoders (CLIP vs. SigLIP, frozen vs. fine-tuned). Experiments on the SNEI dataset demonstrate that SocialNav-MoE achieves an excellent balance between navigation accuracy and efficiency. The proposed SSR function is more effective than hard-level and character-level rewards. Source code will be released upon acceptance.

</details>


### [11] [The Renaissance of Expert Systems: Optical Recognition of Printed Chinese Jianpu Musical Scores with Lyrics](https://arxiv.org/abs/2512.14758)
*Fan Bu,Rongfeng Li,Zijin Li,Ya Li,Linfeng Fan,Pei Huang*

Main category: cs.CV

> 研究提出了一种将印刷的简谱及其歌词转换为机器可读的MusicXML和MIDI的模块化专家系统管道，实现了高精度的旋律和对齐歌词的识别。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大规模光学音乐识别研究主要集中在西方乐谱上，而对中国简谱及其丰富的歌词资源研究不足。

**Method:** 采用了自上而下的专家系统设计，结合传统的计算机视觉技术和无监督深度学习模块。

**Result:** 在《中国民歌选集》数据集上进行了评估，对旋律和对齐歌词的识别精度分别达到了F1 = 0.951和F1 = 0.931。

**Conclusion:** 这种方法在不需要大规模标注训练数据的情况下实现了高精度的识别。

**Abstract:** Large-scale optical music recognition (OMR) research has focused mainly on Western staff notation, leaving Chinese Jianpu (numbered notation) and its rich lyric resources underexplored. We present a modular expert-system pipeline that converts printed Jianpu scores with lyrics into machine-readable MusicXML and MIDI, without requiring massive annotated training data. Our approach adopts a top-down expert-system design, leveraging traditional computer-vision techniques (e.g., phrase correlation, skeleton analysis) to capitalize on prior knowledge, while integrating unsupervised deep-learning modules for image feature embeddings. This hybrid strategy strikes a balance between interpretability and accuracy. Evaluated on The Anthology of Chinese Folk Songs, our system massively digitizes (i) a melody-only collection of more than 5,000 songs (> 300,000 notes) and (ii) a curated subset with lyrics comprising over 1,400 songs (> 100,000 notes). The system achieves high-precision recognition on both melody (note-wise F1 = 0.951) and aligned lyrics (character-wise F1 = 0.931).

</details>


### [12] [AquaDiff: Diffusion-Based Underwater Image Enhancement for Addressing Color Distortion](https://arxiv.org/abs/2512.14760)
*Afrah Shaahid,Muzammil Behzad*

Main category: cs.CV

> AquaDiff is a diffusion-based underwater image enhancement framework that corrects chromatic distortions and preserves image fidelity, outperforming state-of-the-art methods on various benchmarks.

<details>
  <summary>Details</summary>

**Motivation:** Underwater images suffer from color distortion, low contrast, and loss of details due to light absorption and scattering, hindering vision-based underwater applications.

**Method:** AquaDiff, a diffusion-based framework, integrates a chromatic prior-guided color compensation strategy and a conditional diffusion process with cross-attention to fuse degraded inputs and noisy latent states. An enhanced denoising backbone with residual dense blocks and multi-resolution attention is used, along with a cross-domain consistency loss to enforce various fidelities.

**Result:** Extensive experiments show that AquaDiff achieves superior color correction and competitive overall image quality compared to state-of-the-art methods on challenging underwater benchmarks.

**Conclusion:** AquaDiff effectively enhances underwater images, preserving structural and perceptual fidelity while correcting chromatic distortions, outperforming traditional, CNN, GAN, and diffusion-based methods.

**Abstract:** Underwater images are severely degraded by wavelength-dependent light absorption and scattering, resulting in color distortion, low contrast, and loss of fine details that hinder vision-based underwater applications. To address these challenges, we propose AquaDiff, a diffusion-based underwater image enhancement framework designed to correct chromatic distortions while preserving structural and perceptual fidelity. AquaDiff integrates a chromatic prior-guided color compensation strategy with a conditional diffusion process, where cross-attention dynamically fuses degraded inputs and noisy latent states at each denoising step. An enhanced denoising backbone with residual dense blocks and multi-resolution attention captures both global color context and local details. Furthermore, a novel cross-domain consistency loss jointly enforces pixel-level accuracy, perceptual similarity, structural integrity, and frequency-domain fidelity. Extensive experiments on multiple challenging underwater benchmarks demonstrate that AquaDiff provides good results as compared to the state-of-the-art traditional, CNN-, GAN-, and diffusion-based methods, achieving superior color correction and competitive overall image quality across diverse underwater conditions.

</details>


### [13] [Improving VQA Reliability: A Dual-Assessment Approach with Self-Reflection and Cross-Model Verification](https://arxiv.org/abs/2512.14770)
*Xixian Wu,Yang Ou,Pengchao Tian,Zian Yang,Jielei Zhang,Peiyi Li,Longwen Gao*

Main category: cs.CV

> 论文提出了用于提高视觉语言模型可靠性的DAVR框架，采用双路径结构进行响应可靠性评估和跨模型事实核查，实验证明其在可靠VQA挑战赛中表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 视觉语言模型（VLMs）在视觉问答（VQA）方面表现出巨大潜力，但它们对幻觉的易感性会导致答案的可靠性和准确性降低，该框架旨在解决这一问题。

**Method:** 该论文提出了一种名为Dual-Assessment for VLM Reliability (DAVR) 的新框架，该框架包括自我反思和跨模型验证两个路径，用于综合估计不确定性，并通过融合VLM潜在特征与QA嵌入以及外部参考模型的事实核查来评估响应的可靠性。

**Result:** 在ICCV-CLVL 2025可靠VQA挑战中，DAVR框架在$Φ_{100}$得分上达到39.64，100-AUC得分为97.22，取得了第一名的成绩。

**Conclusion:** DAVR框架有效地提高了VLM响应的可靠性，从而增强了模型的可信度。

**Abstract:** Vision-language models (VLMs) have demonstrated significant potential in Visual Question Answering (VQA). However, the susceptibility of VLMs to hallucinations can lead to overconfident yet incorrect answers, severely undermining answer reliability. To address this, we propose Dual-Assessment for VLM Reliability (DAVR), a novel framework that integrates Self-Reflection and Cross-Model Verification for comprehensive uncertainty estimation. The DAVR framework features a dual-pathway architecture: one pathway leverages dual selector modules to assess response reliability by fusing VLM latent features with QA embeddings, while the other deploys external reference models for factual cross-checking to mitigate hallucinations. Evaluated in the Reliable VQA Challenge at ICCV-CLVL 2025, DAVR achieves a leading $Φ_{100}$ score of 39.64 and a 100-AUC of 97.22, securing first place and demonstrating its effectiveness in enhancing the trustworthiness of VLM responses.

</details>
