<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 40]
- [cs.CV](#cs.CV) [Total: 37]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale](https://arxiv.org/abs/2507.08865)
*Javis AI Team,Amrendra Singh,Maulik Shah,Dharshan Sampath*

Main category: cs.CL

> 本研究提出了Spatial ModernBERT模型，一种结合了空间嵌入的变压器模型，用于从复杂财务文件中准确检测和提取表格数据及键值对。模型通过多头分类和后处理方法提高了准确性，在真实世界文档中表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 从财务文档中提取表格和键值对对于诸如审计、数据分析和自动发票处理等商业工作流程至关重要。本研究旨在介绍能够从复杂财务文档中准确检测和提取表格数据及键值对的技术。

**Method:** Spatial ModernBERT模型结合了空间嵌入和变压器模型，从复杂的财务文件中准确检测和提取表格数据及键值对。该模型将任务分为三个头进行标记分类：(1) 标签头，将每个标记分类为某种标签；(2) 列头，预测列索引；(3) 行头，区分项目行和表头行。模型首先在PubTables-1M数据集上进行预训练，然后在财务文档数据集上进行微调。此外，还提出了一种后处理方法，使用B-I-IB标签合并标记，重构表格布局并提取键值对。

**Result:** 实验表明，Spatial ModernBERT模型能够有效地利用文本和空间线索，实现对真实世界财务文档中表格和键值对的高度准确提取。

**Conclusion:** Spatial ModernBERT能够在复杂财务文档中利用文本和空间信息进行有效且准确的表格及键值对提取，展示出其在真实世界应用中的潜力。

**Abstract:** Extracting tables and key-value pairs from financial documents is essential
for business workflows such as auditing, data analytics, and automated invoice
processing. In this work, we introduce Spatial ModernBERT-a transformer-based
model augmented with spatial embeddings-to accurately detect and extract
tabular data and key-value fields from complex financial documents. We cast the
extraction task as token classification across three heads: (1) Label Head,
classifying each token as a label (e.g., PO Number, PO Date, Item Description,
Quantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;
(3) Row Head, distinguishing the start of item rows and header rows. The model
is pretrained on the PubTables-1M dataset, then fine-tuned on a financial
document dataset, achieving robust performance through cross-entropy loss on
each classification head. We propose a post-processing method to merge tokens
using B-I-IB tagging, reconstruct the tabular layout, and extract key-value
pairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages
both textual and spatial cues, facilitating highly accurate table and key-value
extraction in real-world financial documents.

</details>


### [2] [SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems](https://arxiv.org/abs/2507.08898)
*Wenliang Shan,Michael Fu,Rui Yang,Chakkrit,Tantithamthavorn*

Main category: cs.CL

> 本文提出了SEALGuard来解决LLM系统多语言安全对齐的问题，通过低秩自适应技术在多语种提示中提升了防护性能。实验结果显示SEALGuard检测多语言不安全提示的效果优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 虽然LLM驱动的防护措施（如LlamaGuard）在英语中的不安全输入检测方面表现出高精度，但是它们对于多语言不安全输入的处理效果较差，这使得LLM系统容易受到低资源语言（如东南亚语言）中不安全和破解提示的攻击。该研究旨在填补现有防护措施中的多语言安全对齐空白，解决低资源语言中的安全问题，提升语言模型系统的安全性。

**Method:** 介绍了一种叫做SEALGuard的多语言防护措施，用于改进语言模型系统中的多语言安全对齐。该研究采用低秩自适应（LoRA）技术将通用多语言语言模型适应成一种多语言防护措施。研究同时构建了一个大规模多语言安全对齐数据集SEALSBench，包含超过260,000个提示，涵盖了10种语言的安全、不安全和破解提示。该方法通过对比SEALGuard和最先进防护措施在SEALSBench上的表现来评估性能。实验结果表明，SEALGuard在检测多语言不安全和破解提示方面比现有防护措施表现出色，提升了48%的防御成功率（DSR），并且在最佳DSR、精确率和F1分数方面表现最佳。研究还通过消融研究揭示了适应策略和模型规模对SEALGuard整体性能的贡献。

**Result:** 实验表明，对于多语言不安全和破解提示，最先进防护措施LlamaGuard的防御成功率分别下降了9%和18%。而SEALGuard在检测多语言不安全和破解提示方面表现优于现有防护措施，提升了48%的防御成功率，并且在最佳防御成功率DSR、精确率和F1分数方面表现出色。

**Conclusion:** 本文介绍的SEALGuard通过引入高效的多语言防护措施，推动了LLM系统的安全对齐。适应策略和模型规模的影响进一步揭示了模型的性能贡献，对于提升LLM系统在多语言环境下的安全性具有重要影响。

**Abstract:** Safety alignment is critical for LLM-powered systems. While recent
LLM-powered guardrail approaches such as LlamaGuard achieve high detection
accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),
they struggle with multilingual unsafe inputs. This limitation leaves LLM
systems vulnerable to unsafe and jailbreak prompts written in low-resource
languages such as those in Southeast Asia. This paper introduces SEALGuard, a
multilingual guardrail designed to improve the safety alignment across diverse
languages. It aims to address the multilingual safety alignment gap of existing
guardrails and ensure effective filtering of unsafe and jailbreak prompts in
LLM-powered systems. We adapt a general-purpose multilingual language model
into a multilingual guardrail using low-rank adaptation (LoRA). We construct
SEALSBench, a large-scale multilingual safety alignment dataset containing over
260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.
We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on
this benchmark. Our findings show that multilingual unsafe and jailbreak
prompts substantially degrade the performance of the state-of-the-art
LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and
18%, respectively, compared to its performance on English-only prompts. In
contrast, SEALGuard outperforms existing guardrails in detecting multilingual
unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and
achieving the best DSR, precision, and F1-score. Our ablation study further
reveals the contributions of adaptation strategies and model size to the
overall performance of SEALGuard. SEALGuard advances the safety alignment of
LLM systems by introducing an effective multilingual guardrail.

</details>


### [3] [Evaluating LLMs in Medicine: A Call for Rigor, Transparency](https://arxiv.org/abs/2507.08916)
*Mahmoud Alwakeel,Aditya Nagori,Vijay Krishnamoorthy,Rishikesan Kamaleswaran*

Main category: cs.CL

> 研究评估了LLMs在医疗问答中的限制，指出现有数据集缺乏临床现实性等问题，强调了需要标准化框架来评估医学领域的LLMs。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在评估大型语言模型在医疗领域中的表现和数据集的优劣，识别当前存在的问题，并提出可能的解决方案。

**Method:** 评估大型语言模型（LLMs）在医疗问答上的当前限制，重点在于评估所用数据集的质量。研究了包括MedQA、MedMCQA、PubMedQA和MMLU在内的广泛使用的基准数据集，审查了它们的严格性、透明度和与临床场景的相关性。同时分析了医学期刊中的挑战性问题，以识别其作为无偏评估工具的潜力。

**Result:** 大多数现有的数据集在临床现实性、透明度和稳健的验证过程方面存在不足。公开的挑战性问题具有一些优势，但也受限于规模小、范围狭窄和LLM训练中的暴露。这些差距突显了需要安全、全面且具有代表性的数据集。

**Conclusion:** 为了评估医疗领域的大型语言模型，制定标准化框架至关重要。需要机构和政策制定者共同努力，确保数据集和方法论严格、无偏且能反映出临床复杂性。

**Abstract:** Objectives: To evaluate the current limitations of large language models
(LLMs) in medical question answering, focusing on the quality of datasets used
for their evaluation. Materials and Methods: Widely-used benchmark datasets,
including MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,
transparency, and relevance to clinical scenarios. Alternatives, such as
challenge questions in medical journals, were also analyzed to identify their
potential as unbiased evaluation tools. Results: Most existing datasets lack
clinical realism, transparency, and robust validation processes. Publicly
available challenge questions offer some benefits but are limited by their
small size, narrow scope, and exposure to LLM training. These gaps highlight
the need for secure, comprehensive, and representative datasets. Conclusion: A
standardized framework is critical for evaluating LLMs in medicine.
Collaborative efforts among institutions and policymakers are needed to ensure
datasets and methodologies are rigorous, unbiased, and reflective of clinical
complexities.

</details>


### [4] [From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation](https://arxiv.org/abs/2507.08924)
*Seokhee Hong,Sunkyoung Kim,Guijin Son,Soyeon Kim,Yeonjung Hong,Jinsik Lee*

Main category: cs.CL

> 本文介绍了两个朝鲜语专家级别的基准测试KMMLU-Redux和KMMLU-Pro，以更好地评估大型语言模型在工业领域应用的适用性。

<details>
  <summary>Details</summary>

**Motivation:** 开发大型语言模型需要稳健的基准测试，不仅要涵盖学术领域，还要包括工业领域，以有效地评估其在实际场景中的适用性。

**Method:** KMMLU-Redux基于现有的KMMLU，从韩国国家技术资格考试中抽取问题，并删除了关键错误以提高可靠性。KMMLU-Pro基于韩国国家专业执照考试，反映了韩国的专业知识。

**Result:** 实验表明，这些基准测试全面代表了韩国的工业知识。

**Conclusion:** 研究团队公开发布了他们的数据集。

**Abstract:** The development of Large Language Models (LLMs) requires robust benchmarks
that encompass not only academic domains but also industrial fields to
effectively evaluate their applicability in real-world scenarios. In this
paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,
reconstructed from the existing KMMLU, consists of questions from the Korean
National Technical Qualification exams, with critical errors removed to enhance
reliability. KMMLU-Pro is based on Korean National Professional Licensure exams
to reflect professional knowledge in Korea. Our experiments demonstrate that
these benchmarks comprehensively represent industrial knowledge in Korea. We
release our dataset publicly available.

</details>


### [5] [Self-Improving Model Steering](https://arxiv.org/abs/2507.08967)
*Rongyi Zhu,Yuhui Wang,Tanqiu Jiang,Jiacheng Liang,Ting Wang*

Main category: cs.CL

> This paper introduces SIMS, a self-improving model-steering framework that does not rely on external data and demonstrates superior performance over existing methods in steering large language models effectively and adaptively.

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of conventional model steering methods that rely on externally annotated data, which limit adaptability and depend on annotation quality.

**Method:** SIMS, a self-improving model-steering framework that autonomously generates and refines contrastive samples without relying on external supervision, incorporating novel strategies such as prompt ranking and contrast sampling.

**Result:** SIMS outperformed existing methods in terms of steering effectiveness and adaptability across diverse LLMs and benchmarks.

**Conclusion:** Self-improving model steering, like SIMS, presents a promising approach for aligning LLMs with human preferences at inference time.

**Abstract:** Model steering represents a powerful technique that dynamically aligns large
language models (LLMs) with human preferences during inference. However,
conventional model-steering methods rely heavily on externally annotated data,
not only limiting their adaptability to varying contexts but also tethering
their effectiveness to annotation quality. In this paper, we present SIMS, the
first self-improving model-steering framework that operates without relying on
external supervision. At its core, SIMS autonomously generates and refines
contrastive samples through iterative self-improvement cycles, enabling
adaptive, context-specific steering. Additionally, SIMS employs novel
strategies, including prompt ranking and contrast sampling, to further enhance
steering efficacy. Extensive evaluation across diverse LLMs and benchmarks
demonstrates that SIMS substantially outperforms existing methods in steering
effectiveness and adaptability, highlighting self-improving model steering as a
promising direction for future research on inference-time LLM alignment.

</details>


### [6] [Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR](https://arxiv.org/abs/2507.08969)
*Drew Walker,Jennifer Love,Swati Rajwal,Isabel C Walker,Hannah LF Cooper,Abeed Sarker,Melvin Livingston III*

Main category: cs.CL

> 研究发现，在电子健康记录中，某些患者的标签和怀疑用语使用率较高，包括非洲裔美国人、使用政府保险或自费支付的患者以及患有某些疾病或精神疾病的患者。护理人员和社会工作者使用贬损语言的频率也较高。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在通过分析电子健康记录中的语言特征，识别和量化患者污名化的语言标记，以及怀疑标记。

**Method:** 使用扩大的词汇匹配和监督学习分类器，在MIMIC-III电子健康记录中识别怀疑标记和污名化标签的语言特征；通过Poisson回归模型评估这些语言特征的变化预测因素。

**Result:** 研究发现，非洲裔美国人患者、使用联邦医疗保险/医疗补助或政府运营保险的患者、自费患者以及患有多种污名化疾病和精神疾病的患者的污名化标签使用率较高。护理人员和社会工作者使用的污名化标签的频率也较高。

**Conclusion:** 历史性污名化的患者中，污名化语言的使用率较高，并且由多种医疗服务提供者类型所推动。

**Abstract:** Introduction: Electronic health records (EHR) are a critical medium through
which patient stigmatization is perpetuated among healthcare teams. Methods: We
identified linguistic features of doubt markers and stigmatizing labels in
MIMIC-III EHR via expanded lexicon matching and supervised learning
classifiers. Predictors of rates of linguistic features were assessed using
Poisson regression models. Results: We found higher rates of stigmatizing
labels per chart among patients who were Black or African American (RR: 1.16),
patients with Medicare/Medicaid or government-run insurance (RR: 2.46),
self-pay (RR: 2.12), and patients with a variety of stigmatizing disease and
mental health conditions. Patterns among doubt markers were similar, though
male patients had higher rates of doubt markers (RR: 1.25). We found increased
stigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),
with similar patterns of doubt markers. Discussion: Stigmatizing language
occurred at higher rates among historically stigmatized patients, perpetuated
by multiple provider types.

</details>


### [7] [Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery](https://arxiv.org/abs/2507.09011)
*Ana Chkhaidze,Reshanne R. Reeder,Connor Gag,Anastasia Kiyonaga,Seana Coulson*

Main category: cs.CL

> 本研究使用自然语言处理工具分析了超过4000名参与者在Ganzflicker诱导的视觉幻觉体验，发现拥有不同视觉想象能力的人看到的内容有明显差异，这可能反映了他们在视觉区域间的协调性存在个体差异。

<details>
  <summary>Details</summary>

**Motivation:** 最近关于图像频谱的提议提出，个体之间的视觉系统差异会影响其他内部生成的视觉体验的复杂性。本研究旨在探讨具有不同类型视觉想象能力的人在Ganzflicker诱导的幻觉中看到的内容是否存在差异。

**Method:** 使用自然语言处理工具对超过4000名参与者在Ganzflicker诱导的幻觉期间自发描述的幻觉进行了分析。

**Result:** 强想象者描述了复杂的、自然的内容，而弱想象者报告了简单的几何图案。视觉语言模型的嵌入比仅使用文本的语言模型更好地捕捉了这些差异，并且具有更强想象能力的参与者使用了具有更丰富感觉运动关联的语言。

**Conclusion:** 这些发现可能反映了早期视觉区域和与图像频谱相关的较高层区域之间的个体协调差异。

**Abstract:** A rapidly alternating red and black display known as Ganzflicker induces
visual hallucinations that reflect the generative capacity of the visual
system. Recent proposals regarding the imagery spectrum, that is, differences
in the visual system of individuals with absent imagery, typical imagery, and
vivid imagery, suggest these differences should impact the complexity of other
internally generated visual experiences. Here, we used tools from natural
language processing to analyze free-text descriptions of hallucinations from
over 4,000 participants, asking whether people with different imagery
phenotypes see different things in their mind's eye during Ganzflicker-induced
hallucinations. Strong imagers described complex, naturalistic content, while
weak imagers reported simple geometric patterns. Embeddings from vision
language models better captured these differences than text-only language
models, and participants with stronger imagery used language with richer
sensorimotor associations. These findings may reflect individual variation in
coordination between early visual areas and higher-order regions relevant for
the imagery spectrum.

</details>


### [8] [Lizard: An Efficient Linearization Framework for Large Language Models](https://arxiv.org/abs/2507.09025)
*Chien Van Nguyen,Ruiyi Zhang,Hanieh Deilamsalehy,Puneet Mathur,Viet Dac Lai,Haoliang Wang,Jayakumar Subramanian,Ryan A. Rossi,Trung Bui,Nikos Vlassis,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

> Lizard proposes a new method for making Transformer-based Large Language Models more efficient for long-context tasks by introducing subquadratic attention and a gating mechanism, leading to improved performance in language modeling benchmarks.

<details>
  <summary>Details</summary>

**Motivation:** To overcome the quadratic complexity challenges and memory bottlenecks encountered by Transformer-based LLMs as context lengths increase, especially in infinite-context generation scenarios.

**Method:** Lizard, a linearization framework which transforms pretrained Transformer-based Large Language Models (LLMs) into flexible, subquadratic architectures, introduces a subquadratic attention mechanism and a gating module to enable adaptive memory control and support constant-memory inference.

**Result:** The experiments show that Lizard achieves near-lossless recovery of the teacher model's performance on standard language modeling tasks and outperforms previous linearization methods, demonstrating a 18 points improvement on the 5-shot MMLU benchmark and significant improvements in associative recall tasks.

**Conclusion:** Lizard offers a promising approach to addressing the memory and computational overhead in large language models through the innovative integration of subquadratic attention and gating mechanisms, providing significant performance and flexibility improvements.

**Abstract:** We propose Lizard, a linearization framework that transforms pretrained
Transformer-based Large Language Models (LLMs) into flexible, subquadratic
architectures for infinite-context generation. Transformer-based LLMs face
significant memory and computational bottlenecks as context lengths increase,
due to the quadratic complexity of softmax attention and the growing key-value
(KV) cache. Lizard addresses these limitations by introducing a subquadratic
attention mechanism that closely approximates softmax attention while
preserving the output quality. Unlike previous linearization methods, which are
often limited by fixed model structures and therefore exclude gating
mechanisms, Lizard incorporates a gating module inspired by recent
state-of-the-art linear models. This enables adaptive memory control, supports
constant-memory inference, offers strong length generalization, and allows more
flexible model design. Lizard combines gated linear attention for global
context compression with sliding window attention enhanced by meta memory,
forming a hybrid mechanism that captures both long-range dependencies and
fine-grained local interactions. Moreover, we introduce a hardware-aware
algorithm that accelerates the training speed of our models. Extensive
experiments show that Lizard achieves near-lossless recovery of the teacher
model's performance across standard language modeling tasks, while
significantly outperforming previous linearization methods. On the 5-shot MMLU
benchmark, Lizard improves over prior models by 18 points and shows significant
improvements on associative recall tasks.

</details>


### [9] [ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making](https://arxiv.org/abs/2507.09037)
*Bharadwaj Ravichandran,David Joy,Paul Elliott,Brian Hu,Jadie Adams,Christopher Funk,Emily Veenhuis,Anthony Hoogs,Arslan Basharat*

Main category: cs.CL

> The paper introduces the ALIGN system for dynamic personalization of LLM-based decision-makers using prompt-based alignment to a set of fine-grained attributes, enabling comparative analyses in different domains.

<details>
  <summary>Details</summary>

**Motivation:** To address the diverse values and preferences of users when using LLMs as decision aids, requiring novel methods for alignment and personalization beyond existing benchmarking-focused tools.

**Method:** Developed the ALIGN system with robust configuration management, structured output generation with reasoning, and swappable LLM backends. Also created a user interface for qualitative comparisons and performed a quantitative analysis in two domains.

**Result:** The ALIGN framework enables a modular, qualitative, and quantitative comparison of LLMs and their alignment to various attributes across domains like demographic alignment and value alignment.

**Conclusion:** The ALIGN system opens new research opportunities for reliable, responsible, and personalized LLM-based decision-makers by providing an open-source framework for diverse analyses and comparisons.

**Abstract:** Large language models (LLMs) are increasingly being used as decision aids.
However, users have diverse values and preferences that can affect their
decision-making, which requires novel methods for LLM alignment and
personalization. Existing LLM comparison tools largely focus on benchmarking
tasks, such as knowledge-based question answering. In contrast, our proposed
ALIGN system focuses on dynamic personalization of LLM-based decision-makers
through prompt-based alignment to a set of fine-grained attributes. Key
features of our system include robust configuration management, structured
output generation with reasoning, and several algorithm implementations with
swappable LLM backbones, enabling different types of analyses. Our user
interface enables a qualitative, side-by-side comparison of LLMs and their
alignment to various attributes, with a modular backend for easy algorithm
integration. Additionally, we perform a quantitative analysis comparing
alignment approaches in two different domains: demographic alignment for public
opinion surveys and value alignment for medical triage decision-making. The
entire ALIGN framework is open source and will enable new research on reliable,
responsible, and personalized LLM-based decision-makers.

</details>


### [10] [OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique](https://arxiv.org/abs/2507.09075)
*Wasi Uddin Ahmad,Somshubra Majumdar,Aleksander Ficek,Sean Narenthiran,Mehrzad Samadi,Jocelyn Huang,Siddhartha Jain,Vahid Noroozi,Boris Ginsburg*

Main category: cs.CL

> 本研究提出了一个包含2.5M问题-解决方案-评论三元组的数据集OpenCodeReasoning-II，并通过两阶段微调策略提高了Qwen2.5-Instruct模型在代码生成和代码审查上的性能。

<details>
  <summary>Details</summary>

**Motivation:** 代码生成和代码审查的进步在很大程度上依赖于大规模高质量的数据集。本研究旨在通过引入更大规模的数据集和改进的微调策略，提高代码生成和代码审查的性能，从而推动这两个领域的发展。

**Method:** 本研究采用两阶段监督微调策略。第一阶段专注于代码生成的微调，第二阶段则共同训练代码生成和代码审查模型。此外，引入了OpenCodeReasoning-II数据集，包含大约35K个独特的编程问题，总计2.5M个问题-解决方案-评论三元组，使其成为目前最大规模公开的代码推理数据集之一。研究还扩展了LiveCodeBench基准测试，支持C++编程语言，以更全面地评估LLMs。

**Result:** 研究表明，采用两阶段微调策略后的Qwen2.5-Instruct模型在代码生成方面的性能超过或等于现有最好的公开模型。同时，集成代码生成和代码审查模型的方法，在竞争性编程中的表现有了显著提升。

**Conclusion:** 通过引入大规模数据集OpenCodeReasoning-II和采取两阶段微调策略，成功提升了代码生成和代码审查模型的性能。此外，扩展LiveCodeBench基准测试支持C++有助于更全面地评估LLMs在编程任务中的表现。

**Abstract:** Recent advancements in reasoning-based Large Language Models (LLMs),
particularly their potential through test-time scaling, have created
significant opportunities for distillation in code generation and critique.
However, progress in both areas fundamentally depends on large-scale,
high-quality datasets. In this work, we introduce OpenCodeReasoning-II, a
dataset consists of 2.5M question-solution-critique triples (approx. 35K unique
programming questions), making it nearly twice the size of the previous largest
publicly available code reasoning dataset. In this work, we employ a two-stage
supervised fine-tuning strategy. The first stage focuses on fine-tuning for
code generation, while the second stage involves the joint training of models
for both code generation and critique. Our resulting finetuned Qwen2.5-Instruct
models achieve performance in code generation that either exceeds or equals the
best prior open-weight distilled models. Notably, the integration of our code
generation and critique models leads to significant improvements in competitive
coding performance. Furthermore, we present an extension of the LiveCodeBench
benchmark to specifically support the C++ programming language, thereby
facilitating more comprehensive LLM evaluation using this benchmark.

</details>


### [11] [Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation](https://arxiv.org/abs/2507.09076)
*Jialong Mai,Xiaofen Xing,Yawei Li,Zhipeng Li,Jingyuan Xing,Xiangmin Xu*

Main category: cs.CL

> Introduces DPM for unlimited-length audio processing with limited SLLM context windows, enhancing SER outcomes.

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of speech large language models (SLLMs) in processing high frame rate audio and capturing emotional continuity across multiple conversation turns for speech emotion recognition.

**Method:** Dynamic Parameter Memory (DPM) mechanism with contextual semantics and sentence-level emotion encoding is proposed to enable the processing of unlimited-length audio with limited context windows in SLLM for speech emotion recognition.

**Result:** Experimental results on the IEMOCAP dataset indicate significant improvements in emotion recognition capabilities of SLLM when processing long audio sequences, achieving state-of-the-art performance.

**Conclusion:** The Dynamic Parameter Memory (DPM) mechanism improves long audio sequence emotion recognition capabilities in speech emotion recognition tasks, marking a new state-of-the-art performance.

**Abstract:** Recent research has focused on applying speech large language model (SLLM) to
improve speech emotion recognition (SER). However, the inherently high frame
rate in speech modality severely limits the signal processing and understanding
capabilities of SLLM. For example, a SLLM with a 4K context window can only
process 80 seconds of audio at 50Hz feature sampling rate before reaching its
capacity limit. Input token compression methods used in SLLM overlook the
continuity and inertia of emotions across multiple conversation turns. This
paper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual
semantics and sentence-level emotion encoding, enabling processing of
unlimited-length audio with limited context windows in SLLM. Specifically, DPM
progressively encodes sentence-level information and emotions into a temporary
LoRA module during inference to effectively "memorize" the contextual
information. We trained an emotion SLLM as a backbone and incorporated our DPM
into inference for emotion recognition in conversation (ERC). Experimental
results on the IEMOCAP dataset show that DPM significantly improves the emotion
recognition capabilities of SLLM when processing long audio sequences,
achieving state-of-the-art performance.

</details>


### [12] [CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards](https://arxiv.org/abs/2507.09104)
*Taolin Zhang,Maosong Cao,Alexander Lam,Songyang Zhang,Kai Chen*

Main category: cs.CL

> This paper introduces CompassJudger-2, a generalist judge model designed to overcome the narrow specialization and limited robustness of current judge models by using a task-driven, multi-domain data curation and refined learning objective.

<details>
  <summary>Details</summary>

**Motivation:** Current LLM-as-judge models lack broad specialization and robustness, which limits their ability to conduct comprehensive evaluations. This paper aims to address these issues.

**Method:** Central to our approach is supervising judgment tasks with verifiable rewards and using a task-driven, multi-domain data curation strategy. We also introduced a refined learning objective with margin policy gradient loss.

**Result:** CompassJudger-2 achieves superior results across multiple judge and reward benchmarks, demonstrating competitive judgment accuracy comparable to significantly larger models.

**Conclusion:** The contributions of this work advance robust, scalable LLM judgment capabilities and establish new performance and evaluation standards, offering a significant step in enhancing the robustness and generalizability of judge models.

**Abstract:** Recently, the role of LLM-as-judge in evaluating large language models has
gained prominence. However, current judge models suffer from narrow
specialization and limited robustness, undermining their capacity for
comprehensive evaluations. In this work, we present CompassJudger-2, a novel
generalist judge model that overcomes these limitations via a task-driven,
multi-domain data curation strategy. Central to our approach is supervising
judgment tasks with verifiable rewards, guiding intrinsic critical reasoning
through rejection sampling to foster robust, generalizable judgment
capabilities. We introduce a refined learning objective with margin policy
gradient loss to enhance performance. Empirically, CompassJudger-2 achieves
superior results across multiple judge and reward benchmarks, and our 7B model
demonstrates competitive judgment accuracy with significantly larger models
like DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a
comprehensive benchmark evaluating cross-domain judgment accuracy and rank
consistency to standardize judge model evaluation. These contributions advance
robust, scalable LLM judgment and establish new performance and evaluation
standards.

</details>


### [13] [OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering](https://arxiv.org/abs/2507.09155)
*Ali Vosoughi,Ayoub Shahnazari,Yufeng Xi,Zeliang Zhang,Griffin Hess,Chenliang Xu,Niaz Abdolrahim*

Main category: cs.CL

> OPENXRD是为晶体学问答任务设计的开放图书管道，通过使用AI生成的辅助材料显著提高了模型的理解能力和准确性，为科学领域的NLP工具奠定了基础。

<details>
  <summary>Details</summary>

**Motivation:** 使用扫描教科书可能会导致版权问题，因此，本研究开发了OPENXRD，用于提升较小语言模型在晶体学领域中的理解能力。

**Method:** 本研究提出了OPENXRD，一个开放图书管道，用于晶体学问答任务。该管道结合了文本提示和由GPT-4.5生成的简洁辅助内容。为了规避版权问题，OPENXRD生成紧凑的领域相关的参考内容，帮助较小的语言模型理解X射线衍射的关键概念。

**Result:** 实验结果表明，使用GPT-4.5生成的总结内容显著提升了语言模型在专家级别X射线衍射问题上的准确率，特别是在前期训练较少的模型中。

**Conclusion:** OPENXRD证明了使用AI生成文本可以帮助较小模型更有效地进行科学任务推理，并为未来更广泛的自然语言处理工具提供了基础。

**Abstract:** This work presents OPENXRD, an open-book pipeline designed for
crystallography question answering, which integrates textual prompts with
concise supporting content generated by GPT-4.5. Instead of using scanned
textbooks, which may lead to copyright issues, OPENXRD generates compact,
domain-specific references that help smaller models understand key concepts in
X-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217
expert-level XRD questions by comparing different vision-language models,
including GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,
under both closed-book (without supporting material) and open-book (with
supporting material) conditions. Our experimental results show significant
accuracy improvements in models that use the GPT-4.5-generated summaries,
particularly those with limited prior training in crystallography. OPENXRD uses
knowledge from larger models to fill knowledge gaps in crystallography and
shows that AI-generated texts can help smaller models reason more effectively
in scientific tasks. While the current version of OPENXRD focuses on text-based
inputs, we also explore future extensions such as adding real crystal diagrams
or diffraction patterns to improve interpretation in specialized materials
science contexts. Overall, OPENXRD shows that specialized open-book systems can
be useful in materials science and provides a foundation for broader natural
language processing (NLP) tools in critical scientific fields.

</details>


### [14] [PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning](https://arxiv.org/abs/2507.09157)
*Bhavinkumar Vinodbhai Kuwar,Bikrant Bikram Pratap Maurya,Priyanshu Gupta,Nitin Choudhury*

Main category: cs.CL

> 本文提出了一种新的欺诈检测模型PU-Lie，在Diplomacy数据集中取得了0.60的新基准宏观F1值，同时将可训练参数减少了650多倍。

<details>
  <summary>Details</summary>

**Motivation:** 面对语言的微妙性和欺骗性与真实交流之间极端的类别不平衡问题，本文重新审视了Diplomacy数据集中欺诈检测的问题，其中标注的欺骗消息少于5%。

**Method:** 本文提出了一种轻量且有效的模型，该模型结合了冻结的BERT嵌入、可解释的语言特征和与游戏相关的特征，以及正样本-未标注样本（PU）学习目标。PU-Lie模型专门为只有少量欺骗性信息被标注的情况设计，大多数数据为未标注状态。

**Result:** 该模型在七个模型的综合评估和消融研究中，取得了新基准的0.60宏观F1值。

**Conclusion:** 研究强调在这种问题设置中准确检测欺骗比识别真实信息更重要。PU-Lie模型有效地降低了可训练参数，实现了欺诈检测的目标。

**Abstract:** Detecting deception in strategic dialogues is a complex and high-stakes task
due to the subtlety of language and extreme class imbalance between deceptive
and truthful communications. In this work, we revisit deception detection in
the Diplomacy dataset, where less than 5% of messages are labeled deceptive. We
introduce a lightweight yet effective model combining frozen BERT embeddings,
interpretable linguistic and game-specific features, and a Positive-Unlabeled
(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is
tailored for situations where only a small portion of deceptive messages are
labeled, and the majority are unlabeled. Our model achieves a new best macro F1
of 0.60 while reducing trainable parameters by over 650x. Through comprehensive
evaluations and ablation studies across seven models, we demonstrate the value
of PU learning, linguistic interpretability, and speaker-aware representations.
Notably, we emphasize that in this problem setting, accurately detecting
deception is more critical than identifying truthful messages. This priority
guides our choice of PU learning, which explicitly models the rare but vital
deceptive class.

</details>


### [15] [RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking](https://arxiv.org/abs/2507.09174)
*Shuo Yang,Zijian Yu,Zhenzhe Ying,Yuqin Dai,Guoqing Wang,Jun Lan,Jinfeng Xu,Jinze Li,Edith C. H. Ngai*

Main category: cs.CL

> RAMA是一个新的检索增强的多代理框架，用于核查多媒体虚假信息，实验显示，它在含糊或不可能的声明验证上表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 多模式虚假信息的迅速蔓延给自动化事实核查系统带来了挑战，尤其是在声明含糊或缺乏足够上下文的情况下。因此，作者提出了RAMA，旨在解决这一问题。

**Method:** RAMA是一个结合了三项核心创新的检索增强型多代理框架，用于验证多媒体虚假信息：(1) 将多模式声明转化为精确的网络搜索查询；(2) 从多样化的权威来源汇总交叉验证证据；(3) 一个多代理集成架构，利用多个多模态语言模型和提示变体的互补优势。

**Result:** 实验结果表明，RAMA在基准数据集上表现出色，特别擅长解决含糊或不可能的声明，通过检索的事实证据来验证。

**Conclusion:** 研究结果强调，为了实现可信的多媒体验证，需要整合基于网络的证据和多代理推理，为更可靠和可扩展的事实核查解决方案开辟道路。

**Abstract:** The rapid proliferation of multimodal misinformation presents significant
challenges for automated fact-checking systems, especially when claims are
ambiguous or lack sufficient context. We introduce RAMA, a novel
retrieval-augmented multi-agent framework designed for verifying multimedia
misinformation. RAMA incorporates three core innovations: (1) strategic query
formulation that transforms multimodal claims into precise web search queries;
(2) cross-verification evidence aggregation from diverse, authoritative
sources; and (3) a multi-agent ensemble architecture that leverages the
complementary strengths of multiple multimodal large language models and prompt
variants. Extensive experiments demonstrate that RAMA achieves superior
performance on benchmark datasets, particularly excelling in resolving
ambiguous or improbable claims by grounding verification in retrieved factual
evidence. Our findings underscore the necessity of integrating web-based
evidence and multi-agent reasoning for trustworthy multimedia verification,
paving the way for more reliable and scalable fact-checking solutions. RAMA
will be publicly available at https://github.com/kalendsyang/RAMA.git.

</details>


### [16] [Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models](https://arxiv.org/abs/2507.09185)
*Ameen Ali,Shahar Katz,Lior Wolf,Ivan Titov*

Main category: cs.CL

> 本文提出了一种通过识别和修剪与特定数据集相关的神经元来改进大型语言模型的泛化能力的微调方法。实验结果显示，该方法在多项选择基准测试中的表现优于之前的适应方法。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型倾向于依赖特定数据集的特定机制，这虽然在特定情况下是有益的，但在遇到新任务或数据分布时会降低性能。本文旨在提高模型的泛化能力。

**Method:** Structure

**Result:** {"tldr": "本文提出了一种通过识别和修剪与特定数据集相关的神经元来改进大型语言模型的泛化能力的微调方法。实验结果显示，该方法在多项选择基准测试中的表现优于之前的适应方法。", "motivation": "大型语言模型倾向于依赖特定数据集的特定机制，这虽然在特定情况下是有益的，但在遇到新任务或数据分布时会降低性能。本文旨在提高模型的泛化能力。", "method": "本文方法使用Integrated Gradients来量化每个神经元对高置信度预测的影响，识别那些促进特定数据集性能但不支持稳健、可转移推理的神经元，并选择性地修剪它们。", "result": "实验结果显示，修剪神经元后模型在多项选择基准测试中的性能显著提高，并优于之前的非修剪适应方法。", "conclusion": "通过该方法，可以使模型更依赖于通用表达，从而提高其泛化能力。"}

**Conclusion:** 通过该方法，可以使模型更依赖于通用表达，从而提高其泛化能力。

**Abstract:** Large language models (LLMs) often develop learned mechanisms specialized to
specific datasets, such as reliance on domain-specific correlations, which
yield high-confidence predictions without generalizable reasoning. While
beneficial in one setting, these dataset-specific mechanisms typically degrade
performance when models encounter novel tasks or distributions. In this work,
we introduce a fine-tuning approach designed to enhance generalization by
identifying and pruning neurons associated with dataset-specific mechanisms in
transformer-based LLMs. Our method employs Integrated Gradients to quantify
each neuron's influence on high-confidence predictions, pinpointing those that
disproportionately contribute to dataset-specific performance without
supporting robust, transferable reasoning. Selectively pruning these neurons
compels the model to depend on generalizable representations. Evaluated across
multiple-choice benchmarks, our pruning-based fine-tuning significantly
enhances performance, surpassing prior (non-pruning) adaptation methods.

</details>


### [17] [Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training](https://arxiv.org/abs/2507.09205)
*Leiyu Pan,Bojian Xiong,Lei Yang,Renren Jin,Shaowei Zhang,Yue Chen,Ling Shi,Jiang Zhou,Junru Wu,Zhen Wang,Jianxiang Peng,Juesi Xiao,Tianyu Dong,Zhuowen Han,Zhuo Chen,Sangjee Dondrub,Caizang Tai,Haixing Zhao,Huaque Cairang,Suonan Cairang,Rou Te,Lengben Zhaxi,Gazang Zhaxi,Zhonglin Ye,Yuhui Zheng,Chunyan Peng,Secha Jia,Pema Tashi,Cizhen Jiacuo,Pema Dorjee,Hongkai Liu,Pema Yanggon,Tsehang Dorjee,Jiaxin Han,Qiongying Hu,Jilin Man,Huanke You,Yuqi Ren,Duo La,Deyi Xiong*

Main category: cs.CL

> Researchers developed Banzhida, a Tibetan-tailored multilingual large language model using the largest Tibetan pre-training corpus. It outperforms other similar models in various tasks.

<details>
  <summary>Details</summary>

**Motivation:** To bridge the gap in performance for Tibetan, a low-resource language, given the scarcity of high-quality training corpora for existing language models.

**Method:** The researchers created the largest Tibetan pre-training corpus to date by aggregating data from various sources and applying a dedicated data cleaning and processing pipeline for Tibetan. They then pre/post-trained a multilingual base model to develop Banzhida, a multilingual large language model tailored for Tibetan.

**Result:** Banzhida demonstrates significant performance improvement over both open-source models of similar scale and Tibetan-tailored models across various tasks.

**Conclusion:** The creation of the Tibetan pre-training corpus and the development of Banzhida contribute to advancing generative AI for Tibetan, showing promising results in enhancing the performance of models for low-resource languages.

**Abstract:** Large language models have achieved remarkable progress across many
languages. However, Tibetan, as a representative low-resource language, is
particularly underrepresented in existing models due to the scarcity of
high-quality training corpora. To address this gap, we curate the largest
Tibetan pre-training corpus to date, aggregating data from diverse sources and
applying a dedicated data cleaning and processing pipeline tailored for
Tibetan. With the curated data, we continue pre/post-training a multilingual
base model into Banzhida, a multilingual large language model that advances
generative AI for Tibetan. To evaluate the Tibetan capabilities of the model,
we create new high-quality Tibetan benchmarks, and complement them with
existing public benchmarks. Experimental results demonstrate that Banzhida
consistently and significantly outperforms both open-source models of similar
scale and Tibetan-tailored models across a wide range of tasks.

</details>


### [18] [MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis](https://arxiv.org/abs/2507.09225)
*Biagio Scalingi,Chiara Barattieri di San Pietro,Paolo Canal,Valentina Bambini*

Main category: cs.CL

> 研究开发了一个气候变迁视觉隐喻数据库，表明相较于字面图像，视觉隐喻在理解和认知上带来更大压力，但其审美愉悦性和激发积极情感体验的优势作为回报。

<details>
  <summary>Details</summary>

**Motivation:** 尽管气候变迁的视觉隐喻被视为应对环境挑战的有效工具，但很少有研究探讨它们对沟通的影响，部分原因是材料分散。本研究旨在填补这一空白。

**Method:** 本研究创建了一个名为MetaClimage的气候变迁视觉隐喻数据库，并将其与字面图像配对，添加了人类评分。对于每张图像，收集了难度、有效性、艺术质量、情感唤醒度等方面的人类评分，以及参与者生成的用于总结信息的标签数量。通过自然语言处理从标签中衍生出了语义和情感变量。

**Result:** 视觉隐喻在理解难度上较高，但在审美愉悦性上也更高，而在有效性和唤醒度方面与字面图像无异。参与者的认知需求较高时，视觉隐喻的唤醒度较高。视觉隐喻更常收到更多的标签，往往涉及到图像中未描绘的实体，并引发了更多具有积极情感和支配力的词语。这些结果表明视觉隐喻的认知压力更大，但可能会引发更深的思考和抽象思考。尽管它们不被认为更有效或更具唤醒度，但视觉隐喻似乎能产生更好的审美欣赏和积极体验。

**Conclusion:** 本研究通过对气候变迁的视觉隐喻的影响进行探讨，为未来研究提供了一个数据库，并揭示了在塑造环境沟通时需要考虑的利弊权衡。

**Abstract:** Visual metaphors of climate change (e.g., melting glaciers depicted as a
melting ice grenade) are regarded as valuable tools for addressing the
complexity of environmental challenges. However, few studies have examined
their impact on communication, also due to scattered availability of material.
Here, we present a novel database of Metaphors of Climate Change in Images
(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal
images and enriched with human ratings. For each image, we collected values of
difficulty, efficacy, artistic quality, and emotional arousal from human
rating, as well as number of tags generated by participants to summarize the
message. Semantic and emotion variables were further derived from the tags via
Natural Language Processing. Visual metaphors were rated as more difficult to
understand, yet more aesthetically pleasant than literal images, but did not
differ in efficacy and arousal. The latter for visual metaphors, however, was
higher in participants with higher Need For Cognition. Furthermore, visual
metaphors received more tags, often referring to entities not depicted in the
image, and elicited words with more positive valence and greater dominance than
literal images. These results evidence the greater cognitive load of visual
metaphors, which nevertheless might induce positive effects such as deeper
cognitive elaboration and abstraction compared to literal stimuli. Furthermore,
while they are not deemed as more effective and arousing, visual metaphors seem
to generate superior aesthetic appreciation and a more positively valenced
experience. Overall, this study contributes to understanding the impact of
visual metaphors of climate change both by offering a database for future
research and by elucidating a cost-benefit trade-off to take into account when
shaping environmental communication.

</details>


### [19] [Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources](https://arxiv.org/abs/2507.09245)
*Deshan Sumanathilaka,Sameera Perera,Sachithya Dharmasiri,Maneesha Athukorala,Anuja Dilrukshi Herath,Rukshan Dias,Pasindu Gamage,Ruvan Weerasinghe,Y. H. P. P. Priyadarshana*

Main category: cs.CL

> 本文介绍了Swa-bhasha Resource Hub，这是一个提供罗马化僧伽罗语到僧伽罗语转写资源的综合平台，对僧伽罗语自然语言处理研究有显著贡献。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机是提供一个公开且易于访问的平台，以便研究人员和开发者能够利用现有的数据集和工具进行僧伽罗语的自然语言处理研究，特别是转写模型的培训和应用开发。

**Method:** 本文主要介绍了Swa-bhasha Resource Hub，这是2020至2025年间为罗马化僧伽罗语到僧伽罗语的转写开发的数据资源和算法的综合集合。这些资源对于推动僧伽罗语自然语言处理的研究起到了很大作用，尤其是训练转写模型和开发涉及罗马化僧伽罗语的应用。

**Result:** 本文提供了作者贡献资源的详细介绍，以及领域内现有转写应用的比较分析。

**Conclusion:** Swa-bhasha Resource Hub为罗马化僧伽罗语到僧伽罗语的转写研究提供了宝贵的资源，促进了该领域的发展。

**Abstract:** The Swa-bhasha Resource Hub provides a comprehensive collection of data
resources and algorithms developed for Romanized Sinhala to Sinhala
transliteration between 2020 and 2025. These resources have played a
significant role in advancing research in Sinhala Natural Language Processing
(NLP), particularly in training transliteration models and developing
applications involving Romanized Sinhala. The current openly accessible data
sets and corresponding tools are made publicly available through this hub. This
paper presents a detailed overview of the resources contributed by the authors
and includes a comparative analysis of existing transliteration applications in
the domain.

</details>


### [20] [Psychology-Driven Enhancement of Humour Translation](https://arxiv.org/abs/2507.09259)
*Yuchen Su,Yonghua Zhu,Yang Chen,Diana Benavides-Prado,Michael Witbrock*

Main category: cs.CL

> 提出了一种心理学启发的幽默分解机制，显著提升了幽默翻译的质量。

<details>
  <summary>Details</summary>

**Motivation:** 尽管现有的大型语言模型（LLMs）能够完成一般翻译任务，但在幽默翻译方面仍存在语言干扰和缺乏幽默性的问题。

**Method:** 采用心理学启发的幽默分解机制（HDM），利用CoT模仿人类思维过程，以优化翻译文本的可读性，并整合幽默理论进一步增强翻译文本中的幽默元素。

**Result:** 在开源幽默数据集上的自动评估实验表明，该方法显著提高了幽默翻译的质量，平均提高了7.75%的幽默性，2.81%的流畅度和6.13%的一致性。

**Conclusion:** 该研究证明了使用幽默分解机制和结合幽默理论可以有效提高幽默文本翻译的质量和效果。

**Abstract:** Humour translation plays a vital role as a bridge between different cultures,
fostering understanding and communication. Although most existing Large
Language Models (LLMs) are capable of general translation tasks, these models
still struggle with humour translation, which is especially reflected through
linguistic interference and lacking humour in translated text. In this paper,
we propose a psychology-inspired Humour Decomposition Mechanism (HDM) that
utilises Chain-of-Thought (CoT) to imitate the ability of the human thought
process, stimulating LLMs to optimise the readability of translated humorous
texts. Moreover, we integrate humour theory in HDM to further enhance the
humorous elements in the translated text. Our automatic evaluation experiments
on open-source humour datasets demonstrate that our method significantly
improves the quality of humour translation, yielding average gains of 7.75\% in
humour, 2.81\% in fluency, and 6.13\% in coherence of the generated text.

</details>


### [21] [ClaritySpeech: Dementia Obfuscation in Speech](https://arxiv.org/abs/2507.09282)
*Dominika Woszczyk,Ranya Aloufi,Soteris Demetriou*

Main category: cs.CL

> 研究提出了一种新型的痴呆症言语加密框架，可以校正受痴呆症影响的言语，同时在低数据环境下无需微调并保持说话者身份，有效提高了隐私性和访问性。

<details>
  <summary>Details</summary>

**Motivation:** 痴呆症改变了言语模式，造成沟通障碍并引起隐私问题，现有的语音技术难以处理这一问题，本研究旨在解决这些问题，提高隐私性和访问性。

**Method:** 提出了一种名为ClaritySpeech的框架，整合了自动语音识别（ASR）、文本加密和零样本文本转语音（TTS）技术，以在无需微调的低数据环境下校正受痴呆症影响的语音并保持说话者身份。

**Result:** 该系统在ADReSS和ADReSSo数据集中分别将WER从0.73降低到0.08和0.15，增强了语音质量，并在不同的对抗设置和模式下使F1评分下降了16%和10%。

**Conclusion:** 该研究证明了ClaritySpeech框架的有效性，该框架在处理患有痴呆症人群的言语障碍方面具有潜在的应用价值，有助于改善其沟通隐私和无障碍访问。

**Abstract:** Dementia, a neurodegenerative disease, alters speech patterns, creating
communication barriers and raising privacy concerns. Current speech
technologies, such as automatic speech transcription (ASR), struggle with
dementia and atypical speech, further challenging accessibility. This paper
presents a novel dementia obfuscation in speech framework, ClaritySpeech,
integrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to
correct dementia-affected speech while preserving speaker identity in low-data
environments without fine-tuning. Results show a 16% and 10% drop in mean F1
score across various adversarial settings and modalities (audio, text, fusion)
for ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We
also find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15
for ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and
accessibility.

</details>


### [22] [DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models](https://arxiv.org/abs/2507.09424)
*Cathy Jiao,Yijun Pan,Emily Xiao,Daisy Sheng,Niket Jain,Hanzhang Zhao,Ishita Dasgupta,Jiaqi W. Ma,Chenyan Xiong*

Main category: cs.CL

> DATE-LM是用于评估语言模型中数据归因方法的基准平台，通过该平台，研究者对现有方法进行了评估，发现各个方法各有优劣。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机是系统性地评估语言模型中的数据归因方法，填补在这一领域中的关键空白。

**Method:** 内容介绍了一种名为DATE-LM的数据归因评估基准，它用于评估语言模型中的数据归因方法。DATE-LM通过三个关键任务——训练数据选择、毒性/偏见过滤和事实归因来衡量归因质量。研究者还利用DATE-LM对现有的数据归因方法进行了大规模评估。

**Result:** 研究结果显示没有单一的方法能在所有任务中占主导地位，数据归因方法相对于简单的基线方法有其优缺点，并且其性能对任务特定的评估设计敏感。

**Conclusion:** 希望DATE-LM能够成为未来语言模型中数据归因研究的基础。

**Abstract:** Data attribution methods quantify the influence of training data on model
outputs and are becoming increasingly relevant for a wide range of LLM research
and applications, including dataset curation, model interpretability, data
valuation. However, there remain critical gaps in systematic LLM-centric
evaluation of data attribution methods. To this end, we introduce DATE-LM (Data
Attribution Evaluation in Language Models), a unified benchmark for evaluating
data attribution methods through real-world LLM applications. DATE-LM measures
attribution quality through three key tasks -- training data selection,
toxicity/bias filtering, and factual attribution. Our benchmark is designed for
ease of use, enabling researchers to configure and run large-scale evaluations
across diverse tasks and LLM architectures. Furthermore, we use DATE-LM to
conduct a large-scale evaluation of existing data attribution methods. Our
findings show that no single method dominates across all tasks, data
attribution methods have trade-offs with simpler baselines, and method
performance is sensitive to task-specific evaluation design. Finally, we
release a public leaderboard for quick comparison of methods and to facilitate
community engagement. We hope DATE-LM serves as a foundation for future data
attribution research in LLMs.

</details>


### [23] [Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models](https://arxiv.org/abs/2507.09470)
*Mingchuan Yang,Ziyuan Huang*

Main category: cs.CL

> 通过调整超参数、预处理和模型架构，优化了joeranbosma/dragon-longformer-base-mixed-domain模型，用于临床文本的二元分类，模型在准确率、精确度、召回率以及F1-score上都有显著提升。

<details>
  <summary>Details</summary>

**Motivation:** 研究通过对DRAGON Longformer基线模型进行优化，特别是针对包含结构化医疗观察的临床案例描述的二元分类任务，以提高模型在临床应用中的效果。

**Method:** 使用包含500个临床案例的数据集进行研究，分为400个训练样本和100个验证样本。优化策略包括增加序列长度至1024个token，调整学习率，延长训练周期，以及加入专用的医学术语。

**Result:** 优化后的模型在准确率、精确度、召回率和F1-score上都有显著提升，分别为85.2%，84.1%，86.3%以及85.2%。这些改进经统计分析证明具有显著性(p < .001)。

**Conclusion:** 研究结果对特定领域的语言模型研究作出了贡献，并为临床自然语言处理的实际应用提供了有益见解。优化后的模型因其强大的表现，可望广泛应用于医疗环境。

**Abstract:** This study explores the optimization of the DRAGON Longformer base model for
clinical text classification, specifically targeting the binary classification
of medical case descriptions. A dataset of 500 clinical cases containing
structured medical observations was used, with 400 cases for training and 100
for validation. Enhancements to the pre-trained
joeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter
tuning, domain-specific preprocessing, and architectural adjustments. Key
modifications involved increasing sequence length from 512 to 1024 tokens,
adjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5
to 8, and incorporating specialized medical terminology. The optimized model
achieved notable performance gains: accuracy improved from 72.0% to 85.2%,
precision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from
71.0% to 85.2%. Statistical analysis confirmed the significance of these
improvements (p < .001). The model demonstrated enhanced capability in
interpreting medical terminology, anatomical measurements, and clinical
observations. These findings contribute to domain-specific language model
research and offer practical implications for clinical natural language
processing applications. The optimized model's strong performance across
diverse medical conditions underscores its potential for broad use in
healthcare settings.

</details>


### [24] [The CoNLL-2013 Shared Task on Grammatical Error Correction](https://arxiv.org/abs/2507.09474)
*Hwee Tou Ng,Siew Mei Wu,Yuanbin Wu,Christian Hadiwinoto,Joel Tetreault*

Main category: cs.CL

> 本文概述了CoNLL-2013共享任务，该任务聚焦语法错误纠正。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机是定义语法规错任务、提供数据集、评估指标和评分程序，并总结参与者的方法。

**Method:** 论文没有详细描述具体的研究方法，而是介绍了共享任务中各参与团队采用的各种方法。

**Result:** 论文提出了共享任务的评估结果，但没有具体的结果数据。

**Conclusion:** 论文总结了CoNLL-2013共享任务中的各种方法及其评估结果，该任务侧重于语法错误纠正。

**Abstract:** The CoNLL-2013 shared task was devoted to grammatical error correction. In
this paper, we give the task definition, present the data sets, and describe
the evaluation metric and scorer used in the shared task. We also give an
overview of the various approaches adopted by the participating teams, and
present the evaluation results.

</details>


### [25] [Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs](https://arxiv.org/abs/2507.09477)
*Yangning Li,Weizhi Zhang,Yuyao Yang,Wei-Chieh Huang,Yaozu Wu,Junyu Luo,Yuanchen Bei,Henry Peng Zou,Xiao Luo,Yusheng Zhao,Chunkit Chan,Yankai Chen,Zhongfen Deng,Yinghui Li,Hai-Tao Zheng,Dongyuan Li,Renhe Jiang,Ming Zhang,Yangqiu Song,Philip S. Yu*

Main category: cs.CL

> 本文综述了推理增强的RAG方法和RAG增强推理方法，提出了协同RAG-推理框架，进一步提升知识密集型任务的性能。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有的检索增强生成模型在需要多步推理的问题上表现不佳的问题，同时避免纯推理方法可能出现的事实性错误。通过综合这两种方法，提出一个统一的推理-检索视角。

**Method:** 通过将推理增强注入检索增强生成(RAG)的各个阶段来改进事实生成模型，并探讨不同类型检索知识如何为复杂推理提供前提和扩展上下文。同时，提出了协同RAG-推理框架，其中代理式大型语言模型迭代地交织搜索和推理以实现知识密集型基准的最先进性能。

**Result:** 论文分类了方法、数据集和开放挑战，以及提出的研究方向，以实现更有效、更具多模态适应性、值得信赖且以人类为中心的深度RAG-推理系统。

**Conclusion:** RAG和推理方法的结合为未来的研究提供了新的方向和挑战，旨在开发更有效的知识密集型任务系统。

**Abstract:** Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language
Models (LLMs) by injecting external knowledge, yet it falls short on problems
that demand multi-step inference; conversely, purely reasoning-oriented
approaches often hallucinate or mis-ground facts. This survey synthesizes both
strands under a unified reasoning-retrieval perspective. We first map how
advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,
we show how retrieved knowledge of different type supply missing premises and
expand context for complex inference (RAG-Enhanced Reasoning). Finally, we
spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs
iteratively interleave search and reasoning to achieve state-of-the-art
performance across knowledge-intensive benchmarks. We categorize methods,
datasets, and open challenges, and outline research avenues toward deeper
RAG-Reasoning systems that are more effective, multimodally-adaptive,
trustworthy, and human-centric. The collection is available at
https://github.com/DavidZWZ/Awesome-RAG-Reasoning.

</details>


### [26] [ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning](https://arxiv.org/abs/2507.09482)
*Changli Wang,Rui Wu,Fang Yin*

Main category: cs.CL

> M2SaG数据集和ViSP生成框架提升了讽刺文本的生成质量，尤其在提高讽刺意图表达方面。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于现有的讽刺生成研究大多依赖于文本模态，忽略了视觉线索，以及现有数据集中图像内容与讽刺意图之间的不匹配，本研究旨在开发一个综合图像和文本的讽刺生成数据集和相应的生成模型。

**Method:** 本文引入了M2SaG数据集，该数据集包含4,970个样本，每个样本包括一张图片、一段讽刺文本和讽刺对象。为了基准测试M2SaG，本文提出了ViSP生成框架，该框架结合了近似策略优化（PPO）和对比学习。PPO利用DIP中的奖励分数来引导讽刺文本的生成，而对比学习鼓励模型更多生成具有更高奖励分数的输出。这些策略提高了总体生成质量和讽刺意图的表达。

**Result:** ViSP在五个指标集上超越了所有基线模型，生成的文本具有更高的讽刺程度和事实不一致度。具体来说，生成的文本的平均讽刺分数为0.898，高于原始数据集的0.770；事实不一致度为0.768，高于原始的0.739。

**Conclusion:** 本文利用M2SaG数据集和ViSP生成框架成功增强了讽刺文本的生成质量，并展示了在讽刺意图表达方面的显著改进，揭示了大型语言模型在讽刺生成任务上的局限性。

**Abstract:** Human emotions are complex, with sarcasm being a subtle and distinctive form.
Despite progress in sarcasm research, sarcasm generation remains underexplored,
primarily due to the overreliance on textual modalities and the neglect of
visual cues, as well as the mismatch between image content and sarcastic intent
in existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm
generation dataset with 4,970 samples, each containing an image, a sarcastic
text, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation
framework that integrates Proximal Policy Optimization (PPO) and contrastive
learning. PPO utilizes reward scores from DIP to steer the generation of
sarcastic texts, while contrastive learning encourages the model to favor
outputs with higher reward scores. These strategies improve overall generation
quality and produce texts with more pronounced sarcastic intent. We evaluate
ViSP across five metric sets and find it surpasses all baselines, including
large language models, underscoring their limitations in sarcasm generation.
Furthermore, we analyze the distributions of Sarcasm Scores and Factual
Incongruity for both M2SaG and the texts generated by ViSP. The generated texts
exhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity
(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic
content than the original dataset. % The dataset and code will be publicly
available. Our dataset and code will be released at
\textit{https://github.com/wclapply/ViSP}.

</details>


### [27] [Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.09485)
*Junjie Liu,Yuanhe Tian,Yan Song*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in
social media scenarios to identify the sentiment polarity of specific aspect
terms in a sentence. Although many existing studies leverage large language
models (LLMs) to perform ABSA due to their strong context understanding
capabilities, they still face challenges to learn the context information in
the running text because of the short text, as well as the small and unbalanced
labeled training data, where most data are labeled with positive sentiment.
Data augmentation (DA) is a feasible strategy for providing richer contextual
information, especially when using LLMs to create synthetic training data, but
faces challenges in ensuring a high quality of the augmented data.In this
paper, we propose an LLM-based ABSA approach with training data
augmentation.Specifically, an LLM is prompted to generate augmented training
data based on the original training data, so as to construct a new training
data with larger size and balanced label distributions to better train an ABSA
model. Meanwhile, in order to improve the quality of the augmented data, we
propose a reinforcement learning approach to optimize the data augmentation.
LLM.Experiment results and further analyses on English benchmark datasets for
ABSA demonstrate the effectiveness of our approach, where superior performance
is observed over strong baselines and most existing studies.

</details>


### [28] [GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities](https://arxiv.org/abs/2507.09497)
*Siyi Wu,Zeyu Wang,Xinyuan Song,Zhengpeng Zhou,Lifan Sun,Tianyu Shi*

Main category: cs.CL

> 研究提出了GoalfyMax框架，以新的通信方式和记忆系统提高多智能体之间的协调和适应能力，其在实验中展示了优于基线框架的表现。

<details>
  <summary>Details</summary>

**Motivation:** 为了应对传统单一用途AI系统在现实环境中扩展性受限的问题，该研究提出了GoalfyMax框架，以解决智能体之间缺乏协调、记忆复用和任务分解能力的问题。

**Method:** GoalfyMax采用基于Model Context Protocol (MCP)的Agent-to-Agent (A2A)通信层，实现智能体之间的异步协调。此外，它采用Experience Pack (XP)架构，这是一种多层记忆系统，用来保留任务理性和执行轨迹，支持结构化知识保存和持续学习。系统还能支援多轮上下文对话、长短时记忆模块及动态安全性验证，支持健壮、实时的战略适应。

**Result:** 实验证明，GoalfyMax在复杂任务编排基准和案例研究中，相较于基线框架，具有更高的适应性、协调性和经验重用度。

**Conclusion:** 这些发现表明GoalfyMax作为一个可伸缩、具备未来技术能力的多智能体智能系统基础框架有潜在的实用价值。

**Abstract:** Modern enterprise environments demand intelligent systems capable of handling
complex, dynamic, and multi-faceted tasks with high levels of autonomy and
adaptability. However, traditional single-purpose AI systems often lack
sufficient coordination, memory reuse, and task decomposition capabilities,
limiting their scalability in realistic settings. To address these challenges,
we present \textbf{GoalfyMax}, a protocol-driven framework for end-to-end
multi-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent
(A2A) communication layer built on the Model Context Protocol (MCP), allowing
independent agents to coordinate through asynchronous, protocol-compliant
interactions. It incorporates the Experience Pack (XP) architecture, a layered
memory system that preserves both task rationales and execution traces,
enabling structured knowledge retention and continual learning. Moreover, our
system integrates advanced features including multi-turn contextual dialogue,
long-short term memory modules, and dynamic safety validation, supporting
robust, real-time strategy adaptation. Empirical results on complex task
orchestration benchmarks and case study demonstrate that GoalfyMax achieves
superior adaptability, coordination, and experience reuse compared to baseline
frameworks. These findings highlight its potential as a scalable, future-ready
foundation for multi-agent intelligent systems.

</details>


### [29] [Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models](https://arxiv.org/abs/2507.09506)
*Junjie Wu,Gefei Gu,Yanan Zheng,Dit-Yan Yeung,Arman Cohan*

Main category: cs.CL

> 该论文提出了Ref-Long，一个评估LCLMs在长上下文引用中表现的新基准，实验和分析揭示了当前模型的不足。

<details>
  <summary>Details</summary>

**Motivation:** 由于长上下文引用是长上下文理解中的重要任务，而这一领域尚未得到充分的探索，该论文的目的在于填补这一研究空白。

**Method:** 该论文设计了一个新的基准测试Ref-Long，用于评估长上下文语言模型（LCLMs）的长上下文引用能力，包括合成到现实场景的三个子集。

**Result:** 实验结果显示，即使是像GPT-4o这样的先进模型在长上下文引用方面也存在显著不足。

**Conclusion:** 通过全面的分析，包括人类评估、任务格式调整、微调实验和错误分析，作者得出了几个关键见解。

**Abstract:** Long-context language models (LCLMs) have exhibited impressive capabilities
in long-context understanding tasks. Among these, long-context referencing -- a
crucial task that requires LCLMs to attribute items of interest to specific
parts of long-context data -- remains underexplored. To bridge this gap, this
paper proposes Referencing Evaluation for Long-context Language Models
(Ref-Long), a novel benchmark designed to assess the long-context referencing
capability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the
indexes of documents that reference a specific key, emphasizing contextual
relationships between the key and the documents over simple retrieval. Based on
the task design, we construct three subsets ranging from synthetic to realistic
scenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs
reveal significant shortcomings in long-context referencing, even among
advanced models like GPT-4o. To further investigate these challenges, we
conduct comprehensive analyses, including human evaluations, task format
adjustments, fine-tuning experiments, and error analyses, leading to several
key insights. Our data and code can be found in https://github.
com/wujunjie1998/Ref-Long.

</details>


### [30] [How Important is `Perfect' English for Machine Translation Prompts?](https://arxiv.org/abs/2507.09509)
*Patrícia Schmidtová,Niyati Bafna,Seth Aycock,Gianluca Vico,Wiktor Kamzela,Katharina Hämmerl,Vilém Zouhar*

Main category: cs.CL

> 研究发现提示词的质量对大规模语言模型的机器翻译性能有显著影响，且不同类型的噪声对翻译质量的影响也不同。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在评估提示错误对大规模语言模型（LLMs）机器翻译性能的影响，理解不同类型的错误如何影响翻译质量，并提供有关模型如何响应用户提示中不同噪声水平的定性和定量见解。

**Method:** 通过系统地研究人类理解和合成错误对大规模语言模型（LLMs）在机器翻译和机器翻译评估任务上性能的影响，采用定量分析和定性分析方法来评估不同类型的噪声对翻译性能的影响。

**Result:** 该研究系统地评估了人类可理解误差和合成误差对大规模语言模型（LLMs）在机器翻译和机器翻译评估任务上性能的影响。研究表明，提示词的质量强烈影响翻译性能：当误差多时，即使是一个好的提示可能比没有误差的最小或较差的提示表现更差。然而，不同类型的噪声对翻译质量的影响不同，字符级别的和组合的噪声比语义结构的扰动更严重地影响性能。定性分析表明，提示词质量下降主要是由于模型较差的指令遵循，而不是直接影响翻译质量。此外，LLMs 甚至能在一个人类难以理解的提示场景中进行翻译。

**Conclusion:** 提示词的质量对翻译性能有强烈的影响，不同的噪声类型对性能的影响也各异。质量差的提示主要影响的是指令遵循能力而非翻译质量本身。即便在过于随机的噪声场景下，大规模语言模型仍能进行翻译。

**Abstract:** Large language models (LLMs) have achieved top results in recent machine
translation evaluations, but they are also known to be sensitive to errors and
perturbations in their prompts. We systematically evaluate how both humanly
plausible and synthetic errors in user prompts affect LLMs' performance on two
related tasks: Machine translation and machine translation evaluation. We
provide both a quantitative analysis and qualitative insights into how the
models respond to increasing noise in the user prompt.
  The prompt quality strongly affects the translation performance: With many
errors, even a good prompt can underperform a minimal or poor prompt without
errors. However, different noise types impact translation quality differently,
with character-level and combined noisers degrading performance more than
phrasal perturbations. Qualitative analysis reveals that lower prompt quality
largely leads to poorer instruction following, rather than directly affecting
translation quality itself. Further, LLMs can still translate in scenarios with
overwhelming random noise that would make the prompt illegible to humans.

</details>


### [31] [Adapting Definition Modeling for New Languages: A Case Study on Belarusian](https://arxiv.org/abs/2507.09536)
*Daniela Kazakouskaya,Timothee Mickus,Janine Siewert*

Main category: cs.CL

> 研究提出了一种新的定义生成数据集，并将其应用于白俄罗斯语的模型适配中。

<details>
  <summary>Details</summary>

**Motivation:** 定义模型生成任务在为语言学家记录不同方言和语言提供了巨大前景，但需要进一步研究如何将现有模型应用于尚未支持的语言中。

**Method:** 本研究提出了一个包含43,150个定义的新数据集，专注于将现有的定义生成模型适应到白俄罗斯语上。

**Result:** 实验表明，将定义生成系统适应到新语言需要相对较少的数据，但当前的自动评估指标仍存在一些不足。

**Conclusion:** 适应现有模型到白俄罗斯语中的研究表明，该过程需要的数据量相对较小，但自动评估指标尚不完善。

**Abstract:** Definition modeling, the task of generating new definitions for words in
context, holds great prospect as a means to assist the work of lexicographers
in documenting a broader variety of lects and languages, yet much remains to be
done in order to assess how we can leverage pre-existing models for as-of-yet
unsupported languages. In this work, we focus on adapting existing models to
Belarusian, for which we propose a novel dataset of 43,150 definitions. Our
experiments demonstrate that adapting a definition modeling systems requires
minimal amounts of data, but that there currently are gaps in what automatic
metrics do capture.

</details>


### [32] [NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance](https://arxiv.org/abs/2507.09601)
*Hanwool Lee,Sara Yu,Yewon Hwang,Jonghyun Choi,Heejae Ahn,Sungbum Jung,Youngjae Yu*

Main category: cs.CL

> 本文介绍了NMIXX，一种用于跨语言探索金融领域的神经嵌入模型，并发布了KorFinSTS基准测试。结果显示，在金融STS任务上，NMIXX显著优于其他开源模型。这强调了分词器设计在金融领域和低资源语言中的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 由于领域特定的行话、时间意义的转变以及双语词汇表的对齐问题，通用句子嵌入模型很难捕捉到特定于金融领域的语义，尤其是在韩语这样的低资源语言中。通过引入NMIXX，旨在填补这一空白。

**Method:** 介绍了一种名为NMIXX（Neural eMbeddings for Cross-lingual eXploration of Finance）的方法，这是一种跨语言嵌入模型，通过使用18.8K对高置信度的三元组进行微调，这些三元组包括领域内的近义词对、基于语义变化类型学的硬反例以及韩英精确翻译。同时发布了KorFinSTS，这是一个包含1,921对韩语金融STS基准测试，涵盖了新闻、披露、研究报告和法规，旨在揭示一般基准测试可能遗漏的细微差别。

**Result:** 在对七种开源基线进行评估时，NMIXX的多语言bge-m3变体在English FinSTS上获得了+0.10的Spearman's rho得分提升，在KorFinSTS上获得了+0.22的Spearman's rho得分提升，超过其他模型并且比未预适应的检查点表现更好。

**Conclusion:** 研究表明，具有更丰富的韩语标记覆盖范围的模型能更有效地适应，突显了在低资源跨语言情境中分词器设计的重要性。通过公开提供模型和基准，为金融领域的领域自适应和多语言表征学习提供了有力工具。

**Abstract:** General-purpose sentence embedding models often struggle to capture
specialized financial semantics, especially in low-resource languages like
Korean, due to domain-specific jargon, temporal meaning shifts, and misaligned
bilingual vocabularies. To address these gaps, we introduce NMIXX (Neural
eMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual
embedding models fine-tuned with 18.8K high-confidence triplets that pair
in-domain paraphrases, hard negatives derived from a semantic-shift typology,
and exact Korean-English translations. Concurrently, we release KorFinSTS, a
1,921-pair Korean financial STS benchmark spanning news, disclosures, research
reports, and regulations, designed to expose nuances that general benchmarks
miss.
  When evaluated against seven open-license baselines, NMIXX's multilingual
bge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and
+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing
other models by the largest margin, while revealing a modest trade-off in
general STS performance. Our analysis further shows that models with richer
Korean token coverage adapt more effectively, underscoring the importance of
tokenizer design in low-resource, cross-lingual settings. By making both models
and the benchmark publicly available, we provide the community with robust
tools for domain-adapted, multilingual representation learning in finance.

</details>


### [33] [SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks](https://arxiv.org/abs/2507.09628)
*Salvatore Citraro,Edith Haim,Alessandra Carini,Cynthia S. Q. Siew,Giulio Rossetti,Massimo Stella*

Main category: cs.CL

> 介绍SpreadPy，一个用于模拟认知过程扩散激活的Python库。通过三个案例研究表明其在理解认知、心理和临床现象中的作用。

<details>
  <summary>Details</summary>

**Motivation:** 研发SpreadPy的动机是为认知、心理和临床现象提供一个数值模拟工具，以分析激活动力学背后的机制。

**Method:** 我们介绍了SpreadPy，这是一个用于模拟认知单层和多层网络中扩散激活的Python库。工具旨在执行数值模拟以测试认知过程中的结构-功能关系。通过将模拟结果与知识建模中的理论进行比较，SpreadPy使系统地调查激活动力学如何反映认知、心理和临床现象成为可能。

**Result:** 通过三个案例研究展示了该库的实用性：(1) 高数学焦虑与低数学焦虑学生的关联知识网络上的扩散激活模式不同，揭示了焦虑相关的结构差异。(2) 创造性任务的模拟显示，激活轨迹随任务难度变化，揭示了认知负荷如何调节词汇访问。(3) 在失语症个体中，词汇网络上的模拟激活模式与命名任务中的实证错误类型（语义与语音学）相关联，将网络结构与临床损伤联系起来。

**Conclusion:** SpreadPy是一款灵活的框架，研究人员可以使用经验性或理论网络来模拟这些过程，为个体差异和认知损伤提供机制性见解。该库公开提供，支持心理学、神经科学和教育研究的可复制研究。

**Abstract:** We introduce SpreadPy as a Python library for simulating spreading activation
in cognitive single-layer and multiplex networks. Our tool is designed to
perform numerical simulations testing structure-function relationships in
cognitive processes. By comparing simulation results with grounded theories in
knowledge modelling, SpreadPy enables systematic investigations of how
activation dynamics reflect cognitive, psychological and clinical phenomena. We
demonstrate the library's utility through three case studies: (1) Spreading
activation on associative knowledge networks distinguishes students with high
versus low math anxiety, revealing anxiety-related structural differences in
conceptual organization; (2) Simulations of a creativity task show that
activation trajectories vary with task difficulty, exposing how cognitive load
modulates lexical access; (3) In individuals with aphasia, simulated activation
patterns on lexical networks correlate with empirical error types (semantic vs.
phonological) during picture-naming tasks, linking network structure to
clinical impairments. SpreadPy's flexible framework allows researchers to model
these processes using empirically derived or theoretical networks, providing
mechanistic insights into individual differences and cognitive impairments. The
library is openly available, supporting reproducible research in psychology,
neuroscience, and education research.

</details>


### [34] [An Exploration of Knowledge Editing for Arabic](https://arxiv.org/abs/2507.09629)
*Basel Mousi,Nadir Durrani,Fahim Dalvi*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** While Knowledge Editing (KE) has been widely explored in English, its
behavior in morphologically rich languages like Arabic remains underexamined.
In this work, we present the first study of Arabic KE. We evaluate four methods
(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact
benchmarks, analyzing both multilingual and cross-lingual settings. Our
experiments on Llama-2-7B-chat show show that parameter-based methods struggle
with cross-lingual generalization, while instruction-tuned methods perform more
robustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show
that joint Arabic-English training improves both editability and transfer. We
release Arabic KE benchmarks and multilingual training for LTE data to support
future research.

</details>


### [35] [Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?](https://arxiv.org/abs/2507.09638)
*Pawitsapak Akarajaradwong,Chompakorn Chaksangchaichot,Pirat Pothavorn,Attapol Thamrongrattanarit-Rutherford,Ekapol Chuangsuwanich,Sarana Nutanong*

Main category: cs.CL

> 采用GRPO方法并利用BGE-M3嵌入式作为奖励机制，显著提高泰国法律问答中引用准确性和回答质量，且成本效益高。

<details>
  <summary>Details</summary>

**Motivation:** 针对泰国法律问答中RETRIEVAL-AUGMENTED GENERATION (RAG)系统在需要复杂法律推理的问题上的表现不佳，提出改进方法。

**Method:** 采用Group-Relative Policy Optimization (GRPO)方法，并利用BGE-M3嵌入式作为成本效益高的语义相似度奖励，来提高泰国法律问题回答中法律引用的准确性及回答质量。

**Result:** 在NitiBench基准测试中，相较于基础模型，GRPO实现了高达90%的引用F1值提升，并在联合质量指标上比指令调优高出31%。

**Conclusion:** 该方法相较于指令调优，在复杂的法律推理任务上表现出更强的鲁棒性，为提升泰国法律LLMs提供了有效的资源高效解决方案。

**Abstract:** The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal
question answering is still limited, especially for questions requiring
extensive, complex legal reasoning. To address these limitations, we introduce
an approach aligning LLMs toward improved law citation accuracy and better
response quality using Group-Relative Policy Optimization (GRPO). Our approach
leverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,
significantly reducing computational expenses up to 2.5x compared to large
language model judges. Experiments on the NitiBench benchmark demonstrate
substantial improvements: GRPO achieves up to 90% citation-F1 gains from the
base model and a 31% increase in joint quality metrics over instruction tuning.
Crucially, our method shows enhanced robustness on complex legal reasoning
tasks compared to instruction tuning, providing an effective and
resource-efficient solution for enhancing Thai legal LLMs.

</details>


### [36] [MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs](https://arxiv.org/abs/2507.09701)
*Shulin Huang,Linyi Yang,Yue Zhang*

Main category: cs.CL

> MCEval是一个新的多语言评估框架，评估13种文化和13种语言下的文化意识和偏见，揭示了最佳文化表现与语言分布和语言-文化一致性相关，并暴露了英语场景中方法在其他语言下的不公平性。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型表现出文化偏见和跨文化理解能力有限，在服务多样化的全球用户群体时尤为明显。为了改进这一点，需要一个评估框架来衡量语言模型的文化理解和偏见。

**Method:** 提出MCEval，这是一个新型的多语言评估框架，利用动态文化问题构建，并通过反事实重述和混淆因素重述实现因果分析。该框架覆盖了13种文化和13种语言，系统地评估不同语言背景下的文化意识和文化偏见。

**Result:** 实验结果显示，在不同的语言背景下，性能存在差异。最佳的文化表现不仅与训练数据的分布有关，还与语言文化的一致性有关。该评估还表明，在英语场景中看似成功的方法在其他语言中可能会有严重的劣势。

**Conclusion:** MCEval是第一个全面的多语言文化评估框架，提供了更深入的洞察，以理解语言模型的文化理解能力。

**Abstract:** Large language models exhibit cultural biases and limited cross-cultural
understanding capabilities, particularly when serving diverse global user
populations. We propose MCEval, a novel multilingual evaluation framework that
employs dynamic cultural question construction and enables causal analysis
through Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive
evaluation spans 13 cultures and 13 languages, systematically assessing both
cultural awareness and cultural bias across different linguistic scenarios. The
framework provides 39,897 cultural awareness instances and 17,940 cultural bias
instances. Experimental results reveal performance disparities across different
linguistic scenarios, demonstrating that optimal cultural performance is not
only linked to training data distribution, but also is related to
language-culture alignment. The evaluation results also expose the fairness
issue, where approaches appearing successful in the English scenario create
substantial disadvantages. MCEval represents the first comprehensive
multilingual cultural evaluation framework that provides deeper insights into
LLMs' cultural understanding.

</details>


### [37] [Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces](https://arxiv.org/abs/2507.09709)
*Baturay Saglam,Paul Kassianik,Blaine Nelson,Sajana Weerawardhena,Yaron Singer,Amin Karbasi*

Main category: cs.CL

> 研究发现大规模语言模型的潜在空间几何结构在语义理解中起到重要作用，支持开发新的工具检测并减轻潜在的有害内容。

<details>
  <summary>Details</summary>

**Motivation:** 研究大规模语言模型（LLMs）如何在内部组织与语义理解相关的表示，以解释其行为并提高对齐性。

**Method:** 通过大规模实证研究，分析了11个基于transformer的仅解码器模型，在6个科学主题和每个主题的12个层次上的隐藏状态。

**Result:** 发现高层次的语义信息一致存在于低维子空间中，并在不同领域中形成线性可分表示。这种可分性在更深的层和需要结构化推理或对齐行为的提示下变得更加明显。这种几何结构使隐藏空间中的简单因果干预成为可能，例如，可以通过一个向量方向捕捉到链式思维等推理模式。

**Conclusion:** 这些发现支持开发几何感知工具，直接对潜在表示进行操作，以检测和减轻有害或恶意内容，例如使用基于传输的防御方法。作为概念验证，训练了一个简单的MLP分类器作为轻量级的潜在空间守门员，能够高精度地检测对抗性和恶意提示。

**Abstract:** Understanding the latent space geometry of large language models (LLMs) is
key to interpreting their behavior and improving alignment. \baturay{However,
it remains unclear to what extent LLMs internally organize representations
related to semantic understanding. To investigate this, we conduct a
large-scale empirical study of hidden states in transformer-based LLMs,
analyzing 11 decoder-only models across 6 scientific topics and 12 layers each.
We find that high-level semantic information consistently lies in
low-dimensional subspaces that form linearly separable representations across
distinct domains. This separability becomes more pronounced in deeper layers
and under prompts that trigger structured reasoning or alignment
behaviors$\unicode{x2013}$even when surface content is unchanged. This geometry
enables simple yet effective causal interventions in hidden space; for example,
reasoning patterns like chain-of-thought can be captured by a single vector
direction. Together, these findings support the development of geometry-aware
tools that operate directly on latent representations to detect and mitigate
harmful or adversarial content, using methods such as transport-based defenses
that leverage this separability. As a proof of concept, we demonstrate this
potential by training a simple MLP classifier as a lightweight latent-space
guardrail, which detects adversarial and malicious prompts with high precision.

</details>


### [38] [Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding](https://arxiv.org/abs/2507.09758)
*Qi Feng,Yihong Liu,Hinrich Schütze*

Main category: cs.CL

> 论文介绍了基于PLMs预测难度分数的自适应课程学习范式，探索了多种训练策略，验证了该方法在自然语言理解任务中的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 大多数现有的课程学习方法依赖于手动定义的难度指标，这可能无法准确反映模型自身的视角。

**Method:** 该论文提出了一种自适应课程学习范式，根据预训练语言模型（PLMs）自己预测的难度分数对微调示例进行优先级排序。

**Result:** 实验证明，该方法相比标准的随机抽样方法，能够更快地收敛并改善性能。

**Conclusion:** 实验结果表明，自适应课程学习范式可以提高学习效率和模型性能。

**Abstract:** Curriculum learning is a widely adopted training strategy in natural language
processing (NLP), where models are exposed to examples organized by increasing
difficulty to enhance learning efficiency and performance. However, most
existing approaches rely on manually defined difficulty metrics -- such as text
length -- which may not accurately reflect the model's own perspective. To
overcome this limitation, we present a self-adaptive curriculum learning
paradigm that prioritizes fine-tuning examples based on difficulty scores
predicted by pre-trained language models (PLMs) themselves. Building on these
scores, we explore various training strategies that differ in the ordering of
examples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed
sampling. We evaluate our method on four natural language understanding (NLU)
datasets covering both binary and multi-class classification tasks.
Experimental results show that our approach leads to faster convergence and
improved performance compared to standard random sampling.

</details>


### [39] [Te Ahorré Un Click: A Revised Definition of Clickbait and Detection in Spanish News](https://arxiv.org/abs/2507.09777)
*Gabriel Mordecki,Guillermo Moncecchi,Javier Couto*

Main category: cs.CL

> The paper revises the definition of clickbait, proposes a new definition focusing on the creation of a curiosity gap, and develops a new dataset for Spanish clickbait detection.

<details>
  <summary>Details</summary>

**Motivation:** To revise the definition of clickbait and argue that the creation of a curiosity gap is the key concept that distinguishes clickbait from other related phenomena.

**Method:** We introduce a new approach to clickbait detection datasets creation, by refining the concept limits and annotations criteria, minimizing the subjectivity in the decision as much as possible.

**Result:** They created TA1C, the first open source dataset for clickbait detection in Spanish, consisting of 3,500 tweets from 18 media sources, manually annotated and with a 0.825 Fleiss' K inter annotator agreement.

**Conclusion:** The study introduces a new open source dataset for clickbait detection in Spanish and implements strong baselines achieving 0.84 in F1-score.

**Abstract:** We revise the definition of clickbait, which lacks current consensus, and
argue that the creation of a curiosity gap is the key concept that
distinguishes clickbait from other related phenomena such as sensationalism and
headlines that do not deliver what they promise or diverge from the article.
Therefore, we propose a new definition: clickbait is a technique for generating
headlines and teasers that deliberately omit part of the information with the
goal of raising the readers' curiosity, capturing their attention and enticing
them to click. We introduce a new approach to clickbait detection datasets
creation, by refining the concept limits and annotations criteria, minimizing
the subjectivity in the decision as much as possible. Following it, we created
and release TA1C (for Te Ahorr\'e Un Click, Spanish for Saved You A Click), the
first open source dataset for clickbait detection in Spanish. It consists of
3,500 tweets coming from 18 well known media sources, manually annotated and
reaching a 0.825 Fleiss' K inter annotator agreement. We implement strong
baselines that achieve 0.84 in F1-score.

</details>


### [40] [Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition](https://arxiv.org/abs/2507.09875)
*Qinyuan Ye,Robin Jia,Xiang Ren*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Large language models demonstrate the intriguing ability to perform unseen
tasks via in-context learning. However, it remains unclear what mechanisms
inside the model drive such task-level generalization. In this work, we
approach this question through the lens of off-by-one addition (i.e., 1+1=3,
2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function
as a second step. Leveraging circuit-style interpretability techniques such as
path patching, we analyze the models' internal computations behind their
notable performance and present three key findings. First, we uncover a
function induction mechanism that explains the model's generalization from
standard addition to off-by-one addition. This mechanism resembles the
structure of the induction head mechanism found in prior work and elevates it
to a higher level of abstraction. Second, we show that the induction of the +1
function is governed by multiple attention heads in parallel, each of which
emits a distinct piece of the +1 function. Finally, we find that this function
induction mechanism is reused in a broader range of tasks, including synthetic
tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8
addition. Overall, our findings offer deeper insights into how reusable and
composable structures within language models enable task-level generalization.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [41] [View Invariant Learning for Vision-Language Navigation in Continuous Environments](https://arxiv.org/abs/2507.08831)
*Josh Qixuan Sun,Xiaoying Xing,Huaiyuan Weng,Chul Min Yeum,Mark Crowley*

Main category: cs.CV

> 本文提出了V2-VLNCE（带有变化视角的VLNCE）场景，并提出了一种视点不变的学习策略VIL，增强了现有导航策略对相机视点变化的鲁棒性，在实验中VIL的表现优于最先进的方法。

<details>
  <summary>Details</summary>

**Motivation:** 大多数导航策略对视点变化敏感，而视点变化又会影响代理的观察。

**Method:** 该论文采用了一种对比学习框架学习稀疏视点不变特征，并引入了一个教师-学生框架用于Waypoint预测器模块，通过一个视点相关的教师模型将知识传授给视点不变的学生模型，最终采用端到端训练模式进行联合优化。

**Result:** 实验结果显示，该方法在V2-VLNCE场景下，对于两个标准的数据集R2R-CE和RxR-CE，其成功率超过了最先进的方法8-15%。

**Conclusion:** 该论文提出的方法不仅在变化视点的场景下表现出色，甚至在标准VLNCE设置中也能提升性能，可以说是‘即插即用’的后训练策略。

**Abstract:** Vision-Language Navigation in Continuous Environments (VLNCE), where an agent
follows instructions and moves freely to reach a destination, is a key research
problem in embodied AI. However, most navigation policies are sensitive to
viewpoint changes, i.e., variations in camera height and viewing angle that
alter the agent's observation. In this paper, we introduce a generalized
scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View
Invariant Learning), a view-invariant post-training strategy that enhances the
robustness of existing navigation policies to changes in camera viewpoint. VIL
employs a contrastive learning framework to learn sparse and view-invariant
features. Additionally, we introduce a teacher-student framework for the
Waypoint Predictor Module, a core component of most VLNCE baselines, where a
view-dependent teacher model distills knowledge into a view-invariant student
model. We employ an end-to-end training paradigm to jointly optimize these
components, thus eliminating the cost for individual module training. Empirical
results show that our method outperforms state-of-the-art approaches on
V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets
R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE
setting and find that, despite being trained for varied viewpoints, it often
still improves performance. On the more challenging RxR-CE dataset, our method
also achieved state-of-the-art performance across all metrics when compared to
other map-free methods. This suggests that adding VIL does not diminish the
standard viewpoint performance and can serve as a plug-and-play post-training
method.

</details>


### [42] [Detecting Deepfake Talking Heads from Facial Biometric Anomalies](https://arxiv.org/abs/2507.08917)
*Justin D. Norman,Hany Farid*

Main category: cs.CV

> 本文提出了一种基于机器学习的新方法来检测深度伪造视频里的冒充行为。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于高度逼真的声音克隆和视觉吸引的头像、面部替换或唇同步深度伪造视频生成，使得用任何人说任何内容的视频变得相对容易制造，这样的深度伪造形象经常被用来驱动欺诈、骗局和政治误导。

**Method:** 我们提出了一种新颖的基于面部生物特征中不自然模式的机器学习取证技术，用于检测深度伪造视频冒充。

**Result:** 我们对该技术进行了广泛的评估，涵盖了众多的深度伪造技术和冒充案例，并且评估了该技术在视频洗白中的可靠性以及它对以前未见过的视频深度伪造生成器的泛化能力。

**Conclusion:** 我们的技术可以通过检测面部生物特征中的异常模式来有效地识别深度伪造视频中的冒充行为。

**Abstract:** The combination of highly realistic voice cloning, along with visually
compelling avatar, face-swap, or lip-sync deepfake video generation, makes it
relatively easy to create a video of anyone saying anything. Today, such
deepfake impersonations are often used to power frauds, scams, and political
disinformation. We propose a novel forensic machine learning technique for the
detection of deepfake video impersonations that leverages unnatural patterns in
facial biometrics. We evaluate this technique across a large dataset of
deepfake techniques and impersonations, as well as assess its reliability to
video laundering and its generalization to previously unseen video deepfake
generators.

</details>


### [43] [PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection](https://arxiv.org/abs/2507.08979)
*Mahdiyar Molahasani,Azadeh Motamedi,Michael Greenspan,Il-Min Kim,Ali Etemad*

Main category: cs.CV

> PRISM是一种数据无关、任务无关的方案，旨在通过新颖的对比去偏技术，减轻视觉-语言模型的偏差，且不依赖于预定义的偏差类别或额外的外部数据。

<details>
  <summary>Details</summary>

**Motivation:** 视觉-语言模型（如CLIP）继承并放大了训练数据中的偏差，导致预测偏斜，PRISM为解决这个问题而设计。

**Method:** PRISM采用两阶段方法来减轻视觉-语言模型中的偏差，首先使用LLM生成带有偶然关联的场景描述，其次使用创新的对比去偏损失学习映射嵌入到最小化偶然关联并保持图像和文本嵌入对齐的潜在空间的投影。

**Result:** 实验表明，PRISM在Waterbirds和CelebA数据集上优于现有的去偏差方法。

**Conclusion:** PRISM提供了一种有效且无需额外数据的方法来减轻视觉-语言模型中的偏差，提高了模型的公平性和准确性。

**Abstract:** We introduce Projection-based Reduction of Implicit Spurious bias in
vision-language Models (PRISM), a new data-free and task-agnostic solution for
bias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in
their training data, leading to skewed predictions. PRISM is designed to debias
VLMs without relying on predefined bias categories or additional external data.
It operates in two stages: first, an LLM is prompted with simple class prompts
to generate scene descriptions that contain spurious correlations. Next, PRISM
uses our novel contrastive-style debiasing loss to learn a projection that maps
the embeddings onto a latent space that minimizes spurious correlations while
preserving the alignment between image and text embeddings.Extensive
experiments demonstrate that PRISM outperforms current debiasing methods on the
commonly used Waterbirds and CelebA datasets We make our code public at:
https://github.com/MahdiyarMM/PRISM.

</details>


### [44] [Video Inference for Human Mesh Recovery with Vision Transformer](https://arxiv.org/abs/2507.08981)
*Hanbyel Cho,Jaesung Ahn,Yooshin Cho,Junmo Kim*

Main category: cs.CV

> 提出了HMR-ViT，结合时空信息进行人体网格恢复，利用CRM和视觉转换器技术，在3DPW和Human3.6M数据集上取得了有竞争力的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的人体网格恢复方法要么利用时间信息，要么利用运动学关系，但没有同时使用这两种信息。因此，我们提出了一种结合两者的方法。

**Method:** HMR-ViT通过使用视频帧中的特征向量构建时空特征图像，利用通道重排矩阵(CRM)使相似的运动学特征在空间上靠近，并通过视觉转换器进一步编码该特征图像，最后使用回归网络推断SMPL姿态和形状参数。

**Result:** 在3DPW和Human3.6M数据集上进行了广泛的评估，表明HMR-ViT在此任务中取得了有竞争力的性能。

**Conclusion:** HMR-ViT展示了在人体网格恢复任务中结合时间和运动学信息的有效性，使用视觉转换器提高了网格恢复的性能。

**Abstract:** Human Mesh Recovery (HMR) from an image is a challenging problem because of
the inherent ambiguity of the task. Existing HMR methods utilized either
temporal information or kinematic relationships to achieve higher accuracy, but
there is no method using both. Hence, we propose "Video Inference for Human
Mesh Recovery with Vision Transformer (HMR-ViT)" that can take into account
both temporal and kinematic information. In HMR-ViT, a Temporal-kinematic
Feature Image is constructed using feature vectors obtained from video frames
by an image encoder. When generating the feature image, we use a Channel
Rearranging Matrix (CRM) so that similar kinematic features could be located
spatially close together. The feature image is then further encoded using
Vision Transformer, and the SMPL pose and shape parameters are finally inferred
using a regression network. Extensive evaluation on the 3DPW and Human3.6M
datasets indicates that our method achieves a competitive performance in HMR.

</details>


### [45] [From images to properties: a NeRF-driven framework for granular material parameter inversion](https://arxiv.org/abs/2507.09005)
*Cheng-Hsi Hsiao,Krishna Kumar*

Main category: cs.CV

> 本文提出了一种结合NeRF和MPM的新框架，通过视觉观测估计颗粒材料的摩擦角，实现误差在2度之内的高精度估计。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在解决现实场景中直接测量颗粒材料特性不切实际或不可能的问题，提出了一种从视觉观测数据中推断材料特性的创新方法。

**Method:** 本文提出了一种将Neural Radiance Fields (NeRF)与Material Point Method (MPM)模拟相结合的新框架，用于从视觉观测中推断出颗粒材料的特性。具体方法是从合成实验数据开始，模拟犁与沙子的相互作用，生成多视角图像，并利用NeRF重构初始状态的3D几何，重建的几何用于MPM仿真中材料点位置的初始化。通过贝叶斯优化最小化图像损失，估计最佳拟合摩擦角。

**Result:** 通过对比仿真图像与观测图像，本文成功地将摩擦角估计误差限定在2度以内，验证了所提方法的有效性。

**Conclusion:** 研究表明，从视觉观察中估计颗粒材料摩擦角的方法是有效的，这种方法为无法直接测量材料特性的现实场景提供了一种解决方案。

**Abstract:** We introduce a novel framework that integrates Neural Radiance Fields (NeRF)
with Material Point Method (MPM) simulation to infer granular material
properties from visual observations. Our approach begins by generating
synthetic experimental data, simulating an plow interacting with sand. The
experiment is rendered into realistic images as the photographic observations.
These observations include multi-view images of the experiment's initial state
and time-sequenced images from two fixed cameras. Using NeRF, we reconstruct
the 3D geometry from the initial multi-view images, leveraging its capability
to synthesize novel viewpoints and capture intricate surface details. The
reconstructed geometry is then used to initialize material point positions for
the MPM simulation, where the friction angle remains unknown. We render images
of the simulation under the same camera setup and compare them to the observed
images. By employing Bayesian optimization, we minimize the image loss to
estimate the best-fitting friction angle. Our results demonstrate that friction
angle can be estimated with an error within 2 degrees, highlighting the
effectiveness of inverse analysis through purely visual observations. This
approach offers a promising solution for characterizing granular materials in
real-world scenarios where direct measurement is impractical or impossible.

</details>


### [46] [VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels](https://arxiv.org/abs/2507.09008)
*Xiwei Xuan,Xiaoqi Wang,Wenbin He,Jorge Piazentin Ono,Liang Gou,Kwan-Liu Ma,Liu Ren*

Main category: cs.CV

> 本文提出了VISTA框架，以改进多模态模型生成标签的质量，通过多阶段数据验证策略与人类专业知识相结合的方式，提高了模型在复杂下游任务中的表现。

<details>
  <summary>Details</summary>

**Motivation:** 尽管多模态基础模型的进展促进了大规模数据集的自动生成标签，提升了模型在开放词汇目标检测和分割等棘手下游任务中的表现，但现有方法大多关注数据量而非数据质量，且在缺乏真实标签的情况下，验证大量数据极具挑战。现有方法通常依赖有限的指标来识别问题数据，或是仅对一小部分数据进行人工验证，无法全面解决问题。

**Method:** 提出了一种名为VISTA的视觉分析框架，以改善数据质量，进而提高多模态模型的性能。VISTA针对开放词汇图像分割这一复杂领域，集成了多阶段数据验证策略与人类专业知识，使人类能够识别、理解和纠正多模态基础模型生成标签中的隐藏问题。

**Result:** 通过两个基准数据集的详细用例和专家评审，从定量和定性两个角度证明了VISTA的有效性。

**Conclusion:** VISTA通过结合多阶段数据验证策略和人类专业知识，能够有效改善多模态基础模型生成标签的质量，提升模型在开放词汇图像分割等任务中的性能。

**Abstract:** The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA)
have facilitated the auto-labeling of large-scale datasets, enhancing model
performance in challenging downstream tasks such as open-vocabulary object
detection and segmentation. However, the quality of FM-generated labels is less
studied as existing approaches focus more on data quantity over quality. This
is because validating large volumes of data without ground truth presents a
considerable challenge in practice. Existing methods typically rely on limited
metrics to identify problematic data, lacking a comprehensive perspective, or
apply human validation to only a small data fraction, failing to address the
full spectrum of potential issues. To overcome these challenges, we introduce
VISTA, a visual analytics framework that improves data quality to enhance the
performance of multi-modal models. Targeting the complex and demanding domain
of open-vocabulary image segmentation, VISTA integrates multi-phased data
validation strategies with human expertise, enabling humans to identify,
understand, and correct hidden issues within FM-generated labels. Through
detailed use cases on two benchmark datasets and expert reviews, we demonstrate
VISTA's effectiveness from both quantitative and qualitative perspectives.

</details>


### [47] [BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis](https://arxiv.org/abs/2507.09036)
*Florian Kofler,Marcel Rosier,Mehdi Astaraki,Hendrik Möller,Ilhem Isra Mekki,Josef A. Buchner,Anton Schmick,Arianna Pfiffer,Eva Oswald,Lucas Zimmer,Ezequiel de la Rosa,Sarthak Pati,Julian Canisius,Arianna Piffer,Ujjwal Baid,Mahyar Valizadeh,Akis Linardos,Jan C. Peeken,Surprosanna Shit,Felix Steinbauer,Daniel Rueckert,Rolf Heckemann,Spyridon Bakas,Jan Kirschke,Constantin von See,Ivan Ezhov,Marie Piraud,Benedikt Wiestler,Bjoern Menze*

Main category: cs.CV

> BrainLesion Suite 是用于脑部病变图像分析的 Python 工具集，它提供了一个高效的工作流程以及量化分析工具，可适应多种生物医学成像应用。

<details>
  <summary>Details</summary>

**Motivation:** 开发 BrainLesion Suite 的动机是为临床和科学研究提供一个高效、灵活的脑部病变图像分析工具集，最初的目标是针对胶质瘤、转移瘤和多发性硬化症等脑部病变的图像分析，但也可以调整用于其他生物医学图像分析的应用。

**Method:** BrainLesion Suite利用Python编程语言，构建了一个用于脑部病变图像分析的模块化工具集。该工具集遵循Pythonic原则，旨在提供一个减少认知负担、简化复杂工作流创建的开发体验。核心是一个可适应的预处理模块，它执行共配准、大脑 atlas 注册，以及可选的去颅骨和图像脱敏处理，适用于任意多模态输入图像。

**Result:** BrainLesion Suite 使用 BraTS 挑战中的算法来合成缺失的模态图像，填充病变区域，生成特定病理的肿瘤分割。此外，它还提供了量化分割模型性能的工具，例如 panoptica 计算病灶级别的度量。

**Conclusion:** 因此，BrainLesion Suite 不仅提供了一个强大的脑部病变图像分析解决方案，而且通过其模块化设计展示了高度的适应性和扩展性，有能力为其他生物医学图像分析需求提供服务。

**Abstract:** BrainLesion Suite is a versatile toolkit for building modular brain lesion
image analysis pipelines in Python. Following Pythonic principles, BrainLesion
Suite is designed to provide a 'brainless' development experience, minimizing
cognitive effort and streamlining the creation of complex workflows for
clinical and scientific practice. At its core is an adaptable preprocessing
module that performs co-registration, atlas registration, and optional
skull-stripping and defacing on arbitrary multi-modal input images. BrainLesion
Suite leverages algorithms from the BraTS challenge to synthesize missing
modalities, inpaint lesions, and generate pathology-specific tumor
segmentations. BrainLesion Suite also enables quantifying segmentation model
performance, with tools such as panoptica to compute lesion-wise metrics.
Although BrainLesion Suite was originally developed for image analysis
pipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,
it can be adapted for other biomedical image analysis applications. The
individual BrainLesion Suite packages and tutorials are accessible on GitHub.

</details>


### [48] [Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?](https://arxiv.org/abs/2507.09052)
*Fang Chen,Alex Villa,Gongbo Liang,Xiaoyi Lu,Meng Tang*

Main category: cs.CV

> 引入两种对比损失函数来提高类不平衡扩散模型中尾部类别图像的多样性，同时保持头部类别图像的质量和多样性，实验结果在多个数据集上优于标准 DDPM 和其他方法。

<details>
  <summary>Details</summary>

**Motivation:** 解决训练数据在类条件图像合成中存在的长尾分布问题，避免模式崩塌，提高尾部类别图像的多样性。

**Method:** 使用无监督的 InfoNCE 损失和 MSE 损失两种对比损失函数，增加合成图像之间的距离和多样性，特别是对于尾部类别。

**Result:** 对比损失框架易于实现，并在多个数据集上超过了标准 DDPM 和其他方法，包括 CIFAR10/100-LT, PlacesLT, TinyImageNetLT 和 ImageNetLT。

**Conclusion:** 成功地将对比学习应用到类不平衡的扩散模型中，提高了尾部类别图像的多样性，同时保持了头部类别图像的质量和多样性。

**Abstract:** Training data for class-conditional image synthesis often exhibit a
long-tailed distribution with limited images for tail classes. Such an
imbalance causes mode collapse and reduces the diversity of synthesized images
for tail classes. For class-conditional diffusion models trained on imbalanced
data, we aim to improve the diversity of tail class images without compromising
the fidelity and diversity of head class images. We achieve this by introducing
two deceptively simple but highly effective contrastive loss functions.
Firstly, we employ an unsupervised InfoNCE loss utilizing negative samples to
increase the distance/dissimilarity among synthetic images, particularly for
tail classes. To further enhance the diversity of tail classes, our second loss
is an MSE loss that contrasts class-conditional generation with unconditional
generation at large timesteps. This second loss makes the denoising process
insensitive to class conditions for the initial steps, which enriches tail
classes through knowledge sharing from head classes. Conditional-unconditional
alignment has been shown to enhance the performance of long-tailed GAN. We are
the first to adapt such alignment to diffusion models. We successfully
leveraged contrastive learning for class-imbalanced diffusion models. Our
contrastive learning framework is easy to implement and outperforms standard
DDPM and alternative methods for class-imbalanced diffusion models across
various datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and
ImageNetLT.

</details>


### [49] [Infinite Video Understanding](https://arxiv.org/abs/2507.09068)
*Dell Zhang,Xiangyu Chen,Jixiang Luo,Mengxi Jia,Changzhi Sun,Ruilong Ren,Jingren Liu,Hao Sun,Xuelong Li*

Main category: cs.CV

> 本文提出视频理解的一个未来研究方向：无界视频理解。此方向将推动AI研究在多个领域的发展。

<details>
  <summary>Details</summary>

**Motivation:** 由于大型语言模型和多模态扩展在视频理解中的快速进展，但面对时长较长的视频，现有模型在计算和内存方面仍受限。本文旨在探讨无界视频理解作为未来多媒体和AI研究领域的长远目标。

**Method:** 本文并未详细描述具体的方法，而是讨论了目前视频理解领域的挑战，并提出了无界视频理解这一未来研究方向。

**Result:** 本文并未展示具体结果，而是详细分析了当前技术的局限性。

**Conclusion:** 文章呼吁多媒体和AI研究社区将无界视频理解视为研究标杆，推动在流媒体架构、持久记忆机制、层次化自适应表示、以事件为中心的推理等方面的创新。

**Abstract:** The rapid advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have ushered in remarkable progress in video understanding.
However, a fundamental challenge persists: effectively processing and
comprehending video content that extends beyond minutes or hours. While recent
efforts like Video-XL-2 have demonstrated novel architectural solutions for
extreme efficiency, and advancements in positional encoding such as HoPE and
VideoRoPE++ aim to improve spatio-temporal understanding over extensive
contexts, current state-of-the-art models still encounter significant
computational and memory constraints when faced with the sheer volume of visual
tokens from lengthy sequences. Furthermore, maintaining temporal coherence,
tracking complex events, and preserving fine-grained details over extended
periods remain formidable hurdles, despite progress in agentic reasoning
systems like Deep Video Discovery. This position paper posits that a logical,
albeit ambitious, next frontier for multimedia research is Infinite Video
Understanding -- the capability for models to continuously process, understand,
and reason about video data of arbitrary, potentially never-ending duration. We
argue that framing Infinite Video Understanding as a blue-sky research
objective provides a vital north star for the multimedia, and the wider AI,
research communities, driving innovation in areas such as streaming
architectures, persistent memory mechanisms, hierarchical and adaptive
representations, event-centric reasoning, and novel evaluation paradigms.
Drawing inspiration from recent work on long/ultra-long video understanding and
several closely related fields, we outline the core challenges and key research
directions towards achieving this transformative capability.

</details>


### [50] [BlindSight: Harnessing Sparsity for Efficient VLMs](https://arxiv.org/abs/2507.09071)
*Tharun Adithya Srikrishnan,Deval Shah,Steven K. Reinhardt*

Main category: cs.CV

> 研究提出BlindSight，一种优化VLM推理的方法，无需训练，利用输入模板感知注意力稀疏性掩码，减少了计算量，并保持了模型准确度。

<details>
  <summary>Details</summary>

**Motivation:** 大型视觉语言模型（VLM）在处理文本和图像时面临预填充时间长的问题，主要是由于视觉数据的加入导致提示长度增加。提出的方法旨在通过利用注意力计算中的固有稀疏性来缓解这一瓶颈。

**Method:** 通过分析VLM中的注意力模式，发现在大多数层中存在极少的跨图像注意力，仅通过每个图像的注意力汇点。基于此，提出BlindSight：一种无需训练的输入模板感知注意力稀疏性掩码，以优化VLM推理过程。

**Result:** BlindSight在大多数评估的多图像理解基准上实现了平均32%-41%的计算量（FLOPs）减少，与原始模型相比准确度在-2%-+2%之间。

**Conclusion:** BlindSight展示了在不牺牲模型性能的情况下显著减少VLM推理计算量的潜力，为未来的视觉语言模型设计提供了重要的方法和思路。

**Abstract:** Large vision-language models (VLMs) enable the joint processing of text and
images. However, the inclusion of vision data significantly expands the prompt
length. Along with the quadratic complexity of the attention computation, this
results in a longer prefill duration. An approach to mitigate this bottleneck
is to leverage the inherent sparsity in the attention computation. In our
analysis of attention patterns in VLMs, we observe that a substantial portion
of layers exhibit minimal cross-image attention, except through attention-sink
tokens per image. These sparse attention patterns fall into distinct
categories: sink-only, document mask and a hybrid document-sink mask. Based on
this, we propose BlindSight: a training-free approach to optimize VLM inference
using a input template-aware attention sparsity mask. We utilize samples from a
dataset to derive a prompt-agnostic sparsity categorization for every attention
head. We evaluate the proposed technique using VLMs such as Qwen2-VL,
Qwen2.5-VL and Gemma-3. BlindSight results in a 32%-41% reduction in FLOPs on
average with -2%-+2% accuracy compared to the original model in most evaluated
multi-image understanding benchmarks.

</details>


### [51] [From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion](https://arxiv.org/abs/2507.09081)
*Zhenyu Yu,Mohd Yamani Idna Idris,Hua Wang,Pei Wang,Junyi Chen,Kun Wang*

Main category: cs.CV

> 本文回顾了遥感反演技术从物理模型向机器学习再向基础模型的演变，讨论了各方法的优缺点及其面临的挑战，并对未来发展提出了展望。

<details>
  <summary>Details</summary>

**Motivation:** 随着遥感系统和人工智能的发展，传统的基于物理的方法正在被数据驱动和基于基础模型的方法所取代。本文旨在系统回顾这些演变，并重点分析近期基础模型在自监督预训练、多模态整合和跨任务适应方面的进展。

**Method:** 论文系统地回顾了反演技术方法的演变过程，从物理模型（如PROSPECT, SCOPE, DART）到机器学习方法（如深度学习，多模态融合），再到基础模型（如SatMAE, GFM, mmEarth）。

**Result:** 文章比较了各个范式在建模假设、应用场景和限制方面的特点，并且凸显了在物理可解释性、领域泛化、有限监督和不确定性量化方面的持续挑战。

**Conclusion:** 论文展望了下一代遥感反演的基础模型的发展，强调统一建模能力、跨领域泛化和物理可解释性的重要性。

**Abstract:** Quantitative remote sensing inversion aims to estimate continuous surface
variables-such as biomass, vegetation indices, and evapotranspiration-from
satellite observations, supporting applications in ecosystem monitoring, carbon
accounting, and land management. With the evolution of remote sensing systems
and artificial intelligence, traditional physics-based paradigms are giving way
to data-driven and foundation model (FM)-based approaches. This paper
systematically reviews the methodological evolution of inversion techniques,
from physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods
(e.g., deep learning, multimodal fusion), and further to foundation models
(e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application
scenarios, and limitations of each paradigm, with emphasis on recent FM
advances in self-supervised pretraining, multi-modal integration, and
cross-task adaptation. We also highlight persistent challenges in physical
interpretability, domain generalization, limited supervision, and uncertainty
quantification. Finally, we envision the development of next-generation
foundation models for remote sensing inversion, emphasizing unified modeling
capacity, cross-domain generalization, and physical interpretability.

</details>


### [52] [Taming generative video models for zero-shot optical flow extraction](https://arxiv.org/abs/2507.09082)
*Seungwoo Kim,Khai Loong Aw,Klemen Kotar,Cristobal Eyzaguirre,Wanhee Lee,Yunong Liu,Jared Watrous,Stefan Stojanov,Juan Carlos Niebles,Jiajun Wu,Daniel L. K. Yamins*

Main category: cs.CV

> 研究通过冻结自监督未来帧预测视频模型，并提出KL追踪技术，来实现无需微调的零样本光流提取，结果在多个数据集上优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探索现有的大规模预训练模型能否在不进行特定调整的情况下直接用于提取光流。现有提取深度或光照特征的方法需要针对视频生成模型进行微调，但这在光流提取任务中是不可实现的，因为标注数据稀缺，且合成数据集存在从虚拟到现实的差距。

**Method:** 提出的方法基于Local Random Access Sequence (LRAS)架构，通过KL追踪技术，在测试时向第一帧注入局部扰动，然后计算扰动前后的预测分布的Kullback-Leibler散度。这种方法取代了繁琐的微调过程，表现出优越性。

**Result:** 本研究探讨了通过冻结自监督视频模型来提取无监督光流的可能性。该方法受到反事实世界模型（CWM）的启发，主要通过在下一帧预测器中注入少量追踪扰动并追踪其传播，来获得点到点的对应关系。研究发现，成功实现零样本光流提取的模型需要具备三种特性：分布预测未来帧的能力（避免模糊或噪声输出）、独立处理每个时空补丁的因素隐变量、以及能够以任意子集未来像素为条件的随机访问解码。研究提出的KL追踪方法，在现实世界和合成数据集上均取得了良好的效果。研究结果表明，通过反事实提示的可控制视频生成模型，可以作为监督学习或光度损失方法的替代方案，用于高质量光流提取。

**Conclusion:** 研究结论强调了该方法的有效性和相比已有的光流提取方法的优点。在无需针对光流进行特定调整的情况下，本方法在多个数据集上均取得了显著优于现有模型的表现。

**Abstract:** Extracting optical flow from videos remains a core computer vision problem.
Motivated by the success of large general-purpose models, we ask whether frozen
self-supervised video models trained only for future frame prediction can be
prompted, without fine-tuning, to output flow. Prior work reading out depth or
illumination from video generators required fine-tuning, which is impractical
for flow where labels are scarce and synthetic datasets suffer from a
sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm,
which can obtain point-wise correspondences by injecting a small tracer
perturbation into a next-frame predictor and tracking its propagation, we
extend this idea to generative video models. We explore several popular
architectures and find that successful zero-shot flow extraction in this manner
is aided by three model properties: (1) distributional prediction of future
frames (avoiding blurry or noisy outputs); (2) factorized latents that treat
each spatio-temporal patch independently; and (3) random-access decoding that
can condition on any subset of future pixels. These properties are uniquely
present in the recent Local Random Access Sequence (LRAS) architecture.
Building on LRAS, we propose KL-tracing: a novel test-time procedure that
injects a localized perturbation into the first frame, rolls out the model one
step, and computes the Kullback-Leibler divergence between perturbed and
unperturbed predictive distributions. Without any flow-specific fine-tuning,
our method outperforms state-of-the-art models on real-world TAP-Vid DAVIS
dataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid
Kubric (4.7% relative improvement). Our results indicate that counterfactual
prompting of controllable generative video models is a scalable and effective
alternative to supervised or photometric-loss approaches for high-quality flow.

</details>


### [53] [MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks](https://arxiv.org/abs/2507.09092)
*Ram S Iyer,Narayan S Iyer,Rugmini Ammal P*

Main category: cs.CV

> 本文提出了一种基于互信息和激活映射的新后验视觉解释方法MI CAM，它能可视化说明卷积神经网络的推理过程，并且在多个标准上超越了现有的方法。

<details>
  <summary>Details</summary>

**Motivation:** 随着机器视觉在医疗保健和自动化工厂等日常必需品中的应用，人们对卷积神经网络的内部机制以及网络提供特定推理的原因产生了兴趣。

**Method:** MI CAM基于激活映射提出了一种新的后验视觉解释方法。与之前的基于类别激活映射的方法不同，MI CAM通过特征图与输入图像的互信息进行加权，最终结果由权重和激活图的线性组合生成。此外，它还能通过反事实分析来验证其因果解释。

**Result:** 所提出的方法在视觉性能和模型推理过程的无偏解释方面展示了表现。该方法在定性和定量测量上与所有最先进的方法相当，尤其在某些方面超过了它们。

**Conclusion:** MI CAM方法提供了一种新的视角来解释卷积神经网络的推理过程，并在定性和定量评估标准上表现出优异性能。

**Abstract:** With the intervention of machine vision in our crucial day to day necessities
including healthcare and automated power plants, attention has been drawn to
the internal mechanisms of convolutional neural networks, and the reason why
the network provides specific inferences. This paper proposes a novel post-hoc
visual explanation method called MI CAM based on activation mapping. Differing
from previous class activation mapping based approaches, MI CAM produces
saliency visualizations by weighing each feature map through its mutual
information with the input image and the final result is generated by a linear
combination of weights and activation maps. It also adheres to producing causal
interpretations as validated with the help of counterfactual analysis. We aim
to exhibit the visual performance and unbiased justifications for the model
inferencing procedure achieved by MI CAM. Our approach works at par with all
state-of-the-art methods but particularly outperforms some in terms of
qualitative and quantitative measures. The implementation of proposed method
can be found on https://anonymous.4open.science/r/MI-CAM-4D27

</details>


### [54] [RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze](https://arxiv.org/abs/2507.09097)
*Yunsoo Kim,Jinge Wu,Honghan Wu*

Main category: cs.CV

> 该研究提出了RadEyeVideo方法，将放射科医生的眼动数据作为视频序列整合，用以指导视觉-语言模型（LVLMs）进行胸腔X光（CXR）分析和疾病诊断，提升了任务表现。

<details>
  <summary>Details</summary>

**Motivation:** 现有的研究在胸部X光片分析中使用视觉-语言模型时，仅通过热图或文本提示结合放射科医生的眼动数据，忽略了眼动的顺序信息，而这些顺序信息也能提供重要见解。因此，本研究旨在通过将眼动数据以视频序列形式加入，提升模型在生成报告和疾病诊断的具体任务表现。

**Method:** 研究使用将放射科医生的眼动数据作为视频序列的方法，称为RadEyeVideo，并在三个通用LVLMs上进行胸腔X光报告生成和疾病诊断任务的评估。

**Result:** 在使用这种新的方法后，模型在报告生成任务上的表现提高了最多24.6%，在两个任务上平均提升了15.2%。该方法甚至使通用领域的LVLM模型的性能超过了专用于医学领域的模型。

**Conclusion:** 该工作展示了当领域专家的特定知识（在这里是眼动信息）有效整合到视觉-语言模型中时，可以显著提升模型在临床任务中的能力。RadEyeVideo这一方法是对一种可扩展的、以人为中心的医疗影像分析应用视觉语言模型方法的推进。

**Abstract:** Large Vision-Language Models (LVLMs) have demonstrated promising performance
in chest X-ray (CXR) analysis. To enhance human-computer interaction, several
studies have incorporated radiologists' eye gaze, typically through heatmaps or
textual prompts. However, these methods often overlook the sequential order of
eye movements, which could provide valuable insights by highlighting both the
areas of interest and the order in which they are examined. In this work, we
propose a novel approach called RadEyeVideo that integrates radiologists'
eye-fixation data as a video sequence, capturing both the temporal and spatial
dynamics of their gaze. We evaluate this method in CXR report generation and
disease diagnosis using three general-domain, open-source LVLMs with video
input capabilities. When prompted with eye-gaze videos, model performance
improves by up to 24.6% in the report generation task and on average 15.2% for
both tasks using scaled evaluation metrics. Notably, RadEyeVideo enhanced an
open-domain LVLM model, LLaVA-OneVision, to surpass task-specific medical LVLMs
such as MAIRA-2 and CheXagent, trained on large Chest X-ray data. This work
highlights that domain expert's knowledge (eye-gaze information in this case),
when effectively integrated with LVLMs, can significantly enhance
general-domain models' capabilities in clinical tasks. RadEyeVideo is a step
toward a scalable human-centered approach of utilizing LVLMs in medical image
analytics.

</details>


### [55] [Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning](https://arxiv.org/abs/2507.09102)
*Yiyang Chen,Shanshan Zhao,Lunhao Duan,Changxing Ding,Dacheng Tao*

Main category: cs.CV

> 本文提出PointSD框架，旨在利用经过大规模数据集训练的Stable Diffusion模型来改善3D点云的自监督学习效果。

<details>
  <summary>Details</summary>

**Motivation:** 假设强大的文本到图像扩散模型，特别是经过大规模数据集训练的Stable Diffusion (SD)模型，能够帮助克服3D扩散模型在学习点云表示上的局限性。

**Method:** 通过将SD模型的文本编码器替换为3D编码器，训练了一个由点云引导渲染的噪声图像去噪的点云到图像扩散模型。接着，使用无噪声图像作为输入，点云作为条件，提取SD特征，并通过使3D骨架的特征与这些SD特征对齐来进行直接语义学习。

**Result:** 在下游点云任务和消融研究中的全面实验表明，SD模型能够增强点云的无监督学习。

**Conclusion:** PointSD框架通过使用SD模型来改进3D自监督学习，并证明了该模型的有效性。

**Abstract:** Diffusion-based models, widely used in text-to-image generation, have proven
effective in 2D representation learning. Recently, this framework has been
extended to 3D self-supervised learning by constructing a conditional point
generator for enhancing 3D representations. However, its performance remains
constrained by the 3D diffusion model, which is trained on the available 3D
datasets with limited size. We hypothesize that the robust capabilities of
text-to-image diffusion models, particularly Stable Diffusion (SD), which is
trained on large-scale datasets, can help overcome these limitations. To
investigate this hypothesis, we propose PointSD, a framework that leverages the
SD model for 3D self-supervised learning. By replacing the SD model's text
encoder with a 3D encoder, we train a point-to-image diffusion model that
allows point clouds to guide the denoising of rendered noisy images. With the
trained point-to-image diffusion model, we use noise-free images as the input
and point clouds as the condition to extract SD features. Next, we train a 3D
backbone by aligning its features with these SD features, thereby facilitating
direct semantic learning. Comprehensive experiments on downstream point cloud
tasks and ablation studies demonstrate that the SD model can enhance point
cloud self-supervised learning. Code is publicly available at
https://github.com/wdttt/PointSD.

</details>


### [56] [Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production](https://arxiv.org/abs/2507.09105)
*Maoxiao Ye,Xinfeng Ye,Mano Manoharan*

Main category: cs.CV

> We introduce a hybrid autoregressive and diffusion model approach for SLP, which includes modules for multi-scale pose representation and confidence-aware causal attention to enhance the accuracy and real-time performance of the model.

<details>
  <summary>Details</summary>

**Motivation:** The motivation comes from the need to improve Sign Language Production models which face issues such as error accumulation during inference and the inability to provide real-time output.

**Method:** Our method involves a hybrid approach combining autoregressive and diffusion models for Sign Language Production (SLP). It includes a Multi-Scale Pose Representation module for capturing fine-grained body movements, as well as a Confidence-Aware Causal Attention mechanism to improve accuracy.

**Result:** Experiments on the PHOENIX14T and How2Sign datasets show that our method performs well in terms of generation quality and real-time streaming efficiency.

**Conclusion:** We conclude that the hybrid framework is effective in addressing the limitations of previous models by integrating the temporal modeling ability of autoregressive models with the refinement capabilities of diffusion models.

**Abstract:** Earlier Sign Language Production (SLP) models typically relied on
autoregressive methods that generate output tokens one by one, which inherently
provide temporal alignment. Although techniques like Teacher Forcing can
prevent model collapse during training, they still cannot solve the problem of
error accumulation during inference, since ground truth is unavailable at that
stage. In contrast, more recent approaches based on diffusion models leverage
step-by-step denoising to enable high-quality generation. However, the
iterative nature of these models and the requirement to denoise entire
sequences limit their applicability in real-time tasks like SLP. To address it,
we apply a hybrid approach combining autoregressive and diffusion models to SLP
for the first time, leveraging the strengths of both models in sequential
dependency modeling and output refinement. To capture fine-grained body
movements, we design a Multi-Scale Pose Representation module that separately
extracts detailed features from distinct articulators and integrates them via a
Multi-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal
Attention mechanism that utilizes joint-level confidence scores to dynamically
guide the pose generation process, improving accuracy and robustness. Extensive
experiments on the PHOENIX14T and How2Sign datasets demonstrate the
effectiveness of our method in both generation quality and real-time streaming
efficiency.

</details>


### [57] [RoHOI: Robustness Benchmark for Human-Object Interaction Detection](https://arxiv.org/abs/2507.09111)
*Di Wen,Kunyu Peng,Kailun Yang,Yufan Chen,Ruiping Liu,Junwei Zheng,Alina Roitberg,Rainer Stiefelhagen*

Main category: cs.CV

> Introduces RoHOI, a robustness benchmark for HOI detection, and SAMPL, a strategy that enhances the robustness of models to handle diverse corruptions effectively.

<details>
  <summary>Details</summary>

**Motivation:** To create a robust benchmark for Human-Object Interaction (HOI) detection under various corruptive conditions, as existing models perform poorly in real-world scenarios.

**Method:** Semantic-Aware Masking-based Progressive Learning (SAMPL) strategy to optimize models based on holistic and partial cues.

**Result:** The proposed SAMPL strategy significantly outperforms state-of-the-art methods in HOI detection robustness.

**Conclusion:** The new robustness-focused metric in the RoHOI benchmark highlights performance drops under corruptions, demonstrating the need for more robust strategies like SAMPL in HOI detection models.

**Abstract:** Human-Object Interaction (HOI) detection is crucial for robot-human
assistance, enabling context-aware support. However, models trained on clean
datasets degrade in real-world conditions due to unforeseen corruptions,
leading to inaccurate prediction. To address this, we introduce the first
robustness benchmark for HOI detection, evaluating model resilience under
diverse challenges. Despite advances, current models struggle with
environmental variability, occlusion, and noise. Our benchmark, RoHOI, includes
20 corruption types based on HICO-DET and V-COCO datasets and a new
robustness-focused metric. We systematically analyze existing models in the
related field, revealing significant performance drops under corruptions. To
improve robustness, we propose a Semantic-Aware Masking-based Progressive
Learning (SAMPL) strategy to guide the model to be optimized based on holistic
and partial cues, dynamically adjusting the model's optimization to enhance
robust feature learning. Extensive experiments show our approach outperforms
state-of-the-art methods, setting a new standard for robust HOI detection.
Benchmarks, datasets, and code will be made publicly available at
https://github.com/Kratos-Wen/RoHOI.

</details>


### [58] [Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning](https://arxiv.org/abs/2507.09118)
*Linlan Huang,Xusheng Cao,Haori Lu,Yifan Meng,Fei Yang,Xialei Liu*

Main category: cs.CV

> 该论文提出了一种名为MG-CLIP的新方法，旨在解决连续学习中视觉-语言模型的模态差距问题。通过维护模态差距和补偿新的数据，该方法能够有效减少遗忘并提高连续学习的表现。该方法在多个基准测试中表现出色，且不需要额外的回放数据。

<details>
  <summary>Details</summary>

**Motivation:** 论文的动力源于利用CLIP进行连续学习时，忽视了模态差距这一关键因素，该因素影响着模型的泛化和适应能力。

**Method:** 论文中的方法是分析视觉-语言预训练模型在微调过程中的模态差距变化，并提出MG-CLIP，它通过保持模态差距和补偿新的数据来减少遗忘并增强新的数据容量。

**Result:** 实验结果表明，MG-CLIP在多个基准测试中的表现优于现有方法，且无需额外的回放数据。

**Conclusion:** 该结论表明，作者提出的MG-CLIP通过维持模态差距来减少遗忘，并通过补偿新的数据来增强连续学习的性能，为连续学习提供了一种新颖的模态差距视角。

**Abstract:** Continual learning aims to enable models to learn sequentially from
continuously incoming data while retaining performance on previously learned
tasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting
strong capabilities across various downstream tasks, there has been growing
interest in leveraging CLIP for continual learning in such scenarios. Most
existing works overlook the inherent modality gap in CLIP, a key factor in its
generalization and adaptability. In this paper, we analyze the variations in
the modality gap during the fine-tuning of vision-language pre-trained models.
Our observations reveal that the modality gap effectively reflects the extent
to which pre-trained knowledge is preserved. Based on these insights, we
propose a simple yet effective method, MG-CLIP, that improves CLIP's
performance in class-incremental learning. Our approach leverages modality gap
preservation to mitigate forgetting and modality gap compensation to enhance
the capacity for new data, introducing a novel modality-gap-based perspective
for continual learning. Extensive experiments on multiple benchmarks
demonstrate that our method outperforms existing approaches without requiring
additional replay data. Our code is available at
https://github.com/linlany/MindtheGap.

</details>


### [59] [SnapMoGen: Human Motion Generation from Expressive Texts](https://arxiv.org/abs/2507.09122)
*Chuan Guo,Inwoo Hwang,Jian Wang,Bing Zhou*

Main category: cs.CV

> Introduce SnapMoGen, a new dataset for text-to-motion generation, and MoMask++, a generative transformer model, which together improve the quality and controllability of synthesized motion.

<details>
  <summary>Details</summary>

**Motivation:** The primary motivation is to address the limitations of current text-to-motion models and to enable more detailed control and better generalization through a new, richly annotated dataset and improved model architecture.

**Method:** SnapMoGen, a dataset of text-motion featuring 20K motion clips and 122K detailed textual descriptions, is introduced. MoMask++, a generative masked transformer model, converts motion into multi-scale token sequences for enhanced performance.

**Result:** MoMask++ achieves state-of-the-art performance on both the HumanML3D and SnapMoGen benchmarks, demonstrating improved fine-grained controllability and long-term motion synthesis capabilities.

**Conclusion:** The introduction of SnapMoGen and the MoMask++ model represents a significant step towards more sophisticated and controllable text-to-motion synthesis, facilitating research into detailed motion and long-term sequence generation.

**Abstract:** Text-to-motion generation has experienced remarkable progress in recent
years. However, current approaches remain limited to synthesizing motion from
short or general text prompts, primarily due to dataset constraints. This
limitation undermines fine-grained controllability and generalization to unseen
prompts. In this paper, we introduce SnapMoGen, a new text-motion dataset
featuring high-quality motion capture data paired with accurate, expressive
textual annotations. The dataset comprises 20K motion clips totaling 44 hours,
accompanied by 122K detailed textual descriptions averaging 48 words per
description (vs. 12 words of HumanML3D). Importantly, these motion clips
preserve original temporal continuity as they were in long sequences,
facilitating research in long-term motion generation and blending. We also
improve upon previous generative masked modeling approaches. Our model,
MoMask++, transforms motion into multi-scale token sequences that better
exploit the token capacity, and learns to generate all tokens using a single
generative masked transformer. MoMask++ achieves state-of-the-art performance
on both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the
ability to process casual user prompts by employing an LLM to reformat inputs
to align with the expressivity and narration style of SnapMoGen. Project
webpage: https://snap-research.github.io/SnapMoGen/

</details>


### [60] [PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP Alignment](https://arxiv.org/abs/2507.09139)
*Dewen Zhang,Tahir Hussain,Wangpeng An,Hayaru Shouno*

Main category: cs.CV

> PoseLLM, 一种大语言模型驱动的全新姿势估计方法，采用非线性MLP连接视觉和语言特征，提升了定位精度，并保持了出色的零样本泛化能力，超越了现有的LocLLM方法。

<details>
  <summary>Details</summary>

**Motivation:** 改进传统的依赖编码关键点先验知识的姿势估计架构，而这些架构限制了对新颖姿势或未见过关键点的泛化能力。为此引入了语言指导的方法，并通过改进的视觉-语言转换进一步提高了精度。

**Method:** PoseLLM, 一种基于大语言模型的姿势估计框架，通过用非线性MLP连接视觉和语言特征来取代线性投影器，改进了复杂的空间-文本交互问题，提升定位精度。

**Result:** 在COCO验证集上实现了77.8 AP，比LocLLM高出+0.4 AP，并在Human-Art和MPII上保持了强大的零样本泛化能力。

**Conclusion:** 简单而强大的非线性连接显著提升了定位精度，同时没有牺牲其泛化能力，推进了语言引导姿势估计的最新技术。

**Abstract:** Human pose estimation traditionally relies on architectures that encode
keypoint priors, limiting their generalization to novel poses or unseen
keypoints. Recent language-guided approaches like LocLLM reformulate keypoint
localization as a vision-language task, enabling zero-shot generalization
through textual descriptions. However, LocLLM's linear projector fails to
capture complex spatial-textual interactions critical for high-precision
localization. To address this, we propose PoseLLM, the first Large Language
Model (LLM)-based pose estimation framework that replaces the linear projector
with a nonlinear MLP vision-language connector. This lightweight two-layer MLP
with GELU activation enables hierarchical cross-modal feature transformation,
enhancing the fusion of visual patches and textual keypoint descriptions.
Trained exclusively on COCO data, PoseLLM achieves 77.8 AP on the COCO
validation set, outperforming LocLLM by +0.4 AP, while maintaining strong
zero-shot generalization on Human-Art and MPII. Our work demonstrates that a
simple yet powerful nonlinear connector significantly boosts localization
accuracy without sacrificing generalization, advancing the state-of-the-art in
language-guided pose estimation. Code is available at
https://github.com/Ody-trek/PoseLLM.

</details>


### [61] [$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting](https://arxiv.org/abs/2507.09144)
*Zhimin Liao,Ping Wei,Ruijie Zhang,Shuaijia Chen,Haoxuan Wang,Ziyang Ren*

Main category: cs.CV

> 提出了$I^{2}$-World框架，通过将场景分隔符分为场景内和场景间分隔符来优化4D占用预测，从而有效处理自动驾驶中超复杂3D场景的问题，并在性能和效率上取得显著成就。

<details>
  <summary>Details</summary>

**Motivation:** 当前高效地对复杂3D场景进行分词化仍是一个挑战，该问题阻碍了3D世界模型的发展，特别是对于自动驾驶中罕见情况的处理。

**Method:** 利用多尺度残差量化策略进行3D场景的层次化压缩，同时保留空间细节。同时通过残差时间聚合跨时间步长的依赖关系。此外，采用了编码器-解码器架构，以确保生成时的时空一致性。

**Result:** 在4D占用预测中，与其他方法相比，$I^{2}$-World在mIoU和IoU上分别提高了25.1%和36.9%。实验还表明，该框架在计算效率方面表现出色，只需2.9GB的训练内存和37.0 FPS的实时推理。

**Conclusion:** 研究证实了$I^{2}$-World在自动驾驶场景中解决复杂3D场景预测问题上的出色性能与效率。

**Abstract:** Forecasting the evolution of 3D scenes and generating unseen scenarios via
occupancy-based world models offers substantial potential for addressing corner
cases in autonomous driving systems. While tokenization has revolutionized
image and video generation, efficiently tokenizing complex 3D scenes remains a
critical challenge for 3D world models. To address this, we propose
$I^{2}$-World, an efficient framework for 4D occupancy forecasting. Our method
decouples scene tokenization into intra-scene and inter-scene tokenizers. The
intra-scene tokenizer employs a multi-scale residual quantization strategy to
hierarchically compress 3D scenes while preserving spatial details. The
inter-scene tokenizer residually aggregates temporal dependencies across
timesteps. This dual design preserves the compactness of 3D tokenizers while
retaining the dynamic expressiveness of 4D tokenizers. Unlike decoder-only
GPT-style autoregressive models, $I^{2}$-World adopts an encoder-decoder
architecture. The encoder aggregates spatial context from the current scene and
predicts a transformation matrix to enable high-level control over scene
generation. The decoder, conditioned on this matrix and historical tokens,
ensures temporal consistency during generation. Experiments demonstrate that
$I^{2}$-World achieves state-of-the-art performance, outperforming existing
methods by 25.1\% in mIoU and 36.9\% in IoU for 4D occupancy forecasting while
exhibiting exceptional computational efficiency: it requires merely 2.9 GB of
training memory and achieves real-time inference at 37.0 FPS. Our code is
available on https://github.com/lzzzzzm/II-World.

</details>


### [62] [Stable Score Distillation](https://arxiv.org/abs/2507.09168)
*Haiming Zhu,Yangyang Xu,Chenshu Xu,Tingrui Shen,Wenxi Liu,Yong Du,Jun Yu,Shengfeng He*

Main category: cs.CV

> SSD 是一种用于文本引导图像和3D编辑的高效框架，通过简化结构和使用无分类器指引（CFG）来解决稳定性、空间控制和编辑强度的问题，同时保持编辑与原始内容的一致性。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有方法如Delta Denoising Score在图像和3D编辑中稳定性、空间控制和编辑强度方面的问题。

**Method:** SSD利用无分类器指引（CFG）方程实现跨提示的对齐，并引入常量无文本分支以稳定优化过程，同时有一个增强编辑分支来提高编辑强度。

**Result:** SSD在2D和3D编辑任务，如NeRF和文本驱动风格编辑中达到了最先进的结果，具有更快的收敛性和较低的复杂度。

**Conclusion:** SSD提供了一种用于文本引导编辑的稳健和高效解决方案，通过简化结构和提高一致性，解决了现有编辑方法的局限性。

**Abstract:** Text-guided image and 3D editing have advanced with diffusion-based models,
yet methods like Delta Denoising Score often struggle with stability, spatial
control, and editing strength. These limitations stem from reliance on complex
auxiliary structures, which introduce conflicting optimization signals and
restrict precise, localized edits. We introduce Stable Score Distillation
(SSD), a streamlined framework that enhances stability and alignment in the
editing process by anchoring a single classifier to the source prompt.
Specifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves
cross-prompt alignment, and introduces a constant term null-text branch to
stabilize the optimization process. This approach preserves the original
content's structure and ensures that editing trajectories are closely aligned
with the source prompt, enabling smooth, prompt-specific modifications while
maintaining coherence in surrounding regions. Additionally, SSD incorporates a
prompt enhancement branch to boost editing strength, particularly for style
transformations. Our method achieves state-of-the-art results in 2D and 3D
editing tasks, including NeRF and text-driven style edits, with faster
convergence and reduced complexity, providing a robust and efficient solution
for text-guided editing.

</details>


### [63] [Learning and Transferring Better with Depth Information in Visual Reinforcement Learning](https://arxiv.org/abs/2507.09180)
*Zichun Xu,Yuntao Li,Zhaomin Wang,Lei Zhuang,Guocai Yang,Jingdong Zhao*

Main category: cs.CV

> 本文提出了一种融合RGB和深度信息的视觉模型，用于增强模型泛化能力，并提出了加速学习过程的方法。

<details>
  <summary>Details</summary>

**Motivation:** 深度信息对于场景外观变化具有鲁棒性，并且本质上携带了3D空间细节。这种方法试图通过融合RGB和深度模态来提高模型的泛化能力。

**Method:** 本文提出了一种基于视觉转换器的视觉骨干网络，用于融合RGB和深度模态以增强泛化能力。不同的模态首先通过单独的CNN主干进行处理，然后将结合的卷积特征传递给可扩展的视觉转换器以获得视觉表示。此外，设计了一种对比无监督学习方案，使用掩码和未掩码标记来加速强化学习过程中的样本效率。对于sim2real迁移，开发了一种灵活的课程学习计划，在训练过程中部署领域随机化。

**Result:** 虽然具体的实验结果未在摘要中详细说明，但可以推测，这种融合方法在某些3D视觉任务的泛化能力上表现更好，特别是在从仿真到现实世界的迁移学习中。

**Conclusion:** 通过融合RGB和深度信息以及课程学习和领域随机化的应用，该方法在增强模型泛化能力和加速学习效率上显示出潜力。

**Abstract:** Depth information is robust to scene appearance variations and inherently
carries 3D spatial details. In this paper, a visual backbone based on the
vision transformer is proposed to fuse RGB and depth modalities for enhancing
generalization. Different modalities are first processed by separate CNN stems,
and the combined convolutional features are delivered to the scalable vision
transformer to obtain visual representations. Moreover, a contrastive
unsupervised learning scheme is designed with masked and unmasked tokens to
accelerate the sample efficiency during the reinforcement learning progress.
For sim2real transfer, a flexible curriculum learning schedule is developed to
deploy domain randomization over training processes.

</details>


### [64] [Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning](https://arxiv.org/abs/2507.09183)
*Yongwei Jiang,Yixiong Zou,Yuhua Li,Ruixuan Li*

Main category: cs.CV

> 本文提出了针对FSCIL问题的解决方案LGSP-Prompt。该方法克服了传统基于池的提示方法在FSCIL中遇到的性能下降问题，通过空间维度生成和选择提示，增强了模型的增量学习性能。

<details>
  <summary>Details</summary>

**Motivation:** 虽然基于池的提示方法在传统增量学习中已经取得了成功，但它们在FSCIL(少量样本增量学习)设置中的有效性仍有待探索。本文首次研究了当前提示池方法在FSCIL任务中的应用，发现了在增量会话中意外的性能下降。通过对这种现象的详细分析，研究人员发现，由于数据量有限，过多的提示会争夺任务相关信息，导致模型过拟合。

**Method:** 本文提出了一种名为LGSP-Prompt（局部-全局空间提示）的方法，该方法创造性地将基于池的提示学习从标记维度转移到空间维度。LGSP-Prompt通过协同结合局部空间特征和全局频域表示来生成空间提示，以突出输入图像中的关键模式。研究构建了两个空间提示池，以实现动态提示选择，既保持了已获得的知识，又有效地学习了新的会话。

**Result:** 广泛实验表明，该方法在多个FSCIL基准测试中达到了最先进的性能水平，在保持基础知识和增量学习上均展示出显著的优势。

**Conclusion:** 实验结果表明，LGSP-Prompt在多个FSCIL基准上的表现达到了最先进的水平，不仅在保持基础知识方面，而且在增量学习方面都显示出了显著的优势。

**Abstract:** Few-Shot Class-Incremental Learning (FSCIL) faces dual challenges of data
scarcity and incremental learning in real-world scenarios. While pool-based
prompting methods have demonstrated success in traditional incremental
learning, their effectiveness in FSCIL settings remains unexplored. This paper
presents the first study of current prompt pool methods in FSCIL tasks,
revealing an unanticipated performance degradation in incremental sessions.
Through comprehensive analysis, we identify that this phenomenon stems from
token-dimension saturation: with limited data, excessive prompts compete for
task-relevant information, leading to model overfitting. Based on this finding,
we propose LGSP-Prompt (Local-Global Spatial Prompting), which innovatively
shifts pool-based prompt learning from the token dimension to the spatial
dimension. LGSP-Prompt generates spatial prompts by synergistically combining
local spatial features and global frequency-domain representations to highlight
key patterns in input images. We construct two spatial prompt pools enabling
dynamic prompt selection to maintain acquired knowledge while effectively
learning novel sessions. Extensive experiments demonstrate that our approach
achieves state-of-the-art performance across multiple FSCIL benchmarks, showing
significant advantages in both base knowledge preservation and incremental
learning. Our implementation is available at
https://github.com/Jywsuperman/LGSP.

</details>


### [65] [MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2507.09184)
*Qiyan Zhao,Xiaofeng Zhang,Yiheng Li,Yun Xing,Xiaosong Yuan,Feilong Tang,Sinan Fan,Xuhang Chen,Xuyao Zhang,Dahan Wang*

Main category: cs.CV

> 本文提出MCA-LLaVA方法来缓解大型视觉语言模型中的幻觉问题，通过将长距离衰减扩展到二维多方向衰减，并整合图像标记的一维序列顺序和二维空间位置。

<details>
  <summary>Details</summary>

**Motivation:** 解决大型视觉语言模型（LVLMs）中存在的幻觉问题，特别是由于Rotary Position Encoding（RoPE）长期衰减导致的模态对齐负面影响。

**Method:** 提出MCA-LLaVA方法，该方法基于曼哈顿距离，将长期衰减扩展到二维多方向空间衰减，整合了一维序列顺序和二维空间位置的图像标记，以改善指令对不同空间位置的图像标记的感知，减轻图像对齐偏差，从而缓解幻觉问题。

**Result:** 实验结果显示，MCA-LLaVA在不同的幻觉和通用基准测试中表现出色，展示了其有效性和通用性。

**Conclusion:** MCA-LLaVA通过缓解图像对齐偏差显著提高了模型性能，减少了幻觉现象，其代码可在指定GitHub仓库获取。

**Abstract:** Hallucinations pose a significant challenge in Large Vision Language Models
(LVLMs), with misalignment between multimodal features identified as a key
contributing factor. This paper reveals the negative impact of the long-term
decay in Rotary Position Encoding (RoPE), used for positional modeling in
LVLMs, on multimodal alignment. Concretely, under long-term decay, instruction
tokens exhibit uneven perception of image tokens located at different positions
within the two-dimensional space: prioritizing image tokens from the
bottom-right region since in the one-dimensional sequence, these tokens are
positionally closer to the instruction tokens. This biased perception leads to
insufficient image-instruction interaction and suboptimal multimodal alignment.
We refer to this phenomenon as image alignment bias. To enhance instruction's
perception of image tokens at different spatial locations, we propose
MCA-LLaVA, based on Manhattan distance, which extends the long-term decay to a
two-dimensional, multi-directional spatial decay. MCA-LLaVA integrates the
one-dimensional sequence order and two-dimensional spatial position of image
tokens for positional modeling, mitigating hallucinations by alleviating image
alignment bias. Experimental results of MCA-LLaVA across various hallucination
and general benchmarks demonstrate its effectiveness and generality. The code
can be accessed in https://github.com/ErikZ719/MCA-LLaVA.

</details>


### [66] [THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage](https://arxiv.org/abs/2507.09200)
*Trong-Thuan Nguyen,Pha Nguyen,Jackson Cothren,Alper Yilmaz,Minh-Triet Tran,Khoa Luu*

Main category: cs.CV

> THYME方法结合分层特征聚合和循环时间细化，解决了多尺度空间上下文建模及帧间时间一致性问题，大幅提升了场景理解的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的场景图生成方法通常无法同时捕获精细的空间细节和长期的时间依赖性。THYME旨在解决这些问题，在多种场景中提升动态场景理解。

**Method:** 我们提出了Temporal Hierarchical Cyclic Scene Graph (THYME) 方法，该方法通过分层特征聚合和循环时间细化相结合的方式，来解决多尺度空间上下文建模及帧间时间一致性问题。

**Result:** 在ASPIRe和AeroEye-v1.0数据集上的广泛实验表明，提出的THYME方法大幅超越现有方法，提供更准确和连贯的场景图，并改进了地平视角和航拍场景的场景理解能力。

**Conclusion:** THYME为动态场景理解引入了一种有效的方法，可以在多个尺度上建模空间上下文，并在帧间维护时间一致性，提高了场景理解的准确性。

**Abstract:** The rapid proliferation of video in applications such as autonomous driving,
surveillance, and sports analytics necessitates robust methods for dynamic
scene understanding. Despite advances in static scene graph generation and
early attempts at video scene graph generation, previous methods often suffer
from fragmented representations, failing to capture fine-grained spatial
details and long-range temporal dependencies simultaneously. To address these
limitations, we introduce the Temporal Hierarchical Cyclic Scene Graph (THYME)
approach, which synergistically integrates hierarchical feature aggregation
with cyclic temporal refinement to address these limitations. In particular,
THYME effectively models multi-scale spatial context and enforces temporal
consistency across frames, yielding more accurate and coherent scene graphs. In
addition, we present AeroEye-v1.0, a novel aerial video dataset enriched with
five types of interactivity that overcome the constraints of existing datasets
and provide a comprehensive benchmark for dynamic scene graph generation.
Empirically, extensive experiments on ASPIRe and AeroEye-v1.0 demonstrate that
the proposed THYME approach outperforms state-of-the-art methods, offering
improved scene understanding in ground-view and aerial scenarios.

</details>


### [67] [Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves](https://arxiv.org/abs/2507.09207)
*Alexander C. Ogren,Berthy T. Feng,Jihoon Ahn,Katherine L. Bouman,Chiara Daraio*

Main category: cs.CV

> 通过视频分析表面波动，本研究提出了一种非侵入式测量结构厚度和刚度的方法，验证结果与真实值匹配良好，展示了潜在的健康监测应用价值。

<details>
  <summary>Details</summary>

**Motivation:** 此研究的动机在于通过分析材料表面波动来获取其下方物理特性信息，进而实现无需侵入即可测量如组织属性等信息。

**Method:** 我们提出了一种方法，该方法通过从视频中提取色散关系，然后解决一个基于物理的优化问题来推断结构的厚度和刚度。

**Result:** 该方法在模拟和实际数据上都得到了验证，显示出与真实测量值有很强的一致性。

**Conclusion:** 这项技术为家庭健康监测提供了概念验证，并且可以应用于诸如人机交互等领域。

**Abstract:** Wave propagation on the surface of a material contains information about
physical properties beneath its surface. We propose a method for inferring the
thickness and stiffness of a structure from just a video of waves on its
surface. Our method works by extracting a dispersion relation from the video
and then solving a physics-based optimization problem to find the best-fitting
thickness and stiffness parameters. We validate our method on both simulated
and real data, in both cases showing strong agreement with ground-truth
measurements. Our technique provides a proof-of-concept for at-home health
monitoring of medically-informative tissue properties, and it is further
applicable to fields such as human-computer interaction.

</details>


### [68] [Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models](https://arxiv.org/abs/2507.09209)
*Xiao Liang,Di Wang,Zhicheng Jiao,Ronghan Li,Pengfei Yang,Quan Wang,Tat-Seng Chua*

Main category: cs.CV

> 提出了一种无需额外训练的专家参与框架来提升医学视觉语言模型的性能和可靠性，并证明该方法在资源受限的临床环境中具有可行性。

<details>
  <summary>Details</summary>

**Motivation:** 医学视觉语言模型在医学应用中有产生错误或未经验证响应的问题，而现有的通过微调或模型结构调整来提升性能的方法成本高昂且缺乏与临床专业知识的充分对齐。

**Method:** 通过提出名为专家控制的无分类器自由引导（Expert-CFG）的专家参与框架，解决现有的医学视觉语言模型（MedVLM）无法充分与临床专业知识对齐的问题。该框架包括不确定性估计策略，用于识别不可靠的输出，并检索相关信息，辅助专家突出关键术语，应用无分类器自由引导方法来优化MedVLM的词块编码，确保调整后的输出与专家的突出内容一致。

**Result:** 在三个医疗视觉问答基准上的评估显示，与拥有13B参数的最先进的模型相比，拥有4.2B参数且仅使用有限的专家标注数据的Expert-CFG表现更优。

**Conclusion:** 评估结果表明了在资源受限的临床环境中部署该系统的可行性，同时也证明了专家参与方法在提升模型输出准确性方面的潜力。

**Abstract:** The rapid advancements in Vision Language Models (VLMs) have prompted the
development of multi-modal medical assistant systems. Despite this progress,
current models still have inherent probabilistic uncertainties, often producing
erroneous or unverified responses-an issue with serious implications in medical
applications. Existing methods aim to enhance the performance of Medical Vision
Language Model (MedVLM) by adjusting model structure, fine-tuning with
high-quality data, or through preference fine-tuning. However, these
training-dependent strategies are costly and still lack sufficient alignment
with clinical expertise. To address these issues, we propose an
expert-in-the-loop framework named Expert-Controlled Classifier-Free Guidance
(Expert-CFG) to align MedVLM with clinical expertise without additional
training. This framework introduces an uncertainty estimation strategy to
identify unreliable outputs. It then retrieves relevant references to assist
experts in highlighting key terms and applies classifier-free guidance to
refine the token embeddings of MedVLM, ensuring that the adjusted outputs are
correct and align with expert highlights. Evaluations across three medical
visual question answering benchmarks demonstrate that the proposed Expert-CFG,
with 4.2B parameters and limited expert annotations, outperforms
state-of-the-art models with 13B parameters. The results demonstrate the
feasibility of deploying such a system in resource-limited settings for
clinical use.

</details>


### [69] [Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline](https://arxiv.org/abs/2507.09214)
*Shiyi Mu,Zichong Gu,Hanqi Lyu,Yilin Gao,Shugong Xu*

Main category: cs.CV

> 本文提出了一种基于立体视觉的3D异常对象检测算法(S3AD)，解决了3D检测模型在开放道路环境下对罕见异常类别物体检测不准的问题，通过解耦3D和2D训练策略和提出异常评分算法，增强了算法的泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 为了提高3D检测模型对于任意形状目标的泛化能力，并具备过滤异常物体的能力，特别是处理开放道路上罕见异常类别物体的检测问题，而提出的新算法。

**Method:** 提出了基于立体视觉的3D异常对象检测算法(S3AD)，该算法将3D和2D的训练策略解耦，以释放任意3D前景检测的泛化能力，并提出基于前景置信度预测的异常评分算法，实现目标级异常评分。此外，为了进一步验证和增强异常检测的泛化能力，使用3D渲染方法合成了包含97个新类别的两个增强现实的双目立体3D检测数据集（命名为KITTI-AR）。

**Result:** 通过在合成的增强现实数据集KITTI-AR上进行实验，验证了所提出的算法和数据集在3D异常对象检测中的有效性，尤其是在罕见类别目标检测方面的优越性。

**Conclusion:** 实验验证了算法和数据集的性能。在增强现实数据集KITTI-AR上进行了广泛测试，显示了S3AD算法在处理罕见类别3D异常对象检测方面的优越性能。

**Abstract:** 3D detection technology is widely used in the field of autonomous driving,
with its application scenarios gradually expanding from enclosed highways to
open conventional roads. For rare anomaly categories that appear on the road,
3D detection models trained on closed sets often misdetect or fail to detect
anomaly objects. To address this risk, it is necessary to enhance the
generalization ability of 3D detection models for targets of arbitrary shapes
and to possess the capability to filter out anomalies. The generalization of 3D
detection is limited by two factors: the coupled training of 2D and 3D, and the
insufficient diversity in the scale distribution of training samples. This
paper proposes a Stereo-based 3D Anomaly object Detection (S3AD) algorithm,
which decouples the training strategy of 3D and 2D to release the
generalization ability for arbitrary 3D foreground detection, and proposes an
anomaly scoring algorithm based on foreground confidence prediction, achieving
target-level anomaly scoring. In order to further verify and enhance the
generalization of anomaly detection, we use a 3D rendering method to synthesize
two augmented reality binocular stereo 3D detection datasets which named
KITTI-AR. KITTI-AR extends upon KITTI by adding 97 new categories, totaling 6k
pairs of stereo images. The KITTI-AR-ExD subset includes 39 common categories
as extra training data to address the sparse sample distribution issue.
Additionally, 58 rare categories form the KITTI-AR-OoD subset, which are not
used in training to simulate zero-shot scenarios in real-world settings, solely
for evaluating 3D anomaly detection. Finally, the performance of the algorithm
and the dataset is verified in the experiments. (Code and dataset can be
obtained at https://github.com/xxxx/xxx).

</details>


### [70] [360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models](https://arxiv.org/abs/2507.09216)
*Jingguo Liu,Han Yu,Shigang Li,Jianfeng Li*

Main category: cs.CV

> 本研究表明，采用提出的球面采样技术，可以在全景图像任务中直接利用现有的二维图像预训练模型，并获得了室内数据集上出色的分割性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于缺乏百万规模的全景图像数据集，当前的任务主要依赖于二维预训练模型作为骨干网络，然而这些模型在处理全景图像时表现不佳，其原因在于全景图像中存在的失真和不连续性问题。

**Method:** 提出了一种球面采样方法，能够在全景图像中直接使用预训练的二维图像模型，同时通过球面离散采样和基于模型权重的方法，减轻图像失真，提高模型训练效果。

**Result:** 这篇论文提出了一种用于全景图像的球面采样方法，该方法使得现有的二维图像预训练模型可以直接应用于全景图像任务。通过基于预训练模型权重的球面离散采样，该方法能够有效缓解全景图像中的失真问题，并且在室内数据集Stanford2D3D上实现了显著的分割效果。

**Conclusion:** 实验结果表明，使用上述方法可以显著提高在全景图像上的分割性能，尤其是斯坦福2D3D数据集上的分割效果。

**Abstract:** Due to the current lack of large-scale datasets at the million-scale level,
tasks involving panoramic images predominantly rely on existing two-dimensional
pre-trained image benchmark models as backbone networks. However, these
networks are not equipped to recognize the distortions and discontinuities
inherent in panoramic images, which adversely affects their performance in such
tasks. In this paper, we introduce a novel spherical sampling method for
panoramic images that enables the direct utilization of existing pre-trained
models developed for two-dimensional images. Our method employs spherical
discrete sampling based on the weights of the pre-trained models, effectively
mitigating distortions while achieving favorable initial training values.
Additionally, we apply the proposed sampling method to panoramic image
segmentation, utilizing features obtained from the spherical model as masks for
specific channel attentions, which yields commendable results on commonly used
indoor datasets, Stanford2D3D.

</details>


### [71] [Online Long-term Point Tracking in the Foundation Model Era](https://arxiv.org/abs/2507.09217)
*Görkay Aydemir*

Main category: cs.CV

> 本文介绍了一种名为Track-On的Transformer模型，该模型能实现长期在线点跟踪，不依赖未来帧信息。该模型利用视觉基础模型提供初始几何表示，并通过记忆机制传递外观和上下文信息来维持长时一致性。实验结果表明，该模型在七个公开基准上达到了新的技术水平。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决长期点跟踪问题，特别是在只能根据当前和过去帧进行预测的在线设置中。该问题具有重要意义，因为在诸如直播视频和具身AI的应用中，决策需要立即基于过去的数据做出。

**Method:** Structure

**Result:** {
  "tldr": "本文介绍了一种名为Track-On的Transformer模型，该模型能实现长期在线点跟踪，不依赖未来帧信息。该模型利用视觉基础模型提供初始几何表示，并通过记忆机制传递外观和上下文信息来维持长时一致性。实验结果表明，该模型在七个公开基准上达到了新的技术水平。",
  "motivation": "本文旨在解决长期点跟踪问题，特别是在只能根据当前和过去帧进行预测的在线设置中。该问题具有重要意义，因为在诸如直播视频和具身AI的应用中，决策需要立即基于过去的数据做出。",
  "method": "Track-On模型将每个跟踪点视为查询，逐帧处理视频。模型利用视觉基础模型的初始几何表示，并通过一种记忆机制来传播外观和上下文信息，以维持长时一致性。",
  "result": "Track-On模型在没有未来帧信息的情况下达到长期跟踪的新水平，并在七个公开基准上展示了卓越的性能。",
  "conclusion": "本文提出了有效的解决方案以满足实时决策场景下的长期点跟踪需求，展示了一个新的Transformer架构，它可以在线环境下维持长时间的动作一致性。")

**Conclusion:** 本文提出了有效的解决方案以满足实时决策场景下的长期点跟踪需求，展示了一个新的Transformer架构，它可以在线环境下维持长时间的动作一致性。

**Abstract:** Point tracking aims to identify the same physical point across video frames
and serves as a geometry-aware representation of motion. This representation
supports a wide range of applications, from robotics to augmented reality, by
enabling accurate modeling of dynamic environments. Most existing long-term
tracking approaches operate in an offline setting, where future frames are
available to refine predictions and recover from occlusions. However,
real-world scenarios often demand online predictions: the model must operate
causally, using only current and past frames. This constraint is critical in
streaming video and embodied AI, where decisions must be made immediately based
on past observations. Under such constraints, viewpoint invariance becomes
essential. Visual foundation models, trained on diverse large-scale datasets,
offer the potential for robust geometric representations. While they lack
temporal reasoning on their own, they can be integrated into tracking pipelines
to enrich spatial features. In this thesis, we address the problem of long-term
point tracking in an online setting, where frames are processed sequentially
without access to future information or sliding windows. We begin by evaluating
the suitability of visual foundation models for this task and find that they
can serve as useful initializations and be integrated into tracking pipelines.
However, to enable long-term tracking in an online setting, a dedicated design
is still required. In particular, maintaining coherence over time in this
causal regime requires memory to propagate appearance and context across
frames. To address this, we introduce Track-On, a transformer-based model that
treats each tracked point as a query and processes video frames one at a time.
Track-On sets a new state of the art across seven public benchmarks,
demonstrating the feasibility of long-term tracking without future access.

</details>


### [72] [Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift](https://arxiv.org/abs/2507.09222)
*Behraj Khan,Tahir Syed*

Main category: cs.CV

> 本研究提出了StaRFM框架来解决基础模型在视觉和医学图像分析中的部署挑战，实现了显著的性能改进和跨域性能差距减少。

<details>
  <summary>Details</summary>

**Motivation:** 由于分布转移和信心不匹配问题是基础模型在计算机视觉和医学成像领域部署中的关键挑战，当前的解决方案尚局限在特定领域，尚未有综合性的解决方案。

**Method:** 本论文提出了一个名为StaRFM的统一框架，以解决分布在训练和测试数据中的转移问题以及信心不匹配导致的过度自信的错误预测问题。该框架通过引入Fisher信息惩罚（FIP）以及扩展到3D医疗数据的片区域正则化来减少CLIP和SAM嵌入中的协变量移位。此外，提出了置信度不匹配惩罚（CMP），用于重新评估体素级别的预测结果，以调整分割任务中的不确定性。

**Result:** 实验结果显示，使用StaRFM后，在19个视觉数据集中平均提高了3.5%的准确性和降低了28%的预期校准误差（ECE）。在医学分割任务中，例如BraTS和ATLAS等数据集，DSC提高了84.7%，HD95降低了到4.8mm。相比之下，跨域性能差距与之前的基准方法相比减少了40%。

**Conclusion:** StaRFM框架实现了广泛的性能改进，提升了3.5%的准确性和降低了28%的预期校准误差（ECE）在19个视觉数据集上，实现了84.7%的Dice相似度系数（DSC）和4.8mm的平均Hausdorff距离（HD95）在8个医学分割数据集上，同时与先前的基准方法相比，跨域性能差距减少了40%。

**Abstract:** Foundation models like CLIP and SAM have transformed computer vision and
medical imaging via low-shot transfer learning. However, deployment of these
models hindered by two key challenges: \textit{distribution shift} between
training and test data, and \textit{confidence misalignment} that leads to
overconfident incorrect predictions. These issues manifest differently in
vision-language classification and medical segmentation tasks, yet existing
solutions remain domain-specific. We propose \textit{StaRFM}, a unified
framework addressing both challenges. It introduces a Fisher information
penalty (FIP), extended to 3D medical data via patch-wise regularization, to
reduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence
misalignment penalty (CMP), reformulated for voxel-level predictions,
calibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes
bounds showing FIP controls generalization via the Fisher-Rao norm, while CMP
minimizes calibration error through Brier score optimization. StaRFM shows
consistent performance like \texttt{+}3.5\% accuracy and 28\% lower ECE on 19
vision datasets (e.g., ImageNet, Office-Home), 84.7\% DSC and 4.8mm HD95 in
medical segmentation (e.g., BraTS, ATLAS), and 40\% lower cross-domain
performance gap compared to prior benchmarking methods. The framework is
plug-and-play, requiring minimal architectural changes for seamless integration
with foundation models. Code and models will be released at
https://anonymous.4open.science/r/StaRFM-C0CD/README.md

</details>


### [73] [EgoAnimate: Generating Human Animations from Egocentric top-down Views](https://arxiv.org/abs/2507.09230)
*G. Kutay Türkoglu,Julian Tanke,Iheb Belgacem,Lev Markhasin*

Main category: cs.CV

> 采用基于Stable Diffusion的生成模型解决顶向下视角图像的遮挡问题，生成前视图像并转化为可动画的虚拟形象，提升了远程呈现的体验。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于克服使用第一人称视角（即顶向下视角）捕捉人体和动作时遇到的遮挡和身体比例失真问题，通过生成式方法实现更便携且成本效益更高的远程呈现体验。

**Method:** 该研究采用了基于Stable Diffusion的生成式模型，并结合ControlNet来解决从顶向下视角的单一图像生成真实的前视图问题，从而能够将这种图像转换为动画形象，适用于虚拟现实中的远程呈现系统。

**Result:** 该研究首次展示了如何利用生成式背骨模型从单个顶向下图像中重建可操控的虚拟形象，并将这一技术推进至运动生成的过渡阶段，具有较高的泛化能力。

**Conclusion:** 该研究通过对顶向下视角图像进行处理，生成可应用于动作生成模型的前视图，从而简化了输入，增强了系统的可访问性和泛化能力，为远程呈现体验铺平了道路。

**Abstract:** An ideal digital telepresence experience requires accurate replication of a
person's body, clothing, and movements. To capture and transfer these movements
into virtual reality, the egocentric (first-person) perspective can be adopted,
which enables the use of a portable and cost-effective device without
front-view cameras. However, this viewpoint introduces challenges such as
occlusions and distorted body proportions.
  There are few works reconstructing human appearance from egocentric views,
and none use a generative prior-based approach. Some methods create avatars
from a single egocentric image during inference, but still rely on multi-view
datasets during training. To our knowledge, this is the first study using a
generative backbone to reconstruct animatable avatars from egocentric inputs.
Based on Stable Diffusion, our method reduces training burden and improves
generalizability.
  Inspired by methods such as SiTH and MagicMan, which perform 360-degree
reconstruction from a frontal image, we introduce a pipeline that generates
realistic frontal views from occluded top-down images using ControlNet and a
Stable Diffusion backbone.
  Our goal is to convert a single top-down egocentric image into a realistic
frontal representation and feed it into an image-to-motion model. This enables
generation of avatar motions from minimal input, paving the way for more
accessible and generalizable telepresence systems.

</details>


### [74] [PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process](https://arxiv.org/abs/2507.09242)
*Shiqi Jiang,Xinpeng Li,Xi Mao,Changbo Wang,Chenhui Li*

Main category: cs.CV

> 该研究提出了一种用于评估绘画过程的新框架，引入了Painting Process Assessment Dataset (PPAD) 和 PPJudge 模型，它是一个基于Transformer的模型，用于有效评估绘画过程，并在多个方面超过了现有的评估方法。

<details>
  <summary>Details</summary>

**Motivation:** 尽管近年来该领域看到了大量用于评估绘画审美质量的数据集和方法的涌现，但大多数现有方法仅关注静态的最终图像，忽略了艺术绘画过程的动态和多阶段性质。为了解决这一差距，我们提出了一个新颖的框架用于人类对齐的艺术绘画过程评估。

**Method:** 我们提出了一种新的框架用于人类对齐的艺术绘画过程评估。我们引入了Painting Process Assessment Dataset (PPAD)，这是第一个包含真实和合成绘画过程图像的大规模数据集，并由领域专家根据八个详细的属性进行了标注。此外，我们提出了PPJudge（Painting Process Judge），这是一个基于Transformer的模型，通过增强的时间感知位置编码和异构专家混合架构，实现了有效的绘画过程评估。

**Result:** 实验结果表明，我们的方法在准确性、鲁棒性和与人类判断的对齐方面超过了现有的基线方法。

**Conclusion:** 我们的方法为计算创造力和艺术教育提供了新的视角。

**Abstract:** Artistic image assessment has become a prominent research area in computer
vision. In recent years, the field has witnessed a proliferation of datasets
and methods designed to evaluate the aesthetic quality of paintings. However,
most existing approaches focus solely on static final images, overlooking the
dynamic and multi-stage nature of the artistic painting process. To address
this gap, we propose a novel framework for human-aligned assessment of painting
processes. Specifically, we introduce the Painting Process Assessment Dataset
(PPAD), the first large-scale dataset comprising real and synthetic painting
process images, annotated by domain experts across eight detailed attributes.
Furthermore, we present PPJudge (Painting Process Judge), a Transformer-based
model enhanced with temporally-aware positional encoding and a heterogeneous
mixture-of-experts architecture, enabling effective assessment of the painting
process. Experimental results demonstrate that our method outperforms existing
baselines in accuracy, robustness, and alignment with human judgment, offering
new insights into computational creativity and art education.

</details>


### [75] [AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition](https://arxiv.org/abs/2507.09248)
*Varsha Devi,Amine Bohi,Pardeep Kumar*

Main category: cs.CV

> 论文提出了AGCD-Net，一种注意力引导的因果去偏网络，通过增强特征的重校准减少上下文偏差，实验证明其在情绪识别上表现优异。

<details>
  <summary>Details</summary>

**Motivation:** 传统的上下文感知情绪识别方法容易受到背景与情绪标签之间偶然关联的影响。因此，动机是为了构建一个能够减少这种偏差、提高在真实场景中情绪识别准确性的系统。

**Method:** 提出了AGCD-Net，利用Hybrid ConvNeXt增强特征的重校准，并通过AG-CIM基于因果理论扰动上下文特征，隔离偶然关联，利用面部特征进行注意力引导的修正以减少上下文偏置。

**Result:** AGCD-Net模型通过引入Hybrid ConvNeXt（一种结合了空间变换网络和挤压激励层的卷积编码器）来增强特征重校准，提出了一种注意力引导的因果干预模块AG-CIM，以减少背景情境对情绪识别的偏差。实验结果在CAER-S数据集上验证了该模型的有效性，达到了最先进的性能，证明了因果去偏对于复杂环境下情绪识别的重要性。

**Conclusion:** AGCD-Net通过去偏处理显著提升了情绪识别的准确性，特别是在存在上下文偏差的情况下，表现出色。实验验证了AGCD-Net的性能优于现有方法，并指出了因果去偏在复杂情绪识别任务中的重要性。

**Abstract:** Context-aware emotion recognition (CAER) enhances affective computing in
real-world scenarios, but traditional methods often suffer from context
bias-spurious correlation between background context and emotion labels (e.g.
associating ``garden'' with ``happy''). In this paper, we propose
\textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces
\textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the
ConvNeXt backbone by integrating Spatial Transformer Network and
Squeeze-and-Excitation layers for enhanced feature recalibration. At the core
of AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM),
which applies causal theory, perturbs context features, isolates spurious
correlations, and performs an attention-driven correction guided by face
features to mitigate context bias. Experimental results on the CAER-S dataset
demonstrate the effectiveness of AGCD-Net, achieving state-of-the-art
performance and highlighting the importance of causal debiasing for robust
emotion recognition in complex settings.

</details>


### [76] [Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching](https://arxiv.org/abs/2507.09256)
*Junyu Chen,Yihua Gao,Mingyuan Ge,Mingyong Li*

Main category: cs.CV

> The paper proposes AAHR to tackle ambiguities in image-text matching by using dynamic clustering, adaptive aggregation, GNN, and momentum contrastive learning.

<details>
  <summary>Details</summary>

**Motivation:** To improve image-text matching by better handling semantic ambiguities and leveraging neighborhood relationships.

**Method:** AAHR uses dynamic clustering prototype contrastive learning, global and local feature extraction, GNN, and momentum contrastive learning to enhance semantic understanding.

**Result:** Experimental results show AAHR outperforms state-of-the-art methods on several image-text datasets.

**Conclusion:** AAHR is an effective framework that significantly improves the accuracy and efficiency of image-text matching by addressing the semantic ambiguities in training data.

**Abstract:** Image-text matching is crucial for bridging the semantic gap between computer
vision and natural language processing. However, existing methods still face
challenges in handling high-order associations and semantic ambiguities among
similar instances. These ambiguities arise from subtle differences between soft
positive samples (semantically similar but incorrectly labeled) and soft
negative samples (locally matched but globally inconsistent), creating matching
uncertainties. Furthermore, current methods fail to fully utilize the
neighborhood relationships among semantically similar instances within training
batches, limiting the model's ability to learn high-order shared knowledge.
This paper proposes the Ambiguity-Aware and High-order Relation learning
framework (AAHR) to address these issues. AAHR constructs a unified
representation space through dynamic clustering prototype contrastive learning,
effectively mitigating the soft positive sample problem. The framework
introduces global and local feature extraction mechanisms and an adaptive
aggregation network, significantly enhancing full-grained semantic
understanding capabilities. Additionally, AAHR employs intra-modal and
inter-modal correlation matrices to investigate neighborhood relationships
among sample instances thoroughly. It incorporates GNN to enhance semantic
interactions between instances. Furthermore, AAHR integrates momentum
contrastive learning to expand the negative sample set. These combined
strategies significantly improve the model's ability to discriminate between
features. Experimental results demonstrate that AAHR outperforms existing
state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,
considerably improving the accuracy and efficiency of image-text matching. The
code and model checkpoints for this research are available at
https://github.com/Image-Text-Matching/AAHR .

</details>


### [77] [SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation](https://arxiv.org/abs/2507.09266)
*JianHe Low,Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

> 新的SLT方法通过改进的标记化和对齐策略提高了性能，缩短了输入序列长度，降低了计算需求。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有SLT模型复杂度高、计算成本高且不便于规模化处理的问题。

**Method:** 提出段落感知的视觉标记化框架，利用手势分段将连续视频转化为视觉标记，引入标记对标记的对比对齐目标和双层次监督机制以改善跨模态对齐。

**Result:** 该研究提出了一个基于手势分段的视觉标记化框架，显著缩短了输入序列长度，降低了内存使用。通过引入标记到标记的对比对齐目标和双层次监督机制，改善了跨模态对齐的精细度。在PHOENIX14T基准测试中，该方法显著超越了现有的最佳方法。

**Conclusion:** 提出的段落感知视觉标记化框架及其对齐策略在性能和效率上都优于现有方法，验证了所提出策略的有效性。

**Abstract:** Gloss-free Sign Language Translation (SLT) has advanced rapidly, achieving
strong performances without relying on gloss annotations. However, these gains
have often come with increased model complexity and high computational demands,
raising concerns about scalability, especially as large-scale sign language
datasets become more common. We propose a segment-aware visual tokenization
framework that leverages sign segmentation to convert continuous video into
discrete, sign-informed visual tokens. This reduces input sequence length by up
to 50% compared to prior methods, resulting in up to 2.67x lower memory usage
and better scalability on larger datasets. To bridge the visual and linguistic
modalities, we introduce a token-to-token contrastive alignment objective,
along with a dual-level supervision that aligns both language embeddings and
intermediate hidden states. This improves fine-grained cross-modal alignment
without relying on gloss-level supervision. Our approach notably exceeds the
performance of state-of-the-art methods on the PHOENIX14T benchmark, while
significantly reducing sequence length. Further experiments also demonstrate
our improved performance over prior work under comparable sequence-lengths,
validating the potential of our tokenization and alignment strategies.

</details>
