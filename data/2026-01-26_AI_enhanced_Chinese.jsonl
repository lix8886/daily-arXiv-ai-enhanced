{"id": "2601.16217", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16217", "abs": "https://arxiv.org/abs/2601.16217", "authors": ["Qingyan Yang", "Tongxi Wang", "Yunsheng Luo"], "title": "ChiEngMixBench: Evaluating Large Language Models on Spontaneous and Natural Chinese-English Code-Mixed Generation", "comment": null, "summary": "Code-mixing is increasingly prevalent in interactions between humans and large language models, yet existing work often reduces it to a translation or convertibility problem, making it difficult to assess whether a model's switching behavior is context-appropriate and aligned with human conventions. We introduce ChiEngMixBench, the first benchmark designed to evaluate code-mixing ability in authentic community contexts, built upon a general construction pipeline that enables scalable dataset development across domains and bilingual pairs. ChiEngMixBench formulates code-mixing as a cognitive alignment problem, characterized by two complementary signals: Spontaneity and Naturalness. Empirical evaluation shows that our metrics can systematically distinguish code-mixing performance across models. Beyond benchmarking, we further uncover an implicitly emergent Terminology Layering Strategy, a phenomenon consistent with the Matrix Language Frame (MLF) theory, indicating structured cognitive alignment between multilingual large language models and human communication.", "AI": {"tldr": "介绍ChiEngMixBench基准，评估代码混合在真实社区环境中的能力，强调其与矩阵语言框架理论的一致性，揭示多语言大型语言模型与人类交流之间的认知对齐。", "motivation": "现有的工作往往将代码混合简化为翻译或可转换性问题，这使得评估模型的切换行为是否符合语境和人类习惯变得困难。因此，引入ChiEngMixBench作为第一个评估在真实社区上下文中代码混合能力的基准。", "method": "介绍了一个名为ChiEngMixBench的基准，用于评估代码混合在真实社区环境中的能力。该基准建立在一个通用构建管道上，使跨领域和双语对的可扩展数据集开发成为可能。代码混合被视为一种认知对齐问题，由自发性和自然性两个互补信号组成。", "result": "实验评估表明，提出的评价指标可以系统地区分不同模型的代码混合表现。", "conclusion": "除了作为评估基准，该研究还揭示了一种隐含出现的主题层策略现象，这种现象与矩阵语言框架理论一致，表明多语言大型语言模型与人类交流之间存在结构化的认知对齐。"}}
{"id": "2601.16218", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16218", "abs": "https://arxiv.org/abs/2601.16218", "authors": ["Aleix Torres-Camps", "Nathaniel Mitrani Hadida", "Víctor Conchello Vendrell", "Àlex Batlle Casellas", "Arnau Padrés Masdemont", "Jordi Ros-Giralt"], "title": "M3Kang: Evaluating Multilingual Multimodal Mathematical Reasoning in Vision-Language Models", "comment": "10 pages, 8 figures", "summary": "Despite state-of-the-art vision-language models (VLMs) have demonstrated strong reasoning capabilities, their performance in multilingual mathematical reasoning remains underexplored, particularly when compared to human performance. To bridge this gap, we introduce M3Kang, the first massively multilingual, multimodal mathematical reasoning dataset for VLMs. It is derived from the Kangaroo Math Competition, the world's largest mathematics contest, which annually engages over six million participants under the age of 18 across more than 90 countries. M3Kang includes 1,747 unique multiple-choice problems organized by grade-level difficulty, with translations into 108 culturally diverse languages, some of them including diagrams essential for solving them. Using this dataset, we conduct extensive benchmarking on both closed- and open-source SOTA models. We observe that, despite recent advances, models still struggle with basic math and diagram-based reasoning, with performance scaling with language presence and model size, but not with grade level. We also find that multilingual techniques can be effectively extended to the multimodal setting, resulting in significant improvements over baseline approaches. Our analysis also incorporates performance data from over 68,000 students, enabling direct comparison with human performance. We are open-sourcing M3Kang, including the English-only subset M2Kang, along with the framework and codebase used to construct the dataset.", "AI": {"tldr": "本文发布了M3Kang数据集，挑战现有的视觉-语言模型在多语言多模态数学推理上的表现，并展示了模型在基本数学和图表理解上的不足。", "motivation": "本文旨在填补视觉-语言模型在多语言数学推理能力上的研究空白，并通过一个大规模多语言、多模态的数学推理数据集M3Kang来对比和测试不同模型的表现。", "method": "本文介绍了M3Kang数据集，这是一个面向多语言多模态数学推理的视觉-语言模型（VLM）数据集，来源于Kangaroo 数学竞赛，包含了1,747个不同的多选题，涵盖了不同年级难度，并翻译成108种语言，其中一些问题包含了用于解决问题的关键图表。", "result": "研究发现现有模型在基本数学和图表理解方面仍存在困难，且模型性能与语言种类和模型规模相关，而非年级难度。", "conclusion": "文章建议未来的研究可以进一步优化多语言方法在多模态环境下的应用，并将M3Kang数据集开源以供学术界使用。"}}
{"id": "2601.16219", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16219", "abs": "https://arxiv.org/abs/2601.16219", "authors": ["Erdem Aslan", "Pakize Erdoğmuş"], "title": "Domain Specific Specialization in Low-Resource Settings: The Efficacy of Offline Response-Based Knowledge Distillation in Large Language Models", "comment": "10 pages, 10 tables", "summary": "Large Language Models (LLMs) excel in general tasks but often struggle with hallucinations when handling domain-specific or institutional knowledge absent from their pre-training. We present an offline response-based knowledge distillation method that develops high-accuracy specialized assistants under constrained hardware resources. We evaluate three distinct data strategies: general domain adaptation (15,000 lines), unstructured knowledge injection (2,000 lines), and a context-aware synthetic dataset (500 lines) generated by a teacher model. To minimize computational costs, we utilize the Unsloth library to optimize the Qwen-2.5-7B student model, reducing NVIDIA A100 GPU memory requirements from 40 GB to 16 GB. Experimental results demonstrate that while larger unstructured datasets suffer from persistent hallucinations, the 500-line context-aware dataset achieves a 96.7% accuracy rate and robust rejection capability. These findings validate the LIMA hypothesis, showing that data quality and structural alignment are more critical than quantity for domain adaptation in low-resource settings.", "AI": {"tldr": "本文通过优化Qwen-2.5-7B模型并使用高效的数据蒸馏策略，在低资源环境下实现了高精度专用助手的开发，验证了数据质量的重要性。", "motivation": "大型语言模型在通用任务上表现出色，但在处理未包含在预训练过程中的领域特定或机构知识时往往会出现幻想问题。为了在这种低资源环境中解决这一问题，本文提出了上述方法。", "method": "本文提出了一种基于离线响应的知识蒸馏方法，旨在在硬件资源受限的情况下开发高精度的专用助手。该方法利用了一般领域迁移、非结构化知识注入和上下文感知合成数据集这三种不同的数据策略，并使用Unsloth库优化了Qwen-2.5-7B学生模型，从而大幅度减少了GPU的内存需求。", "result": "实验结果显示，较大的非结构化数据集仍然存在持续的幻想问题，而500行上下文感知数据集达到了96.7%的准确率和强大的拒绝能力。", "conclusion": "研究验证了LIMA假设，表明数据质量和结构一致性比数量对于低资源环境中的领域适应性更具关键性。"}}
{"id": "2601.16220", "categories": ["cs.CL", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.16220", "abs": "https://arxiv.org/abs/2601.16220", "authors": ["Nesta Midavaine", "Christian A. Naesseth", "Grigory Bartosh"], "title": "Towards Latent Diffusion Suitable For Text", "comment": null, "summary": "Language diffusion models aim to improve sampling speed and coherence over autoregressive LLMs. We introduce Neural Flow Diffusion Models for language generation, an extension of NFDM that enables the straightforward application of continuous diffusion models to discrete state spaces. NFDM learns a multivariate forward process from the data, ensuring that the forward process and generative trajectory are a good fit for language modeling. Our model substantially reduces the likelihood gap with autoregressive models of the same size, while achieving sample quality comparable to that of previous latent diffusion models.", "AI": {"tldr": "本文介绍了神经流扩散模型（NFDM）来改进语言生成的速度和连贯性，并且显著减小了与同规模自回归模型的性能差距，同时达到了与之前潜扩散模型相当的采样质量。", "motivation": "通过引入NFDM解决自回归语言模型在生成速度和连贯性上的问题，同时保持高质量的采样效果。", "method": "NFDM通过多变量前向过程从数据中学习，并保证了前向过程和生成轨迹适用于语言模型，从而将连续扩散模型直接应用到离散状态空间。", "result": "NFDM能大幅减少与自回归模型的性能差距，达到了与之前潜扩散模型相当的样本质量。", "conclusion": "研究表明NFDM在语言生成任务中具有优势，特别是在提高生成速度和连贯性方面。"}}
{"id": "2601.16272", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.16272", "abs": "https://arxiv.org/abs/2601.16272", "authors": ["Xiaoyan Xing", "Philipp Henzler", "Junhwa Hur", "Runze Li", "Jonathan T. Barron", "Pratul P. Srinivasan", "Dor Verbin"], "title": "GR3EN: Generative Relighting for 3D Environments", "comment": "project page: https://gr3en-relight.github.io/", "summary": "We present a method for relighting 3D reconstructions of large room-scale environments. Existing solutions for 3D scene relighting often require solving under-determined or ill-conditioned inverse rendering problems, and are as such unable to produce high-quality results on complex real-world scenes. Though recent progress in using generative image and video diffusion models for relighting has been promising, these techniques are either limited to 2D image and video relighting or 3D relighting of individual objects. Our approach enables controllable 3D relighting of room-scale scenes by distilling the outputs of a video-to-video relighting diffusion model into a 3D reconstruction. This side-steps the need to solve a difficult inverse rendering problem, and results in a flexible system that can relight 3D reconstructions of complex real-world scenes. We validate our approach on both synthetic and real-world datasets to show that it can faithfully render novel views of scenes under new lighting conditions.", "AI": {"tldr": "提出了一种对大规模房间级环境的3D重建进行重新照明的方法，通过将视频到视频的重新照明扩散模型的输出提炼到3D重建中，避免了解决困难的逆向渲染问题，实现了对复杂现实场景的灵活3D重新照明。", "motivation": "现有的3D场景重新照明解决方案通常需要解决欠定或病态的逆向渲染问题，这使得它们无法在复杂的现实场景中产生高质量的结果。而这种方法试图解决这一问题，以提供对大空间场景的高质量重新照明。", "method": "方法包括将视频到视频重新照明扩散模型的输出提炼到3D重建中，从而避免了处理逆向渲染问题的需要。", "result": "在合成和真实世界的数据集上验证了该方法，结果显示它可以真实地渲染场景在新照明条件下的新视图。", "conclusion": "该方法提供了一种解决大规模复杂场景3D重新照明问题的新途径，无需解决逆向渲染问题即可获得高质量结果。"}}
{"id": "2601.16224", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16224", "abs": "https://arxiv.org/abs/2601.16224", "authors": ["Sami-ul Ahmed"], "title": "Limits of n-gram Style Control for LLMs via Logit-Space Injection", "comment": "18 pages, 7 figures. Experimental study of decoding-time style control via n-gram logit injection", "summary": "Large language models (LLMs) are typically personalized via prompt engineering or parameter-efficient fine-tuning such as LoRA. However, writing style can be difficult to distill into a single prompt, and LoRA fine-tuning requires computationally intensive training and infrastructure. We investigate a possible lightweight alternative: steering a frozen LLM with n-gram style priors injected in logit space at decoding time. We train an n-gram model on stylistically distinct corpora -- including Don Quixote, CNN/DailyMail news headlines, and arXiv abstracts -- constructing an interpolated 1-to-3-gram prior over next-token probabilities. During generation we modify the LLM's logits by adding a weighted sum of style log-probabilities from each n-gram order that matches the current context, scaled by a control parameter lambda in [0, 1].\n  We sweep lambda and style corpora and report style perplexity under the n-gram model, base-model perplexity as a proxy for fluency, Jensen-Shannon (JS) divergence between the original and steered token distributions, and token-overlap statistics. On TinyLlama-1.1B we identify a single narrow regime (for the Don Quixote corpus at lambda=0.1) where style perplexity improves by 24.7% and base-model perplexity improves by 51.4% relative to the frozen model. Outside this regime, and for multi-author corpora such as CNN/DailyMail and arXiv abstracts, even small nonzero lambda values generally result in worse style and fluency, and larger lambda values lead to collapse with extreme perplexities and incoherent text. Logit-space injection of n-gram style priors provides lightweight, tunable style control, but it is fragile: it operates effectively only within a narrow range of low lambda values and is consistently outperformed by prompting and LoRA.", "AI": {"tldr": "论文探索了一种新的轻量级方法，通过在logit空间注入n-gram风格先验来控制语言模型的风格。虽然有改进的空间，但大多数情况下还是被现有方法超越。", "motivation": "动机在于探索一种轻量级的方法来代替现有的个性化技术（如prompt调整或LoRA等参数高效微调方法），这些方法要么难以体现特定写作风格，要么需要消耗大量的计算资源和基础设施。", "method": "研究了一种轻量级的方法，通过在解码时在logit空间中注入n-gram风格先验来引导固定的LLM。具体来说，训练了一个n-gram模型以学习不同风格的语料库，并在生成过程中调整LLM的logits，通过加权和的方式结合当前上下文匹配的各个n-gram阶的风格对数概率。", "result": "实验结果表明，在对选择的语料库（例如《唐吉诃德》）和较低的控制参数值（如lambda=0.1）情境下，通过n-gram风格先验引导可以显著提高风格准确性和流畅度。但对于多作者语料库（如CNN/DailyMail和arXiv摘要），即使是较小的lambda值也会导致风格和流畅性的下降。", "conclusion": "虽然logit空间中的n-gram风格先验注入方法提供了一种轻量级和可调的风格控制手段，但在较广的参数范围和不同风格的语料库上，其效果往往不稳定，并在很大程度上被prompt调整和LoRA等方法所超越。"}}
{"id": "2601.16296", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.16296", "abs": "https://arxiv.org/abs/2601.16296", "authors": ["Dohun Lee", "Chun-Hao Paul Huang", "Xuelin Chen", "Jong Chul Ye", "Duygu Ceylan", "Hyeonho Jeong"], "title": "Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory", "comment": "Project page: https://dohunlee1.github.io/MemoryV2V", "summary": "Recent foundational video-to-video diffusion models have achieved impressive results in editing user provided videos by modifying appearance, motion, or camera movement. However, real-world video editing is often an iterative process, where users refine results across multiple rounds of interaction. In this multi-turn setting, current video editors struggle to maintain cross-consistency across sequential edits. In this work, we tackle, for the first time, the problem of cross-consistency in multi-turn video editing and introduce Memory-V2V, a simple, yet effective framework that augments existing video-to-video models with explicit memory. Given an external cache of previously edited videos, Memory-V2V employs accurate retrieval and dynamic tokenization strategies to condition the current editing step on prior results. To further mitigate redundancy and computational overhead, we propose a learnable token compressor within the DiT backbone that compresses redundant conditioning tokens while preserving essential visual cues, achieving an overall speedup of 30%. We validate Memory-V2V on challenging tasks including video novel view synthesis and text-conditioned long video editing. Extensive experiments show that Memory-V2V produces videos that are significantly more cross-consistent with minimal computational overhead, while maintaining or even improving task-specific performance over state-of-the-art baselines. Project page: https://dohunlee1.github.io/MemoryV2V", "AI": {"tldr": "Memory-V2V通过引入显式记忆和标记压缩器，解决了多轮视频编辑中的交叉一致性问题，并实现了30%的速度提升。", "motivation": "现有的视频编辑器在多轮编辑过程中难以保持跨编辑的一致性。为了应对这一挑战，提出了Memory-V2V框架，首次解决了多轮视频编辑中的交叉一致性问题。", "method": "Memory-V2V框架通过在现有视频到视频模型中加入显式记忆来解决多轮视频编辑中的交叉一致性问题。Memory-V2V利用准确的检索和动态标记策略，基于先前的编辑结果来条件化当前的编辑步骤。为了进一步减少冗余和计算开销，提出了一个可学习的标记压缩器，以压缩冗余的条件标记，同时保留关键的视觉线索，实现整体30%的速度提升。", "result": "Memory-V2V在涉及视频新颖视角合成和基于文本的长视频编辑等具有挑战性的任务上进行了验证。广泛的实验显示，Memory-V2V生成的视频具有显著的交叉一致性，计算开销较低，并且在任务特定性能方面与最先进的基线相比保持甚至提高。", "conclusion": "Memory-V2V框架有效解决了在多轮视频编辑中保持交叉一致性的挑战，同时降低了计算开销并保持了任务特定性能的提高，这是一个重要的技术突破。"}}
{"id": "2601.16276", "categories": ["cs.CL", "cs.AI", "cs.GT", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.16276", "abs": "https://arxiv.org/abs/2601.16276", "authors": ["Victor Conchello Vendrell", "Max Ruiz Luyten", "Mihaela van der Schaar"], "title": "GameTalk: Training LLMs for Strategic Conversation", "comment": "32 pages, 8 figures", "summary": "Strategic decision-making in multi-agent settings is a key challenge for large language models (LLMs), particularly when coordination and negotiation must unfold over extended conversations. While recent work has explored the use of LLMs in isolated decision tasks, little attention has been given to optimizing long-term objectives through dialogue. We introduce \\textbf{GameTalk}, a framework for training LLMs to make strategic decisions via multi-turn interactions. Unlike prior work that focuses on single-turn objectives or static action prediction, we train LLMs to optimize a global objective across full conversations. We achieve this by adapting fine-tuning methods like GRPO, DPO, and STaR to incorporate reward signals that depend on the entire interaction. We evaluate this approach on a suite of increasingly complex games, designed to stress different aspects of reasoning, coordination, and opponent modeling. Our results show that GameTalk significantly outperforms untrained models, especially under reward shaping, with DPO consistently yielding the strongest gains. These findings position conversational fine-tuning as a promising path for LLMs to reason, negotiate, and act in interactive environments.", "AI": {"tldr": "GameTalk框架通过多轮对话来训练大型语言模型以优化整个对话过程中的全局目标，并在一系列游戏中表现出显著优势。", "motivation": "优化大型语言模型在多智能体场景下长期目标的策略性决策，特别是在需要通过延长对话进行协调与谈判时。", "method": "采用GameTalk框架，将奖励信号适应到整个交互过程的Fine-Tuning方法（如GRPO, DPO, STaR）来训练大型语言模型。", "result": "GameTalk在设计用于评估不同推理、协调和对手建模能力的游戏下取得显著优于未经训练模型的结果。", "conclusion": "通过对话微调的方法，大型语言模型在互动环境中具备解决策略性决策问题的潜力。"}}
{"id": "2601.16302", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.16302", "abs": "https://arxiv.org/abs/2601.16302", "authors": ["Abhijeet Parida", "Antonia Alomar", "Zhifan Jiang", "Pooneh Roshanitabrizi", "Austin Tapp", "Ziyue Xu", "Syed Muhammad Anwar", "Maria J. Ledesma-Carbayo", "Holger R. Roth", "Marius George Linguraru"], "title": "FeTTL: Federated Template and Task Learning for Multi-Institutional Medical Imaging", "comment": null, "summary": "Federated learning enables collaborative model training across geographically distributed medical centers while preserving data privacy. However, domain shifts and heterogeneity in data often lead to a degradation in model performance. Medical imaging applications are particularly affected by variations in acquisition protocols, scanner types, and patient populations. To address these issues, we introduce Federated Template and Task Learning (FeTTL), a novel framework designed to harmonize multi-institutional medical imaging data in federated environments. FeTTL learns a global template together with a task model to align data distributions among clients. We evaluated FeTTL on two challenging and diverse multi-institutional medical imaging tasks: retinal fundus optical disc segmentation and histopathological metastasis classification. Experimental results show that FeTTL significantly outperforms the state-of-the-art federated learning baselines (p-values <0.002) for optical disc segmentation and classification of metastases from multi-institutional data. Our experiments further highlight the importance of jointly learning the template and the task. These findings suggest that FeTTL offers a principled and extensible solution for mitigating distribution shifts in federated learning, supporting robust model deployment in real-world, multi-institutional environments.", "AI": {"tldr": "FeTTL is a novel federated learning framework that improves medical imaging data harmonization and overcomes domain shifts and data heterogeneity, proven effective in specific medical imaging tasks.", "motivation": "To enhance the effectiveness of federated learning in medical imaging, particularly in addressing challenges related to domain shifts and data heterogeneity across different medical centers.", "method": "Federated Template and Task Learning (FeTTL) is introduced to address the issues of domain shifts and heterogeneity in data during federated learning in medical imaging, aiming to harmonize multi-institutional medical imaging data by learning a global template with a task model to align data distributions among clients.", "result": "FeTTL shows significant performance improvement over existing federated learning methods in medical imaging tasks such as retinal fundus optical disc segmentation and histopathological metastasis classification.", "conclusion": "FeTTL provides a robust and effective method for mitigating distribution shifts in federated learning, with potential for broad application and robust model deployment in real-world environments."}}
{"id": "2601.16278", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.16278", "abs": "https://arxiv.org/abs/2601.16278", "authors": ["Branislav Pecher", "Jan Cegin", "Robert Belanec", "Ivan Srba", "Jakub Simko", "Maria Bielikova"], "title": "Better as Generators Than Classifiers: Leveraging LLMs and Synthetic Data for Low-Resource Multilingual Classification", "comment": "Accepted to the Findings of EACL 2026", "summary": "Large Language Models (LLMs) have demonstrated remarkable multilingual capabilities, making them promising tools in both high- and low-resource languages. One particularly valuable use case is generating synthetic samples that can be used to train smaller models in low-resource scenarios where human-labelled data is scarce. In this work, we investigate whether these synthetic data generation capabilities can serve as a form of distillation, producing smaller models that perform on par with or even better than massive LLMs across languages and tasks. To this end, we use a state-of-the-art multilingual LLM to generate synthetic datasets covering 11 languages and 4 classification tasks. These datasets are then used to train smaller models via fine-tuning or instruction tuning, or as synthetic in-context examples for compact LLMs. Our experiments show that even small amounts of synthetic data enable smaller models to outperform the large generator itself, particularly in low-resource languages. Overall, the results suggest that LLMs are best utilised as generators (teachers) rather than classifiers, producing data that empowers smaller and more efficient multilingual models.", "AI": {"tldr": "研究探讨了使用大语言模型生成合成数据来训练小型模型的可能性，结果表明这种方法在低资源语言中特别有效。", "motivation": "研究动机在于探究大语言模型的合成数据生成能力是否可以作为一种蒸馏方式，产生在多种语言和任务中表现与大规模大模型相当甚至更好的小型模型。", "method": "研究使用了最先进的多语言大模型来生成涵盖11种语言和4个分类任务的合成数据集。这些数据集被用来通过微调或指令调节来训练较小的模型，或作为紧凑型大模型中的合成上下文示例。", "result": "实验表明，即使是少量的合成数据，也能使小型模型的表现超越大型生成器本身，特别是在低资源语言中。", "conclusion": "总体而言，研究结果表明大语言模型最好被用作生成器（老师），而不是分类器，产生的数据使小型且更高效的多语言模型受益。"}}
