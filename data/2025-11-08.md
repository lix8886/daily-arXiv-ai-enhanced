<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.CV](#cs.CV) [Total: 58]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs](https://arxiv.org/abs/2511.03738)
*Pranav Bhandari,Nicolas Fay,Sanjeevan Selvaganapathy,Amitava Datta,Usman Naseem,Mehwish Nasim*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Large Language Models exhibit implicit personalities in their generation, but
reliably controlling or aligning these traits to meet specific needs remains an
open challenge. The need for effective mechanisms for behavioural manipulation
of the model during generation is a critical gap in the literature that needs
to be fulfilled. Personality-aware LLMs hold a promising direction towards this
objective. However, the relationship between these psychological constructs and
their representations within LLMs remains underexplored and requires further
investigation. Moreover, it is intriguing to understand and study the use of
these representations to steer the models' behaviour. We propose a novel
pipeline that extracts hidden state activations from transformer layers using
the Big Five Personality Traits (Openness, Conscientiousness, Extraversion,
Agreeableness and Neuroticism), which is a comprehensive and empirically
validated framework to model human personality applies low-rank subspace
discovery methods, and identifies trait-specific optimal layers across
different model architectures for robust injection. The resulting
personality-aligned directions are then operationalised through a flexible
steering framework with dynamic layer selection, enabling precise control of
trait expression in LLM outputs. Our findings reveal that personality traits
occupy a low-rank shared subspace, and that these latent structures can be
transformed into actionable mechanisms for effective steering through careful
perturbations without impacting the fluency, variance and general capabilities,
helping to bridge the gap between psychological theory and practical model
alignment.

</details>


### [2] [TextualVerifier: Verify TextGrad Step-by-Step](https://arxiv.org/abs/2511.03739)
*Eugenius Mario Situmorang,Adila Alfa Krisnadhi,Ari Wibisono*

Main category: cs.CL

> 本文介绍了一种名为TextualVerifier的验证框架，用于解决TextGrad在基于文本的决策过程中缺乏自我验证的问题，显著提高了系统的可靠性和推理的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于TextGrad在执行基于文本的优化时缺乏自我验证机制的问题，提出了TextualVerifier框架，利用链式思维和大语言模型的多数投票来解决这一验证差距。

**Method:** TextualVerifier采用链式思维分解、变体生成、多数投票和共识聚合四个阶段的流程，与TextGrad无侵入式集成，实现了文本基础的自动微分系统的自验证。

**Result:** 在PRM800K数据集上独立评估时，TextualVerifier提高了29%的推理步骤的有效性。在与TextGrad集成使用的时间上，分别在GPQA-Diamond、MMLU-ML和MMLU-CP基准上获得了8.08%、10.71%和3.92%的改善。

**Conclusion:** TextualVerifier是首个基于LLM技术的TextGrad自验证框架，无需数值梯度，提升了基于文本的优化系统的可靠性和验证的新方向。

**Abstract:** TextGrad is a novel approach to text-based automatic differentiation that
enables composite AI systems to perform optimization without explicit numerical
equations. However, it currently lacks self-verification mechanisms that ensure
reasoning validity in text-based decision making. This research introduces
TextualVerifier, a verification framework that leverages chain-of-thought
reasoning and majority voting with large language models to address this
verification gap. TextualVerifier implements a four-stage workflow:
chain-of-thought decomposition, variant generation, majority voting, and
consensus aggregation. It integrates non-invasively with TextGrad at both the
loss function and optimization result verification stages. Experimental
evaluation using the Gemini 1.5 Pro model is conducted in two phases: (1)
standalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad
on GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically
significant improvements (p < 0.001). In phase one, TextualVerifier improves
the validity of reasoning steps by 29 percent. In phase two, integration into
TextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4
percent with a moderate overhead of 5.9 LLM calls on average. Further
evaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92
percentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively.
TextualVerifier thus presents the first self-verification framework for
TextGrad through LLM-based techniques without requiring numerical gradients,
enabling more reliable reasoning and opening new directions for verification in
text-based optimization.

</details>


### [3] [GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning Evaluation](https://arxiv.org/abs/2511.03772)
*Stergios Chatzikyriakidis,Dimitris Papadakis,Sevasti-Ioanna Papaioannou,Erofili Psaltaki*

Main category: cs.CL

> 本文扩展了希腊方言数据集（GRDD+），包含10种希腊方言和超过600万个单词，通过微调实验展示了高质量方言数据对大型语言模型的影响。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是改进现有希腊方言数据集，提高数据集的多样性和规模，并通过微调实验查看高质量方言数据对大型语言模型性能的影响。

**Method:** 通过创建扩展的希腊方言数据集（GRDD+），并基于此数据集对三种大型语言模型架构进行了微调实验。

**Result:** 该研究创建了一个扩展的希腊方言数据集（GRDD+），它补充了现有的GRDD数据集，增加了克里特、塞浦路斯、本都和北希腊的更多数据，并添加了六个新的变体：科西嘉希腊语、南意大利希腊语（Griko）、马尼奥特语、海潘尼西亚语、察康尼安语和卡塔雷夫苏斯希腊语。该数据集共有10种变体和6,374,939个单词，是目前为止具有这种多样性和数量的首个数据集。研究者用该数据集对三种模型架构（Llama-3-8B、Llama-3.1-8B、Krikri-8B）进行了微调实验，以观察高质量的方言数据对大规模语言模型的影响，并将这些模型的表现与前沿模型（Claude-3.7-Sonnet、Gemini-2.5、ChatGPT-5）进行比较。

**Conclusion:** 创建了包含10种希腊语方言、共计超600万单词的GRDD+数据集，并通过实验比较了不同模型在微调后的性能。

**Abstract:** We present an extended Greek Dialectal Dataset (GRDD+) 1that complements the
existing GRDD dataset with more data from Cretan, Cypriot, Pontic and Northern
Greek, while we add six new varieties: Greco-Corsican, Griko (Southern Italian
Greek), Maniot, Heptanesian, Tsakonian, and Katharevusa Greek. The result is a
dataset with total size 6,374,939 words and 10 varieties. This is the first
dataset with such variation and size to date. We conduct a number of
fine-tuning experiments to see the effect of good quality dialectal data on a
number of LLMs. We fine-tune three model architectures (Llama-3-8B,
Llama-3.1-8B, Krikri-8B) and compare the results to frontier models
(Claude-3.7-Sonnet, Gemini-2.5, ChatGPT-5).

</details>


### [4] [PLLuM: A Family of Polish Large Language Models](https://arxiv.org/abs/2511.03823)
*Jan Kocoń,Maciej Piasecki,Arkadiusz Janz,Teddy Ferdinan,Łukasz Radliński,Bartłomiej Koptyra,Marcin Oleksy,Stanisław Woźniak,Paweł Walkowiak,Konrad Wojtasik,Julia Moska,Tomasz Naskręt,Bartosz Walkowiak,Mateusz Gniewkowski,Kamil Szyc,Dawid Motyka,Dawid Banach,Jonatan Dalasiński,Ewa Rudnicka,Bartłomiej Alberski,Tomasz Walkowiak,Aleksander Szczęsny,Maciej Markiewicz,Tomasz Bernaś,Hubert Mazur,Kamil Żyta,Mateusz Tykierko,Grzegorz Chodak,Tomasz Kajdanowicz,Przemysław Kazienko,Agnieszka Karlińska,Karolina Seweryn,Anna Kołos,Maciej Chrabąszcz,Katarzyna Lorenc,Aleksandra Krasnodębska,Artur Wilczek,Katarzyna Dziewulska,Paula Betscher,Zofia Cieślińska,Katarzyna Kowol,Daria Mikoś,Maciej Trzciński,Dawid Krutul,Marek Kozłowski,Sławomir Dadas,Rafał Poświata,Michał Perełkiewicz,Małgorzata Grębowiec,Maciej Kazuła,Marcin Białas,Roman Roszko,Danuta Roszko,Jurgita Vaičenonienė,Andrius Utka,Paweł Levchuk,Paweł Kowalski,Irena Prawdzic-Jankowska,Maciej Ogrodniczuk,Monika Borys,Anna Bulińska,Wiktoria Gumienna,Witold Kieraś,Dorota Komosińska,Katarzyna Krasnowska-Kieraś,Łukasz Kobyliński,Martyna Lewandowska,Marek Łaziński,Mikołaj Łątkowski,Dawid Mastalerz,Beata Milewicz,Agnieszka Anna Mykowiecka,Angelika Peljak-Łapińska,Sandra Penno,Zuzanna Przybysz,Michał Rudolf,Piotr Rybak,Karolina Saputa,Aleksandra Tomaszewska,Aleksander Wawer,Marcin Woliński,Joanna Wołoszyn,Alina Wróblewska,Bartosz Żuk,Filip Żarnecki,Konrad Kaczyński,Anna Cichosz,Zuzanna Deckert,Monika Garnys,Izabela Grabarczyk,Wojciech Janowski,Sylwia Karasińska,Aleksandra Kujawiak,Piotr Misztela,Maria Szymańska,Karolina Walkusz,Igor Siek,Jakub Kwiatkowski,Piotr Pęzik*

Main category: cs.CL

> 本文介绍了PLLuM，一个面向波兰语的大型开放源模型系列，强调其文化相关性、透明度和高质量，旨在促进开放研究并加强波兰主权AI技术。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在现代人工智能中占据核心地位，但其发展主要集中在英语上，导致了对其他语言支持的不足。PLLuM项目旨在填补这一空缺，倡导开放研究，并加强波兰主权AI技术的发展。

**Method:** 本文介绍了一种名为PLLuM（波兰大型语言模型）的波兰语专用大型开放源模型系列，由波兰主要研究机构联盟开发。为了满足高质量、透明和文化相关的语言模型需求，文章描述了模型的开发过程，包括构建一个新的1400亿个波兰语标记的预训练文本语料库，一个77K的自定义指令数据集，以及一个100K的偏好优化数据集。此外，该模型还包括一个负责任的AI框架，包含严格的数据治理和输出校正、安全过滤的混合模块。

**Result:** 该研究详细介绍了模型的架构、训练过程以及针对基础模型和指令调优变体的对齐技术，并展示了其在公共行政领域的下游任务中的实用性。

**Conclusion:** 通过公开发布这些模型，PLLuM项目希望激励开放研究，并增强波兰的主权AI技术。

**Abstract:** Large Language Models (LLMs) play a central role in modern artificial
intelligence, yet their development has been primarily focused on English,
resulting in limited support for other languages. We present PLLuM (Polish
Large Language Model), the largest open-source family of foundation models
tailored specifically for the Polish language. Developed by a consortium of
major Polish research institutions, PLLuM addresses the need for high-quality,
transparent, and culturally relevant language models beyond the English-centric
commercial landscape. We describe the development process, including the
construction of a new 140-billion-token Polish text corpus for pre-training, a
77k custom instructions dataset, and a 100k preference optimization dataset. A
key component is a Responsible AI framework that incorporates strict data
governance and a hybrid module for output correction and safety filtering. We
detail the models' architecture, training procedures, and alignment techniques
for both base and instruction-tuned variants, and demonstrate their utility in
a downstream task within public administration. By releasing these models
publicly, PLLuM aims to foster open research and strengthen sovereign AI
technologies in Poland.

</details>


### [5] [STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models](https://arxiv.org/abs/2511.03827)
*Mohammad Atif Quamar,Mohammad Areeb,Mikhail Kuznetsov,Muslum Ozgur Ozmen,Z. Berkay Celik*

Main category: cs.CL

> STARS通过拒绝采样技术改进大型语言模型与人类价值观的对齐，显著提高计算效率和对齐质量。

<details>
  <summary>Details</summary>

**Motivation:** 对齐大型语言模型与人类价值观对于它们的安全部署至关重要；然而，现有方法（如微调）计算量大且效果不佳。相比之下，在推理时间的方法，如最佳N个采样法要求不切实际的计算来实现最佳对齐。

**Method:** 提出STARS：分段级令牌对齐与拒绝采样，这是一种解码时间算法，通过迭代采样、评分和拒绝/接受固定大小的短令牌段来指导模型生成。

**Result:** 在六种不同的大语言模型中，STARS在胜率上优于监督微调（SFT）最多14.9个百分点，优于直接偏好优化（DPO）最多4.3个百分点，同时与强大的最佳N个基线保持高度竞争力。

**Conclusion:** 本研究建立了精细、奖励导向的采样作为传统微调和全序列排序方法的一般化、稳健且高效的替代方案。

**Abstract:** Aligning large language models with human values is crucial for their safe
deployment; however, existing methods, such as fine-tuning, are computationally
expensive and suboptimal. In contrast, inference-time approaches like Best-of-N
sampling require practically infeasible computation to achieve optimal
alignment. We propose STARS: Segment-level Token Alignment with Rejection
Sampling, a decoding-time algorithm that steers model generation by iteratively
sampling, scoring, and rejecting/accepting short, fixed-size token segments.
This allows for early correction of the generation path, significantly
improving computational efficiency and boosting alignment quality. Across a
suite of six LLMs, we show that STARS outperforms Supervised Fine-Tuning (SFT)
by up to 14.9 percentage points and Direct Preference Optimization (DPO) by up
to 4.3 percentage points on win-rates, while remaining highly competitive with
strong Best-of-N baselines. Our work establishes granular, reward-guided
sampling as a generalizable, robust, and efficient alternative to traditional
fine-tuning and full-sequence ranking methods for aligning LLMs.

</details>


### [6] [Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification](https://arxiv.org/abs/2511.03830)
*Mikołaj Langner,Jan Eliasz,Ewa Rudnicka,Jan Kocoń*

Main category: cs.CL

> A scalable and accurate multi-label text classification method using large language models, demonstrating significant improvements over zero-shot baselines, especially effective on seen dimensions during training.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to develop an efficient method for multi-label text classification that maintains high accuracy while being scalable, focusing on affective text analysis covering 24 dimensions.

**Method:** We introduce a method for efficient multi-label text classification using large language models (LLMs) by reformulating classification tasks into a series of yes/no decisions, querying each target dimension independently, and using a prefix caching mechanism to improve efficiency without losing accuracy.

**Result:** The fine-tuned models show significant improvements over zero-shot baselines, especially on seen dimensions during training.

**Conclusion:** Our approach of decomposing multi-label classification into dichotomic queries, combined with distillation and cache-aware inference, offers a scalable and effective framework for LLM-based classification, which is generalizable across domains.

**Abstract:** We introduce a method for efficient multi-label text classification with
large language models (LLMs), built on reformulating classification tasks as
sequences of dichotomic (yes/no) decisions. Instead of generating all labels in
a single structured response, each target dimension is queried independently,
which, combined with a prefix caching mechanism, yields substantial efficiency
gains for short-text inference without loss of accuracy. To demonstrate the
approach, we focus on affective text analysis, covering 24 dimensions including
emotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator
model (DeepSeek-V3) provides multiple annotations per text, which are
aggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,
Gemma3-1B). The fine-tuned models show significant improvements over zero-shot
baselines, particularly on the dimensions seen during training. Our findings
suggest that decomposing multi-label classification into dichotomic queries,
combined with distillation and cache-aware inference, offers a scalable and
effective framework for LLM-based classification. While we validate the method
on affective states, the approach is general and applicable across domains.

</details>


### [7] [Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered Lens](https://arxiv.org/abs/2511.03880)
*Hellina Hailu Nigatu,Bethelhem Yemane Mamo,Bontu Fufa Balcha,Debora Taye Tesfaye,Elbethel Daniel Zewdie,Ikram Behiru Nesiru,Jitu Ewnetu Hailu,Senait Mengesha Yayo*

Main category: cs.CL

> 研究对三种低资源语言的机器翻译数据集进行了性别表示和质量的评估，发现数据集中存在男性偏向及对女性有害的内容。

<details>
  <summary>Details</summary>

**Motivation:** 探讨低资源语言机器翻译数据集中性别表示和数据质量的问题，旨在提高对低资源语言数据集潜在问题的认识。

**Method:** 分析了Afan Oromo, Amharic和Tigrinya这三种语言的训练数据和基准数据，重点在于性别表示。

**Result:** 发现训练数据主要集中在政治和宗教领域，而基准数据集集中于新闻、健康和体育。数据集显示了对男性的显著偏向，并且有对女性的有害描述。

**Conclusion:** 研究成果强调数量的增长并非质量的保证，鼓励进一步研究低资源语言的数据集，并采取早期措施避免有害内容。

**Abstract:** As low-resourced languages are increasingly incorporated into NLP research,
there is an emphasis on collecting large-scale datasets. But in prioritizing
quantity over quality, we risk 1) building language technologies that perform
poorly for these languages and 2) producing harmful content that perpetuates
societal biases. In this paper, we investigate the quality of Machine
Translation (MT) datasets for three low-resourced languages--Afan Oromo,
Amharic, and Tigrinya, with a focus on the gender representation in the
datasets. Our findings demonstrate that while training data has a large
representation of political and religious domain text, benchmark datasets are
focused on news, health, and sports. We also found a large skew towards the
male gender--in names of persons, the grammatical gender of verbs, and in
stereotypical depictions in the datasets. Further, we found harmful and toxic
depictions against women, which were more prominent for the language with the
largest amount of data, underscoring that quantity does not guarantee quality.
We hope that our work inspires further inquiry into the datasets collected for
low-resourced languages and prompts early mitigation of harmful content.
WARNING: This paper contains discussion of NSFW content that some may find
disturbing.

</details>


### [8] [GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation](https://arxiv.org/abs/2511.03900)
*Manh Nguyen,Sunil Gupta,Dai Do,Hung Le*

Main category: cs.CL

> Introduces GRAD, a method to mitigate hallucinations in large language models by constructing and utilizing a token transition graph during the decoding phase.

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of hallucination in large language models (LLMs) without the fragility and costs associated with existing methods such as prompt-based grounding and symbolic knowledge integration.

**Method:** Graph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds generation in corpus-derived evidence without retraining. GRAD constructs a sparse token transition graph by accumulating next-token logits across a small retrieved corpus in a single forward pass.

**Result:** Across different language models and question-answering benchmarks, GRAD consistently surpasses baselines, showing significant improvements in accuracy, reduction in hallucination rates, and factual correctness.

**Conclusion:** GRAD provides an effective, lightweight solution to mitigate hallucinations in LLMs, leveraging statistical evidence from corpus-level transitions to improve truthfulness and verifiability of model outputs.

**Abstract:** Hallucination mitigation remains a persistent challenge for large language
models (LLMs), even as model scales grow. Existing approaches often rely on
external knowledge sources, such as structured databases or knowledge graphs,
accessed through prompting or retrieval. However, prompt-based grounding is
fragile and domain-sensitive, while symbolic knowledge integration incurs heavy
retrieval and formatting costs. Motivated by knowledge graphs, we introduce
Graph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds
generation in corpus-derived evidence without retraining. GRAD constructs a
sparse token transition graph by accumulating next-token logits across a small
retrieved corpus in a single forward pass. During decoding, graph-retrieved
logits are max-normalized and adaptively fused with model logits to favor
high-evidence continuations while preserving fluency. Across three models and a
range of question-answering benchmarks spanning intrinsic, extrinsic
hallucination, and factuality tasks, GRAD consistently surpasses baselines,
achieving up to 9.7$\%$ higher intrinsic accuracy, 8.6$\%$ lower hallucination
rates, and 6.9$\%$ greater correctness compared to greedy decoding, while
attaining the highest truth--informativeness product score among all methods.
GRAD offers a lightweight, plug-and-play alternative to contrastive decoding
and knowledge graph augmentation, demonstrating that statistical evidence from
corpus-level token transitions can effectively steer generation toward more
truthful and verifiable outputs.

</details>


### [9] [Context informs pragmatic interpretation in vision-language models](https://arxiv.org/abs/2511.03908)
*Alvin Wei Ming Tan,Ben Prystawski,Veronica Boyce,Michael C. Frank*

Main category: cs.CL

> 研究显示视觉语言模型在迭代参照游戏中，若无适当上下文，其表现不理想，但适当上下文提供了显著的性能提升，尽管对于抽象参照物的几轮游戏依然困难重重。

<details>
  <summary>Details</summary>

**Motivation:** 此研究旨在评估语言模型处理多轮参照任务的上下文敏感性和语用推理能力，试图揭示当前模型在理解复杂语言环境方面的能力边界。

**Method:** 本研究通过迭代参照游戏测试人类和视觉语言模型在多轮语言环境中进行语用推理的能力。游戏过程中，玩家反复使用语言挑选新的参照物。研究中改变了给定上下文的数量、顺序和相关性来测试参与者的性能。

**Result:** 在没有相关上下文的情况下，模型的表现略高于随机水平，但显著低于人类。然而，当提供了相关上下文，模型的表现有了显著的提高。然而，对于使用抽象参照物的几轮参照游戏，机器学习模型仍然面临着较大的挑战。

**Conclusion:** 虽然视觉语言模型在提供相关上下文的情况下表现有所提升，但与人类相比仍存在差距，特别是在处理抽象参照物的几轮参照游戏中表现不佳，表明在上下文理解方面存在改进空间。

**Abstract:** Iterated reference games - in which players repeatedly pick out novel
referents using language - present a test case for agents' ability to perform
context-sensitive pragmatic reasoning in multi-turn linguistic environments. We
tested humans and vision-language models on trials from iterated reference
games, varying the given context in terms of amount, order, and relevance.
Without relevant context, models were above chance but substantially worse than
humans. However, with relevant context, model performance increased
dramatically over trials. Few-shot reference games with abstract referents
remain a difficult task for machine learning models.

</details>


### [10] [The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013--2023](https://arxiv.org/abs/2511.03915)
*Stefano M. Iacus,Devika Jain,Andrea Nasuto,Giuseppe Porro,Marcello Carammia,Andrea Vezzulli*

Main category: cs.CL

> 该研究使用人类繁荣地理指数(HFGI)通过分析带有地理位置信息的推文来研究人类繁荣，并提供了社会福利、不平等和社会变化的多学科分析。

<details>
  <summary>Details</summary>

**Motivation:** 现有的衡量标准在空间和时间分辨率上往往不足。为了更好地理解超越经济指标的社会福利，该研究量化了包括幸福、健康、目的、美德、关系和财务稳定在内的多维度的人类繁荣。

**Method:** 该研究通过分析2013年至2023年间大约26亿条带有地理位置信息的美国推文，利用微调后的大型语言模型分类出与哈佛全球繁荣研究框架对齐的48项指标，加上关于移民态度和腐败感知的评估，来进行人类繁荣地理指数(HFGI)的研究。

**Result:** 该数据集提供了以月度和年度为单位的县和州级别的繁荣相关话语指标，并且验证了这些度量准确地反映了这些潜在变量，并显示出与已建立的指标预期的相关性。

**Conclusion:** 这项资源以前所未有的分辨率提供了美国过去十年社会媒体话语中人类繁荣动态的深厚见解。

**Abstract:** Quantifying human flourishing, a multidimensional construct including
happiness, health, purpose, virtue, relationships, and financial stability, is
critical for understanding societal well-being beyond economic indicators.
Existing measures often lack fine spatial and temporal resolution. Here we
introduce the Human Flourishing Geographic Index (HFGI), derived from analyzing
approximately 2.6 billion geolocated U.S. tweets (2013-2023) using fine-tuned
large language models to classify expressions across 48 indicators aligned with
Harvard's Global Flourishing Study framework plus attitudes towards migration
and perception of corruption. The dataset offers monthly and yearly county- and
state-level indicators of flourishing-related discourse, validated to confirm
that the measures accurately represent the underlying constructs and show
expected correlations with established indicators. This resource enables
multidisciplinary analyses of well-being, inequality, and social change at
unprecedented resolution, offering insights into the dynamics of human
flourishing as reflected in social media discourse across the United States
over the past decade.

</details>


### [11] [Direct Semantic Communication Between Large Language Models via Vector Translation](https://arxiv.org/abs/2511.03945)
*Fu-Chun Yang,Jason Eshraghian*

Main category: cs.CL

> 通过矢量翻译形成潜在桥梁，使得Llama-2-7B和Mistral-7B-Instruct之间的表示空间可以进行直接的语义交换，证明跨模型潜在通信的可行性。

<details>
  <summary>Details</summary>

**Motivation:** 在基于多智能体设置的情况下，大型语言模型（LLMs）以纯标记方式传递信息，这限制了信息传输并增加了不必要的计算开销。

**Method:** 通过矢量转换形成的潜在桥梁，使用学习到的映射，使表示空间之间的语义直接交换成为可能。在Llama-2-7B和Mistral-7B-Instruct模型之间训练了一个双编码器翻译器，平均余弦对齐度为0.538。以30%的融合强度注入翻译后的矢量，可以指导目标模型的生成，而不会使对数概率发生动荡。双向评估显示了2.01:1的传输不对称性，表明通用模型比指令调优变体模型更具可转移的表示形式。

**Result:** 保守注入保持了计算稳定性，同时证明了跨模型潜在通信的可行性，使得能够共享语义的协作AI系统成为可能。

**Conclusion:** 跨模型潜在通信是可行的，这使得能够共享语义的协作AI系统成为可能。

**Abstract:** In multi-agent settings, such as debate, reflection, or tool-calling, large
language models (LLMs) pass messages as plain tokens, discarding most latent
semantics. This constrains information transfer and adds unnecessary
computational overhead. We form a latent bridge via vector translations, which
use learned mappings that enable direct semantic exchange between
representation spaces. A dual-encoder translator trained between Llama-2-7B and
Mistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the
translated vectors at 30 percent blending strength steers the target model's
generation without destabilizing logits. Bidirectional evaluation shows a
2.01:1 transfer asymmetry, indicating that general-purpose models yield more
transferable representations than instruction-tuned variants. This conservative
injection preserves computational stability while demonstrating that
cross-model latent communication is feasible, enabling collaborative AI systems
that share meaning rather than tokens.

</details>


### [12] [Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises](https://arxiv.org/abs/2511.04020)
*Shiyin Lin*

Main category: cs.CL

> 此论文提出了一种框架，将演绎推理整合到检索增强的大型语言模型（RAG）中，以提高知识密集型任务的性能，在面对证据不完整时表现尤其出色。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在通过引入演绎推理来解决现有检索增强生成模型(RAG)在证据不完整时推理能力受限的问题。

**Method:** 本研究提出的方法包括检测不足的证据、生成候选的缺失前提，并通过一致性和合理性检查来验证这些候选前提。

**Result:** 在演绎推理和多跳问答基准测试上的实验结果表明，该方法不仅提高了答案的准确性也增强了推理的可信度。

**Conclusion:** 该研究强调演绎推理是增强RAG系统鲁棒性和可解释性的有前景的方向。

**Abstract:** Large Language Models (LLMs) enhanced with retrieval -- commonly referred to
as Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance
in knowledge-intensive tasks. However, RAG pipelines often fail when retrieved
evidence is incomplete, leaving gaps in the reasoning process. In such cases,
\emph{abductive inference} -- the process of generating plausible missing
premises to explain observations -- offers a principled approach to bridge
these gaps. In this paper, we propose a framework that integrates abductive
inference into retrieval-augmented LLMs. Our method detects insufficient
evidence, generates candidate missing premises, and validates them through
consistency and plausibility checks. Experimental results on abductive
reasoning and multi-hop QA benchmarks show that our approach improves both
answer accuracy and reasoning faithfulness. This work highlights abductive
inference as a promising direction for enhancing the robustness and
explainability of RAG systems.

</details>


### [13] [WST: Weakly Supervised Transducer for Automatic Speech Recognition](https://arxiv.org/abs/2511.04035)
*Dongji Gao,Chenda Liao,Changliang Liu,Matthew Wiesner,Leibny Paola Garcia,Daniel Povey,Sanjeev Khudanpur,Jian Wu*

Main category: cs.CL

> 本文提出Weakly Supervised Transducer（WST），它能更鲁棒地处理错误的脚本，且无需额外的置信度估计，即使在高错误率转录的情况下也能表现良好。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于减轻RNN-T对大规模、高质量标注数据的依赖，这些数据通常成本高昂且难以获取。

**Method:** 我们提出了一种名为弱监督转导器（WST）的方法，该方法集成了一个灵活的训练图，可以鲁棒地处理错误的脚本，而无需额外的置信度估计或辅助预训练模型。

**Result:** 实证评估显示，WST能够有效维持性能，即使转录错误率达到70%，它也始终优于现有的基于连接时序分类（CTC）的弱监督方法，如绕道时间分类（BTC）和全时序分类（OTC）。

**Conclusion:** 这些结果展示了WST在实际语音识别设置中的实用性和鲁棒性，并将公开实现的代码。

**Abstract:** The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in
end-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily
on large-scale, high-quality annotated data, which are often costly and
difficult to obtain. To mitigate this reliance, we propose a Weakly Supervised
Transducer (WST), which integrates a flexible training graph designed to
robustly handle errors in the transcripts without requiring additional
confidence estimation or auxiliary pre-trained models. Empirical evaluations on
synthetic and industrial datasets reveal that WST effectively maintains
performance even with transcription error rates of up to 70%, consistently
outperforming existing Connectionist Temporal Classification (CTC)-based weakly
supervised approaches, such as Bypass Temporal Classification (BTC) and
Omni-Temporal Classification (OTC). These results demonstrate the practical
utility and robustness of WST in realistic ASR settings. The implementation
will be publicly available.

</details>


### [14] [T-FIX: Text-Based Explanations with Features Interpretable to eXperts](https://arxiv.org/abs/2511.04070)
*Shreya Havaldar,Helen Jin,Chaehyeon Kim,Anton Xue,Weiqiu You,Marco Gatti,Bhuvnesh Jain,Helen Qu,Daniel A Hashimoto,Amin Madani,Rajat Deo,Sameed Ahmed M. Khatana,Gary E. Weissman,Lyle Ungar,Eric Wong*

Main category: cs.CL

> 本研究开发了名为T-FIX的新基准和度量标准，评估大语言模型生成的解释是否符合知识密集型领域中的专家判断，强调了在专业应用中专家对齐度的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 在诸如外科手术、天文学和心理治疗等知识密集型领域中部署大语言模型（LLM）时，用户不仅需要答案，还希望得到能够体现专家级推理的有意义解释。然而，当前的评估方案主要侧重于解释的表面合理性或内在一致性，未能捕捉到解释内容是否真正符合专家直觉。

**Method:** 本研究提出T-FIX作为基准，涵盖七个知识密集型领域，旨在衡量大语言模型（LLM）生成的解释与专家直觉的对齐程度。通过与领域专家合作，开发了新的度量标准来评估LLM解释是否符合专家判断。

**Result:** 开发了新的评估标准来衡量大语言模型生成的解释与专家判断的一致性，但没有具体结果展示。

**Conclusion:** 通过引入T-FIX基准和开发新的度量方法，本研究提供了一种更为准确的方式来评估大语言模型生成解释的专家对齐度，从而在知识密集型应用中提升用户信任度。

**Abstract:** As LLMs are deployed in knowledge-intensive settings (e.g., surgery,
astronomy, therapy), users expect not just answers, but also meaningful
explanations for those answers. In these settings, users are often domain
experts (e.g., doctors, astrophysicists, psychologists) who require
explanations that reflect expert-level reasoning. However, current evaluation
schemes primarily emphasize plausibility or internal faithfulness of the
explanation, which fail to capture whether the content of the explanation truly
aligns with expert intuition. We formalize expert alignment as a criterion for
evaluating explanations with T-FIX, a benchmark spanning seven
knowledge-intensive domains. In collaboration with domain experts, we develop
novel metrics to measure the alignment of LLM explanations with expert
judgment.

</details>


### [15] [Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04072)
*Xinying Qian,Ying Zhang,Yu Zhao,Baohang Zhou,Xuhui Sui,Xiaojie Yuan*

Main category: cs.CL

> 提出了一个结合结构化规划与对比式时间知识检索的框架PoK，显著提高了基于大语言模型的时间知识图谱问答系统在时间推理方面的精确性和事实一致性，相较于现有方法最高提升了56.0%的性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有方法在理解时间约束的复杂语义信息方面存在不足，同时大语言模型虽然具有强大的语义理解和推理泛化能力，但在时间推理方面的能力有限，且容易出现幻觉和知识缺乏等问题，因此提出PoK框架解决这些问题。

**Method:** PoK框架包含一个计划知识模块和一个时间知识存储。计划知识模块通过预定义的工具将复杂的时序问题分解为一系列子目标，提供中间推理指导；时间知识存储部分使用对比式检索框架从时序知识图谱中选择性地检索语义和时间对齐的事实。

**Result:** 在四个基准时间图谱问答数据集上的实验表明，PoK显著提高了大语言模型的时间检索精度和推理准确性，相对于最先进的时序知识图谱问答方法，性能最多提高了56.0%。

**Conclusion:** PoK框架通过结合结构化规划和时间知识检索，有效提升了时序推理的解释性和事实一致性。

**Abstract:** Temporal Knowledge Graph Question Answering (TKGQA) aims to answer
time-sensitive questions by leveraging factual information from Temporal
Knowledge Graphs (TKGs). While previous studies have employed pre-trained TKG
embeddings or graph neural networks to inject temporal knowledge, they fail to
fully understand the complex semantic information of time constraints.
Recently, Large Language Models (LLMs) have shown remarkable progress,
benefiting from their strong semantic understanding and reasoning
generalization capabilities. However, their temporal reasoning ability remains
limited. LLMs frequently suffer from hallucination and a lack of knowledge. To
address these limitations, we propose the Plan of Knowledge framework with a
contrastive temporal retriever, which is named PoK. Specifically, the proposed
Plan of Knowledge module decomposes a complex temporal question into a sequence
of sub-objectives from the pre-defined tools, serving as intermediate guidance
for reasoning exploration. In parallel, we construct a Temporal Knowledge Store
(TKS) with a contrastive retrieval framework, enabling the model to selectively
retrieve semantically and temporally aligned facts from TKGs. By combining
structured planning with temporal knowledge retrieval, PoK effectively enhances
the interpretability and factual consistency of temporal reasoning. Extensive
experiments on four benchmark TKGQA datasets demonstrate that PoK significantly
improves the retrieval precision and reasoning accuracy of LLMs, surpassing the
performance of the state-of-the-art TKGQA methods by 56.0% at most.

</details>


### [16] [The truth is no diaper: Human and AI-generated associations to emotional words](https://arxiv.org/abs/2511.04077)
*Špela Vintar,Jan Jona Javoršek*

Main category: cs.CL

> 研究比较了人类和大型语言模型对情感词语的联想反应，发现两者之间有一定的相似性，但大型语言模型生成的联想更可预测，创造性较低。

<details>
  <summary>Details</summary>

**Motivation:** 研究人类与大型语言模型的联想行为差异，特别是在处理带有情感色彩的词汇时的反应，以了解其中的异同。

**Method:** 通过比较人类与大型语言模型对带有情感词汇的关联反应来进行研究，以确定大型语言模型是否与人类产生类似的关联。

**Result:** 研究发现，人类与大型语言模型之间的关联重叠度中等，但大型语言模型倾向于放大刺激词的情感负荷，并且它们的关联反应更可预测、创造性较低。

**Conclusion:** 大型语言模型与人类在处理情感词汇的关联上存在某些相似性，但显示出不同的特征。

**Abstract:** Human word associations are a well-known method of gaining insight into the
internal mental lexicon, but the responses spontaneously offered by human
participants to word cues are not always predictable as they may be influenced
by personal experience, emotions or individual cognitive styles. The ability to
form associative links between seemingly unrelated concepts can be the driving
mechanisms of creativity. We perform a comparison of the associative behaviour
of humans compared to large language models. More specifically, we explore
associations to emotionally loaded words and try to determine whether large
language models generate associations in a similar way to humans. We find that
the overlap between humans and LLMs is moderate, but also that the associations
of LLMs tend to amplify the underlying emotional load of the stimulus, and that
they tend to be more predictable and less creative than human ones.

</details>


### [17] [Improving the Performance of Radiology Report De-identification with Large-Scale Training and Benchmarking Against Cloud Vendor Methods](https://arxiv.org/abs/2511.04079)
*Eva Prakash,Maayane Attias,Pierre Chambon,Justin Xu,Steven Truong,Jean-Benoit Delbrouck,Tessa Cook,Curtis Langlotz*

Main category: cs.CL

> 研究通过大规模训练数据优化了基于transformer的模型，用于放射学报告的自动化去标识处理，并在基准测试中优于商业云厂商的系统。

<details>
  <summary>Details</summary>

**Motivation:** 提升放射学报告自动化去标识技术，通过扩展训练数据集来优化基于transformer的模型，并对比商业云服务提供商的性能以确定保护健康信息（PHI）检测的准确性。

**Method:** 本研究在基于transformer的医疗信息去标识流水线的基础上进行微调，利用斯坦福大学的两个大型标注放射学语料库进行训练并增加了一个新的PHI类别（年龄）。同时，使用“隐藏在明处”的方法评估合成PHI生成的稳定性。

**Result:** 研究模型在宾夕法尼亚大学测试集上的整体F1得分为0.973，在斯坦福大学测试集上的整体F1得分为0.996，优于或保持了之前最先进的模型的表现。在50个独立去标识化的宾夕法尼亚大学的数据集上，合成PHI的检测一致性保持在高水准（0.959）。研究模型优于所有厂商系统。

**Conclusion:** 大规模、多模态的训练增强了跨机构的推广性和鲁棒性，同时生成的合成PHI保持了数据的实用性并保障了隐私保护。基于transformer的去标识模型在PHI检测上设立了新基准。

**Abstract:** Objective: To enhance automated de-identification of radiology reports by
scaling transformer-based models through extensive training datasets and
benchmarking performance against commercial cloud vendor systems for protected
health information (PHI) detection. Materials and Methods: In this
retrospective study, we built upon a state-of-the-art, transformer-based, PHI
de-identification pipeline by fine-tuning on two large annotated radiology
corpora from Stanford University, encompassing chest X-ray, chest CT,
abdomen/pelvis CT, and brain MR reports and introducing an additional PHI
category (AGE) into the architecture. Model performance was evaluated on test
sets from Stanford and the University of Pennsylvania (Penn) for token-level
PHI detection. We further assessed (1) the stability of synthetic PHI
generation using a "hide-in-plain-sight" method and (2) performance against
commercial systems. Precision, recall, and F1 scores were computed across all
PHI categories. Results: Our model achieved overall F1 scores of 0.973 on the
Penn dataset and 0.996 on the Stanford dataset, outperforming or maintaining
the previous state-of-the-art model performance. Synthetic PHI evaluation
showed consistent detectability (overall F1: 0.959 [0.958-0.960]) across 50
independently de-identified Penn datasets. Our model outperformed all vendor
systems on synthetic Penn reports (overall F1: 0.960 vs. 0.632-0.754).
Discussion: Large-scale, multimodal training improved cross-institutional
generalization and robustness. Synthetic PHI generation preserved data utility
while ensuring privacy. Conclusion: A transformer-based de-identification model
trained on diverse radiology datasets outperforms prior academic and commercial
systems in PHI detection and establishes a new benchmark for secure clinical
text processing.

</details>


### [18] [A Characterization of List Language Identification in the Limit](https://arxiv.org/abs/2511.04103)
*Moses Charikar,Chirag Pabbaraju,Ambuj Tewari*

Main category: cs.CL

> 本文研究在学习者具备提出k个猜测能力的情况下，探讨语言识别问题。基于对Angluin结果的递归扩展，精确刻画了哪些语言集合在极限条件下可被k列表识别，并确立了统计设置中的识别速率。

<details>
  <summary>Details</summary>

**Motivation:** 受近期关于语言生成正面结果的启发，本研究重新探讨了经典语言识别问题，特别是给予学习者提出k个猜测的能力，以试图解决Gold早期提出的经典结论，即对于任何有趣的语言集合，语言在极限时的识别是不可能的。

**Method:** 研究方法是通过引入允许学习者在每个时间步骤提出k个猜测的能力来重新审视经典的语言识别问题。这个方法基于Angluin对语言识别（列表大小为1）的特征化，提出了一个递归版本，以精确刻画哪些语言集合能够以k列表方式识别。此外，还探讨了统计设置中的识别速率问题，即输入是从集合中某个语言上的分布中以i.i.d.方式抽取。

**Result:** 结果表明，一个语言集合如果能以k列表方式识别，在统计设置中，它也可以以指数级速率进行k列表识别，并且这是最佳的。反之，如果一个集合不能以k列表方式识别，那么它在任何趋向于零的速率下都无法被k列表识别。

**Conclusion:** 研究得出的结论是，一个语言集合可以以k列表方式在极限条件下识别出来，当且仅当这个集合可以分解为k个子集，每个子集都可以单独在极限条件下识别（列表大小为1）。

**Abstract:** We study the problem of language identification in the limit, where given a
sequence of examples from a target language, the goal of the learner is to
output a sequence of guesses for the target language such that all the guesses
beyond some finite time are correct. Classical results of Gold showed that
language identification in the limit is impossible for essentially any
interesting collection of languages. Later, Angluin gave a precise
characterization of language collections for which this task is possible.
Motivated by recent positive results for the related problem of language
generation, we revisit the classic language identification problem in the
setting where the learner is given the additional power of producing a list of
$k$ guesses at each time step. The goal is to ensure that beyond some finite
time, one of the guesses is correct at each time step.
  We give an exact characterization of collections of languages that can be
$k$-list identified in the limit, based on a recursive version of Angluin's
characterization (for language identification with a list of size $1$). This
further leads to a conceptually appealing characterization: A language
collection can be $k$-list identified in the limit if and only if the
collection can be decomposed into $k$ collections of languages, each of which
can be identified in the limit (with a list of size $1$). We also use our
characterization to establish rates for list identification in the statistical
setting where the input is drawn as an i.i.d. stream from a distribution
supported on some language in the collection. Our results show that if a
collection is $k$-list identifiable in the limit, then the collection can be
$k$-list identified at an exponential rate, and this is best possible. On the
other hand, if a collection is not $k$-list identifiable in the limit, then it
cannot be $k$-list identified at any rate that goes to zero.

</details>


### [19] [Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models](https://arxiv.org/abs/2511.04108)
*Wenmo Qiu,Saurabh Srivastava*

Main category: cs.CL

> 批处理不仅能摊销大型语言模型的推理成本，还能作为一种正则化技术改善推理行为，提高准确性和效率。

<details>
  <summary>Details</summary>

**Motivation:** 论文探讨批处理作为一种策略，不仅可以摊销大型语言模型中的推理成本，还可以作为一种正则化手段来改善多步骤推理行为，提高效率和可靠性。

**Method:** 通过跨13个不同基准的综合研究，探讨了批处理对大型语言模型推理准确性的影响，并通过详细的分析研究了批处理如何抑制过度思考，减少模棱两可的语言，并鼓励更果断的回答。

**Result:** 研究发现批处理可以提高模型的准确性，同时显著减少推理令牌的使用量。此外，批处理显示出新兴的集体效应，模型可以将先前例子中的模式泛化以解决同一批次中的更难问题。

**Conclusion:** 批处理对于大型推理模型不仅是一个吞吐量优化措施，也是提高推理效率和可靠性的重要方法。

**Abstract:** Recent work has explored batch prompting as a strategy to amortize inference
cost in large language models (LLMs). In this paper, we show that batching
offers an additional, underappreciated benefit: it regularizes model behavior
during multi-step reasoning for Large Reasoning Models (LRMs). We conduct a
comprehensive study across 13 diverse benchmarks and observe that batching
improves accuracy while substantially reducing reasoning token usage, often by
3x-5x. Through detailed behavioral analysis, we find that batching suppresses
overthinking, reduces hedging language (e.g., repetitive self-corrections), and
encourages more decisive answers. Surprisingly, we also observe emergent
collective effects in batched inference: models often generalize patterns from
earlier examples to solve harder ones in the same batch. These findings
position batching not just as a throughput optimization, but as a powerful
inference-time regularizer for more efficient and reliable LLM reasoning.

</details>


### [20] [RIDE: Difficulty Evolving Perturbation with Item Response Theory for Mathematical Reasoning](https://arxiv.org/abs/2511.04120)
*Xinyuan Li,Murong Xu,Wenbiao Tao,Hanlun Zhu,Yike Zhao,Jipeng Zhang,Yunshi Lan*

Main category: cs.CL

> 提出一种新的对抗性问题重写框架RIDE来严格评估和生成更具有挑战性的数学问题，以此揭示大语言模型在数学推理上的局限。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于规则的变形方法经常生成不合理的问题，并阻碍了问题难度的系统评估和基准的演化，需要一种新的对抗性评估方法来衡量真正的数学推理能力。

**Method:** 提出了RIDE框架，该框架使用项目反应理论（IRT）来严格衡量问题难度，并生成更具有挑战性且合理的问题变体。利用35个大语言模型模拟学生，构建了难度评估器，该评估器提供了强化学习过程中的奖励信号，并引导问题重构模型重新设计不同难度等级的问题变体。

**Result:** 将RIDE应用于竞赛级数学基准测试，生成的变形版本使高级大语言模型的表现下降约21.73%，证明了在数学推理上的脆弱性，并确定了评估方法的有效性。

**Conclusion:** 实验结果表明，大语言模型在数学推理上存在局限性，特别是在面对经过RIDE框架变形的数学问题时。这显示了RIDE的有效性和评估方法的合理性。

**Abstract:** Large language models (LLMs) achieve high performance on mathematical
reasoning, but these results can be inflated by training data leakage or
superficial pattern matching rather than genuine reasoning. To this end, an
adversarial perturbation-based evaluation is needed to measure true
mathematical reasoning ability. Current rule-based perturbation methods often
generate ill-posed questions and impede the systematic evaluation of question
difficulty and the evolution of benchmarks. To bridge this gap, we propose
RIDE, a novel adversarial question-rewriting framework that leverages Item
Response Theory (IRT) to rigorously measure question difficulty and to generate
intrinsically more challenging, well-posed variations of mathematical problems.
We employ 35 LLMs to simulate students and build a difficulty ranker from their
responses. This ranker provides a reward signal during reinforcement learning
and guides a question-rewriting model to reformulate existing questions across
difficulty levels. Applying RIDE to competition-level mathematical benchmarks
yields perturbed versions that degrade advanced LLM performance, with
experiments showing an average 21.73% drop across 26 models, thereby exposing
limited robustness in mathematical reasoning and confirming the validity of our
evaluation approach.

</details>


### [21] [CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese](https://arxiv.org/abs/2511.04139)
*Dazhong Chen,Yi-Cheng Lin,Yuchen Huang,Ziwei Gong,Di Jiang,Zeying Xie,Yi R.,Fung*

Main category: cs.CL

> 本文提出了CantoASR方法，利用强制对齐和改进的Whisper与Qwen-Audio相结合，显著提高了低资源粤语的语音识别精度。

<details>
  <summary>Details</summary>

**Motivation:** 项目动机在于解决低资源粤语的自动语音识别问题，特别是涉及有限的标注数据、六个声调、声调变化和发音差异。现有模型如Whisper常常表现出较高的词错误率。

**Method:** 该研究引入了CantoASR框架，该框架整合了强制对齐的声学特征提取技术，对Whisper进行了LoRA微调以改进语气识别，并使用了经过指示调整的Qwen-Audio进行韵律感知校正。

**Result:** 研究提出了CantoASR框架，结合了强制对齐的声学特征提取、Whisper的LoRA微调和Qwen-Audio的指示调整，提高了语气和韵律的识别准确率。在粤语口语数据上的测试显示，CantoASR相较于Whisper-Large-V3有显著的字符错误率提升。这表明结合声学线索与大型语言模型推理为低资源声调和方言的语音识别提供了一个可扩展的策略。

**Conclusion:** 研究表明，在融合声学线索与大型语言模型推理的基础上，这项策略为低资源声调和方言的语音识别问题提供了一种可扩展解决方法。

**Abstract:** Automatic speech recognition (ASR) is critical for language accessibility,
yet low-resource Cantonese remains challenging due to limited annotated data,
six lexical tones, tone sandhi, and accent variation. Existing ASR models, such
as Whisper, often suffer from high word error rates. Large audio-language
models (LALMs), in contrast, can leverage broader contextual reasoning but
still require explicit tonal and prosodic acoustic cues. We introduce CantoASR,
a collaborative ASR-LALM error correction framework that integrates forced
alignment for acoustic feature extraction, a LoRA-finetuned Whisper for
improved tone discrimination, and an instruction-tuned Qwen-Audio for
prosody-aware correction. Evaluations on spontaneous Cantonese data show
substantial CER gains over Whisper-Large-V3. These findings suggest that
integrating acoustic cues with LALM reasoning provides a scalable strategy for
low-resource tonal and dialectal ASR.

</details>


### [22] [BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation](https://arxiv.org/abs/2511.04153)
*Fahim Ahmed,Md Mubtasim Ahasan,Jahir Sadik Monon,Muntasir Wahed,M Ashraful Amin,A K M Mahbubur Rahman,Amin Ahsan Ali*

Main category: cs.CL

> 本文通过比较和实验三种多智能体LLM管道，证明了多智能体讨论和特别的LLM推理者-编码者管道可以显著改善小型模型生成SQL查询的性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于大型语言模型在处理自然语言生成SQL查询方面存在较大挑战，尤其是面对大型模式大小和复杂推理时，本文研究了如何通过多智能体管道改进小模型的性能。

**Method:** 本文探索了三种多智能体LLM管道，包括多智能体讨论管道、规划者-编码者管道和编码者-聚合者管道。多智能体讨论管道涉及智能体迭代地批评和细化SQL查询，由一个法官综合最终答案。规划者-编码者管道由一个思考模型规划者生成逐步SQL生成计划，编码者综合查询。编码者-聚合者管道涉及多个编码者独立生成SQL查询，一个推理智能体选择最佳查询。

**Result:** 实验表明，多智能体讨论可以提高小型模型的表现，使用Qwen2.5-7b-Instruct在三次讨论后执行准确率提高了10.6%。在所有管道中，LLM推理者-编码者管道表现最好，通过DeepSeek-R1-32B和QwQ-32B规划者将Gemma 3 27B IT的准确率从52.4%提升至56.4%。

**Conclusion:** 研究表明，多智能体方法，尤其是规划者-编码者管道，能够有效提升小模型生成SQL查询的准确性和性能。

**Abstract:** Text-to-SQL systems provide a natural language interface that can enable even
laymen to access information stored in databases. However, existing Large
Language Models (LLM) struggle with SQL generation from natural instructions
due to large schema sizes and complex reasoning. Prior work often focuses on
complex, somewhat impractical pipelines using flagship models, while smaller,
efficient models remain overlooked. In this work, we explore three multi-agent
LLM pipelines, with systematic performance benchmarking across a range of small
to large open-source models: (1) Multi-agent discussion pipeline, where agents
iteratively critique and refine SQL queries, and a judge synthesizes the final
answer; (2) Planner-Coder pipeline, where a thinking model planner generates
stepwise SQL generation plans and a coder synthesizes queries; and (3)
Coder-Aggregator pipeline, where multiple coders independently generate SQL
queries, and a reasoning agent selects the best query. Experiments on the
Bird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small
model performance, with up to 10.6% increase in Execution Accuracy for
Qwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines,
the LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B
and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest
score of 56.4%. Codes are available at
https://github.com/treeDweller98/bappa-sql.

</details>


### [23] [Trustworthy LLM-Mediated Communication: Evaluating Information Fidelity in LLM as a Communicator (LAAC) Framework in Multiple Application Domains](https://arxiv.org/abs/2511.04184)
*Mohammed Musthafa Rafi,Adarsh Krishnamurthy,Aditya Balu*

Main category: cs.CL

> LAAC (LLM as a Communicator) aims to solve the problem of AI-generated content exaggeration and compression by becoming an intelligent intermediary, but its deployment raises concerns about information fidelity, consistency, and reliability, which the paper evaluates.

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the issue of AI-generated content leading to non-authentic communication and proposes LAAC to act as a trusted communication intermediary.

**Method:** The trustworthiness of LAAC is evaluated through three dimensions: information capture fidelity, reproducibility, and query response integrity, assessed through controlled experiments.

**Result:** Preliminary findings indicate measurable trust gaps, suggesting that LAAC's reliability needs improvement before it can be deployed in sensitive communication contexts.

**Conclusion:** The paper concludes that while LAAC shows promise in genuine communication facilitation, there are significant trustworthiness issues to resolve before it can be fully implemented.

**Abstract:** The proliferation of AI-generated content has created an absurd communication
theater where senders use LLMs to inflate simple ideas into verbose content,
recipients use LLMs to compress them back into summaries, and as a consequence
neither party engage with authentic content. LAAC (LLM as a Communicator)
proposes a paradigm shift - positioning LLMs as intelligent communication
intermediaries that capture the sender's intent through structured dialogue and
facilitate genuine knowledge exchange with recipients. Rather than perpetuating
cycles of AI-generated inflation and compression, LAAC enables authentic
communication across diverse contexts including academic papers, proposals,
professional emails, and cross-platform content generation. However, deploying
LLMs as trusted communication intermediaries raises critical questions about
information fidelity, consistency, and reliability. This position paper
systematically evaluates the trustworthiness requirements for LAAC's deployment
across multiple communication domains. We investigate three fundamental
dimensions: (1) Information Capture Fidelity - accuracy of intent extraction
during sender interviews across different communication types, (2)
Reproducibility - consistency of structured knowledge across multiple
interaction instances, and (3) Query Response Integrity - reliability of
recipient-facing responses without hallucination, source conflation, or
fabrication. Through controlled experiments spanning multiple LAAC use cases,
we assess these trust dimensions using LAAC's multi-agent architecture.
Preliminary findings reveal measurable trust gaps that must be addressed before
LAAC can be reliably deployed in high-stakes communication scenarios.

</details>


### [24] [Computational Turing Test Reveals Systematic Differences Between Human and AI Language](https://arxiv.org/abs/2511.04195)
*Nicolò Pagan,Petter Törnberg,Christopher A. Bail,Anikó Hannák,Christopher Barrie*

Main category: cs.CL

> 对大型语言模型（LLMs）在模拟人类语言和行为时的一系列验证和评估，发现LLMs在情感表达上与人类文本有所区别，微调并不总是提高类人性，且加强人类相似度会降低语义保真度。

<details>
  <summary>Details</summary>

**Motivation:** 现有的验证努力很大程度上依赖于基于人类判断的评估，这些判断被认为是粗糙和不可靠的。因此，领域缺乏评估LLM生成文本逼真度的工具或校准模型到现实数据的方法。

**Method:** 介绍了一种计算图灵测试：一种验证框架，整合了基于BERT的可检测性、语义相似性的集合指标与可解释的语言特征（如风格标记和主题模式），以评估LLMs在给定数据集中近似人类语言的程度。并且系统地比较了九个开源权重的大型语言模型在五种校准策略（包括微调、风格提示、以及上下文检索）下的表现，评估它们在X（原Twitter）、Bluesky和Reddit上模拟用户互动的能力。

**Result:** 即使经过校准，LLM输出依然在情感表达方面明显区别于人类文本。指令调优模型的表现不如其基础版本，增加模型大小并不会提高其类人性。研究表明，优化人类相似度往往以牺牲语义保真度为代价，反之亦然。

**Conclusion:** 这些结果提供了一种必要的可扩展验证和校准框架，用于LLM模拟，并提出了当前模型在捕捉人类交流方面的局限性的警告。

**Abstract:** Large language models (LLMs) are increasingly used in the social sciences to
simulate human behavior, based on the assumption that they can generate
realistic, human-like text. Yet this assumption remains largely untested.
Existing validation efforts rely heavily on human-judgment-based evaluations --
testing whether humans can distinguish AI from human output -- despite evidence
that such judgments are blunt and unreliable. As a result, the field lacks
robust tools for assessing the realism of LLM-generated text or for calibrating
models to real-world data. This paper makes two contributions. First, we
introduce a computational Turing test: a validation framework that integrates
aggregate metrics (BERT-based detectability and semantic similarity) with
interpretable linguistic features (stylistic markers and topical patterns) to
assess how closely LLMs approximate human language within a given dataset.
Second, we systematically compare nine open-weight LLMs across five calibration
strategies -- including fine-tuning, stylistic prompting, and context retrieval
-- benchmarking their ability to reproduce user interactions on X (formerly
Twitter), Bluesky, and Reddit. Our findings challenge core assumptions in the
literature. Even after calibration, LLM outputs remain clearly distinguishable
from human text, particularly in affective tone and emotional expression.
Instruction-tuned models underperform their base counterparts, and scaling up
model size does not enhance human-likeness. Crucially, we identify a trade-off:
optimizing for human-likeness often comes at the cost of semantic fidelity, and
vice versa. These results provide a much-needed scalable framework for
validation and calibration in LLM simulations -- and offer a cautionary note
about their current limitations in capturing human communication.

</details>


### [25] [LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal](https://arxiv.org/abs/2511.04205)
*Michał Karp,Anna Kubaszewska,Magdalena Król,Robert Król,Aleksander Smywiński-Pohl,Mateusz Szymański,Witold Wydmański*

Main category: cs.CL

> 研究表明，尽管技术进步迅速，但现有的大型语言模型（LLMs）在波兰公共采购裁决中还不能替代人类法官或独立考官。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于评估当前的大型语言模型（LLMs）是否能够通过波兰国家上诉庭（Krajowa Izba Odwoławcza）的官方资格考试。

**Method:** 该研究讨论了使用大型语言模型（LLMs）作为波兰国家上诉庭正式资格考试的实际考生，并探讨了“LLM作为法官”的方法，该方法中模型生成的答案将由其他模型自动评估。研究测试了包括GPT-4.1、Claude 4 Sonnet和Bielik-11B-v2.6在内的多个LLM在闭卷和多种检索增强生成设置下的表现。

**Result:** 研究发现，尽管模型在知识测试中取得了令人满意的成绩，但在实际文字部分均未达到及格线，且“LLM作为法官”的评估结果经常与官方审查委员会的判断不符。主要的限制因素包括生成不实信息、不正确的法律条款引用、逻辑论证不足，以及需要法律专家和技术团队间的紧密合作。

**Conclusion:** 该研究的结论是，因技术限制如不实信息生成、逻辑论证缺失等问题，当前的LLMs在波兰公共采购裁决中还不能替代人类法官或独立考官。

**Abstract:** This study provides an empirical assessment of whether current large language
models (LLMs) can pass the official qualifying examination for membership in
Poland's National Appeal Chamber (Krajowa Izba Odwo{\l}awcza). The authors
examine two related ideas: using LLM as actual exam candidates and applying the
'LLM-as-a-judge' approach, in which model-generated answers are automatically
evaluated by other models. The paper describes the structure of the exam, which
includes a multiple-choice knowledge test on public procurement law and a
written judgment, and presents the hybrid information recovery and extraction
pipeline built to support the models. Several LLMs (including GPT-4.1, Claude 4
Sonnet and Bielik-11B-v2.6) were tested in closed-book and various
Retrieval-Augmented Generation settings. The results show that although the
models achieved satisfactory scores in the knowledge test, none met the passing
threshold in the practical written part, and the evaluations of the
'LLM-as-a-judge' often diverged from the judgments of the official examining
committee. The authors highlight key limitations: susceptibility to
hallucinations, incorrect citation of legal provisions, weaknesses in logical
argumentation, and the need for close collaboration between legal experts and
technical teams. The findings indicate that, despite rapid technological
progress, current LLMs cannot yet replace human judges or independent examiners
in Polish public procurement adjudication.

</details>


### [26] [REMIND: Input Loss Landscapes Reveal Residual Memorization in Post-Unlearning LLMs](https://arxiv.org/abs/2511.04228)
*Liran Cohen,Yaniv Nemcovesky,Avi Mendelson*

Main category: cs.CL

> The authors of the paper introduce REMIND as a novel evaluation method for assessing machine unlearning, which evaluates models by analyzing the loss landscape around input variations, thereby detecting residual influences of unlearned data that traditional methods might miss.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitations of existing unlearning evaluation methods which assess forgetting at the level of individual inputs, potentially overlooking residual influence in semantically similar examples, which can compromise privacy.

**Method:** The paper proposes REMIND, an evaluation method to detect subtle remaining influence of unlearned data and classify if the data has been effectively forgotten by analyzing the model's loss over small input variations.

**Result:** REMIND is shown to outperform existing methods under similar constraints by delivering more sensitive and interpretable measures of unlearning effectiveness and demonstrating robustness across different models, datasets, and paraphrased inputs.

**Conclusion:** REMIND provides a reliable framework to assess unlearning in language models by offering a sensitive measure and a new perspective on memorization and unlearning.

**Abstract:** Machine unlearning aims to remove the influence of specific training data
from a model without requiring full retraining. This capability is crucial for
ensuring privacy, safety, and regulatory compliance. Therefore, verifying
whether a model has truly forgotten target data is essential for maintaining
reliability and trustworthiness. However, existing evaluation methods often
assess forgetting at the level of individual inputs. This approach may overlook
residual influence present in semantically similar examples. Such influence can
compromise privacy and lead to indirect information leakage. We propose REMIND
(Residual Memorization In Neighborhood Dynamics), a novel evaluation method
aiming to detect the subtle remaining influence of unlearned data and classify
whether the data has been effectively forgotten. REMIND analyzes the model's
loss over small input variations and reveals patterns unnoticed by single-point
evaluations. We show that unlearned data yield flatter, less steep loss
landscapes, while retained or unrelated data exhibit sharper, more volatile
patterns. REMIND requires only query-based access, outperforms existing methods
under similar constraints, and demonstrates robustness across different models,
datasets, and paraphrased inputs, making it practical for real-world
deployment. By providing a more sensitive and interpretable measure of
unlearning effectiveness, REMIND provides a reliable framework to assess
unlearning in language models. As a result, REMIND offers a novel perspective
on memorization and unlearning.

</details>


### [27] [Reusing Pre-Training Data at Test Time is a Compute Multiplier](https://arxiv.org/abs/2511.04234)
*Alex Fang,Thomas Voice,Ruoming Pang,Ludwig Schmidt,Tom Gunter*

Main category: cs.CL

> 研究表明当前的预训练方法并未充分利用现有的预训练数据集中的信息，还有很大的进步空间。

<details>
  <summary>Details</summary>

**Motivation:** 尽管研究人员努力改进这些数据集，但对于预训练装置从数据中提取想法和知识的效率的研究很少。

**Method:** 使用检索增强生成及测试时计算量来量化预训练过程从数据中提取想法和知识的效率，并探讨这一效率如何随规模变化。

**Result:** 实验结果显示，从标准和大多数开源数据集中进行预训练然后检索，可以在MMLU、Math-500和SimpleQA上获得显著的准确性提升，并且这些提升在去污染后仍然存在。

**Conclusion:** 结果表明，与仅使用预训练相比，检索行为在MMLU上相当于~5倍的计算增益。通过在测试时利用额外的计算量来解析检索到的上下文，可以进一步改善这些结果，例如对于公开的LLaMA 3.1 8B模型在MMLU上可以提高10个百分点。总体来说，预训练方法没有充分利用现有预训练数据集中的信息，这为未来研究留下了很大的提升空间。

**Abstract:** Large language models learn from their vast pre-training corpora, gaining the
ability to solve an ever increasing variety of tasks; yet although researchers
work to improve these datasets, there is little effort to understand how
efficient the pre-training apparatus is at extracting ideas and knowledge from
the data. In this work, we use retrieval augmented generation along with
test-time compute as a way to quantify how much dataset value was left behind
by the process of pre-training, and how this changes across scale. We
demonstrate that pre-training then retrieving from standard and largely
open-sourced datasets results in significant accuracy gains in MMLU, Math-500,
and SimpleQA, which persist through decontamination. For MMLU we observe that
retrieval acts as a ~5x compute multiplier versus pre-training alone. We show
that these results can be further improved by leveraging additional compute at
test time to parse the retrieved context, demonstrating a 10 percentage point
improvement on MMLU for the public LLaMA 3.1 8B model. Overall, our results
suggest that today's pre-training methods do not make full use of the
information in existing pre-training datasets, leaving significant room for
progress.

</details>


### [28] [Efficient Topic Extraction via Graph-Based Labeling: A Lightweight Alternative to Deep Models](https://arxiv.org/abs/2511.04248)
*Salma Mekaoui,Hiba Sofyan,Imane Amaaz,Imane Benchrif,Arsalane Zarghili,Ilham Chaker,Nikola S. Nikolov*

Main category: cs.CL

> The paper proposes a graph-based method for topic labeling that is computationally efficient and effective, surpassing traditional benchmarks and matching results with ChatGPT-3.5.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to offer an effective and less computationally demanding alternative to existing topic extraction methods. By proposing a graph-based approach, we aim to achieve better interpretability without relying on complex models.

**Method:** Our method involves using a graph-based approach to enrich and label topics derived from topic modeling. This method focuses on finding semantically related terms within topics and analyzing their relationships to derive meaningful labels.

**Result:** The proposed method achieved better results than traditional benchmarks as measured by BERTScore and cosine similarity. It also produced results comparable to those generated by ChatGPT-3.5, while being more computationally efficient.

**Conclusion:** The paper concludes that the proposed graph-based topic labeling method is effective for improving topic interpretability while remaining computationally efficient. It suggests future research directions for enhancing topic labeling interpretability and automation.

**Abstract:** Extracting topics from text has become an essential task, especially with the
rapid growth of unstructured textual data. Most existing works rely on highly
computational methods to address this challenge. In this paper, we argue that
probabilistic and statistical approaches, such as topic modeling (TM), can
offer effective alternatives that require fewer computational resources. TM is
a statistical method that automatically discovers topics in large collections
of unlabeled text; however, it produces topics as distributions of
representative words, which often lack clear interpretability. Our objective is
to perform topic labeling by assigning meaningful labels to these sets of
words. To achieve this without relying on computationally expensive models, we
propose a graph-based approach that not only enriches topic words with
semantically related terms but also explores the relationships among them. By
analyzing these connections within the graph, we derive suitable labels that
accurately capture each topic's meaning. We present a comparative study between
our proposed method and several benchmarks, including ChatGPT-3.5, across two
different datasets. Our method achieved consistently better results than
traditional benchmarks in terms of BERTScore and cosine similarity and produced
results comparable to ChatGPT-3.5, while remaining computationally efficient.
Finally, we discuss future directions for topic labeling and highlight
potential research avenues for enhancing interpretability and automation.

</details>


### [29] [SSPO: Subsentence-level Policy Optimization](https://arxiv.org/abs/2511.04256)
*Kun Yang,Zikang chen,Yanmeng Wang,Zhigen Li*

Main category: cs.CL

> This paper introduces SSPO, which applies sentence-level importance ratio to avoid the instability of GRPO and GSPO and achieves superior results on multiple datasets.

<details>
  <summary>Details</summary>

**Motivation:** The motivation of the paper is to address the issues of instability and inefficiency in using sampling data when training LLMs with RLVR algorithms such as GRPO and GSPO.

**Method:** The method SSPO uses sentence-level importance ratio to balance the drawbacks of GRPO and GSPO, and applies sentence entropy to PPO-CLIP for clipping bounds adjustment.

**Result:** SSPO achieves an average score of 46.57 across five datasets and surpasses both GRPO and GSPO, demonstrating its effectiveness.

**Conclusion:** SSPO effectively improves the stability and usage of sampling data in training LLMs, achieving state-of-the-art performance on multiple datasets.

**Abstract:** As a significant part of post-training of the Large Language Models (LLMs),
Reinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs'
reasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative
Policy Optimization) and GSPO (Group Sequence Policy Optimization), are
observed to suffer from unstable policy updates and low usage of sampling data,
respectively. The importance ratio of GRPO is calculated at the token level,
which focuses more on optimizing a single token. This will be easily affected
by outliers, leading to model training collapse. GSPO proposed the calculation
of the response level importance ratio, which solves the problem of high
variance and training noise accumulation in the calculation of the GRPO
importance ratio. However, since all the response tokens share a common
importance ratio, extreme values can easily raise or lower the overall mean,
leading to the entire response being mistakenly discarded, resulting in a
decrease in the utilization of sampled data. This paper introduces SSPO, which
applies sentence-level importance ratio, taking the balance between GRPO and
GSPO. SSPO not only avoids training collapse and high variance, but also
prevents the whole response tokens from being abandoned by the clipping
mechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily
adjust the clipping bounds, encouraging high-entropy tokens to explore and
narrow the clipping range of low-entropy tokens. In particular, SSPO achieves
an average score of 46.57 across five datasets, surpassing GRPO (43.01) and
GSPO (44.42), and wins state-of-the-art performance on three datasets. These
results highlight SSPO's effectiveness in leveraging generated data by taking
the essence of GSPO but rejecting its shortcomings.

</details>


### [30] [Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning](https://arxiv.org/abs/2511.04406)
*Mohammad Amin Ghanizadeh,Mohammad Javad Dousti*

Main category: cs.CL

> This paper proposes a data selection methodology that enhances the training effectiveness of machine translation models, leading to improved translation performance and computational efficiency.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the performance of machine translation models by effectively selecting high-quality training data, leading to more robust and accurate translation systems.

**Method:** Our method evaluates the utility of data points for training in machine translation models using a learnability score. It employs a batch selection strategy considering interdependencies among data points, aiming to enhance training efficiency and effectiveness.

**Result:** Experiments show that our method can improve data efficiency up to fivefold compared to a baseline method, and enhance computational efficiency by 24 by utilizing cached embeddings.

**Conclusion:** The approach not only achieves superior translation performance compared to random selection but also enhances the generalization of the model, making it a valuable method for fine-tuning machine translation systems.

**Abstract:** Data quality and its effective selection are fundamental to improving the
performance of machine translation models, serving as cornerstones for
achieving robust and reliable translation systems. This paper presents a data
selection methodology specifically designed for fine-tuning machine translation
systems, which leverages the synergy between a learner model and a pre-trained
reference model to enhance overall training effectiveness. By defining a
learnability score, our approach systematically evaluates the utility of data
points for training, ensuring that only the most relevant and impactful
examples contribute to the fine-tuning process. Furthermore, our method employs
a batch selection strategy which considers interdependencies among data points,
optimizing the efficiency of the training process while maintaining a focus on
data relevance. Experiments on English to Persian and several other language
pairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that
our method can achieve up to a fivefold improvement in data efficiency compared
to an iid baseline. Experimental results indicate that our approach improves
computational efficiency by 24 when utilizing cached embeddings, as it requires
fewer training data points. Additionally, it enhances generalization, resulting
in superior translation performance compared to random selection method.

</details>


### [31] [If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs](https://arxiv.org/abs/2511.04432)
*Lars Bungum,Charles Yijia Huang,Abeer Kashar*

Main category: cs.CL

> 本研究发现 LLM 使用英语答题比挪威语好，而使用更大规模的模型能提高答题结果。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在探索 LLM 在时序推理方面的潜力，尤其是在特定历史背景下的知识应用。

**Method:** 该研究使用了一本1940年的挪威 trivia 问题书籍作为数据源，让 LLM 以1940年的视角回答这些问题，并测试了英语和挪威语两种语言下的表现。评估结果通过另一种 LLM 进行评分，并由挪威语母语者进行抽样检查。

**Result:** 结果显示，使用英语提问比挪威语提问能得到更好的结果，这是一个意外发现。同时，使用更大规模的 LLM 能够改善结果。

**Conclusion:** 该研究结论强调了语言差异和模型大小对 LLM 在特定任务中性能的影响，展示了 LLM 在复杂推理任务上的潜力。

**Abstract:** In this study, we experiment with the ability of LLMs to do temporal
reasoning. Using a Norwegian book from 1940 containing trivia questions, we
prompt the LLMs to answer the questions as if it were 1940. We also pose the
questions in both English and Norwegian. Correct answers are often presented as
sentences, and grading is done by means of LLM-as-judge, with sampled checks by
a native speaker. Prompting in English consistently gave better results than in
Norwegian, an unexpected result. In contrast, using larger LLMs improved
results. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families,
and also the largest available LLM especially crafted for Norwegian.

</details>


### [32] [Probabilistic Textual Time Series Depression Detection](https://arxiv.org/abs/2511.04476)
*Fabian Schmidt,Seyedehmoniba Ravan,Vladimir Vlassov*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Accurate and interpretable predictions of depression severity are essential
for clinical decision support, yet existing models often lack uncertainty
estimates and temporal modeling. We propose PTTSD, a Probabilistic Textual Time
Series Depression Detection framework that predicts PHQ-8 scores from
utterance-level clinical interviews while modeling uncertainty over time. PTTSD
includes sequence-to-sequence and sequence-to-one variants, both combining
bidirectional LSTMs, self-attention, and residual connections with Gaussian or
Student-t output heads trained via negative log-likelihood. Evaluated on E-DAIC
and DAIC-WOZ, PTTSD achieves state-of-the-art performance among text-only
systems (e.g., MAE = 3.85 on E-DAIC, 3.55 on DAIC) and produces well-calibrated
prediction intervals. Ablations confirm the value of attention and
probabilistic modeling, while comparisons with MentalBERT establish generality.
A three-part calibration analysis and qualitative case studies further
highlight the interpretability and clinical relevance of uncertainty-aware
forecasting.

</details>


### [33] [ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai](https://arxiv.org/abs/2511.04479)
*Surapon Nonesung,Teetouch Jaknamon,Sirinya Chaiophat,Natapong Nitarach,Chanakan Wittayasakpan,Warit Sirichotedumrong,Adisai Na-Thalang,Kunat Pipatanakul*

Main category: cs.CL

> 开发了ThaiOCRBench，填补泰语文本密集型视觉理解任务评估的标准，指出专有模型在评估中优于开源系统，并确定了关键挑战及提供改进建议。

<details>
  <summary>Details</summary>

**Motivation:** 尽管多模态建模取得了进展，但现有的基准主要集中在高资源语言上，忽略了泰语等低资源语言，特别是在需要文档结构理解的任务方面。因此，我们构建了ThaiOCRBench来填补这一空白。

**Method:** 我们提出了ThaiOCRBench，这是第一个针对泰语文本密集型视觉理解任务评估视觉-语言模型（VLMs）的全面基准。此基准由2,808个样本组成，跨越了13个任务类别。

**Result:** 在零样本设置下，对广泛的最新的VLMs进行了评估，包括专有和开源系统。结果显示出显著的表现差异，专有模型（如Gemini 2.5 Pro）优于开源模型。特别是，细粒度文本识别和手写内容提取方面，开源模型的表现显著下降。

**Conclusion:** 通过对详细错误的分析，确定了关键挑战，如语言偏差、结构错配和内容生成的不准确性。ThaiOCRBench提供了一个标准化框架，用以评估VLMs在低资源、复杂脚本环境下的性能，并为改进泰文文档理解提供了实践见解。

**Abstract:** We present ThaiOCRBench, the first comprehensive benchmark for evaluating
vision-language models (VLMs) on Thai text-rich visual understanding tasks.
Despite recent progress in multimodal modeling, existing benchmarks
predominantly focus on high-resource languages, leaving Thai underrepresented,
especially in tasks requiring document structure understanding. ThaiOCRBench
addresses this gap by offering a diverse, human-annotated dataset comprising
2,808 samples across 13 task categories. We evaluate a wide range of
state-of-the-art VLMs in a zero-shot setting, spanning both proprietary and
open-source systems. Results show a significant performance gap, with
proprietary models (e.g., Gemini 2.5 Pro) outperforming open-source
counterparts. Notably, fine-grained text recognition and handwritten content
extraction exhibit the steepest performance drops among open-source models.
Through detailed error analysis, we identify key challenges such as language
bias, structural mismatch, and hallucinated content. ThaiOCRBench provides a
standardized framework for assessing VLMs in low-resource, script-complex
settings, and provides actionable insights for improving Thai-language document
understanding.

</details>


### [34] [RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables](https://arxiv.org/abs/2511.04491)
*Nikhil Abhyankar,Purvi Chaurasia,Sanchit Kabra,Ananya Srivastava,Vivek Gupta,Chandan K. Reddy*

Main category: cs.CL

> RUST-BENCH是一个大型真实世界表格推理基准，涵盖了科技与体育领域，展示了大语言模型在处理复杂表格推理任务时的不足。

<details>
  <summary>Details</summary>

**Motivation:** 现有的表格推理基准测试大多集中在小而统一的表格上，无法充分反映出大语言模型在处理现实数据时的推理能力。RUST-BENCH旨在填补这一空白。

**Method:** 提出RUST-BENCH基准测试，包含7966个问题和2031个真实世界的表格，涵盖科学和体育两个领域，用以评估大语言模型在处理异构、领域特定和复杂推理任务时的能力。

**Result:** 实验显示，现有模型在处理异构模式和复杂多步推理上表现欠佳，这揭示了当前架构的薄弱环节。

**Conclusion:** RUST-BENCH为推进表格推理研究提供了具有挑战性的新测试平台。

**Abstract:** Existing tabular reasoning benchmarks mostly test models on small, uniform
tables, underrepresenting the complexity of real-world data and giving an
incomplete view of Large Language Models' (LLMs) reasoning abilities. Real
tables are long, heterogeneous, and domain-specific, mixing structured fields
with free text and requiring multi-hop reasoning across thousands of tokens. To
address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from
2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)
and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates
LLMs jointly across scale, heterogeneity, domain specificity, and reasoning
complexity. Experiments with open-source and proprietary models show that LLMs
struggle with heterogeneous schemas and complex multi-hop inference, revealing
persistent weaknesses in current architectures and prompting strategies.
RUST-BENCH establishes a challenging new testbed for advancing tabular
reasoning research.

</details>


### [35] [OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation](https://arxiv.org/abs/2511.04495)
*Cuong Huynh,Jie Cao*

Main category: cs.CL

> 本文介绍了提交给TSAR-2025共享任务的OUNLP系统，该系统用于通过基于大型语言模型提示生成的简化文本来控制文本可读性。基于对提示基文本简化方法的分析发现，源文本与目标文本的CEFR等级差距与文本简化性能密切关联。因此，文章提出了两种多轮简化方法：基于规则的简化（MRS-Rule）和基于规则与语言模型联合的简化（MRS-Joint），并使用GPT-4o生成。其系统排名20支队伍中的第7位，并通过进一步改进MRS-Joint发现，以语言模型简洁化的候选作为起点能进一步提升多轮简化性能。

<details>
  <summary>Details</summary>

**Motivation:** 基于文本简化性能与CEFR等级之间差距的观察，研究动机在于探索改进文本简化的方法，特别是通过多轮次的简化过程。

**Method:** 提出了两种简化文本的方法：基于规则的简化（MRS-Rule）和基于规则与语言模型的联合简化（MRS-Joint）。这些方法使用GPT-4o生成简化后的文本。

**Result:** OUNLP系统在TSAR-2025共享任务中排名第7，使用MRS-Joint方法的后期改进表明，以语言模型生成的简化作为起点可进一步提升简化效果。

**Conclusion:** 研究得出文本简化性能高度依赖于源文本与目标文本间的CEFR等级差距的结论，并且基于语言模型的多轮简化方法展示了提升简化解性能的潜力。

**Abstract:** This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task
(Alva-Manchego et al., 2025), designed for readability-controlled text
simplification using LLM-prompting-based generation. Based on the analysis of
prompt-based text simplification methods, we discovered an interesting finding
that text simplification performance is highly related to the gap between the
source CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by
this finding, we propose two multi-round simplification methods and generate
them via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based
LLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams.
Later improvements with MRS-Joint show that taking the LLM simplified
candidates as the starting point could further boost the multi-round
simplification performance.

</details>


### [36] [Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering](https://arxiv.org/abs/2511.04499)
*Christos-Nikolaos Zacharopoulos,Revekka Kyriakoglou*

Main category: cs.CL

> 本论文使用BFI-2框架评估了六个大型语言模型的性格特质表现，发现了受温度调整影响的神经质和外向性，并提出模型架构特性可能导致模型倾向于稳定的特性模式。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型（LLMs）成为人类中心应用的组成部分，了解它们类似于个性的行为对于负责任的发展和部署越来越重要。

**Method:** 应用了Big Five Inventory-2 (BFI-2) 框架系统地评估了六个大型语言模型，在不同的采样温度下评估其性格特质的表现。

**Result:** 发现了五个性格维度中的四个存在显著差异，神经质和外向性受温度调整的影响。层次聚类显示了不同的模型聚类，表明架构特性可能导致某些模型倾向于稳定的特性模式。

**Conclusion:** 这些结果为理解大型语言模型中的性格模式提供了新的见解，并为模型调优、选择以及人工智能系统的道德治理提供了新的视角。

**Abstract:** As Large Language Models (LLMs) become integral to human-centered
applications, understanding their personality-like behaviors is increasingly
important for responsible development and deployment. This paper systematically
evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to
assess trait expressions under varying sampling temperatures. We find
significant differences across four of the five personality dimensions, with
Neuroticism and Extraversion susceptible to temperature adjustments. Further,
hierarchical clustering reveals distinct model clusters, suggesting that
architectural features may predispose certain models toward stable trait
profiles. Taken together, these results offer new insights into the emergence
of personality-like patterns in LLMs and provide a new perspective on model
tuning, selection, and the ethical governance of AI systems. We share the data
and code for this analysis here:
https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1

</details>


### [37] [RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific RAG](https://arxiv.org/abs/2511.04502)
*Joshua Gao,Quoc Huy Pham,Subin Varghese,Silwal Saurav,Vedhus Hoskere*

Main category: cs.CL

> 本文介绍了一种自动化、人类对齐的代理框架RAGalyst，用于专门领域中RAG系统的严格评估。

<details>
  <summary>Details</summary>

**Motivation:** 现有的评估框架往往依赖于无法捕捉领域特定细微差别的基于启发式的度量方法，或者使用未经验证与人类判断一致性的LLM作为裁判的方法。

**Method:** RAGalyst框架包括一个代理管道，从源文件生成高质量的合成问答数据集，并使用代理过滤步骤确保数据保真度。框架通过提示优化来增强两个关键的LLM-as-a-Judge指标，即答案正确性和问题可解答性，以达到与人类标注的高度相关性。

**Result:** 在军事行动、网络安全和桥梁工程三个领域中，对各种RAG组件进行评估后，发现性能受到高度情景依赖的影响，没有一个嵌入模型、LLM或超参数配置是普遍优化的。

**Conclusion:** 研究结果表明，性能高度依赖于具体的应用场景，没有单一的嵌入模型、LLM或超参数配置能够普遍适用。RAGalyst框架有助于从业者发现特定领域中的权衡，并做出更明智的设计决策，从而建立可靠的RAG系统。RAGalyst框架已开源。

**Abstract:** Retrieval-Augmented Generation (RAG) is a critical technique for grounding
Large Language Models (LLMs) in factual evidence, yet evaluating RAG systems in
specialized, safety-critical domains remains a significant challenge. Existing
evaluation frameworks often rely on heuristic-based metrics that fail to
capture domain-specific nuances and other works utilize LLM-as-a-Judge
approaches that lack validated alignment with human judgment. This paper
introduces RAGalyst, an automated, human-aligned agentic framework designed for
the rigorous evaluation of domain-specific RAG systems. RAGalyst features an
agentic pipeline that generates high-quality, synthetic question-answering (QA)
datasets from source documents, incorporating an agentic filtering step to
ensure data fidelity. The framework refines two key LLM-as-a-Judge
metrics-Answer Correctness and Answerability-using prompt optimization to
achieve a strong correlation with human annotations. Applying this framework to
evaluate various RAG components across three distinct domains (military
operations, cybersecurity, and bridge engineering), we find that performance is
highly context-dependent. No single embedding model, LLM, or hyperparameter
configuration proves universally optimal. Additionally, we provide an analysis
on the most common low Answer Correctness reasons in RAG. These findings
highlight the necessity of a systematic evaluation framework like RAGalyst,
which empowers practitioners to uncover domain-specific trade-offs and make
informed design choices for building reliable and effective RAG systems.
RAGalyst is available on our Github.

</details>


### [38] [Modeling Clinical Uncertainty in Radiology Reports: from Explicit Uncertainty Markers to Implicit Reasoning Pathways](https://arxiv.org/abs/2511.04506)
*Paloma Rabaey,Jong Hak Moon,Jung-Oh Lee,Min Gwan Kim,Hangyul Yoon,Thomas Demeester,Edward Choi*

Main category: cs.CL

> 研究提出了一种框架来处理放射学报告中的显式和隐式不确定性，将不确定性映射到概率，并加入子发现来丰富结构化的放射学报告。

<details>
  <summary>Details</summary>

**Motivation:** 对放射学报告中的不确定性的显式和隐式分类感兴趣，探讨了如何使用机器可读格式进行自动化分析。

**Method:** 采用了一个两部分框架来量化显式和隐式不确定性。通过专家验证的LLM基参考排名以及映射到概率值，量化了显式不确定性。通过系统地添加从专家定义的14种常见诊断路径中得出的子发现，建立了隐式不确定性模型。

**Result:** 发布了一个扩展的、能识别不确定性的Lunguage++，这是一个细粒度的结构化放射学报告基准的扩展版本。

**Conclusion:** 该资源支持识别不确定性的图像分类，可靠的诊断推理，以及诊断不确定性临床影响的新研究。

**Abstract:** Radiology reports are invaluable for clinical decision-making and hold great
potential for automated analysis when structured into machine-readable formats.
These reports often contain uncertainty, which we categorize into two distinct
types: (i) Explicit uncertainty reflects doubt about the presence or absence of
findings, conveyed through hedging phrases. These vary in meaning depending on
the context, making rule-based systems insufficient to quantify the level of
uncertainty for specific findings; (ii) Implicit uncertainty arises when
radiologists omit parts of their reasoning, recording only key findings or
diagnoses. Here, it is often unclear whether omitted findings are truly absent
or simply unmentioned for brevity. We address these challenges with a two-part
framework. We quantify explicit uncertainty by creating an expert-validated,
LLM-based reference ranking of common hedging phrases, and mapping each finding
to a probability value based on this reference. In addition, we model implicit
uncertainty through an expansion framework that systematically adds
characteristic sub-findings derived from expert-defined diagnostic pathways for
14 common diagnoses. Using these methods, we release Lunguage++, an expanded,
uncertainty-aware version of the Lunguage benchmark of fine-grained structured
radiology reports. This enriched resource enables uncertainty-aware image
classification, faithful diagnostic reasoning, and new investigations into the
clinical impact of diagnostic uncertainty.

</details>


### [39] [Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics](https://arxiv.org/abs/2511.04527)
*Amir Zur,Atticus Geiger,Ekdeep Singh Lubana,Eric Bigelow*

Main category: cs.CL

> 本文研究了语言模型在生成文本时如何表示其可能采取的其他路径，并发现模型的隐藏激活能用来控制和预测其不确定性，表明模型隐式地表示了可能路径的空间。

<details>
  <summary>Details</summary>

**Motivation:** 语言模型在生成文本时，由于词汇选择的不同，可能导致推理路径的极大差异，使得难以量化不确定性。作者希望探索语言模型是否代表了在生成过程中的其他可能路径。

**Method:** 本研究通过使用隐藏激活来控制和预测语言模型在连贯思维推理过程中的不确定性。

**Result:** 研究发现，模型在不同词汇的不确定性与其可被活动干预控制的程度有明显的相关性。此外，隐藏激活能够预测模型对未来结果分布的预测。

**Conclusion:** 模型在未完全确定最终答案之前，活动干预的效果最为显著，表明模型内在地表示了可能的路径空间。

**Abstract:** When a language model generates text, the selection of individual tokens
might lead it down very different reasoning paths, making uncertainty difficult
to quantify. In this work, we consider whether reasoning language models
represent the alternate paths that they could take during generation. To test
this hypothesis, we use hidden activations to control and predict a language
model's uncertainty during chain-of-thought reasoning. In our experiments, we
find a clear correlation between how uncertain a model is at different tokens,
and how easily the model can be steered by controlling its activations. This
suggests that activation interventions are most effective when there are
alternate paths available to the model -- in other words, when it has not yet
committed to a particular final answer. We also find that hidden activations
can predict a model's future outcome distribution, demonstrating that models
implicitly represent the space of possible paths.

</details>


### [40] [IntelliProof: An Argumentation Network-based Conversational Helper for Organized Reflection](https://arxiv.org/abs/2511.04528)
*Kaveh Eskandari Miandoab,Katharine Kowalyshyn,Kabir Pamnani,Anesu Gavhera,Vasanth Sarathy,Matthias Scheutz*

Main category: cs.CL

> IntelliProof是一种交互式分析论证性文章的系统，通过构建一个论证图来增强对文章连贯性的理解和评估，同时也为用户提供一套工具来自然语言理解文章和图表。

<details>
  <summary>Details</summary>

**Motivation:** IntelliProof的目标是提高用户体验，不仅自动评估文章，还提供详尽的结构化解释和对文章连贯性的定量度量，使用户能够快速探索论点质量并保持人工监督。

**Method:** IntelliProof通过将文章结构化为论证图来分析论证性文章，图中的主张作为节点，支持证据作为节点属性，边则表示支持或攻击关系。它利用大语言模型对每种关系进行初始分类和评分，并允许用户进行可视化理解和进一步调整。

**Result:** 该系统能够为论证性文章提供结构化分析，并通过提供用户友好的界面和工具来促进对文章质量的快速探索和理解。

**Conclusion:** IntelliProof为用户提供了更好地理解和评估论证性文章的新方法，通过结合大语言模型的自动评分和可视化工具增强用户理解力。

**Abstract:** We present IntelliProof, an interactive system for analyzing argumentative
essays through LLMs. IntelliProof structures an essay as an argumentation
graph, where claims are represented as nodes, supporting evidence is attached
as node properties, and edges encode supporting or attacking relations. Unlike
existing automated essay scoring systems, IntelliProof emphasizes the user
experience: each relation is initially classified and scored by an LLM, then
visualized for enhanced understanding. The system provides justifications for
classifications and produces quantitative measures for essay coherence. It
enables rapid exploration of argumentative quality while retaining human
oversight. In addition, IntelliProof provides a set of tools for a better
understanding of an argumentative essay and its corresponding graph in natural
language, bridging the gap between the structural semantics of argumentative
essays and the user's understanding of a given text. A live demo and the system
are available here to try: \textbf{https://intelliproof.vercel.app}

</details>


### [41] [From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting](https://arxiv.org/abs/2511.04538)
*Cyril Vallez,Alexander Sternfeld,Andrei Kucharavy,Ljiljana Dolamic*

Main category: cs.CL

> 文章指出即使最新的开放权重模型也会生成已知的网络安全漏洞，且针对这一问题提出了新的度量标准和评分系统来评估这些漏洞的严重性及其分布情况。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型(Large Language Models)在软件开发中扮演的角色越来越重要，它们生成代码中的漏洞对整体网络安全也日益突出。而研究目前提出的代码安全基准和改进方法对广泛使用的编码LLM模型的影响程度尚不明确，这成为本论文的动力源。

**Method:** 该论文通过分析最新的开放权重模型在实际使用场景下的漏洞生成情况，引入了一个名为Prompt Exposure (PE)的新度量标准，用于反映由LLM生成的漏洞风险。这个度量标准考虑了漏洞的严重性、生成概率以及激发生成漏洞代码的提示语句的形式。

**Result:** 该论文主要研究了大型语言模型在代码生成中固有的安全性问题，尤其是这些模型生成的漏洞在网络安全方面的潜在影响。研究发现，即使是最新的开放权重模型，在实际应用中也存在已被识别的漏洞场景。为了缓解此问题，提出了一个漏洞严重度的新度量标准，即Prompt Exposure (PE)，并进一步定义了Model Exposure (ME)分值以评估模型生成漏洞的严重性和普遍性。

**Conclusion:** 论文提出了一个新的模型漏洞暴露度评分(Model Exposure (ME))，表明了现有模型生成漏洞的严重性和普遍性，并指出这些模型的生成安全性和功能之间的权衡导致了漏洞的有效修补并未实现有效改进。

**Abstract:** As the role of Large Language Models (LLM)-based coding assistants in
software development becomes more critical, so does the role of the bugs they
generate in the overall cybersecurity landscape. While a number of LLM code
security benchmarks have been proposed alongside approaches to improve the
security of generated code, it remains unclear to what extent they have
impacted widely used coding LLMs. Here, we show that even the latest
open-weight models are vulnerable in the earliest reported vulnerability
scenarios in a realistic use setting, suggesting that the safety-functionality
trade-off has until now prevented effective patching of vulnerabilities. To
help address this issue, we introduce a new severity metric that reflects the
risk posed by an LLM-generated vulnerability, accounting for vulnerability
severity, generation chance, and the formulation of the prompt that induces
vulnerable code generation - Prompt Exposure (PE). To encourage the mitigation
of the most serious and prevalent vulnerabilities, we use PE to define the
Model Exposure (ME) score, which indicates the severity and prevalence of
vulnerabilities a model generates.

</details>


### [42] [BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation Strategies for Bangla Biomedical Question Answering](https://arxiv.org/abs/2511.04560)
*Sadia Sultana,Saiyma Sittul Muna,Mosammat Zannatul Samarukh,Ajwad Abrar,Tareque Mohmud Chowdhury*

Main category: cs.CL

> 本论文介绍了BanglaMedQA和BanglaMMedBench，这是首个为评估医学AI推理和检索能力而设计的大型孟加拉语生物医学多项选择题数据集，实验表明代理RAG策略在准确性和合理性方面表现最佳。

<details>
  <summary>Details</summary>

**Motivation:** 论文旨在解决在低资源语言中开发精确的生物医学问答系统的挑战，这限制了可靠医疗知识的公平获取。

**Method:** 该研究应用了几种检索增强生成（RAG）策略，包括传统方法、零样本回退、代理方法、迭代反馈和聚合RAG，目的在于通过结合基于教科书和网络检索与生成性推理来提高事实准确性。特别是，使用光学字符识别（OCR）将孟加拉语医学教科书整合进来，并实现了一种代理RAG流水线，能够动态选择检索与推理策略。

**Result:** 实验结果显示，代理RAG达到了最高的准确率89.54%，使用openai/gpt-oss-120b模型，优于其他配置，并显示出更好的合理性质量。

**Conclusion:** 这些发现强调了基于RAG的方法在提升孟加拉语医学问答系统的可靠性和可访问性方面的潜力，为未来在多语言医学人工智能研究奠定了基础。

**Abstract:** Developing accurate biomedical Question Answering (QA) systems in
low-resource languages remains a major challenge, limiting equitable access to
reliable medical knowledge. This paper introduces BanglaMedQA and
BanglaMMedBench, the first large-scale Bangla biomedical Multiple Choice
Question (MCQ) datasets designed to evaluate reasoning and retrieval in medical
artificial intelligence (AI). The study applies and benchmarks several
Retrieval-Augmented Generation (RAG) strategies, including Traditional,
Zero-Shot Fallback, Agentic, Iterative Feedback, and Aggregate RAG, combining
textbook-based and web retrieval with generative reasoning to improve factual
accuracy. A key novelty lies in integrating a Bangla medical textbook corpus
through Optical Character Recognition (OCR) and implementing an Agentic RAG
pipeline that dynamically selects between retrieval and reasoning strategies.
Experimental results show that the Agentic RAG achieved the highest accuracy
89.54% with openai/gpt-oss-120b, outperforming other configurations and
demonstrating superior rationale quality. These findings highlight the
potential of RAG-based methods to enhance the reliability and accessibility of
Bangla medical QA, establishing a foundation for future research in
multilingual medical artificial intelligence.

</details>


### [43] [When retrieval outperforms generation: Dense evidence retrieval for scalable fake news detection](https://arxiv.org/abs/2511.04643)
*Alamgir Munir Qazi,John P. McCrae,Jamal Abdul Nasir*

Main category: cs.CL

> DeReC凭借密集检索和专门分类，实现在事实验证任务上的高效和准确，优于现有基于LLM的方法。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有基于LLM的方法存在计算负担重和幻觉风险，研究提出了DeReC以实现高效且准确的事实验证。

**Method:** DeReC采用密集检索结合专门分类的方法，利用通用文本嵌入来代替自回归LLM方法进行事实验证。

**Result:** 相比解释生成LLM，DeReC在效率上提高95%和92%，在RAWFC上F1得分为65.58%，超越现有方法L-Defense的61.20%。

**Conclusion:** 研究表明，精心设计的检索系统在特定任务上可以匹敌甚至超越LLM的表现，并且更适合实际部署。

**Abstract:** The proliferation of misinformation necessitates robust yet computationally
efficient fact verification systems. While current state-of-the-art approaches
leverage Large Language Models (LLMs) for generating explanatory rationales,
these methods face significant computational barriers and hallucination risks
in real-world deployments. We present DeReC (Dense Retrieval Classification), a
lightweight framework that demonstrates how general-purpose text embeddings can
effectively replace autoregressive LLM-based approaches in fact verification
tasks. By combining dense retrieval with specialized classification, our system
achieves better accuracy while being significantly more efficient. DeReC
outperforms explanation-generating LLMs in efficiency, reducing runtime by 95%
on RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92%
on LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds),
showcasing its effectiveness across varying dataset sizes. On the RAWFC
dataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art
method L-Defense (61.20%). Our results demonstrate that carefully engineered
retrieval-based systems can match or exceed LLM performance in specialized
tasks while being significantly more practical for real-world deployment.

</details>


### [44] [Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.04654)
*Mohammad Atif Quamar,Mohammad Areeb*

Main category: cs.CL

> 本论文提出LEASH算法，它通过监控令牌生成过程中的两个内在信号来自适应地停止生成，从而减少资源消耗，提高推理链生成的效率。

<details>
  <summary>Details</summary>

**Motivation:** 自动生成完整且固定长度的推理链在计算上是浪费资源的，增加了令牌使用量和延迟。我们希望通过LEASH来解决这个问题，使得推理更加高效。

**Method:** 我们提出了LEASH：一种无训练的解码算法，它能够根据两个内在信号自适应地停止推理链的生成。这两个信号是：逐令牌熵的斜率和顶级令牌边界的改进。当这两个信号趋于平稳时，算法即结束生成，指示模型达到了稳定的推理状态。

**Result:** 在四个通过指令调整的模型上，LEASH在GSM8K和AQuA-RAT基准测试中的令牌生成平均减少了30-35%，延迟减少了27%，准确率相对CoT下降了10个百分点。

**Conclusion:** LEASH是一种无需额外训练或监督就能提高推理链生成效率的方法，为CoT解码提供了一个简单而高效的替代方案。

**Abstract:** Chain-of-Thought (CoT) prompting is a key technique for enabling complex
reasoning in large language models. However, generating full, fixed-length
rationales is computationally wasteful, inflating both token usage and latency.
We introduce LEASH: Logit-Entropy Adaptive Stopping Heuristic, a training-free
decoding algorithm that adaptively halts rationale generation. LEASH monitors
two intrinsic signals: the slope of token-level entropy and the improvement in
the top-logit margin. It terminates the generation once both signals plateau,
indicating the model has reached a stable reasoning state. Across four
instruction-tuned models on the GSM8K and AQuA-RAT benchmarks, LEASH reduces
average token generation by 30--35% and latency by 27%, while incurring a 10
p.p. accuracy drop relative to CoT. LEASH is model-agnostic and requires no
additional training or supervision, offering a simple and efficient alternative
to CoT decoding.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [45] [LoRA-Edge: Tensor-Train-Assisted LoRA for Practical CNN Fine-Tuning on Edge Devices](https://arxiv.org/abs/2511.03765)
*Hyunseok Kwak,Kyeongwon Lee,Jae-Jin Lee,Woojoo Lee*

Main category: cs.CV

> LoRA-Edge enables efficient on-device fine-tuning of CNNs, significantly reducing the number of trainable parameters and accelerating convergence, making it suitable for edge platforms.

<details>
  <summary>Details</summary>

**Motivation:** To address the critical need for on-device fine-tuning of CNNs to handle domain shift in edge applications like HAR, under strict resource constraints.

**Method:** LoRA-Edge, a parameter-efficient fine-tuning (PEFT) method based on Low-Rank Adaptation (LoRA) and tensor-train assistance, tailored for on-device fine-tuning of CNNs in edge applications.

**Result:** Achieves accuracy within 4.7% of full fine-tuning across diverse HAR datasets and CNN backbones while updating at most 1.49% of parameters, with 1.4-3.8x faster convergence compared to prior methods.

**Conclusion:** LoRA-Edge proves to be a practical method for on-device CNN adaptation, delivering efficiency in memory, compute, and energy usage.

**Abstract:** On-device fine-tuning of CNNs is essential to withstand domain shift in edge
applications such as Human Activity Recognition (HAR), yet full fine-tuning is
infeasible under strict memory, compute, and energy budgets. We present
LoRA-Edge, a parameter-efficient fine-tuning (PEFT) method that builds on
Low-Rank Adaptation (LoRA) with tensor-train assistance. LoRA-Edge (i) applies
Tensor-Train Singular Value Decomposition (TT-SVD) to pre-trained convolutional
layers, (ii) selectively updates only the output-side core with
zero-initialization to keep the auxiliary path inactive at the start, and (iii)
fuses the update back into dense kernels, leaving inference cost unchanged.
This design preserves convolutional structure and reduces the number of
trainable parameters by up to two orders of magnitude compared to full
fine-tuning. Across diverse HAR datasets and CNN backbones, LoRA-Edge achieves
accuracy within 4.7% of full fine-tuning while updating at most 1.49% of
parameters, consistently outperforming prior parameter-efficient baselines
under similar budgets. On a Jetson Orin Nano, TT-SVD initialization and
selective-core training yield 1.4-3.8x faster convergence to target F1.
LoRA-Edge thus makes structure-aligned, parameter-efficient on-device CNN
adaptation practical for edge platforms.

</details>


### [46] [SILVI: Simple Interface for Labeling Video Interactions](https://arxiv.org/abs/2511.03819)
*Ozan Kanbertay,Richard Vogg,Elif Karakoc,Peter M. Kappeler,Claudia Fichtel,Alexander S. Ecker*

Main category: cs.CV

> 本文提出了一个名为SILVI的开源标注软件，它可以同时进行行为标注和个体定位，有助于研究人员分析动物的社交和个体行为。

<details>
  <summary>Details</summary>

**Motivation:** 现有的开源标注工具要么只能进行行为标注而无法定位个体，要么只能进行个体定位而无法捕捉互动。本文旨在填补这一空白，支持对动物行为和社会互动的细致分析。

**Method:** 本文介绍了一种开源标注软件SILVI，它集成了行为标注和个体定位功能，允许研究人员直接在视频数据中注解行为和互动，生成适用于训练和验证计算机视觉模型的结构化输出。

**Result:** SILVI有助于连接行为生态学和计算机视觉，促进自动方法发展，以进行细腻的行为分析，不仅可以应用于动物行为，还可广泛应用于需要提取动态场景图的人际互动视频注解中。

**Conclusion:** SILVI软件及其文档和下载说明可以在提供的网址获取。

**Abstract:** Computer vision methods are increasingly used for the automated analysis of
large volumes of video data collected through camera traps, drones, or direct
observations of animals in the wild. While recent advances have focused
primarily on detecting individual actions, much less work has addressed the
detection and annotation of interactions -- a crucial aspect for understanding
social and individualized animal behavior. Existing open-source annotation
tools support either behavioral labeling without localization of individuals,
or localization without the capacity to capture interactions. To bridge this
gap, we present SILVI, an open-source labeling software that integrates both
functionalities. SILVI enables researchers to annotate behaviors and
interactions directly within video data, generating structured outputs suitable
for training and validating computer vision models. By linking behavioral
ecology with computer vision, SILVI facilitates the development of automated
approaches for fine-grained behavioral analyses. Although developed primarily
in the context of animal behavior, SILVI could be useful more broadly to
annotate human interactions in other videos that require extracting dynamic
scene graphs. The software, along with documentation and download instructions,
is available at: https://gitlab.gwdg.de/kanbertay/interaction-labelling-app.

</details>


### [47] [Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets](https://arxiv.org/abs/2511.03855)
*Duong Mai,Lawrence Hall*

Main category: cs.CV

> 研究展示通过训练时加入噪声注入可以显著减少模型在训练数据分布（ID）和未见数据分布（OOD）上的性能差距，从0.10-0.20降低至0.01-0.06。

<details>
  <summary>Details</summary>

**Motivation:** 深度学习模型在处理来自不同设备或人群的数据时通常表现不佳，特别是在使用胸部X光片（CXR）进行COVID-19检测时，模型往往利用特定源的不通用特征来提升训练集内的表现，而不是学习合理的生物标志物，导致无法很好地泛化到新的临床数据源。

**Method:** 通过在训练过程中引入基本的噪声注入技术（如高斯噪声、斑点噪声、泊松噪声和椒盐噪声）来增强模型对数据分布变化的鲁棒性。

**Result:** 实验结果表明，在ID数据和OOD数据上的评估指标（如AUC、F1、准确率、召回率和特异性等）的平均差距可以显著缩小。

**Conclusion:** 该研究表明，在模型训练过程中引入噪声可以增强其对新的临床数据源的鲁棒性，减少性能差异，提高模型在不同数据源上的可靠性。

**Abstract:** Deep learned (DL) models for image recognition have been shown to fail to
generalize to data from different devices, populations, etc. COVID-19 detection
from Chest X-rays (CXRs), in particular, has been shown to fail to generalize
to out-of-distribution (OOD) data from new clinical sources not covered in the
training set. This occurs because models learn to exploit shortcuts -
source-specific artifacts that do not translate to new distributions - rather
than reasonable biomarkers to maximize performance on in-distribution (ID)
data. Rendering the models more robust to distribution shifts, our study
investigates the use of fundamental noise injection techniques (Gaussian,
Speckle, Poisson, and Salt and Pepper) during training. Our empirical results
demonstrate that this technique can significantly reduce the performance gap
between ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results
averaged over ten random seeds across key metrics such as AUC, F1, accuracy,
recall and specificity. Our source code is publicly available at
https://github.com/Duongmai127/Noisy-ood

</details>


### [48] [Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures](https://arxiv.org/abs/2511.03882)
*Florence Klitzner,Blanca Inigo,Benjamin D. Killeen,Lalithkumar Seenivasan,Michelle Song,Axel Krieger,Mathias Unberath*

Main category: cs.CV

> 通过模拟训练模仿学习策略，用于X光引导下的脊柱手术中的套管插入，初步结果显示该策略在大多数情况下可以成功规划和控制，但也存在入口点精度方面的限制。

<details>
  <summary>Details</summary>

**Motivation:** 探索基于模仿学习的策略在X光引导下的脊柱手术中的应用机会与挑战。

**Method:** 创建X光引导下脊柱手术的模拟环境，收集正确轨迹和对应的双平面X光序列数据，训练模仿学习策略来进行规划和开放环控制。

**Result:** 策略在68.5%的情况下一次成功完成套管对准，能够在不同椎体级别中保持安全的轨迹。策略还对复杂解剖结构和不同的初始化条件具有鲁棒性。

**Conclusion:** 初步结果显示该模仿学习策略在X光引导的脊柱手术中具有应用潜力，但仍存在入口点精度的限制，需进一步改进。

**Abstract:** Imitation learning-based robot control policies are enjoying renewed interest
in video-based robotics. However, it remains unclear whether this approach
applies to X-ray-guided procedures, such as spine instrumentation. This is
because interpretation of multi-view X-rays is complex. We examine
opportunities and challenges for imitation policy learning in bi-plane-guided
cannula insertion. We develop an in silico sandbox for scalable, automated
simulation of X-ray-guided spine procedures with a high degree of realism. We
curate a dataset of correct trajectories and corresponding bi-planar X-ray
sequences that emulate the stepwise alignment of providers. We then train
imitation learning policies for planning and open-loop control that iteratively
align a cannula solely based on visual information. This precisely controlled
setup offers insights into limitations and capabilities of this method. Our
policy succeeded on the first attempt in 68.5% of cases, maintaining safe
intra-pedicular trajectories across diverse vertebral levels. The policy
generalized to complex anatomy, including fractures, and remained robust to
varied initializations. Rollouts on real bi-planar X-rays further suggest that
the model can produce plausible trajectories, despite training exclusively in
simulation. While these preliminary results are promising, we also identify
limitations, especially in entry point precision. Full closed-look control will
require additional considerations around how to provide sufficiently frequent
feedback. With more robust priors and domain knowledge, such models may provide
a foundation for future efforts toward lightweight and CT-free robotic
intra-operative spinal navigation.

</details>


### [49] [Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model](https://arxiv.org/abs/2511.03888)
*Abdulmumin Sa'ad,Sulaimon Oyeniyi Adebayo,Abdul Jabbar Siddiqui*

Main category: cs.CV

> 本文提出了一种优化的废物检测框架，通过轻量级YOLOv12结合自我对抗训练和数据增强策略，实现了沙漠环境下实时废物检测的高准确性，同时保持低延迟和紧凑的模型大小，适合无人机部署。

<details>
  <summary>Details</summary>

**Motivation:** 传统废物收集方法在偏远或恶劣环境地区（如下沙漠）中具有劳动密集、低效和危险特性。而计算机视觉和深度学习的最新进展为自动废物检测系统打开了大门。然而，大多数研究集中在城市环境和可回收材料上，忽略了有机和危险废物以及未被探索的土地，比如沙漠。本工作旨在填补这一空白。

**Method:** 我们提出了一种增强的实时目标检测框架，基于剪枝后的轻量级YOLOv12，并结合自我对抗训练（SAT）和专门的数据增强策略。这种方法旨在提高检测精度、召回率和平均精度均值（mAP），同时保持低延迟和紧凑的模型大小，适合资源受限的无人机使用。

**Result:** 实验中使用DroneTrashNet数据集，表明模型在精度、召回率和平均精度均值（mAP）方面有了显著提升。与现有的轻量级YOLO变体模型进行基准测试，也显示了模型在精度和效率方面的优越平衡。

**Conclusion:** 我们的研究表明，在沙漠环境中，结合数据和模型的改进可以实现稳健和实时的废物检测，验证了所提方法的有效性。

**Abstract:** The global waste crisis is escalating, with solid waste generation expected
to increase by 70% by 2050. Traditional waste collection methods, particularly
in remote or harsh environments like deserts, are labor-intensive, inefficient,
and often hazardous. Recent advances in computer vision and deep learning have
opened the door to automated waste detection systems, yet most research focuses
on urban environments and recyclable materials, overlooking organic and
hazardous waste and underexplored terrains such as deserts. In this work, we
propose an enhanced real-time object detection framework based on a pruned,
lightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT)
and specialized data augmentation strategies. Using the DroneTrashNet dataset,
we demonstrate significant improvements in precision, recall, and mean average
precision (mAP), while achieving low latency and compact model size suitable
for deployment on resource-constrained aerial drones. Benchmarking our model
against state-of-the-art lightweight YOLO variants further highlights its
optimal balance of accuracy and efficiency. Our results validate the
effectiveness of combining data-centric and model-centric enhancements for
robust, real-time waste detection in desert environments.

</details>


### [50] [Improving Diagnostic Performance on Small and Imbalanced Datasets Using Class-Based Input Image Composition](https://arxiv.org/abs/2511.03891)
*Hlali Azzeddine,Majid Ben Yakhlef,Soulaiman El Hazzat*

Main category: cs.CV

> 通过融合相同类别的多个图像来创建复合输入图像，该方法在处理小规模和类别不平衡的数据集时显著提升了诊断准确性。

<details>
  <summary>Details</summary>

**Motivation:** 解决小规模数据集和类别不平衡问题，降低深度学习模型的误判率。

**Method:** 通过Class-Based Image Composition技术，生成复合输入图像，增强类内方差和单位训练样本的信息密度。

**Result:** 在OCTDL数据集上，使用VGG16模型实验，新方法在准确率、F1值和AUC上分别达到99.6%、0.995和0.9996，显著低于误判率。

**Conclusion:** 该方法能显著提高在小规模和类别不平衡数据集上的诊断准确性。

**Abstract:** Small, imbalanced datasets and poor input image quality can lead to high
false predictions rates with deep learning models. This paper introduces
Class-Based Image Composition, an approach that allows us to reformulate
training inputs through a fusion of multiple images of the same class into
combined visual composites, named Composite Input Images (CoImg). That enhances
the intra-class variance and improves the valuable information density per
training sample and increases the ability of the model to distinguish between
subtle disease patterns. Our method was evaluated on the Optical Coherence
Tomography Dataset for Image-Based Deep Learning Methods (OCTDL) (Kulyabin et
al., 2024), which contains 2,064 high-resolution optical coherence tomography
(OCT) scans of the human retina, representing seven distinct diseases with a
significant class imbalance. We constructed a perfectly class-balanced version
of this dataset, named Co-OCTDL, where each scan is resented as a 3x1 layout
composite image. To assess the effectiveness of this new representation, we
conducted a comparative analysis between the original dataset and its variant
using a VGG16 model. A fair comparison was ensured by utilizing the identical
model architecture and hyperparameters for all experiments. The proposed
approach markedly improved diagnostic results.The enhanced Dataset achieved
near-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996), compared
to a baseline model trained on raw dataset. The false prediction rate was also
significantly lower, this demonstrates that the method can producehigh-quality
predictions even for weak datasets affected by class imbalance or small sample
size.

</details>


### [51] [I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging](https://arxiv.org/abs/2511.03912)
*Nand Kumar Yadav,Rodrigue Rizk,William CW Chen,KC Santosh*

Main category: cs.CV

> 本文介绍了一种无监督的方法，用来在医疗成像中检测未知异常，并且方法有效，尤其适用于数据标签稀缺的场景。

<details>
  <summary>Details</summary>

**Motivation:** 医疗影像中的未知异常检测由于标注异常的稀缺性和专家监督的成本高昂，仍然是一个基本的挑战。

**Method:** 本文提出了一种无监督且无需专家监督的框架，用于在医疗成像中检测未知异常。该框架从一个小的正常图像种子集开始，通过轻量级适配器更新和不确定性门控样本录取交替进行，逐步扩展可信的正常样本集。适配器与冻结的预训练视觉骨干网络结合使用，确保快速领域适应。通过k-NN评分来检测异常，并采用双概率门控机制确保安全的增量扩展。

**Result:** 该方法在三个数据集上取得了优于基线模型的结果，在COVID-CXR上的ROC-AUC从0.9489提升到0.9982，F1从0.8048提升到0.9746; 在肺炎CXR上的ROC-AUC从0.6834提升到0.8968; 在Brain MRI ND-5上的ROC-AUC从0.6041提升到0.7269，PR-AUC从0.7539提升到0.8211。

**Conclusion:** 此研究提出的方法无需任何异常标签即可促使正常样本集的增量扩展，有效地在现实中标签稀缺的医疗成像应用中提升了检测性能。

**Abstract:** Unknown anomaly detection in medical imaging remains a fundamental challenge
due to the scarcity of labeled anomalies and the high cost of expert
supervision. We introduce an unsupervised, oracle-free framework that
incrementally expands a trusted set of normal samples without any anomaly
labels. Starting from a small, verified seed of normal images, our method
alternates between lightweight adapter updates and uncertainty-gated sample
admission. A frozen pretrained vision backbone is augmented with tiny
convolutional adapters, ensuring rapid domain adaptation with negligible
computational overhead. Extracted embeddings are stored in a compact coreset
enabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during
incremental expansion is enforced by dual probabilistic gates, a sample is
admitted into the normal memory only if its distance to the existing coreset
lies within a calibrated z-score threshold, and its SWAG-based epistemic
uncertainty remains below a seed-calibrated bound. This mechanism prevents
drift and false inclusions without relying on generative reconstruction or
replay buffers. Empirically, our system steadily refines the notion of
normality as unlabeled data arrive, producing substantial gains over baselines.
On COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on
Pneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5,
ROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These
results highlight the effectiveness and efficiency of the proposed framework
for real-world, label-scarce medical imaging applications.

</details>


### [52] [Adaptive Temporal Refinement: Continuous Depth Allocation and Distance Regression for Efficient Action Localization](https://arxiv.org/abs/2511.03943)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.CV

> 研究提出了BDR与ATR两种方法，提升边界检测精度，同时优化计算资源分配，减少计算量并降低训练成本，适用于多种架构。

<details>
  <summary>Details</summary>

**Motivation:** 论文的动机在于现有的方法在处理边界定位时没有考虑到其在不同边界上的难度差异，而应用了统一的计算。这导致边界定位不够精确，因此提出了边界距离回归（BDR）和自适应时序细化（ATR）来提高边界检测的精确性和计算效率。

**Method:** 该论文提出了两种互补的贡献：首先，边界距离回归（BDR）通过有符号距离回归而不是分类来提供信息理论上最优的定位，实现边界峰值锐利度提高43%。BDR可以应用于现有的方法，仅需大约50行代码，从而在多种架构中实现1.8%到3.1%的mAP@0.7改进。其次，自适应时序细化（ATR）通过连续深度选择τ∈[0,1]分配计算，使得可以直接进行端到端可微优化而无需强化学习。

**Result:** 在THUMOS14数据集上，ATR达到了56.5%的mAP@0.7，尽管计算量减少了18%。对于短动作来说，改进效果更显著，达到4.2%的提升。通过知识蒸馏，训练成本得到了缓解，轻量级的学生模型在基线成本下可保持99%的性能。这些结果在四个基准上进行了严格的统计测试验证。

**Conclusion:** 研究展示了边界距离回归（BDR）和自适应时序细化（ATR）的有效性，它们不仅可以在多种架构上提高边界检测的精确度，还可以减少计算量，同时通过知识蒸馏技术降低了训练成本。证明了这两种方法在视频动作定位任务上的实用性和高效性。

**Abstract:** Temporal action localization requires precise boundary detection; however,
current methods apply uniform computation despite significant variations in
difficulty across boundaries. We present two complementary contributions.
First, Boundary Distance Regression (BDR) provides information-theoretically
optimal localization through signed-distance regression rather than
classification, achieving 43\% sharper boundary peaks. BDR retrofits to
existing methods with approximately 50 lines of code, yielding consistent 1.8
to 3.1\% mAP@0.7 improvements across diverse architectures. Second, Adaptive
Temporal Refinement (ATR) allocates computation via continuous depth selection
$\tau \in [0,1]$, enabling end-to-end differentiable optimization without
reinforcement learning. On THUMOS14, ATR achieves 56.5\% mAP@0.7 at 162G FLOPs,
compared to 53.6\% at 198G for uniform processing, providing a 2.9\%
improvement with 18\% less compute. Gains scale with boundary heterogeneity,
showing 4.2\% improvement on short actions. Training cost is mitigated via
knowledge distillation, with lightweight students retaining 99\% performance at
baseline cost. Results are validated across four benchmarks with rigorous
statistical testing.

</details>


### [53] [Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization](https://arxiv.org/abs/2511.03950)
*Zhejia Cai,Puhua Jiang,Shiwei Mao,Hongkun Cao,Ruqi Huang*

Main category: cs.CV

> The paper presents an unified framework for optimizing 3D mesh geometry and appearance together, which is significant for downstream editing tasks in 3D content creation.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to overcome the limitation of existing methods which often decouple geometry and appearance optimization, leading to less seamless 3D reconstructions that are not ideal for downstream editing tasks.

**Method:** This paper proposes a novel framework to jointly optimize mesh geometry and appearance using Gaussian-guided mesh differentiable rendering.

**Result:** High-quality 3D reconstructions are obtained, which can be beneficial for tasks like relighting and shape deformation.

**Conclusion:** Joint optimization of geometry and appearance leads to more seamless Gaussian-mesh reconstructions, enhancing the performance in downstream editing tasks.

**Abstract:** Reconstructing real-world objects from multi-view images is essential for
applications in 3D editing, AR/VR, and digital content creation. Existing
methods typically prioritize either geometric accuracy (Multi-View Stereo) or
photorealistic rendering (Novel View Synthesis), often decoupling geometry and
appearance optimization, which hinders downstream editing tasks. This paper
advocates an unified treatment on geometry and appearance optimization for
seamless Gaussian-mesh joint optimization. More specifically, we propose a
novel framework that simultaneously optimizes mesh geometry (vertex positions
and faces) and vertex colors via Gaussian-guided mesh differentiable rendering,
leveraging photometric consistency from input images and geometric
regularization from normal and depth maps. The obtained high-quality 3D
reconstruction can be further exploit in down-stream editing tasks, such as
relighting and shape deformation. The code will be publicly available upon
acceptance.

</details>


### [54] [A Linear Fractional Transformation Model and Calibration Method for Light Field Camera](https://arxiv.org/abs/2511.03962)
*Zhong Chen,Changfeng Chen*

Main category: cs.CV

> This paper introduces a novel calibration method using a linear fractional transformation parameter for light field cameras to improve 3D reconstruction accuracy.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the accuracy of the calibration method for internal parameters, which is essential but challenging for 3D reconstruction using light field cameras.

**Method:** The paper proposes a linear fractional transformation (LFT) parameter $\alpha$ to decouple the main lens and micro lens array (MLA) in light field cameras, with an analytical solution using least squares and nonlinear refinement.

**Result:** Experiments with both physical and simulated data have validated the effectiveness of the new calibration method and demonstrated its potential for enhancing simulation speeds of raw light field images.

**Conclusion:** The new calibration approach allows for more accurate calibration of light field cameras and accelerates the simulation of raw light field images, which is critical for data-driven deep learning methods.

**Abstract:** Accurate calibration of internal parameters is a crucial yet challenging
prerequisite for 3D reconstruction using light field cameras. In this paper, we
propose a linear fractional transformation(LFT) parameter $\alpha$ to decoupled
the main lens and micro lens array (MLA). The proposed method includes an
analytical solution based on least squares, followed by nonlinear refinement.
The method for detecting features from the raw images is also introduced.
Experimental results on both physical and simulated data have verified the
performance of proposed method. Based on proposed model, the simulation of raw
light field images becomes faster, which is crucial for data-driven deep
learning methods. The corresponding code can be obtained from the author's
website.

</details>


### [55] [Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images](https://arxiv.org/abs/2511.03970)
*Sam Bahrami,Dylan Campbell*

Main category: cs.CV

> 提出'Room Envelopes'合成数据集，包含RGB图像和两个点图，支持直接监督前馈单目几何估计器预测情景可见和布局表面，从而推进场景重建的研究。

<details>
  <summary>Details</summary>

**Motivation:** 现代场景重建方法虽然能准确恢复一个或多个图像中可见的3D表面，但会导致不完全重建，无法捕捉到所有被遮挡的表面。作者认为，由于场景中的元素如墙壁、地板和天花板通常是平面的、重复的且简单，因此这些场景元素应该相对容易预测，可以采用成本较低的方法。

**Method:** 通过创建一个名为'Room Envelopes'的合成数据集来推进这一任务，该数据集为每个RGB图像提供了两个关联的点图：一个捕获可见表面，另一个捕获移除装饰和装置后的第一个表面，即结构布局。这允许对前馈单目几何估计器进行直接监督，这些估计器可以预测第一个可见表面和第一个布局表面。

**Result:** 此项工作展示了一个合成数据集，它可以作为直接监督前馈单目几何估计器的基础，这些估计器能够预测第一个可见表面和第一个布局表面。这有助于理解场景的范围，以及其中对象的形状和位置。

**Conclusion:** 该数据集为预测场景的结构布局提供了直接监督，有助于理解场景的范围及其内对象的形状和位置，为不完全场景重建提供了一种新的研究和解决方法。

**Abstract:** Modern scene reconstruction methods are able to accurately recover 3D
surfaces that are visible in one or more images. However, this leads to
incomplete reconstructions, missing all occluded surfaces. While much progress
has been made on reconstructing entire objects given partial observations using
generative models, the structural elements of a scene, like the walls, floors
and ceilings, have received less attention. We argue that these scene elements
should be relatively easy to predict, since they are typically planar,
repetitive and simple, and so less costly approaches may be suitable. In this
work, we present a synthetic dataset -- Room Envelopes -- that facilitates
progress on this task by providing a set of RGB images and two associated
pointmaps for each image: one capturing the visible surface and one capturing
the first surface once fittings and fixtures are removed, that is, the
structural layout. As we show, this enables direct supervision for feed-forward
monocular geometry estimators that predict both the first visible surface and
the first layout surface. This confers an understanding of the scene's extent,
as well as the shape and location of its objects.

</details>


### [56] [Simple 3D Pose Features Support Human and Machine Social Scene Understanding](https://arxiv.org/abs/2511.03988)
*Wenshuo Qin,Leyla Isik*

Main category: cs.CV

> 研究发现人类在判断社交互动时依赖3D空间姿态信息，这在大多数AI视觉模型中缺失，提取3D关节位置的数据优于这些模型，简单的3D姿态特征能够显著改善现有视觉模型的表现。

<details>
  <summary>Details</summary>

**Motivation:** 理解人类如何通过视觉输入快速、轻松地从社交互动中提取信息的计算过程，并测试是否人类依赖于3D空间姿态信息来进行社交互动判断。

**Method:** 结合最先进的姿态和深度估计算法来提取短视频中人物的3D关节位置，并将它们预测人类社交互动判断的能力与当前AI视觉模型进行比较。

**Result:** 3D关节位置的表现优于大多数现有的AI视觉模型，表明关键的社会信息存在于显式的身体姿势中，而不是现有的学习到的特征。

**Conclusion:** 研究表明人类理解社交场景依靠显式的3D姿态表达并且可以通过简单的、结构化的视觉-空间基本特征来支持。

**Abstract:** Humans can quickly and effortlessly extract a variety of information about
others' social interactions from visual input, ranging from visuospatial cues
like whether two people are facing each other to higher-level information. Yet,
the computations supporting these abilities remain poorly understood, and
social interaction recognition continues to challenge even the most advanced AI
vision systems. Here, we hypothesized that humans rely on 3D visuospatial pose
information to make social interaction judgments, which is absent in most AI
vision models. To test this, we combined state-of-the-art pose and depth
estimation algorithms to extract 3D joint positions of people in short video
clips depicting everyday human actions and compared their ability to predict
human social interaction judgments with current AI vision models. Strikingly,
3D joint positions outperformed most current AI vision models, revealing that
key social information is available in explicit body position but not in the
learned features of most vision models, including even the layer-wise
embeddings of the pose models used to extract joint positions. To uncover the
critical pose features humans use to make social judgments, we derived a
compact set of 3D social pose features describing only the 3D position and
direction of faces in the videos. We found that these minimal descriptors
matched the predictive strength of the full set of 3D joints and significantly
improved the performance of off-the-shelf AI vision models when combined with
their embeddings. Moreover, the degree to which 3D social pose features were
represented in each off-the-shelf AI vision model predicted the model's ability
to match human social judgments. Together, our findings provide strong evidence
that human social scene understanding relies on explicit representations of 3D
pose and can be supported by simple, structured visuospatial primitives.

</details>


### [57] [CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation](https://arxiv.org/abs/2511.03992)
*Yuwen Tao,Kanglei Zhou,Xin Tan,Yuan Xie*

Main category: cs.CV

> Camera Aware Referring Field (CaRF) achieves multi-view consistency in 3D Gaussian splatting by incorporating camera geometry into Gaussian text interactions, aligning per Gaussian logits across views during training, and outperforms existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind the method is to address the limitations of existing cross-modal alignment techniques that suffer from cross-view consistency issues due to reliance on 2D rendered pseudo supervision and view-specific feature learning. The goal is to achieve more consistent and reliable 3D scene understanding.

**Method:** The method proposes Camera Aware Referring Field (CaRF), a fully differentiable framework that operates in 3D Gaussian space. It includes Gaussian Field Camera Encoding (GFCE), which integrates camera geometry into Gaussian text interactions to explicitly model view-dependent variations. Additionally, In Training Paired View Supervision (ITPVS) is introduced to align per Gaussian logits across calibrated views during training, aiming to mitigate single view overfitting and optimize for inter-view discrepancies.

**Result:** Experiments show that the proposed CaRF framework outperforms state-of-the-art methods on the Ref LERF, LERF OVS, and 3D OVS datasets with average mIoU improvements of 16.8%, 4.3%, and 2.0%, respectively.

**Conclusion:** The conclusion is that the framework, named CaRF, promotes a more reliable and view-consistent 3D scene understanding, with potential application benefits for embodied AI, AR/VR interaction, and autonomous perception.

**Abstract:** Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret
free-form language expressions and localize the corresponding 3D regions in
Gaussian fields. While recent advances have introduced cross-modal alignment
between language and 3D geometry, existing pipelines still struggle with
cross-view consistency due to their reliance on 2D rendered pseudo supervision
and view specific feature learning. In this work, we present Camera Aware
Referring Field (CaRF), a fully differentiable framework that operates directly
in the 3D Gaussian space and achieves multi view consistency. Specifically,
CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates
camera geometry into Gaussian text interactions to explicitly model view
dependent variations and enhance geometric reasoning. Building on this, In
Training Paired View Supervision (ITPVS) is proposed to align per Gaussian
logits across calibrated views during training, effectively mitigating single
view overfitting and exposing inter view discrepancies for optimization.
Extensive experiments on three representative benchmarks demonstrate that CaRF
achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of
the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively.
Moreover, this work promotes more reliable and view consistent 3D scene
understanding, with potential benefits for embodied AI, AR/VR interaction, and
autonomous perception.

</details>


### [58] [PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection](https://arxiv.org/abs/2511.03997)
*Peiyao Wang,Weining Wang,Qi Li*

Main category: cs.CV

> 本文提出了一种名为PhysCorr的框架，以提升视频生成中的物理一致性，通过PhysicsRM和PhyDPO的结合，改善了视频生成模型中的物理现实性，同时保持了视觉保真度和语义对齐。

<details>
  <summary>Details</summary>

**Motivation:** 现有的文本到视频生成技术在感知质量上取得了显著进步，但生成的内容往往违背了物理合理性的基本原则，如不合理的物体运动、不连贯的交互和不现实的运动模式。这些问题阻碍了视频生成模型在具身AI、机器人学和仿真领域中的应用。为了填补这一差距，提出了PhysCorr框架。

**Method:** 本文提出了PhysCorr框架，该框架用于建模、评估和优化视频生成中的物理一致性。具体来说，引入了PhysicsRM，这是第一个双重维度的奖励模型，它量化了单个对象的稳定性和对象间的相互作用。在此基础上，开发了PhyDPO，这是一种利用对比反馈和物理感知重权的新型直接偏好优化流水线，引导生成物理上合理的结果。

**Result:** 实验结果表明，PhysCorr能够在多个基准测试中显著提高物理真实感，同时保持视觉保真度和语义对齐。

**Conclusion:** 该工作朝着物理现实和可靠的视频生成迈出了关键的一步。

**Abstract:** Recent advances in text-to-video generation have achieved impressive
perceptual quality, yet generated content often violates fundamental principles
of physical plausibility - manifesting as implausible object dynamics,
incoherent interactions, and unrealistic motion patterns. Such failures hinder
the deployment of video generation models in embodied AI, robotics, and
simulation-intensive domains. To bridge this gap, we propose PhysCorr, a
unified framework for modeling, evaluating, and optimizing physical consistency
in video generation. Specifically, we introduce PhysicsRM, the first
dual-dimensional reward model that quantifies both intra-object stability and
inter-object interactions. On this foundation, we develop PhyDPO, a novel
direct preference optimization pipeline that leverages contrastive feedback and
physics-aware reweighting to guide generation toward physically coherent
outputs. Our approach is model-agnostic and scalable, enabling seamless
integration into a wide range of video diffusion and transformer-based
backbones. Extensive experiments across multiple benchmarks demonstrate that
PhysCorr achieves significant improvements in physical realism while preserving
visual fidelity and semantic alignment. This work takes a critical step toward
physically grounded and trustworthy video generation.

</details>


### [59] [GNN-MoE: Context-Aware Patch Routing using GNNs for Parameter-Efficient Domain Generalization](https://arxiv.org/abs/2511.04008)
*Mahmoud Soliman,Omar Abdelaziz,Ahmed Radwan,Anand,Mohamed Shehata*

Main category: cs.CV

> 本研究提出 GNN-MoE 方法，使用 GNN 在补丁间建立关系图，并动态分配补丁到专门化的专家模型中，这种方法在模拟不同领域适应性时表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于，当前直接微调预训练的 ViT 以适应未知领域是昂贵且可能损害一般化性能的。GNN-MoE 提出了一种更高效、参数更精简的调整方法，以解决这一问题。

**Method:** GNN-MoE 方法利用混合专家（MoE）框架，并采用高效的 Kronecker 适配器来增强预训练 Vision Transformer (ViT) 的调整效率。该方法创造性地使用图神经网络（GNN）路由器（如GCN, GAT, SAGE）在补丁间构建关系图，并根据这些关系动态分配补丁到相应的专家模型中。

**Result:** GNN-MoE 在域泛化基准测试中表现出色，达到了目前的行业领先水平或具有竞争力的性能，证明了图基背景下路由对于领域泛化问题的实用性和轻量级优势。

**Conclusion:** 结论是，通过采用基于图的上下文路由机制，GNN-MoE 为 Vision Transformer 提供了一种高效的领域泛化方法，并强调了此方法在参数效率高的同时，依然能够保持出色的模型性能。

**Abstract:** Domain generalization (DG) seeks robust Vision Transformer (ViT) performance
on unseen domains. Efficiently adapting pretrained ViTs for DG is challenging;
standard fine-tuning is costly and can impair generalization. We propose
GNN-MoE, enhancing Parameter-Efficient Fine-Tuning (PEFT) for DG with a
Mixture-of-Experts (MoE) framework using efficient Kronecker adapters. Instead
of token-based routing, a novel Graph Neural Network (GNN) router (GCN, GAT,
SAGE) operates on inter-patch graphs to dynamically assign patches to
specialized experts. This context-aware GNN routing leverages inter-patch
relationships for better adaptation to domain shifts. GNN-MoE achieves
state-of-the-art or competitive DG benchmark performance with high parameter
efficiency, highlighting the utility of graph-based contextual routing for
robust, lightweight DG.

</details>


### [60] [MedDChest: A Content-Aware Multimodal Foundational Vision Model for Thoracic Imaging](https://arxiv.org/abs/2511.04016)
*Mahmoud Soliman,Islam Osman,Mohamed S. Shehata,Rasika Rajapakshe*

Main category: cs.CV

> 提出MedDChest，一种专门针对胸部成像任务进行预训练的Vision Transformer模型，结合了新的数据增强策略，显著提高了医学图像模型的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的视觉模型在医学成像性能方面受到阻碍，主要是因为这些模型通常是基于非医学领域图像进行预训练后再微调获得的。为了克服这一领域的根本差异，我们特别提出了MedDChest。

**Method:** 我们提出了MedDChest，这是一种专门为胸部成像优化的基础视觉变压器模型(ViT)，它从零开始在超过120万张不同模态的胸片图像（包括X光和CT图像）上预训练，这些图像来源于10个公共数据集。提出了一种名为Guided Random Resized Crops的内容感知数据增强策略，它偏向于选择解剖学相关的区域，从而克服了标准裁剪技术在医学图像上的低效性。

**Result:** 将MedDChest微调后应用于多种下游诊断任务中，实验结果证明其显著优于使用公共ImageNet预训练模型的强大模型。实验还展示了大规模和领域内预训练与领域特定数据增强策略相结合的优越性。

**Conclusion:** MedDChest提供了一个强大的特征提取器作为起点，适用于广泛的胸部诊断任务，这为进一步研究和应用程序奠定了坚实的基础。

**Abstract:** The performance of vision models in medical imaging is often hindered by the
prevailing paradigm of fine-tuning backbones pre-trained on out-of-domain
natural images. To address this fundamental domain gap, we propose MedDChest, a
new foundational Vision Transformer (ViT) model optimized specifically for
thoracic imaging. We pre-trained MedDChest from scratch on a massive, curated,
multimodal dataset of over 1.2 million images, encompassing different
modalities including Chest X-ray and Computed Tomography (CT) compiled from 10
public sources. A core technical contribution of our work is Guided Random
Resized Crops, a novel content-aware data augmentation strategy that biases
sampling towards anatomically relevant regions, overcoming the inefficiency of
standard cropping techniques on medical scans. We validate our model's
effectiveness by fine-tuning it on a diverse set of downstream diagnostic
tasks. Comprehensive experiments empirically demonstrate that MedDChest
significantly outperforms strong, publicly available ImageNet-pretrained
models. By establishing the superiority of large-scale, in-domain pre-training
combined with domain-specific data augmentation, MedDChest provides a powerful
and robust feature extractor that serves as a significantly better starting
point for a wide array of thoracic diagnostic tasks. The model weights will be
made publicly available to foster future research and applications.

</details>


### [61] [Near-Lossless 3D Voxel Representation Free from Iso-surface](https://arxiv.org/abs/2511.04029)
*Yihao Luo,Xianglong He,Chuanyu Pan,Yiwen Chen,Jiaqi Wu,Yangguang Li,Wanli Ouyang,Yuanming Hu,Guang Yang,ChoonHwai Yap*

Main category: cs.CV

> 提出了一种名为Faithful Contouring的稀疏体素表示方法，该方法无需将网格转换为场函数或在重新网格化时提取等值面，就能实现高保真度的三维网格表示，并优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于等值面的体素表示方法依赖于加强水密性或渲染优化，这不可避免地会降低几何保真度。

**Method:** 提出的方法名为Faithful Contouring，是一种稀疏体素表示，支持2048+分辨率的任意网格表示，它可以保持尖锐度和内部结构，即使在具有复杂几何和拓扑的情况下也能实现近无损保真度。此外，该方法还表现出纹理化、操作和编辑的灵活性。

**Result:** 该方法超越了现有方法，在表示和重建的准确性和效率方面都表现更好。作为直接表示，它实现了10^-5等级的距离误差；作为网格重建，它将Chamfer Distance减少了93%，F-score提升了35%。

**Conclusion:** Faithful Contouring方法确认了其作为3D学习任务表示的优越保真度。

**Abstract:** Accurate and efficient voxelized representations of 3D meshes are the
foundation of 3D reconstruction and generation. However, existing
representations based on iso-surface heavily rely on water-tightening or
rendering optimization, which inevitably compromise geometric fidelity. We
propose Faithful Contouring, a sparse voxelized representation that supports
2048+ resolutions for arbitrary meshes, requiring neither converting meshes to
field functions nor extracting the isosurface during remeshing. It achieves
near-lossless fidelity by preserving sharpness and internal structures, even
for challenging cases with complex geometry and topology. The proposed method
also shows flexibility for texturing, manipulation, and editing. Beyond
representation, we design a dual-mode autoencoder for Faithful Contouring,
enabling scalable and detail-preserving shape reconstruction. Extensive
experiments show that Faithful Contouring surpasses existing methods in
accuracy and efficiency for both representation and reconstruction. For direct
representation, it achieves distance errors at the $10^{-5}$ level; for mesh
reconstruction, it yields a 93\% reduction in Chamfer Distance and a 35\%
improvement in F-score over strong baselines, confirming superior fidelity as a
representation for 3D learning tasks.

</details>


### [62] [A Hybrid Deep Learning Model for Robust Biometric Authentication from Low-Frame-Rate PPG Signals](https://arxiv.org/abs/2511.04037)
*Arfina Rahman,Mahesh Banavar*

Main category: cs.CV

> A robust biometric authentication system using PPG signals from fingertip videos achieves 98% accuracy, addressing challenges like motion artifacts and is ideal for low-cost wearable devices.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to leverage the advantages of PPG signals for biometric authentication while addressing challenges like motion artifacts and inter-subject variability, making the system suitable for low-cost wearable devices.

**Method:** This paper proposes a lightweight biometric authentication framework using PPG signals from low-frame-rate fingertip videos. A standard preprocessing pipeline and a hybrid deep learning model (CVT-ConvMixer-LSTM) are used to convert raw PPG signals into robust 2D time-frequency scalograms, capturing transient cardiovascular dynamics.

**Result:** Experimental results show an authentication accuracy of 98% on a dataset of 46 subjects, demonstrating the model's robustness to noise and subject variability.

**Conclusion:** The proposed system is efficient, scalable, and inherently capable of liveness detection, making it suitable for real-world mobile and embedded biometric security applications.

**Abstract:** Photoplethysmography (PPG) signals, which measure changes in blood volume in
the skin using light, have recently gained attention in biometric
authentication because of their non-invasive acquisition, inherent liveness
detection, and suitability for low-cost wearable devices. However, PPG signal
quality is challenged by motion artifacts, illumination changes, and
inter-subject physiological variability, making robust feature extraction and
classification crucial. This study proposes a lightweight and cost-effective
biometric authentication framework based on PPG signals extracted from
low-frame-rate fingertip videos. The CFIHSR dataset, comprising PPG recordings
from 46 subjects at a sampling rate of 14 Hz, is employed for evaluation. The
raw PPG signals undergo a standard preprocessing pipeline involving baseline
drift removal, motion artifact suppression using Principal Component Analysis
(PCA), bandpass filtering, Fourier-based resampling, and amplitude
normalization. To generate robust representations, each one-dimensional PPG
segment is converted into a two-dimensional time-frequency scalogram via the
Continuous Wavelet Transform (CWT), effectively capturing transient
cardiovascular dynamics. We developed a hybrid deep learning model, termed
CVT-ConvMixer-LSTM, by combining spatial features from the Convolutional Vision
Transformer (CVT) and ConvMixer branches with temporal features from a Long
Short-Term Memory network (LSTM). The experimental results on 46 subjects
demonstrate an authentication accuracy of 98%, validating the robustness of the
model to noise and variability between subjects. Due to its efficiency,
scalability, and inherent liveness detection capability, the proposed system is
well-suited for real-world mobile and embedded biometric security applications.

</details>


### [63] [Unveiling Deep Semantic Uncertainty Perception for Language-Anchored Multi-modal Vision-Brain Alignment](https://arxiv.org/abs/2511.04078)
*Zehui Feng,Chenqi Zhang,Mingru Wang,Minuo Wei,Shiwei Cheng,Cuntai Guan,Ting Han*

Main category: cs.CV

> Bratrix提出了一种多模态语言锚定的视觉-大脑对齐框架，能够更好地解释大脑中的视觉语义，并提升了针对EEG、MEG和fMRI信号的处理性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法主要直接将神经活动与视觉嵌入对齐，但纯视觉表示经常无法捕捉潜在的语义维度，这限制了可解释性和深度鲁棒性。为了解决这些限制，提出了Bratrix框架。

**Method:** Bratrix提出了一种多模态语言锚定视觉-大脑对齐的端到端框架。它将视觉刺激分解为分层的视觉和语言语义组件，并将视觉和大脑表示投影到共享的潜在空间中，从而形成对齐的视觉-语言和大脑-语言嵌入。此外，Bratrix引入了一个新颖的不确定性感知模块，以增强在对齐过程中处理嘈杂神经信号的能力。Bratrix-M采用了单模态预训练后进行多模态微调的两阶段训练策略，以提高对齐精度。

**Result:** 实验表明，Bratrix在EEG、MEG和fMRI基准测试上的检索、重建和描述性能优于最先进的方法，在200路EEG检索任务上超过了最先进的方法14.3%。

**Conclusion:** 通过引入新颖的不确定性感知模块和采用两阶段训练策略，Bratrix能够更准确地将视觉和语言语义与大脑信号对齐。该框架在多个任务上超出现有方法的表现。

**Abstract:** Unveiling visual semantics from neural signals such as EEG, MEG, and fMRI
remains a fundamental challenge due to subject variability and the entangled
nature of visual features. Existing approaches primarily align neural activity
directly with visual embeddings, but visual-only representations often fail to
capture latent semantic dimensions, limiting interpretability and deep
robustness. To address these limitations, we propose Bratrix, the first
end-to-end framework to achieve multimodal Language-Anchored Vision-Brain
alignment. Bratrix decouples visual stimuli into hierarchical visual and
linguistic semantic components, and projects both visual and brain
representations into a shared latent space, enabling the formation of aligned
visual-language and brain-language embeddings. To emulate human-like perceptual
reliability and handle noisy neural signals, Bratrix incorporates a novel
uncertainty perception module that applies uncertainty-aware weighting during
alignment. By leveraging learnable language-anchored semantic matrices to
enhance cross-modal correlations and employing a two-stage training strategy of
single-modality pretraining followed by multimodal fine-tuning, Bratrix-M
improves alignment precision. Extensive experiments on EEG, MEG, and fMRI
benchmarks demonstrate that Bratrix improves retrieval, reconstruction, and
captioning performance compared to state-of-the-art methods, specifically
surpassing 14.3% in 200-way EEG retrieval task. Code and model are available.

</details>


### [64] [Adversarial and Score-Based CT Denoising: CycleGAN vs Noise2Score](https://arxiv.org/abs/2511.04083)
*Abu Hanif Muhammad Syarubany*

Main category: cs.CV

> 本研究评估了CycleGAN和Noise2Score在CT图像非配对和自监督去噪中的性能。结果显示CycleGAN提供了较好的图像质量，而Noise2Score则在很大程度上依赖于无噪声输入的情况下提供了一个有效的方法。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机旨在通过两种不同的无成对数据和自监督训练方法提高CT图像的去噪效果。

**Method:** 我们研究了在非配对和自监督条件下的CT图像去噪，评估了两种高效的数据训练范式：基于CycleGAN的残差转换器和基于Noise2Score的分数匹配去噪器。

**Result:** 选定的CycleGAN将噪声输入从34.66 dB / 0.9234 SSIM提高到38.913 dB / 0.971 SSIM，并获得了1.9441的估计分数和1.9343的未知集（Kaggle排行榜）评分。Noise2Score在绝对PSNR / SSIM上略逊一筹，但在处理非常噪声输入时表现出显著优势。

**Conclusion:** 总体上，CycleGAN提供了最强的最终图像质量，而Noise2Score则提供了一个强大的无需成对数据的替代方案，具有竞争力的性能。

**Abstract:** We study CT image denoising in the unpaired and self-supervised regimes by
evaluating two strong, training-data-efficient paradigms: a CycleGAN-based
residual translator and a Noise2Score (N2S) score-matching denoiser. Under a
common evaluation protocol, a configuration sweep identifies a simple standard
U-Net backbone within CycleGAN (lambda_cycle = 30, lambda_iden = 2, ngf = ndf =
64) as the most reliable setting; we then train it to convergence with a longer
schedule. The selected CycleGAN improves the noisy input from 34.66 dB / 0.9234
SSIM to 38.913 dB / 0.971 SSIM and attains an estimated score of 1.9441 and an
unseen-set (Kaggle leaderboard) score of 1.9343. Noise2Score, while slightly
behind in absolute PSNR / SSIM, achieves large gains over very noisy inputs,
highlighting its utility when clean pairs are unavailable. Overall, CycleGAN
offers the strongest final image quality, whereas Noise2Score provides a robust
pair-free alternative with competitive performance. Source code is available at
https://github.com/hanifsyarubany/CT-Scan-Image-Denoising-using-CycleGAN-and-Noise2Score.

</details>


### [65] [When Swin Transformer Meets KANs: An Improved Transformer Architecture for Medical Image Segmentation](https://arxiv.org/abs/2511.04084)
*Nishchal Sapkota,Haoyan Shi,Yejia Zhang,Xianshi Ma,Bofang Zheng,Danny Z. Chen*

Main category: cs.CV

> 论文提出UKAST，结合有理函数网络与Transformer，提高医学图像分割的数据效率和精度，特别是在数据稀缺情境下增强超越基线方法的性能。

<details>
  <summary>Details</summary>

**Motivation:** 医学图像分割对于诊断和治疗计划至关重要，但复杂的人体结构和受限的标注训练数据使其成为难题。CNN在局部特征提取中表现出色，但在建模长距离依赖关系方面存在不足；而Transformer尽管能更有效地捕捉全局上下文，但需要大量数据且计算成本高昂。本文旨在解决这些问题。

**Method:** 本文提出了UKAST架构，该架构整合了基于有理函数的Kolmogorov-Arnold网络(KANs)到Swin Transformer编码器中。通过使用来自Kolmogorov-Arnold Transformer (KAT)的有理基函数和分组有理KANs (GR-KANs)，该架构解决了普通样条基KANs的低效问题，提高了框架的表达能力和数据效率，同时仅微增参数量。

**Result:** UKAST在四个不同类型的2D和3D医学图像分割基准上表现出色，持续超越基线的CNN和Transformer方法。特别是在数据稀缺的情况下，其精度优于其他方法，缓解了标准视觉Transformer的数据密集型限制。

**Conclusion:** 实验结果展示了通过KAN增强的Transformer提升数据高效的医学图像分割的潜力。代码可在https://github.com/nsapkota417/UKAST获取。

**Abstract:** Medical image segmentation is critical for accurate diagnostics and treatment
planning, but remains challenging due to complex anatomical structures and
limited annotated training data. CNN-based segmentation methods excel at local
feature extraction, but struggle with modeling long-range dependencies.
Transformers, on the other hand, capture global context more effectively, but
are inherently data-hungry and computationally expensive. In this work, we
introduce UKAST, a U-Net like architecture that integrates rational-function
based Kolmogorov-Arnold Networks (KANs) into Swin Transformer encoders. By
leveraging rational base functions and Group Rational KANs (GR-KANs) from the
Kolmogorov-Arnold Transformer (KAT), our architecture addresses the
inefficiencies of vanilla spline-based KANs, yielding a more expressive and
data-efficient framework with reduced FLOPs and only a very small increase in
parameter count compared to SwinUNETR. UKAST achieves state-of-the-art
performance on four diverse 2D and 3D medical image segmentation benchmarks,
consistently surpassing both CNN- and Transformer-based baselines. Notably, it
attains superior accuracy in data-scarce settings, alleviating the data-hungry
limitations of standard Vision Transformers. These results show the potential
of KAN-enhanced Transformers to advance data-efficient medical image
segmentation. Code is available at: https://github.com/nsapkota417/UKAST

</details>


### [66] [SpatialLock: Precise Spatial Control in Text-to-Image Synthesis](https://arxiv.org/abs/2511.04112)
*Biao Liu,Yuanzhi Liang*

Main category: cs.CV

> 提出SpatialLock框架以解决文本到图像生成中的对象精确定位问题，该框架通过PoI和PoG组件实现空间信息的整合和感知监督，实验结果显示其在多数据集上显著提升了定位精度。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法未能充分利用位置信息，导致对象的空间布局理解不充分，从而难以精确控制生成图像中的对象定位。

**Method:** SpatialLock框架，包含Position-Engaged Injection (PoI)和Position-Guided Learning (PoG)，前者通过注意力层直接整合空间信息，后者通过感知监督进一步完善对象定位。

**Result:** 实验表明，SpatialLock在精确对象定位上达到了新的先进水平，在多个数据集上实现了超过0.9的IOU得分。

**Conclusion:** SpatialLock框架能够生成具备精确空间排列的对象，同时还提高了生成图像的视觉质量。

**Abstract:** Text-to-Image (T2I) synthesis has made significant advancements in recent
years, driving applications such as generating datasets automatically. However,
precise control over object localization in generated images remains a
challenge. Existing methods fail to fully utilize positional information,
leading to an inadequate understanding of object spatial layouts. To address
this issue, we propose SpatialLock, a novel framework that leverages perception
signals and grounding information to jointly control the generation of spatial
locations. SpatialLock incorporates two components: Position-Engaged Injection
(PoI) and Position-Guided Learning (PoG). PoI directly integrates spatial
information through an attention layer, encouraging the model to learn the
grounding information effectively. PoG employs perception-based supervision to
further refine object localization. Together, these components enable the model
to generate objects with precise spatial arrangements and improve the visual
quality of the generated images. Experiments show that SpatialLock sets a new
state-of-the-art for precise object positioning, achieving IOU scores above 0.9
across multiple datasets.

</details>


### [67] [Tortoise and Hare Guidance: Accelerating Diffusion Model Inference with Multirate Integration](https://arxiv.org/abs/2511.04117)
*Yunghee Lee,Byeonghyun Pak,Junwha Hong,Hoseong Kim*

Main category: cs.CV

> 提出Tortoise和Hare Guidance（THG），一种无需训练就能加速扩散采样同时保持高质量生成的策略。通过重新构造Classifier-Free Guidance（CFG）ODE作为多速率ODE系统，揭示了附加指导分支对近似更为鲁棒，减少额外指导的计算，同时也引入了误差界限感知的时间步长采样方法和指导尺度调度器，THG在几乎没有任何生成保真度损失的情况下，最多将函数评估次数减少了30%。

<details>
  <summary>Details</summary>

**Motivation:** 为了在保持高质量生成的前提下加速扩散模型的采样过程。

**Method:** 通过将Classifier-Free Guidance (CFG) ODE重新构造为一个多速率ODE系统，分析误差界限，揭示附加指导分支的鲁棒性，并提出一种新的多速率加速策略THG，其中噪声估计采用细粒度的时间步长，而附加指导只在粗步长上进行积分，同时引入了误差界限感知的时间步长采样器和指导尺度调度器。

**Result:** THG最多可以将函数评估次数（NFE）减少30%，而且在几乎没有任何生成保真度损失的情况下，性能优于现有的CFG加速器。

**Conclusion:** THG策略展示了多速率公式化方法在扩散求解器中的潜力，为不用重新训练模型实现高质量图像的实时合成铺平了道路。

**Abstract:** In this paper, we propose Tortoise and Hare Guidance (THG), a training-free
strategy that accelerates diffusion sampling while maintaining high-fidelity
generation. We demonstrate that the noise estimate and the additional guidance
term exhibit markedly different sensitivity to numerical error by reformulating
the classifier-free guidance (CFG) ODE as a multirate system of ODEs. Our
error-bound analysis shows that the additional guidance branch is more robust
to approximation, revealing substantial redundancy that conventional solvers
fail to exploit. Building on this insight, THG significantly reduces the
computation of the additional guidance: the noise estimate is integrated with
the tortoise equation on the original, fine-grained timestep grid, while the
additional guidance is integrated with the hare equation only on a coarse grid.
We also introduce (i) an error-bound-aware timestep sampler that adaptively
selects step sizes and (ii) a guidance-scale scheduler that stabilizes large
extrapolation spans. THG reduces the number of function evaluations (NFE) by up
to 30% with virtually no loss in generation fidelity ($\Delta$ImageReward
$\leq$ 0.032) and outperforms state-of-the-art CFG-based training-free
accelerators under identical computation budgets. Our findings highlight the
potential of multirate formulations for diffusion solvers, paving the way for
real-time high-quality image synthesis without any model retraining. The source
code is available at https://github.com/yhlee-add/THG.

</details>


### [68] [Text to Sketch Generation with Multi-Styles](https://arxiv.org/abs/2511.04123)
*Tengjie Li,Shikui Tu,Lei Xu*

Main category: cs.CV

> 本文提出了一种基于扩散模型的训练-free框架，该框架通过文本提示和参考样式草图实现明确的样式指导，解决了现有方法缺乏对草图样式的精确控制的问题。实验表明，该方法在高质量的草图生成、准确的样式对齐和样式的灵活性控制方面表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 尽管视觉语言模型的进步促进了草图生成的发展，但现有的专业化方法主要集中在通用合成上，缺乏对草图样式的精确控制的机制。因此，本文动机在于解决这一问题，提供一种新的框架以实现更精细化的样控制。

**Method:** Structure

**Result:** {"tldr": "本文提出了一种基于扩散模型的训练-free框架，该框架通过文本提示和参考样式草图实现明确的样式指导，解决了现有方法缺乏对草图样式的精确控制的问题。实验表明，该方法在高质量的草图生成、准确的样式对齐和样式的灵活性控制方面表现出色。", "motivation": "尽管视觉语言模型的进步促进了草图生成的发展，但现有的专业化方法主要集中在通用合成上，缺乏对草图样式的精确控制的机制。因此，本文动机在于解决这一问题，提供一种新的框架以实现更精细化的样控制。", "method": "提出的方法基于扩散模型，并通过文本提示及参考样式的草图实现样式指导。这一方法并不像之前的样式转换方法那样通过覆盖自注意力中的键和值矩阵来工作，而是将参考特征作为辅助信息，并采用风格和内容引导机制，减少参考草图内容泄漏，提升合成质量。此外，通过整合多个参考草图的特征，利用联合AdaIN模块，使框架支持可控的多种样式生成。", "result": "实验显示，本方法在高质量草图生成、准确的样式对齐以及样式控制灵活性方面表现出色。", "conclusion": "该框架通过引入文本提示和参考样式草图，实现了草图生成中的精确风格控制，实验结果表明了其在提升草图质量和控制灵活性方面的优势。"}

**Conclusion:** 该框架通过引入文本提示和参考样式草图，实现了草图生成中的精确风格控制，实验结果表明了其在提升草图质量和控制灵活性方面的优势。

**Abstract:** Recent advances in vision-language models have facilitated progress in sketch
generation. However, existing specialized methods primarily focus on generic
synthesis and lack mechanisms for precise control over sketch styles. In this
work, we propose a training-free framework based on diffusion models that
enables explicit style guidance via textual prompts and referenced style
sketches. Unlike previous style transfer methods that overwrite key and value
matrices in self-attention, we incorporate the reference features as auxiliary
information with linear smoothing and leverage a style-content guidance
mechanism. This design effectively reduces content leakage from reference
sketches and enhances synthesis quality, especially in cases with low
structural similarity between reference and target sketches. Furthermore, we
extend our framework to support controllable multi-style generation by
integrating features from multiple reference sketches, coordinated via a joint
AdaIN module. Extensive experiments demonstrate that our approach achieves
high-quality sketch generation with accurate style alignment and improved
flexibility in style control. The official implementation of M3S is available
at https://github.com/CMACH508/M3S.

</details>


### [69] [Automated Tennis Player and Ball Tracking with Court Keypoints Detection (Hawk Eye System)](https://arxiv.org/abs/2511.04126)
*Venkata Manikanta Desu,Syed Fawaz Ali*

Main category: cs.CV

> The paper presents a comprehensive pipeline for automated tennis match analysis using deep learning models for player detection, ball tracking, and court keypoint detection, providing valuable game analytics.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind the paper is to develop a system that can automatically analyze tennis matches, providing detailed analytics such as player movement, ball speed, shot accuracy, and reaction times, to assist coaches, broadcasters, and players.

**Method:** The method involves using YOLOv8 for player detection, a custom-trained YOLOv5 model for ball tracking, and a ResNet50-based architecture for detecting court keypoints to facilitate spatial references.

**Result:** The experimental results indicate robust performance under various court conditions and match scenarios, with the system outputting annotated videos and detailed performance metrics.

**Conclusion:** The study concludes that the full pipeline for automated tennis analysis can effectively extract meaningful data, offering beneficial insights into the game dynamics for multiple stakeholders.

**Abstract:** This study presents a complete pipeline for automated tennis match analysis.
Our framework integrates multiple deep learning models to detect and track
players and the tennis ball in real time, while also identifying court
keypoints for spatial reference. Using YOLOv8 for player detection, a
custom-trained YOLOv5 model for ball tracking, and a ResNet50-based
architecture for court keypoint detection, our system provides detailed
analytics including player movement patterns, ball speed, shot accuracy, and
player reaction times. The experimental results demonstrate robust performance
in varying court conditions and match scenarios. The model outputs an annotated
video along with detailed performance metrics, enabling coaches, broadcasters,
and players to gain actionable insights into the dynamics of the game.

</details>


### [70] [DMSORT: An efficient parallel maritime multi-object tracking architecture for unmanned vessel platforms](https://arxiv.org/abs/2511.04128)
*Shengyu Tang,Zeyuan Lu,Jiazhi Dong,Changdong Yu,Xiaoyu Wang,Yaohui Lyu,Weihao Xia*

Main category: cs.CV

> 本文提出了一种双分支海事SORT（DMSORT）方法，通过结合鲁棒对象检测和外观特征提取以及运动补偿，实现了复杂海上环境中多目标跟踪的实时性和鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 准确感知复杂的海洋环境对于确保船舶安全航行和有效的海上监视至关重要。复杂的海洋环境引起的摄像机运动和随之而来的视觉退化对多目标跟踪（MOT）构成重大挑战。本文旨在通过提出DMSORT方法解决这一挑战。

**Method:** 该论文提出了一种双分支海事SORT（DMSORT）方法，用于海事多目标跟踪。该方法的核心是一个带有仿射补偿的并行跟踪器，包含对象检测与重新识别分支，以及一个专门的动态摄像机运动估计分支。具体来说，检测模块中集成了一个可逆柱状检测网络（RCDN），以利用多级视觉特征进行鲁棒对象检测。此外，设计了一种轻量级Transformer外观提取器（Li-TAE），以捕获全局上下文信息并生成鲁棒的外观特征。另一个分支通过构建投影变换，拆分平台诱发的和目标固有的运动，并在卡尔曼滤波器中应用平台运动补偿，从而稳定真实目标轨迹。最后，一个聚类优化特征融合模块有效结合了运动和外观线索，以确保在噪声、遮挡和漂移情况下的身份一致性。

**Result:** 在新加坡海洋数据集上的广泛评估表明，DMSORT实现了最先进的性能。值得注意的是，在现有的基于ReID的MOT框架中，DMSORT具有最快运行时间，同时保持了高度的身份一致性和对抖动和遮挡的鲁棒性。

**Conclusion:** 该论文提出的DMSORT方法展示了在海运环境中进行多目标跟踪的有效性，它通过结合仿射补偿、轻量级Transformer外观提取器和聚类优化特征融合，实现了鲁棒的身份跟踪。DMSORT不仅在速度上领先，而且在身份保持和抗遮挡能力方面也有出色表现。

**Abstract:** Accurate perception of the marine environment through robust multi-object
tracking (MOT) is essential for ensuring safe vessel navigation and effective
maritime surveillance. However, the complicated maritime environment often
causes camera motion and subsequent visual degradation, posing significant
challenges to MOT. To address this challenge, we propose an efficient
Dual-branch Maritime SORT (DMSORT) method for maritime MOT. The core of the
framework is a parallel tracker with affine compensation, which incorporates an
object detection and re-identification (ReID) branch, along with a dedicated
branch for dynamic camera motion estimation. Specifically, a Reversible
Columnar Detection Network (RCDN) is integrated into the detection module to
leverage multi-level visual features for robust object detection. Furthermore,
a lightweight Transformer-based appearance extractor (Li-TAE) is designed to
capture global contextual information and generate robust appearance features.
Another branch decouples platform-induced and target-intrinsic motion by
constructing a projective transformation, applying platform-motion compensation
within the Kalman filter, and thereby stabilizing true object trajectories.
Finally, a clustering-optimized feature fusion module effectively combines
motion and appearance cues to ensure identity consistency under noise,
occlusion, and drift. Extensive evaluations on the Singapore Maritime Dataset
demonstrate that DMSORT achieves state-of-the-art performance. Notably, DMSORT
attains the fastest runtime among existing ReID-based MOT frameworks while
maintaining high identity consistency and robustness to jitter and occlusion.
Code is available at:
https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-.

</details>


### [71] [Learning from Online Videos at Inference Time for Computer-Use Agents](https://arxiv.org/abs/2511.04137)
*Yujian Liu,Ze Wang,Hao Chen,Ximeng Sun,Xiaodong Yu,Jialian Wu,Jiang Liu,Emad Barsoum,Zicheng Liu,Shiyu Chang*

Main category: cs.CV

> 本文提出了一种使计算机使用代理从在线视频中学习的方法，该方法建立了能够检索与任务相关的在线视频并将其转化为结构化演示轨迹的框架，并在实验中证明了框架的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 人类可以通过观看视频教程，搜索、浏览和有选择地模仿与其当前子目标相匹配的短片段，从而弥补计算机操作代理的任务执行差距。本文旨在研究如何有效地使计算机使用代理从在线视频中学习。

**Method:** 本文提出了一个框架，该框架能够从在线视频中检索和筛选教程，将视频转化为结构化的演示轨迹，并在执行过程中动态选择轨迹作为情境指导。框架使用视觉语言模型（VLM）来推断用户界面操作，将视频分割成一组短的动作序列，并为目标任务分配相应的文本描述。在推理过程中，采用两阶段选择机制，在每一步动态选择一个轨迹添加到上下文中，从而使代理聚焦于最有助于其下一次决策的帮助信息。

**Result:** 实验结果表明，本文提出的框架在两个广泛使用的基准测试中始终优于强大的基础代理以及仅使用文本教程或字幕变种的代理。研究表明，轨迹分割和选择、动作筛选以及视觉信息是提高计算机使用代理性能的关键因素。

**Conclusion:** 分析结果表明，使用丰富的在线视频可以系统地提炼出可以转化为行动指导的信息，从而提高计算机用代理在推理时的性能。实验中充分展示该方法在两个不同基准测试中的强大性能，展现出从视频中抽取指导信息作为上下文的重要作用。

**Abstract:** Computer-use agents can operate computers and automate laborious tasks, but
despite recent rapid progress, they still lag behind human users, especially
when tasks require domain-specific procedural knowledge about particular
applications, platforms, and multi-step workflows. Humans can bridge this gap
by watching video tutorials: we search, skim, and selectively imitate short
segments that match our current subgoal. In this paper, we study how to enable
computer-use agents to learn from online videos at inference time effectively.
We propose a framework that retrieves and filters tutorial videos, converts
them into structured demonstration trajectories, and dynamically selects
trajectories as in-context guidance during execution. Particularly, using a
VLM, we infer UI actions, segment videos into short subsequences of actions,
and assign each subsequence a textual objective. At inference time, a two-stage
selection mechanism dynamically chooses a single trajectory to add in context
at each step, focusing the agent on the most helpful local guidance for its
next decision. Experiments on two widely used benchmarks show that our
framework consistently outperforms strong base agents and variants that use
only textual tutorials or transcripts. Analyses highlight the importance of
trajectory segmentation and selection, action filtering, and visual
information, suggesting that abundant online videos can be systematically
distilled into actionable guidance that improves computer-use agents at
inference time. Our code is available at
https://github.com/UCSB-NLP-Chang/video_demo.

</details>


### [72] [Seeing Straight: Document Orientation Detection for Efficient OCR](https://arxiv.org/abs/2511.04161)
*Suranjan Goswami,Abhinav Ravi,Raja Kolla,Ali Faraz,Shaharukh Khan,Akash,Chandra Khatri,Shubham Agarwal*

Main category: cs.CV

> This paper introduces a new benchmark ORB for evaluating OCR robustness to image rotations and presents a high-performing rotation classification pipeline that increases OCR accuracy by correcting document orientation errors.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this study is to address the persistent challenge of accurately correcting the orientation of scanned or photographed documents, which significantly impacts the performance of downstream tasks, specifically Optical Character Recognition (OCR).

**Method:** The paper introduces a fast, robust and lightweight rotation classification pipeline built upon the vision encoder of Phi-3.5-Vision model with dynamic image cropping, fine-tuned for a 4-class rotation task.

**Result:** The method achieves 96% and 92% accuracy in identifying rotations on ORB-En and ORB-Indic datasets respectively, improving OCR performance across different models.

**Conclusion:** The paper concludes by highlighting the importance of accurate rotation correction in enhancing OCR performance in real-world scenarios.

**Abstract:** Despite significant advances in document understanding, determining the
correct orientation of scanned or photographed documents remains a critical
pre-processing step in the real world settings. Accurate rotation correction is
essential for enhancing the performance of downstream tasks such as Optical
Character Recognition (OCR) where misalignment commonly arises due to user
errors, particularly incorrect base orientations of the camera during capture.
In this study, we first introduce OCR-Rotation-Bench (ORB), a new benchmark for
evaluating OCR robustness to image rotations, comprising (i) ORB-En, built from
rotation-transformed structured and free-form English OCR datasets, and (ii)
ORB-Indic, a novel multilingual set spanning 11 Indic mid to low-resource
languages. We also present a fast, robust and lightweight rotation
classification pipeline built on the vision encoder of Phi-3.5-Vision model
with dynamic image cropping, fine-tuned specifically for 4-class rotation task
in a standalone fashion. Our method achieves near-perfect 96% and 92% accuracy
on identifying the rotations respectively on both the datasets. Beyond
classification, we demonstrate the critical role of our module in boosting OCR
performance: closed-source (up to 14%) and open-weights models (up to 4x) in
the simulated real-world setting.

</details>


### [73] [Systematic Evaluation of Preprocessing Techniques for Accurate Image Registration in Digital Pathology](https://arxiv.org/abs/2511.04171)
*Fatemehzahra Darzi,Rodrigo Escobar Diaz Guerrero,Thomas Bocklitz*

Main category: cs.CV

> The paper explores how different color transformation techniques affect image registration accuracy between hematoxylin and eosin (H&E) stained images and non-linear multimodal images in digital pathology.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the integration and direct comparison of information from different stains or imaging modalities in digital pathology, which is essential for applications such as biomarker analysis and tissue reconstruction.

**Method:** Structure

**Result:** CycleGAN color transformation achieved the lowest registration errors after applying various color transformations and preprocessing steps to a dataset of 20 tissue sample pairs.

**Conclusion:** Applying color transformation before registration improves alignment between images from different modalities, supporting more reliable analysis in digital pathology.

**Abstract:** Image registration refers to the process of spatially aligning two or more
images by mapping them into a common coordinate system, so that corresponding
anatomical or tissue structures are matched across images. In digital
pathology, registration enables direct comparison and integration of
information from different stains or imaging modalities, sup-porting
applications such as biomarker analysis and tissue reconstruction. Accurate
registration of images from different modalities is an essential step in
digital pathology. In this study, we investigated how various color
transformation techniques affect image registration between hematoxylin and
eosin (H&E) stained images and non-linear multimodal images. We used a dataset
of 20 tissue sample pairs, with each pair undergoing several preprocessing
steps, including different color transformation (CycleGAN, Macenko, Reinhard,
Vahadane), inversion, contrast adjustment, intensity normalization, and
denoising. All images were registered using the VALIS registration method,
which first applies rigid registration and then performs non-rigid registration
in two steps on both low and high-resolution images. Registration performance
was evaluated using the relative Target Registration Error (rTRE). We reported
the median of median rTRE values (MMrTRE) and the average of median rTRE values
(AMrTRE) for each method. In addition, we performed a custom point-based
evaluation using ten manually selected key points. Registration was done
separately for two scenarios, using either the original or inverted multimodal
images. In both scenarios, CycleGAN color transformation achieved the lowest
registration errors, while the other methods showed higher errors. These
findings show that applying color transformation before registration improves
alignment between images from different modalities and supports more reliable
analysis in digital pathology.

</details>


### [74] [Covariance Descriptors Meet General Vision Encoders: Riemannian Deep Learning for Medical Image Classification](https://arxiv.org/abs/2511.04190)
*Josef Mayr,Anna Reithmeir,Maxime Di Folco,Julia A. Schnabel*

Main category: cs.CV

> 研究发现协方差描述符在医学图像分类中效果显著，特别是当与预训练视觉编码器结合使用时。实验展示了这种组合在多个医学图像数据集上的优越性能。

<details>
  <summary>Details</summary>

**Motivation:** 协方差描述符在一般计算机视觉任务中展示了强大的性能，但在医学成像中仍然未被广泛研究。本研究旨在探讨协方差描述符在医学图像分类中的效果，特别是在对称正定（SPD）矩阵分类网络中的应用。

**Method:** 我们研究了协方差描述符在医学图像分类中的有效性，对比了由预训练的一般视觉编码器（GVEs）提取的特征构造的协方差描述符与手工特征构造的协方差描述符。采用了两个GVEs（DINOv2和MedSAM），并在MedMNSIT基准的十一个二分类和多分类数据集上进行评估。

**Result:** 实验结果显示，由GVE特征构造的协方差描述符在所有数据集中都优于手工特征构造的协方差描述符，并且当与DINOv2特征结合使用时，SPDNet的表现超过了现有的最先进方法。

**Conclusion:** 研究结果表明，结合协方差描述符与强大的预训练视觉编码器在医学图像分析中具有巨大潜力。

**Abstract:** Covariance descriptors capture second-order statistics of image features.
They have shown strong performance in general computer vision tasks, but remain
underexplored in medical imaging. We investigate their effectiveness for both
conventional and learning-based medical image classification, with a particular
focus on SPDNet, a classification network specifically designed for symmetric
positive definite (SPD) matrices. We propose constructing covariance
descriptors from features extracted by pre-trained general vision encoders
(GVEs) and comparing them with handcrafted descriptors. Two GVEs - DINOv2 and
MedSAM - are evaluated across eleven binary and multi-class datasets from the
MedMNSIT benchmark. Our results show that covariance descriptors derived from
GVE features consistently outperform those derived from handcrafted features.
Moreover, SPDNet yields superior performance to state-of-the-art methods when
combined with DINOv2 features. Our findings highlight the potential of
combining covariance descriptors with powerful pretrained vision encoders for
medical image analysis.

</details>


### [75] [AStF: Motion Style Transfer via Adaptive Statistics Fusor](https://arxiv.org/abs/2511.04192)
*Hanmo Chen,Chenghao Xu,Jiexi Yan,Cheng Deng*

Main category: cs.CV

> 本文提出自适应统计融合器（AStF），改进传统运动风格转换方法，引入偏度和峰度参数，实现了更优的运动风格转换效果。

<details>
  <summary>Details</summary>

**Motivation:** 传统任意图像风格转换通常处理均值和方差，这一方法已被证明在运动风格转换中是不够的。因此，本文引入了偏度和峰度两个更多参数来分析运动风格。

**Method:** 本论文提出了一种新的自适应统计融合器（AStF），它由风格分离模块（SDM）和高阶多统计注意（HOS-Attn）组成。AStF在训练时结合了运动一致性正则化（MCR）鉴别器。

**Result:** 实验结果表明，AStF通过提供更全面的时空统计模式模型，在运动风格转换中优于现有的其他方法。

**Conclusion:** 此论文创新性地提出AStF，它能更好地捕获运动数据的复杂动态模式和时空连贯性特性，极大地提升了运动风格转换的效果。

**Abstract:** Human motion style transfer allows characters to appear less rigidity and
more realism with specific style. Traditional arbitrary image style transfer
typically process mean and variance which is proved effective. Meanwhile,
similar methods have been adapted for motion style transfer. However, due to
the fundamental differences between images and motion, relying on mean and
variance is insufficient to fully capture the complex dynamic patterns and
spatiotemporal coherence properties of motion data. Building upon this, our key
insight is to bring two more coefficient, skewness and kurtosis, into the
analysis of motion style. Specifically, we propose a novel Adaptive Statistics
Fusor (AStF) which consists of Style Disentanglement Module (SDM) and
High-Order Multi-Statistics Attention (HOS-Attn). We trained our AStF in
conjunction with a Motion Consistency Regularization (MCR) discriminator.
Experimental results show that, by providing a more comprehensive model of the
spatiotemporal statistical patterns inherent in dynamic styles, our proposed
AStF shows proficiency superiority in motion style transfers over
state-of-the-arts. Our code and model are available at
https://github.com/CHMimilanlan/AStF.

</details>


### [76] [MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection](https://arxiv.org/abs/2511.04255)
*Marawan Elbatel,Anbang Wang,Keyuan Liu,Kaouther Mouheb,Enrique Almar-Munoz,Lizhuo Lin,Yanqi Yang,Karim Lekadir,Xiaomeng Li*

Main category: cs.CV

> 研究通过多数据集预训练的方法，将人体为核心基础模型Sapiens适应到医疗影像，创建了MedSapiens，超越了多个现有先进模型的性能。

<details>
  <summary>Details</summary>

**Motivation:** 尽管关键点检测传统上依赖于领域特定的模型，大型预训练视觉模型的出现为关键点检测提供了新的机会，尤其是将人类为中心的基础模型应用于解剖关键点检测的潜力一直未被充分挖掘。

**Method:** 此研究通过多数据集预训练的方法，将设计用于姿态估计的Sapiens模型适应到医学影像中，形成了一个新的模型：MedSapiens。

**Result:** 通过对现有最先进模型的基准测试，MedSapiens展示了最高5.26%的平均成功率检测率(SDR)的提升，相比于泛化模型，最高21.81%的提升相比于专科模型。同时，在低数据设置中，它在成功检测率(SDR)方面比少样本状态的最先进方法提高了2.69%。

**Conclusion:** MedSapiens的实验结果表明，人类为中心的基础模型，优化的空间姿态定位，为解剖关键点检测提供了强大的先验知识，并展现出在低数据环境下的良好适应性。

**Abstract:** This paper does not introduce a novel architecture; instead, it revisits a
fundamental yet overlooked baseline: adapting human-centric foundation models
for anatomical landmark detection in medical imaging. While landmark detection
has traditionally relied on domain-specific models, the emergence of
large-scale pre-trained vision models presents new opportunities. In this
study, we investigate the adaptation of Sapiens, a human-centric foundation
model designed for pose estimation, to medical imaging through multi-dataset
pretraining, establishing a new state of the art across multiple datasets. Our
proposed model, MedSapiens, demonstrates that human-centric foundation models,
inherently optimized for spatial pose localization, provide strong priors for
anatomical landmark detection, yet this potential has remained largely
untapped. We benchmark MedSapiens against existing state-of-the-art models,
achieving up to 5.26% improvement over generalist models and up to 21.81%
improvement over specialist models in the average success detection rate (SDR).
To further assess MedSapiens adaptability to novel downstream tasks with few
annotations, we evaluate its performance in limited-data settings, achieving
2.69% improvement over the few-shot state of the art in SDR. Code and model
weights are available at https://github.com/xmed-lab/MedSapiens .

</details>


### [77] [Proto-LeakNet: Towards Signal-Leak Aware Attribution in Synthetic Human Face Imagery](https://arxiv.org/abs/2511.04260)
*Claudio Giusti,Luca Guarnera,Sebastiano Battiato*

Main category: cs.CV

> 本文提出的一个名为Proto-LeakNet框架，在潜在空间中通过信号泄露识别生成器特性，无需重新训练数据即可分析未见过的生成器，达到高精度和鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 自动生成和深度伪造模型的进步导致了现代计算机视觉系统中源归属和真实性验证面临的挑战。本研究的动机在于通过识别这些模型的'信号泄露'来创建一种更加可靠和透明的图像与深度伪造鉴别方法。

**Method:** Proto-LeakNet使用了一个时间注意力编码器来聚合多步潜在特征，同时使用了一个特征加权原型头来组织嵌入空间，从而实现透明归因。这种方法依赖于识别扩散模型输出中保留的生成器特异性线索。

**Result:** 最近的研究表明，扩散模型在生成图像时会无意中留下持久的统计痕迹，称为信号泄露。本文提出了一种名为Proto-LeakNet的新框架，该框架利用信号泄露和解释性归因机制，通过在扩散模型的潜在域中重新模拟部分扩散过程来识别生成器特有的残差线索。这种方法结合了闭集分类和基于密度的开集评估，能够分析未见过的生成器，而无需重新训练。实验结果表明，Proto-LeakNet在闭集数据上的Macro AUC达到了98.13%，并且不因后处理影响其鲁棒性，超越了目前最先进的方法。这表明在潜在空间中建模信号泄露偏见可以实现可靠的AI图像和深度伪造分析。

**Conclusion:** 实验结果表明，Proto-LeakNet在潜在空间中建模信号泄露偏见具备高精度和鲁棒性，尤其在分析未见过的生成器时能超越现有方法，实现对AI图像和深度伪造的可靠分析。此研究为未来进一步研究指明了方向。

**Abstract:** The growing sophistication of synthetic image and deepfake generation models
has turned source attribution and authenticity verification into a critical
challenge for modern computer vision systems. Recent studies suggest that
diffusion pipelines unintentionally imprint persistent statistical traces,
known as signal leaks, within their outputs, particularly in latent
representations. Building on this observation, we propose Proto-LeakNet, a
signal-leak-aware and interpretable attribution framework that integrates
closed-set classification with a density-based open-set evaluation on the
learned embeddings, enabling analysis of unseen generators without retraining.
Operating in the latent domain of diffusion models, our method re-simulates
partial forward diffusion to expose residual generator-specific cues. A
temporal attention encoder aggregates multi-step latent features, while a
feature-weighted prototype head structures the embedding space and enables
transparent attribution. Trained solely on closed data and achieving a Macro
AUC of 98.13%, Proto-LeakNet learns a latent geometry that remains robust under
post-processing, surpassing state-of-the-art methods, and achieves strong
separability between known and unseen generators. These results demonstrate
that modeling signal-leak bias in latent space enables reliable and
interpretable AI-image and deepfake forensics. The code for the whole work will
be available upon submission.

</details>


### [78] [DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification](https://arxiv.org/abs/2511.04281)
*Yujie Yang,Shuang Li,Jun Ye,Neng Dong,Fan Li,Huafeng Li*

Main category: cs.CV

> 提出了DinoGRL框架，利用DINOv2学习步伐特征，引入SASGL和PBMGE以增强步伐和外观流的特征表示，从而提升跨模态视频匹配的表现。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法倾向于利用模态不变的视觉特征，而忽略了模态不变且具有丰富时间动态的步伐特征，这限制了其建模跨模态视频匹配所需的时空一致性的能力。步伐特征不仅模态不变，还具有丰富的时空动态，因此有必要对其加以利用来提升性能。

**Method:** 通过提出名为DinoGRL的框架，该框架利用DINOv2的丰富视觉先验来学习与外观特征互补的步伐特征，从而实现健壮的跨模态检索。具体而言，引入了语义感知轮廓和步伐学习（SASGL）模型，利用DINOv2的通用语义先验生成和增强轮廓表示，并与ReID目标联合优化，实现语义丰富且任务适应的步伐特征学习。此外，开发了渐进双向多粒度增强（PBMGE）模块，通过使步伐和外观流在多个空间粒度上进行双向交互，逐步优化特征表示，充分利用它们的互补性来提升全局表示的丰富局部细节，并产生高度判别性特征。

**Result:** 实验结果在HITSZ-VCM和BUPT数据集上显示，该方法显著优于现有最先进的方法。

**Conclusion:** 该方法通过同步学习步伐特征和支持模态不变的视觉特征，显著提高了跨视频模态行人重新识别任务的性能。

**Abstract:** Video-based Visible-Infrared person re-identification (VVI-ReID) aims to
retrieve the same pedestrian across visible and infrared modalities from video
sequences. Existing methods tend to exploit modality-invariant visual features
but largely overlook gait features, which are not only modality-invariant but
also rich in temporal dynamics, thus limiting their ability to model the
spatiotemporal consistency essential for cross-modal video matching. To address
these challenges, we propose a DINOv2-Driven Gait Representation Learning
(DinoGRL) framework that leverages the rich visual priors of DINOv2 to learn
gait features complementary to appearance cues, facilitating robust
sequence-level representations for cross-modal retrieval. Specifically, we
introduce a Semantic-Aware Silhouette and Gait Learning (SASGL) model, which
generates and enhances silhouette representations with general-purpose semantic
priors from DINOv2 and jointly optimizes them with the ReID objective to
achieve semantically enriched and task-adaptive gait feature learning.
Furthermore, we develop a Progressive Bidirectional Multi-Granularity
Enhancement (PBMGE) module, which progressively refines feature representations
by enabling bidirectional interactions between gait and appearance streams
across multiple spatial granularities, fully leveraging their complementarity
to enhance global representations with rich local details and produce highly
discriminative features. Extensive experiments on HITSZ-VCM and BUPT datasets
demonstrate the superiority of our approach, significantly outperforming
existing state-of-the-art methods.

</details>


### [79] [FastGS: Training 3D Gaussian Splatting in 100 Seconds](https://arxiv.org/abs/2511.04283)
*Shiwei Ren,Tianci Wen,Yongchun Fang,Biao Lu*

Main category: cs.CV

> FastGS是一种基于多视角一致性的稠化和剪枝策略的新型加速框架，能够显著提升3D高斯散斑方法的训练速度而不影响渲染质量。

<details>
  <summary>Details</summary>

**Motivation:** 当前的3D高斯散斑加速方法未能妥善调控训练过程中的高斯数量，导致冗余的计算时间消耗。为解决这一问题并平衡训练时间与渲染质量，本文作者提出FastGS框架。

**Method:** FastGS框架摒弃了预算机制，创新性地提出了基于多视角一致性的稠化和剪枝策略，以调节训练过程中的高斯数量，从而提高计算效率。

**Result:** 该论文提出了一种名为FastGS的新型加速框架，通过基于多视角一致性的重要性设计稠化和剪枝策略，解决了现有3D高斯散斑加速方法在训练中未合理调控高斯数的问题。实验结果显示，与现有方法相比，FastGS在多个数据集上显著提升了训练速度，同时保持了渲染质量。它表现出强大的通用性，适用于多种任务。

**Conclusion:** 实验表明，FastGS在Mip-NeRF 360和Deep Blending数据集上分别达到了3.32倍和15.45倍的加速效果，且在多种场景下显示了良好的通用性和高的训练加速率。

**Abstract:** The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to
properly regulate the number of Gaussians during training, causing redundant
computational time overhead. In this paper, we propose FastGS, a novel, simple,
and general acceleration framework that fully considers the importance of each
Gaussian based on multi-view consistency, efficiently solving the trade-off
between training time and rendering quality. We innovatively design a
densification and pruning strategy based on multi-view consistency, dispensing
with the budgeting mechanism. Extensive experiments on Mip-NeRF 360, Tanks &
Temples, and Deep Blending datasets demonstrate that our method significantly
outperforms the state-of-the-art methods in training speed, achieving a
3.32$\times$ training acceleration and comparable rendering quality compared
with DashGaussian on the Mip-NeRF 360 dataset and a 15.45$\times$ acceleration
compared with vanilla 3DGS on the Deep Blending dataset. We demonstrate that
FastGS exhibits strong generality, delivering 2-7$\times$ training acceleration
across various tasks, including dynamic scene reconstruction, surface
reconstruction, sparse-view reconstruction, large-scale reconstruction, and
simultaneous localization and mapping. The project page is available at
https://fastgs.github.io/

</details>


### [80] [Vision Foundation Models in Agriculture: Toward Domain-Specific Adaptation for Weed Herbicide Trials Assessment](https://arxiv.org/abs/2511.04288)
*Leire Benito-Del-Valle,Artzai Picón,Daniel Mugica,Manuel Ramos,Eva Portillo,Javier Romero,Carlos Javier Jimenez,Ramón Navarra-Mestre*

Main category: cs.CV

> 通过对通用视觉基础模型进行农业领域的自监督学习，提高除草剂试验植物识别及损害评估准确性；领域特定模型改善了物种识别和损害分类并提高了低标注情况下的分割精度。

<details>
  <summary>Details</summary>

**Motivation:** 提升除草剂试验中植物物种识别和除草剂损害评估的准确性。通用视觉基础模型在复杂视觉领域展示出良好的潜力，但在农业领域，需要更细粒度的物种和损害类型区分，因此需要改进。

**Method:** 将通用视觉基础模型适应于除草剂试验的识别。通过自监督学习方式在大型精心策划的农业数据集上训练模型，使其学习到适用于除草剂试验图像的丰富且可迁移的表示方法。

**Result:** 领域特定模型在物种识别和损害分类方面显著优于通用基础模型。此外，在数据稀缺情况下，领域特定模型的标注效率也更高。

**Conclusion:** 领域特定基础模型展示了其泛化能力，可以显著降低人工标注工作量，为除草剂试验分析提供了一种可扩展和自动化的解决方案。

**Abstract:** Herbicide field trials require accurate identification of plant species and
assessment of herbicide-induced damage across diverse environments. While
general-purpose vision foundation models have shown promising results in
complex visual domains, their performance can be limited in agriculture, where
fine-grained distinctions between species and damage types are critical.
  In this work, we adapt a general-purpose vision foundation model to herbicide
trial characterization. Trained using a self-supervised learning approach on a
large, curated agricultural dataset, the model learns rich and transferable
representations optimized for herbicide trials images.
  Our domain-specific model significantly outperforms the best general-purpose
foundation model in both species identification (F1 score improvement from 0.91
to 0.94) and damage classification (from 0.26 to 0.33). Under unseen conditions
(new locations and other time), it achieves even greater gains (species
identification from 0.56 to 0.66; damage classification from 0.17 to 0.27). In
domain-shift scenarios, such as drone imagery, it maintains strong performance
(species classification from 0.49 to 0.60).
  Additionally, we show that domain-specific pretraining enhances segmentation
accuracy, particularly in low-annotation regimes. An annotation-efficiency
analysis reveals that, under unseen conditions, the domain-specific model
achieves 5.4% higher F1 score than the general-purpose model, while using 80%
fewer labeled samples.
  These results demonstrate the generalization capabilities of domain-specific
foundation models and their potential to significantly reduce manual annotation
efforts, offering a scalable and automated solution for herbicide trial
analysis.

</details>


### [81] [Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data](https://arxiv.org/abs/2511.04304)
*Robin Spanier,Thorsten Hoeser,Claudia Kuenzer*

Main category: cs.CV

> 研究展示了合成数据在海上基础设施检测中的重要作用，提升了全球海上设施监测的有效性和准确度。

<details>
  <summary>Details</summary>

**Motivation:** 旨在通过合成数据的使用改进针对海上基础设施检测的深度学习模型的性能。

**Method:** 使用合成和真实的Sentinel-1卫星图像，在四个地区训练YOLOv10目标检测模型，以提升模型性能。

**Result:** 模型在全球三处未见区域能够有效检测海上平台，共检测到3,529个海上平台，并在包含合成数据后F1分数提高到了0.90。

**Conclusion:** 强调了平衡数据集的重要性，展示深度学习策略结合合成数据生成，有效应对海上基础设施全球监测的挑战。

**Abstract:** The recent and ongoing expansion of marine infrastructure, including offshore
wind farms, oil and gas platforms, artificial islands, and aquaculture
facilities, highlights the need for effective monitoring systems. The
development of robust models for offshore infrastructure detection relies on
comprehensive, balanced datasets, but falls short when samples are scarce,
particularly for underrepresented object classes, shapes, and sizes. By
training deep learning-based YOLOv10 object detection models with a combination
of synthetic and real Sentinel-1 satellite imagery acquired in the fourth
quarter of 2023 from four regions (Caspian Sea, South China Sea, Gulf of
Guinea, and Coast of Brazil), this study investigates the use of synthetic
training data to enhance model performance. We evaluated this approach by
applying the model to detect offshore platforms in three unseen regions (Gulf
of Mexico, North Sea, Persian Gulf) and thereby assess geographic
transferability. This region-holdout evaluation demonstrated that the model
generalises beyond the training areas. In total, 3,529 offshore platforms were
detected, including 411 in the North Sea, 1,519 in the Gulf of Mexico, and
1,593 in the Persian Gulf. The model achieved an F1 score of 0.85, which
improved to 0.90 upon incorporating synthetic data. We analysed how synthetic
data enhances the representation of unbalanced classes and overall model
performance, taking a first step toward globally transferable detection of
offshore infrastructure. This study underscores the importance of balanced
datasets and highlights synthetic data generation as an effective strategy to
address common challenges in remote sensing, demonstrating the potential of
deep learning for scalable, global offshore infrastructure monitoring.

</details>


### [82] [RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation](https://arxiv.org/abs/2511.04317)
*Xiangjun Zhang,Litong Gong,Yinglin Zheng,Yansong Liu,Wentao Jiang,Mingyi Xu,Biao Wang,Tiezheng Ge,Ming Zeng*

Main category: cs.CV

> RISE-T2V通过集成提示重述和语义特征提取，增强了文本到视频扩散模型的能力，以生成更高质量且符合用户意图的视频。

<details>
  <summary>Details</summary>

**Motivation:** 当前的文本到视频扩散模型依赖于预训练的文本编码器进行语义对齐，但它们在处理简洁的提示时常常无法保持视频质量，并且这些模型不具备在线重新阐述提示的能力，限制了模型的扩展性和实用性。

**Method:** RISE-T2V整合提示重述和语义特征提取为一个单一且无缝的步骤，其关键是Rephrasing Adapter模块，该模块用于扩散模型在LLM的下一个词汇预测中使用文本隐藏状态作为视频生成的条件。

**Result:** RISE-T2V是一个多功能框架，适用于不同的视频扩散模型架构，大大提高了T2V模型生成与用户意图高度一致的高质量视频的能力。实验结果表明，引入Rephrasing Adapter可以隐性地将基本的提示扩展成更能准确体现用户意图的表述，并且结合LLMs的强大功能，使视频生成模型能够完成更广泛的文本到视频任务。

**Conclusion:** RISE-T2V证明了其作为一个多功能框架，通过引入Rephrasing Adapter和利用LLMs的强大功能，在不同视频扩散模型架构中提高文本到视频生成质量的有效性。

**Abstract:** Most text-to-video(T2V) diffusion models depend on pre-trained text encoders
for semantic alignment, yet they often fail to maintain video quality when
provided with concise prompts rather than well-designed ones. The primary issue
lies in their limited textual semantics understanding. Moreover, these text
encoders cannot rephrase prompts online to better align with user intentions,
which limits both the scalability and usability of the models, To address these
challenges, we introduce RISE-T2V, which uniquely integrates the processes of
prompt rephrasing and semantic feature extraction into a single and seamless
step instead of two separate steps. RISE-T2V is universal and can be applied to
various pre-trained LLMs and video diffusion models(VDMs), significantly
enhancing their capabilities for T2V tasks. We propose an innovative module
called the Rephrasing Adapter, enabling diffusion models to utilize text hidden
states during the next token prediction of the LLM as a condition for video
generation. By employing a Rephrasing Adapter, the video generation model can
implicitly rephrase basic prompts into more comprehensive representations that
better match the user's intent. Furthermore, we leverage the powerful
capabilities of LLMs to enable video generation models to accomplish a broader
range of T2V tasks. Extensive experiments demonstrate that RISE-T2V is a
versatile framework applicable to different video diffusion model
architectures, significantly enhancing the ability of T2V models to generate
high-quality videos that align with user intent. Visual results are available
on the webpage at https://rise-t2v.github.io.

</details>


### [83] [Submanifold Sparse Convolutional Networks for Automated 3D Segmentation of Kidneys and Kidney Tumours in Computed Tomography](https://arxiv.org/abs/2511.04334)
*Saúl Alonso-Monsalve,Leigh H. Whitehead,Adam Aurisano,Lorena Escudero Sanchez*

Main category: cs.CV

> This paper addresses the issue of time-intensive tumour segmentation in 3D medical images by proposing a method that uses voxel sparsification and submanifold sparse convolutions, achieving high accuracy and saving significant computational resources.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this paper is to address the challenge of time-consuming and specialized task of tumour delineation in radiological images, especially in 3D scans with a high number of voxels.

**Method:** The proposed methodology involves a two-stage process: voxel sparsification and the use of submanifold sparse convolutional networks to reduce the computational load while maintaining high-resolution and accurate 3D segmentations.

**Result:** The proposed method achieves Dice similarity coefficients of 95.8% for kidneys + masses, 85.7% for tumours + cysts, and 80.3% for tumours alone in CT images of renal cancer patients. The method also significantly reduces inference time by up to 60% and VRAM usage by up to 75% compared to an equivalent dense architecture.

**Conclusion:** The paper concludes that the proposed methodology provides state-of-the-art accuracy while significantly reducing computational resources, making quantitative analyses more feasible in a clinical setting.

**Abstract:** The accurate delineation of tumours in radiological images like Computed
Tomography is a very specialised and time-consuming task, and currently a
bottleneck preventing quantitative analyses to be performed routinely in the
clinical setting. For this reason, developing methods for the automated
segmentation of tumours in medical imaging is of the utmost importance and has
driven significant efforts in recent years. However, challenges regarding the
impracticality of 3D scans, given the large amount of voxels to be analysed,
usually requires the downsampling of such images or using patches thereof when
applying traditional convolutional neural networks. To overcome this problem,
in this paper we propose a new methodology that uses, divided into two stages,
voxel sparsification and submanifold sparse convolutional networks. This method
allows segmentations to be performed with high-resolution inputs and a native
3D model architecture, obtaining state-of-the-art accuracies while
significantly reducing the computational resources needed in terms of GPU
memory and time. We studied the deployment of this methodology in the context
of Computed Tomography images of renal cancer patients from the KiTS23
challenge, and our method achieved results competitive with the challenge
winners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7%
for tumours + cysts, and 80.3% for tumours alone. Crucially, our method also
offers significant computational improvements, achieving up to a 60% reduction
in inference time and up to a 75\% reduction in VRAM usage compared to an
equivalent dense architecture, across both CPU and various GPU cards tested.

</details>


### [84] [Comparative Study of CNN Architectures for Binary Classification of Horses and Motorcycles in the VOC 2008 Dataset](https://arxiv.org/abs/2511.04344)
*Muhammad Annas Shaikh,Hamza Zaman,Arbaz Asif*

Main category: cs.CV

> 研究了不同卷积神经网络架构在处理不平衡类别二元分类问题时的性能，采用数据增强技术提高少数类检测。

<details>
  <summary>Details</summary>

**Motivation:** 这项研究的动机在于探讨架构选择对不平衡二元分类任务的影响力，并量化数据增强策略在减轻类不平衡问题上的效果。

**Method:** 本文评估了九种卷积神经网络架构在VOC 2008数据集中对马和摩托车的二元分类问题上的表现，并通过实施少数类增强技术来应对显著的类不平衡问题。

**Result:** 实验结果显示性能差异显著，其中ConvNeXt-Tiny在马检测中的平均精确度（AP）为95.53%，摩托车检测中的AP为89.12%。数据增强显著提高了少数类检测的性能，尤其是对更深的架构有所帮助。

**Conclusion:** 本研究为架构选择提供了见解，并且指出了数据增强策略在解决类不平衡问题时的重要性。

**Abstract:** This paper presents a comprehensive evaluation of nine convolutional neural
network architectures for binary classification of horses and motorcycles in
the VOC 2008 dataset. We address the significant class imbalance problem by
implementing minority-class augmentation techniques. Our experiments compare
modern architectures including ResNet-50, ConvNeXt-Tiny, DenseNet-121, and
Vision Transformer across multiple performance metrics. Results demonstrate
substantial performance variations, with ConvNeXt-Tiny achieving the highest
Average Precision (AP) of 95.53% for horse detection and 89.12% for motorcycle
detection. We observe that data augmentation significantly improves minority
class detection, particularly benefiting deeper architectures. This study
provides insights into architecture selection for imbalanced binary
classification tasks and quantifies the impact of data augmentation strategies
in mitigating class imbalance issues in object detection.

</details>


### [85] [Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection](https://arxiv.org/abs/2511.04347)
*Sanjay Kumar,Tim Brophy,Eoin Martino Grua,Ganesh Sistu,Valentina Donzella,Ciaran Eising*

Main category: cs.CV

> 摘要：研究了遮挡条件下基于鸟瞰视图的相机和激光雷达的3D对象检测性能，发现在遮挡情况下，激光雷达比相机更受重影时影响较大，且在传感器融合中激光雷达的作用更为关键。

<details>
  <summary>Details</summary>

**Motivation:** 动机：虽然基于鸟瞰视图（BEV）的多模式融合架构已经表现出强大的性能，但是遮挡对3D检测准确性的影响尚未得到充分研究。这项工作旨在探究遮挡对基于BEV的相机和激光雷达输出的影响。

**Method:** 方法：使用BEVFusion架构，在nuScenes数据集上评估了相机和激光雷达在不同遮挡条件下的3D对象检测性能。

**Result:** 结果：中度遮挡导致仅基于相机的检测平均精度（mAP）下降41.3%，而激光雷达的性能在重度遮挡时急剧下降47.3%。在多传感器融合的情况下，遮挡相机对性能影响较小，仅为4.1%，而遮挡激光雷达则影响较大，下降26.8%。

**Conclusion:** 结论：研究表明，需要进一步研究遮挡感知评估方法和技术，以改善传感器融合技术，确保在部分传感器失效或环境中不利条件导致的性能退化时仍能保持检测精度。

**Abstract:** Accurate 3D object detection is essential for automated vehicles to navigate
safely in complex real-world environments. Bird's Eye View (BEV)
representations, which project multi-sensor data into a top-down spatial
format, have emerged as a powerful approach for robust perception. Although
BEV-based fusion architectures have demonstrated strong performance through
multimodal integration, the effects of sensor occlusions, caused by
environmental conditions such as fog, haze, or physical obstructions, on 3D
detection accuracy remain underexplored. In this work, we investigate the
impact of occlusions on both camera and Light Detection and Ranging (LiDAR)
outputs using the BEVFusion architecture, evaluated on the nuScenes dataset.
Detection performance is measured using mean Average Precision (mAP) and the
nuScenes Detection Score (NDS). Our results show that moderate camera
occlusions lead to a 41.3% drop in mAP (from 35.6% to 20.9%) when detection is
based only on the camera. On the other hand, LiDAR sharply drops in performance
only under heavy occlusion, with mAP falling by 47.3% (from 64.7% to 34.1%),
with a severe impact on long-range detection. In fused settings, the effect
depends on which sensor is occluded: occluding the camera leads to a minor 4.1%
drop (from 68.5% to 65.7%), while occluding LiDAR results in a larger 26.8%
drop (to 50.1%), revealing the model's stronger reliance on LiDAR for the task
of 3D object detection. Our results highlight the need for future research into
occlusion-aware evaluation methods and improved sensor fusion techniques that
can maintain detection accuracy in the presence of partial sensor failure or
degradation due to adverse environmental conditions.

</details>


### [86] [A MATLAB tutorial on deep feature extraction combined with chemometrics for analytical applications](https://arxiv.org/abs/2511.04349)
*Puneet Mishra,Martijntje Vollebregt,Yizhou Ma,Maria Font-i-Furnols*

Main category: cs.CV

> 该教程提供使用开源深度学习模型从成像数据中提取空间信息并加以整合的分步指南，重点不在于训练模型，而在于利用现有模型提取深度特征。

<details>
  <summary>Details</summary>

**Motivation:** 由于缺乏结构化的分步实施指导，尽管有现成的开源深度学习模型，但它们在分析化学中的采用仍受到限制。因此，该论文的目标是通过提供深度学习方法的分步指南来填补这一空白。

**Method:** 该教程重点不是训练深度学习模型进行图像处理，而是使用现有的开源模型来从图像数据中提取深度特征，其方法包括了分步指导，使用到了MATLAB代码演示，处理各种分析化学中常见的成像模态的成像数据。

**Result:** 该教程提供了使用MATLAB代码的演示，涵盖从不同常见于分析化学的成像模态中处理图像数据的方法，演示了如何利用深度学习模型从图像中提取深层特征。

**Conclusion:** 本教程为读者使用自己的数据集独立运行步骤提供了必要条件，通过展示如何将成像数据从各种成像模态中提取出来，并与其它数据源（如光谱信息）进行整合，来强化了分析化学图像数据的空间信息提取功能。

**Abstract:** Background In analytical chemistry, spatial information about materials is
commonly captured through imaging techniques, such as traditional color cameras
or with advanced hyperspectral cameras and microscopes. However, efficiently
extracting and analyzing this spatial information for exploratory and
predictive purposes remains a challenge, especially when using traditional
chemometric methods. Recent advances in deep learning and artificial
intelligence have significantly enhanced image processing capabilities,
enabling the extraction of multiscale deep features that are otherwise
challenging to capture with conventional image processing techniques. Despite
the wide availability of open-source deep learning models, adoption in
analytical chemistry remains limited because of the absence of structured,
step-by-step guidance for implementing these models.
  Results This tutorial aims to bridge this gap by providing a step-by-step
guide for applying deep learning approaches to extract spatial information from
imaging data and integrating it with other data sources, such as spectral
information. Importantly, the focus of this work is not on training deep
learning models for image processing but on using existing open source models
to extract deep features from imaging data.
  Significance The tutorial provides MATLAB code tutorial demonstrations,
showcasing the processing of imaging data from various imaging modalities
commonly encountered in analytical chemistry. Readers must run the tutorial
steps on their own datasets using the codes presented in this tutorial.

</details>


### [87] [Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA](https://arxiv.org/abs/2511.04384)
*Itbaan Safwan,Muhammad Annas Shaikh,Muhammad Haaris,Ramail Khan,Muhammad Atif Tahir*

Main category: cs.CV

> 本论文提出了一种针对MediaEval Medico 2025挑战的多任务框架，利用微调后的Florence-2模型同时进行视觉问题回答、解释生成和视觉定位。实验表明，相比单一任务基线模型，该多任务模型在回答准确性和视觉定位性能上均有显著提升。

<details>
  <summary>Details</summary>

**Motivation:** 多任务框架能够同时学习视觉定位、推理和解释，从而生成准确且可解释的回答。这一框架旨在通过利用多个经过精心挑选的数据集，提高医学图像的视觉问题回答的准确性和解释性。

**Method:** Structure

**Result:** {
  "tldr": "本论文提出了一种针对MediaEval Medico 2025挑战的多任务框架，利用微调后的Florence-2模型同时进行视觉问题回答、解释生成和视觉定位。实验表明，相比单一任务基线模型，该多任务模型在回答准确性和视觉定位性能上均有显著提升。",
  "motivation": "多任务框架能够同时学习视觉定位、推理和解释，从而生成准确且可解释的回答。这一框架旨在通过利用多个经过精心挑选的数据集，提高医学图像的视觉问题回答的准确性和解释性。",
  "method": "论文采用了Florence-2模型，并结合了三个不同的数据集：Kvasir-VQA-x1用于问答学习、合成增强的解释数据集提供结构化的医疗推理、以及文本到区域的对来链接视觉特征与分割掩码，以实现问答、解释生成和视觉定位的多任务学习。",
  "result": "通过详细的评估，证明了该多任务设置下的模型相比单一任务模型具有显著性能提升，在答案准确性和视觉定位方面都得到了改进，展示了多任务学习方法的有效性。",
  "conclusion": "结论表明，通过多任务学习可以提高医学视觉问答任务的性能，这种方法不仅可以提高回答的准确性，还能增强回答的解释能力，对医学应用具有重要价值。这一框架的广泛应用有望进一步推动医学图像分析与解释的发展。"]}


**Conclusion:** 结论表明，通过多任务学习可以提高医学视觉问答任务的性能，这种方法不仅可以提高回答的准确性，还能增强回答的解释能力，对医学应用具有重要价值。这一框架的广泛应用有望进一步推动医学图像分析与解释的发展。

**Abstract:** We present a multi-task framework for the MediaEval Medico 2025 challenge,
leveraging a LoRA-tuned Florence-2 model for simultaneous visual question
answering (VQA), explanation generation, and visual grounding. The proposed
system integrates three curated datasets: (1) Kvasir-VQA-x1 for question-answer
learning, (2) a synthetically enriched explanation dataset offering structured
medical reasoning, and (3) text-to-region pairs linking visual features with
segmentation masks. This multi-task setup enables the model to jointly learn
visual grounding, reasoning, and interpretation, producing responses that are
both accurate and interpretable. Extensive evaluation demonstrates that our
approach substantially improves over single-task baselines in both answer
accuracy and visual localization, highlighting the effectiveness of grounded
multi-task learning for medical VQA applications.

</details>


### [88] [BoRe-Depth: Self-supervised Monocular Depth Estimation with Boundary Refinement for Embedded Systems](https://arxiv.org/abs/2511.04388)
*Chang Liu,Juan Li,Sheng Zhang,Chang Liu,Jie Li,Xu Zhang*

Main category: cs.CV

> 本文提出了一种名为BoRe-Depth的轻量级单目深度估计模型，包含8.7M参数，旨在提高嵌入式系统中的深度估计性能和边界清晰度。模型在NVIDIA Jetson Orin上运行，达到50.7 FPS，优于现有的轻量级模型。

<details>
  <summary>Details</summary>

**Motivation:** 为了克服现有单目深度估计技术在嵌入式系统上的性能不足和边界模糊问题，本文提出了一种新的深度估计模型。

**Method:** BoRe-Depth 包括增强特征自适应融合模块（EFAF），自适应融合深度特征以增强边界细节的表示，并在编码器中集成语义知识以提高物体识别和边界感知能力。

**Result:** 实验表明，BoRe-Depth 在多个具有挑战性的数据集上显著优于其他轻量级模型，并且提供了详细的方法消融研究。

**Conclusion:** BoRe-Depth 在保持低参数量和高效率的前提下，改善了深度地图的估计和边界质量，特别是在嵌入式系统中取得了更好的性能。

**Abstract:** Depth estimation is one of the key technologies for realizing 3D perception
in unmanned systems. Monocular depth estimation has been widely researched
because of its low-cost advantage, but the existing methods face the challenges
of poor depth estimation performance and blurred object boundaries on embedded
systems. In this paper, we propose a novel monocular depth estimation model,
BoRe-Depth, which contains only 8.7M parameters. It can accurately estimate
depth maps on embedded systems and significantly improves boundary quality.
Firstly, we design an Enhanced Feature Adaptive Fusion Module (EFAF) which
adaptively fuses depth features to enhance boundary detail representation.
Secondly, we integrate semantic knowledge into the encoder to improve the
object recognition and boundary perception capabilities. Finally, BoRe-Depth is
deployed on NVIDIA Jetson Orin, and runs efficiently at 50.7 FPS. We
demonstrate that the proposed model significantly outperforms previous
lightweight models on multiple challenging datasets, and we provide detailed
ablation studies for the proposed methods. The code is available at
https://github.com/liangxiansheng093/BoRe-Depth.

</details>


### [89] [DORAEMON: A Unified Library for Visual Object Modeling and Representation Learning at Scale](https://arxiv.org/abs/2511.04394)
*Ke Du,Yimin Peng,Chao Gao,Fan Zhou,Siqiao Xue*

Main category: cs.CV

> DORAEMON是一个开放源代码的PyTorch库，它统一了视觉对象建模和表示学习，提供了一个高效、可扩展的研究平台。

<details>
  <summary>Details</summary>

**Motivation:** DORAEMON的动机在于通过集成数据集、模型和训练技术，提供一个统一的实验平台，来加速视觉识别和表示学习的研究，并让这些研究成果能够高效地应用于实际场景中。

**Method:** DORAEMON采用了开放源代码的PyTorch库，统一了跨不同尺度的视觉对象建模和表示学习。它提供了一个单一的、由YAML驱动的工作流，涵盖分类、检索和度量学习。此外，它通过与timm兼容的接口提供了超过1000个预训练主干模型，以及模块化的损失函数、增强技术和分布式训练工具。

**Result:** 实验结果表明，DORAEMON提供的可重复使用的配方在ImageNet-1K、MS-Celeb-1M和Stanford在线产品数据集上能达到或超过参考结果。同时，它还能通过一键导出到ONNX或HuggingFace来连接研究和部署。

**Conclusion:** 总之，DORAEMON通过提供一个可重复使用的框架，使得在视觉识别和表示学习领域的研究成果能够更容易地转化为实际应用。

**Abstract:** DORAEMON is an open-source PyTorch library that unifies visual object
modeling and representation learning across diverse scales. A single
YAML-driven workflow covers classification, retrieval and metric learning; more
than 1000 pretrained backbones are exposed through a timm-compatible interface,
together with modular losses, augmentations and distributed-training utilities.
Reproducible recipes match or exceed reference results on ImageNet-1K,
MS-Celeb-1M and Stanford online products, while one-command export to ONNX or
HuggingFace bridges research and deployment. By consolidating datasets, models,
and training techniques into one platform, DORAEMON offers a scalable
foundation for rapid experimentation in visual recognition and representation
learning, enabling efficient transfer of research advances to real-world
applications. The repository is available at https://github.com/wuji3/DORAEMON.

</details>


### [90] [HideAndSeg: an AI-based tool with automated prompting for octopus segmentation in natural habitats](https://arxiv.org/abs/2511.04426)
*Alan de Aguiar,Michaella Pereira Andrade,Charles Morphy D. Santos,João Paulo Gois*

Main category: cs.CV

> 该论文提出了一个极小监督的AI工具（HideAndSeg），用于分割章鱼视频，通过结合SAM2和YOLOv11进行自动化处理，能够有效减少需手动分析的工作量，用以提高野生头足类动物行为研究的效率。

<details>
  <summary>Details</summary>

**Motivation:** 该论文旨在解决章鱼在自然栖息地分析的挑战，如其伪装能力、皮肤纹理和颜色的快速变化、非刚性体变形以及频发的遮挡，所有这些因素都进一步受到变化多端的水下光照和浑浊度的影响。

**Method:** 该论文介绍了一种名为HideAndSeg的新型、极小监督的AI工具，用于分割章鱼视频。HideAndSeg结合了SAM2和一个定制训练的YOLOv11目标检测器。首先，用户提供点坐标来生成初始分割掩模，作为YOLO模型的训练数据。通过提供边界框提示给SAM2，该方法实现了整个流程的自动化。

**Result:** HideAndSeg在没有真实世界信息的情况下达到令人满意的性能，减少了分割噪声，并能够在自然环境中的完全遮挡后重新识别和细分章鱼，这是手动提示模型所不能做到的。

**Conclusion:** 通过减少现实世界场景需要的手动分析工作量，该研究提供了一种实用工具，为更有效地进行野生头足类动物的行为研究铺平道路。

**Abstract:** Analyzing octopuses in their natural habitats is challenging due to their
camouflage capability, rapid changes in skin texture and color, non-rigid body
deformations, and frequent occlusions, all of which are compounded by variable
underwater lighting and turbidity. Addressing the lack of large-scale annotated
datasets, this paper introduces HideAndSeg, a novel, minimally supervised
AI-based tool for segmenting videos of octopuses. It establishes a quantitative
baseline for this task. HideAndSeg integrates SAM2 with a custom-trained
YOLOv11 object detector. First, the user provides point coordinates to generate
the initial segmentation masks with SAM2. These masks serve as training data
for the YOLO model. After that, our approach fully automates the pipeline by
providing a bounding box prompt to SAM2, eliminating the need for further
manual intervention. We introduce two unsupervised metrics - temporal
consistency $DICE_t$ and new component count $NC_t$ - to quantitatively
evaluate segmentation quality and guide mask refinement in the absence of
ground-truth data, i.e., real-world information that serves to train, validate,
and test AI models. Results show that HideAndSeg achieves satisfactory
performance, reducing segmentation noise compared to the manually prompted
approach. Our method can re-identify and segment the octopus even after periods
of complete occlusion in natural environments, a scenario in which the manually
prompted model fails. By reducing the need for manual analysis in real-world
scenarios, this work provides a practical tool that paves the way for more
efficient behavioral studies of wild cephalopods.

</details>


### [91] [Solving Convex Partition Visual Jigsaw Puzzles](https://arxiv.org/abs/2511.04450)
*Yaniv Ohayon,Ofir Itzhak Shahar,Ohad Ben-Shahar*

Main category: cs.CV

> 研究扩展了自动拼图求解器的应用范围，处理凸多边形拼图，提出了新的贪婪求解器，并构建了首个基准数据集。

<details>
  <summary>Details</summary>

**Motivation:** 尽管自动拼图求解器在多个应用领域具有潜在的重要影响，但大多数研究集中在方形拼图求解器的开发上，极大地限制了其实际应用。本研究旨在扩展计算处理的拼图类型。

**Method:** 本研究利用几何和图像兼容性的方法，提出了一种贪婪求解器，并引入了凸多边形拼图的首个基准数据集。

**Result:** 报告了几种性能衡量指标，并提出了凸多边形拼图的首个基准数据集。

**Conclusion:** 本研究通过处理凸分区拼图，显著扩展了自动拼图求解器的应用范围。

**Abstract:** Jigsaw puzzle solving requires the rearrangement of unordered pieces into
their original pose in order to reconstruct a coherent whole, often an image,
and is known to be an intractable problem. While the possible impact of
automatic puzzle solvers can be disruptive in various application domains, most
of the literature has focused on developing solvers for square jigsaw puzzles,
severely limiting their practical use. In this work, we significantly expand
the types of puzzles handled computationally, focusing on what is known as
Convex Partitions, a major subset of polygonal puzzles whose pieces are convex.
We utilize both geometrical and pictorial compatibilities, introduce a greedy
solver, and report several performance measures next to the first benchmark
dataset of such puzzles.

</details>


### [92] [V-Thinker: Interactive Thinking with Images](https://arxiv.org/abs/2511.04460)
*Runqi Qiao,Qiuna Tan,Minghan Yang,Guanting Dong,Peiqing Yang,Shiqiang Lang,Enhui Wan,Xiaowan Wang,Yida Xu,Lan Yang,Chong Sun,Chen Li,Honggang Zhang*

Main category: cs.CV

> 本文介绍了V-Thinker，一种旨在通过强化学习来实现互动、视觉中心多模态推理的系统，该系统在各类实验中展示了出色的表现。

<details>
  <summary>Details</summary>

**Motivation:** 该论文旨在克服现有的限制，这些限制包括有限的视觉工具空间和任务特定的工作流程设计。

**Method:** 该论文提出了V-Thinker，一种通过端到端强化学习实现互动、视觉中心思维的一般多模态推理助手。V-Thinker由两个关键组件组成：数据进化飞轮和视觉渐进训练课程。

**Result:** 实验表明，V-Thinker在一般的和交互式的推理场景中都优于基于LMM的强基线方法。

**Conclusion:** V-Thinker为推动图像互动推理应用的发展提供了有价值的见解。

**Abstract:** Empowering Large Multimodal Models (LMMs) to deeply integrate image
interaction with long-horizon reasoning capabilities remains a long-standing
challenge in this field. Recent advances in vision-centric reasoning explore a
promising "Thinking with Images" paradigm for LMMs, marking a shift from
image-assisted reasoning to image-interactive thinking. While this milestone
enables models to focus on fine-grained image regions, progress remains
constrained by limited visual tool spaces and task-specific workflow designs.
To bridge this gap, we present V-Thinker, a general-purpose multimodal
reasoning assistant that enables interactive, vision-centric thinking through
end-to-end reinforcement learning. V-Thinker comprises two key components: (1)
a Data Evolution Flywheel that automatically synthesizes, evolves, and verifies
interactive reasoning datasets across three dimensions-diversity, quality, and
difficulty; and (2) a Visual Progressive Training Curriculum that first aligns
perception via point-level supervision, then integrates interactive reasoning
through a two-stage reinforcement learning framework. Furthermore, we introduce
VTBench, an expert-verified benchmark targeting vision-centric interactive
reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently
outperforms strong LMM-based baselines in both general and interactive
reasoning scenarios, providing valuable insights for advancing
image-interactive reasoning applications.

</details>


### [93] [Landslide Hazard Mapping with Geospatial Foundation Models: Geographical Generalizability, Data Scarcity, and Band Adaptability](https://arxiv.org/abs/2511.04474)
*Wenwen Li,Sizhe Wang,Hyunho Lee,Chenyan Lu,Sujit Roy,Rahul Ramachandran,Chia-Yu Hsu*

Main category: cs.CV

> 本文通过一个涵盖传感器、标签和领域的三轴分析框架来优化地球空间基础模型（GeoFMs），专门应用于Prithvi-EO-2.0进行滑坡制图。该模型在多种实验中表现出色，提升了滑坡预测的准确性和泛化能力，但面临计算成本高等问题。

<details>
  <summary>Details</summary>

**Motivation:** 传统的深度学习模型在应用于不同传感器、区域或训练数据有限的情况下，往往性能不佳。为了应对这些挑战，提出了该方法。

**Method:** 通过一个涵盖传感器、标签和领域的三轴分析框架来适应地球空间基础模型（GeoFMs），专注于Prithvi-EO-2.0进行滑坡制图。该模型基于全球预训练、自我监督和适应性微调。

**Result:** 实验结果显示，该模型在对滑坡制图方面，优于特定任务的CNN（U-Net, U-Net++）、视觉变压器（Segformer, SwinV2-B），以及其他GeoFMs（TerraMind, SatMAE）。模型显示了对光谱变异的抵抗力，即使标签稀缺也保持了准确性，并在多种数据集和地理设定间表现出了良好的泛化能力。

**Conclusion:** 本研究将GeoFMs定位为减少滑坡风险和环境监测更稳健、更可扩展的方法之一，尽管存在计算成本和缺乏可用于滑坡研究的可重复使用的AI训练数据等挑战。

**Abstract:** Landslides cause severe damage to lives, infrastructure, and the environment,
making accurate and timely mapping essential for disaster preparedness and
response. However, conventional deep learning models often struggle when
applied across different sensors, regions, or under conditions of limited
training data. To address these challenges, we present a three-axis analytical
framework of sensor, label, and domain for adapting geospatial foundation
models (GeoFMs), focusing on Prithvi-EO-2.0 for landslide mapping. Through a
series of experiments, we show that it consistently outperforms task-specific
CNNs (U-Net, U-Net++), vision transformers (Segformer, SwinV2-B), and other
GeoFMs (TerraMind, SatMAE). The model, built on global pretraining,
self-supervision, and adaptable fine-tuning, proved resilient to spectral
variation, maintained accuracy under label scarcity, and generalized more
reliably across diverse datasets and geographic settings. Alongside these
strengths, we also highlight remaining challenges such as computational cost
and the limited availability of reusable AI-ready training data for landslide
research. Overall, our study positions GeoFMs as a step toward more robust and
scalable approaches for landslide risk reduction and environmental monitoring.

</details>


### [94] [Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm](https://arxiv.org/abs/2511.04570)
*Jingqi Tong,Yurong Mou,Hangcheng Li,Mingzhe Li,Yongzhuo Yang,Ming Zhang,Qiguang Chen,Tianyi Liang,Xiaomeng Hu,Yining Zheng,Xinchi Chen,Jun Zhao,Xuanjing Huang,Xipeng Qiu*

Main category: cs.CV

> 本研究提出“基于视频思考”的新范式，利用视频生成模型（如Sora-2）解决了现有范式在动态过程和多模态理解上的局限性，并通过实验验证了Sora-2在多类型任务上的表现。

<details>
  <summary>Details</summary>

**Motivation:** 此研究的动机是解决“基于文本思考”和“基于图像思考”范式中存在的局限性，包括图像难以表示动态过程或连续变化，以及文本和视觉作为不同模态分离导致的统一多模态理解和生成的障碍。

**Method:** 本研究提出了“基于视频思考”的新范式，利用视频生成模型（如Sora-2）来统一视觉和文本推理，并在时间框架内进行整合。为此，我们开发了Video Thinking Benchmark（VideoThinkBench），该基准涵盖了两类任务：视觉为中心的任务（如眼力谜题）和文本为中心的任务（如GSM8K、MMMU的子集）。

**Result:** 评估结果表明，Sora-2在视觉为中心任务中一般可与现有的最先进的VLM模型相媲美，并在某些任务上超过了这些模型，例如眼力游戏。在文本为中心的任务上，Sora-2在MATH任务中达到92%的准确率，在MMMU任务中达到75.53%的准确率。

**Conclusion:** 研究结果表明，视频生成模型可能成为统一多模态理解和生成的模型，确立了“基于视频思考”作为统一多模态推理范式。

**Abstract:** "Thinking with Text" and "Thinking with Images" paradigm significantly
improve the reasoning ability of large language models (LLMs) and Vision
Language Models (VLMs). However, these paradigms have inherent limitations. (1)
Images capture only single moments and fail to represent dynamic processes or
continuous changes, and (2) The separation of text and vision as distinct
modalities, hindering unified multimodal understanding and generation. To
overcome these limitations, we introduce "Thinking with Video", a new paradigm
that leverages video generation models, such as Sora-2, to bridge visual and
textual reasoning in a unified temporal framework. To support this exploration,
we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench
encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing
Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our
evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,
Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even
surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric
tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.
Furthermore, we systematically analyse the source of these abilities. We also
find that self-consistency and in-context learning can improve Sora-2's
performance. In summary, our findings demonstrate that the video generation
model is the potential unified multimodal understanding and generation model,
positions "thinking with video" as a unified multimodal reasoning paradigm.

</details>


### [95] [THEval. Evaluation Framework for Talking Head Video Generation](https://arxiv.org/abs/2511.04520)
*Nabyl Quignon,Baptiste Chopin,Yaohui Wang,Antitza Dantcheva*

Main category: cs.CV

> A new multi-metric framework for evaluating talking head videos is proposed, highlighting the importance of quality, naturalness, and synchronization. The framework addresses the need for more comprehensive evaluation as generative methods advance.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this study is the rapid development in video generation technology, which has outpaced the development of proper evaluation metrics. There is a need for a more comprehensive and efficient evaluation framework that can better guide the improvement of generative methods.

**Method:** Content covers the background and the rapid progress in video generation, the current evaluation limitation, and introduces a new multi-metric evaluation framework designed for talking head videos focusing on quality, naturalness, and synchronization. The framework is tested on a large number of videos generated by advanced models, highlighting shortcomings in expressiveness and artifact detail.

**Result:** The experiments show that many state-of-the-art models perform well in lip synchronization but struggle with generating expressiveness and artifact-free details.

**Conclusion:** The new evaluation framework provides a more detailed insight into the performance of talking head video generation methods beyond just lip synchronization. The framework's metrics and the curating of a novel dataset for evaluation aim to push the field forward by providing clear targets for improvement.

**Abstract:** Video generation has achieved remarkable progress, with generated videos
increasingly resembling real ones. However, the rapid advance in generation has
outpaced the development of adequate evaluation metrics. Currently, the
assessment of talking head generation primarily relies on limited metrics,
evaluating general video quality, lip synchronization, and on conducting user
studies. Motivated by this, we propose a new evaluation framework comprising 8
metrics related to three dimensions (i) quality, (ii) naturalness, and (iii)
synchronization. In selecting the metrics, we place emphasis on efficiency, as
well as alignment with human preferences. Based on this considerations, we
streamline to analyze fine-grained dynamics of head, mouth, and eyebrows, as
well as face quality. Our extensive experiments on 85,000 videos generated by
17 state-of-the-art models suggest that while many algorithms excel in lip
synchronization, they face challenges with generating expressiveness and
artifact-free details. These videos were generated based on a novel real
dataset, that we have curated, in order to mitigate bias of training data. Our
proposed benchmark framework is aimed at evaluating the improvement of
generative methods. Original code, dataset and leaderboards will be publicly
released and regularly updated with new methods, in order to reflect progress
in the field.

</details>


### [96] [Learning from Single Timestamps: Complexity Estimation in Laparoscopic Cholecystectomy](https://arxiv.org/abs/2511.04525)
*Dimitrios Anastasiou,Santiago Barbarisi,Lucy Culshaw,Jayna Patel,Evangelos B. Mazomenos,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

> STC-Net是一个用于基于时间戳的腹腔镜胆囊切除术复杂性评估的框架，该框架能够在较弱的时间监督下从完整手术视频中进行复杂性评估，并在评估人体数据集上显示出优异的性能。

<details>
  <summary>Details</summary>

**Motivation:** 迫切需要有效的自动化手段来评估腹腔镜胆囊切除术中的炎症严重程度，现有的方法无法处理未经手动筛选的完整手术视频。

**Method:** STC-Net包括时空定位模块、窗口提议模块和分级模块，通过结合硬定位和软定位目标以及背景感知分级监督损失，来实现对腹腔镜胆囊切除术手术视频中炎症严重程度的自动评估。

**Result:** 实验结果显示，STC-Net在1,859段LC视频上的准确率为62.11%，F1评分为61.42%，比非定位基础模型在两个指标上都提高了10%以上，证明了这一框架在自动评估手术复杂性方面的有效性。

**Conclusion:** STC-Net提出了一种可扩展且有效的框架，能够从完整腹腔镜胆囊切除术视频中自动估计基于PGS的手术复杂性，具有广阔的临床应用前景。

**Abstract:** Purpose: Accurate assessment of surgical complexity is essential in
Laparoscopic Cholecystectomy (LC), where severe inflammation is associated with
longer operative times and increased risk of postoperative complications. The
Parkland Grading Scale (PGS) provides a clinically validated framework for
stratifying inflammation severity; however, its automation in surgical videos
remains largely unexplored, particularly in realistic scenarios where complete
videos must be analyzed without prior manual curation. Methods: In this work,
we introduce STC-Net, a novel framework for SingleTimestamp-based Complexity
estimation in LC via the PGS, designed to operate under weak temporal
supervision. Unlike prior methods limited to static images or manually trimmed
clips, STC-Net operates directly on full videos. It jointly performs temporal
localization and grading through a localization, window proposal, and grading
module. We introduce a novel loss formulation combining hard and soft
localization objectives and background-aware grading supervision. Results:
Evaluated on a private dataset of 1,859 LC videos, STC-Net achieves an accuracy
of 62.11% and an F1-score of 61.42%, outperforming non-localized baselines by
over 10% in both metrics and highlighting the effectiveness of weak supervision
for surgical complexity assessment. Conclusion: STC-Net demonstrates a scalable
and effective approach for automated PGS-based surgical complexity estimation
from full LC videos, making it promising for post-operative analysis and
surgical training.

</details>


### [97] [UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction](https://arxiv.org/abs/2511.04595)
*Chen Shi,Shaoshuai Shi,Xiaoyang Lyu,Chunyang Liu,Kehua Sheng,Bo Zhang,Li Jiang*

Main category: cs.CV

> UniSplat是一个能够处理稀疏、非重叠摄像机视图以及复杂动态场景的前馈3D重建框架，通过统一的潜时空融合学习将动态场景进行高精度重建。

<details>
  <summary>Details</summary>

**Motivation:** 尽管前馈3D重建在自动驾驶领域已经取得了迅速发展，但现有方法依然难以应对稀疏、不重叠的摄像机视图和复杂场景动态的联合挑战。因此，提出了UniSplat来解决这些挑战。

**Method:** UniSplat方法通过统一的潜在时空融合学习稳健的动态场景重建。该方法首先构建一个3D潜在支架，利用预训练的基础模型捕捉场景的几何和语义上下文。为了有效整合空间视图和时间帧中的信息，引入了一种在3D支架内操作的高效融合机制，实现了一致的空间和时间对齐。设计了双分支解码器，结合点锚定细化与体素生成，从融合的支架中生成动态感知高斯，并保持持久的静态高斯记忆，以实现实时场景补全，超过当前摄像头视图范围。

**Result:** 实验结果表明，UniSplat在新颖视角合成方面达到了最先进的性能，并且即使对于超出原始摄像头覆盖范围的视角，也提供了强大而高质量的渲染。

**Conclusion:** 该研究证实了UniSplat在复杂场景下的高效重建能力，特别是在动态场景中，展示了其在自动驾驶领域前馈3D重建方面的优势。

**Abstract:** Feed-forward 3D reconstruction for autonomous driving has advanced rapidly,
yet existing methods struggle with the joint challenges of sparse,
non-overlapping camera views and complex scene dynamics. We present UniSplat, a
general feed-forward framework that learns robust dynamic scene reconstruction
through unified latent spatio-temporal fusion. UniSplat constructs a 3D latent
scaffold, a structured representation that captures geometric and semantic
scene context by leveraging pretrained foundation models. To effectively
integrate information across spatial views and temporal frames, we introduce an
efficient fusion mechanism that operates directly within the 3D scaffold,
enabling consistent spatio-temporal alignment. To ensure complete and detailed
reconstructions, we design a dual-branch decoder that generates dynamic-aware
Gaussians from the fused scaffold by combining point-anchored refinement with
voxel-based generation, and maintain a persistent memory of static Gaussians to
enable streaming scene completion beyond current camera coverage. Extensive
experiments on real-world datasets demonstrate that UniSplat achieves
state-of-the-art performance in novel view synthesis, while providing robust
and high-quality renderings even for viewpoints outside the original camera
coverage.

</details>


### [98] [PixCLIP: Achieving Fine-grained Visual Language Understanding via Any-granularity Pixel-Text Alignment Learning](https://arxiv.org/abs/2511.04601)
*Yicheng Xiao,Yu Chen,Haoxuan Ma,Jiale Hong,Caorui Li,Lingxiang Wu,Haiyun Guo,Jinqiao Wang*

Main category: cs.CV

> 本文提出了PixCLIP框架，用于改进CLIP模型在细粒度图像文本对齐方面的性能。

<details>
  <summary>Details</summary>

**Motivation:** 尽管CLIP模型在多种下游视觉语言理解任务中表现优异，但提高其对细粒度图像文本对齐能力仍然是一个活跃的研究方向。现有研究主要通过增加视觉信息处理的粒度来实现，而关于多模态大型语言模型的研究表明，使用长而详细的文本描述可以有效提升模型的细粒度视觉-语言对齐效果。然而，CLIP文本编码器固有的令牌长度限制阻碍了其处理长文本描述中的细粒度文本信息的能力。

**Method:** 我们提出了一个名为PixCLIP的新框架，该框架能够同时处理视觉提示输入和长文本描述。我们首先建立了一个自动化注释管道，可以生成像素级的长文本描述。然后，我们用大型语言模型（LLM）替换了CLIP的原始文本编码器，并提出了一个三分支像素-文本对齐学习框架，以实现图像区域和其对应文本描述之间的细粒度对齐。

**Result:** 实验表明，PixCLIP在像素级别交互和长文本处理方面取得了突破性进展，达到了最先进的性能水平。

**Conclusion:** PixCLIP框架通过结合视觉和文本内容处理的粒度增强优势，实现了像素级的交互和长文本处理能力的显著提升，达到了目前最先进的性能水平。

**Abstract:** While the Contrastive Language-Image Pretraining(CLIP) model has achieved
remarkable success in a variety of downstream vison language understanding
tasks, enhancing its capability for fine-grained image-text alignment remains
an active research focus. To this end, most existing works adopt the strategy
of explicitly increasing the granularity of visual information processing,
e.g., incorporating visual prompts to guide the model focus on specific local
regions within the image. Meanwhile, researches on Multimodal Large Language
Models(MLLMs) have demonstrated that training with long and detailed textual
descriptions can effectively improve the model's fine-grained vision-language
alignment. However, the inherent token length limitation of CLIP's text encoder
fundamentally limits CLIP to process more granular textual information embedded
in long text sequences. To synergistically leverage the advantages of enhancing
both visual and textual content processing granularity, we propose PixCLIP, a
novel framework designed to concurrently accommodate visual prompt inputs and
process lengthy textual descriptions. Specifically, we first establish an
automated annotation pipeline capable of generating pixel-level localized,
long-form textual descriptions for images. Utilizing this pipeline, we
construct LongGRIT, a high-quality dataset comprising nearly 1.5 million
samples. Secondly, we replace CLIP's original text encoder with the LLM and
propose a three-branch pixel-text alignment learning framework, facilitating
fine-grained alignment between image regions and corresponding textual
descriptions at arbitrary granularity. Experiments demonstrate that PixCLIP
showcases breakthroughs in pixel-level interaction and handling long-form
texts, achieving state-of-the-art performance.

</details>


### [99] [Building Trust in Virtual Immunohistochemistry: Automated Assessment of Image Quality](https://arxiv.org/abs/2511.04615)
*Tushar Kataria,Shikha Dubey,Mary Bronner,Jolanta Jedrzkiewicz,Ben J. Brintz,Shireen Y. Elhabian,Beatrice S. Knudsen*

Main category: cs.CV

> 研究人员提出了一种自动且基于准确性的框架，用于评估六种配对或非配对图像翻译模型生成的虚拟免疫组化(IHC)染色图像的质量。通过使用颜色分离生成的真实和虚拟IHC图像的分割掩模，他们计算出了可以量化像素级别正确的IHC染色准确性指标。研究发现，传统的图像保真度指标与染色准确性相关性较差，而配对模型如PyramidPix2Pix和AdaptiveNCE在染色准确性上表现最佳。除此之外，全幅图像级别的评估显示了在切片级别的评估下不易发现的性能下降问题。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在解决深度学习模型生成虚拟IHC染色图像时，传统基于纹理和分布的图像质量评估指标无法准确反映IHC染色准确性的问题。

**Method:** 通过颜色分离生成真实和虚拟IHC图像中棕色(即阳性)染色像素的掩模，并使用这些掩模计算染色准确性指标(如Dice、IoU、Hausdorff距离)，以直接量化正确的像素级别标签，而无需专家的手动注释。

**Result:** 研究发现，传统图像保真度指标，如FID、PSNR和SSIM，与染色准确性和病理学家评估的相关性较差。配对模型如PyramidPix2Pix和AdaptiveNCE在染色准确性上表现最佳，而基于扩散和生成对抗网络的非配对模型提供准确的IHC阳性像素标签的可靠性较低。在全幅图像级别的评估中，性能下降的现象更加明显。

**Conclusion:** 该框架提供了一种可重复的评估虚拟IHC模型质量的方法，这是加速向病理学家的常规应用转化的关键步骤。

**Abstract:** Deep learning models can generate virtual immunohistochemistry (IHC) stains
from hematoxylin and eosin (H&E) images, offering a scalable and low-cost
alternative to laboratory IHC. However, reliable evaluation of image quality
remains a challenge as current texture- and distribution-based metrics quantify
image fidelity rather than the accuracy of IHC staining. Here, we introduce an
automated and accuracy grounded framework to determine image quality across
sixteen paired or unpaired image translation models. Using color deconvolution,
we generate masks of pixels stained brown (i.e., IHC-positive) as predicted by
each virtual IHC model. We use the segmented masks of real and virtual IHC to
compute stain accuracy metrics (Dice, IoU, Hausdorff distance) that directly
quantify correct pixel - level labeling without needing expert manual
annotations. Our results demonstrate that conventional image fidelity metrics,
including Frechet Inception Distance (FID), peak signal-to-noise ratio (PSNR),
and structural similarity (SSIM), correlate poorly with stain accuracy and
pathologist assessment. Paired models such as PyramidPix2Pix and AdaptiveNCE
achieve the highest stain accuracy, whereas unpaired diffusion- and GAN-based
models are less reliable in providing accurate IHC positive pixel labels.
Moreover, whole-slide images (WSI) reveal performance declines that are
invisible in patch-based evaluations, emphasizing the need for WSI-level
benchmarks. Together, this framework defines a reproducible approach for
assessing the quality of virtual IHC models, a critical step to accelerate
translation towards routine use by pathologists.

</details>


### [100] [NovisVQ: A Streaming Convolutional Neural Network for No-Reference Opinion-Unaware Frame Quality Assessment](https://arxiv.org/abs/2511.04628)
*Kylie Cancilla,Alexander Moore,Amar Saini,Carmen Carrano*

Main category: cs.CV

> 研究开发了一种基于流的无参考无观感偏见的VQA模型，用于对退化视频的质量进行评估，模型表现优于现有的方法。

<details>
  <summary>Details</summary>

**Motivation:** 尽管视频质量评估对于计算机视觉任务至关重要，但现有的FR（全参考）方法需要清洁的参考视频，大多数NR（无参考）模型依赖于昂贵的人类意见标签进行训练。此外，大多数未考虑观感的NR方法基于图像处理，忽略了对视频目标检测至关重要的时间上下文。

**Method:** 本研究提出了一种可扩展的、基于流的无参考无主观偏见的视频质量评估（VQA）模型。该模型利用DAVIS数据集中的合成退化数据训练了一个时间感知卷积架构，可以直接从退化的视频中预测全参考指标（LPIPS、PSNR、SSIM），而在推理时不依赖于参考视频。

**Result:** 研究表明，与基于图像的基线模型相比，该方法对多种退化情况的泛化能力更强，证明了在现实世界的视觉系统中实现可扩展VQA时，时间建模的价值。此外，该模型与全参考指标的相关性也优于广泛使用的基于主观洞察的图像质量评估基准BRISQUE。

**Conclusion:** 这项工作介绍了一种新的VQA模型，它在广泛的退化情况下表现更好，并展示出与全参考指标和现有方法相比更高的相关性，证明了本研究方法的有效性。

**Abstract:** Video quality assessment (VQA) is vital for computer vision tasks, but
existing approaches face major limitations: full-reference (FR) metrics require
clean reference videos, and most no-reference (NR) models depend on training on
costly human opinion labels. Moreover, most opinion-unaware NR methods are
image-based, ignoring temporal context critical for video object detection. In
this work, we present a scalable, streaming-based VQA model that is both
no-reference and opinion-unaware. Our model leverages synthetic degradations of
the DAVIS dataset, training a temporal-aware convolutional architecture to
predict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, without
references at inference. We show that our streaming approach outperforms our
own image-based baseline by generalizing across diverse degradations,
underscoring the value of temporal modeling for scalable VQA in real-world
vision systems. Additionally, we demonstrate that our model achieves higher
correlation with full-reference metrics compared to BRISQUE, a widely-used
opinion-aware image quality assessment baseline, validating the effectiveness
of our temporal, opinion-unaware approach.

</details>


### [101] [Polarization-resolved imaging improves eye tracking](https://arxiv.org/abs/2511.04652)
*Mantas Žurauskas,Tom Bu,Sanaz Alali,Beyza Kalkanli,Derek Shi,Fernando Alamos,Gauresh Pandit,Christopher Mei,Ali Behrooz,Ramin Mirjalili,Dave Stronks,Alexander Fix,Dmitri Model*

Main category: cs.CV

> 本研究利用偏振光成像技术对眼球进行追踪，实验结果表明使用该技术的眼球追踪系统在多种条件下比传统方法更精准。

<details>
  <summary>Details</summary>

**Motivation:** 为了提高眼球跟踪技术的准确性和鲁棒性，特别是在存在诸如眼睑遮挡、眼距变化和瞳孔大小变化等条件下的表现，本研究旨在利用偏振光特性来增强眼球追踪系统的性能。

**Method:** 本研究提出了一种名为极化启用的眼球追踪系统（PET），该系统通过结合使用偏振滤镜阵列相机和线性偏振近红外光源，能够在眼睑遮挡、眼距变化和瞳孔大小变化的情况下提取可追踪特征。

**Result:** 在对346名参与者的数据进行测试时，基于卷积神经网络训练的模型将95%分位数的绝对注视误差相对基线降低了10-16%。

**Conclusion:** 本研究证实了偏振光特性在眼球追踪中的实际应用价值，并提出这种技术可以作为一种简单且鲁棒的传感方式，适用于未来的可穿戴设备。

**Abstract:** Polarization-resolved near-infrared imaging adds a useful optical contrast
mechanism to eye tracking by measuring the polarization state of light
reflected by ocular tissues in addition to its intensity. In this paper we
demonstrate how this contrast can be used to enable eye tracking. Specifically,
we demonstrate that a polarization-enabled eye tracking (PET) system composed
of a polarization--filter--array camera paired with a linearly polarized
near-infrared illuminator can reveal trackable features across the sclera and
gaze-informative patterns on the cornea, largely absent in intensity-only
images. Across a cohort of 346 participants, convolutional neural network based
machine learning models trained on data from PET reduced the median
95th-percentile absolute gaze error by 10--16\% relative to capacity-matched
intensity baselines under nominal conditions and in the presence of eyelid
occlusions, eye-relief changes, and pupil-size variation. These results link
light--tissue polarization effects to practical gains in human--computer
interaction and position PET as a simple, robust sensing modality for future
wearable devices.

</details>


### [102] [Benchmark Designers Should "Train on the Test Set" to Expose Exploitable Non-Visual Shortcuts](https://arxiv.org/abs/2511.04655)
*Ellis Brown,Jihan Yang,Shusheng Yang,Rob Fergus,Saining Xie*

Main category: cs.CV

> 本文提出了一种用于评估多模态大规模语言模型(MLLMs)的基准测试设计方法，该方法包括诊断和消除非视觉偏差，以确保模型具有真正的视觉理解能力。

<details>
  <summary>Details</summary>

**Motivation:** 作者发现，模型可以在许多多模态基准测试中表现良好，但这并不意味着它们具有强大的视觉理解力，而是因为它们利用了偏差、语言先验和表面模式。这对需要视觉输入的以视觉为中心的基准测试来说尤其成问题。

**Method:** 采用诊断原则设计基准测试：如果基准测试可以被操纵，它就会被操纵。为了实现这一标准，作者采用了两种方法。首先，他们通过"测试集压力测试"（TsT）方法诊断基准测试的易受攻击性。其次，他们通过"迭代偏差修剪"（IBP）程序消减基准测试中的偏差。

**Result:** 通过对四个基准测试（VSI-Bench、CV-Bench、MMMU和VideoMME）的应用，作者发现普遍存在非视觉偏差，并创建了去偏的VSI-Bench-Debiased版本，其结果展示出更低的非视觉可解性和更广阔的视觉盲性能差距。

**Conclusion:** 本研究展示了一种有效的诊断和去偏方法，可用于改进多模态基准测试的设计，确保它们能够真实反映模型的视觉理解能力，阻止模型仅通过利用偏差和非视觉信息获得高分。

**Abstract:** Robust benchmarks are crucial for evaluating Multimodal Large Language Models
(MLLMs). Yet we find that models can ace many multimodal benchmarks without
strong visual understanding, instead exploiting biases, linguistic priors, and
superficial patterns. This is especially problematic for vision-centric
benchmarks that are meant to require visual inputs. We adopt a diagnostic
principle for benchmark design: if a benchmark can be gamed, it will be.
Designers should therefore try to ``game'' their own benchmarks first, using
diagnostic and debiasing procedures to systematically identify and mitigate
non-visual biases. Effective diagnosis requires directly ``training on the test
set'' -- probing the released test set for its intrinsic, exploitable patterns.
  We operationalize this standard with two components. First, we diagnose
benchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology.
Our primary diagnostic tool involves fine-tuning a powerful Large Language
Model via $k$-fold cross-validation on exclusively the non-visual, textual
inputs of the test set to reveal shortcut performance and assign each sample a
bias score $s(x)$. We complement this with a lightweight Random Forest-based
diagnostic operating on hand-crafted features for fast, interpretable auditing.
Second, we debias benchmarks by filtering high-bias samples using an
``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four
benchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive
non-visual biases. As a case study, we apply our full framework to create
VSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider
vision-blind performance gap than the original.

</details>
