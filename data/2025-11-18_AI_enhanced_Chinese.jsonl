{"id": "2511.11594", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11594", "abs": "https://arxiv.org/abs/2511.11594", "authors": ["James McCammon"], "title": "TimeStampEval: A Simple LLM Eval and a Little Fuzzy Matching Trick to Improve Search Accuracy", "comment": null, "summary": "Traditional fuzzy matching often fails when searching for quotes that are semantically identical but syntactically different across documents-a common issue when aligning official written records with speech-to-text transcripts. We introduce TimeStampEval, a benchmark for retrieving precise millisecond timestamps from long transcripts given non-verbatim quotes. Our simple two-stage method dramatically improves retrieval accuracy while cutting inference costs by over 90%. The motivating use case is an automated long-form podcast that assembles Congressional Record clips into AI-hosted narration. The technical challenge: given a sentence-timestamped transcript and a target quote that may differ due to transcription or editorial drift, return exact start and end boundaries. Standard algorithms handle verbatim text but break under fuzzier variants. Evaluating six modern LLMs on a 2,800-sentence (120k-token) transcript revealed four key findings. (1) Prompt design matters more than model choice: placing the query before the transcript and using compact formatting improved accuracy by 3-20 points while reducing token count by 30-40%. (2) Off-by-one errors form a distinct category, showing models understand the task but misplace boundaries. (3) A modest reasoning budget (600-850 tokens) raises accuracy from 37% to 77% for weak setups and to above 90% for strong ones. (4) Our \"Assisted Fuzzy\" approach-RapidFuzz pre-filtering followed by LLM verification on short snippets-improves fuzzy match accuracy by up to 50 points while halving latency and reducing cost per correct result by up to 96%. Extended tests on ten transcripts (50k-900k tokens, 1989-2025) confirm robustness to transcript length, vocabulary drift, and domain change, maintaining 95-100% rejection accuracy for absent targets.", "AI": {"tldr": "介绍了一种新的基准测试和方法TimeStampEval，用来从长的转录本中精确匹配非逐字引用的时间戳，通过RapidFuzz筛选和LLM验证大大提高了匹配准确性并降低了成本。", "motivation": "动机在于解决现有模糊匹配技术在面对语义相同但语法不同引用时的失效问题，尤其适用于官方记录和语音转文本转录本之间的对齐场景。例如，用于自动长篇播客，从国会记录中剪辑片段，并通过AI进行叙述。", "method": "这种方法是一个简单的两阶段过程，首先采用RapidFuzz进行预筛选，然后使用LLM验证短片段以提高模糊匹配的准确性。此外，通过精心设计的提示格式，展示了在有效减少消耗的token数的同时，提升了模型的准确性。", "result": "bstract介绍了TimeStampEval，这是一个基于非逐字引用从长文本中检索精确毫秒时间戳的基准。提出的方法是两阶段的，显著提高了检索准确性，同时降低了超过90%的推理成本。通过使用六种现代语言模型（LLMs）在包含2,800个句子的转录本上进行评估，发现了四个关键点：（1）提示设计比模型选择更重要；（2）模型在理解和任务存在偏差；（3）增加推理预算可以提高准确度；（4）结合RapidFuzz筛选和LLM验证的“辅助模糊匹配”方法显著提高了匹配准确度并减少了成本。实验显示，此方法在不同长度的转录本和词汇变化的情况下表现稳健。", "conclusion": "通过多个实验，包括使用六种现代语言模型的评估，得出了四个关键结论，指出了提示设计的重要性，展示了结合RapidFuzz预筛选和LLM验证的“辅助模糊匹配”方法对于拓展现有模糊匹配技术的有效性。结果表明该方法在不同长度和时间段的转录本中表现出良好的稳定性和高准确性。"}}
{"id": "2511.11793", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11793", "abs": "https://arxiv.org/abs/2511.11793", "authors": ["MiroMind Team", "Song Bai", "Lidong Bing", "Carson Chen", "Guanzheng Chen", "Yuntao Chen", "Zhe Chen", "Ziyi Chen", "Jifeng Dai", "Xuan Dong", "Yue Deng", "Yunjie Fu", "Junqi Ge", "Chenxia Han", "Tammy Huang", "Zhenhang Huang", "Jerry Jiao", "Shilei Jiang", "Tianyu Jiao", "Xiaoqi Jian", "Lei Lei", "Ruilin Li", "Ryan Luo", "Tiantong Li", "Xiang Lin", "Ziyuan Liu", "Zhiqi Li", "Jie Ni", "Qiang Ren", "Pax Sun", "Shiqian Su", "Chenxin Tao", "Bin Wang", "Hellen Wang", "Haonan Wang", "James Wang", "Jin Wang", "Jojo Wang", "Letian Wang", "Shizun Wang", "Weizhi Wang", "Zixuan Wang", "Jinfan Xu", "Sen Xing", "Chenyu Yang", "Hai Ye", "Jiaheng Yu", "Yue Yu", "Muyan Zhong", "Tianchen Zhao", "Xizhou Zhu", "Yanpeng Zhou", "Yifan Zhang", "Zhi Zhu"], "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling", "comment": "Technical Report", "summary": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.", "AI": {"tldr": "MiroThinker v1.0, an open-source agent, advances research capabilities by focusing on interaction depth as a third aspect of scaling along with model size and context length, achieving significant performance improvements across multiple benchmarks.", "motivation": "The motivation for MiroThinker is to address limitations with previous methods by offering a new approach to scaling, focusing on interaction depth as a key performance factor rather than just larger models or longer context.", "method": "Unlike previous approaches that focus on increasing model size or context length, MiroThinker employs reinforcement learning to deepen and increase the frequency of agent-environment interactions.", "result": "MiroThinker, a 72B parameter model with a 256K context window, outperforms other open-source agents and approaches commercial performance across four benchmarks including achieving up to 81.9% accuracy on GAIA.", "conclusion": "By demonstrating the effectiveness of interaction scaling through interaction depth, MiroThinker paves the way for next-generation research agents that can perform sustained multi-turn reasoning and complex workflows efficiently."}}
{"id": "2511.11810", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11810", "abs": "https://arxiv.org/abs/2511.11810", "authors": ["Bertram Højer"], "title": "On the Notion that Language Models Reason", "comment": "Accepted at the 1st Workshop on Epistemic Intelligence in Machine Learning, EurIPS 2025", "summary": "Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \\textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are \"statistical pattern matchers\"\" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.", "AI": {"tldr": "论文探讨语言模型（LMs）是否存在真正推理的能力，认为LMs输出推理样式的文本源于统计规律而非逻辑机制，这强调了在研究中描述计算过程的准确性的必要性。", "motivation": "探讨语言模型是否具备推理能力，以及其推理样式的输出是否不具备逻辑一致性，以此来阐明NLP领域中系统构建和分析的计算过程描述的重要性。", "method": "评估推理的定义及其在NLP领域的使用，论证这些定义与语言模型训练、信息处理和新令牌生成方式不一致。假设基于变压器的语言模型实现隐式有限阶马尔可夫核，该核将上下文映射到条件令牌分布。推理样式的输出对应于在学习核中的统计规律和近似统计不变量，而非明确的逻辑机制。", "result": "语言模型的推理样式的输出是由于统计规律和近似统计不变量导致，而不是通过明确的逻辑机制实现。", "conclusion": "语言模型本质上是“统计模式匹配器”，而非真正的推理者。这一视角阐明了为什么语言模型会生成推理样式的输出，但无法保证逻辑一致性。"}}
{"id": "2511.11821", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11821", "abs": "https://arxiv.org/abs/2511.11821", "authors": ["Hong-Jun Yoon", "Faisal Ashraf", "Thomas A. Ruggles", "Debjani Singh"], "title": "Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis", "comment": "18 pages, zero figures, Preprint submitted to Environmental Modeling and Software", "summary": "Information extraction from regulatory documents using large language models presents critical trade-offs between performance and computational resources. We evaluated seven open-weight models (0.6B-70B parameters) on hydropower licensing documentation to provide empirical deployment guidance.\n  Our analysis identified a pronounced 14B parameter threshold where validation methods transition from ineffective (F1 $<$ 0.15) to viable (F1 = 0.64). Consumer-deployable models achieve 64\\% F1 through appropriate validation, while smaller models plateau at 51\\%. Large-scale models approach 77\\% F1 but require enterprise infrastructure.\n  We identified systematic hallucination patterns where perfect recall indicates extraction failure rather than success in smaller models. Our findings establish the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, enabling evidence-based model selection.\n  These results provide immediate value for hydropower compliance while contributing insights into parameter scaling effects that generalize across information extraction tasks.", "AI": {"tldr": "本文对用于从法规文件中抽取信息的大型语言模型进行了研究，确立了14B参数作为验证模型效果的阈值，并提供了关于模型选择的实证指南。", "motivation": "研究动机在于探讨使用大型语言模型从法规文件中进行信息抽取时面临的主要权衡问题，即性能与计算资源之间的关系。", "method": "本文通过评估包含0.6B到70B参数的七个开源权重模型在水电许可证文件上的表现，以提供实用的部署指南。", "result": "研究发现14B参数是一个显著的阈值，在此之下模型验证方法无效（F1评分低于0.15），在此之上则变得可行（F1评分达到0.64）。通过适当的验证，可为消费者部署的模型能达到64%的F1评分，而较小的模型则停留在51%的水平上。相较于普及型部署模型，大规模模型接近77%的F1评分，但需要企业的基础设施。另外还发现中小型模型中存在系统性的错配模式，当这些模型表现出完美召回时实际上是抽取失败而非成功。", "conclusion": "此研究的发现确立了开源权重信息抽取的首个全面资源-性能映射表，适用于法规环境，对于水电合规工作具有立竿见影的价值，同时对于信息抽取任务中的参数扩展效应提供了广泛见解。"}}
{"id": "2511.11633", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11633", "abs": "https://arxiv.org/abs/2511.11633", "authors": ["Abhijeet Kumar", "Chetan Agarwal", "Pronoy B. Neogi", "Mayank Goswami"], "title": "Psychological stress during Examination and its estimation by handwriting in answer script", "comment": "10 Pages, 6 Figures and 1 Table", "summary": "This research explores the fusion of graphology and artificial intelligence to quantify psychological stress levels in students by analyzing their handwritten examination scripts. By leveraging Optical Character Recognition and transformer based sentiment analysis models, we present a data driven approach that transcends traditional grading systems, offering deeper insights into cognitive and emotional states during examinations. The system integrates high resolution image processing, TrOCR, and sentiment entropy fusion using RoBERTa based models to generate a numerical Stress Index. Our method achieves robustness through a five model voting mechanism and unsupervised anomaly detection, making it an innovative framework in academic forensics.", "AI": {"tldr": "研究探索了图谱学和人工智能的融合，通过分析学生的考试手写脚本来量化他们的心理压力，并提出了一个数据驱动的方法来达到这一目的。", "motivation": "本研究旨在超越传统的评分系统，通过图谱学和人工智能的融合，提供对考试期间的认知和情感状态的深入洞察。", "method": "通过结合光学字符识别和基于变压器的情感分析模型，提出了一个数据驱动的方法来分析学生的考试手写脚本，以量化他们的心理压力水平。该系统整合了高分辨率图像处理、TrOCR以及基于RoBERTa的情感熵融合模型来生成数值压力指数。", "result": "该系统通过五模型投票机制和无监督异常检测实现了稳健性，被认为是在学术取证中的一个创新框架。", "conclusion": "此方法不仅能够生成数值压力指数，还能对考试过程中的认知和情感状态提供更深层次的理解，超越了传统的评分方法。"}}
{"id": "2511.11829", "categories": ["cs.CL", "cs.AI", "cs.FL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.11829", "abs": "https://arxiv.org/abs/2511.11829", "authors": ["Mihir Gupte", "Ramesh S"], "title": "Towards Autoformalization of LLM-generated Outputs for Requirement Verification", "comment": "To be submitted for publication", "summary": "Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.", "AI": {"tldr": "本研究使用一个基于LLMs的简单自动形式化工具来验证LLMs生成的输出准确性，并展示了其在检测逻辑等价性和不一致性方面的潜力。", "motivation": "由于目前没有正式的方法来验证LLMs生成的结构化输出的准确性，此研究旨在探索使用简单的基于LLMs的自动形式化工具来填补这一空白。", "method": "本研究通过简单的基于大型语言模型（LLMs）的自动形式化工具来验证LLMs生成的输出是否准确，提出了两种实验方法。", "result": "在实验中，自动形式化工具成功识别了两个用不同措辞书写的自然语言需求在逻辑上是等价的，并且工具能识别给定的自然语言要求和LLMs生成的输出之间存在的逻辑不一致。", "conclusion": "尽管发现有限，但结果表明，自动形式化对于确保LLMs生成的输出的准确性和逻辑一致性具有显著潜力，为未来更广泛的研究奠定了基础。"}}
{"id": "2511.11643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11643", "abs": "https://arxiv.org/abs/2511.11643", "authors": ["Aswath Muthuselvam", "Jeevak Raj S", "Mohanaprasad K"], "title": "Real-time pothole detection with onboard sensors and camera on vehicles", "comment": null, "summary": "Road conditions play an important role in our everyday commute. With the proliferating number of vehicles on the road each year, it has become necessary to access the road conditions very frequently, this would ensure that the traffic also flows smoothly. Even the smallest crack in the road could be easily be chipped into a large pothole due to changing surface temperatures of the road and from the force of vehicles riding over it. In this paper, we have addressed how we could better identify these potholes in realtime with the help of onboard sensors in vehicles so that the data could be useful for analysis and better management of potholes on a large scale. For the implementation, we used an SVM classifier to detect potholes, we achieved 98.1% accuracy based on data collected from a local road for about 2 km which had 26 potholes distributed along the road. Code is available at: https://github.com/aswathselvam/Potholes", "AI": {"tldr": "使用车载传感器和SVM分类器实时检测路面坑洞，实现98.1%的准确性。", "motivation": "随着道路上车辆数量的增多，需要更频繁地评估路面状况以确保交通顺畅。即使是小裂缝也可能因为温度变化和车辆行驶力导致变成大坑洞。", "method": "使用车载传感器收集路面数据，并应用SVM分类器模型来进行坑洞识别。", "result": "在约2公里的测试道路上，分布有26个坑洞，实现了98.1%的检测准确率。", "conclusion": "研究显示，基于车载传感器数据的实时坑洞识别为路面的维护和管理提供了有效的解决方案。"}}
{"id": "2511.11857", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11857", "abs": "https://arxiv.org/abs/2511.11857", "authors": ["Taimur Khan", "Ramoza Ahsan", "Mohib Hameed"], "title": "Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection", "comment": "18 pages", "summary": "Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.", "AI": {"tldr": "本文介绍了一个分析电影剧本情绪弧线和人物背景的框架，使用LabMTsimple storylab模块建立的自定义词库进行情感分析，同时通过Wards层次聚类技术对相似情绪曲线进行聚类。实验结果显示这种方法对于帮助选择故事具有实际应用价值。", "motivation": "动机在于解决故事理解和分析这一自然语言理解领域中的重要且具有挑战性的问题，特别是在大规模叙事数据中需要自动语义分析和计算学习而不仅仅依赖于手动分析。", "method": "本文提出了一种分析电影剧本情绪曲线并进行扩展分析的框架，该扩展分析涉及故事中涉及的人物背景。框架可以提取叙事中传达的高级和低级概念。方法采用基于词典的情绪分析，利用LabMTsimple storylab模块构建的自定义词库进行情感分析，并使用Wards层次聚类技术对相似的情绪曲线进行聚类分析。", "result": "实验评估表明，该分析方法对于帮助消费者和读者选择叙事或故事是有益的。", "conclusion": "该框架通过自定义词库和Wards层次聚类技术在电影脚本中成功实现了情绪曲线的识别和聚类，有助于提高故事选择的效率和效果。"}}
{"id": "2511.11659", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11659", "abs": "https://arxiv.org/abs/2511.11659", "authors": ["Kesong Zheng", "Zhi Song", "Peizhou Li", "Shuyi Yao", "Zhenxing Bian"], "title": "A Method for Identifying Farmland System Habitat Types Based on the Dynamic-Weighted Feature Fusion Network Model", "comment": "30 pages,12 figures", "summary": "Addressing the current lack of a standardized habitat classification system for cultivated land ecosystems, incomplete coverage of habitat types, and the inability of existing models to effectively integrate semantic and texture features-resulting in insufficient segmentation accuracy and blurred boundaries for multi-scale habitats (e.g., large-scale field plots and micro-habitats)-this study developed a comprehensively annotated ultra-high-resolution remote sensing image dataset encompassing 15 categories of cultivated land system habitats. Furthermore, we propose a Dynamic-Weighted Feature Fusion Network (DWFF-Net). The encoder of this model utilizes a frozen-parameter DINOv3 to extract foundational features. By analyzing the relationships between different category images and feature maps, we introduce a data-level adaptive dynamic weighting strategy for feature fusion. The decoder incorporates a dynamic weight computation network to achieve thorough integration of multi-layer features, and a hybrid loss function is adopted to optimize model training. Experimental results on the constructed dataset demonstrate that the proposed model achieves a mean Intersection over Union (mIoU) of 0.6979 and an F1-score of 0.8049, outperforming the baseline network by 0.021 and 0.0161, respectively. Ablation studies further confirm the complementary nature of multi-layer feature fusion, which effectively improves the IoU for micro-habitat categories such as field ridges. This study establishes a habitat identification framework for cultivated land systems based on adaptive multi-layer feature fusion, enabling sub-meter precision habitat mapping at a low cost and providing robust technical support for fine-grained habitat monitoring in cultivated landscapes.", "AI": {"tldr": "本文提出了一个基于自适应多层特征融合的生态位识别框架，以实现低成本、高精度的农业生态系统微生态位制图，并建立了涵盖15类农业生态系统生态位的超高清遥感图像数据集，实验表明该方法在mIoU和F1分数上优于基线网络。", "motivation": "本文旨在解决当前农业生态系统中缺乏标准化的生态位分类体系、覆盖的生态位类型不全面以及现有模型无法有效整合语义和纹理特征，导致多尺度生态位(如大规模土地种植带和微生态位)的分割精度不足和边界模糊的问题。", "method": "本文提出了一种动态加权特征融合网络（DWFF-Net），其编码器使用带有冻结参数的DINOv3来提取基础特征。通过分析不同类别图像和特征图之间的关系，引入了数据层次上的自适应动态加权策略进行特征融合。解码器包含一个动态权重计算网络以实现多层特征的深入融合，并采用混合损失函数优化模型训练。", "result": "实验结果表明，提出的模型在建立的数据集上达到了0.6979的平均交并比（mIoU）和0.8049的F1分数，分别超出基线网络0.021和0.0161。消融研究进一步验证了多层特征融合的有效性，能显著改善微生态位如种植带的IoU。", "conclusion": "本文建立了一个基于自适应多层特征融合的农业生态系统生态位识别框架，能够以低成本实现亚米级精度的生态位制图，为农业景观中精细生态位监测提供了稳健的技术支持。"}}
{"id": "2511.11867", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11867", "abs": "https://arxiv.org/abs/2511.11867", "authors": ["Namu Park", "Giridhar Kaushik Ramachandran", "Kevin Lybarger", "Fei Xia", "Ozlem Uzuner", "Meliha Yetisgen", "Martin Gunn"], "title": "Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches", "comment": "Submitted to LREC 2026", "summary": "Large language models (LLMs) have shown considerable promise in clinical natural language processing, yet few domain-specific datasets exist to rigorously evaluate their performance on radiology tasks. In this work, we introduce an annotated corpus of 6,393 radiology reports from 586 patients, each labeled for follow-up imaging status, to support the development and benchmarking of follow-up adherence detection systems. Using this corpus, we systematically compared traditional machine-learning classifiers, including logistic regression (LR), support vector machines (SVM), Longformer, and a fully fine-tuned Llama3-8B-Instruct, with recent generative LLMs. To evaluate generative LLMs, we tested GPT-4o and the open-source GPT-OSS-20B under two configurations: a baseline (Base) and a task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and their surrounding context. A refined prompt for GPT-OSS-20B further improved reasoning accuracy. Performance was assessed using precision, recall, and F1 scores with 95% confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846). GPT-4o (Advanced) achieved the best performance (F1 = 0.832), followed closely by GPT-OSS-20B (Advanced; F1 = 0.828). LR and SVM also performed strongly (F1 = 0.776 and 0.775), underscoring that while LLMs approach human-level agreement through prompt optimization, interpretable and resource-efficient models remain valuable baselines.", "AI": {"tldr": "本研究通过比较包括逻辑回归、支持向量机、Longformer、完全微调的Llama3-8B-Instruct以及生成式模型在内的多种模型，提出了一个用于放射学报告后续随访检测系统的数据集。结果显示，优化的生成式模型表现出色，但传统模型表现亦佳。", "motivation": "由于临床自然语言处理领域，特别是放射学领域，存在特定领域的数据集不足问题，本研究介绍了包含6393份放射报告的数据集，旨在支持和评估后续随访探测系统的研发和基准测试。", "method": "使用了传统机器学习分类器，如逻辑回归(LR)，支持向量机(SVM)，Longformer，完全微调的Llama3-8B-Instruct，以及最近的生成式大型语言模型，包括GPT-4o和开放源码模型GPT-OSS-20B。为了评估这些模型，研究者在两个设置下测试了GPT-4o和GPT-OSS-20B：基线配置和工作优化配置，后者专注于元数据、建议句子及其上下文。", "result": "对于F1分数的评估，模型性能最高的是GPT-4o（优化版本）的F1分数为0.832，GPT-OSS-20B（优化版本）紧随其后，F1分数为0.828。LR和SVM表现同样良好，F1分数分别为0.776和0.775。", "conclusion": "尽管通过提示优化大型语言模型能够接近人类的一致性，但可解释性强且资源利用率高的模型仍然是有价值的基准。"}}
{"id": "2511.11662", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11662", "abs": "https://arxiv.org/abs/2511.11662", "authors": ["Ziyuan Gao"], "title": "AGENet: Adaptive Edge-aware Geodesic Distance Learning for Few-Shot Medical Image Segmentation", "comment": "Accepted for publication in WACV 2026 (Round 2)", "summary": "Medical image segmentation requires large annotated datasets, creating a significant bottleneck for clinical applications. While few-shot segmentation methods can learn from minimal examples, existing approaches demonstrate suboptimal performance in precise boundary delineation for medical images, particularly when anatomically similar regions appear without sufficient spatial context. We propose AGENet (Adaptive Geodesic Edge-aware Network), a novel framework that incorporates spatial relationships through edge-aware geodesic distance learning. Our key insight is that medical structures follow predictable geometric patterns that can guide prototype extraction even with limited training data. Unlike methods relying on complex architectural components or heavy neural networks, our approach leverages computationally lightweight geometric modeling. The framework combines three main components: (1) An edge-aware geodesic distance learning module that respects anatomical boundaries through iterative Fast Marching refinement, (2) adaptive prototype extraction that captures both global structure and local boundary details via spatially-weighted aggregation, and (3) adaptive parameter learning that automatically adjusts to different organ characteristics. Extensive experiments across diverse medical imaging datasets demonstrate improvements over state-of-the-art methods. Notably, our method reduces boundary errors compared to existing approaches while maintaining computational efficiency, making it highly suitable for clinical applications requiring precise segmentation with limited annotated data.", "AI": {"tldr": "本文提出AGENet (Adaptive Geodesic Edge-aware Network)，一种轻量级几何建模，用于解决医疗图像分割的精确边界刻画问题，特别在少量样本和无足够背景信息的情况下进行高效的自动调整参数学习。", "motivation": "医疗图像分割需要大量注释的数据集，这对临床应用形成了巨大的瓶颈。现有的少量样本分割方法在医疗图像的精确边界划分方面表现不佳，尤其是在没有足够空间背景的解剖学相似区域中。因此，提出了一种新的解决方案AGENet，尤其在有限的训练数据下指导原型提取。", "method": "AGENet (Adaptive Geodesic Edge-aware Network) 是一种新的框架，通过边缘感知测地线距离学习来结合空间关系。该框架包含三个主要组件：(1) 一个边缘感知测地线距离学习模块，通过迭代Fast Marching细化来尊重解剖学边界，(2) 自适应原型提取，通过空间加权聚合捕获全局结构和局部边界细节，(3) 自适应参数学习，可以自动调整以适应不同的器官特性。", "result": "相比现有方法，AGENet提高了精确的边界划分性能，减少了边界错误，并且在保持计算效率的同时适应各种器官特性。", "conclusion": "广泛的实验表明AGENet在多样的医学成像数据集上提高了现有方法的性能，减少了边界错误，同时保持了计算效率，适于临床应用中需要精确分割且标注图像有限的情况。"}}
{"id": "2511.11878", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11878", "abs": "https://arxiv.org/abs/2511.11878", "authors": ["Fernanda Bufon Färber", "Iago Alves Brito", "Julia Soares Dollis", "Pedro Schindler Freire Brasil Ribeiro", "Rafael Teixeira Sousa", "Arlindo Rodrigues Galvão Filho"], "title": "MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese Speakers", "comment": "11 pages, 3 tables, 2 figures", "summary": "While large language models (LLMs) show transformative potential in healthcare, their development remains focused on high-resource languages, creating a critical barrier for others as simple translation fails to capture unique clinical and cultural nuances, such as endemic diseases. To address this, we introduce MedPT, the first large-scale, real-world corpus for Brazilian Portuguese, comprising 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol, using a hybrid quantitative-qualitative analysis to filter noise and contextually enrich thousands of ambiguous queries. We further augmented the corpus via LLM-driven annotation, classifying questions into seven semantic types to capture user intent. Our analysis reveals its thematic breadth (3,200 topics) and unique linguistic properties, like the natural asymmetry in patient-doctor communication. To validate its utility, we benchmark a medical specialty routing task: fine-tuning a 1.7B parameter model achieves an outstanding 94\\% F1-score on a 20-class setup. Furthermore, our qualitative error analysis shows misclassifications are not random but reflect genuine clinical ambiguities (e.g., between comorbid conditions), proving the dataset's deep semantic richness. We publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world.", "AI": {"tldr": "This paper presents MedPT, a large-scale, curated Brazilian Portuguese medical dataset, enabling more culturally and clinically accurate health tech innovations.", "motivation": "The development of large language models is focused on high-resource languages, neglecting the unique clinical and cultural nuances in other languages, whichmotivates the creation of MedPT to address healthcare inequalities.", "method": "The paper introduces MedPT, the first large-scale, real-world corpus for Brazilian Portuguese, which includes 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset is curated through a multi-stage process that combines quantitative and qualitative analysis. LLM-driven annotation is used to classify questions into seven semantic types to capture user intent.", "result": "The benchmarking of a medical specialty routing task achieved a 94% F1-score with a 1.7B parameter model. Qualitative error analysis showed that misclassifications reflect genuine clinical ambiguities, highlighting the dataset's deep semantic richness.", "conclusion": "The release of MedPT aims to support the development of more equitable, accurate, and culturally-aware medical technologies for Portuguese-speaking regions."}}
{"id": "2511.11700", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.11700", "abs": "https://arxiv.org/abs/2511.11700", "authors": ["Jiahui Wang", "Haiyue Zhu", "Haoren Guo", "Abdullah Al Mamun", "Cheng Xiang", "Tong Heng Lee"], "title": "EPSegFZ: Efficient Point Cloud Semantic Segmentation for Few- and Zero-Shot Scenarios with Language Guidance", "comment": "AAAI 2026", "summary": "Recent approaches for few-shot 3D point cloud semantic segmentation typically require a two-stage learning process, i.e., a pre-training stage followed by a few-shot training stage. While effective, these methods face overreliance on pre-training, which hinders model flexibility and adaptability. Some models tried to avoid pre-training yet failed to capture ample information. In addition, current approaches focus on visual information in the support set and neglect or do not fully exploit other useful data, such as textual annotations. This inadequate utilization of support information impairs the performance of the model and restricts its zero-shot ability. To address these limitations, we present a novel pre-training-free network, named Efficient Point Cloud Semantic Segmentation for Few- and Zero-shot scenarios. Our EPSegFZ incorporates three key components. A Prototype-Enhanced Registers Attention (ProERA) module and a Dual Relative Positional Encoding (DRPE)-based cross-attention mechanism for improved feature extraction and accurate query-prototype correspondence construction without pre-training. A Language-Guided Prototype Embedding (LGPE) module that effectively leverages textual information from the support set to improve few-shot performance and enable zero-shot inference. Extensive experiments show that our method outperforms the state-of-the-art method by 5.68% and 3.82% on the S3DIS and ScanNet benchmarks, respectively.", "AI": {"tldr": "A novel pre-training-free network for few- and zero-shot 3D point cloud semantic segmentation is proposed, which outperforms state-of-the-art methods on S3DIS and ScanNet benchmarks.", "motivation": "The motivation for this paper is to address the overreliance on pre-training and the inadequate utilization of support information in current few-shot 3D point cloud semantic segmentation methods, which hinder model flexibility, adaptability, and zero-shot ability.", "method": "A pre-training-free network named EPSegFZ is proposed, which includes three key components: a Prototype-Enhanced Registers Attention (ProERA) module, a Dual Relative Positional Encoding (DRPE)-based cross-attention mechanism, and a Language-Guided Prototype Embedding (LGPE) module.", "result": "Extensive experiments show that the proposed method outperforms the state-of-the-art method by 5.68% and 3.82% on the S3DIS and ScanNet benchmarks, respectively.", "conclusion": "The proposed EPSegFZ method provides an effective solution to the limitations of existing few-shot 3D point cloud semantic segmentation methods by integrating various improvements, leading to superior performance and zero-shot inference capabilities."}}
{"id": "2511.11883", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11883", "abs": "https://arxiv.org/abs/2511.11883", "authors": ["Karthikeyan K", "Raghuveer Thirukovalluru", "David Carlson"], "title": "ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts", "comment": null, "summary": "Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.", "AI": {"tldr": "ClinStructor通过将临床自由文本转换为结构化问题-答案对来提高机器学习模型在临床环境中的性能、透明度和可泛化性。", "motivation": "解决临床笔记中的无意偏见、跨临床设置的泛化能力差以及可解释性差等问题。", "method": "ClinStructor采用大型语言模型将临床自由文本转换为结构化的问题-答案对，以改善模型的透明度和可控性。", "result": "与直接微调相比，使用ClinStructor的预测性能仅略有下降（AUC下降2-3%），但提高了透明度和可解释性。", "conclusion": "ClinStructor为在临床环境中构建可靠、可解释和通用的机器学习模型提供了坚实的基础。"}}
{"id": "2511.11702", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.11702", "abs": "https://arxiv.org/abs/2511.11702", "authors": ["Lian He", "Meng Liu", "Qilang Ye", "Yu Zhou", "Xiang Deng", "Gangyi Ding"], "title": "Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement", "comment": null, "summary": "Understanding 3D scene-level affordances from natural language instructions is essential for enabling embodied agents to interact meaningfully in complex environments. However, this task remains challenging due to the need for semantic reasoning and spatial grounding. Existing methods mainly focus on object-level affordances or merely lift 2D predictions to 3D, neglecting rich geometric structure information in point clouds and incurring high computational costs. To address these limitations, we introduce Task-Aware 3D Scene-level Affordance segmentation (TASA), a novel geometry-optimized framework that jointly leverages 2D semantic cues and 3D geometric reasoning in a coarse-to-fine manner. To improve the affordance detection efficiency, TASA features a task-aware 2D affordance detection module to identify manipulable points from language and visual inputs, guiding the selection of task-relevant views. To fully exploit 3D geometric information, a 3D affordance refinement module is proposed to integrate 2D semantic priors with local 3D geometry, resulting in accurate and spatially coherent 3D affordance masks. Experiments on SceneFun3D demonstrate that TASA significantly outperforms the baselines in both accuracy and efficiency in scene-level affordance segmentation.", "AI": {"tldr": "TASA 是一个几何优化框架，结合2D语义线索和3D几何推理，来识别和细化可操作点，成功地解决了现有方法忽视3D几何结构信息并计算成本高昂的问题。", "motivation": "理解来自自然语言指令的3D场景级操作对使具身代理在复杂环境中进行有意义的交互至关重要。然而，这一任务由于需要进行语义推理和空间定位而具有挑战性。现有方法主要集中在对象级别的操作上，或者仅从2D预测提升到3D，忽视了点云中的丰富几何结构信息，并产生了高计算成本。", "method": "TASA (Task-Aware 3D Scene-level Affordance segmentation) 是一个几何优化框架，结合2D语义线索和3D几何推理，以粗到细的方式进行操作。它有一个任务相关的2D操作检测模块来识别从语言和视觉输入中的可操作点，指导任务相关视图的选择。此外，还有一个3D操作细化模块将2D语义先验与局部3D几何信息结合，生成准确且空间连续的3D操作掩模。", "result": "TASA 在准确性和效率方面显著优于基准方法。", "conclusion": "实验在SceneFun3D上显示，TASA在场景级操作分割的准确性和效率方面显著优于基准方法。"}}
{"id": "2511.11884", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11884", "abs": "https://arxiv.org/abs/2511.11884", "authors": ["Eric Hua Qing Zhang", "Julia Ive"], "title": "Context-Emotion Aware Therapeutic Dialogue Generation: A Multi-component Reinforcement Learning Approach to Language Models for Mental Health Support", "comment": null, "summary": "Mental health illness represents a substantial global socioeconomic burden, with COVID-19 further exacerbating accessibility challenges and driving increased demand for telehealth mental health support. While large language models (LLMs) offer promising solutions through 24/7 availability and non-judgmental interactions, pre-trained models often lack the contextual and emotional awareness necessary for appropriate therapeutic responses. This paper investigated the application of supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance GPT-2's capacity for therapeutic dialogue generation. The methodology restructured input formats to enable simultaneous processing of contextual information and emotional states alongside user input, employing a multi-component reward function that aligned model outputs with professional therapist responses and annotated emotions. Results demonstrated improvements through reinforcement learning over baseline GPT-2 across multiple evaluation metrics: BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581). LLM evaluation confirmed high contextual relevance and professionalism, while reinforcement learning achieved 99.34% emotion accuracy compared to 66.96% for baseline GPT-2. These findings demonstrate reinforcement learning's effectiveness in developing therapeutic dialogue systems that can serve as valuable assistive tools for therapists while maintaining essential human clinical oversight.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.11708", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11708", "abs": "https://arxiv.org/abs/2511.11708", "authors": ["Pouya Shiri", "Amirali Baniasadi"], "title": "LE-CapsNet: A Light and Enhanced Capsule Network", "comment": null, "summary": "Capsule Network (CapsNet) classifier has several advantages over CNNs, including better detection of images containing overlapping categories and higher accuracy on transformed images. Despite the advantages, CapsNet is slow due to its different structure. In addition, CapsNet is resource-hungry, includes many parameters and lags in accuracy compared to CNNs. In this work, we propose LE-CapsNet as a light, enhanced and more accurate variant of CapsNet. Using 3.8M weights, LECapsNet obtains 76.73% accuracy on the CIFAR-10 dataset while performing inference 4x faster than CapsNet. In addition, our proposed network is more robust at detecting images with affine transformations compared to CapsNet. We achieve 94.3% accuracy on the AffNIST dataset (compared to CapsNet 90.52%).", "AI": {"tldr": "LE-CapsNet, a lighter, enhanced version of CapsNet, achieves higher accuracy, faster inference, and better robustness compared to traditional CapsNet and CNNs.", "motivation": "Despite CapsNet's advantages, its slowness and high resource requirement limit its practical use. This work aims to create a more efficient and accurate alternative.", "method": "Develop LE-CapsNet using fewer parameters (3.8M) than CapsNet to achieve higher accuracy and faster inference.", "result": "LE-CapsNet achieves 76.73% accuracy on CIFAR-10 and is 4x faster than CapsNet, with high accuracy (94.3%) on the AffNIST dataset.", "conclusion": "LE-CapsNet successfully enhances the performance of CapsNet, offering a more efficient and accurate alternative for image classification tasks."}}
{"id": "2511.11922", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11922", "abs": "https://arxiv.org/abs/2511.11922", "authors": ["Karthikeyan K", "Raghuveer Thirukovalluru", "David Carlson"], "title": "Additive Large Language Models for Semi-Structured Text", "comment": null, "summary": "Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \\textbf{CALM}, short for \\textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.", "AI": {"tldr": "提出了一种适用于半结构化文本的可解释性机器学习框架CALM，该框架通过使每个文本组件的贡献清晰明了来提供可解释的风险预测，并且在临床文档中实现了与传统语言模型分类器相当的性能。", "motivation": "大型语言模型已经推动了临床文本分类的发展，但其不透明的预测仍然是实际应用的关键障碍，特别是在研究人员和医生需要理解哪些部分的病历驱动风险信号的研究和临床环境中。CALM旨在解决这个问题。", "method": "CALM是一种通过大型语言模型进行分类的可解释框架，适用于由具有语义意义的组件（如入院记录的各个部分或问答回答字段）组成的半结构化文本。CALM将结果预测为每个组件贡献的总和，使这些贡献成为前向计算的一部分，并能够为患者和群体层面提供准确的解释。其加性结构还可以生成清晰的可视化，类似于广义加性模型中的组件级风险曲线，使得学习到的关系更容易进行检查和表达。", "result": "CALM达到了与传统语言模型分类器相当的性能水平，同时提高了信任度，支持质量保证检查，并在模型开发和审计过程中揭示了临床意义的模式。", "conclusion": "CALM不仅证明了其在性能方面的有效性，而且提高了模型的信任度，有助于进行质量保证检查，并揭示了临床有意义的模式，为实际应用中的理解和信任提供了重要基础。"}}
{"id": "2511.11710", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.11710", "abs": "https://arxiv.org/abs/2511.11710", "authors": ["Zhou Xu", "Qi Wang", "Yuxiao Yang", "Luyuan Zhang", "Zhang Liang", "Yang Li"], "title": "Target-Balanced Score Distillation", "comment": null, "summary": "Score Distillation Sampling (SDS) enables 3D asset generation by distilling priors from pretrained 2D text-to-image diffusion models, but vanilla SDS suffers from over-saturation and over-smoothing. To mitigate this issue, recent variants have incorporated negative prompts. However, these methods face a critical trade-off: limited texture optimization, or significant texture gains with shape distortion. In this work, we first conduct a systematic analysis and reveal that this trade-off is fundamentally governed by the utilization of the negative prompts, where Target Negative Prompts (TNP) that embed target information in the negative prompts dramatically enhancing texture realism and fidelity but inducing shape distortions. Informed by this key insight, we introduce the Target-Balanced Score Distillation (TBSD). It formulates generation as a multi-objective optimization problem and introduces an adaptive strategy that effectively resolves the aforementioned trade-off. Extensive experiments demonstrate that TBSD significantly outperforms existing state-of-the-art methods, yielding 3D assets with high-fidelity textures and geometrically accurate shape.", "AI": {"tldr": "我们提出了一种新的3D资产生成方法——目标平衡得分蒸馏(TBSD)，该方法有效解决了现有方法中存在的纹理优化与形状失真之间的权衡问题。", "motivation": "为了克服现有方法在3D资产生成过程中出现的纹理优化与形状失真之间的权衡问题，我们进行了一项系统性分析，揭示了负提示在生成过程中的作用。", "method": "我们提出了目标平衡得分蒸馏（TBSD）方法，该方法将生成过程视为一个多目标优化问题，并引入了一种自适应策略来有效解决负提示带来的纹理与形状之间的权衡问题。", "result": "实验结果表明，TBSD在生成高质量的3D资产时，不仅在纹理真实感和保真度上有所提高，同时还保持了几何形状的准确性，显著优于现有的最先进方法。", "conclusion": "通过引入TBSD方法，我们解决了在使用负提示生成高真实感纹理的过程中出现的形状失真问题，实现了高质量3D资产的生成。"}}
{"id": "2511.11933", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11933", "abs": "https://arxiv.org/abs/2511.11933", "authors": ["Karthikeyan K", "Raghuveer Thirukovalluru", "Bhuwan Dhingra", "David Edwin Carlson"], "title": "InData: Towards Secure Multi-Step, Tool-Based Data Analysis", "comment": null, "summary": "Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.", "AI": {"tldr": "论文提出了一种安全驱动的方法，限制大型语言模型直接生成代码和访问敏感数据，引入了InData数据集来评估模型的多步骤工具推理能力，并在三种难度级别的任务上进行了评估。", "motivation": "大型语言模型直接在数据库上生成和执行代码的方法在应用于敏感数据时存在重大安全风险。为此，提出了一个安全驱动的改进方案，并评估模型的多步骤工具推理能力。", "method": "提出了一种安全驱动的替代方案，限制LLM直接生成代码和访问数据，要求它们仅通过一组预定义的安全验证工具与数据互动。为了评估LLM的多步骤工具推理能力，引入了Indirect Data Engagement (InData)数据集。", "result": "在InData数据集上评估了15个开源LLM，结果显示大型模型（如gpt-oss-120b）在简单任务上达到高精度（97.3%），但在困难任务上的性能下降明显（69.6%），表明现有LLM仍缺乏强大的多步骤工具推理能力。", "conclusion": "论文介绍了InData数据集的用途及实验结果，强调了开发和评估具有多步骤工具使用能力的LLM的重要性，并将公开发布该数据集和代码。"}}
{"id": "2511.11716", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11716", "abs": "https://arxiv.org/abs/2511.11716", "authors": ["Sudhakar Sah", "Nikhil Chabbra", "Matthieu Durnerin"], "title": "CompressNAS : A Fast and Efficient Technique for Model Compression using Decomposition", "comment": "11 pages, 6 figures", "summary": "Deep Convolutional Neural Networks (CNNs) are increasingly difficult to deploy on microcontrollers (MCUs) and lightweight NPUs (Neural Processing Units) due to their growing size and compute demands. Low-rank tensor decomposition, such as Tucker factorization, is a promising way to reduce parameters and operations with reasonable accuracy loss. However, existing approaches select ranks locally and often ignore global trade-offs between compression and accuracy. We introduce CompressNAS, a MicroNAS-inspired framework that treats rank selection as a global search problem. CompressNAS employs a fast accuracy estimator to evaluate candidate decompositions, enabling efficient yet exhaustive rank exploration under memory and accuracy constraints. In ImageNet, CompressNAS compresses ResNet-18 by 8x with less than 4% accuracy drop; on COCO, we achieve 2x compression of YOLOv5s without any accuracy drop and 2x compression of YOLOv5n with a 2.5% drop. Finally, we present a new family of compressed models, STResNet, with competitive performance compared to other efficient models.", "AI": {"tldr": "提出CompressNAS框架，提高了在微控制器和轻量级神经处理单元上部署深度卷积神经网络的可能性，同时保持较高准确性。", "motivation": "传统的低秩张量分解方法局部选择秩，忽视了压缩和准确性之间的全局权衡。", "method": "使用了类似于MicroNAS的框架CompressNAS，该框架将秩选择视为全局搜索问题，并采用了一个快速的准确性估计器来评估候选分解，从而在内存和准确性约束下高效地探索不同的秩。", "result": "在ImageNet上，ResNet-18通过CompressNAS实现了几乎8倍的压缩，并且准确率下降不到4%；在COCO上，实现了YOLOv5s模型2倍的压缩而没有任何准确率下降，并且YOLOv5n模型实现了2倍的压缩，准确率下降仅为2.5%。", "conclusion": "引入了STResNet，这是一种新型的压缩模型家族，与现有的高效模型相比具有竞争力。"}}
{"id": "2511.11946", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11946", "abs": "https://arxiv.org/abs/2511.11946", "authors": ["Hadi Sheikhi", "Chenyang Huang", "Osmar R. Zaïane"], "title": "Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization", "comment": null, "summary": "Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.", "AI": {"tldr": "研究提出了评估大型语言模型在知识图谱对话生成中使用外部知识情况的程序LLM-KAT，并通过实体匿名化技术提高了模型利用外部知识的效果。", "motivation": "观察到尽管大型语言模型在各种NLP任务上取得显著进展，但在将知识图谱整合进对话生成方面尚未充分发挥潜力。", "method": "提出了LLM-KAT评估程序来测量大型语言模型对话生成中对知识的依赖程度，同时提出了一种实体匿名化方法，鼓励模型更好地利用外部知识。", "result": "实验结果表明，所提出的方法能够改善大型语言模型对外部知识的依赖。", "conclusion": "实体匿名化技术作为一种简单却有效的方法，能够显著提高大型语言模型在对话生成任务中对外部知识的利用效率。"}}
{"id": "2511.11720", "categories": ["cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11720", "abs": "https://arxiv.org/abs/2511.11720", "authors": ["Jiao Chen", "Haoyi Wang", "Jianhua Tang", "Junyi Wang"], "title": "AdaptFly: Prompt-Guided Adaptation of Foundation Models for Low-Altitude UAV Networks", "comment": null, "summary": "Low-altitude Unmanned Aerial Vehicle (UAV) networks rely on robust semantic segmentation as a foundational enabler for distributed sensing-communication-control co-design across heterogeneous agents within the network. However, segmentation foundation models deteriorate quickly under weather, lighting, and viewpoint drift. Resource-limited UAVs cannot run gradient-based test-time adaptation, while resource-massive UAVs adapt independently, wasting shared experience. To address these challenges, we propose AdaptFly, a prompt-guided test-time adaptation framework that adjusts segmentation models without weight updates. AdaptFly features two complementary adaptation modes. For resource-limited UAVs, it employs lightweight token-prompt retrieval from a shared global memory. For resource-massive UAVs, it uses gradient-free sparse visual prompt optimization via Covariance Matrix Adaptation Evolution Strategy. An activation-statistic detector triggers adaptation, while cross-UAV knowledge pool consolidates prompt knowledge and enables fleet-wide collaboration with negligible bandwidth overhead. Extensive experiments on UAVid and VDD benchmarks, along with real-world UAV deployments under diverse weather conditions, demonstrate that AdaptFly significantly improves segmentation accuracy and robustness over static models and state-of-the-art TTA baselines. The results highlight a practical path to resilient, communication-efficient perception in the emerging low-altitude economy.", "AI": {"tldr": "本文提出了AdaptFly框架，通过两种适应模式解决了低空无人机网络中的语义分割问题，显著提高了分割精准度和鲁棒性，适用于低资源和高资源的无人机，并展示了实际应用中的潜力。", "motivation": "低空无人机网络依赖于健壮的语义分割作为分布式感知、通信和控制协同设计的基础。然而，当前的语义分割基础模型在面对天气、光照和视角变化时表现不佳。此外，资源受限的无人机无法运行基于梯度的测试时间适应，同时资源丰富的无人机只能独立适应，共享经验的能力不足。为了应对这些挑战，提出了一个新的解决方案。", "method": "提出了一种名为AdaptFly的启发式测试时间适应框架，该框架使用两种互补的适应模式。对于资源受限的无人机，AdaptFly采用从共享全局内存检索的轻量级令牌提示。对于资源丰富的无人机，它使用无梯度稀疏视觉提示优化，方法为协方差矩阵适应演化策略。通过激活统计检测器触发适应，并通过跨无人机知识池进行提示信息整合，实现队伍级别的协作，同时对带宽的消耗可以忽略不计。", "result": "在UAVid和VDD基准测试以及多样天气条件下的真实世界无人机部署中进行的广泛实验表明，AdaptFly显著提高了分割精度和鲁棒性，超越了静态模型和最先进的TTA基线。", "conclusion": "该论文提出的AdaptFly框架为低空无人机感知的弹性及通信效率提供了一个实用的解决方案。这一框架拥有提高分割模型适应性的能力，不受天气、照明和视角变化的影响，证明了其在日益发展的低空经济中的应用潜力。"}}
{"id": "2511.11966", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.11966", "abs": "https://arxiv.org/abs/2511.11966", "authors": ["Steven Cao", "Gregory Valiant", "Percy Liang"], "title": "On the Entropy Calibration of Language Models", "comment": "Neurips 2025", "summary": "We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.", "AI": {"tldr": "该研究探讨了语言模型熵校准问题，发现模型规模增加并不会显著改善校准误差，但理论上存在一种方法可以同时减少熵并保持对数损失。", "motivation": "研究语言模型产生的文本的熵是否与人类文本上的对数损失相匹配的问题，特别是探讨规模增加时校准是否存在改进的可能及其理论上的可能性。", "method": "我们首先在简化理论设置中研究了不准确度随数据集大小的变化情况，接着在0.5B到70B参数的语言模型中进行了实证测量。最后，我们从理论上证明了减少熵同时保持对数损失的可能性。", "result": "发现不准确度的积累随模型大小的增加改善非常缓慢，而且即使使用更大的模型，我们也需要与较小的模型相似程度的截断。", "conclusion": "证明了在理论上，如果我们能够预测文本的未来熵，那么减少熵同时保持对数损失是可能的。"}}
{"id": "2511.11725", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11725", "abs": "https://arxiv.org/abs/2511.11725", "authors": ["Zekai Shi", "Zhixi Cai", "Kalin Stefanov"], "title": "Do Blind Spots Matter for Word-Referent Mapping? A Computational Study with Infant Egocentric Video", "comment": null, "summary": "Typically, children start to learn their first words between 6 and 9 months, linking spoken utterances to their visual referents. Without prior knowledge, a word encountered for the first time can be interpreted in countless ways; it might refer to any of the objects in the environment, their components, or attributes. Using longitudinal, egocentric, and ecologically valid data from the experience of one child, in this work, we propose a self-supervised and biologically plausible strategy to learn strong visual representations. Our masked autoencoder-based visual backbone incorporates knowledge about the blind spot in human eyes to define a novel masking strategy. This mask and reconstruct approach attempts to mimic the way the human brain fills the gaps in the eyes' field of view. This represents a significant shift from standard random masking strategies, which are difficult to justify from a biological perspective. The pretrained encoder is utilized in a contrastive learning-based video-text model capable of acquiring word-referent mappings. Extensive evaluation suggests that the proposed biologically plausible masking strategy is at least as effective as random masking for learning word-referent mappings from cross-situational and temporally extended episodes.", "AI": {"tldr": "研究提出了一种符合生物学原理的视觉表示学习方法，通过遮罩自动编码器和基于人类眼睛盲点的遮罩策略，可以高效学习词与环境物体之间的映射关系。", "motivation": "儿童在6到9个月大时开始学习他们的第一个词，将口语表达与视觉引用联系起来。对于初次遇到的词，没有先验知识的情况下，它可以指代环境中的任何对象、它们的组成部分或属性等。该研究旨在利用纵向、自我中心且生态有效的单一儿童体验数据，探索一种自监督且符合生物学原理的学习方法。", "method": "本研究提出了一种自我监督且符合生物学原理的学习视觉表示的方法。使用遮罩自动编码器作为视觉基础结构，并根据人眼的盲点定义了一种新的遮罩策略。这种方法试图模仿人脑填充眼睛视野缺口的方式，与标准的随机遮罩策略相比更容易从生物学角度进行解释。", "result": "研究证明，所提出的符合生物学原理的遮罩策略在学习跨情境和时间扩展场景中的词-引用映射至少与随机遮罩策略一样有效。", "conclusion": "通过对新对象的模仿自监督学习策略，本研究提出的方法符合理性且人类视觉感知的生物学原理，同时在学习词与物理环境之间联系方面表现出色。"}}
{"id": "2511.11978", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11978", "abs": "https://arxiv.org/abs/2511.11978", "authors": ["Hui Huang", "Yanping Chen", "Ruizhang Huang", "Chuan Lin", "Yongbin Qin"], "title": "A Reasoning Paradigm for Named Entity Recognition", "comment": "Accepted at AAAI 2026", "summary": "Generative LLMs typically improve Named Entity Recognition (NER) performance through instruction tuning. They excel at generating entities by semantic pattern matching but lack an explicit, verifiable reasoning mechanism. This \"cognitive shortcutting\" leads to suboptimal performance and brittle generalization, especially in zero-shot and lowresource scenarios where reasoning from limited contextual cues is crucial. To address this issue, a reasoning framework is proposed for NER, which shifts the extraction paradigm from implicit pattern matching to explicit reasoning. This framework consists of three stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset annotated with NER-oriented CoTs is generated, which contain task-relevant reasoning chains. Then, they are used to tune the NER model to generate coherent rationales before deriving the final answer. Finally, a reasoning enhancement stage is implemented to optimize the reasoning process using a comprehensive reward signal. This stage ensures explicit and verifiable extractions. Experiments show that ReasoningNER demonstrates impressive cognitive ability in the NER task, achieving competitive performance. In zero-shot settings, it achieves state-of-the-art (SOTA) performance, outperforming GPT-4 by 12.3 percentage points on the F1 score. Analytical results also demonstrate its great potential to advance research in reasoningoriented information extraction. Our codes are available at https://github.com/HuiResearch/ReasoningIE.", "AI": {"tldr": "本文提出了一种新的用于命名实体识别的显式推理框架ReasoningNER，实现了比现有方法更好的性能，特别是在零样本情况下。", "motivation": "当前生成型大语言模型（LLMs）在通过指令调优提高命名实体识别（NER）性能时，虽然擅长通过语义模式匹配生成实体，但由于缺乏显式、可验证的推理机制，导致性能不佳，特别是在零样本和低资源场景中，从有限的上下文中推理尤为重要。", "method": "本文提出了一种用于命名实体识别（NER）的推理框架，该框架将提取范式从隐含的模式匹配转向显式的推理。该框架包含三个阶段：思维链（CoT）生成、CoT调优和推理增强。", "result": "实验表明，ReasoningNER在命名实体识别任务中展现出卓越的认知能力，取得了具有竞争力的性能，在零样本设置下，其F1分数比GPT-4高出12.3个百分点。", "conclusion": "该方法的分析结果也展示了在推理导向的信息提取研究中具有巨大的潜力。"}}
{"id": "2511.11730", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11730", "abs": "https://arxiv.org/abs/2511.11730", "authors": ["Yongjun Xiao", "Dian Meng", "Xinlei Huang", "Yanran Liu", "Shiwei Ruan", "Ziyue Qiao", "Xubin Zheng"], "title": "GROVER: Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion", "comment": "8 pages, 3 figures, Accepted to AAAI 2026", "summary": "Effectively modeling multimodal spatial omics data is critical for understanding tissue complexity and underlying biological mechanisms. While spatial transcriptomics, proteomics, and epigenomics capture molecular features, they lack pathological morphological context. Integrating these omics with histopathological images is therefore essential for comprehensive disease tissue analysis. However, substantial heterogeneity across omics, imaging, and spatial modalities poses significant challenges. Naive fusion of semantically distinct sources often leads to ambiguous representations. Additionally, the resolution mismatch between high-resolution histology images and lower-resolution sequencing spots complicates spatial alignment. Biological perturbations during sample preparation further distort modality-specific signals, hindering accurate integration. To address these challenges, we propose Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion (GROVER), a novel framework for adaptive integration of spatial multi-omics data. GROVER leverages a Graph Convolutional Network encoder based on Kolmogorov-Arnold Networks to capture the nonlinear dependencies between each modality and its associated spatial structure, thereby producing expressive, modality-specific embeddings. To align these representations, we introduce a spot-feature-pair contrastive learning strategy that explicitly optimizes the correspondence across modalities at each spot. Furthermore, we design a dynamic expert routing mechanism that adaptively selects informative modalities for each spot while suppressing noisy or low-quality inputs. Experiments on real-world spatial omics datasets demonstrate that GROVER outperforms state-of-the-art baselines, providing a robust and reliable solution for multimodal integration.", "AI": {"tldr": "本文提出了一种名为GROVER的新框架，用以整合空间多组学数据，通过对比学习策略和动态专家路由机制来克服数据异质性和分辨率不匹配的挑战，实验表明GROVER优于现有基准方法。", "motivation": "空间转录组学、蛋白质组学和表观基因组学捕获分子特征，但缺乏病理形态学背景。将这些组学与组织学图象整合在一起对于全面的疾病组织分析是必不可少的。然而，组学、成像和空间模态之间的大量异质性提出了重大挑战。", "method": "本研究提出了一种新的框架GROVER，用于自适应整合空间多组学数据。GROVER利用基于Kolmogorov-Arnold网络的图卷积网络编码器来捕获各个模态与其相关空间结构之间的非线性依赖关系，从而产生具有表现力的模态特定嵌入。为了对齐这些表示，我们引入了一种斑点-特征-对对比学习策略，该策略在每个斑点上显式优化跨模态的对应关系。此外，我们设计了一个动态专家路由机制，该机制自适应地选择每个斑点的有用模态，同时抑制噪声或低质量输入。", "result": "在真实世界的空间组学数据集上的实验表明，GROVER优于最先进的基线方法，为多模态集成提供了强大可靠的解决方案。", "conclusion": "本研究提出了一种新的框架GROVER，用于自适应整合空间多组学数据，实验显示其在整合多组学数据方面优于现有方法。"}}
{"id": "2511.12001", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.12001", "abs": "https://arxiv.org/abs/2511.12001", "authors": ["Eunkyu Park", "Wesley Hanwen Deng", "Vasudha Varadarajan", "Mingxi Yan", "Gunhee Kim", "Maarten Sap", "Motahhare Eslami"], "title": "Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations", "comment": "Under review; 16 pages, 15 figures", "summary": "Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.", "AI": {"tldr": "论文研究了Chain-of-Thought（CoT）解释在多模态道德场景中的双刃剑作用，发现自信的解释风格可能抑制用户识别错误的能力。", "motivation": "探讨解释作为透明工具时，如何防止使用者陷入确认偏误，特别是当输出看起来合理时就假设推理正确。", "method": "通过系统地扰动推理链并操纵传递语气来研究CoT解释在多模态道德场景中的双刃剑作用。具体分析视觉语言模型中的推理错误及其对用户信任和错误检测能力的影响。", "result": "发现用户经常将信任与结果一致相等同，即使推理有误也继续依赖系统；自信的语气会抑制错误检测但维持依赖性，表明传递风格可以覆盖正确性。", "conclusion": "研究结果强调CoT解释既可以澄清也可以误导，突显出NLP系统应提供鼓励审查和批判性思维的解释，而非盲目信任。"}}
{"id": "2511.11732", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11732", "abs": "https://arxiv.org/abs/2511.11732", "authors": ["Aditya Mehta", "Swarnim Chaudhary", "Pratik Narang", "Jagat Sesh Challa"], "title": "Exposing DeepFakes via Hyperspectral Domain Mapping", "comment": "Accepted at AAAI 2026 Student Abstract", "summary": "Modern generative and diffusion models produce highly realistic images that can mislead human perception and even sophisticated automated detection systems. Most detection methods operate in RGB space and thus analyze only three spectral channels. We propose HSI-Detect, a two-stage pipeline that reconstructs a 31-channel hyperspectral image from a standard RGB input and performs detection in the hyperspectral domain. Expanding the input representation into denser spectral bands amplifies manipulation artifacts that are often weak or invisible in the RGB domain, particularly in specific frequency bands. We evaluate HSI-Detect across FaceForensics++ dataset and show the consistent improvements over RGB-only baselines, illustrating the promise of spectral-domain mapping for Deepfake detection.", "AI": {"tldr": "HSI-Detect amplifies manipulation artifacts in the hyperspectral domain from RGB input and shows better performance over RGB-only methods in Deepfake detection.", "motivation": "To address the limitation of most detection methods operating in RGB space, where weak or invisible manipulation artifacts in the RGB domain can be amplified in the hyperspectral domain.", "method": "HSI-Detect, a two-stage pipeline that reconstructs a 31-channel hyperspectral image from a standard RGB input and performs detection in the hyperspectral domain.", "result": "Evaluating HSI-Detect over the FaceForensics++ dataset shows consistent improvements over RGB-only baselines.", "conclusion": "HSI-Detect demonstrates the promise of spectral-domain mapping for Deepfake detection, effectively amplifying manipulation artifacts that are otherwise weak or invisible in solely RGB-based detection methods."}}
{"id": "2511.12014", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.12014", "abs": "https://arxiv.org/abs/2511.12014", "authors": ["Truong Vo", "Sanmi Koyejo"], "title": "CURE: Cultural Understanding and Reasoning Evaluation - A Framework for \"Thick\" Culture Alignment Evaluation in LLMs", "comment": "7 pages, 5 figures", "summary": "Large language models (LLMs) are increasingly deployed in culturally diverse environments, yet existing evaluations of cultural competence remain limited. Existing methods focus on de-contextualized correctness or forced-choice judgments, overlooking the need for cultural understanding and reasoning required for appropriate responses. To address this gap, we introduce a set of benchmarks that, instead of directly probing abstract norms or isolated statements, present models with realistic situational contexts that require culturally grounded reasoning. In addition to the standard Exact Match metric, we introduce four complementary metrics (Coverage, Specificity, Connotation, and Coherence) to capture different dimensions of model's response quality. Empirical analysis across frontier models reveals that thin evaluation systematically overestimates cultural competence and produces unstable assessments with high variance. In contrast, thick evaluation exposes differences in reasoning depth, reduces variance, and provides more stable, interpretable signals of cultural understanding.", "AI": {"tldr": "针对大型语言模型在文化多样性环境中的表现，引入了一套新的基准测试和补充度量标准来评估模型的文化理解能力，显示深层评估方法能更准确反映模型的文化能力。", "motivation": "现有方法对大型语言模型的文化能力评估有限，主要集中于去情境化的正确性或强加选择判断，忽略了适当响应所需的文化理解和推理。", "method": "提出了一套基准测试，该测试不直接探测抽象规范或孤立的陈述，而是将模型置于需要文化相关性推理的真实情境中。除了标准的精确匹配度量标准外，还引入了四个补充度量标准（Coverage, Specificity, Connotation, 和 Coherence），以捕捉模型响应质量的不同维度。", "result": "实证分析显示，浅层评估系统性地高估了文化能力，并且产生了高方差的不稳定评估结果。相比之下，深层评估揭示了推理深度的差异，减少了方差，提供了关于文化理解的更稳定、可解释的信号。", "conclusion": "通过引入新的测试基准和补充评估度量，深层评估展示了其相较于标准评估方法在揭示模型文化理解能力方面的优越性。"}}
{"id": "2511.11735", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.11735", "abs": "https://arxiv.org/abs/2511.11735", "authors": ["Yonatan Sverdlov", "Eitan Rosen", "Nadav Dym"], "title": "Toward bilipshiz geometric models", "comment": null, "summary": "Many neural networks for point clouds are, by design, invariant to the symmetries of this datatype: permutations and rigid motions. The purpose of this paper is to examine whether such networks preserve natural symmetry aware distances on the point cloud spaces, through the notion of bi-Lipschitz equivalence. This inquiry is motivated by recent work in the Equivariant learning literature which highlights the advantages of bi-Lipschitz models in other scenarios.\n  We consider two symmetry aware metrics on point clouds: (a) The Procrustes Matching (PM) metric and (b) Hard Gromov Wasserstien distances. We show that these two distances themselves are not bi-Lipschitz equivalent, and as a corollary deduce that popular invariant networks for point clouds are not bi-Lipschitz with respect to the PM metric. We then show how these networks can be modified so that they do obtain bi-Lipschitz guarantees. Finally, we provide initial experiments showing the advantage of the proposed bi-Lipschitz model over standard invariant models, for the tasks of finding correspondences between 3D point clouds.", "AI": {"tldr": "该论文探讨了点云网络是否保持自然对称感知距离，并发现流行的不变性点云网络在Procrustes匹配距离下不是bi-Lipschitz的，提出了可成为bi-Lipschitz模型的修改方案，并通过实验验证了新的模型在3D点云对应任务中的优势。", "motivation": "受到Equiavariant learning文献中bi-Lipschitz模型优点的启发，本文探讨点云网络是否保持自然对称感知距离。", "method": "该研究考虑了两种适用于点云的对称感知度量：Procrustes匹配距离和Hard Gromov Wasserstien距离，并分析了这些距离是否是bi-Lipschitz等价的。", "result": "结果表明，上述两种度量本身不形成bi-Lipschitz等价关系，从而得出结论，流行的不变性点云网络在Procrustes匹配距离下不是bi-Lipschitz的。此外，论文提出了一种可以保障网络达到bi-Lipschitz性质的修改方案。", "conclusion": "初步实验表明，提出的新bi-Lipschitz模型在3D点云对应任务中优于标准的不变性模型。"}}
{"id": "2511.12109", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12109", "abs": "https://arxiv.org/abs/2511.12109", "authors": ["Felipe Fujita", "Hideyuki Takada"], "title": "Exploring Parameter-Efficient Fine-Tuning and Backtranslation for the WMT 25 General Translation Task", "comment": null, "summary": "In this paper, we explore the effectiveness of combining fine-tuning and backtranslation on a small Japanese corpus for neural machine translation. Starting from a baseline English{\\textrightarrow}Japanese model (COMET = 0.460), we first apply backtranslation (BT) using synthetic data generated from monolingual Japanese corpora, yielding a modest increase (COMET = 0.468). Next, we fine-tune (FT) the model on a genuine small parallel dataset drawn from diverse Japanese news and literary corpora, achieving a substantial jump to COMET = 0.589 when using Mistral 7B. Finally, we integrate both backtranslation and fine-tuning{ -- }first augmenting the small dataset with BT generated examples, then adapting via FT{ -- }which further boosts performance to COMET = 0.597. These results demonstrate that, even with limited training data, the synergistic use of backtranslation and targeted fine-tuning on Japanese corpora can significantly enhance translation quality, outperforming each technique in isolation. This approach offers a lightweight yet powerful strategy for improving low-resource language pairs.", "AI": {"tldr": "该研究发现，即使在有限的训练数据下，通过回译与针对性微调的协同作用也能大幅改善日语神经机器翻译的质量。", "motivation": "研究的动机在于探索结合回译和微调的效果，特别是在小样本日语语料库上的神经机器翻译任务中。", "method": "本文中使用了回译（BT）和微调（FT）的方法来提升小规模日语语料库的神经机器翻译效果。首先，基于单语日语文本生成的合成数据进行回译，然后使用从小规模的真实平行数据集中抽取的日语新闻和文学语料库数据进行微调。最后，通过结合回译生成的样本先进行数据扩充，再进行微调，以进一步提升翻译质量。", "result": "实验结果显示，该方法显著提升了翻译质量，从基础模型的 COMET = 0.460 提升到 COMET = 0.597。", "conclusion": "研究结论表明，回译和微调结合的方法对于低资源语言对的翻译质量改进是一种轻量级但强劲的策略，其效果优于单独使用这些技术。"}}
{"id": "2511.11751", "categories": ["cs.CV", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11751", "abs": "https://arxiv.org/abs/2511.11751", "authors": ["Sanchit Sinha", "Guangzhi Xiong", "Zhenghao He", "Aidong Zhang"], "title": "Concept-RuleNet: Grounded Multi-Agent Neurosymbolic Reasoning in Vision Language Models", "comment": "AAAI 2026 (oral)", "summary": "Modern vision-language models (VLMs) deliver impressive predictive accuracy yet offer little insight into 'why' a decision is reached, frequently hallucinating facts, particularly when encountering out-of-distribution data. Neurosymbolic frameworks address this by pairing black-box perception with interpretable symbolic reasoning, but current methods extract their symbols solely from task labels, leaving them weakly grounded in the underlying visual data. In this paper, we introduce a multi-agent system - Concept-RuleNet that reinstates visual grounding while retaining transparent reasoning. Specifically, a multimodal concept generator first mines discriminative visual concepts directly from a representative subset of training images. Next, these visual concepts are utilized to condition symbol discovery, anchoring the generations in real image statistics and mitigating label bias. Subsequently, symbols are composed into executable first-order rules by a large language model reasoner agent - yielding interpretable neurosymbolic rules. Finally, during inference, a vision verifier agent quantifies the degree of presence of each symbol and triggers rule execution in tandem with outputs of black-box neural models, predictions with explicit reasoning pathways. Experiments on five benchmarks, including two challenging medical-imaging tasks and three underrepresented natural-image datasets, show that our system augments state-of-the-art neurosymbolic baselines by an average of 5% while also reducing the occurrence of hallucinated symbols in rules by up to 50%.", "AI": {"tldr": "本文提出了一个名为Concept-RuleNet的多智能体系统，改进了神经符号模型的性能和可解释性，它增强视觉定位并减少了符号幻觉情况，这是通过结合多模态概念生成、符号推理和视觉验证完成的。", "motivation": "现代视觉语言模型虽然预测准确率很高，但往往无法提供决策的依据，特别在面对分布外的数据时容易产生幻觉事实。神经符号框架通过结合黑盒感知和可解释的符号推理来解决这一问题，但目前的方法只能从任务标签中提取符号，导致符号与视觉数据的底层联系较弱。因此，作者旨在重新建立视觉定位并保持透明推理的系统。", "method": "在本文中，作者提出了一种多智能体系统——Concept-RuleNet，它结合了多模态概念生成器、大型语言模型推理智能体和视觉验证器智能体。首先，多模态概念生成器从训练图像的代表性子集中直接挖掘辨别性视觉概念。然后，这些视觉概念用于条件符号发现，这使得生成的符号扎根于真实的图像统计，并减轻了标签偏差。之后，大语言模型推理智能体将符号组合成可执行的一阶规则，产生可解释的神经符号规则。在推理过程中，视觉验证器智能体量化每个符号的存在程度，并与黑盒神经模型的输出配合触发规则执行，提供具有显式推理路径的预测。", "result": "实验结果表明，在包括两个具有挑战性的医疗成像任务和三个自然图像数据集的五个基准测试上，该系统相比于最先进的神经符号基线模型提升了平均5%的性能，并且减少了规则中出现幻觉符号的情况，最高可达50%。", "conclusion": "本文提出的Concept-RuleNet系统通过多智能体方法提高了神经符号模型的性能和可解释性，其方法已经在多个数据集上得到了验证，展示了其有效性和鲁棒性。"}}
{"id": "2511.12116", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12116", "abs": "https://arxiv.org/abs/2511.12116", "authors": ["Piotr Pęzik", "Konrad Kaczyński", "Maria Szymańska", "Filip Żarnecki", "Zuzanna Deckert", "Jakub Kwiatkowski", "Wojciech Janowski"], "title": "LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.", "AI": {"tldr": "提出LLMLagBench基准，评估大语言模型训练数据的时间边界，提高模型输出信息的可靠性。", "motivation": "大语言模型由于时间截止的存在，会潜在地将过时的时间敏感信息与一般知识混合，影响响应的准确性。因此，需要一个系统的方法来识别模型训练数据的时间边界。", "method": "提出LLMLagBench基准，评估LLM对于近期事件的知识，识别模型训练数据的时间边界。", "result": "", "conclusion": "引入LLMLagBench作为系统方法来识别LLM训练数据的最早可能的时间边界，并应用该基准评估了一组具有明确和未明确训练截止日期的LLM，通过手动验证和公开发布的LLM预训练信息进行可靠性评估。"}}
{"id": "2511.11754", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11754", "abs": "https://arxiv.org/abs/2511.11754", "authors": ["Stanislav Selitskiy"], "title": "Batch Transformer Architecture: Case of Synthetic Image Generation for Emotion Expression Facial Recognition", "comment": null, "summary": "A novel Transformer variation architecture is proposed in the implicit sparse style. Unlike \"traditional\" Transformers, instead of attention to sequential or batch entities in their entirety of whole dimensionality, in the proposed Batch Transformers, attention to the \"important\" dimensions (primary components) is implemented. In such a way, the \"important\" dimensions or feature selection allows for a significant reduction of the bottleneck size in the encoder-decoder ANN architectures. The proposed architecture is tested on the synthetic image generation for the face recognition task in the case of the makeup and occlusion data set, allowing for increased variability of the limited original data set.", "AI": {"tldr": "提出了一种新的Batch Transformers架构，通过关注“重要”维度，有助于减少编码器和解码器架构的瓶颈。实验在合成图像生成中取得了良好效果。", "motivation": "减少传统Transformer架构中的瓶颈问题，通过关注“重要”特征维度来提高模型效率。", "method": "提出了在隐式稀疏风格中的新型Transformer变体架构。与传统Transformer不同，新架构（Batch Transformers）实现了对“重要”维度或主成分的关注，而不是对序列或批量实体的整体维度进行关注。这样可以显著减少编码器-解码器ANN架构中的瓶颈大小。", "result": "该架构在带有化妆和遮挡数据集的合成图像生成中用于面部识别任务，增加了有限原始数据集的多样性。", "conclusion": "通过实验验证，新的Batch Transformers架构在增加数据多样性的同时，保持了有效的关注机制，适合用于复杂数据集如带有遮挡和化妆的面部识别数据。"}}
{"id": "2511.12130", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12130", "abs": "https://arxiv.org/abs/2511.12130", "authors": ["Bingbing Wang", "Zhixin Bai", "Zhengda Jin", "Zihan Wang", "Xintong Song", "Jingjie Lin", "Sixuan Li", "Jing Li", "Ruifeng Xu"], "title": "PRISM of Opinions: A Persona-Reasoned Multimodal Framework for User-centric Conversational Stance Detection", "comment": null, "summary": "The rapid proliferation of multimodal social media content has driven research in Multimodal Conversational Stance Detection (MCSD), which aims to interpret users' attitudes toward specific targets within complex discussions. However, existing studies remain limited by: **1) pseudo-multimodality**, where visual cues appear only in source posts while comments are treated as text-only, misaligning with real-world multimodal interactions; and **2) user homogeneity**, where diverse users are treated uniformly, neglecting personal traits that shape stance expression. To address these issues, we introduce **U-MStance**, the first user-centric MCSD dataset, containing over 40k annotated comments across six real-world targets. We further propose **PRISM**, a **P**ersona-**R**easoned mult**I**modal **S**tance **M**odel for MCSD. PRISM first derives longitudinal user personas from historical posts and comments to capture individual traits, then aligns textual and visual cues within conversational context via Chain-of-Thought to bridge semantic and pragmatic gaps across modalities. Finally, a mutual task reinforcement mechanism is employed to jointly optimize stance detection and stance-aware response generation for bidirectional knowledge transfer. Experiments on U-MStance demonstrate that PRISM yields significant gains over strong baselines, underscoring the effectiveness of user-centric and context-grounded multimodal reasoning for realistic stance understanding.", "AI": {"tldr": "A new method called PRISM is proposed for Multimodal Conversational Stance Detection, which uses user-centric and context-aware techniques and demonstrates significant performance improvements over baseline methods on a newly developed dataset.", "motivation": "To address the issues of pseudo-multimodality and user homogeneity in existing Multimodal Conversational Stance Detection (MCSD) studies, which misalign with real-world multimodal interactions and neglect personal traits that shape stance expression.", "method": "PRISM, a Persona-Reasoned multimodal Stance Model, is introduced, which derives longitudinal user personas to capture individual traits and aligns textual and visual cues within conversational context to bridge semantic and pragmatic gaps across modalities. It also employs a mutual task reinforcement mechanism to optimize stance detection and stance-aware response generation.", "result": "Experiments on the newly introduced user-centric MCSD dataset, U-MStance, show that PRISM yields significant improvements over strong baselines.", "conclusion": "The effectiveness of user-centric and context-grounded multimodal reasoning for realistic stance understanding is underscored by the performance increases over baselines on the U-MStance dataset."}}
{"id": "2511.11780", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11780", "abs": "https://arxiv.org/abs/2511.11780", "authors": ["Hossein Mohebbi", "Mohammed Abdulrahman", "Yanting Miao", "Pascal Poupart", "Suraj Kothawade"], "title": "Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing", "comment": null, "summary": "Recent advances in text-to-image generation have produced strong single-shot models, yet no individual system reliably executes the long, compositional prompts typical of creative workflows. We introduce Image-POSER, a reflective reinforcement learning framework that (i) orchestrates a diverse registry of pretrained text-to-image and image-to-image experts, (ii) handles long-form prompts end-to-end through dynamic task decomposition, and (iii) supervises alignment at each step via structured feedback from a vision-language model critic. By casting image synthesis and editing as a Markov Decision Process, we learn non-trivial expert pipelines that adaptively combine strengths across models. Experiments show that Image-POSER outperforms baselines, including frontier models, across industry-standard and custom benchmarks in alignment, fidelity, and aesthetics, and is consistently preferred in human evaluations. These results highlight that reinforcement learning can endow AI systems with the capacity to autonomously decompose, reorder, and combine visual models, moving towards general-purpose visual assistants.", "AI": {"tldr": "介绍了一个名为Image-POSER的系统，能够通过动态任务分解和强化学习方法，有效处理长文本提示，生成高质量的图像，展示出在视觉助手领域的潜力。", "motivation": "当前的文本到图像生成系统难以可靠地执行长且具有组合性的提示，因此提出了Image-POSER来解决这一问题。", "method": "引入了一个名为Image-POSER的反思强化学习框架，该框架能够协调多种预训练的文本到图像和图像到图像的专家系统，处理长文本提示并通过动态任务分解和结构化反馈监督来提升效果。", "result": "实验结果显示，Image-POSER在行业标准和定制基准上的对齐度、保真度和美学评分方面，以及在人类评估中的偏好上都优于基线以及前沿模型。", "conclusion": "Image-POSER证明了强化学习能够赋予AI系统自主分解、重新排序和组合视觉模型的能力，朝着通用视觉助手的目标迈进了一步。"}}
{"id": "2511.12133", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12133", "abs": "https://arxiv.org/abs/2511.12133", "authors": ["Qingyu Zhang", "Chunlei Xin", "Xuanang Chen", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Le Sun", "Qing Ye", "Qianlong Xie", "Xingxing Wang"], "title": "AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing", "comment": null, "summary": "Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.", "AI": {"tldr": "本文介绍了AI-Salesman框架及其发布的TeleSalesCorpus数据集，解决了销售对话中存在的挑战，实验结果表明该框架优于基准模型。", "motivation": "现有大型语言模型在目标驱动的销售对话任务上存在战略脆弱和事实错觉问题，本文针对缺乏任务特定数据和现有挑战，提出了解决方案。", "method": "采用了针对噪声对话设计的巴氏监督强化学习算法进行策略学习，并在推理阶段引入了动态大纲引导代理。", "result": "{\n  \"tldr\": \"本文介绍了AI-Salesman框架，该框架通过巴氏监督强化学习算法和动态大纲引导代理在销售对话中实现高效的策略学习和执行，同时发布了TeleSalesCorpus数据集。\",\n  \"motivation\": \"由于缺乏任务特定的数据和现有模型的战略脆弱性和事实错觉问题，大型语言模型在驱动目标的说服性对话中存在困难。本文旨在通过AI-Salesman解决这些问题。\",\n  \"method\": \"提出了AI-Salesman，包括两个阶段：训练阶段采用基于贝叶斯监督的强化学习来学习鲁棒的销售策略；推理阶段则利用动态大纲引导代理提供动态的战略指导。\",\n  \"result\": \"实验证明，AI-Salesman在自动化指标和全面的人类评估中都显著优于基准模型。\",\n  \"conclusion\": \"AI-Salesman在复杂说服场景中的有效性得到了实验证实。提供的框架为未来的销售对话系统研究奠定了基础。建议进一步改善模型的长期记忆和策略灵活性。\"}\n}", "conclusion": "AI-Salesman框架在销售对话中的表现通过实验得到了证实，为构建更加有效的销售系统提供了新的视角。"}}
{"id": "2511.11824", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11824", "abs": "https://arxiv.org/abs/2511.11824", "authors": ["Zhongping Dong", "Pengyang Yu", "Shuangjian Li", "Liming Chen", "Mohand Tahar Kechadi"], "title": "SOTFormer: A Minimal Transformer for Unified Object Tracking and Trajectory Prediction", "comment": null, "summary": "Accurate single-object tracking and short-term motion forecasting remain challenging under occlusion, scale variation, and temporal drift, which disrupt the temporal coherence required for real-time perception. We introduce \\textbf{SOTFormer}, a minimal constant-memory temporal transformer that unifies object detection, tracking, and short-horizon trajectory prediction within a single end-to-end framework. Unlike prior models with recurrent or stacked temporal encoders, SOTFormer achieves stable identity propagation through a ground-truth-primed memory and a burn-in anchor loss that explicitly stabilizes initialization. A single lightweight temporal-attention layer refines embeddings across frames, enabling real-time inference with fixed GPU memory. On the Mini-LaSOT (20%) benchmark, SOTFormer attains 76.3 AUC and 53.7 FPS (AMP, 4.3 GB VRAM), outperforming transformer baselines such as TrackFormer and MOTRv2 under fast motion, scale change, and occlusion.", "AI": {"tldr": "SOTFormer是一种新型的实时单目标跟踪模型，通过创新的记忆模块和轻量级时序注意力机制实现了在快速运动、尺度变化和遮挡环境下的高准确性。", "motivation": "为了提高单目标跟踪和短期运动预测的准确性，特别是在遮挡、尺度变化和时间漂移等挑战环境下，需要一个能够稳定目标身份同时保持实时能力和稳定内存消耗的模型。", "method": "SOTFormer采用了一种常量内存的时序Transformer模型，通过引入一个基于真实值的初始化记忆模块和烧入锚点损失，解决了之前模型在目标跟踪和预测中遇到的识别身份传播不稳定问题。此外，它还使用了一个轻量级的时序注意力层来优化帧间的嵌入表示，从而实现了实时推理并保持固定GPU内存使用。", "result": "在Mini-LaSOT(20%)基准测试中，SOTFormer达到了76.3的AUC和53.7的FPS（自动混合精度，4.3GB显存），在快速运动、尺度变化和遮挡等情况下超越了Transform基准线比如TrackFormer和MOTRv2。", "conclusion": "通过提出SOTFormer，该研究展示了通过融合目标检测、跟踪和短期轨迹预测在一个端到端的框架中能够解决实时感知中所需的时序连贯性问题，并在计算效率上表现出色，达到了实时处理的要求。"}}
{"id": "2511.12140", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12140", "abs": "https://arxiv.org/abs/2511.12140", "authors": ["Pinxue Guo", "Chongruo Wu", "Xinyu Zhou", "Lingyi Hong", "Zhaoyu Chen", "Jinglun Li", "Kaixun Jiang", "Sen-ching Samson Cheung", "Wei Zhang", "Wenqiang Zhang"], "title": "Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of \"Seeing is Believing\", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.", "AI": {"tldr": "本文介绍了一个新颖的参考无关的MLLM幻觉检测框架--VBackChecker，结合了像素级定位语言模型的推理和定点分割能力，其检测效果达到了最新水平，并超越了GPT-4o。", "motivation": "MLLM在跨模态能力方面具有强大的表现，但仍然存在幻觉问题，这影响了其在实际应用中的可靠性。因此，准确检测幻觉对于确保MLLM的可靠性至关重要。", "method": "结构化分析方法，包括参考无关的幻觉检测框架（VBackChecker）和用于生成指令微调数据的创新管道（R-Instruct）。VBackChecker结合了像素级定位语言模型的推理和定点分割能力，用于验证MLLM生成的响应与视觉输入的一致性。", "result": "VBackChecker在R^2-HalBench上超越了以前的复杂框架，并达到了幻觉检测的最新性能，甚至能与GPT-4o的检测能力相匹敌。它还在像素级定位任务中优越于以前的方法，实现了超过10%的改进。", "conclusion": "该研究提出了VBackChecker框架和R-Instruct数据生成管道，数据集R^2-HalBench也初次亮相。结果表明，VBackChecker在幻觉检测方面表现优异，优于现有方法。"}}
{"id": "2511.11837", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11837", "abs": "https://arxiv.org/abs/2511.11837", "authors": ["Fatemeh Elhambakhsh", "Gaurav Ameta", "Aditi Roy", "Hyunwoong Ko"], "title": "MP-GFormer: A 3D-Geometry-Aware Dynamic Graph Transformer Approach for Machining Process Planning", "comment": null, "summary": "Machining process planning (MP) is inherently complex due to structural and geometrical dependencies among part features and machining operations. A key challenge lies in capturing dynamic interdependencies that evolve with distinct part geometries as operations are performed. Machine learning has been applied to address challenges in MP, such as operation selection and machining sequence prediction. Dynamic graph learning (DGL) has been widely used to model dynamic systems, thanks to its ability to integrate spatio-temporal relationships. However, in MP, while existing DGL approaches can capture these dependencies, they fail to incorporate three-dimensional (3D) geometric information of parts and thus lack domain awareness in predicting machining operation sequences. To address this limitation, we propose MP-GFormer, a 3D-geometry-aware dynamic graph transformer that integrates evolving 3D geometric representations into DGL through an attention mechanism to predict machining operation sequences. Our approach leverages StereoLithography surface meshes representing the 3D geometry of a part after each machining operation, with the boundary representation method used for the initial 3D designs. We evaluate MP-GFormer on a synthesized dataset and demonstrate that the method achieves improvements of 24\\% and 36\\% in accuracy for main and sub-operation predictions, respectively, compared to state-of-the-art approaches.", "AI": {"tldr": "MP-GFormer is proposed to integrate 3D geometric information into DGL for predicting machining operation sequences, showing significant accuracy improvements over existing methods.", "motivation": "Existing DGL approaches fail to incorporate 3D geometric information of parts, leading to inadequate domain awareness in predicting machining operation sequences.", "method": "MP-GFormer, a 3D-geometry-aware dynamic graph transformer integrating evolving 3D geometric representations into DGL through an attention mechanism.", "result": "The method achieves 24% and 36% improvements in accuracy for main and sub-operation predictions, respectively, compared to state-of-the-art approaches.", "conclusion": "MP-GFormer effectively addresses the challenge of predicting machining operation sequences by incorporating 3D geometric information."}}
{"id": "2511.12159", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12159", "abs": "https://arxiv.org/abs/2511.12159", "authors": ["Yaocheng Zhang", "Haohuan Huang", "Zijun Song", "Yuanheng Zhu", "Qichao Zhang", "Zijie Zhao", "Dongbin Zhao"], "title": "CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic", "comment": "17 pages, 10 figures", "summary": "Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.", "AI": {"tldr": "本文提出CriticSearch框架，通过回顾性批评机制提升搜索代理流水线训练效率和稳定性，在多跳推理任务中实现更佳性能。", "motivation": "现有搜索代理流水线通常依赖强化学习优化，这经常因为稀疏结果奖励而导致探索效率低下和训练不稳定，我们提出CriticSearch旨在解决这些痛点。", "method": "本文提出了一种名为CriticSearch的细粒度信用分配框架，通过回顾性批评机制，在训练过程中提供密集的回合级反馈，采用冷冻的非对称批评LLM基于完整轨迹和金标准答案评估每一轮，将这些评估转化为稳定密集的奖励来指导策略改进。", "result": "实验结果表明，CriticSearch在多种多跳推理基准上始终优于现有基线，实现了更快的收敛速度、改进了训练稳定性并提升了性能。", "conclusion": "CriticSearch框架在回顾性批评机制下提供稳定密集奖励，显著提升搜索代理流水线的训练效率和性能，是复杂任务中适应性和泛化的进步。"}}
{"id": "2511.11851", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.11851", "abs": "https://arxiv.org/abs/2511.11851", "authors": ["Wei-Jia Chen", "Min-Yen Tsai", "Cheng-Yi Lee", "Chia-Mu Yu"], "title": "Defending Unauthorized Model Merging via Dual-Stage Weight Protection", "comment": "10 pages, under review", "summary": "The rapid proliferation of pretrained models and open repositories has made model merging a convenient yet risky practice, allowing free-riders to combine fine-tuned models into a new multi-capability model without authorization. Such unauthorized model merging not only violates intellectual property rights but also undermines model ownership and accountability. To address this issue, we present MergeGuard, a proactive dual-stage weight protection framework that disrupts merging compatibility while maintaining task fidelity. In the first stage, we redistribute task-relevant information across layers via L2-regularized optimization, ensuring that important gradients are evenly dispersed. In the second stage, we inject structured perturbations to misalign task subspaces, breaking curvature compatibility in the loss landscape. Together, these stages reshape the model's parameter geometry such that merged models collapse into destructive interference while the protected model remains fully functional. Extensive experiments on both vision (ViT-L-14) and language (Llama2, Gemma2, Mistral) models demonstrate that MergeGuard reduces merged model accuracy by up to 90% with less than 1.5% performance loss on the protected model.", "AI": {"tldr": "MergeGuard is a two-stage framework for protecting models from unauthorized merging, ensuring intellectual property rights and model integrity by disrupting merging compatibility.", "motivation": "The motivation behind MergeGuard is to address the issue of unauthorized model merging, which poses risks to intellectual property rights, model ownership, and accountability while enabling free-riders to benefit from the work of others.", "method": "The paper introduces MergeGuard, a dual-stage weight protection framework. In the first stage, it uses L2-regularized optimization to redistribute task-relevant information across layers. In the second stage, it injects structured perturbations to misalign task subspaces, disrupting merging compatibility.", "result": "Experiments on vision and language models show that MergeGuard effectively reduces the accuracy of merged models by up to 90%, with minimal impact (less than 1.5%) on the performance of the protected model.", "conclusion": "MergeGuard offers a proactive solution to the problem of unauthorized model merging by disrupting the compatibility of merged models while preserving the functionality of the original model."}}
{"id": "2511.12213", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12213", "abs": "https://arxiv.org/abs/2511.12213", "authors": ["Liang Xue", "Haoyu Liu", "Yajun Tian", "Xinyu Zhong", "Yang Liu"], "title": "MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues", "comment": null, "summary": "Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.", "AI": {"tldr": "本文提出了MME-RAG框架，用于改进现有模型在任务导向对话中进行细粒度实体识别的领域适应性和检索控制性问题。该框架在多个数据集上表现优异，并被验证为一种可扩展和可解释的自适应对话理解解决方案。", "motivation": "细粒度实体识别对于任务导向对话中的推理和决策至关重要，但现有的大型语言模型在领域适应性和检索控制性方面仍面临挑战。", "method": "我们提出了MME-RAG框架，该框架通过两个协调步骤完成实体识别：轻量级管理者进行类型级别的判断，专业专家进行片段级别的提取。每个专家都有一个KeyInfo检索器，在推理过程中注入语义对齐的少量示例，实现精确且适应领域的提取，无需额外训练。", "result": "实验显示MME-RAG在大多数领域表现优于最近的基线。消融研究进一步证明，层次分解和KeyInfo指导的检索是提高模型稳健性和跨领域泛化能力的关键因素。", "conclusion": "MME-RAG被确立为一种可扩展且可解释的自适应对话理解解决方案。"}}
{"id": "2511.11864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11864", "abs": "https://arxiv.org/abs/2511.11864", "authors": ["Muzammal Shafique", "Nasir Rahim", "Jamil Ahmad", "Mohammad Siadat", "Khalid Malik", "Ghaus Malik"], "title": "FocusSDF: Boundary-Aware Learning for Medical Image Segmentation via Signed Distance Supervision", "comment": null, "summary": "Segmentation of medical images constitutes an essential component of medical image analysis, providing the foundation for precise diagnosis and efficient therapeutic interventions in clinical practices. Despite substantial progress, most segmentation models do not explicitly encode boundary information; as a result, making boundary preservation a persistent challenge in medical image segmentation. To address this challenge, we introduce FocusSDF, a novel loss function based on the signed distance functions (SDFs), which redirects the network to concentrate on boundary regions by adaptively assigning higher weights to pixels closer to the lesion or organ boundary, effectively making it boundary aware. To rigorously validate FocusSDF, we perform extensive evaluations against five state-of-the-art medical image segmentation models, including the foundation model MedSAM, using four distance-based loss functions across diverse datasets covering cerebral aneurysm, stroke, liver, and breast tumor segmentation tasks spanning multiple imaging modalities. The experimental results consistently demonstrate the superior performance of FocusSDF over existing distance transform based loss functions.", "AI": {"tldr": "The paper introduces FocusSDF, a novel loss function based on signed distance functions (SDFs) to improve boundary preservation in medical image segmentation. It evaluates FocusSDF on various medical image datasets and shows better performance than other distance transform based loss functions.", "motivation": "Most existing segmentation models lack explicit boundary information encoding, leading to persistent challenges in accurate medical image segmentation. This paper aims to improve boundary preservation through a specialized loss function.", "method": "FocusSDF, based on SDFs, is proposed to redirect the network to focus on boundary areas by giving higher weights to closer boundary pixels.", "result": "Rigorous evaluation using different state-of-the-art models and datasets demonstrates FocusSDF's superior performance compared to conventional distance-based loss functions.", "conclusion": "FocusSDF enhances the accuracy of medical image segmentation by improving boundary preservation, exhibiting better performance than existing methods across multiple medical imaging tasks."}}
{"id": "2511.12236", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12236", "abs": "https://arxiv.org/abs/2511.12236", "authors": ["Raavi Gupta", "Pranav Hari Panicker", "Sumit Bhatia", "Ganesh Ramakrishnan"], "title": "Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts", "comment": "To appear at International Joint Conference on Natural Language Processing & Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL), 2025", "summary": "Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.", "AI": {"tldr": "研究提出了一种新的事实性错误检测方法CONFACTCHECK，不依赖外部知识库，能够在检测大型语言模型生成的虚假事实时有效地减少资源使用并对现有方法有更高的准确性。", "motivation": "该项目旨在解决大型语言模型在医疗、金融、客户服务等领域中由于生成不真实文本而引发的风险问题。特别是在只能通过API使用这些模型且无法访问模型权重或进行微调的情况下，现有的检测方法通常需要多次API调用，增加了延迟和成本。", "method": "CONFACTCHECK采用了一种高效的事实性错误检测方法，这种方法不依赖任何外部知识库，而是基于这样一个简单直觉：对于生成文本中的事实性探针，同一LLM或不同LLM生成的回应应该是一致的。这种方法在检测事实性错误时可以有效减少资源使用并提高检测准确率。", "result": "实验结果表明，CONFACTCHECK在减少资源使用的情况下，能够更高效地检测出虚假事实，并且在不同基准测试中的准确性得分高于现有的方法。", "conclusion": "CONFACTCHECK展示了高效的事实性错误检测潜力，尤其在受限或资源约束条件下表现出色，能够在减少资源消耗的同时发现并纠正虚假事实。"}}
