<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 19]
- [cs.CV](#cs.CV) [Total: 22]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Are you sure? Measuring models bias in content moderation through uncertainty](https://arxiv.org/abs/2509.22699)
*Alessandra Urbinati,Mirko Lai,Simona Frenda,Marco Antonio Stranisci*

Main category: cs.CL

> 本研究提出了一种无监督方法，利用模型分类的不确定性来检验其对女性和非白人标注者是否存在偏见。

<details>
  <summary>Details</summary>

**Motivation:** 自动化内容审查对于确保社交媒体的安全至关重要，但基于语言模型的分类器已被证实会延续种族和社会偏见。尽管开发了多种资源和基准语料库来应对这一问题，但在内容审查中测量模型的公平性仍然是一个开放问题。

**Method:** 采用了无监督的方法来评估模型在分类属于弱势群体的人所标注的信息时的不确定性。使用了符合预测技术计算的不确定性作为分析模型对女性和非白人标注者偏见的代理指标。

**Result:** 结果表明，一些预训练模型能够高精度预测来自少数群体的标签，即使它们对预测的置信度较低。通过测量模型的置信度，可以识别出在预训练模型中更好代表的注释者群体。

**Conclusion:** 通过衡量模型的置信度，可以识别出预训练模型中更受代表的注释者群体，并在模型实际应用前指导其去偏过程。

**Abstract:** Automatic content moderation is crucial to ensuring safety in social media.
Language Model-based classifiers are being increasingly adopted for this task,
but it has been shown that they perpetuate racial and social biases. Even if
several resources and benchmark corpora have been developed to challenge this
issue, measuring the fairness of models in content moderation remains an open
issue. In this work, we present an unsupervised approach that benchmarks models
on the basis of their uncertainty in classifying messages annotated by people
belonging to vulnerable groups. We use uncertainty, computed by means of the
conformal prediction technique, as a proxy to analyze the bias of 11 models
against women and non-white annotators and observe to what extent it diverges
from metrics based on performance, such as the $F_1$ score. The results show
that some pre-trained models predict with high accuracy the labels coming from
minority groups, even if the confidence in their prediction is low. Therefore,
by measuring the confidence of models, we are able to see which groups of
annotators are better represented in pre-trained models and lead the debiasing
process of these models before their effective use.

</details>


### [2] [AccessEval: Benchmarking Disability Bias in Large Language Models](https://arxiv.org/abs/2509.22703)
*Srikant Panda,Amit Agarwal,Hitesh Laxmichand Patel*

Main category: cs.CL

> AccessEval测试了大语言模型在处理不同类型的残疾意识查询时的准确性、社会感知和情感倾向，发现这些模型在处理残疾相关的查询时表现出更多的负面情感、刻板印象和事实错误。

<details>
  <summary>Details</summary>

**Motivation:** LLMs在处理现实生活查询时表现不一，研究其在不同残疾上下文中的表现，揭示大规模语言模型中的偏见为何可能会对残障用户造成实质性伤害。

**Method:** 通过引入AccessEval（无障碍评估）基准测试，系统化地研究在不同残疾语境中的影响，测试了21个闭源和开源的大语言模型，在6个现实世界领域和9种残疾类型中，使用配对的中性和残疾导向查询。

**Result:** 研究结果显示，对于残疾意识查询，模型的回答往往具有更多的负面情绪、增加的刻板印象和更高的事实错误。这些影响在领域和残疾类型之间存在显著差异，其中听力、言语和移动障碍的影响尤为严重。

**Conclusion:** 通过在现实世界决策上下文中检验模型性能，更好地揭示了这种偏见如何转化为针对残障用户的固定伤害。这有助于在技术和用户影响之间的评估上建立桥梁，强化日常应用中减少偏见的重要性。

**Abstract:** Large Language Models (LLMs) are increasingly deployed across diverse domains
but often exhibit disparities in how they handle real-life queries. To
systematically investigate these effects within various disability contexts, we
introduce \textbf{AccessEval (Accessibility Evaluation)}, a benchmark
evaluating 21 closed- and open-source LLMs across 6 real-world domains and 9
disability types using paired Neutral and Disability-Aware Queries. We
evaluated model outputs with metrics for sentiment, social perception, and
factual accuracy.
  Our analysis reveals that responses to disability-aware queries tend to have
a more negative tone, increased stereotyping, and higher factual error compared
to neutral queries. These effects show notable variation by domain and
disability type, with disabilities affecting hearing, speech, and mobility
disproportionately impacted. These disparities reflect persistent forms of
ableism embedded in model behavior.
  By examining model performance in real-world decision-making contexts, we
better illuminate how such biases can translate into tangible harms for
disabled users. This framing helps bridges the gap between technical evaluation
and user impact, reinforcing importance of bias mitigation in day-to-day
applications. Our dataset is publicly available at:
https://huggingface.co/datasets/Srikant86/AccessEval

</details>


### [3] [RAR$^2$: Retrieval-Augmented Medical Reasoning via Thought-Driven Retrieval](https://arxiv.org/abs/2509.22713)
*Kaishuai Xu,Wenjun Hou,Yi Cheng,Wenjie Li*

Main category: cs.CL

> RAR$^2$框架提升了医学问答的性能，通过结合推理增强检索和检索增强推理来优化模型。实验表明该框架优于传统的RAG方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的RAG方法在处理需要复杂推理的医学问题上表现不佳。这些问题通常需要超出表面层的输入才能反映出任务的真实知识需求。

**Method:** RAR$^2$, 一个联合学习框架，同时优化推理增强检索和检索增强推理。RAR$^2$ 构建了一个思维过程来揭示隐含的知识需求，并用它来指导检索和回答生成。

**Result:** 实验结果证明了RAR$^2$在多个生物医学问答数据集上的有效性，超过了RAG基线模型，无论是否进行了微调。

**Conclusion:** RAR$^2$通过优化推理和检索的联合过程，能够更有效地识别和整合临床相关知识，在医学问答任务中有良好的表现。

**Abstract:** Large Language Models (LLMs) have shown promising performance on diverse
medical benchmarks, highlighting their potential in supporting real-world
clinical tasks. Retrieval-Augmented Generation (RAG) has emerged as a key
approach for mitigating knowledge gaps and hallucinations by incorporating
external medical information. However, RAG still struggles with complex medical
questions that require intensive reasoning, as surface-level input often fails
to reflect the true knowledge needs of the task. Existing methods typically
focus on refining queries without explicitly modeling the reasoning process,
limiting their ability to retrieve and integrate clinically relevant knowledge.
In this work, we propose RAR$^2$, a joint learning framework that improves both
Reasoning-Augmented Retrieval and Retrieval-Augmented Reasoning. RAR$^2$
constructs a thought process to uncover implicit knowledge requirements and
uses it to guide retrieval and answer generation. We build a training dataset
of mixed preference pairs and apply Direct Preference Optimization (DPO) to
train the model. Moreover, we design two test-time scaling strategies to
explore the boundaries of our framework. Experiments demonstrate the
effectiveness of RAR$^2$ across several biomedical question answering datasets,
outperforming RAG baselines with or without fine-tuning.

</details>


### [4] [TRUEBench: Can LLM Response Meet Real-world Constraints as Productivity Assistant?](https://arxiv.org/abs/2509.22715)
*Jiho Park,Jongyoon Song,Minjin Choi,Kyuho Heo,Taehun Huh,Ji Won Kim*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Large language models (LLMs) are increasingly integral as productivity
assistants, but existing benchmarks fall short in rigorously evaluating their
real-world instruction-following capabilities. Current benchmarks often (i)
lack sufficient multilinguality, (ii) fail to capture the implicit constraints
inherent in user requests, and (iii) overlook the complexities of multi-turn
dialogue. To address these critical gaps and provide a more realistic
assessment, we introduce TRUEBench (Trustworthy Real-world Usage Evaluation
Benchmark)1, a novel benchmark specifically designed for LLM-based productivity
assistants. TRUEBench distinguishes itself by featuring input prompts across 12
languages, incorporating intra-instance multilingual instructions, employing
rigorous evaluation criteria to capture both explicit and implicit constraints,
and including complex multi-turn dialogue scenarios with both accumulating
constraints and context switches. Furthermore, to ensure reliability in
evaluation, we refined constraints using an LLM validator. Extensive
experiments demonstrate that TRUEBench presents significantly greater
challenges than existing benchmarks; for instance, a strong model like OpenAI
o1 achieved only a 69.07% overall pass rate. TRUEBench offers a demanding and
realistic assessment of LLMs in practical productivity settings, highlighting
their capabilities and limitations.

</details>


### [5] [Multi-Modal Sentiment Analysis with Dynamic Attention Fusion](https://arxiv.org/abs/2509.22729)
*Sadia Abdulhalim,Muaz Albaghdadi,Moshiur Farazi*

Main category: cs.CL

> DAF模型结合文本和语音特征进行情感分析，提高了预测准确性。

<details>
  <summary>Details</summary>

**Motivation:** 传统的基于文本的单模态情感分析忽视了如语音语调等非语言线索，这些线索对捕获真实情感意图至关重要。

**Method:** 提出Dynamic Attention Fusion (DAF)，一个轻量级框架，该框架利用自适应注意力机制将来自预训练语言模型的冻结文本嵌入与来自语音编码器的声学特征相结合。

**Result:** 在大规模多模态基准测试中，无需调整底层编码器的情况下，提出的DAF模型在F1分数上取得了显著增益，并减少了预测误差。

**Conclusion:** 通过有效整合语言和非语言信息，该方法为情感预测提供了更强大的基础，并对情感计算应用（如情感识别、心理健康评估及更自然的人机交互）具有更广泛的影响。

**Abstract:** Traditional sentiment analysis has long been a unimodal task, relying solely
on text. This approach overlooks non-verbal cues such as vocal tone and prosody
that are essential for capturing true emotional intent. We introduce Dynamic
Attention Fusion (DAF), a lightweight framework that combines frozen text
embeddings from a pretrained language model with acoustic features from a
speech encoder, using an adaptive attention mechanism to weight each modality
per utterance. Without any finetuning of the underlying encoders, our proposed
DAF model consistently outperforms both static fusion and unimodal baselines on
a large multimodal benchmark. We report notable gains in F1-score and
reductions in prediction error and perform a variety of ablation studies that
support our hypothesis that the dynamic weighting strategy is crucial for
modeling emotionally complex inputs. By effectively integrating verbal and
non-verbal information, our approach offers a more robust foundation for
sentiment prediction and carries broader impact for affective computing
applications -- from emotion recognition and mental health assessment to more
natural human computer interaction.

</details>


### [6] [Enabling Approximate Joint Sampling in Diffusion LMs](https://arxiv.org/abs/2509.22738)
*Parikshit Bansal,Sujay Sanghavi*

Main category: cs.CL

> 本文提出了一种在单次完整模型前向传播中近似地从联合分布中采样多个令牌的方法，通过开发一个新的轻量级单层采样器来实现。该方法提高了采样效率，同时在语言建模和数学编程任务上保持了高性能。

<details>
  <summary>Details</summary>

**Motivation:** 传统的自动回归语言模型通过条件生成每个令牌来采样字符串，而掩码扩散语言模型则通过非顺序及并行的方式解除屏蔽来生成文本。解除屏蔽令牌的并行数量越多，生成的字符串就离真正的联合分布越远。我们旨在提出一种方法，以近似的方式在单次完整的模型前向传播过程中从联合分布中采样多个令牌。

**Method:** 我们提出了一种新的轻量级单层“采样器”，它位于现有的大型扩散语言模型之上。通过这种方法，在一次完整的模型前向传播后，可以进行多次仅通过采样器层的前向传播，从而生成多个未屏蔽的令牌。采样器被训练来模仿从固定完整模型的联合分布中进行精确采样。

**Result:** 我们的逼近联合采样方法在仅预训练（Dream-7B-Base）和指令调优（Dream-7B-Instruct）模型的语言建模和数学及编程任务上表明了有效性。当为每个完整的模型去噪步骤解除屏蔽四个令牌时，我们的采样算法相对于真实联合分布的MAUVE得分为0.87，相较于边缘基准的0.31有了显著提升。

**Conclusion:** 研究表明，我们提出的方法可以在不牺牲性能的情况下，显著提高文本生成的速度。该方法成功地通过有限的完整模型前向传播来实现接近真实联合分布的采样，对于实际应用具有重要意义。

**Abstract:** In autoregressive language models, each token is sampled by conditioning on
all the past tokens; the overall string has thus been sampled from the correct
underlying joint distribution represented by the model. In contrast, masked
diffusion language models generate text by unmasking tokens out of order and
potentially in parallel. Generating an overall string sampled from the correct
underlying joint distribution would (again) require exactly one token unmasking
in every full-model forward pass. The more tokens unmasked in parallel, the
further away the string is from the true joint; this can be seen in the
resulting drop in accuracy (but, increase in speed). In this paper we devise a
way to {\em approximately} sample multiple tokens from the joint distribution
in a single full-model forward pass; we do so by developing a new lightweight
single-layer ``sampler" on top of an existing large diffusion LM. One forward
pass of the full model can now be followed by multiple forward passes of only
this sampler layer, to yield multiple unmasked tokens. Our sampler is trained
to mimic exact joint sampling from the (frozen) full model. We show the
effectiveness of our approximate joint sampling for both pretrained-only
(Dream-7B-Base) and instruction-tuned (Dream-7B-Instruct) models on language
modeling and math \& coding tasks. When four tokens are unmasked for each
full-model denoising step, our sampling algorithm achieves a MAUVE score of
0.87 (vs marginal baseline of 0.31) with respect to the true joint
distribution.

</details>


### [7] [Painless Activation Steering: An Automated, Lightweight Approach for Post-Training Large Language Models](https://arxiv.org/abs/2509.22739)
*Sasha Cui,Zhongren Chen*

Main category: cs.CL

> PAS, an automated method for steering activation (AS), improves performance for behavior tasks in LMs without the need for manual prompt creation or labeling.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to provide a cheaper, faster, and more controllable alternative to existing post-training methods like weight-based training and prompt-based steering, which can be time-consuming and less precise.

**Method:** Painless Activation Steering (PAS) is introduced as a fully automated method for activation steering (AS) that can be applied with any labeled dataset without the need for prompts or human labeling.

**Result:** PAS was evaluated on multiple open-weight models and tasks, showing reliable performance improvements for behavior tasks but not for intelligence-oriented tasks. iPAS, the introspective variant, showed strong causal steering effects across specific metrics.

**Conclusion:** The conclusion is that PAS, a practical and automated method for LM post-training, effectively improves behavior tasks and can be used in conjunction with other methods like In-Context Learning (ICL) and Supervised Fine-Tuning (SFT).

**Abstract:** Language models (LMs) are typically post-trained for desired capabilities and
behaviors via weight-based or prompt-based steering, but the former is
time-consuming and expensive, and the latter is not precisely controllable and
often requires manual trial-and-error. While activation steering (AS) promises
a cheap, fast, and controllable alternative to the two existing post-training
methods, current AS techniques require hand-crafted prompt pairs or
labor-intensive feature annotation, making them more inconvenient than the
plug-and-play methods such as Reinforcement Learning (RL) and Supervised
Fine-Tuning (SFT). We introduce Painless Activation Steering (PAS), a family of
fully automated methods that make AS readily usable with any given labeled
dataset, with no need for prompt construction, feature labeling, or human
intervention. We evaluate PAS on three open-weight models
(Llama3.1-8B-Instruct, DeepSeek-R1-Distill-8B, and Nous-Hermes-2) and 18 tasks;
we find that PAS reliably improves performance for behavior tasks, but not for
intelligence-oriented tasks. The introspective variant (iPAS) delivers the
strongest causal steering effects (10.1% on Bias, 5.2% on Morality, and 34.8%
on Alignment). We also show PAS delivers additional gains on top of In-Context
Learning (ICL) and SFT. PAS constructs a fast, lightweight activation vector
that can be cheaply trained, easily stored, and activated at will. Our results
provide a characterization of where AS helps, where it fails, and how to deploy
it as a practical, automated LM post-training option.

</details>


### [8] [MIRAGE: Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions](https://arxiv.org/abs/2509.22750)
*Jeonghyun Park,Ingeol Baek,Seunghyun Yoon,Haeun Jang,Aparna Garimella,Akriti Jain,Nedim Lipka,Hwanhee Lee*

Main category: cs.CL

> 研究探讨了多跳问答中的歧义问题，并为研究提供了基准MIRAGE和适应歧义推理的CLARION框架，后者在评估中表现突出。

<details>
  <summary>Details</summary>

**Motivation:** 当前的大型语言模型在处理多跳推理中的歧义问题时遇到挑战，容易走错推理路径并产生不完整答案。为促进该难题的研究，作者创建了一个评估基准MIRAGE。

**Method:** 提出了一种称为CLARION的多代理框架来解决多跳推理中的歧义问题，显著优于现有方法。CLARION框架用于处理复杂的推理链和歧义子问题，以获得更完整和精确的答案。

**Result:** 实验结果表明，即使是最先进的模型在MIRAGE上也表现不佳，CLARION框架在MIRAGE上显著优于现有方法。这证实了处理歧义与多步推断相结合的挑战性。

**Conclusion:** 通过引入MIRAGE评估基准和CLARION框架，研究揭示了解决多跳推理中歧义问题的重要性，为构建更加适应和健壮的推理系统铺平了道路。

**Abstract:** Real-world Multi-hop Question Answering (QA) often involves ambiguity that is
inseparable from the reasoning process itself. This ambiguity creates a
distinct challenge, where multiple reasoning paths emerge from a single
question, each requiring independent resolution. Since each sub-question is
ambiguous, the model must resolve ambiguity at every step. Thus, answering a
single question requires handling multiple layers of ambiguity throughout the
reasoning chain. We find that current Large Language Models (LLMs) struggle in
this setting, typically exploring wrong reasoning paths and producing
incomplete answers. To facilitate research on multi-hop ambiguity, we introduce
MultI-hop Reasoning with AmbiGuity Evaluation for Illusory Questions (MIRAGE),
a benchmark designed to analyze and evaluate this challenging intersection of
ambiguity interpretation and multi-hop reasoning. MIRAGE contains 1,142
high-quality examples of ambiguous multi-hop questions, categorized under a
taxonomy of syntactic, general, and semantic ambiguity, and curated through a
rigorous multi-LLM verification pipeline. Our experiments reveal that even
state-of-the-art models struggle on MIRAGE, confirming that resolving ambiguity
combined with multi-step inference is a distinct and significant challenge. To
establish a robust baseline, we propose CLarifying Ambiguity with a Reasoning
and InstructiON (CLARION), a multi-agent framework that significantly
outperforms existing approaches on MIRAGE, paving the way for more adaptive and
robust reasoning systems.

</details>


### [9] [ML2B: Multi-Lingual ML Benchmark For AutoML](https://arxiv.org/abs/2509.22768)
*Ekaterina Trofimova,Zosia Shamina,Maria Selifanova,Artem Zaitsev,Remi Savchuk,Maxim Minets,Daria Ozerova,Emil Sataev,Denis Zuenko,Andrey E. Ustyuzhanin*

Main category: cs.CL

> 本文介绍了一个新的多语言机器学习代码生成基准测试ML2B，它包含了13种语言的Kaggle竞赛，评估显示非英语任务的性能下降，指出了多语言代码生成中的挑战。

<details>
  <summary>Details</summary>

**Motivation:** 现有的机器学习代码生成基准测试主要局限于英语，忽视了全球和多语言的机器学习研究和实践特性。为了填补这一空白，提出了ML2B基准测试。

**Method:** ML2B基准测试是第一个用于评估多语言机器学习代码生成的基准测试，包含了13种自然语言翻译的30个Kaggle竞赛，涵盖了表格、文本和图像数据类型，提供了结构化的元数据和经过验证的人工翻译。用于评估的是一个名为AIDE的自动化框架，用于评估端到端的数据科学管道。

**Result:** 实验结果显示在非英语任务上表现下降了15-45%，凸显了多语言表示学习代码生成中的关键挑战。

**Conclusion:** 这项研究表明，现有的模型在处理多语言机器学习代码生成任务时存在显著的性能下降问题。这为未来的研究指出了方向，特别是在多语言表示学习方面。

**Abstract:** Large language models (LLMs) have recently demonstrated strong capabilities
in generating machine learning (ML) code, enabling end-to-end pipeline
construction from natural language instructions. However, existing benchmarks
for ML code generation are mainly restricted to English, overlooking the global
and multilingual nature of ML research and practice. To address this gap, we
present ML2B, the first benchmark for evaluating multilingual ML code
generation. ML2B consists of 30 Kaggle competitions translated into 13 natural
languages, covering tabular, text, and image data types, with structured
metadata and validated human-reviewed translations. For evaluation, we employ
AIDE, an automated framework for end-to-end assessment of data science
pipelines, and provide insights into cross-lingual model performance. Our
results reveal substantial 15-45% performance degradation on non-English tasks,
highlighting critical challenges in multilingual representation learning for
code generation. The benchmark, evaluation framework, and comprehensive results
are made available through our GitHub repository to facilitate future research
in multilingual ML code generation: https://github.com/enaix/ml2b.

</details>


### [10] [ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection](https://arxiv.org/abs/2509.22808)
*Mohamed Maged,Alhassan Ehab,Ali Mekky,Besher Hassan,Shady Shehata*

Main category: cs.CL

> 本文介绍了首个多方言阿拉伯语语音欺骗数据集，并通过训练分类器和使用RawNet2等方法评估了不同模型生成的合成语音难度。研究发现，在卡萨布兰卡语料库中，FishSpeech比其他TTS模型在阿拉伯语语音克隆上表现更佳，合成的语音更具真实性和挑战性。但仅依赖单一TTS模型创建数据集可能限制其普遍适用性。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于生成式文本到语音模型的发展，甄别真实和合成语音变得富有挑战性，尤其是对于研究较少的阿拉伯语音。以往语音欺骗检测的努力主要集中在英语上，而对于阿拉伯语及其众多方言则关注不足，因此，本文旨在填补这一研究空白。

**Method:** 文中介绍了使用两种方法来评估不同模型生成的阿拉伯语语音合成样本的难度：一种是基于现代嵌入方法结合分类器的方法，另一种是基于MFCC特征的传统机器学习算法，此外还使用了RawNet2架构。评估过程中还加入了基于人工评分计算的平均意见得分，以及通过自动语音识别模型处理原始和合成数据集来测量词错误率。

**Result:** 研究结果表明，在卡萨布兰卡语料库中，FishSpeech相较于其他TTS模型在阿拉伯语语音克隆方面表现更优，生成的合成语音更为逼真且更具挑战性。

**Conclusion:** 研究表明FishSpeech在生成逼真且具挑战性的阿拉伯语语音样本方面表现最佳，然而，过于依赖某一TTS模型可能会影响数据集的普适性。

**Abstract:** With the rise of generative text-to-speech models, distinguishing between
real and synthetic speech has become challenging, especially for Arabic that
have received limited research attention. Most spoof detection efforts have
focused on English, leaving a significant gap for Arabic and its many dialects.
In this work, we introduce the first multi-dialect Arabic spoofed speech
dataset. To evaluate the difficulty of the synthesized audio from each model
and determine which produces the most challenging samples, we aimed to guide
the construction of our final dataset either by merging audios from multiple
models or by selecting the best-performing model, we conducted an evaluation
pipeline that included training classifiers using two approaches: modern
embedding-based methods combined with classifier heads; classical machine
learning algorithms applied to MFCC features; and the RawNet2 architecture. The
pipeline further incorporated the calculation of Mean Opinion Score based on
human ratings, as well as processing both original and synthesized datasets
through an Automatic Speech Recognition model to measure the Word Error Rate.
Our results demonstrate that FishSpeech outperforms other TTS models in Arabic
voice cloning on the Casablanca corpus, producing more realistic and
challenging synthetic speech samples. However, relying on a single TTS for
dataset creation may limit generalizability.

</details>


### [11] [EditGRPO: Reinforcement Learning with Post -Rollout Edits for Clinically Accurate Chest X-Ray Report Generation](https://arxiv.org/abs/2509.22812)
*Kai Zhang,Christopher Malon,Lichao Sun,Martin Renqiang Min*

Main category: cs.CL

> 研究介绍了一种新的混合策略强化学习算法——EditGRPO，其通过详细纠正信息在训练过程中结合了on-policy探索和off-policy指导，从而提高了胸部X光报告生成的质量和泛化性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管近期的创新，特别是多模态大型语言模型（MLLMs），已经展示了改进的性能，但它们的监督微调（SFT）目标没有明确与临床效能对齐。因此，有必要引入一种新的方法来优化医学影像分析、时间推理和文本生成这些领域中的临床报告生成任务。

**Method:** 本文介绍了一种名为EditGRPO的混合策略强化学习算法，该算法专门针对通过临床动机奖励来优化生成过程。EditGRPO通过在训练周期中注入句子级的详细纠正信息，集成了on-policy探索与off-policy指导。这种混合策略方法解决了强化学习中通常遇到的探索困境和采样效率问题。

**Result:** 与SFT和vanilla GRPO基线方法相比，应用于经过监督微调（SFT）初始化的Qwen2.5-VL-3B MLLM的EditGRPO，在CheXbert、GREEN、Radgraph和RATEScore等四个主要胸部X光报告生成数据集上实现了平均3.4%的改进。此外，对于未见过的数据集，EditGRPO还表现出更强的泛化能力，平均性能提升了5.9%。

**Conclusion:** 本文所提出的EditGRPO方法显著改善了放射学报告生成任务的训练效果和泛化性能，这对提高临床实践中的报告质量和可靠性具有重要意义。

**Abstract:** Radiology report generation requires advanced medical image analysis,
effective temporal reasoning, and accurate text generation. Although recent
innovations, particularly multimodal large language models (MLLMs), have shown
improved performance, their supervised fine-tuning (SFT) objective is not
explicitly aligned with clinical efficacy. In this work, we introduce EditGRPO,
a mixed-policy reinforcement learning (RL) algorithm designed specifically to
optimize the generation through clinically motivated rewards. EditGRPO
integrates on-policy exploration with off-policy guidance by injecting
sentence-level detailed corrections during training rollouts. This mixed-policy
approach addresses the exploration dilemma and sampling efficiency issues
typically encountered in RL. Applied to a Qwen2.5-VL-3B MLLM initialized with
supervised fine-tuning (SFT), EditGRPO outperforms both SFT and vanilla GRPO
baselines, achieving an average improvement of 3.4% in CheXbert, GREEN,
Radgraph, and RATEScore metrics across four major chest X-ray report generation
datasets. Notably, EditGRPO also demonstrates superior out-of-domain
generalization, with an average performance gain of 5.9% on unseen datasets.

</details>


### [12] [Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning](https://arxiv.org/abs/2509.22824)
*Chi Ruan,Dongfu Jiang,Yubo Wang,Wenhu Chen*

Main category: cs.CL

> 本文提出了一种新的强化学习方法——CRL，通过训练模型对给定的问题和解决方案对进行批判，来增强模型的批判和反思能力。通过训练模型，并在多个基准测试中评估它们，实验结果表明CRL训练的模型在各种任务上都优于标准RL训练的模型。

<details>
  <summary>Details</summary>

**Motivation:** 强化学习虽然在训练模型生成响应方面表现良好，但它缺乏明确鼓励批判或反思的机制。基于此动机，作者提出了CRL，以增强模型的批判能力。

**Method:** 本文提出了CRL，用于任务需求生成批判，CRL的奖励完全由生成批判的最终判断标签与真值的匹配度决定。同时，本文引入了一个新的模型Critique-Coder，通过混合使用RL和CRL训练数据来训练Critique-Coder。

**Result:** 多个基准测试显示，Critique-Coder模型的性能优于标准RL训练的模型，特别是在LiveCodeBench (v5)上表现最好，超过了其他诸如DeepCoder-14B和GPT-o1等推理模型。

**Conclusion:** CRL通过改进标准的RL方法，能有效提升模型在批判和推理方面的能力，这种改进不仅限于代码生成任务，还能应用于广泛的其他任务上，因而适合作为标准RL的一种补充。

**Abstract:** Reinforcement Learning (RL) has emerged as a popular training paradigm,
particularly when paired with reasoning models. While effective, it primarily
focuses on generating responses and lacks mechanisms to explicitly foster
critique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT)
and Critique-Guided-Distillation (CGD) have shown the benefits of explicitly
teaching LLMs how to critique. Motivated by them, we propose Critique
Reinforcement Learning (CRL), where the model is tasked with generating a
critique for a given (question, solution) pair. The reward is determined solely
by whether the final judgment label $c \in \{\texttt{True}, \texttt{False}\}$
of the generated critique aligns with the ground-truth judgment $c^*$. Building
on this point, we introduce \textsc{Critique-Coder}, which is trained on a
hybrid of RL and CRL by substituting 20\% of the standard RL data with CRL
data. We fine-tune multiple models (\textsc{Critique-Coder}) and evaluate them
on different benchmarks to show their advantages over RL-only models. We show
that \textsc{Critique-Coder} consistently outperforms RL-only baselines on all
the evaluated benchmarks. Notably, our \textsc{Critique-Coder-8B} can reach
over 60\% on LiveCodeBench (v5), outperforming other reasoning models like
DeepCoder-14B and GPT-o1. Beyond code generation, \textsc{Critique-Coder} also
demonstrates enhanced general reasoning abilities, as evidenced by its better
performance on logic reasoning tasks from the BBEH dataset. This indicates that
the application of CRL on coding datasets enhances general reasoning and
critique abilities, which are transferable across a broad range of tasks.
Hence, we believe that CRL works as a great complement to standard RL for LLM
reasoning.

</details>


### [13] [ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents](https://arxiv.org/abs/2509.22830)
*Hwan Chang,Yonghyun Jun,Hwanhee Lee*

Main category: cs.CL

> 提出了一种名为ChatInject的攻击方法，显示了大语言模型在某些类型的攻击中的脆弱性，强调了对于现有防御策略的不足之处。

<details>
  <summary>Details</summary>

**Motivation:** 旨在探讨大语言模型对于依赖结构化聊天模板和易受说服性多轮对话情境操纵的依赖性这一尚未充分研究的漏洞。

**Method:** 引入了一种名为ChatInject的攻击方法，该方法将恶意载荷格式化为模仿原生聊天模板，以此利用模型的指令跟随倾向。发展了一种基于说服的多轮变体，通过对话轮次使代理接受并执行通常有疑点的动作。

**Result:** 实验表明，ChatInject攻击成功率显著高于传统指令注入方法。多轮对话尤其表现出色，平均成功率高达52.33%。基于聊天模板的载荷显示了跨模型的强烈可转移性，即使是对封闭源LLM也有效。现有基于提示的防御措施对此攻击方法基本上无效，尤其是对抗多轮变体。

**Conclusion:** 研究结果突显了当前代理系统中的漏洞。

**Abstract:** The growing deployment of large language model (LLM) based agents that
interact with external environments has created new attack surfaces for
adversarial manipulation. One major threat is indirect prompt injection, where
attackers embed malicious instructions in external environment output, causing
agents to interpret and execute them as if they were legitimate prompts. While
previous research has focused primarily on plain-text injection attacks, we
find a significant yet underexplored vulnerability: LLMs' dependence on
structured chat templates and their susceptibility to contextual manipulation
through persuasive multi-turn dialogues. To this end, we introduce ChatInject,
an attack that formats malicious payloads to mimic native chat templates,
thereby exploiting the model's inherent instruction-following tendencies.
Building on this foundation, we develop a persuasion-driven Multi-turn variant
that primes the agent across conversational turns to accept and execute
otherwise suspicious actions. Through comprehensive experiments across frontier
LLMs, we demonstrate three critical findings: (1) ChatInject achieves
significantly higher average attack success rates than traditional prompt
injection methods, improving from 5.18% to 32.05% on AgentDojo and from 15.13%
to 45.90% on InjecAgent, with multi-turn dialogues showing particularly strong
performance at average 52.33% success rate on InjecAgent, (2)
chat-template-based payloads demonstrate strong transferability across models
and remain effective even against closed-source LLMs, despite their unknown
template structures, and (3) existing prompt-based defenses are largely
ineffective against this attack approach, especially against Multi-turn
variants. These findings highlight vulnerabilities in current agent systems.

</details>


### [14] [Learning to Detect Relevant Contexts and Knowledge for Response Selection in Retrieval-based Dialogue Systems](https://arxiv.org/abs/2509.22845)
*Kai Hua,Zhiyuan Feng,Chongyang Tao,Rui Yan,Lu Zhang*

Main category: cs.CL

> 本文提出了一个名为RSM-DCK的对话响应选择模型，通过层次结构的选择机制，可以有效提高对话模型在检索响应时的准确性和鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 当前的检索式对话系统在匹配模型上投入了大量努力以利用神经网络，但过多无用信息会影响匹配过程并导致性能下降。为了处理这一问题，提出了能够提取上下文中重要部分的模型。

**Method:** 本文提出了一种名为RSM-DCK的多轮对话响应选择模型，该模型可以检测上下文和知识集合中相关部分。模型首先使用最近的上下文作为查询，以词级别和话语级别语义预选相关部分。接着，响应候选者与所选上下文和知识集合分别进行交互。最后，上下文和响应候选者的融合表示用于更自信地后选相关知识集合以进行匹配。

**Result:** 该模型在两个基准数据集上进行了测试，评估结果表明方法优于现有方法，并能够有效地识别响应选择中相关的上下文和知识。

**Conclusion:** 本模型能够增强对话系统在选取上下文和知识集合中更为相关部分的能力，最终提升了匹配效果与对话质量。

**Abstract:** Recently, knowledge-grounded conversations in the open domain gain great
attention from researchers. Existing works on retrieval-based dialogue systems
have paid tremendous efforts to utilize neural networks to build a matching
model, where all of the context and knowledge contents are used to match the
response candidate with various representation methods. Actually, different
parts of the context and knowledge are differentially important for recognizing
the proper response candidate, as many utterances are useless due to the topic
shift. Those excessive useless information in the context and knowledge can
influence the matching process and leads to inferior performance. To address
this problem, we propose a multi-turn \textbf{R}esponse \textbf{S}election
\textbf{M}odel that can \textbf{D}etect the relevant parts of the
\textbf{C}ontext and \textbf{K}nowledge collection (\textbf{RSM-DCK}). Our
model first uses the recent context as a query to pre-select relevant parts of
the context and knowledge collection at the word-level and utterance-level
semantics. Further, the response candidate interacts with the selected context
and knowledge collection respectively. In the end, The fused representation of
the context and response candidate is utilized to post-select the relevant
parts of the knowledge collection more confidently for matching. We test our
proposed model on two benchmark datasets. Evaluation results indicate that our
model achieves better performance than the existing methods, and can
effectively detect the relevant context and knowledge for response selection.

</details>


### [15] [Towards Generalizable Implicit In-Context Learning with Attention Routing](https://arxiv.org/abs/2509.22854)
*Jiaqian Li,Yanshu Li,Ligong Han,Ruixiang Tang,Wenya Wang*

Main category: cs.CL

> In-Context Routing (ICR) is proposed as a more generalizable implicit in-context learning method that outperforms existing approaches in evaluations and across out-of-domain tasks.

<details>
  <summary>Details</summary>

**Motivation:** To improve generalizability over existing implicit in-context learning methods which largely rely on injecting shift vectors into residual flows, constructed from labeled demonstrations or task-specific alignment.

**Method:** In-Context Routing (ICR), a novel implicit ICL method that internalizes generalizable ICL patterns at the attention logits level, extracting reusable structural directions and employing a learnable input-conditioned router to modulate attention logits.

**Result:** ICR consistently outperforms prior implicit ICL methods that require task-specific retrieval or training on 12 real-world datasets across various domains and multiple LLMs, while showing robust generalization to out-of-domain tasks.

**Conclusion:** ICR pushes the boundary of ICL's practical value by enabling a train-once-and-reuse framework and demonstrating better generalization and performance than existing methods.

**Abstract:** Implicit in-context learning (ICL) has newly emerged as a promising paradigm
that simulates ICL behaviors in the representation space of Large Language
Models (LLMs), aiming to attain few-shot performance at zero-shot cost.
However, existing approaches largely rely on injecting shift vectors into
residual flows, which are typically constructed from labeled demonstrations or
task-specific alignment. Such designs fall short of utilizing the structural
mechanisms underlying ICL and suffer from limited generalizability. To address
this, we propose In-Context Routing (ICR), a novel implicit ICL method that
internalizes generalizable ICL patterns at the attention logits level. It
extracts reusable structural directions that emerge during ICL and employs a
learnable input-conditioned router to modulate attention logits accordingly,
enabling a train-once-and-reuse framework. We evaluate ICR on 12 real-world
datasets spanning diverse domains and multiple LLMs. The results show that ICR
consistently outperforms prior implicit ICL methods that require task-specific
retrieval or training, while demonstrating robust generalization to
out-of-domain tasks where existing methods struggle. These findings position
ICR to push the boundary of ICL's practical value.

</details>


### [16] [The Bias is in the Details: An Assessment of Cognitive Bias in LLMs](https://arxiv.org/abs/2509.22856)
*R. Alexander Knipper,Charles S. Knipper,Kaiqi Zhang,Valerie Sims,Clint Bowers,Santu Karmaker*

Main category: cs.CL

> 研究大规模评估了45个LLMs中的8个基本认知偏见，发现LLMs在不同判断和决策情境中表现出偏见的一致性行为，模型大小和提示的特定性会影响偏见的易感性。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型（LLMs）越来越多地嵌入到现实世界的决策过程中，检查它们表现出的认知偏差的程度变得至关重要。

**Method:** 本研究提出了一种基于多项选择任务的新评估框架，与心理学家合作定制了220个决策场景的数据集，针对基本的认知偏见，并提出了一种可扩展的方法，从人类编写的情景模板生成多样化的提示。

**Result:** 分析结果显示，各模型在针对锚定、可用性、确认、框架、解释、过度归因、前景理论、代表性偏见等方面判断和决策情境中，表现出偏见一致性行为的比例为17.8%-57.3%。研究还发现，模型大小超过32B参数时可降低偏见39.5%，而更高细粒度的提示可以将大多数偏见减少高达14.9%，除过度归因（被煽动高达8.8%之外）。

**Conclusion:** 本研究证明了LLMs在判断和决策情境中展现认知偏见一致性行为，并进一步探索了模型大小和提示特定性对偏见影响的复杂性。

**Abstract:** As Large Language Models (LLMs) are increasingly embedded in real-world
decision-making processes, it becomes crucial to examine the extent to which
they exhibit cognitive biases. Extensively studied in the field of psychology,
cognitive biases appear as systematic distortions commonly observed in human
judgments. This paper presents a large-scale evaluation of eight
well-established cognitive biases across 45 LLMs, analyzing over 2.8 million
LLM responses generated through controlled prompt variations. To achieve this,
we introduce a novel evaluation framework based on multiple-choice tasks,
hand-curate a dataset of 220 decision scenarios targeting fundamental cognitive
biases in collaboration with psychologists, and propose a scalable approach for
generating diverse prompts from human-authored scenario templates. Our analysis
shows that LLMs exhibit bias-consistent behavior in 17.8-57.3% of instances
across a range of judgment and decision-making contexts targeting anchoring,
availability, confirmation, framing, interpretation, overattribution, prospect
theory, and representativeness biases. We find that both model size and prompt
specificity play a significant role on bias susceptibility as follows: larger
size (>32B parameters) can reduce bias in 39.5% of cases, while higher prompt
detail reduces most biases by up to 14.9%, except in one case
(Overattribution), which is exacerbated by up to 8.8%.

</details>


### [17] [Lexicon-Enriched Graph Modeling for Arabic Document Readability Prediction](https://arxiv.org/abs/2509.22870)
*Passant Elchafei,Mayar Osama,Mohamed Rageh,Mervat Abuelkheir*

Main category: cs.CL

> 研究阐述了一种结合词汇表信息和上下文嵌入的图基方法来预测阿拉伯语文档的可读性，并发现融合多种模型的预测结果能提高文档层面的预测准确度。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机是提出一种有效的方法来预测阿拉伯语文档级别的可读性。

**Method:** 该方法使用基于图的方法，结合词汇表信息和上下文嵌入来预测文档可读性。通过图神经网络和转换器句编码器的晚期融合来结合两种模型的预测结果。

**Result:** 我们提出了一种基于图的方法，结合词汇表来预测阿拉伯语文档级别的可读性，这是作为BAREC共享任务2025约束性赛道的一部分开发的。该系统将每个文档建模为一个句子级图，节点表示句子和词根，边捕捉诸如词汇共现和类别成员资格等语言关系。句子节点通过Samer词汇表中的特征和阿拉伯语转换器模型的上下文嵌入进行丰富。图神经网络（GNN）和转换器句编码器作为两个独立的分支进行训练，其预测结果在推理过程中通过晚期融合相结合。对于文档级别的预测，句子级别的输出使用最大池化聚集，以反映最困难的句子。实验结果表明，这种方法在多个可读性指标上优于单独的GNN或转换器分支。总体而言，研究结果表明融合在文档级别有优势，但GNN单独的方法在句子级别的可读性预测方面仍然更强大。

**Conclusion:** 研究得出结论，融合模型在文档级可读性预测中表现出色，但单独使用GNN模型在句子级别的可读性预测中更为准确。

**Abstract:** We present a graph-based approach enriched with lexicons to predict
document-level readability in Arabic, developed as part of the Constrained
Track of the BAREC Shared Task 2025. Our system models each document as a
sentence-level graph, where nodes represent sentences and lemmas, and edges
capture linguistic relationships such as lexical co-occurrence and class
membership. Sentence nodes are enriched with features from the SAMER lexicon as
well as contextual embeddings from the Arabic transformer model. The graph
neural network (GNN) and transformer sentence encoder are trained as two
independent branches, and their predictions are combined via late fusion at
inference. For document-level prediction, sentence-level outputs are aggregated
using max pooling to reflect the most difficult sentence. Experimental results
show that this hybrid method outperforms standalone GNN or transformer branches
across multiple readability metrics. Overall, the findings highlight that
fusion offers advantages at the document level, but the GNN-only approach
remains stronger for precise prediction of sentence-level readability.

</details>


### [18] [HEART: Emotionally-driven test-time scaling of Language Models](https://arxiv.org/abs/2509.22876)
*Gabriela Pinto,Palash Goyal,Yiwen Song,Souradip Chakraborty,Zifeng Wang,Tomas Pfister,Hamid Palangi*

Main category: cs.CL

> HEART框架通过情感反馈进行迭代自我修正，显著提升了语言模型在复杂推理任务上的性能，但在没有外部验证的情况下仍存在问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有的test-time scaling策略，如自我反省，主要集中在逻辑或结构性改进上，忽视了情感反馈的指导潜力。受心理研究启发，提出HEART框架，探索情感在引导认知表现上的潜力。

**Method:** HEART框架使用基于六种基本情绪（由Paul Ekman博士分类）的情感驱动提示进行迭代自我修正，旨在通过情感反馈指导模型克服逻辑或结构上的短板，探索更合理的推理路径。

**Result:** 在具有挑战性的推理基准测试如OlympiadBench, Humanity's Last Exam和SimpleQA上，HEART方法在有oracle验证器的指导下展现了明显的更深层次的推理能力，并且在准确性上取得了显著且一致的进步。

**Conclusion:** 研究结果表明，机器推理的下一步可能不仅在于逻辑的精炼，还在于理解和利用模型背后的情感核心。然而，实际部署中仍面临关键瓶颈，在没有验证器的情况下，性能一致性较差，这将是未来研究的关键挑战。

**Abstract:** Test-time scaling has shown considerable success in improving the performance
of language models on complex reasoning tasks without requiring fine-tuning.
However, current strategies such as self-reflection primarily focus on logical
or structural refinement. They do not leverage the guiding potential of
affective feedback. Inspired by psychological research showing that emotions
can modulate cognitive performance, we introduce HEART--a novel framework that
uses emotionally-driven prompts for iterative self-correction. HEART provides
feedback on a model's incorrect response using a curated set of concise,
emotionally charged phrases based on the six universal emotions categorized by
Dr. Paul Ekman. By systematically varying the emotional tone of the feedback
across iterations, our method guides the model to escape flawed reasoning paths
and explore more promising alternatives. We evaluate our framework on
challenging reasoning benchmarks including OlympiadBench, Humanity's Last Exam,
and SimpleQA. Our results reveal a significant new phenomenon: when guided by
an oracle verifier, this affective iteration protocol unlocks significantly
deeper reasoning, leading to consistent and substantial increases in accuracy
over state-of-the-art baselines with the same verifier. However, we also
identify a critical bottleneck for practical deployment. In a verifier-free
setting, it struggles to harness these gains consistently, highlighting as a
key challenge for future work. Our findings suggest that the next frontier in
machine reasoning may lie not just in refining logic, but also in understanding
and leveraging the `HEART' of the models.

</details>


### [19] [Infusing Theory of Mind into Socially Intelligent LLM Agents](https://arxiv.org/abs/2509.22887)
*EunJeong Hwang,Yuwei Yin,Giuseppe Carenini,Peter West,Vered Shwartz*

Main category: cs.CL

> 引入ToMAgent，通过整合ToM和对话前瞻，训练LLM生成有助于实现对话目标的心理状态，提高了对话代理的社会智能。

<details>
  <summary>Details</summary>

**Motivation:** 动机是解决当前聊天机器人和基于LLM的社交代理通常不集成ToM的问题，旨在展示显式使用ToM的LLM在对话中表现得更好，能够更有效地实现目标。

**Method:** 在本研究中，通过简单提示模型在对话轮次之间生成心理状态，已经能显著提高对话质量。进一步提出了ToMAgent（ToMA），该智能对话代理通过将ToM与对话前瞻相结合，训练模型生成最有助于实现对话目标的心理状态。

**Result:** 实验结果表明，在Sotopia交互式社交评估基准上的测试，本方法在一系列基线中表现优异。

**Conclusion:** 研究结果表明，ToMA显示出更具战略性和目标导向的行为，能够在保持良好人际关系的同时，实现长期适应。这是将ToM整合到构建社交智能LLM代理的一个飞跃。

**Abstract:** Theory of Mind (ToM)-an understanding of the mental states of others-is a key
aspect of human social intelligence, yet, chatbots and LLM-based social agents
do not typically integrate it. In this work, we demonstrate that LLMs that
explicitly use ToM get better at dialogue, achieving goals more effectively.
After showing that simply prompting models to generate mental states between
dialogue turns already provides significant benefit, we further introduce
ToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM
with dialogue lookahead to produce mental states that are maximally useful for
achieving dialogue goals. Experiments on the Sotopia interactive social
evaluation benchmark demonstrate the effectiveness of our method over a range
of baselines. Comprehensive analysis shows that ToMA exhibits more strategic,
goal-oriented reasoning behaviors, which enable long-horizon adaptation, while
maintaining better relationships with their partners. Our results suggest a
step forward in integrating ToM for building socially intelligent LLM agents.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [20] [Pathological Truth Bias in Vision-Language Models](https://arxiv.org/abs/2509.22674)
*Yash Thube*

Main category: cs.CV

> 研究提出MATS来评估视觉语言模型在面对视觉矛盾陈述时的表现，发现生成式模型容易失效，而对比编码器则更可靠。

<details>
  <summary>Details</summary>

**Motivation:** 视觉语言模型（VLMs）虽然在迅速进步，但标准基准测试可能会隐藏系统性失败，从而降低现实世界的信任度。

**Method:** 引入了MATS（多模态真实空间化审计），用于测量模型是否拒绝视觉矛盾的陈述，并提出了两个指标：空间一致性得分（SCS）和错误同意率（IAR）。

**Result:** 指令调优的生成式VLMs（如LLaVA 1.5，QwenVLchat）显示出很低的SCS和很高的IAR，而对比编码器（如CLIP，SigLIP）则更为稳健。

**Conclusion:** 激活补丁法定位了模型失效的具体原因，并提出了具体的修复路径，该方法可以用于改进模型。

**Abstract:** Vision Language Models (VLMs) are improving quickly, but standard benchmarks
can hide systematic failures that reduce real world trust. We introduce MATS
(Multimodal Audit for Truthful Spatialization), a compact behavioral audit that
measures whether models reject visually contradicted statements, and two
metrics Spatial Consistency Score (SCS) and Incorrect Agreement Rate (IAR).
Instruction tuned generative VLMs (LLaVA 1.5, QwenVLchat) exhibit very low SCS
and high IAR, while contrastive encoders (CLIP, SigLIP) are far more robust.
Activation patching causally localizes failure loci (mid to late cross
attention for generative models, pooled projection components for contrastive
models) and suggests concrete repair paths.

</details>


### [21] [Scale and Rotation Estimation of Similarity-Transformed Images via Cross-Correlation Maximization Based on Auxiliary Function Method](https://arxiv.org/abs/2509.22686)
*Shinji Yamashita,Yuma Kinoshita,Hitoshi Kiya*

Main category: cs.CV

> The paper introduces an efficient algorithm for joint scale and rotation estimation between two images with sub-pixel precision, using Fourier transform in log-polar coordinates and a cross-correlation maximization strategy.

<details>
  <summary>Details</summary>

**Motivation:** Traditional phase-correlation methods are limited in handling scale and rotation changes, common in camera zooming or rotational movements, and are incapable of providing accurate alignment in these cases.

**Method:** The proposed method integrates scale and rotation estimation through Fourier transform in log-polar coordinates and maximizes cross-correlation using an auxiliary function method to achieve sub-pixel precision.

**Result:** Experiments show lower mean estimation errors for scale and rotation compared to conventional Fourier transform-based techniques that employ discrete cross-correlation.

**Conclusion:** The new algorithm offers a more precise alternative to existing methods for image alignment when scale and rotation changes are involved.

**Abstract:** This paper introduces a highly efficient algorithm capable of jointly
estimating scale and rotation between two images with sub-pixel precision.
Image alignment serves as a critical process for spatially registering images
captured from different viewpoints, and finds extensive use in domains such as
medical imaging and computer vision. Traditional phase-correlation techniques
are effective in determining translational shifts; however, they are inadequate
when addressing scale and rotation changes, which often arise due to camera
zooming or rotational movements. In this paper, we propose a novel algorithm
that integrates scale and rotation estimation based on the Fourier transform in
log-polar coordinates with a cross-correlation maximization strategy,
leveraging the auxiliary function method. By incorporating sub-pixel-level
cross-correlation our method enables precise estimation of both scale and
rotation. Experimental results demonstrate that the proposed method achieves
lower mean estimation errors for scale and rotation than conventional Fourier
transform-based techniques that rely on discrete cross-correlation.

</details>


### [22] [Robust Object Detection for Autonomous Driving via Curriculum-Guided Group Relative Policy Optimization](https://arxiv.org/abs/2509.22688)
*Xu Jia*

Main category: cs.CV

> 为了改进多模式大型语言模型在结构化感知任务中的表现，提出了一种强化学习框架，该框架通过策略优化和课程学习来提升模型的鲁棒性和精确性，实验结果和消融研究均支持该方法的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 多模式大型语言模型（MLLMs）在视觉-语言推理方面表现出色，但在需要精确定位和鲁棒性的结构化感知任务中表现不佳。

**Method:** 提出了一种基于强化学习的框架，该框架通过结合基于课程的数据调度和困难感知过滤来增强组相对策略优化（GRPO）。

**Result:** 在自动驾驶基准测试中展示了检测精度和鲁棒性的显著提升。消融研究证实了奖励设计、KL正则化和课程节奏对于收敛稳定性和泛化能力的重要性。

**Conclusion:** 发现以强化学习驱动的优化结合结构化数据课程作为提升鲁棒性和可解释性多模式检测的可扩展路径。

**Abstract:** Multimodal Large Language Models (MLLMs) excel in vision-language reasoning
but often struggle with structured perception tasks requiring precise
localization and robustness. We propose a reinforcement learning framework that
augments Group Relative Policy Optimization (GRPO) with curriculum-based data
scheduling and difficulty-aware filtering. This approach stabilizes
optimization under sparse, noisy rewards and enables progressive adaptation to
complex samples. Evaluations on autonomous driving benchmarks demonstrate
substantial improvements in detection accuracy and robustness. Ablation studies
confirm the importance of reward design, KL regularization, and curriculum
pacing for convergence stability and generalization. Our findings highlight
reinforcement-driven optimization with structured data curricula as a scalable
path toward robust and interpretable multimodal detection.

</details>


### [23] [Graph-Theoretic Consistency for Robust and Topology-Aware Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2509.22689)
*Ha-Hieu Pham,Minh Le,Han Huynh,Nguyen Quoc Khanh Le,Huy-Hieu Pham*

Main category: cs.CV

> 提出了一种新的框架Topology Graph Consistency (TGC)，用于半监督语义分割，在计算病理学中显著改善了噪声伪标签的问题，并在GlaS和CRAG数据集上展示了优越的性能。

<details>
  <summary>Details</summary>

**Motivation:** 半监督语义分割（Semi-supervised semantic segmentation, SSSS）对于计算病理学至关重要，在计算病理学中，密集的注释成本高昂且有限。现有方法通常依赖于像素级一致性，这会传播噪声伪标签并生成破碎或拓扑无效的掩码。

**Method:** 我们提出了一种名为拓扑图一致性（Topology Graph Consistency, TGC）的框架，该框架通过在预测图和参考图之间对齐拉普拉斯谱、连通分量数量和邻接统计量，整合了图论约束，从而强制全局拓扑结构，提高了分割准确性。

**Result:** 实验结果表明，TGC在5-10%的监督下达到了最先进的性能，并大大缩短了与完全监督之间的差距。

**Conclusion:** TGC是一个能够显著提高半监督语义分割效果的方法，在计算病理学领域显示出巨大潜力。

**Abstract:** Semi-supervised semantic segmentation (SSSS) is vital in computational
pathology, where dense annotations are costly and limited. Existing methods
often rely on pixel-level consistency, which propagates noisy pseudo-labels and
produces fragmented or topologically invalid masks. We propose Topology Graph
Consistency (TGC), a framework that integrates graph-theoretic constraints by
aligning Laplacian spectra, component counts, and adjacency statistics between
prediction graphs and references. This enforces global topology and improves
segmentation accuracy. Experiments on GlaS and CRAG demonstrate that TGC
achieves state-of-the-art performance under 5-10% supervision and significantly
narrows the gap to full supervision. Code is available at
https://github.com/hieuphamha19/TGC.

</details>


### [24] [A review of Recent Techniques for Person Re-Identification](https://arxiv.org/abs/2509.22690)
*Andrea Asperti,Salvatore Fiorilla,Simone Nardi,Lorenzo Orsini*

Main category: cs.CV

> The paper surveys both supervised and unsupervised person re-identification techniques, highlighting advancements and the narrowing performance gap between them.

<details>
  <summary>Details</summary>

**Motivation:** To address the scalability issues of supervised person re-identification due to the need for large amounts of labeled data, the paper reviews both supervised and unsupervised techniques, focusing on recent advancements.

**Method:** The paper categorizes and reviews significant publications in supervised person re-identification and explores recent advancements in unsupervised person re-identification.

**Result:** The survey uncovers limited scope for improvement in supervised methods and highlights promising developments in unsupervised methods.

**Conclusion:** The paper concludes by suggesting that performance gaps between supervised and unsupervised methods are narrowing, with potential convergence on the horizon.

**Abstract:** Person re-identification (ReId), a crucial task in surveillance, involves
matching individuals across different camera views. The advent of Deep
Learning, especially supervised techniques like Convolutional Neural Networks
and Attention Mechanisms, has significantly enhanced person Re-ID. However, the
success of supervised approaches hinges on vast amounts of annotated data,
posing scalability challenges in data labeling and computational costs. To
address these limitations, recent research has shifted towards unsupervised
person re-identification. Leveraging abundant unlabeled data, unsupervised
methods aim to overcome the need for pairwise labelled data. Although
traditionally trailing behind supervised approaches, unsupervised techniques
have shown promising developments in recent years, signalling a narrowing
performance gap. Motivated by this evolving landscape, our survey pursues two
primary objectives. First, we review and categorize significant publications in
supervised person re-identification, providing an in-depth overview of the
current state-of-the-art and emphasizing little room for further improvement in
this domain. Second, we explore the latest advancements in unsupervised person
re-identification over the past three years, offering insights into emerging
trends and shedding light on the potential convergence of performance between
supervised and unsupervised paradigms. This dual-focus survey aims to
contribute to the evolving narrative of person re-identification, capturing
both the mature landscape of supervised techniques and the promising outcomes
in the realm of unsupervised learning.

</details>


### [25] [Sequential Token Merging: Revisiting Hidden States](https://arxiv.org/abs/2509.22691)
*Yan Wen,Peng Ye,Lin Zhang,Baopu Li,Jiakang Yuan,Yaoxin Yang,Tao Chen*

Main category: cs.CV

> The paper introduces Sequential Token Merging (STM) to improve the efficiency of Vision Mambas (ViMs) by addressing their intrinsic Limited Directional Sequential Dependence (LDSD), resulting in better token efficiency with minimal accuracy losses and paving the way for more efficient state-space models. Codes will be released soon for further research and development in this field.

<details>
  <summary>Details</summary>

**Motivation:** The existing methods do not fully address the efficiency issues of Vision Mambas (ViMs) due to overlooked intrinsic Limited Directional Sequential Dependence (LDSD). The goal is to improve token efficiency by reducing redundancy while preserving critical information flow.

**Method:** Sequential Token Merging (STM) is proposed, which includes bidirectional nearest neighbor merging to maintain sequential dependencies and hidden states protection to stabilize states around the class token. This method uses the layer-wise loss convergence of Mamba to enhance stability over time.

**Result:** Experiments show that STM can achieve a 20% token reduction with only a 1.0% accuracy drop for ViM-Ti and a 40% token reduction with only a 1.4% degradation for ViM-S.

**Conclusion:** STM successfully improves the efficiency of ViMs with minimal accuracy degradation, establishing a new state-of-the-art in efficiency. The method also provides new insights into the state-space model dynamics.

**Abstract:** Vision Mambas (ViMs) achieve remarkable success with sub-quadratic
complexity, but their efficiency remains constrained by quadratic token scaling
with image resolution. While existing methods address token redundancy, they
overlook ViMs' intrinsic Limited Directional Sequential Dependence (LDSD) - a
critical information flow mechanism revealed in our analysis. We further
identify Mamba's selective scan enables gradual information aggregation in
hidden states. Based on these insights, we propose Sequential Token Merging
(STM), featuring: 1) Bidirectional nearest neighbor merging to preserve
sequential dependencies through symmetric spatial aggregation, and 2) Hidden
states protection to stabilize the hidden states around the class token. STM
strategically leverages Mamba's layer-wise loss convergence to convert temporal
forgetfulness into stability. Experiments demonstrate STM's superiority: 1.0%
accuracy drop for ViM-Ti at 20% token reduction, and only 1.4% degradation for
ViM-S at 40% reduction. Our method achieves state-of-the-art efficiency with
minimal complexity, while providing new insights into state-space model
dynamics. Codes will be released soon.

</details>


### [26] [Deep Learning Empowered Super-Resolution: A Comprehensive Survey and Future Prospects](https://arxiv.org/abs/2509.22692)
*Le Zhang,Ao Li,Qibin Hou,Ce Zhu,Yonina C. Eldar*

Main category: cs.CV

> 该论文提供了关于各类超分辨率方法的全面综述，包括150多种单图像超分辨率（SISR）、近70种视频超分辨率（VSR）和大约30种立体超分辨率（SSR）以及光场超分辨率（LFSR）技术。论文还探讨了领域的开放性问题。

<details>
  <summary>Details</summary>

**Motivation:** 由于深度学习技术的发展和高质量视觉应用需求的增长，超分辨率成为计算机视觉领域的一个热门话题。目前大多数综述只专注于某些特定领域，缺乏一个全面的覆盖。因此，该论文旨在填补这一空白。

**Method:** Structure

**Result:** {
  "tldr": "该论文提供了关于各类超分辨率方法的全面综述，包括150多种单图像超分辨率（SISR）、近70种视频超分辨率（VSR）和大约30种立体超分辨率（SSR）以及光场超分辨率（LFSR）技术。论文还探讨了领域的开放性问题。", 
  "motivation": "由于深度学习技术的发展和高质量视觉应用需求的增长，超分辨率成为计算机视觉领域的一个热门话题。目前大多数综述只专注于某些特定领域，缺乏一个全面的覆盖。因此，该论文旨在填补这一空白。", 
  "method": "论文综述了多种超分辨率方法，并对这些方法进行了分类和分析，还涵盖了相关数据集、评估协议、实证结果和复杂性。", 
  "result": "该论文分析了多种方法并基于不同的骨干结构对进行类别划分，探讨了有价值但较少被研究的问题。", 
  "conclusion": "此研究提供了一个宝贵的资源，用于指导超分辨率领域的研究人员。此外，创建了一个名为https://github.com/AVC2-UESTC/Holistic-Super-Resolution-Review的公开库来便于访问相关信息。" 
}

**Conclusion:** 此研究提供了一个宝贵的资源，用于指导超分辨率领域的研究人员。此外，创建了一个名为https://github.com/AVC2-UESTC/Holistic-Super-Resolution-Review的公开库来便于访问相关信息。

**Abstract:** Super-resolution (SR) has garnered significant attention within the computer
vision community, driven by advances in deep learning (DL) techniques and the
growing demand for high-quality visual applications. With the expansion of this
field, numerous surveys have emerged. Most existing surveys focus on specific
domains, lacking a comprehensive overview of this field. Here, we present an
in-depth review of diverse SR methods, encompassing single image
super-resolution (SISR), video super-resolution (VSR), stereo super-resolution
(SSR), and light field super-resolution (LFSR). We extensively cover over 150
SISR methods, nearly 70 VSR approaches, and approximately 30 techniques for SSR
and LFSR. We analyze methodologies, datasets, evaluation protocols, empirical
results, and complexity. In addition, we conducted a taxonomy based on each
backbone structure according to the diverse purposes. We also explore valuable
yet under-studied open issues in the field. We believe that this work will
serve as a valuable resource and offer guidance to researchers in this domain.
To facilitate access to related work, we created a dedicated repository
available at https://github.com/AVC2-UESTC/Holistic-Super-Resolution-Review.

</details>


### [27] [Learning Hyperspectral Images with Curated Text Prompts for Efficient Multimodal Alignment](https://arxiv.org/abs/2509.22697)
*Abhiroop Chatterjee,Susmita Ghosh*

Main category: cs.CV

> 文章提出了一种基于CLIP框架的对比学习方法，用于优化视觉语言模型处理高光谱图像的任务。通过引入描述性质的提示，提高视觉和文本特征的对齐效果，仅需更新总参数的0.07%就获得了显著的性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 因模型缩放成本高，文章希望优化高光谱图像处理方法，特别是视觉语言模型在高光谱领域的跨模态对齐问题，以更高效地利用数据。

**Method:** 该方法利用CLIP样式的对比框架，通过一个可训练的探测器将视觉特征与大嵌入模型的文本词表示对齐，同时引入描述性质的提示以增强这种对齐。

**Result:** 实验结果显示，该方法在Indian Pines (IP) 和Pavia University (PU) 数据集上显著优于一模态和多模态基线模型，且参数量远低于传统的DCTN和SS-TMNet模型。

**Conclusion:** 实验结果证明，该方法通过少量参数更新，便可大幅提升高光谱图像分析性能，为视觉语言模型在高光谱领域的应用提供了一种高效新途径。

**Abstract:** As data requirements continue to grow, efficient learning increasingly
depends on the curation and distillation of high-value data rather than
brute-force scaling of model sizes. In the case of a hyperspectral image (HSI),
the challenge is amplified by the high-dimensional 3D voxel structure, where
each spatial location is associated with hundreds of contiguous spectral
channels. While vision and language models have been optimized effectively for
natural image or text tasks, their cross-modal alignment in the hyperspectral
domain remains an open and underexplored problem. In this article, we make an
attempt to optimize a Vision-Language Model (VLM) for hyperspectral scene
understanding by exploiting a CLIP-style contrastive training framework. Our
framework maps voxel-level embeddings from a vision backbone onto the latent
space of a frozen large embedding model (LEM), where a trainable probe aligns
vision features with the model's textual token representations. The two
modalities are aligned via a contrastive loss restricted to a curated set of
hard (closest wrong classes) and semi-hard (random distractors) negatives,
along with positive pairs. To further enhance alignment, descriptive prompts
that encode class semantics are introduced and act as structured anchors for
the HSI embeddings. It is seen that the proposed method updates only 0.07
percent of the total parameters, yet yields state-of-the-art performance. For
example, on Indian Pines (IP) the model produces better results over unimodal
and multimodal baselines by +0.92 Overall Accuracy (OA) and +1.60 Kappa
($\kappa$), while on Pavia University (PU) data it provides gains of +0.69 OA
and +0.90 $\kappa$. Moreover, this is achieved with the set of parameters,
nearly 50$\times$ smaller than DCTN and 90$\times$ smaller than SS-TMNet.

</details>


### [28] [Global Prompt Refinement with Non-Interfering Attention Masking for One-Shot Federated Learning](https://arxiv.org/abs/2509.22700)
*Zhuang Qi,Pan Yu,Lei Meng,Sijin Zhou,Han Yu,Xiaoxiao Li,Xiangxu Meng*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Federated Prompt Learning (FPL) enables communication-efficient adaptation by
tuning lightweight prompts on top of frozen pre-trained models. Existing FPL
methods typically rely on global information, which is only available after the
second training round, to facilitate collaboration among client models.
Therefore, they are inherently dependent on multi-round communication to fully
exhibit their strengths. Moreover, existing one-shot federated learning methods
typically focus on fitting seen tasks, but lack cross-task generalization. To
bridge this gap, we propose the Global Prompt Refinement with Non-Interfering
Attention Masking (GPR-NIAM) method for one-shot FPL. The core idea is to
design a masking mechanism that restricts excessive interaction between the
original text embeddings and the learnable prompt embeddings. GPR-NIAM achieves
this through the collaboration of two key modules. Firstly, the attention
isolation module suppresses attention from the learnable prompt tokens to the
original text tokens, and reweights the reverse attention which preserves
generalization across tasks. Secondly, the cross-silo collaborative refinement
module integrates decentralized visual knowledge into a unified base and
calibrates the global prompt through multi-source cross-modal knowledge
alignment, further mitigating the inconsistency caused by data heterogeneity.
Extensive experiments conducted on ten benchmark datasets under two tasks show
that GPR-NIAM outperforms eight state-of-the-art methods in both class-level
and domain-level generalization.

</details>


### [29] [GZSL-MoE: Apprentissage G{é}n{é}ralis{é} Z{é}ro-Shot bas{é} sur le M{é}lange d'Experts pour la Segmentation S{é}mantique de Nuages de Points 3DAppliqu{é} {à} un Jeu de Donn{é}es d'Environnement de Collaboration Humain-Robot](https://arxiv.org/abs/2509.22708)
*Ahed Alboody*

Main category: cs.CV

> 本文介绍了一种结合Mixture-of-Experts的广义零样本学习模型（GZSL-MoE），应用于3D点云的语义分割任务，特别是在人机协作环境中，该模型通过合成未见类别的特征来改进传统的零样本学习。

<details>
  <summary>Details</summary>

**Motivation:** 广义零样本学习已在3D点云的语义分割中显示出重大潜力。然而，当全面的训练数据不可用时，要识别未见类别仍是一个挑战。本文旨在通过结合Mixture-of-Experts技术解决这一问题，从而提高了对复杂3D环境的理解能力。

**Method:** 本文提出了基于专家混合的广义零样本学习模型（GZSL-MoE），该模型结合了生成对抗网络（GAN）或变分自编码器（VAE）以及专家混合层（MoE），用于改进3D点云语义分割任务中的零样本学习能力。MoE层被融入到生成器和判别器中，目的是产生与使用预训练KPConv模型提取的真实特征相近的合成特征，以增强对未见类别的识别。

**Result:** GZSL-MoE模型在语义分割任务中的性能评估显示，它不仅提升了已见类别的性能，对未见类别的识别能力也有所提升，这证明了模型在处理包含未见过的对象类别的3D环境中的有效性。

**Conclusion:** 本文提出的GZSL-MoE模型，通过结合Mixture-of-Experts技术改进了零样本学习模型的生成器和判别器，特别适用于处理在Human-Robot Collaboration环境中训练数据不足的问题，这对于理解复杂3D环境具有重要意义。

**Abstract:** Generative Zero-Shot Learning approach (GZSL) has demonstrated significant
potential in 3D point cloud semantic segmentation tasks. GZSL leverages
generative models like GANs or VAEs to synthesize realistic features (real
features) of unseen classes. This allows the model to label unseen classes
during testing, despite being trained only on seen classes. In this context, we
introduce the Generalized Zero-Shot Learning based-upon Mixture-of-Experts
(GZSL-MoE) model. This model incorporates Mixture-of-Experts layers (MoE) to
generate fake features that closely resemble real features extracted using a
pre-trained KPConv (Kernel Point Convolution) model on seen classes. The main
contribution of this paper is the integration of Mixture-of-Experts into the
Generator and Discriminator components of the Generative Zero-Shot Learning
model for 3D point cloud semantic segmentation, applied to the COVERED dataset
(CollabOratiVE Robot Environment Dataset) for Human-Robot Collaboration (HRC)
environments. By combining the Generative Zero-Shot Learning model with
Mixture-of- Experts, GZSL-MoE for 3D point cloud semantic segmentation provides
a promising solution for understanding complex 3D environments, especially when
comprehensive training data for all object classes is unavailable. The
performance evaluation of the GZSL-MoE model highlights its ability to enhance
performance on both seen and unseen classes. Keywords Generalized Zero-Shot
Learning (GZSL), 3D Point Cloud, 3D Semantic Segmentation, Human-Robot
Collaboration, COVERED (CollabOratiVE Robot Environment Dataset), KPConv,
Mixture-of Experts

</details>


### [30] [IBiT: Utilizing Inductive Biases to Create a More Data Efficient Attention Mechanism](https://arxiv.org/abs/2509.22719)
*Adithya Giri*

Main category: cs.CV

> 文章提出了在Vision Transformers中引入归纳偏置的方法，该方法有助于模型在小数据集上实现更好的性能，同时保持模型的可解释性。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于Transformers模型虽然具有很好的可解释性和随着数据集大小增加而扩展的能力，但缺乏卷积神经网络的归纳偏置特性。而这些偏置特性可以在大数据集上学习到，但在小数据集上则难以实现。

**Method:** 文章提出了在Vision Transformers中引入通过学习到的掩码来实现归纳偏置的方法。

**Result:** 实验结果显示，引入这种归纳偏置的Vision Transformers（称为IBiT）在小数据集上的准确性更高，并且保持了Transformers的可解释性。

**Conclusion:** 通过引入归纳偏置，Vision Transformers不仅在小数据集上表现更好，而且保留了其固有的可解释性优势。

**Abstract:** In recent years, Transformer-based architectures have become the dominant
method for Computer Vision applications. While Transformers are explainable and
scale well with dataset size, they lack the inductive biases of Convolutional
Neural Networks. While these biases may be learned on large datasets, we show
that introducing these inductive biases through learned masks allow Vision
Transformers to learn on much smaller datasets without Knowledge Distillation.
These Transformers, which we call Inductively Biased Image Transformers (IBiT),
are significantly more accurate on small datasets, while retaining the
explainability Transformers.

</details>


### [31] [LayoutAgent: A Vision-Language Agent Guided Compositional Diffusion for Spatial Layout Planning](https://arxiv.org/abs/2509.22720)
*Zezhong Fan,Xiaohan Li,Luyi Ma,Kai Zhao,Liang Peng,Topojoy Biswas,Evren Korpeoglu,Kaushiki Nag,Kannan Achan*

Main category: cs.CV

> LayoutAgent 提出了一种统一视觉-语言推理与组合扩散的方法来生成逼真的多对象场景布局。

<details>
  <summary>Details</summary>

**Motivation:** 促使设计 LayoutAgent 的动机是当前图像生成方法在生成逼真的多对象场景方面存在局限性，特别是在空间布局方面，缺乏视觉上的丰富性和语义一致性。

**Method:** LayoutAgent 方法首先使用视觉-语言模型对输入图像进行预处理，然后采用组合扩散合成尊重场景图对象关系的包围盒，最后使用前景条件图像生成器组成完整的场景。

**Result:** {
  "tldr": "LayoutAgent is proposed to unify vision-language reasoning with compositional diffusion for realistic multi-object scene layout generation, exhibiting superior performance in layout coherence, spatial realism, and aesthetic alignment.", 
  "motivation": "The motivation is to address the limitations of current approaches in generating realistic multi-object scenes where spatial layouts are either semantically or visually lacking, aiming to develop a method that can bridge the gap between image generation and spatial planning capabilities.", 
  "method": "LayoutAgent uses a visual-language model for preprocessing multiple input images, followed by compositional diffusion for synthesize bounding boxes respecting object relations, and an image generator that composes the complete scene using designed prompts.", 
  "result": "Experiments show that LayoutAgent outperforms other state-of-the-art layout generation models in terms of layout coherence, spatial realism, and aesthetic alignment in generated scenes.", 
  "conclusion": "The conclusion is that by integrating vision-language processing with compositional diffusion, LayoutAgent effectively addresses the challenges of generating realistic multi-object scene layouts, offering a superior method compared to existing models in layout generation.", 
  "name": "Structure", 
  "arguments": {
    "result": "Experiments demonstrate that LayoutAgent outperforms other state-of-the-art layout generation models in layout coherence, spatial realism and aesthetic alignment.", 
    "motivation": "While recent advances in diffusion models enable high-quality image generation, they lack explicit spatial reasoning. Traditional spatial planning methods struggle to capture semantic richness in visual scenes, leading to a need to bridge this gap.", 
    "method": "LayoutAgent employs visual-language model to preprocess the inputs and uses compositional diffusion to synthesize bounding boxes respecting object relations, guided by designed prompts for image generation.", 
    "tldr": "LayoutAgent proposes a method unifying vision-language reasoning with compositional diffusion for generating realistic multi-object scene layouts.", 
    "conclusion": "LayoutAgent effectively bridges the gap between high-quality image generation and spatial-planning by integrating vision-language processing with compositional diffusion, showing superior performance in layout coherence, spatial realism, and aesthetic alignment.", 
    "result": "Experiments demonstrate that LayoutAgent outperforms other state-of-the-art layout generation models in layout coherence, spatial realism and aesthetic alignment. "
  }
}

**Conclusion:** 通过将视觉-语言处理与组合扩散相结合，LayoutAgent 有效地解决了生成逼真的多对象场景布局的挑战，并在场景布局一致性、空间真实性和审美一致性方面表现优于现有模型。

**Abstract:** Designing realistic multi-object scenes requires not only generating images,
but also planning spatial layouts that respect semantic relations and physical
plausibility. On one hand, while recent advances in diffusion models have
enabled high-quality image generation, they lack explicit spatial reasoning,
leading to unrealistic object layouts. On the other hand, traditional spatial
planning methods in robotics emphasize geometric and relational consistency,
but they struggle to capture semantic richness in visual scenes. To bridge this
gap, in this paper, we propose LayoutAgent, an agentic framework that unifies
vision-language reasoning with compositional diffusion for layout generation.
Given multiple input images with target objects in them, our method first
employs visual-language model to preprocess the inputs through segmentation,
object size estimation, scene graph construction, and prompt rewriting. Then we
leverage compositional diffusion-a method traditionally used in robotics-to
synthesize bounding boxes that respect object relations encoded in the scene
graph for spatial layouts. In the end, a foreground-conditioned image generator
composes the complete scene by rendering the objects into the planned layout
guided by designed prompts. Experiments demonstrate that LayoutAgent
outperforms other state-of-the-art layout generation models in layout
coherence, spatial realism and aesthetic alignment.

</details>


### [32] [CompareBench: A Benchmark for Visual Comparison Reasoning in Vision-Language Models](https://arxiv.org/abs/2509.22737)
*Jie Cai,Kangning Yang,Lan Fu,Jiaming Ding,Jinlong Li,Huiming Sun,Daitao Xing,Jinglin Shen,Zibo Meng*

Main category: cs.CV

> 研究团队开发了CompareBench基准测试，用于评估视觉语言模型的视觉比较推理能力，发现现有模型在基本任务上也存在局限性，为未来的改进提供了方向。

<details>
  <summary>Details</summary>

**Motivation:** 目的是评估视觉语言模型的一个基本但研究较少的技能——视觉比较推理，并借此发现现有模型的局限性。

**Method:** 该研究提出了一个名为CompareBench的基准测试，用于评估视觉语言模型的视觉比较推理能力。该基准测试包含1000个问题答案对，涵盖四个任务：数量(600)，时间(100)，几何(200)和空间(100)。

**Result:** 结果显示了一些明显的规模趋势，但也揭示了一些关键的局限性：即使是表现最强的模型，在时间顺序和空间关系上的表现也不理想，甚至在基本的计数和几何比较上也会出现人类看来很容易的错误。

**Conclusion:** 研究表明视觉比较仍然是当前视觉语言模型的一个系统性盲点。通过提供控制良好、多样化和诊断性的评估，CompareBench为增进更加可靠的多模态推理提供了基础。

**Abstract:** We introduce CompareBench, a benchmark for evaluating visual comparison
reasoning in vision-language models (VLMs), a fundamental yet understudied
skill. CompareBench consists of 1000 QA pairs across four tasks: quantity
(600), temporal (100), geometric (200), and spatial (100). It is derived from
two auxiliary datasets that we constructed: TallyBench (2000 counting images
with QA) and HistCaps (515 historical images with bilingual captions). We
evaluate both closed-source APIs (OpenAI, Gemini, Claude) and open-source
models (Qwen2.5-VL and Qwen3-VL series). Results show clear scaling trends but
also reveal critical limitations: even the strongest models consistently fail
at temporal ordering and spatial relations, and they often make mistakes in
basic counting and geometric comparisons that are trivial for humans. These
findings demonstrate that visual comparison remains a systematic blind spot for
current VLMs. By providing controlled, diverse, and diagnostic evaluation,
CompareBench establishes a foundation for advancing more reliable multimodal
reasoning.

</details>


### [33] [MILR: Improving Multimodal Image Generation via Test-Time Latent Reasoning](https://arxiv.org/abs/2509.22761)
*Yapeng Mi,Hengli Li,Yanpeng Zhao,Chenxi Li,Huimin Wu,Xiaojian Ma,Song-Chun Zhu,Ying Nian Wu,Qing Li*

Main category: cs.CV

> MILR, a reasoning-augmented method enabling joint reasoning over image and text in a latent vector space, outperforms baselines by 80% on the knowledge-intensive WISE benchmark, demonstrating its efficacy in cross-modal reasoning.

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of existing reasoning-based methods in image generation, which either limit reasoning to a single modality or require high-quality reasoning data for fine-tuning.

**Method:** MILR is a test-time method that performs reasoning over both image and text within a unified latent vector space, utilizing vector representations of discrete image and text tokens guided by an image quality critic implemented through the policy gradient method.

**Result:** MILR achieves state-of-the-art results on GenEval, T2I-CompBench, and WISE, notably improving over the baseline by 80% on WISE.

**Conclusion:** Joint reasoning in the unified latent space is a critical factor in MILR's strong performance, and it also shows capability in temporal and cultural reasoning.

**Abstract:** Reasoning-augmented machine learning systems have shown improved performance
in various domains, including image generation. However, existing
reasoning-based methods for image generation either restrict reasoning to a
single modality (image or text) or rely on high-quality reasoning data for
fine-tuning. To tackle these limitations, we propose MILR, a test-time method
that jointly reasons over image and text in a unified latent vector space.
Reasoning in MILR is performed by searching through vector representations of
discrete image and text tokens. Practically, this is implemented via the policy
gradient method, guided by an image quality critic. We instantiate MILR within
the unified multimodal understanding and generation (MUG) framework that
natively supports language reasoning before image synthesis and thus
facilitates cross-modal reasoning. The intermediate model outputs, which are to
be optimized, serve as the unified latent space, enabling MILR to operate
entirely at test time. We evaluate MILR on GenEval, T2I-CompBench, and WISE,
achieving state-of-the-art results on all benchmarks. Notably, on
knowledge-intensive WISE, MILR attains an overall score of 0.63, improving over
the baseline by 80%. Our further analysis indicates that joint reasoning in the
unified latent space is the key to its strong performance. Moreover, our
qualitative studies reveal MILR's non-trivial ability in temporal and cultural
reasoning, highlighting the efficacy of our reasoning method.

</details>


### [34] [UESA-Net: U-Shaped Embedded Multidirectional Shrinkage Attention Network for Ultrasound Nodule Segmentation](https://arxiv.org/abs/2509.22763)
*Tangqi Shi,Pietro Lio*

Main category: cs.CV

> UESA-Net, a U-shaped network with multidirectional shrinkage attention, is proposed for better ultrasound image segmentation of breast and thyroid cancers, achieving state-of-the-art performance.

<details>
  <summary>Details</summary>

**Motivation:** To develop a segmentation framework that improves the detection and segmentation of lesions in noisy ultrasound images of breast and thyroid cancers, addressing the semantic gap between global context and local detail.

**Method:** UESA-Net utilizes an encoder-decoder structure with attention modules that operate in horizontal, vertical, and depth directions. It incorporates a shrinkage strategy to enhance the integration of prior knowledge and local features for better context modeling.

**Result:** UESA-Net demonstrated superior performance on TN3K and BUSI datasets with IoU scores of 0.8487 and 0.6495, respectively.

**Conclusion:** UESA-Net effectively enhances robustness and accuracy in ultrasound segmentation by integrating multidirectional spatial information and prior knowledge, outperforming existing methods.

**Abstract:** Background: Breast and thyroid cancers pose an increasing public-health
burden. Ultrasound imaging is a cost-effective, real-time modality for lesion
detection and segmentation, yet suffers from speckle noise, overlapping
structures, and weak global-local feature interactions. Existing networks
struggle to reconcile high-level semantics with low-level spatial details. We
aim to develop a segmentation framework that bridges the semantic gap between
global context and local detail in noisy ultrasound images.
  Methods: We propose UESA-Net, a U-shaped network with multidirectional
shrinkage attention. The encoder-decoder architecture captures long-range
dependencies and fine-grained structures of lesions. Within each encoding
block, attention modules operate along horizontal, vertical, and depth
directions to exploit spatial details, while a shrinkage (threshold) strategy
integrates prior knowledge and local features. The decoder mirrors the encoder
but applies a pairwise shrinkage mechanism, combining prior low-level physical
cues with corresponding encoder features to enhance context modeling.
  Results: On two public datasets - TN3K (3493 images) and BUSI (780 images) -
UESA-Net achieved state-of-the-art performance with intersection-over-union
(IoU) scores of 0.8487 and 0.6495, respectively.
  Conclusions: UESA-Net effectively aggregates multidirectional spatial
information and prior knowledge to improve robustness and accuracy in breast
and thyroid ultrasound segmentation, demonstrating superior performance to
existing methods on multiple benchmarks.

</details>


### [35] [PartCo: Part-Level Correspondence Priors Enhance Category Discovery](https://arxiv.org/abs/2509.22769)
*Fernando Julio Cendra,Kai Han*

Main category: cs.CV

> PartCo 是一种新的框架，通过引入零件级视觉特征对应关系来改进类别发现，从而弥补语义标签和零件级视觉组成之间的差距，并在多个基准数据集上达到了最先进的结果。

<details>
  <summary>Details</summary>

**Motivation:** 现有的一般化类别发现（GCD）方法主要依赖语义标签和全局图像表示，忽视了对于区分紧密相关的类别至关重要的局部细节线索。

**Method:** PartCo 引入了零件级视觉特征对应关系，捕捉更精细的语义结构，增强了现有GCD方法下的类别发现能力，并且能够无缝地集成到现有的GCD方法中。

**Result:** 在多个基准数据集上的大量实验表明，PartCo 显著提升了当前GCD方法的性能，达到了最先进的水平。

**Conclusion:** PartCo 通过引入零件级视觉特征对应关系，弥补了现有GCD方法中语义标签和部分级视觉组成的差距，成功提升了类别发现的性能，并为GCD设立了新标杆。

**Abstract:** Generalized Category Discovery (GCD) aims to identify both known and novel
categories within unlabeled data by leveraging a set of labeled examples from
known categories. Existing GCD methods primarily depend on semantic labels and
global image representations, often overlooking the detailed part-level cues
that are crucial for distinguishing closely related categories. In this paper,
we introduce PartCo, short for Part-Level Correspondence Prior, a novel
framework that enhances category discovery by incorporating part-level visual
feature correspondences. By leveraging part-level relationships, PartCo
captures finer-grained semantic structures, enabling a more nuanced
understanding of category relationships. Importantly, PartCo seamlessly
integrates with existing GCD methods without requiring significant
modifications. Our extensive experiments on multiple benchmark datasets
demonstrate that PartCo significantly improves the performance of current GCD
approaches, achieving state-of-the-art results by bridging the gap between
semantic labels and part-level visual compositions, thereby setting new
benchmarks for GCD. Project page: https://visual-ai.github.io/partco

</details>


### [36] [DEFT: Decompositional Efficient Fine-Tuning for Text-to-Image Models](https://arxiv.org/abs/2509.22793)
*Komal Kumar,Rao Muhammad Anwer,Fahad Shahbaz Khan,Salman Khan,Ivan Laptev,Hisham Cholakkal*

Main category: cs.CV

> DEFT框架通过在权重更新中应用低秩分解技术，实现高效微调，同时保持多种任务中的可编辑性，在多个数据集与模型上取得了领先性能。

<details>
  <summary>Details</summary>

**Motivation:** 解决文本到图像模型微调中的问题，包括在适应特定任务或数据集时，利用最少的计算资源和限制可训练参数数量，同时保持不同上下文生成的理解能力。

**Method:** DEFT, 分解式高效微调框架通过将预训练权重矩阵的更新分解为两个部分，其中一个部分是向低秩矩阵张成的低秩子空间的补空间的投影，另一个部分是低秩更新。单个可训练的低秩矩阵定义了子空间，而另一个可训练的低秩矩阵允许在该子空间内灵活地调整参数。

**Result:** 在个性化方面使用了Dreambooth和Dreambench Plus，物体和场景适应方面使用了InsDet数据集，以及通过视觉上下文学习实现的通用图像生成框架VisualCloze数据集，并结合了Stable Diffusion和统一模型，展现了最先进的性能，凸显了有效微调的潜力。

**Conclusion:** 实验结果表明DEFT能够高效地进行微调，并突出展现了高效微调框架的潜力和性能优势。

**Abstract:** Efficient fine-tuning of pre-trained Text-to-Image (T2I) models involves
adjusting the model to suit a particular task or dataset while minimizing
computational resources and limiting the number of trainable parameters.
However, it often faces challenges in striking a trade-off between aligning
with the target distribution: learning a novel concept from a limited image for
personalization and retaining the instruction ability needed for unifying
multiple tasks, all while maintaining editability (aligning with a variety of
prompts or in-context generation). In this work, we introduce DEFT,
Decompositional Efficient Fine-Tuning, an efficient fine-tuning framework that
adapts a pre-trained weight matrix by decomposing its update into two
components with two trainable matrices: (1) a projection onto the complement of
a low-rank subspace spanned by a low-rank matrix, and (2) a low-rank update.
The single trainable low-rank matrix defines the subspace, while the other
trainable low-rank matrix enables flexible parameter adaptation within that
subspace. We conducted extensive experiments on the Dreambooth and Dreambench
Plus datasets for personalization, the InsDet dataset for object and scene
adaptation, and the VisualCloze dataset for a universal image generation
framework through visual in-context learning with both Stable Diffusion and a
unified model. Our results demonstrated state-of-the-art performance,
highlighting the emergent properties of efficient fine-tuning. Our code is
available on \href{https://github.com/MAXNORM8650/DEFT}{DEFTBase}.

</details>


### [37] [VideoScore2: Think before You Score in Generative Video Evaluation](https://arxiv.org/abs/2509.22799)
*Xuan He,Dongfu Jiang,Ping Nie,Minghao Liu,Zhengxuan Jiang,Mingyi Su,Wentao Ma,Junru Lin,Chun Ye,Yi Lu,Keming Wu,Benjamin Schneider,Quy Duc Do,Zhuofeng Li,Yiming Jia,Yuxuan Zhang,Guo Cheng,Haozhe Wang,Wangchunshu Zhou,Qunshu Lin,Yuanxing Zhang,Ge Zhang,Wenhao Huang,Wenhu Chen*

Main category: cs.CV

> 提出VideoScore2，一个多维、可解释且与人类评价一致的框架，用于评估视频的质量、文本与视频的对齐程度以及物理和常识一致性，并提供详细的推理过程。

<details>
  <summary>Details</summary>

**Motivation:** 现有的评估器和奖励模型要么只能提供单一的、不透明的评分，要么解释性较差，或者只能提供粗略的分析，无法全面捕捉视频质量评估的特性。

**Method:** 该框架通过监督微调和强化学习（使用Group Relative Policy Optimization (GRPO)）的两阶段训练流程，训练在大规模数据集VideoFeedback2上的模型。

**Result:** VideoScore2在内部基准VideoScore-Bench-v2上取得了44.35（+5.94）的精度，并在四个外部基准上平均性能为50.37（+4.32），且提供了可解释的评估。

**Conclusion:** 实验结果表明，VideoScore2不仅性能优越，而且提供了解释性评估，有效连接了评估和可控生成通过有效的奖励建模进行Best-of-N采样。

**Abstract:** Recent advances in text-to-video generation have produced increasingly
realistic and diverse content, yet evaluating such videos remains a fundamental
challenge due to their multi-faceted nature encompassing visual quality,
semantic alignment, and physical consistency. Existing evaluators and reward
models are limited to single opaque scores, lack interpretability, or provide
only coarse analysis, making them insufficient for capturing the comprehensive
nature of video quality assessment. We present VideoScore2, a
multi-dimensional, interpretable, and human-aligned framework that explicitly
evaluates visual quality, text-to-video alignment, and physical/common-sense
consistency while producing detailed chain-of-thought rationales. Our model is
trained on a large-scale dataset VideoFeedback2 containing 27,168
human-annotated videos with both scores and reasoning traces across three
dimensions, using a two-stage pipeline of supervised fine-tuning followed by
reinforcement learning with Group Relative Policy Optimization (GRPO) to
enhance analytical robustness. Extensive experiments demonstrate that
VideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our
in-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance
across four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc),
while providing interpretable assessments that bridge the gap between
evaluation and controllable generation through effective reward modeling for
Best-of-N sampling. Project Page: https://tiger-ai-lab.github.io/VideoScore2/

</details>


### [38] [TRUST: Test-Time Refinement using Uncertainty-Guided SSM Traverses](https://arxiv.org/abs/2509.22813)
*Sahar Dastani,Ali Bahri,Gustavo Adolfo Vargas Hakim,Moslem Yazdanpanah,Mehrdad Noori,David Osowiechi,Samuel Barbeau,Ismail Ben Ayed,Herve Lombaert,Christian Desrosiers*

Main category: cs.CV

> TRUST is a new method for test-time adaptation in state space models, improving their robustness and performance under distribution shifts.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the generalization performance of SSMs like VMamba under distribution shifts, as their performance often degrades significantly in these scenarios.

**Method:** State Space Models (SSMs) are used, with TRUST introduced as a test-time adaptation (TTA) method that uses diverse traversal permutations to generate multiple causal perspectives, and model predictions serve as pseudo-labels to guide updates of the model parameters.

**Result:** TRUST is shown to improve robustness and outperform existing TTA methods, as evidenced by experiments on seven benchmarks.

**Conclusion:** TRUST is the first approach that explicitly leverages the unique architectural properties of SSMs for adaptation, leading to improved performance in terms of robustness.

**Abstract:** State Space Models (SSMs) have emerged as efficient alternatives to Vision
Transformers (ViTs), with VMamba standing out as a pioneering architecture
designed for vision tasks. However, their generalization performance degrades
significantly under distribution shifts. To address this limitation, we propose
TRUST (Test-Time Refinement using Uncertainty-Guided SSM Traverses), a novel
test-time adaptation (TTA) method that leverages diverse traversal permutations
to generate multiple causal perspectives of the input image. Model predictions
serve as pseudo-labels to guide updates of the Mamba-specific parameters, and
the adapted weights are averaged to integrate the learned information across
traversal scans. Altogether, TRUST is the first approach that explicitly
leverages the unique architectural properties of SSMs for adaptation.
Experiments on seven benchmarks show that TRUST consistently improves
robustness and outperforms existing TTA methods.

</details>


### [39] [MMPB: It's Time for Multi-Modal Personalization](https://arxiv.org/abs/2509.22820)
*Jaeik Kim,Woojin Kim,Woohyeon Park,Jaeyoung Do*

Main category: cs.CV

> 本研究通过MMPB全面评估VLM的个性化能力，发现这些模型在多个维度上存在局限性，并提出未来的研究方向。

<details>
  <summary>Details</summary>

**Motivation:** 视觉个性化在面向用户的AI系统中至关重要，如智能家居和医疗保健，但现有VLM在个性化方面的潜力未充分探索。本研究旨在评估VLM的个性化能力。

**Method:** 引入了MMPB，这是首个全面评估视觉-语言模型（VLM）个性化能力的基准测试。MMPB包含10k图像-查询对，并涵盖111个可个性化概念，分为四类：人类、动物、物体和角色。评估个性化性能的过程分为三阶段：概念注入、多轮对话和个人化查询。

**Result:** 结果表明，大多数VLM在维持对话一致性、处理用户偏好和适应视觉线索方面遇到困难。结论揭示了VLM在个性化领域的挑战和发展空间。

**Conclusion:** MMPB通过识别VLM的局限性并提供可扩展基准，为未来的多模态AI研究提供了有价值的洞见和坚实基础。

**Abstract:** Visual personalization is essential in user-facing AI systems such as smart
homes and healthcare, where aligning model behavior with user-centric concepts
is critical. However, recent large Vision-Language Models (VLMs), despite their
broad applicability, remain underexplored in their ability to adapt to
individual users. In this paper, we introduce MMPB, the first extensive
benchmark for evaluating VLMs on personalization. MMPB comprises 10k
image-query pairs and includes 111 personalizable concepts across four
categories: humans, animals, objects, and characters, with the human category
enriched with preference-grounded queries. We structure personalization into
three main task types, each highlighting a different key property of VLMs.
Using 23 widely used VLMs including both open- and closed-source models, we
evaluate personalization performance via a three-stage protocol: concept
injection, multi-turn dialogue, and personalized querying. Our findings
indicate that most VLMs (including some closed-source models) struggle with
personalization, particularly in maintaining consistency over dialogue,
handling user preferences, and adapting to visual cues. Our analysis reveals
that the challenges in VLM personalization (such as refusal behaviors and
long-context forgetting) highlight substantial room for improvement. By
identifying these limitations and offering a scalable benchmark, MMPB offers
valuable insights and a solid foundation for future research toward truly
personalized multi-modal AI. Project Page: aidaslab.github.io/MMPB

</details>


### [40] [Seeing Isn't Believing: Context-Aware Adversarial Patch Synthesis via Conditional GAN](https://arxiv.org/abs/2509.22836)
*Roie Kazoom,Alon Goldberg,Hodaya Cohen,Ofer Hadar*

Main category: cs.CV

> This paper introduces a novel framework for adversarial patch generation that can generate visually realistic and highly effective patches with fully controllable input and target class, achieving state-of-the-art performance across multiple network architectures.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the limitations of existing adversarial patch attacks which are either based on unrealistic white-box assumptions, untargeted objectives, or produce visibly obvious patches.

**Method:** Our method combines a generative U-Net design with Grad-CAM-guided patch placement, enabling semantic-aware localization that maximizes attack effectiveness while preserving visual realism.

**Result:** Extensive experiments demonstrate that the approach achieves state-of-the-art performance, with attack success rates (ASR) and target-class success (TCS) consistently exceeding 99%, surpassing both past white-box attacks and untargeted baselines.

**Conclusion:** By ensuring realism, targeted control, and black-box applicability, the framework proposed sets a new benchmark for adversarial robustness, bridging the gap between theoretical attack strength and practical stealthiness.

**Abstract:** Adversarial patch attacks pose a severe threat to deep neural networks, yet
most existing approaches rely on unrealistic white-box assumptions, untargeted
objectives, or produce visually conspicuous patches that limit real-world
applicability. In this work, we introduce a novel framework for fully
controllable adversarial patch generation, where the attacker can freely choose
both the input image x and the target class y target, thereby dictating the
exact misclassification outcome. Our method combines a generative U-Net design
with Grad-CAM-guided patch placement, enabling semantic-aware localization that
maximizes attack effectiveness while preserving visual realism. Extensive
experiments across convolutional networks (DenseNet-121, ResNet-50) and vision
transformers (ViT-B/16, Swin-B/16, among others) demonstrate that our approach
achieves state-of-the-art performance across all settings, with attack success
rates (ASR) and target-class success (TCS) consistently exceeding 99%.
  Importantly, we show that our method not only outperforms prior white-box
attacks and untargeted baselines, but also surpasses existing non-realistic
approaches that produce detectable artifacts. By simultaneously ensuring
realism, targeted control, and black-box applicability-the three most
challenging dimensions of patch-based attacks-our framework establishes a new
benchmark for adversarial robustness research, bridging the gap between
theoretical attack strength and practical stealthiness.

</details>


### [41] [Learning Temporal Saliency for Time Series Forecasting with Cross-Scale Attention](https://arxiv.org/abs/2509.22839)
*Ibrahim Delibasoglu,Fredrik Heintz*

Main category: cs.CV

> 本文提出了CrossScaleNet，一种结合注意力机制和多尺度处理的新型架构，实现了高性能和高透明度的时间序列预测。实验证明其在时间显著性检测和预测准确性方面均优于现有的基于变压器的模型。

<details>
  <summary>Details</summary>

**Motivation:** 提高时间序列预测模型的可解释性，以提高模型透明度并支持决策制定。

**Method:** CrossScaleNet结合基于patch的跨注意力机制与多尺度处理，以实现高性能和增强的时间解释性。通过在训练过程中嵌入注意力机制，该模型提供了内在的时间显著性解释性，使其决策过程更加透明。

**Result:** 实验表明，CrossScaleNet在合成数据集和公共基准数据集上表现出了识别时间显著性的鲁棒性。在实际预测任务上，该方法在大多数基于变压器的模型之外提供了更好的解释性和预测准确性。

**Conclusion:** CrossScaleNet提供了一种平衡的方法，既能有效捕捉时间显著性，又能在不同复杂度的数据集上实现一流的预测表现。

**Abstract:** Explainability in time series forecasting is essential for improving model
transparency and supporting informed decision-making. In this work, we present
CrossScaleNet, an innovative architecture that combines a patch-based
cross-attention mechanism with multi-scale processing to achieve both high
performance and enhanced temporal explainability. By embedding attention
mechanisms into the training process, our model provides intrinsic
explainability for temporal saliency, making its decision-making process more
transparent. Traditional post-hoc methods for temporal saliency detection are
computationally expensive, particularly when compared to feature importance
detection. While ablation techniques may suffice for datasets with fewer
features, identifying temporal saliency poses greater challenges due to its
complexity. We validate CrossScaleNet on synthetic datasets with known saliency
ground truth and on established public benchmarks, demonstrating the robustness
of our method in identifying temporal saliency. Experiments on real-world
datasets for forecasting task show that our approach consistently outperforms
most transformer-based models, offering better explainability without
sacrificing predictive accuracy. Our evaluations demonstrate superior
performance in both temporal saliency detection and forecasting accuracy.
Moreover, we highlight that existing models claiming explainability often fail
to maintain strong performance on standard benchmarks. CrossScaleNet addresses
this gap, offering a balanced approach that captures temporal saliency
effectively while delivering state-of-the-art forecasting performance across
datasets of varying complexity.

</details>
