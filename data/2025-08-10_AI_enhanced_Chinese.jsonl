{"id": "2508.04795", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.04795", "abs": "https://arxiv.org/abs/2508.04795", "authors": ["Thomas Thebaud", "Yen-Ju Lu", "Matthew Wiesner", "Peter Viechnicki", "Najim Dehak"], "title": "Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM", "comment": "Accepted in the 2025 IEEE Automatic Speech Recognition and\n  Understanding Workshop", "summary": "In dialogue transcription pipelines, Large Language Models (LLMs) are\nfrequently employed in post-processing to improve grammar, punctuation, and\nreadability. We explore a complementary post-processing step: enriching\ntranscribed dialogues by adding metadata tags for speaker characteristics such\nas age, gender, and emotion. Some of the tags are global to the entire\ndialogue, while some are time-variant. Our approach couples frozen audio\nfoundation models, such as Whisper or WavLM, with a frozen LLAMA language model\nto infer these speaker attributes, without requiring task-specific fine-tuning\nof either model. Using lightweight, efficient connectors to bridge audio and\nlanguage representations, we achieve competitive performance on speaker\nprofiling tasks while preserving modularity and speed. Additionally, we\ndemonstrate that a frozen LLAMA model can compare x-vectors directly, achieving\nan Equal Error Rate of 8.8% in some scenarios.", "AI": {"tldr": "本研究提出了一种新的对话转录后处理方法，通过结合冻结的音频基础模型和语言模型来添加说话人特征的元数据标签，如年龄、性别和情绪，从而丰富转录的对话内容。这种方法无需特定任务的微调，并达到了竞争性的性能表现。", "motivation": "现有的对话转录后处理主要侧重于改善语法、标点和可读性，而本研究旨在通过添加时间不变或时间变化的元数据标签来丰富对话的层次信息，从而提供更全面的对话理解支持。", "method": "该研究的方法是利用冻结的音频基础模型和LLAMA语言模型，通过轻量高效的连接器将音频和语言表征连接起来，以推断出对话中的说话人特征。", "result": "研究结果表明，利用冻结的LLAMA模型可以直接比较x-vector，在某些情境下实现了8.8%的等错误率，同时保持模块化和效率。", "conclusion": "研究证明，所提出的方法能有效地为转录后的对话添加元数据标签，提高对话理解的深度，而无需对基础模型进行特定任务的微调。"}}
{"id": "2508.04796", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04796", "abs": "https://arxiv.org/abs/2508.04796", "authors": ["Negar Foroutan", "Clara Meister", "Debjit Paul", "Joel Niklaus", "Sina Ahmadi", "Antoine Bosselut", "Rico Sennrich"], "title": "Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization", "comment": null, "summary": "Tokenization is the first -- and often least scrutinized -- step of most NLP\npipelines. Standard algorithms for learning tokenizers rely on frequency-based\nobjectives, which favor languages dominant in the training data and\nconsequently leave lower-resource languages with tokenizations that are\ndisproportionately longer, morphologically implausible, or even riddled with\n<UNK> placeholders. This phenomenon ultimately amplifies computational and\nfinancial inequalities between users from different language backgrounds. To\nremedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of\nthe widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes\nthe compression gain of the currently worst-compressed language, trading a\nsmall amount of global compression for cross-lingual parity. We find\nempirically that Parity-aware BPE leads to more equitable token counts across\nlanguages, with negligible impact on global compression rate and no substantial\neffect on language-model performance in downstream tasks.", "AI": {"tldr": "本文提出了一种新的分词方法 Parity-aware BPE，旨在解决传统 BPE 算法对资源较少的语言不公平的问题，通过最大化最差压缩语言的压缩增益实现了跨语言的分词平衡，实验证明该方法在维持较低压缩率的同时，未对语言模型性能产生负面影响。", "motivation": "标准的 BPE 算法依赖于基于频率的目标，这意味着占主导地位的语言在训练数据中的表现更佳，而资源较少的语言则会出现长度不一、形态不合理甚至充斥着未定义字符 (UNK) 的分词结果。这样的现象加剧了来自不同语言背景的用户在计算能力和财力上的不平等。", "method": "提出了一种名为Parity-aware Byte Pair Encoding (BPE) 的算法变体，该方法在每次合并步骤中最大化当前压缩效果最差的语言的压缩增益，以此来换取跨语言的均衡性。", "result": "实证研究显示，Parity-aware BPE 方法能够有效地在不同语言之间实现更为均衡的词汇计数，且对全球压缩率的影响微乎其微，并未对下游任务的语言模型性能造成显著影响。", "conclusion": "Parity-aware BPE 提出了一个创新的方式来解决传统的 BPE 算法在处理多语言分词时存在的不均衡问题，保证了压缩效率的同时实现了语言间的公平性。"}}
{"id": "2508.04814", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.04814", "abs": "https://arxiv.org/abs/2508.04814", "authors": ["David Sasu", "Natalie Schluter"], "title": "Pitch Accent Detection improves Pretrained Automatic Speech Recognition", "comment": null, "summary": "We show the performance of Automatic Speech Recognition (ASR) systems that\nuse semi-supervised speech representations can be boosted by a complimentary\npitch accent detection module, by introducing a joint ASR and pitch accent\ndetection model. The pitch accent detection component of our model achieves a\nsignificant improvement on the state-of-the-art for the task, closing the gap\nin F1-score by 41%. Additionally, the ASR performance in joint training\ndecreases WER by 28.3% on LibriSpeech, under limited resource fine-tuning. With\nthese results, we show the importance of extending pretrained speech models to\nretain or re-learn important prosodic cues such as pitch accent.", "AI": {"tldr": "研究发现，通过联合训练ASR和音调重音检测模型，能够显著提升其各自的性能表现，尤其在改进重音检测和减少AR系统的词错率方面表现突出。", "motivation": "研究旨在提升基于半监督语音表示的ASR系统的性能，尤其是在有限资源的情况下，通过结合音调重音检测来增强语义理解。", "method": "研究提出了一个联合自动语音识别(ASR)和音调重音检测的模型，通过整合ASR与重音检测任务，提高两种任务的性能。", "result": "研究展示了使用半监督语音表示的自动语音识别(ASR)系统的性能可以通过引入联合ASR和音调重音检测模型得到提升。该模型的音调重音检测部分在任务上达到了显著的改进，F1分数上的差距缩小了41%。此外，在有限资源微调下，联合训练中的ASR性能将词错误率(WER)降低了28.3%。这些结果表明，扩展预训练语音模型以保留或重新学习关键的韵律线索（如音调重音）的重要性。", "conclusion": "研究证明了音调重音检测模块对于提升半监督ASR系统性能的重要性，并展示了通过联合训练保留或重新学习重音信息的必要性。"}}
{"id": "2508.04826", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04826", "abs": "https://arxiv.org/abs/2508.04826", "authors": ["Tommaso Tosato", "Saskia Helbling", "Yorguin-Jose Mantilla-Ramos", "Mahmood Hegazy", "Alberto Tosato", "David John Lemay", "Irina Rish", "Guillaume Dumas"], "title": "Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History", "comment": null, "summary": "Large language models require consistent behavioral patterns for safe\ndeployment, yet their personality-like traits remain poorly understood. We\npresent PERSIST (PERsonality Stability in Synthetic Text), a comprehensive\nevaluation framework testing 25+ open-source models (1B-671B parameters) across\n500,000+ responses. Using traditional (BFI-44, SD3) and novel LLM-adapted\npersonality instruments, we systematically vary question order, paraphrasing,\npersonas, and reasoning modes. Our findings challenge fundamental deployment\nassumptions: (1) Even 400B+ models exhibit substantial response variability (SD\n> 0.4); (2) Minor prompt reordering alone shifts personality measurements by up\nto 20%; (3) Interventions expected to stabilize behavior, such as\nchain-of-thought reasoning, detailed personas instruction, inclusion of\nconversation history, can paradoxically increase variability; (4) LLM-adapted\ninstruments show equal instability to human-centric versions, confirming\narchitectural rather than translational limitations. This persistent\ninstability across scales and mitigation strategies suggests current LLMs lack\nthe foundations for genuine behavioral consistency. For safety-critical\napplications requiring predictable behavior, these findings indicate that\npersonality-based alignment strategies may be fundamentally inadequate.", "AI": {"tldr": "提出了PERSIST框架评估语言模型的人格稳定性，发现即便非常大的模型在一致性上也有显著问题，这对安全需求高的应用提出了挑战。", "motivation": "大型语言模型在就安全部署需要一致的行为模式，然而，它们类似于人格特征尚不清楚，为此进行了全面性评估。", "method": "本文提出了PERSIST（PERsonality Stability in Synthetic Text），这是一个全面的评估框架，用于测试25个以上的开源语言模型（参数规模从1B到671B不等）超过500,000次的响应。研究使用了传统的（BFI-44, SD3）和新颖的适合LLM的人格测试工具，系统地改变了问题顺序、同义词使用、人设描述和推理模式。", "result": "研究发现挑战了基础的部署假设：1. 即便是400B参数以上的语言模型也表现出极大的响应差异（SD > 0.4）；2. 单纯改变提示词的顺序可以将人格测量值改变高达20%；3. 预期能稳定行为的干预措施，例如链式思考推理、详细的个性指导，以及包含对话历史，反而可能增加变异性；4. 适合LLM的新测试工具表现出与人类版本同样不稳定，确认了架构而非翻译上的限制。", "conclusion": "跨模型规模和缓解策略的这种持续的不稳定表明，现有语言模型缺乏真正行为一致性的基础。对于需要可预测行为的安全关键应用，这些发现表明基于人格的对齐策略可能是基本不足的。"}}
{"id": "2508.04797", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04797", "abs": "https://arxiv.org/abs/2508.04797", "authors": ["Mohab Kishawy", "Ali Abdellatif Hussein", "Jun Chen"], "title": "RetinexDual: Retinex-based Dual Nature Approach for Generalized Ultra-High-Definition Image Restoration", "comment": null, "summary": "Advancements in image sensing have elevated the importance of\nUltra-High-Definition Image Restoration (UHD IR). Traditional methods, such as\nextreme downsampling or transformation from the spatial to the frequency\ndomain, encounter significant drawbacks: downsampling induces irreversible\ninformation loss in UHD images, while our frequency analysis reveals that pure\nfrequency-domain approaches are ineffective for spatially confined image\nartifacts, primarily due to the loss of degradation locality. To overcome these\nlimitations, we present RetinexDual, a novel Retinex theory-based framework\ndesigned for generalized UHD IR tasks. RetinexDual leverages two complementary\nsub-networks: the Scale-Attentive maMBA (SAMBA) and the Frequency Illumination\nAdaptor (FIA). SAMBA, responsible for correcting the reflectance component,\nutilizes a coarse-to-fine mechanism to overcome the causal modeling of mamba,\nwhich effectively reduces artifacts and restores intricate details. On the\nother hand, FIA ensures precise correction of color and illumination\ndistortions by operating in the frequency domain and leveraging the global\ncontext provided by it. Evaluating RetinexDual on four UHD IR tasks, namely\nderaining, deblurring, dehazing, and Low-Light Image Enhancement (LLIE), shows\nthat it outperforms recent methods qualitatively and quantitatively. Ablation\nstudies demonstrate the importance of employing distinct designs for each\nbranch in RetinexDual, as well as the effectiveness of its various components.", "AI": {"tldr": "提出RetinexDual框架，利用互补的SAMBA和FIA子网络解决超高清图像恢复难题，展现优于现有方法的性能。", "motivation": "传统的图像降采样和频域转换方法在处理超高清图像时面临不可逆的信息损失和处理空间局限性图像瑕疵的不足。", "method": "RetinexDual采用基于Retinex理论的框架，包含两个互补子网络：SAMBA和FIA。SAMBA用于反射分量的校正，利用粗细机制来减少因果建模带来的瑕疵。FIA则负责在频域内进行精确的颜色和光照矫正，利用全局上下文来确保充足的修复效果。", "result": "RetinexDual在四项超高清图像恢复任务（去雨、去模糊、去雾、低光图像增强）中，无论是定性还是定量评价都优于近期的方法。消融研究显示每个分支的独特设计及框架各组件的有效性。", "conclusion": "基于Retinex理论的RetinexDual框架，结合SAMBA和FIA子网络，有效克服了传统方法在超高清图像恢复中的局限性，显示了其在多种任务上的优越性能。"}}
{"id": "2508.04903", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.04903", "abs": "https://arxiv.org/abs/2508.04903", "authors": ["Jun Liu", "Zhenglun Kong", "Changdi Yang", "Fan Yang", "Tianqi Li", "Peiyan Dong", "Joannah Nanjekye", "Hao Tang", "Geng Yuan", "Wei Niu", "Wenbin Zhang", "Pu Zhao", "Xue Lin", "Dong Huang", "Yanzhi Wang"], "title": "RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM Systems with Structured Memory", "comment": null, "summary": "Multi-agent large language model (LLM) systems have shown strong potential in\ncomplex reasoning and collaborative decision-making tasks. However, most\nexisting coordination schemes rely on static or full-context routing\nstrategies, which lead to excessive token consumption, redundant memory\nexposure, and limited adaptability across interaction rounds. We introduce\nRCR-Router, a modular and role-aware context routing framework designed to\nenable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge,\nthis is the first routing approach that dynamically selects semantically\nrelevant memory subsets for each agent based on its role and task stage, while\nadhering to a strict token budget. A lightweight scoring policy guides memory\nselection, and agent outputs are iteratively integrated into a shared memory\nstore to facilitate progressive context refinement. To better evaluate model\nbehavior, we further propose an Answer Quality Score metric that captures\nLLM-generated explanations beyond standard QA accuracy. Experiments on three\nmulti-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate\nthat RCR-Router reduces token usage (up to 30%) while improving or maintaining\nanswer quality. These results highlight the importance of structured memory\nrouting and output-aware evaluation in advancing scalable multi-agent LLM\nsystems.", "AI": {"tldr": "本文提出了RCR-Router，一种新的多代理大语言模型协作框架，它可以动态选择相关的记忆子集，以减少令牌消耗和提高问答质量。实验表明，该框架能够减少令牌的使用量并保持或提高问答的质量。", "motivation": "现有大多数多代理大语言模型系统依赖静态或全上下文路由策略，这导致过多的令牌消耗、冗余的记忆暴露以及有限的适应性。研究动机在于通过提出一种更有效的路由方法来克服这些问题。", "method": "本研究提出了一种名为RCR-Router的模块化和基于角色的上下文路由框架，用于促进多代理大语言模型中的高效适应性协作。该框架可以根据代理的角色和任务阶段动态选择语义相关的记忆子集，同时还遵守严格的令牌预算。通过轻量级的评分策略指导记忆选择，并逐步将代理的输出集成到共享的记忆存储中，以促进渐进式的上下文细化。", "result": "该研究通过使用三个多跳问答基准测试（HotPotQA，MuSiQue 和 2WikiMultihop）证明了RCR-Router能够降低高达30%的令牌使用量，同时保持或提高问答质量。", "conclusion": "研究结果强调了在大规模多代理语言模型系统发展中的结构化记忆路由和输出感知评估的重要性。"}}
{"id": "2508.04801", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04801", "abs": "https://arxiv.org/abs/2508.04801", "authors": ["Trong-Thuan Nguyen", "Viet-Tham Huynh", "Thao Thi Phuong Dao", "Ha Nguyen Thi", "Tien To Vu Thuy", "Uyen Hanh Tran", "Tam V. Nguyen", "Thanh Dinh Le", "Minh-Triet Tran"], "title": "ACM Multimedia Grand Challenge on ENT Endoscopy Analysis", "comment": null, "summary": "Automated analysis of endoscopic imagery is a critical yet underdeveloped\ncomponent of ENT (ear, nose, and throat) care, hindered by variability in\ndevices and operators, subtle and localized findings, and fine-grained\ndistinctions such as laterality and vocal-fold state. In addition to\nclassification, clinicians require reliable retrieval of similar cases, both\nvisually and through concise textual descriptions. These capabilities are\nrarely supported by existing public benchmarks. To this end, we introduce\nENTRep, the ACM Multimedia 2025 Grand Challenge on ENT endoscopy analysis,\nwhich integrates fine-grained anatomical classification with image-to-image and\ntext-to-image retrieval under bilingual (Vietnamese and English) clinical\nsupervision. Specifically, the dataset comprises expert-annotated images,\nlabeled for anatomical region and normal or abnormal status, and accompanied by\ndual-language narrative descriptions. In addition, we define three benchmark\ntasks, standardize the submission protocol, and evaluate performance on public\nand private test splits using server-side scoring. Moreover, we report results\nfrom the top-performing teams and provide an insight discussion.", "AI": {"tldr": "本文提出了ENTRep挑战，针对耳鼻喉内窥镜图像分析的自动化问题，提供了精细的解剖分类和多语言支持的检索能力。", "motivation": "该研究的动机在于解决耳鼻喉内窥镜图像分析中的自动化分析问题，这个问题因为设备和操作者之间的可变性、症状的微妙性和局域性、解剖细节差异性等问题而进展缓慢。现有的公开基准很少支持这些功能。", "method": "本文提出了ENTRep，这是ACM Multimedia 2025 耳鼻喉内窥镜分析大赛的挑战。该挑战集成了精细的解剖分类，以及在双语（越南语和英语）临床监督下的图像对图像和文本对图像检索能力。", "result": "作者定义了三项基准任务，标准化提交流程，并结合公共和私有测试集上的服务器端评分来评估性能。此外，本文报告了顶尖团队的结果，并提供了深度讨论。", "conclusion": "综上所述，本文提出的ENTRep挑战首次涵盖了耳鼻喉内窥镜图像解剖学分类和双语临床监督下的图像和文本检索功能，推动了解析和检索工具的发展，以适应临床需求。"}}
{"id": "2508.04939", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.04939", "abs": "https://arxiv.org/abs/2508.04939", "authors": ["Julia Kharchenko", "Tanya Roosta", "Aman Chadha", "Chirag Shah"], "title": "I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations", "comment": null, "summary": "This paper introduces a comprehensive benchmark for evaluating how Large\nLanguage Models (LLMs) respond to linguistic shibboleths: subtle linguistic\nmarkers that can inadvertently reveal demographic attributes such as gender,\nsocial class, or regional background. Through carefully constructed interview\nsimulations using 100 validated question-response pairs, we demonstrate how\nLLMs systematically penalize certain linguistic patterns, particularly hedging\nlanguage, despite equivalent content quality. Our benchmark generates\ncontrolled linguistic variations that isolate specific phenomena while\nmaintaining semantic equivalence, which enables the precise measurement of\ndemographic bias in automated evaluation systems. We validate our approach\nalong multiple linguistic dimensions, showing that hedged responses receive\n25.6% lower ratings on average, and demonstrate the benchmark's effectiveness\nin identifying model-specific biases. This work establishes a foundational\nframework for detecting and measuring linguistic discrimination in AI systems,\nwith broad applications to fairness in automated decision-making contexts.", "AI": {"tldr": "论文引入了一种全面的基准测试，用于评估大型语言模型（LLMs）对语言线索的敏感度以及由此产生的潜在歧视性偏差。", "motivation": "介绍了一种全面的基准测试，用于评估大型语言模型对语言特征的反应，这些特征可以无意中揭示性别、社会阶层等人口统计学属性。", "method": "通过精心设计的面试模拟，使用100个验证过的问答对，来展示LLMs如何系统性地惩罚某些语言模式，特别是犹豫性的语言，尽管这些语言的内容质量相当。", "result": "基准测试表明，犹豫性的回答平均得分低25.6%，证明了该基准测试在识别模型特定偏见方面的有效性。", "conclusion": "这项工作建立了一个基础框架，用于检测和衡量AI系统中的语言歧视，对于自动决策中的公平性有广泛的应用。"}}
{"id": "2508.04816", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04816", "abs": "https://arxiv.org/abs/2508.04816", "authors": ["Sriram Mandalika", "Lalitha V"], "title": "CoMAD: A Multiple-Teacher Self-Supervised Distillation Framework", "comment": "8 Pages, 2 Figures", "summary": "Numerous self-supervised learning paradigms, such as contrastive learning and\nmasked image modeling, learn powerful representations from unlabeled data but\nare typically pretrained in isolation, overlooking complementary insights and\nyielding large models that are impractical for resource-constrained deployment.\nTo overcome these challenges, we introduce Consensus-oriented Masked\nDistillation (CoMAD), a lightweight, parameter-free framework that unifies\nknowledge from multiple current state-of-the-art self-supervised Vision\nTransformers into a compact student network. CoMAD distills from three\npretrained ViT-Base teachers, MAE, MoCo v3, and iBOT, each offering distinct\nsemantic and contextual priors. Rather than naively averaging teacher outputs,\nwe apply asymmetric masking: the student sees only 25 percent of patches while\neach teacher receives a progressively lighter, unique mask, forcing the student\nto interpolate missing features under richer contexts. Teacher embeddings are\naligned to the student's space via a linear adapter and layer normalization,\nthen fused through our joint consensus gating, which weights each token by\ncombining cosine affinity with inter-teacher agreement. The student is trained\nwith dual-level KL divergence on visible tokens and reconstructed feature maps,\ncapturing both local and global structure. On ImageNet-1K, CoMAD's ViT-Tiny\nachieves 75.4 percent Top-1, an increment of 0.4 percent over the previous\nstate-of-the-art. In dense-prediction transfers, it attains 47.3 percent mIoU\non ADE20K, and 44.5 percent box average precision and 40.5 percent mask average\nprecision on MS-COCO, establishing a new state-of-the-art in compact SSL\ndistillation.", "AI": {"tldr": "介绍了共识导向掩码蒸馏(CoMAD)框架，该框架将多个最先进的自监督视觉Transformer（ViT-Base）的知识蒸馏到一个紧凑的学生网络中，以解决独立预训练带来的限制并实现更高效的资源使用。", "motivation": "克服现有的自监督学习范式如对比学习和掩码图像建模的局限性，这些范式通常独立预训练，忽视了互补性的见解，且生成的模型过于庞大，不易适用于资源受限的部署环境。", "method": "引入了一种名为共识导向掩码蒸馏（CoMAD）的轻量、无参数框架，该框架统一了多个当前最先进的自监督视觉Transformer（ViT-Base）教师模型的知识，并将其蒸馏到一个小巧的学生网络中。CoMAD从三个预训练的教师模型：MAE、MoCo v3 和 iBOT中进行蒸馏，这三个模型分别提供了不同的语义和上下文先验。它通过非对称掩码，即学生仅看到25%的图像块而每个教师接受一个逐渐减轻的独特掩码，使学生在网络丰富的上下文中完成缺失特征。教师模型的嵌入通过线性适配器和层归一化对齐到学生模型的空间，并通过联合共识门控融合，联合共识门控根据教师模型之间的余弦相似性和一致性加权每个token。学生模型通过对可见token和重构特征图的双层次KL散度进行训练，捕捉局部和全局结构。", "result": "在ImageNet-1K上的实验表明，CoMAD的学生模型ViT-Tiny达到了75.4%的Top-1准确率，比之前的最先进方法提升了0.4%。在密集预测转移任务中，它在ADE20K上达到了47.3%的mIoU，在MS-COCO上分别取得了44.5%的box平均精度和40.5%的mask平均精度，确立了在紧凑型SSL蒸馏领域的最先进地位。", "conclusion": "CoMAD框架通过一个创新的知识整合方法提高了小模型的性能，展示了在自监督学习和模型蒸馏领域的潜力，特别是对于那些资源受限的场景。该方法在多个主流视觉任务上的表现展示了它在实际应用场景中的价值。"}}
{"id": "2508.04945", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04945", "abs": "https://arxiv.org/abs/2508.04945", "authors": ["Louie Hong Yao", "Nicholas Jarvis", "Tianyu Jiang"], "title": "Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering", "comment": "18 pages, 5 figures", "summary": "Evaluating visual activity recognition systems is challenging due to inherent\nambiguities in verb semantics and image interpretation. When describing actions\nin images, synonymous verbs can refer to the same event (e.g., brushing vs.\ngrooming), while different perspectives can lead to equally valid but distinct\nverb choices (e.g., piloting vs. operating). Standard exact-match evaluation,\nwhich relies on a single gold answer, fails to capture these ambiguities,\nresulting in an incomplete assessment of model performance. To address this, we\npropose a vision-language clustering framework that constructs verb sense\nclusters, providing a more robust evaluation. Our analysis of the imSitu\ndataset shows that each image maps to an average of 2.8 sense clusters, with\neach cluster representing a distinct perspective of the image. We evaluate\nmultiple activity recognition models and compare our cluster-based evaluation\nwith standard evaluation methods. Additionally, our human alignment analysis\nsuggests that the cluster-based evaluation better aligns with human judgements,\noffering a more nuanced assessment of model performance.", "AI": {"tldr": "本文提出了一种视觉-语言聚类框架用于动词意义聚类，以更稳健地评估视觉活动识别系统的性能。", "motivation": "标准的精确匹配评估仅依赖于单一的正确答案，无法捕捉动词语义和图像解释中存在的歧义，导致对模型性能的评估不完整。为了应对这一挑战，本文提出了一种新的评估方法。", "method": "提出了一种视觉-语言聚类框架，用于构建动词意义聚类，以提供更为稳健的评估方法。", "result": "分析imSitu数据集得出每个图像平均映射到2.8个意义聚类，每个聚类代表图像的一种不同视角。人类一致性分析表明，基于聚类的评估更好地与人类判断相吻合，提供更细致的模型性能评估。", "conclusion": "实验结果表明，基于聚类的评估方法与人类判断更加一致，能更好地评估活动识别模型的性能。"}}
{"id": "2508.04818", "categories": ["cs.CV", "eess.IV", "stat.ML", "62H35, 68T07, 62M40, 68T45", "I.2.6; I.2.10; I.4.6; I.4.8; I.5.1; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.04818", "abs": "https://arxiv.org/abs/2508.04818", "authors": ["Mehrdad Moradi", "Marco Grasso", "Bianca Maria Colosimo", "Kamran Paynabar"], "title": "Single-Step Reconstruction-Free Anomaly Detection and Segmentation via Diffusion Models", "comment": "9 pages, 8 figures, 2 tables. Submitted to an IEEE conference", "summary": "Generative models have demonstrated significant success in anomaly detection\nand segmentation over the past decade. Recently, diffusion models have emerged\nas a powerful alternative, outperforming previous approaches such as GANs and\nVAEs. In typical diffusion-based anomaly detection, a model is trained on\nnormal data, and during inference, anomalous images are perturbed to a\npredefined intermediate step in the forward diffusion process. The\ncorresponding normal image is then reconstructed through iterative reverse\nsampling.\n  However, reconstruction-based approaches present three major challenges: (1)\nthe reconstruction process is computationally expensive due to multiple\nsampling steps, making real-time applications impractical; (2) for complex or\nsubtle patterns, the reconstructed image may correspond to a different normal\npattern rather than the original input; and (3) Choosing an appropriate\nintermediate noise level is challenging because it is application-dependent and\noften assumes prior knowledge of anomalies, an assumption that does not hold in\nunsupervised settings.\n  We introduce Reconstruction-free Anomaly Detection with Attention-based\ndiffusion models in Real-time (RADAR), which overcomes the limitations of\nreconstruction-based anomaly detection. Unlike current SOTA methods that\nreconstruct the input image, RADAR directly produces anomaly maps from the\ndiffusion model, improving both detection accuracy and computational\nefficiency. We evaluate RADAR on real-world 3D-printed material and the\nMVTec-AD dataset. Our approach surpasses state-of-the-art diffusion-based and\nstatistical machine learning models across all key metrics, including accuracy,\nprecision, recall, and F1 score. Specifically, RADAR improves F1 score by 7% on\nMVTec-AD and 13% on the 3D-printed material dataset compared to the next best\nmodel.\n  Code available at: https://github.com/mehrdadmoradi124/RADAR", "AI": {"tldr": "该研究提出了RADAR方法，通过基于注意力机制的扩散模型解决了现有异常检测中的几个主要挑战，能够实时、更准确地检测出异常。", "motivation": "由于现有基于重构的异常检测方法存在计算成本高、容易误判以及选择合适噪声水平困难的问题，研究者们提出了一种新的、无重构的异常检测方法，以解决以上问题并进一步提升检测性能。", "method": "RADAR方法利用基于注意力机制的扩散模型直接生成异常图，从而避免了现有重构方法中多次采样带来的计算复杂度，并且能够直接从扩散模型中获得异常映射，提高了检测准确性和计算效率。", "result": "该研究引入了一种名为RADAR的方法，通过基于注意力机制的扩散模型实现了无重构实时异常检测，解决了现有重构方法计算复杂、可能出现误判及选择合适的噪声水平困难的问题。RADAR在多个评估指标上超越了现有的扩散模型和统计机器学习方法，特别是在MVTec-AD数据集上F1分数提升了7%，在3D打印材料数据集上提升了13%。", "conclusion": "RADAR方法在MVTec-AD和3D打印材料数据集中超越了现有的扩散模型和统计机器学习方法，证明了其优越的异常检测能力和计算效率。"}}
{"id": "2508.05003", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05003", "abs": "https://arxiv.org/abs/2508.05003", "authors": ["Song Wang", "Yishu Wei", "Haotian Ma", "Max Lovitt", "Kelly Deng", "Yuan Meng", "Zihan Xu", "Jingze Zhang", "Yunyu Xiao", "Ying Ding", "Xuhai Xu", "Joydeep Ghosh", "Yifan Peng"], "title": "A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health", "comment": null, "summary": "Background: Understanding social determinants of health (SDoH) factors\ncontributing to suicide incidents is crucial for early intervention and\nprevention. However, data-driven approaches to this goal face challenges such\nas long-tailed factor distributions, analyzing pivotal stressors preceding\nsuicide incidents, and limited model explainability. Methods: We present a\nmulti-stage large language model framework to enhance SDoH factor extraction\nfrom unstructured text. Our approach was compared to other state-of-the-art\nlanguage models (i.e., pre-trained BioBERT and GPT-3.5-turbo) and reasoning\nmodels (i.e., DeepSeek-R1). We also evaluated how the model's explanations help\npeople annotate SDoH factors more quickly and accurately. The analysis included\nboth automated comparisons and a pilot user study. Results: We show that our\nproposed framework demonstrated performance boosts in the overarching task of\nextracting SDoH factors and in the finer-grained tasks of retrieving relevant\ncontext. Additionally, we show that fine-tuning a smaller, task-specific model\nachieves comparable or better performance with reduced inference costs. The\nmulti-stage design not only enhances extraction but also provides intermediate\nexplanations, improving model explainability. Conclusions: Our approach\nimproves both the accuracy and transparency of extracting suicide-related SDoH\nfrom unstructured texts. These advancements have the potential to support early\nidentification of individuals at risk and inform more effective prevention\nstrategies.", "AI": {"tldr": "本研究提出了一种多阶段的大型语言模型框架，用于从非结构化文本中提取社会健康决定因素（SDoH）。它在提取SDoH因素方面表现优越，并提高了模型的透明度。", "motivation": "理解影响自杀事件的社会健康决定因素（SDoH）对于早期干预和预防至关重要。然而，以数据驱动的方法实现这一目标面临一些挑战，如存在长尾的因子分布、分析在自杀事件之前的决定性压力，以及模型解释能力有限。", "method": "我们提出了一种多阶段的大型语言模型框架，用于从非结构化文本中增强社会健康决定因素（SDoH）的提取。我们的方法与其他最先进的语言模型（如预训练的BioBERT和GPT-3.5-turbo）以及推理模型（如DeepSeek-R1）进行了比较。我们还评估了模型的解释如何帮助人们更快更准确地标注SDoH因素。分析包括自动比较和初步用户研究。", "result": "结果表明，我们提出的框架在提取SDoH因素的总体任务以及提取相关上下文的细粒度任务中显示出了性能提升。此外，我们证明了对较小的任务特定模型进行微调可以实现相当甚至更好的性能，同时减少了推理成本。多阶段设计不仅提高了提取能力，而且提供了中间解释，增强了模型的解释性。", "conclusion": "我们的方法可以提高从非结构化文本中提取与自杀相关SDoH因素的准确性和透明度。这些进步有助于提前识别有风险的个体及制定更有效的预防策略。"}}
{"id": "2508.04827", "categories": ["cs.CV", "68T05, 68T07", "I.2.10; I.5.1; I.4.8; J.4"], "pdf": "https://arxiv.org/pdf/2508.04827", "abs": "https://arxiv.org/abs/2508.04827", "authors": ["Chirag Seth", "Divya Naiken", "Keyan Lin"], "title": "A deep learning approach to track eye movements based on events", "comment": null, "summary": "This research project addresses the challenge of accurately tracking eye\nmovements during specific events by leveraging previous research. Given the\nrapid movements of human eyes, which can reach speeds of 300{\\deg}/s, precise\neye tracking typically requires expensive and high-speed cameras. Our primary\nobjective is to locate the eye center position (x, y) using inputs from an\nevent camera. Eye movement analysis has extensive applications in consumer\nelectronics, especially in VR and AR product development. Therefore, our\nultimate goal is to develop an interpretable and cost-effective algorithm using\ndeep learning methods to predict human attention, thereby improving device\ncomfort and enhancing overall user experience. To achieve this goal, we\nexplored various approaches, with the CNN\\_LSTM model proving most effective,\nachieving approximately 81\\% accuracy. Additionally, we propose future work\nfocusing on Layer-wise Relevance Propagation (LRP) to further enhance the\nmodel's interpretability and predictive performance.", "AI": {"tldr": "研究通过深度学习方法，尤其是CNN_LSTM模型，开发了一个成本效益高的算法来预测人类注意力，实现81%的准确率，并提出使用LRP来提升模型的解释性。", "motivation": "精确的眼球追踪通常需要昂贵和高速的摄像头，因此研究的目的是开发一个可解释且低成本的算法，利用事件相机的输入来定位眼睛中心位置(x, y)。", "method": "使用深度学习方法，特别是CNN_LSTM模型，来预测人类注意力，从而改善设备舒适度和用户体验。", "result": "提出的CNN_LSTM模型取得了约81%的准确率。", "conclusion": "研究提出了未来工作将集中在通过图层相关传播(LRP)来进一步提升模型的可解释性和预测性能。"}}
{"id": "2508.05023", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05023", "abs": "https://arxiv.org/abs/2508.05023", "authors": ["Kun Peng", "Cong Cao", "Hao Peng", "Zhifeng Hao", "Lei Jiang", "Kongjing Gu", "Yanbing Liu", "Philip S. Yu"], "title": "Dialogues Aspect-based Sentiment Quadruple Extraction via Structural Entropy Minimization Partitioning", "comment": "Accepted by CIKM2025", "summary": "Dialogues Aspect-based Sentiment Quadruple Extraction (DiaASQ) aims to\nextract all target-aspect-opinion-sentiment quadruples from a given\nmulti-round, multi-participant dialogue. Existing methods typically learn word\nrelations across entire dialogues, assuming a uniform distribution of sentiment\nelements. However, we find that dialogues often contain multiple semantically\nindependent sub-dialogues without clear dependencies between them. Therefore,\nlearning word relationships across the entire dialogue inevitably introduces\nadditional noise into the extraction process. To address this, our method\nfocuses on partitioning dialogues into semantically independent sub-dialogues.\nAchieving completeness while minimizing these sub-dialogues presents a\nsignificant challenge. Simply partitioning based on reply relationships is\nineffective. Instead, we propose utilizing a structural entropy minimization\nalgorithm to partition the dialogues. This approach aims to preserve relevant\nutterances while distinguishing irrelevant ones as much as possible.\nFurthermore, we introduce a two-step framework for quadruple extraction: first\nextracting individual sentiment elements at the utterance level, then matching\nquadruples at the sub-dialogue level. Extensive experiments demonstrate that\nour approach achieves state-of-the-art performance in DiaASQ with much lower\ncomputational costs.", "AI": {"tldr": "The paper targets the extraction of sentiment quadruples from dialogues by partitioning dialogues into semantically independent parts, using a structural entropy minimization algorithm and a two-step extraction framework, achieving state-of-the-art performance with lower computation.", "motivation": "The motivation behind this paper is to address the existing methods' limitation of learning word relations across entire dialogues, which introduces additional noise due to the presence of semantically independent sub-dialogues.", "method": "Our method focuses on partitioning dialogues into semantically independent sub-dialogues using a structural entropy minimization algorithm. Then, a two-step framework for quadruple extraction is introduced: first extracting individual sentiment elements at the utterance level, then matching quadruples at the sub-dialogue level.", "result": "Experiments show that their approach achieves state-of-the-art performance in DiaASQ with much lower computational costs.", "conclusion": "The paper concludes that by partitioning dialogues into semantically independent sub-dialogues and using a two-step framework for quadruple extraction, they are able to achieve superior performance and lower computational costs in the DiaASQ task."}}
{"id": "2508.04847", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04847", "abs": "https://arxiv.org/abs/2508.04847", "authors": ["Md Zahidul Hasan", "A. Ben Hamza", "Nizar Bouguila"], "title": "LuKAN: A Kolmogorov-Arnold Network Framework for 3D Human Motion Prediction", "comment": null, "summary": "The goal of 3D human motion prediction is to forecast future 3D poses of the\nhuman body based on historical motion data. Existing methods often face\nlimitations in achieving a balance between prediction accuracy and\ncomputational efficiency. In this paper, we present LuKAN, an effective model\nbased on Kolmogorov-Arnold Networks (KANs) with Lucas polynomial activations.\nOur model first applies the discrete wavelet transform to encode temporal\ninformation in the input motion sequence. Then, a spatial projection layer is\nused to capture inter-joint dependencies, ensuring structural consistency of\nthe human body. At the core of LuKAN is the Temporal Dependency Learner, which\nemploys a KAN layer parameterized by Lucas polynomials for efficient function\napproximation. These polynomials provide computational efficiency and an\nenhanced capability to handle oscillatory behaviors. Finally, the inverse\ndiscrete wavelet transform reconstructs motion sequences in the time domain,\ngenerating temporally coherent predictions. Extensive experiments on three\nbenchmark datasets demonstrate the competitive performance of our model\ncompared to strong baselines, as evidenced by both quantitative and qualitative\nevaluations. Moreover, its compact architecture coupled with the linear\nrecurrence of Lucas polynomials, ensures computational efficiency.", "AI": {"tldr": "LuKAN is an efficient model for 3D human motion prediction using a combination of wavelet transform, KANs with Lucas polynomials, and spatial projections, achieving competitive accuracy while maintaining computational efficiency.", "motivation": "The motivation of this paper is to address the limitations of existing 3D human motion prediction models, which struggle to balance between prediction accuracy and computational efficiency.", "method": "Our model, LuKAN, consists of several key components: discrete wavelet transform, spatial projection layer, Temporal Dependency Learner with KAN layers using Lucas polynomials, and inverse discrete wavelet transform to generate coherent motion predictions.", "result": "Experiments on three benchmark datasets show that LuKAN outperforms or matches the performance of strong baselines with both higher accuracy and lower computational cost.", "conclusion": "The paper concludes that LuKAN achieves a balance between prediction accuracy and computational efficiency, demonstrating competitive performance through extensive benchmark testing."}}
{"id": "2508.05028", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05028", "abs": "https://arxiv.org/abs/2508.05028", "authors": ["Shu Han Ho"], "title": "Evaluation of LLMs in AMR Parsing", "comment": "27 pages, 32 figures", "summary": "Meaning Representation (AMR) is a semantic formalism that encodes sentence\nmeaning as rooted, directed, acyclic graphs, where nodes represent concepts and\nedges denote semantic relations. Finetuning decoder only Large Language Models\n(LLMs) represent a promising novel straightfoward direction for AMR parsing.\nThis paper presents a comprehensive evaluation of finetuning four distinct LLM\narchitectures, Phi 3.5, Gemma 2, LLaMA 3.2, and DeepSeek R1 LLaMA Distilled\nusing the LDC2020T02 Gold AMR3.0 test set. Our results have shown that\nstraightfoward finetuning of decoder only LLMs can achieve comparable\nperformance to complex State of the Art (SOTA) AMR parsers. Notably, LLaMA 3.2\ndemonstrates competitive performance against SOTA AMR parsers given a\nstraightforward finetuning approach. We achieved SMATCH F1: 0.804 on the full\nLDC2020T02 test split, on par with APT + Silver (IBM) at 0.804 and approaching\nGraphene Smatch (MBSE) at 0.854. Across our analysis, we also observed a\nconsistent pattern where LLaMA 3.2 leads in semantic performance while Phi 3.5\nexcels in structural validity.", "AI": {"tldr": "该论文评估了通过简单微调仅解码器的大型语言模型(LLMs)进行AMR解析的性能，显示了LLaMA 3.2的表现与最新技术(SOTA)AMR解析器相当，达到SMATCH F1：0.804。", "motivation": "研究的动机是评估简单微调解码器大型语言模型在AMR解析任务上的有效性，探索这一方法是否可以作为一种简单但强大的解决方案，替代现有的复杂方法。", "method": "论文中研究者微调了四个不同的大型语言模型架构（Phi 3.5、Gemma 2、LLaMA 3.2、DeepSeek R1 LLaMA Distilled），并在LDC2020T02 Gold AMR3.0测试集上进行评估。", "result": "研究结果表明，通过对仅解码器的大型语言模型进行简单微调可以达到与复杂状态的艺术(AMR)解析器相当的表现。LLaMA 3.2表现出了与领先技术AMR解析器相近的性能。", "conclusion": "LLaMA 3.2在语义性能方面领先，而Phi 3.5则在结构有效性方面表现更好。这表明简单微调方法可以在AMR解析任务上达到和复杂方法相似的性能水平。"}}
{"id": "2508.04852", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04852", "abs": "https://arxiv.org/abs/2508.04852", "authors": ["Chenhui Qiang", "Zhaoyang Wei", "Xumeng Han Zipeng Wang", "Siyao Li", "Xiangyuan Lan", "Jianbin Jiao", "Zhenjun Han"], "title": "VER-Bench: Evaluating MLLMs on Reasoning with Fine-Grained Visual Evidence", "comment": "Accept by ACMM2025 Dataset track", "summary": "With the rapid development of MLLMs, evaluating their visual capabilities has\nbecome increasingly crucial. Current benchmarks primarily fall into two main\ntypes: basic perception benchmarks, which focus on local details but lack deep\nreasoning (e.g., \"what is in the image?\"), and mainstream reasoning benchmarks,\nwhich concentrate on prominent image elements but may fail to assess subtle\nclues requiring intricate analysis. However, profound visual understanding and\ncomplex reasoning depend more on interpreting subtle, inconspicuous local\ndetails than on perceiving salient, macro-level objects. These details, though\noccupying minimal image area, often contain richer, more critical information\nfor robust analysis. To bridge this gap, we introduce the VER-Bench, a novel\nframework to evaluate MLLMs' ability to: 1) identify fine-grained visual clues,\noften occupying on average just 0.25% of the image area; 2) integrate these\nclues with world knowledge for complex reasoning. Comprising 374 carefully\ndesigned questions across Geospatial, Temporal, Situational, Intent, System\nState, and Symbolic reasoning, each question in VER-Bench is accompanied by\nstructured evidence: visual clues and question-related reasoning derived from\nthem. VER-Bench reveals current models' limitations in extracting subtle visual\nevidence and constructing evidence-based arguments, highlighting the need to\nenhance models's capabilities in fine-grained visual evidence extraction,\nintegration, and reasoning for genuine visual understanding and human-like\nanalysis. Dataset and additional materials are available\nhttps://github.com/verbta/ACMMM-25-Materials.", "AI": {"tldr": "VER-Bench是一个评估MLLMs在复杂视觉任务上的能力的新框架，尤其关注于模型能否识别细微线索并进行复杂推理，超越了现有评估标准的能力。", "motivation": "当前的评估基准主要集中在局部细节或显著图像元素上，而忽略了细微线索的深度推理。这样的评估无法全面衡量模型的视觉理解能力。为了弥补这一不足并评估模型在细粒度视觉证据提取、整合和推理方面的性能，提出了VER-Bench。", "method": "介绍了一个新的评估框架VER-Bench，用于评估MLLMs在识别细微视觉线索和结合世界知识进行复杂推理的能力。该框架包含374个精心设计的问题，涵盖地理空间、时间、情景、意图、系统状态和符号推理等方面，每个问题都配有结构化的证据：视觉线索和与其相关的推理。", "result": "VER-Bench揭示了现有模型在提取细微视觉证据和构建基于证据的论据方面的局限性。", "conclusion": "为了实现真实的视觉理解和类似人类的分析，VER-Bench强调了提升模型在细粒度视觉证据提取、整合和推理方面能力的必要性。"}}
{"id": "2508.05078", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05078", "abs": "https://arxiv.org/abs/2508.05078", "authors": ["Jinda Liu", "Bo Cheng", "Yi Chang", "Yuan Wu"], "title": "Align, Don't Divide: Revisiting the LoRA Architecture in Multi-Task Learning", "comment": null, "summary": "Parameter-Efficient Fine-Tuning (PEFT) is essential for adapting Large\nLanguage Models (LLMs). In practice, LLMs are often required to handle a\ndiverse set of tasks from multiple domains, a scenario naturally addressed by\nmulti-task learning (MTL). Within this MTL context, a prevailing trend involves\nLoRA variants with multiple adapters or heads, which advocate for structural\ndiversity to capture task-specific knowledge. Our findings present a direct\nchallenge to this paradigm. We first show that a simplified multi-head\narchitecture with high inter-head similarity substantially outperforms complex\nmulti-adapter and multi-head systems. This leads us to question the\nmulti-component paradigm itself, and we further demonstrate that a standard\nsingle-adapter LoRA, with a sufficiently increased rank, also achieves highly\ncompetitive performance. These results lead us to a new hypothesis: effective\nMTL generalization hinges on learning robust shared representations, not\nisolating task-specific features. To validate this, we propose Align-LoRA,\nwhich incorporates an explicit loss to align task representations within the\nshared adapter space. Experiments confirm that Align-LoRA significantly\nsurpasses all baselines, establishing a simpler yet more effective paradigm for\nadapting LLMs to multiple tasks. The code is available at\nhttps://github.com/jinda-liu/Align-LoRA.", "AI": {"tldr": "研究人员挑战了现有通过多适配器或多头结构来完成多任务学习的有效性，并提出了Align-LoRA方法，通过行列对齐来共享表示，显示出更强的性能。", "motivation": "该研究挑战了当前主流的多组件参数高效微调（PEFT）方法，该方法倾向于使用多适配器或多头结构来处理来自多个领域的多样化任务。作者希望通过简化模型架构和提高单适配器的秩来提高性能。", "method": "论文提出了一个新的假设，即有效的多任务学习泛化依赖于学习坚实的共享表示，而不只是隔离任务特定特征。为此，作者提出了一种新的方法：Align-LoRA，它通过在共享适配器空间内引入显式的对齐损失来统一任务表示。", "result": "研究发现，简化后的多头架构以及增加秩的单适配器方法在多任务学习中表现良好，并提出了Align-LoRA模型，展示了在多任务适应方面的强大性能。", "conclusion": "实验结果表明，不管是简化后的多头架构，还是增加秩的单适配器方式，都能实现较强的性能。Align-LoRA能够在共享适配器空间内使任务表示对齐，从而显著超越基线模型，提供了一个更简单而有效的多任务适应大语言模型的方法。"}}
{"id": "2508.04868", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04868", "abs": "https://arxiv.org/abs/2508.04868", "authors": ["Noreen Anwar", "Guillaume-Alexandre Bilodeau", "Wassim Bouachir"], "title": "Dual-Stream Attention with Multi-Modal Queries for Object Detection in Transportation Applications", "comment": "10 pages", "summary": "Transformer-based object detectors often struggle with occlusions,\nfine-grained localization, and computational inefficiency caused by fixed\nqueries and dense attention. We propose DAMM, Dual-stream Attention with\nMulti-Modal queries, a novel framework introducing both query adaptation and\nstructured cross-attention for improved accuracy and efficiency. DAMM\ncapitalizes on three types of queries: appearance-based queries from\nvision-language models, positional queries using polygonal embeddings, and\nrandom learned queries for general scene coverage. Furthermore, a dual-stream\ncross-attention module separately refines semantic and spatial features,\nboosting localization precision in cluttered scenes. We evaluated DAMM on four\nchallenging benchmarks, and it achieved state-of-the-art performance in average\nprecision (AP) and recall, demonstrating the effectiveness of multi-modal query\nadaptation and dual-stream attention. Source code is at:\n\\href{https://github.com/DET-LIP/DAMM}{GitHub}.", "AI": {"tldr": "DAMM 提出了一种新型框架，引入查询适应性和结构化交叉注意力机制，提升了遮挡处理、细粒度定位能力和计算效率。", "motivation": "Transformer 基础的对象检测器通常难以应对遮挡、细粒度定位和由固定查询和密集注意力引起的计算低效问题。", "method": "DAMM, Dual-stream Attention with Multi-Modal queries, 首次引入查询适应性和结构化交叉注意力机制，改进准确性和效率。DAMM 使用三种查询：基于视觉语言模型的外观查询，使用多边形嵌入的位置查询，以及用于一般场景覆盖的随机学习查询。此外，双流交叉注意力模块分别优化语义和空间特征，增强在杂乱场景中的定位精度。", "result": "在四个具有挑战性的基准数据集上进行评估，DAMM 达到了最先进的平均精度（AP）和召回率，证明了多模态查询适应性和双流注意力的有效性。", "conclusion": "实验结果表明，DAMM 通过多模态查询适应性和双流注意力机制，在平均精度和召回率方面达到了最先进的性能。"}}
{"id": "2508.05097", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05097", "abs": "https://arxiv.org/abs/2508.05097", "authors": ["Aditya Kishore", "Gaurav Kumar", "Jasabanta Patro"], "title": "Multimodal Fact Checking with Unified Visual, Textual, and Contextual Representations", "comment": null, "summary": "The growing rate of multimodal misinformation, where claims are supported by\nboth text and images, poses significant challenges to fact-checking systems\nthat rely primarily on textual evidence. In this work, we have proposed a\nunified framework for fine-grained multimodal fact verification called\n\"MultiCheck\", designed to reason over structured textual and visual signals.\nOur architecture combines dedicated encoders for text and images with a fusion\nmodule that captures cross-modal relationships using element-wise interactions.\nA classification head then predicts the veracity of a claim, supported by a\ncontrastive learning objective that encourages semantic alignment between\nclaim-evidence pairs in a shared latent space. We evaluate our approach on the\nFactify 2 dataset, achieving a weighted F1 score of 0.84, substantially\noutperforming the baseline. These results highlight the effectiveness of\nexplicit multimodal reasoning and demonstrate the potential of our approach for\nscalable and interpretable fact-checking in complex, real-world scenarios.", "AI": {"tldr": "A unified framework for fine-grained multimodal fact verification is proposed, achieving a weighted F1 score of 0.84 on the Factify 2 dataset.", "motivation": "The growing rate of multimodal misinformation, supported by both text and images, poses significant challenges to fact-checking systems that rely primarily on textual evidence.", "method": "Our architecture combines dedicated encoders for text and images with a fusion module that captures cross-modal relationships using element-wise interactions. A classification head then predicts the veracity of a claim, supported by a contrastive learning objective.", "result": "Achieving a weighted F1 score of 0.84, substantially outperforming the baseline.", "conclusion": "These results highlight the effectiveness of explicit multimodal reasoning and demonstrate the potential of our approach for scalable and interpretable fact-checking in complex, real-world scenarios."}}
{"id": "2508.04900", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04900", "abs": "https://arxiv.org/abs/2508.04900", "authors": ["Shuonan Yang", "Tailin Chen", "Rahul Singh", "Jiangbei Yue", "Jianbo Jiao", "Zeyu Fu"], "title": "Revealing Temporal Label Noise in Multimodal Hateful Video Classification", "comment": null, "summary": "The rapid proliferation of online multimedia content has intensified the\nspread of hate speech, presenting critical societal and regulatory challenges.\nWhile recent work has advanced multimodal hateful video detection, most\napproaches rely on coarse, video-level annotations that overlook the temporal\ngranularity of hateful content. This introduces substantial label noise, as\nvideos annotated as hateful often contain long non-hateful segments. In this\npaper, we investigate the impact of such label ambiguity through a fine-grained\napproach. Specifically, we trim hateful videos from the HateMM and\nMultiHateClip English datasets using annotated timestamps to isolate explicitly\nhateful segments. We then conduct an exploratory analysis of these trimmed\nsegments to examine the distribution and characteristics of both hateful and\nnon-hateful content. This analysis highlights the degree of semantic overlap\nand the confusion introduced by coarse, video-level annotations. Finally,\ncontrolled experiments demonstrated that time-stamp noise fundamentally alters\nmodel decision boundaries and weakens classification confidence, highlighting\nthe inherent context dependency and temporal continuity of hate speech\nexpression. Our findings provide new insights into the temporal dynamics of\nmultimodal hateful videos and highlight the need for temporally aware models\nand benchmarks for improved robustness and interpretability. Code and data are\navailable at\nhttps://github.com/Multimodal-Intelligence-Lab-MIL/HatefulVideoLabelNoise.", "AI": {"tldr": "本文通过细粒度方法探索多模态仇恨视频中时间戳标注噪声的影响，通过精确分割仇恨片段并分析，发现时间戳噪声对模型决策的负面影响，并提出需要更智能的模型和基准。", "motivation": "动机在于解决当前多模态仇恨视频检测中粗略的视频级注释引起的重要标注噪声问题，这种噪声导致视频中虽然标注为仇恨但含有长时间无仇恨片段的问题。", "method": "本文采用细粒度方法处理多模态仇恨视频检测问题，通过使用带时间戳注释的HateMM和MultiHateClip英文数据集来隔离明确的仇恨片段，并对其进行探索性分析，以检查仇恨和非仇恨内容的分布和特征。", "result": "实验表明，时间戳噪声确实影响了模型的决策边界，降低了分类信心，证实了仇恨言论时间连续性和上下文依赖性的重要性。", "conclusion": "研究结论指出，时间戳噪声从根本上改变了模型的决策边界，降低了分类信心，强调了仇恨言论表达的上下文依赖性和时间连续性。这些发现为多模态仇恨视频的时间动态提供了新的见解，并强调了需要时间感知模型和基准来提高鲁棒性和可解释性。"}}
{"id": "2508.05100", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05100", "abs": "https://arxiv.org/abs/2508.05100", "authors": ["Yuhao Wang", "Ruiyang Ren", "Yucheng Wang", "Jing Liu", "Wayne Xin Zhao", "Hua Wu", "Haifeng Wang"], "title": "BEE-RAG: Balanced Entropy Engineering for Retrieval-Augmented Generation", "comment": null, "summary": "With the rapid advancement of large language models (LLMs),\nretrieval-augmented generation (RAG) has emerged as a critical approach to\nsupplement the inherent knowledge limitations of LLMs. However, due to the\ntypically large volume of retrieved information, RAG tends to operate with long\ncontext lengths. From the perspective of entropy engineering, we identify\nunconstrained entropy growth and attention dilution due to long retrieval\ncontext as significant factors affecting RAG performance. In this paper, we\npropose the balanced entropy-engineered RAG (BEE-RAG) framework, which improves\nthe adaptability of RAG systems to varying context lengths through the\nprinciple of entropy invariance. By leveraging balanced context entropy to\nreformulate attention dynamics, BEE-RAG separates attention sensitivity from\ncontext length, ensuring a stable entropy level. Building upon this, we\nintroduce a zero-shot inference strategy for multi-importance estimation and a\nparameter-efficient adaptive fine-tuning mechanism to obtain the optimal\nbalancing factor for different settings. Extensive experiments across multiple\nRAG tasks demonstrate the effectiveness of BEE-RAG.", "AI": {"tldr": "本文提出BEE-RAG框架，通过熵不变性原则提高检索增强生成（RAG）系统在不同上下文长度下的性能。", "motivation": "由于检索返回的信息量通常较大，RAG倾向于使用较长的上下文长度，作者从熵工程的角度指出了由于较长检索上下文导致的无约束熵增长和注意力稀释是影响RAG性能的重要因素。", "method": "该论文提出了平衡熵工程的RAG（BEE-RAG）框架，通过熵不变性原则提高RAG系统在不同上下文长度下的适应性。通过利用平衡上下文熵重新定义注意力动态，BEE-RAG将注意力敏感度与上下文长度分离，确保了稳定的熵水平。同时，提出了多重要性估计的零样本推断策略和参数高效的自适应微调机制，以获得不同设置下的最优平衡因子。", "result": "多项RAG任务上的广泛实验表明BEE-RAG的有效性。", "conclusion": "实验结果证明了BEE-RAG在处理长检索上下文时能够提高注意力机制的稳定性和效果。"}}
{"id": "2508.04924", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04924", "abs": "https://arxiv.org/abs/2508.04924", "authors": ["Zahidul Islam", "Sujoy Paul", "Mrigank Rochan"], "title": "Test-Time Adaptation for Video Highlight Detection Using Meta-Auxiliary Learning and Cross-Modality Hallucinations", "comment": null, "summary": "Existing video highlight detection methods, although advanced, struggle to\ngeneralize well to all test videos. These methods typically employ a generic\nhighlight detection model for each test video, which is suboptimal as it fails\nto account for the unique characteristics and variations of individual test\nvideos. Such fixed models do not adapt to the diverse content, styles, or audio\nand visual qualities present in new, unseen test videos, leading to reduced\nhighlight detection performance. In this paper, we propose Highlight-TTA, a\ntest-time adaptation framework for video highlight detection that addresses\nthis limitation by dynamically adapting the model during testing to better\nalign with the specific characteristics of each test video, thereby improving\ngeneralization and highlight detection performance. Highlight-TTA is jointly\noptimized with an auxiliary task, cross-modality hallucinations, alongside the\nprimary highlight detection task. We utilize a meta-auxiliary training scheme\nto enable effective adaptation through the auxiliary task while enhancing the\nprimary task. During testing, we adapt the trained model using the auxiliary\ntask on the test video to further enhance its highlight detection performance.\nExtensive experiments with three state-of-the-art highlight detection models\nand three benchmark datasets show that the introduction of Highlight-TTA to\nthese models improves their performance, yielding superior results.", "AI": {"tldr": "提出Highlight-TTA以改善视频高光检测模型的泛化性能，通过与辅助任务联合优化，实现在测试时的模型适应。", "motivation": "现有的高光检测方法难以很好地推广到所有的测试视频中，因为它们通常使用一个通用的高光检测模型，无法适应新视频的多样性。", "method": "Highlight-TTA, 一种测试时适应框架，它在测试期间动态调整模型以更好地适应每个测试视频的具体特征，通过与跨模态幻觉的辅助任务共同优化来实现这一点。", "result": "引入Highlight-TTA后，三种现有的高光检测模型在三个基准数据集上均表现出更好的性能，实现了更优的结果。", "conclusion": "实验表明，通过Highlight-TTA框架，能够显著提高现有高光检测模型的适应性与检测性能，证明了其有效性。"}}
{"id": "2508.05128", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05128", "abs": "https://arxiv.org/abs/2508.05128", "authors": ["Zihao Yi", "Delong Zeng", "Zhenqing Ling", "Haohao Luo", "Zhe Xu", "Wei Liu", "Jian Luan", "Wanxia Cao", "Ying Shen"], "title": "Attention Basin: Why Contextual Position Matters in Large Language Models", "comment": null, "summary": "The performance of Large Language Models (LLMs) is significantly sensitive to\nthe contextual position of information in the input. To investigate the\nmechanism behind this positional bias, our extensive experiments reveal a\nconsistent phenomenon we term the attention basin: when presented with a\nsequence of structured items (e.g., retrieved documents or few-shot examples),\nmodels systematically assign higher attention to the items at the beginning and\nend of the sequence, while neglecting those in the middle. Crucially, our\nanalysis further reveals that allocating higher attention to critical\ninformation is key to enhancing model performance. Based on these insights, we\nintroduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i)\nestimates a model's intrinsic positional attention preferences using a small\ncalibration set, and (ii) reorders retrieved documents or few-shot examples to\nalign the most salient content with these high-attention positions. AttnRank is\na model-agnostic, training-free, and plug-and-play method with minimal\ncomputational overhead. Experiments on multi-hop QA and few-shot in-context\nlearning tasks demonstrate that AttnRank achieves substantial improvements\nacross 10 large language models of varying architectures and scales, without\nmodifying model parameters or training procedures.", "AI": {"tldr": "研究揭示大语言模型中存在关注基准现象，提出了AttnRank方法，通过重新排序输入内容来优化模型性能，实验证明该方法有效。", "motivation": "研究动机在于理解和解决大语言模型对输入信息位置敏感的问题，以改善模型表现。", "method": "该研究通过广泛实验揭示了大语言模型输入中信息位置对模型性能的影响，并提出了Attention-Driven Reranking (AttnRank) 两阶段框架来提高模型性能。", "result": "实验结果显示，AttnRank 在多跳问答和少样本学习任务中跨越不同架构和规模的 10 个大语言模型均取得显著提升。", "conclusion": "AttnRank 是一个模型无关、无需训练、可直接插入且计算开销小的方法，能有效提升大模型性能。"}}
{"id": "2508.04928", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04928", "abs": "https://arxiv.org/abs/2508.04928", "authors": ["Suchisrit Gangopadhyay", "Jung-Hee Kim", "Xien Chen", "Patrick Rim", "Hyoungseob Park", "Alex Wong"], "title": "Extending Foundational Monocular Depth Estimators to Fisheye Cameras with Calibration Tokens", "comment": null, "summary": "We propose a method to extend foundational monocular depth estimators\n(FMDEs), trained on perspective images, to fisheye images. Despite being\ntrained on tens of millions of images, FMDEs are susceptible to the covariate\nshift introduced by changes in camera calibration (intrinsic, distortion)\nparameters, leading to erroneous depth estimates. Our method aligns the\ndistribution of latent embeddings encoding fisheye images to those of\nperspective images, enabling the reuse of FMDEs for fisheye cameras without\nretraining or finetuning. To this end, we introduce a set of Calibration Tokens\nas a light-weight adaptation mechanism that modulates the latent embeddings for\nalignment. By exploiting the already expressive latent space of FMDEs, we posit\nthat modulating their embeddings avoids the negative impact of artifacts and\nloss introduced in conventional recalibration or map projection to a canonical\nreference frame in the image space. Our method is self-supervised and does not\nrequire fisheye images but leverages publicly available large-scale perspective\nimage datasets. This is done by recalibrating perspective images to fisheye\nimages, and enforcing consistency between their estimates during training. We\nevaluate our approach with several FMDEs, on both indoors and outdoors, where\nwe consistently improve over state-of-the-art methods using a single set of\ntokens for both. Code available at:\nhttps://github.com/JungHeeKim29/calibration-token.", "AI": {"tldr": "提出一种方法，通过调整校准参数来使得单目深度估计器能够在鱼眼图像上使用。", "motivation": "解决单目深度估计器在鱼眼图像上的性能问题，因为它们在训练时使用的是不同相机校准参数下的透视图图像。", "method": "引入校准令牌（Calibration Tokens）作为适应机制，调制潜在嵌入以对齐鱼眼图像和透视图图像的分布，无需重新训练或微调单目深度估计器。", "result": "该方法在室内和室外场景下使用多种单目深度估计器进行了评估，并且只使用一套校准令牌对两个场景都给出了优于现有方法的表现。", "conclusion": "通过调整潜在嵌入空间中的校准令牌，并利用大范围的透视图图像数据集，该方法能够在不重新训练或微调现有单目深度估计算法的前提下，提升其在鱼眼图片上的深度估计性能。"}}
{"id": "2508.05132", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05132", "abs": "https://arxiv.org/abs/2508.05132", "authors": ["Chang Hong", "Minghao Wu", "Qingying Xiao", "Yuchi Wang", "Xiang Wan", "Guangjun Yu", "Benyou Wang", "Yan Hu"], "title": "Towards Assessing Medical Ethics from Knowledge to Practice", "comment": null, "summary": "The integration of large language models into healthcare necessitates a\nrigorous evaluation of their ethical reasoning, an area current benchmarks\noften overlook. We introduce PrinciplismQA, a comprehensive benchmark with\n3,648 questions designed to systematically assess LLMs' alignment with core\nmedical ethics. Grounded in Principlism, our benchmark features a high-quality\ndataset. This includes multiple-choice questions curated from authoritative\ntextbooks and open-ended questions sourced from authoritative medical ethics\ncase study literature, all validated by medical experts. Our experiments reveal\na significant gap between models' ethical knowledge and their practical\napplication, especially in dynamically applying ethical principles to\nreal-world scenarios. Most LLMs struggle with dilemmas concerning Beneficence,\noften over-emphasizing other principles. Frontier closed-source models, driven\nby strong general capabilities, currently lead the benchmark. Notably, medical\ndomain fine-tuning can enhance models' overall ethical competence, but further\nprogress requires better alignment with medical ethical knowledge.\nPrinciplismQA offers a scalable framework to diagnose these specific ethical\nweaknesses, paving the way for more balanced and responsible medical AI.", "AI": {"tldr": "论文提出PrinciplismQA，一个系统评估LLMs在医学伦理方面表现的基准测试。实验显示，尽管前沿模型在基准测试中表现领先，但在应用伦理原则方面仍存在不足，特别是在动态应用Principlism中的Beneficence原则。", "motivation": "整合大型语言模型到医疗保健需要严格评估其伦理推理能力，而当前的基准测试常常忽视这一点。", "method": "介绍了PrinciplismQA，这是一个全面的基准测试，包含了3,648个问题，旨在系统地评估LLMs在核心医学伦理方面的对齐度。基准测试基于Principlism原则，包括多选题和开放式问题，这些问题分别来源于权威教科书和医学伦理案例研究文献，并由医学专家验证。", "result": "实验揭示了模型在伦理知识和实践应用之间存在显著差距，尤其是在动态应用伦理原则到现实世界场景方面。大多数LLMs在涉及Beneficence的难题上挣扎，常常过于强调其他原则。前沿的闭源模型在基准测试中领先。", "conclusion": "PrinciplismQA提供了一个可扩展的框架，用于诊断这些特定的伦理缺陷，为更平衡和负责任的医疗AI铺平道路。"}}
{"id": "2508.04941", "categories": ["cs.CV", "cs.LG", "68T07"], "pdf": "https://arxiv.org/pdf/2508.04941", "abs": "https://arxiv.org/abs/2508.04941", "authors": ["Bo Deng", "Levi Heath"], "title": "Toward Errorless Training ImageNet-1k", "comment": "14 pages, 2 figures", "summary": "In this paper, we describe a feedforward artificial neural network trained on\nthe ImageNet 2012 contest dataset [7] with the new method of [5] to an accuracy\nrate of 98.3% with a 99.69 Top-1 rate, and an average of 285.9 labels that are\nperfectly classified over the 10 batch partitions of the dataset. The best\nperforming model uses 322,430,160 parameters, with 4 decimal places precision.\nWe conjecture that the reason our model does not achieve a 100% accuracy rate\nis due to a double-labeling problem, by which there are duplicate images in the\ndataset with different labels.", "AI": {"tldr": "本文通过改进的神经网络训练方法，提高了ImageNet数据集的分类准确性，提出了双标签问题可能影响100%准确率的观点。", "motivation": "本研究旨在通过改进神经网络训练方法提高对ImageNet数据集的分类精确度。", "method": "本论文描述了一种使用ImageNet 2012竞赛数据集训练的前馈人工神经网络，并采用了一种新的方法[5]。", "result": "该方法实现了98.3%的准确率和Top-1的99.69%，平均在数据集的10批划分中，有285.9个标签被完全正确分类。最佳模型使用了322,430,160个参数，精确到4位小数。", "conclusion": "作者推测模型未能达到100%的准确率的原因在于数据集中的双标签问题，即存在具有不同标签的重复图像。"}}
