{"id": "2510.07346", "categories": ["cs.CV", "cs.LG", "68T07, 68T45, 68U10, 62H30, 94A08", "I.2.10; I.4.8; I.5.4; I.2.6; C.3"], "pdf": "https://arxiv.org/pdf/2510.07346", "abs": "https://arxiv.org/abs/2510.07346", "authors": ["Nader Nemati"], "title": "Enhancing Maritime Object Detection in Real-Time with RT-DETR and Data Augmentation", "comment": "13 pages, 10 figures", "summary": "Maritime object detection faces essential challenges due to the small target\nsize and limitations of labeled real RGB data. This paper will present a\nreal-time object detection system based on RT-DETR, enhanced by employing\naugmented synthetic images while strictly evaluating on real data. This study\nemploys RT-DETR for the maritime environment by combining multi-scale feature\nfusion, uncertainty-minimizing query selection, and smart weight between\nsynthetic and real training samples. The fusion module in DETR enhances the\ndetection of small, low-contrast vessels, query selection focuses on the most\nreliable proposals, and the weighting strategy helps reduce the visual gap\nbetween synthetic and real domains. This design preserves DETR's refined\nend-to-end set prediction while allowing users to adjust between speed and\naccuracy at inference time. Data augmentation techniques were also used to\nbalance the different classes of the dataset to improve the robustness and\naccuracy of the model. Regarding this study, a full Python robust maritime\ndetection pipeline is delivered that maintains real-time performance even under\npractical limits. It also verifies how each module contributes, and how the\nsystem handles failures in extreme lighting or sea conditions. This study also\nincludes a component analysis to quantify the contribution of each\narchitectural module and explore its interactions.", "AI": {"tldr": "The paper proposes a real-time maritime object detection system using RT-DETR, enhanced by augmented synthetic images and special strategies to balance synthetic and real data. It maintains real-time detection of small maritime objects and offers a full Python pipeline.", "motivation": "The motivation behind this paper is to improve maritime object detection which is inherently complicated due to the small size of targets in the maritime environment and the insufficiency of labeled real RGB data.", "method": "This paper presents a real-time maritime object detection system based on RT-DETR, which is enhanced by the use of augmented synthetic images. It employs multi-scale feature fusion, uncertainty-minimizing query selection, and a smart weight strategy for balancing synthetic and real training samples to minimize the domain gap.", "result": "The system presents improved detection of small, low-contrast vessels, and it maintains real-time performance under practical limits. The component analysis provides insights into each module's contribution and their interactions.", "conclusion": "The study concludes that the proposed method using RT-DETR with enhancements can offer superior maritime object detection capabilities, while the full Python pipeline is provided as a robust design that balances speed and accuracy."}}
{"id": "2510.07441", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07441", "abs": "https://arxiv.org/abs/2510.07441", "authors": ["Nithin C. Babu", "Aniruddha Mahapatra", "Harsh Rangwani", "Rajiv Soundararajan", "Kuldeep Kulkarni"], "title": "DynamicEval: Rethinking Evaluation for Dynamic Text-to-Video Synthesis", "comment": "Preprint. Under review. 26 pages, 11 figures, 11 tables. Access the\n  project page in https://nithincbabu7.github.io/DynamicEval", "summary": "Existing text-to-video (T2V) evaluation benchmarks, such as VBench and\nEvalCrafter, suffer from two limitations. (i) While the emphasis is on\nsubject-centric prompts or static camera scenes, camera motion essential for\nproducing cinematic shots and existing metrics under dynamic motion are largely\nunexplored. (ii) These benchmarks typically aggregate video-level scores into a\nsingle model-level score for ranking generative models. Such aggregation,\nhowever, overlook video-level evaluation, which is vital to selecting the\nbetter video among the candidate videos generated for a given prompt. To\naddress these gaps, we introduce DynamicEval, a benchmark consisting of\nsystematically curated prompts emphasizing dynamic camera motion, paired with\n45k human annotations on video pairs from 3k videos generated by ten T2V\nmodels. DynamicEval evaluates two key dimensions of video quality: background\nscene consistency and foreground object consistency. For background scene\nconsistency, we obtain the interpretable error maps based on the Vbench motion\nsmoothness metric. We observe that while the Vbench motion smoothness metric\nshows promising alignment with human judgments, it fails in two cases:\nocclusions/disocclusions arising from camera and foreground object movements.\nBuilding on this, we propose a new background consistency metric that leverages\nobject error maps to correct two failure cases in a principled manner. Our\nsecond innovation is the introduction of a foreground consistency metric that\ntracks points and their neighbors within each object instance to assess object\nfidelity. Extensive experiments demonstrate that our proposed metrics achieve\nstronger correlations with human preferences at both the video level and the\nmodel level (an improvement of more than 2% points), establishing DynamicEval\nas a more comprehensive benchmark for evaluating T2V models under dynamic\ncamera motion.", "AI": {"tldr": "介绍了DynamicEval，一种系统地强调摄像机动态场景并包含45000个人工注释的基准。提出新的背景和前景一致性度量，提高了衡量文本到视频模型性能的准确性。", "motivation": "现有的文本到视频(T2V)评估基准，如VBench和EvalCrafter，存在两个局限性。首先，这些基准强调以主题为中心的提示或静态摄像机场景，但对于生成电影镜头至关重要的摄像机动态几乎没有涉及。其次，这些基准通常将视频级别的得分汇总为单一的模型级别得分，以排名生成模型，这忽略了对给定提示生成的候选视频进行视频级别评估的重要性。为了解决这些不足，引入了DynamicEval，一个系统地强调摄像机动态的提示集，并包含来自十种T2V模型生成的3000个视频中成对的45000个人工注释。", "method": "DynamicEval 评估视频质量的两个关键维度：背景场景一致性以及前景对象一致性。针对背景场景一致性，基于 Vbench 运动平滑度指标获取可解释的误差图。发现 Vbench 指标在人类判断上表现出良好的一致性，但在摄像机和前景物体运动产生的遮挡/非遮挡情况下表现不佳。在此基础上，提出一种新的背景一致性度量，利用物体误差图在一定程度上纠正两个失败情况。第二个创新是引入前景物体一致性度量，跟踪每个物体实例内部点及其邻居，以评估对象的保真度。", "result": "广泛的实验表明，提出的新指标在与人类偏好的相关性方面在视频级别和模型级别都实现了更强的相关性（提高了2%以上），确立了DynamicEval作为评估动态摄像机运动下T2V模型的更全面基准。", "conclusion": "通过提出一种新的背景一致性度量和前景物体一致性度量，DynamicEval 作为评估文本到视频模型的基准更加全面，尤其是在动态摄像机运动方面。"}}
{"id": "2510.07470", "categories": ["cs.CV", "94A08, 68U10"], "pdf": "https://arxiv.org/pdf/2510.07470", "abs": "https://arxiv.org/abs/2510.07470", "authors": ["Marien Renaud", "Julien Hermant", "Deliang Wei", "Yu Sun"], "title": "Provably Accelerated Imaging with Restarted Inertia and Score-based Image Priors", "comment": "62 pages", "summary": "Fast convergence and high-quality image recovery are two essential features\nof algorithms for solving ill-posed imaging inverse problems. Existing methods,\nsuch as regularization by denoising (RED), often focus on designing\nsophisticated image priors to improve reconstruction quality, while leaving\nconvergence acceleration to heuristics. To bridge the gap, we propose Restarted\nInertia with Score-based Priors (RISP) as a principled extension of RED. RISP\nincorporates a restarting inertia for fast convergence, while still allowing\nscore-based image priors for high-quality reconstruction. We prove that RISP\nattains a faster stationary-point convergence rate than RED, without requiring\nthe convexity of the image prior. We further derive and analyze the associated\ncontinuous-time dynamical system, offering insight into the connection between\nRISP and the heavy-ball ordinary differential equation (ODE). Experiments\nacross a range of imaging inverse problems demonstrate that RISP enables fast\nconvergence while achieving high-quality reconstructions.", "AI": {"tldr": "RISP combines fast convergence and high-quality reconstruction in imaging problems, outperforming traditional RED methods both theoretically and practically.", "motivation": "To address the common issue in existing imaging inverse problem solving algorithms where fast convergence and high-quality reconstruction trade off against each other.", "method": "Restarted Inertia with Score-based Priors (RISP) is proposed as an improvement over existing RED methods by incorporating a restarting inertia for quicker convergence without sacrificing reconstruction quality.", "result": "Proven faster stationary-point convergence than RED without the need for the image prior to be convex, along with experimental demonstrations of both speed and quality improvements.", "conclusion": "RISP offers a balance of fast convergence and high-quality image recovery, supported by theoretical proofs and empirical evidence."}}
{"id": "2510.07492", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07492", "abs": "https://arxiv.org/abs/2510.07492", "authors": ["Guoliang Gong", "Man Yu"], "title": "A Denoising Framework for Real-World Ultra-Low Dose Lung CT Images Based on an Image Purification Strategy", "comment": null, "summary": "Ultra-low dose CT (uLDCT) significantly reduces radiation exposure but\nintroduces severe noise and artifacts. It also leads to substantial spatial\nmisalignment between uLDCT and normal dose CT (NDCT) image pairs. This poses\nchallenges for directly applying existing denoising networks trained on\nsynthetic noise or aligned data. To address this core challenge in uLDCT\ndenoising, this paper proposes an innovative denoising framework based on an\nImage Purification (IP) strategy. First, we construct a real clinical uLDCT\nlung dataset. Then, we propose an Image Purification strategy that generates\nstructurally aligned uLDCT-NDCT image pairs, providing a high-quality data\nfoundation for network training. Building upon this, we propose a\nFrequency-domain Flow Matching (FFM) model, which works synergistically with\nthe IP strategy to excellently preserve the anatomical structure integrity of\ndenoised images. Experiments on the real clinical dataset demonstrate that our\nIP strategy significantly enhances the performance of multiple mainstream\ndenoising models on the uLDCT task. Notably, our proposed FFM model combined\nwith the IP strategy achieves state-of-the-art (SOTA) results in anatomical\nstructure preservation. This study provides an effective solution to the data\nmismatch problem in real-world uLDCT denoising. Code and dataset are available\nat https://github.com/MonkeyDadLufy/flow-matching.", "AI": {"tldr": "本文提出了基于图像净化策略（IP）的超低剂量CT（uLDCT）降噪框架，并引入了频率域流匹配（FFM）模型来进行有效降噪，提升了uLDCT图像解剖结构的完整性。", "motivation": "超低剂量CT (uLDCT) 虽然显著减少了辐射暴露，但引入了严重的噪声和伪影，并导致uLDCT和正常剂量CT (NDCT) 图像间的空间错位。因此现有降噪网络无法直接应用于uLDCT图像，从而提出了本研究来解决uLDCT图像降噪挑战。", "method": "本文提出了一种基于图像净化策略（IP）的超低剂量CT（uLDCT）降噪框架。IP策略用于生成结构对齐的uLDCT-NDCT图像对，为网络训练提供了高质量的数据基础。在此基础上，提出了频率域流匹配（FFM）模型，协同IP策略以极佳地保持了降噪图像的解剖结构完整性。", "result": "实验结果表明，IP策略显著提升了多种主流降噪模型在uLDCT任务中的性能。而结合IP策略的FFM模型达到了最先进的（SOTA）结构保留效果。", "conclusion": "本研究为解决现实世界中uLDCT图像降噪的数据不匹配问题提供了有效方案。代码和数据集可在https://github.com/MonkeyDadLufy/flow-matching 找到。"}}
{"id": "2510.07359", "categories": ["cs.CL", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.07359", "abs": "https://arxiv.org/abs/2510.07359", "authors": ["Jingfei Huang", "Han Tu"], "title": "Inconsistent Affective Reaction: Sentiment of Perception and Opinion in Urban Environments", "comment": "10 pages", "summary": "The ascension of social media platforms has transformed our understanding of\nurban environments, giving rise to nuanced variations in sentiment reaction\nembedded within human perception and opinion, and challenging existing\nmultidimensional sentiment analysis approaches in urban studies. This study\npresents novel methodologies for identifying and elucidating sentiment\ninconsistency, constructing a dataset encompassing 140,750 Baidu and Tencent\nStreet view images to measure perceptions, and 984,024 Weibo social media text\nposts to measure opinions. A reaction index is developed, integrating object\ndetection and natural language processing techniques to classify sentiment in\nBeijing Second Ring for 2016 and 2022. Classified sentiment reaction is\nanalysed and visualized using regression analysis, image segmentation, and word\nfrequency based on land-use distribution to discern underlying factors. The\nperception affective reaction trend map reveals a shift toward more evenly\ndistributed positive sentiment, while the opinion affective reaction trend map\nshows more extreme changes. Our mismatch map indicates significant disparities\nbetween the sentiments of human perception and opinion of urban areas over the\nyears. Changes in sentiment reactions have significant relationships with\nelements such as dense buildings and pedestrian presence. Our inconsistent maps\npresent perception and opinion sentiments before and after the pandemic and\noffer potential explanations and directions for environmental management, in\nformulating strategies for urban renewal.", "AI": {"tldr": "通过结合社交媒体和街景图像，研究提出了一种新的方法来识别和阐明北京市的感知与意见情感反应不一致现象，并分析了这些情感反应的变化与城市元素的关系。", "motivation": "传统的多维情感分析方法在解释城市情感方面遇到挑战，而社交媒体数据提供了丰富的感知与意见情感信息，可以为此提供新的研究视角。", "method": "构建了包含百度和腾讯街景图片及微博文本数据的数据集，并开发了一种结合目标检测和自然语言处理的情感反应指数，使用回归分析、图像分割和词频分析等方法进行情感分类分析。", "result": "感知情感趋势图显示正向情绪分布更均匀，而意见情感趋势图则显示了更为极端的变化，不匹配图谱表明了在多年间城市区域感知与意见情感之间的重大差异，情感反应的变化与密集建筑和行人存在等因素有显著关系。", "conclusion": "情感感知与意见之间的不一致揭示了城市环境的变化，特别是疫情前后的变化，这对于城市更新策略的制定提供了方向和解释。"}}
{"id": "2510.07538", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07538", "abs": "https://arxiv.org/abs/2510.07538", "authors": ["Pragati Shuddhodhan Meshram", "Varun Chandrasekaran"], "title": "D2RA: Dual Domain Regeneration Attack", "comment": null, "summary": "The growing use of generative models has intensified the need for\nwatermarking methods that ensure content attribution and provenance. While\nrecent semantic watermarking schemes improve robustness by embedding signals in\nlatent or frequency representations, we show they remain vulnerable even under\nresource-constrained adversarial settings. We present D2RA, a training-free,\nsingle-image attack that removes or weakens watermarks without access to the\nunderlying model. By projecting watermarked images onto natural priors across\ncomplementary representations, D2RA suppresses watermark signals while\npreserving visual fidelity. Experiments across diverse watermarking schemes\ndemonstrate that our approach consistently reduces watermark detectability,\nrevealing fundamental weaknesses in current designs. Our code is available at\nhttps://github.com/Pragati-Meshram/DAWN.", "AI": {"tldr": "研究提出了D2RA，一种无须训练且适用于单一图像的方法，用以在资源受限的对抗环境中移除或削弱生成模型中的水印，展示了现有水印技术的不足之处。", "motivation": "随着生成模型使用的增加，确保内容归属和来源的水印方法变得尤为重要。而最新的语义水印方案虽然通过在潜伏或频率表示中嵌入信号提高了鲁棒性，但我们发现它们在资源受限的对抗环境中仍然存在漏洞。", "method": "我们提出了D2RA方法，这是一种无需训练、单一图像攻击的方法，可以在不访问底层模型的情况下移除或削弱水印。通过将带有水印的图像投影到互补表示中的自然先验上，D2RA能够抑制水印信号同时保持视觉保真度。", "result": "实验结果显示，我们的方法在多种水印方案下始终能够降低水印的可检测性，揭示了现有设计的基本弱点。", "conclusion": "我们的研究揭示了现有水印设计的基本弱点，并提供了一种有效的方法来削弱或移除这些水印，凸显了在对抗性设置下水印技术面临的挑战。"}}
{"id": "2510.07414", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.07414", "abs": "https://arxiv.org/abs/2510.07414", "authors": ["Mufei Li", "Dongqi Fu", "Limei Wang", "Si Zhang", "Hanqing Zeng", "Kaan Sancak", "Ruizhong Qiu", "Haoyu Wang", "Xiaoxin He", "Xavier Bresson", "Yinglong Xia", "Chonglin Sun", "Pan Li"], "title": "Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation", "comment": "Code available at https://github.com/Graph-COM/HaystackCraft", "summary": "Modern long-context large language models (LLMs) perform well on synthetic\n\"needle-in-a-haystack\" (NIAH) benchmarks, but such tests overlook how noisy\ncontexts arise from biased retrieval and agentic workflows. We argue that\nhaystack engineering is necessary to construct noisy long contexts that\nfaithfully capture key real-world factors -- distraction from heterogeneous\nbiased retrievers and cascading errors in agentic workflows -- to test models'\nlong-context robustness. We instantiate it through HaystackCraft, a new NIAH\nbenchmark built on the full English Wikipedia hyperlink network with multi-hop\nquestions. HaystackCraft evaluates how heterogeneous retrieval strategies\n(e.g., sparse, dense, hybrid, and graph-based) affect distractor composition,\nhaystack ordering, and downstream LLM performance. HaystackCraft further\nextends NIAH to dynamic, LLM-dependent settings that simulate agentic\noperations, where models refine queries, reflect on their past reasonings, and\ndecide when to stop. Experiments with 15 long-context models show that (1)\nwhile stronger dense retrievers can introduce more challenging distractors,\ngraph-based reranking simultaneously improves retrieval effectiveness and\nmitigates more harmful distractors; (2) in agentic tests, even advanced models\nlike Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated\ndistractors or struggle to perform early stops. These results highlight\npersistent challenges in agentic long-context reasoning and establish\nHaystackCraft as a valuable testbed for future progress.", "AI": {"tldr": "介绍了HaystackCraft，一个新的“针在haystack中”基准测试，用于评估LLMs在嘈杂长上下文中的表现，实验结果揭示了在机构设置下的持续推理挑战。", "motivation": "当前，现代长上下文大型语言模型（LLMs）在合成的“针在haystack中”基准测试中表现良好，但这些测试忽视了来自有偏检索和机构工作流的嘈杂上下文。认为需要haystack工程来构建忠实反映关键现实因素（如异构有偏检索器产生的干扰和机构工作流中的级联错误）嘈杂长上下文，以测试模型的长上下文鲁棒性。", "method": "通过HaystackCraft，一个新的基于全英文维基百科超链接网络的多跳问题构建的“针在 haystack中”基准测试，研究异构检索策略（例如稀疏、密集、混合和基于图的）如何影响干扰成分、haystack顺序以及下游LLM性能。此外，在模拟机构操作的动态、LLM依赖环境下，模型可以优化查询，反思之前的推理并决定何时停止。", "result": "实验结果指出，(1) 更强的密集检索器可以引入更具挑战性的干扰，而基于图的重新排序同时提高了检索有效性并缓解了更多的有害干扰；(2) 在机构测试中，即使像Gemini 2.5 Pro和GPT-5这样的先进模型也会因自我生成的干扰或困难进行早期停止而出现级联失败。", "conclusion": "实验结果表明了在机构长上下文推理中持久存在的挑战，并建立了HaystackCraft作为未来研究的有价值测试平台。"}}
{"id": "2510.07546", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07546", "abs": "https://arxiv.org/abs/2510.07546", "authors": ["Soroush Mehraban", "Vida Adeli", "Jacob Rommann", "Babak Taati", "Kyryl Truskovskyi"], "title": "PickStyle: Video-to-Video Style Transfer with Context-Style Adapters", "comment": null, "summary": "We address the task of video style transfer with diffusion models, where the\ngoal is to preserve the context of an input video while rendering it in a\ntarget style specified by a text prompt. A major challenge is the lack of\npaired video data for supervision. We propose PickStyle, a video-to-video style\ntransfer framework that augments pretrained video diffusion backbones with\nstyle adapters and benefits from paired still image data with source-style\ncorrespondences for training. PickStyle inserts low-rank adapters into the\nself-attention layers of conditioning modules, enabling efficient\nspecialization for motion-style transfer while maintaining strong alignment\nbetween video content and style. To bridge the gap between static image\nsupervision and dynamic video, we construct synthetic training clips from\npaired images by applying shared augmentations that simulate camera motion,\nensuring temporal priors are preserved. In addition, we introduce Context-Style\nClassifier-Free Guidance (CS-CFG), a novel factorization of classifier-free\nguidance into independent text (style) and video (context) directions. CS-CFG\nensures that context is preserved in generated video while the style is\neffectively transferred. Experiments across benchmarks show that our approach\nachieves temporally coherent, style-faithful, and content-preserving video\ntranslations, outperforming existing baselines both qualitatively and\nquantitatively.", "AI": {"tldr": "该研究解决了视频风格迁移问题，提出了PickStyle框架，不仅利用样式适配器实现了高效率的风格迁移，还通过新型无分类器指导技术（CS-CFG）确保了生成视频的上下文保持与风格正确转移。", "motivation": "针对视频风格迁移任务，特别是在缺乏成对视频数据进行监督的情况下，需要开发一种有效的解决方案来实现内容的保真和风格的迁移。", "method": "提出PickStyle框架，通过在预训练的视频扩散模型中添加样式适配器，并利用配对的静态图像数据进行训练来解决视频风格迁移问题。PickStyle在条件模块的自注意力层中插入低秩适配器，增强了动作风格迁移的效率，同时保持了视频内容和风格的一致性。为了弥合静态图像监督与动态视频之间的差距，通过应用模拟摄像机运动的共享增强技术，从配对图像中构建合成训练片段。此外，引入了背景样式无分类器指导（CS-CFG），这是一种新型的无分类器指导因子，能够确保生成的视频在保持上下文的同时有效转移风格。", "result": "", "conclusion": "实验结果表明，该方法能实现时间连贯的、风格保留的、且内容保持的视频翻译，无论是定性还是定量评估都优于现有的基线方法。"}}
{"id": "2510.07434", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07434", "abs": "https://arxiv.org/abs/2510.07434", "authors": ["Olia Toporkov", "Alan Akbik", "Rodrigo Agerri"], "title": "Lemma Dilemma: On Lemma Generation Without Domain- or Language-Specific Training Data", "comment": "14 pages, 2 figures, 5 tables. Accepted to EMNLP Findings 2025", "summary": "Lemmatization is the task of transforming all words in a given text to their\ndictionary forms. While large language models (LLMs) have demonstrated their\nability to achieve competitive results across a wide range of NLP tasks, there\nis no prior evidence of how effective they are in the contextual lemmatization\ntask. In this paper, we empirically investigate the capacity of the latest\ngeneration of LLMs to perform in-context lemmatization, comparing it to the\ntraditional fully supervised approach. In particular, we consider the setting\nin which supervised training data is not available for a target domain or\nlanguage, comparing (i) encoder-only supervised approaches, fine-tuned\nout-of-domain, and (ii) cross-lingual methods, against direct in-context lemma\ngeneration with LLMs. Our experimental investigation across 12 languages of\ndifferent morphological complexity finds that, while encoders remain\ncompetitive in out-of-domain settings when fine-tuned on gold data, current\nLLMs reach state-of-the-art results for most languages by directly generating\nlemmas in-context without prior fine-tuning, provided just with a few examples.\nData and code available upon publication:\nhttps://github.com/oltoporkov/lemma-dilemma", "AI": {"tldr": "研究了大型语言模型(LLMs)在上下文引理化任务中未经过优化前的表现，并与传统的监督方法进行了比较，发现LLMs只需要少量示例即可在大多数语言中实现最先进的结果。", "motivation": "此前没有关于大型语言模型在上下文引语化任务中有效性的证据，研究其在没有监督训练数据的目标域或语言中的性能。", "method": "比较了监督方法（包括仅对编码器优化和跨语言方法）与直接使用LLMs通过上下文生成引语的方法。研究覆盖了12种不同形态学复杂度的语言。", "result": "发现即使编码器在经过非目标域数据优化后仍具有竞争力，但未经优化的当前LLM在大多数语言中通过上下文直接生成引语达到了最先进的结果。", "conclusion": "在没有目标领域或语言的监督训练数据情况下，LLMs通过上下文生成引语的研究表明，在提供了一些例子后，它们可以达到最先进的性能，凸显了LLMs在未见过的数据集上的适应性。"}}
{"id": "2510.07550", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07550", "abs": "https://arxiv.org/abs/2510.07550", "authors": ["Saman Motamed", "Minghao Chen", "Luc Van Gool", "Iro Laina"], "title": "TRAVL: A Recipe for Making Video-Language Models Better Judges of Physics Implausibility", "comment": null, "summary": "Despite impressive visual fidelity, modern video generative models frequently\nproduce sequences that violate intuitive physical laws, such as objects\nfloating, teleporting, or morphing in ways that defy causality. While humans\ncan easily detect such implausibilities, there remains no robust method for\nquantitatively assessing physical realism in video. In this work, we explore\nwhether Video-Language Models (VLMs) can be trained to serve as reliable judges\nof physical plausibility. We find that existing VLMs struggle to identify\nphysics violations, exposing fundamental limitations in their temporal and\ncausal reasoning. To address this, we introduce TRAVL, a fine-tuning recipe\nthat combines a balanced training dataset with a trajectory-aware attention\nmodule to improve motion encoding and discrimination in VLMs. To evaluate\nphysical reasoning more rigorously, we propose ImplausiBench, a benchmark of\n300 videos (150 real, 150 generated) that removes linguistic biases and\nisolates visual-temporal understanding. Performance is reported both with\ngold-standard human judgments and stricter LLM-as-judge metrics. Together,\nTRAVL and ImplausiBench offer a unified framework for probing and improving\nphysical plausibility in multimodal models, shedding light on a challenging and\nunderexplored aspect of visual-temporal understanding.", "AI": {"tldr": "研究提出TRAVL和ImplausiBench，用于改进视频-语言模型中的物理合理性评估，展示了未来在改进视觉-时间理解方面的工作方向。", "motivation": "尽管现代视频生成模型提供了惊人的视觉保真度，但它们产生的序列经常违反直觉上的物理定律。此研究旨在探索如何通过视频-语言模型来更可靠地评估视频中的物理现实性。", "method": "探索使用视频-语言模型（VLM）来作为物理合理性的判断者，并介绍了TRA VL，一个结合了平衡训练数据集与轨迹感知注意力模块的微调配方，以改进VLM中的运动编码和分辨能力。", "result": "通过引入ImplausiBench，一个包含300个视频（150个真实，150个生成）的基准，该研究更加严格地评估了物理推理能力的表现。结果表明TRAVL在物理合理性的判断上有显著提升。", "conclusion": "TRAVL和ImplausiBench一起提供了一个完整的框架，用于探究和改进跨模态模型中的物理合理性，突显了视觉-时间理解中的挑战和未被充分探索的领域。"}}
{"id": "2510.07437", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.07437", "abs": "https://arxiv.org/abs/2510.07437", "authors": ["Amruta Parulekar", "Preethi Jyothi"], "title": "LASER: An LLM-based ASR Scoring and Evaluation Rubric", "comment": "Accepted to EMNLP 2025", "summary": "Standard ASR evaluation metrics like Word Error Rate (WER) tend to unfairly\npenalize morphological and syntactic nuances that do not significantly alter\nsentence semantics. We introduce an LLM-based scoring rubric LASER that\nleverages state-of-the-art LLMs' in-context learning abilities to learn from\nprompts with detailed examples. Hindi LASER scores using Gemini 2.5 Pro\nachieved a very high correlation score of 94% with human annotations. Hindi\nexamples in the prompt were also effective in analyzing errors in other Indian\nlanguages such as Marathi, Kannada and Malayalam. We also demonstrate how a\nsmaller LLM like Llama 3 can be finetuned on word-pair examples derived from\nreference and ASR predictions to predict what kind of penalty should be applied\nwith close to 89% accuracy.", "AI": {"tldr": "提出LASER评分标准，用于更公平地评估语音识别结果，避免对不影响语义的细节惩罚过重。使用先进LLM Gemini 2.5 Pro达到了与人类标注的高度相关性，并且证明了方法在其他印度语言的有效性。", "motivation": "传统的语音识别评估指标如词错误率（WER）倾向于不公平地惩罚那些不会显著改变句子语义的形态和句法细微差别。", "method": "引入了一种基于LLM的评分标准LASER，该标准利用了先进LLM的上下文学习能力，通过包含详细示例的提示进行学习。此外，还展示了如何通过对从参考文本和ASR预测中导出的词对示例进行微调，使较小的LLM（如Llama 3）能够准确预测应施加何种处罚。", "result": "使用Gemini 2.5 Pro的Hindi LASER得分与人类标注的高度相关性达到了94%，且Hindi示例也有效地用于分析其他印度语言（如Marathi、Kannada和Malayalam）中的错误。此外，Llama 3 LLM在预测应适用何种惩罚时的准确率接近89%。", "conclusion": "该研究提出了一种新的评估语音识别系统性能的方法，采用基于LLM和上下文学习的评分标准LASER，展示了如何有效地衡量那些不改变句子语义的细微差别，并且证明了这种方法在多个印度语言中的有效性。"}}
{"id": "2510.07556", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07556", "abs": "https://arxiv.org/abs/2510.07556", "authors": ["Rafin Hassan", "Zarin Tasnim Roshni", "Rafiqul Bari", "Alimul Islam", "Nabeel Mohammed", "Moshiur Farazi", "Shafin Rahman"], "title": "Label Semantics for Robust Hyperspectral Image Classification", "comment": "This work has been accepted for publication in the proceedings of\n  IJCNN 2025", "summary": "Hyperspectral imaging (HSI) classification is a critical tool with widespread\napplications across diverse fields such as agriculture, environmental\nmonitoring, medicine, and materials science. Due to the limited availability of\nhigh-quality training samples and the high dimensionality of spectral data, HSI\nclassification models are prone to overfitting and often face challenges in\nbalancing accuracy and computational complexity. Furthermore, most of HSI\nclassification models are monomodal, where it solely relies on spectral-spatial\ndata to learn decision boundaries in the high dimensional embedding space. To\naddress this, we propose a general-purpose Semantic Spectral-Spatial Fusion\nNetwork (S3FN) that uses contextual, class specific textual descriptions to\ncomplement the training of an HSI classification model. Specifically, S3FN\nleverages LLMs to generate comprehensive textual descriptions for each class\nlabel that captures their unique characteristics and spectral behaviors. These\ndescriptions are then embedded into a vector space using a pre-trained text\nencoder such as BERT or RoBERTa to extract meaningful label semantics which in\nturn leads to a better feature-label alignment for improved classification\nperformance. To demonstrate the effectiveness of our approach, we evaluate our\nmodel on three diverse HSI benchmark datasets - Hyperspectral Wood,\nHyperspectralBlueberries, and DeepHS-Fruit and report significant performance\nboost. Our results highlight the synergy between textual semantics and\nspectral-spatial data, paving the way for further advancements in semantically\naugmented HSI classification models. Codes are be available in:\nhttps://github.com/milab-nsu/S3FN", "AI": {"tldr": "S3FN利用特定类别的文本描述增强HSI分类，提升了性能。", "motivation": "光谱图像（HSI）分类面临着样本量有限和高维光谱数据导致的过度拟合问题。现有模型大多为单模态，仅依赖光谱-空间数据来学习高维嵌入空间中的决策边界。", "method": "提出了一种通用的语义光谱-空间融合网络（S3FN），利用上下文相关的、特定类别的文本描述来补充HSI分类模型的训练。S3FN使用大型语言模型为每个类别标签生成详细的文本描述，并利用预训练的文本编码器如BERT或RoBERTa将其嵌入向量空间，以捕捉独特的类别语义。", "result": "通过在三个多样的HSI基准数据集上评估模型，展示了显著的性能提升。", "conclusion": "研究展示了文本语义与光谱-空间数据之间的协同作用，为语义增强的HSI分类模型的发展铺平了道路。"}}
{"id": "2510.07453", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07453", "abs": "https://arxiv.org/abs/2510.07453", "authors": ["Zifan Jiang", "Colin Leong", "Amit Moryossef", "Anne Göhring", "Annette Rios", "Oliver Cory", "Maksym Ivashechkin", "Neha Tarigopula", "Biao Zhang", "Rico Sennrich", "Sarah Ebling"], "title": "Meaningful Pose-Based Sign Language Evaluation", "comment": "Accepted at WMT 2025", "summary": "We present a comprehensive study on meaningfully evaluating sign language\nutterances in the form of human skeletal poses. The study covers keypoint\ndistance-based, embedding-based, and back-translation-based metrics. We show\ntradeoffs between different metrics in different scenarios through automatic\nmeta-evaluation of sign-level retrieval and a human correlation study of\ntext-to-pose translation across different sign languages. Our findings and the\nopen-source pose-evaluation toolkit provide a practical and reproducible way of\ndeveloping and evaluating sign language translation or generation systems.", "AI": {"tldr": "本文研究了针对人类骨骼姿态形式的手语表达有意义的评估方法，希望通过评估工具包促进手语翻译或生成系统的开发。", "motivation": "本文旨在为手语翻译或生成系统的开发和评估提供实践性和可重复性的方式。", "method": "本文通过自动元评估手势级检索和跨不同手语的文本到姿态翻译的人类相关性研究，对手势语表达（以人类骨骼姿态的形式）进行有意义的评估。研究涵盖了基于关键点距离、嵌入式以及反向翻译的度量方法。", "result": "研究表明不同度量方法在不同场景下的权衡，并提供了一个开源的姿态评估工具包，有助于实践性和可重复性的手语翻译或生成系统的开发和评估。", "conclusion": "研究成果和发布的开源姿态评估工具包为手语翻译或生成系统的开发和评估提供了一个实践性和可重复性的方法。"}}
{"id": "2510.07567", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07567", "abs": "https://arxiv.org/abs/2510.07567", "authors": ["Karuna Bhaila", "Aneesh Komanduri", "Minh-Hao Van", "Xintao Wu"], "title": "Cross-Modal Attention Guided Unlearning in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated immense capabilities in\nmulti-modal understanding and inference tasks such as Visual Question Answering\n(VQA), which requires models to infer outputs based on visual and textual\ncontext simultaneously. Such inference abilities of large-scale pretrained\nmodels are often attributed to the massive scale of pre-training data collected\nacross several domains. However, the models may memorize private and/or\nsensitive information during training and regurgitate it in inference.\nRecently, machine unlearning has been leveraged to address the leakage of\nprivate data in LLMs. VLMs add a layer of complexity to this process, as the\nvisual context in the query may also contain sensitive information in addition\nto the text. To address this issue, we explore unlearning for vision-language\nmodels, specifically for the VQA task. We explore the role of visual tokens for\noutput generation in VLMs using cross-modal attention and utilize it to\nformulate Cross-Modal Attention Guided Unlearning (CAGUL), a lightweight and\nefficient VLM unlearning framework. In contrast to computationally expensive\nmodel finetuning methods, CAGUL utilizes external modules to encode unlearning\ninformation in visual tokens of low importance for relevant queries. We find\nthat the transformed visual tokens not only prevent leakage but also retain\nreference model behavior. Experimental results show that our method performs\nbetter or on par with finetuning-based baselines without altering the\npre-trained model parameters or incurring retraining costs, making it a\npractical and effective unlearning solution for VLMs.", "AI": {"tldr": "本文提出了一种轻量级的视觉语言模型的卸载框架CAGUL，该方法利用跨模态注意力机制，在不改变预训练模型参数的情况下，有效防止模型在推理过程中泄露敏感信息。", "motivation": "视觉语言模型在处理多模态理解任务时具有强大的能力，但同时存在泄露训练过程中学到的敏感信息的风险。针对这一问题，本文旨在探索视觉语言模型尤其是VQA任务的卸载方法。", "method": "本文提出了一种名为CAGUL（Cross-Modal Attention Guided Unlearning）的轻量级视觉语言模型的卸载框架，该框架利用跨模态注意力来编码不需要的视觉记号，从而防止敏感信息泄露，同时保持预训练模型的行为。不同于计算成本高的微调方法，CAGUL侧重于对外部模块的利用，将卸载信息编码进与查询相关且重要性较低的视觉记号中。", "result": "实验结果显示，与微调基线方法相比，CAGUL在不改变预训练模型参数或不需重新训练的情况下，性能表现相当或优于基线方法，是一个既实用又有效的视觉语言模型卸载解决方案。", "conclusion": "CAGUL框架是一种去除视觉和语言模型中隐私信息的有效方法，具有轻量且高效的特点，非常适合解决视觉语言模型在推理时泄露敏感信息的问题。"}}
{"id": "2510.07458", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07458", "abs": "https://arxiv.org/abs/2510.07458", "authors": ["Eduardo Ryô Tamaki", "Yujin J. Jung", "Julia Chatterley", "Grant Mitchell", "Semir Dzebo", "Cristóbal Sandoval", "Levente Littvay", "Kirk A. Hawkins"], "title": "Populism Meets AI: Advancing Populism Research with LLMs", "comment": "27 pages, 3 figures. Preprint version under review", "summary": "Measuring the ideational content of populism remains a challenge. Traditional\nstrategies based on textual analysis have been critical for building the\nfield's foundations and providing a valid, objective indicator of populist\nframing. Yet these approaches are costly, time consuming, and difficult to\nscale across languages, contexts, and large corpora. Here we present the\nresults from a rubric and anchor guided chain of thought (CoT) prompting\napproach that mirrors human coder training. By leveraging the Global Populism\nDatabase (GPD), a comprehensive dataset of global leaders' speeches annotated\nfor degrees of populism, we replicate the process used to train human coders by\nprompting the LLM with an adapted version of the same documentation to guide\nthe model's reasoning. We then test multiple proprietary and open weight models\nby replicating scores in the GPD. Our findings reveal that this domain specific\nprompting strategy enables the LLM to achieve classification accuracy on par\nwith expert human coders, demonstrating its ability to navigate the nuanced,\ncontext sensitive aspects of populism.", "AI": {"tldr": "提出了一种类似人工编码员训练过程的指南和锚定指导的连贯思考（CoT）提示方法，以解决跨语言、语境和大型语料库中民粹主义观念内容测量的问题。实验结果显示，这种方法使LLM能够达到专家人工编码员的分类准确性。", "motivation": "针对基于文本分析的传统策略在跨语言、语境和大型语料库中扩展成本高昂、耗时且困难的问题，提出了一种新方法来解决民粹主义观念内容测量的挑战。", "method": "通过使用一个指南和锚定指导的连贯思考（CoT）提示方法，该方法模仿了人工编码员的培训过程。借助Global Populism Database (GPD)，这是一个全球领导人演讲的详尽数据集，并已根据民粹主义的程度进行了注释。", "result": "实验结果表明，该方法使LLM达到了与专家人工编码员相似的分类准确性，证明了这种方法在处理民粹主义复杂和语境敏感方面的能力。", "conclusion": "实验结果表明，通过域特定的提示策略，大语言模型可以达到与专家人工编码员相当的分类准确性，显示了模型处理民粹主义复杂、语境敏感方面的能力。"}}
{"id": "2510.07580", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07580", "abs": "https://arxiv.org/abs/2510.07580", "authors": ["Dewi Endah Kharismawati", "Toni Kazic"], "title": "MaizeStandCounting (MaSC): Automated and Accurate Maize Stand Counting from UAV Imagery Using Image Processing and Deep Learning", "comment": "10 pages, 11 figures. Submitted to IEEE Journal of Selected Topics in\n  Signal Processing (JSTSP) Special Series on Artificial Intelligence for Smart\n  Agriculture", "summary": "Accurate maize stand counts are essential for crop management and research,\ninforming yield prediction, planting density optimization, and early detection\nof germination issues. Manual counting is labor-intensive, slow, and\nerror-prone, especially across large or variable fields. We present\nMaizeStandCounting (MaSC), a robust algorithm for automated maize seedling\nstand counting from RGB imagery captured by low-cost UAVs and processed on\naffordable hardware. MaSC operates in two modes: (1) mosaic images divided into\npatches, and (2) raw video frames aligned using homography matrices. Both modes\nuse a lightweight YOLOv9 model trained to detect maize seedlings from V2-V10\ngrowth stages. MaSC distinguishes maize from weeds and other vegetation, then\nperforms row and range segmentation based on the spatial distribution of\ndetections to produce precise row-wise stand counts. Evaluation against\nin-field manual counts from our 2024 summer nursery showed strong agreement\nwith ground truth (R^2= 0.616 for mosaics, R^2 = 0.906 for raw frames). MaSC\nprocessed 83 full-resolution frames in 60.63 s, including inference and\npost-processing, highlighting its potential for real-time operation. These\nresults demonstrate MaSC's effectiveness as a scalable, low-cost, and accurate\ntool for automated maize stand counting in both research and production\nenvironments.", "AI": {"tldr": "MaSC是一个利用轻量级YOLOv9模型进行玉米幼苗计数的算法，通过无人机捕获的RGB图像进行操作。该算法在研究和生产环境中展示出了准确和实时计数的性能。", "motivation": "手动计数玉米幼苗耗时且容易出错，尤其是在大面积或可变性田地上。为了提高农业管理和研究的效率，需要一个快速、准确、低成本的方法来计数玉米幼苗。", "method": "MaizeStandCounting (MaSC)使用轻量级YOLOv9模型从RGB图像中检测玉米幼苗，这些图像是由低成本无人机捕获并经过经济硬件处理的。该算法有两种模式：1) 马赛克图像被分割成补丁；2) 通过使用单应性矩阵对齐的原始视频帧。MaSC能够区分玉米和杂草及其他植被，并基于检测的空间分布进行行和范围分割，从而获得精确的行数计数。", "result": "在田野实地的手动计数基准测试中，对于马赛克图像而言，相关系数达到了0.616，而对于原始视频帧来讲，相关系数达到了0.906。MaSC可以在60.63秒内处理83个全分辨率帧，包括推理和后处理，展示了其实时运行的能力。", "conclusion": "MaSC使用低硬件预算和低成本无人机图像实现了精确的玉米幼苗计数，具有可扩展性和高准确性，适用于研究和生产环境中的自动化玉米幼苗计数。"}}
{"id": "2510.07475", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07475", "abs": "https://arxiv.org/abs/2510.07475", "authors": ["Zheyuan Zhang", "Lin Ge", "Hongjiang Li", "Weicheng Zhu", "Chuxu Zhang", "Yanfang Ye"], "title": "MAPRO: Recasting Multi-Agent Prompt Optimization as Maximum a Posteriori Inference", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, and LLM-based agents further extend these abilities to various\npractical workflows. While recent progress shows that multi-agent systems (MAS)\ncan outperform single agents by coordinating specialized roles, designing\neffective MAS remains difficult due to prompt sensitivity and the compounded\ninstability MAS creates. To cope with the challenge, recent efforts in\nautomated prompt design have reduced manual effort. However, multi-agent prompt\noptimization remains largely unexplored. Challenges like exponentially\nexpanding search space and ambiguous credit assignment together make systematic\ndesign intractable without principled methods. Therefore, we introduce\nM}ulti-Agent PRompt Optimization (MAPRO), a four-stage framework that first\nformulates MAS prompt optimization as a Maximum a Posteriori (MAP) inference\nproblem and solves it using a language-guided variant of max-product belief\npropagation algorithm. To address credit assignment and updates the system\niteratively, MAPRO employs a topology-aware refinement mechanism that\nintegrates execution feedback and downstream blames to selectively update agent\nprompts. Through this process, MAPRO progressively converges to a coordinated\nset of agent-specific prompt policies. Across benchmarks in various tasks,\nMAPRO achieves state-of-the-art performance, consistently surpassing manually\nengineered baselines and recent automated alternatives. Beyond performance, our\nMAP-based formulation also delivers general guidelines for building more\nreliable and principled multi-agent systems in the future", "AI": {"tldr": "研究提出了MAPRO，一个高效的多代理系统提示优化框架，该框架通过拓扑感知精炼机制在最大后验概率推理框架下实现逐步收敛，以解决搜索空间过大和责任分配难以明确的问题。实验结果表明，该方法在性能上优于手动设计和自动化替代方案，并提供了未来工作的指导。", "motivation": "大语言模型（LLMs）及其驱动的代理在多样任务中表现出色。然而，尽管多方努力，多代理系统（MAS）的自动提示设计仍然缺乏系统性方法。这是因为搜索空间爆炸性增长和模糊责任分配所造成的障碍。因此，作者提出了MAPRO来解决这些问题。", "method": "本文提出了一个名为MAPRO（Multi-Agent PRompt Optimization）的四阶段框架，该框架首先将多代理系统的提示优化问题建模为最大后验概率（MAP）推理问题，并利用语言引导的max-product信念传播算法求解。为了应对信用分配问题，并允许系统迭代更新，MAPRO引入了一种拓扑感知的精炼机制，通过集成执行反馈和下游责备来选择性地更新代理提示。", "result": "在各种任务的基准测试中，MAPRO达到了最先进的性能水平，持续超越手动设计的基线和最近的自动化替代方案。", "conclusion": "除了性能优势外，基于MAP的公式还为构建更可靠、更系统的未来多代理系统提供了通用指导。"}}
{"id": "2510.07600", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07600", "abs": "https://arxiv.org/abs/2510.07600", "authors": ["Pouya Shiri", "Ramin Sharifi", "Amirali Baniasadi"], "title": "Quick-CapsNet (QCN): A fast alternative to Capsule Networks", "comment": null, "summary": "The basic computational unit in Capsule Network (CapsNet) is a capsule (vs.\nneurons in Convolutional Neural Networks (CNNs)). A capsule is a set of\nneurons, which form a vector. CapsNet is used for supervised classification of\ndata and has achieved state-of-the-art accuracy on MNIST digit recognition\ndataset, outperforming conventional CNNs in detecting overlapping digits.\nMoreover, CapsNet shows higher robustness towards affine transformation when\ncompared to CNNs for MNIST datasets. One of the drawbacks of CapsNet, however,\nis slow training and testing. This can be a bottleneck for applications that\nrequire a fast network, especially during inference. In this work, we introduce\nQuick-CapsNet (QCN) as a fast alternative to CapsNet, which can be a starting\npoint to develop CapsNet for fast real-time applications. QCN builds on\nproducing a fewer number of capsules, which results in a faster network. QCN\nachieves this at the cost of marginal loss in accuracy. Inference is 5x faster\non MNIST, F-MNIST, SVHN and Cifar-10 datasets. We also further enhanced QCN by\nemploying a more powerful decoder instead of the default decoder to further\nimprove QCN.", "AI": {"tldr": "The paper presents Quick-CapsNet (QCN), a faster version of CapsNet that reduces the number of capsules but incurs a minor accuracy loss, achieving 5x faster inference on several datasets with an enhanced decoder for better performance.", "motivation": "The motivation behind the research is to address the slow training and testing issues of CapsNet, facilitating its use in applications requiring real-time performance.", "method": "The method introduces Quick-CapsNet (QCN), a variant of CapsNet that reduces the number of capsules to speed up the network while incurring a minor drop in accuracy. An enhanced decoder is also employed to further improve the performance of QCN.", "result": "QCN achieves a 5x faster inference on MNIST, F-MNIST, SVHN, and Cifar-10 datasets compared to the original CapsNet, with a slight decrease in accuracy.", "conclusion": "The paper concludes that QCN provides a balance between speed and accuracy, making it suitable for real-time applications where faster inference is desired."}}
{"id": "2510.07486", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07486", "abs": "https://arxiv.org/abs/2510.07486", "authors": ["Shuqing Luo", "Yilin Guan", "Pingzhi Li", "Hanrui Wang", "Tianlong Chen"], "title": "AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse Decoding", "comment": "14 pages, 17 figures", "summary": "Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT),\nbut the linear KV-cache growth amplifies the memory-bound bottleneck of LLM\ndecoding. Query-aware page-level sparse decoding can achieve state-of-the-art\nperformance under constrained FLOPs budgets, but is limited by both\nsequential-dependent page filtering and coarse-grained token selection,\nhampering serving efficiency and model performance on TTS tasks under high\nconcurrency and long CoT scenarios (consuming even higher runtime than the\nforward pipeline itself). In this paper, we first find that the current-step\nquery state can be accurately approximated in a unified manner from a short\nwindow of recent queries, enabling training-free query-aware sparsity without\nwaiting in the decoding loop. We propose AsyncSpade, an asynchronous framework\nfor efficient TTS built on two core components: (1) a novel light-weight\ntemporal-regressive module that predicts the next-token query state; (2) an\nasynchronous and disaggregated framework that decouples the KV cache filtering\nfrom the auto-regressive decoding loop, overlapping the token-level KV\nselection with the forward inference computation through asynchronism. To our\nknowledge, AsyncSpade is the first to eliminate the sequential dependence\nwithout sacrificing model performance. We validate the effectiveness of\nAsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade\nfully overlaps KV-cache operations with the inference pipeline, achieving\ntheoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade\ndelivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and\nat least 50% TPOT reduction compared to full attention on Qwen3-8B and\nQwen3-32B models, while matching or surpassing their accuracy on various TTS\nbenchmarks (AIME-24/25, GPQA-Diamond, MATH-500).", "AI": {"tldr": "本文提出AsyncSpade框架，通过异步计算和轻量级时间回归模块，在不牺牲模型性能的情况下提高解码效率，尤其适用于长链式思考和高并发场景，显著降低了TPOT。", "motivation": "解决LLM在长链式思考任务中因线性KV缓存增长与查询感知稀疏解码方法导致的内存瓶颈及高并发场景下的延迟问题。", "method": "提出了一种异步框架AsyncSpade，包含两个核心组件：一种轻量级的时间回归模块，用于预测下一词的查询状态；一个异步分层框架，用于解耦KV缓存过滤与自回归解码过程，通过异步机制实现令牌级KV选择与前向推理计算的重叠。", "result": "在A100节点上进行的实验结果显示，相比当前最佳基准（Quest）和完全注意力机制，AsyncSpade在Qwen3-8B和Qwen3-32B模型上TPOT分别降低了20%和至少50%，同时在各种测试时间缩放基准上保持或超过了其准确性。", "conclusion": "AsyncSpade通过消除序列依赖，首次实现了理论最优的TPOT，显著提升了长链推理和高并发场景下的解码效率，证明了其在LLM服务中的优越性。"}}
{"id": "2510.07631", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07631", "abs": "https://arxiv.org/abs/2510.07631", "authors": ["Shreshth Saini", "Shashank Gupta", "Alan C. Bovik"], "title": "Rectified-CFG++ for Flow Based Models", "comment": "Accepted at NeurIPS 2025", "summary": "Classifier-free guidance (CFG) is the workhorse for steering large diffusion\nmodels toward text-conditioned targets, yet its native application to rectified\nflow (RF) based models provokes severe off-manifold drift, yielding visual\nartifacts, text misalignment, and brittle behaviour. We present\nRectified-CFG++, an adaptive predictor-corrector guidance that couples the\ndeterministic efficiency of rectified flows with a geometry-aware conditioning\nrule. Each inference step first executes a conditional RF update that anchors\nthe sample near the learned transport path, then applies a weighted conditional\ncorrection that interpolates between conditional and unconditional velocity\nfields. We prove that the resulting velocity field is marginally consistent and\nthat its trajectories remain within a bounded tubular neighbourhood of the data\nmanifold, ensuring stability across a wide range of guidance strengths.\nExtensive experiments on large-scale text-to-image models (Flux, Stable\nDiffusion 3/3.5, Lumina) show that Rectified-CFG++ consistently outperforms\nstandard CFG on benchmark datasets such as MS-COCO, LAION-Aesthetic, and\nT2I-CompBench. Project page: https://rectified-cfgpp.github.io/", "AI": {"tldr": "本文介绍了一种改进的Rectified-CFG++方法，适用于文本条件的大规模扩散模型，在多个数据集上均显示出优于标准CFG的性能。", "motivation": "这项研究的动机在于解决分类器无关指导（CFG）在应用到校正流（RF）基础模型时会引发严重离流形漂移的问题，导致视觉伪影，文本对齐不良和脆弱行为。", "method": "我们提出了Rectified-CFG++, 一种自适应的预测-校正引导方法，结合了校正流的确定性效率和几何感知条件规则。每个推理步骤首先执行条件RF更新，将样本固定在学习的传输路径附近，然后应用加权的条件校正，插值条件和非条件速度场。", "result": "实验结果表明，Rectified-CFG++在大规模文本到图像模型上（如Flux，Stable Diffusion 3/3.5，Lumina）在基准数据集如MS-COCO，LAION-Aesthetic和T2I-CompBench上均优于标准的CFG。", "conclusion": "Rectified-CFG++通过确保速度场的边缘一致性，在数据流形的有界管状邻域内保持轨迹，从而在广泛的引导强度范围内保证了稳定性，并表现出比标准CFG更好的性能。"}}
{"id": "2510.07488", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07488", "abs": "https://arxiv.org/abs/2510.07488", "authors": ["Rasika Muralidharan", "Jaewoon Kwak", "Jisun An"], "title": "Can Lessons From Human Teams Be Applied to Multi-Agent Systems? The Role of Structure, Diversity, and Interaction Dynamics", "comment": "Under Review at ARR", "summary": "Multi-Agent Systems (MAS) with Large Language Model (LLM)-powered agents are\ngaining attention, yet fewer studies explore their team dynamics. Inspired by\nhuman team science, we propose a multi-agent framework to examine core aspects\nof team science: structure, diversity, and interaction dynamics. We evaluate\nteam performance across four tasks: CommonsenseQA, StrategyQA, Social IQa, and\nLatent Implicit Hate, spanning commonsense and social reasoning. Our results\nshow that flat teams tend to perform better than hierarchical ones, while\ndiversity has a nuanced impact. Interviews suggest agents are overconfident\nabout their team performance, yet post-task reflections reveal both\nappreciation for collaboration and challenges in integration, including limited\nconversational coordination.", "AI": {"tldr": "研究了大型语言模型驱动的多智能体系统的团队结构、多样性和互动，发现扁平化优于分层，多样性影响复杂。", "motivation": "研究大型语言模型驱动的多智能体系统的团队动态。", "method": "提出了一种基于人类团队科学的多智能体框架，用于研究团队科学的核心方面：结构、多样性以及互动动态。", "result": "实验结果表明，扁平化的团队比分层团队表现更好；多样性对团队有复杂的影响。", "conclusion": "智能体对团队表现过于自信，但对协作的挑战，包括有限的交流协调仍有一定认识。"}}
{"id": "2510.07636", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07636", "abs": "https://arxiv.org/abs/2510.07636", "authors": ["Shashank Gupta", "Gregoire Phillips", "Alan C. Bovik"], "title": "PIT-QMM: A Large Multimodal Model For No-Reference Point Cloud Quality Assessment", "comment": "Oral presentation at ICIP 2025", "summary": "Large Multimodal Models (LMMs) have recently enabled considerable advances in\nthe realm of image and video quality assessment, but this progress has yet to\nbe fully explored in the domain of 3D assets. We are interested in using these\nmodels to conduct No-Reference Point Cloud Quality Assessment (NR-PCQA), where\nthe aim is to automatically evaluate the perceptual quality of a point cloud in\nabsence of a reference. We begin with the observation that different modalities\nof data - text descriptions, 2D projections, and 3D point cloud views - provide\ncomplementary information about point cloud quality. We then construct PIT-QMM,\na novel LMM for NR-PCQA that is capable of consuming text, images and point\nclouds end-to-end to predict quality scores. Extensive experimentation shows\nthat our proposed method outperforms the state-of-the-art by significant\nmargins on popular benchmarks with fewer training iterations. We also\ndemonstrate that our framework enables distortion localization and\nidentification, which paves a new way forward for model explainability and\ninteractivity. Code and datasets are available at\nhttps://www.github.com/shngt/pit-qmm.", "AI": {"tldr": "A new method, PIT-QMM, uses text, images, and point clouds to assess the quality of 3D point clouds automatically without needing a reference image, outperforming current methods.", "motivation": "The motivation is to explore the potential of Large Multimodal Models in the domain of 3D assets, specifically for No-Reference Point Cloud Quality Assessment, an area where the progress enabled by such models hasn't been fully explored yet.", "method": "Content describes the development of PIT-QMM, a novel Large Multimodal Model for No-Reference Point Cloud Quality Assessment that utilizes text, 2D projections, and 3D point clouds to predict perceptual quality scores in absence of a reference.", "result": "The method outperformed the state-of-the-art on popular benchmarks with fewer training iterations. It also demonstrated the capability to enable distortion localization and identification.", "conclusion": "The conclusion is that PIT-QMM shows significant improvements in predicting the perceptual quality of point clouds without a reference, and opens new possibilities for model explainability and interactivity."}}
{"id": "2510.07497", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.07497", "abs": "https://arxiv.org/abs/2510.07497", "authors": ["Yi-Jen Shih", "Desh Raj", "Chunyang Wu", "Wei Zhou", "SK Bong", "Yashesh Gaur", "Jay Mahadeokar", "Ozlem Kalinli", "Mike Seltzer"], "title": "Can Speech LLMs Think while Listening?", "comment": null, "summary": "Recent advances in speech large language models (speech LLMs) have enabled\nseamless spoken interactions, but these systems still struggle with complex\nreasoning tasks. Previously, chain-of-thought (CoT) prompting or fine-tuning\nhas been to shown to significantly improve the reasoning abilities of\ntext-based LLMs. In this work, we investigate the effect of CoT fine-tuning for\nmulti-stream speech LLMs, demonstrating that reasoning in text space improves\nthe accuracy of speech LLMs by 2.4x, on average, over a suite of spoken\nreasoning tasks. Beyond accuracy, the latency of the spoken response is a\ncrucial factor for interacting with voice-based agents. Inspired by the human\nbehavior of \"thinking while listening,\" we propose methods to reduce the\nadditional latency from reasoning by allowing the model to start reasoning\nbefore the user query has ended. To achieve this, we introduce an entropy-based\nmetric, \"question completeness,\" which acts as an indicator to guide the model\non the optimal time to start reasoning. This method provides greater control\nover the accuracy-latency trade-off compared with heuristic-based approaches\nand, under equivalent latency conditions, yields a 4% accuracy gain on\nARC-Easy. Finally, we use Direct Preference Optimization (DPO) on preference\ndata created using rejection sampling to push the accuracy-latency pareto\nfrontier further, resulting in a 70% reduction in latency without loss in\naccuracy.", "AI": {"tldr": "The research shows that fine-tuning multi-stream speech LLMs with CoT increases their accuracy substantially and proposes a method to start reasoning before query completion, reducing latency without accuracy loss. DPO is applied to further optimize accuracy and latency.", "motivation": "The motivation behind this research is to address the challenge that speech LLMs face when it comes to handling complex reasoning tasks, aiming to improve their accuracy and reduce latency in spoken interactions.", "method": "In this study, CoT fine-tuning is applied to multi-stream speech LLMs to enhance their reasoning capabilities. An entropy-based 'question completeness' metric is introduced to allow the model to begin reasoning before the user's query is complete. Additionally, Direct Preference Optimization (DPO) on preference data generated via rejection sampling is used to optimize accuracy and latency trade-offs.", "result": "The study demonstrates that CoT fine-tuning increases the accuracy of speech LLMs by an average of 2.4x. The 'question completeness' metric achieves a 4% accuracy improvement without changing latency. By employing DPO, the study also manages to reduce latency by 70% while maintaining accuracy.", "conclusion": "The conclusion is that by applying CoT fine-tuning to speech LLMs and utilizing advanced techniques such as the 'question completeness' metric and DPO, there is a significant improvement in the accuracy and efficiency of these models in handling reasoning tasks."}}
{"id": "2510.07652", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07652", "abs": "https://arxiv.org/abs/2510.07652", "authors": ["Harshala Gammulle", "Clinton Fookes", "Sridha Sridharan", "Simon Denman"], "title": "Dual-Stream Alignment for Action Segmentation", "comment": "Journal Submission", "summary": "Action segmentation is a challenging yet active research area that involves\nidentifying when and where specific actions occur in continuous video streams.\nMost existing work has focused on single-stream approaches that model the\nspatio-temporal aspects of frame sequences. However, recent research has\nshifted toward two-stream methods that learn action-wise features to enhance\naction segmentation performance. In this work, we propose the Dual-Stream\nAlignment Network (DSA Net) and investigate the impact of incorporating a\nsecond stream of learned action features to guide segmentation by capturing\nboth action and action-transition cues. Communication between the two streams\nis facilitated by a Temporal Context (TC) block, which fuses complementary\ninformation using cross-attention and Quantum-based Action-Guided Modulation\n(Q-ActGM), enhancing the expressive power of the fused features. To the best of\nour knowledge, this is the first study to introduce a hybrid quantum-classical\nmachine learning framework for action segmentation. Our primary objective is\nfor the two streams (frame-wise and action-wise) to learn a shared feature\nspace through feature alignment. This is encouraged by the proposed Dual-Stream\nAlignment Loss, which comprises three components: relational consistency,\ncross-level contrastive, and cycle-consistency reconstruction losses. Following\nprior work, we evaluate DSA Net on several diverse benchmark datasets: GTEA,\nBreakfast, 50Salads, and EgoProcel. We further demonstrate the effectiveness of\neach component through extensive ablation studies. Notably, DSA Net achieves\nstate-of-the-art performance, significantly outperforming existing", "AI": {"tldr": "本文提出Dual-Stream Alignment Network (DSA Net)，结合帧流和动作流以提高动作分割表现。通过Temporal Context (TC)模块融合互补信息，提出Dual-Stream Alignment Loss，提升了动作和过渡动作的识别。在多个基准数据集上的评估中，DSA Net展现了优越性能，达到当前最佳水平。", "motivation": "尽管现有的动作分割方法大多集中在单流方法上，本文提出了一个双流方法，结合动作特征来增强动作分割效果，旨在通过帧流和动作流的协作提升分割性能。", "method": "Structure", "result": "{\n  \"tldr\": \"本文提出Dual-Stream Alignment Network (DSA Net)，结合帧流和动作流以提高动作分割表现。通过Temporal Context (TC)模块融合互补信息，提出Dual-Stream Alignment Loss，提升了动作和过渡动作的识别。在多个基准数据集上的评估中，DSA Net展现了优越性能，达到当前最佳水平。\", \n  \"motivation\": \"尽管现有的动作分割方法大多集中在单流方法上，本文提出了一个双流方法，结合动作特征来增强动作分割效果，旨在通过帧流和动作流的协作提升分割性能。\", \n  \"method\": \"本文提出采用Dual-Stream Alignment Network (DSA Net)，创新性地引入了Temporal Context (TC)块，使用跨注意力机制和量子动作导向调制(Q-ActGM)来融合流间的互补信息。同时还提出了Dual-Stream Alignment Loss，包含关联一致性、跨层对比、循环一致性重构损失三个部分。\", \n  \"result\": \"在GTEA、Breakfast、50Salads和EgoProcel等多样化的基准数据集上进行了评估，展示了模型的有效性和每个组件的作用。实验结果显示，DSA Net达到了状态-of-the-art的表现，优于现有的方法。\", \n  \"conclusion\": \"本文提出的双流网络框架通过融合帧和动作信息，不仅提高了动作分割性能，还在多个公共基准数据集上达到了当前最佳水平，同时推动了量子-经典混合机器学习框架在动作分割中的应用。\"]}", "conclusion": "本文提出的双流网络框架通过融合帧和动作信息，不仅提高了动作分割性能，还在多个公共基准数据集上达到了当前最佳水平，同时推动了量子-经典混合机器学习框架在动作分割中的应用。"}}
{"id": "2510.07499", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07499", "abs": "https://arxiv.org/abs/2510.07499", "authors": ["Soyeong Jeong", "Taehee Jung", "Sung Ju Hwang", "Joo-Kyung Kim", "Dongyeop Kang"], "title": "When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs", "comment": null, "summary": "Recent Long-Context Language Models (LCLMs) can process hundreds of thousands\nof tokens in a single prompt, enabling new opportunities for\nknowledge-intensive multi-hop reasoning by integrating large sets of retrieved\ndocuments or, in some cases, directly all necessary information. However,\nsimply feeding more documents into the context window fails to capture how\nevidence should be connected. We address this gap with thought templates, which\nrecast reasoning as reusable thought caches, derived from prior problem solving\ntraces, structuring how evidence is combined and guiding multi-hop inference\nwith factual documents. To keep these templates effective, we propose an update\nstrategy that iteratively refines templates derived from training data through\nnatural-language feedback. Across diverse benchmarks and LCLM families, our\napproach delivers consistent gains over strong baselines in both\nretrieval-based and retrieval-free settings. Furthermore, we show that\noptimized templates can be distilled into smaller open-source models,\ndemonstrating its broad applicability and transparent reasoning reuse. We refer\nto our framework as Thought Template Augmented LCLMs (ToTAL).", "AI": {"tldr": "本文介绍了思想模板增强的长上下文语言模型框架（ToTAL），该方法通过思想模板和迭代更新策略提高模型的推理能力。", "motivation": "近年来，长上下文语言模型尽管可以处理大量的标记，但仅仅增加文档并不能解决证据如何连接的问题。为此，提出了思想模板来解决这一问题。", "method": "使用思想模板将推理重新表述为可重用的思想缓存，这些缓存源自先前的问题解决轨迹，并提出了一种迭代更新策略以通过自然语言反馈优化这些模板。", "result": "该方法在各种基准测试和LCLM系列中，相比强基线方法在基于检索和无检索设置下都取得了显著提高。", "conclusion": "证明了经过优化的思想模板可以被提炼进更小型的开源模型中，展示了其广泛适用性和透明推理复用。"}}
{"id": "2510.07654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07654", "abs": "https://arxiv.org/abs/2510.07654", "authors": ["Yanjie Pan", "Qingdong He", "Lidong Wang", "Bo Peng", "Mingmin Chi"], "title": "Once Is Enough: Lightweight DiT-Based Video Virtual Try-On via One-Time Garment Appearance Injection", "comment": "5 pages (including references), 4 figures. Code and models will be\n  released upon publication", "summary": "Video virtual try-on aims to replace the clothing of a person in a video with\na target garment. Current dual-branch architectures have achieved significant\nsuccess in diffusion models based on the U-Net; however, adapting them to\ndiffusion models built upon the Diffusion Transformer remains challenging.\nInitially, introducing latent space features from the garment reference branch\nrequires adding or modifying the backbone network, leading to a large number of\ntrainable parameters. Subsequently, the latent space features of garments lack\ninherent temporal characteristics and thus require additional learning. To\naddress these challenges, we propose a novel approach, OIE (Once is Enough), a\nvirtual try-on strategy based on first-frame clothing replacement:\nspecifically, we employ an image-based clothing transfer model to replace the\nclothing in the initial frame, and then, under the content control of the\nedited first frame, utilize pose and mask information to guide the temporal\nprior of the video generation model in synthesizing the remaining frames\nsequentially. Experiments show that our method achieves superior parameter\nefficiency and computational efficiency while still maintaining leading\nperformance under these constraints.", "AI": {"tldr": "该研究提出了一种新的视频虚拟试穿方法OIE，通过第一帧衣物替换和利用姿态及掩码信息顺序生成后续帧，解决了现有方法参数量大和缺乏时间特性的问题。", "motivation": "现有的双分支架构基于U-Net的扩散模型已经取得了显著的成功，但将其适应到基于扩散Transformer的模型中仍具有挑战性。主要是因为引入来自衣物参考分支的潜在空间特征需要添加或修改骨干网络，导致训练参数量大增。此外，衣物的潜在空间特征缺乏内在的时间特性，这也需要额外的学习。", "method": "该研究提出了一种名为OIE（一次即足够）的新方法，这是一种基于第一帧衣物替换的虚拟试穿策略。具体来说，使用基于图像的衣物转移模型替换了初始帧的衣物，然后在编辑过的初始帧的内容控制下，利用姿态和掩码信息引导视频生成模型的时间先验，顺序合成其余帧。", "result": "实验表明，该方法在参数效率和计算效率方面表现出色，同时仍能保持高约束条件下的领先性能。", "conclusion": "该研究提出的方法在保持高性能的同时，显著提高了参数和计算效率，为视频虚拟试衣提供了一个有前景的解决方案。"}}
{"id": "2510.07520", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07520", "abs": "https://arxiv.org/abs/2510.07520", "authors": ["Rayyan Merchant", "Kevin Tang"], "title": "ParsTranslit: Truly Versatile Tajik-Farsi Transliteration", "comment": null, "summary": "As a digraphic language, the Persian language utilizes two written standards:\nPerso-Arabic in Afghanistan and Iran, and Tajik-Cyrillic in Tajikistan. Despite\nthe significant similarity between the dialects of each country, script\ndifferences prevent simple one-to-one mapping, hindering written communication\nand interaction between Tajikistan and its Persian-speaking ``siblings''. To\novercome this, previously-published efforts have investigated machine\ntransliteration models to convert between the two scripts. Unfortunately, most\nefforts did not use datasets other than those they created, limiting these\nmodels to certain domains of text such as archaic poetry or word lists. A truly\nusable transliteration system must be capable of handling varied domains,\nmeaning that suck models lack the versatility required for real-world usage.\nThe contrast in domain between data also obscures the task's true difficulty.\nWe present a new state-of-the-art sequence-to-sequence model for Tajik-Farsi\ntransliteration trained across all available datasets, and present two datasets\nof our own. Our results across domains provide clearer understanding of the\ntask, and set comprehensive comparable leading benchmarks. Overall, our model\nachieves chrF++ and Normalized CER scores of 87.91 and 0.05 from Farsi to Tajik\nand 92.28 and 0.04 from Tajik to Farsi. Our model, data, and code are available\nat https://anonymous.4open.science/r/ParsTranslit-FB30/.", "AI": {"tldr": "本文在一个新的更大范围的数据集上，提出了一个先进的塔吉克-波斯语转写模型，避免了先前模型的数据局限性，提高了实际应用中的灵活性和性能。", "motivation": "鉴于之前的努力未能使用其他数据集限制了模型仅适用于某些领域的文本，本文旨在解决这一问题，提出一个更通用的转写系统。", "method": "本文提出了一种最新状态的序列到序列模型，用于塔吉克语-波斯语的转写，并训练了所有可用的数据集。此外，还提供了两个自己的数据集。", "result": "本文模型在Farsi到Tajik和Tajik到Farsi的转写中分别达到了chrF++和Normalised CER得分为87.91和0.05，92.28和0.04。", "conclusion": "研究结果提供了跨领域任务的清晰理解，设定了全面的领先基准。"}}
{"id": "2510.07656", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07656", "abs": "https://arxiv.org/abs/2510.07656", "authors": ["James Baker"], "title": "MONKEY: Masking ON KEY-Value Activation Adapter for Personalization", "comment": null, "summary": "Personalizing diffusion models allows users to generate new images that\nincorporate a given subject, allowing more control than a text prompt. These\nmodels often suffer somewhat when they end up just recreating the subject\nimage, and ignoring the text prompt. We observe that one popular method for\npersonalization, the IP-Adapter automatically generates masks that we\ndefinitively segment the subject from the background during inference. We\npropose to use this automatically generated mask on a second pass to mask the\nimage tokens, thus restricting them to the subject, not the background,\nallowing the text prompt to attend to the rest of the image. For text prompts\ndescribing locations and places, this produces images that accurately depict\nthe subject while definitively matching the prompt. We compare our method to a\nfew other test time personalization methods, and find our method displays high\nprompt and source image alignment.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.07535", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07535", "abs": "https://arxiv.org/abs/2510.07535", "authors": ["Jaeseong Lee", "seung-won hwang", "Aurick Qiao", "Gabriele Oliaro", "Ye Wang", "Samyam Rajbhandari"], "title": "OWL: Overcoming Window Length-Dependence in Speculative Decoding for Long-Context Inputs", "comment": null, "summary": "Speculative decoding promises faster inference for large language models\n(LLMs), yet existing methods fail to generalize to real-world settings.\nBenchmarks typically assume short contexts (e.g., 2K tokens), whereas practical\nworkloads involve long contexts. We find current approaches degrade severely\nwith long contexts; for instance, EAGLE3 even slows down the generation speed\nby 0.81x. We address these limitations by releasing a new long-context\nbenchmark (LongSpecBench) and introducing a novel model (OWL). OWL achieves\nabout 5x higher acceptance length than EAGLE3 on long-context inputs through\nthree innovations: (1) an LSTM-based drafter conditioned only on the last-token\nstate, making it generalize to various lengths, (2) a special token [SPEC] in\nthe verifier that produces richer representation for drafter, and (3) a hybrid\nalgorithm combining both tree and non-tree decoding methods. We release all\ncode and datasets to advance future research.", "AI": {"tldr": "论文提出了一种新的长上下文推测解码方法，解决了现有方法在长上下文任务中的性能退化问题。", "motivation": "当前推测解码方法在长上下文任务中效果不佳，此论文希望通过引入新的长上下文基准（LongSpecBench）和OWL模型来解决这一问题。", "method": "该论文引入了OWL模型并通过三个创新点来改进长上下文推测解码性能：1) 一个基于LSTM的起草者，仅依赖于最后一个标记的状态，使其能够适应各种长度；2) 在验证者中使用特殊标记[SPEC]以产生更丰富的表示形式；3) 结合树状和非树状解码方法的混合算法。", "result": "OWL模型在长上下文输入下实现了比EAGLE3高出约5倍的接受长度。", "conclusion": "通过引入新的长上下文基准（LongSpecBench）和OWL模型，实现了在长上下文任务中的推测解码性能提升，有助于推动未来的研究方向。"}}
{"id": "2510.07665", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07665", "abs": "https://arxiv.org/abs/2510.07665", "authors": ["Jun Muraoka", "Daichi Haraguchi", "Naoto Inoue", "Wataru Shimoda", "Kota Yamaguchi", "Seiichi Uchida"], "title": "Automatic Text Box Placement for Supporting Typographic Design", "comment": null, "summary": "In layout design for advertisements and web pages, balancing visual appeal\nand communication efficiency is crucial. This study examines automated text box\nplacement in incomplete layouts, comparing a standard Transformer-based method,\na small Vision and Language Model (Phi3.5-vision), a large pretrained VLM\n(Gemini), and an extended Transformer that processes multiple images.\nEvaluations on the Crello dataset show the standard Transformer-based models\ngenerally outperform VLM-based approaches, particularly when incorporating\nricher appearance information. However, all methods face challenges with very\nsmall text or densely populated layouts. These findings highlight the benefits\nof task-specific architectures and suggest avenues for further improvement in\nautomated layout design.", "AI": {"tldr": "研究对比了不同模型在不完整布局中的文本框自动放置效果，标准Transformer表现最优，但所有模型在处理小型文本和密集布局时均有局限。", "motivation": "在广告和网页设计的布局设计中，平衡视觉吸引力和沟通效率至关重要。", "method": "本研究比较了标准的Transformer模型、小型视觉语言模型(Phi3.5-vision)、大型预训练视觉语言模型(Gemini)以及扩展的处理多图像的Transformer模型，在不完整布局设计中自动放置文本框的效果。", "result": "在Crello数据集上的评估显示，标准的Transformer模型通常优于视觉语言模型方法，尤其是在包含更丰富外观信息的情况下。然而，所有方法在处理非常小的文本或充满元素的布局时都遇到了挑战。", "conclusion": "这些发现强调了任务特定架构的优势，并且为自动化布局设计的进一步改进指明了方向。"}}
{"id": "2510.07545", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07545", "abs": "https://arxiv.org/abs/2510.07545", "authors": ["Md Tahmid Rahman Laskar", "Mohammed Saidul Islam", "Ridwan Mahbub", "Mizanur Rahman", "Amran Bhuiyan", "Israt Jahan", "Mir Tafseer Nayeem", "Shafiq Joty", "Enamul Hoque", "Jimmy Huang"], "title": "Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models: Lessons Learned and Best Practices", "comment": "Accepted to the EMNLP 2025 Industry Track", "summary": "Large Vision-Language Models (LVLMs) with only 7B parameters have shown\npromise as automated judges in chart comprehension tasks. However, tiny models\n(<=2B parameters) still perform poorly as judges, limiting their real-world use\nin resource-constrained settings. To address this, we propose two approaches to\nensure cost-efficient evaluation: (i) multi-criteria prompting, which combines\nseparate evaluation criteria into a single query, and (ii) domain-adaptive\ntransfer learning, in which we fine-tune a 2B-parameter LVLM on synthetic\njudgments in a chart dataset to create the ChartJudge. Experiments show that\nmulti-criteria prompting exposes robustness gaps, which led to a huge drop in\nperformance for 7B models, including specialized LVLM judges like LLaVA-Critic.\nIn addition, we find that our tiny LVLM (ChartJudge) can effectively transfer\nknowledge from one dataset to another to make it a more specialized model. Our\nfine-grained analysis across chart types and query complexities offers\nactionable insights into trade-offs between model size, prompt design, and\ntransferability, enabling scalable, low-cost evaluation for chart reasoning\ntasks. Our code and the data will be made publicly available.", "AI": {"tldr": "本文讨论了如何通过多标准提示技术和领域自适应迁移学习来改善小规模视觉语言模型在图表理解任务中的性能，以实现在资源受限环境下的有效应用。", "motivation": "尽管具有70亿参数的大规模视觉语言模型（LVLM）在图表理解任务中表现出了希望，但小规模模型（参数小于等于20亿）的表现仍然不佳。这限制了在资源受限环境中它们的真实世界使用。为了应对这一问题，研究人员希望通过减少模型大小同时提高系统性能来解决这一问题。", "method": "本文提出两种方法来确保评估的成本效益：多标准提示和领域自适应迁移学习。多标准提示将不同的评估标准合并为单个查询。领域自适应迁移学习则是在一个图表数据集上的合成评判数据上微调一个20亿参数的LVLM，以创建ChartJudge模型。", "result": "实验显示，多标准提示暴露了结构上的脆弱点，导致7B模型（包括专门的LVLM评判者如LLaVA-Critic）的性能大幅下降。此外，研究表明，小规模LVLM（ChartJudge）可以从一个数据集转移到另一个数据集，使其成为一个更专业的模型。", "conclusion": "通过对图表类型和查询复杂度的细粒度分析，提供了关于模型大小、提示设计和可移植性之间权衡的实用性见解，这使得图表推理任务的大规模、低成本评估成为可能。"}}
{"id": "2510.07666", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07666", "abs": "https://arxiv.org/abs/2510.07666", "authors": ["Heming Wu", "Di Wang", "Tai Ma", "Peng Zhao", "Yubin Xiao", "Zhongke Wu", "Xing-Ce Wang", "Chuang Li", "Xuan Wu", "You Zhou"], "title": "TCIP: Threshold-Controlled Iterative Pyramid Network for Deformable Medical Image Registration", "comment": null, "summary": "Although pyramid networks have demonstrated superior performance in\ndeformable medical image registration, their decoder architectures are\ninherently prone to propagating and accumulating anatomical structure\nmisalignments. Moreover, most existing models do not adaptively determine the\nnumber of iterations for optimization under varying deformation requirements\nacross images, resulting in either premature termination or excessive\niterations that degrades registration accuracy. To effectively mitigate the\naccumulation of anatomical misalignments, we propose the Feature-Enhanced\nResidual Module (FERM) as the core component of each decoding layer in the\npyramid network. FERM comprises three sequential blocks that extract anatomical\nsemantic features, learn to suppress irrelevant features, and estimate the\nfinal deformation field, respectively. To adaptively determine the number of\niterations for varying images, we propose the dual-stage Threshold-Controlled\nIterative (TCI) strategy. In the first stage, TCI assesses registration\nstability and with asserted stability, it continues with the second stage to\nevaluate convergence. We coin the model that integrates FERM and TCI as\nThreshold-Controlled Iterative Pyramid (TCIP). Extensive experiments on three\npublic brain MRI datasets and one abdomen CT dataset demonstrate that TCIP\noutperforms the state-of-the-art (SOTA) registration networks in terms of\naccuracy, while maintaining comparable inference speed and a compact model\nparameter size. Finally, we assess the generalizability of FERM and TCI by\nintegrating them with existing registration networks and further conduct\nablation studies to validate the effectiveness of these two proposed methods.", "AI": {"tldr": "本文提出TCIP模型，结合FERM和TCI策略，有效减少了解剖结构对齐误差，自适应迭代次数，提升了图像配准的精度。", "motivation": "现有的金字塔网络在医学图像配准中表现出色，但其解码器结构容易积累解剖结构误差，并且无法根据不同的图像自适应确定迭代次数，这会导致过早终止或不必要的迭代，降低配准精度。", "method": "本文提出Feature-Enhanced Residual Module (FERM) 和双阶段阈值控制迭代（TCI）策略，用于解决现有金字塔网络解码器结构容易积累解剖结构对齐误差以及无法自适应地确定迭代次数的问题。", "result": "在三个公共的脑MRI数据集和一个腹部CT数据集上的大量实验表明，TCIP在精度上超越了现有的最先进的注册网络，同时保持了相当的推理速度和紧凑的模型参数大小。", "conclusion": "实验结果表明，TCIP在医学图像配准中表现优越，功能增强残差模块和双阶段阈值控制迭代策略有效提高了对齐精度。"}}
{"id": "2510.07566", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07566", "abs": "https://arxiv.org/abs/2510.07566", "authors": ["Junyi Zhu", "Savas Ozkan", "Andrea Maracani", "Sinan Mutlu", "Cho Jung Min", "Mete Ozay"], "title": "Multi-Task Pre-Finetuning of Lightweight Transformer Encoders for Text Classification and NER", "comment": "Accepted by EMNLP 2025 Industry Track", "summary": "Deploying natural language processing (NLP) models on mobile platforms\nrequires models that can adapt across diverse applications while remaining\nefficient in memory and computation. We investigate pre-finetuning strategies\nto enhance the adaptability of lightweight BERT-like encoders for two\nfundamental NLP task families: named entity recognition (NER) and text\nclassification. While pre-finetuning improves downstream performance for each\ntask family individually, we find that na\\\"ive multi-task pre-finetuning\nintroduces conflicting optimization signals that degrade overall performance.\nTo address this, we propose a simple yet effective multi-task pre-finetuning\nframework based on task-primary LoRA modules, which enables a single shared\nencoder backbone with modular adapters. Our approach achieves performance\ncomparable to individual pre-finetuning while meeting practical deployment\nconstraint. Experiments on 21 downstream tasks show average improvements of\n+0.8% for NER and +8.8% for text classification, demonstrating the\neffectiveness of our method for versatile mobile NLP applications.", "AI": {"tldr": "本文提出了一种新的多任务预微调框架，用于优化轻量级BERT样编码器在移动设备上的NLP应用，该框架解决了优化冲突问题，并在NER和文本分类任务上取得了显著效果。", "motivation": "研究动机是探索策略，以提高轻量级BERT样编码器在跨多种应用的同时保持低内存和计算效率的能力，特别是在命名实体识别（NER）和文本分类两个基本NLP任务家族中。", "method": "本研究提出了一种基于任务主导LoRA模块的多任务预微调框架，以解决传统多任务预微调中的优化冲突问题，从而实现在单一共享编码器基础上的模块化适配器。", "result": "实验结果表明，在21个下游任务上，NER任务平均改善了+0.8%，文本分类任务平均改善了+8.8%，显示了该方法对移动NLP应用的有效性。", "conclusion": "研究结论是，所提出的方法不仅达到了独立预微调的性能，还满足了实际部署的约束，有效地提高了移动平台上的NLP模型适应性。"}}
{"id": "2510.07670", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07670", "abs": "https://arxiv.org/abs/2510.07670", "authors": ["Haoyi Duan", "Yunzhi Zhang", "Yilun Du", "Jiajun Wu"], "title": "Controllable Video Synthesis via Variational Inference", "comment": "Project page: https://video-synthesis-variational.github.io/", "summary": "Many video workflows benefit from a mixture of user controls with varying\ngranularity, from exact 4D object trajectories and camera paths to coarse text\nprompts, while existing video generative models are typically trained for fixed\ninput formats. We develop a video synthesis method that addresses this need and\ngenerates samples with high controllability for specified elements while\nmaintaining diversity for under-specified ones. We cast the task as variational\ninference to approximate a composed distribution, leveraging multiple video\ngeneration backbones to account for all task constraints collectively. To\naddress the optimization challenge, we break down the problem into step-wise KL\ndivergence minimization over an annealed sequence of distributions, and further\npropose a context-conditioned factorization technique that reduces modes in the\nsolution space to circumvent local optima. Experiments suggest that our method\nproduces samples with improved controllability, diversity, and 3D consistency\ncompared to prior works.", "AI": {"tldr": "本文提出了一种视频合成方法，通过变分推理，使用多个生成模型共同完成任务，以提供高度可控的生成同时保持样本的多样性，实验显示该方法优于先前的工作。", "motivation": "许多视频工作流程受益于混合用户控制的使用，控制的粒度从精确的4D对象轨迹和相机路径到粗略的文字提示不等，而现有的视频生成模型通常针对固定输入格式进行训练。我们开发的方法解决了这一需求，为指定元素提供高度控制的同时保持对未指定元素的多样性。", "method": "我们开发了一种将任务视为变分推断的视频合成方法，通过使用多个视频生成模型来共同满足所有任务约束。为了应对优化挑战，我们将问题分解为通过逐步最小化KL散度来优化一系列退火分布，并提出了一种上下文条件因子化技术，以减少解空间中的模式，从而避免局部最优解。", "result": "实验表明，与以前的工作相比，我们的方法生成的样本具有改进的可控性，多样性以及3D一致性。", "conclusion": "本研究提出的方法在可控性，多样性和3D一致性方面都优于先前工作。此方法展示了在视频合成领域结合深入的用户控制和多样化表现的可能性。"}}
{"id": "2510.07579", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07579", "abs": "https://arxiv.org/abs/2510.07579", "authors": ["Mkululi Sikosana", "Sean Maudsley-Barton", "Oluwaseun Ajao"], "title": "Linguistic Patterns in Pandemic-Related Content: A Comparative Analysis of COVID-19, Constraint, and Monkeypox Datasets", "comment": "16 pages", "summary": "This study conducts a computational linguistic analysis of pandemic-related\nonline discourse to examine how language distinguishes health misinformation\nfrom factual communication. Drawing on three corpora: COVID-19 false narratives\n(n = 7588), general COVID-19 content (n = 10700), and Monkeypox-related posts\n(n = 5787), we identify significant differences in readability, rhetorical\nmarkers, and persuasive language use. COVID-19 misinformation exhibited\nmarkedly lower readability scores and contained over twice the frequency of\nfear-related or persuasive terms compared to the other datasets. It also showed\nminimal use of exclamation marks, contrasting with the more emotive style of\nMonkeypox content. These patterns suggest that misinformation employs a\ndeliberately complex rhetorical style embedded with emotional cues, a\ncombination that may enhance its perceived credibility. Our findings contribute\nto the growing body of work on digital health misinformation by highlighting\nlinguistic indicators that may aid detection efforts. They also inform public\nhealth messaging strategies and theoretical models of crisis communication in\nnetworked media environments. At the same time, the study acknowledges\nlimitations, including reliance on traditional readability indices, use of a\ndeliberately narrow persuasive lexicon, and reliance on static aggregate\nanalysis. Future research should therefore incorporate longitudinal designs,\nbroader emotion lexicons, and platform-sensitive approaches to strengthen\nrobustness.", "AI": {"tldr": "本研究通过计算语言学分析明确了与新冠疫情相关的误导信息和正确信息之间在语言使用上的区别，尤其是可读性和情感化语言的使用。", "motivation": "研究动机在于通过分析不同类型的新冠疫情相关内容的语言特征，揭示健康误导信息与事实性信息之间的差异，以期为识别新冠疫情中的误导性信息提供语言学依据，并对公共卫生信息传播策略和网络媒体环境下的危机沟通理论模型提供理论支持。", "method": "本研究利用计算语言学分析了与疫情相关的在线话语，以探讨语言如何将健康误导信息与事实信息区分开来。研究基于三个语料库：7588条关于新冠疫情的错误叙述、10700条一般新冠疫情内容和5787条关于猴痘的帖子，识别了可读性、修辞标记和说服性语言使用的显著差异。", "result": "研究结果表明，新冠疫情的误导信息可读性得分明显较低，并且包含恐惧相关或说服性词汇的频率是其他数据集的两倍多。误导性信息极少使用感叹号，与具有更多情感化风格的猴痘内容形成对比。", "conclusion": "研究认为，误导信息倾向于使用带有情感线索的复杂修辞风格，这种组合可能增强了其被感知的可信度。同时研究也承认存在一些局限性，包括对传统可读性指数的依赖和对说服性词汇的狭隘选择，这促使未来的研究采用更纵向的设计和更广泛的感性词汇以及基于平台的方法来提升研究的准确性。"}}
{"id": "2510.07692", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07692", "abs": "https://arxiv.org/abs/2510.07692", "authors": ["Tangin Amir Smrity", "MD Zahin Muntaqim Hasan Muhammad Kafi", "Abu Saleh Musa Miah", "Najmul Hassan", "Yuichi Okuyama", "Nobuyoshi Asai", "Taro Suzuki", "Jungpil Shin"], "title": "Hybrid CNN-BYOL Approach for Fault Detection in Induction Motors Using Thermal Images", "comment": null, "summary": "Induction motors (IMs) are indispensable in industrial and daily life, but\nthey are susceptible to various faults that can lead to overheating, wasted\nenergy consumption, and service failure. Early detection of faults is essential\nto protect the motor and prolong its lifespan. This paper presents a hybrid\nmethod that integrates BYOL with CNNs for classifying thermal images of\ninduction motors for fault detection. The thermal dataset used in this work\nincludes different operating states of the motor, such as normal operation,\noverload, and faults. We employed multiple deep learning (DL) models for the\nBYOL technique, ranging from popular architectures such as ResNet-50,\nDenseNet-121, DenseNet-169, EfficientNetB0, VGG16, and MobileNetV2.\nAdditionally, we introduced a new high-performance yet lightweight CNN model\nnamed BYOL-IMNet, which comprises four custom-designed blocks tailored for\nfault classification in thermal images. Our experimental results demonstrate\nthat the proposed BYOL-IMNet achieves 99.89\\% test accuracy and an inference\ntime of 5.7 ms per image, outperforming state-of-the-art models. This study\nhighlights the promising performance of the CNN-BYOL hybrid method in enhancing\naccuracy for detecting faults in induction motors, offering a robust\nmethodology for online monitoring in industrial settings.", "AI": {"tldr": "A hybrid BYOL-CNN method is proposed to detect faults in induction motors by classifying thermal images, with the newly introduced BYOL-IMNet achieving 99.89% accuracy and a rapid inference time, surpassing existing models.", "motivation": "The purpose of this study is to develop a method that can accurately and swiftly detect faults in induction motors to prevent overheating, wasted energy, and service failure, thereby extending the motor's lifespan and ensuring safety.", "method": "This paper presents a hybrid method that integrates Bootstrap Your Own Latent (BYOL) with Convolutional Neural Networks (CNNs) to classify thermal images of induction motors for fault detection. It explores multiple deep learning models for the BYOL technique, including ResNet-50, DenseNet-121, DenseNet-169, EfficientNetB0, VGG16, and MobileNetV2. Additionally, it introduces a new CNN model, BYOL-IMNet, tailored for fault classification in thermal images with high performance and light weight.", "result": "Experiments show that BYOL-IMNet achieves a remarkable test accuracy of 99.89% with an inference speed of 5.7 ms per image, outperforming current state-of-the-art models.", "conclusion": "The proposed BYOL-IMNet hybrid method exhibits exceptional performance in fault detection for induction motors by combining BYOL and CNNs. It offers a robust solution for online motor condition monitoring in industrial environments."}}
{"id": "2510.07591", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07591", "abs": "https://arxiv.org/abs/2510.07591", "authors": ["Chihiro Taguchi", "Richard Sproat"], "title": "IASC: Interactive Agentic System for ConLangs", "comment": "Initial draft", "summary": "We present a system that uses LLMs as a tool in the development of\nConstructed Languages. The system is modular in that one first creates a target\nphonology for the language using an agentic approach that refines its output at\neach step with commentary feedback on its previous attempt. Next, a set of\nsentences is 'translated' from their English original into a morphosyntactic\nmarkup that reflects the word order and morphosyntactic feature specifications\nof the desired target language, with affixes represented as morphosyntactic\nfeature bundles. From this translated corpus, a lexicon is constructed using\nthe phonological model and the set of morphemes (stems and affixes) extracted\nfrom the 'translated' sentences. The system is then instructed to provide an\northography for the language, using an existing script such as Latin or\nCyrillic. Finally, the system writes a brief grammatical handbook of the\nlanguage. The system can also translate further sentences into the target\nlanguage.\n  Our goal is twofold. First, we hope that these tools will be fun to use for\ncreating artificially constructed languages. Second, we are interested in\nexploring what LLMs 'know' about language-not what they know about any\nparticular language or linguistic phenomenon, but how much they know about and\nunderstand language and linguistic concepts. As we shall see, there is a fairly\nwide gulf in capabilities both among different LLMs and among different\nlinguistic specifications, with it being notably easier for systems to deal\nwith more common patterns than rarer ones. An additional avenue that we explore\nis the application of our approach to translating from high-resource into\nlow-resource languages. While the results so far are mostly negative, we\nprovide some evidence that an improved version of the present system could\nafford some real gains in such tasks.\n  https://github.com/SakanaAI/IASC", "AI": {"tldr": "使用LLM开发的构造语系统展示了开发人造语言的潜力，并展示了LLM对语言理解的能力以及这种系统在未来可能在语言翻译任务中的应用前景。", "motivation": "研究的动机是通过该系统创建人造语言应具有娱乐性，并探索LLM对语言的理解，而非针对特定语言或语言现象的认知。此外，系统还可用于从高资源语言到低资源语言的翻译。", "method": "该系统使用大型语言模型（LLM）作为辅助工具开发构造语。系统是模块化的，首先通过代理方法为语言创建目标音系，该方法在每一步中通过反馈修改其输出。接下来，将英语句子‘翻译’为形态句法标记，然后构建一个词汇表，最后创建正字法并编写简要语法手册。", "result": "研究结果展示了不同LLM在不同语言规格上的能力差异，对常见模式的处理比对罕见模式的处理要容易得多。此外，虽然目前从高资源到低资源语言的翻译结果大多负面，但研究也提供了证据表明改进后的系统可能带来实质性改进。", "conclusion": "系统展示了LLM在语言能力上的瓶颈，同时表明这种方法在未来改进后可能对语言翻译任务有帮助，特别是对于高资源语言到低资源语言的翻译。"}}
{"id": "2510.07703", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07703", "abs": "https://arxiv.org/abs/2510.07703", "authors": ["Xiaoxu Ma", "Runhao Li", "Zhenyu Weng"], "title": "Mutual Learning for Hashing: Unlocking Strong Hash Functions from Weak Supervision", "comment": null, "summary": "Deep hashing has been widely adopted for large-scale image retrieval, with\nnumerous strategies proposed to optimize hash function learning. Pairwise-based\nmethods are effective in learning hash functions that preserve local similarity\nrelationships, whereas center-based methods typically achieve superior\nperformance by more effectively capturing global data distributions. However,\nthe strength of center-based methods in modeling global structures often comes\nat the expense of underutilizing important local similarity information. To\naddress this limitation, we propose Mutual Learning for Hashing (MLH), a novel\nweak-to-strong framework that enhances a center-based hashing branch by\ntransferring knowledge from a weaker pairwise-based branch. MLH consists of two\nbranches: a strong center-based branch and a weaker pairwise-based branch.\nThrough an iterative mutual learning process, the center-based branch leverages\nlocal similarity cues learned by the pairwise-based branch. Furthermore,\ninspired by the mixture-of-experts paradigm, we introduce a novel\nmixture-of-hash-experts module that enables effective cross-branch interaction,\nfurther enhancing the performance of both branches. Extensive experiments\ndemonstrate that MLH consistently outperforms state-of-the-art hashing methods\nacross multiple benchmark datasets.", "AI": {"tldr": "该论文提出了一个名为MLH的框架，通过互相学习和跨分支交互的方法，解决了基于中心方法在学习哈希函数时局部相似性信息利用不足的问题，在多个基准数据集上表现优于最先进的哈希方法。", "motivation": "该论文本旨在解决基于中心的方法在建模全局结构时往往未能充分利用重要局部相似性信息的问题，提出了一种名为互学习哈希（MLH）的新框架。", "method": "该论文提出了一种名为互学习哈希（MLH）的新框架，包含一个强的基于中心的分支和一个较弱的基于成对的分支。通过互相学习来增强基于中心分支的局部相似性。同时还引入了一种名为哈希专家混合模块的新模块，以增强跨分支的有效交互。", "result": "{", "conclusion": "该论文得出结论，MLH方法在多个基准数据集上能超越其他最先进的哈希方法。这个框架通过引入哈希专家混合模块，增强了跨分支的有效交互和学习，从而改善了局部相似性的利用并提高了整体性能。"}}
{"id": "2510.07613", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07613", "abs": "https://arxiv.org/abs/2510.07613", "authors": ["Isabel Papadimitriou", "Jacob Prince"], "title": "Vocabulary embeddings organize linguistic structure early in language model training", "comment": null, "summary": "Large language models (LLMs) work by manipulating the geometry of input\nembedding vectors over multiple layers. Here, we ask: how are the input\nvocabulary representations of language models structured, and how and when does\nthis structure evolve over training? To answer this question, we use\nrepresentational similarity analysis, running a suite of experiments that\ncorrelate the geometric structure of the input embeddings and output embeddings\nof two open-source models (Pythia 12B and OLMo 7B) with semantic, syntactic,\nand frequency-based metrics over the course of training. Our key findings are\nas follows: 1) During training, the vocabulary embedding geometry quickly\nconverges to high correlations with a suite of semantic and syntactic features;\n2) Embeddings of high-frequency and function words (e.g., \"the,\" \"of\") converge\nto their final vectors faster than lexical and low-frequency words, which\nretain some alignment with the bias in their random initializations. These\nfindings help map the dynamic trajectory by which input embeddings organize\naround linguistic structure, revealing distinct roles for word frequency and\nfunction. Our findings motivate a deeper study of how the evolution of\nvocabulary geometry may facilitate specific capability gains during model\ntraining.", "AI": {"tldr": "研究了语言模型在训练过程中输入词汇表示的结构，揭示了高频词汇和低频词汇在训练过程中的不同演化过程。", "motivation": "研究语言模型的输入词汇表示是如何被结构化的，以及这种结构在训练过程中如何和何时发生变化。", "method": "使用表示相似性分析，进行了一系列实验，将两个开源模型（Pythia 12B 和 OLMo 7B）的输入嵌入和输出嵌入的几何结构与语义、句法和频率相关的度量相关联。", "result": "发现一：在训练过程中，词汇嵌入几何结构快速地与一系列语义和句法特征高度相关；发现二：高频词汇和功能词汇的嵌入（如“the”、“of”）比词汇和低频词汇更快收敛到其最终向量，且低频词汇在某种程度上保持了与随机初始化的对齐。", "conclusion": "这些发现有助于了解输入嵌入如何围绕语言结构组织的动态轨迹，揭示了词汇频率和功能的不同作用。进一步的研究可能探索词汇几何结构的演化如何促进模型训练中的特定能力提升。"}}
{"id": "2510.07721", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07721", "abs": "https://arxiv.org/abs/2510.07721", "authors": ["Zipeng Guo", "Lichen Ma", "Xiaolong Fu", "Gaojing Zhou", "Lan Yang", "Yuchen Zhou", "Linkai Liu", "Yu He", "Ximan Liu", "Shiping Dong", "Jingling Fu", "Zhen Chen", "Yu Shi", "Junshi Huang", "Jason Li", "Chao Gou"], "title": "RePainter: Empowering E-commerce Object Removal via Spatial-matting Reinforcement Learning", "comment": null, "summary": "In web data, product images are central to boosting user engagement and\nadvertising efficacy on e-commerce platforms, yet the intrusive elements such\nas watermarks and promotional text remain major obstacles to delivering clear\nand appealing product visuals. Although diffusion-based inpainting methods have\nadvanced, they still face challenges in commercial settings due to unreliable\nobject removal and limited domain-specific adaptation. To tackle these\nchallenges, we propose Repainter, a reinforcement learning framework that\nintegrates spatial-matting trajectory refinement with Group Relative Policy\nOptimization (GRPO). Our approach modulates attention mechanisms to emphasize\nbackground context, generating higher-reward samples and reducing unwanted\nobject insertion. We also introduce a composite reward mechanism that balances\nglobal, local, and semantic constraints, effectively reducing visual artifacts\nand reward hacking. Additionally, we contribute EcomPaint-100K, a high-quality,\nlarge-scale e-commerce inpainting dataset, and a standardized benchmark\nEcomPaint-Bench for fair evaluation. Extensive experiments demonstrate that\nRepainter significantly outperforms state-of-the-art methods, especially in\nchallenging scenes with intricate compositions. We will release our code and\nweights upon acceptance.", "AI": {"tldr": "针对电子商务平台产品图片中的扰动元素问题，提出Repainter，利用强化学习技术改进图像修复效果，减少了广告和水印等元素对视觉效果的影响。", "motivation": "在电子商务平台中，产品图片对于提高用户参与度及广告效果至关重要，但水印和促销文字等侵入元素仍然是提供清晰吸引人的产品视觉的主要障碍。虽然基于扩散的图像修复方法已经进步，但在实际商业应用中仍然面临物体去除不可靠和领域特定适应性有限的挑战。", "method": "Repainter, 一个结合空间抠图轨迹细化与组相对策略优化（GRPO）的强化学习框架，调整注意力机制强调背景上下文，生成高奖励样本并减少不必要的物体插入。", "result": "通过引入综合奖励机制来平衡全局、局部和语义约束，减少视觉伪影并防止奖励欺骗，实验结果表明Repainter显著优于最先进的方法，在具有复杂构成的挑战性场景中尤其如此。", "conclusion": "Repainter通过强化学习方法显著改进了电子商务领域的图像修复效果，特别是在解决复杂背景下的物体移除问题上展现了优势。"}}
{"id": "2510.07629", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07629", "abs": "https://arxiv.org/abs/2510.07629", "authors": ["Zhangdie Yuan", "Han-Chin Shing", "Mitch Strong", "Chaitanya Shivade"], "title": "Toward Reliable Clinical Coding with Language Models: Verification and Lightweight Adaptation", "comment": null, "summary": "Accurate clinical coding is essential for healthcare documentation, billing,\nand decision-making. While prior work shows that off-the-shelf LLMs struggle\nwith this task, evaluations based on exact match metrics often overlook errors\nwhere predicted codes are hierarchically close but incorrect. Our analysis\nreveals that such hierarchical misalignments account for a substantial portion\nof LLM failures. We show that lightweight interventions, including prompt\nengineering and small-scale fine-tuning, can improve accuracy without the\ncomputational overhead of search-based methods. To address hierarchically\nnear-miss errors, we introduce clinical code verification as both a standalone\ntask and a pipeline component. To mitigate the limitations in existing\ndatasets, such as incomplete evidence and inpatient bias in MIMIC, we release\nan expert double-annotated benchmark of outpatient clinical notes with ICD-10\ncodes. Our results highlight verification as an effective and reliable step\ntoward improving LLM-based medical coding.", "AI": {"tldr": "研究展示轻量级干预措施可以提升LLMs在临床编码任务中的准确性，强调了临床代码验证的有效性，并发布了新的门诊临床记录数据集。", "motivation": "临床编码的准确性对于医疗文档、计费和决策至关重要。尽管之前的工作表明现成的LLMs在此任务中表现不佳，但也发现单纯的精确匹配度量标准往往忽视了预测编码在层级上接近却错误的问题。", "method": "通过轻量级干预措施，包括提示工程和小规模微调来提高准确性，而不需借助计算昂贵的搜索方法。为了处理层级接近的错误编码问题，引入了临床代码验证作为独立任务和管道组件。", "result": "揭示了这样的层级不一致占了LLMs错误的很大比例。验证作为一种步骤显示出改进LLM医疗编码的有效性和可靠性。释放了一个经过专家双重标注的门诊临床记录数据集，该数据集包含ICD-10编码，用以缓解现有数据集的不足，如证据不完整和MIMIC的住院偏见。", "conclusion": "引入了临床代码验证作为一种有效和可靠的步骤，能够提高基于LLM的医疗编码的准确性。同时，发布了门诊临床记录的专家双重标注数据集，改进了现有数据集的不足。"}}
{"id": "2510.07723", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07723", "abs": "https://arxiv.org/abs/2510.07723", "authors": ["Wenyue Chen", "Peng Li", "Wangguandong Zheng", "Chengfeng Zhao", "Mengfei Li", "Yaolong Zhu", "Zhiyang Dou", "Ronggang Wang", "Yuan Liu"], "title": "SyncHuman: Synchronizing 2D and 3D Generative Models for Single-view Human Reconstruction", "comment": "NIPS 2025", "summary": "Photorealistic 3D full-body human reconstruction from a single image is a\ncritical yet challenging task for applications in films and video games due to\ninherent ambiguities and severe self-occlusions. While recent approaches\nleverage SMPL estimation and SMPL-conditioned image generative models to\nhallucinate novel views, they suffer from inaccurate 3D priors estimated from\nSMPL meshes and have difficulty in handling difficult human poses and\nreconstructing fine details. In this paper, we propose SyncHuman, a novel\nframework that combines 2D multiview generative model and 3D native generative\nmodel for the first time, enabling high-quality clothed human mesh\nreconstruction from single-view images even under challenging human poses.\nMultiview generative model excels at capturing fine 2D details but struggles\nwith structural consistency, whereas 3D native generative model generates\ncoarse yet structurally consistent 3D shapes. By integrating the complementary\nstrengths of these two approaches, we develop a more effective generation\nframework. Specifically, we first jointly fine-tune the multiview generative\nmodel and the 3D native generative model with proposed pixel-aligned 2D-3D\nsynchronization attention to produce geometrically aligned 3D shapes and 2D\nmultiview images. To further improve details, we introduce a feature injection\nmechanism that lifts fine details from 2D multiview images onto the aligned 3D\nshapes, enabling accurate and high-fidelity reconstruction. Extensive\nexperiments demonstrate that SyncHuman achieves robust and photo-realistic 3D\nhuman reconstruction, even for images with challenging poses. Our method\noutperforms baseline methods in geometric accuracy and visual fidelity,\ndemonstrating a promising direction for future 3D generation models.", "AI": {"tldr": "本文介绍了一种名为SyncHuman的框架，能够从单视角图像重建高质量的着装人体网格，即使在面对挑战性姿态时，依然表现出色。", "motivation": "传统的方法依赖于SMPL估计和受SMPL条件限制的图像生成模型来产生新视角，但这些方法面临3D先验估计不准确的问题，并且难以处理复杂的人体姿态和重建精细的细节。", "method": "本文提出了SyncHuman框架，首次结合了2D多视角生成模型和3D原生生成模型。2D多视角生成模型擅长捕捉细小的2D细节但难以保持结构一致性，而3D原生生成模型在生成粗糙但结构一致的3D形状方面更有优势。通过结合这两种方法的优势，作者开发了更有效的生成框架。", "result": "实验结果表明，SyncHuman框架实现了鲁棒且逼真的3D人体重建，即使对于具有挑战性的姿态图像，也能表现良好。本文的方法在几何准确性和视觉保真度上均优于基线方法。", "conclusion": "SyncHuman证明了结合2D多视角生成模型和3D原生生成模型的潜力，不仅在几何准确性上表现优越，在视觉保真度上也领先于基线方法，为未来3D生成模型研究开辟了新的方向。"}}
{"id": "2510.07642", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07642", "abs": "https://arxiv.org/abs/2510.07642", "authors": ["Đorđe Klisura", "Joseph Khoury", "Ashish Kundu", "Ram Krishnan", "Anthony Rios"], "title": "Role-Conditioned Refusals: Evaluating Access Control Reasoning in Large Language Models", "comment": "8 pages + Appendix", "summary": "Access control is a cornerstone of secure computing, yet large language\nmodels often blur role boundaries by producing unrestricted responses. We study\nrole-conditioned refusals, focusing on the LLM's ability to adhere to access\ncontrol policies by answering when authorized and refusing when not. To\nevaluate this behavior, we created a novel dataset that extends the Spider and\nBIRD text-to-SQL datasets, both of which have been modified with realistic\nPostgreSQL role-based policies at the table and column levels. We compare three\ndesigns: (i) zero or few-shot prompting, (ii) a two-step generator-verifier\npipeline that checks SQL against policy, and (iii) LoRA fine-tuned models that\nlearn permission awareness directly. Across multiple model families, explicit\nverification (the two-step framework) improves refusal precision and lowers\nfalse permits. At the same time, fine-tuning achieves a stronger balance\nbetween safety and utility (i.e., when considering execution accuracy). Longer\nand more complex policies consistently reduce the reliability of all systems.\nWe release RBAC-augmented datasets and code.", "AI": {"tldr": "研究考察了大型语言模型根据角色权限拒绝或回答的能力，使用增强的SQL数据集进行评估，并比较了三种权限控制的设计策略。", "motivation": "大型语言模型在保持角色边界方面存在挑战，它们可能会产生不受限制的响应。本研究旨在评估和改进大型语言模型遵守访问控制策略的能力。", "method": "本研究通过创建一个结合了Spider和BIRD数据集的新型数据集，这个数据集加入了基于PostgreSQL角色的真实权限策略，来评估大型语言模型在不同设计下的表现。研究比较了三种设计：1. 零样本或少样本提示；2. 两步生成-验证流水线，该流水线通过验证SQL语句是否符合策略来过滤内容；3. 使用LoRA进行微调的模型，其目标是直接学习权限感知能力。", "result": "研究发现，使用明确验证的两步框架可以提高拒绝的精确度和减少不正确的许可情况，而微调模型则在安全性和实用性之间取得了更好的平衡。不过，当面对更多的角色和访问策略时，所有系统的性能下降。", "conclusion": "研究结果显示，明确的验证过程（两步框架方法）可以提高拒绝的精确度，减少错误许可的情况。微调方法则在安全性与实用性之间取得了更好的平衡。但是，更长更复杂的策略会降低所有系统的可靠性。"}}
{"id": "2510.07729", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07729", "abs": "https://arxiv.org/abs/2510.07729", "authors": ["Jian Gao", "Mengqi Yuan", "Yifei Zeng", "Chang Zeng", "Zhihao Li", "Zhenyu Chen", "Weichao Qiu", "Xiao-Xiao Long", "Hao Zhu", "Xun Cao", "Yao Yao"], "title": "ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral Probes", "comment": null, "summary": "Gaussian Splatting (GS) enables immersive rendering, but realistic 3D\nobject-scene composition remains challenging. Baked appearance and shadow\ninformation in GS radiance fields cause inconsistencies when combining objects\nand scenes. Addressing this requires relightable object reconstruction and\nscene lighting estimation. For relightable object reconstruction, existing\nGaussian-based inverse rendering methods often rely on ray tracing, leading to\nlow efficiency. We introduce Surface Octahedral Probes (SOPs), which store\nlighting and occlusion information and allow efficient 3D querying via\ninterpolation, avoiding expensive ray tracing. SOPs provide at least a 2x\nspeedup in reconstruction and enable real-time shadow computation in Gaussian\nscenes. For lighting estimation, existing Gaussian-based inverse rendering\nmethods struggle to model intricate light transport and often fail in complex\nscenes, while learning-based methods predict lighting from a single image and\nare viewpoint-sensitive. We observe that 3D object-scene composition primarily\nconcerns the object's appearance and nearby shadows. Thus, we simplify the\nchallenging task of full scene lighting estimation by focusing on the\nenvironment lighting at the object's placement. Specifically, we capture a 360\ndegrees reconstructed radiance field of the scene at the location and fine-tune\na diffusion model to complete the lighting. Building on these advances, we\npropose ComGS, a novel 3D object-scene composition framework. Our method\nachieves high-quality, real-time rendering at around 28 FPS, produces visually\nharmonious results with vivid shadows, and requires only 36 seconds for\nediting. Code and dataset are available at\nhttps://nju-3dv.github.io/projects/ComGS/.", "AI": {"tldr": "本文提出ComGS框架，通过Surface Octahedral Probes(SOPs)提高高斯渲染效率并解决真实3D物体与场景组合中出现的不一致性问题。", "motivation": "现有高斯散射技术在渲染真实3D对象时存在镶嵌不一致的问题，需要开发新的方法来解决这些挑战。", "method": "采用Surface Octahedral Probes (SOPs)存储光照和遮挡信息，提高渲染效率并简化复杂场景的光照估计。", "result": "ComGS实现了高质量，实时渲染，帧速率达到28 FPS，且编辑时间为36秒，视觉效果良好。", "conclusion": "ComGS是一种新的3D对象-场景组合框架，显著提高了渲染质量和速度，解决了现有方法中存在的问题。"}}
{"id": "2510.07645", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07645", "abs": "https://arxiv.org/abs/2510.07645", "authors": ["Xin Jie Chua", "Jeraelyn Ming Li Tan", "Jia Xuan Tan", "Soon Chang Poh", "Yi Xian Goh", "Debbie Hui Tian Choong", "Chee Mun Foong", "Sze Jue Yang", "Chee Seng Chan"], "title": "Banking Done Right: Redefining Retail Banking with Language-Centric AI", "comment": "Accepted at EMNLP2025 Industry Track", "summary": "This paper presents Ryt AI, an LLM-native agentic framework that powers Ryt\nBank to enable customers to execute core financial transactions through natural\nlanguage conversation. This represents the first global regulator-approved\ndeployment worldwide where conversational AI functions as the primary banking\ninterface, in contrast to prior assistants that have been limited to advisory\nor support roles. Built entirely in-house, Ryt AI is powered by ILMU, a\nclosed-source LLM developed internally, and replaces rigid multi-screen\nworkflows with a single dialogue orchestrated by four LLM-powered agents\n(Guardrails, Intent, Payment, and FAQ). Each agent attaches a task-specific\nLoRA adapter to ILMU, which is hosted within the bank's infrastructure to\nensure consistent behavior with minimal overhead. Deterministic guardrails,\nhuman-in-the-loop confirmation, and a stateless audit architecture provide\ndefense-in-depth for security and compliance. The result is Banking Done Right:\ndemonstrating that regulator-approved natural-language interfaces can reliably\nsupport core financial operations under strict governance.", "AI": {"tldr": "论文展示了Ryt AI框架如何通过自然语言对话界面使客户执行核心金融交易，作为全球首个获得监管批准的对话式AI银行界面，该框架通过四个LLM代理和安全措施提供了可靠的金融服务。", "motivation": "Ryt AI是全球首个获得监管机构批准的部署项目，它使得对话式AI作为主要的银行界面，与之前仅限于咨询或支持角色的助手形成对比。该框架通过确定性的护栏、人类互动环节确认和无状态审计架构提供了多层次的安全和合规保障。", "method": "本论文介绍了一种名为Ryt AI的LLM原生代理框架，该框架通过自然语言对话支持客户执行核心金融交易。Ryt AI完全由内部开发，由ILMU（内部开发的闭源LLM）驱动，它通过四个LLM代理（Guardrails、Intent、Payment和FAQ）取代了僵化的多屏工作流，每个代理都附加了特定任务的LoRA适配器，以确保一致的行为同时保持较低的开销。", "result": "结果证明，监管批准的自然语言界面可以可靠地支持在严格治理下进行的核心金融操作。", "conclusion": "研究结论表明，经过监管机构批准的自然语言界面可以支持严格的治理下的核心金融操作，实现了银行业务的正确执行。"}}
{"id": "2510.07741", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07741", "abs": "https://arxiv.org/abs/2510.07741", "authors": ["Yuang Meng", "Xin Jin", "Lina Lei", "Chun-Le Guo", "Chongyi Li"], "title": "UltraLED: Learning to See Everything in Ultra-High Dynamic Range Scenes", "comment": null, "summary": "Ultra-high dynamic range (UHDR) scenes exhibit significant exposure\ndisparities between bright and dark regions. Such conditions are commonly\nencountered in nighttime scenes with light sources. Even with standard exposure\nsettings, a bimodal intensity distribution with boundary peaks often emerges,\nmaking it difficult to preserve both highlight and shadow details\nsimultaneously. RGB-based bracketing methods can capture details at both ends\nusing short-long exposure pairs, but are susceptible to misalignment and\nghosting artifacts. We found that a short-exposure image already retains\nsufficient highlight detail. The main challenge of UHDR reconstruction lies in\ndenoising and recovering information in dark regions. In comparison to the RGB\nimages, RAW images, thanks to their higher bit depth and more predictable noise\ncharacteristics, offer greater potential for addressing this challenge. This\nraises a key question: can we learn to see everything in UHDR scenes using only\na single short-exposure RAW image? In this study, we rely solely on a single\nshort-exposure frame, which inherently avoids ghosting and motion blur, making\nit particularly robust in dynamic scenes. To achieve that, we introduce\nUltraLED, a two-stage framework that performs exposure correction via a ratio\nmap to balance dynamic range, followed by a brightness-aware RAW denoiser to\nenhance detail recovery in dark regions. To support this setting, we design a\n9-stop bracketing pipeline to synthesize realistic UHDR images and contribute a\ncorresponding dataset based on diverse scenes, using only the shortest exposure\nas input for reconstruction. Extensive experiments show that UltraLED\nsignificantly outperforms existing single-frame approaches. Our code and\ndataset are made publicly available at\nhttps://srameo.github.io/projects/ultraled.", "AI": {"tldr": "本文介绍了一种仅使用单个短曝光RAW图像来处理超高动态范围场景的方法，避免了鬼影和运动模糊，提升了暗区细节的恢复效果。", "motivation": "夜间场景中由于光线变化较大，高动态范围场景的明亮和暗区之间的曝光差异显著。现有的基于RGB的包围曝光方法存在伪影和对齐问题。", "method": "提出了一种两阶段框架UltraLED，首先通过比率图执行曝光校正来平衡动态范围，然后使用亮度感知的RAW图像去噪器来增强暗区细节的恢复。", "result": "通过使用9档包围曝光管线合成现实场景的超高动态范围图像，实验表明UltraLED方法显著优于现有的单帧处理方法。", "conclusion": "实验表明，UltraLED显著优于现有的单帧方法。代码和数据集已公开。"}}
{"id": "2510.07651", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07651", "abs": "https://arxiv.org/abs/2510.07651", "authors": ["Yuzhe Gu", "Xiyu Liang", "Jiaojiao Zhao", "Enmao Diao"], "title": "OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM Inference", "comment": null, "summary": "Large language models (LLMs) with extended context windows enable powerful\ndownstream applications but impose significant memory overhead, as caching all\nkey-value (KV) states scales linearly with sequence length and batch size.\nExisting cache eviction methods address this by exploiting attention sparsity,\nyet they typically rank tokens heuristically using accumulated attention\nweights without considering their true impact on attention outputs. We propose\nOptimal Brain Cache (OBCache), a principled framework that formulates cache\neviction as a layer-wise structured pruning problem. Building upon the Optimal\nBrain Damage (OBD) theory, OBCache quantifies token saliency by measuring the\nperturbation in attention outputs induced by pruning tokens, with closed-form\nscores derived for isolated keys, isolated values, and joint key-value pairs.\nOur scores account not only for attention weights but also for information from\nvalue states and attention outputs, thereby enhancing existing eviction\nstrategies with output-aware signals. Experiments on LLaMA and Qwen models\ndemonstrate that replacing the heuristic scores in existing works, which\nestimate token saliency across different query positions, with OBCache's\noutput-aware scores consistently improves long-context accuracy.", "AI": {"tldr": "提出了OBCache框架，以缓存替换问题为基础，通过关注令牌对注意力输出的影响来提高大型语言模型在长上下文应用中的性能。", "motivation": "大型语言模型（LLMs）拥有扩展的上下文窗口，但这些模型的缓存会给内存带来显著的负担。现有的缓存替换方法通常基于积累的注意力权重对令牌进行启发式排序，但这没有考虑令牌对注意力输出的真实影响。", "method": "构建了一个名为OBCache的框架，该框架将缓存替换视为分层结构化剪枝问题。它基于最优脑损伤（OBD）理论，通过测量剪枝令牌引起的注意力输出扰动来量化令牌的重要性。得分考虑了注意力权重、值状态和注意力输出的信息。", "result": "实验结果表明，使用OBCache得分替换现有工作的启发式得分可以提高长上下文的准确性。", "conclusion": "OBCache通过提供输出感知信号增强了现有的缓存替换策略，从而提升了长上下文的应用效果。"}}
{"id": "2510.07752", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07752", "abs": "https://arxiv.org/abs/2510.07752", "authors": ["Junhao He", "Jiaxu Wang", "Jia Li", "Mingyuan Sun", "Qiang Zhang", "Jiahang Cao", "Ziyi Zhang", "Yi Gu", "Jingkai Sun", "Renjing Xu"], "title": "DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event Stream", "comment": "Accepted by TVCG", "summary": "Reconstructing Dynamic 3D Gaussian Splatting (3DGS) from low-framerate RGB\nvideos is challenging. This is because large inter-frame motions will increase\nthe uncertainty of the solution space. For example, one pixel in the first\nframe might have more choices to reach the corresponding pixel in the second\nframe. Event cameras can asynchronously capture rapid visual changes and are\nrobust to motion blur, but they do not provide color information. Intuitively,\nthe event stream can provide deterministic constraints for the inter-frame\nlarge motion by the event trajectories. Hence, combining\nlow-temporal-resolution images with high-framerate event streams can address\nthis challenge. However, it is challenging to jointly optimize Dynamic 3DGS\nusing both RGB and event modalities due to the significant discrepancy between\nthese two data modalities. This paper introduces a novel framework that jointly\noptimizes dynamic 3DGS from the two modalities. The key idea is to adopt event\nmotion priors to guide the optimization of the deformation fields. First, we\nextract the motion priors encoded in event streams by using the proposed LoCM\nunsupervised fine-tuning framework to adapt an event flow estimator to a\ncertain unseen scene. Then, we present the geometry-aware data association\nmethod to build the event-Gaussian motion correspondence, which is the primary\nfoundation of the pipeline, accompanied by two useful strategies, namely motion\ndecomposition and inter-frame pseudo-label. Extensive experiments show that our\nmethod outperforms existing image and event-based approaches across synthetic\nand real scenes and prove that our method can effectively optimize dynamic 3DGS\nwith the help of event data.", "AI": {"tldr": "本文提出了一种结合RGB图像和事件流数据来优化动态3DGS的新框架。通过使用运动先验指导形变场的优化，解决了动态3D高斯散斑重建中帧间大运动带来的不确定性问题。", "motivation": "由于帧间大运动增加了动态3D高斯散斑重建的不确定性，而RGB图像和事件数据模态间的巨大差异使联合优化变得具有挑战性。因此，本文旨在通过结合RGB和事件流数据来解决这个问题。", "method": "本文提出了一种新的框架，该框架结合了RGB图像和事件流数据，用于动态3D高斯散斑重建。该方法首先使用提出的LoCM无监督微调框架来提取事件流中的运动先验，然后通过几何感知数据关联方法建立事件-高斯运动对应关系。", "result": "实验结果表明，相较于现有基于图像和基于事件的方法，该方法在合成场景和真实场景中都表现出色，证明了在事件数据的帮助下可以有效地优化动态3DGS。", "conclusion": "本文提出了一种联合优化动态3DGS的新方法，结合了RGB图像和高帧率事件流数据，并通过实验验证了其有效性。"}}
{"id": "2510.07662", "categories": ["cs.CL", "cs.CY", "I.2.7; K.4.2"], "pdf": "https://arxiv.org/pdf/2510.07662", "abs": "https://arxiv.org/abs/2510.07662", "authors": ["Virginia K. Felkner", "Allison Lim", "Jonathan May"], "title": "Textual Entailment and Token Probability as Bias Evaluation Metrics", "comment": "16 pages, 9 figures, under ARR review", "summary": "Measurement of social bias in language models is typically by token\nprobability (TP) metrics, which are broadly applicable but have been criticized\nfor their distance from real-world langugage model use cases and harms. In this\nwork, we test natural language inference (NLI) as a more realistic alternative\nbias metric. We show that, curiously, NLI and TP bias evaluation behave\nsubstantially differently, with very low correlation among different NLI\nmetrics and between NLI and TP metrics. We find that NLI metrics are more\nlikely to detect \"underdebiased\" cases. However, NLI metrics seem to be more\nbrittle and sensitive to wording of counterstereotypical sentences than TP\napproaches. We conclude that neither token probability nor natural language\ninference is a \"better\" bias metric in all cases, and we recommend a\ncombination of TP, NLI, and downstream bias evaluations to ensure comprehensive\nevaluation of language models.\n  Content Warning: This paper contains examples of anti-LGBTQ+ stereotypes.", "AI": {"tldr": "研究测试了NLI作为语言模型偏见度量的替代方法，发现NLI和TP评估存在显著差异，建议将两者与其他下游评价方法结合使用。", "motivation": "当前，语言模型中的社会偏见通常是通过TP指标来测量的，虽然TP指标的适用性广泛，但它们与现实世界的语言模型用途和危害之间的距离受到了批评。本研究旨在探索替代偏见度量指标的合理性。", "method": "本研究测试了自然语言推理（NLI）作为一种更真实的语言模型偏见度量的替代方法，并将其与基于标记概率（TP）的方法进行了比较。", "result": "研究显示，NLI和TP偏见评估行为存在显著差异，NLI度量更可能检测到欠矫正的案例，但似乎对反刻板语言的措辞更加脆弱和敏感。", "conclusion": "结论是，TP和NLI都不是所有情况下更好的偏见度量，建议结合TP、NLI和下游偏见评估以确保对语言模型进行全面评价。"}}
{"id": "2510.07785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07785", "abs": "https://arxiv.org/abs/2510.07785", "authors": ["Ming Jie Ong", "Sze Yinn Ung", "Sim Kuan Goh", "Jimmy Y. Zhong"], "title": "Demystifying Deep Learning-based Brain Tumor Segmentation with 3D UNets and Explainable AI (XAI): A Comparative Analysis", "comment": null, "summary": "The current study investigated the use of Explainable Artificial Intelligence\n(XAI) to improve the accuracy of brain tumor segmentation in MRI images, with\nthe goal of assisting physicians in clinical decision-making. The study focused\non applying UNet models for brain tumor segmentation and using the XAI\ntechniques of Gradient-weighted Class Activation Mapping (Grad-CAM) and\nattention-based visualization to enhance the understanding of these models.\nThree deep learning models - UNet, Residual UNet (ResUNet), and Attention UNet\n(AttUNet) - were evaluated to identify the best-performing model. XAI was\nemployed with the aims of clarifying model decisions and increasing physicians'\ntrust in these models. We compared the performance of two UNet variants\n(ResUNet and AttUNet) with the conventional UNet in segmenting brain tumors\nfrom the BraTS2020 public dataset and analyzed model predictions with Grad-CAM\nand attention-based visualization. Using the latest computer hardware, we\ntrained and validated each model using the Adam optimizer and assessed their\nperformance with respect to: (i) training, validation, and inference times,\n(ii) segmentation similarity coefficients and loss functions, and (iii)\nclassification performance. Notably, during the final testing phase, ResUNet\noutperformed the other models with respect to Dice and Jaccard similarity\nscores, as well as accuracy, recall, and F1 scores. Grad-CAM provided\nvisuospatial insights into the tumor subregions each UNet model focused on\nwhile attention-based visualization provided valuable insights into the working\nmechanisms of AttUNet's attention modules. These results demonstrated ResUNet\nas the best-performing model and we conclude by recommending its use for\nautomated brain tumor segmentation in future clinical assessments. Our source\ncode and checkpoint are available at\nhttps://github.com/ethanong98/MultiModel-XAI-Brats2020", "AI": {"tldr": "研究使用可解释的人工智能技术提高MRI图像中脑瘤分割的准确性，应用了UNet、ResUNet、和AttUNet三种模型，其中ResUNet表现最佳并推荐用于临床评估。", "motivation": "该研究的动机是通过XAI技术提升脑瘤图像分割的准确性，辅助医生的临床诊断。", "method": "研究评估了三种深度学习模型：UNet、ResUNet、和AttUNet，使用了Grad-CAM和注意力可视化技术，基于BraTS2020数据集进行模型训练和评估。", "result": "ResUNet在Dice和Jaccard相似性分数以及准确率、召回率和F1分数上表现出色。", "conclusion": "研究结论推荐使用ResUNet进行脑瘤自动化分割，以提高未来的临床评估准确性。"}}
{"id": "2510.07686", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07686", "abs": "https://arxiv.org/abs/2510.07686", "authors": ["Jifan Zhang", "Henry Sleight", "Andi Peng", "John Schulman", "Esin Durmus"], "title": "Stress-Testing Model Specs Reveals Character Differences among Language Models", "comment": null, "summary": "Large language models (LLMs) are increasingly trained from AI constitutions\nand model specifications that establish behavioral guidelines and ethical\nprinciples. However, these specifications face critical challenges, including\ninternal conflicts between principles and insufficient coverage of nuanced\nscenarios. We present a systematic methodology for stress-testing model\ncharacter specifications, automatically identifying numerous cases of principle\ncontradictions and interpretive ambiguities in current model specs.\n  We stress test current model specs by generating scenarios that force\nexplicit tradeoffs between competing value-based principles. Using a\ncomprehensive taxonomy we generate diverse value tradeoff scenarios where\nmodels must choose between pairs of legitimate principles that cannot be\nsimultaneously satisfied. We evaluate responses from twelve frontier LLMs\nacross major providers (Anthropic, OpenAI, Google, xAI) and measure behavioral\ndisagreement through value classification scores. Among these scenarios, we\nidentify over 70,000 cases exhibiting significant behavioral divergence.\nEmpirically, we show this high divergence in model behavior strongly predicts\nunderlying problems in model specifications. Through qualitative analysis, we\nprovide numerous example issues in current model specs such as direct\ncontradiction and interpretive ambiguities of several principles. Additionally,\nour generated dataset also reveals both clear misalignment cases and\nfalse-positive refusals across all of the frontier models we study. Lastly, we\nalso provide value prioritization patterns and differences of these models.", "AI": {"tldr": "The paper introduces a method to stress-test AI model specifications, revealing over 70,000 instances of behavioral inconsistencies across twelve major LLMs.", "motivation": "To address the critical challenges faced by AI constitutions and model specifications, including internal conflicts and insufficient coverage.", "method": "Developing a systematic and automatic approach to generate scenarios that force LLMs to choose between conflicting value-based principles and measuring behavioral disagreement across models.", "result": "Identified over 70,000 cases of significant behavioral divergence among tested models, indicating underlying problems in their specifications.", "conclusion": "The method effectively highlights issues such as contradictions and interpretive ambiguities, providing valuable insights for refining AI model specifications."}}
{"id": "2510.07791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07791", "abs": "https://arxiv.org/abs/2510.07791", "authors": ["Qinghongbing Xie", "Zhaoyuan Xia", "Feng Zhu", "Lijun Gong", "Ziyue Li", "Rui Zhao", "Long Zeng"], "title": "GTR-Bench: Evaluating Geo-Temporal Reasoning in Vision-Language Models", "comment": "20 pages, 13 figures", "summary": "Recently spatial-temporal intelligence of Visual-Language Models (VLMs) has\nattracted much attention due to its importance for Autonomous Driving, Embodied\nAI and General Artificial Intelligence. Existing spatial-temporal benchmarks\nmainly focus on egocentric perspective reasoning with images/video context, or\ngeographic perspective reasoning with graphics context (eg. a map), thus fail\nto assess VLMs' geographic spatial-temporal intelligence with both images/video\nand graphics context, which is important for areas like traffic management and\nemergency response. To address the gaps, we introduce Geo-Temporal Reasoning\nbenchmark (GTR-Bench), a novel challenge for geographic temporal reasoning of\nmoving targets in a large-scale camera network. GTR-Bench is more challenging\nas it requires multiple perspective switches between maps and videos, joint\nreasoning across multiple videos with non-overlapping fields of view, and\ninference over spatial-temporal regions that are unobserved by any video\ncontext. Evaluations of more than 10 popular VLMs on GTR-Bench demonstrate that\neven the best proprietary model, Gemini-2.5-Pro (34.9%), significantly lags\nbehind human performance (78.61%) on geo-temporal reasoning. Moreover, our\ncomprehensive analysis on GTR-Bench reveals three primary deficiencies of\ncurrent models for geo-temporal reasoning. (1) VLMs' reasoning is impaired by\nan imbalanced utilization of spatial-temporal context. (2) VLMs are weak in\ntemporal forecasting, which leads to worse performance on temporal-emphasized\ntasks than on spatial-emphasized tasks. (3) VLMs lack the proficiency to\ncomprehend or align the map data with multi-view video inputs. We believe\nGTR-Bench offers valuable insights and opens up new opportunities for research\nand applications in spatial-temporal intelligence. Benchmark and code will be\nreleased at https://github.com/X-Luffy/GTR-Bench.", "AI": {"tldr": "介绍了GTR-Bench来评估VLMs在地理时空推理中的能力，展示了它们与人类表现之间的差距，并指出了当前模型的缺陷。", "motivation": "现有的时空基准主要侧重于基于图像/视频上下文的以自我为中心的视角推理，或基于图形上下文的地理视角推理，这无法评估VLMs在结合图像/视频和图形上下文时的地理时空智能。这项研究是为了填补这一空白。", "method": "引入了Geo-Temporal Reasoning基准(GTR-Bench)，这是一个在大规模摄像头网络中对移动目标进行地理时间推理的新挑战。该基准提出了在地图与视频间进行多次视角转换、跨多个非重叠视场的视频进行联合推理，以及对任何视频上下文未观察到的时空区域进行推理的要求。", "result": "在GTR-Bench上对10多个流行的VLMs进行评估表明，即使是最先进的Gemini-2.5-Pro模型（准确率为34.9%），也显著落后于人类（准确率为78.61%）在地理时空推理上的表现。", "conclusion": "这项研究揭示了当前模型在地理时空推理上的三个主要缺陷：不平衡使用时空上下文、弱时间预测能力，以及理解和对齐地图数据与多视角视频输入能力的不足。GTR-Bench为时空智能的研究和应用提供了有价值的见解和新的机会。"}}
{"id": "2510.07706", "categories": ["cs.CL", "cs.CE", "cs.LG", "q-bio.CB"], "pdf": "https://arxiv.org/pdf/2510.07706", "abs": "https://arxiv.org/abs/2510.07706", "authors": ["Krinos Li", "Xianglu Xiao", "Shenglong Deng", "Lucas He", "Zijun Zhong", "Yuanjie Zou", "Zhonghao Zhan", "Zheng Hui", "Weiye Bao", "Guang Yang"], "title": "Large Language Models Meet Virtual Cell: A Survey", "comment": null, "summary": "Large language models (LLMs) are transforming cellular biology by enabling\nthe development of \"virtual cells\"--computational systems that represent,\npredict, and reason about cellular states and behaviors. This work provides a\ncomprehensive review of LLMs for virtual cell modeling. We propose a unified\ntaxonomy that organizes existing methods into two paradigms: LLMs as Oracles,\nfor direct cellular modeling, and LLMs as Agents, for orchestrating complex\nscientific tasks. We identify three core tasks--cellular representation,\nperturbation prediction, and gene regulation inference--and review their\nassociated models, datasets, evaluation benchmarks, as well as the critical\nchallenges in scalability, generalizability, and interpretability.", "AI": {"tldr": "本文综述了大型语言模型在虚拟细胞建模中的应用，提出了两种范式，并概述了三个核心任务及面临的挑战。", "motivation": "随着大型语言模型的进步，它们在细胞生物学领域展现出新的潜力，能够用于构建能够表示、预测和推理细胞状态和行为的“虚拟细胞”。因此，对这一领域进行全面的综述显得尤为重要。", "method": "本文提出了一种统一的分类法，将现有方法分为两种范式：作为直接细胞建模的Oracle模式和作为复杂科学任务协调者的Agent模式。", "result": "研究归纳了细胞表示、扰动预测、基因调节推断三个核心任务，并分析了相关模型、数据集以及评估基准。", "conclusion": "随着大型语言模型应用于细胞建模，研究指出了扩展性、泛化能力和可解释性等关键挑战。"}}
{"id": "2510.07810", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07810", "abs": "https://arxiv.org/abs/2510.07810", "authors": ["Luu Tu Nguyen", "Vu Tram Anh Khuong", "Thi Bich Phuong Man", "Thi Duyen Ngo", "Thanh Ha Le"], "title": "FMANet: A Novel Dual-Phase Optical Flow Approach with Fusion Motion Attention Network for Robust Micro-expression Recognition", "comment": null, "summary": "Facial micro-expressions, characterized by their subtle and brief nature, are\nvaluable indicators of genuine emotions. Despite their significance in\npsychology, security, and behavioral analysis, micro-expression recognition\nremains challenging due to the difficulty of capturing subtle facial movements.\nOptical flow has been widely employed as an input modality for this task due to\nits effectiveness. However, most existing methods compute optical flow only\nbetween the onset and apex frames, thereby overlooking essential motion\ninformation in the apex-to-offset phase. To address this limitation, we first\nintroduce a comprehensive motion representation, termed Magnitude-Modulated\nCombined Optical Flow (MM-COF), which integrates motion dynamics from both\nmicro-expression phases into a unified descriptor suitable for direct use in\nrecognition networks. Building upon this principle, we then propose FMANet, a\nnovel end-to-end neural network architecture that internalizes the dual-phase\nanalysis and magnitude modulation into learnable modules. This allows the\nnetwork to adaptively fuse motion cues and focus on salient facial regions for\nclassification. Experimental evaluations on the MMEW, SMIC, CASME-II, and SAMM\ndatasets, widely recognized as standard benchmarks, demonstrate that our\nproposed MM-COF representation and FMANet outperforms existing methods,\nunderscoring the potential of a learnable, dual-phase framework in advancing\nmicro-expression recognition.", "AI": {"tldr": "本文提出了一种新的光流表示方法MM-COF及相应的神经网络架构FMANet，实验证明此方法能有效提升微表情识别性能。", "motivation": "现有的方法仅在微表情开始和顶峰帧之间计算光流，忽略了峰点到结束阶段的重要运动信息。为了改进这一点，本文提出了更全面的光流表示方法和识别网络模型。", "method": "本文提出了一种新的运动表示方法——幅度调制组合光流（MM-COF），该方法结合了微表情的两个阶段的运动动力学，进而在识别网络中使用。此外，还设计了一个端到端的神经网络架构FMANet，该架构将两个阶段的分析和幅度调制纳入可学习模块，使网络能自适应地融合运动线索并专注于面部关键区域进行分类。", "result": "实验结果表明，在MMEW、SMIC、CASME-II和SAMM四个数据集上，所提出的MM-COF表示方法和FMANet网络优于现有方法。", "conclusion": "论文表明，利用可学习的双阶段框架能够提升微表情识别的性能。"}}
{"id": "2510.07707", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07707", "abs": "https://arxiv.org/abs/2510.07707", "authors": ["Chengshuai Zhao", "Shu Wan", "Paras Sheth", "Karan Patwa", "K. Selçuk Candan", "Huan Liu"], "title": "Causality Guided Representation Learning for Cross-Style Hate Speech Detection", "comment": null, "summary": "The proliferation of online hate speech poses a significant threat to the\nharmony of the web. While explicit hate is easily recognized through overt\nslurs, implicit hate speech is often conveyed through sarcasm, irony,\nstereotypes, or coded language -- making it harder to detect. Existing hate\nspeech detection models, which predominantly rely on surface-level linguistic\ncues, fail to generalize effectively across diverse stylistic variations.\nMoreover, hate speech spread on different platforms often targets distinct\ngroups and adopts unique styles, potentially inducing spurious correlations\nbetween them and labels, further challenging current detection approaches.\nMotivated by these observations, we hypothesize that the generation of hate\nspeech can be modeled as a causal graph involving key factors: contextual\nenvironment, creator motivation, target, and style. Guided by this graph, we\npropose CADET, a causal representation learning framework that disentangles\nhate speech into interpretable latent factors and then controls confounders,\nthereby isolating genuine hate intent from superficial linguistic cues.\nFurthermore, CADET allows counterfactual reasoning by intervening on style\nwithin the latent space, naturally guiding the model to robustly identify hate\nspeech in varying forms. CADET demonstrates superior performance in\ncomprehensive experiments, highlighting the potential of causal priors in\nadvancing generalizable hate speech detection.", "AI": {"tldr": "论文提出CADET框架来解决已有的仇恨言论检测模型在处理隐性仇恨言论时遇到的挑战，通过因果图分解仇恨言论并控制混杂因素以提高检测效果。", "motivation": "现有的仇恨言论检测模型主要依赖于表面层面的语言线索，在面对复杂的修辞手法和不同的平台特点时，缺乏泛化能力。为了应对这一挑战，作者假设仇恨言论的生成过程可以建模为一个因果图，该图涉及关键因素：上下文环境、创作者动机、目标、和风格。", "method": "CADET是一种因果表征学习框架，该框架将仇恨言论分解为可解释的潜在因素，并控制混杂因素，从而将真正的仇恨意图与表面上的语言线索区分开来。此外，CADET允许通过在潜在空间中对风格进行干预来进行反事实推理，引导模型稳健地识别各种形式的仇恨言论。", "result": "CADET在全面的实验中表现出色，证明了因果先验在推进可泛化的仇恨言论检测中的潜力。", "conclusion": "该论文提出的方法CADT展示了利用因果关系进行仇恨言论检测的有效性，相较于现有方法，它不仅提升了模型的检测能力，还增强了模型在不同风格和环境下的鲁棒性。"}}
{"id": "2510.07817", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07817", "abs": "https://arxiv.org/abs/2510.07817", "authors": ["Kanglin Ning", "Ruzhao Chen", "Penghong Wang", "Xingtao Wang", "Ruiqin Xiong", "Xiaopeng Fan"], "title": "An End-to-End Room Geometry Constrained Depth Estimation Framework for Indoor Panorama Images", "comment": null, "summary": "Predicting spherical pixel depth from monocular $360^{\\circ}$ indoor\npanoramas is critical for many vision applications. However, existing methods\nfocus on pixel-level accuracy, causing oversmoothed room corners and noise\nsensitivity. In this paper, we propose a depth estimation framework based on\nroom geometry constraints, which extracts room geometry information through\nlayout prediction and integrates those information into the depth estimation\nprocess through background segmentation mechanism. At the model level, our\nframework comprises a shared feature encoder followed by task-specific decoders\nfor layout estimation, depth estimation, and background segmentation. The\nshared encoder extracts multi-scale features, which are subsequently processed\nby individual decoders to generate initial predictions: a depth map, a room\nlayout map, and a background segmentation map. Furthermore, our framework\nincorporates two strategies: a room geometry-based background depth resolving\nstrategy and a background-segmentation-guided fusion mechanism. The proposed\nroom-geometry-based background depth resolving strategy leverages the room\nlayout and the depth decoder's output to generate the corresponding background\ndepth map. Then, a background-segmentation-guided fusion strategy derives\nfusion weights for the background and coarse depth maps from the segmentation\ndecoder's predictions. Extensive experimental results on the Stanford2D3D,\nMatterport3D and Structured3D datasets show that our proposed methods can\nachieve significantly superior performance than current open-source methods.\nOur code is available at https://github.com/emiyaning/RGCNet.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.07713", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07713", "abs": "https://arxiv.org/abs/2510.07713", "authors": ["Shuo Yu", "Mingyue Cheng", "Daoyu Wang", "Qi Liu", "Zirui Liu", "Ze Guo", "Xiaoyu Tao"], "title": "MemWeaver: A Hierarchical Memory from Textual Interactive Behaviors for Personalized Generation", "comment": "12 pages, 8 figures", "summary": "The primary form of user-internet engagement is shifting from leveraging\nimplicit feedback signals, such as browsing and clicks, to harnessing the rich\nexplicit feedback provided by textual interactive behaviors. This shift unlocks\na rich source of user textual history, presenting a profound opportunity for a\ndeeper form of personalization. However, prevailing approaches offer only a\nshallow form of personalization, as they treat user history as a flat list of\ntexts for retrieval and fail to model the rich temporal and semantic structures\nreflecting dynamic nature of user interests. In this work, we propose\n\\textbf{MemWeaver}, a framework that weaves the user's entire textual history\ninto a hierarchical memory to power deeply personalized generation. The core\ninnovation of our memory lies in its ability to capture both the temporal\nevolution of interests and the semantic relationships between different\nactivities. To achieve this, MemWeaver builds two complementary memory\ncomponents that both integrate temporal and semantic information, but at\ndifferent levels of abstraction: behavioral memory, which captures specific\nuser actions, and cognitive memory, which represents long-term preferences.\nThis dual-component memory serves as a unified representation of the user,\nallowing large language models (LLMs) to reason over both concrete behaviors\nand abstracted traits. Experiments on the Language Model Personalization (LaMP)\nbenchmark validate the efficacy of MemWeaver. Our code is\navailable\\footnote{https://github.com/fishsure/MemWeaver}.", "AI": {"tldr": "本文提出MemWeaver框架，将用户的文本历史编织成一个分层记忆，以实现深度个性化生成，并在LaMP基准上验证了有效。", "motivation": "文章动机是用户互联网互动方式的变化从隐式反馈转向显式文本反馈，传统的个性化策略对待用户历史方式较浅，未能捕捉深层的时序和语义结构。因此提出MemWeaver框架来更深地挖掘用户的个性化信息。", "method": "本文提出了MemWeaver框架，将用户的完整文本历史编织成一个分层记忆，以支持深度个性化的生成。该记忆的核心创新在于能够捕捉兴趣的时间演变和不同活动之间的语义关系。MemWeaver构建了两种互补的记忆组件：行为记忆和认知记忆，它们在不同的抽象层次上整合时间信息和语义信息，共同为大型语言模型提供了一个统一的用户表示，以支持具体行为和抽象特性的推理。", "result": "实验结果在语言模型个性化（LaMP）基准上验证了MemWeaver的有效性。", "conclusion": "通过MemWeaver框架，作者证明了正在改进的记忆形式和双组分记忆架构能够提高个性化语言模型的性能。"}}
{"id": "2510.07823", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07823", "abs": "https://arxiv.org/abs/2510.07823", "authors": ["Shohei Enomoto"], "title": "Enhancing Visual Prompting through Expanded Transformation Space and Overfitting Mitigation", "comment": "Accepted to NeurIPS2025", "summary": "Visual prompting (VP) has emerged as a promising parameter-efficient\nfine-tuning approach for adapting pre-trained vision models to downstream tasks\nwithout modifying model parameters. Despite offering advantages like negligible\ncomputational overhead and compatibility with black-box models, conventional VP\nmethods typically achieve lower accuracy than other adaptation approaches. Our\nanalysis reveals two critical limitations: the restricted expressivity of\nsimple additive transformation and a tendency toward overfitting when the\nparameter count increases. To address these challenges, we propose ACAVP\n(Affine, Color, and Additive Visual Prompting), which enhances VP's expressive\npower by introducing complementary transformation operations: affine\ntransformation for creating task-specific prompt regions while preserving\noriginal image information, and color transformation for emphasizing\ntask-relevant visual features. Additionally, we identify that overfitting is a\ncritical issue in VP training and introduce TrivialAugment as an effective data\naugmentation, which not only benefits our approach but also significantly\nimproves existing VP methods, with performance gains of up to 12 percentage\npoints on certain datasets. This demonstrates that appropriate data\naugmentation is universally beneficial for VP training. Extensive experiments\nacross twelve diverse image classification datasets with two different model\narchitectures demonstrate that ACAVP achieves state-of-the-art accuracy among\nVP methods, surpasses linear probing in average accuracy, and exhibits superior\nrobustness to distribution shifts, all while maintaining minimal computational\noverhead during inference.", "AI": {"tldr": "研究提出了ACAVP方法，通过引入仿射变换和颜色变换以及数据增强技术，提高了视觉提示技术在图像分类任务中的准确性和鲁棒性。", "motivation": "该研究旨在提升视觉提示方法的准确性和泛化能力，解决其表达能力有限和容易过拟合的问题，提出了一种新的方法 ACAVP，以提高图像分类任务的性能。", "method": "ACAVP (Affine, Color, and Additive Visual Prompting) 是一种增强视觉提示表达能力的方法，通过引入仿射变换和颜色变换来创建任务特定的提示区域并强调与任务相关的视觉特征，解决了传统视觉提示方法表达能力有限和过拟合的问题。此外，还使用了 TrivialAugment 数据增强技术来缓解过拟合问题，这对现有方法也有明显提升。", "result": "在十二个不同的图像分类数据集上，使用两种不同的模型架构，ACAVP 达到了视觉提示方法中的最先进准确率，超过了线性探测的平均准确率，并且在保持最小计算开销的同时，展示出对分布偏移的良好鲁棒性。", "conclusion": "通过使用仿射变换、颜色变换和数据增强技术，ACAVP 方法能够有效提升图像分类任务的性能，并展示出对分布偏移的良好鲁棒性，同时保持较小的计算开销。这表明适当的数据增强对于视觉提示技术训练是普遍有益的。"}}
{"id": "2510.07718", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07718", "abs": "https://arxiv.org/abs/2510.07718", "authors": ["Jiaoyang Li", "Junhao Ruan", "Shengwei Tang", "Saihan Chen", "Kaiyan Chang", "Yuan Ge", "Tong Xiao", "Jingbo Zhu"], "title": "SUBQRAG: sub-question driven dynamic graph rag", "comment": "5 pages, 1 figure", "summary": "Graph Retrieval-Augmented Generation (Graph RAG) effectively builds a\nknowledge graph (KG) to connect disparate facts across a large document corpus.\nHowever, this broad-view approach often lacks the deep structured reasoning\nneeded for complex multi-hop question answering (QA), leading to incomplete\nevidence and error accumulation. To address these limitations, we propose\nSubQRAG, a sub-question-driven framework that enhances reasoning depth. SubQRAG\ndecomposes a complex question into an ordered chain of verifiable\nsub-questions. For each sub-question, it retrieves relevant triples from the\ngraph. When the existing graph is insufficient, the system dynamically expands\nit by extracting new triples from source documents in real time. All triples\nused in the reasoning process are aggregated into a \"graph memory,\" forming a\nstructured and traceable evidence path for final answer generation. Experiments\non three multi-hop QA benchmarks demonstrate that SubQRAG achieves consistent\nand significant improvements, especially in Exact Match scores.", "AI": {"tldr": "提出了一种叫做SubQRAG的框架，通过将复杂问题分解成子问题链来增强多跳问题回答中的深层数理能力。实验表明，这种方法在精确匹配度上显著优于现有方法。", "motivation": "图检索增强生成（Graph RAG）虽然能够有效构建知识图谱，但其宽泛视角的方法往往缺乏处理复杂多跳问题需要的深层次结构化推理，导致证据不完整和错误累积。为了应对这些局限，提出了SubQRAG。", "method": "SubQRAG, 一种基于子问题驱动的框架，通过将复杂问题分解成有序的可验证子问题链来增强推理深度。对于每个子问题，它会从图中检索相关三元组。当现有图不足时，系统会通过实时从源文档中提取新的三元组来动态扩展图。用于推理过程的所有三元组将被整合到'图记忆'中，形成最终答案生成的结构化且可追踪的证据路径。", "result": "在三个多跳问题回答基准测试上的实验表明，SubQRAG 在精确匹配得分方面取得了一致且显著的改进。", "conclusion": "SubQRAG通过分解复杂问题并实时扩展知识图谱，实现了多跳问题回答中的更深层次推理，尤其在精确匹配评分上显著提升。"}}
{"id": "2510.07828", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07828", "abs": "https://arxiv.org/abs/2510.07828", "authors": ["Kaen Kogashi", "Anoop Cherian", "Meng-Yu Jennifer Kuo"], "title": "MMHOI: Modeling Complex 3D Multi-Human Multi-Object Interactions", "comment": null, "summary": "Real-world scenes often feature multiple humans interacting with multiple\nobjects in ways that are causal, goal-oriented, or cooperative. Yet existing 3D\nhuman-object interaction (HOI) benchmarks consider only a fraction of these\ncomplex interactions. To close this gap, we present MMHOI -- a large-scale,\nMulti-human Multi-object Interaction dataset consisting of images from 12\neveryday scenarios. MMHOI offers complete 3D shape and pose annotations for\nevery person and object, along with labels for 78 action categories and 14\ninteraction-specific body parts, providing a comprehensive testbed for\nnext-generation HOI research. Building on MMHOI, we present MMHOI-Net, an\nend-to-end transformer-based neural network for jointly estimating human-object\n3D geometries, their interactions, and associated actions. A key innovation in\nour framework is a structured dual-patch representation for modeling objects\nand their interactions, combined with action recognition to enhance the\ninteraction prediction. Experiments on MMHOI and the recently proposed CORE4D\ndatasets demonstrate that our approach achieves state-of-the-art performance in\nmulti-HOI modeling, excelling in both accuracy and reconstruction quality.", "AI": {"tldr": "本文提出了一种新的MOHOI数据集和相应的MMHOI-Net模型，该模型利用结构化的双补丁表示和动作识别技术实现了多人类物体交互的精确建模和预测。", "motivation": "由于现有的3D人体物体交互数据集仅涵盖了复杂交互行为的一部分，本文提出了一个大规模数据集MMHOI，以填补这一空白。MMHOI提供了12种日常场景的图像，并拥有完整的人体和物体3D形状和姿态注释，以及78个动作类别和14个交互特定体部分的标签，为下一代HOI研究提供了全面的测试平台。", "method": "本文介绍了一种名为MMHOI-Net的端到端变压器神经网络，该网络能够同时估计人类和物体的三维几何形状，它们之间的相互作用以及相关动作。其中，关键创新在于采用了一个结构化双补丁表示法来建模物体及其交互，并结合了动作识别来提高交互预测的准确性。", "result": "实验表明，作者提出的方法在多个数据集上达到了最先进的多人类-物体交互建模性能，不仅在准确性方面表现优异，在重建质量方面也表现出色。", "conclusion": "本文提出了一种新的大规模多人类多物体交互数据集MMHOI和一个名为MMHOI-Net的模型，在多个数据集上达到了最先进的多人类-物体交互建模性能。"}}
{"id": "2510.07736", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07736", "abs": "https://arxiv.org/abs/2510.07736", "authors": ["Cunli Mao", "Xiaofei Gao", "Ran Song", "Shizhu He", "Shengxiang Gao", "Kang Liu", "Zhengtao Yu"], "title": "Multilingual Knowledge Graph Completion via Efficient Multilingual Knowledge Sharing", "comment": "EMNLP 2025, Findings, Long Paper", "summary": "Large language models (LLMs) based Multilingual Knowledge Graph Completion\n(MKGC) aim to predict missing facts by leveraging LLMs' multilingual\nunderstanding capabilities, improving the completeness of multilingual\nknowledge graphs (KGs). However, existing MKGC research underutilizes the\nmultilingual capabilities of LLMs and ignores the shareability of cross-lingual\nknowledge. In this paper, we propose a novel MKGC framework that leverages\nmultilingual shared knowledge to significantly enhance performance through two\ncomponents: Knowledge-level Grouped Mixture of Experts (KL-GMoE) and Iterative\nEntity Reranking (IER). KL-GMoE efficiently models shared knowledge, while IER\nsignificantly enhances its utilization. To evaluate our framework, we\nconstructed a mKG dataset containing 5 languages and conducted comprehensive\ncomparative experiments with existing state-of-the-art (SOTA) MKGC method. The\nexperimental results demonstrate that our framework achieves improvements of\n5.47%, 3.27%, and 1.01% in the Hits@1, Hits@3, and Hits@10 metrics,\nrespectively, compared with SOTA MKGC method. Further experimental analysis\nrevealed the properties of knowledge sharing in settings of unseen and\nunbalanced languages. We have released the dataset and code for our work on\nhttps://github.com/gaoxiaofei07/KL-GMoE.", "AI": {"tldr": "Proposed a novel MKGC framework with KL-GMoE and IER, achieving notable performance improvements by sharing multilingual knowledge, outperforming existing SOTA MKGC methods in extensive experiments.", "motivation": "To address the underutilization of LLMs' multilingual capabilities and the lack of cross-lingual knowledge shareability in current MKGC research.", "method": "Large language models (LLMs) based Multilingual Knowledge Graph Completion (MKGC) utilizing multilingual shared knowledge. The approach consists of two key components: Knowledge-level Grouped Mixture of Experts (KL-GMoE) and Iterative Entity Reranking (IER).", "result": "Experimental evaluation on a multilingual KG dataset (containing 5 languages) showed performance improvements of 5.47%, 3.27%, and 1.01% in Hits@1, Hits@3, and Hits@10 metrics, respectively, compared to the state-of-the-art MKGC method.", "conclusion": "The proposed MKGC framework, leveraging multilingual shared knowledge through KL-GMoE and IER, significantly enhances the performance of multilingual KG completion, especially in handling unseen and unbalanced languages."}}
{"id": "2510.07830", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07830", "abs": "https://arxiv.org/abs/2510.07830", "authors": ["Houqiang Zhong", "Zhenglong Wu", "Sihua Fu", "Zihan Zheng", "Xin Jin", "Xiaoyun Zhang", "Li Song", "Qiang Hu"], "title": "PrismGS: Physically-Grounded Anti-Aliasing for High-Fidelity Large-Scale 3D Gaussian Splatting", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has recently enabled real-time photorealistic\nrendering in compact scenes, but scaling to large urban environments introduces\nsevere aliasing artifacts and optimization instability, especially under\nhigh-resolution (e.g., 4K) rendering. These artifacts, manifesting as\nflickering textures and jagged edges, arise from the mismatch between Gaussian\nprimitives and the multi-scale nature of urban geometry. While existing\n``divide-and-conquer'' pipelines address scalability, they fail to resolve this\nfidelity gap. In this paper, we propose PrismGS, a physically-grounded\nregularization framework that improves the intrinsic rendering behavior of 3D\nGaussians. PrismGS integrates two synergistic regularizers. The first is\npyramidal multi-scale supervision, which enforces consistency by supervising\nthe rendering against a pre-filtered image pyramid. This compels the model to\nlearn an inherently anti-aliased representation that remains coherent across\ndifferent viewing scales, directly mitigating flickering textures. This is\ncomplemented by an explicit size regularization that imposes a\nphysically-grounded lower bound on the dimensions of the 3D Gaussians. This\nprevents the formation of degenerate, view-dependent primitives, leading to\nmore stable and plausible geometric surfaces and reducing jagged edges. Our\nmethod is plug-and-play and compatible with existing pipelines. Extensive\nexperiments on MatrixCity, Mill-19, and UrbanScene3D demonstrate that PrismGS\nachieves state-of-the-art performance, yielding significant PSNR gains around\n1.5 dB against CityGaussian, while maintaining its superior quality and\nrobustness under demanding 4K rendering.", "AI": {"tldr": "PrismGS方法改进了3D Gaussian Splatting在大规模城市环境中的渲染效果，通过金字塔多尺度监督和显式大小正则化减少了锯齿边缘和光影闪烁，提升了4K渲染质量。", "motivation": "尽管'分而治之'的管道处理方法已经解决了大规模场景的可伸缩性问题，但对于抗锯齿和优化稳定性问题尚不能有效解决。这些问题是由于高斯原始几何体与城市几何体的多尺度性质不匹配而导致的。为了解决这些问题，我们需要改进3D高斯的渲染行为，以适应大规模城市环境中的渲染。", "method": "PrismGS采用了一种基于物理的正则化框架，以改进3D高斯粒子在大规模城市环境下的渲染效果。该方法结合了两种协同的正则化手段：金字塔多尺度监督和显式的大小正则化。前者通过监督渲染结果与预过滤图像金字塔的一致性来学习抗锯齿表示，后者通过对3D高斯粒子的尺寸设置物理合理性的下限来防止视图依赖异常形态的形成，从而提高几何表面的稳定性和减少锯齿边缘。", "result": "实验表明，PrismGS在MatrixCity, Mill-19和UrbanScene3D数据集上表现优越，相比CityGaussian方法有了大约1.5 dB的PSNR增益，在苛刻的4K渲染条件下仍能保持高质量和鲁棒性。", "conclusion": "PrismGS方法能有效提高3D高斯渲染的稳定性和质量，特别在大规模城市环境中。其性能优越，兼容现有的渲染管道，且在4K等高分辨率渲染条件下具有显著的效果。"}}
{"id": "2510.07737", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07737", "abs": "https://arxiv.org/abs/2510.07737", "authors": ["Fu Chen", "Peng Wang", "Xiyin Li", "Wen Li", "Shichi Lei", "Dongdong Xiang"], "title": "ToolExpander: Extending the Frontiers of Tool-Using Reinforcement Learning to Weak LLMs", "comment": null, "summary": "Training Large Language Models (LLMs) with Group Relative Policy Optimization\n(GRPO) encounters a significant challenge: models often fail to produce\naccurate responses, particularly in small-scale architectures. This limitation\nnot only diminishes performance improvements and undermines the potential of\nGRPO but also frequently leads to mid-training collapse, adversely affecting\nstability and final efficacy. To address these issues, we propose ToolExpander,\na novel framework that advances tool-oriented reinforcement learning for\nresource-constrained LLMs through two key innovations:(1) Dynamic Multi-Round\nHard Sampling, which dynamically substitutes challenging samples(those without\ncorrect outputs over 10 rollouts) with high-quality few-shot demonstrations\nduring training, coupled with an exponential learning rate decay strategy to\nmitigate oscillations;(2) Self-Exemplifying Thinking, an enhanced GRPO\nframework that eliminates KL divergence and incorporates adjusted clipping\ncoefficients, encouraging models to autonomously generate and analyze few-shot\nexamples via a minimal additional reward (0.01).Experimental results\ndemonstrate that ToolExpander significantly enhances tool-using capabilities in\nLLMs, especially in weaker small-scale models, improving both training\nstability and overall performance.", "AI": {"tldr": "为了解决GRPO训练大规模语言模型时的问题，提出ToolExpander框架，通过两种创新改进面向工具的强化学习，实验表明该方法显著提高小规模模型的性能和稳定性。", "motivation": "解决大规模语言模型在使用组相对策略优化（GRPO）时遇到的问题，尤其是小规模架构中难以产生准确响应的问题，这些问题会影响性能提升和模型训练的稳定性。", "method": "ToolExpander框架通过两种创新来改进面向工具的强化学习：1) 动态多轮硬采样，使用高质量的少样本演示替换训练中难以处理的样本，并结合指数学习率衰减策略来减少振荡；2) 自举证明思考，增强的GRPO框架，消除了KL散度并调整了剪辑系数，鼓励模型自主生成和分析少样本示例。", "result": "实验结果表明，ToolExpander显著提高了大规模语言模型的工具使用能力，特别是在较弱的小规模模型中，提高了训练稳定性和整体性能。", "conclusion": "ToolExpander框架在提高小规模语言模型工具使用能力和训练稳定性方面显示出显著效果，从而解决GRPO训练中的常见问题。"}}
{"id": "2510.07837", "categories": ["cs.CV", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.07837", "abs": "https://arxiv.org/abs/2510.07837", "authors": ["Harsh Kavediya", "Vighnesh Nayak", "Bheeshm Sharma", "Balamurugan Palaniappan"], "title": "IsoSignVid2Aud: Sign Language Video to Audio Conversion without Text Intermediaries", "comment": "Accepted in AIML-Systems-2025", "summary": "Sign language to spoken language audio translation is important to connect\nthe hearing- and speech-challenged humans with others. We consider sign\nlanguage videos with isolated sign sequences rather than continuous grammatical\nsigning. Such videos are useful in educational applications and sign prompt\ninterfaces. Towards this, we propose IsoSignVid2Aud, a novel end-to-end\nframework that translates sign language videos with a sequence of possibly\nnon-grammatic continuous signs to speech without requiring intermediate text\nrepresentation, providing immediate communication benefits while avoiding the\nlatency and cascading errors inherent in multi-stage translation systems. Our\napproach combines an I3D-based feature extraction module with a specialized\nfeature transformation network and an audio generation pipeline, utilizing a\nnovel Non-Maximal Suppression (NMS) algorithm for the temporal detection of\nsigns in non-grammatic continuous sequences. Experimental results demonstrate\ncompetitive performance on ASL-Citizen-1500 and WLASL-100 datasets with Top-1\naccuracies of 72.01\\% and 78.67\\%, respectively, and audio quality metrics\n(PESQ: 2.67, STOI: 0.73) indicating intelligible speech output. Code is\navailable at: https://github.com/BheeshmSharma/IsoSignVid2Aud_AIMLsystems-2025.", "AI": {"tldr": "本文提出了一种新的端到端框架IsoSignVid2Aud，用于翻译连续的但非语法的手语视频序列到语音，实验显示良好的准确性和音频质量。", "motivation": "手语到口语的音频翻译对于连通听力和言语障碍人士和其他人非常重要。我们考虑具有独立手势序列的手语视频，而非连续的语法规则手势，此类视频在教育应用和手势提示界面有用。", "method": "IsoSignVid2Aud采用了一种端到端的框架，该框架结合了基于I3D的特征提取模块、专门的特征转换网络和音频生成管道，使用一种新颖的非极大值抑制(NMS)算法来检测非语法连续序列中的手势。", "result": "实验结果表明，在ASL-Citizen-1500和WLASL-100数据集上，Top-1准确率分别为72.01%和78.67%，音频质量指标（PESQ：2.67，STOI：0.73）表明输出的语音具有可理解性。", "conclusion": "本研究证明了IsoSignVid2Aud框架在手语视频转换为口语音频方面的有效性，特别是在直接通信中避免了多阶段翻译系统的延迟和级联错误。"}}
{"id": "2510.07743", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07743", "abs": "https://arxiv.org/abs/2510.07743", "authors": ["Tianci Liu", "Ran Xu", "Tony Yu", "Ilgee Hong", "Carl Yang", "Tuo Zhao", "Haoyu Wang"], "title": "OpenRubrics: Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment", "comment": "The first two authors contributed equally", "summary": "Reward modeling lies at the core of reinforcement learning from human\nfeedback (RLHF), yet most existing reward models rely on scalar or pairwise\njudgments that fail to capture the multifaceted nature of human preferences.\nRecent studies have explored rubrics-as-rewards (RaR) that uses structured\nnatural language criteria that capture multiple dimensions of response quality.\nHowever, producing rubrics that are both reliable and scalable remains a key\nchallenge. In this work, we introduce OpenRubrics, a diverse, large-scale\ncollection of (prompt, rubric) pairs for training rubric-generation and\nrubric-based reward models. To elicit discriminative and comprehensive\nevaluation signals, we introduce Contrastive Rubric Generation (CRG), which\nderives both hard rules (explicit constraints) and principles (implicit\nqualities) by contrasting preferred and rejected responses. We further improve\nreliability by enforcing preference-label consistency via rejection sampling to\nremove noisy rubrics. Across multiple reward-modeling benchmarks, our\nrubric-based reward model, Rubric-RM, surpasses strong size-matched baselines\nby 6.8%. These gains transfer to policy models on instruction-following and\nbiomedical benchmarks. Our results show that rubrics provide scalable alignment\nsignals that narrow the gap between costly human evaluation and automated\nreward modeling, enabling a new principle-driven paradigm for LLM alignment.", "AI": {"tldr": "本文提出了OpenRubrics和对比式评分生成CRG，以产生可靠且可扩展的rubric，并借此改进奖励模型，显著提高了模型性能。", "motivation": "当前的奖励模型大多依赖于标量或成对的判断，无法捕捉人类偏好的多方面性。因此，研究基于评分标准奖励模型（RaR）来捕捉多个维度的响应质量的多方面挑战，生产可靠且可扩展的评分标准是一项关键任务。", "method": "我们提出了OpenRubrics，这是一个多样化的、大规模的(prompt, rubric)对集合，用于训练rubric生成和基于rubric的奖励模型。我们引入了对比式评分生成(CRG)，通过对比优选和被拒的回答来衍生出明确的规则和隐含的原则。此外，我们通过拒绝采样来确保偏好标签的一致性，从而提高可靠性。", "result": "在多个奖励模型基准测试中，我们的基于rubric的奖励模型Rubric-RM超过了尺寸匹配的强大基线模型，提高了6.8%。这些改进也应用到了指令遵循和生物医学基准测试的策略模型上。", "conclusion": "研究表明，评分标准提供了可扩展的一致信号，缩小了代价高昂的人类评价和自动奖励建模之间的差距，从而为LLM对其对齐提供了一个新的以原则为导向的范例。"}}
{"id": "2510.07839", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07839", "abs": "https://arxiv.org/abs/2510.07839", "authors": ["Yijie Gao", "Houqiang Zhong", "Tianchi Zhu", "Zhengxue Cheng", "Qiang Hu", "Li Song"], "title": "AlignGS: Aligning Geometry and Semantics for Robust Indoor Reconstruction from Sparse Views", "comment": null, "summary": "The demand for semantically rich 3D models of indoor scenes is rapidly\ngrowing, driven by applications in augmented reality, virtual reality, and\nrobotics. However, creating them from sparse views remains a challenge due to\ngeometric ambiguity. Existing methods often treat semantics as a passive\nfeature painted on an already-formed, and potentially flawed, geometry. We\nposit that for robust sparse-view reconstruction, semantic understanding\ninstead be an active, guiding force. This paper introduces AlignGS, a novel\nframework that actualizes this vision by pioneering a synergistic, end-to-end\noptimization of geometry and semantics. Our method distills rich priors from 2D\nfoundation models and uses them to directly regularize the 3D representation\nthrough a set of novel semantic-to-geometry guidance mechanisms, including\ndepth consistency and multi-faceted normal regularization. Extensive\nevaluations on standard benchmarks demonstrate that our approach achieves\nstate-of-the-art results in novel view synthesis and produces reconstructions\nwith superior geometric accuracy. The results validate that leveraging semantic\npriors as a geometric regularizer leads to more coherent and complete 3D models\nfrom limited input views. Our code is avaliable at\nhttps://github.com/MediaX-SJTU/AlignGS .", "AI": {"tldr": "论文提出了一种全新的方法_aligngs，该方法可以同时优化3D场景的几何形状和语义信息，从而更好地从有限视角重建出更完整且连贯的3D模型。", "motivation": "现有的方法往往将语义视为已经形成的、可能是有缺陷的几何模型上的被动特征。该论文提出，为了实现健壮的稀疏视图重建，语义理解应该成为积极的指导力量。", "method": "AlignGS框架采用了一种新颖的协同端到端优化几何和语义的方法，通过从2D基础模型中提炼丰富的先验知识，使用一系列新的语义到几何的指导机制（包括深度一致性和多方面法线规则化）直接规整3D表示。", "result": "实验结果表明，该方法在新视图合成方面达到了最先进的结果，并产生了具有更高几何精度的重建模型。结果验证了利用语义先验作为几何规整器可以生成更具连贯性和完整性的3D模型。", "conclusion": "研究结果表明，通过从有限视角输入中利用语义先验作为几何规整器，能够生成更连贯和完整的3D模型。这种方法对于提出内场景的3D模型重建有重要的意义。"}}
{"id": "2510.07745", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07745", "abs": "https://arxiv.org/abs/2510.07745", "authors": ["Runyang You", "Yongqi Li", "Meng Liu", "Wenjie Wang", "Liqiang Nie", "Wenjie Li"], "title": "Parallel Test-Time Scaling for Latent Reasoning Models", "comment": null, "summary": "Parallel test-time scaling (TTS) is a pivotal approach for enhancing large\nlanguage models (LLMs), typically by sampling multiple token-based\nchains-of-thought in parallel and aggregating outcomes through voting or\nsearch. Recent advances in latent reasoning, where intermediate reasoning\nunfolds in continuous vector spaces, offer a more efficient alternative to\nexplicit Chain-of-Thought, yet whether such latent models can similarly benefit\nfrom parallel TTS remains open, mainly due to the absence of sampling\nmechanisms in continuous space, and the lack of probabilistic signals for\nadvanced trajectory aggregation. \\ This work enables parallel TTS for latent\nreasoning models by addressing the above issues. For sampling, we introduce two\nuncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive\nGaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM)\ntrained with step-wise contrastive objective to score and guide latent\nreasoning. Extensive experiments and visualization analyses show that both\nsampling strategies scale effectively with compute and exhibit distinct\nexploration dynamics, while LatentRM enables effective trajectory selection.\nTogether, our explorations open a new direction for scalable inference in\ncontinuous spaces. Code released at https://github.com/YRYangang/LatentTTS.", "AI": {"tldr": "论文提出了一种用于潜在推理模型的并行测试时间扩展方法，引入了新的采样策略和聚合技术，从而使得潜在推理模型也能受益于并行 TTS。", "motivation": "由于缺乏在连续空间中的采样机制和高级轨迹聚合的概率信号，现有的隐式推理模型能否像显式的链式思考模型一样通过并行测试时间扩展（TTS）获益仍然是开放问题。本文旨在解决这些问题，并使潜在推理模型也能从并行TTS中受益。", "method": "此论文引入了两种基于不确定性的采样策略：蒙特卡洛 dropout 和加性高斯噪声，以解决在连续空间中缺少采样机制的问题。同时设计了一个基于对比目标训练的潜在奖励模型（LatentRM）来评估和指导潜在推理。", "result": "广泛的实验表明，两种采样策略都能有效地扩展计算资源，并展现出不同的探索动态特性。LatentRM 能够实现有效的轨迹选择。", "conclusion": "该论文的研究为在连续空间中实现可扩展推理开辟了新的研究方向。"}}
{"id": "2510.07853", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07853", "abs": "https://arxiv.org/abs/2510.07853", "authors": ["Thomas Lautenschlager", "Nils Friederich", "Angelo Jovin Yamachui Sitcheu", "Katja Nau", "Gaëlle Hayot", "Thomas Dickmeis", "Ralf Mikut"], "title": "Self-Supervised Learning Strategies for a Platform to Test the Toxicity of New Chemicals and Materials", "comment": null, "summary": "High-throughput toxicity testing offers a fast and cost-effective way to test\nlarge amounts of compounds. A key component for such systems is the automated\nevaluation via machine learning models. In this paper, we address critical\nchallenges in this domain and demonstrate how representations learned via\nself-supervised learning can effectively identify toxicant-induced changes. We\nprovide a proof-of-concept that utilizes the publicly available EmbryoNet\ndataset, which contains ten zebrafish embryo phenotypes elicited by various\nchemical compounds targeting different processes in early embryonic\ndevelopment. Our analysis shows that the learned representations using\nself-supervised learning are suitable for effectively distinguishing between\nthe modes-of-action of different compounds. Finally, we discuss the integration\nof machine learning models in a physical toxicity testing device in the context\nof the TOXBOX project.", "AI": {"tldr": "该论文探讨了利用自监督学习在毒性测试中识别化合物诱导变化的方法，并通过EmbryoNet数据集证明了这种方法的有效性，同时讨论了在TOXBOX项目中毒性测试设备中集成机器学习模型的可能性。", "motivation": "研究的动机在于解决高通量毒性测试中的关键挑战，特别是利用机器学习模型进行自动化评估的挑战。通过探索自监督学习方法的应用，研究希望找到一种有效的方法来识别毒物诱导的变化。", "method": "该论文利用自监督学习来生成能够区分不同化合物作用模式的表现形式。采用的方法基于EmbryoNet数据集，这个数据包含由不同的化学成分引起的十个斑马鱼胚胎表型的变化。", "result": "研究表明，通过自监督学习学习到的表现形式能够有效地识别出不同化合物的毒性作用模式。这为解决高通量毒性测试中的关键问题提供了前景。", "conclusion": "论文结论指出，自监督学习方法在区分不同化合物毒性作用模式方面是有潜力的，这为进一步整合机器学习模型到毒性测试设备提供了可能性。"}}
{"id": "2510.07761", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07761", "abs": "https://arxiv.org/abs/2510.07761", "authors": ["Nishant Balepur", "Atrey Desai", "Rachel Rudinger"], "title": "Test-Time Reasoners Are Strategic Multiple-Choice Test-Takers", "comment": "In-progress Preprint", "summary": "Large language models (LLMs) now give reasoning before answering, excelling\nin tasks like multiple-choice question answering (MCQA). Yet, a concern is that\nLLMs do not solve MCQs as intended, as work finds LLMs sans reasoning succeed\nin MCQA without using the question, i.e., choices-only. Such partial-input\nsuccess is often deemed problematic, but reasoning traces could reveal if these\nstrategies are truly shallow in choices-only settings. To study these\nstrategies, reasoning LLMs solve MCQs in full and choices-only inputs;\ntest-time reasoning often boosts accuracy on full and in choices-only half the\ntime. While possibly due to shallow shortcuts, choices-only success is barely\naffected by the length of reasoning traces, and after finding traces pass\nfaithfulness tests, we show they use less problematic strategies like inferring\nmissing questions. In all, we challenge claims that partial-input success is\nalways a flaw, so we discuss how reasoning traces could separate problematic\ndata from less problematic reasoning.", "AI": {"tldr": "研究探讨了具备推理能力的大语言模型在解决多项选择题时的策略，尤其是在仅使用选项的情况下，发现其成功率并未显著下降，并且这些模型能够通过可信度测试，表明它们采用了较少有问题的策略。", "motivation": "尽管已有研究表明，不使用推理的大语言模型在仅使用选项的情况下也能在多项选择题上取得成功，但这种成功的可靠性受到质疑。这项研究旨在通过分析具备推理能力的大语言模型的推理路径，探究它们在仅使用选项时采用的策略是否为浅层策略。", "method": "通过让具备推理能力的大语言模型在全输入和仅选项输入下解决多项选择题，来研究它们的解题策略。", "result": "具备推理能力的模型在解决多项选择题时，即使在仅使用选项的情况下也能取得一定程度的成功。这种情况下，推理路径的长度对其成功率影响不大，并且这些模型能够通过可信度测试，表明它们采用了较少有问题的策略，例如推断丢失的问题。", "conclusion": "研究挑战了部分输入成功总是缺陷的观点，并讨论了如何通过推理路径来区分有问题的数据和较少有问题的推理。"}}
{"id": "2510.07856", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07856", "abs": "https://arxiv.org/abs/2510.07856", "authors": ["Haochen Yu", "Qiankun Liu", "Hongyuan Liu", "Jianfei Jiang", "Juntao Lyu", "Jiansheng Chen", "Huimin Ma"], "title": "XYZCylinder: Feedforward Reconstruction for Driving Scenes Based on A Unified Cylinder Lifting Method", "comment": "Project page: https://yuyuyu223.github.io/XYZCYlinder-projectpage/", "summary": "Recently, more attention has been paid to feedforward reconstruction\nparadigms, which mainly learn a fixed view transformation implicitly and\nreconstruct the scene with a single representation. However, their\ngeneralization capability and reconstruction accuracy are still limited while\nreconstructing driving scenes, which results from two aspects: (1) The fixed\nview transformation fails when the camera configuration changes, limiting the\ngeneralization capability across different driving scenes equipped with\ndifferent camera configurations. (2) The small overlapping regions between\nsparse views of the $360^\\circ$ panorama and the complexity of driving scenes\nincrease the learning difficulty, reducing the reconstruction accuracy. To\nhandle these difficulties, we propose \\textbf{XYZCylinder}, a feedforward model\nbased on a unified cylinder lifting method which involves camera modeling and\nfeature lifting. Specifically, to improve the generalization capability, we\ndesign a Unified Cylinder Camera Modeling (UCCM) strategy, which avoids the\nlearning of viewpoint-dependent spatial correspondence and unifies different\ncamera configurations with adjustable parameters. To improve the reconstruction\naccuracy, we propose a hybrid representation with several dedicated modules\nbased on newly designed Cylinder Plane Feature Group (CPFG) to lift 2D image\nfeatures to 3D space. Experimental results show that XYZCylinder achieves\nstate-of-the-art performance under different evaluation settings, and can be\ngeneralized to other driving scenes in a zero-shot manner. Project page:\n\\href{https://yuyuyu223.github.io/XYZCYlinder-projectpage/}{here}.", "AI": {"tldr": "XYZCylinder model is developed to improve driving scene reconstruction by adapting to different camera configurations and using a cylinder lifting method for better 2D to 3D feature transformation.", "motivation": "To address the limitations of existing feedforward reconstruction paradigms regarding generalization and accuracy in driving scenes, mainly caused by fixed view transformations and complex scenes.", "method": "The paper proposes XYZCylinder, a feedforward model that uses a unified cylinder lifting method involving camera modeling (Unified Cylinder Camera Modeling - UCCM) and feature lifting (Cylinder Plane Feature Group - CPFG) to improve generalization capability and reconstruction accuracy for driving scenes.", "result": "The model achieves state-of-the-art performance under various evaluation settings and can generalize to other driving scenes without additional training.", "conclusion": "The proposed XYZCylinder model demonstrates significant improvements in both generalization capability and reconstruction accuracy for driving scenes, showcasing its applicability to a wide range of scenarios."}}
{"id": "2510.07768", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07768", "abs": "https://arxiv.org/abs/2510.07768", "authors": ["Murong Yue", "Zhiwei Liu", "Liangwei Yang", "Jianguo Zhang", "Zuxin Liu", "Haolin Chen", "Ziyu Yao", "Silvio Savarese", "Caiming Xiong", "Shelby Heinecke", "Huan Wang"], "title": "ToolLibGen: Scalable Automatic Tool Creation and Aggregation for LLM Reasoning", "comment": null, "summary": "Large Language Models (LLMs) equipped with external tools have demonstrated\nenhanced performance on complex reasoning tasks. The widespread adoption of\nthis tool-augmented reasoning is hindered by the scarcity of domain-specific\ntools. For instance, in domains such as physics question answering, suitable\nand specialized tools are often missing. Recent work has explored automating\ntool creation by extracting reusable functions from Chain-of-Thought (CoT)\nreasoning traces; however, these approaches face a critical scalability\nbottleneck. As the number of generated tools grows, storing them in an\nunstructured collection leads to significant retrieval challenges, including an\nexpanding search space and ambiguity between function-related tools. To address\nthis, we propose a systematic approach to automatically refactor an\nunstructured collection of tools into a structured tool library. Our system\nfirst generates discrete, task-specific tools and clusters them into\nsemantically coherent topics. Within each cluster, we introduce a multi-agent\nframework to consolidate scattered functionalities: a code agent refactors code\nto extract shared logic and creates versatile, aggregated tools, while a\nreviewing agent ensures that these aggregated tools maintain the complete\nfunctional capabilities of the original set. This process transforms numerous\nquestion-specific tools into a smaller set of powerful, aggregated tools\nwithout loss of functionality. Experimental results demonstrate that our\napproach significantly improves tool retrieval accuracy and overall reasoning\nperformance across multiple reasoning tasks. Furthermore, our method shows\nenhanced scalability compared with baselines as the number of question-specific\nincreases.", "AI": {"tldr": "本文提出的方法可以自动将大量针对特定问题的工具整合成少数量多功能的聚合工具，提高工具检索准确性和整体推理性能。", "motivation": "广泛采用工具增强推理受到领域特定工具稀缺的限制。特别是，对于推理任务如物理问答，合适的特定工具往往缺失。现有工作面临生成工具数量增加带来的存储和检索挑战。", "method": "本文提出了一种系统的方法，可以将无结构的工具集合自动重构为结构化的工具库。系统首先生成特定任务的离散工具，并将它们聚类为语义连贯的主题。在每个聚类中，引入了一个多代理框架来整合分散的功能：代码代理重构代码以提取共享逻辑并创建多功能的聚合工具，而审核代理确保这些聚合工具保持原有完整功能。", "result": "实验结果表明，本文方法在多种推理任务中显著提高了工具检索准确性和整体推理性能。此外，随着特定问题数量的增加，本文方法相较于基线方法展示出增强的可扩展性。", "conclusion": "本文提出的方法实现将无结构工具集自动重构为结构化的工具库，显著改善了工具检索性能和推理性能，并在可扩展性方面优于现有方法。"}}
{"id": "2510.07915", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07915", "abs": "https://arxiv.org/abs/2510.07915", "authors": ["Peiran Wu", "Zhuorui Yu", "Yunze Liu", "Chi-Hao Wu", "Enmin Zhou", "Junxiao Shen"], "title": "MARC: Memory-Augmented RL Token Compression for Efficient Video Understanding", "comment": null, "summary": "The rapid progress of large language models (LLMs) has laid the foundation\nfor multimodal models. However, visual language models (VLMs) still face heavy\ncomputational costs when extended from images to videos due to high frame rates\nand long durations. Token compression is a promising solution, yet most\nexisting training-free methods cause information loss and performance\ndegradation. To overcome this, we propose \\textbf{Memory-Augmented\nReinforcement Learning-based Token Compression (MARC)}, which integrates\nstructured retrieval and RL-based distillation. MARC adopts a\n\\textit{retrieve-then-compress} strategy using a \\textbf{Visual Memory\nRetriever (VMR)} to select key clips and a \\textbf{Compression Group Relative\nPolicy Optimization (C-GRPO)} framework to distil reasoning ability from a\nteacher to a student model. Experiments on six video benchmarks show that MARC\nachieves near-baseline accuracy using only one frame's tokens -- reducing\nvisual tokens by \\textbf{95\\%}, GPU memory by \\textbf{72\\%}, and latency by\n\\textbf{23.9\\%}. This demonstrates its potential for efficient, real-time video\nunderstanding in resource-constrained settings such as video QA, surveillance,\nand autonomous driving.", "AI": {"tldr": "MARC is a novel approach that effectively condenses visual tokens in videos with reinforcement learning and memory augmentation, resulting in reduced computational resources and improved efficiency without notable performance loss.", "motivation": "The primary motivation is to address the computational inefficiencies encountered by visual language models (VLMs) in handling videos characterized by high frame rates and extended durations, especially in resource-limited settings.", "method": "Memory-Augmented Reinforcement Learning-based Token Compression (MARC) is introduced, forming a retrieve-then-compress strategy that utilizes the Visual Memory Retriever (VMR) for selecting key clips and Compression Group Relative Policy Optimization (C-GRPO) for enhancing student model reasoning through teacher model distillation.", "result": "MARC accomplishes near-par performance to benchmarks with a significant reduction in visual tokens (95%), a decrease in GPU memory usage (72%), and latency (23.9%).", "conclusion": "The findings indicate that MARC has the potential to enable more efficient video understanding, particularly suitable for applications like real-time video questioning and answering, surveillance, and autonomous driving."}}
{"id": "2510.07774", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07774", "abs": "https://arxiv.org/abs/2510.07774", "authors": ["Youliang Yuan", "Qiuyang Mang", "Jingbang Chen", "Hong Wan", "Xiaoyuan Liu", "Junjielong Xu", "Jen-tse Huang", "Wenxuan Wang", "Wenxiang Jiao", "Pinjia He"], "title": "Curing Miracle Steps in LLM Mathematical Reasoning with Rubric Rewards", "comment": "25 pages, 11 figures, 6 Tables", "summary": "Large language models for mathematical reasoning are typically trained with\noutcome-based rewards, which credit only the final answer. In our experiments,\nwe observe that this paradigm is highly susceptible to reward hacking, leading\nto a substantial overestimation of a model's reasoning ability. This is\nevidenced by a high incidence of false positives - solutions that reach the\ncorrect final answer through an unsound reasoning process. Through a systematic\nanalysis with human verification, we establish a taxonomy of these failure\nmodes, identifying patterns like Miracle Steps - abrupt jumps to a correct\noutput without a valid preceding derivation. Probing experiments suggest a\nstrong association between these Miracle Steps and memorization, where the\nmodel appears to recall the answer directly rather than deriving it. To\nmitigate this systemic issue, we introduce the Rubric Reward Model (RRM), a\nprocess-oriented reward function that evaluates the entire reasoning trajectory\nagainst problem-specific rubrics. The generative RRM provides fine-grained,\ncalibrated rewards (0-1) that explicitly penalize logical flaws and encourage\nrigorous deduction. When integrated into a reinforcement learning pipeline,\nRRM-based training consistently outperforms outcome-only supervision across\nfour math benchmarks. Notably, it boosts Verified Pass@1024 on AIME2024 from\n26.7% to 62.6% and reduces the incidence of Miracle Steps by 71%. Our work\ndemonstrates that rewarding the solution process is crucial for building models\nthat are not only more accurate but also more reliable.", "AI": {"tldr": "本文指出针对数学理性的大模型利用基于结果的奖励方法训练会高估其推理能力，因此提出了基于评分标准的RRM过程奖励方法，提高了模型的准确性和可靠性。", "motivation": "传统的基于结果的奖励训练方法容易导致奖励作弊，会导致模型的推理能力被高估。通过高频率的假阳性，解决方案通过不正当的推理过程也能得出正确答案。", "method": "系统性分析结合人工验证，提出了一种针对解决方案过程的评分奖励模型（RRM），该模型会根据问题特定的评分标准来评估整个推理过程，并给予精确的奖励。通过集成到强化学习管道中，这种方法可以在四个数学基准测试中持续优于仅基于结果的监督方法。", "result": "采用RRM奖励模型训练的大模型显著提高了准确性，减少了奇迹步骤的发生，具体提高了AIME2024的通过率，并减少71%的奇迹步骤。", "conclusion": "大模型估算在数学推理上的真实性与准确性，决定性因素在于奖励过程而非只是奖励结果，采用RRM模型能提高模型的准确度和可靠性。"}}
{"id": "2510.07927", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07927", "abs": "https://arxiv.org/abs/2510.07927", "authors": ["Qunyi Zhang", "Songan Zhang", "Jinbao Wang", "Xiaoning Lei", "Guoyang Xie", "Guannan Jiang", "Zhichao Lu"], "title": "ASBench: Image Anomalies Synthesis Benchmark for Anomaly Detection", "comment": null, "summary": "Anomaly detection plays a pivotal role in manufacturing quality control, yet\nits application is constrained by limited abnormal samples and high manual\nannotation costs. While anomaly synthesis offers a promising solution, existing\nstudies predominantly treat anomaly synthesis as an auxiliary component within\nanomaly detection frameworks, lacking systematic evaluation of anomaly\nsynthesis algorithms. Current research also overlook crucial factors specific\nto anomaly synthesis, such as decoupling its impact from detection,\nquantitative analysis of synthetic data and adaptability across different\nscenarios. To address these limitations, we propose ASBench, the first\ncomprehensive benchmarking framework dedicated to evaluating anomaly synthesis\nmethods. Our framework introduces four critical evaluation dimensions: (i) the\ngeneralization performance across different datasets and pipelines (ii) the\nratio of synthetic to real data (iii) the correlation between intrinsic metrics\nof synthesis images and anomaly detection performance metrics , and (iv)\nstrategies for hybrid anomaly synthesis methods. Through extensive experiments,\nASBench not only reveals limitations in current anomaly synthesis methods but\nalso provides actionable insights for future research directions in anomaly\nsynthesis", "AI": {"tldr": "该论文提出了ASBench，这是首个专门用于评估异常合成方法的综合基准框架，该框架可以帮助我们理解现有的异常合成方法的优势和劣势，并为未来的研究指明方向。", "motivation": "现有的异常检测研究将异常合成视为一个辅助组件，很少有系统的异常合成算法评估。并且当前的研究忽视了某些特定的异常合成因素，例如将其影响与检测分离，合成数据的定量分析以及不同场景下的适应性。", "method": "提出ASBench，这是一个专门用于评估异常合成方法的综合基准框架，包含四个关键评估维度：(i) 通用性能跨不同数据集和管道(ii) 合成数据与真实数据的比例(iii) 合成图像的内在指标与异常检测性能指标间的相关性，(iv) 混合异常合成方法的策略。", "result": "通过广泛的实验，ASBench不仅揭示了当前异常合成方法的局限性，还为未来的异常合成研究提供了实用见解。", "conclusion": "通过ASBench，研究人员可以更好地评估和改进现有的异常合成方法，从而有利于制造业的质量控制。"}}
{"id": "2510.07775", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07775", "abs": "https://arxiv.org/abs/2510.07775", "authors": ["Omar Mahmoud", "Ali Khalil", "Buddhika Laknath Semage", "Thommen George Karimpanal", "Santu Rana"], "title": "The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs", "comment": null, "summary": "Hallucination in large language models (LLMs) has been widely studied in\nrecent years, with progress in both detection and mitigation aimed at improving\ntruthfulness. Yet, a critical side effect remains largely overlooked: enhancing\ntruthfulness can negatively impact safety alignment. In this paper, we\ninvestigate this trade-off and show that increasing factual accuracy often\ncomes at the cost of weakened refusal behavior. Our analysis reveals that this\narises from overlapping components in the model that simultaneously encode\nhallucination and refusal information, leading alignment methods to suppress\nfactual knowledge unintentionally. We further examine how fine-tuning on benign\ndatasets, even when curated for safety, can degrade alignment for the same\nreason. To address this, we propose a method that disentangles refusal-related\nfeatures from hallucination features using sparse autoencoders, and preserves\nrefusal behavior during fine-tuning through subspace orthogonalization. This\napproach prevents hallucinations from increasing while maintaining safety\nalignment.We evaluate our method on commonsense reasoning tasks and harmful\nbenchmarks (AdvBench and StrongReject). Results demonstrate that our approach\npreserves refusal behavior and task utility, mitigating the trade-off between\ntruthfulness and safety.", "AI": {"tldr": "The paper tackles the issue of improving truthfulness negatively impacting safety alignment in large language models by proposing a method to disentangle refusal and hallucination components.", "motivation": "The paper aims to investigate the trade-off between enhancing truthfulness and safety alignment in large language models, which occurs due to overlapping components encoding both hallucination and refusal information.", "method": "Our approach involves using sparse autoencoders to disentangle refusal-related features from hallucination features and preserving refusal behavior during fine-tuning through subspace orthogonalization.", "result": "The evaluation on commonsense reasoning tasks and harmful benchmarks shows that the method preserves refusal behavior and task utility, effectively managing the trade-off.", "conclusion": "The proposed method successfully mitigates the trade-off between truthfulness and safety, preserving refusal behavior and task utility."}}
{"id": "2510.07940", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.07940", "abs": "https://arxiv.org/abs/2510.07940", "authors": ["Leigang Qu", "Ziyang Wang", "Na Zheng", "Wenjie Wang", "Liqiang Nie", "Tat-Seng Chua"], "title": "TTOM: Test-Time Optimization and Memorization for Compositional Video Generation", "comment": "Project page: https://ttom-t2v.github.io/", "summary": "Video Foundation Models (VFMs) exhibit remarkable visual generation\nperformance, but struggle in compositional scenarios (e.g., motion, numeracy,\nand spatial relation). In this work, we introduce Test-Time Optimization and\nMemorization (TTOM), a training-free framework that aligns VFM outputs with\nspatiotemporal layouts during inference for better text-image alignment. Rather\nthan direct intervention to latents or attention per-sample in existing work,\nwe integrate and optimize new parameters guided by a general layout-attention\nobjective. Furthermore, we formulate video generation within a streaming\nsetting, and maintain historical optimization contexts with a parametric memory\nmechanism that supports flexible operations, such as insert, read, update, and\ndelete. Notably, we found that TTOM disentangles compositional world knowledge,\nshowing powerful transferability and generalization. Experimental results on\nthe T2V-CompBench and Vbench benchmarks establish TTOM as an effective,\npractical, scalable, and efficient framework to achieve cross-modal alignment\nfor compositional video generation on the fly.", "AI": {"tldr": "The paper presents TTOM, a method to improve compositional video generation by optimizing VFM outputs with spatiotemporal layouts during inference, achieving better text-image alignment and showcasing good performance on key benchmarks.", "motivation": "The motivation behind this work is to enhance the compositional abilities of VFMs, which are currently weak in handling scenarios that involve motion, numeracy, and spatial relations. TTOM aims to improve the alignment of generated visuals with the intended text descriptions during inference.", "method": "Video Foundation Models (VFMs) face challenges in compositional scenarios like motion and spatial relation. The paper introduces Test-Time Optimization and Memorization (TTOM), a training-free framework that optimizes VFM outputs with spatiotemporal layouts during inference for improved text-image alignment. TTOM uses a general layout-attention objective to optimize new parameters. It also supports video generation in a streaming setting, using a memory mechanism to manage historical optimization contexts.", "result": "TTOM exhibits powerful transferability and generalization, effectively disentangling compositional world knowledge. It shows good performance in cross-modal alignment for compositional video generation, as evidenced by results on T2V-CompBench and Vbench benchmarks.", "conclusion": "TTOM is a practical and efficient framework for achieving better cross-modal alignment in compositional video generation. It demonstrates scalability and effectiveness in optimizing VFM outputs with minimal intervention, making it a promising approach for enhancing video generation capabilities."}}
{"id": "2510.07776", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07776", "abs": "https://arxiv.org/abs/2510.07776", "authors": ["Shiman Zhao", "Shangyuan Li", "Wei Chen", "Tengjiao Wang", "Jiahui Yao", "Jiabin Zheng", "Kam Fai Wong"], "title": "Instance Relation Learning Network with Label Knowledge Propagation for Few-shot Multi-label Intent Detection", "comment": null, "summary": "Few-shot Multi-label Intent Detection (MID) is crucial for dialogue systems,\naiming to detect multiple intents of utterances in low-resource dialogue\ndomains. Previous studies focus on a two-stage pipeline. They first learn\nrepresentations of utterances with multiple labels and then use a\nthreshold-based strategy to identify multi-label results. However, these\nmethods rely on representation classification and ignore instance relations,\nleading to error propagation. To solve the above issues, we propose a\nmulti-label joint learning method for few-shot MID in an end-to-end manner,\nwhich constructs an instance relation learning network with label knowledge\npropagation to eliminate error propagation. Concretely, we learn the\ninteraction relations between instances with class information to propagate\nlabel knowledge between a few labeled (support set) and unlabeled (query set)\ninstances. With label knowledge propagation, the relation strength between\ninstances directly indicates whether two utterances belong to the same intent\nfor multi-label prediction. Besides, a dual relation-enhanced loss is developed\nto optimize support- and query-level relation strength to improve performance.\nExperiments show that we outperform strong baselines by an average of 9.54% AUC\nand 11.19% Macro-F1 in 1-shot scenarios.", "AI": {"tldr": "Proposes a multi-label joint learning method to enhance few-shot multi-label intent detection, demonstrating significant improvements over existing methods in 1-shot scenarios.", "motivation": "solve the error propagation issue in previous two-stage pipeline methods and improve performance in few-shot multi-label intent detection.", "method": "multi-label joint learning method for few-shot MID in an end-to-end manner, which constructs an instance relation learning network with label knowledge propagation to eliminate error propagation.", "result": "outperform strong baselines by an average of 9.54% AUC and 11.19% Macro-F1 in 1-shot scenarios.", "conclusion": "the proposed multi-label joint learning method with instance relation learning network and label knowledge propagation can effectively improve performance in few-shot multi-label intent detection tasks."}}
{"id": "2510.07944", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07944", "abs": "https://arxiv.org/abs/2510.07944", "authors": ["Tianrui Zhang", "Yichen Liu", "Zilin Guo", "Yuxin Guo", "Jingcheng Ni", "Chenjing Ding", "Dan Xu", "Lewei Lu", "Zehuan Wu"], "title": "CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving", "comment": null, "summary": "Generative models have been widely applied to world modeling for environment\nsimulation and future state prediction. With advancements in autonomous\ndriving, there is a growing demand not only for high-fidelity video generation\nunder various controls, but also for producing diverse and meaningful\ninformation such as depth estimation. To address this, we propose CVD-STORM, a\ncross-view video diffusion model utilizing a spatial-temporal reconstruction\nVariational Autoencoder (VAE) that generates long-term, multi-view videos with\n4D reconstruction capabilities under various control inputs. Our approach first\nfine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its\nability to encode 3D structures and temporal dynamics. Subsequently, we\nintegrate this VAE into the video diffusion process to significantly improve\ngeneration quality. Experimental results demonstrate that our model achieves\nsubstantial improvements in both FID and FVD metrics. Additionally, the\njointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic\nscenes, providing valuable geometric information for comprehensive scene\nunderstanding.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.07777", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07777", "abs": "https://arxiv.org/abs/2510.07777", "authors": ["Vardhan Dongre", "Ryan A. Rossi", "Viet Dac Lai", "David Seunghyun Yoon", "Dilek Hakkani-Tür", "Trung Bui"], "title": "Drift No More? Context Equilibria in Multi-Turn LLM Interactions", "comment": null, "summary": "Large Language Models (LLMs) excel at single-turn tasks such as instruction\nfollowing and summarization, yet real-world deployments require sustained\nmulti-turn interactions where user goals and conversational context persist and\nevolve. A recurring challenge in this setting is context drift: the gradual\ndivergence of a model's outputs from goal-consistent behavior across turns.\nUnlike single-turn errors, drift unfolds temporally and is poorly captured by\nstatic evaluation metrics. In this work, we present a study of context drift in\nmulti-turn interactions and propose a simple dynamical framework to interpret\nits behavior. We formalize drift as the turn-wise KL divergence between the\ntoken-level predictive distributions of the test model and a goal-consistent\nreference model, and propose a recurrence model that interprets its evolution\nas a bounded stochastic process with restoring forces and controllable\ninterventions. We instantiate this framework in both synthetic long-horizon\nrewriting tasks and realistic user-agent simulations such as in $\\tau$-Bench,\nmeasuring drift for several open-weight LLMs that are used as user simulators.\nOur experiments consistently reveal stable, noise-limited equilibria rather\nthan runaway degradation, and demonstrate that simple reminder interventions\nreliably reduce divergence in line with theoretical predictions. Together,\nthese results suggest that multi-turn drift can be understood as a controllable\nequilibrium phenomenon rather than as inevitable decay, providing a foundation\nfor studying and mitigating context drift in extended interactions.", "AI": {"tldr": "通过递归模型，研究了大型语言模型在多轮次任务中的上下文漂移，发现可通过简单干预控制漂移，无需担心性能不可避免地衰退。", "motivation": "当前，大语言模型虽然在单轮交互任务中表现出色，但现实部署要求可持续且多轮次交互，这需要模型在对话中保持和调整用户目标和上下文，而上下文漂移是一个挑战。", "method": "通过将多轮次交互中的上下文漂移形式化为测试模型与目标一致参考模型之间基于时间步长的KL散度，并提出一个递归模型来解释其演变，该模型认为这是一个具有恢复力和可控干预的受限随机过程。", "result": "实验使用了开放权重的大型语言模型，作为用户模拟器，显示了在长周期改写任务和现实用户代理模拟中的稳定平衡而不是失控退化，简单的提示干预能有效减少差异。", "conclusion": "研究表明，多轮次交互中的上下文漂移可以被理解为一个可控的平衡现象，而不是不可避免的衰减，这为研究和缓解长时间互动中的上下文漂移提供了基础。"}}
{"id": "2510.07951", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07951", "abs": "https://arxiv.org/abs/2510.07951", "authors": ["Ziyi Dong", "Yurui Zhang", "Changmao Li", "Naomi Rue Golding", "Qing Long"], "title": "A Large-scale Dataset for Robust Complex Anime Scene Text Detection", "comment": null, "summary": "Current text detection datasets primarily target natural or document scenes,\nwhere text typically appear in regular font and shapes, monotonous colors, and\norderly layouts. The text usually arranged along straight or curved lines.\nHowever, these characteristics differ significantly from anime scenes, where\ntext is often diverse in style, irregularly arranged, and easily confused with\ncomplex visual elements such as symbols and decorative patterns. Text in anime\nscene also includes a large number of handwritten and stylized fonts. Motivated\nby this gap, we introduce AnimeText, a large-scale dataset containing 735K\nimages and 4.2M annotated text blocks. It features hierarchical annotations and\nhard negative samples tailored for anime scenarios. %Cross-dataset evaluations\nusing state-of-the-art methods demonstrate that models trained on AnimeText\nachieve superior performance in anime text detection tasks compared to existing\ndatasets. To evaluate the robustness of AnimeText in complex anime scenes, we\nconducted cross-dataset benchmarking using state-of-the-art text detection\nmethods. Experimental results demonstrate that models trained on AnimeText\noutperform those trained on existing datasets in anime scene text detection\ntasks. AnimeText on HuggingFace:\nhttps://huggingface.co/datasets/deepghs/AnimeText", "AI": {"tldr": "This paper presents AnimeText, a specialized text detection dataset for anime scenes, which demonstrates superior performance in text detection tasks in anime compared to existing datasets.", "motivation": "The motivation behind this work is to address the gap in current text detection datasets when applied to anime scenes, which are characterized by diverse text styles, irregular arrangements, and complex visual elements that are not typically encountered in natural or document scenes.", "method": "The paper introduces AnimeText, a dataset specifically designed for anime scenes. It consists of hierarchical annotations and hard negative samples, tailored for the unique characteristics of text within anime, such as diverse styles, irregular arrangements, and complex visual elements.", "result": "Cross-dataset evaluations reveal that models trained on AnimeText outperform those trained on traditional datasets in text detection tasks specific to anime scenes.", "conclusion": "The paper concludes that AnimeText, due to its specific design for anime scenes, provides a valuable resource for training models that can more effectively detect text in complex anime environments compared to models trained on traditional text detection datasets."}}
{"id": "2510.07782", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07782", "abs": "https://arxiv.org/abs/2510.07782", "authors": ["Shuichiro Haruta", "Kazunori Matsumoto", "Zhi Li", "Yanan Wang", "Mori Kurokawa"], "title": "RCPU: Rotation-Constrained Error Compensation for Structured Pruning of a Large Language Model", "comment": null, "summary": "In this paper, we propose a rotation-constrained compensation method to\naddress the errors introduced by structured pruning of large language models\n(LLMs). LLMs are trained on massive datasets and accumulate rich semantic\nknowledge in their representation space. In contrast, pruning is typically\ncarried out with only a small amount of calibration data, which makes output\nmismatches unavoidable. Although direct least-squares fitting can reduce such\nerrors, it tends to overfit to the limited calibration set, destructively\nmodifying pretrained weights. To overcome this difficulty, we update the pruned\nparameters under a rotation constraint. This constrained update preserves the\ngeometry of output representations (i.e., norms and inner products) and\nsimultaneously re-aligns the pruned subspace with the original outputs.\nFurthermore, in rotation-constrained compensation, removing components that\nstrongly contribute to the principal directions of the output makes error\nrecovery difficult. Since input dimensions with large variance strongly affect\nthese principal directions, we design a variance-aware importance score that\nensures such dimensions are preferentially kept in the pruned model. By\ncombining this scoring rule with rotation-constrained updates, the proposed\nmethod effectively compensates errors while retaining the components likely to\nbe more important in a geometry-preserving manner. In the experiments, we apply\nthe proposed method to LLaMA-7B and evaluate it on WikiText-2 and multiple\nlanguage understanding benchmarks. The results demonstrate consistently better\nperplexity and task accuracy compared with existing baselines.", "AI": {"tldr": "The authors propose a rotation-constrained compensation method that combines a variance-aware importance score and geometry-preserving updates to effectively reduce errors caused by structured pruning of large language models, achieving superior perplexity and accuracy compared to baseline models on various benchmarks.", "motivation": "The motivation behind this paper is to address the issue of overfitting to a limited calibration set during the pruning process of LLMs, which can lead to output mismatches due to the reduced size of the calibration data as compared to the extensive training data.", "method": "In this paper, the authors introduce a rotation-constrained compensation method to mitigate errors caused by structured pruning in large language models (LLMs). They preserve the geometry of the output representations while modifying the pruned subspace. Additionally, they design a variance-aware importance score to maintain dimensions with large variance, which are crucial for the principal directions of output.", "result": "The experimental results show that the proposed method achieves better perplexity and task accuracy on WikiText-2 and multiple language understanding benchmarks when compared to existing pruning methods for the LLaMA-7B model.", "conclusion": "The paper concludes that the proposed rotation-constrained compensation method is successful in compensating for the errors introduced by pruning large language models, thereby improving the overall performance of the pruned models on language understanding tasks."}}
{"id": "2510.07953", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07953", "abs": "https://arxiv.org/abs/2510.07953", "authors": ["Yifang Yin", "Shengkai Chen", "Yiyao Li", "Lu Wang", "Ruibing Jin", "Wei Cui", "Shili Xiang"], "title": "SimCast: Enhancing Precipitation Nowcasting with Short-to-Long Term Knowledge Distillation", "comment": "accepted by ICME 2025", "summary": "Precipitation nowcasting predicts future radar sequences based on current\nobservations, which is a highly challenging task driven by the inherent\ncomplexity of the Earth system. Accurate nowcasting is of utmost importance for\naddressing various societal needs, including disaster management, agriculture,\ntransportation, and energy optimization. As a complementary to existing\nnon-autoregressive nowcasting approaches, we investigate the impact of\nprediction horizons on nowcasting models and propose SimCast, a novel training\npipeline featuring a short-to-long term knowledge distillation technique\ncoupled with a weighted MSE loss to prioritize heavy rainfall regions. Improved\nnowcasting predictions can be obtained without introducing additional overhead\nduring inference. As SimCast generates deterministic predictions, we further\nintegrate it into a diffusion-based framework named CasCast, leveraging the\nstrengths from probabilistic models to overcome limitations such as blurriness\nand distribution shift in deterministic outputs. Extensive experimental results\non three benchmark datasets validate the effectiveness of the proposed\nframework, achieving mean CSI scores of 0.452 on SEVIR, 0.474 on HKO-7, and\n0.361 on MeteoNet, which outperforms existing approaches by a significant\nmargin.", "AI": {"tldr": "", "motivation": "", "method": "", "result": "{\n  \"tldr\": \"The paper proposes SimCast, a novel training pipeline for precipitation nowcasting using a short-to-long term knowledge distillation technique and a weighted MSE loss. It then integrates SimCast into a diffusion-based framework named CasCast to improve prediction accuracy, achieving high CSI scores on three benchmark datasets.\",\n  \"motivation\": \"The motivation is to address the challenges in accurate precipitation nowcasting, which is crucial for societal needs like disaster management and agriculture, by improving upon existing non-autoregressive approaches.\",\n  \"method\": \"The method involves developing SimCast, a training pipeline that employs a short-to-long term knowledge distillation technique and a weighted MSE loss to prioritize heavy rainfall regions, and integrating it into a diffusion-based framework named CasCast.\",\n  \"result\": \"Experimental results show that the proposed framework outperforms existing approaches by a significant margin, achieving mean CSI scores of 0.452 on SEVIR, 0.474 on HKO-7, and 0.361 on MeteoNet.\",\n  \"conclusion\": \"The conclusion is that the proposed pipeline and framework enhance the accuracy of precipitation nowcasting, making it more effective for various applications without increasing computational overhead in inference.\")", "conclusion": ""}}
{"id": "2510.07793", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07793", "abs": "https://arxiv.org/abs/2510.07793", "authors": ["Sajib Acharjee Dip", "Adrika Zafor", "Bikash Kumar Paul", "Uddip Acharjee Shuvo", "Muhit Islam Emon", "Xuan Wang", "Liqing Zhang"], "title": "LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell Biology", "comment": "34 pages, 5 figures, 7 tables", "summary": "Large language models (LLMs) and emerging agentic frameworks are beginning to\ntransform single-cell biology by enabling natural-language reasoning,\ngenerative annotation, and multimodal data integration. However, progress\nremains fragmented across data modalities, architectures, and evaluation\nstandards. LLM4Cell presents the first unified survey of 58 foundation and\nagentic models developed for single-cell research, spanning RNA, ATAC,\nmulti-omic, and spatial modalities. We categorize these methods into five\nfamilies-foundation, text-bridge, spatial, multimodal, epigenomic, and\nagentic-and map them to eight key analytical tasks including annotation,\ntrajectory and perturbation modeling, and drug-response prediction. Drawing on\nover 40 public datasets, we analyze benchmark suitability, data diversity, and\nethical or scalability constraints, and evaluate models across 10 domain\ndimensions covering biological grounding, multi-omics alignment, fairness,\nprivacy, and explainability. By linking datasets, models, and evaluation\ndomains, LLM4Cell provides the first integrated view of language-driven\nsingle-cell intelligence and outlines open challenges in interpretability,\nstandardization, and trustworthy model development.", "AI": {"tldr": "LLM4Cell是首个关于单细胞研究的基础和代理模型的统一综述，旨在整合语言驱动的单细胞智能，分析模型在解释性、标准化和可信赖模型开发方面的开放挑战。", "motivation": "研究动机：为了应对当前在单细胞生物学领域中，进展在数据模态、架构和评估标准方面分散的问题，提出LLM4Cell作为第一个整合语言驱动的单细胞智能的综述，分析模型在解释性、标准化和可信赖模型开发方面的开放挑战。", "method": "内容概述：大型语言模型(LLMs)和新兴的代理框架正在通过实现自然语言推理、生成注释和多模态数据集成，对单细胞生物学产生变革性的影响。然而，这些进展在整个数据模态、架构和评估标准方面仍然分散。LLM4Cell是首个涵盖58个单细胞研究基础和代理模型的统一综述，涵盖了RNA、ATAC、多组学和空间模态。", "result": "研究成果：论文通过分析超过40个公开数据集，研究了模型跨10个领域维度的适配性，覆盖了生物学基础性、多组学协同、公平性、隐私和可解释性。", "conclusion": "研究结论：LLM4Cell链接了数据集、模型和评估领域，提供了首个关于语言驱动的单细胞智能的整合视野，并概述了在解释性、标准化和可信模型开发中的开放挑战。"}}
{"id": "2510.07961", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07961", "abs": "https://arxiv.org/abs/2510.07961", "authors": ["Yidi Liu", "Xueyang Fu", "Jie Huang", "Jie Xiao", "Dong Li", "Wenlong Zhang", "Lei Bai", "Zheng-Jun Zha"], "title": "Latent Harmony: Synergistic Unified UHD Image Restoration via Latent Space Regularization and Controllable Refinement", "comment": "Accepted by NeurIPS 2025", "summary": "Ultra-High Definition (UHD) image restoration faces a trade-off between\ncomputational efficiency and high-frequency detail retention. While Variational\nAutoencoders (VAEs) improve efficiency via latent-space processing, their\nGaussian constraint often discards degradation-specific high-frequency\ninformation, hurting reconstruction fidelity. To overcome this, we propose\nLatent Harmony, a two-stage framework that redefines VAEs for UHD restoration\nby jointly regularizing the latent space and enforcing high-frequency-aware\nreconstruction.In Stage One, we introduce LH-VAE, which enhances semantic\nrobustness through visual semantic constraints and progressive degradation\nperturbations, while latent equivariance strengthens high-frequency\nreconstruction.Stage Two jointly trains this refined VAE with a restoration\nmodel using High-Frequency Low-Rank Adaptation (HF-LoRA): an encoder LoRA\nguided by a fidelity-oriented high-frequency alignment loss to recover\nauthentic details, and a decoder LoRA driven by a perception-oriented loss to\nsynthesize realistic textures. Both LoRA modules are trained via alternating\noptimization with selective gradient propagation to preserve the pretrained\nlatent structure.At inference, a tunable parameter {\\alpha} enables flexible\nfidelity-perception trade-offs.Experiments show Latent Harmony achieves\nstate-of-the-art performance across UHD and standard-resolution tasks,\neffectively balancing efficiency, perceptual quality, and reconstruction\naccuracy.", "AI": {"tldr": "本文提出了一种名为Latent Harmony的两阶段框架，用于改善UHD图像的恢复，通过引入视觉语义约束和HF-LoRA来克服VAE的高斯约束对高频重构建的限制，实现了性能和效率的提升。", "motivation": "UHD图像恢复面临计算效率和高频细节保留之间的权衡。尽管变分自编码器（VAEs）通过潜空间处理提高了效率，但由于其高斯约束往往会丢弃特定退化的高频信息，从而损害重建保真度。本文旨在克服这一问题。", "method": "本文提出了一种名为Latent Harmony的两阶段框架，旨在改善UHD图像的恢复。第一阶段提出了LH-VAE，通过视觉语义约束和渐进退化扰动增强语义鲁棒性，同时使用潜变量等价性加强高频重建。第二阶段通过HF-LoRA，对经过改进的VAE与恢复模型共同训练，HF-LoRA包括对高频对齐损失导向的编码器LoRA和对感知导向损失驱动的解码器LoRA，二者通过选择性梯度传播交替优化以保持预训练的潜变量结构。", "result": "实验显示，Latent Harmony在UHD和标准分辨率任务上均取得了最先进的性能，有效平衡了效率、感知质量和重建准确性。", "conclusion": "Latent Harmony通过联合正则化潜空间和强制高频感知重建，成功改善了UHD图像恢复的性能，同时保持了计算效率，表现出色。"}}
{"id": "2510.07794", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07794", "abs": "https://arxiv.org/abs/2510.07794", "authors": ["Peilin Wu", "Mian Zhang", "Kun Wan", "Wentian Zhao", "Kaiyu He", "Xinya Du", "Zhiyu Chen"], "title": "HiPRAG: Hierarchical Process Rewards for Efficient Agentic Retrieval Augmented Generation", "comment": "Under review", "summary": "Agentic RAG is a powerful technique for incorporating external information\nthat LLMs lack, enabling better problem solving and question answering.\nHowever, suboptimal search behaviors exist widely, such as over-search\n(retrieving information already known) and under-search (failing to search when\nnecessary), which leads to unnecessary overhead and unreliable outputs. Current\ntraining methods, which typically rely on outcome-based rewards in a RL\nframework, lack the fine-grained control needed to address these\ninefficiencies. To overcome this, we introduce Hierarchical Process Rewards for\nEfficient agentic RAG (HiPRAG), a training methodology that incorporates a\nfine-grained, knowledge-grounded process reward into the RL training. Our\napproach evaluates the necessity of each search decision on-the-fly by\ndecomposing the agent's reasoning trajectory into discrete, parsable steps. We\nthen apply a hierarchical reward function that provides an additional bonus\nbased on the proportion of optimal search and non-search steps, on top of\ncommonly used outcome and format rewards. Experiments on the Qwen2.5 and\nLlama-3.2 models across seven diverse QA benchmarks show that our method\nachieves average accuracies of 65.4% (3B) and 67.2% (7B). This is accomplished\nwhile improving search efficiency, reducing the over-search rate to just 2.3%\nand concurrently lowering the under-search rate. These results demonstrate the\nefficacy of optimizing the reasoning process itself, not just the final\noutcome. Further experiments and analysis demonstrate that HiPRAG shows good\ngeneralizability across a wide range of RL algorithms, model families, sizes,\nand types. This work demonstrates the importance and potential of fine-grained\ncontrol through RL, for improving the efficiency and optimality of reasoning\nfor search agents.", "AI": {"tldr": "本文介绍了一种新的训练方法HiPRAG，解决了代理RAG中搜索行为效率低下问题，并展示了其在提高搜索效率和准确性方面的有效性。", "motivation": "当前的训练方法依靠基于结果的奖励，缺乏解决代理RAG中搜索行为效率低下问题所需的精细控制。这些问题包括过度搜索和不足搜索，导致不必要的开销和不可靠的结果。", "method": "本文提出了HiPRAG，一种用于高效代理RAG的训练方法。该方法引入了一个细粒度、基于知识的过程奖励，通过将智能体的推理轨迹分解成离散、可解析的步骤来实时评估每个搜索决定的必要性。在此基础上，应用了一个分层奖励函数，该函数根据最优搜索和非搜索步骤的比例提供额外奖励，同时使用常见的结果和格式奖励。", "result": "在七种不同的问答基准测试中，Qwen2.5和Llama-3.2模型的实验表明，该方法实现了平均准确率65.4%（3B）和67.2%（7B），降低了过度搜索率至2.3%，同时减少了不足搜索率。实验和分析还显示，HiPRAG展示了良好的泛化能力，适用于广泛的强化学习算法、模型家族、大小和类型。", "conclusion": "本文工作展示了细粒度控制通过强化学习优化搜索代理推理效率和最优性的潜力和重要性。这些结果证明了优化推理过程本身的重要性，而不仅仅是最终结果。"}}
{"id": "2510.07976", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07976", "abs": "https://arxiv.org/abs/2510.07976", "authors": ["Darya Baranouskaya", "Andrea Cavallaro"], "title": "The impact of abstract and object tags on image privacy classification", "comment": "This work has been submitted to the ICASSP 2026", "summary": "Object tags denote concrete entities and are central to many computer vision\ntasks, whereas abstract tags capture higher-level information, which is\nrelevant for tasks that require a contextual, potentially subjective scene\nunderstanding. Object and abstract tags extracted from images also facilitate\ninterpretability. In this paper, we explore which type of tags is more suitable\nfor the context-dependent and inherently subjective task of image privacy.\nWhile object tags are generally used for privacy classification, we show that\nabstract tags are more effective when the tag budget is limited. Conversely,\nwhen a larger number of tags per image is available, object-related information\nis as useful. We believe that these findings will guide future research in\ndeveloping more accurate image privacy classifiers, informed by the role of tag\ntypes and quantity.", "AI": {"tldr": "在图像隐私任务中，当标签数量有限时，抽象标签比物体标签更有效；标签数量较多时，两者的效用相近。", "motivation": "物体标签表示具体的实体，是许多计算机视觉任务的核心，而抽象标签捕捉高层次的信息，对于需要上下文、可能是主观场景理解的任务是相关的。我们希望探索哪种类型的标签更适合用于图像隐私任务，并指导未来更准确的图像隐私分类器的研究。", "method": "我们探索了在依赖背景且具有主观性的图像隐私任务中，使用哪种类型的标签更为合适。虽然物体标签通常用于隐私分类，但我们发现当标签数量受到限制时，抽象标签更加有效。另一方面，当每个图像可用的标签数量较大时，与物体相关的标签同样是有用的。", "result": "研究表明，在标签预算有限的情况下，抽象标签比物体标签在图像隐私识别上更有效。然而，当标签数量较多时，物体标签也能发挥同样重要的作用。", "conclusion": "这些发现可以指导未来在开发更加准确的图像隐私分类器方面的研究，考虑到标签类型和数量的作用。"}}
{"id": "2510.07799", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07799", "abs": "https://arxiv.org/abs/2510.07799", "authors": ["Eric Hanchen Jiang", "Guancheng Wan", "Sophia Yin", "Mengting Li", "Yuchen Wu", "Xiao Liang", "Xinfeng Li", "Yizhou Sun", "Wei Wang", "Kai-Wei Chang", "Ying Nian Wu"], "title": "Dynamic Generation of Multi-LLM Agents Communication Topologies with Graph Diffusion Models", "comment": null, "summary": "The efficiency of multi-agent systems driven by large language models (LLMs)\nlargely hinges on their communication topology. However, designing an optimal\ntopology is a non-trivial challenge, as it requires balancing competing\nobjectives such as task performance, communication cost, and robustness.\nExisting frameworks often rely on static or hand-crafted topologies, which\ninherently fail to adapt to diverse task requirements, leading to either\nexcessive token consumption for simple problems or performance bottlenecks for\ncomplex ones. To address this challenge, we introduce a novel generative\nframework called \\textit{Guided Topology Diffusion (GTD)}. Inspired by\nconditional discrete graph diffusion models, GTD formulates topology synthesis\nas an iterative construction process. At each step, the generation is steered\nby a lightweight proxy model that predicts multi-objective rewards (e.g.,\naccuracy, utility, cost), enabling real-time, gradient-free optimization\ntowards task-adaptive topologies. This iterative, guided synthesis process\ndistinguishes GTD from single-step generative frameworks, enabling it to better\nnavigate complex design trade-offs. We validated GTD across multiple\nbenchmarks, and experiments show that this framework can generate highly\ntask-adaptive, sparse, and efficient communication topologies, significantly\noutperforming existing methods in LLM agent collaboration.", "AI": {"tldr": "本文解决了多智能体系统中通信拓扑设计复杂的问题，提出了GTD框架，该框架能够生成适应性强、效率高的通信拓扑。", "motivation": "现有的多智能体系统的通信拓扑设计主要依赖于静态或人为设计的拓扑结构，这种做法无法灵活应对不同的任务需求，导致要么过度消耗令牌，要么在处理复杂任务时出现性能瓶颈。因此，作者提出了GTD框架来解决这一问题。", "method": "该论文提出了一种名为Guided Topology Diffusion (GTD)的新框架。通过迭代构造过程并利用轻量级代理模型预测多目标奖励，GTD可以在实时无梯度优化下生成任务自适应的通信拓扑。", "result": "实验结果显示，GTD框架能够生成高度任务自适应的、稀疏且高效的通信拓扑结构，并且在大规模语言模型代理间的合作中明显优于现有方法。", "conclusion": "提出的GTD框架显著提升了LLM驱动的多智能体系统中的通信效率。实验验证了该方法的有效性和优越性，适用于生成复杂的、任务自适应的通信拓扑。"}}
{"id": "2510.07984", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07984", "abs": "https://arxiv.org/abs/2510.07984", "authors": ["Chandresh Sutariya", "Nitin Singh"], "title": "Is Architectural Complexity Always the Answer? A Case Study on SwinIR vs. an Efficient CNN", "comment": "7 pages, 4 figures", "summary": "The simultaneous restoration of high-frequency details and suppression of\nsevere noise in low-light imagery presents a significant and persistent\nchallenge in computer vision. While large-scale Transformer models like SwinIR\nhave set the state of the art in performance, their high computational cost can\nbe a barrier for practical applications. This paper investigates the critical\ntrade-off between performance and efficiency by comparing the state-of-the-art\nSwinIR model against a standard, lightweight Convolutional Neural Network (CNN)\non this challenging task. Our experimental results reveal a nuanced but\nimportant finding. While the Transformer-based SwinIR model achieves a higher\npeak performance, with a Peak Signal-to-Noise Ratio (PSNR) of 39.03 dB, the\nlightweight CNN delivers a surprisingly competitive PSNR of 37.4 dB. Crucially,\nthe CNN reached this performance after converging in only 10 epochs of\ntraining, whereas the more complex SwinIR model required 132 epochs. This\nefficiency is further underscored by the model's size; the CNN is over 55 times\nsmaller than SwinIR. This work demonstrates that a standard CNN can provide a\nnear state-of-the-art result with significantly lower computational overhead,\npresenting a compelling case for its use in real-world scenarios where resource\nconstraints are a primary concern.", "AI": {"tldr": "本文探讨了高性能模型SwinIR与轻量级CNN在低光照图像处理中的表现，结果表明轻量级模型在计算成本更低的情况下，仍能提供接近顶尖的PSNR性能。", "motivation": "尽管大型Transformer模型如SwinIR在性能上表现优异，但其高计算成本限制了实际应用。因此，探讨高性能和低计算成本之间权衡的重要性。", "method": "通过比较最新的SwinIR模型和一个标准的轻量级卷积神经网络(CNN)在低光照图像中的表现，研究性能与效率之间的关键权衡。", "result": "实验结果显示，基于Transformer的SwinIR模型达到最高性能，PSNR为39.03 dB，而轻量级CNN以较小的计算成本达到了37.4 dB的PSNR，且训练收敛需要的周期数仅为10，比SwinIR少很多。", "conclusion": "标准CNN能在显著降低计算成本的情况下提供接近顶尖的效果，这表明它在资源受限的实际场景中是一个值得考虑的选择。"}}
{"id": "2510.07812", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07812", "abs": "https://arxiv.org/abs/2510.07812", "authors": ["Yuxin Huang", "Simeng Wu", "Ran Song", "Yan Xiang", "Yantuan Xian", "Shengxiang Gao", "Zhengtao Yu"], "title": "Multilingual Generative Retrieval via Cross-lingual Semantic Compression", "comment": "EMNLP 2025, Findings, Long", "summary": "Generative Information Retrieval is an emerging retrieval paradigm that\nexhibits remarkable performance in monolingual scenarios.However, applying\nthese methods to multilingual retrieval still encounters two primary\nchallenges, cross-lingual identifier misalignment and identifier inflation. To\naddress these limitations, we propose Multilingual Generative Retrieval via\nCross-lingual Semantic Compression (MGR-CSC), a novel framework that unifies\nsemantically equivalent multilingual keywords into shared atoms to align\nsemantics and compresses the identifier space, and we propose a dynamic\nmulti-step constrained decoding strategy during retrieval. MGR-CSC improves\ncross-lingual alignment by assigning consistent identifiers and enhances\ndecoding efficiency by reducing redundancy. Experiments demonstrate that\nMGR-CSC achieves outstanding retrieval accuracy, improving by 6.83% on\nmMarco100k and 4.77% on mNQ320k, while reducing document identifiers length by\n74.51% and 78.2%, respectively.", "AI": {"tldr": "本文提出了一种针对跨语言标识符错位和膨胀问题的多语言生成检索新框架 MGR-CSC，该框架通过语义等价关键词统合成共享原子来对齐语义并压缩标识空间，提升检索精度并减少文档标识符长度。", "motivation": "生成式信息检索在单语场景中表现出色，但在多语言检索中仍存在跨语言标识符错位和标识符膨胀两大挑战。为了应对这些挑战，从而设计了该方法。", "method": "提出了一种通过跨语言语义压缩（MGR-CSC）实现多语言生成检索的新框架。该框架将语义等价的多语言关键词统合成共享原子来对齐语义并压缩标志空间，并提出了一种动态多步约束解码策略。", "result": "实验结果表明，MGR-CSC 在 mMarco100k 和 mNQ320k 上分别提高了 6.83% 和 4.77% 的检索精度，同时分别将文档标识符长度减少至 74.51% 和 78.2%。", "conclusion": "MGR-CSC 通过一致分配标识符提高了跨语言对齐，并通过减少冗余提高了解码效率。"}}
{"id": "2510.07990", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07990", "abs": "https://arxiv.org/abs/2510.07990", "authors": ["Gaurvi Goyal", "Pham Cong Thuong", "Arren Glover", "Masayoshi Mizuno", "Chiara Bartolozzi"], "title": "GraphEnet: Event-driven Human Pose Estimation with a Graph Neural Network", "comment": null, "summary": "Human Pose Estimation is a crucial module in human-machine interaction\napplications and, especially since the rise in deep learning technology, robust\nmethods are available to consumers using RGB cameras and commercial GPUs. On\nthe other hand, event-based cameras have gained popularity in the vision\nresearch community for their low latency and low energy advantages that make\nthem ideal for applications where those resources are constrained like portable\nelectronics and mobile robots. In this work we propose a Graph Neural Network,\nGraphEnet, that leverages the sparse nature of event camera output, with an\nintermediate line based event representation, to estimate 2D Human Pose of a\nsingle person at a high frequency. The architecture incorporates a novel offset\nvector learning paradigm with confidence based pooling to estimate the human\npose. This is the first work that applies Graph Neural Networks to event data\nfor Human Pose Estimation. The code is open-source at\nhttps://github.com/event-driven-robotics/GraphEnet-NeVi-ICCV2025.", "AI": {"tldr": "本论文提出一种基于图神经网络的GraphEnet方法，用于估计事件相机数据中的2D人体姿态，适用于资源受限的设备。", "motivation": "论文旨在利用事件相机的低延迟和低能耗特性，结合图神经网络技术，来改善资源受限环境（如便携式电子设备和移动机器人）中的人体姿态估计问题。", "method": "本论文提出了一种基于图神经网络（Graph Neural Network）的方法GraphEnet，该方法利用了事件相机输出的稀疏特性，并采用基于线的中间事件表示来估计单人2D人体姿态，频率较高。该架构引入了一种新的偏移向量学习范式以及基于置信度的汇聚策略。", "result": "该工作是首次将图神经网络应用于事件数据进行人体姿态估计的研究，展现了此方法在资源受限应用中的潜力。", "conclusion": "GraphEnet通过采用基于图神经网络的创新设计，在利用事件相机进行高频率的人体姿态估计方面取得了进展，为资源受限的设备提供了有益的解决方案。"}}
{"id": "2510.07842", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07842", "abs": "https://arxiv.org/abs/2510.07842", "authors": ["Jingyu Peng", "Maolin Wang", "Hengyi Cai", "Yuchen Li", "Kai Zhang", "Shuaiqiang Wang", "Dawei Yin", "Xiangyu Zhao"], "title": "AdaSwitch: Adaptive Switching Generation for Knowledge Distillation", "comment": null, "summary": "Small language models (SLMs) are crucial for applications with strict latency\nand computational constraints, yet achieving high performance remains\nchallenging. Knowledge distillation (KD) can transfer capabilities from large\nteacher models, but existing methods involve trade-offs: off-policy\ndistillation provides high-quality supervision but introduces a\ntraining-inference mismatch, while on-policy approaches maintain consistency\nbut rely on low-quality student outputs. To address these issues, we propose\nAdaSwitch, a novel approach that dynamically combines on-policy and off-policy\ngeneration at the token level. AdaSwitch allows the student to first explore\nits own predictions and then selectively integrate teacher guidance based on\nreal-time quality assessment. This approach simultaneously preserves\nconsistency and maintains supervision quality. Experiments on three datasets\nwith two teacher-student LLM pairs demonstrate that AdaSwitch consistently\nimproves accuracy, offering a practical and effective method for distilling\nSLMs with acceptable additional overhead.", "AI": {"tldr": "研究提出了一种名为 AdaSwitch 的方法，它可以同时保持训练生成的一致性和监督的质量，从而提高小语言模型的性能。实验证明了这种方法的有效性和实用性。", "motivation": "小语言模型(SLMs)在延迟和计算约束严格的应用中非常重要，但要实现高性能仍然是具有挑战性的。现有的知识蒸馏方法存在权衡：离线策略蒸馏提供高质量的监督但引入了训练推理不匹配，而在线方法保持一致性但依赖低质量的学生输出。AdaSwitch 的动机是解决这些权衡问题。", "method": "AdaSwitch 是一种新颖的方法，它能够在令牌级别动态组合在线和离线策略生成。AdaSwitch 允许学生模型首先探索自己的预测，然后基于实时质量评估选择性地整合教师指导。这个方法同时保持了一致性和监督质量。", "result": "在三个数据集和两对教师-学生大型语言模型上的实验表明，AdaSwitch 能够一致地提高准确性，提供了一种实用且有效的方法来蒸馏小语言模型，并且计算开销在可接受范围内。", "conclusion": "AdaSwitch 为蒸馏小语言模型提供了一种同时保持训练生成一致性和高质量监督的方法，该方法表现出色并且计算开销可控。"}}
{"id": "2510.08003", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08003", "abs": "https://arxiv.org/abs/2510.08003", "authors": ["Weihuang Lin", "Yiwei Ma", "Jiayi Ji", "Xiaoshuai Sun", "Rongrong Ji"], "title": "CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning", "comment": null, "summary": "Composed Image Retrieval (CIR), which aims to find a target image from a\nreference image and a modification text, presents the core challenge of\nperforming unified reasoning across visual and semantic modalities. While\ncurrent approaches based on Vision-Language Models (VLMs, e.g., CLIP) and more\nrecent Multimodal Large Language Models (MLLMs, e.g., Qwen-VL) have shown\nprogress, they predominantly function as ``black boxes.\" This inherent opacity\nnot only prevents users from understanding the retrieval rationale but also\nrestricts the models' ability to follow complex, fine-grained instructions. To\novercome these limitations, we introduce CIR-CoT, the first end-to-end\nretrieval-oriented MLLM designed to integrate explicit Chain-of-Thought (CoT)\nreasoning. By compelling the model to first generate an interpretable reasoning\nchain, CIR-CoT enhances its ability to capture crucial cross-modal\ninteractions, leading to more accurate retrieval while making its decision\nprocess transparent. Since existing datasets like FashionIQ and CIRR lack the\nnecessary reasoning data, a key contribution of our work is the creation of\nstructured CoT annotations using a three-stage process involving a caption,\nreasoning, and conclusion. Our model is then fine-tuned to produce this\nstructured output before encoding its final retrieval intent into a dedicated\nembedding. Comprehensive experiments show that CIR-CoT achieves highly\ncompetitive performance on in-domain datasets (FashionIQ, CIRR) and\ndemonstrates remarkable generalization on the out-of-domain CIRCO dataset,\nestablishing a new path toward more effective and trustworthy retrieval\nsystems.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.07877", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07877", "abs": "https://arxiv.org/abs/2510.07877", "authors": ["Md. Faiyaz Abdullah Sayeedi", "Md. Mahbub Alam", "Subhey Sadi Rahman", "Md. Adnanul Islam", "Jannatul Ferdous Deepti", "Tasnim Mohiuddin", "Md Mofijul Islam", "Swakkhar Shatabda"], "title": "Ready to Translate, Not to Represent? Bias and Performance Gaps in Multilingual LLMs Across Language Families and Domains", "comment": null, "summary": "The rise of Large Language Models (LLMs) has redefined Machine Translation\n(MT), enabling context-aware and fluent translations across hundreds of\nlanguages and textual domains. Despite their remarkable capabilities, LLMs\noften exhibit uneven performance across language families and specialized\ndomains. Moreover, recent evidence reveals that these models can encode and\namplify different biases present in their training data, posing serious\nconcerns for fairness, especially in low-resource languages. To address these\ngaps, we introduce Translation Tangles, a unified framework and dataset for\nevaluating the translation quality and fairness of open-source LLMs. Our\napproach benchmarks 24 bidirectional language pairs across multiple domains\nusing different metrics. We further propose a hybrid bias detection pipeline\nthat integrates rule-based heuristics, semantic similarity filtering, and\nLLM-based validation. We also introduce a high-quality, bias-annotated dataset\nbased on human evaluations of 1,439 translation-reference pairs. The code and\ndataset are accessible on GitHub:\nhttps://github.com/faiyazabdullah/TranslationTangles", "AI": {"tldr": "介绍Translation Tangles框架和数据集以评估开源LLMs在翻译质量和公平性方面的表现，并提出一种混合偏差检测方法。", "motivation": "尽管LLMs在机器翻译方面表现出色，但在不同语言家族和专业领域中的性能表现参差不齐，并且它们在训练数据中编码并放大不同偏见的问题严重，尤其是在资源较少的语言中。为了应对这些问题，我们设计了此框架和数据集。", "method": "我们提出了Translation Tangles，一个统一的框架和数据集，用于评估开源LLMs的翻译质量和公平性。我们的方法涉及24种双向语言对的跨多个领域的基准测试，并采用不同的评估指标。此外，我们提出了一种混合偏差检测管道，结合了基于规则的启发式方法、语义相似性过滤以及LLM验证。", "result": "创建了一个包含1,439个翻译-参考对的高质量偏差标注数据集，并在GitHub上公开了代码和数据集。", "conclusion": "Translation Tangles框架和数据集为评估LLMs在翻译质量和公平性方面的表现提供了一个基准测试平台，并有助于解决LLMs的偏差问题。"}}
{"id": "2510.08017", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08017", "abs": "https://arxiv.org/abs/2510.08017", "authors": ["Shaohong Wang", "Bin Lu", "Xinyu Xiao", "Hanzhi Zhong", "Bowen Pang", "Tong Wang", "Zhiyu Xiang", "Hangguan Shan", "Eryun Liu"], "title": "RayFusion: Ray Fusion Enhanced Collaborative Visual Perception", "comment": "Accepted by NeurIPS2025", "summary": "Collaborative visual perception methods have gained widespread attention in\nthe autonomous driving community in recent years due to their ability to\naddress sensor limitation problems. However, the absence of explicit depth\ninformation often makes it difficult for camera-based perception systems, e.g.,\n3D object detection, to generate accurate predictions. To alleviate the\nambiguity in depth estimation, we propose RayFusion, a ray-based fusion method\nfor collaborative visual perception. Using ray occupancy information from\ncollaborators, RayFusion reduces redundancy and false positive predictions\nalong camera rays, enhancing the detection performance of purely camera-based\ncollaborative perception systems. Comprehensive experiments show that our\nmethod consistently outperforms existing state-of-the-art models, substantially\nadvancing the performance of collaborative visual perception. The code is\navailable at https://github.com/wangsh0111/RayFusion.", "AI": {"tldr": "RayFusion is a proposed method that uses ray occupancy information from collaborators to enhance depth estimation and performance in camera-based collaborative perception systems for autonomous driving.", "motivation": "To address the issue of inaccurate depth estimation in camera-based perception systems for autonomous driving, which often stems from the lack of explicit depth information.", "method": "The paper proposes RayFusion, a ray-based fusion method that uses occupancy information from collaborators to improve the depth estimation accuracy in camera-based collaborative perception systems.", "result": "Experiments show that RayFusion outperforms current state-of-the-art methods, enhancing the performance of collaborative visual perception systems.", "conclusion": "RayFusion, by leveraging ray occupancy information from collaborators, improves the performance of collaborative visual perception systems, suggesting a promising direction for future research in autonomous driving."}}
{"id": "2510.07880", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07880", "abs": "https://arxiv.org/abs/2510.07880", "authors": ["Xinliang Frederick Zhang", "Anhad Mohananey", "Alexandra Chronopoulou", "Pinelopi Papalampidi", "Somit Gupta", "Tsendsuren Munkhdalai", "Lu Wang", "Shyam Upadhyay"], "title": "Do LLMs Really Need 10+ Thoughts for \"Find the Time 1000 Days Later\"? Towards Structural Understanding of LLM Overthinking", "comment": "30 pages, 41 figures, 10 tables. Preprint", "summary": "Models employing long chain-of-thought (CoT) reasoning have shown superior\nperformance on complex reasoning tasks. Yet, this capability introduces a\ncritical and often overlooked inefficiency -- overthinking -- models often\nengage in unnecessarily extensive reasoning even for simple queries, incurring\nsignificant computations without accuracy improvements. While prior work has\nexplored solutions to mitigate overthinking, a fundamental gap remains in our\nunderstanding of its underlying causes. Most existing analyses are limited to\nsuperficial, profiling-based observations, failing to delve into LLMs' inner\nworkings. This study introduces a systematic, fine-grained analyzer of LLMs'\nthought process to bridge the gap, TRACE. We first benchmark the overthinking\nissue, confirming that long-thinking models are five to twenty times slower on\nsimple tasks with no substantial gains. We then use TRACE to first decompose\nthe thought process into minimally complete sub-thoughts. Next, by inferring\ndiscourse relationships among sub-thoughts, we construct granular thought\nprogression graphs and subsequently identify common thinking patterns for\ntopically similar queries. Our analysis reveals two major patterns for\nopen-weight thinking models -- Explorer and Late Landing. This finding provides\nevidence that over-verification and over-exploration are the primary drivers of\noverthinking in LLMs. Grounded in thought structures, we propose a\nutility-based definition of overthinking, which moves beyond length-based\nmetrics. This revised definition offers a more insightful understanding of\nLLMs' thought progression, as well as practical guidelines for principled\noverthinking management.", "AI": {"tldr": "本文通过引入TRACE方法分析长链思维模型中的过度思考问题，发现过度验证和过度探索是过度思考的主要原因，并提出了基于效用的过度思考定义。", "motivation": "先前的工作虽然探讨了缓解过度思考的方法，但对于过度思考的根本原因理解不足。这项研究旨在通过引入TRACE分析器来弥补这一缺口，从而更深入地理解LLMs的内部运作。", "method": "该研究引入了一种系统化的LLMs思维过程细粒度分析器TRACE。首先，通过基准测试来确认过度思考的问题，然后通过分解思维过程为最小完整的子思维，并推断子思维之间的讨论关系，构建细粒度的思想进展图，从而识别出类似主题查询的常见思维模式。", "result": "研究揭示了开放权重思维模型的两种主要模式 - 探索者和晚期着陆。发现过度验证和过度探索是LLMs过度思考的主要驱动因素。", "conclusion": "基于思想结构，该研究提出了一个基于效用的过度思考定义，超越了基于长度的度量，为理解LLMs的思想进程提供了更深入的认知，并为有原则地管理过度思考提供了实用指南。"}}
{"id": "2510.08052", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08052", "abs": "https://arxiv.org/abs/2510.08052", "authors": ["Bheeshm Sharma", "Karthikeyan Jaganathan", "Balamurugan Palaniappan"], "title": "RASALoRE: Region Aware Spatial Attention with Location-based Random Embeddings for Weakly Supervised Anomaly Detection in Brain MRI Scans", "comment": "Accepted in BMVC-2025", "summary": "Weakly Supervised Anomaly detection (WSAD) in brain MRI scans is an important\nchallenge useful to obtain quick and accurate detection of brain anomalies when\nprecise pixel-level anomaly annotations are unavailable and only weak labels\n(e.g., slice-level) are available. In this work, we propose RASALoRE: Region\nAware Spatial Attention with Location-based Random Embeddings, a novel\ntwo-stage WSAD framework. In the first stage, we introduce a Discriminative\nDual Prompt Tuning (DDPT) mechanism that generates high-quality pseudo weak\nmasks based on slice-level labels, serving as coarse localization cues. In the\nsecond stage, we propose a segmentation network with a region-aware spatial\nattention mechanism that relies on fixed location-based random embeddings. This\ndesign enables the model to effectively focus on anomalous regions. Our\napproach achieves state-of-the-art anomaly detection performance, significantly\noutperforming existing WSAD methods while utilizing less than 8 million\nparameters. Extensive evaluations on the BraTS20, BraTS21, BraTS23, and MSD\ndatasets demonstrate a substantial performance improvement coupled with a\nsignificant reduction in computational complexity. Code is available at:\nhttps://github.com/BheeshmSharma/RASALoRE-BMVC-2025/.", "AI": {"tldr": "该研究提出了一种基于区域感知空间注意力和位置嵌入的两阶段弱监督异常检测方法，用于脑MRI图像的异常检测，显示出优秀的检测性能及较低的计算复杂度。", "motivation": "研究动机在于利用弱监督学习在只能够获得切片级别标签而无法获得精确像素级异常标注时，依然能够实现快速且准确的脑异常检测。", "method": "该论文提出了一种名为RASALoRE的新型两阶段弱监督异常检测框架。第一阶段使用判别式双提示微调机制生成基于切片级别标签的伪弱掩模，作为粗略定位线索。第二阶段使用带有基于位置的随机嵌入的区域感知空间注意力机制的分割网络，使模型能够有效关注异常区域。", "result": "该方法在BraTS20、BraTS21、BraTS23和MSD等数据集上进行的广泛评估表明，它不仅性能优越，而且显著减少了计算复杂度。", "conclusion": "实验结果表明RASALoRE不仅在脑MRI异常检测任务中达到了最先进的性能，而且由于其模型参数数量少于800万，其计算复杂度显著降低。"}}
{"id": "2510.07881", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07881", "abs": "https://arxiv.org/abs/2510.07881", "authors": ["Heyang Liu", "Yuhao Wang", "Ziyang Cheng", "Ronghua Wu", "Qunshan Gu", "Yanfeng Wang", "Yu Wang"], "title": "CS3-Bench: Evaluating and Enhancing Speech-to-Speech LLMs for Mandarin-English Code-Switching", "comment": null, "summary": "The advancement of multimodal large language models has accelerated the\ndevelopment of speech-to-speech interaction systems. While natural monolingual\ninteraction has been achieved, we find existing models exhibit deficiencies in\nlanguage alignment. In our proposed Code-Switching Speech-to-Speech Benchmark\n(CS3-Bench), experiments on 7 mainstream models demonstrate a relative\nperformance drop of up to 66% in knowledge-intensive question answering and\nvarying degrees of misunderstanding in open-ended conversations. Starting from\na model with severe performance deterioration, we propose both data\nconstructions and training approaches to improve the language alignment\ncapabilities, specifically employing Chain of Recognition (CoR) to enhance\nunderstanding and Keyword Highlighting (KH) to guide generation. Our approach\nimproves the knowledge accuracy from 25.14% to 46.13%, with open-ended\nunderstanding rate from 64.5% to 86.5%, and significantly reduces pronunciation\nerrors in the secondary language. CS3-Bench is available at\nhttps://huggingface.co/datasets/VocalNet/CS3-Bench.", "AI": {"tldr": "研究人员发现多模态大型语言模型在处理代码切换任务时表现较差。使用Chain of Recognition和Keyword Highlighting技术显著提高了模型的知识准确性和开放对话理解率，且降低了次要语言中的发音错误率。", "motivation": "动机是解决现有模型在多语言交互，特别是代码切换语音到语音交互中存在的语言对齐问题，这些问题可能导致理解或产生错误。", "method": "研究中使用了7种主要的多模态大型语言模型，并且通过实验发现它们在处理知识密集型和开放性对话任务时存在不足。为了改进这些模型的能力，作者提出了Chain of Recognition和Keyword Highlighting两种方法。", "result": "{\n  \"tldr\": \"该研究发现多模态大型语言模型在语言对齐方面存在不足，特别是在代码切换的语音到语音的任务中表现较差。通过引入Chain of Recognition和Keyword Highlighting技术，他们显著提高了知识准确性与开放对话理解率，同时减少了次要语言中的发音错误。\", \n  \"motivation\": \"尽管自然单语交互已经实现，但现有模型在语言对齐能力上存在缺陷，特别是在处理代码切换的任务时。\", \n  \"method\": \"实验基于7种主流模型，并使用Chain of Recognition（CoR）和Keyword Highlighting（KH）两种方法来改进语言对齐能力。\", \n  \"result\": \"知识准确性从25.14%提高到46.13%，开放对话理解率从64.5%提升到86.5%，并且减少了次要语言中的发音错误。\", \n  \"conclusion\": \"通过引入新的数据构建和训练方法，本文提出的解决方案显著提高了多模态大规模语言模型的语言对齐能力。\"}\n}", "conclusion": "通过引入新的数据构建和训练方法，特别是在Chain of Recognition和Keyword Highlighting技术的帮助下，多模态语言模型在处理代码切换任务时的语言对齐能力得到了显著提升。"}}
{"id": "2510.08054", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08054", "abs": "https://arxiv.org/abs/2510.08054", "authors": ["Moon Ye-Bin", "Roy Miles", "Tae-Hyun Oh", "Ismail Elezi", "Jiankang Deng"], "title": "RetouchLLM: Training-free White-box Image Retouching", "comment": null, "summary": "Image retouching not only enhances visual quality but also serves as a means\nof expressing personal preferences and emotions. However, existing\nlearning-based approaches require large-scale paired data and operate as black\nboxes, making the retouching process opaque and limiting their adaptability to\nhandle diverse, user- or image-specific adjustments. In this work, we propose\nRetouchLLM, a training-free white-box image retouching system, which requires\nno training data and performs interpretable, code-based retouching directly on\nhigh-resolution images. Our framework progressively enhances the image in a\nmanner similar to how humans perform multi-step retouching, allowing\nexploration of diverse adjustment paths. It comprises of two main modules: a\nvisual critic that identifies differences between the input and reference\nimages, and a code generator that produces executable codes. Experiments\ndemonstrate that our approach generalizes well across diverse retouching\nstyles, while natural language-based user interaction enables interpretable and\ncontrollable adjustments tailored to user intent.", "AI": {"tldr": "RetouchLLM是一种无需训练数据的白盒图像修复系统，它通过可视化评判模块和代码生成模块，可以直接对高分辨率图像进行可解释的修复，增强了用户交互的灵活性和图像修复的适应性。", "motivation": "现有的学习型方法需要大量配对数据，并且作为一个黑盒操作，使得修复过程不透明，限制了其适应性以处理多样化的、用户或图像特定的调整需求。", "method": "提出了一种无需训练的白盒图像修复系统RetouchLLM，该系统不依赖大规模配对数据，直接对高分辨率图像进行基于代码的可解释修复。框架以类似于人类多步骤修复的方式逐步增强图像，包含一个可视化评判模块和一个代码生成模块。", "result": "实验表明，该方法在多种修复风格上具有良好的泛化能力，基于自然语言的用户交互使得调整更符合用户意图，更加可解释和可控。", "conclusion": "RetouchLLM作为无需训练的框架，为高分辨率图像的基于代码的修复提供了可行性，支持多样化的调整路径，实现了人机之间可解释和可控的修复过程。"}}
{"id": "2510.07884", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07884", "abs": "https://arxiv.org/abs/2510.07884", "authors": ["Houcheng Jiang", "Junfeng Fang", "Jiaxin Wu", "Tianyu Zhang", "Chen Gao", "Yong Li", "Xiang Wang", "Xiangnan He", "Yang Deng"], "title": "Contrastive Weak-to-strong Generalization", "comment": null, "summary": "Weak-to-strong generalization provides a promising paradigm for scaling large\nlanguage models (LLMs) by training stronger models on samples from aligned\nweaker ones, without requiring human feedback or explicit reward modeling.\nHowever, its robustness and generalization are hindered by the noise and biases\nin weak-model outputs, which limit its applicability in practice. To address\nthis challenge, we leverage implicit rewards, which approximate explicit\nrewards through log-likelihood ratios, and reveal their structural equivalence\nwith Contrastive Decoding (CD), a decoding strategy shown to reduce noise in\nLLM generation. Building on this connection, we propose Contrastive\nWeak-to-Strong Generalization (ConG), a framework that employs contrastive\ndecoding between pre- and post-alignment weak models to generate higher-quality\nsamples. This approach enables more reliable capability transfer, denoising,\nand improved robustness, substantially mitigating the limitations of\ntraditional weak-to-strong methods. Empirical results across different model\nfamilies confirm consistent improvements, demonstrating the generality and\neffectiveness of ConG. Taken together, our findings highlight the potential of\nConG to advance weak-to-strong generalization and provide a promising pathway\ntoward AGI.", "AI": {"tldr": "提出ConG框架，通过对比解码方式，改进从弱到强的大语言模型推广，提高模型泛化能力和鲁棒性。", "motivation": "解决弱模型输出中的噪声和偏差带来的传统弱到强推广方法的局限性。其目标是提高大语言模型的强模型训练的稳健性和泛化能力，尤其是在没有人工反馈或明确奖励建模的情况下。", "method": "提出Contrastive Weak-to-Strong Generalization (ConG)框架，利用对比解码在预对齐和后对齐的弱模型之间生成更高质量的样本。这种方法通过对比解码来提高弱模型到强模型的迁移能力，减少噪声并提高鲁棒性。", "result": "实验结果显示，ConG方法在降低噪声和提高强模型训练的质量方面取得了显著改进，展示了其在弱到强推广领域中的潜力。", "conclusion": "实验证明了ConG在不同模型家族中的普遍适用性和有效性，为弱到强推广的进步提供了新途径，也为通往AGI提供了一条可行的路径。"}}
{"id": "2510.08060", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08060", "abs": "https://arxiv.org/abs/2510.08060", "authors": ["Giulio Weikmann", "Gianmarco Perantoni", "Lorenzo Bruzzone"], "title": "A class-driven hierarchical ResNet for classification of multispectral remote sensing images", "comment": "11 pages, 2 figures, accepted conference paper at SPIE REMOTE\n  SENSING, 3-7 September 2023, Amsterdam, Netherlands", "summary": "This work presents a multitemporal class-driven hierarchical Residual Neural\nNetwork (ResNet) designed for modelling the classification of Time Series (TS)\nof multispectral images at different semantical class levels. The architecture\nconsists of a modification of the ResNet where we introduce additional branches\nto perform the classification at the different hierarchy levels and leverage on\nhierarchy-penalty maps to discourage incoherent hierarchical transitions within\nthe classification. In this way, we improve the discrimination capabilities of\nclasses at different levels of semantic details and train a modular\narchitecture that can be used as a backbone network for introducing new\nspecific classes and additional tasks considering limited training samples\navailable. We exploit the class-hierarchy labels to train efficiently the\ndifferent layers of the architecture, allowing the first layers to train faster\non the first levels of the hierarchy modeling general classes (i.e., the\nmacro-classes) and the intermediate classes, while using the last ones to\ndiscriminate more specific classes (i.e., the micro-classes). In this way, the\ntargets are constrained in following the hierarchy defined, improving the\nclassification of classes at the most detailed level. The proposed modular\nnetwork has intrinsic adaptation capability that can be obtained through fine\ntuning. The experimental results, obtained on two tiles of the Amazonian Forest\non 12 monthly composites of Sentinel 2 images acquired during 2019, demonstrate\nthe effectiveness of the hierarchical approach in both generalizing over\ndifferent hierarchical levels and learning discriminant features for an\naccurate classification at the micro-class level on a new target area, with a\nbetter representation of the minoritarian classes.", "AI": {"tldr": "本文提出了一种改进ResNet的层次残差神经网络，能够有效地对多光谱图像时间序列进行分类，并具备处理有限训练样本的能力。", "motivation": "该方法旨在提高不同详细程度的分类识别能力，并训练一个模块化的架构，可以作为骨干网络用于引入新的特定类别和考虑有限训练样本的额外任务。", "method": "本文提出了一种多时态类驱动的层次残差神经网络（ResNet），用于对不同语义层次的多光谱图像时间序列进行分类。网络架构是基于修改的ResNet，引入了额外的分支来进行不同层次的分类，并利用层次惩罚图来避免分类中的不一致层次转换。", "result": "实验结果在2019年获取的亚马逊森林两个地块上12个每月合成的Sentinel 2图像中得到，表明层次方法在不同层次上的泛化能力和对新目标区域的微分类准确分类中都能表现出较好的性能。", "conclusion": "提出的模块化网络具有内在的适应能力，可通过对网络进行微调实现，展示了一种能够有效处理不同详细程度类别分类的方法。"}}
{"id": "2510.07890", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07890", "abs": "https://arxiv.org/abs/2510.07890", "authors": ["Verena Blaschke", "Miriam Winkler", "Barbara Plank"], "title": "Standard-to-Dialect Transfer Trends Differ across Text and Speech: A Case Study on Intent and Topic Classification in German Dialects", "comment": null, "summary": "Research on cross-dialectal transfer from a standard to a non-standard\ndialect variety has typically focused on text data. However, dialects are\nprimarily spoken, and non-standard spellings are known to cause issues in text\nprocessing. We compare standard-to-dialect transfer in three settings: text\nmodels, speech models, and cascaded systems where speech first gets\nautomatically transcribed and then further processed by a text model. In our\nexperiments, we focus on German and multiple German dialects in the context of\nwritten and spoken intent and topic classification. To that end, we release the\nfirst dialectal audio intent classification dataset. We find that the\nspeech-only setup provides the best results on the dialect data while the\ntext-only setup works best on the standard data. While the cascaded systems lag\nbehind the text-only models for German, they perform relatively well on the\ndialectal data if the transcription system generates normalized, standard-like\noutput.", "AI": {"tldr": "研究比较了文本模型、语音模型和级联系统在标准到非标准德语方言转换中的效果，首次发布了方言音频意图分类数据集。", "motivation": "跨方言转换的研究主要集中在文本数据上，但方言主要是口头交流的，非标准拼写会在文本处理中造成问题。", "method": "", "result": "", "conclusion": "纯语音系统的设置在方言数据上表现最好，而纯文本系统的设置在标准数据上效果最佳。级联系统在非标准数据上的表现相当好，如果转录系统产生了标准化的输出。"}}
{"id": "2510.08067", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08067", "abs": "https://arxiv.org/abs/2510.08067", "authors": ["Junyu Shi", "Minghui Li", "Junguo Zuo", "Zhifei Yu", "Yipeng Lin", "Shengshan Hu", "Ziqi Zhou", "Yechao Zhang", "Wei Wan", "Yinzhe Xu", "Leo Yu Zhang"], "title": "Towards Real-World Deepfake Detection: A Diverse In-the-wild Dataset of Forgery Faces", "comment": null, "summary": "Deepfakes, leveraging advanced AIGC (Artificial Intelligence-Generated\nContent) techniques, create hyper-realistic synthetic images and videos of\nhuman faces, posing a significant threat to the authenticity of social media.\nWhile this real-world threat is increasingly prevalent, existing academic\nevaluations and benchmarks for detecting deepfake forgery often fall short to\nachieve effective application for their lack of specificity, limited deepfake\ndiversity, restricted manipulation techniques.To address these limitations, we\nintroduce RedFace (Real-world-oriented Deepfake Face), a specialized facial\ndeepfake dataset, comprising over 60,000 forged images and 1,000 manipulated\nvideos derived from authentic facial features, to bridge the gap between\nacademic evaluations and real-world necessity. Unlike prior benchmarks, which\ntypically rely on academic methods to generate deepfakes, RedFace utilizes 9\ncommercial online platforms to integrate the latest deepfake technologies found\n\"in the wild\", effectively simulating real-world black-box scenarios.Moreover,\nRedFace's deepfakes are synthesized using bespoke algorithms, allowing it to\ncapture diverse and evolving methods used by real-world deepfake creators.\nExtensive experimental results on RedFace (including cross-domain,\nintra-domain, and real-world social network dissemination simulations) verify\nthe limited practicality of existing deepfake detection schemes against\nreal-world applications. We further perform a detailed analysis of the RedFace\ndataset, elucidating the reason of its impact on detection performance compared\nto conventional datasets. Our dataset is available at:\nhttps://github.com/kikyou-220/RedFace.", "AI": {"tldr": "论文介绍了RedFace数据集，该数据集通过使用9个商业在线平台整合最新的深伪技术来弥补现有深伪检测数据集的局限性，可以帮助实现更有效的深伪检测应用。", "motivation": "现有的深伪检测评估和基准测试往往由于缺乏具体性、受限的深伪多样性以及限制性的操作技术，无法实现有效的应用。为了针对这些限制，研究人员提出了RedFace数据集，以弥合学术评估和现实需求之间的差距。", "method": "本研究介绍了RedFace数据集，这是一个专为检测深伪(Deepfake)伪造而设计的面部数据集，包含超过60,000张伪造图像和1,000个篡改过的视频。与以往的基准测试不同，RedFace数据集利用了9个商业在线平台来整合实际存在的最新深伪技术，模拟真实的黑盒场景。", "result": "研究表明，现有的深伪检测方案在真实世界应用中的实用性有限，而通过使用RedFace数据集可以有效提升深伪检测的准确性。", "conclusion": "实验结果表明，现有深伪检测方案对比实际应用存在局限性。RedFace数据集的使用能够更准确地模拟真实世界的深伪情况，研究中还详细分析了RedFace数据集对于检测性能的影响，并指出其明显优于常规数据集的优势。"}}
{"id": "2510.07892", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07892", "abs": "https://arxiv.org/abs/2510.07892", "authors": ["Hyeonseok Moon", "Seongtae Hong", "Jaehyung Seo", "Heuiseok Lim"], "title": "Metric Calculating Benchmark: Code-Verifiable Complicate Instruction Following Benchmark for Large Language Models", "comment": "Accepted to the EMNLP2025", "summary": "Recent frontier-level LLMs have saturated many previously difficult\nbenchmarks, leaving little room for further differentiation. This progress\nhighlights the need for challenging benchmarks that provide objective\nverification. In this paper, we introduce MCBench, a benchmark designed to\nevaluate whether LLMs can execute string-matching NLP metrics by strictly\nfollowing step-by-step instructions. Unlike prior benchmarks that depend on\nsubjective judgments or general reasoning, MCBench offers an objective,\ndeterministic and codeverifiable evaluation. This setup allows us to\nsystematically test whether LLMs can maintain accurate step-by-step execution,\nincluding instruction adherence, numerical computation, and long-range\nconsistency in handling intermediate results. To ensure objective evaluation of\nthese abilities, we provide a parallel reference code that can evaluate the\naccuracy of LLM output. We provide three evaluative metrics and three benchmark\nvariants designed to measure the detailed instruction understanding capability\nof LLMs. Our analyses show that MCBench serves as an effective and objective\ntool for evaluating the capabilities of cutting-edge LLMs.", "AI": {"tldr": "本文提出了MCBench，一种可以客观、确定和可编程验证的基准测试，用于评估大语言模型的逐步执行和指令理解能力。", "motivation": "要设计出具有挑战性的基准测试工具，以便客观地验证大模型的性能，特别是对逐步执行能力和指令理解能力的评估。", "method": "介绍了一个名为MCBench的基准测试工具，该工具用于评估大模型是否能按步骤执行字符串匹配NLP任务，区别于依赖主观判断或通用推理的先前基准测试。", "result": "MCBench能够作为衡量最先进大模型能力的有效且客观工具。", "conclusion": "MCBench被证明是一个有效的、客观的评估最先进语言模型能力的工具。"}}
{"id": "2510.08073", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08073", "abs": "https://arxiv.org/abs/2510.08073", "authors": ["Shuhai Zhang", "ZiHao Lian", "Jiahao Yang", "Daiyuan Li", "Guoxuan Pang", "Feng Liu", "Bo Han", "Shutao Li", "Mingkui Tan"], "title": "Physics-Driven Spatiotemporal Modeling for AI-Generated Video Detection", "comment": "Accepted at NeurIPS 2025 spotlight", "summary": "AI-generated videos have achieved near-perfect visual realism (e.g., Sora),\nurgently necessitating reliable detection mechanisms. However, detecting such\nvideos faces significant challenges in modeling high-dimensional spatiotemporal\ndynamics and identifying subtle anomalies that violate physical laws. In this\npaper, we propose a physics-driven AI-generated video detection paradigm based\non probability flow conservation principles. Specifically, we propose a\nstatistic called Normalized Spatiotemporal Gradient (NSG), which quantifies the\nratio of spatial probability gradients to temporal density changes, explicitly\ncapturing deviations from natural video dynamics. Leveraging pre-trained\ndiffusion models, we develop an NSG estimator through spatial gradients\napproximation and motion-aware temporal modeling without complex motion\ndecomposition while preserving physical constraints. Building on this, we\npropose an NSG-based video detection method (NSG-VD) that computes the Maximum\nMean Discrepancy (MMD) between NSG features of the test and real videos as a\ndetection metric. Last, we derive an upper bound of NSG feature distances\nbetween real and generated videos, proving that generated videos exhibit\namplified discrepancies due to distributional shifts. Extensive experiments\nconfirm that NSG-VD outperforms state-of-the-art baselines by 16.00% in Recall\nand 10.75% in F1-Score, validating the superior performance of NSG-VD. The\nsource code is available at https://github.com/ZSHsh98/NSG-VD.", "AI": {"tldr": "该研究提出了一种基于物理原理的AI生成视频检测方法NSG-VD，该方法基于NSG特征的MMD实现，实验表明它在检测AI生成视频方面优于现有方法。", "motivation": "面对高度现实的AI生成视频（如Sora），需要开发可靠的检测手段来辨别。但在建模高维时空动态和识别违反物理定律的细微异常方面存在重大挑战。因此，研究提出了一种新的检测手段，改善这一现状。", "method": "提出了基于概率流守恒原理的物理驱动AI生成视频检测范式。具体而言，提出了一个称为标准化时空梯度（NSG）的统计指标，该指标量化了空间概率梯度与时间密度变化的比例，明确捕捉到了与自然视频动态的偏差。利用预训练的扩散模型，通过空间梯度近似和感知时间建模的方法开发了NSG估计器，无需复杂的运动分解即可保持物理约束。进而，提出了一个基于NSG的视频检测方法（NSG-VD），该方法通过计算测试视频与真实视频NSG特征之间的最大平均差异（MMD）作为检测指标。最后，推导出了真实视频和生成视频间NSG特征距离的上界，证明了由于分布偏移，生成视频展示出放大的偏差。", "result": "实验显示NSG-VD方法在召回率上比现有方法高出16%，F1分数上高出10.75%，证明了其检测性能的优越性。", "conclusion": "实验结果验证了NSG-VD在召回率和F1分数上分别比最先进的基线高出16.00%和10.75%，证明了NSG-VD的优越性能。源代码在https://github.com/ZSHsh98/NSG-VD提供。"}}
{"id": "2510.07896", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07896", "abs": "https://arxiv.org/abs/2510.07896", "authors": ["Jiayu Yang", "Yuxuan Fan", "Songning Lai", "Shengen Wu", "Jiaqi Tang", "Chun Kang", "Zhijiang Guo", "Yutao Yue"], "title": "ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall", "comment": null, "summary": "Large Language Models (LLMs) require efficient knowledge editing (KE) to\nupdate factual information, yet existing methods exhibit significant\nperformance decay in multi-hop factual recall. This failure is particularly\nacute when edits involve intermediate implicit subjects within reasoning\nchains. Through causal analysis, we reveal that this limitation stems from an\noversight of how chained knowledge is dynamically represented and utilized at\nthe neuron level. We discover that during multi hop reasoning, implicit\nsubjects function as query neurons, which sequentially activate corresponding\nvalue neurons across transformer layers to accumulate information toward the\nfinal answer, a dynamic prior KE work has overlooked. Guided by this insight,\nwe propose ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual\nRecall, a framework that leverages neuron-level attribution to identify and\nedit these critical query-value (Q-V) pathways. ACE provides a mechanistically\ngrounded solution for multi-hop KE, empirically outperforming state-of-the-art\nmethods by 9.44% on GPT-J and 37.46% on Qwen3-8B. Our analysis further reveals\nmore fine-grained activation patterns in Qwen3 and demonstrates that the\nsemantic interpretability of value neurons is orchestrated by query-driven\naccumulation. These findings establish a new pathway for advancing KE\ncapabilities based on the principled understanding of internal reasoning\nmechanisms.", "AI": {"tldr": "本文揭示了多步事实记忆中的知识编辑问题，提出ACE框架，利用神经元级别的归因而识别和编辑关键通路，大幅提升了性能表现。", "motivation": "大型语言模型（LLMs）需要高效的KE来更新事实信息，但是现有的方法在多步事实回溯中表现大大衰退。尤其是在编辑涉及推理链中的中间隐含主题时，这种情况尤为严重。", "method": "通过因果分析，我们揭示了这种局限性源于对链式知识在神经元层面动态表示和利用方式的忽视。我们发现，在多步推理过程中，隐含主体充当查询神经元，这些查询神经元依次激活跨变压器层的相关值神经元，以向最终答案累积信息，这是先前知识编辑工作所忽视的动态先验。基于这一见解，我们提出了ACE：归因控制知识编辑框架，它利用神经元级别归因而识别和编辑这些关键的查询-值通路。", "result": "ACE在GPT-J和Qwen3-8B上分别优于现有尖端技术9.44%和37.46%。进一步分析还揭示了Qwen3中更精细的激活模式，表明值神经元的语义可解释性是由查询驱动的累积所编排的。", "conclusion": "这些发现建立了一条基于对内部推理机制的原理性理解来推进KE能力的新途径。"}}
{"id": "2510.08094", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08094", "abs": "https://arxiv.org/abs/2510.08094", "authors": ["Ziqi Zhou", "Menghao Deng", "Yufei Song", "Hangtao Zhang", "Wei Wan", "Shengshan Hu", "Minghui Li", "Leo Yu Zhang", "Dezhong Yao"], "title": "DarkHash: A Data-Free Backdoor Attack Against Deep Hashing", "comment": "Accepted by TIFS 2025", "summary": "Benefiting from its superior feature learning capabilities and efficiency,\ndeep hashing has achieved remarkable success in large-scale image retrieval.\nRecent studies have demonstrated the vulnerability of deep hashing models to\nbackdoor attacks. Although these studies have shown promising attack results,\nthey rely on access to the training dataset to implant the backdoor. In the\nreal world, obtaining such data (e.g., identity information) is often\nprohibited due to privacy protection and intellectual property concerns.\nEmbedding backdoors into deep hashing models without access to the training\ndata, while maintaining retrieval accuracy for the original task, presents a\nnovel and challenging problem. In this paper, we propose DarkHash, the first\ndata-free backdoor attack against deep hashing. Specifically, we design a novel\nshadow backdoor attack framework with dual-semantic guidance. It embeds\nbackdoor functionality and maintains original retrieval accuracy by fine-tuning\nonly specific layers of the victim model using a surrogate dataset. We consider\nleveraging the relationship between individual samples and their neighbors to\nenhance backdoor attacks during training. By designing a topological alignment\nloss, we optimize both individual and neighboring poisoned samples toward the\ntarget sample, further enhancing the attack capability. Experimental results on\nfour image datasets, five model architectures, and two hashing methods\ndemonstrate the high effectiveness of DarkHash, outperforming existing\nstate-of-the-art backdoor attack methods. Defense experiments show that\nDarkHash can withstand existing mainstream backdoor defense methods.", "AI": {"tldr": "本文提出DarkHash，一种数据自由后门攻击技术，能有效植入后门功能而不影响原有的深度哈希模型性能，并展示了其在不同模型和哈希技术上的优越性。", "motivation": "现有研究依赖于访问训练数据来植入后门，但在实际应用中，获取此类数据（如身份信息）是受限的。因此，探索一种无需访问训练数据，并且能够维持原有检索准确度的后门植入方法成为亟待解决的问题。", "method": "我们提出DarkHash，这是一种无需访问训练数据的新型数据自由后门攻击方法。通过利用双重语义引导的方法，在不降低原有检索准确度的情况下，仅使用替代数据集微调受害者模型中的特定层以嵌入后门功能。此外，我们设计了一种拓扑对齐损失，优化被污染样本及其邻居向目标样本靠近，从而进一步增强攻击效果。", "result": "通过在四个图像数据集、五种模型架构以及两种哈希方法上进行的实验，验证了DarkHash的高度有效性，超过了现有的最先进后门攻击方法。防御实验表明DarkHash能够抵抗现有的主流后门防御方法。", "conclusion": "DarkHash首次提出了无需训练数据的后门攻击框架，实现了高效且准确的后门嵌入，并在不同程度上抵抗现有的后门防御策略。"}}
{"id": "2510.07912", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07912", "abs": "https://arxiv.org/abs/2510.07912", "authors": ["Fanwei Zhua", "Jiaxuan He", "Xiaoxiao Chen", "Zulong Chen", "Quan Lu", "Chenrui Mei"], "title": "Towards Human-Like Grading: A Unified LLM-Enhanced Framework for Subjective Question Evaluation", "comment": null, "summary": "Automatic grading of subjective questions remains a significant challenge in\nexamination assessment due to the diversity in question formats and the\nopen-ended nature of student responses. Existing works primarily focus on a\nspecific type of subjective question and lack the generality to support\ncomprehensive exams that contain diverse question types. In this paper, we\npropose a unified Large Language Model (LLM)-enhanced auto-grading framework\nthat provides human-like evaluation for all types of subjective questions\nacross various domains. Our framework integrates four complementary modules to\nholistically evaluate student answers. In addition to a basic text matching\nmodule that provides a foundational assessment of content similarity, we\nleverage the powerful reasoning and generative capabilities of LLMs to: (1)\ncompare key knowledge points extracted from both student and reference answers,\n(2) generate a pseudo-question from the student answer to assess its relevance\nto the original question, and (3) simulate human evaluation by identifying\ncontent-related and non-content strengths and weaknesses. Extensive experiments\non both general-purpose and domain-specific datasets show that our framework\nconsistently outperforms traditional and LLM-based baselines across multiple\ngrading metrics. Moreover, the proposed system has been successfully deployed\nin real-world training and certification exams at a major e-commerce\nenterprise.", "AI": {"tldr": "研究提出了一种基于大语言模型的统一自动评分框架，可以对各类主观题进行评估，并表明在多个数据集上性能优于其他系统。", "motivation": "目前的自动评分系统主要关注特定类型的主观题，缺乏对综合性考试中多种题型的支持能力，这成为了考试评估中的一个挑战。", "method": "本研究提出了一种统一的大语言模型（LLM）增强的自动评分框架，旨在为所有类型的主观题提供类似人类的评估。该框架集成了四个互补模块，包括基础文本匹配模块、关键知识点比对模块、伪问题生成模块以及模拟人类评估的模块。", "result": "实验结果表明，该框架在通用数据集和专业数据集上，相对于传统的和基于LLM的基线系统，在多种评分指标上都表现出色。", "conclusion": "该系统已经在一家大型电子商务企业的实际训练和认证考试中成功部署。"}}
{"id": "2510.08096", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08096", "abs": "https://arxiv.org/abs/2510.08096", "authors": ["Ankit Gahlawat", "Anirban Mukherjee", "Dinesh Babu Jayagopi"], "title": "Efficient Label Refinement for Face Parsing Under Extreme Poses Using 3D Gaussian Splatting", "comment": "Accepted to VCIP 2025 (International Conference on Visual\n  Communications and Image Processing 2025)", "summary": "Accurate face parsing under extreme viewing angles remains a significant\nchallenge due to limited labeled data in such poses. Manual annotation is\ncostly and often impractical at scale. We propose a novel label refinement\npipeline that leverages 3D Gaussian Splatting (3DGS) to generate accurate\nsegmentation masks from noisy multiview predictions. By jointly fitting two\n3DGS models, one to RGB images and one to their initial segmentation maps, our\nmethod enforces multiview consistency through shared geometry, enabling the\nsynthesis of pose-diverse training data with only minimal post-processing.\nFine-tuning a face parsing model on this refined dataset significantly improves\naccuracy on challenging head poses, while maintaining strong performance on\nstandard views. Extensive experiments, including human evaluations, demonstrate\nthat our approach achieves superior results compared to state-of-the-art\nmethods, despite requiring no ground-truth 3D annotations and using only a\nsmall set of initial images. Our method offers a scalable and effective\nsolution for improving face parsing robustness in real-world settings.", "AI": {"tldr": "The paper introduces a label refinement pipeline leveraging 3D Gaussian Splatting to improve face parsing accuracy in extreme poses. It enables better parsing performance on challenging poses with minimal manual intervention.", "motivation": "The goal is to tackle the difficulty of achieving accurate face parsing in extreme viewing angles due to lack of labeled data in those poses, and to provide a solution that is both scalable and effective for real-world applications without needing a large amount of manual labeling.", "method": "Our method involves generating accurate segmentation masks using a novel label refinement pipeline that employs 3D Gaussian Splatting (3DGS). This pipeline fits two 3DGS models jointly, one to RGB images and the other to initial segmentation maps, thus enforcing multiview consistency through shared geometry to synthesize pose-diverse training data.", "result": "Experiments, including human evaluations, prove that this technique surpasses state-of-the-art methods in terms of face parsing accuracy, especially for atypical poses. Achievement of these results rely on pose-diverse training data generated from initial noisy predictions.", "conclusion": "The approach leads to improved accuracy on challenging head poses while retaining high performance on standard views, indicating that by using minimal post-processing and no ground-truth 3D annotations, the proposed method outperforms existing state-of-the-art methods."}}
