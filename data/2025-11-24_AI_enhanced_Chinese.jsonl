{"id": "2511.16680", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16680", "abs": "https://arxiv.org/abs/2511.16680", "authors": ["Happymore Masoka"], "title": "Shona spaCy: A Morphological Analyzer for an Under-Resourced Bantu Language", "comment": null, "summary": "Despite rapid advances in multilingual natural language processing (NLP), the Bantu language Shona remains under-served in terms of morphological analysis and language-aware tools. This paper presents Shona spaCy, an open-source, rule-based morphological pipeline for Shona built on the spaCy framework. The system combines a curated JSON lexicon with linguistically grounded rules to model noun-class prefixes (Mupanda 1-18), verbal subject concords, tense-aspect markers, ideophones, and clitics, integrating these into token-level annotations for lemma, part-of-speech, and morphological features. The toolkit is available via pip install shona-spacy, with source code at https://github.com/HappymoreMasoka/shona-spacy and a PyPI release at https://pypi.org/project/shona-spacy/0.1.4/. Evaluation on formal and informal Shona corpora yields 90% POS-tagging accuracy and 88% morphological-feature accuracy, while maintaining transparency in its linguistic decisions. By bridging descriptive grammar and computational implementation, Shona spaCy advances NLP accessibility and digital inclusion for Shona speakers and provides a template for morphological analysis tools for other under-resourced Bantu languages.", "AI": {"tldr": "本文介绍了一个名为Shona spaCy的开源的、基于规则的面向尚邦语形态分析的工具包，它结合了精心设计的JSON词典和基于语言学的规则来改善尚邦语的自然语言处理能力。", "motivation": "本文旨在为尚邦语提供形态分析和语言意识工具，弥补其在多语种自然语言处理中的不足。", "method": "通过在一个已有的spaCy框架上，结合精细的JSON词典和基于语言学的规则，开发一个名为Shona spaCy的程序，以分析尚邦语的名词类别前缀、谓语主语一致性、时态-体标、拟声词和后缀等。", "result": "在正式和非正式尚邦语文本上进行的测试显示，这个程序的词性标注准确率为90%，形态特征准确率为88%。", "conclusion": "这个工具为尚邦语社区带来了NLP的可访问性和数字包容性，并且可以为其他资源匮乏的尚桑语提供模板。"}}
{"id": "2511.16681", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16681", "abs": "https://arxiv.org/abs/2511.16681", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "Towards Hyper-Efficient RAG Systems in VecDBs: Distributed Parallel Multi-Resolution Vector Search", "comment": "Accepted to IEEE International Conference on Parallel and Distributed Systems 2025 (ICPADS 2025 Oral)", "summary": "Retrieval-Augmented Generation (RAG) systems have become a dominant approach to augment large language models (LLMs) with external knowledge. However, existing vector database (VecDB) retrieval pipelines rely on flat or single-resolution indexing structures, which cannot adapt to the varying semantic granularity required by diverse user queries. This limitation leads to suboptimal trade-offs between retrieval speed and contextual relevance.\n  To address this, we propose \\textbf{Semantic Pyramid Indexing (SPI)}, a novel multi-resolution vector indexing framework that introduces query-adaptive resolution control for RAG in VecDBs. Unlike existing hierarchical methods that require offline tuning or separate model training, SPI constructs a semantic pyramid over document embeddings and dynamically selects the optimal resolution level per query through a lightweight classifier. This adaptive approach enables progressive retrieval from coarse-to-fine representations, significantly accelerating search while maintaining semantic coverage.\n  We implement SPI as a plugin for both FAISS and Qdrant backends and evaluate it across multiple RAG tasks including MS MARCO, Natural Questions, and multimodal retrieval benchmarks. SPI achieves up to \\textbf{5.7$\\times$} retrieval speedup and \\textbf{1.8$\\times$} memory efficiency gain while improving end-to-end QA F1 scores by up to \\textbf{2.5 points} compared to strong baselines. Our theoretical analysis provides guarantees on retrieval quality and latency bounds, while extensive ablation studies validate the contribution of each component. The framework's compatibility with existing VecDB infrastructures makes it readily deployable in production RAG systems. Code is availabe at \\href{https://github.com/FastLM/SPI_VecDB}{https://github.com/FastLM/SPI\\_VecDB}.", "AI": {"tldr": "本文提出了一种新颖的多分辨率向量索引框架Semantic Pyramid Indexing (SPI)，针对Retrieval-Augmented Generation (RAG)系统存在的检索速度与上下文相关性之间的权衡问题提供了一种解决方案，实现了5.7倍的检索加速和1.8倍的内存效率提升，同时提高了终端问题回答的准确性。", "motivation": "现有的向量数据库检索管道依赖于平坦或单分辨率索引结构，无法适应不同用户查询所需的语义粒度变化，导致检索速度和上下文相关性之间的次优权衡。", "method": "提出了Semantic Pyramid Indexing (SPI)，这是一种新型多分辨率向量索引框架，能够在向量数据库中为RAG实现查询自适应分辨率控制。SPI通过轻量级分类器动态选择每条查询的最优分辨率级别，允许从粗略到精细的表示中实现渐进式检索。", "result": "SPI作为插件实现到FAISS和Qdrant后端中，并在多个RAG任务上进行评估，包括MS MARCO、Natural Questions和多模态检索基准测试，实现了5.7倍的检索加速和1.8倍的内存效率提升，比强大的基线模型提高了终端问题回答准确度2.5个百分点。", "conclusion": "本文的SPI框架提供了检索质量和延迟的保证，并通过广泛的消融研究验证了每个组件的贡献，框架易于部署到现有向量数据库基础设施中作为RAG系统使用。"}}
{"id": "2511.16682", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.16682", "abs": "https://arxiv.org/abs/2511.16682", "authors": ["Linus Stuhlmann", "Mauricio Fadel Argerich", "Jonathan Fürst"], "title": "Bench360: Benchmarking Local LLM Inference from 360°", "comment": null, "summary": "Running large language models (LLMs) locally is becoming increasingly common. While the growing availability of small open-source models and inference engines has lowered the entry barrier, users now face an overwhelming number of configuration choices. Identifying an optimal configuration -- balancing functional and non-functional requirements -- requires substantial manual effort. While several benchmarks target LLM inference, they are designed for narrow evaluation goals and not user-focused. They fail to integrate relevant system and task-specific metrics into a unified, easy-to-use benchmark that supports multiple inference engines, usage scenarios, and quantization levels. To address this gap, we present Bench360 -- Benchmarking Local LLM Inference from 360°. Bench360 allows users to easily define their own custom tasks along with datasets and relevant task-specific metrics and then automatically benchmarks selected LLMs, inference engines, and quantization levels across different usage scenarios (single stream, batch & server). Bench360 tracks a wide range of metrics, including (1) system metrics -- such as Computing Performance (e.g., latency, throughput), Resource Usage (e.g., energy per query), and Deployment (e.g., cold start time) -- and (2) task-specific metrics such as ROUGE, F1 score or accuracy. We demonstrate Bench360 on four common LLM tasks -- General Knowledge & Reasoning, QA, Summarization and Text-to-SQL -- across three hardware platforms and four state of the art inference engines. Our results reveal several interesting trade-offs between task performance and system-level efficiency, highlighting the differences in inference engines and models. Most importantly, there is no single best setup for local inference, which strongly motivates the need for a framework such as Bench360.", "AI": {"tldr": "我们引入Bench360，一个用于本地大型语言模型推理的多维度基准测试工具，它可以让用户自定义任务和度量标准。我们展示了其对于四个常见任务的评估，发现多种硬件和引擎组合之间的权衡，强调了框架的需求。", "motivation": "当前已有的基准测试针对特定的评估目标设计，无法提供统一且容易使用的基准以支持多种推断引擎、使用场景和量化水平。本研究旨在提供一个工具来解决这个问题，特别是针对本地运行大型语言模型的用户面临的多个配置选择难题。", "method": "我们提出了Bench360，这是一个用于本地大型语言模型推理的各种情况的全面基准测试工具。它可以自定义任务及相关的数据集和指标，并自动评估选定的模型、推断引擎和量化水平在多种使用场景下的表现。Bench360跟踪广泛的度量指标，包括计算性能、资源使用、部署因素和任务特定指标。", "result": "我们在四个常见的大型语言模型任务（通用知识与推理、QA、摘要生成和文本到SQL）上展示了Bench360，覆盖了三个硬件平台和四个最先进的推断引擎。结果揭示了任务性能和系统级效率之间的有趣权衡，这强力证明了Bench360之类的框架的需求。", "conclusion": "鉴于我们在不同硬件平台和不同推断引擎上测试的发现，没有单一最好的本地推理设置。因此，Bench360框架对于进一步的研究和开发而言是至关重要的。"}}
{"id": "2511.16683", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16683", "abs": "https://arxiv.org/abs/2511.16683", "authors": ["Mohamed Mahdi"], "title": "How Well Do LLMs Understand Tunisian Arabic?", "comment": null, "summary": "Large Language Models (LLMs) are the engines driving today's AI agents. The better these models understand human languages, the more natural and user-friendly the interaction with AI becomes, from everyday devices like computers and smartwatches to any tool that can act intelligently. Yet, the ability of industrial-scale LLMs to comprehend low-resource languages, such as Tunisian Arabic (Tunizi), is often overlooked. This neglect risks excluding millions of Tunisians from fully interacting with AI in their own language, pushing them toward French or English. Such a shift not only threatens the preservation of the Tunisian dialect but may also create challenges for literacy and influence younger generations to favor foreign languages. In this study, we introduce a novel dataset containing parallel Tunizi, standard Tunisian Arabic, and English translations, along with sentiment labels. We benchmark several popular LLMs on three tasks: transliteration, translation, and sentiment analysis. Our results reveal significant differences between models, highlighting both their strengths and limitations in understanding and processing Tunisian dialects. By quantifying these gaps, this work underscores the importance of including low-resource languages in the next generation of AI systems, ensuring technology remains accessible, inclusive, and culturally grounded.", "AI": {"tldr": "研究通过基准测试多个大型语言模型在处理突尼斯方言上的性能，揭示了模型的差异性，强调了为低资源语言发展AI的重要性。", "motivation": "改善工业规模大型语言模型对低资源语言如Tunizi的理解，避免排除数百万突尼斯人，防止文化语言流失，并促进包容性技术发展。", "method": "创建包含Tunizi（突尼斯阿拉伯语方言）、标准突尼斯阿拉伯语和英语平行翻译及情感标签的数据集，并对几种流行的大型语言模型在转写、翻译和情感分析三个任务上进行基准测试。", "result": "结果显示，不同模型在理解和处理突尼斯方言方面存在显著差异，表明它们既有优点也有局限性。", "conclusion": "通过量化这些差异，本研究强调了在下一代AI系统中纳入低资源语言的重要性，确保技术的可访问性、包容性和文化相关性。"}}
{"id": "2511.16695", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16695", "abs": "https://arxiv.org/abs/2511.16695", "authors": ["Reetikaa Reddy Munnangi", "Barbara Giunti"], "title": "The persistence of painting styles", "comment": "8 pages, 4 figures, and 8 tables. Short YouTube video with highlights of the paper available at https://www.youtube.com/watch?v=sJSnidrEabM on the AATRN YouTube channel", "summary": "Art is a deeply personal and expressive medium, where each artist brings their own style, technique, and cultural background into their work. Traditionally, identifying artistic styles has been the job of art historians or critics, relying on visual intuition and experience. However, with the advancement of mathematical tools, we can explore art through more structured lens. In this work, we show how persistent homology (PH), a method from topological data analysis, provides objective and interpretable insights on artistic styles. We show how PH can, with statistical certainty, differentiate between artists, both from different artistic currents and from the same one, and distinguish images of an artist from an AI-generated image in the artist's style.", "AI": {"tldr": "本研究展示了如何利用持久同调(PH)方法客观地分析艺术风格，并能够区分不同艺术流派的艺术家以及艺术家作品与AI生成图像的差异。", "motivation": "传统的艺术风格鉴定依赖于艺术史家或评论家的视觉直觉和经验，本研究旨在通过数学工具提供更结构化的艺术分析方法。", "method": "本研究采用了拓扑数据分析中的持久同调(PH)方法，以提供关于艺术风格的客观和可解释的见解。", "result": "PH方法可以区分不同艺术风格和同一流派的艺术家作品，也能分辨出艺术家的作品和使用AI生成的艺术作品。", "conclusion": "研究结果表明，持久同调方法可以以统计上的确定性区分来自不同艺术流派的艺术家，以及同一个艺术流派内的艺术家，并能够区分艺术家的真实作品与AI生成的图像。"}}
{"id": "2511.16685", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16685", "abs": "https://arxiv.org/abs/2511.16685", "authors": ["Yuetian Zou", "Hanlei Zhang", "Hua Xu", "Songze Li", "Long Xiao"], "title": "Ellipsoid-Based Decision Boundaries for Open Intent Classification", "comment": null, "summary": "Textual open intent classification is crucial for real-world dialogue systems, enabling robust detection of unknown user intents without prior knowledge and contributing to the robustness of the system. While adaptive decision boundary methods have shown great potential by eliminating manual threshold tuning, existing approaches assume isotropic distributions of known classes, restricting boundaries to balls and overlooking distributional variance along different directions. To address this limitation, we propose EliDecide, a novel method that learns ellipsoid decision boundaries with varying scales along different feature directions. First, we employ supervised contrastive learning to obtain a discriminative feature space for known samples. Second, we apply learnable matrices to parameterize ellipsoids as the boundaries of each known class, offering greater flexibility than spherical boundaries defined solely by centers and radii. Third, we optimize the boundaries via a novelly designed dual loss function that balances empirical and open-space risks: expanding boundaries to cover known samples while contracting them against synthesized pseudo-open samples. Our method achieves state-of-the-art performance on multiple text intent benchmarks and further on a question classification dataset. The flexibility of the ellipsoids demonstrates superior open intent detection capability and strong potential for generalization to more text classification tasks in diverse complex open-world scenarios.", "AI": {"tldr": "本文提出EliDecide方法，通过学习椭球边界克服现有开放式意图分类方法中传统球形边界带来的局限性，实验表明该方法在不同场景下表现出色。", "motivation": "本文旨在解决现有文本开放式意图分类方法中因采用球形边界而忽视了不同特征方向上分布差异的问题，提出更为灵活的椭球边界方案以增强系统鲁棒性。", "method": "EliDecide方法包括三个主要步骤：第一步，通过监督对比学习获取已知样本的判别特征空间；第二步，使用学习矩阵定义每个已知类的椭球边界；第三步，通过双损失函数优化边界，以平衡已知样本的覆盖与合成的伪开放式样本之间的距离。", "result": "该论文解决了现有方法在文本开放式意图分类中采用的球形决策边界存在的局限性，提出了EliDecide方法。EliDecide通过监督对比学习获取已知样本的判别特征空间，并用可学习矩阵定义每个已知类的椭球边界，实现了更灵活的决策边界。此外，利用双损失函数优化边界，使之在扩展以覆盖已知样本的同时，收缩以对抗合成的伪开放样本。实验结果表明，EliDecide在多项文本意图分类基准测试中取得了最先进的性能，并在问题分类数据集上也表现出色。此方法显示出强大的开放式意图检测和泛化能力。", "conclusion": "EliDecide在多项文本意图分类基准测试中展示了最先进的性能，并在问题分类数据集上表现优异，证明了其在广泛复杂开放世界场景中的开放式意图检测和泛化能力的优越性。"}}
{"id": "2511.16711", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.16711", "abs": "https://arxiv.org/abs/2511.16711", "authors": ["Takuya Igaue", "Catia Correia-Caeiro", "Akito Yoshida", "Takako Miyabe-Nishiwaki", "Ryusuke Hayashi"], "title": "Motion Transfer-Enhanced StyleGAN for Generating Diverse Macaque Facial Expressions", "comment": null, "summary": "Generating animal faces using generative AI techniques is challenging because the available training images are limited both in quantity and variation, particularly for facial expressions across individuals. In this study, we focus on macaque monkeys, widely studied in systems neuroscience and evolutionary research, and propose a method to generate their facial expressions using a style-based generative image model (i.e., StyleGAN2). To address data limitations, we implemented: 1) data augmentation by synthesizing new facial expression images using a motion transfer to animate still images with computer graphics, 2) sample selection based on the latent representation of macaque faces from an initially trained StyleGAN2 model to ensure the variation and uniform sampling in training dataset, and 3) loss function refinement to ensure the accurate reproduction of subtle movements, such as eye movements. Our results demonstrate that the proposed method enables the generation of diverse facial expressions for multiple macaque individuals, outperforming models trained solely on original still images. Additionally, we show that our model is effective for style-based image editing, where specific style parameters correspond to distinct facial movements. These findings underscore the model's potential for disentangling motion components as style parameters, providing a valuable tool for research on macaque facial expressions.", "AI": {"tldr": "To overcome data limitations in generating macaque monkey facial expressions, the authors use StyleGAN2, enhancing data with motion synthesis and improving model performance through loss function refinement, demonstrating diverse expression generation and style-based editing.", "motivation": "The challenge of generating animal faces, specifically macaque monkey facial expressions, due to limited training images, both in quantity and variation, motivates this study.", "method": "The paper proposes a method using StyleGAN2 to generate macaque monkey facial expressions, addressing data limitations through data augmentation, latent sampling, and loss function refinement for accurate movement reproduction.", "result": "The proposed method successfully generates diverse facial expressions for macaque monkeys, outperforming models trained solely on original still images, and supports style-based image editing.", "conclusion": "The findings indicate the potential for disentangling motion components as style parameters, offering a valuable tool for macaque facial expression research."}}
{"id": "2511.16688", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16688", "abs": "https://arxiv.org/abs/2511.16688", "authors": ["Giulio Antonio Abbo", "Tony Belpaeme"], "title": "Prompt-Based Value Steering of Large Language Models", "comment": "9 pages, 1 figure, 4 tables. Presented at the 3rd International Workshop on Value Engineering in AI (VALE 2025), 28th European Conference on AI. To appear in Springer LNCS", "summary": "Large language models are increasingly used in applications where alignment with human values is critical. While model fine-tuning is often employed to ensure safe responses, this technique is static and does not lend itself to everyday situations involving dynamic values and preferences. In this paper, we present a practical, reproducible, and model-agnostic procedure to evaluate whether a prompt candidate can effectively steer generated text toward specific human values, formalising a scoring method to quantify the presence and gain of target values in generated responses. We apply our method to a variant of the Wizard-Vicuna language model, using Schwartz's theory of basic human values and a structured evaluation through a dialogue dataset. With this setup, we compare a baseline prompt to one explicitly conditioned on values, and show that value steering is possible even without altering the model or dynamically optimising prompts.", "AI": {"tldr": "本研究提出了一种方法来评估和量化提示是否能有效引导语言模型生成的文本朝向特定的人类价值观，并通过实验展示了其有效性。", "motivation": "虽然模型调优常用于确保安全的反应，但该技术是静态的，不适用于涉及动态价值观和偏好的日常情况。研究动机在于解决这一问题，使语言模型能够更好地与人类动态的价值观相一致。", "method": "本研究提出了一种实用的、可复制的、与模型无关的方法，用于评估候选提示是否能有效引导生成的文本朝向特定的人类价值观，并制定了一个量化目标值在生成响应中存在和增益的评分方法。", "result": "作者将该方法应用于Wizard-Vicuna语言模型的变体中，使用Schwartz的基本人类价值观理论和对话数据集进行了结构化评估。通过这种方法，作者能够将基线提示与明确以价值观为条件的提示进行比较，展示了即使不改变模型或动态优化提示，也能够有目的地引导价值观。", "conclusion": "研究结果显示，通过特定的提示，语言模型能够在不改变模型本身或动态优化提示的情况下引导生成文本朝向特定的人类价值观。这种方法为实现语言模型与人类价值观的动态一致性提供了可能。"}}
{"id": "2511.16712", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16712", "abs": "https://arxiv.org/abs/2511.16712", "authors": ["Ting Pan", "Ye Wang", "Peiguang Jing", "Rui Ma", "Zili Yi", "Yu Liu"], "title": "PairHuman: A High-Fidelity Photographic Dataset for Customized Dual-Person Generation", "comment": "46 pages, 31 figures", "summary": "Personalized dual-person portrait customization has considerable potential applications, such as preserving emotional memories and facilitating wedding photography planning. However, the absence of a benchmark dataset hinders the pursuit of high-quality customization in dual-person portrait generation. In this paper, we propose the PairHuman dataset, which is the first large-scale benchmark dataset specifically designed for generating dual-person portraits that meet high photographic standards. The PairHuman dataset contains more than 100K images that capture a variety of scenes, attire, and dual-person interactions, along with rich metadata, including detailed image descriptions, person localization, human keypoints, and attribute tags. We also introduce DHumanDiff, which is a baseline specifically crafted for dual-person portrait generation that features enhanced facial consistency and simultaneously balances in personalized person generation and semantic-driven scene creation. Finally, the experimental results demonstrate that our dataset and method produce highly customized portraits with superior visual quality that are tailored to human preferences. Our dataset is publicly available at https://github.com/annaoooo/PairHuman.", "AI": {"tldr": "提出了PairHuman数据集和DHumanDiff方法，以提高双人肖像生成的质量和个人化水平，实验表明效果显著。", "motivation": "双人肖像的个性化定制在情感记忆保存和婚纱摄影规划等方面具有广泛的应用潜力，但由于缺乏基准数据集，高质量的双人肖像生成面临着挑战。", "method": "我们提出了PairHuman数据集，这是一个第一个专注于生成高质量双人肖像的大型基准数据集，包含超过100K张图像，涵盖多种场景、服饰和人物互动，以及丰富的元数据。同时，我们引入了DHumanDiff，这是一套用于双人肖像生成的基线模型，专注于增强面部一致性，并在个性化人物生成和语义驱动的场景创造之间取得平衡。", "result": "实验结果表明，我们的数据集和方法能够生成高度定制且视觉质量优越的双人肖像，符合人类的偏好。", "conclusion": "我们公开的PairHuman数据集和DHumanDiff方法证明了在生成符合人类偏好的高质量个性化双人肖像方面的有效性。"}}
{"id": "2511.16689", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16689", "abs": "https://arxiv.org/abs/2511.16689", "authors": ["Samarth Garg", "Deeksha Varshney", "Divya Singh"], "title": "Concept-Based Interpretability for Toxicity Detection", "comment": "16 pages", "summary": "The rise of social networks has not only facilitated communication but also allowed the spread of harmful content. Although significant advances have been made in detecting toxic language in textual data, the exploration of concept-based explanations in toxicity detection remains limited. In this study, we leverage various subtype attributes present in toxicity detection datasets, such as obscene, threat, insult, identity attack, and sexual explicit as concepts that serve as strong indicators to identify whether language is toxic. However, disproportionate attribution of concepts towards the target class often results in classification errors. Our work introduces an interpretability technique based on the Concept Gradient (CG) method which provides a more causal interpretation by measuring how changes in concepts directly affect the output of the model. This is an extension of traditional gradient-based methods in machine learning, which often focus solely on input features. We propose the curation of Targeted Lexicon Set, which captures toxic words that contribute to misclassifications in text classification models. To assess the significance of these lexicon sets in misclassification, we compute Word-Concept Alignment (WCA) scores, which quantify the extent to which these words lead to errors due to over-attribution to toxic concepts. Finally, we introduce a lexicon-free augmentation strategy by generating toxic samples that exclude predefined toxic lexicon sets. This approach allows us to examine whether over-attribution persists when explicit lexical overlap is removed, providing insights into the model's attribution on broader toxic language patterns.", "AI": {"tldr": "研究通过利用各种毒性检测数据集中的亚类型属性作为概念，引入基于概念梯度的解释方法，以及计算WCA分数和提出无词典增强策略来评估误分类。", "motivation": "尽管在检测文本数据中的有毒语言方面取得了显著进展，但在毒性检测中基于概念的解释仍然有限。", "method": "利用基于概念梯度(CG)的方法来提供更为因果的解释，通过量化概念的变化对模型输出的影响。", "result": "计算Word-Concept Alignment (WCA)分数以评估这些词典集在误分类中的重要性，并提出了一种无词典增强策略生成排除预定义词典集的有毒样本。", "conclusion": "这种方法为模型对更广泛的有毒语言模式的归因提供了新的见解。"}}
{"id": "2511.16717", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16717", "abs": "https://arxiv.org/abs/2511.16717", "authors": ["Asya Y. Akkus", "Bradley T. Wolfe", "Pinghan Chu", "Chengkun Huang", "Chris S. Campbell", "Mariana Alvarado Alvarez", "Petr Volegov", "David Fittinghoff", "Robert Reinovsky", "Zhehui Wang"], "title": "A Machine Learning-Driven Solution for Denoising Inertial Confinement Fusion Images", "comment": null, "summary": "Neutron imaging is important in optimizing analysis of inertial confinement fusion (ICF) events such as those at the National Ignition Facility (NIF) and improving current and future ICF platforms. However, images of neutron sources are often degraded by various types of noise. Most commonly, Gaussian and Poisson noise often coexist within one image, obscuring fine details and blurring edges. These noise types often overlap, making them difficult to distinguish and remove using conventional filtering and thresholding methods. As a result, noise removal techniques that preserve image fidelity are important for analyzing and interpreting images of a neutron source. Current solutions include a combination of filtering and thresholding methodologies. In the past, machine learning approaches were rarely implemented due to a lack of ground truth neutron imaging data for ICF processes. However, recent advances in synthetic data production, particularly in the fusion imaging field, have opened opportunities to investigate new denoising procedures using both supervised and unsupervised machine learning methods. In this study, we implement an unsupervised autoencoder with a Cohen-Daubechies- Feauveau (CDF 97) wavelet transform in the latent space for mixed Gaussian-Poisson denoising. The network successfully denoises neutron imaging data. Additionally, it demonstrates lower reconstruction error and superior edge preservation metrics when benchmarked with data generated by a forward model and compared to non-ML-based filtering mechanisms such as Block-matching and 3D filtering (BM3D). This approach presents a promising advancement in neutron image noise reduction and three-dimensional reconstruction analysis of ICF experiments.", "AI": {"tldr": "研究提出了一种无监督自动编码器，用于去除非热中子成像中的混合高斯-泊松噪声。这种方法在实验中表现出优于传统方法的性能。", "motivation": "中子成像在惯性约束聚变（ICF）事件分析中至关重要，但由于噪声的影响，图像质量常受影响。传统的滤波和阈值方法难以有效去除混合噪声，影响了图像的细节数和边缘清晰度。", "method": "提出了一种结合CDF 97小波变换的无监督自动编码器，用于处理中子成像数据中的高斯-泊松混合噪声。", "result": "该方法成功去除了中子成像数据中的噪声，相比非机器学习的滤波方法（如BM3D），该方法显示出更低的重构误差和更优的边缘保持性能。", "conclusion": "该研究方法呈现了在中子图像噪声减少和ICF实验三维重建分析方面的前景。"}}
{"id": "2511.16690", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16690", "abs": "https://arxiv.org/abs/2511.16690", "authors": ["Saleh Almohaimeed", "Saad Almohaimeed", "Mousa Jari", "Khaled A. Alobaid", "Fahad Alotaibi"], "title": "Falsely Accused: How AI Detectors Misjudge Slightly Polished Arabic Articles", "comment": "Submitted to Artificial Intelligence Review Journal", "summary": "Many AI detection models have been developed to counter the presence of articles created by artificial intelligence (AI). However, if a human-authored article is slightly polished by AI, a shift will occur in the borderline decision of these AI detection models, leading them to consider it AI-generated article. This misclassification may result in falsely accusing authors of AI plagiarism and harm the credibility of AI detector models. In English, some efforts were made to meet this challenge, but not in Arabic. In this paper, we generated two datasets. The first dataset contains 800 Arabic articles, half AI-generated and half human-authored. We used it to evaluate 14 Large Language models (LLMs) and commercial AI detectors to assess their ability in distinguishing between human-authored and AI-generated articles. The best 8 models were chosen to act as detectors for our primary concern, which is whether they would consider slightly polished human text as AI-generated. The second dataset, Ar-APT, contains 400 Arabic human-authored articles polished by 10 LLMs using 4 polishing settings, totaling 16400 samples. We use it to evaluate the 8 nominated models and determine whether slight polishing will affect their performance. The results reveal that all AI detectors incorrectly attribute a significant number of articles to AI. The best performing LLM, Claude-4 Sonnet, achieved 83.51%, their performance decreased to 57.63% for articles slightly polished by LLaMA-3. Whereas for the best performing commercial model, originality.AI, that achieves 92% accuracy, dropped to 12% for articles slightly polished by Mistral or Gemma-3.", "AI": {"tldr": "研究展示了AI检测模型在识别被轻微修改的人类文章时存在问题，导致误判文章为AI生成。研究使用了两个阿拉伯语数据集，测试了多种大型语言模型和商业化AI检测工具的检测能力，发现轻微修改会导致检测准确性大幅下降。最佳模型Claude-4 Sonnet在未经修改的文章检测中达到83.51%，而在被LLaMA-3轻微修改的文章中准确性下降到57.63%。商业化模型originality.AI准确性从92%降至12%。", "motivation": "对被人工智能校对过的文本进行准确分类，避免错误地将人类撰写的文本误判为AI生成内容，从而影响AI检测模型的可信度。由于目前在阿拉伯语方面的研究较少，本研究填补了这一空白。", "method": "研究生成了两个数据集。第一个数据集包括800篇阿拉伯文章，一半是AI生成的，一半是人类撰写的。使用该数据集评估了14种大型语言模型和商业AI检测器，以评估其区分AI生成和人类撰写内容的能力。选取表现最好的8种模型用于后续研究。第二个数据集Ar-APT包含400篇被10种大型语言模型通过4种校对设置轻微修改的人类撰写文章，该数据集用于评估稍加修改的文章是否会降低模型的检测性能。", "result": "所有AI检测器在检测稍加修改的文章时会出现较高的误判率。最佳表现的模型Claude-4 Sonnet在未经修改的文章中达到83.51%的准确性，而遇到被LLaMA-3稍加修改的文章时，准确性下降至57.63%。而最优秀的商业模型originality.AI在准确性上从92%下降到了12%。", "conclusion": "文章表明，略微修改的人类撰写文章容易被现有的AI检测器误分类为AI生成，提示当前的AI检测技术在细微修改检测方面存在显著挑战。这提示我们需要改进AI检测模型以提高其在轻微修改文本检测方面的准确性。"}}
{"id": "2511.16719", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16719", "abs": "https://arxiv.org/abs/2511.16719", "authors": ["Nicolas Carion", "Laura Gustafson", "Yuan-Ting Hu", "Shoubhik Debnath", "Ronghang Hu", "Didac Suris", "Chaitanya Ryali", "Kalyan Vasudev Alwala", "Haitham Khedr", "Andrew Huang", "Jie Lei", "Tengyu Ma", "Baishan Guo", "Arpit Kalla", "Markus Marks", "Joseph Greer", "Meng Wang", "Peize Sun", "Roman Rädle", "Triantafyllos Afouras", "Effrosyni Mavroudi", "Katherine Xu", "Tsung-Han Wu", "Yu Zhou", "Liliane Momeni", "Rishi Hazra", "Shuangrui Ding", "Sagar Vaze", "Francois Porcher", "Feng Li", "Siyuan Li", "Aishwarya Kamath", "Ho Kei Cheng", "Piotr Dollár", "Nikhila Ravi", "Kate Saenko", "Pengchuan Zhang", "Christoph Feichtenhofer"], "title": "SAM 3: Segment Anything with Concepts", "comment": null, "summary": "We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., \"yellow school bus\"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.", "AI": {"tldr": "本文介绍了Segment Anything Model (SAM) 3，这是一个统一的模型，能够根据概念提示（名词短语或图像实例）检测、分割和跟踪图像和视频中的对象。SAM 3通过引入一个新的数据引擎，提高了在图像和视频中的概念分割性能，并开放了源代码和新的评估基准。", "motivation": "为了提高基于概念提示的物体检测、分割和跟踪的性能，尤其是在处理图像和视频时。", "method": "SAM 3模型包括三个主要组件：一个基于图像的检测器、一个基于内存的视频追踪器，以及一个用于解耦识别和定位的出现头部。模型还依赖于一个大规模数据引擎来获取高质量的数据集。", "result": "相较现有系统，SAM 3在图像和视频的概念分割任务中提高了两倍的准确率，并且在视觉分割任务上也优于之前的SAM版本。", "conclusion": "通过引入基于概念提示的分割模型SAM 3以及一个新的数据集和评估基准，该工作显著改善了图像和视频中的对象检测、分割和跟踪技术。模型及其数据集已开源，可供研究者使用。"}}
{"id": "2511.16691", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16691", "abs": "https://arxiv.org/abs/2511.16691", "authors": ["Boyang Zhou", "Johan Lindqvist", "Lindsey Li"], "title": "Reproducibility Report: Test-Time Training on Nearest Neighbors for Large Language Models", "comment": null, "summary": "We reproduce the central claims of Test-Time Training on Nearest Neighbors for Large Language Models (Hardt and Sun, 2024), which proposes adapting a language model at inference time by fine-tuning on retrieved nearest-neighbor sequences. Using pretrained RoBERTa embeddings indexed with Faiss, we retrieve 20 neighbors per test input and apply one gradient update per neighbor across GPT-2 (117M, 774M), GPT-Neo (1.3B), and R1-Distilled-Qwen2.5-1.5B. Our experiments confirm that test-time training significantly reduces perplexity and bits-per-byte metrics across diverse domains from The Pile, with the largest improvements in structured or specialized datasets such as GitHub and EuroParl. We further validate that models not pretrained on The Pile benefit more from this adaptation than models already trained on similar data, allowing smaller models to approach the performance of larger ones. Due to infrastructure limitations, we introduce a memory-efficient retrieval implementation that loads only required line offsets rather than entire files, reducing RAM requirements from over 128 GB per server to 32 GB. We also extend the original study by evaluating R1-Distilled-Qwen2.5-1.5B, showing that test-time training yields consistent gains even for modern reasoning-optimized architectures. Overall, our results support the robustness and generality of nearest-neighbor test-time training while highlighting practical considerations for reproducing large-scale retrieval-augmented adaptation.", "AI": {"tldr": "研究证实了测试时在最近邻序列上微调语言模型的有效性，特别是在结构化或专业化数据集上，同时提供了一种节省内存的检索实现方式。", "motivation": "验证Test-Time Training on Nearest Neighbors for Large Language Models（Hardt和Sun，2024）中的中心主张，即在推理时通过在检索到的最近邻序列上进行微调来适应语言模型。", "method": "使用预先训练的RoBERTa嵌入并通过Faiss索引检索20个最近邻序列，对GPT-2（117M, 774M）、GPT-Neo（1.3B）和R1-Distilled-Qwen2.5-1.5B模型进行测试时训练。具体而言，针对每个测试输入，对每个最近邻序列应用一次梯度更新。", "result": "实验证实，测试时训练显著降低了包括GitHub和EuroParl等结构化或专业化数据集上的困惑度和字节/位等指标。此外，对于未在The Pile上预训练的模型，其改进效果更为显著，使较小的模型性能接近较大的模型。", "conclusion": "研究结果支持了最近邻测试时训练的鲁棒性和通用性，同时强调了大规模检索增强适应的实现注意事项。还值得注意的是，由于基础设施限制，引入了一种更节省内存的检索实现方式，将每台服务器的RAM需求从超过128GB减少到32GB。"}}
{"id": "2511.16743", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16743", "abs": "https://arxiv.org/abs/2511.16743", "authors": ["Adeel Yousaf", "Joseph Fioresi", "James Beetham", "Amrit Singh Bedi", "Mubarak Shah"], "title": "SafeR-CLIP: Mitigating NSFW Content in Vision-Language Models While Preserving Pre-Trained Knowledge", "comment": "AAAI 2026 (Main Technical Track)", "summary": "Improving the safety of vision-language models like CLIP via fine-tuning often comes at a steep price, causing significant drops in their generalization performance. We find this trade-off stems from rigid alignment strategies that force unsafe concepts toward single, predefined safe targets, disrupting the model's learned semantic structure. To address this, we propose a proximity-aware approach: redirecting unsafe concepts to their semantically closest safe alternatives to minimize representational change. We introduce SaFeR-CLIP, a fine-tuning framework that applies this principle of minimal intervention. SaFeR-CLIP successfully reconciles safety and performance, recovering up to 8.0% in zero-shot accuracy over prior methods while maintaining robust safety. To support more rigorous evaluation, we also contribute NSFW-Caps, a new benchmark of 1,000 highly-aligned pairs for testing safety under distributional shift. Our work shows that respecting the geometry of pretrained representations is key to achieving safety without sacrificing performance.", "AI": {"tldr": "本文提出了一种新的模型微调框架SaFeR-CLIP，通过将不安全的概念重定向到最近的、安全的替代概念，实现了模型的安全性和性能的平衡。", "motivation": "提升如CLIP这类视觉语言模型安全性的微调往往以模型泛化性能的大幅度下降为代价。研究者发现，这种取舍来源于一种刚性的对齐策略，该策略将不安全的概念强制对齐到单一的、预定义的安全目标上，这破坏了模型学习到的语义结构。", "method": "我们提出了一种接近度感知的方法：将不安全的概念重定向到最近的、安全的替代概念上，以最小化表示变化。同时，我们引入了SaFeR-CLIP这个微调框架，实现了这一最小干预的原则。", "result": "SaFeR-CLIP 有效解决了模型的安全性和性能之间的平衡问题，在零样本准确率上超过了先前方法8.0%，同时保证了模型的安全性。", "conclusion": "我们的工作表明，尊重预训练表示的几何结构是实现安全而不牺牲性能的关键。同时也提供了一个新的用于测试在分布变化下的安全性的基准NSFW-Caps。"}}
{"id": "2511.16693", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16693", "abs": "https://arxiv.org/abs/2511.16693", "authors": ["JaeSeong Kim", "Suan Lee"], "title": "How Language Directions Align with Token Geometry in Multilingual LLMs", "comment": "4 pages", "summary": "Multilingual LLMs demonstrate strong performance across diverse languages, yet there has been limited systematic analysis of how language information is structured within their internal representation space and how it emerges across layers. We conduct a comprehensive probing study on six multilingual LLMs, covering all 268 transformer layers, using linear and nonlinear probes together with a new Token--Language Alignment analysis to quantify the layer-wise dynamics and geometric structure of language encoding. Our results show that language information becomes sharply separated in the first transformer block (+76.4$\\pm$8.2 percentage points from Layer 0 to 1) and remains almost fully linearly separable throughout model depth. We further find that the alignment between language directions and vocabulary embeddings is strongly tied to the language composition of the training data. Notably, Chinese-inclusive models achieve a ZH Match@Peak of 16.43\\%, whereas English-centric models achieve only 3.90\\%, revealing a 4.21$\\times$ structural imprinting effect. These findings indicate that multilingual LLMs distinguish languages not by surface script features but by latent representational structures shaped by the training corpus. Our analysis provides practical insights for data composition strategies and fairness in multilingual representation learning. All code and analysis scripts are publicly available at: https://github.com/thisiskorea/How-Language-Directions-Align-with-Token-Geometry-in-Multilingual-LLMs.", "AI": {"tldr": "研究发现多语言大语言模型在第一层Transformer中语言信息开始分离，并在更深层中保持高度可分离性。模型中语言方向与词汇嵌入的对齐度揭示了数据构成对模型语言表示的影响。", "motivation": "研究旨在系统分析多语言大语言模型中语言信息在其内部表示空间中的结构及其在各层中的演变过程。", "method": "研究通过使用线性和非线性探针以及一种新的Token--Language Alignment分析方法，对六个多语言大语言模型的全部268个Transformer层进行了详尽的探针研究。", "result": "研究结果显示语言信息在第一层Transformer中变得明显分离，并在整个模型深度中几乎完全线性可分。语言方向与词汇嵌入的对齐度也与训练数据中的语言组成密切相关。", "conclusion": "研究成果表明，多语言大语言模型通过训练语料库中形成的潜在表示结构，而非表面上的文字特征来区分语言。此分析提供了对于多语言表示学习中的数据构成策略和公平性的实际见解。"}}
