<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 8]
- [cs.CV](#cs.CV) [Total: 12]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [From Intuition to Expertise: Rubric-Based Cognitive Calibration for Human Detection of LLM-Generated Korean Text](https://arxiv.org/abs/2601.19913)
*Shinwoo Park,Yo-Sub Han*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Distinguishing human-written Korean text from fluent LLM outputs remains difficult even for linguistically trained readers, who can over-trust surface well-formedness. We study whether expert detection can be treated as a learnable skill and improved through structured calibration. We introduce LREAD, a rubric derived from national Korean writing standards and adapted to target micro-level artifacts (e.g., punctuation optionality, spacing behavior, and register shifts). In a three-phase longitudinal blind protocol with Korean linguistics majors, Phase 1 measures intuition-only detection, Phase 2 enforces criterion-level scoring with explicit justifications, and Phase 3 evaluates domain-focused mastery on held-out elementary essays. Across phases, majority-vote accuracy increases from 60% to 100%, accompanied by stronger inter-annotator agreement (Fleiss' kappa: -0.09 --> 0.82). Compared to state-of-the-art LLM detectors, calibrated humans rely more on language-specific micro-diagnostics that are not well captured by coarse discourse priors. Our findings suggest that rubric-scaffolded expert judgment can serve as an interpretable complement to automated detectors for non-English settings, and we release the full rubric and a taxonomy of calibrated detection signatures.

</details>


### [2] [Simulating Complex Multi-Turn Tool Calling Interactions in Stateless Execution Environments](https://arxiv.org/abs/2601.19914)
*Maxwell Crouse,Ibrahim Abdelaziz,Kshitij Fadnis,Siva Sankalp Patel,Kinjal Basu,Chulaka Gunasekara,Sadhana Kumaravel,Asim Munawar,Pavan Kapanipathi*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Synthetic data has proven itself to be a valuable resource for tuning smaller, cost-effective language models to handle the complexities of multi-turn tool calling conversations. While many frameworks and systems for producing synthetic multi-turn tool calling data have been proposed, prior works have frequently assumed that any tool calling interactions will take place in an execution environment that maintains state. When such an environment is available, this is advantageous as it allows for the validity of an interaction to be determined by whether or not the state of the execution environment matches to some prespecified objective. Unfortunately, this does not hold in many real-world tool use settings, e.g., in enterprise settings where data security is of the utmost importance or in cases where tool specifications are synthesized from multiple sources. In this work, we address this gap by introducing a data generation method, DiGiT-TC, that is designed to produce tool calling conversations that have the characteristics of conversations generated through search in a stateful environment. The key to our technique lies in a novel generation pattern that allows our approach to implicitly represent certain tool calls in the user request. We validate our approach on standard tool calling benchmarks and demonstrate that, even in stateful problem settings, our approach results in strong performance gains.

</details>


### [3] [Modeling Next-Token Prediction as Left-Nested Intuitionistic Implication](https://arxiv.org/abs/2601.19915)
*Paul Tarau*

Main category: cs.CL

> 本研究提出了一种基于直觉逻辑的神经架构，通过左嵌套蕴涵链表示序列，用证明论的方法建立模型，并展示了该模型与RNN和Transformers的关系。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在通过基于逻辑的方法来设计神经架构，分析和验证在非交换与交换序列、单标记与多标记预测选择之间的关系。通过证明论解释下个标记预测，本研究为神经架构的设计提供了新的视角。

**Method:** 介绍了一种名为\emph{Arrow Language Model}的神经架构，该架构来源于对下一个标记预测的直觉逻辑解释。与使用相加嵌入并通过注意力混合词元的表示方法不同，本架构将前缀编码成一个保留顺序的\emph{左嵌套蕴涵链}，顺序通过非交换组成保持。下一个标记预测对应于\emph{假言推理}，而序列处理则成为构造性证明的扩展，对应于柯里—霍华德的对应关系。

**Result:** 使用基于Prolog的专用定理证明器验证了该神经模型的基本属性，研究探讨了模型在非交换与交换序列、单标记与多标记预测上的差异。证明了在乘法RNN等效神经架构下的实用性。

**Conclusion:** 该研究说明了一个与乘法RNN等效的神经架构如何自然地从一个证明论解释下个标记预测为嵌套直觉蕴涵中出现，介绍了实用的低秩神经实现方式，并将其模型设置于Transformers和状态空间模型中。

**Abstract:** We introduce the \emph{Arrow Language Model}, a neural architecture derived from an intuitionistic-logic interpretation of next-token prediction. Instead of representing tokens as additive embeddings mixed by attention, we encode a prefix as a \emph{left-nested implication chain} whose structure preserves order through non-commutative composition. Next-token prediction corresponds to \emph{modus ponens}, and sequence processing becomes constructive proof extension under the Curry--Howard correspondence. Our Prolog-based specialized theorem provers validate fundamental properties of the neural models, among which relations between commutative vs. non-commutative sequencing and single-token vs. multi-token prediction choices. We show that a neural architecture equivalent to multiplicative RNNs arises naturally from a proof-theoretic interpretation of next-token prediction as nested intuitionistic implication, we present a practical low-rank neural realization and position the model relative to Transformers and state-space models.
  Keywords: logic-based derivation of neural architectures, intuitionistic implicational logic, token-as-operator neural models, state-space models, alternatives to transformer-based foundational models.

</details>


### [4] [PaperAudit-Bench: Benchmarking Error Detection in Research Papers for Critical Automated Peer Review](https://arxiv.org/abs/2601.19916)
*Songjun Tu,Yiwen Ma,Jiahao Lin,Qichao Zhang,Xiangyuan Lan,Junfeng. Li,Nan Xu,Linjing Li,Dongbin Zhao*

Main category: cs.CL

> 提出了PaperAudit-Bench，包括用于长文本分析的PaperAudit-Dataset和自动化审查框架PaperAudit-Review。实验显示，加入显式的错误检测可以系统地提高审查的严格性和辨别力，同时支持通过微调和强化学习训练轻量级LLM检错器。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型虽然可以生成流利的同行评审，但在识别论文中细微且分布广泛的实质性问题时，往往缺乏足够的批判性严谨性。

**Method:** 研究包含两个组成部分：(1) PaperAudit-Dataset，涵盖单个段落和需要跨段落推理才能发现问题的数据集，用于长文本背景下受控评估；(2) PaperAudit-Review，集成结构化错误检测与基于证据的审查生成的自动化审查框架。

**Result:** 实验揭示了在长文本背景下，不同模型检测错误的可变性极高。加入显式错误检测的自动化流程能系统地提升审查的严格性和辨别性。

**Conclusion:** PaperAudit-Bench不仅能够支持更加严格和具有辨别力的同行评审，还可以通过微调和强化学习训练轻量级的LLM错误检测器，以减少计算成本，提高检错效率。

**Abstract:** Large language models can generate fluent peer reviews, yet their assessments often lack sufficient critical rigor when substantive issues are subtle and distributed across a paper. In this paper, we introduce PaperAudit-Bench, which consists of two components: (1) PaperAudit-Dataset, an error dataset covering both errors identifiable within individual sections and those requiring cross-section reasoning, designed for controlled evaluation under long-context settings; and (2) PaperAudit-Review, an automated review framework that integrates structured error detection with evidence-aware review generation to support critical assessment. Experiments on PaperAudit-Bench reveal large variability in error detectability across models and detection depths, highlighting the difficulty of identifying such errors under long-context settings. Relative to representative automated reviewing baselines, incorporating explicit error detection into the review workflow produces systematically stricter and more discriminative evaluations, demonstrating its suitability for peer review. Finally, we show that the dataset supports training lightweight LLM detectors via SFT and RL, enabling effective error detection at reduced computational cost.

</details>


### [5] [PILOT: Planning via Internalized Latent Optimization Trajectories for Large Language Models](https://arxiv.org/abs/2601.19917)
*Haoyu Zheng,Yun Zhu,Yuqian Yuan,Bo Yuan,Wenqiao Zhang,Siliang Tang,Jun Xiao*

Main category: cs.CL

> PILOT is a method to enhance reasoning in compact LLMs by internalizing strategic guidance from a teacher model into the model through a lightweight Hyper-Network.

<details>
  <summary>Details</summary>

**Motivation:** Compact LLMs tend to have limited capacity for global strategic planning, leading to errors in complex tasks; this work aims to improve their reasoning ability.

**Method:** PILOT (Planning via Internalized Latent Optimization Trajectories), a framework that uses a lightweight Hyper-Network to synthesize a query-conditioned Latent Guidance vector to guide the model's reasoning.

**Result:** Experiments show PILOT stabilizes reasoning, outperforming baselines (e.g., +8.9% on MATH500) with no significant increase in inference time.

**Conclusion:** PILOT effectively enhances LLMs' strategic reasoning capabilities without altering core model architecture, achieving better performance with minimal computational overhead.

**Abstract:** Strategic planning is critical for multi-step reasoning, yet compact Large Language Models (LLMs) often lack the capacity to formulate global strategies, leading to error propagation in long-horizon tasks. Our analysis reveals that LLMs possess latent reasoning capabilities that can be unlocked when conditioned on explicit plans from a teacher model; however, runtime reliance on external guidance is often impractical due to latency and availability constraints. To bridge this gap, we propose PILOT (Planning via Internalized Latent Optimization Trajectories), a non-invasive framework designed to internalize the strategic oversight of large models into intrinsic Latent Guidance. Instead of altering backbone weights, PILOT employs a lightweight Hyper-Network to synthesize a query-conditioned Latent Guidance vector. This vector acts as an internal steering mechanism, guiding the model's representations toward optimal reasoning paths. Extensive experiments on mathematical and coding benchmarks demonstrate that PILOT effectively stabilizes reasoning trajectories, consistently outperforming strong baselines (e.g., +8.9% on MATH500) with negligible inference latency.

</details>


### [6] [Lowest Span Confidence: A Zero-Shot Metric for Efficient and Black-Box Hallucination Detection in LLMs](https://arxiv.org/abs/2601.19918)
*Yitong Qiao,Licheng Pan,Yu Mi,Lei Liu,Yue Shen,Fei Sun,Zhixuan Chu*

Main category: cs.CL

> 研究提出了用于检测LLMs幻觉的LSC度量，该方法在资源紧张条件下表现出色，超越现有零样本基线方法。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLMs）在高风险环境中可靠部署面临生成事实错误内容的问题。现有幻觉检测方法通常操作在不现实的假设上，要么需要复杂的采样策略，要么需要模型内部状态，这些在API场景中难以实现。

**Method:** 我们提出了一种名为最低跨度置信度（LSC）的新颖零样本度量方法，用于在最低资源假设下检测幻觉。LSC通过滑动窗口机制评估语义连贯跨度的联合概率，并通过识别不同长度n-gram的最低边缘置信度区域来捕获与事实不一致强相关的本地不确定性模式。

**Result:** 实验表明，LSC在资源受限条件下仍能提供出色的检测性能，优于现有零样本基线方法。

**Conclusion:** LSC可以显著减少困惑度的稀释效应和最小令牌概率的噪声敏感性，提供更强大的事实不确定性估计，在各种基准测试中优于同类零样本方法。

**Abstract:** Hallucinations in Large Language Models (LLMs), i.e., the tendency to generate plausible but non-factual content, pose a significant challenge for their reliable deployment in high-stakes environments. However, existing hallucination detection methods generally operate under unrealistic assumptions, i.e., either requiring expensive intensive sampling strategies for consistency checks or white-box LLM states, which are unavailable or inefficient in common API-based scenarios. To this end, we propose a novel efficient zero-shot metric called Lowest Span Confidence (LSC) for hallucination detection under minimal resource assumptions, only requiring a single forward with output probabilities. Concretely, LSC evaluates the joint likelihood of semantically coherent spans via a sliding window mechanism. By identifying regions of lowest marginal confidence across variable-length n-grams, LSC could well capture local uncertainty patterns strongly correlated with factual inconsistency. Importantly, LSC can mitigate the dilution effect of perplexity and the noise sensitivity of minimum token probability, offering a more robust estimate of factual uncertainty. Extensive experiments across multiple state-of-the-art (SOTA) LLMs and diverse benchmarks show that LSC consistently outperforms existing zero-shot baselines, delivering strong detection performance even under resource-constrained conditions.

</details>


### [7] [FastWhisper: Adaptive Self-knowledge Distillation for Real-time Automatic Speech Recognition](https://arxiv.org/abs/2601.19919)
*Junseok Lee,Nahoon Kim,Sangyong Lee,Chang-Jae Chun*

Main category: cs.CL

> 提出了一种自适应自我知识蒸馏(ASKD)方法来减少学生模型对教师模型的依赖并提高其泛化能力，将Whisper模型蒸馏成一个更小的变体FastWhisper，在后训练设置中，FastWhisper的字错误率比教师模型低1.07%，推理速度快5倍。

<details>
  <summary>Details</summary>

**Motivation:** 知识蒸馏是最有效的模型压缩方法之一。然而，学生模型在训练过程中可能会继承教师模型的缺点，从而导致泛化能力下降。为了解决这个问题，

**Method:** 提出了自适应自我知识蒸馏(ASKD)方法，该方法动态降低对教师模型的依赖，提升自训练能力，并通过自我知识蒸馏提高学生模型的泛化能力。

**Result:** 在后训练设置中，FastWhisper的字错误率比Whisper模型低1.07%，而其相对推理时间快5倍。

**Conclusion:** 该研究通过自适应自我知识蒸馏增强了学生模型的泛化能力，同时提升了其推理速度。

**Abstract:** Knowledge distillation is one of the most effective methods for model compression. Previous studies have focused on the student model effectively training the predictive distribution of the teacher model. However, during training, the student model may inherit the shortcomings of the teacher model, which can lead to a decline in generalization capacity. To mitigate this issue, we propose adaptive self-knowledge distillation (ASKD), which dynamically reduces the dependence of the teacher model to improve the self-training capacity, and performs the self-knowledge distillation method to improve the generalization capacity of the student model. We further distill the Whisper model into a smaller variant, called FastWhisper. In our post-training setting, FastWhisper achieved a word error rate of 1.07% lower than the teacher model Whisper, and its relative inference time was 5 times faster.

</details>


### [8] [Demystifying Multi-Agent Debate: The Role of Confidence and Diversity](https://arxiv.org/abs/2601.19921)
*Xiaochen Zhu,Caiqi Zhang,Yizhou Chi,Tom Stafford,Nigel Collier,Andreas Vlachos*

Main category: cs.CL

> 通过在多智能体辩论增加多样性的初始假设和代理之间的置信度交流，改进了传统MAD的有效性，达到了更好的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的MAD在同质的代理和均匀的置信度更新情况下，无法改善结果，并且经常在计算成本更高的同时性能不如简单的多数投票。因此，作者通过借鉴人类决策制定的研究，提出改进措施。

**Method:** 通过引入两种轻量级干预措施来改进多智能体辩论（MAD）：1) 多样性意识的初始化，以选择更多样化的候选答案，增加正确假设在辩论开始时出现的可能性。2) 置信度调节的辩论协议，其中代理表达校准过的置信度并根据其他代理的置信度更新自己的观点。

**Result:** 该研究通过理论和实验表明，多样性意识初始化提高了MAD成功的先验概率，而置信度调节更新使得辩论能够系统性地偏向正确假设。实验结果在六个基于推理的问题回答基准上显示出优于传统MAD和多数投票的结果。

**Conclusion:** 研究结果表明，基于人类决策过程的简单且基于原则的修改可以显著增强MAD的效果，在计算成本增加不多的情况下提供了优于传统MAD和简单多数投票的性能。

**Abstract:** Multi-agent debate (MAD) is widely used to improve large language model (LLM) performance through test-time scaling, yet recent work shows that vanilla MAD often underperforms simple majority vote despite higher computational cost. Studies show that, under homogeneous agents and uniform belief updates, debate preserves expected correctness and therefore cannot reliably improve outcomes. Drawing on findings from human deliberation and collective decision-making, we identify two key mechanisms missing from vanilla MAD: (i) diversity of initial viewpoints and (ii) explicit, calibrated confidence communication. We propose two lightweight interventions. First, a diversity-aware initialisation that selects a more diverse pool of candidate answers, increasing the likelihood that a correct hypothesis is present at the start of debate. Second, a confidence-modulated debate protocol in which agents express calibrated confidence and condition their updates on others' confidence. We show theoretically that diversity-aware initialisation improves the prior probability of MAD success without changing the underlying update dynamics, while confidence-modulated updates enable debate to systematically drift to the correct hypothesis. Empirically, across six reasoning-oriented QA benchmarks, our methods consistently outperform vanilla MAD and majority vote. Our results connect human deliberation with LLM-based debate and demonstrate that simple, principled modifications can substantially enhance debate effectiveness.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [9] [Size Matters: Reconstructing Real-Scale 3D Models from Monocular Images for Food Portion Estimation](https://arxiv.org/abs/2601.20051)
*Gautham Vinod,Bruce Coburn,Siddeshwar Raghavan,Jiangpeng He,Fengqing Zhu*

Main category: cs.CV

> A method to accurately estimate the real-world scale of food objects from monocular images using 3D reconstruction and AI models, significantly reducing volume-estimation errors for precision nutrition applications.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind the paper is to accurately estimate the portion size of food from monocular images for better assessment of food intake, which is essential for monitoring chronic diseases like obesity and diabetes.

**Method:** The paper proposes a method to recover a true-to-scale 3D reconstructed object from a monocular image by using rich visual features extracted from models trained on large-scale datasets.

**Result:** The method was tested on two publicly available datasets, and it showed a nearly 30% reduction in mean absolute volume-estimation error compared to existing techniques.

**Conclusion:** The proposed method can enhance the field of precision nutrition by providing true-to-life, physically meaningful models of food portions for accurate dietary assessment.

**Abstract:** The rise of chronic diseases related to diet, such as obesity and diabetes, emphasizes the need for accurate monitoring of food intake. While AI-driven dietary assessment has made strides in recent years, the ill-posed nature of recovering size (portion) information from monocular images for accurate estimation of ``how much did you eat?'' is a pressing challenge. Some 3D reconstruction methods have achieved impressive geometric reconstruction but fail to recover the crucial real-world scale of the reconstructed object, limiting its usage in precision nutrition. In this paper, we bridge the gap between 3D computer vision and digital health by proposing a method that recovers a true-to-scale 3D reconstructed object from a monocular image. Our approach leverages rich visual features extracted from models trained on large-scale datasets to estimate the scale of the reconstructed object. This learned scale enables us to convert single-view 3D reconstructions into true-to-life, physically meaningful models. Extensive experiments and ablation studies on two publicly available datasets show that our method consistently outperforms existing techniques, achieving nearly a 30% reduction in mean absolute volume-estimation error, showcasing its potential to enhance the domain of precision nutrition. Code: https://gitlab.com/viper-purdue/size-matters

</details>


### [10] [DiSa: Saliency-Aware Foreground-Background Disentangled Framework for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2601.20064)
*Zhen Yao,Xin Li,Taotao Jing,Shuai Zhang,Mooi Choo Chuah*

Main category: cs.CV

> 提出DiSa框架解决视觉语言模型在开放词汇语义分割任务中的前景偏移和空间定位模糊问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有的视觉语言模型在处理语义分割任务时存在前景偏移和空间定位模糊的问题，为了解决这些问题，引入了DiSa框架。

**Method:** DiSa, 一种新颖的注意力驱动的前景背景分离框架，通过设计的注意力感知解耦模块（SDM）和分层细化模块（HRM）来解决现有模型在开放词汇语义分割中的偏移和空间定位不足问题。

**Result:** 在六个基准数据集上的大量实验表明，DiSa在语义分割任务中优于现有的最先进方法。

**Conclusion:** 基于设计的注意力感知解耦模块和分层细化模块，DiSa在开放词汇语义分割中表现出色。

**Abstract:** Open-vocabulary semantic segmentation aims to assign labels to every pixel in an image based on text labels. Existing approaches typically utilize vision-language models (VLMs), such as CLIP, for dense prediction. However, VLMs, pre-trained on image-text pairs, are biased toward salient, object-centric regions and exhibit two critical limitations when adapted to segmentation: (i) Foreground Bias, which tends to ignore background regions, and (ii) Limited Spatial Localization, resulting in blurred object boundaries. To address these limitations, we introduce DiSa, a novel saliency-aware foreground-background disentangled framework. By explicitly incorporating saliency cues in our designed Saliency-aware Disentanglement Module (SDM), DiSa separately models foreground and background ensemble features in a divide-and-conquer manner. Additionally, we propose a Hierarchical Refinement Module (HRM) that leverages pixel-wise spatial contexts and enables channel-wise feature refinement through multi-level updates. Extensive experiments on six benchmarks demonstrate that DiSa consistently outperforms state-of-the-art methods.

</details>


### [11] [Semi-Supervised Masked Autoencoders: Unlocking Vision Transformer Potential with Limited Data](https://arxiv.org/abs/2601.20072)
*Atik Faysal,Mohammad Rostami,Reihaneh Gh. Roshan,Nikhil Muralidhar,Huaxia Wang*

Main category: cs.CV

> SSMAE通过引入验证驱动的门控机制，实现了在标记数据稀缺的情况下显著提升Vision Transformers (ViTs)的性能。

<details>
  <summary>Details</summary>

**Motivation:** 解决在标签数据稀缺而未标记数据丰富的情况下训练视觉变压器时遇到的挑战。

**Method:** 提出了一个框架，称为半监督掩码自动编码器（SSMAE），该框架利用未标记和标记样本在遮蔽图像重建和分类上进行联合优化，并且使用了动态选择的伪标签。引入了一种验证驱动的门控机制，以减少确认偏误。

**Result:** 提出的Semi-Supervised Masked Autoencoder (SSMAE)框架，通过验证驱动的门控机制，仅在模型达到可靠的高置信度预测时激活伪标签，从而在低标签数据量下显著提升了Vision Transformers (ViTs)的效果，特别是在CIFAR-10和CIFAR-100数据集上，与监督式ViT和微调后的MAE相比，性能有明显提高，特别是在标签数量很少的情况下。

**Conclusion:** 研究结果表明，何时引入伪标签与如何生成伪标签同样重要，特别是在高效的数据驱动变压器训练中。

**Abstract:** We address the challenge of training Vision Transformers (ViTs) when labeled data is scarce but unlabeled data is abundant. We propose Semi-Supervised Masked Autoencoder (SSMAE), a framework that jointly optimizes masked image reconstruction and classification using both unlabeled and labeled samples with dynamically selected pseudo-labels. SSMAE introduces a validation-driven gating mechanism that activates pseudo-labeling only after the model achieves reliable, high-confidence predictions that are consistent across both weakly and strongly augmented views of the same image, reducing confirmation bias. On CIFAR-10 and CIFAR-100, SSMAE consistently outperforms supervised ViT and fine-tuned MAE, with the largest gains in low-label regimes (+9.24% over ViT on CIFAR-10 with 10% labels). Our results demonstrate that when pseudo-labels are introduced is as important as how they are generated for data-efficient transformer training. Codes are available at https://github.com/atik666/ssmae.

</details>


### [12] [Sparse CLIP: Co-Optimizing Interpretability and Performance in Contrastive Learning](https://arxiv.org/abs/2601.20075)
*Chuan Qin,Constantin Venhoff,Sonia Joseph,Fanyi Xiao,Stefan Scherer*

Main category: cs.CV

> 本文提出了一种将稀疏性直接整合到CLIP训练中的方法，挑战了可解释性和性能之间不可兼得的传统观念，证明了在保持高性能的同时可以实现可解释性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管CLIP在视觉-语言表示学习方面取得了成功，但其密集且不透明的潜在表示带来了显著的可解释性挑战。现有方法常因采用后处理手段而导致性能下降或多模态能力丧失。为了挑战“可解释性与性能相冲突”的传统观念，我们提出了一种新的方法。

**Method:** 我们提出了一种将稀疏性直接整合到CLIP训练中的简单且有效的方法，从而生成既可解释又具有高性能的表现形式。与SAEs相比，我们的稀疏CLIP表示保持了下游任务的高性能，实现了更优的可解释性，并保留了多模态能力。

**Result:** 我们展示了多模态稀疏特征如何使语义概念对齐变得简单，并揭示了跨模态知识如何随训练动态产生。此外，作为一个概念验证，我们在稀疏CLIP表示上训练了一个视觉-语言模型，展示了解释性视觉引导能力。

**Conclusion:** 本研究成功地挑战了“解释性必须牺牲精度”的传统看法，并展示了解释性和性能可以共同优化，这为将来的模型设计提供了一个有前景的原则。

**Abstract:** Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in vision-language representation learning, powering diverse downstream tasks and serving as the default vision backbone in multimodal large language models (MLLMs). Despite its success, CLIP's dense and opaque latent representations pose significant interpretability challenges. A common assumption is that interpretability and performance are in tension: enforcing sparsity during training degrades accuracy, motivating recent post-hoc approaches such as Sparse Autoencoders (SAEs). However, these post-hoc approaches often suffer from degraded downstream performance and loss of CLIP's inherent multimodal capabilities, with most learned features remaining unimodal.
  We propose a simple yet effective approach that integrates sparsity directly into CLIP training, yielding representations that are both interpretable and performant. Compared to SAEs, our Sparse CLIP representations preserve strong downstream task performance, achieve superior interpretability, and retain multimodal capabilities. We show that multimodal sparse features enable straightforward semantic concept alignment and reveal training dynamics of how cross-modal knowledge emerges. Finally, as a proof of concept, we train a vision-language model on sparse CLIP representations that enables interpretable, vision-based steering capabilities. Our findings challenge conventional wisdom that interpretability requires sacrificing accuracy and demonstrate that interpretability and performance can be co-optimized, offering a promising design principle for future models.

</details>


### [13] [NucFuseRank: Dataset Fusion and Performance Ranking for Nuclei Instance Segmentation](https://arxiv.org/abs/2601.20104)
*Nima Torbati,Anastasia Meshcheryakova,Ramona Woitek,Sepideh Hatamikia,Diana Mechtcheriakova,Amirreza Mahbod*

Main category: cs.CV

> 研究评估了用于H&E染色图像中核实例分割的手动注释数据集，并提出一套新的基准数据集用于模型训练、测试和评估。

<details>
  <summary>Details</summary>

**Motivation:** 现有的研究大多集中在开发新的分割算法并仅在有限数量的公共数据集上进行基准测试，而本研究则将重点放在数据集的评估之上，目的是为H&E染色组织图像中的核实例分割提供新的基准。

**Method:** 通过对大量文献的回顾，确定了用于核实例分割的手动注释的公开数据集，并将它们统一为单一输入和注释格式。使用两种最先进的分割模型（一种基于CNN，另一种基于CNN与ViT的混合架构）对这些数据集进行了系统的评估和排名，并提出了一个统一的测试集（NucFuse-test）和训练集（NucFuse-train）。

**Result:** 研究系统地评估并对用于核实例分割的数据集进行了排名，生成了融合数据集，并进行了外部验证，提供了一个新的基准用于训练、测试和评估H&E染色组织图像中的核实例分割模型。

**Conclusion:** 该研究通过评估、排名数据集并生成统一的训练集和测试集，为H&E染色组织图像中的核实例分割模型提供了一个新的基准。同时，这也是未来更多相关研究的基础。

**Abstract:** Nuclei instance segmentation in hematoxylin and eosin (H&E)-stained images plays an important role in automated histological image analysis, with various applications in downstream tasks. While several machine learning and deep learning approaches have been proposed for nuclei instance segmentation, most research in this field focuses on developing new segmentation algorithms and benchmarking them on a limited number of arbitrarily selected public datasets.
  In this work, rather than focusing on model development, we focused on the datasets used for this task. Based on an extensive literature review, we identified manually annotated, publicly available datasets of H&E-stained images for nuclei instance segmentation and standardized them into a unified input and annotation format. Using two state-of-the-art segmentation models, one based on convolutional neural networks (CNNs) and one based on a hybrid CNN and vision transformer architecture, we systematically evaluated and ranked these datasets based on their nuclei instance segmentation performance. Furthermore, we proposed a unified test set (NucFuse-test) for fair cross-dataset evaluation and a unified training set (NucFuse-train) for improved segmentation performance by merging images from multiple datasets.
  By evaluating and ranking the datasets, performing comprehensive analyses, generating fused datasets, conducting external validation, and making our implementation publicly available, we provided a new benchmark for training, testing, and evaluating nuclei instance segmentation models on H&E-stained histological images.

</details>


### [14] [Look in the Middle: Structural Anchor Pruning for Scalable Visual RAG Indexing](https://arxiv.org/abs/2601.20107)
*Zhuchenyang Liu,Ziyu Hu,Yao Zhang,Yu Xiao*

Main category: cs.CV

> A new training-free pruning method, SAP, is proposed to compress index vectors for Vision-Language Models with high efficiency while maintaining retrieval fidelity. The study advances understanding of pruning techniques in Visual Document Retrieval, revealing the importance of middle layer patches over final layer ones.

<details>
  <summary>Details</summary>

**Motivation:** The motivation stems from the large index vector size overheads of recent Vision-Language Models used for fine-grained Visual Document Retrieval (VDR). Existing pruning solutions fall short in high-compression scenarios, questioning the feasibility of training-free methods. This research aims to address this challenge and test the hypothesis regarding visual token importance being query-dependent.

**Method:** Structural Anchor Pruning (SAP) is introduced as a training-free pruning method that targets middle layers to identify key visual patches for high performance compression. The Oracle Score Retention (OSR) protocol is also introduced to evaluate the impact of layer-wise information on compression efficiency.

**Result:** Evaluations on the ViDoRe benchmark show that SAP can reduce index vectors by over 90% while maintaining retrieval fidelity. The OSR-based analysis demonstrates the persistence of semantic structural anchor patches in middle layers, which is not the case for traditional pruning methods.

**Conclusion:** SAP provides a highly scalable solution for Visual RAG by efficiently compressing index vectors without compromising retrieval performance. The results support the idea that semantic structural anchor patches in middle layers are crucial for effective pruning, aligning with the notion that traditional approaches focusing on the final layer are less effective.

**Abstract:** Recent Vision-Language Models (e.g., ColPali) enable fine-grained Visual Document Retrieval (VDR) but incur prohibitive index vector size overheads. Training-free pruning solutions (e.g., EOS-attention based methods) can reduce index vector size by approximately 60% without model adaptation, but often underperform random selection in high-compression scenarios (> 80%). Prior research (e.g., Light-ColPali) attributes this to the conclusion that visual token importance is inherently query-dependent, thereby questioning the feasibility of training-free pruning. In this work, we propose Structural Anchor Pruning (SAP), a training-free pruning method that identifies key visual patches from middle layers to achieve high performance compression. We also introduce Oracle Score Retention (OSR) protocol to evaluate how layer-wise information affects compression efficiency. Evaluations on the ViDoRe benchmark demonstrate that SAP reduces index vectors by over 90% while maintaining robust retrieval fidelity, providing a highly scalable solution for Visual RAG. Furthermore, our OSR-based analysis reveals that semantic structural anchor patches persist in the middle layers, unlike traditional pruning solutions that focus on the final layer where structural signals dissipate.

</details>


### [15] [Efficient Token Pruning for LLaDA-V](https://arxiv.org/abs/2601.20168)
*Zhewen Wan,Tianchen Song,Chen Lin,Zhiyong Zhao,Xianpeng Lang*

Main category: cs.CV

> 提出了逐步标记修剪策略，以减少基于扩散的大规模多模态模型LLaDA-V的计算开销，同时保持性能。这项工作是首次在扩散模型中研究结构化标记修剪的应用。

<details>
  <summary>Details</summary>

**Motivation:** 通过深入的注意力分析发现，LLaDA-V在中间到较晚层聚集跨模式信息，导致语义对齐延迟。因此，为了减少计算开销，提出了本修剪策略。

**Method:** 提出了一种受FastV启发的结构化标记修剪策略，该策略在指定层选择性地移除一部分视觉标记，以减少浮点运算次数同时保持关键语义信息。与FastV仅关注浅层修剪不同，本方法针对第一次去噪步骤的中间到较晚层，以维持输出质量，而第一步修剪策略可以减少所有后续步骤的计算。

**Result:** 实验结果显示，最佳配置减少计算成本最高可达65%，同时保持平均95%的任务性能。

**Conclusion:** 所提出的框架为高效LLaDA-V推理提供了实验证据，并突出了扩散型多模态模型中视觉感知修剪的潜力。

**Abstract:** Diffusion-based large multimodal models, such as LLaDA-V, have demonstrated impressive capabilities in vision-language understanding and generation. However, their bidirectional attention mechanism and diffusion-style iterative denoising paradigm introduce significant computational overhead, as visual tokens are repeatedly processed across all layers and denoising steps. In this work, we conduct an in-depth attention analysis and reveal that, unlike autoregressive decoders, LLaDA-V aggregates cross-modal information predominantly in middle-to-late layers, leading to delayed semantic alignment. Motivated by this observation, we propose a structured token pruning strategy inspired by FastV, selectively removing a proportion of visual tokens at designated layers to reduce FLOPs while preserving critical semantic information. To the best of our knowledge, this is the first work to investigate structured token pruning in diffusion-based large multimodal models. Unlike FastV, which focuses on shallow-layer pruning, our method targets the middle-to-late layers of the first denoising step to align with LLaDA-V's delayed attention aggregation to maintain output quality, and the first-step pruning strategy reduces the computation across all subsequent steps. Our framework provides an empirical basis for efficient LLaDA-V inference and highlights the potential of vision-aware pruning in diffusion-based multimodal models. Across multiple benchmarks, our best configuration reduces computational cost by up to 65% while preserving an average of 95% task performance.

</details>


### [16] [TeleStyle: Content-Preserving Style Transfer in Images and Videos](https://arxiv.org/abs/2601.20175)
*Shiwen Zhang,Xiaoyan Yang,Bojia Zi,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

> TeleStyle 是一个轻量但有效的模型，用于图像和视频风格化，基于Qwen-Image-Edit构建，通过Curriculum Continual Learning框架在精心策划和合成的高质量数据集上训练，实现对未见过风格的通用性和内容忠实性。此外，还引入了视频到视频的风格化模块以提高时间一致性。该模型在风格相似性、内容一致性、美学质量三个核心评估指标上达到最先进的性能。代码和预训练模型在GitHub上提供。

<details>
  <summary>Details</summary>

**Motivation:** Diffusion Transformers (DiTs) 在生成风格化输出时遇到了内容和风格特征纠缠的问题，难以解决内容保留和风格转移的挑战。

**Method:** TeleStyle 模型利用了Qwen-Image-Edit的稳健功能，在经过精选和合成的高质量数据集上通过Curriculum Continual Learning框架训练，以提高模型的风格泛化能力和内容忠实性。同时引入了视频到视频的风格化模块提高时间一致性。

**Result:** TeleStyle 达到了在风格相似性、内容一致性、美学质量三个评估指标上的最佳表现。

**Conclusion:** TeleStyle 是一个用于图像和视频风格化的高性能模型，通过Curriculum Continual Learning框架克服了Diffusion Transformers在内容和风格特征表示上的纠缠问题，同时维护了内容的精确性。此外，视频到视频的风格化模块确保了时间上的一致性和优质的视觉效果。代码和预训练模型公开，有利于进一步的研究和应用。

**Abstract:** Content-preserving style transfer, generating stylized outputs based on content and style references, remains a significant challenge for Diffusion Transformers (DiTs) due to the inherent entanglement of content and style features in their internal representations. In this technical report, we present TeleStyle, a lightweight yet effective model for both image and video stylization. Built upon Qwen-Image-Edit, TeleStyle leverages the base model's robust capabilities in content preservation and style customization. To facilitate effective training, we curated a high-quality dataset of distinct specific styles and further synthesized triplets using thousands of diverse, in-the-wild style categories. We introduce a Curriculum Continual Learning framework to train TeleStyle on this hybrid dataset of clean (curated) and noisy (synthetic) triplets. This approach enables the model to generalize to unseen styles without compromising precise content fidelity. Additionally, we introduce a video-to-video stylization module to enhance temporal consistency and visual quality. TeleStyle achieves state-of-the-art performance across three core evaluation metrics: style similarity, content consistency, and aesthetic quality. Code and pre-trained models are available at https://github.com/Tele-AI/TeleStyle

</details>


### [17] [Automated Marine Biofouling Assessment: Benchmarking Computer Vision and Multimodal LLMs on the Level of Fouling Scale](https://arxiv.org/abs/2601.20196)
*Brayden Hamilton,Tim Cashmore,Peter Driscoll,Trevor Gee,Henry Williams*

Main category: cs.CV

> 本研究利用自定义计算机视觉模型和大型多模态语言模型来评估船舶船体上生物污损的程度，发现混合方法对于生物污损的可扩展性评估具有潜力。

<details>
  <summary>Details</summary>

**Motivation:** 这项研究的动机是解决传统依赖潜水员检查的生物污损调查方法中存在的危险性和可扩展性限制问题。

**Method:** 本研究采用自定义的计算机视觉模型和大型多模态语言模型（LLMs）来自动分类LoF尺度上的生物污损程度。具体采用了卷积神经网络、基于变压器的分割模型和零样本学习LLMs进行评估。

**Result:** 计算机视觉模型在极端LoF分类上准确度高，但在中间等级上由于数据集不平衡和图像取景问题表现不佳。LLMs通过结构化提示和检索方法，在无需训练的情况下达到了可比的性能，并提供了可解释的输出。

**Conclusion:** 研究结果表明，不同方法之间存在互补优势。结合分割覆盖和LLMs的推理的混合方法，提供了一条有前景的可扩展且具有可解释性的生物污损评估路径。

**Abstract:** Marine biofouling on vessel hulls poses major ecological, economic, and biosecurity risks. Traditional survey methods rely on diver inspections, which are hazardous and limited in scalability. This work investigates automated classification of biofouling severity on the Level of Fouling (LoF) scale using both custom computer vision models and large multimodal language models (LLMs). Convolutional neural networks, transformer-based segmentation, and zero-shot LLMs were evaluated on an expert-labelled dataset from the New Zealand Ministry for Primary Industries. Computer vision models showed high accuracy at extreme LoF categories but struggled with intermediate levels due to dataset imbalance and image framing. LLMs, guided by structured prompts and retrieval, achieved competitive performance without training and provided interpretable outputs. The results demonstrate complementary strengths across approaches and suggest that hybrid methods integrating segmentation coverage with LLM reasoning offer a promising pathway toward scalable and interpretable biofouling assessment.

</details>


### [18] [DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment](https://arxiv.org/abs/2601.20218)
*Haoyou Deng,Keyu Yan,Chaojie Mao,Xiang Wang,Yu Liu,Changxin Gao,Nong Sang*

Main category: cs.CV

> 提出DenseGRPO框架来改进文本到图像生成的奖励对齐问题，通过密集奖励和自适应探索空间调整，提高模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 解决文本到图像生成中基于GRPO的方法存在稀疏奖励问题，导致中间去噪步骤与全局反馈信号不匹配。

**Method:** 采用DenseGRPO框架解决稀疏奖励问题，包括预测每一步的奖励增益作为密集奖励和基于奖励调整探索空间的策略。

**Result:** 在多个标准基准上的广泛实验表明，提出的DenseGRPO框架是有效的，并突出了在流匹配模型对齐中有效密集奖励的关键角色。

**Conclusion:** DenseGRPO框架通过密集奖励评估每个去噪步骤的细粒度贡献，解决了稀疏奖励问题，并通过自适应调整每一时刻的随机性注入来校准探索空间。

**Abstract:** Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce \textbf{DenseGRPO}, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.

</details>


### [19] [Feature Projection Learning for Better Vision-Language Reasoning](https://arxiv.org/abs/2601.20224)
*Yi Zhang,Weicheng Lin,Liang-Jie Zhang*

Main category: cs.CV

> 提出了FPL方法，通过特征投影解决了CLIP模型在适应下游任务时效率和准确性的问题，实验表明其超越了现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法在适应下游任务时存在性能有限、可训练参数过多或训练时间过长的问题，阻碍了CLIP模型在下游任务上的适应。

**Method:** 我们提出了一种名为FPL（Feature Projection Learning）的方法，通过开发一个投影模型，将类原型特征投影到查询图像特征空间，并重建查询图像特征图。采用负的平均平方重建误差作为分类得分，将分类问题转化为特征投影问题。最终输出是投影模型和原始预训练CLIP的预测结果的结合。

**Result:** 实验评估表明，FPL方法在准确性上超越了当前最先进的方法。

**Conclusion:** FPL方法是一种简单、高效且有效的方法，解决了现有方法在适应下游任务时存在的问题，并在准确性上超越了当前最先进的方法。

**Abstract:** Vision-Language Pre-Trained models, notably CLIP, that utilize contrastive learning have proven highly adept at extracting generalizable visual features. To inherit the well-learned knowledge of VLP models for downstream tasks, several approaches aim to adapt them efficiently with limited supervision. However, these methods either suffer from limited performance, excessive learnable parameters, or extended training times, all of which hinder their effectiveness in adapting the CLIP model to downstream tasks. In this work, we propose a simple yet efficient and effective method called \textit{\textbf{F}eature \textbf{P}rojection \textbf{L}earning(FPL)} to address these problems. Specifically, we develop a projection model that projects class prototype features into the query image feature space and reconstructs the query image feature map. The negative average squared reconstruction error is used as the class score. In this way, we transform the classification problem into a feature projection problem. The final output of this method is a combination of the prediction from the projection model and the original pre-trained CLIP. Comprehensive empirical evaluations confirm that FPL delivers superior accuracy, surpassing the current state-of-the-art methods by a substantial margin.

</details>


### [20] [Visual Prompt-Agnostic Evolution](https://arxiv.org/abs/2601.20232)
*Junze Wang,Lei Fan,Dezheng Zhang,Weipeng Jing,Donglin Di,Yang Song,Sidong Liu,Cong Cong*

Main category: cs.CV

> 提出Prompt-Agnostic Evolution（PAE）改进视觉提示调整，解决现有方法训练不稳定的问题，加速收敛并提升性能。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有视觉提示调整方法在训练过程中动态不稳定、浅层提示过早停滞、深层提示振幅大导致跨层不匹配等问题，从而降低收敛速度和最终性能。

**Method:** 通过显式建模提示动态来强化视觉提示调整，从频域视角初始化任务感知方向的提示，共享Koopman算子确保跨层一致进化，并引入Lyapunov稳定性理论约束进化过程中的误差放大。

**Result:** 在25个数据集上加速收敛1.41倍，性能提高1-3%，适用于多种下游任务。PAE是提示无关、轻量级的，无需修改骨干网络或改变推断时的行为。

**Conclusion:** PAE是一种有效的改进视觉提示调整的方法，能够加速收敛，提升性能，并且能够无缝集成到不同的VPT变体中。

**Abstract:** Visual Prompt Tuning (VPT) adapts a frozen Vision Transformer (ViT) to downstream tasks by inserting a small number of learnable prompt tokens into the token sequence at each layer. However, we observe that existing VPT variants often suffer from unstable training dynamics, characterized by gradient oscillations. A layer-wise analysis reveals that shallow-layer prompts tend to stagnate early, while deeper-layer prompts exhibit high-variance oscillations, leading to cross-layer mismatch. These issues slow convergence and degrade final performance. To address these challenges, we propose Prompt-Agnostic Evolution ($\mathtt{PAE}$), which strengthens vision prompt tuning by explicitly modeling prompt dynamics. From a frequency-domain perspective, we initialize prompts in a task-aware direction by uncovering and propagating frequency shortcut patterns that the backbone inherently exploits for recognition. To ensure coherent evolution across layers, we employ a shared Koopman operator that imposes a global linear transformation instead of uncoordinated, layer-specific updates. Finally, inspired by Lyapunov stability theory, we introduce a regularizer that constrains error amplification during evolution. Extensive experiments show that $\mathtt{PAE}$ accelerates convergence with an average $1.41\times$ speedup and improves accuracy by 1--3% on 25 datasets across multiple downstream tasks. Beyond performance, $\mathtt{PAE}$ is prompt-agnostic and lightweight, and it integrates seamlessly with diverse VPT variants without backbone modification or inference-time changes.

</details>
