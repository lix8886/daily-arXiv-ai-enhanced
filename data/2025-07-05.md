<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 7]
- [cs.CV](#cs.CV) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
*Tian Lan,Xiangdong Su,Xu Liu,Ruirui Wang,Ke Chang,Jiang Li,Guanglai Gao*

Main category: cs.CL

> 提出了一个多任务中文偏见评估基准（McBE），全面覆盖了多个偏见类别，适用于评估以中文和中国文化为基础的LLMs的偏见问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有的偏见评估数据集主要聚焦于英语和北美文化，适用于其他文化的数据集稀缺。此外，这些数据集通常只支持单一评估任务，不能从多个角度评估LLMs的偏见。

**Method:** 构建了一个多任务中文偏见评估基准（McBE），包括4,077个偏见评估实例，涵盖12个单偏见类别，82个子类别，并引入了5个评估任务。

**Result:** 评估了几种具有不同参数规模的流行LLMs，所有这些模型在不同程度上都表现出偏见。

**Conclusion:** 通过深入分析结果，对LLMs的偏见提供了新的见解，表明多任务评估基准对于理解并减轻LLMs的伦理风险很有帮助。

**Abstract:** As large language models (LLMs) are increasingly applied to various NLP
tasks, their inherent biases are gradually disclosed. Therefore, measuring
biases in LLMs is crucial to mitigate its ethical risks. However, most existing
bias evaluation datasets focus on English and North American culture, and their
bias categories are not fully applicable to other cultures. The datasets
grounded in the Chinese language and culture are scarce. More importantly,
these datasets usually only support single evaluation tasks and cannot evaluate
the bias from multiple aspects in LLMs. To address these issues, we present a
Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias
evaluation instances, covering 12 single bias categories, 82 subcategories and
introducing 5 evaluation tasks, providing extensive category coverage, content
diversity, and measuring comprehensiveness. Additionally, we evaluate several
popular LLMs from different series and with parameter sizes. In general, all
these LLMs demonstrated varying degrees of bias. We conduct an in-depth
analysis of results, offering novel insights into bias in LLMs.

</details>


### [2] [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)
*Keyan Jin,Yapeng Wang,Leonel Santos,Tao Fang,Xu Yang,Sio Kei Im,Hugo Gonçalo Oliveira*

Main category: cs.CL

> 研究了对话总结任务中长链推理架构的有效性，发现这类架构并不能一致提高总结质量，有时还会导致冗余、事实不准确和总结不够简洁。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于评估目前长链推理的大语言模型在不同类型的对话总结中表现如何，并探索这些模型在复杂对话背景中的局限性。

**Method:** 通过对比推理大语言模型和非推理模型，在通用对话、角色导向和问题导向的总结范式中进行全面系统分析，使用多个强基准和高级的评估协议。

**Result:** 研究发现，在对话总结任务中，显式的逐步推理并不一定有助于提高总结质量。推理模型往往比非推理模型更冗长、不准确、不够简洁。

**Conclusion:** 研究表明，目前的推理大语言模型在对话总结中有局限性，并提出需要特定模型和评估策略来优化真实世界的对话总结。

**Abstract:** Dialogue summarization is a challenging task with significant practical value
in customer service, meeting analysis, and conversational AI. Although large
language models (LLMs) have achieved substantial progress in summarization
tasks, the performance of step-by-step reasoning architectures-specifically
Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and
DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent
abstraction and conciseness. In this work, we present the first comprehensive
and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning
LLMs across three major paradigms-generic, role-oriented, and query-oriented
dialogue summarization. Our study spans diverse languages, domains, and summary
lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and
advanced evaluation protocols that include both LLM-based automatic metrics and
human-inspired criteria. Contrary to trends in other reasoning-intensive tasks,
our findings show that explicit stepwise reasoning does not consistently
improve dialogue summarization quality. Instead, reasoning LLMs are often prone
to verbosity, factual inconsistencies, and less concise summaries compared to
their non-reasoning counterparts. Through scenario-specific analyses and
detailed case studies, we further identify when and why explicit reasoning may
fail to benefit-or even hinder-summarization in complex dialogue contexts. Our
work provides new insights into the limitations of current reasoning LLMs and
highlights the need for targeted modeling and evaluation strategies for
real-world dialogue summarization.

</details>


### [3] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
*Wenquan Lu,Yuechuan Yang,Kyle Lee,Yanshu Li,Enqi Liu*

Main category: cs.CL

> 我们研究了Huginn-3.5B的内部推理结构，发现有限的证据支持其潜在的链式思维，并且递归深度增加的效果不明显。

<details>
  <summary>Details</summary>

**Motivation:** 我们希望通过这项研究来探索递归架构是否能在潜在空间内部化推理，特别是在Huginn-3.5B模型中，这种推理是否能支持潜在的链式思维。

**Method:** 我们通过一系列探针技术，包括Logit Lens和Coda Lens，来研究Huginn-3.5B模型在算术任务中的内部行为，以此探究其是否在潜在空间中形成了可解释的链式思维（CoT）结构。

**Result:** 研究发现，通过追踪最终结果和中间结果标记的排名轨迹，我们发现有限的证据表明存在可解释的潜在链式思维。同时，我们还发现探针技术在递归块之间的不一致性，隐藏状态的可解释性在很大程度上依赖于层数索引和解码方法。此外，我们还发现在递归深度增加时，效果的提升幅度有限，不及那些显式外部化推理步骤的模型。

**Conclusion:** 我们得出的结论是，尽管Huginn-3.5B模型在某些层面上展示了潜在的链式思维能力，但其在内部化推理方面仍有很大的局限性，尤其是在增加递归深度方面的提升并不显著。

**Abstract:** Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [4] [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)
*Steven Song,Anirudh Subramanyam,Zhenyu Zhang,Aarti Venkat,Robert L. Grossman*

Main category: cs.CL

> 研究团队开发了一个开源工具GDC Cohort Copilot，该工具可以把用户的自然语言描述转换成GDC队列过滤条件，从而辅助用户更快地创建和调整队列。

<details>
  <summary>Details</summary>

**Motivation:** GDC提供了一个统一的整理和分析平台，通过这个平台可以获取高质量的癌症基因组数据。尽管用户的交互式创建复杂队列非常方便，但新用户可能在数百个可能的字段和属性中找到特定队列描述存在困难。我们发现用户更可能用自然语言来描述他们想要的队列。

**Method:** 我们引入了GDC队列助理（GDC Cohort Copilot），这是一个开源工具，用于从GDC中策划队列。该工具能够根据用户输入的自然语言描述自动生成相应的GDC队列筛选条件，并导出回GDC进行进一步分析。我们开发并评估了多个大型语言模型（LLMs），证明了本地服务的开源GDC队列LLM在生成GDC队列方面优于GPT-4的提示方法。

**Result:** 我们开发了一款开源工具GDC队列助理，它可以依据用户提供的自然语言描述自动生成特定患者的队列过滤条件。我们通过使用多种大型语言模型进行实验并展示了我们的模型优于现有模型的表现。

**Conclusion:** 我们成功开发了GDC队列助理这个开源工具，它可以自动生成符合用户自然语言描述的GDC队列过滤条件，用户还可以通过交互式用户界面进一步调整生成的队列。

**Abstract:** Motivation: The Genomic Data Commons (GDC) provides access to high quality,
harmonized cancer genomics data through a unified curation and analysis
platform centered around patient cohorts. While GDC users can interactively
create complex cohorts through the graphical Cohort Builder, users (especially
new ones) may struggle to find specific cohort descriptors across hundreds of
possible fields and properties. However, users may be better able to describe
their desired cohort in free-text natural language.
  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for
curating cohorts from the GDC. GDC Cohort Copilot automatically generates the
GDC cohort filter corresponding to a user-input natural language description of
their desired cohort, before exporting the cohort back to the GDC for further
analysis. An interactive user interface allows users to further refine the
generated cohort. We develop and evaluate multiple large language models (LLMs)
for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC
Cohort LLM achieves better results than GPT-4o prompting in generating GDC
cohorts.
  Availability and implementation: The standalone docker image for GDC Cohort
Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.
Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC
Cohort LLM weights are available at https://huggingface.co/uc-ctds.

</details>


### [5] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
*Hongli Yu,Tinghong Chen,Jiangtao Feng,Jiangjie Chen,Weinan Dai,Qiying Yu,Ya-Qin Zhang,Wei-Ying Ma,Jingjing Liu,Mingxuan Wang,Hao Zhou*

Main category: cs.CL

> 提出了MemAgent，一种新的Agent工作流，以解决长文本处理中的性能问题，展示了领先的长文档处理能力。

<details>
  <summary>Details</summary>

**Motivation:** 尽管在长度外推、高效注意力和内存模块方面有所改进，但长文本处理中仍存在一个棘手问题：如何在处理无限长度文档时保持线性复杂度而不影响性能。因此，直接针对长文本任务进行端到端优化至关重要。

**Method:** 本研究引入了一种新的Agent工作流MemAgent，该工作流通过段落读取文本并采用覆盖策略更新内存。研究还扩展了DAPO算法以支持通过独立上下文多对话生成进行训练。

**Result:** MemAgent展示了出色的长上下文能力，能够从8K上下文中调整以32K文本为训练基准并在3.5M QA任务中表现，性能损失小于5%。同时，在512K RULER测试中达到了95%以上的正确率。

**Conclusion:** MemAgent在处理长文本任务时表现出色，能够有效地处理大段落的文本并保持高水平的性能，证明了其在长文本处理中的巨大潜力。

**Abstract:** Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [6] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

> 我们提出了DoMIX，一种基于LoRA模块的新方法，解决了连续域适应预训练中的计算成本高、对增量数据顺序敏感及无法提供针对特定任务的定制模型等问题，适用于标准LLM微调场景。

<details>
  <summary>Details</summary>

**Motivation:** 现有的连续域适应预训练方法面临着高计算成本和GPU内存使用、对增量数据顺序敏感以及提供的单一通用模型与域适应的核心思想相矛盾等多重挑战。因此，提出了一种新的解决方案。

**Method:** 通过利用LoRA模块，一种典型的参数高效微调(PEFT)方法，我们提出了一种新的方法DoMIX，该方法解决了现有的连续域适应预训练方法的挑战，实现了对域顺序鲁棒的高效并行域自适应预训练，并有效地利用积累的知识，为特定任务提供定制的预训练模型。

**Result:** 我们的方法不仅能够提高预训练模型在不同域上的表现，还能够减少计算成本和GPU内存的使用。此外，它对增量数据的顺序也更加鲁棒。我们还展示了我们的方法可以扩展到标准LLM微调场景以外的领域自适应预训练设置。

**Conclusion:** DoMIX方法成功解决了当前连续域适应预训练存在的问题，增加了对域顺序的鲁棒性，同时减少了计算成本和GPU内存需求，能够在标准LLM微调场景之外扩展应用。代码可在https://github.com/dohoonkim-ai/DoMIX获取。

**Abstract:** Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [7] [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)
*Christian Jaumann,Annemarie Friedrich,Rainer Lienhart*

Main category: cs.CL

> 本文介绍了一个用于科学视觉问答任务的系统，该系统利用多模态大语言模型进行问答，并通过选择模型及检索策略来提升性能。系统在评估中表现良好，排名第三。

<details>
  <summary>Details</summary>

**Motivation:** 为了参加SciVQA 2025共享任务（科学视觉问答任务），开发了一个用于科学视觉问答的系统。

**Method:** 采用包含两个多模态大语言模型的集成系统以及多种基于少量样本的检索策略。系统根据图形和问题类型选择模型和少量样本设置，并基于模型的置信水平选择答案。

**Result:** 在盲测数据上，该系统排名第三，平均F1得分为85.12。评分标准包括ROUGE-1、ROUGE-L、BERTS。

**Conclusion:** 该系统达到了满意的性能，并且相关代码已公开。

**Abstract:** This paper describes our system for the SciVQA 2025 Shared Task on Scientific
Visual Question Answering. Our system employs an ensemble of two Multimodal
Large Language Models and various few-shot example retrieval strategies. The
model and few-shot setting are selected based on the figure and question type.
We also select answers based on the models' confidence levels. On the blind
test data, our system ranks third out of seven with an average F1 score of
85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges](https://arxiv.org/abs/2507.02074)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.CV

> 本文综述了利用大规模语言模型和视觉语言模型从视频数据中检测碰撞的方法，并提出了融合策略的结构化分类，总结了关键数据集，分析了模型架构，比较了性能基准，并讨论了当前的挑战和机遇。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于利用最新的大规模语言模型和视觉语言模型技术提高智能交通系统中碰撞检测的准确性和效率。

**Method:** 文章采用了综述的方法，介绍了融合策略，总结了数据集，分析了模型架构，比较了性能基准。

**Result:** 文章提供了一个关于视频理解和基础模型这一快速发展的交叉领域的未来研究基础。

**Conclusion:** 通过综述，文章为未来的研究提供了一个坚实的基础，特别是在视频理解和基础模型的交叉领域中更多的创新与发展。

**Abstract:** Crash detection from video feeds is a critical problem in intelligent
transportation systems. Recent developments in large language models (LLMs) and
vision-language models (VLMs) have transformed how we process, reason about,
and summarize multimodal information. This paper surveys recent methods
leveraging LLMs for crash detection from video data. We present a structured
taxonomy of fusion strategies, summarize key datasets, analyze model
architectures, compare performance benchmarks, and discuss ongoing challenges
and opportunities. Our review provides a foundation for future research in this
fast-growing intersection of video understanding and foundation models.

</details>


### [9] [Underwater Monocular Metric Depth Estimation: Real-World Benchmarks and Synthetic Fine-Tuning](https://arxiv.org/abs/2507.02148)
*Zijie Cai,Christopher Metzler*

Main category: cs.CV

> 本文对单目度量深度预测在水下环境中的表现进行了全面评估，发现只在地面数据集上训练的模型在水下效果不佳。作者通过微调模型以适应水下数据，提高了整体性能。

<details>
  <summary>Details</summary>

**Motivation:** 单目深度估计在水下环境中的可靠性有限，主要是因为光衰减、颜色失真、浑浊度和缺乏高质量的度量真实数据。本文旨在提高水下环境中的单目度量深度预测的性能。

**Method:** 通过在使用基于物理的水下图像生成模型生成的合成水下Hypersim数据集上对Depth Anything V2（具有ViT-S主干编码器）进行微调，来改善水下环境中的单目度量深度预测。

**Result:** 实验结果表明，大规模模型在地面数据（真实或合成）上训练虽然在空气中表现良好，但在水下表现不佳，因为存在显著的数据域差异。微调模型在所有基准测试中的性能都有所提高，优于仅在清洁的Hypersim数据集上训练的模型。

**Conclusion:** 研究结果强调了领域适应和尺度感知监管对于实现水下场景中稳健和通用的度量深度预测的重要性，这是未来研究的重点。

**Abstract:** Monocular depth estimation has recently advanced to provide not only relative
but also metric depth predictions. However, its reliability in underwater
environments remains limited due to light attenuation and scattering, color
distortion, turbidity, and the lack of high-quality metric ground-truth data.
In this paper, we present a comprehensive benchmark of zero-shot and fine-tuned
monocular metric depth estimation models on real-world underwater datasets with
metric depth annotations, such as FLSea and SQUID. We evaluate a diverse set of
state-of-the-art models across a range of underwater conditions with different
ranges. Our results show that large-scale models trained on terrestrial (real
or synthetic) data, while effective in in-air settings, perform poorly
underwater due to significant domain shifts. To address this, we fine-tune
Depth Anything V2 with a ViT-S backbone encoder on a synthetic underwater
variant of the Hypersim dataset, which we generated using a physically based
underwater image formation model. We demonstrate our fine-tuned model
consistently improves performance across all benchmarks and outperforms
baselines trained only on the clean in-air Hypersim dataset. Our study provides
a detailed evaluation and visualization for monocular metric depth estimation
in underwater scenes, highlighting the importance of domain adaptation and
scale-aware supervision for achieving robust and generalizable metric depth
predictions in challenging underwater environments for future research.

</details>


### [10] [ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.02200)
*Xiao Wang,Jingtao Jiang,Qiang Chen,Lan Chen,Lin Zhu,Yaowei Wang,Yonghong Tian,Jin Tang*

Main category: cs.CV

> 本文提出了一个新的基于事件流的场景文本识别框架ESTR-CoT，结合视觉变换器和大规模语言模型进行解释性推理，通过三级数据处理生成了大规模推理数据集，实验表明该框架在多个基准数据集上效果显著。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于事件流的场景文本识别框架存在可解释性和逻辑推理能力不足的问题，因此本文提出了ESTR-CoT框架以提升识别效果。

**Method:** 方法包括使用EVA-CLIP将事件流转换为标记，Q-former将视觉标记与大规模语言模型对齐，并同时输出答案和推理过程。

**Result:** 实验结果显示ESTR-CoT框架在多个基准数据集上具有显著的识别效果和解释能力。

**Conclusion:** ESTR-CoT框架通过引入大规模语言模型进行解释性推理，可有效地提高事件流场景文本识别的性能，且提供了大规模的推理数据集供后续模型开发。

**Abstract:** Event stream based scene text recognition is a newly arising research topic
in recent years which performs better than the widely used RGB cameras in
extremely challenging scenarios, especially the low illumination, fast motion.
Existing works either adopt end-to-end encoder-decoder framework or large
language models for enhanced recognition, however, they are still limited by
the challenges of insufficient interpretability and weak contextual logical
reasoning. In this work, we propose a novel chain-of-thought reasoning based
event stream scene text recognition framework, termed ESTR-CoT. Specifically,
we first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input
event stream into tokens and utilize a Llama tokenizer to encode the given
generation prompt. A Q-former is used to align the vision token to the
pre-trained large language model Vicuna-7B and output both the answer and
chain-of-thought (CoT) reasoning process simultaneously. Our framework can be
optimized using supervised fine-tuning in an end-to-end manner. In addition, we
also propose a large-scale CoT dataset to train our framework via a three stage
processing (i.e., generation, polish, and expert verification). This dataset
provides a solid data foundation for the development of subsequent
reasoning-based large models. Extensive experiments on three event stream STR
benchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the
effectiveness and interpretability of our proposed framework. The source code
and pre-trained models will be released on
https://github.com/Event-AHU/ESTR-CoT.

</details>
