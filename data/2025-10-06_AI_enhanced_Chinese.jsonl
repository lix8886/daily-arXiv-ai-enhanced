{"id": "2510.02324", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02324", "abs": "https://arxiv.org/abs/2510.02324", "authors": ["Wannan Yang", "Xinchi Qiu", "Lei Yu", "Yuchen Zhang", "Oliver Aobo Yang", "Narine Kokhlikyan", "Nicola Cancedda", "Diego Garcia-Olano"], "title": "Hallucination reduction with CASAL: Contrastive Activation Steering For Amortized Learning", "comment": null, "summary": "Large Language Models (LLMs) exhibit impressive capabilities but often\nhallucinate, confidently providing incorrect answers instead of admitting\nignorance. Prior work has shown that models encode linear representations of\ntheir own knowledge and that activation steering can reduce hallucinations.\nThese approaches, however, require real-time monitoring and intervention during\ninference. We introduce Contrastive Activation Steering for Amortized Learning\n(CASAL), an efficient algorithm that connects interpretability with amortized\noptimization. CASAL directly bakes the benefits of activation steering into\nmodel's weights. Once trained, LLMs answer questions they know while abstaining\nfrom answering those they do not. CASAL's light-weight design requires training\nonly a submodule of a single transformer layer and yet reduces hallucination by\n30%-40% across multiple short-form QA benchmarks. CASAL is 30x more\ncompute-efficient and 20x more data-efficient than strong LoRA-based baselines\nsuch as SFT and DPO, boosting its practical applicability in data scarce\ndomains. Importantly, CASAL also generalizes effectively to out-of-distribution\n(OOD) domains. We showcase CASAL's flexibility in mitigating hallucinations in\nboth text-only and vision-language models. To our knowledge, CASAL is the first\nsteering-based training method that has been shown to be effective for both\ndense and Mixture-of-Experts (MoE) models. CASAL represents a promising step\nforward for applying interpretability-inspired method for practical deployment\nin production systems.", "AI": {"tldr": "CASAL efficiently reduces hallucinations in LLMs by encoding activation steering into model weights, improving compute and data efficiency while generalizing well to OOD domains.", "motivation": "To address the issue of LLMs frequently hallucinating, providing incorrect answers without admitting knowledge limitations, which requires real-time monitoring in prior methods.", "method": "Contrastive Activation Steering for Amortized Learning (CASAL) is introduced. It connects interpretability with amortized optimization by encoding the benefits of activation steering directly into the model's weights, allowing LLMs to answer questions within their knowledge and abstain from those they do not know.", "result": "CASAL reduces hallucinations by 30%-40% in multiple short-form QA benchmarks, is 30x more compute-efficient, and 20x more data-efficient than LoRA-based baselines such as SFT and DPO, and generalizes effectively to out-of-distribution (OOD) domains.", "conclusion": "CASAL demonstrates the potential of applying interpretability-inspired methods in practical production systems, especially in data-scarce domains, and is effective for both dense and Mixture-of-Experts (MoE) models."}}
{"id": "2510.02326", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02326", "abs": "https://arxiv.org/abs/2510.02326", "authors": ["Vivek Bhavsar", "Joseph Ereifej", "Aravanan Gurusami"], "title": "Hallucination-Resistant, Domain-Specific Research Assistant with Self-Evaluation and Vector-Grounded Retrieval", "comment": "21 pages, 5 figures", "summary": "Large language models accelerate literature synthesis but can hallucinate and\nmis-cite, limiting their usefulness in expert workflows. We present RA-FSM\n(Research Assistant - Finite State Machine), a modular GPT-based research\nassistant that wraps generation in a finite-state control loop: Relevance ->\nConfidence -> Knowledge. The system is grounded in vector retrieval and a\ndeterministic citation pipeline. The controller filters out-of-scope queries,\nscores answerability, decomposes questions, and triggers retrieval only when\nneeded, and emits answers with confidence labels and in-corpus, de-duplicated\nreferences. A ranked-tier ingestion workflow constructs a domain knowledge base\nfrom journals, conferences, indices, preprints, and patents, writing both to a\ndense vector index and to a relational store of normalized metrics. We\nimplement the system for photonics and evaluate it on six task categories:\nanalytical reasoning, numerical analysis, methodological critique, comparative\nsynthesis, factual extraction, and application design. In blinded A/B reviews,\ndomain experts prefer RA-FSM to both a strong Notebook LM (NLM) and a vanilla\nDefault GPT API call single-pass baseline, citing stronger boundary-condition\nhandling and more defensible evidence use. Coverage and novelty analyses\nindicate that RA-FSM explores beyond the NLM while incurring tunable latency\nand cost overheads. The design emphasizes transparent, well-cited answers for\nhigh-stakes technical work and is generalizable to other scientific domains.", "AI": {"tldr": "研究提出了一种模块化基于GPT的研究助手RA-FSM，通过有限状态控制循环提高文献综合的准确性和可靠性，并通过向量检索和确定性引用来增强其功能，在不同任务类别上，专家们更偏好于使用RA-FSM系统。", "motivation": "大型语言模型加速文献综合，但它们可能会出现幻觉和错误引用，从而限制了其在专家工作流程中的有用性。", "method": "我们提出了RA-FSM（研究助手-有限状态机），这是一个模块化的基于GPT的研究助手，它围绕一个有限状态控制循环来包装生成过程：相关性 -> 置信度 -> 知识。该系统建立在向量检索和确定性引用管道的基础上。控制组件可以过滤掉范围之外的查询，评分可回答性，分解问题，并在需要时触发检索，同时输出带有置信标签和在库内去重的引用的答案。", "result": "我们为光子学实现了该系统，并在六个任务类别上对其进行了评估：分析推理、数值分析、方法批判、比较综合、事实提取和应用设计。在盲审A/B审查中，域专家更偏好RA-FSM系统，尤其是在边界条件处理和证据使用方面。范围和新颖性分析表明，RA-FSM在探索内容方面超越了强Notebook LM（NLM），同时带来可调节的延迟和成本开销。", "conclusion": "在盲审A/B测试中，领域专家更喜欢RA-FSM系统，因为它在边界条件下处理得更好，引用证据也更有说服力。范围和新颖性分析表明，RA-FSM能探索比强Notebook LM（NLM）更多的内容，同时可以调节延迟和成本开销。设计重点是为高风险技术工作提供透明、引用明确的答案，并且该设计可以推广到其他科学领域。"}}
{"id": "2510.02327", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.02327", "abs": "https://arxiv.org/abs/2510.02327", "authors": ["So Kuroki", "Yotaro Kubo", "Takuya Akiba", "Yujin Tang"], "title": "KAME: Tandem Architecture for Enhancing Knowledge in Real-Time Speech-to-Speech Conversational AI", "comment": null, "summary": "Real-time speech-to-speech (S2S) models excel at generating natural,\nlow-latency conversational responses but often lack deep knowledge and semantic\nunderstanding. Conversely, cascaded systems combining automatic speech\nrecognition, a text-based Large Language Model (LLM), and text-to-speech\nsynthesis offer superior knowledge representation at the cost of high latency,\nwhich disrupts the flow of natural interaction. This paper introduces a novel\nhybrid architecture that bridges the gap between these two paradigms. Our\nframework processes user speech through an S2S transformer for immediate\nresponsiveness while concurrently relaying the query to a powerful back-end\nLLM. The LLM's text-based response is then injected in real time to guide the\nS2S model's speech generation, effectively infusing its output with rich\nknowledge without the full latency penalty of a cascaded system. We evaluated\nour method using a speech-synthesized variant of the MT-Bench benchmark that\nconsists of multi-turn question-answering sessions. The results demonstrate\nthat our system substantially outperforms a baseline S2S model in response\ncorrectness, approaching that of a cascaded system, while maintaining a latency\non par with the baseline.", "AI": {"tldr": "提出了一种混合架构，结合实时语音到语音模型和大语言模型的优势，使得生成的回应既有即时性也有知识性，解决了单纯模型各自存在的问题。", "motivation": "解决实时语音到语音模型响应低延迟但知识深度不足的问题，同时避免纯级联系统因为高延迟破坏自然交互的问题。", "method": "引入一种新颖的混合架构，结合即时响应的S2S变压器模型和后台的强大LLM。S2S模型负责即时生成回应，而LLM的文本回应实时注入到S2S模型中，用于指导其生成更具知识性的语音输出。", "result": "通过采用基于语音合成的MT-Bench基准测试，实验结果表明该系统在响应准确性上显著优于基线S2S模型，接近级联系统水平的同时，保持了与基线模型相同的低延迟。", "conclusion": "该系统可以有效提高实时语音到语音模型的知识性和响应准确性，并保持较低的延迟，适用于需要快速互动并且富含知识性的场景。"}}
{"id": "2510.02328", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.02328", "abs": "https://arxiv.org/abs/2510.02328", "authors": ["Ziqing Wang", "Chengsheng Mao", "Xiaole Wen", "Yuan Luo", "Kaize Ding"], "title": "AMANDA: Agentic Medical Knowledge Augmentation for Data-Efficient Medical Visual Question Answering", "comment": "EMNLP Findings", "summary": "Medical Multimodal Large Language Models (Med-MLLMs) have shown great promise\nin medical visual question answering (Med-VQA). However, when deployed in\nlow-resource settings where abundant labeled data are unavailable, existing\nMed-MLLMs commonly fail due to their medical reasoning capability bottlenecks:\n(i) the intrinsic reasoning bottleneck that ignores the details from the\nmedical image; (ii) the extrinsic reasoning bottleneck that fails to\nincorporate specialized medical knowledge. To address those limitations, we\npropose AMANDA, a training-free agentic framework that performs medical\nknowledge augmentation via LLM agents. Specifically, our intrinsic medical\nknowledge augmentation focuses on coarse-to-fine question decomposition for\ncomprehensive diagnosis, while extrinsic medical knowledge augmentation grounds\nthe reasoning process via biomedical knowledge graph retrieval. Extensive\nexperiments across eight Med-VQA benchmarks demonstrate substantial\nimprovements in both zero-shot and few-shot Med-VQA settings. The code is\navailable at https://github.com/REAL-Lab-NU/AMANDA.", "AI": {"tldr": "AMANDA框架通过医学知识增强提高了在低资源环境下的医疗视觉问答性能，显著改善了零样本和少量样本情况下的技术水平。", "motivation": "解决现有医疗多模态大语言模型在低资源环境下因医学推理能力瓶颈（内禀推理瓶颈和外在推理瓶颈）而导致性能下降的问题。", "method": "提出了一种名为AMANDA的无训练代理框架，通过LLM代理进行医学知识增强。具体来说，内在的医学知识增强侧重于细粒度的问题分解以进行全面诊断，而外在的医学知识增强则通过生物医学知识图谱检索来将推理过程扎根于特定的专业知识中。", "result": "在八个医疗视觉问答基准测试上展示了零样本和少量样本情况下显著的性能提升。", "conclusion": "AMANDA框架通过整合粗细粒度的问题分解和生物医学知识图谱检索来增强医学知识，从而有效提升医疗视觉问答系统的性能。"}}
{"id": "2510.02543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02543", "abs": "https://arxiv.org/abs/2510.02543", "authors": ["JoonHo Lee", "Sunho Park"], "title": "Exploring OCR-augmented Generation for Bilingual VQA", "comment": null, "summary": "We investigate OCR-augmented generation with Vision Language Models (VLMs),\nexploring tasks in Korean and English toward multilingualism. To support\nresearch in this domain, we train and release KLOCR, a strong bilingual OCR\nbaseline trained on 100M instances to augment VLMs with OCR ability. To\ncomplement existing VQA benchmarks, we curate KOCRBench for Korean VQA, and\nanalyze different prompting methods. Extensive experiments show that\nOCR-extracted text significantly boosts performance across open source and\ncommercial models. Our work offers new insights into OCR-augmented generation\nfor bilingual VQA. Model, code, and data are available at\nhttps://github.com/JHLee0513/KLOCR.", "AI": {"tldr": "论文研究OCR增强在韩语和英语双语VQA任务中的应用，发布KLOCR模型和KOCRBench基准测试，表明OCR技术显著提升模型性能。", "motivation": "我们希望通过OCR增强研究为视觉语言模型在多语言生成任务上的性能提升提供新的见解，特别是在韩语和英语的双语VQA任务上。", "method": "我们研究了OCR增强的生成任务，在韩语和英语中探索了视觉语言模型的多语言能力。为了支持这个领域的研究，我们训练并发布了KLOCR，这是一个强大的双语OCR基线模型，基于1亿个实例进行训练，可以增强VLMs的OCR能力。为了补充现有的VQA基准测试，我们整理了KOCRBench用于韩语VQA，并分析了不同的提示方法。", "result": "广泛的实验表明，通过OCR提取的文本显著提高了开源模型和商用模型的性能。", "conclusion": "我们的工作为OCR增强的双语VQA生成提供了新的见解。相关的模型、代码和数据可在https://github.com/JHLee0513/KLOCR获得。"}}
{"id": "2510.02329", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02329", "abs": "https://arxiv.org/abs/2510.02329", "authors": ["Kanghoon Yoon", "Minsub Kim", "Sungjae Lee", "Joonhyung Lee", "Sunghyeon Woo", "Yeonjun In", "Se Jung Kwon", "Chanyoung Park", "Dongsoo Lee"], "title": "SelfJudge: Faster Speculative Decoding via Self-Supervised Judge Verification", "comment": null, "summary": "Speculative decoding accelerates LLM inference by verifying candidate tokens\nfrom a draft model against a larger target model. Recent judge decoding boosts\nthis process by relaxing verification criteria by accepting draft tokens that\nmay exhibit minor discrepancies from target model output, but existing methods\nare restricted by their reliance on human annotations or tasks with verifiable\nground truths, limiting generalizability across diverse NLP tasks. We propose\nSelfJudge, which trains judge verifiers via self-supervision of the target\nmodel. Our method measures semantic preservation by assessing whether\ntoken-substituted responses preserve the meaning of original responses,\nenabling automatic verifier training across diverse NLP tasks. Our experiments\nshow SelfJudge achieves superior inference-accuracy trade-offs than judge\ndecoding baselines, offering a broadly applicable solution for faster LLM\ninference.", "AI": {"tldr": "本文提出了SelfJudge，一种通过自监督训练目标模型评判验证器的方法，能够自动训练验证器并且适用于多种NLP任务，在实验中显示出优越的推理速度与准确性。", "motivation": "现有的评判解码方法受限于对人类标注或具有可验证的ground truth任务的依赖，这限制了它们在各种NLP任务中的通用性。我们提出了SelfJudge以解决这一问题。", "method": "通过自监督方式训练目标模型的评判验证器，评估替换token后的回复是否保留了原回复的意义，从而实现自动验证器训练，适用于各种NLP任务。", "result": "SelfJudge方法在多种NLP任务上实现了比基准方法更好的推理-准确性权衡。", "conclusion": "实验表明，SelfJudge相比基准的评判解码方法，在推理准确性的权衡上表现更优，为更快的LLM推理提供了广泛应用的解决方案。"}}
{"id": "2510.02561", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02561", "abs": "https://arxiv.org/abs/2510.02561", "authors": ["Derek Shi", "Ruben Glatt", "Christine Klymko", "Shubham Mohole", "Hongjun Choi", "Shashank Kushwaha", "Sam Sakla", "Felipe Leno da Silva"], "title": "Oracle-RLAIF: An Improved Fine-Tuning Framework for Multi-modal Video Models through Reinforcement Learning from Ranking Feedback", "comment": "Proceedings of the 39th Annual Conference on Neural Information\n  Processing Systems, ARLET Workshop (Aligning Reinforcement Learning\n  Experimentalists and Theorists)", "summary": "Recent advances in large video-language models (VLMs) rely on extensive\nfine-tuning techniques that strengthen alignment between textual and visual\ncomprehension. Leading pipelines typically pair supervised fine-tuning (SFT)\nwith reinforcement learning from preference data to enhance video\ncomprehension. However, as VLMs scale in parameter size, so does the cost of\ngathering enough human feedback. To make fine-tuning more cost-effective,\nrecent frameworks explore reinforcement learning with AI feedback (RLAIF),\nwhich replace human preference with AI as a judge. Current RLAIF frameworks\nrely on a specialized reward model trained with video narratives to create\ncalibrated scalar rewards -- an expensive and restrictive pipeline. We propose\nOracle-RLAIF, a novel framework that replaces the trained reward model with a\nmore general Oracle ranker which acts as a drop-in model ranking candidate\nmodel responses rather than scoring them. Alongside Oracle-RLAIF, we introduce\n$GRPO_{rank}$, a novel rank-based loss function based on Group Relative Policy\nOptimization (GRPO) that directly optimizes ordinal feedback with rank-aware\nadvantages. Empirically, we demonstrate that Oracle-RLAIF consistently\noutperforms leading VLMs using existing fine-tuning methods when evaluated\nacross various video comprehension benchmarks. Oracle-RLAIF paves the path to\ncreating flexible and data-efficient frameworks for aligning large multi-modal\nvideo models with reinforcement learning from rank rather than score.", "AI": {"tldr": "本研究提出了Oracle-RLAIF框架和$GRPO_{rank}$损失函数，旨在使用AI反馈进行强化学习，从而降低成本并提升大规模多模态视频模型的排序排名效果。", "motivation": "随着VLMs参数规模的扩大，收集足够的用户反馈变得成本高昂。为了使微调更经济，该研究探索使用AI反馈的强化学习（RLAIF），这可以以AI代替人类作为评判标准。目前的RLAIF框架依赖于专门训练的奖励模型来生成校准的标量奖励，这成本高昂且限制性较大。", "method": "我们提出了Oracle-RLAIF框架，用一个更通用的Oracle排序器替代了专门训练的奖励模型，以排名候选模型响应。同时引入了基于Group Relative Policy Optimization (GRPO)的新型排序损失函数$GRPO_{rank}$，直接优化顺序反馈的等级感知优势。", "result": "实验证明，Oracle-RLAIF在不同视频理解基准测试中，比使用现有微调方法的领先VLM表现得更为出色。", "conclusion": "Oracle-RLAIF为创建灵活和数据高效的框架奠定了基础，可以使用强化学习从排名，而不是评分，来对大规模多模态视频模型进行对齐。"}}
{"id": "2510.02330", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02330", "abs": "https://arxiv.org/abs/2510.02330", "authors": ["Junlong Jia", "Ziyang Chen", "Xing Wu", "Chaochen Gao", "Zijia Lin", "Debing Zhang", "Songlin Hu", "Binghui Guo"], "title": "EntropyLong: Effective Long-Context Training via Predictive Uncertainty", "comment": "work in progress; Correspondence to: Xing Wu <wuxing@iie.ac.cn>", "summary": "Training long-context language models to capture long-range dependencies\nrequires specialized data construction. Current approaches, such as generic\ntext concatenation or heuristic-based variants, frequently fail to guarantee\ngenuine long-range dependencies. We propose EntropyLong, a novel data\nconstruction method that leverages predictive uncertainty to verify dependency\nquality. Our approach identifies high-entropy positions in documents, retrieves\nsemantically relevant contexts from large corpora, and verifies their utility\nby assessing whether they reduce prediction entropy. This model-in-the-loop\nverification ensures each dependency represents measurable information gain\nrather than spurious correlation. We construct training samples with long-range\ndependencies by combining original documents with these verified contextual\nsupplements. Using FineWebEdu and Cosmopedia, we generate a dataset of\n128K-length sequences with verified dependencies. Models trained on this data\ndemonstrate significant improvements on RULER benchmarks, particularly in tasks\nrequiring distant information. Following instruction fine-tuning, our models\nalso achieve substantial gains on LongBenchv2, demonstrating enhanced\nlong-context understanding. Extensive ablation studies further validate the\nnecessity and effectiveness of entropybased verification for long-context\ntraining.", "AI": {"tldr": "EntropyLong方法通过利用高熵位置预测的不确定性来验证并创建高质量的长距离依赖关系，从而生成包含真实长距离上下文的训练样本，这种方法在长上下文理解任务上表现显著优于现有方法。", "motivation": "现有的长上下文语言模型训练方法，如普通文本拼接或基于启发式方法的变体，通常无法保证真实的长距离依赖关系。因此，开发一种能够有效构建包含真实长距离依赖关系的训练数据的方法是必要的。", "method": "我们提出了一种名为EntropyLong的新颖数据构建方法，该方法利用预测不确定性来验证依赖关系的质量。具体而言，EntropyLong通过识别文档中的高熵位置，从大型语料库中检索语义相关上下文，并通过评估是否能够降低预测熵来验证这些上下文的有用性，以此确保每个依赖关系都代表了可测量的信息增益，而不是虚假关联。", "result": "使用FineWebEdu和Cosmopedia生成的长度为128K的序列数据集进行实验表明，训练模型在RULER基准测试上取得了显著的性能提升，尤其是在需要获取远处信息的任务中。经过指令细化微调后，在LongBenchv2上的表现也有显著提升，显示了模型理解长上下文的能力得到了增强。", "conclusion": "EntropyLong方法和生成的长距离依赖数据集被证明能够显著提升模型在处理长上下文理解任务上的表现，尤其是显著提高了RULER基准测试和LongBenchv2的任务得分。这说明熵验证对于长上下文训练的有效性是非常关键的。"}}
{"id": "2510.02566", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02566", "abs": "https://arxiv.org/abs/2510.02566", "authors": ["Qiao Feng", "Yiming Huang", "Yufu Wang", "Jiatao Gu", "Lingjie Liu"], "title": "PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction", "comment": null, "summary": "Reconstructing physically plausible human motion from monocular videos\nremains a challenging problem in computer vision and graphics. Existing methods\nprimarily focus on kinematics-based pose estimation, often leading to\nunrealistic results due to the lack of physical constraints. To address such\nartifacts, prior methods have typically relied on physics-based post-processing\nfollowing the initial kinematics-based motion estimation. However, this\ntwo-stage design introduces error accumulation, ultimately limiting the overall\nreconstruction quality. In this paper, we present PhysHMR, a unified framework\nthat directly learns a visual-to-action policy for humanoid control in a\nphysics-based simulator, enabling motion reconstruction that is both physically\ngrounded and visually aligned with the input video. A key component of our\napproach is the pixel-as-ray strategy, which lifts 2D keypoints into 3D spatial\nrays and transforms them into global space. These rays are incorporated as\npolicy inputs, providing robust global pose guidance without depending on noisy\n3D root predictions. This soft global grounding, combined with local visual\nfeatures from a pretrained encoder, allows the policy to reason over both\ndetailed pose and global positioning. To overcome the sample inefficiency of\nreinforcement learning, we further introduce a distillation scheme that\ntransfers motion knowledge from a mocap-trained expert to the\nvision-conditioned policy, which is then refined using physically motivated\nreinforcement learning rewards. Extensive experiments demonstrate that PhysHMR\nproduces high-fidelity, physically plausible motion across diverse scenarios,\noutperforming prior approaches in both visual accuracy and physical realism.", "AI": {"tldr": "PhysHMR直接在物理模拟器中学习从视频到动作的策略，通过像素为光线策略和知识蒸馏技术，实现高准确性和物理真实性的动作重建。", "motivation": "解决现有方法仅依靠基于动力学的姿态估计，缺少物理约束，导致结果不真实的问题，并避免了两阶段设计中累积误差造成的整体重建质量下降。", "method": "描述了PhysHMR，一个集成的框架，直接从单目视频中学习动作策略，并在物理模拟器中控制人形机器人，以进行物理上合理且与输入视频可视化对齐的动作重建。该方法采用了“像素为光线”的策略，将2D关键点提升为3D光束，并将其转换到全局空间。这些光线被作为策略输入，提供全局姿态指导。通过结合局部视觉特征，该策略能够同时处理详细的姿态和全局定位。此外，引入了知识蒸馏方案以增加样本效率，通过使用从运动捕捉训练的专家到视觉条件策略的知识转移，并用基于物理的强化学习奖励进一步优化策略。", "result": "实验显示PhysHMR在多样场景中创造了高保真度、物理上合理的动作，不仅在视觉准确性上，在物理现实性上也超越了先前的方法。", "conclusion": "PhysHMR在不牺牲视觉精确度的情况下，能够产生物理上合理的动作重建，验证了该方法在动作重建领域的优越性能。"}}
{"id": "2510.02331", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02331", "abs": "https://arxiv.org/abs/2510.02331", "authors": ["Moonkyung Ryu", "Chih-Wei Hsu", "Yinlam Chow", "Mohammad Ghavamzadeh", "Craig Boutilier"], "title": "Synthetic Dialogue Generation for Interactive Conversational Elicitation & Recommendation (ICER)", "comment": null, "summary": "While language models (LMs) offer great potential for conversational\nrecommender systems (CRSs), the paucity of public CRS data makes fine-tuning\nLMs for CRSs challenging. In response, LMs as user simulators qua data\ngenerators can be used to train LM-based CRSs, but often lack behavioral\nconsistency, generating utterance sequences inconsistent with those of any real\nuser. To address this, we develop a methodology for generating natural\ndialogues that are consistent with a user's underlying state using behavior\nsimulators together with LM-prompting. We illustrate our approach by generating\na large, open-source CRS data set with both preference elicitation and example\ncritiquing. Rater evaluation on some of these dialogues shows them to exhibit\nconsiderable consistency, factuality and naturalness.", "AI": {"tldr": "该研究提出了利用行为模拟器和语言模型提示生成具有用户内在状态一致性的自然对话的方法，并创建了一个包含偏好提取和示例批判的大型开源CRS数据集。", "motivation": "由于公共CRS数据的缺乏使得语言模型在CRS中的微调颇具挑战性，该研究旨在通过生成与用户内在状态一致的自然对话来解决这一问题。", "method": "结合行为模拟器和LM提示生成具有行为一致性的对话。", "result": "生成了一个包含偏好提取和示例批判的大型开源CRS数据集，评估显示这些对话在一致性、事实性和自然性方面表现良好。", "conclusion": "该方法能够生成高质量的CRS对话数据，有望解决公共CRS数据不足的问题，并促进CRS的研究与发展。"}}
{"id": "2510.02570", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02570", "abs": "https://arxiv.org/abs/2510.02570", "authors": ["P. Jonathon Phillips", "Geraldine Jeckeln", "Carina A. Hahn", "Amy N. Yates", "Peter C. Fontana", "Alice J. O'Toole"], "title": "Unlocking the power of partnership: How humans and machines can work together to improve face recognition", "comment": null, "summary": "Human review of consequential decisions by face recognition algorithms\ncreates a \"collaborative\" human-machine system. Individual differences between\npeople and machines, however, affect whether collaboration improves or degrades\naccuracy in any given case. We establish the circumstances under which\ncombining human and machine face identification decisions improves accuracy.\nUsing data from expert and non-expert face identifiers, we examined the\nbenefits of human-human and human-machine collaborations. The benefits of\ncollaboration increased as the difference in baseline accuracy between\ncollaborators decreased-following the Proximal Accuracy Rule (PAR). This rule\npredicted collaborative (fusion) benefit across a wide range of baseline\nabilities, from people with no training to those with extensive training. Using\nthe PAR, we established a critical fusion zone, where humans are less accurate\nthan the machine, but fusing the two improves system accuracy. This zone was\nsurprisingly large. We implemented \"intelligent human-machine fusion\" by\nselecting people with the potential to increase the accuracy of a\nhigh-performing machine. Intelligent fusion was more accurate than the machine\noperating alone and more accurate than combining all human and machine\njudgments. The highest system-wide accuracy achievable with human-only\npartnerships was found by graph theory. This fully human system approximated\nthe average performance achieved by intelligent human-machine collaboration.\nHowever, intelligent human-machine collaboration more effectively minimized the\nimpact of low-performing humans on system-wide accuracy. The results\ndemonstrate a meaningful role for both humans and machines in assuring accurate\nface identification. This study offers an evidence-based road map for the\nintelligent use of AI in face identification.", "AI": {"tldr": "研究展示了人类与机器协作在人脸识别准确性上的影响，提出了一种智能融合策略，其中选择具有潜在提高高性能机器准确性的人类合作伙伴，并证明了这种混合合作方式比单独的机器或所有人类与机器判断的组合更准确。", "motivation": "旨在探究人类与机器协作在人脸识别中的作用及如何优化协作系统的准确性。", "method": "使用专家与非专家的人脸识别数据，考察人与人、人与机器协作的优点，并根据Proximal Accuracy Rule (PAR)原则研究不同基线准确性下的协作效果。", "result": "发现了人类与机器协作的价值在于二者基线准确性相差不大时能够提高整体准确性。智能融合策略通过选择合适的人类伙伴提高了系统的准确性。", "conclusion": "人类和机器协作在准确识别人脸中扮演着重要角色。提出了一种智能人脸识别系统，减少低表现人类对系统准确性的负面影响，提供了一个基于证据的AI应用指南。"}}
{"id": "2510.02332", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.02332", "abs": "https://arxiv.org/abs/2510.02332", "authors": ["Yapei Feng", "Feng Jiang", "Shanhao Wu", "Hua Zhong"], "title": "A High-Capacity and Secure Disambiguation Algorithm for Neural Linguistic Steganography", "comment": "13 pages,7 figures", "summary": "Neural linguistic steganography aims to embed information\n  into natural text while preserving statistical undetectability. A fundamental\nchallenge in this ffeld stems from tokenization ambiguity in modern tokenizers,\nwhich can lead to catastrophic decoding failures. The recent method, SyncPool,\naddresses this ambiguity\n  by employing a coarse-grained synchronization mechanism over groups of\nambiguous candidates. However, SyncPool sacriffces embedding capacity, as it\nutilizes the entire Shannon entropy of an ambiguous group solely for\nsynchronization rather than for payload embedding. We propose a method named\nlook-ahead Sync, which overcomes the capacity limitation of SyncPool while\nretaining its provable security guarantees. Our approach performs minimal\nsynchronized sampling only on truly indistinguishable token sequences, while\nstrategically preserving all other discernible paths to maximize embedding\ncapacity. We provide theoretical proofs for the security of our method and\nanalyze the gap between its achievable embedding capacity and the theoretical\nupper bound. Experiments on English (using Llama 3) and Chinese (using Qwen\n2.5) benchmarks show that our method consistently approaches the theoretical\ncapacity upper bound and signiffcantly outperforms SyncPool. The improvement in\nembedding rate exceeds 160% in English and 25% in Chinese, particularly in\nsettings with larger candidate pools. This work represents a signiffcant step\ntoward practical high-capacity provably secure linguistic steganography.", "AI": {"tldr": "The paper proposes look-ahead Sync, which addresses the embedding capacity issues of SyncPool and attains higher embedding rates in both English and Chinese contexts.", "motivation": "Overcome the capacity limitation of SyncPool while retaining its provable security guarantees in neural linguistic steganography.", "method": "look-ahead Sync, which performs minimal synchronized sampling on truly indistinguishable token sequences and strategically preserves all other discernible paths to maximize embedding capacity.", "result": "The method consistently approaches the theoretical capacity upper bound, with a significant improvement over SyncPool: over 160% in English and 25% in Chinese, especially in scenarios with larger candidate pools.", "conclusion": "The proposed method represents a significant advancement toward practical high-capacity and provably secure linguistic steganography."}}
{"id": "2510.02571", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02571", "abs": "https://arxiv.org/abs/2510.02571", "authors": ["Zhiting Mei", "Ola Shorinwa", "Anirudha Majumdar"], "title": "How Confident are Video Models? Empowering Video Models to Express their Uncertainty", "comment": null, "summary": "Generative video models demonstrate impressive text-to-video capabilities,\nspurring widespread adoption in many real-world applications. However, like\nlarge language models (LLMs), video generation models tend to hallucinate,\nproducing plausible videos even when they are factually wrong. Although\nuncertainty quantification (UQ) of LLMs has been extensively studied in prior\nwork, no UQ method for video models exists, raising critical safety concerns.\nTo our knowledge, this paper represents the first work towards quantifying the\nuncertainty of video models. We present a framework for uncertainty\nquantification of generative video models, consisting of: (i) a metric for\nevaluating the calibration of video models based on robust rank correlation\nestimation with no stringent modeling assumptions; (ii) a black-box UQ method\nfor video models (termed S-QUBED), which leverages latent modeling to\nrigorously decompose predictive uncertainty into its aleatoric and epistemic\ncomponents; and (iii) a UQ dataset to facilitate benchmarking calibration in\nvideo models. By conditioning the generation task in the latent space, we\ndisentangle uncertainty arising due to vague task specifications from that\narising from lack of knowledge. Through extensive experiments on benchmark\nvideo datasets, we demonstrate that S-QUBED computes calibrated total\nuncertainty estimates that are negatively correlated with the task accuracy and\neffectively computes the aleatoric and epistemic constituents.", "AI": {"tldr": "论文介绍了一种用于量化生成式视频模型不确定性的框架，包括不确定性量化的指标、方法和数据集，为了解决现有视频模型中不确定性量化不足的问题。", "motivation": "论文的动机在于解决视频生成模型与生成式语言模型类似，容易产生与事实不符但看似合理的视频的问题，以及缺乏视频模型的不确定性量化方法带来了严重安全问题。", "method": "该论文提出了一种用于生成式视频模型的不确定性量化框架，包括一种基于鲁棒秩相关估计的视频模型校准评估指标、一种称为S-QUBED的黑盒不确定性量化方法以及一个用于视频模型校准基准测试的不确定性数据集。此外，通过在潜在空间中条件化生成任务，论文成功地将由于任务规格的模糊性和知识的缺乏而产生的不确定性区分开来。", "result": "实验结果表明，S-QUBED可以计算出与任务准确性负相关的校准后的总不确定估计，并能有效计算随机不确定性和认知不确定性成分。", "conclusion": "该研究是首次尝试量化视频模型的不确定性，并提供了方法框架，有助于提高生成式视频模型的安全性和可靠性。"}}
{"id": "2510.02333", "categories": ["cs.CL", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.02333", "abs": "https://arxiv.org/abs/2510.02333", "authors": ["Chiara Pugliese", "Francesco Lettich", "Guido Rocchietti", "Chiara Renso", "Fabio Pinelli"], "title": "Human Mobility Datasets Enriched With Contextual and Social Dimensions", "comment": "5 pages, 3 figures, 1 table", "summary": "In this resource paper, we present two publicly available datasets of\nsemantically enriched human trajectories, together with the pipeline to build\nthem. The trajectories are publicly available GPS traces retrieved from\nOpenStreetMap. Each dataset includes contextual layers such as stops, moves,\npoints of interest (POIs), inferred transportation modes, and weather data. A\nnovel semantic feature is the inclusion of synthetic, realistic social media\nposts generated by Large Language Models (LLMs), enabling multimodal and\nsemantic mobility analysis. The datasets are available in both tabular and\nResource Description Framework (RDF) formats, supporting semantic reasoning and\nFAIR data practices. They cover two structurally distinct, large cities: Paris\nand New York. Our open source reproducible pipeline allows for dataset\ncustomization, while the datasets support research tasks such as behavior\nmodeling, mobility prediction, knowledge graph construction, and LLM-based\napplications. To our knowledge, our resource is the first to combine real-world\nmovement, structured semantic enrichment, LLM-generated text, and semantic web\ncompatibility in a reusable framework.", "AI": {"tldr": "The paper introduces semantically enriched trajectory datasets for Paris and New York, combining real-world GPS data with contextual layers and LLM-generated social media posts, available in RDF and tabular formats. A reproducible pipeline is provided for dataset customization and further research.", "motivation": "The motivation behind the paper is to provide researchers with enriched, semantically annotated trajectory datasets along with a reproducible method for building similar resources. This enhances the understanding of human movement patterns and supports the development of mobility-related applications with the added value of multimodal semantic analysis.", "method": "Content presents two datasets of semantically enriched human trajectories with a detailed pipeline for their creation. The datasets, derived from OpenStreetMap, integrate GPS traces with contextual layers including POIs, transportation modes, weather data, and LLM-generated social media posts. The data is available in RDF and tabular formats, enhancing semantic reasoning and FAIR principles. The datasets cover Paris and New York, facilitating diverse research applications such as behavior modeling, mobility prediction, and knowledge graph construction.", "result": "The result is the creation and publication of two semantically enriched trajectory datasets that integrate real-world movement data with contextual layers, and for the first time, LLM-generated social media posts, all in a FAIR-aligned RDF and tabular formats, supporting diverse mobility research tasks.", "conclusion": "The datasets and open source pipeline facilitate multimodal and semantic mobility research, enabling advancements in mobility prediction, behavior modeling, and knowledge graph construction, and are the first to integrate real-world movement with semantic and LLM-generated content in a reusable form."}}
{"id": "2510.02599", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02599", "abs": "https://arxiv.org/abs/2510.02599", "authors": ["Hovhannes Margaryan", "Bo Wan", "Tinne Tuytelaars"], "title": "PEO: Training-Free Aesthetic Quality Enhancement in Pre-Trained Text-to-Image Diffusion Models with Prompt Embedding Optimization", "comment": null, "summary": "This paper introduces a novel approach to aesthetic quality improvement in\npre-trained text-to-image diffusion models when given a simple prompt. Our\nmethod, dubbed Prompt Embedding Optimization (PEO), leverages a pre-trained\ntext-to-image diffusion model as a backbone and optimizes the text embedding of\na given simple and uncurated prompt to enhance the visual quality of the\ngenerated image. We achieve this by a tripartite objective function that\nimproves the aesthetic fidelity of the generated image, ensures adherence to\nthe optimized text embedding, and minimal divergence from the initial prompt.\nThe latter is accomplished through a prompt preservation term. Additionally,\nPEO is training-free and backbone-independent. Quantitative and qualitative\nevaluations confirm the effectiveness of the proposed method, exceeding or\nequating the performance of state-of-the-art text-to-image and prompt\nadaptation methods.", "AI": {"tldr": "The paper presents Prompt Embedding Optimization (PEO), a method for enhancing the aesthetic quality of images generated by pre-trained text-to-image models given a simple prompt.", "motivation": "The motivation is to improve the aesthetic quality of images generated by text-to-image models when a simple prompt is provided.", "method": "Our method, Prompt Embedding Optimization (PEO), leverages a pre-trained text-to-image diffusion model and optimizes the text embedding of a simple prompt to enhance the aesthetic quality of the generated image.", "result": "Quantitative and qualitative evaluations demonstrate that PEO outperforms or matches state-of-the-art text-to-image and prompt adaptation methods in terms of aesthetic quality.", "conclusion": "PEO is effective in improving the visual quality of images generated from simple prompts, without requiring additional training and being backbone-independent."}}
