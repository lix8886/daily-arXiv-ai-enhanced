<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 36]
- [cs.CV](#cs.CV) [Total: 33]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Towards Probabilistic Question Answering Over Tabular Data](https://arxiv.org/abs/2506.20747)
*Chen Shen,Sajjadur Rahman,Estevam Hruschka*

Main category: cs.CL

> The paper introduces LUCARIO, a new benchmark and framework for probabilistic question answering over large tabular data, demonstrating significant improvements over existing methods by utilizing a hybrid symbolic-neural approach.

<details>
  <summary>Details</summary>

**Motivation:** Current QA systems perform well for factual questions but lack the capability to handle probabilistic questions requiring reasoning under uncertainty.

**Method:** Our method induces Bayesian Networks from tables, translates natural language queries into probabilistic queries, and uses large language models to generate final answers.

**Result:** Empirical results demonstrate significant improvements over baselines.

**Conclusion:** The study highlights the benefits of hybrid symbolic-neural reasoning for probabilistic question answering over large tabular data.

**Abstract:** Current approaches for question answering (QA) over tabular data, such as
NL2SQL systems, perform well for factual questions where answers are directly
retrieved from tables. However, they fall short on probabilistic questions
requiring reasoning under uncertainty. In this paper, we introduce a new
benchmark LUCARIO and a framework for probabilistic QA over large tabular data.
Our method induces Bayesian Networks from tables, translates natural language
queries into probabilistic queries, and uses large language models (LLMs) to
generate final answers. Empirical results demonstrate significant improvements
over baselines, highlighting the benefits of hybrid symbolic-neural reasoning.

</details>


### [2] [Multi-lingual Functional Evaluation for Large Language Models](https://arxiv.org/abs/2506.20793)
*Victor Ojewale,Inioluwa Deborah Raji,Suresh Venkatasubramanian*

Main category: cs.CL

> 本研究通过创建多语言功能基准来评估大型语言模型的实用性与鲁棒性，发现与其他现有基准相比，新的基准更能准确反映模型在多种语言环境下的真实表现。

<details>
  <summary>Details</summary>

**Motivation:** 现有的多语言评估方式常常未能提供对模型跨多种语言环境中实际表现和鲁棒性的充分理解。

**Method:** 通过将现有的功能基准模板从英语翻译成法语、西班牙语、印地语、阿拉伯语和约鲁巴语，创建了多语言功能基准——跨语言小学数学符号（CL-GSM Symbolic）和跨语言指令跟随评估（CL-IFEval）来回应静态数据评估在多语言环境下的表现理解不足的问题。

**Result:** 研究结果显示，一些静态多语言基准捕获功能表现比其他基准更接近。在CL-GSM Symbolic中，英文、法文和西班牙文的表现分别下降了24%、17%和18%；而在CL-IFEval中，性能下降范围介于15%到24%之间。不同语言之间的模型鲁棒性表现出显著差异。

**Conclusion:** 本研究证明了多语言功能基准对于评估大型语言模型跨多种语言的表现和鲁棒性的价值，同时指出了各类基准在不同语言表现上的差异性。

**Abstract:** Multi-lingual competence in large language models is often evaluated via
static data benchmarks such as Belebele, M-MMLU and M-GSM. However, these
evaluations often fail to provide an adequate understanding of the practical
performance and robustness of models across multi-lingual settings. In
response, we create multi-lingual functional benchmarks -- Cross-Lingual Grade
School Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following
Eval (CL-IFEval)-- by translating existing functional benchmark templates from
English to five additional languages that span the range of resources available
for NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that
some static multi-lingual benchmarks capture functional performance much more
closely than others (i.e. across models, there is a 24%, 17% and 18% decrease
in performance between M-GSM and CL-GSM Symbolic in English, French and Spanish
respectively; similarly there's a 15 - 24% performance drop across languages
between Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between
M-MMLU and CL-IFEval). Similarly, we find that model robustness across
languages varies significantly, with certain languages (eg. Arabic, English)
being the most consistently well performing across evaluation iterations.

</details>


### [3] [The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas](https://arxiv.org/abs/2506.20803)
*Chenglei Si,Tatsunori Hashimoto,Diyi Yang*

Main category: cs.CL

> 本研究通过实验证实，尽管人工智能生成的研究想法在创意阶段表现新颖，但在执行阶段其质量下降显著，低于人类生成的想法。这揭示了大型语言模型生成有效研究想法的局限性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管有研究表明，大型语言模型生成的研究想法在新颖性上可能超过人类专家的想法，但高质量的想法不仅应当新颖，还应在执行后带来更好的研究结果。因此，本研究旨在检验由人工智能生成的想法是否能提升研究的成效。

**Method:** 通过招募43位专家研究人员执行随机分配的研究想法（这些想法是由专家编写或由大型语言模型生成的），来测试人工智能生成的想法是否能带来更好的研究结果。每位专家投入超过100小时实施想法，并撰写一篇4页的短文记录实验过程。所有执行的项目由专家自然语言处理研究员进行匿名评审。

**Result:** 执行后，大型语言模型生成的想法的评审分数（新颖性、兴奋性、有效性、总体）显著下降，而且下降幅度高于专家编写的想法，这意味着人工智能生成的想法在执行后其质量不及预期。在许多评审指标中，人类生成的想法得分甚至超过了人工智能生成的想法。

**Conclusion:** 这种想法生成与执行之间的差异突显了当前大型语言模型在生成真正有效的研究想法方面的局限性，也暴露了在没有执行结果的情况下评估研究想法的挑战。

**Abstract:** Large Language Models (LLMs) have shown promise in accelerating the
scientific research pipeline. A key capability for this process is the ability
to generate novel research ideas, and prior studies have found settings in
which LLM-generated research ideas were judged as more novel than human-expert
ideas. However, a good idea should not simply appear to be novel, it should
also result in better research after being executed. To test whether
AI-generated ideas lead to better research outcomes, we conduct an execution
study by recruiting 43 expert researchers to execute randomly-assigned ideas,
either written by experts or generated by an LLM. Each expert spent over 100
hours implementing the idea and wrote a 4-page short paper to document the
experiments. All the executed projects are then reviewed blindly by expert NLP
researchers. Comparing the review scores of the same ideas before and after
execution, the scores of the LLM-generated ideas decrease significantly more
than expert-written ideas on all evaluation metrics (novelty, excitement,
effectiveness, and overall; p < 0.05), closing the gap between LLM and human
ideas observed at the ideation stage. When comparing the aggregated review
scores from the execution study, we even observe that for many metrics there is
a flip in rankings where human ideas score higher than LLM ideas. This
ideation-execution gap highlights the limitations of current LLMs in generating
truly effective research ideas and the challenge of evaluating research ideas
in the absence of execution outcomes.

</details>


### [4] [MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering](https://arxiv.org/abs/2506.20821)
*Chinmay Gondhalekar,Urjitkumar Patel,Fang-Chun Yeh*

Main category: cs.CL

> 提出了MultiFinRAG，一种专门为财务问答任务设计的检索增强生成框架。该方法通过分层次递归策略，实现了在普通硬件上跨模态推理的高效准确问答，表现出比ChatGPT-4o更高的准确率。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机是由于财务文件（如年度报告10-Ks、季度报告10-Qs和投资者演示）通常包含大量文本、表格和图像等多种模态，并且对这些内容进行答案提取往往需要跨模态推理，这超出了传统大型语言模型和检索增强生成管道的能力，因为后者会受到token限制、布局损失和跨模态上下文碎片化的困扰。

**Method:** MultiFinRAG方法首先进行多模态提取，将表格和图像分成批次并发送给一个轻量级的开源多模态大型语言模型，该模型生成结构化的JSON输出和简洁的文本摘要。这些输出连同叙事文本一起嵌入并根据模态感知相似度阈值进行索引，以便精确检索。该方法采用分层递归策略，根据需要从文本到文本+表格+图像上下文进行动态升级，实现在减少无关上下文的同时的跨模态推理能力。

**Result:** 在涉及文本、表格、图像和结合多模态推理的复杂财务问题回答任务中，尽管在普通硬件上运行，MultiFinRAG仍比ChatGPT-4o（免费级别）高出19个百分点的准确率。

**Conclusion:** 研究结果表明，MultiFinRAG作为一种专门针对财务领域的问答任务构建的检索增强生成框架，能够克服传统大型语言模型和检索增强生成方法的限制，通过分层递归策略实现在普通硬件上进行有效的跨模态推理，同时提高了问题回答准确性。

**Abstract:** Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span
hundreds of pages and combine diverse modalities, including dense narrative
text, structured tables, and complex figures. Answering questions over such
content often requires joint reasoning across modalities, which strains
traditional large language models (LLMs) and retrieval-augmented generation
(RAG) pipelines due to token limitations, layout loss, and fragmented
cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation
framework purpose-built for financial QA. MultiFinRAG first performs multimodal
extraction by grouping table and figure images into batches and sending them to
a lightweight, quantized open-source multimodal LLM, which produces both
structured JSON outputs and concise textual summaries. These outputs, along
with narrative text, are embedded and indexed with modality-aware similarity
thresholds for precise retrieval. A tiered fallback strategy then dynamically
escalates from text-only to text+table+image contexts when necessary, enabling
cross-modal reasoning while reducing irrelevant context. Despite running on
commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy
than ChatGPT-4o (free-tier) on complex financial QA tasks involving text,
tables, images, and combined multimodal reasoning.

</details>


### [5] [Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes](https://arxiv.org/abs/2506.20822)
*Quintin Myers,Yanjun Gao*

Main category: cs.CL

> 研究评估了大型语言模型对暴力内容反应的倾向，发现这些模型的表面反应与其内部倾向不一致，并且模型显示的暴力倾向与个人的社会人口学特征有关。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型越来越多地被提议用于在线检测和响应暴力内容，但这些模型在处理道德上模棱两可的现实世界场景时的能力仍然没有得到充分研究。因此，本研究旨在填补这一空白。

**Method:** 研究通过使用《暴力行为小品问卷》（VBVQ）这一经过验证的社会科学研究工具来评估大型语言模型（LLMs）对现实世界冲突事件的反应能力。为了评估潜在的偏见，研究引入了基于人物设定的提示，该设定变化了美国境内的种族、年龄、地理认同等因素。评估了六个在不同地缘政治组织背景下开发的LLMs在统一的零样本情况下对暴力内容的检测与响应能力。

**Result:** 研究揭示了两个关键发现：(1) LLMs的表面文本生成经常与其对暴力反应的内在偏好相矛盾；(2) 它们的暴力倾向在不同的社会人群中有所差异，通常与犯罪学、社会科学和心理学中已确立的发现不符。

**Conclusion:** 本研究发现大型语言模型在生成文本时表现出的暴力倾向和其内在的暴力倾向之间存在差异，且这种倾向性具有社会人口学差异，这与现有社会科学领域发现有悖。

**Abstract:** Large language models (LLMs) are increasingly proposed for detecting and
responding to violent content online, yet their ability to reason about morally
ambiguous, real-world scenarios remains underexamined. We present the first
study to evaluate LLMs using a validated social science instrument designed to
measure human response to everyday conflict, namely the Violent Behavior
Vignette Questionnaire (VBVQ). To assess potential bias, we introduce
persona-based prompting that varies race, age, and geographic identity within
the United States. Six LLMs developed across different geopolitical and
organizational contexts are evaluated under a unified zero-shot setting. Our
study reveals two key findings: (1) LLMs surface-level text generation often
diverges from their internal preference for violent responses; (2) their
violent tendencies vary across demographics, frequently contradicting
established findings in criminology, social science, and psychology.

</details>


### [6] [Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine](https://arxiv.org/abs/2506.20876)
*Sebastian Joseph,Lily Chen,Barry Wei,Michael Mackert,Iain J. Marshall,Paul Pu Liang,Ramez Kouzy,Byron C. Wallace,Junyi Jessy Li*

Main category: cs.CL

> 研究探讨了在医学领域应用端到端事实核查系统时遇到的基本挑战，包括连接声明与科学证据的难度和声明的不确定性。

<details>
  <summary>Details</summary>

**Motivation:** 随着技术的发展，自动事实核查在医学领域内的应用引起关注。然而，此类系统的实际应用并不广泛，本研究旨在探索其背后的原因。

**Method:** 本研究通过分析临床专家如何核实社交媒体上的真实声明，从而综合医学证据，探讨了在医学领域应用端到端事实核查的基本挑战。

**Result:** 研究表明，在医学应用中的端到端事实核查面临着将野外声明与临床试验联系起来的困难，以及意图不匹配和声明的模糊性等挑战。

**Conclusion:** 事实核查应当被视作一种交互沟通问题而非端到端过程进行处理和评估。

**Abstract:** Technological progress has led to concrete advancements in tasks that were
regarded as challenging, such as automatic fact-checking. Interest in adopting
these systems for public health and medicine has grown due to the high-stakes
nature of medical decisions and challenges in critically appraising a vast and
diverse medical literature. Evidence-based medicine connects to every
individual, and yet the nature of it is highly technical, rendering the medical
literacy of majority users inadequate to sufficiently navigate the domain. Such
problems with medical communication ripens the ground for end-to-end
fact-checking agents: check a claim against current medical literature and
return with an evidence-backed verdict. And yet, such systems remain largely
unused. To understand this, we present the first study examining how clinical
experts verify real claims from social media by synthesizing medical evidence.
In searching for this upper-bound, we reveal fundamental challenges in
end-to-end fact-checking when applied to medicine: Difficulties connecting
claims in the wild to scientific evidence in the form of clinical trials;
ambiguities in underspecified claims mixed with mismatched intentions; and
inherently subjective veracity labels. We argue that fact-checking should be
approached and evaluated as an interactive communication problem, rather than
an end-to-end process.

</details>


### [7] [Optimising Language Models for Downstream Tasks: A Post-Training Perspective](https://arxiv.org/abs/2506.20917)
*Zhengyan Shi*

Main category: cs.CL

> 论文提出了一系列方法以更有效地将语言模型（LM）适应于下游应用，包括从无标签数据中提取任务相关信息的新技术，计算成本低、性能优越的参数高效微调方法，以及评估和基准测试的新方法。结果表明，这些方法显著地提高了LM的鲁棒性、效率和泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 随着语言模型规模和复杂性的增长，它们在特定任务上进行有效和稳固地微调仍然面临挑战，如：对标注数据的低估、小样本任务中过拟合以及计算成本高等问题，这些限制阻碍了语言模型在真实世界语言任务中的应用。

**Method:** 论文引入了从无标签数据中提取任务相关信息的新技术，开发了计算成本低廉、性能优越的参数高效微调技术，改进了监督微调方法，使得语言模型能够在标注数据稀缺的情况下更好地遵循指令，以及开发了新的评估方法和基准测试，如多层次空间推理任务。

**Result:** 广泛的经验研究表明，这些方法显著提高了语言模型在各种NLP任务上的鲁棒性、效率和泛化能力，使得它们在不同应用场景中更加灵活适应。

**Conclusion:** 这些进展标志着向更为稳固和高效的语言模型迈进的重要一步，并接近了人工智能的目标。

**Abstract:** Language models (LMs) have demonstrated remarkable capabilities in NLP, yet
adapting them efficiently and robustly to specific tasks remains challenging.
As their scale and complexity grow, fine-tuning LMs on labelled data often
underutilizes available unlabelled data, leads to overfitting on small
task-specific sets, and imposes significant computational costs. These
limitations hamper their application to the open-ended landscape of real-world
language tasks.
  This thesis proposes a series of methods to better adapt LMs to downstream
applications. First, we explore strategies for extracting task-relevant
knowledge from unlabelled data, introducing a novel continued pre-training
technique that outperforms state-of-the-art semi-supervised approaches. Next,
we present a parameter-efficient fine-tuning method that substantially reduces
memory and compute costs while maintaining competitive performance. We also
introduce improved supervised fine-tuning methods that enable LMs to better
follow instructions, especially when labelled data is scarce, enhancing their
performance across a range of NLP tasks, including open-ended generation.
Finally, we develop new evaluation methods and benchmarks, such as multi-hop
spatial reasoning tasks, to assess LM capabilities and adaptation more
comprehensively.
  Through extensive empirical studies across diverse NLP tasks, our results
demonstrate that these approaches substantially improve LM robustness,
efficiency, and generalization, making them more adaptable to a broad range of
applications. These advances mark a significant step towards more robust and
efficient LMs, bringing us closer to the goal of artificial general
intelligence.

</details>


### [8] [FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language](https://arxiv.org/abs/2506.20920)
*Guilherme Penedo,Hynek Kydlíček,Vinko Sabolčec,Bettina Messmer,Negar Foroutan,Amir Hossein Kargaran,Colin Raffel,Martin Jaggi,Leandro Von Werra,Thomas Wolf*

Main category: cs.CL

> 提出一种新的自动适应多语言的预训练数据集策划管线，并创建了一个20TB的多语言数据集(FineWeb2)。

<details>
  <summary>Details</summary>

**Motivation:** 由于难以将过滤和重复数据删除管线调整到大量语言中，训练高性能的多语言LLM仍面临挑战。

**Method:** 介绍了一种新的基于FineWeb的预训练数据集策划管线，该管线可自动适应任何语言。同时提出了一种简单而严谨的方法来调和数据集，该方法考虑了重复度和质量。

**Result:** 该管线可以用来创建非英语语料库，其生成的模型比之前的数据集性能更佳。最终管线扩展到超过1000种语言，生成了一个新的20TB多语言数据集(FineWeb2)。

**Conclusion:** 证明了新的策划管线能够生成性能更好的多语言LLM。

**Abstract:** Pre-training state-of-the-art large language models (LLMs) requires vast
amounts of clean and diverse text data. While the open development of large
high-quality English pre-training datasets has seen substantial recent
progress, training performant multilingual LLMs remains a challenge, in large
part due to the inherent difficulty of tailoring filtering and deduplication
pipelines to a large number of languages. In this work, we introduce a new
pre-training dataset curation pipeline based on FineWeb that can be
automatically adapted to support any language. We extensively ablate our
pipeline design choices on a set of nine diverse languages, guided by a set of
meaningful and informative evaluation tasks that were chosen through a novel
selection process based on measurable criteria. Ultimately, we show that our
pipeline can be used to create non-English corpora that produce more performant
models than prior datasets. We additionally introduce a straightforward and
principled approach to rebalance datasets that takes into consideration both
duplication count and quality, providing an additional performance uplift.
Finally, we scale our pipeline to over 1000 languages using almost 100 Common
Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)
multilingual dataset which we release along with our pipeline, training, and
evaluation codebases.

</details>


### [9] [KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model](https://arxiv.org/abs/2506.20923)
*Xinping Zhao,Xinshuo Hu,Zifei Shan,Shouzheng Huang,Yao Zhou,Zetian Sun,Zhenyu Liu,Dongfang Li,Xinyuan Wei,Qian Chen,Youcheng Pan,Yang Xiang,Meishan Zhang,Haofen Wang,Jun Yu,Baotian Hu,Min Zhang*

Main category: cs.CL

> 本文提出了KaLM-Embedding-V2模型，该模型在各种文本嵌入任务中表现出色，特点是多功能和参数量少于10亿。

<details>
  <summary>Details</summary>

**Motivation:** 我们旨在创造一种多功能且紧凑的文本嵌入模型，它能在各种文本嵌入任务中保持高性能，同时使用较少的参数（小于10亿）。

**Method:** 我们提出了KaLM-Embedding-V2，一个多功能且紧凑的嵌入模型，通过利用先进的训练技术和数据在通用文本嵌入任务中表现出色。我们主要的创新包括：(1) 为了更好地使架构与表示学习保持一致，我们移除了因果注意力掩码，采用了一种全双向Transformer，并通过简单的均值池化来生成固定长度的嵌入；(2) 我们采用了多阶段训练流程：(i) 在大规模弱监督的开源语料库上进行预训练；(ii) 在高质量检索和非检索数据集上进行微调；(iii) 模型soup参数平均以增强鲁棒性。此外，我们引入了一种焦点样式的重新加权机制，集中学习困难的样本，并采用了在线难负样本混合策略，以持续丰富难负样本而不需昂贵的离线挖掘；(3) 我们收集了超过20类预训练数据和100类微调数据，以提升嵌入模型的性能和泛化能力。

**Result:** 在大规模文本嵌入基准（MTEB）的中文和英文评价中，我们的模型在与同等规模的模型相比时表现出显著优势，并与3倍、14倍、18倍和26倍大大小小的嵌入模型相比，取得了竞争甚至超越的效果。

**Conclusion:** 我们的创新方法和广泛的训练数据使得KaLM-Embedding-V2模型在通用文本嵌入任务中表现出色，尤其是在参数量较小的情况下。

**Abstract:** In this paper, we propose KaLM-Embedding-V2, a versatile and compact
embedding model, which achieves impressive performance in general-purpose text
embedding tasks by leveraging superior training techniques and data. Our key
innovations include: (1) To better align the architecture with representation
learning, we remove the causal attention mask and adopt a fully bidirectional
transformer with simple yet effective mean-pooling to produce fixed-length
embeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on
large-scale weakly supervised open-source corpora; (ii) fine-tuning on
high-quality retrieval and non-retrieval datasets; and (iii) model-soup
parameter averaging for robust generalization. Besides, we introduce a
focal-style reweighting mechanism that concentrates learning on difficult
samples and an online hard-negative mixing strategy to continuously enrich hard
negatives without expensive offline mining; (3) We collect over 20 categories
of data for pre-training and 100 categories of data for fine-tuning, to boost
both the performance and generalization of the embedding model. Extensive
evaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English
show that our model significantly outperforms others of comparable size, and
competes with 3x, 14x, 18x, and 26x larger embedding models, setting a new
standard for a versatile and compact embedding model with less than 1B
parameters.

</details>


### [10] [Can Gradient Descent Simulate Prompting?](https://arxiv.org/abs/2506.20989)
*Eric Zhang,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

> 研究提出了一种元训练语言模型的方法，使得梯度更新可以模仿提示的效果，展示了在适当初始化情况下，梯度下降的高效性。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探究是否可以通过调整模型，使得微调模仿提示的效果，解决模型更新时长期内存成本以及微调不如提示有效的问题。

**Method:** 本研究提出了一种元训练语言模型的方法，该方法利用基于梯度的元学习工具，但使用语言模型自身被提示后的预测作为目标，从而消除了对地面真实标签的需求。通过这种方式，后续的梯度下降训练可以在某种程度上恢复被提示模型的表现。

**Result:** 实验结果显示，适当初始化后，梯度下降具有较强的表达能力，甚至在单次梯度更新后，可以改善“反转诅咒”任务的表现，并回答关于文本段落的问题。

**Conclusion:** 研究提供了长上下文建模的新途径，并为基于梯度的学习概括能力提供了见解。

**Abstract:** There are two primary ways of incorporating new information into a language
model (LM): changing its prompt or changing its parameters, e.g. via
fine-tuning. Parameter updates incur no long-term storage cost for model
changes. However, for many model updates, prompting is significantly more
effective: prompted models can generalize robustly from single examples and
draw logical inferences that do not occur under standard fine-tuning. Can
models be modified so that fine-tuning does emulate prompting? This paper
describes a method for meta-training LMs such that gradient updates emulate the
effects of conditioning on new information. Our approach uses tools from
gradient-based meta-learning but uses an LM's own prompted predictions as
targets, eliminating the need for ground-truth labels. Subsequent gradient
descent training recovers some (and occasionally all) of prompted model
performance -- showing improvement on the ``reversal curse'' tasks, and
answering questions about text passages after a single gradient update. These
results suggest that, with appropriate initialization, gradient descent can be
surprisingly expressive. Our results suggest new avenues for long-context
modeling and offer insight into the generalization capabilities of
gradient-based learning.

</details>


### [11] [SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control](https://arxiv.org/abs/2506.20993)
*Adithya Chittem,Aishna Shrivastava,Sai Tarun Pendela,Jagat Sesh Challa,Dhruv Kumar*

Main category: cs.CL

> 本研究通过采用16PF模型和开发SAC框架解决了LLMs性格建模的两大限制，实现了对16种不同性格特质的细腻控，并表明多维性格结构的存在，推动了人机交互的发展。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是解决现有LLM性格建模的两个主要限制：依赖提供粗略性格维度的五大性格模型（OCEAN框架），以及缺乏控制性格强度的机制。

**Method:** 本研究扩展了机器性格量表(MPI)，从原本使用的五大性格特质模型改用16PF模型，以实现对16种不同特质的表达控制。同时，开发了一个结构化的框架，称为特定属性控制(SAC)，用于评估和动态诱导LLM的性格强度。方法引入了基于形容词的语义锚定来指导性格强度的表达，并结合了五个强度因素（频率、深度、阈值、努力、意愿）的行为问题。

**Result:** 实验表明，将强度建模为连续谱比二进制特质切换能产生更为一致和可控制的性格表达。此外，目标特质强度的变化系统地影响了与之紧密相关的特质，心理上是连贯的，表明LLMs内化了多维性格结构，而不仅仅是孤立地处理特质。

**Conclusion:** 该研究为控制和细腻的人机交互开辟了新的途径，尤其是在医疗保健、教育和面试等领域，使我们更接近真正类人的社交机器。

**Abstract:** Large language models (LLMs) have gained significant traction across a wide
range of fields in recent years. There is also a growing expectation for them
to display human-like personalities during interactions. To meet this
expectation, numerous studies have proposed methods for modelling LLM
personalities through psychometric evaluations. However, most existing models
face two major limitations: they rely on the Big Five (OCEAN) framework, which
only provides coarse personality dimensions, and they lack mechanisms for
controlling trait intensity. In this paper, we address this gap by extending
the Machine Personality Inventory (MPI), which originally used the Big Five
model, to incorporate the 16 Personality Factor (16PF) model, allowing
expressive control over sixteen distinct traits. We also developed a structured
framework known as Specific Attribute Control (SAC) for evaluating and
dynamically inducing trait intensity in LLMs. Our method introduces
adjective-based semantic anchoring to guide trait intensity expression and
leverages behavioural questions across five intensity factors:
\textit{Frequency}, \textit{Depth}, \textit{Threshold}, \textit{Effort}, and
\textit{Willingness}. Through experimentation, we find that modelling intensity
as a continuous spectrum yields substantially more consistent and controllable
personality expression compared to binary trait toggling. Moreover, we observe
that changes in target trait intensity systematically influence closely related
traits in psychologically coherent directions, suggesting that LLMs internalize
multi-dimensional personality structures rather than treating traits in
isolation. Our work opens new pathways for controlled and nuanced human-machine
interactions in domains such as healthcare, education, and interviewing
processes, bringing us one step closer to truly human-like social machines.

</details>


### [12] [Large Language Models Acing Chartered Accountancy](https://arxiv.org/abs/2506.21031)
*Jatin Gupta,Akhil Sharma,Saransh Singhania,Mohammad Adnan,Sakshi Deo,Ali Imam Abidi,Keshav Gupta*

Main category: cs.CL

> 研究引入CA-Ben基准评估了六种主要LLMs在印度财务实践中的表现，凸显了其在法律推理方面的能力及遇到的挑战，建议未来进行方法改进。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在解决在印度金融背景下，大型语言模型（LLMs）捕获和应用特定领域金融知识的有效性问题，填补这一领域的空白。

**Method:** 研究设计了CA-Ben基准测试，该测试基于印度注册会计师协会（ICAI）的严格考试，涵盖了注册会计师课程的基础、中级和高级阶段结构化问题-答案数据集。使用标准协议对六种主要LLMs进行了评估。

**Result:** 研究通过引入CA-Ben基准评估了六种大型语言模型（包括GPT 4o和Claude 3.5 Sonnet等）在印度财务实践中的表现。结果显示在概念和法律推理方面表现较好，但在数值计算和法律解释方面存在困难。这表明了模型的优势和局限，未来可以通过混合推理和检索增强生成方法进行改进。

**Conclusion:** 研究结果强调了当前LLMs在金融领域应用中的强项和局限性，为未来通过混合逻辑推理和检索增强生成举措的改进提供了一定方向，特别是针对定量分析和准确法律解读的提升。

**Abstract:** Advanced intelligent systems, particularly Large Language Models (LLMs), are
significantly reshaping financial practices through advancements in Natural
Language Processing (NLP). However, the extent to which these models
effectively capture and apply domain-specific financial knowledge remains
uncertain. Addressing a critical gap in the expansive Indian financial context,
this paper introduces CA-Ben, a Chartered Accountancy benchmark specifically
designed to evaluate the financial, legal, and quantitative reasoning
capabilities of LLMs. CA-Ben comprises structured question-answer datasets
derived from the rigorous examinations conducted by the Institute of Chartered
Accountants of India (ICAI), spanning foundational, intermediate, and advanced
CA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1
405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated
using standardized protocols. Results indicate variations in performance, with
Claude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and
legal reasoning. Notable challenges emerged in numerical computations and legal
interpretations. The findings emphasize the strengths and limitations of
current LLMs, suggesting future improvements through hybrid reasoning and
retrieval-augmented generation methods, particularly for quantitative analysis
and accurate legal interpretation.

</details>


### [13] [A Semi-supervised Scalable Unified Framework for E-commerce Query Classification](https://arxiv.org/abs/2506.21049)
*Chunyuan Yuan,Chong Zhang,Zheng Fang,Ming Pang,Xue Jiang,Changping Peng,Zhangang Lin,Ching Law*

Main category: cs.CL

> 本文提出了一种新的半监督可扩展统一框架（SSUF），解决了电子商务查询分类中存在的信息不足、依赖后验标签及子任务缺乏统一框架的问题。实验显示，该框架优于现有先进模型。

<details>
  <summary>Details</summary>

**Motivation:** 电子商务查询通常较短且缺乏上下文，标签间的相关信息无法使用，导致建模时先验信息不足。现有的工业查询分类方法大多依赖用户的后置点击行为来构建训练样本，这会导致一个马太效应的恶性循环。此外，查询分类的子任务缺乏统一的框架，导致算法优化效率低下。因此，有必要提出一种新的解决方案来应对这些问题。

**Method:** 本文提出了一种名为半监督可扩展统一框架（SSUF）的新方法，该框架包含多个增强模块以统一查询分类任务。知识增强模块利用世界知识增强查询表示，解决查询信息不足的问题。标签增强模块利用标签语义和半监督信号减少对后验标签的依赖。结构增强模块依据复杂的标签关系增强标签表示。每个模块高度可插拔，可根据每个子任务的需求添加或移除输入特征。

**Result:** 作者通过广泛的线上线下A/B测试实验验证了SSUF的有效性，实验结果表明，SSUF显著优于现有的先进模型。

**Conclusion:** 本文提出的SSUF框架通过多个增强模块统一了查询分类任务，显著提高了模型性能，解决了现有方法中存在的问题。

**Abstract:** Query classification, including multiple subtasks such as intent and category
prediction, is vital to e-commerce applications. E-commerce queries are usually
short and lack context, and the information between labels cannot be used,
resulting in insufficient prior information for modeling. Most existing
industrial query classification methods rely on users' posterior click behavior
to construct training samples, resulting in a Matthew vicious cycle.
Furthermore, the subtasks of query classification lack a unified framework,
leading to low efficiency for algorithm optimization.
  In this paper, we propose a novel Semi-supervised Scalable Unified Framework
(SSUF), containing multiple enhanced modules to unify the query classification
tasks. The knowledge-enhanced module uses world knowledge to enhance query
representations and solve the problem of insufficient query information. The
label-enhanced module uses label semantics and semi-supervised signals to
reduce the dependence on posterior labels. The structure-enhanced module
enhances the label representation based on the complex label relations. Each
module is highly pluggable, and input features can be added or removed as
needed according to each subtask. We conduct extensive offline and online A/B
experiments, and the results show that SSUF significantly outperforms the
state-of-the-art models.

</details>


### [14] [MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for Conversational Stance Detection](https://arxiv.org/abs/2506.21053)
*Fuqiang Niu,Genan Dai,Yisha Lu,Jiayu Liao,Xiang Li,Hu Huang,Bowen Zhang*

Main category: cs.CL

> The paper introduces MT2-CSD, a new dataset for multi-target, multi-turn conversational stance detection, and proposes LLM-CRAN, a model that enhances conversational understanding through reasoning capabilities of large language models.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitations of existing datasets and models in capturing the complexities of multi-party discussions in social media for stance detection.

**Method:** This paper proposes the Large Language model enhanced Conversational Relational Attention Network (LLM-CRAN), which uses the reasoning capabilities of large language models to improve understanding in multi-target, multi-turn conversations.

**Result:** Experiments on the new dataset show that LLM-CRAN significantly outperforms strong baseline models.

**Conclusion:** The introduction of MT2-CSD and the performance enhancements provided by LLM-CRAN represent significant advancements in the field of conversational stance detection.

**Abstract:** In the realm of contemporary social media, automatic stance detection is
pivotal for opinion mining, as it synthesizes and examines user perspectives on
contentious topics to uncover prevailing trends and sentiments. Traditional
stance detection research often targets individual instances, thereby limiting
its capacity to model multi-party discussions typical in real social media
scenarios. This shortcoming largely stems from the scarcity of datasets that
authentically capture the dynamics of social media interactions, hindering
advancements in conversational stance detection. In this paper, we introduce
MT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational
stance detection. To the best of our knowledge, MT2-CSD is the largest dataset
available for this purpose, comprising 24,457 annotated instances and
exhibiting the greatest conversational depth, thereby presenting new challenges
for stance detection. To address these challenges, we propose the Large
Language model enhanced Conversational Relational Attention Network (LLM-CRAN),
which exploits the reasoning capabilities of LLMs to improve conversational
understanding. We conduct extensive experiments to evaluate the efficacy of
LLM-CRAN on the MT2-CSD dataset. The experimental results indicate that
LLM-CRAN significantly outperforms strong baseline models in the task of
conversational stance detection.

</details>


### [15] [DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning](https://arxiv.org/abs/2506.21096)
*Kang He,Yuzhe Ding. Haining Wang,Fei Li,Chong Teng,Donghong Ji*

Main category: cs.CL

> The paper introduces DALR, a dual-level alignment learning method for multimodal sentence representation which addresses cross-modal misalignment and intra-modal semantic divergence, demonstrating superior performance across evaluation tasks compared to existing methods.

<details>
  <summary>Details</summary>

**Motivation:** There is a need to improve the quality of multimodal sentence representations by addressing the issues of cross-modal misalignment bias and intra-modal semantic divergence that current methods struggle with.

**Method:** Previous methods focus on coarse alignment between images and text, which often leads to significant degradation in sentence representation due to cross-modal misalignment bias and intra-modal semantic divergence. DALR addresses these issues through a consistency learning module and a ranking distillation mechanism, enhancing the quality of multimodal sentence representations.

**Result:** Experiments on STS and transfer tasks prove the effectiveness of DALR, showing its consistent superiority over state-of-the-art approaches.

**Conclusion:** The proposed DALR method significantly improves the quality of multimodal sentence representations by addressing cross-modal misalignment and intra-modal semantic divergence, outperforming existing methods across various evaluation tasks.

**Abstract:** Previous multimodal sentence representation learning methods have achieved
impressive performance. However, most approaches focus on aligning images and
text at a coarse level, facing two critical challenges:cross-modal misalignment
bias and intra-modal semantic divergence, which significantly degrade sentence
representation quality. To address these challenges, we propose DALR
(Dual-level Alignment Learning for Multimodal Sentence Representation). For
cross-modal alignment, we propose a consistency learning module that softens
negative samples and utilizes semantic similarity from an auxiliary task to
achieve fine-grained cross-modal alignment. Additionally, we contend that
sentence relationships go beyond binary positive-negative labels, exhibiting a
more intricate ranking structure. To better capture these relationships and
enhance representation quality, we integrate ranking distillation with global
intra-modal alignment learning. Comprehensive experiments on semantic textual
similarity (STS) and transfer (TR) tasks validate the effectiveness of our
approach, consistently demonstrating its superiority over state-of-the-art
baselines.

</details>


### [16] [ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry](https://arxiv.org/abs/2506.21098)
*Qinwen Chen,Wenbiao Tao,Zhiwei Zhu,Mingfan Xi,Liangzhong Guo,Yuan Wang,Wei Wang,Yunshi Lan*

Main category: cs.CL

> ComRAG is an effective retrieval-augmented generation model for CQA platforms, incorporating historical and external knowledge with memory mechanisms for efficient performance.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to overcome the limitations of current methods by effectively integrating external knowledge and historical QA interactions in a real-time industrial CQA context.

**Method:** The paper introduces ComRAG, a retrieval-augmented generation framework for CQA that merges static domain knowledge with dynamic historical QA pairs using a centroid-based memory mechanism, tailored for retrieval, generation, and storage efficiency.

**Result:** ComRAG demonstrates significant improvements in vector similarity, latency reduction, and efficient memory usage compared to baselines.

**Conclusion:** ComRAG outperforms existing methods by improving vector similarity up to 25.9%, decreasing latency by 8.7% to 23.3%, and reducing chunk growth from 20.23% to 2.06% across three industrial CQA datasets.

**Abstract:** Community Question Answering (CQA) platforms can be deemed as important
knowledge bases in community, but effectively leveraging historical
interactions and domain knowledge in real-time remains a challenge. Existing
methods often underutilize external knowledge, fail to incorporate dynamic
historical QA context, or lack memory mechanisms suited for industrial
deployment. We propose ComRAG, a retrieval-augmented generation framework for
real-time industrial CQA that integrates static knowledge with dynamic
historical QA pairs via a centroid-based memory mechanism designed for
retrieval, generation, and efficient storage. Evaluated on three industrial CQA
datasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%
improvement in vector similarity, reducing latency by 8.7% to 23.3%, and
lowering chunk growth from 20.23% to 2.06% over iterations.

</details>


### [17] [Progtuning: Progressive Fine-tuning Framework for Transformer-based Language Models](https://arxiv.org/abs/2506.21119)
*Xiaoshuang Ji,Zhendong Zhao,Xiaojun Chen,Xin Zhao,Zeyao Liu*

Main category: cs.CL

> 论文提出了Progtuning微调框架，该框架结合渐进学习，通过根据贡献逐步减少更新的Transformer块数量，减少了计算资源的消耗。

<details>
  <summary>Details</summary>

**Motivation:** 现有的微调方法及高效的参数微调方法忽视了Transformer块之间贡献的不平等，导致计算资源分配极不高效。

**Method:** 提出了Progtuning框架，结合渐进学习和Transformer模型微调，通过根据贡献逐步减少更新的Transformer块数量来优化资源分配。

**Result:** Progtuning框架将更新参数的数量减少了大约25%，同时保持了具有竞争力的性能，并且在各种适配场景下都表现出色。

**Conclusion:** Progtuning不仅优化了资源分配，减少了更新参数的数量，还能兼容高效的参数微调方法，具有很强的适应性。

**Abstract:** Fine-tuning is a promising technique for leveraging Transformer-based
language models in downstream tasks. As model sizes continue to grow, updating
all model parameters becomes increasingly costly. Parameter-efficient
fine-tuning methods effectively address this issue by selectively updating a
small subset of parameters. However, fine-tuning and most existing
parameter-efficient fine-tuning methods require updating the same number of
parameters as the initial size, ignoring the unequal contribution across
Transformer blocks and leading to extremely inefficient allocation of computing
resources. In this paper, we propose Progtuning, the novel fine-tuning
framework combined with progressive learning for Transformer-based language
models. Specifically, Progtuning progressively reduces the number of updated
transformer blocks based on the contribution. Remarkably, Progtuning optimizes
resource allocation and reduces the number of updated parameters by
approximately 25\%, while still maintaining competitive performance. And it
also exhibits high adaptability with parameter-efficient fine-tuning methods,
demonstrating excellent performance across various adaptation scenarios.

</details>


### [18] [Compressed and Smooth Latent Space for Text Diffusion Modeling](https://arxiv.org/abs/2506.21170)
*Viacheslav Meshchaninov,Egor Chimbulatov,Alexander Shabalin,Aleksandr Abramov,Dmitry Vetrov*

Main category: cs.CL

> 本文提出了一种全新方法Cosmos，用于文本生成，这种方法通过学习一个强大的压缩平滑潜在空间来进行操作，实验证明了其在文本表示压缩和生成质量上的优越性，同时加快了生成速度。

<details>
  <summary>Details</summary>

**Motivation:** 自回归语言模型在现代文本生成中占据主导地位，但它们的顺序特性带来了根本性的限制：解码速度慢，维持全局一致性具有挑战性。扩散模型提供了一种有前途的替代方案，但它们在文本生成中的应用受制于令牌级表示的高维度。

**Method:** 我们引入了Cosmos，这是一种全新的文本生成方法，它完全在专门为扩散模型设计的压缩平滑潜在空间中操作。这个空间通过一个同时训练令牌级重建和对齐预训练语言编码器冻结激活的自编码器来学习，这提供了有力的语义定位，并使基于扰动的增强成为可能。

**Result:** 实验证明，文本表示可以压缩8倍，同时保持与令牌级扩散模型相当的生成质量。此外，增加潜在序列长度使Cosmos能够超越扩散式和自回归基线。在四个多样化的生成任务上（包括故事生成、问题生成、总结和解毒）评估了Cosmos，并将其与各种生成范式进行了比较。Cosmos在生成质量方面达到了相当或更高的水平，并且推理速度提高了2倍以上。

**Conclusion:** 相较于自回归模型和扩散模型，Cosmos在文本生成质量上达到或超过了现有模型，并且在推理效率上提高了至少一倍，这使得它成为一种更为高效的文本生成工具。

**Abstract:** Autoregressive language models dominate modern text generation, yet their
sequential nature introduces fundamental limitations: decoding is slow, and
maintaining global coherence remains challenging. Diffusion models offer a
promising alternative by enabling parallel generation and flexible control;
however, their application to text generation is hindered by the high
dimensionality of token-level representations. We introduce Cosmos, a novel
approach to text generation that operates entirely in a compressed, smooth
latent space tailored specifically for diffusion. This space is learned using
an autoencoder trained simultaneously for token-level reconstruction and
alignment with frozen activations from a pretrained language encoder, providing
robust semantic grounding and enabling effective perturbation-based
augmentations. Empirically, we demonstrate that text representations can be
compressed by $8\times$ while maintaining generation quality comparable to
token-level diffusion models. Furthermore, increasing the latent sequence
length allows Cosmos to surpass both diffusion-based and autoregressive
baselines. We evaluate Cosmos on four diverse generative tasks including story
generation, question generation, summarization, and detoxification and compare
it with various generative paradigms. Cosmos achieves comparable or superior
generation quality while offering more than $2\times$ faster inference.

</details>


### [19] [Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks](https://arxiv.org/abs/2506.21182)
*Isaac Chung,Imene Kerboua,Marton Kardos,Roman Solomatin,Kenneth Enevoldsen*

Main category: cs.CL

> The paper covers the engineering strategies to uphold the quality, generalizability, and usability of the Massive Text Embedding Benchmark (MTEB) for assessing text embedding models.

<details>
  <summary>Details</summary>

**Motivation:** The aim is to ensure the MTEB benchmark's effectiveness and relevance by employing advanced engineering practices that guarantee reproducibility and allow for ease of use and future expansion.

**Method:** Structure

**Result:** {
  "tldr": "The paper discusses the engineering efforts to maintain the Massive Text Embedding Benchmark (MTEB) for reproducibility and extensibility, including continuous integration pipelines and community contributions.",
  "motivation": "The goal is to ensure that MTEB remains a high-quality, comprehensive, and relevant benchmark for evaluating text embedding models by enhancing reproducibility and usability.",
  "method": "The authors describe the use of continuous integration pipelines to validate dataset integrity, automate tests, and evaluate benchmark performance. They also detail management of community contributions for extending the benchmark.",
  "result": "MTEB has been effectively maintained and expanded with robust engineering practices, improving its generalizability and relevance in the field of text embedding evaluation.",
  "conclusion": "The engineering practices outlined in the paper provide valuable lessons for maintaining and improving the reproducibility and usability of machine learning evaluation frameworks like MTEB.")

**Conclusion:** The strategies employed in maintaining MTEB highlight lessons for others involved in the upkeep of machine learning benchmarks.

**Abstract:** The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation
platform for text embedding models. While previous work has established the
core benchmark methodology, this paper focuses on the engineering aspects that
ensure MTEB's continued reproducibility and extensibility. We present our
approach to maintaining robust continuous integration pipelines that validate
dataset integrity, automate test execution, and assess benchmark results'
generalizability. We detail the design choices that collectively enhance
reproducibility and usability. Furthermore, we discuss our strategies for
handling community contributions and extending the benchmark with new tasks and
datasets. These engineering practices have been instrumental in scaling MTEB to
become more comprehensive while maintaining quality and, ultimately, relevance
to the field. Our experiences offer valuable insights for benchmark maintainers
facing similar challenges in ensuring reproducibility and usability in machine
learning evaluation frameworks. The MTEB repository is available at:
https://github.com/embeddings-benchmark/mteb

</details>


### [20] [Prompt-Guided Turn-Taking Prediction](https://arxiv.org/abs/2506.21191)
*Koji Inoue,Mikey Elmers,Yahui Fu,Zi Haur Pang,Divesh Lala,Keiko Ochi,Tatsuya Kawahara*

Main category: cs.CL

> 本研究提出了一种可以根据文本提示动态调整换位预测的新模型，该模型基于变压器式的语音活动投影模型并加入了文本提示嵌入，实验表明其提高了预测精度并实现了换位时间的行为变化。

<details>
  <summary>Details</summary>

**Motivation:** 换位预测模型是口语对话系统和会话机器人的重要组成部分。最近的方法利用变压器式的架构来连续进行实时的语音活动预测。这项研究的动机是为了通过直观明确的指令控制来改进换位预测。

**Method:** 本研究提出了一种新的模型，它可以通过文本提示来动态控制换位预测。该模型基于变压器式的语音活动投影（VAP）模型，并将文本提示嵌入到通道式变压器和跨通道变压器中。

**Result:** 实验结果表明，该模型提高了预测准确性，有效地根据文本提示变化换位时间行为。

**Conclusion:** 研究表明，通过提出的新模型，可以通过文本提示动态调整换位预测，增强了模型的可控制性和适应性。

**Abstract:** Turn-taking prediction models are essential components in spoken dialogue
systems and conversational robots. Recent approaches leverage transformer-based
architectures to predict speech activity continuously and in real-time. In this
study, we propose a novel model that enables turn-taking prediction to be
dynamically controlled via textual prompts. This approach allows intuitive and
explicit control through instructions such as "faster" or "calmer" adapting
dynamically to conversational partners and contexts. The proposed model builds
upon a transformer-based voice activity projection (VAP) model, incorporating
textual prompt embeddings into both channel-wise transformers and a
cross-channel transformer. We evaluated the feasibility of our approach using
over 950 hours of human-human spoken dialogue data. Since textual prompt data
for the proposed approach was not available in existing datasets, we utilized a
large language model (LLM) to generate synthetic prompt sentences. Experimental
results demonstrated that the proposed model improved prediction accuracy and
effectively varied turn-taking timing behaviors according to the textual
prompts.

</details>


### [21] [Enhancing Automatic Term Extraction with Large Language Models via Syntactic Retrieval](https://arxiv.org/abs/2506.21222)
*Yongchan Chun,Minhyuk Kim,Dongjun Kim,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

> 研究提出了一种句法检索方法，用于改进基于大型语言模型的自动术语提取，使其在少样本情况下更具优势。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型在许多NLP任务中取得了显著进展，但其在自动术语提取中的潜力尚未得到充分研究。

**Method:** 本论文提出了一种基于检索的提示策略，该策略在少样本设置中根据句法相似性而不是语义相似性选择实例。

**Result:** 实验结果表明，基于句法的检索方法在三个专门的自动术语提取基准测试中提高了F1分数。

**Conclusion:** 研究结果强调了句法线索在适应LLMs进行术语提取任务时的重要性。

**Abstract:** Automatic Term Extraction (ATE) identifies domain-specific expressions that
are crucial for downstream tasks such as machine translation and information
retrieval. Although large language models (LLMs) have significantly advanced
various NLP tasks, their potential for ATE has scarcely been examined. We
propose a retrieval-based prompting strategy that, in the few-shot setting,
selects demonstrations according to \emph{syntactic} rather than semantic
similarity. This syntactic retrieval method is domain-agnostic and provides
more reliable guidance for capturing term boundaries. We evaluate the approach
in both in-domain and cross-domain settings, analyzing how lexical overlap
between the query sentence and its retrieved examples affects performance.
Experiments on three specialized ATE benchmarks show that syntactic retrieval
improves F1-score. These findings highlight the importance of syntactic cues
when adapting LLMs to terminology-extraction tasks.

</details>


### [22] [Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents](https://arxiv.org/abs/2506.21252)
*Tianyi Men,Zhuoran Jin,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

> 研究提出一个评估MLLMs奖励建模能力的基准—Agent-RewardBench，解决了由于缺乏外部反馈导致的多模态代理自我纠正和泛化问题。

<details>
  <summary>Details</summary>

**Motivation:** 由于无法获取外部反馈，多模态代理在自我纠正和泛化方面存在困难。研究提出构建一个针对代理的奖励评估基准的迫切需求。

**Method:** 提出Agent-RewardBench，一个用于评估MLLMs奖励建模能力的基准，该基准具有多重维度评估、逐步骤奖励评估以及适当的难度控制和数据验证三个关键特点。

**Result:** 实验表明，即使是先进的多模态模型在奖励建模方面的表现也有限，突显了需要专门训练的需求。

**Conclusion:** 需要一个针对代理的奖励建模训练基准，以克服现有模型的局限性，提升其在任务中的表现。

**Abstract:** As Multimodal Large Language Models (MLLMs) advance, multimodal agents show
promise in real-world tasks like web navigation and embodied intelligence.
However, due to limitations in a lack of external feedback, these agents
struggle with self-correction and generalization. A promising approach is to
use reward models as external feedback, but there is no clear on how to select
reward models for agents. Thus, there is an urgent need to build a reward bench
targeted at agents. To address these challenges, we propose Agent-RewardBench,
a benchmark designed to evaluate reward modeling ability in MLLMs. The
benchmark is characterized by three key features: (1) Multiple dimensions and
real-world agent scenarios evaluation. It covers perception, planning, and
safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the
assessment of agent capabilities at the individual steps of a task, providing a
more granular view of performance during the planning process; and (3)
Appropriately difficulty and high-quality. We carefully sample from 10 diverse
models, difficulty control to maintain task challenges, and manual verification
to ensure the integrity of the data. Experiments demonstrate that even
state-of-the-art multimodal models show limited performance, highlighting the
need for specialized training in agent reward modeling. Code is available at
github.

</details>


### [23] [Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?](https://arxiv.org/abs/2506.21274)
*Andrea McGlinchey,Peter J Barclay*

Main category: cs.CL

> 本研究对比了Gemini和GPT在生成古典侦探小说风格‘虚假文本’上的能力，并发现尽管Gemini的生成能力有所增强，但目前的检测手段仍有效。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于大型语言模型能够伪造如学术写作、产品评论和政治新闻领域的“虚假文本”，本研究旨在探讨这些模型是否以及如何能够绕过检测器，进而分析虚假文本检测是否可能实现稳定效果。

**Method:** 本研究通过对比Gemini和GPT在产生古典侦探小说风格的‘虚假文本’能力上的差异，来评估统计分类器检测此类文本的能力。

**Result:** 研究发现，在版本更新0.5的情况下，Gemini生成更具欺骗性文本的能力有所提升，而GPT则没有。这表明，尽管模型变得越来越大，但检测虚假文本仍然是可行的。然而，新型模型架构可能会提升其欺骗性。

**Conclusion:** 研究表明，即使模型参数和训练数据量不断增加，使用相对简单的分类器仍能够以适度的资源实现良好的检测准确性，暗示检测虚假文本仍然可能保持可靠性。

**Abstract:** Large language models can produce convincing "fake text" in domains such as
academic writing, product reviews, and political news. Many approaches have
been investigated for the detection of artificially generated text. While this
may seem to presage an endless "arms race", we note that newer LLMs use ever
more parameters, training data, and energy, while relatively simple classifiers
demonstrate a good level of detection accuracy with modest resources. To
approach the question of whether the models' ability to beat the detectors may
therefore reach a plateau, we examine the ability of statistical classifiers to
identify "fake text" in the style of classical detective fiction. Over a 0.5
version increase, we found that Gemini showed an increased ability to generate
deceptive text, while GPT did not. This suggests that reliable detection of
fake text may remain feasible even for ever-larger models, though new model
architectures may improve their deceptiveness

</details>


### [24] [Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning](https://arxiv.org/abs/2506.21285)
*Xin Xu,Tianhao Chen,Fan Zhang,Wanlong Liu,Pengxiang Li,Ajay Kumar Jaiswal,Yuchen Yan,Jishan Hu,Yang Wang,Hao Chen,Shiwei Liu,Shizhe Diao,Can Yang,Lu Yin*

Main category: cs.CL

> 本文介绍了一种名为Double-Checker的框架，通过自我批判和迭代改进，提高了慢思考大型语言模型的推理能力。通过训练，Double-Checker显著提升了模型在推理基准测试中的表现，尤其在挑战性较高的AIME基准测试中的准确率有显著提升。

<details>
  <summary>Details</summary>

**Motivation:** 尽管慢思考大型语言模型展示了解决问题时的“灵光一闪”时刻，但在产生信息性评论和改进先前解决方案方面能力有限。作者旨在通过增强模型自我批判和迭代改进前一个解决方案的能力来提升其推理能力。

**Method:** 本文提出了Double-Checker框架，该框架通过培训模型在1,730个自我批判实例上进行自我批判和迭代改进，直到它们认为自己的解决方案在自我生成的批判中是正确的。

**Result:** Double-Checker框架在各种推理基准测试中都表现出色，尤其在挑战性较高的AIME基准测试中，通过自我批判迭代，准确率从4.4%显著提升到18.2%。

**Conclusion:** 研究结果表明，通过自我批判和迭代改进，慢思考大型语言模型的推理能力可以得到显著增强。Double-Checker框架为开发更值得信赖和更有效的语言模型提供了有希望的方向。

**Abstract:** While slow-thinking large language models (LLMs) exhibit reflection-like
reasoning, commonly referred to as the "aha moment:, their ability to generate
informative critiques and refine prior solutions remains limited. In this
paper, we introduce Double-Checker, a principled framework designed to enhance
the reasoning capabilities of slow-thinking LLMs by fostering explicit
self-critique and iterative refinement of their previous solutions. By
fine-tuning on our curated 1,730 self-critical instances, Double-Checker
empowers long-CoT LLMs to iteratively critique and refine their outputs during
inference until they evaluate their solutions as correct under self-generated
critiques. We validate the efficacy of Double-Checker across a comprehensive
suite of reasoning benchmarks, demonstrating that iterative self-critique
significantly enhances the reasoning capabilities of long-CoT LLMs. Notably,
our Double-Checker increases the pass@1 performance on challenging AIME
benchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These
results highlight a promising direction for developing more trustworthy and
effective LLMs capable of structured self-critique.

</details>


### [25] [Small Encoders Can Rival Large Decoders in Detecting Groundedness](https://arxiv.org/abs/2506.21288)
*Istabrak Abbes,Gabriele Prato,Quentin Fournier,Fernando Rodriguez,Alaa Boukhary,Adam Elwood,Sarath Chandar*

Main category: cs.CL

> 本研究使用轻量级编码器模型提升查询依据性检测的效率，相比大型语言模型能显著减少推理延迟。

<details>
  <summary>Details</summary>

**Motivation:** 减少由于大型语言模型在上下文信息缺失时进行无根据猜测，带来的时间和资源浪费，通过提前检测查询的依据性来提高输出的可靠性和一致性。

**Method:** 本研究通过使用轻量级的任务特定编码模型（如RoBERTa和NomicBERT），对经过精心策划的数据集进行微调来实现准确的groundedness检测，这些模型能够以比大型语言模型少得多的推理延迟达到几乎相同的准确率。

**Result:** 研究结果表明，轻量级编码模型可以在减少推理时间的同时，达到与状态-of-the-art大型语言模型相当的groundedness检测准确率。

**Conclusion:** 轻量级、特定任务的编码模型在减少资源消耗和推理时间的同时，能够有效检测查询是否依据上下文，从而避免大型语言模型在上下文信息不足时进行无根据猜测。

**Abstract:** Augmenting large language models (LLMs) with external context significantly
improves their performance in natural language processing (NLP) tasks. However,
LLMs struggle to answer queries reliably when the provided context lacks
information, often resorting to ungrounded speculation or internal knowledge.
Groundedness - generating responses strictly supported by the context - is
essential for ensuring factual consistency and trustworthiness. This study
focuses on detecting whether a given query is grounded in a document provided
in context before the costly answer generation by LLMs. Such a detection
mechanism can significantly reduce both inference time and resource
consumption. We show that lightweight, task specific encoder models such as
RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy
comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in
groundedness detection while reducing inference latency by orders of magnitude.
The code is available at : https://github.com/chandarlab/Hallucinate-less

</details>


### [26] [Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models](https://arxiv.org/abs/2506.21294)
*Bram Willemsen,Gabriel Skantze*

Main category: cs.CL

> 研究发现，即使仅使用文本及适中的LLM和数据集，也可有效地抽取对话中的视觉参照提及，但强调了该任务本为多模态性质，存在单一模式方法的局限性。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探讨仅凭语言环境能否指导检测在对话视觉背景中有视觉可感知参照的提及。

**Method:** 本研究采用了一种仅基于文本的自回归语言模型方法来从视觉化的对话中抽取出指示表达。特别地，通过调整一个预训练的大型语言模型(LLM)，利用下一个词汇预测来标注对话中的提及片段边界，从而实现较为粗略的提及片段标注。

**Result:** 研究结果显示，即便是使用规模适中的LLM、相对较小的数据集和参数高效的微调，纯文本方法也可以有效地完成任务，强调了语言环境在这项任务中的相对重要性。

**Conclusion:** 尽管纯文本方法在任务中发现了一定的效果，但研究者认为此任务本质上是多模态的，需要进一步探讨和使用多元数据和方法来提升效果。

**Abstract:** In this paper, we explore the use of a text-only, autoregressive language
modeling approach for the extraction of referring expressions from visually
grounded dialogue. More specifically, the aim is to investigate the extent to
which the linguistic context alone can inform the detection of mentions that
have a (visually perceivable) referent in the visual context of the
conversation. To this end, we adapt a pretrained large language model (LLM) to
perform a relatively course-grained annotation of mention spans in unfolding
conversations by demarcating mention span boundaries in text via next-token
prediction. Our findings indicate that even when using a moderately sized LLM,
relatively small datasets, and parameter-efficient fine-tuning, a text-only
approach can be effective, highlighting the relative importance of the
linguistic context for this task. Nevertheless, we argue that the task
represents an inherently multimodal problem and discuss limitations fundamental
to unimodal approaches.

</details>


### [27] [Structuralist Approach to AI Literary Criticism: Leveraging Greimas Semiotic Square for Large Language Models](https://arxiv.org/abs/2506.21360)
*Fangzhou Dong,Yifan Zeng,Yingpeng Sang,Hong Shen*

Main category: cs.CL

> This paper proposes GLASS, a structured analytical framework to enhance LLMs' ability to conduct in-depth literary analysis, showing high performance and providing insights into literary engagement.

<details>
  <summary>Details</summary>

**Motivation:** LLMs struggle with providing professional literary criticism for works with profound thoughts and complex narratives, and this research aims to address this issue.

**Method:** GLASS (Greimas Literary Analysis via Semiotic Square) is proposed, a structured analytical framework based on Greimas Semiotic Square (GSS) to help LLMs with in-depth literary analysis.

**Result:** The framework shows high performance, and it is applied to 39 classic works, producing original and high-quality analyses.

**Conclusion:** This research provides an AI-based tool for literary research and education, offering insights into the cognitive mechanisms underlying literary engagement.

**Abstract:** Large Language Models (LLMs) excel in understanding and generating text but
struggle with providing professional literary criticism for works with profound
thoughts and complex narratives. This paper proposes GLASS (Greimas Literary
Analysis via Semiotic Square), a structured analytical framework based on
Greimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth
literary analysis. GLASS facilitates the rapid dissection of narrative
structures and deep meanings in narrative works. We propose the first dataset
for GSS-based literary criticism, featuring detailed analyses of 48 works. Then
we propose quantitative metrics for GSS-based literary criticism using the
LLM-as-a-judge paradigm. Our framework's results, compared with expert
criticism across multiple works and LLMs, show high performance. Finally, we
applied GLASS to 39 classic works, producing original and high-quality analyses
that address existing research gaps. This research provides an AI-based tool
for literary research and education, offering insights into the cognitive
mechanisms underlying literary engagement.

</details>


### [28] [Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21384)
*Guanting Dong,Xiaoxi Li,Yuyao Zhang,Mengjie Deng*

Main category: cs.CL

> Omni-RAG是一个旨在增强RAG系统在开放领域实时应用中稳健性和有效性的新框架，特别解决了处理复杂、含噪声查询的问题。

<details>
  <summary>Details</summary>

**Motivation:** 为了应对现实世界中包含噪音、歧义和多种意图的用户查询对RAG系统的挑战，特别是这些系统在训练或评估时依赖于更干净的数据集。

**Method:** Omni-RAG采用LLM辅助的查询理解，通过三个关键模块来预处理用户输入：(1) 深度查询理解和分解，使用定制提示的LLM来去除噪音（如纠正拼写错误）并将多意图查询分解为结构化的子查询；(2) 意图感知的知识检索，对每个子查询从语料库中检索（如使用OpenSearch从FineWeb获取）并聚合结果；(3) 重排序和生成，一个重排序器（如BGE）优化文档选择，然后由LLM（如Falcon-10B）使用连贯思维提示生成最终回复。

**Result:** 通过实验表明，Omni-RAG能够更有效地处理复杂的用户查询，显著提高了回答的准确性和相关性。

**Conclusion:** Omni-RAG通过LLM辅助的查询理解和处理，能够更好地应对开放领域实时查询的挑战，代表了RAG系统向实际应用需求的实质性进步。

**Abstract:** Real-world live retrieval-augmented generation (RAG) systems face significant
challenges when processing user queries that are often noisy, ambiguous, and
contain multiple intents. While RAG enhances large language models (LLMs) with
external knowledge, current systems typically struggle with such complex
inputs, as they are often trained or evaluated on cleaner data. This paper
introduces Omni-RAG, a novel framework designed to improve the robustness and
effectiveness of RAG systems in live, open-domain settings. Omni-RAG employs
LLM-assisted query understanding to preprocess user inputs through three key
modules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs
with tailored prompts to denoise queries (e.g., correcting spelling errors) and
decompose multi-intent queries into structured sub-queries; (2) Intent-Aware
Knowledge Retrieval, which performs retrieval for each sub-query from a corpus
(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking
and Generation, where a reranker (i.e., BGE) refines document selection before
a final response is generated by an LLM (i.e., Falcon-10B) using a
chain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG
capabilities and the demands of real-world applications, such as those
highlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex
and noisy queries.

</details>


### [29] [Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection](https://arxiv.org/abs/2506.21443)
*Ali Şenol,Garima Agrawal,Huan Liu*

Main category: cs.CL

> 提出了一种DK-Enhanced LLM框架，用于在动态平台上检测欺骗性对话，包含域知识集成和概念漂移检测。实验表明，该方法具有高准确性及其在高风险NLP应用中的改进效果。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机是解决在动态平台上检测欺骗性对话的日益挑战性问题，尤其是由于语言模式的演变和概念漂移所导致的问题，这些问题使准确分类变得困难。

**Method:** 提出了一种基于领域知识（DK）的大型语言模型（LLM）框架，用于欺诈和概念漂移检测。该架构包括三个主要组件：（1）DK-LLM模块用于检测虚假或欺骗性对话；（2）漂移检测单元（OCDD）用于判断语义漂移是否发生；（3）另一个DK-LLM模块用于将漂移分类为良性或欺诈性的。

**Result:** 实验结果表明，该系统能够在高准确率下检测虚假对话，并有效地分类漂移的性质。基于LLaMA的实现达到了98%的分类准确率。

**Conclusion:** 研究表明，通过结构化提示引导的LLaMA实现表现出了极高的分类准确率，以及在高风险NLP应用中的性能、可解释性和鲁棒性的明显提升。相较于零样本基准的比较研究也展示了该框架在这些领域的优势。

**Abstract:** Detecting deceptive conversations on dynamic platforms is increasingly
difficult due to evolving language patterns and Concept Drift (CD)\-i.e.,
semantic or topical shifts that alter the context or intent of interactions
over time. These shifts can obscure malicious intent or mimic normal dialogue,
making accurate classification challenging. While Large Language Models (LLMs)
show strong performance in natural language tasks, they often struggle with
contextual ambiguity and hallucinations in risk\-sensitive scenarios. To
address these challenges, we present a Domain Knowledge (DK)\-Enhanced LLM
framework that integrates pretrained LLMs with structured, task\-specific
insights to perform fraud and concept drift detection. The proposed
architecture consists of three main components: (1) a DK\-LLM module to detect
fake or deceptive conversations; (2) a drift detection unit (OCDD) to determine
whether a semantic shift has occurred; and (3) a second DK\-LLM module to
classify the drift as either benign or fraudulent. We first validate the value
of domain knowledge using a fake review dataset and then apply our full
framework to SEConvo, a multiturn dialogue dataset that includes various types
of fraud and spam attacks. Results show that our system detects fake
conversations with high accuracy and effectively classifies the nature of
drift. Guided by structured prompts, the LLaMA\-based implementation achieves
98\% classification accuracy. Comparative studies against zero\-shot baselines
demonstrate that incorporating domain knowledge and drift awareness
significantly improves performance, interpretability, and robustness in
high\-stakes NLP applications.

</details>


### [30] [Text2Cypher Across Languages: Evaluating Foundational Models Beyond English](https://arxiv.org/abs/2506.21445)
*Makbule Gulcin Ozsoy,William Tai*

Main category: cs.CL

> 该研究创建了一套多语言Text2Cypher测试集，结果表明英文最佳，西班牙语次之，土耳其语最差。提示词翻译影响有限。强调了在多语言查询生成中需要更包容的评估和发展。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在解决当前大多数研究仅关注英文，而对其他语言的评估有限的问题。研究动机是探讨基础语言模型在Text2Cypher任务上多语言性能的表现。

**Method:** 本研究通过将英文问题翻译成西班牙语和土耳其语来创建和发布多语言测试集，同时保留原有的Cypher查询，以实现公平的跨语言比较。评估了多个基础模型，并使用了标准化的提示词和度量标准。

**Result:** 研究结果显示，语言模型在执行Text2Cypher任务时，英文表现最好，其次是西班牙语，而土耳其语表现最差。此外，将任务提示词翻译成西班牙语和土耳其语对评估指标几乎没有影响。

**Conclusion:** 研究结果表明，英文的表现最好，其次是西班牙语，土耳其语最差。这些差异归因于训练数据的可用性和语言特征差异。此外，探索了将任务提示词翻译成西班牙语和土耳其语的影响，结果表明这几乎没有改变评估指标。因此，需要在跨语言查询生成的评估和发展中更加包容。未来的工作将包括模式本地化和跨多种语言的微调。

**Abstract:** Recent advances in large language models have enabled natural language
interfaces that translate user questions into database queries, such as
Text2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database
accessibility, most research today focuses solely on English, with limited
evaluation in other languages. This paper investigates the performance of
foundational LLMs on the Text2Cypher task across multiple languages. We create
and release a multilingual test set by translating English questions into
Spanish and Turkish while preserving the original Cypher queries, enabling fair
cross-lingual comparison. We evaluate multiple foundational models using
standardized prompts and metrics. Our results show a consistent performance
pattern: highest on English, then Spanish, and lowest on Turkish. We attribute
this to differences in training data availability and linguistic
characteristics. Additionally, we explore the impact of translating task
prompts into Spanish and Turkish. Results show little to no change in
evaluation metrics, suggesting prompt translation has minor impact. Our
findings highlight the need for more inclusive evaluation and development in
multilingual query generation. Future work includes schema localization and
fine-tuning across diverse languages.

</details>


### [31] [Aligning Spoken Dialogue Models from User Interactions](https://arxiv.org/abs/2506.21463)
*Anne Wu,Laurent Mazaré,Neil Zeghidour,Alexandre Défossez*

Main category: cs.CL

> 研究提出并验证了一种新的偏好对齐框架，用于改善实时语音对话系统的性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有的偏好学习方法主要关注文本语言模型，这类方法不适合处理实时语音交互中的复杂动态（如打断、插话）和发言人轮次之间无明确分隔的情况。因此，设计一种适应性更好的偏好学习方法是验证这种框架动机。

**Method:** 提出了一种新颖的偏好对齐框架，旨在通过用户交互改善实时对话模型。该框架利用大规模的多轮语音对话数据集（超过150,000个偏好对），这些数据集覆盖了语言内容和时间上下文的变化，并使用离线对齐方法微调全双工自回归语音到语音模型。

**Result:** 实验表明，基于通用对话反馈对模型的微调可以显著提升对话系统在事实正确性、安全性及上下文相关性方面的表现。

**Conclusion:** 这项研究强调了实现实时自然语音对话系统中各种动态因素之间平衡的重要性。

**Abstract:** We propose a novel preference alignment framework for improving spoken
dialogue models on real-time conversations from user interactions. Current
preference learning methods primarily focus on text-based language models, and
are not directly suited to the complexities of real-time speech interactions,
with richer dynamics (e.g. interruption, interjection) and no explicit
segmentation between speaker turns.We create a large-scale dataset of more than
150,000 preference pairs from raw multi-turn speech conversations, annotated
with AI feedback, to cover preferences over both linguistic content and
temporal context variations. We leverage offline alignment methods to finetune
a full-duplex autoregressive speech-to-speech model. Extensive experiments
demonstrate that feedback on generic conversations can be consistently
effective in improving spoken dialogue models to produce more factual, safer
and more contextually aligned interactions. We deploy the finetuned model and
conduct holistic human evaluations to assess the impact beyond single-turn
conversations. Our findings shed light on the importance of a well-calibrated
balance among various dynamics, crucial for natural real-time speech dialogue
systems.

</details>


### [32] [TopK Language Models](https://arxiv.org/abs/2506.21468)
*Ryosuke Takahashi,Tatsuro Inaba,Kentaro Inui,Benjamin Heinzerling*

Main category: cs.CL

> 改进了Transformer架构，集成TopK激活函数，克服了传统SAEs的局限性，增强了语言模型的解释性和可控性。

<details>
  <summary>Details</summary>

**Motivation:** 解决稀疏自编码器（SAEs）在分析和解释基于变压器的语言模型（LMs）的激活空间时遇到的问题，包括内在有效性不足，特性不稳定和难以在不同检查点之间比较特性。

**Method:** 提出了在变压器架构中引入TopK激活函数的修改，使隐藏状态等同于TopK稀疏自编码器的潜特征，从而在无需事后训练的情况下提供了与SAEs相当的可解释性。

**Result:** TopK LMs在维持原模型能力的同时，增强了解释性；可以通过针对性的神经元干预成功引导，方便分析检查点和层级间神经元的形成过程。

**Conclusion:** TopK LMs作为理解和研究语言模型如何学习和表示概念的稳定可靠工具，对该领域未来的研究有重大推进作用。

**Abstract:** Sparse autoencoders (SAEs) have become an important tool for analyzing and
interpreting the activation space of transformer-based language models (LMs).
However, SAEs suffer several shortcomings that diminish their utility and
internal validity. Since SAEs are trained post-hoc, it is unclear if the
failure to discover a particular concept is a failure on the SAE's side or due
to the underlying LM not representing this concept. This problem is exacerbated
by training conditions and architecture choices affecting which features an SAE
learns. When tracing how LMs learn concepts during training, the lack of
feature stability also makes it difficult to compare SAEs features across
different checkpoints. To address these limitations, we introduce a
modification to the transformer architecture that incorporates a TopK
activation function at chosen layers, making the model's hidden states
equivalent to the latent features of a TopK SAE. This approach eliminates the
need for post-hoc training while providing interpretability comparable to SAEs.
The resulting TopK LMs offer a favorable trade-off between model size,
computational efficiency, and interpretability. Despite this simple
architectural change, TopK LMs maintain their original capabilities while
providing robust interpretability benefits. Our experiments demonstrate that
the sparse representations learned by TopK LMs enable successful steering
through targeted neuron interventions and facilitate detailed analysis of
neuron formation processes across checkpoints and layers. These features make
TopK LMs stable and reliable tools for understanding how language models learn
and represent concepts, which we believe will significantly advance future
research on model interpretability and controllability.

</details>


### [33] [Bridging Offline and Online Reinforcement Learning for LLMs](https://arxiv.org/abs/2506.21495)
*Jack Lanchantin,Angelica Chen,Janice Lan,Xian Li,Swarnadeep Saha,Tianlu Wang,Jing Xu,Ping Yu,Weizhe Yuan,Jason E Weston,Sainbayar Sukhbaatar,Ilia Kulikov*

Main category: cs.CL

> 本研究探讨了在不同在线模式下，强化学习方法对大规模语言模型的微调效果，并指出多任务学习能够提升性能。

<details>
  <summary>Details</summary>

**Motivation:** 本文的动机在于理解强化学习方法在不同在线模式下微调大规模语言模型的有效性，以提高模型在可验证和不可验证任务中的表现。

**Method:** 我们研究了强化学习方法在从离线到半在线再到完全在线模式下微调大规模语言模型的有效性，涵盖可验证任务（如数学）和不可验证任务（如指令跟随）。

**Result:** 我们在这些设置下对比了在线和半在线的直接偏好优化和分组奖励优化目标，意外发现这些变体在性能和收敛性上相似，并且都大幅超越了离线方法。

**Conclusion:** 我们的研究表明，使用可验证和不可验证奖励进行多任务学习可以提高所有类型任务的性能。

**Abstract:** We investigate the effectiveness of reinforcement learning methods for
finetuning large language models when transitioning from offline to semi-online
to fully online regimes for both verifiable and non-verifiable tasks. Our
experiments cover training on verifiable math as well as non-verifiable
instruction following with a set of benchmark evaluations for both. Across
these settings, we extensively compare online and semi-online Direct Preference
Optimization and Group Reward Policy Optimization objectives, and surprisingly
find similar performance and convergence between these variants, which all
strongly outperform offline methods. We provide a detailed analysis of the
training dynamics and hyperparameter selection strategies to achieve optimal
results. Finally, we show that multi-tasking with verifiable and non-verifiable
rewards jointly yields improved performance across both task types.

</details>


### [34] [Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM Alignments](https://arxiv.org/abs/2506.21497)
*Jiashuo Wang,Kaitao Song,Chunpu Xu,Changhe Song,Yang Xiao,Dongsheng Li,Lili Qiu,Wenjie Li*

Main category: cs.CL

> This paper proposes a method to enhance user engagement in interactive dialogue systems by leveraging future user reactions as feedback during training with LLMs.

<details>
  <summary>Details</summary>

**Motivation:** To improve user engagement in socially-driven dialogues, the method shifts the focus from merely reasoning over relevant knowledge or planning a dialogue flow (as done in prior works) to enhancing how LLMs learn from user reactions to their responses.

**Method:** The paper adopts a forward-looking method to reinforce learning for interactive language models (LLMs) by using user reactions post-interaction as feedback. It introduces a user simulator, interacts with the LLMs using an i×MCTS (Monte Carlo Tree Search for interaction) algorithm, and employs direct preference optimization (DPO) based on higher-quality experiences extracted from the collected dataset.

**Result:** Experiments conducted in two socially-driven dialogue scenarios, emotional support conversations and persuasive dialogues, effectively demonstrate the enhancement in user engagement achieved with the proposed method.

**Conclusion:** The research underscores the importance of forward-looking, data-driven feedback mechanisms in optimizing interactive language models for better user engagement in socially-driven dialogues.

**Abstract:** Enhancing user engagement through interactions plays an essential role in
socially-driven dialogues. While prior works have optimized models to reason
over relevant knowledge or plan a dialogue act flow, the relationship between
user engagement and knowledge or dialogue acts is subtle and does not guarantee
user engagement in socially-driven dialogues. To this end, we enable
interactive LLMs to learn user engagement by leveraging signals from the future
development of conversations. Specifically, we adopt a more direct and relevant
indicator of user engagement, i.e., the user's reaction related to dialogue
intention after the interaction, as a reward to align interactive LLMs. To
achieve this, we develop a user simulator to interact with target interactive
LLMs and explore interactions between the user and the interactive LLM system
via \textit{i$\times$MCTS} (\textit{M}onte \textit{C}arlo \textit{T}ree
\textit{S}earch for \textit{i}nteraction). In this way, we collect a dataset
containing pairs of higher and lower-quality experiences using
\textit{i$\times$MCTS}, and align interactive LLMs for high-level user
engagement by direct preference optimization (DPO) accordingly. Experiments
conducted on two socially-driven dialogue scenarios (emotional support
conversations and persuasion for good) demonstrate that our method effectively
enhances user engagement in interactive LLMs.

</details>


### [35] [skLEP: A Slovak General Language Understanding Benchmark](https://arxiv.org/abs/2506.21508)
*Marek Šuppa,Andrej Ridzik,Daniel Hládek,Tomáš Javůrek,Viktória Ondrejová,Kristína Sásiková,Martin Tamajka,Marián Šimko*

Main category: cs.CL

> 研究人员创建了skLEP，一种针对斯洛伐克语NLU模型的基准测试工具，并对其进行了系统评估，同时发布了相关的工具和数据。

<details>
  <summary>Details</summary>

**Motivation:** 为了评估斯洛伐克自然语言理解（NLU）模型的能力，研究团队开发了skLEP，这是首个针对斯洛伐克语的全面基准测试工具。

**Method:** skLEP包含了九项多样化的任务，涉及词汇级、句子对和文档级别的挑战。研究所使用的数据集包括全新制作以及翻译自英文NLU资源的数据。

**Result:** 研究对一系列专门用于斯洛伐克语的预训练语言模型、多语言模型和英文模型进行了系统的评估。

**Conclusion:** 研究团队发布了完整的基准测试数据、开源工具包和公共排行榜，以促进研究的可重复性，并希望推动未来斯洛伐克NLU的研究。

**Abstract:** In this work, we introduce skLEP, the first comprehensive benchmark
specifically designed for evaluating Slovak natural language understanding
(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span
token-level, sentence-pair, and document-level challenges, thereby offering a
thorough assessment of model capabilities. To create this benchmark, we curated
new, original datasets tailored for Slovak and meticulously translated
established English NLU resources. Within this paper, we also present the first
systematic and extensive evaluation of a wide array of Slovak-specific,
multilingual, and English pre-trained language models using the skLEP tasks.
Finally, we also release the complete benchmark data, an open-source toolkit
facilitating both fine-tuning and evaluation of models, and a public
leaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering
reproducibility and drive future research in Slovak NLU.

</details>


### [36] [Potemkin Understanding in Large Language Models](https://arxiv.org/abs/2506.21521)
*Marina Mancoridis,Bec Weeks,Keyon Vafa,Sendhil Mullainathan*

Main category: cs.CL

> 该论文引入了一个正式框架来评估大语言模型（LLMs）的基准测试的有效性，并提出了两种量化LLMs中假象理解（potemkins）存在的程序，结果显示这种假象理解普遍存在。

<details>
  <summary>Details</summary>

**Motivation:** 探讨为什么可以根据对大语言模型（LLMs）在一套精选问题上的回答来推断它们的能力，并指出如果LLMs误解概念的方式与人类相似，那么这些基准测试才是有效的。否则，它们的成功可能仅是假象理解的体现。

**Method:** 提出两种量化假象理解存在的方式：一种是基于在三个领域中设计的专门基准测试，另一种是提供假象理解存在下限的一般过程。

**Result:** 发现假象理解在模型、任务和领域中普遍存在，这些失败不仅反映了错误的理解，还反映了概念表示中的内部不一致性。

**Conclusion:** 现有测试LLMs的基准在没有综合对比人类理解误差点的基础上有效性和可靠性存疑，提示需要开发更完善的评价框架。

**Abstract:** Large language models (LLMs) are regularly evaluated using benchmark
datasets. But what justifies making inferences about an LLM's capabilities
based on its answers to a curated set of questions? This paper first introduces
a formal framework to address this question. The key is to note that the
benchmarks used to test LLMs -- such as AP exams -- are also those used to test
people. However, this raises an implication: these benchmarks are only valid
tests if LLMs misunderstand concepts in ways that mirror human
misunderstandings. Otherwise, success on benchmarks only demonstrates potemkin
understanding: the illusion of understanding driven by answers irreconcilable
with how any human would interpret a concept. We present two procedures for
quantifying the existence of potemkins: one using a specially designed
benchmark in three domains, the other using a general procedure that provides a
lower-bound on their prevalence. We find that potemkins are ubiquitous across
models, tasks, and domains. We also find that these failures reflect not just
incorrect understanding, but deeper internal incoherence in concept
representations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [37] [OTSurv: A Novel Multiple Instance Learning Framework for Survival Prediction with Heterogeneity-aware Optimal Transport](https://arxiv.org/abs/2506.20741)
*Qin Ren,Yifan Wang,Ruogu Fang,Haibin Ling,Chenyu You*

Main category: cs.CV

> 提出了一种用于使用全幻灯片图像（WSIs）进行生存预测的最优传输（OT）方法OTSurv，该方法可以全局和局部的层面捕捉病理异质性，在生存预测中取得了新的最先进结果。

<details>
  <summary>Details</summary>

**Motivation:** 传统的方法在处理全幻灯片图像（WSIs）中的生存预测时无法充分考虑病理异质性。这种异质性体现在全局的长尾形态分布和局部的瓦片级预测不确定性上。而最优传输理论提供了一个通过边际分布约束来建模异质性的方法，因此作者提出了OTSurv模型。

**Method:** OTSurv模型利用最优传输理论，以异质性意识的最优传输问题来制定生存预测，包括两个约束条件：一个全局长尾约束条件和一个地方不确定性意识约束条件。全局长尾约束条件可以调节运输质量分配；地方不确定性意识约束条件可以通过逐步增加运输质量来抑制噪声，优先考虑高可信度的补丁。

**Result:** 实验结果显示，OTSurv模型在六个流行基准上实现了新的最先进的结果，平均C指标上的绝对改善率为3.6%。此外，OTSurv还在对数秩检验中达到了统计显著性，并提供了高可解释性。

**Conclusion:** OTSurv通过将最优运输理论应用于多实例学习框架，成功地捕捉到病理异质性，并在生存预测中取得了优异的性能。

**Abstract:** Survival prediction using whole slide images (WSIs) can be formulated as a
multiple instance learning (MIL) problem. However, existing MIL methods often
fail to explicitly capture pathological heterogeneity within WSIs, both
globally -- through long-tailed morphological distributions, and locally
through -- tile-level prediction uncertainty. Optimal transport (OT) provides a
principled way of modeling such heterogeneity by incorporating marginal
distribution constraints. Building on this insight, we propose OTSurv, a novel
MIL framework from an optimal transport perspective. Specifically, OTSurv
formulates survival predictions as a heterogeneity-aware OT problem with two
constraints: (1) global long-tail constraint that models prior morphological
distributions to avert both mode collapse and excessive uniformity by
regulating transport mass allocation, and (2) local uncertainty-aware
constraint that prioritizes high-confidence patches while suppressing noise by
progressively raising the total transport mass. We then recast the initial OT
problem, augmented by these constraints, into an unbalanced OT formulation that
can be solved with an efficient, hardware-friendly matrix scaling algorithm.
Empirically, OTSurv sets new state-of-the-art results across six popular
benchmarks, achieving an absolute 3.6% improvement in average C-index. In
addition, OTSurv achieves statistical significance in log-rank tests and offers
high interpretability, making it a powerful tool for survival prediction in
digital pathology. Our codes are available at
https://github.com/Y-Research-SBU/OTSurv.

</details>


### [38] [StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation](https://arxiv.org/abs/2506.20756)
*Haodong Li,Chen Wang,Jiahui Lei,Kostas Daniilidis,Lingjie Liu*

Main category: cs.CV

> StereoDiff, a novel two-stage video depth estimation model, blends stereo matching and video depth diffusion techniques to achieve superior performance on dynamic video benchmarks by addressing the unique needs of static and dynamic regions within videos.

<details>
  <summary>Details</summary>

**Motivation:** The motivation stems from the observation that current video depth estimation methods, which adapt image depth estimation paradigms, fail to account for the unique temporal consistency requirements of static and dynamic regions in videos.

**Method:** The method introduces StereoDiff, a two-stage video depth estimation model that combines stereo matching for static areas and video depth diffusion for dynamic areas, based on the observation that video depth estimation requires different approaches for static and dynamic regions.

**Result:** Experimental results show that StereoDiff outperforms existing methods on zero-shot, real-world dynamic video depth benchmarks, both indoor and outdoor, in terms of video depth estimation consistency and accuracy.

**Conclusion:** The conclusion is that the synergy between stereo matching for static areas and video depth diffusion for dynamic areas not only addresses the shortcomings of naive extensions of image depth estimation but also provides a more effective approach for video depth estimation.

**Abstract:** Recent video depth estimation methods achieve great performance by following
the paradigm of image depth estimation, i.e., typically fine-tuning pre-trained
video diffusion models with massive data. However, we argue that video depth
estimation is not a naive extension of image depth estimation. The temporal
consistency requirements for dynamic and static regions in videos are
fundamentally different. Consistent video depth in static regions, typically
backgrounds, can be more effectively achieved via stereo matching across all
frames, which provides much stronger global 3D cues. While the consistency for
dynamic regions still should be learned from large-scale video depth data to
ensure smooth transitions, due to the violation of triangulation constraints.
Based on these insights, we introduce StereoDiff, a two-stage video depth
estimator that synergizes stereo matching for mainly the static areas with
video depth diffusion for maintaining consistent depth transitions in dynamic
areas. We mathematically demonstrate how stereo matching and video depth
diffusion offer complementary strengths through frequency domain analysis,
highlighting the effectiveness of their synergy in capturing the advantages of
both. Experimental results on zero-shot, real-world, dynamic video depth
benchmarks, both indoor and outdoor, demonstrate StereoDiff's SoTA performance,
showcasing its superior consistency and accuracy in video depth estimation.

</details>


### [39] [ConViTac: Aligning Visual-Tactile Fusion with Contrastive Representations](https://arxiv.org/abs/2506.20757)
*Zhiyuan Wu,Yongqiang Zhao,Shan Luo*

Main category: cs.CV

> 提出了一种名为ConViTac的视觉-触觉网络，利用对比嵌入条件(CEC)机制来改善特征融合，从而提高下游任务性能。

<details>
  <summary>Details</summary>

**Motivation:** 视觉和触觉是机器人感知和操作任务中的重要感官模态，提供互补信息。以往的研究试图通过直接组合（如特征加法和拼接）来融合模态，但这导致特征整合效果差。

**Method:** 提出了一种名为ConViTac的视觉-触觉表示学习网络，旨在通过对比表示来增强融合期间的特征对齐。关键贡献是一种对比嵌入条件（CEC）机制，该机制利用通过自监督对比学习预训练的对比编码器将视觉和触觉输入投影到统一的潜在嵌入中。通过跨模态注意力，这些嵌入用于结合视觉-触觉特征融合，以对齐统一表示并提高下游任务的表现。

**Result:** 实验表明ConViTac在现实世界中优于当前最先进的方法，CEC机制在材料分类和抓握预测任务中提高了12.0%的精度。

**Conclusion:** ConViTac网络和CEC机制在提升视觉-触觉融合方面表现出优越性。

**Abstract:** Vision and touch are two fundamental sensory modalities for robots, offering
complementary information that enhances perception and manipulation tasks.
Previous research has attempted to jointly learn visual-tactile representations
to extract more meaningful information. However, these approaches often rely on
direct combination, such as feature addition and concatenation, for modality
fusion, which tend to result in poor feature integration. In this paper, we
propose ConViTac, a visual-tactile representation learning network designed to
enhance the alignment of features during fusion using contrastive
representations. Our key contribution is a Contrastive Embedding Conditioning
(CEC) mechanism that leverages a contrastive encoder pretrained through
self-supervised contrastive learning to project visual and tactile inputs into
unified latent embeddings. These embeddings are used to couple visual-tactile
feature fusion through cross-modal attention, aiming at aligning the unified
representations and enhancing performance on downstream tasks. We conduct
extensive experiments to demonstrate the superiority of ConViTac in real world
over current state-of-the-art methods and the effectiveness of our proposed CEC
mechanism, which improves accuracy by up to 12.0% in material classification
and grasping prediction tasks.

</details>


### [40] [AI-Driven MRI-based Brain Tumour Segmentation Benchmarking](https://arxiv.org/abs/2506.20786)
*Connor Ludwig,Khashayar Namdar,Farzad Khalvati*

Main category: cs.CV

> 研究通过在BraTS 2023成人胶质瘤和儿科数据集上使用多种模型的零样本推理，评估和比较了不同种类的提示质量对分割结果的影响。虽然SAM和SAM 2在给定极高精度的边界框提示时可达到超过nnU-Net的分割性能，但由于提供高精度提示在实践中不切实际，nnU-Net仍然是医学图像分割领域的主流模型。

<details>
  <summary>Details</summary>

**Motivation:** 研究目的是填补关于各种医学图像分割模型评估和比较的空白，特别是针对不同质量提示的应用效果。

**Method:** 该研究通过零样本推理和微调的方式，评估了包括SAM, SAM 2, MedSAM, SAM-Med-3D 和 nnU-Net在内的模型在BraTS 2023数据集上的性能，具体采用了点提示和边界框提示。

**Result:** 结果显示，SAM 和 SAM 2 在给定高度准确的边界框提示时，达到了非常高的Dice得分(0.894和0.893)，超过了nnU-Net的分割性能。但是，通过微调，SAM, SAM 2, MedSAM 和 SAM-Med-3D在儿科数据集上的性能提升，虽然在点提示上有很大进展，但仍然比不上边界框提示或nnU-Net的性能。

**Conclusion:** 尽管一些具有提示功能的新模型在特定条件下表现优异，但给予高精度提示的实际困难决定了nnU-Net依然是医学图像分割最好的模型。微调虽然对新模型在点提示上的性能有显著提升，但尚未超越nnU-Net。

**Abstract:** Medical image segmentation has greatly aided medical diagnosis, with U-Net
based architectures and nnU-Net providing state-of-the-art performance. There
have been numerous general promptable models and medical variations introduced
in recent years, but there is currently a lack of evaluation and comparison of
these models across a variety of prompt qualities on a common medical dataset.
This research uses Segment Anything Model (SAM), Segment Anything Model 2 (SAM
2), MedSAM, SAM-Med-3D, and nnU-Net to obtain zero-shot inference on the BraTS
2023 adult glioma and pediatrics dataset across multiple prompt qualities for
both points and bounding boxes. Several of these models exhibit promising Dice
scores, particularly SAM and SAM 2 achieving scores of up to 0.894 and 0.893,
respectively when given extremely accurate bounding box prompts which exceeds
nnU-Net's segmentation performance. However, nnU-Net remains the dominant
medical image segmentation network due to the impracticality of providing
highly accurate prompts to the models. The model and prompt evaluation, as well
as the comparison, are extended through fine-tuning SAM, SAM 2, MedSAM, and
SAM-Med-3D on the pediatrics dataset. The improvements in point prompt
performance after fine-tuning are substantial and show promise for future
investigation, but are unable to achieve better segmentation than bounding
boxes or nnU-Net.

</details>


### [41] [How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?](https://arxiv.org/abs/2506.20795)
*Stephanie Käs,Anton Burenko,Louis Markert,Onur Alp Culha,Dennis Mack,Timm Linder,Bastian Leibe*

Main category: cs.CV

> 研究比较了基于VFMs、VLMs和骨架的手势识别方法，结果表明HD-GCN表现最佳，但V-JEPA次之，并展示了减少系统复杂性潜力。

<details>
  <summary>Details</summary>

**Motivation:** 传统的基于深度学习的手势识别依赖于使用图像、视频或骨骼姿态估计作为输入的任务特定架构，而视觉基础模型和视觉语言模型强大的泛化能力能简化系统。

**Method:** 此研究探讨了视觉基础模型（VFMs）和视觉语言模型（VLMs）在动态全身手势识别中的应用，并将V-JEPA（一种先进的VFM）、Gemini Flash 2.0（一种多模式VLM）和HD-GCN（一种顶尖的基于骨架的方法）进行了比较。

**Result:** 在评估不同手势识别方法的实验中，使用NUGGET数据集，HD-GCN的性能最优，而V-JEPA紧随其后，并展现了作为共享多任务模型的潜力。Gemini在仅使用文本描述时难以区分手势，说明输入表征仍需进一步研究。

**Conclusion:** HD-GCN在动态全身手势识别中表现出色，但V-JEPA也表现了简化系统复杂性的潜力，揭示了手势识别输入表征的进一步研究需求。

**Abstract:** Gestures enable non-verbal human-robot communication, especially in noisy
environments like agile production. Traditional deep learning-based gesture
recognition relies on task-specific architectures using images, videos, or
skeletal pose estimates as input. Meanwhile, Vision Foundation Models (VFMs)
and Vision Language Models (VLMs) with their strong generalization abilities
offer potential to reduce system complexity by replacing dedicated
task-specific modules. This study investigates adapting such models for
dynamic, full-body gesture recognition, comparing V-JEPA (a state-of-the-art
VFM), Gemini Flash 2.0 (a multimodal VLM), and HD-GCN (a top-performing
skeleton-based approach). We introduce NUGGET, a dataset tailored for
human-robot communication in intralogistics environments, to evaluate the
different gesture recognition approaches. In our experiments, HD-GCN achieves
best performance, but V-JEPA comes close with a simple, task-specific
classification head - thus paving a possible way towards reducing system
complexity, by using it as a shared multi-task model. In contrast, Gemini
struggles to differentiate gestures based solely on textual descriptions in the
zero-shot setting, highlighting the need of further research on suitable input
representations for gestures.

</details>


### [42] [Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models](https://arxiv.org/abs/2506.20832)
*Cansu Korkmaz,Ahmet Murat Tekalp,Zafer Dogan*

Main category: cs.CV

> 研究提出利用VLM评估语义正确性、视觉质量和伪影存在情况，从扩散模型生成的超分辨率图像中确定最值得信赖的画面，提出了一个新的可信度评分（TWS），展示了该方法与人类偏好有强相关性，并优于传统指标如PSNR和LPIPS。

<details>
  <summary>Details</summary>

**Motivation:** 由于回归SR模型引入了可能在信息关键应用中造成歧义的伪影，扩散模型虽然可以生成多样性丰富的SR图像，但是挑选出最值得信赖的解决方案仍然是一个挑战。该研究的动机就是提出一个自动化的框架解决这个问题。

**Method:** 该研究开发了一个利用视觉语言模型（VLM）的鲁棒、自动框架，以确定从扩散模型生成的一组超分辨率（SR）图像中最值得信赖的画面。具体来说，使用如BLIP-2、GPT-4o等VLM处理结构化查询，评估语义正确性、视觉质量及伪影存在情况。通过综合排名最高的SR候选图像，最终得到单一的值得信赖的输出，且成本效益高。

**Result:** 研究通过实验证明了TWS与人类偏好在含糊不清及自然图像中有强相关性，并证明了VLM引导的选择可以持续产生高的TWS值，从而表明该方法在超分辨率生成中的优越性及可信度。

**Conclusion:** 研究结果表明所提的可信度评分（TWS）在含糊不清和自然图像中与人类偏好有强相关性，并且VLM引导选择一直产生高的TWS值。该工作通过与人类期望及语义正确性对齐，为生成SR的可信度设定了新的基准。

**Abstract:** Super-resolution (SR) is an ill-posed inverse problem with many feasible
solutions consistent with a given low-resolution image. On one hand, regressive
SR models aim to balance fidelity and perceptual quality to yield a single
solution, but this trade-off often introduces artifacts that create ambiguity
in information-critical applications such as recognizing digits or letters. On
the other hand, diffusion models generate a diverse set of SR images, but
selecting the most trustworthy solution from this set remains a challenge. This
paper introduces a robust, automated framework for identifying the most
trustworthy SR sample from a diffusion-generated set by leveraging the semantic
reasoning capabilities of vision-language models (VLMs). Specifically, VLMs
such as BLIP-2, GPT-4o, and their variants are prompted with structured queries
to assess semantic correctness, visual quality, and artifact presence. The
top-ranked SR candidates are then ensembled to yield a single trustworthy
output in a cost-effective manner. To rigorously assess the validity of
VLM-selected samples, we propose a novel Trustworthiness Score (TWS) a hybrid
metric that quantifies SR reliability based on three complementary components:
semantic similarity via CLIP embeddings, structural integrity using SSIM on
edge maps, and artifact sensitivity through multi-level wavelet decomposition.
We empirically show that TWS correlates strongly with human preference in both
ambiguous and natural images, and that VLM-guided selections consistently yield
high TWS values. Compared to conventional metrics like PSNR, LPIPS, which fail
to reflect information fidelity, our approach offers a principled, scalable,
and generalizable solution for navigating the uncertainty of the diffusion SR
space. By aligning outputs with human expectations and semantic correctness,
this work sets a new benchmark for trustworthiness in generative SR.

</details>


### [43] [FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization](https://arxiv.org/abs/2506.20841)
*Ha Min Son,Shahbaz Rezaei,Xin Liu*

Main category: cs.CV

> 提出FixCLR方法，有效解决标签稀缺情况下的领域泛化问题，适用于多种SSDG和半监督方法的性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 解决标签稀缺情况下泛化到分布外数据的问题，现有SSDG方法未能显式地正则化以学习跨所有领域的领域不变表示，这是领域泛化的关键目标。

**Method:** FixCLR，通过借鉴自监督学习的成功经验，调整了对比学习的两个关键组成部分，以实现显式的领域不变正则化：利用伪标签中的类别信息以及仅使用排斥项。

**Result:** 进行了广泛实验，包括基准测试不同对半监督方法的改进、评估预训练模型与非预训练模型的性能以及测试具有多个领域的数据集。

**Conclusion:** FixCLR证明是一种有效的SSDG方法，尤其当与其它半监督方法结合使用时。

**Abstract:** Semi-supervised domain generalization (SSDG) aims to solve the problem of
generalizing to out-of-distribution data when only a few labels are available.
Due to label scarcity, applying domain generalization methods often
underperform. Consequently, existing SSDG methods combine semi-supervised
learning methods with various regularization terms. However, these methods do
not explicitly regularize to learn domains invariant representations across all
domains, which is a key goal for domain generalization. To address this, we
introduce FixCLR. Inspired by success in self-supervised learning, we change
two crucial components to adapt contrastive learning for explicit domain
invariance regularization: utilization of class information from pseudo-labels
and using only a repelling term. FixCLR can also be added on top of most
existing SSDG and semi-supervised methods for complementary performance
improvements. Our research includes extensive experiments that have not been
previously explored in SSDG studies. These experiments include benchmarking
different improvements to semi-supervised methods, evaluating the performance
of pretrained versus non-pretrained models, and testing on datasets with many
domains. Overall, FixCLR proves to be an effective SSDG method, especially when
combined with other semi-supervised methods.

</details>


### [44] [Vector Contrastive Learning For Pixel-Wise Pretraining In Medical Vision](https://arxiv.org/abs/2506.20850)
*Yuting He,Shuo Li*

Main category: cs.CV

> 我们提出了一种矢量对比学习方法，解决了像素级表征的分散问题，通过新的COntrast in VEctor Regression (COVER)框架，在多个医疗视觉任务中表现出了显著的改进。

<details>
  <summary>Details</summary>

**Motivation:** 对比学习已成为了基础模型自监督预训练的基石，但在医疗视觉中至关重要的像素级表征上，对比学习的延伸至今仍是一个未解决的问题。标准对比学习格式将自监督预训练当作一个二元优化问题，对特征分散的过度追求导致了像素级特征相关性的破坏。

**Method:** 我们的方法称为COntrast in VEctor Regression (COVER)框架，将对比学习重新定义为向量回归问题，通过回归位移向量来量化特征距离，从而保留像素级特征的相关性。COVER框架采用了可扩展的向量学习方式，一致的从向量回归到距离建模的优化流程，以及用于粒度适应的向量金字塔架构。

**Result:** 实验结果涵盖了8个任务，包括2个维度和4种模态，显示COVER显著提高了像素级的自监督预训练，推动了可泛化的医疗视觉基础模型的发展。

**Conclusion:** COVER框架成功地解决了标准对比学习在像素级表征上的局限性，对于医疗图像中有意义的像素间相关性提供了更好的保存，从而提升了预训练模型的泛化能力。

**Abstract:** Contrastive learning (CL) has become a cornerstone of self-supervised
pretraining (SSP) in foundation models, however, extending CL to pixel-wise
representation, crucial for medical vision, remains an open problem. Standard
CL formulates SSP as a binary optimization problem (binary CL) where the
excessive pursuit of feature dispersion leads to an over-dispersion problem,
breaking pixel-wise feature correlation thus disrupting the intra-class
distribution. Our vector CL reformulates CL as a vector regression problem,
enabling dispersion quantification in pixel-wise pretraining via modeling
feature distances in regressing displacement vectors. To implement this novel
paradigm, we propose the COntrast in VEctor Regression (COVER) framework. COVER
establishes an extendable vector-based self-learning, enforces a consistent
optimization flow from vector regression to distance modeling, and leverages a
vector pyramid architecture for granularity adaptation, thus preserving
pixel-wise feature correlations in SSP. Extensive experiments across 8 tasks,
spanning 2 dimensions and 4 modalities, show that COVER significantly improves
pixel-wise SSP, advancing generalizable medical visual foundation models.

</details>


### [45] [Enhancing Ambiguous Dynamic Facial Expression Recognition with Soft Label-based Data Augmentation](https://arxiv.org/abs/2506.20867)
*Ryosuke Kawamura,Hideaki Hayashi,Shunsuke Otake,Noriko Takemura,Hajime Nagahara*

Main category: cs.CV

> 研究人员提出了一种名为MIDAS的数据增强方法，专门用于提升对模糊面部表情的动态面部表情识别性能，并在实验中验证了其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 对于实际应用而言，准确识别野外数据中常见的模糊面部表情至关重要。提出MIDAS的目的是解决这一难题。

**Method:** MIDAS,一种用于动态面部表情识别的数据增强方法，通过将视频帧及其对应的情绪类别标签进行凸组合来生成软标签，从而提高对模糊面部表情数据的识别能力。

**Result:** 与使用原始数据集训练的最先进的方法相比，使用MIDAS增强数据训练的模型在DFEW数据集和一个新构建的FERV39k-Plus数据集上表现出更好的性能。

**Conclusion:** MIDAS方法通过扩展mixup到软标签视频数据，为解决动态面部表情识别中的模糊性提供了有效解决方案。实验结果表明其在处理模糊面部表情识别问题上的优越性。

**Abstract:** Dynamic facial expression recognition (DFER) is a task that estimates
emotions from facial expression video sequences. For practical applications,
accurately recognizing ambiguous facial expressions -- frequently encountered
in in-the-wild data -- is essential. In this study, we propose MIDAS, a data
augmentation method designed to enhance DFER performance for ambiguous facial
expression data using soft labels representing probabilities of multiple
emotion classes. MIDAS augments training data by convexly combining pairs of
video frames and their corresponding emotion class labels. This approach
extends mixup to soft-labeled video data, offering a simple yet highly
effective method for handling ambiguity in DFER. To evaluate MIDAS, we
conducted experiments on both the DFEW dataset and FERV39k-Plus, a newly
constructed dataset that assigns soft labels to an existing DFER dataset. The
results demonstrate that models trained with MIDAS-augmented data achieve
superior performance compared to the state-of-the-art method trained on the
original dataset.

</details>


### [46] [THIRDEYE: Cue-Aware Monocular Depth Estimation via Brain-Inspired Multi-Stage Fusion](https://arxiv.org/abs/2506.20877)
*Calin Teodor Ioan*

Main category: cs.CV

> ThirdEye方法明确地识别并融合单目线索，来改进单目深度估计的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 传统的单目深度估计方法仅仅依靠从RGB像素直接推断深度，而忽视了人眼视觉系统依赖的明确单目线索，如遮挡边界、阴影和透视。而ThirdEye的动机是通过专门的网络来提供这些线索，并通过一个权重可靠的皮层层次结构来融合它们。

**Method:** ThirdEye方法通过专门的、预训练的和冻结的网络来提供每个线索，这些网络专门识别如遮挡边界、阴影和透视等单目线索。这些线索在三阶段皮层层次结构（V1->V2->V3）中被融合，该层次结构配备了一个根据可靠性加权线索的关键值工作记忆模块。最后，一个自适应bins的Transformer头部产生高分辨率的视差图。

**Result:** 由于线索专家网络是冻结的，ThirdEye继承了大量外部监督，仅需进行适度微调。详细的架构、神经科学技术动机及实验方案在扩展版本中有所提及，定量结果将在未来的修订中提供。

**Conclusion:** ThirdEye通过专门的技巧来提高单目深度估计的准确性和可靠性，这是一种创新的方法。

**Abstract:** Monocular depth estimation methods traditionally train deep models to infer
depth directly from RGB pixels. This implicit learning often overlooks explicit
monocular cues that the human visual system relies on, such as occlusion
boundaries, shading, and perspective. Rather than expecting a network to
discover these cues unaided, we present ThirdEye, a cue-aware pipeline that
deliberately supplies each cue through specialised, pre-trained, and frozen
networks. These cues are fused in a three-stage cortical hierarchy (V1->V2->V3)
equipped with a key-value working-memory module that weights them by
reliability. An adaptive-bins transformer head then produces a high-resolution
disparity map. Because the cue experts are frozen, ThirdEye inherits large
amounts of external supervision while requiring only modest fine-tuning. This
extended version provides additional architectural detail, neuroscientific
motivation, and an expanded experimental protocol; quantitative results will
appear in a future revision.

</details>


### [47] [MultiHuman-Testbench: Benchmarking Image Generation for Multiple Humans](https://arxiv.org/abs/2506.20879)
*Shubhankar Borse,Seokeon Choi,Sunghyun Park,Jeongho Kim,Shreya Kadambi,Risheek Garrepalli,Sungrack Yun,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

> 研究提出了一个名为MultiHuman-Testbench的新基准，用于评估多人体生成模型，包含1800个样本和5550个面部图像，通过四个关键指标进行多方面评估，研究认为这些样本和关键发现为多人体图像生成研究提供了重要见解和标准化工具。

<details>
  <summary>Details</summary>

**Motivation:** 生成多个人体、执行复杂动作同时保留面部身份的图像是一项重大挑战，而缺乏专门的基准测试是这一挑战的关键因素。为此，研究提出了MultiHuman-Testbench。

**Method:** 该研究提出了MultiHuman-Testbench，这是一个新的基准测试，用于评估多人体图像生成的生成模型。基准测试包含1800个样本，包括精心策划的文本提示，描述从简单到复杂的动作，并且匹配有5550个独特的面部图像。基准测试还提供人类选择的姿势条件图像，并使用四个关键指标来衡量人脸数量、身份相似度、文本提示一致性以及动作检测。

**Result:** 研究对多种模型进行了全面评估，包括零样本方法和训练方法，有和没有区域先验。研究还提出了一种新型技术，利用人体分割和匈牙利匹配来进行图像和区域隔离，显著提高了身份的相似度。

**Conclusion:** 研究提供的基准和关键发现为多人体图像生成领域的发展提供了重要的标准工具和见解。

**Abstract:** Generation of images containing multiple humans, performing complex actions,
while preserving their facial identities, is a significant challenge. A major
factor contributing to this is the lack of a a dedicated benchmark. To address
this, we introduce MultiHuman-Testbench, a novel benchmark for rigorously
evaluating generative models for multi-human generation. The benchmark
comprises 1800 samples, including carefully curated text prompts, describing a
range of simple to complex human actions. These prompts are matched with a
total of 5,550 unique human face images, sampled uniformly to ensure diversity
across age, ethnic background, and gender. Alongside captions, we provide
human-selected pose conditioning images which accurately match the prompt. We
propose a multi-faceted evaluation suite employing four key metrics to quantify
face count, ID similarity, prompt alignment, and action detection. We conduct a
thorough evaluation of a diverse set of models, including zero-shot approaches
and training-based methods, with and without regional priors. We also propose
novel techniques to incorporate image and region isolation using human
segmentation and Hungarian matching, significantly improving ID similarity. Our
proposed benchmark and key findings provide valuable insights and a
standardized tool for advancing research in multi-human image generation.

</details>


### [48] [The Role of Cyclopean-Eye in Stereo Vision](https://arxiv.org/abs/2506.20900)
*Sherlon Almeida da Silva,Davi Geiger,Luiz Velho,Moacir Antonelli Ponti*

Main category: cs.CV

> 本文探讨了现代立体视觉系统中的几何基础，提出新的几何约束处理遮挡和深度不连续性，通过理论分析和实际数据集实验验证结合几何先验和学习特征的方法能够提供理解立体视觉系统的内部抽象。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于理解3D结构和人类启发的感知如何有助于提高深度重建的准确性，并通过几何约束和深度学习结合的方式提供内部抽象用于理解立体视觉系统。

**Method:** 本文通过重新审视Cyclopean Eye模型并提出新的几何约束条件来处理遮挡和深度不连续性问题，结合深度学习模型获得的立体特征匹配质量和注意力机制在恢复有意义的3D表面中的作用，对现代立体视觉系统的几何基础进行了深入探讨。

**Result:** 研究结果表明，通过理论分析和现实数据集上的实证研究表明，结合强大的几何先验知识与学习到的特征可为理解立体视觉系统提供内部抽象，从而改善深度重建的准确性。

**Conclusion:** 结合强几何先验与学习特征的方法为理解立体视觉系统提供了内部抽象，并证明了这种方法在3D结构感知和深度重建中的有效性。

**Abstract:** This work investigates the geometric foundations of modern stereo vision
systems, with a focus on how 3D structure and human-inspired perception
contribute to accurate depth reconstruction. We revisit the Cyclopean Eye model
and propose novel geometric constraints that account for occlusions and depth
discontinuities. Our analysis includes the evaluation of stereo feature
matching quality derived from deep learning models, as well as the role of
attention mechanisms in recovering meaningful 3D surfaces. Through both
theoretical insights and empirical studies on real datasets, we demonstrate
that combining strong geometric priors with learned features provides internal
abstractions for understanding stereo vision systems.

</details>


### [49] [FaSTA$^*$: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient Multi-turn Image Editing](https://arxiv.org/abs/2506.20911)
*Advait Gupta,Rishie Raj,Dang Nguyen,Tianyi Zhou*

Main category: cs.CV

> 提出了一种名为FaSTA$^*$的高效神经符号图像编辑代理，能够在处理多轮图像编辑任务时节省成本，同时保持与现有先进方法相当的成功率。

<details>
  <summary>Details</summary>

**Motivation:** 目的是提出一种可以高效解决复杂多轮次图像编辑任务的智能代理系统，同时保持较高的成功率，通过结合大语言模型的快速规划和局部搜索的精确执行来最小化成本。

**Method:** 该方法首次使用高阶的LLMs进行任务的快速规划，随后用A$^*$算法进行工具路径的详细规划。并且创新性利用LLMs分析过去的执行路径，学习可重用的子程序，以减少未来的搜索成本，构建了一个能够快速思考和慢速精确执行的代理系统FaSTA$^*$。

**Result:** 该研究开发了一种成本效益高的神经符号代理，以解决多轮图像编辑任务，此类任务涉及复杂的图像操作。高阶任务规划由大语言模型（LLMs）执行，而具体任务规划由局部A$^*$搜索完成。为了节省相似任务上的A$^*$计算成本，研究通过LLMs提取之前成功的操作路径，不断抽取和提炼出可重复利用的子程序，使其作为新的工具用于未来任务。该代理能够快速处理常见的子任务，仅在遇到新颖或具有挑战性的任务时才激活慢速的A$^*$搜索，因此在计算效率方面显著优于现有方法，同时在成功率上与最先进方法保持竞争力。

**Conclusion:** 该研究设计的FaSTA$^*$代理以经济的成本实现了复杂图像编辑任务的处理，可以快速规划一般任务并节省搜索时间，同时也能够处理新的具有挑战性的案例。实验证明，这种方法在成功率达到最先进水平的同时，提高了总体的计算效率。

**Abstract:** We develop a cost-efficient neurosymbolic agent to address challenging
multi-turn image editing tasks such as "Detect the bench in the image while
recoloring it to pink. Also, remove the cat for a clearer view and recolor the
wall to yellow.'' It combines the fast, high-level subtask planning by large
language models (LLMs) with the slow, accurate, tool-use, and local A$^*$
search per subtask to find a cost-efficient toolpath -- a sequence of calls to
AI tools. To save the cost of A$^*$ on similar subtasks, we perform inductive
reasoning on previously successful toolpaths via LLMs to continuously
extract/refine frequently used subroutines and reuse them as new tools for
future tasks in an adaptive fast-slow planning, where the higher-level
subroutines are explored first, and only when they fail, the low-level A$^*$
search is activated. The reusable symbolic subroutines considerably save
exploration cost on the same types of subtasks applied to similar images,
yielding a human-like fast-slow toolpath agent "FaSTA$^*$'': fast subtask
planning followed by rule-based subroutine selection per subtask is attempted
by LLMs at first, which is expected to cover most tasks, while slow A$^*$
search is only triggered for novel and challenging subtasks. By comparing with
recent image editing approaches, we demonstrate FaSTA$^*$ is significantly more
computationally efficient while remaining competitive with the state-of-the-art
baseline in terms of success rate.

</details>


### [50] [M2SFormer: Multi-Spectral and Multi-Scale Attention with Edge-Aware Difficulty Guidance for Image Forgery Localization](https://arxiv.org/abs/2506.20922)
*Ju-Hyeon Nam,Dong-Hyun Moon,Sang-Chul Lee*

Main category: cs.CV

> 本文提出了M2SFormer框架，这是一种改进的伪造检测方法，它在跳跃连接中统一处理多频率和多尺度信息，并通过全局先验图和难度引导的注意力模块来提高细微操作的检测精度和泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 随着图像编辑技术的快速发展，在创新应用和恶意篡改数字图像方面都面临的新的挑战。尽管基于深度学习的方法在像素级伪造定位方面取得了高精度，但它们经常受到计算开销大和表征能力有限的影响。这项工作旨在解决这些问题，并提供更好的伪造图像检测方法。

**Method:** 本论文提出了M2SFormer，这是一种基于Transformer编码器的框架，旨在解决现有方法在计算开销和表征能力上的局限性。M2SFormer通过在跳跃连接中统一多频率和多尺度注意力，利用全局上下文来更好地捕捉各种伪造痕迹。此外，它采用了一个全局先验图和一个指示伪造定位难度的曲率度量，用于指导难度引导的注意力模块，从而更有效地保持对细微操作的检测能力。

**Result:** 实验结果表明，M2SFormer在多个基准数据集上都超越了现有的最先进模型，在检测和定位伪造图像方面表现出了更强的泛化能力。

**Conclusion:** M2SFormer通过创新的结构和注意力机制，能够有效地处理复杂且细微的图像篡改，展现出在检测伪造图像方面的优越性能。

**Abstract:** Image editing techniques have rapidly advanced, facilitating both innovative
use cases and malicious manipulation of digital images. Deep learning-based
methods have recently achieved high accuracy in pixel-level forgery
localization, yet they frequently struggle with computational overhead and
limited representation power, particularly for subtle or complex tampering. In
this paper, we propose M2SFormer, a novel Transformer encoder-based framework
designed to overcome these challenges. Unlike approaches that process spatial
and frequency cues separately, M2SFormer unifies multi-frequency and
multi-scale attentions in the skip connection, harnessing global context to
better capture diverse forgery artifacts. Additionally, our framework addresses
the loss of fine detail during upsampling by utilizing a global prior map, a
curvature metric indicating the difficulty of forgery localization, which then
guides a difficulty-guided attention module to preserve subtle manipulations
more effectively. Extensive experiments on multiple benchmark datasets
demonstrate that M2SFormer outperforms existing state-of-the-art models,
offering superior generalization in detecting and localizing forgeries across
unseen domains.

</details>


### [51] [PhysRig: Differentiable Physics-Based Skinning and Rigging Framework for Realistic Articulated Object Modeling](https://arxiv.org/abs/2506.20936)
*Hao Zhang,Haolan Xu,Chun Feng,Varun Jampani,Narendra Ahuja*

Main category: cs.CV

> This paper introduces PhysRig, a new differentiable physics-based skinning method that overcomes the limitations of traditional LBS by embedding a rigid skeleton in a volumetric deformation model. This approach achieves more realistic elasticity and deformation for soft tissues, fur, and flexible appendages.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this work is to address the limitations of Linear Blend Skinning (LBS), such as volume loss and unnatural deformations, especially for elastic materials like soft tissues, fur, and flexible appendages. The authors aim to provide a more realistic and physically plausible method for skinning and rigging in animation, articulated object reconstruction, motion transfer, and 4D generation.

**Method:** The paper proposes PhysRig, a physics-based skinning and rigging framework that uses a volumetric representation of the rigid skeleton to simulate deformable soft-body structures. This approach leverages continuum mechanics and discretizes objects as particles embedded in an Eulerian grid for differentiability with respect to material properties and skeletal motion.

**Result:** The results of the paper demonstrate that the proposed PhysRig method consistently outperforms LBS-based approaches in generating realistically deformable soft-body structures for a variety of objects and motion patterns.

**Conclusion:** Concluding, the paper establishes PhysRig as an effective and versatile framework for skinning and rigging, capable of producing physically plausible results that surpass traditional methods, especially for soft and complex materials. The method also shows promise in pose transfer applications.

**Abstract:** Skinning and rigging are fundamental components in animation, articulated
object reconstruction, motion transfer, and 4D generation. Existing approaches
predominantly rely on Linear Blend Skinning (LBS), due to its simplicity and
differentiability. However, LBS introduces artifacts such as volume loss and
unnatural deformations, and it fails to model elastic materials like soft
tissues, fur, and flexible appendages (e.g., elephant trunks, ears, and fatty
tissues). In this work, we propose PhysRig: a differentiable physics-based
skinning and rigging framework that overcomes these limitations by embedding
the rigid skeleton into a volumetric representation (e.g., a tetrahedral mesh),
which is simulated as a deformable soft-body structure driven by the animated
skeleton. Our method leverages continuum mechanics and discretizes the object
as particles embedded in an Eulerian background grid to ensure
differentiability with respect to both material properties and skeletal motion.
Additionally, we introduce material prototypes, significantly reducing the
learning space while maintaining high expressiveness. To evaluate our
framework, we construct a comprehensive synthetic dataset using meshes from
Objaverse, The Amazing Animals Zoo, and MixaMo, covering diverse object
categories and motion patterns. Our method consistently outperforms traditional
LBS-based approaches, generating more realistic and physically plausible
results. Furthermore, we demonstrate the applicability of our framework in the
pose transfer task highlighting its versatility for articulated object
modeling.

</details>


### [52] [AIR-VIEW: The Aviation Image Repository for Visibility Estimation of Weather, A Dataset and Benchmark](https://arxiv.org/abs/2506.20939)
*Chad Mourning,Zhewei Wang,Justin Murray*

Main category: cs.CV

> 本文通过一个新数据集评估了几种方法在航空范围内的大气能见度估计中的性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于缺乏适用于航空距离的大规模、多样化位置的能见度估计数据集，本文旨在填补这一空白并提供一个适合监督学习的数据集，从而推动低成本航空气象研究的发展。

**Method:** 本文介绍了从FAA天气摄像头网络收集的图像数据集，并通过将其与最近批准的ASTM标准进行比较来评估三种常用方法和一个通用基准线的性能。

**Result:** 与现有的公开数据集相比，该数据集在大气能见度估计的应用中提供了新的基准测试。

**Conclusion:** 该数据集及其实验结果为下一步的研究和发展提供了有价值的参考。

**Abstract:** Machine Learning for aviation weather is a growing area of research for
providing low-cost alternatives for traditional, expensive weather sensors;
however, in the area of atmospheric visibility estimation, publicly available
datasets, tagged with visibility estimates, of distances relevant for aviation,
of diverse locations, of sufficient size for use in supervised learning, are
absent. This paper introduces a new dataset which represents the culmination of
a year-long data collection campaign of images from the FAA weather camera
network suitable for this purpose. We also present a benchmark when applying
three commonly used approaches and a general-purpose baseline when trained and
tested on three publicly available datasets, in addition to our own, when
compared against a recently ratified ASTM standard.

</details>


### [53] [Hierarchical Sub-action Tree for Continuous Sign Language Recognition](https://arxiv.org/abs/2506.20947)
*Dejie Yang,Zhu Xu,Xinjie Gao,Yang Liu*

Main category: cs.CV

> This paper proposes HST-CSLR, which uses a Hierarchical Sub-action Tree to better integrate gloss knowledge with visual representations, thereby improving continuous sign language recognition.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to overcome the limitations of continuous sign language recognition (CSLR) due to insufficient training data by more effectively utilizing gloss knowledge.

**Method:** Our method, HST-CSLR, utilizes a Hierarchical Sub-action Tree (HST) to combine gloss knowledge with visual representation learning. It incorporates gloss-specific knowledge from large language models and uses a contrastive alignment enhancement to facilitate the alignment between visual and textual modalities.

**Result:** Experiments on PHOENIX-2014, PHOENIX-2014T, CSL-Daily, and Sign Language Gesture datasets show the effectiveness of HST-CSLR in improving CSLR.

**Conclusion:** The paper concludes that the HST-CSLR approach, by leveraging gloss knowledge and contrastive alignment enhancements, can effectively increase the performance of continuous sign language recognition systems.

**Abstract:** Continuous sign language recognition (CSLR) aims to transcribe untrimmed
videos into glosses, which are typically textual words. Recent studies indicate
that the lack of large datasets and precise annotations has become a bottleneck
for CSLR due to insufficient training data. To address this, some works have
developed cross-modal solutions to align visual and textual modalities.
However, they typically extract textual features from glosses without fully
utilizing their knowledge. In this paper, we propose the Hierarchical
Sub-action Tree (HST), termed HST-CSLR, to efficiently combine gloss knowledge
with visual representation learning. By incorporating gloss-specific knowledge
from large language models, our approach leverages textual information more
effectively. Specifically, we construct an HST for textual information
representation, aligning visual and textual modalities step-by-step and
benefiting from the tree structure to reduce computational complexity.
Additionally, we impose a contrastive alignment enhancement to bridge the gap
between the two modalities. Experiments on four datasets (PHOENIX-2014,
PHOENIX-2014T, CSL-Daily, and Sign Language Gesture) demonstrate the
effectiveness of our HST-CSLR.

</details>


### [54] [OmniEval: A Benchmark for Evaluating Omni-modal Models with Visual, Auditory, and Textual Inputs](https://arxiv.org/abs/2506.20960)
*Yiman Zhang,Ziheng Luo,Qiangyu Yan,Wei He,Borui Jiang,Xinghao Chen,Kai Han*

Main category: cs.CV

> 本文介绍了OmniEval基准测试，用于评估多模态模型（如MiniCPM-O 2.6），该模型能处理视觉、听觉和文本输入，具有模态协作、视频多样性及任务多样性等特征。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于提供一种全面评估多模态模型能力的方法，从而增强对跨模态信息的理解和整合。

**Method:** 设计了一个包含810个音视频同步视频和2617个问答对的基准测试，重点强调了模态间的紧密合作并引入了更细致的视频定位任务。

**Result:** 在OmniEval上对多个多模态模型进行了实验，展示了OmniEval作为评估平台的有效性。

**Conclusion:** OmniEval为多模态模型提供评估平台，有助于更好地理解和整合多模态信息。

**Abstract:** In this paper, we introduce OmniEval, a benchmark for evaluating
omni-modality models like MiniCPM-O 2.6, which encompasses visual, auditory,
and textual inputs. Compared with existing benchmarks, our OmniEval has several
distinctive features: (i) Full-modal collaboration: We design evaluation tasks
that highlight the strong coupling between audio and video, requiring models to
effectively leverage the collaborative perception of all modalities; (ii)
Diversity of videos: OmniEval includes 810 audio-visual synchronized videos,
285 Chinese videos and 525 English videos; (iii) Diversity and granularity of
tasks: OmniEval contains 2617 question-answer pairs, comprising 1412 open-ended
questions and 1205 multiple-choice questions. These questions are divided into
3 major task types and 12 sub-task types to achieve comprehensive evaluation.
Among them, we introduce a more granular video localization task named
Grounding. Then we conduct experiments on OmniEval with several omni-modality
models. We hope that our OmniEval can provide a platform for evaluating the
ability to construct and understand coherence from the context of all
modalities. Codes and data could be found at https://omnieval.github.io/.

</details>


### [55] [Evidence-based diagnostic reasoning with multi-agent copilot for human pathology](https://arxiv.org/abs/2506.20964)
*Chengkuan Chen,Luca L. Weishaupt,Drew F. K. Williamson,Richard J. Chen,Tong Ding,Bowen Chen,Anurag Vaidya,Long Phi Le,Guillaume Jaume,Ming Y. Lu,Faisal Mahmood*

Main category: cs.CV

> 介绍PathChat+，一种专门为人类病理学设计的多模式大语言模型，展示了其在处理病理数据上的优越性，并引入SlideSeek，一个利用PathChat+进行自主分析的多智能体AI系统。

<details>
  <summary>Details</summary>

**Motivation:** 传统的计算病理模型主要集中在图像分析上，缺乏对自然语言指令和丰富文本背景的集成。同时，当前的多模态大语言模型在训练数据、多图像理解支持和自主诊断推理能力方面存在不足。

**Method:** 开发了PathChat+，这是一个专为病理学设计的大语言模型，用超过100万的多样化指令样本和近550万的问答回合训练。还创建了SlideSeek，一个多智能体AI系统，能够自主评估巨像素全幻灯片图像，通过迭代的层级诊断推理。

**Result:** PathChat+在各种病理基准上的表现显著优于之前的PathChat和当前其他通用或特定于病理学的模型。SlideSeek在DDxBench基准上达到了高精度。

**Conclusion:** PathChat+和SlideSeek在病理学中的应用展示了在多模式学习和AI推理上的进步，可能对未来的病理学实践产生深远影响。

**Abstract:** Pathology is experiencing rapid digital transformation driven by whole-slide
imaging and artificial intelligence (AI). While deep learning-based
computational pathology has achieved notable success, traditional models
primarily focus on image analysis without integrating natural language
instruction or rich, text-based context. Current multimodal large language
models (MLLMs) in computational pathology face limitations, including
insufficient training data, inadequate support and evaluation for multi-image
understanding, and a lack of autonomous, diagnostic reasoning capabilities. To
address these limitations, we introduce PathChat+, a new MLLM specifically
designed for human pathology, trained on over 1 million diverse,
pathology-specific instruction samples and nearly 5.5 million question answer
turns. Extensive evaluations across diverse pathology benchmarks demonstrated
that PathChat+ substantially outperforms the prior PathChat copilot, as well as
both state-of-the-art (SOTA) general-purpose and other pathology-specific
models. Furthermore, we present SlideSeek, a reasoning-enabled multi-agent AI
system leveraging PathChat+ to autonomously evaluate gigapixel whole-slide
images (WSIs) through iterative, hierarchical diagnostic reasoning, reaching
high accuracy on DDxBench, a challenging open-ended differential diagnosis
benchmark, while also capable of generating visually grounded,
humanly-interpretable summary reports.

</details>


### [56] [DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing](https://arxiv.org/abs/2506.20967)
*Lingling Cai,Kang Zhao,Hangjie Yuan,Xiang Wang,Yingya Zhang,Kejie Huang*

Main category: cs.CV

> DFVEdit是一种面向Video DiTs的零拍视频编辑方法，通过流变换技术操作清洁潜空间，显著提高编辑效率，证明在处理Video DiTs时比现有技术具有显著优势。

<details>
  <summary>Details</summary>

**Motivation:** 现有的视频编辑方法应用到Video DiTs上时通常会带来巨大的计算开销，因为这些方法往往涉及资源密集型的注意力修改或微调。为了缓解这个问题，研究者开发了DFVEdit。

**Method:** DFVEdit采用直接作用于清洁潜在空间的流变换方法，以消除对注意机制修改和微调的需求。它提出了条件差分流向量(CDFV)——一个理论上无偏的DFV估算——并结合隐式交叉注意(ICA)引导和嵌入强化(ER)以进一步提高编辑质量。

**Result:** 实验表明，DFVEdit可以在不牺牲性能的情况下，与基于注意力工程的编辑方法相比，提供至少20倍的推理速度加快和85%的内存减少，并且在结构保真度、时空一致性和编辑质量上达到业界顶尖水平。

**Conclusion:** 鉴于其实用效率和实验验证的出色性能，DFVEdit为Video DiT的零拍视频编辑提供了一个高效的方法。

**Abstract:** The advent of Video Diffusion Transformers (Video DiTs) marks a milestone in
video generation. However, directly applying existing video editing methods to
Video DiTs often incurs substantial computational overhead, due to
resource-intensive attention modification or finetuning. To alleviate this
problem, we present DFVEdit, an efficient zero-shot video editing method
tailored for Video DiTs. DFVEdit eliminates the need for both attention
modification and fine-tuning by directly operating on clean latents via flow
transformation. To be more specific, we observe that editing and sampling can
be unified under the continuous flow perspective. Building upon this
foundation, we propose the Conditional Delta Flow Vector (CDFV) -- a
theoretically unbiased estimation of DFV -- and integrate Implicit Cross
Attention (ICA) guidance as well as Embedding Reinforcement (ER) to further
enhance editing quality. DFVEdit excels in practical efficiency, offering at
least 20x inference speed-up and 85\% memory reduction on Video DiTs compared
to attention-engineering-based editing methods. Extensive quantitative and
qualitative experiments demonstrate that DFVEdit can be seamlessly applied to
popular Video DiTs (e.g., CogVideoX and Wan2.1), attaining state-of-the-art
performance on structural fidelity, spatial-temporal consistency, and editing
quality.

</details>


### [57] [From Cradle to Cane: A Two-Pass Framework for High-Fidelity Lifespan Face Aging](https://arxiv.org/abs/2506.20977)
*Tao Liu,Dafeng Zhang,Gengchen Li,Shizhuo Liu,Yongqi Song,Senmao Li,Shiqi Yang,Boqian Li,Kai Wang,Yaxing Wang*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Face aging has become a crucial task in computer vision, with applications
ranging from entertainment to healthcare. However, existing methods struggle
with achieving a realistic and seamless transformation across the entire
lifespan, especially when handling large age gaps or extreme head poses. The
core challenge lies in balancing age accuracy and identity preservation--what
we refer to as the Age-ID trade-off. Most prior methods either prioritize age
transformation at the expense of identity consistency or vice versa. In this
work, we address this issue by proposing a two-pass face aging framework, named
Cradle2Cane, based on few-step text-to-image (T2I) diffusion models. The first
pass focuses on solving age accuracy by introducing an adaptive noise injection
(AdaNI) mechanism. This mechanism is guided by including prompt descriptions of
age and gender for the given person as the textual condition. Also, by
adjusting the noise level, we can control the strength of aging while allowing
more flexibility in transforming the face. However, identity preservation is
weakly ensured here to facilitate stronger age transformations. In the second
pass, we enhance identity preservation while maintaining age-specific features
by conditioning the model on two identity-aware embeddings (IDEmb): SVR-ArcFace
and Rotate-CLIP. This pass allows for denoising the transformed image from the
first pass, ensuring stronger identity preservation without compromising the
aging accuracy. Both passes are jointly trained in an end-to-end way. Extensive
experiments on the CelebA-HQ test dataset, evaluated through Face++ and Qwen-VL
protocols, show that our Cradle2Cane outperforms existing face aging methods in
age accuracy and identity consistency.

</details>


### [58] [3D Scene-Camera Representation with Joint Camera Photometric Optimization](https://arxiv.org/abs/2506.20979)
*Weichen Dai,Kangcheng Ma,Jiaxin Wang,Kecen Pan,Yuhang Ming,Hua Zhang,Wanzeng Kong*

Main category: cs.CV

> 本研究提出了一种新颖的三维场景-相机表示方法，结合相机光度优化，有效分离了场景无关信息，提升了三维场景表示的质量。

<details>
  <summary>Details</summary>

**Motivation:** 传统的多视图图像三维表示方法可能会因相机成像时的光度失真而导致图像质量下降，进而影响三维场景表示的质量。

**Method:** 引入内部和外部光度模型，提出了完整的光度模型及相应的相机表示，通过同步优化相机参数，防止三维场景表示拟合无关信息。

**Result:** 实验结果表明，该方法能够在图像退化条件下（如暗角和污迹）也能实现高质量的三维场景表示。

**Conclusion:** 所提出的方法能够有效改善由相机光度失真引起的问题，适用于广泛的成像退化情况下的三维场景表示。

**Abstract:** Representing scenes from multi-view images is a crucial task in computer
vision with extensive applications. However, inherent photometric distortions
in the camera imaging can significantly degrade image quality. Without
accounting for these distortions, the 3D scene representation may inadvertently
incorporate erroneous information unrelated to the scene, diminishing the
quality of the representation. In this paper, we propose a novel 3D
scene-camera representation with joint camera photometric optimization. By
introducing internal and external photometric model, we propose a full
photometric model and corresponding camera representation. Based on
simultaneously optimizing the parameters of the camera representation, the
proposed method effectively separates scene-unrelated information from the 3D
scene representation. Additionally, during the optimization of the photometric
parameters, we introduce a depth regularization to prevent the 3D scene
representation from fitting scene-unrelated information. By incorporating the
camera model as part of the mapping process, the proposed method constructs a
complete map that includes both the scene radiance field and the camera
photometric model. Experimental results demonstrate that the proposed method
can achieve high-quality 3D scene representations, even under conditions of
imaging degradation, such as vignetting and dirt.

</details>


### [59] [Rethink Sparse Signals for Pose-guided Text-to-image Generation](https://arxiv.org/abs/2506.20983)
*Wenjie Xuan,Jing Zhang,Juhua Liu,Bo Du,Dacheng Tao*

Main category: cs.CV

> 本文提出了一种新的Spatial-Pose ControlNet (SP-Ctrl)，以提高稀疏信号在姿态引导图像生成中的可控性，结果展示这种方法在动物和人像图像生成任务中性能优秀，并具有跨物种生成的能力。

<details>
  <summary>Details</summary>

**Motivation:** 当前研究倾向于使用密集信号来提供详细的姿态引导图像生成指导，但也带来了新的挑战，如编辑难度和与文本提示的潜在不一致。因此，研究者重新考虑使用稀疏信号，因其具有简单性和形状无关特性。

**Method:** 此论文提出了一种名为Spatial-Pose ControlNet (SP-Ctrl)的新方法，它增强了稀疏信号在引导图像生成中的控制能力。具体而言，该方法将OpenPose扩展为一种可学习的空间表示形式，使得关键点嵌入具有区分性和表现力，并引入了关键点概念学习，以改进姿态对齐。

**Result:** 实验结果表明，该方法在动物和人像图像生成任务中优于基于稀疏姿态引导的空间可控文本到图像生成方法，并且在性能上甚至与基于密集信号的方法相匹配。

**Conclusion:** 该论文的结论是SP-Ctrl展示了在通过稀疏信号进行多样性和跨物种生成中的良好能力。

**Abstract:** Recent works favored dense signals (e.g., depth, DensePose), as an
alternative to sparse signals (e.g., OpenPose), to provide detailed spatial
guidance for pose-guided text-to-image generation. However, dense
representations raised new challenges, including editing difficulties and
potential inconsistencies with textual prompts. This fact motivates us to
revisit sparse signals for pose guidance, owing to their simplicity and
shape-agnostic nature, which remains underexplored. This paper proposes a novel
Spatial-Pose ControlNet(SP-Ctrl), equipping sparse signals with robust
controllability for pose-guided image generation. Specifically, we extend
OpenPose to a learnable spatial representation, making keypoint embeddings
discriminative and expressive. Additionally, we introduce keypoint concept
learning, which encourages keypoint tokens to attend to the spatial positions
of each keypoint, thus improving pose alignment. Experiments on animal- and
human-centric image generation tasks demonstrate that our method outperforms
recent spatially controllable T2I generation approaches under sparse-pose
guidance and even matches the performance of dense signal-based methods.
Moreover, SP-Ctrl shows promising capabilities in diverse and cross-species
generation through sparse signals. Codes will be available at
https://github.com/DREAMXFAR/SP-Ctrl.

</details>


### [60] [EVA: Mixture-of-Experts Semantic Variant Alignment for Compositional Zero-Shot Learning](https://arxiv.org/abs/2506.20986)
*Xiao Zhang,Yongqiang Ma,Haodong Jing,Nanning Zheng*

Main category: cs.CV

> 引入EVA框架，解决现有CZSL方法在处理不同语义子集个体和组合差异方面的不足，显著提高在标准测试中的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有CZSL方法通过简单的组合原型映射来提取原始特征，这种方法对于划分成不同语义子集的个体是次优的，并且所有到一个的跨模式原始匹配忽略了相同状态或物体内的组合差异，限制了图像-组合的细微对齐。

**Method:** EVA框架，使用多专家适应来实现标记感知学习和高质量原始概念表示的建模，以及语义变体对齐来实现准确的图像-概念匹配。

**Result:** 该方法在三个流行的基准测试中，在封闭世界和开放世界设置下显著优于其他最先进的CZSL方法。

**Conclusion:** 研究提出的EVA框架证明了其对CZSL问题的适应性和有效性。

**Abstract:** Compositional Zero-Shot Learning (CZSL) investigates compositional
generalization capacity to recognize unknown state-object pairs based on
learned primitive concepts. Existing CZSL methods typically derive primitives
features through a simple composition-prototype mapping, which is suboptimal
for a set of individuals that can be divided into distinct semantic subsets.
Moreover, the all-to-one cross-modal primitives matching neglects compositional
divergence within identical states or objects, limiting fine-grained
image-composition alignment. In this study, we propose EVA, a
Mixture-of-Experts Semantic Variant Alignment framework for CZSL. Specifically,
we introduce domain-expert adaption, leveraging multiple experts to achieve
token-aware learning and model high-quality primitive representations. To
enable accurate compositional generalization, we further present semantic
variant alignment to select semantically relevant representation for
image-primitives matching. Our method significantly outperforms other
state-of-the-art CZSL methods on three popular benchmarks in both closed- and
open-world settings, demonstrating the efficacy of the proposed insight.

</details>


### [61] [Segment Anything in Pathology Images with Natural Language](https://arxiv.org/abs/2506.20988)
*Zhixuan Chen,Junlin Hou,Liqi Lin,Yihui Wang,Yequan Bie,Xi Wang,Yanning Zhou,Ronald Cheong Kin Chan,Hao Chen*

Main category: cs.CV

> 本论文介绍了PathSegmentor，一种用于病理解析图像的文本提示分割模型，其在准确度和适用性上超越了现有模型，有助于提升病理诊断的准确性和决策的可解释性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法在临床应用中面临问题，主要是由于注释数据有限和类别定义限制。为了克服这些限制，该研究提出了新的解决方案。

**Method:** 提出PathSegmentor，这是一种针对病理解析图像的首次采用文本提示的分割基础模型。同时引入了PathSeg数据集，这是最大的病理分割数据集，包含来自17个公开来源的275k张图像、掩模和标签，共计160类。此模型能够通过自然语言提示实现语义分割，不需要繁琐的空间输入。

**Result:** 实验表明，PathSegmentor在整体Dice评分上，分别超过现有的空间提示模型和文本提示模型0.145和0.429，显示出强大的鲁棒性和泛化能力。

**Conclusion:** 此工作促进了精准肿瘤学中可解释AI的发展，通过增强诊断模型的可解释性，为临床决策提供证据支持。

**Abstract:** Pathology image segmentation is crucial in computational pathology for
analyzing histological features relevant to cancer diagnosis and prognosis.
However, current methods face major challenges in clinical applications due to
limited annotated data and restricted category definitions. To address these
limitations, we propose PathSegmentor, the first text-prompted segmentation
foundation model designed specifically for pathology images. We also introduce
PathSeg , the largest and most comprehensive dataset for pathology
segmentation, built from 17 public sources and containing 275k image-mask-label
triples across 160 diverse categories. With PathSegmentor, users can perform
semantic segmentation using natural language prompts, eliminating the need for
laborious spatial inputs such as points or boxes. Extensive experiments
demonstrate that PathSegmentor outperforms specialized models with higher
accuracy and broader applicability, while maintaining a compact architecture.
It significantly surpasses existing spatial- and text-prompted models by 0.145
and 0.429 in overall Dice scores, respectively, showing strong robustness in
segmenting complex structures and generalizing to external datasets. Moreover,
PathSegmentor's outputs enhance the interpretability of diagnostic models
through feature importance estimation and imaging biomarker discovery, offering
pathologists evidence-based support for clinical decision-making. This work
advances the development of explainable AI in precision oncology.

</details>


### [62] [TSDASeg: A Two-Stage Model with Direct Alignment for Interactive Point Cloud Segmentation](https://arxiv.org/abs/2506.20991)
*Chade Li,Pengju Zhang,Yihong Wu*

Main category: cs.CV

> TSDASeg is a Two-Stage model with a Direct cross-modal Alignment module and memory module, designed for interactive point cloud segmentation, that achieves state-of-the-art performance.

<details>
  <summary>Details</summary>

**Motivation:** To address the poor performance of existing methods in point-level tasks due to missing direct 3D-text alignment, which hinders their ability to link local 3D features with textual context.

**Method:** We propose TSDASeg, which uses a Two-Stage model coupled with a Direct cross-modal Alignment module and memory module for interactive point cloud segmentation.

**Result:** Experiments on multiple 3D instruction, reference, and semantic segmentation datasets show that the proposed method achieves state-of-the-art performance.

**Conclusion:** By introducing direct cross-modal alignment and dynamic memory banks, TSDASeg effectively improves the consistency and performance of interactive point cloud segmentation across various scenarios.

**Abstract:** The rapid advancement of 3D vision-language models (VLMs) has spurred
significant interest in interactive point cloud processing tasks, particularly
for real-world applications. However, existing methods often underperform in
point-level tasks, such as segmentation, due to missing direct 3D-text
alignment, limiting their ability to link local 3D features with textual
context. To solve this problem, we propose TSDASeg, a Two-Stage model coupled
with a Direct cross-modal Alignment module and memory module for interactive
point cloud Segmentation. We introduce the direct cross-modal alignment module
to establish explicit alignment between 3D point clouds and textual/2D image
data. Within the memory module, we employ multiple dedicated memory banks to
separately store text features, visual features, and their cross-modal
correspondence mappings. These memory banks are dynamically leveraged through
self-attention and cross-attention mechanisms to update scene-specific features
based on prior stored data, effectively addressing inconsistencies in
interactive segmentation results across diverse scenarios. Experiments
conducted on multiple 3D instruction, reference, and semantic segmentation
datasets demonstrate that the proposed method achieves state-of-the-art
performance.

</details>


### [63] [Step-by-Step Video-to-Audio Synthesis via Negative Audio Guidance](https://arxiv.org/abs/2506.20995)
*Akio Hayakawa,Masato Ishii,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.CV

> 本研究提出了一种新的视频到音频生成方法，该方法顺序生成音频轨道并模仿传统的Foley声音制作流程。这一方法比现有技术能生成质量更高的音频内容。

<details>
  <summary>Details</summary>

**Motivation:** 我们的目标是提出一种方法，能够全面地捕捉由给定视频引起的全部声音事件，模仿传统的Foley工作流程。

**Method:** 我们提出了一种新的分步视频到音频生成方法，该方法顺序地产生单独的音频轨道，每个音频轨道对应视频中的一个特定声音事件。每一步的生成都被定义为一个指导式的视频到音频合成任务，受先前组合生成框架中概念否定的想法启发。为了实现这种指导式生成，我们引入了一个训练框架，该框架利用预训练的视频到音频模型，并消除了对特殊配对数据集的需求，允许在更易于获取的数据上进行训练。

**Result:** 实验结果表明，我们的方法能够为单个输入视频生成多个语义上不同的音频轨道，从而产生比现有基线更高的合成音频质量。

**Conclusion:** 该方法的结论是：通过我们的新方法，可以更精确地生成视频中相关的音频内容，提高了合成音频的质量，并展示了在广泛可用的数据上进行训练的能力。

**Abstract:** We propose a novel step-by-step video-to-audio generation method that
sequentially produces individual audio tracks, each corresponding to a specific
sound event in the video. Our approach mirrors traditional Foley workflows,
aiming to capture all sound events induced by a given video comprehensively.
Each generation step is formulated as a guided video-to-audio synthesis task,
conditioned on a target text prompt and previously generated audio tracks. This
design is inspired by the idea of concept negation from prior compositional
generation frameworks. To enable this guided generation, we introduce a
training framework that leverages pre-trained video-to-audio models and
eliminates the need for specialized paired datasets, allowing training on more
accessible data. Experimental results demonstrate that our method generates
multiple semantically distinct audio tracks for a single input video, leading
to higher-quality composite audio synthesis than existing baselines.

</details>


### [64] [DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting](https://arxiv.org/abs/2506.20998)
*Yeon-Ji Song,Jaein Kim,Byung-Ju Kim,Byoung-Tak Zhang*

Main category: cs.CV

> 提出了一种基于稀疏控制高斯图的运动感知动态视图合成方法（DBMovi-GS），用于从模糊的单目视频中合成动态场景视图，能够在动态模糊场景中实现稳健的新视图合成。

<details>
  <summary>Details</summary>

**Motivation:** 现有视图合成方法受限于对高分辨率图像的依赖或对静态几何和刚性场景假设的依赖，难以在包含动态物体和相机运动的真实环境中保持稳定性。因此，需要一个新方法来解决从模糊单目视频中合成动态场景视图的问题。

**Method:** 新的方法生成密集的三维高斯分布，能够从模糊视频中恢复清晰度并重建受动态变化影响的精细三维场景几何。

**Result:** 实验结果显示该方法能够稳健地在动态模糊场景中实现新视图的合成，并且在使用模糊单目视频输入的现实视图合成方面树立了新的标杆。

**Conclusion:** DBMovi-GS为从动态模糊单目视频中合成新视图提供了一种有效的解决方案，提升了实际环境中的表现。

**Abstract:** Novel view synthesis is a task of generating scenes from unseen perspectives;
however, synthesizing dynamic scenes from blurry monocular videos remains an
unresolved challenge that has yet to be effectively addressed. Existing novel
view synthesis methods are often constrained by their reliance on
high-resolution images or strong assumptions about static geometry and rigid
scene priors. Consequently, their approaches lack robustness in real-world
environments with dynamic object and camera motion, leading to instability and
degraded visual fidelity. To address this, we propose Motion-aware Dynamic View
Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting
(DBMovi-GS), a method designed for dynamic view synthesis from blurry monocular
videos. Our model generates dense 3D Gaussians, restoring sharpness from blurry
videos and reconstructing detailed 3D geometry of the scene affected by dynamic
motion variations. Our model achieves robust performance in novel view
synthesis under dynamic blurry scenes and sets a new benchmark in realistic
novel view synthesis for blurry monocular video inputs.

</details>


### [65] [Style-Aligned Image Composition for Robust Detection of Abnormal Cells in Cytopathology](https://arxiv.org/abs/2506.21001)
*Qiuyi Qi,Xin Li,Ming Kong,Zikang Xu,Bingdi Chen,Qiang Zhu,S Kevin Zhou*

Main category: cs.CV

> The paper presents the SAIC method to improve abnormal cell detection robustness and effectiveness in cytopathology by addressing data challenges. Synthetic images generated via SAIC boost detection performance, validating the method's clinical applicability.

<details>
  <summary>Details</summary>

**Motivation:** The motivation of the research is to address the challenges in training neural networks for abnormal cell detection due to lack of high-quality annotations, long-tailed data distributions, and inconsistent staining styles in cytopathology.

**Method:** This paper introduces the style-aligned image composition (SAIC) method to enhance the robustness and effectiveness of neural network detection for abnormal cells in cytopathology. The method selects an appropriate candidate from the abnormal cell bank based on attribute guidance, performs high-frequency feature reconstruction to align the style and maintain high fidelity of the composition, and uses a large vision-language model to filter high-quality synthesized images.

**Result:** Experimental results show that using SAIC-synthesized images significantly improves the detection performance for tail categories and styles, proving the method's enhancement in detection performance.

**Conclusion:** SAIC demonstrates its generalizability and practicality in enhancing the robustness and performance of detection models for abnormal cells. The research suggests that the SAIC method can achieve better performance in clinical applications compared to previous methods.

**Abstract:** Challenges such as the lack of high-quality annotations, long-tailed data
distributions, and inconsistent staining styles pose significant obstacles to
training neural networks to detect abnormal cells in cytopathology robustly.
This paper proposes a style-aligned image composition (SAIC) method that
composes high-fidelity and style-preserved pathological images to enhance the
effectiveness and robustness of detection models. Without additional training,
SAIC first selects an appropriate candidate from the abnormal cell bank based
on attribute guidance. Then, it employs a high-frequency feature reconstruction
to achieve a style-aligned and high-fidelity composition of abnormal cells and
pathological backgrounds. Finally, it introduces a large vision-language model
to filter high-quality synthesis images. Experimental results demonstrate that
incorporating SAIC-synthesized images effectively enhances the performance and
robustness of abnormal cell detection for tail categories and styles, thereby
improving overall detection performance. The comprehensive quality evaluation
further confirms the generalizability and practicality of SAIC in clinical
application scenarios. Our code will be released at
https://github.com/Joey-Qi/SAIC.

</details>


### [66] [Inverse Scene Text Removal](https://arxiv.org/abs/2506.21002)
*Takumi Yoshimatsu,Shumpei Takezaki,Seiichi Uchida*

Main category: cs.CV

> 本文研究了逆STR技术，目标是检测和分析STR处理过的图像，包括高精度的二分类检测，定位移除文本区域，并尝试恢复文本内容，以减少STR误用及改善其技术。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于STR技术的发展和误用风险增加，该研究的动机是开发一种方法来检测和分析STR处理过的图像，以减少误用，并进一步提升STR技术的准确性和安全性。通过IREA技术的研究，可以更好地理解和规范STR的应用。

**Method:** STR技术旨在从图像中移除文本元素，最初用于移除隐私敏感或不需要的文本，但如今也应用于排版图像。STR通常包括文本区域检测和随后的区域修复。尽管STR通过神经网络和合成数据取得了进展，但其误用风险也随之增加。本文研究了逆STR（ISTR），分析了经过STR处理的图像，重点关注二分类识别（检测图像是否经过STR处理）和定位移除的文本区域。我们实验表明，这些任务可以达到高准确度，以此来检测潜在误用并优化STR。我们也尝试通过训练文本识别器来恢复移除的文本内容，分析其难度。

**Result:** 实验表明二分类检测和定位移除的文本区域任务可以达到高准确度，且证明了通过训练文本识别器恢复移除的文本内容在技术上的可能性。这些结果有助于检测STR的误用，并指导STR技术的进一步改进。

**Conclusion:** 逆STR（ISTR）技术能够有效检测和定位经过STR处理的图像，并显示出通过训练文本识别器恢复文本内容的技术潜力。这对于防止STR的误用及完善STR技术都是有益的。

**Abstract:** Scene text removal (STR) aims to erase textual elements from images. It was
originally intended for removing privacy-sensitiveor undesired texts from
natural scene images, but is now also appliedto typographic images. STR
typically detects text regions and theninpaints them. Although STR has advanced
through neural networksand synthetic data, misuse risks have increased. This
paper investi-gates Inverse STR (ISTR), which analyzes STR-processed images
andfocuses on binary classification (detecting whether an image has un-dergone
STR) and localizing removed text regions. We demonstrate inexperiments that
these tasks are achievable with high accuracies, en-abling detection of
potential misuse and improving STR. We also at-tempt to recover the removed
text content by training a text recognizerto understand its difficulty.

</details>


### [67] [VisionGuard: Synergistic Framework for Helmet Violation Detection](https://arxiv.org/abs/2506.21005)
*Lam-Huy Nguyen,Thinh-Phuc Nguyen,Thanh-Hai Nguyen,Gia-Huy Dinh,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

> VisionGuard是一个多阶段框架，通过自适应标签和上下文扩展模块来改进摩托车驾驶员头盔佩戴情况的自动检测，解决了由于环境变化和数据不一致导致的检测难题，并在整体mAP上比基线检测器提高了3.1%。

<details>
  <summary>Details</summary>

**Motivation:** 为解决摩托车驾乘人员头盔佩戴情况自动检测中的环境变化、相机角度和数据不一致等问题，从而提高道路交通安全和管理系统的有效性。

**Method:** VisionGuard框架结合了自适应标签模块和上下文扩展模块，前者通过跟踪算法来修正分类错误，后者则通过生成虚拟边界框来提高代表性不足类别的召回率，共同解决了数据不平衡和标注不一致的问题。

**Result:** 实验结果显示，VisionGuard比基线检测器整体mAP提高了3.1%，表现出了良好的实际应用潜力。

**Conclusion:** VisionGuard框架在提升摩托车驾驶员头盔佩戴检测准确性上取得了显著效果，适合应用于实际的交通监控系统，促进道路安全和规章遵守。

**Abstract:** Enforcing helmet regulations among motorcyclists is essential for enhancing
road safety and ensuring the effectiveness of traffic management systems.
However, automatic detection of helmet violations faces significant challenges
due to environmental variability, camera angles, and inconsistencies in the
data. These factors hinder reliable detection of motorcycles and riders and
disrupt consistent object classification. To address these challenges, we
propose VisionGuard, a synergistic multi-stage framework designed to overcome
the limitations of frame-wise detectors, especially in scenarios with class
imbalance and inconsistent annotations. VisionGuard integrates two key
components: Adaptive Labeling and Contextual Expander modules. The Adaptive
Labeling module is a tracking-based refinement technique that enhances
classification consistency by leveraging a tracking algorithm to assign
persistent labels across frames and correct misclassifications. The Contextual
Expander module improves recall for underrepresented classes by generating
virtual bounding boxes with appropriate confidence scores, effectively
addressing the impact of data imbalance. Experimental results show that
VisionGuard improves overall mAP by 3.1% compared to baseline detectors,
demonstrating its effectiveness and potential for real-world deployment in
traffic surveillance systems, ultimately promoting safety and regulatory
compliance.

</details>


### [68] [Detection of Breast Cancer Lumpectomy Margin with SAM-incorporated Forward-Forward Contrastive Learning](https://arxiv.org/abs/2506.21006)
*Tyler Ward,Xiaoqin Wang,Braxton McFarland,Md Atik Ahamed,Sahar Nozad,Talal Arshad,Hafsa Nebbache,Jin Chen,Abdullah Imran*

Main category: cs.CV

> 本研究提出了一个结合Segment Anything Model (SAM)与Forward-Forward Contrastive Learning (FFCL)的深度学习框架，用于提升术中乳腺肿瘤边缘评估的准确性和速度，从而减少手术复发和再手术的需要。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有的2D手术样本放射影像在术中评估手术样本边缘状态过程中出现的准确性问题，减少近四分之一的患者需要额外手术的需要。

**Method:** 结合Segment Anything Model (SAM)与Forward-Forward Contrastive Learning (FFCL)的深度学习框架，用于2D手术样本放射影像中肿瘤边缘的分类和分割。首先，利用局部和全局对比学习的预训练策略对标注的样本影像进行训练，再利用训练好的ResNet-18骨干网络进行边界状态分类，之后利用粗略的二值掩码提示SAM进行精确的边缘分割。

**Result:** 该方法实现了0.8455的AUC值用于边界分类，分割边界时的Dice相似性比基线模型提高了27.4%，同时将推理时间减少到47毫秒/图像。

**Conclusion:** FFCL-SAM既提高了术中边缘评估的速度也提高了其准确性，有潜力减少再手术率，并改善乳腺癌治疗的手术结果。

**Abstract:** Complete removal of cancer tumors with a negative specimen margin during
lumpectomy is essential in reducing breast cancer recurrence. However, 2D
specimen radiography (SR), the current method used to assess intraoperative
specimen margin status, has limited accuracy, resulting in nearly a quarter of
patients requiring additional surgery. To address this, we propose a novel deep
learning framework combining the Segment Anything Model (SAM) with
Forward-Forward Contrastive Learning (FFCL), a pre-training strategy leveraging
both local and global contrastive learning for patch-level classification of SR
images. After annotating SR images with regions of known maligancy,
non-malignant tissue, and pathology-confirmed margins, we pre-train a ResNet-18
backbone with FFCL to classify margin status, then reconstruct coarse binary
masks to prompt SAM for refined tumor margin segmentation. Our approach
achieved an AUC of 0.8455 for margin classification and segmented margins with
a 27.4% improvement in Dice similarity over baseline models, while reducing
inference time to 47 milliseconds per image. These results demonstrate that
FFCL-SAM significantly enhances both the speed and accuracy of intraoperative
margin assessment, with strong potential to reduce re-excision rates and
improve surgical outcomes in breast cancer treatment. Our code is available at
https://github.com/tbwa233/FFCL-SAM/.

</details>


### [69] [The Aging Multiverse: Generating Condition-Aware Facial Aging Tree via Training-Free Diffusion](https://arxiv.org/abs/2506.21008)
*Bang Gong,Luchao Qi,Jiaye Wu,Zhicheng Fu,Chunbo Song,David W. Jacobs,John Nicholson,Roni Sengupta*

Main category: cs.CV

> 论文介绍了一个名为老化多元宇宙的框架，可以基于单一图像生成多条条件老化轨迹，无训练的扩散方法被用来平衡身份保持、年龄准确性及条件控制，此方法在多项性能指标上优于现有模型。

<details>
  <summary>Details</summary>

**Motivation:** 动机是开发一个框架，可以从单一图像生成多条基于环境、健康和生活方式等外部因素的面部老化轨迹，不同于先前方法将老化建模为单一确定路径，本方法创建了一个可以可视化多样化未来的老化树。

**Method:** 提出了一种无训练的扩散方法，该方法平衡了身份保持、年龄准确性以及条件控制。此方法的关键贡献包括注意力混合以调节编辑强度和模拟老化正则化策略以稳定编辑。

**Result:** 广泛的实验和用户研究表明，该方法在身份保持、老化真实性以及条件一致性方面表现出最先进的性能，超越了现有的编辑和年龄进展模型。

**Conclusion:** 通过将老化转化为一个多维的、可控制的、可解释的过程，该方法在数字故事讲述、健康教育和个人化可视化方面开辟了新的创作和实用途径。

**Abstract:** We introduce the Aging Multiverse, a framework for generating multiple
plausible facial aging trajectories from a single image, each conditioned on
external factors such as environment, health, and lifestyle. Unlike prior
methods that model aging as a single deterministic path, our approach creates
an aging tree that visualizes diverse futures. To enable this, we propose a
training-free diffusion-based method that balances identity preservation, age
accuracy, and condition control. Our key contributions include attention mixing
to modulate editing strength and a Simulated Aging Regularization strategy to
stabilize edits. Extensive experiments and user studies demonstrate
state-of-the-art performance across identity preservation, aging realism, and
conditional alignment, outperforming existing editing and age-progression
models, which often fail to account for one or more of the editing criteria. By
transforming aging into a multi-dimensional, controllable, and interpretable
process, our approach opens up new creative and practical avenues in digital
storytelling, health education, and personalized visualization.

</details>
