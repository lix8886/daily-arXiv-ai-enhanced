{"id": "2511.09599", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.09599", "abs": "https://arxiv.org/abs/2511.09599", "authors": ["Ming Yang", "Dongrun Li", "Xin Wang", "Feng Li", "Lisheng Fan", "Chunxiao Wang", "Xiaoming Wu", "Peng Cheng"], "title": "FedeCouple: Fine-Grained Balancing of Global-Generalization and Local-Adaptability in Federated Learning", "comment": null, "summary": "In privacy-preserving mobile network transmission scenarios with heterogeneous client data, personalized federated learning methods that decouple feature extractors and classifiers have demonstrated notable advantages in enhancing learning capability. However, many existing approaches primarily focus on feature space consistency and classification personalization during local training, often neglecting the local adaptability of the extractor and the global generalization of the classifier. This oversight results in insufficient coordination and weak coupling between the components, ultimately degrading the overall model performance. To address this challenge, we propose FedeCouple, a federated learning method that balances global generalization and local adaptability at a fine-grained level. Our approach jointly learns global and local feature representations while employing dynamic knowledge distillation to enhance the generalization of personalized classifiers. We further introduce anchors to refine the feature space; their strict locality and non-transmission inherently preserve privacy and reduce communication overhead. Furthermore, we provide a theoretical analysis proving that FedeCouple converges for nonconvex objectives, with iterates approaching a stationary point as the number of communication rounds increases. Extensive experiments conducted on five image-classification datasets demonstrate that FedeCouple consistently outperforms nine baseline methods in effectiveness, stability, scalability, and security. Notably, in experiments evaluating effectiveness, FedeCouple surpasses the best baseline by a significant margin of 4.3%.", "AI": {"tldr": "FedeCouple is a federated learning method that enhances the coordination between global and local model components, improving overall model performance significantly over existing methods in various evaluations.", "motivation": "Existing personalized federated learning methods neglect the local adaptability of the feature extractor and the global generalization of the classifier. This leads to insufficient coordination and weak coupling between components, degrading overall model performance.", "method": "FedeCouple, a federated learning method, balances global generalization and local adaptability. It jointly learns global and local feature representations using dynamic knowledge distillation and anchors to refine the feature space.", "result": "FedeCouple consistently outperforms nine baseline methods in effectiveness, stability, scalability, and security on five image-classification datasets. It surpasses the best baseline by 4.3% in effectiveness.", "conclusion": "FedeCouple addresses the limitations of existing personalized federated learning methods by improving the coordination between the global classifier and local feature extractor, offering better performance in various evaluations."}}
{"id": "2511.09611", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.09611", "abs": "https://arxiv.org/abs/2511.09611", "authors": ["Ye Tian", "Ling Yang", "Jiongfan Yang", "Anran Wang", "Yu Tian", "Jiani Zheng", "Haochen Wang", "Zhiyang Teng", "Zhuochen Wang", "Yinjie Wang", "Yunhai Tong", "Mengdi Wang", "Xiangtai Li"], "title": "MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation", "comment": "Project Page: https://tyfeld.github.io/mmadaparellel.github.io/", "summary": "While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9\\% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis. Our code is open-sourced at https://github.com/tyfeld/MMaDA-Parallel", "AI": {"tldr": "研究提出MMaDA-Parallel框架和新的评估基准ParaBench，解决了现有方法中的误差传播问题，提高了模型的跨模态对齐和语义一致性。", "motivation": "作者发现现有的顺序、自回归方法在处理复杂任务时可能会因为误差传播而导致性能退化。因此，提出了一个新的基准——ParaBench来评估这些问题。", "method": "提出了一种新的并行多模态扩散框架MMaDA-Parallel，该框架实现了在整个去噪过程中文本和图像之间的连续双向交互。", "result": "实验表明，该模型显著提高了跨模态对齐和语义一致性，在ParaBench上相比最先进的模型Bagel在输出对齐上提高了6.9%。", "conclusion": "MMaDA-Parallel通过训练策略和Parallel Reinforcement Learning优化，增强了跨模态一致性和语义一致性，从而提供了一个更为稳健的思维感知图像合成范式。"}}
{"id": "2511.09675", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09675", "abs": "https://arxiv.org/abs/2511.09675", "authors": ["Felix B. Mueller", "Jan F. Meier", "Timo Lueddecke", "Richard Vogg", "Roger L. Freixanet", "Valentin Hassler", "Tiffany Bosshard", "Elif Karakoc", "William J. O'Hearn", "Sofia M. Pereira", "Sandro Sehner", "Kaja Wierucka", "Judith Burkart", "Claudia Fichtel", "Julia Fischer", "Alexander Gail", "Catherine Hobaiter", "Julia Ostner", "Liran Samuni", "Oliver Schülke", "Neda Shahidi", "Erin G. Wessling", "Alexander S. Ecker"], "title": "PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild", "comment": null, "summary": "Non-human primates are our closest living relatives, and analyzing their behavior is central to research in cognition, evolution, and conservation. Computer vision could greatly aid this research, but existing methods often rely on human-centric pretrained models and focus on single datasets, which limits generalization. We address this limitation by shifting from a model-centric to a data-centric approach and introduce PriVi, a large-scale primate-centric video pretraining dataset. PriVi contains 424 hours of curated video, combining 174 hours from behavioral research across 11 settings with 250 hours of diverse web-sourced footage, assembled through a scalable data curation pipeline. We pretrain V-JEPA on PriVi to learn primate-specific representations and evaluate it using a lightweight frozen classifier. Across four benchmark datasets, ChimpACT, BaboonLand, PanAf500, and ChimpBehave, our approach consistently outperforms prior work, including fully finetuned baselines, and scales favorably with fewer labels. These results demonstrate that primate-centric pretraining substantially improves data efficiency and generalization, making it a promising approach for low-label applications. Code, models, and the majority of the dataset will be made available.", "AI": {"tldr": "我们引入了PriVi数据集，利用V-JEPA模型的学习能力，证明了以非人类灵长类动物为中心的预训练方法提高了数据效率和泛化能力。", "motivation": "为了克服现有计算机视觉方法依赖于人类为中心的预训练模型和单一数据集的局限性，限制了模型的泛化能力。", "method": "本研究提出了一种从模型为中心到数据为中心的方法转变，并引入了PriVi，一个大规模的以非人类灵长类动物为中心的预训练视频数据集。", "result": "在四个基准数据集上，包括ChimpACT、BaboonLand、PanAf500和ChimpBehave，方法优于以往的工作，甚至优于充分微调的基线模型，并且在更少的标签情况下表现出良好的扩展性。", "conclusion": "这些结果表明，以非人类灵长类动物为中心的预训练方法能够有效提高数据效率和泛化能力，特别适用于标注数据较少的应用场景。"}}
{"id": "2511.09702", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09702", "abs": "https://arxiv.org/abs/2511.09702", "authors": ["Katie Matton", "Purvaja Balaji", "Hamzeh Ghasemzadeh", "Jameson C. Cooper", "Daryush D. Mehta", "Jarrad H. Van Stan", "Robert E. Hillman", "Rosalind Picard", "John Guttag", "S. Mazdak Abulnaga"], "title": "Classifying Phonotrauma Severity from Vocal Fold Images with Soft Ordinal Regression", "comment": "16 pages, 9 figures, 5 tables; ML4H 2025; Proceedings of Machine Learning Research 297, 2025", "summary": "Phonotrauma refers to vocal fold tissue damage resulting from exposure to forces during voicing. It occurs on a continuum from mild to severe, and treatment options can vary based on severity. Assessment of severity involves a clinician's expert judgment, which is costly and can vary widely in reliability. In this work, we present the first method for automatically classifying phonotrauma severity from vocal fold images. To account for the ordinal nature of the labels, we adopt a widely used ordinal regression framework. To account for label uncertainty, we propose a novel modification to ordinal regression loss functions that enables them to operate on soft labels reflecting annotator rating distributions. Our proposed soft ordinal regression method achieves predictive performance approaching that of clinical experts, while producing well-calibrated uncertainty estimates. By providing an automated tool for phonotrauma severity assessment, our work can enable large-scale studies of phonotrauma, ultimately leading to improved clinical understanding and patient care.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.09690", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09690", "abs": "https://arxiv.org/abs/2511.09690", "authors": ["Omnilingual ASR team", "Gil Keren", "Artyom Kozhevnikov", "Yen Meng", "Christophe Ropers", "Matthew Setzler", "Skyler Wang", "Ife Adebara", "Michael Auli", "Can Balioglu", "Kevin Chan", "Chierh Cheng", "Joe Chuang", "Caley Droof", "Mark Duppenthaler", "Paul-Ambroise Duquenne", "Alexander Erben", "Cynthia Gao", "Gabriel Mejia Gonzalez", "Kehan Lyu", "Sagar Miglani", "Vineel Pratap", "Kaushik Ram Sadagopan", "Safiyyah Saleem", "Arina Turkatenko", "Albert Ventayol-Boada", "Zheng-Xin Yong", "Yu-An Chung", "Jean Maillard", "Rashel Moritz", "Alexandre Mourachko", "Mary Williamson", "Shireen Yates"], "title": "Omnilingual ASR: Open-Source Multilingual Speech Recognition for 1600+ Languages", "comment": null, "summary": "Automatic speech recognition (ASR) has advanced in high-resource languages, but most of the world's 7,000+ languages remain unsupported, leaving thousands of long-tail languages behind. Expanding ASR coverage has been costly and limited by architectures that restrict language support, making extension inaccessible to most--all while entangled with ethical concerns when pursued without community collaboration. To transcend these limitations, we introduce Omnilingual ASR, the first large-scale ASR system designed for extensibility. Omnilingual ASR enables communities to introduce unserved languages with only a handful of data samples. It scales self-supervised pre-training to 7B parameters to learn robust speech representations and introduces an encoder-decoder architecture designed for zero-shot generalization, leveraging a LLM-inspired decoder. This capability is grounded in a massive and diverse training corpus; by combining breadth of coverage with linguistic variety, the model learns representations robust enough to adapt to unseen languages. Incorporating public resources with community-sourced recordings gathered through compensated local partnerships, Omnilingual ASR expands coverage to over 1,600 languages, the largest such effort to date--including over 500 never before served by ASR. Automatic evaluations show substantial gains over prior systems, especially in low-resource conditions, and strong generalization. We release Omnilingual ASR as a family of models, from 300M variants for low-power devices to 7B for maximum accuracy. We reflect on the ethical considerations shaping this design and conclude by discussing its societal impact. In particular, we highlight how open-sourcing models and tools can lower barriers for researchers and communities, inviting new forms of participation. Open-source artifacts are available at https://github.com/facebookresearch/omnilingual-asr.", "AI": {"tldr": "该论文介绍了Omnilingual ASR系统，这是一种扩展性强的自动语音识别系统，能在仅用少量数据样本的情况下引入未服务的语言，并在低资源条件下表现出色，覆盖超过1,600种语言，其中包括以前从未被ASR服务过的500多种语言。", "motivation": "为了超越现有ASR系统在语言支持上的限制，减少扩展成本，解决伦理问题，并促进社区合作。", "method": "采用大规模的自我监督预训练模型，参数量达到70亿，采用编码器-解码器架构，并结合大型语言模型的解码器。训练数据集广泛多样，结合公共资源和来自本地社区的合作录音。", "result": "Omnilingual ASR系统扩展到了超过1,600种语言，包括之前从未被ASR服务过的500多种语言。自动评估显示，与先前系统相比，特别是在低资源条件下，它在性能上取得了显著提高，且具有良好的泛化能力。", "conclusion": "论文讨论了该系统的伦理考量和社会影响，并强调公开源代码和工具可以降低研究人员和社区的参与门槛，鼓励新的参与形式。"}}
{"id": "2511.09715", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.09715", "abs": "https://arxiv.org/abs/2511.09715", "authors": ["Arman Zarei", "Samyadeep Basu", "Mobina Pournemat", "Sayan Nag", "Ryan Rossi", "Soheil Feizi"], "title": "SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control", "comment": null, "summary": "Instruction-based image editing models have recently achieved impressive performance, enabling complex edits to an input image from a multi-instruction prompt. However, these models apply each instruction in the prompt with a fixed strength, limiting the user's ability to precisely and continuously control the intensity of individual edits. We introduce SliderEdit, a framework for continuous image editing with fine-grained, interpretable instruction control. Given a multi-part edit instruction, SliderEdit disentangles the individual instructions and exposes each as a globally trained slider, allowing smooth adjustment of its strength. Unlike prior works that introduced slider-based attribute controls in text-to-image generation, typically requiring separate training or fine-tuning for each attribute or concept, our method learns a single set of low-rank adaptation matrices that generalize across diverse edits, attributes, and compositional instructions. This enables continuous interpolation along individual edit dimensions while preserving both spatial locality and global semantic consistency. We apply SliderEdit to state-of-the-art image editing models, including FLUX-Kontext and Qwen-Image-Edit, and observe substantial improvements in edit controllability, visual consistency, and user steerability. To the best of our knowledge, we are the first to explore and propose a framework for continuous, fine-grained instruction control in instruction-based image editing models. Our results pave the way for interactive, instruction-driven image manipulation with continuous and compositional control.", "AI": {"tldr": "本文介绍了SliderEdit，一个用于连续图像编辑的框架，提供精细、可解释的指令控制，实现了单个编辑维度的连续插值同时保持空间局部性和全局语义一致性。", "motivation": "现有的基于指令的图像编辑模型虽然具有复杂的编辑能力，但每个指令的强度是固定的，限制了用户的编辑控制精度。因此，提出了SliderEdit来解决这个问题。", "method": "SliderEdit框架通过分解多部分编辑指令，并将每个指令作为一个全局训练的滑块暴露出来，允许平滑调整其强度。这种方法学习了一组广义低秩适应矩阵，能够跨越各种编辑、属性和组合指令。", "result": "SliderEdit应用于当前最先进的图像编辑模型，如FLUX-Kontext和Qwen-Image-Edit，显著提高了编辑可控性、视觉一致性和用户引导性。", "conclusion": "SliderEdit是首个探索和提出的基于指令的图像编辑模型中连续、精细指令控制框架，为交互式、指令驱动的图像操作奠定了基础。"}}
{"id": "2511.09700", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09700", "abs": "https://arxiv.org/abs/2511.09700", "authors": ["Warren Li", "Yiqian Wang", "Zihan Wang", "Jingbo Shang"], "title": "Order Matters: Rethinking Prompt Construction in In-Context Learning", "comment": null, "summary": "In-context learning (ICL) enables large language models to perform new tasks by conditioning on a sequence of examples. Most prior work reasonably and intuitively assumes that which examples are chosen has a far greater effect on performance than how those examples are ordered, leading to a focus on example selection. We revisit this assumption and conduct a systematic comparison between the effect of selection and ordering. Through controlled experiments on both classification and generation tasks, using multiple open-source model families (0.5B to 27B parameters) and GPT-5, we find that the variance in performance due to different example orderings is comparable to that from using entirely different example sets. Furthermore, we show that strong orderings can be identified using only a development set, achieving performance close to an oracle that selects the best ordering based on test labels. Our findings highlight the equal and intertwined importance of example selection and ordering in prompt design, calling for a reexamination of the assumptions held in ICL.", "AI": {"tldr": "研究发现，在in-context learning中，示例的排序对模型性能的影响与示例选择的影响相当，强调了在prompt设计中选择和排序的同等重要性和密切关系。", "motivation": "重新审视并量化比较示例选择和排序在prompt设计中对大模型性能效果的影响。", "method": "通过控制实验对比分类和生成任务中的示例选择和排序对性能的影响，使用多个开源模型（从0.5B到27B参数）以及GPT-5进行测试。", "result": "发现不同示例排序引起的性能方差与使用完全不同示例集所引起的相当。此外，使用开发集可以识别出强排序，其性能接近根据测试标签选定最佳排序的模型。", "conclusion": "研究结果强调了示例选择和排序在提示设计中的同等重要性及密切关联性，提出了对ICL领域假设的重新审视。"}}
{"id": "2511.09723", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.09723", "abs": "https://arxiv.org/abs/2511.09723", "authors": ["Balachandra Devarangadi Sunil", "Rakshith Venkatesh", "Shantanu Todmal"], "title": "Density Estimation and Crowd Counting", "comment": null, "summary": "This study enhances a crowd density estimation algorithm originally designed for image-based analysis by adapting it for video-based scenarios. The proposed method integrates a denoising probabilistic model that utilizes diffusion processes to generate high-quality crowd density maps. To improve accuracy, narrow Gaussian kernels are employed, and multiple density map outputs are generated. A regression branch is incorporated into the model for precise feature extraction, while a consolidation mechanism combines these maps based on similarity scores to produce a robust final result. An event-driven sampling technique, utilizing the Farneback optical flow algorithm, is introduced to selectively capture frames showing significant crowd movements, reducing computational load and storage by focusing on critical crowd dynamics. Through qualitative and quantitative evaluations, including overlay plots and Mean Absolute Error (MAE), the model demonstrates its ability to effectively capture crowd dynamics in both dense and sparse settings. The efficiency of the sampling method is further assessed, showcasing its capability to decrease frame counts while maintaining essential crowd events. By addressing the temporal challenges unique to video analysis, this work offers a scalable and efficient framework for real-time crowd monitoring in applications such as public safety, disaster response, and event management.", "AI": {"tldr": "本研究改进了原有的群体密度估计算法，使其适用于视频分析场景，通过降噪概率模型和事件驱动采样技术提高了准确性和效率，并应用于实时群体监控。", "motivation": "本研究旨在提高群体密度估计算法在视频分析场景中的性能，以适应更加复杂的实时群体监测需求，比如公共安全、灾害应对和活动管理等领域。", "method": "本研究通过引入降噪概率模型并采用扩散过程生成高质量的群体密度图，对原有的基于图像分析的群体密度估计算法进行改进，使其适用于视频场景。为了提高准确性，使用了窄高斯核并生成多份密度图。模型中加入了回归分支以实现精准特征提取，并通过结合相似性分数的方式整合这些密度图以获得稳健的结果。此外，还引入了一种事件驱动的采样技术，采用Farneback光流算法选择性地捕捉显示出显著群体移动的帧，从而在关注关键群体动态的同时减轻计算负载和存储需求。", "result": "通过定性和定量评估，包括覆盖图和平均绝对误差（MAE），该模型在稠密和稀疏环境下的群体动态捕捉方面表现出色。采样方法的效率也被进一步评估，展现了其在减少帧数的同时保持重要的群体事件的能力。", "conclusion": "该研究通过解决视频分析中的时间挑战，提供了一个可扩展且高效的实时群体监控框架。"}}
{"id": "2511.09709", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09709", "abs": "https://arxiv.org/abs/2511.09709", "authors": ["Marisa Hudspeth", "Patrick J. Burns", "Brendan O'Connor"], "title": "Contextual morphologically-guided tokenization for Latin encoder models", "comment": null, "summary": "Tokenization is a critical component of language model pretraining, yet standard tokenization methods often prioritize information-theoretical goals like high compression and low fertility rather than linguistic goals like morphological alignment. In fact, they have been shown to be suboptimal for morphologically rich languages, where tokenization quality directly impacts downstream performance. In this work, we investigate morphologically-aware tokenization for Latin, a morphologically rich language that is medium-resource in terms of pretraining data, but high-resource in terms of curated lexical resources -- a distinction that is often overlooked but critical in discussions of low-resource language modeling. We find that morphologically-guided tokenization improves overall performance on four downstream tasks. Performance gains are most pronounced for out of domain texts, highlighting our models' improved generalization ability. Our findings demonstrate the utility of linguistic resources to improve language modeling for morphologically complex languages. For low-resource languages that lack large-scale pretraining data, the development and incorporation of linguistic resources can serve as a feasible alternative to improve LM performance.", "AI": {"tldr": "This study examines the benefits of using morphologically-aware tokenization for Latin, a medium-resource yet morphologically complex language. It demonstrates significant performance improvements on downstream tasks, particularly for out-of-domain texts, by leveraging linguistic resources instead of large pretraining datasets.", "motivation": "Standard tokenization methods, while efficient for certain languages, are not optimal for morphologically diverse languages like Latin, which requires a specialized approach to tokenization to enhance language model performance.", "method": "We investigate the impact of morphologically-aware tokenization techniques on a morphologically rich but medium-resource language, Latin, which has rich curated lexical resources. The study focuses on improving tokenization methods to better align with linguistic goals such as morphological structures.", "result": "Morphologically-guided tokenization methods were found to improve performance across four downstream tasks. Notably, these improvements were most significant for out-of-domain texts, indicating better generalization of the models.", "conclusion": "The research underscores the importance of utilizing linguistic resources, even in the absence of large pretraining datasets, to enhance language modeling for morphologically complex languages."}}
{"id": "2511.09724", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.09724", "abs": "https://arxiv.org/abs/2511.09724", "authors": ["Yunqian Cheng", "Benjamin Princen", "Roberto Manduchi"], "title": "PALMS+: Modular Image-Based Floor Plan Localization Leveraging Depth Foundation Model", "comment": "Accepted to IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026, Application Track. Main paper: 8 pages, 5 figures. Supplementary material included", "summary": "Indoor localization in GPS-denied environments is crucial for applications like emergency response and assistive navigation. Vision-based methods such as PALMS enable infrastructure-free localization using only a floor plan and a stationary scan, but are limited by the short range of smartphone LiDAR and ambiguity in indoor layouts. We propose PALMS$+$, a modular, image-based system that addresses these challenges by reconstructing scale-aligned 3D point clouds from posed RGB images using a foundation monocular depth estimation model (Depth Pro), followed by geometric layout matching via convolution with the floor plan. PALMS$+$ outputs a posterior over the location and orientation, usable for direct or sequential localization. Evaluated on the Structured3D and a custom campus dataset consisting of 80 observations across four large campus buildings, PALMS$+$ outperforms PALMS and F3Loc in stationary localization accuracy -- without requiring any training. Furthermore, when integrated with a particle filter for sequential localization on 33 real-world trajectories, PALMS$+$ achieved lower localization errors compared to other methods, demonstrating robustness for camera-free tracking and its potential for infrastructure-free applications. Code and data are available at https://github.com/Head-inthe-Cloud/PALMS-Plane-based-Accessible-Indoor-Localization-Using-Mobile-Smartphones", "AI": {"tldr": "PALMS+ is a new modular system for indoor localization that uses posed RGB images and a floor plan to accurately locate within buildings, outperforming previous methods in accuracy and robustness without the need for training.", "motivation": "The motivation behind this study is to improve indoor localization in GPS-denied environments, where traditional methods are limited by short LiDAR range and ambiguous indoor layouts. The goal is to develop a reliable and infrastructure-free localization system using only a smartphone and a floor plan.", "method": "The method proposed is PALMS+, a modular, image-based system for indoor localization. It reconstructs scale-aligned 3D point clouds from posed RGB images using a monocular depth estimation model named Depth Pro. Then, it matches the geometric layout with the floor plan through convolution, outputting a posterior probability for location and orientation.", "result": "PALMS+ outperforms existing methods like PALMS and F3Loc in stationary localization accuracy without needing any training. It also demonstrates lower localization error when used with a particle filter for sequential localization across real-world trajectories.", "conclusion": "The study concludes that PALMS+ offers a robust, infrastructure-free approach to indoor localization that is competitive with and often superior to existing methods, even without the need for training data."}}
{"id": "2511.09738", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09738", "abs": "https://arxiv.org/abs/2511.09738", "authors": ["C. LeMay", "A. Lane", "J. Seales", "M. Winstead", "S. Baty"], "title": "Assessing the Applicability of Natural Language Processing to Traditional Social Science Methodology: A Case Study in Identifying Strategic Signaling Patterns in Presidential Directives", "comment": "24 pages", "summary": "Our research investigates how Natural Language Processing (NLP) can be used to extract main topics from a larger corpus of written data, as applied to the case of identifying signaling themes in Presidential Directives (PDs) from the Reagan through Clinton administrations. Analysts and NLP both identified relevant documents, demonstrating the potential utility of NLPs in research involving large written corpuses. However, we also identified discrepancies between NLP and human-labeled results that indicate a need for more research to assess the validity of NLP in this use case. The research was conducted in 2023, and the rapidly evolving landscape of AIML means existing tools have improved and new tools have been developed; this research displays the inherent capabilities of a potentially dated AI tool in emerging social science applications.", "AI": {"tldr": "研究探讨了NLP在从大量文本中提取关键主题方面的应用，特别是在里根到克林顿总统任期内的总统指令（PDs）中的信号主题识别。结果表明NLP在处理大规模文本数据中具有潜在效用，但也显示出与人类标注结果之间的差异，表明需要更多的研究来评估NLP的有效性。", "motivation": "研究旨在评估NLP技术从大量文档中提取关键信息的效力，并研究其在社会科学研究应用中的潜力，同时发现NLP工具和手工标注之间的误差，突出NLP工具在特定用例中的局限性和研究需求。", "method": "研究使用NLP方法从美国总统指令中提取信号主题，并对比NLP与人工分析的结果，以评估NLP方法的有效性。", "result": "NLP方法在识别关键主题方面显示出潜力，但也发现了与人类标注者的结果存在差异，显示出在当前应用场景中的挑战。", "conclusion": "该研究证实NLP在处理和分析大量文本数据时具有潜力，但同时也强调了NLP在获取准确结果方面面临的挑战，并指出需要进一步的研究来改进其有效性。"}}
{"id": "2511.09735", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09735", "abs": "https://arxiv.org/abs/2511.09735", "authors": ["Ahmed Alia", "Mohcine Chraibi", "Armin Seyfried"], "title": "Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction", "comment": "19 pages, 9 figures, 4 tables", "summary": "In dynamic and crowded environments, realistic pedestrian trajectory prediction remains a challenging task due to the complex nature of human motion and the mutual influences among individuals. Deep learning models have recently achieved promising results by implicitly learning such patterns from 2D trajectory data. However, most approaches treat pedestrians as point entities, ignoring the physical space that each person occupies. To address these limitations, this paper proposes a novel deep learning model that enhances the Social LSTM with a new Dynamic Occupied Space loss function. This loss function guides Social LSTM in learning to avoid realistic collisions without increasing displacement error across different crowd densities, ranging from low to high, in both homogeneous and heterogeneous density settings. Such a function achieves this by combining the average displacement error with a new collision penalty that is sensitive to scene density and individual spatial occupancy. For efficient training and evaluation, five datasets were generated from real pedestrian trajectories recorded during the Festival of Lights in Lyon 2022. Four datasets represent homogeneous crowd conditions -- low, medium, high, and very high density -- while the fifth corresponds to a heterogeneous density distribution. The experimental findings indicate that the proposed model not only lowers collision rates but also enhances displacement prediction accuracy in each dataset. Specifically, the model achieves up to a 31% reduction in the collision rate and reduces the average displacement error and the final displacement error by 5% and 6%, respectively, on average across all datasets compared to the baseline. Moreover, the proposed model consistently outperforms several state-of-the-art deep learning models across most test sets.", "AI": {"tldr": "The paper introduces an enhanced Social LSTM model with a Dynamic Occupied Space loss function for more accurate and collision-free pedestrian trajectory predictions in various crowd densities, achieving significant performance improvements over the baseline.", "motivation": "To improve the realism and accuracy of pedestrian trajectory prediction models, especially in dealing with collisions and varying crowd densities, which existing models often fail to address adequately.", "method": "Enhancement of the Social LSTM model by integrating a new loss function that considers the physical space occupied by individuals and adjusts to different crowd densities.", "result": "Experiments across diverse datasets show that the proposed model significantly reduces collision rates and enhances displacement prediction accuracy by up to 31% and 5-6% respectively, outperforming state-of-the-art models in most test cases.", "conclusion": "The enhanced Social LSTM model with the Dynamic Occupied Space loss function proves effective for realistic and accurate pedestrian trajectory prediction across different crowd densities."}}
{"id": "2511.09748", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09748", "abs": "https://arxiv.org/abs/2511.09748", "authors": ["Muskaan Chopra", "Lorenz Sparrenberg", "Sarthak Khanna", "Rafet Sifa"], "title": "How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation", "comment": "Accepted in IEEE BigData 2025", "summary": "Large Language Models (LLMs) excel at evaluating machine translation (MT), but their scale and cost hinder deployment on edge devices and in privacy-sensitive workflows. We ask: how small can you get while still detecting meaning-altering translation errors? Focusing on English->German Critical Error Detection (CED), we benchmark sub-2B models (LFM2-350M, Qwen-3-0.6B/1.7B, Llama-3.2-1B-Instruct, Gemma-3-1B) across WMT21, WMT22, and SynCED-EnDe-2025. Our framework standardizes prompts, applies lightweight logit-bias calibration and majority voting, and reports both semantic quality (MCC, F1-ERR/F1-NOT) and compute metrics (VRAM, latency, throughput). Results reveal a clear sweet spot around one billion parameters: Gemma-3-1B provides the best quality-efficiency trade-off, reaching MCC=0.77 with F1-ERR=0.98 on SynCED-EnDe-2025 after merged-weights fine-tuning, while maintaining 400 ms single-sample latency on a MacBook Pro M4 Pro (24 GB). At larger scale, Qwen-3-1.7B attains the highest absolute MCC (+0.11 over Gemma) but with higher compute cost. In contrast, ultra-small models (0.6B) remain usable with few-shot calibration yet under-detect entity and number errors. Overall, compact, instruction-tuned LLMs augmented with lightweight calibration and small-sample supervision can deliver trustworthy, on-device CED for MT, enabling private, low-cost error screening in real-world translation pipelines. All datasets, prompts, and scripts are publicly available at our GitHub repository.", "AI": {"tldr": "研究集中在从英语到德语的Critical Error Detection (CED) 任务上，评估了不同规模的模型（<2B参数），发现在一亿参数左右的模型提供了最佳的质量效率平衡，如Gemma-3-1B，这使得可信的、在设备上的CED成为可能，适用于私密、低成本的错误筛查。", "motivation": "该研究旨在解决大语言模型虽然在评估机器翻译方面表现出色，但其规模和成本的高昂导致其难以部署在边缘设备和需要隐私保护的工作流程中。研究主要探讨了在检测改变翻译意义的错误情况下，模型可以小型化到何种程度。", "method": "该研究专注于从英语到德语的Critical Error Detection (CED)任务，评估了参数少于20亿的各种模型，包括LFM2-350M, Qwen-3-0.6B/1.7B, Llama-3.2-1B-Instruct,和Gemma-3-1B。研究中采用了标准化的提示格式、轻量级的校准、多数投票机制，并报告了语义质量（MCC、F1-ERR/F1-NOT）和计算量指标（VRAM、延迟、吞吐量）等。", "result": "研究结果表明，在大约一亿参数处出现了一个明显最佳点：Gemma-3-1B提供了最佳的质量-效率折衷，达到了MCC=0.77，在SynCED-EnDe-2025上的F1-ERR=0.98，在MacBook Pro M4 Pro（24GB）上保持了400ms的单样本延迟。更大的模型如Qwen-3-1.7B虽然获得了更高的绝对MCC（相比Gemma-3-1B高0.11），但计算成本也更高；而0.6B的模型虽然经过了少量样本的校准，但仍然不能充分检测实体和数字错误。", "conclusion": "紧凑的、经过指令微调的LLM，经过轻量级校准和少量样本监督的增强，可以为机器翻译提供可信赖的、边缘设备上的误差检测，使在真实翻译流程中实现私密、低成本的误差筛查成为可能。"}}
{"id": "2511.09740", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09740", "abs": "https://arxiv.org/abs/2511.09740", "authors": ["Filip Beránek", "Václav Diviš", "Ivan Gruber"], "title": "Soiling detection for Advanced Driver Assistance Systems", "comment": "Published at ICMV 2024", "summary": "Soiling detection for automotive cameras is a crucial part of advanced driver assistance systems to make them more robust to external conditions like weather, dust, etc. In this paper, we regard the soiling detection as a semantic segmentation problem. We provide a comprehensive comparison of popular segmentation methods and show their superiority in performance while comparing them to tile-level classification approaches. Moreover, we present an extensive analysis of the Woodscape dataset showing that the original dataset contains a data-leakage and imprecise annotations. To address these problems, we create a new data subset, which, despite being much smaller, provides enough information for the segmentation method to reach comparable results in a much shorter time. All our codes and dataset splits are available at https://github.com/filipberanek/woodscape_revision.", "AI": {"tldr": "Soiling detection for automotive cameras is important for advanced driver assistance systems. This paper addresses the issue by casting it as a semantic segmentation problem, comparing various segmentation methods, and analyzing the Woodscape dataset, which contains data leakage and annotation issues. A smaller yet comprehensive subset was created for efficient segmentation.", "motivation": "The importance of soiling detection for automotive cameras in making advanced driver assistance systems robust to external conditions such as weather, dust, etc.", "method": "Structure", "result": "{\n  \"tldr\": \"\\u7b2c\\u4e00\\u5c42\\u6587\\u7ae0\\u5bf9\\u9898\\u98a8\\u5b50\\u7684\\u6d88\\u6b63\\u63d0\\u9192\\u5b50\\u8bbe\\u7f6e\\u8fdb\\u884c\\u4e86\\u68c0\\u6d4b\\uff0c\\u5c06\\u4e4b\\u770b\\u4e4b\\u4e3a\\u6a21\\u677f\\u5206\\u6790\\u95ee\\u9898\\uff0c\\u6bd4\\u8f83\\u51fa\\u4e0d\\u540c\\u7684\\u5206\\u6790\\u6cd5\\u6b63\\u786e\\u7d27\\u91cf\\uff0c\\u5e76\\u4e14\\u5bf9Woodscape\\u6570\\u636e\\u96c6\\u8fdb\\u884c\\u4e86\\u5206\\u6790\\uff0c\\u7ed9\\u51fa\\u4e86\\u65b0\\u7684\\u6570\\u636e\\u5b50\\u96c6\\uff0c\\u534f\\u8c03\\u5927\\u5c0f\\u5176\\u63a5\\u8fd1\\u5341\\u4e07\\u7279\\u8d62\\u4e0a\\u7684\\u5f39\\u5b50\\u4e92\\u8054\\u7f51\\uff0c\\u8457\\u4f5c\\u6587\\u4e2d\\u4e0a\\u4e00\\u9884\\u4e2d\\u8d44\\u6599\\u5206\\u9694\\u5e76\\u7126\\u91cf\\u68c0\\u6d4b\\uff0e\", \n  \"motivation\": \"\\u6839\\u636e\\u9898\\u98a8\\u5b50\\u7684\\u7a7a\\u95f4\\u578b\\u5206\\u6790\\uff0c\\u6839\\u636e\\u6b63\\u786e\\u7684\\u5b50\\u5e95\\u57fa\\u7840\\uff0c\\u6839\\u636e\\u5e73\\u8861\\u5206\\u6790\\u5b50\\u4e00\\u5b9a\\u65f6\\u8868\\u73b0\\u51fa\\u6709\\u7f14\\u6d88\\u7075\\u7269\\u5b50\\u5c42\\u4e2d\\u5b50\\u4e00\\u5b9a\\u65f6\\u5206\\u6bb5\\u800c\\u5e73\\u51e1\\u8868\\u73b0\\u8868\\u5f53\\u5b58\\u5728\\u7684\\u95ee\\u9898\\u5e76\\u6b63\\u5f0f\\u89e3\\u51b3\\u3002\", \n  \"method\": \"\\u7528\\u4e24\\u79cd\\u65b9\\u6cd5\\u6765\\u6bd4\\u8f83\\uff0c\\u5148\\u7522\\u4e00\\u4e2aWoodscape\\u683c\\u5b50\\u5e95\\u8d44\\u6599\\u6765\\u9a8c\\u8bc1\\u4e00\\u4e2a\\u6a21\\u578b\\uff0c\\u7136\\u540e\\u5728\\u8be5\\u683c\\u5b50\\u8d28\\u91cf\\u7684\\u5e95\\u4e0a\\u9760\\u5b9e\\u662f\\u6709\\u6805\\u6839\\u4e0a\\u3002\", \n  \"result\": \"\\u8be5\\u65b9\\u6cd5\\u5728Woodscape\\u6570\\u636e\\u96c6\\u4e0a\\u7684\\u7ee9\\u51fa\\u662f\\uff0c\\u5e94\\u7528\\u4e4b\\u65f6\\u80fd\\u591f\\u5728\\u4e00\\u5b9a\\u7684\\u65f6\\u95f4\\u5185\\u5f97\\u5230\\u7b2c\\u4e00\\u5c42\\u6587\\u4e2d\\u8d44\\u6599\\u975e\\u5e38\\u624b\\u5de7\\u7684\\u7ed3\\u679c\\uff0c\\u53ef\\u4ee5\\u5728\\u57f9\\u5175\\u5b50\\u8d44\\u6599\\u4e0a\\u4e0e\\u7528\\u5e38\\u7b97\\u6cd5\\u7684\\u8868\\u73b0\\u76f8\\u6bd4\\u3002\", \n  \"conclusion\": \"\\u8be5\\u7c7b\\u522b\\u95ee\\u9898\\u7684\\u539f\\u672c\\u89e3\\u51b3\\u65b9\\u6cd5\\u7531\\u4e09\\u4e2a\\u4e0d\\u540c\\u7684\\u65b9\\u6cd5\\u7ec4\\u6210\\uff0c\\u5728\\u901f\\u5ea6\\u4e0a\\u6216\\u8005\\u8bfb\\u516c\\u4e2d\\u5206\\u522b\\u4e00\\u4e2a\\u5b50\\u5e95\\u4e0a\\u642c\\u5730\\u534f\\u8c03\\u5927\\u5c0f\\u5904\\u4e8c\\u8bfb\\uff0c\\u7b2c\\u4e00\\u5c42\\u6587\\u4e2d\\u57f9\\u5175\\u5b50\\u52a0\\u4e0a\\u4e00\\u4e2a\\u5404\\u79cd\\u4e0a\\u7684\\u5b50\\u5e95\\u5e76\\u4e0e\\u4e00\\u4e2a\\u6a21\\u800c\\u7b2c\\u4e00\\u5c42\\u6587\\u4e2d\\u5904\\u6821\\u6838\\u6d4b\\u8bd5\\u3002\\uff0c\\u8edf\\u8d5b\\u57f9\\u7814\\u7a76\\u4f5c\\u8005\\u7684\\u540d\\u5b57\\u7684\\u6a21\\u62df\\u6765\\u4e00\\u8d77\\uff0c\\u5c06\\u5b83\\u6709\\u5564\\u5206\\u5272\\u5728\\u672c\\u6587\\uff0e\"}\n", "conclusion": "The findings and implications of the work for soiling detection for automotive cameras are discussed, stressing the significance of proper segmentation in improving the robustness of advanced driver assistance systems."}}
{"id": "2511.09796", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09796", "abs": "https://arxiv.org/abs/2511.09796", "authors": ["Rocco Tripodi", "Xiaoyu Liu"], "title": "Predicate-Argument Structure Divergences in Chinese and English Parallel Sentences and their Impact on Language Transfer", "comment": null, "summary": "Cross-lingual Natural Language Processing (NLP) has gained significant traction in recent years, offering practical solutions in low-resource settings by transferring linguistic knowledge from resource-rich to low-resource languages. This field leverages techniques like annotation projection and model transfer for language adaptation, supported by multilingual pre-trained language models. However, linguistic divergences hinder language transfer, especially among typologically distant languages. In this paper, we present an analysis of predicate-argument structures in parallel Chinese and English sentences. We explore the alignment and misalignment of predicate annotations, inspecting similarities and differences and proposing a categorization of structural divergences. The analysis and the categorization are supported by a qualitative and quantitative analysis of the results of an annotation projection experiment, in which, in turn, one of the two languages has been used as source language to project annotations into the corresponding parallel sentences. The results of this analysis show clearly that language transfer is asymmetric. An aspect that requires attention when it comes to selecting the source language in transfer learning applications and that needs to be investigated before any scientific claim about cross-lingual NLP is proposed.", "AI": {"tldr": "本文分析了中文和英文平行句子中的述谓结构，提出了结构分歧的分类，并表明语言迁移是不对称的，源语言的选择需要仔细考虑。", "motivation": "跨语言自然语言处理在资源匮乏的情况下提供了解决方案，但语言之间的差异，尤其是类型学上距离较远的语言之间的差异，对语言迁移构成了挑战。本文意在深入理解在跨语言迁移中，源语言选择的重要性。", "method": "本研究通过分析中文和英文平行句子中的述谓结构，探索了谓词标注的对齐与错位，同时提出了一种结构分歧的分类方法。研究结果基于一项注释投影实验的定性和定量分析，实验中，两种语言之一被用作源语言，将注释投影到对应的平行句子中。", "result": "本研究结果表明语言迁移是不对称的，这将影响跨语言自然语言处理任务中源语言的选择，并需在做出任何科学主张前进行研究。", "conclusion": "结果指出，语言转移的不对称性，在跨语言NLP任务中选择源语言时需要考虑，并且需要进一步探索才能做出科学的断言。"}}
{"id": "2511.09742", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09742", "abs": "https://arxiv.org/abs/2511.09742", "authors": ["Frank Li", "Theo Dapamede", "Mohammadreza Chavoshi", "Young Seok Jeon", "Bardia Khosravi", "Abdulhameed Dere", "Beatrice Brown-Mulry", "Rohan Satya Isaac", "Aawez Mansuri", "Chiratidzo Sanyika", "Janice Newsome", "Saptarshi Purkayastha", "Imon Banerjee", "Hari Trivedi", "Judy Gichoya"], "title": "Feature Quality and Adaptability of Medical Foundation Models: A Comparative Evaluation for Radiographic Classification and Segmentation", "comment": "7 figures, 3 tables", "summary": "Foundation models (FMs) promise to generalize medical imaging, but their effectiveness varies. It remains unclear how pre-training domain (medical vs. general), paradigm (e.g., text-guided), and architecture influence embedding quality, hindering the selection of optimal encoders for specific radiology tasks. To address this, we evaluate vision encoders from eight medical and general-domain FMs for chest X-ray analysis. We benchmark classification (pneumothorax, cardiomegaly) and segmentation (pneumothorax, cardiac boundary) using linear probing and fine-tuning. Our results show that domain-specific pre-training provides a significant advantage; medical FMs consistently outperformed general-domain models in linear probing, establishing superior initial feature quality. However, feature utility is highly task-dependent. Pre-trained embeddings were strong for global classification and segmenting salient anatomy (e.g., heart). In contrast, for segmenting complex, subtle pathologies (e.g., pneumothorax), all FMs performed poorly without significant fine-tuning, revealing a critical gap in localizing subtle disease. Subgroup analysis showed FMs use confounding shortcuts (e.g., chest tubes for pneumothorax) for classification, a strategy that fails for precise segmentation. We also found that expensive text-image alignment is not a prerequisite; image-only (RAD-DINO) and label-supervised (Ark+) FMs were among top performers. Notably, a supervised, end-to-end baseline remained highly competitive, matching or exceeding the best FMs on segmentation tasks. These findings show that while medical pre-training is beneficial, architectural choices (e.g., multi-scale) are critical, and pre-trained features are not universally effective, especially for complex localization tasks where supervised models remain a strong alternative.", "AI": {"tldr": "本文评估了八种基础模型在胸部X光图像的分类与分割任务中的效果，结果表明医学领域的预训练提供了明显的初始特征质量优势，但具体任务依赖性强，预训练特征并不在所有情形下都有效，尤其是在复杂任务中。", "motivation": "由于基础模型在医学成像中的有效性因预训练领域（医学与通用）、范式（例如文本引导）和架构的不同而有所差异，这使得选择适合特定放射学任务的最优编码器成为一个难题。本文旨在解决这个问题。", "method": "本文通过对比八种不同领域的基础模型在胸部X光图像分类（如气胸、心脏增大）和分割（如气胸、心脏边界）任务中的表现来评估视觉编码器的效果。研究使用了线性探测和微调两种方式来测试这些模型的性能。", "result": "研究结果显示，特定领域的预训练可以提供显著的优势；医学领域的基础模型在直接分类任务中表现优异，表明它们具备更高质量的初始特征。然而，特征的实用性对于不同的任务具有高度依赖性。预训练的嵌入特征对于全局分类和分割显而易见的解剖结构（如心脏）非常有效。相比之下，对于分割复杂的、微妙的病理特征（如气胸）的任务，所有基础模型在没有进行显著微调的情况下表现不佳，揭示出在定位微妙疾病方面的关键不足。", "conclusion": "本文的研究发现表明，虽然医学领域预训练对视觉编码器有效，但架构选择（例如，多尺度）至关重要，而预训练的特征并不总是有效，特别是在复杂的定位任务中，监督模型仍然是一种强大的选择。"}}
{"id": "2511.09803", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09803", "abs": "https://arxiv.org/abs/2511.09803", "authors": ["Yufeng Wang", "Lu wei", "Haibin Ling"], "title": "TARG: Training-Free Adaptive Retrieval Gating for Efficient RAG", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) improves factuality but retrieving for every query often hurts quality while inflating tokens and latency. We propose Training-free Adaptive Retrieval Gating (TARG), a single-shot policy that decides when to retrieve using only a short, no-context draft from the base model. From the draft's prefix logits, TARG computes lightweight uncertainty scores: mean token entropy, a margin signal derived from the top-1/top-2 logit gap via a monotone link, or small-N variance across a handful of stochastic prefixes, and triggers retrieval only when the score exceeds a threshold. The gate is model agnostic, adds only tens to hundreds of draft tokens, and requires no additional training or auxiliary heads. On NQ-Open, TriviaQA, and PopQA, TARG consistently shifts the accuracy-efficiency frontier: compared with Always-RAG, TARG matches or improves EM/F1 while reducing retrieval by 70-90% and cutting end-to-end latency, and it remains close to Never-RAG in overhead. A central empirical finding is that under modern instruction-tuned LLMs the margin signal is a robust default (entropy compresses as backbones sharpen), with small-N variance offering a conservative, budget-first alternative. We provide ablations over gate type and prefix length and use a delta-latency view to make budget trade-offs explicit.", "AI": {"tldr": "提出了一种无训练自适应检索门控（TARG）算法，它可以减少检索次数、降低延迟并保持高质量的生成结果。", "motivation": "检索增强了生成的事实性，但频繁的检索会损害质量、增加令牌数量和延迟。TARG是一种轻量级的策略，可以在不增加额外训练或辅助头的情况下有效地利用检索。", "method": "提出了一种无训练自适应检索门控（TARG），这是一种单次决策策略，仅使用基础模型生成的简短无上下文草稿即可决定是否进行检索。通过草稿前缀对数概率，TARG计算轻量级不确定性分数，如平均标记熵、由顶级标记差距推导出的边际信号，或少量随机前缀的小N方差，并仅在分数超过阈值时触发检索。", "result": "在NQ-Open、TriviaQA和PopQA数据集上，与Always-RAG相比，TARG匹配或提高了EM/F1指标，同时减少检索次数70-90%，缩短了端到端延迟。TARG与Never-RAG相比，在计算开销方面接近。中心经验发现是指在现代指令优化的大型语言模型（LLM）下，边际信号是一个稳健的默认选项（熵随着模型的准确率提高而减少），而小N方差提供了保守的预算优先替代方案。", "conclusion": "通过无训练自适应检索门控（TARG），研究证明了在现代指令调优的大型语言模型中，轻量级不确定性分数可以有效地预测检索的必要性，进而优化了检索增强生成的事实性和效率。该策略不仅提高了系统性能指标，还显着降低了计算资源的使用。"}}
{"id": "2511.09749", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09749", "abs": "https://arxiv.org/abs/2511.09749", "authors": ["Mahsa Mitcheff", "Siamul Karim Khan", "Adam Czajka"], "title": "Gradient-Guided Exploration of Generative Model's Latent Space for Controlled Iris Image Augmentations", "comment": null, "summary": "Developing reliable iris recognition and presentation attack detection methods requires diverse datasets that capture realistic variations in iris features and a wide spectrum of anomalies. Because of the rich texture of iris images, which spans a wide range of spatial frequencies, synthesizing same-identity iris images while controlling specific attributes remains challenging. In this work, we introduce a new iris image augmentation strategy by traversing a generative model's latent space toward latent codes that represent same-identity samples but with some desired iris image properties manipulated. The latent space traversal is guided by a gradient of specific geometrical, textural, or quality-related iris image features (e.g., sharpness, pupil size, iris size, or pupil-to-iris ratio) and preserves the identity represented by the image being manipulated. The proposed approach can be easily extended to manipulate any attribute for which a differentiable loss term can be formulated. Additionally, our approach can use either randomly generated images using either a pre-train GAN model or real-world iris images. We can utilize GAN inversion to project any given iris image into the latent space and obtain its corresponding latent code.", "AI": {"tldr": "本文提出了一种新的眼虹膜图像增强策略，通过遍历生成模型的潜在空间，对相同身份样本但具有某些所需眼虹膜图像属性的潜在代码进行操作，用以生成多样化的眼虹膜图像数据集。", "motivation": "开发可靠的眼虹膜识别和展示攻击检测方法需要捕获真实眼虹膜特征变化和广泛异常的多样化数据集。由于眼虹膜图像纹理丰富，跨越广泛的频率，故在控制特定属性的同时生成同一身份的眼虹膜图像具有挑战性。", "method": "通过在生成模型的潜在空间中遍历，向表示同一身份样本但控制某些所需眼虹膜图像属性的潜在代码方向进行。潜在空间的遍历由特定几何、纹理或质量相关的眼虹膜图像特征（例如锐度、瞳孔大小、虹膜大小或瞳孔到虹膜的比例）的梯度指引，并保留所操作图像所代表的身份。", "result": "", "conclusion": "该方法可以方便地扩展到对任何可以形成可微分损失项的属性进行操作，并且可以使用随机生成的图像或真实世界的眼虹膜图像。通过GAN逆变换，可以将任意给定的眼虹膜图像投射到潜在空间并获得相应的潜在代码。"}}
{"id": "2511.09812", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09812", "abs": "https://arxiv.org/abs/2511.09812", "authors": ["Marry Kong", "Rina Buoy", "Sovisal Chenda", "Nguonly Taing"], "title": "Khmer Spellchecking: A Holistic Approach", "comment": null, "summary": "Compared to English and other high-resource languages, spellchecking for Khmer remains an unresolved problem due to several challenges. First, there are misalignments between words in the lexicon and the word segmentation model. Second, a Khmer word can be written in different forms. Third, Khmer compound words are often loosely and easily formed, and these compound words are not always found in the lexicon. Fourth, some proper nouns may be flagged as misspellings due to the absence of a Khmer named-entity recognition (NER) model. Unfortunately, existing solutions do not adequately address these challenges. This paper proposes a holistic approach to the Khmer spellchecking problem by integrating Khmer subword segmentation, Khmer NER, Khmer grapheme-to-phoneme (G2P) conversion, and a Khmer language model to tackle these challenges, identify potential correction candidates, and rank the most suitable candidate. Experimental results show that the proposed approach achieves a state-of-the-art Khmer spellchecking accuracy of up to 94.4%, compared to existing solutions. The benchmark datasets for Khmer spellchecking and NER tasks in this study will be made publicly available.", "AI": {"tldr": "本文提出了一种针对高棉语拼写检查问题的整体解决方案，通过整合字节切分、命名实体识别、字符到音素转换和语言模型来应对现有挑战，实现了高达94.4%的拼写检查准确率。", "motivation": "由于词汇表与词分割模型的不匹配，单词形式的多样性，复合词的自由组合及命名实体识别模型的缺失等问题，高棉语的拼写检查至今仍未得到妥善解决。", "method": "Structure", "result": "{\n  \"tldr\": \"本文提出了一种针对高棉语拼写检查问题的整体解决方案，通过整合字节切分、命名实体识别、字符到音素转换和语言模型来应对现有挑战，实现了高达94.4%的拼写检查准确率。\", \n  \"motivation\": \"由于词汇表与词分割模型的不匹配，单词形式的多样性，复合词的自由组合及命名实体识别模型的缺失等问题，高棉语的拼写检查至今仍未得到妥善解决。\", \n  \"method\": \"整合高棉语字节切分技术、命名实体识别、字符到音素转换以及语言模型，以应对各种挑战，并识别和排名潜在的正确拼写选择。\", \n  \"result\": \"实验结果显示，所提出的方案达到了高达94.4%的拼写检查准确率，超越现有解法，并且研究中所用的语料库将公开共享。\", \n  \"conclusion\": \"本文提出的方法有效提升了高棉语拼写检查的性能，表明了多技术整合策略在解决少资源语言拼写检查问题上的潜力。\"}\n}", "conclusion": "本文提出的方法有效提升了高棉语拼写检查的性能，表明了多技术整合策略在解决少资源语言拼写检查问题上的潜力。"}}
{"id": "2511.09771", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.09771", "abs": "https://arxiv.org/abs/2511.09771", "authors": ["Yu Deng", "Teng Cao", "Hikaru Shindo", "Jiahong Xue", "Quentin Delfosse", "Kristian Kersting"], "title": "STORM: Segment, Track, and Object Re-Localization from a Single 3D Model", "comment": null, "summary": "Accurate 6D pose estimation and tracking are fundamental capabilities for physical AI systems such as robots. However, existing approaches typically rely on a manually annotated segmentation mask of the target in the first frame, which is labor-intensive and leads to reduced performance when faced with occlusions or rapid movement. To address these limi- tations, we propose STORM (Segment, Track, and Object Re-localization from a single 3D Model), an open-source robust real-time 6D pose estimation system that requires no manual annotation. STORM employs a novel three-stage pipeline combining vision-language understanding with self-supervised feature matching: contextual object descriptions guide localization, self-cross-attention mechanisms identify candidate regions, and a segmentation model produces precise masks for accurate pose estimation. Another key innovation is our automatic re-registration mechanism that detects tracking failures through feature similarity monitoring and recovers from severe occlusions or rapid motion. STORM achieves state-of-the-art accuracy on challenging industrial datasets featuring multi-object occlusions, high-speed motion, and varying illumination, while operating at real-time speeds without additional training. This annotation-free approach significantly reduces deployment overhead, providing a practical solution for modern applications, such as flexible manufacturing and intelligent quality control.", "AI": {"tldr": "STORM是一个无需手动注释的强健实时6D姿态估计系统，结合了视觉语言理解和自我监督特征匹配，采用自动重新注册机制来应对遮挡和快速运动。在具有挑战性的情况下实现了高准确性和实时性能。", "motivation": "现状是现有的方法通常依赖于初始帧的目标的手动注释分割掩模，这既费力又在面对遮挡或快速移动时导致性能下降。", "method": "STORM采用一种创新的三阶段流水线，结合视觉语言理解和自我监督特征匹配，通过情境对象描述引导定位，自交叉注意力机制识别候选区域，并通过分割模型生成精确的遮罩以进行准确的姿态估计。此外，STORM还具有自动重新注册机制，通过特征相似度监控检测追踪失败，并从严重的遮挡或快速运动中恢复。", "result": "STORM在复杂的工业数据集上实现了最先进的准确性，这些数据集包含多对象遮挡、高速运动和多变的光照条件，并且在不进行额外训练的情况下以实时速度运行。", "conclusion": "STORM提供了一种无需注释的方法，大大减少了部署负担，为现代应用如柔性制造和智能质量控制提供了实用的解决方案。"}}
{"id": "2511.09819", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.09819", "abs": "https://arxiv.org/abs/2511.09819", "authors": ["Rahul Soni", "Basem Suleiman", "Sonit Singh"], "title": "Improving Graduate Outcomes by Identifying Skills Gaps and Recommending Courses Based on Career Interests", "comment": "10 pages", "summary": "This paper aims to address the challenge of selecting relevant courses for students by proposing the design and development of a course recommendation system. The course recommendation system utilises a combination of data analytics techniques and machine learning algorithms to recommend courses that align with current industry trends and requirements. In order to provide customised suggestions, the study entails the design and implementation of an extensive algorithmic framework that combines machine learning methods, user preferences, and academic criteria. The system employs data mining and collaborative filtering techniques to examine past courses and individual career goals in order to provide course recommendations. Moreover, to improve the accessibility and usefulness of the recommendation system, special attention is given to the development of an easy-to-use front-end interface. The front-end design prioritises visual clarity, interaction, and simplicity through iterative prototyping and user input revisions, guaranteeing a smooth and captivating user experience. We refined and optimised the proposed system by incorporating user feedback, ensuring that it effectively meets the needs and preferences of its target users. The proposed course recommendation system could be a useful tool for students, instructors, and career advisers to use in promoting lifelong learning and professional progression as it fills the gap between university learning and industry expectations. We hope that the proposed course recommendation system will help university students in making data-drive and industry-informed course decisions, in turn, improving graduate outcomes for the university sector.", "AI": {"tldr": "论文提出了一种课程推荐系统，它利用数据分析与机器学习技术来推荐适合学生的课程，同时强调用户体验，旨在连接大学教育与行业需求，促进终身学习和职业发展。", "motivation": "论文旨在解决为学生选择相关课程的挑战，开发一个既符合当前行业趋势和要求，又能提供个性化建议的课程推荐系统。", "method": "此论文提出了一种结合数据分析技术和机器学习算法的课程推荐系统，利用数据挖掘和协同过滤技术来分析过去的课程和个人职业目标以提供课程推荐。此外，该论文注重前端设计的用户体验，强调视觉清晰、交互和简洁。", "result": "通过用户反馈的不断优化，该课程推荐系统符合目标用户的需求和偏好，能够在促进终身学习和职业发展上提供帮助。", "conclusion": "论文提出了一种新的推荐系统，它能够帮助学生、教师和职业顾问做出数据驱动且符合行业需求的课程决策，进而改善大学毕业生的就业情况。"}}
{"id": "2511.09791", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.09791", "abs": "https://arxiv.org/abs/2511.09791", "authors": ["Siddeshwar Raghavan", "Jiangpeng He", "Fengqing Zhu"], "title": "PANDA - Patch And Distribution-Aware Augmentation for Long-Tailed Exemplar-Free Continual Learning", "comment": "Accepted in AAAI 2026 Main Technical Track", "summary": "Exemplar-Free Continual Learning (EFCL) restricts the storage of previous task data and is highly susceptible to catastrophic forgetting. While pre-trained models (PTMs) are increasingly leveraged for EFCL, existing methods often overlook the inherent imbalance of real-world data distributions. We discovered that real-world data streams commonly exhibit dual-level imbalances, dataset-level distributions combined with extreme or reversed skews within individual tasks, creating both intra-task and inter-task disparities that hinder effective learning and generalization. To address these challenges, we propose PANDA, a Patch-and-Distribution-Aware Augmentation framework that integrates seamlessly with existing PTM-based EFCL methods. PANDA amplifies low-frequency classes by using a CLIP encoder to identify representative regions and transplanting those into frequent-class samples within each task. Furthermore, PANDA incorporates an adaptive balancing strategy that leverages prior task distributions to smooth inter-task imbalances, reducing the overall gap between average samples across tasks and enabling fairer learning with frozen PTMs. Extensive experiments and ablation studies demonstrate PANDA's capability to work with existing PTM-based CL methods, improving accuracy and reducing catastrophic forgetting.", "AI": {"tldr": "PANDA 是一个 Patch-and-Distribution-Aware Augmentation 框架，它能够与现有的基于预训练模型的 EFCL 方法无缝集成，通过识别代表性区域并移植到频繁类别样本中来放大低频类别，并采用自适应平衡策略平滑任务间差异，从而提高学习效果和减少灾难性遗忘。", "motivation": "解决现实世界数据流中的双层不平衡问题，同时减少灾难性遗忘对 EFCL 方法的影响。", "method": "Structure", "result": "{\n  \"tldr\": \"PANDA 是一个 Patch-and-Distribution-Aware Augmentation 框架，它能够与现有的基于预训练模型的 EFCL 方法无缝集成，通过识别代表性区域并移植到频繁类别样本中来放大低频类别，并采用自适应平衡策略平滑任务间差异，从而提高学习效果和减少灾难性遗忘。\", \n  \"motivation\": \"解决现实世界数据流中的双层不平衡问题，同时减少灾难性遗忘对 EFCL 方法的影响。\", \n  \"method\": \"提出 PANDA 框架，使用 CLIP 编码器识别样本中的代表性区域，并应用到任务内和任务间的平衡策略上。\", \n  \"result\": \"实验表明，PANDA 可以与现有基于预训练模型的 EFCL 方法集成，提高精度并减少灾难性遗忘。\", \n  \"conclusion\": \"PANDA 是一个有效的框架，能够解决现实数据流中的任务间和任务内部不平衡问题，为基于预训练模型的 EFCL 提供了改进解决方案。\"}\n}", "conclusion": "PANDA 是一个有效的框架，能够解决现实数据流中的任务间和任务内部不平衡问题，为基于预训练模型的 EFCL 提供了改进解决方案。"}}
{"id": "2511.09831", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.09831", "abs": "https://arxiv.org/abs/2511.09831", "authors": ["Neo Wang", "Sonit Singh"], "title": "Answering Students' Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM", "comment": "8 pages", "summary": "The course forums are increasingly significant and play vital role in facilitating student discussions and answering their questions related to the course. It provides a platform for students to post their questions related to the content and admin issues related to the course. However, there are several challenges due to the increase in the number of students enrolled in the course. The primary challenge is that students' queries cannot be responded immediately and the instructors have to face lots of repetitive questions. To mitigate these issues, we propose a question answering system based on large language model with retrieval augmented generation (RAG) method. This work focuses on designing a question answering system with open source Large Language Model (LLM) and fine-tuning it on the relevant course dataset. To further improve the performance, we use a local knowledge base and applied RAG method to retrieve relevant documents relevant to students' queries, where the local knowledge base contains all the course content. To mitigate the hallucination of LLMs, We also integrate it with multi chain-of-thought reasoning to overcome the challenge of hallucination in LLMs. In this work, we experiment fine-tuned LLM with RAG method on the HotpotQA dataset. The experimental results demonstrate that the fine-tuned LLM with RAG method has a strong performance on question answering task.", "AI": {"tldr": "研究提出一种结合大型语言模型和检索增强生成技术的问题回答系统，以解决在线课程论坛中学生查询回应缓慢和重复问题相关的问题，实验显示该系统在HotpotQA数据集上表现出优秀的性能。", "motivation": "随着课程注册学生数量的增加，论坛面临几大挑战，包括学生的问题得不到及时回答，教师需要面对大量重复的问题。这项工作旨在通过引入一种新的问题回答系统来缓解这些问题。", "method": "设计了一个基于大型语言模型并采用检索增强生成（RAG）方法的问题回答系统。系统利用开源大语言模型，针对特定课程的数据集进行微调，并结合本地知识库，应用RAG方法来检索与学生查询相关的文档，以提高性能。为了防止大型语言模型产生幻觉，还集成了多链条思维推理。", "result": "实验结果表明，在HotpotQA数据集上进行微调的大型语言模型结合RAG方法在问题回答任务中表现出色。", "conclusion": "设计和实施一种基于大型语言模型和RAG方法的问题回答系统可以在在线课程论坛环境中更有效地处理学生的问题。"}}
{"id": "2511.09809", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09809", "abs": "https://arxiv.org/abs/2511.09809", "authors": ["Konstantinos M. Dafnis", "Dimitris N. Metaxas"], "title": "Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models", "comment": "NeurIPS 2025", "summary": "Vision-Language Models (VLMs) excel at zero-shot inference but often degrade under test-time domain shifts. For this reason, episodic test-time adaptation strategies have recently emerged as powerful techniques for adapting VLMs to a single unlabeled image. However, existing adaptation strategies, such as test-time prompt tuning, typically require backpropagating through large encoder weights or altering core model components. In this work, we introduce Spectrum-Aware Test-Time Steering (STS), a lightweight adaptation framework that extracts a spectral subspace from the textual embeddings to define principal semantic directions and learns to steer latent representations in a spectrum-aware manner by adapting a small number of per-sample shift parameters to minimize entropy across augmented views. STS operates entirely at inference in the latent space, without backpropagation through or modification of the frozen encoders. Building on standard evaluation protocols, our comprehensive experiments demonstrate that STS largely surpasses or compares favorably against state-of-the-art test-time adaptation methods, while introducing only a handful of additional parameters and achieving inference speeds up to 8x faster with a 12x smaller memory footprint than conventional test-time prompt tuning. The code is available at https://github.com/kdafnis/STS.", "AI": {"tldr": "本文提出了 Spectrum-Aware Test-Time Steering (STS)，一种轻量级且高效的视觉-语言模型适应框架，可在不进行反向传播或修改编码器的情况下增强模型的适应性，并且具有更高的推理速度和更小的内存占用。", "motivation": "现有的适应策略通常需要通过大型编码器权重进行反向传播或修改核心模型组件，而 STS 方法不要求这些操作，旨在提供一种更高效和轻量级的适应策略。", "method": "Spectrum-Aware Test-Time Steering (STS) 是一种轻量级的适应框架，它从文本嵌入中提取频谱子空间来定义主要的语义方向，通过调整每样本的少量偏移参数，以使增强视图的熵最小化来学习在频谱感知方式下引导潜在表示。STS 完全在推断的潜在空间中运行，不需要反向传播或修改冻结的编码器。", "result": "全面的实验表明，STS 在标准评估协议下显著优于或与最先进的测试时适应方法相比表现优越，同时只引入了少量附加参数，实现了高达8倍的推理速度提升及12倍的内存占用减少。", "conclusion": "Spectrum-Aware Test-Time Steering (STS) 是一种轻量级的适应性方法，具有高效性和低资源需求，它超越了现有的测试时适应方法，同时减少了计算资源的使用。"}}
{"id": "2511.09854", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09854", "abs": "https://arxiv.org/abs/2511.09854", "authors": ["Yidan Sun", "Mengying Zhu", "Feiyue Chen", "Yangyang Wu", "Xiaolei Dan", "Mengyuan Yang", "Xiaolin Zheng", "Shenglin Ben"], "title": "TermGPT: Multi-Level Contrastive Fine-Tuning for Terminology Adaptation in Legal and Financial Domain", "comment": "13 pages, 4 figures", "summary": "Large language models (LLMs) have demonstrated impressive performance in text generation tasks; however, their embedding spaces often suffer from the isotropy problem, resulting in poor discrimination of domain-specific terminology, particularly in legal and financial contexts. This weakness in terminology-level representation can severely hinder downstream tasks such as legal judgment prediction or financial risk analysis, where subtle semantic distinctions are critical. To address this problem, we propose TermGPT, a multi-level contrastive fine-tuning framework designed for terminology adaptation. We first construct a sentence graph to capture semantic and structural relations, and generate semantically consistent yet discriminative positive and negative samples based on contextual and topological cues. We then devise a multi-level contrastive learning approach at both the sentence and token levels, enhancing global contextual understanding and fine-grained terminology discrimination. To support robust evaluation, we construct the first financial terminology dataset derived from official regulatory documents. Experiments show that TermGPT outperforms existing baselines in term discrimination tasks within the finance and legal domains.", "AI": {"tldr": "TermGPT improves term discrimination in LLMs through multi-level contrastive learning, benefiting tasks like legal judgment prediction and financial risk analysis.", "motivation": "LLMs struggle with discriminating domain-specific terminology due to isotropy in their embedding space, which negatively impacts tasks requiring nuanced semantic interpretation, such as legal judgment prediction or financial risk assessment.", "method": "Our approach, TermGPT, involves constructing a sentence graph to capture both semantic and structural relations, generating positive and negative samples that are semantically consistent yet discriminative, and applying multi-level contrastive learning at the sentence and token levels.", "result": "Experiments demonstrate that TermGPT performs better than existing methods in discriminating terms in the finance and legal domains.", "conclusion": "TermGPT, using a multi-level contrastive learning mechanism, shows significant improvement in term discrimination and is a promising solution for enhancing LLMs' capability in domain-specific tasks."}}
{"id": "2511.09818", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.09818", "abs": "https://arxiv.org/abs/2511.09818", "authors": ["Hanzhou Liu", "Peng Jiang", "Jia Huang", "Mi Lu"], "title": "Lumos3D: A Single-Forward Framework for Low-Light 3D Scene Restoration", "comment": null, "summary": "Restoring 3D scenes captured under low-light con- ditions remains a fundamental yet challenging problem. Most existing approaches depend on precomputed camera poses and scene-specific optimization, which greatly restricts their scala- bility to dynamic real-world environments. To overcome these limitations, we introduce Lumos3D, a generalizable pose-free framework for 3D low-light scene restoration. Trained once on a single dataset, Lumos3D performs inference in a purely feed- forward manner, directly restoring illumination and structure from unposed, low-light multi-view images without any per- scene training or optimization. Built upon a geometry-grounded backbone, Lumos3D reconstructs a normal-light 3D Gaussian representation that restores illumination while faithfully pre- serving structural details. During training, a cross-illumination distillation scheme is employed, where the teacher network is distilled on normal-light ground truth to transfer accurate geometric information, such as depth, to the student model. A dedicated Lumos loss is further introduced to promote photomet- ric consistency within the reconstructed 3D space. Experiments on real-world datasets demonstrate that Lumos3D achieves high- fidelity low-light 3D scene restoration with accurate geometry and strong generalization to unseen cases. Furthermore, the framework naturally extends to handle over-exposure correction, highlighting its versatility for diverse lighting restoration tasks.", "AI": {"tldr": "Lumos3D is proposed to restore 3D scenes under low light conditions without needing camera poses or scene-specific optimizations, demonstrating high fidelity and generalization.", "motivation": "To address the limitations of existing methods that require precomputed camera poses and scene-specific optimization, which restrict scalability in dynamic environments.", "method": "Lumos3D, a generalizable pose-free framework, uses a geometry-grounded backbone to reconstruct a normal-light 3D Gaussian representation, employing a cross-illumination distillation scheme and a Lumos loss for photometric consistency.", "result": "Lumos3D achieves high-fidelity 3D scene restoration from unposed, low-light multi-view images, showing accurate geometry and strong generalization to unseen cases.", "conclusion": "Lumos3D offers a scalable solution for 3D scene restoration under low-light conditions, with potential for handling over-exposure correction."}}
{"id": "2511.09865", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09865", "abs": "https://arxiv.org/abs/2511.09865", "authors": ["Mingye Zhu", "Yi Liu", "Zheren Fu", "Quan Wang", "Yongdong Zhang"], "title": "In-Token Rationality Optimization: Towards Accurate and Concise LLM Reasoning via Self-Feedback", "comment": "AAAI 2026 Oral", "summary": "Training Large Language Models (LLMs) for chain-of-thought reasoning presents a significant challenge: supervised fine-tuning on a single \"golden\" rationale hurts generalization as it penalizes equally valid alternatives, whereas reinforcement learning with verifiable rewards struggles with credit assignment and prohibitive computational cost. To tackle these limitations, we introduce InTRO (In-Token Rationality Optimization), a new framework that enables both token-level exploration and self-feedback for accurate and concise reasoning. Instead of directly optimizing an intractable objective over all valid reasoning paths, InTRO leverages correction factors-token-wise importance weights estimated by the information discrepancy between the generative policy and its answer-conditioned counterpart, for informative next token selection. This approach allows the model to perform token-level exploration and receive self-generated feedback within a single forward pass, ultimately encouraging accurate and concise rationales. Across six math-reasoning benchmarks, InTRO consistently outperforms other baselines, raising solution accuracy by up to 20% relative to the base model. Its chains of thought are also notably more concise, exhibiting reduced verbosity. Beyond this, InTRO enables cross-domain transfer, successfully adapting to out-of-domain reasoning tasks that extend beyond the realm of mathematics, demonstrating robust generalization.", "AI": {"tldr": "InTRO框架通过引入逐令牌探索和自我反馈机制，解决了大规模语言模型在链式思维推理方面的训练挑战，提高了准确率和绕过了泛化和计算成本问题。", "motivation": "解决大型语言模型在训练链式思维推理时面临的问题，包括监督微调单个“黄金”理性时泛化能力差，以及强化学习中可验证奖励的信用分配难题和计算成本高昂的问题。", "method": "InTRO（In-Token Rationality Optimization）框架，该框架使模型能够进行逐令牌探索和自我反馈，以实现准确且简洁的推理。它通过令牌级别的修正因子（令牌级别的重要性权重）进行优化，这些因子由生成策略与其答案条件化版本之间的信息差异估计得出，从而实现有效的令牌选择。", "result": "在六个数学推理基准测试中，InTRO框架相对基础模型提高了最多20%的解决方案准确性，并减少了冗余性。同时，它还展示了跨领域的泛化能力。", "conclusion": "InTRO框架通过令牌级别的探索和反馈优化了大型语言模型的推理能力，不仅提高了模型的准确性，还增强了其在不同任务上的泛化能力。"}}
{"id": "2511.09820", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09820", "abs": "https://arxiv.org/abs/2511.09820", "authors": ["Jeongho Min", "Dongyoung Kim", "Jaehyup Lee"], "title": "From Street to Orbit: Training-Free Cross-View Retrieval via Location Semantics and LLM Guidance", "comment": "Accepted to WACV 2026, 10pages, 4 figures", "summary": "Cross-view image retrieval, particularly street-to-satellite matching, is a critical task for applications such as autonomous navigation, urban planning, and localization in GPS-denied environments. However, existing approaches often require supervised training on curated datasets and rely on panoramic or UAV-based images, which limits real-world deployment. In this paper, we present a simple yet effective cross-view image retrieval framework that leverages a pretrained vision encoder and a large language model (LLM), requiring no additional training. Given a monocular street-view image, our method extracts geographic cues through web-based image search and LLM-based location inference, generates a satellite query via geocoding API, and retrieves matching tiles using a pretrained vision encoder (e.g., DINOv2) with PCA-based whitening feature refinement. Despite using no ground-truth supervision or finetuning, our proposed method outperforms prior learning-based approaches on the benchmark dataset under zero-shot settings. Moreover, our pipeline enables automatic construction of semantically aligned street-to-satellite datasets, which is offering a scalable and cost-efficient alternative to manual annotation. All source codes will be made publicly available at https://jeonghomin.github.io/street2orbit.github.io/.", "AI": {"tldr": "A no-training-needed cross-view image retrieval solution uses web-based geographic cues and a pretrained vision model to match street-view images to satellite imagery.", "motivation": "To address the limitations of existing methods that require supervised training and curated datasets, the goal is to develop a framework that leverages pretrained models and web-based resources for cross-view image retrieval.", "method": "Given a monocular street-view image, the method extracts geographic cues via web-based image search and LLM-based inference, generates a satellite query using geocoding API, and retrieves matching tiles with a vision encoder after PCA whitening.", "result": "The method outperforms previous learning-based approaches under zero-shot settings and can automatically construct semantically aligned street-to-satellite datasets.", "conclusion": "This work introduces a simple yet effective solution for cross-view image retrieval using a pretrained vision encoder and a large language model without additional training or datasets."}}
{"id": "2511.09873", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09873", "abs": "https://arxiv.org/abs/2511.09873", "authors": ["Nikunj Gupta", "Bill Guo", "Rajgopal Kannan", "Viktor K. Prasanna"], "title": "HierRouter: Coordinated Routing of Specialized Large Language Models via Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) deliver state-of-the-art performance across many tasks but impose high computational and memory costs, limiting their deployment in resource-constrained or real-time settings. To address this, we propose HierRouter, a hierarchical routing approach that dynamically assembles inference pipelines from a pool of specialized, lightweight language models. Formulated as a finite-horizon Markov Decision Process (MDP), our approach trains a Proximal Policy Optimization (PPO)-based reinforcement learning agent to iteratively select which models to invoke at each stage of multi-hop inference. The agent conditions on the evolving context and accumulated cost to make context-aware routing decisions. Experiments with three open-source candidate LLMs across six benchmarks, including QA, code generation, and mathematical reasoning, show that HierRouter improves response quality by up to 2.4x compared to using individual models independently, while incurring only a minimal additional inference cost on average. These results highlight the promise of hierarchical routing for cost-efficient, high-performance LLM inference. All codes can be found here https://github.com/ Nikunj-Gupta/hierouter.", "AI": {"tldr": "本文引入了HierRouter，一种使用有限时间范围MDP和PPO强化学习的分层路由策略，可在保持高性能的同时降低大型语言模型的资源消耗。", "motivation": "大型语言模型在许多任务上表现出色，但需要高昂的计算和内存成本，限制了它们在资源受限或实时环境中的应用。为了应对这一挑战，本文提出了一种新的方法。", "method": "本文提出了HierRouter，这是一种层次路由方法，它将推理流水线动态组装来自一组专业且轻量级的语言模型。该方法被设计为有限时间范围的马尔可夫决策过程（MDP），并通过基于近端策略优化（PPO）的强化学习代理来迭代选择在多跳推理的每个阶段调用哪些模型。代理基于不断变化的上下文和累积成本做出上下文感知的路由决策。", "result": "在六个基准测试中，包括问答、代码生成和数学推理等测试，使用三个开源候选语言模型的实验结果证明，与单独使用各模型相比，HierRouter可以将响应质量提高最多2.4倍，同时，平均而言，仅带来极低的额外推理成本。", "conclusion": "这些结果显示了分层路由在实现低成本、高性能语言模型推理方面的潜力。"}}
{"id": "2511.09827", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.09827", "abs": "https://arxiv.org/abs/2511.09827", "authors": ["Aymen Mir", "Jian Wang", "Riza Alp Guler", "Chuan Guo", "Gerard Pons-Moll", "Bing Zhou"], "title": "AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting", "comment": null, "summary": "We present a novel framework for animating humans in 3D scenes using 3D Gaussian Splatting (3DGS), a neural scene representation that has recently achieved state-of-the-art photorealistic results for novel-view synthesis but remains under-explored for human-scene animation and interaction. Unlike existing animation pipelines that use meshes or point clouds as the underlying 3D representation, our approach introduces the use of 3DGS as the 3D representation to the problem of animating humans in scenes. By representing humans and scenes as Gaussians, our approach allows for geometry-consistent free-viewpoint rendering of humans interacting with 3D scenes. Our key insight is that the rendering can be decoupled from the motion synthesis and each sub-problem can be addressed independently, without the need for paired human-scene data. Central to our method is a Gaussian-aligned motion module that synthesizes motion without explicit scene geometry, using opacity-based cues and projected Gaussian structures to guide human placement and pose alignment. To ensure natural interactions, we further propose a human-scene Gaussian refinement optimization that enforces realistic contact and navigation. We evaluate our approach on scenes from Scannet++ and the SuperSplat library, and on avatars reconstructed from sparse and dense multi-view human capture. Finally, we demonstrate that our framework allows for novel applications such as geometry-consistent free-viewpoint rendering of edited monocular RGB videos with new animated humans, showcasing the unique advantage of 3DGS for monocular video-based human animation.", "AI": {"tldr": "This paper presents a framework using 3D Gaussian Splatting for animating humans in 3D scenes, enabling realistic interactions and free-viewpoint rendering without requiring paired human-scene data.", "motivation": "The motivation is to explore the potential of 3DGS, which has shown state-of-the-art results in novel-view synthesis, for human-scene animation and interaction, offering a novel framework compared to traditional approaches using meshes or point clouds.", "method": "Our approach uses 3D Gaussian Splatting (3DGS) as the 3D representation to animate humans in scenes, enabling geometry-consistent free-viewpoint rendering of humans interacting with 3D scenes. The method decouples motion synthesis from rendering and uses a Gaussian-aligned motion module for motion synthesis, with a human-scene Gaussian refinement optimization for realistic interactions.", "result": "The approach is evaluated on scenes from Scannet++ and the SuperSplat library, and on avatars from multi-view human capture. The results demonstrate geometry-consistent free-viewpoint rendering of humans interacting with scenes, applicable even for monocular RGB videos.", "conclusion": "The conclusion is that the proposed framework offers a new way to animate humans in 3D scenes using 3DGS, providing realistic and free-viewpoint rendering suitable for monocular video-based human animation."}}
{"id": "2511.09880", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.09880", "abs": "https://arxiv.org/abs/2511.09880", "authors": ["Jialin Wu", "Kecen Li", "Zhicong Huang", "Xinfeng Li", "Xiaofeng Wang", "Cheng Hong"], "title": "EnchTable: Unified Safety Alignment Transfer in Fine-tuned Large Language Models", "comment": "Accepted by IEEE Symposium on Security and Privacy (S&P) 2026", "summary": "Many machine learning models are fine-tuned from large language models (LLMs) to achieve high performance in specialized domains like code generation, biomedical analysis, and mathematical problem solving. However, this fine-tuning process often introduces a critical vulnerability: the systematic degradation of safety alignment, undermining ethical guidelines and increasing the risk of harmful outputs. Addressing this challenge, we introduce EnchTable, a novel framework designed to transfer and maintain safety alignment in downstream LLMs without requiring extensive retraining. EnchTable leverages a Neural Tangent Kernel (NTK)-based safety vector distillation method to decouple safety constraints from task-specific reasoning, ensuring compatibility across diverse model architectures and sizes. Additionally, our interference-aware merging technique effectively balances safety and utility, minimizing performance compromises across various task domains. We implemented a fully functional prototype of EnchTable on three different task domains and three distinct LLM architectures, and evaluated its performance through extensive experiments on eleven diverse datasets, assessing both utility and model safety. Our evaluations include LLMs from different vendors, demonstrating EnchTable's generalization capability. Furthermore, EnchTable exhibits robust resistance to static and dynamic jailbreaking attacks, outperforming vendor-released safety models in mitigating adversarial prompts. Comparative analyses with six parameter modification methods and two inference-time alignment baselines reveal that EnchTable achieves a significantly lower unsafe rate, higher utility score, and universal applicability across different task domains. Additionally, we validate EnchTable can be seamlessly integrated into various deployment pipelines without significant overhead.", "AI": {"tldr": "EnchTable是一种新型框架，用于在下游大型语言模型中转移并保持安全对齐。通过NTK基础的安全向量提取方法和干扰感知合并技术，分别优化了安全性和实用性，减少了性能妥协。实验表明它在各种任务领域内具有较强的通用性和安全性。", "motivation": "在对大语言模型进行微调以实现专业领域的高性能时，安全对齐系统地退化带来了关键漏洞，使道德准则受损并增加了有害输出的风险。EnchTable旨在解决这个问题。", "method": "EnchTable采用Neural Tangent Kernel (NTK)为基础的Safety Vector Distillation方法，用于在下游大语言模型中转移并保持安全对齐。这种方法能够将安全约束与任务特定的理由分离开来，确保与各种模型架构和大小的兼容性。同时，采用了干扰感知的合并技术，有效平衡了安全和实用性。", "result": "在三个不同的任务领域和三种不同的LLM架构上实施EnchTable后，对其性能进行了广泛的实验评估。实验结果证明了EnchTable具有显著较低的不安全率、更高的实用性分数，并且在不同的任务领域中表现出普遍适用性。此外，EnchTable还能无缝地集成到各种部署管道之中。", "conclusion": "EnchTable不仅证实了其在维护和转移安全对齐方面的有效性，而且还能抵抗静态和动态的越狱攻击，超越了供应商提供的安全模型。这标志着在提升LLM安全性方面的一步进展。"}}
{"id": "2511.09834", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09834", "abs": "https://arxiv.org/abs/2511.09834", "authors": ["Xuntao Lyu", "Ching-Chi Lin", "Abdullah Al Arafat", "Georg von der Brüggen", "Jian-Jia Chen", "Zhishan Guo"], "title": "CertMask: Certifiable Defense Against Adversarial Patches via Theoretically Optimal Mask Coverage", "comment": null, "summary": "Adversarial patch attacks inject localized perturbations into images to mislead deep vision models. These attacks can be physically deployed, posing serious risks to real-world applications. In this paper, we propose CertMask, a certifiably robust defense that constructs a provably sufficient set of binary masks to neutralize patch effects with strong theoretical guarantees. While the state-of-the-art approach (PatchCleanser) requires two rounds of masking and incurs $O(n^2)$ inference cost, CertMask performs only a single round of masking with $O(n)$ time complexity, where $n$ is the cardinality of the mask set to cover an input image. Our proposed mask set is computed using a mathematically rigorous coverage strategy that ensures each possible patch location is covered at least $k$ times, providing both efficiency and robustness. We offer a theoretical analysis of the coverage condition and prove its sufficiency for certification. Experiments on ImageNet, ImageNette, and CIFAR-10 show that CertMask improves certified robust accuracy by up to +13.4\\% over PatchCleanser, while maintaining clean accuracy nearly identical to the vanilla model.", "AI": {"tldr": "本文提出了CertMask防御方法，能够更有效地防御图像中的局部对抗性攻击，相比于PatchCleanser，其推理成本更低，认证的鲁棒精度更高。", "motivation": "鉴于基于贴片的对抗性攻击可能在现实世界中部署，对深度视觉模型造成严重的威胁，研究者希望能够提出一种能够快速、高效地防御这种攻击的方法。", "method": "CertMask是一种确证性鲁棒防御方法，其使用一套二进制掩码，以理论上的保证来中和局部攻击的影响。相较于PatchCleanser需要两轮掩码处理并产生$O(n^2)$的推理成本，CertMask只需一轮掩码处理，时间复杂度为$O(n)$。CertMask计算了数学上严格的覆盖策略来确保每种可能的贴片位置至少被覆盖k次，从而实现效率和鲁棒性的平衡。", "result": "实验结果表明，相对于当前最优方法PatchCleanser，CertMask在ImageNet、ImageNette和CIFAR-10上能够将认证的鲁棒精度提升最多达到+13.4%，且保持清洁精度与原始模型几乎相同。", "conclusion": "CertMask作为一个确证性鲁棒防御机制，不仅提供了一种更加高效和资源友好的方式来应对贴片攻击，而且还通过对覆盖条件的理论分析提供了一个强有力的数学保证。"}}
{"id": "2511.09915", "categories": ["cs.CL", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.09915", "abs": "https://arxiv.org/abs/2511.09915", "authors": ["Zhiming Ma", "Shiyu Gan", "Junhao Zhao", "Xianming Li", "Qingyun Pan", "Peidong Wang", "Mingjun Pan", "Yuhao Mo", "Jiajie Cheng", "Chengxin Chen", "Zhonglun Cao", "Chonghan Liu", "Shi Cheng"], "title": "HI-TransPA: Hearing Impairments Translation Personal Assistant", "comment": null, "summary": "To provide a unified and flexible solution for daily communication among hearing-impaired individuals, we introduce the Omni-Model paradigm into assistive technology and present HI-TransPA, an instruction-driven audio-visual personal assistant. The model fuses indistinct speech with high-frame-rate lip dynamics, enabling both translation and dialogue within a single multimodal framework. To tackle the challenges of noisy and heterogeneous raw data and the limited adaptability of existing Omni-Models to hearing-impaired speech, we construct a comprehensive preprocessing and curation pipeline that detects facial landmarks, isolates and stabilizes the lip region, and quantitatively assesses multimodal sample quality. These quality scores guide a curriculum learning strategy that first trains on clean, high-confidence samples and progressively incorporates harder cases to strengthen model robustness. We further adopt a SigLIP encoder combined with a Unified 3D-Resampler to efficiently encode high-frame-rate lip motion. Experiments on our purpose-built HI-Dialogue dataset show that HI-TransPA achieves state-of-the-art performance in both literal accuracy and semantic fidelity. This work establishes a foundation for applying Omni-Models to assistive communication technology, providing an end-to-end modeling framework and essential processing tools for future research.", "AI": {"tldr": "本文引入了Omni-Model范式到辅助技术中，提出了HI-TransPA，这是一种语音驱动的视听个人助理，能够融合不清晰的语音与高帧率唇形变化，实现在单一多模态框架内的翻译和对话功能。通过全面的预处理和策展管道，针对噪音和异质原始数据以及现有Omni-Model适应听力障碍者语音的有限性进行了改进，展现了卓越的绝对准确性和语义保真度。", "motivation": "面对听力障碍者日常交流过程中的挑战，提出一种统一且灵活的解决方案，提高该群体交流的质量和便捷性。", "method": "文章采用Omni-Model范式，提出了HI-TransPA系统，该系统包含了复杂的预处理流程、高质量样本的检测与筛选、课程学习策略和高效的SigLIP编码器结合Unified 3D-Resampler来编码高帧率的唇形动态变化。", "result": "实验表明，HI-TransPA在目的构建的HI-Dialogue数据集上实现了既定的精确性和语义保真度，证明了其在辅助通讯技术中的应用能力和潜力。", "conclusion": "这项工作为Omni-Model在辅助通讯技术领域的应用奠定了基础，提供了一个端到端的建模框架及必要的处理工具，推进了未来对该领域研究的发展。"}}
{"id": "2511.09843", "categories": ["cs.CV", "astro-ph.IM", "astro-ph.SR"], "pdf": "https://arxiv.org/pdf/2511.09843", "abs": "https://arxiv.org/abs/2511.09843", "authors": ["Daniela Martin", "Jinsu Hong", "Connor O'Brien", "Valmir P Moraes Filho", "Jasmine R. Kobayashi", "Evangelia Samara", "Joseph Gallego"], "title": "CORONA-Fields: Leveraging Foundation Models for Classification of Solar Wind Phenomena", "comment": null, "summary": "Space weather at Earth, driven by the solar activity, poses growing risks to satellites around our planet as well as to critical ground-based technological infrastructure. Major space weather contributors are the solar wind and coronal mass ejections whose variable density, speed, temperature, and magnetic field make the automated classification of those structures challenging. In this work, we adapt a foundation model for solar physics, originally trained on Solar Dynamics Observatory imagery, to create embeddings suitable for solar wind structure analysis. These embeddings are concatenated with the spacecraft position and solar magnetic connectivity encoded using Fourier features which generates a neural field-based model. The full deep learning architecture is fine-tuned bridging the gap between remote sensing and in situ observations. Labels are derived from Parker Solar Probe measurements, forming a downstream classification task that maps plasma properties to solar wind structures. Although overall classification performance is modest, likely due to coarse labeling, class imbalance, and limited transferability of the pretrained model, this study demonstrates the feasibility of leveraging foundation model embeddings for in situ solar wind tasks. As a first proof-of-concept, it lays the groundwork for future improvements toward more reliable space weather predictions. The code and configuration files used in this study are publicly available to support reproducibility.", "AI": {"tldr": "研究人员通过调整基础模型，创建了适用于太阳风结构分析的嵌入模型，结合飞行器位置和太阳磁场信息形成了基于神经场的模型，显著提升了太阳风结构的预测能力。", "motivation": "为了应对太阳活动导致的空间天气对地球周围卫星和地基基础设施的风险，通过改进模型来更准确地分析太阳风结构。", "method": "使用基础模型对太阳动态观测仪器图像进行训练，将生成的嵌入与飞行器位置和太阳磁场的傅里叶特征相结合，形成了神经场模型。", "result": "{\n  \"tldr\": \"研究人员通过调整基础模型，创建了适用于太阳风结构分析的嵌入模型，结合飞行器位置和太阳磁场信息形成了基于神经场的模型，显著提升了太阳风结构的预测能力。\",\n  \"motivation\": \"为了应对太阳活动导致的空间天气对地球周围卫星和地基基础设施的风险，通过改进模型来更准确地分析太阳风结构。\",\n  \"method\": \"使用基础模型对太阳动态观测仪器图像进行训练，将生成的嵌入与飞行器位置和太阳磁场的傅里叶特征相结合，形成了神经场模型。\",\n  \"result\": \"尽管总体分类性能因标签粗糙、类不平衡和基础模型的迁移能力有限而较一般，但此研究证明了利用基础模型嵌入处理太阳风任务的可行性。\",\n  \"conclusion\": \"这项初步概念验证为未来提高空间天气预测的可靠性奠定了基础。\",\n  \"code_availability\": \"研究中使用的代码和配置文件公开可用以支持可重复性。\"}\n}", "conclusion": "这项初步概念验证为未来提高空间天气预测的可靠性奠定了基础。"}}
{"id": "2511.09918", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09918", "abs": "https://arxiv.org/abs/2511.09918", "authors": ["Pritish Sahu", "Anirudh Som", "Dimitra Vergyri", "Ajay Divakaran"], "title": "MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection", "comment": "IJCNLP-AACL 2025", "summary": "Social norms are implicit, culturally grounded expectations that guide interpersonal communication. Unlike factual commonsense, norm reasoning is subjective, context-dependent, and varies across cultures, posing challenges for computational models. Prior works provide valuable normative annotations but mostly target isolated utterances or synthetic dialogues, limiting their ability to capture the fluid, multi-turn nature of real-world conversations. In this work, we present Norm-RAG, a retrieval-augmented, agentic framework for nuanced social norm inference in multi-turn dialogues. Norm-RAG models utterance-level attributes including communicative intent, speaker roles, interpersonal framing, and linguistic cues and grounds them in structured normative documentation retrieved via a novel Semantic Chunking approach. This enables interpretable and context-aware reasoning about norm adherence and violation across multilingual dialogues. We further introduce MINDS (Multilingual Interactions with Norm-Driven Speech), a bilingual dataset comprising 31 multi-turn Mandarin-English and Spanish-English conversations. Each turn is annotated for norm category and adherence status using multi-annotator consensus, reflecting cross-cultural and realistic norm expression. Our experiments show that Norm-RAG improves norm detection and generalization, demonstrates improved performance for culturally adaptive and socially intelligent dialogue systems.", "AI": {"tldr": "本文提出了Norm-RAG框架，这是一个用于多轮对话中社会规范推理的检索增强代理框架，并引入了MINDS数据集，展示了Norm-RAG在规范检测和跨文化泛化方面的优越性能。", "motivation": "现有的模型在处理实时、多回合的特点和跨文化的社会沟通规范方面有局限性，本文旨在通过新的框架和数据集提高跨文化适应性对话系统的性能。", "method": "Norm-RAG框架通过新颖的语义分块方法检索结构化的规范性文档来推理多语言对话中的社会规范，并分析言语层面的属性如传播意图、角色、人际框架和语言提示。", "result": "实验结果显示Norm-RAG在规范检测方面表现更好，并证明了它在具有跨文化适应性和社会智能的对话系统中的发挥作用。", "conclusion": "Norm-RAG框架通过引入更加灵活的对话理解和跨文化交流机制，可以在多语言、多情境的对话中有效的捕捉和推理社会规范。"}}
{"id": "2511.09866", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.09866", "abs": "https://arxiv.org/abs/2511.09866", "authors": ["Shogo Sato", "Takuhiro Kaneko", "Shoichiro Takeda", "Tomoyasu Shimada", "Kazuhiko Murasaki", "Taiga Yoshida", "Ryuichi Tanida", "Akisato Kimura"], "title": "IPCD: Intrinsic Point-Cloud Decomposition", "comment": "Accepted in WACV2026", "summary": "Point clouds are widely used in various fields, including augmented reality (AR) and robotics, where relighting and texture editing are crucial for realistic visualization. Achieving these tasks requires accurately separating albedo from shade. However, performing this separation on point clouds presents two key challenges: (1) the non-grid structure of point clouds makes conventional image-based decomposition models ineffective, and (2) point-cloud models designed for other tasks do not explicitly consider global-light direction, resulting in inaccurate shade. In this paper, we introduce \\textbf{Intrinsic Point-Cloud Decomposition (IPCD)}, which extends image decomposition to the direct decomposition of colored point clouds into albedo and shade. To overcome challenge (1), we propose \\textbf{IPCD-Net} that extends image-based model with point-wise feature aggregation for non-grid data processing. For challenge (2), we introduce \\textbf{Projection-based Luminance Distribution (PLD)} with a hierarchical feature refinement, capturing global-light ques via multi-view projection. For comprehensive evaluation, we create a synthetic outdoor-scene dataset. Experimental results demonstrate that IPCD-Net reduces cast shadows in albedo and enhances color accuracy in shade. Furthermore, we showcase its applications in texture editing, relighting, and point-cloud registration under varying illumination. Finally, we verify the real-world applicability of IPCD-Net.", "AI": {"tldr": "本文提出了IPCD方法，解决了点云反射率和阴影分离的难题，能够提高渲染的真实感，并展示了其在多个应用场景中的有效性。", "motivation": "点云在AR和机器人等领域中广泛应用，需要精确地分离出反射率和阴影以实现真实感渲染，但现有方法在处理点云方面的效果不佳。", "method": "IPCD方法通过引入IPCD-Net和PLD解决了点云分解中的两个关键技术难题：通过点特征聚合扩展了图像分解模型来处理非网格结构，并引入了多视角投影捕获全局光信息。", "result": "实验结果表明，IPCD-Net能够有效减少反射率中的阴影并提高阴影的颜色准确性，并且在纹理编辑、重新打光和点云配准方面具有良好的应用效果。", "conclusion": "IPCD方法在合成数据集和实际数据集上都展现了良好的应用效果，验证了其在真实世界中的适用性，为点云技术的发展提供了新方法。"}}
{"id": "2511.09935", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.09935", "abs": "https://arxiv.org/abs/2511.09935", "authors": ["Canwen Wang", "Jionghao Lin", "Kenneth R. Koedinger"], "title": "Leveraging Large Language Models for Identifying Knowledge Components", "comment": "Accepted as an extended abstract in The International Conference on Learning Analytics & Knowledge (LAK'25) Workshop: LLMs for Qualitative Analysis in Education", "summary": "Knowledge Components (KCs) are foundational to adaptive learning systems, but their manual identification by domain experts is a significant bottleneck. While Large Language Models (LLMs) offer a promising avenue for automating this process, prior research has been limited to small datasets and has been shown to produce superfluous, redundant KC labels. This study addresses these limitations by first scaling a \"simulated textbook\" LLM prompting strategy (using GPT-4o-mini) to a larger dataset of 646 multiple-choice questions. We found that this initial automated approach performed significantly worse than an expert-designed KC model (RMSE 0.4285 vs. 0.4206) and generated an excessive number of KCs (569 vs. 101). To address the issue of redundancy, we proposed and evaluated a novel method for merging semantically similar KC labels based on their cosine similarity. This merging strategy significantly improved the model's performance; a model using a cosine similarity threshold of 0.8 achieved the best result, reducing the KC count to 428 and improving the RMSE to 0.4259. This demonstrates that while scaled LLM generation alone is insufficient, combining it with a semantic merging technique offers a viable path toward automating and refining KC identification.", "AI": {"tldr": "研究扩大了LLM生成KC标签的方法，并引入了基于余弦相似性合并技术减少冗余标签，表明结合两者能有效改善KC标签的质量。", "motivation": "解决手动识别KC的瓶颈问题，改进LLMs生成KC标签的有效性和减少冗余标签。", "method": "使用GPT-4o-mini扩展现有的“模拟教科书”提示策略，以生成知识组件(KCs)标签。随后，采用基于余弦相似性的方法合并语义相似的KC标签。", "result": "初始方法在生成KC标签时表现不佳且产生的KC标签过多。使用0.8的余弦相似性阈值合并语义相似的KC标签后，取得了最佳效果，将KC标签减少到了428个，并将RMSE值改善到0.4259。", "conclusion": "仅扩大LLM生成规模并不足以解决KC标签生成问题，但结合语义相似性合并技术可以为自动化和改进KC识别提供可行的路径。"}}
{"id": "2511.09868", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.09868", "abs": "https://arxiv.org/abs/2511.09868", "authors": ["Peng Gao", "Yujian Lee", "Xiaofeng Zhang", "Zailong Chen", "Hui Zhang"], "title": "Remember Me: Bridging the Long-Range Gap in LVLMs with Three-Step Inference-Only Decay Resilience Strategies", "comment": "Accepted in AAAI 2026", "summary": "Large Vision-Language Models (LVLMs) have achieved impressive performance across a wide range of multimodal tasks. However, they still face critical challenges in modeling long-range dependencies under the usage of Rotary Positional Encoding (ROPE). Although it can facilitate precise modeling of token positions, it induces progressive attention decay as token distance increases, especially with progressive attention decay over distant token pairs, which severely impairs the model's ability to remember global context. To alleviate this issue, we propose inference-only Three-step Decay Resilience Strategies (T-DRS), comprising (1) Semantic-Driven DRS (SD-DRS), amplifying semantically meaningful but distant signals via content-aware residuals, (2) Distance-aware Control DRS (DC-DRS), which can purify attention by smoothly modulating weights based on positional distances, suppressing noise while preserving locality, and (3) re-Reinforce Distant DRS (reRD-DRS), consolidating the remaining informative remote dependencies to maintain global coherence. Together, the T-DRS recover suppressed long-range token pairs without harming local inductive biases. Extensive experiments on Vision Question Answering (VQA) benchmarks demonstrate that T-DRS can consistently improve performance in a training-free manner. The code can be accessed in https://github.com/labixiaoq-qq/Remember-me", "AI": {"tldr": "论文提出了T-DRS策略，用于解决ROPE引起的远距离注意力衰减问题，该策略在VQA任务上取得了优于基线的性能，且无需额外训练。", "motivation": "在使用旋转位置编码（ROPE）的大规模视觉语言模型（LVLM）中，面临着长距离依赖建模的重大挑战，特别是由于渐进式注意力衰减，这对模型记忆全局上下文的能力造成了严重损害。为了缓解这个问题，提出了T-DRS策略。", "method": "提出了一种仅在推理阶段使用的三步衰减韧性策略（T-DRS），包括语义驱动的衰减韧性策略（SD-DRS）、基于距离感知的控制衰减韧性策略（DC-DRS）和重新强化的遥远衰减韧性策略（reRD-DRS）。", "result": "广泛的VQA基准测试实验表明，T-DRS能够在不进行训练的情况下持续提升性能。", "conclusion": "T-DRS可以恢复被抑制的远距离令牌对，同时不损害局部归纳偏置，这意味着该方法能够有效缓解长距离依赖建模的挑战且不损坏局部注意力结构。"}}
{"id": "2511.09966", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09966", "abs": "https://arxiv.org/abs/2511.09966", "authors": ["Yijie Zhu", "Haojie Zhou", "Wanting Hong", "Tailin Liu", "Ning Wang"], "title": "REAP: Enhancing RAG with Recursive Evaluation and Adaptive Planning for Multi-Hop Question Answering", "comment": "To be published in AAAI 2026", "summary": "Retrieval-augmented generation (RAG) has been extensively employed to mitigate hallucinations in large language models (LLMs). However, existing methods for multi-hop reasoning tasks often lack global planning, increasing the risk of falling into local reasoning impasses. Insufficient exploitation of retrieved content and the neglect of latent clues fail to ensure the accuracy of reasoning outcomes. To overcome these limitations, we propose Recursive Evaluation and Adaptive Planning (REAP), whose core idea is to explicitly maintain structured sub-tasks and facts related to the current task through the Sub-task Planner (SP) and Fact Extractor (FE) modules. SP maintains a global perspective, guiding the overall reasoning direction and evaluating the task state based on the outcomes of FE, enabling dynamic optimization of the task-solving trajectory. FE performs fine-grained analysis over retrieved content to extract reliable answers and clues. These two modules incrementally enrich a logically coherent representation of global knowledge, enhancing the reliability and the traceability of the reasoning process. Furthermore, we propose a unified task paradigm design that enables effective multi-task fine-tuning, significantly enhancing SP's performance on complex, data-scarce tasks. We conduct extensive experiments on multiple public multi-hop datasets, and the results demonstrate that our method significantly outperforms existing RAG methods in both in-domain and out-of-domain settings, validating its effectiveness in complex multi-hop reasoning tasks.", "AI": {"tldr": "This paper proposes REAP to improve multi-hop reasoning by adding global planning and better exploitation of retrieved information, demonstrating superior performance in experiments.", "motivation": "The motivation is to address the lack of global planning and insufficient use of retrieved information in multi-hop reasoning tasks, which can lead to errors in reasoning.", "method": "Recursive Evaluation and Adaptive Planning (REAP) is used, which includes the Sub-task Planner (SP) and Fact Extractor (FE) modules. SP provides global planning and evaluation, while FE performs detailed analysis to extract useful information.", "result": "Experimental results on multiple public datasets show that the proposed method outperforms traditional retrieval-augmented generation methods in both in-domain and out-of-domain reasoning tasks.", "conclusion": "The study concludes that REAP enhances the reliability and traceability of the reasoning process in multi-hop tasks, showing significant improvement over existing methods."}}
{"id": "2511.09870", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.09870", "abs": "https://arxiv.org/abs/2511.09870", "authors": ["Jia Lin", "Xiaofei Zhou", "Jiyuan Liu", "Runmin Cong", "Guodao Zhang", "Zhi Liu", "Jiyong Zhang"], "title": "SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection", "comment": "Accepted to 40th AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "Recently segment anything model (SAM) has attracted widespread concerns, and it is often treated as a vision foundation model for universal segmentation. Some researchers have attempted to directly apply the foundation model to the RGB-D video salient object detection (RGB-D VSOD) task, which often encounters three challenges, including the dependence on manual prompts, the high memory consumption of sequential adapters, and the computational burden of memory attention. To address the limitations, we propose a novel method, namely Segment Anything Model with Depth-guided Adaptive Queries (SAM-DAQ), which adapts SAM2 to pop-out salient objects from videos by seamlessly integrating depth and temporal cues within a unified framework. Firstly, we deploy a parallel adapter-based multi-modal image encoder (PAMIE), which incorporates several depth-guided parallel adapters (DPAs) in a skip-connection way. Remarkably, we fine-tune the frozen SAM encoder under prompt-free conditions, where the DPA utilizes depth cues to facilitate the fusion of multi-modal features. Secondly, we deploy a query-driven temporal memory (QTM) module, which unifies the memory bank and prompt embeddings into a learnable pipeline. Concretely, by leveraging both frame-level queries and video-level queries simultaneously, the QTM module can not only selectively extract temporal consistency features but also iteratively update the temporal representations of the queries. Extensive experiments are conducted on three RGB-D VSOD datasets, and the results show that the proposed SAM-DAQ consistently outperforms state-of-the-art methods in terms of all evaluation metrics.", "AI": {"tldr": "The paper introduces SAM-DAQ, a method that integrates depth and temporal information into the Segment Anything Model (SAM) to improve salient object detection in RGB-D videos, overcoming limitations like manual prompt dependence and high memory usage.", "motivation": "The research aims to address the challenges encountered when directly applying the segment anything model (SAM) to the RGB-D video salient object detection (RGB-D VSOD) task, such as manual prompt dependence, high memory usage, and heavy computation.", "method": "The study proposes the SAM-DAQ method, which involves a parallel adapter-based multi-modal image encoder (PAMIE) incorporating depth-guided parallel adapters (DPAs) and a query-driven temporal memory (QTM) module to extract and update temporal representations efficiently.", "result": "Experiments on three RGB-D VSOD datasets demonstrate that SAM-DAQ outperforms existing state-of-the-art methods in all evaluation metrics.", "conclusion": "The proposed SAM-DAQ method effectively improves the performance of RGB-D video salient object detection by integrating depth information and leveraging temporal contexts, achieving superior results compared to previous methodologies."}}
{"id": "2511.09971", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09971", "abs": "https://arxiv.org/abs/2511.09971", "authors": ["Peter Røysland Aarnes", "Vinay Setty"], "title": "NumPert: Numerical Perturbations to Probe Language Models for Veracity Prediction", "comment": "Accepted in ICJNLP/AACL SRW", "summary": "Large language models show strong performance on knowledge intensive tasks such as fact-checking and question answering, yet they often struggle with numerical reasoning. We present a systematic evaluation of state-of-the-art models for veracity prediction on numerical claims and evidence pairs using controlled perturbations, including label-flipping probes, to test robustness. Our results indicate that even leading proprietary systems experience accuracy drops of up to 62\\% under certain perturbations. No model proves to be robust across all conditions. We further find that increasing context length generally reduces accuracy, but when extended context is enriched with perturbed demonstrations, most models substantially recover. These findings highlight critical limitations in numerical fact-checking and suggest that robustness remains an open challenge for current language models.", "AI": {"tldr": "Systematic evaluation of language models on numerical fact-checking shows they struggle with robustness and numerical reasoning despite overall strong performance.", "motivation": "To assess the numerical reasoning capabilities of large language models, especially their performance and robustness on knowledge-intensive tasks like fact-checking.", "method": "We conducted a systematic evaluation of state-of-the-art models for veracity prediction on numerical claims and evidence pairs, using controlled perturbations like label-flipping probes.", "result": "Leading proprietary systems saw accuracy drops of up to 62% under certain perturbations. Increasing context length generally reduced accuracy, but adding perturbed demonstrations often helped improve model performance.", "conclusion": "The study highlights critical limitations in numerical fact-checking and indicates that robustness remains an open challenge for current language models."}}
{"id": "2511.09878", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.09878", "abs": "https://arxiv.org/abs/2511.09878", "authors": ["Wenzhe He", "Xiaojun Chen", "Wentang Chen", "Hongyu Wang", "Ying Liu", "Ruihui Li"], "title": "RWKV-PCSSC: Exploring RWKV Model for Point Cloud Semantic Scene Completion", "comment": "13 pages, 8 figures, published to ACM MM", "summary": "Semantic Scene Completion (SSC) aims to generate a complete semantic scene from an incomplete input. Existing approaches often employ dense network architectures with a high parameter count, leading to increased model complexity and resource demands. To address these limitations, we propose RWKV-PCSSC, a lightweight point cloud semantic scene completion network inspired by the Receptance Weighted Key Value (RWKV) mechanism. Specifically, we introduce a RWKV Seed Generator (RWKV-SG) module that can aggregate features from a partial point cloud to produce a coarse point cloud with coarse features. Subsequently, the point-wise feature of the point cloud is progressively restored through multiple stages of the RWKV Point Deconvolution (RWKV-PD) modules. By leveraging a compact and efficient design, our method achieves a lightweight model representation. Experimental results demonstrate that RWKV-PCSSC reduces the parameter count by 4.18$\\times$ and improves memory efficiency by 1.37$\\times$ compared to state-of-the-art methods PointSSC. Furthermore, our network achieves state-of-the-art performance on established indoor (SSC-PC, NYUCAD-PC) and outdoor (PointSSC) scene dataset, as well as on our proposed datasets (NYUCAD-PC-V2, 3D-FRONT-PC).", "AI": {"tldr": "提出了RWKV-PCSSC网络，在减少参数量和提高内存效率的同时，取得优于现有方法的性能。", "motivation": "解决现有方法常常使用密集的网络结构，导致模型复杂度和资源需求增加的局限性。", "method": "提出了一种名为RWKV-PCSSC的轻量级点云语义场景完成网络，其灵感来源于接受加权键值(RWKV)机制。具体来说，引入了RWKV种子生成器(RWKV-SG)模块，可以从部分点云中聚合特征以生产具有粗略特征的粗略点云。随后，通过多个RWKV点解卷积(RWKV-PD)模块的阶段逐步恢复点云的逐点特征。通过利用紧凑和高效的设计，该方法实现了轻量化的模型表示。", "result": "实验结果表明，与最先进的方法PointSSC相比，RWKV-PCSSC将参数量减少了4.18倍，提高了1.37倍的内存效率。此外，在已建立的室内（SSC-PC、NYUCAD-PC）和室外（PointSSC）场景数据集以及新提出的数据集（NYUCAD-PC-V2、3D-FRONT-PC）上，该网络达到了最先进的性能。", "conclusion": "RWKV-PCSSC网络通过使用轻量化的架构设计，在减少参数量和提高内存效率上取得了重大成果，同时保持了在多个数据集上的高性能表现。"}}
{"id": "2511.09980", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09980", "abs": "https://arxiv.org/abs/2511.09980", "authors": ["Bo Li", "Tian Tian", "Zhenghua Xu", "Hao Cheng", "Shikun Zhang", "Wei Ye"], "title": "Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG", "comment": "AAAI'26, Oral Paper", "summary": "Dynamic retrieval-augmented generation (RAG) allows large language models (LLMs) to fetch external knowledge on demand, offering greater adaptability than static RAG. A central challenge in this setting lies in determining the optimal timing for retrieval. Existing methods often trigger retrieval based on low token-level confidence, which may lead to delayed intervention after errors have already propagated. We introduce Entropy-Trend Constraint (ETC), a training-free method that determines optimal retrieval timing by modeling the dynamics of token-level uncertainty. Specifically, ETC utilizes first- and second-order differences of the entropy sequence to detect emerging uncertainty trends, enabling earlier and more precise retrieval. Experiments on six QA benchmarks with three LLM backbones demonstrate that ETC consistently outperforms strong baselines while reducing retrieval frequency. ETC is particularly effective in domain-specific scenarios, exhibiting robust generalization capabilities. Ablation studies and qualitative analyses further confirm that trend-aware uncertainty modeling yields more effective retrieval timing. The method is plug-and-play, model-agnostic, and readily integrable into existing decoding pipelines. Implementation code is included in the supplementary materials.", "AI": {"tldr": "ETC是一种无需训练的方法，通过建模令牌级不确定性动态来决定最佳检索时机，使得大型语言模型在问答任务上的表现优于现有方法，特别是在特定领域场景中具有较强的泛化能力。", "motivation": "现有方法基于令牌级别的低置信度来触发检索，可能导致在错误传播之后才进行干预，因此引入了ETC以解决此问题，实现更早和更精确的检索时机。", "method": "ETC方法利用熵序列的一阶和二阶差分来检测不确定性趋势，该方法无需训练且模型无关，可以轻松集成到现有的解码流程中。", "result": "实验表明，ETC在六个问答基准数据集上使用三种大型语言模型骨干时，性能优于强大的基线模型，同时减少了检索频率。", "conclusion": "通过趋势感知的不确定性建模，ETC可以提供更有效的检索时机，特别是在特定领域的场景中表现出色。"}}
{"id": "2511.09883", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.09883", "abs": "https://arxiv.org/abs/2511.09883", "authors": ["Liheng Zhang", "Jin Wang", "Hui Li", "Bingfeng Zhang", "Weifeng Liu"], "title": "HCC-3D: Hierarchical Compensatory Compression for 98% 3D Token Reduction in Vision-Language Models", "comment": null, "summary": "3D understanding has drawn significant attention recently, leveraging Vision-Language Models (VLMs) to enable multi-modal reasoning between point cloud and text data. Current 3D-VLMs directly embed the 3D point clouds into 3D tokens, following large 2D-VLMs with powerful reasoning capabilities. However, this framework has a great computational cost limiting its application, where we identify that the bottleneck lies in processing all 3D tokens in the Large Language Model (LLM) part. This raises the question: how can we reduce the computational overhead introduced by 3D tokens while preserving the integrity of their essential information? To address this question, we introduce Hierarchical Compensatory Compression (HCC-3D) to efficiently compress 3D tokens while maintaining critical detail retention. Specifically, we first propose a global structure compression (GSC), in which we design global queries to compress all 3D tokens into a few key tokens while keeping overall structural information. Then, to compensate for the information loss in GSC, we further propose an adaptive detail mining (ADM) module that selectively recompresses salient but under-attended features through complementary scoring. Extensive experiments demonstrate that HCC-3D not only achieves extreme compression ratios (approximately 98%) compared to previous 3D-VLMs, but also achieves new state-of-the-art performance, showing the great improvements on both efficiency and performance.", "AI": {"tldr": "The paper presents HCC-3D to compress 3D tokens efficiently, maintaining essential details and structure, and achieves high efficiency and performance in comparison to earlier 3D-VLMs.", "motivation": "To reduce the computational complexity of 3D-Vision-Language Models (3D-VLMs) while preserving the integrity and essential information of 3D point clouds.", "method": "Hierarchical Compensatory Compression (HCC-3D), which includes global structure compression (GSC) and adaptive detail mining (ADM).", "result": "HCC-3D not only achieves extreme compression rates but also shows better efficiency and state-of-the-art performance on various benchmarks.", "conclusion": "HCC-3D effectively reduces computational costs and enhances performance in 3D-VLMs, making it a promising approach for future research."}}
{"id": "2511.09984", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09984", "abs": "https://arxiv.org/abs/2511.09984", "authors": ["Bo Li", "Zhenghua Xu", "Rui Xie"], "title": "Language Drift in Multilingual Retrieval-Augmented Generation: Characterization and Decoding-Time Mitigation", "comment": "AAAI'26, Oral Paper", "summary": "Multilingual Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to perform knowledge-intensive tasks in multilingual settings by leveraging retrieved documents as external evidence. However, when the retrieved evidence differs in language from the user query and in-context exemplars, the model often exhibits language drift by generating responses in an unintended language. This phenomenon is especially pronounced during reasoning-intensive decoding, such as Chain-of-Thought (CoT) generation, where intermediate steps introduce further language instability. In this paper, we systematically study output language drift in multilingual RAG across multiple datasets, languages, and LLM backbones. Our controlled experiments reveal that the drift results not from comprehension failure but from decoder-level collapse, where dominant token distributions and high-frequency English patterns dominate the intended generation language. We further observe that English serves as a semantic attractor under cross-lingual conditions, emerging as both the strongest interference source and the most frequent fallback language.\n  To mitigate this, we propose Soft Constrained Decoding (SCD), a lightweight, training-free decoding strategy that gently steers generation toward the target language by penalizing non-target-language tokens. SCD is model-agnostic and can be applied to any generation algorithm without modifying the architecture or requiring additional data. Experiments across three multilingual datasets and multiple typologically diverse languages show that SCD consistently improves language alignment and task performance, providing an effective and generalizable solution in multilingual RAG.", "AI": {"tldr": "本文提出了Soft Constrained Decoding（SCD）方法来解决多语言RAG中的语言漂移问题，这种策略无需额外训练和架构修改，实验结果显示了SCD在多个多语言数据集和多种语言中的有效性和通用性。", "motivation": "本文旨在解决多语言检索增强生成（RAG）中语言漂移的问题，尤其是在推理密集型解码过程中，如Chain-of-Thought (CoT)生成，中间步骤引入了进一步的语言不稳定性。", "method": "我们提出了一种轻量级的无训练解码策略——Soft Constrained Decoding (SCD)，通过惩罚非目标语言的标记，将生成导向目标语言。SCD 是模型无关的，可以应用于任何生成算法，无需修改架构或额外数据。", "result": "实验结果表明，SCD能够一致地提高语言对齐并改进多语言RAG的任务表现。", "conclusion": "实验表明，SCD 有效地提高了语言一致性，并改善了任务表现，为多语言RAG提供了有效且通用的解决方案。"}}
{"id": "2511.09891", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09891", "abs": "https://arxiv.org/abs/2511.09891", "authors": ["Jinfu Li", "Yuqi Huang", "Hong Song", "Ting Wang", "Jianghan Xia", "Yucong Lin", "Jingfan Fan", "Jian Yang"], "title": "Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images", "comment": null, "summary": "Recently, despite the remarkable advancements in object detection, modern detectors still struggle to detect tiny objects in aerial images. One key reason is that tiny objects carry limited features that are inevitably degraded or lost during long-distance network propagation. Another is that smaller objects receive disproportionately greater regression penalties than larger ones during training. To tackle these issues, we propose a Scale-Aware Relay Layer (SARL) and a Scale-Adaptive Loss (SAL) for tiny object detection, both of which are seamlessly compatible with the top-performing frameworks. Specifically, SARL employs a cross-scale spatial-channel attention to progressively enrich the meaningful features of each layer and strengthen the cross-layer feature sharing. SAL reshapes the vanilla IoU-based losses so as to dynamically assign lower weights to larger objects. This loss is able to focus training on tiny objects while reducing the influence on large objects. Extensive experiments are conducted on three benchmarks (\\textit{i.e.,} AI-TOD, DOTA-v2.0 and VisDrone2019), and the results demonstrate that the proposed method boosts the generalization ability by 5.5\\% Average Precision (AP) when embedded in YOLOv5 (anchor-based) and YOLOx (anchor-free) baselines. Moreover, it also promotes the robust performance with 29.0\\% AP on the real-world noisy dataset (\\textit{i.e.,} AI-TOD-v2.0).", "AI": {"tldr": "Presents SARL and SAL methods to improve tiny object detection in aerial images, showing marked performance improvement on multiple benchmarks.", "motivation": "To improve detection of tiny objects in aerial images, which is affected by feature degradation during network propagation and uneven regression penalties during training.", "method": "SARL (Scale-Aware Relay Layer) and SAL (Scale-Adaptive Loss) introduced for enhancing tiny object detection in aerial images. SARL uses cross-scale attention for feature enrichment, while SAL modifies IoU-based loss to lower the penalty on larger objects.", "result": "Experiments on three benchmarks show an improvement of 5.5% in Average Precision (AP) with YOLOv5 and YOLOx baselines, and 29.0% AP on noisy data.", "conclusion": "The SARL and SAL methods effectively enhance the detection of tiny objects, showing significant improvements in precision and robustness."}}
{"id": "2511.09997", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09997", "abs": "https://arxiv.org/abs/2511.09997", "authors": ["Yu-Shiang Huang", "Yun-Yu Lee", "Tzu-Hsin Chou", "Che Lin", "Chuan-Ju Wang"], "title": "FinNuE: Exposing the Risks of Using BERTScore for Numerical Semantic Evaluation in Finance", "comment": "In CIKM 2025 Workshop on Advances in Financial AI: Innovations, Risk, and Responsibility in the Era of LLMs (Non-archival) (FinAI@CIKM 2025)", "summary": "BERTScore has become a widely adopted metric for evaluating semantic similarity between natural language sentences. However, we identify a critical limitation: BERTScore exhibits low sensitivity to numerical variation, a significant weakness in finance where numerical precision directly affects meaning (e.g., distinguishing a 2% gain from a 20% loss). We introduce FinNuE, a diagnostic dataset constructed with controlled numerical perturbations across earnings calls, regulatory filings, social media, and news articles. Using FinNuE, demonstrate that BERTScore fails to distinguish semantically critical numerical differences, often assigning high similarity scores to financially divergent text pairs. Our findings reveal fundamental limitations of embedding-based metrics for finance and motivate numerically-aware evaluation frameworks for financial NLP.", "AI": {"tldr": "本文介绍了BERTScore在金融领域数值变化方面的局限性，并构建了FinNuE数据集以展示这一局限性，突显了金融自然语言处理中数值感知评估框架的重要性。", "motivation": "我们发现BERTScore在数值变化方面的敏感性低，这对于金融领域来说是一个重要的弱点，因为数值精度直接影响意义。", "method": "我们构建了一个名为FinNuE的诊断数据集，该数据集包含在收益电话会议、监管文件、社交媒体和新闻文章中进行的受控数值扰动。", "result": "使用FinNuE，我们展示了BERTScore无法区分语义上重要的数值差异，通常会给财务上不同的文本对分配高的相似度评分。", "conclusion": "我们的研究结果揭示了基于嵌入的度量对金融领域应用的基本局限性，并提出了针对金融自然语言处理的数值感知评估框架的需求。"}}
{"id": "2511.09893", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09893", "abs": "https://arxiv.org/abs/2511.09893", "authors": ["Zubia Naz", "Farhan Asghar", "Muhammad Ishfaq Hussain", "Yahya Hadadi", "Muhammad Aasim Rafique", "Wookjin Choi", "Moongu Jeon"], "title": "Regional Attention-Enhanced Swin Transformer for Clinically Relevant Medical Image Captioning", "comment": null, "summary": "Automated medical image captioning translates complex radiological images into diagnostic narratives that can support reporting workflows. We present a Swin-BART encoder-decoder system with a lightweight regional attention module that amplifies diagnostically salient regions before cross-attention. Trained and evaluated on ROCO, our model achieves state-of-the-art semantic fidelity while remaining compact and interpretable. We report results as mean$\\pm$std over three seeds and include $95\\%$ confidence intervals. Compared with baselines, our approach improves ROUGE (proposed 0.603, ResNet-CNN 0.356, BLIP2-OPT 0.255) and BERTScore (proposed 0.807, BLIP2-OPT 0.645, ResNet-CNN 0.623), with competitive BLEU, CIDEr, and METEOR. We further provide ablations (regional attention on/off and token-count sweep), per-modality analysis (CT/MRI/X-ray), paired significance tests, and qualitative heatmaps that visualize the regions driving each description. Decoding uses beam search (beam size $=4$), length penalty $=1.1$, $no\\_repeat\\_ngram\\_size$ $=3$, and max length $=128$. The proposed design yields accurate, clinically phrased captions and transparent regional attributions, supporting safe research use with a human in the loop.", "AI": {"tldr": "研究提出的新模型通过引入区域注意力机制，提升了在医学影像描述生成中的语义保真度，并且实现了优秀的可解释性。", "motivation": "该研究旨在通过将复杂的放射图像转换为诊断叙述来支持医学报告工作流程。", "method": "研究提出了一种基于Swin-BART编码器-解码器架构的自动化医学影像描述生成系统，增加了轻量级的区域注意力模块，以放大具有诊断意义的区域，随后进行交叉注意力机制计算。", "result": "该模型在ROCO数据集上进行了训练和评估，实现了语义保真的最先进水平，同时保持了紧凑且可解释的特性。相比于基线方法，研究方法在ROUGE和BERTScore指标上取得了显著提升。", "conclusion": "研究证明，提出的模型能够生成准确且符合临床语言习惯的图像描述，并提供透明的区域归因，支持安全研究使用，同时保持人类在环中。"}}
{"id": "2511.10002", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10002", "abs": "https://arxiv.org/abs/2511.10002", "authors": ["Shivam Sharma", "Riya Naik", "Tejas Gawas", "Heramb Patil", "Kunal Korgaonkar"], "title": "PustakAI: Curriculum-Aligned and Interactive Textbooks Using Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating human-like content. This has revolutionized various sectors such as healthcare, software development, and education. In education, LLMs offer potential for personalized and interactive learning experiences, especially in regions with limited teaching resources. However, adapting these models effectively to curriculum-specific content, such as the National Council of Educational Research and Training (NCERT) syllabus in India, presents unique challenges in terms of accuracy, alignment, and pedagogical relevance. In this paper, we present the framework \"PustakAI\"\\footnote{Pustak means `book' in many Indian languages.} for the design and evaluation of a novel question-answering dataset \"NCERT-QA\" aligned with the NCERT curriculum for English and Science subjects of grades 6 to 8. We classify the curated QA pairs as Factoid, Inferential, and Others (evaluative and reasoning). We evaluate the dataset with various prompting techniques, such as meta-prompt, few-shot, and CoT-style prompting, using diverse evaluation metrics to understand which approach aligns more efficiently with the structure and demands of the curriculum. Along with the usability of the dataset, we analyze the strengths and limitations of current open-source LLMs (Gemma3:1b, Llama3.2:3b, and Nemotron-mini:4b) and high-end LLMs (Llama-4-Scout-17B and Deepseek-r1-70B) as AI-based learning tools in formal education systems.", "AI": {"tldr": "该论文提出了一个名为“PustakAI”的框架，用于设计和评估与印度NCERT课程对齐的小学六至八年级英语和科学科目的问答数据集'NCERT-QA'，同时分析了开源和高阶大型语言模型作为AI学习工具在正式教育系统中的优缺点。", "motivation": "随着大型语言模型在理解和生成人类似内容方面表现出的强大能力，它们已彻底改变了诸如教育等多个行业。该研究旨在探索在教育资源匮乏地区的个性化和互动学习体验，特别是针对印度NCERT课程的AI适配问题。", "method": "研究者们分类整理问答对为事实性问题、推理性问题和其他类型的问题，并使用元提示、少样本提示和CoT式提示等多种提示技术评估数据集。", "result": "通过不同的评估指标来了解哪种提示方法更有效地与课程结构和要求相契合，同时分析了开源和高负荷大型语言模型在教育系统中的使用情况。", "conclusion": "论文最终得出结论，评估了与NCERT课程对齐的新型问答数据集的有效性以及当前大型语言模型在教育领域的适用性，并指出了其优势与不足。"}}
{"id": "2511.09909", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.09909", "abs": "https://arxiv.org/abs/2511.09909", "authors": ["Zihao Zhang", "Yang Li", "Aming Wu", "Yahong Han"], "title": "Simulating Distribution Dynamics: Liquid Temporal Feature Evolution for Single-Domain Generalized Object Detection", "comment": null, "summary": "In this paper, we focus on Single-Domain Generalized Object Detection (Single-DGOD), aiming to transfer a detector trained on one source domain to multiple unknown domains. Existing methods for Single-DGOD typically rely on discrete data augmentation or static perturbation methods to expand data diversity, thereby mitigating the lack of access to target domain data. However, in real-world scenarios such as changes in weather or lighting conditions, domain shifts often occur continuously and gradually. Discrete augmentations and static perturbations fail to effectively capture the dynamic variation of feature distributions, thereby limiting the model's ability to perceive fine-grained cross-domain differences. To this end, we propose a new method, Liquid Temporal Feature Evolution, which simulates the progressive evolution of features from the source domain to simulated latent distributions by incorporating temporal modeling and liquid neural network-driven parameter adjustment. Specifically, we introduce controllable Gaussian noise injection and multi-scale Gaussian blurring to simulate initial feature perturbations, followed by temporal modeling and a liquid parameter adjustment mechanism to generate adaptive modulation parameters, enabling a smooth and continuous adaptation across domains. By capturing progressive cross-domain feature evolution and dynamically regulating adaptation paths, our method bridges the source-unknown domain distribution gap, significantly boosting generalization and robustness to unseen shifts. Significant performance improvements on the Diverse Weather dataset and Real-to-Art benchmark demonstrate the superiority of our method. Our code is available at https://github.com/2490o/LTFE.", "AI": {"tldr": "The paper proposes Liquid Temporal Feature Evolution, a method that dynamically models the evolution of features to improve the robustness and adaptability of object detection across various domains, particularly in continuous and gradual domain shifts.", "motivation": "The motivation is to improve the performance of object detection models when facing continuous and gradual domain shifts that discrete data augmentation methods fail to handle effectively.", "method": "Single-Domain Generalized Object Detection (Single-DGOD) employs discrete augmentation and static perturbation, but these are not effective in simulating continuous domain shifts like weather changes. To address this, the authors propose Liquid Temporal Feature Evolution, which integrates Gaussian noise injection, Gaussian blurring, temporal modeling, and liquid neural network parameter adjustment to simulate progressive feature evolution.", "result": "The method shows significant improvements on the Diverse Weather dataset and Real-to-Art benchmark.", "conclusion": "The proposed method, Liquid Temporal Feature Evolution, provides a more dynamic way to capture cross-domain feature evolution and adapt to unseen domain shifts, leading to better generalization and robustness."}}
{"id": "2511.10029", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10029", "abs": "https://arxiv.org/abs/2511.10029", "authors": ["Jiangshu Du", "Wenpeng Yin", "Philip Yu"], "title": "ScaleFormer: Span Representation Cumulation for Long-Context Transformer", "comment": "Accepted by SIGIR-AP'25", "summary": "The quadratic complexity of standard self-attention severely limits the application of Transformer-based models to long-context tasks. While efficient Transformer variants exist, they often require architectural changes and costly pre-training from scratch. To circumvent this, we propose ScaleFormer(Span Representation Cumulation for Long-Context Transformer) - a simple and effective plug-and-play framework that adapts off-the-shelf pre-trained encoder-decoder models to process long sequences without requiring architectural modifications. Our approach segments long inputs into overlapping chunks and generates a compressed, context-aware representation for the decoder. The core of our method is a novel, parameter-free fusion mechanism that endows each chunk's representation with structural awareness of its position within the document. It achieves this by enriching each chunk's boundary representations with cumulative context vectors from all preceding and succeeding chunks. This strategy provides the model with a strong signal of the document's narrative flow, achieves linear complexity, and enables pre-trained models to reason effectively over long-form text. Experiments on long-document summarization show that our method is highly competitive with and often outperforms state-of-the-art approaches without requiring architectural modifications or external retrieval mechanisms.", "AI": {"tldr": "ScaleFormer提出了一种处理长文本的即插即用框架，通过分割长输入和结合前后文信息处理每个块，实现线性复杂度的同时保持与最先进方法的竞争力。", "motivation": "目前标准的自注意机制二次复杂性限制了基于Transformer模型在长上下文任务中的应用。虽然存在高效的Transformer变体，但它们通常需要架构上的修改并从零开始昂贵的预训练。为了避免这种情况，ScaleFormer被提出。", "method": "ScaleFormer(Span Representation Cumulation for Long-Context Transformer)提出了一种简单有效的即插即用框架，将现有的预训练的编码-解码模型适应到长序列的处理中，无需进行架构修改。该方法将长输入分割成重叠的块，并生成压缩的、上下文感知的表示以供解码器使用。其核心是一个新颖的、无参数的融合机制，该机制为每个块的表示赋予结构化的文档位置感知。通过用所有前驱和后继块的累积上下文向量来丰富每个块边界表示，该策略为模型提供了文档叙述流程的强烈信号，同时实现线性复杂度，并使预训练模型能够有效地处理长文本。", "result": "实验结果在长文档摘要任务中显示，该方法与最先进的方法具有高度竞争力，并且往往优于其表现，无需进行架构修改或外部检索机制。", "conclusion": "ScaleFormer能够使预训练模型有效处理长文本，同时保持与最新方法的竞争力，甚至优于它们，且无需进行架构修改和外部检索机制。"}}
{"id": "2511.09919", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.09919", "abs": "https://arxiv.org/abs/2511.09919", "authors": ["Ketong Chen", "Yuhao Chen", "Yang Xue"], "title": "MosaicDoc: A Large-Scale Bilingual Benchmark for Visually Rich Document Understanding", "comment": null, "summary": "Despite the rapid progress of Vision-Language Models (VLMs), their capabilities are inadequately assessed by existing benchmarks, which are predominantly English-centric, feature simplistic layouts, and support limited tasks. Consequently, they fail to evaluate model performance for Visually Rich Document Understanding (VRDU), a critical challenge involving complex layouts and dense text. To address this, we introduce DocWeaver, a novel multi-agent pipeline that leverages Large Language Models to automatically generate a new benchmark. The result is MosaicDoc, a large-scale, bilingual (Chinese and English) resource designed to push the boundaries of VRDU. Sourced from newspapers and magazines, MosaicDoc features diverse and complex layouts (including multi-column and non-Manhattan), rich stylistic variety from 196 publishers, and comprehensive multi-task annotations (OCR, VQA, reading order, and localization). With 72K images and over 600K QA pairs, MosaicDoc serves as a definitive benchmark for the field. Our extensive evaluation of state-of-the-art models on this benchmark reveals their current limitations in handling real-world document complexity and charts a clear path for future research.", "AI": {"tldr": "本文提出了MosaicDoc，一个用于评估视觉丰富文档理解能力的新基准测试集，展示了现有模型的局限性并指导未来研究。", "motivation": "现有的基准测试主要以英语为中心，布局简单，支持的任务有限，无法全面评估模型在视觉丰富文档理解（VRDU）方面的能力。", "method": "我们引入了一个名为DocWeaver的新多智能体管道，它使用大语言模型自动生成一个新的基准测试。", "result": "生成了MosaicDoc，这是一个大规模、双语（中文和英语）资源，适用于复杂布局的文档理解，包含72K张图像和超过600K个QA对。", "conclusion": "对最先进模型的广泛评估揭示了它们在处理真实世界文档复杂性方面的当前局限性，并为未来的研究指明了方向。"}}
{"id": "2511.10045", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10045", "abs": "https://arxiv.org/abs/2511.10045", "authors": ["Jinhong Jeong", "Sunghyun Lee", "Jaeyoung Lee", "Seonah Han", "Youngjae Yu"], "title": "Do Language Models Associate Sound with Meaning? A Multimodal Study of Sound Symbolism", "comment": "33 pages, 27 tables, 10 figures", "summary": "Sound symbolism is a linguistic concept that refers to non-arbitrary associations between phonetic forms and their meanings. We suggest that this can be a compelling probe into how Multimodal Large Language Models (MLLMs) interpret auditory information in human languages. We investigate MLLMs' performance on phonetic iconicity across textual (orthographic and IPA) and auditory forms of inputs with up to 25 semantic dimensions (e.g., sharp vs. round), observing models' layer-wise information processing by measuring phoneme-level attention fraction scores. To this end, we present LEX-ICON, an extensive mimetic word dataset consisting of 8,052 words from four natural languages (English, French, Japanese, and Korean) and 2,930 systematically constructed pseudo-words, annotated with semantic features applied across both text and audio modalities. Our key findings demonstrate (1) MLLMs' phonetic intuitions that align with existing linguistic research across multiple semantic dimensions and (2) phonosemantic attention patterns that highlight models' focus on iconic phonemes. These results bridge domains of artificial intelligence and cognitive linguistics, providing the first large-scale, quantitative analyses of phonetic iconicity in terms of MLLMs' interpretability.", "AI": {"tldr": "研究分析了多模态大型语言模型（MLLMs）对语音与意义非任意联系的理解，揭示了模型的符号意识，并展示了其在多个语义维度上的注意力模式。", "motivation": "探讨语音象征性概念，即语音形式与意义之间的非任意联系，如何作为探究MLLMs如何解释人声信息的有效手段。通过研究MLLMs在语音象征性表现上的能力，旨在桥梁人工智能和认知语言学领域，提供首个关于MLLMs可解释性中的语音象征性大规模定量分析。", "method": "本研究利用LEX-ICON数据集（包含8,052个来自四种自然语言的词汇和2,930个系统性构造的伪词），分析了多模态大型语言模型（MLLMs）在处理文本（正字法和国际音标）和声音形式输入时的语音象征性表现。通过测量音素层面的注意力分数，观察模型各层的信息处理方式。", "result": "研究结果表明，MLLMs在多个语义维度上对语音的直觉与现有语言研究相符，并且模型在处理语音时对象征性音素表现出关注。", "conclusion": "此研究表明，通过分析MLLMs的表现，可以提供对语音象征性和模型可解释性的见解，从而在人工智能和认知语言学之间建立了联系。"}}
{"id": "2511.09926", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09926", "abs": "https://arxiv.org/abs/2511.09926", "authors": ["Xuan Rao", "Simian Xu", "Zheng Li", "Bo Zhao", "Derong Liu", "Mingming Ha", "Cesare Alippi"], "title": "Compensating Distribution Drifts in Class-incremental Learning of Pre-trained Vision Transformers", "comment": "The 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "Recent advances have shown that sequential fine-tuning (SeqFT) of pre-trained vision transformers (ViTs), followed by classifier refinement using approximate distributions of class features, can be an effective strategy for class-incremental learning (CIL). However, this approach is susceptible to distribution drift, caused by the sequential optimization of shared backbone parameters. This results in a mismatch between the distributions of the previously learned classes and that of the updater model, ultimately degrading the effectiveness of classifier performance over time. To address this issue, we introduce a latent space transition operator and propose Sequential Learning with Drift Compensation (SLDC). SLDC aims to align feature distributions across tasks to mitigate the impact of drift. First, we present a linear variant of SLDC, which learns a linear operator by solving a regularized least-squares problem that maps features before and after fine-tuning. Next, we extend this with a weakly nonlinear SLDC variant, which assumes that the ideal transition operator lies between purely linear and fully nonlinear transformations. This is implemented using learnable, weakly nonlinear mappings that balance flexibility and generalization. To further reduce representation drift, we apply knowledge distillation (KD) in both algorithmic variants. Extensive experiments on standard CIL benchmarks demonstrate that SLDC significantly improves the performance of SeqFT. Notably, by combining KD to address representation drift with SLDC to compensate distribution drift, SeqFT achieves performance comparable to joint training across all evaluated datasets. Code: https://github.com/raoxuan98-hash/sldc.git.", "AI": {"tldr": "本文提出了一种称为SLDC的方法来解决SeqFT中的分布漂移问题，通过线性和非线性版本的SLDC和知识蒸馏来改善CIL中的性能，并达到了联合训练的水平。", "motivation": "传统的序列微调（SeqFT）方法在分类增量学习（CIL）中容易受到共享骨干参数的分布漂移影响，导致分类器性能随时间下降。为了应对这个问题，本文提出了SLDC方法。", "method": "本文提出了SLDC方法，它包括一个线性版本和一个弱非线性版本。线性版本通过求解正则化最小二乘问题学习一个线性算子，非线性版本则通过可学习的弱非线性映射来实现，这两个版本都结合了知识蒸馏来最小化表示漂移。", "result": "实验显示，SLDC显著提高了SeqFT的性能，并且结合KD后，SeqFT的性能接近于联合训练的结果。", "conclusion": "SLDC方法通过最小化特征分布之间的差距和减少表示漂移来改善SeqFT的性能，其效果已经通过标准CIL基准测试得到验证。"}}
{"id": "2511.10051", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10051", "abs": "https://arxiv.org/abs/2511.10051", "authors": ["Zhenhe Li", "Can Lin", "Ling Zheng", "Wen-Da Wei", "Junli Liang", "Qi Song"], "title": "GraphIF: Enhancing Multi-Turn Instruction Following for Large Language Models with Relation Graph Prompt", "comment": null, "summary": "Multi-turn instruction following is essential for building intelligent conversational systems that can consistently adhere to instructions across dialogue turns. However, existing approaches to enhancing multi-turn instruction following primarily rely on collecting or generating large-scale multi-turn dialogue datasets to fine-tune large language models (LLMs), which treat each response generation as an isolated task and fail to explicitly incorporate multi-turn instruction following into the optimization objectives. As a result, instruction-tuned LLMs often struggle with complex long-distance constraints. In multi-turn dialogues, relational constraints across turns can be naturally modeled as labeled directed edges, making graph structures particularly suitable for modeling multi-turn instruction following. Despite this potential, leveraging graph structures to enhance the multi-turn instruction following capabilities of LLMs remains unexplored. To bridge this gap, we propose GraphIF, a plug-and-play framework that models multi-turn dialogues as directed relation graphs and leverages graph prompts to enhance the instruction following capabilities of LLMs. GraphIF comprises three key components: (1) an agent-based relation extraction module that captures inter-turn semantic relations via action-triggered mechanisms to construct structured graphs; (2) a relation graph prompt generation module that converts structured graph information into natural language prompts; and (3) a response rewriting module that refines initial LLM outputs using the generated graph prompts. Extensive experiments on two long multi-turn dialogue datasets demonstrate that GraphIF can be seamlessly integrated into instruction-tuned LLMs and leads to significant improvements across all four multi-turn instruction-following evaluation metrics.", "AI": {"tldr": "该论文提出GraphIF框架，利用图结构捕捉多轮对话中的语义关系，通过图提示增强LLM的多轮指令遵守能力，并通过实验验证了其有效性。", "motivation": "现有方法在增强多轮指令遵守能力时，通常是通过生成大规模多轮对话数据集来微调大规模语言模型，这会造成对复杂长距离限制的处理能力不足。", "method": "GraphIF框架包括三个关键组件：基于智能体的关系提取模块，结构化图信息转换模块，以及响应重写模块，利用图提示优化LLM的输出结果。", "result": "实验表明，GraphIF可以无缝集成到指令微调后的LLM中，并且在四个多轮指令遵守评估指标上都显著提升。", "conclusion": "GraphIF通过采用图结构和图提示来增强大规模语言模型的多轮指令遵守能力，在长对话上有效改善了语言模型的表现。"}}
{"id": "2511.09933", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.09933", "abs": "https://arxiv.org/abs/2511.09933", "authors": ["Yuhang Zhou", "Yanxiang Zhao", "Zhongyun Hua", "Zhipu Liu", "Zhaoquan Gu", "Qing Liao", "Leo Yu Zhang"], "title": "Debiased Dual-Invariant Defense for Adversarially Robust Person Re-Identification", "comment": "Accepted by AAAI 2026", "summary": "Person re-identification (ReID) is a fundamental task in many real-world applications such as pedestrian trajectory tracking. However, advanced deep learning-based ReID models are highly susceptible to adversarial attacks, where imperceptible perturbations to pedestrian images can cause entirely incorrect predictions, posing significant security threats. Although numerous adversarial defense strategies have been proposed for classification tasks, their extension to metric learning tasks such as person ReID remains relatively unexplored. Moreover, the several existing defenses for person ReID fail to address the inherent unique challenges of adversarially robust ReID. In this paper, we systematically identify the challenges of adversarial defense in person ReID into two key issues: model bias and composite generalization requirements. To address them, we propose a debiased dual-invariant defense framework composed of two main phases. In the data balancing phase, we mitigate model bias using a diffusion-model-based data resampling strategy that promotes fairness and diversity in training data. In the bi-adversarial self-meta defense phase, we introduce a novel metric adversarial training approach incorporating farthest negative extension softening to overcome the robustness degradation caused by the absence of classifier. Additionally, we introduce an adversarially-enhanced self-meta mechanism to achieve dual-generalization for both unseen identities and unseen attack types. Experiments demonstrate that our method significantly outperforms existing state-of-the-art defenses.", "AI": {"tldr": "提出了一种新的行人重识别（ReID）的对抗性防御方法，提高了系统的鲁棒性和泛化能力。", "motivation": "现有的防御策略在行人重识别（ReID）任务中未能解决对抗性鲁棒防御的独特挑战，包括模型偏差和复合泛化要求等问题。", "method": "通过扩散模型进行数据再采样以减轻模型偏差，并在双对抗自我元防御阶段引入一种新的度量对抗训练方法，该方法加入了最远负样本软化处理以克服由于缺乏分类器引起的鲁棒性下降问题，同时引入对抗增强自我元机制以实现对未见身份和攻击类型的双重泛化。", "result": "提出的去偏见双不变防御框架，在实验中显著优于现有的最先进的防御策略。", "conclusion": "该方法能够有效提升行人重识别模型在对抗攻击面前的鲁棒性，并能更好地泛化到未见过的身份和攻击类型。"}}
{"id": "2511.10070", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10070", "abs": "https://arxiv.org/abs/2511.10070", "authors": ["Haroun Elleuch", "Salima Mdhaffar", "Yannick Estève", "Fethi Bougares"], "title": "ADI-20: Arabic Dialect Identification dataset and models", "comment": "Published in Interspeech 2025", "summary": "We present ADI-20, an extension of the previously published ADI-17 Arabic Dialect Identification (ADI) dataset. ADI-20 covers all Arabic-speaking countries' dialects. It comprises 3,556 hours from 19 Arabic dialects in addition to Modern Standard Arabic (MSA). We used this dataset to train and evaluate various state-of-the-art ADI systems. We explored fine-tuning pre-trained ECAPA-TDNN-based models, as well as Whisper encoder blocks coupled with an attention pooling layer and a classification dense layer. We investigated the effect of (i) training data size and (ii) the model's number of parameters on identification performance. Our results show a small decrease in F1 score while using only 30% of the original training data. We open-source our collected data and trained models to enable the reproduction of our work, as well as support further research in ADI.", "AI": {"tldr": "ADI-20扩展了ADI-17数据集，包含了所有阿拉伯国家方言的数据，并使用该数据集评估了几种最先进的方言识别模型，分析了训练数据量和模型参数数量对识别性能的影响。", "motivation": "扩展和丰富阿拉伯方言数据集，评估最新的方言识别模型表现，并探索训练数据量和模型大小的影响。", "method": "使用ECAPA-TDNN和Whisper编码器模块结合注意力池化层及分类层，训练并评估模型。", "result": "当仅使用30%的原始训练数据时，F1得分有轻微下降。", "conclusion": "发布数据集和模型以支持后续研究。"}}
{"id": "2511.09942", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09942", "abs": "https://arxiv.org/abs/2511.09942", "authors": ["Mustafa Munir", "Md Mostafijur Rahman", "Radu Marculescu"], "title": "AdaptViG: Adaptive Vision GNN with Exponential Decay Gating", "comment": "Accepted in 2026 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2026)", "summary": "Vision Graph Neural Networks (ViGs) offer a new direction for advancements in vision architectures. While powerful, ViGs often face substantial computational challenges stemming from their graph construction phase, which can hinder their efficiency. To address this issue we propose AdaptViG, an efficient and powerful hybrid Vision GNN that introduces a novel graph construction mechanism called Adaptive Graph Convolution. This mechanism builds upon a highly efficient static axial scaffold and a dynamic, content-aware gating strategy called Exponential Decay Gating. This gating mechanism selectively weighs long-range connections based on feature similarity. Furthermore, AdaptViG employs a hybrid strategy, utilizing our efficient gating mechanism in the early stages and a full Global Attention block in the final stage for maximum feature aggregation. Our method achieves a new state-of-the-art trade-off between accuracy and efficiency among Vision GNNs. For instance, our AdaptViG-M achieves 82.6% top-1 accuracy, outperforming ViG-B by 0.3% while using 80% fewer parameters and 84% fewer GMACs. On downstream tasks, AdaptViG-M obtains 45.8 mIoU, 44.8 APbox, and 41.1 APmask, surpassing the much larger EfficientFormer-L7 by 0.7 mIoU, 2.2 APbox, and 2.1 APmask, respectively, with 78% fewer parameters.", "AI": {"tldr": "AdaptViG提供了视觉架构的新方向，通过自适应图卷积和指数衰减门机制提高了计算效率，同时保持了高准确率，达到了视觉GNN中的新效率-准确率的权衡。", "motivation": "视觉图神经网络（ViGs）虽然强大，但在图构建阶段会遇到大量的计算挑战，影响其效率。AdaptViG的设计目的是为了克服这一问题，提高ViGs的效率和性能。", "method": "AdaptViG采用了一种新颖的图构建机制，称为自适应图卷积。该机制基于高效静态轴向支架和一种名为指数衰减门的动态、内容感知策略。这种门机制根据特征相似性对远程连接进行加权。此外，AdaptViG采用了一种混合策略，在早期阶段使用高效的门机制，在最终阶段使用一个全局注意力块来进行最大特征聚合。", "result": "AdaptViG-M在ImageNet数据集上实现了82.6%的top-1准确率，在参数量减少80%、GMACs减少84%的情况下依然优于ViG-B。在下游任务上，AdaptViG-M的表现也优于参数量少78%的EfficientFormer-L7。", "conclusion": "研究证明，AdaptViG通过创新的图构建机制和高效的混合策略，在提高视觉GNN的效率和性能上取得了显著效果，提高了准确率-效率的权衡。"}}
{"id": "2511.10075", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10075", "abs": "https://arxiv.org/abs/2511.10075", "authors": ["Xanh Ho", "Yun-Ang Wu", "Sunisth Kumar", "Florian Boudin", "Atsuhiro Takasu", "Akiko Aizawa"], "title": "Format Matters: The Robustness of Multimodal LLMs in Reviewing Evidence from Tables and Charts", "comment": "Accepted at AAAI 2026", "summary": "With the growing number of submitted scientific papers, there is an increasing demand for systems that can assist reviewers in evaluating research claims. Experimental results are a core component of scientific work, often presented in varying formats such as tables or charts. Understanding how robust current multimodal large language models (multimodal LLMs) are at verifying scientific claims across different evidence formats remains an important and underexplored challenge. In this paper, we design and conduct a series of experiments to assess the ability of multimodal LLMs to verify scientific claims using both tables and charts as evidence. To enable this evaluation, we adapt two existing datasets of scientific papers by incorporating annotations and structures necessary for a multimodal claim verification task. Using this adapted dataset, we evaluate 12 multimodal LLMs and find that current models perform better with table-based evidence while struggling with chart-based evidence. We further conduct human evaluations and observe that humans maintain strong performance across both formats, unlike the models. Our analysis also reveals that smaller multimodal LLMs (under 8B) show weak correlation in performance between table-based and chart-based tasks, indicating limited cross-modal generalization. These findings highlight a critical gap in current models' multimodal reasoning capabilities. We suggest that future multimodal LLMs should place greater emphasis on improving chart understanding to better support scientific claim verification.", "AI": {"tldr": "本文评估了多模态LLMs验证科学声明的能力，发现在使用表格证据时表现较好，而在使用图表证据时则表现不佳。表明跨模态泛化能力不足，未来应提高对图表的理解。", "motivation": "随着提交的科学论文数量的增加，出现了对帮助审稿人评估研究主张的系统的更高需求。了解当前多模态LLMs在不同证据格式之间验证科学主张的能力是一个重要且未充分探索的挑战。", "method": "设计并进行了一系列实验来评估多模态大语言模型（multimodal LLMs）使用表格和图表验证科学研究声明的能力。为此，对两个现有的科学论文数据集进行了改编，加入了必要的注释和结构以支持多模态声明验证任务。", "result": "评估了12个多模态LLMs，发现当前模型在表格证据上的表现优于图表证据。人类评估显示人类在两种格式上的表现都很强，而模型则表现出色不同。更小的多模态LLMs（低于8B）在表格和图表任务上的表现之间显示出弱相关性，表明跨模态泛化能力有限。", "conclusion": "这些发现强调了当前模型在多模态推理能力上的关键差距。建议未来的多模态LLMs应更加重视提高对图表的理解，以更好地支持科学主张的验证。"}}
{"id": "2511.09944", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.09944", "abs": "https://arxiv.org/abs/2511.09944", "authors": ["Zhiyuan Xu", "Nan Min", "Yuhang Guo", "Tong Wei"], "title": "TSPE-GS: Probabilistic Depth Extraction for Semi-Transparent Surface Reconstruction via 3D Gaussian Splatting", "comment": "AAAI26 Poster", "summary": "3D Gaussian Splatting offers a strong speed-quality trade-off but struggles to reconstruct semi-transparent surfaces because most methods assume a single depth per pixel, which fails when multiple surfaces are visible. We propose TSPE-GS (Transparent Surface Probabilistic Extraction for Gaussian Splatting), which uniformly samples transmittance to model a pixel-wise multi-modal distribution of opacity and depth, replacing the prior single-peak assumption and resolving cross-surface depth ambiguity. By progressively fusing truncated signed distance functions, TSPE-GS reconstructs external and internal surfaces separately within a unified framework. The method generalizes to other Gaussian-based reconstruction pipelines without extra training overhead. Extensive experiments on public and self-collected semi-transparent and opaque datasets show TSPE-GS significantly improves semi-transparent geometry reconstruction while maintaining performance on opaque scenes.", "AI": {"tldr": "TSPE-GS is introduced to address the issue of cross-surface depth ambiguity in 3D Gaussian Splatting, particularly for semi-transparent surfaces, by modeling opacity and depth with a multi-modal distribution and reconstructing surfaces separately.", "motivation": "The motivation arises from the limitations of current 3D Gaussian Splatting techniques in handling semi-transparent surfaces. These methods typically assume a single depth per pixel, which does not work well when multiple surfaces are visible at the same time.", "method": "TSPE-GS (Transparent Surface Probabilistic Extraction for Gaussian Splatting) is proposed. It samples transmittance uniformly to create a pixel-wise multi-modal distribution of opacity and depth, which replaces the traditional assumption of a single depth per pixel. This helps solve the problem of cross-surface depth ambiguity. The method also integrates truncated signed distance functions to reconstruct external and internal surfaces separately within a unified framework.", "result": "Extensive experiments on both public and self-collected datasets with semi-transparent as well as opaque objects show that TSPE-GS significantly enhances the reconstruction of semi-transparent geometries. At the same time, it maintains a high performance level on opaque scenes.", "conclusion": "TSPE-GS not only improves the reconstruction of semi-transparent surfaces but also has the potential to be integrated into other Gaussian-based reconstruction methods without incurring additional training costs."}}
{"id": "2511.10090", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10090", "abs": "https://arxiv.org/abs/2511.10090", "authors": ["Haroun Elleuch", "Youssef Saidi", "Salima Mdhaffar", "Yannick Estève", "Fethi Bougares"], "title": "ELYADATA & LIA at NADI 2025: ASR and ADI Subtasks", "comment": "Published in Proceedings of the ArabicNLP 2025 Workshop (co-located with EMNLP 2025), Association for Computational Linguistics, 2025", "summary": "This paper describes Elyadata \\& LIA's joint submission to the NADI multi-dialectal Arabic Speech Processing 2025. We participated in the Spoken Arabic Dialect Identification (ADI) and multi-dialectal Arabic ASR subtasks. Our submission ranked first for the ADI subtask and second for the multi-dialectal Arabic ASR subtask among all participants. Our ADI system is a fine-tuned Whisper-large-v3 encoder with data augmentation. This system obtained the highest ADI accuracy score of \\textbf{79.83\\%} on the official test set. For multi-dialectal Arabic ASR, we fine-tuned SeamlessM4T-v2 Large (Egyptian variant) separately for each of the eight considered dialects. Overall, we obtained an average WER and CER of \\textbf{38.54\\%} and \\textbf{14.53\\%}, respectively, on the test set. Our results demonstrate the effectiveness of large pre-trained speech models with targeted fine-tuning for Arabic speech processing.", "AI": {"tldr": "本文介绍Elyadata与LIA联合提交至NADI多方言阿拉伯语语音处理2025项目的成果，提交方案在口头阿拉伯语方言识别(ADI)任务中排名第一，多方言阿拉伯语ASR任务中排名第二。", "motivation": "旨在展示经过微调的大型预训练语音模型在阿拉伯语语音处理上的有效性。", "method": "对于ADI任务，采用了经过数据扩增的Whisper-large-v3编码器微调模型；对于多方言阿拉伯语ASR任务，则分别对SeamlessM4T-v2 Large (埃及版)针对八种方言进行微调。", "result": "ADI系统在官方测试集上达到79.83%的准确率，多方言阿拉伯语ASR任务获得平均WER为38.54%，CER为14.53%。", "conclusion": "结果显示大型预训练语音模型经过定向微调后在阿拉伯语音处理中的有效性。"}}
{"id": "2511.09948", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09948", "abs": "https://arxiv.org/abs/2511.09948", "authors": ["Zhicheng Liao", "Dongxu Wu", "Zhenshan Shi", "Sijie Mai", "Hanwei Zhu", "Lingyu Zhu", "Yuncheng Jiang", "Baoliang Chen"], "title": "Beyond Cosine Similarity Magnitude-Aware CLIP for No-Reference Image Quality Assessment", "comment": null, "summary": "Recent efforts have repurposed the Contrastive Language-Image Pre-training (CLIP) model for No-Reference Image Quality Assessment (NR-IQA) by measuring the cosine similarity between the image embedding and textual prompts such as \"a good photo\" or \"a bad photo.\" However, this semantic similarity overlooks a critical yet underexplored cue: the magnitude of the CLIP image features, which we empirically find to exhibit a strong correlation with perceptual quality. In this work, we introduce a novel adaptive fusion framework that complements cosine similarity with a magnitude-aware quality cue. Specifically, we first extract the absolute CLIP image features and apply a Box-Cox transformation to statistically normalize the feature distribution and mitigate semantic sensitivity. The resulting scalar summary serves as a semantically-normalized auxiliary cue that complements cosine-based prompt matching. To integrate both cues effectively, we further design a confidence-guided fusion scheme that adaptively weighs each term according to its relative strength. Extensive experiments on multiple benchmark IQA datasets demonstrate that our method consistently outperforms standard CLIP-based IQA and state-of-the-art baselines, without any task-specific training.", "AI": {"tldr": "本文提出了一种新颖的自适应融合框架，将余弦相似度与CLIP特征幅度相结合，用于无参考图像质量评估（NR-IQA），在多个基准数据集上表现出优于标准CLIP模型及其他先进基线的效果。", "motivation": "当前的CLIP模型对于无参考图像质量评估（NR-IQA）主要基于图像嵌入与文本提示之间的余弦相似度，这忽略了CLIP图像特征的幅度这一关键但未充分探索的线索。我们发现该幅度在经验上与感知质量表现出较强的相关性。", "method": "我们提出了一种新颖的自适应融合框架，该框架结合了余弦相似度与幅度感知的质量提示。具体来说，我们首先提取CLIP图像特征的绝对值，并应用Box-Cox变换来统计性地归一化特征分布，从而减少语义敏感度。这样得到的标量总结作为语义归一化的辅助提示，补充了基于余弦的提示匹配。为了有效地整合这两个提示，我们还设计了一种自信引导的融合方案，根据每个项目的相对强度自适应加权。", "result": "实验结果表明，我们的方法在多个IQ数据集上取得了比标准CLIP-based IQA和最新的基准更高的性能。", "conclusion": "实验证明，我们的方法在多个IQA基准数据集上表现优于标准CLIP-based IQA和最先进的基线方法，并且无需进行特定任务的训练。"}}
