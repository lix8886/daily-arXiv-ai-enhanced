{"id": "2510.00125", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.00125", "abs": "https://arxiv.org/abs/2510.00125", "authors": ["Hong kyu Lee", "Ruixuan Liu", "Li Xiong"], "title": "Direct Token Optimization: A Self-contained Approach to Large Language Model Unlearning", "comment": null, "summary": "Machine unlearning is an emerging technique that removes the influence of a\nsubset of training data (forget set) from a model without full retraining, with\napplications including privacy protection, content moderation, and model\ncorrection. The key challenge lies in ensuring that the model completely\nforgets the knowledge of the forget set without compromising its overall\nutility. Existing unlearning methods for large language models (LLMs) often\nutilize auxiliary language models, retain datasets, or even commercial AI\nservices for effective unlearning and maintaining the model utility. However,\ndependence on these external resources is often impractical and could\npotentially introduce additional privacy risks. In this work, we propose direct\ntoken optimization (DTO), a novel self-contained unlearning approach for LLMs\nthat directly optimizes the token level objectives and eliminates the need for\nexternal resources. Given a sequence to unlearn, we identify two categories of\ntokens: target tokens, which capture critical knowledge for unlearning, and the\nremaining non-target tokens, which are crucial for maintaining the model\nutility. The former are used to optimize the unlearning objective, while the\nlatter serve to preserve the model's performance. The experimental results show\nthat the proposed DTO achieves up to 16.8$\\times$ improvement in forget quality\non several benchmark datasets than the latest baselines while maintaining a\ncomparable level of model utility.", "AI": {"tldr": "本文提出了一种名为直接令牌优化（DTO）的自包含机器遗忘方法，该方法提高了遗忘质量和保持了模型性能，且不依赖外部资源。", "motivation": "现有的大型语言模型的遗忘方法通常依赖外部资源如辅助语言模型、保留数据集或甚至商业AI服务，这在实践中往往是不切实际的，并且可能会引入额外的隐私风险。", "method": "提出了一种名为直接令牌优化（DTO）的新自包含机器遗忘方法，该方法通过直接优化令牌级别的目标，消除了对外部资源的需求。", "result": "实验结果表明，所提出的DTO在多个基准数据集上的遗忘质量比最新的基线方法提高了多达16.8倍，同时保持了可比的模型实用性。", "conclusion": "DTO方法通过直接优化令牌级别的目标，实现了消除外部资源依赖，提高了遗忘质量和保持模型实用性。"}}
{"id": "2510.00161", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00161", "abs": "https://arxiv.org/abs/2510.00161", "authors": ["Kimihiro Hasegawa", "Wiradee Imrattanatrai", "Masaki Asada", "Ken Fukuda", "Teruko Mitamura"], "title": "TAMA: Tool-Augmented Multimodal Agent for Procedural Activity Understanding", "comment": "21 pages. Code: https://github.com/kimihiroh/tama", "summary": "Procedural activity assistants potentially support humans in a variety of\nsettings, from our daily lives, e.g., cooking or assembling flat-pack\nfurniture, to professional situations, e.g., manufacturing or biological\nexperiments. Despite its potential use cases, the system development tailored\nfor such an assistant is still underexplored. In this paper, we propose a novel\nframework, called TAMA, a Tool-Augmented Multimodal Agent, for procedural\nactivity understanding. TAMA enables interleaved multimodal reasoning by making\nuse of multimedia-returning tools in a training-free setting. Our experimental\nresult on the multimodal procedural QA dataset, ProMQA-Assembly, shows that our\napproach can improve the performance of vision-language models, especially\nGPT-5 and MiMo-VL. Furthermore, our ablation studies provide empirical support\nfor the effectiveness of two features that characterize our framework,\nmultimedia-returning tools and agentic flexible tool selection. We believe our\nproposed framework and experimental results facilitate the thinking with images\nparadigm for video and multimodal tasks, let alone the development of\nprocedural activity assistants.", "AI": {"tldr": "本文介绍了一种创新的程序性活动理解框架TAMA，该框架使用无需训练的多媒体工具在多模态情况下工作，提升了与视觉语言模型结合后的性能表现。", "motivation": "论文旨在解决专为程序性活动助手开发的系统尚不成熟的问题，目标是支撑人类在日常生活和专业环境中进行程序性活动的理解与辅助。", "method": "我们提出了一种名为TAMA（Tool-Augmented Multimodal Agent）的框架，该框架能够在无需训练的情况下通过使用多媒体工具进行交错的多模态推理。", "result": "实验结果表明，与GPT-5和MiMo-VL等视觉语言模型结合时，我们的方法能提升性能。另外，消融研究亦证实了框架的两个关键特征：多媒体返回工具和代理灵活工具选择的有效性。", "conclusion": "我们的框架和实验结果促进了图像思考范式的应用，不仅局限于视频和多模态任务领域，还促进了程序性活动助手的发展。"}}
{"id": "2510.00172", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00172", "abs": "https://arxiv.org/abs/2510.00172", "authors": ["Amirhossein Abaskohi", "Tianyi Chen", "Miguel Muñoz-Mármol", "Curtis Fox", "Amrutha Varshini Ramesh", "Étienne Marcotte", "Xing Han Lù", "Nicolas Chapados", "Spandana Gella", "Christopher Pal", "Alexandre Drouin", "Issam H. Laradji"], "title": "DRBench: A Realistic Benchmark for Enterprise Deep Research", "comment": null, "summary": "We introduce DRBench, a benchmark for evaluating AI agents on complex,\nopen-ended deep research tasks in enterprise settings. Unlike prior benchmarks\nthat focus on simple questions or web-only queries, DRBench evaluates agents on\nmulti-step queries (for example, ``What changes should we make to our product\nroadmap to ensure compliance with this standard?\") that require identifying\nsupporting facts from both the public web and private company knowledge base.\nEach task is grounded in realistic user personas and enterprise context,\nspanning a heterogeneous search space that includes productivity software,\ncloud file systems, emails, chat conversations, and the open web. Tasks are\ngenerated through a carefully designed synthesis pipeline with\nhuman-in-the-loop verification, and agents are evaluated on their ability to\nrecall relevant insights, maintain factual accuracy, and produce coherent,\nwell-structured reports. We release 15 deep research tasks across 10 domains,\nsuch as Sales, Cybersecurity, and Compliance. We demonstrate the effectiveness\nof DRBench by evaluating diverse DR agents across open- and closed-source\nmodels (such as GPT, Llama, and Qwen) and DR strategies, highlighting their\nstrengths, weaknesses, and the critical path for advancing enterprise deep\nresearch. Code is available at https://github.com/ServiceNow/drbench.", "AI": {"tldr": "本文提出了DRBench，一个针对企业环境中复杂开放性深度研究任务的AI代理评估基准。", "motivation": "传统的基准测试专注于简单问题或仅限于网络查询，但并不能完全捕捉到现实世界中企业环境下复杂的问题解决过程。因此，我们需要一个能评估AI代理在多步骤查询和寻找异构数据源中相关信息能力的基准。", "method": "我们提出了DRBench，这是一个用于评估企业环境中AI代理在复杂开放性深度研究任务上的基准测试工具。DRBench通过多步骤查询评估代理，这些查询需要从公共网络和私人公司知识库中识别支持性事实。每个任务都是基于现实的用户角色和企业情境设计的，涵盖了包括生产力软件、云文件系统、电子邮件、聊天对话和开放网络在内的异构搜索空间。任务通过一个精心设计的合成管道生成，并经由人工验证。代理的评估基于它们召回相关信息、保持事实准确性以及生成连贯且结构良好的报告的能力。", "result": "我们发布了跨越10个领域的15个深度研究任务，如销售、网络安全和合规性。通过评估来自开源和闭源模型（如GPT、Llama和Qwen）的多样DR代理以及不同的DR策略，我们展示了DRBench的有效性，指出了这些模型和策略的优势、劣势以及推进企业深度研究的关键路径。", "conclusion": "DRBench作为一个全面的、针对企业深度研究任务的评估框架，强调了复杂信息整合能力的需求，为提升AI代理在企业应用场景中的性能提供了指导。"}}
{"id": "2510.00174", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00174", "abs": "https://arxiv.org/abs/2510.00174", "authors": ["Rik Koncel-Kedziorski", "Brihi Joshi", "Tim Paek"], "title": "PrimeX: A Dataset of Worldview, Opinion, and Explanation", "comment": "EMNLP 2025 Main", "summary": "As the adoption of language models advances, so does the need to better\nrepresent individual users to the model. Are there aspects of an individual's\nbelief system that a language model can utilize for improved alignment?\nFollowing prior research, we investigate this question in the domain of opinion\nprediction by developing PrimeX, a dataset of public opinion survey data from\n858 US residents with two additional sources of belief information: written\nexplanations from the respondents for why they hold specific opinions, and the\nPrimal World Belief survey for assessing respondent worldview. We provide an\nextensive initial analysis of our data and show the value of belief\nexplanations and worldview for personalizing language models. Our results\ndemonstrate how the additional belief information in PrimeX can benefit both\nthe NLP and psychological research communities, opening up avenues for further\nstudy.", "AI": {"tldr": "本研究开发了PrimeX数据集，以研究个人信念在增强语言模型用户对齐中的作用，发现信念解释和世界观能够促进语言模型的个性化。", "motivation": "随着语言模型的应用越来越广泛，更好地表示个人用户对模型的需求也在增加。本研究试图探究个人信念系统是否存在可以用于增强语言模型与用户对齐的信息。", "method": "通过开发PrimeX数据集，该数据集包含了来自858名美国居民的公共意见调查数据，以及两个额外的个人信念信息来源：受访者撰写的具体意见的理由，以及评估受访者世界观的原始世界信念调查。", "result": "初步分析结果表明，信念解释和世界观对于个性化语言模型具有价值。此外，PrimeX中的额外信念信息可以对自然语言处理社区和心理学研究社区产生积极作用。", "conclusion": "研究表明，PrimeX数据集中的信念信息可以为个性化语言模型提供帮助，并为NLP和心理学研究开辟新的研究途径。"}}
{"id": "2510.00033", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00033", "abs": "https://arxiv.org/abs/2510.00033", "authors": ["Usman Muhammad", "Jorma Laaksonen"], "title": "Hybrid Deep Learning for Hyperspectral Single Image Super-Resolution", "comment": null, "summary": "Hyperspectral single image super-resolution (SISR) is a challenging task due\nto the difficulty of restoring fine spatial details while preserving spectral\nfidelity across a wide range of wavelengths, which limits the performance of\nconventional deep learning models. To address this challenge, we introduce\nSpectral-Spatial Unmixing Fusion (SSUF), a novel module that can be seamlessly\nintegrated into standard 2D convolutional architectures to enhance both spatial\nresolution and spectral integrity. The SSUF combines spectral unmixing with\nspectral--spatial feature extraction and guides a ResNet-based convolutional\nneural network for improved reconstruction. In addition, we propose a custom\nSpatial-Spectral Gradient Loss function that integrates mean squared error with\nspatial and spectral gradient components, encouraging accurate reconstruction\nof both spatial and spectral features. Experiments on three public remote\nsensing hyperspectral datasets demonstrate that the proposed hybrid deep\nlearning model achieves competitive performance while reducing model\ncomplexity.", "AI": {"tldr": "本文介绍了一种新的模型SSUF，它能够提高高光谱图像的空间分辨率和光谱保真度，并通过实验验证了该模型在减少复杂度的同时保持了高性能。", "motivation": "提出SSUF的目的在于解决高光谱单图像超分辨率（SISR）的难题，即在恢复精细空间细节的同时保持广泛的波长范围内的光谱保真度，这限制了常规深度学习模型的性能。", "method": "提出了一种新的模块Spectral-Spatial Unmixing Fusion (SSUF)，可以无缝集成到标准的2D卷积架构中，以提升空间分辨率和光谱完整性。SSUF结合了光谱解混与光谱-空间特征提取，指导基于ResNet的卷积神经网络进行改进的重建。另外，还提出了一种自定义的空谱梯度损失函数，整合了均方误差和空谱梯度组件，以鼓励精确重建空间和光谱特征。", "result": "实验在三个公开的遥感高光谱数据集上进行，验证了所提出的混合深度学习模型在减少模型复杂性的同时实现了竞争性的性能。", "conclusion": "该研究通过提出SSUF模块和自定义的空谱梯度损失函数，改进了高光谱单图像超分辨率的性能，并在这种改进基础上减少了模型的复杂性。"}}
{"id": "2510.00177", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00177", "abs": "https://arxiv.org/abs/2510.00177", "authors": ["Shuyue Stella Li", "Avinandan Bose", "Faeze Brahman", "Simon Shaolei Du", "Pang Wei Koh", "Maryam Fazel", "Yulia Tsvetkov"], "title": "Personalized Reasoning: Just-In-Time Personalization and Why LLMs Fail At It", "comment": "57 pages, 6 figures", "summary": "Current large language model (LLM) development treats task-solving and\npreference alignment as separate challenges, optimizing first for objective\ncorrectness, then for alignment to aggregated human preferences. This paradigm\nfails in human-facing applications where solving a problem correctly is\ninsufficient if the response mismatches the user's needs. This challenge\nintensifies in just-in-time scenarios where no prior user interaction history\nexists due to cold-start conditions or privacy constraints. LLMs need to\nidentify what they don't know about user preferences, strategically elicit\npreference values through questioning, then adapt their reasoning processes and\nresponses accordingly -- a complicated chain of cognitive processes which we\nterm personalized reasoning. We introduce PREFDISCO, an evaluation methodology\nthat transforms static benchmarks into interactive personalization tasks using\npsychologically-grounded personas with sparse preferences. Our framework\ncreates scenarios where identical questions require different reasoning chains\ndepending on user context, as optimal explanation approaches vary by individual\nexpertise and preferences while maintaining factual accuracy. Evaluation of 21\nfrontier models across 10 tasks reveals 29.0% of naive personalization attempts\nproduce worse preference alignment than generic responses, yet generic\nresponses also fail to serve individual user needs effectively. These findings\nsuggest personalized reasoning requires dedicated development rather than\nemerging naturally. PREFDISCO establishes personalized reasoning as a\nmeasurable research frontier and reveals fundamental limitations in current\nLLMs' interactive capabilities, providing a foundation for developing systems\nthat can adapt to individual users in education, healthcare, and technical\ndomains where personalization is critical.", "AI": {"tldr": "PREFDISCO 方法被提出以帮助语言模型理解和应对个性化的推理需求，特别是当在没有用户交互历史的情况下需要即时满足个性化需求时。发现 29%的个性化推理尝试未达到理想效果。", "motivation": "当前的语言模型开发框架无法有效应对即时应用场景中用户个性化需求，尤其是在冷启动条件或隐私限制下的情况下。这种挑战要求模型不仅仅是解决任务正确，还需要精确地匹配用户的特定偏好。", "method": "当前的大语言模型(LLM)开发将任务解决和偏好对齐视为两个独立的挑战，先优化客观正确性，然后对齐众多人类偏好。然而，在面向人类的应用中，仅解决问题是不够的，如果响应与用户需求不匹配，则会失败。在冷启动条件或隐私限制下，即时场景中没有之前的用户交互历史，这一挑战更为严重。LLM 需要识别它们对用户偏好的未知部分，通过提问策略性地获取偏好值，然后相应地调整它们的推理过程和响应。我们引入了一个名为 PREFDISCO 的评估方法，该方法可以将静态基准转化为基于心理学的具有稀疏偏好的人设的互动个性化任务。框架创建了情景，其中相同的问题需要根据用户背景来不同的推理链，因为最优解释方法取决于个人专业知识和偏好，同时保持事实准确性。评估前沿模型的性能显示，29.0% 的个人化尝试表现出比通用响应更差的偏好对齐，而通用响应也无法很好地满足单个用户的需求。这些发现表明个性化推理需要专门开发而不应自然产生。PREFDISCO 建立了个性化推理作为研究前沿可度量的领域，并揭示了当前 LLM 交互能力的基本限制，为开发教育、医疗保健和技术领域的个性化系统奠定了基础，这些领域的个性化至关重要。", "result": "评估显示，21 个前沿 LLM 模型在 10 个任务中表现不理想，表现出个性化推理需要专门开发，而不仅仅是自然演化。", "conclusion": "研究突出了个性化推理作为语言模型研究的新领域，强调了当前 LLM 的交互能力存在基本限制。PREFDISCO 为未来在教育、医疗保健和技术领域的个性化系统发展提供了基础。"}}
{"id": "2510.00034", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00034", "abs": "https://arxiv.org/abs/2510.00034", "authors": ["Zhengyi Ho", "Siyuan Liang", "Dacheng Tao"], "title": "Review of Hallucination Understanding in Large Language and Vision Models", "comment": null, "summary": "The widespread adoption of large language and vision models in real-world\napplications has made urgent the need to address hallucinations -- instances\nwhere models produce incorrect or nonsensical outputs. These errors can\npropagate misinformation during deployment, leading to both financial and\noperational harm. Although much research has been devoted to mitigating\nhallucinations, our understanding of it is still incomplete and fragmented.\nWithout a coherent understanding of hallucinations, proposed solutions risk\nmitigating surface symptoms rather than underlying causes, limiting their\neffectiveness and generalizability in deployment. To tackle this gap, we first\npresent a unified, multi-level framework for characterizing both image and text\nhallucinations across diverse applications, aiming to reduce conceptual\nfragmentation. We then link these hallucinations to specific mechanisms within\na model's lifecycle, using a task-modality interleaved approach to promote a\nmore integrated understanding. Our investigations reveal that hallucinations\noften stem from predictable patterns in data distributions and inherited\nbiases. By deepening our understanding, this survey provides a foundation for\ndeveloping more robust and effective solutions to hallucinations in real-world\ngenerative AI systems.", "AI": {"tldr": "研究提供了全面描述图像和文本幻觉现象的框架，揭示幻觉背后的可预测模式和偏差，为开发更强大的生成AI系统提供了解决问题的基石。", "motivation": "幻觉现象不仅会导致传播错误信息，还可能导致财务和运营上的危害，构建一个全面理解幻觉的框架十分有必要。", "method": "通过构建一个统一的多层次框架来描述图像和文本幻觉，此框架涵盖了多种应用场景，并采用任务-模式交织的方法来促进对幻觉的综合理解。研究揭示幻觉通常源于数据分布中的可预测模式和继承偏差。", "result": "调查研究表明，幻觉往往源自数据分布中的可预测模式和继承偏差，提供了一个坚实的基础来开发更强大且有效的方法解决现实世界生成AI系统的幻觉问题。", "conclusion": "本调查提供了一个全面理解幻觉现象的基础，有助于支持在生成AI系统中开发更强大且有效的方法。"}}
{"id": "2510.00232", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00232", "abs": "https://arxiv.org/abs/2510.00232", "authors": ["Xin Xu", "Xunzhi He", "Churan Zhi", "Ruizhe Chen", "Julian McAuley", "Zexue He"], "title": "BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses", "comment": "Work in progress", "summary": "Existing studies on bias mitigation methods for large language models (LLMs)\nuse diverse baselines and metrics to evaluate debiasing performance, leading to\ninconsistent comparisons among them. Moreover, their evaluations are mostly\nbased on the comparison between LLMs' probabilities of biased and unbiased\ncontexts, which ignores the gap between such evaluations and real-world use\ncases where users interact with LLMs by reading model responses and expect fair\nand safe outputs rather than LLMs' probabilities. To enable consistent\nevaluation across debiasing methods and bridge this gap, we introduce\nBiasFreeBench, an empirical benchmark that comprehensively compares eight\nmainstream bias mitigation techniques (covering four prompting-based and four\ntraining-based methods) on two test scenarios (multi-choice QA and open-ended\nmulti-turn QA) by reorganizing existing datasets into a unified query-response\nsetting. We further introduce a response-level metric, Bias-Free Score, to\nmeasure the extent to which LLM responses are fair, safe, and\nanti-stereotypical. Debiasing performances are systematically compared and\nanalyzed across key dimensions: the prompting vs. training paradigm, model\nsize, and generalization of different training strategies to unseen bias types.\nWe will publicly release our benchmark, aiming to establish a unified testbed\nfor bias mitigation research.", "AI": {"tldr": "研究者提出了BiasFreeBench，一个用于比较八种主流去偏方法的统一评价基准，同时引入了Bias-Free Score来衡量模型回复的公平性和安全性，旨在解决现有研究的一致性和现实差距问题。", "motivation": "现有的去偏方法存在着因评估方法不同而导致的不一致问题，并且现有的评估方式忽视了现实使用场景的差异。因此，为了提供更加统一和贴近实际使用情况的评估方式，引入了BiasFreeBench和Bias-Free Score，为目的在此。", "method": "现有的研究在评估大型语言模型（LLMs）的去偏方法时使用了多样化的基线和指标，导致了不一致的对比结果。此外，这些评估大多依赖于LLMs对偏见和非偏见上下文概率的比较，忽视了与真实使用场景之间的差距。为了能够在去偏方法之间进行一致的评估并填补这一差距，研究人员引入了BiasFreeBench，这是一个实证基准，全面比较了八种主流的去偏技巧（包括四种基于提示的方法和四种基于训练的方法），这八种方法在两个测试场景（多选题QA和开放性的多轮QA）中进行比较，通过重组现有数据集进行统一的查询-响应设置。另外，引入了一个针对回复的指标，即Bias-Free Score，用于衡量LLM回复是否公平、安全、以及反刻板印象的程度。去偏性能会根据几个关键维度进行系统比较和分析：提示法与训练法的范式、模型大小、以及不同训练策略向未见过的偏见类型推广的一般化能力。", "result": "此次研究比较了八种主流的去偏方法（四种基于提示和四种基于训练的方法），提出通过Bias-Free Score来衡量模型回复的质量，并展示了一套可以系统地比较和分析去偏效果的方法。", "conclusion": "通过BiasFreeBench和Bias-Free Score的引入，研究者希望提供一个统一且接近实际应用场景的评估环境，以鼓励更加公平和安全的LLM模型的发展。同时，作者还计划公开此基准，以促进行业内对去偏研究的统一性和协作性。"}}
{"id": "2510.00037", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00037", "abs": "https://arxiv.org/abs/2510.00037", "authors": ["Jianing Guo", "Zhenhong Wu", "Chang Tu", "Yiyao Ma", "Xiangqi Kong", "Zhiqian Liu", "Jiaming Ji", "Shuning Zhang", "Yuanpei Chen", "Kai Chen", "Xianglong Liu", "Qi Dou", "Yaodong Yang", "Huijie Zhao", "Weifeng Lv", "Simin Li"], "title": "On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations", "comment": null, "summary": "In Vision-Language-Action (VLA) models, robustness to real-world\nperturbations is critical for deployment. Existing methods target simple visual\ndisturbances, overlooking the broader multi-modal perturbations that arise in\nactions, instructions, environments, and observations. Here, we first evaluate\nthe robustness of mainstream VLAs under 17 perturbations across four\nmodalities. We find (1) actions as the most fragile modality, (2) Existing\nvisual-robust VLA do not gain robustness in other modality, and (3) pi0\ndemonstrates superior robustness with a diffusion-based action head. To build\nmulti-modal robust VLAs, we propose RobustVLA against perturbations in VLA\ninputs and outputs. For output robustness, we perform offline robust\noptimization against worst-case action noise that maximizes mismatch in flow\nmatching objective. This can be seen as adversarial training, label smoothing,\nand outlier penalization. For input robustness, we enforce consistent actions\nacross input variations that preserve task semantics. To account for multiple\nperturbations, we formulate robustness as a multi-armed bandit problem and\napply an upper confidence bound algorithm to automatically identify the most\nharmful noise. Experiments on LIBERO demonstrate our RobustVLA delivers\nabsolute gains over baselines of 12.6% on the pi0 backbone and 10.4% on the\nOpenVLA backbone across all 17 perturbations, achieving 50.6x faster inference\nthan existing visual-robust VLAs, and a 10.4% gain under mixed perturbations.\nOur RobustVLA is particularly effective on real-world FR5 robot with limited\ndemonstrations, showing absolute gains by 65.6% under perturbations of four\nmodalities.", "AI": {"tldr": "提出了一种新的VLA模型RobustVLA，它在输入和输出方面都获得了更好的鲁棒性，显著提高了在实际应用中的性能。", "motivation": "在VLA模型中，对于实际世界中产生的多模态扰动的鲁棒性对于模型部署至关重要。当前的VLA方法专注于简单的视觉干扰，忽略了在动作、指令、环境和观测中产生的更广泛的多模态扰动。为了提高VLA模型的多模态鲁棒性，提出了RobustVLA。", "method": "提出了一种针对VLA输入和输出鲁棒性的方法RobustVLA。在输出鲁棒性方面，进行了离线鲁棒优化，对抗最坏的动作噪声以最大化流匹配目标的不匹配度。这可以看作是对抗训练、标签平滑和离群值惩罚。在输入鲁棒性方面，强加了动作在保持任务语义的输入变化中的一致性。为了处理多个扰动，通过将鲁棒性建模为多臂强盗问题，并应用上置信界算法来自动识别最具危害性的噪声。", "result": "在LIBERO实验中，RobustVLA在所有17种扰动下相对于基线分别取得了pi0架构12.6%，OpenVLA架构10.4%的绝对增益，并且推理速度比现有的视觉鲁棒VLA快50.6倍。在四种模态的扰动下，应用于具有有限示范的真实FR5机器人时，绝对增益达到了65.6%。", "conclusion": "实验证明，所提出的RobustVLA模型在对抗输入和输出扰动方面增强了鲁棒性，并在综合多个扰动的情况下表现更为优越。"}}
{"id": "2510.00255", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00255", "abs": "https://arxiv.org/abs/2510.00255", "authors": ["Monishwaran Maheswaran", "Marco Carini", "Christian Federmann", "Tony Diaz"], "title": "TASER: Translation Assessment via Systematic Evaluation and Reasoning", "comment": null, "summary": "We introduce TASER (Translation Assessment via Systematic Evaluation and\nReasoning), a metric that uses Large Reasoning Models (LRMs) for automated\ntranslation quality assessment. TASER harnesses the explicit reasoning\ncapabilities of LRMs to conduct systematic, step-by-step evaluation of\ntranslation quality. We evaluate TASER on the WMT24 Metrics Shared Task across\nboth reference-based and reference-free scenarios, demonstrating\nstate-of-the-art performance. In system-level evaluation, TASER achieves the\nhighest soft pairwise accuracy in both reference-based and reference-free\nsettings, outperforming all existing metrics. At the segment level, TASER\nmaintains competitive performance with our reference-free variant ranking as\nthe top-performing metric among all reference-free approaches. Our experiments\nreveal that structured prompting templates yield superior results with LRMs\ncompared to the open-ended approaches that proved optimal for traditional LLMs.\nWe evaluate o3, a large reasoning model from OpenAI, with varying reasoning\nefforts, providing insights into the relationship between reasoning depth and\nevaluation quality. The explicit reasoning process in LRMs offers\ninterpretability and visibility, addressing a key limitation of existing\nautomated metrics. Our results demonstrate that Large Reasoning Models show a\nmeasurable advancement in translation quality assessment, combining improved\naccuracy with transparent evaluation across diverse language pairs.", "AI": {"tldr": "本研究开发了TASER，一种通过利用大型推理模型（LRMs）进行自动化翻译质量评估的指标，在多个评估任务中表现出色。", "motivation": "这项研究的动机是为了克服现行自动化评估指标的问题，通过引入系统、分步骤的评估方法，利用LRMs的显式推理能力来提升翻译质量评估的准确性和透明度。", "method": "我们提出了TASER（通过系统评估和推理进行翻译评估）这一指标，该指标利用大型推理模型（LRMs）来进行自动化翻译质量评估。TASER利用LRMs的显式推理能力，进行系统化的、分步骤的评估。", "result": "TASER在WMT24 Metrics Shared Task中的系统级评估中，在基于参考和非基于参考设置下均取得了最高的软成对准确性，并在段落层面的非基于参考评估中表现优异。", "conclusion": "研究证明，大型推理模型在翻译质量评估方面比传统语言模型有明显的进步，结合了更高的准确性和透明性，可以应用于多种语言对。"}}
{"id": "2510.00040", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00040", "abs": "https://arxiv.org/abs/2510.00040", "authors": ["Junjie Li", "Ziao Wang", "Jianghong Ma", "Xiaofeng Zhang"], "title": "Uncovering Intrinsic Capabilities: A Paradigm for Data Curation in Vision-Language Models", "comment": null, "summary": "Large vision-language models (VLMs) achieve strong benchmark performance, but\ncontrolling their behavior through instruction tuning remains difficult.\nReducing the budget of instruction tuning dataset often causes regressions, as\nheuristic strategies treat models as black boxes and overlook the latent\ncapabilities that govern learning. We introduce Capability-Attributed Data\nCuration (CADC), a framework that shifts curation from task-specific heuristics\nto intrinsic capability analysis. CADC discovers intrinsic capabilities in an\nunsupervised manner from gradient-based learning trajectories, attributes\ntraining data to these capabilities via influence estimation, and curates\ncapability-aware curricula through balanced selection and staged sequencing.\nThis transforms black-box instruction tuning into a controllable,\ncapability-driven process. With as little as 5% of the original data, CADC\nsurpasses full-data training on multimodal benchmarks. These results validate\nintrinsic capabilities as the fundamental building blocks of model learning and\nestablish CADC as a principle paradigm for instruction data curation.", "AI": {"tldr": "CADC框架调整了从任务特定启发式到内在能力分析的策划过程，即使使用减少了95%的数据量，仍能超越全数据训练在多模态基准上的表现。", "motivation": "强大的VLMs模型面临着指令调优困难的问题，特别是在减少指令调优数据集预算时性能下降。传统启发式策略将模型视为黑盒，忽视了指导学习的潜在能力。", "method": "CADC框架通过基于梯度的学习轨迹在无监督的情况下发现内在能力，并根据影响力估计将训练数据归因于这些能力。通过平衡选择和分阶段排序来策划能力感知的课程，从而将指令调优从任务特定的启发式方法转变为内在能力分析。", "result": "使用仅仅5％的原始数据，CADC在多模态基准测试上超越了全数据训练。", "conclusion": "结果表明，内在能力是模型学习的基本构建模块，CADC作为指令数据策划的主要范例得到了确立。"}}
{"id": "2510.00261", "categories": ["cs.CL", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.00261", "abs": "https://arxiv.org/abs/2510.00261", "authors": ["Xiaoyu Song", "William Han", "Tony Chen", "Chaojing Duan", "Michael A. Rosenberg", "Emerson Liu", "Ding Zhao"], "title": "Retrieval-Augmented Generation for Electrocardiogram-Language Models", "comment": "5 pages, 2 figures; Submitted to ICASSP 2026", "summary": "Interest in generative Electrocardiogram-Language Models (ELMs) is growing,\nas they can produce textual responses conditioned on ECG signals and textual\nqueries. Unlike traditional classifiers that output label probabilities, ELMs\nare more versatile, supporting domain-specific tasks (e.g., waveform analysis,\ndiagnosis, prognosis) as well as general tasks (e.g., open-ended questions,\ndialogue). Retrieval-Augmented Generation (RAG), widely used in Large Language\nModels (LLMs) to ground LLM outputs in retrieved knowledge, helps reduce\nhallucinations and improve natural language generation (NLG). However, despite\nits promise, no open-source implementation or systematic study of RAG pipeline\ndesign for ELMs currently exists. To address this gap, we present the first\nopen-source RAG pipeline for ELMs, along with baselines and ablation studies\nfor NLG. Experiments on three public datasets show that ELMs with RAG\nconsistently improves performance over non-RAG baselines and highlights key ELM\ndesign considerations. Our code is available at:\nhttps://github.com/willxxy/ECG-Bench.", "AI": {"tldr": "我们开发了首个开源RAG管道，用于生成性心电图语言模型，提高了模型的可信度和生成结果，已在三个公开数据集上验证了这一效果。", "motivation": "由于现有研究中缺乏针对ELMs的RAG管道设计的系统性研究及开源实现，本研究旨在填补这一空白，并改善生成模型的可信赖性和生成质量。", "method": "本研究提出了首个开源的检索增强生成（RAG）管道，用于生成性心电图语言模型（ELMs），并通过基线和消融研究来评估其在自然语言生成（NLG）方面的能力。", "result": "实验表明，在多个公共数据集上，使用RAG的ELM模型在生成性能方面优于未使用RAG的基线模型。", "conclusion": "实验结果表明，在加入RAG后，ELMs在自然语言生成任务中的性能有了显著的提升，此研究还强调了ELM设计时需要考虑的关键点。"}}
{"id": "2510.00041", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00041", "abs": "https://arxiv.org/abs/2510.00041", "authors": ["Yuchen Song", "Andong Chen", "Wenxin Zhu", "Kehai Chen", "Xuefeng Bai", "Muyun Yang", "Tiejun Zhao"], "title": "Culture In a Frame: C$^3$B as a Comic-Based Benchmark for Multimodal Culturally Awareness", "comment": null, "summary": "Cultural awareness capabilities has emerged as a critical capability for\nMultimodal Large Language Models (MLLMs). However, current benchmarks lack\nprogressed difficulty in their task design and are deficient in cross-lingual\ntasks. Moreover, current benchmarks often use real-world images. Each\nreal-world image typically contains one culture, making these benchmarks\nrelatively easy for MLLMs. Based on this, we propose C$^3$B ($\\textbf{C}$omics\n$\\textbf{C}$ross-$\\textbf{C}$ultural $\\textbf{B}$enchmark), a novel\nmulticultural, multitask and multilingual cultural awareness capabilities\nbenchmark. C$^3$B comprises over 2000 images and over 18000 QA pairs,\nconstructed on three tasks with progressed difficulties, from basic visual\nrecognition to higher-level cultural conflict understanding, and finally to\ncultural content generation. We conducted evaluations on 11 open-source MLLMs,\nrevealing a significant performance gap between MLLMs and human performance.\nThe gap demonstrates that C$^3$B poses substantial challenges for current\nMLLMs, encouraging future research to advance the cultural awareness\ncapabilities of MLLMs.", "AI": {"tldr": "提出了一种新型的CMC交叉文化基准C$^3$B，以提高MLLMs的文化意识能力评估，通过包含多种文化、多任务和多语言的高难度任务来挑战当前的MLLMs。", "motivation": "当前基准存在任务设计难度提高不足和跨语言任务缺乏的问题。此外，基准往往使用现实世界的图像，每个图像通常只包含一种文化，这使得基准对MLLMs来说相对较容易。基于这些问题，我们提出了C$^3$B。", "method": "C$^3$B (Comics Cross-Cultural Benchmark)是一种新型的多元文化、多任务和多语言的文化意识能力评估基准。它包含了超过2000张图像和超过18000个问答对，基于三种难度逐渐增加的任务构建，从基本的视觉识别到更高级的文化冲突理解，最后是文化内容生成。", "result": "评估了11个开放源码的MLLMs的性能，显示MLLMs与人类的性能之间存在显著差距。这表明C$^3$B对当前的MLLMs提出了实质性的挑战。", "conclusion": "C$^3$B展示出对于当前MLLMs来说是一个重大的挑战，鼓励未来的研究提高MLLMs的文化意识能力。"}}
{"id": "2510.00263", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00263", "abs": "https://arxiv.org/abs/2510.00263", "authors": ["Zhuohang Li", "Xiaowei Li", "Chengyu Huang", "Guowang Li", "Katayoon Goshvadi", "Bo Dai", "Dale Schuurmans", "Paul Zhou", "Hamid Palangi", "Yiwen Song", "Palash Goyal", "Murat Kantarcioglu", "Bradley A. Malin", "Yuan Xue"], "title": "Judging with Confidence: Calibrating Autoraters to Preference Distributions", "comment": null, "summary": "The alignment of large language models (LLMs) with human values increasingly\nrelies on using other LLMs as automated judges, or ``autoraters''. However,\ntheir reliability is limited by a foundational issue: they are trained on\ndiscrete preference labels, forcing a single ground truth onto tasks that are\noften subjective, ambiguous, or nuanced. We argue that a reliable autorater\nmust learn to model the full distribution of preferences defined by a target\npopulation. In this paper, we propose a general framework for calibrating\nprobabilistic autoraters to any given preference distribution. We formalize the\nproblem and present two learning methods tailored to different data conditions:\n1) a direct supervised fine-tuning for dense, probabilistic labels, and 2) a\nreinforcement learning approach for sparse, binary labels. Our empirical\nresults show that finetuning autoraters with a distribution-matching objective\nleads to verbalized probability predictions that are better aligned with the\ntarget preference distribution, with improved calibration and significantly\nlower positional bias, all while preserving performance on objective tasks.", "AI": {"tldr": "提出了一个通用框架，用于校准与目标人群定义的偏好完整分布相匹配的概率自动化评分者。", "motivation": "自动化评分者（autoraters）的可靠性受限于训练时使用离散偏好标签导致的单一真实情况，这在任务往往是主观、模棱两可或细微差别时构成挑战。为了提高可靠性，自动化评分者必须学习包含目标人群定义的偏好完整分布。", "method": "我们提出了一种用于校准概率自动化评分者的通用框架，该框架可以根据给定的偏好分布进行调整。针对不同的数据条件，我们提出了两种学习方法：1）针对密集的概率标签的直接监督微调；2）针对稀疏的二进制标签的强化学习方法。", "result": "实验结果表明，采用分布匹配目标微调的自动化评分者能产生更符合目标偏好分布的概率预测，具有更好的校准效果以及显著降低的位置偏差，同时保留了在客观任务上的性能。", "conclusion": "该研究提出的方法改善了自动化评分者与人类价值观的对齐问题，通过学习完整偏好的分布来提高自动化评分者的可靠性和一致性。"}}
{"id": "2510.00045", "categories": ["cs.CV", "cs.AI", "I.2 ARTIFICIAL INTELLIGENCE"], "pdf": "https://arxiv.org/pdf/2510.00045", "abs": "https://arxiv.org/abs/2510.00045", "authors": ["Franck Vandewiele", "Remi Synave", "Samuel Delepoulle", "Remi Cozot"], "title": "Beyond the Prompt: Gender Bias in Text-to-Image Models, with a Case Study on Hospital Professions", "comment": null, "summary": "Text-to-image (TTI) models are increasingly used in professional,\neducational, and creative contexts, yet their outputs often embed and amplify\nsocial biases. This paper investigates gender representation in six\nstate-of-the-art open-weight models: HunyuanImage 2.1, HiDream-I1-dev,\nQwen-Image, FLUX.1-dev, Stable-Diffusion 3.5 Large, and Stable-Diffusion-XL.\nUsing carefully designed prompts, we generated 100 images for each combination\nof five hospital-related professions (cardiologist, hospital director, nurse,\nparamedic, surgeon) and five portrait qualifiers (\"\", corporate, neutral,\naesthetic, beautiful).\n  Our analysis reveals systematic occupational stereotypes: all models produced\nnurses exclusively as women and surgeons predominantly as men. However,\ndifferences emerge across models: Qwen-Image and SDXL enforce rigid male\ndominance, HiDream-I1-dev shows mixed outcomes, and FLUX.1-dev skews female in\nmost roles. HunyuanImage 2.1 and Stable-Diffusion 3.5 Large also reproduce\ngender stereotypes but with varying degrees of sensitivity to prompt\nformulation. Portrait qualifiers further modulate gender balance, with terms\nlike corporate reinforcing male depictions and beautiful favoring female ones.\nSensitivity varies widely: Qwen-Image remains nearly unaffected, while\nFLUX.1-dev, SDXL, and SD3.5 show strong prompt dependence.\n  These findings demonstrate that gender bias in TTI models is both systematic\nand model-specific. Beyond documenting disparities, we argue that prompt\nwording plays a critical role in shaping demographic outcomes. The results\nunderscore the need for bias-aware design, balanced defaults, and user guidance\nto prevent the reinforcement of occupational stereotypes in generative AI.", "AI": {"tldr": "研究发现，这些模型在生成护士时大多呈现女性，外科医生则主要呈现男性。性别刻板印象不仅系统化而且因模型而异，提示词的选择对性别表现有重要影响。", "motivation": "研究动机是探讨六种最先进的开源文本到图像模型（HunyuanImage 2.1、HiDream-I1-dev、Qwen-Image、FLUX.1-dev、Stable-Diffusion 3.5 Large、Stable-Diffusion-XL）在性别表现上的系统性职业刻板印象。", "method": "本研究通过精心设计的提示词来生成与五个医院相关职业（心脏病学家、医院主管、护士、救护人员、外科医生）相关的100张图像，并结合五个肖像描述符（“”、商务、中性、美学、美丽）。", "result": "研究揭示了所有模型生成的护士形象均为女性，而外科医生形象主要是男性。Qwen-Image 和 SDXL 模型表现出严格男性主导，HiDream-I1-dev 结果混杂，FLUX.1-dev 大多数角色趋向女性。不同的肖像描述符也调制了性别平衡，例如商务描述符强化男性形象，而美丽描述符则更有利于女性形象。", "conclusion": "研究结果表明，性别偏见在文本到图像模型中既具有系统性又因模型而异。提示词的选择在塑造人口统计结果中发挥着关键作用，进一步强调了开发偏见意识设计、平衡的默认设置以及用户指导的重要性，以防止存在于生成人工智能中的职业刻板印象的强化。"}}
{"id": "2510.00268", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00268", "abs": "https://arxiv.org/abs/2510.00268", "authors": ["Zhexiong Liu", "Diane Litman"], "title": "Efficient Layer-wise LLM Fine-tuning for Revision Intention Prediction", "comment": "In The Conference on Empirical Methods in Natural Language Processing\n  (EMNLP), November 2025", "summary": "Large Language Models (LLMs) have shown extraordinary success across various\ntext generation tasks; however, their potential for simple yet essential text\nclassification remains underexplored, as LLM pre-training tends to emphasize\ngeneration over classification. While LLMs with instruction tuning can\ntransform classification into a generation task, they often struggle to\ncategorize nuanced texts. One such example is text revision, which involves\nnuanced edits between pairs of texts. Although simply fine-tuning LLMs for\nrevision classification seems plausible, it requires a large amount of revision\nannotations, which are exceptionally expensive and scarce in the community. To\naddress this issue, we introduce a plug-and-play layer-wise parameter-efficient\nfine-tuning (PEFT) framework, i.e., IR-Tuning, which fine-tunes a subset of\nimportant LLM layers that are dynamically selected based on their gradient norm\ndistribution, while freezing those of redundant layers. Extensive experiments\nsuggest that IR-Tuning surpasses several layer-wise PEFT baselines over diverse\ntext revisions, while achieving fast convergence, low GPU memory consumption,\nand effectiveness on small revision corpora.", "AI": {"tldr": "论文提出了一种新的参数高效的微调方案IR-Tuning，能够在无需大量标注数据的情况下，提高大型语言模型在文本修订分类任务上的性能和效率。", "motivation": "虽然大型语言模型在生成任务中表现出色，但它们在文本分类上的潜力尚未充分挖掘，特别是在需要注释数据的文本修订任务中。由于相关标注耗时且成本高，本文旨在通过介绍一种新的微调框架解决这一问题。", "method": "本文提出了IR-Tuning，一种分层参数高效微调（PEFT）框架，该框架通过基于梯度范数分布动态选择重要层进行微调，同时冻结冗余层以减少参数量。", "result": "该论文提出了一种新的微调框架IR-Tuning，用于解决大型语言模型在文本修订分类任务上面临的挑战。该框架通过动态选择重要层进行参数高效的微调，同时冻结冗余层，从而在减少标注数据需求的同时，实现快速收敛、低GPU内存消耗和在小规模修订语料库上的有效性。实验结果显示，IR-Tuning在不同的文本修订任务上超越了几种分层参数高效微调的基准方法。", "conclusion": "实验结果表明，IR-Tuning在各种文本修订任务上效果超越了几种分层PEFT基准方法，同时具有快速收敛、低GPU内存消耗和在少量修订语料上的有效性。"}}
{"id": "2510.00046", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00046", "abs": "https://arxiv.org/abs/2510.00046", "authors": ["Xiaotian Zou"], "title": "Reinforcement Learning-Based Prompt Template Stealing for Text-to-Image Models", "comment": "10 pages, 3 figures", "summary": "Multimodal Large Language Models (MLLMs) have transformed text-to-image\nworkflows, allowing designers to create novel visual concepts with\nunprecedented speed. This progress has given rise to a thriving prompt trading\nmarket, where curated prompts that induce trademark styles are bought and sold.\nAlthough commercially attractive, prompt trading also introduces a largely\nunexamined security risk: the prompts themselves can be stolen.\n  In this paper, we expose this vulnerability and present RLStealer, a\nreinforcement learning based prompt inversion framework that recovers its\ntemplate from only a small set of example images. RLStealer treats template\nstealing as a sequential decision making problem and employs multiple\nsimilarity based feedback signals as reward functions to effectively explore\nthe prompt space. Comprehensive experiments on publicly available benchmarks\ndemonstrate that RLStealer gets state-of-the-art performance while reducing the\ntotal attack cost to under 13% of that required by existing baselines. Our\nfurther analysis confirms that RLStealer can effectively generalize across\ndifferent image styles to efficiently steal unseen prompt templates. Our study\nhighlights an urgent security threat inherent in prompt trading and lays the\ngroundwork for developing protective standards in the emerging MLLMs\nmarketplace.", "AI": {"tldr": "本研究介绍了RLStealer，一种基于强化学习的提示逆向框架，能够从少量示例图像中恢复提示模板，这揭示了提示交易市场中的安全隐患。", "motivation": "鉴于提示交易市场的兴起及其未被充分研究的安全风险，本研究指出窃取提示本身的威胁，并提出一种有效的解决方案。", "method": "RLStealer将模板窃取视为一个顺序决策问题，利用多个基于相似性的反馈信号作为奖励函数来探索提示空间。", "result": "实验表明，RLStealer达到了最先进的性能，并将攻击成本降低到现有基线要求的13%以下。", "conclusion": "本研究强调了提示交易市场中存在的紧急安全威胁，并为开发保护标准奠定了基础。"}}
{"id": "2510.00276", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00276", "abs": "https://arxiv.org/abs/2510.00276", "authors": ["Joe Barrow", "Raj Patel", "Misha Kharkovski", "Ben Davies", "Ryan Schmitt"], "title": "SafePassage: High-Fidelity Information Extraction with Black Box LLMs", "comment": null, "summary": "Black box large language models (LLMs) make information extraction (IE) easy\nto configure, but hard to trust. Unlike traditional information extraction\npipelines, the information \"extracted\" is not guaranteed to be grounded in the\ndocument. To prevent this, this paper introduces the notion of a \"safe\npassage\": context generated by the LLM that is both grounded in the document\nand consistent with the extracted information. This is operationalized via a\nthree-step pipeline, SafePassage, which consists of: (1) an LLM extractor that\ngenerates structured entities and their contexts from a document, (2) a\nstring-based global aligner, and (3) a scoring model. Results show that using\nthese three parts in conjunction reduces hallucinations by up to 85% on\ninformation extraction tasks with minimal risk of flagging non-hallucinations.\nHigh agreement between the SafePassage pipeline and human judgments of\nextraction quality mean that the pipeline can be dually used to evaluate LLMs.\nSurprisingly, results also show that using a transformer encoder fine-tuned on\na small number of task-specific examples can outperform an LLM scoring model at\nflagging unsafe passages. These annotations can be collected in as little as\n1-2 hours.", "AI": {"tldr": "本论文提出了一种新的信息提取方法SafePassage，解决了黑盒大语言模型在信息提取中可能出现的问题，能有效减少幻觉并保持高精度。", "motivation": "黑盒大型语言模型（LLMs）使信息提取变得容易配置，但难以信任。为了防止信息提取不基于文档的情况，引入了\"安全段落\"的概念。", "method": "介绍了一种名为SafePassage的三步流水线，用来生成既基于文档又能与提取信息保持一致的安全段落。该流程包括：(1) LLM提取器，从文档中生成结构化的实体及其上下文；(2) 字符串全局对齐器；(3) 分数模型。", "result": "实验结果表明，在信息提取任务中，这三个部分协同使用可以将幻觉率减少高达85%，且不会显著增加误判非幻觉的比例。此外，小规模任务特定样例上微调的变压器编码器在识别不安全段落方面可以优于LLM评分模型。", "conclusion": "SafePassage流水线和人类提取质量判断之间有高度的一致性，这意味着该流水线可以用于评估LLM。同时，实验表明微调的变压器编码器在标记不安全段落方面表现优于LLM模型。"}}
{"id": "2510.00047", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00047", "abs": "https://arxiv.org/abs/2510.00047", "authors": ["Sihao Ding", "Santosh Vasa", "Aditi Ramadwar"], "title": "Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations", "comment": "NeurIPS 2025 workshop on Regulatable ML", "summary": "Vision-Language Models (VLMs) often produce fluent Natural Language\nExplanations (NLEs) that sound convincing but may not reflect the causal\nfactors driving predictions. This mismatch of plausibility and faithfulness\nposes technical and governance risks. We introduce Explanation-Driven\nCounterfactual Testing (EDCT), a fully automated verification procedure for a\ntarget VLM that treats the model's own explanation as a falsifiable hypothesis.\nGiven an image-question pair, EDCT: (1) obtains the model's answer and NLE, (2)\nparses the NLE into testable visual concepts, (3) generates targeted\ncounterfactual edits via generative inpainting, and (4) computes a\nCounterfactual Consistency Score (CCS) using LLM-assisted analysis of changes\nin both answers and explanations. Across 120 curated OK-VQA examples and\nmultiple VLMs, EDCT uncovers substantial faithfulness gaps and provides\nregulator-aligned audit artifacts indicating when cited concepts fail causal\ntests.", "AI": {"tldr": "我们提出了一个完全自动化的验证程序（EDCT），用于检测视觉语言模型生成的解释是否与模型预测的因果因素一致，结果表明存在显著的不一致，并提供了相应的审核工件。", "motivation": "视觉语言模型（VLMs）经常产生听起来可信但并不能反映预测因果因素的流畅的自然语言解释。这种可信度和一致性之间的差距带来了技术和管理上的风险。为了缓解这些问题，我们提出了 EDCT 方法。", "method": "我们提出了解释驱动的反事实测试（EDCT），这是一种完全自动化的验证程序，用于验证目标视觉语言模型（VLMs）。给定一张图片和一个问题，EDCT 会执行以下步骤：(1) 获得模型的答案和自然语言解释（NLE），(2) 将 NLE 解析成可检测的视觉概念，(3) 通过生成性图像修复生成针对性的反事实修改，(4) 使用语言模型辅助分析答案和解释的变化来计算反事实一致性分数（CCS）。", "result": "在 120 个策划的 OK-VQA 示例和多个 VLMs 上，EDCT 揭示了显著的一致性差距，并提供了监管部门对齐的审核工件，指出当引用的概念失败因果测试时的情景。", "conclusion": "EDCT 方法可以有效地检测视觉语言模型解释中的因果一致性问题，并提供监管者的审计工件，有助于理解和改善这些模型的表现。"}}
{"id": "2510.00280", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00280", "abs": "https://arxiv.org/abs/2510.00280", "authors": ["Ruochen Li", "Jun Li", "Bailiang Jian", "Kun Yuan", "Youxiang Zhu"], "title": "ReEvalMed: Rethinking Medical Report Evaluation by Aligning Metrics with Real-World Clinical Judgment", "comment": null, "summary": "Automatically generated radiology reports often receive high scores from\nexisting evaluation metrics but fail to earn clinicians' trust. This gap\nreveals fundamental flaws in how current metrics assess the quality of\ngenerated reports. We rethink the design and evaluation of these metrics and\npropose a clinically grounded Meta-Evaluation framework. We define clinically\ngrounded criteria spanning clinical alignment and key metric capabilities,\nincluding discrimination, robustness, and monotonicity. Using a fine-grained\ndataset of ground truth and rewritten report pairs annotated with error types,\nclinical significance labels, and explanations, we systematically evaluate\nexisting metrics and reveal their limitations in interpreting clinical\nsemantics, such as failing to distinguish clinically significant errors,\nover-penalizing harmless variations, and lacking consistency across error\nseverity levels. Our framework offers guidance for building more clinically\nreliable evaluation methods.", "AI": {"tldr": "本文揭示了现有自动医学报告生成质量评估方法的缺陷，并提出了一种基于临床的Meta-Evaluation框架，旨在构建更可靠、更具临床相关性的评估方法。", "motivation": "现有的自动生成医学报告的评估指标虽然能给出高分，但却无法赢得医生的信任。这种差距暴露了当前评估方法在判断生成报告质量时存在的根本问题。", "method": "本文提出了一个基于临床的Meta-Evaluation框架，用于重新思考和评估当前自动医学报告生成质量的评估指标。该框架以临床一致性及关键指标能力等临床标准为依据，旨在提高生成报告的临床可靠性。", "result": "研究使用了一个细化的数据集，该数据集中包含了真实的医学报告和改写后的报告对，并标注了错误类型、临床重要性标签和解释。通过系统评估现有指标，揭示了它们在解释临床语义方面的局限性，如无法辨别临床重要的错误、过度惩罚无害的变化以及严重性不同错误的一致性差等问题。", "conclusion": "本文通过一个基于临床的Meta-Evaluation框架，明确了改进现有自动生成医学报告的评估方法的方向。"}}
{"id": "2510.00054", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00054", "abs": "https://arxiv.org/abs/2510.00054", "authors": ["Xianjie Liu", "Yiman Hu", "Yixiong Zou", "Liang Wu", "Jian Xu", "Bo Zheng"], "title": "HiDe: Rethinking The Zoom-IN method in High Resolution MLLMs via Hierarchical Decoupling", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have made significant strides in\nvisual understanding tasks. However, their performance on high-resolution\nimages remains suboptimal. While existing approaches often attribute this\nlimitation to perceptual constraints and argue that MLLMs struggle to recognize\nsmall objects, leading them to use \"zoom in\" strategies for better detail, our\nanalysis reveals a different cause: the main issue is not object size, but\nrather caused by complex background interference. We systematically analyze\nthis \"zoom in\" operation through a series of decoupling experiments and propose\nthe Hierarchical Decoupling Framework (HiDe), a training-free framework that\nuses Token-wise Attention Decoupling (TAD) to decouple the question tokens and\nidentify the key information tokens, then leverages their attention weights to\nachieve precise alignment with the target visual regions. Subsequently, it\nemploys Layout-Preserving Decoupling (LPD) to decouple these regions from the\nbackground and reconstructs a compact representation that preserves essential\nspatial layouts while eliminating background interference. HiDe sets a new SOTA\non V*Bench, HRBench4K, and HRBench8K, boosting Qwen2.5-VL 7B and InternVL3 8B\nto SOTA (92.1% and 91.6% on V*Bench), even surpassing RL methods. After\noptimization, HiDe uses 75% less memory than the previous training-free\napproach. Code is provided in https://github.com/Tennine2077/HiDe.", "AI": {"tldr": "提出了HiDe框架，通过Token-wise Attention Decoupling和Layout-Preserving Decoupling技术解决MLLMs在高分辨率图像上的性能瓶颈问题，并在多个基准测试中取得了最优结果。", "motivation": "虽然MLLMs在视觉理解任务上取得了显著进展，但在高分辨率图像上表现不佳，传统的'放大'策略未能从根本上解决问题。研究提出一个新的问题视角，并设计了解决方案。", "method": "HiDe框架利用Token-wise Attention Decoupling来识别关键信息并精确对齐目标视觉区域，使用Layout-Preserving Decoupling从背景中分离这些区域，重建一个紧凑的、保留必要空间布局的图像表示。", "result": "HiDe在V*Bench, HRBench4K和HRBench8K上达到了当前的最佳性能，并优化了内存使用。", "conclusion": "HiDe框架利用一种无训练的方法成功解决了MLLMs在处理高分辨率图像时的关键问题，并且在性能和资源利用率上表现优异。"}}
{"id": "2510.00288", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00288", "abs": "https://arxiv.org/abs/2510.00288", "authors": ["Ľuboš Kriš", "Jaroslav Kopčan", "Qiwei Peng", "Andrej Ridzik", "Marcel Veselý", "Martin Tamajka"], "title": "o-MEGA: Optimized Methods for Explanation Generation and Analysis", "comment": null, "summary": "The proliferation of transformer-based language models has revolutionized NLP\ndomain while simultaneously introduced significant challenges regarding model\ntransparency and trustworthiness. The complexity of achieving explainable\nsystems in this domain is evidenced by the extensive array of explanation\nmethods and evaluation metrics developed by researchers. To address the\nchallenge of selecting optimal explainability approaches, we present\n\\textbf{\\texttt{o-mega}}, a hyperparameter optimization tool designed to\nautomatically identify the most effective explainable AI methods and their\nconfigurations within the semantic matching domain. We evaluate o-mega on a\npost-claim matching pipeline using a curated dataset of social media posts\npaired with refuting claims. Our tool systematically explores different\nexplainable methods and their hyperparameters, demonstrating improved\ntransparency in automated fact-checking systems. As a result, such automated\noptimization of explanation methods can significantly enhance the\ninterpretability of claim-matching models in critical applications such as\nmisinformation detection, contributing to more trustworthy and transparent AI\nsystems.", "AI": {"tldr": "本文提出了o-mega工具，优化可解释AI方法，提高事实核查系统在语义匹配领域的透明度和可信度。", "motivation": "鉴于Transformer语言模型在NLP领域的广泛应用带来了关于模型透明度和可信度的重大挑战，本文旨在解决选择最优解释方法的难题。", "method": "本文介绍了一种名为o-mega的超参数优化工具，该工具旨在自动识别在语义匹配领域中最有效的可解释AI方法及其配置。", "result": "通过使用精心挑选的社交媒体帖子与反驳声明配对的数据集进行评估，该工具展示了在自动化事实核查系统的透明度方面的改善。", "conclusion": "o-mega能显著提高错误信息检测等关键应用中声明匹配模型的可解释性，从而为更可信和透明的AI系统做出贡献。"}}
{"id": "2510.00059", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00059", "abs": "https://arxiv.org/abs/2510.00059", "authors": ["Jiahao Fu", "Yinfeng Yu", "Liejun Wang"], "title": "FSDENet: A Frequency and Spatial Domains based Detail Enhancement Network for Remote Sensing Semantic Segmentation", "comment": "Accepted for publication by IEEE Journal of Selected Topics in\n  Applied Earth Observations and Remote Sensing", "summary": "To fully leverage spatial information for remote sensing image segmentation\nand address semantic edge ambiguities caused by grayscale variations (e.g.,\nshadows and low-contrast regions), we propose the Frequency and Spatial Domains\nbased Detail Enhancement Network (FSDENet). Our framework employs spatial\nprocessing methods to extract rich multi-scale spatial features and\nfine-grained semantic details. By effectively integrating global and\nfrequency-domain information through the Fast Fourier Transform (FFT) in global\nmappings, the model's capability to discern global representations under\ngrayscale variations is significantly strengthened. Additionally, we utilize\nHaar wavelet transform to decompose features into high- and low-frequency\ncomponents, leveraging their distinct sensitivity to edge information to refine\nboundary segmentation. The model achieves dual-domain synergy by integrating\nspatial granularity with frequency-domain edge sensitivity, substantially\nimproving segmentation accuracy in boundary regions and grayscale transition\nzones. Comprehensive experimental results demonstrate that FSDENet achieves\nstate-of-the-art (SOTA) performance on four widely adopted datasets: LoveDA,\nVaihingen, Potsdam, and iSAID.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.00311", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00311", "abs": "https://arxiv.org/abs/2510.00311", "authors": ["Bowen Wei", "Yuan Shen Tay", "Howard Liu", "Jinhao Pan", "Kun Luo", "Ziwei Zhu", "Chris Jordan"], "title": "CORTEX: Collaborative LLM Agents for High-Stakes Alert Triage", "comment": null, "summary": "Security Operations Centers (SOCs) are overwhelmed by tens of thousands of\ndaily alerts, with only a small fraction corresponding to genuine attacks. This\noverload creates alert fatigue, leading to overlooked threats and analyst\nburnout. Classical detection pipelines are brittle and context-poor, while\nrecent LLM-based approaches typically rely on a single model to interpret logs,\nretrieve context, and adjudicate alerts end-to-end -- an approach that\nstruggles with noisy enterprise data and offers limited transparency. We\npropose CORTEX, a multi-agent LLM architecture for high-stakes alert triage in\nwhich specialized agents collaborate over real evidence: a behavior-analysis\nagent inspects activity sequences, evidence-gathering agents query external\nsystems, and a reasoning agent synthesizes findings into an auditable decision.\nTo support training and evaluation, we release a dataset of fine-grained SOC\ninvestigations from production environments, capturing step-by-step analyst\nactions and linked tool outputs. Across diverse enterprise scenarios, CORTEX\nsubstantially reduces false positives and improves investigation quality over\nstate-of-the-art single-agent LLMs.", "AI": {"tldr": "我们提出CORTEX，一个多智能体LLM架构，用于处理企业环境下的高风险警报分类，减少假阳性，并提高了调查质量。", "motivation": "传统的检测管道是脆弱且缺乏上下文的，而最近基于LLM的方法通常依赖单一模型来解释日志、检索上下文以及最终决定告警，这种方法对于嘈杂的企业数据难以处理，并且透明度有限。", "method": "我们提出了一种名为CORTEX的多智能体LLM架构，用于高风险的警报分类。该架构包含专门的智能体，它们能协同工作处理实际证据：行为分析智能体检查活动序列，证据收集智能体查询外部系统，推理智能体将发现综合为可审计的决策。", "result": "在不同的企业场景中，CORTEX显著减少了假阳性和提高了调查质量，优于最先进的单一智能体LLM方法。", "conclusion": "CORTEX在处理企业环境中高风险警报分类方面表现出色，降低了假阳性警报的数量，并提高了调查质量。"}}
