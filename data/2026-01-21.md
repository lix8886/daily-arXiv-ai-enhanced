<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Context Discipline and Performance Correlation: Analyzing LLM Performance and Quality Degradation Under Varying Context Lengths](https://arxiv.org/abs/2601.11564)
*Ahilan Ayyachamy Nadar Ponnusamy,Karthic Chandran,M Maruf Hossain*

Main category: cs.CL

> 研究探讨了在大型语言模型中增加上下文窗口对系统性能和模型质量的影响，揭示了密钥-值缓存增长导致的非线性性能下降，并指出了混合专家架构在高令牌量下可能存在的问题。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型上下文窗口的扩大，需要探索这种扩展对系统性能和模型质量的权衡。研究背景在于处理额外上下文所带来的计算成本。

**Method:** 研究调查了在大规模无关和分散的上下文环境中，使用密集变压器架构（如 Llama-3.1-70B 和 Qwen1.5-14B）时的系统性能与模型质量之间的权衡。

**Result:** 研究表明，随着密钥-值缓存的增大，系统性能出现非线性下降。在较高令牌量下，混合专家架构表现出了独特的行为异常，这可能导致基础设施瓶颈。

**Conclusion:** 研究提出，密钥-值缓存的增长对系统性能有负面影响，而混合专家架构在处理大规模上下文时面临独特挑战，暗示可能存在的基础设施问题。这种方法下，架构优势可能被高令牌量的基础设施瓶颈所掩盖。

**Abstract:** The scaling trend in Large Language Models (LLMs) has prioritized increasing the maximum context window to facilitate complex, long-form reasoning and document analysis. However, managing this expanded context introduces severe computational overhead. This paper investigates the critical trade-off between system performance and model quality when dense transformer architectures--specifically Llama-3.1-70B and Qwen1.5-14B--are exposed to large volumes of irrelevant and distracting context. The research identifies a non-linear performance degradation tied to the growth of the Key-Value (KV) cache. Furthermore, an extended analysis of the Mixture-of-Experts (MoE) architecture reveals unique behavioral anomalies at varying context scales, suggesting that architectural benefits may be masked by infrastructure bottlenecks at high token volumes.

</details>


### [2] [Compass-Embedding v4: Robust Contrastive Learning for Multilingual E-commerce Embeddings](https://arxiv.org/abs/2601.11565)
*Pakorn Ueareeworakul,Shuman Liu,Jinghao Feng,Ling Hu,Zhantang Shi,Chengqi Sun,Liang Yao,Panyi Ouyang,Haibo Zhang,Anxiang Zeng*

Main category: cs.CL

> 本文介绍的Compass-Embedding v4多语言模型专门针对东南亚电商问题进行优化，通过引入CAM、多样化训练语料库和优化生产部署技术，解决了数据稀缺等挑战，并在多个基准测试任务中表现优秀。

<details>
  <summary>Details</summary>

**Motivation:** 随着全球电子商务迅速扩展到新兴市场，而低资源语言缺乏高质量的语义表征成为了检索、推荐和搜索系统的关键瓶颈。为了应对这些挑战，本文提出了Compass-Embedding v4来解决数据稀缺、监督噪声以及严格的生产约束对表征学习造成的难题。

**Method:** Compass-Embedding v4 是一个专门针对东南亚电商场景的高度高效的多语言嵌入框架。它通过Class-Aware Masking (CAM)减轻了系统性伪负样本的影响；通过多样化训练语料库的构建解决了资源匮乏和数据分布不均的问题；通过大批次训练和球形模型合并技术优化了生产部署时的推理速度和嵌入质量。

**Result:** 研究结果表明，在多语言基准测试和专有电子商务任务中，Compass-Embedding v4在东南亚主要语言上达到了最新的性能标准。它在领域特定的检索和分类任务中显著超越了通用嵌入模型，同时在资源丰富的语言上也保持了竞争力。

**Conclusion:** Compass-Embedding v4 提供了一种在资源受限的东南亚语言电商环境下，提升检索、推荐和搜索系统性能的解决方案。它不仅提高了语义对齐的准确性，还优化了生产部署时的推理速度和嵌入质量。

**Abstract:** As global e-commerce rapidly expands into emerging markets, the lack of high-quality semantic representations for low-resource languages has become a decisive bottleneck for retrieval, recommendation, and search systems. In this work, we present Compass-Embedding v4, a high-efficiency multilingual embedding framework specifically optimized for Southeast Asian (SEA) e-commerce scenarios, where data scarcity, noisy supervision, and strict production constraints jointly challenge representation learning. Compass-Embedding v4 addresses three core challenges. First, large-batch contrastive training under mixed task supervision introduces systematic false negatives that degrade semantic alignment. We propose Class-Aware Masking (CAM), a lightweight modification to the InfoNCE objective that suppresses invalid in-batch negatives and improves semantic discrimination without altering training efficiency. Second, low-resource SEA languages suffer from limited and uneven data coverage. We construct a diversified training corpus through context-grounded synthetic data generation, cross-lingual translation, and structured e-commerce data construction, enabling robust multilingual and domain-specific learning. Third, production deployment requires high-throughput inference while preserving embedding quality. We combine robustness-driven large-batch training with spherical model merging to mitigate catastrophic forgetting, and optimize inference via vLLM and FP8 quantization. Extensive evaluations across multilingual benchmarks and proprietary e-commerce tasks show that Compass-Embedding v4 achieves state-of-the-art performance on major SEA languages, significantly outperforming general-purpose embedding models in domain-specific retrieval and classification, while maintaining competitive performance on high-resource languages.

</details>


### [3] [Measuring Stability Beyond Accuracy in Small Open-Source Medical Large Language Models for Pediatric Endocrinology](https://arxiv.org/abs/2601.11567)
*Vanessa D'Amario,Randy Daniel,Alessandro Zanetti,Dhruv Edamadaka,Nitya Alaparthy,Joshua Tarkoff*

Main category: cs.CL

> 研究评估了六种小型开源医疗大模型在儿科内分泌学领域的性能，发现一致性高的模型不一定正确，并且模型输出受提示变化的影响较大，强调了需要更广泛的诊断框架来理解潜在的临床决策支持问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有的小型开源医疗大模型在评估时主要关注在医学选择题上的准确率，缺乏对一致性、鲁棒性和推理行为等方面的评估。因此，本研究旨在通过人工评估和临床回顾来全面评估这些模型的性能。

**Method:** 本研究通过结合医学选择题和人类评估及临床回顾，评估了六款小型开源医疗大模型在儿科内分泌学领域的表现，包括确定性条件下的提示变化对输出和自我评估偏差的影响以及随机条件下的输出变异性和其与正确性的关系。

**Result:** 结果表明，HuatuoGPT-o1-8B在性能上表现最优，但模型输出的一致性并不能作为正确性的指标。同时，这些模型在推理选择时存在自我评估偏差。

**Conclusion:** 本研究强调了小规模提示变化可能导致不同的输出结果，这可能会影响LLM评估结果的可重复性，并指出了在实际临床决策支持场景中模型输出变异性的潜在问题，强调需要一个更广泛的诊断框架来理解潜在的陷阱。

**Abstract:** Small open-source medical large language models (LLMs) offer promising opportunities for low-resource deployment and broader accessibility. However, their evaluation is often limited to accuracy on medical multiple choice question (MCQ) benchmarks, and lacks evaluation of consistency, robustness, or reasoning behavior. We use MCQ coupled to human evaluation and clinical review to assess six small open-source medical LLMs (HuatuoGPT-o1 (Chen 2024), Diabetica-7B, Diabetica-o1 (Wei 2024), Meditron3-8B (Sallinen2025), MedFound-7B (Liu 2025), and ClinicaGPT-base-zh (Wang 2023)) in pediatric endocrinology. In deterministic settings, we examine the effect of prompt variation on models' output and self-assessment bias. In stochastic settings, we evaluate output variability and investigate the relationship between consistency and correctness. HuatuoGPT-o1-8B achieved the highest performance. The results show that high consistency across the model response is not an indicator of correctness, although HuatuoGPT-o1-8B showed the highest consistency rate. When tasked with selecting correct reasoning, both HuatuoGPT-o1-8B and Diabetica-o1 exhibit self-assessment bias and dependency on the order of the candidate explanations. Expert review of incorrect reasoning rationales identified a mix of clinically acceptable responses and clinical oversight. We further show that system-level perturbations, such as differences in CUDA builds, can yield statistically significant shifts in model output despite stable accuracy. This work demonstrates that small, semantically negligible prompt perturbations lead to divergent outputs, raising concerns about reproducibility of LLM-based evaluations and highlights the output variability under different stochastic regimes, emphasizing the need of a broader diagnostic framework to understand potential pitfalls in real-world clinical decision support scenarios.

</details>


### [4] [An Empirical Analysis of Fine-Tuning Large Language Models on Bioinformatics Literature: PRSGPT and BioStarsGPT](https://arxiv.org/abs/2601.11573)
*Muhammad Muneeb,David B. Ascher*

Main category: cs.CL

> 本文提出了一种可重现的流水线，用于专业生物信息学数据对大型语言模型（LLMs）进行微调。该流水线被应用于PRSGPT和BioStarsGPT两个案例中，分别专注于多基因风险评分（PRS）工具和社区论坛讨论。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLMs）通常缺乏针对复杂生物信息学应用的专业知识。为了改善这一状况，我们提出了一种流水线方法，旨在对LLMs进行专业微调，使其在生物信息学领域内更加有用。

**Method:** 我们提出了一种可重现的流水线，用于对大型语言模型（LLMs）进行专业生物信息学数据的微调。这一流水线包含九个步骤，包括多种数据源的整合、结构化的预处理、基于提示的问题-回答生成（通过Google Gemini）、自然语言推理（NLI）质量控制、语义去重、基于聚类的数据分割，以及使用LoRA进行参数高效的微调。

**Result:** 我们对三个预训练的LLMs（LLaMA-3.2-3B, Qwen2.5-7B, Gemma）进行了微调，并在超过14个词汇和语义度量上进行了基准测试。Qwen2.5-7B 在PRSGPT上表现出82%和70%的BLEU-4和ROUGE-1提升，在BioStarsGPT上则分别为6%和18%。

**Conclusion:** 我们的流水线使LLMs在生物信息学领域的应用更加完善，促进了可扩展、特定领域的微调模型的发展。这一方案也为隐私保护和本地部署生物信息学助手提供了可能性，并探讨了实际应用中面临的问题和解决方案。

**Abstract:** Large language models (LLMs) often lack specialized knowledge for complex bioinformatics applications. We present a reproducible pipeline for fine-tuning LLMs on specialized bioinformatics data, demonstrated through two use cases: PRSGPT, focused on polygenic risk score (PRS) tools, and BioStarsGPT, trained on community forum discussions. The nine-step pipeline integrates diverse data sources, structured preprocessing, prompt-based question-answer (QA) generation (via Google Gemini), natural language inference (NLI) for quality control, semantic deduplication, clustering-based data splitting, and parameter-efficient fine-tuning using LoRA. We fine-tuned three LLMs (LLaMA-3.2-3B, Qwen2.5-7B, Gemma) and benchmarked them on over 14 lexical and semantic metrics. Qwen2.5-7B emerged as the best performer, with BLEU-4 and ROUGE-1 improvements of 82\% and 70\% for PRSGPT and 6\% and 18\% for BioStarsGPT, respectively. The open-source datasets produced include over 28,000 QA pairs for PRSGPT and 154,282 for BioStarsGPT. Human evaluation of PRSGPT yielded 61.9\% accuracy on the PRS tools comparison task, comparable to Google Gemini (61.4\%), but with richer methodological detail and accurate citations. BioStarsGPT demonstrated 59\% conceptual accuracy across 142 curated bioinformatics questions. Our pipeline enables scalable, domain-specific fine-tuning of LLMs. It enables privacy-preserving, locally deployable bioinformatics assistants, explores their practical applications, and addresses the challenges, limitations, and mitigation strategies associated with their development and use.

</details>


### [5] [Concept Attractors in LLMs and their Applications](https://arxiv.org/abs/2601.11575)
*Sotirios Panagiotis Chytas,Vikas Singh*

Main category: cs.CL

> The paper explores how LLMs create similar internal representations for semantically related prompts and leverages this through Attractors to develop simple, efficient methods for solving diverse language tasks without needing fine-tuning.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this approach is to offer an efficient alternative to the heavy fine-tuning usually required for specific tasks in LLMs, and to provide methods that can generalize well even in cases where task-specific baselines do not perform adequately.

**Method:** We utilize Iterated Function Systems (IFS) to explain the behavior of large language models (LLMs) that map semantically related prompts to similar internal representations. Based on this insight, we develop simple, training-free methods that act on concept-specific Attractors to address various practical tasks.

**Result:** The Attractor-based interventions developed by the authors match or outperform specialized baselines across different tasks such as language translation, hallucination reduction, guardrailing, and synthetic data generation, highlighting the effectiveness and efficiency of the proposed approach.

**Conclusion:** This research indicates that using the concept of Attractors derived from IFS can lead to simple, effective methods for intervening in LLMs to accomplish various tasks without the need for extensive training, thus representing a promising avenue for future research.

**Abstract:** Large language models (LLMs) often map semantically related prompts to similar internal representations at specific layers, even when their surface forms differ widely. We show that this behavior can be explained through Iterated Function Systems (IFS), where layers act as contractive mappings toward concept-specific Attractors. We leverage this insight and develop simple, training-free methods that operate directly on these Attractors to solve a wide range of practical tasks, including language translation, hallucination reduction, guardrailing, and synthetic data generation. Despite their simplicity, these Attractor-based interventions match or exceed specialized baselines, offering an efficient alternative to heavy fine-tuning, generalizable in scenarios where baselines underperform.

</details>


### [6] [LimAgents: Multi-Agent LLMs for Generating Research Limitations](https://arxiv.org/abs/2601.11578)
*Ibrahim Al Azher,Zhishuai Guo,Hamed Alhoori*

Main category: cs.CL

> LimAgents 是一个改进现有零样本大型语言模型生成实质性科研局限陈述的多智能体语言模型框架。

<details>
  <summary>Details</summary>

**Motivation:** 为了更透明、严谨的科学研究，识别和阐述限制是必要的。然而，零样本大语言模型通常会产生表面或泛泛的限制声明，重复作者报告的限制，而不是深入到方法论问题和情境差距。由于许多作者只披露部分或琐碎的限制，这个问题变得更加严重。

**Method:** 提出 LimAgents，这是一个多智能体语言模型框架，用于生成实质性限制。LimAgents 结合了 OpenReview 评论和作者报告的限制，以提供更强的基准。使用引用和引用该论文的文章来捕捉更广泛的情境弱点。在该框架中，不同的智能体具有特定的角色，按顺序执行任务：一些智能体提取明确的限制，一些分析方法论差距，一些模拟同行评审观点，而引用智能体则将工作置于更广泛文献中。裁判智能体会完善它们的输出，主控智能体则将它们整合成一个明确的集合。

**Result:** 实验显示 LimAgents 显著提高了性能。RAG + 多智能体 GPT-4o mini 配置实现了比零样本基线高出 15.51% 的覆盖增益，而 Llama 3 8B 多智能体设置则实现了 4.41% 的改进。

**Conclusion:** LimAgents 被证明可以系统地识别显式、隐式、同伴评审和文献信息的限制，从而比零样本基线表现更好。

**Abstract:** Identifying and articulating limitations is essential for transparent and rigorous scientific research. However, zero-shot large language models (LLMs) approach often produce superficial or general limitation statements (e.g., dataset bias or generalizability). They usually repeat limitations reported by authors without looking at deeper methodological issues and contextual gaps. This problem is made worse because many authors disclose only partial or trivial limitations. We propose LimAgents, a multi-agent LLM framework for generating substantive limitations. LimAgents integrates OpenReview comments and author-stated limitations to provide stronger ground truth. It also uses cited and citing papers to capture broader contextual weaknesses. In this setup, different agents have specific roles as sequential role: some extract explicit limitations, others analyze methodological gaps, some simulate the viewpoint of a peer reviewer, and a citation agent places the work within the larger body of literature. A Judge agent refines their outputs, and a Master agent consolidates them into a clear set. This structure allows for systematic identification of explicit, implicit, peer review-focused, and literature-informed limitations. Moreover, traditional NLP metrics like BLEU, ROUGE, and cosine similarity rely heavily on n-gram or embedding overlap. They often overlook semantically similar limitations. To address this, we introduce a pointwise evaluation protocol that uses an LLM-as-a-Judge to measure coverage more accurately. Experiments show that LimAgents substantially improve performance. The RAG + multi-agent GPT-4o mini configuration achieves a +15.51% coverage gain over zero-shot baselines, while the Llama 3 8B multi-agent setup yields a +4.41% improvement.

</details>


### [7] [Bielik 11B v3: Multilingual Large Language Model for European Languages](https://arxiv.org/abs/2601.11579)
*Krzysztof Ociepa,Łukasz Flis,Remigiusz Kinas,Krzysztof Wróbel,Adrian Gwoździej*

Main category: cs.CL

> 本文介绍了Bielik 11B v3，一个专为波兰语优化的语言模型，它扩展了Mistral 7B v0.2架构，在四个阶段的训练后，展示了卓越的语言处理能力。

<details>
  <summary>Details</summary>

**Motivation:** 开发一个专门为波兰语优化的语言模型Bielik 11B v3，同时保持在其他欧洲语言上的强大能力。

**Method:** 该论文扩展了Mistral 7B v0.2架构，通过增加深度将参数量扩展到110亿。训练过程分为四个阶段：连续预训练、监督微调（SFT）、直接偏好优化（DPO）和强化学习。

**Result:** 实验结果表明，Bielik 11B v3在广泛的任务上表现优异，包括基础语言理解和复杂推理，超越了专门针对波兰语的模型和其他更大的模型。

**Conclusion:** Bielik 11B v3不仅推进了波兰语的AI能力，而且为开发资源高效的、高性能的少用语言模型设定了新的基准。

**Abstract:** We present Bielik 11B v3, a state-of-the-art language model highly optimized for the Polish language, while also maintaining strong capabilities in other European languages. This model extends the Mistral 7B v0.2 architecture, scaled to 11B parameters via depth up-scaling. Its development involved a comprehensive four-stage training pipeline: continuous pre-training, supervised fine-tuning (SFT), Direct Preference Optimization (DPO), and reinforcement learning.
  Comprehensive evaluations demonstrate that Bielik 11B v3 achieves exceptional performance. It significantly surpasses other specialized Polish language models and outperforms many larger models (with 2-6 times more parameters) on a wide range of tasks, from basic linguistic understanding to complex reasoning.
  The model's parameter efficiency, combined with extensive quantization options, allows for effective deployment across diverse hardware configurations. Bielik 11B v3 not only advances AI capabilities for the Polish language but also establishes a new benchmark for developing resource-efficient, high-performance models for less-represented languages.

</details>


### [8] [Speculative Decoding: Performance or Illusion?](https://arxiv.org/abs/2601.11580)
*Xiaoxuan Liu,Jiaxiang Yu,Jongseok Park,Ion Stoica,Alvin Cheung*

Main category: cs.CL

> 本研究深入探讨了在生产级推理引擎vLLM上采用各种变体的推测性解码技术的性能表现和理论速度提升上限，指出目标模型的验证步骤占据主要执行时间，且接受长度在不同输出位置、请求和数据集间差异显著。

<details>
  <summary>Details</summary>

**Motivation:** 现有的推测性解码技术评价大多依赖于研究原型和极小的批次大小，导致其实用效能的不确定性。为了填补这一研究空白，本研究进行了大规模、系统化的推测性解码技术评估。

**Method:** 通过在生产级别的推理引擎vLLM上，系统地测试多种推测性解码技术，包括n-gram、EAGLE、EAGLE-3、Draft-Model、多重令牌预测等方法，评估它们在不同工作负载、模型规模和批量大小条件下的效果。

**Result:** 结果表明，目标模型的验证是执行的主要部分；输出令牌位置、请求和数据集之间接受长度存在显著差异。观察到的成绩与理论上限之间存在较大差异。

**Conclusion:** 研究揭示了实际上观察到的速度提升远低于理论上限，这为未来在提升推测性解码性能方面提供了新的研究机遇。

**Abstract:** Speculative decoding (SD) has become a popular technique to accelerate Large Language Model (LLM) inference, yet its real-world effectiveness remains unclear as prior evaluations rely on research prototypes and unrealistically small batch sizes. We present, to our knowledge, the first systematic study of SD on a production-grade and widely deployed inference engine (vLLM), covering multiple SD variants ($n$-gram, EAGLE/EAGLE-3, Draft-Model, Multi-Token Prediction) across diverse workloads, model scales, and batch sizes. We analyze key factors governing SD performance, and quantify a theoretical upper bound on SD speedup. Our results show that verification by the target model dominates the execution, while acceptance length varies markedly across output token positions, requests, and datasets. Comparing measured performance with theoretical bounds reveals substantial gaps between observed and theoretical upper bounds, and we leverage this observation to highlight new research opportunities that our study opens up in improving SD.

</details>


### [9] [Enhancing the QA Model through a Multi-domain Debiasing Framework](https://arxiv.org/abs/2601.11581)
*Yuefeng Wang,ChangJae Lee*

Main category: cs.CL

> 本研究针对ELECTRA-small问答模型在SQuAD v1.1和对抗数据集上的表现进行了评估，并提出了一个包含知识蒸馏、去偏技术和领域扩展的多领域去偏框架，证明了这些策略能提高模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 问答模型在阅读理解方面取得了显著进展，但在处理复杂查询和对抗条件下表现不佳，存在语言偏差、数值推理和实体识别方面的错误。本研究旨在通过开发去偏框架来提高模型在这些条件下的表现。

**Method:** 通过识别ELECTRA-small模型在SQuAD v1.1和对抗数据集AddSent和AddOneSent中的错误，发展了一个包含知识蒸馏、去偏技术和领域扩展的多领域去偏框架。

**Result:** 研究结果显示，不同测试集上的精确匹配(EM)和F1分数最高提高了2.6个百分点，特别是在对抗性环境中。

**Conclusion:** 针对偏见的缓解策略可以提高自然语言理解系统的鲁棒性和可靠性。

**Abstract:** Question-answering (QA) models have advanced significantly in machine reading comprehension but often exhibit biases that hinder their performance, particularly with complex queries in adversarial conditions. This study evaluates the ELECTRA-small model on the Stanford Question Answering Dataset (SQuAD) v1.1 and adversarial datasets AddSent and AddOneSent. By identifying errors related to lexical bias, numerical reasoning, and entity recognition, we develop a multi-domain debiasing framework incorporating knowledge distillation, debiasing techniques, and domain expansion. Our results demonstrate up to 2.6 percentage point improvements in Exact Match (EM) and F1 scores across all test sets, with gains in adversarial contexts. These findings highlight the potential of targeted bias mitigation strategies to enhance the robustness and reliability of natural language understanding systems.

</details>


### [10] [Entropic Context Shaping: Information-Theoretic Filtering for Context-Aware LLM Agents](https://arxiv.org/abs/2601.11585)
*Hyunjun Kim*

Main category: cs.CL

> 本文引入ECS框架以提高LLM代理的上下文实用性筛选能力，相比TF-IDF方法在细粒度轮次选择上有了显著提升。

<details>
  <summary>Details</summary>

**Motivation:** 在大型语言模型（LLM）代理的上下文工程中，需要区分实用信息和误导的干扰项。为了提高模型在多轮对话中的上下文选择能力，本文提出了ECS框架。

**Method:** 本文提出了一个叫做熵上下文塑造（Entropic Context Shaping, ECS）的信息理论框架，用于衡量模型上下文的实用性。ECS通过模型回答分布向正确答案的转变来测量上下文的实用性。与依赖词汇重叠的方法不同，ECS捕捉到了实用效用，即一个段落是否实际上有助于回答问题。

**Result:** 实验表明，使用ECS的Llama-3.1-8B模型在细粒度轮次选择上达到F1=0.265的得分，相比TF-IDF方法有71.83%的相对提升。

**Conclusion:** 实验结果表明，ECS方法相比TF-IDF在细粒度轮次选择上具有显著优势，特别是在需要精准上下文选择的任务上。

**Abstract:** Context engineering for large language model (LLM) agents requires distinguishing pragmatically useful information from misleading distractors. We introduce Entropic Context Shaping (ECS), an information-theoretic framework that measures context utility via the shift in the model's answer distribution toward the correct answer. Unlike lexical similarity methods that rely on word overlap, ECS captures pragmatic utility -- whether a passage actually helps answer the question. We formalize utility as the signed change in answer probability and provide theoretical analysis showing that task-irrelevant updates yield near-zero distribution shift. We evaluate on multi-turn context selection tasks using LongMemEval (session-level) and LoCoMo (turn-level) benchmarks. On fine-grained turn selection, ECS with Llama-3.1-8B achieves F1=0.265, a 71.83% relative improvement over TF-IDF (F1=0.154), demonstrating that pragmatic utility outperforms lexical similarity when precise context selection matters. Code and data are available in the supplementary materials.

</details>
