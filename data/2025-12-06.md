<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 12]
- [cs.CV](#cs.CV) [Total: 15]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral](https://arxiv.org/abs/2512.04220)
*Wenlong Deng,Yushu Li,Boying Gong,Yi Ren,Christos Thrampoulidis,Xiaoxiao Li*

Main category: cs.CL

> 该论文解决了工具集成强化学习中训练崩溃的问题，提出了LLDS正则化方法，可以稳定训练并提升性能。

<details>
  <summary>Details</summary>

**Motivation:** 解决在工具集成的强化学习中，模型训练过程中出现的训练崩溃问题，以提升模型的稳定性和性能。

**Method:** 提出了一个新的正则化方法LLDS（Lazy Likelihood Displacement Stabilization），当轨迹的似然度降低时激活，只对受影响的令牌进行正则化。

**Result:** LLDS方法在七个公开领域和多跳QA基准测试中稳定了训练，防止了梯度爆炸，并在Qwen2.5-3B和Qwen2.5-7B模型上获得了显著的性能提升（分别提高37.8%和32.0%）。

**Conclusion:** LLD被认为是GRPO（Group Relative Policy Optimization）在工具集成的强化学习中训练失败的根本原因，而LLDS提供了一种方法来稳定训练并提升性能。

**Abstract:** Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.

</details>


### [2] [Computational Linguistics Meets Libyan Dialect: A Study on Dialect Identification](https://arxiv.org/abs/2512.04257)
*Mansour Essgaer,Khamis Massud,Rabia Al Mamlook,Najah Ghmaid*

Main category: cs.CL

> 研究使用逻辑回归、线性支持向量机、多项式朴素贝叶斯和伯努利朴素贝叶斯分类器对来自Twitter的利比亚方言语句进行分类。实验表明，多项式朴素贝叶斯使用(1,2)词元n-gram和(1,5)字符n-gram表示法达到了最高准确率85.89%和F1-score0.85741。其他模型在准确率方面略低。研究表明精心选择的n-gram表示法和模型对于提高利比亚方言识别准确度至关重要。

<details>
  <summary>Details</summary>

**Motivation:** 为了识别利比亚方言，本文旨在通过不同的分类器和n-gram表示方法来测试其在阿拉伯语方言分类任务中的表现。

**Method:** 采用QADI语料库的540,000个句子，使用logistic regression, linear SVM, multinomial Naive Bayes, and Bernoulli Naive Bayes分类器进行实验。通过chi-square分析进行了特征选择，并使用不同词元和字符n-gram表示法进行了分类实验。

**Result:** 实验结果表明，多项式朴素贝叶斯分类器在(1,2)词元n-gram和(1,5)字符n-gram表示法下达到了最高准确率85.89%和F1-score0.85741，相比之下，逻辑回归和线性SVM的准确率分别为84.41%和84.73%。其他计算指标，包括log loss，Cohen kappa和Matthews相关系数进一步支持了多项式朴素贝叶斯的有效性。

**Conclusion:** 精心选择的n-gram表示法和分类模型对于提高利比亚方言识别的准确性具有重要作用。研究提供了未来阿拉伯语方言NLP应用研究的理论基础和见解。

**Abstract:** This study investigates logistic regression, linear support vector machine, multinomial Naive Bayes, and Bernoulli Naive Bayes for classifying Libyan dialect utterances gathered from Twitter. The dataset used is the QADI corpus, which consists of 540,000 sentences across 18 Arabic dialects. Preprocessing challenges include handling inconsistent orthographic variations and non-standard spellings typical of the Libyan dialect. The chi-square analysis revealed that certain features, such as email mentions and emotion indicators, were not significantly associated with dialect classification and were thus excluded from further analysis. Two main experiments were conducted: (1) evaluating the significance of meta-features extracted from the corpus using the chi-square test and (2) assessing classifier performance using different word and character n-gram representations. The classification experiments showed that Multinomial Naive Bayes (MNB) achieved the highest accuracy of 85.89% and an F1-score of 0.85741 when using a (1,2) word n-gram and (1,5) character n-gram representation. In contrast, Logistic Regression and Linear SVM exhibited slightly lower performance, with maximum accuracies of 84.41% and 84.73%, respectively. Additional evaluation metrics, including log loss, Cohen kappa, and Matthew correlation coefficient, further supported the effectiveness of MNB in this task. The results indicate that carefully selected n-gram representations and classification models play a crucial role in improving the accuracy of Libyan dialect identification. This study provides empirical benchmarks and insights for future research in Arabic dialect NLP applications.

</details>


### [3] [SQuARE: Structured Query & Adaptive Retrieval Engine For Tabular Formats](https://arxiv.org/abs/2512.04292)
*Chinmay Gondhalekar,Urjitkumar Patel,Fang-Chun Yeh*

Main category: cs.CL

> 本文介绍了一种名为SQuARE的混合检索框架，通过智能解析多行表头、单元格合并等问题，实现更准确高效的表格问答，其性能优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 目前准确的表格问答系统遇到多行表头、合并单元格和单位注释等挑战，而刚性的SQL视图在处理没有一致模式的文件时效率低下。

**Method:** 我们提出了一种名为SQuARE的混合检索框架，该框架根据表头深度和合并密度计算连续评分，并将查询通过保存结构的片段检索或SQL路由到自动构建的关联表示中。当置信度较低时，一个轻量级代理会监督检索、细化或组合来自两个路径的结果。这种设计可以维持表头层次、时间标签和单位，确保返回的值忠实于原始单元格且易于验证。

**Result:** 在多表头的企业资产负债表、高度合并的世界银行工作表和多样化的公共数据集中评估后，SQuARE在检索精度和端到端答案准确性上超越了单策略基线和ChatGPT-4o，并保持了可预测的延迟。

**Conclusion:** 该系统将检索与模型选择分离，使之与正在发展的表格基础模型兼容，并提供了一个迈向更强大表格理解的实用桥梁。

**Abstract:** Accurate question answering over real spreadsheets remains difficult due to multirow headers, merged cells, and unit annotations that disrupt naive chunking, while rigid SQL views fail on files lacking consistent schemas. We present SQuARE, a hybrid retrieval framework with sheet-level, complexity-aware routing. It computes a continuous score based on header depth and merge density, then routes queries either through structure-preserving chunk retrieval or SQL over an automatically constructed relational representation. A lightweight agent supervises retrieval, refinement, or combination of results across both paths when confidence is low. This design maintains header hierarchies, time labels, and units, ensuring that returned values are faithful to the original cells and straightforward to verify. Evaluated on multi-header corporate balance sheets, a heavily merged World Bank workbook, and diverse public datasets, SQuARE consistently surpasses single-strategy baselines and ChatGPT-4o on both retrieval precision and end-to-end answer accuracy while keeping latency predictable. By decoupling retrieval from model choice, the system is compatible with emerging tabular foundation models and offers a practical bridge toward a more robust table understanding.

</details>


### [4] [DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle](https://arxiv.org/abs/2512.04324)
*Fangyu Lei,Jinxiang Meng,Yiming Huang,Junjie Zhao,Yitong Zhang,Jianwen Luo,Xin Zou,Ruiyi Yang,Wenbo Shi,Yan Gao,Shizhu He,Zuo Wang,Qian Liu,Yang Wang,Ke Wang,Jun Zhao,Kang Liu*

Main category: cs.CL

> DAComp是一个包含210个任务的基准，用于评估自动数据代理处理复杂企业数据智能流程的能力，揭示了当前代理在处理数据工程和数据分析任务上的不足。

<details>
  <summary>Details</summary>

**Motivation:** 为真实的企业数据智能工作流程提供一个综合的评估标准，特别是针对自动化的数据代理能力的测试。

**Method:** 通过引入DAComp基准测试，包含210个任务来模拟复杂的数据智能流程，分为数据工程任务和数据分析任务。数据工程任务评估多阶段SQL管道设计和构建，数据分析任务则评估解决开放性商业问题的能力。

**Result:** 实验表明，即使是最先进的代理在DAComp上表现不佳，特别是在数据工程任务上的成功率低于20%。开放性任务得分也低于40%，突显出开放式推理的显著不足。

**Conclusion:** DAComp揭示了数据工程和数据分析之间的能力差距，并为开发用于企业环境的真正有能力的自主数据代理提供了一个严格的现实测试平台。

**Abstract:** Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io

</details>


### [5] [ClusterFusion: Hybrid Clustering with Embedding Guidance and LLM Adaptation](https://arxiv.org/abs/2512.04350)
*Yiming Xu,Yuan Yuan,Vijay Viswanathan,Graham Neubig*

Main category: cs.CL

> 提出ClusterFusion框架，结合轻量级的嵌入方法和大型语言模型，实现了文本聚类的新方法。

<details>
  <summary>Details</summary>

**Motivation:** 传统文本聚类算法在特定领域应用时需要昂贵的微调，且之前的研究主要将大型语言模型作为辅助模块来精调嵌入或调整类别边界。

**Method:** ClusterFusion框架包含三个阶段：嵌入引导的子集划分、语言模型驱动的主题总结和语言模型主导的主题分配。

**Result:** 实验结果显示，ClusterFusion不仅在标准数据集上达到最先进的性能，还在特定领域数据集上表现优越。

**Conclusion:** ClusterFusion能有效结合领域知识和用户偏好，充分利用大型语言模型的上下文适应能力。

**Abstract:** Text clustering is a fundamental task in natural language processing, yet traditional clustering algorithms with pre-trained embeddings often struggle in domain-specific contexts without costly fine-tuning. Large language models (LLMs) provide strong contextual reasoning, yet prior work mainly uses them as auxiliary modules to refine embeddings or adjust cluster boundaries. We propose ClusterFusion, a hybrid framework that instead treats the LLM as the clustering core, guided by lightweight embedding methods. The framework proceeds in three stages: embedding-guided subset partition, LLM-driven topic summarization, and LLM-based topic assignment. This design enables direct incorporation of domain knowledge and user preferences, fully leveraging the contextual adaptability of LLMs. Experiments on three public benchmarks and two new domain-specific datasets demonstrate that ClusterFusion not only achieves state-of-the-art performance on standard tasks but also delivers substantial gains in specialized domains. To support future work, we release our newly constructed dataset and results on all benchmarks.

</details>


### [6] [LangSAT: A Novel Framework Combining NLP and Reinforcement Learning for SAT Solving](https://arxiv.org/abs/2512.04374)
*Muyu Pan,Matthew Walter,Dheeraj Kodakandla,Mahfuza Farooque*

Main category: cs.CL

> LangSAT bridges the gap between natural language and SAT-solving, with Lang2Logic translating English to CNF, and SmartSAT using RL to optimize the solving process.

<details>
  <summary>Details</summary>

**Motivation:** The aim is to make SAT-solving more accessible by allowing natural language inputs instead of requiring CNF expressions, which are typically needed by existing SAT-solving platforms.

**Method:** Our work introduces LangSAT, a framework combining Lang2Logic and SmartSAT. Lang2Logic translates English descriptions into CNFs, and SmartSAT employs RL to optimize the CDCL heuristics, using graph representations and global feature extraction to enhance SAT solving efficiency.

**Result:** Lang2Logic successfully processed diverse English inputs up to 450 words long, converting them into CNF expressions. SmartSAT showed comparable solving times to traditional CDCL approaches. Together, they form a more accessible SAT-solving tool.

**Conclusion:** LangSAT, with its Lang2Logic and SmartSAT components, represents a breakthrough in SAT-solving by simplifying user input with natural language support and improving solving efficiency with RL-enhanced CDCL.

**Abstract:** Our work presents a novel reinforcement learning (RL) based framework to optimize heuristic selection within the conflict-driven clause learning (CDCL) process, improving the efficiency of Boolean satisfiability (SAT) solving. The proposed system, LangSAT, bridges the gap between natural language inputs and propositional logic by converting English descriptions into Conjunctive Normal Form (CNF) expressions and solving them using an RL-enhanced CDCL SAT solver. Unlike existing SAT-solving platforms that require CNF as input, LangSAT enables users to input standard English descriptions, making SAT-solving more accessible. The framework comprises two key components: Lang2Logic, which translates English sentences into CNF expressions, and SmartSAT, an RL-based SAT solver. SmartSAT encodes clause-variable relationships as structured graph representations and extracts global features specific to the SAT problem. This implementation provides the RL agent with deeper contextual information, enabling SAT problems to be solved more efficiently. Lang2Logic was evaluated on diverse natural language inputs, processing descriptions up to 450 words. The generated CNFs were solved by SmartSAT, which demonstrated comparable performance to traditional CDCL heuristics with respect to solving time. The combined LangSAT framework offers a more accessible and scalable solution for SAT-solving tasks across reasoning, formal verification, and debugging.

</details>


### [7] [MASE: Interpretable NLP Models via Model-Agnostic Saliency Estimation](https://arxiv.org/abs/2512.04386)
*Zhou Yang,Shunyan Luo,Jiazhen Zhu,Fang Jin*

Main category: cs.CL

> 本文提出MASE框架，以提高深度神经网络在自然语言处理中决策过程的解释性。

<details>
  <summary>Details</summary>

**Motivation:** 深度神经网络虽然在自然语言处理中取得了显著进展，但其可解释性差，特别是对其复杂的决策过程难以理解。传统的解释方法如注意力图和特征可视化可能不适用于文本数据的离散性。

**Method:** MASE框架通过在嵌入层应用归一化线性高斯扰动（NLGP），而非直接在原始单词输入上操作，为基于文本的预测模型提供局部解释，无需深入了解模型内部结构。

**Result:** MASE框架相较于其他模型不可知解释方法在Delta Accuracy指标上表现出色，证明其为解释NLP中基于文本模型操作的有效工具。

**Conclusion:** MASE框架展示了其在NLP模型可解释性领域的潜力，特别是在提供局部解释方面优于现有方法。

**Abstract:** Deep neural networks (DNNs) have made significant strides in Natural Language Processing (NLP), yet their interpretability remains elusive, particularly when evaluating their intricate decision-making processes. Traditional methods often rely on post-hoc interpretations, such as saliency maps or feature visualization, which might not be directly applicable to the discrete nature of word data in NLP. Addressing this, we introduce the Model-agnostic Saliency Estimation (MASE) framework. MASE offers local explanations for text-based predictive models without necessitating in-depth knowledge of a model's internal architecture. By leveraging Normalized Linear Gaussian Perturbations (NLGP) on the embedding layer instead of raw word inputs, MASE efficiently estimates input saliency. Our results indicate MASE's superiority over other model-agnostic interpretation methods, especially in terms of Delta Accuracy, positioning it as a promising tool for elucidating the operations of text-based models in NLP.

</details>


### [8] [Sarcasm Detection on Reddit Using Classical Machine Learning and Feature Engineering](https://arxiv.org/abs/2512.04396)
*Subrata Karmaker*

Main category: cs.CL

> 本文探讨了在不使用神经网络或父评论上下文的情况下，仅通过经典机器学习方法和显式特征工程进行讽刺识别的问题，采用词级别和字符级别的TF-IDF特征与简单风格特征，实验了四种模型，其中朴素贝叶斯和逻辑回归表现较好，达到了约0.57的F1值，尽管缺乏对话上下文限制了性能，但是提供了一个轻量级且可解释的方法进行讽刺探测的清晰基准。

<details>
  <summary>Details</summary>

**Motivation:** 讽刺在网络讨论中普遍存在，但由于其隐含意义通常与字面意思相矛盾，使得机器识别讽刺具有挑战性。作者旨在研究在不使用神经网络或考虑其它评论上下文的前提下，仅依靠经典机器学习方法来检测讽刺，提供一种简单的模型基准。

**Method:** 通过使用Self-Annotated Reddit Corpus（SARC 2.0）中的100,000条评论的子样本，结合词级别和字符级别的TF-IDF特征与简单的风格指标。进行了四种模型的实验：逻辑回归、线性SVM、多项式朴素贝叶斯和随机森林。

**Result:** 朴素贝叶斯和逻辑回归的表现最佳，达成了约0.57的F1值。

**Conclusion:** 尽管缺乏对话上下文使得性能受到限制，模型依然为基于轻量级和可解释方法的讽刺探测提供了一个清晰可靠的基准。

**Abstract:** Sarcasm is common in online discussions, yet difficult for machines to identify because the intended meaning often contradicts the literal wording. In this work, I study sarcasm detection using only classical machine learning methods and explicit feature engineering, without relying on neural networks or context from parent comments. Using a 100,000-comment subsample of the Self-Annotated Reddit Corpus (SARC 2.0), I combine word-level and character-level TF-IDF features with simple stylistic indicators. Four models are evaluated: logistic regression, a linear SVM, multinomial Naive Bayes, and a random forest. Naive Bayes and logistic regression perform the strongest, achieving F1-scores around 0.57 for sarcastic comments. Although the lack of conversational context limits performance, the results offer a clear and reproducible baseline for sarcasm detection using lightweight and interpretable methods.

</details>


### [9] [RapidUn: Influence-Driven Parameter Reweighting for Efficient Large Language Model Unlearning](https://arxiv.org/abs/2512.04457)
*Guoshenghui Zhao,Huawei Lin,Weijie Zhao*

Main category: cs.CL

> RapidUn, an influence-driven and parameter-efficient framework for unlearning in LLMs, achieves high efficiency and consistent performance improvements over existing methods.

<details>
  <summary>Details</summary>

**Motivation:** Retraining LLMs to remove specific data influence is costly and current approximate unlearning methods are often unstable, especially with small or imbalanced forget sets.

**Method:** RapidUn, a framework that estimates per-sample influence through a fast estimation module and maps these scores into adaptive update weights for selective parameter updates to remove specific data influence from large language models.

**Result:** Achieves up to 100 times higher efficiency than full retraining and outperforms Fisher, GA, and LoReUn on both in-distribution and out-of-distribution forgetting.

**Conclusion:** Influence-guided parameter reweighting as a scalable and interpretable paradigm for unlearning in large language models is established.

**Abstract:** Removing specific data influence from large language models (LLMs) remains challenging, as retraining is costly and existing approximate unlearning methods are often unstable. The challenge is exacerbated when the forget set is small or imbalanced. We introduce RapidUn, an influence-driven and parameter-efficient unlearning framework. It first estimates per-sample influence through a fast estimation module, then maps these scores into adaptive update weights that guide selective parameter updates -- forgetting harmful behavior while retaining general knowledge. On Mistral-7B and Llama-3-8B across Dolly-15k and Alpaca-57k, RapidUn achieves up to 100 times higher efficiency than full retraining and consistently outperforms Fisher, GA, and LoReUn on both in-distribution and out-of-distribution forgetting. These results establish influence-guided parameter reweighting as a scalable and interpretable paradigm for LLM unlearning.

</details>


### [10] [MSME: A Multi-Stage Multi-Expert Framework for Zero-Shot Stance Detection](https://arxiv.org/abs/2512.04492)
*Yuanshuo Zhang,Aohua Li,Bo Chen,Jingbo Sun,Xiaobing Zhao*

Main category: cs.CL

> MSME框架通过分阶段整合多专家分析，致力于提升零样本立场检测在复杂场景中的表现。

<details>
  <summary>Details</summary>

**Motivation:** LLM方法最近在零样本立场检测中取得了不错的成果，但这些方法在复杂真实世界场景中的表现仍然有限，特别是需要动态背景知识、处理复杂目标定义或识别修辞手法的时候。为了应对这些挑战，提出了MSME框架。

**Method:** MSME是一个用于零样本立场检测的多阶段、多专家框架。该框架由三个阶段组成：（1）知识准备，其中检索相关背景知识并明确立场标签；（2）专家推理，包括三个专门的模块，知识专家提炼相关事实并从知识视角进行推理，标签专家细化立场标签并进行相应推理，语用专家通过识别修辞手法如讽刺等从实用角度推断作者意图；（3）决策聚合，其中元裁判综合所有专家分析以产生最终的立场预测。

**Result:** 实验表明，MSME在三个公开数据集上均达到目前最先进的性能。

**Conclusion:** MSME框架通过多阶段、多专家的分析方法，在复杂真实世界场景下提高了零样本立场检测的性能。

**Abstract:** LLM-based approaches have recently achieved impressive results in zero-shot stance detection. However, they still struggle in complex real-world scenarios, where stance understanding requires dynamic background knowledge, target definitions involve compound entities or events that must be explicitly linked to stance labels, and rhetorical devices such as irony often obscure the author's actual intent. To address these challenges, we propose MSME, a Multi-Stage, Multi-Expert framework for zero-shot stance detection. MSME consists of three stages: (1) Knowledge Preparation, where relevant background knowledge is retrieved and stance labels are clarified; (2) Expert Reasoning, involving three specialized modules-Knowledge Expert distills salient facts and reasons from a knowledge perspective, Label Expert refines stance labels and reasons accordingly, and Pragmatic Expert detects rhetorical cues such as irony to infer intent from a pragmatic angle; (3) Decision Aggregation, where a Meta-Judge integrates all expert analyses to produce the final stance prediction. Experiments on three public datasets show that MSME achieves state-of-the-art performance across the board.

</details>


### [11] [UW-BioNLP at ChemoTimelines 2025: Thinking, Fine-Tuning, and Dictionary-Enhanced LLM Systems for Chemotherapy Timeline Extraction](https://arxiv.org/abs/2512.04518)
*Tianmai M. Zhang,Zhaoyi Sun,Sihang Zeng,Chenxi Li,Neil F. Abernethy,Barbara D. Lam,Fei Xia,Meliha Yetisgen*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** The ChemoTimelines shared task benchmarks methods for constructing timelines of systemic anticancer treatment from electronic health records of cancer patients. This paper describes our methods, results, and findings for subtask 2 -- generating patient chemotherapy timelines from raw clinical notes. We evaluated strategies involving chain-of-thought thinking, supervised fine-tuning, direct preference optimization, and dictionary-based lookup to improve timeline extraction. All of our approaches followed a two-step workflow, wherein an LLM first extracted chemotherapy events from individual clinical notes, and then an algorithm normalized and aggregated events into patient-level timelines. Each specific method differed in how the associated LLM was utilized and trained. Multiple approaches yielded competitive performances on the test set leaderboard, with fine-tuned Qwen3-14B achieving the best official score of 0.678. Our results and analyses could provide useful insights for future attempts on this task as well as the design of similar tasks.

</details>


### [12] [EvoEdit: Lifelong Free-Text Knowledge Editing through Latent Perturbation Augmentation and Knowledge-driven Parameter Fusion](https://arxiv.org/abs/2512.04545)
*Pengfei Cao,Zeao Ji,Daojian Zeng,Jun Zhao,Kang Liu*

Main category: cs.CL

> The paper introduces Lifelong Free-text Knowledge Editing (LF-Edit) and addresses its challenges by proposing MRLF-Bench and EvoEdit.

<details>
  <summary>Details</summary>

**Motivation:** Adjusting outdated knowledge in LLMs is difficult, and current methods lack the ability to handle nuanced free-text relationships and support continuous updates.

**Method:** EvoEdit is introduced, which includes Latent Perturbation Augmentation for knowledge injection and Knowledge-driven Parameter Fusion to preserve prior information.

**Result:** EvoEdit demonstrates superior performance compared to existing knowledge editing methods on the novel LF-Edit task.

**Conclusion:** The LF-Edit task, MRLF-Bench benchmark, and EvoEdit approach represent significant advancements toward enabling LLMs to continuously and accurately update their knowledge.

**Abstract:** Adjusting the outdated knowledge of large language models (LLMs) after deployment remains a major challenge. This difficulty has spurred the development of knowledge editing, which seeks to accurately and efficiently modify a model's internal (parametric) knowledge without retraining it from scratch. However, existing methods suffer from two limitations. First, they depend on structured triplets that are misaligned with the free-text nature of LLM pretraining and fail to capture the nuanced relationships among facts. Second, they typically support one-time knowledge updates, with relatively limited research on the problem of sequential or lifelong editing. To address these gaps, we propose a new task, Lifelong Free-text Knowledge Editing (LF-Edit), which enables models to incorporate updates expressed in natural language and supports continual editing over time. Despite its promise, LF-Edit faces the dual challenge of integrating new knowledge while mitigating the forgetting of prior information. To foster research on this new task, we construct a large-scale benchmark, Multi-Rank Lifelong Free-text Editing Benchmark (MRLF-Bench), containing 16,835 free-text edit requests. We further design a cognitively inspired multi-rank evaluation framework encompassing four levels: memorization, understanding, constrained comprehension, and reasoning. To tackle the challenges inherent in LF-Edit, we introduce a novel approach named EvoEdit that enhances knowledge injection through Latent Perturbation Augmentation and preserves prior information via Knowledge-driven Parameter Fusion. Experimental results demonstrate that EvoEdit substantially outperforms existing knowledge editing methods on the proposed LF-Edit task.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [13] [Beyond Flicker: Detecting Kinematic Inconsistencies for Generalizable Deepfake Video Detection](https://arxiv.org/abs/2512.04175)
*Alejandro Cobo,Roberto Valle,José Miguel Buenaposada,Luis Baumela*

Main category: cs.CV

> 本文提出了一种新的合成视频生成方法，通过制造微妙的运动不一致性，提升对深度伪造视频的检测能力。这种方法在多个数据集上表现出了优越的泛化性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法通常将时间不一致性视为帧间不稳定，但忽视了不同面部区域自然运动依赖性的破坏这一关键漏洞，这在处理视频领域的深度伪造检测时是一大挑战。

**Method:** 我们提出了一种合成视频生成方法，旨在创建具有微妙运动不一致性的训练数据。通过训练自编码器来分解面部关键点配置成运动基底，并操纵这些基底以打破面部运动的自然相关性，并通过面部变形把这些瑕疵引入到原始视频中。

**Result:** 在我们的数据上训练的网络能够发现这些复杂的生物力学缺陷，在几个流行的数据集上实现了最先进的泛化结果。

**Conclusion:** 我们的方法通过创建具有微妙运动不一致性的合成视频来训练深度学习网络，提高了对未见过的深度伪造视频的检测能力。

**Abstract:** Generalizing deepfake detection to unseen manipulations remains a key challenge. A recent approach to tackle this issue is to train a network with pristine face images that have been manipulated with hand-crafted artifacts to extract more generalizable clues. While effective for static images, extending this to the video domain is an open issue. Existing methods model temporal artifacts as frame-to-frame instabilities, overlooking a key vulnerability: the violation of natural motion dependencies between different facial regions. In this paper, we propose a synthetic video generation method that creates training data with subtle kinematic inconsistencies. We train an autoencoder to decompose facial landmark configurations into motion bases. By manipulating these bases, we selectively break the natural correlations in facial movements and introduce these artifacts into pristine videos via face morphing. A network trained on our data learns to spot these sophisticated biomechanical flaws, achieving state-of-the-art generalization results on several popular benchmarks.

</details>


### [14] [OnSight Pathology: A real-time platform-agnostic computational pathology companion for histopathology](https://arxiv.org/abs/2512.04187)
*Jinzhen Hu,Kevin Faust,Parsa Babaei Zadeh,Adrienn Bourkas,Shane Eaton,Andrew Young,Anzar Alvi,Dimitrios George Oreopoulos,Ameesha Paliwal,Assem Saleh Alrumeh,Evelyn Rose Kamski-Hennekam,Phedias Diamandis*

Main category: cs.CV

> OnSight Pathology is a standalone, platform-agnostic computer vision software that provides real-time AI analysis of digital slide images directly on consumer-grade PCs, overcoming deployment barriers in pathology.

<details>
  <summary>Details</summary>

**Motivation:** To address the subjective nature of expert interpretation in pathology and the barriers posed by proprietary digital pathology solutions, there is a need for an accessible, flexible, and efficient AI solution that can work across different platforms and devices.

**Method:** OnSight Pathology uses continuous custom screen captures to provide real-time AI inferences as users review digital slide images, and includes a multi-modal chat assistant for image descriptions.

**Result:** The software was tested using over 2,500 publicly available whole slide images and clinical cases, demonstrating robust performance in tasks such as tumor classification, mitosis detection, and quantification of immunohistochemical stains.

**Conclusion:** OnSight Pathology offers a versatile and user-friendly AI solution that can seamlessly integrate into various pathology workflows, enhancing the accuracy and efficiency of histological analysis across different settings.

**Abstract:** The microscopic examination of surgical tissue remains a cornerstone of disease classification but relies on subjective interpretations and access to highly specialized experts, which can compromise accuracy and clinical care. While emerging breakthroughs in artificial intelligence (AI) offer promise for automated histological analysis, the growing number of proprietary digital pathology solutions has created barriers to real-world deployment. To address these challenges, we introduce OnSight Pathology, a platform-agnostic computer vision software that uses continuous custom screen captures to provide real-time AI inferences to users as they review digital slide images. Accessible as a single, self-contained executable file (https://onsightpathology.github.io/ ), OnSight Pathology operates locally on consumer-grade personal computers without complex software integration, enabling cost-effective and secure deployment in research and clinical workflows. Here we demonstrate the utility of OnSight Pathology using over 2,500 publicly available whole slide images across different slide viewers, as well as cases from our clinical digital pathology setup. The software's robustness is highlighted across routine histopathological tasks, including the classification of common brain tumor types, mitosis detection, and the quantification of immunohistochemical stains. A built-in multi-modal chat assistant provides verifiable descriptions of images, free of rigid class labels, for added quality control. Lastly, we show compatibility with live microscope camera feeds, including from personal smartphones, offering potential for deployment in more analog, inter-operative, and telepathology settings. Together, we highlight how OnSight Pathology can deliver real-time AI inferences across a broad range of pathology pipelines, removing key barriers to the adoption of AI tools in histopathology.

</details>


### [15] [Look Around and Pay Attention: Multi-camera Point Tracking Reimagined with Transformers](https://arxiv.org/abs/2512.04213)
*Bishoy Galoaa,Xiangyu Bai,Shayda Moezzi,Utsav Nandi,Sai Siddhartha Vivek Dhir Rangoju,Somaieh Amraee,Sarah Ostadabbas*

Main category: cs.CV

> LAPA 是一种创新的多摄像机点追踪架构，使用具备几何约束和外观匹配的变压器结构。

<details>
  <summary>Details</summary>

**Motivation:** 传统的将检测、关联和追踪分离的方法会导致错误传播和时间不一致，尤其是在具有挑战性的条件下。

**Method:** LAPA 使用具有跨视图和几何注意力机制的变压器方法来联合跨多个视角和时间跟踪点，并结合部分观察和不确定性。

**Result:** {
  "tldr": "LAPA is an innovative architecture for multi-camera point tracking using transformers with geometric constraints and appearance-based matching.", 
  "motivation": "Traditional methods that decouple detection, association, and tracking result in error propagation and temporal inconsistency, especially under challenging conditions.", 
  "method": "LAPA uses a transformer-based approach with cross-view and geometric-based attention mechanisms to jointly track points across multiple views and over time, incorporating partial observations and uncertainty.", 
  "result": "Performance tests on TAPVid-3D-MC and PointOdyssey-MC revealed significant improvements over existing methods, with specific figures of 37.5% APD and 90.3% APD, respectively.", 
  "conclusion": "The unified approach of LAPA effectively addresses the limitations of current multi-camera tracking systems, showcasing advanced performance in complex motion and occlusion scenarios.")

**Conclusion:** LAPA 的统一方法有效地解决了当前多摄像机追踪系统的限制，展示出在复杂运动和遮挡情况下的高级性能。

**Abstract:** This paper presents LAPA (Look Around and Pay Attention), a novel end-to-end transformer-based architecture for multi-camera point tracking that integrates appearance-based matching with geometric constraints. Traditional pipelines decouple detection, association, and tracking, leading to error propagation and temporal inconsistency in challenging scenarios. LAPA addresses these limitations by leveraging attention mechanisms to jointly reason across views and time, establishing soft correspondences through a cross-view attention mechanism enhanced with geometric priors. Instead of relying on classical triangulation, we construct 3D point representations via attention-weighted aggregation, inherently accommodating uncertainty and partial observations. Temporal consistency is further maintained through a transformer decoder that models long-range dependencies, preserving identities through extended occlusions. Extensive experiments on challenging datasets, including our newly created multi-camera (MC) versions of TAPVid-3D panoptic and PointOdyssey, demonstrate that our unified approach significantly outperforms existing methods, achieving 37.5% APD on TAPVid-3D-MC and 90.3% APD on PointOdyssey-MC, particularly excelling in scenarios with complex motions and occlusions. Code is available at https://github.com/ostadabbas/Look-Around-and-Pay-Attention-LAPA-

</details>


### [16] [Generalized Event Partonomy Inference with Structured Hierarchical Predictive Learning](https://arxiv.org/abs/2512.04219)
*Zhou Chen,Joe Lin,Sathyanarayanan N. Aakur\\*

Main category: cs.CV

> 本文提出了PARSE框架，能够从无监督的流媒体视频中学习多尺度的事件结构，实现了预测性和分层次的视频分割，并在多个基准测试中表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 受到人类能够自然地将连续的经验感知为分层嵌套的时间事件结构的启发，作者希望在计算机视觉中复制这种结构，从而使模型不仅能回溯性地分割视频，而且能够预测性地和分层次地分割。

**Method:** 本文介绍了PARSE框架，该框架能从流媒体视频中学习多尺度的事件结构，而无需监督。PARSE将感知组织成一个由递归预测器组成的层级结构，每一层都在其自己的时间粒度上操作：低层的预测器建模短期动态，而高层的预测器则通过基于注意力的反馈整合长期上下文。事件边界自然地作为预测误差的瞬时峰值出现，从而产生在时间上一致且嵌套的类别结构，类似于人类事件感知中的包含关系。

**Result:** 在Breakfast Actions, 50 Salads和Assembly 101三个基准测试中，PARSE在流媒体方法中达到了最先进的性能，并且在时间对齐（H-GEBD）和结构一致性（TED, hF1）方面与离线基线方法的表现相当。

**Conclusion:** 研究结果表明，不确定性条件下的预测学习提供了一种可扩展的路径，能够实现类人的时序抽象和组合事件理解。

**Abstract:** Humans naturally perceive continuous experience as a hierarchy of temporally nested events, fine-grained actions embedded within coarser routines. Replicating this structure in computer vision requires models that can segment video not just retrospectively, but predictively and hierarchically. We introduce PARSE, a unified framework that learns multiscale event structure directly from streaming video without supervision. PARSE organizes perception into a hierarchy of recurrent predictors, each operating at its own temporal granularity: lower layers model short-term dynamics while higher layers integrate longer-term context through attention-based feedback. Event boundaries emerge naturally as transient peaks in prediction error, yielding temporally coherent, nested partonomies that mirror the containment relations observed in human event perception. Evaluated across three benchmarks, Breakfast Actions, 50 Salads, and Assembly 101, PARSE achieves state-of-the-art performance among streaming methods and rivals offline baselines in both temporal alignment (H-GEBD) and structural consistency (TED, hF1). The results demonstrate that predictive learning under uncertainty provides a scalable path toward human-like temporal abstraction and compositional event understanding.

</details>


### [17] [MoReGen: Multi-Agent Motion-Reasoning Engine for Code-based Text-to-Video Synthesis](https://arxiv.org/abs/2512.04221)
*Xiangyu Bai,He Liang,Bishoy Galoaa,Utsav Nandi,Shayda Moezzi,Yuhang He,Sarah Ostadabbas*

Main category: cs.CV

> 研究介绍了MoReGen框架，该框架整合了多代理语言模型、物理模拟器和渲染器，从文本描述生成物理上准确的视频。研究还提出了MoReSet数据集，用于评估视频生成中的物理有效性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管文本到视频生成在写实方面取得了显著进展，但生成能够忠实于物理原理的意图一致的视频仍然是一个核心挑战。本研究系统地探讨了基于牛顿运动控制的文本到视频生成与评估，重点是物理精度和运动连贯性。

**Method:** 我们提出了MoReGen，这是一个基于物理原理的文本到视频生成框架，它结合了多代理语言模型、物理模拟器和渲染器，从代码域中的文本提示中生成可重复的、物理上准确的视频。

**Result:** 通过MoReSet数据集的实验，我们的结果揭示了最先进模型在保持物理有效性上的困难，同时MoReGen为物理连贯的视频合成确立了一个实用的方向。

**Conclusion:** MoReGen框架证实了在文本到视频生成中整合物理模拟器和语言模型的有效性，为物理连贯视频生成提供了一个新的解决方案。

**Abstract:** While text-to-video (T2V) generation has achieved remarkable progress in photorealism, generating intent-aligned videos that faithfully obey physics principles remains a core challenge. In this work, we systematically study Newtonian motion-controlled text-to-video generation and evaluation, emphasizing physical precision and motion coherence. We introduce MoReGen, a motion-aware, physics-grounded T2V framework that integrates multi-agent LLMs, physics simulators, and renderers to generate reproducible, physically accurate videos from text prompts in the code domain. To quantitatively assess physical validity, we propose object-trajectory correspondence as a direct evaluation metric and present MoReSet, a benchmark of 1,275 human-annotated videos spanning nine classes of Newtonian phenomena with scene descriptions, spatiotemporal relations, and ground-truth trajectories. Using MoReSet, we conduct experiments on existing T2V models, evaluating their physical validity through both our MoRe metrics and existing physics-based evaluators. Our results reveal that state-of-the-art models struggle to maintain physical validity, while MoReGen establishes a principled direction toward physically coherent video synthesis.

</details>


### [18] [ReasonX: MLLM-Guided Intrinsic Image Decomposition](https://arxiv.org/abs/2512.04222)
*Alara Dirik,Tuanfeng Wang,Duygu Ceylan,Stefanos Zafeiriou,Anna Frühstück*

Main category: cs.CV

> ReasonX, a framework that uses MLLMs to guide intrinsic image decomposition via comparative supervision, shows significant performance improvements on real-world datasets.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of generalizing intrinsic image decomposition models to diverse, real-world scenarios without the need for paired supervision from synthetic datasets.

**Method:** Intrinsic image decomposition using a novel framework called ReasonX, which leverages a multimodal large language model (MLLM) to provide relative intrinsic comparisons as rewards for fine-tuning models on unlabeled images.

**Result:** ReasonX yields significant improvements across various metrics, including a 9-25% WHDR reduction on IIW albedo and up to 46% depth accuracy gains on ETH3D.

**Conclusion:** The use of MLLM-guided comparative supervision demonstrates promise in bridging the gap between low- and high-level vision reasoning for intrinsic image decomposition.

**Abstract:** Intrinsic image decomposition aims to separate images into physical components such as albedo, depth, normals, and illumination. While recent diffusion- and transformer-based models benefit from paired supervision from synthetic datasets, their generalization to diverse, real-world scenarios remains challenging. We propose ReasonX, a novel framework that leverages a multimodal large language model (MLLM) as a perceptual judge providing relative intrinsic comparisons, and uses these comparisons as GRPO rewards for fine-tuning intrinsic decomposition models on unlabeled, in-the-wild images. Unlike RL methods for generative models, our framework aligns conditional intrinsic predictors by rewarding agreement between the judge's relational assessments and analytically derived relations from the model's outputs. ReasonX is model-agnostic and can be applied to different intrinsic predictors. Across multiple base architectures and modalities, ReasonX yields significant improvements, including 9-25% WHDR reduction on IIW albedo and up to 46% depth accuracy gains on ETH3D, highlighting the promise of MLLM-guided comparative supervision to bridge low- and high-level vision reasoning.

</details>


### [19] [6 Fingers, 1 Kidney: Natural Adversarial Medical Images Reveal Critical Weaknesses of Vision-Language Models](https://arxiv.org/abs/2512.04238)
*Leon Mayer,Piotr Kalinowski,Caroline Ebersbach,Marcel Knopp,Tim Rädsch,Evangelia Christodoulou,Annika Reinke,Fiona R. Kolbinger,Lena Maier-Hein*

Main category: cs.CV

> 引入了AdversarialAnatomyBench，一个针对罕见解剖变异的基准测试，揭示了现有视觉-语言模型在处理罕见解剖结构时表现不佳的问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有基准测试主要评估常见解剖结构的表现，未能捕捉到罕见变异带来的挑战。

**Method:** 创建了一个包含多种成像方式和解剖区域的罕见解剖变异基准测试AdversarialAnatomyBench。

**Result:** 测试了22个最先进的VLM，发现其在罕见解剖变异上的准确性显著下降，并且即使在最佳模型中也存在性能下降。

**Conclusion:** 这一基准测试揭示了当前VLM在罕见解剖结构上的重大局限性，为系统地测量和减轻多模态医学AI系统的解剖偏见提供了基础。

**Abstract:** Vision-language models are increasingly integrated into clinical workflows. However, existing benchmarks primarily assess performance on common anatomical presentations and fail to capture the challenges posed by rare variants. To address this gap, we introduce AdversarialAnatomyBench, the first benchmark comprising naturally occurring rare anatomical variants across diverse imaging modalities and anatomical regions. We call such variants that violate learned priors about "typical" human anatomy natural adversarial anatomy. Benchmarking 22 state-of-the-art VLMs with AdversarialAnatomyBench yielded three key insights. First, when queried with basic medical perception tasks, mean accuracy dropped from 74% on typical to 29% on atypical anatomy. Even the best-performing models, GPT-5, Gemini 2.5 Pro, and Llama 4 Maverick, showed performance drops of 41-51%. Second, model errors closely mirrored expected anatomical biases. Third, neither model scaling nor interventions, including bias-aware prompting and test-time reasoning, resolved these issues. These findings highlight a critical and previously unquantified limitation in current VLM: their poor generalization to rare anatomical presentations. AdversarialAnatomyBench provides a foundation for systematically measuring and mitigating anatomical bias in multimodal medical AI systems.

</details>


### [20] [MVRoom: Controllable 3D Indoor Scene Generation with Multi-View Diffusion Models](https://arxiv.org/abs/2512.04248)
*Shaoheng Fang,Chaohui Yu,Fan Wang,Qixing Huang*

Main category: cs.CV

> MVRoom是一种用于3D室内场景的可控新型视图合成管道，通过两阶段设计，结合3D布局和多视角扩散，实现了高保真的3D场景生成，并且在多种方面优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在提供一种可控的新型视图合成（NVS）管道，用于生成3D室内场景，通过多视角扩散方法并依据粗糙的3D布局进行控制，以提高生成场景的一致性和可控性。

**Method:** MVRoom采用两阶段设计，第一阶段使用新的表示方法将3D布局与一致的图像条件信号结合，用于多视角生成。第二阶段进行图像条件的多视角生成，并引入了布局感知的极线注意力机制以增强多视角一致性。

**Result:** 实验结果表明，该方法在NVS的3D场景生成中达到了高保真度和可控性，无论是定量还是定性分析都优于现有的最先进的基准方法。消融研究进一步验证了该生成管道中关键组件的有效性。

**Conclusion:** 研究展示了MVRoom在生成高保真的3D场景时的卓越性能，证明了其布局感知极线注意力机制的有效性以及整个生成管道的优越性。

**Abstract:** We introduce MVRoom, a controllable novel view synthesis (NVS) pipeline for 3D indoor scenes that uses multi-view diffusion conditioned on a coarse 3D layout. MVRoom employs a two-stage design in which the 3D layout is used throughout to enforce multi-view consistency. The first stage employs novel representations to effectively bridge the 3D layout and consistent image-based condition signals for multi-view generation. The second stage performs image-conditioned multi-view generation, incorporating a layout-aware epipolar attention mechanism to enhance multi-view consistency during the diffusion process. Additionally, we introduce an iterative framework that generates 3D scenes with varying numbers of objects and scene complexities by recursively performing multi-view generation (MVRoom), supporting text-to-scene generation. Experimental results demonstrate that our approach achieves high-fidelity and controllable 3D scene generation for NVS, outperforming state-of-the-art baseline methods both quantitatively and qualitatively. Ablation studies further validate the effectiveness of key components within our generation pipeline.

</details>


### [21] [UniLight: A Unified Representation for Lighting](https://arxiv.org/abs/2512.04267)
*Zitian Zhang,Iliyan Georgiev,Michael Fischer,Yannick Hold-Geoffroy,Jean-François Lalonde,Valentin Deschaintre*

Main category: cs.CV

> 本文提出UniLight，一种可以在文本、图像、辐射度和环境图之间进行无缝转换的照明表示方法，并通过对比学习和辅助任务增强其表现力。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有的照明表示方法（如环境图、辐射度、球谐或文本）之间不兼容，无法进行跨模态转换。

**Method:** 本文提出了UniLight，这是一种联合潜在空间的照明表示方法，可以统一多种模态的共享嵌入。通过对文本、图像、辐射度和环境图进行对比训练，使其表示对齐，并加入辅助的球谐预测任务以增强方向理解。

**Result:** 实验表明，该表示方法能够捕捉一致且可转移的照明特征，使得在不同模态之间的灵活操作成为可能。

**Conclusion:** 根据实验结果，该联合潜在空间的照明表示方法能有效捕捉和转移跨模态的照明特征。

**Abstract:** Lighting has a strong influence on visual appearance, yet understanding and representing lighting in images remains notoriously difficult. Various lighting representations exist, such as environment maps, irradiance, spherical harmonics, or text, but they are incompatible, which limits cross-modal transfer. We thus propose UniLight, a joint latent space as lighting representation, that unifies multiple modalities within a shared embedding. Modality-specific encoders for text, images, irradiance, and environment maps are trained contrastively to align their representations, with an auxiliary spherical-harmonics prediction task reinforcing directional understanding. Our multi-modal data pipeline enables large-scale training and evaluation across three tasks: lighting-based retrieval, environment-map generation, and lighting control in diffusion-based image synthesis. Experiments show that our representation captures consistent and transferable lighting features, enabling flexible manipulation across modalities.

</details>


### [22] [Inference-time Stochastic Refinement of GRU-Normalizing Flow for Real-time Video Motion Transfer](https://arxiv.org/abs/2512.04282)
*Tasmiah Haque,Srinjoy Das*

Main category: cs.CV

> 本文提出了一种新的推理时优化技术(GRU-SNF)，通过结合GRU-NF和随机抽样方法，在不重新训练的情况下生成多样且准确的输出，特别是在时间序列预测方面展现出了潜力。

<details>
  <summary>Details</summary>

**Motivation:** 旨在改进对序列预测的多样性，支持更真实的合成和不确定性下的稳健决策，特别是应用于沉浸式游戏和基于视觉的异常检测等领域。

**Method:** 本文提出了一种新的推理时优化技术，结合了门控循环单元-规范化流（GRU-NF）与随机抽样方法。通过在GRU-NF推理中引入马尔可夫链蒙特卡洛（MCMC）步骤，模型可以在不重新训练的情况下探索更丰富的输出空间，并更好地逼近真实数据分布。

**Result:** 实验结果表明，所提出的推理框架（Gated Recurrent Unit- Stochastic Normalizing Flows，GRU-SNF）在生成多样化输出方面优于GRU-NF，即便是在更长的预测窗口下，也未牺牲预测准确性。

**Conclusion:** 这些结果凸显了将随机动力学与基于流的顺序模型相结合在生成时间序列预测中的潜力。

**Abstract:** Real-time video motion transfer applications such as immersive gaming and vision-based anomaly detection require accurate yet diverse future predictions to support realistic synthesis and robust downstream decision making under uncertainty. To improve the diversity of such sequential forecasts we propose a novel inference-time refinement technique that combines Gated Recurrent Unit-Normalizing Flows (GRU-NF) with stochastic sampling methods. While GRU-NF can capture multimodal distributions through its integration of normalizing flows within a temporal forecasting framework, its deterministic transformation structure can limit expressivity. To address this, inspired by Stochastic Normalizing Flows (SNF), we introduce Markov Chain Monte Carlo (MCMC) steps during GRU-NF inference, enabling the model to explore a richer output space and better approximate the true data distribution without retraining. We validate our approach in a keypoint-based video motion transfer pipeline, where capturing temporally coherent and perceptually diverse future trajectories is essential for realistic samples and low bandwidth communication. Experiments show that our inference framework, Gated Recurrent Unit- Stochastic Normalizing Flows (GRU-SNF) outperforms GRU-NF in generating diverse outputs without sacrificing accuracy, even under longer prediction horizons. By injecting stochasticity during inference, our approach captures multimodal behavior more effectively. These results highlight the potential of integrating stochastic dynamics with flow-based sequence models for generative time series forecasting.

</details>


### [23] [Plug-and-Play Image Restoration with Flow Matching: A Continuous Viewpoint](https://arxiv.org/abs/2512.04283)
*Fan Jia,Yuhao Huang,Shih-Hsin Wang,Cristina Garcia-Cardona,Andrea L. Bertozzi,Bao Wang*

Main category: cs.CV

> The paper derives a stochastic differential equation (SDE) model for PnP-Flow and uses it to inform improvements in image restoration, achieving better performance in various tasks compared to existing methods.

<details>
  <summary>Details</summary>

**Motivation:** To close the gap between the empirical success of PnP-Flow for image restoration and the lack of theoretical understanding, and to introduce modifications that enhance its error reduction and performance acceleration.

**Method:** Flow matching-based generative models are integrated into image restoration, leading to the PnP-Flow model. The authors derive a continuous limit for PnP-Flow, resulting in an SDE surrogate model that provides insights for improving PnP-Flow.

**Result:** The SDE model enables the quantification of image restoration errors and the acceleration of PnP-Flow models via extrapolation. Numerical results demonstrate significant improvement in performance over the baseline PnP-Flow and other state-of-the-art approaches.

**Conclusion:** The continuous time limit of PnP-Flow provides a theoretical foundation for understanding and enhancing the model through adaptations like improved step scheduling and extrapolation acceleration.

**Abstract:** Flow matching-based generative models have been integrated into the plug-and-play image restoration framework, and the resulting plug-and-play flow matching (PnP-Flow) model has achieved some remarkable empirical success for image restoration. However, the theoretical understanding of PnP-Flow lags its empirical success. In this paper, we derive a continuous limit for PnP-Flow, resulting in a stochastic differential equation (SDE) surrogate model of PnP-Flow. The SDE model provides two particular insights to improve PnP-Flow for image restoration: (1) It enables us to quantify the error for image restoration, informing us to improve step scheduling and regularize the Lipschitz constant of the neural network-parameterized vector field for error reduction. (2) It informs us to accelerate off-the-shelf PnP-Flow models via extrapolation, resulting in a rescaled version of the proposed SDE model. We validate the efficacy of the SDE-informed improved PnP-Flow using several benchmark tasks, including image denoising, deblurring, super-resolution, and inpainting. Numerical results show that our method significantly outperforms the baseline PnP-Flow and other state-of-the-art approaches, achieving superior performance across evaluation metrics.

</details>


### [24] [Learning Single-Image Super-Resolution in the JPEG Compressed Domain](https://arxiv.org/abs/2512.04284)
*Sruthi Srinivasan,Elham Shakibapour,Rajy Rawther,Mehdi Saeedi*

Main category: cs.CV

> 本文提出了一种直接基于JPEG编码特征训练模型的方法，针对单图像超分辨率任务，提高了数据加载和训练效率。

<details>
  <summary>Details</summary>

**Motivation:** 尽管专用于深度学习的硬件有了显著进步，数据加载仍然是一个重大瓶颈，限制了训练和推断的速度。为了提高超分辨率任务中的数据加载和整体处理速度，本文提出了一种新的方法。

**Method:** 我们提出了一种在JPEG编码特征上直接训练模型的方法，这种方法减少了完整的JPEG解码所需的计算开销，从而显著提高了数据加载效率。我们的方法针对的是单图像超分辨率（SISR）任务，构建了一个轻量级的超分辨率管道，该管道在JPEG离散余弦变换（DCT）系数的频率域上操作。

**Result:** 我们的方法在数据加载速度方面实现了2.6倍的提升，在训练速度方面实现了2.5倍的加速，同时保持了与标准SISR方法相当的视觉质量。

**Conclusion:** 本文的研究表明，在JPEG编码特征上直接进行超分辨率模型的训练不仅是可行的，而且在不牺牲视觉质量的同时，还能显著提升数据加载和训练的速度。这为未来的深度学习研究提供了一种新的思路。

**Abstract:** Deep learning models have grown increasingly complex, with input data sizes scaling accordingly. Despite substantial advances in specialized deep learning hardware, data loading continues to be a major bottleneck that limits training and inference speed. To address this challenge, we propose training models directly on encoded JPEG features, reducing the computational overhead associated with full JPEG decoding and significantly improving data loading efficiency. While prior works have focused on recognition tasks, we investigate the effectiveness of this approach for the restoration task of single-image super-resolution (SISR). We present a lightweight super-resolution pipeline that operates on JPEG discrete cosine transform (DCT) coefficients in the frequency domain. Our pipeline achieves a 2.6x speedup in data loading and a 2.5x speedup in training, while preserving visual quality comparable to standard SISR approaches.

</details>


### [25] [Gamma-from-Mono: Road-Relative, Metric, Self-Supervised Monocular Geometry for Vehicular Applications](https://arxiv.org/abs/2512.04303)
*Gasser Elazab,Maximilian Jansen,Michael Unterreiner,Olaf Hellwich*

Main category: cs.CV

> 该研究提出了一种轻量级的单目几何估计方法--GfM，该方法能够更准确地重建车辆周围的3D环境，尤其是在道路精细几何特征上，如坡度、凸起和表面不规则性。

<details>
  <summary>Details</summary>

**Motivation:** 传统的单目深度估计往往对道路细节进行过度平滑，导致重要的运动规划和稳定信息丢失。为了改善这一问题，提出了GfM方法。

**Method:** GfM采用轻量级单目几何估计方法，通过解耦全局和局部结构来解决单相机重建中的投影模糊问题。GfM预测一个主导道路平面，并通过γ值表达垂直偏差，γ定义为点的高度与相机深度比值，从而确定性恢复度量深度，避免完全外参标定，并自然优先近道路细节。

**Result:** 在KITTI和道路表面重建数据集(RSRD)上的评估显示，GfM在深度和γ估计上达到了最先进的近场精度，同时在全球深度性能上也保持了竞争力。

**Conclusion:** 该研究展示了GfM方法在恢复上下文的重要性、全球深度性能，并初步探讨了其自我监督学习的潜力。同时，该方法在没有大量注释数据集的情况下，可以达到较高的精度。

**Abstract:** Accurate perception of the vehicle's 3D surroundings, including fine-scale road geometry, such as bumps, slopes, and surface irregularities, is essential for safe and comfortable vehicle control. However, conventional monocular depth estimation often oversmooths these features, losing critical information for motion planning and stability. To address this, we introduce Gamma-from-Mono (GfM), a lightweight monocular geometry estimation method that resolves the projective ambiguity in single-camera reconstruction by decoupling global and local structure. GfM predicts a dominant road surface plane together with residual variations expressed by gamma, a dimensionless measure of vertical deviation from the plane, defined as the ratio of a point's height above it to its depth from the camera, and grounded in established planar parallax geometry. With only the camera's height above ground, this representation deterministically recovers metric depth via a closed form, avoiding full extrinsic calibration and naturally prioritizing near-road detail. Its physically interpretable formulation makes it well suited for self-supervised learning, eliminating the need for large annotated datasets. Evaluated on KITTI and the Road Surface Reconstruction Dataset (RSRD), GfM achieves state-of-the-art near-field accuracy in both depth and gamma estimation while maintaining competitive global depth performance. Our lightweight 8.88M-parameter model adapts robustly across diverse camera setups and, to our knowledge, is the first self-supervised monocular approach evaluated on RSRD.

</details>


### [26] [How (Mis)calibrated is Your Federated CLIP and What To Do About It?](https://arxiv.org/abs/2512.04305)
*Mainak Singha,Masih Aminbeidokhti,Paolo Casari,Elisa Ricci,Subhankar Roy*

Main category: cs.CV

> This paper investigates CLIP calibration in FL, showing Textual Prompt Tuning degrades calibration and proposing ΔFL^2oRA for improved calibration without additional procedures.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to explore how federated learning affects CLIP's calibration, and to propose strategies to enhance the reliability of CLIP models in a distributed federated setting.

**Method:** We first analyze Textual Prompt Tuning approaches under federated learning (FL) and find they degrade calibration metrics. We then evaluate existing in-training calibration techniques across four global aggregation methods, and propose a LoRA-based method called ΔFL^2oRA to improve calibration in FL without explicit calibration procedures.

**Result:** The results show that Textual Prompt Tuning degrades calibration under FL, and existing in-training calibration techniques provide limited improvement. The proposed ΔFL^2oRA method consistently improves calibration without explicit calibration procedures.

**Conclusion:** The conclusion is that the key to improving calibration in federated learning lies in the choice of components for fine-tuning. The proposed method ΔFL^2oRA, based on LoRA, enhances calibration effectiveness in federated settings without additional calibration measures.

**Abstract:** While vision-language models like CLIP have been extensively studied, their calibration, crucial for reliable predictions, has received limited attention. Although a few prior works have examined CLIP calibration in offline settings, the impact of fine-tuning CLIP in a federated learning (FL) setup remains unexplored. In this work, we investigate how FL affects CLIP calibration and propose strategies to improve reliability in this distributed setting. We first analyze Textual Prompt Tuning approaches and show that they degrade calibration metrics when operating under FL. We also evaluate existing in-training calibration techniques across four global aggregation methods, finding that they provide limited improvements. Our results suggest that the key challenge lies not only in how we aggregate or calibrate, but in which components we choose to fine-tune. Motivated by this insight, we propose $\text{FL}^2\text{oRA}$, a straightforward LoRA-based approach that naturally improves calibration in FL, and we analyze the factors behind its effectiveness. Experiments on multiple benchmarks demonstrate that $\text{FL}^2\text{oRA}$ consistently produces well-calibrated models, reducing the need for explicit calibration procedures. Codes are available at https://github.com/mainaksingha01/FL2oRA.

</details>


### [27] [Text-Only Training for Image Captioning with Retrieval Augmentation and Modality Gap Correction](https://arxiv.org/abs/2512.04309)
*Rui Fonseca,Bruno Martins,Gil Rocha*

Main category: cs.CV

> 本论文提出了TOMCap方法，通过优化预训练语言模型减少模态差异，提高无监督图像描述性能。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于降低对精心策划数据的依赖，探索无须任何人工标注图像-文本对就能完成图像描述的技术。

**Method:** TOMCap方法通过在CLIP表示之上进行模态差异减少处理后，利用一个预训练的语言模型解码器来生成图像描述。该方法使用检索到的描述实例和潜在向量表示来辅助生成过程，从而在无需图像-文字配对的情况下完成图像描述任务。

**Result:** 通过广泛的实验，TOMCap方法在不使用图像-文字对的训练和无需额外训练的方法中表现出色。

**Conclusion:** 论文结论指出，TOMCap在无监督和文本训练方法中的优越性，并分析了不同检索增强和模态差距减少策略对结果的影响。

**Abstract:** Image captioning has drawn considerable attention from the natural language processing and computer vision fields. Aiming to reduce the reliance on curated data, several studies have explored image captioning without any humanly-annotated image-text pairs for training, although existing methods are still outperformed by fully supervised approaches. This paper proposes TOMCap, i.e., an improved text-only training method that performs captioning without the need for aligned image-caption pairs. The method is based on prompting a pre-trained language model decoder with information derived from a CLIP representation, after undergoing a process to reduce the modality gap. We specifically tested the combined use of retrieved examples of captions, and latent vector representations, to guide the generation process. Through extensive experiments, we show that TOMCap outperforms other training-free and text-only methods. We also analyze the impact of different choices regarding the configuration of the retrieval-augmentation and modality gap reduction components.

</details>
