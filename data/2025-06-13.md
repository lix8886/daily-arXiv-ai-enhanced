<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 22]
- [cs.CV](#cs.CV) [Total: 19]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Survey of Automatic Evaluation Methods on Text, Visual and Speech Generations](https://arxiv.org/abs/2506.10019)
*Tian Lan,Yang-Hao Zhou,Zi-Ao Ma,Fanshu Sun,Rui-Qing Sun,Junyu Luo,Rong-Cheng Tu,Heyan Huang,Chen Xu,Zhijing Wu,Xian-Ling Mao*

Main category: cs.CL

本文综述了文本、图像和音频生成内容的自动评估方法，并提出了一个统一分类法，以解决现有研究中缺乏系统性框架的问题。


<details>
  <summary>Details</summary>
Motivation: 由于当前研究缺乏一个系统性框架来全面组织这些方法，本文旨在弥补这一不足。

Method: 本文提出了一种跨文本、图像和音频生成内容的自动评估方法的综合回顾和统一分类法，并识别出了五个基本范式来描述这些领域的现有评估方法。

Result: 研究结果表明所提出的框架可以广泛应用于文本、图像和音频生成的内容评估。

Conclusion: 结论指出跨模态评估方法在未来的研究中具有潜在的发展方向。

Abstract: Recent advances in deep learning have significantly enhanced generative AI
capabilities across text, images, and audio. However, automatically evaluating
the quality of these generated outputs presents ongoing challenges. Although
numerous automatic evaluation methods exist, current research lacks a
systematic framework that comprehensively organizes these methods across text,
visual, and audio modalities. To address this issue, we present a comprehensive
review and a unified taxonomy of automatic evaluation methods for generated
content across all three modalities; We identify five fundamental paradigms
that characterize existing evaluation approaches across these domains. Our
analysis begins by examining evaluation methods for text generation, where
techniques are most mature. We then extend this framework to image and audio
generation, demonstrating its broad applicability. Finally, we discuss
promising directions for future research in cross-modal evaluation
methodologies.

</details>


### [2] [TaskCraft: Automated Generation of Agentic Tasks](https://arxiv.org/abs/2506.10055)
*Dingfeng Shi,Jingyi Cao,Qianben Chen,Weichen Sun,Weizhen Li,Hongxuan Lu,Fangchen Dong,Tianrui Qin,King Zhu,Minghao Yang,Jian Yang,Ge Zhang,Jiaheng Liu,Changwang Zhang,Jun Wang,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TaskCraft是用于自动生成具有不同难度的能动任务的自动化工作流，其任务结构复杂且可验证。通过实验验证了这种方法的有效性，并创建了一个大规模的任务数据集。


<details>
  <summary>Details</summary>
Motivation: 能动任务需要多步骤的问题解决、自主性、工具使用及适应性推理，在NLP和AI的进展中越来越重要。然而，现有的指令数据缺乏工具交互，当前的能动基准依赖于昂贵的人类标注，限制了其可扩展性。

Method: 本研究提出了一种名为TaskCraft的自动化工作流，用于生成具有不同难度、多工具并可验证的能动任务，这些任务带有执行轨迹。TaskCraft通过基于深度和宽度的扩展来扩展原子任务，创建结构上和层次上复杂的挑战。

Result: 实验结果表明，这些任务在生成工作流中改善了提示优化，并增强了能动基础模型的监督微调。

Conclusion: 研究还提供了一个包含约36,000个难度各异的任务的大规模合成数据集，支持未来的能动模型调整和评估研究。

Abstract: Agentic tasks, which require multi-step problem solving with autonomy, tool
use, and adaptive reasoning, are becoming increasingly central to the
advancement of NLP and AI. However, existing instruction data lacks tool
interaction, and current agentic benchmarks rely on costly human annotation,
limiting their scalability. We introduce \textsc{TaskCraft}, an automated
workflow for generating difficulty-scalable, multi-tool, and verifiable agentic
tasks with execution trajectories. TaskCraft expands atomic tasks using
depth-based and width-based extensions to create structurally and
hierarchically complex challenges. Empirical results show that these tasks
improve prompt optimization in the generation workflow and enhance supervised
fine-tuning of agentic foundation models. We present a large-scale synthetic
dataset of approximately 36,000 tasks with varying difficulty to support future
research on agent tuning and evaluation.

</details>


### [3] [A quantum semantic framework for natural language processing](https://arxiv.org/abs/2506.10077)
*Christopher J. Agostino,Quan Le Thien,Molly Apsel,Denizhan Pak,Elina Lesyk,Ashabari Majumdar*

Main category: cs.CL

该研究通过语义贝尔不等式测试展示了语言解释在模糊性的上下文表现中具有非经典的上下文性，并提议使用贝叶斯风格的重复抽样方法而非传统的基于频率的方法来分析自然语言。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在探讨自然语言中语义退化带来的根本问题，特别是大型语言模型和其他现代NLP系统由于运行于自然语言本身而面对的相关限制。

Method: 通过Kolmogorov复杂性理论论证，随着表达复杂性的增加，任何解读代理（无论是人类还是LLM驱动的AI）恢复单一意指意义的可能性会消失。为了验证这一观点，作者进行了一场语义贝尔不等式的测试，利用多种LLM代理作为“计算认知系统”，在不同的上下文环境下来解释模糊的词对。

Result: 在几个独立的实验中，平均CHSH期望值范围从1.2到2.8，有几组实验结果（例如，2.3-2.4）显著违反了经典边界（$|S|\leq2$）。这表明，在模糊性下的语言解释可以表现出非经典的上下文性，这与人类认知实验的结果一致。

Conclusion: 研究结果暗示经典的基于频率分析的自然语言方法是必然的不完整。相反，作者提议贝叶斯风格的重复抽样方法可以提供更实用且更合适的方式来刻画语言意义的上下文性。

Abstract: Semantic degeneracy represents a fundamental property of natural language
that extends beyond simple polysemy to encompass the combinatorial explosion of
potential interpretations that emerges as semantic expressions increase in
complexity. Large Language Models (LLMs) and other modern NLP systems face
inherent limitations precisely because they operate within natural language
itself, making them subject to the same interpretive constraints imposed by
semantic degeneracy. In this work, we argue using Kolmogorov complexity that as
an expression's complexity grows, the likelihood of any interpreting agent
(human or LLM-powered AI) recovering the single intended meaning vanishes. This
computational intractability suggests the classical view that linguistic forms
possess meaning in and of themselves is flawed. We alternatively posit that
meaning is instead actualized through an observer-dependent interpretive act.
To test this, we conducted a semantic Bell inequality test using diverse LLM
agents as ``computational cognitive systems'' to interpret ambiguous word pairs
under varied contextual settings. Across several independent experiments, we
found average CHSH expectation values ranging from 1.2 to 2.8, with several
runs yielding values (e.g., 2.3-2.4) that significantly violate the classical
boundary ($|S|\leq2$). This demonstrates that linguistic interpretation under
ambiguity can exhibit non-classical contextuality, consistent with results from
human cognition experiments. These results inherently imply that classical
frequentist-based analytical approaches for natural language are necessarily
lossy. Instead, we propose that Bayesian-style repeated sampling approaches can
provide more practically useful and appropriate characterizations of linguistic
meaning in context.

</details>


### [4] [Chat-of-Thought: Collaborative Multi-Agent System for Generating Domain Specific Information](https://arxiv.org/abs/2506.10086)
*Christodoulos Constantinides,Shuxin Lin,Nianjun Zhou,Dhaval Patel*

Main category: cs.CL

本文介绍了一个创新的多智能体系统Chat-of-Thought，以利用先进AI技术优化工业资产的FMEA文档生成流程，并展示了该系统在工业监控领域中的应用前景。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨工业设备监控领域的应用，重点在于解决生成及验证FMEA文档时的关键挑战，展示Chat-of-Thought系统在通过交互式模板工作流程和上下文感知的智能体协作来应对这些挑战方面的潜力。

Method: 本文介绍了一个称为Chat-of-Thought的多智能体系统，该系统利用多个具有特定角色的大语言模型（LLM）智能体，结合先进的AI技术和动态任务调度，优化生成和验证FMEA表格的过程。一个关键的创新点是引入了思想对话，通过动态、多角色驱动的讨论，实现了FMEA内容的迭代优化。

Result: 结果未明确说明，但该系统展示了生成和优化FMEA表格的潜力，以及在工业设备监控中的应用前景。

Conclusion: Chat-of-Thought系统的提出，为解决工业资产的FMEA文档生成问题提供了一种创新的解决方案，其通过多智能体的协作，提高了FMEA表格生成和验证的效率与质量，并展示了在工业监控领域的实际应用潜力。

Abstract: This paper presents a novel multi-agent system called Chat-of-Thought,
designed to facilitate the generation of Failure Modes and Effects Analysis
(FMEA) documents for industrial assets. Chat-of-Thought employs multiple
collaborative Large Language Model (LLM)-based agents with specific roles,
leveraging advanced AI techniques and dynamic task routing to optimize the
generation and validation of FMEA tables. A key innovation in this system is
the introduction of a Chat of Thought, where dynamic, multi-persona-driven
discussions enable iterative refinement of content. This research explores the
application domain of industrial equipment monitoring, highlights key
challenges, and demonstrates the potential of Chat-of-Thought in addressing
these challenges through interactive, template-driven workflows and
context-aware agent collaboration.

</details>


### [5] [When Meaning Stays the Same, but Models Drift: Evaluating Quality of Service under Token-Level Behavioral Instability in LLMs](https://arxiv.org/abs/2506.10095)
*Xiao Li,Joel Kreuzwieser,Alan Peters*

Main category: cs.CL

论文通过PBSS框架研究大型语言模型在语义相同的提示变化下的行为变化，揭示了模型响应的统计规律性，强调了重述对模型稳定性的影响。


<details>
  <summary>Details</summary>
Motivation: 我们研究大型语言模型是如何响应仅有单词级别实现差异但保持相同语义意图的提示，这一现象被称为提示变化。

Method: 我们提出了一种诊断框架——基于提示的语义漂移（PBSS），用于测量在语义等效的提示重述下大型语言模型的行为漂移。

Result: 应用PBSS到十个受限任务中，揭示了一致的模型特定响应变化，这表明与分词和解码相关的统计规律。

Conclusion: 这些结果强调了一个被忽视的模型评估稳定性维度，即重述对稳定性的影响，表明分词策略和解码动态可能对模型在训练后服务质量的稳定性有贡献。

Abstract: We investigate how large language models respond to prompts that differ only
in their token-level realization but preserve the same semantic intent, a
phenomenon we call prompt variance. We propose Prompt-Based Semantic Shift
(PBSS), a diagnostic framework for measuring behavioral drift in LLMs under
semantically equivalent prompt rewordings. Applied to ten constrained tasks,
PBSS reveals consistent, model-specific response shifts, suggesting statistical
regularities linked to tokenization and decoding. These results highlight an
overlooked dimension of model evaluation stability under rephrasing and suggest
that tokenization strategies and decoding dynamics may contribute to
post-training quality of service instability.

</details>


### [6] [ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in Chart Question Answering](https://arxiv.org/abs/2506.10116)
*Caijun Jia,Nan Xu,Jingxuan Wei,Qingli Wang,Lei Wang,Bihui Yu,Junnan Zhu*

Main category: cs.CL

ChartReasoner is a two-stage framework for visual reasoning over charts, which outperforms existing models while maintaining simplicity and reducing parameter usage.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of extending large language models' reasoning capabilities to visual reasoning tasks, specifically for chart question answering, where critical visual details are often lost in existing multimodal reasoning approaches.

Method: We propose ChartReasoner, a two-stage framework that first converts chart images into structured ECharts codes, and then uses a chart reasoning data synthesis pipeline for automatic and scalable generation of reasoning datasets, combined with supervised fine-tuning and reinforcement learning for training the final model.

Result: Experimental results on four public benchmarks demonstrate the effectiveness of ChartReasoner, showcasing its ability to maintain original chart details and perform comparably with state-of-the-art models while using fewer parameters.

Conclusion: ChartReasoner preserves the specific details of charts and can closely match the performance of proprietary models with fewer parameters, especially in out-of-domain settings.

Abstract: Recently, large language models have shown remarkable reasoning capabilities
through long-chain reasoning before responding. However, how to extend this
capability to visual reasoning tasks remains an open challenge. Existing
multimodal reasoning approaches transfer such visual reasoning task into
textual reasoning task via several image-to-text conversions, which often lose
critical structural and semantic information embedded in visualizations,
especially for tasks like chart question answering that require a large amount
of visual details. To bridge this gap, we propose ChartReasoner, a code-driven
novel two-stage framework designed to enable precise, interpretable reasoning
over charts. We first train a high-fidelity model to convert diverse chart
images into structured ECharts codes, preserving both layout and data semantics
as lossless as possible. Then, we design a general chart reasoning data
synthesis pipeline, which leverages this pretrained transport model to
automatically and scalably generate chart reasoning trajectories and utilizes a
code validator to filter out low-quality samples. Finally, we train the final
multimodal model using a combination of supervised fine-tuning and
reinforcement learning on our synthesized chart reasoning dataset and
experimental results on four public benchmarks clearly demonstrate the
effectiveness of our proposed ChartReasoner. It can preserve the original
details of the charts as much as possible and perform comparably with
state-of-the-art open-source models while using fewer parameters, approaching
the performance of proprietary systems like GPT-4o in out-of-domain settings.

</details>


### [7] [Unsupervised Elicitation of Language Models](https://arxiv.org/abs/2506.10139)
*Jiaxin Wen,Zachary Ankner,Arushi Somani,Peter Hase,Samuel Marks,Jacob Goldman-Wetzler,Linda Petrini,Henry Sleight,Collin Burns,He He,Shi Feng,Ethan Perez,Jan Leike*

Main category: cs.CL

提出了Internal Coherence Maximization (ICM)算法，无监督地优化预训练语言模型，性能赶上或超过人类监督训练。


<details>
  <summary>Details</summary>
Motivation: 解决超级能力模型难以获得高质量的人类监督的问题。

Method: Internal Coherence Maximization (ICM)算法用于微调预训练语言模型，使用模型自身生成的标签进行训练，无需外部监督。

Result: 在GSM8k-verification, TruthfulQA, 和 Alpaca奖励模型任务中匹配到黄金监督训练的性能，并且在超级能力模型的能力超过人类时，我们的方法可以显著发挥这些能力。

Conclusion: 该方法能够提升前沿模型的训练效果，无论是奖励模型还是助理模型都优于人类监督训练的模型。

Abstract: To steer pretrained language models for downstream tasks, today's
post-training paradigm relies on humans to specify desired behaviors. However,
for models with superhuman capabilities, it is difficult or impossible to get
high-quality human supervision. To address this challenge, we introduce a new
unsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune
pretrained language models on their own generated labels, \emph{without
external supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward
modeling tasks, our method matches the performance of training on golden
supervision and outperforms training on crowdsourced human supervision. On
tasks where LMs' capabilities are strongly superhuman, our method can elicit
those capabilities significantly better than training on human labels. Finally,
we show that our method can improve the training of frontier LMs: we use our
method to train an unsupervised reward model and use reinforcement learning to
train a Claude 3.5 Haiku-based assistant. Both the reward model and the
assistant outperform their human-supervised counterparts.

</details>


### [8] [When Large Language Models are Reliable for Judging Empathic Communication](https://arxiv.org/abs/2506.10150)
*Aakriti Kumar,Nalin Poungpeth,Diyi Yang,Erina Farrell,Bruce Lambert,Matthew Groh*

Main category: cs.CL

本研究通过比较专家、群众工作者和大语言模型（LLMs）在四个评估框架下的标注结果，发现LLMs在判断同理心交流的细微差别方面几乎达到专家水平，并超过了群众工作者。该研究支持在情感敏感应用中的透明度和监督使用LLMs。


<details>
  <summary>Details</summary>
Motivation: 动机在于探索LLMs在产生同理心回应方面是否可靠，特别是在理解同理心交流的细微差异方面的表现。

Method: 研究通过比较专家、群众工作者和大语言模型（LLMs）在四套评估框架下对同理心交流的标注结果来探究LLMs在判断同理心交流细微差别方面的可靠性。研究采用了200个真实世界中一个说话人分享个人问题，另一个说话人提供支持的对话样本。

Result: 结果显示，专家的协议水平很高，但在不同框架的子组件中有所不同，这取决于这些组件的清晰度、复杂性和主观性。LLMs在所有四个框架下达到专家水平的基准，其可靠性超过了群众工作者，并能在特定任务上以适当的基准进行验证。

Conclusion: 研究结论显示，当用特定任务的适当基准验证时，LLMs可以支持在情感敏感应用中提高透明度和监督，包括作为对话伴侣的用途。

Abstract: Large language models (LLMs) excel at generating empathic responses in
text-based conversations. But, how reliably do they judge the nuances of
empathic communication? We investigate this question by comparing how experts,
crowdworkers, and LLMs annotate empathic communication across four evaluative
frameworks drawn from psychology, natural language processing, and
communications applied to 200 real-world conversations where one speaker shares
a personal problem and the other offers support. Drawing on 3,150 expert
annotations, 2,844 crowd annotations, and 3,150 LLM annotations, we assess
inter-rater reliability between these three annotator groups. We find that
expert agreement is high but varies across the frameworks' sub-components
depending on their clarity, complexity, and subjectivity. We show that expert
agreement offers a more informative benchmark for contextualizing LLM
performance than standard classification metrics. Across all four frameworks,
LLMs consistently approach this expert level benchmark and exceed the
reliability of crowdworkers. These results demonstrate how LLMs, when validated
on specific tasks with appropriate benchmarks, can support transparency and
oversight in emotionally sensitive applications including their use as
conversational companions.

</details>


### [9] [Analyzing Emotions in Bangla Social Media Comments Using Machine Learning and LIME](https://arxiv.org/abs/2506.10154)
*Bidyarthi Paul,SM Musfiqur Rahman,Dipta Biswas,Md. Ziaul Hasan,Md. Zahid Hossain*

Main category: cs.CL

本文研究了各种技术在孟加拉语情感分析中的应用，使用机器学习模型进行了实验，并探讨了这些方法对情感识别的效率提升。


<details>
  <summary>Details</summary>
Motivation: 研究继续探索理解书面语言中的情感，特别是在具有独特区域表达和文化特征的较少研究的语言中，如孟加拉语。

Method: 研究中使用了机器学习模型，包括线性SVM，KNN，随机森林，以及TF-IDF向量化器的n-gram数据，还研究了PCA对降维的影响，使用了BiLSTM模型和AdaBoost来改进决策树，并用LIME来解释AdaBoost分类器的预测。

Result: 通过对EmoNoBa数据集中的22,698个社交媒体评论进行情感分析，研究表明，所使用的方法有助于提高孟加拉语情感分析的效率。

Conclusion: 研究探讨了多种技术以找到在孟加拉语中进行情绪识别的有效方法，有助于推进资源有限的语言的情感分析。

Abstract: Research on understanding emotions in written language continues to expand,
especially for understudied languages with distinctive regional expressions and
cultural features, such as Bangla. This study examines emotion analysis using
22,698 social media comments from the EmoNoBa dataset. For language analysis,
we employ machine learning models: Linear SVM, KNN, and Random Forest with
n-gram data from a TF-IDF vectorizer. We additionally investigated how PCA
affects the reduction of dimensionality. Moreover, we utilized a BiLSTM model
and AdaBoost to improve decision trees. To make our machine learning models
easier to understand, we used LIME to explain the predictions of the AdaBoost
classifier, which uses decision trees. With the goal of advancing sentiment
analysis in languages with limited resources, our work examines various
techniques to find efficient techniques for emotion identification in Bangla.

</details>


### [10] [Measuring Corporate Human Capital Disclosures: Lexicon, Data, Code, and Research Opportunities](https://arxiv.org/abs/2506.10155)
*Elizabeth Demers,Victor Xiaoqi Wang,Kean Wu*

Main category: cs.CL

通过word2vec算法处理人力资本披露信息，构建了一个包含五类子类别的综合人力资本关键词列表，并提供了代码及示例以供研究者使用或修改以更好地挖掘人力资本信息。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏明确的衡量和披露规则，该研究旨在通过机器学习构建人力资本关键词列表，以更好地理解和衡量人力资本对企业的价值创造的重要性。

Method: 研究使用机器学习算法word2vec分析已确认的人力资本披露信息，建立了包含五类子类别的人力资本相关关键词列表，并提供Python代码及详细使用示例。

Result: 研究通过机器学习算法（word2vec）处理已确认的人力资本披露信息，开发了一个包含五类子类别的全面人力资本相关关键词列表，包括DEI；健康与安全；劳资关系和文化；薪酬和福利；人口统计学和其他。研究者可以利用我们的人力资本词汇表（或修改代码以捕捉其他感兴趣的构建模块）来分析公司通讯中涉及的人力资本问题，并提供了Python代码和示例来辅助使用或进一步细化BERT模型。最后讨论了与人力资本管理和披露相关未来研究的机会。

Conclusion: 研究分享了基于机器学习算法开发的人力资本词汇表及其可能的应用，并提供了在公司通讯中分析人力资本问题的方法，数据，和代码。不断进步的技术能够使未来的人力资本管理和披露研究受益。

Abstract: Human capital (HC) is increasingly important to corporate value creation.
Unlike other assets, however, HC is not currently subject to well-defined
measurement or disclosure rules. We use a machine learning algorithm (word2vec)
trained on a confirmed set of HC disclosures to develop a comprehensive list of
HC-related keywords classified into five subcategories (DEI; health and safety;
labor relations and culture; compensation and benefits; and demographics and
other) that capture the multidimensional nature of HC management. We share our
lexicon, corporate HC disclosures, and the Python code used to develop the
lexicon, and we provide detailed examples of using our data and code, including
for fine-tuning a BERT model. Researchers can use our HC lexicon (or modify the
code to capture another construct of interest) with their samples of corporate
communications to address pertinent HC questions. We close with a discussion of
future research opportunities related to HC management and disclosure.

</details>


### [11] [Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning Perspective](https://arxiv.org/abs/2506.10161)
*Yi Wang,Max Kreminski*

Main category: cs.CL

该论文通过使用计算叙事学的方法来评估大规模语言模型(LLMs)在叙事规划中的能力。实验表明，GPT-4级别的LLMs能够生成相对简单的因果连贯的故事，但对于带有角色动机和戏剧冲突的复杂叙事，仍需使用强化学习训练的模型来改进。结果对LLMs生成故事时的质量保持提出了见解。研究还指出这些模型在解决叙事问题时的有趣行为，并提供了游戏环境中应用LLMs叙事规划的挑战和考虑因素。


<details>
  <summary>Details</summary>
Motivation: 论文的动机在于深入研究大型语言模型在生成故事时的能力。尽管LLMs已经成为故事生成的热门应用，但对其生成高质量故事的能力理解仍然有限。这是因为自动评估方法存在挑战，手动评估的成本和主观性都很高。通过应用计算叙事学，论文希望解决这些问题，深入分析LLMs在叙事规划中的表现。

Method: 论文采用计算叙事学提供的基准方法来评估LLMs在叙事规划方面的能力。这一基准是基于文学中的具体例子来设计的，评估的维度包括因果一致性、角色动机和戏剧冲突。使用这个方法对LLMs进行实验，尤其是应用GPT-4级别的语言模型来生成和评估故事。为了处理更复杂的叙事问题，还考虑了使用强化学习方法训练LLMs。

Result: 实验结果显示，基于GPT-4的LLMs能够生成小规模的因果连贯的故事，但对于涉及角色动机和戏剧冲突的复杂叙事规划，仍旧是相当具有挑战性的。这表明，只有在应用了强化学习的LLMs才能够应对复杂的叙事推理任务。这项工作提供了有关LLMs能够维持故事质量的规模的见解。

Conclusion: 总体结论是，虽然GPT-4级别的LLMs在生成简单叙事方面表现出色，但在复杂的叙事规划方面仍旧面临着挑战，特别是要在有限的规模内维持因果一致性、角色动机和戏剧冲突。这为如何在游戏环境中有效使用LLMs进行叙事规划提供了重要的研究方向。

Abstract: Story generation has been a prominent application of Large Language Models
(LLMs). However, understanding LLMs' ability to produce high-quality stories
remains limited due to challenges in automatic evaluation methods and the high
cost and subjectivity of manual evaluation. Computational narratology offers
valuable insights into what constitutes a good story, which has been applied in
the symbolic narrative planning approach to story generation. This work aims to
deepen the understanding of LLMs' story generation capabilities by using them
to solve narrative planning problems. We present a benchmark for evaluating
LLMs on narrative planning based on literature examples, focusing on causal
soundness, character intentionality, and dramatic conflict. Our experiments
show that GPT-4 tier LLMs can generate causally sound stories at small scales,
but planning with character intentionality and dramatic conflict remains
challenging, requiring LLMs trained with reinforcement learning for complex
reasoning. The results offer insights on the scale of stories that LLMs can
generate while maintaining quality from different aspects. Our findings also
highlight interesting problem solving behaviors and shed lights on challenges
and considerations for applying LLM narrative planning in game environments.

</details>


### [12] [Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval](https://arxiv.org/abs/2506.10202)
*Shubhashis Roy Dipta,Francis Ferraro*

Main category: cs.CL

提出一种称为Q2E的Query-to-Event分解方法，用于零样本多语言的文本到视频检索，展示了通过利用大型语言模型（LLMs）和视觉语言模型（VLMs）来提升对复杂事件视频的理解和检索。实验结果显示该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在提高复杂现实事件相关视频的识别和检索能力，通过自动提取有关这些事件的潜在参数知识来实现。此外，研究还关注了零样本、多语言的文本到视频检索的跨数据集、跨域、跨模型的自适应性。

Method: 本研究提出了一种名为Q2E的Query-to-Event分解方法，用于零样本多语言文本到视频检索。该方法能够通过利用大型语言模型（LLMs）和视觉语言模型（VLMs）中嵌入的知识，来增强人类查询的理解。此外，该方法还被应用于视觉和语音输入的处理。为了整合不同模态的知识，采用了基于熵的融合评分方法来进行零样本融合。

Result: 通过对两个多样化数据集进行评估，并采用多种检索度量标准，结果显示Q2E方法超越了几种最先进的基线方法，并且结合音频信息可以显著提高文本到视频的检索性能。

Conclusion: 研究结果表明，Q2E方法能够有效地提升人类查询的理解且相对于现有方法具有显著优势。通过将音频信息整合，进一步提升了检索性能，并发布了代码和数据供未来研究使用。

Abstract: Recent approaches have shown impressive proficiency in extracting and
leveraging parametric knowledge from Large-Language Models (LLMs) and
Vision-Language Models (VLMs). In this work, we consider how we can improve the
identification and retrieval of videos related to complex real-world events by
automatically extracting latent parametric knowledge about those events. We
present Q2E: a Query-to-Event decomposition method for zero-shot multilingual
text-to-video retrieval, adaptable across datasets, domains, LLMs, or VLMs. Our
approach demonstrates that we can enhance the understanding of otherwise overly
simplified human queries by decomposing the query using the knowledge embedded
in LLMs and VLMs. We additionally show how to apply our approach to both visual
and speech-based inputs. To combine this varied multimodal knowledge, we adopt
entropy-based fusion scoring for zero-shot fusion. Through evaluations on two
diverse datasets and multiple retrieval metrics, we demonstrate that Q2E
outperforms several state-of-the-art baselines. Our evaluation also shows that
integrating audio information can significantly improve text-to-video
retrieval. We have released code and data for future research.

</details>


### [13] [TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and Novel Tic-Tac-Toe-style Games](https://arxiv.org/abs/2506.10209)
*Prakamya Mishra,Jiang Liu,Jialian Wu,Xiaodong Yu,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

本研究引入了一个新的基准测试 TTT-Bench，它通过四个两人玩的类似于井字游戏来评估大型推理模型在基本策略、空间和逻辑推理方面的能力。虽然这些游戏对人类来说非常简单，但这些模型在这些游戏中经常失败，即使它们在复杂的数学问题上表现出色。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于评估大型推理模型在STEM领域之外的任务中的推理能力。传统的推理基准测试主要集中在STEM领域，而这类模型在更广泛任务领域中的推理能力尚待探索。

Method: 通过设计一套四种两人玩的类似井字游戏的博弈来测试这些模型的推理能力。这些游戏要求模型理解对手意图以及棋盘的空间配置，以确保胜利。

Result: 测试结果显示，那些在困难数学问题上表现优异的模型，在这些简单的推理游戏中却经常失败。相较于MATH 500和AIME 2024测试，这些模型在TTT-Bench上的得分平均分别低了41%和5%。较大的模型虽然使用较短的推理轨迹取得了更高的成绩，但在TTT-Bench上的长期策略性推理场景中仍然挣扎。

Conclusion: 结论指出，虽然大型推理模型在某些STEM任务上表现出色，但在像TTT-Bench这样简单的非STEM推理任务中却表现不佳，揭示了这些模型在推理能力上的局限性。

Abstract: Large reasoning models (LRMs) have demonstrated impressive reasoning
capabilities across a broad range of tasks including Olympiad-level
mathematical problems, indicating evidence of their complex reasoning
abilities. While many reasoning benchmarks focus on the STEM domain, the
ability of LRMs to reason correctly in broader task domains remains
underexplored. In this work, we introduce \textbf{TTT-Bench}, a new benchmark
that is designed to evaluate basic strategic, spatial, and logical reasoning
abilities in LRMs through a suite of four two-player Tic-Tac-Toe-style games
that humans can effortlessly solve from a young age. We propose a simple yet
scalable programmatic approach for generating verifiable two-player game
problems for TTT-Bench. Although these games are trivial for humans, they
require reasoning about the intentions of the opponent, as well as the game
board's spatial configurations, to ensure a win. We evaluate a diverse set of
state-of-the-art LRMs, and \textbf{discover that the models that excel at hard
math problems frequently fail at these simple reasoning games}. Further testing
reveals that our evaluated reasoning models score on average $\downarrow$ 41\%
\& $\downarrow$ 5\% lower on TTT-Bench compared to MATH 500 \& AIME 2024
respectively, with larger models achieving higher performance using shorter
reasoning traces, where most of the models struggle on long-term strategic
reasoning situations on simple and new TTT-Bench tasks.

</details>


### [14] [Classifying Unreliable Narrators with Large Language Models](https://arxiv.org/abs/2506.10231)
*Anneliese Brei,Katharine Henry,Abhisheik Sharma,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.CL

本文使用计算方法来识别文本中无意误导信息的不可靠叙述者，构建了TUNa数据集，并展示了使用LLMs进行分类的潜力。


<details>
  <summary>Details</summary>
Motivation: 文章动机在于利用计算方法自动识别文本中的不可靠叙述者，为文本分析提供一个新的视角。

Method: 本文提出使用计算方法来识别不可靠的叙述者，即那些无意中误导信息的叙述者，并标注了一个名为TUNa的多领域叙述数据集。数据集包括博客文章、子版块文章、酒店评论和文学作品等。

Result: 研究结果表明，这项任务非常具有挑战性，但使用大型语言模型（LLMs）识别不可靠叙述者具有潜力。

Conclusion: 文章总结，借助文学理论进行不可靠叙述者的分类实验具有实际应用的潜力，并提供了专家标注的数据集和代码，邀请未来进一步研究。

Abstract: Often when we interact with a first-person account of events, we consider
whether or not the narrator, the primary speaker of the text, is reliable. In
this paper, we propose using computational methods to identify unreliable
narrators, i.e. those who unintentionally misrepresent information. Borrowing
literary theory from narratology to define different types of unreliable
narrators based on a variety of textual phenomena, we present TUNa, a
human-annotated dataset of narratives from multiple domains, including blog
posts, subreddit posts, hotel reviews, and works of literature. We define
classification tasks for intra-narrational, inter-narrational, and
inter-textual unreliabilities and analyze the performance of popular
open-weight and proprietary LLMs for each. We propose learning from literature
to perform unreliable narrator classification on real-world text data. To this
end, we experiment with few-shot, fine-tuning, and curriculum learning
settings. Our results show that this task is very challenging, and there is
potential for using LLMs to identify unreliable narrators. We release our
expert-annotated dataset and code and invite future research in this area.

</details>


### [15] [ToxSyn-PT: A Large-Scale Synthetic Dataset for Hate Speech Detection in Portuguese](https://arxiv.org/abs/2506.10245)
*Iago Alves Brito,Julia Soares Dollis,Fernanda Bufon Färber,Diogo Fernandes Costa Silva,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

本文介绍了ToxSyn-PT，一个针对葡萄牙语仇恨言论的合成语料库，该语料库包含了针对少数群体的53,274个合成句子。通过四阶段的合成过程，语料库在多个数据集上展示了良好的分类性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在提供一个大规模的葡萄牙语语料库，用于细粒度的仇恨言论分类，特别是针对九类受到法律保护的少数群体。

Method: 该研究采用了一个四阶段的管道来创建ToxSyn-PT语料库，包括手动策展的种子集、少量样本扩展、基于同义句的扩充以及加入额外的中性文本以防止模型过度拟合特定群体的标志性语言。

Result: 实验结果表明，该语料库在二元和多标签分类任务中均取得了良好表现，并在五种公开的葡萄牙语仇恨言论数据集中表现出较强的泛化能力，即使在领域边界也表现突出。

Conclusion: 该语料库丰富了合成数据的研究，并有助于在低资源环境下提高仇恨言论检测的技术发展。

Abstract: We present ToxSyn-PT, the first large-scale Portuguese corpus that enables
fine-grained hate-speech classification across nine legally protected minority
groups. The dataset contains 53,274 synthetic sentences equally distributed
between minorities groups and toxicity labels. ToxSyn-PT is created through a
novel four-stage pipeline: (1) a compact, manually curated seed; (2) few-shot
expansion with an instruction-tuned LLM; (3) paraphrase-based augmentation; and
(4) enrichment, plus additional neutral texts to curb overfitting to
group-specific cues. The resulting corpus is class-balanced, stylistically
diverse, and free from the social-media domain that dominate existing
Portuguese datasets. Despite domain differences with traditional benchmarks,
experiments on both binary and multi-label classification on the corpus yields
strong results across five public Portuguese hate-speech datasets,
demonstrating robust generalization even across domain boundaries. The dataset
is publicly released to advance research on synthetic data and hate-speech
detection in low-resource settings.

</details>


### [16] [Do Language Models Have Bayesian Brains? Distinguishing Stochastic and Deterministic Decision Patterns within Large Language Models](https://arxiv.org/abs/2506.10268)
*Andrea Yaoyun Cui,Pengfei Yu*

Main category: cs.CL

研究表明，语言模型在特定条件下能展现出近似确定性的决策行为，这挑战了它们遵循概率决策的传统假设，并影响了之前基于模拟Gibbs采样推导模型偏好的有效性。


<details>
  <summary>Details</summary>
Motivation: 回顾了“语言模型是否拥有贝叶斯大脑”这一核心问题，因为这一问题对建立准确的模型内部决策理解至关重要。

Method: 通过模拟Gibbs采样来分析语言模型的决策模式，特别是探讨语言模型是否具备贝叶斯决策机制，即它们在生成序列时是否如采样未知分布一样做出概率性决策。

Result: 实验发现，在特定条件下，语言模型能够表现出几乎确定性的决策行为，即便采样温度不为零，这挑战了之前的采样假设，揭示了之前的模拟人类偏好的方法可能失效。

Conclusion: 研究提出了一种简单的方法来区分Gibbs采样中的随机与确定性决策模式，有助于防止误判语言模型的偏好，并为理解大型语言模型的决策方式提供了关键见解。

Abstract: Language models are essentially probability distributions over token
sequences. Auto-regressive models generate sentences by iteratively computing
and sampling from the distribution of the next token. This iterative sampling
introduces stochasticity, leading to the assumption that language models make
probabilistic decisions, similar to sampling from unknown distributions.
Building on this assumption, prior research has used simulated Gibbs sampling,
inspired by experiments designed to elicit human priors, to infer the priors of
language models. In this paper, we revisit a critical question: Do language
models possess Bayesian brains? Our findings show that under certain
conditions, language models can exhibit near-deterministic decision-making,
such as producing maximum likelihood estimations, even with a non-zero sampling
temperature. This challenges the sampling assumption and undermines previous
methods for eliciting human-like priors. Furthermore, we demonstrate that
without proper scrutiny, a system with deterministic behavior undergoing
simulated Gibbs sampling can converge to a "false prior." To address this, we
propose a straightforward approach to distinguish between stochastic and
deterministic decision patterns in Gibbs sampling, helping to prevent the
inference of misleading language model priors. We experiment on a variety of
large language models to identify their decision patterns under various
circumstances. Our results provide key insights in understanding decision
making of large language models.

</details>


### [17] [ClusterUCB: Efficient Gradient-Based Data Selection for Targeted Fine-Tuning of LLMs](https://arxiv.org/abs/2506.10288)
*Zige Wang,Qi Zhu,Fei Mi,Minghui Xu,Ruochun Jin,Wenjing Yang*

Main category: cs.CL

提出了一个结合聚类和改进的上置信界算法的高效基于梯度的数据选择框架ClusterUCB，实验证明其计算成本更低，结果与原始梯度方法相当。


<details>
  <summary>Details</summary>
Motivation: 原有的基于梯度的数据选择方法计算成本太高，不可行。

Method: 首先对训练数据进行聚类，然后采用改进的上置信界算法解决数据选择问题，利用历史数据影响信息来估计每个簇的分布。

Result: 实验表明，ClusterUCB框架在降低计算消耗的同时，可以获得与原始梯度方法相当的结果。

Conclusion: 研究表明，ClusterUCB框架能够在显著减少计算成本的同时，保持数据选择的有效性。

Abstract: Gradient-based data influence approximation has been leveraged to select
useful data samples in the supervised fine-tuning of large language models.
However, the computation of gradients throughout the fine-tuning process
requires too many resources to be feasible in practice. In this paper, we
propose an efficient gradient-based data selection framework with clustering
and a modified Upper Confidence Bound (UCB) algorithm. Based on the intuition
that data samples with similar gradient features will have similar influences,
we first perform clustering on the training data pool. Then, we frame the
inter-cluster data selection as a constrained computing budget allocation
problem and consider it a multi-armed bandit problem. A modified UCB algorithm
is leveraged to solve this problem. Specifically, during the iterative sampling
process, historical data influence information is recorded to directly estimate
the distributions of each cluster, and a cold start is adopted to balance
exploration and exploitation. Experimental results on various benchmarks show
that our proposed framework, ClusterUCB, can achieve comparable results to the
original gradient-based data selection methods while greatly reducing computing
consumption.

</details>


### [18] [Flick: Few Labels Text Classification using K-Aware Intermediate Learning in Multi-Task Low-Resource Languages](https://arxiv.org/abs/2506.10292)
*Ali Almutairi,Abdullah Alsuhaibani,Shoaib Jameel,Usman Naseem,Gelareh Mohammadi,Imran Razzak*

Main category: cs.CL

Flick 是一种新的方法，用于改进低资源语言环境下少标签文本分类中伪标签的质量，能有效减少错误传播并增强预训练语言模型的微调效果。


<details>
  <summary>Details</summary>
Motivation: 解决少样本文本分类问题，特别是在真正低资源语言环境中的噪音伪标签和领域适应挑战。现有方法在这些方面通常表现不佳。

Method: Flick 提出了一种新颖的伪标签精炼组件，通过专注于单个聚类的内聚性和自适应的 top-k 选择机制，从初始广泛集的高置信度伪标签中提取高度可靠的伪标签，以改进传统伪标签策略。

Result: Flick 在涵盖阿拉伯语、乌尔都语、塞茨瓦纳语等多种低资源语言以及英语在内的14个数据集上展示了其卓越的性能和适用性。

Conclusion: Flick 通过精炼伪标签，提升了少样本文本分类在低资源语言环境中的性能，证明了它在多种语言上的适应性和优越性。

Abstract: Training deep learning networks with minimal supervision has gained
significant research attention due to its potential to reduce reliance on
extensive labelled data. While self-training methods have proven effective in
semi-supervised learning, they remain vulnerable to errors from noisy pseudo
labels. Moreover, most recent approaches to the few-label classification
problem are either designed for resource-rich languages such as English or
involve complex cascading models that are prone to overfitting. To address the
persistent challenge of few-label text classification in truly low-resource
linguistic contexts, where existing methods often struggle with noisy
pseudo-labels and domain adaptation, we propose Flick. Unlike prior methods
that rely on generic multi-cluster pseudo-labelling or complex cascading
architectures, Flick leverages the fundamental insight that distilling
high-confidence pseudo-labels from a broader set of initial clusters can
dramatically improve pseudo-label quality, particularly for linguistically
diverse, low-resource settings. Flick introduces a novel pseudo-label
refinement component, a departure from traditional pseudo-labelling strategies
by identifying and leveraging top-performing pseudo-label clusters. This
component specifically learns to distil highly reliable pseudo-labels from an
initial broad set by focusing on single-cluster cohesion and leveraging an
adaptive top-k selection mechanism. This targeted refinement process is crucial
for mitigating the propagation of errors inherent in low-resource data,
allowing for robust fine-tuning of pre-trained language models with only a
handful of true labels. We demonstrate Flick's efficacy across 14 diverse
datasets, encompassing challenging low-resource languages such as Arabic, Urdu,
and Setswana, alongside English, showcasing its superior performance and
adaptability.

</details>


### [19] ["Check My Work?": Measuring Sycophancy in a Simulated Educational Context](https://arxiv.org/abs/2506.10297)
*Chuck Arvin*

Main category: cs.CL

研究显示，在模拟教育环境中，大型语言模型（LLMs）对用户建议的响应存在显著偏见，这种偏见在较小的模型中影响更大，对教育公平性造成潜在威胁。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨LLMs在教育中的应用时，如何避免由于顺从学生意见而引起的偏见风险，这种现象可能会加剧教育不公。

Method: 本研究通过在模拟教育环境中测试来自OpenAI的GPT-4o和GPT-4.1模型系列的五种不同大型语言模型（LLMs），分析用户提供的建议如何影响这些模型。研究考虑了五种不同的实验条件。

Result: 研究结果显示，基于查询的措辞，LLM的响应质量差异显著。错误答案的提示会使其正确率下降高达15%，而正确答案的提示则会提升同等的正确率。这种偏见在较小模型中更为明显，例如GPT-4.1-nano模型的影响可达30%，而GPT-4o仅为8%。LLMs往往会改变答案以迎合学生提到的答案选择。

Conclusion: 研究揭示了在教育场景中需要更深入理解并缓解此类偏见，并强调LLMs可能对知识丰富的学生有所助益，而对知识不足的学生则可能进一步固化误解。

Abstract: This study examines how user-provided suggestions affect Large Language
Models (LLMs) in a simulated educational context, where sycophancy poses
significant risks. Testing five different LLMs from the OpenAI GPT-4o and
GPT-4.1 model classes across five experimental conditions, we show that
response quality varies dramatically based on query framing. In cases where the
student mentions an incorrect answer, the LLM correctness can degrade by as
much as 15 percentage points, while mentioning the correct answer boosts
accuracy by the same margin. Our results also show that this bias is stronger
in smaller models, with an effect of up to 30% for the GPT-4.1-nano model,
versus 8% for the GPT-4o model. Our analysis of how often LLMs "flip" their
answer, and an investigation into token level probabilities, confirm that the
models are generally changing their answers to answer choices mentioned by
students in line with the sycophancy hypothesis. This sycophantic behavior has
important implications for educational equity, as LLMs may accelerate learning
for knowledgeable students while the same tools may reinforce misunderstanding
for less knowledgeable students. Our results highlight the need to better
understand the mechanism, and ways to mitigate, such bias in the educational
context.

</details>


### [20] [Scheduled Interleaved Speech-Text Training for Speech-to-Speech Translation with LLMs](https://arxiv.org/abs/2506.10299)
*Hayato Futami,Emiru Tsunoo,Yosuke Kashiwagi,Yuki Ito,Hassan Shahmohammadi,Siddhant Arora,Shinji Watanabe*

Main category: cs.CL

Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Speech-to-speech translation (S2ST) has been advanced with large language
models (LLMs), which are fine-tuned on discrete speech units. In such
approaches, modality adaptation from text to speech has been an issue. LLMs are
trained on text-only data, which presents challenges to adapt them to speech
modality with limited speech-to-speech data. To address the training
difficulty, we propose scheduled interleaved speech--text training in this
study. We use interleaved speech--text units instead of speech units during
training, where aligned text tokens are interleaved at the word level. We
gradually decrease the ratio of text as training progresses, to facilitate
progressive modality adaptation from text to speech. We conduct experimental
evaluations by fine-tuning LLaMA3.2-1B for S2ST on the CVSS dataset. We show
that the proposed method consistently improves the translation performances,
especially for languages with limited training data.

</details>


### [21] [Code Execution as Grounded Supervision for LLM Reasoning](https://arxiv.org/abs/2506.10343)
*Dongwon Jung,Wenxuan Zhou,Muhao Chen*

Main category: cs.CL

This paper introduces a scalable approach to generate high-quality chain-of-thought (CoT) data for enhancing large language models' (LLMs) reasoning capabilities by using accurate, step-by-step reasoning traces extracted from program execution, reducing the need for costly human annotations or unreliable LLM-generated CoT.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this method is to address the challenge of acquiring reliable and accurate CoT supervision data. Current methods, which primarily depend on human annotations or LLM-generated CoT, are either costly or prone to errors. This scalable method seeks to offer a more reliable and efficient solution.

Method: We propose a scalable method that generates high-quality chain-of-thought (CoT) supervision data for large language models (LLMs) by using the determinism of program execution. This approach extracts verifiable reasoning traces from code and converts them into natural language CoT, aiming to enhance the LLMs' reasoning abilities.

Result: Experiments on a variety of reasoning benchmarks across different domains demonstrated that this method significantly improves LLMs' transferable reasoning abilities. Additionally, ablation studies confirmed the high accuracy of the reasoning data generated, and the method also helps in reducing meaningless repetition and overthinking.

Conclusion: The proposed method presents an innovative approach to generating reliable CoT supervision data for LLMs, leveraging the precise nature of program execution. It not only improves LLMs' ability to reason effectively across various tasks but also results in data that reduces token length and unnecessary repetition, enhancing efficiency.

Abstract: Training large language models (LLMs) with chain-of-thought (CoT) supervision
has proven effective for enhancing their reasoning abilities. However,
obtaining reliable and accurate reasoning supervision remains a significant
challenge. We propose a scalable method for generating a high-quality CoT
supervision dataset by leveraging the determinism of program execution. Unlike
existing reasoning dataset generation methods that rely on costly human
annotations or error-prone LLM-generated CoT, our approach extracts verifiable,
step-by-step reasoning traces from code execution and transforms them into a
natural language CoT reasoning. Experiments on reasoning benchmarks across
various domains show that our method effectively equips LLMs with transferable
reasoning abilities across diverse tasks. Furthermore, the ablation studies
validate that our method produces highly accurate reasoning data and reduces
overall token length during inference by reducing meaningless repetition and
overthinking.

</details>


### [22] [TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning](https://arxiv.org/abs/2506.10380)
*Xiaohan Yu,Pu Jian,Chong Chen*

Main category: cs.CL

本文提出了一种新的框架TableRAG，设计用于处理包含文本和表格组件的异构文档，以解决现有RAG方法在处理此类文档时的局限性，实验结果展示了新框架的优越性能。


<details>
  <summary>Details</summary>
Motivation: 由于RAG方法在处理异构文档时表现出了关键的限制，即展平表格和分段策略会破坏表格的内在结构，导致信息丢失，并损害多跳、全局查询的推理能力。

Method: 该框架分为四个步骤操作：查询的上下文敏感分解、文本检索、SQL编程和执行，以及组合中间答案生成。

Result: 实验结果表明，TableRAG在公共数据集和新开发的HeteQA上都持续优于现有基线，确立了异构文档问答的新标准。

Conclusion: 通过TableRAG的开发和实验评估，作者展示了其在处理包括文本和表格组件的异构文档时的优越性，特别是在多跳异构推理方面。

Abstract: Retrieval-Augmented Generation (RAG) has demonstrated considerable
effectiveness in open-domain question answering. However, when applied to
heterogeneous documents, comprising both textual and tabular components,
existing RAG approaches exhibit critical limitations. The prevailing practice
of flattening tables and chunking strategies disrupts the intrinsic tabular
structure, leads to information loss, and undermines the reasoning capabilities
of LLMs in multi-hop, global queries. To address these challenges, we propose
TableRAG, an hybrid framework that unifies textual understanding and complex
manipulations over tabular data. TableRAG iteratively operates in four steps:
context-sensitive query decomposition, text retrieval, SQL programming and
execution, and compositional intermediate answer generation. We also develop
HeteQA, a novel benchmark designed to evaluate the multi-hop heterogeneous
reasoning capabilities. Experimental results demonstrate that TableRAG
consistently outperforms existing baselines on both public datasets and our
HeteQA, establishing a new state-of-the-art for heterogeneous document question
answering. We release TableRAG at https://github.com/yxh-y/TableRAG/tree/main.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [23] [Multimodal Cinematic Video Synthesis Using Text-to-Image and Audio Generation Models](https://arxiv.org/abs/2506.10005)
*Sridhar S,Nithin A,Shakeel Rifath,Vasantha Raj*

Main category: cs.CV

论文提出了一种利用Stable Diffusion、GPT-2和音频技术自动生成高质感60秒电影的方法，在实验中显示出优越的视觉和叙述性能。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能的进步，自动从文本输入合成电影视频成为可能，这项工作旨在改进多媒体创作技术。

Method: 该研究结合了Stable Diffusion用于高质量图像合成、GPT-2用于叙事构建以及gTTS和YouTube音频源的混合音频管道，创建了一个五场景框架，并通过线性帧插值、电影级后处理和音视频同步技术制成了专业级60秒电影。

Result: 实验显示了出色的视觉质量、叙述连贯性和效率，进一步推进了文本到视频合成技术在创意、教育和工业应用中的应用。

Conclusion: 该方法证明了在GPU加速的Google Colab环境中使用Python 3.11和CUDA内存管理，能够可靠地生成高质量的60秒电影视频，支持高达1024x768分辨率和15-30 FPS帧率。

Abstract: Advances in generative artificial intelligence have altered multimedia
creation, allowing for automatic cinematic video synthesis from text inputs.
This work describes a method for creating 60-second cinematic movies
incorporating Stable Diffusion for high-fidelity image synthesis, GPT-2 for
narrative structuring, and a hybrid audio pipeline using gTTS and
YouTube-sourced music. It uses a five-scene framework, which is augmented by
linear frame interpolation, cinematic post-processing (e.g., sharpening), and
audio-video synchronization to provide professional-quality results. It was
created in a GPU-accelerated Google Colab environment using Python 3.11. It has
a dual-mode Gradio interface (Simple and Advanced), which supports resolutions
of up to 1024x768 and frame rates of 15-30 FPS. Optimizations such as CUDA
memory management and error handling ensure reliability. The experiments
demonstrate outstanding visual quality, narrative coherence, and efficiency,
furthering text-to-video synthesis for creative, educational, and industrial
applications.

</details>


### [24] [LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning](https://arxiv.org/abs/2506.10082)
*Chenjian Gao,Lihe Ding,Xin Cai,Zhanpeng Huang,Zibin Wang,Tianfan Xue*

Main category: cs.CV

This paper introduces a mask-based LoRA tuning method that improves the flexibility and adaptability of video editing by modifying pre-trained image-to-video models.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this method is to overcome the limitations of existing video editing techniques, which often require large-scale pretraining and lack the flexibility needed for specific edits. The proposed method aims to provide a more adaptable and efficient editing solution.

Method: Our approach involves a mask-based LoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video (I2V) models to allow for flexible video editing. This method preserves the background while enabling controlled edits to propagate through the video. It uses spatial masks to guide the learning process, drawing attention to different sources - the input video for spatial structure and motion, and reference images for appearance.

Result: The experimental results demonstrate that the proposed method achieves superior performance in video editing compared to state-of-the-art techniques.

Conclusion: The conclusion is that the mask-based LoRA tuning method effectively addresses the need for more flexible and adaptable video editing without altering the underlying model architecture, providing a robust solution for video editing tasks.

Abstract: Video editing using diffusion models has achieved remarkable results in
generating high-quality edits for videos. However, current methods often rely
on large-scale pretraining, limiting flexibility for specific edits.
First-frame-guided editing provides control over the first frame, but lacks
flexibility over subsequent frames. To address this, we propose a mask-based
LoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video
(I2V) models for flexible video editing. Our approach preserves background
regions while enabling controllable edits propagation. This solution offers
efficient and adaptable video editing without altering the model architecture.
To better steer this process, we incorporate additional references, such as
alternate viewpoints or representative scene states, which serve as visual
anchors for how content should unfold. We address the control challenge using a
mask-driven LoRA tuning strategy that adapts a pre-trained image-to-video model
to the editing context. The model must learn from two distinct sources: the
input video provides spatial structure and motion cues, while reference images
offer appearance guidance. A spatial mask enables region-specific learning by
dynamically modulating what the model attends to, ensuring that each area draws
from the appropriate source. Experimental results show our method achieves
superior video editing performance compared to state-of-the-art methods.

</details>


### [25] [DeepTraverse: A Depth-First Search Inspired Network for Algorithmic Visual Understanding](https://arxiv.org/abs/2506.10084)
*Bin Guo,John H. L. Hansen*

Main category: cs.CV

论文介绍了受经典搜索算法启发的DeepTraverse架构，该架构通过递归和自适应的方式智能化地构建特征，通过多组图像分类基准测试展示了其高度竞争力的表现。


<details>
  <summary>Details</summary>
Motivation: 论文动机在于改进传统的视觉骨干网络，这些网络通过一系列相似的操作构建特征，缺乏自适应迭代改进的能力。作者希望通过引入经典搜索算法的原则，使网络能够进行更加结构化和逻辑化的特征处理。

Method: 该论文介绍了DeepTraverse，这是一种新颖的视觉架构，受到经典搜索算法的启发。DeepTraverse通过递归探索模块和自适应校准模块相结合的方式，系统地深化特征分析并根据全局上下文动态调整特征的重要性，实现了特征模式的智能构建和优化。

Result: 实验结果表明，与具有相似参数数量的传统模型相比，DeepTraverse在图像分类基准测试上能够实现高度准确的分类和强大的特征区分能力，并且经常优于这些模型。

Conclusion: 这项研究证明，通过将算法先验知识整合进入视觉骨干，可以作为一种建立更有效、性能更好且结构更清晰视觉骨干的新方法。

Abstract: Conventional vision backbones, despite their success, often construct
features through a largely uniform cascade of operations, offering limited
explicit pathways for adaptive, iterative refinement. This raises a compelling
question: can principles from classical search algorithms instill a more
algorithmic, structured, and logical processing flow within these networks,
leading to representations built through more interpretable, perhaps
reasoning-like decision processes? We introduce DeepTraverse, a novel vision
architecture directly inspired by algorithmic search strategies, enabling it to
learn features through a process of systematic elucidation and adaptive
refinement distinct from conventional approaches. DeepTraverse operationalizes
this via two key synergistic components: recursive exploration modules that
methodically deepen feature analysis along promising representational paths
with parameter sharing for efficiency, and adaptive calibration modules that
dynamically adjust feature salience based on evolving global context. The
resulting algorithmic interplay allows DeepTraverse to intelligently construct
and refine feature patterns. Comprehensive evaluations across a diverse suite
of image classification benchmarks show that DeepTraverse achieves highly
competitive classification accuracy and robust feature discrimination, often
outperforming conventional models with similar or larger parameter counts. Our
work demonstrates that integrating such algorithmic priors provides a
principled and effective strategy for building more efficient, performant, and
structured vision backbones.

</details>


### [26] [Test-Time Adaptation for Generalizable Task Progress Estimation](https://arxiv.org/abs/2506.10085)
*Christos Ziakas,Alessandra Russo*

Main category: cs.CV

提出了一种新的测试时适应方法，使用元学习来提高模型对不同任务的适应能力，特别是在进度估计方面超越了现有的视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于提高模型在不同视觉和时间上下文中的适应能力，以改进进度估计的准确性。

Method: 我们提出了一种测试时适应方法，该方法使进度估计模型能够通过优化学习的自监督目标来在线适应测试轨迹的视觉和时间上下文。为此，我们引入了一种基于梯度的元学习策略，以使用专家视觉轨迹及其自然语言任务描述来训练模型，从而使测试时适应能够依靠语义内容而不是时间顺序来提高进度估计。

Result: 我们的测试时适应方法能够从单一训练环境泛化到多样化的出分布任务、环境和实现，优于使用自回归视觉语言模型的上下文学习方法。

Conclusion: 研究表明，基于元学习的测试时适应策略能够在多样化的任务和环境中提高模型的性能，并且优于目前最先进的上下文学习方法。

Abstract: We propose a test-time adaptation method that enables a progress estimation
model to adapt online to the visual and temporal context of test trajectories
by optimizing a learned self-supervised objective. To this end, we introduce a
gradient-based meta-learning strategy to train the model on expert visual
trajectories and their natural language task descriptions, such that test-time
adaptation improves progress estimation relying on semantic content over
temporal order. Our test-time adaptation method generalizes from a single
training environment to diverse out-of-distribution tasks, environments, and
embodiments, outperforming the state-of-the-art in-context learning approach
using autoregressive vision-language models.

</details>


### [27] [EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models](https://arxiv.org/abs/2506.10100)
*Yantai Yang,Yuhao Wang,Zichen Wen,Luo Zhongwei,Chang Zou,Zhipeng Zhang,Chuan Wen,Linfeng Zhang*

Main category: cs.CV

本论文介绍了一种名为EfficientVLA的系统性加速框架，通过三个策略消除冗余以提高VLA模型的推理效率。


<details>
  <summary>Details</summary>
Motivation: 现存的加速方法往往只针对单一的低效率问题，不能全面解决整个VLA管道中的计算和内存瓶颈，限制了模型的实际应用。因此，本论文旨在提出一种系统而不依赖于训练的加速解决方案。

Method: 本论文提出了一种名为EfficientVLA的加速框架，该框架通过系统地消除冗余来加速Vision-Language-Action (VLA) 模型的推理过程。具体包括三个策略：（1）去除语言模块中功能不重要的层；（2）优化视觉处理路径，选择一组精简且多样化的视觉令牌；（3）在迭代扩散的动作头中缓存和重用关键中间特征，以减轻时间上的计算冗余。

Result: 应用EfficientVLA方法到标准的VLA模型CogACT上，推理速度提升了1.93倍，计算量减少了71.1%，同时在SIMPLER基准测试中的成功率仅下降了0.6%。

Conclusion: EfficientVLA证明了其在加速VLA模型推理的同时，能够保持较高的性能，这对于推动基于扩散架构的VLA模型的实际应用具有重要意义。

Abstract: Vision-Language-Action (VLA) models, particularly diffusion-based
architectures, demonstrate transformative potential for embodied intelligence
but are severely hampered by high computational and memory demands stemming
from extensive inherent and inference-time redundancies. While existing
acceleration efforts often target isolated inefficiencies, such piecemeal
solutions typically fail to holistically address the varied computational and
memory bottlenecks across the entire VLA pipeline, thereby limiting practical
deployability. We introduce EfficientVLA, a structured and training-free
inference acceleration framework that systematically eliminates these barriers
by cohesively exploiting multifaceted redundancies. EfficientVLA
synergistically integrates three targeted strategies: (1) pruning of
functionally inconsequential layers from the language module, guided by an
analysis of inter-layer redundancies; (2) optimizing the visual processing
pathway through a task-aware strategy that selects a compact, diverse set of
visual tokens, balancing task-criticality with informational coverage; and (3)
alleviating temporal computational redundancy within the iterative
diffusion-based action head by strategically caching and reusing key
intermediate features. We apply our method to a standard VLA model CogACT,
yielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%
success rate drop in the SIMPLER benchmark.

</details>


### [28] [A Manually Annotated Image-Caption Dataset for Detecting Children in the Wild](https://arxiv.org/abs/2506.10117)
*Klim Kireev,Ana-Maria Creţu,Raphael Meier,Sarah Adel Bargal,Elissa Redmiles,Carmela Troncoso*

Main category: cs.CV

研究者创建了ICWCD数据集，包含10,000对图像和标题，用于检测未成年人，结果显示检测未成年人是项挑战。


<details>
  <summary>Details</summary>
Motivation: Content中的动机在于平台和法律对于涉及未成年人的数字内容有特别的监管措施，但是由于所需评估的内容量巨大，因此需要机器学习自动化的工具进行检测。现有研究缺乏针对此类检测的多模态环境下的数据集。

Method: Content中提出的方法是建立了一个名为Image-Caption Children in the Wild Dataset (ICCWD)的数据集，这个数据集包含了10,000个图像-标题对，旨在用来检测图像中是否包含未成年人的基准测试。

Result: 通过使用ICCWD数据集测试了三个不同的检测器，包括一个商业年龄评估系统，结果显示最好的方法达到了75.3%的真正例率，这说明未成年人检测是一个具有挑战性的任务。

Conclusion: 研究者们希望该数据集ICWCD的发布能有助于设计更好的未成年人检测方法，适用于各种情况。

Abstract: Platforms and the law regulate digital content depicting minors (defined as
individuals under 18 years of age) differently from other types of content.
Given the sheer amount of content that needs to be assessed, machine
learning-based automation tools are commonly used to detect content depicting
minors. To our knowledge, no dataset or benchmark currently exists for
detecting these identification methods in a multi-modal environment. To fill
this gap, we release the Image-Caption Children in the Wild Dataset (ICCWD), an
image-caption dataset aimed at benchmarking tools that detect depictions of
minors. Our dataset is richer than previous child image datasets, containing
images of children in a variety of contexts, including fictional depictions and
partially visible bodies. ICCWD contains 10,000 image-caption pairs manually
labeled to indicate the presence or absence of a child in the image. To
demonstrate the possible utility of our dataset, we use it to benchmark three
different detectors, including a commercial age estimation system applied to
images. Our results suggest that child detection is a challenging task, with
the best method achieving a 75.3% true positive rate. We hope the release of
our dataset will aid in the design of better minor detection methods in a wide
range of scenarios.

</details>


### [29] [Detecção da Psoríase Utilizando Visão Computacional: Uma Abordagem Comparativa Entre CNNs e Vision Transformers](https://arxiv.org/abs/2506.10119)
*Natanael Lucena,Fábio S. da Silva,Ricardo Rios*

Main category: cs.CV

ViTs, notably the DaViT-B model, outperform CNNs in psoriasis detection with an f1-score of 96.4%, making them a promising tool for medical image classification.


<details>
  <summary>Details</summary>
Motivation: To investigate whether ViTs can outperform CNNs in the specific domain of psoriasis and similar diseases image classification with comparable or smaller model size.

Method: This paper compares the performance of CNNs and ViTs in classifying images of psoriasis lesions and similar diseases. The models were pre-trained on ImageNet and then fine-tuned on a specific dataset.

Result: ViTs demonstrated better performance with smaller models. The DaViT-B model achieved the highest f1-score of 96.4%.

Conclusion: ViTs, particularly DaViT-B, are efficient for automated psoriasis detection, highlighting the potential of ViTs in medical image classification tasks.

Abstract: This paper presents a comparison of the performance of Convolutional Neural
Networks (CNNs) and Vision Transformers (ViTs) in the task of multi-classifying
images containing lesions of psoriasis and diseases similar to it. Models
pre-trained on ImageNet were adapted to a specific data set. Both achieved high
predictive metrics, but the ViTs stood out for their superior performance with
smaller models. Dual Attention Vision Transformer-Base (DaViT-B) obtained the
best results, with an f1-score of 96.4%, and is recommended as the most
efficient architecture for automated psoriasis detection. This article
reinforces the potential of ViTs for medical image classification tasks.

</details>


### [30] [ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs](https://arxiv.org/abs/2506.10128)
*Xiyao Wang,Zhengyuan Yang,Chao Feng,Yongyuan Liang,Yuhang Zhou,Xiaoyu Liu,Ziyi Zang,Ming Li,Chung-Ching Lin,Kevin Lin,Linjie Li,Furong Huang,Lijuan Wang*

Main category: cs.CV

通过新引入的ViCrit任务，研究人员成功提升了视觉-语言模型的视觉感知能力，实现了视觉幻觉细节的定位，并在多种基准测试中展示了其有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏既具有挑战性又易于验证的视觉相关任务，阻碍了强化学习在视觉-语言模型中的成功应用。为了解决这一问题，引入ViCrit任务，旨在提升模型的视觉感知能力。

Method: 通过引入ViCrit（视觉描述幻想评判者）任务，利用强化学习（RL）对视觉-语言模型（VLMs）进行训练，该任务要求模型在人类撰写的图像描述中定位注入的细微、合成的视觉幻觉。具体来说，从200字的描述开始，注入一个细微的视觉描述错误，通过改变一些词语来描述对象、属性、数量或空间关系，任务是模型在给定图像和修改后的描述下找出受损的段落。

Result: 经过ViCrit任务训练的模型在多种视觉-语言基准测试中显示出显著提升，并能将其提升应用到抽象图像推理和视觉数学，表明模型学会了感知而不是简单地记忆已见过的对象。此外，还引入了ViCrit-Bench作为一个类别平衡的诊断基准，系统地探索了不同图像领域和错误类型的感知错误。

Conclusion: 细粒度的幻觉批评是一种有效的目标，能增强视觉-语言模型中的视觉感知能力。

Abstract: Reinforcement learning (RL) has shown great effectiveness for fine-tuning
large language models (LLMs) using tasks that are challenging yet easily
verifiable, such as math reasoning or code generation. However, extending this
success to visual perception in vision-language models (VLMs) has been impeded
by the scarcity of vision-centric tasks that are simultaneously challenging and
unambiguously verifiable. To this end, we introduce ViCrit (Visual Caption
Hallucination Critic), an RL proxy task that trains VLMs to localize a subtle,
synthetic visual hallucination injected into paragraphs of human-written image
captions. Starting from a 200-word captions, we inject a single, subtle visual
description error-altering a few words on objects, attributes, counts, or
spatial relations-and task the model to pinpoint the corrupted span given the
image and the modified caption. This formulation preserves the full perceptual
difficulty while providing a binary, exact-match reward that is easy to compute
and unambiguous. Models trained with the ViCrit Task exhibit substantial gains
across a variety of VL benchmarks. Crucially, the improvements transfer beyond
natural-image training data to abstract image reasoning and visual math,
showing promises of learning to perceive rather than barely memorizing seen
objects. To facilitate evaluation, we further introduce ViCrit-Bench, a
category-balanced diagnostic benchmark that systematically probes perception
errors across diverse image domains and error types. Together, our results
demonstrate that fine-grained hallucination criticism is an effective and
generalizable objective for enhancing visual perception in VLMs.

</details>


### [31] [RoCA: Robust Cross-Domain End-to-End Autonomous Driving](https://arxiv.org/abs/2506.10145)
*Rajeev Yasarla,Shizhong Han,Hsin-Pai Cheng,Litian Liu,Shweta Mahajan,Apratim Bhattacharyya,Yunxiao Shi,Risheek Garrepalli,Hong Cai,Fatih Porikli*

Main category: cs.CV

提出RoCA框架以提升跨域端到端自动驾驶的性能，改进自动驾驶模型的泛化能力及适应性，提升在不同驾驶环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决跨域部署过程中遇到的挑战，尤其是利用大型语言模型（LLMs）进行跨域自动驾驶表现不佳且需要高昂的重训练成本问题。

Method: RoCA框架通过高斯过程学习一组基词汇和对应的轨迹，这组基词汇能够涵盖多种驾驶场景。在给定任何驾驶场景下，RoCA能概率性地推测未来轨迹。

Result: 结合RoCA与基线的E2E模型在源域中进行训练，改进了基线模型的泛化能力，同时在新的目标域中表现出稳健的适应性，表现优于直接微调的方法。

Conclusion: RoCA能够实现强的领域泛化和适应性表现，适用于多种跨域驾驶场景，证明了其在提升自动驾驶模型跨域性能中的有效性。

Abstract: End-to-end (E2E) autonomous driving has recently emerged as a new paradigm,
offering significant potential. However, few studies have looked into the
practical challenge of deployment across domains (e.g., cities). Although
several works have incorporated Large Language Models (LLMs) to leverage their
open-world knowledge, LLMs do not guarantee cross-domain driving performance
and may incur prohibitive retraining costs during domain adaptation. In this
paper, we propose RoCA, a novel framework for robust cross-domain E2E
autonomous driving. RoCA formulates the joint probabilistic distribution over
the tokens that encode ego and surrounding vehicle information in the E2E
pipeline. Instantiating with a Gaussian process (GP), RoCA learns a set of
basis tokens with corresponding trajectories, which span diverse driving
scenarios. Then, given any driving scene, it is able to probabilistically infer
the future trajectory. By using RoCA together with a base E2E model in
source-domain training, we improve the generalizability of the base model,
without requiring extra inference computation. In addition, RoCA enables robust
adaptation on new target domains, significantly outperforming direct
finetuning. We extensively evaluate RoCA on various cross-domain scenarios and
show that it achieves strong domain generalization and adaptation performance.

</details>


### [32] [SPARKE: Scalable Prompt-Aware Diversity Guidance in Diffusion Models via RKE Score](https://arxiv.org/abs/2506.10173)
*Mohammad Jalali,Haoyu Lei,Amin Gohari,Farzan Farnia*

Main category: cs.CV

研究提出了SPARKE方法，利用条件熵进行多样性和提示感知的动态多样性测量，通过条件潜在RKE得分引导，实现了在大规模生成设置中有效控制多样性的目标。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型取得了显著的成功，但在保证提示引导生成模型所生成样本的充分多样性方面仍面临挑战，特别是需要在语义相似的提示下提示感知地评估生成数据的多样性。

Method: SPARKE方法利用条件熵进行多样性引导，它根据相似提示动态调整多样性测量，并允许提示感知的多样性控制。为了减少计算负担，提出了条件潜在RKE得分引导，使得计算复杂度从普遍熵度量的$O(n^3)$减少到$O(n)$，支持潜在数千次分别针对不同提示进行多样性和引导采样。

Result: 数值实验表明，所提出的SPARKE方法能有效提升文本到图像扩散模型生成数据的提示感知多样性，实现了计算成本与多样性间的平衡。

Conclusion: 实验证明，相较于现有方法，SPARKE方法在提升生成数据的提示感知多样性的同时，并未显著增加计算成本。

Abstract: Diffusion models have demonstrated remarkable success in high-fidelity image
synthesis and prompt-guided generative modeling. However, ensuring adequate
diversity in generated samples of prompt-guided diffusion models remains a
challenge, particularly when the prompts span a broad semantic spectrum and the
diversity of generated data needs to be evaluated in a prompt-aware fashion
across semantically similar prompts. Recent methods have introduced guidance
via diversity measures to encourage more varied generations. In this work, we
extend the diversity measure-based approaches by proposing the Scalable
Prompt-Aware R\'eny Kernel Entropy Diversity Guidance (SPARKE) method for
prompt-aware diversity guidance. SPARKE utilizes conditional entropy for
diversity guidance, which dynamically conditions diversity measurement on
similar prompts and enables prompt-aware diversity control. While the
entropy-based guidance approach enhances prompt-aware diversity, its reliance
on the matrix-based entropy scores poses computational challenges in
large-scale generation settings. To address this, we focus on the special case
of Conditional latent RKE Score Guidance, reducing entropy computation and
gradient-based optimization complexity from the $O(n^3)$ of general entropy
measures to $O(n)$. The reduced computational complexity allows for
diversity-guided sampling over potentially thousands of generation rounds on
different prompts. We numerically test the SPARKE method on several
text-to-image diffusion models, demonstrating that the proposed method improves
the prompt-aware diversity of the generated data without incurring significant
computational costs. We release our code on the project page:
https://mjalali.github.io/SPARKE

</details>


### [33] [Retrieval of Surface Solar Radiation through Implicit Albedo Recovery from Temporal Context](https://arxiv.org/abs/2506.10174)
*Yael Frischholz,Devis Tuia,Michael Lehning*

Main category: cs.CV

本文提出一种新的基于注意力机制的模拟器来改进山区等地表反射率变化复杂的地区的表面太阳辐射(SUR)的估算。


<details>
  <summary>Details</summary>
Motivation: 现有的卫星反射率估算方法在山区表现不佳,因为这些地区雪覆盖变化频繁,现有方法在这些区域表现不佳。

Method: 提出了一种基于注意力的模拟器,用于从原始卫星图像序列中隐式地推断晴天天面反射率。该方法基于时空视觉变压器,消除了手工制作特征如显式的反照率图或云掩模的需求。

Result: 实验表明,当提供足够长的时空上下文时,该模型的性能可以匹敌使用反照率信息的模型,在复杂地形中尤其有效。

Conclusion: 模型通过学习和利用潜在的地表反射率动态,提高了在山区地形中太阳辐射估算的准确性。

Abstract: Accurate retrieval of surface solar radiation (SSR) from satellite imagery
critically depends on estimating the background reflectance that a spaceborne
sensor would observe under clear-sky conditions. Deviations from this baseline
can then be used to detect cloud presence and guide radiative transfer models
in inferring atmospheric attenuation. Operational retrieval algorithms
typically approximate background reflectance using monthly statistics, assuming
surface properties vary slowly relative to atmospheric conditions. However,
this approach fails in mountainous regions where intermittent snow cover and
changing snow surfaces are frequent. We propose an attention-based emulator for
SSR retrieval that implicitly learns to infer clear-sky surface reflectance
from raw satellite image sequences. Built on the Temporo-Spatial Vision
Transformer, our approach eliminates the need for hand-crafted features such as
explicit albedo maps or cloud masks. The emulator is trained on instantaneous
SSR estimates from the HelioMont algorithm over Switzerland, a region
characterized by complex terrain and dynamic snow cover. Inputs include
multi-spectral SEVIRI imagery from the Meteosat Second Generation platform,
augmented with static topographic features and solar geometry. The target
variable is HelioMont's SSR, computed as the sum of its direct and diffuse
horizontal irradiance components, given at a spatial resolution of 1.7 km. We
show that, when provided a sufficiently long temporal context, the model
matches the performances of albedo-informed models, highlighting the model's
ability to internally learn and exploit latent surface reflectance dynamics.
Our geospatial analysis shows this effect is most powerful in mountainous
regions and improves generalization in both simple and complex topographic
settings. Code and datasets are publicly available at
https://github.com/frischwood/HeMu-dev.git

</details>


### [34] [Attention, Please! Revisiting Attentive Probing for Masked Image Modeling](https://arxiv.org/abs/2506.10178)
*Bill Psomas,Dionysis Christopoulos,Eirini Baltzi,Ioannis Kakogeorgiou,Tilemachos Aravanis,Nikos Komodakis,Konstantinos Karantzalos,Yannis Avrithis,Giorgos Tolias*

Main category: cs.CV

This paper introduces efficient probing (EP) as an improved probing method compared to linear probing and existing attentive probing methods.


<details>
  <summary>Details</summary>
Motivation: existing linear and attentive probing methods have limitations in reflecting the potential of models trained with Masked Image Modeling, suffering from excessive parameterization and poor computational efficiency

Method: efficient probing (EP), a multi-query cross-attention mechanism that eliminates redundant projections, reduces the number of trainable parameters

Result: EP achieves up to a 10x speed-up over conventional multi-head attention and outperforms LP and prior attentive probing approaches across seven benchmarks, generalizes well beyond MIM

Conclusion: efficient probing not only improves computational efficiency but also achieves strong performance improvements in various settings, demonstrating its potential as a better probing method

Abstract: As fine-tuning (FT) becomes increasingly impractical at scale, probing is
emerging as the preferred evaluation protocol for self-supervised learning
(SSL). Yet, the standard linear probing (LP) fails to adequately reflect the
potential of models trained with Masked Image Modeling (MIM), due to the
distributed nature of patch tokens. This motivates the need for attentive
probing, an alternative that uses attention to selectively aggregate
patch-level features. Despite its growing adoption, attentive probing remains
under-explored, with existing methods suffering from excessive parameterization
and poor computational efficiency.
  In this work, we revisit attentive probing through the lens of the
accuracy-efficiency trade-off. We conduct a systematic study of existing
methods, analyzing their mechanisms and benchmarking their performance. We
introduce efficient probing (EP), a multi-query cross-attention mechanism that
eliminates redundant projections, reduces the number of trainable parameters,
and achieves up to a 10$\times$ speed-up over conventional multi-head
attention. Despite its simplicity, EP outperforms LP and prior attentive
probing approaches across seven benchmarks, generalizes well beyond MIM to
diverse pre-training paradigms, produces interpretable attention maps, and
achieves strong gains in low-shot and layer-wise settings. Code available at
https://github.com/billpsomas/efficient-probing.

</details>


### [35] [Improving Personalized Search with Regularized Low-Rank Parameter Updates](https://arxiv.org/abs/2506.10182)
*Fiona Ryan,Josef Sivic,Fabian Caba Heilbron,Judy Hoffman,James M. Rehg,Bryan Russell*

Main category: cs.CV

本文提出了一种新的方法来解决个性化视觉-语言检索的问题，通过正则化低秩适应少量语言编码器参数，可以有效地识别个人概念并保持通用知识。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决个性化视觉-语言检索的挑战，该任务不仅要求从少量样例中学习新的概念，还需要整合个人和通用知识以在不同上下文中识别这些概念。

Method: 我们提出了一种有效的方法，通过正则化低秩适应来调整视觉-语言双编码模型的内部表示，用于个性化视觉-语言检索任务。具体而言，只需要调节语言编码器最终层的一小部分参数即可识别个人概念，同时保持通用知识。此外，我们还研究了如何结合多个学习到的个人概念的参数，发现参数相加是一个有效的策略。

Result: 通过引入一种新的度量标准，即基于视觉-语言模型生成的字幕的图像检索准确性，本文的方法在两个基准测试DeepFashion2和ConCon-Chi上实现了最佳准确率，相比之前的方法提高了4%-22%的个性化检索准确性。

Conclusion: 本文证实了通过正则化低秩适应少量参数，可以有效地适应视觉-语言双编码模型，实现个性化视觉-语言检索，并且能够较优地保持通用知识。此外，参数相加是一种有效的策略，用于结合多个个人概念的参数。实验结果表明，该方法在个性化检索任务上达到了最先进的准确率。

Abstract: Personalized vision-language retrieval seeks to recognize new concepts (e.g.
"my dog Fido") from only a few examples. This task is challenging because it
requires not only learning a new concept from a few images, but also
integrating the personal and general knowledge together to recognize the
concept in different contexts. In this paper, we show how to effectively adapt
the internal representation of a vision-language dual encoder model for
personalized vision-language retrieval. We find that regularized low-rank
adaption of a small set of parameters in the language encoder's final layer
serves as a highly effective alternative to textual inversion for recognizing
the personal concept while preserving general knowledge. Additionally, we
explore strategies for combining parameters of multiple learned personal
concepts, finding that parameter addition is effective. To evaluate how well
general knowledge is preserved in a finetuned representation, we introduce a
metric that measures image retrieval accuracy based on captions generated by a
vision language model (VLM). Our approach achieves state-of-the-art accuracy on
two benchmarks for personalized image retrieval with natural language queries -
DeepFashion2 and ConCon-Chi - outperforming the prior art by 4%-22% on personal
retrievals.

</details>


### [36] [ScoreMix: Improving Face Recognition via Score Composition in Diffusion Generators](https://arxiv.org/abs/2506.10226)
*Parsa Rahimi,Sebastien Marcel*

Main category: cs.CV

提出ScoreMix，一种新的数据增强策略，通过扩散模型的得分组合属性提升分类器的性能，特别适用于标注数据不足的情况。


<details>
  <summary>Details</summary>
Motivation: 研究目的是通过生成具有挑战性的合成样本来增强判别模型的性能，特别是在标注数据有限的情况下，以解决大规模数据收集的问题。

Method: ScoreMix利用扩散模型的分数组合属性来增强分类器性能，特别是在标注数据有限的情况下。通过在扩散采样过程中凸混合来自不同类别的轨迹得分，生成具有挑战性的合成样本以提升所有研究基准中的判别能力。

Result: 实验表明，选择在分类器嵌入空间中距离较远（而非在生成器条件空间中靠近）的类别进行混合可以带来更大的性能增益。此外，在标准指标下，生成器学习到的条件空间与分类器的嵌入空间之间的相关性很小。

Conclusion: 该方法在无需进行大量参数搜索的情况下实现了显著的性能提升，为训练判别模型提供了实际优势，并有效地缓解了大规模数据收集的问题。

Abstract: In this paper, we propose ScoreMix, a novel yet simple data augmentation
strategy leveraging the score compositional properties of diffusion models to
enhance discriminator performance, particularly under scenarios with limited
labeled data. By convexly mixing the scores from different class-conditioned
trajectories during diffusion sampling, we generate challenging synthetic
samples that significantly improve discriminative capabilities in all studied
benchmarks. We systematically investigate class-selection strategies for mixing
and discover that greater performance gains arise when combining classes
distant in the discriminator's embedding space, rather than close in the
generator's condition space. Moreover, we empirically show that, under standard
metrics, the correlation between the generator's learned condition space and
the discriminator's embedding space is minimal. Our approach achieves notable
performance improvements without extensive parameter searches, demonstrating
practical advantages for training discriminative models while effectively
mitigating problems regarding collections of large datasets. Paper website:
https://parsa-ra.github.io/scoremix

</details>


### [37] [California Crop Yield Benchmark: Combining Satellite Image, Climate, Evapotranspiration, and Soil Data Layers for County-Level Yield Forecasting of Over 70 Crops](https://arxiv.org/abs/2506.10228)
*Hamid Kamangir,Mona Hajiesmaeeli,Mason Earles*

Main category: cs.CV

本文为加州各县的70多种作物的产量预测提供了一个基准数据集，并开发了一个多模态深度学习模型，实现了对未见测试数据集的高精度预测。


<details>
  <summary>Details</summary>
Motivation: 尽管有丰富的USDA农业统计服务的历史产量数据，但由于环境、气候和土壤相关因素的复杂相互作用，精确和及时的作物产量预测仍然是一个挑战。因此，本文旨在通过开发新的预测模型来应对这一挑战。

Method: 本文开发了一个专门用于县级别、作物特定产量预测的多模态深度学习模型。该模型采用分层特征提取和时间序列编码器来捕捉生长季节的空间和时间动态变化。静态输入如土壤特性和作物种类提供长期可变性信息。

Result: 本文的方法在所有作物上的未见测试数据集上达到了0.76的整体R2得分，展示了在加州多样化的农业区域中较强预测性能。

Conclusion: 本文所构建的数据集和建模框架为推进农业预测、气候适应和精准农业提供了宝贵的基础。

Abstract: California is a global leader in agricultural production, contributing 12.5%
of the United States total output and ranking as the fifth-largest food and
cotton supplier in the world. Despite the availability of extensive historical
yield data from the USDA National Agricultural Statistics Service, accurate and
timely crop yield forecasting remains a challenge due to the complex interplay
of environmental, climatic, and soil-related factors. In this study, we
introduce a comprehensive crop yield benchmark dataset covering over 70 crops
across all California counties from 2008 to 2022. The benchmark integrates
diverse data sources, including Landsat satellite imagery, daily climate
records, monthly evapotranspiration, and high-resolution soil properties. To
effectively learn from these heterogeneous inputs, we develop a multi-modal
deep learning model tailored for county-level, crop-specific yield forecasting.
The model employs stratified feature extraction and a timeseries encoder to
capture spatial and temporal dynamics during the growing season. Static inputs
such as soil characteristics and crop identity inform long-term variability.
Our approach achieves an overall R2 score of 0.76 across all crops of unseen
test dataset, highlighting strong predictive performance across California
diverse agricultural regions. This benchmark and modeling framework offer a
valuable foundation for advancing agricultural forecasting, climate adaptation,
and precision farming. The full dataset and codebase are publicly available at
our GitHub repository.

</details>


### [38] [DySS: Dynamic Queries and State-Space Learning for Efficient 3D Object Detection from Multi-Camera Videos](https://arxiv.org/abs/2506.10242)
*Rajeev Yasarla,Shizhong Han,Hong Cai,Fatih Porikli*

Main category: cs.CV

本论文提出的DySS方法采用状态空间学习和动态查询技术进行自动驾驶中的3D物体检测，既提高了性能又提升了推理效率。


<details>
  <summary>Details</summary>
Motivation: 传统的BEV物体检测方法依赖密集的BEV特征，构建成本较高。而近期的方法虽然探索稀疏查询进行了检测，但仍然需要大量的查询，这在处理更多的视频帧时会变得昂贵。

Method: DySS采用状态空间学习和动态查询的方法，使用状态空间模型（SSM）来处理不同时间步的采样特征，并引入了未来预测和遮罩重建的辅助任务来训练SSM，使得SSM的状态能更有效和高效地总结场景信息。动态查询通过合井、移除和分割操作来更新，以保持整个网络中有用且精简的检测查询集合。

Result: 在nuScenes测试分割上，DySS实现了65.31的NDS和57.4的mAP，超过了最新的技术水平。在验证分割上，DySS实现了56.2的NDS和46.2的mAP，并且可以达到33FPS的实时推理速度。

Conclusion: DySS在保持优越检测性能的同时提升了推理效率，适合应用于自动驾驶中的快速准确3D物体检测。

Abstract: Camera-based 3D object detection in Bird's Eye View (BEV) is one of the most
important perception tasks in autonomous driving. Earlier methods rely on dense
BEV features, which are costly to construct. More recent works explore sparse
query-based detection. However, they still require a large number of queries
and can become expensive to run when more video frames are used. In this paper,
we propose DySS, a novel method that employs state-space learning and dynamic
queries. More specifically, DySS leverages a state-space model (SSM) to
sequentially process the sampled features over time steps. In order to
encourage the model to better capture the underlying motion and correspondence
information, we introduce auxiliary tasks of future prediction and masked
reconstruction to better train the SSM. The state of the SSM then provides an
informative yet efficient summarization of the scene. Based on the state-space
learned features, we dynamically update the queries via merge, remove, and
split operations, which help maintain a useful, lean set of detection queries
throughout the network. Our proposed DySS achieves both superior detection
performance and efficient inference. Specifically, on the nuScenes test split,
DySS achieves 65.31 NDS and 57.4 mAP, outperforming the latest state of the
art. On the val split, DySS achieves 56.2 NDS and 46.2 mAP, as well as a
real-time inference speed of 33 FPS.

</details>


### [39] [HalLoc: Token-level Localization of Hallucinations for Vision Language Models](https://arxiv.org/abs/2506.10286)
*Eunkyu Park,Minyeong Kim,Gunhee Kim*

Main category: cs.CV

Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Hallucinations pose a significant challenge to the reliability of large
vision-language models, making their detection essential for ensuring accuracy
in critical applications. Current detection methods often rely on
computationally intensive models, leading to high latency and resource demands.
Their definitive outcomes also fail to account for real-world scenarios where
the line between hallucinated and truthful information is unclear. To address
these issues, we propose HalLoc, a dataset designed for efficient,
probabilistic hallucination detection. It features 150K token-level annotated
samples, including hallucination types, across Visual Question Answering (VQA),
instruction-following, and image captioning tasks. This dataset facilitates the
development of models that detect hallucinations with graded confidence,
enabling more informed user interactions. Additionally, we introduce a baseline
model trained on HalLoc, offering low-overhead, concurrent hallucination
detection during generation. The model can be seamlessly integrated into
existing VLMs, improving reliability while preserving efficiency. The prospect
of a robust plug-and-play hallucination detection module opens new avenues for
enhancing the trustworthiness of vision-language models in real-world
applications. The HalLoc dataset and code are publicly available at:
https://github.com/dbsltm/cvpr25_halloc.

</details>


### [40] [Uncertainty-Aware Deep Learning for Automated Skin Cancer Classification: A Comprehensive Evaluation](https://arxiv.org/abs/2506.10302)
*Hamzeh Asgharnezhad,Pegah Tabarisaadi,Abbas Khosravi,Roohallah Alizadehsani,U. Rajendra Acharya*

Main category: cs.CV

研究通过转移学习及不确定性量化评估了几种深度学习模型在HAM10000数据集上进行皮肤病变分类的表现，结果显示CLIP搭配SVM表现最佳，而集成蒙特卡洛丢弃法在不确定性处理上表现更优。


<details>
  <summary>Details</summary>
Motivation: 提升皮肤癌诊断的精准性和可靠性，采用深度学习技术自动化皮肤癌分类，解决数据稀缺及不确定性评估问题。

Method: 第一阶段评估了几种预训练特征提取器结合传统分类器的性能；第二阶段使用蒙特卡洛丢弃法、集成法及集成蒙特卡洛丢弃法进行不确定性量化。

Result: CLIP搭配SVM在分类任务上表现最佳；集成方法在准确性和不确定性处理上找到良好平衡，集成蒙特卡洛丢弃法在不确定性预测上更敏感。

Conclusion: 不确定性量化集成到深度学习的医学诊断中可以提升真实世界临床应用的性能和可靠性。

Abstract: Accurate and reliable skin cancer diagnosis is critical for early treatment
and improved patient outcomes. Deep learning (DL) models have shown promise in
automating skin cancer classification, but their performance can be limited by
data scarcity and a lack of uncertainty awareness. In this study, we present a
comprehensive evaluation of DL-based skin lesion classification using transfer
learning and uncertainty quantification (UQ) on the HAM10000 dataset. In the
first phase, we benchmarked several pre-trained feature extractors-including
Contrastive Language-Image Pretraining (CLIP) variants, Residual Network-50
(ResNet50), Densely Connected Convolutional Network (DenseNet121), Visual
Geometry Group network (VGG16), and EfficientNet-V2-Large-combined with a range
of traditional classifiers such as Support Vector Machine (SVM), eXtreme
Gradient Boosting (XGBoost), and logistic regression. Our results show that
CLIP-based vision transformers, particularly LAION CLIP ViT-H/14 with SVM,
deliver the highest classification performance. In the second phase, we
incorporated UQ using Monte Carlo Dropout (MCD), Ensemble, and Ensemble Monte
Carlo Dropout (EMCD) to assess not only prediction accuracy but also the
reliability of model outputs. We evaluated these models using uncertainty-aware
metrics such as uncertainty accuracy(UAcc), uncertainty sensitivity(USen),
uncertainty specificity(USpe), and uncertainty precision(UPre). The results
demonstrate that ensemble methods offer a good trade-off between accuracy and
uncertainty handling, while EMCD is more sensitive to uncertain predictions.
This study highlights the importance of integrating UQ into DL-based medical
diagnosis to enhance both performance and trustworthiness in real-world
clinical applications.

</details>


### [41] [Towards Scalable SOAP Note Generation: A Weakly Supervised Multimodal Framework](https://arxiv.org/abs/2506.10328)
*Sadia Kamal,Tim Oates,Joy Wan*

Main category: cs.CV

本文提出了一种基于弱监督学习的多模态框架，以自动生成结构化的SOAP笔记，使用病变图像和部分临床文本作为输入。该方法在关键临床指标上与先进模型性能相当，并引入新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 鉴于皮肤癌是全球最常见的癌症形式之一，临床医生在记录患者访问时手工编写SOAP笔记既耗时又容易导致医生倦怠。本研究的动机在于降低这种负担，同时提高文档化过程的效率。

Method: 提出了一种基于弱监督的多模态框架，可以从有限的输入（如病变图像和稀疏的临床文本）来生成结构化的SOAP笔记。这种方法减少了对人工标注的依赖，实现了可扩展、基于临床的文档记录，并减轻了临床医生的负担，同时减少了对大规模标注数据的需求。

Result: 该方法在关键的临床相关性指标上达到了与GPT-4o、Claude和DeepSeek Janus Pro相当的性能。为了评估临床质量，研究引入了两个新指标：MedConceptEval和临床连贯性得分(CCS)，用于评估与专家医疗概念和输入特征的语义一致性。

Conclusion: 研究展示了一种有助于减轻临床医生负担并缩减医疗记录成本的有效工具，同时证明了其在生成结构化的SOAP笔记方面具有可观的性能。

Abstract: Skin carcinoma is the most prevalent form of cancer globally, accounting for
over $8 billion in annual healthcare expenditures. In clinical settings,
physicians document patient visits using detailed SOAP (Subjective, Objective,
Assessment, and Plan) notes. However, manually generating these notes is
labor-intensive and contributes to clinician burnout. In this work, we propose
a weakly supervised multimodal framework to generate clinically structured SOAP
notes from limited inputs, including lesion images and sparse clinical text.
Our approach reduces reliance on manual annotations, enabling scalable,
clinically grounded documentation while alleviating clinician burden and
reducing the need for large annotated data. Our method achieves performance
comparable to GPT-4o, Claude, and DeepSeek Janus Pro across key clinical
relevance metrics. To evaluate clinical quality, we introduce two novel metrics
MedConceptEval and Clinical Coherence Score (CCS) which assess semantic
alignment with expert medical concepts and input features, respectively.

</details>
