{"id": "2510.19840", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19840", "abs": "https://arxiv.org/abs/2510.19840", "authors": ["Sai Teja Erukude", "Viswa Chaitanya Marella", "Suhasnadh Reddy Veluru"], "title": "Fourier-Based GAN Fingerprint Detection using ResNet50", "comment": "6 pages. Published in IEEE", "summary": "The rapid rise of photorealistic images produced from Generative Adversarial\nNetworks (GANs) poses a serious challenge for image forensics and industrial\nsystems requiring reliable content authenticity. This paper uses\nfrequency-domain analysis combined with deep learning to solve the problem of\ndistinguishing StyleGAN-generated images from real ones. Specifically, a\ntwo-dimensional Discrete Fourier Transform (2D DFT) was applied to transform\nimages into the Fourier domain, where subtle periodic artifacts become\ndetectable. A ResNet50 neural network is trained on these transformed images to\ndifferentiate between real and synthetic ones. The experiments demonstrate that\nthe frequency-domain model achieves a 92.8 percent and an AUC of 0.95,\nsignificantly outperforming the equivalent model trained on raw spatial-domain\nimages. These results indicate that the GAN-generated images have unique\nfrequency-domain signatures or \"fingerprints\". The method proposed highlights\nthe industrial potential of combining signal processing techniques and deep\nlearning to enhance digital forensics and strengthen the trustworthiness of\nindustrial AI systems.", "AI": {"tldr": "Frequency-domain analysis combined with a ResNet50 neural network effectively differentiates StyleGAN-generated images from real images with high accuracy.", "motivation": "To address the challenge of detecting GAN-generated photorealistic images in image forensics and industrial systems.", "method": "The paper uses frequency-domain analysis combined with a ResNet50 neural network to distinguish StyleGAN-generated images from real ones.", "result": "The frequency-domain model achieves a 92.8% accuracy and an AUC of 0.95, outperforming models trained on raw spatial-domain images.", "conclusion": "Combining signal processing techniques with deep learning can enhance digital forensics for distinguishing GAN-generated images, highlighting potential for industrial applications."}}
{"id": "2510.19955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19955", "abs": "https://arxiv.org/abs/2510.19955", "authors": ["Márcus Vinícius Lobo Costa", "Sherlon Almeida da Silva", "Bárbara Caroline Benato", "Leo Sampaio Ferraz Ribeiro", "Moacir Antonelli Ponti"], "title": "Transformed Multi-view 3D Shape Features with Contrastive Learning", "comment": null, "summary": "This paper addresses the challenges in representation learning of 3D shape\nfeatures by investigating state-of-the-art backbones paired with both\ncontrastive supervised and self-supervised learning objectives. Computer vision\nmethods struggle with recognizing 3D objects from 2D images, often requiring\nextensive labeled data and relying on Convolutional Neural Networks (CNNs) that\nmay overlook crucial shape relationships. Our work demonstrates that Vision\nTransformers (ViTs) based architectures, when paired with modern contrastive\nobjectives, achieve promising results in multi-view 3D analysis on our\ndownstream tasks, unifying contrastive and 3D shape understanding pipelines.\nFor example, supervised contrastive losses reached about 90.6% accuracy on\nModelNet10. The use of ViTs and contrastive learning, leveraging ViTs' ability\nto understand overall shapes and contrastive learning's effectiveness,\novercomes the need for extensive labeled data and the limitations of CNNs in\ncapturing crucial shape relationships. The success stems from capturing global\nshape semantics via ViTs and refining local discriminative features through\ncontrastive optimization. Importantly, our approach is empirical, as it is\ngrounded on extensive experimental evaluation to validate the effectiveness of\ncombining ViTs with contrastive objectives for 3D representation learning.", "AI": {"tldr": "This paper presents a method using Vision Transformers and contrastive learning for 3D shape feature representation, achieving high accuracy on ModelNet10, and it argues that this method overcomes the limitations of CNNs.", "motivation": "The motivation behind this paper is to overcome the limitations of CNNs in recognizing 3D objects from 2D images, which often require extensive labeled data and may miss out on important shape relationships.", "method": "Our work uses Vision Transformers (ViTs) paired with modern contrastive learning objectives to tackle the challenges in representation learning of 3D shape features.", "result": "The paper demonstrates promising results using supervised contrastive losses, achieving about 90.6% accuracy on ModelNet10.", "conclusion": "The conclusion drawn is that the combination of ViTs and contrastive learning is effective in overcoming the need for extensive labeled data and enhancing the understanding of 3D shapes by capturing global semantics and refining local features."}}
{"id": "2510.19981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19981", "abs": "https://arxiv.org/abs/2510.19981", "authors": ["Martha Teiko Teye", "Ori Maoz", "Matthias Rottmann"], "title": "FutrTrack: A Camera-LiDAR Fusion Transformer for 3D Multiple Object Tracking", "comment": null, "summary": "We propose FutrTrack, a modular camera-LiDAR multi-object tracking framework\nthat builds on existing 3D detectors by introducing a transformer-based\nsmoother and a fusion-driven tracker. Inspired by query-based tracking\nframeworks, FutrTrack employs a multimodal two-stage transformer refinement and\ntracking pipeline. Our fusion tracker integrates bounding boxes with multimodal\nbird's-eye-view (BEV) fusion features from multiple cameras and LiDAR without\nthe need for an explicit motion model. The tracker assigns and propagates\nidentities across frames, leveraging both geometric and semantic cues for\nrobust re-identification under occlusion and viewpoint changes. Prior to\ntracking, we refine sequences of bounding boxes with a temporal smoother over a\nmoving window to refine trajectories, reduce jitter, and improve spatial\nconsistency. Evaluated on nuScenes and KITTI, FutrTrack demonstrates that\nquery-based transformer tracking methods benefit significantly from multimodal\nsensor features compared with previous single-sensor approaches. With an aMOTA\nof 74.7 on the nuScenes test set, FutrTrack achieves strong performance on 3D\nMOT benchmarks, reducing identity switches while maintaining competitive\naccuracy. Our approach provides an efficient framework for improving\ntransformer-based trackers to compete with other neural-network-based methods\neven with limited data and without pretraining.", "AI": {"tldr": "FutrTrack是一个模块化相机-LiDAR多目标追踪框架，其性能优越，基于多模态传感器特征进行追踪，适用于缺乏大量数据和预训练模型的情况。", "motivation": "旨在改进基于神经网络的追踪方法在处理多模态传感器数据时的表现，特别是在没有显式运动模型的情况下，提供一种高效的方法来提高变换基于的追踪器性能。", "method": "提出了一种称为FutrTrack的模块化相机-LiDAR多目标跟踪框架，该框架建立在现有3D检测器的基础上，引入了基于变换的平滑器和融合驱动的追踪器。采用多模态的两阶段变换优化和跟踪流水线，利用几何和语义线索，以在遮挡和视角变化的情况下进行鲁棒的重新识别。在追踪之前，通过一个移动窗口的时间平滑器优化了边界框序列，以精化轨迹、减少抖动并提高空间一致性。", "result": "在nuScenes和KITTI数据集上进行评估时，FutrTrack展示出其基于变换的追踪方法在多模态传感器特征方面显著优于之前的单传感器方法，实现了74.7的aMOTA，减少了身份切换，保持了竞争优势。", "conclusion": "FutrTrack提供了一种有效的框架，旨在通过利用多模态传感器信息来提高基于变换的追踪方法的性能，即使在数据有限的情况下也能保持强劲的竞争力，并减少身份切换。"}}
{"id": "2510.20011", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20011", "abs": "https://arxiv.org/abs/2510.20011", "authors": ["Kushan Choudhury", "Shubhrodeep Roy", "Ankur Chanda", "Shubhajit Biswas", "Somenath Kuiry"], "title": "Improving Predictive Confidence in Medical Imaging via Online Label Smoothing", "comment": "Accepted and presented in International Conference on Advancing\n  Science and Technologies in Health Science", "summary": "Deep learning models, especially convolutional neural networks, have achieved\nimpressive results in medical image classification. However, these models often\nproduce overconfident predictions, which can undermine their reliability in\ncritical healthcare settings. While traditional label smoothing offers a simple\nway to reduce such overconfidence, it fails to consider relationships between\nclasses by treating all non-target classes equally. In this study, we explore\nthe use of Online Label Smoothing (OLS), a dynamic approach that adjusts soft\nlabels throughout training based on the model's own prediction patterns. We\nevaluate OLS on the large-scale RadImageNet dataset using three widely used\narchitectures: ResNet-50, MobileNetV2, and VGG-19. Our results show that OLS\nconsistently improves both Top-1 and Top-5 classification accuracy compared to\nstandard training methods, including hard labels, conventional label smoothing,\nand teacher-free knowledge distillation. In addition to accuracy gains, OLS\nleads to more compact and well-separated feature embeddings, indicating\nimproved representation learning. These findings suggest that OLS not only\nstrengthens predictive performance but also enhances calibration, making it a\npractical and effective solution for developing trustworthy AI systems in the\nmedical imaging domain.", "AI": {"tldr": "本研究通过引入在线标签平滑（OLS）的方法，提高了医学图像分类的预测准确性和校准性。实验结果表明，OLS在三大主流架构中均表现出更好性能，这为开发可靠的医学AI系统提供了有效的解决方案。", "motivation": "深度学习模型，特别是卷积神经网络，在医学图像分类中取得了显著成果，但容易产生过度自信的预测结果，这在关键医疗环境中会削弱模型的可靠性。尽管传统的标签平滑方法提供了一种简单的方式来降低过度自信，但它通过同等对待所有非目标类来操作，忽略了类与类之间的关系。因此，研究旨在开发一种考虑类间关系的方法来改进模型的预测性能和校准性。", "method": "本研究探讨使用在线标签平滑（OLS）方法，这是一种根据模型在训练过程中的预测模式动态调整软标签的动态方法。OLS的实现基于大量的RadImageNet数据集，测试了三大主流架构：ResNet-50、MobileNetV2 和 VGG-19。", "result": "该研究提出了一种名为在线标签平滑（Online Label Smoothing，OLS）的方法，以解决深度学习模型在医学图像分类中的过度自信问题。与传统标签平滑不同，OLS是一种动态方法，它根据模型在训练过程中自身的预测模式来调整软标签。实验表明，与标准训练方法相比，OLS在Top-1和Top-5分类准确度上均有所提升，同时还有助于改进特征表示学习。结果显示，OLS不仅可以增强预测性能，还能改善校准性，从而为开发可靠的医学成像AI系统提供了实用有效的解决方案。", "conclusion": "研究结果证明，相较于传统训练方法，OLS在Top-1和Top-5分类准确度上持续改进，同时改进了特征表示能力，表明其不仅增强预测性能，还提高模型校准性，是医疗领域开发可靠AI系统的实用解决方案。"}}
{"id": "2510.19858", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19858", "abs": "https://arxiv.org/abs/2510.19858", "authors": ["Jindi Wang", "Yidi Zhang", "Zhaoxing Li"], "title": "DeBERTa-KC: A Transformer-Based Classifier for Knowledge Construction in Online Learning Discourse", "comment": null, "summary": "This study presents DeBERTa-KC, a transformer-based model for automatic\nclassification of knowledge construction (KC) levels in online science learning\ndiscourse. Using comments collected from four popular YouTube science channels\n(2022--2024), a balanced corpus of 20,000 manually annotated samples was\ncreated across four KC categories: \\textit{nonKC}, \\textit{Share},\n\\textit{Explore}, and \\textit{Negotiate}. The proposed model extends DeBERTa-v3\nwith Focal Loss, Label Smoothing, and R-Drop regularization to address class\nimbalance and enhance generalization. A reproducible end-to-end pipeline was\nimplemented, encompassing data extraction, annotation, preprocessing, training,\nand evaluation. Across 10-fold stratified cross-validation, DeBERTa-KC achieved\na macro-F1 of $0.836 \\pm 0.008$, significantly out-performing both classical\nand transformer baselines ($p<0.01$). Per-category results indicate strong\nsensitivity to higher-order epistemic engagement, particularly in\n\\textit{Explore} and \\textit{Negotiate} discourse. These findings demonstrate\nthat large language models can effectively capture nuanced indicators of\nknowledge construction in informal digital learning environments, offering\nscalable, theory-informed approaches to discourse analysis and the development\nof automated tools for assessing epistemic engagement.", "AI": {"tldr": "本研究提出了一种基于DeBERTa-v3的模型DeBERTa-KC，通过改进的损失函数和正则化技术提高了对类别不均衡和泛化能力的处理，用于自动分类在线科学讨论的知识构建级别，并展示了其效果优于现有模型。", "motivation": "研究动机在于开发一个模型来自动分类在线科学学习讨论中的知识构建级别，从而提供理论指导的方法进行话语分析，并为评价知识建构提供自动化的工具。", "method": "DeBERTa-KC模型为在线科学学习讨论中的知识构建(KC)级别提供自动分类。该模型基于DeBERTa-v3，并通过Focal Loss、标签平滑和R-Drop正则化来应对类别不均衡问题，增强模型的泛化能力。", "result": "在10折分层交叉验证中，DeBERTa-KC实现了0.836±0.008的宏观F1值，显著优于传统的和变换器基准模型(p<0.01)。在探索和协商讨论中的高阶知识建构敏感度表现尤为突出。", "conclusion": "研究结果表明，大型语言模型能有效捕捉非正式数字学习环境中知识构建的细微指标，提供可扩展的、理论指导的方法进行话语分析。"}}
{"id": "2510.20016", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20016", "abs": "https://arxiv.org/abs/2510.20016", "authors": ["Neema Jakisa Owor", "Joshua Kofi Asamoah", "Tanner Wambui Muturi", "Anneliese Jakisa Owor", "Blessing Agyei Kyem", "Andrews Danyo", "Yaw Adu-Gyamfi", "Armstrong Aboah"], "title": "A Unified Detection Pipeline for Robust Object Detection in Fisheye-Based Traffic Surveillance", "comment": "The paper was accepted at ICCV 2025 and published in CVF database", "summary": "Fisheye cameras offer an efficient solution for wide-area traffic\nsurveillance by capturing large fields of view from a single vantage point.\nHowever, the strong radial distortion and nonuniform resolution inherent in\nfisheye imagery introduce substantial challenges for standard object detectors,\nparticularly near image boundaries where object appearance is severely\ndegraded. In this work, we present a detection framework designed to operate\nrobustly under these conditions. Our approach employs a simple yet effective\npre and post processing pipeline that enhances detection consistency across the\nimage, especially in regions affected by severe distortion. We train several\nstate-of-the-art detection models on the fisheye traffic imagery and combine\ntheir outputs through an ensemble strategy to improve overall detection\naccuracy. Our method achieves an F1 score of0.6366 on the 2025 AI City\nChallenge Track 4, placing 8thoverall out of 62 teams. These results\ndemonstrate the effectiveness of our framework in addressing issues inherent to\nfisheye imagery.", "AI": {"tldr": "针对鱼眼摄像机图像中严重的径向畸变和非均匀分辨率，本文提出了一种检测框架，通过预处理和后处理流水线以及模型集成策略，显著提高了关键区域的物体检测准确性。", "motivation": "鱼眼摄像机提供了一种有效的解决方案，用于从单个视角捕获大视野范围内的交通监控。然而，鱼眼图像中的径向畸变和非均匀分辨率对标准物体检测器提出了巨大挑战，尤其是在图像边界附近物体外观严重退化的地方。因此，开发一种有效的方法来解决这些问题很有必要。", "method": "本文提出了一种检测框架，旨在在鱼眼摄像机图像的条件下稳健地运行。该方法采用了一种简单而有效的预处理和后处理流水线，以提高整个图像区域的检测一致性，特别是在受严重扭曲影响的区域。通过训练几种最先进的检测模型并对结果进行集成，以提高总体检测准确性。", "result": "该方法在2025年AI City挑战赛Track 4中取得了0.6366的F1分数，排名62支队伍中的第8位。", "conclusion": "实验结果证明了该框架在处理鱼眼图像固有问题方面的有效性。"}}
{"id": "2510.19866", "categories": ["cs.CL", "cs.AI", "G.1.10; G.4; I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.19866", "abs": "https://arxiv.org/abs/2510.19866", "authors": ["Xincheng Liu"], "title": "An Evaluation of the Pedagogical Soundness and Usability of AI-Generated Lesson Plans Across Different Models and Prompt Frameworks in High-School Physics", "comment": "20 pages, 6 tables", "summary": "This study evaluates the pedagogical soundness and usability of AI-generated\nlesson plans across five leading large language models: ChatGPT (GPT-5), Claude\nSonnet 4.5, Gemini 2.5 Flash, DeepSeek V3.2, and Grok 4. Beyond model choice,\nthree structured prompt frameworks were tested: TAG (Task, Audience, Goal),\nRACE (Role, Audience, Context, Execution), and COSTAR (Context, Objective,\nStyle, Tone, Audience, Response Format).\n  Fifteen lesson plans were generated for a single high-school physics topic,\nThe Electromagnetic Spectrum. The lesson plans were analyzed through four\nautomated computational metrics: (1) readability and linguistic complexity, (2)\nfactual accuracy and hallucination detection, (3) standards and curriculum\nalignment, and (4) cognitive demand of learning objectives.\n  Results indicate that model selection exerted the strongest influence on\nlinguistic accessibility, with DeepSeek producing the most readable teaching\nplan (FKGL = 8.64) and Claude generating the densest language (FKGL = 19.89).\n  The prompt framework structure most strongly affected the factual accuracy\nand pedagogical completeness, with the RACE framework yielding the lowest\nhallucination index and the highest incidental alignment with NGSS curriculum\nstandards. Across all models, the learning objectives in the fifteen lesson\nplans clustered at the Remember and Understand tiers of Bloom's taxonomy. There\nwere limited higher-order verbs in the learning objectives extracted.\n  Overall, the findings suggest that readability is significantly governed by\nmodel design, while instructional reliability and curricular alignment depend\nmore on the prompt framework. The most effective configuration for lesson plans\nidentified in the results was to combine a readability-optimized model with the\nRACE framework and an explicit checklist of physics concepts, curriculum\nstandards, and higher-order objectives.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.20027", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.20027", "abs": "https://arxiv.org/abs/2510.20027", "authors": ["Damian Bowness", "Charalambos Poullis"], "title": "Extreme Views: 3DGS Filter for Novel View Synthesis from Out-of-Distribution Camera Poses", "comment": null, "summary": "When viewing a 3D Gaussian Splatting (3DGS) model from camera positions\nsignificantly outside the training data distribution, substantial visual noise\ncommonly occurs. These artifacts result from the lack of training data in these\nextrapolated regions, leading to uncertain density, color, and geometry\npredictions from the model.\n  To address this issue, we propose a novel real-time render-aware filtering\nmethod. Our approach leverages sensitivity scores derived from intermediate\ngradients, explicitly targeting instabilities caused by anisotropic\norientations rather than isotropic variance. This filtering method directly\naddresses the core issue of generative uncertainty, allowing 3D reconstruction\nsystems to maintain high visual fidelity even when users freely navigate\noutside the original training viewpoints.\n  Experimental evaluation demonstrates that our method substantially improves\nvisual quality, realism, and consistency compared to existing Neural Radiance\nField (NeRF)-based approaches such as BayesRays. Critically, our filter\nseamlessly integrates into existing 3DGS rendering pipelines in real-time,\nunlike methods that require extensive post-hoc retraining or fine-tuning.\n  Code and results at https://damian-bowness.github.io/EV3DGS", "AI": {"tldr": "This paper presents a novel filtering technique for 3D Gaussian Splatting models that improves visual quality and consistency when viewing from untrained camera positions, solving the problem of instability in such regions without requiring further training.", "motivation": "The motivation behind this paper is to enhance the visual fidelity of 3DGS models by reducing artifacts caused by uncertainty in density, color, and geometry predictions when viewing from outside the training data distribution.", "method": "Our approach uses a real-time render-aware filtering method based on sensitivity scores derived from intermediate gradients to address visual noise and instability issues in 3D Gaussian Splatting models when viewing from untrained camera positions.", "result": "The experimental results show significant improvements in visual quality, realism, and consistency compared to existing methods like BayesRays.", "conclusion": "The proposed filter method effectively enhances the performance of 3D reconstruction systems for exploring beyond the original training viewpoints, allowing for seamless integration into real-time 3DGS rendering pipelines without the need for extensive retraining."}}
{"id": "2510.19871", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19871", "abs": "https://arxiv.org/abs/2510.19871", "authors": ["Yatai Ji", "Teng Wang", "Yuying Ge", "Zhiheng Liu", "Sidi Yang", "Ying Shan", "Ping Luo"], "title": "From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model", "comment": null, "summary": "Discrete diffusion models have emerged as a promising direction for\nvision-language tasks, offering bidirectional context modeling and theoretical\nparallelization. However, their practical application is severely hindered by a\ntrain-inference discrepancy, which leads to catastrophic error cascades:\ninitial token errors during parallel decoding pollute the generation context,\ntriggering a chain reaction of compounding errors and leading to syntactic\nerrors and semantic hallucinations. To address this fundamental challenge, we\nreframe the generation process from passive denoising to active refining. We\nintroduce ReDiff, a refining-enhanced diffusion framework that teaches the\nmodel to identify and correct its own errors. Our approach features a two-stage\ntraining process: first, we instill a foundational revision capability by\ntraining the model to revise synthetic errors; second, we implement a novel\nonline self-correction loop where the model is explicitly trained to revise its\nown flawed drafts by learning from an expert's corrections. This mistake-driven\nlearning endows the model with the crucial ability to revisit and refine its\nalready generated output, effectively breaking the error cascade. Extensive\nexperiments demonstrate that ReDiff significantly improves the coherence and\nfactual accuracy of generated content, enabling stable and efficient parallel\ngeneration far superior to traditional denoising methods. Our codes and models\nare available at https://rediff-hku.github.io/.", "AI": {"tldr": "为了解决离散扩散模型在视觉-语言任务中训练-推理不一致导致错误级联的问题，本文提出了ReDiff，通过主动修正策略改善了生成文本的一致性和准确性。", "motivation": "解决离散扩散模型在视觉-语言任务中训练-推理不一致导致的错误级联问题。", "method": "提出ReDiff框架，这是一种增强型的改进扩散模型，采用两阶段训练过程。首先，通过训练模型修订合成错误来培养其基础的修订能力；其次，通过模型自我修正循环，让模型显式地学习修正自身草稿中的错误，并从专家的修正中学习。", "result": "ReDiff显著提升了生成内容的一致性和事实准确性，实现了比传统去噪方法更稳定、更高效的并行生成。", "conclusion": "ReDiff通过主动修正策略，显著提升了生成文本的质量，使其在视觉-语言任务中具有更广泛的应用潜力。"}}
{"id": "2510.20029", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20029", "abs": "https://arxiv.org/abs/2510.20029", "authors": ["Shengyu Chen", "Shihang Feng", "Yi Luo", "Xiaowei Jia", "Youzuo Lin"], "title": "BrainPuzzle: Hybrid Physics and Data-Driven Reconstruction for Transcranial Ultrasound Tomography", "comment": "13 pages", "summary": "Ultrasound brain imaging remains challenging due to the large difference in\nsound speed between the skull and brain tissues and the difficulty of coupling\nlarge probes to the skull. This work aims to achieve quantitative transcranial\nultrasound by reconstructing an accurate speed-of-sound (SoS) map of the brain.\nTraditional physics-based full-waveform inversion (FWI) is limited by weak\nsignals caused by skull-induced attenuation, mode conversion, and phase\naberration, as well as incomplete spatial coverage since full-aperture arrays\nare clinically impractical. In contrast, purely data-driven methods that learn\ndirectly from raw ultrasound data often fail to model the complex nonlinear and\nnonlocal wave propagation through bone, leading to anatomically plausible but\nquantitatively biased SoS maps under low signal-to-noise and sparse-aperture\nconditions. To address these issues, we propose BrainPuzzle, a hybrid two-stage\nframework that combines physical modeling with machine learning. In the first\nstage, reverse time migration (time-reversal acoustics) is applied to\nmulti-angle acquisitions to produce migration fragments that preserve\nstructural details even under low SNR. In the second stage, a transformer-based\nsuper-resolution encoder-decoder with a graph-based attention unit (GAU) fuses\nthese fragments into a coherent and quantitatively accurate SoS image. A\npartial-array acquisition strategy using a movable low-count transducer set\nimproves feasibility and coupling, while the hybrid algorithm compensates for\nthe missing aperture. Experiments on two synthetic datasets show that\nBrainPuzzle achieves superior SoS reconstruction accuracy and image\ncompleteness, demonstrating its potential for advancing quantitative ultrasound\nbrain imaging.", "AI": {"tldr": "本文提出了一种新的两阶段混合框架BrainPuzzle，结合物理建模与机器学习，有效解决了超声脑成像中的难题，展示了在定量超声脑成像方面的巨大潜力。", "motivation": "旨在通过重建大脑的精确声速（SoS）图来实现定量颅内超声。传统基于物理的全波形反演（FWI）受到颅骨引起的衰减、模态转换和相位畸变所致的弱信号以及因全孔径阵列在临床上不切实际而造成的空间覆盖不完整等因素的限制。而纯粹数据驱动的方法往往无法建模复杂的非线性和非局部的波传播，导致在低信噪比和稀疏孔径条件下形成的解剖学上看似合理但定量上偏差的SoS图。", "method": "提出BrainPuzzle，这是一种结合物理建模和机器学习的两阶段混合框架。第一阶段使用逆时迁移（时间反转声学）处理多角度采集以生成保留结构细节的迁移片段，甚至在低信噪比下也是如此。第二阶段通过基于Transformer的超级分辨率编解码器融合这些片段，该编解码器配备有图注意力单元（GAU），从而生成连贯且定量准确的声速图像。", "result": "在两个合成数据集上的实验结果表明，BrainPuzzle能够实现更高的声速重建精度和更完整的图像。", "conclusion": "实验在两个合成数据集上表明，BrainPuzzle在声速重建精度和图像完整性方面表现优异，证明了它在未来定量超声脑成像领域的潜在应用价值。"}}
{"id": "2510.19875", "categories": ["cs.CL", "cs.AI", "68T40", "I.2.11"], "pdf": "https://arxiv.org/pdf/2510.19875", "abs": "https://arxiv.org/abs/2510.19875", "authors": ["J Rosser", "José Luis Redondo García", "Gustavo Penha", "Konstantina Palla", "Hugues Bouchard"], "title": "Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs via Sparse Attention", "comment": null, "summary": "As Large Language Models (LLMs) scale to million-token contexts, traditional\nMechanistic Interpretability techniques for analyzing attention scale\nquadratically with context length, demanding terabytes of memory beyond 100,000\ntokens. We introduce Sparse Tracing, a novel technique that leverages dynamic\nsparse attention to efficiently analyze long context attention patterns. We\npresent Stream, a compilable hierarchical pruning algorithm that estimates\nper-head sparse attention masks in near-linear time $O(T \\log T)$ and linear\nspace $O(T)$, enabling one-pass interpretability at scale. Stream performs a\nbinary-search-style refinement to retain only the top-$k$ key blocks per query\nwhile preserving the model's next-token behavior. We apply Stream to long\nchain-of-thought reasoning traces and identify thought anchors while pruning\n97-99\\% of token interactions. On the RULER benchmark, Stream preserves\ncritical retrieval paths while discarding 90-96\\% of interactions and exposes\nlayer-wise routes from the needle to output. Our method offers a practical\ndrop-in tool for analyzing attention patterns and tracing information flow\nwithout terabytes of caches. By making long context interpretability feasible\non consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring.\nCode is available at https://anonymous.4open.science/r/stream-03B8/.", "AI": {"tldr": "Introduces Sparse Tracing, enabling efficient analysis of attention patterns in large language models with long contexts, making it feasible on consumer GPUs.", "motivation": "Traditional interpretability techniques for large language models scale quadratically with context length, requiring infeasible amounts of memory. This paper aims to make interpretability feasible on consumer hardware for long-context models.", "method": "Sparse Tracing, a technique that leverages dynamic sparse attention to analyze long context attention patterns efficiently. It involves Stream, a hierarchical algorithm that estimates sparse attention masks in near-linear time and linear space, refining token interactions to retain only the top-$k$ key blocks per query.", "result": "Stream retains thought anchors and key information flow while pruning 97-99% of token interactions. On the RULER benchmark, it discards 90-96% of interactions while preserving critical retrieval paths and exposes layer-wise routes.", "conclusion": "Sparse Tracing is a practical tool for analyzing attention in long-context models without requiring terabytes of memory, thus democratizing interpretability of chain-of-thought monitoring."}}
{"id": "2510.20042", "categories": ["cs.CV", "I.2.10; I.2.6; I.4.9"], "pdf": "https://arxiv.org/pdf/2510.20042", "abs": "https://arxiv.org/abs/2510.20042", "authors": ["Huichan Seo", "Sieun Choi", "Minki Hong", "Yi Zhou", "Junseo Kim", "Lukman Ismaila", "Naome Etori", "Mehul Agarwal", "Zhixuan Liu", "Jihie Kim", "Jean Oh"], "title": "Exposing Blindspots: Cultural Bias Evaluation in Generative Image Models", "comment": "28 pages, 8 figures. Submitted to the Second Conference of the\n  International Association for Safe and Ethical Artificial Intelligence\n  (IASEAI '26)", "summary": "Generative image models produce striking visuals yet often misrepresent\nculture. Prior work has examined cultural bias mainly in text-to-image (T2I)\nsystems, leaving image-to-image (I2I) editors underexplored. We bridge this gap\nwith a unified evaluation across six countries, an 8-category/36-subcategory\nschema, and era-aware prompts, auditing both T2I generation and I2I editing\nunder a standardized protocol that yields comparable diagnostics. Using open\nmodels with fixed settings, we derive cross-country, cross-era, and\ncross-category evaluations. Our framework combines standard automatic metrics,\na culture-aware retrieval-augmented VQA, and expert human judgments collected\nfrom native reviewers. To enable reproducibility, we release the complete image\ncorpus, prompts, and configurations. Our study reveals three findings: (1)\nunder country-agnostic prompts, models default to Global-North, modern-leaning\ndepictions that flatten cross-country distinctions; (2) iterative I2I editing\nerodes cultural fidelity even when conventional metrics remain flat or improve;\nand (3) I2I models apply superficial cues (palette shifts, generic props)\nrather than era-consistent, context-aware changes, often retaining source\nidentity for Global-South targets. These results highlight that\nculture-sensitive edits remain unreliable in current systems. By releasing\nstandardized data, prompts, and human evaluation protocols, we provide a\nreproducible, culture-centered benchmark for diagnosing and tracking cultural\nbias in generative image models.", "AI": {"tldr": "本文探讨了文本到图像和图像到图像模型在文化表征上的偏差，并提出了一个标准化评估框架，结果显示这些模型在文化细节上的表征尚不准确，特别是在全球化北方发达国家和现代描述上更为突出。通过公开标准化数据、提示与人类评估协议，提出了一个文化中心的基准。", "motivation": "先前的大多数文化偏差研究主要集中在文本到图像系统，而图像到图像编辑系统的研究较少。本文旨在弥补这一研究空白。", "method": "通过对六个国家、八个主要类别和三十六个子类别的统一评估，结合时代意识提示，对文本到图像(T2I)生成和图像到图像(I2I)编辑进行了标准化协议下的审查。采用固定设置的开源模型进行跨国、跨时代、跨类别的评估。框架结合了标准的自动指标、文化意识的检索增强型VQA，以及从本地审查者收集的专家人工判断。为了确保可重复性，完整图像语料库、提示和配置已经公开发布。", "result": "研究揭示了以下三个发现：(1)在没有特定国家提示的情况下，模型倾向于生成偏向北方发达国家、现代倾向的描述，从而抹平了跨国别差异；(2)迭代的I2I编辑虽然在传统指标上保持平稳或改进，但实际上却侵蚀了文化保真度；(3)I2I模型应用了表面上的线索（如颜色调整、通用道具），而不是时代一致、上下文感知的变化，尤其是对于发展中国家目标，往往保留了来源的身份。", "conclusion": "这些结果显示，当前系统的文化敏感编辑仍然不一致可靠。通过发布标准化数据、提示和人类评估协议，为诊断和跟踪生成图像模型中的文化偏见提供了一个可复制、以文化为中心的基准。"}}
{"id": "2510.19879", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19879", "abs": "https://arxiv.org/abs/2510.19879", "authors": ["Lang Zhou", "Amrish Jhingoer", "Yinghao Luo", "Klaske Vliegenthart--Jongbloed", "Carlijn Jordans", "Ben Werkhoven", "Tom Seinen", "Erik van Mulligen", "Casper Rokx", "Yunlei Li"], "title": "Automated HIV Screening on Dutch EHR with Large Language Models", "comment": "28 pages, 6 figures", "summary": "Efficient screening and early diagnosis of HIV are critical for reducing\nonward transmission. Although large scale laboratory testing is not feasible,\nthe widespread adoption of Electronic Health Records (EHRs) offers new\nopportunities to address this challenge. Existing research primarily focuses on\napplying machine learning methods to structured data, such as patient\ndemographics, for improving HIV diagnosis. However, these approaches often\noverlook unstructured text data such as clinical notes, which potentially\ncontain valuable information relevant to HIV risk. In this study, we propose a\nnovel pipeline that leverages a Large Language Model (LLM) to analyze\nunstructured EHR text and determine a patient's eligibility for further HIV\ntesting. Experimental results on clinical data from Erasmus University Medical\nCenter Rotterdam demonstrate that our pipeline achieved high accuracy while\nmaintaining a low false negative rate.", "AI": {"tldr": "本研究提出一种新方法，利用LLM分析非结构化EHR文本数据以改进HIV检测的早期诊断，实验结果展示了其高准确率和低误诊率的优势。", "motivation": "高效筛查和早期诊断HIV对于减少传播至关重要。然而，大规模的实验室检测并不现实，而电子健康记录（EHRs）的广泛采用为解决这一挑战提供了新机遇。现有研究主要关注于应用机器学习方法处理结构化的数据（如患者人口统计信息）来改善HIV诊断，但这些方法往往忽略了包含潜在有价值信息的非结构化文本数据如临床记录。", "method": "本研究提出了一种新的流程，利用大型语言模型（LLM）分析非结构化的电子健康记录文本，以确定患者是否符合进一步进行HIV检测的条件。", "result": "实验结果表明，在荷兰鹿特丹伊拉斯姆斯大学医学中心的临床数据上，我们的流程达到了高准确率并且保持了较低的误诊率。", "conclusion": "研究表明，通过利用LLM分析非结构化的EHR文本，可以有效提高HIV检测的准确性和效率。"}}
{"id": "2510.20071", "categories": ["cs.CV", "I.4.1"], "pdf": "https://arxiv.org/pdf/2510.20071", "abs": "https://arxiv.org/abs/2510.20071", "authors": ["Bernd Pfrommer"], "title": "Filter-Based Reconstruction of Images from Events", "comment": null, "summary": "Reconstructing an intensity image from the events of a moving event camera is\na challenging task that is typically approached with neural networks deployed\non graphics processing units. This paper presents a much simpler, FIlter Based\nAsynchronous Reconstruction method (FIBAR). First, intensity changes signaled\nby events are integrated with a temporal digital IIR filter. To reduce\nreconstruction noise, stale pixels are detected by a novel algorithm that\nregulates a window of recently updated pixels. Arguing that for a moving\ncamera, the absence of events at a pixel location likely implies a low image\ngradient, stale pixels are then blurred with a Gaussian filter. In contrast to\nmost existing methods, FIBAR is asynchronous and permits image read-out at an\narbitrary time. It runs on a modern laptop CPU at about 42(140) million\nevents/s with (without) spatial filtering enabled. A few simple qualitative\nexperiments are presented that show the difference in image reconstruction\nbetween FIBAR and a neural network-based approach (FireNet). FIBAR's\nreconstruction is noisier than neural network-based methods and suffers from\nghost images. However, it is sufficient for certain tasks such as the detection\nof fiducial markers. Code is available at\nhttps://github.com/ros-event-camera/event_image_reconstruction_fibar", "AI": {"tldr": "The paper presents FIBAR, a simpler and faster filter-based method for reconstructing images from event cameras, compared to neural networks. It works well in certain tasks like marker detection, though image quality is not as good.", "motivation": "The paper aims to introduce a simpler method for reconstructing intensity images from event camera data, contrasting with the typically used complex neural network approaches.", "method": "First, intensity changes signaled by events are integrated with a temporal digital IIR filter. To reduce reconstruction noise, a novel algorithm detects stale pixels by regulating a window of recently updated pixels. Stale pixels are then blurred with a Gaussian filter.", "result": "FIBAR runs on a modern laptop CPU at about 42(140) million events/s with (without) spatial filtering enabled. Although it produces noisier images and suffers from ghost images compared to neural network methods, it is sufficient for tasks like detecting fiducial markers.", "conclusion": "Despite its noisier and ghost image issues, FIBAR is suitable for applications like detecting fiducial markers and performs much faster than existing neural network methods."}}
{"id": "2510.19886", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19886", "abs": "https://arxiv.org/abs/2510.19886", "authors": ["Artur Donaldson", "Bharathan Balaji", "Cajetan Oriekezie", "Manish Kumar", "Laure Patouillard"], "title": "An Expert-grounded benchmark of General Purpose LLMs in LCA", "comment": null, "summary": "Purpose: Artificial intelligence (AI), and in particular large language\nmodels (LLMs), are increasingly being explored as tools to support life cycle\nassessment (LCA). While demonstrations exist across environmental and social\ndomains, systematic evidence on their reliability, robustness, and usability\nremains limited. This study provides the first expert-grounded benchmark of\nLLMs in LCA, addressing the absence of standardized evaluation frameworks in a\nfield where no clear ground truth or consensus protocols exist.\n  Methods: We evaluated eleven general-purpose LLMs, spanning both commercial\nand open-source families, across 22 LCA-related tasks. Seventeen experienced\npractitioners reviewed model outputs against criteria directly relevant to LCA\npractice, including scientific accuracy, explanation quality, robustness,\nverifiability, and adherence to instructions. We collected 168 expert reviews.\n  Results: Experts judged 37% of responses to contain inaccurate or misleading\ninformation. Ratings of accuracy and quality of explanation were generally\nrated average or good on many models even smaller models, and format adherence\nwas generally rated favourably. Hallucination rates varied significantly, with\nsome models producing hallucinated citations at rates of up to 40%. There was\nno clear-cut distinction between ratings on open-weight versus closed-weight\nLLMs, with open-weight models outperforming or competing on par with\nclosed-weight models on criteria such as accuracy and quality of explanation.\n  Conclusion: These findings highlight the risks of applying LLMs na\\\"ively in\nLCA, such as when LLMs are treated as free-form oracles, while also showing\nbenefits especially around quality of explanation and alleviating labour\nintensiveness of simple tasks. The use of general-purpose LLMs without\ngrounding mechanisms presents ...", "AI": {"tldr": "该研究对11个通用大型语言模型（LLM）在生命周期评估（LCA）相关任务上的性能进行了基准测试，结果显示37%的响应包含不准确或误导性的信息，但一些任务的质量解释和指令遵守情况得到正面评价。开放权重模型在准确性和平行模型方面表现不俗。研究强调了在LCA中使用LLM的风险和潜在好处，但指出缺乏接地机制可能导致问题。", "motivation": "该研究旨在填补生命周期评估（LCA）领域关于大型语言模型（LLM）可靠性和稳健性系统证据的空白，并通过专家评估提供标准评价框架。", "method": "研究评估了11个通用LLM，通过22个LCA相关任务，由17名经验丰富的从业者根据LCA实践中的科学准确性、解释质量、稳健性、可验证性和指令遵守度等标准审查模型输出，共收集了168个专家评审。", "result": "结果显示37%的响应包含不准确或误导性的信息。模型的准确性和平行性评级一般为平均或良好。开放权重模型在某些准确性和平行性标准上与封闭权重模型表现相当或更好。", "conclusion": "研究发现凸显了LLM在LCA中的盲目使用风险，尤其是在作为自由形式的预言机时。但也展示了质量解释和减轻劳动力方面的潜在好处。研究还强调了使用通用LLM时缺乏接地机制的问题。"}}
{"id": "2510.20077", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20077", "abs": "https://arxiv.org/abs/2510.20077", "authors": ["Hui Chen", "Xinjie Wang", "Xianchao Xiu", "Wanquan Liu"], "title": "Data-Adaptive Transformed Bilateral Tensor Low-Rank Representation for Clustering", "comment": null, "summary": "Tensor low-rank representation (TLRR) has demonstrated significant success in\nimage clustering. However, most existing methods rely on fixed transformations\nand suffer from poor robustness to noise. In this paper, we propose a novel\ntransformed bilateral tensor low-rank representation model called TBTLRR, which\nintroduces a data-adaptive tensor nuclear norm by learning arbitrary unitary\ntransforms, allowing for more effective capture of global correlations. In\naddition, by leveraging the bilateral structure of latent tensor data, TBTLRR\nis able to exploit local correlations between image samples and features.\nFurthermore, TBTLRR integrates the $\\ell_{1/2}$-norm and Frobenius norm\nregularization terms for better dealing with complex noise in real-world\nscenarios. To solve the proposed nonconvex model, we develop an efficient\noptimization algorithm inspired by the alternating direction method of\nmultipliers (ADMM) and provide theoretical convergence. Extensive experiments\nvalidate its superiority over the state-of-the-art methods in clustering. The\ncode will be available at https://github.com/xianchaoxiu/TBTLRR.", "AI": {"tldr": "本文提出了一种称为TBTLRR的新模型，该模型在张量子空间表示中引入了数据自适应的张量核范数，能够处理复杂噪声并提升图像聚类的效果。", "motivation": "张量子空间表示（TLRR）在图像聚类中取得了显著成功，但大多数现有方法依赖于固定的转换，难以有效地处理带有噪声的图像。为了改进这一点，提出了TBTLRR模型。", "method": "提出了一种名为TBTLRR的新模型，通过学习任意酉变换引入了数据自适应的张量核范数，这使得TBTLRR能够更有效地捕捉全局相关性。此外，TBTLRR利用潜在张量数据的双边结构来利用图像样本和特征之间的局部相关性。模型结合了$\\ell_{1/2}$-范数和Frobenius范数正则项，以更好地处理现实场景中的复杂噪声。为了求解该非凸优化模型，研究人员开发了一种高效的优化算法，该算法受乘子交替方向法（ADMM）启发，并提供了理论收敛性证明。", "result": "广泛的实验验证了TBTLRR在聚类任务上的性能优于其他最先进的方法。", "conclusion": "TBTLRR模型在处理噪声方面的鲁棒性得到了显著提高，并在图像聚类任务上的实验结果相较于现有方法有显著提升。"}}
{"id": "2510.19892", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19892", "abs": "https://arxiv.org/abs/2510.19892", "authors": ["Nishant Balepur", "Dang Nguyen", "Dayeon Ki"], "title": "Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities", "comment": "Accepted as a Spotlight paper at the EMNLP 2025 Wordplay Workshop", "summary": "Multi-modal large language models (MLMs) are often assessed on static,\nindividual benchmarks -- which cannot jointly assess MLM capabilities in a\nsingle task -- or rely on human or model pairwise comparisons -- which is\nhighly subjective, expensive, and allows models to exploit superficial\nshortcuts (e.g., verbosity) to inflate their win-rates. To overcome these\nissues, we propose game-based evaluations to holistically assess MLM\ncapabilities. Games require multiple abilities for players to win, are\ninherently competitive, and are governed by fix, objective rules, and makes\nevaluation more engaging, providing a robust framework to address the\naforementioned challenges. We manifest this evaluation specifically through\nDixit, a fantasy card game where players must generate captions for a card that\ntrick some, but not all players, into selecting the played card. Our\nquantitative experiments with five MLMs show Dixit win-rate rankings are\nperfectly correlated with those on popular MLM benchmarks, while games between\nhuman and MLM players in Dixit reveal several differences between agent\nstrategies and areas of improvement for MLM reasoning.", "AI": {"tldr": "本文提出了一种基于游戏评估的方法，特别是在Dixit卡牌游戏中评估多模态语言模型（MLMs）的能力，这种方法可以提出更客观、更具挑战性的评估框架，展现模型的不同策略及其改进空间。", "motivation": "文章的动机源自于现有的多模态大语言模型评估方法存在的问题，包括静态独立基准测试无法全面评估模型能力，以及依赖于人工或模型间两两比较的方式存在主观性和成本过高的问题，这会导致模型通过表面策略如冗长来提高胜率，不能真实反映其能力。", "method": "本文提出了基于游戏的评估方法来全面评估多模态大语言模型（MLMs）的能力。这种方法通过要求玩家（模型或人类）具备多种能力来赢得游戏，从而解决了现有评估方法的弊端，如无法综合评估模型在单一任务中的能力，或评估过程中高度主观、昂贵等问题。具体实现是通过一个名为Dixit的幻想卡片游戏，游戏中玩家需要为卡片生成描述，使得部分其他玩家能够正确识别出自己的卡片，但又不能让所有玩家都识别出来，以此作为评估模型能力的一种方式。", "result": "通过五个多模态大语言模型在Dixit游戏中的定量实验，显示了Dixit游戏胜率排名与流行多模态语言模型基准测试上的排名完全相关，与此同时，人与模型在Dixit游戏中的对战揭示了模型策略上的差异及逻辑推理方面的改进空间。", "conclusion": "本文通过引入游戏评估的方法，提供了一种评估多模态大语言模型综合能力的新方式，这种方法不仅更具挑战性，还揭示了模型在策略选择和逻辑推理上的特性和改进空间。"}}
{"id": "2510.20087", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20087", "abs": "https://arxiv.org/abs/2510.20087", "authors": ["Lorenzo Arboit", "Dennis N. Schneider", "Britty Baby", "Vinkle Srivastav", "Pietro Mascagni", "Nicolas Padoy"], "title": "Endoshare: A Source Available Solution to De-Identify and Manage Surgical Videos", "comment": "13 pages, 6 figures. Source-available software:\n  https://camma-public.github.io/Endoshare/", "summary": "Video-based assessment and surgical data science can advance surgical\ntraining, research, and quality improvement. However, widespread use remains\nlimited by heterogeneous recording formats and privacy concerns associated with\nvideo sharing. We present Endoshare, a source-available, cross-platform\napplication for merging, standardizing, and de-identifying endoscopic videos in\nminimally invasive surgery. Development followed the software development life\ncycle with iterative, user-centered feedback. During the analysis phase, an\ninternal survey of clinicians and computer scientists based on ten usability\nheuristics identified key requirements that guided a privacy-by-design\narchitecture. In the testing phase, an external clinician survey combined the\nsame heuristics with Technology Acceptance Model constructs to assess usability\nand adoption, complemented by benchmarking across different hardware\nconfigurations. Four clinicians and four computer scientists initially tested\nthe prototype, reporting high usability (4.68 +/- 0.40/5 and 4.03 +/- 0.51/5),\nwith the lowest score (4.00 +/- 0.93/5) relating to label clarity. After\nrefinement, the testing phase surveyed ten surgeons who reported high perceived\nusefulness (5.07 +/- 1.75/7), ease of use (5.15 +/- 1.71/7), heuristic\nusability (4.38 +/- 0.48/5), and strong recommendation (9.20 +/- 0.79/10).\nProcessing time varied with processing mode, video duration (both p <= 0.001),\nand machine computational power (p = 0.041). Endoshare provides a transparent,\nuser-friendly pipeline for standardized, privacy-preserving surgical video\nmanagement. Compliance certification and broader interoperability validation\nare needed to establish it as a deployable alternative to proprietary systems.\nThe software is available at https://camma-public.github.io/Endoshare/", "AI": {"tldr": "Endoshare is a software tool for merging, standardizing, and de-identifying minimally invasive surgery videos, aiming to enhance video-based assessments and surgical data analysis.", "motivation": "The motivation is to address the limitations posed by varied recording formats and privacy issues related to video sharing in surgical contexts.", "method": "Using an iterative, user-centered design process, Endoshare is developed with a focus on adopting a privacy-by-design approach, ensuring the software meets clinician and researcher needs.", "result": "Tested by clinicians and computer scientists, Endoshare demonstrated strong usability scores and positive feedback, showing promise for real-world application in the surgical field.", "conclusion": "The software is deemed user-friendly and useful, though it requires additional compliance certification and validation for broader deployment."}}
{"id": "2510.19895", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19895", "abs": "https://arxiv.org/abs/2510.19895", "authors": ["Guoyun Zhang"], "title": "Large Language Model enabled Mathematical Modeling", "comment": null, "summary": "The integration of Large Language Models (LLMs) with optimization modeling\noffers a promising avenue for advancing decision-making in operations research\n(OR). Traditional optimization methods,such as linear programming, mixed\ninteger programming, and simulation depend heavily on domain expertise to\ntranslate real-world problems into solvable mathematical models. While solvers\nlike Gurobi and COPT are powerful, expert input remains essential for defining\nobjectives, constraints, and variables. This research investigates the\npotential of LLMs, specifically the DeepSeek-R1 model, to bridge this\nformulation gap using natural language understanding and code generation.\nAlthough prior models like GPT-4, Claude, and Bard have shown strong\nperformance in NLP and reasoning tasks, their high token costs and tendency\ntoward hallucinations limit real-world applicability in supply chain contexts.\nIn contrast, DeepSeek-R1, a cost-efficient and high-performing model trained\nwith reinforcement learning, presents a viable alternative. Despite its success\nin benchmarks such as LiveCodeBench and Math-500, its effectiveness in applied\nOR scenarios remains under explored. This study systematically evaluates\nDeepSeek-R1 across four key OR benchmarks: NL4OPT, IndustryOR, EasyLP, and\nComplexOR. Our methodology includes baseline assessments, the development of a\nhallucination taxonomy, and the application of mitigation strategies like\nLLM-as-a-Judge, Few-shot Learning (FSL), Tool Calling, and a Multi-agent\nFramework. These techniques aim to reduce hallucinations, enhance formulation\naccuracy, and better align model outputs with user intent.", "AI": {"tldr": "研究探讨了大型语言模型（LLMs）在弥合运筹学中公式化差距的潜力。特别地，研究了成本效益高且性能好的基于强化学习训练的DeepSeek-R1模型。研究系统性地评估了DeepSeek-R1在四个关键运营研究基准上表现，并采用几种策略减少幻觉现象。", "motivation": "传统的优化方法，例如线性规划、混合整数规划和仿真，严重依赖领域专业知识将现实世界的问题转化为可解的数学模型。即使像Gurobi和COPT这样的求解器也很强大，但专家输入仍然对于定义目标、约束和变量至关重要。该研究旨在调查深度学习模型（特别是DeepSeek-R1模型）如何通过自然语言理解和代码生成来弥合这一公式差距。", "method": "该研究系统评估了DeepSeek-R1在四个关键运筹学基准上的表现：NL4OPT、IndustryOR、EasyLP和ComplexOR，并采用了基线评估、幻觉分类法的开发，以及幻觉缓和策略如LLM-as-a-Judge、少样本学习（FSL）、工具调用和多智能体框架等方法。", "result": "虽然未在摘要中直接提及具体结果，但研究方法表明是旨在减少幻觉、提升公式准确性，更好地将模型输出与用户意图相匹配。", "conclusion": "虽然研究中未直接说明结论，但可以推测结论应该是关于DeepSeek-R1在处理运筹学问题时的潜在能力和仍需进一步研究的方面。"}}
{"id": "2510.20092", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20092", "abs": "https://arxiv.org/abs/2510.20092", "authors": ["Hao Yu", "Haoyu Chen", "Yan Jiang", "Wei Peng", "Zhaodong Sun", "Samuel Kaski", "Guoying Zhao"], "title": "Attentive Convolution: Unifying the Expressivity of Self-Attention with Convolutional Efficiency", "comment": null, "summary": "Self-attention (SA) has become the cornerstone of modern vision backbones for\nits powerful expressivity over traditional Convolutions (Conv). However, its\nquadratic complexity remains a critical bottleneck for practical applications.\nGiven that Conv offers linear complexity and strong visual priors, continuing\nefforts have been made to promote the renaissance of Conv. However, a\npersistent performance chasm remains, highlighting that these modernizations\nhave not yet captured the intrinsic expressivity that defines SA. In this\npaper, we re-examine the design of the CNNs, directed by a key question: what\nprinciples give SA its edge over Conv? As a result, we reveal two fundamental\ninsights that challenge the long-standing design intuitions in prior research\n(e.g., Receptive field). The two findings are: (1) \\textit{Adaptive routing}:\nSA dynamically regulates positional information flow according to semantic\ncontent, whereas Conv employs static kernels uniformly across all positions.\n(2) \\textit{Lateral inhibition}: SA induces score competition among token\nweighting, effectively suppressing redundancy and sharpening representations,\nwhereas Conv filters lack such inhibitory dynamics and exhibit considerable\nredundancy. Based on this, we propose \\textit{Attentive Convolution} (ATConv),\na principled reformulation of the convolutional operator that intrinsically\ninjects these principles. Interestingly, with only $3\\times3$ kernels, ATConv\nconsistently outperforms various SA mechanisms in fundamental vision tasks.\nBuilding on ATConv, we introduce AttNet, a CNN family that can attain\n\\textbf{84.4\\%} ImageNet-1K Top-1 accuracy with only 27M parameters. In\ndiffusion-based image generation, replacing all SA with the proposed $3\\times\n3$ ATConv in SiT-XL/2 reduces ImageNet FID by 0.15 in 400k steps with faster\nsampling. Code is available at: github.com/price112/Attentive-Convolution.", "AI": {"tldr": "论文探讨了卷积层与自注意力机制之间的差距，揭示了自注意力机制在表现力上的两大原则，提出了一种Attentive Convolution，能够仅用3x3的卷积核，在基础视觉任务中超越各种自注意力机制，并构建了高性能的CNN家族AttNet。", "motivation": "虽然卷积(Convolutions, Conv)提供了线性复杂度和强大的视觉先验，但卷积神经网络(CNNs)在性能上与自注意力机制(Self-attention, SA)之间存在的性能鸿沟表明，目前的升级尚未完全捕捉到定义SA的内在表现力。因此，论文旨在揭示SA相对于Conv的优势原则。", "method": "基于提出的原理，该论文提出了一种名为Attentive Convolution (ATConv) 的卷积运算原理性重新定义，以注入自注意力机制中的关键原则。利用仅3x3的卷积核，ATConv在基础视觉任务中稳定地超越了各种自注意力机制。在此基础上，构建了AttNet，一个能够达到84.4%的ImageNet-1K Top-1准确率，且仅有27M参数的CNN家族。", "result": "提出的AttNet在ImageNet-1K数据集上达到了84.4%的Top-1准确率，同时参数数量仅为27M。在扩散模型中的ImageNet生成任务中，将所有自注意力机制替换为提出的3x3 Attentive Convolution的模型在400k步训练后，FID降低了0.15，并且采样速度更快。", "conclusion": "通过引入Attentive Convolution，成功地弥合了卷积网络与自注意力机制之间的性能差距，并在基础视觉任务和扩散生成模型中提高了性能，展示了卷积层与自注意力机制结合的潜力。"}}
{"id": "2510.19897", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19897", "abs": "https://arxiv.org/abs/2510.19897", "authors": ["Jackson Hassell", "Dan Zhang", "Hannah Kim", "Tom Mitchell", "Estevam Hruschka"], "title": "Learning from Supervision with Semantic and Episodic Memory: A Reflective Approach to Agent Adaptation", "comment": "11 pages", "summary": "We investigate how agents built on pretrained large language models can learn\ntarget classification functions from labeled examples without parameter\nupdates. While conventional approaches like fine-tuning are often costly,\ninflexible, and opaque, we propose a memory-augmented framework that leverages\nboth labeled data and LLM-generated critiques. Our framework uses episodic\nmemory to store instance-level critiques-capturing specific past\nexperiences-and semantic memory to distill these into reusable, task-level\nguidance. Across a diverse set of tasks, incorporating critiques yields up to a\n24.8 percent accuracy improvement over retrieval-based (RAG-style) baselines\nthat rely only on labels. Through extensive empirical evaluation, we uncover\ndistinct behavioral differences between OpenAI and opensource models,\nparticularly in how they handle fact-oriented versus preference-based data. To\ninterpret how models respond to different representations of supervision\nencoded in memory, we introduce a novel metric, suggestibility. This helps\nexplain observed behaviors and illuminates how model characteristics and memory\nstrategies jointly shape learning dynamics. Our findings highlight the promise\nof memory-driven, reflective learning for building more adaptive and\ninterpretable LLM agents.", "AI": {"tldr": "本文探讨了基于预训练大型语言模型构建的代理如何通过有标签的例子学习目标分类函数，而无需参数更新。研究引入了一种记忆增强框架，无需参数更新，准确率得到了显著提升。", "motivation": "传统的做法如微调往往是昂贵的、不可灵活调整的，并且不透明。为了克服这些局限性，我们提出了新的方法。", "method": "我们提出了一种记忆增强框架，该框架结合了标签数据和大型语言模型生成的批评意见。我们的框架使用情节记忆来存储实例级别的批评意见——捕捉特定的过去经验，并使用语义记忆将这些批评意见提炼成可重用的任务级指导。", "result": "通过广泛的实证评估，我们发现与其他只依赖标签的检索基线（如RAG风格的）相比，引入批评意见的框架在各种任务上准确率提高了达24.8%。", "conclusion": "我们的研究强调了记忆驱动的反思学习在构建更具有适应性和解释性的大型语言模型代理方面的潜力。"}}
{"id": "2510.20093", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20093", "abs": "https://arxiv.org/abs/2510.20093", "authors": ["Jiho Park", "Sieun Choi", "Jaeyoon Seo", "Jihie Kim"], "title": "StableSketcher: Enhancing Diffusion Model for Pixel-based Sketch Generation via Visual Question Answering Feedback", "comment": "Under review at IEEE Access. Author-submitted preprint. Not the\n  IEEE-published version", "summary": "Although recent advancements in diffusion models have significantly enriched\nthe quality of generated images, challenges remain in synthesizing pixel-based\nhuman-drawn sketches, a representative example of abstract expression. To\ncombat these challenges, we propose StableSketcher, a novel framework that\nempowers diffusion models to generate hand-drawn sketches with high prompt\nfidelity. Within this framework, we fine-tune the variational autoencoder to\noptimize latent decoding, enabling it to better capture the characteristics of\nsketches. In parallel, we integrate a new reward function for reinforcement\nlearning based on visual question answering, which improves text-image\nalignment and semantic consistency. Extensive experiments demonstrate that\nStableSketcher generates sketches with improved stylistic fidelity, achieving\nbetter alignment with prompts compared to the Stable Diffusion baseline.\nAdditionally, we introduce SketchDUO, to the best of our knowledge, the first\ndataset comprising instance-level sketches paired with captions and\nquestion-answer pairs, thereby addressing the limitations of existing datasets\nthat rely on image-label pairs. Our code and dataset will be made publicly\navailable upon acceptance.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.19967", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19967", "abs": "https://arxiv.org/abs/2510.19967", "authors": ["Le Ren", "Xiangjian Zeng", "Qingqiang Wu", "Ruoxuan Liang"], "title": "LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework For Controllable Lyric Translation", "comment": "submitted to ICASSP 2026", "summary": "Lyric translation is a challenging task that requires balancing multiple\nmusical constraints. Existing methods often rely on hand-crafted rules and\nsentence-level modeling, which restrict their ability to internalize\nmusical-linguistic patterns and to generalize effectively at the paragraph\nlevel, where cross-line coherence and global rhyme are crucial. In this work,\nwe propose LyriCAR, a novel framework for controllable lyric translation that\noperates in a fully unsupervised manner. LyriCAR introduces a difficulty-aware\ncurriculum designer and an adaptive curriculum strategy, ensuring efficient\nallocation of training resources, accelerating convergence, and improving\noverall translation quality by guiding the model with increasingly complex\nchallenges. Extensive experiments on the EN-ZH lyric translation task show that\nLyriCAR achieves state-of-the-art results across both standard translation\nmetrics and multi-dimensional reward scores, surpassing strong baselines.\nNotably, the adaptive curriculum strategy reduces training steps by nearly 40%\nwhile maintaining superior performance. Code, data and model can be accessed at\nhttps://github.com/rle27/LyriCAR.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.20095", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20095", "abs": "https://arxiv.org/abs/2510.20095", "authors": ["Ziheng Zhang", "Xinyue Ma", "Arpita Chowdhury", "Elizabeth G. Campolongo", "Matthew J. Thompson", "Net Zhang", "Samuel Stevens", "Hilmar Lapp", "Tanya Berger-Wolf", "Yu Su", "Wei-Lun Chao", "Jianyang Gu"], "title": "BIOCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models", "comment": "Project page: https://imageomics.github.io/biocap/", "summary": "This work investigates descriptive captions as an additional source of\nsupervision for biological multimodal foundation models. Images and captions\ncan be viewed as complementary samples from the latent morphospace of a\nspecies, each capturing certain biological traits. Incorporating captions\nduring training encourages alignment with this shared latent structure,\nemphasizing potentially diagnostic characters while suppressing spurious\ncorrelations. The main challenge, however, lies in obtaining faithful,\ninstance-specific captions at scale. This requirement has limited the\nutilization of natural language supervision in organismal biology compared with\nmany other scientific domains. We complement this gap by generating synthetic\ncaptions with multimodal large language models (MLLMs), guided by\nWikipedia-derived visual information and taxon-tailored format examples. These\ndomain-specific contexts help reduce hallucination and yield accurate,\ninstance-based descriptive captions. Using these captions, we train BIOCAP\n(i.e., BIOCLIP with Captions), a biological foundation model that captures rich\nsemantics and achieves strong performance in species classification and\ntext-image retrieval. These results demonstrate the value of descriptive\ncaptions beyond labels in bridging biological images with multimodal foundation\nmodels.", "AI": {"tldr": "这项研究通过使用多模态大型语言模型生成基于实例的描述性标题，克服了在生物领域使用自然语言监督的局限性，从而提高了生物基础模型在物种分类和文本图像检索中的性能。", "motivation": "研究动机在于探索描述性标题作为生物多模态基础模型附加监督源的潜力。描述性标题和图像可以在潜在的形态空间中互补，有助于强调诊断特征并抑制虚假关联。生成忠实的、实例特定的描述性标题是主要挑战。", "method": "本研究通过生成合成描述性标题来补充自然语言监督在生物领域利用的不足。合成描述性标题的生成是以维基百科衍生的视觉信息和特定分类的格式示例为指导，使用多模态大型语言模型（MLLMs）。这种方法有助于减少虚构内容并生成准确的、基于实例的描述性标题。", "result": "使用这些合成描述性标题训练的BIOCAP（即带描述性标题的BIOCLIP）生物基础模型展示了强大的物种分类和文本图像检索性能，证明了描述性标题在连接生物图像和多模态基础模型方面的价值。", "conclusion": "研究结果表明，在训练期间引入描述性标题有助于生物多模态基础模型的学习，提升了其在生物视觉任务中的表现能力。"}}
{"id": "2510.19988", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19988", "abs": "https://arxiv.org/abs/2510.19988", "authors": ["Xin Lian", "Kenneth D. Forbus"], "title": "LLM-Augmented Symbolic NLU System for More Reliable Continuous Causal Statement Interpretation", "comment": "18 pages, 2 figures", "summary": "Despite the broad applicability of large language models (LLMs), their\nreliance on probabilistic inference makes them vulnerable to errors such as\nhallucination in generated facts and inconsistent output structure in natural\nlanguage understanding (NLU) tasks. By contrast, symbolic NLU systems provide\ninterpretable understanding grounded in curated lexicons, semantic resources,\nand syntactic & semantic interpretation rules. They produce relational\nrepresentations that can be used for accurate reasoning and planning, as well\nas incremental debuggable learning. However, symbolic NLU systems tend to be\nmore limited in coverage than LLMs and require scarce knowledge representation\nand linguistics skills to extend and maintain. This paper explores a hybrid\napproach that integrates the broad-coverage language processing of LLMs with\nthe symbolic NLU capabilities of producing structured relational\nrepresentations to hopefully get the best of both approaches. We use LLMs for\nrephrasing and text simplification, to provide broad coverage, and as a source\nof information to fill in knowledge gaps more automatically. We use symbolic\nNLU to produce representations that can be used for reasoning and for\nincremental learning. We evaluate this approach on the task of extracting and\ninterpreting quantities and causal laws from commonsense science texts, along\nwith symbolic- and LLM-only pipelines. Our results suggest that our hybrid\nmethod works significantly better than the symbolic-only pipeline.", "AI": {"tldr": "本文提出了一种混合方法，结合了大语言模型的广泛语言处理能力和符号自然语言理解系统的结构化关系表示能力，以期取得两种方法的最优效果。", "motivation": "尽管大语言模型（LLMs）具有广泛应用，但它们基于概率推理的特性使其容易产生错误，例如生成事实的幻觉和自然语言理解任务中的输出结构不一致。相比之下，符号NLU系统提供基于精心策划的词典、语义资源和语义及语法解释规则的可解释理解。通过结合两者的优势，以实现更全面和准确的自然语言处理。", "method": "本研究探索了一种结合大语言模型（LLMs）广泛语言处理能力和符号自然语言理解（NLU）系统生成结构化关系表示能力的混合方法。LLMs用于重新表述和文本简化，以提供广泛覆盖，并作为自动填补知识空白的信息源。符号NLU用于生成可用于推理和增量学习的表示。", "result": "方法在从常识科学文本中提取和解释数量和因果法则的任务上进行评估，结果表明该混合方法比仅使用符号方法的管线效果要好得多。", "conclusion": "研究结果表明，将大语言模型的优势与符号NLU系统的结构化关系表示能力相结合的混合方法在特定任务上表现优于单独使用符号NLU的方法。"}}
{"id": "2510.20126", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20126", "abs": "https://arxiv.org/abs/2510.20126", "authors": ["Prithvi Raj Singh", "Raju Gottumukkala", "Anthony S. Maida", "Alan B. Barhorst", "Vijaya Gopu"], "title": "Physics-Guided Fusion for Robust 3D Tracking of Fast Moving Small Objects", "comment": "13 pages, 6 figures", "summary": "While computer vision has advanced considerably for general object detection\nand tracking, the specific problem of fast-moving tiny objects remains\nunderexplored. This paper addresses the significant challenge of detecting and\ntracking rapidly moving small objects using an RGB-D camera. Our novel system\ncombines deep learning-based detection with physics-based tracking to overcome\nthe limitations of existing approaches. Our contributions include: (1) a\ncomprehensive system design for object detection and tracking of fast-moving\nsmall objects in 3D space, (2) an innovative physics-based tracking algorithm\nthat integrates kinematics motion equations to handle outliers and missed\ndetections, and (3) an outlier detection and correction module that\nsignificantly improves tracking performance in challenging scenarios such as\nocclusions and rapid direction changes. We evaluated our proposed system on a\ncustom racquetball dataset. Our evaluation shows our system surpassing kalman\nfilter based trackers with up to 70\\% less Average Displacement Error. Our\nsystem has significant applications for improving robot perception on\nautonomous platforms and demonstrates the effectiveness of combining\nphysics-based models with deep learning approaches for real-time 3D detection\nand tracking of challenging small objects.", "AI": {"tldr": "本文提出了一种融合深度学习与物理模型的综合系统设计，有效提高了快速移动小目标在3D空间中的检测与追踪性能，尤其在遮挡和快速方向变化的条件下，显著优于基于卡尔曼滤波的追踪器。", "motivation": "计算机视觉在通用目标检测和追踪方面取得了显著进展，但对于快速移动的小目标检测仍较少被探索。本文旨在解决这一检测难题，提供一种高性能的解决方案。", "method": "本文提出了一种综合系统设计，用于在三维空间中快速移动的小目标检测与追踪。该系统结合了基于深度学习的检测方法和基于物理的追踪算法。其中，追踪算法融合了动力学运动方程以处理异常值和漏检情况，同时还包含了一个异常检测和校正模块，以提高在遮挡和快速方向变化时的追踪性能。", "result": "本文提出的系统在快速移动小目标的检测与追踪上取得了显著的效果，特别在遮挡和快速方向变化等挑战性场景中，展示了相比现有方法的优势。", "conclusion": "本文提出的系统在定制的壁球数据集上进行了评估，结果表明，与基于卡尔曼滤波的追踪器相比，本文系统将平均位移误差减少了70%。该系统在自动驾驶平台的机器人感知方面具有重要的应用前景，证明了结合基于物理模型和深度学习方法对实时三维检测和追踪具有显著效果。"}}
{"id": "2510.19996", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19996", "abs": "https://arxiv.org/abs/2510.19996", "authors": ["Michael A. Covington"], "title": "A Fundamental Algorithm for Dependency Parsing (With Corrections)", "comment": "Corrected version of an already widely cited paper", "summary": "This paper presents a fundamental algorithm for parsing natural language\nsentences into dependency trees. Unlike phrase-structure (constituency)\nparsers, this algorithm operates one word at a time, attaching each word as\nsoon as it can be attached, corresponding to properties claimed for the parser\nin the human brain. Like phrase-structure parsing, its worst-case complexity is\n$O(n^3)$, but in human language, the worst case occurs only for small $n$.", "AI": {"tldr": "本文提出了一种将自然语言句子解析为依存关系树的基本算法。这种算法按照一个词接一个词的方式进行解析，模仿了人类大脑中的解析过程。尽管其最坏情况下的复杂度与短语结构解析相同，为$O(n^3)$，但在人类语言中，最坏情况只出现在小规模样本中。", "motivation": "针对现有的短语结构解析器的不足，本文目的是提出一种更接近人类大脑解析机制的依存关系解析算法。", "method": "依存关系解析算法，每解析一个词就进行一次连接操作。", "result": "算法的最坏情况复杂度为$O(n^3)$，但在实际应用中，这种情况极为少见。", "conclusion": "本文提出的方法提供了一种模仿人类大脑的语言解析过程的算法，尽管有理论上的最坏情况复杂度，但在实际应用中表现良好。"}}
{"id": "2510.20132", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20132", "abs": "https://arxiv.org/abs/2510.20132", "authors": ["Hyunjun Jung", "Hae-Gon Jeon"], "title": "Inverse Image-Based Rendering for Light Field Generation from Single Images", "comment": null, "summary": "A concept of light-fields computed from multiple view images on regular grids\nhas proven its benefit for scene representations, and supported realistic\nrenderings of novel views and photographic effects such as refocusing and\nshallow depth of field. In spite of its effectiveness of light flow\ncomputations, obtaining light fields requires either computational costs or\nspecialized devices like a bulky camera setup and a specialized microlens\narray. In an effort to broaden its benefit and applicability, in this paper, we\npropose a novel view synthesis method for light field generation from only\nsingle images, named inverse image-based rendering. Unlike previous attempts to\nimplicitly rebuild 3D geometry or to explicitly represent objective scenes, our\nmethod reconstructs light flows in a space from image pixels, which behaves in\nthe opposite way to image-based rendering. To accomplish this, we design a\nneural rendering pipeline to render a target ray in an arbitrary viewpoint. Our\nneural renderer first stores the light flow of source rays from the input\nimage, then computes the relationships among them through cross-attention, and\nfinally predicts the color of the target ray based on these relationships.\nAfter the rendering pipeline generates the first novel view from a single input\nimage, the generated out-of-view contents are updated to the set of source\nrays. This procedure is iteratively performed while ensuring the consistent\ngeneration of occluded contents. We demonstrate that our inverse image-based\nrendering works well with various challenging datasets without any retraining\nor finetuning after once trained on synthetic dataset, and outperforms relevant\nstate-of-the-art novel view synthesis methods.", "AI": {"tldr": "本文提出了一种名为逆图像渲染的新视图合成方法，可以从单张图片生成光场。该方法通过神经渲染管线利用交叉注意力机制在任意视点下预测目标光线的颜色，并迭代生成一致的被遮挡内容，且无需针对不同数据集进行微调。", "motivation": "鉴于光场计算的有效性和获取光场所需的高计算成本或特殊设备，本文旨在通过提出一种从单张图片生成光场的方法来拓宽光场技术的应用范围。", "method": "本文提出的方法是逆图像渲染，通过设计神经渲染管线，在输入图片源光线的基础上，利用交叉注意力机制计算光线关系，预测目标光线颜色。该过程迭代进行，每次迭代都会更新生成的视线内容，以确保被遮挡内容的一致性生成。", "result": "实验表明，本方法可以很好地适用于各种具有挑战性的数据集，且无需再次训练或微调。与现有的状态-of-the-art 新视图合成方法相比，本方法的性能也更优。", "conclusion": "逆图像渲染方法为从单张图像生成一致的光场提供了一种新的方法，适用于不同数据集且性能优越。这一方法的成功也证明了逆图像渲染在视图合成领域中的潜力。"}}
{"id": "2510.20001", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20001", "abs": "https://arxiv.org/abs/2510.20001", "authors": ["Yunpeng Xiao", "Carl Yang", "Mark Mai", "Xiao Hu", "Kai Shu"], "title": "Beyond MedQA: Towards Real-world Clinical Decision Making in the Era of LLMs", "comment": "13 pages, 3 figures", "summary": "Large language models (LLMs) show promise for clinical use. They are often\nevaluated using datasets such as MedQA. However, Many medical datasets, such as\nMedQA, rely on simplified Question-Answering (Q\\A) that underrepresents\nreal-world clinical decision-making. Based on this, we propose a unifying\nparadigm that characterizes clinical decision-making tasks along two\ndimensions: Clinical Backgrounds and Clinical Questions. As the background and\nquestions approach the real clinical environment, the difficulty increases. We\nsummarize the settings of existing datasets and benchmarks along two\ndimensions. Then we review methods to address clinical decision-making,\nincluding training-time and test-time techniques, and summarize when they help.\nNext, we extend evaluation beyond accuracy to include efficiency,\nexplainability. Finally, we highlight open challenges. Our paradigm clarifies\nassumptions, standardizes comparisons, and guides the development of clinically\nmeaningful LLMs.", "AI": {"tldr": "本文提出了一种新的临床决策任务统一范式，该范式将任务分为临床背景和临床问题两个维度，并扩展了评估标准。", "motivation": "许多医疗数据集使用简化的问答形式，无法完全代表实际临床决策。", "method": "总结现有数据集和基准测试在两个维度上的设置，回顾临床决策的方法，包括训练时间和测试时间技术，并扩展评估标准。", "result": "未详细说明具体结果，但提出了指导临床应用语言模型发展的范式。", "conclusion": "该范式有助于明确假设、标准化比较并指导开发具有临床意义的语言模型。"}}
{"id": "2510.20134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20134", "abs": "https://arxiv.org/abs/2510.20134", "authors": ["Jiachen Liang", "Ruibing Hou", "Minyang Hu", "Hong Chang", "Shiguang Shan", "Xilin Chen"], "title": "Revisiting Logit Distributions for Reliable Out-of-Distribution Detection", "comment": "Accepted by NeurIPS 2025", "summary": "Out-of-distribution (OOD) detection is critical for ensuring the reliability\nof deep learning models in open-world applications. While post-hoc methods are\nfavored for their efficiency and ease of deployment, existing approaches often\nunderexploit the rich information embedded in the model's logits space. In this\npaper, we propose LogitGap, a novel post-hoc OOD detection method that\nexplicitly exploits the relationship between the maximum logit and the\nremaining logits to enhance the separability between in-distribution (ID) and\nOOD samples. To further improve its effectiveness, we refine LogitGap by\nfocusing on a more compact and informative subset of the logit space.\nSpecifically, we introduce a training-free strategy that automatically\nidentifies the most informative logits for scoring. We provide both theoretical\nanalysis and empirical evidence to validate the effectiveness of our approach.\nExtensive experiments on both vision-language and vision-only models\ndemonstrate that LogitGap consistently achieves state-of-the-art performance\nacross diverse OOD detection scenarios and benchmarks. Code is available at\nhttps://github.com/GIT-LJc/LogitGap.", "AI": {"tldr": "本文提出了一种名为LogitGap的后验OOD检测方法，通过关注logits空间的部分信息提升了OOD检测的性能，实验证明了其在多个基准上的优越性。", "motivation": "现有的后验方法虽然效率高且易于部署，但经常未能充分利用模型logits空间中的丰富信息。因此，本文提出了LogitGap来解决这一问题。", "method": "本文提出了LogitGap，这是一种新颖的后验OOD检测方法，通过明确利用最大logit和其他logits之间的关系来增强分布内（ID）和分布外（OOD）样本的可分离性。为了进一步提升其效果，作者通过一种训练自由的策略，自动识别出最具信息量的logits子集用于评分。", "result": "广泛的实验表明，LogitGap在视觉-语言和纯视觉模型上的不同OOD检测场景和基准测试中，一致实现了最先进的性能。", "conclusion": "作者通过理论分析和实验证据验证了LogitGap的有效性，证明了该方法在多个OOD检测场景中具有优越性能。"}}
{"id": "2510.20002", "categories": ["cs.CL", "cs.AI", "68T50, 68T07, 68U35"], "pdf": "https://arxiv.org/pdf/2510.20002", "abs": "https://arxiv.org/abs/2510.20002", "authors": ["Alexandra Apostolopoulou", "Konstantinos Kanaris", "Athanasios Koursaris", "Dimitris Tsakalidis", "George Domalis", "Ioannis E. Livieris"], "title": "Forging GEMs: Advancing Greek NLP through Quality-Based Corpus Curation and Specialized Pre-training", "comment": null, "summary": "The advancement of natural language processing for morphologically rich,\nmoderately-resourced languages like Modern Greek is often hindered by a\nfragmented research landscape, a lack of architectural diversity and reliance\non limited context-length models. This is particularly true in specialized,\nhigh-value domains such as law, where existing models are frequently confined\nto early transformer architectures with a restrictive 512-token window,\ninsufficient for analyzing long legal documents. To address these challenges,\nthis paper presents Greek Embedding Models, a new family of transformer models\nfor Greek language built upon a foundation of extensive, quality-driven data\ncuration. We detail the construction of several large-scale Greek corpora,\nemphasizing a rigorous, quality-based filtering and preprocessing methodology\nto create high-value training datasets from both general-domain and specialized\nlegal sources. On this carefully curated foundation, we pre-train and\nsystematically evaluate a diverse suite of modern architectures, which has not\npreviously applied to Greek language, such as ELECTRA, ConvBERT and ModernBERT.\nFurthermore, we propose the first bilingual Greek-English Embedding Models\ntailored for the legal domain. The extensive experiments on downstream tasks\ndemonstrate that the new class of models establish the effectiveness of the\nproposed approach, highlighting that the GEM-RoBERTa and GEM-ConvBERT models\nsignificantly outperform existing baselines.", "AI": {"tldr": "This paper introduces Greek Embedding Models, a set of new transformer models specifically for the Greek language. Focused on lifting the fragmented landscape of NLP research in Greek, especially in the legal domain, the authors employ a variety of modern architectures and emphasize rigorous data curation.", "motivation": "The motivation is to overcome the challenges in advancing NLP for Greek, particularly in the legal domain, where existing models are limited, primarily relying on early transformer architectures with insufficient context lengths.", "method": "The method involves the creation of large-scale corpora, including both general-domain and specialized legal sources, employing rigorous data curation techniques. Various modern transformer architectures (ELECTRA, ConvBERT, ModernBERT) are then applied and evaluated.", "result": "The experiments on downstream tasks show that the new models, especially GEM-RoBERTa and GEM-ConvBERT, significantly outperform existing baselines, demonstrating the effectiveness of the proposed approach.", "conclusion": "The conclusion highlights the success of Greek Embedding Models in addressing the limitations of existing NLP models for Greek, especially in handling long legal documents and the effectiveness of a rigorous data-driven methodology."}}
{"id": "2510.20155", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20155", "abs": "https://arxiv.org/abs/2510.20155", "authors": ["Penghao Wang", "Yiyang He", "Xin Lv", "Yukai Zhou", "Lan Xu", "Jingyi Yu", "Jiayuan Gu"], "title": "PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding", "comment": "NeurIPS 2025 DB Track. Project page:\n  https://authoritywang.github.io/partnext", "summary": "Understanding objects at the level of their constituent parts is fundamental\nto advancing computer vision, graphics, and robotics. While datasets like\nPartNet have driven progress in 3D part understanding, their reliance on\nuntextured geometries and expert-dependent annotation limits scalability and\nusability. We introduce PartNeXt, a next-generation dataset addressing these\ngaps with over 23,000 high-quality, textured 3D models annotated with\nfine-grained, hierarchical part labels across 50 categories. We benchmark\nPartNeXt on two tasks: (1) class-agnostic part segmentation, where\nstate-of-the-art methods (e.g., PartField, SAMPart3D) struggle with\nfine-grained and leaf-level parts, and (2) 3D part-centric question answering,\na new benchmark for 3D-LLMs that reveals significant gaps in open-vocabulary\npart grounding. Additionally, training Point-SAM on PartNeXt yields substantial\ngains over PartNet, underscoring the dataset's superior quality and diversity.\nBy combining scalable annotation, texture-aware labels, and multi-task\nevaluation, PartNeXt opens new avenues for research in structured 3D\nunderstanding.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.20033", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20033", "abs": "https://arxiv.org/abs/2510.20033", "authors": ["David Dukić"], "title": "Improving Transfer Learning for Sequence Labeling Tasks by Adapting Pre-trained Neural Language Models", "comment": null, "summary": "This doctoral thesis improves the transfer learning for sequence labeling\ntasks by adapting pre-trained neural language models. The proposed improvements\nin transfer learning involve introducing a multi-task model that incorporates\nan additional signal, a method based on architectural modifications in\nautoregressive large language models, and a sequence labeling framework for\nautoregressive large language models utilizing supervised in-context\nfine-tuning combined with response-oriented adaptation strategies. The first\nimprovement is given in the context of domain transfer for the event trigger\ndetection task. The domain transfer of the event trigger detection task can be\nimproved by incorporating an additional signal obtained from a\ndomain-independent text processing system into a multi-task model. The second\nimprovement involves modifying the model's architecture. For that purpose, a\nmethod is proposed to enable bidirectional information flow across layers of\nautoregressive large language models. The third improvement utilizes\nautoregressive large language models as text generators through a generative\nsupervised in-context fine-tuning framework. The proposed model, method, and\nframework demonstrate that pre-trained neural language models achieve their\nbest performance on sequence labeling tasks when adapted through targeted\ntransfer learning paradigms.", "AI": {"tldr": "论文通过三方面的改进：多任务学习引入额外信号，架构修改提升信息流动，以及使用生成框架进行上下文微调，显著提升了迁移学习在序列标注任务上的效果。", "motivation": "本研究旨在改进迁移学习在序列标注任务中的应用，从而提高模型在不同任务中的适应性和效果。", "method": "此博士论文通过调整预训练的神经语言模型，提出了三种改进迁移学习的方法来增强序列标注任务的效果。第一种改进是在多任务模型中引入额外的信号，这涉及到领域转换事件触发器检测任务；第二种改进是对模型架构进行修改，特别是通过允许自回归大型语言模型各层之间的双向信息流动；第三种改进是通过生成监督上下文微调框架，将自回归大型语言模型用作文本生成器。", "result": "研究显示，通过这三种改进，预训练的神经语言模型在序列标注任务上获得了最佳的表现。", "conclusion": "综上所述，通过所提出的模型、方法和框架，预训练的神经语言模型在经过定向迁移学习训练后，在序列标注任务上可以达到最佳性能。"}}
{"id": "2510.20158", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20158", "abs": "https://arxiv.org/abs/2510.20158", "authors": ["Eduardo R. Corral-Soto", "Yang Liu", "Yuan Ren", "Bai Dongfeng", "Liu Bingbing"], "title": "Monocular Visual 8D Pose Estimation for Articulated Bicycles and Cyclists", "comment": null, "summary": "In Autonomous Driving, cyclists belong to the safety-critical class of\nVulnerable Road Users (VRU), and accurate estimation of their pose is critical\nfor cyclist crossing intention classification, behavior prediction, and\ncollision avoidance. Unlike rigid objects, articulated bicycles are composed of\nmovable rigid parts linked by joints and constrained by a kinematic structure.\n6D pose methods can estimate the 3D rotation and translation of rigid bicycles,\nbut 6D becomes insufficient when the steering/pedals angles of the bicycle\nvary. That is because: 1) varying the articulated pose of the bicycle causes\nits 3D bounding box to vary as well, and 2) the 3D box orientation is not\nnecessarily aligned to the orientation of the steering which determines the\nactual intended travel direction. In this work, we introduce a method for\ncategory-level 8D pose estimation for articulated bicycles and cyclists from a\nsingle RGB image. Besides being able to estimate the 3D translation and\nrotation of a bicycle from a single image, our method also estimates the\nrotations of its steering handles and pedals with respect to the bicycle body\nframe. These two new parameters enable the estimation of a more fine-grained\nbicycle pose state and travel direction. Our proposed model jointly estimates\nthe 8D pose and the 3D Keypoints of articulated bicycles, and trains with a mix\nof synthetic and real image data to generalize on real images. We include an\nevaluation section where we evaluate the accuracy of our estimated 8D pose\nparameters, and our method shows promising results by achieving competitive\nscores when compared against state-of-the-art category-level 6D pose estimators\nthat use rigid canonical object templates for matching.", "AI": {"tldr": "本研究提出了一种从单张RGB图像中对自行车及骑车人进行8D姿态估计的方法，此方法能更细粒度地估计自行车姿态和车行方向。", "motivation": "传统的6D姿态估计方法对于自行车等由关节连接的可动部件组成的车辆不适用，因为其无法捕捉到因车把和踏板角度变化带来的车体姿态变化以及可能的方向改变。", "method": "Structure", "result": "该模型通过使用合成和真实图像数据训练，能够有效估计骑车人及自行车的姿势和3D关键点、踏板和把手方向，展示出富有竞争力的准确率。", "conclusion": "相较于使用刚性标准模型的方法，本方法在估计自行车等可动部件组成的车辆姿态上具有更高的精度和适应性。"}}
{"id": "2510.20036", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20036", "abs": "https://arxiv.org/abs/2510.20036", "authors": ["Marianne Menglin Liu", "Daniel Garcia", "Fjona Parllaku", "Vikas Upadhyay", "Syed Fahad Allam Shah", "Dan Roth"], "title": "ToolScope: Enhancing LLM Agent Tool Use through Tool Merging and Context-Aware Filtering", "comment": "Preprint under review", "summary": "Large language model (LLM) agents rely on external tools to solve complex\ntasks, but real-world toolsets often contain redundant tools with overlapping\nnames and descriptions, introducing ambiguity and reducing selection accuracy.\nLLMs also face strict input context limits, preventing efficient consideration\nof large toolsets. To address these challenges, we propose ToolScope, which\nincludes: (1) ToolScopeMerger with Auto-Correction to automatically audit and\nfix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and\nselect only the most relevant tools for each query, compressing toolsets to fit\nwithin context limits without sacrificing accuracy. Evaluations on three\nstate-of-the-art LLMs and three open-source tool-use benchmarks show gains of\n8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's\neffectiveness in enhancing LLM tool use.", "AI": {"tldr": "ToolScope通过合并和修正工具来减少冗余，并选择最相关的工具以适应LLM的上下文限制，增强了工具选择的准确性。", "motivation": "大型语言模型依赖外部工具来解决复杂数字任务，但现实世界中的工具集往往包含具有重复名称和描述的冗余工具，从而导致歧义并降低选择准确性。此外，LLM面临严格的输入上下文限制，妨碍了对大工具集的有效考虑。", "method": "通过ToolScopeMerger与自动修正来审计并修正工具合并，减少冗余，并通过ToolScopeRetriever来为每个查询排名并选择最相关的工具，压缩工具集以适应上下文限制而不牺牲准确性。", "result": "在三个最先进的大型语言模型和三个开源工具使用基准测试中，工具选择准确性提高了8.38%到38.6%，显示了ToolScope的有效性。", "conclusion": "ToolScope有效提升了LLM在使用工具时的选择准确性，特别是在解决工具集冗余问题上表现出显著优势。"}}
{"id": "2510.20162", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20162", "abs": "https://arxiv.org/abs/2510.20162", "authors": ["Xudong Yan", "Songhe Feng"], "title": "TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning", "comment": "Accepted to NeurIPS 2025", "summary": "Compositional Zero-Shot Learning (CZSL) aims to recognize novel\nattribute-object compositions based on the knowledge learned from seen ones.\nExisting methods suffer from performance degradation caused by the distribution\nshift of label space at test time, which stems from the inclusion of unseen\ncompositions recombined from attributes and objects. To overcome the challenge,\nwe propose a novel approach that accumulates comprehensive knowledge in both\ntextual and visual modalities from unsupervised data to update multimodal\nprototypes at test time. Building on this, we further design an adaptive update\nweight to control the degree of prototype adjustment, enabling the model to\nflexibly adapt to distribution shift during testing. Moreover, a dynamic\npriority queue is introduced that stores high-confidence images to acquire\nvisual knowledge from historical images for inference. Considering the semantic\nconsistency of multimodal knowledge, we align textual and visual prototypes by\nmultimodal collaborative representation learning. Extensive experiments\nindicate that our approach achieves state-of-the-art performance on four\nbenchmark datasets under both closed-world and open-world settings. Code will\nbe available at https://github.com/xud-yan/TOMCAT .", "AI": {"tldr": "本研究提出了一种新的零样本学习方法，旨在解决由未见过的组合引起的测试时的性能下降问题，并在四个基准数据集上实现了最先进的性能。", "motivation": "解决现有方法在测试时因标签空间分布变化（由未见过的组合引起）导致的性能下降问题。", "method": "提出了一种新方法，该方法从无监督数据中累积文本和视觉两模态的全面知识，以更新测试时的多模态原型。设计了一个自适应更新权重来控制原型调整的程度，使模型能够灵活地适应测试时的分布变化。此外，引入了一个动态优先级队列来存储高置信度的图像，以从历史图像中获取视觉知识用于推理。考虑到多模态知识的语义一致性，通过多模态协作表征学习对文本和视觉原型进行对齐。", "result": "{", "conclusion": "实验表明，该方法在四个基准数据集上实现了最先进的性能，无论是封闭世界还是开放世界设置。"}}
{"id": "2510.20043", "categories": ["cs.CL", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.20043", "abs": "https://arxiv.org/abs/2510.20043", "authors": ["Nafis Chowdhury", "Moinul Haque", "Anika Ahmed", "Nazia Tasnim", "Md. Istiak Hossain Shihab", "Sajjadur Rahman", "Farig Sadeque"], "title": "From Facts to Folklore: Evaluating Large Language Models on Bengali Cultural Knowledge", "comment": "4 pages", "summary": "Recent progress in NLP research has demonstrated remarkable capabilities of\nlarge language models (LLMs) across a wide range of tasks. While recent\nmultilingual benchmarks have advanced cultural evaluation for LLMs, critical\ngaps remain in capturing the nuances of low-resource cultures. Our work\naddresses these limitations through a Bengali Language Cultural Knowledge\n(BLanCK) dataset including folk traditions, culinary arts, and regional\ndialects. Our investigation of several multilingual language models shows that\nwhile these models perform well in non-cultural categories, they struggle\nsignificantly with cultural knowledge and performance improves substantially\nacross all models when context is provided, emphasizing context-aware\narchitectures and culturally curated training data.", "AI": {"tldr": "研究分析了多语言模型在Bengali文化任务上的表现，发现模型在文化知识方面需要更多上下文信息才能提高表现，指出了文化数据训练和上下文感知模型的重要性。", "motivation": "解决目前多语言语言模型在低资源文化中的局限性，并强调上下文感知架构和文化训练数据的重要性。", "method": "构建了一个包含民间传统、烹饪艺术和地方方言的Bengali语言文化知识数据集，并研究了几种多语言语言模型在文化知识方面的表现。", "result": "该研究通过Bengali语言文化知识(BLanCK)数据集，包括民间传统、烹饪艺术和地方方言，解决多语言语言模型在捕捉低资源文化细微差别方面的局限性。研究发现，虽然多语言语言模型在非文化类别中表现良好，但在文化知识方面表现不佳，而在提供上下文的情况下，模型性能会有显著改善。这强调了具有上下文感知架构和文化训练数据的重要性。", "conclusion": "通过上下文信息的提供，多语言模型在文化知识方面的表现可以得到显著提升，表明了具有上下文感知能力和文化数据训练模型的重要性。"}}
{"id": "2510.20165", "categories": ["cs.CV", "cs.AI", "68T45 (Machine learning in discrete mathematics), 68T07 (Artificial\n  neural networks and deep learning)"], "pdf": "https://arxiv.org/pdf/2510.20165", "abs": "https://arxiv.org/abs/2510.20165", "authors": ["Insu Jeon", "Wonkwang Lee", "Myeongjang Pyeon", "Gunhee Kim"], "title": "IB-GAN: Disentangled Representation Learning with Information Bottleneck Generative Adversarial Networks", "comment": "Published in the Proceedings of the Thirty Fifth AAAI Conference on\n  Artificial Intelligence (AAAI 2021), paper number 7926", "summary": "We propose a new GAN-based unsupervised model for disentangled representation\nlearning. The new model is discovered in an attempt to utilize the Information\nBottleneck (IB) framework to the optimization of GAN, thereby named IB-GAN. The\narchitecture of IB-GAN is partially similar to that of InfoGAN but has a\ncritical difference; an intermediate layer of the generator is leveraged to\nconstrain the mutual information between the input and the generated output.\nThe intermediate stochastic layer can serve as a learnable latent distribution\nthat is trained with the generator jointly in an end-to-end fashion. As a\nresult, the generator of IB-GAN can harness the latent space in a disentangled\nand interpretable manner. With the experiments on dSprites and Color-dSprites\ndataset, we demonstrate that IB-GAN achieves competitive disentanglement scores\nto those of state-of-the-art \\b{eta}-VAEs and outperforms InfoGAN. Moreover,\nthe visual quality and the diversity of samples generated by IB-GAN are often\nbetter than those by \\b{eta}-VAEs and Info-GAN in terms of FID score on CelebA\nand 3D Chairs dataset.", "AI": {"tldr": "A new GAN model, named IB-GAN, is proposed to achieve disentangled representation learning. It utilizes the Information Bottleneck framework and demonstrates superior performance over existing methods on several datasets.", "motivation": "The motivation is to improve disentangled representation learning in an unsupervised manner, addressing the limitations of previous approaches and achieving competitive scores with state-of-the-art methods.", "method": "We propose a new GAN-based model named IB-GAN, which leverages the Information Bottleneck (IB) framework to constrain the mutual information between the input and the generated output through an intermediate layer of the generator.", "result": "Experiments on dSprites and Color-dSprites show competitive disentanglement scores compared to \b{eta}-VAEs and superior performance to InfoGAN. On CelebA and 3D Chairs dataset, the visual quality and diversity of samples generated by IB-GAN are better than those of \b{eta}-VAEs and Info-GAN in terms of FID score.", "conclusion": "IB-GAN offers an alternative approach to achieving disentangled and interpretable representations, outperforming state-of-the-art models in various metrics and datasets."}}
{"id": "2510.20059", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20059", "abs": "https://arxiv.org/abs/2510.20059", "authors": ["Mehrdad Ghassabi", "Sadra Hakim", "Hamidreza Baradaran Kashani", "Pedram Rostami"], "title": "Enhancing Reasoning Skills in Small Persian Medical Language Models Can Outperform Large-Scale Data Training", "comment": "6 pages, 4 figures", "summary": "Enhancing reasoning capabilities in small language models is critical for\nspecialized applications such as medical question answering, particularly in\nunderrepresented languages like Persian. In this study, we employ Reinforcement\nLearning with AI Feedback (RLAIF) and Direct preference optimization (DPO) to\nimprove the reasoning skills of a general-purpose Persian language model. To\nachieve this, we translated a multiple-choice medical question-answering\ndataset into Persian and used RLAIF to generate rejected-preferred answer\npairs, which are essential for DPO training. By prompting both teacher and\nstudent models to produce Chain-of-Thought (CoT) reasoning responses, we\ncompiled a dataset containing correct and incorrect reasoning trajectories.\nThis dataset, comprising 2 million tokens in preferred answers and 2.5 million\ntokens in rejected ones, was used to train a baseline model, significantly\nenhancing its medical reasoning capabilities in Persian. Remarkably, the\nresulting model outperformed its predecessor, gaokerena-V, which was trained on\napproximately 57 million tokens, despite leveraging a much smaller dataset.\nThese results highlight the efficiency and effectiveness of reasoning-focused\ntraining approaches in developing domain-specific language models with limited\ndata availability.", "AI": {"tldr": "研究通过RLAIF和DPO方法提升波斯语语言模型的医疗推理能力，使用较小的数据集获得了显著效果。", "motivation": "为了改进波斯语等少数语言在医学问答等专有应用中的推理能力。", "method": "通过强化学习与AI反馈（RLAIF）和直接偏好优化（DPO）来提升一款通用波斯语语言模型的推理能力。具体操作方法包括：将一个多选题医疗问答数据集翻译成波斯语；使用RLAIF生成DPO训练所需的被拒绝的答案与偏好答案对；引导教师模型和学生模型生成链式思维（CoT）推理反应，从而收集正确和错误的推理轨迹；使用包括200万个偏好答案令牌和250万个被拒绝答案令牌的训练数据集进行训练。", "result": "训练后，波斯语语言模型的医疗推理能力得到了显著提升，甚至超越了基于更大数据集训练的前代模型gaokerena-V。", "conclusion": "该研究表明，以推理为中心的训练方法在数据有限的情况下可以有效地提升领域特定语言模型的性能。"}}
{"id": "2510.20178", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20178", "abs": "https://arxiv.org/abs/2510.20178", "authors": ["Yun Wang", "Junjie Hu", "Qiaole Dong", "Yongjian Zhang", "Yanwei Fu", "Tin Lun Lam", "Dapeng Wu"], "title": "PPMStereo: Pick-and-Play Memory Construction for Consistent Dynamic Stereo Matching", "comment": null, "summary": "Temporally consistent depth estimation from stereo video is critical for\nreal-world applications such as augmented reality, where inconsistent depth\nestimation disrupts the immersion of users. Despite its importance, this task\nremains challenging due to the difficulty in modeling long-term temporal\nconsistency in a computationally efficient manner. Previous methods attempt to\naddress this by aggregating spatio-temporal information but face a fundamental\ntrade-off: limited temporal modeling provides only modest gains, whereas\ncapturing long-range dependencies significantly increases computational cost.\nTo address this limitation, we introduce a memory buffer for modeling\nlong-range spatio-temporal consistency while achieving efficient dynamic stereo\nmatching. Inspired by the two-stage decision-making process in humans, we\npropose a \\textbf{P}ick-and-\\textbf{P}lay \\textbf{M}emory (PPM) construction\nmodule for dynamic \\textbf{Stereo} matching, dubbed as \\textbf{PPMStereo}. PPM\nconsists of a `pick' process that identifies the most relevant frames and a\n`play' process that weights the selected frames adaptively for spatio-temporal\naggregation. This two-stage collaborative process maintains a compact yet\nhighly informative memory buffer while achieving temporally consistent\ninformation aggregation. Extensive experiments validate the effectiveness of\nPPMStereo, demonstrating state-of-the-art performance in both accuracy and\ntemporal consistency. % Notably, PPMStereo achieves 0.62/1.11 TEPE on the\nSintel clean/final (17.3\\% \\& 9.02\\% improvements over BiDAStereo) with fewer\ncomputational costs. Codes are available at\n\\textcolor{blue}{https://github.com/cocowy1/PPMStereo}.", "AI": {"tldr": "本文提出了一种新的方法PPMStereo，通过两阶段过程（选择和播放）来维护时空一致性，实现更高效和准确的动态立体匹配。", "motivation": "随着深度估计在时间上的一致性对于增强现实等应用场景至关重要，而之前的方法难以在计算效率上实现长期时间一致性建模，因此需要一种新的方法来解决这一问题。", "method": "提出了一种名为PPMStereo的方法，该方法由一个选择相关帧的'选择'过程和一个自适应加权选定帧的'播放'过程组成，以此来维持紧凑而高度信息的内存缓冲区，并实现时空一致的信息聚合。", "result": "实验表明，PPMStereo在准确性和时间一致性方面都表现出了领先的成绩，在计算成本比BiDAStereo少的情况下，Sintel clean/final数据集上的TEPE值达到了0.62/1.11，分别比BiDAStereo提高了17.3%和9.02%。", "conclusion": "PPMStereo方法通过使用选择和播放的过程，能够有效地实现时间上的一致性深度估计，并且在准确性和效率上都优于现有方法。"}}
