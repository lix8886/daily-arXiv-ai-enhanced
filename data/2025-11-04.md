<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 8]
- [cs.CV](#cs.CV) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization](https://arxiv.org/abs/2511.00010)
*Jiajun Zhang,Jianke Zhang,Zeyu Cui,Jiaxi Yang,Lei Zhang,Binyuan Hui,Qiang Liu,Zilei Wang,Liang Wang,Junyang Lin*

Main category: cs.CL

> 介绍了PlotCraft，一个新的包含1000个复杂数据可视化任务的基准测试，以及为了提高处理复杂可视化任务的能力而开发的SynthVis-30K数据集和PlotCraftor模型，该模型在复杂数据可视化方面表现优异。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大型语言模型在生成代码方面表现出色，但它们在创建复杂数据可视化方面的性能尚未充分开发和评估。

**Method:** 开发了PlotCraft基准测试，包括1000个挑战性可视化任务，并通过综合数据集SynthVis-30K和模型PlotCraftor来评估和改进复杂数据可视化的性能。

**Result:** PlotCraftor在多个基准测试上表现良好，特别是在困难任务上，性能提高了50%以上。

**Conclusion:** PlotCraft或能有效突出大型语言模型在数据可视化领域的不足，而PlotCraftor模型在复杂数据可视化方面显示出卓越的能力。

**Abstract:** Recent Large Language Models (LLMs) have demonstrated remarkable profi-
ciency in code generation. However, their ability to create complex visualiza-
tions for scaled and structured data remains largely unevaluated and
underdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark
featuring 1k challenging visualization tasks that cover a wide range of topics,
such as fi- nance, scientific research, and sociology. The benchmark is
structured around seven high-level visualization tasks and encompasses 48
distinct chart types. Cru- cially, it is the first to systematically evaluate
both single-turn generation and multi-turn refinement across a diverse spectrum
of task complexities. Our com- prehensive evaluation of 23 leading LLMs on
PlotCraft reveals obvious per- formance deficiencies in handling sophisticated
visualization tasks. To bridge this performance gap, we develope SynthVis-30K,
a large-scale, high-quality dataset of complex visualization code synthesized
via a collaborative agent frame- work. Building upon this dataset, we develope
PlotCraftor, a novel code gener- ation model that achieves strong capabilities
in complex data visualization with a remarkably small size. Across VisEval,
PandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance
comparable to that of leading propri- etary approaches. Especially, on hard
task, Our model achieves over 50% per- formance improvement. We will release
the benchmark, dataset, and code at
https://github.com/Speakn0w/PlotCraft-Benchmark.

</details>


### [2] [Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference](https://arxiv.org/abs/2511.00115)
*Haoyuan Li,Yuanbo Tong,Yuchen Li,Zirui Wang,Chunhou Liu,Jiamou Liu*

Main category: cs.CL

> 本研究提出了一个认知对齐的框架ProtoMBTI，用于MBTI的推理任务，并证明了这种方法在准确性、解释性、和迁移性上的改进。

<details>
  <summary>Details</summary>

**Motivation:** 典型的方法将个性识别视为硬标签分类，这掩盖了人类个性判断的分级和原型特征。因此，本研究旨在通过将原型理论应用于基于语言模型的流程来创建一个认知对齐的框架。

**Method:** 我们提出了ProtoMBTI框架，该框架通过LLM引导的多维增强（语义、语言、情感）来构建平衡且质量控制的语料库，然后通过LoRA微调一个轻量级的编码器来学习有判别力的嵌入并标准化个性原型库。在推理阶段，模型通过检索-利用-修订-保留循环来处理查询帖子，并通过提示式的投票聚合原型证据，修正不一致，并保留正确的预测以丰富原型库。

**Result:** 在Kaggle和Pandora基准测试中，ProtoMBTI在MBTI的四个二元维度和完整16型任务上均优于基线模型，并表现出强大的跨数据集泛化能力。

**Conclusion:** 研究结果表明，将推理过程与心理学原型推理对齐，提高了基于文本个性建模的准确性、可解释性和迁移性。

**Abstract:** Personality recognition from text is typically cast as hard-label
classification, which obscures the graded, prototype-like nature of human
personality judgments. We present ProtoMBTI, a cognitively aligned framework
for MBTI inference that operationalizes prototype theory within an LLM-based
pipeline. First, we construct a balanced, quality-controlled corpus via
LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).
Next, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative
embeddings and to standardize a bank of personality prototypes. At inference,
we retrieve top-k prototypes for a query post and perform a
retrieve--reuse--revise--retain cycle: the model aggregates prototype evidence
via prompt-based voting, revises when inconsistencies arise, and, upon correct
prediction, retains the sample to continually enrich the prototype library.
Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both
the four MBTI dichotomies and the full 16-type task, and exhibits robust
cross-dataset generalization. Our results indicate that aligning the inference
process with psychological prototype reasoning yields gains in accuracy,
interpretability, and transfer for text-based personality modeling.

</details>


### [3] [ParaScopes: What do Language Models Activations Encode About Future Text?](https://arxiv.org/abs/2511.00180)
*Nicky Pochinkov,Yulia Volkova,Anna Vasileva,Sai V R Chereddy*

Main category: cs.CL

> 研究提出了一种名为残差流解码器的框架，用于检测和理解大型语言模型中关于段落乃至文件级别的规划信息，揭示了模型中可能编码的长期规划信息的解码方法。

<details>
  <summary>Details</summary>

**Motivation:** 随着语言模型的时间跨度变得越来越长，对于理解这些模型中的信息处理和长期规划能力的方法显得相对有限。该项研究的动机在于提供更深入的方法来理解语言模型的长跨度任务处理能力。

**Method:** 采用了残差流解码器框架，研究了小型语言模型中等效于5个以上词汇未来上下文信息的解码方法。

**Result:** 研究发现，通过该框架可以在小型模型中解码出相当于5个以上未来词汇上下文的信息。

**Conclusion:** 这项研究为更好地监控语言模型和理解其如何编码长期规划信息奠定了基础。

**Abstract:** Interpretability studies in language models often investigate forward-looking
representations of activations. However, as language models become capable of
doing ever longer time horizon tasks, methods for understanding activations
often remain limited to testing specific concepts or tokens. We develop a
framework of Residual Stream Decoders as a method of probing model activations
for paragraph-scale and document-scale plans. We test several methods and find
information can be decoded equivalent to 5+ tokens of future context in small
models. These results lay the groundwork for better monitoring of language
models and better understanding how they might encode longer-term planning
information.

</details>


### [4] [Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap](https://arxiv.org/abs/2511.00198)
*Chun-Hao Yang,Bo-Han Feng,Tzu-Yuan Lai,Yan Yu Chen,Yin-Kai Dean Huang,Shou-De Lin*

Main category: cs.CL

> 该研究提出了一种通过预测信息丰富的标记来优化大型语言模型（LLM）训练性能的方法，验证了这种方法在三种任务中的有效性，并推进了对目标标记选择策略的理论理解。

<details>
  <summary>Details</summary>

**Motivation:** 当前大型语言模型在提高模型性能的同时保持计算成本是一个关键挑战。该研究旨在提出一种更有效的方法来提高LLM的训练效率。

**Method:** 该研究提出了一种新的方法——通过预测信息丰富的标记而不是常规的下一个标记来训练大型语言模型，以此改进训练效果。

**Result:** 本文在三种任务——算术运算、文本的多标签分类和自然语言生成中，验证了该方法的有效性。

**Conclusion:** 该研究提供了一种优化大型语言模型训练的方法，既提高了模型性能，也推进了对目标标记选择策略的理论理解。

**Abstract:** Optimizing training performance in large language models (LLMs) remains an
essential challenge, particularly in improving model performance while
maintaining computational costs. This work challenges the conventional approach
of training LLMs using next-token prediction (NTP), arguing that by predicting
information-rich tokens during training, there is a more effective way to train
LLMs. We investigate the impact of the proposed solution in three kinds of
tasks for LLMs: arithmetic, multi-label classification of text, and
natural-language generation. This work offers a principled approach to
optimizing LLM training, advancing both model performance and theoretical
understanding of the target-token selection strategies.

</details>


### [5] [Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2511.00222)
*Marwa Abdulhai,Ryan Cheng,Donovan Clay,Tim Althoff,Sergey Levine,Natasha Jaques*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Large Language Models (LLMs) are increasingly used to simulate human users in
interactive settings such as therapy, education, and social role-play. While
these simulations enable scalable training and evaluation of AI agents,
off-the-shelf LLMs often drift from their assigned personas, contradict earlier
statements, or abandon role-appropriate behavior. We introduce a unified
framework for evaluating and improving persona consistency in LLM-generated
dialogue. We define three automatic metrics: prompt-to-line consistency,
line-to-line consistency, and Q&A consistency, that capture different types of
persona drift and validate each against human annotations. Using these metrics
as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs
for three user roles: a patient, a student, and a social chat partner. Our
method reduces inconsistency by over 55%, resulting in more coherent and
faithful simulated users.

</details>


### [6] [AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding](https://arxiv.org/abs/2511.00265)
*Arman Anwar,Zefang Liu*

Main category: cs.CL

> AgentBnB是一种基于浏览器的网络安全桌面演练解决方案，利用大型语言模型，为学习者提供按需认知性提示，进行单人试点研究取得积极反馈，未来计划添加更多功能扩展。

<details>
  <summary>Details</summary>

**Motivation:** 传统的网络安全桌面演练提供有价值的培训，但往往是脚本化的、资源密集的，并且难以扩展。

**Method:** 引入了AgentBnB，这是一种基于浏览器的Backdoors & Breaches游戏的重新设计版本。它结合了大型语言模型队员和与Bloom对齐的检索增强助手（C2D2）。该系统扩展了精心策划的语料库，提供即时、认知目标性的提示。通过提示工程，代理会根据学习者的信心逐步减少帮助。

**Result:** 在一项涉及四名研究生的单人试点研究中，参与者表示他们更倾向于使用基于代理版本，而不是物理卡片版，并认为它更具可扩展性，尽管在简单的知识测试中有天花板效应出现。

**Conclusion:** 尽管存在样本量小、仅关注单人模式和狭窄语料库的限制，这些初步发现表明，通过大型语言模型增强的桌面演练可以提供轻量级、可重复的练习，而无需传统演练的后勤负担。计划扩展包括多玩家模式、以遥测为基础的教练以及与更大群体的比较研究。

**Abstract:** Traditional cybersecurity tabletop exercises (TTXs) provide valuable training
but are often scripted, resource-intensive, and difficult to scale. We
introduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches
game that integrates large language model teammates with a Bloom-aligned,
retrieval-augmented copilot (C2D2). The system expands a curated corpus into
factual, conceptual, procedural, and metacognitive snippets, delivering
on-demand, cognitively targeted hints. Prompt-engineered agents employ a
scaffolding ladder that gradually fades as learner confidence grows. In a
solo-player pilot with four graduate students, participants reported greater
intention to use the agent-based version compared to the physical card deck and
viewed it as more scalable, though a ceiling effect emerged on a simple
knowledge quiz. Despite limitations of small sample size, single-player focus,
and narrow corpus, these early findings suggest that large language model
augmented TTXs can provide lightweight, repeatable practice without the
logistical burden of traditional exercises. Planned extensions include
multi-player modes, telemetry-driven coaching, and comparative studies with
larger cohorts.

</details>


### [7] [IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval](https://arxiv.org/abs/2511.00268)
*Shounak Paul,Dhananjay Ghumare,Pawan Goyal,Saptarshi Ghosh,Ashutosh Modi*

Main category: cs.CL

> 本文提出了IL-PCR，一个用于法律判例和法规检索的综合语料库，以结合两者间的相关性进行模型开发。实验表明，基于LLM的重排序方法性能最佳。

<details>
  <summary>Details</summary>

**Motivation:** 现有的法律文本检索研究分别处理法规和判例检索任务，缺少一个同时涵盖两者且能利用两者间相关性的综合平台。

**Method:** 开发IL-PCR语料库，使用多种基准模型进行实验，包括词汇模型、语义模型和基于GNN的集成模型，并提出一种基于LLM的重排序方法利用两项任务间的依赖关系。

**Result:** 实验表明，基于LLM的重排序方法在法规和判例检索任务上具有最佳性能。

**Conclusion:** IL-PCR语料库为相关性高的法律文本检索提供了一个整合的平台，基于LLM的重排序方法证明是一种有效的方法。

**Abstract:** Identifying/retrieving relevant statutes and prior cases/precedents for a
given legal situation are common tasks exercised by law practitioners.
Researchers to date have addressed the two tasks independently, thus developing
completely different datasets and models for each task; however, both retrieval
tasks are inherently related, e.g., similar cases tend to cite similar statutes
(due to similar factual situation). In this paper, we address this gap. We
propose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),
which is a unique corpus that provides a common testbed for developing models
for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit
the dependence between the two. We experiment extensively with several baseline
models on the tasks, including lexical models, semantic models and ensemble
based on GNNs. Further, to exploit the dependence between the two tasks, we
develop an LLM-based re-ranking approach that gives the best performance.

</details>


### [8] [POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation](https://arxiv.org/abs/2511.00270)
*Abhinav Joshi,Vaibhav Sharma,Sanjeet Singh,Ashutosh Modi*

Main category: cs.CL

> POSESTITCH-SLT is a novel pre-training method inspired by linguistic-template-based sentence generation, which significantly improves the performance of sign language translation tasks using a simple transformer architecture and achieves state-of-the-art BLEU-4 score improvements on two datasets.

<details>
  <summary>Details</summary>

**Motivation:** The scarcity of large-scale, sentence-aligned datasets for sign language translation presents a significant challenge. The aim is to overcome this issue with a pre-training method that relies on synthetic data generated from templates.

**Method:** POSESTITCH-SLT leverages a template-driven synthetic supervision approach to create sentence pairs for training a transformer-based encoder-decoder architecture designed for sign language translation.

**Result:** The method achieves notable improvements in BLEU-4 scores on the How2Sign (1.97 to 4.56) and iSign (0.55 to 3.43) datasets, surpassing the performance of previous state-of-the-art methods.

**Conclusion:** The study shows that incorporating template-generated sentence pairs into the training set can significantly enhance translation performance in low-resource sign language settings.

**Abstract:** Sign language translation remains a challenging task due to the scarcity of
large-scale, sentence-aligned datasets. Prior arts have focused on various
feature extraction and architectural changes to support neural machine
translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training
scheme that is inspired by linguistic-templates-based sentence generation
technique. With translation comparison on two sign language datasets, How2Sign
and iSign, we show that a simple transformer-based encoder-decoder architecture
outperforms the prior art when considering template-generated sentence pairs in
training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign
and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for
pose-based gloss-free translation. The results demonstrate the effectiveness of
template-driven synthetic supervision in low-resource sign language settings.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [9] [Generative human motion mimicking through feature extraction in denoising diffusion settings](https://arxiv.org/abs/2511.00011)
*Alexander Okupnik,Johannes Schneider,Kyriakos Flouris*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Recent success with large language models has sparked a new wave of verbal
human-AI interaction. While such models support users in a variety of creative
tasks, they lack the embodied nature of human interaction. Dance, as a primal
form of human expression, is predestined to complement this experience. To
explore creative human-AI interaction exemplified by dance, we build an
interactive model based on motion capture (MoCap) data. It generates an
artificial other by partially mimicking and also "creatively" enhancing an
incoming sequence of movement data. It is the first model, which leverages
single-person motion data and high level features in order to do so and, thus,
it does not rely on low level human-human interaction data. It combines ideas
of two diffusion models, motion inpainting, and motion style transfer to
generate movement representations that are both temporally coherent and
responsive to a chosen movement reference. The success of the model is
demonstrated by quantitatively assessing the convergence of the feature
distribution of the generated samples and the test set which serves as
simulating the human performer. We show that our generations are first steps to
creative dancing with AI as they are both diverse showing various deviations
from the human partner while appearing realistic.

</details>


### [10] [Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets](https://arxiv.org/abs/2511.00021)
*Julio Jerison E. Macrohon,Gordon Hung*

Main category: cs.CV

> 本研究开发了一种基于机器学习的珊瑚白化分类系统，通过比较三种顶尖模型，最终确定CNN模型的性能最佳，准确率达到88%，为珊瑚自动监测提供重要见解。

<details>
  <summary>Details</summary>

**Motivation:** 珊瑚礁支撑着众多海洋生物，是防风暴和洪水的重要海岸保护来源，是海洋生态系统的重要组成部分。然而，珊瑚礁面临着污染、海洋酸化和海水温度异常等不断增加的威胁，因此有效保护和监测珊瑚礁显得尤为重要。

**Method:** 本研究提出了一种基于机器学习的珊瑚白化分类系统，该系统基于全球多样化的数据集，包括健康和白化的珊瑚样本，以及不同的环境条件，如深海、沼泽和沿海区域。我们评估并对比了三种最先进的模型：残差神经网络（ResNet）、视觉变压器（ViT）和卷积神经网络（CNN）。

**Result:** 经过全面的超参数调整后，CNN模型达到了最高的88%准确率，超过了现有的基准。

**Conclusion:** 研究结果为自动化的珊瑚监测提供了重要的见解，并对广泛使用的计算机视觉模型进行了全面分析。

**Abstract:** Coral reefs support numerous marine organisms and are an important source of
coastal protection from storms and floods, representing a major part of marine
ecosystems. However coral reefs face increasing threats from pollution, ocean
acidification, and sea temperature anomalies, making efficient protection and
monitoring heavily urgent. Therefore, this study presents a novel
machine-learning-based coral bleaching classification system based on a diverse
global dataset with samples of healthy and bleached corals under varying
environmental conditions, including deep seas, marshes, and coastal zones. We
benchmarked and compared three state-of-the-art models: Residual Neural Network
(ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN).
After comprehensive hyperparameter tuning, the CNN model achieved the highest
accuracy of 88%, outperforming existing benchmarks. Our findings offer
important insights into autonomous coral monitoring and present a comprehensive
analysis of the most widely used computer vision models.

</details>


### [11] [Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline](https://arxiv.org/abs/2511.00022)
*Jules Gerard,Leandro Di Bella,Filip Huyghe,Marc Kochzius*

Main category: cs.CV

> 本文通过基于YOLOv8的深度学习技术，展示了它在西印度洋地区自动鱼类识别上的潜力，作为对传统监控方法的补充。

<details>
  <summary>Details</summary>

**Motivation:** 由于珊瑚礁监测在西印度洋受到水下目视调查所需的劳动需求限制，本文的目的是评估基于深度学习的方法对这一地区进行自动珊瑚礁鱼类监控的潜力。

**Method:** 本文提出了一种基于YOLOv8的深度学习流水线，用于从肯尼亚和坦桑尼亚收集的视频横断面中自动识别鱼类家庭级别的物种。

**Result:** 最佳模型在mAP@0.5指标上达到了0.52，对于数量较多的鱼类家族来说，识别准确度较高，但对于稀有或复杂的物种识别效果相对较弱。

**Conclusion:** 结果表明，深度学习可以作为一种规模化的传统监测方法的补充。

**Abstract:** Coral reef monitoring in the Western Indian Ocean is limited by the labor
demands of underwater visual censuses. This work evaluates a YOLOv8-based deep
learning pipeline for automating family-level fish identification from video
transects collected in Kenya and Tanzania. A curated dataset of 24 families was
tested under different configurations, providing the first region-specific
benchmark for automated reef fish monitoring in the Western Indian Ocean. The
best model achieved mAP@0.5 of 0.52, with high accuracy for abundant families
but weaker detection of rare or complex taxa. Results demonstrate the potential
of deep learning as a scalable complement to traditional monitoring methods.

</details>


### [12] [Mutual Information guided Visual Contrastive Learning](https://arxiv.org/abs/2511.00028)
*Hanyang Chen,Yanchao Yang*

Main category: cs.CV

> 研究提出了一种基于场景中自然扰动下高互信息补丁的对比学习数据增强方法，通过评价多个基准测试，展示了该方法的有效性，为未来的表征学习方向提供了新思路。

<details>
  <summary>Details</summary>

**Motivation:** 尽管使用InfoNCE损失的表示学习方法在减少人工标注方面表现出色，但如果数据选择和增强仍高度依赖人类假设或工程，这种假设可能是次优的。因此，研究提出了基于场景中补丁之间高互信息的数据选择策略，以改善模型在开放环境中的泛化能力。

**Method:** 该方法通过计算自然扰动（如颜色变化和运动）下场景中补丁之间的互信息，选择具有高互信息的补丁作为对学习过程中有用的正样本，利用对比损失实现这一目标。

**Result:** 实验结果展示了该互信息导向的数据增强方法在多个基准上和多种最先进的表示学习框架中的有效性，增强了学习到的特征能够更好的泛化。

**Conclusion:** 此研究确定了提出的数据增强策略作为未来研究的有希望的方向，展示了在开放环境下提高模型表现的潜力。

**Abstract:** Representation learning methods utilizing the InfoNCE loss have demonstrated
considerable capacity in reducing human annotation effort by training invariant
neural feature extractors. Although different variants of the training
objective adhere to the information maximization principle between the data and
learned features, data selection and augmentation still rely on human
hypotheses or engineering, which may be suboptimal. For instance, data
augmentation in contrastive learning primarily focuses on color jittering,
aiming to emulate real-world illumination changes. In this work, we investigate
the potential of selecting training data based on their mutual information
computed from real-world distributions, which, in principle, should endow the
learned features with better generalization when applied in open environments.
Specifically, we consider patches attached to scenes that exhibit high mutual
information under natural perturbations, such as color changes and motion, as
positive samples for learning with contrastive loss. We evaluate the proposed
mutual-information-informed data augmentation method on several benchmarks
across multiple state-of-the-art representation learning frameworks,
demonstrating its effectiveness and establishing it as a promising direction
for future research.

</details>
