<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 8]
- [cs.CV](#cs.CV) [Total: 12]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LLMs for Game Theory: Entropy-Guided In-Context Learning and Adaptive CoT Reasoning](https://arxiv.org/abs/2601.10775)
*Tommaso Felice Banfi,Sashenka Gamage*

Main category: cs.CL

> 研究提出了一种基于大语言模型的自适应推理框架，通过在井字游戏中使用不确定性作为引导，降低了不确定性，并提升了决策质量。

<details>
  <summary>Details</summary>

**Motivation:** 该工作的目的是展示基于大语言模型的自适应推理框架，通过不确定性的指导来优化离散、游戏理论情境下的决策过程。

**Method:** 我们提出了一种基于大语言模型（LLM）的框架，用于解决离散的游戏理论任务，以井字游戏为例。该方法结合了上下文学习、熵引导的推理链和自适应上下文检索。模型根据标记级别的不确定性动态调整检索示例的数量和推理路径：当不确定性较低时，使用简洁的推理和最少的上下文；当不确定性较高时，触发扩展的多路径推理探索。

**Result:** 实验表明，与次优的算法对手相比，熵引导的自适应推理显著提高了决策质量，将平均游戏结果从使用基线LLM时的-11.6%提高到了使用熵引导自适应推理时的+9.5%（在100场比赛中，胜利=+1，平局=0，失败=-1），同时保持较低的比赛中的LLM查询次数。

**Conclusion:** 研究结果表明，不确定性引导的自适应推理可以有效地改善在顺序决策环境中的大语言模型性能。

**Abstract:** We propose a novel LLM-based framework for reasoning in discrete, game-theoretic tasks, illustrated with \emph{Tic-Tac-Toe}. The method integrates in-context learning with entropy-guided chain-of-thought (CoT) reasoning and adaptive context retrieval. The model dynamically adjusts both the number of retrieved examples and reasoning paths according to token-level uncertainty: concise reasoning with minimal context is used when uncertainty is low, whereas higher uncertainty triggers expanded multi-path CoT exploration. Experimental evaluation against a sub-optimal algorithmic opponent shows that entropy-aware adaptive reasoning substantially improves decision quality, increasing the average game outcome from \(-11.6\%\) with the baseline LLM to \(+9.5\%\) with entropy-guided adaptive reasoning over 100 games (win = +1, tie = 0, loss = -1), while maintaining a relatively low number of LLM queries per game. Statistical validation confirms that the improvement is significant, and correlation analysis reveals a negative association between token-level entropy and move optimality. These findings demonstrate that uncertainty-guided adaptive reasoning effectively enhances LLM performance in sequential decision-making environments.

</details>


### [2] [BYOL: Bring Your Own Language Into LLMs](https://arxiv.org/abs/2601.10804)
*Syed Waqas Zamir,Wassim Hamidouche,Boulbaba Ben Amor,Luana Marotti,Inbal Becker-Reshef,Juan Lavista Ferres*

Main category: cs.CL

> 研究提出了一种针对低资源和极度低资源语言的大规模语言模型开发框架BYOL，希望能够解决语言资源严重不平衡的问题，并且已经对某些语言有了显著的改进。

<details>
  <summary>Details</summary>

**Motivation:** 大规模语言模型虽然表现出强大的多语言能力，但全球语言资源的严重不平衡限制了它们的发展，少数语言影响了现代语言模型的训练，导致对少数语言资源和极度低资源语言的系统性表现不足。

**Method:** 提出了一种名为Bring Your Own Language (BYOL) 的统一框架来解决语言资源的不平衡问题。该框架首先将语言分为四个层级（极度低资源、低资源、中资源、高资源），并基于此分类为每一个层级的语言选择适当的集成路径。对于低资源语言，提出了一种完整的数据精炼和扩展流水线。对于极度低资源语言，引入了一种基于翻译的包含路径。

**Result:** 应用这一方法对奇切瓦语和毛利语进行了实验，与强大的多语言基线相比，在12项基准测试中实现了约12%的平均改进。对极度低资源语言因纽特语的研究表明，定制的机器翻译系统提升了4个BLEU分值。

**Conclusion:** BYOL框架能够有效改善低资源和极度低资源语言的大量语言模型的性能，同时保持英语和多语言能力。此外，作者发布了翻译版本的Global MMLU-Lite基准，和代码及模型对公众开放。

**Abstract:** Large Language Models (LLMs) exhibit strong multilingual capabilities, yet remain fundamentally constrained by the severe imbalance in global language resources. While over 7,000 languages are spoken worldwide, only a small subset (fewer than 100) has sufficient digital presence to meaningfully influence modern LLM training. This disparity leads to systematic underperformance, cultural misalignment, and limited accessibility for speakers of low-resource and extreme-low-resource languages. To address this gap, we introduce Bring Your Own Language (BYOL), a unified framework for scalable, language-aware LLM development tailored to each language's digital footprint. BYOL begins with a language resource classification that maps languages into four tiers (Extreme-Low, Low, Mid, High) using curated web-scale corpora, and uses this classification to select the appropriate integration pathway. For low-resource languages, we propose a full-stack data refinement and expansion pipeline that combines corpus cleaning, synthetic text generation, continual pretraining, and supervised finetuning. Applied to Chichewa and Maori, this pipeline yields language-specific LLMs that achieve approximately 12 percent average improvement over strong multilingual baselines across 12 benchmarks, while preserving English and multilingual capabilities via weight-space model merging. For extreme-low-resource languages, we introduce a translation-mediated inclusion pathway, and show on Inuktitut that a tailored machine translation system improves over a commercial baseline by 4 BLEU, enabling high-accuracy LLM access when direct language modeling is infeasible. Finally, we release human-translated versions of the Global MMLU-Lite benchmark in Chichewa, Maori, and Inuktitut, and make our codebase and models publicly available at https://github.com/microsoft/byol .

</details>


### [3] [A Concise Agent is Less Expert: Revealing Side Effects of Using Style Features on Conversational Agents](https://arxiv.org/abs/2601.10809)
*Young-Min Cho,Yuan Yuan,Sharath Chandra Guntuku,Lyle Ungar*

Main category: cs.CL

> 研究阐述了大规模语言模型中不同风格特征间的交互作用，揭示了控制一个风格特征会对其他风格特征产生副作用，呼吁采取多目标的方法对风格进行引导。

<details>
  <summary>Details</summary>

**Motivation:** 目的是研究大型语言模型（LLM）的对话代理中的跨特征风格副作用，这是首次对此进行系统研究。

**Method:** 通过控制合成对话，采用成对的LLM作为评估框架来量化为一种风格特征打分如何因果地影响其他风格特征，研究了任务导向和开放领域的设置。识别出12种常用的风格特征，并测量它们之间的相互影响。

**Result:** 结果显示为一种风格特征打分会显著影响到其他风格特征。例如，打分简洁性会显著减少感知的专业性，表明风格特征是密切相关的而非彼此独立。发现了一些一致且结构化的副作用。

**Conclusion:** 研究挑战了在LLM中对风格忠实控制的假设，指出需要多目标和更为原则的方法来进行安全、标准的风格引导。

**Abstract:** Style features such as friendly, helpful, or concise are widely used in prompts to steer the behavior of Large Language Model (LLM) conversational agents, yet their unintended side effects remain poorly understood. In this work, we present the first systematic study of cross-feature stylistic side effects. We conduct a comprehensive survey of 127 conversational agent papers from ACL Anthology and identify 12 frequently used style features. Using controlled, synthetic dialogues across task-oriented and open domain settings, we quantify how prompting for one style feature causally affects others via a pairwise LLM as a Judge evaluation framework. Our results reveal consistent and structured side effects, such as prompting for conciseness significantly reduces perceived expertise. They demonstrate that style features are deeply entangled rather than orthogonal. To support future research, we introduce CASSE (Conversational Agent Stylistic Side Effects), a dataset capturing these complex interactions. We further evaluate prompt based and activation steering based mitigation strategies and find that while they can partially restore suppressed traits, they often degrade the primary intended style. These findings challenge the assumption of faithful style control in LLMs and highlight the need for multi-objective and more principled approaches to safe, targeted stylistic steering in conversational agents.

</details>


### [4] [Reasoning Models Generate Societies of Thought](https://arxiv.org/abs/2601.10825)
*Junsol Kim,Shiyang Lai,Nino Scherrer,Blaise Agüera y Arcas,James Evans*

Main category: cs.CL

> 研究表明，增强推理能力来自于模拟多智能体互动，其内部差异化视角和专业知识在推理中表现出更大的冲突，这种结构促进了更有效的解决方案探索，与集体智能相似。

<details>
  <summary>Details</summary>

**Motivation:** 研究揭示，增强的推理能力不仅来自于延长计算，还来自于模拟多智能体互动，这种社会性的思考可以使得内部认知视角（具有不同个性特征和领域专业知识）得以分化和辩论。

**Method:** 通过定量分析和机制可解释方法应用于推理轨迹，发现推理模型如DeepSeek-R1和QwQ-32B展示了比指令调优模型更大的观点多样性，激活了异质性人格和专业知识特征之间的更广泛冲突。这些多智能体结构表现在对话行为中，包括问答、视角转换和冲突观点的调和，以及描述尖锐对话的社会情感角色。受控强化学习实验表明，当基础模型仅因推理准确性而获奖时，对话行为会增加，且对话支撑的微调模型优于基础模型。

**Result:** 发现精细调整的模型中对话行为增加，这有助于在推理任务中的准确性优势。

**Conclusion:** 这些发现表明，思想的社会组织可以有效地探索解决方案空间，建立计算与人类群体集体智慧的平行关系，为利用集体智慧的新组织代理提供了机会。

**Abstract:** Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification and debate among internal cognitive perspectives characterized by distinct personality traits and domain expertise. Through quantitative analysis and mechanistic interpretability methods applied to reasoning traces, we find that reasoning models like DeepSeek-R1 and QwQ-32B exhibit much greater perspective diversity than instruction-tuned models, activating broader conflict between heterogeneous personality- and expertise-related features during reasoning. This multi-agent structure manifests in conversational behaviors, including question-answering, perspective shifts, and the reconciliation of conflicting views, and in socio-emotional roles that characterize sharp back-and-forth conversations, together accounting for the accuracy advantage in reasoning tasks. Controlled reinforcement learning experiments reveal that base models increase conversational behaviors when rewarded solely for reasoning accuracy, and fine-tuning models with conversational scaffolding accelerates reasoning improvement over base models. These findings indicate that the social organization of thought enables effective exploration of solution spaces. We suggest that reasoning models establish a computational parallel to collective intelligence in human groups, where diversity enables superior problem-solving when systematically structured, which suggests new opportunities for agent organization to harness the wisdom of crowds.

</details>


### [5] [EncodeRec: An Embedding Backbone for Recommendation Systems](https://arxiv.org/abs/2601.10837)
*Guy Hadad,Neomi Rabaev,Bracha Shapira*

Main category: cs.CL

> 文章提出了EncodeRec方法，用于解决推荐系统中预训练语言模型生成的嵌入所存在的问题，实验表明该方法在多个推荐任务中表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 近年来，推荐系统越来越多地利用大型预训练语言模型（PLMs）的嵌入，但这些嵌入存在两个主要问题：1）PLMs并没有被明确优化以生成结构化和有区分力的嵌入空间；2）它们的表示往往过于通用，无法捕捉推荐任务中至关重要的领域特定语义。因此，提出了EncodeRec以解决这些问题。

**Method:** EncodeRec方法通过从项目描述中直接学习紧凑且信息量大的嵌入，使得文本表示与推荐目标对齐，同时保持语言模型参数不变，从而在不牺牲语义保真度的情况下提高计算效率。

**Result:** 实验结果显示，在包括序列推荐模型骨干和语义ID标记化在内的核心推荐基准上，EncodeRec表现优于基于PLM和嵌入模型的基线，显示出显著的性能提升。

**Conclusion:** 研究结果强调了嵌入调整在填补通用语言模型与实际推荐系统之间差距的关键作用。

**Abstract:** Recent recommender systems increasingly leverage embeddings from large pre-trained language models (PLMs). However, such embeddings exhibit two key limitations: (1) PLMs are not explicitly optimized to produce structured and discriminative embedding spaces, and (2) their representations remain overly generic, often failing to capture the domain-specific semantics crucial for recommendation tasks. We present EncodeRec, an approach designed to align textual representations with recommendation objectives while learning compact, informative embeddings directly from item descriptions. EncodeRec keeps the language model parameters frozen during recommender system training, making it computationally efficient without sacrificing semantic fidelity. Experiments across core recommendation benchmarks demonstrate its effectiveness both as a backbone for sequential recommendation models and for semantic ID tokenization, showing substantial gains over PLM-based and embedding model baselines. These results underscore the pivotal role of embedding adaptation in bridging the gap between general-purpose language models and practical recommender systems.

</details>


### [6] [DialDefer: A Framework for Detecting and Mitigating LLM Dialogic Deference](https://arxiv.org/abs/2601.10896)
*Parisa Rabbani,Priyam Sahoo,Ruben Mathew,Aishee Mondal,Harshita Ketharaman,Nimet Beyza Bozdag,Dilek Hakkani-Tür*

Main category: cs.CL

> 研究发现，大型语言模型在评价对话中发言者时存在对话情境偏差（DialDefer），并提出DDS评分来检测这一问题。

<details>
  <summary>Details</summary>

**Motivation:** 研究大型语言模型（LLMs）在不同话语场景下对同一内容进行不同评价的现象，探索其背后的原因。

**Method:** 通过引入DialDefer框架来检测和缓解由于对话情境导致的判断偏移问题，并提出对话情境偏差评分（DDS）来捕捉这些偏移。

**Result:** 在九个领域，超过3000个实例以及四个模型中，对话情境诱导的判断偏移显著，而准确性保持稳定，表明人类与AI之间归因是产生偏移的主要原因。

**Conclusion:** 大型语言模型在不同语境下的判断偏差是一个校准问题，不仅仅是准确性的问题，试图减轻这种偏移可能会过度校正为怀疑论。

**Abstract:** LLMs are increasingly used as third-party judges, yet their reliability when evaluating speakers in dialogue remains poorly understood. We show that LLMs judge identical claims differently depending on framing: the same content elicits different verdicts when presented as a statement to verify ("Is this statement correct?") versus attributed to a speaker ("Is this speaker correct?"). We call this dialogic deference and introduce DialDefer, a framework for detecting and mitigating these framing-induced judgment shifts. Our Dialogic Deference Score (DDS) captures directional shifts that aggregate accuracy obscures. Across nine domains, 3k+ instances, and four models, conversational framing induces large shifts (|DDS| up to 87pp, p < .0001) while accuracy remains stable (<2pp), with effects amplifying 2-4x on naturalistic Reddit conversations. Models can shift toward agreement (deference) or disagreement (skepticism) depending on domain -- the same model ranges from DDS = -53 on graduate-level science to +58 on social judgment. Ablations reveal that human-vs-LLM attribution drives the largest shifts (17.7pp swing), suggesting models treat disagreement with humans as more costly than with AI. Mitigation attempts reduce deference but can over-correct into skepticism, framing this as a calibration problem beyond accuracy optimization.

</details>


### [7] [Neural Induction of Finite-State Transducers](https://arxiv.org/abs/2601.10918)
*Michael Ginn,Alexis Palmer,Mans Hulden*

Main category: cs.CL

> 提出了一种新的方法，可以自动构建未加权的有限状态转换器，其性能在多个数据集上远超传统算法。

<details>
  <summary>Details</summary>

**Motivation:** 手工构建有限状态转换器非常困难，这种方法旨在实现自动化的构建过程。

**Method:** 采用循环神经网络学习的隐藏状态几何图形来自动构建未加权的有限状态转换器。

**Result:** 在形态变化、音素预测和历史规范化的实际数据集上进行了评估，显示构建的有限状态转换器具有高度准确性和鲁棒性。

**Conclusion:** 该方法生成的有限状态转换器在保留了高精度的同时，相比传统学习算法有明显的性能提升。

**Abstract:** Finite-State Transducers (FSTs) are effective models for string-to-string rewriting tasks, often providing the efficiency necessary for high-performance applications, but constructing transducers by hand is difficult. In this work, we propose a novel method for automatically constructing unweighted FSTs following the hidden state geometry learned by a recurrent neural network. We evaluate our methods on real-world datasets for morphological inflection, grapheme-to-phoneme prediction, and historical normalization, showing that the constructed FSTs are highly accurate and robust for many datasets, substantially outperforming classical transducer learning algorithms by up to 87% accuracy on held-out test sets.

</details>


### [8] [Massively Multilingual Joint Segmentation and Glossing](https://arxiv.org/abs/2601.10925)
*Michael Ginn,Lindia Tjuatja,Enora Rice,Ali Marashian,Maria Valentini,Jasmine Xu,Graham Neubig,Alexis Palmer*

Main category: cs.CL

> 本研究通过改进的神经网络模型PolyGloss，提高了语言文档化工作的准确性、可解释性和实用性。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的主要动机是改进神经网络模型在语言文档化工作中的实用性，尤其是在真实世界的场景下。通过改进模型的预测准确性和可解释性，提高人类注解员的信任度。

**Method:** 本研究采用了神经网络模型，旨在解决现有模型在预测词形分割边界方面的不足，并首次进行了联合预测词间注释和词形分割的研究。研究团队通过扩展GlossLM的训练语料库，对PolyGloss模型进行了预训练，该模型是一种seq2seq的多语言模型，用于联合词形分割和注释预测。

**Result:** 实验结果表明，PolyGloss模型在注释预测方面优于GlossLM，并在词形分割和对齐方面超越了多种开源大语言模型。此外，研究还表明PolyGloss模型可以通过低秩适应快速适应新的数据集。

**Conclusion:** 结论是，PolyGloss模型在联合词形分割和注释预测方面表现出色，能够满足更多真实的语言文档化需求，且具有良好的可扩展性，可以通过低秩适应快速调整到新的数据集上。这为语言文档化工作提供了一种新的解决思路。

**Abstract:** Automated interlinear gloss prediction with neural networks is a promising approach to accelerate language documentation efforts. However, while state-of-the-art models like GlossLM achieve high scores on glossing benchmarks, user studies with linguists have found critical barriers to the usefulness of such models in real-world scenarios. In particular, existing models typically generate morpheme-level glosses but assign them to whole words without predicting the actual morpheme boundaries, making the predictions less interpretable and thus untrustworthy to human annotators.
  We conduct the first study on neural models that jointly predict interlinear glosses and the corresponding morphological segmentation from raw text. We run experiments to determine the optimal way to train models that balance segmentation and glossing accuracy, as well as the alignment between the two tasks. We extend the training corpus of GlossLM and pretrain PolyGloss, a family of seq2seq multilingual models for joint segmentation and glossing that outperforms GlossLM on glossing and beats various open-source LLMs on segmentation, glossing, and alignment. In addition, we demonstrate that PolyGloss can be quickly adapted to a new dataset via low-rank adaptation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [9] [Future Optical Flow Prediction Improves Robot Control & Video Generation](https://arxiv.org/abs/2601.10781)
*Kanchana Ranasinghe,Honglu Zhou,Yu Fang,Luyu Yang,Le Xue,Ran Xu,Caiming Xiong,Silvio Savarese,Michael S Ryoo,Juan Carlos Niebles*

Main category: cs.CV

> FOFPred模型结合了VLM和Diffusion架构，实现了基于大规模人类活动数据的未来光流预测，表现出跨域适用性。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于解决从真实世界的噪声数据中学习可泛化的空间密集型运动表示是一项关键挑战，并为此提出了统一的VLM和扩散架构。

**Method:** 本文介绍了一种新颖的语言条件下的光流预测模型FOFPred，该模型结合了视觉语言模型（VLM）和扩散模型，以实现强大的多模态推理和像素级生成保真度。

**Result:** FOFPred模型在机器人操作和语言驱动的视频生成中进行了评估，证明了其跨域适用性和大规模网络数据学习的价值。

**Conclusion:** 研究结果确认了VLM-Diffusion架构和多样化网络数据规模化学习对未来光流预测的价值。

**Abstract:** Future motion representations, such as optical flow, offer immense value for control and generative tasks. However, forecasting generalizable spatially dense motion representations remains a key challenge, and learning such forecasting from noisy, real-world data remains relatively unexplored. We introduce FOFPred, a novel language-conditioned optical flow forecasting model featuring a unified Vision-Language Model (VLM) and Diffusion architecture. This unique combination enables strong multimodal reasoning with pixel-level generative fidelity for future motion prediction. Our model is trained on web-scale human activity data-a highly scalable but unstructured source. To extract meaningful signals from this noisy video-caption data, we employ crucial data preprocessing techniques and our unified architecture with strong image pretraining. The resulting trained model is then extended to tackle two distinct downstream tasks in control and generation. Evaluations across robotic manipulation and video generation under language-driven settings establish the cross-domain versatility of FOFPred, confirming the value of a unified VLM-Diffusion architecture and scalable learning from diverse web data for future optical flow prediction.

</details>


### [10] [ICONIC-444: A 3.1-Million-Image Dataset for OOD Detection Research](https://arxiv.org/abs/2601.10802)
*Gerhard Krumpl,Henning Avenhaus,Horst Possegger*

Main category: cs.CV

> This paper presents ICONIC-444, a specialized, large-scale dataset for enhancing research in out-of-distribution (OOD) detection in industrial settings, with over 3.1 million images and 444 classes, to support comprehensive evaluation across varying task complexities and OOD levels.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is the lack of high-quality datasets that can support comprehensive OOD detection research, especially for both fine- and coarse-grained tasks.ICONIC-444 aims to fill this gap by providing a rich and challenging dataset.

**Method:** The paper introduces ICONIC-444, a large-scale industrial image dataset designed for OOD detection research, containing over 3.1 million images across 444 classes, capturing varying levels of OOD complexity.

**Result:** The authors provide baseline results for 22 state-of-the-art post-hoc OOD detection methods on the ICONIC-444 dataset, setting a benchmark for future research.

**Conclusion:** The conclusion of this paper is that ICONIC-444 offers a valuable resource for advancing OOD detection research by providing a large, diverse, and structured dataset with clear OOD categories and varying complexity levels.

**Abstract:** Current progress in out-of-distribution (OOD) detection is limited by the lack of large, high-quality datasets with clearly defined OOD categories across varying difficulty levels (near- to far-OOD) that support both fine- and coarse-grained computer vision tasks. To address this limitation, we introduce ICONIC-444 (Image Classification and OOD Detection with Numerous Intricate Complexities), a specialized large-scale industrial image dataset containing over 3.1 million RGB images spanning 444 classes tailored for OOD detection research. Captured with a prototype industrial sorting machine, ICONIC-444 closely mimics real-world tasks. It complements existing datasets by offering structured, diverse data suited for rigorous OOD evaluation across a spectrum of task complexities. We define four reference tasks within ICONIC-444 to benchmark and advance OOD detection research and provide baseline results for 22 state-of-the-art post-hoc OOD detection methods.

</details>


### [11] [A Unified 3D Object Perception Framework for Real-Time Outside-In Multi-Camera Systems](https://arxiv.org/abs/2601.10819)
*Yizhou Wang,Sameer Pusegaonkar,Yuxing Wang,Anqi Li,Vishal Kumar,Chetan Sethi,Ganapathy Aiyer,Yun He,Kartikay Thakkar,Swapnil Rathi,Bhushan Rupde,Zheng Tang,Sujit Biswas*

Main category: cs.CV

> 本文针对大型基础设施环境，提出了一种优化的Sparse4D框架，通过绝对世界坐标几何先验和遮挡感知的ReID嵌入模块来处理遮挡和异构摄像头布置问题。使用NVIDIA COSMOS框架的生成数据增强策略以增强模型的环境适应能力。

<details>
  <summary>Details</summary>

**Motivation:** 准确的3D物体感知和多目标多摄像头（MTMC）跟踪对于实现工业基础设施的数字化至关重要。但是将“由内向外”的自动驾驶模型转化为“由外向内”的静态摄像头网络时，由于异构摄像头布置和极端遮挡问题，转型面临巨大挑战。

**Method:** 本文提出了一种专门针对大型基础设施环境优化的Sparse4D框架。系统利用绝对世界坐标几何先验，并引入了一种遮挡感知的ReID嵌入模块，以在分布式传感器网络中保持身份稳定性。为了跨越Sim2Real领域差距而不进行手动标注，我们采用了一种基于NVIDIA COSMOS框架的生成数据增强策略，创造出多样化的环境风格，从而增强模型的外观不变性。此外，我们通过开发Multi-Scale Deformable Aggregation (MSDA)的优化TensorRT插件来解决实时部署约束问题。硬件加速的实现使得在现代GPU架构中速度提升了2.15倍，一个Blackwell级GPU能够支持超过64个并发摄像头流。

**Result:** 在2025年的AI City Challenge基准测试中，我们的仅摄像头框架达到了45.22的HOTA分数，成为该领域的最先进水平。硬件加速的实现使得在现代GPU架构中的速度提升了2.15倍。

**Conclusion:** 通过引入遮挡感知的ReID嵌入模块与生成式数据增强策略，我们的Sparse4D框架在大型基础设施环境中的多目标多摄像头跟踪任务中取得了优异的性能，同时实现了高效的实时处理能力。

**Abstract:** Accurate 3D object perception and multi-target multi-camera (MTMC) tracking are fundamental for the digital transformation of industrial infrastructure. However, transitioning "inside-out" autonomous driving models to "outside-in" static camera networks presents significant challenges due to heterogeneous camera placements and extreme occlusion. In this paper, we present an adapted Sparse4D framework specifically optimized for large-scale infrastructure environments. Our system leverages absolute world-coordinate geometric priors and introduces an occlusion-aware ReID embedding module to maintain identity stability across distributed sensor networks. To bridge the Sim2Real domain gap without manual labeling, we employ a generative data augmentation strategy using the NVIDIA COSMOS framework, creating diverse environmental styles that enhance the model's appearance-invariance. Evaluated on the AI City Challenge 2025 benchmark, our camera-only framework achieves a state-of-the-art HOTA of $45.22$. Furthermore, we address real-time deployment constraints by developing an optimized TensorRT plugin for Multi-Scale Deformable Aggregation (MSDA). Our hardware-accelerated implementation achieves a $2.15\times$ speedup on modern GPU architectures, enabling a single Blackwell-class GPU to support over 64 concurrent camera streams.

</details>


### [12] [Can Vision-Language Models Understand Construction Workers? An Exploratory Study](https://arxiv.org/abs/2601.10835)
*Hieu Bui,Nathaniel E. Chodosh,Arash Tavakoli*

Main category: cs.CV

> 研究评估了VLMs对建筑工地工人行为和情感识别的性能，GPT-4o表现最佳，但在语义相近类别上仍存在困难。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于数据标签稀缺以及工作状态和情感监控的需求，VLMs作为一种无需大量领域特定训练就能识别人类行为的工具，在建筑施工现场展现出巨大的潜力。

**Method:** 使用1000张标注了行为和情感的工地图像数据集，考察了GPT-4o、Florence 2和LLaVa-1.5三个VLM模型的性能。

**Result:** GPT-4o在行为和情感识别上均得分最高，分别为F1-score 0.756和0.712，准确率分别为0.799和0.773；Florence 2和LLaVa-1.5表现较差。

**Conclusion:** 虽然VLMs在建筑工地的初步表现不错，但未来还需改进如领域适应和多模态感知以提高其在现实场景中的可靠性。

**Abstract:** As robotics become increasingly integrated into construction workflows, their ability to interpret and respond to human behavior will be essential for enabling safe and effective collaboration. Vision-Language Models (VLMs) have emerged as a promising tool for visual understanding tasks and offer the potential to recognize human behaviors without extensive domain-specific training. This capability makes them particularly appealing in the construction domain, where labeled data is scarce and monitoring worker actions and emotional states is critical for safety and productivity. In this study, we evaluate the performance of three leading VLMs, GPT-4o, Florence 2, and LLaVa-1.5, in detecting construction worker actions and emotions from static site images. Using a curated dataset of 1,000 images annotated across ten action and ten emotion categories, we assess each model's outputs through standardized inference pipelines and multiple evaluation metrics. GPT-4o consistently achieved the highest scores across both tasks, with an average F1-score of 0.756 and accuracy of 0.799 in action recognition, and an F1-score of 0.712 and accuracy of 0.773 in emotion recognition. Florence 2 performed moderately, with F1-scores of 0.497 for action and 0.414 for emotion, while LLaVa-1.5 showed the lowest overall performance, with F1-scores of 0.466 for action and 0.461 for emotion. Confusion matrix analyses revealed that all models struggled to distinguish semantically close categories, such as collaborating in teams versus communicating with supervisors. While the results indicate that general-purpose VLMs can offer a baseline capability for human behavior recognition in construction environments, further improvements, such as domain adaptation, temporal modeling, or multimodal sensing, may be needed for real-world reliability.

</details>


### [13] [One Model, Many Behaviors: Training-Induced Effects on Out-of-Distribution Detection](https://arxiv.org/abs/2601.10836)
*Gerhard Krumpl,Henning Avenhaus,Horst Possegger*

Main category: cs.CV

> 本研究调查了现代训练策略下ID准确性和OOD检测性能之间的关系，使用广泛的实验方法和数据集基准测试，发现这些之间存在复杂的非单调关系，并没有单一最优的OOD检测方案。

<details>
  <summary>Details</summary>

**Motivation:** 尽管在OOD检测方面取得了稳步进展，但当前研究对现代训练管道（旨在最大化ID准确性和泛化能力）与OOD检测器之间的相互作用探索不足。研究的主要动机是探索这种关系。

**Method:** 该研究通过一个全面的实验研究，固定采用广泛使用的ResNet-50架构，对21种最先进的后验OOD检测方法进行了基准测试，这些方法是在56个通过不同训练策略获得的ImageNet训练模型基础上评估的，并在八个OOD测试集上进行了测试。

**Result:** 研究揭示了OOD检测性能与ID准确性的非单调关系，发现了训练策略与OOD检测器选择之间的强关联性。

**Conclusion:** 研究发现，更高的ID准确性并不一定意味着更好的OOD检测性能，这种关系是非单调的：OOD性能随着准确性提高而提高，但是一旦先进的训练方法将准确性提高到超过基线水平，OOD性能反而下降。此外，训练策略、检测器选择和最终的OOD性能之间存在很强的相互依赖性，表明没有单一的方法在所有情况下都是最优的。

**Abstract:** Out-of-distribution (OOD) detection is crucial for deploying robust and reliable machine-learning systems in open-world settings. Despite steady advances in OOD detectors, their interplay with modern training pipelines that maximize in-distribution (ID) accuracy and generalization remains under-explored. We investigate this link through a comprehensive empirical study. Fixing the architecture to the widely adopted ResNet-50, we benchmark 21 post-hoc, state-of-the-art OOD detection methods across 56 ImageNet-trained models obtained via diverse training strategies and evaluate them on eight OOD test sets. Contrary to the common assumption that higher ID accuracy implies better OOD detection performance, we uncover a non-monotonic relationship: OOD performance initially improves with accuracy but declines once advanced training recipes push accuracy beyond the baseline. Moreover, we observe a strong interdependence between training strategy, detector choice, and resulting OOD performance, indicating that no single method is universally optimal.

</details>


### [14] [Effects of Different Attention Mechanisms Applied on 3D Models in Video Classification](https://arxiv.org/abs/2601.10854)
*Mohammad Rasras,Iuliana Marin,Serban Radu,Irina Mocanu*

Main category: cs.CV

> The paper examines 3D Resnet-based models' performance when temporal data knowledge is reduced and frame resolution is increased, with attention mechanisms added. The best result achieved 88.98% accuracy on UCF101 with multi-headed attention in R(2+1)D.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to understand the effect of changing temporal data resolution on action recognition performance while enhancing the models with attention mechanisms.

**Method:** This paper investigates 3D Resnet-based CNN models to explore the impact of reducing temporal data knowledge while increasing frame resolution. It designs three variations (MC3, R3D, R(2+1)D), each with a dropout layer and ten new versions incorporating special blocks such as CBAM, TCN, multi-headed, and channel attention mechanisms.

**Result:** Testing the models on UCF101 yielded an accuracy of 88.98% for the modified R(2+1)D model with added multiheaded attention. The models showed differing class-level accuracy despite similar enhancements.

**Conclusion:** The study concludes that the loss of temporal features significantly affects the performance of high-resolution models. Class-level accuracy varied among the modified models.

**Abstract:** Human action recognition has become an important research focus in computer vision due to the wide range of applications where it is used. 3D Resnet-based CNN models, particularly MC3, R3D, and R(2+1)D, have different convolutional filters to extract spatiotemporal features. This paper investigates the impact of reducing the captured knowledge from temporal data, while increasing the resolution of the frames. To establish this experiment, we created similar designs to the three originals, but with a dropout layer added before the final classifier. Secondly, we then developed ten new versions for each one of these three designs. The variants include special attention blocks within their architecture, such as convolutional block attention module (CBAM), temporal convolution networks (TCN), in addition to multi-headed and channel attention mechanisms. The purpose behind that is to observe the extent of the influence each of these blocks has on performance for the restricted-temporal models. The results of testing all the models on UCF101 have shown accuracy of 88.98% for the variant with multiheaded attention added to the modified R(2+1)D. This paper concludes the significance of missing temporal features in the performance of the newly created increased resolution models. The variants had different behavior on class-level accuracy, despite the similarity of their enhancements to the overall performance.

</details>


### [15] [Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation](https://arxiv.org/abs/2601.10880)
*Chongcong Jiang,Tianxingjian Ding,Chuhan Song,Jiachen Tu,Ziyang Yan,Yihua Shao,Zhenyi Wang,Yuzhang Shang,Tianyu Han,Yu Tian*

Main category: cs.CV

> Medical SAM3 是通过在大规模异构2D和3D医学影像数据集上进行全面微调而获得的基础模型，适用于通用提示驱动的医学影像分割任务。研究表明，直接使用SAM3进行医学影像分割效果较差，这促使我们进行全面模型适应，而不仅仅是提示工程，从而显著提升了分割效果，尤其是在复杂情况下表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 原版的SAM3在医学影像分割上的性能显著下降，主要依赖于强几何先验，并且在面对具有语义模糊、复杂形态和长范围3D上下文的任务时表现不佳，因此需要进行全面的模型优化以适应这一特定领域。

**Method:** 通过对33个跨越10种医学影像模式的数据集进行全面微调来获取Medical SAM3。这种方法即保持了提示驱动的灵活性，同时又获得了稳健的领域特定表示。

**Result:** 在器官、成像模式和维度的广泛实验展示出一致且显著的性能提升，特别是在语义模糊、复杂形态和长范围3D上下文的挑战场景下表现优异，确立了Medical SAM3作为医学成像领域的通用文本引导分割基础模型的地位。

**Conclusion:** 通过跨域进行模型参数微调，Medical SAM3 成为医学影像普适的提示驱动分割基础模型，强调了在严重域偏移下获取鲁棒的提示驱动分割的综合模型适应的重要性。

**Abstract:** Promptable segmentation foundation models such as SAM3 have demonstrated strong generalization capabilities through interactive and concept-based prompting. However, their direct applicability to medical image segmentation remains limited by severe domain shifts, the absence of privileged spatial prompts, and the need to reason over complex anatomical and volumetric structures. Here we present Medical SAM3, a foundation model for universal prompt-driven medical image segmentation, obtained by fully fine-tuning SAM3 on large-scale, heterogeneous 2D and 3D medical imaging datasets with paired segmentation masks and text prompts. Through a systematic analysis of vanilla SAM3, we observe that its performance degrades substantially on medical data, with its apparent competitiveness largely relying on strong geometric priors such as ground-truth-derived bounding boxes. These findings motivate full model adaptation beyond prompt engineering alone. By fine-tuning SAM3's model parameters on 33 datasets spanning 10 medical imaging modalities, Medical SAM3 acquires robust domain-specific representations while preserving prompt-driven flexibility. Extensive experiments across organs, imaging modalities, and dimensionalities demonstrate consistent and significant performance gains, particularly in challenging scenarios characterized by semantic ambiguity, complex morphology, and long-range 3D context. Our results establish Medical SAM3 as a universal, text-guided segmentation foundation model for medical imaging and highlight the importance of holistic model adaptation for achieving robust prompt-driven segmentation under severe domain shift. Code and model will be made available at https://github.com/AIM-Research-Lab/Medical-SAM3.

</details>


### [16] [FrankenMotion: Part-level Human Motion Generation and Composition](https://arxiv.org/abs/2601.10909)
*Chuqiao Li,Xianghui Xie,Yong Cao,Andreas Geiger,Gerard Pons-Moll*

Main category: cs.CV

> 引入了FrankenMotion，一种新的基于扩散模型的运动生成框架，该模型能通过细粒度文本提示控制身体各部位运动。该模型基于一个使用大型语言模型生成的时间感知的、细粒度部分级注释的数据集。

<details>
  <summary>Details</summary>

**Motivation:** 现有的文本提示生成人体运动的方法主要依赖于序列级或动作级描述，这限制了对单个身体部分的控制能力。为了解决这个问题，提出了一种新的方法。

**Method:** 构建了一个高质量的运动数据集，该数据集使用大型语言模型（LLMs）的能力，提供了原子级、时间感知的细粒度部分文本注释。基于此数据集，提出了一个扩散式感知部位的运动生成框架——FrankenMotion。每个身体部位都由其自己的时间结构化的文本提示引导。

**Result:** 实验表明，FrankenMotion 在重新训练和调整的基线模型上表现出了更好的性能，并且该模型可以在训练过程中生成未见过的动作组合。

**Conclusion:** 这是第一个提供原子级别、时间感知部分级运动注释，并且能够控制空间（身体部位）和时间（原子动作）来生成运动的模型。

**Abstract:** Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct a high-quality motion dataset with atomic, temporally-aware part-level text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce a diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have a model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication.

</details>


### [17] [Classification of Chest XRay Diseases through image processing and analysis techniques](https://arxiv.org/abs/2601.10913)
*Santiago Martínez Novoa,María Catalina Ibáñez,Lina Gómez Mesa,Jeremias Kramer*

Main category: cs.CV

> 本文比较分析了多种方法用于多分类胸部X光图像的诊断，并开发在线应用，探讨了方法的缺点和未来改进方向。

<details>
  <summary>Details</summary>

**Motivation:** 多分类胸部X光图像在诊断胸部疾病方面应用广泛，本文旨在通过多种方法提升其诊断准确性，并开发易于使用的应用程序。

**Method:** 本研究使用DenseNet121模型，并开发了一个开源的基于网络的应用程序，以便进行实用性和准确度的测试。

**Result:** 本文通过比较几种方法，包括DenseNet121，对多分类胸部X光图像进行了研究。此外，研究中还开发了一个基于网络的开源应用程序，并对提出的方法进行了弱点分析，提出了未来的改进方向。代码可在指定的GitHub仓库中找到。

**Conclusion:** 研究比较了包括DenseNet121在内的多种方法，并开发了一个基于网络的开源应用程序，体现了方法的应用效果，同时指出了需要改进的地方。

**Abstract:** Multi-Classification Chest X-Ray Images are one of the most prevalent forms of radiological examination used for diagnosing thoracic diseases. In this study, we offer a concise overview of several methods employed for tackling this task, including DenseNet121. In addition, we deploy an open-source web-based application. In our study, we conduct tests to compare different methods and see how well they work. We also look closely at the weaknesses of the methods we propose and suggest ideas for making them better in the future. Our code is available at: https://github.com/AML4206-MINE20242/Proyecto_AML

</details>


### [18] [Self-learned representation-guided latent diffusion model for breast cancer classification in deep ultraviolet whole surface images](https://arxiv.org/abs/2601.10917)
*Pouya Afshin,David Helminiak,Tianling Niu,Julie M. Jorns,Tina Yen,Bing Yu,Dong Hye Ye*

Main category: cs.CV

> 本研究提出了一种自监督学习（SSL）引导的潜扩散模型（LDM），以深度紫外荧光扫描显微镜（DUV-FSM）产生的乳腺组织数据为基础，生成高质量合成训练片段，有助于提升BCS手术中的术中边缘评估准确性。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在解决深紫外荧光扫描显微镜（DUV-FSM）在乳腺保留手术（BCS）中进行术中边缘评估时，由于标注数据稀缺而难以训练鲁棒深度学习模型的问题。

**Method:** SSL-guided Latent Diffusion Model结合自监督学习（SSL）引导的潜扩散模型（LDM）生成高质量的合成训练片段，并使用带有DINO教师模型的编码来注入细胞结构的丰富语义细节。通过结合真实和合成补丁来微调Vision Transformer（ViT），并利用补丁预测聚合进行WSI级别分类。

**Result:** 实验使用5折交叉验证表明，该方法实现了96.47％的准确率，并将FID评分降低到45.72，显著优于基于类条件的基线。

**Conclusion:** 研究提出的方法通过生成高质量的合成训练数据并结合真实数据来提高对乳腺组织边缘评估的准确性，为术中决策提供了可靠的辅助工具。

**Abstract:** Breast-Conserving Surgery (BCS) requires precise intraoperative margin assessment to preserve healthy tissue. Deep Ultraviolet Fluorescence Scanning Microscopy (DUV-FSM) offers rapid, high-resolution surface imaging for this purpose; however, the scarcity of annotated DUV data hinders the training of robust deep learning models. To address this, we propose an Self-Supervised Learning (SSL)-guided Latent Diffusion Model (LDM) to generate high-quality synthetic training patches. By guiding the LDM with embeddings from a fine-tuned DINO teacher, we inject rich semantic details of cellular structures into the synthetic data. We combine real and synthetic patches to fine-tune a Vision Transformer (ViT), utilizing patch prediction aggregation for WSI-level classification. Experiments using 5-fold cross-validation demonstrate that our method achieves 96.47 % accuracy and reduces the FID score to 45.72, significantly outperforming class-conditioned baselines.

</details>


### [19] [RobuMTL: Enhancing Multi-Task Learning Robustness Against Weather Conditions](https://arxiv.org/abs/2601.10921)
*Tasneem Shaffee,Sherief Reda*

Main category: cs.CV

> 提出了一种名为RobuMTL的新架构，该架构利用动态任务特定分层LoRA模块和LoRA专家小组，在恶劣天气条件下大幅提高了自动驾驶系统模型的鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 自动驾驶系统在现实世界环境中运行时，恶劣天气条件会严重影响模型的性能和可靠性。为了提高在这种条件下的模型鲁棒性，研究者提出了RobuMTL方法。

**Method:** 介绍了一种名为RobuMTL的新架构，它通过根据输入扰动动态选择任务特定的分层LoRA模块和LoRA专家小组来适应视觉退化。框架可以在混合专家模式下工作，以实现基于输入特性的自适应专业化，提高模型在多种现实世界条件下鲁棒性。

**Result:** 在PASCAL和NYUD-v2数据集上评估了该框架。在PASCAL基准测试中，RobuMTL在单一扰动下平均相对改进+2.8％，在混合天气条件下相比MTL基线最高改进+44.4％。在NYUD-v2上，RobuMTL实现了跨任务的+9.7％平均相对改进。

**Conclusion:** 与单任务模型，标准多任务学习基线和最先进方法相比，RobuMTL在恶劣天气条件下展示了更好的鲁棒性和性能。

**Abstract:** Robust Multi-Task Learning (MTL) is crucial for autonomous systems operating in real-world environments, where adverse weather conditions can severely degrade model performance and reliability. In this paper, we introduce RobuMTL, a novel architecture designed to adaptively address visual degradation by dynamically selecting task-specific hierarchical Low-Rank Adaptation (LoRA) modules and a LoRA expert squad based on input perturbations in a mixture-of-experts fashion. Our framework enables adaptive specialization based on input characteristics, improving robustness across diverse real-world conditions. To validate our approach, we evaluated it on the PASCAL and NYUD-v2 datasets and compared it against single-task models, standard MTL baselines, and state-of-the-art methods. On the PASCAL benchmark, RobuMTL delivers a +2.8% average relative improvement under single perturbations and up to +44.4% under mixed weather conditions compared to the MTL baseline. On NYUD-v2, RobuMTL achieves a +9.7% average relative improvement across tasks. The code is available at GitHub.

</details>


### [20] [Sparse Data Tree Canopy Segmentation: Fine-Tuning Leading Pretrained Models on Only 150 Images](https://arxiv.org/abs/2601.10931)
*David Szczecina,Hudson Sun,Anthony Bertnyk,Niloofar Azad,Kyle Gao,Lincoln Linlin Xu*

Main category: cs.CV

> 在极度数据稀缺的情况下，基于卷积的预训练模型YOLOv11和Mask R-CNN相较于基于Transformer的模型表现更佳。

<details>
  <summary>Details</summary>

**Motivation:** 本文研究的动机来源于数据标注稀缺的情况，尤其是在处理来自空中影像的树冠检测任务时。环境监测、城市规划和生态系统分析等领域都需要这一技术。

**Method:** 本研究评估了五种代表性架构（YOLOv11、Mask R-CNN、DeepLabv3、Swin-UNet 和 DINOv2），用于在极度数据稀缺的情况下进行树冠分割的适用性。

**Result:** 实验结果显示，基于卷积的预训练模型YOLOv11和Mask R-CNN在极端稀缺数据的条件下表现明显优于基于Transformer的模型。

**Conclusion:** 基于卷积的轻量级模型在少量数据上的树冠检测任务上更为可靠，而基于Transformer的架构在没有大量预训练或数据增强的情况下难以应对少量数据的情况。

**Abstract:** Tree canopy detection from aerial imagery is an important task for environmental monitoring, urban planning, and ecosystem analysis. Simulating real-life data annotation scarcity, the Solafune Tree Canopy Detection competition provides a small and imbalanced dataset of only 150 annotated images, posing significant challenges for training deep models without severe overfitting. In this work, we evaluate five representative architectures, YOLOv11, Mask R-CNN, DeepLabv3, Swin-UNet, and DINOv2, to assess their suitability for canopy segmentation under extreme data scarcity. Our experiments show that pretrained convolution-based models, particularly YOLOv11 and Mask R-CNN, generalize significantly better than pretrained transformer-based models. DeeplabV3, Swin-UNet and DINOv2 underperform likely due to differences between semantic and instance segmentation tasks, the high data requirements of Vision Transformers, and the lack of strong inductive biases. These findings confirm that transformer-based architectures struggle in low-data regimes without substantial pretraining or augmentation and that differences between semantic and instance segmentation further affect model performance. We provide a detailed analysis of training strategies, augmentation policies, and model behavior under the small-data constraint and demonstrate that lightweight CNN-based methods remain the most reliable for canopy detection on limited imagery.

</details>
