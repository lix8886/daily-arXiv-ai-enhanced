{"id": "2507.02088", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02088", "abs": "https://arxiv.org/abs/2507.02088", "authors": ["Tian Lan", "Xiangdong Su", "Xu Liu", "Ruirui Wang", "Ke Chang", "Jiang Li", "Guanglai Gao"], "title": "McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models", "comment": "24 pages, 9 figures", "summary": "As large language models (LLMs) are increasingly applied to various NLP\ntasks, their inherent biases are gradually disclosed. Therefore, measuring\nbiases in LLMs is crucial to mitigate its ethical risks. However, most existing\nbias evaluation datasets focus on English and North American culture, and their\nbias categories are not fully applicable to other cultures. The datasets\ngrounded in the Chinese language and culture are scarce. More importantly,\nthese datasets usually only support single evaluation tasks and cannot evaluate\nthe bias from multiple aspects in LLMs. To address these issues, we present a\nMulti-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias\nevaluation instances, covering 12 single bias categories, 82 subcategories and\nintroducing 5 evaluation tasks, providing extensive category coverage, content\ndiversity, and measuring comprehensiveness. Additionally, we evaluate several\npopular LLMs from different series and with parameter sizes. In general, all\nthese LLMs demonstrated varying degrees of bias. We conduct an in-depth\nanalysis of results, offering novel insights into bias in LLMs.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.02145", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02145", "abs": "https://arxiv.org/abs/2507.02145", "authors": ["Keyan Jin", "Yapeng Wang", "Leonel Santos", "Tao Fang", "Xu Yang", "Sio Kei Im", "Hugo Gonçalo Oliveira"], "title": "Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization", "comment": null, "summary": "Dialogue summarization is a challenging task with significant practical value\nin customer service, meeting analysis, and conversational AI. Although large\nlanguage models (LLMs) have achieved substantial progress in summarization\ntasks, the performance of step-by-step reasoning architectures-specifically\nLong Chain-of-Thought (CoT) implementations such as OpenAI-o1 and\nDeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent\nabstraction and conciseness. In this work, we present the first comprehensive\nand systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning\nLLMs across three major paradigms-generic, role-oriented, and query-oriented\ndialogue summarization. Our study spans diverse languages, domains, and summary\nlengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and\nadvanced evaluation protocols that include both LLM-based automatic metrics and\nhuman-inspired criteria. Contrary to trends in other reasoning-intensive tasks,\nour findings show that explicit stepwise reasoning does not consistently\nimprove dialogue summarization quality. Instead, reasoning LLMs are often prone\nto verbosity, factual inconsistencies, and less concise summaries compared to\ntheir non-reasoning counterparts. Through scenario-specific analyses and\ndetailed case studies, we further identify when and why explicit reasoning may\nfail to benefit-or even hinder-summarization in complex dialogue contexts. Our\nwork provides new insights into the limitations of current reasoning LLMs and\nhighlights the need for targeted modeling and evaluation strategies for\nreal-world dialogue summarization.", "AI": {"tldr": "尽管大型语言模型（LLMs）在许多任务上取得了进展，但这项研究表明，对于对话摘要任务，明确的分步推理并不总是带来更好的效果。研究对多种语言的对话进行了评估，指出分步推理模型容易出现冗长和不够简洁的摘要。", "motivation": "尽管大型语言模型（LLMs）在摘要任务上取得了显著进展，但专门针对对话场景所需的同时抽象性和简洁性之间的平衡，长期链式思考（CoT）实现的分步推理架构表现如何，尚未得到充分探索。因此，本研究旨在探讨这一方面的局限性。", "method": "该研究对现有推理LLM和非推理LLM在通用、角色导向和查询导向的对话摘要范式进行了全面和系统的评估，涵盖了多种语言、领域和摘要长度。研究使用了强基准测试（如SAMSum, DialogSum, CSDS, 和QMSum）和高级评估协议，包括基于LLM的自动指标和人类启发的标准。", "result": "该研究发现，与其他需要大量推理的任务趋势相反，明确的分步推理并未一致提高对话摘要的质量。相反，推理LLM往往存在冗长、事实不一致和非简洁的问题。", "conclusion": "这项工作揭示了当前推理LLM在复杂对话上下文中的局限性，并强调了需要有针对性地进行建模和评估策略来提高现实世界对话摘要的准确性和简洁性。"}}
{"id": "2507.02199", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02199", "abs": "https://arxiv.org/abs/2507.02199", "authors": ["Wenquan Lu", "Yuechuan Yang", "Kyle Lee", "Yanshu Li", "Enqi Liu"], "title": "Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer", "comment": null, "summary": "Chain-of-thought (CoT) reasoning has enabled transformer-based language\nmodels to excel at complex mathematics and multi-step planning. However, in\nstandard decoder-only architectures, these reasoning steps are externalized in\nnatural language, improving interpretability at the cost of efficiency. To\ncapture reasoning that is not easily represented in words, many works have\nexplored recurrent architectures that aim to internalize reasoning in latent\nspace, potentially supporting latent CoT. In this paper, we investigate whether\nsuch reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer\nthat reuses layers at inference time without increasing parameter count. We\nexamine the model's internal behavior on arithmetic tasks using a suite of\nprobing techniques including the Logit Lens and Coda Lens. Our findings reveal\nlimited evidence of interpretable latent CoT by tracking rank trajectories of\nfinal and intermediate result tokens. Furthermore, we uncover significant\nprobing inconsistencies across recurrent blocks, where the interpretability of\nhidden states depends heavily on both the layer index and the decoding method.\nFinally, we empirically show that increasing recurrence depth yields only\nmarginal gains and falls well short of models that explicitly externalize\nreasoning steps. The code is available at\nhttps://github.com/wenquanlu/huginn-latent-cot.", "AI": {"tldr": "研究Huginn-3.5B模型在算术任务中的内在链式思考能力，发现有但证据有限。", "motivation": "探索在不增加参数量的情况下,通过推理时重复使用层级能否在Huginn-3.5B中生成可解释的内在链式思考。", "method": "通过探针技术套件如Logit Lens和Coda Lens探究Huginn-3.5B在算术任务上的内部行为。", "result": "研究表明，在递归块之间存在探测不一致性，隐含状态的可解释性依赖于层级索引和解码方法。此外，追踪最终和中间结果标记的排名轨迹显示了内部可解释链式思考的有限证据。", "conclusion": "发现深度递增对内在链式思考能力的提升有限，仍未达到显式外部化推理步骤模型的表现。"}}
{"id": "2507.02221", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02221", "abs": "https://arxiv.org/abs/2507.02221", "authors": ["Steven Song", "Anirudh Subramanyam", "Zhenyu Zhang", "Aarti Venkat", "Robert L. Grossman"], "title": "GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons", "comment": "11 pages, 1 figure, 7 tables", "summary": "Motivation: The Genomic Data Commons (GDC) provides access to high quality,\nharmonized cancer genomics data through a unified curation and analysis\nplatform centered around patient cohorts. While GDC users can interactively\ncreate complex cohorts through the graphical Cohort Builder, users (especially\nnew ones) may struggle to find specific cohort descriptors across hundreds of\npossible fields and properties. However, users may be better able to describe\ntheir desired cohort in free-text natural language.\n  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for\ncurating cohorts from the GDC. GDC Cohort Copilot automatically generates the\nGDC cohort filter corresponding to a user-input natural language description of\ntheir desired cohort, before exporting the cohort back to the GDC for further\nanalysis. An interactive user interface allows users to further refine the\ngenerated cohort. We develop and evaluate multiple large language models (LLMs)\nfor GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC\nCohort LLM achieves better results than GPT-4o prompting in generating GDC\ncohorts.\n  Availability and implementation: The standalone docker image for GDC Cohort\nCopilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.\nSource code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC\nCohort LLM weights are available at https://huggingface.co/uc-ctds.", "AI": {"tldr": "GDC Cohort Copilot is a tool that automatically generates GDC cohort filters based on natural language input, offering better outcomes with its locally-served LLM than with GPT-4.", "motivation": "The motivation behind GDC Cohort Copilot is to address the difficulty users face in creating specific cohorts through GDC's Cohort Builder by allowing them to describe desired cohorts in natural language.", "method": "The paper develops and evaluates multiple large language models (LLMs) to generate GDC cohort filters from user-provided natural language descriptions, focusing on a locally-served, open-source GDC Cohort LLM.", "result": "The locally-served, open-source GDC Cohort LLM demonstrates superior performance in generating GDC cohorts compared to GPT-4 prompting.", "conclusion": "GDC Cohort Copilot is made available as an open-source tool, demonstrating the effectiveness of local LLMs over generic models like GPT-4 for generating GDC cohorts."}}
{"id": "2507.02074", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02074", "abs": "https://arxiv.org/abs/2507.02074", "authors": ["Sanjeda Akter", "Ibne Farabi Shihab", "Anuj Sharma"], "title": "Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges", "comment": null, "summary": "Crash detection from video feeds is a critical problem in intelligent\ntransportation systems. Recent developments in large language models (LLMs) and\nvision-language models (VLMs) have transformed how we process, reason about,\nand summarize multimodal information. This paper surveys recent methods\nleveraging LLMs for crash detection from video data. We present a structured\ntaxonomy of fusion strategies, summarize key datasets, analyze model\narchitectures, compare performance benchmarks, and discuss ongoing challenges\nand opportunities. Our review provides a foundation for future research in this\nfast-growing intersection of video understanding and foundation models.", "AI": {"tldr": "本文综述了使用LLMs从视频中检测碰撞的方法，提出了融合策略的分类，总结了关键数据集和模型架构，比较了性能基准，并讨论了挑战和机会，为这一快速发展的领域奠定了基础。", "motivation": "随着大语言模型(LLMs)和视觉语言模型(VLMs)的发展，处理、推理和总结多模态信息的方式发生了转变。本文旨在解决智能交通系统中的碰撞检测问题。", "method": "本文综述了利用大语言模型(LLMs)从视频数据中进行碰撞检测的方法，提出了融合策略的结构化分类，总结了关键数据集，分析了模型架构，并比较了性能基准。", "result": "本文比较了性能基准，讨论了正在进行的挑战和机会，但没有提供具体的实验结果。", "conclusion": "本文综述了视频理解和基础模型的交叉领域中的关键进展，并指出了未来的研究方向。"}}
{"id": "2507.02259", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02259", "abs": "https://arxiv.org/abs/2507.02259", "authors": ["Hongli Yu", "Tinghong Chen", "Jiangtao Feng", "Jiangjie Chen", "Weinan Dai", "Qiying Yu", "Ya-Qin Zhang", "Wei-Ying Ma", "Jingjing Liu", "Mingxuan Wang", "Hao Zhou"], "title": "MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent", "comment": "Project Page: https://memagent-sialab.github.io/", "summary": "Despite improvements by length extrapolation, efficient attention and memory\nmodules, handling infinitely long documents with linear complexity without\nperformance degradation during extrapolation remains the ultimate challenge in\nlong-text processing. We directly optimize for long-text tasks in an end-to-end\nfashion and introduce a novel agent workflow, MemAgent, which reads text in\nsegments and updates the memory using an overwrite strategy. We extend the DAPO\nalgorithm to facilitate training via independent-context multi-conversation\ngeneration. MemAgent has demonstrated superb long-context capabilities, being\nable to extrapolate from an 8K context trained on 32K text to a 3.5M QA task\nwith performance loss < 5% and achieves 95%+ in 512K RULER test.", "AI": {"tldr": "我们通过MemAgent解决了长文本处理中的性能瓶颈问题，并在大规模测试中表现良好。", "motivation": "尽管长度外推、高效注意和记忆模块有所改进，但在长文本处理中，如何以线性复杂度处理无限长度文档而不降低外推性能的挑战依然存在。我们的动机是解决这一问题。", "method": "我们提出了名为MemAgent的新代理工作流，该工作流以段落为单位读取文本并通过覆盖策略更新记忆。同时，我们扩展了DAPO算法，以通过独立上下文多对话生成来促进训练。", "result": "MemAgent展示了卓越的长上下文能力，能够从8K上下文扩展到32K文本训练，并在3.5M QA任务中表现性能损失<5%，同时在512K RULER测试中达到95%+。", "conclusion": "MemAgent能够有效地处理长文本，并在保持高性能的同时实现了良好的扩展性。这为长文本任务中的高效处理开辟了新的途径。"}}
{"id": "2507.02148", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02148", "abs": "https://arxiv.org/abs/2507.02148", "authors": ["Zijie Cai", "Christopher Metzler"], "title": "Underwater Monocular Metric Depth Estimation: Real-World Benchmarks and Synthetic Fine-Tuning", "comment": null, "summary": "Monocular depth estimation has recently advanced to provide not only relative\nbut also metric depth predictions. However, its reliability in underwater\nenvironments remains limited due to light attenuation and scattering, color\ndistortion, turbidity, and the lack of high-quality metric ground-truth data.\nIn this paper, we present a comprehensive benchmark of zero-shot and fine-tuned\nmonocular metric depth estimation models on real-world underwater datasets with\nmetric depth annotations, such as FLSea and SQUID. We evaluate a diverse set of\nstate-of-the-art models across a range of underwater conditions with different\nranges. Our results show that large-scale models trained on terrestrial (real\nor synthetic) data, while effective in in-air settings, perform poorly\nunderwater due to significant domain shifts. To address this, we fine-tune\nDepth Anything V2 with a ViT-S backbone encoder on a synthetic underwater\nvariant of the Hypersim dataset, which we generated using a physically based\nunderwater image formation model. We demonstrate our fine-tuned model\nconsistently improves performance across all benchmarks and outperforms\nbaselines trained only on the clean in-air Hypersim dataset. Our study provides\na detailed evaluation and visualization for monocular metric depth estimation\nin underwater scenes, highlighting the importance of domain adaptation and\nscale-aware supervision for achieving robust and generalizable metric depth\npredictions in challenging underwater environments for future research.", "AI": {"tldr": "在水下环境中测试了几种单目深度估计模型后，该研究指出那些在空气环境中有效的大型模型在水下表现不佳。研究中介绍了一种细调后的深度模型，它在水下环境中表现优异，改进了在水下障碍环境中的深度估计性能。", "motivation": "单目深度估计在空中环境中的有效性和在水下环境中由于光衰减、散射、色偏差和浑浊等因素而受到限制，特别是缺乏高质量的地面真实数据，使得评估水下环境中的单目深度估计的性能至关重要。", "method": "研究中采用全面的基准测试方法，评估了零样本和微调的单目度量深度估计模型在带有度量深度标注的真实世界水下数据集上的性能。特别是使用了经过物理基础模型生成的合成水下版本的Hypersim数据集对Depth Anything V2模型进行微调。", "result": "该论文通过在真实的水下数据集FLSea和SQUID上评估几种先进的单目深度估计模型，指出了在空气环境下有效的大型模型在水下表现不佳的问题。为了改善这一情况，作者利用基于物理的水下图像生成模型对Hypersim数据集生成了合成水下版本，并用此对Depth Anything V2进行了微调。实验表明，经过微调的模型在各种基准上均表现出色，优于仅在无污染的Hypersim数据集上训练的模型。这项研究表明领域适应和尺度感知监督对于在挑战性的水下环境中实现稳健和通用的深度预测至关重要。", "conclusion": "研究表明，针对水下环境进行领域适应和采用尺度感知监督的重要性，这对实现未来研究中更为稳健和具有广泛适用性的度量深度预测至关重要。"}}
{"id": "2507.02302", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02302", "abs": "https://arxiv.org/abs/2507.02302", "authors": ["Dohoon Kim", "Donghun Kang", "Taesup Moon"], "title": "DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning", "comment": "22 pages, 5 figures, ACL 2025 Main", "summary": "Domain-Adaptive Pre-training (DAP) has recently gained attention for its\neffectiveness in fine-tuning pre-trained models. Building on this, continual\nDAP has been explored to develop pre-trained models capable of incrementally\nincorporating different domain datasets. However, existing continual DAP\nmethods face several limitations: (1) high computational cost and GPU memory\nusage during training; (2) sensitivity to incremental data order; and (3)\nproviding a single, generalized model for all end tasks, which contradicts the\nessence of DAP. In this paper, we propose DoMIX, a novel approach that\naddresses these challenges by leveraging LoRA modules, a representative\nparameter-efficient fine-tuning (PEFT) method. Our approach enables efficient\nand parallel domain-adaptive pre-training that is robust to domain order and\neffectively utilizes accumulated knowledge to provide tailored pre-trained\nmodels for specific tasks. We also demonstrate that our method can be extended\nbeyond the DAP setting to standard LLM fine-tuning scenarios. Code is available\nat https://github.com/dohoonkim-ai/DoMIX.", "AI": {"tldr": "本文提出了一种名为DoMIX的新方法，该方法利用LoRA模块解决了现有连续领域自适应预训练方法中存在的问题。DoMIX方法能够实现对不同领域数据的高效并行预训练，适用于特定任务并可扩展到标准的大型语言模型微调场景。", "motivation": "为了解决现有的连续领域自适应预训练方法（continual DAP）面临的问题，包括高昂的计算成本和GPU内存使用、对增量数据顺序的敏感性、以及未能为最终任务提供特定模型的普遍性模型，作者提出了DoMIX方法。", "method": "通过利用LoRA模块，一种代表性的参数高效微调(PEFT)方法，DoMIX能够实现对不同领域数据的高效并行预训练，并且对领域顺序具有鲁棒性，同时有效利用积累的知识为特定任务提供定制化的预训练模型。", "result": "该方法不仅解决了现有问题，还证明了其可以扩展到标准的LLM微调场景。", "conclusion": "DoMIX通过利用LoRA模块在有效管理计算成本和内存的同时，解决了现有连续领域自适应预训练方法中的问题，提供了一种高效并行预训练方法，适用于特定任务。该方法还证明了其扩展性，不仅限于DAP设置，还可应用于标准的大型语言模型微调场景。"}}
{"id": "2507.02200", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02200", "abs": "https://arxiv.org/abs/2507.02200", "authors": ["Xiao Wang", "Jingtao Jiang", "Qiang Chen", "Lan Chen", "Lin Zhu", "Yaowei Wang", "Yonghong Tian", "Jin Tang"], "title": "ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning", "comment": "A Strong Baseline for Reasoning based Event Stream Scene Text\n  Recognition", "summary": "Event stream based scene text recognition is a newly arising research topic\nin recent years which performs better than the widely used RGB cameras in\nextremely challenging scenarios, especially the low illumination, fast motion.\nExisting works either adopt end-to-end encoder-decoder framework or large\nlanguage models for enhanced recognition, however, they are still limited by\nthe challenges of insufficient interpretability and weak contextual logical\nreasoning. In this work, we propose a novel chain-of-thought reasoning based\nevent stream scene text recognition framework, termed ESTR-CoT. Specifically,\nwe first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input\nevent stream into tokens and utilize a Llama tokenizer to encode the given\ngeneration prompt. A Q-former is used to align the vision token to the\npre-trained large language model Vicuna-7B and output both the answer and\nchain-of-thought (CoT) reasoning process simultaneously. Our framework can be\noptimized using supervised fine-tuning in an end-to-end manner. In addition, we\nalso propose a large-scale CoT dataset to train our framework via a three stage\nprocessing (i.e., generation, polish, and expert verification). This dataset\nprovides a solid data foundation for the development of subsequent\nreasoning-based large models. Extensive experiments on three event stream STR\nbenchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the\neffectiveness and interpretability of our proposed framework. The source code\nand pre-trained models will be released on\nhttps://github.com/Event-AHU/ESTR-CoT.", "AI": {"tldr": "提出了一种基于chain-of-thought推理的事件流场景文本识别框架（ESTR-CoT），具有更好的解释性和逻辑推理能力，并在三个基准数据集上进行了验证。", "motivation": "现有的方法在极低光照和快速运动等极端条件下性能有限，主要基于端到端的编码器-解码器框架或大规模语言模型以增强识别，但仍然受限于解释性和逻辑推理能力弱的问题。", "method": "采用视觉编码器EVA-CLIP（ViT-G/14）将输入事件流转换为tokens，并使用Llama tokenizer对给定的生成提示进行编码。通过Q-former将视觉tokens与预训练的大规模语言模型Vicuna-7B进行对齐，同时输出答案和推理过程（CoT）。", "result": "在三个事件流STR基准数据集（EventSTR，WordArt*，IC15*）上进行了广泛的实验，证实了所提出框架的有效性和可解释性。", "conclusion": "开发了大规模的CoT数据集，通过三阶段处理（生成、优化和专家验证）训练框架，为后续推理型大规模模型的发展奠定了坚实的数据基础。"}}
{"id": "2507.02357", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02357", "abs": "https://arxiv.org/abs/2507.02357", "authors": ["Christian Jaumann", "Annemarie Friedrich", "Rainer Lienhart"], "title": "Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models", "comment": "Accepted at 5th Workshop on Scholarly Document Processing @ ACL 2025", "summary": "This paper describes our system for the SciVQA 2025 Shared Task on Scientific\nVisual Question Answering. Our system employs an ensemble of two Multimodal\nLarge Language Models and various few-shot example retrieval strategies. The\nmodel and few-shot setting are selected based on the figure and question type.\nWe also select answers based on the models' confidence levels. On the blind\ntest data, our system ranks third out of seven with an average F1 score of\n85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.", "AI": {"tldr": "本文描述了一个用于SciVQA 2025共享任务的系统，该任务涉及科学视觉问答。该系统利用双多模态大模型组合和少量样本检索策略。在盲测中排名第三，平均F1评分为85.12。", "motivation": "旨在参与SciVQA 2025共享任务的挑战，特别是在科学视觉问答领域，希望通过结合多种技术和策略来提高回答的准确性和可靠性。", "method": "该系统采用两个多模态大型语言模型的组合，并结合不同的少量样本检索策略。模型和少量样本设置的选择基于图形和问题类型。同时，答案的选择基于模型的信心水平。", "result": "该系统在盲测中排名第三，平均F1评分为85.12，此评分涵盖了ROUGE-1、ROUGE-L和BERTS三项评估指标。", "conclusion": "系统展示了多模态大型语言模型结合适当策略处理科学视觉问题的有效性。源代码已公开。"}}
{"id": "2507.02205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02205", "abs": "https://arxiv.org/abs/2507.02205", "authors": ["Elena Ryumina", "Maxim Markitantov", "Alexandr Axyonov", "Dmitry Ryumin", "Mikhail Dolgushin", "Alexey Karpov"], "title": "Team RAS in 9th ABAW Competition: Multimodal Compound Expression Recognition Approach", "comment": "8", "summary": "Compound Expression Recognition (CER), a subfield of affective computing,\naims to detect complex emotional states formed by combinations of basic\nemotions. In this work, we present a novel zero-shot multimodal approach for\nCER that combines six heterogeneous modalities into a single pipeline: static\nand dynamic facial expressions, scene and label matching, scene context, audio,\nand text. Unlike previous approaches relying on task-specific training data,\nour approach uses zero-shot components, including Contrastive Language-Image\nPretraining (CLIP)-based label matching and Qwen-VL for semantic scene\nunderstanding. We further introduce a Multi-Head Probability Fusion (MHPF)\nmodule that dynamically weights modality-specific predictions, followed by a\nCompound Expressions (CE) transformation module that uses Pair-Wise Probability\nAggregation (PPA) and Pair-Wise Feature Similarity Aggregation (PFSA) methods\nto produce interpretable compound emotion outputs. Evaluated under multi-corpus\ntraining, the proposed approach shows F1 scores of 46.95% on AffWild2, 49.02%\non Acted Facial Expressions in The Wild (AFEW), and 34.85% on C-EXPR-DB via\nzero-shot testing, which is comparable to the results of supervised approaches\ntrained on target data. This demonstrates the effectiveness of the proposed\napproach for capturing CE without domain adaptation. The source code is\npublicly available.", "AI": {"tldr": "提出一种新的零样本多模态方法，结合多种模态检测复合情感状态，结果表明该方法在检测复合情感时无需领域适应，效果与监督学习相当。", "motivation": "旨在检测由基本情感组合形成的复杂情感状态，与依赖特定任务训练数据的传统方法不同，这种方法采用零样本组件，无需领域适应即可捕获复合情感。", "method": "提出了一种结合六种异构模态的零样本多模态方法，这些模态包括静态和动态面部表情、场景和标签匹配、场景上下文、音频和文字。该方法使用CLIP为基础的标签匹配和Qwen-VL进行语义场景理解，并引入了多头概率融合（MHPF）模块以及复合表情变换模块，后者使用成对概率聚合（PPA）和特征相似性聚合（PFSA）方法生成可解释的复合情感输出。", "result": "该方法在AffWild2上实现了46.95%的F1值，在AFEW上为49.02%，在C-EXPR-DB上为34.85%，其效果与监督训练方法相当，表明了该方法的有效性。", "conclusion": "本研究开发的零样本复合情感识别方法被证明能够有效识别复合表情，为情感计算领域提供了一种新的有效方法。"}}
{"id": "2507.02364", "categories": ["cs.CL", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.02364", "abs": "https://arxiv.org/abs/2507.02364", "authors": ["Pilsung Kang"], "title": "QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers", "comment": null, "summary": "Parameterized quantum circuits (PQCs) have recently emerged as promising\ncomponents for enhancing the expressibility of neural architectures. In this\nwork, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the\nfeedforward network (FFN) modules of a compact BERT variant are replaced by\nPQC-based layers. This design is motivated by the dominant parameter\ncontribution of FFNs, which account for approximately two-thirds of the\nparameters within standard Transformer encoder blocks. While prior studies have\nprimarily integrated PQCs into self-attention modules, our work focuses on the\nFFN and systematically investigates the trade-offs between PQC depth,\nexpressibility, and trainability. Our final PQC architecture incorporates a\nresidual connection, both $R_Y$ and $R_Z$ rotations, and an alternating\nentanglement strategy to ensure stable training and high expressibility. Our\nexperiments, conducted on a classical simulator, on the SST-2 and DBpedia\nbenchmarks demonstrate two key findings. First, a carefully configured\nQFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its\nclassical counterpart in a full-data setting while reducing FFN-specific\nparameters by over 99%. Second, our model exhibits a consistent and competitive\nedge in few-shot learning scenarios, confirming its potential for superior data\nefficiency. These results, supported by an ablation study on a non-optimized\nPQC that failed to learn, confirm that PQCs can serve as powerful and\nparameter-efficient alternatives to classical FFNs when co-designed with\nfoundational deep learning principles.", "AI": {"tldr": "本文提出在BERT中用PQC代替FFN模块，降低了参数量且提高了数据效率，实验结果表明其能够在少量样本学习场景中表现出优越性。", "motivation": "考虑到BERT中的FFN模块贡献了标准Transformer编码器块中大约三分之二的参数，本文旨在通过替换FFN模块来降低参数量，并探讨PQC深度、表达能力与可训练性的权衡。", "method": "本文介绍了一种量子-经典混合的变压器模型QFFN-BERT，该模型用参数化量子电路（PQC）层替换了紧凑型BERT变体中的前馈神经网络（FFN）模块。PQC层包括剩余连接、$R_Y$和$R_Z$旋转以及交替纠缠策略，以确保稳定训练和高表达能力。", "result": "实验结果：1) 在全量数据集上，优化配置的QFFN-BERT可以达到基线准确率的102.0%，同时减少了超过99%的FFN特定参数。2) 在少量样本学习场景中，模型表现出了持续和竞争性的优势，证实了其在数据效率方面的潜力。", "conclusion": "实验及消融研究确认，当与基础深度学习原则协同设计时，PQC可以作为功能强大且参数高效的经典FFN的替代品。"}}
{"id": "2507.02212", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02212", "abs": "https://arxiv.org/abs/2507.02212", "authors": ["Takuro Kawada", "Shunsuke Kitada", "Sota Nemoto", "Hitoshi Iyatomi"], "title": "SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers", "comment": "21 pages, 15 figures, 4 tables. Project Page:\n  https://iyatomilab.github.io/SciGA/", "summary": "Graphical Abstracts (GAs) play a crucial role in visually conveying the key\nfindings of scientific papers. While recent research has increasingly\nincorporated visual materials such as Figure 1 as de facto GAs, their potential\nto enhance scientific communication remains largely unexplored. Moreover,\ndesigning effective GAs requires advanced visualization skills, creating a\nbarrier to their widespread adoption. To tackle these challenges, we introduce\nSciGA-145k, a large-scale dataset comprising approximately 145,000 scientific\npapers and 1.14 million figures, explicitly designed for supporting GA\nselection and recommendation as well as facilitating research in automated GA\ngeneration. As a preliminary step toward GA design support, we define two\ntasks: 1) Intra-GA recommendation, which identifies figures within a given\npaper that are well-suited to serve as GAs, and 2) Inter-GA recommendation,\nwhich retrieves GAs from other papers to inspire the creation of new GAs. We\nprovide reasonable baseline models for these tasks. Furthermore, we propose\nConfidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation\nmetric that offers a fine-grained analysis of model behavior. CAR addresses\nlimitations in traditional ranking-based metrics by considering cases where\nmultiple figures within a paper, beyond the explicitly labeled GA, may also\nserve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a\nfoundation for advancing visual scientific communication while contributing to\nthe development of AI for Science.", "AI": {"tldr": "引入由约145,000篇科学论文和1.14百万张图像组成的数据集SciGA-145k，用于GA的推荐和生成研究，同时提出两个GA任务及新的评估指标CAR。", "motivation": "图形摘要在科学论文中起着重要视觉传达作用。然而，目前对图形摘要的潜力提升方式仍不清楚，设计有效的图形摘要需要高级可视化技能，这也阻碍了其广泛应用。因此，构建了这个数据集来支持研究在自动化图形摘要生成方面。", "method": "通过构建名为SciGA-145k的大规模数据集来解决图形摘要(GA)选择和推荐的问题，并为自动化GA生成研究提供支持。数据集覆盖约145,000篇科学论文以及1.14百万张图片。同时，为GA设计支持制定了两个任务：1) 通过在给定论文中识别适合用作GA的图片，来实现GA内部推荐。2) 从其它论文中检索GA，以启发新的GA设计。还提供这些任务的基本模型，并提出一个新的推荐指标Confidence Adjusted top-1 ground truth Ratio (CAR)，来弥补传统排名度量指标的不足。", "result": "成功构造了包含约145,000篇科学论文和1.14百万张图像的SciGA-145k数据集，并提供了一些任务和推荐指标，为GA生成的自动化研究提供了支持。通过提出的评估指标CAR，可以更细致地分析模型行为。", "conclusion": "通过统一这些任务和度量，SciGA-145k奠定了推动可视化科学交流的基础，同时为科学领域AI的发展做出了贡献。"}}
{"id": "2507.02378", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02378", "abs": "https://arxiv.org/abs/2507.02378", "authors": ["Weijie Lyu", "Sheng-Jun Huang", "Xuan Xia"], "title": "Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection", "comment": null, "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved code generation and program comprehension, accelerating the evolution\nof software engineering. Current methods primarily enhance model performance by\nleveraging vast amounts of data, focusing on data quantity while often\noverlooking data quality, thereby reducing training efficiency. To address\nthis, we introduce an approach that utilizes a parametric model for code data\nselection, aimed at improving both training efficiency and model performance.\nOur method optimizes the parametric model to ensure distribution consistency\nand diversity within the selected subset, guaranteeing high-quality data.\nExperimental results demonstrate that using only 10K samples, our method\nachieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled\nbaseline, outperforming other sampling approaches in both performance and\nefficiency. This underscores that our method effectively boosts model\nperformance while significantly reducing computational costs.", "AI": {"tldr": "A parametric model for selecting high-quality code data is introduced to improve training efficiency and model performance, achieving better results with fewer samples.", "motivation": "To address the issue of current methods focusing on data quantity while often overlooking data quality, thereby reducing training efficiency.", "method": "An approach that utilizes a parametric model for code data selection, aimed at improving both training efficiency and model performance.", "result": "Experimental results demonstrate that using only 10K samples, our method achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled baseline, outperforming other sampling approaches in both performance and efficiency.", "conclusion": "This underscores that our method effectively boosts model performance while significantly reducing computational costs."}}
{"id": "2507.02217", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02217", "abs": "https://arxiv.org/abs/2507.02217", "authors": ["Brandon Trabucco", "Qasim Wani", "Benjamin Pikus", "Vasu Sharma"], "title": "Understanding Trade offs When Conditioning Synthetic Data", "comment": null, "summary": "Learning robust object detectors from only a handful of images is a critical\nchallenge in industrial vision systems, where collecting high quality training\ndata can take months. Synthetic data has emerged as a key solution for data\nefficient visual inspection and pick and place robotics. Current pipelines rely\non 3D engines such as Blender or Unreal, which offer fine control but still\nrequire weeks to render a small dataset, and the resulting images often suffer\nfrom a large gap between simulation and reality. Diffusion models promise a\nstep change because they can generate high quality images in minutes, yet\nprecise control, especially in low data regimes, remains difficult. Although\nmany adapters now extend diffusion beyond plain text prompts, the effect of\ndifferent conditioning schemes on synthetic data quality is poorly understood.\nWe study eighty diverse visual concepts drawn from four standard object\ndetection benchmarks and compare two conditioning strategies: prompt based and\nlayout based. When the set of conditioning cues is narrow, prompt conditioning\nyields higher quality synthetic data; as diversity grows, layout conditioning\nbecomes superior. When layout cues match the full training distribution,\nsynthetic data raises mean average precision by an average of thirty four\npercent and by as much as one hundred seventy seven percent compared with using\nreal data alone.", "AI": {"tldr": "研究了不同条件策略对合成数据质量的影响，发现基于提示和基于布局的条件策略各有优势，基于布局的条件策略在某些情况下可以使合成数据的性能比使用真实数据提高很多。", "motivation": "学习仅从少量图像中获得稳健的对象检测器是工业视觉系统中的一个关键挑战，因为高质量训练数据的收集可能需要数月时间。目前的方法依赖于3D引擎，渲染数据仍然需要数周时间，并且合成图像与真实图像之间存在较大的差距。", "method": "研究了从四个标准对象检测基准中抽取的八十种不同视觉概念，并比较了两种条件策略：基于提示的和基于布局的。当条件线索集较窄时，基于提示的条件生成更高质量的合成数据；当多样性增加，基于布局的条件效果更好。", "result": "当布局线索与完整的训练分布相匹配时，合成数据将平均精度提高了34%，在某些情况下甚至提高了177%，相比仅使用真实数据。", "conclusion": "在不同的条件策略下，合成数据的质量会有所不同。基于提示和基于布局的条件策略在合成数据生成中各有优势，可以根据条件线索的多样性选择合适的策略。"}}
{"id": "2507.02407", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.02407", "abs": "https://arxiv.org/abs/2507.02407", "authors": ["Mark Atta Mensah", "Isaac Wiafe", "Akon Ekpezu", "Justice Kwame Appati", "Jamal-Deen Abdulai", "Akosua Nyarkoa Wiafe-Akenten", "Frank Ernest Yeboah", "Gifty Odame"], "title": "Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability", "comment": "This version has been reviewed and accepted for presentation at the\n  Future Technologies Conference (FTC) 2025, to be held on 6 & 7 November 2025\n  in Munich, Germany. 17 pages, 4 figures, 1 table", "summary": "Most existing automatic speech recognition (ASR) research evaluate models\nusing in-domain datasets. However, they seldom evaluate how they generalize\nacross diverse speech contexts. This study addresses this gap by benchmarking\nseven Akan ASR models built on transformer architectures, such as Whisper and\nWav2Vec2, using four Akan speech corpora to determine their performance. These\ndatasets encompass various domains, including culturally relevant image\ndescriptions, informal conversations, biblical scripture readings, and\nspontaneous financial dialogues. A comparison of the word error rate and\ncharacter error rate highlighted domain dependency, with models performing\noptimally only within their training domains while showing marked accuracy\ndegradation in mismatched scenarios. This study also identified distinct error\nbehaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned\nWhisper Akan models led to more fluent but potentially misleading transcription\nerrors, Wav2Vec2 produced more obvious yet less interpretable outputs when\nencountering unfamiliar inputs. This trade-off between readability and\ntransparency in ASR errors should be considered when selecting architectures\nfor low-resource language (LRL) applications. These findings highlight the need\nfor targeted domain adaptation techniques, adaptive routing strategies, and\nmultilingual training frameworks for Akan and other LRLs.", "AI": {"tldr": "本研究评估了基于变压器架构的七种阿坎语ASR模型在不同类型的语音数据上的表现，发现模型在与训练域不匹配的情况下准确率显著下降。", "motivation": "大多数现有的自动语音识别（ASR）研究在使用域内数据集评估模型性能时，很少考虑这些模型如何泛化到不同的语音环境中。这项研究填补了这一知识空白。", "method": "本研究通过使用四个阿坎语语料库（包含文化相关的图像描述、非正式对话、圣经经文朗诵和即兴金融对话）来评估七种基于变压器架构的ASR模型（如Whisper和Wav2Vec2）的性能。", "result": "研究结果表明ASR模型在不同领域的性能存在很大差异，它们仅在训练领域表现最佳，而在不匹配的场景中准确率显著下降。", "conclusion": "研究结果揭示了为LRLs（包括阿坎语）研发针对域适应技术、自适应路由策略和多语言训练框架的需求。此外，该研究指出在选择ASR模型架构时需要权衡错误输出的可读性和透明度。"}}
{"id": "2507.02222", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02222", "abs": "https://arxiv.org/abs/2507.02222", "authors": ["Tian Gao", "Zhiyuan Zhang", "Kaijie Yin", "Xu-Cheng Zhong", "Hui Kong"], "title": "High-Fidelity Differential-information Driven Binary Vision Transformer", "comment": null, "summary": "The binarization of vision transformers (ViTs) offers a promising approach to\naddressing the trade-off between high computational/storage demands and the\nconstraints of edge-device deployment. However, existing binary ViT methods\noften suffer from severe performance degradation or rely heavily on\nfull-precision modules. To address these issues, we propose DIDB-ViT, a novel\nbinary ViT that is highly informative while maintaining the original ViT\narchitecture and computational efficiency. Specifically, we design an\ninformative attention module incorporating differential information to mitigate\ninformation loss caused by binarization and enhance high-frequency retention.\nTo preserve the fidelity of the similarity calculations between binary Q and K\ntensors, we apply frequency decomposition using the discrete Haar wavelet and\nintegrate similarities across different frequencies. Additionally, we introduce\nan improved RPReLU activation function to restructure the activation\ndistribution, expanding the model's representational capacity. Experimental\nresults demonstrate that our DIDB-ViT significantly outperforms\nstate-of-the-art network quantization methods in multiple ViT architectures,\nachieving superior image classification and segmentation performance.", "AI": {"tldr": "DIDB-ViT是一个新型的二值视觉变压器，通过引入信息注意力模块、频率分解和改进的RPReLU激活函数，解决了二值化过程中性能下降和信息丢失的问题，并在多种ViT架构上达到了最优的图像分类和分割效果。", "motivation": "为了克服现有的二值ViT方法性能下降严重或依赖全精度模块的问题，我们提出了一个新的二值ViT模型——DIDB-ViT。", "method": "我们设计了一个包含差异信息的信息注意力模块来缓解由二值化引起的性能下降，进一步引入改进的RPReLU激活函数来扩展模型的表达能力。此外，我们通过离散哈氏小波进行频率分解，以保留二值Q和K张量之间相似性计算的保真度。", "result": "实验结果显示，我们的DIDB-ViT在多种ViT架构中显著超越了当前最先进的网络量化方法，在图像分类和分割性能上表现出色。", "conclusion": "DIDB-ViT通过设计信息注意力模块、频率分解和改进的RPReLU激活函数，有效提升了高频率信息的保留和计算精确度，从而在轻量和性能之间找到更好的平衡。"}}
{"id": "2507.02428", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02428", "abs": "https://arxiv.org/abs/2507.02428", "authors": ["Sumaya Ahmed Salihs", "Isaac Wiafe", "Jamal-Deen Abdulai", "Elikem Doe Atsakpo", "Gifty Ayoka", "Richard Cave", "Akon Obu Ekpezu", "Catherine Holloway", "Katrin Tomanek", "Fiifi Baffoe Payin Winful"], "title": "A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages", "comment": "This version has been reviewed and accepted for presentation at the\n  InterSpeech 2025 conference to be held in Rotterdam from 17 to 21 August. 5\n  pages and 3 tables", "summary": "This study presents an approach for collecting speech samples to build\nAutomatic Speech Recognition (ASR) models for impaired speech, particularly,\nlow-resource languages. It aims to democratize ASR technology and data\ncollection by developing a \"cookbook\" of best practices and training for\ncommunity-driven data collection and ASR model building. As a proof-of-concept,\nthis study curated the first open-source dataset of impaired speech in Akan: a\nwidely spoken indigenous language in Ghana. The study involved participants\nfrom diverse backgrounds with speech impairments. The resulting dataset, along\nwith the cookbook and open-source tools, are publicly available to enable\nresearchers and practitioners to create inclusive ASR technologies tailored to\nthe unique needs of speech impaired individuals. In addition, this study\npresents the initial results of fine-tuning open-source ASR models to better\nrecognize impaired speech in Akan.", "AI": {"tldr": "该研究通过开发社区驱动的数据收集最佳实践和培训，构建了针对低资源语言阿肯语的首份公开源代码的失常语音数据集，并展示了用这些数据训练的ASR模型初版结果。", "motivation": "该研究旨在使ASR技术及数据收集民主化，并特别针对资源较少的语言构建ASR模型。", "method": "该研究开发了一套“烹饪书”，包含最佳实践和培训，以实现由社区驱动的数据收集和ASR模型构建。作为概念验证，研究整理了第一份公开源代码的阿肯语（加纳广泛使用的土著语言）的失常语音数据集。", "result": "研究整理的数据集，连同烹饪书和开源工具，均公开可用，以促进研究人员和实践者开发适用于失常语音个体的独特需求的包容性ASR技术。", "conclusion": "初步结果显示，基于开源ASR模型对阿肯语失常语音的微调改善了识别性能。"}}
{"id": "2507.02250", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02250", "abs": "https://arxiv.org/abs/2507.02250", "authors": ["Jiangxia Chen", "Tongyuan Huang", "Ke Song"], "title": "FMOcc: TPV-Driven Flow Matching for 3D Occupancy Prediction with Selective State Space Model", "comment": null, "summary": "3D semantic occupancy prediction plays a pivotal role in autonomous driving.\nHowever, inherent limitations of fewframe images and redundancy in 3D space\ncompromise prediction accuracy for occluded and distant scenes. Existing\nmethods enhance performance by fusing historical frame data, which need\nadditional data and significant computational resources. To address these\nissues, this paper propose FMOcc, a Tri-perspective View (TPV) refinement\noccupancy network with flow matching selective state space model for few-frame\n3D occupancy prediction. Firstly, to generate missing features, we designed a\nfeature refinement module based on a flow matching model, which is called Flow\nMatching SSM module (FMSSM). Furthermore, by designing the TPV SSM layer and\nPlane Selective SSM (PS3M), we selectively filter TPV features to reduce the\nimpact of air voxels on non-air voxels, thereby enhancing the overall\nefficiency of the model and prediction capability for distant scenes. Finally,\nwe design the Mask Training (MT) method to enhance the robustness of FMOcc and\naddress the issue of sensor data loss. Experimental results on the\nOcc3D-nuScenes and OpenOcc datasets show that our FMOcc outperforms existing\nstate-of-theart methods. Our FMOcc with two frame input achieves notable scores\nof 43.1% RayIoU and 39.8% mIoU on Occ3D-nuScenes validation, 42.6% RayIoU on\nOpenOcc with 5.4 G inference memory and 330ms inference time.", "AI": {"tldr": "该论文提出FMOcc方法，用于解决基于少数帧图像的3D占用预测问题，尤其改善了遮挡和远距离场景的预测。实验表明，该方法在Occ3D-nuScenes和OpenOcc数据集上优于现有技术。", "motivation": "当前3D占用预测方法依赖于融合历史帧数据以提高性能，这需要额外的数据和显著的计算资源。论文旨在解决使用少数帧图像进行预测时遇到的准确性问题，特别是在遮挡和远处场景的预测时。", "method": "该论文提出了一种名为FMOcc的新方法，该方法结合了Tri-perspective View (TPV) refinement occupancy网络与flow matching selective state space模型（FMSSM），专门用于解决基于少数帧图像的3D占用预测问题。FMOcc设计了一个基于流匹配模型的特性细化模块，用于产生丢失的特征，并引入了TPV SSM层和选择性平面SSM（PS3M）来提高整体效率和对远处场景的预测能力。此外，还设计了一种掩模训练（MT）方法以增强FMOcc的鲁棒性并应对传感器数据分析的不足。", "result": "在Occ3D-nuScenes和OpenOcc数据集上的实验结果表明，FMOcc优于现有的最先进技术。使用两帧输入时，在Occ3D-nuScenes验证集上获得43.1%的RayIoU和39.8%的mIoU，在OpenOcc上得到42.6%的RayIoU，使用5.4 G推理内存和330ms的推理时间。", "conclusion": "该研究成功展示了FMOcc在网络效率和远处场景预测能力方面的改进，给自动驾驶中的3D语义占用预测问题提供了一个新颖且有效的解决方案。"}}
{"id": "2507.02506", "categories": ["cs.CL", "cs.AI", "cs.LG", "91B14, 68T50", "I.2.7; K.4.1; K.5.2"], "pdf": "https://arxiv.org/pdf/2507.02506", "abs": "https://arxiv.org/abs/2507.02506", "authors": ["Sneha Deshmukh", "Prathmesh Kamble"], "title": "IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders", "comment": "9 pages, 9 figures, 2 tables. Dataset available at Hugging Face and\n  GitHub. Submitted to arXiv for open access", "summary": "Legal NLP remains underdeveloped in regions like India due to the scarcity of\nstructured datasets. We introduce IndianBailJudgments-1200, a new benchmark\ndataset comprising 1200 Indian court judgments on bail decisions, annotated\nacross 20+ attributes including bail outcome, IPC sections, crime type, and\nlegal reasoning. Annotations were generated using a prompt-engineered GPT-4o\npipeline and verified for consistency. This resource supports a wide range of\nlegal NLP tasks such as outcome prediction, summarization, and fairness\nanalysis, and is the first publicly available dataset focused specifically on\nIndian bail jurisprudence.", "AI": {"tldr": "A new dataset called IndianBailJudgments-1200 has been developed to address the lack of structured legal data in India. It includes 1200 bail judgments annotated with over 20 attributes, and it supports several legal NLP tasks.", "motivation": "Legal NLP is underdeveloped in regions like India due to the scarcity of structured datasets. This paper aims to address this issue by creating a new benchmark dataset focused on Indian bail jurisprudence.", "method": "We introduce IndianBailJudgments-1200, a new benchmark dataset comprising 1200 Indian court judgments on bail decisions, annotated across 20+ attributes. Annotations were generated using a prompt-engineered GPT-4 pipeline and verified for consistency.", "result": "The resource supports a wide range of legal NLP tasks such as outcome prediction, summarization, and fairness analysis.", "conclusion": "This dataset is the first publicly available resource focused specifically on Indian bail jurisprudence."}}
