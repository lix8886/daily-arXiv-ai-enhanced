<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 74]
- [cs.CV](#cs.CV) [Total: 114]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Empathy by Design: Aligning Large Language Models for Healthcare Dialogue](https://arxiv.org/abs/2512.06097)
*Emre Umucu,Guillermina Solis,Leon Garza,Emilia Rivas,Beatrice Lee,Anantaa Kotal,Aritran Piplai*

Main category: cs.CL

> 研究提出了基于DPO的对齐框架，通过优化微调后的大规模语言模型，成功提高了其在医疗护理场景中的事实准确性、语义连贯性和人类中心品质。

<details>
  <summary>Details</summary>

**Motivation:** 尽管通用的大型语言模型在生成和推理方面表现出色，但由于事实准确性不足和缺乏同理心沟通能力，它们在医疗护理应用中仍受到限制。

**Method:** 本研究提出了一种基于直接偏好优化（DPO）的对齐框架，旨在通过使用成对偏好数据微调特定领域的LLM，提高其在护工-患者对话中事实上的正确性、语义连贯性和人类中心的品质（如同情、礼貌和简洁性）。

**Result:** 实验结果显示，使用DPO微调后的模型在多个公开和专有的LLMs上均表现出了更高的语义一致性、改进的事实准确性以及更强的人类中心评估分数，优于基线模型和谷歌医疗对话系统等商业选项。

**Conclusion:** 这种偏好对齐的方法提供了一条可扩展和透明的路径，能够开发出值得信赖、富有同情心并且具有临床见解的AI助手，以改善护工和医疗沟通。

**Abstract:** General-purpose large language models (LLMs) have demonstrated remarkable generative and reasoning capabilities but remain limited in healthcare and caregiving applications due to two key deficiencies: factual unreliability and a lack of empathetic communication. These shortcomings pose significant risks in sensitive contexts where users, particularly non-professionals and caregivers, seek medically relevant guidance or emotional reassurance. To address these challenges, we introduce a Direct Preference Optimization (DPO)-based alignment framework designed to improve factual correctness, semantic coherence, and human-centric qualities such as empathy, politeness, and simplicity in caregiver-patient dialogues. Our approach fine-tunes domain-adapted LLMs using pairwise preference data, where preferred responses reflect supportive and accessible communication styles while rejected ones represent prescriptive or overly technical tones. This direct optimization method aligns model outputs with human preferences more efficiently than traditional reinforcement-learning-based alignment. Empirical evaluations across multiple open and proprietary LLMs show that our DPO-tuned models achieve higher semantic alignment, improved factual accuracy, and stronger human-centric evaluation scores compared to baseline and commercial alternatives such as Google medical dialogue systems. These improvements demonstrate that preference-based alignment offers a scalable and transparent pathway toward developing trustworthy, empathetic, and clinically informed AI assistants for caregiver and healthcare communication. Our open-source code is available at: https://github.com/LeonG19/Empathy-by-Design

</details>


### [2] [Morphologically-Informed Tokenizers for Languages with Non-Concatenative Morphology: A case study of Yoloxóchtil Mixtec ASR](https://arxiv.org/abs/2512.06169)
*Chris Crawford*

Main category: cs.CL

> 本研究探讨了使用形态信息分词器来协助并简化Yoloxóchitl Mixtec语言音频语料库的逐词注释，通过结合自动语音识别(ASR)和基于文本的序列到序列工具，提高了效率并减少了人工注释的工作量。

<details>
  <summary>Details</summary>

**Motivation:** 目的是通过结合自动语音识别(ASR)和文本序列到序列工具，结合形态学信息分词，以提高纯粹语言处理工具对非线性形态的适应性和整体性能，从而改进工效。

**Method:** 描述了两种新型分词方案，它们以非线性方式分离单词，尽量保留语气形态的信息。其中，段落和旋律分词器仅提取音调而不预测分割，另一序列过程分词器则能够预测单词分割。

**Result:** 结果显示，这些新型分词器在性能上可以与BPE和Unigram模型相竞争，尤其是段落-旋律模型在词错误率上有更好的表现，虽然字符错误率上没有达到传统分词器的水平。

**Conclusion:** 研究结论指出，专门为语言非连接形态设计的非线性分词器在性能上与常规的BPE和Unigram模型竞争，而且还通过形态学和信息论度量分析找到了与下游性能预测相关的相关性。

**Abstract:** This paper investigates the impact of using morphologically-informed tokenizers to aid and streamline the interlinear gloss annotation of an audio corpus of Yoloxóchitl Mixtec (YM) using a combination of ASR and text-based sequence-to-sequence tools, with the goal of improving efficiency while reducing the workload of a human annotator. We present two novel tokenization schemes that separate words in a nonlinear manner, preserving information about tonal morphology as much as possible. One of these approaches, a Segment and Melody tokenizer, simply extracts the tones without predicting segmentation. The other, a Sequence of Processes tokenizer, predicts segmentation for the words, which could allow an end-to-end ASR system to produce segmented and unsegmented transcriptions in a single pass. We find that these novel tokenizers are competitive with BPE and Unigram models, and the Segment-and-Melody model outperforms traditional tokenizers in terms of word error rate but does not reach the same character error rate. In addition, we analyze tokenizers on morphological and information-theoretic metrics to find predictive correlations with downstream performance. Our results suggest that nonlinear tokenizers designed specifically for the non-concatenative morphology of a language are competitive with conventional BPE and Unigram models for ASR. Further research will be necessary to determine the applicability of these tokenizers in downstream processing tasks.

</details>


### [3] [Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots](https://arxiv.org/abs/2512.06193)
*Jihyung Park,Saleh Afroogh,Junfeng Jiao*

Main category: cs.CL

> 论文提出GAUGE框架，用以实时检测大型语言模型对话中的隐性情感升级，解决现有机制无法及时捕捉情感动态的问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有护栏机制依赖外部分类器或临床准则，这些可能落后于不断发展的对话中的细微、实时动态。需要能够检测隐性危害的机制。

**Method:** 提出GAUGE（Guarding Affective Utterance Generation Escalation），一种轻量级、基于logit的框架，用于实时检测隐藏的对话升级。GAUGE衡量大型语言模型的输出如何在概率上改变对话的情感状态。

**Result:** 未提供具体结果，但框架设计目标是实时检测对话中可能的隐性情感升级。

**Conclusion:** GAUGE作为实时检测对话中隐性情感升级的潜在解决方案，提供了一种轻量级的评估方法。

**Abstract:** Large Language Models (LLM) are increasingly integrated into everyday interactions, serving not only as information assistants but also as emotional companions. Even in the absence of explicit toxicity, repeated emotional reinforcement or affective drift can gradually escalate distress in a form of \textit{implicit harm} that traditional toxicity filters fail to detect. Existing guardrail mechanisms often rely on external classifiers or clinical rubrics that may lag behind the nuanced, real-time dynamics of a developing conversation. To address this gap, we propose GAUGE (Guarding Affective Utterance Generation Escalation), a lightweight, logit-based framework for the real-time detection of hidden conversational escalation. GAUGE measures how an LLM's output probabilistically shifts the affective state of a dialogue.

</details>


### [4] [Automated Data Enrichment using Confidence-Aware Fine-Grained Debate among Open-Source LLMs for Mental Health and Online Safety](https://arxiv.org/abs/2512.06227)
*Junyu Mao,Anthony Hills,Talia Tseriotou,Maria Liakata,Aya Shamir,Dan Sayda,Dana Atzil-Slonim,Natalie Djohari,Arpan Mandal,Silke Roth,Pamela Ugwudike,Mahesan Niranjan,Stuart E. Middleton*

Main category: cs.CL

> 论文提出了新的CFD框架，通过LLM代理之间的细粒度证据辩论实现更稳健的数据丰富化，并在多个任务中显示其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 改善自然语言处理任务中的真实世界指标标注问题，该问题是由于生活事件和在线安全行为等动态事件的标注成本高且困难。

**Method:** 此论文介绍了一个名为置信度感知细粒度辩论（CFD）的新框架，在该框架中，多个LLM代理模拟人类注释者，通过交换细致的证据以达成共识。

**Result:** CFD框架在丰富数据性能方面相比多种基线方法表现更稳健，增强了的特征通过辩论记录在下游任务中一致地产生了更好的效果，特别是在在线安全任务中比非增强的基准提高了10.1%。

**Conclusion:** CFD框架能够有效提高数据丰富化，并且优于现有方法，特别是在在线安全任务中的表现尤为突出。

**Abstract:** Real-world indicators are important for improving natural language processing (NLP) tasks such as life events for mental health analysis and risky behaviour for online safety, yet labelling such information in NLP training datasets is often costly and/or difficult given the dynamic nature of such events. This paper compares several LLM-based data enrichment methods and introduces a novel Confidence-Aware Fine-Grained Debate (CFD) framework in which multiple LLM agents simulate human annotators and exchange fine-grained evidence to reach consensus. We describe two new expert-annotated datasets, a mental health Reddit wellbeing dataset and an online safety Facebook sharenting risk dataset. Our CFD framework achieves the most robust data enrichment performance compared to a range of baselines and we show that this type of data enrichment consistently improves downstream tasks. Enriched features incorporated via debate transcripts yield the largest gains, outperforming the non-enriched baseline by 10.1% for the online safety task.

</details>


### [5] [Policy-based Sentence Simplification: Replacing Parallel Corpora with LLM-as-a-Judge](https://arxiv.org/abs/2512.06228)
*Xuanxin Wu,Yuki Arase,Masaaki Nagata*

Main category: cs.CL

> 本文引入了一种新的基于大型语言模型作为评判者的句子简化方法，它能够自动创建符合特定政策的训练数据，展示了在词汇导向简化和整体重写上的优越性。

<details>
  <summary>Details</summary>

**Motivation:** 研究表明，尽管句子简化的目标是将句子修改得更容易阅读和理解，同时保持其意思，但如何实现这种基于政策的控制仍然是一个开放的问题。

**Method:** 我们提出了一种利用大型语言模型作为评判者（LLM-as-a-Judge）来自动构建符合政策的训练数据的方法，从而避免了昂贵的人工标注或平行语料库的需求。

**Result:** 本研究的实验结果显示，即使是小型的开源LLM，例如Phi-3-mini-3.8B，在词汇导向的简化上也超过了GPT-4，而在整体重写上则达到了相似的表现。

**Conclusion:** 该方法展示了在构建简化系统时能够适应多种简化政策的能力，并且这种方法在不同模型家族和大小上都表现出稳健性。

**Abstract:** Sentence simplification aims to modify a sentence to make it easier to read and understand while preserving the meaning. Different applications require distinct simplification policies, such as replacing only complex words at the lexical level or rewriting the entire sentence while trading off details for simplicity. However, achieving such policy-driven control remains an open challenge. In this work, we introduce a simple yet powerful approach that leverages Large Language Model-as-a-Judge (LLM-as-a-Judge) to automatically construct policy-aligned training data, completely removing the need for costly human annotation or parallel corpora. Our method enables building simplification systems that adapt to diverse simplification policies. Remarkably, even small-scale open-source LLMs such as Phi-3-mini-3.8B surpass GPT-4o on lexical-oriented simplification, while achieving comparable performance on overall rewriting, as verified by both automatic metrics and human evaluations. The consistent improvements across model families and sizes demonstrate the robustness of our approach.

</details>


### [6] [LOCUS: A System and Method for Low-Cost Customization for Universal Specialization](https://arxiv.org/abs/2512.06239)
*Dhanasekar Sundararaman,Keying Li,Wayne Xiong,Aashna Garg*

Main category: cs.CL

> 研究提出了LOCUS系统，用于通过少量样本实现高效模型训练，适用于NLP任务如命名实体识别和文本分类，有效降低了成本和模型大小，同时保持高水平的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的目标是解决在迁移学习和小样本学习中的效率问题，特别是对于需要大量标注数据的模型训练过程。通过LOCUS系统，希望能够在降低模型大小和成本的同时，依然保持良好的性能。

**Method:** 我们提出了LOCUS系统，该系统利用少量样本数据通过针对性检索、合成数据生成和参数高效调优来简化自然语言处理模型的构建和训练过程。该系统使用少量标注示例，从广泛的数据仓库中发现相关数据，通过上下文数据生成合成额外的训练样本，并使用全参数或低秩参数（LoRA）进行模型微调。

**Result:** 该研究专注于命名实体识别（NER）和文本分类（TC）基准测试，结果表明，LOCUS系统的表现超越了强基线（包括GPT-4o），同时显著降低了成本和模型大小。优化后的模型在大多数情况下只需使用GPT-4o不到1%的参数就能达到99%的全量微调准确率。

**Conclusion:** LOCUS系统在保持高准确率的同时显著降低了模型的大小和计算成本，是一个高效的小样本学习解决方案，尤其是在命名实体识别和文本分类任务上。

**Abstract:** We present LOCUS (LOw-cost Customization for Universal Specialization), a pipeline that consumes few-shot data to streamline the construction and training of NLP models through targeted retrieval, synthetic data generation, and parameter-efficient tuning. With only a small number of labeled examples, LOCUS discovers pertinent data in a broad repository, synthesizes additional training samples via in-context data generation, and fine-tunes models using either full or low-rank (LoRA) parameter adaptation. Our approach targets named entity recognition (NER) and text classification (TC) benchmarks, consistently outperforming strong baselines (including GPT-4o) while substantially lowering costs and model sizes. Our resultant memory-optimized models retain 99% of fully fine-tuned accuracy while using barely 5% of the memory footprint, also beating GPT-4o on several benchmarks with less than 1% of its parameters.

</details>


### [7] [Convergence of Outputs When Two Large Language Models Interact in a Multi-Agentic Setup](https://arxiv.org/abs/2512.06256)
*Aniruddha Maiti,Satya Nimmagadda,Kartha Veerya Jammuladinne,Niladri Sengupta,Ananya Jana*

Main category: cs.CL

> 研究观察到两个大型语言模型在设定条件下相互反复响应的行为，显示出对话最终呈现出重复性汇聚状态。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探讨在多智能体环境中，没有外部输入的大型语言模型之间的交互是否存在某种规律性，尤其是它们的对话是否能保持新鲜性而不陷入重复。

**Method:** 研究使用了Mistral Nemo Base 2407和Llama 2 13B hf两个大型语言模型，在多智能体设置中让它们相互响应多个回合，不进行任何外部输入。初始设置使用一个简短的种子句子，每个模型阅读对方的输出并生成回应，重复固定次数。通过词汇和嵌入基度量研究了对话如何偏离初始种子以及两个模型生成的输出相似性变化。

**Result:** 观察到大多数对话最初是有条理的，但随后陷入了重复。许多运行中，一个简短的短语会在各轮次中反复出现。一旦重复开始，两个模型倾向于产生相似的输出，而不是引导对话进入新的方向。导致出现相同的或相似的文本不断重复。

**Conclusion:** 尽管这些模型规模大、单独训练且未经过任何提示指令，但行为依然表现为一种汇聚。

**Abstract:** In this work, we report what happens when two large language models respond to each other for many turns without any outside input in a multi-agent setup. The setup begins with a short seed sentence. After that, each model reads the other's output and generates a response. This continues for a fixed number of steps. We used Mistral Nemo Base 2407 and Llama 2 13B hf. We observed that most conversations start coherently but later fall into repetition. In many runs, a short phrase appears and repeats across turns. Once repetition begins, both models tend to produce similar output rather than introducing a new direction in the conversation. This leads to a loop where the same or similar text is produced repeatedly. We describe this behavior as a form of convergence. It occurs even though the models are large, trained separately, and not given any prompt instructions. To study this behavior, we apply lexical and embedding-based metrics to measure how far the conversation drifts from the initial seed and how similar the outputs of the two models becomes as the conversation progresses.

</details>


### [8] [Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models](https://arxiv.org/abs/2512.06266)
*Chen Yang,Guangyue Peng,Jiaying Zhu,Ran Le,Ruixiang Feng,Tao Zhang,Wei Ruan,Xiaoqi Liu,Xiaoxue Cheng,Xiyun Xu,Yang Song,Yanzipeng Gao,Yiming Jia,Yun Xing,Yuntao Wen,Zekai Wang,Zhenwei An,Zhicong Sun,Zongchao Chen*

Main category: cs.CL

> Nanbeige4-3B, a small-scale language model pretrained on 23T tokens and finetuned on over 30M instructions, extends small model boundaries using innovative training techniques and achieves strong performance.

<details>
  <summary>Details</summary>

**Motivation:** To extend the performance boundaries of small-scale language models and achieve high performance with practical resource usage.

**Method:** In pre-training, a Fine-Grained Warmup-Stable-Decay (FG-WSD) training scheduler is used to progressively refine data mixtures across stages. In post-training, a joint mechanism combining deliberative generation refinement and chain-of-thought reconstruction is introduced to enhance SFT data quality. Dual Preference Distillation (DPD) is employed to distill the model further. A multi-stage reinforcement learning phase with verifiable rewards and preference modeling is applied to improve reasoning and human alignment.

**Result:** Nanbeige4-3B outperforms comparable models and rivals much larger models across various benchmarks.

**Conclusion:** Nanbeige4-3B demonstrates that with optimized training techniques, small-scale models can compete with larger models and excel in diverse tasks.

**Abstract:** We present Nanbeige4-3B, a family of small-scale but high-performing language models. Pretrained on 23T high-quality tokens and finetuned on over 30 million diverse instructions, we extend the boundary of the scaling law for small language models. In pre-training, we design a Fine-Grained Warmup-Stable-Decay (FG-WSD) training scheduler, which progressively refines data mixtures across stages to boost model performance. In post-training, to improve the quality of the SFT data, we design a joint mechanism that integrates deliberative generation refinement and chain-of-thought reconstruction, yielding substantial gains on complex tasks. Following SFT, we employ our flagship reasoning model to distill Nanbeige4-3B through our proposed Dual Preference Distillation (DPD) method, which leads to further performance gains. Finally, a multi-stage reinforcement learning phase was applied, leveraging verifiable rewards and preference modeling to strengthen abilities on both reasoning and human alignment. Extensive evaluations show that Nanbeige4-3B not only significantly outperforms models of comparable parameter scale but also rivals much larger models across a wide range of benchmarks. The model checkpoints are available at https://huggingface.co/Nanbeige.

</details>


### [9] [Modeling Contextual Passage Utility for Multihop Question Answering](https://arxiv.org/abs/2512.06464)
*Akriti Jain,Aparna Garimella*

Main category: cs.CL

> 研究提出了一种轻量级的方法，用于预测多跳问答系统的片段效用，通过考虑片段间的依赖关系和利用推理轨迹获取合成训练数据，改进了片段再排名和下游问答性能。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于识别并合成多跳问答系统中多个文本片段的信息。研究提出的方法能够通过评估片段的效用，帮助去除冗余片段，从而减少因无关片段产生的噪音和答案不准确的问题。

**Method:** 本研究提出了一种轻量级的方法来建模上下文相关的片段效用，考虑了片段间的依赖关系。研究通过微调一个小型的基于Transformer的模型来预测用于多跳问答的片段效用分数。研究利用了一个先进推理模型的推理轨迹来捕捉回答问题时片段使用的顺序，并以此获得合成训练数据。

**Result:** 通过全面的实验，研究验证了基于效用的片段再排序比基于相关性的重新排序方法在多跳问答任务上表现更好。

**Conclusion:** 研究证明，通过预测片段在多跳问答中的效用得分，并根据这些得分重新排序片段，可以提高问答系统的性能。

**Abstract:** Multihop Question Answering (QA) requires systems to identify and synthesize information from multiple text passages. While most prior retrieval methods assist in identifying relevant passages for QA, further assessing the utility of the passages can help in removing redundant ones, which may otherwise add to noise and inaccuracies in the generated answers. Existing utility prediction approaches model passage utility independently, overlooking a critical aspect of multihop reasoning: the utility of a passage can be context-dependent, influenced by its relation to other passages - whether it provides complementary information or forms a crucial link in conjunction with others. In this paper, we propose a lightweight approach to model contextual passage utility, accounting for inter-passage dependencies. We fine-tune a small transformer-based model to predict passage utility scores for multihop QA. We leverage the reasoning traces from an advanced reasoning model to capture the order in which passages are used to answer a question and obtain synthetic training data. Through comprehensive experiments, we demonstrate that our utility-based scoring of retrieved passages leads to improved reranking and downstream QA performance compared to relevance-based reranking methods.

</details>


### [10] [Knowing What's Missing: Assessing Information Sufficiency in Question Answering](https://arxiv.org/abs/2512.06476)
*Akriti Jain,Aparna Garimella*

Main category: cs.CL

> 提出了一种识别然后验证的框架，用于提升问答系统在判断上下文信息是否充足的能力，特别是在需要推理的问题上。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有问答系统在处理需要推理的问题时，难以区分上下文信息是否充足的问题。

**Method:** 设计了一种识别然后验证的框架，首先生成多个假设来识别缺少的信息，并建立语义一致，之后对这些信息进行验证以确认是否确实缺失。

**Result:** 与现有基准相比，在多跳推理和事实性问答数据集上表现更优，更具准确的充足性判断能力，并明确指出信息缺口。

**Conclusion:** 该框架通过指导模型证明其关于缺失信息的主张，提高了问答系统的准确性及信息缺失的清晰度。

**Abstract:** Determining whether a provided context contains sufficient information to answer a question is a critical challenge for building reliable question-answering systems. While simple prompting strategies have shown success on factual questions, they frequently fail on inferential ones that require reasoning beyond direct text extraction. We hypothesize that asking a model to first reason about what specific information is missing provides a more reliable, implicit signal for assessing overall sufficiency. To this end, we propose a structured Identify-then-Verify framework for robust sufficiency modeling. Our method first generates multiple hypotheses about missing information and establishes a semantic consensus. It then performs a critical verification step, forcing the model to re-examine the source text to confirm whether this information is truly absent. We evaluate our method against established baselines across diverse multi-hop and factual QA datasets. The results demonstrate that by guiding the model to justify its claims about missing information, our framework produces more accurate sufficiency judgments while clearly articulating any information gaps.

</details>


### [11] [Classifying German Language Proficiency Levels Using Large Language Models](https://arxiv.org/abs/2512.06483)
*Elias-Leander Ahlers,Witold Brunsmann,Malte Schilling*

Main category: cs.CL

> 论文研究了使用大型语言模型自动将德语文本分类到不同的CEFR水平，结果表明LLMs在CEFR分类中表现优异。

<details>
  <summary>Details</summary>

**Motivation:** 该论文旨在研究使用大型语言模型（LLMs）自动将德语文本根据欧洲语言共同参考框架（CEFR）分类到不同的熟练度水平，以便为教育提供量身定制的教学方案。

**Method:** 此论文通过结合多个现有的CEFR注释语料库和合成数据来构建一个多样化数据集，并且评估了提示工程策略、对LLaMA-3-8B-Instruct模型的微调以及利用LLM内部神经状态的探针方法进行分类。

**Result:** 结果表明，LLMs在可靠和可扩展的CEFR分类上优于先前的方法，并且表现有所提升。

**Conclusion:** 论文结论指出了使用LLMs进行CEFR分类的潜力，并且在性能上显示出了一致的提升。

**Abstract:** Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach that utilizes the internal neural state of the LLM for classification. Our results show a consistent performance improvement over prior methods, highlighting the potential of LLMs for reliable and scalable CEFR classification.

</details>


### [12] [ProSocialAlign: Preference Conditioned Test Time Alignment in Language Models](https://arxiv.org/abs/2512.06515)
*Somnath Banerjee,Sayan Layek,Sayantan Adak,Mykola Pechenizkiy,Animesh Mukherjee,Rima Hazra*

Main category: cs.CL

> 提出了ProSocialAlign框架以提升语言模型在高风险情境中的安全性、同理心和价值对齐，实验证明其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 当前的语言模型安全范式在情绪激烈或高风险环境中常常不足，仅拒绝式应对可能会疏远用户，而未经训练的顺从会增加风险。

**Method:** 我们提出了ProSocialAlign，这是一个测试时使用的、参数高效的框架，旨在在不重新训练基础模型的情况下，导向生成安全、富有同理心和价值观对齐的响应。该方法结合了方向性调节和偏好感知的自回归奖励建模。

**Result:** 在五个安全基准的实验评估中表现出最先进的性能，减少了不安全的内容泄漏，增强了对人类价值观的对齐，多个评估指标上都有显著的提升。

**Conclusion:** ProSocialAlign提供了一个强大的、模块化的基础，可以在推断时生成敏感上下文、安全和人类对齐的响应。

**Abstract:** Current language model safety paradigms often fall short in emotionally charged or high-stakes settings, where refusal-only approaches may alienate users and naive compliance can amplify risk. We propose ProSocialAlign, a test-time, parameter-efficient framework that steers generation toward safe, empathetic, and value-aligned responses without retraining the base model. We formalize five human-centered objectives and cast safety as lexicographic constrained generation: first, applying hard constraints to eliminate harmful continuations; then optimizing for prosocial quality within the safe set. Our method combines (i) directional regulation, a harm-mitigation mechanism that subtracts a learned "harm vector" in parameter space, and (ii) preference-aware autoregressive reward modeling trained jointly across attributes with gradient conflict resolution, enabling fine-grained, user-controllable decoding. Empirical evaluations across five safety benchmarks demonstrate state-of-the-art performance, reducing unsafe leakage and boosting alignment to human values, with strong gains across multiple evaluation metrics. ProSocialAlign offers a robust and modular foundation for generating context-sensitive, safe, and human-aligned responses at inference time.

</details>


### [13] [Adapting AlignScore Mertic for Factual Consistency Evaluation of Text in Russian: A Student Abstract](https://arxiv.org/abs/2512.06586)
*Mikhail Zimin,Milyausha Shamsutdinova,Georgii Andriushchenko*

Main category: cs.CL

> 该论文介绍了AlignRuScore，这是一个针对俄语文本的事实一致性评估工具，基于RuBERT模型并进行了特定任务的微调。研究成果表明，统一的事实一致性评估指标可以成功应用于俄语，为多语言事实一致性的评估奠定了基础。

<details>
  <summary>Details</summary>

**Motivation:** 当前缺乏针对俄语文本的事实一致性评估工具，现有工具主要是针对英文语料。本文旨在填补这一空白。

**Method:** Structure

**Result:** {
  "tldr": "该论文介绍了AlignRuScore，这是一个针对俄语文本的事实一致性评估工具，基于RuBERT模型并进行了特定任务的微调。研究成果表明，统一的事实一致性评估指标可以成功应用于俄语，为多语言事实一致性的评估奠定了基础。", 
  "motivation": "当前缺乏针对俄语文本的事实一致性评估工具，现有工具主要是针对英文语料。本文旨在填补这一空白。", 
  "method": "通过在俄语和翻译成俄语的英语数据集上训练带有特定任务分类和回归头的RuBERT对齐模型来调整AlignScore指标。", 
  "result": "证明了统一的事实一致性评估指标可以被成功移植到俄语上。", 
  "conclusion": "该研究为健全的多语言事实一致性评估奠定了基础，同时发布了相关翻译语料库、模型检查点和代码以支持进一步研究。", 
  "paper_text": "Ensure factual consistency in generated text is essential for reliable natural language processing applications. There is a lack of evaluation tools for factual consistency in Russian texts, as existing tools mainly focus on English corpora. This paper introduces AlignRuScore, a comprehensive adaptation of the AlignScore metric for Russian. The adaptation of the metric was achieved by fine-tuning a RuBERT-based alignment model with task-specific classification and regression heads on Russian and translated English datasets. Results show that a unified alignment metric can be successfully ported to Russian, laying the groundwork for robust multilingual factual consistency evaluation. The translated corpora, model checkpoints, and code are released to support further research. "
}

**Conclusion:** 该研究为健全的多语言事实一致性评估奠定了基础，同时发布了相关翻译语料库、模型检查点和代码以支持进一步研究。

**Abstract:** Ensuring factual consistency in generated text is crucial for reliable natural language processing applications. However, there is a lack of evaluation tools for factual consistency in Russian texts, as existing tools primarily focus on English corpora. To bridge this gap, we introduce AlignRuScore, a comprehensive adaptation of the AlignScore metric for Russian. To adapt the metric, we fine-tuned a RuBERT-based alignment model with task-specific classification and regression heads on Russian and translated English datasets. Our results demonstrate that a unified alignment metric can be successfully ported to Russian, laying the groundwork for robust multilingual factual consistency evaluation. We release the translated corpora, model checkpoints, and code to support further research.

</details>


### [14] [The Online Discourse of Virtual Reality and Anxiety](https://arxiv.org/abs/2512.06656)
*Kwabena Yamoah,Cass Dykeman*

Main category: cs.CL

> 研究利用语料语言学方法分析了有关虚拟现实和焦虑的在线讨论。结果显示讨论很多围绕VR硬件（如Oculus头显）和VR在设计、体验和开发方面的应用。这些发现打开了新的虚拟治疗途径。

<details>
  <summary>Details</summary>

**Motivation:** 理解用户对这一技术的看法可能进一步支持其疗效，旨在识别词汇和词汇网络以更好地了解在线讨论中的虚拟现实与焦虑。

**Method:** 本研究采用语料语言学方法识别虚拟现实与焦虑在线讨论中的常用词汇和词汇网络，使用了Sketch Engine软件进行分析。

**Result:** 研究结果显示，在VR与焦虑子语料库中，VR、Oculus和头显是最常被讨论的话题，研究还发现与设计、体验和开发相关的介词短语共现情况。

**Conclusion:** 这些发现为VR与焦虑在一般讨论中的交流提供了新视角，并为通过开发和提升可访问性来支持咨询需求提供了新的途径。

**Abstract:** VR in the treatment of clinical concerns such as generalized anxiety disorder or social anxiety. VR has created additional pathways to support patient well-being and care. Understanding online discussion of what users think about this technology may further support its efficacy. The purpose of this study was to employ a corpus linguistic methodology to identify the words and word networks that shed light on the online discussion of virtual reality and anxiety. Using corpus linguistics, frequently used words in discussion along with collocation were identified by utilizing Sketch Engine software. The results of the study, based upon the English Trends corpus, identified VR, Oculus, and headset as the most frequently discussed within the VR and anxiety subcorpus. These results point to the development of the virtual system, along with the physical apparatus that makes viewing and engaging with the virtual environment possible. Additional results point to collocation of prepositional phrases such as of virtual reality, in virtual reality, and for virtual reality relating to the design, experience, and development, respectively. These findings offer new perspective on how VR and anxiety together are discussed in general discourse and offer pathways for future opportunities to support counseling needs through development and accessibility. Keywords: anxiety disorders, corpus linguistics, Sketch Engine, and virtual reality VR

</details>


### [15] [CMV-Fuse: Cross Modal-View Fusion of AMR, Syntax, and Knowledge Representations for Aspect Based Sentiment Analysis](https://arxiv.org/abs/2512.06679)
*Smitha Muthya Sudheendra,Mani Deep Cherukuri,Jaideep Srivastava*

Main category: cs.CL

> CMV-Fuse通过融合多个语言视角，增强了基于方面的意见分析（ABSA），在标准基准测试上取得了比强基线更优的表现。

<details>
  <summary>Details</summary>

**Motivation:** 自然语言理解本质上需要综合多个从表面语法到深层语义和世界知识的互补视角，但当前的基于方面的意见分析（ABSA）系统通常只利用孤立的语言视角，忽视了人类自然而然利用的结构表示之间的复杂交互。

**Method:** 我们提出了CMV-Fuse框架，该框架融合了四个语言视角：抽象意义表示、句法分析、依赖关系语法和语义注意，并结合外部知识集成。通过层次化的门控注意融合机制，CMV-Fuse在局部句法、中间语义和全局知识层面捕捉细微的结构模式和广泛的上下文理解。此外，还设计了一种通过结构感知多视角对比学习机制，保证了互补表示的一致性，同时保持计算效率。

**Result:** 实验结果表明，相较于强基线，CMV-Fuse在标准基准测试上取得了显著的改进。分析显示，每个语言视角都为更稳健的意见分析做出了贡献。

**Conclusion:** 实验结果表明，CMV-Fuse在标准基准测试上的表现显著优于强基线，并且分析显示每个语言视角的贡献有助于更稳健的意见分析。

**Abstract:** Natural language understanding inherently depends on integrating multiple complementary perspectives spanning from surface syntax to deep semantics and world knowledge. However, current Aspect-Based Sentiment Analysis (ABSA) systems typically exploit isolated linguistic views, thereby overlooking the intricate interplay between structural representations that humans naturally leverage. We propose CMV-Fuse, a Cross-Modal View fusion framework that emulates human language processing by systematically combining multiple linguistic perspectives. Our approach systematically orchestrates four linguistic perspectives: Abstract Meaning Representations, constituency parsing, dependency syntax, and semantic attention, enhanced with external knowledge integration. Through hierarchical gated attention fusion across local syntactic, intermediate semantic, and global knowledge levels, CMV-Fuse captures both fine-grained structural patterns and broad contextual understanding. A novel structure aware multi-view contrastive learning mechanism ensures consistency across complementary representations while maintaining computational efficiency. Extensive experiments demonstrate substantial improvements over strong baselines on standard benchmarks, with analysis revealing how each linguistic view contributes to more robust sentiment analysis.

</details>


### [16] [Mechanistic Interpretability of GPT-2: Lexical and Contextual Layers in Sentiment Analysis](https://arxiv.org/abs/2512.06681)
*Amartya Hatua*

Main category: cs.CL

> 该研究通过因果分析探讨了GPT-2在处理情感信息时的不同阶段，并发现其不同于预期的分层处理模式，情感的上下文整合主要发生在深层结构中。

<details>
  <summary>Details</summary>

**Motivation:** 研究GPT-2对情感信息的处理过程，尤其是探讨其假设的情感分析两阶段——早期词检测和中期上下文整合阶段。

**Method:** Structure

**Result:** <tool_call>
{{"name": "Structure", "arguments": {"tldr": "该研究通过因果分析探讨了GPT-2在处理情感信息时的不同阶段，并发现其不同于预期的分层处理模式，情感的上下文整合主要发生在深层结构中。", "motivation": "研究GPT-2对情感信息的处理过程，尤其是探讨其假设的情感分析两阶段——早期词检测和中期上下文整合阶段。", "method": "使用系统化的激活修补技术，对GPT-2的12层进行了实验，以检查其情感处理机制。", "result": "研究表明，早期层（0-3）是词汇情感检测器，而上下文整合并不像预期那样在中期层集中，而是在深层（8-11）通过统一机制实现。", "conclusion": "实验提供了因果证据，证明GPT-2的情感计算不同于预期的分层模式，需要进一步对大型语言模型中的上下文整合进行经验性的特征刻画。"}}}
</tool_call>

**Conclusion:** 实验提供了因果证据，证明GPT-2的情感计算不同于预期的分层模式，需要进一步对大型语言模型中的上下文整合进行经验性的特征刻画。

**Abstract:** We present a mechanistic interpretability study of GPT-2 that causally examines how sentiment information is processed across its transformer layers. Using systematic activation patching across all 12 layers, we test the hypothesized two-stage sentiment architecture comprising early lexical detection and mid-layer contextual integration. Our experiments confirm that early layers (0-3) act as lexical sentiment detectors, encoding stable, position specific polarity signals that are largely independent of context. However, all three contextual integration hypotheses: Middle Layer Concentration, Phenomenon Specificity, and Distributed Processing are falsified. Instead of mid-layer specialization, we find that contextual phenomena such as negation, sarcasm, domain shifts etc. are integrated primarily in late layers (8-11) through a unified, non-modular mechanism. These experimental findings provide causal evidence that GPT-2's sentiment computation differs from the predicted hierarchical pattern, highlighting the need for further empirical characterization of contextual integration in large language models.

</details>


### [17] [PersonaMem-v2: Towards Personalized Intelligence via Learning Implicit User Personas and Agentic Memory](https://arxiv.org/abs/2512.06688)
*Bowen Jiang,Yuan Yuan,Maohao Shen,Zhuoqun Hao,Zhangchen Xu,Zichen Chen,Ziyi Liu,Anvesh Rao Vijjini,Jiashu He,Hanchao Yu,Radha Poovendran,Gregory Wornell,Lyle Ungar,Dan Roth,Sihao Chen,Camillo Jose Taylor*

Main category: cs.CL

> 研究通过强化细调训练Qwen3-4B，使其在隐式个性化方面优于GPT-5，并提出了一个基于代理记忆系统的框架，展示了在较少输入代的情况下达到了55%的准确率。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在通过强化细调技术提高LLM在理解用户和个性化方面的长期上下文推理能力，同时探索代理记忆系统作为个性化智能的一个可扩展路径。

**Method:** 我们采用强化细调技术来训练大型语言模型（LLM）以改善其对长期上下文的理解和个性化能力。我们还开发了一个训练代理记忆系统的框架，这个系统随着时间的推移为每个用户维护单一的人类可读记忆。

**Result:** 结果显示，使用强化细调，Qwen3-4B取得了53%的隐式个性化准确度，优于其他前沿的LLM。代理记忆框架通过使用一个2k令牌记忆而非完整的32k对话历史，实现了55%的准确率，输入令牌减少了16倍。

**Conclusion:** 我们的结果强调了PersonaMem-v2数据集在促进隐式个性化研究中的重要性，并表明代理记忆系统是实现真实世界个性化智能的一个可扩展的方法。

**Abstract:** Personalization is one of the next milestones in advancing AI capability and alignment. We introduce PersonaMem-v2, the state-of-the-art dataset for LLM personalization that simulates 1,000 realistic user-chatbot interactions on 300+ scenarios, 20,000+ user preferences, and 128k-token context windows, where most user preferences are implicitly revealed to reflect real-world interactions. Using this data, we investigate how reinforcement fine-tuning enables a model to improve its long-context reasoning capabilities for user understanding and personalization. We also develop a framework for training an agentic memory system, which maintains a single, human-readable memory that grows with each user over time.
  In our experiments, frontier LLMs still struggle with implicit personalization, achieving only 37-48% accuracy. While they support long context windows, reasoning remains the bottleneck for implicit personalization tasks. Using reinforcement fine-tuning, we successfully train Qwen3-4B to outperforms GPT-5, reaching 53% accuracy in implicit personalization. Moreover, our agentic memory framework achieves state-of-the-art 55% accuracy while using 16x fewer input tokens, relying on a 2k-token memory instead of full 32k conversation histories. These results underscore the impact of our dataset and demonstrate agentic memory as a scalable path toward real-world personalized intelligence.

</details>


### [18] [Think-While-Generating: On-the-Fly Reasoning for Personalized Long-Form Generation](https://arxiv.org/abs/2512.06690)
*Chengbing Wang,Yang Zhang,Wenjie Wang,Xiaoyan Zhao,Fuli Feng,Xiangnan He,Tat-Seng Chua*

Main category: cs.CL

> The paper introduces FlyThinker, a novel 'think-while-generating' framework that improves personalized long-form generation in large language models by enabling concurrent reasoning and generation, thus maintaining efficiency.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of existing 'think-then-generate' methods in capturing and adapting to individual users' implicit preferences, particularly in the context of long-form generation, where static reasoning is insufficient.

**Method:** FlyThinker, an efficient 'think-while-generating' framework for personalized long-form generation, employs a separate reasoning model that generates latent token-level reasoning in parallel, fused into the generation model to dynamically guide response generation.

**Result:** Extensive experiments on real-world benchmarks show that FlyThinker achieves better personalized generation while maintaining training and inference efficiency.

**Conclusion:** FlyThinker demonstrates superior performance in personalized generation tasks, while also preserving computational efficiency through its innovative design that supports parallel training and inference.

**Abstract:** Preference alignment has enabled large language models (LLMs) to better reflect human expectations, but current methods mostly optimize for population-level preferences, overlooking individual users. Personalization is essential, yet early approaches-such as prompt customization or fine-tuning-struggle to reason over implicit preferences, limiting real-world effectiveness. Recent "think-then-generate" methods address this by reasoning before response generation. However, they face challenges in long-form generation: their static one-shot reasoning must capture all relevant information for the full response generation, making learning difficult and limiting adaptability to evolving content. To address this issue, we propose FlyThinker, an efficient "think-while-generating" framework for personalized long-form generation. FlyThinker employs a separate reasoning model that generates latent token-level reasoning in parallel, which is fused into the generation model to dynamically guide response generation. This design enables reasoning and generation to run concurrently, ensuring inference efficiency. In addition, the reasoning model is designed to depend only on previous responses rather than its own prior outputs, which preserves training parallelism across different positions-allowing all reasoning tokens for training data to be produced in a single forward pass like standard LLM training, ensuring training efficiency. Extensive experiments on real-world benchmarks demonstrate that FlyThinker achieves better personalized generation while keeping training and inference efficiency.

</details>


### [19] [TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction](https://arxiv.org/abs/2512.06694)
*Aoi Fujita,Taichi Yamamoto,Yuri Nakayama,Ryota Kobayashi*

Main category: cs.CL

> 提出TopiCLEAR方法，通过结合词嵌入和迭代聚类技术改进社交媒体短文本的主题模型，并在多个数据集上验证了其优越性。

<details>
  <summary>Details</summary>

**Motivation:** 传统的主题建模方法对于社交媒体中的短文本分析不够有效，本文旨在解决由于短文本所导致的挑战，例如片段化的语义和非正式语言。

**Method:** TopiCLEAR方法包括两步：使用Sentence-BERT生成文本嵌入，并使用高斯混合模型进行初步聚类；通过线性判别分析和迭代的GMM聚类进行改进，直到收敛。方法可以直接处理原始文本，无需预处理。

**Result:** 在20News、AgNewsTitle、Reddit和TweetTopic四个数据集中进行了评估，结果显示本方法在多个人机标注数据集上取得了最高相似度，并超过了七个基准方法。

**Conclusion:** 研究表明，TopiCLEAR能够在社交媒体数据和网络内容分析中生成更具解释性的主题，提供一种有效的主题建模方法。

**Abstract:** Rapid expansion of social media platforms such as X (formerly Twitter), Facebook, and Reddit has enabled large-scale analysis of public perceptions on diverse topics, including social issues, politics, natural disasters, and consumer sentiment. Topic modeling is a widely used approach for uncovering latent themes in text data, typically framed as an unsupervised classification task. However, traditional models, originally designed for longer and more formal documents, struggle with short social media posts due to limited co-occurrence statistics, fragmented semantics, inconsistent spelling, and informal language. To address these challenges, we propose a new method, TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction. Specifically, each text is embedded using Sentence-BERT (SBERT) and provisionally clustered using Gaussian Mixture Models (GMM). The clusters are then refined iteratively using a supervised projection based on linear discriminant analysis, followed by GMM-based clustering until convergence. Notably, our method operates directly on raw text, eliminating the need for preprocessing steps such as stop word removal. We evaluate our approach on four diverse datasets, 20News, AgNewsTitle, Reddit, and TweetTopic, each containing human-labeled topic information. Compared with seven baseline methods, including a recent SBERT-based method and a zero-shot generative AI method, our approach achieves the highest similarity to human-annotated topics, with significant improvements for both social media posts and online news articles. Additionally, qualitative analysis shows that our method produces more interpretable topics, highlighting its potential for applications in social media data and web content analytics.

</details>


### [20] [Parameter-Efficient Fine-Tuning with Differential Privacy for Robust Instruction Adaptation in Large Language Models](https://arxiv.org/abs/2512.06711)
*Yulin Huang,Yaxuan Luan,Jinxu Guo,Xiangchen Song,Yuchen Liu*

Main category: cs.CL

> 本文提出一种参数高效的指令微调方法，集成差分隐私噪音分配与梯度裁剪，应用于大型语言模型中，有效解决了隐私保护和效率问题。实验表明该方法在准确度、隐私预算和参数效率上优于基线模型，并在各种数据条件下保持稳定性能。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于解决大規模语言模型微调过程中的隐私保护和效率问题，特别是在多个任务指令场景中减少性能波动和隐私风险。

**Method:** 方法通过结合差分隐私噪音分配与梯度裁剪构建了一个协作优化框架。基础模型保持冻结，并通过低维投影子空间更新参数，同时在梯度计算过程中引入裁剪和自适应噪音分配。

**Result:** 实验结果表明，该方法在准确性、隐私预算以及参数效率方面均可超越基线模型，并在不同和不确定的数据条件下保持良好的稳定性。

**Conclusion:** 研究丰富了差分隐私和参数高效微调的理论集成，并展示了在指令任务中的实践适应性，为复杂指令环境中安全训练提供了一个切实可行的解决方案。

**Abstract:** This study addresses the issues of privacy protection and efficiency in instruction fine-tuning of large-scale language models by proposing a parameter-efficient method that integrates differential privacy noise allocation with gradient clipping in a collaborative optimization framework. The method keeps the backbone model frozen and updates parameters through a low-dimensional projection subspace, while introducing clipping and adaptive noise allocation during gradient computation. This design reduces privacy budget consumption and ensures training stability and robustness. The unified framework combines gradient constraints, noise allocation, and parameter projection, effectively mitigating performance fluctuations and privacy risks in multi-task instruction scenarios. Experiments are conducted across hyperparameter, environment, and data sensitivity dimensions. Results show that the method outperforms baseline models in accuracy, privacy budget, and parameter efficiency, and maintains stable performance under diverse and uncertain data conditions. The findings enrich the theoretical integration of differential privacy and parameter-efficient fine-tuning and demonstrate its practical adaptability in instruction tasks, providing a feasible solution for secure training in complex instruction environments.

</details>


### [21] ["The Dentist is an involved parent, the bartender is not": Revealing Implicit Biases in QA with Implicit BBQ](https://arxiv.org/abs/2512.06732)
*Aarushi Wagh,Saniya Srivastava*

Main category: cs.CL

> 引入 ImplicitBBQ 基准测试，发现 LLMs 存在隐性偏见，且这些偏见未在显式基准测试中被发现。

<details>
  <summary>Details</summary>

**Motivation:** 现有的评估大型语言模型（LLMs）偏差的基准通常依赖于显式提示，例如直接声明宗教、种族、性别等受保护属性。然而，现实世界中的互动往往包含通过名字、文化暗示或特征等微妙地揭示的隐性偏见。这一关键疏漏在公平性评估中造成了一个显著的盲点。

**Method:** 引入了名为 ImplicitBBQ 的基准测试，该测试基于问答偏差基准测试（BBQ），扩展了涵盖 6 类别中的隐含提示的受保护属性。

**Result:** GPT-4o 在 ImplicitBBQ 上的评估结果显示与显式 BBQ 促进符相比存在明显的性能差异，在性取向子类别中准确率下降高达 7%，大多数其他类别也显示出一致的准确率下降。

**Conclusion:** 这表明当前的 LLMs 包含未被显式基准测试发现的隐性偏见。ImplicitBBQ 为 NLP 中的精细公平性评估提供了一个关键工具。

**Abstract:** Existing benchmarks evaluating biases in large language models (LLMs) primarily rely on explicit cues, declaring protected attributes like religion, race, gender by name. However, real-world interactions often contain implicit biases, inferred subtly through names, cultural cues, or traits. This critical oversight creates a significant blind spot in fairness evaluation. We introduce ImplicitBBQ, a benchmark extending the Bias Benchmark for QA (BBQ) with implicitly cued protected attributes across 6 categories. Our evaluation of GPT-4o on ImplicitBBQ illustrates troubling performance disparity from explicit BBQ prompts, with accuracy declining up to 7% in the "sexual orientation" subcategory and consistent decline located across most other categories. This indicates that current LLMs contain implicit biases undetected by explicit benchmarks. ImplicitBBQ offers a crucial tool for nuanced fairness evaluation in NLP.

</details>


### [22] [A Patient-Doctor-NLP-System to contest inequality for less privileged](https://arxiv.org/abs/2512.06734)
*Subrit Dikshit,Ritu Tiwari,Priyank Jain*

Main category: cs.CL

> PDFTEMRA模型在保持语言理解性能的同时减少了计算成本，适用于低资源医疗NLP应用，特别是在农村地区的无障碍医疗场景中使用印度语的视障用户和低资源语言使用者。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在解决资源受限条件下，尤其是在农村环境中为视障人士和低资源语言用户提供医疗帮助的问题。

**Method:** 提出PDFTEMRA模型，该模型结合了模型蒸馏、频域调制、集成学习和随机激活模式，以降低计算成本同时保持语言理解性能。

**Result:** 与标准NLP模型相比，PDFTEMRA在医疗问答和咨询数据集上展示了相当的性能，但计算需求显著降低。

**Conclusion:** 结果表明PDFTEMRA适用于无障碍和包容性的低资源医疗NLP应用场景。

**Abstract:** Transfer Learning (TL) has accelerated the rapid development and availability of large language models (LLMs) for mainstream natural language processing (NLP) use cases. However, training and deploying such gigantic LLMs in resource-constrained, real-world healthcare situations remains challenging. This study addresses the limited support available to visually impaired users and speakers of low-resource languages such as Hindi who require medical assistance in rural environments. We propose PDFTEMRA (Performant Distilled Frequency Transformer Ensemble Model with Random Activations), a compact transformer-based architecture that integrates model distillation, frequency-domain modulation, ensemble learning, and randomized activation patterns to reduce computational cost while preserving language understanding performance. The model is trained and evaluated on medical question-answering and consultation datasets tailored to Hindi and accessibility scenarios, and its performance is compared against standard NLP state-of-the-art model baselines. Results demonstrate that PDFTEMRA achieves comparable performance with substantially lower computational requirements, indicating its suitability for accessible, inclusive, low-resource medical NLP applications.

</details>


### [23] [One Word Is Not Enough: Simple Prompts Improve Word Embeddings](https://arxiv.org/abs/2512.06744)
*Rajeev Ranjan*

Main category: cs.CL

> 

<details>
  <summary>Details</summary>

**Motivation:** 

**Method:** 

**Result:** <tool_call>
default
{{"name": "Structure", "arguments": {"tldr": "通过在单词前加语义提示，可以显著提高文本嵌入模型在单词相似度任务上的表现，某些模型在没有任何提示时甚至无法工作，但通过加提示可以提升高达0.73的相关性评分。这种方法不需要训练，适用于任何文本嵌入模型。", "motivation": "许多文本嵌入模型主要用于句子级别任务，对其在孤立词语上的行为了解不多，尝试通过简单添加语义提示来评估其在词语相似度上的表现。", "method": "测试了7种文本嵌入模型，如text-embedding-3-large、embed-english-v3.0等，使用诸如'meaning: {word}'或'Represent the semantic concept: {word}'的提示。", "result": "结果表明，使用提示可以显著提高在多个标准基准（如SimLex-999、WordSim-353、MEN-3000）上的斯皮尔曼等级相关性，最好的模型在SimLex-999上可以达到0.692的相关性。", "conclusion": "零样本技巧加语义提示的方法提高了基于嵌入模型的语义相似性评估效果，超越了诸如Word2Vec和LexVec的传统静态嵌入方法，确立了纯嵌入方法的新基准。"}}}
</tool_call>

**Conclusion:** 

**Abstract:** Text embedding models are designed for sentence-level applications like retrieval and semantic similarity, and are primarily evaluated on sentence-level benchmarks. Their behavior on isolated words is less understood. We show that simply prepending semantic prompts to words before embedding substantially improves word similarity correlations. Testing 7 text embedding models, including text-embedding-3-large (OpenAI), embed-english-v3.0 (Cohere), voyage-3(Voyage AI), all-mpnet-base-v2, and Qwen3-Embedding-8B, on 3 standard benchmarks (SimLex-999, WordSim-353, MEN-3000), we find that prompts like "meaning: {word}" or "Represent the semantic concept: {word}" improve Spearman correlations by up to +0.29 on SimLex-999. Some models fail completely on bare words (correlation = 0) but recover with prompts (+0.73 improvement). Our best results achieve correlation = 0.692 on SimLex-999 with embed-english-v3.0 (Cohere), correlation = 0.811 on WordSim-353, and correlation = 0.855 on MEN-3000 with text-embedding-3-large (OpenAI). These results outperform classic static embeddings like Word2Vec (correlation = 0.40) and even the best static method LexVec (correlation = 0.48) on SimLex-999, establishing a new state-of-the-art for pure embedding methods. This zero-shot technique requires no training and works with any text embedding model.

</details>


### [24] [Becoming Experienced Judges: Selective Test-Time Learning for Evaluators](https://arxiv.org/abs/2512.06751)
*Seungyeon Jwa,Daechul Ahn,Reokyoung Kim,Dongyeop Kang,Jonghyun Choi*

Main category: cs.CL

> 论文提出了一种在推理过程中逐步提高的评估框架LWE及其选择性版本，展示了评估器可以通过自我反馈和学习提高其评估能力，且选择性更新方法效率更高。

<details>
  <summary>Details</summary>

**Motivation:** 当前基于语言模型的自动评估方法通常以隔离的方式评估每个案例，错过了积累经验的机会，同时依赖于统一固定的评估提示，忽略了样本特定评估标准的需要。这篇论文的动机是解决这一问题，提高评估器在推理过程中的性能。

**Method:** 提出了一种名为LWE(在评估中学习)的框架，允许评估器在推理时无需训练集或验证集就能逐步提高。LWE通过自生反馈维持一个不断演化的元提示，能够生成样本特定的评估指令，并通过自我生成的反馈来自我改进。同时，还提出了一种选择性LWE方法，仅在评估器自我不一致的案例上更新元提示，以提高计算效率。

**Result:** 选择性LWE方法在两个对比较准测试中超越了强大的基准模型，实证表明评估器可以通过选择性更新在顺序测试中提高自身的评估性能，更有效学习困难的案例。

**Conclusion:** 通过提出LWE框架及其选择性版本，研究表明自动评估器可以通过自我生成的反馈在推理时逐步提高，且这些改进可以通过选择性更新来高效实现，特别是在最容易导致评估不一致的案例上。

**Abstract:** Automatic evaluation with large language models, commonly known as LLM-as-a-judge, is now standard across reasoning and alignment tasks. Despite evaluating many samples in deployment, these evaluators typically (i) treat each case independently, missing the opportunity to accumulate experience, and (ii) rely on a single fixed prompt for all cases, neglecting the need for sample-specific evaluation criteria. We introduce Learning While Evaluating (LWE), a framework that allows evaluators to improve sequentially at inference time without requiring training or validation sets. LWE maintains an evolving meta-prompt that (i) produces sample-specific evaluation instructions and (ii) refines itself through self-generated feedback. Furthermore, we propose Selective LWE, which updates the meta-prompt only on self-inconsistent cases, focusing computation where it matters most. This selective approach retains the benefits of sequential learning while being far more cost-effective. Across two pairwise comparison benchmarks, Selective LWE outperforms strong baselines, empirically demonstrating that evaluators can improve during sequential testing with a simple selective update, learning most from the cases they struggle with.

</details>


### [25] [From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs](https://arxiv.org/abs/2512.06776)
*Yuchuan Tian,Yuchen Liang,Jiacheng Sun,Shuo Zhang,Guangwen Yang,Yingte Shu,Sibo Fang,Tianyu Guo,Kai Han,Chao Xu,Hanting Chen,Xinghao Chen,Yunhe Wang*

Main category: cs.CL

> 提出了一种新的自回归到块扩散模型的适应方法，该方法结合了两者的优点，有效提高了语言模型的生成效率，同时保持了对长上下文建模和推理的能力，获得了70亿参数级扩散语言模型中的最佳性能。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于解决大型语言模型在生成上的效率瓶颈问题，通过适应性地从现有的AR模型中提取知识并转换为块扩散模型，以实现生成并行化和块内双向推理，同时避免从零开始训练大型扩散语言模型所需的巨大计算成本。

**Method:** 该研究设计了一种从自回归（AR）到块式扩散语言模型的适应路径，采用了上下文因果注意力掩码、高效的并行适应过程、辅助AR损失来最大化数据利用率并保持预训练知识，以及逐步增加生成块大小的方法。此方案与掩码块扩散模型兼容，并保持训练和推理之间的一致性。

**Result:** 本研究提出了一种新的方法来适应性地将自回归（AR）语言模型转换为块扩散（Block-Diffusion）语言模型，从而结合了两者的优势，以提高生成效率并保留预训练知识。该方法改变了先前的适应策略，提出了一条从AR到块扩散的路径，即将AR视为块大小等于1的块扩散模型。实验结果表明，该方法能够在保持长上下文建模和推理能力的同时，实现同类70亿参数模型中的最佳性能，并在通用知识、数学和代码基准测试中显著优于强基线模型。

**Conclusion:** 该研究通过设计一条从自回归到块扩散模型的适应路径，成功建立了一个70亿参数的语言模型NBDiff，其表现优于同类模型，并说明了从自回归模型到块扩散模型的原则性适应是一种既有效又计算高效的替代方案，避免了从零开始训练扩散模型的高成本。

**Abstract:** Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior "adaptation" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.

</details>


### [26] [LLM4SFC: Sequential Function Chart Generation via Large Language Models](https://arxiv.org/abs/2512.06787)
*Ofek Glick,Vladimir Tchuiev,Marah Ghoummaid,Michal Moshkovitz,Dotan Di-Castro*

Main category: cs.CL

> 论文提出并实现了一个名为LLM4SFC的框架，用于根据自然语言描述生成符合工业标准的顺序功能图(SFC)。通过微调和技术优化，成功率达到75% - 94%，实现了将自然语言转换为可执行SFC程序的自动化过程。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型目前广泛用于合成PLC编程语言，如结构化文本，但对于IEC 61131-3标准下的图形语言生成仍处于探索阶段。生成顺序功能图(SFC)具有挑战性，因为其图形性质和嵌入的ST代码与标准生成技术不兼容，往往产生无法执行的代码。因此，我们提出了LLM4SFC，旨在使其成为能够从自然语言描述生成可执行SFC的首个框架。

**Method:** 该论文提出了LLM4SFC框架，用于根据自然语言描述生成可执行的顺序功能图(SFC)，框架基于三个方面：(i) 一种简化的结构化表示方法，用于捕捉SFC的基本拓扑结构、嵌入的ST代码以及减少文本冗余；(ii) 微调和少量样本检索增强的生成方法，以满足与SFC编程惯例的对齐；(iii) 结构化生成方法，实时修剪非法标记，以确保与SFC文本格式的合规性。

**Result:** 作者使用来自自动化制造项目的实际SFC数据集评估了LLM4SFC，实验结果表明，LLM4SFC能够可靠地生成有效的SFC程序，成功率达到75% - 94%。

**Conclusion:** LLM4SFC为动态生成可执行SFC提供了有效解决方案，这为自动化工业编程过程奠定了基础，显著推动了将图形语言和文本编程语言结合的进程。

**Abstract:** While Large Language Models (LLMs) are increasingly used for synthesizing textual PLC programming languages like Structured Text (ST) code, other IEC 61131-3 standard graphical languages like Sequential Function Charts (SFCs) remain underexplored. Generating SFCs is challenging due to graphical nature and ST actions embedded within, which are not directly compatible with standard generation techniques, often leading to non-executable code that is incompatible with industrial tool-chains In this work, we introduce LLM4SFC, the first framework to receive natural-language descriptions of industrial workflows and provide executable SFCs. LLM4SFC is based on three components: (i) A reduced structured representation that captures essential topology and in-line ST and reduced textual verbosity; (ii) Fine-tuning and few-shot retrieval-augmented generation (RAG) for alignment with SFC programming conventions; and (iii) A structured generation approach that prunes illegal tokens in real-time to ensure compliance with the textual format of SFCs. We evaluate LLM4SFC on a dataset of real-world SFCs from automated manufacturing projects, using both open-source and proprietary LLMs. The results show that LLM4SFC reliably generates syntactically valid SFC programs effectively bridging graphical and textual PLC languages, achieving a generation generation success of 75% - 94%, paving the way for automated industrial programming.

</details>


### [27] [Large Language Model-Based Generation of Discharge Summaries](https://arxiv.org/abs/2512.06812)
*Tiago Rodrigues,Carla Teixeira Lopes*

Main category: cs.CL

> 论文研究表明，专有语言模型，特别是Gemini模型，优于开源模型，成功生成了相似度高的出院总结，尽管存在一些不足。

<details>
  <summary>Details</summary>

**Motivation:** 出院总结包含了患者护理的重要信息，自动化的生成可以减少医护人员的工作量，降低错误率，并确保关键信息易于访问及操作。

**Method:** 论文探讨了五种大型语言模型在自动生成出院总结方面的应用，这些模型包括开源模型（如Mistral和Llama 2）以及专有系统（如GPT-3、GPT-4和Gemini 1.5 Pro），并使用了MIMIC-III摘要和笔记作为数据来源。模型被评估采用精确匹配、软重叠及无参考指标。

**Result:** 实验结果表明，专有模型，特别是使用单次提示的Gemini模型，表现最佳，生成的摘要与标准摘要的相似度最高。开源模型虽有潜力，但在幻象和重复信息方面存在不足，即使是调整后的Mistral也是如此。临床专家的人工评估证实了专有模型生成摘要的实际价值。

**Conclusion:** 研究发现，尽管存在幻象和信息缺失等挑战，但大型语言模型，尤其是专有模型，在保证数据隐私的前提下，是自动出院总结生成的有希望的选择。

**Abstract:** Discharge Summaries are documents written by medical professionals that detail a patient's visit to a care facility. They contain a wealth of information crucial for patient care, and automating their generation could significantly reduce the effort required from healthcare professionals, minimize errors, and ensure that critical patient information is easily accessible and actionable. In this work, we explore the use of five Large Language Models on this task, from open-source models (Mistral, Llama 2) to proprietary systems (GPT-3, GPT-4, Gemini 1.5 Pro), leveraging MIMIC-III summaries and notes. We evaluate them using exact-match, soft-overlap, and reference-free metrics. Our results show that proprietary models, particularly Gemini with one-shot prompting, outperformed others, producing summaries with the highest similarity to the gold-standard ones. Open-source models, while promising, especially Mistral after fine-tuning, lagged in performance, often struggling with hallucinations and repeated information. Human evaluation by a clinical expert confirmed the practical utility of the summaries generated by proprietary models. Despite the challenges, such as hallucinations and missing information, the findings suggest that LLMs, especially proprietary models, are promising candidates for automatic discharge summary generation as long as data privacy is ensured.

</details>


### [28] [CAuSE: Decoding Multimodal Classifiers using Faithful Natural Language Explanation](https://arxiv.org/abs/2512.06814)
*Dibyanayan Bandyopadhyay,Soham Bhattacharjee,Mohammed Hasanuzzaman,Asif Ekbal*

Main category: cs.CL

> 研究提出CAuSE框架，用于为多模态分类器生成忠实的自然语言解释，展现了其在多种场景下的优势并公开了代码。

<details>
  <summary>Details</summary>

**Motivation:** 多种解释分类器预测的技术存在，但极少像自然语言解释一样直观和易于理解。为了建立信任，这些解释必须忠实反映分类器的内部决策行为。

**Method:** 本研究提出了一种名为CAuSE (Causal Abstraction under Simulated Explanations)的新框架，旨在为预训练的多模态分类器生成忠实的自然语言解释(NLEs)。通过互换干预训练CAuSE，以形成对底层分类器的因果抽象。

**Result:** 研究显示，CAuSE在多个数据集和模型上泛化良好，特别是在重新设计的多模态设置下度量因果忠实性的指标上超过了其他方法。定性分析进一步证实了其优势。

**Conclusion:** CAuSE作为一个新的框架，不仅能生成忠实的自然语言解释，而且在不同数据集和模型上表现出了泛化能力，通过互换干预训练进一步验证了其因果抽象特性。

**Abstract:** Multimodal classifiers function as opaque black box models. While several techniques exist to interpret their predictions, very few of them are as intuitive and accessible as natural language explanations (NLEs). To build trust, such explanations must faithfully capture the classifier's internal decision making behavior, a property known as faithfulness. In this paper, we propose CAuSE (Causal Abstraction under Simulated Explanations), a novel framework to generate faithful NLEs for any pretrained multimodal classifier. We demonstrate that CAuSE generalizes across datasets and models through extensive empirical evaluations. Theoretically, we show that CAuSE, trained via interchange intervention, forms a causal abstraction of the underlying classifier. We further validate this through a redesigned metric for measuring causal faithfulness in multimodal settings. CAuSE surpasses other methods on this metric, with qualitative analysis reinforcing its advantages. We perform detailed error analysis to pinpoint the failure cases of CAuSE. For replicability, we make the codes available at https://github.com/newcodevelop/CAuSE

</details>


### [29] [AquaFusionNet: Lightweight VisionSensor Fusion Framework for Real-Time Pathogen Detection and Water Quality Anomaly Prediction on Edge Devices](https://arxiv.org/abs/2512.06848)
*Sepyan Purnama Kristanto,Lutfi Hakim,Hermansyah*

Main category: cs.CL

> AquaFusionNet是一套轻量级跨模式框架，能有效结合显微镜图像和理化传感器数据来实时监测饮用水中的微生物污染，相较于单一模式检测器，具有更高的准确性和更低的功耗。

<details>
  <summary>Details</summary>

**Motivation:** 现有监测工具只能捕获微观污染行为的一部分，难以进行实时决策。AquaFusionNet旨在通过结合显微镜成像和理化传感器数据，为小规模饮用水系统的实时监测提供更可靠的方法。

**Method:** AquaFusionNet采用了一种轻量级的跨模式框架，结合了显微镜图像和理化传感器数据，在一个边缘部署的模型中统一了这两种信息源。通过专门为低功耗硬件设计的门控交叉注意力机制，AquaFusionNet学习了微生物外观和传感器动态之间的统计依赖关系。

**Result:** 经过六个月在印度尼西亚东爪哇的七个设施中的部署测试，AquaFusionNet处理了184万帧数据，微生物污染事件的检测准确率为94.8% mAP@0.5，异常预测准确率为96.3%，在Jetson Nano上运行功耗为4.8W。跨模式耦合减少了单一模式检测器常见的故障模式。

**Conclusion:** AquaFusionNet为小规模饮用水系统的实时和持续监测提供了有效的解决方案，并且其较低的功耗使其适用于边缘部署，展示了在现实中对水质安全的重要帮助。

**Abstract:** Evidence from many low and middle income regions shows that microbial contamination in small scale drinking water systems often fluctuates rapidly, yet existing monitoring tools capture only fragments of this behaviour. Microscopic imaging provides organism level visibility, whereas physicochemical sensors reveal shortterm changes in water chemistry; in practice, operators must interpret these streams separately, making realtime decision-making unreliable. This study introduces AquaFusionNet, a lightweight cross-modal framework that unifies both information sources inside a single edge deployable model. Unlike prior work that treats microscopic detection and water quality prediction as independent tasks, AquaFusionNet learns the statistical dependencies between microbial appearance and concurrent sensor dynamics through a gated crossattention mechanism designed specifically for lowpower hardware. The framework is trained on AquaMicro12K, a new dataset comprising 12,846 annotated 1000 micrographs curated for drinking water contexts, an area where publicly accessible microscopic datasets are scarce. Deployed for six months across seven facilities in East Java, Indonesia, the system processed 1.84 million frames and consistently detected contamination events with 94.8% mAP@0.5 and 96.3% anomaly prediction accuracy, while operating at 4.8 W on a Jetson Nano. Comparative experiments against representative lightweight detectors show that AquaFusionNet provides higher accuracy at comparable or lower power, and field results indicate that cross-modal coupling reduces common failure modes of unimodal detectors, particularly under fouling, turbidity spikes, and inconsistent illumination. All models, data, and hardware designs are released openly to facilitate replication and adaptation in decentralized water safety infrastructures.

</details>


### [30] [Rhea: Role-aware Heuristic Episodic Attention for Conversational LLMs](https://arxiv.org/abs/2512.06869)
*Wanyang Hong,Zhaoning Zhang,Yi Chen,Libo Zhang,Baihui Liu,Linbo Qiao,Zhiliang Tian,Dongsheng Li*

Main category: cs.CL

> 提出Rhea框架，解决多轮对话中大语言模型性能下降的问题，通过分离指令记忆和情景记忆，提高对话精确度和一致性。

<details>
  <summary>Details</summary>

**Motivation:** 解决大语言模型在多轮对话中的性能下降问题，具体是因为注意力机制导致的上下文累积衰减问题。

**Method:** 提出Rhea框架，包括指令记忆和情景记忆两个独立的记忆模块，通过优先级注意力机制构建高质量的上下文。

**Result:** 在多个多轮对话基准测试中（如MT-Eval和Long-MT-Bench+），Rhea提高了整体准确性并保持了指令的一致性。

**Conclusion:** Rhea框架提供了一个构建更精确、指令一致的多轮对话模型的有效方案。

**Abstract:** Large Language Models (LLMs) have achieved remarkable performance on single-turn tasks, yet their effectiveness deteriorates in multi-turn conversations. We define this phenomenon as cumulative contextual decay - a progressive degradation of contextual integrity caused by attention pollution, dilution, and drift. To address this challenge, we propose Rhea (Role-aware Heuristic Episodic Attention), a novel framework that decouples conversation history into two functionally independent memory modules: (1) an Instructional Memory (IM) that persistently stores high-fidelity global constraints via a structural priority mechanism, and (2) an Episodic Memory (EM) that dynamically manages user-model interactions via asymmetric noise control and heuristic context retrieval. During inference, Rhea constructs a high signal-to-noise context by applying its priority attention: selectively integrating relevant episodic information while always prioritizing global instructions. To validate this approach, experiments on multiple multi-turn conversation benchmarks - including MT-Eval and Long-MT-Bench+ - show that Rhea mitigates performance decay and improves overall accuracy by 1.04 points on a 10-point scale (a 16% relative gain over strong baselines). Moreover, Rhea maintains near-perfect instruction fidelity (IAR > 8.1) across long-horizon interactions. These results demonstrate that Rhea provides a principled and effective framework for building more precise, instruction-consistent conversational LLMs.

</details>


### [31] [An Analysis of Large Language Models for Simulating User Responses in Surveys](https://arxiv.org/abs/2512.06874)
*Ziyun Yu,Yiru Zhou,Chen Zhao,Hongyi Wen*

Main category: cs.CL

> The study examines the use of LLMs for simulating diverse user opinions and introduces CLAIMSIM, a method aimed at diversifying LLM responses. Both direct and chain-of-thought prompting methods show limitations in accurately representing diverse demographic viewpoints.

<details>
  <summary>Details</summary>

**Motivation:** The concern that LLMs may exhibit biases towards dominant viewpoints and fail to represent diverse demographic and cultural backgrounds motivates this study.

**Method:** The researchers use direct prompting and chain-of-thought prompting to simulate human responses to cross-domain survey questions. They propose CLAIMSIM to diversify LLM responses by utilizing contextual input from parametric knowledge.

**Result:** The results indicate that while CLAIMSIM increases diversity in the responses generated by LLMs, both methods struggle to accurately simulate user profiles. LLMs often generate single-perspective claims and have difficulty reasoning about nuanced demographic differences.

**Conclusion:** LLMs have inherent limitations in simulating diverse user opinions due to their tendency to hold fixed viewpoints and their inability to reason effectively over nuanced demographic differences.

**Abstract:** Using Large Language Models (LLMs) to simulate user opinions has received growing attention. Yet LLMs, especially trained with reinforcement learning from human feedback (RLHF), are known to exhibit biases toward dominant viewpoints, raising concerns about their ability to represent users from diverse demographic and cultural backgrounds. In this work, we examine the extent to which LLMs can simulate human responses to cross-domain survey questions through direct prompting and chain-of-thought prompting. We further propose a claim diversification method CLAIMSIM, which elicits viewpoints from LLM parametric knowledge as contextual input. Experiments on the survey question answering task indicate that, while CLAIMSIM produces more diverse responses, both approaches struggle to accurately simulate users. Further analysis reveals two key limitations: (1) LLMs tend to maintain fixed viewpoints across varying demographic features, and generate single-perspective claims; and (2) when presented with conflicting claims, LLMs struggle to reason over nuanced differences among demographic features, limiting their ability to adapt responses to specific user profiles.

</details>


### [32] [Automated PRO-CTCAE Symptom Selection based on Prior Adverse Event Profiles](https://arxiv.org/abs/2512.06919)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla*

Main category: cs.CL

> An automated method for selecting a minimal yet comprehensive subset of PRO-CTCAE items is introduced, leveraging historical safety data and MedDRA semantics to balance adverse event coverage and patient burden.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of selecting a proper subset of PRO-CTCAE items that neither burdens patients nor misses important safety signals in oncology trials.

**Method:** We present an automated method to select a minimal yet comprehensive PRO-CTCAE subset based on historical safety data. Symptom terms are mapped to MedDRA Preferred Terms and encoded into Safeterm. A utility function combining relevance and incidence is used, and spectral analysis identifies an orthogonal set of medical concepts balancing relevance and diversity.

**Result:** The automated approach is evaluated using simulations and oncology case studies. It streamlines PRO-CTCAE design by balancing signal coverage against patient burden.

**Conclusion:** The automated method offers an objective and reproducible way to select a PRO-CTCAE subset that balances comprehensive coverage of adverse events with reduced patient burden, enhancing clinical trial design efficiency and quality.

**Abstract:** The PRO-CTCAE is an NCI-developed patient-reported outcome system for capturing symptomatic adverse events in oncology trials. It comprises a large library drawn from the CTCAE vocabulary, and item selection for a given trial is typically guided by expected toxicity profiles from prior data. Selecting too many PRO-CTCAE items can burden patients and reduce compliance, while too few may miss important safety signals. We present an automated method to select a minimal yet comprehensive PRO-CTCAE subset based on historical safety data. Each candidate PRO-CTCAE symptom term is first mapped to its corresponding MedDRA Preferred Terms (PTs), which are then encoded into Safeterm, a high-dimensional semantic space capturing clinical and contextual diversity in MedDRA terminology. We score each candidate PRO item for relevance to the historical list of adverse event PTs and combine relevance and incidence into a utility function. Spectral analysis is then applied to the combined utility and diversity matrix to identify an orthogonal set of medical concepts that balances relevance and diversity. Symptoms are rank-ordered by importance, and a cut-off is suggested based on the explained information. The tool is implemented as part of the Safeterm trial-safety app. We evaluate its performance using simulations and oncology case studies in which PRO-CTCAE was employed. This automated approach can streamline PRO-CTCAE design by leveraging MedDRA semantics and historical data, providing an objective and reproducible method to balance signal coverage against patient burden.

</details>


### [33] [Large Language Models and Forensic Linguistics: Navigating Opportunities and Threats in the Age of Generative AI](https://arxiv.org/abs/2512.06922)
*George Mikros*

Main category: cs.CL

> 大型语言模型（LLMs）既是法医语言学的工具，也挑战了传统方法。尽管存在差异但难以克服，现有的文本检测技术正面临技术与伦理挑战，需要方法革新来适应。

<details>
  <summary>Details</summary>

**Motivation:** 文章旨在分析大型语言模型对法医语言学现有方法的影响，并讨论如何调整方法以适应新出现的挑战，同时保证法律上的可采纳性。

**Method:** 本文探讨了大型语言模型（LLMs）在法医语言学中的双重挑战，包括作为分析工具的作用和对个人语言特征假设的破坏。文章还讨论了LLMs在模仿风格、模糊作者身份和生成合成文本方面的表现。

**Result:** 研究发现，尽管LLMs可以近似模仿表面的语言特征，但它们与人类书写者之间仍存在可检测的差异。现有的AI文本检测技术存在较高的误报率和其他局限性。

**Conclusion:** 文章总结认为，为了保持法医语言学的科学可信度和法律采纳性，需要在方法上进行调整，包括采用混合的人机工作流、有助于解释的检测范式以及跨多个人群的误差和偏见度量的验证体系。

**Abstract:** Large language models (LLMs) present a dual challenge for forensic linguistics. They serve as powerful analytical tools enabling scalable corpus analysis and embedding-based authorship attribution, while simultaneously destabilising foundational assumptions about idiolect through style mimicry, authorship obfuscation, and the proliferation of synthetic texts. Recent stylometric research indicates that LLMs can approximate surface stylistic features yet exhibit detectable differences from human writers, a tension with significant forensic implications. However, current AI-text detection techniques, whether classifier-based, stylometric, or watermarking approaches, face substantial limitations: high false positive rates for non-native English writers and vulnerability to adversarial strategies such as homoglyph substitution. These uncertainties raise concerns under legal admissibility standards, particularly the Daubert and Kumho Tire frameworks. The article concludes that forensic linguistics requires methodological reconfiguration to remain scientifically credible and legally admissible. Proposed adaptations include hybrid human-AI workflows, explainable detection paradigms beyond binary classification, and validation regimes measuring error and bias across diverse populations. The discipline's core insight, i.e., that language reveals information about its producer, remains valid but must accommodate increasingly complex chains of human and machine authorship.

</details>


### [34] [XAM: Interactive Explainability for Authorship Attribution Models](https://arxiv.org/abs/2512.06924)
*Milad Alshomary,Anisha Bhatnagar,Peter Zeng,Smaranda Muresan,Owen Rambow,Kathleen McKeown*

Main category: cs.CL

> IXAM是一个用于作者归属模型的交互式解释框架，它允许用户探索模型的嵌入空间并构造预测解释。

<details>
  <summary>Details</summary>

**Motivation:** 提供一个工具以帮助用户理解嵌入式作者归属模型如何作出预测。

**Method:** 通过交互方式探索模型的嵌入空间，并构建基于不同粒度级别的写作风格特征的预测解释。

**Result:** 用户评估表明，该框架相较于预先定义的风格解释更有价值。

**Conclusion:** IXAM展示了交互式解释框架在提升作者归属模型可解释性方面的潜力。

**Abstract:** We present IXAM, an Interactive eXplainability framework for Authorship Attribution Models. Given an authorship attribution (AA) task and an embedding-based AA model, our tool enables users to interactively explore the model's embedding space and construct an explanation of the model's prediction as a set of writing style features at different levels of granularity. Through a user evaluation, we demonstrate the value of our framework compared to predefined stylistic explanations.

</details>


### [35] [Progress Ratio Embeddings: An Impatience Signal for Robust Length Control in Neural Text Generation](https://arxiv.org/abs/2512.06938)
*Ivanhoé Botcazou,Tassadit Amghar,Sylvain Lamprier,Frédéric Saubion*

Main category: cs.CL

> 本研究提出Progress Ratio Embeddings (PRE)解决了长度控制方法在现代神经语言模型中的局限性，提供了稳定的文本生成长度控制，且在多个基准测试中表现良好。

<details>
  <summary>Details</summary>

**Motivation:** 现代神经语言模型在文本生成中取得了高精度，但对于生成长度的精确控制仍不成熟。现有方法在要求超出训练分布的控制时存在局限性，本研究旨在解决这一问题。

**Method:** 使用Progress Ratio Embeddings (PRE)来提供稳健的生成长度控制。PRE基于三角函数的信号，可以无缝集成到标准的Transformer架构中，从而在不降低文本准确性的情况下提供稳定的长度控制。

**Result:** 实验结果显示，PRE在两个广泛使用的新闻摘要基准上表现良好，验证了其在未见过的目标长度上的泛化能力。

**Conclusion:** 研究证明，基于三角函数信号的PRE为现代语言模型提供了稳健的长度控制方法，这对于提高文本生成的可控性和准确性是非常有价值的。

**Abstract:** Modern neural language models achieve high accuracy in text generation, yet precise control over generation length remains underdeveloped. In this paper, we first investigate a recent length control method based on Reverse Positional Embeddings (RPE) and show its limits when control is requested beyond the training distribution. In particular, using a discrete countdown signal tied to the absolute remaining token count leads to instability. To provide robust length control, we introduce Progress Ratio Embeddings (PRE), as continuous embeddings tied to a trigonometric impatience signal. PRE integrates seamlessly into standard Transformer architectures, providing stable length fidelity without degrading text accuracy under standard evaluation metrics. We further show that PRE generalizes well to unseen target lengths. Experiments on two widely used news-summarization benchmarks validate these findings.

</details>


### [36] [Prompting-in-a-Series: Psychology-Informed Contents and Embeddings for Personality Recognition With Decoder-Only Models](https://arxiv.org/abs/2512.06991)
*Jing Jie Tan,Ban-Hoe Kwan,Danny Wee-Kiat Ng,Yan-Chai Hum,Anissa Mokraoui,Shih-Yu Lo*

Main category: cs.CL

> 本文介绍了一种名为PICEPR（基于心理学内容嵌入的人格识别）的新算法，该算法通过模块化的解码器式大规模语言模型实现内容摘要或生成，用以增强人格特征的识别和生成人格相关的内容。实验表明PICEPR在人格识别方面比现有方法提高了5-15%的性能。

<details>
  <summary>Details</summary>

**Motivation:** 随着大规模语言模型在自然语言处理任务中表现出色，研究者希望通过一个创新算法来进一步提升在人格识别任务上的效果。

**Method:** 研究采用“系列提示”算法PICEPR，该算法有两套流程：内容生成和嵌入。通过一个模块化的解码器式大规模语言模型，能够总结或生成有助于人格特征识别的内容。

**Result:** 实验展示了PICEPR算法的有效性，并通过对比闭源模型如OpenAI的gpt4o和Google的gemini，以及开源模型如Mistral AI的mistral，证明了该算法在生成内容质量上的优越性。

**Conclusion:** PICEPR算法在人格识别任务上实现了5-15%的性能提升，达到了新的最高水平。

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks. This research introduces a novel "Prompting-in-a-Series" algorithm, termed PICEPR (Psychology-Informed Contents Embeddings for Personality Recognition), featuring two pipelines: (a) Contents and (b) Embeddings. The approach demonstrates how a modularised decoder-only LLM can summarize or generate content, which can aid in classifying or enhancing personality recognition functions as a personality feature extractor and a generator for personality-rich content. We conducted various experiments to provide evidence to justify the rationale behind the PICEPR algorithm. Meanwhile, we also explored closed-source models such as \textit{gpt4o} from OpenAI and \textit{gemini} from Google, along with open-source models like \textit{mistral} from Mistral AI, to compare the quality of the generated content. The PICEPR algorithm has achieved a new state-of-the-art performance for personality recognition by 5-15\% improvement. The work repository and models' weight can be found at https://research.jingjietan.com/?q=PICEPR.

</details>


### [37] [FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations](https://arxiv.org/abs/2512.07015)
*Mayank Ravishankara*

Main category: cs.CL

> FVA-RAG tackles the problem of sycophantic hallucinations in RAG models through an adversarial retrieval policy that looks for contradictions.

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of Retrieval Sycophancy in Retrieval-Augmented Generation (RAG) systems, where responses may align with false premises or common misconceptions.

**Method:** Falsification-Verification Alignment RAG (FVA-RAG), which shifts the retrieval paradigm from seeking support to seeking disproof by generating 'Kill Queries' to find contradictory evidence.

**Result:** Preliminary experiments show that FVA-RAG improves robustness against sycophantic hallucinations compared to standard RAG systems.

**Conclusion:** FVA-RAG effectively enhances the factual accuracy of generated responses by actively seeking out contradictory evidence.

**Abstract:** Retrieval-Augmented Generation (RAG) systems have significantly reduced hallucinations in Large Language Models (LLMs) by grounding responses in external context. However, standard RAG architectures suffer from a critical vulnerability: Retrieval Sycophancy. When presented with a query based on a false premise or a common misconception, vector-based retrievers tend to fetch documents that align with the user's bias rather than objective truth, leading the model to "hallucinate with citations."
  In this work, we introduce Falsification-Verification Alignment RAG (FVA-RAG), a framework that shifts the retrieval paradigm from Inductive Verification (seeking support) to Deductive Falsification (seeking disproof). Unlike existing "Self-Correction" methods that rely on internal consistency, FVA-RAG deploys a distinct Adversarial Retrieval Policy that actively generates "Kill Queries"-targeted search terms designed to surface contradictory evidence. We introduce a dual-verification mechanism that explicitly weighs the draft answer against this "Anti-Context." Preliminary experiments on a dataset of common misconceptions demonstrate that FVA-RAG significantly improves robustness against sycophantic hallucinations compared to standard RAG baselines, effectively acting as an inference-time "Red Team" for factual generation.

</details>


### [38] [Replicating TEMPEST at Scale: Multi-Turn Adversarial Attacks Against Trillion-Parameter Frontier Models](https://arxiv.org/abs/2512.07059)
*Richard Young*

Main category: cs.CL

> 研究使用TEMPEST框架评估了10个前沿语言模型在多回合攻击下的安全性能，发现模型的安全对齐效果存在显著差异，模型规模并不决定对抗鲁棒性，而深度推理模式可以提升安全性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管在安全对齐方面进行了大量投入，但大型语言模型在多回合复杂对抗性攻击中的脆弱性仍不清楚。研究旨在评估模型规模和推理模式对对抗鲁棒性的影响。

**Method:** 研究采用了TEMPEST框架来评估来自8个供应商的10个前沿模型，在1000种有害行为下进行评估，生成了超过97000次API查询，通过独立的安全分类器进行自动化评估。

**Result:** 六种模型的攻击成功率超过96%，而四种模型表现出明显的抵抗力，攻击成功率在42%至78%之间；启用深度推理可以将攻击成功率从97%降低到42%。

**Conclusion:** 这些发现表明各供应商之间的安全对齐质量差异显著，模型规模并不影响其对抗鲁棒性，而深度推理提供了一种可部署的安全改进措施。

**Abstract:** Despite substantial investment in safety alignment, the vulnerability of large language models to sophisticated multi-turn adversarial attacks remains poorly characterized, and whether model scale or inference mode affects robustness is unknown. This study employed the TEMPEST multi-turn attack framework to evaluate ten frontier models from eight vendors across 1,000 harmful behaviors, generating over 97,000 API queries across adversarial conversations with automated evaluation by independent safety classifiers. Results demonstrated a spectrum of vulnerability: six models achieved 96% to 100% attack success rate (ASR), while four showed meaningful resistance, with ASR ranging from 42% to 78%; enabling extended reasoning on identical architecture reduced ASR from 97% to 42%. These findings indicate that safety alignment quality varies substantially across vendors, that model scale does not predict adversarial robustness, and that thinking mode provides a deployable safety enhancement. Collectively, this work establishes that current alignment techniques remain fundamentally vulnerable to adaptive multi-turn attacks regardless of model scale, while identifying deliberative inference as a promising defense direction.

</details>


### [39] [SETUP: Sentence-level English-To-Uniform Meaning Representation Parser](https://arxiv.org/abs/2512.07068)
*Emma Markle,Javier Gutierrez Bach,Shira Wein*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Uniform Meaning Representation (UMR) is a novel graph-based semantic representation which captures the core meaning of a text, with flexibility incorporated into the annotation schema such that the breadth of the world's languages can be annotated (including low-resource languages). While UMR shows promise in enabling language documentation, improving low-resource language technologies, and adding interpretability, the downstream applications of UMR can only be fully explored when text-to-UMR parsers enable the automatic large-scale production of accurate UMR graphs at test time. Prior work on text-to-UMR parsing is limited to date. In this paper, we introduce two methods for English text-to-UMR parsing, one of which fine-tunes existing parsers for Abstract Meaning Representation and the other, which leverages a converter from Universal Dependencies, using prior work as a baseline. Our best-performing model, which we call SETUP, achieves an AnCast score of 84 and a SMATCH++ score of 91, indicating substantial gains towards automatic UMR parsing.

</details>


### [40] [Do Large Language Models Truly Understand Cross-cultural Differences?](https://arxiv.org/abs/2512.07075)
*Shiwei Guo,Sihang Jiang,Qianxi He,Yanghua Xiao,Jiaqing Liang,Bi Yude,Minggui He,Shimin Tao,Li Zhang*

Main category: cs.CL

> 本文提出了SAGE，一种评估大型语言模型跨文化理解能力的新基准。通过基于文化理论框架构建的跨文化能力评估，研究揭示了模型在跨文化理解中的系统性局限性。

<details>
  <summary>Details</summary>

**Motivation:** 当前的评估基准在跨文化理解能力评估方面存在三个关键的局限性：缺乏情境场景，跨文化概念映射不足，以及深层文化推理能力有限。

**Method:** 我们提出了SAGE，这是一个通过跨文化核心概念对齐和生成任务设计构建的基于场景的基准，用于评估大型语言模型的跨文化理解和推理能力。

**Result:** SAGE数据集支持持续扩展，实验验证了它在其他语言中的迁移能力。该数据集揭示了模型在维度和场景上的弱点，暴露了在跨文化推理方面的系统性限制。

**Conclusion:** 尽管取得了进展，但大型语言模型在达到真正的跨文化理解方面仍然有很长的路要走。未来版本中，我们将在线公开数据和代码。

**Abstract:** In recent years, large language models (LLMs) have demonstrated strong performance on multilingual tasks. Given its wide range of applications, cross-cultural understanding capability is a crucial competency. However, existing benchmarks for evaluating whether LLMs genuinely possess this capability suffer from three key limitations: a lack of contextual scenarios, insufficient cross-cultural concept mapping, and limited deep cultural reasoning capabilities. To address these gaps, we propose SAGE, a scenario-based benchmark built via cross-cultural core concept alignment and generative task design, to evaluate LLMs' cross-cultural understanding and reasoning. Grounded in cultural theory, we categorize cross-cultural capabilities into nine dimensions. Using this framework, we curated 210 core concepts and constructed 4530 test items across 15 specific real-world scenarios, organized under four broader categories of cross-cultural situations, following established item design principles. The SAGE dataset supports continuous expansion, and experiments confirm its transferability to other languages. It reveals model weaknesses across both dimensions and scenarios, exposing systematic limitations in cross-cultural reasoning. While progress has been made, LLMs are still some distance away from reaching a truly nuanced cross-cultural understanding. In compliance with the anonymity policy, we include data and code in the supplement materials. In future versions, we will make them publicly available online.

</details>


### [41] [Leveraging KV Similarity for Online Structured Pruning in LLMs](https://arxiv.org/abs/2512.07090)
*Jungmin Lee,Gwangeun Byeon,Yulhwa Kim,Seokin Hong*

Main category: cs.CL

> 本文引入了Token Filtering技术，一种轻量级的在线结构剪枝技术，它在推理过程中直接根据键值相似度测量剪枝和跳过冗余的注意力计算。实验表明，该方法在保持模型准确性的同时，大幅提高了推理性能。

<details>
  <summary>Details</summary>

**Motivation:** 剪枝技术是加速大型语言模型推理的一种有前景的方法，然而现有的方法往往因为依赖离线校准数据而存在不稳定性，这些数据可能无法跨输入泛化。

**Method:** 我们在本文中提出了Token Filtering，这是一种轻量级的在线结构剪枝技术，它在推理过程中直接做出剪枝决策，无需任何校准数据。我们的关键思想是通过测量键值相似度来衡量token冗余，并跳过冗余的注意力计算，从而在减少推理成本的同时保留关键信息。为了进一步提高稳定性，我们设计了一个方差感知融合策略，该策略自适应地加权不同头的关键值相似度，确保重要token即使在高剪枝比例下也能被保留。

**Result:** 在LLaMA-2（7B/13B）、LLaMA-3（8B）和Mistral（7B）上的大量实验表明，Token Filtering 方法在常识推理基准测试中保持了准确性，并且在如MMLU这样的复杂任务上甚至在50%的剪枝率下维持了强大的性能，表现优于先前的结构剪枝方法。

**Conclusion:** Token Filtering方法引入了无额外内存开销的更可靠的token重要性标准，提高了结构剪枝的稳定性和性能。

**Abstract:** Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.

</details>


### [42] [DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning](https://arxiv.org/abs/2512.07132)
*Nithin Sivakumaran,Justin Chih-Yao Chen,David Wan,Yue Zhang,Jaehong Yoon,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

> DART是一个多智能体框架，通过辩论解决视觉智能体间的分歧，引入新工具并促进讨论，从而提高问题解决效率。

<details>
  <summary>Details</summary>

**Motivation:** 专用于视觉的工具可以增强大型语言模型或视觉语言模型的专业知识，但知道何时调用这些工具可能具有挑战性。该研究旨在引入DART框架来解决这一问题。

**Method:** DART 是一个多智能体框架，通过多个辩论视觉智能体之间的分歧识别实用的视觉工具（如目标检测、OCR、空间推理等），以解决智能体间的分歧。这些工具通过引入新信息和提供与工具一致的同意分数来促进智能体之间的有益讨论。

**Result:** DART 在四个不同的基准测试中表现优于多智能体辩论方法和其他单智能体多工具呼叫框架，在A-OKVQA和MMMU测试中分别领先第二强基线3.4%和2.4%。

**Conclusion:** DART 能有效地在新的应用领域中适应新的工具，这对于提高模型在特定领域的表现具有重要价值。此外，DART的研究也首次测量了文本跨轮重叠情况，突显了该方法相较于现有方法所展现出的丰富讨论。

**Abstract:** Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.

</details>


### [43] [GUMBridge: a Corpus for Varieties of Bridging Anaphora](https://arxiv.org/abs/2512.07134)
*Lauren Levine,Amir Zeldes*

Main category: cs.CL

> The paper presents GUMBridge, a new resource for bridging anaphora with extensive genre coverage and detailed annotation, and shows that these tasks remain challenging for contemporary LLMs.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to address the limitations of existing resources for bridging anaphora by offering a more comprehensive dataset that covers various genres and provides detailed annotations.

**Method:** The paper introduces GUMBridge, a new resource for bridging anaphora that includes 16 diverse genres of English, providing both broad coverage and granular annotations for subtype categorization of bridging varieties.

**Result:** The evaluation of annotation quality is presented, as well as the baseline performance of open and closed source LLMs on three tasks related to the data, indicating that bridging resolution and subtype classification remain challenging tasks even for advanced LLMs.

**Conclusion:** The conclusion highlights that despite the advent of large language models (LLMs), the tasks of bridging resolution and subtype classification are still difficult, underscoring the importance of resources like GUMBridge for improving NLP systems.

**Abstract:** Bridging is an anaphoric phenomenon where the referent of an entity in a discourse is dependent on a previous, non-identical entity for interpretation, such as in "There is 'a house'. 'The door' is red," where the door is specifically understood to be the door of the aforementioned house. While there are several existing resources in English for bridging anaphora, most are small, provide limited coverage of the phenomenon, and/or provide limited genre coverage. In this paper, we introduce GUMBridge, a new resource for bridging, which includes 16 diverse genres of English, providing both broad coverage for the phenomenon and granular annotations for the subtype categorization of bridging varieties. We also present an evaluation of annotation quality and report on baseline performance using open and closed source contemporary LLMs on three tasks underlying our data, showing that bridging resolution and subtype classification remain difficult NLP tasks in the age of LLMs.

</details>


### [44] [MASim: Multilingual Agent-Based Simulation for Social Science](https://arxiv.org/abs/2512.07195)
*Xuan Zhang,Wenxuan Zhang,Anxu Wang,See-Kiong Ng,Yang Deng*

Main category: cs.CL

> 本文介绍MASim，这是一个多语言智能体模拟框架，用于研究语言智能体的社会行为，并展示了其在模拟社会文化现象中的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 论文提出的动机是现有模拟大多为单语言模拟，无法模拟跨语言交互，而跨语言交互是真实社会中的一个关键属性。

**Method:** 本文引入了MASim，这是一个支持多轮交互的多语言智能体模拟框架，模拟具有不同社会语言学特征的生成智能体之间的互动。

**Result:** 实验结果表明，MASim能够重现社会文化现象，强调了多语言模拟在可扩展、可控的计算社会科学中的重要性。

**Conclusion:** 论文结论指出，MASim框架在重现社会文化现象方面具有有效性，并强调了多语言模拟在可扩展、可控的计算社会科学中的重要性。

**Abstract:** Multi-agent role-playing has recently shown promise for studying social behavior with language agents, but existing simulations are mostly monolingual and fail to model cross-lingual interaction, an essential property of real societies. We introduce MASim, the first multilingual agent-based simulation framework that supports multi-turn interaction among generative agents with diverse sociolinguistic profiles. MASim offers two key analyses: (i) global public opinion modeling, by simulating how attitudes toward open-domain hypotheses evolve across languages and cultures, and (ii) media influence and information diffusion, via autonomous news agents that dynamically generate content and shape user behavior. To instantiate simulations, we construct the MAPS benchmark, which combines survey questions and demographic personas drawn from global population distributions. Experiments on calibration, sensitivity, consistency, and cultural case studies show that MASim reproduces sociocultural phenomena and highlights the importance of multilingual simulation for scalable, controlled computational social science.

</details>


### [45] [NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models](https://arxiv.org/abs/2512.07218)
*Feng Liang,Weixin Zeng,Runhao Zhao,Xiang Zhao*

Main category: cs.CL

> 介绍了一种新方法 NeSTR，通过集成结构化的符号表示与反思性推理来增强基于大语言模型的时间推理能力，实验结果显示，NeSTR 在多样化的时间推理测试中展示了零样本更佳性能和优越的时间理解能力。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型（LLM）在各种自然语言处理任务中表现出色，但在复杂的时序约束下的时序推理仍然是一个重大挑战。现有的方法，虽然在符号表示方法和反思机制两个方向进行了探索，但这些方法未能充分利用 LLM 的推理能力或者缺乏结构化的时间表示，导致含正确时间背景的数据仍可能被不准确解释。

**Method:** 提出了一种名为Neuro-Symbolic Temporal Reasoning (NeSTR) 的新框架，该框架将结构化的符号表示与混合反思性推理结合，以提高大语言模型在时间推理任务上的敏感度。NeSTR 通过符号编码保留显式的时间关系，通过验证确保逻辑一致性，并通过假设性反思纠正有缺陷的推理。

**Result:** 实验通过多个不同时间推理问答基准的广泛测试，展示了 NeSTR 在没有微调的情况下，实现了零样本的超凡性能，改进了时间推理的效果。这表明，神经符号集成在提升大语言模型的时间理解中有显著优势。

**Conclusion:** 经多样化的时间推理问答基准测试，NeSTR 展示了在不需要任何微调的情况下更佳的零样本性能，表明了神经符号集成在增强大语言模型时间理解方面的优势。

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, temporal reasoning, particularly under complex temporal constraints, remains a major challenge. To this end, existing approaches have explored symbolic methods, which encode temporal structure explicitly, and reflective mechanisms, which revise reasoning errors through multi-step inference. Nonetheless, symbolic approaches often underutilize the reasoning capabilities of LLMs, while reflective methods typically lack structured temporal representations, which can result in inconsistent or hallucinated reasoning. As a result, even when the correct temporal context is available, LLMs may still misinterpret or misapply time-related information, leading to incomplete or inaccurate answers. To address these limitations, in this work, we propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that integrates structured symbolic representations with hybrid reflective reasoning to enhance the temporal sensitivity of LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection. Extensive experiments on diverse temporal question answering benchmarks demonstrate that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning, showcasing the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models.

</details>


### [46] [Ensembling LLM-Induced Decision Trees for Explainable and Robust Error Detection](https://arxiv.org/abs/2512.07246)
*Mengqi Wang,Jianwei Wang,Qing Liu,Xiwei Xu,Zhenchang Xing,Liming Zhu,Wenjie Zhang*

Main category: cs.CL

> 提出了一种使用大语言模型（LLM）诱导决策树进行错误检测（ED）的方法，并通过集成多个此类决策树以提高检测结果的解释性和鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 虽然现有的错误检测方法利用LLM来标记错误单元格，但这种方法不提供解释且缺乏稳健性。因此，该研究旨在通过使用LLM诱导解释性强且稳健的决策树来改进错误检测。

**Method:** 研究人员提出了LLM诱导框架，该框架基于数据上下文、决策树规格和输出要求向LLM查询，以诱导出包含三类节点的决策树（规则节点、GNN节点和叶子节点），并将这些树集成以形成随机森林进行共识检测。

**Result:** 通过广泛的实验，他们的方法在F1分数上平均超过最佳基线16.1%，证实了其准确性、解释性和鲁棒性。

**Conclusion:** 所提出的方法，即TreeED和ForestED，提供了改进的错误检测性能，特别是在解释性和鲁棒性方面。

**Abstract:** Error detection (ED), which aims to identify incorrect or inconsistent cell values in tabular data, is important for ensuring data quality. Recent state-of-the-art ED methods leverage the pre-trained knowledge and semantic capability embedded in large language models (LLMs) to directly label whether a cell is erroneous. However, this LLM-as-a-labeler pipeline (1) relies on the black box, implicit decision process, thus failing to provide explainability for the detection results, and (2) is highly sensitive to prompts, yielding inconsistent outputs due to inherent model stochasticity, therefore lacking robustness. To address these limitations, we propose an LLM-as-an-inducer framework that adopts LLM to induce the decision tree for ED (termed TreeED) and further ensembles multiple such trees for consensus detection (termed ForestED), thereby improving explainability and robustness. Specifically, based on prompts derived from data context, decision tree specifications and output requirements, TreeED queries the LLM to induce the decision tree skeleton, whose root-to-leaf decision paths specify the stepwise procedure for evaluating a given sample. Each tree contains three types of nodes: (1) rule nodes that perform simple validation checks (e.g., format or range), (2) Graph Neural Network (GNN) nodes that capture complex patterns (e.g., functional dependencies), and (3) leaf nodes that output the final decision types (error or clean). Furthermore, ForestED employs uncertainty-based sampling to obtain multiple row subsets, constructing a decision tree for each subset using TreeED. It then leverages an Expectation-Maximization-based algorithm that jointly estimates tree reliability and optimizes the consensus ED prediction. Extensive xperiments demonstrate that our methods are accurate, explainable and robust, achieving an average F1-score improvement of 16.1% over the best baseline.

</details>


### [47] [TeluguST-46: A Benchmark Corpus and Comprehensive Evaluation for Telugu-English Speech Translation](https://arxiv.org/abs/2512.07265)
*Bhavana Akkiraju,Srihari Bandarupalli,Swathi Sambangi,Vasavi Ravuri,R Vijaya Saraswathi,Anil Kumar Vuppala*

Main category: cs.CL

> 本研究开发了泰卢固语-英语语音翻译基准，并表明即使在资源有限的情况下，通过合适的参数调整和充足的平行数据，端到端系统也可以达到与级联系统相当的性能。

<details>
  <summary>Details</summary>

**Motivation:** 泰卢固语是世界上人口最多的语言之一，但其语音翻译研究尚未得到充分探索。本研究希望通过构建高质量的语音翻译基准填补这一空白。

**Method:** 本研究通过开发从46小时的手动验证CSTD语料库数据（训练/验证/测试集时间为30小时/8小时/8小时）的高质量泰卢固语-英语语音翻译基准来解决该领域研究不足的问题。系统地比较了级联架构和端到端架构，发现尽管IndicWhisper + IndicMT因其大量的泰卢固语特定训练数据而达到最高性能，但微调后的SeamlessM4T模型仍然表现出色，尽管使用了显著较少的泰卢固语特定训练数据。

**Result:** 研究结论指出，传统的翻译评估指标比BERTScore更能区分泰卢固语-英语翻译的质量。

**Conclusion:** 本研究提供了可重复使用的泰卢固语-英语语音翻译基准、表明端到端系统在资源有限场景下的表现潜力，以及针对形态复杂语言对的自动评估的实际指导。

**Abstract:** Despite Telugu being spoken by over 80 million people, speech translation research for this morphologically rich language remains severely underexplored. We address this gap by developing a high-quality Telugu--English speech translation benchmark from 46 hours of manually verified CSTD corpus data (30h/8h/8h train/dev/test split). Our systematic comparison of cascaded versus end-to-end architectures shows that while IndicWhisper + IndicMT achieves the highest performance due to extensive Telugu-specific training data, finetuned SeamlessM4T models demonstrate remarkable competitiveness despite using significantly less Telugu-specific training data. This finding suggests that with careful hyperparameter tuning and sufficient parallel data (potentially less than 100 hours), end-to-end systems can achieve performance comparable to cascaded approaches in low-resource settings. Our metric reliability study evaluating BLEU, METEOR, ChrF++, ROUGE-L, TER, and BERTScore against human judgments reveals that traditional metrics provide better quality discrimination than BERTScore for Telugu--English translation. The work delivers three key contributions: a reproducible Telugu--English benchmark, empirical evidence of competitive end-to-end performance potential in low-resource scenarios, and practical guidance for automatic evaluation in morphologically complex language pairs.

</details>


### [48] [Efficient ASR for Low-Resource Languages: Leveraging Cross-Lingual Unlabeled Data](https://arxiv.org/abs/2512.07277)
*Srihari Bandarupalli,Bhavana Akkiraju,Charan Devarakonda,Vamsiraghusimha Narsinga,Anil Kumar Vuppala*

Main category: cs.CL

> 本研究展示了通过利用未标注的多语种数据，可以实现低资源语言上自动语音识别的高水平性能，这对于未被充分代表的语言发展具重要意义。

<details>
  <summary>Details</summary>

**Motivation:** 由于标注数据和计算资源短缺，低资源语言的自动语音识别受到了根本性的限制。本研究旨在证明战略性利用未标注的语音数据可以有效弥补资源缺口，而不会牺牲识别准确性。

**Method:** 本研究通过使用波斯-阿拉伯语（波斯语、阿拉伯语和乌尔都语）作为主要案例研究，系统地探讨了低资源语言中的跨语言连续预训练。通过构建3000小时的多语言语料库和基于形态感知的标记化结合定向持续预训练，开发了一个3亿参数的模型。

**Result:** 研究结果表明，尽管参数和标注数据显著较少，该模型在波斯语上超越了15亿参数的Whisper Large v3，并在阿拉伯语和乌尔都语上取得了有竞争力的结果。

**Conclusion:** 这些发现挑战了自动语音识别质量主要依赖于模型大小的普遍假设，揭示了数据相关性和策略性预训练对于低资源情形更为关键。此工作为向低资源语言提供包容性的语音技术开辟了实用路径。

**Abstract:** Automatic speech recognition for low-resource languages remains fundamentally constrained by the scarcity of labeled data and computational resources required by state-of-the-art models. We present a systematic investigation into cross-lingual continuous pretraining for low-resource languages, using Perso-Arabic languages (Persian, Arabic, and Urdu) as our primary case study. Our approach demonstrates that strategic utilization of unlabeled speech data can effectively bridge the resource gap without sacrificing recognition accuracy. We construct a 3,000-hour multilingual corpus through a scalable unlabeled data collection pipeline and employ targeted continual pretraining combined with morphologically-aware tokenization to develop a 300M parameter model that achieves performance comparable to systems 5 times larger. Our model outperforms Whisper Large v3 (1.5B parameters) on Persian and achieves competitive results on Arabic and Urdu despite using significantly fewer parameters and substantially less labeled data. These findings challenge the prevailing assumption that ASR quality scales primarily with model size, revealing instead that data relevance and strategic pretraining are more critical factors for low-resource scenarios. This work provides a practical pathway toward inclusive speech technology, enabling effective ASR for underrepresented languages without dependence on massive computational infrastructure or proprietary datasets.

</details>


### [49] [Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models](https://arxiv.org/abs/2512.07288)
*Tomoki Doi,Masaru Isonuma,Hitomi Yanaka*

Main category: cs.CL

> 研究展示了通过训练可以提高大语言模型在各种分类任务和不同解释风格下的自我解释忠实度，并且这种改进在多词环境和未见过的任务中也表现出泛化性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管已有研究指出大语言模型的自我解释往往缺乏忠实度，但如何提升忠实度仍是一个未充分探讨的问题。此外，不同解释风格之间的改进是否有泛化性也不明确。

**Method:** 研究通过在三种分类任务和三种解释风格下，使用基于特征归因方法构造的一词受限解释进行持续学习，来分析训练对忠实自我解释的影响及其泛化程度。

**Result:** 实验表明，在指令调整模型上使用构造的一词受限自我解释进行训练，可以在所有分类任务和解释风格下提升自我解释的忠实度，并且这些改进在多词环境和未见过的任务中也表现出泛化性。

**Conclusion:** 实验结果提示，训练可能有助于更广泛的提升模型自我解释的忠实度，并且在实验中观察到的跨风格改进表明这种提升具有广泛的意义。

**Abstract:** Large language models have the potential to generate explanations for their own predictions in a variety of styles based on user instructions. Recent research has examined whether these self-explanations faithfully reflect the models' actual behavior and has found that they often lack faithfulness. However, the question of how to improve faithfulness remains underexplored. Moreover, because different explanation styles have superficially distinct characteristics, it is unclear whether improvements observed in one style also arise when using other styles. This study analyzes the effects of training for faithful self-explanations and the extent to which these effects generalize, using three classification tasks and three explanation styles. We construct one-word constrained explanations that are likely to be faithful using a feature attribution method, and use these pseudo-faithful self-explanations for continual learning on instruction-tuned models. Our experiments demonstrate that training can improve self-explanation faithfulness across all classification tasks and explanation styles, and that these improvements also show signs of generalization to the multi-word settings and to unseen tasks. Furthermore, we find consistent cross-style generalization among three styles, suggesting that training may contribute to a broader improvement in faithful self-explanation ability.

</details>


### [50] [Multilingual corpora for the study of new concepts in the social sciences and humanities:](https://arxiv.org/abs/2512.07367)
*Revekka Kyriakoglou,Anna Pappa*

Main category: cs.CL

> 本文介绍了一种混合方法论，用于构建多语言语料库，聚焦于人文科学和社会科学的新兴概念，例如“非技术性创新”。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在构建一种可重复和可扩展的资源，用于分析围绕新兴概念的词彙变化，并为自然语言处理应用程序生成数据集。

**Method:** 本文提出了一种混合方法论，用于构建支持人文科学和社会科学新兴概念研究的多语言语料库，通过“非技术性创新”的案例来说明。该语料库依赖于两个互补的数据源：1) 从公司网站自动提取的法语和英语文本内容，2) 根据文档标准（年份、格式、重复性）自动筛选的年度报告。处理流程包括自动语言检测、过滤非相关内容、提取相关片段以及元数据增强。

**Result:** 通过本方法，从原始语料库中创建了一个英语数据集，用于机器学习目的。每个专家词汇表中的术语出现都会提取一个包含五句话的上下文块（包括术语所在句子前后的两句话），并标注相应的主题类别。

**Conclusion:** 此方法产生了一个可用于分析新兴概念词彙变化和生成自然语言处理应用数据集的可重复和可扩展的资源。

**Abstract:** This article presents a hybrid methodology for building a multilingual corpus designed to support the study of emerging concepts in the humanities and social sciences (HSS), illustrated here through the case of ``non-technological innovation''. The corpus relies on two complementary sources: (1) textual content automatically extracted from company websites, cleaned for French and English, and (2) annual reports collected and automatically filtered according to documentary criteria (year, format, duplication). The processing pipeline includes automatic language detection, filtering of non-relevant content, extraction of relevant segments, and enrichment with structural metadata. From this initial corpus, a derived dataset in English is created for machine learning purposes. For each occurrence of a term from the expert lexicon, a contextual block of five sentences is extracted (two preceding and two following the sentence containing the term). Each occurrence is annotated with the thematic category associated with the term, enabling the construction of data suitable for supervised classification tasks. This approach results in a reproducible and extensible resource, suitable both for analyzing lexical variability around emerging concepts and for generating datasets dedicated to natural language processing applications.

</details>


### [51] [Training Language Models to Use Prolog as a Tool](https://arxiv.org/abs/2512.07407)
*Niklas Mellgren,Peter Schneider-Kamp,Lukas Galke Poech*

Main category: cs.CL

> 研究通过微调语言模型使用Prolog实现可靠计算的方法，显著提升了AI系统的安全性和可审计性。

<details>
  <summary>Details</summary>

**Motivation:** 通过微调语言模型使用Prolog作为外部工具进行可验证计算，以确保AI系统在使用工具时的安全性，解决语言模型常用不可靠的推理路径问题。

**Method:** 使用Group Relative Policy Optimization (GRPO) 对Qwen2.5-3B-Instruct进行微调，使用了清理过的GSM8K-Prolog-Prover数据集，同时变化提示结构、奖励构成和推理协议，来研究模型使用Prolog作为外部工具进行可验证计算的可能性。

**Result:** 强化学习方法优于监督微调，3B模型在零样本MMLU表现中与7B模型少样本结果相当。发现：1) 提示、奖励和推理的联合调优塑造了程序语法和逻辑；2) 使用外部Prolog验证的best-of-N配置在GSM8K上达到最高准确率；3) 具有内部修复的代理推理在MMLU-Stem和MMLU-Pro上零样本泛化表现出色。

**Conclusion:** 将模型推理扎根于正式验证系统显著提高了安全关键应用中的可靠性和可审计性。

**Abstract:** Ensuring reliable tool use is critical for safe agentic AI systems. Language models frequently produce unreliable reasoning with plausible but incorrect solutions that are difficult to verify. To address this, we investigate fine-tuning models to use Prolog as an external tool for verifiable computation. Using Group Relative Policy Optimization (GRPO), we fine-tune Qwen2.5-3B-Instruct on a cleaned GSM8K-Prolog-Prover dataset while varying (i) prompt structure, (ii) reward composition (execution, syntax, semantics, structure), and (iii) inference protocol: single-shot, best-of-N, and two agentic modes where Prolog is invoked internally or independently. Our reinforcement learning approach outperforms supervised fine-tuning, with our 3B model achieving zero-shot MMLU performance comparable to 7B few-shot results. Our findings reveal that: 1) joint tuning of prompt, reward, and inference shapes program syntax and logic; 2) best-of-N with external Prolog verification maximizes accuracy on GSM8K; 3) agentic inference with internal repair yields superior zero-shot generalization on MMLU-Stem and MMLU-Pro. These results demonstrate that grounding model reasoning in formal verification systems substantially improves reliability and auditability for safety-critical applications. The source code for reproducing our experiments is available under https://github.com/niklasmellgren/grpo-prolog-inference

</details>


### [52] [Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning](https://arxiv.org/abs/2512.07454)
*Amir Mohammad Akhlaghi,Amirhossein Shabani,Mostafa Abdolmaleki,Saeed Reza Kheradpisheh*

Main category: cs.CL

> 本文介绍了一种名为Persian-Phi的模型，它挑战了强大的多语言能力需要大规模模型的假设，通过资源高效的方法成功将其应用于波斯语，并实现了在HuggingFace上Open Persian LLM Leaderboard的竞争力结果。

<details>
  <summary>Details</summary>

**Motivation:** 当前人工智能的民主化受到高昂计算成本的阻碍，这些成本用于训练低资源语言的大规模语言模型。本论文推广了微软Phi-3 Mini模型，通过一种方法将该模型从小语种英语有效地转换为波斯语，以应对这一挑战。

**Method:** 本研究提出了Persian-Phi模型，这是一种3.8B参数的模型，旨在证明强大的多语言能力并不一定需要庞大的模型尺寸或多语言基线。研究方法包括使用一种新颖的、资源高效的课程学习管道来适应波斯语。该方法包括使用双语叙事（微型故事）进行“热身”阶段，以在大规模训练前对嵌入进行对齐，然后进行持续的预训练和指令调整，并使用了参数高效微调（PEFT）。

**Result:** 尽管Persian-Phi模型参数量相对较小，但它仍能达到HuggingFace平台上Open Persian LLM Leaderboard的竞争性结果。

**Conclusion:** 该研究提供了一种经过验证、可扩展的框架，能够以最少的硬件资源扩展最先进的大语言模型的应用范围，特别是为那些代表性不足的语言提供服务。

**Abstract:** The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique "warm-up" stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at https://huggingface.co/amirakhlaghiqqq/PersianPhi.

</details>


### [53] [Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning](https://arxiv.org/abs/2512.07461)
*Tong Wu,Yang Liu,Jun Bai,Zixia Jia,Shuyi Zhang,Ziyong Lin,Yanting Wang,Song-Chun Zhu,Zilong Zheng*

Main category: cs.CL

> 本文提出了一种无需教师指导的Native Parallel Reasoner框架，通过三种创新技术（自我蒸馏的逐步训练、并行感知策略优化算法、稳固的NPR引擎），使大型语言模型能够自我进化出真正的并行推理能力，在多个基准测试中大幅提升了性能和加速了推理速度。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于开发一种无需外部教师指导，即可使大型语言模型自我进化出真正的并行推理能力的框架。

**Method:** 本文介绍了Native Parallel Reasoner (NPR)框架，该框架通过三种关键创新让大型语言模型（LLMs）能够自我进化出真正的并行推理能力。这些创新包括：1） 自我蒸馏的逐步训练范式，从“冷启动”格式发现过渡到严格的拓扑约束，无需外部监督；2） 一种新的并行感知策略优化（PAPO）算法，直接在执行图内优化分支策略，使模型通过试错学习适应性分解；3） 一个稳固的NPR引擎，重构了SGLang的内存管理和流程控制，以实现稳定的大规模并行强化学习训练。

**Result:** 在八个推理基准测试中，基于Qwen3-4B的NPR实现了高达24.5%的性能提升和最高4.6倍的推理加速。与之前依赖自回归解码的基线模型不同，NPR实现了100%真实的并行执行，树立了自我进化、高效和可扩展的代理推理的新标准。

**Conclusion:** NPR通过上述创新，实现了比现有方法更高的性能提升和推理加速，展现了100%的真并行执行能力，是自我进化高效代理推理的新标杆。

**Abstract:** We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.

</details>


### [54] [Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling Policy Optimization](https://arxiv.org/abs/2512.07478)
*Zhuoran Zhuang,Ye Chen,Jianghao Su,Chao Luo,Luhui Liu,Xia Zeng*

Main category: cs.CL

> 提出两种技术PRS和VSPO，以提高LLMs在复杂任务中TIR的能力，解决稀疏奖励和梯度退化的问题，展示在QA任务中的优越性。

<details>
  <summary>Details</summary>

**Motivation:** 解决Agentic RL在优化LLMs过程中遇到的稀疏非指导性奖励和梯度退化问题。

**Method:** 提出PRS技术来设计课程灵感的奖励方案，以及VSPO改进版本的GRPO，以解决梯度退化问题。

**Result:** 实验表明，PRS在短文中和长文中QA基准中均优于传统的二进制奖励，而VSPO展现更好的稳定性、更快的收敛速度和更高的最终性能。

**Conclusion:** PRS和VSPO共同作用下，基于LLM的TIR代理在不同领域的泛化能力更强。

**Abstract:** Large Language Models (LLMs) empowered with Tool-Integrated Reasoning (TIR) can iteratively plan, call external tools, and integrate returned information to solve complex, long-horizon reasoning tasks. Agentic Reinforcement Learning (Agentic RL) optimizes such models over full tool-interaction trajectories, but two key challenges hinder effectiveness: (1) Sparse, non-instructive rewards, such as binary 0-1 verifiable signals, provide limited guidance for intermediate steps and slow convergence; (2) Gradient degradation in Group Relative Policy Optimization (GRPO), where identical rewards within a rollout group yield zero advantage, reducing sample efficiency and destabilizing training. To address these challenges, we propose two complementary techniques: Progressive Reward Shaping (PRS) and Value-based Sampling Policy Optimization (VSPO). PRS is a curriculum-inspired reward design that introduces dense, stage-wise feedback - encouraging models to first master parseable and properly formatted tool calls, then optimize for factual correctness and answer quality. We instantiate PRS for short-form QA (with a length-aware BLEU to fairly score concise answers) and long-form QA (with LLM-as-a-Judge scoring to prevent reward hacking). VSPO is an enhanced GRPO variant that replaces low-value samples with prompts selected by a task-value metric balancing difficulty and uncertainty, and applies value-smoothing clipping to stabilize gradient updates. Experiments on multiple short-form and long-form QA benchmarks show that PRS consistently outperforms traditional binary rewards, and VSPO achieves superior stability, faster convergence, and higher final performance compared to PPO, GRPO, CISPO, and SFT-only baselines. Together, PRS and VSPO yield LLM-based TIR agents that generalize better across domains.

</details>


### [55] [SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG](https://arxiv.org/abs/2512.07515)
*Pengqian Lu,Jie Lu,Anjin Liu,Guangquan Zhang*

Main category: cs.CL

> SPAD通过对每个词的概率进行七个来源的归因，并根据词性标签聚合这些分数，来识别异常情况，实现了对Retrieval-Augmented Generation (RAG)系统中幻觉的检测，达到了最新的性能水平。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法将幻觉归因于内部知识和检索上下文之间的二元冲突，这种观点是不完整的，忽略了生成过程中其他组件的影响，例如用户查询、先前生成的标记、当前标记本身以及最终的LayerNorm调整。

**Method:** 首先，我们将每个词的概率归因于七个不同的来源：查询、RAG、过去生成的词、当前词、前馈网络（FFN）、最终的LayerNorm调整和初始嵌入。这种归因量化了每个来源对当前词生成所做出的贡献。然后，我们根据词性标签聚合这些分数，量化不同的组件如何驱动物特定的语义类别。通过识别诸如名词依赖于最终LayerNorm调整等异常情况，SPAD有效地检测幻觉。

**Result:** 实验表明，SPAD在幻觉检测上达到了最新的性能水平。

**Conclusion:** 通过引入SPAD，我们克服了现有方法只关注内部知识和检索上下文之间的二元冲突的局限性，实现了更加精确的幻觉检测。

**Abstract:** Detecting hallucinations in Retrieval-Augmented Generation (RAG) remains a challenge. Prior approaches attribute hallucinations to a binary conflict between internal knowledge (stored in FFNs) and retrieved context. However, this perspective is incomplete, failing to account for the impact of other components in the generative process, such as the user query, previously generated tokens, the current token itself, and the final LayerNorm adjustment. To address this, we introduce SPAD. First, we mathematically attribute each token's probability into seven distinct sources: Query, RAG, Past, Current Token, FFN, Final LayerNorm, and Initial Embedding. This attribution quantifies how each source contributes to the generation of the current token. Then, we aggregate these scores by POS tags to quantify how different components drive specific linguistic categories. By identifying anomalies, such as Nouns relying on Final LayerNorm, SPAD effectively detects hallucinations. Extensive experiments demonstrate that SPAD achieves state-of-the-art performance

</details>


### [56] [LIME: Making LLM Data More Efficient with Linguistic Metadata Embeddings](https://arxiv.org/abs/2512.07522)
*Sebastian Sztwiertnia,Felix Friedrich,Kristian Kersting,Patrick Schramowski,Björn Deiseroth*

Main category: cs.CL

> 本文提出了一种方法LIME，利用元数据提升decoder-only语言模型的预训练效率和性能。LIME+1变体展示了显著的推理和算术性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 虽然元数据常用于创建和整理高质量数据集，但其作为直接训练信号的潜力尚待深入探索。本研究挑战现状，旨在通过利用元数据来提升预训练效率及性能。

**Method:** 我们提出了一种名为LIME（Linguistic Metadata Embeddings）的方法，该方法通过语法、语义和上下文特性等元数据来丰富token的嵌入表示。此外，我们还开发了一个变体LIME+1，它可以根据下一个token的元数据来引导token生成。

**Result:** LIME改善了预训练效率，加速了56%的数据分布适应，仅增加了0.01%的参数，计算开销可忽略不计。它增强了语言模型能力和生成任务表现。LIME+1变体在推理性能和算术准确度上分别提升了38%和35%。

**Conclusion:** 通过LIME方法，我们证明了元数据在改善语言模型的训练效率和性能方面的显著价值。这项研究为未来的工作提供了理论和实践基础，推动领域向前发展。

**Abstract:** Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. LIME substantially improves pre-training efficiency. Specifically, it adapts up to 56% faster to the training data distribution, while introducing only 0.01% additional parameters at negligible compute overhead. Beyond efficiency, LIME improves tokenization, leading to remarkably stronger language modeling capabilities and generative task performance. These benefits persist across model scales (500M to 2B). In addition, we develop a variant with shifted metadata, LIME+1, that can guide token generation. Given prior metadata for the next token, LIME+1 improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.

</details>


### [57] [Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs](https://arxiv.org/abs/2512.07525)
*Xiaoran Liu,Yuerong Song,Zhigeng Liu,Zengfeng Huang,Qipeng Guo,Zhaoxiang Liu,Shiguo Lian,Ziwei He,Xipeng Qiu*

Main category: cs.CL

> 本文提出重新引入复数点积虚部的方法，以增强长上下文依赖关系的建模能力，并在多个评估中表现出性能改善。

<details>
  <summary>Details</summary>

**Motivation:** 标准的RoPE实现仅利用复数值点积的实际部分来进行注意力分数计算，这种简化丢弃了包含宝贵相位信息的虚部，可能丢失对于建模长上下文依赖关系至关重要的关系细节。

**Method:** 本文提出了一种方法，重新引入了被忽略的虚部，利用完整的复数值表示创建了双组件注意力分数。

**Result:** 我们的方法在长上下文语言建模基准测试中的评估显示出了一致的性能改进，随着上下文长度的增加，这种改进变得更加显著。

**Conclusion:** 该扩展方法通过保留更多的位置信息，增强了对长上下文依赖关系的建模能力。

**Abstract:** Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.

</details>


### [58] [SwissGov-RSD: A Human-annotated, Cross-lingual Benchmark for Token-level Recognition of Semantic Differences Between Related Documents](https://arxiv.org/abs/2512.07538)
*Michelle Wastl,Jannis Vamvas,Rico Sennrich*

Main category: cs.CL

> 提出了一个新的跨语言文档级语义差异识别数据集SwissGov-RSD，并评估了现有大语言模型和编码模型在这一数据集上的表现，显示了它们在跨语言任务中相比单语言任务表现较差的问题。

<details>
  <summary>Details</summary>

**Motivation:** 认识到跨文档的语义差异，尤其是在不同语言之间，对于文本生成评估和多语言内容对齐至关重要。然而，作为一个独立的任务，它尚未得到足够的关注。

**Method:** 介绍了SwissGov-RSD数据集，这是一个自然主义、文档级别的跨语言数据集，用于语义差异识别。该数据集包含224个多平行文档，涵盖了英德、英法和英意三种语言组合，并由人工标注者进行了词级别差异标注。

**Result:** 评估了多种开源和闭源的大型语言模型及编码模型在不同微调设置下的性能。结果显示，当前的自动方法与单语言、句子级别和合成基准的表现相比，表现较差，揭示了对于大型语言模型及编码模型而言，存在显著差距。

**Conclusion:** 发布了代码和数据集，揭示了当前模型在跨语言文档级语义差异识别上的显著差距。

**Abstract:** Recognizing semantic differences across documents, especially in different languages, is crucial for text generation evaluation and multilingual content alignment. However, as a standalone task it has received little attention. We address this by introducing SwissGov-RSD, the first naturalistic, document-level, cross-lingual dataset for semantic difference recognition. It encompasses a total of 224 multi-parallel documents in English-German, English-French, and English-Italian with token-level difference annotations by human annotators. We evaluate a variety of open-source and closed source large language models as well as encoder models across different fine-tuning settings on this new benchmark. Our results show that current automatic approaches perform poorly compared to their performance on monolingual, sentence-level, and synthetic benchmarks, revealing a considerable gap for both LLMs and encoder models. We make our code and datasets publicly available.

</details>


### [59] [Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation](https://arxiv.org/abs/2512.07540)
*Boxuan Lyu,Haiyue Song,Hidetaka Kamigaito,Chenchen Ding,Hideki Tanaka,Masao Utiyama,Kotaro Funakoshi,Manabu Okumura*

Main category: cs.CL

> 我们提出了一种新的误差跨度检测(ESD)方法，采用MBR解码代替传统的MAP解码，通过效用函数选择最接近人类注释的候选假设，实验中该方法表现出优于基准结果，并可以通过MBR蒸馏来降低计算成本。

<details>
  <summary>Details</summary>

**Motivation:** 状态-of-the-art生成型ESD方法假定模型估计的概率与人类注释相似度之间存在完全相关性，但是我们认为这不是真实的。我们的动机是提高ESD模型的性能和效率，通过优化解码策略来减少计算成本。

**Method:** 我们通过使用最小贝叶斯风险(MBR)解码来解决生成型误差跨度检测(ESD)模型中的问题，该问题表现为模型估计的概率与人类注释相似度之间的不完全匹配。具体而言，我们采用句子和跨度级别相似性度量作为效用函数，以选择与人类注释相似度最高的候选假设。

**Result:** 实验结果表明，相较于MAP基线，我们的MBR解码在系统级、句子级和跨度级别上都表现得更好。此外，通过MBR蒸馏来降低MBR解码的计算成本，使得标准贪婪模型能够匹配MBR解码的性能，从而解决了推理阶段的延迟瓶颈。

**Conclusion:** MBR解码相较于MAP基线，在降低计算成本的同时，提高了生成型ESD模型的性能。通过MBR蒸馏技术，能够进一步优化模型推理阶段的表现，简化模型结构。

**Abstract:** Error Span Detection (ESD) is a subtask of automatic machine translation evaluation that localizes error spans in translations and labels their severity. State-of-the-art generative ESD methods typically decode using Maximum a Posteriori (MAP), assuming that model-estimated probabilities are perfectly correlated with similarity to human annotation. However, we observed that annotations dissimilar to the human annotation could achieve a higher model likelihood than the human annotation. We address this issue by applying Minimum Bayes Risk (MBR) decoding to generative ESD models. Specifically, we employ sentence- and span-level similarity metrics as utility functions to select candidate hypotheses based on their approximate similarity to the human annotation. Extensive experimental results show that our MBR decoding outperforms the MAP baseline at the system, sentence, and span-levels. Furthermore, to mitigate the computational cost of MBR decoding, we demonstrate that applying MBR distillation enables a standard greedy model to match MBR decoding performance, effectively eliminating the inference-time latency bottleneck.

</details>


### [60] [Most over-representation of phonological features in basic vocabulary disappears when controlling for spatial and phylogenetic effects](https://arxiv.org/abs/2512.07543)
*Frederic Blum*

Main category: cs.CL

> 研究修改了对语言间空间和基因依赖性的控制以测试之前关于声音象征性的研究结果，发现大多数之前观察到的模式不稳健，仅少量模式显著。

<details>
  <summary>Details</summary>

**Motivation:** 许多关于声音象征性的研究没有充分控制采样语言之间的基因和地域依赖性，因此引发了对结果稳健性的质疑。这项研究旨在测试关于基础词汇概念的声音象征性的一个近期研究结果的稳健性。

**Method:** 研究修改了原始模型，增加了对语言间空间和系统发育依赖性的统计控制，使用了来自Lexibank的2864种语言的新样本。

**Result:** 新的结果显示，以往观察到的大多数模式在增加了基因和地域控制后并未保持稳健，许多模式完全消失，仅少数模式高度稳定。

**Conclusion:** 研究强调了在各种层面对语言的普遍性主张进行稳健性测试的必要性，并通过新的分析在更大范围内评估了声音象征性的分布情况。

**Abstract:** The statistical over-representation of phonological features in the basic vocabulary of languages is often interpreted as reflecting potentially universal sound symbolic patterns. However, most of those results have not been tested explicitly for reproducibility and might be prone to biases in the study samples or models. Many studies on the topic do not adequately control for genealogical and areal dependencies between sampled languages, casting doubts on the robustness of the results. In this study, we test the robustness of a recent study on sound symbolism of basic vocabulary concepts which analyzed245 languages.The new sample includes data on 2864 languages from Lexibank. We modify the original model by adding statistical controls for spatial and phylogenetic dependencies between languages. The new results show that most of the previously observed patterns are not robust, and in fact many patterns disappear completely when adding the genealogical and areal controls. A small number of patterns, however, emerges as highly stable even with the new sample. Through the new analysis, we are able to assess the distribution of sound symbolism on a larger scale than previously. The study further highlights the need for testing all universal claims on language for robustness on various levels.

</details>


### [61] [MoCoRP: Modeling Consistent Relations between Persona and Response for Persona-based Dialogue](https://arxiv.org/abs/2512.07544)
*Kyungro Lee,Dongha Choi,Hyunju Lee*

Main category: cs.CL

> 研究提出了MoCoRP框架，该框架增强了以人物为中心的对话系统，提升了对话的一致性和吸引力，源代码在GitHub上可用。

<details>
  <summary>Details</summary>

**Motivation:** 针对现有的以人物为中心的对话数据集中缺乏人物句子和回复之间的显式关系，这使得模型难以有效地捕捉人物信息这一问题。

**Method:** 提出MoCoRP框架，该框架利用NLI专家显式提取人物句子和回复之间的NLI关系，使模型能够有效地将上下文中的适当人物信息纳入其回复中。此框架应用于BART等预训练模型，并通过对齐调整进一步扩展到现代大型语言模型(LLMs)。

**Result:** 在ConvAI2和MPChat等公共数据集上的实验结果表明，MoCoRP在人物一致性以及生成吸引人的、上下文敏感的对话方面优于现有基线，不仅在定量指标上表现出色，在定性方面也有所提升。

**Conclusion:** 这项研究结果强调了在以人物为中心的对话中显式建模人物回复关系的有效性。

**Abstract:** As dialogue systems become increasingly important across various domains, a key challenge in persona-based dialogue is generating engaging and context-specific interactions while ensuring the model acts with a coherent personality. However, existing persona-based dialogue datasets lack explicit relations between persona sentences and responses, which makes it difficult for models to effectively capture persona information. To address these issues, we propose MoCoRP (Modeling Consistent Relations between Persona and Response), a framework that incorporates explicit relations into language models. MoCoRP leverages an NLI expert to explicitly extract the NLI relations between persona sentences and responses, enabling the model to effectively incorporate appropriate persona information from the context into its responses. We applied this framework to pre-trained models like BART and further extended it to modern large language models (LLMs) through alignment tuning. Experimental results on the public datasets ConvAI2 and MPChat demonstrate that MoCoRP outperforms existing baselines, achieving superior persona consistency and engaging, context-aware dialogue generation. Furthermore, our model not only excels in quantitative metrics but also shows significant improvements in qualitative aspects. These results highlight the effectiveness of explicitly modeling persona-response relations in persona-based dialogue. The source codes of MoCoRP are available at https://github.com/DMCB-GIST/MoCoRP.

</details>


### [62] [Performance of the SafeTerm AI-Based MedDRA Query System Against Standardised MedDRA Queries](https://arxiv.org/abs/2512.07552)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta*

Main category: cs.CL

> 研究中，SafeTerm AMQ系统通过使用多准则统计方法在中等相似度阈值下取得高召回率（94%）,并提升精确度高达89%；在繁琐的人工阈值设定中，自动阈值选择更注重召回率（0.58）而非精确度（0.29），证明了其作为MedDRA查询生成辅助工具有一定价值。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的目标是在药物上市前的安全评估中，评估自动医学查询（AMQ）在MedDRA SMQs上的表现。SMQs或OCMQs是检测信号的关键。

**Method:** 我们评估了SafeTerm自动医学查询（AMQ）在MedDRA SMQs上的表现。该系统通过多准则统计方法为给定输入查询自动检索相关的MedDRA首选术语（PTs），并按相关性得分（0-1）进行排名。系统将医学查询和MedDRA PTs嵌入多维向量空间，并应用余弦相似度和极值聚类生成排序列表。

**Result:** 在多种相似度阈值下，计算了精度、召回率和F1值。在中等相似度阈值下获得了高召回率（94%），表明良好的检索灵敏度。而更高的阈值可以过滤掉更多术语，提高精确度（高达89%）。最优阈值（0.70）在所有110个查询中获得了48%的总体召回率和45%的精确度。限制到狭窄术语PTs可以在适度增加（+0.05）相似度阈值的情况下获得更好的性能，确认了狭窄与广泛术语之间的相关性增加。自动选择阈值（0.66）更注重于召回率（0.58），而非精确度（0.29）。

**Conclusion:** SafeTerm AMQ在SMQs和清理后的OCMQs上实现了可比且令人满意的性能，因此是生成自动MedDRA查询的一个可行的补充方法，它平衡了召回率和精确度。建议在查询制定时使用合适的MedDRA PT术语，并采用自动化阈值选择方法来优化召回率。增加相似性得分可以实现更精确的狭窄术语选择。

**Abstract:** In pre-market drug safety review, grouping related adverse event terms into SMQs or OCMQs is critical for signal detection. We assess the performance of SafeTerm Automated Medical Query (AMQ) on MedDRA SMQs. The AMQ is a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score (0-1) using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity, and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against tier-1 SMQs (110 queries, v28.1). Precision, recall and F1 were computed at multiple similarity-thresholds, defined either manually or using an automated method. High recall (94%)) is achieved at moderate similarity thresholds, indicative of good retrieval sensitivity. Higher thresholds filter out more terms, resulting in improved precision (up to 89%). The optimal threshold (0.70)) yielded an overall recall of (48%) and precision of (45%) across all 110 queries. Restricting to narrow-term PTs achieved slightly better performance at an increased (+0.05) similarity threshold, confirming increased relatedness of narrow versus broad terms. The automatic threshold (0.66) selection prioritizes recall (0.58) to precision (0.29). SafeTerm AMQ achieves comparable, satisfactory performance on SMQs and sanitized OCMQs. It is therefore a viable supplementary method for automated MedDRA query generation, balancing recall and precision. We recommend using suitable MedDRA PT terminology in query formulation and applying the automated threshold method to optimise recall. Increasing similarity scores allows refined, narrow terms selection.

</details>


### [63] [A Simple Method to Enhance Pre-trained Language Models with Speech Tokens for Classification](https://arxiv.org/abs/2512.07571)
*Nicolas Calbucura,Valentin Barriere*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** This paper presents a simple method that allows to easily enhance textual pre-trained large language models with speech information, when fine-tuned for a specific classification task. A classical issue with the fusion of many embeddings from audio with text is the large length of the audio sequence compared to the text one. Our method benefits from an existing speech tokenizer trained for Audio Speech Recognition that output long sequences of tokens from a large vocabulary, making it difficult to integrate it at low cost in a large language model. By applying a simple lasso-based feature selection on multimodal Bag-of-Words representation, we retain only the most important audio tokens for the task, and adapt the language model to them with a self-supervised language modeling objective, before fine-tuning it on the downstream task. We show this helps to improve the performances compared to an unimodal model, to a bigger SpeechLM or to integrating audio via a learned representation. We show the effectiveness of our method on two recent Argumentative Fallacy Detection and Classification tasks where the use of audio was believed counterproductive, reaching state-of-the-art results. We also provide an in-depth analysis of the method, showing that even a random audio token selection helps enhancing the unimodal model. Our code is available [online](https://github.com/salocinc/EACL26SpeechTokFallacy/).

</details>


### [64] [Complementary Learning Approach for Text Classification using Large Language Models](https://arxiv.org/abs/2512.07583)
*Navid Asgari,Benjamin M. Cole*

Main category: cs.CL

> 本文提出了一种成本效益高且高效的人机结合的方法，用于管理LLMs固有的弱点，并演示其在制药联盟新闻稿评级差异上的应用。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在展示一种人和机器结合的工作方法，利用低成本且细致的方法来管理和减轻LLMs的固有弱点，以及如何通过合理的低代价技术实现这一点。

**Method:** 本研究提出了一种结构化的方法论，该方法论利用大型语言模型（LLMs）以成本效益和经济高效的方式，结合了学者和机器的优势，同时弥补了它们各自的不足。该方法论通过计算机科学中的思考链和少样本学习提示来实现，将最佳实践从定性研究的共同作者团队扩展到了定量研究中的人机团队。

**Result:** 本研究中，作者演示了如何使用该方法来调查人机评级差异，实例来源于1990-2017年宣布的1,934份制药联盟的新闻稿。

**Conclusion:** 该方法突显了学术界如何使用周密且低成本的技术来应对LLMs的内在弱点，并通过人机团队以批判性分析的方式审视机器和人类各自的工作。

**Abstract:** In this study, we propose a structured methodology that utilizes large language models (LLMs) in a cost-efficient and parsimonious manner, integrating the strengths of scholars and machines while offsetting their respective weaknesses. Our methodology, facilitated through a chain of thought and few-shot learning prompting from computer science, extends best practices for co-author teams in qualitative research to human-machine teams in quantitative research. This allows humans to utilize abductive reasoning and natural language to interrogate not just what the machine has done but also what the human has done. Our method highlights how scholars can manage inherent weaknesses OF LLMs using careful, low-cost techniques. We demonstrate how to use the methodology to interrogate human-machine rating discrepancies for a sample of 1,934 press releases announcing pharmaceutical alliances (1990-2017).

</details>


### [65] [Metric-Fair Prompting: Treating Similar Samples Similarly](https://arxiv.org/abs/2512.07608)
*Jing Wang,Jie Shen,Xing Niu,Tong Zhang,Jeremy Weiss*

Main category: cs.CL

> 本文提出了'Metric-Fair Prompting'框架，通过促使大规模语言模型在满足公平性约束的前提下进行决策，来提升医学选择题回答的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 文章旨在开发一种新的公平性感知的提示框架，用以在大规模语言模型上进行具有公平性约束的决策，特别是针对具有高风险的临床选择题问题，以提高这类问题的准确性。

**Method:** 提出了一种名为'Metric-Fair Prompting'的公平性感知提示框架，指导大规模语言模型（LLMs）在满足指标公平性约束的情况下进行决策。在选择题形式的医学问题回答应用中，每个{(问题, 选项)}对被视为带有标签 $+1$ (正确) 或 $-1$ (不正确) 的二进制实例。为了促进个体公平性---相似实例应被相似对待---框架使用NLP嵌入计算问题相似性，并解决“相似问题的联合对”而不是孤立地解决每个问题。提示设计了一个全局决策协议：抽取决定性的临床特征，将每个(	ext{问题}, 	ext{选项})映射到一个代表置信度的分数 $f(x)$，并施加类似于Lipschitz的约束，使其相似的输入得到相似的分数和一致的输出。

**Result:** 在MedQA (US)基准测试上进行的评估表明，Metric-Fair Prompting相比于标准的单项提示方法，能够提高性能，同时证明以公平性和置信度为导向的推理可以提升大规模语言模型在高风险临床选择题上的准确性。

**Conclusion:** 这项研究证明了'Metric-Fair Prompting'框架的有效性，它在处理临床选择题时能够做到既公平又准确，为未来开发能在高风险场景下应用的语言模型提供了新的方向。

**Abstract:** We introduce \emph{Metric-Fair Prompting}, a fairness-aware prompting framework that guides large language models (LLMs) to make decisions under metric-fairness constraints. In the application of multiple-choice medical question answering, each {(question, option)} pair is treated as a binary instance with label $+1$ (correct) or $-1$ (incorrect). To promote {individual fairness}~--~treating similar instances similarly~--~we compute question similarity using NLP embeddings and solve items in \emph{joint pairs of similar questions} rather than in isolation. The prompt enforces a global decision protocol: extract decisive clinical features, map each \((\text{question}, \text{option})\) to a score $f(x)$ that acts as confidence, and impose a Lipschitz-style constraint so that similar inputs receive similar scores and, hence, consistent outputs. Evaluated on the {MedQA (US)} benchmark, Metric-Fair Prompting is shown to improve performance over standard single-item prompting, demonstrating that fairness-guided, confidence-oriented reasoning can enhance LLM accuracy on high-stakes clinical multiple-choice questions.

</details>


### [66] [PCMind-2.1-Kaiyuan-2B Technical Report](https://arxiv.org/abs/2512.07612)
*Kairong Luo,Zhenbo Sun,Xinyu Shi,Shengqi Chen,Bowen Yu,Yunyi Chen,Chenyi Dang,Hengtao Tao,Hui Wang,Fangming Liu,Kaifeng Lyu,Wenguang Chen*

Main category: cs.CL

> 研究介绍了一种新的20亿参数的全开源语言模型PCMind-2.1-Kaiyuan-2B，旨在通过创新技术和策略提升资源受限环境下的训练效率和性能表现。

<details>
  <summary>Details</summary>

**Motivation:** 解决大型语言模型（LLMs）快速发展过程中开源社区与工业界间出现的知识差距，特别是针对后者依赖闭源的高质量数据和训练方法的问题。

**Method:** 介绍了一种名为PCMind-2.1-Kaiyuan-2B的全新开源模型，该模型有20亿参数，旨在提高资源受限环境下的训练效率和效果。具体包括三种关键创新：1）分位数数据基准测试方法来系统比较开源数据集，给出数据混合同策略的见解；2）多阶段中的战略选择性重复方案来有效使用稀疏的高质量数据；3）多领域课程训练策略，按质量排序样本。此外，通过高度优化的数据预处理管道和FP16稳定性架构修改进一步提升模型性能。

**Result:** Kaiyuan-2B模型的性能表现可以与最先进全开源模型匹敌，提供了在资源受限环境下进行预训练的实际且可扩展的解决方案。

**Conclusion:** 该研究在解决大型语言模型快速发展带来的知识差距方面提出了具体方法，特别是在资源受限的预训练环境中，展示了具有竞争力的解决方案。

**Abstract:** The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.

</details>


### [67] [Bridging Code Graphs and Large Language Models for Better Code Understanding](https://arxiv.org/abs/2512.07666)
*Zeqi Chen,Zhaoyang Chu,Yi Gui,Feng Guo,Yao Wan,Chuan Shi*

Main category: cs.CL

> CGBridge 提出了一种新的插件方法，通过外部可训练的桥接模块增强大型语言模型（LLM）的代码图信息。该方法在代码智能任务中表现优于原模型和图增强提示法，同时提高推理速度。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大语言模型虽然在代码智能任务上有出色表现，但由于依赖线性化的标记序列，限制了它们理解程序结构语义的能力。已有的图增强提示和结构感知预训练方法要么受限于提示长度，要么需要特定的任务架构改变。

**Method:** CGBridge 通过一个可训练的桥接模块增强 LLM，该模块首先通过自监督学习在一个大型的代码图数据集上预训练代码图编码器，然后训练一个外部模块来通过跨模态注意力机制对齐代码、图和文本的信息结构。

**Result:** 实验显示，CGBridge 在代码总结的 LLM-as-a-Judge 任务上相较于原始模型和图增强提示法分别提高了 16.19% 和 9.12%，在代码翻译的执行准确性上提高了 9.84% 和 38.87%，并且推理速度提高了四倍以上。

**Conclusion:** CGBridge 方法成功地解决了大语言模型在理解代码的结构语义上的限制，并在保持高效性的同时提高了解决代码智能任务的性能。

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable performance in code intelligence tasks such as code generation, summarization, and translation. However, their reliance on linearized token sequences limits their ability to understand the structural semantics of programs. While prior studies have explored graphaugmented prompting and structure-aware pretraining, they either suffer from prompt length constraints or require task-specific architectural changes that are incompatible with large-scale instructionfollowing LLMs. To address these limitations, this paper proposes CGBridge, a novel plug-and-play method that enhances LLMs with Code Graph information through an external, trainable Bridge module. CGBridge first pre-trains a code graph encoder via selfsupervised learning on a large-scale dataset of 270K code graphs to learn structural code semantics. It then trains an external module to bridge the modality gap among code, graph, and text by aligning their semantics through cross-modal attention mechanisms. Finally, the bridge module generates structure-informed prompts, which are injected into a frozen LLM, and is fine-tuned for downstream code intelligence tasks. Experiments show that CGBridge achieves notable improvements over both the original model and the graphaugmented prompting method. Specifically, it yields a 16.19% and 9.12% relative gain in LLM-as-a-Judge on code summarization, and a 9.84% and 38.87% relative gain in Execution Accuracy on code translation. Moreover, CGBridge achieves over 4x faster inference than LoRA-tuned models, demonstrating both effectiveness and efficiency in structure-aware code understanding.

</details>


### [68] [When Large Language Models Do Not Work: Online Incivility Prediction through Graph Neural Networks](https://arxiv.org/abs/2512.07684)
*Zihan Chen,Lanyu Yu*

Main category: cs.CL

> 开发了一种图神经网络框架，用于检测维基百科中的毒性、攻击性和人身攻击行为，利用动态注意力机制平衡节点和拓扑特征，实验证明该框架优于12个先进的大语言模型，具有更低的推理成本。

<details>
  <summary>Details</summary>

**Motivation:** 现有的在线不文明行为检测方法在准确性和效率方面都存在局限，该研究旨在提出一种更有效的检测框架，利用图神经网络和动态调整注意力机制来改进检测效果。

**Method:** 提出了一种基于图神经网络（GNN）框架来检测英语维基百科社区中的三种不文明行为（即毒性、攻击性和人身攻击）。该模型将每个用户评论表示为节点，评论之间的文本相似性定义为边，从而使得网络能够联合学习语言内容和评论之间的关系结构。此外，我们引入了一种动态调整的注意力机制，可以在信息聚合过程中自适应地平衡节点和拓扑特征。

**Result:** 实证评估表明，所提出的框架在多个指标上优于12个最先进的大语言模型（LLMs），同时显著降低了推理成本。发现结果强调了在检测在线不文明行为中结构环境的重要性，并且弥补了基于文本的LLM预测行为的不足。

**Conclusion:** 实验结果表明，该框架在检测在线不文明行为方面优于现有的大语言模型，并强调了结构环境的重要性。研究中使用的数据集和比较结果将公开提供，以支持进一步的研究和重现性。

**Abstract:** Online incivility has emerged as a widespread and persistent problem in digital communities, imposing substantial social and psychological burdens on users. Although many platforms attempt to curb incivility through moderation and automated detection, the performance of existing approaches often remains limited in both accuracy and efficiency. To address this challenge, we propose a Graph Neural Network (GNN) framework for detecting three types of uncivil behavior (i.e., toxicity, aggression, and personal attacks) within the English Wikipedia community. Our model represents each user comment as a node, with textual similarity between comments defining the edges, allowing the network to jointly learn from both linguistic content and relational structures among comments. We also introduce a dynamically adjusted attention mechanism that adaptively balances nodal and topological features during information aggregation. Empirical evaluations demonstrate that our proposed architecture outperforms 12 state-of-the-art Large Language Models (LLMs) across multiple metrics while requiring significantly lower inference cost. These findings highlight the crucial role of structural context in detecting online incivility and address the limitations of text-only LLM paradigms in behavioral prediction. All datasets and comparative outputs will be publicly available in our repository to support further research and reproducibility.

</details>


### [69] [HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs](https://arxiv.org/abs/2512.07687)
*Sujoy Nath,Arkaprabha Basu,Sharanya Dasgupta,Swagatam Das*

Main category: cs.CL

> 研究提出了一种新的方法\textsc{\textsc{HalluShift++}}，通过分析多模态大语言模型内部动态中的异常来检测幻觉，这种方法扩展到多模态场景。

<details>
  <summary>Details</summary>

**Motivation:** 多模态大语言模型在视觉语言理解任务中表现出色，但它们经常生成与视觉内容事实不一致的描述，这可能导致不良后果。因此，评估MLLM中的幻觉变得日益重要。

**Method:** 本研究提出了一种假设，即幻觉在多模态大语言模型（MLLMs）的内部层动态中表现为可测量的异常，这不仅仅是由于分布变化，还体现在对特定假设的逐层分析中。通过引入这些修改，\textsc{\textsc{HalluShift++}}扩大了幻觉检测的有效性，从基于文本的大语言模型（LLMs）到多模态场景。

**Result:** 本研究未在摘要中具体说明实验结果，但主要集中在提出新的假设和方法上，以改进幻觉的检测。

**Conclusion:** 该论文提出了一种新的方法，通过分析多模态大语言模型（MLLMs）内部动态中的异常来解决幻觉问题，这种方法不仅限于文本场景，也适用于多模态场景。

**Abstract:** Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding tasks. While these models often produce linguistically coherent output, they often suffer from hallucinations, generating descriptions that are factually inconsistent with the visual content, potentially leading to adverse consequences. Therefore, the assessment of hallucinations in MLLM has become increasingly crucial in the model development process. Contemporary methodologies predominantly depend on external LLM evaluators, which are themselves susceptible to hallucinations and may present challenges in terms of domain adaptation. In this study, we propose the hypothesis that hallucination manifests as measurable irregularities within the internal layer dynamics of MLLMs, not merely due to distributional shifts but also in the context of layer-wise analysis of specific assumptions. By incorporating such modifications, \textsc{\textsc{HalluShift++}} broadens the efficacy of hallucination detection from text-based large language models (LLMs) to encompass multimodal scenarios. Our codebase is available at https://github.com/C0mRD/HalluShift_Plus.

</details>


### [70] [Automated Generation of Custom MedDRA Queries Using SafeTerm Medical Map](https://arxiv.org/abs/2512.07694)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta*

Main category: cs.CL

> 本文介绍了一个名为SafeTerm的人工智能系统，它理解和处理医学术语，自动检索相关的MedDRA首选术语，并按照相关性进行排序。该系统在不同阈值下表现出色，提供了一种自动化MedDRA查询生成的补充方法。

<details>
  <summary>Details</summary>

**Motivation:** 在药品上市前的安全审查中，将相关的不良事件合并到标准化的MedDRA查询或FDA新药办公室定制的医疗查询(OCMQ)中，对于信号检测至关重要。

**Method:** 我们提出了一种新的定量人工智能系统，该系统能够理解和处理医学术语，并使用多标准统计方法，自动检索相关MedDRA首选术语(PT)，并根据相关性评分进行排序。该系统（SafeTerm）将医学查询术语和MedDRA PT嵌入多维向量空间中，然后应用余弦相似性和极值聚类生成排名的PT列表。

**Result:** 验证是在104个查询的FDA OCMQ v3.0（限制在有效的MedDRA PT）上进行的。在中等阈值下实现了高召回率（>95%）。更高的阈值提高了精度（高达86%）。最优阈值（0.70 - 0.75）的召回率约为50%，精确度约为33%。窄术语PT子集表现相似，但需要稍高的相似性阈值。

**Conclusion:** SafeTerm人工智能驱动系统提供了一种可行的补充方法，用于自动化的MedDRA查询生成。建议最初采用的相似性阈值约为0.60，对于精细的术语选择可调整阈值为更高。

**Abstract:** In pre-market drug safety review, grouping related adverse event terms into standardised MedDRA queries or the FDA Office of New Drugs Custom Medical Queries (OCMQs) is critical for signal detection. We present a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against the FDA OCMQ v3.0 (104 queries), restricted to valid MedDRA PTs. Precision, recall and F1 were computed across similarity-thresholds. High recall (>95%) is achieved at moderate thresholds. Higher thresholds improve precision (up to 86%). The optimal threshold (~0.70 - 0.75) yielded recall ~50% and precision ~33%. Narrow-term PT subsets performed similarly but required slightly higher similarity thresholds. The SafeTerm AI-driven system provides a viable supplementary method for automated MedDRA query generation. A similarity threshold of ~0.60 is recommended initially, with increased thresholds for refined term selection.

</details>


### [71] [Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in Narratives?](https://arxiv.org/abs/2512.07777)
*Karin de Langis,Püren Öncel,Ryan Peters,Andrew Elfenbein,Laura Kristen Allen,Andreas Schramm,Dongyeop Kang*

Main category: cs.CL

> 通过对大型语言模型（LLMs）使用叙事数据集，研究发现尽管LLMs能够可靠内部分辨出不连贯的叙述，但表现出在处理连贯性叙述上的局限性，这表明LLMs在理解复杂叙述方面存在不足。

<details>
  <summary>Details</summary>

**Motivation:** 研究目的是调查LLMs在辨识和生成连贯性叙述方面的表现，特别是其内部状态和表述行为之间存在的差异。

**Method:** 使用了一组配对叙事数据集，研究大型语言模型（LLMs）在区分不连贯和连贯故事方面的可靠性。通过探针研究来分析LLMs的内部表征是否能够可靠地识别不连贯的叙述。

**Result:** 研究发现，尽管LLMs的内部表征能够可靠地识别出不连贯的叙述，但它们生成响应评分时，无法充分地在不同的引言变化中区分连贯和不连贯的叙述，显示出在理解故事编排方面的差距。实验表明，LLMs更多依赖于原型世界知识，而不是基于意义的叙述连贯性来判定不连贯的叙述。

**Conclusion:** 研究结果表明LLMs在叙述的连贯性方面尚未完全掌握。

**Abstract:** Leveraging a dataset of paired narratives, we investigate the extent to which large language models (LLMs) can reliably separate incoherent and coherent stories. A probing study finds that LLMs' internal representations can reliably identify incoherent narratives. However, LLMs generate responses to rating questions that fail to satisfactorily separate the coherent and incoherent narratives across several prompt variations, hinting at a gap in LLM's understanding of storytelling. The reasoning LLMs tested do not eliminate these deficits, indicating that thought strings may not be able to fully address the discrepancy between model internal state and behavior. Additionally, we find that LLMs appear to be more sensitive to incoherence resulting from an event that violates the setting (e.g., a rainy day in the desert) than to incoherence arising from a character violating an established trait (e.g., Mary, a vegetarian, later orders a cheeseburger), suggesting that LLMs may rely more on prototypical world knowledge than building meaning-based narrative coherence. The consistent asymmetry found in our results suggests that LLMs do not have a complete grasp on narrative coherence.

</details>


### [72] [On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models](https://arxiv.org/abs/2512.07783)
*Charlie Zhang,Graham Neubig,Xiang Yue*

Main category: cs.CL

> 本文通过一个完全受控的实验框架揭示了预训练、中期训练和基于RL后训练对语言模型推理能力的因果贡献，阐明了这些训练策略之间的作用关系。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决现代化训练流程中的控制问题，特别是在大型预训练语料库的不透明性、中期训练的忽视以及RL目标与未知先前知识的复杂互动。

**Method:** 本文开发了一个完全受控的实验框架，用于分离预训练、中期训练和基于RL的后训练的因果贡献。该方法使用了具有显式原子操作、可解析的逐步推理痕迹和训练分布系统性操作的合成推理任务。

**Result:** 研究结果表明：1) RL仅在预训练留下足够的改进空间并且RL数据针对模型的边缘能力，即那些难以但尚未超出能力范围的任务上，产生真正的能力提升(通过pass@128衡量)。2) 对于上下文的普遍性，需要预训练的最小且足够的暴露，之后RL可以可靠地进行转移。3) 在固定计算资源下，中期训练显著提高性能，显示出其在训练流程中的中心但未充分探索的角色。4) 过程级别的奖励减少了奖励操纵，提高了推理的准确度。

**Conclusion:** 这些结果明确了预训练、中期训练和RL之间的相互作用，为我们理解并改进语言模型的推理训练策略提供了基础。

**Abstract:** Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.

</details>


### [73] [Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support](https://arxiv.org/abs/2512.07801)
*Raunak Jain,Mudita Khurana*

Main category: cs.CL

> 研究当前AI在高风险决策支持中的不足，提出协作因果认知框架以改善人机协作，提升团队决策效率。

<details>
  <summary>Details</summary>

**Motivation:** 当前，虽然基于语言模型的代理被迅速集成到专家决策支持中，但在混乱或高风险环境中，它们很少能使团队变得更聪明。人机团队经常整体表现出低于最佳个体的表现，专家们则在核实循环和过度依赖之间摇摆，所期望的互补性未能实现。

**Method:** 提出协作因果认知（CCS）作为研究议程和决策支持代理的组织框架：旨在作为认知工作的合作伙伴设计的系统，保持特定专家如何推理的演变模型，帮助表达和修订目标，共同构建和压力测试因果假设，并从共同决策的结果中学习，以便人和代理随着时间的推移而改善。

**Result:** 未直接提及具体研究结果，但提出的问题和方向是设计一种能够更好地与人类专家协作的AI决策支持系统。

**Conclusion:** 建议的方向可以重新定位多智能体系统研究，围绕参与协作认知的智能体，作为与人类合作伙伴共同思考的AI队友。

**Abstract:** LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.

</details>


### [74] [Do Generalisation Results Generalise?](https://arxiv.org/abs/2512.07832)
*Matteo Boglioni,Andrea Sgobbi,Gabriel Tavernini,Francesco Rita,Marius Mosbach,Tiago Pimentel*

Main category: cs.CL

> 研究分析了大语言模型在多个分布外测试集上的表现，发现这些泛化性能的相关性高度依赖于具体分析的模型选择，没有发现总体一致的泛化表现趋势。

<details>
  <summary>Details</summary>

**Motivation:** 此前对大语言模型泛化性能的评估大多只关注单一的分布外数据集，这种做法可能无法精确评估模型的能力，因为部署模型时遇到的数据分布变化更加多样。因此，本研究试图探究分布外泛化结果是否在不同测试集间具有一致性。

**Method:** 研究通过在整个微调过程中对多个分布外测试集进行模型性能评估来探讨模型的分布外泛化能力。具体来说，研究计算除去领域内表现后的各类分布外测试集表现之间的偏相关性，以此来评估控制领域内表现后各类泛化表现的相关性。

**Result:** 评估结果显示，一旦控制了领域内表现，不同分布外测试集之间的表现相关性不同，且这种相关性与具体模型的选择有很强的关联。

**Conclusion:** 通过对OLMo2和OPT模型进行分析，研究发现不存在总体一致的泛化表现趋势：不同分布外测试集之间的正相关或负相关性取决于所分析的具体模型选择。

**Abstract:** A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [75] [Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven' Matrices](https://arxiv.org/abs/2512.05969)
*Hokin Deng*

Main category: cs.CV

> 研究展示了视频生成模型现在具备推理能力，并通过建立一个强大实验范式和代码框架验证了模型在多种任务上的表现，为未来的模型改进提供了机会。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于探索视频生成模型是否能够进行推理，通过建立一个强大的实验范式，我们可以验证模型的这种能力，并为未来强化学习提高视频模型的推理能力提供机会。

**Method:** 我们构建了一个基于"任务配对"设计的实验范式，通过这个范式，我们可以测试和评估视频生成模型在多个任务（如国际象棋、迷宫、数独、心理旋转、Raven矩阵等）上的推理能力。我们还创建了一个支持这个范式的代码框架，该框架允许用户轻松添加模型和任务以进行扩展。

**Result:** 实验结果显示，领先的模型（如Sora-2）在这些任务上取得了60%的成功率，并且我们的自动化评估方法与人类的判断有很强的相关性。

**Conclusion:** 我们的研究证明了视频生成模型现在可以进行推理，而且通过提供的实验范式和代码框架，我们可以进一步研究如何通过强化学习来提高视频模型的推理能力。

**Abstract:** We show that video generation models could reason now. Testing on tasks such as chess, maze, Sudoku, mental rotation, and Raven's Matrices, leading models such as Sora-2 achieve sixty percent success rates. We establish a robust experimental paradigm centered on the "Task Pair" design. We build a code framework, with 39 models available already, that supports this paradigm and allows for easy scaling - users can add models and tasks efficiently. We show our automated evaluation strongly correlates with human judgment, and therefore this paradigm is highly scalable. We see an opportunity, given the availability of our paradigm, to do reinforcement learning for improving reasoning in video models. You could checkout all of our raw $\href{https://grow-ai-like-a-child.com/video-reason/}{results}$ and our $\href{https://github.com/hokindeng/VMEvalKit}{VMEvalKit}$ codebase.

</details>


### [76] [Adaptive Dataset Quantization: A New Direction for Dataset Pruning](https://arxiv.org/abs/2512.05987)
*Chenyue Yu,Jianyu Yu*

Main category: cs.CV

> A novel dataset quantization method is proposed for edge devices to reduce intra-sample redundancy, maintaining training performance while significantly compressing datasets compared to traditional methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address storage and communication costs issues for large-scale datasets in resource-constrained edge devices by focusing on intra-sample redundancy reduction.

**Method:** This paper proposes a dataset quantization approach to reduce intra-sample redundancy in large-scale datasets for edge devices. It involves linear symmetric quantization for initial quantization and an adaptive quantization allocation algorithm to maintain a constant total compression ratio while accommodating varying precision requirements.

**Result:** The proposed method maintains model training performance while achieving significant dataset compression, outperforming traditional quantization and dataset pruning methods under the same compression ratios, validated through experiments on CIFAR-10, CIFAR-100, and ImageNet-1K.

**Conclusion:** The paper concludes that the proposed dataset-level quantization algorithm with adaptive ratio allocation successfully reduces storage requirements without compromising training performance on datasets such as CIFAR-10, CIFAR-100, and ImageNet-1K.

**Abstract:** This paper addresses the challenges of storage and communication costs for large-scale datasets in resource-constrained edge devices by proposing a novel dataset quantization approach to reduce intra-sample redundancy. Unlike traditional dataset pruning and distillation methods that focus on inter-sample redundancy, the proposed method compresses each image by reducing redundant or less informative content within samples while preserving essential features. It first applies linear symmetric quantization to obtain an initial quantization range and scale for each sample. Then, an adaptive quantization allocation algorithm is introduced to distribute different quantization ratios for samples with varying precision requirements, maintaining a constant total compression ratio. The main contributions include: (1) being the first to use limited bits to represent datasets for storage reduction; (2) introducing a dataset-level quantization algorithm with adaptive ratio allocation; and (3) validating the method's effectiveness through extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K. Results show that the method maintains model training performance while achieving significant dataset compression, outperforming traditional quantization and dataset pruning baselines under the same compression ratios.

</details>


### [77] [VG3T: Visual Geometry Grounded Gaussian Transformer](https://arxiv.org/abs/2512.05988)
*Junho Kim,Seongwon Lee*

Main category: cs.CV

> 本文提出了VG3T，一种新的3D场景生成方法，能够在少用46%基础数据的情况下提升1.7%的mIoU性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法在进行多视图融合时，通常会产生片段化和次优的3D表现。为解决这一问题，本文提出了VG3T。

**Method:** 本文提出了VG3T，一种新颖的多视图前馈网络，通过3D高斯表示预测3D语义占用，从而解决了多视图融合的问题。与从前基于单视角图像推断高斯分布的方法不同，VG3T直接在整个多视图联合处理中预测一组语义属性化的高斯分布。

**Result:** 在nuScenes基准测试上，VG3T在实现mIoU提升1.7%的同时，使用较少的46%的原始数据，优于当前的最先进方法，显示出更高的效率和性能。

**Conclusion:** 整体来看，本文提出的方法能够有效克服因逐视图处理带来的碎片化和不一致性，为几何和语义提供了一个统一的表示，具有显著效率优势。

**Abstract:** Generating a coherent 3D scene representation from multi-view images is a fundamental yet challenging task. Existing methods often struggle with multi-view fusion, leading to fragmented 3D representations and sub-optimal performance. To address this, we introduce VG3T, a novel multi-view feed-forward network that predicts a 3D semantic occupancy via a 3D Gaussian representation. Unlike prior methods that infer Gaussians from single-view images, our model directly predicts a set of semantically attributed Gaussians in a joint, multi-view fashion. This novel approach overcomes the fragmentation and inconsistency inherent in view-by-view processing, offering a unified paradigm to represent both geometry and semantics. We also introduce two key components, Grid-Based Sampling and Positional Refinement, to mitigate the distance-dependent density bias common in pixel-aligned Gaussian initialization methods. Our VG3T shows a notable 1.7%p improvement in mIoU while using 46% fewer primitives than the previous state-of-the-art on the nuScenes benchmark, highlighting its superior efficiency and performance.

</details>


### [78] [EmoDiffTalk:Emotion-aware Diffusion for Editable 3D Gaussian Talking Head](https://arxiv.org/abs/2512.05991)
*Chang Liu,Tianjiao Jing,Chengcheng Ma,Xuanqi Zhou,Zhengxuan Lian,Qin Jin,Hongliang Yuan,Shi-Sheng Huang*

Main category: cs.CV

> 本文提出了一种名为EmoDiffTalk的新方法，以改进3D谈话头像生成中的情感表达控制，实现了通过文本输入进行精细和广泛的动态情感编辑，实验结果验证了该方法的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 目前基于3D高斯喷射的真实感3D谈话头像在情感表达操纵方面仍然存在显著不足，尤其是在使用多模态控制进行细微和广泛的动态情感编辑方面。

**Method:** 本文提出了一种可编辑的3D高斯谈话头像，即EmoDiffTalk。该方法的核心思想是情感感知的高斯扩散，包括用于精细动画的脸部动作单元（AU）提示高斯扩散过程，以及一个准确的基于文本的情感到AU控制器，以实现通过文本输入进行精细和广泛的动态情感编辑。

**Result:** 在公共的EmoTalk3D和RenderMe-360数据集上的实验结果证明，我们的EmoDiffTalk在情感细微度、唇行同步的精确度以及可控制性方面优于之前的其他方法，为高质量的、基于扩散驱动的多模态可编辑3D谈话头合成提供了一个原则性的途径。

**Conclusion:** 根据我们的了解，我们的EmoDiffTalk是首批几个3D高斯喷射谈话头像生成框架之一，特别是支持基于AU表情空间内的连续多模态情感编辑。

**Abstract:** Recent photo-realistic 3D talking head via 3D Gaussian Splatting still has significant shortcoming in emotional expression manipulation, especially for fine-grained and expansive dynamics emotional editing using multi-modal control. This paper introduces a new editable 3D Gaussian talking head, i.e. EmoDiffTalk. Our key idea is a novel Emotion-aware Gaussian Diffusion, which includes an action unit (AU) prompt Gaussian diffusion process for fine-grained facial animator, and moreover an accurate text-to-AU emotion controller to provide accurate and expansive dynamic emotional editing using text input. Experiments on public EmoTalk3D and RenderMe-360 datasets demonstrate superior emotional subtlety, lip-sync fidelity, and controllability of our EmoDiffTalk over previous works, establishing a principled pathway toward high-quality, diffusion-driven, multimodal editable 3D talking-head synthesis. To our best knowledge, our EmoDiffTalk is one of the first few 3D Gaussian Splatting talking-head generation framework, especially supporting continuous, multimodal emotional editing within the AU-based expression space.

</details>


### [79] [Domain-Specific Foundation Model Improves AI-Based Analysis of Neuropathology](https://arxiv.org/abs/2512.05993)
*Ruchika Verma,Shrishtee Kandoi,Robina Afzal,Shengjia Chen,Jannes Jegminat,Michael W. Karlovich,Melissa Umphlett,Timothy E. Richardson,Kevin Clare,Quazi Hossain,Jorge Samanamud,Phyllis L. Faust,Elan D. Louis,Ann C. McKee,Thor D. Stein,Jonathan D. Cherry,Jesse Mez,Anya C. McGoldrick,Dalilah D. Quintana Mora,Melissa J. Nirenberg,Ruth H. Walker,Yolfrankcis Mendez,Susan Morgello,Dennis W. Dickson,Melissa E. Murray,Carlos Cordon-Cardo,Nadejda M. Tsankova,Jamie M. Walker,Diana K. Dangoor,Stephanie McQuillan,Emma L. Thorn,Claudia De Sanctis,Shuying Li,Thomas J. Fuchs,Kurt Farrell,John F. Crary,Gabriele Campanella*

Main category: cs.CV

> 研究团队开发了NeuroFM，专门用于神经病理全切片图像，其在神经病理学特定下游任务中的表现优于通用模型，显示了专用于特定医学领域的基础模型的发展潜力。

<details>
  <summary>Details</summary>

**Motivation:** 已有的基础模型多基于手术病理学数据，这些数据侧重非神经系统组织，而神经病理学具有自身独特的细胞类型、细胞结构和疾病特定病理特征，这导致现有模型可能无法捕捉对神经退行性疾病解释至关重要的形态学模式。

**Method:** 开发了NeuroFM，这是一种专门针对脑组织全切片图像的深度学习模型，涵盖多种神经退行性疾病。

**Result:** NeuroFM在几个特定神经病理任务中表现优于通用模型，包括混合性痴呆分类、海马区域分割和神经退行性小脑共济失调识别。

**Conclusion:** 脑组织特定领域的基础模型可以更好地捕捉神经病理学特定特征，这为利用AI进行脑疾病诊断和研究提供了更准确可靠的工具，并为特定区域数字病理中的模型开发树立了典范。

**Abstract:** Foundation models have transformed computational pathology by providing generalizable representations from large-scale histology datasets. However, existing models are predominantly trained on surgical pathology data, which is enriched for non-nervous tissue and overrepresents neoplastic, inflammatory, metabolic, and other non-neurological diseases. Neuropathology represents a markedly different domain of histopathology, characterized by unique cell types (neurons, glia, etc.), distinct cytoarchitecture, and disease-specific pathological features including neurofibrillary tangles, amyloid plaques, Lewy bodies, and pattern-specific neurodegeneration. This domain mismatch may limit the ability of general-purpose foundation models to capture the morphological patterns critical for interpreting neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, and cerebellar ataxias. To address this gap, we developed NeuroFM, a foundation model trained specifically on whole-slide images of brain tissue spanning diverse neurodegenerative pathologies. NeuroFM demonstrates superior performance compared to general-purpose models across multiple neuropathology-specific downstream tasks, including mixed dementia disease classification, hippocampal region segmentation, and neurodegenerative ataxia identification encompassing cerebellar essential tremor and spinocerebellar ataxia subtypes. This work establishes that domain-specialized foundation models trained on brain tissue can better capture neuropathology-specific features than models trained on general surgical pathology datasets. By tailoring foundation models to the unique morphological landscape of neurodegenerative diseases, NeuroFM enables more accurate and reliable AI-based analysis for brain disease diagnosis and research, setting a precedent for domain-specific model development in specialized areas of digital pathology.

</details>


### [80] [FishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting](https://arxiv.org/abs/2512.05996)
*Yi Liu,Jingyu Song,Vedanth Kallakuri,Katherine A. Skinner*

Main category: cs.CV

> FishDetector-R1 是一种在弱监督下进行水下鱼类检测、分割和计数的框架，能够显著提升水下视觉理解的精度。

<details>
  <summary>Details</summary>

**Motivation:** 由于视觉退化和昂贵的标注成本，分析水下鱼类影像对于生态监测来说仍然很困难。

**Method:** FishDetector-R1 是一个基于 MLLM 的统一框架，用于在弱监督条件下进行水下鱼类检测、分割和计数。该方法包括两个关键组件：一种新型的 detect-to-count 提示，它强制执行空间一致的检测和计数，以及使用稀疏点标签的可验证奖励的强化学习（RLVR）。

**Result:** 在 DeepFish 数据集上，FishDetector-R1 比基线方法显著提高，平均精度（AP）提高了 20%，平均交并比（mIoU）提高了 10%，同时将平均绝对误差（MAE）降低了 30%，全局平均误差（GAME）降低了 35%。

**Conclusion:** FishDetector-R1 提供了一种可靠的、可扩展的解决方案，能够在弱监督条件下实现准确的水下视觉理解，并表现出强大的跨域鲁棒性。

**Abstract:** Analyzing underwater fish imagery is critical for ecological monitoring but remains difficult due to visual degradation and costly annotations. We introduce FishDetector-R1, a unified MLLM-based framework for fish detection, segmentation, and counting under weak supervision. On the DeepFish dataset, our framework achieves substantial gains over baselines, improving AP by 20% and mIoU by 10%, while reducing MAE by 30% and GAME by 35%. These improvements stem from two key components: a novel detect-to-count prompt that enforces spatially consistent detections and counts, and Reinforcement Learning from Verifiable Reward (RLVR) with a complementary scalable paradigm leveraging sparse point labels. Ablation studies further validate the effectiveness of this reward design. Moreover, the improvement generalizes well to other underwater datasets, confirming strong cross-domain robustness. Overall, FishDetector-R1 provides a reliable and scalable solution for accurate marine visual understanding via weak supervision. The project page for FishDetector-R1 is https://umfieldrobotics.github.io/FishDetector-R1.

</details>


### [81] [PrunedCaps: A Case For Primary Capsules Discrimination](https://arxiv.org/abs/2512.06003)
*Ramin Sharifi,Pouya Shiri,Amirali Baniasadi*

Main category: cs.CV

> This paper demonstrates that pruning Primary Capsules in CapsNets can significantly increase computational efficiency without compromising accuracy on several standard datasets.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to reduce the computational cost and resource consumption of CapsNets, which are currently known to be resource-intensive due to a high number of Primary Capsules.

**Method:** The method involves pruning the Primary Capsules in CapsNets and evaluating the performance on datasets such as MNIST, Fashion-MNIST, CIFAR-10, and SVHN to assess the impact on accuracy and computational efficiency.

**Result:** The results show that a pruned CapsNet can achieve a speedup of up to 9.90 times and a reduction of more than 95.36 percent of floating-point operations in the dynamic routing stage without accuracy loss.

**Conclusion:** The conclusion is that Primary Capsules pruning can dramatically improve the efficiency of CapsNets in terms of speed and computational resources, although the benefit can vary across different datasets.

**Abstract:** Capsule Networks (CapsNets) are a generation of image classifiers with proven advantages over Convolutional Neural Networks (CNNs). Better robustness to affine transformation and overlapping image detection are some of the benefits associated with CapsNets. However, CapsNets cannot be classified as resource-efficient deep learning architecture due to the high number of Primary Capsules (PCs). In addition, CapsNets' training and testing are slow and resource hungry. This paper investigates the possibility of Primary Capsules pruning in CapsNets on MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and SVHN datasets. We show that a pruned version of CapsNet performs up to 9.90 times faster than the conventional architecture by removing 95 percent of Capsules without a loss of accuracy. Also, our pruned architecture saves on more than 95.36 percent of floating-point operations in the dynamic routing stage of the architecture. Moreover, we provide insight into why some datasets benefit significantly from pruning while others fall behind.

</details>


### [82] [Simple Agents Outperform Experts in Biomedical Imaging Workflow Optimization](https://arxiv.org/abs/2512.06006)
*Xuefei,Wang,Kai A. Horstmann,Ethan Lin,Jonathan Chen,Alexander R. Farhang,Sophia Stiles,Atharva Sehgal,Jonathan Light,David Van Valen,Yisong Yue,Jennifer J. Sun*

Main category: cs.CV

> 利用AI代理自動化科研數據集的生產級計算機視覺工具適應問題，研究證明了簡單代理框架可生成優於人工編碼的代碼。

<details>
  <summary>Details</summary>

**Motivation:** 研究主要是為了克服在將生產級計算機視覺工具適應於特殊科研數據集時遇到的困難，這通常需要大量標記數據或大量手動編碼工作。

**Method:** 本研究提出了一种用于评估代碼優化的代理框架，並使用該框架研究了三種生物醫學成像管道。

**Result:** 研究提出了實用的代理設計路線圖，並將代理生成的函數部署到生產管道中，顯示了明確的實際應用途徑。

**Conclusion:** 研究證明，簡單的代理框架能夠一致地生成優於人工專家解決方案的適應代碼，並揭示了常見的複雜代理架構並非在所有情形下都有好處。

**Abstract:** Adapting production-level computer vision tools to bespoke scientific datasets is a critical "last mile" bottleneck. Current solutions are impractical: fine-tuning requires large annotated datasets scientists often lack, while manual code adaptation costs scientists weeks to months of effort. We consider using AI agents to automate this manual coding, and focus on the open question of optimal agent design for this targeted task. We introduce a systematic evaluation framework for agentic code optimization and use it to study three production-level biomedical imaging pipelines. We demonstrate that a simple agent framework consistently generates adaptation code that outperforms human-expert solutions. Our analysis reveals that common, complex agent architectures are not universally beneficial, leading to a practical roadmap for agent design. We open source our framework and validate our approach by deploying agent-generated functions into a production pipeline, demonstrating a clear pathway for real-world impact.

</details>


### [83] [Fast and Flexible Robustness Certificates for Semantic Segmentation](https://arxiv.org/abs/2512.06010)
*Thomas Massena,Corentin Friedrich,Franck Mamalet,Mathieu Serrurier*

Main category: cs.CV

> 本论文提出了一种新型的具有内置Lipschitz约束的可证可鲁棒语义分割网络，并实现了一个新的框架以提高计算效率。研究表明这种方法的计算时间比随机平滑方法快600倍。

<details>
  <summary>Details</summary>

**Motivation:** 目前已经有很多研究致力于提升神经网络的鲁棒性（例如，通过对抗训练）或为神经网络决策设定一个鲁棒性的保证（例如，通过随机平滑、形式化方法或Lipschitz边界）。然而，这些研究大多集中在分类任务上，目前有效的保证方法在语义分割任务中尚未充分探讨。因此，本研究旨在解决这一问题。

**Method:** 本研究引入了一种新的拥有内置Lipschitz约束的语义分割网络类别，这些网络可以高效地进行训练，并在如Cityscapes等具有挑战性的数据集上实现了竞争性的像素精度。此外，我们提供了一个新的框架，用于泛化语义分割任务中的鲁棒性证书，展示了使用Lipschitz网络的灵活性和计算效率。这种方法首次实现了实时兼容的可证明鲁棒的语义分割，并允许计算在$\ell_2$攻击下的最坏情况性能。

**Result:** 本研究的网络在Cityscapes数据集上实现了有竞争力的像素精度。更重要的是，该方法能够在GPU上以实时速度运行，并且显著提升了计算效率，比随机平滑方法快600倍。

**Conclusion:** 基于以上结果，本研究提出了一种高效且计算效率高的梯度约束网络以进行鲁棒性语义分割，实现了在对抗攻击下的像素级鲁棒性和实时性，并超越了现有的随机平滑方法。

**Abstract:** Deep Neural Networks are vulnerable to small perturbations that can drastically alter their predictions for perceptually unchanged inputs. The literature on adversarially robust Deep Learning attempts to either enhance the robustness of neural networks (e.g, via adversarial training) or to certify their decisions up to a given robustness level (e.g, by using randomized smoothing, formal methods or Lipschitz bounds). These studies mostly focus on classification tasks and few efficient certification procedures currently exist for semantic segmentation. In this work, we introduce a new class of certifiably robust Semantic Segmentation networks with built-in Lipschitz constraints that are efficiently trainable and achieve competitive pixel accuracy on challenging datasets such as Cityscapes. Additionally, we provide a novel framework that generalizes robustness certificates for semantic segmentation tasks, where we showcase the flexibility and computational efficiency of using Lipschitz networks. Our approach unlocks real-time compatible certifiably robust semantic segmentation for the first time. Moreover, it allows the computation of worst-case performance under $\ell_2$ attacks of radius $ε$ across a wide range of performance measures. Crucially, we benchmark the runtime of our certification process and find our approach to be around 600 times faster than randomized smoothing methods at inference with comparable certificates on an NVIDIA A100 GPU. Finally, we evaluate the tightness of our worstcase certificates against state-of-the-art adversarial attacks to further validate the performance of our method.

</details>


### [84] [High-Throughput Unsupervised Profiling of the Morphology of 316L Powder Particles for Use in Additive Manufacturing](https://arxiv.org/abs/2512.06012)
*Emmanuel Akeweje,Conall Kirk,Chi-Wai Chan,Denis Dowling,Mimi Zhang*

Main category: cs.CV

> 

<details>
  <summary>Details</summary>

**Motivation:** 

**Method:** Structure

**Result:** {
  "tldr": "研究人员提出了一种结合高通量成像与形态提取及聚类分析的机器学习框架，用于对金属粉末的形态进行规模化分析，目的在于提高SLM工艺的零件质量。研究在大约126,000张粉末图像数据集上评估了三种聚类管道，结果显示Fourier描述子+k-means的管道最为有效。",
  "motivation": "研究动机在于克服传统粉末表征方法低吞吐量和定性分析的局限性，开发一个自动化机器学习框架，以捕捉工业化产量批次的异质性，从而提高SLM工艺的零件质量。",
  "method": "该研究采用高通量成像技术并结合形态提取和聚类分析，开发了三种聚类管道：自动编码器管道、形态描述符管道以及功能性数据管道。其中，通过内部有效性指标评估，Fourier描述子结合k-means的聚类管道效果最佳。",
  "result": "结果表明，Fourier描述子结合k-means的聚类管道可以在亚毫秒内处理每个颗粒，同时具有最低的Davies-Bouldin指数和最高的Calinski-Harabasz得分。",
  "conclusion": "虽然这项研究主要集中在建立形态聚类框架上，但由此产生的形态类别为未来研究它们与流动性、填充密度以及SLM零件质量的关系奠定了基础。总体而言，该无监督学习框架能够快速高效地进行粉末形态评估，并支持监视重用周期内的形态演化，为实现SLM流程中的实时进料监控提供了路径。\n"}
}

**Conclusion:** 

**Abstract:** Selective Laser Melting (SLM) is a powder-bed additive manufacturing technique whose part quality depends critically on feedstock morphology. However, conventional powder characterization methods are low-throughput and qualitative, failing to capture the heterogeneity of industrial-scale batches. We present an automated, machine learning framework that couples high-throughput imaging with shape extraction and clustering to profile metallic powder morphology at scale. We develop and evaluate three clustering pipelines: an autoencoder pipeline, a shape-descriptor pipeline, and a functional-data pipeline. Across a dataset of approximately 126,000 powder images (0.5-102 micrometer diameter), internal validity metrics identify the Fourier-descriptor + k-means pipeline as the most effective, achieving the lowest Davies-Bouldin index and highest Calinski-Harabasz score while maintaining sub-millisecond runtime per particle on a standard desktop workstation. Although the present work focuses on establishing the morphological-clustering framework, the resulting shape groups form a basis for future studies examining their relationship to flowability, packing density, and SLM part quality. Overall, this unsupervised learning framework enables rapid, automated assessment of powder morphology and supports tracking of shape evolution across reuse cycles, offering a path toward real-time feedstock monitoring in SLM workflows.

</details>


### [85] [VAT: Vision Action Transformer by Unlocking Full Representation of ViT](https://arxiv.org/abs/2512.06013)
*Wenhao Li,Chengwei Ma,Weixin Mao*

Main category: cs.CV

> 本文提出了 VAT 架构，它利用整个 ViT 的特征层次，以实现更好的视觉感知与动作生成融合，显著提高了机器人模拟任务的成功率。

<details>
  <summary>Details</summary>

**Motivation:** 尽管 Vision Transformers (ViTs) 在机器人学习中用于视觉感知的标准方法，但大多数方法通过仅使用最终层的特征来丢弃有价值的信息。这导致表示不足，因此提出了 VAT，解锁了 ViT 的完整特征层次结构。

**Method:** VAT (Vision Action Transformer) 是一种从 ViT 扩展而来的新型架构，它利用 ViT 的完整特征层次结构。通过跨所有变压器层处理每个动作的动作代币和视觉特征，它实现了感知与动作生成的深度和渐进式融合。

**Result:** 在一套模拟的操控任务中，VAT 达到了 98.15% 的平均成功率，这在四个 LIBERO 基准测试中确立了新的最佳表现，超越了先前的方法，如 OpenVLA-OFT。

**Conclusion:** 展示了利用视觉模型的完整“表示轨迹”来推进机器人策略的重要性，同时展示了一种强大的模仿学习模型。

**Abstract:** In robot learning, Vision Transformers (ViTs) are standard for visual perception, yet most methods discard valuable information by using only the final layer's features. We argue this provides an insufficient representation and propose the Vision Action Transformer (VAT), a novel architecture that is extended from ViT and unlocks the full feature hierarchy of ViT. VAT processes specialized action tokens with visual features across all transformer layers, enabling a deep and progressive fusion of perception and action generation. On a suite of simulated manipulation tasks, VAT achieves a 98.15\% average success rate across four LIBERO benchmarks, establishing a new state-of-the-art by outperforming prior methods like OpenVLA-OFT. Our work presents not only a powerful model for imitation learning but also demonstrates the critical importance of leveraging the complete ''representation trajectory'' of vision models to advance robotic policy. The GitHub URL for the project code is https://github.com/sellerbubble/VAT.

</details>


### [86] [Benchmarking CXR Foundation Models With Publicly Available MIMIC-CXR and NIH-CXR14 Datasets](https://arxiv.org/abs/2512.06014)
*Jiho Shin,Dominic Marshall,Matthieu Komorowski*

Main category: cs.CV

> 研究评估了两个胸部X光图像嵌入模型在公共数据集上的性能，MedImageInsight在大多数任务中表现更佳，CXR-Foundation稳定性更高。

<details>
  <summary>Details</summary>

**Motivation:** 虽然最近的基础模型在医学图像表示学习中表现出色，但它们在不同数据集上的表现差异仍未得到充分探索。

**Method:** 本研究使用两个大规模胸部X光图像嵌入模型(CXR-Foundation (ELIXR v2.0) 和 MedImageInsight) 在MIMIC-CR和NIH ChestX-ray14公共数据集上进行基准测试。为了确保可重复比较，每个模型都使用统一的预处理流程和固定的下游分类器。直接从预训练的编码器中提取嵌入，训练轻量级的LightGBM分类器，并报告多个疾病标签的平均AUROC和F1-score的95%置信区间。

**Result:** MedImageInsight在大多数任务中获得了稍高的性能，而CXR-Foundation展示了较强的跨数据集稳定性。MedImageInsight嵌入的无监督聚类还揭示了一个与定量结果一致的疾病特异性结构。

**Conclusion:** 研究结果强调了需要标准化评估医疗基础模型，并为未来的多模态和临床集成研究建立了可重复的基线。

**Abstract:** Recent foundation models have demonstrated strong performance in medical image representation learning, yet their comparative behaviour across datasets remains underexplored. This work benchmarks two large-scale chest X-ray (CXR) embedding models (CXR-Foundation (ELIXR v2.0) and MedImagelnsight) on public MIMIC-CR and NIH ChestX-ray14 datasets. Each model was evaluated using a unified preprocessing pipeline and fixed downstream classifiers to ensure reproducible comparison. We extracted embeddings directly from pre-trained encoders, trained lightweight LightGBM classifiers on multiple disease labels, and reported mean AUROC, and F1-score with 95% confidence intervals. MedImageInsight achieved slightly higher performance across most tasks, while CXR-Foundation exhibited strong cross-dataset stability. Unsupervised clustering of MedImageIn-sight embeddings further revealed a coherent disease-specific structure consistent with quantitative results. The results highlight the need for standardised evaluation of medical foundation models and establish reproducible baselines for future multimodal and clinical integration studies.

</details>


### [87] [PrefGen: Multimodal Preference Learning for Preference-Conditioned Image Generation](https://arxiv.org/abs/2512.06020)
*Wenyi Mo,Tianyu Zhang,Yalong Bai,Ligong Han,Ying Ba,Dimitris N. Metaxas*

Main category: cs.CV

> 本文提出了一种新的框架，改进了现有个性化图像生成技术，通过将多模态大语言模型与图像生成模型相结合，更好地捕捉用户的个性化偏好。

<details>
  <summary>Details</summary>

**Motivation:** 尽管最近有了进展，现有的方法要么无法捕捉到细微的用户偏好，要么缺乏有效的机制来编码个性化的视觉信号。因此，本文旨在解决这些问题。

**Method:** 我们提出了一种多模态框架，利用多模态大语言模型（MLLMs）来提取丰富的用户表示并将其注入扩散基础的图像生成模型中。为了捕捉细粒度的语义线索，用偏好导向的视觉问答任务训练MLLM。为了分离偏好相关的特征，我们引入了两种互补的探测任务：跨用户辨别和用户内的偏好辨别。为了确保与扩散文本编码器的兼容性，我们设计了一个基于最大平均差异的对齐损失以弥合模态差距同时保留多模态结构。最后，这些嵌入被用来条件化生成器，使生成器能够忠实于提示文本和用户偏好。

**Result:** 广泛的实验表明，我们提出的方法在图像质量和偏好对齐上都明显优于强基线模型，验证了表示提取和对齐在个性化生成中的有效性。

**Conclusion:** 本文的方法表现出了在个性化图像生成任务中的优越性，能够准确捕捉和反映用户的偏好。

**Abstract:** Preference-conditioned image generation seeks to adapt generative models to individual users, producing outputs that reflect personal aesthetic choices beyond the given textual prompt. Despite recent progress, existing approaches either fail to capture nuanced user preferences or lack effective mechanisms to encode personalized visual signals. In this work, we propose a multimodal framework that leverages multimodal large language models (MLLMs) to extract rich user representations and inject them into diffusion-based image generation. We train the MLLM with a preference-oriented visual question answering task to capture fine-grained semantic cues. To isolate preference-relevant features, we introduce two complementary probing tasks: inter-user discrimination to distinguish between different users, and intra-user discrimination to separate liked from disliked content. To ensure compatibility with diffusion text encoders, we design a maximum mean discrepancy-based alignment loss that bridges the modality gap while preserving multimodal structure. The resulting embeddings are used to condition the generator, enabling faithful adherence to both prompts and user preferences. Extensive experiments demonstrate that our method substantially outperforms strong baselines in both image quality and preference alignment, highlighting the effectiveness of representation extraction and alignment for personalized generation.

</details>


### [88] [Neural reconstruction of 3D ocean wave hydrodynamics from camera sensing](https://arxiv.org/abs/2512.06024)
*Jiabin Liu,Zihao Zhou,Jialei Yan,Anxin Guo,Alvise Benetazzo,Hui Li*

Main category: cs.CV

> 该论文提出了一种专为海洋波浪多尺度和时间连续性特征设计的注意力增强金字塔结构的视觉重建神经网络，实现了高质量的三维波浪自由表面及速度场重建，表现出优越的性能和对遮挡状态的鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 精确的三维波浪自由表面及关联速度场重建对于全面理解海洋物理至关重要。然而，针对长期海洋波浪观测任务，密集视觉重建面临着高昂的计算成本和持续视觉遮挡带来的挑战。

**Method:** 该研究采用了一种基于物理学的约束条件，从不断演化的自由表面边界中进行随时间解析的非线性三维速度场重建。模型设计为神经网络，采用了注意力增强的金字塔结构，特别针对波浪运动的多尺度和时间连续性特征。

**Result:** 实验在真实海况下进行，结果表明该模型在中央区域的波浪高度预测精度达到毫米级别，主频率误差低于0.01 Hz，能够精确估计高频谱功率定律，并且实现了高保真的非线性三维速度场重建，同时能够在一分钟内完成两百万个点的密集重建。基于立体视觉数据集，该模型的表现优于传统的视觉重建方法，并且在被遮挡的条件下也表现出强大的泛化能力，这归功于其全局多尺度注意力机制及其对波浪传播动力学的编码能力。

**Conclusion:** 该研究提出了一种基于注意力增强的金字塔结构的波浪自由表面视觉重建神经网络，实现了精确的非线性三维自由表面波浪及速度场重建，同时还克服了长时海洋波浪观测中的计算成本高昂及持续视觉遮挡带来的挑战。

**Abstract:** Precise three-dimensional (3D) reconstruction of wave free surfaces and associated velocity fields is essential for developing a comprehensive understanding of ocean physics. To address the high computational cost of dense visual reconstruction in long-term ocean wave observation tasks and the challenges introduced by persistent visual occlusions, we propose an wave free surface visual reconstruction neural network, which is designed as an attention-augmented pyramid architecture tailored to the multi-scale and temporally continuous characteristics of wave motions. Using physics-based constraints, we perform time-resolved reconstruction of nonlinear 3D velocity fields from the evolving free-surface boundary. Experiments under real-sea conditions demonstrate millimetre-level wave elevation prediction in the central region, dominant-frequency errors below 0.01 Hz, precise estimation of high-frequency spectral power laws, and high-fidelity 3D reconstruction of nonlinear velocity fields, while enabling dense reconstruction of two million points in only 1.35 s. Built on a stereo-vision dataset, the model outperforms conventional visual reconstruction approaches and maintains strong generalization in occluded conditions, owing to its global multi-scale attention and its learned encoding of wave propagation dynamics.

</details>


### [89] [The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation](https://arxiv.org/abs/2512.06032)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.CV

> The paper analyzes the fundamental differences between SAM2 and SAM3, concluding that SAM3 is fundamentally different and introduces a new concept-driven approach to segmentation.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to highlight the discontinuity between SAM2 and SAM3 to understand why expertise and knowledge in SAM2 cannot be directly applied to SAM3, which takes a new approach to segment anything through multimodal concepts.

**Method:** This paper contrasts the prompt-based segmentation of SAM2 with the multimodal concept-driven paradigm of SAM3 by analyzing five components: the conceptual difference, architectural divergence, dataset and annotation differences, training and hyperparameter distinctions, and evaluation metrics.

**Result:** The result is a structured analysis showing significant differences between SAM2 and SAM3 in terms of segmentation approach, architecture, datasets, training methods, and evaluation metrics.

**Conclusion:** The conclusion is that SAM3 represents a new class of foundation models in segmentation, driven by multimodal concepts, and sets the stage for the future of concept-driven segmentation.

**Abstract:** This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3. We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAM3. SAM2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAM3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAM2 with multimodal fusion and text-conditioned mask generation of SAM3; (2) Architectural Divergence, detailing pure vision-temporal design of SAM2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAM3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAM3; (4) Training and Hyperparameter Distinctions, showing why SAM2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAM3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.

</details>


### [90] [Representation Learning for Point Cloud Understanding](https://arxiv.org/abs/2512.06058)
*Siming Yan*

Main category: cs.CV

> This dissertation develops methods for improving 3D data processing and understanding by integrating 2D knowledge, showing significant advancements through detailed experiments in point cloud representation learning.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to advance 3D understanding and representation learning by effectively integrating 2D knowledge and enhancing 3D data processing capabilities beyond merely transforming 2D data.

**Method:** The dissertation explores supervised representation learning for point cloud primitive segmentation, self-supervised learning methods, and the integration of pre-trained 2D models to support 3D network training.

**Result:** The results are demonstrated through extensive experiments that validate the effectiveness of the proposed methods in point cloud representation learning.

**Conclusion:** The conclusion is that integrating pre-trained 2D models into 3D network training significantly improves the understanding and processing of 3D data.

**Abstract:** With the rapid advancement of technology, 3D data acquisition and utilization have become increasingly prevalent across various fields, including computer vision, robotics, and geospatial analysis. 3D data, captured through methods such as 3D scanners, LiDARs, and RGB-D cameras, provides rich geometric, shape, and scale information. When combined with 2D images, 3D data offers machines a comprehensive understanding of their environment, benefiting applications like autonomous driving, robotics, remote sensing, and medical treatment. This dissertation focuses on three main areas: supervised representation learning for point cloud primitive segmentation, self-supervised learning methods, and transfer learning from 2D to 3D. Our approach, which integrates pre-trained 2D models to support 3D network training, significantly improves 3D understanding without merely transforming 2D data. Extensive experiments validate the effectiveness of our methods, showcasing their potential to advance point cloud representation learning by effectively integrating 2D knowledge.

</details>


### [91] [EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing](https://arxiv.org/abs/2512.06065)
*Runjia Li,Moayed Haji-Ali,Ashkan Mirzaei,Chaoyang Wang,Arpit Sahni,Ivan Skorokhodov,Aliaksandr Siarohin,Tomas Jakab,Junlin Han,Sergey Tulyakov,Philip Torr,Willi Menapace*

Main category: cs.CV

> 本文研究针对第一人称视角视频的人工智能编辑技术，并开发了一整套解决方案。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有方法在处理快速自我运动和频繁的手物互动的第一人称视角视频编辑时面临的主要缺点，如实时性和准确性。

**Method:** 提出了一套用于第一人称视角视频编辑的生态系统，包括EgoEditData数据集、EgoEdit实时视频编辑器和EgoEditBench评估系统。

**Result:** EgoEdit在一人称视角编辑任务上表现优异，同时与最强基线方法在通用编辑任务上保持相当水平。

**Conclusion:** EgoEdit展示出在处理第一人称视角视频编辑时的优越性能，其方法可以支持实时流式推理，适用于交互式增强现实应用。

**Abstract:** We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit

</details>


### [92] [Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light](https://arxiv.org/abs/2512.06080)
*Tzofi Klinghoffer,Siddharth Somasundaram,Xiaoyu Xiang,Yuchen Fan,Christian Richardt,Akshat Dave,Ramesh Raskar,Rakesh Ranjan*

Main category: cs.CV

> 论文提出了一种基于数据驱动的方法，使用大规模模拟数据集来解决单光子雷达在复杂室内场景中的3D场景重建问题，能够处理遮挡区域和镜面材料，从单次测量中推断3D几何结构。

<details>
  <summary>Details</summary>

**Motivation:** 传统的单光子雷达仅在激光依次照射单个场景点时展示其应用场景，而我们在更实用但更具挑战性的场景下（同时照亮多个场景点），解决了由多路照射、二次反射光、阴影和镜面反射等产生的复杂光线传输问题。

**Method:** 我们提出了一种基于数据驱动的方法来解析单光子雷达中的复杂光线传输。为此，我们创建了首个大规模的室内场景单光子雷达瞬态数据集，包含约10万条数据。通过这个数据集，我们学习到了复杂光线传输的先验知识，从而可以将测量到的二次反射光分解为每个激光点的独立贡献。

**Result:** 实验表明，我们可以通过分解的光信息从单次测量中推断出具有遮挡和镜面的场景的3D几何结构，证明了数据驱动方法的有效性。

**Conclusion:** 我们展示了使用大规模模拟数据集和数据驱动方法来解析复杂光线传输问题的潜力，实现了从单次测量中推断室内复杂场景（包括遮挡和镜面）的3D几何结构的目标。

**Abstract:** 3D scene reconstruction from a single measurement is challenging, especially in the presence of occluded regions and specular materials, such as mirrors. We address these challenges by leveraging single-photon lidars. These lidars estimate depth from light that is emitted into the scene and reflected directly back to the sensor. However, they can also measure light that bounces multiple times in the scene before reaching the sensor. This multi-bounce light contains additional information that can be used to recover dense depth, occluded geometry, and material properties. Prior work with single-photon lidar, however, has only demonstrated these use cases when a laser sequentially illuminates one scene point at a time. We instead focus on the more practical - and challenging - scenario of illuminating multiple scene points simultaneously. The complexity of light transport due to the combined effects of multiplexed illumination, two-bounce light, shadows, and specular reflections is challenging to invert analytically. Instead, we propose a data-driven method to invert light transport in single-photon lidar. To enable this approach, we create the first large-scale simulated dataset of ~100k lidar transients for indoor scenes. We use this dataset to learn a prior on complex light transport, enabling measured two-bounce light to be decomposed into the constituent contributions from each laser spot. Finally, we experimentally demonstrate how this decomposed light can be used to infer 3D geometry in scenes with occlusions and mirrors from a single measurement. Our code and dataset are released at https://shoot-bounce-3d.github.io.

</details>


### [93] [BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving](https://arxiv.org/abs/2512.06096)
*Karthik Mohan,Sonam Singh,Amit Arvind Kale*

Main category: cs.CV

> BeLLA 是一种连接统一 360°顶视图表示与大型语言模型的端到端架构，用于自动驾驶中的问题回答，显著提高了需要更复杂空间推理的任务表现。

<details>
  <summary>Details</summary>

**Motivation:** 本研究针对现有方法在处理多摄像头系统时存在的空间结构利用不足或缺乏统一空间表示的问题，提出了一种新的架构来改善自动驾驶场景理解、上下文感知推理和决策解释。

**Method:** BeLLA 使用统一的 360°鸟瞰图表示，并与大型语言模型结合，实现了端到端的架构，用于自动驾驶领域的问题解答。

**Result:** 在 NuScenes-QA 和 DriveLM 两个基准测试中，BeLLA 在需要更复杂空间推理的问题上表现优于现有方法，某些任务中的绝对改善达到了+9.3%。

**Conclusion:** BeLLA 证明了其在处理需要空间推理的问题上的优越性，同时也能处理多样化的其他问题，提升了自动驾驶决策的可解释性。

**Abstract:** The rapid development of Vision-Language models (VLMs) and Multimodal Language Models (MLLMs) in autonomous driving research has significantly reshaped the landscape by enabling richer scene understanding, context-aware reasoning, and more interpretable decision-making. However, a lot of existing work often relies on either single-view encoders that fail to exploit the spatial structure of multi-camera systems or operate on aggregated multi-view features, which lack a unified spatial representation, making it more challenging to reason about ego-centric directions, object relations, and the wider context. We thus present BeLLA, an end-to-end architecture that connects unified 360° BEV representations with a large language model for question answering in autonomous driving. We primarily evaluate our work using two benchmarks - NuScenes-QA and DriveLM, where BeLLA consistently outperforms existing approaches on questions that require greater spatial reasoning, such as those involving relative object positioning and behavioral understanding of nearby objects, achieving up to +9.3% absolute improvement in certain tasks. In other categories, BeLLA performs competitively, demonstrating the capability of handling a diverse range of questions.

</details>


### [94] [SpectraIrisPAD: Leveraging Vision Foundation Models for Spectrally Conditioned Multispectral Iris Presentation Attack Detection](https://arxiv.org/abs/2512.06103)
*Raghavendra Ramachandra,Sushma Venkatesh*

Main category: cs.CV

> 本文提出了SpectraIrisPAD框架，利用多光谱成像和深度学习技术来提高虹膜识别系统的PAD性能，并引入了一个全面的新数据集MSIrPAD来测试该框架的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 虹膜识别被广泛认为是最准确的生物识别方式之一。然而，其在现实世界应用中的部署越来越多地引发对其易受表现攻击(PAs)的影响的担忧。因此，有效的表现攻击检测(PAD)对于确保基于虹膜的生物识别系统的完整性和安全性至关重要。

**Method:** 提出了一种名为SpectraIrisPAD的新型基于深度学习的框架，用于鲁棒的多光谱虹膜PAD。该框架采用配备了可学习的光谱位置编码、令牌融合和对比学习的DINOv2视觉转换器(ViT)骨干网，以提取区分性的波长特定特征，能够有效地将真实样本与各种欺骗手工艺品区分开来。

**Result:** 通过全面的实验，特别是在未见过的攻击评估协议下，该方法展示出了在检测各种表现攻击方面，超越多个最新基线的强大鲁棒性和通用性。

**Conclusion:** SpectraIrisPAD框架在多光谱虹膜PAD方面表现出了优秀的鲁棒性和通用性，超越了多个最新基线，在所有性能指标上都表现出色。

**Abstract:** Iris recognition is widely recognized as one of the most accurate biometric modalities. However, its growing deployment in real-world applications raises significant concerns regarding its vulnerability to Presentation Attacks (PAs). Effective Presentation Attack Detection (PAD) is therefore critical to ensure the integrity and security of iris-based biometric systems. While conventional iris recognition systems predominantly operate in the near-infrared (NIR) spectrum, multispectral imaging across multiple NIR bands provides complementary reflectance information that can enhance the generalizability of PAD methods. In this work, we propose \textbf{SpectraIrisPAD}, a novel deep learning-based framework for robust multispectral iris PAD. The SpectraIrisPAD leverages a DINOv2 Vision Transformer (ViT) backbone equipped with learnable spectral positional encoding, token fusion, and contrastive learning to extract discriminative, band-specific features that effectively distinguish bona fide samples from various spoofing artifacts. Furthermore, we introduce a new comprehensive dataset Multispectral Iris PAD (\textbf{MSIrPAD}) with diverse PAIs, captured using a custom-designed multispectral iris sensor operating at five distinct NIR wavelengths (800\,nm, 830\,nm, 850\,nm, 870\,nm, and 980\,nm). The dataset includes 18,848 iris images encompassing eight diverse PAI categories, including five textured contact lenses, print attacks, and display-based attacks. We conduct comprehensive experiments under unseen attack evaluation protocols to assess the generalization capability of the proposed method. SpectraIrisPAD consistently outperforms several state-of-the-art baselines across all performance metrics, demonstrating superior robustness and generalizability in detecting a wide range of presentation attacks.

</details>


### [95] [Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report Generation](https://arxiv.org/abs/2512.06105)
*Junwen Zheng,Xinran Xu,Li Rong Wang,Chang Cai,Lucinda Siyun Tan,Dingyuan Wang,Hong Liang Tey,Xiuyi Fan*

Main category: cs.CV

> 文章提出了跨模态可解释框架CEFM，提高了深度学习在黑色素瘤分类中临床解释性和信任度，同时保持了高性能的分类结果。

<details>
  <summary>Details</summary>

**Motivation:** 深度学习在黑色素瘤分类中展示了专家级的表现，使其成为临床皮肤病学中的强大工具。然而，模型的不透明性和缺乏可解释性仍然是临床采用的关键障碍，因为临床医生常常难以信任黑盒模型的决策过程。为了解决这一问题，我们提出了CEFM。

**Method:** 提出了一种名为CEFM的跨模态可解释框架，该框架利用对比学习作为实现可解释性的核心机制。具体来说，CEFM使用双投影头将黑色素瘤诊断的临床标准（即不对称性、边缘和颜色（ABC））映射到视觉变压器嵌入空间中，从而将临床语义与视觉特征对齐。随后，通过自然语言生成将对齐的表示转换为结构化的文本解释，创建了从原始图像数据到临床解释的透明链接。

**Result:** 在公共数据集上的实验表明，该方法达到了92.79%的准确率和0.961的AUC，并在多个可解释性指标上取得了显著的改进。定性分析还表明，学习到的嵌入的空间排列与临床医生应用的ABC规则相一致，有效地弥合了高性能分类与临床信任之间的差距。

**Conclusion:** 研究表明，通过对比学习和自然语言生成技术，CEFM不仅可以实现高精度的黑色素瘤分类，还能提供可解释的临床解释，这对提高临床医生对深度学习模型的信任度至关重要。

**Abstract:** Deep learning has demonstrated expert-level performance in melanoma classification, positioning it as a powerful tool in clinical dermatology. However, model opacity and the lack of interpretability remain critical barriers to clinical adoption, as clinicians often struggle to trust the decision-making processes of black-box models. To address this gap, we present a Cross-modal Explainable Framework for Melanoma (CEFM) that leverages contrastive learning as the core mechanism for achieving interpretability. Specifically, CEFM maps clinical criteria for melanoma diagnosis-namely Asymmetry, Border, and Color (ABC)-into the Vision Transformer embedding space using dual projection heads, thereby aligning clinical semantics with visual features. The aligned representations are subsequently translated into structured textual explanations via natural language generation, creating a transparent link between raw image data and clinical interpretation. Experiments on public datasets demonstrate 92.79% accuracy and an AUC of 0.961, along with significant improvements across multiple interpretability metrics. Qualitative analyses further show that the spatial arrangement of the learned embeddings aligns with clinicians' application of the ABC rule, effectively bridging the gap between high-performance classification and clinical trust.

</details>


### [96] [Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation](https://arxiv.org/abs/2512.06158)
*Su Sun,Cheng Zhao,Himangi Mittal,Gaurav Mittal,Rohith Kukkala,Yingjie Victor Chen,Mei Chen*

Main category: cs.CV

> The paper introduces Track4DGen, a two-stage framework for generating dynamic 4D objects from sparse inputs, which uses motion tracking and hybrid reconstruction to improve temporal consistency and cross-view coherence, while also presenting a new benchmark dataset, Sketchfab28, for 4D object generation research.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind Track4DGen is to address the challenges in generating dynamic 4D objects from sparse inputs by ensuring joint preservation of appearance and motion coherence while mitigating artifacts and temporal drift, which current approaches struggle with.

**Method:** Track4DGen consists of two stages: Stage One uses a multi-view video diffusion model with explicit motion tracking to ensure temporally consistent features. Stage Two reconstructs a dynamic 4D object using a hybrid motion encoding that combines diffusion features with Hex-plane features and 4D Spherical Harmonics for enhanced dynamics.

**Result:** Track4DGen outperforms baseline methods in multi-view video and 4D object generation tasks, delivering more stable and high-fidelity dynamic 4D assets.

**Conclusion:** Track4DGen significantly enhances the quality and stability of 4D object generation from sparse inputs, demonstrating the importance of incorporating motion tracking at the feature level. Additionally, the creation of Sketchfab28 advances the field by providing a new dataset for benchmarking and future research.

**Abstract:** Generating dynamic 4D objects from sparse inputs is difficult because it demands joint preservation of appearance and motion coherence across views and time while suppressing artifacts and temporal drift. We hypothesize that the view discrepancy arises from supervision limited to pixel- or latent-space video-diffusion losses, which lack explicitly temporally aware, feature-level tracking guidance. We present \emph{Track4DGen}, a two-stage framework that couples a multi-view video diffusion model with a foundation point tracker and a hybrid 4D Gaussian Splatting (4D-GS) reconstructor. The central idea is to explicitly inject tracker-derived motion priors into intermediate feature representations for both multi-view video generation and 4D-GS. In Stage One, we enforce dense, feature-level point correspondences inside the diffusion generator, producing temporally consistent features that curb appearance drift and enhance cross-view coherence. In Stage Two, we reconstruct a dynamic 4D-GS using a hybrid motion encoding that concatenates co-located diffusion features (carrying Stage-One tracking priors) with Hex-plane features, and augment them with 4D Spherical Harmonics for higher-fidelity dynamics modeling. \emph{Track4DGen} surpasses baselines on both multi-view video generation and 4D generation benchmarks, yielding temporally stable, text-editable 4D assets. Lastly, we curate \emph{Sketchfab28}, a high-quality dataset for benchmarking object-centric 4D generation and fostering future research.

</details>


### [97] [Automated Annotation of Shearographic Measurements Enabling Weakly Supervised Defect Detection](https://arxiv.org/abs/2512.06171)
*Jessica Plassmann,Nicolas Schuler,Michael Schuth,Georg von Freymann*

Main category: cs.CV

> 本文介绍了一种新的自动化工作流，利用深度学习来生成剪切波测量中的缺陷注释，从而减少手动标注的工作量。

<details>
  <summary>Details</summary>

**Motivation:** 剪切波技术在检测关键安全组件的亚表面缺陷方面具有高灵敏度，但缺乏高质量的标注数据阻碍了其在工业上的应用。手动标注的代价高昂且难以标准化。

**Method:** 我们提出了一种利用深度学习从剪切波测量中生成缺陷注释的自动化工作流，可以生成高分辨率的分割和边界框标签。

**Result:** 与专家标注的数据相比，该方法表现出足够的准确性，可以支持弱监督训练，减少手动工作量和促进大规模数据集的创建。

**Conclusion:** 我们提出的方法可以有效地生成标注数据，有助于解决工业界在剪切波技术应用中遇到的数据标注难题。

**Abstract:** Shearography is an interferometric technique sensitive to surface displacement gradients, providing high sensitivity for detecting subsurface defects in safety-critical components. A key limitation to industrial adoption is the lack of high-quality annotated datasets, since manual labeling remains labor-intensive, subjective, and difficult to standardize. We introduce an automated workflow that generates defect annotations from shearography measurements using deep learning, producing high-resolution segmentation and bounding-box labels. Evaluation against expert-labeled data demonstrates sufficient accuracy to enable weakly supervised training, reducing manual effort and supporting scalable dataset creation for robust defect detection.

</details>


### [98] [Physics-Grounded Shadow Generation from Monocular 3D Geometry Priors and Approximate Light Direction](https://arxiv.org/abs/2512.06174)
*Shilin Hu,Jingyi Xu,Akshat Dave,Dimitris Samaras,Hieu Le*

Main category: cs.CV

> 本文提出了一个基于物理模型和深度学习的阴影生成框架，在保持物理一致性的同时生成视觉上真实的阴影，特别在复杂的几何结构和光照环境下优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的深度学习阴影生成方法很少使用显式物理建模，即几何结构和光线照射。这篇文章旨在通过嵌入显式物理模型来提高阴影生成的质量，使其更加真实且与场景的一致性更高。

**Method:** 该方法首先根据单目RGB图像估计3D几何结构和主要光线方向，利用这些信息基于物理过程计算阴影的位置和形状，然后将这个物理估计整合进扩散框架进行精细调整，生成高保真的阴影效果。

**Result:** 

**Conclusion:** 实验结果表明，该模型生成的阴影在DESOBAV2数据集上表现出视觉上真实且物理上一致的特点，尤其在处理复杂几何和光照条件时表现突出。

**Abstract:** Shadow generation aims to produce photorealistic shadows that are visually consistent with object geometry and scene illumination. In the physics of shadow formation, the occluder blocks some light rays casting from the light source that would otherwise arrive at the surface, creating a shadow that follows the silhouette of the occluder. However, such explicit physical modeling has rarely been used in deep-learning-based shadow generation. In this paper, we propose a novel framework that embeds explicit physical modeling - geometry and illumination - into deep-learning-based shadow generation. First, given a monocular RGB image, we obtain approximate 3D geometry in the form of dense point maps and predict a single dominant light direction. These signals allow us to recover fairly accurate shadow location and shape based on the physics of shadow formation. We then integrate this physics-based initial estimate into a diffusion framework that refines the shadow into a realistic, high-fidelity appearance while ensuring consistency with scene geometry and illumination. Trained on DESOBAV2, our model produces shadows that are both visually realistic and physically coherent, outperforming existing approaches, especially in scenes with complex geometry or ambiguous lighting.

</details>


### [99] [Physics-Grounded Attached Shadow Detection Using Approximate 3D Geometry and Light Direction](https://arxiv.org/abs/2512.06179)
*Shilin Hu,Jingyi Xu,Sagnik Das,Dimitris Samaras,Hieu Le*

Main category: cs.CV

> A new framework is introduced for detecting both cast and attached shadows by jointly reasoning about shadows, light direction, and object geometry, leading to improved accuracy in attached shadow detection.

<details>
  <summary>Details</summary>

**Motivation:** The main motivation is to address the lack of dedicated approaches for detecting attached shadows, an important aspect for defining the 3D structure of objects, by developing a framework that reasons about shadows, illumination, and geometry in a joint manner.

**Method:** Our system consists of a shadow detection module that predicts both cast and attached shadows, and a light estimation module that infers the light direction from the detected shadows. This allows the derivation of a geometry-consistent partial map identifying self-occluded regions. This map refines shadow predictions iteratively.

**Result:** Experiments show a significant 33% BER reduction in detecting attached shadows, alongside maintaining strong performance in detecting full and cast shadows.

**Conclusion:** The iterative reasoning process about geometry and illumination in the proposed framework leads to improved accuracy in shadow detection.

**Abstract:** Attached shadows occur on the surface of the occluder where light cannot reach because of self-occlusion. They are crucial for defining the three-dimensional structure of objects and enhancing scene understanding. Yet existing shadow detection methods mainly target cast shadows, and there are no dedicated datasets or models for detecting attached shadows. To address this gap, we introduce a framework that jointly detects cast and attached shadows by reasoning about their mutual relationship with scene illumination and geometry. Our system consists of a shadow detection module that predicts both shadow types separately, and a light estimation module that infers the light direction from the detected shadows. The estimated light direction, combined with surface normals, allows us to derive a geometry-consistent partial map that identifies regions likely to be self-occluded. This partial map is then fed back to refine shadow predictions, forming a closed-loop reasoning process that iteratively improves both shadow segmentation and light estimation. In order to train our method, we have constructed a dataset of 1,458 images with separate annotations for cast and attached shadows, enabling training and quantitative evaluation of both. Experimental results demonstrate that this iterative geometry-illumination reasoning substantially improves the detection of attached shadows, with at least 33% BER reduction, while maintaining strong full and cast shadow performance.

</details>


### [100] [SPOOF: Simple Pixel Operations for Out-of-Distribution Fooling](https://arxiv.org/abs/2512.06185)
*Ankit Gupta,Christoph Adami,Emily Dolson*

Main category: cs.CV

> 论文通过重新实现并测试各种针对现代深度分类器的欺骗攻击方法，揭示了当前模型对非自然图像输入的脆弱性，并提出了一种更高效的攻击方法SPOOF。

<details>
  <summary>Details</summary>

**Motivation:** 现代深度神经网络在图像识别任务中表现出色，但在一些与自然图像不相似的输入上仍然表现出过高的置信度。论文旨在分析现代深度分类器对欺骗性图像的脆弱性。

**Method:** 该论文重新实现了Nguyen等人提出的基于CPPN和直接编码的进化欺骗攻击，并在其上针对现代卷积和变压器分类器进行实验。此外，论文还引入了一种称为SPOOF的黑盒攻击方法，该方法简单、高效、一致。

**Result:** 研究发现，即使是在最先进的网络中，基于变压器的ViT-B/16容易受到欺骗攻击。SPOOF能够通过极少的像素修改生成高度置信的欺骗图像，并且重新训练这些模型加入欺骗图像作为额外类别只能提供部分防护。

**Conclusion:** 尽管进行了防守措施，现代深度分类器仍然易受欺骗攻击，并且这种脆弱性在未来的工作中需要进一步研究和解决。

**Abstract:** Deep neural networks (DNNs) excel across image recognition tasks, yet continue to exhibit overconfidence on inputs that bear no resemblance to natural images. Revisiting the "fooling images" work introduced by Nguyen et al. (2015), we re-implement both CPPN-based and direct-encoding-based evolutionary fooling attacks on modern architectures, including convolutional and transformer classifiers. Our re-implementation confirm that high-confidence fooling persists even in state-of-the-art networks, with transformer-based ViT-B/16 emerging as the most susceptible--achieving near-certain misclassifications with substantially fewer queries than convolution-based models. We then introduce SPOOF, a minimalist, consistent, and more efficient black-box attack generating high-confidence fooling images. Despite its simplicity, SPOOF generates unrecognizable fooling images with minimal pixel modifications and drastically reduced compute. Furthermore, retraining with fooling images as an additional class provides only partial resistance, as SPOOF continues to fool consistently with slightly higher query budgets--highlighting persistent fragility of modern deep classifiers.

</details>


### [101] [Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying](https://arxiv.org/abs/2512.06190)
*Shichen Li,Ahmadreza Eslaminia,Chenhui Shao*

Main category: cs.CV

> 本文提出了一种新的多模态颜色轨迹预测方法，能够准确高效地预测食品干燥过程中的颜色变化。

<details>
  <summary>Details</summary>

**Motivation:** 现有的研究主要依靠低维度的颜色特征来分析食品干燥过程中的颜色变化，并且这些方法缺乏在未知干燥条件下泛化的能力。

**Method:** 提出了一种新的多模态颜色轨迹预测方法，结合高维度的时间颜色信息和干燥过程参数。

**Result:** 模型在未知干燥条件下，对饼干和苹果干燥的颜色变化预测RMSE分别为2.12和1.29，相比基线模型降低了超过90%的误差。

**Conclusion:** 该模型展示了其在颜色变化预测方面的优越准确性、稳健性和广泛适用性。

**Abstract:** Food drying is widely used to reduce moisture content, ensure safety, and extend shelf life. Color evolution of food samples is an important indicator of product quality in food drying. Although existing studies have examined color changes under different drying conditions, current approaches primarily rely on low-dimensional color features and cannot fully capture the complex, dynamic color trajectories of food samples. Moreover, existing modeling approaches lack the ability to generalize to unseen process conditions. To address these limitations, we develop a novel multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters to enable accurate and data-efficient color trajectory prediction. Under unseen drying conditions, the model attains RMSEs of 2.12 for cookie drying and 1.29 for apple drying, reducing errors by over 90% compared with baseline models. These experimental results demonstrate the model's superior accuracy, robustness, and broad applicability.

</details>


### [102] [The MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024: Efficient and Robust Aggregation Methods for Federated Learning](https://arxiv.org/abs/2512.06206)
*Akis Linardos,Sarthak Pati,Ujjwal Baid,Brandon Edwards,Patrick Foley,Kevin Ta,Verena Chung,Micah Sheller,Muhammad Irfan Khan,Mojtaba Jafaritadi,Elina Kontio,Suleiman Khan,Leon Mächler,Ivan Ezhov,Suprosanna Shit,Johannes C. Paetzold,Gustav Grimberg,Manuel A. Nickel,David Naccache,Vasilis Siomos,Jonathan Passerat-Palmbach,Giacomo Tarroni,Daewoon Kim,Leonard L. Klausmann,Prashant Shah,Bjoern Menze,Dimitrios Makris,Spyridon Bakas*

Main category: cs.CV

> FeTS 2024挑战赛聚焦于胶质瘤分割的联邦学习方法及权重聚合优化，参赛团队中PID控制器方法表现最优。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于通过标准化的联邦学习框架和多机构数据集评估新权重聚合方法在胶质瘤分割上的鲁棒性和效率。

**Method:** 本次挑战赛基于联邦学习（FL）方法，旨在提升多参数MRI中胶质瘤亚区域分割的鲁棒性和效率，通过PID控制器优化权重聚合方法。

**Result:** 最终，采用PID控制器的方法获得最高综合评分，对于增强肿瘤（ET）、肿瘤核心（TC）和整个肿瘤（WT）的平均Dice相似系数（DSC）分别为0.733、0.761和0.751，相应的95% Hausdorff距离（HD95）分别为33.922 mm、33.623 mm和32.309 mm，并且在通信效率方面得分0.764。

**Conclusion:** 结果表明PID控制器是一种有效的优化权重聚合机制，在提升医疗图像中的联邦学习状态上取得了显著进展。

**Abstract:** We present the design and results of the MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024, which focuses on federated learning (FL) for glioma sub-region segmentation in multi-parametric MRI and evaluates new weight aggregation methods aimed at improving robustness and efficiency. Six participating teams were evaluated using a standardized FL setup and a multi-institutional dataset derived from the BraTS glioma benchmark, consisting of 1,251 training cases, 219 validation cases, and 570 hidden test cases with segmentations for enhancing tumor (ET), tumor core (TC), and whole tumor (WT). Teams were ranked using a cumulative scoring system that considered both segmentation performance, measured by Dice Similarity Coefficient (DSC) and the 95th percentile Hausdorff Distance (HD95), and communication efficiency assessed through the convergence score. A PID-controller-based method achieved the top overall ranking, obtaining mean DSC values of 0.733, 0.761, and 0.751 for ET, TC, and WT, respectively, with corresponding HD95 values of 33.922 mm, 33.623 mm, and 32.309 mm, while also demonstrating the highest communication efficiency with a convergence score of 0.764. These findings advance the state of federated learning for medical imaging, surpassing top-performing methods from previous challenge iterations and highlighting PID controllers as effective mechanisms for stabilizing and optimizing weight aggregation in FL. The challenge code is available at https://github.com/FeTS-AI/Challenge.

</details>


### [103] [Revisiting SVD and Wavelet Difference Reduction for Lossy Image Compression: A Reproducibility Study](https://arxiv.org/abs/2512.06221)
*Alena Makarova*

Main category: cs.CV

> A reproducibility study of an SVD+WDR image compression method shows it does not surpass JPEG2000 as originally claimed, and discusses reproducibility issues due to unclear documentation.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to verify the reproducibility and performance claims made by the original paper about the SVD+WDR image compression technique.

**Method:** This study re-implements an image compression method combining SVD and WDR, and conducts experiments to evaluate its performance compared to JPEG2000 and WDR.

**Result:** The study found that the SVD+WDR technique does not outperform JPEG2000 in PSNR and only has a limited improvement over JPEG2000 in SSIM.

**Conclusion:** The study emphasizes issues with reproducibility due to ambiguities in the original paper's description and highlights the importance of clear documentation for implementation details such as quantization and threshold initialization.

**Abstract:** This work presents an independent reproducibility study of a lossy image compression technique that integrates singular value decomposition (SVD) and wavelet difference reduction (WDR). The original paper claims that combining SVD and WDR yields better visual quality and higher compression ratios than JPEG2000 and standalone WDR. I re-implemented the proposed method, carefully examined missing implementation details, and replicated the original experiments as closely as possible. I then conducted additional experiments on new images and evaluated performance using PSNR and SSIM. In contrast to the original claims, my results indicate that the SVD+WDR technique generally does not surpass JPEG2000 or WDR in terms of PSNR, and only partially improves SSIM relative to JPEG2000. The study highlights ambiguities in the original description (e.g., quantization and threshold initialization) and illustrates how such gaps can significantly impact reproducibility and reported performance.

</details>


### [104] [GPU-GLMB: Assessing the Scalability of GPU-Accelerated Multi-Hypothesis Tracking](https://arxiv.org/abs/2512.06230)
*Pranav Balakrishnan,Sidisha Barik,Sean M. O'Rourke,Benjamin M. Marlin*

Main category: cs.CV

> 研究了一种GLMB滤波器的变体，允许单一传感器对每个对象进行多次检测，改进了并行更新能力，提高了在GPU上的部署效率。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于解决多假设方法在标准测量模型下高度计算密集的问题，即使应用了假设剪枝近似方法。由于多目标贝叶斯滤波的封闭解，维持多重假设变得高度计算昂贵。提出的变体目标是在分布式网络机器学习虚拟传感器环境中实现有效跟踪。

**Method:** 本文研究了一种GLMB滤波器的变体，允许来自同一传感器的每个对象有多次检测。这种变体打破了标准GLMB滤波器中检测间的依赖关系，使得更新具有显著的并行可扩展性，并能够利用GPU硬件进行高效部署。

**Result:** 报告了其在GPU加速实现中的初步分析结果，特别是在随着目标数量和最大保留假设数量的变化，运行时间可扩展性的几个方面。

**Conclusion:** 通过引入新方法，达到了显著提升并行计算能力的效果，并证明了其在现代硬件上高效部署的潜力。

**Abstract:** Much recent research on multi-target tracking has focused on multi-hypothesis approaches leveraging random finite sets. Of particular interest are labeled random finite set methods that maintain temporally coherent labels for each object. While these methods enjoy important theoretical properties as closed-form solutions to the multi-target Bayes filter, the maintenance of multiple hypotheses under the standard measurement model is highly computationally expensive, even when hypothesis pruning approximations are applied. In this work, we focus on the Generalized Labeled Multi-Bernoulli (GLMB) filter as an example of this class of methods. We investigate a variant of the filter that allows multiple detections per object from the same sensor, a critical capability when deploying tracking in the context of distributed networks of machine learning-based virtual sensors. We show that this breaks the inter-detection dependencies in the filter updates of the standard GLMB filter, allowing updates with significantly improved parallel scalability and enabling efficient deployment on GPU hardware. We report the results of a preliminary analysis of a GPU-accelerated implementation of our proposed GLMB tracker, with a focus on run time scalability with respect to the number of objects and the maximum number of retained hypotheses.

</details>


### [105] [Opinion: Learning Intuitive Physics May Require More than Visual Data](https://arxiv.org/abs/2512.06232)
*Ellen Su,Solim Legris,Todd M. Gureckis,Mengye Ren*

Main category: cs.CV

> Training a deep learning model on a small, developmentally realistic dataset does not lead to significant improvements in intuitive physics benchmarks, suggesting that more than just data volume and distribution is needed.

<details>
  <summary>Details</summary>

**Motivation:** To investigate if the distribution of training data, rather than the volume of data, holds the key to improving deep learning models' performance in understanding intuitive physics.

**Method:** We pretrain a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam, a developmentally realistic, egocentric video dataset that captures the visual experiences of children.

**Result:** Training on this dataset, which represents only 0.01% of the data volume used to train state-of-the-art models, does not significantly improve performance on the IntPhys2 benchmark.

**Conclusion:** Simply varying the visual data volume and distribution, even to match human developmental patterns, may not be enough for current models to learn representations that support intuitive physics accurately.

**Abstract:** Humans expertly navigate the world by building rich internal models founded on an intuitive understanding of physics. Meanwhile, despite training on vast quantities of internet video data, state-of-the-art deep learning models still fall short of human-level performance on intuitive physics benchmarks. This work investigates whether data distribution, rather than volume, is the key to learning these principles. We pretrain a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam, a developmentally realistic, egocentric video dataset partially capturing three children's everyday visual experiences. We find that training on this dataset, which represents 0.01% of the data volume used to train SOTA models, does not lead to significant performance improvements on the IntPhys2 benchmark. Our results suggest that merely training on a developmentally realistic dataset is insufficient for current architectures to learn representations that support intuitive physics. We conclude that varying visual data volume and distribution alone may not be sufficient for building systems with artificial intuitive physics.

</details>


### [106] [NexusFlow: Unifying Disparate Tasks under Partial Supervision via Invertible Flow Networks](https://arxiv.org/abs/2512.06251)
*Fangzhou Lin,Yuping Wang,Yuliang Guo,Zixun Huang,Xinyu Huang,Haichong Zhang,Kazunori Yamada,Zhengzhong Tu,Liu Ren,Ziming Zhang*

Main category: cs.CV

> 研究提出了NexusFlow，一种新型的部分监督多任务学习框架，能够有效地处理结构多样化任务，通过可逆耦合层对齐潜在特征分布实现知识转移，并在自动驾驶和图像分割任务上展现出优越性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法主要集中在结构相同和密集预测任务的简单场景，而忽略了结构多样化任务的学习挑战。因此，提出NexusFlow来解决这一更具现实性的挑战，特别是在注释不完整的情况下跨任务利用知识。

**Method:** NexusFlow采用一套具有可逆耦合层的替代网络来对齐任务的潜在特征分布，从而创建一个统一的表示形式，实现有效的知识转移。可逆耦合层是双射的，能保存信息同时将特征映射到共享的公共空间。这种可逆性避免了表达能力的下降，并允许在结构不同的任务之间进行对齐而不会减少其表达能力。

**Result:** 在核心挑战领域划分的自动驾驶场景中，NexusFlow在nuScenes数据集上取得了新的最佳结果，超越了强大的部分监督基线模型。进一步在NYUv2数据集上测试，使用三个同质密集预测任务（分割、深度和表面法线）作为代表性的N任务PS-MTL场景，NexusFlow在所有任务上都取得了持续的性能提升。

**Conclusion:** NexusFlow是一个新颖且轻量级的框架，能有效地在结构多样化和同质密集预测任务上实现部分监督多任务学习。实验结果证实了其在不同场景中的广泛适用性和性能提升。

**Abstract:** Partially Supervised Multi-Task Learning (PS-MTL) aims to leverage knowledge across tasks when annotations are incomplete. Existing approaches, however, have largely focused on the simpler setting of homogeneous, dense prediction tasks, leaving the more realistic challenge of learning from structurally diverse tasks unexplored. To this end, we introduce NexusFlow, a novel, lightweight, and plug-and-play framework effective in both settings. NexusFlow introduces a set of surrogate networks with invertible coupling layers to align the latent feature distributions of tasks, creating a unified representation that enables effective knowledge transfer. The coupling layers are bijective, preserving information while mapping features into a shared canonical space. This invertibility avoids representational collapse and enables alignment across structurally different tasks without reducing expressive capacity. We first evaluate NexusFlow on the core challenge of domain-partitioned autonomous driving, where dense map reconstruction and sparse multi-object tracking are supervised in different geographic regions, creating both structural disparity and a strong domain gap. NexusFlow sets a new state-of-the-art result on nuScenes, outperforming strong partially supervised baselines. To demonstrate generality, we further test NexusFlow on NYUv2 using three homogeneous dense prediction tasks, segmentation, depth, and surface normals, as a representative N-task PS-MTL scenario. NexusFlow yields consistent gains across all tasks, confirming its broad applicability.

</details>


### [107] [Language-driven Fine-grained Retrieval](https://arxiv.org/abs/2512.06255)
*Shijie Wang,Xin Yu,Yadan Luo,Zijian Wang,Pengfei Zhang,Zi Huang*

Main category: cs.CV

> 论文引入了LaFG框架，通过借助大规模语言模型和视觉-语言模型将类别名称转化为基于属性的监督，以改进细粒度图像检索方法的泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 现有的细粒度图像检索方法采用了稀疏的一对一标签作为监督，忽视了类别名称中的丰富语义，从而限制了跨类别详细信息的可比性和对未见类别的泛化能力。因此，提出了LaFG来解决上述问题。

**Method:** 该论文引入了一个基于语言驱动的细粒度检索框架LaFG，它利用大规模语言模型（LLMs）和视觉-语言模型（VLMs）将类别名称转换为基于属性的监督。通过将类别名称作为语义锚点，LaFG利用LLMs生成详细的、基于属性的描述，并使用冻结的VLM将这些描述投影到视觉对齐的空间中，从而形成一个跨类别的属性词汇表。这些属性随后被集成成类别的语言原型，用于监督检索模型的训练。

**Result:** 具体的结果未在摘要中明确给出，但表明了新的框架通过属性级监督提升了细粒度图像检索的效果。

**Conclusion:** LaFG框架通过利用类别名称的丰富语义信息，为细粒度图像检索提供了更加丰富的属性级别监督，有望提升检索结果的质量。

**Abstract:** Existing fine-grained image retrieval (FGIR) methods learn discriminative embeddings by adopting semantically sparse one-hot labels derived from category names as supervision. While effective on seen classes, such supervision overlooks the rich semantics encoded in category names, hindering the modeling of comparability among cross-category details and, in turn, limiting generalization to unseen categories. To tackle this, we introduce LaFG, a Language-driven framework for Fine-Grained Retrieval that converts class names into attribute-level supervision using large language models (LLMs) and vision-language models (VLMs). Treating each name as a semantic anchor, LaFG prompts an LLM to generate detailed, attribute-oriented descriptions. To mitigate attribute omission in these descriptions, it leverages a frozen VLM to project them into a vision-aligned space, clustering them into a dataset-wide attribute vocabulary while harvesting complementary attributes from related categories. Leveraging this vocabulary, a global prompt template selects category-relevant attributes, which are aggregated into category-specific linguistic prototypes. These prototypes supervise the retrieval model to steer

</details>


### [108] [Knowing the Answer Isn't Enough: Fixing Reasoning Path Failures in LVLMs](https://arxiv.org/abs/2512.06258)
*Chaoyang Wang,Yangfan He,Yiyang Zhou,Yixuan Wang,Jiaqi Liu,Peng Xia,Zhengzhong Tu,Mohit Bansal,Huaxiu Yao*

Main category: cs.CV

> LVLMs tend to choose incorrect reasoning paths despite knowing correct answers. PSO, a two-stage framework using GRPO and preference optimization, increases reasoning accuracy and stability of LVLMs.

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of Large Vision-Language Models (LVLMs) favoring incorrect reasoning paths over correct ones, leading to unreliable outcomes.

**Method:** PSO (Path-Select Optimization), a two-stage post-training framework where the first stage uses Group Relative Policy Optimization (GRPO) with specific rewards, and the second stage does online preference optimization with Negative Replay Memory (NRM) to avoid suboptimal paths.

**Result:** Experiments demonstrate that PSO improves reasoning accuracy by 7.4% on average, increases reasoning stability, and offers more consistent chains of thought in LVLMs.

**Conclusion:** The PSO framework enhances the reasoning abilities and stability of LVLMs by systematically reducing incorrect reasoning paths and promoting accurate ones.

**Abstract:** We reveal a critical yet underexplored flaw in Large Vision-Language Models (LVLMs): even when these models know the correct answer, they frequently arrive there through incorrect reasoning paths. The core issue is not a lack of knowledge, but a path selection bias within the vast reasoning search space. Although LVLMs are often capable of sampling correct solution trajectories, they disproportionately favor unstable or logically inconsistent ones, leading to erratic and unreliable outcomes. The substantial disparity between Pass@K (with large K) and Pass@1 across numerous models provides compelling evidence that such failures primarily stem from misreasoning rather than ignorance. To systematically investigate and address this issue, we propose PSO (Path-Select Optimization), a two-stage post-training framework designed to enhance both the reasoning performance and stability of existing LVLMs. In the first stage, we employ Group Relative Policy Optimization (GRPO) with template and answer-based rewards to cultivate structured, step-by-step reasoning. In the second stage, we conduct online preference optimization, where the model samples reasoning paths from GRPO-generated data, self-evaluates them, and aligns itself toward the preferred trajectories. Incorrect or suboptimal paths are concurrently stored in a Negative Replay Memory (NRM) as hard negatives, which are periodically revisited to prevent the model from repeating prior mistakes and to facilitate continual reasoning refinement. Extensive experiments show that PSO effectively prunes invalid reasoning paths, substantially enhances reasoning accuracy (with 7.4% improvements on average), and yields more stable and consistent chains of thought. Our code will be available at https://github.com/aiming-lab/PSO.

</details>


### [109] [TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting](https://arxiv.org/abs/2512.06269)
*Quan Tran,Tuan Dang*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** 3D Gaussian Splatting is crucial for real-time novel view synthesis due to its efficiency and ability to render photorealistic images. However, building a 3D Gaussian is guided solely by photometric loss, which can result in inconsistencies in reconstruction. This under-constrained process often results in "floater" artifacts and unstructured geometry, preventing the extraction of high-fidelity surfaces. To address this issue, our paper introduces a novel method that improves reconstruction by enforcing global geometry consistency through constrained multi-view triangulation. Our approach aims to achieve a consensus on 3D representation in the physical world by utilizing various estimated views. We optimize this process by penalizing the deviation of a rendered 3D point from a robust consensus point, which is re-triangulated from a bundle of neighboring views in a self-supervised fashion. We demonstrate the effectiveness of our method across multiple datasets, achieving state-of-the-art results. On the DTU dataset, our method attains a mean Chamfer Distance of 0.50 mm, outperforming comparable explicit methods. We will make our code open-source to facilitate community validation and ensure reproducibility.

</details>


### [110] [FacePhys: State of the Heart Learning](https://arxiv.org/abs/2512.06275)
*Kegang Wang,Jiankai Tang,Yuntao Wang,Xin Liu,Yuxuan Fan,Jiatong Ji,Yuanchun Shi,Daniel McDuff*

Main category: cs.CV

> 本文提出了一个名为FacePhys的内存高效的远程光电容积脉搏波描记法(rPPG)算法，实现了模型可扩展性、跨数据集泛化和实时操作的三者兼顾。相比现有方法，FacePhys在准确性和实时性能方面都有显著提升，具体表现为误差减少49%，内存占用降至3.6MB，每帧处理时间仅为9.46ms。

<details>
  <summary>Details</summary>

**Motivation:** 传统rPPG技术在实际部署中受到前端设备计算能力和数据传输中信号质量下降的限制，本文旨在解决这些问题，提升健康监测的舒适性和普遍性。

**Method:** 本文提出的方法FacePhys利用时空状态空间二元性理论，捕捉视频帧间微妙的周期变化，同时保持较低的计算开销。

**Result:** 研究结果显示，FacePhys相比现有技术有大幅改进，具体表现为误差减少49%，内存占用降低至3.6MB，每帧处理时间减少83%-99%。

**Conclusion:** FacePhys能够在实际部署中实现可靠的实时性能，并通过在线视频展示了它的实用性。

**Abstract:** Vital sign measurement using cameras presents opportunities for comfortable, ubiquitous health monitoring. Remote photoplethysmography (rPPG), a foundational technology, enables cardiac measurement through minute changes in light reflected from the skin. However, practical deployment is limited by the computational constraints of performing analysis on front-end devices and the accuracy degradation of transmitting data through compressive channels that reduce signal quality. We propose a memory efficient rPPG algorithm - \emph{FacePhys} - built on temporal-spatial state space duality, which resolves the trilemma of model scalability, cross-dataset generalization, and real-time operation. Leveraging a transferable heart state, FacePhys captures subtle periodic variations across video frames while maintaining a minimal computational overhead, enabling training on extended video sequences and supporting low-latency inference. FacePhys establishes a new state-of-the-art, with a substantial 49\% reduction in error. Our solution enables real-time inference with a memory footprint of 3.6 MB and per-frame latency of 9.46 ms -- surpassing existing methods by 83\% to 99\%. These results translate into reliable real-time performance in practical deployments, and a live demo is available at https://www.facephys.com/.

</details>


### [111] [RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension](https://arxiv.org/abs/2512.06276)
*Tianyi Gao,Hao Li,Han Fang,Xin Wei,Xiaodong Dong,Hongbo Sun,Ye Yuan,Zhongjiang He,Jinglin Xu,Jingmin Xin,Hao Sun*

Main category: cs.CV

> 本文提出RefBench-PRO，一个全面的引导向量理解基准，分解成六个子任务，并提出Ref-R1学习方案，以建立更强大的引用表达理解基线。

<details>
  <summary>Details</summary>

**Motivation:** 现有的REC基准主要评估感知能力，缺乏可解释的评分机制，无法展示MLLM在不同认知能力方面的接地能力。为了克服这一局限，提出了RefBench-PRO。

**Method:** 提出RefBench-PRO，一个全面的REC基准，它将引用表达式分解为感知和推理两个核心维度，并进一步划分为六个渐进挑战性的任务：属性、位置、相互作用、常识、关系和拒绝。此外，还开发了一个全自动的数据生成流水线，以产生这六个子维度的多样化引用表达式。同时提出了Ref-R1，一个基于RL的学习方案，该方案结合了基于动态IoU的GRPO方法，以提高在越来越复杂的推理条件下定位准确度，建立了更强的REC基准。

**Result:** 实验表明，RefBench-PRO能够对MLLM在引用表达式理解方面的性能进行更可解释的评估，并在感知和推理方面提出了更大的挑战。

**Conclusion:** RefBench-PRO能够更好地评估MLLM在引用表达式理解方面的性能，并在感知和推理方面提出了更大的挑战。

**Abstract:** Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.

</details>


### [112] [Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models](https://arxiv.org/abs/2512.06281)
*Hengzhuang Li,Xinsong Zhang,Qiming Peng,Bin Luo,Han Hu,Dengyang Jiang,Han-Jia Ye,Teng Zhang,Hai Jin*

Main category: cs.CV

> 本文提出LaVer框架，通过改进MLLMs中视觉信息的处理，解决了视觉表示同质化的问题，增强其在视觉任务上的表现。

<details>
  <summary>Details</summary>

**Motivation:** 解决MLLMs在视觉和文本信息处理上的不平衡问题，特别是在深层次网络中视觉信息被忽视，导致视觉性能下降或产生幻觉的现象。

**Method:** 提出了一种新的训练框架LaVer，通过在联合的潜在语义空间中进行掩码图像建模，帮助MLLMs学习更有区分性的视觉表示，从而直接激活MLLMs，增加其对视觉信息的利用。

**Result:** 实验结果表明，该方法在多个基准测试中表现出色，尤其是在需要密集视觉能力的场景中。

**Conclusion:** LaVer框架通过直接向MLLMs提供视觉激活，改善了其视觉注意力分配，证明了在具备密集视觉能力需求的场景中的优越性。相关代码已在GitHub上公开。

**Abstract:** Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in multimodal tasks. Despite their impressive performance, MLLMs suffer from the modality imbalance issue, where visual information is often underutilized compared to textual representations in deeper layers, leading to degraded visual performance or hallucinations. This issue stems from the predominant reliance on next-text-token-prediction during training, which fails to provide direct visual supervisory signals, resulting in progressive homogenization of visual representations throughout the layers. To this end, we propose Latent Visual Reconstruction (LaVer), a novel training framework that facilitates MLLMs in learning more discriminative visual representations via masked image modeling in the joint latent semantic space of LLM. Our method offers direct visual activation to MLLMs, which exhibit increased visual attention allocation, indicating enhanced utilization of visual information. Extensive experiments across diverse benchmarks prove the superiority of our approach in various scenarios, especially those requiring dense visual capabilities. Code of LaVer is available at https://github.com/Fir-lat/LaVer.

</details>


### [113] [A Sleep Monitoring System Based on Audio, Video and Depth Information](https://arxiv.org/abs/2512.06282)
*Lyn Chao-ling Chen,Kuan-Wen Chen,Yi-Ping Hung*

Main category: cs.CV

> 本研究开发了一种非侵入性的睡眠障碍监测系统，通过包括红外深度传感器、RGB摄像头和四麦克风阵列的设备，在几乎无光源的环境中实现实时监测。系统采用事件驱动方法对睡眠中的动作、光线和噪声等三种类型事件进行分类。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于开发一种可以对睡眠障碍进行定量评估的非侵入式监测系统，以便更精确地了解和改善睡眠质量。

**Method:** 系统通过使用红外深度传感器、RGB摄像头及四麦克风阵列捕捉睡眠过程中发生的不同类型事件。通过对深度信号和颜色图像建立背景模型以分别测量运动大小和光线变化幅度。引入事件检测算法来识别事件的发生。

**Result:** 实验在实际睡眠条件下进行，结果验证了该系统的可靠性和有效性。

**Conclusion:** 本研究成功地开发了一个可以对家居环境中发生的不同睡眠障碍相关事件进行监测的系统，为量化评估睡眠质量提供了新方法。

**Abstract:** For quantitative evaluation of sleep disturbances, a noninvasive monitoring system is developed by introducing an event-based method. We observe sleeping in home context and classify the sleep disturbances into three types of events: motion events, light-on/off events and noise events. A device with an infrared depth sensor, a RGB camera, and a four-microphone array is used in sleep monitoring in an environment with barely light sources. One background model is established in depth signals for measuring magnitude of movements. Because depth signals cannot observe lighting changes, another background model is established in color images for measuring magnitude of lighting effects. An event detection algorithm is used to detect occurrences of events from the processed data of the three types of sensors. The system was tested in sleep condition and the experiment result validates the system reliability.

</details>


### [114] [StrokeNet: Unveiling How to Learn Fine-Grained Interactions in Online Handwritten Stroke Classification](https://arxiv.org/abs/2512.06290)
*Yiheng Huang,Shuang She,Zewei Wei,Jianmin Lin,Ming Yang,Wenyin Liu*

Main category: cs.CV

> StrokeNet improves stroke classification by dynamically selecting reference points and encoding stroke interactions with innovative modules, demonstrating superior performance on public datasets.

<details>
  <summary>Details</summary>

**Motivation:** The core challenge in stroke classification is addressing localized stroke interactions without introducing redundancy. Existing deep learning methods fall short in capturing the semantic relationships between strokes.

**Method:** Our method, StrokeNet, encodes strokes as reference pair representations with dynamic reference point selection and sequencing. An Inline Sequence Attention (ISA) module and a Cross-Ellipse Query (CEQ) mechanism are employed to represent strokes and capture feature interactions. A joint optimization framework is used for predicting stroke categories and adjacent stroke semantic transitions.

**Result:** Experiments on multiple public online handwritten datasets demonstrate state-of-the-art performance, with a significant improvement in accuracy on the CASIA-onDo dataset.

**Conclusion:** The StrokeNet approach effectively tackles stroke classification challenges, showcasing high accuracy and robustness, especially in handling localized stroke interactions.

**Abstract:** Stroke classification remains challenging due to variations in writing style, ambiguous content, and dynamic writing positions. The core challenge in stroke classification is modeling the semantic relationships between strokes. Our observations indicate that stroke interactions are typically localized, making it difficult for existing deep learning methods to capture such fine-grained relationships. Although viewing strokes from a point-level perspective can address this issue, it introduces redundancy. However, by selecting reference points and using their sequential order to represent strokes in a fine-grained manner, this problem can be effectively solved. This insight inspired StrokeNet, a novel network architecture encoding strokes as reference pair representations (points + feature vectors), where reference points enable spatial queries and features mediate interaction modeling. Specifically, we dynamically select reference points for each stroke and sequence them, employing an Inline Sequence Attention (ISA) module to construct contextual features. To capture spatial feature interactions, we devised a Cross-Ellipse Query (CEQ) mechanism that clusters reference points and extracts features across varying spatial scales. Finally, a joint optimization framework simultaneously predicts stroke categories via reference points regression and adjacent stroke semantic transition modeling through an Auxiliary Branch (Aux-Branch). Experimental results show that our method achieves state-of-the-art performance on multiple public online handwritten datasets. Notably, on the CASIA-onDo dataset, the accuracy improves from 93.81$\%$ to 95.54$\%$, demonstrating the effectiveness and robustness of our approach.

</details>


### [115] [Exploiting Spatiotemporal Properties for Efficient Event-Driven Human Pose Estimation](https://arxiv.org/abs/2512.06306)
*Haoxian Zhou,Chuanzhi Xu,Langyi Chen,Haodong Chen,Yuk Ying Chung,Qiang Qu,Xaoming Chen,Weidong Cai*

Main category: cs.CV

> A method for human pose estimation using spatiotemporal properties of event streams without converting to dense frames, showing improved performance.

<details>
  <summary>Details</summary>

**Motivation:** To exploit the spatiotemporal properties of event streams without converting them into dense frames, which increases computation and lessens the high temporal resolution.

**Method:** We design an Event Temporal Slicing Convolution module to capture short-term dependencies and combine it with the Event Slice Sequencing module for structured temporal modeling. We also apply edge enhancement to improve spatial edge information under sparse conditions.

**Result:** Our proposed method shows consistent improvement across three point cloud backbones: PointNet, DGCNN, and Point Transformer on the DHP19 dataset.

**Conclusion:** The use of event streams directly for human pose estimation enhances performance without the overhead of dense frame conversion, indicating potential for real-time applications.

**Abstract:** Human pose estimation focuses on predicting body keypoints to analyze human motion. Event cameras provide high temporal resolution and low latency, enabling robust estimation under challenging conditions. However, most existing methods convert event streams into dense event frames, which adds extra computation and sacrifices the high temporal resolution of the event signal. In this work, we aim to exploit the spatiotemporal properties of event streams based on point cloud-based framework, designed to enhance human pose estimation performance. We design Event Temporal Slicing Convolution module to capture short-term dependencies across event slices, and combine it with Event Slice Sequencing module for structured temporal modeling. We also apply edge enhancement in point cloud-based event representation to enhance spatial edge information under sparse event conditions to further improve performance. Experiments on the DHP19 dataset show our proposed method consistently improves performance across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer.

</details>


### [116] [ReCAD: Reinforcement Learning Enhanced Parametric CAD Model Generation with Vision-Language Models](https://arxiv.org/abs/2512.06328)
*Jiahao Li,Yusheng Luo,Yunzhong Lou,Xiangdong Zhou*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** We present ReCAD, a reinforcement learning (RL) framework that bootstraps pretrained large models (PLMs) to generate precise parametric computer-aided design (CAD) models from multimodal inputs by leveraging their inherent generative capabilities. With just access to simple functional interfaces (e.g., point coordinates), our approach enables the emergence of complex CAD operations (e.g., pattern replication and mirror). This stands in contrast to previous methods, which typically rely on knowledge injected through supervised fine-tuning (SFT), offer limited support for editability, and fail to exploit the strong generative priors of PLMs. Specifically, the ReCAD framework begins by fine-tuning vision-language models (VLMs) to equip them with basic CAD model generation capabilities, where we rewrite CAD scripts into parameterized code that is leveraged to generate accurate textual descriptions for supervision. Then, we propose a novel RL strategy that incorporates parameterized code as guidance to enhance the model's reasoning on challenging questions. Furthermore, we employ a hierarchical primitive learning process to progressively teach structured and compositional skills under a unified reward function that ensures both geometric accuracy and semantic fidelity. ReCAD sets a new state-of-the-art in both text-to-CAD and image-to-CAD tasks, significantly improving geometric accuracy across in-distribution and out-of-distribution settings. In the image-to-CAD task, for instance, it reduces the mean Chamfer Distance from 73.47 to 29.61 (in-distribution) and from 272.06 to 80.23 (out-of-distribution), outperforming existing baselines by a substantial margin.

</details>


### [117] [S2WMamba: A Spectral-Spatial Wavelet Mamba for Pansharpening](https://arxiv.org/abs/2512.06330)
*Haoyu Zhang,Junhan Luo,Yugang Cao,Siran Peng,Jie Huang,Liangjian-Deng*

Main category: cs.CV

> 研究提出了一种新的图像融合方法S2WMamba，利用小波变换分离频率信息并进行跨模态交互，以此提高多光谱图像融合的质量。实验结果表明该方法在多种数据集上的性能优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机在于克服多光谱图像融合中的一个主要难题：如何在保持空间细节的同时也保持光谱保真度。该研究提出了一种新方法来解决这一问题，以提高遥感图像的质量。

**Method:** S2WMamba方法通过明确分离频率信息并进行轻量级的跨模态交互来解决PAN图像和MS图像融合时空间细节与光谱保真度的纠缠问题。具体地，采用2D Haar离散小波变换（DWT）对PAN图像进行空间边缘和纹理定位，同时采用通道维度上的1D DWT对每个像素的光谱视为1D信号以分离低频和高频分量，以此限制光谱失真。光谱分支将小波提取的空间细节注入到MS特征中，而空间分支则利用来自1D金字塔的光谱来精细PAN特征，两个分支通过基于Mamba的跨模态交互来交换长程信息，最后使用多尺度动态门（乘法+加法）自适应地融合分支输出。

**Result:** 实验结果表明，在WV3、GF2和QB数据集上，S2WMamba方法表现优异，与近期的竞争算法（如FusionMamba、CANNet、U2Net、ARConv）相比，PSNR（峰值信噪比）最高提升达0.23 dB，HQNR（高质量噪声比）在全分辨率的WV3数据集上达到0.956。

**Conclusion:** 实验和消融研究验证了S2WMamba方法的有效性，显示了2D/1D DWT的位置、平行双分支结构以及融合门的选择的合理性。代码已公开。

**Abstract:** Pansharpening fuses a high-resolution PAN image with a low-resolution multispectral (LRMS) image to produce an HRMS image. A key difficulty is that jointly processing PAN and MS often entangles spatial detail with spectral fidelity. We propose S2WMamba, which explicitly disentangles frequency information and then performs lightweight cross-modal interaction. Concretely, a 2D Haar DWT is applied to PAN to localize spatial edges and textures, while a channel-wise 1D Haar DWT treats each pixel's spectrum as a 1D signal to separate low/high-frequency components and limit spectral distortion. The resulting Spectral branch injects wavelet-extracted spatial details into MS features, and the Spatial branch refines PAN features using spectra from the 1D pyramid; the two branches exchange information through Mamba-based cross-modulation that models long-range dependencies with linear complexity. A multi-scale dynamic gate (multiplicative + additive) then adaptively fuses branch outputs.On WV3, GF2, and QB, S2WMamba matches or surpasses recent strong baselines (FusionMamba, CANNet, U2Net, ARConv), improving PSNR by up to 0.23 dB and reaching HQNR 0.956 on full-resolution WV3. Ablations justify the choice of 2D/1D DWT placement, parallel dual branches, and the fusion gate. Our code is available at https://github.com/KagUYa66/S2WMamba.

</details>


### [118] [CryoHype: Reconstructing a thousand cryo-EM structures with transformer-based hypernetworks](https://arxiv.org/abs/2512.06332)
*Jeffrey Gu,Minkyu Jeon,Ambri Ma,Serena Yeung-Levy,Ellen D. Zhong*

Main category: cs.CV

> Proposal of CryoHype, a transformer-based method that dynamically adjusts weights to resolve compositional heterogeneity in cryo-EM, demonstrating state-of-the-art performance and scalability on a benchmark of 100 structures and extending to 1,000 structures

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of resolving compositional heterogeneity arising from mixtures of many distinct molecular species in cryo-EM, as existing methods only model conformational heterogeneity within a single or a few structures

**Method:** CryoHype, a transformer-based hypernetwork for cryo-EM reconstruction that dynamically adjusts the weights of an implicit neural representation

**Result:** Achieved state-of-the-art results on a challenging benchmark dataset containing 100 structures and demonstrated scalability for the reconstruction of 1,000 distinct structures from unlabeled cryo-EM images in the fixed-pose setting

**Conclusion:** CryoHype effectively reconstructs multiple distinct molecular structures simultaneously from cryo-EM images, enabling high-throughput and scalability in cryo-EM reconstruction

**Abstract:** Cryo-electron microscopy (cryo-EM) is an indispensable technique for determining the 3D structures of dynamic biomolecular complexes. While typically applied to image a single molecular species, cryo-EM has the potential for structure determination of many targets simultaneously in a high-throughput fashion. However, existing methods typically focus on modeling conformational heterogeneity within a single or a few structures and are not designed to resolve compositional heterogeneity arising from mixtures of many distinct molecular species. To address this challenge, we propose CryoHype, a transformer-based hypernetwork for cryo-EM reconstruction that dynamically adjusts the weights of an implicit neural representation. Using CryoHype, we achieve state-of-the-art results on a challenging benchmark dataset containing 100 structures. We further demonstrate that CryoHype scales to the reconstruction of 1,000 distinct structures from unlabeled cryo-EM images in the fixed-pose setting.

</details>


### [119] [Beyond Hallucinations: A Multimodal-Guided Task-Aware Generative Image Compression for Ultra-Low Bitrate](https://arxiv.org/abs/2512.06344)
*Kaile Wang,Lijun He,Haisheng Fu,Haixia Bi,Fan Li*

Main category: cs.CV

> 本文提出一种多模态任务感知引导生成图像压缩框架（MTGC），通过集成三种模态增强超低码率下的语义一致性，并通过实验验证其在语义一致性、感知质量和像素级保真度上的显著提升。

<details>
  <summary>Details</summary>

**Motivation:** 生成图像压缩技术在超低码率下经常遇到语义偏差问题，影响其在带宽受限的6G语义通信场景中的可靠部署。为了增强图像压缩的语义一致性，MTGC被提出。

**Method:** MTGC框架整合三种指导模式以增强语义一致性：简洁而健壮的文本字幕捕获全局语义信息；压缩的高保真度图像（HCI）保留低级视觉信息；任务相关的语义伪字（SPWs）提供细粒度任务相关语义信息。SPWs通过任务导向语义压缩模块（TASCM）生成，该模块驱动多头自我注意机制聚焦和提取生成任务相关的语义信息，同时过滤掉冗余信息。为实现这三种指导模式的协同作用，MTGC设计了多模态引导扩散解码器（MGDD），采用双路径协同指导机制将指导注入扩散过程，并利用扩散模型的强大生成先验知识重构图像。

**Result:** 实验结果表明，MTGC能在超低码率下持续提高语义一致性，同时在感知质量和像素级保真度上也取得了显著提升，例如在DIV2K数据集上DISTS下降了10.59%。

**Conclusion:** MTGC框架通过多模态任务感知引导提供了在超低码率下生成图像压缩的优越性能，增强了语义一致性，并在感知质量和像素级保真度方面获得了显著提升。

**Abstract:** Generative image compression has recently shown impressive perceptual quality, but often suffers from semantic deviations caused by generative hallucinations at ultra-low bitrate (bpp < 0.05), limiting its reliable deployment in bandwidth-constrained 6G semantic communication scenarios. In this work, we reassess the positioning and role of of multimodal guidance, and propose a Multimodal-Guided Task-Aware Generative Image Compression (MTGC) framework. Specifically, MTGC integrates three guidance modalities to enhance semantic consistency: a concise but robust text caption for global semantics, a highly compressed image (HCI) retaining low-level visual information, and Semantic Pseudo-Words (SPWs) for fine-grained task-relevant semantics. The SPWs are generated by our designed Task-Aware Semantic Compression Module (TASCM), which operates in a task-oriented manner to drive the multi-head self-attention mechanism to focus on and extract semantics relevant to the generation task while filtering out redundancy. Subsequently, to facilitate the synergistic guidance of these modalities, we design a Multimodal-Guided Diffusion Decoder (MGDD) employing a dual-path cooperative guidance mechanism that synergizes cross-attention and ControlNet additive residuals to precisely inject these three guidance into the diffusion process, and leverages the diffusion model's powerful generative priors to reconstruct the image. Extensive experiments demonstrate that MTGC consistently improves semantic consistency (e.g., DISTS drops by 10.59% on the DIV2K dataset) while also achieving remarkable gains in perceptual quality and pixel-level fidelity at ultra-low bitrate.

</details>


### [120] [CLUENet: Cluster Attention Makes Neural Networks Have Eyes](https://arxiv.org/abs/2512.06345)
*Xiangshuai Song,Jun-Jie Huang,Tianrui Liu,Ke Liang,Chang Tang*

Main category: cs.CV

> 本文提出了CLUENet，一种利用多项创新技术解决解释性和准确性问题的透明深度模型，提升了视觉语义理解任务的性能。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决基于卷积和注意力的模型在建模不规则空间模式时的局限性以及解释性问题，同时提高聚类模型的准确性、效率和避免训练过程中的梯度消失。

**Method:** 本文提出了一种名为CLUENet的透明深度架构，旨在提升视觉语义理解。该架构包含三项关键创新：(i) 全局软聚合和硬分配，结合温度缩放余弦注意力机制和门控残差连接，以增强局部建模；(ii) 互块间的硬分配和共享特征派遣；(iii) 改进的聚类池化策略。

**Result:** 实验结果表明，CLUENet在CIFAR-100和Mini-ImageNet上的表现优于现有的聚类方法和主流视觉模型。

**Conclusion:** 本文提出的CLUENet实现了准确率、效率和透明度之间的良好平衡。

**Abstract:** Despite the success of convolution- and attention-based models in vision tasks, their rigid receptive fields and complex architectures limit their ability to model irregular spatial patterns and hinder interpretability, therefore posing challenges for tasks requiring high model transparency. Clustering paradigms offer promising interpretability and flexible semantic modeling, but suffer from limited accuracy, low efficiency, and gradient vanishing during training. To address these issues, we propose CLUster attEntion Network (CLUENet), an transparent deep architecture for visual semantic understanding. We propose three key innovations include (i) a Global Soft Aggregation and Hard Assignment with a Temperature-Scaled Cosin Attention and gated residual connections for enhanced local modeling, (ii) inter-block Hard and Shared Feature Dispatching, and (iii) an improved cluster pooling strategy. These enhancements significantly improve both classification performance and visual interpretability. Experiments on CIFAR-100 and Mini-ImageNet demonstrate that CLUENet outperforms existing clustering methods and mainstream visual models, offering a compelling balance of accuracy, efficiency, and transparency.

</details>


### [121] [TreeQ: Pushing the Quantization Boundary of Diffusion Transformer via Tree-Structured Mixed-Precision Search](https://arxiv.org/abs/2512.06353)
*Kaicheng Yang,Kaisen Yang,Baiting Wu,Xun Zhang,Qianrui Yang,Haotong Qin,He Zhang,Yulun Zhang*

Main category: cs.CV

> 本文介绍了TreeQ，一个针对Diffusion Transformers (DiTs) 量化的统一框架，通过引入Tree Structured Search, Environmental Noise Guidance和General Monarch Branch技术，解决了效率、优化目标与信息瓶颈等问题，展示了在DiT-XL/2模型上的超越现有技术表现，特别是在4位PTQ量化方面。

<details>
  <summary>Details</summary>

**Motivation:** 尽管DiTs在图像生成任务中表现出色，但其高昂的运算与内存需求阻碍了其实际应用。为了降低这些成本，作者提出了TreeQ框架，旨在专门解决DiTs量化的挑战。

**Method:** 通过Tree Structured Search (TSS)、Environmental Noise Guidance (ENG) 和 General Monarch Branch (GMB) 三项技术，TreeQ框架对DiTs的量化进行了优化。TSS改进了搜索空间的遍历效率，ENG统一了PTQ和QAT的优化目标，GMB解决信息瓶颈问题，保证细节生成。

**Result:** 实验显示，TreeQ在W3A3和W4A4 PTQ/PEFT设置下实现了DiT-XL/2模型的性能优化，尤其是在4位PTQ量化者取得了接近无损的表现。

**Conclusion:** 本文通过提出TreeQ框架，有效解决了DiTs量化中的问题，展示了在量化极限下保持高性能的可能，并发布了代码和模型以供研究和应用。

**Abstract:** Diffusion Transformers (DiTs) have emerged as a highly scalable and effective backbone for image generation, outperforming U-Net architectures in both scalability and performance. However, their real-world deployment remains challenging due to high computational and memory demands. Mixed-Precision Quantization (MPQ), designed to push the limits of quantization, has demonstrated remarkable success in advancing U-Net quantization to sub-4bit settings while significantly reducing computational and memory overhead. Nevertheless, its application to DiT architectures remains limited and underexplored. In this work, we propose TreeQ, a unified framework addressing key challenges in DiT quantization. First, to tackle inefficient search and proxy misalignment, we introduce Tree Structured Search (TSS). This DiT-specific approach leverages the architecture's linear properties to traverse the solution space in O(n) time while improving objective accuracy through comparison-based pruning. Second, to unify optimization objectives, we propose Environmental Noise Guidance (ENG), which aligns Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) configurations using a single hyperparameter. Third, to mitigate information bottlenecks in ultra-low-bit regimes, we design the General Monarch Branch (GMB). This structured sparse branch prevents irreversible information loss, enabling finer detail generation. Through extensive experiments, our TreeQ framework demonstrates state-of-the-art performance on DiT-XL/2 under W3A3 and W4A4 PTQ/PEFT settings. Notably, our work is the first to achieve near-lossless 4-bit PTQ performance on DiT models. The code and models will be available at https://github.com/racoonykc/TreeQ

</details>


### [122] [Rectifying Latent Space for Generative Single-Image Reflection Removal](https://arxiv.org/abs/2512.06358)
*Mingjia Li,Jin Hu,Hainuo Wang,Qiming Hu,Jiarui Wang,Xiaojie Guo*

Main category: cs.CV

> 本文通过改进潜在扩散模型处理反射去除问题，提出新的组件来优化模型，实现在多个基准上的SOTA性能和对现实案例的良好泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 本文重新构想了一个用于编辑目的的潜在扩散模型，以有效地理解和处理高度模糊、分层的图像输入，从而产生高质量的输出。

**Method:** 本文方法建立在三个协同组件之上：包括反射等方变分自编码器，该模型使潜空间与反射形成的线性物理相一致；一种可学习的任务特定文本嵌入，用于精确指导以避开模糊的语言；以及一种基于深度的早期分支采样策略，以利用生成的随机性获得有希望的结果。

**Result:** 实验结果表明，本文模型在多个基准测试中实现了新的SOTA性能，并且很好地泛化到了具有挑战性的现实世界案例中。

**Conclusion:** 本文方法通过重新设计潜在扩散模型并引入新的组件，显著提高了单图像反射去除的效果和泛化能力。

**Abstract:** Single-image reflection removal is a highly ill-posed problem, where existing methods struggle to reason about the composition of corrupted regions, causing them to fail at recovery and generalization in the wild. This work reframes an editing-purpose latent diffusion model to effectively perceive and process highly ambiguous, layered image inputs, yielding high-quality outputs. We argue that the challenge of this conversion stems from a critical yet overlooked issue, i.e., the latent space of semantic encoders lacks the inherent structure to interpret a composite image as a linear superposition of its constituent layers. Our approach is built on three synergistic components, including a reflection-equivariant VAE that aligns the latent space with the linear physics of reflection formation, a learnable task-specific text embedding for precise guidance that bypasses ambiguous language, and a depth-guided early-branching sampling strategy to harness generative stochasticity for promising results. Extensive experiments reveal that our model achieves new SOTA performance on multiple benchmarks and generalizes well to challenging real-world cases.

</details>


### [123] [Spoofing-aware Prompt Learning for Unified Physical-Digital Facial Attack Detection](https://arxiv.org/abs/2512.06363)
*Jiabao Guo,Yadian Wang,Hui Ma,Yuhao Fu,Ju Jia,Hui Liu,Shengeng Tang,Lechao Cheng,Yunfeng Diao,Ajian Liu*

Main category: cs.CV

> 提出了一种名为SPL-UAD的新框架，通过在提示空间中分离物理攻击和数字攻击的优化路径，从而提高面部分析系统的鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 当前的面部分析系统容易受到物理展示攻击（PAs）和数字伪造攻击（DFs）。现有的解决方法使用带有正则化约束的CLIP来增强模型泛化，但在同一类别提示空间下的物理和数字攻击检测存在优化方向冲突。

**Method:** 我们提出了一种名为SPL-UAD的框架，通过在提示空间中分离物理攻击和数字攻击的优化分支来改进现有的CLIP方法。该框架包括自适应仿冒上下文提示生成和基于双提示机制的线索感知增强技术，以提高模型的鲁棒性。

**Result:** 在大规模的UniAttackDataPlus数据集上进行了广泛的实验，证明所提出的方法在统一攻击检测任务中取得了显著的性能改进。

**Conclusion:** 研究表明，SPL-UAD框架可以在不牺牲精度的情况下，有效应对新型攻击，为生物识别数据提供全面的保护。

**Abstract:** Real-world face recognition systems are vulnerable to both physical presentation attacks (PAs) and digital forgery attacks (DFs). We aim to achieve comprehensive protection of biometric data by implementing a unified physical-digital defense framework with advanced detection. Existing approaches primarily employ CLIP with regularization constraints to enhance model generalization across both tasks. However, these methods suffer from conflicting optimization directions between physical and digital attack detection under same category prompt spaces. To overcome this limitation, we propose a Spoofing-aware Prompt Learning for Unified Attack Detection (SPL-UAD) framework, which decouples optimization branches for physical and digital attacks in the prompt space. Specifically, we construct a learnable parallel prompt branch enhanced with adaptive Spoofing Context Prompt Generation, enabling independent control of optimization for each attack type. Furthermore, we design a Cues-awareness Augmentation that leverages the dual-prompt mechanism to generate challenging sample mining tasks on data, significantly enhancing the model's robustness against unseen attack types. Extensive experiments on the large-scale UniAttackDataPlus dataset demonstrate that the proposed method achieves significant performance improvements in unified attack detection tasks.

</details>


### [124] [Human3R: Incorporating Human Priors for Better 3D Dynamic Reconstruction from Monocular Videos](https://arxiv.org/abs/2512.06368)
*Weitao Xiong,Zhiyuan Yuan,Jiahao Lu,Chengfeng Zhao,Peng Li,Yuan Liu*

Main category: cs.CV

> 本文提出Human3R方法，结合SMPL人体模型和单目深度估计，通过分层管道架构和特征融合模块，提升了动态人体重建的几何一致性与细节捕获能力。

<details>
  <summary>Details</summary>

**Motivation:** 针对单目动态视频重建中几何不一致性和分辨率退化的问题，现有方法缺乏对3D人体结构的理解，导致重建结果的肢体比例扭曲和自然的人体-物体融合不佳，且内存约束下的下采样会造成人体边界向背景几何偏移。

**Method:** Human3R采用分层管道架构，首先处理全分辨率图像以获得整体场景几何结构，接着通过战略裁剪和交叉注意力融合来增强人体特写细节。引入特征融合模块以整合SMPL人体模型，从而确保几何上合理的重建同时保持精细的人体边界。

**Result:** 实验结果显示，该方法在TUM Dynamics和GTA-IM数据集上实现了更优的动态人体重建性能。

**Conclusion:** 研究表明，引入混合几何先验和特征融合模块能够有效改善动态场景中单目视频的人体结构理解与重建效果，提供几何一致且细节丰富的重建结果。

**Abstract:** Monocular dynamic video reconstruction faces significant challenges in dynamic human scenes due to geometric inconsistencies and resolution degradation issues. Existing methods lack 3D human structural understanding, producing geometrically inconsistent results with distorted limb proportions and unnatural human-object fusion, while memory-constrained downsampling causes human boundary drift toward background geometry. To address these limitations, we propose to incorporate hybrid geometric priors that combine SMPL human body models with monocular depth estimation. Our approach leverages structured human priors to maintain surface consistency while capturing fine-grained geometric details in human regions. We introduce Human3R, featuring a hierarchical pipeline with refinement components that processes full-resolution images for overall scene geometry, then applies strategic cropping and cross-attention fusion for human-specific detail enhancement. The method integrates SMPL priors through a Feature Fusion Module to ensure geometrically plausible reconstruction while preserving fine-grained human boundaries. Extensive experiments on TUM Dynamics and GTA-IM datasets demonstrate superior performance in dynamic human reconstruction.

</details>


### [125] [VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning](https://arxiv.org/abs/2512.06373)
*Yuji Wang,Wenlong Liu,Jingxuan Niu,Haoji Zhang,Yansong Tang*

Main category: cs.CV

> 研究提出了一种名为VG-Refiner的新框架，通过引入两阶段思考-再思考机制和修正奖励，解决了现有工具集成视觉推理方法中存在的问题，提高了准确性并保持了预训练模型的通用能力。

<details>
  <summary>Details</summary>

**Motivation:** 现有工具集成的视觉推理方法主要集中于通过强化学习整合各种视觉工具，而忽略了设计有效的机制来处理不可靠或错误的工具输出。这一点在引用和背景任务中尤为明显，因为不准确的检测工具预测常常误导视觉推理模型生成幻觉推理。为解决这一问题，作者提出了新的框架。

**Method:** 提出了一种名为VG-Refiner的框架，旨在实现工具优化的引用和背景推理。该框架技术上采用了两阶段思考-再思考机制，使模型能够明确分析和响应工具反馈，并引入了修正奖励以鼓励在应对不良工具结果时进行有效的修正。此外，还提出了两个新的评估指标和公平的评估协议，以系统地衡量当前模型的修正能力。

**Result:** 采用少量任务特定的数据增强了VG-Refiner模型的优化能力，在引用和推理背景基准测试中显著提高了准确性和修正能力。

**Conclusion:** 通过引入两阶段思考-再思考机制和修正奖励，VG-Refiner框架在系统地改善了引用和推理背景任务中的准确性，并且保持了预训练模型的通用能力。

**Abstract:** Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.

</details>


### [126] [Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework](https://arxiv.org/abs/2512.06376)
*Xinhao Xiang,Abhijeet Rastogi,Jiawei Zhang*

Main category: cs.CV

> The paper explores the reliability of AI-generated driving videos (AIGVs) for autonomous driving model training and proposes tools to assess video quality and mitigate risks.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to evaluate the reliability of AI-generated driving videos for training autonomous driving models, addressing questions about their utility versus real or simulator data.

**Method:** The paper introduces a diagnostic framework to study the reliability of AI-generated driving videos (AIGVs) for training and evaluating autonomous driving models. It includes identifying failure modes, creating a benchmark (ADGV-Bench), and proposing a driving-aware evaluator (ADGVE) for quality assessment.

**Result:** Experiments indicate that raw AIGVs can degrade perception performance, but using ADGVE as a filter improves quality assessment metrics and supports the AD models.

**Conclusion:** The study concludes that AIGVs carry both risks and potential benefits. Practical tools are provided to enhance the quality of AI-generated videos, making them a useful asset to real-world data in AD pipelines.

**Abstract:** Recent text-to-video models have enabled the generation of high-resolution driving scenes from natural language prompts. These AI-generated driving videos (AIGVs) offer a low-cost, scalable alternative to real or simulator data for autonomous driving (AD). But a key question remains: can such videos reliably support training and evaluation of AD models? We present a diagnostic framework that systematically studies this question. First, we introduce a taxonomy of frequent AIGV failure modes, including visual artifacts, physically implausible motion, and violations of traffic semantics, and demonstrate their negative impact on object detection, tracking, and instance segmentation. To support this analysis, we build ADGV-Bench, a driving-focused benchmark with human quality annotations and dense labels for multiple perception tasks. We then propose ADGVE, a driving-aware evaluator that combines static semantics, temporal cues, lane obedience signals, and Vision-Language Model(VLM)-guided reasoning into a single quality score for each clip. Experiments show that blindly adding raw AIGVs can degrade perception performance, while filtering them with ADGVE consistently improves both general video quality assessment metrics and downstream AD models, and turns AIGVs into a beneficial complement to real-world data. Our study highlights both the risks and the promise of AIGVs, and provides practical tools for safely leveraging large-scale video generation in future AD pipelines.

</details>


### [127] [VAD-Net: Multidimensional Facial Expression Recognition in Intelligent Education System](https://arxiv.org/abs/2512.06377)
*Yi Huo,Yun Ge*

Main category: cs.CV

> 本文通过在FER2013数据集中标注VAD（情绪、唤醒度、优势）维度，特别是D维度，采用正交卷积以提升特征多样性和预测准确率，同时提出一种基于ResNet的VAD情绪预测网络基准，并公开了数据集和代码。

<details>
  <summary>Details</summary>

**Motivation:** 传统的FER数据集标注主要依赖于有限的情绪类别，而未来情感计算需要更精细和全面的情绪指标，通过VAD参数进行标注。AffectNet尝试标注VA维，但仍缺乏D维标注。

**Method:** 在FER2013数据集中标注VAD信息，并引入正交卷积方法以增强网络的特征提取能力。

**Result:** 实验结果显示，D维标注及预测相较于VA维更为困难。正交卷积的引入确实提升了VAD预测的准确性。

**Conclusion:** 研究提供了D维情绪标注的基准，提出了基于正交卷积的VAD预测网络，并构建了一套标注的VAD FER数据集作为基准数据。

**Abstract:** Current FER (Facial Expression Recognition) dataset is mostly labeled by emotion categories, such as happy, angry, sad, fear, disgust, surprise, and neutral which are limited in expressiveness. However, future affective computing requires more comprehensive and precise emotion metrics which could be measured by VAD(Valence-Arousal-Dominance) multidimension parameters. To address this, AffectNet has tried to add VA (Valence and Arousal) information, but still lacks D(Dominance). Thus, the research introduces VAD annotation on FER2013 dataset, takes the initiative to label D(Dominance) dimension. Then, to further improve network capacity, it enforces orthogonalized convolution on it, which extracts more diverse and expressive features and will finally increase the prediction accuracy. Experiment results show that D dimension could be measured but is difficult to obtain compared with V and A dimension no matter in manual annotation or regression network prediction. Secondly, the ablation test by introducing orthogonal convolution verifies that better VAD prediction could be obtained in the configuration of orthogonal convolution. Therefore, the research provides an initiative labelling for D dimension on FER dataset, and proposes a better prediction network for VAD prediction through orthogonal convolution. The newly built VAD annotated FER2013 dataset could act as a benchmark to measure VAD multidimensional emotions, while the orthogonalized regression network based on ResNet could act as the facial expression recognition baseline for VAD emotion prediction. The newly labeled dataset and implementation code is publicly available on https://github.com/YeeHoran/VAD-Net .

</details>


### [128] [OCFER-Net: Recognizing Facial Expression in Online Learning System](https://arxiv.org/abs/2512.06379)
*Yi Huo,Lei Zhang*

Main category: cs.CV

> 本研究提出了一种新的面部表情识别方法OCFER-Net，通过在卷积核上施加强制正交性来提高特征的多样性和表达力，在实验中表现优于基线模型。

<details>
  <summary>Details</summary>

**Motivation:** 本研究的动机是提高在线学习中教师对学生情感状况的感知能力，特别是在COVID-19背景下，这依赖于面部表情识别技术的精度提升。

**Method:** 本研究通过在卷积核上施加强制正交性的正则化器，从而提出了OCFER-Net。

**Result:** 最近，在COVID-19全球大流行背景下，在线学习非常受欢迎。除了知识传播外，情感互动也很重要，这可以通过面部表情识别(FER)来实现。由于FER的准确性对教师获取学生情感状况有重要帮助，该项目深入研究了一系列FER方法，并发现很少有工作探索卷积矩阵的正交性。因此，通过正则化器强制核函数正交化，提取更具多样性和表达力的特征，提出了OCFER-Net。在具有挑战性的FER-2013数据集上进行的实验表明，与基线相比，性能提高了1.087。该研究项目的代码可以在https://github.com/YeeHoran/OCFERNet上公开获取。

**Conclusion:** 本研究通过对卷积矩阵正交性的探索，提出了一种新的面部表情识别方法OCFER-Net，并在实验中验证了其相对于基线方法的优越性能。

**Abstract:** Recently, online learning is very popular, especially under the global epidemic of COVID-19. Besides knowledge distribution, emotion interaction is also very important. It can be obtained by employing Facial Expression Recognition (FER). Since the FER accuracy is substantial in assisting teachers to acquire the emotional situation, the project explores a series of FER methods and finds that few works engage in exploiting the orthogonality of convolutional matrix. Therefore, it enforces orthogonality on kernels by a regularizer, which extracts features with more diversity and expressiveness, and delivers OCFER-Net. Experiments are carried out on FER-2013, which is a challenging dataset. Results show superior performance over baselines by 1.087. The code of the research project is publicly available on https://github.com/YeeHoran/OCFERNet.

</details>


### [129] [Perceptual Region-Driven Infrared-Visible Co-Fusion for Extreme Scene Enhancement](https://arxiv.org/abs/2512.06400)
*Jing Tao,Yonghong Zong,Banglei Guana,Pengju Sun,Taihang Lei,Yang Shanga,Qifeng Yu*

Main category: cs.CV

> 解决光度测量中混合红外和可见光谱数据的挑战，提出了一种新的融合框架，提高了图像清晰度和整体性能。

<details>
  <summary>Details</summary>

**Motivation:** 在光度测量中，准确融合红外光和可见光谱同时保持可见特征的几何保真度并融合热辐射是一个重大挑战，特别是在极端条件下。现有方法往往折中可见成像的质量，影响测量精度。

**Method:** 提出了一种基于区域感知的融合框架，结合了多曝光和多模态成像，使用空间变化曝光（SVE）相机。该框架首先进行基于区域感知的特征融合以确保精确的多模态配准，然后进行自适应融合并增强对比度，通过受区域显著性图引导的结构相似性补偿机制优化红外-可见光谱的集成。此外，该框架还适应单曝光场景以实现不同条件下的鲁棒融合。

**Result:** 实验证明，该框架在合成数据和真实数据上均显示出优于现有技术的方法的图像清晰度和性能改进，通过定量和视觉评估得到验证。

**Conclusion:** 实验结果表明，所提出的框架在不同条件下均能实现高质量的图像融合，为高精度测量铺平了道路。

**Abstract:** In photogrammetry, accurately fusing infrared (IR) and visible (VIS) spectra while preserving the geometric fidelity of visible features and incorporating thermal radiation is a significant challenge, particularly under extreme conditions. Existing methods often compromise visible imagery quality, impacting measurement accuracy. To solve this, we propose a region perception-based fusion framework that combines multi-exposure and multi-modal imaging using a spatially varying exposure (SVE) camera. This framework co-fuses multi-modal and multi-exposure data, overcoming single-exposure method limitations in extreme environments. The framework begins with region perception-based feature fusion to ensure precise multi-modal registration, followed by adaptive fusion with contrast enhancement. A structural similarity compensation mechanism, guided by regional saliency maps, optimizes IR-VIS spectral integration. Moreover, the framework adapts to single-exposure scenarios for robust fusion across different conditions. Experiments conducted on both synthetic and real-world data demonstrate superior image clarity and improved performance compared to state-of-the-art methods, as evidenced by both quantitative and visual evaluations.

</details>


### [130] [Rethinking Training Dynamics in Scale-wise Autoregressive Generation](https://arxiv.org/abs/2512.06421)
*Gengze Zhou,Chongjian Ge,Hao Tan,Feng Liu,Yicong Hong*

Main category: cs.CV

> SAR, a post-training method for AR models, uses SSR to train-test alignment and CSFL for adequate supervision, resulting in improved image generation quality with minimal resources.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind SAR is to address the limitations of scale-wise autoregressive models in media synthesis, specifically the exposure bias that degrades generation quality, including issues of train-test mismatch and imbalanced scale-wise learning difficulty.

**Method:** SAR (Self-Autoregressive Refinement) introduces a Stagger-Scale Rollout (SSR) mechanism to align train-test patterns and a Contrastive Student-Forcing Loss (CSFL) to provide sufficient supervision for self-generated contexts, addressing the exposure bias issue in scale-wise AR models.

**Result:** Experimental results demonstrate that SAR applied to pretrained AR models improves generation quality with minimal computational overhead, e.g., reducing FID by 5.2% on FlexVAR-d16 with ImageNet 256 in just 5 hours on 32xA100 GPUs.

**Conclusion:** SAR is an efficient, scalable, and effective post-training method for improving visual autoregressive generation, making it a reliable solution for enhancing pretrained AR models with minimal additional resources.

**Abstract:** Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.

</details>


### [131] [A Perception CNN for Facial Expression Recognition](https://arxiv.org/abs/2512.06422)
*Chunwei Tian,Jingyuan Xie,Lingjun Li,Wangmeng Zuo,Yanning Zhang,David Zhang*

Main category: cs.CV

> 本文提出了一种感知卷积神经网络PCNN用于面部表情识别，通过五个并行网络学习局部特征，多域交互机制和二阶段损失函数，提升了面部表情识别的准确性和效果。

<details>
  <summary>Details</summary>

**Motivation:** 虽然卷积神经网络能够自动学习数据模式来表达面部图像以进行面部表情识别，但它们可能忽视面部分割对面部表情识别效果的影响。因此，本文提出一种新的方法来提高面部表情识别的性能。

**Method:** 本文提出了一种名为PCNN的感知卷积神经网络，用于面部表情识别。首先，PCNN使用五个并行网络同时学习基于眼睛、脸颊和嘴巴的局部面部特征，以捕捉面部表情识别中的微妙变化。其次，通过多领域交互机制，将局部感官器官特征与全局面部结构特征融合，更好地表达面部表情识别的图像特征。最后，设计了二阶段损失函数以确保所获得感官信息和重建面部图像的准确性。

**Result:** 实验结果表明，PCNN在多个实验室和实际面部表情识别基准数据集CK+、JAFFE、FER2013、FERPlus、RAF-DB以及遮挡和姿态变化的数据集上具有优越的结果。

**Conclusion:** 通过提出的PCNN方法，能够有效地进行面部表情识别，显示出优于现有方法的性能。

**Abstract:** Convolutional neural networks (CNNs) can automatically learn data patterns to express face images for facial expression recognition (FER). However, they may ignore effect of facial segmentation of FER. In this paper, we propose a perception CNN for FER as well as PCNN. Firstly, PCNN can use five parallel networks to simultaneously learn local facial features based on eyes, cheeks and mouth to realize the sensitive capture of the subtle changes in FER. Secondly, we utilize a multi-domain interaction mechanism to register and fuse between local sense organ features and global facial structural features to better express face images for FER. Finally, we design a two-phase loss function to restrict accuracy of obtained sense information and reconstructed face images to guarantee performance of obtained PCNN in FER. Experimental results show that our PCNN achieves superior results on several lab and real-world FER benchmarks: CK+, JAFFE, FER2013, FERPlus, RAF-DB and Occlusion and Pose Variant Dataset. Its code is available at https://github.com/hellloxiaotian/PCNN.

</details>


### [132] [DragMesh: Interactive 3D Generation Made Easy](https://arxiv.org/abs/2512.06424)
*Tianshan Zhang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

> 本文介绍了DragMesh，一种用于生成基于用户交互的实时3D关节动画的强大框架。它通过解耦的运动学推理和运动生成设计解决了现有的性能和一致性问题，以适应不同的物体而无需重新训练。

<details>
  <summary>Details</summary>

**Motivation:** 尽管生成模型在创建静态3D内容方面表现出色，但理解物体如何运动和响应交互仍是一个基本挑战。现有的关节运动方法要么在物理上一致但速度太慢无法应用于实时，要么具有生成能力但违反了基本的运动学约束。DragMesh旨在解决这一挑战，提供一种既可以满足实时性能又不违反运动学约束的系统。

**Method:** 提出了一种名为DragMesh的实时交互式3D动画的强大框架。该框架由轻量级运动生成核心构成，核心贡献是一个新颖的解耦的运动学推理和运动生成框架。通过分割语义意图推理（确定关节类型）和几何回归（使用Kinematics Prediction Network (KPP-Net) 确定轴和原点），推断潜在的关节参数。接着，采用Dual Quaternion VAE (DQ-VAE) 利用预测的先验和其他参数生成一整条合理的运动轨迹。为了确保运动学一致性，框架中使用Feature-wise Linear Modulation (FiLM) 调制在DQ-VAE的每一层注入关节先验。这种方法允许DragMesh实现实时性能，并且在未经重新训练的情况下对新的物体进行合理的、生成式的动画，迈向了3D生成智能的实际一步。

**Result:** DragMesh通过将运动学推理和运动生成解耦，并结合使用Dual Quaternion VAE和FiLM条件，实现了既保持实时性能，又能生成合理且符合运动学的3D动画目标，满足了现有的技术缺口。

**Conclusion:** DragMesh框架提供了在未经重新训练的情况下对新物体进行生成式合理动画的能力，使得它成为迈向3D生成智能的重要一步。

**Abstract:** While generative models have excelled at creating static 3D content, the pursuit of systems that understand how objects move and respond to interactions remains a fundamental challenge. Current methods for articulated motion lie at a crossroads: they are either physically consistent but too slow for real-time use, or generative but violate basic kinematic constraints. We present DragMesh, a robust framework for real-time interactive 3D articulation built around a lightweight motion generation core. Our core contribution is a novel decoupled kinematic reasoning and motion generation framework. First, we infer the latent joint parameters by decoupling semantic intent reasoning (which determines the joint type) from geometric regression (which determines the axis and origin using our Kinematics Prediction Network (KPP-Net)). Second, to leverage the compact, continuous, and singularity-free properties of dual quaternions for representing rigid body motion, we develop a novel Dual Quaternion VAE (DQ-VAE). This DQ-VAE receives these predicted priors, along with the original user drag, to generate a complete, plausible motion trajectory. To ensure strict adherence to kinematics, we inject the joint priors at every layer of the DQ-VAE's non-autoregressive Transformer decoder using FiLM (Feature-wise Linear Modulation) conditioning. This persistent, multi-scale guidance is complemented by a numerically-stable cross-product loss to guarantee axis alignment. This decoupled design allows DragMesh to achieve real-time performance and enables plausible, generative articulation on novel objects without retraining, offering a practical step toward generative 3D intelligence. Code: https://github.com/AIGeeksGroup/DragMesh. Website: https://aigeeksgroup.github.io/DragMesh.

</details>


### [133] [When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition](https://arxiv.org/abs/2512.06426)
*Nzakiese Mbongo,Kailash A. Hambarde,Hugo Proença*

Main category: cs.CV

> A dual-path transformer framework using CLIP for gender recognition at a distance, achieving better results than current state-of-the-art methods.

<details>
  <summary>Details</summary>

**Motivation:** Accurate gender recognition from extreme long-range imagery is challenging due to factors such as limited spatial resolution, viewpoint variability, and the loss of facial cues. This paper aims to address these challenges.

**Method:** We present a dual-path transformer framework that integrates a direct visual path and an attribute-mediated path for gender recognition at a distance. This framework leverages CLIP to jointly model visual and attribute-driven cues, enhancing discriminative localization under occlusion and low resolution through spatial channel attention modules.

**Result:** The proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), showing consistent robustness to distance, angle, and height variations.

**Conclusion:** Language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.

**Abstract:** Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.

</details>


### [134] [Automated Deep Learning Estimation of Anthropometric Measurements for Preparticipation Cardiovascular Screening](https://arxiv.org/abs/2512.06434)
*Lucas R. Mareque,Ricardo L. Armentano,Leandro J. Cymberknop*

Main category: cs.CV

> 研究提出了一种全自动的深度学习方法，通过2D合成人体图像估计人体测量数据，可以高精度检测潜在心脏疾病风险，适用于大规模运动员心脏健康筛查。

<details>
  <summary>Details</summary>

**Motivation:** 传统的手动方法用于检测Marfan综合征等心脏异常的运动员的人体测量，但这种方法劳动密集、依赖操作员，且难以扩展。本研究旨在提供一个全自动的深度学习方法解决这些问题。

**Method:** 采用深度学习方法，使用2D合成人体图像来估计五个关键的人体测量数据。使用从3D身体模型得出的100,000张图像的数据集，对VGG19，ResNet50和DenseNet121带有全连接层的回归进行了训练和评估。

**Result:** 所有模型都实现了亚厘米级的精度，其中ResNet50表现最好，所有测量的平均MAE为0.668厘米。

**Conclusion:** 深度学习可以大规模地提供准确的人体测量数据，为补充运动员筛选协议提供了实用工具。未来的工作将在真实世界图像上验证这些模型，以扩大其应用范围。

**Abstract:** Preparticipation cardiovascular examination (PPCE) aims to prevent sudden cardiac death (SCD) by identifying athletes with structural or electrical cardiac abnormalities. Anthropometric measurements, such as waist circumference, limb lengths, and torso proportions to detect Marfan syndrome, can indicate elevated cardiovascular risk. Traditional manual methods are labor-intensive, operator-dependent, and challenging to scale. We present a fully automated deep-learning approach to estimate five key anthropometric measurements from 2D synthetic human body images. Using a dataset of 100,000 images derived from 3D body meshes, we trained and evaluated VGG19, ResNet50, and DenseNet121 with fully connected layers for regression. All models achieved sub-centimeter accuracy, with ResNet50 performing best, achieving a mean MAE of 0.668 cm across all measurements. Our results demonstrate that deep learning can deliver accurate anthropometric data at scale, offering a practical tool to complement athlete screening protocols. Future work will validate the models on real-world images to extend applicability.

</details>


### [135] [AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars](https://arxiv.org/abs/2512.06438)
*Ramazan Fazylov,Sergey Zagoruyko,Aleksandr Parkin,Stamatis Lefkimmiatis,Ivan Laptev*

Main category: cs.CV

> AGORA框架通过结合3D高斯辐射和生成对抗网络技术，实现了高保真度、可动画化的3D人类角色生成，在保证表达准确性和实时渲染的同时，大幅提高了性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于隐式表示如NeRF的方法存在渲染速度慢和动态不一致的问题，而3D高斯辐射方法通常只限于静态头部生成，缺乏动态控制。我们希望通过AGORA解决这些问题，生成既能精确控制又能实时渲染的3D人类角色。

**Method:** 我们提出了AGORA，这是一个新的框架，它将3D高斯辐射扩展到生成对抗网络中，以生成可动画的3D人类角色。主要贡献是一个轻量级的、基于FLAME的变形分支，它预测高斯残差，实现了在保持身份特征的前提下，进行精确的表情控制，并支持实时推理。

**Result:** 我们的方法在表达准确性方面超越了最先进的基于NeRF的方法，同时在单一GPU上可以达到250+ FPS的渲染速度，CPU-only推理时达到约9 FPS，这是首次展示实用的CPU-only动画3DGS头像合成。

**Conclusion:** 这项工作代表了向实用的高性能数字人物生成迈出的重要一步。

**Abstract:** The generation of high-fidelity, animatable 3D human avatars remains a core challenge in computer graphics and vision, with applications in VR, telepresence, and entertainment. Existing approaches based on implicit representations like NeRFs suffer from slow rendering and dynamic inconsistencies, while 3D Gaussian Splatting (3DGS) methods are typically limited to static head generation, lacking dynamic control. We bridge this gap by introducing AGORA, a novel framework that extends 3DGS within a generative adversarial network to produce animatable avatars. Our key contribution is a lightweight, FLAME-conditioned deformation branch that predicts per-Gaussian residuals, enabling identity-preserving, fine-grained expression control while allowing real-time inference. Expression fidelity is enforced via a dual-discriminator training scheme leveraging synthetic renderings of the parametric mesh. AGORA generates avatars that are not only visually realistic but also precisely controllable. Quantitatively, we outperform state-of-the-art NeRF-based methods on expression accuracy while rendering at 250+ FPS on a single GPU, and, notably, at $\sim$9 FPS under CPU-only inference - representing, to our knowledge, the first demonstration of practical CPU-only animatable 3DGS avatar synthesis. This work represents a significant step toward practical, high-performance digital humans. Project website: https://ramazan793.github.io/AGORA/

</details>


### [136] [Towards Stable Cross-Domain Depression Recognition under Missing Modalities](https://arxiv.org/abs/2512.06447)
*Jiuyi Chen,Mingkui Tan,Haifeng Lu,Qiuna Xu,Zhihua Wang,Runhao Zeng,Xiping Hu*

Main category: cs.CV

> 本文提出了一种基于多模态大规模语言模型的抑郁识别框架，解决现有方法在多样性和稳定性上的不足，实验结果显示框架在各种测试中表现突出。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有的基于音频和视频的自动抑郁检测（ADD）方法缺乏统一和通用的框架，且在面对现实世界中常见的缺失模态时表现出稳定性不足的问题，因此本研究旨在提出一个稳定且适应多种抑郁识别场景的解决方案。

**Method:** 本文提出了一种基于多模态大规模语言模型的稳定跨域抑郁识别统一框架（SCD-MLLM），该框架包括多源数据输入适配器（MDIA）和模态感知自适应融合模块（MAFM），以支持来自不同来源的异构抑郁症相关数据的整合和处理，并在存在不完整模态输入的情况下保持稳定性。

**Result:** 通过在五个公开可用且来自不同场景的异构抑郁数据集上的全面实验，包括CMDC、AVEC2014、DAIC-WOZ、DVlog和EATD，SCD-MLLM在完整模态和部分模态设置下均优于现有最先进的模型以及领先的商用LLM（Gemini和GPT）。

**Conclusion:** 该研究表明，SCD-MLLM展示出了优秀的跨域泛化能力，在实际应用中能够更好地捕捉多模态抑郁症线索，并具备强模态缺失稳定性。

**Abstract:** Depression poses serious public health risks, including suicide, underscoring the urgency of timely and scalable screening. Multimodal automatic depression detection (ADD) offers a promising solution; however, widely studied audio- and video-based ADD methods lack a unified, generalizable framework for diverse depression recognition scenarios and show limited stability to missing modalities, which are common in real-world data. In this work, we propose a unified framework for Stable Cross-Domain Depression Recognition based on Multimodal Large Language Model (SCD-MLLM). The framework supports the integration and processing of heterogeneous depression-related data collected from varied sources while maintaining stability in the presence of incomplete modality inputs. Specifically, SCD-MLLM introduces two key components: (i) Multi-Source Data Input Adapter (MDIA), which employs masking mechanism and task-specific prompts to transform heterogeneous depression-related inputs into uniform token sequences, addressing inconsistency across diverse data sources; (ii) Modality-Aware Adaptive Fusion Module (MAFM), which adaptively integrates audio and visual features via a shared projection mechanism, enhancing resilience under missing modality conditions. e conduct comprehensive experiments under multi-dataset joint training settings on five publicly available and heterogeneous depression datasets from diverse scenarios: CMDC, AVEC2014, DAIC-WOZ, DVlog, and EATD. Across both complete and partial modality settings, SCD-MLLM outperforms state-of-the-art (SOTA) models as well as leading commercial LLMs (Gemini and GPT), demonstrating superior cross-domain generalization, enhanced ability to capture multimodal cues of depression, and strong stability to missing modality cases in real-world applications.

</details>


### [137] [Sanvaad: A Multimodal Accessibility Framework for ISL Recognition and Voice-Based Interaction](https://arxiv.org/abs/2512.06485)
*Kush Revankar,Shreyas Deshpande,Araham Sayeed,Ansh Tandale,Sarika Bobde*

Main category: cs.CV

> Sanvaad is a lightweight multimodal framework enabling real-time, bidirectional communication for deaf, visually impaired, and hearing users, featuring ISL recognition and voice-to-sign conversion.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for Sanvaad is to improve communication between deaf and visually impaired individuals and the general hearing population by creating a system that supports two-way interaction, addressing the limitations of current tools which usually support only one direction.

**Method:** The method includes ISL recognition using MediaPipe landmarks for deaf users, voice-to-sign conversion via mapping detected speech to predefined phrases and producing GIFs for sign language, and a screen-free voice interface with mutli-lingual speech recognition and text-to-speech for visually impaired users.

**Result:** The framework utilizes Streamlit-based interfaces, making it accessible on desktop and mobile devices, thus enabling real-time, bidirectional communication, addressing the communication barriers faced by deaf and visually impaired people.

**Conclusion:** Sanvaad successfully provides an accessible and practical solution for inclusive communication by combining lightweight computer vision and speech processing tools within a single framework that can run on common devices without special hardware.

**Abstract:** Communication between deaf users, visually im paired users, and the general hearing population often relies on tools that support only one direction of interaction. To address this limitation, this work presents Sanvaad, a lightweight multimodal accessibility framework designed to support real time, two-way communication. For deaf users, Sanvaad includes an ISL recognition module built on MediaPipe landmarks. MediaPipe is chosen primarily for its efficiency and low computational load, enabling the system to run smoothly on edge devices without requiring dedicated hardware. Spoken input from a phone can also be translated into sign representations through a voice-to-sign component that maps detected speech to predefined phrases and produces corresponding GIFs or alphabet-based visualizations. For visually impaired users, the framework provides a screen free voice interface that integrates multilingual speech recognition, text summarization, and text-to-speech generation. These components work together through a Streamlit-based interface, making the system usable on both desktop and mobile environments. Overall, Sanvaad aims to offer a practical and accessible pathway for inclusive communication by combining lightweight computer vision and speech processing tools within a unified framework.

</details>


### [138] [Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion](https://arxiv.org/abs/2512.06504)
*Andrii Lysyi,Anatoliy Sachenko,Pavlo Radiuk,Mykola Lysyi,Oleksandr Melnychenko,Diana Zahorodnia*

Main category: cs.CV

> 研究开发了一个智能集成框架，用于自动化检测光伏基础设施，解决了传统方法中存在的热成像调色板偏差、数据冗余和高通信带宽需求等问题。

<details>
  <summary>Details</summary>

**Motivation:** 设计和开发一个从数据收集到生成可操作地理定位维护警报的自动化监控工作流的综合性多模式系统，旨在提高光伏基础设施的安全性和运行效率。

**Method:** 采用协同架构，包括调色板不变性热嵌入学习，通过门控机制与对比归一化的RGB流融合。使用罗德里格斯更新的闭环自适应重新采集控制器和基于DBSCAN的地理空间去重模块为目标确认模糊异常和聚类冗余警报。

**Result:** 在公开的PVF-10基准测试中达到了0.903的平均精度（mAP@0.5），比单模式基线提高了12-15%。现场测试证明了系统准备就绪，实现了96%的召回率，去冗余过程减少了15-20%的重复引起的假阳性，仅相关遥测数据将航空航天数据传输减少了60-70%。

**Conclusion:** 提出的系统建立了光伏主动检测的新范式，提高了效率和安全性，并取得了卓越的技术性能。

**Abstract:** The subject of this research is the development of an intelligent, integrated framework for the automated inspection of photovoltaic (PV) infrastructure that addresses the critical shortcomings of conventional methods, including thermal palette bias, data redundancy, and high communication bandwidth requirements. The goal of this study is to design, develop, and validate a comprehensive, multi-modal system that fully automates the monitoring workflow, from data acquisition to the generation of actionable, geo-located maintenance alerts, thereby enhancing plant safety and operational efficiency. The methods employed involve a synergistic architecture that begins with a palette-invariant thermal embedding, learned by enforcing representational consistency, which is fused with a contrast-normalized RGB stream via a gated mechanism. This is supplemented by a closed-loop, adaptive re-acquisition controller that uses Rodrigues-based updates for targeted confirmation of ambiguous anomalies and a geospatial deduplication module that clusters redundant alerts using DBSCAN over the haversine distance. In conclusion, this study establishes a powerful new paradigm for proactive PV inspection, with the proposed system achieving a mean Average Precision (mAP@0.5) of 0.903 on the public PVF-10 benchmark, a significant 12-15% improvement over single-modality baselines. Field validation confirmed the system's readiness, achieving 96% recall, while the de-duplication process reduced duplicate-induced false positives by 15-20%, and relevance-only telemetry cut airborne data transmission by 60-70%.

</details>


### [139] [ShadowWolf -- Automatic Labelling, Evaluation and Model Training Optimised for Camera Trap Wildlife Images](https://arxiv.org/abs/2512.06521)
*Jens Dede,Anna Förster*

Main category: cs.CV

> 本研究针对野生动物监测中AI模型面临的环境因素挑战，提出了名为ShadowWolf的框架，通过动态模型再训练适应环境变化，提高监测效率和准确性。

<details>
  <summary>Details</summary>

**Motivation:** 随着全球人口的持续增长，人类栖息地的扩张导致野生动物空间减少及人类与野生动物互动增加，人工智能在解决自动识别图像和视频中的动物问题方面提供了有效途径。然而，环境因素的多变性对模型的稳健性和适应性提出了挑战。

**Method:** 本研究提出了一种名为ShadowWolf的统一框架，旨在通过整合和优化AI模型训练及评估的阶段来应对这些挑战。该框架支持动态模型再训练，以适应环境变化和应用需求，从而减少标注工作量并允许现场模型适应。

**Result:** 未详细说明具体实验结果。

**Conclusion:** 这种自适应和统一的方法增强了野生动物监测系统的准确性和效率，促进了更有效和可扩展的保护工作。

**Abstract:** The continuous growth of the global human population is leading to the expansion of human habitats, resulting in decreasing wildlife spaces and increasing human-wildlife interactions. These interactions can range from minor disturbances, such as raccoons in urban waste bins, to more severe consequences, including species extinction. As a result, the monitoring of wildlife is gaining significance in various contexts. Artificial intelligence (AI) offers a solution by automating the recognition of animals in images and videos, thereby reducing the manual effort required for wildlife monitoring. Traditional AI training involves three main stages: image collection, labelling, and model training. However, the variability, for example, in the landscape (e.g., mountains, open fields, forests), weather (e.g., rain, fog, sunshine), lighting (e.g., day, night), and camera-animal distances presents significant challenges to model robustness and adaptability in real-world scenarios.
  In this work, we propose a unified framework, called ShadowWolf, designed to address these challenges by integrating and optimizing the stages of AI model training and evaluation. The proposed framework enables dynamic model retraining to adjust to changes in environmental conditions and application requirements, thereby reducing labelling efforts and allowing for on-site model adaptation. This adaptive and unified approach enhances the accuracy and efficiency of wildlife monitoring systems, promoting more effective and scalable conservation efforts.

</details>


### [140] [On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization](https://arxiv.org/abs/2512.06530)
*Mohammed Wattad,Tamir Shor,Alex Bronstein*

Main category: cs.CV

> 本文探讨了通过学习的k空间采样模式来改善加速MRI的重建质量，并提出可以通过引入采样不确定性来提高模型在不同成像条件下的鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的研究在优化单一数据集或成像模式方面取得了成就，但忽略了跨成像域的通用性问题。本文旨在扩展学习的k空间采样模式的益处，即使在跨域场景下也能提供出色的重建性能。

**Method:** 通过对多个数据集和采集方案进行系统评估，本文分析了用学习采样模式训练的模型在跨域场景下的泛化能力，并提出一种增强领域泛化能力的新方法——在训练过程中通过随机扰动k空间轨迹来模拟不同扫描仪和成像条件的变异性。

**Result:** 研究表明，学习采样模式训练的模型能够更好地泛化到不同的成像域，新的方法能够提高领域的鲁棒性。

**Conclusion:** 研究强调了k空间轨迹设计不仅是一个加速手段，还是改善MRI重建中领域泛化的主动自由度。

**Abstract:** Recent work has established learned k-space acquisition patterns as a promising direction for improving reconstruction quality in accelerated Magnetic Resonance Imaging (MRI). Despite encouraging results, most existing research focuses on acquisition patterns optimized for a single dataset or modality, with limited consideration of their transferability across imaging domains. In this work, we demonstrate that the benefits of learned k-space sampling can extend beyond the training domain, enabling superior reconstruction performance under domain shifts. Our study presents two main contributions. First, through systematic evaluation across datasets and acquisition paradigms, we show that models trained with learned sampling patterns exhibitimproved generalization under cross-domain settings. Second, we propose a novel method that enhances domain robustness by introducing acquisition uncertainty during training-stochastically perturbing k-space trajectories to simulate variability across scanners and imaging conditions. Our results highlight the importance of treating kspace trajectory design not merely as an acceleration mechanism, but as an active degree of freedom for improving domain generalization in MRI reconstruction.

</details>


### [141] [Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images](https://arxiv.org/abs/2512.06531)
*Sayan Das,Arghadip Biswas*

Main category: cs.CV

> 本文提出了两种用于脑肿瘤分类和分段的深度学习架构，通过实验验证了其在提高诊断准确性方面的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 由于近年来儿童和青少年脑肿瘤发病率上升，导致数据量剧增，手动检测费时费力。随着人工智能在医学领域的广泛应用，开发自动化的计算机辅助诊断（CAD）系统显得尤为重要，以便能准确地早期检测脑肿瘤。

**Method:** 本文提出了两种新颖的深度学习架构：(a) SAETCN（自注意力增强肿瘤分类网络），用于不同类型脑肿瘤的分类，在验证数据集上达到了99.38%的准确率；(b) SAS-Net（自注意分段网络）用于脑肿瘤的精准分段，总体像素准确率为99.23%。

**Result:** 所提出的SAETCN在包含三种类型的肿瘤（胶质瘤、脑膜瘤和垂体瘤）和非肿瘤病例的训练集上，达到了99.38%的验证数据准确率；SAS-Net达到了99.23%的总体像素准确率。

**Conclusion:** 该研究提出的两种深度学习框架在脑肿瘤的分类和分段方面取得了准确而优异的结果，证明了人工智能技术在这一医疗领域中的应用潜力。

**Abstract:** Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.

</details>


### [142] [Bridging spatial awareness and global context in medical image segmentation](https://arxiv.org/abs/2512.06560)
*Dalia Alzu'bi,A. Ben Hamza*

Main category: cs.CV

> U-CycleMLP, introduced as a novel U-shaped encoder-decoder network for medical image segmentation, improves segmentation accuracy and computational efficiency by incorporating innovative architectural components such as position attention weight excitation blocks and channel CycleMLP blocks. It outperforms existing models on three benchmark datasets, highlighting its superiority in capturing both local and global context in medical images.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve upon existing segmentation models' limitations, which often fail to effectively capture both local and global contextual information, resulting in errors such as boundary pixel loss and segmentation inaccuracies. The aim is to achieve a balance between segmentation accuracy and computational efficiency, crucial for clinical applications.

**Method:** Medical image segmentation is addressed using a novel U-shaped encoder-decoder network called U-CycleMLP. The model integrates position attention weight excitation blocks, dense atrous blocks, and channel CycleMLP blocks into its architecture to better capture local and global contextual information, enhancing segmentation accuracy. The decoder refines segmentation predictions by incorporating the channel CycleMLP blocks through skip connections, ensuring precise boundary delineation and maintaining linear computational complexity.

**Result:** The experimental outcomes indicate that U-CycleMLP outperforms state-of-the-art segmentation methods in terms of accuracy across all test datasets. It demonstrates robustness across different medical imaging modalities, confirming the value of its innovative architectural design.

**Conclusion:** The conclusion affirms the competitive performance of U-CycleMLP across three benchmark datasets, showcasing superior segmentation accuracy and the capability to capture fine-grained anatomical structures. Ablation studies validate the importance of the model's design choices, confirming the effectiveness of its core components in enhancing segmentation.

**Abstract:** Medical image segmentation is a fundamental task in computer-aided diagnosis, requiring models that balance segmentation accuracy and computational efficiency. However, existing segmentation models often struggle to effectively capture local and global contextual information, leading to boundary pixel loss and segmentation errors. In this paper, we propose U-CycleMLP, a novel U-shaped encoder-decoder network designed to enhance segmentation performance while maintaining a lightweight architecture. The encoder learns multiscale contextual features using position attention weight excitation blocks, dense atrous blocks, and downsampling operations, effectively capturing both local and global contextual information. The decoder reconstructs high-resolution segmentation masks through upsampling operations, dense atrous blocks, and feature fusion mechanisms, ensuring precise boundary delineation. To further refine segmentation predictions, channel CycleMLP blocks are incorporated into the decoder along the skip connections, enhancing feature integration while maintaining linear computational complexity relative to input size. Experimental results, both quantitative and qualitative, across three benchmark datasets demonstrate the competitive performance of U-CycleMLP in comparison with state-of-the-art methods, achieving better segmentation accuracy across all datasets, capturing fine-grained anatomical structures, and demonstrating robustness across different medical imaging modalities. Ablation studies further highlight the importance of the model's core architectural components in enhancing segmentation accuracy.

</details>


### [143] [SUGAR: A Sweeter Spot for Generative Unlearning of Many Identities](https://arxiv.org/abs/2512.06562)
*Dung Thuy Nguyen,Quang Nguyen,Preston K. Robinette,Eli Jiang,Taylor T. Johnson,Kevin Leach*

Main category: cs.CV

> SUGAR框架实现了高效率的身份移除，同时保持了模型的质量和多样性，并且没有显著退化。

<details>
  <summary>Details</summary>

**Motivation:** 解决用户同意问题以及从生成模型输出空间中移除特定个体的需求。

**Method:** SUGAR框架通过为每个身份学习个性化的替代潜在空间，避免了将不需要的身份投影到不现实输出或依赖静态模板脸的方法，从而实现身份移除而不必重新训练整个模型。此外，SUGAR引入了一个连续的效用保持目标，以防止随着更多身份被遗忘而出现的模型退化。

**Result:** SUGAR实现了去除多达200个身份的顶级性能，保留效用比现有基线提高了700%。

**Conclusion:** SUGAR框架提供了一种有效的解决方案，以解决从3D感知生成模型中移除特定个体的问题，同时保持了模型的高保真度和多样性。代码公开，证明了该方法的有效性和实用性。

**Abstract:** Recent advances in 3D-aware generative models have enabled high-fidelity image synthesis of human identities. However, this progress raises urgent questions around user consent and the ability to remove specific individuals from a model's output space. We address this by introducing SUGAR, a framework for scalable generative unlearning that enables the removal of many identities (simultaneously or sequentially) without retraining the entire model. Rather than projecting unwanted identities to unrealistic outputs or relying on static template faces, SUGAR learns a personalized surrogate latent for each identity, diverting reconstructions to visually coherent alternatives while preserving the model's quality and diversity. We further introduce a continual utility preservation objective that guards against degradation as more identities are forgotten. SUGAR achieves state-of-the-art performance in removing up to 200 identities, while delivering up to a 700% improvement in retention utility compared to existing baselines. Our code is publicly available at https://github.com/judydnguyen/SUGAR-Generative-Unlearn.

</details>


### [144] [GNC-Pose: Geometry-Aware GNC-PnP for Accurate 6D Pose Estimation](https://arxiv.org/abs/2512.06565)
*Xiujin Liu*

Main category: cs.CV

> GNC-Pose is a learning-free pipeline for 6D object pose estimation, characterized by its robustness and competitive accuracy compared to both learning-based and learning-free approaches.

<details>
  <summary>Details</summary>

**Motivation:** The aim is to implement a fully learning-free approach for 6D pose estimation in textured objects, ensuring robust estimation in the presence of outliers.

**Method:** We present GNC-Pose, which uses a rendering-based initialization and a geometry-aware correspondence weighting mechanism under the GNC principle to estimate 6D object poses. It introduces a cluster-based weighting method that evaluates the 3D structural consistency and applies a final LM refinement.

**Result:** The method achieves competitive accuracy on The YCB Object and Model Set without using learned features, training data or category-specific priors.

**Conclusion:** GNC-Pose provides a simple, robust, and practical solution for learning-free 6D pose estimation.

**Abstract:** We present GNC-Pose, a fully learning-free monocular 6D object pose estimation pipeline for textured objects that combines rendering-based initialization, geometry-aware correspondence weighting, and robust GNC optimization. Starting from coarse 2D-3D correspondences obtained through feature matching and rendering-based alignment, our method builds upon the Graduated Non-Convexity (GNC) principle and introduces a geometry-aware, cluster-based weighting mechanism that assigns robust per point confidence based on the 3D structural consistency of the model. This geometric prior and weighting strategy significantly stabilizes the optimization under severe outlier contamination. A final LM refinement further improve accuracy. We tested GNC-Pose on The YCB Object and Model Set, despite requiring no learned features, training data, or category-specific priors, GNC-Pose achieves competitive accuracy compared with both learning-based and learning-free methods, and offers a simple, robust, and practical solution for learning-free 6D pose estimation.

</details>


### [145] [MedGRPO: Multi-Task Reinforcement Learning for Heterogeneous Medical Video Understanding](https://arxiv.org/abs/2512.06581)
*Yuhao Su,Anwesa Choudhuri,Zhongpai Gao,Benjamin Planche,Van Nguyen Nguyen,Meng Zheng,Yuhan Shen,Arun Innanje,Terrence Chen,Ehsan Elhamifar,Ziyan Wu*

Main category: cs.CV

> 本文提出了MedVidBench，一个专注于医疗视频理解的大规模基准，包含531,850个视频-指令对。此外，还引入了MedGRPO，一种新的用于跨多个数据集平衡训练的强化学习框架，解决了标准强化学习中的优化失衡问题。通过这些创新，本文在医学视觉语言模型的多个任务上实现了更好的性能。

<details>
  <summary>Details</summary>

**Motivation:** 医学视频理解要求高度的空间精确性、时间推理和临床语义，但现有的大规模视觉-语言模型在这方面的表现不佳，因此需要一个新的基准和更强大的训练方法。

**Method:** 引入MedVidBench作为评估基准，并提出了MedGRPO框架，包括跨数据集奖励归一化和一个医疗LLM裁判，用以进行平衡多数据集训练。

**Result:** 监督微调Qwen2.5-VL-7B在MedVidBench上性能显著优于GPT-4.1和Gemini-2.5-Flash，特别是在基线任务和描述任务中，MedGRPO框架进一步提升了性能。

**Conclusion:** 本文的工作为医学领域中的视觉语言模型提供了一个基础性基准和强大的训练方法，并证明了其在多个任务上的有效性。

**Abstract:** Large vision-language models struggle with medical video understanding, where spatial precision, temporal reasoning, and clinical semantics are critical. To address this, we first introduce \textbf{MedVidBench}, a large-scale benchmark of 531,850 video-instruction pairs across 8 medical sources spanning video, segment, and frame-level tasks, curated through a rigorous quality assurance pipeline with expert-guided prompting and dual-model validation. While supervised fine-tuning on MedVidBench yields noticeable gains, standard Reinforcement Learning (RL) fails due to imbalanced reward scales across datasets, which destabilizes optimization and leads to training collapse. To overcome this, we introduce \textbf{MedGRPO}, a novel RL framework for balanced multi-dataset training with two key innovations: (1) \emph{cross-dataset reward normalization} that maps each dataset's median performance to a common reward value, ensuring fair optimization regardless of difficulty, and (2) a \emph{medical LLM judge} that evaluates caption quality on five clinical dimensions through comparative similarity scoring. Supervised fine-tuning Qwen2.5-VL-7B on MedVidBench substantially outperforms GPT-4.1 and Gemini-2.5-Flash across all tasks, demonstrating MedVidBench's efficacy, while our MedGRPO framework further improves upon the SFT baseline across grounding and captioning tasks. Our work establishes a foundational benchmark and robust training methodology for advancing vision-language models in medical domains. Our project website is available at https://yuhaosu.github.io/MedGRPO/.

</details>


### [146] [The Role of Entropy in Visual Grounding: Analysis and Optimization](https://arxiv.org/abs/2512.06726)
*Shuo Li,Jiajun Sun,Zhihao Zhang,Xiaoran Fan,Senjie Jin,Hui Li,Yuming Yang,Junjie Ye,Lixing Shen,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CV

> The paper introduces ECVGPO, an interpretable algorithm for entropy control in visual grounding tasks. It achieves broad improvements across benchmarks and models.

<details>
  <summary>Details</summary>

**Motivation:** To explore the role and characteristics of entropy in perception-oriented tasks, such as visual grounding, and to develop effective strategies for entropy control.

**Method:** The method involves analyzing entropy in visual grounding tasks compared to reasoning tasks, leading to the development of ECVGPO algorithm for effective entropy regulation.

**Result:** Experiments demonstrate that the ECVGPO algorithm achieves broad improvements across various benchmarks and models.

**Conclusion:** Effective entropy control via ECVGPO improves the performance of visual grounding tasks by better balancing the trade-off between exploration and exploitation.

**Abstract:** Recent advances in fine-tuning multimodal large language models (MLLMs) using reinforcement learning have achieved remarkable progress, particularly with the introduction of various entropy control techniques. However, the role and characteristics of entropy in perception-oriented tasks like visual grounding, as well as effective strategies for controlling it, remain largely unexplored. To address this issue, we focus on the visual grounding task and analyze the role and characteristics of entropy in comparison to reasoning tasks. Building on these findings, we introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization), an interpretable algorithm designed for effective entropy regulation. Through entropy control, the trade-off between exploration and exploitation is better balanced. Experiments show that ECVGPO achieves broad improvements across various benchmarks and models.

</details>


### [147] [From Remote Sensing to Multiple Time Horizons Forecasts: Transformers Model for CyanoHAB Intensity in Lake Champlain](https://arxiv.org/abs/2512.06598)
*Muhammad Adil,Patrick J. Clemins,Andrew W. Schroth,Panagiotis D. Oikonomou,Donna M. Rizzo,Peter D. F. Isles,Xiaohan Zhang,Kareem I. Hannoun,Scott Turnbull,Noah B. Beckage,Asim Zia,Safwan Wshah*

Main category: cs.CV

> 研究开发了一种仅基于遥感数据，结合Transformers和BiLSTM的模型来预测蓝藻大量繁殖事件，展示出良好的预测性能，提供了可靠的早期预警。

<details>
  <summary>Details</summary>

**Motivation:** 该论文旨在解决阿米戈湖反复发生的蓝藻有害大量繁殖事件的监测和预测问题，为管理提供可靠的早期预警。

**Method:** 该研究提出了一个仅基于遥感数据的预测框架，结合了Transformers和BiLSTM来预测蓝藻大量繁殖的强度，预测时间可达14天。数据预处理包括前向填充和加权时间插值，以及平滑处理。特征提取包括基于频率等分的细胞色素指数值和提取的温度统计值。

**Result:** 模型在多个预测时间范围内的预测性能表现良好，一天、两天和三天预测的F1分数分别为89.5%、86.4%和85.5%，在14天预测范围内的F1分数为78.9%，AUC为82.6%。

**Conclusion:** 研究表明，结合了Transformers和BiLSTM的模型能够从稀疏的卫星数据中捕捉复杂的空间和时间动态，为蓝藻有害大量繁殖事件的管理提供可靠的早期预警。

**Abstract:** Cyanobacterial Harmful Algal Blooms (CyanoHABs) pose significant threats to aquatic ecosystems and public health globally. Lake Champlain is particularly vulnerable to recurring CyanoHAB events, especially in its northern segment: Missisquoi Bay, St. Albans Bay, and Northeast Arm, due to nutrient enrichment and climatic variability. Remote sensing provides a scalable solution for monitoring and forecasting these events, offering continuous coverage where in situ observations are sparse or unavailable. In this study, we present a remote sensing only forecasting framework that combines Transformers and BiLSTM to predict CyanoHAB intensities up to 14 days in advance. The system utilizes Cyanobacterial Index data from the Cyanobacterial Assessment Network and temperature data from Moderate Resolution Imaging Spectroradiometer satellites to capture long range dependencies and sequential dynamics in satellite time series. The dataset is very sparse, missing more than 30% of the Cyanobacterial Index data and 90% of the temperature data. A two stage preprocessing pipeline addressed data gaps by applying forward fill and weighted temporal imputation at the pixel level, followed by smoothing to reduce the discontinuities of CyanoHAB events. The raw dataset is transformed into meaningful features through equal frequency binning for the Cyanobacterial Index values and extracted temperature statistics. Transformer BiLSTM model demonstrates strong forecasting performance across multiple horizons, achieving F1 scores of 89.5%, 86.4%, and 85.5% at one, two, and three-day forecasts, respectively, and maintaining an F1 score of 78.9% with an AUC of 82.6% at the 14-day horizon. These results confirm the model's ability to capture complex spatiotemporal dynamics from sparse satellite data and to provide reliable early warning for CyanoHABs management.

</details>


### [148] [MMDuet2: Enhancing Proactive Interaction of Video MLLMs with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2512.06810)
*Yueqian Wang,Songxiang Liu,Disong Wang,Nuo Xu,Guanglu Wan,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

> 我们提出了MMDuet2，一种新颖的文本到文本方法，用于视频多模态语言模型中的主动式对话，通过多轮强化学习方法训练，实现了更高质量和更准确时机的回复。

<details>
  <summary>Details</summary>

**Motivation:** 尽管现有的大多数系统都是在用户回合之后才能回复的轮流方式进行操作，但主动决定在视频播放过程中何时回复对于实时应用来说是一个有前途但具有挑战性的方向。

**Method:** 我们提出了一种新颖的文本到文本的方法来进行主动交互，使得模型可以根据对话历史和视频的当前帧的视觉上下文自主决定是否在每一轮回复或保持沉默。为了克服以前方法中手动调整回复决策阈值和精确标注回复时间的困难，我们引入了一种多轮强化学习训练方法，鼓励及时和准确的回复，而无需精确的回复时间标注。

**Result:** 我们在包含52,000个视频的两个类型的对话数据集上使用监督微调和强化学习训练了我们的模型MMDuet2。实验结果显示，MMDuet2在响应时机和质量上超越了现有的主动视频多模态大语言模型基线，实现了在ProactiveVideoQA基准上的最新技术水平。

**Conclusion:** 该研究展示了多轮强化学习在训练视频多模态语言模型中的潜力，该模型能够在多轮对话中主动决定是否回复，并在现有的基准测试中达到了最先进的效果。

**Abstract:** Recent advances in video multimodal large language models (Video MLLMs) have significantly enhanced video understanding and multi-modal interaction capabilities. While most existing systems operate in a turn-based manner where the model can only reply after user turns, proactively deciding when to reply during video playback presents a promising yet challenging direction for real-time applications. In this work, we propose a novel text-to-text approach to proactive interaction, where the model autonomously determines whether to respond or remain silent at each turn based on dialogue history and visual context up to current frame of an streaming video. To overcome difficulties in previous methods such as manually tuning response decision thresholds and annotating precise reply times, we introduce a multi-turn RL based training method that encourages timely and accurate responses without requiring precise response time annotations. We train our model MMDuet2 on a dataset of 52k videos with two types of dialogues via SFT and RL. Experimental results demonstrate that MMDuet2 outperforms existing proactive Video MLLM baselines in response timing and quality, achieving state-of-the-art performance on the ProactiveVideoQA benchmark.

</details>


### [149] [Learning Relative Gene Expression Trends from Pathology Images in Spatial Transcriptomics](https://arxiv.org/abs/2512.06612)
*Kazuya Nishimura,Haruka Hirose,Ryoma Bise,Kaito Shiku,Yasuhiro Kojima*

Main category: cs.CV

> The paper introduces STRank, a new loss function for learning relative gene expression patterns from pathology images, which is robust to noise and batch effects, improving accuracy over traditional methods.

<details>
  <summary>Details</summary>

**Motivation:** To reduce the cost associated with RNA sequencing and improve accuracy in gene expression estimation from pathology images, given the challenges of stochastic noise and batch effects in observed gene expression.

**Method:** We propose learning relative gene expression patterns rather than absolute levels due to issues with stochastic noise and batch effects. A novel loss function, STRank, is introduced to model this relation and is robust to noise and batch effects.

**Result:** Experiments on both synthetic and real datasets confirm the effectiveness of the proposed method.

**Conclusion:** STRank effectively addresses the issue of estimating gene expression from pathology images by focusing on relative expression patterns instead of absolute values, offering a solution that is resilient to noise and batch effects.

**Abstract:** Gene expression estimation from pathology images has the potential to reduce the RNA sequencing cost. Point-wise loss functions have been widely used to minimize the discrepancy between predicted and absolute gene expression values. However, due to the complexity of the sequencing techniques and intrinsic variability across cells, the observed gene expression contains stochastic noise and batch effects, and estimating the absolute expression values accurately remains a significant challenge. To mitigate this, we propose a novel objective of learning relative expression patterns rather than absolute levels. We assume that the relative expression levels of genes exhibit consistent patterns across independent experiments, even when absolute expression values are affected by batch effects and stochastic noise in tissue samples. Based on the assumption, we model the relation and propose a novel loss function called STRank that is robust to noise and batch effects. Experiments using synthetic datasets and real datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/naivete5656/STRank.

</details>


### [150] [Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior](https://arxiv.org/abs/2512.06866)
*Yulin Li,Haokun Gui,Ziyang Fan,Junjie Wang,Bin Kang,Bin Chen,Zhuotao Tian*

Main category: cs.CV

> 提出了DyToK方法，利用VLLMs的内在注意力机制压缩视频中的视觉token，提高长视频理解的计算效率，同时保持高准确性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管视频大语言模型(VLLMs)在视频理解方面表现出色，但处理长视频时面临计算效率问题。现有关键帧采样方法虽然提高了时间建模效率，但也引入了额外的计算成本，且二进制帧选择方法效果不佳。

**Method:** DyToK通过使用VLLMs的注意力机制动态调整每帧的token保留比例，优先保留语义丰富的帧并抑制冗余。这种方法无需训练，能够和现有压缩方法兼容。

**Result:** 实验表明DyToK达到了最先进的效率与准确性的权衡，使推理速度提升4.3倍同时在多个VLLM上保持准确性。

**Conclusion:** DyToK是一种无需训练的动态token压缩方案，可以通过利用VLLMs的注意力机制提高视频处理的效率和准确性。

**Abstract:** Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at https://github.com/yu-lin-li/DyToK .

</details>


### [151] [Hierarchical Deep Learning for Diatom Image Classification: A Multi-Level Taxonomic Approach](https://arxiv.org/abs/2512.06613)
*Yueying Ke*

Main category: cs.CV

> This paper presents a hierarchical neural network for diatom classification, improving accuracy and localization of errors across taxonomic ranks.

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy and error locality in the taxonomic identification of diatoms by integrating the taxonomic hierarchy into neural network architectures.

**Method:** We introduce a hierarchical convolutional network with five cascaded heads for taxonomic classification of diatoms, each head predicting a different taxonomic rank (class, order, family, genus, species). Binary masks ensure that predictions are valid descendants.

**Result:** The hierarchical model performs equally to flat models at species level (69.4% accuracy) and better at higher taxonomic levels, with errors remaining taxonomically local. It reduces mean taxonomic distance by 38.2% compared to flat baselines.

**Conclusion:** The hierarchical model produces more robust, interpretable, and biologically aligned predictions for multi-level taxonomic classification than flat models, suggesting a significant advancement in automated diatom identification.

**Abstract:** Accurate taxonomic identification of diatoms is essential for aquatic ecosystem monitoring, yet conventional methods depend heavily on expert taxonomists. Recent deep learning approaches improve automation, but most treat diatom recognition as flat classification predicting only one taxonomic rank. We investigate whether embedding taxonomic hierarchy into neural network architectures can improve both accuracy and error locality.
  We introduce a hierarchical convolutional network with five cascaded heads that jointly predict class, order, family, genus, and species. Each head receives shared backbone features and probability distributions from higher levels, with binary masks restricting predictions to valid descendants during training and inference. Using a filtered dataset of 1,456 diatom images covering 82 species, we compare hierarchical and flat models under identical settings.
  The hierarchical model matches flat baselines at species level (69.4% accuracy) while outperforming at all upper taxonomic levels. When species predictions fail, errors remain taxonomically local: 92.5 % of misclassified species are correctly predicted at genus level, versus 67.2% for flat baselines. The hierarchical model reduces mean taxonomic distance by 38.2% (1.209 vs. 1.955).
  Progressive training reveals bidirectional mechanisms: hierarchical constraint masks operate top-down to constrain prediction space, while gradients from fine-grained levels propagate bottom-up through the shared backbone, refining features. This improves class accuracy from 96.2% to 99.5% and yields 6-8% gains at upper levels, producing more robust, interpretable, and biologically aligned predictions for multi-level taxonomic classification.

</details>


### [152] [NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification](https://arxiv.org/abs/2512.06921)
*Ziyang Song,Zelin Zang,Xiaofan Ye,Boqiang Xu,Long Bai,Jinlin Wu,Hongliang Ren,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.CV

> 本研究提出了一份多模态基准测试（NeuroABench），用于评估神经外科领域中的解剖理解，并发现现有技术仍显著落后于人类专家水平。

<details>
  <summary>Details</summary>

**Motivation:** 尽管多模态大型语言模型在手术视频理解领域表现出色，但现有研究和数据集主要关注手术流程的理解，忽视了解剖理解的关键作用。为了弥补这一不足，本研究提出了NeuroABench。

**Method:** 本研究提出了Neurosurgical Anatomy Benchmark (NeuroABench)，这是一个针对神经外科解剖理解的多模态基准测试。该基准由9小时的标注神经外科视频组成，包含了89种不同的手术程序，使用一种新型多模态标注流水线和多个回顾周期开发。

**Result:** 实验结果表明，最先进的多模态大型语言模型在解剖识别任务中表现出显著的限制性，最佳模型精度仅为40.87%。与神经外科实习医生的测试结果比较，最佳模型的表现接近最低分学生，但明显落后于平均表现。

**Conclusion:** 本研究强调了多模态大型语言模型在解剖理解上取得的进步和仍然存在的显著差距。

**Abstract:** Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group's average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.

</details>


### [153] [Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution](https://arxiv.org/abs/2512.06642)
*Achmad Ardani Prasha,Clavino Ourizqi Rachmadi,Muhamad Fauzan Ibnu Syahlan,Naufal Rahfi Anugerah,Nanda Garin Raditya,Putri Amelia,Sabrina Laila Mutiara,Hilman Syachr Ramadhan*

Main category: cs.CV

> 本文提出了一种在强引力透镜图像中使用掩码自编码器进行预训练的方法，结果表明，通过此方法预训练的模型在分类和图像超分辨率任务上表现出色，并展示了MAE预训练方法的灵活性和可重用性。

<details>
  <summary>Details</summary>

**Motivation:** 强引力透镜效应可以揭示星系中暗物质子结构的影响，但分析这些效应存在困难，因为信号噪杂且分辨率低。本文旨在提出一种有效的预训练策略来解决这一问题。

**Method:** 本文提出了一种基于掩码自编码器（MAE）的预训练策略，应用于DeepLense ML4SCI基准库中的模拟强引力透镜图像，以学习可泛化的表示，用于两个下游任务：1) 对基础暗物质模型进行分类（冷暗物质、轴子类似物或无子结构）；2) 通过超分辨率增强低分辨率的镜像图像。首先使用掩码图像建模目标预训练Vision Transformer编码器，然后分别针对每个任务调整编码器。

**Result:** 特别是在90%掩码比率的情况下，调整后的分类器实现了0.968的宏AUC值和88.65%的准确率，优于从零开始训练的基线（AUC 0.957，准确率82.46%）。对于超分辨率任务，该模型在从16x16放大到64x64时可以重建出PSNR约为33 dB和SSIM 0.961的图像，并在一定程度上优于从零开始训练的结果。

**Conclusion:** 研究发现，通过适当的掩码比率调整，MAE预训练方法可以获得一个共享编码器，该编码器的性能与从零开始训练的ViT相匹配或超过。这展示了基于物理丰富的模拟进行MAE预训练的灵活性和重新使用性。

**Abstract:** Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.

</details>


### [154] [Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models](https://arxiv.org/abs/2512.07141)
*Fenghua Weng,Chaochao Lu,Xia Hu,Wenqi Shao,Wenjie Wang*

Main category: cs.CV

> 提出了一种名为Think-Reflect-Revise (TRR)的三阶段训练框架，通过政策指导的自我反思增强大型视觉语言模型的安全对齐。实验表明，TRR显著提高了模型的安全性能，同时保持了在通用基准上的稳定性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有单次通过的思考-回答模式对上下文或视觉越狱攻击仍脆弱，提出通过反思自我修正以提高安全性。

**Method:** 构建了包含5,000个跟随思考-反思-修正过程的Reflective Safety Reasoning (ReSafe) 数据集，并通过该数据集和强化学习对目标模型进行初始化和反思行为加强。

**Result:** TRR 提高了Qwen2.5-VL-7B的安全响应率，从42.8%提升至87.7%，同时保持了对通用基准的稳定性能。

**Conclusion:** 实验表明，TRR框架大幅提升了LVLMs的安全性能，不仅改善了安全性，还保持了模型的通用性能。

**Abstract:** As multimodal reasoning improves the overall capabilities of Large Vision Language Models (LVLMs), recent studies have begun to explore safety-oriented reasoning, aiming to enhance safety awareness by analyzing potential safety risks during the reasoning process before generating the final response. Although such approaches improve safety awareness and interpretability, this single-pass think-then-answer paradigm remains vulnerable to contextual or visual jailbreak attacks. This reveals a critical flaw: single-pass reasoning may overlook explicit harmful content in its own output. Our key insight is to exploit this wasted signal through reflection, which can effectively leverage the malicious content revealed in the first-pass reasoning to enable genuine self-correction and prevent unsafe generations. Motivated by this, we propose Think-Reflect-Revise (TRR), a three-stage training framework designed to enhance the safety alignment of LVLMs through policy-guided self-reflection. We first build a Reflective Safety Reasoning (ReSafe) dataset with 5,000 examples that follow a think-reflect-revise process. We then fine-tune the target model using the ReSafe dataset to initialize reflective behavior, and finally reinforce policy-guided reflection through reinforcement learning. Experimental results show that TRR substantially improves the safety performance of LVLMs across both safety-awareness benchmarks and jailbreak attack evaluations, increasing the overall safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B, while preserving stable performance on general benchmarks such as MMMU and MMStar. The project page is available at https://think-reflect-revise.github.io/.

</details>


### [155] [TextMamba: Scene Text Detector with Mamba](https://arxiv.org/abs/2512.06657)
*Qiyan Zhao,Yue Yan,Da-Han Wang*

Main category: cs.CV

> The paper introduces a new scene text detector that integrates Mamba's state space model with Transformer attention layers, achieving state-of-the-art performance through improved long-range dependency modeling and multi-scale feature fusion.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the limitations of traditional Transformer-based methods in scene text detection, which often forget important information or focus on irrelevant representations when modeling long-range dependencies.

**Method:** The paper proposes a novel scene text detector that integrates the state space model Mamba with attention layers, utilizing the Top_k algorithm to select key information and reduce irrelevant information interference. It also introduces a dual-scale feed-forward network and an embedding pyramid enhancement module to improve high-dimensional hidden state interactions and multi-scale feature fusion.

**Result:** The proposed method achieves state-of-the-art or competitive performance on various benchmarks, with F-measures of 89.7%, 89.2%, and 78.5% on CTW1500, TotalText, and ICDAR19ArT, respectively.

**Conclusion:** The integration of Mamba with attention layers and the introduction of key information selection and multi-scale feature fusion techniques significantly improves the scene text detection performance over existing methods.

**Abstract:** In scene text detection, Transformer-based methods have addressed the global feature extraction limitations inherent in traditional convolution neural network-based methods. However, most directly rely on native Transformer attention layers as encoders without evaluating their cross-domain limitations and inherent shortcomings: forgetting important information or focusing on irrelevant representations when modeling long-range dependencies for text detection. The recently proposed state space model Mamba has demonstrated better long-range dependencies modeling through a linear complexity selection mechanism. Therefore, we propose a novel scene text detector based on Mamba that integrates the selection mechanism with attention layers, enhancing the encoder's ability to extract relevant information from long sequences. We adopt the Top\_k algorithm to explicitly select key information and reduce the interference of irrelevant information in Mamba modeling. Additionally, we design a dual-scale feed-forward network and an embedding pyramid enhancement module to facilitate high-dimensional hidden state interactions and multi-scale feature fusion. Our method achieves state-of-the-art or competitive performance on various benchmarks, with F-measures of 89.7\%, 89.2\%, and 78.5\% on CTW1500, TotalText, and ICDAR19ArT, respectively. Codes will be available.

</details>


### [156] [Generating Storytelling Images with Rich Chains-of-Reasoning](https://arxiv.org/abs/2512.07198)
*Xiujie Song,Qi Jia,Shota Watanabe,Xiaoyi Pang,Ruijie Chen,Mengyue Wu,Kenny Q. Zhu*

Main category: cs.CV

> 该论文提出了一种名为StorytellingPainter的两阶段管道，结合大语言模型和文本到图像模型来生成叙事图像，并开发了一个评估框架，同时提出了名为Mini-Storytellers的轻量级模型以减少开源和专有语言模型之间的性能差距，实验显示了方法的可行性和有效性。

<details>
  <summary>Details</summary>

**Motivation:** 故事讲述图像是指能够展示丰富、逻辑上相互关联的视觉线索的图像，这类图像因复杂的语义性质而难以创作。研究目的是通过人工智能生成模型来进行此类图像的创作。

**Method:** 提出了名为StorytellingPainter的两阶段管道，结合大语言模型的创造性推理能力和文本到图像模型的视觉合成能力来生成叙事图像。此外，设计了一个由三个主要评估器组成的评估框架，并提出了名为Mini-Storytellers的轻量级模型。

**Result:** 实验结果展示了所提出方法的可行性和有效性。

**Conclusion:** 该研究显示了通过结合语言模型和图像合成技术来生成叙事图像的新途径，以及开发轻量级模型来减少语言模型性能差距的可能性。

**Abstract:** An image can convey a compelling story by presenting rich, logically connected visual clues. These connections form Chains-of-Reasoning (CoRs) within the image, enabling viewers to infer events, causal relationships, and other information, thereby understanding the underlying story. In this paper, we focus on these semantically rich images and define them as Storytelling Images. Such images have diverse applications beyond illustration creation and cognitive screening, leveraging their ability to convey multi-layered information visually and inspire active interpretation. However, due to their complex semantic nature, Storytelling Images are inherently challenging to create, and thus remain relatively scarce. To address this challenge, we introduce the Storytelling Image Generation task, which explores how generative AI models can be leveraged to create such images. Specifically, we propose a two-stage pipeline, StorytellingPainter, which combines the creative reasoning abilities of Large Language Models (LLMs) with the visual synthesis capabilities of Text-to-Image (T2I) models to generate Storytelling Images. Alongside this pipeline, we develop a dedicated evaluation framework comprising three main evaluators: a Semantic Complexity Evaluator, a KNN-based Diversity Evaluator and a Story-Image Alignment Evaluator. Given the critical role of story generation in the Storytelling Image Generation task and the performance disparity between open-source and proprietary LLMs, we further explore tailored training strategies to reduce this gap, resulting in a series of lightweight yet effective models named Mini-Storytellers. Experimental results demonstrate the feasibility and effectiveness of our approaches. The code is available at https://github.com/xiujiesong/StorytellingImageGeneration.

</details>


### [157] [Personalized Image Descriptions from Attention Sequences](https://arxiv.org/abs/2512.06662)
*Ruoyu Xue,Hieu Le,Jingyi Xu,Sounak Mondal,Abe Leite,Gregory Zelinsky,Minh Hoai,Dimitris Samaras*

Main category: cs.CV

> DEPER 通过学习个人的语言风格和观看行为来生成更符合人类感知的图像描述。

<details>
  <summary>Details</summary>

**Motivation:** 现有的个性化图像描述模型只关注语言风格，而忽视了个人观看模式的影响。该论文通过显式地将个性化观看行为作为描述生成的核心因素来填补这一空白。

**Method:** DEPER (DEscription-PERception persona encoder) 使用辅助注意力预测任务学习包含语言风格和个人观察能力的主题嵌入，并通过轻量级适配器将其与冻结的视觉语言模型对齐，以实现少量的个性化调整，无需重新训练。

**Result:** 在四个涵盖不同观察任务和描述长度的数据集中，DEPER 平均提升 24%，验证了个性化注意力对高质量描述生成的重要性。

**Conclusion:** 通过理解人们的视觉方式，可以预测他们的描述风格，这能提高多模态系统的性能和与人类行为的对齐。

**Abstract:** People can view the same image differently: they focus on different regions, objects, and details in varying orders and describe them in distinct linguistic styles. This leads to substantial variability in image descriptions. However, existing models for personalized image description focus on linguistic style alone, with no prior work leveraging individual viewing patterns. We address this gap by explicitly modeling personalized viewing behavior as a core factor in description generation. Our method, DEPER (DEscription-PERception persona encoder), learns a subject embedding that captures both linguistic style and viewing behavior, guided by an auxiliary attention-prediction task. A lightweight adapter aligns these embeddings with a frozen vision-language model, enabling few-shot personalization without retraining. Across four datasets spanning diverse viewing tasks and both short and detailed descriptions, DEPER achieves a 24% average improvement, showing that modeling personalized attention produces more human-aligned and high-quality descriptions. We posit that understanding how people see helps predict what they say; modeling human diversity in perception can improve both performance and human alignment in multimodal systems.

</details>


### [158] [Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models](https://arxiv.org/abs/2512.07564)
*Kassoum Sanogo,Renzo Ardiccioni*

Main category: cs.CV

> A training-free self-correction framework uses multidimensional uncertainty quantification and attention-guided cropping to reduce hallucinations and improve accuracy in VLMs.

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of VLMs generating plausible but incorrect claims about image content, we aim to reduce hallucination rates through a training-free self-correction framework.

**Method:** Our method combines multidimensional uncertainty quantification with attention-guided cropping of under-explored regions to iteratively refine responses of VLMs without retraining.

**Result:** Experimental results demonstrate a reduction in hallucination rates by 9.8 percentage points and an improvement in object existence accuracy by 4.7 points on adversarial splits, compared to the baseline.

**Conclusion:** Our approach successfully grounds corrections in visual evidence where standard decoding fails and can be applied on frozen, pretrained VLMs without requiring any further training.

**Abstract:** Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.

</details>


### [159] [CoT4Det: A Chain-of-Thought Framework for Perception-Oriented Vision-Language Tasks](https://arxiv.org/abs/2512.06663)
*Yu Qi,Yumeng Zhang,Chenting Gong,Xiao Tan,Weiming Zhang,Wei Zhang,Jingdong Wang*

Main category: cs.CV

> CoT4Det enhances perception capabilities of LVLMs significantly by decomposing tasks into classification, counting, and grounding, achieving notable improvements on COCO2017 and other benchmarks.

<details>
  <summary>Details</summary>

**Motivation:** To overcome LVLMs' performance limitations on perception-centric tasks such as object detection, which are currently subpar compared to specialized models.

**Method:** Chain-of-Thought for Detection (CoT4Det) reformulates perception tasks into classification, counting, and grounding steps, leveraging the reasoning capabilities of Large Vision-Language Models (LVLMs).

**Result:** The method significantly improves perception performance, boosting mAP from 19.0% to 33.0% on COCO2017 val, with gains of +2% on RefCOCO benchmarks and 19% on Flickr30k entities.

**Conclusion:** The proposed CoT4Det strategy effectively bridges the performance gap between LVLMs and task-specific expert models in perception-centric tasks without negatively impacting general vision-language performance.

**Abstract:** Large Vision-Language Models (LVLMs) have demonstrated remarkable success in a broad range of vision-language tasks, such as general visual question answering and optical character recognition (OCR). However, their performance on perception-centric tasks -- such as object detection, semantic segmentation, and depth estimation -- remains significantly inferior to that of task-specific expert models. For example, Qwen2.5-VL-7B-Instruct achieves only 19% mAP on COCO2017 val, particularly struggling with dense scenes and small object recall. In this work, we introduce Chain-of-Thought for Detection (CoT4Det), a simple but efficient strategy that reformulates perception tasks into three interpretable steps: classification, counting, and grounding -- each more naturally aligned with the reasoning capabilities of LVLMs. Extensive experiments demonstrate that our method significantly improves perception performance without compromising general vision language capabilities. With a standard Qwen2.5-VL-7B-Instruct, CoT4Det boosts mAP from 19.0% to 33.0% on COCO2017 val and achieves competitive results across a variety of perception benchmarks, outperforming baselines by +2% on RefCOCO series and 19% on Flickr30k entities.

</details>


### [160] [1 + 1 > 2: Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning](https://arxiv.org/abs/2512.06673)
*Shida Gao,Feng Xue,Xiangfeng Wang,Anlong Ming,Teng Long,Yihua Shao,Haozhe Wang,Zhaowen Lin,Wei Wang,Nicu Sebe*

Main category: cs.CV

> 本文提出了DEViL模型，通过结合视频语言模型和OVD来改善视频中的事件空间和时间定位，并引入了TTReg技术确保目标对象的时序一致性，实验结果显示了其在多种视频理解任务中的优势。

<details>
  <summary>Details</summary>

**Motivation:** 传统的多模态语言模型在处理视频中事件的空间定位时面临输出序列过长、定位结果逐渐漂移等问题。为解决这些问题，本研究提出了新的模型和方法。

**Method:** 本文提出了一种名为Detector-Empowered Video LLM (DEViL) 的模型，它结合了视频语言模型和open-vocabulary检测器(OVD)。这种方法通过参考语义标记(RST)连接MLLM和检测器，RST将用户查询转化为丰富的语义表示，不仅作为一个控制信号，也替代了OVD的文本嵌入，从而实现指代理解和空间定位的端到端学习。此外，还提出了管状时间正则化(TTReg)，确保目标对象的时序一致性。

**Result:** 实验结果显示，DEViL模型在多种细粒度视频理解任务中表现出色，尤其是在STVG和GroundedVQA两个任务上优于现有的解决方法。

**Conclusion:** 实验表明，DEViL在各种细粒度视频理解任务中表现优异，尤其是空间时间视频接地(STVG)和接地视觉问答(GroundedVQA)任务。

**Abstract:** Spatio-temporal grounding and reasoning aims to locate the temporal segment and spatial region of an event in a video given a user query, while also reasoning about semantics such as causality, temporal order, and action relationships. To achieve this, current MLLMs primarily treats bounding boxes as text tokens and generates them autoregressively. However, such autoregressive spatial decoding leads to very-long output sequences, causing spatial errors to accumulated over time and the localization results to progressively drift across a video. To address this, we present a Detector-Empowered Video LLM, short for DEViL, which couples a Video LLM with an open-vocabulary detector (OVD). Specifically, the MLLM and detector are connected via a reference-semantic token (RST) that distills the user query into a rich semantic representation. Unlike tokens that merely serve as spatial prompts or segmentor switches, the RST functions as both a control signal and a replacement for the OVD's text embedding, enabling end-to-end learning of both referential understanding and spatial localization. Furthermore, we propose a tube-mined temporal regularization (TTReg) within OVD, which drives the OVD to generate temporally-consistent queries for target objects, thereby ensuring effective temporal association. Experiments demonstrate that DEViL achieves strong performance across various fine-grained video understanding tasks, particularly STVG and GroundedVQA. Code will be released on https://github.com/gaostar123/DeViL.

</details>


### [161] [RunawayEvil: Jailbreaking the Image-to-Video Generative Models](https://arxiv.org/abs/2512.06674)
*Songping Wang,Rufan Qian,Yueming Lyu,Qinglong Liu,Linzhuang Zou,Jie Qin,Songhua Liu,Caifeng Shan*

Main category: cs.CV

> 本文介绍RunawayEvil，一种针对I2V模型的多模态逃狱框架，具有自我进化的攻击能力，能够在不需人为干预的情况下不断自我优化，有效提升了攻击成功率。

<details>
  <summary>Details</summary>

**Motivation:** 图像到视频生成系统在安全性和易受逃狱攻击方面研究不足，该工作旨在填补这一空白，提供一种分析I2V模型漏洞的工具。

**Method:** 提出RunawayEvil框架，这是一个具有动态进化能力的多模态逃狱框架，基于"策略-战术-行动"范式，包含策略感知指令单元、多模态战术策划单元和战术行动单元，实现自我放大式攻击。

**Result:** 实验表明，RunawayEvil在如Open-Sora 2.0和CogVideoX等商用I2V模型上实现了最高的攻击成功率，相比现有方法在COCO2017数据集上提高了58.5%至79%。

**Conclusion:** 该工作首次提出一种能够自动适应和加强攻击策略的策略框架，为构建更稳健的视频生成系统提供基础。

**Abstract:** Image-to-Video (I2V) generation synthesizes dynamic visual content from image and text inputs, providing significant creative control. However, the security of such multimodal systems, particularly their vulnerability to jailbreak attacks, remains critically underexplored. To bridge this gap, we propose RunawayEvil, the first multimodal jailbreak framework for I2V models with dynamic evolutionary capability. Built on a "Strategy-Tactic-Action" paradigm, our framework exhibits self-amplifying attack through three core components: (1) Strategy-Aware Command Unit that enables the attack to self-evolve its strategies through reinforcement learning-driven strategy customization and LLM-based strategy exploration; (2) Multimodal Tactical Planning Unit that generates coordinated text jailbreak instructions and image tampering guidelines based on the selected strategies; (3) Tactical Action Unit that executes and evaluates the multimodal coordinated attacks. This self-evolving architecture allows the framework to continuously adapt and intensify its attack strategies without human intervention. Extensive experiments demonstrate RunawayEvil achieves state-of-the-art attack success rates on commercial I2V models, such as Open-Sora 2.0 and CogVideoX. Specifically, RunawayEvil outperforms existing methods by 58.5 to 79 percent on COCO2017. This work provides a critical tool for vulnerability analysis of I2V models, thereby laying a foundation for more robust video generation systems.

</details>


### [162] [EMGauss: Continuous Slice-to-3D Reconstruction via Dynamic Gaussian Modeling in Volume Electron Microscopy](https://arxiv.org/abs/2512.06684)
*Yumeng He,Zanwei Zhou,Yekun Zheng,Chen Liang,Yunbo Wang,Xiaokang Yang*

Main category: cs.CV

> 介绍了EMGauss，一种用于从平面扫描的2D切片中重建3D结构的一般框架，特别适用于体积电子显微镜（vEM）的3D重建问题，克服了基于同轴度方法的局限性。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有的深度学习方法在处理由体积电子显微镜（vEM）扫描得到的低轴向分辨率的体积数据时失效的问题，并提供一种比传统方法更有效的3D重建策略。

**Method:** EMGauss, 一种基于高斯插值的3D重建框架，它将切片到3D的重建视为基于高斯插值的三维动态场景渲染问题，并引入了老师-学生自助机制来增强数据稀疏条件下的重建保真度。

**Result:** 通过将3D重建框架应用于vEM，EMGauss能够绕过同轴度方法固有的限制，大幅度提高插值质量，并得以实现连续切片合成等其他优势。

**Conclusion:** 与扩散模型和生成对抗网络（GAN）的重建方法相比，EMGauss显著提高了插值质量，实现了连续切片合成，并且无需大规模预训练。此外，它还可能为不同的成像领域提供切片到3D的一般解决方案。

**Abstract:** Volume electron microscopy (vEM) enables nanoscale 3D imaging of biological structures but remains constrained by acquisition trade-offs, leading to anisotropic volumes with limited axial resolution. Existing deep learning methods seek to restore isotropy by leveraging lateral priors, yet their assumptions break down for morphologically anisotropic structures. We present EMGauss, a general framework for 3D reconstruction from planar scanned 2D slices with applications in vEM, which circumvents the inherent limitations of isotropy-based approaches. Our key innovation is to reframe slice-to-3D reconstruction as a 3D dynamic scene rendering problem based on Gaussian splatting, where the progression of axial slices is modeled as the temporal evolution of 2D Gaussian point clouds. To enhance fidelity in data-sparse regimes, we incorporate a Teacher-Student bootstrapping mechanism that uses high-confidence predictions on unobserved slices as pseudo-supervisory signals. Compared with diffusion- and GAN-based reconstruction methods, EMGauss substantially improves interpolation quality, enables continuous slice synthesis, and eliminates the need for large-scale pretraining. Beyond vEM, it potentially provides a generalizable slice-to-3D solution across diverse imaging domains.

</details>


### [163] [Lightweight Wasserstein Audio-Visual Model for Unified Speech Enhancement and Separation](https://arxiv.org/abs/2512.06689)
*Jisoo Park,Seonghak Lee,Guisik Kim,Taewoo Kim,Junseok Kwon*

Main category: cs.CV

> UniVoiceLite is a proposed lightweight, unsupervised method that unifies speech enhancement and speech separation using audio visual cues, achieving strong performance in noisy and multi-speaker environments.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is the need for a unified solution for speech enhancement and speech separation tasks, especially in real-world scenarios involving both background noise and overlapping speakers, where previous methods were complex, parameter-heavy, and relied on supervised training.

**Method:** UniVoiceLite uses lip motion and facial identity cues to guide speech extraction and Wasserstein distance regularization to stabilize the latent space without requiring paired noisy-clean data. It is a lightweight and unsupervised audio-visual framework.

**Result:** Experiments show that UniVoiceLite has strong performance in both noisy and multi-speaker settings.

**Conclusion:** UniVoiceLite achieves strong performance in both noisy and multi-speaker scenarios, combining efficiency with robust generalization.

**Abstract:** Speech Enhancement (SE) and Speech Separation (SS) have traditionally been treated as distinct tasks in speech processing. However, real-world audio often involves both background noise and overlapping speakers, motivating the need for a unified solution. While recent approaches have attempted to integrate SE and SS within multi-stage architectures, these approaches typically involve complex, parameter-heavy models and rely on supervised training, limiting scalability and generalization. In this work, we propose UniVoiceLite, a lightweight and unsupervised audio-visual framework that unifies SE and SS within a single model. UniVoiceLite leverages lip motion and facial identity cues to guide speech extraction and employs Wasserstein distance regularization to stabilize the latent space without requiring paired noisy-clean data. Experimental results demonstrate that UniVoiceLite achieves strong performance in both noisy and multi-speaker scenarios, combining efficiency with robust generalization. The source code is available at https://github.com/jisoo-o/UniVoiceLite.

</details>


### [164] [Graph Convolutional Long Short-Term Memory Attention Network for Post-Stroke Compensatory Movement Detection Based on Skeleton Data](https://arxiv.org/abs/2512.06736)
*Jiaxing Fan,Jiaojiao Liu,Wenkong Wang,Yang Zhang,Xin Ma,Jichen Zhang*

Main category: cs.CV

> 研究使用GCN-LSTM-ATT模型，通过Kinect深度相机收集的骨架数据，准确检测中风患者的代偿性运动，准确率达到0.8580。

<details>
  <summary>Details</summary>

**Motivation:** 因中风患者在康复训练中经常出现代偿性运动，这对其长期恢复不利，故检测代偿性运动具有重要意义。

**Method:** 本研究提出了一种基于骨架数据的图卷积长短期记忆注意力网络（GCN-LSTM-ATT）来检测中风后的代偿性运动。

**Result:** 研究结果表明，GCN-LSTM-ATT模型的检测准确率达到了0.8580，显著高于传统的机器学习算法。

**Conclusion:** 这些发现为检测中风后的代偿性运动提供了更精确和强大的工具，并有望改善中风患者的康复训练策略。

**Abstract:** Most stroke patients experience upper limb motor dysfunction. Compensatory movements are prevalent during rehabilitation training, which is detrimental to patients' long-term recovery. Therefore, detecting compensatory movements is of great significance. In this study, a Graph Convolutional Long Short-Term Memory Attention Network (GCN-LSTM-ATT) based on skeleton data is proposed for the detection of compensatory movements after stroke. Sixteen stroke patients were selected in the research. The skeleton data of the patients performing specific rehabilitation movements were collected using the Kinect depth camera. After data processing, detection models were constructed respectively using the GCN-LSTM-ATT model, the Support Vector Machine(SVM), the K-Nearest Neighbor algorithm(KNN), and the Random Forest(RF). The results show that the detection accuracy of the GCN-LSTM-ATT model reaches 0.8580, which is significantly higher than that of traditional machine learning algorithms. Ablation experiments indicate that each component of the model contributes significantly to the performance improvement. These findings provide a more precise and powerful tool for the detection of compensatory movements after stroke, and are expected to facilitate the optimization of rehabilitation training strategies for stroke patients.

</details>


### [165] [FedSCAl: Leveraging Server and Client Alignment for Unsupervised Federated Source-Free Domain Adaptation](https://arxiv.org/abs/2512.06738)
*M Yashwanth,Sampath Koti,Arunabh Singh,Shyam Marjit,Anirban Chakraborty*

Main category: cs.CV

> 本文提出了一种新的联邦学习框架FedSCAl，该框架针对联邦源自由域适应问题，通过客户端与服务器预测的对齐来提高伪标签的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决联邦源自由域适应问题，其中客户端持有的数据没有标签且存在显著的域差距。传统的联邦学习方法在这种情境中往往处理困难，导致伪标签的准确性不可靠。

**Method:** 本文提出了FedSCAl框架，该框架利用服务器-客户端对齐机制（SCAl）来处理联邦源自由域适应问题。SCAl机制有助于在客户端和服务器模型的预测之间进行对齐，以缓解客户端偏移问题，提高客户端伪标签的准确性。

**Result:** 本文提出的FedSCAl方法在基准视觉数据集上的实验结果表明，与现有的联邦学习方法相比，其在联邦源自由域适应的问题上表现出了优越性。

**Conclusion:** 实验表明，通过引入SCAl机制，FedSCAl能够有效减少客户端偏移问题，并在多项分类任务中显著提升了性能，优于现有的联邦学习方法。

**Abstract:** We address the Federated source-Free Domain Adaptation (FFreeDA) problem, with clients holding unlabeled data with significant inter-client domain gaps. The FFreeDA setup constrains the FL frameworks to employ only a pre-trained server model as the setup restricts access to the source dataset during the training rounds. Often, this source domain dataset has a distinct distribution to the clients' domains. To address the challenges posed by the FFreeDA setup, adaptation of the Source-Free Domain Adaptation (SFDA) methods to FL struggles with client-drift in real-world scenarios due to extreme data heterogeneity caused by the aforementioned domain gaps, resulting in unreliable pseudo-labels. In this paper, we introduce FedSCAl, an FL framework leveraging our proposed Server-Client Alignment (SCAl) mechanism to regularize client updates by aligning the clients' and server model's predictions. We observe an improvement in the clients' pseudo-labeling accuracy post alignment, as the SCAl mechanism helps to mitigate the client-drift. Further, we present extensive experiments on benchmark vision datasets showcasing how FedSCAl consistently outperforms state-of-the-art FL methods in the FFreeDA setup for classification tasks.

</details>


### [166] [Task-Model Alignment: A Simple Path to Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2512.06746)
*Ruoxin Chen,Jiahui Gao,Kaiqing Lin,Keyue Zhang,Yandan Zhao,Isabel Guan,Taiping Yao,Shouhong Ding*

Main category: cs.CV

> 本文探讨了视觉语言模型在AI生成图像检测中的局限性，提出通过任务-模型对齐原则来改进检测效果，并设计了双分支检测器AlignGemini，该方法显著提升了检测精度。

<details>
  <summary>Details</summary>

**Motivation:** 本文探讨了视觉语言模型在AI生成图像检测中的核心问题，通过实验证明，仅凭视觉语言模型难以精确检测出细节上的偏差，因此提出了任务与模型之间的对齐策略。

**Method:** 本文提出了任务-模型对齐原则，并将其具体实现为一个双分支检测器AlignGemini。该检测器包括一个仅用纯语义监督进行微调的视觉语言模型和一个仅用纯像素细节监督训练的像素细节专家。通过在两个简化的数据集上施加正交监督，每个分支都能发挥其优势，产生互补的语义和像素线索判定。

**Result:** 在五个野外基准数据集上，AlignGemini实现了平均准确率9.5%的提升，证明了任务-模型对齐可以作为一种有效的方法来实现通用的AI生成图像检测。

**Conclusion:** 视觉语言模型在高阶语义监督下的表现优于低级像素偏差监督，两种类型的模型对不同任务有不同的适应性，通过任务-模型对齐可以提高AI生成图像检测的准确性。

**Abstract:** Vision Language Models (VLMs) are increasingly adopted for AI-generated images (AIGI) detection, yet converting VLMs into detectors requires substantial resource, while the resulting models still exhibit severe hallucinations. To probe the core issue, we conduct an empirical analysis and observe two characteristic behaviors: (i) fine-tuning VLMs on high-level semantic supervision strengthens semantic discrimination and well generalize to unseen data; (ii) fine-tuning VLMs on low-level pixel-artifact supervision yields poor transfer. We attribute VLMs' underperformance to task-model misalignment: semantics-oriented VLMs inherently lack sensitivity to fine-grained pixel artifacts, and semantically non-discriminative pixel artifacts thus exceeds their inductive biases. In contrast, we observe that conventional pixel-artifact detectors capture low-level pixel artifacts yet exhibit limited semantic awareness relative to VLMs, highlighting that distinct models are better matched to distinct tasks. In this paper, we formalize AIGI detection as two complementary tasks--semantic consistency checking and pixel-artifact detection--and show that neglecting either induces systematic blind spots. Guided by this view, we introduce the Task-Model Alignment principle and instantiate it as a two-branch detector, AlignGemini, comprising a VLM fine-tuned exclusively with pure semantic supervision and a pixel-artifact expert trained exclusively with pure pixel-artifact supervision. By enforcing orthogonal supervision on two simplified datasets, each branch trains to its strengths, producing complementary discrimination over semantic and pixel cues. On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy, supporting task-model alignment as an effective path to generalizable AIGI detection.

</details>


### [167] [UARE: A Unified Vision-Language Model for Image Quality Assessment, Restoration, and Enhancement](https://arxiv.org/abs/2512.06750)
*Weiqi Li,Xuanyu Zhang,Bin Chen,Jingfen Xie,Yan Wang,Kexin Zhang,Junlin Li,Li Zhang,Jian Zhang,Shijie Zhao*

Main category: cs.CV

> The paper presents UARE, a unified vision-language model for image quality assessment, restoration, and enhancement, showing how quality assessment can guide restoration processes.

<details>
  <summary>Details</summary>

**Motivation:** This work is motivated by the potential to improve restoration performance through integrated understanding of image quality. The paper highlights the underexplored area of combining IQA with restoration and enhancement in a single model.

**Method:** The paper introduces a two-stage training framework for UARE, which first uses a progressive schedule to train on single-type and then mixed degradations, and second fine-tunes the quality understanding and restoration with text-image data to align IQA with restoration objectives.

**Result:** Experiments show that UARE performs well across image quality assessment, restoration, and enhancement tasks, establishing its efficacy as a unified model.

**Conclusion:** UARE demonstrates the effectiveness of leveraging IQA to improve restoration and enhancement performance, setting a valuable direction for multimodal understanding and generation tasks.

**Abstract:** Image quality assessment (IQA) and image restoration are fundamental problems in low-level vision. Although IQA and restoration are closely connected conceptually, most existing work treats them in isolation. Recent advances in unified multimodal understanding-generation models demonstrate promising results and indicate that stronger understanding can improve generative performance. This motivates a single model that unifies IQA and restoration and explicitly studies how IQA can guide restoration, a setting that remains largely underexplored yet highly valuable. In this paper, we propose UARE, to our knowledge the first Unified vision-language model for image quality Assessment, Restoration, and Enhancement. Built on pretrained unified understanding and generation models, we introduce a two-stage training framework. First, a progressive, easy-to-hard schedule expands from single-type distortions to higher-order mixed degradations, enabling UARE to handle multiple degradations. Second, we perform unified fine-tuning of quality understanding and restoration with interleaved text-image data, aligning IQA signals with restoration objectives. Through multi-task co-training, UARE leverages IQA to boost restoration and enhancement performance. Extensive experiments across IQA, restoration, and enhancement tasks demonstrate the effectiveness of UARE. The code and models will be available at https://github.com/lwq20020127/UARE.

</details>


### [168] [VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors](https://arxiv.org/abs/2512.06759)
*Wenbo Lyu,Yingjun Du,Jinglin Zhao,Xianton Zhen,Ling Shao*

Main category: cs.CV

> 本文提出了VisChainBench，一个评估大型视觉语言模型多步骤视觉推理能力的基准测试，包含1,457个任务涵盖超过20,000张图片，能够测试模型在复杂的视觉语言任务中的表现。

<details>
  <summary>Details</summary>

**Motivation:** 现有基准测试主要集中在静态或横向比较，如识别视觉差异或评估合适性，这忽视了依赖上下文的推理和视觉到视觉的推理。因此，有必要构建一个能够评估视觉语言模型在多步骤视觉推理上能力的基准测试。

**Method:** 为了解决现有基准测试中缺少对多图像、多回合情景理解的挑战，提出了VisChainBench，一个大型的视觉语言模型基准测试。该基准测试包含1,457个任务，涵盖了超过20,000张图片，通过多智能体生成管道创建，确保了高视觉多样性和语言偏差的控制。

**Result:** VisChainBench的引入填补了当前基准测试的空白，它提供了一个高标准的测试平台，用于评估大型视觉语言模型执行多步骤视觉推理的能力。

**Conclusion:** 通过VisChainBench，研究者可以更好地理解现有大型视觉语言模型的局限性，并指导未来模型的发展方向。所有基准测试数据和代码均可通过提供的链接下载获取。

**Abstract:** Understanding multi-image, multi-turn scenarios is a critical yet underexplored capability for Large Vision-Language Models (LVLMs). Existing benchmarks predominantly focus on static or horizontal comparisons -- e.g., spotting visual differences or assessing appropriateness -- while relying heavily on language cues. Such settings overlook progressive, context-dependent reasoning and the challenge of visual-to-visual inference. To bridge this gap, we present VisChainBench, a large-scale benchmark designed to rigorously evaluate LVLMs' ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance. VisChainBench contains 1,457 tasks spanning over 20,000 images across three diverse domains (e.g., daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes. Uniquely, the benchmark is constructed using a multi-agent generation pipeline, ensuring high visual diversity and controlled language bias. All the benchmark data and code for benchmark construction are available for viewing and download via following Link: https://huggingface.co/datasets/eyehole/VisChainBench

</details>


### [169] [JOCA: Task-Driven Joint Optimisation of Camera Hardware and Adaptive Camera Control Algorithms](https://arxiv.org/abs/2512.06763)
*Chengyang Yan,Mitch Bryson,Donald G. Dansereau*

Main category: cs.CV

> 本研究提出了一种联合优化相机硬件与自适应控制算法的方法，实验结果表明，这种方法能够显著提升下游任务在复杂环境下的性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于传统方法主要优化在制造时固定的相机参数，而许多参数如曝光设置需要在运行时进行自适应控制，因此本论文旨在联合优化相机硬件和自适应相机控制算法。

**Method:** 本论文提出了一种将相机硬件优化与下游视觉任务相联合的方法，并引入了一个统一的优化框架，该框架结合了基于梯度和非基于梯度的方法。此外，为了处理如运动模糊等不可微分的效果，提出了DF-Grad混合优化策略，使用无监督任务驱动学习与非基于梯度优化器信号训练自适应控制网络。

**Result:** 实验结果证明了该方法在各种具有挑战性的条件下比现有基线方法表现出更高的感知性能。

**Conclusion:** 实验表明，与分别优化静态和动态参数的基线方法相比，该方法在低光和快速运动条件下表现更优，证明了联合优化硬件参数与自适应控制算法可以提升感知性能，并提供了一种任务驱动的相机系统设计方法。

**Abstract:** The quality of captured images strongly influences the performance of downstream perception tasks. Recent works on co-designing camera systems with perception tasks have shown improved task performance. However, most prior approaches focus on optimising fixed camera parameters set at manufacturing, while many parameters, such as exposure settings, require adaptive control at runtime. This paper introduces a method that jointly optimises camera hardware and adaptive camera control algorithms with downstream vision tasks. We present a unified optimisation framework that integrates gradient-based and derivative-free methods, enabling support for both continuous and discrete parameters, non-differentiable image formation processes, and neural network-based adaptive control algorithms. To address non-differentiable effects such as motion blur, we propose DF-Grad, a hybrid optimisation strategy that trains adaptive control networks using signals from a derivative-free optimiser alongside unsupervised task-driven learning. Experiments show that our method outperforms baselines that optimise static and dynamic parameters separately, particularly under challenging conditions such as low light and fast motion. These results demonstrate that jointly optimising hardware parameters and adaptive control algorithms improves perception performance and provides a unified approach to task-driven camera system design.

</details>


### [170] [Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial Understanding](https://arxiv.org/abs/2512.06769)
*Hang Yin,Xiaomin He,PeiWen Yuan,Yiwei Li,Jiayi Shi,Wenxiao Fan,Shaoxiong Feng,Kan Li*

Main category: cs.CV

> 研究人员提出了一种名为SiTe的方法来增强视觉语言模型的空间理解能力，通过拼接图像并生成基于拼接图像布局的空间感知注释来注入结构化的空间监督信息，提升模型在空间理解任务上的准确性，同时保持对广义视觉语言任务的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的视觉语言模型常常在描述对象之间的相对位置时出现错误，研究人员认为这主要是由于图像和文本之间不对称性质导致的问题。

**Method:** Structure

**Result:** {
  "tldr": "研究人员提出了一种名为SiTe的方法来增强视觉语言模型的空间理解能力，通过拼接图像并生成基于拼接图像布局的空间感知注释来注入结构化的空间监督信息，提升模型在空间理解任务上的准确性，同时保持对广义视觉语言任务的性能。", 
  "motivation": "现有的视觉语言模型常常在描述对象之间的相对位置时出现错误，研究人员认为这主要是由于图像和文本之间不对称性质导致的问题。", 
  "method": "SiTe方法通过将图像沿空间轴拼接，并基于拼接图像的布局生成空间感知注释或问答对，最大限度减少对成本高昂高级模型或人工介入的依赖，仅需注入结构化的空间监督信息到数据中即可。", 
  "result": "实验结果显示，SiTe提升了包括MME_Position和Spatial-MM在内的空间理解任务的性能，同时保持或提升了包括COCO-QA和MMBench在内的广义视觉语言基准的得分。", 
  "conclusion": "研究表明，显式地将空间感知结构注入训练数据可以有效缓解空间幻觉问题，并改善空间理解，同时保持对广义视觉语言任务的能力。"}
}

**Conclusion:** 研究表明，显式地将空间感知结构注入训练数据可以有效缓解空间幻觉问题，并改善空间理解，同时保持对广义视觉语言任务的能力。

**Abstract:** Existing vision-language models often suffer from spatial hallucinations, i.e., generating incorrect descriptions about the relative positions of objects in an image. We argue that this problem mainly stems from the asymmetric properties between images and text. To enrich the spatial understanding ability of vision-language models, we propose a simple, annotation-free, plug-and-play method named $\text{Stitch and Tell}$ (abbreviated as SiTe), which injects structured spatial supervision into data. It constructs stitched image-text pairs by stitching images along a spatial axis and generating spatially-aware captions or question answer pairs based on the layout of stitched image, without relying on costly advanced models or human involvement. We evaluate SiTe across three architectures including LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B and HALVA-7B, two training datasets, and eight benchmarks. Experiments show that SiTe improves spatial understanding tasks such as $\text{MME}_{\text{Position}}$ (+5.50%) and Spatial-MM (+4.19%), while maintaining or improving performance on general vision-language benchmarks including COCO-QA (+1.02%) and MMBench (+4.76%). Our findings suggest that explicitly injecting spatially-aware structure into training data offers an effective way to mitigate spatial hallucinations and improve spatial understanding, while preserving general vision-language capabilities.

</details>


### [171] [RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting](https://arxiv.org/abs/2512.06774)
*Longjie Zhao,Ziming Hong,Zhenyang Ren,Runnan Chen,Mingming Gong,Tongliang Liu*

Main category: cs.CV

> 本文提出了RDSplat技术，有效解决了3D高斯渲染水印抵抗扩散编辑的问题，保持了水印的不可见性，达到了最先进的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有3D高斯渲染水印方法容易受到扩散编辑的影响而被擦除，因此需要一种能够抵抗扩散编辑的水印技术。

**Method:** RDSplat, 一种针对3D高斯渲染技术的鲁棒水印技术，通过聚焦低频高斯分布和对抗性训练来实现对抗扩散编辑的水印嵌入。

**Result:** 实验表明，RDSplat在抵抗扩散编辑方面表现出优越的鲁棒性，并且保持了水印的不可见性，实现了最先进的性能。

**Conclusion:** RDSplat为3D高斯渲染提供了更强的版权保护，尤其在处理扩散编辑时表现优异。

**Abstract:** 3D Gaussian Splatting (3DGS) has enabled the creation of digital assets and downstream applications, underscoring the need for robust copyright protection via digital watermarking. However, existing 3DGS watermarking methods remain highly vulnerable to diffusion-based editing, which can easily erase embedded provenance. This challenge highlights the urgent need for 3DGS watermarking techniques that are intrinsically resilient to diffusion-based editing. In this paper, we introduce RDSplat, a Robust watermarking paradigm against Diffusion editing for 3D Gaussian Splatting. RDSplat embeds watermarks into 3DGS components that diffusion-based editing inherently preserve, achieved through (i) proactively targeting low-frequency Gaussians and (ii) adversarial training with a diffusion proxy. Specifically, we introduce a multi-domain framework that operates natively in 3DGS space and embeds watermarks into diffusion-editing-preserved low-frequency Gaussians via coordinated covariance regularization and 2D filtering. In addition, we exploit the low-pass filtering behavior of diffusion-based editing by using Gaussian blur as an efficient training surrogate, enabling adversarial fine-tuning that further enhances watermark robustness against diffusion-based editing. Empirically, comprehensive quantitative and qualitative evaluations on three benchmark datasets demonstrate that RDSplat not only maintains superior robustness under diffusion-based editing, but also preserves watermark invisibility, achieving state-of-the-art performance.

</details>


### [172] [Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos](https://arxiv.org/abs/2512.06783)
*Tobias Leuthold,Michele Xiloyannis,Yves Zimmermann*

Main category: cs.CV

> 这篇文章提出了一种改进BlazePose姿态估计的方法，通过优化融合BlazePose的3D和2D估计，降低了姿态估计误差，特别适合身体训练辅助应用。

<details>
  <summary>Details</summary>

**Motivation:** 现有的自动身体训练辅导应用程序依赖于使用单目视频流的精确和稳健的姿态估计。最先进的模型如BlazePose在实时姿态跟踪方面表现出色，但缺乏解剖约束表明可以通过纳入物理知识进行改进。

**Method:** 提出了一种实时后处理算法，将BlazePose的3D和2D估计进行加权优化融合，通过惩罚偏离预期骨头长度和生物力学模型的程度。使用卡尔曼滤波器根据个体解剖学自适应调整测量信任来细化骨头长度估计。

**Result:** 评估使用Physio2.2M数据集表明，与BlazePose 3D估计相比，3D均方根关节位置误差（MPJPE）减少了10.2个百分点，身体各部分之间的角度误差减少了16.6个百分点。

**Conclusion:** 该方法提供了一种基于计算效率高的视频到3D姿态估计的鲁棒且解剖学一致的姿态估计，适用于自动理疗、医疗保健和运动辅导，可在消费级笔记本电脑和移动设备上运行。

**Abstract:** Applications providing automated coaching for physical training are increasing in popularity, for example physical therapy. These applications rely on accurate and robust pose estimation using monocular video streams. State-of-the-art models like BlazePose excel in real-time pose tracking, but their lack of anatomical constraints indicates improvement potential by including physical knowledge. We present a real-time post-processing algorithm fusing the strengths of BlazePose 3D and 2D estimations using a weighted optimization, penalizing deviations from expected bone length and biomechanical models. Bone length estimations are refined to the individual anatomy using a Kalman filter with adapting measurement trust. Evaluation using the Physio2.2M dataset shows a 10.2 percent reduction in 3D MPJPE and a 16.6 percent decrease in errors of angles between body segments compared to BlazePose 3D estimation. Our method provides a robust, anatomically consistent pose estimation based on a computationally efficient video-to-3D pose estimation, suitable for automated physiotherapy, healthcare, and sports coaching on consumer-level laptops and mobile devices. The refinement runs on the backend with anonymized data only.

</details>


### [173] [Generalized Geometry Encoding Volume for Real-time Stereo Matching](https://arxiv.org/abs/2512.06793)
*Jiaxin Liu,Gangwei Xu,Xianqi Wang,Chengliang Zhang,Xin Yang*

Main category: cs.CV

> The paper introduces Generalized Geometry Encoding Volume (GGEV), a novel real-time stereo matching network with strong generalization capability, outperforming existing methods in terms of zero-shot generalization and achieving state-of-the-art results on benchmark datasets.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the trade-off between real-time performance and generalization in stereo matching methods, striving to achieve strong real-world generalization without sacrificing inference speed.

**Method:** We propose a novel real-time stereo matching network called Generalized Geometry Encoding Volume (GGEV), which extracts depth-aware features as domain-invariant structural priors for guidance in cost aggregation. A Depth-aware Dynamic Cost Aggregation (DDCA) module is introduced to adaptively incorporate these priors into each disparity hypothesis, aiming to enhance matching in unseen scenes. This approach is designed to be lightweight and composed of two complementary steps to build a generalized geometry encoding volume.

**Result:** Experimental results show that GGEV exceeds existing real-time stereo matching methods in zero-shot generalization capability, and achieves state-of-the-art performance on KITTI 2012, KITTI 2015, and ETH3D datasets.

**Conclusion:** The proposed GGEV method effectively enhances stereo matching in unseen scenarios while maintaining real-time performance, showcasing superior generalization and state-of-the-art benchmark performance.

**Abstract:** Real-time stereo matching methods primarily focus on enhancing in-domain performance but often overlook the critical importance of generalization in real-world applications. In contrast, recent stereo foundation models leverage monocular foundation models (MFMs) to improve generalization, but typically suffer from substantial inference latency. To address this trade-off, we propose Generalized Geometry Encoding Volume (GGEV), a novel real-time stereo matching network that achieves strong generalization. We first extract depth-aware features that encode domain-invariant structural priors as guidance for cost aggregation. Subsequently, we introduce a Depth-aware Dynamic Cost Aggregation (DDCA) module that adaptively incorporates these priors into each disparity hypothesis, effectively enhancing fragile matching relationships in unseen scenes. Both steps are lightweight and complementary, leading to the construction of a generalized geometry encoding volume with strong generalization capability. Experimental results demonstrate that our GGEV surpasses all existing real-time methods in zero-shot generalization capability, and achieves state-of-the-art performance on the KITTI 2012, KITTI 2015, and ETH3D benchmarks.

</details>


### [174] [VDOT: Efficient Unified Video Creation via Optimal Transport Distillation](https://arxiv.org/abs/2512.06802)
*Yutong Wang,Haiyu Zhang,Tianfan Xue,Yu Qiao,Yaohui Wang,Chang Xu,Xinyuan Chen*

Main category: cs.CV

> 提出了一个名为VDOT的高效统一视频生成模型，通过计算最优传输技术优化的分布匹配蒸馏方法提升生成效率与质量。

<details>
  <summary>Details</summary>

**Motivation:** 现有视频生成模型要么专注于少数特定条件，要么因模型推理复杂导致生成时间过长，不适用于实际应用。

**Method:** 使用一种创新的计算最优传输技术优化真实和假视频评分分布之间的差异，同时引入判别器以提高生成视频的质量。

**Result:** 实验结果表明，VDOT在4步生成流程中的表现优于或与需要100步去噪的其他基准模型相当。

**Conclusion:** VDOT通过几何约束增强了训练过程的效率和稳定性，适用于多种视频生成任务。

**Abstract:** The rapid development of generative models has significantly advanced image and video applications. Among these, video creation, aimed at generating videos under various conditions, has gained substantial attention. However, existing video creation models either focus solely on a few specific conditions or suffer from excessively long generation times due to complex model inference, making them impractical for real-world applications. To mitigate these issues, we propose an efficient unified video creation model, named VDOT. Concretely, we model the training process with the distribution matching distillation (DMD) paradigm. Instead of using the Kullback-Leibler (KL) minimization, we additionally employ a novel computational optimal transport (OT) technique to optimize the discrepancy between the real and fake score distributions. The OT distance inherently imposes geometric constraints, mitigating potential zero-forcing or gradient collapse issues that may arise during KL-based distillation within the few-step generation scenario, and thus, enhances the efficiency and stability of the distillation process. Further, we integrate a discriminator to enable the model to perceive real video data, thereby enhancing the quality of generated videos. To support training unified video creation models, we propose a fully automated pipeline for video data annotation and filtering that accommodates multiple video creation tasks. Meanwhile, we curate a unified testing benchmark, UVCBench, to standardize evaluation. Experiments demonstrate that our 4-step VDOT outperforms or matches other baselines with 100 denoising steps.

</details>


### [175] [RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models](https://arxiv.org/abs/2512.06811)
*Xiang Lin,Weixin Li,Shu Guo,Lihong Wang,Di Huang*

Main category: cs.CV

> 介绍了一种新颖的重构多模态适配器(RMAdapter)，通过在少量样本场景中进行参数高效的微调，解决了预训练视觉-语言模型的微调挑战，并在多种任务上超越现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 针对少量样本场景中预训练视觉-语言模型微调的平衡挑战以及目前方法以提示为基础，而适配器方法研究不足的问题。

**Method:** RMAdapter采用双分支架构，包括适应分支和重构分支。适应分支通过参数高效的微调注入任务特定知识，重构分支通过重构潜在空间特征到原始特征空间来保持通用知识。重构损失在每一层本地计算并共享投影模块，以保持计算开销最小。

**Result:** 通过全面的评估，RMAdapter在新类别泛化、新目标数据集泛化和领域泛化上表现优于现有方法。

**Conclusion:** RMAdapter在三种具有代表性的任务上超越了现有最先进方法。

**Abstract:** Pre-trained Vision-Language Models (VLMs), \textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.

</details>


### [176] [MeshSplatting: Differentiable Rendering with Opaque Meshes](https://arxiv.org/abs/2512.06818)
*Jan Held,Sanghyun Son,Renaud Vandeghen,Daniel Rebain,Matheus Gadelha,Yi Zhou,Anthony Cioppa,Ming C. Lin,Marc Van Droogenbroeck,Andrea Tagliasacchi*

Main category: cs.CV

> MeshSplatting采用基于网格的方法，通过可微渲染实现几何和外观的联合优化，解决了点表示与网格表示不兼容的问题，实现了平滑且高效的实时渲染。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决基于点的方法如3D高斯点散射无法与AR/VR和游戏引擎中的网格管道兼容的问题，从而实现流畅的实时渲染。

**Method:** MeshSplatting采用了一种基于网格的重建方法，通过可微渲染技术对几何和外观进行联合优化。通过受限的Delaunay三角剖分保证网格的连通性，并细化表面的一致性。

**Result:** 在Mip-NeRF360数据集上，MeshSplatting相比现有最先进的MiLo方法提高了+0.69 dB的PSNR，同时训练速度提高2倍，内存使用减少一半。

**Conclusion:** MeshSplatting成功地缩短了神经渲染和交互式3D图形之间的距离，为无缝实时场景交互提供了基础。

**Abstract:** Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.

</details>


### [177] [SparseCoop: Cooperative Perception with Kinematic-Grounded Queries](https://arxiv.org/abs/2512.06838)
*Jiahao Wang,Zhongwei Jiang,Wenchao Sun,Jiaru Zhong,Haibao Yu,Yuner Zhang,Chenyang Lu,Chuang Zhang,Lei He,Shaobing Xu,Jianqiang Wang*

Main category: cs.CV

> 论文提出了一种名为SparseCoop的全稀疏合作感知框架，它通过几种创新技术提高了3D物体检测和跟踪的性能和效率。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有共享密集鸟瞰图（BEV）特征的方法受到二次扩展通信成本的限制，并且缺乏跨异步或不同视角的精确对齐的灵活性和可解释性，该论文旨在解决这些限制。特别是针对新兴的基于稀疏查询的方法存在的不足，如不适当的几何表示、次优的数据融合策略及时训练不稳定的问题，提出了SparseCoop框架。

**Method:** 本论文提出了一种名为SparseCoop的全稀疏合作感知框架，用于3D检测和跟踪，完全抛弃了中间的BEV表示。该框架有三个创新点：基于动力学的实例查询，使用带有3D几何和速度的显式状态向量以实现精确的时空对齐；粗到细的聚合模块，以实现稳健的数据融合；以及一种合作实例降噪任务，用于加速和稳定训练过程。

**Result:** 实验结果显示，SparseCoop在V2X-Seq和Griffin数据集上达到了最先进的性能水平，同时还实现了计算效率高，传输成本低和对通信延迟的强鲁棒性。

**Conclusion:** SparseCoop通过采用稀疏查询方法、改进的数据融合模块和特殊的训练稳定策略，解决了当前合作感知技术中的多个问题，并展示了其实用性及在自动驾驶中的应用潜力。

**Abstract:** Cooperative perception is critical for autonomous driving, overcoming the inherent limitations of a single vehicle, such as occlusions and constrained fields-of-view. However, current approaches sharing dense Bird's-Eye-View (BEV) features are constrained by quadratically-scaling communication costs and the lack of flexibility and interpretability for precise alignment across asynchronous or disparate viewpoints. While emerging sparse query-based methods offer an alternative, they often suffer from inadequate geometric representations, suboptimal fusion strategies, and training instability. In this paper, we propose SparseCoop, a fully sparse cooperative perception framework for 3D detection and tracking that completely discards intermediate BEV representations. Our framework features a trio of innovations: a kinematic-grounded instance query that uses an explicit state vector with 3D geometry and velocity for precise spatio-temporal alignment; a coarse-to-fine aggregation module for robust fusion; and a cooperative instance denoising task to accelerate and stabilize training. Experiments on V2X-Seq and Griffin datasets show SparseCoop achieves state-of-the-art performance. Notably, it delivers this with superior computational efficiency, low transmission cost, and strong robustness to communication latency. Code is available at https://github.com/wang-jh18-SVM/SparseCoop.

</details>


### [178] [CADE: Continual Weakly-supervised Video Anomaly Detection with Ensembles](https://arxiv.org/abs/2512.06840)
*Satoshi Hashimoto,Tatsuya Konishi,Tomoya Kaichi,Kazunori Matsumoto,Mori Kurokawa*

Main category: cs.CV

> A new continual learning-based method for weakly-supervised video anomaly detection (CADE) is proposed to handle data imbalance, label uncertainty, and domain shifts in video anomaly detection while preventing forgetting of previous data patterns.

<details>
  <summary>Details</summary>

**Motivation:** The main motivation for this work is to address the limitations of existing WVAD methods when dealing with data domain shifts and the potential performance degradation resulting from naive approaches to retraining models with new data. The introduction of the CADE method aims to maintain high detection performance across both old and new data distributions.

**Method:** The paper introduces a novel approach called Continual Anomaly Detection with Ensembles (CADE), which integrates continual learning and weakly-supervised video anomaly detection (WVAD). CADE uses a Dual-Generator (DG) to handle data imbalance and label uncertainty, and a Multi-Discriminator (MD) ensemble to capture missed anomalies and prevent forgetting.

**Result:** CADE significantly outperforms existing VAD methods on the common multi-scene VAD datasets, such as ShanghaiTech and Charlotte Anomaly datasets, showing a noteworthy improvement in anomaly detection under domain shifts and continual learning settings.

**Conclusion:** The proposed CADE method successfully demonstrates its effectiveness in detecting anomalies in various scenes without performance degradation due to forgetting, outperforming existing VAD methods on standard datasets such as ShanghaiTech and Charlotte.

**Abstract:** Video anomaly detection (VAD) has long been studied as a crucial problem in public security and crime prevention. In recent years, weakly-supervised VAD (WVAD) have attracted considerable attention due to their easy annotation process and promising research results. While existing WVAD methods tackle mainly on static datasets, the possibility that the domain of data can vary has been neglected. To adapt such domain-shift, the continual learning (CL) perspective is required because otherwise additional training only with new coming data could easily cause performance degradation for previous data, i.e., forgetting. Therefore, we propose a brand-new approach, called Continual Anomaly Detection with Ensembles (CADE) that is the first work combining CL and WVAD viewpoints. Specifically, CADE uses the Dual-Generator(DG) to address data imbalance and label uncertainty in WVAD. We also found that forgetting exacerbates the "incompleteness'' where the model becomes biased towards certain anomaly modes, leading to missed detections of various anomalies. To address this, we propose to ensemble Multi-Discriminator (MD) that capture missed anomalies in past scenes due to forgetting, using multiple models. Extensive experiments show that CADE significantly outperforms existing VAD methods on the common multi-scene VAD datasets, such as ShanghaiTech and Charlotte Anomaly datasets.

</details>


### [179] [Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2512.06845)
*Satoshi Hashimoto,Hitoshi Nishimura,Yanan Wang,Mori Kurokawa*

Main category: cs.CV

> 本文提出了一种生成驱动的方法PA-VAD，通过合成伪异常视频而不使用真实的异常视频，实现了高精度的视频异常检测，为实际部署提供了解决方案。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决真实异常视频稀缺和收集成本高的问题，以支持视频异常检测的实用部署。

**Method:** Structure

**Result:** { quadruple: 'Video异常检测的部署受到真实异常视频稀缺和收集成本的阻碍。本文提出的PA-VAD方法通过生成伪异常视频与真实正常视频配对进行学习，而不需要任何真实异常视频，从而缓解了这一问题。实验表明，该方法在ShanghaiTech和UCF-Crime数据集上分别达到了98.2%和82.5%的准确性，优于现有的方法。', motivation: '解决真实异常视频稀缺和成本高的问题，使视频异常检测能有效部署。', method: '使用CLIP选择与类别相关的初始图像，并用视觉语言模型改进文本提示来提高合成视频的保真度和场景一致性，然后调用视频扩散模型进行合成。训练时使用领域对齐正则化模块来减轻合成异常中的空间时间幅度。', result: '实验结果表明，该方法在ShanghaiTech数据集上的准确率为98.2%，在UCF-Crime数据集上为82.5%，优于现有方法。', conclusion: '该研究证明了无需收集真实异常视频即可获得高精度的异常检测，为视频异常检测的规模化部署提供了一个实用路径。' }

**Conclusion:** 研究证实，即使不收集真实的异常视频，也能实现高精度的异常检测，从而为视频异常检测的实践应用提供了可行的路径。

**Abstract:** Deploying video anomaly detection in practice is hampered by the scarcity and collection cost of real abnormal footage. We address this by training without any real abnormal videos while evaluating under the standard weakly supervised split, and we introduce PA-VAD, a generation-driven approach that learns a detector from synthesized pseudo-abnormal videos paired with real normal videos, using only a small set of real normal images to drive synthesis. For synthesis, we select class-relevant initial images with CLIP and refine textual prompts with a vision-language model to improve fidelity and scene consistency before invoking a video diffusion model. For training, we mitigate excessive spatiotemporal magnitude in synthesized anomalies by an domain-aligned regularized module that combines domain alignment and memory usage-aware updates. Extensive experiments show that our approach reaches 98.2% on ShanghaiTech and 82.5% on UCF-Crime, surpassing the strongest real-abnormal method on ShanghaiTech by +0.6% and outperforming the UVAD state-of-the-art on UCF-Crime by +1.9%. The results demonstrate that high-accuracy anomaly detection can be obtained without collecting real anomalies, providing a practical path toward scalable deployment.

</details>


### [180] [Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT](https://arxiv.org/abs/2512.06849)
*Matan Atad,Alexander W. Marka,Lisa Steinhelfer,Anna Curto-Vilalta,Yannik Leonhardt,Sarah C. Foreman,Anna-Sophia Walburga Dietrich,Robert Graf,Alexandra S. Gersing,Bjoern Menze,Daniel Rueckert,Jan S. Kirschke,Hendrik Möller*

Main category: cs.CV

> 该研究提出了一种仅需椎体层面健康的或恶性的标签，无需任何病变掩码的弱监督方法，有效提升椎体转移的分割精度。

<details>
  <summary>Details</summary>

**Motivation:** 椎体转移的精确分割在临床上至关重要，但由于体素级标注稀缺以及恶性变化与良性退化变化相似，这种方法难以扩展。

**Method:** 弱监督方法，结合扩散自编码器（DAE）和像素差异图，以及隐藏和寻找归因机制。

**Result:** 在放射科医生的注释数据上，该方法表现出了较强的良性和恶性的分割性能：良性F1得分为0.91，Dice得分为0.87；恶性F1得分为0.85，Dice得分为0.78，远超基线模型。

**Conclusion:** 这项研究表明，椎体层面的标签可以转化为可靠的病变掩码，这种生成编辑结合选择性遮挡支持CT中弱监督分割的准确度。

**Abstract:** Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy/malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic/lytic performance despite no mask supervision (F1: 0.91/0.85; Dice: 0.87/0.78), exceeding baselines (F1: 0.79/0.67; Dice: 0.74/0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.

</details>


### [181] [Omni-Referring Image Segmentation](https://arxiv.org/abs/2512.06862)
*Qiancheng Zheng,Yunhang Shen,Gen Luo,Baiyang Song,Xing Sun,Xiaoshuai Sun,Yiyi Zhou,Rongrong Ji*

Main category: cs.CV

> 本文提出OmniRIS任务和OmniSegNet模型，用于实现基于多模态提示的高度通用的图像分割。通过设计大型数据集OmniRef和全面的评估系统，验证了其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在提出一种新的任务，能够处理更复杂的图像分割问题，涵盖更多的实际应用情景，从而推动图像分割领域向更通用化的方向发展。

**Method:** 本文提出了一种新的任务，称为Omni-Referring Image Segmentation (OmniRIS), 旨在实现高度通用的图像分割。相比现有的单一模态条件分割任务，如RIS和视觉RIS，OmniRIS支持使用文本指令和参考图像（带有掩码、框或涂鸦）作为多模态提示。这使得它可以充分运用文本和视觉模态的内在优势，即细粒度属性引用和不常见对象定位。此外，OmniRIS还可以处理各种分割设置，例如一对一和多对多，进一步促进其实用性。

**Result:** 本文设计并构造了一个大型数据集OmniRef，包含了30,956张图像的186,939个多模态提示，并建立了一个全面的评估系统。此外，还提出了一种强大的基准模型OmniSegNet，用于解决OmniRIS的关键挑战，如多模态提示编码。

**Conclusion:** 实验结果验证了OmniSegNet遵循多模态指令的能力，并展示了OmniRIS在高度通用的图像分割中的优越性。

**Abstract:** In this paper, we propose a novel task termed Omni-Referring Image Segmentation (OmniRIS) towards highly generalized image segmentation. Compared with existing unimodally conditioned segmentation tasks, such as RIS and visual RIS, OmniRIS supports the input of text instructions and reference images with masks, boxes or scribbles as omni-prompts. This property makes it can well exploit the intrinsic merits of both text and visual modalities, i.e., granular attribute referring and uncommon object grounding, respectively. Besides, OmniRIS can also handle various segmentation settings, such as one v.s. many and many v.s. many, further facilitating its practical use. To promote the research of OmniRIS, we also rigorously design and construct a large dataset termed OmniRef, which consists of 186,939 omni-prompts for 30,956 images, and establish a comprehensive evaluation system. Moreover, a strong and general baseline termed OmniSegNet is also proposed to tackle the key challenges of OmniRIS, such as omni-prompt encoding. The extensive experiments not only validate the capability of OmniSegNet in following omni-modal instructions, but also show the superiority of OmniRIS for highly generalized image segmentation.

</details>


### [182] [Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training](https://arxiv.org/abs/2512.06864)
*Kaixuan Lu,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

> 介绍了AutoQ-VIS，这是一种创新的无监督方法，旨在通过质量引导的自训练来解决视频实例分割的标注挑战，实验结果表明其具备优秀的性能。

<details>
  <summary>Details</summary>

**Motivation:** 视频实例分割（VIS）面临主要的标注挑战，因为它需要像素级的掩模和时间一致性标签。而最近的无监督方法虽然解决了光流依赖的问题，但仍然受限于合成数据到真实数据之间的领域差距。

**Method:** AutoQ-VIS采用一种基于质量引导的自训练框架来解决从合成数据到真实数据之间的领域差距问题。该方法建立了一个伪标签生成与自动质量评估之间的闭环系统，从而逐步适应从合成视频到真实视频的转换。

**Result:** 实验结果显示，AutoQ-VIS在YouTubeVIS-2019验证集上达到了52.6的AP50值，超过了之前最好的方法VideoCutLER 4.4%，且不需要任何人工标注。

**Conclusion:** 这展示了质量意识自训练在无监督视频实例分割中的可行性和效果。AutoQ-VIS证明了其在无需人工标注的情况下进行高质量分割的能力，代码计划开源。

**Abstract:** Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 $\text{AP}_{50}$ on YouTubeVIS-2019 $\texttt{val}$ set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.

</details>


### [183] [Spatial Retrieval Augmented Autonomous Driving](https://arxiv.org/abs/2512.06865)
*Xiaosong Jia,Chenhe Zhang,Yule Jiang,Songbur Wong,Zhiyuan Zhang,Chen Chen,Shaofeng Zhang,Xuanhe Zhou,Xue Yang,Junchi Yan,Yu-Gang Jiang*

Main category: cs.CV

> 论文提出了一种新的自动驾驶范式——空间检索范式，通过引入离线地理图像，提高了在某些自动驾驶任务中的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的自动驾驶系统依赖于车载传感器（摄像头、LiDAR、IMU等）进行环境感知，但这些系统在视野范围有限、遮挡或极端条件（如黑暗和雨天）下容易失效。而人类驾驶员在这种情况下仍能回忆起道路结构。为了赋予模型这种“回忆”能力，提出了空间检索范式。

**Method:** 通过引入离线检索的地理图像作为额外输入，提出了空间检索范式。这些图像可以从离线缓存中轻松获取（例如，Google Maps或存储的自动驾驶数据集），无需额外传感器，使其成为现有AD任务的即插即用扩展。

**Result:** 在扩展的nuScenes数据集上，通过Google Maps API检索地理图像并与自车轨迹对齐后，建立了五个核心自动驾驶任务的基线：目标检测、在线制图、占用预测、端到端规划和生成式世界建模。大量实验表明，扩展的模态可以提高某些任务的性能。

**Conclusion:** 通过开放数据集整理代码、数据和基准，进一步研究这种新的自动驾驶范式。实验表明，离线地理图像可以作为现有AD任务的有效补充。

**Abstract:** Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception. However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain. In contrast, human drivers are able to recall road structure even under poor visibility. To endow models with this ``recall" ability, we propose the spatial retrieval paradigm, introducing offline retrieved geographic images as an additional input. These images are easy to obtain from offline caches (e.g, Google Maps or stored autonomous driving datasets) without requiring additional sensors, making it a plug-and-play extension for existing AD tasks.
  For experiments, we first extend the nuScenes dataset with geographic images retrieved via Google Maps APIs and align the new data with ego-vehicle trajectories. We establish baselines across five core autonomous driving tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling. Extensive experiments show that the extended modality could enhance the performance of certain tasks. We will open-source dataset curation code, data, and benchmarks for further study of this new autonomous driving paradigm.

</details>


### [184] [Towards Robust Pseudo-Label Learning in Semantic Segmentation: An Encoding Perspective](https://arxiv.org/abs/2512.06870)
*Wangkai Li,Rui Sun,Zhaoyang Li,Tianzhu Zhang*

Main category: cs.CV

> 提出ECOCSeg方法来改善伪标签学习的质量，通过纠错输出编码实现更稳定的分类和更好的标签除噪。

<details>
  <summary>Details</summary>

**Motivation:** 伪标签学习在标签稀缺的情况下广泛用于语义分割，例如无监督领域适应（UDA）和半监督学习（SSL），但其会产生错误标签并放大误差。

**Method:** 我们提出了ECOCSeg，它采用纠错输出编码（ECOC）对每一类进行精细化编码，以解决伪标签学习中产生错误标签的问题。通过ECOC基分类器来区分各个类别为属性，并能够处理部分不准确的比特，从而提高了伪标签学习的稳定性和泛化能力。同时，开发了位级标签除噪机制来生成质量更高的伪标签，为无标签图像提供充足而稳健的监督信号。

**Result:** ECOCSeg能够与现有方法轻松集成，并在多个UDA和SSL基准测试以及不同分割架构上展示出显著的改进。

**Conclusion:** ECOCSeg展示了在多种UDA和SSL基准测试中的有效性，并能在各种分割架构上实现显著的性能提升。代码已公开。

**Abstract:** Pseudo-label learning is widely used in semantic segmentation, particularly in label-scarce scenarios such as unsupervised domain adaptation (UDA) and semisupervised learning (SSL). Despite its success, this paradigm can generate erroneous pseudo-labels, which are further amplified during training due to utilization of one-hot encoding. To address this issue, we propose ECOCSeg, a novel perspective for segmentation models that utilizes error-correcting output codes (ECOC) to create a fine-grained encoding for each class. ECOCSeg offers several advantages. First, an ECOC-based classifier is introduced, enabling model to disentangle classes into attributes and handle partial inaccurate bits, improving stability and generalization in pseudo-label learning. Second, a bit-level label denoising mechanism is developed to generate higher-quality pseudo-labels, providing adequate and robust supervision for unlabeled images. ECOCSeg can be easily integrated with existing methods and consistently demonstrates significant improvements on multiple UDA and SSL benchmarks across different segmentation architectures. Code is available at https://github.com/Woof6/ECOCSeg.

</details>


### [185] [SceneMixer: Exploring Convolutional Mixing Networks for Remote Sensing Scene Classification](https://arxiv.org/abs/2512.06877)
*Mohammed Q. Alkhatib,Ali Jamali,Swalpa Kumar Roy*

Main category: cs.CV

> The paper proposes a lightweight scene classification model based on convolutional mixer architecture for remote sensing, demonstrating its effectiveness in balancing accuracy and efficiency on AID and EuroSAT datasets.

<details>
  <summary>Details</summary>

**Motivation:** To improve scene classification in remote sensing, addressing challenges like variations in spatial resolution, viewpoint, orientation, and background conditions that affect model generalization.

**Method:** A lightweight architecture based on the convolutional mixer paradigm that alternates between spatial mixing through depthwise convolutions at different scales and channel mixing through pointwise operations.

**Result:** Achieved 74.7%, 74.57%, and 73.79% for overall accuracy, average accuracy, and Kappa on AID; and 93.90%, 93.93%, and 93.22% for these metrics on EuroSAT.

**Conclusion:** The proposed model provides a good balance between accuracy and efficiency compared to CNN- and transformer-based models for remote sensing scene classification.

**Abstract:** Remote sensing scene classification plays a key role in Earth observation by enabling the automatic identification of land use and land cover (LULC) patterns from aerial and satellite imagery. Despite recent progress with convolutional neural networks (CNNs) and vision transformers (ViTs), the task remains challenging due to variations in spatial resolution, viewpoint, orientation, and background conditions, which often reduce the generalization ability of existing models. To address these challenges, this paper proposes a lightweight architecture based on the convolutional mixer paradigm. The model alternates between spatial mixing through depthwise convolutions at multiple scales and channel mixing through pointwise operations, enabling efficient extraction of both local and contextual information while keeping the number of parameters and computations low. Extensive experiments were conducted on the AID and EuroSAT benchmarks. The proposed model achieved overall accuracy, average accuracy, and Kappa values of 74.7%, 74.57%, and 73.79 on the AID dataset, and 93.90%, 93.93%, and 93.22 on EuroSAT, respectively. These results demonstrate that the proposed approach provides a good balance between accuracy and efficiency compared with widely used CNN- and transformer-based models. Code will be publicly available on: https://github.com/mqalkhatib/SceneMixer

</details>


### [186] [Hierarchical Image-Guided 3D Point Cloud Segmentation in Industrial Scenes via Multi-View Bayesian Fusion](https://arxiv.org/abs/2512.06882)
*Yu Zhu,Naoya Chiba,Koichi Hashimoto*

Main category: cs.CV

> 提出了一种层次化的图像引导3D分割框架，逐步细化从实例级别到部件级别的分割，有效处理遮挡和结构复杂性，适用于工业环境中的3D分割任务。

<details>
  <summary>Details</summary>

**Motivation:** 现有3D点云计算方法需要昂贵的注释，而图像引导的方法通常会因为在不同视角之间的语义不一致而表现不佳。特别是在工业环境中，由于密集布局和多尺度对象，3D分割的准确性和鲁棒性受到挑战。

**Method:** 该方法首先使用顶视图渲染和SAM生成的掩模提示，将YOLO-World生成的掩模重新投影到3D点云上，获取实例分割结果。接下来，对每个实例进行多视图渲染，并应用相同的2D分割和重新投影过程，再通过贝叶斯更新融合确保视角之间的语义一致性，实现部件级别的分割。

**Result:** 实验结果表明，在真实世界工厂数据上该方法能有效处理遮挡和结构复杂性，得到各分类较高的平均交并比分数。同时在公共数据集上的额外评估也证明了该框架具有良好的泛化能力。

**Conclusion:** 所提出的方法在有效减少注释成本的同时，展示出强大的鲁棒性、适应各种3D环境的能力和良好的语义一致性，显示出其在复杂工业场景下实现高质量3D分割的巨大潜力。

**Abstract:** Reliable 3D segmentation is critical for understanding complex scenes with dense layouts and multi-scale objects, as commonly seen in industrial environments. In such scenarios, heavy occlusion weakens geometric boundaries between objects, and large differences in object scale will cause end-to-end models fail to capture both coarse and fine details accurately. Existing 3D point-based methods require costly annotations, while image-guided methods often suffer from semantic inconsistencies across views. To address these challenges, we propose a hierarchical image-guided 3D segmentation framework that progressively refines segmentation from instance-level to part-level. Instance segmentation involves rendering a top-view image and projecting SAM-generated masks prompted by YOLO-World back onto the 3D point cloud. Part-level segmentation is subsequently performed by rendering multi-view images of each instance obtained from the previous stage and applying the same 2D segmentation and back-projection process at each view, followed by Bayesian updating fusion to ensure semantic consistency across views. Experiments on real-world factory data demonstrate that our method effectively handles occlusion and structural complexity, achieving consistently high per-class mIoU scores. Additional evaluations on public dataset confirm the generalization ability of our framework, highlighting its robustness, annotation efficiency, and adaptability to diverse 3D environments.

</details>


### [187] [JoPano: Unified Panorama Generation via Joint Modeling](https://arxiv.org/abs/2512.06885)
*Wancheng Feng,Chen An,Zhenliang He,Meina Kan,Shiguang Shan,Lukun Wang*

Main category: cs.CV

> 本文提出了JoPano方法，通过DiT模型和特定适配器的设计解决了全景生成中的视觉质量和模型冗余问题，实验证明该方法能够生成高质量的全景图像，并在多个评价指标上达到了最优水平。

<details>
  <summary>Details</summary>

**Motivation:** 现有全景生成方法面临两大挑战：一是基于U-Net架构的生成结果在视觉质量上受限；二是独立处理文本到全景和视角到全景任务导致了模型冗余和效率低下。为了解决这些问题，作者提出了JoPano方法。

**Method:** 我们提出了一种联合面部全景（JoPano）生成方法，该方法基于DiT模型统一了全景生成的两个核心任务。为了将DiT模型在自然图像中学习到的强大生成能力转移至全景领域，我们设计了一个基于球形立方体图表示的联合面部适配器，使预训练的DiT能够同时建模和生成全景的不同视角。此外，我们通过泊松融合技术减少立方体面之间的边界不一致，并提出了Seam-SSIM和Seam-Sobel度量来定量评估边缘一致性。最后，我们引入了一种条件切换机制，使得单一模型能够统一处理文本到全景和视角到全景生成任务。

**Result:** 实验结果表明，JoPano能够在文本到全景和视角到全景生成任务中生成高质量全景图像，并在FID、CLIP-FID、IS以及CLIP-Score等指标上达到了最新技术水平。

**Conclusion:** 研究通过实验展示了JoPano方法的有效性，能够在保证生成全景视觉质量的同时，减少模型冗余并提高效率。

**Abstract:** Panorama generation has recently attracted growing interest in the research community, with two core tasks, text-to-panorama and view-to-panorama generation. However, existing methods still face two major challenges: their U-Net-based architectures constrain the visual quality of the generated panoramas, and they usually treat the two core tasks independently, which leads to modeling redundancy and inefficiency. To overcome these challenges, we propose a joint-face panorama (JoPano) generation approach that unifies the two core tasks within a DiT-based model. To transfer the rich generative capabilities of existing DiT backbones learned from natural images to the panorama domain, we propose a Joint-Face Adapter built on the cubemap representation of panoramas, which enables a pretrained DiT to jointly model and generate different views of a panorama. We further apply Poisson Blending to reduce seam inconsistencies that often appear at the boundaries between cube faces. Correspondingly, we introduce Seam-SSIM and Seam-Sobel metrics to quantitatively evaluate the seam consistency. Moreover, we propose a condition switching mechanism that unifies text-to-panorama and view-to-panorama tasks within a single model. Comprehensive experiments show that JoPano can generate high-quality panoramas for both text-to-panorama and view-to-panorama generation tasks, achieving state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics.

</details>


### [188] [Balanced Learning for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2512.06886)
*Wangkai Li,Rui Sun,Bohao Liao,Zhaoyang Li,Tianzhu Zhang*

Main category: cs.CV

> BLDA方法通过直接评估和修正类别偏差，提高了无监督领域适应的语义分割性能，特别是在不足预测类别的表现上。该方法结合现有技术，在标准化测试集上取得了更好的效果。

<details>
  <summary>Details</summary>

**Motivation:** 在无监督领域适应（UDA）语义分割中，尽管自我训练技术已经证明是有效的，但由于类别不平衡和领域间数据及标签空间的分布变化，它们难以以平衡的方式学习每个类别。BLDA正是为了解决这一问题而提出的。

**Method:** Balanced Learning for Domain Adaptation (BLDA) 是一种新颖的方法，用于直接评估和减轻类别偏差，而不需要关于分布变化的先验知识。首先，通过分析预测logits的分布来识别过度预测和不足预测的类别。然后，引入一个后处理方法，使用共享锚点分布来对齐不同类别的logits分布。此外，在自训练过程中，为了生成无偏伪标签，估计logits分布，并将logits校正项纳入损失函数中。还利用累计密度作为领域共享结构知识来连接源领域和目标领域。

**Result:** 在两个标准的UDA语义分割基准上进行了广泛的实验，结果表明，BLDA可以显著提升性能，特别是在那些不足预测类别上的表现。当与现有方法结合时，BLDA表现出了一致的性能改进。

**Conclusion:** BLDA通过改进UDA语义分割中类别的平衡性，有效地解决了自训练技术面临的问题，从而改善了模型在分割任务上的性能。代码可在https://github.com/Woof6/BLDA获取。

**Abstract:** Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Despite the effectiveness of self-training techniques in UDA, they struggle to learn each class in a balanced manner due to inherent class imbalance and distribution shift in both data and label space between domains. To address this issue, we propose Balanced Learning for Domain Adaptation (BLDA), a novel approach to directly assess and alleviate class bias without requiring prior knowledge about the distribution shift. First, we identify over-predicted and under-predicted classes by analyzing the distribution of predicted logits. Subsequently, we introduce a post-hoc approach to align the logits distributions across different classes using shared anchor distributions. To further consider the network's need to generate unbiased pseudo-labels during self-training, we estimate logits distributions online and incorporate logits correction terms into the loss function. Moreover, we leverage the resulting cumulative density as domain-shared structural knowledge to connect the source and target domains. Extensive experiments on two standard UDA semantic segmentation benchmarks demonstrate that BLDA consistently improves performance, especially for under-predicted classes, when integrated into various existing methods. Code is available at https://github.com/Woof6/BLDA.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [189] [Proof of Concept for Mammography Classification with Enhanced Compactness and Separability Modules](https://arxiv.org/abs/2512.06575)
*Fariza Dahes*

Main category: eess.IV

> An extension of a methodological framework for medical image classification is applied to mammography, improving malignant case discriminability but finding limited utility for a specific loss function, underlining potential for further architectural improvements.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to validate and extend a recent methodological framework and architecture improvements for medical image classification to mammography, specifically to improve feature discriminability and reduce misclassifications.

**Method:** This study uses a Kaggle dataset consolidated from INbreast, MIAS, and DDSM mammography collections to compare a baseline CNN, ConvNeXt Tiny, and InceptionV3 with GAGM and SEVector modules for mammography classification.

**Result:** Results show that GAGM and SEVector modules enhance feature discriminability and reduce false negatives, particularly for malignant cases. The Feature Smoothing Loss did not yield improvements in mammography classification under the current conditions.

**Conclusion:** The work extends the framework with multi-metric evaluation, feature interpretability analysis, and an interactive dashboard. Future work entails exploring alternative methods to enhance intra-class compactness and inter-class separability in mammography classification.

**Abstract:** This study presents a validation and extension of a recent methodological framework for medical image classification. While an improved ConvNeXt Tiny architecture, integrating Global Average and Max Pooling fusion (GAGM), lightweight channel attention (SEVector), and Feature Smoothing Loss (FSL), demonstrated promising results on Alzheimer MRI under CPU friendly conditions, our work investigates its transposability to mammography classification. Using a Kaggle dataset that consolidates INbreast, MIAS, and DDSM mammography collections, we compare a baseline CNN, ConvNeXt Tiny, and InceptionV3 backbones enriched with GAGM and SEVector modules. Results confirm the effectiveness of GAGM and SEVector in enhancing feature discriminability and reducing false negatives, particularly for malignant cases. In our experiments, however, the Feature Smoothing Loss did not yield measurable improvements under mammography classification conditions, suggesting that its effectiveness may depend on specific architectural and computational assumptions. Beyond validation, our contribution extends the original framework through multi metric evaluation (macro F1, per class recall variance, ROC/AUC), feature interpretability analysis (Grad CAM), and the development of an interactive dashboard for clinical exploration. As a perspective, we highlight the need to explore alternative approaches to improve intra class compactness and inter class separability, with the specific goal of enhancing the distinction between malignant and benign cases in mammography classification.

</details>
