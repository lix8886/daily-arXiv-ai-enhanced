{"id": "2601.08835", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08835", "abs": "https://arxiv.org/abs/2601.08835", "authors": ["Vaarunay Kaushal", "Taranveer Singh"], "title": "DeliberationBench: When Do More Voices Hurt? A Controlled Study of Multi-LLM Deliberation Protocols", "comment": "6 pages, 5 figures", "summary": "Multi-agent systems where Large Language Models (LLMs) deliberate to form consensus have gained significant attention, yet their practical value over simpler methods remains under-scrutinized. We introduce DELIBERATIONBENCH, a controlled benchmark evaluating three deliberation protocols against a strong baseline of selecting the best response from a pool of model outputs. Across 270 questions and three independent seeds (810 total evaluations), we find a striking negative result: the best-single baseline achieves an 82.5% +- 3.3% win rate, dramatically outperforming the best deliberation protocol(13.8% +- 2.6%). This 6.0x performance gap is statistically significant (p < 0.01) and comes at 1.5-2.5x higher computational cost. Our findings challenge assumptions that complexity enhances quality in multi-LLM systems.", "AI": {"tldr": "DELIBERATIONBENCH评估了三种多代理系统中的语言模型商议协议，发现最佳单一模型的输出显著优于各种商议协议，尽管计算成本更高。", "motivation": "研究多代理系统中语言模型商议形成共识的有效性，并质疑其在简单方法上的实际价值。", "method": "引入DELIBERATIONBENCH基准测试，比较三种商议协议和单一模型选择策略的表现。", "result": "最佳单一模型策略在810次独立评估中以82.5%的成功率大幅领先，而最佳商议协议仅为13.8%。", "conclusion": "研究结果挑战了多语言模型系统中复杂性提升质量的假设。"}}
{"id": "2601.08836", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.08836", "abs": "https://arxiv.org/abs/2601.08836", "authors": ["Zaber Al Hassan Ayon", "Nur Hafieza Ismail", "Nur Shazwani Kamarudin"], "title": "A Review: PTSD in Pre-Existing Medical Condition on Social Media", "comment": "Published in (IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 15, No. 11, 2024", "summary": "Post-Traumatic Stress Disorder (PTSD) is a multifaceted mental health condition, particularly challenging for individuals with pre-existing medical conditions. This review critically examines the intersection of PTSD and chronic illnesses as expressed on social media platforms. By systematically analyzing literature from 2008 to 2024, the study explores how PTSD manifests and is managed in individuals with chronic conditions such as cancer, heart disease, and autoimmune disorders, with a focus on online expressions on platforms like X (formally known as Twitter) and Facebook. Findings demonstrate that social media data offers valuable insights into the unique challenges faced by individuals with both PTSD and chronic illnesses. Specifically, natural language processing (NLP) and machine learning (ML) techniques can identify potential PTSD cases among these populations, achieving accuracy rates between 74% and 90%. Furthermore, the role of online support communities in shaping coping strategies and facilitating early interventions is highlighted. This review underscores the necessity of incorporating considerations of pre-existing medical conditions in PTSD research and treatment, emphasizing social media's potential as a monitoring and support tool for vulnerable groups. Future research directions and clinical implications are also discussed, with an emphasis on developing targeted interventions.", "AI": {"tldr": "该研究通过分析社交平台上的数据，发现使用NLP和ML技术可以高准确性识别患有慢性疾病的PTSD案例，社交支持网络对早期干预和治疗策略有重要影响。", "motivation": "鉴于创伤后应激障碍（PTSD）是一种复杂的心理健康状况，尤其对患有慢性疾病的个体更具挑战性，本综述批判性地审视了PTSD与慢性疾病的交叉点，特别是在社交媒体平台上的表现。", "method": "通过系统性分析2008年至2024年的文献，本研究探讨了患有慢性疾病（如癌症、心脏病和自身免疫疾病）的人群中创伤后应激障碍（PTSD）的呈现和管理情况，重点分析了X（原推特）和Facebook等社交平台上的在线表达方式。", "result": "研究发现，社交媒体数据能够提供关于同时患有PTSD和慢性疾病的个体所面临的独特挑战的重要见解。通过使用自然语言处理（NLP）和机器学习（ML）技术，可以在这些人群中识别出潜在的PTSD案例，其准确性在74%到90%之间。此外，还强调了在线支持社区在形成应对策略和促进早期干预方面的作用。", "conclusion": "本综述强调了在PTSD的科研和诊疗中纳入预存医疗状况的重要性，强调了社交媒体作为监控和支持易受损群体的潜在工具。同时还讨论了未来研究的潜在方向及临床应用，特别是针对性干预措施的开发。"}}
{"id": "2601.08837", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08837", "abs": "https://arxiv.org/abs/2601.08837", "authors": ["Piercosma Bisconti", "Marcello Galisai", "Matteo Prandi", "Federico Pierucci", "Olga Sorokoletova", "Francesco Giarrusso", "Vincenzo Suriani", "Marcantonio Brancale", "Daniele Nardi"], "title": "From Adversarial Poetry to Adversarial Tales: An Interpretability Research Agenda", "comment": null, "summary": "Safety mechanisms in LLMs remain vulnerable to attacks that reframe harmful requests through culturally coded structures. We introduce Adversarial Tales, a jailbreak technique that embeds harmful content within cyberpunk narratives and prompts models to perform functional analysis inspired by Vladimir Propp's morphology of folktales. By casting the task as structural decomposition, the attack induces models to reconstruct harmful procedures as legitimate narrative interpretation. Across 26 frontier models from nine providers, we observe an average attack success rate of 71.3%, with no model family proving reliably robust. Together with our prior work on Adversarial Poetry, these findings suggest that structurally-grounded jailbreaks constitute a broad vulnerability class rather than isolated techniques. The space of culturally coded frames that can mediate harmful intent is vast, likely inexhaustible by pattern-matching defenses alone. Understanding why these attacks succeed is therefore essential: we outline a mechanistic interpretability research agenda to investigate how narrative cues reshape model representations and whether models can learn to recognize harmful intent independently of surface form.", "AI": {"tldr": "本研究表明，通过文化编码的结构嵌入有害内容的攻击技术对多种大型语言模型产生了显著的攻击成功率，突显出模型在识别和抵制隐含有害请求方面的普遍脆弱性。", "motivation": "大型语言模型中的安全机制仍然容易受到攻击，这些攻击通过文化编码的结构重新表述有害请求。", "method": "本研究介绍了Adversarial Tales，一种通过将有害内容嵌入赛博朋克叙述中，并利用弗拉基米尔·普罗普的民间故事形态学启发模型进行功能分析的攻击技术。通过将任务视为结构分解的方式，该攻击诱导模型重构有害程序作为一个合法的叙述解释。", "result": "研究对来自九家供应商的26个前沿模型进行了测试，观察到攻击成功率平均为71.3%，没有任何模型家族表现出可靠的鲁棒性。与之前关于Adversarial Poetry的工作结合来看，这些发现表明结构化地牢逃脱构成了一个广泛的漏洞类别，而不仅仅是孤立的技术。", "conclusion": "这些攻击的成功理解至关重要：研究提出了一个机械解释性研究议程，以调查叙述线索如何重塑模型表示形式，以及模型是否可以学习在不依赖表层形式的情况下识别有害意图。"}}
{"id": "2601.08838", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08838", "abs": "https://arxiv.org/abs/2601.08838", "authors": ["Jiahui Chen", "Lei Fu", "Jian Cui", "Yu Lei", "Zhenning Dong"], "title": "Companion Agents: A Table-Information Mining Paradigm for Text-to-SQL", "comment": "11 pages", "summary": "Large-scale Text-to-SQL benchmarks such as BIRD typically assume complete and accurate database annotations as well as readily available external knowledge, which fails to reflect common industrial settings where annotations are missing, incomplete, or erroneous. This mismatch substantially limits the real-world applicability of state-of-the-art (SOTA) Text-to-SQL systems. To bridge this gap, we explore a database-centric approach that leverages intrinsic, fine-grained information residing in relational databases to construct missing evidence and improve Text-to-SQL accuracy under annotation-scarce conditions. Our key hypothesis is that when a query requires multi-step reasoning over extensive table information, existing methods often struggle to reliably identify and utilize the truly relevant knowledge. We therefore propose to \"cache\" query-relevant knowledge on the database side in advance, so that it can be selectively activated at inference time. Based on this idea, we introduce Companion Agents (CA), a new Text-to-SQL paradigm that incorporates a group of agents accompanying database schemas to proactively mine and consolidate hidden inter-table relations, value-domain distributions, statistical regularities, and latent semantic cues before query generation. Experiments on BIRD under the fully missing evidence setting show that CA recovers +4.49 / +4.37 / +14.13 execution accuracy points on RSL-SQL / CHESS / DAIL-SQL, respectively, with larger gains on the Challenging subset +9.65 / +7.58 / +16.71. These improvements stem from CA's automatic database-side mining and evidence construction, suggesting a practical path toward industrial-grade Text-to-SQL deployment without reliance on human-curated evidence.", "AI": {"tldr": "论文提出数据库中心方法'Companion Agents'，在大规模文本到SQL任务中，通过提前缓存查询相关的知识，自动挖掘和归纳数据库中的隐藏信息，以提高在证据稀缺条件下的Text-to-SQL准确性。", "motivation": "现有大型Text-to-SQL基准如BIRD假设完善的数据库标注和外部知识的存在，这在工业界通常不符合实际情况。该方法旨在解决标注缺失、不完整或错误的问题，提升系统的实际应用性。", "method": "提出的Companion Agents（CA）是一种新的Text-to-SQL范式，包括一组伴随数据库模式的代理，提前挖掘和整合隐藏的表间关系、值域分布、统计规律和潜在意图线索。", "result": "在BIRD基准测试中，当所有证据完全缺失时，CA分别在RSL-SQL、CHESS和DAIL-SQL上恢复了+4.49、+4.37、+14.13的执行准确率点，尤其在挑战性子集上表现更佳。", "conclusion": "CA方法展示了自动化数据库侧的证据构造和挖掘能力，在减少对人工标注依赖的同时提高了Text-to-SQL系统在工业界的可行性。"}}
{"id": "2601.08834", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08834", "abs": "https://arxiv.org/abs/2601.08834", "authors": ["Yufeng Zhong", "Lei Chen", "Zhixiong Zeng", "Xuanle Zhao", "Deyang Jiang", "Liming Zheng", "Jing Huang", "Haibo Qiu", "Peng Shi", "Siqi Yang", "Lin Ma"], "title": "Reading or Reasoning? Format Decoupled Reinforcement Learning for Document OCR", "comment": "technical report", "summary": "Reading text from images or scanned documents via OCR models has been a longstanding focus of researchers. Intuitively, text reading is perceived as a straightforward perceptual task, and existing work primarily focuses on constructing enriched data engineering to enhance SFT capabilities. In this work, we observe that even advanced OCR models exhibit significantly higher entropy in formatted text (\\emph{e.g.}, formula, table, etc.) compared to plain text, often by an order of magnitude. These statistical patterns reveal that advanced OCR models struggle with high output uncertainty when dealing with format sensitive document, suggesting that reasoning over diverse reading pathways may improve OCR performance. To address this, we propose format decoupled reinforcement learning (FD-RL), which leverages high-entropy patterns for targeted optimization. Our approach employs entropy-based data filtration strategy to identify format-intensive instances, and adopt format decoupled rewards tailored to different format types, enabling format-level validation rather than token-level memorization. FD-RL achieves an average score of 90.41 on OmniDocBench, setting a new record for end-to-end models on this highly popular benchmark. More importantly, we conduct comprehensive ablation studies over data, training, filtering, and rewarding strategies, thoroughly validating their effectiveness.", "AI": {"tldr": "本文提出FD-RL方法，通过熵值和格式信息优化OCR处理复杂格式文本的性能，取得了显著成果。", "motivation": "传统的OCR模型在处理公式、表格等格式化的文本时展现出较高的输出不确定性。本文旨在改善OCR模型处理格式敏感文件的性能。", "method": "本文提出了一种名为格式解耦强化学习（FD-RL）的方法，通过熵值较高的数据过滤策略来识别格式密集的实例，并采用针对不同格式类型的格式解耦奖励，以实现格式级别的验证，而非逐个标记级别的记忆。", "result": "FD-RL方法在OmniDocBench基准测试上的平均分数达到了90.41，为端到端模型设定了新的记录。", "conclusion": "本文通过详细的数据、训练、过滤和奖励策略的消融研究，证明了它们的有效性，对提高OCR模型处理格式化文本的效率有积极的影响。"}}
{"id": "2601.08839", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08839", "abs": "https://arxiv.org/abs/2601.08839", "authors": ["Toshiyuki Shigemura"], "title": "Recursive Knowledge Synthesis for Multi-LLM Systems: Stability Analysis and Tri-Agent Audit Framework", "comment": "25 pages, 9 figures. Pilot feasibility study using public-access large language models without API-level orchestration", "summary": "This paper presents a tri-agent cross-validation framework for analyzing stability and explainability in multi-model large language systems. The architecture integrates three heterogeneous LLMs-used for semantic generation, analytical consistency checking, and transparency auditing-into a recursive interaction cycle. This design induces Recursive Knowledge Synthesis (RKS), where intermediate representations are continuously refined through mutually constraining transformations irreducible to single-model behavior. Across 47 controlled trials using public-access LLM deployments (October 2025), we evaluated system stability via four metrics: Reflex Reliability Score (RRS), Transparency Score (TS), Deviation Detection Rate (DDR), and Correction Success Rate (CSR). The system achieved mean RRS = 0.78+-0.06 and maintained TS >= 0.8 in about 68% of trials. Approximately 89% of trials converged, supporting the theoretical prediction that transparency auditing acts as a contraction operator within the composite validation mapping. The contributions are threefold: (1) a structured tri-agent framework for coordinated reasoning across heterogeneous LLMs, (2) a formal RKS model grounded in fixed-point theory, and (3) empirical evaluation of inter-model stability under realistic, non-API public-access conditions. These results provide initial empirical evidence that a safety-preserving, humansupervised multi-LLM architecture can achieve stable recursive knowledge synthesis in realistic, publicly deployed environments.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2601.08860", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08860", "abs": "https://arxiv.org/abs/2601.08860", "authors": ["Tarannum Mithila"], "title": "Bias Detection and Rotation-Robustness Mitigation in Vision-Language Models and Generative Image Models", "comment": "Preprint. This work is derived from the author's Master's research. Code and supplementary materials will be released separately", "summary": "Vision-Language Models (VLMs) and generative image models have achieved remarkable performance across multimodal tasks, yet their robustness and fairness under input transformations remain insufficiently explored. This work investigates bias propagation and robustness degradation in state-of-the-art vision-language and generative models, with a particular focus on image rotation and distributional shifts. We analyze how rotation-induced perturbations affect model predictions, confidence calibration, and demographic bias patterns. To address these issues, we propose rotation-robust mitigation strategies that combine data augmentation, representation alignment, and model-level regularization. Experimental results across multiple datasets demonstrate that the proposed methods significantly improve robustness while reducing bias amplification without sacrificing overall performance. This study highlights critical limitations of current multimodal systems and provides practical mitigation techniques for building more reliable and fair AI models.", "AI": {"tldr": "研究了视觉语言和生成模型在图像旋转和分布偏移下的偏见传播和鲁棒性退化问题，提出旋转鲁棒性缓解策略，实验证明改进了模型的鲁棒性并减少了偏见放大。", "motivation": "视觉语言模型和生成图像模型在多模态任务中表现出色，但它们在输入变换下的鲁棒性和公平性尚未得到充分研究。本工作旨在探究最先进的视觉语言和生成模型在偏见传播和鲁棒性退化方面的问题。", "method": "此研究通过分析图像旋转和分布偏移对模型预测、置信度校准和人口统计偏见模式的影响，提出了一种旋转鲁棒性的缓解策略。该策略结合了数据增强、表示对齐和模型级正则化。", "result": "实验结果显示，提出的方法显著改善了模型的鲁棒性，并在不牺牲总体性能的情况下减少了偏见的放大。", "conclusion": "这项研究表明了当前多模态系统的关键限制，并提供了构建更可靠和公平的AI模型的实际缓解技术。"}}
{"id": "2601.08840", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08840", "abs": "https://arxiv.org/abs/2601.08840", "authors": ["Xiaoqi Han", "Víctor Gutiérrez-Basulto", "Ru Li", "Xiaoli Li", "Jiye Liang", "Jeff Z. Pan"], "title": "Consistency-Aware Editing for Entity-level Unlearning in Language Models", "comment": null, "summary": "Large language models (LLMs) risk retaining sensitive, copyrighted, or harmful information from their training data. Entity-level unlearning addresses this issue by removing all knowledge of a specific entity while preserving the model's overall capabilities. Existing approaches typically rely on full-model fine-tuning or prompt-based interventions, which can be computationally expensive or brittle when handling paraphrased queries. Recently, model editing has emerged as an efficient alternative for updating knowledge in LLMs, offering a promising direction for unlearning. However, existing editing techniques are typically designed for instance-level updates, modifying responses to specific attributes of an entity rather than eliminating all knowledge associated with the entity. In this paper, we investigate how editing techniques can be adapted for effective and efficient entity-level unlearning. To this end, we introduce a novel consistency-aware editing (CAE) framework. CAE aggregates a diverse set of prompts related to a target entity, including its attributes, relations, and adversarial paraphrases. It then jointly learns a low-rank update guided by a consistency regularizer that aligns the editing directions across prompts. This promotes robust and comprehensive forgetting while minimizing interference with unrelated knowledge. We further examine where different entities are stored within the model and how many diverse prompts are needed for successful unlearning. We evaluate CAE on two challenging benchmarks, RWKU and ToFU, and demonstrate that it (i) provides insights into how entity-level knowledge is internally represented and deleted in LLMs, (ii) significantly improves forgetting accuracy and robustness over traditional unlearning and editing baselines, and (iii) enables scalable entity removal using only tens of carefully selected prompts.", "AI": {"tldr": "本文提出了一种新的编辑框架 CAE，用于实现高效且有效的实体级别删除。CAE 通过一致性正则化器学习低秩更新，提高了遗忘准确性和鲁棒性，减少了需要的多样化提示数量。", "motivation": "大型语言模型(LLM)在训练数据中可能保留敏感、受版权保护或有害信息。实体级别删除旨在移除特定实体的所有知识，同时保持模型的整体能力。现有的方法如全模型微调或提示干预要么计算量大要么处理释义查询时表现脆弱。本文通过提出适应于实体级别删除的编辑技术来解决这个问题。", "method": "本文提出了一种新的基于一致性的编辑框架(CAE)，该框架集成了与目标实体相关的一系列多样化提示，包括实体的属性、关系以及对抗性释义。CAE 通过一致性正则化器指导低秩更新的学习，这有助于在保持与无关知识的最小干扰的同时，实现健壮且全面的遗忘。", "result": "实验表明，CAE 在两个具有挑战性的基准测试(RWKU 和 ToFU)上表现良好，提高了遗忘准确性与鲁棒性，减少了成功遗忘所需的多样化提示数量。", "conclusion": "本文证明了提出的 CAE 框架能够提供关于实体级别知识在 LLM 中如何表示以及如何删除的见解，并且提升了遗忘的准确性和鲁棒性，同时实现了使用数十个精心选择的提示进行可扩展实体删除的目标。"}}
{"id": "2601.08867", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08867", "abs": "https://arxiv.org/abs/2601.08867", "authors": ["Qingyu Liu", "Zhongjie Ba", "Jianmin Guo", "Qiu Wang", "Zhibo Wang", "Jie Shi", "Kui Ren"], "title": "R$^2$BD: A Reconstruction-Based Method for Generalizable and Efficient Detection of Fake Images", "comment": null, "summary": "Recently, reconstruction-based methods have gained attention for AIGC image detection. These methods leverage pre-trained diffusion models to reconstruct inputs and measure residuals for distinguishing real from fake images. Their key advantage lies in reducing reliance on dataset-specific artifacts and improving generalization under distribution shifts. However, they are limited by significant inefficiency due to multi-step inversion and reconstruction, and their reliance on diffusion backbones further limits generalization to other generative paradigms such as GANs.\n  In this paper, we propose a novel fake image detection framework, called R$^2$BD, built upon two key designs: (1) G-LDM, a unified reconstruction model that simulates the generation behaviors of VAEs, GANs, and diffusion models, thereby broadening the detection scope beyond prior diffusion-only approaches; and (2) a residual bias calculation module that distinguishes real and fake images in a single inference step, which is a significant efficiency improvement over existing methods that typically require 20$+$ steps.\n  Extensive experiments on the benchmark from 10 public datasets demonstrate that R$^2$BD is over 22$\\times$ faster than existing reconstruction-based methods while achieving superior detection accuracy. In cross-dataset evaluations, it outperforms state-of-the-art methods by an average of 13.87\\%, showing strong efficiency and generalization across diverse generative methods. The code and dataset used for evaluation are available at https://github.com/QingyuLiu/RRBD.", "AI": {"tldr": "The paper proposes R$^2$BD, a fake image detection framework that is more efficient (22x faster) and generalizes better across different generative methods like VAEs and GANs, compared to current diffusion-only approaches.", "motivation": "The motivation is to address the inefficiency and limited generality of current reconstruction-based methods by proposing a more efficient and versatile fake image detection framework.", "method": "The paper introduces R$^2$BD, which consists of G-LDM and a residual bias calculation module for fake image detection. G-LDM simulates the generation behaviors of VAEs, GANs, and diffusion models, while the residual bias module distinguishes real and fake images in one step.", "result": "Experiments on 10 public datasets show that R$^2$BD is over 22 times faster than existing methods and has superior detection accuracy on both in-distribution and cross-dataset evaluations.", "conclusion": "The proposed R$^2$BD framework outperforms state-of-the-art methods, demonstrating strong efficiency and generalization when detecting fake images generated from various paradigms (VAEs, GANs, diffusion models)."}}
{"id": "2601.08841", "categories": ["cs.CL", "cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2601.08841", "abs": "https://arxiv.org/abs/2601.08841", "authors": ["Mihael Arcan"], "title": "Triples and Knowledge-Infused Embeddings for Clustering and Classification of Scientific Documents", "comment": null, "summary": "The increasing volume and complexity of scientific literature demand robust methods for organizing and understanding research documents. In this study, we explore how structured knowledge, specifically, subject-predicate-object triples, can enhance the clustering and classification of scientific papers. We propose a modular pipeline that combines unsupervised clustering and supervised classification over multiple document representations: raw abstracts, extracted triples, and hybrid formats that integrate both. Using a filtered arXiv corpus, we extract relational triples from abstracts and construct four text representations, which we embed using four state-of-the-art transformer models: MiniLM, MPNet, SciBERT, and SPECTER. We evaluate the resulting embeddings with KMeans, GMM, and HDBSCAN for unsupervised clustering, and fine-tune classification models for arXiv subject prediction. Our results show that full abstract text yields the most coherent clusters, but that hybrid representations incorporating triples consistently improve classification performance, reaching up to 92.6% accuracy and 0.925 macro-F1. We also find that lightweight sentence encoders (MiniLM, MPNet) outperform domain-specific models (SciBERT, SPECTER) in clustering, while SciBERT excels in structured-input classification. These findings highlight the complementary benefits of combining unstructured text with structured knowledge, offering new insights into knowledge-infused representations for semantic organization of scientific documents.", "AI": {"tldr": "针对科学文献处理的难题，本研究提出一种结合无监督和监督方法、及多种文档表示形式的处理流程。使用了从摘要中提取的三元组，进一步利用多种模型进行了嵌入处理。研究结果展示了三元组与文本结合所带来的性能提升，提供了新的知识注入表示方法的见解。", "motivation": "面对科学文献日益增长的数量和复杂性，本研究旨在探索如何使用结构化的知识，特别是主题-谓词-对象三元组，来改进科学论文的分类和聚类处理。", "method": "本研究提出了一个模块化的处理流程，结合了无监督聚类和监督分类，处理多种文档表示形式：原始摘要、提取出的三元组、以及混合格式的文档。从经过筛选的arXiv语料库中，研究人员从摘要中提取出关系三元组，并构建四种文本表示形式，使用四种前沿的transformer模型进行嵌入处理：MiniLM、MPNet、SciBERT、SPECTER。", "result": "研究结果表明，完整的摘要文本能够生成最连贯的聚类，但结合了三元组的混合表示形式显著提升了分类性能，最高达到了92.6%的准确率和0.925的macro-F1评分。此外，简易的句子编码器（MiniLM，MPNet）在聚类任务中表现优于领域特定模型（SciBERT，SPECTER），而在结构化输入分类中，SciBERT表现最佳。", "conclusion": "这些发现凸显了结合非结构化文本与结构化知识所带来的互补优势，为科学文献的语义组织提供了新的洞察。"}}
{"id": "2601.08868", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08868", "abs": "https://arxiv.org/abs/2601.08868", "authors": ["Yi Wang", "Yinfeng Yu", "Bin Ren"], "title": "Residual Cross-Modal Fusion Networks for Audio-Visual Navigation", "comment": "Main paper (10 pages). Accepted for publication by the 14th international conference on Computational Visual Media (CVM 2026)", "summary": "Audio-visual embodied navigation aims to enable an agent to autonomously localize and reach a sound source in unseen 3D environments by leveraging auditory cues. The key challenge of this task lies in effectively modeling the interaction between heterogeneous features during multimodal fusion, so as to avoid single-modality dominance or information degradation, particularly in cross-domain scenarios. To address this, we propose a Cross-Modal Residual Fusion Network, which introduces bidirectional residual interactions between audio and visual streams to achieve complementary modeling and fine-grained alignment, while maintaining the independence of their representations. Unlike conventional methods that rely on simple concatenation or attention gating, CRFN explicitly models cross-modal interactions via residual connections and incorporates stabilization techniques to improve convergence and robustness. Experiments on the Replica and Matterport3D datasets demonstrate that CRFN significantly outperforms state-of-the-art fusion baselines and achieves stronger cross-domain generalization. Notably, our experiments also reveal that agents exhibit differentiated modality dependence across different datasets. The discovery of this phenomenon provides a new perspective for understanding the cross-modal collaboration mechanism of embodied agents.", "AI": {"tldr": "我们提出了一种新的跨模态残差融合网络（CRFN），它在跨域场景下针对音频-视觉具身导航任务实现了更强的跨域泛化能力，并揭示了不同数据集下具身代理对不同模态的依赖性差异。", "motivation": "音频-视觉具身导航任务的关键挑战在于跨模态融合过程中有效建模异质特征间的交互，以避免单模态主导或信息损耗，特别是在跨域场景中。", "method": "我们提出了一种跨模态残差融合网络（CRFN），该网络通过引入音频流和视觉流之间的双向残差交互来实现互补建模和精细对齐，同时保持它们表示的独立性。CRFN 通过残差连接明确建模跨模态交互，并结合稳定性技术以提高收敛性和鲁棒性。", "result": "在 Replica 和 Matterport3D 数据集上进行的实验表明，CRFN 在多模态融合基线上显著优于最先进的方法，并实现了更强的跨域泛化能力。实验还揭示了不同数据集下具身代理对不同模态的依赖性差异。", "conclusion": "CRFN 的提出为跨模态协作机制提供了一种新的视角，特别是通过其在跨域泛化方面的能力以及揭示的跨数据集模态依赖性现象。"}}
{"id": "2601.08842", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08842", "abs": "https://arxiv.org/abs/2601.08842", "authors": ["Felipe Biava Cataneo"], "title": "Resisting Correction: How RLHF Makes Language Models Ignore External Safety Signals in Natural Conversation", "comment": null, "summary": "Safety architectures for language models increasingly rely on external monitors to detect errors and inject corrective signals at inference time. For such systems to function in interactive settings, models must be able to incorporate externally provided confidence information into their verbal responses. In this work, we test whether instruction-tuned language models preserve this controllability across different interaction modes.\n  Using Llama-3.2-3B on GSM8K, we perform a causal intervention study in which explicit external confidence signals are injected and model compliance is measured under multiple prompt strategies. We find that base models exhibit near-perfect controllability (Spearman rho close to 1.0), while instruction-tuned models display a striking context dependence: they fully comply with external corrections under explicit command prompts (bias approximately 0 percent, rho = 0.93), yet systematically ignore the same signals in natural conversational queries (bias plus 40 percent, rho = 0.04).\n  This behavior is not a capability failure; the model can process the signal, but an emergent property of RLHF optimization that prioritizes conversational fluency over external calibration cues in natural dialogue. We further show that internal token-level confidence in small models is uninformative (r = 0.035), underscoring the necessity of external supervision. Our findings highlight a deployment-critical failure mode: the interaction style users expect is precisely where safety corrections are least effective.", "AI": {"tldr": "安全架构依靠外部监护检测错误并在推理时注入纠正信号。研究发现指令调整的模型对不同交互模式下的外部置信信号反应不同，尤其是自然对话中表现出系统性忽视。", "motivation": "研究的动机在于探讨语言模型在互动场景中如何有效整合外部置信信息，以及指令调整后的模型在不同互动模式下的可控性。", "method": "通过在GSM8K数据集上使用Llama-3.2-3B模型进行因果干预研究，测试了指令调整的语言模型在不同交互模式下是否保持可控性。研究中注入了明确的外部置信信号，并通过多种提示策略测量模型的遵从性。", "result": "基础模型表现出近完美的可控性，而指令调整后的模型显示出显著的上下文依赖性：它们在明确命令提示下完全遵循外部纠正（偏差约0%，Spearman相关系数为0.93），但在自然对话查询中却系统地忽略相同的信号（偏差加40%，相关系数为0.04）。", "conclusion": "研究揭示了一个部署关键的失败模式：用户期望的交互风格正是安全修正最无效的地方。这凸显了外部监督的必要性，因为小型模型的内部令牌置信度是不具信息性的。"}}
{"id": "2601.08873", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.08873", "abs": "https://arxiv.org/abs/2601.08873", "authors": ["Hema Hariharan Samson"], "title": "ForensicFormer: Hierarchical Multi-Scale Reasoning for Cross-Domain Image Forgery Detection", "comment": "9 pages, 4 figures, 5 tables. Technical report on hierarchical multi-scale image forgery detection", "summary": "The proliferation of AI-generated imagery and sophisticated editing tools has rendered traditional forensic methods ineffective for cross-domain forgery detection. We present ForensicFormer, a hierarchical multi-scale framework that unifies low-level artifact detection, mid-level boundary analysis, and high-level semantic reasoning via cross-attention transformers. Unlike prior single-paradigm approaches, which achieve <75% accuracy on out-of-distribution datasets, our method maintains 86.8% average accuracy across seven diverse test sets, spanning traditional manipulations, GAN-generated images, and diffusion model outputs - a significant improvement over state-of-the-art universal detectors. We demonstrate superior robustness to JPEG compression (83% accuracy at Q=70 vs. 66% for baselines) and provide pixel-level forgery localization with a 0.76 F1-score. Extensive ablation studies validate that each hierarchical component contributes 4-10% accuracy improvement, and qualitative analysis reveals interpretable forensic features aligned with human expert reasoning. Our work bridges classical image forensics and modern deep learning, offering a practical solution for real-world deployment where manipulation techniques are unknown a priori.", "AI": {"tldr": "本文提出了一种名为ForensicFormer的多尺度分层框架，用于跨域伪造检测，结合低级、中级和高级推理，解决了当前AI生成图像的挑战。", "motivation": "传统的伪造检测方法在面对高级AI生成图像时变得无效，本文旨在解决跨领域的伪造检测问题，提高检测精度。", "method": "ForensicFormer采用跨注意力Transformer统一低级、中级和高级推理，并通过多级组件提升性能。", "result": "在七种多样化的测试集上，ForensicFormer的平均准确率为86.8%，比基于人工的方法提高了11.8％以上，并且在JPEG压缩下的准确率为83%，比基准高出17%。", "conclusion": "本文的工作融合了传统图像取证和现代深度学习，提供了应对未知先行操作技巧的实用解决方案。"}}
{"id": "2601.08843", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08843", "abs": "https://arxiv.org/abs/2601.08843", "authors": ["Haotian Deng", "Chris Farber", "Jiyoon Lee", "David Tang"], "title": "Rubric-Conditioned LLM Grading: Alignment, Uncertainty, and Robustness", "comment": null, "summary": "Automated short-answer grading (ASAG) remains a challenging task due to the linguistic variability of student responses and the need for nuanced, rubric-aligned partial credit. While Large Language Models (LLMs) offer a promising solution, their reliability as automated judges in rubric-based settings requires rigorous assessment. In this paper, we systematically evaluate the performance of LLM-judges for rubric-based short-answer grading. We investigate three key aspects: the alignment of LLM grading with expert judgment across varying rubric complexities, the trade-off between uncertainty and accuracy facilitated by a consensus-based deferral mechanism, and the model's robustness under random input perturbations and adversarial attacks. Using the SciEntsBank benchmark and Qwen 2.5-72B, we find that alignment is strong for binary tasks but degrades with increased rubric granularity. Our \"Trust Curve\" analysis demonstrates a clear trade-off where filtering low-confidence predictions improves accuracy on the remaining subset. Additionally, robustness experiments reveal that while the model is resilient to prompt injection, it is sensitive to synonym substitutions. Our work provides critical insights into the capabilities and limitations of rubric-conditioned LLM judges, highlighting the importance of uncertainty estimation and robustness testing for reliable deployment.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
