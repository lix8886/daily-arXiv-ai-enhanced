<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 39]
- [cs.CV](#cs.CV) [Total: 35]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Structured Information Matters: Explainable ICD Coding with Patient-Level Knowledge Graphs](https://arxiv.org/abs/2509.09699)
*Mingyang Li,Viktor Schlegel,Tingting Mu,Warren Del-Pinto,Goran Nenadic*

Main category: cs.CL

> 研究采用知识图谱集成到临床文档中，实现了23%文本信息的高效表示，同时提高了自动化ICD-9编码的效率和精确度以及模型的解释能力，Macro-F1最高提升3.20%。

<details>
  <summary>Details</summary>

**Motivation:** 临床文档映射到标准化临床词汇表是一项重要任务，因为它为信息检索和分析提供了结构化数据，这对临床研究、医院管理和改善患者护理至关重要。然而，手动编码既困难又耗时，这使得在更大规模上操作不切实际。

**Method:** 本研究通过计算输入文档的结构化表示，使用了文档级别的知识图谱（KG），以提供患者病情的全面结构化视图。该知识图谱能够以原文本23%的长度有效表示以患者为中心的输入文件，同时保留了90%的信息。

**Result:** 将知识图谱整合到最新的ICD编码架构PLM-ICD中进行自动化ICD-9编码时，实验结果显示在流行基准上的Macro-F1得分最高提高了3.20%，同时提高了训练效率。

**Conclusion:** 本研究展示了使用知识图谱作为输入表示对于改善自动化临床编码的效果，提高了模型的性能和训练效率，并增强了模型的可解释性。

**Abstract:** Mapping clinical documents to standardised clinical vocabularies is an
important task, as it provides structured data for information retrieval and
analysis, which is essential to clinical research, hospital administration and
improving patient care. However, manual coding is both difficult and
time-consuming, making it impractical at scale. Automated coding can
potentially alleviate this burden, improving the availability and accuracy of
structured clinical data. The task is difficult to automate, as it requires
mapping to high-dimensional and long-tailed target spaces, such as the
International Classification of Diseases (ICD). While external knowledge
sources have been readily utilised to enhance output code representation, the
use of external resources for representing the input documents has been
underexplored. In this work, we compute a structured representation of the
input documents, making use of document-level knowledge graphs (KGs) that
provide a comprehensive structured view of a patient's condition. The resulting
knowledge graph efficiently represents the patient-centred input documents with
23\% of the original text while retaining 90\% of the information. We assess
the effectiveness of this graph for automated ICD-9 coding by integrating it
into the state-of-the-art ICD coding architecture PLM-ICD. Our experiments
yield improved Macro-F1 scores by up to 3.20\% on popular benchmarks, while
improving training efficiency. We attribute this improvement to different types
of entities and relationships in the KG, and demonstrate the improved
explainability potential of the approach over the text-only baseline.

</details>


### [2] [Cross-Layer Attention Probing for Fine-Grained Hallucination Detection](https://arxiv.org/abs/2509.09700)
*Malavika Suresh,Rahaf Aljundi,Ikechukwu Nkisi-Orji,Nirmalie Wiratunga*

Main category: cs.CL

> The paper introduces CLAP, an effective technique for hallucination detection in LLMs, leading to better reliability.

<details>
  <summary>Details</summary>

**Motivation:** To address the reliability concerns of LLMs due to their tendency to generate inaccurate text.

**Method:** Cross-Layer Attention Probing (CLAP), a novel activation probing technique for hallucination detection in LLMs.

**Result:** CLAP improves hallucination detection compared to baselines and allows for a detect-then-mitigate strategy to reduce hallucinations.

**Conclusion:** CLAP not only enhances the detection of hallucinations but also ensures high reliability even when applied out-of-distribution.

**Abstract:** With the large-scale adoption of Large Language Models (LLMs) in various
applications, there is a growing reliability concern due to their tendency to
generate inaccurate text, i.e. hallucinations. In this work, we propose
Cross-Layer Attention Probing (CLAP), a novel activation probing technique for
hallucination detection, which processes the LLM activations across the entire
residual stream as a joint sequence. Our empirical evaluations using five LLMs
and three tasks show that CLAP improves hallucination detection compared to
baselines on both greedy decoded responses as well as responses sampled at
higher temperatures, thus enabling fine-grained detection, i.e. the ability to
disambiguate hallucinations and non-hallucinations among different sampled
responses to a given prompt. This allows us to propose a detect-then-mitigate
strategy using CLAP to reduce hallucinations and improve LLM reliability
compared to direct mitigation approaches. Finally, we show that CLAP maintains
high reliability even when applied out-of-distribution.

</details>


### [3] [Optimal Multi-Task Learning at Regularization Horizon for Speech Translation Task](https://arxiv.org/abs/2509.09701)
*JungHo Jung,Junhyun Lee*

Main category: cs.CL

> 本文通过正则化方法在MTL中研究语音到文本翻译，提出正则化视界策略并在实验中显示出良好的性能。

<details>
  <summary>Details</summary>

**Motivation:** 端到端的语音到文本翻译通常因配对的语音-文本数据稀缺而受限。一种克服这一缺点的方法是利用来自机器翻译（MT）任务的双语数据并执行多任务学习（MTL）。

**Method:** 本文从正则化的角度制定了多任务学习（MTL），并探索了如何在不同模态内和跨模态内在序列中应用正则化。通过彻底研究一致性正则化（不同模态）和R-drop（相同模态）的影响，展示了它们对总正则化的贡献。还证明了在MTL设置中，机器翻译（MT）损失的系数是另一种正则化来源。通过这三个正则化的来源，引入了高维空间中的最优正则化轮廓——称为正则化视界。

**Result:** 实验表明，在正则化视界内调整超参数可以在MuST-C数据集上实现接近最先进性能的结果。

**Conclusion:** 研究表明，通过调节超参数以位于正则化视界内，可以实现接近最先进性能的结果。

**Abstract:** End-to-end speech-to-text translation typically suffers from the scarcity of
paired speech-text data. One way to overcome this shortcoming is to utilize the
bitext data from the Machine Translation (MT) task and perform Multi-Task
Learning (MTL). In this paper, we formulate MTL from a regularization
perspective and explore how sequences can be regularized within and across
modalities. By thoroughly investigating the effect of consistency
regularization (different modality) and R-drop (same modality), we show how
they respectively contribute to the total regularization. We also demonstrate
that the coefficient of MT loss serves as another source of regularization in
the MTL setting. With these three sources of regularization, we introduce the
optimal regularization contour in the high-dimensional space, called the
regularization horizon. Experiments show that tuning the hyperparameters within
the regularization horizon achieves near state-of-the-art performance on the
MuST-C dataset.

</details>


### [4] [Creativity Benchmark: A benchmark for marketing creativity for LLM models](https://arxiv.org/abs/2509.09702)
*Ninad Bhat,Kieran Browne,Pip Bingemann*

Main category: cs.CL

> 研究提出针对LLMs在营销创意评估的Creativity Benchmark框架，结果显示没有模型能显著超越其他模型，建议专家人类评估和多样化的评估方法对于此类任务是必要的。

<details>
  <summary>Details</summary>

**Motivation:** 目的是评估大规模语言模型在营销创意上的效能，并通过人类专家评估来验证自动评估之间的相关性以及对品牌限定任务的有效性。

**Method:** 提出Creativity Benchmark评估框架，以评估大规模语言模型（LLMs）在营销创意方面的能力。该框架覆盖100个品牌（12个类别）和三种提示类型（洞察、创意、大胆创意）。通过Bradley-Terry模型分析了678位实践创意人员的人类对对比较偏好（共11,012个匿名比较）。

**Result:** 发现性能紧密聚集，没有模型能在品牌或提示类型上占据优势：最高与最低模型的差距是$\Delta\theta \approx 0.45$，意味着一对一胜利概率为$0.61$；最高评价模型仅能在约$61	extbackslash	extpercent$的情况下胜过最低评价的模型。此外，通过余弦距离分析模型多样性，捕捉模型内部和之间的变化，并对提示重新构建的敏感性进行分析。比较三个以LLM为评委的设置与人类排名揭示了较弱且不一致的相关性以及评分人特定的偏见，凸显出自动化评委无法替代人类评估。传统创造力测试部分地适用于品牌约束的任务。

**Conclusion:** 总体而言，研究结果表明需要专家人类评估和多样化工作流程。

**Abstract:** We introduce Creativity Benchmark, an evaluation framework for large language
models (LLMs) in marketing creativity. The benchmark covers 100 brands (12
categories) and three prompt types (Insights, Ideas, Wild Ideas). Human
pairwise preferences from 678 practising creatives over 11,012 anonymised
comparisons, analysed with Bradley-Terry models, show tightly clustered
performance with no model dominating across brands or prompt types: the
top-bottom spread is $\Delta\theta \approx 0.45$, which implies a head-to-head
win probability of $0.61$; the highest-rated model beats the lowest only about
$61\%$ of the time. We also analyse model diversity using cosine distances to
capture intra- and inter-model variation and sensitivity to prompt reframing.
Comparing three LLM-as-judge setups with human rankings reveals weak,
inconsistent correlations and judge-specific biases, underscoring that
automated judges cannot substitute for human evaluation. Conventional
creativity tests also transfer only partially to brand-constrained tasks.
Overall, the results highlight the need for expert human evaluation and
diversity-aware workflows.

</details>


### [5] [CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor](https://arxiv.org/abs/2509.09703)
*Zhenhua Xu,Xixiang Zhao,Xubin Yue,Shengwei Tian,Changting Lin,Meng Han*

Main category: cs.CL

> CTCC is a novel rule-driven framework for model fingerprinting of large language models, offering improved stealth and robustness over existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the problems of existing model fingerprinting methods which are either detectable, vulnerable to adversarial modifications, or invalidated once revealed.

**Method:** The paper introduces CTCC, a rule-driven fingerprinting framework that encodes contextual correlations across multiple dialogue turns to embed verifiable ownership traces into large language models.

**Result:** The experiments show that CTCC achieves stronger stealth and robustness compared to previous methods across multiple LLM architectures.

**Conclusion:** CTCC is positioned as a reliable and practical solution for ownership verification in real-world deployment scenarios of LLMs, considering its strength in stealth and robustness.

**Abstract:** The widespread deployment of large language models (LLMs) has intensified
concerns around intellectual property (IP) protection, as model theft and
unauthorized redistribution become increasingly feasible. To address this,
model fingerprinting aims to embed verifiable ownership traces into LLMs.
However, existing methods face inherent trade-offs between stealthness,
robustness, and generalizability, being either detectable via distributional
shifts, vulnerable to adversarial modifications, or easily invalidated once the
fingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven
fingerprinting framework that encodes contextual correlations across multiple
dialogue turns, such as counterfactual, rather than relying on token-level or
single-turn triggers. CTCC enables fingerprint verification under black-box
access while mitigating false positives and fingerprint leakage, supporting
continuous construction under a shared semantic rule even if partial triggers
are exposed. Extensive experiments across multiple LLM architectures
demonstrate that CTCC consistently achieves stronger stealth and robustness
than prior work. Our findings position CTCC as a reliable and practical
solution for ownership verification in real-world LLM deployment scenarios. Our
code and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.

</details>


### [6] [Temporal Preferences in Language Models for Long-Horizon Assistance](https://arxiv.org/abs/2509.09704)
*Ali Mazyaki,Mohammad Naghizadeh,Samaneh Ranjkhah Zonouzaghi,Hossein Setareh*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** We study whether language models (LMs) exhibit future- versus
present-oriented preferences in intertemporal choice and whether those
preferences can be systematically manipulated. Using adapted human experimental
protocols, we evaluate multiple LMs on time-tradeoff tasks and benchmark them
against a sample of human decision makers. We introduce an operational metric,
the Manipulability of Time Orientation (MTO), defined as the change in an LM's
revealed time preference between future- and present-oriented prompts. In our
tests, reasoning-focused models (e.g., DeepSeek-Reasoner and grok-3-mini)
choose later options under future-oriented prompts but only partially
personalize decisions across identities or geographies. Moreover, models that
correctly reason about time orientation internalize a future orientation for
themselves as AI decision makers. We discuss design implications for AI
assistants that should align with heterogeneous, long-horizon goals and outline
a research agenda on personalized contextual calibration and socially aware
deployment.

</details>


### [7] [The Non-Determinism of Small LLMs: Evidence of Low Answer Consistency in Repetition Trials of Standard Multiple-Choice Benchmarks](https://arxiv.org/abs/2509.09705)
*Claudio Pinhanez,Paulo Cavalin,Cassia Sanctos,Marcelo Grave,Yago Primerano*

Main category: cs.CL

> 本研究通过分析小规模和中型LLMs在多次回答相同问题时的表现，以确定模型的准确性和一致性之间的关系。研究表明，模型在低推理温度下的一致性表现较好，而且一致性越高，准确性相对也会越高。

<details>
  <summary>Details</summary>

**Motivation:** 本项工作的动机在于提供一种方法来评估小规模和中型LLMs在重复回答时的表现，特别是在准确性和一致性方面，以及探讨在这种情况下最优模型的选择。

**Method:** 本研究通过分析小规模LLMs（2B-8B参数）在多次回答同一问题时的一致性来探讨其表现。研究对象包括知名开源LLMs，它们需重复回答来自MMLU-Redux和MedQA多选题基准的10次提问。研究考虑了不同的推理温度（低温和中温）、小模型（50B-80B）与微调模型与基础模型的对比以及其他参数的影响。

**Result:** 结果表明，能够一致回答的问题数量在不同模型之间差异显著，但对于低温推理的小模型而言，这一比例通常在50%到80%之间。对于中型模型，测试结果显示其答案一致性水平更高。此外，一致性答案的准确性似乎与总体准确性有合理的关系。

**Conclusion:** 研究提出了一些新的分析和图形工具来支持这些研究。得出的结论是，对于小规模和中型LLMs，其答案的一致性与准确性之间存在一定的相关性，这为选择最佳模型提供了依据。

**Abstract:** This work explores the consistency of small LLMs (2B-8B parameters) in
answering multiple times the same question. We present a study on known,
open-source LLMs responding to 10 repetitions of questions from the
multiple-choice benchmarks MMLU-Redux and MedQA, considering different
inference temperatures, small vs. medium models (50B-80B), finetuned vs. base
models, and other parameters. We also look into the effects of requiring
multi-trial answer consistency on accuracy and the trade-offs involved in
deciding which model best provides both of them. To support those studies, we
propose some new analytical and graphical tools. Results show that the number
of questions which can be answered consistently vary considerably among models
but are typically in the 50%-80% range for small models at low inference
temperatures. Also, accuracy among consistent answers seems to reasonably
correlate with overall accuracy. Results for medium-sized models seem to
indicate much higher levels of answer consistency.

</details>


### [8] [Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal](https://arxiv.org/abs/2509.09708)
*Nirmalendu Prakash,Yeo Wei Jie,Amir Abdullah,Ranjan Satapathy,Erik Cambria,Roy Ka Wei Lee*

Main category: cs.CL

> 研究了两个公开的指令调优模型，并通过稀疏自动编码器和特征选择技术揭示了模型拒绝有害提示行为的内在机制。结果表明存在冗余特征，这些特征在早期特征被抑制时才会活跃，指出可以通过操作可解释的潜层空间来进行细粒度审计和针对性干涉安全行为。

<details>
  <summary>Details</summary>

**Motivation:** 理解指令调优大型语言模型（LLMs）在面对有害提示时的拒绝行为的内在原因。

**Method:** 使用稀疏自动编码器（SAEs）对残差流激活进行训练，以研究两个公开的指令调优模型（Gemma-2-2B-IT 和 LLaMA-3.1-8B-IT）在面对有害提示时拒绝行为的内部原因。具体步骤包括：（1）寻找一个介导拒绝的方向并收集附近的 SAE 特征；（2）通过贪婪筛选来简化特征集；（3）使用因子分解机（FM）拟合剩余活跃特征和最小特征集之间的非线性交互。

**Result:** 发现了能将模型从拒绝转变为顺从特征集，揭示了拒绝行为的因果影响，并且证明存在冗余特征，这些特征在早期特征受到抑制时才会活跃。

**Conclusion:** 通过对可解释潜层空间的操作，可以实现对安全行为的细粒度审计和技术干预，为模型的安全性改进提供了新的方向。

**Abstract:** Refusal on harmful prompts is a key safety behaviour in instruction-tuned
large language models (LLMs), yet the internal causes of this behaviour remain
poorly understood. We study two public instruction-tuned models, Gemma-2-2B-IT
and LLaMA-3.1-8B-IT, using sparse autoencoders (SAEs) trained on
residual-stream activations. Given a harmful prompt, we search the SAE latent
space for feature sets whose ablation flips the model from refusal to
compliance, demonstrating causal influence and creating a jailbreak. Our search
proceeds in three stages: (1) Refusal Direction: find a refusal-mediating
direction and collect SAE features near that direction; (2) Greedy Filtering:
prune to a minimal set; and (3) Interaction Discovery: fit a factorization
machine (FM) that captures nonlinear interactions among the remaining active
features and the minimal set. This pipeline yields a broad set of
jailbreak-critical features, offering insight into the mechanistic basis of
refusal. Moreover, we find evidence of redundant features that remain dormant
unless earlier features are suppressed. Our findings highlight the potential
for fine-grained auditing and targeted intervention in safety behaviours by
manipulating the interpretable latent space.

</details>


### [9] [Assisting Research Proposal Writing with Large Language Models: Evaluation and Refinement](https://arxiv.org/abs/2509.09709)
*Jing Ren,Weiqi Wang*

Main category: cs.CL

> 本文提出了一种定量评估大型语言模型写作质量的方法，包括内容质量和引用有效性两个方面，基于这些指标，迭代提示方法提升了写作质量并减少了伦理问题。

<details>
  <summary>Details</summary>

**Motivation:** 解决大型语言模型在学术写作中由于无效或捏造引用而产生的伦理问题，以及目前内容质量评估依赖于耗时且缺乏客观性的人类判断的问题。

**Method:** 提出两个关键评估指标——内容质量和引用有效性，并基于这两个指标的得分提出了一种迭代提示方法。

**Result:** 实验结果显示，提出的指标为评估ChatGPT的写作性能提供了客观、定量的框架，并且迭代提示显著提高了内容质量，减少了引用不准确和捏造的问题。

**Conclusion:** 该方法能够有效解决学术环境中使用LLMs时的重要伦理挑战。

**Abstract:** Large language models (LLMs) like ChatGPT are increasingly used in academic
writing, yet issues such as incorrect or fabricated references raise ethical
concerns. Moreover, current content quality evaluations often rely on
subjective human judgment, which is labor-intensive and lacks objectivity,
potentially compromising the consistency and reliability. In this study, to
provide a quantitative evaluation and enhance research proposal writing
capabilities of LLMs, we propose two key evaluation metrics--content quality
and reference validity--and an iterative prompting method based on the scores
derived from these two metrics. Our extensive experiments show that the
proposed metrics provide an objective, quantitative framework for assessing
ChatGPT's writing performance. Additionally, iterative prompting significantly
enhances content quality while reducing reference inaccuracies and
fabrications, addressing critical ethical challenges in academic contexts.

</details>


### [10] [Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data](https://arxiv.org/abs/2509.09710)
*Sepehr Golrokh Amin,Devin Rhoads,Fatemeh Fakhrmoosavi,Nicholas E. Lownes,John N. Ivan*

Main category: cs.CL

> 本文介绍了一种用于生成智能交通模型中个人旅行日记的大语言模型（LLM）方案。该方法使用开源数据创建随机角色，并通过直接提示生成日记。该研究提出了一种新颖的群体现实主义评分，用于验证生成的日记与实际日记之间的相似性。结果显示，LLM生成的日记在总体现实性和统计代表性方面与传统方法相当甚至更好。

<details>
  <summary>Details</summary>

**Motivation:** 传统的旅行日记生成方法依赖于大量的专有家庭旅行调查数据，而本文的方法依赖于开源数据进行随机角色生成，并使用直接提示技术合成日记，以提高数据的开放性和可用性。

**Method:** 研究开发了一种新颖的群体现实主义评分，这款评分由四个指标组成（旅行次数评分、时间间隔评分、目的评分和模式评分），用于验证生成的日记和实际日记之间的分布相似性。

**Result:** 验证结果显示，LLM生成的日记在确定旅行目的以及一致性上有更好的表现，虽然传统的负二项式和多项式逻辑模型在旅行次数和活动时长的数值估计上略胜一筹，但LLM在整体现实性和统计代表性方面还是能够与传统方法媲美。

**Conclusion:** LLM生成的旅行日记在统计代表性和总体现实性方面具有较强的零样本执行力，进而证明其对于未来合成日记评估系统的可行性。

**Abstract:** This study introduces a Large Language Model (LLM) scheme for generating
individual travel diaries in agent-based transportation models. While
traditional approaches rely on large quantities of proprietary household travel
surveys, the method presented in this study generates personas stochastically
from open-source American Community Survey (ACS) and Smart Location Database
(SLD) data, then synthesizes diaries through direct prompting. This study
features a novel one-to-cohort realism score: a composite of four metrics (Trip
Count Score, Interval Score, Purpose Score, and Mode Score) validated against
the Connecticut Statewide Transportation Study (CSTS) diaries, matched across
demographic variables. The validation utilizes Jensen-Shannon Divergence to
measure distributional similarities between generated and real diaries. When
compared to diaries generated with classical methods (Negative Binomial for
trip generation; Multinomial Logit for mode/purpose) calibrated on the
validation set, LLM-generated diaries achieve comparable overall realism (LLM
mean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and
demonstrates greater consistency (narrower realism score distribution), while
classical models lead in numerical estimates of trip count and activity
duration. Aggregate validation confirms the LLM's statistical
representativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot
viability and establishing a quantifiable metric of diary realism for future
synthetic diary evaluation systems.

</details>


### [11] [Psychiatry-Bench: A Multi-Task Benchmark for LLMs in Psychiatry](https://arxiv.org/abs/2509.09711)
*Aya E. Fouda,Abdelrahamn A. Hassan,Radwa J. Hanafy,Mohammed E. Fouda*

Main category: cs.CL

> 本研究推出PsychiatryBench，以规范大型语言模型在精神健康领域的应用，发现现有模型在临床一致性与安全性上存在显著不足。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLMs）在提升精神健康实践中有巨大潜力，但现有的评估资源存在局限，依赖小样本临床访谈、社交媒体帖子或合成对话，这限制了其临床有效性。通过PsychiatryBench，研究旨在提供一个更有效的评估方法。

**Method:** 本研究引入了PsychiatryBench，这是一个基于权威和专家验证的教科书及案例集构建的严格筛选基准。该基准包含十一项不同的问答任务，涵盖了从诊断推理和治疗规划到纵向随访、管理规划、临床方式、序列案例分析以及多种选择/扩展匹配格式，总共超过5,300个专家标注的项目。

**Result:** 研究结果表明，在多回合随访和管理任务上，模型在临床一致性与安全性上存在显著差距，这反映了当前模型需要进行专门调优并存在更具挑战性的评估范式需求。

**Conclusion:** PsychiatryBench提供了一种模块化、可扩展的模型测评与提升平台，特别是在高风险的心理健康应用领域。

**Abstract:** Large language models (LLMs) hold great promise in enhancing psychiatric
practice, from improving diagnostic accuracy to streamlining clinical
documentation and therapeutic support. However, existing evaluation resources
heavily rely on small clinical interview corpora, social media posts, or
synthetic dialogues, which limits their clinical validity and fails to capture
the full complexity of psychiatric reasoning. In this work, we introduce
PsychiatryBench, a rigorously curated benchmark grounded exclusively in
authoritative, expert-validated psychiatric textbooks and casebooks.
PsychiatryBench comprises eleven distinct question-answering tasks ranging from
diagnostic reasoning and treatment planning to longitudinal follow-up,
management planning, clinical approach, sequential case analysis, and
multiple-choice/extended matching formats totaling over 5,300 expert-annotated
items. We evaluate a diverse set of frontier LLMs (including Google Gemini,
DeepSeek, LLaMA 3, and QWQ-32) alongside leading open-source medical models
(e.g., OpenBiloLLM, MedGemma) using both conventional metrics and an
"LLM-as-judge" similarity scoring framework. Our results reveal substantial
gaps in clinical consistency and safety, particularly in multi-turn follow-up
and management tasks, underscoring the need for specialized model tuning and
more robust evaluation paradigms. PsychiatryBench offers a modular, extensible
platform for benchmarking and improving LLM performance in high-stakes mental
health applications.

</details>


### [12] [The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization](https://arxiv.org/abs/2509.09712)
*Talha Tahir*

Main category: cs.CL

> 本研究发现，相较于监督微调（SFT），通过赔率比策略优化（ORPO）训练的小型语言模型在ACT忠实体现出色，并在治疗共情方面优于仅采用SFT或基础模型。链式思维（COT）仅对SFT有提升作用。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在探讨训练后的方法和技术对小型开放权重大语言模型（LLM）提供ACT疗法能力的影响。

**Method:** 本研究通过两种不同的方法对Llama-3.2-3b-Instruct模型进行了训练，分别是监督微调（SFT）和赔率比策略优化（ORPO），每种方法分别加入或不加入显式的链式思维（COT）步骤。使用由Mistral-Large生成的50套合成ACT对话记录来进行训练。

**Result:** 研究发现，经过ORPO训练的模型在ACT保真度和治疗共情方面显著优于SFT和基础Instruct模型。加入COT步骤仅对SFT模型有显著的积极作用，提高了ACT-FM分数，而对ORPO和基础Instruct模型没有明显好处。

**Conclusion:** 研究表明，偏好对齐的策略优化可以有效地在小型LLM中嵌入ACT能力，并且显式推理的实用价值高度依赖于基础训练范式。

**Abstract:** Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral
therapy with emerging evidence of efficacy in several psychiatric conditions.
This study investigates the impact of post-training methodology and explicit
reasoning on the ability of a small open-weight large language model (LLM) to
deliver ACT. Using 50 sets of synthetic ACT transcripts generated by
Mistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches,
supervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each
with and without an explicit chain-of-thought (COT) reasoning step. Performance
was evaluated by comparing these four post-trained variants against the base
Instruct model. These models were benchmarked in simulated therapy sessions,
with performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM)
and the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned
on human evaluations. Our findings demonstrate that the ORPO-trained models
significantly outperformed both their SFT and Instruct counterparts on ACT
fidelity ($\chi^2(5) = 185.15, p < .001$) and therapeutic empathy ($\chi^2(5) =
140.37, p < .001$). The effect of COT was conditional as it provided a
significant benefit to SFT models, improving ACT-FM scores by an average of
2.68 points ($p < .001$), while offering no discernible advantage to the
superior ORPO or instruct-tuned variants. We posit that the superiority of ORPO
stems from its ability to learn the therapeutic `process' over imitating
`content,' a key aspect of ACT, while COT acts as a necessary scaffold for
models trained only via imitation. This study establishes that
preference-aligned policy optimization can effectively instill ACT competencies
in small LLMs, and that the utility of explicit reasoning is highly dependent
on the underlying training paradigm.

</details>


### [13] [HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2509.09713)
*Duolin Sun,Dan Yang,Yue Shen,Yihan Jiao,Zhehao Tan,Jie Feng,Lianzhen Zhong,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.CL

> HANRAG框架通过改进查询处理过程，提高了应对单跳和多跳问题的性能。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在解决现有RAG方法在处理多跳查询时遇到的问题，如过多依赖迭代检索和使用原始复杂查询检索等问题，旨在提高处理多样化查询的能力。

**Method:** HANRAG采用了一种基于启发式的方法，通过一个强大的指示器将查询进行路由、分解成子查询，并从检索到的文档中过滤噪音，从而提高系统的适应性和噪声抵抗力。

**Result:** 实验结果显示，HANRAG框架在单跳和多跳问题的回答任务中都表现出优越的性能。

**Conclusion:** HANRAG作为一种新颖的启发式框架，提升了系统对于不同复杂度查询问题的处理能力，并且在多种基准测试中优于其他工业方法。

**Abstract:** The Retrieval-Augmented Generation (RAG) approach enhances question-answering
systems and dialogue generation tasks by integrating information retrieval (IR)
technologies with large language models (LLMs). This strategy, which retrieves
information from external knowledge bases to bolster the response capabilities
of generative models, has achieved certain successes. However, current RAG
methods still face numerous challenges when dealing with multi-hop queries. For
instance, some approaches overly rely on iterative retrieval, wasting too many
retrieval steps on compound queries. Additionally, using the original complex
query for retrieval may fail to capture content relevant to specific
sub-queries, resulting in noisy retrieved content. If the noise is not managed,
it can lead to the problem of noise accumulation. To address these issues, we
introduce HANRAG, a novel heuristic-based framework designed to efficiently
tackle problems of varying complexity. Driven by a powerful revelator, HANRAG
routes queries, decomposes them into sub-queries, and filters noise from
retrieved documents. This enhances the system's adaptability and noise
resistance, making it highly capable of handling diverse queries. We compare
the proposed framework against other leading industry methods across various
benchmarks. The results demonstrate that our framework obtains superior
performance in both single-hop and multi-hop question-answering tasks.

</details>


### [14] [How Small Transformation Expose the Weakness of Semantic Similarity Measures](https://arxiv.org/abs/2509.09714)
*Serge Lionel Nikiema,Albérick Euraste Djire,Abdoul Aziz Bonkoungou,Micheline Bénédicte Moumoula,Jordan Samhi,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.CL

> 研究评估了18种不同的语义相似度测量方法，包括基于词的方法、嵌入技术、基于LLM的系统和结构感知算法。结果显示，一些嵌入方法在识别语义相反的概念时存在严重问题，而基于LLM的方法表现更好。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在探讨不同方法测量语义相似度的能力，特别是在软件工程应用中的表现，如代码搜索、API推荐、自动化代码审查和重构工具。

**Method:** Structure

**Result:** 一些基于嵌入的方法错误地将语义相反的概念识别为相似，而基于LLM的方法在区分语义差异方面表现更好。同时，嵌入方法的性能可以通过将距离计算从欧几里得距离改为余弦相似度来提高。

**Conclusion:** 基于嵌入的方法在识别语义相似度时表现不佳，而基于LLM的方法则显示出更高的准确性和区分度。

**Abstract:** This research examines how well different methods measure semantic
similarity, which is important for various software engineering applications
such as code search, API recommendations, automated code reviews, and
refactoring tools. While large language models are increasingly used for these
similarity assessments, questions remain about whether they truly understand
semantic relationships or merely recognize surface patterns.
  The study tested 18 different similarity measurement approaches, including
word-based methods, embedding techniques, LLM-based systems, and
structure-aware algorithms. The researchers created a systematic testing
framework that applies controlled changes to text and code to evaluate how well
each method handles different types of semantic relationships.
  The results revealed significant issues with commonly used metrics. Some
embedding-based methods incorrectly identified semantic opposites as similar up
to 99.9 percent of the time, while certain transformer-based approaches
occasionally rated opposite meanings as more similar than synonymous ones. The
study found that embedding methods' poor performance often stemmed from how
they calculate distances; switching from Euclidean distance to cosine
similarity improved results by 24 to 66 percent. LLM-based approaches performed
better at distinguishing semantic differences, producing low similarity scores
(0.00 to 0.29) for genuinely different meanings, compared to embedding methods
that incorrectly assigned high scores (0.82 to 0.99) to dissimilar content.

</details>


### [15] [Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA](https://arxiv.org/abs/2509.09715)
*Naveen Lamba,Sanju Tiwari,Manas Gaur*

Main category: cs.CL

> 研究识别了使大语言模型（LLMs）固有易受幻觉影响的关键特性，并通过转换现有问答数据集格式证实了这些特性，发现即使随着模型规模增大，由符号性元素导致的幻觉现象依然显著存在。

<details>
  <summary>Details</summary>

**Motivation:** LLM内在的幻觉问题已被广泛研究，但其内在脆弱性的具体特性尚未明确。本研究旨在识别并描述这些特性，从而找出模型内部机制中的弱点。

**Method:** 使用了HaluEval和TruthfulQA两个已建立的数据集，将现有问答格式转换为其他格式，以便缩小问题特性。

**Result:** 发现对于Gemma-2-2B，其幻觉率平均为79.0%，随着模型规模的增加到Gemma-2-9B幻觉率降低到73.6%，Gemma-2-27B时幻觉率降为63.9%。但是，象征性元素依然导致显著幻觉。

**Conclusion:** 研究发现随着模型规模的增加，尽管幻觉率有所下降，由象征性元素（如修饰语和专有名词）引起的显著幻觉仍然存在，这表明LLM处理这类输入有基础性缺陷。

**Abstract:** Hallucination in Large Language Models (LLMs) is a well studied problem.
However, the properties that make LLM intrinsically vulnerable to
hallucinations have not been identified and studied. This research identifies
and characterizes the key properties, allowing us to pinpoint vulnerabilities
within the model's internal mechanisms. To solidify on these properties, we
utilized two established datasets, HaluEval and TruthfulQA and convert their
existing format of question answering into various other formats to narrow down
these properties as the reason for the hallucinations. Our findings reveal that
hallucination percentages across symbolic properties are notably high for
Gemma-2-2B, averaging 79.0% across tasks and datasets. With increased model
scale, hallucination drops to 73.6% for Gemma-2-9B and 63.9% for Gemma-2-27B,
reflecting a 15 percentage point reduction overall. Although the hallucination
rate decreases as the model size increases, a substantial amount of
hallucination caused by symbolic properties still persists. This is especially
evident for modifiers (ranging from 84.76% to 94.98%) and named entities
(ranging from 83.87% to 93.96%) across all Gemma models and both datasets.
These findings indicate that symbolic elements continue to confuse the models,
pointing to a fundamental weakness in how these LLMs process such
inputs--regardless of their scale.

</details>


### [16] [ALIGNS: Unlocking nomological networks in psychological measurement through a large language model](https://arxiv.org/abs/2509.09723)
*Kai R. Larsen,Sen Yan,Roland Müller,Lan Sang,Mikko Rönkkö,Ravi Starzl,Donald Edmondson*

Main category: cs.CL

> ALIGNS, a large language model, is introduced to help build nomological networks, leading to more insightful classification and validation of questionnaire measures across various fields.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in building nomological networks for validated measurements, which is critical for fields like clinical trials and public policy, leading to more effective outcomes.

**Method:** Analysis of Latent Indicators to Generate Nomological Structures (ALIGNS), a large language model-based system trained with validated questionnaire measures.

**Result:** ALIGNS created three comprehensive nomological networks with over 550,000 indicators across various fields. Three evaluations were conducted demonstrating its capacity to provide new insights and serve as an important complement to traditional validation methods.

**Conclusion:** ALIGNS represents an innovative approach by using large language models to build nomological networks, which helps overcome the limitations of traditional validation methods by offering large-scale analysis.

**Abstract:** Psychological measurement is critical to many disciplines. Despite advances
in measurement, building nomological networks, theoretical maps of how concepts
and measures relate to establish validity, remains a challenge 70 years after
Cronbach and Meehl proposed them as fundamental to validation. This limitation
has practical consequences: clinical trials may fail to detect treatment
effects, and public policy may target the wrong outcomes. We introduce Analysis
of Latent Indicators to Generate Nomological Structures (ALIGNS), a large
language model-based system trained with validated questionnaire measures.
ALIGNS provides three comprehensive nomological networks containing over
550,000 indicators across psychology, medicine, social policy, and other
fields. This represents the first application of large language models to solve
a foundational problem in measurement validation. We report classification
accuracy tests used to develop the model, as well as three evaluations. In the
first evaluation, the widely used NIH PROMIS anxiety and depression instruments
are shown to converge into a single dimension of emotional distress. The second
evaluation examines child temperament measures and identifies four potential
dimensions not captured by current frameworks, and questions one existing
dimension. The third evaluation, an applicability check, engages expert
psychometricians who assess the system's importance, accessibility, and
suitability. ALIGNS is freely available at nomologicalnetwork.org,
complementing traditional validation methods with large-scale nomological
analysis.

</details>


### [17] [DiTTO-LLM: Framework for Discovering Topic-based Technology Opportunities via Large Language Model](https://arxiv.org/abs/2509.09724)
*Wonyoung Kim,Sujeong Seo,Juhyun Lee*

Main category: cs.CL

> 本文提出了一种基于专利数据的技术机会识别框架，通过追踪技术主题的变化来发现新技术机会，实验证明了该框架的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 技术机会是推动技术、行业和创新发展的关键信息。本文提出的框架旨在通过分析专利数据中的技术时间关系来识别新兴技术机会。

**Method:** 本文提出了一种基于技术之间时间关系的框架来识别新兴技术机会。该框架首先从专利数据集中提取文本，然后通过映射文本主题来发现技术之间的关系。通过追踪这些主题随时间的变化来识别技术机会。为了提高效率，该框架利用大型语言模型提取主题，并使用对话式语言模型的提示来支持技术机会的发现。

**Result:** 该框架通过使用美国专利商标局提供的AI专利数据集进行了验证，实验结果表明人工智能技术正在朝向便于日常使用的方向发展。

**Conclusion:** 本文研究表明，所提出的框架具有识别未来技术机会的潜力。

**Abstract:** Technology opportunities are critical information that serve as a foundation
for advancements in technology, industry, and innovation. This paper proposes a
framework based on the temporal relationships between technologies to identify
emerging technology opportunities. The proposed framework begins by extracting
text from a patent dataset, followed by mapping text-based topics to discover
inter-technology relationships. Technology opportunities are then identified by
tracking changes in these topics over time. To enhance efficiency, the
framework leverages a large language model to extract topics and employs a
prompt for a chat-based language model to support the discovery of technology
opportunities. The framework was evaluated using an artificial intelligence
patent dataset provided by the United States Patent and Trademark Office. The
experimental results suggest that artificial intelligence technology is
evolving into forms that facilitate everyday accessibility. This approach
demonstrates the potential of the proposed framework to identify future
technology opportunities.

</details>


### [18] [BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025](https://arxiv.org/abs/2509.09725)
*Chunyu Li,Xindi Zheng,Siqi Liu*

Main category: cs.CL

> 本文针对生物医学文本中的多语言和嵌套实体链接问题，提出了一种通过少量调整实现显著改善的方法，该方法在比赛中表现良好。

<details>
  <summary>Details</summary>

**Motivation:** 现有的生物医学实体链接研究主要针对英文并且只处理扁平提及，忽略了嵌套和多语言提及的复杂情况。本文旨在通过最小化修改解决这一问题。

**Method:** 本研究采用了一种轻量级流水线，并修改其中三个组件：两阶段检索-排序、边界提示和数据集增强，来改进现有的EL模型。

**Result:** 本文研究了生物医学文本中的实体链接（EL），特别是针对英文和俄文的嵌套及多语言提及情况。研究采用了一种轻量级的流水线，通过只修改三个任务相关组件（两阶段检索-排序、边界提示、数据集增强）来改进了现有的EL模型。这些改进包括使用相同的预训练模型进行检索阶段，进行领域特定微调进行排序阶段，以及通过三种互补的数据来源自动扩展排序训练语料库。基于这些改进，该研究提出的系统（BIBERT-Pipe）在BioNNE 2025多语言赛道的排行榜中位列第三，代码在GitHub上公开。

**Conclusion:** 通过上述改进方法，本文提出的系统在BioNNE 2025多语言赛道中表现出了有效性与竞争力。代码已经公开，供后续研究使用。

**Abstract:** Entity linking (EL) for biomedical text is typically benchmarked on
English-only corpora with flat mentions, leaving the more realistic scenario of
nested and multilingual mentions largely unexplored. We present our system for
the BioNNE 2025 Multilingual Biomedical Nested Named Entity Linking shared task
(English & Russian), closing this gap with a lightweight pipeline that keeps
the original EL model intact and modifies only three task-aligned components:
Two-stage retrieval-ranking. We leverage the same base encoder model in both
stages: the retrieval stage uses the original pre-trained model, while the
ranking stage applies domain-specific fine-tuning. Boundary cues. In the
ranking stage, we wrap each mention with learnable [Ms] / [Me] tags, providing
the encoder with an explicit, language-agnostic span before robustness to
overlap and nesting. Dataset augmentation. We also automatically expand the
ranking training corpus with three complementary data sources, enhancing
coverage without extra manual annotation. On the BioNNE 2025 leaderboard, our
two stage system, bilingual bert (BIBERT-Pipe), ranks third in the multilingual
track, demonstrating the effectiveness and competitiveness of these minimal yet
principled modifications. Code are publicly available at
https://github.com/Kaggle-Competitions-Code/BioNNE-L.

</details>


### [19] [Natural Language Translation of Formal Proofs through Informalization of Proof Steps and Recursive Summarization along Proof Structure](https://arxiv.org/abs/2509.09726)
*Seiji Hattori,Takuya Matsuzaki,Makoto Fujiwara*

Main category: cs.CL

> A method using large language models for translating formal proofs into natural language is proposed and evaluated, showing outputs that are highly readable and accurate.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to make formal proofs more accessible and understandable by translating them into natural language proofs, improving readability and accessibility for non-expert users.

**Method:** The paper employs large language models (LLMs) to informally translate machine-verifiable formal proofs into natural language through informalization and summarization capabilities.

**Result:** The proposed method generates natural language proofs that are highly readable and accurate, as demonstrated by its application to formal proof data from a textbook and an existing Lean proof assistant library.

**Conclusion:** The study concludes that leveraging LLMs to translate formal proofs into natural language can significantly enhance the readability and accuracy of such proofs, making them more accessible to a broader audience.

**Abstract:** This paper proposes a natural language translation method for
machine-verifiable formal proofs that leverages the informalization
(verbalization of formal language proof steps) and summarization capabilities
of LLMs. For evaluation, it was applied to formal proof data created in
accordance with natural language proofs taken from an undergraduate-level
textbook, and the quality of the generated natural language proofs was analyzed
in comparison with the original natural language proofs. Furthermore, we will
demonstrate that this method can output highly readable and accurate natural
language proofs by applying it to existing formal proof library of the Lean
proof assistant.

</details>


### [20] [A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs](https://arxiv.org/abs/2509.09727)
*Andy Zhu,Yingjun Du*

Main category: cs.CL

> 针对金融领域问答，本文提出了一种新的多智能体框架，通过基于角色的提示和检索增强生成技术，提升了金融问答系统的准确性和成本效益。

<details>
  <summary>Details</summary>

**Motivation:** 在金融教育领域，现有的大型语言模型在处理财务问题解决所需的复杂且专业化的推理时常常不足，该领域要求多步定量推理、熟悉领域特定术语以及现实场景的理解。因此，有必要提出一个更适合金融教育问答的方法。

**Method:** 我们提出了一种基于多智能体框架的方法来增强特定领域的问答能力。该框架包括一个基础生成器(Base Generator)、一个证据检索器(Evidence Retriever)和一个专家评审员(Expert Reviewer)，这些智能体通过单次迭代工作来生成一个优化的答案。这里利用了基于角色的提示(role-based prompting)策略，以及检索增强生成(RAG)来提供来自6本金融教材的上下文证据。

**Result:** 在Study.com提供的3,532个专家设计的金融教育问题的数据集上进行的实验表明，基于批评的优化(refinement)将答案的准确性提高了6.6-8.3%，相较于零样本链式思考基线（zero-shot Chain-of-Thought baselines）。特别是，Gemini-2.0-Flash表现最佳。此外，我们的方法使GPT-4o-mini能达到与经过金融微调的FinGPT-mt_Llama3-8B_LoRA相当的性能。

**Conclusion:** 该研究表明，我们的多智能体框架可以以成本效益的方式优化金融问答，并为未来的研究提供有关多智能体金融LLM系统的见解。

**Abstract:** Question answering (QA) plays a central role in financial education, yet
existing large language model (LLM) approaches often fail to capture the
nuanced and specialized reasoning required for financial problem-solving. The
financial domain demands multistep quantitative reasoning, familiarity with
domain-specific terminology, and comprehension of real-world scenarios. We
present a multi-agent framework that leverages role-based prompting to enhance
performance on domain-specific QA. Our framework comprises a Base Generator, an
Evidence Retriever, and an Expert Reviewer agent that work in a single-pass
iteration to produce a refined answer. We evaluated our framework on a set of
3,532 expert-designed finance education questions from Study.com, an online
learning platform. We leverage retrieval-augmented generation (RAG) for
contextual evidence from 6 finance textbooks and prompting strategies for a
domain-expert reviewer. Our experiments indicate that critique-based refinement
improves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines,
with the highest performance from Gemini-2.0-Flash. Furthermore, our method
enables GPT-4o-mini to achieve performance comparable to the finance-tuned
FinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to
enhancing financial QA and offer insights for further research in multi-agent
financial LLM systems.

</details>


### [21] [A meta-analysis on the performance of machine-learning based language models for sentiment analysis](https://arxiv.org/abs/2509.09728)
*Elena Rohde,Jonas Klingwort,Christian Borgs*

Main category: cs.CL

> 该论文通过对Twitter数据中的情感分析进行了元分析来评估机器学习性能。研究估计了平均性能，评估了研究间和研究内的异质性，分析了研究特征如何影响模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机是估计机器学习在Twitter情感分析中的平均性能，评估研究间的差异，并分析哪些研究特征对模型性能有影响。

**Method:** Structure

**Result:** {
  "tldr": "该论文通过对Twitter数据中的情感分析进行了元分析来评估机器学习性能。研究估计了平均性能，评估了研究间和研究内的异质性，分析了研究特征如何影响模型性能。", 
  "motivation": "研究的动机是估计机器学习在Twitter情感分析中的平均性能，评估研究间的差异，并分析哪些研究特征对模型性能有影响。", 
  "method": "使用PRISMA指南，搜索了学术数据库并从20项研究中选取了195项试验，这些研究包含12个研究特性。整体准确率是最常用的性能度量，使用双反正弦变换和三层次随机效应模型进行分析。", 
  "result": "最终，AIC优化模型的平均整体准确率为0.80 [0.76, 0.84]。论文提出了两个关键点：1)整体准确率常被误解因为它对类别不均衡和情感分类数量敏感，强调需要进行规范化；2)需要规范报告模型性能，包括报告独立测试集的混淆矩阵，这对于跨研究的ML分类器比较至关重要。", 
  "conclusion": "该研究强调了规范化报告的重要性以及整体准确率指标的局限性，指出为了进行可靠的研究间模型比较，必须采用标准的性能报告方法。"}


**Conclusion:** 该研究强调了规范化报告的重要性以及整体准确率指标的局限性，指出为了进行可靠的研究间模型比较，必须采用标准的性能报告方法。

**Abstract:** This paper presents a meta-analysis evaluating ML performance in sentiment
analysis for Twitter data. The study aims to estimate the average performance,
assess heterogeneity between and within studies, and analyze how study
characteristics influence model performance. Using PRISMA guidelines, we
searched academic databases and selected 195 trials from 20 studies with 12
study features. Overall accuracy, the most reported performance metric, was
analyzed using double arcsine transformation and a three-level random effects
model. The average overall accuracy of the AIC-optimized model was 0.80 [0.76,
0.84]. This paper provides two key insights: 1) Overall accuracy is widely used
but often misleading due to its sensitivity to class imbalance and the number
of sentiment classes, highlighting the need for normalization. 2) Standardized
reporting of model performance, including reporting confusion matrices for
independent test sets, is essential for reliable comparisons of ML classifiers
across studies, which seems far from common practice.

</details>


### [22] [MultimodalHugs: Enabling Sign Language Processing in Hugging Face](https://arxiv.org/abs/2509.09729)
*Gerard Sant,Zifan Jiang,Carlos Escolano,Amit Moryossef,Mathias Müller,Rico Sennrich,Sarah Ebling*

Main category: cs.CL

> 手语处理研究面临复杂定制代码的挑战，MultimodalHugs框架提供了解决方案，提高了实验的灵活性和复现性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的工具，如Hugging Face，对于手语处理实验不够灵活，导致手语处理研究在复现性和公平性上存在问题。此外，这项观点从对手语处理研究人员的调查中得到了证实。

**Method:** 介绍了一个名为MultimodalHugs的新框架，该框架基于Hugging Face构建，可以支持更多样化的数据模态和任务，同时继承了Hugging Face的优点。

**Result:** 通过定量实验展示了MultimodalHugs如何适应多样化的模态，如用于手语的姿势估计数据或用于文本字符的像素数据。

**Conclusion:** MultimodalHugs作为一个新框架，不仅专注于手语处理，其抽象层使其在其他不符合Hugging Face标准模板的情况下也能广泛适用。

**Abstract:** In recent years, sign language processing (SLP) has gained importance in the
general field of Natural Language Processing. However, compared to research on
spoken languages, SLP research is hindered by complex ad-hoc code,
inadvertently leading to low reproducibility and unfair comparisons. Existing
tools that are built for fast and reproducible experimentation, such as Hugging
Face, are not flexible enough to seamlessly integrate sign language
experiments. This view is confirmed by a survey we conducted among SLP
researchers.
  To address these challenges, we introduce MultimodalHugs, a framework built
on top of Hugging Face that enables more diverse data modalities and tasks,
while inheriting the well-known advantages of the Hugging Face ecosystem. Even
though sign languages are our primary focus, MultimodalHugs adds a layer of
abstraction that makes it more widely applicable to other use cases that do not
fit one of the standard templates of Hugging Face. We provide quantitative
experiments to illustrate how MultimodalHugs can accommodate diverse modalities
such as pose estimation data for sign languages, or pixel data for text
characters.

</details>


### [23] [Benchmarking Vision-Language Models on Chinese Ancient Documents: From OCR to Knowledge Reasoning](https://arxiv.org/abs/2509.09731)
*Haiyang Yu,Yuchuan Wu,Fan Shi,Lei Liao,Jinghui Lu,Xiaodong Ge,Han Wang,Minghan Zhuo,Xuecheng Wu,Xiang Fei,Hao Feng,Guozhi Tang,An-Lan Wang,Hanshen Zhu,Yangfan He,Quanhuan Liang,Liyuan Meng,Chao Feng,Can Huang,Jingqun Tang,Bin Li*

Main category: cs.CL

> 介绍AncientDoc，首个用于评估视觉语言模型在处理中文古籍OCR及知识推理方面性能的基准测试。

<details>
  <summary>Details</summary>

**Motivation:** 传统方法仅限于图像扫描，现有的视觉语言模型难以处理中文古籍的视觉和语言复杂性。现有文档基准主要侧重于英文印刷文本或简体中文，缺乏针对中文古籍评估视觉语言模型的基准。

**Method:** 建立AncientDoc基准测试以评估视觉语言模型在中文古籍上的表现，涵盖五项任务：页面级OCR、白话翻译、基于推理的QA、基于知识的QA和语言变体QA。

**Result:** AncientDoc基准测试包含了14种文档类型，超过100本书籍，大约3000页，用于评估主流视觉语言模型的表现。

**Conclusion:** 通过AncientDoc基准测试，使用多种度量标准评估主流视觉语言模型，并辅以与人类评分一致的大规模语言模型进行评分。

**Abstract:** Chinese ancient documents, invaluable carriers of millennia of Chinese
history and culture, hold rich knowledge across diverse fields but face
challenges in digitization and understanding, i.e., traditional methods only
scan images, while current Vision-Language Models (VLMs) struggle with their
visual and linguistic complexity. Existing document benchmarks focus on English
printed texts or simplified Chinese, leaving a gap for evaluating VLMs on
ancient Chinese documents. To address this, we present AncientDoc, the first
benchmark for Chinese ancient documents, designed to assess VLMs from OCR to
knowledge reasoning. AncientDoc includes five tasks (page-level OCR, vernacular
translation, reasoning-based QA, knowledge-based QA, linguistic variant QA) and
covers 14 document types, over 100 books, and about 3,000 pages. Based on
AncientDoc, we evaluate mainstream VLMs using multiple metrics, supplemented by
a human-aligned large language model for scoring.

</details>


### [24] [MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools](https://arxiv.org/abs/2509.09734)
*Zikang Guo,Benfeng Xu,Chiwei Zhu,Wentao Hong,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

> 提出MCP-AgentBench，以填补对基于MCP工作的代理进行准确评估的空白，改进现有工具评估标准。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基准测试无法捕捉到MCP环境下代理的真实性能，导致对其操作价值的误解，无法可靠地区分其能力。因此，需要一个专门的基准来填补这一评估空白。

**Method:** 通过建立MCP-AgentBench基准测试来评估语言代理在MCP中介工具交互中的能力，该基准包括33个操作服务器和188个不同工具的测试环境，以及包含600个系统设计查询的基准测试。

**Result:** 引入了MCP-Eval，这是一种专注于现实任务成功的新评估方法，并通过广泛的实证评估向领先的语言代理提供了初步见解。

**Conclusion:** MCP-AgentBench旨在为研究社区提供标准化和可靠的框架，以建设和验证能够充分利用MCP变革性优势的代理，加速真正有能力且高度互操作的AI系统的进展。

**Abstract:** The Model Context Protocol (MCP) is rapidly emerging as a pivotal open
standard, designed to enhance agent-tool integration and interoperability, and
is positioned to unlock a new era of powerful, interconnected, and genuinely
utilitarian agentic AI. However, despite MCP's growing adoption, existing
benchmarks often fail to capture real-world agent performance within this new
paradigm, leading to a distorted perception of their true operational value and
an inability to reliably differentiate proficiencies. To bridge this critical
evaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark
specifically engineered to rigorously assess language agent capabilities in
MCP-mediated tool interactions. Core contributions of MCP-AgentBench include:
the establishment of a robust MCP testbed comprising 33 operational servers
with 188 distinct tools; the development of a benchmark featuring 600
systematically designed queries distributed across 6 distinct categories of
varying interaction complexity; and the introduction of MCP-Eval, a novel
outcome-oriented evaluation methodology prioritizing real-world task success.
Through extensive empirical evaluation of leading language agents, we provide
foundational insights. MCP-AgentBench aims to equip the research community with
a standardized and reliable framework to build, validate, and advance agents
capable of fully leveraging MCP's transformative benefits, thereby accelerating
progress toward truly capable and interoperable AI systems.

</details>


### [25] [Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making and Summarisation](https://arxiv.org/abs/2509.09735)
*Willem Huijzer,Jieying Chen*

Main category: cs.CL

> 本研究揭示了在GPT-3.5和GPT-4o中性别和年龄的偏见，并测试了跨语言偏见传播和缓解策略，强调了在采用LLMs时需要进行谨慎考虑并制定缓解偏见的策略。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在了解语言模型在背景、性别和年龄方面的偏见，及其对决策和总结任务的影响，并评估跨语言偏见传播和提示策略缓解偏见的有效性。

**Method:** 本研究通过使用Tamkin等人的数据集的荷兰语翻译版本，生成了151,200个关于决策任务的唯一提示和176,400个关于摘要生成任务的唯一提示，并针对GPT-3.5和GPT-4o进行了测试，以研究语言模型中的偏见问题。

**Result:** 研究发现，两个模型在决策任务中有显著偏见，倾向于女性、年轻年龄以及如非洲裔美国人背景。然而，在摘要生成任务方面，除了年龄相关的差异外，基本没有偏见。通过跨语言分析，偏见模式在英语和荷兰语之间相似，但在特定的人口统计类别之间有所不同。

**Conclusion:** 研究强调了在不同背景下对LLMs采用谨慎态度，并进行特定偏见测试的重要性。同时表明了新模型中提示缓解的潜在有效性，需要继续开发有效的缓解策略，确保AI的负责任部署。

**Abstract:** The rapid integration of Large Language Models (LLMs) into various domains
raises concerns about societal inequalities and information bias. This study
examines biases in LLMs related to background, gender, and age, with a focus on
their impact on decision-making and summarization tasks. Additionally, the
research examines the cross-lingual propagation of these biases and evaluates
the effectiveness of prompt-instructed mitigation strategies. Using an adapted
version of the dataset by Tamkin et al. (2023) translated into Dutch, we
created 151,200 unique prompts for the decision task and 176,400 for the
summarisation task. Various demographic variables, instructions, salience
levels, and languages were tested on GPT-3.5 and GPT-4o. Our analysis revealed
that both models were significantly biased during decision-making, favouring
female gender, younger ages, and certain backgrounds such as the
African-American background. In contrast, the summarisation task showed minimal
evidence of bias, though significant age-related differences emerged for
GPT-3.5 in English. Cross-lingual analysis showed that bias patterns were
broadly similar between English and Dutch, though notable differences were
observed across specific demographic categories. The newly proposed mitigation
instructions, while unable to eliminate biases completely, demonstrated
potential in reducing them. The most effective instruction achieved a 27\% mean
reduction in the gap between the most and least favorable demographics.
Notably, contrary to GPT-3.5, GPT-4o displayed reduced biases for all prompts
in English, indicating the specific potential for prompt-based mitigation
within newer models. This research underscores the importance of cautious
adoption of LLMs and context-specific bias testing, highlighting the need for
continued development of effective mitigation strategies to ensure responsible
deployment of AI.

</details>


### [26] [HEFT: A Coarse-to-Fine Hierarchy for Enhancing the Efficiency and Accuracy of Language Model Reasoning](https://arxiv.org/abs/2509.09801)
*Brennen Hill*

Main category: cs.CL

> 本文提出了HEFT方法，通过组合权重空间和表示空间的微调技术，实现了在计算资源有限的情况下提升语言模型推理任务性能的目标。

<details>
  <summary>Details</summary>

**Motivation:** 这篇文章探讨了假设，即这些范式的协同组合可以解锁更优的性能和效率。这一研究的动机在于找到一种更有效率和效果更好的方法来提升大型语言模型在特定推理任务上的能力。

**Method:** HEFT (Hierarchical Efficient Fine-Tuning) 被提出，这是一种分层适配策略，将两种不同的参数高效微调方法以粗细结合的方式组合在一起：首先，在权重空间中利用低秩适配（LoRA）进行广泛的、基础性的适应，然后，利用表示微调（ReFT）对内部激活进行精确的、外科手术式的细化。

**Result:** 在对Llama-2-7B模型进行BoolQ基准测试时，该框架在仅经过三个纪元的微调后便达到了85.17%的准确性，超过了用LoRA-only或ReFT-only方法训练20个纪元的模型。

**Conclusion:** 本研究展示了一种通过HEFT框架对PEFT方法的精心组合来推进语言模型推理能力的有效算法创新，证明了在相对较少的计算资源下达到更优结果的可能性。

**Abstract:** The adaptation of large language models (LLMs) to specialized reasoning tasks
is fundamentally constrained by computational resources. Parameter-Efficient
Fine-Tuning (PEFT) methods have emerged as a powerful solution, yet the
landscape of these techniques is diverse, with distinct methods operating in
either the model's weight space or its representation space. This paper
investigates the hypothesis that a synergistic combination of these paradigms
can unlock superior performance and efficiency. We introduce HEFT (Hierarchical
Efficient Fine-Tuning), a novel hierarchical adaptation strategy that composes
two distinct PEFT methods in a coarse-to-fine manner: first, a broad,
foundational adaptation in the weight space using Low-Rank Adaptation (LoRA),
followed by a precise, surgical refinement of internal activations using
Representation Fine-Tuning (ReFT). We evaluate this approach by fine-tuning a
Llama-2-7B model on the BoolQ benchmark, a challenging dataset for inferential
reasoning. Our results reveal a profound synergistic effect. A model fine-tuned
for only three epochs with our HEFT strategy achieves an accuracy of 85.17\%,
exceeding the performance of models trained for 20 epochs with either LoRA-only
(85.05\%) or ReFT-only (83.36\%) methodologies. This work demonstrates that the
thoughtful composition of PEFT methods is a potent algorithmic innovation,
offering a more efficient and effective path toward advancing the reasoning
capabilities of language models. By achieving superior results with a fraction
of the computational budget, our findings present a principled approach to
overcoming the obstacles inherent in adapting large-scale models for complex
cognitive tasks.

</details>


### [27] [Pragmatic Frames Evoked by Gestures: A FrameNet Brasil Approach to Multimodality in Turn Organization](https://arxiv.org/abs/2509.09804)
*Helen de Andrade Abreu,Tiago Timponi Torrent,Ely Edison da Silva Matos*

Main category: cs.CL

> 提出了一种通过语言和互动手势之间的关联来建模多模态对话轮次组织的框架，并展示了如何利用语用框架的注释增强现有的多模态数据集，以加深对人类认知和语言的理解。

<details>
  <summary>Details</summary>

**Motivation:** 论文旨在填补一个空白，即针对对话轮次组织中参与者使用的具体策略，特别是手势，开发用于机器学习的编码数据集。

**Method:** 开发了一种注释方法论，以在现有的多模态数据集Frame2上增加语用层面的注释，使数据集不仅包括口语和视频中的语义框架，还包括了用于组织对话轮次的手势的注释。

**Result:** 该论文提出了一种通过分析语言与互动手势之间的关联来建模多模态对话轮次组织的框架。论文开发了一种注释方法以丰富现有的多模态数据集，涵盖了对话轮次组织的语用框架模型。通过注释巴西电视剧《Pedro Pelo Mundo》的片段，论文展示了参与者如何在面对面交谈中利用手势进行对话轮次的传递、获取和维持。研究结果表明，语用框架的概念化，包括心理空间、概念合成和概念隐喻，影响了这些手势的使用，并有助于更深入地理解人类认知和语言。

**Conclusion:** 研究结果证实了参与者在面对面交谈中如何使用手势作为一种传递、获取和维持对话轮次的工具，并揭示了一些之前未被记录的手势变体。进一步的数据表明，语用框架的注释有助于揭示人类认知和语言的更深层面。

**Abstract:** This paper proposes a framework for modeling multimodal conversational turn
organization via the proposition of correlations between language and
interactive gestures, based on analysis as to how pragmatic frames are
conceptualized and evoked by communicators. As a means to provide evidence for
the analysis, we developed an annotation methodology to enrich a multimodal
dataset (annotated for semantic frames) with pragmatic frames modeling
conversational turn organization. Although conversational turn organization has
been studied by researchers from diverse fields, the specific strategies,
especially gestures used by communicators, had not yet been encoded in a
dataset that can be used for machine learning. To fill this gap, we enriched
the Frame2 dataset with annotations of gestures used for turn organization. The
Frame2 dataset features 10 episodes from the Brazilian TV series Pedro Pelo
Mundo annotated for semantic frames evoked in both video and text. This dataset
allowed us to closely observe how communicators use interactive gestures
outside a laboratory, in settings, to our knowledge, not previously recorded in
related literature. Our results have confirmed that communicators involved in
face-to-face conversation make use of gestures as a tool for passing, taking
and keeping conversational turns, and also revealed variations of some gestures
that had not been documented before. We propose that the use of these gestures
arises from the conceptualization of pragmatic frames, involving mental spaces,
blending and conceptual metaphors. In addition, our data demonstrate that the
annotation of pragmatic frames contributes to a deeper understanding of human
cognition and language.

</details>


### [28] [Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization](https://arxiv.org/abs/2509.09852)
*Chuyuan Li,Austin Xu,Shafiq Joty,Giuseppe Carenini*

Main category: cs.CL

> 本文提出了一个基于主题引导的强化学习方法，通过Group Relative Policy Optimization (GRPO)框架中的主题奖励提高多文档摘要生成的性能，并通过实验证明了该方法的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 多文档摘要的关键挑战是有效地整合多个来源的信息，同时保持连贯性和主题相关性。尽管大型语言模型在单文档摘要生成中表现出色，但在多文档摘要生成中的表现仍有提升空间。本文旨在通过引入主题引导技术提高该领域的性能。

**Method:** 提出了一种基于主题引导的强化学习方法来改善多文档摘要中的内容选择。该方法首先展示通过显式地使用主题标签提示模型，可以提高生成摘要的信息量。然后在Group Relative Policy Optimization (GRPO)框架中提出了一个新颖的主题奖励机制，用于衡量生成摘要与源文档之间的主题一致性。

**Result:** 在Multi-News和Multi-XScience数据集上的实验结果展示了该方法的表现优于强大的基线模型，强调了在多文档摘要中利用主题线索的有效性。

**Conclusion:** 基于主题引导的强化学习方法通过增强内容选择能力，能更有效地生成多文档摘要。

**Abstract:** A key challenge in Multi-Document Summarization (MDS) is effectively
integrating information from multiple sources while maintaining coherence and
topical relevance. While Large Language Models have shown impressive results in
single-document summarization, their performance on MDS still leaves room for
improvement. In this paper, we propose a topic-guided reinforcement learning
approach to improve content selection in MDS. We first show that explicitly
prompting models with topic labels enhances the informativeness of the
generated summaries. Building on this insight, we propose a novel topic reward
within the Group Relative Policy Optimization (GRPO) framework to measure topic
alignment between the generated summary and source documents. Experimental
results on the Multi-News and Multi-XScience datasets demonstrate that our
method consistently outperforms strong baselines, highlighting the
effectiveness of leveraging topical cues in MDS.

</details>


### [29] [Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case](https://arxiv.org/abs/2509.09871)
*Bastián González-Bustamante,Nando Verelst,Carla Cisternas*

Main category: cs.CL

> 研究评估了多种大语言模型生成合成调查响应的可靠性，并发现尽管存在高异质性，这些模型能够可靠地模拟特定领域的公共意见。

<details>
  <summary>Details</summary>

**Motivation:** 探究大语言模型在模拟人类答案和行为方面，为调查研究开辟了新的途径，但也存在再现训练数据中社会刻板印象和偏见的风险。本研究旨在评估这些模型在调查响应生成上的可靠性及偏差。

**Method:** 通过使用来自智利公共意见概率抽样调查的真实人类响应数据，评估了LLM生成的合成调查响应的可靠性。具体来说，他们对128个提示-模型-问题三元组进行了基准测试，生成了189,696个合成配置文件，并对128个问题子样本对进行了元分析，以测试沿关键社会经济维度的偏差。评测涵盖了OpenAI的GPT系列和o系列推理模型，以及Llama和Qwen检查点。

**Result:** 研究表明，合成响应在信任项上表现出色（F1度量和准确度 > 0.90）。GPT-4o, GPT-4o-mini, 和 Llama 4 Maverick在此任务上的表现相近。45-59岁的受访者中，合成与人类响应的吻合度最高。总体而言，基于大语言模型的合成样本可以近似概率样本的响应，但也存在较大的项目层面异质性。

**Conclusion:** 尽管研究结果表明大语言模型生成的合成调查响应可以近似实际的人类响应，但要捕捉公共意见的全部细微差别仍极具挑战性，需要谨慎调校和额外的分布测试以确保算法准确性和减少错误。

**Abstract:** Large Language Models (LLMs) offer promising avenues for methodological and
applied innovations in survey research by using synthetic respondents to
emulate human answers and behaviour, potentially mitigating measurement and
representation errors. However, the extent to which LLMs recover aggregate item
distributions remains uncertain and downstream applications risk reproducing
social stereotypes and biases inherited from training data. We evaluate the
reliability of LLM-generated synthetic survey responses against ground-truth
human responses from a Chilean public opinion probabilistic survey.
Specifically, we benchmark 128 prompt-model-question triplets, generating
189,696 synthetic profiles, and pool performance metrics (i.e., accuracy,
precision, recall, and F1-score) in a meta-analysis across 128
question-subsample pairs to test for biases along key sociodemographic
dimensions. The evaluation spans OpenAI's GPT family and o-series reasoning
models, as well as Llama and Qwen checkpoints. Three results stand out. First,
synthetic responses achieve excellent performance on trust items (F1-score and
accuracy > 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform
comparably on this task. Third, synthetic-human alignment is highest among
respondents aged 45-59. Overall, LLM-based synthetic samples approximate
responses from a probabilistic sample, though with substantial item-level
heterogeneity. Capturing the full nuance of public opinion remains challenging
and requires careful calibration and additional distributional tests to ensure
algorithmic fidelity and reduce errors.

</details>


### [30] [Large Language Models Meet Legal Artificial Intelligence: A Survey](https://arxiv.org/abs/2509.09969)
*Zhitian Hou,Zihan Ye,Nanli Zeng,Tianyong Hao,Kun Zeng*

Main category: cs.CL

> 本文综述了16种法律LLM系列和47个基于LLM的法律任务框架，提供了15个基准和29个数据集来评估法律能力，旨在促进未来的研究。资源可从GitHub获取。

<details>
  <summary>Details</summary>

**Motivation:** 为了促进法律领域中基于大型语言模型的研究和应用，本文对16种法律LLM系列和47个基于LLM的法律任务框架进行了全面回顾，并整理了15个基准和29个数据集来评估不同的法律能力。

**Method:** 本文通过回顾分析16种法律大型语言模型系列和47个基于LLM的法律任务框架，以及收集15个基准和29个数据集来评估不同的法律能力。

**Result:** 本文为法律领域中的大型语言模型(LLM)方法提供了系统的介绍，并分析了面临的挑战，讨论了未来的研究方向。

**Conclusion:** 本文为法律领域中的大型语言模型(LLM)方法提供了系统的介绍，旨在鼓励未来的研究，并为初学者提供帮助。资源可在https://github.com/ZhitianHou/LLMs4LegalAI获得。

**Abstract:** Large Language Models (LLMs) have significantly advanced the development of
Legal Artificial Intelligence (Legal AI) in recent years, enhancing the
efficiency and accuracy of legal tasks. To advance research and applications of
LLM-based approaches in legal domain, this paper provides a comprehensive
review of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and
also gather 15 benchmarks and 29 datasets to evaluate different legal
capabilities. Additionally, we analyse the challenges and discuss future
directions for LLM-based approaches in the legal domain. We hope this paper
provides a systematic introduction for beginners and encourages future research
in this field. Resources are available at
https://github.com/ZhitianHou/LLMs4LegalAI.

</details>


### [31] [CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China](https://arxiv.org/abs/2509.09990)
*Guixian Xu,Zeli Su,Ziyin Zhang,Jianing Liu,XU Han,Ting Zhang,Yushuang Dong*

Main category: cs.CL

> 本文构建了CMHG数据集，目的在于填补少数民族语言特别是藏语、维吾尔语及蒙古语新闻标题生成任务语料库的匮乏，并提供一个高质量测试集作为未来研究的基准。

<details>
  <summary>Details</summary>

**Motivation:** 由于中国少数民族语言（如藏语、维吾尔语和传统蒙古语）书写系统独特，与国际标准不同，导致缺乏相关语料库，特此解决了这一问题。

**Method:** 本文提出了一个名为中国少数民族新闻标题生成（CMHG）的新数据集，该数据集包含10万条藏文和各5万条维吾尔文及蒙文字条目，专为新闻标题生成任务设计。同时，还提供了一个由本土人士标注的高质量测试集，作为未来研究的基准。

**Result:** 建立了包含藏语、维吾尔语及蒙古语的新闻标题生成数据集，并创建了有价值的测试基准。

**Conclusion:** 该数据集有望成为少数民族语言新闻标题生成研究的重要资源，并推动相关基准的发展。

**Abstract:** Minority languages in China, such as Tibetan, Uyghur, and Traditional
Mongolian, face significant challenges due to their unique writing systems,
which differ from international standards. This discrepancy has led to a severe
lack of relevant corpora, particularly for supervised tasks like headline
generation. To address this gap, we introduce a novel dataset, Chinese Minority
Headline Generation (CMHG), which includes 100,000 entries for Tibetan, and
50,000 entries each for Uyghur and Mongolian, specifically curated for headline
generation tasks. Additionally, we propose a high-quality test set annotated by
native speakers, designed to serve as a benchmark for future research in this
domain. We hope this dataset will become a valuable resource for advancing
headline generation in Chinese minority languages and contribute to the
development of related benchmarks.

</details>


### [32] [Unsupervised Hallucination Detection by Inspecting Reasoning Processes](https://arxiv.org/abs/2509.10004)
*Ponhvoan Srey,Xiaobao Wu,Anh Tuan Luu*

Main category: cs.CL

> IRIS提出了一种无监督的幻觉检测框架，通过利用LLMs的内部表示和响应不确定性作为软伪标签，提高检测的准确性和泛化能力。实验表明该方法优于现有方法，且计算成本低，适合实时应用。

<details>
  <summary>Details</summary>

**Motivation:** IRIS旨在解决现有的无监督幻觉内容检测方法依赖于与事实准确性无关的代理信号的问题，这些信号导致检测偏向于表面或非真相相关方面，从而限制了在不同数据集和场景下的通用性。通过利用与事实准确性相关的内部表示，IRIS能够克服这些限制。

**Method:** IRIS方法利用大型语言模型（LLMs）内部与事实准确性相关的表示来检测幻觉内容。该方法首先通过提示LLM仔细验证给定陈述的真实性，然后获取其经过上下文化处理的嵌入作为训练的有信息量的特征。同时，每个响应的不确定性被视为真实性的软伪标签。

**Result:** 实验结果表明，IRIS在现有的无监督方法中表现出色，保持了高准确性和鲁棒性。该方法不仅不需要人工标注且计算成本低，即使在少量数据的情况下也能实现良好性能，非常适合实时检测应用。

**Conclusion:** IRIS提出了无监督的现象检测框架，证明通过利用内部表示可以提高幻觉内容检测的准确性和鲁棒性。

**Abstract:** Unsupervised hallucination detection aims to identify hallucinated content
generated by large language models (LLMs) without relying on labeled data.
While unsupervised methods have gained popularity by eliminating
labor-intensive human annotations, they frequently rely on proxy signals
unrelated to factual correctness. This misalignment biases detection probes
toward superficial or non-truth-related aspects, limiting generalizability
across datasets and scenarios. To overcome these limitations, we propose IRIS,
an unsupervised hallucination detection framework, leveraging internal
representations intrinsic to factual correctness. IRIS prompts the LLM to
carefully verify the truthfulness of a given statement, and obtain its
contextualized embedding as informative features for training. Meanwhile, the
uncertainty of each response is considered a soft pseudolabel for truthfulness.
Experimental results demonstrate that IRIS consistently outperforms existing
unsupervised methods. Our approach is fully unsupervised, computationally low
cost, and works well even with few training data, making it suitable for
real-time detection.

</details>


### [33] [Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs](https://arxiv.org/abs/2509.10010)
*Adnan Ahmad,Philine Kowol,Stefan Hillmann,Sebastian Möller*

Main category: cs.CL

> 本篇论文分析了使用开源大型语言模型（LLM）进行多标签意图分类的效果，研究了三个热门的开源预训练LLM在MultiWOZ 2.1数据集上的性能差异，并比较了指令驱动微调和监督学习方法。结果显示，在少样本设定下，Mistral-7B-v0.1 在14个意图类别中的11个类别上表现最佳，但是基于BERT的监督分类器表现优于所有少样本生成式LLM。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探索基于开源且可在普通硬件上运行的大型语言模型进行多意图对话分析的可行性，特别是在对话系统领域，以便为任务导向型聊天机器人的自然语言理解提供改进方案。

**Method:** 研究中使用了MultiWOZ 2.1数据集，对LLama2-7B-hf、Mistral-7B-v0.1 和 Yi-6B这三种模型进行了少样本设置下的分类任务，同时将这些模型的性能与基于BertForSequenceClassification的小型变压器模型的监督学习方法进行了对比。

**Result:** 评估的指标包括精确度、召回率以及微平均、宏平均和加权F1分数，结果显示Mistral-7B-v0.1模型在11个意图类别上的加权平均F-Score为0.50，这是在少样本设定下最佳的表现。

**Conclusion:** 研究证明了开源LLM在处理复杂多意图对话中的检测能力，即使相比先进的监督分类器如基于BERT的模型，仍有改进的空间。这为部署在资源受限环境中的开源LLM提供了支持。

**Abstract:** In this paper, we provide an extensive analysis of multi-label intent
classification using Large Language Models (LLMs) that are open-source,
publicly available, and can be run in consumer hardware. We use the MultiWOZ
2.1 dataset, a benchmark in the dialogue system domain, to investigate the
efficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,
Mistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot
setup, giving 20 examples in the prompt with some instructions. Our approach
focuses on the differences in performance of these models across several
performance metrics by methodically assessing these models on multi-label
intent classification tasks. Additionally, we compare the performance of the
instruction-based fine-tuning approach with supervised learning using the
smaller transformer model BertForSequenceClassification as a baseline. To
evaluate the performance of the models, we use evaluation metrics like
accuracy, precision, and recall as well as micro, macro, and weighted F1 score.
We also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1
outperforms two other generative models on 11 intent classes out of 14 in terms
of F-Score, with a weighted average of 0.50. It also has relatively lower
Humming Loss and higher Jaccard Similarity, making it the winning model in the
few-shot setting. We find BERT based supervised classifier having superior
performance compared to the best performing few-shot generative LLM. The study
provides a framework for small open-source LLMs in detecting complex
multi-intent dialogues, enhancing the Natural Language Understanding aspect of
task-oriented chatbots.

</details>


### [34] [Linguistic trajectories of bipolar disorder on social media](https://arxiv.org/abs/2509.10035)
*Laurin Plank,Armin Zlomuzica*

Main category: cs.CL

> 研究通过分析社交媒体上的语言变化来监测双相情感障碍（BD）患者的情绪变化，并发现诊断前后语言上存在显著的情绪困扰、精神共病等特征，且这些变化具有一定的季节性周期性。

<details>
  <summary>Details</summary>

**Motivation:** 临床评估的局限性促使研究人员转向分析社交媒体语言，利用其高时间分辨率和纵向视角，以更好地理解及监测双相情感障碍。

**Method:** 提出了一种确定用户诊断时间的方法，并将其应用于BD确诊前后共24年内的语言轨迹研究。

**Result:** 确诊双相情感障碍的行为伴随着情绪困扰、精神共病等一系列语言特征变化，并在确诊后二十年内观察到了每12个月一次的情绪语言变化周期。

**Conclusion:** 研究证实社交媒体语言分析在监测双相情感障碍急性阶段和慢性阶段语言变化的重要性，为大规模监测精神健康提供了新的视角。

**Abstract:** Language provides valuable markers of affective disorders such as bipolar
disorder (BD), yet clinical assessments remain limited in scale. In response,
analyses of social media (SM) language have gained prominence due to their high
temporal resolution and longitudinal scope. Here, we introduce a method to
determine the timing of users' diagnoses and apply it to study language
trajectories from 3 years before to 21 years after BD diagnosis - contrasted
with uses reporting unipolar depression (UD) and non-affected users (HC). We
show that BD diagnosis is accompanied by pervasive linguistic alterations
reflecting mood disturbance, psychiatric comorbidity, substance abuse,
hospitalization, medical comorbidities, unusual thought content, and
disorganized thought. We further observe recurring mood-related language
changes across two decades after the diagnosis, with a pronounced 12-month
periodicity suggestive of seasonal mood episodes. Finally, trend-level evidence
suggests an increased periodicity in users estimated to be female. In sum, our
findings provide evidence for language alterations in the acute and chronic
phase of BD. This validates and extends recent efforts leveraging SM for
scalable monitoring of mental health.

</details>


### [35] [!MSA at BAREC Shared Task 2025: Ensembling Arabic Transformers for Readability Assessment](https://arxiv.org/abs/2509.10040)
*Mohamed Basem,Mohamed Younes,Seif Ahmed,Abdelrahman Moustafa*

Main category: cs.CL

> 该系统通过集成四个互补的变压器模型并使用多种技术来应对类别不平衡和数据稀缺问题，从而在六个追踪项目中获得第一，并且在句子和文档级别达到了 87.5% 和 87.4% 的二次加权卡帕得分。

<details>
  <summary>Details</summary>

**Motivation:** 该研究是为了应对在阿拉伯语可读性评估任务中常见的数据稀缺和类别不均衡的问题，通过集成多个经过细调的变压器模型来提高预测准确性和鲁棒性。

**Method:** 我们的方法是一个带有置信度加权的集合，它包括四个互补的变压器模型（AraBERTv2、AraELECTRA、MARBERT 和 CAMeLBERT），每个模型通过不同的损失函数进行微调，以捕捉多样化的可读性信号。为了应对严重的类别不平衡和数据稀缺问题，我们采用了加权训练、先进的预处理、使用最强模型重新标记 SAMER 语料库以及通过 Gemini 2.5 Flash 生成合成数据，增加了大约 10,000 个罕见程度的样本。此外，我们还设计了一个针对性的后处理步骤来矫正预测分布的偏差，从而带来 6.3% 的二次加权卡帕（QWK）增益。

**Result:** 这个系统的性能达到了 87.5% 的句子级别和 87.4% 的文档级别的 QWK 分数，证明了其在阿拉伯语可读性评估上的成功。

**Conclusion:** 该系统显示了在阿拉伯语可读性预测中，模型和损失函数多样性、基于信心的融合以及智能增强技术的强大作用，可以在句子和文档级别上实现高度准确的预测。

**Abstract:** We present MSAs winning system for the BAREC 2025 Shared Task on fine-grained
Arabic readability assessment, achieving first place in six of six tracks. Our
approach is a confidence-weighted ensemble of four complementary transformer
models (AraBERTv2, AraELECTRA, MARBERT, and CAMeLBERT) each fine-tuned with
distinct loss functions to capture diverse readability signals. To tackle
severe class imbalance and data scarcity, we applied weighted training,
advanced preprocessing, SAMER corpus relabeling with our strongest model, and
synthetic data generation via Gemini 2.5 Flash, adding about 10,000 rare-level
samples. A targeted post-processing step corrected prediction distribution
skew, delivering a 6.3 percent Quadratic Weighted Kappa (QWK) gain. Our system
reached 87.5 percent QWK at the sentence level and 87.4 percent at the document
level, demonstrating the power of model and loss diversity, confidence-informed
fusion, and intelligent augmentation for robust Arabic readability prediction.

</details>


### [36] [Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models](https://arxiv.org/abs/2509.10078)
*Dongmin Choi,Woojung Song,Jongwook Han,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

> 研究建议不应使用既有心理问卷来评估LLMs，并比较了既有心理问卷和具生态效度的问卷在衡量大语言模型（LLMs）人格特质时的结果。

<details>
  <summary>Details</summary>

**Motivation:** 由于在大语言模型评估中使用人类设计的心理问卷的生态效度受到质疑，本研究旨在探讨既有心理问卷和具生态效度的问卷之间的结果差异及其可能提供的见解。

**Method:** 本研究进行了一项全面的比较分析，比较了既有心理问卷和具生态效度的问卷在衡量大语言模型（LLMs）人格特质时的结果。

**Result:** 研究结果显示，既有问卷可能导致（1）生成与用户查询上下文中表现出的LLMs的心理特征明显不同的资料，（2）评分项目不足，不能提供稳定测量，（3）生成LLMs有稳定构建的误导性印象，以及（4）对于受到人设提示的LLMs，生成夸大的资料。

**Conclusion:** 总体而言，本研究指出应谨慎使用既有心理问卷来衡量大语言模型（LLMs）人格特质，强调了采用生态效度更高的方法的重要性。

**Abstract:** Researchers have applied established psychometric questionnaires (e.g., BFI,
PVQ) to measure the personality traits and values reflected in the responses of
Large Language Models (LLMs). However, concerns have been raised about applying
these human-designed questionnaires to LLMs. One such concern is their lack of
ecological validity--the extent to which survey questions adequately reflect
and resemble real-world contexts in which LLMs generate texts in response to
user queries. However, it remains unclear how established questionnaires and
ecologically valid questionnaires differ in their outcomes, and what insights
these differences may provide. In this paper, we conduct a comprehensive
comparative analysis of the two types of questionnaires. Our analysis reveals
that established questionnaires (1) yield substantially different profiles of
LLMs from ecologically valid ones, deviating from the psychological
characteristics expressed in the context of user queries, (2) suffer from
insufficient items for stable measurement, (3) create misleading impressions
that LLMs possess stable constructs, and (4) yield exaggerated profiles for
persona-prompted LLMs. Overall, our work cautions against the use of
established psychological questionnaires for LLMs. Our code will be released
upon publication.

</details>


### [37] [Querying Climate Knowledge: Semantic Retrieval for Scientific Discovery](https://arxiv.org/abs/2509.10087)
*Mustapha Adamu,Qi Zhang,Huitong Pan,Longin Jan Latecki,Eduard C. Dragut*

Main category: cs.CL

> 本文介绍了一个气候知识图谱，帮助研究人员改善获取气候知识的方式，展示了其实际应用价值。

<details>
  <summary>Details</summary>

**Motivation:** 随着气候科学文献复杂性与数据量的增加，研究人员越来越难以在模型、数据集、区域和变量之间找到相关的信息。

**Method:** 本文介绍了一个基于气候出版物和更广泛科学文本构建的领域特定知识图谱（KG），旨在改善气候知识的获取和使用方式。与基于关键词的搜索不同，该KG支持结构化语义查询，帮助研究人员发现精确的关联，如哪些模型已在特定区域进行了验证，或哪些数据集通常与某些遥相关模式一起使用。

**Result:** 该KG展示了通过Cypher查询回答这些问题的方式，并概述了其与RAG系统中大型语言模型的集成，以提高气候相关问答的透明度和可靠性。

**Conclusion:** 这项工作超越了构建知识图谱范围，展示了其对气候研究人员、模型开发者以及其他依赖准确且具有上下文的科学信息的人们的价值。

**Abstract:** The growing complexity and volume of climate science literature make it
increasingly difficult for researchers to find relevant information across
models, datasets, regions, and variables. This paper introduces a
domain-specific Knowledge Graph (KG) built from climate publications and
broader scientific texts, aimed at improving how climate knowledge is accessed
and used. Unlike keyword based search, our KG supports structured, semantic
queries that help researchers discover precise connections such as which models
have been validated in specific regions or which datasets are commonly used
with certain teleconnection patterns. We demonstrate how the KG answers such
questions using Cypher queries, and outline its integration with large language
models in RAG systems to improve transparency and reliability in
climate-related question answering. This work moves beyond KG construction to
show its real world value for climate researchers, model developers, and others
who rely on accurate, contextual scientific information.

</details>


### [38] [Arabic Large Language Models for Medical Text Generation](https://arxiv.org/abs/2509.10095)
*Abdulrahman Allam,Seif Ahmed,Ali Hamdi,Ammar Mohammed*

Main category: cs.CL

> 该研究展示了通过微调大语言模型来生成阿拉伯语医学文本，对于解决HMS中的挑战具有潜力，提供了一种可扩展和适应全球卫生保健挑战的解决方案，尤其是语言和文化多元化的环境。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于解决医院管理系统（HMS）面临的如过度拥挤、资源有限和紧急医疗服务不足的挑战，特别是缺乏提供准确实时医学建议的能力，尤其是在处理不规则输入和少数语言时。

**Method:** 该研究通过微调大语言模型（LLMs）来生成阿拉伯语医学文本，旨在为患者提供准确的医疗建议、诊断、药物推荐和治疗计划。研究方法包括收集社交媒体平台上的患者和医生之间的实际医疗对话数据，并进行预处理以适应多种阿拉伯方言。细调的模型包括Mistral-7B-Instruct-v0.2, LLaMA-2-7B, 和GPT-2 Medium。

**Result:** 评估结果显示，细调后的Mistral-7B模型表现优于其他模型，其BERT评分的精准度、召回率和F1得分分别为68.5%，69.08%和68.5%。基准测试和定性评估表明该系统能够根据非正式输入生成连贯和相关的医疗回复。

**Conclusion:** 这项研究表明，生成式人工智能在促进HMS方面具有潜力，为解决全球医疗服务中的挑战提供了可扩展和适应性强的解决方案，尤其是在语言和文化多元化的环境中。

**Abstract:** Efficient hospital management systems (HMS) are critical worldwide to address
challenges such as overcrowding, limited resources, and poor availability of
urgent health care. Existing methods often lack the ability to provide
accurate, real-time medical advice, particularly for irregular inputs and
underrepresented languages. To overcome these limitations, this study proposes
an approach that fine-tunes large language models (LLMs) for Arabic medical
text generation. The system is designed to assist patients by providing
accurate medical advice, diagnoses, drug recommendations, and treatment plans
based on user input. The research methodology required the collection of a
unique dataset from social media platforms, capturing real-world medical
conversations between patients and doctors. The dataset, which includes patient
complaints together with medical advice, was properly cleaned and preprocessed
to account for multiple Arabic dialects. Fine-tuning state-of-the-art
generative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2
Medium, optimized the system's ability to generate reliable medical text.
Results from evaluations indicate that the fine-tuned Mistral-7B model
outperformed the other models, achieving average BERT (Bidirectional Encoder
Representations from Transformers) Score values in precision, recall, and
F1-scores of 68.5\%, 69.08\%, and 68.5\%, respectively. Comparative
benchmarking and qualitative assessments validate the system's ability to
produce coherent and relevant medical replies to informal input. This study
highlights the potential of generative artificial intelligence (AI) in
advancing HMS, offering a scalable and adaptable solution for global healthcare
challenges, especially in linguistically and culturally diverse environments.

</details>


### [39] [Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing Generative AI with Synthetic Patient Records](https://arxiv.org/abs/2509.10108)
*Abdulrahman Allam,Seif Ahmed,Ali Hamdi,Khaled Shaban*

Main category: cs.CL

> Synthetic data augmentation using advanced AI systems improved the scalability and generalization of models for developing medical chatbots in Arabic.

<details>
  <summary>Details</summary>

**Motivation:** To address the scarcity of large-scale, high-quality annotated datasets for developing Arabic medical chatbots, resulting in limited scalability and generalization of models.

**Method:** We propose a scalable synthetic data augmentation strategy using advanced generative AI systems like ChatGPT-4o and Gemini 2.5 Pro to generate 80,000 synthetic question-answer pairs and integrate them into the training pipeline.

**Result:** The fine-tuned models showed improved performance using BERTScore metrics and qualitative assessments. An ablation study showed ChatGPT-4o data led to higher F1-scores and fewer hallucinations.

**Conclusion:** Synthetic data augmentation is a viable solution for enhancing domain-specific language models in low-resource medical NLP, leading to more inclusive, scalable, and accurate Arabic healthcare chatbot systems.

**Abstract:** The development of medical chatbots in Arabic is significantly constrained by
the scarcity of large-scale, high-quality annotated datasets. While prior
efforts compiled a dataset of 20,000 Arabic patient-doctor interactions from
social media to fine-tune large language models (LLMs), model scalability and
generalization remained limited. In this study, we propose a scalable synthetic
data augmentation strategy to expand the training corpus to 100,000 records.
Using advanced generative AI systems ChatGPT-4o and Gemini 2.5 Pro we generated
80,000 contextually relevant and medically coherent synthetic question-answer
pairs grounded in the structure of the original dataset. These synthetic
samples were semantically filtered, manually validated, and integrated into the
training pipeline. We fine-tuned five LLMs, including Mistral-7B and AraGPT2,
and evaluated their performance using BERTScore metrics and expert-driven
qualitative assessments. To further analyze the effectiveness of synthetic
sources, we conducted an ablation study comparing ChatGPT-4o and
Gemini-generated data independently. The results showed that ChatGPT-4o data
consistently led to higher F1-scores and fewer hallucinations across all
models. Overall, our findings demonstrate the viability of synthetic
augmentation as a practical solution for enhancing domain-specific language
models in-low resource medical NLP, paving the way for more inclusive,
scalable, and accurate Arabic healthcare chatbot systems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [40] [Australian Supermarket Object Set (ASOS): A Benchmark Dataset of Physical Objects and 3D Models for Robotics and Computer Vision](https://arxiv.org/abs/2509.09720)
*Akansel Cosgun,Lachlan Chumbley,Benjamin J. Meyer*

Main category: cs.CV

> Introduces ASOS, a comprehensive and accessible 3D dataset of common supermarket items for benchmarking robotics and computer vision tasks, acquired via structure-from-motion techniques. It comprises 50 items from 10 categories and focuses on real-world applicability.

<details>
  <summary>Details</summary>

**Motivation:** To provide a real-world and accessible dataset for benchmarking purposes, addressing the limitations of existing datasets that are either synthetically created or consist of specialized objects.

**Method:** The dataset, ASOS, was created using structure-from-motion techniques to generate high-quality 3D textured meshes of 50 common supermarket items, sourced from a major Australian supermarket chain.

**Result:** A comprehensive 3D textured mesh dataset consisting of 50 supermarket items from 10 categories, which are diverse in shape, size, and weight, and are designed for benchmarking in robotics and computer vision applications.

**Conclusion:** The ASOS dataset offers a valuable resource for researchers and developers focused on object detection, pose estimation, and robotics, by providing a cost-effective and accessible set of real-world objects with high-quality 3D representations.

**Abstract:** This paper introduces the Australian Supermarket Object Set (ASOS), a
comprehensive dataset comprising 50 readily available supermarket items with
high-quality 3D textured meshes designed for benchmarking in robotics and
computer vision applications. Unlike existing datasets that rely on synthetic
models or specialized objects with limited accessibility, ASOS provides a
cost-effective collection of common household items that can be sourced from a
major Australian supermarket chain. The dataset spans 10 distinct categories
with diverse shapes, sizes, and weights. 3D meshes are acquired by a
structure-from-motion techniques with high-resolution imaging to generate
watertight meshes. The dataset's emphasis on accessibility and real-world
applicability makes it valuable for benchmarking object detection, pose
estimation, and robotics applications.

</details>


### [41] [A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval](https://arxiv.org/abs/2509.09721)
*Jiayi Miao,Dingxin Lu,Zhuqi Wang*

Main category: cs.CV

> 本论文提出了一种多模态检索增强生成框架，能够提升受灾建筑的损坏评估精度，它结合了图像和文本信息，提高了信息检索和分类的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 在自然灾害发生后，对房屋损坏的准确评估对于保险理赔响应和资源规划来说至关重要。本研究为了改进损害评估的准确性和效率，提出了新的方法和技术。

**Method:** 本论文提出了一种新型的多模态检索增强生成框架（MM-RAG）。该框架在传统的RAG架构之上，设计了两分支的多模态编码结构，其中图像分支使用ResNet和Transformer组成的视觉编码器来提取受灾后建筑损坏的特征，文本分支则使用BERT检索器来将帖子及保险政策转换为文本向量，并构建一个可检索的恢复索引。为了实现跨模态语义对齐，模型整合了一个跨模态交互模块，通过多头注意力机制连接图像和文本的语义表示。在生成模块中，引入的模态注意力门控机制可以动态控制生成过程中视觉证据和文本先验信息的作用。整个框架采用端到端训练，结合对比损失、检索损失与生成损失形成多任务优化目标，实现了图像理解和政策匹配的协同学习。

**Result:** 实验结果表明，该框架在检索准确性和损坏严重程度分类指数上表现出色，Top-1检索准确率提升了9.6%。

**Conclusion:** 本论文中的方法在基于图像和文本数据的灾害损害评估上取得了很好的效果，显示出多模态融合方法在灾害评估中的潜力和优越性。

**Abstract:** After natural disasters, accurate evaluations of damage to housing are
important for insurance claims response and planning of resources. In this
work, we introduce a novel multimodal retrieval-augmented generation (MM-RAG)
framework. On top of classical RAG architecture, we further the framework to
devise a two-branch multimodal encoder structure that the image branch employs
a visual encoder composed of ResNet and Transformer to extract the
characteristic of building damage after disaster, and the text branch harnesses
a BERT retriever for the text vectorization of posts as well as insurance
policies and for the construction of a retrievable restoration index. To impose
cross-modal semantic alignment, the model integrates a cross-modal interaction
module to bridge the semantic representation between image and text via
multi-head attention. Meanwhile, in the generation module, the introduced modal
attention gating mechanism dynamically controls the role of visual evidence and
text prior information during generation. The entire framework takes end-to-end
training, and combines the comparison loss, the retrieval loss and the
generation loss to form multi-task optimization objectives, and achieves image
understanding and policy matching in collaborative learning. The results
demonstrate superior performance in retrieval accuracy and classification index
on damage severity, where the Top-1 retrieval accuracy has been improved by
9.6%.

</details>


### [42] [Improving MLLM Historical Record Extraction with Test-Time Image](https://arxiv.org/abs/2509.09722)
*Taylor Archibald,Tony Martinez*

Main category: cs.CV

> A novel ensemble framework stabilizes text extraction from noisy historical documents using multiple transcriptions and a custom aligner, improving accuracy by 4 percentage points.

<details>
  <summary>Details</summary>

**Motivation:** To stabilize text extraction from noisy historical documents using a novel ensemble framework.

**Method:** We transcribe multiple augmented variants of each image using Gemini 2.0 Flash and fuse these outputs with a custom Needleman Wunsch style aligner to yield a consensus transcription and a confidence score.

**Result:** The method improves transcription accuracy by 4 percentage points relative to a single shot baseline.

**Conclusion:** The approach is simple, scalable, and can be deployed to other document collections and transcription models.

**Abstract:** We present a novel ensemble framework that stabilizes LLM based text
extraction from noisy historical documents. We transcribe multiple augmented
variants of each image with Gemini 2.0 Flash and fuse these outputs with a
custom Needleman Wunsch style aligner that yields both a consensus
transcription and a confidence score. We present a new dataset of 622
Pennsylvania death records, and demonstrate our method improves transcription
accuracy by 4 percentage points relative to a single shot baseline. We find
that padding and blurring are the most useful for improving accuracy, while
grid warp perturbations are best for separating high and low confidence cases.
The approach is simple, scalable, and immediately deployable to other document
collections and transcription models.

</details>


### [43] [MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance](https://arxiv.org/abs/2509.09730)
*Kaikai Zhao,Zhaoxiang Liu,Peng Wang,Xin Wang,Zhicheng Ma,Yajun Xu,Wenjing Zhang,Yibing Nan,Kai Wang,Shiguo Lian*

Main category: cs.CV

> 本研究发布了一个为智能交通监控设计的大规模多模态数据集MITS，此数据集有助于改进现有模型在ITS任务中的表现。

<details>
  <summary>Details</summary>

**Motivation:** 为了应对现有的多模态模型在智能交通监控(ITS)领域的性能限制，提出了一种专门的数据集解决方案。

**Method:** 引入了MITS（多模态智能交通监控），这是首个专为ITS领域设计的大规模多模态基准数据集。MITS包含了170,400张从交通监控摄像头独立收集的真实ITS图像，并根据不同的环境条件，标注了8个主要类别的24个ITS特定对象和事件子类别。此外，通过一个系统化数据生成流程，生成了高质量的图像说明以及5百万个遵循指令的视觉问答对，专门针对五项关键的ITS任务：对象和事件识别、对象计数、对象定位、背景分析和事件推理。

**Result:** 实验结果显示MITS大大提高了各种主流LMM模型在ITS任务上的表现。例如，LLaVA-1.5的表现从0.494提升到0.905（+83.2%），LLaVA-1.6从0.678提升到0.921（+35.8%），Qwen2-VL从0.584提升到0.926（+58.6%），以及Qwen2.5-VL从0.732提升到0.930（+27.0%）。

**Conclusion:** 开放资源的发布为推动ITS和LMM研究提供了极具价值的工具。

**Abstract:** General-domain large multimodal models (LMMs) have achieved significant
advances in various image-text tasks. However, their performance in the
Intelligent Traffic Surveillance (ITS) domain remains limited due to the
absence of dedicated multimodal datasets. To address this gap, we introduce
MITS (Multimodal Intelligent Traffic Surveillance), the first large-scale
multimodal benchmark dataset specifically designed for ITS. MITS includes
170,400 independently collected real-world ITS images sourced from traffic
surveillance cameras, annotated with eight main categories and 24 subcategories
of ITS-specific objects and events under diverse environmental conditions.
Additionally, through a systematic data generation pipeline, we generate
high-quality image captions and 5 million instruction-following visual
question-answer pairs, addressing five critical ITS tasks: object and event
recognition, object counting, object localization, background analysis, and
event reasoning. To demonstrate MITS's effectiveness, we fine-tune mainstream
LMMs on this dataset, enabling the development of ITS-specific applications.
Experimental results show that MITS significantly improves LMM performance in
ITS applications, increasing LLaVA-1.5's performance from 0.494 to 0.905
(+83.2%), LLaVA-1.6's from 0.678 to 0.921 (+35.8%), Qwen2-VL's from 0.584 to
0.926 (+58.6%), and Qwen2.5-VL's from 0.732 to 0.930 (+27.0%). We release the
dataset, code, and models as open-source, providing high-value resources to
advance both ITS and LMM research.

</details>


### [44] [Decomposing Visual Classification: Assessing Tree-Based Reasoning in VLMs](https://arxiv.org/abs/2509.09732)
*Sary Elmansoury,Islam Mesabah,Gerrit Großmann,Peter Neigel,Raj Bhalwankar,Daniel Kondermann,Sebastian J. Vollmer*

Main category: cs.CV

> 尽管有结构化推理框架提升了模型的可解释性，但并未显著改进其在细粒度和粗粒度任务中的分类表现。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于调查有结构的、基于树的推理能否提高视觉语言模型在细粒度任务和大规模层级标签空间中的表现。

**Method:** 该研究采用框架将分类任务分解为使用决策树的可解释决策，并在细粒度（GTSRB）和粗粒度（CIFAR-10）数据集上进行评估。

**Result:** 尽管该模型在理解树知识上的准确率达到了98.2%，但其基于树的推理表现始终不如标准的零样本提示。同时发现，增加LLM生成的类别和图像描述能够提升基于树的和零样本方法的表现。

**Conclusion:** 研究结果揭示了结构化推理在视觉分类中的局限性，并为设计更具可解释性的视觉语言模型提供了见解。

**Abstract:** Vision language models (VLMs) excel at zero-shot visual classification, but
their performance on fine-grained tasks and large hierarchical label spaces is
understudied. This paper investigates whether structured, tree-based reasoning
can enhance VLM performance. We introduce a framework that decomposes
classification into interpretable decisions using decision trees and evaluates
it on fine-grained (GTSRB) and coarse-grained (CIFAR-10) datasets. Although the
model achieves 98.2% accuracy in understanding the tree knowledge, tree-based
reasoning consistently underperforms standard zero-shot prompting. We also
explore enhancing the tree prompts with LLM-generated classes and image
descriptions to improve alignment. The added description enhances the
performance of the tree-based and zero-shot methods. Our findings highlight
limitations of structured reasoning in visual classification and offer insights
for designing more interpretable VLM systems.

</details>


### [45] [World Modeling with Probabilistic Structure Integration](https://arxiv.org/abs/2509.09737)
*Klemen Kotar,Wanhee Lee,Rahul Venkatesh,Honglin Chen,Daniel Bear,Jared Watrous,Simon Kim,Khai Loong Aw,Lilian Naing Chen,Stefan Stojanov,Kevin Feigelis,Imran Thobani,Alex Durango,Khaled Jedoui,Atlas Kazemian,Dan Yamins*

Main category: cs.CV

> 本文介绍了Psi系统，能够通过独特的三步循环从大规模视频数据中学习世界模型，并在视频预测和理解方面取得了显著成效。

<details>
  <summary>Details</summary>

**Motivation:** 开发Psi系统，旨在从数据中学习丰富可控制和灵活提示的世界模型。

**Method:** Psi系统由三步循环构成：概率预测、结构提取与整合。结构提取通过因果推断从Psi中零样本提取低维特性，转化为新的token类型，每次循环都增强Psi的能力，提升其对数据的建模能力和创建新的控制手段。

**Result:** 使用Psi对1.4万亿个视频数据标记进行训练，能够执行多种有用的视频预测和理解推理，提取出最先进的光流、自我监督深度和物体分割，并用这些结构支持完整的预测改进周期。

**Conclusion:** Psi系统通过其独特的方法在视频理解和预测方面展现了卓越的能力，为建立丰富可控制的世界模型开辟了一条新途径。

**Abstract:** We present Probabilistic Structure Integration (PSI), a system for learning
richly controllable and flexibly promptable world models from data. PSI
consists of a three-step cycle. The first step, Probabilistic prediction,
involves building a probabilistic graphical model Psi of the data, in the form
of a random-access autoregressive sequence model. Psi supports a complete set
of learned conditional distributions describing the dependence of any variables
in the data on any other set of variables. In step 2, Structure extraction, we
show how to extract underlying low-dimensional properties in the data,
corresponding to a diverse set of meaningful "intermediate structures", in a
zero-shot fashion via causal inference on Psi. Step 3, Integration, completes
the cycle by converting these structures into new token types that are then
continually mixed back into the training diet as conditioning signals and
prediction targets. Each such cycle augments the capabilities of Psi, both
allowing it to model the underlying data better, and creating new control
handles -- akin to an LLM-like universal prompting language. We train an
instance of Psi on 1.4 trillion tokens of internet video data; we use it to
perform a variety of useful video prediction and understanding inferences; we
extract state-of-the-art optical flow, self-supervised depth and object
segmentation; and we use these structures to support a full cycle of predictive
improvements.

</details>


### [46] [Images in Motion?: A First Look into Video Leakage in Collaborative Deep Learning](https://arxiv.org/abs/2509.09742)
*Md Fazle Rasul,Alanood Alqobaisi,Bruhadeshwar Bezawada,Indrakshi Ray*

Main category: cs.CV

> 本篇论文分析了在联邦学习中视频数据泄漏的情况，特别是通过梯度逆向攻击技术。结果表明，与处理原始视频帧的方法相比，使用预训练特征提取器的方法对梯度逆向攻击更具抵抗力。然而，即使使用特征提取器，若分类器复杂度不足，泄露仍然是可能的。

<details>
  <summary>Details</summary>

**Motivation:** 视频数据在FL中的梯度逆向攻击效果尚未被广泛研究。此研究旨在填补这一空白，探索视频数据在FL中的隐私保护情况。

**Method:** 研究评估了两种常见的视频分类方法：一种使用预训练特征提取器，另一种处理经过简单变换的原始视频帧。通过这些方法来研究视频数据在梯度逆向攻击中的泄漏情况。

**Result:** 初步结果表明，使用预训练特征提取器的方法对梯度逆向攻击更具抵抗力。此外，还展示了如何使用图像超分辨率技术来增强通过梯度逆向攻击获取的帧，从而允许攻击者恢复出更高清晰度的视频。

**Conclusion:** 论文得出结论，FL中的视频数据泄漏是一个现实的威胁，且其发生的条件需要进一步的研究才能更全面地理解。

**Abstract:** Federated learning (FL) allows multiple entities to train a shared model
collaboratively. Its core, privacy-preserving principle is that participants
only exchange model updates, such as gradients, and never their raw, sensitive
data. This approach is fundamental for applications in domains where privacy
and confidentiality are important. However, the security of this very mechanism
is threatened by gradient inversion attacks, which can reverse-engineer private
training data directly from the shared gradients, defeating the purpose of FL.
While the impact of these attacks is known for image, text, and tabular data,
their effect on video data remains an unexamined area of research. This paper
presents the first analysis of video data leakage in FL using gradient
inversion attacks. We evaluate two common video classification approaches: one
employing pre-trained feature extractors and another that processes raw video
frames with simple transformations. Our initial results indicate that the use
of feature extractors offers greater resilience against gradient inversion
attacks. We also demonstrate that image super-resolution techniques can enhance
the frames extracted through gradient inversion attacks, enabling attackers to
reconstruct higher-quality videos. Our experiments validate this across
scenarios where the attacker has access to zero, one, or more reference frames
from the target environment. We find that although feature extractors make
attacks more challenging, leakage is still possible if the classifier lacks
sufficient complexity. We, therefore, conclude that video data leakage in FL is
a viable threat, and the conditions under which it occurs warrant further
investigation.

</details>


### [47] [A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO Networks for Object Detection in Densely Packed Retail Images](https://arxiv.org/abs/2509.09750)
*Hossein Yazdanjouei,Arash Mansouri,Mohammad Shokouhifar*

Main category: cs.CV

> 本文提出了一种半监督协同训练框架以解决密集零售环境下的物体检测问题，通过结合Faster R-CNN和YOLO，同时使用多种分类算法优化了物体检测的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在解决零售环境中物体检测的高难度问题，这些问题包括标记数据有限、环境复杂以及频繁的商品和布局变化。为了提高检测准确性同时降低成本，提出了本方法。

**Method:** 本文提出了一种半监督协同训练框架，用于解决密集零售环境中物体检测问题，这种方法结合了Faster R-CNN（使用ResNet作为骨干网络）进行精确定位和YOLO（使用Darknet作为骨干网络）进行全局环境感知，通过相互交换伪标签提高了遮挡和重叠物体场景下的准确性。此外，为了增强分类性能，利用了XGBoost、随机森林和SVM的集成方法，采用了多种特征表示方式提高鲁棒性。

**Result:** 实验结果显示，所提出的框架能有效改善遮挡和重叠物体检测问题的准确性，减少了手动标注的成本，并适应了零售环境中频繁的商品和布局变动。

**Conclusion:** 实验结果展示了所提出的框架在SKU-110k数据集上的出色性能，证明了其在实际零售应用（如自动化库存追踪、产品监控和结算系统）中的扩展性和实用性。

**Abstract:** This study proposes a semi-supervised co-training framework for object
detection in densely packed retail environments, where limited labeled data and
complex conditions pose major challenges. The framework combines Faster R-CNN
(utilizing a ResNet backbone) for precise localization with YOLO (employing a
Darknet backbone) for global context, enabling mutual pseudo-label exchange
that improves accuracy in scenes with occlusion and overlapping objects. To
strengthen classification, it employs an ensemble of XGBoost, Random Forest,
and SVM, utilizing diverse feature representations for higher robustness.
Hyperparameters are optimized using a metaheuristic-driven algorithm, enhancing
precision and efficiency across models. By minimizing reliance on manual
labeling, the approach reduces annotation costs and adapts effectively to
frequent product and layout changes common in retail. Experiments on the
SKU-110k dataset demonstrate strong performance, highlighting the scalability
and practicality of the proposed framework for real-world retail applications
such as automated inventory tracking, product monitoring, and checkout systems.

</details>


### [48] [Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging](https://arxiv.org/abs/2509.09785)
*Moslem Yazdanpanah,Ali Bahri,Mehrdad Noori,Sahar Dastani,Gustavo Adolfo Vargas Hakim,David Osowiechi,Ismail Ben Ayed,Christian Desrosiers*

Main category: cs.CV

> Token Purging (PG) is a novel backpropagation-free test-time adaptation approach for 3D point cloud classification, offering higher accuracy and efficiency over state-of-the-art methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the issue of performance degradation due to distribution shifts in 3D point cloud classification and to introduce a robust adaptation mechanism without iterative updates.

**Method:** Token Purging (PG) is introduced as a backpropagation-free approach for test-time adaptation (TTA) in 3D point cloud classification. It removes domain-shift-affected tokens before the attention layers. Two variants are proposed: PG-SP using source statistics and PG-SF, a fully source-free method using CLS-token-driven adaptation.

**Result:** Evaluations on ModelNet40-C, ShapeNet-C, and ScanObjectNN-C show that PG-SP provides an average of +10.3% higher accuracy than state-of-the-art backpropagation-free methods, while PG-SF sets new benchmarks for source-free adaptation.

**Conclusion:** The conclusion is that Token Purging (PG) is effective as a test-time adaptation technique, showing significant improvements in accuracy and being more efficient in terms of speed and memory usage compared to the baseline.

**Abstract:** Test-time adaptation (TTA) is crucial for mitigating performance degradation
caused by distribution shifts in 3D point cloud classification. In this work,
we introduce Token Purging (PG), a novel backpropagation-free approach that
removes tokens highly affected by domain shifts before they reach attention
layers. Unlike existing TTA methods, PG operates at the token level, ensuring
robust adaptation without iterative updates. We propose two variants: PG-SP,
which leverages source statistics, and PG-SF, a fully source-free version
relying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C,
ShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of
+10.3\% higher accuracy than state-of-the-art backpropagation-free methods,
while PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is
12.4 times faster and 5.5 times more memory efficient than our baseline, making
it suitable for real-world deployment. Code is available at
\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}

</details>


### [49] [Fine-Grained Cross-View Localization via Local Feature Matching and Monocular Depth Priors](https://arxiv.org/abs/2509.09792)
*Zimin Xia,Chenghao Xu,Alexandre Alahi*

Main category: cs.CV

> A new approach for accurate and interpretable fine-grained cross-view localization between ground-level and aerial images, overcoming limitations of previous methods through direct correspondence matching and scale-aware alignment.

<details>
  <summary>Details</summary>

**Motivation:** To improve the alignment quality with the aerial view by avoiding the information loss due to perspective distortion or compression of height information that occurs in the previous alignment methods.

**Method:** Our method involves direct correspondence matching between ground and aerial images, lifting matched keypoints to BEV space, and using scale-aware Procrustes alignment for estimating the camera pose, which supports both metric and relative depth.

**Result:** The method learns accurate local feature correspondences and achieves superior localization performance, especially under challenging conditions, such as cross-area generalization and unknown orientation.

**Conclusion:** The proposed method displays strong localization performance and is flexible as it is compatible with various relative depth models without per-model finetuning, making it suitable for real-world deployment.

**Abstract:** We propose an accurate and highly interpretable fine-grained cross-view
localization method that estimates the 3 Degrees of Freedom pose of a
ground-level image by matching its local features with a reference aerial
image. Previous methods typically transform the ground image into a bird's-eye
view (BEV) representation and then align it with the aerial image for
localization. However, this transformation often leads to information loss due
to perspective distortion or compression of height information, thereby
degrading alignment quality with the aerial view. In contrast, our method
directly establishes correspondences between ground and aerial images and lifts
only the matched keypoints to BEV space using monocular depth prior. Notably,
modern depth predictors can provide reliable metric depth when the test samples
are similar to the training data. When the depth distribution differs, they
still produce consistent relative depth, i.e., depth accurate up to an unknown
scale. Our method supports both metric and relative depth. It employs a
scale-aware Procrustes alignment to estimate the camera pose from the
correspondences and optionally recover the scale when using relative depth.
Experimental results demonstrate that, with only weak supervision on camera
pose, our method learns accurate local feature correspondences and achieves
superior localization performance under challenging conditions, such as
cross-area generalization and unknown orientation. Moreover, our method is
compatible with various relative depth models without requiring per-model
finetuning. This flexibility, combined with strong localization performance,
makes it well-suited for real-world deployment.

</details>


### [50] [Early Detection of Visual Impairments at Home Using a Smartphone Red-Eye Reflex Test](https://arxiv.org/abs/2509.09808)
*Judith Massmann,Alexander Lichtenstein,Francisco M. López*

Main category: cs.CV

> 本文介绍了一项研究，开发了名为KidsVisionCheck的应用，利用智能手机和深度学习技术进行儿童视力筛查，准确率达到90%，并在数据收集条件上提供了反馈。

<details>
  <summary>Details</summary>

**Motivation:** 随着智能手机和人工智能技术的发展，希望开发一种应用，使用移动设备通过红眼反射图像进行视觉筛查，从而普及儿科视力筛查。

**Method:** 本研究采用深度神经网络模型，该模型基于由眼科医生收集并标注的儿童瞳孔图像进行训练。

**Result:** 模型在未见过的测试数据上达到了90%的准确率，且无需专用设备。

**Conclusion:** 这项工作标志着向全球可及的儿科视力筛查和早期干预视力异常迈进了一步。

**Abstract:** Numerous visual impairments can be detected in red-eye reflex images from
young children. The so-called Bruckner test is traditionally performed by
ophthalmologists in clinical settings. Thanks to the recent technological
advances in smartphones and artificial intelligence, it is now possible to
recreate the Bruckner test using a mobile device. In this paper, we present a
first study conducted during the development of KidsVisionCheck, a free
application that can perform vision screening with a mobile device using
red-eye reflex images. The underlying model relies on deep neural networks
trained on children's pupil images collected and labeled by an ophthalmologist.
With an accuracy of 90% on unseen test data, our model provides highly reliable
performance without the necessity of specialist equipment. Furthermore, we can
identify the optimal conditions for data collection, which can in turn be used
to provide immediate feedback to the users. In summary, this work marks a first
step toward accessible pediatric vision screenings and early intervention for
vision abnormalities worldwide.

</details>


### [51] [DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception](https://arxiv.org/abs/2509.09828)
*Tim Broedermannn,Christos Sakaridis,Luigi Piccinelli,Wim Abbeloos,Luc Van Gool*

Main category: cs.CV

> DGFusion利用深度信息进行多模态融合，改善了自动驾驶环境感知能力，尤其在复杂的条件下表现优秀。

<details>
  <summary>Details</summary>

**Motivation:** 在挑战性条件下，传统传感器融合方法表现不佳。DGFusion提出通过利用深度信息调整传感器数据的融合策略，以提升感知性能。

**Method:** structure

**Result:** <tool_call>
{{"name": "Structure", "arguments": {"tldr": "提出了一种基于深度引导的多模态融合方法DGFusion，通过利用LiDAR测量和学习深度信息，提高了在复杂环境下的语义感知能力。实验表明，该方法在MUSES和DELIVER数据集上实现了最先进的全景和语义分割性能。", "motivation": "现有的传感器融合方法在空间上均匀处理传感器数据，遇到具有挑战性条件时表现不佳。为了解决这个问题，提出了DGFusion方法，它利用深度信息动态调整传感器融合策略。", "method": "DGFusion方法将多模态分割作为多任务问题处理，使用LiDAR测量作为输入和学习深度的地面事实。网络中的辅助深度头帮助学习深度感知特征，并将其编码为空间变化的本地深度令牌，以调节交叉模态融合。", "result": "DGFusion在具有挑战性的MUSES和DELIVER数据集上实现了最先进的全景和语义分割性能。", "conclusion": "总之，DGFusion方法在有效地结合来自具有互补优势的多个传感器的数据方面取得了成功，尤其是在复杂环境下。该方法有助于提高自动驾驶车辆的语义感知性能。"}}}
</tool_call>

**Conclusion:** DGFusion展示了其在复杂环境下的优势，巩固了其在语义感知技术中的地位。

**Abstract:** Robust semantic perception for autonomous vehicles relies on effectively
combining multiple sensors with complementary strengths and weaknesses.
State-of-the-art sensor fusion approaches to semantic perception often treat
sensor data uniformly across the spatial extent of the input, which hinders
performance when faced with challenging conditions. By contrast, we propose a
novel depth-guided multimodal fusion method that upgrades condition-aware
fusion by integrating depth information. Our network, DGFusion, poses
multimodal segmentation as a multi-task problem, utilizing the lidar
measurements, which are typically available in outdoor sensor suites, both as
one of the model's inputs and as ground truth for learning depth. Our
corresponding auxiliary depth head helps to learn depth-aware features, which
are encoded into spatially varying local depth tokens that condition our
attentive cross-modal fusion. Together with a global condition token, these
local depth tokens dynamically adapt sensor fusion to the spatially varying
reliability of each sensor across the scene, which largely depends on depth. In
addition, we propose a robust loss for our depth, which is essential for
learning from lidar inputs that are typically sparse and noisy in adverse
conditions. Our method achieves state-of-the-art panoptic and semantic
segmentation performance on the challenging MUSES and DELIVER datasets. Code
and models will be available at https://github.com/timbroed/DGFusion

</details>


### [52] [Patch-based Automatic Rosacea Detection Using the ResNet Deep Learning Framework](https://arxiv.org/abs/2509.09841)
*Chengyu Yang,Rishik Reddy Yesgari,Chengjun Liu*

Main category: cs.CV

> The paper introduces patch-based strategies using ResNet-18 for detecting rosacea, showing improved accuracy and privacy protection compared to full-image methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to improve the precision and early detection of rosacea, which in turn increases the effectiveness of its treatment by focusing on clinically relevant areas while also preserving patient privacy.

**Method:** This paper proposes a patch-based automatic rosacea detection method using the ResNet-18 deep learning framework. It extracts various facial image patches of different sizes, shapes, and locations for analysis.

**Result:** The patch-based strategies were found to have competitive or superior accuracy and sensitivity compared to full-image based methods, enhancing robustness and interpretability.

**Conclusion:** The proposed strategies are beneficial for improving automated dermatological diagnostics as they focus on relevant areas for detection, enhance robustness, and protect patient privacy.

**Abstract:** Rosacea, which is a chronic inflammatory skin condition that manifests with
facial redness, papules, and visible blood vessels, often requirs precise and
early detection for significantly improving treatment effectiveness. This paper
presents new patch-based automatic rosacea detection strategies using the
ResNet-18 deep learning framework. The contributions of the proposed strategies
come from the following aspects. First, various image pateches are extracted
from the facial images of people in different sizes, shapes, and locations.
Second, a number of investigation studies are carried out to evaluate how the
localized visual information influences the deep learing model performance.
Third, thorough experiments are implemented to reveal that several patch-based
automatic rosacea detection strategies achieve competitive or superior accuracy
and sensitivity than the full-image based methods. And finally, the proposed
patch-based strategies, which use only localized patches, inherently preserve
patient privacy by excluding any identifiable facial features from the data.
The experimental results indicate that the proposed patch-based strategies
guide the deep learning model to focus on clinically relevant regions, enhance
robustness and interpretability, and protect patient privacy. As a result, the
proposed strategies offer practical insights for improving automated
dermatological diagnostics.

</details>


### [53] [Privacy-Preserving Automated Rosacea Detection Based on Medically Inspired Region of Interest Selection](https://arxiv.org/abs/2509.09844)
*Chengyu Yang,Rishik Reddy Yesgari,Chengjun Liu*

Main category: cs.CV

> The paper presents a novel privacy-preserving automated rosacea detection method using deep learning, which achieves superior performance by leveraging redness-informed mask and synthetic data.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of automated rosacea detection, including the diffuse nature of symptoms, scarcity of labeled datasets, and privacy concerns.

**Method:** The method involves constructing a redness-informed mask to focus on diagnostically relevant areas and excluding identity-revealing features, followed by training a ResNet-18 model on synthetic masked facial images.

**Result:** The model exhibits notable improvements in accuracy, recall, and F1 score over full-face baseline models when tested on real-world data.

**Conclusion:** The results suggest that using synthetic data and clinical priors can lead to effective and ethical AI systems for diagnosing rosacea, with particular relevance to privacy-sensitive applications such as telemedicine.

**Abstract:** Rosacea is a common but underdiagnosed inflammatory skin condition that
primarily affects the central face and presents with subtle redness, pustules,
and visible blood vessels. Automated detection remains challenging due to the
diffuse nature of symptoms, the scarcity of labeled datasets, and privacy
concerns associated with using identifiable facial images. A novel
privacy-preserving automated rosacea detection method inspired by clinical
priors and trained entirely on synthetic data is presented in this paper.
Specifically, the proposed method, which leverages the observation that rosacea
manifests predominantly through central facial erythema, first constructs a
fixed redness-informed mask by selecting regions with consistently high red
channel intensity across facial images. The mask thus is able to focus on
diagnostically relevant areas such as the cheeks, nose, and forehead and
exclude identity-revealing features. Second, the ResNet-18 deep learning
method, which is trained on the masked synthetic images, achieves superior
performance over the full-face baselines with notable gains in terms of
accuracy, recall and F1 score when evaluated using the real-world test data.
The experimental results demonstrate that the synthetic data and clinical
priors can jointly enable accurate and ethical dermatological AI systems,
especially for privacy sensitive applications in telemedicine and large-scale
screening.

</details>


### [54] [Investigating the Impact of Various Loss Functions and Learnable Wiener Filter for Laparoscopic Image Desmoking](https://arxiv.org/abs/2509.09849)
*Chengyu Yang,Chengjun Liu*

Main category: cs.CV

> 本文通过详尽的消融研究来评估ULW框架中各个组件的有效性和必要性，该框架用于腹腔镜图像去烟雾处理。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于分析和确认ULW框架中的各个组件对整体性能的具体贡献，特别是当这些组件被单独移除或更改时。

**Method:** 采用U-Net作为主干网络和包含MSE、SSIM和感知损失的复合损失函数，框架中还包括一个可学习的维纳滤波器模块。研究中逐一移除这些组件，以评估其贡献。

**Result:** 通过使用SSIM、PSNR、MSE和CIEDE-2000等定量指标以及定性的视觉比较，所有变体在公共腹腔镜图像数据集上进行基准测试。

**Conclusion:** 这个消融研究旨在提供对ULW框架组件作用的深入理解，并可能指导未来框架的改进。

**Abstract:** To rigorously assess the effectiveness and necessity of individual components
within the recently proposed ULW framework for laparoscopic image desmoking,
this paper presents a comprehensive ablation study. The ULW approach combines a
U-Net based backbone with a compound loss function that comprises mean squared
error (MSE), structural similarity index (SSIM) loss, and perceptual loss. The
framework also incorporates a differentiable, learnable Wiener filter module.
In this study, each component is systematically ablated to evaluate its
specific contribution to the overall performance of the whole framework. The
analysis includes: (1) removal of the learnable Wiener filter, (2) selective
use of individual loss terms from the composite loss function. All variants are
benchmarked on a publicly available paired laparoscopic images dataset using
quantitative metrics (SSIM, PSNR, MSE and CIEDE-2000) alongside qualitative
visual comparisons.

</details>


### [55] [WAVE-DETR Multi-Modal Visible and Acoustic Real-Life Drone Detector](https://arxiv.org/abs/2509.09859)
*Razvan Stefanescu,Ethan Oh,Ruben Vazquez,Chris Mesterharm,Constantin Serban,Ritu Chadha*

Main category: cs.CV

> 这篇论文介绍了一种使用可见光RGB和声学信号来提高无人机检测精度的多模态方法。通过将声学信息与Deformable DETR架构结合，提升了无人机在实际条件下的检测性能。

<details>
  <summary>Details</summary>

**Motivation:** 为了提高无人机在复杂环境下的检测性能，我们结合了视觉和声学两种模态，利用现有的Drone-vs-Bird数据集和新生成的ARDrone数据集（超过7,500个同步图像和音频片段）进行研究。

**Method:** 我们提出了一种多模态的WAVE-DETR无人机检测器，结合可见光RGB和声学信号，在具有挑战性的环境下实现鲁棒的UAV目标检测。我们的方法在依赖于Deformable DETR和Wav2Vec2架构的统一目标检测模型中融合了视觉和声学特征。

**Result:** 通过四种不同的融合配置，基于门控机制、线性层、多层感知机（MLP）和交叉注意力机制，我们的方法成功地将Wav2Vec2声学嵌入与Deformable DETR的多分辨率特征映射融合，显著提高了目标检测性能。其中，门控融合方法在ARDrone数据集（无论是分布内还是分布外）上，将Deformable DETR目标检测器的小型无人机mAP提高了11.1%到15.3%。中型和大型无人机的mAP也得到了增强，总体而言，所有尺寸无人机的mAP增益范围为3.27%到5.84%。

**Conclusion:** 研究结果表明，通过融合视觉和声学信息，并采用特定的融合策略，可以显著提升基于Deformable DETR的无人机检测器性能。门控融合机制表现尤为突出，为无人机检测的准确性和鲁棒性提供了有效提升。

**Abstract:** We introduce a multi-modal WAVE-DETR drone detector combining visible RGB and
acoustic signals for robust real-life UAV object detection. Our approach fuses
visual and acoustic features in a unified object detector model relying on the
Deformable DETR and Wav2Vec2 architectures, achieving strong performance under
challenging environmental conditions. Our work leverage the existing
Drone-vs-Bird dataset and the newly generated ARDrone dataset containing more
than 7,500 synchronized images and audio segments. We show how the acoustic
information is used to improve the performance of the Deformable DETR object
detector on the real ARDrone dataset. We developed, trained and tested four
different fusion configurations based on a gated mechanism, linear layer, MLP
and cross attention. The Wav2Vec2 acoustic embeddings are fused with the multi
resolution feature mappings of the Deformable DETR and enhance the object
detection performance over all drones dimensions. The best performer is the
gated fusion approach, which improves the mAP of the Deformable DETR object
detector on our in-distribution and out-of-distribution ARDrone datasets by
11.1% to 15.3% for small drones across all IoU thresholds between 0.5 and 0.9.
The mAP scores for medium and large drones are also enhanced, with overall
gains across all drone sizes ranging from 3.27% to 5.84%.

</details>


### [56] [Surrogate Supervision for Robust and Generalizable Deformable Image Registration](https://arxiv.org/abs/2509.09869)
*Yihao Liu,Junyu Chen,Lianrui Zuo,Shuwen Wei,Brian D. Boyd,Carmen Andreescu,Olusola Ajilore,Warren D. Taylor,Aaron Carass,Bennett A. Landman*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Objective: Deep learning-based deformable image registration has achieved
strong accuracy, but remains sensitive to variations in input image
characteristics such as artifacts, field-of-view mismatch, or modality
difference. We aim to develop a general training paradigm that improves the
robustness and generalizability of registration networks. Methods: We introduce
surrogate supervision, which decouples the input domain from the supervision
domain by applying estimated spatial transformations to surrogate images. This
allows training on heterogeneous inputs while ensuring supervision is computed
in domains where similarity is well defined. We evaluate the framework through
three representative applications: artifact-robust brain MR registration,
mask-agnostic lung CT registration, and multi-modal MR registration. Results:
Across tasks, surrogate supervision demonstrated strong resilience to input
variations including inhomogeneity field, inconsistent field-of-view, and
modality differences, while maintaining high performance on well-curated data.
Conclusions: Surrogate supervision provides a principled framework for training
robust and generalizable deep learning-based registration models without
increasing complexity. Significance: Surrogate supervision offers a practical
pathway to more robust and generalizable medical image registration, enabling
broader applicability in diverse biomedical imaging scenarios.

</details>


### [57] [An Autoencoder and Vision Transformer-based Interpretability Analysis of the Differences in Automated Staging of Second and Third Molars](https://arxiv.org/abs/2509.09911)
*Barkin Buyukcakir,Jannick De Tobel,Patrick Thevissen,Dirk Vandermeulen,Peter Claes*

Main category: cs.CV

> The study proposes a framework combining a convolutional autoencoder with a Vision Transformer, improving dental age estimation accuracy and providing multi-faceted diagnostic insights, showing high intra-class morphological variability in tooth 38 is a limiting factor.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to overcome the 'black box' nature of deep learning models and enhance both performance and transparency in high-stakes applications such as dental age estimation.

**Method:** The framework uses a combination of a convolutional autoencoder (AE) and a Vision Transformer (ViT) to improve classification accuracy and provide diagnostic insights.

**Result:** The proposed framework improves classification accuracy for molars 37 and 38, and provides insights indicating that the remaining performance gap is due to data-centric issues.

**Conclusion:** The study concludes that relying on single modes of interpretability alone is insufficient and that the framework, which enhances model accuracy and provides clear reasons for its uncertainties, can better support expert decision-making in forensic age estimation.

**Abstract:** The practical adoption of deep learning in high-stakes forensic applications,
such as dental age estimation, is often limited by the 'black box' nature of
the models. This study introduces a framework designed to enhance both
performance and transparency in this context. We use a notable performance
disparity in the automated staging of mandibular second (tooth 37) and third
(tooth 38) molars as a case study. The proposed framework, which combines a
convolutional autoencoder (AE) with a Vision Transformer (ViT), improves
classification accuracy for both teeth over a baseline ViT, increasing from
0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38. Beyond
improving performance, the framework provides multi-faceted diagnostic
insights. Analysis of the AE's latent space metrics and image reconstructions
indicates that the remaining performance gap is data-centric, suggesting high
intra-class morphological variability in the tooth 38 dataset is a primary
limiting factor. This work highlights the insufficiency of relying on a single
mode of interpretability, such as attention maps, which can appear anatomically
plausible yet fail to identify underlying data issues. By offering a
methodology that both enhances accuracy and provides evidence for why a model
may be uncertain, this framework serves as a more robust tool to support expert
decision-making in forensic age estimation.

</details>


### [58] [SCoDA: Self-supervised Continual Domain Adaptation](https://arxiv.org/abs/2509.09935)
*Chirayu Agrawal,Snehasis Mukherjee*

Main category: cs.CV

> SCoDA, a new method for Source-Free Domain Adaptation, improves over existing methods by using self-supervised learning and incorporating geometric information about the latent manifold without source domain data.

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of standard SFDA methods which discard geometric information and require source domain data by introducing a novel approach using self-supervision and geometric manifold alignment.

**Method:** Self-supervised Continual Domain Adaptation (SCoDA) introduces two key approaches: initializing the framework with a self-supervised pre-trained teacher model, and using a composite objective that combines instance-level feature matching with Space Similarity Loss, while updating the teacher's parameters via an Exponential Moving Average (EMA) of the student's parameters.

**Result:** The proposed SCoDA method, tested extensively on benchmark datasets, achieves better performance than state-of-the-art SFDA methods.

**Conclusion:** Experiments show that SCoDA provides significant improvement over state-of-the-art SFDA methods by introducing a composite objective that considers both instance-level feature matching and Space Similarity Loss.

**Abstract:** Source-Free Domain Adaptation (SFDA) addresses the challenge of adapting a
model to a target domain without access to the data of the source domain.
Prevailing methods typically start with a source model pre-trained with full
supervision and distill the knowledge by aligning instance-level features.
However, these approaches, relying on cosine similarity over L2-normalized
feature vectors, inadvertently discard crucial geometric information about the
latent manifold of the source model. We introduce Self-supervised Continual
Domain Adaptation (SCoDA) to address these limitations. We make two key
departures from standard practice: first, we avoid the reliance on supervised
pre-training by initializing the proposed framework with a teacher model
pre-trained entirely via self-supervision (SSL). Second, we adapt the principle
of geometric manifold alignment to the SFDA setting. The student is trained
with a composite objective combining instance-level feature matching with a
Space Similarity Loss. To combat catastrophic forgetting, the teacher's
parameters are updated via an Exponential Moving Average (EMA) of the student's
parameters. Extensive experiments on benchmark datasets demonstrate that SCoDA
significantly outperforms state-of-the-art SFDA methods.

</details>


### [59] [Segment Anything for Cell Tracking](https://arxiv.org/abs/2509.09943)
*Zhu Chen,Mert Edgü,Er Jin,Johannes Stegmaier*

Main category: cs.CV

> 提出了一种无监督的零样本细胞追踪方法，通过集成SAM2，实现了在多种显微镜数据集中的有效追踪，同时不需要手动标记的数据集和微调。

<details>
  <summary>Details</summary>

**Motivation:** 由于细胞分割、低信噪比、边界不清晰、密集簇和细胞间视觉上相似的外观等挑战，追踪细胞和检测显微镜图像序列中的分裂事件仍然是一个困难的任务。本研究旨在克服现有的深度学习方法依赖于手动标记数据集训练以及在未见过的数据集中泛化能力有限的问题。

**Method:** 通过将设计用于通用图像和视频分割的大型基础模型Segment Anything 2 (SAM2)集成到跟踪流程中，提出了一种零样本细胞追踪框架。这种方法是完全无监督的，不依赖于特定的训练数据集，从而能够在多种显微镜数据集之间进行泛化，而无需微调。

**Result:** 该方法在2D和大规模3D时差显微视频中达到了具有竞争力的准确性，同时消除了对数据集特定适应的需要。

**Conclusion:** 该零样本细胞追踪方法克服了现有依赖于手动标记数据集训练的方法的成本高、耗时长和泛化能力有限的问题，实现了更高效的细胞追踪和分裂事件检测。

**Abstract:** Tracking cells and detecting mitotic events in time-lapse microscopy image
sequences is a crucial task in biomedical research. However, it remains highly
challenging due to dividing objects, low signal-tonoise ratios, indistinct
boundaries, dense clusters, and the visually similar appearance of individual
cells. Existing deep learning-based methods rely on manually labeled datasets
for training, which is both costly and time-consuming. Moreover, their
generalizability to unseen datasets remains limited due to the vast diversity
of microscopy data. To overcome these limitations, we propose a zero-shot cell
tracking framework by integrating Segment Anything 2 (SAM2), a large foundation
model designed for general image and video segmentation, into the tracking
pipeline. As a fully-unsupervised approach, our method does not depend on or
inherit biases from any specific training dataset, allowing it to generalize
across diverse microscopy datasets without finetuning. Our approach achieves
competitive accuracy in both 2D and large-scale 3D time-lapse microscopy videos
while eliminating the need for dataset-specific adaptation.

</details>


### [60] [Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation](https://arxiv.org/abs/2509.09946)
*Vu-Minh Le,Thao-Anh Tran,Duc Huy Do,Xuan Canh Do,Huong Ninh,Hai Tran*

Main category: cs.CV

> 本文提出了一种扩展在线2D多相机跟踪系统到3D空间的方法，该方法利用深度信息和点云空间，同时引入一种增强的数据关联机制，最终在比赛数据集中获得第三名。

<details>
  <summary>Details</summary>

**Motivation:** 目标是在不需要替换所有2D跟踪组件的情况下，将现有的多目标多相机跟踪系统扩展到3D空间。

**Method:** 提出了一种方法，通过利用深度信息将目标重建到点云空间中，并通过聚类和偏航角度调整来恢复3D边界框，以将任何在线2D多相机跟踪系统扩展到3D空间。同时还引入了一种增强的在线数据关联机制，利用目标的局部ID一致性来跨帧分配全局ID。

**Result:** 该框架在2025 AI City Challenge的3D MTMC数据集上进行了评估，取得了排行榜第三名的成绩。

**Conclusion:** 为了增强自动大规模监控能力，该方法提供了一种不需要根本性的更改便可扩展现有系统到3D空间的方法，并展示了其在实际数据集上的有效性。

**Abstract:** Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision
task for automating large-scale surveillance. With camera calibration and depth
information, the targets in the scene can be projected into 3D space, offering
unparalleled levels of automatic perception of a 3D environment. However,
tracking in the 3D space requires replacing all 2D tracking components from the
ground up, which may be infeasible for existing MTMC systems. In this paper, we
present an approach for extending any online 2D multi-camera tracking system
into 3D space by utilizing depth information to reconstruct a target in
point-cloud space, and recovering its 3D box through clustering and yaw
refinement following tracking. We also introduced an enhanced online data
association mechanism that leverages the target's local ID consistency to
assign global IDs across frames. The proposed framework is evaluated on the
2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the
leaderboard.

</details>


### [61] [Zero-Shot Referring Expression Comprehension via Visual-Language True/False Verification](https://arxiv.org/abs/2509.09958)
*Jeffrey Liu,Rongbin Hu*

Main category: cs.CV

> 本文提出了一种零样本工作流程，用于指代表达式理解任务，该方法将此任务重新定义为基于框的视觉语言验证，无需专门的微调，并在多个数据集上取得了优异的性能。

<details>
  <summary>Details</summary>

**Motivation:** 我们展示了不进行任何任务特异性训练的零样本工作流程可以达到具有竞争力甚至更优的性能。与传统的任务训练分词模型相对，我们试图探索一种无需专门训练的解决方案。

**Method:** 我们的方法将指代表达式理解（REC）重新定义为基于框的视觉语言验证：给定来自通用检测器（YOLO-World）的提议，一个通用的VLM独立回答每个区域的True/False查询。这种方法减少了框间干扰，支持放弃和多个匹配，并且不需要微调。

**Result:** 在RefCOCO、RefCOCO+和RefCOCOg数据集上，我们的方法不仅超过了零样本基准GroundingDINO的表现，而且还超过了那些针对REC任务进行了训练的GroundingDINO和GroundingDINO + CRG的方法。

**Conclusion:** 总体而言，我们展示了工作流程设计而不是任务特定的预训练是驱动强大的零样本指代表达式理解性能的关键。

**Abstract:** Referring Expression Comprehension (REC) is usually addressed with
task-trained grounding models. We show that a zero-shot workflow, without any
REC-specific training, can achieve competitive or superior performance. Our
approach reformulates REC as box-wise visual-language verification: given
proposals from a COCO-clean generic detector (YOLO-World), a general-purpose
VLM independently answers True/False queries for each region. This simple
procedure reduces cross-box interference, supports abstention and multiple
matches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our
method not only surpasses a zero-shot GroundingDINO baseline but also exceeds
reported results for GroundingDINO trained on REC and GroundingDINO+CRG.
Controlled studies with identical proposals confirm that verification
significantly outperforms selection-based prompting, and results hold with open
VLMs. Overall, we show that workflow design, rather than task-specific
pretraining, drives strong zero-shot REC performance.

</details>


### [62] [Augment to Segment: Tackling Pixel-Level Imbalance in Wheat Disease and Pest Segmentation](https://arxiv.org/abs/2509.09961)
*Tianqi Wei,Xin Yu,Zhi Chen,Scott Chapman,Zi Huang*

Main category: cs.CV

> 本文提出了一种名为随机投影复制粘贴（RPCP）的数据增强技术，以解决小麦叶片病害和虫害分割中由于像素不平衡导致的性能问题。实验表明，该方法显著提高了虫害分割的性能，同时保持或稍微提高了其他类别的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 作者提出了RPCP技术，旨在解决小麦叶片图像中虫害区域像素极为稀少导致的分割性能不佳的问题。

**Method:** RPCP技术通过从标注的训练图像中提取稀有虫害图像补丁，应用随机几何变换来模拟变化，并将其粘贴到适当区域以避免与现有的病斑或损坏区域重叠。此外还应用随机投影滤波器以使局部特征更加自然地与新背景融合。

**Result:** <tool_call>
data: {

**Conclusion:** 实验结果表明，该方法在提高虫害分割性能的同时，维持或略提高了其他类别的准确性，证明了该方法在缓解极端像素不平衡中的有效性。

**Abstract:** Accurate segmentation of foliar diseases and insect damage in wheat is
crucial for effective crop management and disease control. However, the insect
damage typically occupies only a tiny fraction of annotated pixels. This
extreme pixel-level imbalance poses a significant challenge to the segmentation
performance, which can result in overfitting to common classes and insufficient
learning of rare classes, thereby impairing overall performance. In this paper,
we propose a Random Projected Copy-and-Paste (RPCP) augmentation technique to
address the pixel imbalance problem. Specifically, we extract rare
insect-damage patches from annotated training images and apply random geometric
transformations to simulate variations. The transformed patches are then pasted
in appropriate regions while avoiding overlaps with lesions or existing damaged
regions. In addition, we apply a random projection filter to the pasted
regions, refining local features and ensuring a natural blend with the new
background. Experiments show that our method substantially improves
segmentation performance on the insect damage class, while maintaining or even
slightly enhancing accuracy on other categories. Our results highlight the
effectiveness of targeted augmentation in mitigating extreme pixel imbalance,
offering a straightforward yet effective solution for agricultural segmentation
problems.

</details>


### [63] [An HMM-based framework for identity-aware long-term multi-object tracking from sparse and uncertain identification: use case on long-term tracking in livestock](https://arxiv.org/abs/2509.09962)
*Anne Marthe Sophie Ngo Bibinbe,Chiron Bang,Patrick Gagnon,Jamie Ahloy-Dallaire,Eric R. Paquet*

Main category: cs.CV

> 研究提出了一种结合HMM的新型框架，用于长期多目标跟踪任务，这在畜牧业中有很大应用前景，并提高了现有模型的性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于身份切换，现有的多对象跟踪（MOT）方法表现不佳，特别是在长时间跟踪中。本研究旨在解决此种长期跟踪的挑战。

**Method:** 本研究提出了一种新的框架，结合了不确定的身份和跟踪，采用了隐马尔可夫模型（HMM）的制定方式。

**Result:** 该HMM框架提高了ByteTrack的F1分数，这在10分钟的猪跟踪数据集上的实验中得到了验证。此外，在MOT17和MOT20基准数据集上的测试进一步证实了改进的效果。

**Conclusion:** 研究表明，所提出的HMM框架在长时多目标跟踪任务中表现出色，尤其是当有更频繁的身份确认时，性能更好。

**Abstract:** The need for long-term multi-object tracking (MOT) is growing due to the
demand for analyzing individual behaviors in videos that span several minutes.
Unfortunately, due to identity switches between objects, the tracking
performance of existing MOT approaches decreases over time, making them
difficult to apply for long-term tracking. However, in many real-world
applications, such as in the livestock sector, it is possible to obtain
sporadic identifications for some of the animals from sources like feeders. To
address the challenges of long-term MOT, we propose a new framework that
combines both uncertain identities and tracking using a Hidden Markov Model
(HMM) formulation. In addition to providing real-world identities to animals,
our HMM framework improves the F1 score of ByteTrack, a leading MOT approach
even with re-identification, on a 10 minute pig tracking dataset with 21
identifications at the pen's feeding station. We also show that our approach is
robust to the uncertainty of identifications, with performance increasing as
identities are provided more frequently. The improved performance of our HMM
framework was also validated on the MOT17 and MOT20 benchmark datasets using
both ByteTrack and FairMOT. The code for this new HMM framework and the new
10-minute pig tracking video dataset are available at:
https://github.com/ngobibibnbe/uncertain-identity-aware-tracking

</details>


### [64] [Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey](https://arxiv.org/abs/2509.09971)
*Aupendu Kar,Vishnu Raj,Guan-Ming Su*

Main category: cs.CV

> This survey explores the advancements in fusing event-stream captures with traditional frame-based captures for enhancing image/video restoration and 3D reconstruction using deep learning techniques, and discusses the benefits under challenging conditions along with a list of open datasets for reproducible research.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to highlight the synergistic benefits of combining event camera data with traditional image and video data, focusing on improving the quality and performance of visual tasks through deep learning methods.

**Method:** The method involves a systematic review of the major contributions in deep learning for enhancing image/video qualities and 3D reconstruction tasks, categorizing the techniques into temporal and spatial enhancements.

**Result:** The result is an in-depth understanding of recent improvements in image/video enhancement and 3D reconstruction tasks enabled by the fusion of event-stream data and frame-based data, providing a clear pathway for leveraging event cameras in computer vision tasks.

**Conclusion:** The conclusion is that the combination of event-driven sensor data with traditional video/image data through deep learning holds significant potential for advancing visual media restoration and enhancement, and encourages further research in this area.

**Abstract:** Event camera sensors are bio-inspired sensors which asynchronously capture
per-pixel brightness changes and output a stream of events encoding the
polarity, location and time of these changes. These systems are witnessing
rapid advancements as an emerging field, driven by their low latency, reduced
power consumption, and ultra-high capture rates. This survey explores the
evolution of fusing event-stream captured with traditional frame-based capture,
highlighting how this synergy significantly benefits various video restoration
and 3D reconstruction tasks. The paper systematically reviews major deep
learning contributions to image/video enhancement and restoration, focusing on
two dimensions: temporal enhancement (such as frame interpolation and motion
deblurring) and spatial enhancement (including super-resolution, low-light and
HDR enhancement, and artifact reduction). This paper also explores how the 3D
reconstruction domain evolves with the advancement of event driven fusion.
Diverse topics are covered, with in-depth discussions on recent works for
improving visual quality under challenging conditions. Additionally, the survey
compiles a comprehensive list of openly available datasets, enabling
reproducible research and benchmarking. By consolidating recent progress and
insights, this survey aims to inspire further research into leveraging event
camera systems, especially in combination with deep learning, for advanced
visual media restoration and enhancement.

</details>


### [65] [ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking](https://arxiv.org/abs/2509.09977)
*Siying Liu,Zikai Wang,Hanle Zheng,Yifan Hu,Xilin Wang,Qingkai Yang,Jibin Wu,Hao Guo,Lei Deng*

Main category: cs.CV

> 提出了ISTASTrack，一种结合人工神经网络和尖峰神经网络的跟踪器，设计了ISTA适配器，以提高RGB-事件跟踪的性能和能效。

<details>
  <summary>Details</summary>

**Motivation:** 已有的人工神经网络难以充分利用事件流的稀疏性和非同步性，近期一些基于人工神经网络(ANN)和尖峰神经网络(SNN)的混合架构的研究虽有进展，但如何有效地融合异构范式的特征依旧是挑战。

**Method:** 提出了一种称为ISTASTrack的跟踪器，这是一种基于Transformer的ANN-SNN混合跟踪器，配备了ISTA适配器，用于RGB-事件跟踪。该模型的两个分支分别使用视觉Transformer从RGB输入中提取空间上下文，以及使用尖峰Transformer从事件流中捕获时空动态。为了弥合ANN和SNN特征之间的模态和范式差距，系统地设计了一种基于模型的ISTA适配器，用于两个分支之间的双向特征交互，适配器内部还集成了时间下采样注意力模块，改善了时间融合。

**Result:** 实验证明ISTASTrack在像FE240hz，VisEvent，COESOT 和FELT这样的RGB-事件跟踪基准数据集上，达到了最先进的性能，并且保持了高能效，突显了ANN-SNN混合设计的有效性和实用性。

**Conclusion:** 提出的ISTASTrack跟踪器通过结合Transformer和ISTA适配器，成功实现了RGB-事件跟踪中的模态和范式桥梁，证明了ANN-SNN混合架构在稳健视觉跟踪中的有效性和实用性。

**Abstract:** RGB-Event tracking has become a promising trend in visual object tracking to
leverage the complementary strengths of both RGB images and dynamic spike
events for improved performance. However, existing artificial neural networks
(ANNs) struggle to fully exploit the sparse and asynchronous nature of event
streams. Recent efforts toward hybrid architectures combining ANNs and spiking
neural networks (SNNs) have emerged as a promising solution in RGB-Event
perception, yet effectively fusing features across heterogeneous paradigms
remains a challenge. In this work, we propose ISTASTrack, the first
transformer-based \textbf{A}NN-\textbf{S}NN hybrid \textbf{Track}er equipped
with \textbf{ISTA} adapters for RGB-Event tracking. The two-branch model
employs a vision transformer to extract spatial context from RGB inputs and a
spiking transformer to capture spatio-temporal dynamics from event streams. To
bridge the modality and paradigm gap between ANN and SNN features, we
systematically design a model-based ISTA adapter for bidirectional feature
interaction between the two branches, derived from sparse representation theory
by unfolding the iterative shrinkage thresholding algorithm. Additionally, we
incorporate a temporal downsampling attention module within the adapter to
align multi-step SNN features with single-step ANN features in the latent
space, improving temporal fusion. Experimental results on RGB-Event tracking
benchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that
ISTASTrack achieves state-of-the-art performance while maintaining high energy
efficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN
designs for robust visual tracking. The code is publicly available at
https://github.com/lsying009/ISTASTrack.git.

</details>


### [66] [FLARE-SSM: Deep State Space Models with Influence-Balanced Loss for 72-Hour Solar Flare Prediction](https://arxiv.org/abs/2509.09988)
*Yusuke Takagi,Shunya Nagashima,Komei Sugiura*

Main category: cs.CV

> The study introduces an advanced solar flare prediction model using deep state space models and the FLARE loss, which shows significant improvement over baseline methods in predicting solar flare classes under conditions of class imbalance.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the prediction accuracy and reliability of solar flare forecasting, especially in the context of severe class imbalance across flare classes, which is critical for protecting infrastructure from potential impacts caused by solar flares.

**Method:** The paper proposes a solar flare prediction model based on multiple deep state space models along with an introduced method called the frequency & local-boundary-aware reliability loss (FLARE loss) to improve performance under conditions of class imbalance.

**Result:** Experiments conducted on a multi-wavelength solar image dataset showed that the new method outperformed baseline approaches in terms of the Gandin-Murphy-Gerrity score and the true skill statistic.

**Conclusion:** The proposed model demonstrates better performance and reliability in predicting the class of the largest solar flare expected within the next 72 hours, particularly in addressing the challenge of class imbalance across flare classes.

**Abstract:** Accurate and reliable solar flare predictions are essential to mitigate
potential impacts on critical infrastructure. However, the current performance
of solar flare forecasting is insufficient. In this study, we address the task
of predicting the class of the largest solar flare expected to occur within the
next 72 hours. Existing methods often fail to adequately address the severe
class imbalance across flare classes. To address this issue, we propose a solar
flare prediction model based on multiple deep state space models. In addition,
we introduce the frequency & local-boundary-aware reliability loss (FLARE loss)
to improve predictive performance and reliability under class imbalance.
Experiments were conducted on a multi-wavelength solar image dataset covering a
full 11-year solar activity cycle. As a result, our method outperformed
baseline approaches in terms of both the Gandin-Murphy-Gerrity score and the
true skill statistic, which are standard metrics in terms of the performance
and reliability.

</details>


### [67] [TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal Feature Extraction and Cross-Modal Feature Fusion](https://arxiv.org/abs/2509.10005)
*Xiaodong Guo,Tong Liu,Yike Li,Zi'ang Lin,Zhihong Deng*

Main category: cs.CV

> 本文提出了TUNI方法，通过一种新的RGB-T编码器和局部模块实现优越的RGB-T分割效果，不仅在性能上优于现有模型，还实现了更高的实时效率。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于改善现有RGB-T模型存在的问题，包括受限的热特征提取，次优的跨模态特征融合效果以及冗余编码器导致的实时效率不佳。

**Method:** 本文提出了一种名为TUNI的方法，使用RGB-T编码器，该编码器由多个堆叠的块组成，能够在多个模态进行特征提取和跨模态特征融合。并且引入了RGB-T局部模块，通过自适应余弦相似性来增强跨模态局部特征融合能力。编码器通过大规模预训练增强其融合特征的能力，同时通过精简热侧分支来获得更紧凑的架构。

**Result:** 实验结果显示，TUNI在FMB，PST900和CART数据集上取得了与现有最先进的模型相竞争的表现，且参数更少，计算成本更低。同时，在Jetson Orin NX设备上实现了27 FPS的推理速度，展示了其实际部署中的实时能力。

**Conclusion:** 本文提出的TUNI方法在优化RGB-T分割的效果和实时性方面表现出色，并验证了其在资源受限设备上的应用潜力。

**Abstract:** RGB-thermal (RGB-T) semantic segmentation improves the environmental
perception of autonomous platforms in challenging conditions. Prevailing models
employ encoders pre-trained on RGB images to extract features from both RGB and
infrared inputs, and design additional modules to achieve cross-modal feature
fusion. This results in limited thermal feature extraction and suboptimal
cross-modal fusion, while the redundant encoders further compromises the
model's real-time efficiency. To address the above issues, we propose TUNI,
with an RGB-T encoder consisting of multiple stacked blocks that simultaneously
perform multi-modal feature extraction and cross-modal fusion. By leveraging
large-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder
learns to integrate feature extraction and fusion in a unified manner. By
slimming down the thermal branch, the encoder achieves a more compact
architecture. Moreover, we introduce an RGB-T local module to strengthen the
encoder's capacity for cross-modal local feature fusion. The RGB-T local module
employs adaptive cosine similarity to selectively emphasize salient consistent
and distinct local features across RGB-T modalities. Experimental results show
that TUNI achieves competitive performance with state-of-the-art models on FMB,
PST900 and CART, with fewer parameters and lower computational cost. Meanwhile,
it achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its
real-time capability in deployment. Codes are available at
https://github.com/xiaodonguo/TUNI.

</details>


### [68] [Few-Part-Shot Font Generation](https://arxiv.org/abs/2509.10006)
*Masaki Akiba,Shumpei Takezaki,Daichi Haraguchi,Seiichi Uchida*

Main category: cs.CV

> 提出了一种基于部分设计元素生成完整字体的新型少部分样本字体生成模型，提高了字体创建效率并提供了关于部分设计细节如何影响整体字符结构的见解。

<details>
  <summary>Details</summary>

**Motivation:** 解决传统少样本字体生成需要完整字符形状的问题，提高字体设计的灵活性和效率。

**Method:** 基于部分设计元素设计整个字体，而不是传统的需要完整字符形状的方法。

**Result:** 实现了更高效的字体生成方法，并提供了部分设计细节影响整体字符结构的见解。

**Conclusion:** 提出了一个有效的模型来根据部分字体元素设计整个字体，相比于传统方法更为灵活高效。

**Abstract:** This paper proposes a novel model of few-part-shot font generation, which
designs an entire font based on a set of partial design elements, i.e., partial
shapes. Unlike conventional few-shot font generation, which requires entire
character shapes for a couple of character classes, our approach only needs
partial shapes as input. The proposed model not only improves the efficiency of
font creation but also provides insights into how partial design details
influence the entire structure of the individual characters.

</details>


### [69] [Efficient and Accurate Downfacing Visual Inertial Odometry](https://arxiv.org/abs/2509.10021)
*Jonas Kühne,Christian Vogt,Michele Magno,Luca Benini*

Main category: cs.CV

> 本文提出了一种适用于微型和纳米级无人机的高效且精确的视觉惯性里程计(VIO)流水线，基于RISC-V超低功耗并行SoC。通过刚体运动模型减少估计误差，并针对不同特征追踪方法进行了优化和实施。结果表明，优化后的流水线在GAP9超低功耗SoC上使用ORB特征追踪时，RMS误差平均降低至基线流水线的3.65倍。同时，PX4FLOW在低于24像素/帧的速度下与ORB的追踪精度相当，但运行时间更短。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在开发适用于微型和纳米级无人机的VIO流水线，解决传统高精度VIO通常运行在计算能力强大的系统上的问题，为微控制器提供轻量级的VIO实现。

**Method:** 文中提出了几种用于VIO流水线的特征检测和跟踪方法的最新状态（SuperPoint, PX4FLOW, ORB），所有这些方法均已经过优化和量化以适配新兴的RISC-V超低功耗SoC，并通过刚体运动模型减少运动估计误差。

**Result:** 实验评估显示，所提出的VIO流水线在低功耗SoC中具有良好的计算需求和追踪精度表现。通过量化实施，与用ORB特征追踪的基线流水线相比，该流水线在GAP9超低功耗SoC上RMS误差降低了3.65倍。PX4FLOW还可以在较低速度下的优秀追踪性能下保持实时运行。

**Conclusion:** 该研究开发了一种实验性VIO管道，适用于微小型无人机应用领域，在保持高准确性的同时具备较低能量需求，克服了高精度VIO通常依赖于强大的计算系统的问题，并通过量化和实施实现实时跟踪需求。

**Abstract:** Visual Inertial Odometry (VIO) is a widely used computer vision method that
determines an agent's movement through a camera and an IMU sensor. This paper
presents an efficient and accurate VIO pipeline optimized for applications on
micro- and nano-UAVs. The proposed design incorporates state-of-the-art feature
detection and tracking methods (SuperPoint, PX4FLOW, ORB), all optimized and
quantized for emerging RISC-V-based ultra-low-power parallel systems on chips
(SoCs). Furthermore, by employing a rigid body motion model, the pipeline
reduces estimation errors and achieves improved accuracy in planar motion
scenarios. The pipeline's suitability for real-time VIO is assessed on an
ultra-low-power SoC in terms of compute requirements and tracking accuracy
after quantization. The pipeline, including the three feature tracking methods,
was implemented on the SoC for real-world validation. This design bridges the
gap between high-accuracy VIO pipelines that are traditionally run on
computationally powerful systems and lightweight implementations suitable for
microcontrollers. The optimized pipeline on the GAP9 low-power SoC demonstrates
an average reduction in RMSE of up to a factor of 3.65x over the baseline
pipeline when using the ORB feature tracker. The analysis of the computational
complexity of the feature trackers further shows that PX4FLOW achieves on-par
tracking accuracy with ORB at a lower runtime for movement speeds below 24
pixels/frame.

</details>


### [70] [Hierarchical MLANet: Multi-level Attention for 3D Face Reconstruction From Single Images](https://arxiv.org/abs/2509.10024)
*Danling Cao*

Main category: cs.CV

> 提出了一种用于从野外单张2D图像中重建3D人脸模型的方法，通过广泛的实验验证其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 3D人脸模型从2D图像恢复在计算机视觉领域受到了广泛关注，但由于缺少带有真实标记的数据集和复杂的真实环境，仍然存在很大的挑战。

**Method:** 提出了一种基于卷积神经网络的方法，即分层多级注意力网络（MLANet），用于从野外单张2D图像中重建3D人脸模型。该模型从单张图像中预测详细的面部几何形状、纹理、姿态和照明参数。具体来说，采用预训练的分层主干网络，并引入多层次注意力机制，在2D人脸图像特征提取的不同阶段发挥作用。

**Result:** 进行了广泛的实验，包括比较实验和消融实验，在两个基准数据集AFLW2000-3D和MICC Florence上进行，专注于3D人脸重建和3D人脸对齐任务。

**Conclusion:** 定性定量评估结果表明提出的方法是有效的。

**Abstract:** Recovering 3D face models from 2D in-the-wild images has gained considerable
attention in the computer vision community due to its wide range of potential
applications. However, the lack of ground-truth labeled datasets and the
complexity of real-world environments remain significant challenges. In this
chapter, we propose a convolutional neural network-based approach, the
Hierarchical Multi-Level Attention Network (MLANet), for reconstructing 3D face
models from single in-the-wild images. Our model predicts detailed facial
geometry, texture, pose, and illumination parameters from a single image.
Specifically, we employ a pre-trained hierarchical backbone network and
introduce multi-level attention mechanisms at different stages of 2D face image
feature extraction. A semi-supervised training strategy is employed,
incorporating 3D Morphable Model (3DMM) parameters from publicly available
datasets along with a differentiable renderer, enabling an end-to-end training
process. Extensive experiments, including both comparative and ablation
studies, were conducted on two benchmark datasets, AFLW2000-3D and MICC
Florence, focusing on 3D face reconstruction and 3D face alignment tasks. The
effectiveness of the proposed method was evaluated both quantitatively and
qualitatively.

</details>


### [71] [LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA](https://arxiv.org/abs/2509.10026)
*Jing Huang,Zhiya Tan,Shutao Gong,Fanwei Zeng,Jianshu Li*

Main category: cs.CV

> LaV-CoT是第一个语言感知的视觉推理框架，通过多阶段推理流程和奖励优化方法，显著提升了多语言视觉问答任务的准确率和可解释性。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决现有方法过分依赖文本推理，对多语言多模态推理支持匮乏，不能充分应用于实际部署的问题。提出LaV-CoT框架，以支持更强大、更可解释的多语言多模态视觉推理。

**Method:** LaV-CoT框架运用可解释的多阶段推理流程，包括带边界框的文本总结、语言识别、空间对象描述和逐步逻辑推理。通过迭代生成、修正和精炼，设计了全自动的数据整理方法，生成多语言推理注释。LaV-CoT采用两个阶段的训练模式，结合了监督微调和基于语言感知组成员相对策略优化，通过多方面奖励进行指导，以提高推理和泛化能力。

**Result:** LaV-CoT在MMMB和Multilingual MMBench等公开数据集上取得最多9.5%的准确率提升，甚至超越规模大2倍的模型2.6%。此外，LaV-CoT在GPT-4o-0513和Gemini-2.5-flash等先进专有模型上也表现出更优性能。在线A/B测试进一步证实其在现实世界数据上的有效性。

**Conclusion:** 研究证实LaV-CoT框架在多语言多模态视觉问答任务中展现出优越的性能和可解释性，适用于工业部署，并促进了该领域的进一步发展。

**Abstract:** As large vision language models (VLMs) advance, their capabilities in
multilingual visual question answering (mVQA) have significantly improved.
Chain-of-thought (CoT) reasoning has been proven to enhance interpretability
and complex reasoning. However, most existing approaches rely primarily on
textual CoT and provide limited support for multilingual multimodal reasoning,
constraining their deployment in real-world applications. To address this gap,
we introduce \textbf{LaV-CoT}, the first Language-aware Visual CoT framework
with Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable
multi-stage reasoning pipeline consisting of Text Summary with Bounding Box
(BBox), Language Identification, Spatial Object-level Captioning, and
Step-by-step Logical Reasoning. Following this reasoning pipeline, we design an
automated data curation method that generates multilingual CoT annotations
through iterative generation, correction, and refinement, enabling scalable and
high-quality training data. To improve reasoning and generalization, LaV-CoT
adopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT)
with Language-aware Group Relative Policy Optimization (GRPO), guided by
verifiable multi-aspect rewards including language consistency, structural
accuracy, and semantic alignment. Extensive evaluations on public datasets
including MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up
to \(\sim\)9.5\% accuracy improvements over open-source baselines of similar
size and even surpasses models with 2$\times$ larger scales by \(\sim\)2.6\%.
Moreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513
and Gemini-2.5-flash. We further conducted an online A/B test to validate our
method on real-world data, highlighting its effectiveness for industrial
deployment. Our code is available at this link:
\href{https://github.com/HJNVR/LaV-CoT}

</details>


### [72] [Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation](https://arxiv.org/abs/2509.10058)
*Sung-Lin Tsai,Bo-Lun Huang,Yu Ting Shen,Cheng Yu Yeo,Chiang Tseng,Bo-Kai Ruan,Wen-Sheng Lien,Hong-Han Shuai*

Main category: cs.CV

> 提出无需训练的框架，使用LLM解决颜色描述模糊问题，提高T2I模型生成图像的颜色准确性。

<details>
  <summary>Details</summary>

**Motivation:** 为了精确渲染具有提示模糊性的颜色，提出了一种无需训练的框架，以解决现有模型在复杂颜色描述上的困难，这些模型往往产生与人类意图不符的图像。

**Method:** 通过利用大型语言模型（LLM）解析模糊的颜色描述，并在文本嵌入空间中直接指导颜色混合操作来提高颜色保真度。方法首先使用LLM解析文本提示中的模糊颜色术语，然后根据颜色术语在CIELAB颜色空间中的空间关系优化文本嵌入。

**Result:** 实验结果表明，该框架在不牺牲图像质量的情况下提高了颜色对齐效果，弥合了文本语义和视觉生成之间的差距。

**Conclusion:** 本研究提供了一种新的方法，能够在不进行额外训练或使用外部参考图像的情况下改善颜色准确性。

**Abstract:** Accurate color alignment in text-to-image (T2I) generation is critical for
applications such as fashion, product visualization, and interior design, yet
current diffusion models struggle with nuanced and compound color terms (e.g.,
Tiffany blue, lime green, hot pink), often producing images that are misaligned
with human intent. Existing approaches rely on cross-attention manipulation,
reference images, or fine-tuning but fail to systematically resolve ambiguous
color descriptions. To precisely render colors under prompt ambiguity, we
propose a training-free framework that enhances color fidelity by leveraging a
large language model (LLM) to disambiguate color-related prompts and guiding
color blending operations directly in the text embedding space. Our method
first employs a large language model (LLM) to resolve ambiguous color terms in
the text prompt, and then refines the text embeddings based on the spatial
relationships of the resulting color terms in the CIELAB color space. Unlike
prior methods, our approach improves color accuracy without requiring
additional training or external reference images. Experimental results
demonstrate that our framework improves color alignment without compromising
image quality, bridging the gap between text semantics and visual generation.

</details>


### [73] [Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration](https://arxiv.org/abs/2509.10059)
*Yue Zhou,Litong Feng,Mengcheng Lan,Xue Yang,Qingyun Li,Yiping Ke,Xue Jiang,Wayne Zhang*

Main category: cs.CV

> 研究提供了AVI-Math，一个评价无人飞行器视角下的多模态数学推理的基准数据集，通过评估多种视觉语言模型，识别其在需数学推理的任务中的不足，并探索可能的解决方法。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于现有的视觉语言模型在无人飞行器远程感测的数学推理领域难以被充分测试，该研究旨在填补这一空白，尤其是针对几何学、逻辑和代数等领域的推理能力。

**Method:** 介绍了AVI-Math，这是一个专门为评价基于无人飞行器的多模态数学推理而设计的基准测试。数据集包含来自无人飞行器视角捕捉的3,773个高质量车辆相关问题，覆盖了6个数学主题和20个具体话题。

**Result:** 通过对14个主要的VLM模型进行评估，发现尽管这些模型在之前的多模态基准测试中表现良好，但在AVI-Math中的数学推理任务中却面临困难。

**Conclusion:** 研究揭示了现有的视觉语言模型在数学推理能力上的限制，并提出通过细调技术以及链式思考提示法来解决问题的潜力，为提高无人飞行器应用场景中可信赖的VLM模型提供了宝贵的见解。

**Abstract:** Mathematical reasoning is critical for tasks such as precise distance and
area computations, trajectory estimations, and spatial analysis in unmanned
aerial vehicle (UAV) based remote sensing, yet current vision-language models
(VLMs) have not been adequately tested in this domain. To address this gap, we
introduce AVI-Math, the first benchmark to rigorously evaluate multimodal
mathematical reasoning in aerial vehicle imagery, moving beyond simple counting
tasks to include domain-specific knowledge in areas such as geometry, logic,
and algebra. The dataset comprises 3,773 high-quality vehicle-related questions
captured from UAV views, covering 6 mathematical subjects and 20 topics. The
data, collected at varying altitudes and from multiple UAV angles, reflects
real-world UAV scenarios, ensuring the diversity and complexity of the
constructed mathematical problems. In this paper, we benchmark 14 prominent
VLMs through a comprehensive evaluation and demonstrate that, despite their
success on previous multimodal benchmarks, these models struggle with the
reasoning tasks in AVI-Math. Our detailed analysis highlights significant
limitations in the mathematical reasoning capabilities of current VLMs and
suggests avenues for future research. Furthermore, we explore the use of
Chain-of-Thought prompting and fine-tuning techniques, which show promise in
addressing the reasoning challenges in AVI-Math. Our findings not only expose
the limitations of VLMs in mathematical reasoning but also offer valuable
insights for advancing UAV-based trustworthy VLMs in real-world applications.
The code, and datasets will be released at
https://github.com/VisionXLab/avi-math

</details>


### [74] [BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals](https://arxiv.org/abs/2509.10080)
*Minsang Kong,Myeongjun Kim,Sang Gu Kang,Sang Hun Lee*

Main category: cs.CV

> 提出了BEVTraj，一种仅依赖实时传感器数据进行轨迹预测的新框架，无需依赖预建地图。

<details>
  <summary>Details</summary>

**Motivation:** 现有的轨迹预测方法依赖于预建的高精度地图或实时局部地图构建模块，这些方法存在地区限制、无法适应瞬时变化以及可能引入预测错误的缺点。

**Method:** BEVTraj 框架基于鸟瞰视图空间进行轨迹预测，使用可变形注意力机制从密集的BEV特征中高效提取相关上下文，并引入稀疏目标候选提案模块来实现端到端的预测。

**Result:** 实验表明，BEVTraj 达到了与基于高精度地图的最先进模型相当的性能，并提供了更大的灵活性，因为它没有依赖于预建地图。

**Conclusion:** BEVTraj 提供了一种新型的在自动驾驶场景中替代预建地图的轨迹预测解决方案，体现了高度的实时性和灵活性。

**Abstract:** In autonomous driving, trajectory prediction is essential for ensuring safe
and efficient navigation. To improve prediction accuracy, recent approaches
often rely on pre-built high-definition (HD) maps or real-time local map
construction modules to incorporate static environmental information. However,
pre-built HD maps are limited to specific regions and cannot adapt to transient
changes. In addition, local map construction modules, which recognize only
predefined elements, may fail to capture critical scene details or introduce
errors that degrade prediction performance. To overcome these limitations, we
propose Bird's-Eye View Trajectory Prediction (BEVTraj), a novel trajectory
prediction framework that operates directly in the bird's-eye view (BEV) space
utilizing real-time sensor data without relying on any pre-built maps. The
BEVTraj leverages deformable attention to efficiently extract relevant context
from dense BEV features. Furthermore, we introduce a Sparse Goal Candidate
Proposal (SGCP) module, which enables full end-to-end prediction without
requiring any post-processing steps. Extensive experiments demonstrate that the
BEVTraj achieves performance comparable to state-of-the-art HD map-based models
while offering greater flexibility by eliminating the dependency on pre-built
maps. The source code is available at https://github.com/Kongminsang/bevtraj.

</details>
