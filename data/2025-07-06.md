<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 13]
- [cs.CV](#cs.CV) [Total: 16]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
*Tian Lan,Xiangdong Su,Xu Liu,Ruirui Wang,Ke Chang,Jiang Li,Guanglai Gao*

Main category: cs.CL

> 创建了多任务中文偏见评估基准（McBE），用于评估语言模型中的偏见，并揭示不同模型的偏见程度。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于现有偏见评估数据集主要基于英文和北美文化，且不适用于其他文化，本研究旨在弥补中文语言和文化背景下的偏见评估数据集的不足。

**Method:** 构建了多任务中文偏见评估基准（McBE），包含4,077个偏见评估实例，涵盖12类单偏见、82个子类和5个评估任务，提供广泛的类别覆盖、内容多样性和评估全面性。

**Result:** 对多个系列和参数规模的流行语言模型进行评估，所有模型表现出不同程度的偏见。

**Conclusion:** 提供了对语言模型偏见的深入分析，为未来研究提供了新的见解。

**Abstract:** As large language models (LLMs) are increasingly applied to various NLP
tasks, their inherent biases are gradually disclosed. Therefore, measuring
biases in LLMs is crucial to mitigate its ethical risks. However, most existing
bias evaluation datasets focus on English and North American culture, and their
bias categories are not fully applicable to other cultures. The datasets
grounded in the Chinese language and culture are scarce. More importantly,
these datasets usually only support single evaluation tasks and cannot evaluate
the bias from multiple aspects in LLMs. To address these issues, we present a
Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias
evaluation instances, covering 12 single bias categories, 82 subcategories and
introducing 5 evaluation tasks, providing extensive category coverage, content
diversity, and measuring comprehensiveness. Additionally, we evaluate several
popular LLMs from different series and with parameter sizes. In general, all
these LLMs demonstrated varying degrees of bias. We conduct an in-depth
analysis of results, offering novel insights into bias in LLMs.

</details>


### [2] [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)
*Keyan Jin,Yapeng Wang,Leonel Santos,Tao Fang,Xu Yang,Sio Kei Im,Hugo Gonçalo Oliveira*

Main category: cs.CL

> 研究发现，尽管大型语言模型在总结任务上取得显著进展，但在对话总结中，基于逐步推理的模型并不一定优于非推理模型，反而可能产生冗长和不一致的总结。

<details>
  <summary>Details</summary>

**Motivation:** 探索逐步推理模型在对话场景中的总结能力，特别是在要求简洁和抽象能力的情况下。

**Method:** 首次对尖端推理大型语言模型和非推理模型进行了全面系统的评估，涵盖了通用、角色导向和查询导向的对话总结，使用了多种语言、领域和总结长度的强基准。

**Result:** 研究发现，尽管其他推理密集型任务中逐步推理模型表现良好，但在对话总结中它们并不一定更好，反而可能产生冗长和不一致的总结。

**Conclusion:** 研究揭示了当前推理语言模型在对话总结中的局限性，并强调需要针对实际对话总结开发特定的建模和评估策略。

**Abstract:** Dialogue summarization is a challenging task with significant practical value
in customer service, meeting analysis, and conversational AI. Although large
language models (LLMs) have achieved substantial progress in summarization
tasks, the performance of step-by-step reasoning architectures-specifically
Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and
DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent
abstraction and conciseness. In this work, we present the first comprehensive
and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning
LLMs across three major paradigms-generic, role-oriented, and query-oriented
dialogue summarization. Our study spans diverse languages, domains, and summary
lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and
advanced evaluation protocols that include both LLM-based automatic metrics and
human-inspired criteria. Contrary to trends in other reasoning-intensive tasks,
our findings show that explicit stepwise reasoning does not consistently
improve dialogue summarization quality. Instead, reasoning LLMs are often prone
to verbosity, factual inconsistencies, and less concise summaries compared to
their non-reasoning counterparts. Through scenario-specific analyses and
detailed case studies, we further identify when and why explicit reasoning may
fail to benefit-or even hinder-summarization in complex dialogue contexts. Our
work provides new insights into the limitations of current reasoning LLMs and
highlights the need for targeted modeling and evaluation strategies for
real-world dialogue summarization.

</details>


### [3] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
*Wenquan Lu,Yuechuan Yang,Kyle Lee,Yanshu Li,Enqi Liu*

Main category: cs.CL

> 本文探究了递归式变压器模型Huginn-3.5B中是否形成了可解释的内部化推理结构。结果表明，虽然存在一些证据，但是限制较大，递归深度增加对性能提升有限。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在研究Huginn-3.5B（一种在推理时重复使用层数而不会增加参数数量的深度递归变压器）中是否形成了推理结构，并探讨内部化推理在潜在空间中的表现。

**Method:** 本文使用了一系列探测技术，包括Logit Lens和Coda Lens，来探究模型在算术任务上的内部行为。

**Result:** 研究结果显示，通过跟踪最终和中间结果标记的排名轨迹，只有有限的证据表明存在可解释的潜在链式思维。另外，研究还揭示了递归块中的探测不一致性。

**Conclusion:** 实验证明，增加递归深度只能带来微不足道的收益，并远不及那些明确外部化推理步骤的模型。

**Abstract:** Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [4] [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)
*Steven Song,Anirudh Subramanyam,Zhenyu Zhang,Aarti Venkat,Robert L. Grossman*

Main category: cs.CL

> 本文介绍了GDC Cohort Copilot，一个开源工具，用于通过自然语言描述生成GDC队列过滤器，优于商业化的GPT-4方法。

<details>
  <summary>Details</summary>

**Motivation:** 为了简化GDC用户，特别是新用户在数百个可能的字段和属性中寻找特定队列描述符的困难，我们开发了一种可以将用户用自然语言描述的所需的队列转化为GDC队列过滤器的工具。

**Method:** 我们引入了GDC Cohort Copilot，这是一个开源的辅助工具，用于从GDC中整理队列。GDC Cohort Copilot能根据用户输入的自然语言描述自动生成GDC队列过滤器，并可以将队列导回到GDC进行进一步分析。此外，我们还开发和评估了几种大型语言模型（LLMs）用于GDC Cohort Copilot，并证明了我们的本地服务的开源GDC Cohort LLM比GPT-4的提示生成GDC队列效果更好。

**Result:** 我们展示了本地服务的开源GDC Cohort LLM在生成GDC队列方面比GPT-4的提示表现更好。

**Conclusion:** GDC Cohort Copilot及其相关模型已被证明在生成符合用户自然语言描述的GDC队列方面有效，并且其开源和本地服务特性使其对用户群体具有吸引力。

**Abstract:** Motivation: The Genomic Data Commons (GDC) provides access to high quality,
harmonized cancer genomics data through a unified curation and analysis
platform centered around patient cohorts. While GDC users can interactively
create complex cohorts through the graphical Cohort Builder, users (especially
new ones) may struggle to find specific cohort descriptors across hundreds of
possible fields and properties. However, users may be better able to describe
their desired cohort in free-text natural language.
  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for
curating cohorts from the GDC. GDC Cohort Copilot automatically generates the
GDC cohort filter corresponding to a user-input natural language description of
their desired cohort, before exporting the cohort back to the GDC for further
analysis. An interactive user interface allows users to further refine the
generated cohort. We develop and evaluate multiple large language models (LLMs)
for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC
Cohort LLM achieves better results than GPT-4o prompting in generating GDC
cohorts.
  Availability and implementation: The standalone docker image for GDC Cohort
Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.
Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC
Cohort LLM weights are available at https://huggingface.co/uc-ctds.

</details>


### [5] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
*Hongli Yu,Tinghong Chen,Jiangtao Feng,Jiangjie Chen,Weinan Dai,Qiying Yu,Ya-Qin Zhang,Wei-Ying Ma,Jingjing Liu,Mingxuan Wang,Hao Zhou*

Main category: cs.CL

> 论文通过MemAgent和扩展的DAPO算法解决了长上下文处理的挑战，显示了极好的扩展能力。测试表明可以实现从8K上下文扩展到32K文本，并在3.5M QA任务中保持性能损失低于5%。

<details>
  <summary>Details</summary>

**Motivation:** 论文动机在于解决当前长文本处理技术面临的挑战，即在性能不下降的情况下，实现对于无限长文档的线性复杂度计算。

**Method:** 提出了一个新的代理工作流程MemAgent，它采用分段读取文本并使用覆盖策略更新内存的方法。同时，扩展了DAPO算法以便通过独立上下文的多对话生成助力训练过程。

**Result:** 尽管在长度外推、有效注意力和内存模块方面有所改进，但在处理长文本时，如何在性能不降低的前提下实现线性复杂度的无限长文档处理，仍然是一个最终挑战。我们以端到端的方式针对长文本任务进行直接优化，并引入了一种新型的代理工作流程MemAgent，该代理分段读取文本并通过覆盖策略更新内存。我们扩展了DAPO算法，通过独立上下文的多对话生成来促进训练。MemAgent展示了卓越的长上下文处理能力，能够从8K上下文训练扩展到32K文本，并成功完成3.5M QA任务，性能损失小于5%，在512K RULER测试中达到95%以上的表现。

**Conclusion:** MemAgent展示了处理长上下文的强大能力，并能在从较小上下文扩展到大规模任务时维持相对稳定的性能，这证明了对长文档处理的实质性进展。

**Abstract:** Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [6] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

> DoMIX是一个新的算法，用于解决现有的连续领域自适应预训练方法中存在的挑战，该方法效果好且计算成本低，并能为特定任务提供定制化的预训练模型。

<details>
  <summary>Details</summary>

**Motivation:** 针对现有的连续领域自适应预训练方法存在的问题，如计算成本高、对增量数据的顺序敏感、以及提供单一的通用模型与领域自适应预训练的本质相矛盾，DoMIX旨在提供一种更优的解决方案。

**Method:** 通过利用LoRA模块（一种参数高效微调方法），DoMIX提出了一个有效且能够并行进行领域自适应预训练的新方法，这种方法对抗领域顺序的鲁棒性强，并能有效利用积累的知识，提供特定任务所需的预训练模型。

**Result:** 该论文展示了DoMIX方法不仅可以在领域自适应预训练中有效执行，而且可以扩展到标准的大规模语言模型微调场景中。

**Conclusion:** DoMIX能够解决现有连续领域自适应预训练方法的挑战，提供了一种高效、鲁棒且可定制的预训练模型解决方案。此外，这种方法在标准的大规模语言模型微调场景中也显示出良好的适应性。

**Abstract:** Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [7] [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)
*Christian Jaumann,Annemarie Friedrich,Rainer Lienhart*

Main category: cs.CL

> 该论文描述了一个用于SciVQA 2025共享任务——科学视觉问答的系统。系统采用两种多模态大型语言模型的组合和多种少量示例检索策略，并根据图表和问题类型选择模型和少量示例设置，依据模型的置信度选择答案。在盲测数据上，该系统排名第三，平均F1分达到85.12。代码公开可用。

<details>
  <summary>Details</summary>

**Motivation:** 旨在提高科学视觉问答任务的性能，利用多模型组合和少量示例检索策略来提升表现。

**Method:** Structure

**Result:** {
  "tldr": "该论文描述了一个用于SciVQA 2025共享任务——科学视觉问答的系统。系统采用两种多模态大型语言模型的组合和多种少量示例检索策略，并根据图表和问题类型选择模型和少量示例设置，依据模型的置信度选择答案。在盲测数据上，该系统排名第三，平均F1分达到85.12。代码公开可用。",
  "motivation": "旨在提高科学视觉问答任务的性能，利用多模型组合和少量示例检索策略来提升表现。",
  "method": "使用多种多模态大型语言模型组合和少量实例检索策略，根据问题类型和图表类型选择具体的模型和检索策略，依据模型的置信度水平选择最终答案。",
  "result": "在盲测数据上，该系统的平均F1分（基于ROUGE-1, ROUGE-L, 和BERTS）达到了85.12，排名第三。",
  "conclusion": "研究结果表明，多模型组合和少量实例检索策略在提高科学视觉问答的准确性方面是有效的，公开的代码有利于进一步的研究工作。"}
}


**Conclusion:** 研究结果表明，多模型组合和少量实例检索策略在提高科学视觉问答的准确性方面是有效的，公开的代码有利于进一步的研究工作。

**Abstract:** This paper describes our system for the SciVQA 2025 Shared Task on Scientific
Visual Question Answering. Our system employs an ensemble of two Multimodal
Large Language Models and various few-shot example retrieval strategies. The
model and few-shot setting are selected based on the figure and question type.
We also select answers based on the models' confidence levels. On the blind
test data, our system ranks third out of seven with an average F1 score of
85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.

</details>


### [8] [QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers](https://arxiv.org/abs/2507.02364)
*Pilsung Kang*

Main category: cs.CL

> 本研究提出QFFN-BERT，将BERT变体中的前馈网络替换为PQC基层，证明了PQC作为经典FFN替代方案时，可以实现参数高效和数据高效。

<details>
  <summary>Details</summary>

**Motivation:** 在大多数变压器编码器块中，FFN约占有总量的三分之二的参数，而以往的研究主要集中在将PQC集成于自我注意力模块中。因此，本研究选择FFN进行探索，系统地研究了PQC深度、表达能力和可训练性之间的权衡。

**Method:** 本研究设计了一种名为QFFN-BERT的混合量子经典变压器，将紧凑型BERT变体中的前馈网络（FFN）模块替换为基于参数化量子电路（PQC）的层。PQC架构中包含残差连接、$R_Y$和$R_Z$旋转以及交替纠缠策略，以确保稳定训练和高表达能力。

**Result:** 实验结果表明，优化后的QFFN-BERT可以在全数据设置下达到基线准确率的102.0%，同时将FFN相关的参数减少了超过99%。此外，该模型在少量样本学习场景中表现出色，证明了其在数据效率方面的潜在优势。

**Conclusion:** 综合以上结果，可以得出结论，当与基础深度学习原则协同设计时，参数化量子电路可以作为强大的经典前馈网络替代方案，并享有参数高效的优势。

**Abstract:** Parameterized quantum circuits (PQCs) have recently emerged as promising
components for enhancing the expressibility of neural architectures. In this
work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the
feedforward network (FFN) modules of a compact BERT variant are replaced by
PQC-based layers. This design is motivated by the dominant parameter
contribution of FFNs, which account for approximately two-thirds of the
parameters within standard Transformer encoder blocks. While prior studies have
primarily integrated PQCs into self-attention modules, our work focuses on the
FFN and systematically investigates the trade-offs between PQC depth,
expressibility, and trainability. Our final PQC architecture incorporates a
residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating
entanglement strategy to ensure stable training and high expressibility. Our
experiments, conducted on a classical simulator, on the SST-2 and DBpedia
benchmarks demonstrate two key findings. First, a carefully configured
QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its
classical counterpart in a full-data setting while reducing FFN-specific
parameters by over 99%. Second, our model exhibits a consistent and competitive
edge in few-shot learning scenarios, confirming its potential for superior data
efficiency. These results, supported by an ablation study on a non-optimized
PQC that failed to learn, confirm that PQCs can serve as powerful and
parameter-efficient alternatives to classical FFNs when co-designed with
foundational deep learning principles.

</details>


### [9] [Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection](https://arxiv.org/abs/2507.02378)
*Weijie Lyu,Sheng-Jun Huang,Xuan Xia*

Main category: cs.CL

> This paper introduces a parametric model for selecting high-quality code data to improve the training efficiency and performance of large language models.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to improve the training efficiency and performance of LLMs in code generation, which current methods fail to do effectively due to a focus on data quantity over quality.

**Method:** Our method utilizes a parametric model to select high-quality code data, optimizing for distribution consistency and diversity, which improves the efficiency and performance of large language models.

**Result:** Experimental results show that the proposed method achieves performance gains of 2.4% on HumanEval and 2.3% on MBPP, using only 10K samples compared to a 92K full-sampled baseline.

**Conclusion:** The research concludes that their method not only outperforms existing sampling approaches but also does so with significantly fewer data points, highlighting its effectiveness in enhancing model performance while reducing computational costs.

**Abstract:** Recent advancements in large language models (LLMs) have significantly
improved code generation and program comprehension, accelerating the evolution
of software engineering. Current methods primarily enhance model performance by
leveraging vast amounts of data, focusing on data quantity while often
overlooking data quality, thereby reducing training efficiency. To address
this, we introduce an approach that utilizes a parametric model for code data
selection, aimed at improving both training efficiency and model performance.
Our method optimizes the parametric model to ensure distribution consistency
and diversity within the selected subset, guaranteeing high-quality data.
Experimental results demonstrate that using only 10K samples, our method
achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled
baseline, outperforming other sampling approaches in both performance and
efficiency. This underscores that our method effectively boosts model
performance while significantly reducing computational costs.

</details>


### [10] [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)
*Mark Atta Mensah,Isaac Wiafe,Akon Ekpezu,Justice Kwame Appati,Jamal-Deen Abdulai,Akosua Nyarkoa Wiafe-Akenten,Frank Ernest Yeboah,Gifty Odame*

Main category: cs.CL

> 该研究详述了基于Transformer架构的不同ASR模型在处理阿肯语不同领域语音数据时的表现差异，并指出这些模型存在显著的域依赖性及错误行为的区别。

<details>
  <summary>Details</summary>

**Motivation:** 大多数现有的自动语音识别研究使用领域内数据集来评估模型，但很少评估它们跨多样语音环境的表现。这项研究旨在填补这一空白，探讨ASR模型在不同域上的性能表现。

**Method:** 本研究通过使用四种阿肯语语音语料库来评估七种基于Transformer架构的阿肯语自动语音识别（ASR）模型，包括Whisper和Wav2Vec2模型的性能。语料库涵盖了不同的领域，包括文化相关的图像描述、非正式对话、圣经经文朗读以及自发性金融对话。

**Result:** 研究发现，当模型面临与训练域不匹配的情境时，其准确性显著下降，这表明了ASR模型存在域依赖性。此外，Whisper和Wav2Vec2模型在面对不熟悉输入时具有不同的错误行为。

**Conclusion:** 针对低资源语言的ASR应用选择架构时，需要在转录错误的流畅性和透明性之间权衡。研究还提出，为了改善低资源语言的性能，应采取特定的领域适应技术、自适应路由策略以及多语言训练框架。

**Abstract:** Most existing automatic speech recognition (ASR) research evaluate models
using in-domain datasets. However, they seldom evaluate how they generalize
across diverse speech contexts. This study addresses this gap by benchmarking
seven Akan ASR models built on transformer architectures, such as Whisper and
Wav2Vec2, using four Akan speech corpora to determine their performance. These
datasets encompass various domains, including culturally relevant image
descriptions, informal conversations, biblical scripture readings, and
spontaneous financial dialogues. A comparison of the word error rate and
character error rate highlighted domain dependency, with models performing
optimally only within their training domains while showing marked accuracy
degradation in mismatched scenarios. This study also identified distinct error
behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned
Whisper Akan models led to more fluent but potentially misleading transcription
errors, Wav2Vec2 produced more obvious yet less interpretable outputs when
encountering unfamiliar inputs. This trade-off between readability and
transparency in ASR errors should be considered when selecting architectures
for low-resource language (LRL) applications. These findings highlight the need
for targeted domain adaptation techniques, adaptive routing strategies, and
multilingual training frameworks for Akan and other LRLs.

</details>


### [11] [A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages](https://arxiv.org/abs/2507.02428)
*Sumaya Ahmed Salihs,Isaac Wiafe,Jamal-Deen Abdulai,Elikem Doe Atsakpo,Gifty Ayoka,Richard Cave,Akon Obu Ekpezu,Catherine Holloway,Katrin Tomanek,Fiifi Baffoe Payin Winful*

Main category: cs.CL

> 研究开发了一套社区驱动的ASR模型建构方法及数据收集最佳实践，创建了开放资源的阿坎语障碍语音数据集，并展示了针对阿坎语障碍语音的ASR模型初步微调结果。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的目的是通过建立ASR技术和数据收集的最佳实践，推动ASR技术的普及化和数据收集的民主化。

**Method:** 本研究提出了一种收集语音样本的方法，以建立针对有语音障碍者的自动语音识别（ASR）模型，尤其针对资源匮乏的语言。研究通过制定一套“烹饪手册”来推动社区驱动的数据收集和ASR模型构建的技术，并以阿坎语为例，这是加纳广泛使用的土著语言之一。

**Result:** 研究收集了来自不同背景的有语音障碍的参与者的语音数据，形成了第一个开放来源的阿坎语障碍语音数据集，并且这个数据集、手册以及开源工具都是公开可用的。此外，本研究还展示了将开源ASR模型微调以更好地识别阿坎语障碍语音的初步结果。

**Conclusion:** 本研究通过提供开放的数据集、最佳实践指南和开源工具，为研究者和从业者创造包容性ASR技术，以满足有语音障碍者的需求，初步展示了针对阿坎语障碍语音的ASR模型微调的可行性。

**Abstract:** This study presents an approach for collecting speech samples to build
Automatic Speech Recognition (ASR) models for impaired speech, particularly,
low-resource languages. It aims to democratize ASR technology and data
collection by developing a "cookbook" of best practices and training for
community-driven data collection and ASR model building. As a proof-of-concept,
this study curated the first open-source dataset of impaired speech in Akan: a
widely spoken indigenous language in Ghana. The study involved participants
from diverse backgrounds with speech impairments. The resulting dataset, along
with the cookbook and open-source tools, are publicly available to enable
researchers and practitioners to create inclusive ASR technologies tailored to
the unique needs of speech impaired individuals. In addition, this study
presents the initial results of fine-tuning open-source ASR models to better
recognize impaired speech in Akan.

</details>


### [12] [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)
*Sneha Deshmukh,Prathmesh Kamble*

Main category: cs.CL

> 我们介绍了一个新的基准数据集IndianBailJudgments-1200，包含1200份印度法院判决书，注释了20多个属性，如保释结果、相关刑法节、犯罪类型和法律推理。此数据集支持法律NLP任务，如结果预测、摘要和公正性分析。

<details>
  <summary>Details</summary>

**Motivation:** 印度等地区法律NLP发展滞后，主要是因为缺乏结构化数据集。我们的目标是填补这一空白，提供一个丰富的法律文本数据集。

**Method:** 我们使用了经过提示工程优化的GPT-4管道生成注释，并验证了注释的一致性，创建了IndianBailJudgments-1200数据集。

**Result:** 创建了包含1200份印度法院保释决定判决书的数据集，注释了超过20个属性，首次公开发布专门针对印度保释法理学的数据集。

**Conclusion:** 该数据集支持广泛的法律NLP任务，并首次公开发布，专门针对印度保释法理学，填补了印度地区该领域的空白。

**Abstract:** Legal NLP remains underdeveloped in regions like India due to the scarcity of
structured datasets. We introduce IndianBailJudgments-1200, a new benchmark
dataset comprising 1200 Indian court judgments on bail decisions, annotated
across 20+ attributes including bail outcome, IPC sections, crime type, and
legal reasoning. Annotations were generated using a prompt-engineered GPT-4o
pipeline and verified for consistency. This resource supports a wide range of
legal NLP tasks such as outcome prediction, summarization, and fairness
analysis, and is the first publicly available dataset focused specifically on
Indian bail jurisprudence.

</details>


### [13] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Liwen Zhang,Litu Ou,Jialong Wu,Wenbiao Yin,Baixuan Li,Zhengwei Tao,Xinyu Wang,Weizhou Shen,Junkai Zhang,Dingchu Zhang,Xixi Wu,Yong Jiang,Ming Yan,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

> 提出WebSailor方法，通过生成高不确定性的任务和应用DUPO算法提升开源模型在复杂信息搜索任务中的表现，以缩小与专有模型之间的性能差距。

<details>
  <summary>Details</summary>

**Motivation:** 在复杂信息搜索任务中超越人类认知限制，提升开源语言模型的推理能力，实现与专有模型相匹配的表现。

**Method:** 采用生成高不确定性任务、结构化采样和信息混淆等策略，并通过DUPO算法进行高效的基于代理的强化学习训练。

**Result:** WebSailor在复杂信息搜索任务上显著超越了所有开源模型，并达到了与专有模型相当的性能。

**Conclusion:** 证明了WebSailor作为一种完整的方法论，在培养大规模语言模型处理高不确定环境下的系统化能力方面是有效的。

**Abstract:** Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all opensource agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [14] [Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges](https://arxiv.org/abs/2507.02074)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.CV

> 本文结构化地调查并总结了利用大型语言模型进行视频数据中交通事故检测的近期方法，提供了未来研究的坚实基础。

<details>
  <summary>Details</summary>

**Motivation:** 由于大型语言模型（LLMs）和视觉-语言模型（VLMs）的发展，如何处理、推理和总结多模态信息得到了转变，因此本文关注于智能交通系统中超重要的问题之一：从视频流中检测交通事故。

**Method:** 本文通过结构化的分类策略，总结了关键的数据集，分析了模型架构，比较了性能基准，并讨论了正在进行的挑战和机遇，以调查利用大型语言模型（LLMs）进行视频数据中交通事故检测的近期方法。

**Result:** 本文提出了一个融合策略的结构化分类法，总结了关键的数据集，分析了模型架构，比较了性能基准，并讨论了正在进行的挑战和机遇。

**Conclusion:** 本文为视频理解和基础模型这一快速发展的交叉领域的未来研究提供了基础。

**Abstract:** Crash detection from video feeds is a critical problem in intelligent
transportation systems. Recent developments in large language models (LLMs) and
vision-language models (VLMs) have transformed how we process, reason about,
and summarize multimodal information. This paper surveys recent methods
leveraging LLMs for crash detection from video data. We present a structured
taxonomy of fusion strategies, summarize key datasets, analyze model
architectures, compare performance benchmarks, and discuss ongoing challenges
and opportunities. Our review provides a foundation for future research in this
fast-growing intersection of video understanding and foundation models.

</details>


### [15] [Underwater Monocular Metric Depth Estimation: Real-World Benchmarks and Synthetic Fine-Tuning](https://arxiv.org/abs/2507.02148)
*Zijie Cai,Christopher Metzler*

Main category: cs.CV

> 本文评估了单目度量深度估计模型在水下环境中的表现，并通过对模型进行微调来改善其性能。

<details>
  <summary>Details</summary>

**Motivation:** 水下环境由于光衰减、散射、色彩失真和浊度等因素，使得单目度量深度预测的可靠性有限。此外，还缺乏高质量的度量真实数据。本文旨在评测和改善单目深度估计在水下环境中的性能。

**Method:** 本研究使用了大规模在陆地（真实或合成）数据上训练的单目度量深度估计模型，并对其中一个模型Depth Anything V2使用了ViT-S骨干编码器进行了微调，使用的数据集是通过基于物理的水下图像形成模型生成的合成水下Hypersim数据集变体。

**Result:** 研究结果显示，经过微调的模型在所有基准上都显示出比仅在清洁的陆地Hypersim数据集上训练的基线模型更好的性能。

**Conclusion:** 本文提供的详细评估和可视化结果表明，进行领域自适应和尺度感知监督对于在未来研究中在具有挑战性的水下环境中实现稳健且普遍适用的度量深度预测至关重要。

**Abstract:** Monocular depth estimation has recently advanced to provide not only relative
but also metric depth predictions. However, its reliability in underwater
environments remains limited due to light attenuation and scattering, color
distortion, turbidity, and the lack of high-quality metric ground-truth data.
In this paper, we present a comprehensive benchmark of zero-shot and fine-tuned
monocular metric depth estimation models on real-world underwater datasets with
metric depth annotations, such as FLSea and SQUID. We evaluate a diverse set of
state-of-the-art models across a range of underwater conditions with different
ranges. Our results show that large-scale models trained on terrestrial (real
or synthetic) data, while effective in in-air settings, perform poorly
underwater due to significant domain shifts. To address this, we fine-tune
Depth Anything V2 with a ViT-S backbone encoder on a synthetic underwater
variant of the Hypersim dataset, which we generated using a physically based
underwater image formation model. We demonstrate our fine-tuned model
consistently improves performance across all benchmarks and outperforms
baselines trained only on the clean in-air Hypersim dataset. Our study provides
a detailed evaluation and visualization for monocular metric depth estimation
in underwater scenes, highlighting the importance of domain adaptation and
scale-aware supervision for achieving robust and generalizable metric depth
predictions in challenging underwater environments for future research.

</details>


### [16] [ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.02200)
*Xiao Wang,Jingtao Jiang,Qiang Chen,Lan Chen,Lin Zhu,Yaowei Wang,Yonghong Tian,Jin Tang*

Main category: cs.CV

> 提出了一种基于链式思考推理的事件流场景文本识别框架ESTR-CoT，用于在低光照、快速运动等极端情况下进行文本识别，解决了现有方法在可解释性和上下文逻辑推理方面存在的问题。实验结果验证了该框架的有效性和可解释性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于事件流的场景文本识别方法主要采用端到端的编码器-解码器框架或大型语言模型，但这些方法在可解释性和逻辑推理方面仍然存在不足。因此，我们提出了ESTR-CoT框架以解决这些问题。

**Method:** 该框架首先使用EVA-CLIP把事件流转换为令牌，并使用Llama tokenizer对给定的生成提示进行编码。然后通过Q-former将视觉令牌与预训练的大型语言模型Vicuna-7B对齐，并同时输出答案和链式思考过程。

**Result:** 该框架在三个事件流STR基准数据集（EventSTR、WordArt*、IC15*）上进行了广泛实验，结果验证了其有效性和可解释性。

**Conclusion:** ESTR-CoT框架在极端条件下表现良好，特别是在低光照、快速运动的情况下，该框架强调了其在解释能力和上下文逻辑推理方面的能力。

**Abstract:** Event stream based scene text recognition is a newly arising research topic
in recent years which performs better than the widely used RGB cameras in
extremely challenging scenarios, especially the low illumination, fast motion.
Existing works either adopt end-to-end encoder-decoder framework or large
language models for enhanced recognition, however, they are still limited by
the challenges of insufficient interpretability and weak contextual logical
reasoning. In this work, we propose a novel chain-of-thought reasoning based
event stream scene text recognition framework, termed ESTR-CoT. Specifically,
we first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input
event stream into tokens and utilize a Llama tokenizer to encode the given
generation prompt. A Q-former is used to align the vision token to the
pre-trained large language model Vicuna-7B and output both the answer and
chain-of-thought (CoT) reasoning process simultaneously. Our framework can be
optimized using supervised fine-tuning in an end-to-end manner. In addition, we
also propose a large-scale CoT dataset to train our framework via a three stage
processing (i.e., generation, polish, and expert verification). This dataset
provides a solid data foundation for the development of subsequent
reasoning-based large models. Extensive experiments on three event stream STR
benchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the
effectiveness and interpretability of our proposed framework. The source code
and pre-trained models will be released on
https://github.com/Event-AHU/ESTR-CoT.

</details>


### [17] [Team RAS in 9th ABAW Competition: Multimodal Compound Expression Recognition Approach](https://arxiv.org/abs/2507.02205)
*Elena Ryumina,Maxim Markitantov,Alexandr Axyonov,Dmitry Ryumin,Mikhail Dolgushin,Alexey Karpov*

Main category: cs.CV

> 该研究提出了一种新颖的零样本多模态方法进行复合表情识别，结合了六种模态。在多个数据集上显示出可比较于监督方法的性能。

<details>
  <summary>Details</summary>

**Motivation:** 传统的复合表情识别方法通常依赖任务特定的训练数据，该研究旨在提出一种零样本的方法，以减少对大量标注数据的依赖。

**Method:** 该研究提出了一种新颖的零样本多模态方法进行复合表情识别，结合了六种异构模态：静态和动态面部表情、场景和标签匹配、场景上下文、音频和文本。该方法使用Contrastive Language-Image Pretraining (CLIP) 基于的标签匹配和Qwen-VL进行语义场景理解。

**Result:** 在多个语料库训练下，该方法在AffWild2数据集上的F1得分为46.95%，在Acted Facial Expressions in The Wild (AFEW) 数据集上的F1得分为49.02%，在C-EXPR-DB数据集上的零样本测试F1得分为34.85%。

**Conclusion:** 结果表明，该方法在复合情感识别中无需域适应就能获得有效结果，其性能与基于目标数据训练的监督方法相当。

**Abstract:** Compound Expression Recognition (CER), a subfield of affective computing,
aims to detect complex emotional states formed by combinations of basic
emotions. In this work, we present a novel zero-shot multimodal approach for
CER that combines six heterogeneous modalities into a single pipeline: static
and dynamic facial expressions, scene and label matching, scene context, audio,
and text. Unlike previous approaches relying on task-specific training data,
our approach uses zero-shot components, including Contrastive Language-Image
Pretraining (CLIP)-based label matching and Qwen-VL for semantic scene
understanding. We further introduce a Multi-Head Probability Fusion (MHPF)
module that dynamically weights modality-specific predictions, followed by a
Compound Expressions (CE) transformation module that uses Pair-Wise Probability
Aggregation (PPA) and Pair-Wise Feature Similarity Aggregation (PFSA) methods
to produce interpretable compound emotion outputs. Evaluated under multi-corpus
training, the proposed approach shows F1 scores of 46.95% on AffWild2, 49.02%
on Acted Facial Expressions in The Wild (AFEW), and 34.85% on C-EXPR-DB via
zero-shot testing, which is comparable to the results of supervised approaches
trained on target data. This demonstrates the effectiveness of the proposed
approach for capturing CE without domain adaptation. The source code is
publicly available.

</details>


### [18] [SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers](https://arxiv.org/abs/2507.02212)
*Takuro Kawada,Shunsuke Kitada,Sota Nemoto,Hitoshi Iyatomi*

Main category: cs.CV

> The paper introduces SciGA-145k, a large-scale dataset aimed at supporting Graphical Abstracts (GAs) selection, recommendation and automated generation, through two defined tasks and a novel metric named CAR.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to enhance scientific communication through effective GAs, despite the barrier of advanced visualization skills required to design them.

**Method:** The method involves creating a large dataset named SciGA-145k with scientific papers and figures for GA support, defining tasks for GA recommendation and proposing the CAR metric.

**Result:** The paper provides baseline models for GA recommendation tasks and a detailed metric analysis through CAR.

**Conclusion:** The conclusion is that SciGA-145k and the proposed CAR metric lay the groundwork for improving visual scientific communication and AI in science research.

**Abstract:** Graphical Abstracts (GAs) play a crucial role in visually conveying the key
findings of scientific papers. While recent research has increasingly
incorporated visual materials such as Figure 1 as de facto GAs, their potential
to enhance scientific communication remains largely unexplored. Moreover,
designing effective GAs requires advanced visualization skills, creating a
barrier to their widespread adoption. To tackle these challenges, we introduce
SciGA-145k, a large-scale dataset comprising approximately 145,000 scientific
papers and 1.14 million figures, explicitly designed for supporting GA
selection and recommendation as well as facilitating research in automated GA
generation. As a preliminary step toward GA design support, we define two
tasks: 1) Intra-GA recommendation, which identifies figures within a given
paper that are well-suited to serve as GAs, and 2) Inter-GA recommendation,
which retrieves GAs from other papers to inspire the creation of new GAs. We
provide reasonable baseline models for these tasks. Furthermore, we propose
Confidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation
metric that offers a fine-grained analysis of model behavior. CAR addresses
limitations in traditional ranking-based metrics by considering cases where
multiple figures within a paper, beyond the explicitly labeled GA, may also
serve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a
foundation for advancing visual scientific communication while contributing to
the development of AI for Science.

</details>


### [19] [Understanding Trade offs When Conditioning Synthetic Data](https://arxiv.org/abs/2507.02217)
*Brandon Trabucco,Qasim Wani,Benjamin Pikus,Vasu Sharma*

Main category: cs.CV

> 本研究通过分析两种合成数据生成策略，发现基于布局的条件策略在提升对象检测性能方面表现更佳，尤其在数据多样性较高的情况下。使用匹配完整训练分布的布局线索生成的合成数据可显著提升检测准确率。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于解决从有限图像中学习鲁棒对象检测器的挑战。当前使用3D引擎生成数据的方法耗时长且模拟与现实之间存在较大差异。扩散模型虽然可以快速生成高质量图像，但在低数据环境下实现精确控制仍具挑战性。

**Method:** 研究针对四个标准对象检测基准中的八十种多样化视觉概念进行了分析，比较了基于提示和基于布局两种条件策略的效果。

**Result:** 本研究分析了两种条件策略（基于提示和基于布局）在生成高质量合成数据方面的效果，特别是针对对象检测任务。研究发现，在条件线索较窄时，基于提示的条件策略生成的合成数据质量更高；随着多样性的增加，基于布局的条件策略则表现出更好的性能。当布局线索与完整的训练分布相匹配时，使用合成数据可以将平均精度提高34%，甚至最高可以提高177%，相比单独使用真实数据。

**Conclusion:** 研究证明了基于布局的条件策略在合成数据生成中的优越性，特别是在条件多样性增加时。使用合成数据可以显著提高对象检测任务的性能。

**Abstract:** Learning robust object detectors from only a handful of images is a critical
challenge in industrial vision systems, where collecting high quality training
data can take months. Synthetic data has emerged as a key solution for data
efficient visual inspection and pick and place robotics. Current pipelines rely
on 3D engines such as Blender or Unreal, which offer fine control but still
require weeks to render a small dataset, and the resulting images often suffer
from a large gap between simulation and reality. Diffusion models promise a
step change because they can generate high quality images in minutes, yet
precise control, especially in low data regimes, remains difficult. Although
many adapters now extend diffusion beyond plain text prompts, the effect of
different conditioning schemes on synthetic data quality is poorly understood.
We study eighty diverse visual concepts drawn from four standard object
detection benchmarks and compare two conditioning strategies: prompt based and
layout based. When the set of conditioning cues is narrow, prompt conditioning
yields higher quality synthetic data; as diversity grows, layout conditioning
becomes superior. When layout cues match the full training distribution,
synthetic data raises mean average precision by an average of thirty four
percent and by as much as one hundred seventy seven percent compared with using
real data alone.

</details>


### [20] [High-Fidelity Differential-information Driven Binary Vision Transformer](https://arxiv.org/abs/2507.02222)
*Tian Gao,Zhiyuan Zhang,Kaijie Yin,Xu-Cheng Zhong,Hui Kong*

Main category: cs.CV

> 论文提出了一种名为DIDB-ViT的新二值化视觉变压器，以解决现有二值化ViT性能下降或依赖实数操作的问题，以保持信息性和计算效率。

<details>
  <summary>Details</summary>

**Motivation:** 论文指出，视觉变压器（ViTs）的二值化为解决高计算/存储需求与边缘设备部署之间的矛盾提供了一种有前景的方法。然而，现有的二值化ViT方法往往面临着性能显著下降或严重依赖全精度操作的问题。

**Method:** 该论文提出了DIDB-ViT，这是一种新颖的二值化视觉变压器（ViT），在保持原始ViT架构和计算效率的同时，其信息性强。具体来说，设计了一个包含差异信息的明智注意力模块，以减轻二值化引起的信息损失，并增强高频保留。同时，通过离散哈夫小波的频率分解和不同频率下的相似性的集成来保持二值化Q和K张量之间的相似性计算的保真度。此外，引入了改进的RPReLU激活函数来重新构造激活分布，扩大模型的表示能力。

**Result:** 实验结果显示，DIDB-ViT在网络量化领域超越了现有最先进的方法，同时在多种ViT架构中展现了出色的图像分类和分割性能。

**Conclusion:** 该论文提出的方法在保持高信息性和计算效率的同时，解决了ViT二值化面临的关键挑战，其DIDB-ViT方法在不同基准测试中表现优异。

**Abstract:** The binarization of vision transformers (ViTs) offers a promising approach to
addressing the trade-off between high computational/storage demands and the
constraints of edge-device deployment. However, existing binary ViT methods
often suffer from severe performance degradation or rely heavily on
full-precision modules. To address these issues, we propose DIDB-ViT, a novel
binary ViT that is highly informative while maintaining the original ViT
architecture and computational efficiency. Specifically, we design an
informative attention module incorporating differential information to mitigate
information loss caused by binarization and enhance high-frequency retention.
To preserve the fidelity of the similarity calculations between binary Q and K
tensors, we apply frequency decomposition using the discrete Haar wavelet and
integrate similarities across different frequencies. Additionally, we introduce
an improved RPReLU activation function to restructure the activation
distribution, expanding the model's representational capacity. Experimental
results demonstrate that our DIDB-ViT significantly outperforms
state-of-the-art network quantization methods in multiple ViT architectures,
achieving superior image classification and segmentation performance.

</details>


### [21] [FMOcc: TPV-Driven Flow Matching for 3D Occupancy Prediction with Selective State Space Model](https://arxiv.org/abs/2507.02250)
*Jiangxia Chen,Tongyuan Huang,Ke Song*

Main category: cs.CV

> 论文介绍了一种新的网络FMOcc，用于提高基于少量帧数据的3D占用预测准确性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法通过融合历史帧来提高性能，但这需要额外的数据和计算资源。此论文提出FMOcc来解决由于少数帧图像和3D空间冗余导致的遮挡和远距离场景预测精度低的问题。

**Method:** FMOcc, 一个基于流匹配选择性状态空间模型（FMSSM）、TPV SSM层和PS3M的三视角视图（TPV）细化占有率网络，旨在解决少数帧3D占用预测的问题。

**Result:** 实验结果显示FMOcc在Occ3D-nuScenes和OpenOcc数据集上的表现优于现有最先进方法。使用两帧输入时，在Occ3D-nuScenes验证上的RayIoU为43.1%，mIoU为39.8%，在OpenOcc数据集上RayIoU为42.6%，推理内存为5.4G，推理时间为330毫秒。

**Conclusion:** 通过设计FMSSM、TPV SSM层、PS3M和Mask Training方法，FMOcc实现了对远距离场景预测能力的增强，以及模型效率与鲁棒性的提升。相比现有方法，FMOcc在性能上有所提升，同时保持较低的计算资源需求。

**Abstract:** 3D semantic occupancy prediction plays a pivotal role in autonomous driving.
However, inherent limitations of fewframe images and redundancy in 3D space
compromise prediction accuracy for occluded and distant scenes. Existing
methods enhance performance by fusing historical frame data, which need
additional data and significant computational resources. To address these
issues, this paper propose FMOcc, a Tri-perspective View (TPV) refinement
occupancy network with flow matching selective state space model for few-frame
3D occupancy prediction. Firstly, to generate missing features, we designed a
feature refinement module based on a flow matching model, which is called Flow
Matching SSM module (FMSSM). Furthermore, by designing the TPV SSM layer and
Plane Selective SSM (PS3M), we selectively filter TPV features to reduce the
impact of air voxels on non-air voxels, thereby enhancing the overall
efficiency of the model and prediction capability for distant scenes. Finally,
we design the Mask Training (MT) method to enhance the robustness of FMOcc and
address the issue of sensor data loss. Experimental results on the
Occ3D-nuScenes and OpenOcc datasets show that our FMOcc outperforms existing
state-of-theart methods. Our FMOcc with two frame input achieves notable scores
of 43.1% RayIoU and 39.8% mIoU on Occ3D-nuScenes validation, 42.6% RayIoU on
OpenOcc with 5.4 G inference memory and 330ms inference time.

</details>


### [22] [SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement](https://arxiv.org/abs/2507.02252)
*Zeyu Lei,Hongyuan Yu,Jinlin Wu,Zhen Chen*

Main category: cs.CV

> 本文提出了一种名为SurgVisAgent的端到端智能手术视觉代理，它可以识别和减轻多种类型的图像失真，适应复杂的手术环境，提供了统一的手术辅助方案。

<details>
  <summary>Details</summary>

**Motivation:** 尽管在手术算法方面取得了显著进步，但这些算法通常针对特定场景下的单个任务设计，这在复杂的真实世界环境中限制了它们的效果。为了克服这一局限性，需要一个能够应对多种失真类型和严重程度的综合性解决方案。

**Method:** 本文提出了SurgVisAgent，一种基于多模态大规模语言模型的端到端智能手术视觉代理。SurgVisAgent能够识别内窥镜图像中的畸变类别和严重程度，并实现低光增强、过曝校正、运动模糊消除和烟雾去除等增强任务。为了实现对手术场景的深入理解，设计了一个提供领域特定知识的先验模型。此外，通过上下文中的少量学习和链式思维推理，SurgVisAgent能够针对不同类型的畸变和严重程度提供定制化的图像增强。

**Result:** 通过构建一个模拟现实世界手术失真的全面基准，实验结果表明，SurgVisAgent超越了传统的单一任务模型，展示了它作为统一的手术辅助方案的潜力。

**Conclusion:** SurgVisAgent能够应对手术过程中遇到的各种图像失真问题，提供统一的解决方案，相较于传统单一任务模型具有显著优势。这为提升手术过程中的图像质量提供了强有力的支持，进而增强手术成功率和安全性。

**Abstract:** Precise surgical interventions are vital to patient safety, and advanced
enhancement algorithms have been developed to assist surgeons in
decision-making. Despite significant progress, these algorithms are typically
designed for single tasks in specific scenarios, limiting their effectiveness
in complex real-world situations. To address this limitation, we propose
SurgVisAgent, an end-to-end intelligent surgical vision agent built on
multimodal large language models (MLLMs). SurgVisAgent dynamically identifies
distortion categories and severity levels in endoscopic images, enabling it to
perform a variety of enhancement tasks such as low-light enhancement,
overexposure correction, motion blur elimination, and smoke removal.
Specifically, to achieve superior surgical scenario understanding, we design a
prior model that provides domain-specific knowledge. Additionally, through
in-context few-shot learning and chain-of-thought (CoT) reasoning, SurgVisAgent
delivers customized image enhancements tailored to a wide range of distortion
types and severity levels, thereby addressing the diverse requirements of
surgeons. Furthermore, we construct a comprehensive benchmark simulating
real-world surgical distortions, on which extensive experiments demonstrate
that SurgVisAgent surpasses traditional single-task models, highlighting its
potential as a unified solution for surgical assistance.

</details>


### [23] [Multi-Label Classification Framework for Hurricane Damage Assessment](https://arxiv.org/abs/2507.02265)
*Zhangding Liu,Neda Mohammadi,John E. Taylor*

Main category: cs.CV

> 研究提出了一种基于多标签分类框架的飓风损害评估方法，使用航空影像，达到了更好的评估准确性。

<details>
  <summary>Details</summary>

**Motivation:** 传统的单标签分类方法无法捕捉到飓风过后损害的复杂性。因此，需要一种新的方法来及时准确地对损害进行评估，以有效地进行灾害应对。

**Method:** 此研究介绍了一种新型的多标签分类框架，用于利用航空影像评估飓风后的损害。该方法结合了基于ResNet的特征提取模块和特定类别的注意机制，以识别单个图像中的多种损害类型。

**Result:** 使用Michael飓风的Rescuenet数据集，所提出的方法达到了90.23%的平均精度，优于现有的基线方法。

**Conclusion:** 该框架增强了飓风后的损害评估，使其更具针对性和效率，并为灾害预防和恢复策略提出贡献。

**Abstract:** Hurricanes cause widespread destruction, resulting in diverse damage types
and severities that require timely and accurate assessment for effective
disaster response. While traditional single-label classification methods fall
short of capturing the complexity of post-hurricane damage, this study
introduces a novel multi-label classification framework for assessing damage
using aerial imagery. The proposed approach integrates a feature extraction
module based on ResNet and a class-specific attention mechanism to identify
multiple damage types within a single image. Using the Rescuenet dataset from
Hurricane Michael, the proposed method achieves a mean average precision of
90.23%, outperforming existing baseline methods. This framework enhances
post-hurricane damage assessment, enabling more targeted and efficient disaster
response and contributing to future strategies for disaster mitigation and
resilience. This paper has been accepted at the ASCE International Conference
on Computing in Civil Engineering (i3CE 2025), and the camera-ready version
will appear in the official conference proceedings.

</details>


### [24] [Cross-domain Hyperspectral Image Classification based on Bi-directional Domain Adaptation](https://arxiv.org/abs/2507.02268)
*Yuxiang Zhang,Wei Li,Wen Jia,Mengmeng Zhang,Ran Tao,Shunlin Liang*

Main category: cs.CV

> 本研究提出了一种名为BiDA的框架，用于提高跨域高光谱图像分类的适应性和可分离性，在多个实验中表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 高光谱遥感技术可用于土地覆盖类别提取，但是不同地区和时间的图像存在显著的光谱漂移，因此本研究旨在解决跨域场景下的这一挑战。

**Method:** 提出了一种双向领域自适应（BiDA）框架，用于跨域高光谱图像分类，专注于提取源域和目标域的领域不变性和特定领域信息。框架包括三分支Transformer架构（源分支、目标分支和耦合分支），设计了耦合多头交叉注意力机制和双向蒸馏损失，最后提出了适应性增强策略（ARS）。

**Result:** 实验结果表明，该BiDA框架在跨时/场景的航空和卫星数据集上显著优于一些最先进的领域自适应方法。在跨时树木分类任务中，BiDA比最先进方法高3%到5%。

**Conclusion:** 研究结果表明，BiDA框架相比现有方法在跨域高光谱图像分类任务中实现了更优异的性能，尤其在跨时树木分类任务中显示出显著优势。

**Abstract:** Utilizing hyperspectral remote sensing technology enables the extraction of
fine-grained land cover classes. Typically, satellite or airborne images used
for training and testing are acquired from different regions or times, where
the same class has significant spectral shifts in different scenes. In this
paper, we propose a Bi-directional Domain Adaptation (BiDA) framework for
cross-domain hyperspectral image (HSI) classification, which focuses on
extracting both domain-invariant features and domain-specific information in
the independent adaptive space, thereby enhancing the adaptability and
separability to the target scene. In the proposed BiDA, a triple-branch
transformer architecture (the source branch, target branch, and coupled branch)
with semantic tokenizer is designed as the backbone. Specifically, the source
branch and target branch independently learn the adaptive space of source and
target domains, a Coupled Multi-head Cross-attention (CMCA) mechanism is
developed in coupled branch for feature interaction and inter-domain
correlation mining. Furthermore, a bi-directional distillation loss is designed
to guide adaptive space learning using inter-domain correlation. Finally, we
propose an Adaptive Reinforcement Strategy (ARS) to encourage the model to
focus on specific generalized feature extraction within both source and target
scenes in noise condition. Experimental results on cross-temporal/scene
airborne and satellite datasets demonstrate that the proposed BiDA performs
significantly better than some state-of-the-art domain adaptation approaches.
In the cross-temporal tree species classification task, the proposed BiDA is
more than 3\%$\sim$5\% higher than the most advanced method. The codes will be
available from the website:
https://github.com/YuxiangZhang-BIT/IEEE_TCSVT_BiDA.

</details>


### [25] [MAC-Lookup: Multi-Axis Conditional Lookup Model for Underwater Image Enhancement](https://arxiv.org/abs/2507.02270)
*Fanghai Yi,Zehong Zheng,Zexiao Liang,Yihang Dong,Xiyang Fang,Wangyu Wu,Xuhang Chen*

Main category: cs.CV

> 文章提出了MAC-Lookup模型以解决水下图像的能见度和颜色问题，通过条件3D查找表色彩校正和多轴自适应增强技术提升了图像质量。

<details>
  <summary>Details</summary>

**Motivation:** 水下图像由于光照变化、水体浑浊和气泡等原因存在能见度和颜色问题，传统的基于先验和像素的方法往往失效，而深度学习则缺乏高质量的数据集。

**Method:** 引入了MAC-Lookup模型，该模型包含条件3D查找表色彩校正（CLTCC）和多轴自适应增强（MAAE），以提高水下图像的颜色准确性、清晰度和对比度。

**Result:** 实验结果表明，MAC-Lookup模型在恢复水下图像细节和颜色方面优于现有方法。

**Conclusion:** MAC-Lookup模型不仅能提升水下图像质量，还能有效避免过度增强和饱和度问题。

**Abstract:** Enhancing underwater images is crucial for exploration. These images face
visibility and color issues due to light changes, water turbidity, and bubbles.
Traditional prior-based methods and pixel-based methods often fail, while deep
learning lacks sufficient high-quality datasets. We introduce the Multi-Axis
Conditional Lookup (MAC-Lookup) model, which enhances visual quality by
improving color accuracy, sharpness, and contrast. It includes Conditional 3D
Lookup Table Color Correction (CLTCC) for preliminary color and quality
correction and Multi-Axis Adaptive Enhancement (MAAE) for detail refinement.
This model prevents over-enhancement and saturation while handling underwater
challenges. Extensive experiments show that MAC-Lookup excels in enhancing
underwater images by restoring details and colors better than existing methods.
The code is https://github.com/onlycatdoraemon/MAC-Lookup.

</details>


### [26] [Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation](https://arxiv.org/abs/2507.02271)
*Feizhen Huang,Yu Wu,Yutian Lin,Bo Du*

Main category: cs.CV

> 本文提出了一种自我蒸馏方法来改进视频到音频生成技术，使之能够更好地处理电影语言场景中的部分可见目标问题。

<details>
  <summary>Details</summary>

**Motivation:** 当前方法未能充分考虑电影语言，导致在目标部分可见的情况下表现不佳，因此需要改进。

**Method:** 通过模拟电影语言变化，训练模型使视频和音频特征对齐，进而捕捉声音与部分视觉信息之间的关联。

**Result:** 通过提出一种简单的自我蒸馏方法，该方法能够扩展V2A模型以适应电影语言场景，从而当目标仅部分可见时改进音频生成性能。实验表明该方法在所有评估指标中均取得显著改进，并提高了在大规模V2A数据集VGGSound上的性能。

**Conclusion:** 该研究提出了新的自我蒸馏方法，有效提升了在处理部分可见目标时的视频到音频生成质量，并在大规模数据集上表现优异。

**Abstract:** Video-to-Audio (V2A) Generation achieves significant progress and plays a
crucial role in film and video post-production. However, current methods
overlook the cinematic language, a critical component of artistic expression in
filmmaking. As a result, their performance deteriorates in scenarios where
Foley targets are only partially visible. To address this challenge, we propose
a simple self-distillation approach to extend V2A models to cinematic language
scenarios. By simulating the cinematic language variations, the student model
learns to align the video features of training pairs with the same audio-visual
correspondences, enabling it to effectively capture the associations between
sounds and partial visual information. Our method not only achieves impressive
improvements under partial visibility across all evaluation metrics, but also
enhances performance on the large-scale V2A dataset, VGGSound.

</details>


### [27] [LaCo: Efficient Layer-wise Compression of Visual Tokens for Multimodal Large Language Models](https://arxiv.org/abs/2507.02279)
*Juntao Liu,Liqiang Niu,Wenchao Chen,Jie Zhou,Fandong Meng*

Main category: cs.CV

> LaCo is a new framework allowing token compression in intermediate layers of vision encoders, offering better performance and efficiency compared to existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind LaCo is to overcome the limitation of existing visual token compression methods, which predominantly operate as post-encoder modules. The objective is to enable more efficient token compression that can lead to significant gains in training and inference efficiency.

**Method:** LaCo (Layer-wise Visual Token Compression) proposes a novel framework for performing token compression within the intermediate layers of a vision encoder. It introduces two core components: a layer-wise pixel-shuffle mechanism and a residual learning architecture with non-parametric shortcuts.

**Result:** LaCo outperforms existing methods in token compression within the intermediate layers of vision encoders, achieving superior compression efficiency and preserving strong performance in both training and inference.

**Conclusion:** The study concludes that LaCo provides a significant improvement in visual token compression, leading to better efficiency gains and performance in MLLMs (Multimodal Large Language Models).

**Abstract:** Existing visual token compression methods for Multimodal Large Language
Models (MLLMs) predominantly operate as post-encoder modules, limiting their
potential for efficiency gains. To address this limitation, we propose LaCo
(Layer-wise Visual Token Compression), a novel framework that enables effective
token compression within the intermediate layers of the vision encoder. LaCo
introduces two core components: 1) a layer-wise pixel-shuffle mechanism that
systematically merges adjacent tokens through space-to-channel transformations,
and 2) a residual learning architecture with non-parametric shortcuts that
preserves critical visual information during compression. Extensive experiments
indicate that our LaCo outperforms all existing methods when compressing tokens
in the intermediate layers of the vision encoder, demonstrating superior
effectiveness. In addition, compared to external compression, our method
improves training efficiency beyond 20% and inference throughput over 15% while
maintaining strong performance.

</details>


### [28] [Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization](https://arxiv.org/abs/2507.02288)
*De Cheng,Zhipeng Xu,Xinyang Jiang,Dongsheng Li,Nannan Wang,Xinbo Gao*

Main category: cs.CV

> 我们提出了一种基于文本引导的视觉领域适应方法，通过解耦文本特征来指导视觉特征的学习，并引入WERA方法来增强视觉表征的一致性，实验结果表明我们的方法优于现有的领域泛化方法。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有的视觉基础模型在领域泛化中表现出色，但设计能够解耦不同领域中不变特征的提示仍旧是一个关键挑战，因此我们提出了基于文本特征指导的视觉提示调优框架来解决这一问题。

**Method:** 我们提出了一种基于文本特征引导的视觉提示调优框架，通过大语言模型自动解耦文本提示，并以解耦的文本特征为引导学习领域不变的视觉表示。为克服仅依赖语言指导视觉特征解耦的局限性，我们还提出了WERA方法，通过使用抽象提示和风格化图像增强来提高原领域和增强后的分布之间的一致性。

**Result:** 实验结果表明，我们在PACS、VLCS、OfficeHome、DomainNet和TerraInc等主要领域泛化数据集上的表现优于现有的领域泛化方法。

**Conclusion:** 通过引入基于大语言模型的解耦文本特征引导的方法和WERA机制，我们的方法在领域泛化任务中展示了超越现有方法的表现，为设计领域不变的视觉模型提供了新的视角。

**Abstract:** Domain Generalization (DG) seeks to develop a versatile model capable of
performing effectively on unseen target domains. Notably, recent advances in
pre-trained Visual Foundation Models (VFMs), such as CLIP, have demonstrated
considerable potential in enhancing the generalization capabilities of deep
learning models. Despite the increasing attention toward VFM-based domain
prompt tuning within DG, the effective design of prompts capable of
disentangling invariant features across diverse domains remains a critical
challenge. In this paper, we propose addressing this challenge by leveraging
the controllable and flexible language prompt of the VFM. Noting that the text
modality of VFMs is naturally easier to disentangle, we introduce a novel
framework for text feature-guided visual prompt tuning. This framework first
automatically disentangles the text prompt using a large language model (LLM)
and then learns domain-invariant visual representation guided by the
disentangled text feature. However, relying solely on language to guide visual
feature disentanglement has limitations, as visual features can sometimes be
too complex or nuanced to be fully captured by descriptive text. To address
this, we introduce Worst Explicit Representation Alignment (WERA), which
extends text-guided visual prompts by incorporating an additional set of
abstract prompts. These prompts enhance source domain diversity through
stylized image augmentations, while alignment constraints ensure that visual
representations remain consistent across both the original and augmented
distributions. Experiments conducted on major DG datasets, including PACS,
VLCS, OfficeHome, DomainNet, and TerraInc, demonstrate that our proposed method
outperforms state-of-the-art DG methods.

</details>


### [29] [ViRefSAM: Visual Reference-Guided Segment Anything Model for Remote Sensing Segmentation](https://arxiv.org/abs/2507.02294)
*Hanbo Bi,Yulong Xu,Ya Li,Yongqiang Mao,Boyuan Tong,Chongyang Li,Chunbo Lang,Wenhui Diao,Hongqi Wang,Yingchao Feng,Xian Sun*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** The Segment Anything Model (SAM), with its prompt-driven paradigm, exhibits
strong generalization in generic segmentation tasks. However, applying SAM to
remote sensing (RS) images still faces two major challenges. First, manually
constructing precise prompts for each image (e.g., points or boxes) is
labor-intensive and inefficient, especially in RS scenarios with dense small
objects or spatially fragmented distributions. Second, SAM lacks domain
adaptability, as it is pre-trained primarily on natural images and struggles to
capture RS-specific semantics and spatial characteristics, especially when
segmenting novel or unseen classes. To address these issues, inspired by
few-shot learning, we propose ViRefSAM, a novel framework that guides SAM
utilizing only a few annotated reference images that contain class-specific
objects. Without requiring manual prompts, ViRefSAM enables automatic
segmentation of class-consistent objects across RS images. Specifically,
ViRefSAM introduces two key components while keeping SAM's original
architecture intact: (1) a Visual Contextual Prompt Encoder that extracts
class-specific semantic clues from reference images and generates object-aware
prompts via contextual interaction with target images; and (2) a Dynamic Target
Alignment Adapter, integrated into SAM's image encoder, which mitigates the
domain gap by injecting class-specific semantics into target image features,
enabling SAM to dynamically focus on task-relevant regions. Extensive
experiments on three few-shot segmentation benchmarks, including iSAID-5$^i$,
LoveDA-2$^i$, and COCO-20$^i$, demonstrate that ViRefSAM enables accurate and
automatic segmentation of unseen classes by leveraging only a few reference
images and consistently outperforms existing few-shot segmentation methods
across diverse datasets.

</details>
