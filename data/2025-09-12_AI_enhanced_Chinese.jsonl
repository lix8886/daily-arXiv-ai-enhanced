{"id": "2509.08903", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08903", "abs": "https://arxiv.org/abs/2509.08903", "authors": ["Alex Clay", "Ernesto Jiménez-Ruiz", "Pranava Madhyastha"], "title": "Noise or Nuance: An Investigation Into Useful Information and Filtering For LLM Driven AKBC", "comment": "8 pages, 1 figure, accepted to the ISWC 2025 LM-KBC Workshop", "summary": "RAG and fine-tuning are prevalent strategies for improving the quality of LLM\noutputs. However, in constrained situations, such as that of the 2025 LM-KBC\nchallenge, such techniques are restricted. In this work we investigate three\nfacets of the triple completion task: generation, quality assurance, and LLM\nresponse parsing. Our work finds that in this constrained setting: additional\ninformation improves generation quality, LLMs can be effective at filtering\npoor quality triples, and the tradeoff between flexibility and consistency with\nLLM response parsing is setting dependent.", "AI": {"tldr": "研究探讨了受限环境下三元组补全任务，发现额外信息提升生成质量，大语言模型有效过滤低质量三元组，且灵活性与一致性权衡依赖设置。", "motivation": "探讨在受限环境下，如2025年LM-KBC挑战中，RAG和微调策略受限时，如何改进大语言模型输出质量。", "method": "本研究探讨了三元组补全任务中的三个方面：生成、质量保证和大语言模型响应解析。研究在受限环境下进行。", "result": "研究发现：额外信息可以提升生成质量；大语言模型可以有效过滤低质量的三元组；在解析大语言模型响应时，灵活性与一致性之间的权衡取决于具体设置。", "conclusion": "在受限环境中，额外信息的引入对生成质量有正面影响，大语言模型能用于过滤低质量结果，并且在解析响应时需要考虑灵活性和一致性之间的权衡。"}}
{"id": "2509.08907", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08907", "abs": "https://arxiv.org/abs/2509.08907", "authors": ["Imene Kolli", "Ario Saeid Vaghefi", "Chiara Colesanti Senni", "Shantam Raj", "Markus Leippold"], "title": "Automated Evidence Extraction and Scoring for Corporate Climate Policy Engagement: A Multilingual RAG Approach", "comment": null, "summary": "InfluenceMap's LobbyMap Platform monitors the climate policy engagement of\nover 500 companies and 250 industry associations, assessing each entity's\nsupport or opposition to science-based policy pathways for achieving the Paris\nAgreement's goal of limiting global warming to 1.5{\\deg}C. Although\nInfluenceMap has made progress with automating key elements of the analytical\nworkflow, a significant portion of the assessment remains manual, making it\ntime- and labor-intensive and susceptible to human error. We propose an\nAI-assisted framework to accelerate the monitoring of corporate climate policy\nengagement by leveraging Retrieval-Augmented Generation to automate the most\ntime-intensive extraction of relevant evidence from large-scale textual data.\nOur evaluation shows that a combination of layout-aware parsing, the Nomic\nembedding model, and few-shot prompting strategies yields the best performance\nin extracting and classifying evidence from multilingual corporate documents.\nWe conclude that while the automated RAG system effectively accelerates\nevidence extraction, the nuanced nature of the analysis necessitates a\nhuman-in-the-loop approach where the technology augments, rather than replaces,\nexpert judgment to ensure accuracy.", "AI": {"tldr": "本文提出了一种AI辅助的框架，利用检索增强生成技术加快企业气候政策参与情况的监测。评估表明，结合不同的技术组件可以获得最佳的提取和分类效果，而人机交互方法可以确保更高的准确性。", "motivation": "尽管InfluenceMap在自动化分析工作流程的关键部分上取得了显著进展，但仍有一大部分评估工作是手动完成的，这使得该过程耗费时间且容易出现人为错误。我们的目标就是通过AI辅助加快企业气候政策参与监测的过程。", "method": "我们提出了一种基于检索增强生成（Retrieval-Augmented Generation, RAG）框架的AI辅助系统，通过结合布局感知解析、Nomic嵌入模型和少量样本提示策略，来自动从大规模文本数据中提取相关信息，从而加速企业气候政策参与情况的监测。", "result": "评估结果显示，结合布局感知解析、Nomic嵌入模型和少量样本提示策略的方法在从多语言企业文档中提取和分类证据方面表现最佳。", "conclusion": "虽然自动化的RAG系统有效地加快了证据提取的速度，但由于分析的细微特性，仍需要人机交互的方法，即技术应该辅助而不是取代专家判断以确保准确性。"}}
{"id": "2509.08920", "categories": ["cs.CL", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.08920", "abs": "https://arxiv.org/abs/2509.08920", "authors": ["Jinsong Chen"], "title": "Documents Are People and Words Are Items: A Psychometric Approach to Textual Data with Contextual Embeddings", "comment": null, "summary": "This research introduces a novel psychometric method for analyzing textual\ndata using large language models. By leveraging contextual embeddings to create\ncontextual scores, we transform textual data into response data suitable for\npsychometric analysis. Treating documents as individuals and words as items,\nthis approach provides a natural psychometric interpretation under the\nassumption that certain keywords, whose contextual meanings vary significantly\nacross documents, can effectively differentiate documents within a corpus. The\nmodeling process comprises two stages: obtaining contextual scores and\nperforming psychometric analysis. In the first stage, we utilize natural\nlanguage processing techniques and encoder based transformer models to identify\ncommon keywords and generate contextual scores. In the second stage, we employ\nvarious types of factor analysis, including exploratory and bifactor models, to\nextract and define latent factors, determine factor correlations, and identify\nthe most significant words associated with each factor. Applied to the Wiki\nSTEM corpus, our experimental results demonstrate the method's potential to\nuncover latent knowledge dimensions and patterns within textual data. This\napproach not only enhances the psychometric analysis of textual data but also\nholds promise for applications in fields rich in textual information, such as\neducation, psychology, and law.", "AI": {"tldr": "A new psychometric method using large language models transforms textual data into response data for analysis, uncovering latent knowledge and patterns.", "motivation": "To enhance the psychometric analysis of textual data and to potentially apply this method in fields rich in textual information such as education, psychology, and law.", "method": "A novel psychometric method utilizes large language models to analyze textual data by creating contextual embeddings, which are then used to generate contextual scores suitable for psychometric analysis.", "result": "The experimental results on the Wiki STEM corpus demonstrate the method's capability to uncover latent knowledge dimensions and patterns within textual data.", "conclusion": "The introduced method not only advances the analysis of textual data but also opens up new possibilities for various fields."}}
{"id": "2509.08960", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08960", "abs": "https://arxiv.org/abs/2509.08960", "authors": ["Thales Sales Almeida", "Giovana Kerche Bonás", "João Guilherme Alves Santos"], "title": "BRoverbs -- Measuring how much LLMs understand Portuguese proverbs", "comment": null, "summary": "Large Language Models (LLMs) exhibit significant performance variations\ndepending on the linguistic and cultural context in which they are applied.\nThis disparity signals the necessity of mature evaluation frameworks that can\nassess their capabilities in specific regional settings. In the case of\nPortuguese, existing evaluations remain limited, often relying on translated\ndatasets that may not fully capture linguistic nuances or cultural references.\nMeanwhile, native Portuguese-language datasets predominantly focus on\nstructured national exams or sentiment analysis of social media interactions,\nleaving gaps in evaluating broader linguistic understanding. To address this\nlimitation, we introduce BRoverbs, a dataset specifically designed to assess\nLLM performance through Brazilian proverbs. Proverbs serve as a rich linguistic\nresource, encapsulating cultural wisdom, figurative expressions, and complex\nsyntactic structures that challenge the model comprehension of regional\nexpressions. BRoverbs aims to provide a new evaluation tool for\nPortuguese-language LLMs, contributing to advancing regionally informed\nbenchmarking. The benchmark is available at\nhttps://huggingface.co/datasets/Tropic-AI/BRoverbs.", "AI": {"tldr": "提出BRoverbs数据集，专注于评估大型语言模型处理巴西葡萄牙语区域表达（谚语）的能力。", "motivation": "大型语言模型在不同的语言和文化背景下表现出显著的性能差异。针对葡萄牙语，现有的评估方法往往依赖于翻译的数据集，这可能无法完全捕捉到语言细微差别或文化参考。", "method": "介绍了一种名为BRoverbs的数据集，专门用于评估大型语言模型在理解和解释巴西谚语方面的能力。该数据集旨在填补现有评估方法中对葡萄牙语地区表达理解能力评价不足的空白。", "result": "BRoverbs数据集的提出为评估葡萄牙语大型语言模型提供了一个新的评价工具。", "conclusion": "BRoverbs有助于推动地方信息导向的基准测试的发展，提供了评估葡萄牙语大型语言模型在理解和使用区域表达方面的能力。"}}
{"id": "2509.08897", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.08897", "abs": "https://arxiv.org/abs/2509.08897", "authors": ["Davide Caffagni", "Sara Sarto", "Marcella Cornia", "Lorenzo Baraldi", "Rita Cucchiara"], "title": "Recurrence Meets Transformers for Universal Multimodal Retrieval", "comment": null, "summary": "With the rapid advancement of multimodal retrieval and its application in\nLLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged.\nExisting methods predominantly rely on task-specific fine-tuning of\nvision-language models and are limited to single-modality queries or documents.\nIn this paper, we propose ReT-2, a unified retrieval model that supports\nmultimodal queries, composed of both images and text, and searches across\nmultimodal document collections where text and images coexist. ReT-2 leverages\nmulti-layer representations and a recurrent Transformer architecture with\nLSTM-inspired gating mechanisms to dynamically integrate information across\nlayers and modalities, capturing fine-grained visual and textual details. We\nevaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different\nretrieval configurations. Results demonstrate that ReT-2 consistently achieves\nstate-of-the-art performance across diverse settings, while offering faster\ninference and reduced memory usage compared to prior approaches. When\nintegrated into retrieval-augmented generation pipelines, ReT-2 also improves\ndownstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source\ncode and trained models are publicly available at:\nhttps://github.com/aimagelab/ReT-2", "AI": {"tldr": "提出ReT-2模型用于支持多模态查询的统一检索，展示在多种设置中拥有最先进的性能。", "motivation": "当前方法主要依赖于视觉语言模型的任务特定微调，限制于单模态查询或文档。为应对日益复杂的检索任务，该论文提出ReT-2，一个支持多模态查询与检索的统一模型。", "method": "ReT-2使用多层表示和带有LSTM启发式门控机制的循环Transformer架构，动态整合多模式信息，捕捉细腻的视觉和文本细节。", "result": "ReT-2在具有挑战性的M2KR和M-BEIR基准测试中的表现优于前期模型，并提高在Encyclopedic-VQA和InfoSeek数据集上的性能。", "conclusion": "实验结果显示，ReT-2在不同的检索配置下一致表现出最先进的性能，并且相较于之前的方法，提供了更快的推理速度和减少的内存使用量。当集成到检索增强生成管道中时，ReT-2还能提高在Encyclopedic-VQA和InfoSeek数据集的下游性能。"}}
{"id": "2509.09013", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09013", "abs": "https://arxiv.org/abs/2509.09013", "authors": ["Monjoy Narayan Choudhury", "Junling Wang", "Yifan Hou", "Mrinmaya Sachan"], "title": "Can Vision-Language Models Solve Visual Math Equations?", "comment": "Monjoy Narayan Choudhury and Junling Wang contributed equally to this\n  work. Accepted at EMNLP2025 main. Code and datasets are open-sourced with\n  links in the paper", "summary": "Despite strong performance in visual understanding and language-based\nreasoning, Vision-Language Models (VLMs) struggle with tasks requiring\nintegrated perception and symbolic computation. We study this limitation\nthrough visual equation solving, where mathematical equations are embedded in\nimages, variables are represented by object icons, and coefficients must be\ninferred by counting. While VLMs perform well on textual equations, they fail\non visually grounded counterparts. To understand this gap, we decompose the\ntask into coefficient counting and variable recognition, and find that counting\nis the primary bottleneck, even when recognition is accurate. We also observe\nthat composing recognition and reasoning introduces additional errors,\nhighlighting challenges in multi-step visual reasoning. Finally, as equation\ncomplexity increases, symbolic reasoning itself becomes a limiting factor.\nThese findings reveal key weaknesses in current VLMs and point toward future\nimprovements in visually grounded mathematical reasoning.", "AI": {"tldr": "研究表明，视觉-语言模型在处理视觉嵌入方程时面临挑战，主要问题在于系数计数和符号推理。", "motivation": "研究视觉-语言模型在需要集成感知和符号计算的任务上的局限性，特别是通过视觉方程求解任务来理解这些局限性。", "method": "通过视觉方程求解来研究视觉-语言模型在集成感知和符号计算方面的限制，特别是在将数学方程嵌入图像，变量由对象图标表示，系数需要通过计数推断的情况下。", "result": "发现视觉-语言模型在视觉嵌入的方程上表现不佳，尤其是在系数计数方面存在问题，即使变量识别是准确的。随着方程复杂性的增加，符号推理本身也成为一个限制因素。", "conclusion": "这些发现揭示了当前视觉-语言模型的关键弱点，并指出了未来在视觉嵌入数学推理方面改进的方向。"}}
{"id": "2509.08908", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.08908", "abs": "https://arxiv.org/abs/2509.08908", "authors": ["Rogerio Guimaraes", "Frank Xiao", "Pietro Perona", "Markus Marks"], "title": "Diffusion-Based Action Recognition Generalizes to Untrained Domains", "comment": null, "summary": "Humans can recognize the same actions despite large context and viewpoint\nvariations, such as differences between species (walking in spiders vs.\nhorses), viewpoints (egocentric vs. third-person), and contexts (real life vs\nmovies). Current deep learning models struggle with such generalization. We\npropose using features generated by a Vision Diffusion Model (VDM), aggregated\nvia a transformer, to achieve human-like action recognition across these\nchallenging conditions. We find that generalization is enhanced by the use of a\nmodel conditioned on earlier timesteps of the diffusion process to highlight\nsemantic information over pixel level details in the extracted features. We\nexperimentally explore the generalization properties of our approach in\nclassifying actions across animal species, across different viewing angles, and\ndifferent recording contexts. Our model sets a new state-of-the-art across all\nthree generalization benchmarks, bringing machine action recognition closer to\nhuman-like robustness. Project page:\n$\\href{https://www.vision.caltech.edu/actiondiff/}{\\texttt{vision.caltech.edu/actiondiff}}$\nCode:\n$\\href{https://github.com/frankyaoxiao/ActionDiff}{\\texttt{github.com/frankyaoxiao/ActionDiff}}$", "AI": {"tldr": "研究提出了一种新的动作识别方法，通过视觉扩散模型和Transformer实现跨物种、视角和场景的人类般泛化能力。", "motivation": "当前的深度学习模型在处理动作识别的泛化问题时表现不佳。研究动机在于实现机器能在面对物种、视角和场景变化时具有更强的动作识别能力，以达到人类般的稳健性。", "method": "使用由视觉扩散模型（VDM）生成的特征并通过Transformer进行聚合，以实现跨不同物种、视角和场景的人类般动作识别。该方法通过在扩散过程的早期时间步长上条件化的模型来增强泛化能力，突出在提取的特征中的语义信息，而非像素细节。", "result": "该模型在跨物种、不同视角和不同录制场景的动作分类泛化性能基准测试中达到了新的最佳水平，显著提升了机器动作识别的稳健性。", "conclusion": "该方法证明了机器在面对广泛的上下文和视角变化时，能够实现更稳健的动作识别能力，使得机器动作识别更接近人类的表现。"}}
{"id": "2509.09043", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.09043", "abs": "https://arxiv.org/abs/2509.09043", "authors": ["Thomas Manuel Rost", "Martina Figlia", "Bernd Wallraff"], "title": "Stated Preference for Interaction and Continued Engagement (SPICE): Evaluating an LLM's Willingness to Re-engage in Conversation", "comment": null, "summary": "We introduce and evaluate Stated Preference for Interaction and Continued\nEngagement (SPICE), a simple diagnostic signal elicited by asking a Large\nLanguage Model a YES or NO question about its willingness to re-engage with a\nuser's behavior after reviewing a short transcript. In a study using a 3-tone\n(friendly, unclear, abusive) by 10-interaction stimulus set, we tested four\nopen-weight chat models across four framing conditions, resulting in 480\ntrials. Our findings show that SPICE sharply discriminates by user tone.\nFriendly interactions yielded a near-unanimous preference to continue (97.5%\nYES), while abusive interactions yielded a strong preference to discontinue\n(17.9% YES), with unclear interactions falling in between (60.4% YES). This\ncore association remains decisive under multiple dependence-aware statistical\ntests, including Rao-Scott adjustment and cluster permutation tests.\nFurthermore, we demonstrate that SPICE provides a distinct signal from abuse\nclassification. In trials where a model failed to identify abuse, it still\noverwhelmingly stated a preference not to continue the interaction (81% of the\ntime). An exploratory analysis also reveals a significant interaction effect: a\npreamble describing the study context significantly impacts SPICE under\nambiguity, but only when transcripts are presented as a single block of text\nrather than a multi-turn chat. The results validate SPICE as a robust,\nlow-overhead, and reproducible tool for auditing model dispositions,\ncomplementing existing metrics by offering a direct, relational signal of a\nmodel's state. All stimuli, code, and analysis scripts are released to support\nreplication.", "AI": {"tldr": "SPICE是一种通过询问大型语言模型是否愿意在审阅简短对话记录后重新与用户互动的方法，用于检测模型对不同用户语气的反应。测试表明，SPICE能有效区分友好、不明确和恶意的用户，能够提供一个直接的模型态度信号。", "motivation": "为了开发一个简单且有效的方法来检测大型语言模型对用户行为的反应，引入了SPICE方法。", "method": "Structure", "result": "{\"tldr\": \"SPICE是一种通过询问大型语言模型是否愿意在审阅简短对话记录后重新与用户互动的方法，用于检测模型对不同用户语气的反应。测试表明，SPICE能有效区分友好、不明确和恶意的用户，能够提供一个直接的模型态度信号。\", \"motivation\": \"为了开发一个简单且有效的方法来检测大型语言模型对用户行为的反应，引入了SPICE方法。\", \"method\": \"研究使用了四种开放权重的聊天模型，在三种语气（友好、不清楚、恶意）和十种互动刺激条件下进行了测试。\", \"result\": \"测试结果表明，模型对友好互动的继续倾向性为97.5%，对恶意互动的继续倾向性为17.9%，对不清不楚的互动继续倾向性为60.4%。此外，即使在模型未能识别恶意行为时，它依然有81%的时间表达了不愿继续互动的倾向。\", \"conclusion\": \"SPICE方法被验证为一个强大的、低开销且可复制的工具，用于评估模型的倾向性，为现有的度量标准提供了互补的直接关系信号。\"}", "conclusion": "SPICE方法被验证为一个强大的、低开销且可复制的工具，用于评估模型的倾向性，为现有的度量标准提供了互补的直接关系信号。"}}
{"id": "2509.08910", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08910", "abs": "https://arxiv.org/abs/2509.08910", "authors": ["Tung Vu", "Lam Nguyen", "Quynh Dao"], "title": "PromptGuard: An Orchestrated Prompting Framework for Principled Synthetic Text Generation for Vulnerable Populations using LLMs with Enhanced Safety, Fairness, and Controllability", "comment": null, "summary": "The proliferation of Large Language Models (LLMs) in real-world applications\nposes unprecedented risks of generating harmful, biased, or misleading\ninformation to vulnerable populations including LGBTQ+ individuals, single\nparents, and marginalized communities. While existing safety approaches rely on\npost-hoc filtering or generic alignment techniques, they fail to proactively\nprevent harmful outputs at the generation source. This paper introduces\nPromptGuard, a novel modular prompting framework with our breakthrough\ncontribution: VulnGuard Prompt, a hybrid technique that prevents harmful\ninformation generation using real-world data-driven contrastive learning.\nVulnGuard integrates few-shot examples from curated GitHub repositories,\nethical chain-of-thought reasoning, and adaptive role-prompting to create\npopulation-specific protective barriers. Our framework employs theoretical\nmulti-objective optimization with formal proofs demonstrating 25-30% analytical\nharm reduction through entropy bounds and Pareto optimality. PromptGuard\norchestrates six core modules: Input Classification, VulnGuard Prompting,\nEthical Principles Integration, External Tool Interaction, Output Validation,\nand User-System Interaction, creating an intelligent expert system for\nreal-time harm prevention. We provide comprehensive mathematical formalization\nincluding convergence proofs, vulnerability analysis using information theory,\nand theoretical validation framework using GitHub-sourced datasets,\nestablishing mathematical foundations for systematic empirical research.", "AI": {"tldr": "文章提出了一种名为PromptGuard的新型模块化框架，通过VulnGuard提示技术主动阻止大型语言模型生成有害信息。", "motivation": "随着大型语言模型在实际应用中的普及，其生成有害、偏见或误导信息的风险对于LGBTQ+个体、单亲家庭和边缘化社区等脆弱群体构成了前所未有的威胁。现有安全方法侧重于事后过滤或泛化对齐技术，但未能从根本上防止有害输出的产生。", "method": "该论文提出了一种名为PromptGuard的新颖的模块化提示框架，其核心贡献是VulnGuard提示技术，这是一种基于真实数据驱动对比学习的混合技术，旨在从生成源头防止有害信息的产生。VulnGuard结合了少样本例子、道德推理链和自适应角色提示，形成针对特定人群的保护屏障。", "result": "通过理论上的多目标优化，该论文展示了其方法可以将分析的有害信息减少25-30%。", "conclusion": "PromptGuard框架由六个核心模块组成，包括输入分类、VulnGuard提示、道德原则融合、外部工具交互、输出验证和用户-系统交互，形成一个实时防止有害信息产生的智能化专家系统。"}}
{"id": "2509.09055", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09055", "abs": "https://arxiv.org/abs/2509.09055", "authors": ["Piyush Pant"], "title": "Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M", "comment": "17 pages, 3 figures. Code and dataset available at\n  https://github.com/PiyushWithPant/Improving-LLM-Safety-and-Helpfulness-using-SFT-and-DPO", "summary": "This research investigates the effectiveness of alignment techniques,\nSupervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a\ncombined SFT+DPO approach on improving the safety and helpfulness of the\nOPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset,\nwe train and evaluate four models: the base OPT350M, an SFT model, a DPO model,\nand a model trained with both SFT and DPO. We introduce three key evaluation\nmetrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined\nAlignment Score (CAS), all derived from reward model outputs. The results show\nthat while SFT outperforms DPO, The combined SFT+DPO model outperforms all\nothers across all metrics, demonstrating the complementary nature of these\ntechniques. Our findings also highlight challenges posed by noisy data, limited\nGPU resources, and training constraints. This study offers a comprehensive view\nof how fine-tuning strategies affect model alignment and provides a foundation\nfor more robust alignment pipelines in future work.", "AI": {"tldr": "研究结果表明，结合SFT和DPO的模型在提升语言模型的安全性和有用性方面表现最佳，优于单独使用SFT或DPO的技术。", "motivation": "这项研究的主要动机是为了研究对齐技术（包括监督微调(SFT)、直接偏好优化(DPO)和结合的SFT+DPO方法）在提高OPT-350M语言模型的安全性和有用性方面的影响。", "method": "该研究通过使用Anthropic Helpful-Harmless RLHF数据集，对基础的OPT350M模型、SFT模型、DPO模型以及SFT+DPO结合模型进行了训练和评估。", "result": "结果表明，虽然SFT的表现优于DPO，但结合了SFT+DPO的模型在所有评价指标上都表现最佳，这证明了这些技术互补的特性。此外，研究也指出了噪音数据、有限的GPU资源和训练限制所带来的挑战。", "conclusion": "这项研究表明，结合SFT和DPO技术可以显著提高模型的对齐效果，同时也为未来更加健壮的对齐流程提供了基础和参考。"}}
{"id": "2509.08926", "categories": ["cs.CV", "cs.AI", "cs.LG", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.08926", "abs": "https://arxiv.org/abs/2509.08926", "authors": ["Waqar Ahmad", "Evan Murphy", "Vladimir A. Krylov"], "title": "Similarity-based Outlier Detection for Noisy Object Re-Identification Using Beta Mixtures", "comment": null, "summary": "Object re-identification (Re-ID) methods are highly sensitive to label noise,\nwhich typically leads to significant performance degradation. We address this\nchallenge by reframing Re-ID as a supervised image similarity task and adopting\na Siamese network architecture trained to capture discriminative pairwise\nrelationships. Central to our approach is a novel statistical outlier detection\n(OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier\nDetection), which models the distribution of cosine similarities between\nembedding pairs using a two-component Beta distribution mixture model. We\nestablish a novel identifiability result for mixtures of two Beta\ndistributions, ensuring that our learning task is well-posed.The proposed OD\nstep complements the Re-ID architecture combining binary cross-entropy,\ncontrastive, and cosine embedding losses that jointly optimize feature-level\nsimilarity learning.We demonstrate the effectiveness of Beta-SOD in de-noising\nand Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and\nvehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance\ncompared to the state-of-the-art methods across various noise levels (10-30\\%),\ndemonstrating both robustness and broad applicability in noisy Re-ID scenarios.\nThe implementation of Beta-SOD is available at:\nhttps://github.com/waqar3411/Beta-SOD", "AI": {"tldr": "The paper solves label noise issues in object Re-Identification by introducing Beta-SOD, a statistical framework using Beta distribution mixtures for outlier detection, integrated within a Siamese network trained for context-aware image similarity scoring.", "motivation": "The motivation arises from the vulnerability of Re-ID methods to label noise, leading to a decline in performance. The goal is to develop a method that can handle noisy labels effectively, improving the robustness of Re-ID systems.", "method": "We address label noise sensitivity in object Re-ID by reformulating it as a supervised image similarity task with a Siamese network. The method includes a new statistical outlier detection framework called Beta-SOD, which employs a two-component Beta distribution mixture model to model cosine similarities for distinguishing between inlier and outlier pairs. The Re-ID network is trained with a composite loss that includes binary cross-entropy, contrastive, and cosine embedding loss functions to ensure robust feature-level similarity learning.", "result": "Experimental results on CUHK03, Market-1501, and VeRi-776 datasets prove the effectiveness of Beta-SOD in de-noising and Re-ID tasks, offering performance superiority over state-of-the-art competitors under varying levels of noise.", "conclusion": "The conclusion is that the proposed Beta-SOD framework, integrated into the Re-ID process, provides a robust solution to handling noisy labels, showing enhanced performance across multiple benchmark datasets in scenarios of 10-30% noise contamination."}}
{"id": "2509.09082", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09082", "abs": "https://arxiv.org/abs/2509.09082", "authors": ["Zhongqiu Li", "Shiquan Wang", "Ruiyu Fang", "Mengjiao Bao", "Zhenhe Wu", "Shuangyong Song", "Yongxiang Li", "Zhongjiang He"], "title": "MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for Universal Information Extraction", "comment": null, "summary": "Large language models (LLMs) demonstrate robust capabilities across diverse\nresearch domains. However, their performance in universal information\nextraction (UIE) remains insufficient, especially when tackling structured\noutput scenarios that involve complex schema descriptions and require\nmulti-step reasoning. While existing approaches enhance the performance of LLMs\nthrough in-context learning and instruction tuning, significant limitations\nnonetheless persist. To enhance the model's generalization ability, we propose\nintegrating reinforcement learning (RL) with multi-perspective reasoning for\ninformation extraction (IE) tasks. Our work transitions LLMs from passive\nextractors to active reasoners, enabling them to understand not only what to\nextract but also how to reason. Experiments conducted on multiple IE benchmarks\ndemonstrate that MR-UIE consistently elevates extraction accuracy across\ndomains and surpasses state-of-the-art methods on several datasets.\nFurthermore, incorporating multi-perspective reasoning into RL notably enhances\ngeneralization in complex IE tasks, underscoring the critical role of reasoning\nin challenging scenarios.", "AI": {"tldr": "本文提出了一种将强化学习与多视角推理相结合的方法来提升大语言模型在通用信息抽取任务中的性能和泛化能力。", "motivation": "虽然大语言模型（LLMs）在多个研究领域表现出强大的能力，但在结构化输出场景中，尤其是在复杂的模式描述和多步推理方面，其表现仍然不足。现有方法虽然通过上下文学习和指令调优提升了LLMs的性能，但仍存在明显局限。", "method": "通过将强化学习（RL）与多视角推理结合，改进了LLMs在通用信息抽取（UIE）任务中的泛化能力。", "result": "实验结果显示，MR-UIE在多个IE基准测试中提高了提取准确率，并在几个数据集上超过了当前最佳方法。将多视角推理纳入RL显著提高了复杂IE任务中的泛化能力。", "conclusion": "研究成功地将LLMs从被动的提取器转变为能够理解要提取内容及其推理方式的主动推理者。"}}
{"id": "2509.08934", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.08934", "abs": "https://arxiv.org/abs/2509.08934", "authors": ["Nan Mu", "Ruiqi Song", "Zhihui Xu", "Jingfeng Jiang", "Chen Zhao"], "title": "SFD-Mamba2Net: Strcture-Guided Frequency-Enhanced Dual-Stream Mamba2 Network for Coronary Artery Segmentation", "comment": null, "summary": "Background: Coronary Artery Disease (CAD) is one of the leading causes of\ndeath worldwide. Invasive Coronary Angiography (ICA), regarded as the gold\nstandard for CAD diagnosis, necessitates precise vessel segmentation and\nstenosis detection. However, ICA images are typically characterized by low\ncontrast, high noise levels, and complex, fine-grained vascular structures,\nwhich pose significant challenges to the clinical adoption of existing\nsegmentation and detection methods. Objective: This study aims to improve the\naccuracy of coronary artery segmentation and stenosis detection in ICA images\nby integrating multi-scale structural priors, state-space-based long-range\ndependency modeling, and frequency-domain detail enhancement strategies.\nMethods: We propose SFD-Mamba2Net, an end-to-end framework tailored for\nICA-based vascular segmentation and stenosis detection. In the encoder, a\nCurvature-Aware Structural Enhancement (CASE) module is embedded to leverage\nmulti-scale responses for highlighting slender tubular vascular structures,\nsuppressing background interference, and directing attention toward vascular\nregions. In the decoder, we introduce a Progressive High-Frequency Perception\n(PHFP) module that employs multi-level wavelet decomposition to progressively\nrefine high-frequency details while integrating low-frequency global\nstructures. Results and Conclusions: SFD-Mamba2Net consistently outperformed\nstate-of-the-art methods across eight segmentation metrics, and achieved the\nhighest true positive rate and positive predictive value in stenosis detection.", "AI": {"tldr": "提出SFD-Mamba2Net框架改进冠状动脉疾病的诊断图像处理，提高了血管分割和狭窄检测的准确率。", "motivation": "为了提高冠状动脉疾病诊断中血管分割和狭窄检测的准确性，解决ICA图像低对比度、高噪声和复杂结构的问题。", "method": "SFD-Mamba2Net框架，包括CASE模块和PHFP模块，用于ICA图像中的血管分割和狭窄检测。CASE模块用于突出细长的血管结构，PHFP模块用于逐步细化高频细节。", "result": "SFD-Mamba2Net在八个分割指标上优于现有方法，并在狭窄检测中达到了最高的真正例率和阳性预测值。", "conclusion": "SFD-Mamba2Net在冠状动脉疾病的血管分割和狭窄检测中表现优越，有助于改进临床诊断。"}}
{"id": "2509.09101", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09101", "abs": "https://arxiv.org/abs/2509.09101", "authors": ["Nishat Raihan", "Antonios Anastasopoulos", "Marcos Zampieri"], "title": "TigerCoder: A Novel Suite of LLMs for Code Generation in Bangla", "comment": null, "summary": "Despite being the 5th most spoken language, Bangla remains underrepresented\nin Large Language Models (LLMs), particularly for code generation. This\nprimarily stems from the scarcity of high-quality data to pre-train and/or\nfinetune such models. Hence, we introduce the first dedicated family of Code\nLLMs for Bangla (1B & 9B). We offer three major contributions: (1) a\ncomprehensive Bangla code instruction datasets for programming domain\nadaptation; (2) MBPP-Bangla, an evaluation benchmark for Bangla code\ngeneration; and (3) the TigerCoder-family of Code LLMs, achieving significant\n~11-18% performance gains at Pass@1 over existing multilingual and\ngeneral-purpose Bangla LLMs. Our findings show that curated, high-quality\ndatasets can overcome limitations of smaller models for low-resource languages.\nWe open-source all resources to advance further Bangla LLM research.", "AI": {"tldr": "介绍针对孟加拉语的首个专用于代码生成的大型语言模型系列，通过高质量的数据集和评估基准，展示出较现有相关模型显著的性能提升。", "motivation": "解决大型语言模型中孟加拉语代码生成数据匮乏的问题，推动孟加拉语LLM的研究发展。", "method": "通过构建全面的孟加拉语编程指令数据集、MBPP-Bangla评估基准以及TigerCoder系列代码LLM，改善孟加拉语的代码生成质量。", "result": "获得了显著的性能提升，通过Pass@1指标验证，在多语言和通用孟加拉语LLM上提高了约11-18%的性能。", "conclusion": "研究表明，对于资源较少的语言，精心策划和高质量的数据集可以克服较小模型的局限性。"}}
{"id": "2509.08935", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.08935", "abs": "https://arxiv.org/abs/2509.08935", "authors": ["Muhammad Alberb", "Helen Cheung", "Anne Martel"], "title": "Live(r) Die: Predicting Survival in Colorectal Liver Metastasis", "comment": "Thesis at Erasmus Mundus Joint Master's Degree in Medical Imaging and\n  Applications", "summary": "Colorectal cancer frequently metastasizes to the liver, significantly\nreducing long-term survival. While surgical resection is the only potentially\ncurative treatment for colorectal liver metastasis (CRLM), patient outcomes\nvary widely depending on tumor characteristics along with clinical and genomic\nfactors. Current prognostic models, often based on limited clinical or\nmolecular features, lack sufficient predictive power, especially in multifocal\nCRLM cases. We present a fully automated framework for surgical outcome\nprediction from pre- and post-contrast MRI acquired before surgery. Our\nframework consists of a segmentation pipeline and a radiomics pipeline. The\nsegmentation pipeline learns to segment the liver, tumors, and spleen from\npartially annotated data by leveraging promptable foundation models to complete\nmissing labels. Also, we propose SAMONAI, a novel zero-shot 3D prompt\npropagation algorithm that leverages the Segment Anything Model to segment 3D\nregions of interest from a single point prompt, significantly improving our\nsegmentation pipeline's accuracy and efficiency. The predicted pre- and\npost-contrast segmentations are then fed into our radiomics pipeline, which\nextracts features from each tumor and predicts survival using SurvAMINN, a\nnovel autoencoder-based multiple instance neural network for survival analysis.\nSurvAMINN jointly learns dimensionality reduction and hazard prediction from\nright-censored survival data, focusing on the most aggressive tumors. Extensive\nevaluation on an institutional dataset comprising 227 patients demonstrates\nthat our framework surpasses existing clinical and genomic biomarkers,\ndelivering a C-index improvement exceeding 10%. Our results demonstrate the\npotential of integrating automated segmentation algorithms and radiomics-based\nsurvival analysis to deliver accurate, annotation-efficient, and interpretable\noutcome prediction in CRLM.", "AI": {"tldr": "研究提出了一种全自动框架，利用术前和术后的MRI影像进行肝结直肠转移癌手术结果预测，包括分割管道和影像组学管道，提高了生存预测的准确性和效率。", "motivation": "由于现有预后模型基于有限的临床或分子特征，对多灶性肝结直肠转移癌的预测能力不足，因此需要提高预测能力。", "method": "该框架包含分割管道和影像组学管道。分割管道利用可提示的基础模型完成标签预测，而Radiomics管道提取特征并预测生存率。此外，文中提出一种名为SAMONAI的新型零样本3D提示传播算法，提高分割管道的精度和效率。", "result": "基于机构数据集的广泛评估表明，该框架超过现有的临床和基因组生物标志物，在C指数上提升了超过10%。", "conclusion": "结果表明，自动化分割算法与基于影像组学的生存分析相结合具有显著潜力，可以提供准确、标注高效且可解释的结直肠肝转移预后预测。"}}
{"id": "2509.09121", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09121", "abs": "https://arxiv.org/abs/2509.09121", "authors": ["Sophia Maria"], "title": "Compass-v3: Scaling Domain-Specific LLMs for Multilingual E-Commerce in Southeast Asia", "comment": null, "summary": "Large language models (LLMs) excel in general-domain applications, yet their\nperformance often degrades in specialized tasks requiring domain-specific\nknowledge. E-commerce is particularly challenging, as its data are noisy,\nheterogeneous, multilingual, and highly dynamic. We present Compass-v3, a\nvertical-domain Mixture-of-Experts (MoE) model with 245B total parameters and\n71B active per token, designed for Southeast Asian e-commerce. Compass-v3\nadopts fewer but larger experts, combined with hardware-efficient\noptimizations-such as intra-node expert parallelism and a customized memcpy\noperator-to maximize GPU utilization. The model is trained on 12T tokens of\ncurated multilingual corpora and large-scale synthetic e-commerce instructions\nusing a mixed-training strategy. To enhance alignment, we propose\nOptimal-Transport Direct Preference Optimization (OTPO), which captures\ntoken-level distinctions and improves instruction adherence in\ncommerce-specific scenarios. Extensive evaluations demonstrate that Compass-v3\ndelivers state-of-the-art e-commerce performance, surpassing DeepSeek-V3.1,\nGPT-4 series, and Qwen3-235B. Moreover, Compass-v3 demonstrates strong\nmultilingual capability across low-resource Southeast Asian languages\n(Indonesian, Thai, Filipino, Vietnamese, Malay, Taglog) and Portuguese while\nsustaining competitive performance on general benchmarks. It has already been\nwidely applied in Shopee's industrial-scale e-commerce platform and is\ngradually replacing OpenAI's traffic, now accounting for over 70\\% of total LLM\nusage, highlighting its dual strengths in specialized commerce expertise and\nbroad linguistic competence.", "AI": {"tldr": "Compass-v3是一款专为东南亚电子商务设计的245B参数混合专家模型，其高性能和多语言能力在电商领域表现突出，已在工业级应用中广泛使用。", "motivation": "大型语言模型在通用领域表现出色，但在需要特定领域知识的任务中性能往往下降，例如电子商务领域中数据存在噪音、异构性和动态性等问题。", "method": "Compass-v3采用混合专家模型（MoE）设计，含有245B总参数和每个标记71B活跃参数。模型使用更少但更大的专家，结合硬件优化策略，如节点内专家并行和定制化的memcpy操作，以最大化GPU利用率。", "result": "Compass-v3在电商性能评估中达到当前最先进的水平，超过了DeepSeek-V3.1、GPT-4系列和Qwen3-235B。它在东南亚低资源语言中展示了强大的多语言能力，并在通用基准测试中保持了竞争力。", "conclusion": "Compass-v3已经在Shopee的工业级电子商务平台中广泛应用，并逐渐取代OpenAI的流量模型，占据70%以上的LLM使用份额。这突显了其在专门的电商专业知识和广泛的语言能力方面的双重优势。"}}
{"id": "2509.08940", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.08940", "abs": "https://arxiv.org/abs/2509.08940", "authors": ["Lisa Dunlap", "Joseph E. Gonzalez", "Trevor Darrell", "Fabian Caba Heilbron", "Josef Sivic", "Bryan Russell"], "title": "Discovering Divergent Representations between Text-to-Image Models", "comment": "Accepted to ICCV 2025. Code available at\n  https://github.com/adobe-research/CompCon", "summary": "In this paper, we investigate when and how visual representations learned by\ntwo different generative models diverge. Given two text-to-image models, our\ngoal is to discover visual attributes that appear in images generated by one\nmodel but not the other, along with the types of prompts that trigger these\nattribute differences. For example, \"flames\" might appear in one model's\noutputs when given prompts expressing strong emotions, while the other model\ndoes not produce this attribute given the same prompts. We introduce CompCon\n(Comparing Concepts), an evolutionary search algorithm that discovers visual\nattributes more prevalent in one model's output than the other, and uncovers\nthe prompt concepts linked to these visual differences. To evaluate CompCon's\nability to find diverging representations, we create an automated data\ngeneration pipeline to produce ID2, a dataset of 60 input-dependent\ndifferences, and compare our approach to several LLM- and VLM-powered\nbaselines. Finally, we use CompCon to compare popular text-to-image models,\nfinding divergent representations such as how PixArt depicts prompts mentioning\nloneliness with wet streets and Stable Diffusion 3.5 depicts African American\npeople in media professions. Code at: https://github.com/adobe-research/CompCon", "AI": {"tldr": "本研究提出CompCon算法，通过进化搜索发现由两个文本到图像生成模型输出的视觉差异及其相关的提示类型，并创建了ID2数据集来评估其性能。", "motivation": "研究动机是探讨使用两种不同的生成模型学习的视觉表示在何时以及如何产生分歧，识别一个模型生成的图像中出现而另一个模型不出现的视觉属性，以及触发这些属性差异的提示类型。", "method": "本文提出了CompCon（Comparing Concepts），这是一种进化搜索算法，用于发现由两个不同文本到图像模型生成的图像中更加常见的视觉属性，以及引发这些视觉差异的提示类型。", "result": "通过创建一个自动化数据生成管道生成了一个名为ID2的数据集，包含60个输入依赖的不同点，并将该方法与几种基于LLM和VLM的方法进行比较。结果验证了CompCon发现不同表示的能力。", "conclusion": "通过使用CompCon算法，作者们发现了一些不同的图像模型对特定文本提示的不同视觉表达，例如PixArt和Stable Diffusion 3.5对孤独和非洲裔美国人媒体职业的描绘存在差异。"}}
