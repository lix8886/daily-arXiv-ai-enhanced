{"id": "2506.14791", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14791", "abs": "https://arxiv.org/abs/2506.14791", "authors": ["Jingxuan Zhou", "Yuehao Wu", "Yibo Zhang", "Yeyubei Zhang", "Yunchong Liu", "Bolin Huang", "Chunhong Yuan"], "title": "SemIRNet: A Semantic Irony Recognition Network for Multimodal Sarcasm Detection", "comment": "5 pages, 3 figures", "summary": "Aiming at the problem of difficulty in accurately identifying graphical implicit correlations in multimodal irony detection tasks, this paper proposes a Semantic Irony Recognition Network (SemIRNet). The model contains three main innovations: (1) The ConceptNet knowledge base is introduced for the first time to acquire conceptual knowledge, which enhances the model's common-sense reasoning ability; (2) Two cross-modal semantic similarity detection modules at the word level and sample level are designed to model graphic-textual correlations at different granularities; and (3) A contrastive learning loss function is introduced to optimize the spatial distribution of the sample features, which improves the separability of positive and negative samples. Experiments on a publicly available multimodal irony detection benchmark dataset show that the accuracy and F1 value of this model are improved by 1.64% and 2.88% to 88.87% and 86.33%, respectively, compared with the existing optimal methods. Further ablation experiments verify the important role of knowledge fusion and semantic similarity detection in improving the model performance.", "AI": {"tldr": "A new model, SemIRNet, is introduced to improve multimodal irony detection by incorporating ConceptNet for reasoning, cross-modal similarity detection modules, and a contrastive learning loss for feature optimization.", "motivation": "The motivation behind this research is to enhance the accuracy of identifying implicit correlations in multimodal irony detection, a challenging issue in the field of natural language processing.", "method": "The paper introduces a Semantic Irony Recognition Network (SemIRNet) to address challenges in multimodal irony detection. SemIRNet innovates by integrating the ConceptNet knowledge base for common-sense reasoning, designing word-level and sample-level cross-modal semantic similarity detection modules, and employing a contrastive learning loss function for better feature distribution.", "result": "The proposed model, SemIRNet, demonstrates improved accuracy and F1 scores by 1.64% and 2.88% on a multimodal irony detection benchmark dataset compared to current best practices.", "conclusion": "The research concludes that the integration of knowledge fusion and semantic similarity detection modules significantly aids in improving the model's performance in detecting irony in multimodal scenarios."}}
{"id": "2506.14805", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.14805", "abs": "https://arxiv.org/abs/2506.14805", "authors": ["Yang Yao", "Lingyu Li", "Jiaxin Song", "Chiyu Chen", "Zhenqi He", "Yixu Wang", "Xin Wang", "Tianle Gu", "Jie Li", "Yan Teng", "Yingchun Wang"], "title": "Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?", "comment": null, "summary": "As Multimodal Large Language Models (MLLMs) continue to evolve, their cognitive and reasoning capabilities have seen remarkable progress. However, challenges in visual fine-grained perception and commonsense causal inference persist. This paper introduces Argus Inspection, a multimodal benchmark with two levels of difficulty, emphasizing detailed visual recognition while incorporating real-world commonsense understanding to evaluate causal reasoning abilities. Expanding on it, we present the Eye of Panoptes framework, which integrates a binary parametric Sigmoid metric with an indicator function, enabling a more holistic evaluation of MLLMs' responses in opinion-based reasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the highest performance in visual fine-grained reasoning reaches only 0.46, highlighting considerable potential for enhancement. Our research offers valuable perspectives for the continued refinement of MLLMs.", "AI": {"tldr": "The paper presents Argus Inspection, a benchmark for evaluating MLLMs' visual fine-grained perception and commonsense causal reasoning skills. It introduces the Eye of Panoptes framework to assess these abilities more comprehensively.", "motivation": "The motivation for the paper is to address the challenges in visual fine-grained perception and commonsense causal reasoning in MLLMs and to provide a comprehensive benchmark and assessment framework to evaluate their reasoning capabilities.", "method": "The paper uses the Argus Inspection benchmark with varying levels of difficulty. It also employs the Eye of Panoptes framework which combines a binary parametric Sigmoid metric with an indicator function to evaluate opinion-based reasoning tasks.", "result": "Experiments on 26 mainstream MLLMs show that the best performance in visual fine-grained reasoning achieves a score of only 0.46, indicating a need for significant improvements in MLLMs' capabilities.", "conclusion": "The research concludes that the Argus Inspection benchmark and the Eye of Panoptes framework provide useful perspectives for refining the reasoning capabilities of MLLMs."}}
{"id": "2506.14816", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14816", "abs": "https://arxiv.org/abs/2506.14816", "authors": ["Alavikunhu Panthakkan", "Zubair Medammal", "S M Anzar", "Fatma Taher", "Hussain Al-Ahmad"], "title": "A Hybrid ConvNeXt-EfficientNet AI Solution for Precise Falcon Disease Detection", "comment": null, "summary": "Falconry, a revered tradition involving the training and hunting with falcons, requires meticulous health surveillance to ensure the health and safety of these prized birds, particularly in hunting scenarios. This paper presents an innovative method employing a hybrid of ConvNeXt and EfficientNet AI models for the classification of falcon diseases. The study focuses on accurately identifying three conditions: Normal, Liver Disease and 'Aspergillosis'. A substantial dataset was utilized for training and validating the model, with an emphasis on key performance metrics such as accuracy, precision, recall, and F1-score. Extensive testing and analysis have shown that our concatenated AI model outperforms traditional diagnostic methods and individual model architectures. The successful implementation of this hybrid AI model marks a significant step forward in precise falcon disease detection and paves the way for future developments in AI-powered avian healthcare solutions.", "AI": {"tldr": "A hybrid ConvNeXt and EfficientNet AI model is developed to detect falcon diseases, exceeding traditional diagnostic methods in accuracy, precision, recall, and F1-score.", "motivation": "To enhance the health surveillance of falcons in hunting scenarios by improving the diagnosis accuracy and speed of falcon diseases.", "method": "An AI model is created by merging ConvNeXt and EfficientNet neural networks for classification tasks, specifically to identify normal health and two types of diseases: Liver Disease and Aspergillosis.", "result": "The hybrid model demonstrates superior performance in terms of accuracy, precision, recall, and F1-score compared to traditional methods and individual ConvNeXt or EfficientNet models.", "conclusion": "The developed hybrid AI model represents a significant advancement in falcon disease diagnosis, promising advancements in AI applications for avian healthcare."}}
{"id": "2506.14823", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14823", "abs": "https://arxiv.org/abs/2506.14823", "authors": ["Harsha Koduri"], "title": "ViLLa: A Neuro-Symbolic approach for Animal Monitoring", "comment": null, "summary": "Monitoring animal populations in natural environments requires systems that can interpret both visual data and human language queries. This work introduces ViLLa (Vision-Language-Logic Approach), a neuro-symbolic framework designed for interpretable animal monitoring. ViLLa integrates three core components: a visual detection module for identifying animals and their spatial locations in images, a language parser for understanding natural language queries, and a symbolic reasoning layer that applies logic-based inference to answer those queries. Given an image and a question such as \"How many dogs are in the scene?\" or \"Where is the buffalo?\", the system grounds visual detections into symbolic facts and uses predefined rules to compute accurate answers related to count, presence, and location. Unlike end-to-end black-box models, ViLLa separates perception, understanding, and reasoning, offering modularity and transparency. The system was evaluated on a range of animal imagery tasks and demonstrates the ability to bridge visual content with structured, human-interpretable queries.", "AI": {"tldr": "ViLLa 是一种神经符号框架，用于可解释的动物监测，它结合了视觉检测模块、语言解析器和基于逻辑的推理层，以回答关于动物数量、存在和位置的问题。", "motivation": "监测自然环境中的动物种群需要能够解释视觉数据和人类语言查询的系统。ViLLa 是为了填补这一领域的空白，提供一种更加透明和模块化的解决方案。", "method": "Structure", "result": "{\n  \"tldr\": \"ViLLa 是一种神经符号框架，用于可解释的动物监测，它结合了视觉检测模块、语言解析器和基于逻辑的推理层，以回答关于动物数量、存在和位置的问题。\",\n  \"motivation\": \"监测自然环境中的动物种群需要能够解释视觉数据和人类语言查询的系统。ViLLa 是为了填补这一领域的空白，提供一种更加透明和模块化的解决方案。\",\n  \"method\": \"ViLLa 集成了三个核心组件：视觉检测模块、语言解析器和符号推理层，它们共同合作以回答关于动物数量、存在和位置等自然语言查询。\",\n  \"result\": \"该系统在一系列动物图像任务上进行了评估，显示了其将视觉内容与结构化、人类可解释查询相联系的能力。\",\n  \"conclusion\": \"与黑箱端到端模型不同，ViLLa 通过分离感知、理解和推理过程，提供了模块化和透明度。\n}", "conclusion": "与黑箱端到端模型不同，ViLLa 通过分离感知、理解和推理过程，提供了模块化和透明度。"}}
{"id": "2506.14900", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14900", "abs": "https://arxiv.org/abs/2506.14900", "authors": ["Imane Guellil", "Salomé Andres", "Atul Anand", "Bruce Guthrie", "Huayu Zhang", "Abul Hasan", "Honghan Wu", "Beatrice Alex"], "title": "Adverse Event Extraction from Discharge Summaries: A New Dataset, Annotation Scheme, and Initial Findings", "comment": "Accepted and will be published at ACL2025 (main conference)", "summary": "In this work, we present a manually annotated corpus for Adverse Event (AE) extraction from discharge summaries of elderly patients, a population often underrepresented in clinical NLP resources. The dataset includes 14 clinically significant AEs-such as falls, delirium, and intracranial haemorrhage, along with contextual attributes like negation, diagnosis type, and in-hospital occurrence. Uniquely, the annotation schema supports both discontinuous and overlapping entities, addressing challenges rarely tackled in prior work. We evaluate multiple models using FlairNLP across three annotation granularities: fine-grained, coarse-grained, and coarse-grained with negation. While transformer-based models (e.g., BERT-cased) achieve strong performance on document-level coarse-grained extraction (F1 = 0.943), performance drops notably for fine-grained entity-level tasks (e.g., F1 = 0.675), particularly for rare events and complex attributes. These results demonstrate that despite high-level scores, significant challenges remain in detecting underrepresented AEs and capturing nuanced clinical language. Developed within a Trusted Research Environment (TRE), the dataset is available upon request via DataLoch and serves as a robust benchmark for evaluating AE extraction methods and supporting future cross-dataset generalisation.", "AI": {"tldr": "研究创建了一个针对老年患者不良事件的手动标注数据集，并展示了虽然使用先进的模型可以达到很高的性能，但在细粒度的实体提取上有较大挑战，特别是在罕见事件和复杂临床语言的捕捉上。", "motivation": "老年患者在临床自然语言处理资源中经常被低估。这项工作的动机是提供一个专门针对这一群体的手动标注数据集，以改进AE的提取。", "method": "这项工作介绍了一个手动标注的数据集，用于从老年患者的出院总结中提取不良事件（AE）。数据集包含14种临床重要的AE，如跌倒、谵妄和颅内出血，以及上下文属性，如否定表达、诊断类型和院内发生。标注方案支持不连续和重叠实体，这是先前工作中很少处理的。", "result": "在三组不同的标注颗粒度下，使用FlairNLP模型进行了评估，包括细粒度、粗粒度和带有否定表达的粗粒度。结果表明，在文档级别的粗粒度提取上，基于转换器模型（如BERT-cased）达到非常高的F1分数，但在细粒度实体级别的任务上性能显著下降，特别是对于罕见事件和复杂属性。", "conclusion": "尽管有高水平的得分，但结果展示了显著的挑战仍然存在于检测代表性不足的AE和捕捉复杂的临床语言中。"}}
{"id": "2506.14825", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14825", "abs": "https://arxiv.org/abs/2506.14825", "authors": ["Ke Song", "Yunhe Wu", "Chunchit Siu", "Huiyuan Xiong"], "title": "GraphGSOcc: Semantic and Geometric Graph Transformer for 3D Gaussian Splating-based Occupancy Prediction", "comment": null, "summary": "Addressing the task of 3D semantic occupancy prediction for autonomous driving, we tackle two key issues in existing 3D Gaussian Splating (3DGS) methods: (1) unified feature aggregation neglecting semantic correlations among similar categories and across regions, and (2) boundary ambiguities caused by the lack of geometric constraints in MLP iterative optimization. We propose the GraphGSOcc model, a novel framework that combines semantic and geometric graph Transformer for 3D Gaussian Splating-based Occupancy Prediction. We propose the Dual Gaussians Graph Attenntion, which dynamically constructs dual graph structures: a geometric graph adaptively calculating KNN search radii based on Gaussian poses, enabling large-scale Gaussians to aggregate features from broader neighborhoods while compact Gaussians focus on local geometric consistency; a semantic graph retaining top-M highly correlated nodes via cosine similarity to explicitly encode semantic relationships within and across instances. Coupled with the Multi-scale Graph Attention framework, fine-grained attention at lower layers optimizes boundary details, while coarse-grained attention at higher layers models object-level topology. Experiments on the SurroundOcc dataset achieve an mIoU of 24.10%, reducing GPU memory to 6.1 GB, demonstrating a 1.97% mIoU improvement and 13.7% memory reduction compared to GaussianWorld", "AI": {"tldr": "提出GraphGSOcc模型来改进3D高斯插补占据预测，通过语义和几何图注意力机制解决现有问题，实验表明其在精度和内存使用上均有提升。", "motivation": "解决3D高斯插补方法中的统一特征聚合忽略语义关联和区域间关系，以及缺乏几何约束导致边界模糊的问题。", "method": "GraphGSOcc模型结合语义和几何图Transformer进行3D高斯插补占据预测，提出了双高斯图注意力机制来解决现有问题。几何图通过基于高斯姿态自适应计算KNN搜索半径来聚合特征，语义图通过余弦相似性保留高相关节点以编码语义关系。多尺度图注意力框架在不同层优化边界细节和对象拓扑。", "result": "实验在SurroundOcc数据集上的mIoU达到24.10%，减少GPU内存至6.1GB，相比GaussianWorld提高了1.97%的mIoU和13.7%的内存使用。", "conclusion": "GraphGSOcc模型通过结合语义和几何信息，有效提升了3D语义占据预测的精度并降低了内存需求。"}}
{"id": "2506.14901", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14901", "abs": "https://arxiv.org/abs/2506.14901", "authors": ["Marija Šakota", "Robert West"], "title": "Combining Constrained and Unconstrained Decoding via Boosting: BoostCD and Its Application to Information Extraction", "comment": null, "summary": "Many recent approaches to structured NLP tasks use an autoregressive language model $M$ to map unstructured input text $x$ to output text $y$ representing structured objects (such as tuples, lists, trees, code, etc.), where the desired output structure is enforced via constrained decoding. During training, these approaches do not require the model to be aware of the constraints, which are merely implicit in the training outputs $y$. This is advantageous as it allows for dynamic constraints without requiring retraining, but can lead to low-quality output during constrained decoding at test time. We overcome this problem with Boosted Constrained Decoding (BoostCD), which combines constrained and unconstrained decoding in two phases: Phase 1 decodes from the base model $M$ twice, in constrained and unconstrained mode, obtaining two weak predictions. In phase 2, a learned autoregressive boosted model combines the two weak predictions into one final prediction. The mistakes made by the base model with vs. without constraints tend to be complementary, which the boosted model learns to exploit for improved performance. We demonstrate the power of BoostCD by applying it to closed information extraction. Our model, BoostIE, outperforms prior approaches both in and out of distribution, addressing several common errors identified in those approaches.", "AI": {"tldr": "本文提出Boosted Constrained Decoding (BoostCD) 方法，改善了结构化NLP任务中的受限解码输出质量。应用于闭合信息抽取任务中的模型BoostIE表现出色。", "motivation": "克服现有方法在结构化自然语言处理任务中受限解码时产生的低质量输出问题。", "method": "采用Boosted Constrained Decoding (BoostCD) 方法结合受限解码和无限制解码，分为两个阶段解码：第一阶段从基本模型M以受限模式和无限制模式解码两次，获得两个弱预测；第二阶段，一个学习到的自回归强化模型将两个弱预测结合生成一个最终预测。", "result": "提出的模型BoostIE在闭合信息提取任务中优于先前方法，解决了常见的错误。", "conclusion": "BoostCD技术提升了模型在结构化输出任务中的性能，特别是在受限解码情况下提高了输出质量。"}}
{"id": "2506.14827", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14827", "abs": "https://arxiv.org/abs/2506.14827", "authors": ["Yifeng Gao", "Yifan Ding", "Hongyu Su", "Juncheng Li", "Yunhan Zhao", "Lin Luo", "Zixing Chen", "Li Wang", "Xin Wang", "Yixu Wang", "Xingjun Ma", "Yu-Gang Jiang"], "title": "DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning", "comment": null, "summary": "As AI-generated video becomes increasingly pervasive across media platforms, the ability to reliably distinguish synthetic content from authentic footage has become both urgent and essential. Existing approaches have primarily treated this challenge as a binary classification task, offering limited insight into where or why a model identifies a video as AI-generated. However, the core challenge extends beyond simply detecting subtle artifacts; it requires providing fine-grained, persuasive evidence that can convince auditors and end-users alike. To address this critical gap, we introduce DAVID-X, the first dataset to pair AI-generated videos with detailed defect-level, temporal-spatial annotations and written rationales. Leveraging these rich annotations, we present DAVID-XR1, a video-language model designed to deliver an interpretable chain of visual reasoning-including defect categorization, temporal-spatial localization, and natural language explanations. This approach fundamentally transforms AI-generated video detection from an opaque black-box decision into a transparent and verifiable diagnostic process. We demonstrate that a general-purpose backbone, fine-tuned on our compact dataset and enhanced with chain-of-thought distillation, achieves strong generalization across a variety of generators and generation modes. Our results highlight the promise of explainable detection methods for trustworthy identification of AI-generated video content.", "AI": {"tldr": "论文提出了DAVID-X数据集和DAVID-XR1模型，通过详细注释和可解释的推理链方法，改进了AI生成视频的检测过程，使其更加透明和可信。", "motivation": "随着AI生成视频在媒体平台上的普遍存在，可靠地区分合成内容和真实视频变得越来越紧迫。现有方法主要将这一挑战视为一个二元分类任务，但无法提供足够的信息来解释模型为什么会识别某一视频为AI生成。因此，需要提供细化且具有说服力的证据来支持检测结果。", "method": "此论文介绍了一个名为DAVID-X的数据集，该数据集包含了与AI生成视频相关联的详细缺陷级别、时空注释和书面理由。利用这些丰富的注释，作者提出了DAVID-XR1，这是一种视频-语言模型，旨在提供一个可解释的视觉推理链，包括缺陷分类、时空定位和自然语言解释。", "result": "这种方法将AI生成视频检测从一个不透明的黑盒决策转化为一种透明且可验证的诊断过程。结果表明，在较小的数据集上进行微调并结合推理链蒸馏的通用模型，能够有效地推广到各种生成器和生成模式。", "conclusion": "该论文的方法展示了解释性检测方法在可信地识别AI生成视频内容方面的潜力。"}}
{"id": "2506.14912", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14912", "abs": "https://arxiv.org/abs/2506.14912", "authors": ["Dyah Adila", "Shuai Zhang", "Boran Han", "Bonan Min", "Yuyang Wang"], "title": "CrEst: Credibility Estimation for Contexts in LLMs via Weak Supervision", "comment": null, "summary": "The integration of contextual information has significantly enhanced the performance of large language models (LLMs) on knowledge-intensive tasks. However, existing methods often overlook a critical challenge: the credibility of context documents can vary widely, potentially leading to the propagation of unreliable information. In this paper, we introduce CrEst, a novel weakly supervised framework for assessing the credibility of context documents during LLM inference--without requiring manual annotations. Our approach is grounded in the insight that credible documents tend to exhibit higher semantic coherence with other credible documents, enabling automated credibility estimation through inter-document agreement. To incorporate credibility into LLM inference, we propose two integration strategies: a black-box approach for models without access to internal weights or activations, and a white-box method that directly modifies attention mechanisms. Extensive experiments across three model architectures and five datasets demonstrate that CrEst consistently outperforms strong baselines, achieving up to a 26.86% improvement in accuracy and a 3.49% increase in F1 score. Further analysis shows that CrEst maintains robust performance even under high-noise conditions.", "AI": {"tldr": "Paper introduces CrEst, a framework for assessing and integrating the credibility of context documents in LLMs, showing significant performance improvements.", "motivation": "The motivation of this paper is to address the variability in credibility of context documents during LLM inference, which can lead to the propagation of unreliable information. This variability is often overlooked in current methods.", "method": "Our approach, CrEst, is a weakly supervised framework that assesses the credibility of context documents by leveraging semantic coherence between documents. For LLM integration, two strategies are proposed: a black-box method and a white-box method that modifies attention mechanisms.", "result": "CrEst shows significant improvements over strong baselines with a maximum accuracy improvement of 26.86% and an F1 score increase of 3.49% across diverse datasets and LLM architectures. It also shows robust performance under high-noise conditions.", "conclusion": "The paper concludes that CrEst is a promising method for improving the performance of LLMs on knowledge-intensive tasks by enhancing the credibility of context documents during inference."}}
{"id": "2506.14831", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.14831", "abs": "https://arxiv.org/abs/2506.14831", "authors": ["Céline Finet", "Stephane Da Silva Martins", "Jean-Bernard Hayet", "Ioannis Karamouzas", "Javad Amirian", "Sylvie Le Hégarat-Mascle", "Julien Pettré", "Emanuel Aldea"], "title": "Recent Advances in Multi-Agent Human Trajectory Prediction: A Comprehensive Review", "comment": "30 pages", "summary": "With the emergence of powerful data-driven methods in human trajectory prediction (HTP), gaining a finer understanding of multi-agent interactions lies within hand's reach, with important implications in areas such as autonomous navigation and crowd modeling. This survey reviews some of the most recent advancements in deep learning-based multi-agent trajectory prediction, focusing on studies published between 2020 and 2024. We categorize the existing methods based on their architectural design, their input representations, and their overall prediction strategies, placing a particular emphasis on models evaluated using the ETH/UCY benchmark. Furthermore, we highlight key challenges and future research directions in the field of multi-agent HTP.", "AI": {"tldr": "这篇论文回顾了2020年至2024年间基于深度学习的多智能体轨迹预测的最新进展，重点关注了模型的架构设计、输入表示和预测策略，并特别强调了在ETH/UCY基准上评估的模型，同时还指出了该领域的一些关键挑战和未来的研究方向。", "motivation": "随着数据驱动方法在人类轨迹预测领域的兴起，理解多智能体间的交互变得更加可行，该研究对于自动驾驶和人群建模等领域具有重要意义。", "method": "文章通过对2020年至2024年期间已发表的文献进行分类总结，主要根据模型的架构设计、输入表示和预测策略来进行分类。", "result": "文章综述了基于深度学习的多智能体轨迹预测方法，并特别强调了在ETH/UCY基准上评估的模型。也指出了该领域的关键挑战。", "conclusion": "未来的研究需要更加关注模型的架构、输入表示及预测策略的改进，并且应该致力于解决多智能体轨迹预测的关键挑战。"}}
{"id": "2506.14927", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14927", "abs": "https://arxiv.org/abs/2506.14927", "authors": ["Joseph J. Peper", "Wenzhao Qiu", "Ali Payani", "Lu Wang"], "title": "MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance", "comment": "ACL 2025 Findings", "summary": "Natural language processing evaluation has made significant progress, largely driven by the proliferation of powerful large language mod-els (LLMs). New evaluation benchmarks are of increasing priority as the reasoning capabilities of LLMs are expanding at a rapid pace. In particular, while multi-document (MD) reasoning is an area of extreme relevance given LLM capabilities in handling longer-context inputs, few benchmarks exist to rigorously examine model behavior in this setting. Moreover, the multi-document setting is historically challenging for benchmark creation due to the expensive cost of annotating long inputs. In this work, we introduce MDBench, a new dataset for evaluating LLMs on the task of multi-document reasoning. Notably, MDBench is created through a novel synthetic generation process, allowing us to controllably and efficiently generate challenging document sets and the corresponding question-answer (QA) examples. Our novel technique operates on condensed structured seed knowledge, modifying it through LLM-assisted edits to induce MD-specific reasoning challenges. We then convert this structured knowledge into a natural text surface form, generating a document set and corresponding QA example. We analyze the behavior of popular LLMs and prompting techniques, finding that MDBENCH poses significant challenges for all methods, even with relatively short document sets. We also see our knowledge-guided generation technique (1) allows us to readily perform targeted analysis of MD-specific reasoning capabilities and (2) can be adapted quickly to account for new challenges and future modeling improvements.", "AI": {"tldr": "提出MDBench多文档推理数据集，揭示其对于现有LLMs和提示技术构成显著挑战，证明了基于知识引导的生成技术的有效性。", "motivation": "随着LLMs推理能力的快速发展，新的评估基准变得尤为重要。尤其在多文档推理这一与LLMs处理长上下文输入能力高度相关的领域，现有的基准测试不足。此外，由于长输入标注成本高，多文档设置历来对基准创建具有挑战性。", "method": "本研究提出了MDBench，一个新的用于评估LLMs在多文档推理任务上的数据集。MDBench通过一种新颖的合成生成过程创建，可以控制并高效地生成具有挑战性的文档集和对应的问答例子。该方法基于压缩的结构化种子知识，通过LLM辅助的编辑来引入多文档特定的推理挑战，并将这些结构化的知识转化为自然文本形式，从而生成文档集和对应的问答例子。", "result": "研究发现，MDBENCH对所有方法都提出了显著挑战，即使文档集相对较短。该研究的方法（基于知识引导的生成技术）不仅可以方便地对多文档特定推理能力进行针对性分析，而且能够快速适应新的挑战和未来模型改进。", "conclusion": "结论表明，MDBench通过合成生成过程提供了一个新颖的方法来评估LLMs在多文档情境下的推理能力，并能够在快速适应新的挑战和未来模型改进方面发挥作用。"}}
{"id": "2506.14832", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14832", "abs": "https://arxiv.org/abs/2506.14832", "authors": ["Jun Yin", "Jing Zhong", "Pengyu Zeng", "Peilin Li", "Zixuan Dai", "Miao Zhang", "Shuai Lu"], "title": "ArchShapeNet:An Interpretable 3D-CNN Framework for Evaluating Architectural Shapes", "comment": "22 pages, 8 figures", "summary": "In contemporary architectural design, the growing complexity and diversity of design demands have made generative plugin tools essential for quickly producing initial concepts and exploring novel 3D forms. However, objectively analyzing the differences between human-designed and machine-generated 3D forms remains a challenge, limiting our understanding of their respective strengths and hindering the advancement of generative tools.\n  To address this, we built ArchForms-4000, a dataset containing 2,000 architect-designed and 2,000 Evomass-generated 3D forms; Proposed ArchShapeNet, a 3D convolutional neural network tailored for classifying and analyzing architectural forms, incorporating a saliency module to highlight key spatial features aligned with architectural reasoning; And conducted comparative experiments showing our model outperforms human experts in distinguishing form origins, achieving 94.29% accuracy, 96.2% precision, and 98.51% recall.\n  This study not only highlights the distinctive advantages of human-designed forms in spatial organization, proportional harmony, and detail refinement but also provides valuable insights for enhancing generative design tools in the future.", "AI": {"tldr": "研究构建了ArchForms-4000数据集，提出ArchShapeNet模型以区分和分析人类设计与机器生成的3D建筑形态，发现其在区分形态来源方面的效果优于人类专家，并探索了未来改进生成设计工具的方向。", "motivation": "当前的建筑设计中，设计需求的复杂性和多样性不断增加，使得生成插件工具在快速生成初始概念和探索新颖的3D形态方面变得必不可少。然而，客观分析人类设计和机器生成的3D形态之间的差异至今仍是一个挑战，这限制了我们对它们各自优势的理解，并阻碍了生成工具的发展。本研究旨在解决这一问题。", "method": "本研究构建了ArchForms-4000数据集，其中包含2000个建筑师设计的和2000个Evomass生成的3D形态，提出了一种名为ArchShapeNet的3D卷积神经网络，用于分类和分析建筑形态，并集成了一个突出关键空间特征的显著性模块，这些特征与建筑设计逻辑相关。", "result": "通过对比实验，本研究的模型在区分形态来源方面优于人类专家，准确率为94.29%，精确率为96.2%，召回率为98.51%。", "conclusion": "这项研究不仅凸显了人类设计形式在空间组织、比例和谐和细节改进方面的独特优势，还为未来增强生成设计工具提供了宝贵的见解。"}}
{"id": "2506.14949", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14949", "abs": "https://arxiv.org/abs/2506.14949", "authors": ["Shadman Sakib", "Oishy Fatema Akhand", "Ajwad Abrar"], "title": "From Chat to Checkup: Can Large Language Models Assist in Diabetes Prediction?", "comment": "Accepted in 1st IEEE QPAIN 2025", "summary": "While Machine Learning (ML) and Deep Learning (DL) models have been widely used for diabetes prediction, the use of Large Language Models (LLMs) for structured numerical data is still not well explored. In this study, we test the effectiveness of LLMs in predicting diabetes using zero-shot, one-shot, and three-shot prompting methods. We conduct an empirical analysis using the Pima Indian Diabetes Database (PIDD). We evaluate six LLMs, including four open-source models: Gemma-2-27B, Mistral-7B, Llama-3.1-8B, and Llama-3.2-2B. We also test two proprietary models: GPT-4o and Gemini Flash 2.0. In addition, we compare their performance with three traditional machine learning models: Random Forest, Logistic Regression, and Support Vector Machine (SVM). We use accuracy, precision, recall, and F1-score as evaluation metrics. Our results show that proprietary LLMs perform better than open-source ones, with GPT-4o and Gemma-2-27B achieving the highest accuracy in few-shot settings. Notably, Gemma-2-27B also outperforms the traditional ML models in terms of F1-score. However, there are still issues such as performance variation across prompting strategies and the need for domain-specific fine-tuning. This study shows that LLMs can be useful for medical prediction tasks and encourages future work on prompt engineering and hybrid approaches to improve healthcare predictions.", "AI": {"tldr": "The study evaluates the effectiveness of six large language models (LLMs) in predicting diabetes using the Pima Indian Diabetes Database and compares their performance with three traditional machine learning models. Proprietary LLMs outperform open-source ones, with GPT-4o and Gemma-2-27B achieving the highest accuracy.", "motivation": "The main motivation is to explore the potential of LLMs for predicting structured numerical data, specifically in the domain of medical prediction tasks. The authors aim to fill the gap in the literature where LLMs are primarily used for text-related tasks.", "method": "The authors use zero-shot, one-shot, and three-shot prompting methods to evaluate six LLMs and compare their performance with traditional machine learning models. They use the Pima Indian Diabetes Database and consider accuracy, precision, recall, and F1-score as evaluation metrics.", "result": "Results indicate that proprietary LLMs perform better than open-source ones. GPT-4o and Gemma-2-27B achieved the highest accuracy, and Gemma-2-27B also showed better F1-score performance compared to traditional ML models.", "conclusion": "The study demonstrates the potential of LLMs in medical prediction tasks, particularly in diabetes prediction. It suggests further research into prompt engineering and hybrid approaches to enhance their application in healthcare."}}
{"id": "2506.14833", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14833", "abs": "https://arxiv.org/abs/2506.14833", "authors": ["Poojashree Chandrashekar Pankaj M Sajjanar"], "title": "Real-Time, Low-Latency Surveillance Using Entropy-Based Adaptive Buffering and MobileNetV2 on Edge Devices", "comment": "& pages", "summary": "This paper describes a high-performance, low-latency video surveillance system designed for resource-constrained environments. We have proposed a formal entropy-based adaptive frame buffering algorithm and integrated that with MobileNetV2 to achieve high throughput with low latency. The system is capable of processing live streams of video with sub-50ms end-to-end inference latency on resource-constrained devices (embedding platforms) such as Raspberry Pi, Amazon, and NVIDIA Jetson Nano. Our method maintains over 92% detection accuracy on standard datasets focused on video surveillance and exhibits robustness to varying lighting, backgrounds, and speeds. A number of comparative and ablation experiments validate the effectiveness of our design. Finally, our architecture is scalable, inexpensive, and compliant with stricter data privacy regulations than common surveillance systems, so that the system could coexist in a smart city or embedded security architecture.", "AI": {"tldr": "本篇论文描述了一个高性能、低延迟的视频监控系统，专为资源受限的环境设计。通过结合基于熵的自适应帧缓冲算法和MobileNetV2实现高吞吐量和低延迟。该系统能够在资源受限设备上实现低于50毫秒的端到端推理延迟，同时在标准视频监控数据集上保持超过92%的检测准确率。实验表明系统设计的有效性。我们的架构具备可扩展性、成本效益和严格的数据隐私符合性。", "motivation": "动机是在资源受限的环境下，设计一个既能保持高性能又能满足低延迟要求的视频监控系统，并且能够在一个智能城市或嵌入式安全架构中与现有的系统共存。", "method": "通过提出一种基于熵的自适应帧缓冲算法并结合MobileNetV2模型，有效地提高视频处理系统的吞吐量，同时减少延迟。", "result": "系统在资源受限设备如Raspberry Pi, Amazon, 和NVIDIA Jetson Nano上实现了低于50毫秒的端到端推理延迟，同时在标准数据集上的检测准确率超过92%。同时也展现了对不同光照、背景、速率等条件的鲁棒性。", "conclusion": "该研究提供了一种成本效益高、性能良好的视频监控架构，不仅能够稳定运行于资源有限的设备上，而且在数据隐私方面符合更严格的标准。"}}
{"id": "2506.15001", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15001", "abs": "https://arxiv.org/abs/2506.15001", "authors": ["Ignacio Sastre", "Aiala Rosá"], "title": "Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings", "comment": "This paper will be presented at The First Workshop on Large Language Model Memorization (L2M2) at ACL 2025", "summary": "In this work, we observe an interesting phenomenon: it is possible to generate reversible sentence embeddings that allow an LLM to reconstruct the original text exactly, without modifying the model's weights. This is achieved by introducing a special memory token, whose embedding is optimized through training on a fixed sequence. When prompted with this embedding, the model reconstructs the fixed sequence exactly. We evaluate this phenomenon across English and Spanish datasets, sequences of up to approximately 240 tokens, and model scales ranging from 100M to 8B parameters. Notably, Llama 3.1 8B successfully reconstructs all tested sequences. Our findings highlight an interesting capability of LLMs and suggest potential applications in memory-based retrieval, compression, and controlled text generation.", "AI": {"tldr": "本文展示了通过优化特殊标记的嵌入，大型语言模型能够精确重构固定序列，适用于多种语言和模型规模，这为文本处理提供了新思路。", "motivation": "研究动机在于观察并验证一个有趣的现象，即通过特定的方法可以使大型语言模型能够生成可逆的句子嵌入，进而精确重构原始文本。", "method": "本研究通过引入一个特殊的内存标记(token)，并对该标记的嵌入(embedding)进行训练优化，使得大型语言模型（LLM）能够精确重构原始文本，且无需修改模型权重。", "result": "实验结果表明，在英语和西班牙语数据集上，以及模型规模从1亿到80亿参数不等的情况下，最长240个token的序列都能被精确重构，特别是80亿参数的Llama 3.1模型成功重构了所有测试序列。", "conclusion": "研究发现强调了LLM的一个有趣能力，并为基于记忆的检索、文本压缩以及可控文本生成提出了潜在应用可能性。"}}
{"id": "2506.14835", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14835", "abs": "https://arxiv.org/abs/2506.14835", "authors": ["Kiet Dang Vu", "Trung Thai Tran", "Duc Dung Nguyen"], "title": "MonoVQD: Monocular 3D Object Detection with Variational Query Denoising and Self-Distillation", "comment": null, "summary": "Precisely localizing 3D objects from a single image constitutes a central challenge in monocular 3D detection. While DETR-like architectures offer a powerful paradigm, their direct application in this domain encounters inherent limitations, preventing optimal performance. Our work addresses these challenges by introducing MonoVQD, a novel framework designed to fundamentally advance DETR-based monocular 3D detection. We propose three main contributions. First, we propose the Mask Separated Self-Attention mechanism that enables the integration of the denoising process into a DETR architecture. This improves the stability of Hungarian matching to achieve a consistent optimization objective. Second, we present the Variational Query Denoising technique to address the gradient vanishing problem of conventional denoising methods, which severely restricts the efficiency of the denoising process. This explicitly introduces stochastic properties to mitigate this fundamental limitation and unlock substantial performance gains. Finally, we introduce a sophisticated self-distillation strategy, leveraging insights from later decoder layers to synergistically improve query quality in earlier layers, thereby amplifying the iterative refinement process. Rigorous experimentation demonstrates that MonoVQD achieves superior performance on the challenging KITTI monocular benchmark. Highlighting its broad applicability, MonoVQD's core components seamlessly integrate into other architectures, delivering significant performance gains even in multi-view 3D detection scenarios on the nuScenes dataset and underscoring its robust generalization capabilities.", "AI": {"tldr": "MonoVQD, a new framework that improves DETR-based monocular 3D detection by solving inherent limitations of DETR architectures.", "motivation": "Addressing the limitations of DETR-like architectures in monocular 3D detection.", "method": "Mask Separated Self-Attention mechanism and Variational Query Denoising technique, with a self-distillation strategy.", "result": "MonoVQD achieves superior performance on the KITTI monocular benchmark and shows robust generalization capabilities, even improving performance in multi-view 3D detection scenarios on the nuScenes dataset.", "conclusion": "MonoVQD is a novel framework that advances DETR-based monocular 3D detection through innovative mechanisms and approaches."}}
{"id": "2506.15030", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15030", "abs": "https://arxiv.org/abs/2506.15030", "authors": ["Drew Walker", "Swati Rajwal", "Sudeshna Das", "Snigdha Peddireddy", "Abeed Sarker"], "title": "Identifying social isolation themes in NVDRS text narratives using topic modeling and text-classification methods", "comment": "22 pages, 2 figures, 5 tables", "summary": "Social isolation and loneliness, which have been increasing in recent years strongly contribute toward suicide rates. Although social isolation and loneliness are not currently recorded within the US National Violent Death Reporting System's (NVDRS) structured variables, natural language processing (NLP) techniques can be used to identify these constructs in law enforcement and coroner medical examiner narratives. Using topic modeling to generate lexicon development and supervised learning classifiers, we developed high-quality classifiers (average F1: .86, accuracy: .82). Evaluating over 300,000 suicides from 2002 to 2020, we identified 1,198 mentioning chronic social isolation. Decedents had higher odds of chronic social isolation classification if they were men (OR = 1.44; CI: 1.24, 1.69, p<.0001), gay (OR = 3.68; 1.97, 6.33, p<.0001), or were divorced (OR = 3.34; 2.68, 4.19, p<.0001). We found significant predictors for other social isolation topics of recent or impending divorce, child custody loss, eviction or recent move, and break-up. Our methods can improve surveillance and prevention of social isolation and loneliness in the United States.", "AI": {"tldr": "使用NLP技术识别自杀报告中的社会隔离和孤独感，开发出了高质量的分类器，发现多个显著预测因素，改进了该状况的监控和预防。", "motivation": "社会隔离和孤独感在过去几年中增加，对自杀率有重大贡献，但这些因素在NVDRS中没有被记录为结构变量。自然语言处理技术可以用于解决这个问题。", "method": "使用话题建模生成词汇开发和监督学习分类器来识别社会隔离和孤独感。", "result": "开发的分类器具有高质量（平均F1: 0.86，准确率: 0.82），在评估了从2002年到2020年的超过300,000自杀案例中，识别出1,198例提到长期社会隔离的案例。", "conclusion": "研究方法可以改善美国对社会隔离和孤独感的监控和预防。"}}
{"id": "2506.14837", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14837", "abs": "https://arxiv.org/abs/2506.14837", "authors": ["Chengzhi Xu", "Yuyang Wang", "Lai Wei", "Lichao Sun", "Weiran Huang"], "title": "Improved Iterative Refinement for Chart-to-Code Generation via Structured Instruction", "comment": null, "summary": "Recently, multimodal large language models (MLLMs) have attracted increasing research attention due to their powerful visual understanding capabilities. While they have achieved impressive results on various vision tasks, their performance on chart-to-code generation remains suboptimal. This task requires MLLMs to generate executable code that can reproduce a given chart, demanding not only precise visual understanding but also accurate translation of visual elements into structured code. Directly prompting MLLMs to perform this complex task often yields unsatisfactory results. To address this challenge, we propose {ChartIR}, an iterative refinement method based on structured instruction. First, we distinguish two tasks: visual understanding and code translation. To accomplish the visual understanding component, we design two types of structured instructions: description and difference. The description instruction captures the visual elements of the reference chart, while the difference instruction characterizes the discrepancies between the reference chart and the generated chart. These instructions effectively transform visual features into language representations, thereby facilitating the subsequent code translation process. Second, we decompose the overall chart generation pipeline into two stages: initial code generation and iterative refinement, enabling progressive enhancement of the final output. Experimental results show that, compared to other method, our method achieves superior performance on both the open-source model Qwen2-VL and the closed-source model GPT-4o.", "AI": {"tldr": "The paper addresses the issue of suboptimal performance of MLLMs in chart-to-code generation by introducing {ChartIR}, an iterative refinement method that uses structured instructions to enhance visual understanding and code generation.", "motivation": "The motivation is to improve the performance of multimodal large language models (MLLMs) in chart-to-code generation, which requires precise visual understanding and accurate translation into structured code.", "method": "The paper proposes {ChartIR}, an iterative refinement method. It distinguishes between visual understanding and code translation. For visual understanding, two types of structured instructions are designed: description and difference. For the code generation pipeline, it is decomposed into initial code generation and iterative refinement stages.", "result": "Experimental results demonstrate superior performance of the proposed method on both the open-source model Qwen2-VL and closed-source model GPT-4o, in comparison to other methods.", "conclusion": "The paper concludes that {ChartIR} effectively improves the performance of MLLMs in the task of chart-to-code generation, showing better results than existing methods on two different models."}}
{"id": "2506.15068", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15068", "abs": "https://arxiv.org/abs/2506.15068", "authors": ["Zongxia Li", "Yapei Chang", "Yuhang Zhou", "Xiyang Wu", "Zichao Liang", "Yoo Yeon Sung", "Jordan Lee Boyd-Graber"], "title": "Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form Generation", "comment": null, "summary": "Evaluating open-ended long-form generation is challenging because it is hard to define what clearly separates good from bad outputs. Existing methods often miss key aspects like coherence, style, or relevance, or are biased by pretraining data, making open-ended long-form evaluation an underexplored problem. To address this gap, we propose PrefBERT, a scoring model for evaluating open-ended long-form generation in GRPO and guiding its training with distinct rewards for good and bad outputs. Trained on two response evaluation datasets with diverse long-form styles and Likert-rated quality, PrefBERT effectively supports GRPO by offering better semantic reward feedback than traditional metrics ROUGE-L and BERTScore do. Through comprehensive evaluations, including LLM-as-a-judge, human ratings, and qualitative analysis, we show that PrefBERT, trained on multi-sentence and paragraph-length responses, remains reliable across varied long passages and aligns well with the verifiable rewards GRPO needs. Human evaluations confirm that using PrefBERT as the reward signal to train policy models yields responses better aligned with human preferences than those trained with traditional metrics. Our code is available at https://github.com/zli12321/long_form_rl.", "AI": {"tldr": "本文提出了PrefBERT，一种用于评估开放式长文本生成的评分模型，它通过奖励机制来区分好输出和坏输出，并在训练中提供比传统指标更好的语义反馈。", "motivation": "由于开放式长文本生成的评估存在挑战，现有方法难以全面评估文本的连贯性、风格或相关性，或者容易受到预训练数据的影响。", "method": "PrefBERT 使用两个具有多样风格和质量评分的响应评估数据集进行训练，以指导 GRPO 训练，提供积极和消极输出的不同奖励。", "result": "通过多方面的评估，包括 LLM 作为裁判，人类评分和定性分析，显示 PrefBERT 在各种长段落中都能保持可靠的性能，并且与 GRPO 所需的可验证奖励相符。", "conclusion": "人类评估验证了使用 PrefBERT 作为奖励信号训练的策略模型会产生更符合人类偏好的响应，优于使用传统指标训练的模型。"}}
{"id": "2506.14842", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14842", "abs": "https://arxiv.org/abs/2506.14842", "authors": ["Lukas Schiesser", "Cornelius Wolff", "Sophie Haas", "Simon Pukrop"], "title": "PictSure: Pretraining Embeddings Matters for In-Context Learning Image Classifiers", "comment": "15 pages, 10 figures", "summary": "Building image classification models remains cumbersome in data-scarce domains, where collecting large labeled datasets is impractical. In-context learning (ICL) has emerged as a promising paradigm for few-shot image classification (FSIC), enabling models to generalize across domains without gradient-based adaptation. However, prior work has largely overlooked a critical component of ICL-based FSIC pipelines: the role of image embeddings. In this work, we present PictSure, an ICL framework that places the embedding model -- its architecture, pretraining, and training dynamics -- at the center of analysis. We systematically examine the effects of different visual encoder types, pretraining objectives, and fine-tuning strategies on downstream FSIC performance. Our experiments show that the training success and the out-of-domain performance are highly dependent on how the embedding models are pretrained. Consequently, PictSure manages to outperform existing ICL-based FSIC models on out-of-domain benchmarks that differ significantly from the training distribution, while maintaining comparable results on in-domain tasks. Code can be found at https://github.com/PictSure/pictsure-library.", "AI": {"tldr": "PictSure框架专注于图像嵌入模型，以提高少量样本图像分类的性能。", "motivation": "尽管In-context learning已成为少量样本图像分类（FSIC）的一种有前途的方法，但以前的研究很少关注图像嵌入的关键作用。这项工作通过提出PictSure框架，解决在数据稀缺领域中构建图像分类模型的复杂性，特别是在不需要基于梯度的适应的情况下。", "method": "研究系统地探讨了不同类型视觉编码器、预训练目标以及微调策略对下游FSIC性能的影响。", "result": "研究展示了PictSure框架在少量样本图像分类任务上超越现有方法的表现，尤其是在分布不同的域外基准测试上。此框架的核心在于对图像嵌入模型的架构、预训练和训练动态进行全面分析。实验表明，嵌入模型的预训练方式对其训练成功率和域外性能具有重要影响。", "conclusion": "实验结果显示，训练成功和域外性能在很大程度上依赖于嵌入模型的预训练方式。"}}
{"id": "2506.15076", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15076", "abs": "https://arxiv.org/abs/2506.15076", "authors": ["Ruihan Wu", "Konstantin Garov", "Kamalika Chaudhuri"], "title": "Learning-Time Encoding Shapes Unlearning in LLMs", "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in the real world, the ability to ``unlearn'', or remove specific pieces of knowledge post hoc, has become essential for a variety of reasons ranging from privacy regulations to correcting outdated or harmful content. Prior work has proposed unlearning benchmarks and algorithms, and has typically assumed that the training process and the target model are fixed. In this work, we empirically investigate how learning-time choices in knowledge encoding impact the effectiveness of unlearning factual knowledge. Our experiments reveal two key findings: (1) learning with paraphrased descriptions improves unlearning performance and (2) unlearning individual piece of knowledge from a chunk of text is challenging. Our results suggest that learning-time knowledge encoding may play a central role in enabling reliable post-hoc unlearning.", "AI": {"tldr": "本文探讨了大规模语言模型在遗忘特定知识上的能力，发现知识编码方式与其有效性密切相关。", "motivation": "随着大规模语言模型在现实世界的广泛应用，有选择性地遗忘特定知识变得非常重要，如隐私保护、纠正过时或有害内容等。", "method": "本文通过实验研究了知识编码方式对后期知识遗忘效果的影响，特别是探讨了使用同义句描述和从一段文本中单独遗忘特定知识的实际效果。", "result": "实验发现两个重要结果：1) 使用同义句描述可以提高遗忘特定知识的效果；2) 从一段文本中单独遗忘特定知识是具有挑战性的。", "conclusion": "实验结果显示，使用同义句描述可以提高知识遗忘的效果，并暗示知识编码方式对可靠的后期知识遗忘起着关键作用。"}}
{"id": "2506.14846", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.14846", "abs": "https://arxiv.org/abs/2506.14846", "authors": ["Shreyas Rajeev", "B Sathish Babu"], "title": "Finding Optimal Kernel Size and Dimension in Convolutional Neural Networks An Architecture Optimization Approach", "comment": null, "summary": "Kernel size selection in Convolutional Neural Networks (CNNs) is a critical but often overlooked design decision that affects receptive field, feature extraction, computational cost, and model accuracy. This paper proposes the Best Kernel Size Estimation Function (BKSEF), a mathematically grounded and empirically validated framework for optimal, layer-wise kernel size determination. BKSEF balances information gain, computational efficiency, and accuracy improvements by integrating principles from information theory, signal processing, and learning theory. Extensive experiments on CIFAR-10, CIFAR-100, ImageNet-lite, ChestX-ray14, and GTSRB datasets demonstrate that BKSEF-guided architectures achieve up to 3.1 percent accuracy improvement and 42.8 percent reduction in FLOPs compared to traditional models using uniform 3x3 kernels. Two real-world case studies further validate the approach: one for medical image classification in a cloud-based setup, and another for traffic sign recognition on edge devices. The former achieved enhanced interpretability and accuracy, while the latter reduced latency and model size significantly, with minimal accuracy trade-off. These results show that kernel size can be an active, optimizable parameter rather than a fixed heuristic. BKSEF provides practical heuristics and theoretical support for researchers and developers seeking efficient and application-aware CNN designs. It is suitable for integration into neural architecture search pipelines and real-time systems, offering a new perspective on CNN optimization.", "AI": {"tldr": "论文提出BKSEF，一个数学和实验双重验证的框架，用于优化CNN的核大小，大幅度提升准确性并降低计算成本。", "motivation": "论文强调了卷积神经网络中核大小选择的重要性，该选择影响感受野、特征提取、计算成本和模型准确性，然而这一决定通常被忽视。因此，该研究提出一种优化的设计方法。", "method": "该论文提出了最佳核大小估算函数（BKSEF），它是一个基于数学原理并经过实验验证的框架，用于逐层最优核大小的确定。BKSEF通过整合信息论、信号处理和学习理论的原则来平衡信息增益、计算效率和准确性提升。", "result": "实验结果表明，使用BKSEF指导的模型在CIFAR-10、CIFAR-100、ImageNet-lite、ChestX-ray14和GTSRB数据集上可以比传统使用统一3x3核的模型提高最多3.1%的准确性和降低最多42.8%的FLOPs。两个实地案例研究进一步验证了该方法的有效性：一个用于云计算环境中的医学图像分类，另一个用于边缘设备中的交通标志识别。前者提高了可解释性和准确性，而后者显著减少了延迟和模型大小，几乎没有任何准确性的损失。", "conclusion": "结果表明，核大小可以作为一个活跃的优化参数，而不是固定的启发式参数。BKSEF为寻求高效且应用特性的CNN设计的研究人员和开发人员提供了实际的启发和理论支持，适合集成到神经结构搜索流水线和实时系统中。"}}
{"id": "2506.15081", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15081", "abs": "https://arxiv.org/abs/2506.15081", "authors": ["Yaxin Fan", "Peifeng Li", "Qiaoming Zhu"], "title": "Improving Dialogue Discourse Parsing through Discourse-aware Utterance Clarification", "comment": "Accepted by ACL2025(main conference)", "summary": "Dialogue discourse parsing aims to identify and analyze discourse relations between the utterances within dialogues. However, linguistic features in dialogues, such as omission and idiom, frequently introduce ambiguities that obscure the intended discourse relations, posing significant challenges for parsers. To address this issue, we propose a Discourse-aware Clarification Module (DCM) to enhance the performance of the dialogue discourse parser. DCM employs two distinct reasoning processes: clarification type reasoning and discourse goal reasoning. The former analyzes linguistic features, while the latter distinguishes the intended relation from the ambiguous one. Furthermore, we introduce Contribution-aware Preference Optimization (CPO) to mitigate the risk of erroneous clarifications, thereby reducing cascading errors. CPO enables the parser to assess the contributions of the clarifications from DCM and provide feedback to optimize the DCM, enhancing its adaptability and alignment with the parser's requirements. Extensive experiments on the STAC and Molweni datasets demonstrate that our approach effectively resolves ambiguities and significantly outperforms the state-of-the-art (SOTA) baselines.", "AI": {"tldr": "提出了一种对话话语解析方法，通过引入Discourse-aware Clarification Module (DCM) 和 Contribution-aware Preference Optimization (CPO) 来增强解析器的表现，有效解决对话中的模糊性问题，并在实验中显著优于现有方法。", "motivation": "对话中的语言特性，如省略和成语，会导致意义上的模糊，阻碍话语关系的识别，所以提出了解决这些模糊性的方法。", "method": "提出Discourse-aware Clarification Module (DCM)，结合澄清类型推理和话语目标推理来解析话语关系，并引入Contribution-aware Preference Optimization (CPO) 以优化DCM的表现。", "result": "在STAC和Molweni数据集上的广泛实验表明，该方法能有效解决模糊性，并显著优于现有的基线方法。", "conclusion": "通过DCM和CPO的引入，本文提出的方法可以有效地解析对话话语关系，显著提高了解析的准确度。"}}
{"id": "2506.14854", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14854", "abs": "https://arxiv.org/abs/2506.14854", "authors": ["Varun Mannam", "Zhenyu Shi"], "title": "Efficient Retail Video Annotation: A Robust Key Frame Generation Approach for Product and Customer Interaction Analysis", "comment": "Submitting to ICCV 2025 workshop: https://retailvisionworkshop.github.io/", "summary": "Accurate video annotation plays a vital role in modern retail applications, including customer behavior analysis, product interaction detection, and in-store activity recognition. However, conventional annotation methods heavily rely on time-consuming manual labeling by human annotators, introducing non-robust frame selection and increasing operational costs. To address these challenges in the retail domain, we propose a deep learning-based approach that automates key-frame identification in retail videos and provides automatic annotations of products and customers. Our method leverages deep neural networks to learn discriminative features by embedding video frames and incorporating object detection-based techniques tailored for retail environments. Experimental results showcase the superiority of our approach over traditional methods, achieving accuracy comparable to human annotator labeling while enhancing the overall efficiency of retail video annotation. Remarkably, our approach leads to an average of 2 times cost savings in video annotation. By allowing human annotators to verify/adjust less than 5% of detected frames in the video dataset, while automating the annotation process for the remaining frames without reducing annotation quality, retailers can significantly reduce operational costs. The automation of key-frame detection enables substantial time and effort savings in retail video labeling tasks, proving highly valuable for diverse retail applications such as shopper journey analysis, product interaction detection, and in-store security monitoring.", "AI": {"tldr": "This paper proposes a deep learning-based solution for automating key-frame identification and annotation in retail videos, reducing costs and manual labor while maintaining annotation accuracy.", "motivation": "The motivation is to overcome the issues of time-consuming manual labeling by human annotators in retail video annotation, which introduces non-robust frame selection and increases operational costs.", "method": "Our method leverages deep neural networks to automates key-frame identification in retail videos and provides automatic annotations of products and customers. It incorporates object detection techniques tailored for retail environments.", "result": "The method achieves annotation accuracy comparable to human labeling while enhancing process efficiency. It also leads to cost savings of up to 2 times in video annotation.", "conclusion": "The approach automates the video annotation process, reducing the operational costs for retailers and making it more efficient."}}
{"id": "2506.15118", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15118", "abs": "https://arxiv.org/abs/2506.15118", "authors": ["Junke Wang", "Hongshun Ling", "Li Zhang", "Longqian Zhang", "Fang Wang", "Yuan Gao", "Zhi Li"], "title": "CKD-EHR:Clinical Knowledge Distillation for Electronic Health Records", "comment": "20 pages,5 figures", "summary": "Electronic Health Records (EHR)-based disease prediction models have demonstrated significant clinical value in promoting precision medicine and enabling early intervention. However, existing large language models face two major challenges: insufficient representation of medical knowledge and low efficiency in clinical deployment. To address these challenges, this study proposes the CKD-EHR (Clinical Knowledge Distillation for EHR) framework, which achieves efficient and accurate disease risk prediction through knowledge distillation techniques. Specifically, the large language model Qwen2.5-7B is first fine-tuned on medical knowledge-enhanced data to serve as the teacher model.It then generates interpretable soft labels through a multi-granularity attention distillation mechanism. Finally, the distilled knowledge is transferred to a lightweight BERT student model. Experimental results show that on the MIMIC-III dataset, CKD-EHR significantly outperforms the baseline model:diagnostic accuracy is increased by 9%, F1-score is improved by 27%, and a 22.2 times inference speedup is achieved. This innovative solution not only greatly improves resource utilization efficiency but also significantly enhances the accuracy and timeliness of diagnosis, providing a practical technical approach for resource optimization in clinical settings. The code and data for this research are available athttps://github.com/209506702/CKD_EHR.", "AI": {"tldr": "研究提出CKD-EHR框架，通过知识蒸馏技术实现高效准确的疾病风险预测，并展示了在诊断准确性、F1分数和推理速度上的显著提升。", "motivation": "现有大型语言模型在医疗知识的表示和临床部署的效率上存在两大挑战，本研究旨在通过CKD-EHR框架解决这些问题。", "method": "通过对医疗知识增强的数据微调大型语言模型Qwen2.5-7B来作为教师模型，然后通过多粒度注意力蒸馏机制生成可解释的软标签，最后将蒸馏的知识转移到一个轻量级的BERT学生模型中，从而实现高效准确的疾病风险预测。", "result": "实验结果显示，在MIMIC-III数据集上，CKD-EHR相比基线模型显著提升：诊断准确率提升9%，F1分数提高27%，推理速度提升了22.2倍。", "conclusion": "该创新解决方案不仅极大地提高了资源使用效率，也显著提升了诊断的准确性和及时性，为临床环境中的资源优化提供了一个实用的技术方法。"}}
{"id": "2506.14856", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14856", "abs": "https://arxiv.org/abs/2506.14856", "authors": ["Zhengquan Zhang", "Feng Xu", "Mengmi Zhang"], "title": "Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction", "comment": "9 pages, 3 figures in the main text. Under review for NeurIPS 2025", "summary": "Some perspectives naturally provide more information than others. How can an AI system determine which viewpoint offers the most valuable insight for accurate and efficient 3D object reconstruction? Active view selection (AVS) for 3D reconstruction remains a fundamental challenge in computer vision. The aim is to identify the minimal set of views that yields the most accurate 3D reconstruction. Instead of learning radiance fields, like NeRF or 3D Gaussian Splatting, from a current observation and computing uncertainty for each candidate viewpoint, we introduce a novel AVS approach guided by neural uncertainty maps predicted by a lightweight feedforward deep neural network, named UPNet. UPNet takes a single input image of a 3D object and outputs a predicted uncertainty map, representing uncertainty values across all possible candidate viewpoints. By leveraging heuristics derived from observing many natural objects and their associated uncertainty patterns, we train UPNet to learn a direct mapping from viewpoint appearance to uncertainty in the underlying volumetric representations. Next, our approach aggregates all previously predicted neural uncertainty maps to suppress redundant candidate viewpoints and effectively select the most informative one. Using these selected viewpoints, we train 3D neural rendering models and evaluate the quality of novel view synthesis against other competitive AVS methods. Remarkably, despite using half of the viewpoints than the upper bound, our method achieves comparable reconstruction accuracy. In addition, it significantly reduces computational overhead during AVS, achieving up to a 400 times speedup along with over 50\\% reductions in CPU, RAM, and GPU usage compared to baseline methods. Notably, our approach generalizes effectively to AVS tasks involving novel object categories, without requiring any additional training.", "AI": {"tldr": "本文提出了UPNet，一种轻量级的前馈深度神经网络，它从单个输入图像中预测出3D对象的各种观景点的不确定性，并选择出最有信息量的观景点，从而实现高效的3D对象重构，且与现有方法相比，在减少计算资源消耗的同时，保持了较高的重构精度。", "motivation": "旨在解决3D重建中主动视图选择（AVS）的基本挑战，即如何用最少的视图实现最准确的3D重建。", "method": "提出了一种新的AVS方法，使用UPNet预测神经不确定性图，并利用从大量自然物体及其不确定性模式中导出的启发式规则来选择最重要的视点。", "result": "相比基线方法，所提方法在使用一半观景点的情况下，达到了类似的重建精度，并且在AVS过程中实现了高达400倍的速度提升，以及50%以上的CPU、RAM和GPU资源消耗的减少。", "conclusion": "所提出的方法不仅在减少计算资源消耗方面取得了显著进步，而且能够有效地推广到涉及新对象类别的AVS任务，无需额外训练。"}}
{"id": "2506.15131", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15131", "abs": "https://arxiv.org/abs/2506.15131", "authors": ["Jing Yang Lee", "Kong-Aik Lee", "Woon-Seng Gan"], "title": "Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs", "comment": null, "summary": "Open-domain Dialogue (OD) exhibits a one-to-many (o2m) property, whereby multiple appropriate responses exist for a single dialogue context. Despite prior research showing that modeling this property boosts response diversity, most modern LLM-based dialogue agents do not explicitly do so. In this work, we model the o2m property of OD in LLMs by decomposing OD generation into two key tasks: Multi-Response Generation (MRG) and Preference-based Selection (PS), which entail generating a set of n semantically and lexically diverse high-quality responses for a given dialogue context, followed by selecting a single response based on human preference, respectively. To facilitate MRG and PS, we introduce o2mDial, a dialogue corpus explicitly designed to capture the o2m property by featuring multiple plausible responses for each context. Leveraging o2mDial, we propose new in-context learning and instruction-tuning strategies, as well as novel evaluation metrics for MRG, alongside a model-based approach for PS. Empirical results demonstrate that applying the proposed two-stage framework to smaller LLMs for OD generation enhances overall response diversity while maintaining contextual coherence, improving response quality by up to 90%, bringing them closer to the performance of larger models.", "AI": {"tldr": "研究论文提出了一种新的框架来增强小尺寸LLM在开放域对话生成中的响应多样性，通过将对话生成分解为多响应生成和基于偏好的选择，并使用特定的数据集和新策略来改善响应的质量和多样性。", "motivation": "尽管先前的研究表明，对开放领域对话（OD）的一对多性质进行建模可以增加响应的多样性，但大多数现代基于LLM的对话代理并未明确这么做。因此，该研究旨在通过建模一对多性质来提升对话代理的响应多样性。", "method": "该研究通过将开放领域对话生成分解为两个主要任务来建模其一对多的性质：多响应生成（MRG）和基于偏好的选择（PS）。具体而言，MRG旨在为给定的对话上下文生成一组n个语义和词汇上多样且高质量的响应，而PS则根据人类偏好在多个响应中选择一个。", "result": "实验结果表明，应用提出的两阶段框架可以提升小模型的响应多样性，同时保持上下文连贯性，响应质量最多提高90%。", "conclusion": "该研究展示了通过建模开放领域对话的一对多性质，可以有效提升小模型的响应质量和多样性，使其性能更接近大模型。"}}
{"id": "2506.14903", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14903", "abs": "https://arxiv.org/abs/2506.14903", "authors": ["Renjith Prasad", "Abhilekh Borah", "Hasnat Md Abdullah", "Chathurangi Shyalika", "Gurpreet Singh", "Ritvik Garimella", "Rajarshi Roy", "Harshul Surana", "Nasrin Imanpour", "Suranjana Trivedy", "Amit Sheth", "Amitava Das"], "title": "DETONATE: A Benchmark for Text-to-Image Alignment and Kernelized Direct Preference Optimization", "comment": "59 pages, 10 figures", "summary": "Alignment is crucial for text-to-image (T2I) models to ensure that generated images faithfully capture user intent while maintaining safety and fairness. Direct Preference Optimization (DPO), prominent in large language models (LLMs), is extending its influence to T2I systems. This paper introduces DPO-Kernels for T2I models, a novel extension enhancing alignment across three dimensions: (i) Hybrid Loss, integrating embedding-based objectives with traditional probability-based loss for improved optimization; (ii) Kernelized Representations, employing Radial Basis Function (RBF), Polynomial, and Wavelet kernels for richer feature transformations and better separation between safe and unsafe inputs; and (iii) Divergence Selection, expanding beyond DPO's default Kullback-Leibler (KL) regularizer by incorporating Wasserstein and R'enyi divergences for enhanced stability and robustness. We introduce DETONATE, the first large-scale benchmark of its kind, comprising approximately 100K curated image pairs categorized as chosen and rejected. DETONATE encapsulates three axes of social bias and discrimination: Race, Gender, and Disability. Prompts are sourced from hate speech datasets, with images generated by leading T2I models including Stable Diffusion 3.5 Large, Stable Diffusion XL, and Midjourney. Additionally, we propose the Alignment Quality Index (AQI), a novel geometric measure quantifying latent-space separability of safe/unsafe image activations, revealing hidden vulnerabilities. Empirically, we demonstrate that DPO-Kernels maintain strong generalization bounds via Heavy-Tailed Self-Regularization (HT-SR). DETONATE and complete code are publicly released.", "AI": {"tldr": "本文提出了DPO-Kernels框架，旨在提升文本到图像(T2I)模型的对齐性能，通过三种方式：混合损失、核化表示和分歧选择。同时，文章引入了DETONATE数据集以及AQI指标来评估模型对齐质量，并展示了DPO-Kernels在保持安全性和避免偏见上的优越表现。", "motivation": "旨在改进T2I模型生成图像时的忠实度，安全性和公平性。文章希望通过引入DPO-Kernels框架，解决T2I模型在生成图像时可能存在的偏差问题，提升生成图像的质量和对齐度。", "method": "采用DPO-Kernels框架，该框架包含三种技术：混合损失，使用基于嵌入和基于概率的损失；核化表示，采用RBF、多项式和小波核函数；以及分歧选择，引入Wasserstein和R'enyi分歧。", "result": "提出了DETONATE数据集，涵盖种族、性别和残疾三个社会偏见轴线，并引入AQI作为评估模型对齐质量的新指标。实验表明，DPO-Kernels能够通过重尾自我正则化维持良好的泛化性能。", "conclusion": "文章验证了DPO-Kernels在提高T2I模型生成图像的忠实度、安全性及避免偏见的有效性，并公开发布了DETONATE数据集和代码。"}}
{"id": "2506.15138", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15138", "abs": "https://arxiv.org/abs/2506.15138", "authors": ["Gyeongje Cho", "Yeonkyoun So", "Chanwoo Park", "Sangmin Lee", "Sungmok Jung", "Jaejin Lee"], "title": "Thunder-Tok: Minimizing Tokens per Word in Tokenizing Korean Texts for Generative Language Models", "comment": null, "summary": "This paper introduces Thunder-Tok, a new Korean tokenizer designed to reduce token fertility without compromising model performance. Our approach uses a rule-based pre-tokenization method that aligns with the linguistic structure of the Korean language. We also create a seed vocabulary containing tokens that resemble linguistic units and employ a branching entropy-based selection algorithm. These techniques increase the average token length, thus lowering fertility while preserving linguistic information. Experimental results indicate that Thunder-Tok reduces fertility by approximately 10% (i.e., reduces the number of tokens by 10%, improving the inference speed by 10%) compared to BPE without compromising performance across various downstream tasks. These findings demonstrate that our linguistically informed approach is effective and practical for designing efficient tokenizers for language models.", "AI": {"tldr": "The paper introduces Thunder-Tok, a Korean tokenizer that reduces token fertility by modifying pre-tokenization and employing a branching entropy-based selection algorithm, thus improving inference speed without affecting model performance.", "motivation": "To reduce token fertility without compromising model performance.", "method": "Our approach uses a rule-based pre-tokenization method that aligns with the linguistic structure of the Korean language. We also create a seed vocabulary containing tokens that resemble linguistic units and employ a branching entropy-based selection algorithm.", "result": "Experimental results indicate that Thunder-Tok reduces fertility by approximately 10% (i.e., reduces the number of tokens by 10%, improving the inference speed by 10%) compared to BPE without compromising performance across various downstream tasks.", "conclusion": "Our linguistically informed approach is effective and practical for designing efficient tokenizers for language models."}}
{"id": "2506.14907", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14907", "abs": "https://arxiv.org/abs/2506.14907", "authors": ["Yizhen Zhang", "Yang Ding", "Shuoshuo Zhang", "Xinchen Zhang", "Haoling Li", "Zhong-zhi Li", "Peijie Wang", "Jie Wu", "Lei Ji", "Yelong Shen", "Yujiu Yang", "Yeyun Gong"], "title": "PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning", "comment": null, "summary": "Inspired by the impressive reasoning capabilities demonstrated by reinforcement learning approaches like DeepSeek-R1, recent emerging research has begun exploring the use of reinforcement learning (RL) to enhance vision-language models (VLMs) for multimodal reasoning tasks. However, most existing multimodal reinforcement learning approaches remain limited to spatial reasoning within single-image contexts, yet still struggle to generalize to more complex and real-world scenarios involving multi-image positional reasoning, where understanding the relationships across images is crucial. To address this challenge, we propose a general reinforcement learning approach PeRL tailored for interleaved multimodal tasks, and a multi-stage strategy designed to enhance the exploration-exploitation trade-off, thereby improving learning efficiency and task performance. Specifically, we introduce permutation of image sequences to simulate varied positional relationships to explore more spatial and positional diversity. Furthermore, we design a rollout filtering mechanism for resampling to focus on trajectories that contribute most to learning optimal behaviors to exploit learned policies effectively. We evaluate our model on 5 widely-used multi-image benchmarks and 3 single-image benchmarks. Our experiments confirm that PeRL trained model consistently surpasses R1-related and interleaved VLM baselines by a large margin, achieving state-of-the-art performance on multi-image benchmarks, while preserving comparable performance on single-image tasks.", "AI": {"tldr": "本文提出PeRL方法以解决多模态强化学习在处理多图像位置推理中的挑战，通过改进探索策略和利用过滤机制增强了性能，实验表明该方法在多图像任务上表现出色，并在单图像任务上也有良好表现。", "motivation": "为了解决现有的多模态强化学习方法在处理多图像位置推理任务时的局限性，特别是这些方法在理解和处理图像间关系方面的能力有限。", "method": "提出了一种通用的强化学习方法PeRL，配合多阶段策略，包括重新排列图像序列来模拟多样化的图像位置关系，以及rollout过滤机制来精炼学习轨迹。", "result": "本文受DeepSeek-R1等强化学习方法在推理能力方面的启发，探讨了如何通过强化学习技术来提升视觉语言模型（VLMs）在多模态推理任务中的表现，特别是针对多图像位置推理中的挑战。作者提出了一种通用的强化学习方法PeRL，专用于交错多模态任务的处理，并设计了多阶段策略来平衡探索与利用，以提高学习效率和任务性能。通过重排图像序列来模拟多样的位置关系以增强空间和位置的多样性探索，并采用 rollout 过滤机制重新采样，专注于对学习最优行为最有贡献的轨迹。实验结果显示，PeRL 模型在五个广泛使用的多图像基准和三个单图像基准测试上，相比 R1 相关和交错的 VLM 基线表现显著提升，实现了多图像基准测试的最先进性能，同时在单图像任务上也保持了相近的性能表现。", "conclusion": "PeRL模型在多图像基准测试上表现优于相关强化学习和交错VLM方法，达到了最先进的性能水平，并且在单图像任务上同样保持了良好的性能表现。"}}
{"id": "2506.15156", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15156", "abs": "https://arxiv.org/abs/2506.15156", "authors": ["Muhammad Cendekia Airlangga", "Hilal AlQuabeh", "Munachiso S Nwadike", "Kentaro Inui"], "title": "Emergence of Primacy and Recency Effect in Mamba: A Mechanistic Point of View", "comment": null, "summary": "We study memory in state-space language models using primacy and recency effects as behavioral tools to uncover how information is retained and forgotten over time. Applying structured recall tasks to the Mamba architecture, we observe a consistent U-shaped accuracy profile, indicating strong performance at the beginning and end of input sequences. We identify three mechanisms that give rise to this pattern. First, long-term memory is supported by a sparse subset of channels within the model's selective state space block, which persistently encode early input tokens and are causally linked to primacy effects. Second, short-term memory is governed by delta-modulated recurrence: recent inputs receive more weight due to exponential decay, but this recency advantage collapses when distractor items are introduced, revealing a clear limit to memory depth. Third, we find that memory allocation is dynamically modulated by semantic regularity: repeated relations in the input sequence shift the delta gating behavior, increasing the tendency to forget intermediate items. We validate these findings via targeted ablations and input perturbations on two large-scale Mamba-based language models: one with 1.4B and another with 7B parameters.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.14919", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14919", "abs": "https://arxiv.org/abs/2506.14919", "authors": ["Xinkai Zhao", "Yuta Tokuoka", "Junichiro Iwasawa", "Keita Oda"], "title": "Frequency-Calibrated Membership Inference Attacks on Medical Image Diffusion Models", "comment": null, "summary": "The increasing use of diffusion models for image generation, especially in sensitive areas like medical imaging, has raised significant privacy concerns. Membership Inference Attack (MIA) has emerged as a potential approach to determine if a specific image was used to train a diffusion model, thus quantifying privacy risks. Existing MIA methods often rely on diffusion reconstruction errors, where member images are expected to have lower reconstruction errors than non-member images. However, applying these methods directly to medical images faces challenges. Reconstruction error is influenced by inherent image difficulty, and diffusion models struggle with high-frequency detail reconstruction. To address these issues, we propose a Frequency-Calibrated Reconstruction Error (FCRE) method for MIAs on medical image diffusion models. By focusing on reconstruction errors within a specific mid-frequency range and excluding both high-frequency (difficult to reconstruct) and low-frequency (less informative) regions, our frequency-selective approach mitigates the confounding factor of inherent image difficulty. Specifically, we analyze the reverse diffusion process, obtain the mid-frequency reconstruction error, and compute the structural similarity index score between the reconstructed and original images. Membership is determined by comparing this score to a threshold. Experiments on several medical image datasets demonstrate that our FCRE method outperforms existing MIA methods.", "AI": {"tldr": "提出了频率校准重建误差（FCRE）方法，专门用于解决由扩散模型生成的医学图像的成员推理攻击（MIA），该方法关注特定频率范围的重建误差，并在实验中表现出比现有方法更好的效果。", "motivation": "由于扩散模型在图像生成中的广泛应用，尤其是在医学成像等敏感领域引发了重要的隐私问题。现有的MIA方法依赖于扩散重建误差，但是对于医学图像而言，高频率细节重建困难且重建误差受到图像难度的影响。因此，亟需提出一个解决这些问题的方法。", "method": "通过在逆扩散过程中专注于特定的中频范围内的重建误差，并排除难以重建的高频部分和信息较少的低频部分，我们提出了一种称为频率校准重建误差（FCRE）的方法来解决医学图像扩散模型的成员推理攻击问题。通过计算重构图像与原始图像在该频段内的结构相似性指数得分，并与阈值比较来判断成员身份。", "result": "实验表明，我们的FCRE方法在多个医学图像数据集上优于现有的MIA方法。", "conclusion": "本研究提出了一种有效的方法来解决医学图像扩散模型的隐私问题，为保障医学数据在AI模型训练中的隐私安全提供了一个新的策略。"}}
{"id": "2506.15208", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15208", "abs": "https://arxiv.org/abs/2506.15208", "authors": ["Andrea Cadeddu", "Alessandro Chessa", "Vincenzo De Leo", "Gianni Fenu", "Enrico Motta", "Francesco Osborne", "Diego Reforgiato Recupero", "Angelo Salatino", "Luca Secchi"], "title": "A Comparative Study of Task Adaptation Techniques of Large Language Models for Identifying Sustainable Development Goals", "comment": "Submitted to IEEE Access", "summary": "In 2012, the United Nations introduced 17 Sustainable Development Goals (SDGs) aimed at creating a more sustainable and improved future by 2030. However, tracking progress toward these goals is difficult because of the extensive scale and complexity of the data involved. Text classification models have become vital tools in this area, automating the analysis of vast amounts of text from a variety of sources. Additionally, large language models (LLMs) have recently proven indispensable for many natural language processing tasks, including text classification, thanks to their ability to recognize complex linguistic patterns and semantics. This study analyzes various proprietary and open-source LLMs for a single-label, multi-class text classification task focused on the SDGs. Then, it also evaluates the effectiveness of task adaptation techniques (i.e., in-context learning approaches), namely Zero-Shot and Few-Shot Learning, as well as Fine-Tuning within this domain. The results reveal that smaller models, when optimized through prompt engineering, can perform on par with larger models like OpenAI's GPT (Generative Pre-trained Transformer).", "AI": {"tldr": "研究分析了多种专用和开源的大规模语言模型（LLM）在单标签多类文本分类任务中的应用，该任务旨在对联合国可持续发展目标（SDGs）进行分类。研究还评估了任务适应技术（如零样本学习和少样本学习）的有效性。结果显示，通过提示工程进行优化的小型模型可以与像OpenAI的GPT这样的大型模型表现相当。", "motivation": "由于数据规模和复杂性，追踪联合国可持续发展目标的进展颇具挑战性。文本分类模型已经成为自动化分析大量文本的重要工具。本文旨在通过评估大规模语言模型和任务适应技术，提高SDGs相关文本分类的效率和准确性。", "method": "研究比较了各种专用和开源的大规模语言模型，用于单标签多类文本分类任务，专注于可持续发展目标的分类。它利用了零样本学习、少样本学习和微调的适应技术。", "result": "结果显示，经过提示工程优化的小型模型能够达到与大型模型如OpenAI GPT相当的表现。", "conclusion": "本文的结论表明，即使在处理复杂的SDGs文本分类任务时，通过有效利用任务适应技术，较小的模型也可能具有竞争力。这为研究人员和实践者提供了宝贵的见解，关于如何利用现有多样的语言模型来促进实现可持续发展目标的工作。"}}
{"id": "2506.14934", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14934", "abs": "https://arxiv.org/abs/2506.14934", "authors": ["Md Abrar Jahin", "Shahriar Soudeep", "Arian Rahman Aditta", "M. F. Mridha", "Nafiz Fahad", "Md. Jakir Hossen"], "title": "Vision Transformers for End-to-End Quark-Gluon Jet Classification from Calorimeter Images", "comment": "Accepted in Third International Workshop on Generalizing from Limited Resources in the Open World Workshop at International Joint Conference on Artificial Intelligence (IJCAI) 2025", "summary": "Distinguishing between quark- and gluon-initiated jets is a critical and challenging task in high-energy physics, pivotal for improving new physics searches and precision measurements at the Large Hadron Collider. While deep learning, particularly Convolutional Neural Networks (CNNs), has advanced jet tagging using image-based representations, the potential of Vision Transformer (ViT) architectures, renowned for modeling global contextual information, remains largely underexplored for direct calorimeter image analysis, especially under realistic detector and pileup conditions. This paper presents a systematic evaluation of ViTs and ViT-CNN hybrid models for quark-gluon jet classification using simulated 2012 CMS Open Data. We construct multi-channel jet-view images from detector-level energy deposits (ECAL, HCAL) and reconstructed tracks, enabling an end-to-end learning approach. Our comprehensive benchmarking demonstrates that ViT-based models, notably ViT+MaxViT and ViT+ConvNeXt hybrids, consistently outperform established CNN baselines in F1-score, ROC-AUC, and accuracy, highlighting the advantage of capturing long-range spatial correlations within jet substructure. This work establishes the first systematic framework and robust performance baselines for applying ViT architectures to calorimeter image-based jet classification using public collider data, alongside a structured dataset suitable for further deep learning research in this domain.", "AI": {"tldr": "该论文使用Vision Transformer及其与CNN的混合模型对夸克和胶子喷注分类进行了系统性评估，尤其是基于模拟的2012年CMS公开数据。实验表明，ViT模型在分类准确性和性能指标上超越了现有的CNN基准模型。", "motivation": "由于在高能物理学中对区分夸克和胶子喷注的需求至关重要，而基于图像表示的CNN在深度学习领域已经取得了进展，但对ViT架构对直接分析电磁和强子量能器图像的潜力研究不足，尤其是在实际的探测器和堆积条件下。该论文研究旨在填补这一空白。", "method": "该论文采用Vision Transformer (ViT)及其与CNN的混合模型对基于模拟2012年CMS公开数据的夸克-胶子喷注分类进行了系统评估。研究构造了多通道喷注视图图像，利用探测器级别的能量沉积（ECAL，HCAL）和重建轨道，实现了端到端的学习方法。", "result": "研究表明，基于ViT的模型，特别是ViT+MaxViT和ViT+ConvNeXt混合模型，在F1分数、ROC-AUC和精度方面一致超过现有的CNN基准模型，突显了捕捉喷注子结构中的长程空间相关性的优势。", "conclusion": "这篇论文为利用ViT架构进行基于量能器图像的喷注分类提供了首个系统化框架和强大性能基准，并提供了一个适合该领域继续开展深度学习研究的结构化数据集。"}}
{"id": "2506.15211", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15211", "abs": "https://arxiv.org/abs/2506.15211", "authors": ["Feng He", "Zijun Chen", "Xinnian Liang", "Tingting Ma", "Yunqi Qiu", "Shuangzhi Wu", "Junchi Yan"], "title": "ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning in LLMs", "comment": null, "summary": "Recent advances in Large Reasoning Models (LRMs) trained with Long Chain-of-Thought (Long CoT) reasoning have demonstrated remarkable cross-domain generalization capabilities. However, the underlying mechanisms supporting such transfer remain poorly understood. We hypothesize that cross-domain generalization arises from shared abstract reasoning prototypes -- fundamental reasoning patterns that capture the essence of problems across domains. These prototypes minimize the nuances of the representation, revealing that seemingly diverse tasks are grounded in shared reasoning structures.Based on this hypothesis, we propose ProtoReasoning, a framework that enhances the reasoning ability of LLMs by leveraging scalable and verifiable prototypical representations (Prolog for logical reasoning, PDDL for planning).ProtoReasoning features: (1) an automated prototype construction pipeline that transforms problems into corresponding prototype representations; (2) a comprehensive verification system providing reliable feedback through Prolog/PDDL interpreters; (3) the scalability to synthesize problems arbitrarily within prototype space while ensuring correctness. Extensive experiments show that ProtoReasoning achieves 4.7% improvement over baseline models on logical reasoning (Enigmata-Eval), 6.3% improvement on planning tasks, 4.0% improvement on general reasoning (MMLU) and 1.0% on mathematics (AIME24). Significantly, our ablation studies confirm that learning in prototype space also demonstrates enhanced generalization to structurally similar problems compared to training solely on natural language representations, validating our hypothesis that reasoning prototypes serve as the foundation for generalizable reasoning in large language models.", "AI": {"tldr": "我们提出了ProtoReasoning框架，利用原型表示来增强大型语言模型的跨域泛化能力，并通过实验验证了其有效性。", "motivation": "尽管大型推理模型（LRMs）在长链式推理中的跨域泛化能力显著提高，但其背后的机制尚未完全理解。我们的假设是跨域泛化源于共享的抽象推理原型----基本的推理模式，它能捕捉跨域问题的本质。", "method": "我们提出了ProtoReasoning框架，通过利用可扩展和可验证的原型表示（如用于逻辑推理的Prolog和用于规划的PDDL）来增强大型语言模型的推理能力。ProtoReasoning具有：（1）一个自动构建原型的管道，将问题转化为相应的原型表示；（2）一个提供可靠反馈的综合验证系统；（3）在原型空间中合成任意问题的能力，同时确保正确性。", "result": "广泛的实验表明，与基线模型相比，ProtoReasoning在逻辑推理上有4.7%的提升（Enigmata-Eval），在规划任务上提升6.3%，在一般推理上提升4.0%（MMLU），在数学任务上提升1%（AIME24）。我们的消融研究表明，学习原型空间有助于增强对结构相似问题的泛化，相较于只在自然语言表示上训练，效果更好，验证了我们的假设。", "conclusion": "实验验证了分析原型空间在提升大规模语言模型的可泛化推理能力中的重要性。"}}
{"id": "2506.14980", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.14980", "abs": "https://arxiv.org/abs/2506.14980", "authors": ["Ziteng Li", "Malte Kuhlmann", "Ilana Nisky", "Nicolás Navarro-Guerrero"], "title": "Advances in Compliance Detection: Novel Models Using Vision-Based Tactile Sensors", "comment": "Accepted in the IEEE International Conference on Development and Learning (ICDL). The paper contains 8 pages and 7 figures", "summary": "Compliance is a critical parameter for describing objects in engineering, agriculture, and biomedical applications. Traditional compliance detection methods are limited by their lack of portability and scalability, rely on specialized, often expensive equipment, and are unsuitable for robotic applications. Moreover, existing neural network-based approaches using vision-based tactile sensors still suffer from insufficient prediction accuracy. In this paper, we propose two models based on Long-term Recurrent Convolutional Networks (LRCNs) and Transformer architectures that leverage RGB tactile images and other information captured by the vision-based sensor GelSight to predict compliance metrics accurately. We validate the performance of these models using multiple metrics and demonstrate their effectiveness in accurately estimating compliance. The proposed models exhibit significant performance improvement over the baseline. Additionally, we investigated the correlation between sensor compliance and object compliance estimation, which revealed that objects that are harder than the sensor are more challenging to estimate.", "AI": {"tldr": "本文提出了两种基于LRCNs和Transformer的模型，利用RGB触觉图像进行合规性检测，展示了优于传统方法的性能。研究表明物体硬度高于传感器时，合规性估计更难。", "motivation": "传统合规性检测方法存在便携性差、规模性受限等问题，并且依赖于专业且昂贵的设备，不适合机器人应用。现有的基于神经网络的方法仍然存在预测准确性不足的问题。因此，作者提出了新的模型以解决这些问题。", "method": "本研究提出了两种基于长短期递归卷积网络（LRCNs）和Transformer架构的模型，利用RGB触觉图像及其他GelSight视觉传感器捕捉的信息来准确预测合规性指标。", "result": "实验结果显示，提出的模型在多个评估指标下表现出色，并且在准确估计合规性方面优于基线模型。", "conclusion": "研究指出，对于比传感器硬度更高的物体，其合规性的估计更为困难。"}}
{"id": "2506.15215", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15215", "abs": "https://arxiv.org/abs/2506.15215", "authors": ["Yongqi Fan", "Yating Wang", "Guandong Wang", "Jie Zhai", "Jingping Liu", "Qi Ye", "Tong Ruan"], "title": "MinosEval: Distinguishing Factoid and Non-Factoid for Tailored Open-Ended QA Evaluation with LLMs", "comment": null, "summary": "Open-ended question answering (QA) is a key task for evaluating the capabilities of large language models (LLMs). Compared to closed-ended QA, it demands longer answer statements, more nuanced reasoning processes, and diverse expressions, making refined and interpretable automatic evaluation both crucial and challenging. Traditional metrics like ROUGE and BERTScore struggle to capture semantic similarities due to different patterns between model responses and reference answers. Current LLM-based evaluation approaches, such as pairwise or listwise comparisons of candidate answers, lack intuitive interpretability. While pointwise scoring of each response provides some descriptions, it fails to adapt across different question contents. Most notably, existing methods overlook the distinction between factoid and non-factoid questions. To address these challenges, we propose \\textbf{MinosEval}, a novel evaluation method that first distinguishes open-ended questions and then ranks candidate answers using different evaluation strategies. For factoid questions, it applies an adaptive key-point scoring strategy, while for non-factoid questions, it uses an instance-aware listwise ranking strategy. Experiments on multiple open-ended QA datasets, including self-built ones with more candidate responses to complement community resources, show that MinosEval better aligns with human annotations and offers more interpretable results.", "AI": {"tldr": "提出MinosEval方法评估开放式问答，区分事实性和非事实性问题，实验显示其性能优于人类标注。", "motivation": "传统指标难以捕捉语义相似性，现有的基于LLM的评估方法缺乏直观的可解释性，且忽视了事实性和非事实性问题的区别。", "method": "MinosEval是一种新颖的评估方法，它首先区分开放式问题，然后根据不同评估策略对候选答案进行排名。针对事实性问题采用自适应关键点评分策略，非事实性问题则使用实例感知列表排名策略。", "result": "实验结果显示，MinosEval在多个开放式问答数据集上表现优于人类标注，并提供更具解释性的结果。", "conclusion": "MinosEval解决了开放式问答评估中的关键挑战，能够根据不同问题类型提供适应性评分和直观解释。"}}
{"id": "2506.15010", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15010", "abs": "https://arxiv.org/abs/2506.15010", "authors": ["Yijun Lin", "Yao-Yi Chiang"], "title": "Hyper-Local Deformable Transformers for Text Spotting on Historical Maps", "comment": "Published in KDD2024", "summary": "Text on historical maps contains valuable information providing georeferenced historical, political, and cultural contexts. However, text extraction from historical maps is challenging due to the lack of (1) effective methods and (2) training data. Previous approaches use ad-hoc steps tailored to only specific map styles. Recent machine learning-based text spotters (e.g., for scene images) have the potential to solve these challenges because of their flexibility in supporting various types of text instances. However, these methods remain challenges in extracting precise image features for predicting every sub-component (boundary points and characters) in a text instance. This is critical because map text can be lengthy and highly rotated with complex backgrounds, posing difficulties in detecting relevant image features from a rough text region. This paper proposes PALETTE, an end-to-end text spotter for scanned historical maps of a wide variety. PALETTE introduces a novel hyper-local sampling module to explicitly learn localized image features around the target boundary points and characters of a text instance for detection and recognition. PALETTE also enables hyper-local positional embeddings to learn spatial interactions between boundary points and characters within and across text instances. In addition, this paper presents a novel approach to automatically generate synthetic map images, SynthMap+, for training text spotters for historical maps. The experiment shows that PALETTE with SynthMap+ outperforms SOTA text spotters on two new benchmark datasets of historical maps, particularly for long and angled text. We have deployed PALETTE with SynthMap+ to process over 60,000 maps in the David Rumsey Historical Map collection and generated over 100 million text labels to support map searching. The project is released at https://github.com/kartta-foundation/mapkurator-palette-doc.", "AI": {"tldr": "PALETTE 提出了一种用于手绘历史地图的端到端文本识别系统，并开发了一种名为SynthMap+的自动生成合成地图图像的方法以提高模型训练效果。实验表明，PALETTE在历史地图数据集上优于现有方法，特别是在长文本和倾斜文本的识别上。", "motivation": "由于缺乏有效的方法和训练数据，从历史地图中提取文本信息是一项挑战。现有方法对特定风格的地图有效，但缺乏灵活性。", "method": "Structure", "result": "{\"tldr\": \"PALETTE 提出了一种用于手绘历史地图的端到端文本识别系统，并开发了一种名为SynthMap+的自动生成合成地图图像的方法以提高模型训练效果。实验表明，PALETTE在历史地图数据集上优于现有方法，特别是在长文本和倾斜文本的识别上。\", \"motivation\": \"由于缺乏有效的方法和训练数据，从历史地图中提取文本信息是一项挑战。现有方法对特定风格的地图有效，但缺乏灵活性。\", \"method\": \"PALETTE 提出了一个超局部采样模块，用于学习目标边界点和字符周围的局部图像特征。同时，PALETTE 使用超局部位置嵌入来学习边界点和字符之间的空间交互。此外，还开发了一种合成地图图像自动生成方法SynthMap+。\", \"result\": \"实验显示，使用SynthMap+训练的PALETTE在两个新的历史地图基准数据集上优于现有的文本识别方法，特别是在长文本和倾斜文本识别上。\", \"conclusion\": \"PALETTE在David Rumsey历史地图收藏中的6万余张地图上进行了部署，生成了超过1亿个文本标签以支持地图搜索。项目已发布。\"}", "conclusion": "PALETTE在David Rumsey历史地图收藏中的6万余张地图上进行了部署，生成了超过1亿个文本标签以支持地图搜索。项目已发布。"}}
{"id": "2506.15239", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15239", "abs": "https://arxiv.org/abs/2506.15239", "authors": ["Jaione Bengoetxea", "Itziar Gonzalez-Dios", "Rodrigo Agerri"], "title": "Lost in Variation? Evaluating NLI Performance in Basque and Spanish Geographical Variants", "comment": null, "summary": "In this paper, we evaluate the capacity of current language technologies to understand Basque and Spanish language varieties. We use Natural Language Inference (NLI) as a pivot task and introduce a novel, manually-curated parallel dataset in Basque and Spanish, along with their respective variants. Our empirical analysis of crosslingual and in-context learning experiments using encoder-only and decoder-based Large Language Models (LLMs) shows a performance drop when handling linguistic variation, especially in Basque. Error analysis suggests that this decline is not due to lexical overlap, but rather to the linguistic variation itself. Further ablation experiments indicate that encoder-only models particularly struggle with Western Basque, which aligns with linguistic theory that identifies peripheral dialects (e.g., Western) as more distant from the standard. All data and code are publicly available.", "AI": {"tldr": "本文利用NLI任务评估语言技术对巴斯克语和西班牙语变异形式的理解，并发现模型在处理语言变异时表现不佳，特别是在巴斯克语的西部方言。", "motivation": "本文评估当前语言技术对巴斯克语和西班牙语变异形式的理解能力。", "method": "本研究使用自然语言推理（NLI）作为中心任务，引入了一组新的巴斯克语和西班牙语及其变体的手动策划平行数据集。", "result": "实证分析表明，使用编码器仅有和基于解码器的大语言模型（LLMs）在处理语言变异时表现下降，特别是在巴斯克语中。错误分析表明，这种下降不是由于词汇重叠，而是由于语言变异本身。进一步的消融实验表明，编码器仅有模型尤其难以应对西部巴斯克语。", "conclusion": "研究证明，语言变异对语言模型的表现具有显著影响，而这种影响在巴斯克语的西部方言中尤为明显，这与语言学理论相吻合。"}}
{"id": "2506.15033", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15033", "abs": "https://arxiv.org/abs/2506.15033", "authors": ["Gary Song Yan", "Yusen Zhang", "Jinyu Zhao", "Hao Zhang", "Zhangping Yang", "Guanye Xiong", "Yanfei Liu", "Tao Zhang", "Yujie He", "Siyuan Tian", "Yao Gou", "Min Li"], "title": "Break Stylistic Sophon: Are We Really Meant to Confine the Imagination in Style Transfer?", "comment": null, "summary": "In this pioneering study, we introduce StyleWallfacer, a groundbreaking unified training and inference framework, which not only addresses various issues encountered in the style transfer process of traditional methods but also unifies the framework for different tasks. This framework is designed to revolutionize the field by enabling artist level style transfer and text driven stylization. First, we propose a semantic-based style injection method that uses BLIP to generate text descriptions strictly aligned with the semantics of the style image in CLIP space. By leveraging a large language model to remove style-related descriptions from these descriptions, we create a semantic gap. This gap is then used to fine-tune the model, enabling efficient and drift-free injection of style knowledge. Second, we propose a data augmentation strategy based on human feedback, incorporating high-quality samples generated early in the fine-tuning process into the training set to facilitate progressive learning and significantly reduce its overfitting. Finally, we design a training-free triple diffusion process using the fine-tuned model, which manipulates the features of self-attention layers in a manner similar to the cross-attention mechanism. Specifically, in the generation process, the key and value of the content-related process are replaced with those of the style-related process to inject style while maintaining text control over the model. We also introduce query preservation to mitigate disruptions to the original content. Under such a design, we have achieved high-quality image-driven style transfer and text-driven stylization, delivering artist-level style transfer results while preserving the original image content. Moreover, we achieve image color editing during the style transfer process for the first time.", "AI": {"tldr": "介绍StyleWallfacer框架，结合语义风格注入、基于人类反馈的数据增强以及训练自由三重扩散，实现了高质量的风格转移及文本驱动风格化，同时首次融入图像颜色编辑。", "motivation": "旨在解决传统风格转移方法中遇到的各种问题，并统一不同任务的框架。通过创建一个可以实现艺术家级别风格转移和文本驱动风格化的统一训练和推理框架来引领该领域的发展。", "method": "提出了一种基于语义的风格注入方法，利用BLIP生成与风格图像语义严格对齐的文本描述，并通过大型语言模型去除风格相关的描述来创造语义差距，以此调整模型，实现高效且无漂移的风格知识注入。此外，提出了基于人类反馈的数据增强策略，将早期细化过程中生成的高质量样本加入训练集，以促进渐进学习并显著减少过拟合。最后，设计了一个免费训练的三重扩散过程，利用调整好的模型干预自注意力层特征，类似于交叉注意力机制。在生成过程中，替换内容相关的键和值为风格相关的键和值来注入风格，同时保持文本控制，并通过保持查询来减轻对原始内容的干扰。", "result": "实现了高质量的基于图像的风格转移和基于文本的风格化，生成了类似艺术家级别的风格转移结果，同时保留了原始图像内容，并首次在风格转移过程中实现了图像颜色编辑。", "conclusion": "该研究提出的方法显著提升了风格转移的效果，实现了同时保持文本控制和原始内容的高质量风格转移，并拓展至图像颜色编辑，为风格转移领域带来了创新性的贡献。"}}
{"id": "2506.15241", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15241", "abs": "https://arxiv.org/abs/2506.15241", "authors": ["Yang Fan", "Zhang Qi", "Xing Wenqian", "Liu Chang", "Liu Liu"], "title": "Research on Graph-Retrieval Augmented Generation Based on Historical Text Knowledge Graphs", "comment": null, "summary": "This article addresses domain knowledge gaps in general large language models for historical text analysis in the context of computational humanities and AIGC technology. We propose the Graph RAG framework, combining chain-of-thought prompting, self-instruction generation, and process supervision to create a The First Four Histories character relationship dataset with minimal manual annotation. This dataset supports automated historical knowledge extraction, reducing labor costs. In the graph-augmented generation phase, we introduce a collaborative mechanism between knowledge graphs and retrieval-augmented generation, improving the alignment of general models with historical knowledge. Experiments show that the domain-specific model Xunzi-Qwen1.5-14B, with Simplified Chinese input and chain-of-thought prompting, achieves optimal performance in relation extraction (F1 = 0.68). The DeepSeek model integrated with GraphRAG improves F1 by 11% (0.08-0.19) on the open-domain C-CLUE relation extraction dataset, surpassing the F1 value of Xunzi-Qwen1.5-14B (0.12), effectively alleviating hallucinations phenomenon, and improving interpretability. This framework offers a low-resource solution for classical text knowledge extraction, advancing historical knowledge services and humanities research.", "AI": {"tldr": "本文提出Graph RAG框架，在少量人工标注下创建了第一史记人物关系数据集，改进了历史知识抽取，提升了DeepSeek模型在C-CLUE关系抽取数据集上的表现，有效缓解了幻觉现象，并提高了模型的可解释性。", "motivation": "解决普遍大型语言模型在分析历史文本和计算人文领域中所存在的领域知识空白，并减少劳动成本。", "method": "Graph RAG框架结合链式思维提示、自指令生成和过程监督来创建具有少量人工标注的第一史记人物关系数据集，以支持自动化的知识抽取，并在图增强生成阶段引入知识图谱和检索增强生成之间的协作机制，提高历史知识模型与通用模型的对齐程度。", "result": "使用简体中文输入和链式思维提示的专有模型Xunzi-Qwen1.5-14B在关系抽取中达到了0.68的F1值。使用GraphRAG的DeepSeek模型在C-CLUE数据集上提升了11%的F1值，缓解了幻觉现象，增强了可解释性。", "conclusion": "所提出的框架为古典文本知识抽取提供了低资源解决方案，推进了历史知识服务和人文研究。"}}
{"id": "2506.15078", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15078", "abs": "https://arxiv.org/abs/2506.15078", "authors": ["Xianghong Fang", "Litao Guo", "Hengchao Chen", "Yuxuan Zhang", "XiaofanXia", "Dingjie Song", "Yexin Liu", "Hao Wang", "Harry Yang", "Yuan Yuan", "Qiang Sun"], "title": "Enhancing Vector Quantization with Distributional Matching: A Theoretical and Empirical Study", "comment": null, "summary": "The success of autoregressive models largely depends on the effectiveness of vector quantization, a technique that discretizes continuous features by mapping them to the nearest code vectors within a learnable codebook. Two critical issues in existing vector quantization methods are training instability and codebook collapse. Training instability arises from the gradient discrepancy introduced by the straight-through estimator, especially in the presence of significant quantization errors, while codebook collapse occurs when only a small subset of code vectors are utilized during training. A closer examination of these issues reveals that they are primarily driven by a mismatch between the distributions of the features and code vectors, leading to unrepresentative code vectors and significant data information loss during compression. To address this, we employ the Wasserstein distance to align these two distributions, achieving near 100\\% codebook utilization and significantly reducing the quantization error. Both empirical and theoretical analyses validate the effectiveness of the proposed approach.", "AI": {"tldr": "The paper addresses training instability and codebook collapse in autoregressive models by aligning the distributions of features and code vectors using Wasserstein distance, leading to better codebook utilization and reduced quantization error.", "motivation": "To improve the performance of autoregressive models by mitigating the problems of training instability and codebook collapse in vector quantization, caused by a mismatch between feature and code vector distributions.", "method": "The method involves using the Wasserstein distance to better align the distributions of the feature vectors with those of the code vectors to enhance codebook utilization.", "result": "The proposed method achieves nearly 100% codebook utilization and reduces quantization error. Empirical and theoretical analyses confirm the effectiveness of the approach.", "conclusion": "The approach effectively solves the issues of training instability and codebook collapse by using Wasserstein distance to ensure proper alignment of the feature and code vector distributions, leading to improved model performance."}}
{"id": "2506.15246", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15246", "abs": "https://arxiv.org/abs/2506.15246", "authors": ["Juli Bakagianni", "John Pavlopoulos", "Aristidis Likas"], "title": "TopClustRAG at SIGIR 2025 LiveRAG Challenge", "comment": null, "summary": "We present TopClustRAG, a retrieval-augmented generation (RAG) system developed for the LiveRAG Challenge, which evaluates end-to-end question answering over large-scale web corpora. Our system employs a hybrid retrieval strategy combining sparse and dense indices, followed by K-Means clustering to group semantically similar passages. Representative passages from each cluster are used to construct cluster-specific prompts for a large language model (LLM), generating intermediate answers that are filtered, reranked, and finally synthesized into a single, comprehensive response. This multi-stage pipeline enhances answer diversity, relevance, and faithfulness to retrieved evidence. Evaluated on the FineWeb Sample-10BT dataset, TopClustRAG ranked 2nd in faithfulness and 7th in correctness on the official leaderboard, demonstrating the effectiveness of clustering-based context filtering and prompt aggregation in large-scale RAG systems.", "AI": {"tldr": "研究提出了一种名为TopClustRAG的检索增强生成系统，该系统使用了一个包含混合检索策略和K-Means聚类的多阶段管道，以增强答案的质量。此系统在FineWeb Sample-10BT数据集上的表现证明了它在增强RAG系统答案忠实度和正确性方面的有效性。", "motivation": "目的是在大规模网络语料库中进行端到端的问题回答评估，通过这种方式增强答案的多样性、相关性和对检索证据的忠实度。", "method": "采用了一种结合稀疏和稠密索引的混合检索策略，随后使用K-Means聚类算法对语义相似的段落进行分组。从每个聚类中选择有代表性的段落来构建针对大语言模型的聚类特定提示，生成中间答案，然后对这些答案进行筛选、重排序，最后综合成一个全面性的最终答复。", "result": "在FineWeb Sample-10BT数据集上，TopClustRAG在忠实度方面排名第2，在正确性方面排名第7。", "conclusion": "结果表明，基于聚类的上下文过滤和提示聚合方法在大规模RAG系统中是有效的。"}}
{"id": "2506.15153", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15153", "abs": "https://arxiv.org/abs/2506.15153", "authors": ["Yufei Liu", "Haoke Xiao", "Jiaxing Chai", "Yongcun Zhang", "Rong Wang", "Zijie Meng", "Zhiming Luo"], "title": "SynPo: Boosting Training-Free Few-Shot Medical Segmentation via High-Quality Negative Prompts", "comment": null, "summary": "The advent of Large Vision Models (LVMs) offers new opportunities for few-shot medical image segmentation. However, existing training-free methods based on LVMs fail to effectively utilize negative prompts, leading to poor performance on low-contrast medical images. To address this issue, we propose SynPo, a training-free few-shot method based on LVMs (e.g., SAM), with the core insight: improving the quality of negative prompts. To select point prompts in a more reliable confidence map, we design a novel Confidence Map Synergy Module by combining the strengths of DINOv2 and SAM. Based on the confidence map, we select the top-k pixels as the positive points set and choose the negative points set using a Gaussian distribution, followed by independent K-means clustering for both sets. Then, these selected points are leveraged as high-quality prompts for SAM to get the segmentation results. Extensive experiments demonstrate that SynPo achieves performance comparable to state-of-the-art training-based few-shot methods.", "AI": {"tldr": "为了解决现有基于LVM的无需训练方法在低对比度医学图像上表现不佳的问题，我们提出了一种新的方法SynPo，通过改进负样本提示的质量，达到了与基于训练的少样本方法相当的性能。", "motivation": "尽管LVM为少样本医学图像分割提供了新的机遇，但现有的无需训练的方法在利用负样本提示上存在不足，导致在低对比度医学图像上的表现不佳。SynPo通过改进负样本提示的质量解决这一问题。", "method": "我们提出了SynPo，这是一种基于LVM（如SAM）的无需训练的少样本方法，核心思想是提升负样本提示的质量。通过结合DINOv2和SAM的优势，我们设计了一个新的置信度图协同模块来选取更可靠的点提示。基于置信度图，选取前k个像素作为正样本点集，并通过高斯分布选择负样本点集，随后对两个集合分别进行独立的K-means聚类。这些选定点被用作高质量提示给SAM，以获得分割结果。", "result": "实验表明，SynPo能够与最先进的基于训练的少样本方法相媲美。", "conclusion": "实验结果表明，SynPo达到了与最先进的基于训练的少样本方法相当的性能。"}}
{"id": "2506.15266", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15266", "abs": "https://arxiv.org/abs/2506.15266", "authors": ["Sungen Hahm", "Heejin Kim", "Gyuseong Lee", "Hyunji Park", "Jaejin Lee"], "title": "Thunder-DeID: Accurate and Efficient De-identification Framework for Korean Court Judgments", "comment": null, "summary": "To ensure a balance between open access to justice and personal data protection, the South Korean judiciary mandates the de-identification of court judgments before they can be publicly disclosed. However, the current de-identification process is inadequate for handling court judgments at scale while adhering to strict legal requirements. Additionally, the legal definitions and categorizations of personal identifiers are vague and not well-suited for technical solutions. To tackle these challenges, we propose a de-identification framework called Thunder-DeID, which aligns with relevant laws and practices. Specifically, we (i) construct and release the first Korean legal dataset containing annotated judgments along with corresponding lists of entity mentions, (ii) introduce a systematic categorization of Personally Identifiable Information (PII), and (iii) develop an end-to-end deep neural network (DNN)-based de-identification pipeline. Our experimental results demonstrate that our model achieves state-of-the-art performance in the de-identification of court judgments.", "AI": {"tldr": "我们提出了Thunder-DeID框架，解决法院判决去标识化的挑战，并通过实验展示了其优异的性能。", "motivation": "为了平衡司法公开和隐私保护，韩国司法系统要求对公开的法院判决进行去标识化处理。然而，目前的处理流程无法大规模处理法院判决，同时满足严格的法律要求。此外，个人标识符的法律定义和分类也不适合技术解决方案。", "method": "我们提出了一个名为Thunder-DeID的去标识化框架，该框架包括构建首个包含注释判决和实体提及列表的韩语法律数据集，引入一个系统的个人身份信息（PII）分类，以及开发一个基于深度神经网络（DNN）的端到端去标识化管道。", "result": "实验结果表明，我们的模型在法院判决的去标识化中达到了最先进的水平。", "conclusion": "我们的模型在法院判决的去标识化方面达到了最先进的性能。"}}
{"id": "2506.15160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15160", "abs": "https://arxiv.org/abs/2506.15160", "authors": ["Jiaqi Shi", "Jin Xiao", "Xiaoguang Hu", "Boyang Song", "Hao Jiang", "Tianyou Chen", "Baochang Zhang"], "title": "Enhancing point cloud analysis via neighbor aggregation correction based on cross-stage structure correlation", "comment": "17 papes, 7 figures", "summary": "Point cloud analysis is the cornerstone of many downstream tasks, among which aggregating local structures is the basis for understanding point cloud data. While numerous works aggregate neighbor using three-dimensional relative coordinates, there are irrelevant point interference and feature hierarchy gap problems due to the limitation of local coordinates. Although some works address this limitation by refining spatial description though explicit modeling of cross-stage structure, these enhancement methods based on direct geometric structure encoding have problems of high computational overhead and noise sensitivity. To overcome these problems, we propose the Point Distribution Set Abstraction module (PDSA) that utilizes the correlation in the high-dimensional space to correct the feature distribution during aggregation, which improves the computational efficiency and robustness. PDSA distinguishes the point correlation based on a lightweight cross-stage structural descriptor, and enhances structural homogeneity by reducing the variance of the neighbor feature matrix and increasing classes separability though long-distance modeling. Additionally, we introducing a key point mechanism to optimize the computational overhead. The experimental result on semantic segmentation and classification tasks based on different baselines verify the generalization of the method we proposed, and achieve significant performance improvement with less parameter cost. The corresponding ablation and visualization results demonstrate the effectiveness and rationality of our method. The code and training weight is available at: https://github.com/AGENT9717/PointDistribution", "AI": {"tldr": "提出了PDSA模块解决点云分析中特征层次差距和计算量大等问题，通过高维特征关联优化点云聚合过程，实验表明该方法在语义分割和分类任务中性能提升显著。", "motivation": "由于三维相对坐标限制，现有的点云聚合方法存在无关点干扰和特征层次差距的问题。基于直接几何结构编码的改进方法也有计算量大和噪声敏感的问题。", "method": "PDSA模块通过利用高维空间中的关联性在聚合过程中修正特征分布，采用轻量级的跨阶段结构描述符区分点关联，并通过长距离建模减少邻近特征矩阵的方差和增加类别可分性。此外，引入了关键点机制来优化计算开销。", "result": "基于不同基线的语义分割和分类任务的实验结果验证了所提方法的通用性，并且在参数成本更低的情况下实现了显著的性能提升。消融实验和可视化结果进一步证明了方法的有效性和合理性。", "conclusion": "PDSA模块在提升点云分析的计算效率和鲁棒性方面表现出色，实验结果证明其有效性和广泛的适用性。"}}
{"id": "2506.15301", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15301", "abs": "https://arxiv.org/abs/2506.15301", "authors": ["Shrestha Ghosh", "Moritz Schneider", "Carina Reinicke", "Carsten Eickhoff"], "title": "Cohort Discovery: A Survey on LLM-Assisted Clinical Trial Recruitment", "comment": null, "summary": "Recent advances in LLMs have greatly improved general-domain NLP tasks. Yet, their adoption in critical domains, such as clinical trial recruitment, remains limited. As trials are designed in natural language and patient data is represented as both structured and unstructured text, the task of matching trials and patients benefits from knowledge aggregation and reasoning abilities of LLMs. Classical approaches are trial-specific and LLMs with their ability to consolidate distributed knowledge hold the potential to build a more general solution. Yet recent applications of LLM-assisted methods rely on proprietary models and weak evaluation benchmarks. In this survey, we are the first to analyze the task of trial-patient matching and contextualize emerging LLM-based approaches in clinical trial recruitment. We critically examine existing benchmarks, approaches and evaluation frameworks, the challenges to adopting LLM technologies in clinical research and exciting future directions.", "AI": {"tldr": "论文分析了在临床试验患者匹配中应用LLM的潜力和挑战，探讨了现存评估框架的局限性，并指出了未来发展方向。", "motivation": "虽然LLM在通用NLP任务上取得显著进步，但在关键领域的应用仍然有限，特别是临床试验招募方面。论文旨在探讨如何利用LLM解决临床试验与患者匹配问题，并克服现有技术局限。", "method": "该论文通过分析临床试验患者匹配任务，概述了现有的基准测试、方法和评估框架，并探讨了在临床研究中采用LLM技术面临的挑战及未来发展方向。", "result": "论文首次全面分析了临床试验患者匹配任务，并审查了现有基线测试方法及其局限性。", "conclusion": "论文提出，尽管LLM在临床试验患者匹配任务中有广泛应用前景，但仍面临技术、评估和基准测试等方面的挑战；未来将进一步探索这些挑战的解决方案。"}}
{"id": "2506.15166", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15166", "abs": "https://arxiv.org/abs/2506.15166", "authors": ["Abdur Rahman", "Keerthiveena Balraj", "Manojkumar Ramteke", "Anurag Singh Rathore"], "title": "Echo-DND: A dual noise diffusion model for robust and precise left ventricle segmentation in echocardiography", "comment": "Version of record published in Discover Applied Sciences (Springer Nature). The definitive article is available at https://doi.org/10.1007/s42452-025-07055-5", "summary": "Recent advancements in diffusion probabilistic models (DPMs) have revolutionized image processing, demonstrating significant potential in medical applications. Accurate segmentation of the left ventricle (LV) in echocardiograms is crucial for diagnostic procedures and necessary treatments. However, ultrasound images are notoriously noisy with low contrast and ambiguous LV boundaries, thereby complicating the segmentation process. To address these challenges, this paper introduces Echo-DND, a novel dual-noise diffusion model specifically designed for this task. Echo-DND leverages a unique combination of Gaussian and Bernoulli noises. It also incorporates a multi-scale fusion conditioning module to improve segmentation precision. Furthermore, it utilizes spatial coherence calibration to maintain spatial integrity in segmentation masks. The model's performance was rigorously validated on the CAMUS and EchoNet-Dynamic datasets. Extensive evaluations demonstrate that the proposed framework outperforms existing SOTA models. It achieves high Dice scores of 0.962 and 0.939 on these datasets, respectively. The proposed Echo-DND model establishes a new standard in echocardiogram segmentation, and its architecture holds promise for broader applicability in other medical imaging tasks, potentially improving diagnostic accuracy across various medical domains. Project page: https://abdur75648.github.io/Echo-DND", "AI": {"tldr": "本文介绍了Echo-DND，一种专为准确分割心超声图像中的左室设计的双重噪声扩散模型。该模型在两个数据集上实现了高Dice分数，表明其优于现有的方法。", "motivation": "尽管超声图像以噪点严重、对比度低和左室边界含糊不清著称，是分割过程中的一个挑战，该论文旨在通过引入Echo-DND来解决这些问题。", "method": "Echo-DND, 一种专门用于此任务的双重噪声扩散模型，结合了高斯噪声和伯努利噪声，并采用了多尺度融合条件模块来提高分割精度，同时利用空间一致性校准来维护分割掩模的空间完整性。", "result": "模型的性能在CAMUS和EchoNet-Dynamic数据集上进行了严格的验证。广泛的评估表明，所提出的框架优于现有的SOTA模型，分别在这些数据集上达到了0.962和0.939的高Dice分数。", "conclusion": "Echo-DND模型在心超声图像分割方面设立了新标准，并且其架构对于其他医学成像任务也具有广泛适用性的潜力，有望提高各种医学领域的诊断准确性。"}}
{"id": "2506.15304", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15304", "abs": "https://arxiv.org/abs/2506.15304", "authors": ["Negar Foroutan", "Jakhongir Saydaliev", "Ye Eun Kim", "Antoine Bosselut"], "title": "ConLID: Supervised Contrastive Learning for Low-Resource Language Identification", "comment": "Submitted to EMNLP", "summary": "Language identification (LID) is a critical step in curating multilingual LLM pretraining corpora from web crawls. While many studies on LID model training focus on collecting diverse training data to improve performance, low-resource languages -- often limited to single-domain data, such as the Bible -- continue to perform poorly. To resolve these class imbalance and bias issues, we propose a novel supervised contrastive learning (SCL) approach to learn domain-invariant representations for low-resource languages. Through an extensive analysis, we show that our approach improves LID performance on out-of-domain data for low-resource languages by 3.2%, demonstrating its effectiveness in enhancing LID models.", "AI": {"tldr": "我们提出了一种新的监督对比学习方法，有效提升了低资源语言的语言识别性能。", "motivation": "尽管许多关于LID模型训练的研究集中在收集多样化的训练数据以提高性能上，但低资源语言的数据通常局限于特定领域，如《圣经》，这些语言的表现依然不佳。", "method": "我们提出了一种新的监督对比学习（SCL）方法，用于学习低资源语言的领域不变表示，以解决类别不平衡和偏差问题。", "result": "通过广泛的分析，我们的方法在低资源语言的域外数据上的语言识别性能提高了3.2%。", "conclusion": "实验结果表明，我们的方法在提升低资源语言的LID模型性能方面是有效的。"}}
{"id": "2506.15180", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15180", "abs": "https://arxiv.org/abs/2506.15180", "authors": ["Ziling Huang", "Yidan Zhang", "Shin'ichi Satoh"], "title": "ReSeDis: A Dataset for Referring-based Object Search across Large-Scale Image Collections", "comment": null, "summary": "Large-scale visual search engines are expected to solve a dual problem at once: (i) locate every image that truly contains the object described by a sentence and (ii) identify the object's bounding box or exact pixels within each hit. Existing techniques address only one side of this challenge. Visual grounding yields tight boxes and masks but rests on the unrealistic assumption that the object is present in every test image, producing a flood of false alarms when applied to web-scale collections. Text-to-image retrieval excels at sifting through massive databases to rank relevant images, yet it stops at whole-image matches and offers no fine-grained localization. We introduce Referring Search and Discovery (ReSeDis), the first task that unifies corpus-level retrieval with pixel-level grounding. Given a free-form description, a ReSeDis model must decide whether the queried object appears in each image and, if so, where it is, returning bounding boxes or segmentation masks. To enable rigorous study, we curate a benchmark in which every description maps uniquely to object instances scattered across a large, diverse corpus, eliminating unintended matches. We further design a task-specific metric that jointly scores retrieval recall and localization precision. Finally, we provide a straightforward zero-shot baseline using a frozen vision-language model, revealing significant headroom for future study. ReSeDis offers a realistic, end-to-end testbed for building the next generation of robust and scalable multimodal search systems.", "AI": {"tldr": "提出新的任务Referring Search and Discovery（ReSeDis），该任务结合了语料级检索与像素级定位，能更准确地识别图像中的目标物体及位置，为构建下一代稳健且可扩展的多模态搜索系统提供了一个现实的、端到端的测试平台。", "motivation": "现有的技术只能解决大规模视觉搜索引擎所面临问题的一方面。视觉定位得到了紧的边界框和掩码，但是假设每张测试图像中都包含目标物体，应用于网络规模的集合时会产生大量的误报。文本到图像检索在从大量数据库中筛选相关图像方面表现出色，但它只返回整个图像的匹配，不提供细粒度的定位。因此，提出了一种新的任务，旨在同时解决这两个问题。", "method": "ReSeDis 任务结合了语料级检索与像素级定位，要求模型根据自由形式的描述判断目标物体是否出现在每张图片中，如果出现还要返回物体的边界框或分割掩码。", "result": "建立了一个基准测试，其中每个描述都唯一对应于散布在大型、多样化语料库中的物体实例，消除了无意中的匹配。设计了一个任务特定的评价指标，联合评分检索召回率和定位精度。提供了一种使用冻结的视觉语言模型的无训练基线方法，显示出未来研究有很大的改进空间。", "conclusion": "ReSeDis提供了一个真实的、端到端的测试平台，用于构建下一代稳健且可扩展的多模态搜索系统，显示出未来研究有很大的改进空间。"}}
{"id": "2506.15339", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15339", "abs": "https://arxiv.org/abs/2506.15339", "authors": ["Camila Zurdo Tagliabue", "Heloisa Oss Boll", "Aykut Erdem", "Erkut Erdem", "Iacer Calixto"], "title": "DeVisE: Behavioral Testing of Medical Large Language Models", "comment": null, "summary": "Large language models (LLMs) are increasingly used in clinical decision support, yet current evaluation methods often fail to distinguish genuine medical reasoning from superficial patterns. We introduce DeVisE (Demographics and Vital signs Evaluation), a behavioral testing framework for probing fine-grained clinical understanding. We construct a dataset of ICU discharge notes from MIMIC-IV, generating both raw (real-world) and template-based (synthetic) versions with controlled single-variable counterfactuals targeting demographic (age, gender, ethnicity) and vital sign attributes. We evaluate five LLMs spanning general-purpose and medically fine-tuned variants, under both zero-shot and fine-tuned settings. We assess model behavior via (1) input-level sensitivity - how counterfactuals alter the likelihood of a note; and (2) downstream reasoning - how they affect predicted hospital length-of-stay. Our results show that zero-shot models exhibit more coherent counterfactual reasoning patterns, while fine-tuned models tend to be more stable yet less responsive to clinically meaningful changes. Notably, demographic factors subtly but consistently influence outputs, emphasizing the importance of fairness-aware evaluation. This work highlights the utility of behavioral testing in exposing the reasoning strategies of clinical LLMs and informing the design of safer, more transparent medical AI systems.", "AI": {"tldr": "本文提出了DeVisE框架来评估大型语言模型在临床理解中的细微差别，强调了零样本模型在反事实推理中的优势和细分样本模型对临床重要变化的反应不足，以及在评估中需要关注的人口统计学因素的影响。", "motivation": "当前的评估方法往往无法区分真正的医学推理和表面模式，因此本文旨在引入一种新的评估框架，以更有效地检测大规模语言模型在临床决策支持中的细微临床理解和推理能力。", "method": "引入DeVisE（Demographics and Vital signs Evaluation），一个用于检测精细临床理解的行为测试框架。构建了一个基于MIMIC-IV的ICU出院记录数据集，包含真实世界和基于模板的合成版本，并通过控制单变量反事实实例来针对人口统计（年龄、性别、种族）和生命体征属性进行测试。评估了涵盖通用和医疗细分领域的五种大型语言模型（LLMs），包括零样本和细分样本设置。通过（1）输入层面的敏感性，即反事实如何改变笔记的可能性；以及（2）下游推理，即它们如何影响预测的住院时间。", "result": "结论表明，零样本模型表现出更为连贯的反事实推理模式，而细分样本模型则更为稳定但对临床有意义的变化反应不够敏感。值得注意的是，人口统计因素在输出中微妙但一致地影响结果，强调了在评估中关注公平性的重要性。", "conclusion": "这项工作彰显了行为测试在暴露临床大型语言模型推理策略方面的重要性，并为设计更为安全和透明的医疗人工智能系统提供了指导。"}}
