{"id": "2506.11093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.11093", "abs": "https://arxiv.org/abs/2506.11093", "authors": ["Shaibal Saha", "Lanyu Xu"], "title": "EfficientQuant: An Efficient Post-Training Quantization for CNN-Transformer Hybrid Models on Edge Devices", "comment": "Accepted to the 4th Workshop on Transformers for Vision (T4V) at CVPR\n  2025", "summary": "Hybrid models that combine convolutional and transformer blocks offer strong\nperformance in computer vision (CV) tasks but are resource-intensive for edge\ndeployment. Although post-training quantization (PTQ) can help reduce resource\ndemand, its application to hybrid models remains limited. We propose\nEfficientQuant, a novel structure-aware PTQ approach that applies uniform\nquantization to convolutional blocks and $log_2$ quantization to transformer\nblocks. EfficientQuant achieves $2.5 \\times - 8.7 \\times$ latency reduction\nwith minimal accuracy loss on the ImageNet-1K dataset. It further demonstrates\nlow latency and memory efficiency on edge devices, making it practical for\nreal-world deployment.", "AI": {"tldr": "{ Proposes EfficientQuant, a new quantization technique for hybrid CV models, leading to significant performance gains without sacrificing much accuracy, ideal for edge computing }", "motivation": "{ to address the high resource demand of hybrid CV models and enable practical edge deployment }", "method": "{ using structure-aware post-training quantization (EfficientQuant) that applies uniform quantization to convolutional blocks and \\$log_2\\$ quantization to transformer blocks }", "result": "{ efficiency improvement on hybrid models by applying novel quantization method, achieving significant latency reduction with minimal accuracy loss, suitable for edge devices }", "conclusion": "{ EfficientQuant is effective for reducing resource consumption in hybrid models, suitable for edge deployment }"}}
{"id": "2506.11122", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.11122", "abs": "https://arxiv.org/abs/2506.11122", "authors": ["Divya Swetha K", "Ziaul Haque Choudhury", "Hemanta Kumar Bhuyan", "Biswajit Brahma", "Nilayam Kumar Kamila"], "title": "Adaptive Object Detection with ESRGAN-Enhanced Resolution & Faster R-CNN", "comment": null, "summary": "In this study, proposes a method for improved object detection from the\nlow-resolution images by integrating Enhanced Super-Resolution Generative\nAdversarial Networks (ESRGAN) and Faster Region-Convolutional Neural Network\n(Faster R-CNN). ESRGAN enhances low-quality images, restoring details and\nimproving clarity, while Faster R-CNN performs accurate object detection on the\nenhanced images. The combination of these techniques ensures better detection\nperformance, even with poor-quality inputs, offering an effective solution for\napplications where image resolution is in consistent. ESRGAN is employed as a\npre-processing step to enhance the low-resolution input image, effectively\nrestoring lost details and improving overall image quality. Subsequently, the\nenhanced image is fed into the Faster R-CNN model for accurate object detection\nand localization. Experimental results demonstrate that this integrated\napproach yields superior performance compared to traditional methods applied\ndirectly to low-resolution images. The proposed framework provides a promising\nsolution for applications where image quality is variable or limited, enabling\nmore robust and reliable object detection in challenging scenarios. It achieves\na balance between improved image quality and efficient object detection", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408ESRGAN\u548cFaster R-CNN\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f4e\u5206\u8fa8\u56fe\u50cf\u7684\u9ad8\u7cbe\u5ea6\u76ee\u6807\u68c0\u6d4b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u56fe\u50cf\u8d28\u91cf\u53d8\u5316\u6216\u53d7\u9650\u7684\u573a\u5408\u3002", "motivation": "\u8fd9\u9879\u7814\u7a76\u7684\u52a8\u673a\u662f\u89e3\u51b3\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u7684\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\uff0c\u8fd9\u4e9b\u56fe\u50cf\u901a\u5e38\u7ec6\u8282\u4e22\u5931\uff0c\u7cbe\u5ea6\u4e0d\u9ad8\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7ed3\u5408\u589e\u5f3a\u8d85\u5206\u8fa8\u7387\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08ESRGAN\uff09\u548c\u66f4\u5feb\u533a\u57df\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08Faster R-CNN\uff09\u6765\u6539\u8fdb\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u3002ESRGAN\u7528\u4e8e\u589e\u5f3a\u4f4e\u8d28\u91cf\u56fe\u50cf\uff0c\u6062\u590d\u7ec6\u8282\u5e76\u63d0\u9ad8\u6e05\u6670\u5ea6\uff0c\u800cFaster R-CNN\u5219\u5728\u589e\u5f3a\u540e\u7684\u56fe\u50cf\u4e0a\u6267\u884c\u51c6\u786e\u7684\u76ee\u6807\u68c0\u6d4b\u3002ESRGAN\u4f5c\u4e3a\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u7528\u4e8e\u63d0\u5347\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u56fe\u50cf\u7684\u8d28\u91cf\uff0c\u6062\u590d\u4e22\u5931\u7684\u7ec6\u8282\u5e76\u6539\u5584\u6574\u4f53\u56fe\u50cf\u8d28\u91cf\u3002\u968f\u540e\uff0c\u589e\u5f3a\u540e\u7684\u56fe\u50cf\u88ab\u8f93\u5165\u5230Faster R-CNN\u6a21\u578b\u4e2d\u8fdb\u884c\u51c6\u786e\u5b9a\u4f4d\u548c\u8bc6\u522b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u96c6\u6210\u65b9\u6cd5\u76f8\u6bd4\u76f4\u63a5\u5e94\u7528\u4e8e\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u4f20\u7edf\u65b9\u6cd5\uff0c\u6027\u80fd\u66f4\u4f18\u3002\u8be5\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u56fe\u50cf\u8d28\u91cf\u53d8\u5316\u6216\u53d7\u9650\u7684\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u524d\u9014\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u4e2d\u5b9e\u73b0\u66f4\u9c81\u68d2\u548c\u53ef\u9760\u7684\u76ee\u6807\u68c0\u6d4b\u3002\u5b83\u5b9e\u73b0\u4e86\u56fe\u50cf\u8d28\u91cf\u63d0\u5347\u4e0e\u9ad8\u6548\u76ee\u6807\u68c0\u6d4b\u4e4b\u95f4\u7684\u5e73\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u6846\u67b6\u4e3a\u56fe\u50cf\u8d28\u91cf\u53d8\u5316\u6216\u53d7\u9650\u7684\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u56fe\u50cf\u8d28\u91cf\u63d0\u5347\u4e0e\u9ad8\u6548\u76ee\u6807\u68c0\u6d4b\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u4f7f\u76ee\u6807\u68c0\u6d4b\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u4e2d\u53d8\u5f97\u66f4\u52a0\u53ef\u9760\u548c\u7a33\u5065\u3002"}}
{"id": "2506.11124", "categories": ["cs.CV", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11124", "abs": "https://arxiv.org/abs/2506.11124", "authors": ["Yifei Chen", "Ross Greer"], "title": "Technical Report for Argoverse2 Scenario Mining Challenges on Iterative Error Correction and Spatially-Aware Prompting", "comment": null, "summary": "Scenario mining from extensive autonomous driving datasets, such as Argoverse\n2, is crucial for the development and validation of self-driving systems. The\nRefAV framework represents a promising approach by employing Large Language\nModels (LLMs) to translate natural-language queries into executable code for\nidentifying relevant scenarios. However, this method faces challenges,\nincluding runtime errors stemming from LLM-generated code and inaccuracies in\ninterpreting parameters for functions that describe complex multi-object\nspatial relationships. This technical report introduces two key enhancements to\naddress these limitations: (1) a fault-tolerant iterative code-generation\nmechanism that refines code by re-prompting the LLM with error feedback, and\n(2) specialized prompt engineering that improves the LLM's comprehension and\ncorrect application of spatial-relationship functions. Experiments on the\nArgoverse 2 validation set with diverse LLMs-Qwen2.5-VL-7B, Gemini 2.5 Flash,\nand Gemini 2.5 Pro-show consistent gains across multiple metrics; most notably,\nthe proposed system achieves a HOTA-Temporal score of 52.37 on the official\ntest set using Gemini 2.5 Pro. These results underline the efficacy of the\nproposed techniques for reliable, high-precision scenario mining.", "AI": {"tldr": "The paper proposes improvements to the RefAV framework for better scenario mining in autonomous driving datasets by using a fault-tolerant iterative code generation and specialized prompt engineering, resulting in significant improvements in precision.", "motivation": "The motivation is to address the challenges faced by the RefAV framework, such as runtime errors from LLM-generated code and inaccuracies in parameter interpretation for complex multi-object spatial relationships, for the reliable and accurate scenario mining from autonomous driving datasets like Argoverse 2.", "method": "The paper introduces two key enhancements to the RefAV framework: a fault-tolerant iterative code-generation mechanism for refining code by re-prompting the LLM with error feedback and specialized prompt engineering for better understanding and application of spatial-relationship functions.", "result": "Experiments on the Argoverse 2 validation set with various LLMs show consistent gains across multiple metrics, with the system achieving a HOTA-Temporal score of 52.37 on the official test set using Gemini 2.5 Pro.", "conclusion": "The proposed enhancements effectively improve the reliability and precision of scenario mining from extensive autonomous driving datasets, suggesting a viable path for the development and validation of self-driving systems."}}
{"id": "2506.11126", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.11126", "abs": "https://arxiv.org/abs/2506.11126", "authors": ["Artem Solomko", "Oleg Kartashev", "Andrey Golov", "Mikhail Deulin", "Vadim Valynkin", "Vasily Kharin"], "title": "Image-Based Method For Measuring And Classification Of Iron Ore Pellets Using Star-Convex Polygons", "comment": "15 pages, 41 figures", "summary": "We would like to present a comprehensive study on the classification of iron\nore pellets, aimed at identifying quality violations in the final product,\nalongside the development of an innovative imagebased measurement method\nutilizing the StarDist algorithm, which is primarily employed in the medical\nfield. This initiative is motivated by the necessity to accurately identify and\nanalyze objects within densely packed and unstable environments. The process\ninvolves segmenting these objects, determining their contours, classifying\nthem, and measuring their physical dimensions. This is crucial because the size\ndistribution and classification of pellets such as distinguishing between nice\n(quality) and joint (caused by the presence of moisture or indicating a process\nof production failure) types are among the most significant characteristics\nthat define the quality of the final product. Traditional algorithms, including\nimage classification techniques using Vision Transformer (ViT), instance\nsegmentation methods like Mask R-CNN, and various anomaly segmentation\nalgorithms, have not yielded satisfactory results in this context.\nConsequently, we explored methodologies from related fields to enhance our\napproach. The outcome of our research is a novel method designed to detect\nobjects with smoothed boundaries. This advancement significantly improves the\naccuracy of physical dimension measurements and facilitates a more precise\nanalysis of size distribution among the iron ore pellets. By leveraging the\nstrengths of the StarDist algorithm, we aim to provide a robust solution that\naddresses the challenges posed by the complex nature of pellet classification\nand measurement.", "AI": {"tldr": "A novel StarDist-based method for the accurate classification and measurement of iron ore pellets is developed, overcoming the limitations of conventional algorithms.", "motivation": "The motivation stems from the need to accurately analyze and classify pellets to identify quality issues, which cannot be satisfactorily addressed by traditional algorithms like ViT, Mask R-CNN, and anomaly segmentation algorithms.", "method": "The paper develops an image-based measurement method using the StarDist algorithm, focusing on segmenting, contouring, classifying, and measuring iron ore pellets in densely packed environments.", "result": "The outcome is a new method for detecting objects with smoothed boundaries that enhances the precision of physical dimension measurements and size distribution analysis.", "conclusion": "The research offers a robust solution for the classification and measurement of iron ore pellets, making use of the StarDist algorithm to improve on the limitations of existing methods."}}
{"id": "2506.11131", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.11131", "abs": "https://arxiv.org/abs/2506.11131", "authors": ["Tanner Schmidt", "Richard Newcombe"], "title": "Segment This Thing: Foveated Tokenization for Efficient Point-Prompted Segmentation", "comment": null, "summary": "This paper presents Segment This Thing (STT), a new efficient image\nsegmentation model designed to produce a single segment given a single point\nprompt. Instead of following prior work and increasing efficiency by decreasing\nmodel size, we gain efficiency by foveating input images. Given an image and a\npoint prompt, we extract a crop centered on the prompt and apply a novel\nvariable-resolution patch tokenization in which patches are downsampled at a\nrate that increases with increased distance from the prompt. This approach\nyields far fewer image tokens than uniform patch tokenization. As a result we\ncan drastically reduce the computational cost of segmentation without reducing\nmodel size. Furthermore, the foveation focuses the model on the region of\ninterest, a potentially useful inductive bias. We show that our Segment This\nThing model is more efficient than prior work while remaining competitive on\nsegmentation benchmarks. It can easily run at interactive frame rates on\nconsumer hardware and is thus a promising tool for augmented reality or\nrobotics applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Segment This Thing (STT) \u6a21\u578b\uff0c\u901a\u8fc7\u53d8\u5206\u8fa8\u7387\u7684\u5757\u6807\u8bb0\u5316\u6280\u672f\uff0c\u63d0\u9ad8\u4e86\u56fe\u50cf\u5206\u5272\u7684\u6548\u7387\uff0c\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u5728\u5206\u5272\u57fa\u51c6\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u9002\u7528\u4e8e\u4e92\u52a8\u5e27\u7387\u9700\u6c42\u7684\u5e94\u7528\uff0c\u5982\u589e\u5f3a\u73b0\u5b9e\u6216\u673a\u5668\u4eba\u9886\u57df\u3002", "motivation": "\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u6539\u8fdb\u56fe\u50cf\u5206\u5272\u6280\u672f\uff0c\u7279\u522b\u662f\u5728\u63d0\u9ad8\u6a21\u578b\u6548\u7387\u65b9\u9762\uff0c\u4ee5\u9002\u5e94\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u5e94\u7528\u7684\u9700\u6c42\u3002\u4e0e\u4ee5\u5f80\u901a\u8fc7\u51cf\u5c0f\u6a21\u578b\u5c3a\u5bf8\u6765\u63d0\u9ad8\u6548\u7387\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u65b0\u7684\u65b9\u6cd5\u6765\u964d\u4f4e\u5206\u5272\u8fc7\u7a0b\u4e2d\u7684\u8ba1\u7b97\u91cf\u3002", "method": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u662f\uff0c\u7ed9\u5b9a\u4e00\u4e2a\u56fe\u50cf\u548c\u4e00\u4e2a\u70b9\u63d0\u793a\uff0c\u901a\u8fc7\u5bf9\u63d0\u793a\u4e2d\u5fc3\u8fdb\u884c\u88c1\u526a\uff0c\u5e76\u8fd0\u7528\u4e00\u79cd\u65b0\u7684\u53d8\u5206\u8fa8\u7387\u5757\u6807\u8bb0\u5316\u6280\u672f\uff0c\u5176\u4e2d\u8fdc\u79bb\u63d0\u793a\u70b9\u7684\u5757\u8fdb\u884c\u66f4\u591a\u7684\u964d\u91c7\u6837\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u56fe\u50cf\u6807\u8bb0\u7684\u6570\u91cf\u3002", "result": "\u76f8\u8f83\u4e8e\u5148\u524d\u7684\u5de5\u4f5c\uff0cSegment This Thing\u6a21\u578b\u5728\u4e0d\u51cf\u5c11\u6a21\u578b\u5c3a\u5bf8\u7684\u540c\u65f6\u51cf\u5c11\u4e86\u5206\u5272\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u4e14\u5728\u5206\u5272\u57fa\u51c6\u4e0a\u4ecd\u5177\u6709\u7ade\u4e89\u529b\u3002\u5b83\u53ef\u4ee5\u5728\u666e\u901a\u7684\u786c\u4ef6\u8bbe\u5907\u4e0a\u8fbe\u5230\u4e92\u52a8\u5e27\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u65b0\u65b9\u6cd5Segment This Thing\u6a21\u578b\uff0c\u5728\u63d0\u9ad8\u56fe\u50cf\u5206\u5272\u6548\u7387\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u826f\u597d\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u80fd\u591f\u5b9e\u65f6\u8fd0\u884c\uff0c\u5177\u5907\u5728\u589e\u5f3a\u73b0\u5b9e\u6216\u673a\u5668\u4eba\u9886\u57df\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.11132", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11132", "abs": "https://arxiv.org/abs/2506.11132", "authors": ["Dylan Green", "Yuting Shang", "Jiaee Cheong", "Yang Liu", "Hatice Gunes"], "title": "Gender Fairness of Machine Learning Algorithms for Pain Detection", "comment": "To appear as part of the 2025 19th International Conference on\n  Automatic Face and Gesture Recognition (FG) Workshop Proceedings", "summary": "Automated pain detection through machine learning (ML) and deep learning (DL)\nalgorithms holds significant potential in healthcare, particularly for patients\nunable to self-report pain levels. However, the accuracy and fairness of these\nalgorithms across different demographic groups (e.g., gender) remain\nunder-researched. This paper investigates the gender fairness of ML and DL\nmodels trained on the UNBC-McMaster Shoulder Pain Expression Archive Database,\nevaluating the performance of various models in detecting pain based solely on\nthe visual modality of participants' facial expressions. We compare traditional\nML algorithms, Linear Support Vector Machine (L SVM) and Radial Basis Function\nSVM (RBF SVM), with DL methods, Convolutional Neural Network (CNN) and Vision\nTransformer (ViT), using a range of performance and fairness metrics. While ViT\nachieved the highest accuracy and a selection of fairness metrics, all models\nexhibited gender-based biases. These findings highlight the persistent\ntrade-off between accuracy and fairness, emphasising the need for\nfairness-aware techniques to mitigate biases in automated healthcare systems.", "AI": {"tldr": "The study focuses on the accuracy and fairness of machine learning and deep learning models in detecting pain from visual expressions, finding that while high accuracy can be achieved with Vision Transformers, there are significant gender-based biases in all models.", "motivation": "To understand and evaluate the fairness and accuracy of different machine learning and deep learning models in automated pain detection, especially across different genders.", "method": "Structure", "result": "{\n  \"tldr\": \"The study focuses on the accuracy and fairness of machine learning and deep learning models in detecting pain from visual expressions, finding that while high accuracy can be achieved with Vision Transformers, there are significant gender-based biases in all models.\",\n  \"motivation\": \"To understand and evaluate the fairness and accuracy of different machine learning and deep learning models in automated pain detection, especially across different genders.\",\n  \"method\": \"The study uses four models\u2014Linear SVM, RBF SVM, CNN, and Vision Transformer\u2014trained on the UNBC-McMaster Shoulder Pain Expression Archive Database to analyze facial expressions for pain detection.\",\n  \"result\": \"Vision Transformer achieved the highest accuracy, but all models showed gender-based biases.\",\n  \"conclusion\": \"High accuracy in pain detection is possible but comes at the cost of fairness, as exhibited by gender biases in models. This highlights the need for fairness-aware techniques in healthcare AI systems. \"\n}", "conclusion": "High accuracy in pain detection is possible but comes at the cost of fairness, as exhibited by gender biases in models. This highlights the need for fairness-aware techniques in healthcare AI systems."}}
{"id": "2506.11133", "categories": ["cs.CV", "cs.GR", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.11133", "abs": "https://arxiv.org/abs/2506.11133", "authors": ["Christos Pantazopoulos", "Spyridon Thermos", "Gerasimos Potamianos"], "title": "Monocular 3D Hand Pose Estimation with Implicit Camera Alignment", "comment": "Code is available at https://github.com/cpantazop/HandRepo", "summary": "Estimating the 3D hand articulation from a single color image is a\ncontinuously investigated problem with applications in Augmented Reality (AR),\nVirtual Reality (VR), Human-Computer Interaction (HCI), and robotics. Apart\nfrom the absence of depth information, occlusions, articulation complexity, and\nthe need for camera parameters knowledge pose additional challenges. In this\nwork, we propose an optimization pipeline for estimating the 3D hand\narticulation from 2D keypoint input, which includes a keypoint alignment step\nand a fingertip loss to overcome the need to know or estimate the camera\nparameters. We evaluate our approach on the EgoDexter and Dexter+Object\nbenchmarks to showcase that our approach performs competitively with the SotA,\nwhile also demonstrating its robustness when processing \"in-the-wild\" images\nwithout any prior camera knowledge. Our quantitative analysis highlights the\nsensitivity of the 2D keypoint estimation accuracy, despite the use of hand\npriors. Code is available at https://github.com/cpantazop/HandRepo", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.11134", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.11134", "abs": "https://arxiv.org/abs/2506.11134", "authors": ["Benedict Schacht", "Imke Greving", "Simone Frintrop", "Berit Zeller-Plumhoff", "Christian Wilms"], "title": "ContextLoss: Context Information for Topology-Preserving Segmentation", "comment": "13 pages, 7 figures, accepted to ICIP 2025", "summary": "In image segmentation, preserving the topology of segmented structures like\nvessels, membranes, or roads is crucial. For instance, topological errors on\nroad networks can significantly impact navigation. Recently proposed solutions\nare loss functions based on critical pixel masks that consider the whole\nskeleton of the segmented structures in the critical pixel mask. We propose the\nnovel loss function ContextLoss (CLoss) that improves topological correctness\nby considering topological errors with their whole context in the critical\npixel mask. The additional context improves the network focus on the\ntopological errors. Further, we propose two intuitive metrics to verify\nimproved connectivity due to a closing of missed connections. We benchmark our\nproposed CLoss on three public datasets (2D & 3D) and our own 3D nano-imaging\ndataset of bone cement lines. Training with our proposed CLoss increases\nperformance on topology-aware metrics and repairs up to 44% more missed\nconnections than other state-of-the-art methods. We make the code publicly\navailable.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570ContextLoss\uff0c\u8be5\u635f\u5931\u51fd\u6570\u5728\u5173\u952e\u50cf\u7d20\u63a9\u6a21\u4e2d\u8003\u8651\u62d3\u6251\u9519\u8bef\u53ca\u5176\u4e0a\u4e0b\u6587\uff0c\u63d0\u5347\u4e86\u5206\u5272\u7684\u62d3\u6251\u6b63\u786e\u6027\u3002", "motivation": "\u5728\u56fe\u50cf\u5206\u5272\u4e2d\uff0c\u4fdd\u6301\u5206\u5272\u7ed3\u6784\u5982\u8840\u7ba1\u3001\u819c\u6216\u9053\u8def\u7684\u62d3\u6251\u7ed3\u6784\u81f3\u5173\u91cd\u8981\u3002\u4f8b\u5982\uff0c\u9053\u8def\u7f51\u7edc\u4e2d\u7684\u62d3\u6251\u9519\u8bef\u53ef\u80fd\u4f1a\u4e25\u91cd\u5f71\u54cd\u5bfc\u822a\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570ContextLoss\uff08CLoss\uff09\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u5173\u952e\u50cf\u7d20\u63a9\u6a21\u4e2d\u8003\u8651\u62d3\u6251\u9519\u8bef\u53ca\u5176\u4e0a\u4e0b\u6587\u6765\u6539\u8fdb\u62d3\u6251\u6b63\u786e\u6027\u3002", "result": "\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8bad\u7ec3\u65f6\u91c7\u7528CLoss\u5728\u62d3\u6251\u611f\u77e5\u5ea6\u91cf\u4e0a\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5e76\u4e14\u4fee\u590d\u4e86\u6bd4\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u591a\u51fa44%\u7684\u9057\u6f0f\u8fde\u63a5\u3002", "conclusion": "\u6211\u4eec\u5efa\u8bae\u7684\u65b0\u65b9\u6cd5\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548c\u81ea\u5df1\u7684\u4e09\u7ef4\u7eb3\u7c73\u6210\u50cf\u6570\u636e\u96c6\u4e2d\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u663e\u793a\u51fa\u663e\u8457\u7684\u6539\u8fdb\u6548\u679c\u3002\u6211\u4eec\u5df2\u7ecf\u5c06\u4ee3\u7801\u516c\u5f00\u3002"}}
{"id": "2506.11136", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.11136", "abs": "https://arxiv.org/abs/2506.11136", "authors": ["Paul Couairon", "Loick Chambon", "Louis Serrano", "Jean-Emmanuel Haugeard", "Matthieu Cord", "Nicolas Thome"], "title": "JAFAR: Jack up Any Feature at Any Resolution", "comment": "Code available at https://github.com/PaulCouairon/JAFAR", "summary": "Foundation Vision Encoders have become essential for a wide range of dense\nvision tasks. However, their low-resolution spatial feature outputs necessitate\nfeature upsampling to produce the high-resolution modalities required for\ndownstream tasks. In this work, we introduce JAFAR, a lightweight and flexible\nfeature upsampler that enhances the spatial resolution of visual features from\nany Foundation Vision Encoder to an arbitrary target resolution. JAFAR employs\nan attention-based module designed to promote semantic alignment between\nhigh-resolution queries, derived from low-level image features, and\nsemantically enriched low-resolution keys, using Spatial Feature Transform\n(SFT) modulation. Notably, despite the absence of high-resolution supervision,\nwe demonstrate that learning at low upsampling ratios and resolutions\ngeneralizes remarkably well to significantly higher output scales. Extensive\nexperiments show that JAFAR effectively recovers fine-grained spatial details\nand consistently outperforms existing feature upsampling methods across a\ndiverse set of downstream tasks. Project page at\nhttps://jafar-upsampler.github.io", "AI": {"tldr": "JAFAR is a novel, lightweight feature upsampler that aligns spatial features to produce high-resolution outputs, outperforming current methods in dense vision tasks.", "motivation": "The necessity of upsampling low-resolution spatial features from vision encoders to high-resolution modalities for downstream tasks motivated the development of JAFAR.", "method": "JAFAR utilizes an attention-based mechanism that aligns high-resolution image feature queries with semantically enriched low-resolution keys through Spatial Feature Transform (SFT) modulation.", "result": "Experiments demonstrate that JAFAR can recover fine-grained spatial details and perform well even at higher output scales without high-resolution supervision.", "conclusion": "JAFAR, a proposed lightweight and flexible feature upsampler, effectively enhances the spatial resolution of visual features from Foundation Vision Encoders and outperforms existing methods across various downstream tasks."}}
{"id": "2506.11140", "categories": ["cs.CV", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.11140", "abs": "https://arxiv.org/abs/2506.11140", "authors": ["Jin Kim", "Muhammad Wahi-Anwa", "Sangyun Park", "Shawn Shin", "John M. Hoffman", "Matthew S. Brown"], "title": "Autonomous Computer Vision Development with Agentic AI", "comment": "The paper is 13 pages long and contains 4 figures", "summary": "Agentic Artificial Intelligence (AI) systems leveraging Large Language Models\n(LLMs) exhibit significant potential for complex reasoning, planning, and tool\nutilization. We demonstrate that a specialized computer vision system can be\nbuilt autonomously from a natural language prompt using Agentic AI methods.\nThis involved extending SimpleMind (SM), an open-source Cognitive AI\nenvironment with configurable tools for medical image analysis, with an\nLLM-based agent, implemented using OpenManus, to automate the planning (tool\nconfiguration) for a particular computer vision task. We provide a\nproof-of-concept demonstration that an agentic system can interpret a computer\nvision task prompt, plan a corresponding SimpleMind workflow by decomposing the\ntask and configuring appropriate tools. From the user input prompt, \"provide sm\n(SimpleMind) config for lungs, heart, and ribs segmentation for cxr (chest\nx-ray)\"), the agent LLM was able to generate the plan (tool configuration file\nin YAML format), and execute SM-Learn (training) and SM-Think (inference)\nscripts autonomously. The computer vision agent automatically configured,\ntrained, and tested itself on 50 chest x-ray images, achieving mean dice scores\nof 0.96, 0.82, 0.83, for lungs, heart, and ribs, respectively. This work shows\nthe potential for autonomous planning and tool configuration that has\ntraditionally been performed by a data scientist in the development of computer\nvision applications.", "AI": {"tldr": "\u672c\u6587\u5229\u7528Agentic AI\u6280\u672f\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u4ece\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u4e2d\u81ea\u4e3b\u6784\u5efa\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\uff0c\u5305\u62ec\u4efb\u52a1\u89c4\u5212\u3001\u5de5\u5177\u914d\u7f6e\u4e0e\u6267\u884c\u3002", "motivation": "\u65e8\u5728\u5c55\u793aAgentic AI\u7cfb\u7edf\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u590d\u6742\u63a8\u7406\u3001\u89c4\u5212\u548c\u5de5\u5177\u5229\u7528\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5b9e\u73b0\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684\u81ea\u52a8\u6784\u5efa\u548c\u6267\u884c\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u5f00\u6e90\u8ba4\u77e5AI\u73af\u5883SimpleMind\uff0c\u6dfb\u52a0\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7406\uff0c\u672c\u6587\u5c55\u793a\u4e86\u4e00\u79cd\u81ea\u52a8\u6784\u5efa\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u7684Agentic AI\u65b9\u6cd5\u3002\u7528\u6237\u53ef\u4ee5\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u63d0\u793a\uff0c\u4ee3\u7406LLM\u80fd\u591f\u5206\u89e3\u4efb\u52a1\u3001\u914d\u7f6e\u9002\u5f53\u7684\u5de5\u5177\u5e76\u6267\u884c\u76f8\u5e94\u7684SimpleMind\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4ece\u800c\u5b9e\u73b0\u81ea\u52a8\u5316\u89c4\u5212\u548c\u5de5\u5177\u914d\u7f6e\u3002", "result": "\u572850\u5f20\u80f8\u90e8X\u5149\u7247\u4e0a\uff0c\u6a21\u578b\u5bf9\u4e8e\u80ba\u3001\u5fc3\u810f\u548c\u808b\u9aa8\u7684\u5206\u5272\u5206\u522b\u8fbe\u5230\u4e860.96\u30010.82\u548c0.83\u7684\u5e73\u5747Dice\u5206\u6570\uff0c\u8bc1\u660e\u4e86Agentic AI\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u81ea\u52a8\u914d\u7f6e\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7Agentic\u7cfb\u7edf\uff0c\u4ece\u7528\u6237\u8f93\u5165\u7684\u201c\u4e3a\u80f8\u90e8X\u5149\u7247\u63d0\u4f9b\u80ba\u3001\u5fc3\u810f\u548c\u808b\u9aa8\u5206\u5272\u7684SimpleMind\u914d\u7f6e\u201d\u63d0\u793a\uff0c\u7cfb\u7edf\u53ef\u4ee5\u81ea\u52a8\u914d\u7f6e\u3001\u8bad\u7ec3\u5e76\u6d4b\u8bd5\u5176\u6a21\u578b\uff0c\u8fbe\u5230\u826f\u597d\u7684\u5206\u5272\u7cbe\u5ea6\u3002\u8fd9\u8868\u660eAgentic AI\u6709\u53ef\u80fd\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u5f00\u53d1\u4e2d\u66ff\u4ee3\u6570\u636e\u79d1\u5b66\u5bb6\u8fdb\u884c\u81ea\u4e3b\u89c4\u5212\u548c\u5de5\u5177\u914d\u7f6e\u3002"}}
{"id": "2506.11142", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.11142", "abs": "https://arxiv.org/abs/2506.11142", "authors": ["Ebenezer Tarubinga", "Jenifer Kalafatovich"], "title": "FARCLUSS: Fuzzy Adaptive Rebalancing and Contrastive Uncertainty Learning for Semi-Supervised Semantic Segmentation", "comment": "Submitted to Pattern Recognition", "summary": "Semi-supervised semantic segmentation (SSSS) faces persistent challenges in\neffectively leveraging unlabeled data, such as ineffective utilization of\npseudo-labels, exacerbation of class imbalance biases, and neglect of\nprediction uncertainty. Current approaches often discard uncertain regions\nthrough strict thresholding favouring dominant classes. To address these\nlimitations, we introduce a holistic framework that transforms uncertainty into\na learning asset through four principal components: (1) fuzzy pseudo-labeling,\nwhich preserves soft class distributions from top-K predictions to enrich\nsupervision; (2) uncertainty-aware dynamic weighting, that modulate pixel-wise\ncontributions via entropy-based reliability scores; (3) adaptive class\nrebalancing, which dynamically adjust losses to counteract long-tailed class\ndistributions; and (4) lightweight contrastive regularization, that encourage\ncompact and discriminative feature embeddings. Extensive experiments on\nbenchmarks demonstrate that our method outperforms current state-of-the-art\napproaches, achieving significant improvements in the segmentation of\nunder-represented classes and ambiguous regions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u89e3\u51b3\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u4e2d\u5b58\u5728\u7684\u4f7f\u7528\u672a\u6807\u8bb0\u6570\u636e\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u4f7f\u7528\u6a21\u7cca\u4f2a\u6807\u7b7e\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u6743\u91cd\u8c03\u6574\u3001\u7c7b\u522b\u518d\u5e73\u8861\u4ee5\u53ca\u8f7b\u91cf\u7ea7\u5bf9\u6bd4\u6b63\u5219\u5316\uff0c\u6709\u6548\u5730\u63d0\u5347\u4e86\u672a\u88ab\u5145\u5206\u8868\u73b0\u7c7b\u522b\u7684\u5206\u5272\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u5728\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e3b\u8981\u8868\u73b0\u4e3a\u5bf9\u4f2a\u6807\u7b7e\u7684\u6709\u6548\u5229\u7528\u4e0d\u8db3\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u504f\u5dee\u52a0\u5267\u4ee5\u53ca\u5ffd\u89c6\u4e86\u9884\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "Semi-supervised semantic segmentation (SSSS)\u9762\u4e34\u7684\u6301\u7eed\u6311\u6218\u662f\u5982\u4f55\u6709\u6548\u5730\u5229\u7528\u672a\u6807\u8bb0\u7684\u6570\u636e\uff0c\u4f8b\u5982\u65e0\u6548\u5730\u4f7f\u7528\u4f2a\u6807\u7b7e\u3001\u52a0\u5267\u7c7b\u522b\u4e0d\u5e73\u8861\u504f\u5dee\u548c\u5ffd\u89c6\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002\u5f53\u524d\u7684\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u4e25\u683c\u7684\u9608\u503c\u6765\u4e22\u5f03\u4e0d\u786e\u5b9a\u533a\u57df\uff0c\u8fd9\u4f1a\u504f\u5411\u4e8e\u4e3b\u8981\u7c7b\u522b\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6574\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u4e3b\u8981\u7ec4\u4ef6\u5c06\u4e0d\u786e\u5b9a\u6027\u8f6c\u5316\u4e3a\u5b66\u4e60\u8d44\u4ea7\uff1a(1) \u6a21\u7cca\u4f2a\u6807\u8bb0\uff0c\u4fdd\u7559\u6765\u81eatop-K\u9884\u6d4b\u7684\u8f6f\u7c7b\u522b\u5206\u5e03\u4ee5\u4e30\u5bcc\u76d1\u7763\uff1b(2) \u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u52a8\u6001\u52a0\u6743\uff0c\u6839\u636e\u57fa\u4e8e\u71b5\u7684\u53ef\u9760\u6027\u5206\u6570\u8c03\u8282\u50cf\u7d20\u8d21\u732e\uff1b(3) \u9002\u5e94\u6027\u7c7b\u522b\u518d\u5e73\u8861\uff0c\u52a8\u6001\u8c03\u6574\u635f\u5931\u4ee5\u5bf9\u6297\u957f\u5c3e\u7c7b\u522b\u5206\u5e03\uff1b(4) \u8f7b\u91cf\u7ea7\u5bf9\u6bd4\u6b63\u5219\u5316\uff0c\u9f13\u52b1\u7d27\u51d1\u7684\u5224\u522b\u7279\u5f81\u5d4c\u5165\u3002", "result": "\u901a\u8fc7\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u8d85\u8fc7\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u6280\u672f\uff0c\u5c24\u5176\u5728\u672a\u5145\u5206\u8868\u73b0\u7c7b\u522b\u7684\u5206\u5272\u548c\u6a21\u68f1\u4e24\u53ef\u533a\u57df\u7684\u5206\u5272\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u4e00\u4e2a\u7efc\u5408\u6846\u67b6\u7684\u5e94\u7528\u53ef\u4ee5\u5c06\u4e0d\u786e\u5b9a\u6027\u8f6c\u5316\u4e3a\u5b66\u4e60\u4f18\u52bf\uff0c\u901a\u8fc7\u6a21\u7cca\u4f2a\u6807\u7b7e\u3001\u50cf\u7d20\u8d21\u732e\u8c03\u6574\u3001\u7c7b\u522b\u5e73\u8861\u8c03\u6574\u548c\u7279\u5f81\u5d4c\u5165\u8c03\u8282\uff0c\u63d0\u9ad8\u4e86\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u7684\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u5bf9\u672a\u5145\u5206\u8868\u8ff0\u7c7b\u522b\u548c\u6a21\u68f1\u4e24\u53ef\u533a\u57df\u7684\u5206\u5272\u8868\u73b0\u4e0a\u3002"}}
{"id": "2506.11143", "categories": ["cs.CV", "H.5; J.4; I.2.7; I.2.10"], "pdf": "https://arxiv.org/pdf/2506.11143", "abs": "https://arxiv.org/abs/2506.11143", "authors": ["Andreea I. Niculescu", "Jochen Ehnen", "Chen Yi", "Du Jiawei", "Tay Chiat Pin", "Joey Tianyi Zhou", "Vigneshwaran Subbaraju", "Teh Kah Kuan", "Tran Huy Dat", "John Komar", "Gi Soong Chee", "Kenneth Kwok"], "title": "On the development of an AI performance and behavioural measures for teaching and classroom management", "comment": "7 pages, 10 figures, A video demonstration of the teacher trainer\n  dashboard can be accessed here: https://vimeo.com/1076482827", "summary": "This paper presents a two-year research project focused on developing\nAI-driven measures to analyze classroom dynamics, with particular emphasis on\nteacher actions captured through multimodal sensor data. We applied real-time\ndata from classroom sensors and AI techniques to extract meaningful insights\nand support teacher development. Key outcomes include a curated audio-visual\ndataset, novel behavioral measures, and a proof-of-concept teaching review\ndashboard. An initial evaluation with eight researchers from the National\nInstitute for Education (NIE) highlighted the system's clarity, usability, and\nits non-judgmental, automated analysis approach -- which reduces manual\nworkloads and encourages constructive reflection. Although the current version\ndoes not assign performance ratings, it provides an objective snapshot of\nin-class interactions, helping teachers recognize and improve their\ninstructional strategies. Designed and tested in an Asian educational context,\nthis work also contributes a culturally grounded methodology to the growing\nfield of AI-based educational analytics.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u4e24\u5e74\u7684\u9879\u76ee\uff0c\u5229\u7528\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\u548cAI\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u5206\u6790\u6559\u5ba4\u52a8\u6001\u7684\u63aa\u65bd\uff0c\u5305\u62ec\u6784\u5efa\u97f3\u9891\u89c6\u9891\u6570\u636e\u96c6\u3001\u65b0\u884c\u4e3a\u6307\u6807\u548c\u6559\u5b66\u8bc4\u5ba1\u4eea\u8868\u677f\u3002\u521d\u6b65\u8bc4\u4f30\u663e\u793a\u7cfb\u7edf\u6e05\u6670\u6613\u7528\uff0c\u4e14\u80fd\u591f\u5ba2\u89c2\u53cd\u6620\u8bfe\u5802\u4e92\u52a8\uff0c\u5e2e\u52a9\u6559\u5e08\u53cd\u601d\u548c\u6539\u8fdb\u6559\u5b66\u7b56\u7565\u3002", "motivation": "\u8be5\u9879\u76ee\u65e8\u5728\u901a\u8fc7AI\u9a71\u52a8\u7684\u65b9\u6cd5\u5206\u6790\u6559\u5ba4\u52a8\u6001\uff0c\u5c24\u5176\u5173\u6ce8\u6559\u5e08\u884c\u4e3a\uff0c\u4ee5\u652f\u6301\u6559\u5e08\u7684\u53d1\u5c55\uff0c\u5e76\u5728\u4e9a\u6d32\u6559\u80b2\u80cc\u666f\u4e0b\u4e3a\u6b64\u9886\u57df\u8d21\u732e\u4e00\u79cd\u6587\u5316\u80cc\u666f\u7684\u65b9\u6cd5\u8bba\u3002", "method": "\u5e94\u7528\u4e86\u5b9e\u65f6\u7684\u6559\u5ba4\u4f20\u611f\u5668\u6570\u636e\u548cAI\u6280\u672f\uff0c\u63d0\u53d6\u6709\u610f\u4e49\u7684\u89c1\u89e3\uff0c\u5e76\u652f\u6301\u6559\u5e08\u7684\u5f00\u53d1\u3002", "result": "\u5173\u952e\u6210\u679c\u5305\u62ec\u4e00\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u89c6\u542c\u6570\u636e\u96c6\uff0c\u65b0\u7684\u884c\u4e3a\u6307\u6807\uff0c\u4ee5\u53ca\u4e00\u4e2a\u6559\u5b66\u5ba1\u67e5\u7684\u539f\u578b\u4eea\u8868\u677f\u3002\u521d\u671f\u7684\u8bc4\u4f30\u8868\u660e\u7cfb\u7edf\u6e05\u6670\u6613\u7528\uff0c\u4e14\u91c7\u7528\u975e\u8bc4\u5224\u6027\u7684\u81ea\u52a8\u5316\u5206\u6790\u65b9\u5f0f\uff0c\u51cf\u8f7b\u4e86\u4eba\u5de5\u5de5\u4f5c\u91cf\u5e76\u9f13\u52b1\u5efa\u8bbe\u6027\u7684\u53cd\u601d\u3002", "conclusion": "\u5c3d\u7ba1\u5f53\u524d\u7248\u672c\u4e0d\u5206\u914d\u8868\u73b0\u8bc4\u5206\uff0c\u4f46\u5b83\u63d0\u4f9b\u4e86\u5bf9\u8bfe\u5802\u4e92\u52a8\u7684\u5ba2\u89c2\u89c6\u89d2\uff0c\u5e2e\u52a9\u6559\u5e08\u8bc6\u522b\u5e76\u6539\u8fdb\u4ed6\u4eec\u7684\u6559\u5b66\u7b56\u7565\u3002\u5728\u4e9a\u6d32\u6559\u80b2\u80cc\u666f\u4e0b\uff0c\u8fd9\u9879\u5de5\u4f5c\u8fd8\u5bf9\u57fa\u4e8eAI\u7684\u6559\u80b2\u5206\u6790\u9886\u57df\u8d21\u732e\u4e86\u4e00\u79cd\u6587\u5316\u5b9a\u4f4d\u7684\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2506.11144", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.11144", "abs": "https://arxiv.org/abs/2506.11144", "authors": ["Chao Liang", "Jianwen Jiang", "Wang Liao", "Jiaqi Yang", "Zerong zheng", "Weihong Zeng", "Han Liang"], "title": "AlignHuman: Improving Motion and Fidelity via Timestep-Segment Preference Optimization for Audio-Driven Human Animation", "comment": "Homepage: https://alignhuman.github.io/", "summary": "Recent advancements in human video generation and animation tasks, driven by\ndiffusion models, have achieved significant progress. However, expressive and\nrealistic human animation remains challenging due to the trade-off between\nmotion naturalness and visual fidelity. To address this, we propose\n\\textbf{AlignHuman}, a framework that combines Preference Optimization as a\npost-training technique with a divide-and-conquer training strategy to jointly\noptimize these competing objectives. Our key insight stems from an analysis of\nthe denoising process across timesteps: (1) early denoising timesteps primarily\ncontrol motion dynamics, while (2) fidelity and human structure can be\neffectively managed by later timesteps, even if early steps are skipped.\nBuilding on this observation, we propose timestep-segment preference\noptimization (TPO) and introduce two specialized LoRAs as expert alignment\nmodules, each targeting a specific dimension in its corresponding timestep\ninterval. The LoRAs are trained using their respective preference data and\nactivated in the corresponding intervals during inference to enhance motion\nnaturalness and fidelity. Extensive experiments demonstrate that AlignHuman\nimproves strong baselines and reduces NFEs during inference, achieving a\n3.3$\\times$ speedup (from 100 NFEs to 30 NFEs) with minimal impact on\ngeneration quality. Homepage:\n\\href{https://alignhuman.github.io/}{https://alignhuman.github.io/}", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aAlignHuman\u7684\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u7ed3\u5408\u504f\u597d\u4f18\u5316\u548c\u5206\u800c\u6cbb\u4e4b\u7684\u7b56\u7565\uff0c\u6539\u5584\u4e86\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u81ea\u7136\u7684\u4eba\u7c7b\u52a8\u753b\u7684\u6027\u80fd\uff0c\u63d0\u5347\u4e86\u901f\u5ea6\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u7531\u4e8e\u5728\u751f\u6210\u548c\u52a8\u753b\u4eba\u7c7b\u89c6\u9891\u7684\u4efb\u52a1\u4e2d\uff0c\u52a8\u4f5c\u81ea\u7136\u6027\u548c\u56fe\u50cf\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u5e26\u6765\u7684\u6311\u6218\uff0c\u4f7f\u5f97\u751f\u6210\u8868\u8fbe\u4e30\u5bcc\u4e14\u771f\u5b9e\u7684\u4eba\u7c7b\u52a8\u753b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aAlignHuman\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u504f\u597d\u4f18\u5316\u548c\u5206\u800c\u6cbb\u4e4b\u7684\u8bad\u7ec3\u7b56\u7565\u6765\u5e73\u8861\u52a8\u4f5c\u81ea\u7136\u6027\u548c\u56fe\u50cf\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u77db\u76fe\u3002\u4f5c\u8005\u53d1\u73b0\uff0c\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\uff0c\u65e9\u671f\u7684\u53bb\u566a\u6b65\u4e3b\u8981\u662f\u5f71\u54cd\u52a8\u4f5c\u7684\u52a8\u6001\u6027\uff0c\u800c\u4fdd\u771f\u5ea6\u548c\u4eba\u4f53\u7ed3\u6784\u5219\u5728\u540e\u671f\u53bb\u566a\u4e2d\u5f97\u5230\u66f4\u597d\u7684\u5904\u7406\u3002\u57fa\u4e8e\u8fd9\u4e00\u89c2\u5bdf\uff0c\u63d0\u51fa\u4e86\u65f6\u95f4\u6b65\u6bb5\u504f\u597d\u4f18\u5316\uff08TPO\uff09\uff0c\u5e76\u5f15\u5165\u4e86\u4e24\u4e2a\u4e13\u95e8\u7684LoRAs\u4f5c\u4e3a\u4e13\u5bb6\u5bf9\u9f50\u6a21\u5757\uff0c\u5206\u522b\u5728\u5404\u81ea\u7684\u65f6\u95f4\u6bb5\u5185\u5904\u7406\u7279\u5b9a\u7684\u7ef4\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cAlignHuman\u63d0\u5347\u4e86\u5f3a\u57fa\u51c6\u7ebf\u7684\u8868\u73b0\u5e76\u51cf\u5c11\u4e86\u63a8\u7406\u4e2d\u7684NFEs\uff0c\u5b9e\u73b0\u4e863.3\u500d\u7684\u52a0\u901f\uff08\u4ece100\u4e2aNFEs\u51cf\u5c11\u523030\u4e2aNFEs\uff09\uff0c\u540c\u65f6\u5bf9\u751f\u6210\u8d28\u91cf\u51e0\u4e4e\u6ca1\u6709\u5f71\u54cd\u3002", "conclusion": "\u901a\u8fc7\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u52a8\u4f5c\u81ea\u7136\u6027\u4e0e\u56fe\u50cf\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u6709\u6548\u5e73\u8861\uff0c\u4e3a\u76f8\u5173\u4efb\u52a1\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u9014\u5f84\u3002"}}
{"id": "2506.11147", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.11147", "abs": "https://arxiv.org/abs/2506.11147", "authors": ["Xiaotang Gai", "Jiaxiang Liu", "Yichen Li", "Zijie Meng", "Jian Wu", "Zuozhu Liu"], "title": "3D-RAD: A Comprehensive 3D Radiology Med-VQA Dataset with Multi-Temporal Analysis and Diverse Diagnostic Tasks", "comment": null, "summary": "Medical Visual Question Answering (Med-VQA) holds significant potential for\nclinical decision support, yet existing efforts primarily focus on 2D imaging\nwith limited task diversity. This paper presents 3D-RAD, a large-scale dataset\ndesigned to advance 3D Med-VQA using radiology CT scans. The 3D-RAD dataset\nencompasses six diverse VQA tasks: anomaly detection, image observation,\nmedical computation, existence detection, static temporal diagnosis, and\nlongitudinal temporal diagnosis. It supports both open- and closed-ended\nquestions while introducing complex reasoning challenges, including\ncomputational tasks and multi-stage temporal analysis, to enable comprehensive\nbenchmarking. Extensive evaluations demonstrate that existing vision-language\nmodels (VLMs), especially medical VLMs exhibit limited generalization,\nparticularly in multi-temporal tasks, underscoring the challenges of real-world\n3D diagnostic reasoning. To drive future advancements, we release a\nhigh-quality training set 3D-RAD-T of 136,195 expert-aligned samples, showing\nthat fine-tuning on this dataset could significantly enhance model performance.\nOur dataset and code, aiming to catalyze multimodal medical AI research and\nestablish a robust foundation for 3D medical visual understanding, are publicly\navailable at https://github.com/Tang-xiaoxiao/M3D-RAD.", "AI": {"tldr": "The paper presents 3D-RAD, a comprehensive 3D medical VQA dataset that addresses the limitations of current 2D-focused efforts and limited task diversity. It provides a robust benchmark for advancing 3D medical visual understanding and AI research.", "motivation": "The motivation is to advance the clinical decision support ability of 3D Med-VQA, as current research focuses mainly on 2D images and lacks diverse tasks.", "method": "This paper introduces 3D-RAD, a large-scale dataset for 3D medical visual question answering (VQA) using radiology CT scans. It includes six diverse tasks with complex reasoning challenges.", "result": "Extensive evaluations show that existing vision-language models (VLMs), including medical ones, have limitations in generalization, especially in tasks involving multiple time points.", "conclusion": "The release of the 3D-RAD-T training set aims to improve the performance of medical VQA models through fine-tuning, with the dataset and code available to the public to facilitate multimodal medical AI research and 3D visual understanding."}}
{"id": "2506.11017", "categories": ["cs.CL", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2506.11017", "abs": "https://arxiv.org/abs/2506.11017", "authors": ["Yanyan Wang", "Yingying Wang", "Junli Liang", "Yin Xu", "Yunlong Liu", "Yiming Xu", "Zhengwang Jiang", "Zhehe Li", "Fei Li", "Long Zhao", "Kuang Xu", "Qi Song", "Xiangyang Li"], "title": "TeleEval-OS: Performance evaluations of large language models for operations scheduling", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has significantly\npropelled progress in artificial intelligence, demonstrating substantial\napplication potential across multiple specialized domains. Telecommunications\noperation scheduling (OS) is a critical aspect of the telecommunications\nindustry, involving the coordinated management of networks, services, risks,\nand human resources to optimize production scheduling and ensure unified\nservice control. However, the inherent complexity and domain-specific nature of\nOS tasks, coupled with the absence of comprehensive evaluation benchmarks, have\nhindered thorough exploration of LLMs' application potential in this critical\nfield. To address this research gap, we propose the first Telecommunications\nOperation Scheduling Evaluation Benchmark (TeleEval-OS). Specifically, this\nbenchmark comprises 15 datasets across 13 subtasks, comprehensively simulating\nfour key operational stages: intelligent ticket creation, intelligent ticket\nhandling, intelligent ticket closure, and intelligent evaluation. To\nsystematically assess the performance of LLMs on tasks of varying complexity,\nwe categorize their capabilities in telecommunications operation scheduling\ninto four hierarchical levels, arranged in ascending order of difficulty: basic\nNLP, knowledge Q&A, report generation, and report analysis. On TeleEval-OS, we\nleverage zero-shot and few-shot evaluation methods to comprehensively assess 10\nopen-source LLMs (e.g., DeepSeek-V3) and 4 closed-source LLMs (e.g., GPT-4o)\nacross diverse scenarios. Experimental results demonstrate that open-source\nLLMs can outperform closed-source LLMs in specific scenarios, highlighting\ntheir significant potential and value in the field of telecommunications\noperation scheduling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7535\u4fe1\u8fd0\u8425\u8c03\u5ea6\u8bc4\u4f30\u57fa\u51c6TeleEval-OS\uff0c\u8bc4\u4f30\u4e86\u5f00\u6e90\u548c\u95ed\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5f00\u6e90\u6a21\u578b\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u4e8e\u95ed\u6e90\u6a21\u578b\u3002", "motivation": "\u7535\u4fe1\u8fd0\u8425\u8c03\u5ea6\u9886\u57df\u56e0\u4efb\u52a1\u590d\u6742\u3001\u4e13\u4e1a\u6027\u5f3a\u4e14\u7f3a\u4e4f\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u9650\u5236\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u521b\u5efa\u4e86\u5305\u542b15\u4e2a\u6570\u636e\u96c6\u548c13\u4e2a\u5b50\u4efb\u52a1\u7684TeleEval-OS\u57fa\u51c6\uff0c\u5206\u4e3a\u56db\u4e2a\u5173\u952e\u8fd0\u8425\u9636\u6bb5\u8fdb\u884c\u4eff\u771f\uff0c\u5e76\u4f7f\u7528\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bc4\u4f30\u65b9\u6cd5\u6765\u8bc4\u6d4b14\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u4f18\u4e8e\u95ed\u6e90\u6a21\u578b\u3002", "conclusion": "\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u5728\u7535\u4fe1\u8fd0\u8425\u8c03\u5ea6\u9886\u57df\u5c55\u73b0\u51fa\u4e86\u663e\u8457\u7684\u6f5c\u529b\u548c\u4ef7\u503c\u3002"}}
{"id": "2506.11148", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11148", "abs": "https://arxiv.org/abs/2506.11148", "authors": ["Melvin Wong", "Yueming Lyu", "Thiago Rios", "Stefan Menzel", "Yew-Soon Ong"], "title": "LLM-to-Phy3D: Physically Conform Online 3D Object Generation with LLMs", "comment": null, "summary": "The emergence of generative artificial intelligence (GenAI) and large\nlanguage models (LLMs) has revolutionized the landscape of digital content\ncreation in different modalities. However, its potential use in Physical AI for\nengineering design, where the production of physically viable artifacts is\nparamount, remains vastly underexplored. The absence of physical knowledge in\nexisting LLM-to-3D models often results in outputs detached from real-world\nphysical constraints. To address this gap, we introduce LLM-to-Phy3D, a\nphysically conform online 3D object generation that enables existing LLM-to-3D\nmodels to produce physically conforming 3D objects on the fly. LLM-to-Phy3D\nintroduces a novel online black-box refinement loop that empowers large\nlanguage models (LLMs) through synergistic visual and physics-based\nevaluations. By delivering directional feedback in an iterative refinement\nprocess, LLM-to-Phy3D actively drives the discovery of prompts that yield 3D\nartifacts with enhanced physical performance and greater geometric novelty\nrelative to reference objects, marking a substantial contribution to AI-driven\ngenerative design. Systematic evaluations of LLM-to-Phy3D, supported by\nablation studies in vehicle design optimization, reveal various LLM\nimprovements gained by 4.5% to 106.7% in producing physically conform target\ndomain 3D designs over conventional LLM-to-3D models. The encouraging results\nsuggest the potential general use of LLM-to-Phy3D in Physical AI for scientific\nand engineering applications.", "AI": {"tldr": "\u63d0\u51faLLM-to-Phy3D\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u89c6\u89c9\u548c\u57fa\u4e8e\u7269\u7406\u7684\u8bc4\u4f30\u7684\u5728\u7ebf\u4f18\u5316\u56de\u8def\uff0c\u4f7f\u73b0\u6709\u7684LLM-to-3D\u6a21\u578b\u80fd\u591f\u751f\u6210\u7b26\u5408\u73b0\u5b9e\u7269\u7406\u89c4\u5219\u76843D\u5bf9\u8c61\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f18\u5316\u7269\u7406\u8bbe\u8ba1\u76843D\u8bbe\u8ba1\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u9002\u7528\u4e8ePhysical AI\u7684\u79d1\u5b66\u548c\u5de5\u7a0b\u5e94\u7528\u3002", "motivation": "\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u73b0\u6709LLM-to-3D\u6a21\u578b\u7f3a\u4e4f\u7269\u7406\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u6a21\u578b\u751f\u6210\u76843D\u4ea7\u54c1\u5f80\u5f80\u8131\u79bb\u4e86\u73b0\u5b9e\u4e16\u754c\u7684\u7269\u7406\u7ea6\u675f\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u51fa\u4e86LLM-to-Phy3D\u3002", "method": "LLM-to-Phy3D \u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u65b0\u578b\u7684\u5728\u7ebf\u9ed1\u76d2\u4f18\u5316\u56de\u8def\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u57fa\u4e8e\u7269\u7406\u7684\u8bc4\u4f30\uff0c\u4f7f\u73b0\u6709\u7684LLM-to-3D\u6a21\u578b\u80fd\u591f\u5b9e\u65f6\u751f\u6210\u7b26\u5408\u7269\u7406\u89c4\u5219\u76843D\u7269\u4f53\uff0c\u4ece\u800c\u5f25\u8865\u4e86\u751f\u6210AI\u5728\u7269\u7406\u8bbe\u8ba1\u4e0a\u7684\u4e0d\u8db3\u3002", "result": "\u7cfb\u7edf\u8bc4\u4f30\u663e\u793a\uff0cLLM-to-Phy3D\u76f8\u8f83\u4e8e\u4f20\u7edf\u7684LLM-to-3D\u6a21\u578b\u5728\u4ea7\u751f\u7b26\u5408\u7269\u7406\u8bbe\u8ba1\u76843D\u8bbe\u8ba1\u4e0a\u6709\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u6539\u8fdb\u5e45\u5ea6\u4ece4.5%\u5230106.7%\u4e0d\u7b49\uff0c\u5c24\u5176\u662f\u5728\u8f66\u8f86\u8bbe\u8ba1\u4f18\u5316\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u57fa\u4e8e\u5b9e\u9a8c\u8bc1\u636e\uff0cLLM-to-Phy3D\u5c55\u73b0\u51fa\u5728\u751f\u6210\u7b26\u5408\u7269\u7406\u89c4\u5219\u76843D\u8bbe\u8ba1\u4e0a\u7684\u6f5c\u529b\uff0c\u6709\u671b\u5728\u79d1\u5b66\u548c\u5de5\u7a0b\u5e94\u7528\u4e2d\u7684Physical AI\u4e2d\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2506.11063", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11063", "abs": "https://arxiv.org/abs/2506.11063", "authors": ["Jiayu Yao", "Shenghua Liu", "Yiwei Wang", "Lingrui Mei", "Baolong Bi", "Yuyao Ge", "Zhecheng Li", "Xueqi Cheng"], "title": "Who is in the Spotlight: The Hidden Bias Undermining Multimodal Retrieval-Augmented Generation", "comment": null, "summary": "Multimodal Retrieval-Augmented Generation (RAG) systems have become essential\nin knowledge-intensive and open-domain tasks. As retrieval complexity\nincreases, ensuring the robustness of these systems is critical. However,\ncurrent RAG models are highly sensitive to the order in which evidence is\npresented, often resulting in unstable performance and biased reasoning,\nparticularly as the number of retrieved items or modality diversity grows. This\nraises a central question: How does the position of retrieved evidence affect\nmultimodal RAG performance? To answer this, we present the first comprehensive\nstudy of position bias in multimodal RAG systems. Through controlled\nexperiments across text-only, image-only, and mixed-modality tasks, we observe\na consistent U-shaped accuracy curve with respect to evidence position. To\nquantify this bias, we introduce the Position Sensitivity Index ($PSI_p$) and\ndevelop a visualization framework to trace attention allocation patterns across\ndecoder layers. Our results reveal that multimodal interactions intensify\nposition bias compared to unimodal settings, and that this bias increases\nlogarithmically with retrieval range. These findings offer both theoretical and\nempirical foundations for position-aware analysis in RAG, highlighting the need\nfor evidence reordering or debiasing strategies to build more reliable and\nequitable generation systems.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5168\u9762\u7814\u7a76\u4e86\u591a\u6a21\u6001RAG\u7cfb\u7edf\u4e2d\u7684\u4f4d\u7f6e\u504f\u5dee\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u6307\u6570$PSI_p$\uff0c\u5e76\u5f00\u53d1\u4e86\u5bf9\u5e94\u7684\u53ef\u89c6\u5316\u6846\u67b6\u3002", "motivation": "\u968f\u7740\u68c0\u7d22\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u786e\u4fdd\u8fd9\u4e9b\u7cfb\u7edf\u7684\u7a33\u5065\u6027\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684RAG\u6a21\u578b\u5bf9\u5448\u73b0\u8bc1\u636e\u7684\u987a\u5e8f\u975e\u5e38\u654f\u611f\uff0c\u901a\u5e38\u4f1a\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u7684\u8868\u73b0\u548c\u504f\u5dee\u63a8\u7406\uff0c\u7279\u522b\u662f\u5728\u68c0\u7d22\u5230\u7684\u9879\u76ee\u6570\u91cf\u6216\u6a21\u5f0f\u591a\u6837\u6027\u589e\u52a0\u65f6\u3002\u8fd9\u63d0\u51fa\u4e86\u4e00\u4e2a\u95ee\u9898\uff1a\u68c0\u7d22\u8bc1\u636e\u7684\u4f4d\u7f6e\u5982\u4f55\u5f71\u54cd\u591a\u6a21\u6001RAG\u6027\u80fd\uff1f", "method": "\u901a\u8fc7\u5728\u4ec5\u6587\u672c\u3001\u4ec5\u56fe\u50cf\u548c\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\uff0c\u672c\u6587\u89c2\u5bdf\u5230\u4e00\u4e2a\u5173\u4e8e\u8bc1\u636e\u4f4d\u7f6e\u7684\u4e00\u81f4U\u578b\u51c6\u786e\u6027\u66f2\u7ebf\u3002\u4e3a\u4e86\u91cf\u5316\u8fd9\u79cd\u504f\u5dee\uff0c\u672c\u6587\u5f15\u5165\u4e86\u4f4d\u7f6e\u654f\u611f\u5ea6\u6307\u6570($PSI_p$) \u5e76\u5f00\u53d1\u4e86\u53ef\u89c6\u5316\u6846\u67b6\u6765\u8ffd\u8e2a\u89e3\u7801\u5668\u5404\u5c42\u4e2d\u7684\u6ce8\u610f\u529b\u5206\u914d\u6a21\u5f0f\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\uff0c\u591a\u6a21\u6001\u4e92\u52a8\u76f8\u6bd4\u5355\u4e00\u6a21\u6001\u8bbe\u7f6e\uff0c\u589e\u5f3a\u4e86\u4f4d\u7f6e\u504f\u5dee\uff0c\u5e76\u4e14\u8fd9\u79cd\u504f\u5dee\u968f\u7740\u68c0\u7d22\u8303\u56f4\u7684\u589e\u52a0\u800c\u5bf9\u6570\u589e\u957f\u3002", "conclusion": "\u8fd9\u4e9b\u7814\u7a76\u7ed3\u679c\u4e3aRAG\u4e2d\u7684\u4f4d\u7f6e\u611f\u77e5\u5206\u6790\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8bc1\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86\u4e3a\u4e86\u6784\u5efa\u66f4\u53ef\u9760\u548c\u516c\u5e73\u7684\u751f\u6210\u7cfb\u7edf\uff0c\u9700\u8981\u8fdb\u884c\u8bc1\u636e\u91cd\u65b0\u6392\u5e8f\u6216\u53bb\u504f\u5dee\u7b56\u7565\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2506.11151", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.11151", "abs": "https://arxiv.org/abs/2506.11151", "authors": ["Jonathan Grizou", "Carlos de la Torre-Ortiz", "Tuukka Ruotsalo"], "title": "Self-Calibrating BCIs: Ranking and Recovery of Mental Targets Without Labels", "comment": "10 pages, 4 figures, 11 appendix pages, 7 appendix figures", "summary": "We consider the problem of recovering a mental target (e.g., an image of a\nface) that a participant has in mind from paired EEG (i.e., brain responses)\nand image (i.e., perceived faces) data collected during interactive sessions\nwithout access to labeled information. The problem has been previously explored\nwith labeled data but not via self-calibration, where labeled data is\nunavailable. Here, we present the first framework and an algorithm, CURSOR,\nthat learns to recover unknown mental targets without access to labeled data or\npre-trained decoders. Our experiments on naturalistic images of faces\ndemonstrate that CURSOR can (1) predict image similarity scores that correlate\nwith human perceptual judgments without any label information, (2) use these\nscores to rank stimuli against an unknown mental target, and (3) generate new\nstimuli indistinguishable from the unknown mental target (validated via a user\nstudy, N=53).", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86CURSOR\u6846\u67b6\uff0c\u4e00\u79cd\u80fd\u591f\u5728\u6ca1\u6709\u5df2\u6807\u8bb0\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u6062\u590d\u672a\u77e5\u601d\u7ef4\u76ee\u6807\u7684\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u9884\u6d4b\u56fe\u50cf\u76f8\u4f3c\u6027\u8bc4\u5206\uff0c\u6392\u5e8f\u523a\u6fc0\uff0c\u4ee5\u53ca\u751f\u6210\u65b0\u523a\u6fc0\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u76ee\u524d\uff0c\u4ece\u8111\u7535\uff08EEG\uff09\u548c\u56fe\u50cf\u6570\u636e\u4e2d\u6062\u590d\u601d\u7ef4\u76ee\u6807\u7684\u95ee\u9898\u5df2\u7ecf\u7528\u6709\u6807\u7b7e\u7684\u6570\u636e\u8fdb\u884c\u4e86\u7814\u7a76\uff0c\u4f46\u5c1a\u672a\u63a2\u7d22\u65e0\u6807\u7b7e\u6570\u636e\u7684\u81ea\u6211\u6821\u51c6\u65b9\u6cd5\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCURSOR\u7684\u6846\u67b6\u548c\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u5728\u6ca1\u6709\u6807\u7b7e\u6570\u636e\u6216\u9884\u8bad\u7ec3\u89e3\u7801\u5668\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u6062\u590d\u672a\u77e5\u7684\u601d\u7ef4\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCURSOR\u53ef\u4ee5\u5728\u4e0d\u4f7f\u7528\u4efb\u4f55\u6807\u7b7e\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u9884\u6d4b\u56fe\u50cf\u76f8\u4f3c\u6027\u8bc4\u5206\uff0c\u4f7f\u7528\u8fd9\u4e9b\u8bc4\u5206\u5bf9\u523a\u6fc0\u8fdb\u884c\u6392\u5e8f\uff0c\u5e76\u751f\u6210\u4e0e\u672a\u77e5\u601d\u7ef4\u76ee\u6807\u65e0\u6cd5\u533a\u5206\u7684\u65b0\u523a\u6fc0\u3002", "conclusion": "CURSOR\u80fd\u591f\u9884\u6d4b\u4e0e\u4eba\u7c7b\u611f\u77e5\u5224\u65ad\u76f8\u5173\u7684\u56fe\u50cf\u76f8\u4f3c\u6027\u8bc4\u5206\uff0c\u4f7f\u7528\u8fd9\u4e9b\u8bc4\u5206\u5bf9\u523a\u6fc0\u8fdb\u884c\u6392\u5e8f\uff0c\u5e76\u751f\u6210\u4e0e\u672a\u77e5\u601d\u7ef4\u76ee\u6807\u65e0\u6cd5\u533a\u5206\u7684\u65b0\u523a\u6fc0\u3002\u8fd9\u4e9b\u7ed3\u8bba\u901a\u8fc7\u4e00\u9879\u7528\u6237\u7814\u7a76(N=53)\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2506.11065", "categories": ["cs.CL", "Primary 68T50, Secondary 68T05, 91F20", "I.2.7; I.2.6; I.5.4"], "pdf": "https://arxiv.org/pdf/2506.11065", "abs": "https://arxiv.org/abs/2506.11065", "authors": ["Alexey Tikhonov", "Sergei Shteiner", "Anna Bykova", "Ivan P. Yamshchikov"], "title": "Smotrom tvoja pa ander drogoj verden! Resurrecting Dead Pidgin with Generative Models: Russenorsk Case Study", "comment": "ACL Findings 2025", "summary": "Russenorsk, a pidgin language historically used in trade interactions between\nRussian and Norwegian speakers, represents a unique linguistic phenomenon. In\nthis paper, we attempt to analyze its lexicon using modern large language\nmodels (LLMs), based on surviving literary sources. We construct a structured\ndictionary of the language, grouped by synonyms and word origins. Subsequently,\nwe use this dictionary to formulate hypotheses about the core principles of\nword formation and grammatical structure in Russenorsk and show which\nhypotheses generated by large language models correspond to the hypotheses\npreviously proposed ones in the academic literature. We also develop a\n\"reconstruction\" translation agent that generates hypothetical Russenorsk\nrenderings of contemporary Russian and Norwegian texts.", "AI": {"tldr": "{immerse}\u4f7f\u7528LLM\u5206\u6790Russenorsk\u7684\u8bcd\u6c47\uff0c\u5e76\u5f00\u53d1\u4e00\u79cd\u8f6c\u6362\u4ee3\u7406\uff0c\u7528\u4ee5\u751f\u6210\u73b0\u4ee3\u6587\u672c\u7684Russenorsk\u7248\u672c\u3002", "motivation": "{immerse}\u4f7f\u7528\u73b0\u5b58\u6750\u6599\u7814\u7a76\u72ec\u7279\u8bed\u8a00\u73b0\u8c61Russenorsk\uff0c\u4ee5\u5229\u7528\u73b0\u4ee3\u6280\u672f\u52a0\u6df1\u5bf9\u5176\u8bcd\u6c47\u548c\u7ed3\u6784\u89c4\u5219\u7684\u7406\u89e3\u3002", "method": "{immerse}\u901a\u8fc7\u4f7f\u7528\u73b0\u5b58\u6587\u5b66\u4f5c\u54c1\u4e2d\u7684\u6570\u636e\uff0c\u6784\u5efa\u4e86Russenorsk\u7684\u7ed3\u6784\u5316\u8bcd\u5178\u3002\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u51fa\u5173\u4e8e\u8be5\u8bed\u8a00\u5f62\u6210\u6cd5\u5219\u7684\u5047\u8bbe\u3002", "result": "{immerse}\u5206\u6790\u4e86\u4f7f\u7528\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u901a\u8fc7\u73b0\u5b58\u7684\u6587\u5b66\u8d44\u6599\u5bf9\u4fc4\u632a\u8d38\u6613\u7528\u8bedRussenorsk\u7684\u8bcd\u6c47\u8fdb\u884c\u5206\u6790\u7684\u65b9\u6cd5\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u6309\u540c\u4e49\u8bcd\u548c\u8bcd\u6e90\u5206\u7c7b\u7684\u7ed3\u6784\u5316\u8bcd\u5178\uff0c\u5e76\u4ee5\u6b64\u4e3a\u57fa\u7840\u63d0\u51fa\u5173\u4e8eRussenorsk\u8bcd\u5f62\u6210\u548c\u53e5\u6cd5\u7ed3\u6784\u7684\u6838\u5fc3\u539f\u5219\u7684\u5047\u8bbe\u3002\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u201c\u91cd\u5efa\u201d\u7ffb\u8bd1\u4ee3\u7406\uff0c\u7528\u4e8e\u751f\u6210\u5f53\u4ee3\u4fc4\u8bed\u548c\u632a\u5a01\u8bed\u6587\u672c\u7684\u5047\u8bbe\u4fc4\u632a\u8d38\u6613\u7528\u8bed\u7248\u672c\u3002", "conclusion": "{immerse}\u901a\u8fc7\u7814\u7a76\u9a8c\u8bc1\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u91cd\u5efa\u548c\u7406\u89e3\u5931\u4f20\u8bed\u8a00\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.11154", "categories": ["cs.CV", "68T07 (Artificial Intelligence), 68U10 (Image Processing)"], "pdf": "https://arxiv.org/pdf/2506.11154", "abs": "https://arxiv.org/abs/2506.11154", "authors": ["Sharvari Kamble"], "title": "SLRNet: A Real-Time LSTM-Based Sign Language Recognition System", "comment": "9 pages, 5 figures, includes experimental results. Code available at:\n  https://github.com/Khushi-739/SLRNet", "summary": "Sign Language Recognition (SLR) plays a crucial role in bridging the\ncommunication gap between the hearing-impaired community and society. This\npaper introduces SLRNet, a real-time webcam-based ASL recognition system using\nMediaPipe Holistic and Long Short-Term Memory (LSTM) networks. The model\nprocesses video streams to recognize both ASL alphabet letters and functional\nwords. With a validation accuracy of 86.7%, SLRNet demonstrates the feasibility\nof inclusive, hardware-independent gesture recognition.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aSLRNet\u7684\u5b9e\u65f6ASL\u8bc6\u522b\u7cfb\u7edf\uff0c\u4f7f\u7528MediaPipe Holistic\u548cLSTM\u7f51\u7edc\uff0c\u9a8c\u8bc1\u51c6\u786e\u7387\u4e3a86.7%\u3002", "motivation": "\u4e3a\u4e86\u586b\u8865\u542c\u969c\u793e\u533a\u548c\u793e\u4f1a\u4e4b\u95f4\u7684\u6c9f\u901a\u9e3f\u6c9f", "method": "\u4f7f\u7528MediaPipe Holistic\u548cLSTM\u7f51\u7edc\u7ed3\u5408\u7684\u5b9e\u65f6ASL\u8bc6\u522b\u7cfb\u7edf", "result": "\u7cfb\u7edf\u9a8c\u8bc1\u51c6\u786e\u7387\u4e3a86.7%\uff0c\u8bc1\u660e\u4e86\u53ef\u72ec\u7acb\u4e8e\u786c\u4ef6\u7684\u8bc6\u522b\u624b\u52bf\u7684\u53ef\u884c\u6027", "conclusion": "SLRNet\u5c55\u793a\u4e86\u5b9e\u65f6\u624b\u8bed\u8bc6\u522b\u7cfb\u7edf\u5728\u65e0\u969c\u788d\u6c9f\u901a\u65b9\u9762\u7684\u6f5c\u529b\u4e0e\u5b9e\u7528\u6027"}}
{"id": "2506.11067", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.11067", "abs": "https://arxiv.org/abs/2506.11067", "authors": ["Hieu Nghiem", "Hemanth Reddy Singareddy", "Zhuqi Miao", "Jivan Lamichhane", "Abdulaziz Ahmed", "Johnson Thomas", "Dursun Delen", "William Paiva"], "title": "A Large Language Model Based Pipeline for Review of Systems Entity Recognition from Clinical Notes", "comment": null, "summary": "Objective: Develop a cost-effective, large language model (LLM)-based\npipeline for automatically extracting Review of Systems (ROS) entities from\nclinical notes. Materials and Methods: The pipeline extracts ROS sections using\nSecTag, followed by few-shot LLMs to identify ROS entity spans, their\npositive/negative status, and associated body systems. We implemented the\npipeline using open-source LLMs (Mistral, Llama, Gemma) and ChatGPT. The\nevaluation was conducted on 36 general medicine notes containing 341 annotated\nROS entities. Results: When integrating ChatGPT, the pipeline achieved the\nlowest error rates in detecting ROS entity spans and their corresponding\nstatuses/systems (28.2% and 14.5%, respectively). Open-source LLMs enable\nlocal, cost-efficient execution of the pipeline while delivering promising\nperformance with similarly low error rates (span: 30.5-36.7%; status/system:\n24.3-27.3%). Discussion and Conclusion: Our pipeline offers a scalable and\nlocally deployable solution to reduce ROS documentation burden. Open-source\nLLMs present a viable alternative to commercial models in resource-limited\nhealthcare environments.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ba1\u9053\u6765\u4ece\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u81ea\u52a8\u63d0\u53d6\u5ba1\u67e5\u7cfb\u7edf\uff08ROS\uff09\u5b9e\u4f53\uff0c\u4ee5\u5b9e\u73b0\u6210\u672c\u6548\u76ca\u548c\u53ef\u6269\u5c55\u6027\u7684\u63d0\u5347\uff0c\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u8868\u73b0\u51fa\u4e0d\u9519\u7684\u7ed3\u679c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5f00\u53d1\u51fa\u4e00\u4e2a\u6210\u672c\u4f4e\u4e14\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u4ece\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u81ea\u52a8\u63d0\u53d6Review of Systems (ROS)\u5b9e\u4f53\uff0c\u4ee5\u51cf\u8f7bROS\u6587\u6863\u8bb0\u5f55\u8d1f\u62c5\u3002", "method": "\u5229\u7528\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982Mistral, Llama, Gemma\uff09\u548cChatGPT\u8fdb\u884c\u5ba1\u67e5\u7cfb\u7edf\uff08ROS\uff09\u5b9e\u4f53\u7684\u81ea\u52a8\u63d0\u53d6\uff0c\u901a\u8fc7SecTag\u63d0\u53d6ROS\u90e8\u5206\uff0c\u7136\u540e\u7528few-shot\u5927\u8bed\u8a00\u6a21\u578b\u6765\u8bc6\u522b\u5b9e\u4f53\u8de8\u5ea6\u3001\u6b63\u8d1f\u72b6\u6001\u53ca\u5bf9\u5e94\u7684\u8eab\u4f53\u7cfb\u7edf\u3002", "result": "\u572836\u4efd\u5185\u79d1\u75c5\u5386\uff08\u5305\u542b341\u4e2a\u6ce8\u91ca\u7684ROS\u5b9e\u4f53\uff09\u4e2d\uff0c\u96c6\u6210ChatGPT\u7684\u7ba1\u9053\u5728\u53d1\u73b0ROS\u5b9e\u4f53\u8de8\u5ea6\u548c\u5176\u76f8\u5e94\u6b63\u8d1f\u72b6\u6001\u6216\u7cfb\u7edf\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u4f4e\u8bef\u5dee\u7387\uff0828.2%\u548c14.5%\uff09\u3002\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u672c\u5730\u6267\u884c\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u65b9\u6848\uff0c\u5e76\u4e14\u5177\u6709\u540c\u6837\u8f83\u4f4e\u7684\u8bef\u5dee\u7387\uff08\u8de8\u5ea6\u8bef\u5dee\uff1a30.5-36.7%\uff0c\u72b6\u6001/\u7cfb\u7edf\u8bef\u5dee\uff1a24.3-27.3%\uff09\u3002", "conclusion": "\u7814\u7a76\u5f00\u53d1\u7684\u7ba1\u9053\u4f5c\u4e3a\u4e00\u4e2a\u53ef\u6269\u5c55\u5e76\u80fd\u672c\u5730\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u51cf\u5c11ROS\u6587\u6863\u8bb0\u5f55\u7684\u5de5\u4f5c\u8d1f\u62c5\u3002\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u8d44\u6e90\u6709\u9650\u7684\u533b\u7597\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2506.11155", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.11155", "abs": "https://arxiv.org/abs/2506.11155", "authors": ["Linhao Yu", "Xinguang Ji", "Yahui Liu", "Fanheng Kong", "Chenxi Sun", "Jingyuan Zhang", "Hongzhi Zhang", "V. W.", "Fuzheng Zhang", "Deyi Xiong"], "title": "Evaluating Multimodal Large Language Models on Video Captioning via Monte Carlo Tree Search", "comment": "28 pages; ACL 2025(main)", "summary": "Video captioning can be used to assess the video understanding capabilities\nof Multimodal Large Language Models (MLLMs). However, existing benchmarks and\nevaluation protocols suffer from crucial issues, such as inadequate or\nhomogeneous creation of key points, exorbitant cost of data creation, and\nlimited evaluation scopes. To address these issues, we propose an automatic\nframework, named AutoCaption, which leverages Monte Carlo Tree Search (MCTS) to\nconstruct numerous and diverse descriptive sentences (\\textit{i.e.}, key\npoints) that thoroughly represent video content in an iterative way. This\niterative captioning strategy enables the continuous enhancement of video\ndetails such as actions, objects' attributes, environment details, etc. We\napply AutoCaption to curate MCTS-VCB, a fine-grained video caption benchmark\ncovering video details, thereby enabling a comprehensive evaluation of MLLMs on\nthe video captioning task. We evaluate more than 20 open- and closed-source\nMLLMs of varying sizes on MCTS-VCB. Results show that MCTS-VCB can effectively\nand comprehensively evaluate the video captioning capability, with\nGemini-1.5-Pro achieving the highest F1 score of 71.2. Interestingly, we\nfine-tune InternVL2.5-8B with the AutoCaption-generated data, which helps the\nmodel achieve an overall improvement of 25.0% on MCTS-VCB and 16.3% on\nDREAM-1K, further demonstrating the effectiveness of AutoCaption. The code and\ndata are available at https://github.com/tjunlp-lab/MCTS-VCB.", "AI": {"tldr": "\u63d0\u51fa\u4e86AutoCaption\u6846\u67b6\uff0c\u4f7f\u7528MCTS\u65b9\u6cd5\u5728\u89c6\u9891\u7ec6\u8282\u4e0a\u6784\u5efa\u591a\u6837\u7684\u63cf\u8ff0\u6027\u53e5\u5b50\u3002\u8be5\u6846\u67b6\u7528\u4e8e\u521b\u5efa\u65b0\u7684\u89c6\u9891\u5b57\u5e55\u57fa\u51c6MCTS-VCB\uff0c\u53ef\u4ee5\u6709\u6548\u8bc4\u4f30MLLMs\u7684\u89c6\u9891\u5b57\u5e55\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7684\u89c6\u9891\u5b57\u5e55\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u534f\u8bae\u4e2d\u5b58\u5728\u7684\u95ee\u9898\uff0c\u5982\u5173\u952e\u70b9\u521b\u5efa\u4e0d\u8db3\u6216\u540c\u8d28\u5316\u3001\u6210\u672c\u9ad8\u548c\u8bc4\u4f30\u8303\u56f4\u6709\u9650\u3002", "method": "\u4f7f\u7528Monte Carlo\u6811\u641c\u7d22(MCTS)\u6784\u5efa\u81ea\u52a8\u6846\u67b6AutoCaption\uff0c\u4ee5\u751f\u6210\u591a\u6837\u5316\u7684\u63cf\u8ff0\u6027\u53e5\u5b50\uff0c\u6db5\u76d6\u89c6\u9891\u7684\u8be6\u7ec6\u5185\u5bb9\uff0c\u4ece\u800c\u8fed\u4ee3\u63d0\u5347\u89c6\u9891\u7406\u89e3\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u4e86\u8d85\u8fc720\u4e2a\u5f00\u6e90\u548c\u95ed\u6e90\u7684MLLMs\uff0cGemini-1.5-Pro\u5728MCTS-VCB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u9ad8\u7684F1\u5206\u657071.2\u3002\u6b64\u5916\uff0c\u57fa\u4e8eAutoCaption\u751f\u6210\u7684\u6570\u636e\u5fae\u8c03InternVL2.5-8B\uff0c\u5728MCTS-VCB\u4e0a\u7684\u6574\u4f53\u8868\u73b0\u63d0\u5347\u4e8625.0%\uff0c\u5728DREAM-1K\u4e0a\u63d0\u5347\u4e8616.3%\u3002", "conclusion": "AutoCaption\u80fd\u591f\u6709\u6548\u751f\u6210\u591a\u6837\u7684\u63cf\u8ff0\u6027\u53e5\u5b50\uff0c\u8986\u76d6\u89c6\u9891\u7ec6\u8282\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u89c6\u9891\u5b57\u5e55\u57fa\u51c6\u6d4b\u8bd5MCTS-VCB\uff0c\u80fd\u591f\u5168\u9762\u8bc4\u4f30MLLMs\u7684\u89c6\u9891\u5b57\u5e55\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2506.11068", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.11068", "abs": "https://arxiv.org/abs/2506.11068", "authors": ["Bumjin Park", "Jinsil Lee", "Jaesik Choi"], "title": "Deontological Keyword Bias: The Impact of Modal Expressions on Normative Judgments of Language Models", "comment": "20 pages including references and appendix; To appear in ACL 2025\n  main conference", "summary": "Large language models (LLMs) are increasingly engaging in moral and ethical\nreasoning, where criteria for judgment are often unclear, even for humans.\nWhile LLM alignment studies cover many areas, one important yet underexplored\narea is how LLMs make judgments about obligations. This work reveals a strong\ntendency in LLMs to judge non-obligatory contexts as obligations when prompts\nare augmented with modal expressions such as must or ought to. We introduce\nthis phenomenon as Deontological Keyword Bias (DKB). We find that LLMs judge\nover 90\\% of commonsense scenarios as obligations when modal expressions are\npresent. This tendency is consist across various LLM families, question types,\nand answer formats. To mitigate DKB, we propose a judgment strategy that\nintegrates few-shot examples with reasoning prompts. This study sheds light on\nhow modal expressions, as a form of linguistic framing, influence the normative\ndecisions of LLMs and underscores the importance of addressing such biases to\nensure judgment alignment.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5305\u542b\u4e49\u52a1\u76f8\u5173\u6a21\u6001\u8868\u8fbe\u7684\u60c5\u5883\u65f6\uff0c\u5b58\u5728\u663e\u8457\u7684\u8bef\u5224\u503e\u5411\uff0c\u79f0\u4e4b\u4e3a\u201c\u4e49\u52a1\u5173\u952e\u8bcd\u504f\u89c1\u201d\uff08DKB\uff09\u3002\u5efa\u8bae\u901a\u8fc7\u7ed3\u5408\u5c11\u91cf\u793a\u4f8b\u548c\u63a8\u7406\u63d0\u793a\u7684\u65b9\u6cd5\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u53c2\u4e0e\u9053\u5fb7\u548c\u4f26\u7406\u63a8\u7406\uff0c\u5224\u65ad\u6807\u51c6\u5f80\u5f80\u4e0d\u660e\u786e\uff0c\u751a\u81f3\u5bf9\u4eba\u7c7b\u6765\u8bf4\u4e5f\u662f\u5982\u6b64\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u7279\u522b\u662f\u5728\u8bc4\u4f30\u4e49\u52a1\u5224\u65ad\u65b9\u9762\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u5305\u542b\u5982\u201c\u5fc5\u987b\u201d\u6216\u201c\u5e94\u5f53\u201d\u7b49\u6a21\u6001\u8868\u8fbe\u7684\u60c5\u5883\u6709\u5f3a\u70c8\u504f\u89c1\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u201c\u4e49\u52a1\u5173\u952e\u8bcd\u504f\u89c1\u201d\uff08DKB\uff09\u73b0\u8c61\u6765\u68c0\u9a8c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9762\u5bf9\u6a21\u6001\u8868\u8fbe\u65f6\uff0c\u5982\u4f55\u5c06\u975e\u4e49\u52a1\u60c5\u5883\u8bef\u5224\u4e3a\u4e49\u52a1\u3002\u7814\u7a76\u4e2d\u91c7\u7528\u4e86\u51e0\u79cd\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u4e86\u4e0d\u540c\u7684\u95ee\u9898\u7c7b\u578b\u548c\u7b54\u6848\u683c\u5f0f\u3002\u4e3a\u4e86\u7f13\u89e3DKB\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u5c11\u91cf\u793a\u4f8b\u548c\u63a8\u7406\u63d0\u793a\u7684\u5224\u65ad\u7b56\u7565\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u63d0\u793a\u4e2d\u5305\u542b\u6a21\u6001\u8868\u8fbe\u65f6\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u4f1a\u5c06\u8d85\u8fc790%\u7684\u5e38\u8bc6\u60c5\u5883\u8bef\u5224\u4e3a\u4e49\u52a1\uff0c\u8fd9\u79cd\u503e\u5411\u5728\u4e0d\u540c\u5927\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\u3001\u95ee\u9898\u7c7b\u578b\u548c\u7b54\u6848\u683c\u5f0f\u4e2d\u662f\u4e00\u81f4\u7684\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6a21\u6001\u8868\u8fbe\u4f5c\u4e3a\u4e00\u79cd\u8bed\u8a00\u6846\u67b6\u5982\u4f55\u5f71\u54cd\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c4\u8303\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u6307\u51fa\u4e86\u7ea0\u6b63\u6b64\u7c7b\u504f\u89c1\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u786e\u4fdd\u51b3\u7b56\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2506.11156", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.11156", "abs": "https://arxiv.org/abs/2506.11156", "authors": ["Rasha Sinha", "Rekha B S"], "title": "Digitization of Document and Information Extraction using OCR", "comment": null, "summary": "Retrieving accurate details from documents is a crucial task, especially when\nhandling a combination of scanned images and native digital formats. This\ndocument presents a combined framework for text extraction that merges Optical\nCharacter Recognition (OCR) techniques with Large Language Models (LLMs) to\ndeliver structured outputs enriched by contextual understanding and confidence\nindicators. Scanned files are processed using OCR engines, while digital files\nare interpreted through layout-aware libraries. The extracted raw text is\nsubsequently analyzed by an LLM to identify key-value pairs and resolve\nambiguities. A comparative analysis of different OCR tools is presented to\nevaluate their effectiveness concerning accuracy, layout recognition, and\nprocessing speed. The approach demonstrates significant improvements over\ntraditional rule-based and template-based methods, offering enhanced\nflexibility and semantic precision across different document categories", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7ed3\u5408OCR\u548cLLM\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u53d6\u6587\u672c\u5e76\u589e\u5f3a\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u4e0a\u4e0b\u6587\u548c\u51c6\u786e\u5ea6\u3002", "motivation": "\u5904\u7406\u626b\u63cf\u56fe\u50cf\u548c\u6570\u5b57\u5316\u6587\u4ef6\u7684\u7ec4\u5408\uff0c\u51c6\u786e\u63d0\u53d6\u6587\u6863\u7ec6\u8282\u662f\u4e00\u9879\u91cd\u8981\u4efb\u52a1\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408OCR\u6280\u672f\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u626b\u63cf\u56fe\u50cf\u548c\u6570\u5b57\u5316\u6587\u4ef6\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u7684\u6587\u672c\u8f93\u51fa\uff0c\u589e\u5f3a\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u7f6e\u4fe1\u5ea6\u6307\u6807\u3002", "result": "\u901a\u8fc7\u4e0e\u4f20\u7edf\u7684\u57fa\u4e8e\u89c4\u5219\u548c\u6a21\u677f\u7684\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u672c\u6587\u7684\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u51c6\u786e\u5ea6\u3001\u5e03\u5c40\u8bc6\u522b\u548c\u5904\u7406\u901f\u5ea6\u65b9\u9762\u7684\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5bf9\u4e8e\u4e0d\u540c\u7684\u6587\u6863\u7c7b\u522b\u63d0\u4f9b\u4e86\u589e\u5f3a\u7684\u7075\u6d3b\u6027\u548c\u8bed\u4e49\u7cbe\u786e\u6027\u3002"}}
{"id": "2506.11070", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.11070", "abs": "https://arxiv.org/abs/2506.11070", "authors": ["Yu-Zhe Shi", "Mingchen Liu", "Hanlu Ma", "Qiao Xu", "Huamin Qu", "Kun He", "Lecheng Ruan", "Qining Wang"], "title": "Targeted control of fast prototyping through domain-specific interface", "comment": "In International Conference on Machine Learning (ICML'25)", "summary": "Industrial designers have long sought a natural and intuitive way to achieve\nthe targeted control of prototype models -- using simple natural language\ninstructions to configure and adjust the models seamlessly according to their\nintentions, without relying on complex modeling commands. While Large Language\nModels have shown promise in this area, their potential for controlling\nprototype models through language remains partially underutilized. This\nlimitation stems from gaps between designers' languages and modeling languages,\nincluding mismatch in abstraction levels, fluctuation in semantic precision,\nand divergence in lexical scopes. To bridge these gaps, we propose an interface\narchitecture that serves as a medium between the two languages. Grounded in\ndesign principles derived from a systematic investigation of fast prototyping\npractices, we devise the interface's operational mechanism and develop an\nalgorithm for its automated domain specification. Both machine-based\nevaluations and human studies on fast prototyping across various product design\ndomains demonstrate the interface's potential to function as an auxiliary\nmodule for Large Language Models, enabling precise and effective targeted\ncontrol of prototype models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u63a5\u53e3\u67b6\u6784\uff0c\u65e8\u5728\u6865\u63a5\u8bbe\u8ba1\u8005\u8bed\u8a00\u4e0e\u5efa\u6a21\u8bed\u8a00\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u81ea\u52a8\u9886\u57df\u89c4\u8303\u7684\u7b97\u6cd5\u5b9e\u73b0\u5feb\u901f\u539f\u578b\u5236\u4f5c\u4e2d\u7684\u7cbe\u51c6\u5b9a\u5411\u63a7\u5236\u3002", "motivation": "\u5de5\u4e1a\u8bbe\u8ba1\u5e08\u4e00\u76f4\u5e0c\u671b\u6709\u4e00\u79cd\u81ea\u7136\u4e14\u76f4\u89c2\u7684\u65b9\u5f0f\u6765\u5b9e\u73b0\u539f\u578b\u6a21\u578b\u7684\u5b9a\u5411\u63a7\u5236\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u65e0\u7f1d\u5730\u914d\u7f6e\u548c\u8c03\u6574\u6a21\u578b\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u590d\u6742\u7684\u5efa\u6a21\u547d\u4ee4\u3002\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e00\u65b9\u5411\u4e0a\u5c55\u73b0\u4e86\u6f5c\u529b\uff0c\u4f46\u4ecd\u672a\u80fd\u5145\u5206\u5229\u7528\u3002\u5176\u9650\u5236\u6e90\u4e8e\u8bbe\u8ba1\u8bed\u8a00\u4e0e\u5efa\u6a21\u8bed\u8a00\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5305\u62ec\u62bd\u8c61\u7ea7\u522b\u3001\u8bed\u4e49\u7cbe\u5ea6\u548c\u8bcd\u5f5a\u8303\u56f4\u7b49\u65b9\u9762\u7684\u5dee\u5f02\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f5c\u4e3a\u8bbe\u8ba1\u8005\u8bed\u8a00\u4e0e\u5efa\u6a21\u8bed\u8a00\u4e4b\u95f4\u5a92\u4ecb\u7684\u63a5\u53e3\u67b6\u6784\uff0c\u5e76\u5236\u5b9a\u4e86\u81ea\u52a8\u9886\u57df\u89c4\u8303\u7684\u7b97\u6cd5\u3002\u8fd9\u4e00\u67b6\u6784\u7684\u8bbe\u8ba1\u539f\u5219\u6e90\u81ea\u5feb\u901f\u539f\u578b\u5236\u4f5c\u5b9e\u8df5\u7684\u7cfb\u7edf\u8c03\u67e5\u3002", "result": "\u57fa\u4e8e\u673a\u5668\u8bc4\u4f30\u548c\u4eba\u7c7b\u7814\u7a76\u4e2d\u7684\u5feb\u901f\u539f\u578b\u5236\u4f5c\u6d4b\u8bd5\u63ed\u793a\u4e86\u8be5\u63a5\u53e3\u4f5c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8f85\u52a9\u6a21\u5757\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u7cbe\u51c6\u3001\u6709\u6548\u5730\u63a7\u5236\u539f\u578b\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u63a5\u53e3\u67b6\u6784\u4f5c\u4e3a\u8f85\u52a9\u6a21\u5757\uff0c\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u4f7f\u7528\u65f6\u80fd\u591f\u5b9e\u73b0\u5bf9\u539f\u578b\u6a21\u578b\u7684\u7cbe\u51c6\u800c\u6709\u6548\u7684\u63a7\u5236\u3002"}}
{"id": "2506.11162", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11162", "abs": "https://arxiv.org/abs/2506.11162", "authors": ["Tania Chakraborty", "Eylon Caplan", "Dan Goldwasser"], "title": "VIBE: Can a VLM Read the Room?", "comment": "Pre-print, under review", "summary": "Understanding human social behavior such as recognizing emotions and the\nsocial dynamics causing them is an important and challenging problem. While\nLLMs have made remarkable advances, they are limited to the textual domain and\ncannot account for the major role that non-verbal cues play in understanding\nsocial situations. Vision Language Models (VLMs) can potentially account for\nthis gap, however their ability to make correct inferences over such social\ncues has received little attention. In this paper, we explore the capabilities\nof VLMs at social reasoning. We identify a previously overlooked limitation in\nVLMs: the Visual Social-Pragmatic Inference gap. To target this gap, we propose\na new task for VLMs: Visual Social-Pragmatic Inference. We construct a high\nquality dataset to test the abilities of a VLM for this task and benchmark the\nperformance of several VLMs on it.", "AI": {"tldr": "\u6211\u4eec\u8bc6\u522b\u5e76\u7814\u7a76\u4e86VLM\u5728\u7406\u89e3\u793e\u4f1a\u884c\u4e3a\u548c\u975e\u8a00\u8bed\u7ebf\u7d22\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u793e\u4f1a\u8bed\u7528\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u6570\u636e\u96c6\u4ee5\u6b64\u6765\u8bc4\u4f30VLM\u5728\u8be5\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u6587\u672c\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u5b83\u4eec\u5ffd\u7565\u4e86\u4e00\u4e2a\u91cd\u8981\u65b9\u9762\uff0c\u5373\u975e\u8a00\u8bed\u7ebf\u7d22\u5728\u7406\u89e3\u793e\u4f1a\u60c5\u666f\u4e2d\u7684\u4f5c\u7528\u3002VLM\u867d\u7136\u6709\u53ef\u80fd\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\uff0c\u4f46\u5176\u5728\u6b63\u786e\u63a8\u7406\u793e\u4f1a\u7ebf\u7d22\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e00\u4e2a\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u4efb\u52a1\uff1a\u89c6\u89c9\u793e\u4f1a\u8bed\u7528\u63a8\u7406\u3002\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u6765\u6d4b\u8bd5VLM\u5728\u8fd9\u79cd\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u5bf9\u51e0\u79cdVLM\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u4efb\u52a1\u548c\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6d4b\u91cfVLM\u5728\u89c6\u89c9\u793e\u4f1a\u8bed\u7528\u63a8\u7406\u65b9\u9762\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u63d0\u51fa\u4e00\u4e2a\u65b0\u4efb\u52a1\u548c\u6784\u5efa\u6570\u636e\u96c6\uff0c\u6211\u4eec\u4e3a\u8bc4\u4f30VLM\u5728\u89c6\u89c9\u793e\u4f1a\u8bed\u7528\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u57fa\u7ebf\uff0c\u63ed\u793a\u4e86\u5176\u5728\u7406\u89e3\u548c\u63a8\u7406\u590d\u6742\u793e\u4f1a\u60c5\u666f\u4e2d\u7684\u6f5c\u529b\u4e0e\u4e0d\u8db3\u3002"}}
{"id": "2506.11073", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.11073", "abs": "https://arxiv.org/abs/2506.11073", "authors": ["Zekai Ye", "Qiming Li", "Xiaocheng Feng", "Libo Qin", "Yichong Huang", "Baohang Li", "Kui Jiang", "Yang Xiang", "Zhirui Zhang", "Yunfei Lu", "Duyu Tang", "Dandan Tu", "Bing Qin"], "title": "CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention", "comment": "ACL2025 Main", "summary": "Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal\nabilities but remain prone to multilingual object hallucination, with a higher\nlikelihood of generating responses inconsistent with the visual input when\nutilizing queries in non-English languages compared to English. Most existing\napproaches to address these rely on pretraining or fine-tuning, which are\nresource-intensive. In this paper, inspired by observing the disparities in\ncross-modal attention patterns across languages, we propose Cross-Lingual\nAttention Intervention for Mitigating multilingual object hallucination (CLAIM)\nin LVLMs, a novel near training-free method by aligning attention patterns.\nCLAIM first identifies language-specific cross-modal attention heads, then\nestimates language shift vectors from English to the target language, and\nfinally intervenes in the attention outputs during inference to facilitate\ncross-lingual visual perception capability alignment. Extensive experiments\ndemonstrate that CLAIM achieves an average improvement of 13.56% (up to 30% in\nSpanish) on the POPE and 21.75% on the hallucination subsets of the MME\nbenchmark across various languages. Further analysis reveals that multilingual\nattention divergence is most prominent in intermediate layers, highlighting\ntheir critical role in multilingual scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CLAIM\u65b9\u6cd5\uff0c\u4e00\u79cd\u51e0\u4e4e\u4e0d\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u7684\u8de8\u8bed\u8a00\u6ce8\u610f\u529b\u5e72\u9884\u7b56\u7565\uff0c\u7528\u4e8e\u51cf\u5c11\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u8868\u73b0\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u975e\u82f1\u8bed\u67e5\u8be2\u65f6\uff0c\u76f8\u8f83\u4e8e\u82f1\u8bed\u67e5\u8be2\u66f4\u6613\u4ea7\u751f\u4e0e\u89c6\u89c9\u8f93\u5165\u4e0d\u4e00\u81f4\u7684\u56de\u5e94\u8fd9\u4e00\u95ee\u9898\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u6216\u5fae\u8c03\uff0c\u8d44\u6e90\u6d88\u8017\u5927\u3002", "method": "CLAIM\u65b9\u6cd5\u901a\u8fc7\u6821\u6b63\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u6a21\u5f0f\u6765\u7f13\u89e3\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(HVLM)\u7684\u5e7b\u89c9\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u8bc6\u522b\u7279\u5b9a\u8bed\u8a00\u7684\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5934\uff0c\u7136\u540e\u8ba1\u7b97\u4ece\u82f1\u8bed\u5230\u76ee\u6807\u8bed\u8a00\u7684\u6ce8\u610f\u529b\u8f6c\u6362\u5411\u91cf\uff0c\u5e76\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5e72\u9884\u6ce8\u610f\u529b\u8f93\u51fa\uff0c\u4ee5\u4fc3\u8fdb\u8de8\u8bed\u8a00\u95f4\u7684\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0cCLAIM\u65b9\u6cd5\u5728POPE\u548cMME\u57fa\u51c6\u6d4b\u8bd5\u7684\u5e7b\u89c9\u5b50\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5e73\u574713.56%\uff08\u5728\u897f\u73ed\u7259\u8bed\u4e0a\u7684\u6539\u8fdb\u9ad8\u8fbe30%\uff09\u548c21.75%\u7684\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u662f\uff0c\u901a\u8fc7\u8c03\u6574\u4e2d\u95f4\u5c42\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u4ece\u800c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8de8\u8bed\u8a00\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2506.11164", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11164", "abs": "https://arxiv.org/abs/2506.11164", "authors": ["Simon Ghyselincks", "Valeriia Okhmak", "Stefano Zampini", "George Turkiyyah", "David Keyes", "Eldad Haber"], "title": "Synthetic Geology -- Structural Geology Meets Deep Learning", "comment": "10 pages, 8 figures, submitted to \"Communications Earth &\n  Environment\", geological simulation code at\n  https://doi.org/10.5281/zenodo.15244035, generative AI code at\n  https://github.com/chipnbits/flowtrain_stochastic_interpolation/releases/tag/v1.0.0", "summary": "Visualizing the first few kilometers of the Earth's subsurface, a\nlong-standing challenge gating a virtually inexhaustible list of important\napplications, is coming within reach through deep learning. Building on\ntechniques of generative artificial intelligence applied to voxelated images,\nwe demonstrate a method that extends surface geological data supplemented by\nboreholes to a three-dimensional subsurface region by training a neural\nnetwork. The Earth's land area having been extensively mapped for geological\nfeatures, the bottleneck of this or any related technique is the availability\nof data below the surface. We close this data gap in the development of\nsubsurface deep learning by designing a synthetic data-generator process that\nmimics eons of geological activity such as sediment compaction, volcanic\nintrusion, and tectonic dynamics to produce a virtually limitless number of\nsamples of the near lithosphere. A foundation model trained on such synthetic\ndata is able to generate a 3D image of the subsurface from a previously unseen\nmap of surface topography and geology, showing increasing fidelity with\nincreasing access to borehole data, depicting such structures as layers,\nfaults, folds, dikes, and sills. We illustrate the early promise of the\ncombination of a synthetic lithospheric generator with a trained neural network\nmodel using generative flow matching. Ultimately, such models will be\nfine-tuned on data from applicable campaigns, such as mineral prospecting in a\ngiven region. Though useful in itself, a regionally fine-tuned models may be\nemployed not as an end but as a means: as an AI-based regularizer in a more\ntraditional inverse problem application, in which the objective function\nrepresents the mismatch of additional data with physical models with\napplications in resource exploration, hazard assessment, and geotechnical\nengineering.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u6280\u672f\u5bf9\u4f53\u7d20\u56fe\u50cf\u7684\u5e94\u7528\uff0c\u7814\u7a76\u4eba\u5458\u5c55\u793a\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u5c06\u8868\u9762\u5730\u8d28\u6570\u636e\u548c\u52d8\u6d4b\u5b54\u6570\u636e\u6269\u5c55\u5230\u4e09\u7ef4\u5730\u4e0b\u533a\u57df\u3002\u4ed6\u4eec\u901a\u8fc7\u8bbe\u8ba1\u4e00\u4e2a\u6a21\u62df\u5730\u8d28\u6d3b\u52a8\u4ece\u800c\u521b\u5efa\u5927\u91cf\u5408\u6210\u6570\u636e\u7684\u5408\u6210\u8fc7\u7a0b\u6765\u5f25\u8865\u5730\u4e0b\u6570\u636e\u7684\u4e0d\u8db3\uff0c\u4f7f\u5f97\u57fa\u7840\u6a21\u578b\u53ef\u4ee5\u4ece\u8868\u9762\u5730\u5f62\u548c\u5730\u8d28\u5730\u56fe\u751f\u6210\u9ad8\u4fdd\u771f\u7684\u5730\u4e0b3D\u56fe\u50cf\uff0c\u5e76\u8fdb\u4e00\u6b65\u5e94\u7528\u4e8e\u8d44\u6e90\u52d8\u63a2\u3001\u707e\u5bb3\u8bc4\u4f30\u548c\u5730\u57fa\u5de5\u7a0b\u7b49\u9886\u57df\u3002", "motivation": "\u957f\u671f\u4ee5\u6765\uff0c\u53ef\u89c6\u5316\u5730\u7403\u8868\u9762\u4ee5\u4e0b\u51e0\u516c\u91cc\u7684\u5730\u4e0b\u7ed3\u6784\u4e00\u76f4\u662f\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u4e00\u4e2a\u6311\u6218\uff0c\u963b\u788d\u4e86\u6570\u4e0d\u80dc\u6570\u7684\u91cd\u8981\u5e94\u7528\u3002\u4f5c\u8005\u7684\u52a8\u673a\u662f\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u7279\u522b\u662f\u4f7f\u7528\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u6a21\u578b\u4ee5\u514b\u670d\u5730\u4e0b\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u751f\u6210\u5730\u4e0b\u56fe\u50cf\u7684\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4eba\u5458\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u751f\u6210\u5408\u6210\u6570\u636e\u7684\u8fc7\u7a0b\u6765\u514b\u670d\u5730\u4e0b\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u5229\u7528\u8fd9\u4e2a\u5408\u6210\u6570\u636e\u6765\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u3002\u63a5\u4e0b\u6765\uff0c\u4ed6\u4eec\u4f7f\u7528\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e0e\u751f\u6210\u6d41\u5339\u914d\u65b9\u6cd5\u6765\u751f\u6210\u4ece\u8868\u9762\u5730\u8d28\u4e0e\u5730\u5f62\u5730\u56fe\u7684\u4e09\u7ef4\u5730\u4e0b\u56fe\u50cf\u3002", "result": "\u8be5\u6a21\u578b\u80fd\u591f\u6839\u636e\u672a\u89c1\u8fc7\u7684\u8868\u9762\u5730\u8d28\u5730\u56fe\u548c\u5730\u5f62\u6570\u636e\u751f\u6210\u9ad8\u4fdd\u771f\u7684\u4e09\u7ef4\u5730\u4e0b\u56fe\u50cf\uff0c\u800c\u4e14\u8fd8\u80fd\u591f\u968f\u7740\u52d8\u6d4b\u5b54\u6570\u636e\u7684\u589e\u52a0\uff0c\u56fe\u50cf\u4e2d\u5448\u73b0\u51fa\u66f4\u4e3a\u4e30\u5bcc\u7684\u5730\u4e0b\u7ed3\u6784\u7279\u5f81\uff0c\u5982\u5c42\u72b6\u7ed3\u6784\u3001\u65ad\u5c42\u3001\u8936\u76b1\u7b49\u5730\u8d28\u6784\u9020\u3002", "conclusion": "\u4f5c\u8005\u5c55\u793a\u4e86\u901a\u8fc7\u7ed3\u5408\u5408\u6210\u8fd1\u5730\u58f3\u751f\u6210\u5668\u548c\u8bad\u7ec3\u6709\u7d20\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u521d\u6b65\u6f5c\u529b\uff0c\u8fd9\u4e9b\u6a21\u578b\u672a\u6765\u53ef\u4ee5\u901a\u8fc7\u7279\u5b9a\u5730\u533a\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u4ece\u800c\u7ec6\u5316\uff0c\u5e94\u7528\u4e8e\u5982\u8d44\u6e90\u52d8\u63a2\u3001\u707e\u5bb3\u8bc4\u4f30\u3001\u5730\u57fa\u5de5\u7a0b\u7b49\u66f4\u5e7f\u6cdb\u7684\u9886\u57df\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u8fd9\u4e9b\u7ecf\u8fc7\u5730\u57df\u8c03\u6574\u7684\u6a21\u578b\u8fd8\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u57fa\u7840\u7684\u4eba\u5de5\u667a\u80fd\u6b63\u5219\u5316\u5de5\u5177\uff0c\u5728\u4f20\u7edf\u7684\u9006\u5411\u95ee\u9898\u5e94\u7528\u4e2d\u8d77\u5230\u8f85\u52a9\u4f5c\u7528\u3002"}}
{"id": "2506.11077", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.11077", "abs": "https://arxiv.org/abs/2506.11077", "authors": ["Chongyu Fan", "Yihua Zhang", "Jinghan Jia", "Alfred Hero", "Sijia Liu"], "title": "CyclicReflex: Improving Large Reasoning Models via Cyclical Reflection Token Scheduling", "comment": null, "summary": "Large reasoning models (LRMs), such as OpenAI's o1 and DeepSeek-R1, harness\ntest-time scaling to perform multi-step reasoning for complex problem-solving.\nThis reasoning process, executed before producing final answers, is often\nguided by special juncture tokens or textual segments that prompt\nself-evaluative reflection. We refer to these transition markers and reflective\ncues as \"reflection tokens\" (e.g., \"wait\", \"but\", \"alternatively\"). In this\nwork, we treat reflection tokens as a \"resource\" and introduce the problem of\nresource allocation, aimed at improving the test-time compute performance of\nLRMs by adaptively regulating the frequency and placement of reflection tokens.\nThrough empirical analysis, we show that both excessive and insufficient use of\nreflection tokens, referred to as over-reflection and under-reflection, can\ndegrade model performance. To better understand and manage this trade-off, we\ndraw an analogy between reflection token usage and learning rate scheduling in\noptimization. Building on this insight, we propose cyclical reflection token\nscheduling (termed CyclicReflex), a decoding strategy that dynamically\nmodulates reflection token logits using a position-dependent triangular\nwaveform. Experiments on MATH500, AIME2024/2025, and AMC2023 demonstrate that\nCyclicReflex consistently improves performance across model sizes (1.5B-8B),\noutperforming standard decoding and more recent approaches such as TIP (thought\nswitching penalty) and S1. Codes are available at\nhttps://github.com/OPTML-Group/CyclicReflex.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u53cd\u5c04\u6807\u8bb0\u5bf9\u5927\u63a8\u7406\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u7801\u7b56\u7565CyclicReflex\uff0c\u4ee5\u4f18\u5316\u8be5\u8fc7\u7a0b\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u53cd\u601d\u6807\u8bb0\u7684\u9891\u7387\u548c\u4f4d\u7f6e\u6765\u63d0\u9ad8\u5927\u63a8\u7406\u6a21\u578b\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\uff0c\u7814\u7a76\u4e86\u8fc7\u5ea6\u53cd\u601d\uff08\u8fc7\u91cf\u4f7f\u7528\u53cd\u601d\u6807\u8bb0\uff09\u548c\u4e0d\u8db3\u53cd\u601d\uff08\u4e0d\u8db3\u4f7f\u7528\u53cd\u601d\u6807\u8bb0\uff09\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCyclicReflex\u7684\u89e3\u7801\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u4f7f\u7528\u4f9d\u8d56\u4f4d\u7f6e\u7684\u4e09\u89d2\u6ce2\u52a8\u6001\u8c03\u6574\u53cd\u601d\u6807\u8bb0\u7684\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5bf9\u4e8e\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\uff081.5B-8B\uff09\uff0cCyclicReflex\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6027\u80fd\uff0c\u4f18\u4e8e\u6807\u51c6\u89e3\u7801\u548c\u5176\u4ed6\u65b9\u6cd5\uff08\u5982TIP\u548cS1\uff09\u3002", "conclusion": "\u8fd9\u662f\u4e00\u79cd\u6709\u6548\u7684\u7b56\u7565\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u4f18\u5316\u591a\u6b65\u63a8\u7406\u6a21\u578b\u6027\u80fd\u7684\u4e00\u4e2a\u624b\u6bb5\u3002"}}
{"id": "2506.11165", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11165", "abs": "https://arxiv.org/abs/2506.11165", "authors": ["Almustapha A. Wakili", "Babajide J. Asaju", "Woosub Jung"], "title": "Evaluating BiLSTM and CNN+GRU Approaches for Human Activity Recognition Using WiFi CSI Data", "comment": "This Paper has been Accepted and will appear in the 23rd IEEE/ACIS\n  International Conference on Software Engineering, Management and Applications\n  (SERA 2025)", "summary": "This paper compares the performance of BiLSTM and CNN+GRU deep learning\nmodels for Human Activity Recognition (HAR) on two WiFi-based Channel State\nInformation (CSI) datasets: UT-HAR and NTU-Fi HAR. The findings indicate that\nthe CNN+GRU model has a higher accuracy on the UT-HAR dataset (95.20%) thanks\nto its ability to extract spatial features. In contrast, the BiLSTM model\nperforms better on the high-resolution NTU-Fi HAR dataset (92.05%) by\nextracting long-term temporal dependencies more effectively. The findings\nstrongly emphasize the critical role of dataset characteristics and\npreprocessing techniques in model performance improvement. We also show the\nreal-world applicability of such models in applications like healthcare and\nintelligent home systems, highlighting their potential for unobtrusive activity\nrecognition.", "AI": {"tldr": "The paper compares BiLSTM and CNN+GRU models for Human Activity Recognition using WiFi CSI datasets, finding CNN+GRU better for spatial feature extraction on UT-HAR and BiLSTM superior for long-term temporal dependencies on NTU-Fi HAR. It underscores the importance of dataset features and preprocessing for model performance and real-world applications.", "motivation": "The motivation of the paper is to analyze which deep learning model, BiLSTM or CNN+GRU, performs better for human activity recognition using WiFi-based CSI datasets, to understand the impacts of different model architectures on accuracy under various dataset conditions.", "method": "The method involves training and evaluating BiLSTM and CNN+GRU models on two different WiFi-based CSI datasets: UT-HAR and NTU-Fi HAR, and comparing their performance in terms of accuracy and their capability in extracting spatial and temporal features.", "result": "The results show that the CNN+GRU model performs better on the UT-HAR dataset with an accuracy of 95.20%, while the BiLSTM model exceeds on the NTU-Fi HAR dataset with an accuracy of 92.05%. The performance is attributed to the CNN+GRU's advantage in extracting spatial features and BiLSTM's effectiveness in capturing long-term temporal dependencies.", "conclusion": "The conclusion is that the efficacy of deep learning models for Human Activity Recognition significantly depends on the dataset characteristics and the preprocessing techniques. The study also highlights the potential of these models for real-world applications, such as in healthcare and intelligent home systems, for unobtrusive activity recognition."}}
{"id": "2506.11078", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.11078", "abs": "https://arxiv.org/abs/2506.11078", "authors": ["Yuzhou Yang", "Yangming Zhou", "Zhiying Zhu", "Zhenxing Qian", "Xinpeng Zhang", "Sheng Li"], "title": "RoE-FND: A Case-Based Reasoning Approach with Dual Verification for Fake News Detection via LLMs", "comment": null, "summary": "The proliferation of deceptive content online necessitates robust Fake News\nDetection (FND) systems. While evidence-based approaches leverage external\nknowledge to verify claims, existing methods face critical limitations: noisy\nevidence selection, generalization bottlenecks, and unclear decision-making\nprocesses. Recent efforts to harness Large Language Models (LLMs) for FND\nintroduce new challenges, including hallucinated rationales and conclusion\nbias. To address these issues, we propose \\textbf{RoE-FND}\n(\\textbf{\\underline{R}}eason \\textbf{\\underline{o}}n\n\\textbf{\\underline{E}}xperiences FND), a framework that reframes evidence-based\nFND as a logical deduction task by synergizing LLMs with experiential learning.\nRoE-FND encompasses two stages: (1) \\textit{self-reflective knowledge\nbuilding}, where a knowledge base is curated by analyzing past reasoning\nerrors, namely the exploration stage, and (2) \\textit{dynamic criterion\nretrieval}, which synthesizes task-specific reasoning guidelines from\nhistorical cases as experiences during deployment. It further cross-checks\nrationales against internal experience through a devised dual-channel\nprocedure. Key contributions include: a case-based reasoning framework for FND\nthat addresses multiple existing challenges, a training-free approach enabling\nadaptation to evolving situations, and empirical validation of the framework's\nsuperior generalization and effectiveness over state-of-the-art methods across\nthree datasets.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u5047\u65b0\u95fb\u68c0\u6d4b\u6846\u67b6RoE-FND\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u7ecf\u9a8c\u5b66\u4e60\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6709\u6548\u6027\u3002", "motivation": "\u5728\u7ebf\u865a\u5047\u5185\u5bb9\u7684\u6cdb\u6ee5\u9700\u8981\u5f3a\u5927\u7684\u5047\u65b0\u95fb\u68c0\u6d4b\u7cfb\u7edf\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u57fa\u4e8e\u8bc1\u636e\u7684\u65b9\u6cd5\u5b58\u5728\u566a\u58f0\u8bc1\u636e\u9009\u62e9\u3001\u6cdb\u5316\u74f6\u9888\u548c\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u900f\u660e\u7b49\u5173\u952e\u9650\u5236\u3002\u8fd1\u5e74\u6765\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u5047\u65b0\u95fb\u68c0\u6d4b\u7684\u52aa\u529b\u63d0\u51fa\u4e86\u65b0\u7684\u6311\u6218\uff0c\u5305\u62ec\u865a\u6784\u7684\u7406\u7531\u548c\u7ed3\u8bba\u504f\u5dee\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86RoE-FND\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91cd\u65b0\u5b9a\u4e49\u4e86\u57fa\u4e8e\u8bc1\u636e\u7684\u5047\u65b0\u95fb\u68c0\u6d4b\u4e3a\u903b\u8f91\u6f14\u7ece\u4efb\u52a1\u3002RoE-FND\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\uff1a1) \u81ea\u6211\u53cd\u601d\u77e5\u8bc6\u6784\u5efa\uff0c\u5373\u901a\u8fc7\u5206\u6790\u8fc7\u53bb\u7684\u63a8\u7406\u9519\u8bef\u6765\u5efa\u7acb\u77e5\u8bc6\u5e93\u7684\u63a2\u7d22\u9636\u6bb5\uff1b2) \u52a8\u6001\u6807\u51c6\u68c0\u7d22\uff0c\u5373\u5728\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u4ece\u5386\u53f2\u6848\u4f8b\u4e2d\u5408\u6210\u7279\u5b9a\u4efb\u52a1\u7684\u63a8\u7406\u6307\u5357\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u8bbe\u8ba1\u7684\u53cc\u901a\u9053\u7a0b\u5e8f\u6765\u4ea4\u53c9\u9a8c\u8bc1\u63a8\u7406\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u60c5\u51b5\uff0c\u5e76\u63d0\u4f9b\u4e86\u8d85\u8d8a\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6cdb\u5316\u80fd\u529b\u548c\u6709\u6548\u6027\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2506.11166", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11166", "abs": "https://arxiv.org/abs/2506.11166", "authors": ["Ji Young Byun", "Young-Jin Park", "Navid Azizan", "Rama Chellappa"], "title": "Test-Time-Scaling for Zero-Shot Diagnosis with Visual-Language Reasoning", "comment": null, "summary": "As a cornerstone of patient care, clinical decision-making significantly\ninfluences patient outcomes and can be enhanced by large language models\n(LLMs). Although LLMs have demonstrated remarkable performance, their\napplication to visual question answering in medical imaging, particularly for\nreasoning-based diagnosis, remains largely unexplored. Furthermore, supervised\nfine-tuning for reasoning tasks is largely impractical due to limited data\navailability and high annotation costs. In this work, we introduce a zero-shot\nframework for reliable medical image diagnosis that enhances the reasoning\ncapabilities of LLMs in clinical settings through test-time scaling. Given a\nmedical image and a textual prompt, a vision-language model processes a medical\nimage along with a corresponding textual prompt to generate multiple\ndescriptions or interpretations of visual features. These interpretations are\nthen fed to an LLM, where a test-time scaling strategy consolidates multiple\ncandidate outputs into a reliable final diagnosis. We evaluate our approach\nacross various medical imaging modalities -- including radiology,\nophthalmology, and histopathology -- and demonstrate that the proposed\ntest-time scaling strategy enhances diagnostic accuracy for both our and\nbaseline methods. Additionally, we provide an empirical analysis showing that\nthe proposed approach, which allows unbiased prompting in the first stage,\nimproves the reliability of LLM-generated diagnoses and enhances classification\naccuracy.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u5b66\u4e60\u548c\u6d4b\u8bd5\u65f6\u7684\u7f29\u653e\u7b56\u7565\uff0c\u589e\u5f3a\u4e86LLMs\u5728\u533b\u5b66\u56fe\u50cf\u8bca\u65ad\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u867d\u7136LLMs\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u533b\u5b66\u56fe\u50cf\u89c6\u89c9\u95ee\u7b54\u7279\u522b\u662f\u63a8\u7406\u6027\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\u4ecd\u7136\u5f88\u5c11\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u6570\u636e\u6709\u9650\u548c\u9ad8\u6ce8\u91ca\u6210\u672c\uff0c\u76d1\u7763\u5fae\u8c03\u5bf9\u4e8e\u63a8\u7406\u4efb\u52a1\u6765\u8bf4\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u662f\u4e0d\u5207\u5b9e\u9645\u7684\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u6846\u67b6\uff0c\u901a\u8fc7\u6d4b\u8bd5\u65f6\u7f29\u653e\u7b56\u7565\uff0c\u589e\u5f3aLLMs\u5728\u4e34\u5e8a\u73af\u5883\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u3002\u7ed9\u5b9a\u4e00\u5f20\u533b\u5b66\u56fe\u50cf\u548c\u4e00\u4e2a\u6587\u672c\u63d0\u793a\uff0c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4f1a\u5904\u7406\u56fe\u50cf\u548c\u76f8\u5e94\u7684\u6587\u672c\u63d0\u793a\uff0c\u751f\u6210\u591a\u4e2a\u5bf9\u89c6\u89c9\u7279\u5f81\u7684\u63cf\u8ff0\u6216\u89e3\u91ca\u3002\u7136\u540e\u8fd9\u4e9b\u89e3\u91ca\u88ab\u8f93\u5165\u5230LLM\u4e2d\uff0c\u4f7f\u7528\u6d4b\u8bd5\u65f6\u7f29\u653e\u7b56\u7565\u5c06\u591a\u4e2a\u5019\u9009\u8f93\u51fa\u6574\u5408\u6210\u4e00\u4e2a\u53ef\u9760\u7684\u6700\u7ec8\u8bca\u65ad\u3002", "result": "\u4f5c\u8005\u7684\u8bc4\u4f30\u8de8\u8d8a\u4e86\u591a\u79cd\u533b\u5b66\u6210\u50cf\u6a21\u5f0f\uff0c\u5305\u62ec\u653e\u5c04\u5b66\u3001\u773c\u79d1\u548c\u75c5\u7406\u5b66\uff0c\u5e76\u8868\u660e\u63d0\u51fa\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u7b56\u7565\u80fd\u63d0\u9ad8\u8be5\u65b9\u6cd5\u548c\u57fa\u7ebf\u65b9\u6cd5\u7684\u8bca\u65ad\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u96f6\u6837\u672c\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u6d4b\u8bd5\u65f6\u5e94\u7528\u7f29\u653e\u7b56\u7565\uff0c\u80fd\u591f\u63d0\u5347LLMs\u751f\u6210\u8bca\u65ad\u7684\u53ef\u9760\u6027\uff0c\u5e76\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u3002"}}
{"id": "2506.11080", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.11080", "abs": "https://arxiv.org/abs/2506.11080", "authors": ["Han Zhou", "Qitong Xu", "Yiheng Dong", "Xin Yang"], "title": "MANBench: Is Your Multimodal Model Smarter than Human?", "comment": "Multimodal Benchmark, Project Url: https://github.com/micdz/MANBench,\n  ACL2025 Findings", "summary": "The rapid advancement of Multimodal Large Language Models (MLLMs) has ignited\ndiscussions regarding their potential to surpass human performance in\nmultimodal tasks. In response, we introduce MANBench (Multimodal Ability Norms\nBenchmark), a bilingual benchmark (English and Chinese) comprising 1,314\nquestions across nine tasks, spanning knowledge-based and non-knowledge-based\ndomains. MANBench emphasizes intuitive reasoning, seamless cross-modal\nintegration, and real-world complexity, providing a rigorous evaluation\nframework.\n  Through extensive human experiments involving diverse participants, we\ncompared human performance against state-of-the-art MLLMs. The results indicate\nthat while MLLMs excel in tasks like Knowledge and Text-Image Understanding,\nthey struggle with deeper cross-modal reasoning tasks such as Transmorphic\nUnderstanding, Image Consistency, and Multi-image Understanding. Moreover, both\nhumans and MLLMs face challenges in highly complex tasks like Puzzles and\nSpatial Imagination.\n  MANBench highlights the strengths and limitations of MLLMs, revealing that\neven advanced models fall short of achieving human-level performance across\nmany domains. We hope MANBench will inspire efforts to bridge the gap between\nMLLMs and human multimodal capabilities. The code and dataset are available at\nhttps://github.com/micdz/MANBench.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u591a\u6a21\u6001\u80fd\u529b\u7684\u53cc\u8bed\u57fa\u51c6MANBench\u3002\u8be5\u57fa\u51c6\u88ab\u8bbe\u8ba1\u4e3a\u7528\u4e8e\u6bd4\u8f83\u4eba\u7c7b\u548cMLLMs\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u7814\u7a76\u7ed3\u679c\u663e\u793a\u5c3d\u7ba1MLLMs\u5728\u77e5\u8bc6\u7406\u89e3\u548c\u6587\u672c\u56fe\u50cf\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u590d\u6742\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u4ecd\u8fdc\u8fdc\u4e0d\u53ca\u4eba\u7c7b\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5b83\u4eec\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4eba\u7c7b\u6027\u80fd\u7684\u53ef\u80fd\u6027\u5f15\u53d1\u4e86\u8ba8\u8bba\u3002\u4e3a\u56de\u5e94\u8fd9\u4e00\u8d8b\u52bf\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u4efb\u52a1\u8bc4\u4f30\u6846\u67b6\uff0c\u7814\u7a76\u5e76\u5bf9\u6bd4\u4eba\u7c7b\u4e0eMLLMs\u7684\u591a\u6a21\u6001\u80fd\u529b\u5dee\u5f02\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86MANBench\uff08\u591a\u6a21\u6001\u80fd\u529b\u89c4\u8303\u57fa\u51c6\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u6db5\u76d69\u4e2a\u4efb\u52a1\u3001\u5305\u62ec1,314\u4e2a\u95ee\u9898\u7684\u53cc\u8bed\uff08\u82f1\u8bed\u548c\u4e2d\u6587\uff09\u591a\u6a21\u6001\u4efb\u52a1\u8bc4\u4f30\u6846\u67b6\u3002\u8be5\u57fa\u51c6\u5f3a\u8c03\u76f4\u89c2\u63a8\u7406\u3001\u65e0\u7f1d\u8de8\u6a21\u6001\u96c6\u6210\u548c\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u6027\uff0c\u5e76\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u4eba\u7c7b\u548c\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u6d89\u53ca\u4e0d\u540c\u53c2\u4e0e\u8005\u7684\u5e7f\u6cdb\u4eba\u7c7b\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u4e00\u4e9b\u4efb\u52a1\uff08\u5982\u77e5\u8bc6\u7406\u89e3\u4e0e\u6587\u672c\u56fe\u50cf\u7406\u89e3\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u4e8e\u9700\u8981\u6df1\u5165\u8de8\u6a21\u6001\u63a8\u7406\u7684\u4efb\u52a1\uff08\u5982\u8f6c\u6362\u7406\u89e3\u3001\u56fe\u50cf\u4e00\u81f4\u6027\u548c\u591a\u91cd\u56fe\u50cf\u7406\u89e3\uff09\uff0cMLLMs\u7684\u80fd\u529b\u4ecd\u7136\u4e0d\u8db3\u3002\u540c\u65f6\uff0c\u65e0\u8bba\u662f\u4eba\u7c7b\u8fd8\u662fMLLMs\uff0c\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\uff08\u5982\u8c1c\u9898\u548c\u7a7a\u95f4\u60f3\u8c61\u529b\uff09\u65f6\u90fd\u9762\u4e34\u4e00\u5b9a\u7684\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1MLLMs\u5728\u8bf8\u5982\u77e5\u8bc6\u7406\u89e3\u548c\u6587\u672c\u56fe\u50cf\u7406\u89e3\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u8de8\u6a21\u6001\u6df1\u5ea6\u63a8\u7406\u7684\u4efb\u52a1\u4e0a\uff0c\u5982\u8f6c\u6362\u7406\u89e3\u3001\u56fe\u50cf\u4e00\u81f4\u6027\u548c\u591a\u56fe\u50cf\u7406\u89e3\uff0c\u4ed6\u4eec\u8868\u73b0\u4e0d\u4f73\u3002\u53e6\u5916\uff0c\u4eba\u7c7b\u548cMLLMs\u5728\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u5982\u62fc\u56fe\u548c\u7a7a\u95f4\u60f3\u8c61\u4e2d\u90fd\u9762\u4e34\u6311\u6218\u3002MANBench\u63ed\u793a\u4e86MLLMs\u7684\u4f18\u52bf\u4e0e\u4e0d\u8db3\uff0c\u5e76\u6307\u51fa\u5373\u4fbf\u662f\u5148\u8fdb\u7684\u6a21\u578b\u4e5f\u672a\u80fd\u5728\u8bb8\u591a\u9886\u57df\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u7684\u8868\u73b0\uff0c\u65e8\u5728\u6fc0\u52b1\u7814\u7a76\u8005\u4eec\u52aa\u529b\u7f29\u5c0fMLLMs\u548c\u4eba\u7c7b\u591a\u6a21\u6001\u80fd\u529b\u7684\u5dee\u8ddd\u3002"}}
{"id": "2506.11167", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11167", "abs": "https://arxiv.org/abs/2506.11167", "authors": ["Cheng Wang", "Yu Jiang", "Zhihao Peng", "Chenxin Li", "Changbae Bang", "Lin Zhao", "Jinglei Lv", "Jorge Sepulcre", "Carl Yang", "Lifang He", "Tianming Liu", "Daniel Barron", "Quanzheng Li", "Randy Hirschtick", "Byung-Hoon Kim", "Xiang Li", "Yixuan Yuan"], "title": "Towards a general-purpose foundation model for fMRI analysis", "comment": null, "summary": "Functional Magnetic Resonance Imaging (fMRI) is essential for studying brain\nfunction and diagnosing neurological disorders, but current analysis methods\nface reproducibility and transferability issues due to complex pre-processing\nand task-specific models. We introduce NeuroSTORM (Neuroimaging Foundation\nModel with Spatial-Temporal Optimized Representation Modeling), a generalizable\nframework that directly learns from 4D fMRI volumes and enables efficient\nknowledge transfer across diverse applications. NeuroSTORM is pre-trained on\n28.65 million fMRI frames (>9,000 hours) from over 50,000 subjects across\nmultiple centers and ages 5 to 100. Using a Mamba backbone and a shifted\nscanning strategy, it efficiently processes full 4D volumes. We also propose a\nspatial-temporal optimized pre-training approach and task-specific prompt\ntuning to improve transferability. NeuroSTORM outperforms existing methods\nacross five tasks: age/gender prediction, phenotype prediction, disease\ndiagnosis, fMRI-to-image retrieval, and task-based fMRI classification. It\ndemonstrates strong clinical utility on datasets from hospitals in the U.S.,\nSouth Korea, and Australia, achieving top performance in disease diagnosis and\ncognitive phenotype prediction. NeuroSTORM provides a standardized, open-source\nfoundation model to improve reproducibility and transferability in fMRI-based\nclinical research.", "AI": {"tldr": "NeuroSTORM\u662f\u4e00\u4e2a\u57fa\u4e8e4D fMRI\u5b66\u4e60\u7684\u901a\u7528\u6846\u67b6\uff0c\u65e8\u5728\u6539\u5584fMRI\u5206\u6790\u7684\u53ef\u91cd\u590d\u6027\u548c\u53ef\u79fb\u690d\u6027\uff0c\u5728\u591a\u4e2a\u4e34\u5e8a\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u76ee\u524dfMRI\u5206\u6790\u65b9\u6cd5\u5728\u53ef\u91cd\u590d\u6027\u548c\u53ef\u79fb\u690d\u6027\u65b9\u9762\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u7531\u590d\u6742\u7684\u9884\u5904\u7406\u548c\u7279\u5b9a\u4efb\u52a1\u6a21\u578b\u5f15\u8d77\u3002", "method": "\u63d0\u51fa\u4e86NeuroSTORM\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u76f4\u63a5\u4ece4D fMRI\u4f53\u79ef\u4e2d\u5b66\u4e60\uff0c\u5e76\u4fc3\u8fdb\u77e5\u8bc6\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u7684\u9ad8\u6548\u8f6c\u79fb\u3002NeuroSTORM\u4f7f\u7528Mamba\u9aa8\u5e72\u548c\u79fb\u4f4d\u626b\u63cf\u7b56\u7565\u6765\u5904\u7406\u5b8c\u6574\u76844D\u4f53\u79ef\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4-\u65f6\u95f4\u4f18\u5316\u9884\u8bad\u7ec3\u65b9\u6cd5\u548c\u4efb\u52a1\u7279\u5b9a\u7684\u63d0\u793a\u5fae\u8c03\u4ee5\u63d0\u9ad8\u8f6c\u79fb\u80fd\u529b\u3002", "result": "\u8be5\u6a21\u578b\u5728\u591a\u79cd\u4e34\u5e8a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u75be\u75c5\u8bca\u65ad\u548c\u8ba4\u77e5\u8868\u578b\u9884\u6d4b\u65b9\u9762\uff0c\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u3002", "conclusion": "NeuroSTORM\u8bc1\u660e\u4e86\u5176\u5728\u5e74\u9f84/\u6027\u522b\u9884\u6d4b\u3001\u8868\u578b\u9884\u6d4b\u3001\u75be\u75c5\u8bca\u65ad\u3001fMRI-to-image\u68c0\u7d22\u548c\u57fa\u4e8e\u4efb\u52a1\u7684fMRI\u5206\u7c7b\u7b49\u4e94\u4e2a\u4efb\u52a1\u4e0a\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u7f8e\u56fd\u3001\u97e9\u56fd\u548c\u6fb3\u5927\u5229\u4e9a\u533b\u9662\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5f3a\u5927\u4e34\u5e8a\u5e94\u7528\u80fd\u529b\u3002"}}
{"id": "2506.11081", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.11081", "abs": "https://arxiv.org/abs/2506.11081", "authors": ["Aditi", "Hyunwoo Park", "Sicheol Sung", "Yo-Sub Han", "Sang-Ki Ko"], "title": "SAGE:Specification-Aware Grammar Extraction for Automated Test Case Generation with LLMs", "comment": null, "summary": "Grammar-based test case generation has proven effective for competitive\nprogramming problems, but generating valid and general grammars from natural\nlanguage specifications remains a key challenge, especially under limited\nsupervision. Context-Free Grammars with Counters (CCFGs) have recently been\nintroduced as a formalism to represent such specifications with logical\nconstraints by storing and reusing counter values during derivation. In this\nwork, we explore the use of open-source large language models (LLMs) to induce\nCCFGs from specifications using a small number of labeled examples and\nverifiable reward-guided reinforcement learning. Our approach first fine-tunes\nan open-source LLM to perform specification-to-grammar translation, and further\napplies Group Relative Policy Optimization (GRPO) to enhance grammar validity\nand generality. We also examine the effectiveness of iterative feedback for\nopen and closed-source LLMs in correcting syntactic and semantic errors in\ngenerated grammars.\n  Experimental results show that our approach SAGE achieves stronger\ngeneralization and outperforms 17 open and closed-source LLMs in both grammar\nquality and test effectiveness, improving over the state-of-the-art by 15.92%p\nin grammar validity and 12.34%p in test effectiveness. We provide our\nimplementation and dataset at the following anonymous\nrepository:https://anonymous.4open.science/r/SAGE-5714", "AI": {"tldr": "The paper introduces SAGE, a method for generating Context-Free Grammars with Counters (CCFGs) using reinforcement learning-based fine-tuning of large language models, showing significant improvements in grammar validity and test effectiveness for competitive programming problems.", "motivation": "To address the challenge of generating valid and general grammars from natural language specifications under limited supervision for competitive programming problems.", "method": "Grammar-based test case generation using open-source large language models (LLMs) and Group Relative Policy Optimization (GRPO) for inducing Context-Free Grammars with Counters (CCFGs).", "result": "The approach, named SAGE, demonstrates superior generalization and outperforms 17 other LLMs in grammar quality and test effectiveness, showing significant improvements in grammar validity and test effectiveness.", "conclusion": "The study concludes that leveraging fine-tuned open-source LLMs and reinforcement learning improves the generation of effective grammars from specifications, leading to improved test case generation in competitive programming."}}
{"id": "2506.11168", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.11168", "abs": "https://arxiv.org/abs/2506.11168", "authors": ["Yanlong Chen", "Mattia Orlandi", "Pierangelo Maria Rapa", "Simone Benatti", "Luca Benini", "Yawei Li"], "title": "WaveFormer: A Lightweight Transformer Model for sEMG-based Gesture Recognition", "comment": "6 pages, 3 figures, submitted to IEEE EMBS Conference on Neural\n  Engineering (NER)", "summary": "Human-machine interaction, particularly in prosthetic and robotic control,\nhas seen progress with gesture recognition via surface electromyographic (sEMG)\nsignals.However, classifying similar gestures that produce nearly identical\nmuscle signals remains a challenge, often reducing classification accuracy.\nTraditional deep learning models for sEMG gesture recognition are large and\ncomputationally expensive, limiting their deployment on resource-constrained\nembedded systems. In this work, we propose WaveFormer, a lightweight\ntransformer-based architecture tailored for sEMG gesture recognition. Our model\nintegrates time-domain and frequency-domain features through a novel learnable\nwavelet transform, enhancing feature extraction. In particular, the WaveletConv\nmodule, a multi-level wavelet decomposition layer with depthwise separable\nconvolution, ensures both efficiency and compactness. With just 3.1 million\nparameters, WaveFormer achieves 95% classification accuracy on the EPN612\ndataset, outperforming larger models. Furthermore, when profiled on a laptop\nequipped with an Intel CPU, INT8 quantization achieves real-time deployment\nwith a 6.75 ms inference latency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faWaveFormer\u6a21\u578b\uff0c\u89e3\u51b3sEMG\u624b\u52bf\u8bc6\u522b\u9886\u57df\u76f8\u4f3c\u624b\u52bf\u5206\u7c7b\u96be\u4e0e\u80fd\u8017\u5927\u7684\u95ee\u9898\uff0c\u5728\u6709\u9650\u8d44\u6e90\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u624b\u52bf\u8bc6\u522b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfsEMG\u624b\u52bf\u8bc6\u522b\u5b58\u5728\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u76f8\u4f3c\u624b\u52bf\u8bc6\u522b\u51c6\u786e\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u5d4c\u5165\u5f0f\u7cfb\u7edf\u3002", "method": "\u5229\u7528WaveFormer\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u53d8\u538b\u5668\u67b6\u6784\uff0c\u8be5\u6a21\u578b\u901a\u8fc7WaveletConv\u6a21\u5757\u8fdb\u884c\u591a\u7ea7\u5c0f\u6ce2\u5206\u89e3\u7ed3\u5408\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u6765\u6709\u6548\u63d0\u53d6\u7279\u5f81\u3002", "result": "\u901a\u8fc7\u63d0\u51faWaveFormer\uff0c\u4e00\u79cd\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u53d8\u538b\u5668\u67b6\u6784\u7684\u808c\u8089\u7535\u4fe1\u53f7\uff08sEMG\uff09\u624b\u52bf\u8bc6\u522b\u65b9\u6848\uff0c\u672c\u7814\u7a76\u89e3\u51b3\u4e86\u4f20\u7edf\u6a21\u578b\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e0a\u90e8\u7f72\u65f6\u5b58\u5728\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u53ca\u5904\u7406\u76f8\u4f3c\u624b\u52bf\u65f6\u5206\u7c7b\u51c6\u786e\u7387\u4f4e\u7684\u95ee\u9898\u3002WaveFormer\u96c6\u6210\u4e86\u65f6\u57df\u548c\u9891\u57df\u7279\u5f81\uff0c\u901a\u8fc7\u4e00\u79cd\u65b0\u578b\u7684\u53ef\u5b66\u4e60\u5c0f\u6ce2\u53d8\u6362\uff0c\u5728\u4fdd\u6301\u7d27\u51d1\u6027\u548c\u6548\u7387\u7684\u540c\u65f6\u6709\u6548\u5730\u63d0\u53d6\u7279\u5f81\uff0c\u4e14\u4ec5\u542b310\u4e07\u53c2\u6570\uff0c\u5728EPN612\u6570\u636e\u96c6\u4e2d\u8fbe\u523095%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u8d85\u8fc7\u5927\u578b\u6a21\u578b\uff0c\u5e76\u5b9e\u73b0\u91cf\u5316\u540e\u7684\u5b9e\u65f6\u90e8\u7f72\uff0c\u5177\u67096.75\u6beb\u79d2\u7684\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "WaveFormer\u901a\u8fc7\u96c6\u6210\u65f6\u57df\u548c\u9891\u57df\u7279\u5f81\u5e76\u5229\u7528\u6df7\u5408\u5c0f\u6ce2\u5377\u79ef\u6a21\u5757\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7684\u624b\u52bf\u8bc6\u522b\u51c6\u786e\u7387\u548c\u6548\u7387\uff0c\u5e76\u5b9e\u73b0\u4e86\u5b9e\u65f6\u63a8\u7406\u3002"}}
{"id": "2506.11082", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11082", "abs": "https://arxiv.org/abs/2506.11082", "authors": ["Lionel Levine", "John Santerre", "Alex S. Young", "T. Barry Levine", "Francis Campion", "Majid Sarrafzadeh"], "title": "PRISM: A Transformer-based Language Model of Structured Clinical Event Data", "comment": "15 pages, 4 Figures, 1 Table", "summary": "We introduce PRISM (Predictive Reasoning in Sequential Medicine), a\ntransformer-based architecture designed to model the sequential progression of\nclinical decision-making processes. Unlike traditional approaches that rely on\nisolated diagnostic classification, PRISM frames clinical trajectories as\ntokenized sequences of events - including diagnostic tests, laboratory results,\nand diagnoses - and learns to predict the most probable next steps in the\npatient diagnostic journey. Leveraging a large custom clinical vocabulary and\nan autoregressive training objective, PRISM demonstrates the ability to capture\ncomplex dependencies across longitudinal patient timelines. Experimental\nresults show substantial improvements over random baselines in next-token\nprediction tasks, with generated sequences reflecting realistic diagnostic\npathways, laboratory result progressions, and clinician ordering behaviors.\nThese findings highlight the feasibility of applying generative language\nmodeling techniques to structured medical event data, enabling applications in\nclinical decision support, simulation, and education. PRISM establishes a\nfoundation for future advancements in sequence-based healthcare modeling,\nbridging the gap between machine learning architectures and real-world\ndiagnostic reasoning.", "AI": {"tldr": "PRISM\uff0c\u4e00\u79cd\u7528\u4e8e\u9884\u6d4b\u4e34\u5e8a\u51b3\u7b56\u8fc7\u7a0b\u987a\u5e8f\u8fdb\u5c55\u7684\u53d8\u538b\u5668\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5728\u9884\u6d4b\u4e0b\u4e00\u4e2a\u4e8b\u4ef6\u65f6\u76f8\u6bd4\u4e8e\u968f\u673a\u57fa\u7ebf\u7684\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u4e0e\u4f9d\u8d56\u5b64\u7acb\u8bca\u65ad\u5206\u7c7b\u7684\u4f20\u7edf\u65b9\u6cd5\u4e0d\u540c\uff0cPRISM\u4f7f\u7528\u81ea\u56de\u5f52\u8bad\u7ec3\u76ee\u6807\u548c\u5927\u578b\u5b9a\u5236\u4e34\u5e8a\u8bcd\u6c47\u8868\uff0c\u80fd\u591f\u6355\u6349\u8de8\u8d8a\u7eb5\u5411\u60a3\u8005\u65f6\u95f4\u7ebf\u7d22\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u5f15\u5165\u4e86\u57fa\u4e8e\u53d8\u538b\u5668\u67b6\u6784\u7684PRISM\u6a21\u578b\uff0c\u7528\u4e8e\u6a21\u62df\u4e34\u5e8a\u51b3\u7b56\u8fc7\u7a0b\u7684\u987a\u5e8f\u8fdb\u5c55\u3002PRISM\u5c06\u4e34\u5e8a\u8f68\u8ff9\u89c6\u4e3a\u5305\u62ec\u8bca\u65ad\u6d4b\u8bd5\u3001\u5b9e\u9a8c\u5ba4\u7ed3\u679c\u548c\u8bca\u65ad\u4e8b\u4ef6\u7684\u6807\u8bb0\u5e8f\u5217\uff0c\u5e76\u5b66\u4e60\u9884\u6d4b\u60a3\u8005\u8bca\u65ad\u65c5\u7a0b\u4e2d\u4e0b\u4e00\u6b65\u6700\u53ef\u80fd\u53d1\u751f\u7684\u4e8b\u4ef6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u4efb\u52a1\u4e0a\uff0cPRISM\u76f8\u6bd4\u4e8e\u968f\u673a\u57fa\u7ebf\u6709\u4e86\u663e\u8457\u63d0\u5347\uff0c\u751f\u6210\u5e8f\u5217\u53cd\u6620\u4e86\u5b9e\u9645\u7684\u8bca\u65ad\u8def\u5f84\u3001\u5b9e\u9a8c\u5ba4\u7ed3\u679c\u8fdb\u5c55\u548c\u4e34\u5e8a\u533b\u751f\u7684\u64cd\u4f5c\u4e60\u60ef\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u51f8\u663e\u4e86\u5c06\u751f\u6210\u8bed\u8a00\u5efa\u6a21\u6280\u672f\u5e94\u7528\u4e8e\u7ed3\u6784\u5316\u533b\u7597\u4e8b\u4ef6\u6570\u636e\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5f00\u542f\u4e86\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u3001\u6a21\u62df\u548c\u6559\u80b2\u7684\u65b0\u5e94\u7528\u3002PRISM\u4e3a\u672a\u6765\u7684\u57fa\u4e8e\u5e8f\u5217\u7684\u5065\u5eb7\u7ba1\u7406\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u7f29\u5c0f\u4e86\u673a\u5668\u5b66\u4e60\u6846\u67b6\u4e0e\u73b0\u5b9e\u4e16\u754c\u8bca\u65ad\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2506.11175", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.11175", "abs": "https://arxiv.org/abs/2506.11175", "authors": ["Hongyu Chen", "Jiping Liu", "Yong Wang", "Jun Zhu", "Dejun Feng", "Yakun Xie"], "title": "Teaching in adverse scenes: a statistically feedback-driven threshold and mask adjustment teacher-student framework for object detection in UAV images under adverse scenes", "comment": "The manuscript has been accepted by ISPRS Journal of Photogrammetry\n  and Remote Sensing", "summary": "Unsupervised Domain Adaptation (UDA) has shown promise in effectively\nalleviating the performance degradation caused by domain gaps between source\nand target domains, and it can potentially be generalized to UAV object\ndetection in adverse scenes. However, existing UDA studies are based on natural\nimages or clear UAV imagery, and research focused on UAV imagery in adverse\nconditions is still in its infancy. Moreover, due to the unique perspective of\nUAVs and the interference from adverse conditions, these methods often fail to\naccurately align features and are influenced by limited or noisy pseudo-labels.\nTo address this, we propose the first benchmark for UAV object detection in\nadverse scenes, the Statistical Feedback-Driven Threshold and Mask Adjustment\nTeacher-Student Framework (SF-TMAT). Specifically, SF-TMAT introduces a design\ncalled Dynamic Step Feedback Mask Adjustment Autoencoder (DSFMA), which\ndynamically adjusts the mask ratio and reconstructs feature maps by integrating\ntraining progress and loss feedback. This approach dynamically adjusts the\nlearning focus at different training stages to meet the model's needs for\nlearning features at varying levels of granularity. Additionally, we propose a\nunique Variance Feedback Smoothing Threshold (VFST) strategy, which\nstatistically computes the mean confidence of each class and dynamically\nadjusts the selection threshold by incorporating a variance penalty term. This\nstrategy improves the quality of pseudo-labels and uncovers potentially valid\nlabels, thus mitigating domain bias. Extensive experiments demonstrate the\nsuperiority and generalization capability of the proposed SF-TMAT in UAV object\ndetection under adverse scene conditions. The Code is released at\nhttps://github.com/ChenHuyoo .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7edf\u8ba1\u53cd\u9988\u9a71\u52a8\u9608\u503c\u548c\u63a9\u7801\u8c03\u6574\u5e08\u751f\u6846\u67b6\uff08SF-TMAT\uff09\u6765\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u4e0d\u826f\u73af\u5883\u4e0b\u7684\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u63a9\u7801\u6bd4\u4f8b\u548c\u9608\u503c\u9009\u62e9\u7b56\u7565\uff0c\u63d0\u9ad8\u4f2a\u6807\u7b7e\u7684\u8d28\u91cf\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86SF-TMAT\u5728\u4e0d\u826f\u73af\u5883\u4e0b\u7684\u4f18\u8d8a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u57df\u9002\u5e94\u7814\u7a76\u4e3b\u8981\u57fa\u4e8e\u81ea\u7136\u56fe\u50cf\u6216\u6e05\u6670\u7684\u65e0\u4eba\u673a\u56fe\u50cf\uff0c\u800c\u5bf9\u4e0d\u826f\u6761\u4ef6\u4e0b\u65e0\u4eba\u673a\u56fe\u50cf\u7684\u7814\u7a76\u8fd8\u5904\u4e8e\u521d\u7ea7\u9636\u6bb5\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5728\u7279\u5f81\u5bf9\u9f50\u4e0a\u51c6\u786e\u6027\u4e0d\u9ad8\uff0c\u4e14\u5bb9\u6613\u53d7\u5230\u6709\u9650\u6216\u566a\u58f0\u4f2a\u6807\u7b7e\u7684\u5f71\u54cd\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u4e0d\u5229\u73af\u5883\u4e0b\u7684\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u7edf\u8ba1\u53cd\u9988\u9a71\u52a8\u9608\u503c\u548c\u63a9\u7801\u8c03\u6574\u5e08\u751f\u6846\u67b6\uff08SF-TMAT\uff09\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u4e0d\u5229\u73af\u5883\u4e0b\u7684\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0cSF-TMAT\u5f15\u5165\u4e86\u52a8\u6001\u6b65\u9aa4\u53cd\u9988\u63a9\u7801\u8c03\u6574\u81ea\u7f16\u7801\u5668\uff08DSFMA\uff09\u7684\u8bbe\u8ba1\uff0c\u8be5\u8bbe\u8ba1\u52a8\u6001\u8c03\u6574\u63a9\u7801\u6bd4\u4f8b\uff0c\u5e76\u901a\u8fc7\u96c6\u6210\u8bad\u7ec3\u8fdb\u5ea6\u548c\u635f\u5931\u53cd\u9988\u6765\u91cd\u6784\u7279\u5f81\u56fe\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u65b9\u5dee\u53cd\u9988\u5e73\u6ed1\u9608\u503c\uff08VFST\uff09\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u4ece\u7edf\u8ba1\u4e0a\u8ba1\u7b97\u5404\u7c7b\u522b\u7684\u5e73\u5747\u7f6e\u4fe1\u5ea6\u5e76\u901a\u8fc7\u7eb3\u5165\u65b9\u5dee\u60e9\u7f5a\u9879\u6765\u52a8\u6001\u8c03\u6574\u9009\u62e9\u9608\u503c\uff0c\u4ece\u800c\u63d0\u9ad8\u4f2a\u6807\u7b7e\u7684\u8d28\u91cf\u5e76\u51cf\u8f7b\u9886\u57df\u504f\u5dee\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSF-TMAT\u6846\u67b6\u5728\u4e0d\u5229\u73af\u5883\u4e0b\u7684\u65e0\u4eba\u673a\u76ee\u6807\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u4e86\u4f18\u8d8a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5b9e\u9a8c\u7814\u7a76\u8868\u660e\uff0c\u63d0\u51fa\u7684SF-TMAT\u5728\u65e0\u4eba\u673a\u4e0d\u5229\u73af\u5883\u4e0b\u7684\u76ee\u6807\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u4e86\u4f18\u8d8a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.11083", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.11083", "abs": "https://arxiv.org/abs/2506.11083", "authors": ["Ali Asad", "Stephen Obadinma", "Radin Shayanfar", "Xiaodan Zhu"], "title": "RedDebate: Safer Responses through Multi-Agent Red Teaming Debates", "comment": null, "summary": "We propose RedDebate, a novel multi-agent debate framework that leverages\nadversarial argumentation among Large Language Models (LLMs) to proactively\nidentify and mitigate their own unsafe behaviours. Existing AI safety methods\noften depend heavily on costly human evaluations or isolated single-model\nassessment, both subject to scalability constraints and oversight risks.\nRedDebate instead embraces collaborative disagreement, enabling multiple LLMs\nto critically examine one another's reasoning, and systematically uncovering\nunsafe blind spots through automated red-teaming, and iteratively improve their\nresponses. We further integrate distinct types of long-term memory that retain\nlearned safety insights from debate interactions. Evaluating on established\nsafety benchmarks such as HarmBench, we demonstrate the proposed method's\neffectiveness. Debate alone can reduce unsafe behaviours by 17.7%, and when\ncombined with long-term memory modules, achieves reductions exceeding 23.5%. To\nour knowledge, RedDebate constitutes the first fully automated framework that\ncombines multi-agent debates with red-teaming to progressively enhance AI\nsafety without direct human intervention.(Github Repository:\nhttps://github.com/aliasad059/RedDebate)", "AI": {"tldr": "RedDebate\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\u548c\u4e0d\u540c\u7c7b\u578b\u7684\u957f\u671f\u8bb0\u5fc6\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u9ad8AI\u5b89\u5168\u6027\u7684\u81ea\u52a8\u5316\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u5de5\u667a\u80fd\u5b89\u5168\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u6210\u672c\u9ad8\u6602\u7684\u4eba\u7c7b\u8bc4\u4f30\u6216\u5b64\u7acb\u7684\u5355\u6a21\u578b\u8bc4\u4f30\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u90fd\u5b58\u5728\u53ef\u6269\u5c55\u6027\u7ea6\u675f\u548c\u76d1\u7763\u98ce\u9669\u3002", "method": "RedDebate\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e4b\u95f4\u7684\u5bf9\u6297\u6027\u8bba\u8fa9\u6765\u4e3b\u52a8\u8bc6\u522b\u548c\u7f13\u89e3\u5176\u81ea\u8eab\u7684\u4e0d\u5b89\u5168\u884c\u4e3a\u3002\u8fd9\u79cd\u65b9\u6cd5\u96c6\u6210\u4e86\u4e0d\u540c\u7c7b\u578b\u7684\u957f\u671f\u8bb0\u5fc6\u6a21\u5757\uff0c\u4ee5\u4fdd\u7559\u4ece\u8fa9\u8bba\u4ea4\u4e92\u4e2d\u83b7\u5f97\u7684\u5b89\u5168\u89c1\u89e3\u3002", "result": "\u5728HarmBench\u7b49\u65e2\u5b9a\u7684\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ec5\u4ec5\u8fa9\u8bba\u5c31\u80fd\u4f7f\u4e0d\u5b89\u5168\u884c\u4e3a\u51cf\u5c1117.7%\uff0c\u800c\u7ed3\u5408\u957f\u671f\u8bb0\u5fc6\u6a21\u5757\uff0c\u53ef\u4ee5\u5b9e\u73b0\u8d85\u8fc723.5%\u7684\u51cf\u5c11\u3002", "conclusion": "\u636e\u6211\u4eec\u6240\u77e5\uff0cRedDebate\u662f\u7b2c\u4e00\u4e2a\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u548c\u7ea2\u961f\u6d4b\u8bd5\uff0c\u4ee5\u6e10\u8fdb\u7684\u65b9\u5f0f\u63d0\u9ad8\u4eba\u5de5\u667a\u80fd\u7684\u5b89\u5168\u6027\uff0c\u800c\u65e0\u9700\u76f4\u63a5\u7684\u4eba\u7c7b\u5e72\u9884\u3002"}}
{"id": "2506.11178", "categories": ["cs.CV", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2506.11178", "abs": "https://arxiv.org/abs/2506.11178", "authors": ["Nguyen Linh Dan Le", "Jing Ren", "Ciyuan Peng", "Chengyao Xie", "Bowen Li", "Feng Xia"], "title": "BrainMAP: Multimodal Graph Learning For Efficient Brain Disease Localization", "comment": "6 pages, 5 figures", "summary": "Recent years have seen a surge in research focused on leveraging graph\nlearning techniques to detect neurodegenerative diseases. However, existing\ngraph-based approaches typically lack the ability to localize and extract the\nspecific brain regions driving neurodegenerative pathology within the full\nconnectome. Additionally, recent works on multimodal brain graph models often\nsuffer from high computational complexity, limiting their practical use in\nresource-constrained devices. In this study, we present BrainMAP, a novel\nmultimodal graph learning framework designed for precise and computationally\nefficient identification of brain regions affected by neurodegenerative\ndiseases. First, BrainMAP utilizes an atlas-driven filtering approach guided by\nthe AAL atlas to pinpoint and extract critical brain subgraphs. Unlike recent\nstate-of-the-art methods, which model the entire brain network, BrainMAP\nachieves more than 50% reduction in computational overhead by concentrating on\ndisease-relevant subgraphs. Second, we employ an advanced multimodal fusion\nprocess comprising cross-node attention to align functional magnetic resonance\nimaging (fMRI) and diffusion tensor imaging (DTI) data, coupled with an\nadaptive gating mechanism to blend and integrate these modalities dynamically.\nExperimental results demonstrate that BrainMAP outperforms state-of-the-art\nmethods in computational efficiency, without compromising predictive accuracy.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.11088", "categories": ["cs.CL", "cs.AI", "68T50"], "pdf": "https://arxiv.org/pdf/2506.11088", "abs": "https://arxiv.org/abs/2506.11088", "authors": ["Pengbo Wang", "Chaozhuo Li", "Chenxu Wang", "Liwen Zheng", "Litian Zhang", "Xi Zhang"], "title": "Two Birds with One Stone: Improving Factuality and Faithfulness of LLMs via Dynamic Interactive Subspace Editing", "comment": null, "summary": "LLMs have demonstrated unprecedented capabilities in natural language\nprocessing, yet their practical deployment remains hindered by persistent\nfactuality and faithfulness hallucinations. While existing methods address\nthese hallucination types independently, they inadvertently induce performance\ntrade-offs, as interventions targeting one type often exacerbate the other.\nThrough empirical and theoretical analysis of activation space dynamics in\nLLMs, we reveal that these hallucination categories share overlapping subspaces\nwithin neural representations, presenting an opportunity for concurrent\nmitigation. To harness this insight, we propose SPACE, a unified framework that\njointly enhances factuality and faithfulness by editing shared activation\nsubspaces. SPACE establishes a geometric foundation for shared subspace\nexistence through dual-task feature modeling, then identifies and edits these\nsubspaces via a hybrid probe strategy combining spectral clustering and\nattention head saliency scoring. Experimental results across multiple benchmark\ndatasets demonstrate the superiority of our approach.", "AI": {"tldr": "\u6587\u7ae0\u901a\u8fc7\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6fc0\u6d3b\u7a7a\u95f4\uff0c\u53d1\u73b0\u53ef\u4fe1\u5ea6\u548c\u5fe0\u5b9e\u5ea6\u5e7b\u89c9\u5171\u4eab\u795e\u7ecf\u8868\u5f81\u5b50\u7a7a\u95f4\uff0c\u63d0\u51faSPACE\u6846\u67b6\u6765\u540c\u65f6\u7f13\u89e3\u8fd9\u4e24\u79cd\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u867d\u7136\u73b0\u6709\u7684\u65b9\u6cd5\u5206\u522b\u89e3\u51b3\u4e86\u8fd9\u4e24\u79cd\u5e7b\u89c9\u7c7b\u578b\uff0c\u4f46\u5b83\u4eec\u53ef\u80fd\u4f1a\u65e0\u610f\u4e2d\u5f15\u8d77\u6027\u80fd\u6743\u8861\u3002\u5f53\u524d\u65b9\u6cd5\u5bf9\u4e00\u79cd\u7c7b\u578b\u7684\u5e72\u9884\u5f80\u5f80\u4f1a\u52a0\u5267\u53e6\u4e00\u79cd\u7c7b\u578b\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6fc0\u6d3b\u7a7a\u95f4\u52a8\u529b\u5b66\u7684\u5b9e\u8bc1\u548c\u7406\u8bba\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u8fd9\u4e9b\u5e7b\u89c9\u7c7b\u522b\u5728\u795e\u7ecf\u8868\u5f81\u4e2d\u5171\u4eab\u91cd\u53e0\u5b50\u7a7a\u95f4\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u540c\u65f6\u51cf\u8f7b\u4e24\u79cd\u5e7b\u89c9\u7684\u673a\u4f1a\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6SPACE\uff0c\u901a\u8fc7\u7f16\u8f91\u5171\u4eab\u6fc0\u6d3b\u5b50\u7a7a\u95f4\u6765\u540c\u65f6\u63d0\u5347\u53ef\u4fe1\u5ea6\u548c\u5fe0\u5b9e\u5ea6\u3002SPACE\u901a\u8fc7\u53cc\u4efb\u52a1\u7279\u5f81\u5efa\u6a21\u5efa\u7acb\u5171\u4eab\u5b50\u7a7a\u95f4\u5b58\u5728\u7684\u51e0\u4f55\u57fa\u7840\uff0c\u7136\u540e\u901a\u8fc7\u7ed3\u5408\u8c31\u805a\u7c7b\u548c\u6ce8\u610f\u529b\u5f97\u5206\u8bc6\u522b\u5e76\u7f16\u8f91\u8fd9\u4e9b\u5b50\u7a7a\u95f4\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86SPACE\u6846\u67b6\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u548c\u5fe0\u5b9e\u5ea6\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.11239", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11239", "abs": "https://arxiv.org/abs/2506.11239", "authors": ["Amirali Ataee Naeini", "Ashkan Teymouri", "Ghazaleh Jafarsalehi", "Michael Zhang"], "title": "Enhanced Vehicle Speed Detection Considering Lane Recognition Using Drone Videos in California", "comment": "7 pages", "summary": "The increase in vehicle numbers in California, driven by inadequate\ntransportation systems and sparse speed cameras, necessitates effective vehicle\nspeed detection. Detecting vehicle speeds per lane is critical for monitoring\nHigh-Occupancy Vehicle (HOV) lane speeds, distinguishing between cars and heavy\nvehicles with differing speed limits, and enforcing lane restrictions for heavy\nvehicles. While prior works utilized YOLO (You Only Look Once) for vehicle\nspeed detection, they often lacked accuracy, failed to identify vehicle lanes,\nand offered limited or less practical classification categories. This study\nintroduces a fine-tuned YOLOv11 model, trained on almost 800 bird's-eye view\nimages, to enhance vehicle speed detection accuracy which is much higher\ncompare to the previous works. The proposed system identifies the lane for each\nvehicle and classifies vehicles into two categories: cars and heavy vehicles.\nDesigned to meet the specific requirements of traffic monitoring and\nregulation, the model also evaluates the effects of factors such as drone\nheight, distance of Region of Interest (ROI), and vehicle speed on detection\naccuracy and speed measurement. Drone footage collected from Northern\nCalifornia was used to assess the proposed system. The fine-tuned YOLOv11\nachieved its best performance with a mean absolute error (MAE) of 0.97 mph and\nmean squared error (MSE) of 0.94 $\\text{mph}^2$, demonstrating its efficacy in\naddressing challenges in vehicle speed detection and classification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ec6\u8c03YOLOv11\u6a21\u578b\u7684\u8f66\u8f86\u901f\u5ea6\u68c0\u6d4b\u65b9\u6cd5\uff0c\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u8f66\u9053\u5e76\u533a\u5206\u8f7b\u578b\u8f66\u8f86\u548c\u91cd\u578b\u8f66\u8f86\uff0c\u89e3\u51b3\u4e86\u52a0\u5dde\u8f66\u8f86\u6570\u91cf\u589e\u52a0\u6240\u5e26\u6765\u7684\u4ea4\u901a\u7ba1\u7406\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u52a0\u5dde\u8f66\u8f86\u6570\u91cf\u589e\u52a0\uff0c\u4ea4\u901a\u7cfb\u7edf\u4e0d\u5b8c\u5584\u4e14\u6d4b\u901f\u6444\u50cf\u5934\u7a00\u5c11\uff0c\u5bfc\u81f4\u6709\u6548\u8f66\u8f86\u901f\u5ea6\u68c0\u6d4b\u7684\u9700\u6c42\u589e\u52a0\u3002\u73b0\u6709\u5de5\u4f5c\u5728\u8f66\u8f86\u901f\u5ea6\u68c0\u6d4b\u65b9\u9762\u5b58\u5728\u51c6\u786e\u5ea6\u4f4e\u3001\u65e0\u6cd5\u8bc6\u522b\u8f66\u9053\u7b49\u95ee\u9898\u3002", "method": "\u672c\u6587\u91c7\u7528\u7ec6\u8c03\u8fc7\u7684YOLOv11\u6a21\u578b\uff0c\u4f7f\u7528\u8fd1800\u5f20\u9e1f\u77b0\u56fe\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u63d0\u9ad8\u8f66\u8f86\u901f\u5ea6\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002\u6a21\u578b\u80fd\u8bc6\u522b\u6bcf\u8f86\u8f66\u6240\u5728\u7684\u8f66\u9053\u5e76\u5c06\u8f66\u8f86\u5206\u4e3a\u8f7b\u578b\u548c\u91cd\u578b\u4e24\u7c7b\u3002", "result": "\u4f7f\u7528\u8bfa\u65af\u52a0\u5dde\u65e0\u4eba\u673a\u62cd\u6444\u7684\u89c6\u9891\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ec6\u8c03\u540e\u7684YOLOv11\u6a21\u578b\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u4e3a0.97 mph\uff0c\u5e73\u5747\u5e73\u65b9\u8bef\u5dee\uff08MSE\uff09\u4e3a0.94 mph\u00b2\u3002", "conclusion": "\u6b64\u7cfb\u7edf\u5c55\u793a\u4e86\u5728\u8f66\u8f86\u901f\u5ea6\u68c0\u6d4b\u548c\u5206\u7c7b\u6311\u6218\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5bf9\u4e8e\u4ea4\u901a\u76d1\u63a7\u548c\u7ba1\u7406\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.11091", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.11091", "abs": "https://arxiv.org/abs/2506.11091", "authors": ["Shaoshi Ling", "Guoli Ye"], "title": "Customizing Speech Recognition Model with Large Language Model Feedback", "comment": null, "summary": "Automatic speech recognition (ASR) systems have achieved strong performance\non general transcription tasks. However, they continue to struggle with\nrecognizing rare named entities and adapting to domain mismatches. In contrast,\nlarge language models (LLMs), trained on massive internet-scale datasets, are\noften more effective across a wide range of domains. In this work, we propose a\nreinforcement learning based approach for unsupervised domain adaptation,\nleveraging unlabeled data to enhance transcription quality, particularly the\nnamed entities affected by domain mismatch, through feedback from a LLM. Given\ncontextual information, our framework employs a LLM as the reward model to\nscore the hypotheses from the ASR model. These scores serve as reward signals\nto fine-tune the ASR model via reinforcement learning. Our method achieves a\n21\\% improvement on entity word error rate over conventional self-training\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5f15\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u5347\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u5728\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u9886\u57df\u81ea\u9002\u5e94\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u5728\u4e00\u822c\u8f6c\u5f55\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8bc6\u522b\u7f55\u89c1\u547d\u540d\u5b9e\u4f53\u548c\u5e94\u5bf9\u9886\u57df\u4e0d\u5339\u914d\u4e0a\u4ecd\u6709\u56f0\u96be\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u4e92\u8054\u7f51\u6570\u636e\u8bad\u7ec3\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5e7f\u6cdb\u9886\u57df\u7684\u8868\u73b0\u66f4\u4f73\u3002", "method": "\u91c7\u7528\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65e0\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u5229\u7528\u672a\u6807\u8bb0\u7684\u6570\u636e\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u53cd\u9988\u6765\u63d0\u5347\u8f6c\u5f55\u8d28\u91cf\uff0c\u7279\u522b\u662f\u53d7\u9886\u57df\u4e0d\u5339\u914d\u5f71\u54cd\u7684\u547d\u540d\u5b9e\u4f53\u7684\u8bc6\u522b\u3002\u8be5\u6846\u67b6\u4f7f\u7528LLM\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\uff0c\u5bf9ASR\u6a21\u578b\u7684\u5047\u8bbe\u8fdb\u884c\u8bc4\u5206\uff0c\u8fd9\u4e9b\u8bc4\u5206\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u7528\u4e8e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03ASR\u6a21\u578b\u3002", "result": "\u76f8\u6bd4\u4e8e\u4f20\u7edf\u7684\u81ea\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9e\u4f53\u5355\u8bcd\u9519\u8bef\u7387\u4e0a\u63d0\u9ad8\u4e8621%\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u8bc1\u660e\u4e86\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u5728\u7279\u5b9a\u9886\u57df\u7684\u8f6c\u5f55\u51c6\u786e\u6027\u3002"}}
{"id": "2506.11253", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11253", "abs": "https://arxiv.org/abs/2506.11253", "authors": ["Yuwen Tan", "Boqing Gong"], "title": "Lifting Data-Tracing Machine Unlearning to Knowledge-Tracing for Foundation Models", "comment": "21 pages, 3 figures", "summary": "Machine unlearning removes certain training data points and their influence\non AI models (e.g., when a data owner revokes their decision to allow models to\nlearn from the data). In this position paper, we propose to lift data-tracing\nmachine unlearning to knowledge-tracing for foundation models (FMs). We support\nthis position based on practical needs and insights from cognitive studies.\nPractically, tracing data cannot meet the diverse unlearning requests for FMs,\nwhich may be from regulators, enterprise users, product teams, etc., having no\naccess to FMs' massive training data. Instead, it is convenient for these\nparties to issue an unlearning request about the knowledge or capability FMs\n(should not) possess. Cognitively, knowledge-tracing unlearning aligns with how\nthe human brain forgets more closely than tracing individual training data\npoints. Finally, we provide a concrete case study about a vision-language FM to\nillustrate how an unlearner might instantiate the knowledge-tracing machine\nunlearning paradigm.", "AI": {"tldr": "\u672c\u6587\u63d0\u8bae\u5c06\u673a\u5668\u5b66\u4e60\u64a4\u9500\u65b9\u6cd5\u4ece\u57fa\u4e8e\u6570\u636e\u8ffd\u8e2a\u7684\u65b9\u6cd5\u63d0\u5347\u4e3a\u57fa\u4e8e\u77e5\u8bc6\u8ffd\u8e2a\u7684\u65b9\u6cd5\uff0c\u4e3b\u8981\u5e94\u7528\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u4ee5\u6ee1\u8db3\u591a\u6837\u5316\u7684\u64a4\u9500\u9700\u6c42\uff0c\u5e76\u63a5\u8fd1\u4eba\u7c7b\u8bb0\u5fc6\u9057\u5fd8\u7684\u7279\u70b9\u3002", "motivation": "\u672c\u6587\u7684\u52a8\u673a\u6e90\u4e8e\u73b0\u5b9e\u4e2d\u5bf9\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u591a\u6837\u5316\u64a4\u9500\u8bf7\u6c42\uff0c\u8fd9\u4e9b\u8bf7\u6c42\u6765\u81ea\u65e0\u6cd5\u8bbf\u95ee\u6a21\u578b\u5e9e\u5927\u8bad\u7ec3\u6570\u636e\u7684\u4e0d\u540c\u7fa4\u4f53\uff0c\u5982\u76d1\u7ba1\u673a\u6784\u3001\u4f01\u4e1a\u7528\u6237\u548c\u4ea7\u54c1\u56e2\u961f\u7b49\u3002\u540c\u65f6\uff0c\u8ba4\u77e5\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u77e5\u8bc6\u8ffd\u8e2a\u7684\u65b9\u6cd5\u66f4\u597d\u5730\u6a21\u62df\u4e86\u4eba\u7c7b\u9057\u5fd8\u8fc7\u7a0b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u5c06\u4f20\u7edf\u7684\u57fa\u4e8e\u6570\u636e\u8ffd\u8e2a\u7684\u673a\u5668\u5b66\u4e60\u64a4\u9500\u65b9\u6cd5\u63d0\u5347\u4e3a\u57fa\u4e8e\u77e5\u8bc6\u8ffd\u8e2a\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u9488\u5bf9\u57fa\u7840\u6a21\u578b(FMs)\u3002\u8fd9\u79cd\u65b9\u6cd5\u66f4\u7b26\u5408\u591a\u6837\u5316\u64a4\u9500\u8bf7\u6c42\u7684\u9700\u6c42\uff0c\u5e76\u66f4\u8d34\u8fd1\u4eba\u7c7b\u5927\u8111\u9057\u5fd8\u673a\u5236\u7684\u5de5\u4f5c\u65b9\u5f0f\u3002", "result": "\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u5173\u4e8e\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u7684\u5177\u4f53\u6848\u4f8b\u7814\u7a76\uff0c\u672c\u6587\u5c55\u793a\u4e86\u57fa\u4e8e\u77e5\u8bc6\u8ffd\u8e2a\u7684\u673a\u5668\u5b66\u4e60\u64a4\u9500\u8303\u5f0f\u7684\u5b9e\u9645\u5e94\u7528\u53ef\u80fd\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u5b66\u4e60\u64a4\u9500\u6982\u5ff5\uff0c\u5373\u77e5\u8bc6\u8ffd\u8e2a\u64a4\u9500\uff0c\u8fd9\u79cd\u65b9\u6cd5\u66f4\u9002\u5408\u5904\u7406\u57fa\u7840\u6a21\u578b\u7684\u64a4\u9500\u8bf7\u6c42\uff0c\u66f4\u52a0\u8d34\u8fd1\u4eba\u7c7b\u9057\u5fd8\u7684\u5fc3\u7406\u673a\u5236\u3002"}}
{"id": "2506.11092", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.11092", "abs": "https://arxiv.org/abs/2506.11092", "authors": ["Jubin Abhishek Soni", "Amit Anand", "Rajesh Kumar Pandey", "Aniket Abhishek Soni"], "title": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation", "comment": "6 pages, 5 figures, 3 tables. This manuscript has been submitted to\n  IEEE conference. Researchers are welcome to read and build upon this work;\n  please cite it appropriately. For questions or clarifications, feel free to\n  contact me", "summary": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u52a8\u6001\u4e0a\u4e0b\u6587\u8c03\u4f18\uff08DCT\uff09\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u591a\u8f6e\u6b21\u5bf9\u8bdd\u548c\u4e0d\u65ad\u53d8\u5316\u7684\u5de5\u5177\u73af\u5883\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aDCT\u5728\u63d0\u9ad8\u8ba1\u5212\u51c6\u786e\u6027\u3001\u51cf\u5c11\u5e7b\u89c9\u4ee5\u53ca\u6210\u672c\u6548\u76ca\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u901a\u5e38\u53d7\u5230\u9759\u6001\u3001\u5355\u8f6e\u6b21\u4ea4\u4e92\u548c\u56fa\u5b9a\u5de5\u5177\u96c6\u7684\u9650\u5236\uff0c\u4e0d\u9002\u7528\u4e8e\u610f\u56fe\u3001\u53ef\u7528\u5de5\u5177\u548c\u73af\u5883\u56e0\u7d20\u968f\u7740\u65f6\u95f4\u6f14\u53d8\u7684\u52a8\u6001\u9886\u57df\uff0c\u6bd4\u5982\u533b\u7597\u4fdd\u5065\u548c\u667a\u80fd\u5bb6\u5c45\u3002", "method": "\u63d0\u51fa\u4e86\u52a8\u6001\u4e0a\u4e0b\u6587\u8c03\u4f18\uff08DCT\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u4e0a\u4e0b\u6587\u7f13\u5b58\u6765\u8ffd\u8e2a\u76f8\u5173\u7684\u5386\u53f2\u4fe1\u606f\uff0c\u57fa\u4e8eLoRA\u7684\u68c0\u7d22\u6765\u52a8\u6001\u9009\u62e9\u9886\u57df\u7279\u5b9a\u5de5\u5177\uff0c\u4ee5\u53ca\u9ad8\u6548\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\u6765\u7ef4\u6301\u8f93\u5165\u5728\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u9650\u5236\u5185\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aDCT\u63d0\u5347\u4e86\u8ba1\u5212\u7684\u51c6\u786e\u602714%\uff0c\u964d\u4f4e\u4e86\u5e7b\u89c937%\uff0c\u5e76\u80fd\u5728\u4e0d\u4f7f\u7528\u9ad8\u6602\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u5339\u654cGPT-4\u7684\u8868\u73b0\u3002", "conclusion": "DCT\u80fd\u591f\u9002\u5e94\u4e4b\u524d\u672a\u89c1\u8fc7\u7684\u5de5\u5177\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u548c\u9002\u5e94\u6027\u5f3a\u7684AI\u52a9\u624b\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u52a8\u6001\u73af\u5883\u3002"}}
{"id": "2506.11302", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11302", "abs": "https://arxiv.org/abs/2506.11302", "authors": ["H\u00e9ctor Carri\u00f3n", "Yutong Bai", "V\u00edctor A. Hern\u00e1ndez Castro", "Kishan Panaganti", "Ayush Zenith", "Matthew Trang", "Tony Zhang", "Pietro Perona", "Jitendra Malik"], "title": "TARDIS STRIDE: A Spatio-Temporal Road Image Dataset for Exploration and Autonomy", "comment": "Computer Vision, Pattern Recognition, LLMs, Dataset, Data\n  Augmentation", "summary": "World models aim to simulate environments and enable effective agent\nbehavior. However, modeling real-world environments presents unique challenges\nas they dynamically change across both space and, crucially, time. To capture\nthese composed dynamics, we introduce a Spatio-Temporal Road Image Dataset for\nExploration (STRIDE) permuting 360-degree panoramic imagery into rich\ninterconnected observation, state and action nodes. Leveraging this structure,\nwe can simultaneously model the relationship between egocentric views,\npositional coordinates, and movement commands across both space and time. We\nbenchmark this dataset via TARDIS, a transformer-based generative world model\nthat integrates spatial and temporal dynamics through a unified autoregressive\nframework trained on STRIDE. We demonstrate robust performance across a range\nof agentic tasks such as controllable photorealistic image synthesis,\ninstruction following, autonomous self-control, and state-of-the-art\ngeoreferencing. These results suggest a promising direction towards\nsophisticated generalist agents--capable of understanding and manipulating the\nspatial and temporal aspects of their material environments--with enhanced\nembodied reasoning capabilities. Training code, datasets, and model checkpoints\nare made available at https://huggingface.co/datasets/Tera-AI/STRIDE.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86STRIDE\u6570\u636e\u96c6\u548cTARDIS\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u5728\u4e16\u754c\u6a21\u578b\u4e2d\u6a21\u62df\u7a7a\u95f4\u548c\u65f6\u95f4\u52a8\u6001\u53d8\u5316\u7684\u6311\u6218\uff0c\u5e76\u5728\u591a\u4e2a\u4ee3\u7406\u4efb\u52a1\u4e0a\u5c55\u73b0\u4e86\u51fa\u8272\u7684\u6027\u80fd\u3002", "motivation": "\u4e16\u754c\u6a21\u578b\u7684\u76ee\u6807\u662f\u6a21\u62df\u73b0\u5b9e\u73af\u5883\u5e76\u4fc3\u8fdb\u6709\u6548\u7684\u4ee3\u7406\u884c\u4e3a\uff0c\u800c\u73b0\u5b9e\u73af\u5883\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u90fd\u662f\u52a8\u6001\u53d8\u5316\u7684\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u7efc\u5408\u8fd9\u4e9b\u52a8\u6001\u53d8\u5316\u3002", "method": "\u5f15\u5165\u4e86STRIDE\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u91cd\u7ec4360\u5ea6\u5168\u666f\u56fe\u50cf\u4e3a\u4e30\u5bcc\u7684\u4e92\u8fde\u89c2\u5bdf\u3001\u72b6\u6001\u548c\u52a8\u4f5c\u8282\u70b9\u4ee5\u540c\u65f6\u5efa\u6a21\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u3001\u4f4d\u7f6e\u5750\u6807\u548c\u8fd0\u52a8\u547d\u4ee4\u4e4b\u95f4\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u7684\u5173\u7cfb\u3002\u4f7f\u7528TARDIS\uff0c\u4e00\u4e2a\u57fa\u4e8etransformer\u7684\u751f\u6210\u5f0f\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u81ea\u56de\u5f52\u6846\u67b6\u5728STRIDE\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6765\u6574\u5408\u7a7a\u95f4\u548c\u65f6\u95f4\u52a8\u6001\u3002", "result": "\u5728\u53ef\u63a7\u7684\u903c\u771f\u56fe\u50cf\u5408\u6210\u3001\u6307\u4ee4\u8ddf\u968f\u3001\u81ea\u4e3b\u81ea\u6211\u63a7\u5236\u548c\u6700\u5148\u8fdb\u7684\u5730\u7406\u5b9a\u4f4d\u7b49\u4ee3\u7406\u4efb\u52a1\u65b9\u9762\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u671d\u7740\u80fd\u591f\u7406\u89e3\u548c\u64cd\u7eb5\u5176\u7269\u8d28\u73af\u5883\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u65b9\u9762\u4ee5\u53ca\u5177\u5907\u589e\u5f3a\u7684\u8eab\u4f53\u63a8\u7406\u80fd\u529b\u7684\u590d\u6742\u901a\u7528\u4ee3\u7406\u4f53\u53d1\u5c55\u662f\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002"}}
