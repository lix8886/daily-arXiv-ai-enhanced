{"id": "2509.14238", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14238", "abs": "https://arxiv.org/abs/2509.14238", "authors": ["Jinfan Frank Hu"], "title": "Tokenization Strategies for Low-Resource Agglutinative Languages in Word2Vec: Case Study on Turkish and Finnish", "comment": "6 pages, 9 figures, accepted to ACDSA 2025, to be indexed in IEEE\n  Xplore", "summary": "Tokenization plays a critical role in processing agglutinative languages,\nwhere a single word can encode multiple morphemes carrying syntactic and\nsemantic information. This study evaluates the impact of various tokenization\nstrategies - word-level, character-level, n-gram, and Byte Pair Encoding (BPE)\n- on the quality of static word embeddings generated by Word2Vec for Turkish\nand Finnish. Using a 10,000-article Wikipedia corpus, we trained models under\nlow-resource conditions and evaluated them on a Named Entity Recognition (NER)\ntask. Despite the theoretical appeal of subword segmentation, word-level\ntokenization consistently outperformed all alternatives across all tokenization\nstrategies tested. These findings suggest that in agglutinative, low-resource\ncontexts, preserving boundaries via word-level tokenization may yield better\nembedding performance than complex statistical methods. This has practical\nimplications for developing NLP pipelines for under-resourced languages where\nannotated data and computing power are limited.", "AI": {"tldr": "对土耳其语和芬兰语的分词策略实验表明，在生成高质量词嵌入方面，词级分词优于子词分词等其他策略。", "motivation": "研究的动机在于探究不同的分词策略对生成高质量词嵌入的影响，特别是在处理含有大量形态信息的黏着语（如土耳其语和芬兰语）时，不同分词策略的效果。", "method": "此研究评估了不同分词策略（包括词级分词、字符级分词、n元语法分词和字节对编码BPE）对由Word2Vec生成的土耳其语和芬兰语静态词嵌入质量的影响。研究使用了包含10,000篇文章的维基百科语料库，并在低资源条件下训练模型，评估其在命名实体识别（NER）任务上的表现。", "result": "研究结果显示，尽管在理论上子词分段方法有其吸引力，但在所有测试的分词策略中，词级分词始终表现最好。", "conclusion": "研究结论表明，在低资源语境及黏着语中，通过词级分词保持边界可能会比复杂统计方法产生更好的嵌入效果。"}}
{"id": "2509.14249", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14249", "abs": "https://arxiv.org/abs/2509.14249", "authors": ["Happymore Masoka"], "title": "Advancing Conversational AI with Shona Slang: A Dataset and Hybrid Model for Digital Inclusion", "comment": null, "summary": "African languages remain underrepresented in natural language processing\n(NLP), with most corpora limited to formal registers that fail to capture the\nvibrancy of everyday communication. This work addresses this gap for Shona, a\nBantu language spoken in Zimbabwe and Zambia, by introducing a novel\nShona--English slang dataset curated from anonymized social media\nconversations. The dataset is annotated for intent, sentiment, dialogue acts,\ncode-mixing, and tone, and is publicly available at\nhttps://github.com/HappymoreMasoka/Working_with_shona-slang. We fine-tuned a\nmultilingual DistilBERT classifier for intent recognition, achieving 96.4\\%\naccuracy and 96.3\\% F1-score, hosted at https://huggingface.co/HappymoreMasoka.\nThis classifier is integrated into a hybrid chatbot that combines rule-based\nresponses with retrieval-augmented generation (RAG) to handle domain-specific\nqueries, demonstrated through a use case assisting prospective students with\ngraduate program information at Pace University. Qualitative evaluation shows\nthe hybrid system outperforms a RAG-only baseline in cultural relevance and\nuser engagement. By releasing the dataset, model, and methodology, this work\nadvances NLP resources for African languages, promoting inclusive and\nculturally resonant conversational AI.", "AI": {"tldr": "该研究引入了一个新型的绍纳语-英语俚语数据集，并使用此数据集调整了一个多语言DistilBERT分类器，用于意图识别，准确率达到96.4%，F1评分为96.3%，并将其整合进了一个混合聊天机器人中，辅助学生获取研究生项目信息，展示了与仅使用检索增强生成(RAG)模型相比，混合系统在文化相关性和用户参与方面的优势。", "motivation": "动机在于通过引入一个来自匿名社交媒体对话的绍纳语-英语俚语数据集，解决非洲语言在自然语言处理中代表性不足的问题，尤其是日常交流方面的不足。", "method": "通过整理一个带有意图、情感、对话行为、混合编码和音调注释的绍纳语-英语俚语数据集，并使用多语言DistilBERT分类器进行调整，实现意图识别，同时开发了一种结合基于规则的响应和检索增强生成（RAG）方法的混合聊天机器人。", "result": "调整后的多语言DistilBERT分类器在意图识别上达到了96.4%的准确率和96.3%的F1评分。此外，该研究展示了混合聊天机器人在协助学生获取研究生项目信息这一具体应用场景中，在文化相关性和用户参与度上优于仅使用RAG模型的系统。", "conclusion": "通过发布数据集、模型和方法，这项工作推进了非英语自然语言处理资源的发展，促进了包容性和文化共鸣的对话型人工智能系统的构建。"}}
{"id": "2509.14250", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14250", "abs": "https://arxiv.org/abs/2509.14250", "authors": ["Martin Thellefsen", "Amalia Nurma Dewi", "Bent Sorensen"], "title": "The meaning of prompts and the prompts of meaning: Semiotic reflections and modelling", "comment": "22 pages, 2 figures", "summary": "This paper explores prompts and prompting in large language models (LLMs) as\ndynamic semiotic phenomena, drawing on Peirce's triadic model of signs, his\nnine sign types, and the Dynacom model of communication. The aim is to\nreconceptualize prompting not as a technical input mechanism but as a\ncommunicative and epistemic act involving an iterative process of sign\nformation, interpretation, and refinement. The theoretical foundation rests on\nPeirce's semiotics, particularly the interplay between representamen, object,\nand interpretant, and the typological richness of signs: qualisign, sinsign,\nlegisign; icon, index, symbol; rheme, dicent, argument - alongside the\ninterpretant triad captured in the Dynacom model. Analytically, the paper\npositions the LLM as a semiotic resource that generates interpretants in\nresponse to user prompts, thereby participating in meaning-making within shared\nuniverses of discourse. The findings suggest that prompting is a semiotic and\ncommunicative process that redefines how knowledge is organized, searched,\ninterpreted, and co-constructed in digital environments. This perspective\ninvites a reimagining of the theoretical and methodological foundations of\nknowledge organization and information seeking in the age of computational\nsemiosis", "AI": {"tldr": "此论文重新思考提示方法，提出了提示不仅仅是技术输入，而是一种涉及符号形成、解释和改进的交流与知识性行为。", "motivation": "动机在于重新概念化提示的作用，将其视为一种更广泛的交流和知识性行为，而不仅仅是技术输入。", "method": "论文基于皮尔士的符号学理论，包括代表物、对象和解释项之间的相互作用，以及符号的多种类型，来分析大语言模型中的提示。", "result": "研究表明，提示是一种符号学和交流过程，这种过程重新定义了在数字环境中如何组织、搜索、解释和协同构建知识。", "conclusion": "论文提出将知识组织和信息检索的理论与方法重新构思，特别是在计算符号学的时代。"}}
{"id": "2509.14252", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14252", "abs": "https://arxiv.org/abs/2509.14252", "authors": ["Hai Huang", "Yann LeCun", "Randall Balestriero"], "title": "LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures", "comment": null, "summary": "Large Language Model (LLM) pretraining, finetuning, and evaluation rely on\ninput-space reconstruction and generative capabilities. Yet, it has been\nobserved in vision that embedding-space training objectives, e.g., with Joint\nEmbedding Predictive Architectures (JEPAs), are far superior to their\ninput-space counterpart. That mismatch in how training is achieved between\nlanguage and vision opens up a natural question: {\\em can language training\nmethods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is\na testimony of the challenge in designing such objectives for language. In this\nwork, we propose a first step in that direction where we develop LLM-JEPA, a\nJEPA based solution for LLMs applicable both to finetuning and pretraining.\nThus far, LLM-JEPA is able to outperform the standard LLM training objectives\nby a significant margin across models, all while being robust to overfiting.\nThose findings are observed across numerous datasets (NL-RX, GSM8K, Spider,\nRottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo\nfamilies. Code: https://github.com/rbalestr-lab/llm-jepa.", "AI": {"tldr": "本研究提出了LLM-JEPA，一种基于联合嵌入预测架构（JEPA）的解决方案，适用于语言模型的微调和预训练，并在多个数据集和模型上显示出优于标准训练目标的效果。", "motivation": "研究动机在于探索语言训练方法是否可以从视觉训练方法中吸取教训，尤其是嵌入空间训练目标是否比输入空间训练目标更有效。", "method": "提出了一种基于JEPA的解决方案，称为LLM-JEPA，应用于语言模型的微调和预训练。", "result": "实验结果显示，LLM-JEPA在多个数据集（NL-RX、GSM8K、Spider、RottenTomatoes）和不同模型（如Llama3、OpenELM、Gemma2、Olmo）上，均显著优于标准的训练目标，并且具有较强的过拟合鲁棒性。", "conclusion": "结论是LLM-JEPA为语言训练目标设计提供了一个成功的实例，并验证了JEPA类方法在语言模型上的潜力。"}}
{"id": "2509.14420", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14420", "abs": "https://arxiv.org/abs/2509.14420", "authors": ["Zhicheng Lin", "Xiaolin Wu", "Xi Zhang"], "title": "Class-invariant Test-Time Augmentation for Domain Generalization", "comment": null, "summary": "Deep models often suffer significant performance degradation under\ndistribution shifts. Domain generalization (DG) seeks to mitigate this\nchallenge by enabling models to generalize to unseen domains. Most prior\napproaches rely on multi-domain training or computationally intensive test-time\nadaptation. In contrast, we propose a complementary strategy: lightweight\ntest-time augmentation. Specifically, we develop a novel Class-Invariant\nTest-Time Augmentation (CI-TTA) technique. The idea is to generate multiple\nvariants of each input image through elastic and grid deformations that\nnevertheless belong to the same class as the original input. Their predictions\nare aggregated through a confidence-guided filtering scheme that remove\nunreliable outputs, ensuring the final decision relies on consistent and\ntrustworthy cues. Extensive Experiments on PACS and Office-Home datasets\ndemonstrate consistent gains across different DG algorithms and backbones,\nhighlighting the effectiveness and generality of our approach.", "AI": {"tldr": "本文提出了轻量级测试时间增强（CI-TTA）技术来提高深度模型在不同数据分布上的泛化能力。", "motivation": "深度模型在数据分布变化时性能显著下降，传统的域泛化方法依赖于多域训练或计算密集型的测试时间适应。", "method": "轻量级的测试时间增强（CI-TTA）技术，生成多样的输入图像变体，这些变体通过弹性变形和网格变形来保持与原始图像相同的类别。", "result": "在PACS和Office-Home数据集上的大量实验表明，该方法在不同的域泛化算法和骨干网络中都表现出一致的性能提升。", "conclusion": "CI-TTA技术展示了其有效性和普适性，可以有效地应用于提高深度模型的域泛化性能。"}}
{"id": "2509.14253", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14253", "abs": "https://arxiv.org/abs/2509.14253", "authors": ["Ahmad Pouramini", "Hesham Faili"], "title": "CrossPT: Exploring Cross-Task Transferability through Multi-Task Prompt Tuning", "comment": null, "summary": "Prompt tuning offers a parameter-efficient way to adapt large pre-trained\nlanguage models to new tasks, but most existing approaches are designed for\nsingle-task settings, failing to share knowledge across related tasks. We\npropose Cross-task Prompt Tuning (CrossPT), a modular framework for multi-task\nprompt tuning that enables controlled knowledge transfer while maintaining\ntask-specific specialization. CrossPT decomposes each target prompt into\nshared, pre-trained source prompts and task-specific private prompts, combined\nvia a learned attention mechanism. To support robust transfer, we\nsystematically investigate key design factors including prompt initialization,\nbalancing shared and private prompts, number of source prompts, learning rates,\ntask prefixes, and label semantics. Empirical results on GLUE and related\nbenchmarks show that CrossPT achieves higher accuracy and robustness compared\nto traditional prompt tuning and related methods, particularly in low-resource\nscenarios, while maintaining strong parameter efficiency.", "AI": {"tldr": "CrossPT 是一个多任务提示调优框架，通过学习注意力机制实现跨任务知识迁移和专业化，提高了多任务设置的准确性和鲁棒性。", "motivation": "现有的大多数提示调优方法是针对单一任务设置设计的，未能实现跨相关任务的知识共享。为此，研究人员提出了CrossPT。", "method": "Cross-task Prompt Tuning (CrossPT) 是一个多任务提示调优框架，该框架通过学习注意力机制将每个目标提示分解为共享的预训练源提示和特定任务的私有提示，从而实现可控的知识迁移，同时保持任务特定的专业化。", "result": "在GLUE和其他相关基准测试中，实证结果表明，与传统的提示调优方法相比，CrossPT在低资源场景中实现了更高的准确性和鲁棒性，同时保持了良好的参数效率。", "conclusion": "研究证明了CrossPT在多任务设置下的有效性和鲁棒性，尤其是在低资源场景中，同时保持了参数效率。"}}
