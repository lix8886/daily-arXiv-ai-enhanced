{"id": "2507.07108", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.07108", "abs": "https://arxiv.org/abs/2507.07108", "authors": ["Zhiwei Hu", "Víctor Gutiérrez-Basulto", "Zhiliang Xiang", "Ru Li", "Jeff Z. Pan"], "title": "Multi-level Mixture of Experts for Multimodal Entity Linking", "comment": "Accepted at KDD 2025", "summary": "Multimodal Entity Linking (MEL) aims to link ambiguous mentions within\nmultimodal contexts to associated entities in a multimodal knowledge base.\nExisting approaches to MEL introduce multimodal interaction and fusion\nmechanisms to bridge the modality gap and enable multi-grained semantic\nmatching. However, they do not address two important problems: (i) mention\nambiguity, i.e., the lack of semantic content caused by the brevity and\nomission of key information in the mention's textual context; (ii) dynamic\nselection of modal content, i.e., to dynamically distinguish the importance of\ndifferent parts of modal information. To mitigate these issues, we propose a\nMulti-level Mixture of Experts (MMoE) model for MEL. MMoE has four components:\n(i) the description-aware mention enhancement module leverages large language\nmodels to identify the WikiData descriptions that best match a mention,\nconsidering the mention's textual context; (ii) the multimodal feature\nextraction module adopts multimodal feature encoders to obtain textual and\nvisual embeddings for both mentions and entities; (iii)-(iv) the intra-level\nmixture of experts and inter-level mixture of experts modules apply a switch\nmixture of experts mechanism to dynamically and adaptively select features from\nrelevant regions of information. Extensive experiments demonstrate the\noutstanding performance of MMoE compared to the state-of-the-art. MMoE's code\nis available at: https://github.com/zhiweihu1103/MEL-MMoE.", "AI": {"tldr": "The paper proposes a Multi-level Mixture of Experts (MMoE) model for Multimodal Entity Linking (MEL) to address issues of mention ambiguity and dynamic selection of modal content.", "motivation": "Existing approaches to MEL do not address problems such as mention ambiguity caused by brief textual context and the lack of dynamic selection of important modal content.", "method": "MMoE includes a description-aware mention enhancement module, a multimodal feature extraction module, and intra-level and inter-level mixture of experts modules that use a switch mixture of experts mechanism to select features.", "result": "Experiments show that MMoE outperforms state-of-the-art approaches in MEL.", "conclusion": "The proposed MMoE model significantly improves the performance of multimodal entity linking by dynamically selecting and enhancing relevant features."}}
{"id": "2507.07125", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.07125", "abs": "https://arxiv.org/abs/2507.07125", "authors": ["Cristina Mata", "Kanchana Ranasinghe", "Michael S. Ryoo"], "title": "CoPT: Unsupervised Domain Adaptive Segmentation using Domain-Agnostic Text Embeddings", "comment": "ECCV 2024", "summary": "Unsupervised domain adaptation (UDA) involves learning class semantics from\nlabeled data within a source domain that generalize to an unseen target domain.\nUDA methods are particularly impactful for semantic segmentation, where\nannotations are more difficult to collect than in image classification. Despite\nrecent advances in large-scale vision-language representation learning, UDA\nmethods for segmentation have not taken advantage of the domain-agnostic\nproperties of text. To address this, we present a novel Covariance-based\nPixel-Text loss, CoPT, that uses domain-agnostic text embeddings to learn\ndomain-invariant features in an image segmentation encoder. The text embeddings\nare generated through our LLM Domain Template process, where an LLM is used to\ngenerate source and target domain descriptions that are fed to a frozen CLIP\nmodel and combined. In experiments on four benchmarks we show that a model\ntrained using CoPT achieves the new state of the art performance on UDA for\nsegmentation. The code can be found at https://github.com/cfmata/CoPT.", "AI": {"tldr": "本文提出了一种新的无监督领域适应（UDA）方法CoPT，通过利用领域无关的文本嵌入来提升图像分割领域适应的效果，实验表明该方法在四个基准数据集上达到了新的性能水平。", "motivation": "传统的无监督领域适应（UDA）方法在分割任务上效果有限，尽管在大规模视觉语言表征学习方面取得了进展，但这些进展尚未用于分割任务中的UDA方法。为了利用文本的领域无关特性，提出了一种新的方法。", "method": "提出了一种基于协方差的像素-文本损失（CoPT），利用领域无关的文本嵌入来学习图像分割编码器中的领域不变特征。文本嵌入是通过我们的大型语言模型（LLM）领域模板过程生成，该过程使用LLM生成源领域和目标领域的描述，并将其输入到冻结的CLIP模型中进行组合。", "result": "在四个基准数据集上的实验表明，使用CoPT的方法显著提升了UDA在分割任务上的性能，达到了新的SOTA水平。", "conclusion": "实验结果表明，使用CoPT训练的模型在四个基准数据集上实现了分割任务UDA的新SOTA性能，证明了所提出方法的有效性。"}}
{"id": "2507.07139", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07139", "abs": "https://arxiv.org/abs/2507.07139", "authors": ["Renyang Liu", "Guanlin Li", "Tianwei Zhang", "See-Kiong Ng"], "title": "Image Can Bring Your Memory Back: A Novel Multi-Modal Guided Attack against Image Generation Model Unlearning", "comment": null, "summary": "Recent advances in image generation models (IGMs), particularly\ndiffusion-based architectures such as Stable Diffusion (SD), have markedly\nenhanced the quality and diversity of AI-generated visual content. However,\ntheir generative capability has also raised significant ethical, legal, and\nsocietal concerns, including the potential to produce harmful, misleading, or\ncopyright-infringing content. To mitigate these concerns, machine unlearning\n(MU) emerges as a promising solution by selectively removing undesirable\nconcepts from pretrained models. Nevertheless, the robustness and effectiveness\nof existing unlearning techniques remain largely unexplored, particularly in\nthe presence of multi-modal adversarial inputs.\n  To bridge this gap, we propose Recall, a novel adversarial framework\nexplicitly designed to compromise the robustness of unlearned IGMs. Unlike\nexisting approaches that predominantly rely on adversarial text prompts, Recall\nexploits the intrinsic multi-modal conditioning capabilities of diffusion\nmodels by efficiently optimizing adversarial image prompts with guidance from a\nsingle semantically relevant reference image. Extensive experiments across ten\nstate-of-the-art unlearning methods and diverse tasks show that Recall\nconsistently outperforms existing baselines in terms of adversarial\neffectiveness, computational efficiency, and semantic fidelity with the\noriginal textual prompt. These findings reveal critical vulnerabilities in\ncurrent unlearning mechanisms and underscore the need for more robust solutions\nto ensure the safety and reliability of generative models. Code and data are\npublicly available at \\textcolor{blue}{https://github.com/ryliu68/RECALL}.", "AI": {"tldr": "研究表明现有图像生成模型的机器未学习技术存在脆弱性，提出一个名为Recall的新对抗框架来突出这些漏洞。", "motivation": "现有的机器未学习技术的鲁棒性和有效性在多模态对抗性输入的情况下尚未得到充分探索。这项研究旨在填补该研究空白，并揭示当前未学习机制中的关键漏洞。", "method": "提出一种名为Recall的新对抗框架，专门设计来破坏已去除不良概念的图像生成模型的鲁棒性。Recall不同于现有的主要依赖于对抗性文本提示的方法，而是利用扩散模型内在的多模态条件能力，通过从单个语义相关的参考图像中获取指导来有效优化对抗性图像提示。", "result": "广泛的实验结果表明，Recall在对抗有效性、计算效率和与原始文本提示的语义保真度方面始终优于现有的基线方法。", "conclusion": "这些发现揭示了当前未学习机制中的关键脆弱性，并强调了确保生成模型安全性和可靠性的更强大解决方案的需求。代码和数据已经公开获取。"}}
{"id": "2507.07148", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07148", "abs": "https://arxiv.org/abs/2507.07148", "authors": ["Getamesay Haile Dagnaw", "Yanming Zhu", "Muhammad Hassan Maqsood", "Wencheng Yang", "Xingshuai Dong", "Xuefei Yin", "Alan Wee-Chung Liew"], "title": "Explainable Artificial Intelligence in Biomedical Image Analysis: A Comprehensive Survey", "comment": null, "summary": "Explainable artificial intelligence (XAI) has become increasingly important\nin biomedical image analysis to promote transparency, trust, and clinical\nadoption of DL models. While several surveys have reviewed XAI techniques, they\noften lack a modality-aware perspective, overlook recent advances in multimodal\nand vision-language paradigms, and provide limited practical guidance. This\nsurvey addresses this gap through a comprehensive and structured synthesis of\nXAI methods tailored to biomedical image analysis.We systematically categorize\nXAI methods, analyzing their underlying principles, strengths, and limitations\nwithin biomedical contexts. A modality-centered taxonomy is proposed to align\nXAI methods with specific imaging types, highlighting the distinct\ninterpretability challenges across modalities. We further examine the emerging\nrole of multimodal learning and vision-language models in explainable\nbiomedical AI, a topic largely underexplored in previous work. Our\ncontributions also include a summary of widely used evaluation metrics and\nopen-source frameworks, along with a critical discussion of persistent\nchallenges and future directions. This survey offers a timely and in-depth\nfoundation for advancing interpretable DL in biomedical image analysis.", "AI": {"tldr": "本文系统性地审查并分类解释性人工智能（XAI）方法，特别关注生物医学图像分析领域。文章提出了基于成像模式的分类标准，强调了多模式学习和视觉语言模型的重要性，并讨论了该领域的评估指标、开源框架、挑战及未来方向。", "motivation": "由于现有的XAI技术综述往往缺乏基于成像模式的视角，忽视了多模式和视觉语言范式的最新进展，并且提供了有限的实用指南，因此本文旨在从全面和结构化的角度来综合XAI方法，特别针对生物医学图像分析。", "method": "本文通过系统地分类解释性人工智能（XAI）方法，并分析这些方法在生物医学图像分析中的基本原理、优势与局限性，弥补了现有综述的不足。特别是，本文提出了基于成像模式的分类方法，指出了跨模式解释性挑战，并探讨了多模式学习和视觉语言模型在可解释生物医学AI中的新兴作用。", "result": "该研究提供了一个基于成像模式的分类方法，讨论了广泛应用的评估指标和开源框架，并批判性地讨论了持续存在的挑战和未来的研究方向。该综述为推进生物医学图像分析中的可解释深度学习提供了及时且深入的基础。", "conclusion": "综上所述，本文不仅填补了现有XAI综述的空白，还为理解和实施生物医学图像分析中的解释性深度学习提供了全面的指导，并指出了未来的研究方向。"}}
{"id": "2507.07186", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07186", "abs": "https://arxiv.org/abs/2507.07186", "authors": ["Itay Itzhak", "Yonatan Belinkov", "Gabriel Stanovsky"], "title": "Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs", "comment": "CoLM 2025", "summary": "Large language models (LLMs) exhibit cognitive biases -- systematic\ntendencies of irrational decision-making, similar to those seen in humans.\nPrior work has found that these biases vary across models and can be amplified\nby instruction tuning. However, it remains unclear if these differences in\nbiases stem from pretraining, finetuning, or even random noise due to training\nstochasticity. We propose a two-step causal experimental approach to\ndisentangle these factors. First, we finetune models multiple times using\ndifferent random seeds to study how training randomness affects over $30$\ncognitive biases. Second, we introduce \\emph{cross-tuning} -- swapping\ninstruction datasets between models to isolate bias sources. This swap uses\ndatasets that led to different bias patterns, directly testing whether biases\nare dataset-dependent. Our findings reveal that while training randomness\nintroduces some variability, biases are mainly shaped by pretraining: models\nwith the same pretrained backbone exhibit more similar bias patterns than those\nsharing only finetuning data. These insights suggest that understanding biases\nin finetuned models requires considering their pretraining origins beyond\nfinetuning effects. This perspective can guide future efforts to develop\nprincipled strategies for evaluating and mitigating bias in LLMs.", "AI": {"tldr": "本研究通过实验方法揭示了大型语言模型中认知偏见的来源主要由预训练影响，而非微调或训练随机性。", "motivation": "本研究旨在理解大型语言模型中认知偏见的原因，这些偏见可能来源于预训练、微调，或是训练的随机性。", "method": "本研究提出了一个两步的因果实验方法来厘清这些因素。首先，通过使用不同的随机种子多次微调模型来研究训练随机性如何影响30多种认知偏见。其次，引入了\"交叉微调\"--交换微调数据集之间的模型以孤立偏见来源。这种交换使用了导致不同偏见模式的数据集，直接测试偏见是否依赖于数据集。", "result": "研究结果表明，训练随机性引起一定的变异，但偏见主要由预训练决定：具有相同预训练模型的模型比仅共享微调数据集的模型表现出更相似的偏见模式。", "conclusion": "理解微调模型中的偏见需要考虑其预训练的根源，而不仅仅是微调的影响。这种观点可以指导未来开发评估和缓解大型语言模型偏见的原则性策略。"}}
{"id": "2507.07151", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07151", "abs": "https://arxiv.org/abs/2507.07151", "authors": ["Zongmeng Zhang", "Wengang Zhou", "Jie Zhao", "Houqiang Li"], "title": "Robust Multimodal Large Language Models Against Modality Conflict", "comment": "ICML 2025", "summary": "Despite the impressive capabilities of multimodal large language models\n(MLLMs) in vision-language tasks, they are prone to hallucinations in\nreal-world scenarios. This paper investigates the hallucination phenomenon in\nMLLMs from the perspective of modality conflict. Unlike existing works focusing\non the conflicts between model responses and inputs, we study the inherent\nconflicts in inputs from different modalities that place MLLMs in a dilemma and\ndirectly lead to hallucinations. We formally define the modality conflict and\nconstruct a dataset named Multimodal Modality Conflict (MMMC) to simulate this\nphenomenon in vision-language tasks. Three methods based on prompt engineering,\nsupervised fine-tuning, and reinforcement learning are proposed to alleviate\nthe hallucination caused by modality conflict. Extensive experiments are\nconducted on the MMMC dataset to analyze the merits and demerits of these\nmethods. Our results show that the reinforcement learning method achieves the\nbest performance in mitigating the hallucination under modality conflict, while\nthe supervised fine-tuning method shows promising and stable performance. Our\nwork sheds light on the unnoticed modality conflict that leads to\nhallucinations and provides more insights into the robustness of MLLMs.", "AI": {"tldr": "本文探讨了多模态大型语言模型在视觉-语言任务中的模态冲突问题，构建了Multimodal Modality Conflict（MMMC）数据集，并提出了三种缓解模态冲突引起的幻觉的方法，实验结果表明强化学习方法表现最优。", "motivation": "尽管多模态大型语言模型在视觉-语言任务中表现出强大的能力，但在实际应用中它们往往会产生幻觉。目前的研究工作大多是关注模型响应和输入之间的冲突，而本文则着重研究来自不同模态输入之间的内在冲突，这些问题导致模型不具备稳定性。", "method": "本文提出了三种基于prompt工程、有监督微调和强化学习的方法来缓解由模态冲突引起的问题，并在Multimodal Modality Conflict数据集上进行实验以评估这些方法的有效性。", "result": "实验结果显示，基于强化学习的方法在减轻模态冲突造成的幻觉方面表现出最好的性能，而有监督微调方法则表现出非常有潜力和稳定的结果。", "conclusion": "本文揭示了一个值得注意的问题：模态冲突导致多模态大型语言模型产生误解，同时提供了关于增强模型鲁棒性的新见解。"}}
{"id": "2507.07188", "categories": ["cs.CL", "cs.AI", "cs.CY", "J.4"], "pdf": "https://arxiv.org/pdf/2507.07188", "abs": "https://arxiv.org/abs/2507.07188", "authors": ["Jens Rupprecht", "Georg Ahnert", "Markus Strohmaier"], "title": "Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses", "comment": "18 pages, 17 figures", "summary": "Large Language Models (LLMs) are increasingly used as proxies for human\nsubjects in social science surveys, but their reliability and susceptibility to\nknown response biases are poorly understood. This paper investigates the\nresponse robustness of LLMs in normative survey contexts -- we test nine\ndiverse LLMs on questions from the World Values Survey (WVS), applying a\ncomprehensive set of 11 perturbations to both question phrasing and answer\noption structure, resulting in over 167,000 simulated interviews. In doing so,\nwe not only reveal LLMs' vulnerabilities to perturbations but also reveal that\nall tested models exhibit a consistent \\textit{recency bias} varying in\nintensity, disproportionately favoring the last-presented answer option. While\nlarger models are generally more robust, all models remain sensitive to\nsemantic variations like paraphrasing and to combined perturbations. By\napplying a set of perturbations, we reveal that LLMs partially align with\nsurvey response biases identified in humans. This underscores the critical\nimportance of prompt design and robustness testing when using LLMs to generate\nsynthetic survey data.", "AI": {"tldr": "The paper examines LLMs' responses to survey questions, revealing vulnerabilities and a consistent recency bias, and highlights the need for prompt design and robustness testing in generating synthetic survey data.", "motivation": "The motivation is to understand the reliability and susceptibility to known response biases of LLMs when used as proxies for human subjects in social science surveys.", "method": "This paper investigates the response robustness of LLMs in normative survey contexts by testing nine diverse LLMs on questions from the World Values Survey (WVS), applying a comprehensive set of 11 perturbations to both question phrasing and answer option structure.", "result": "The research reveals that all tested models exhibit a consistent recency bias varying in intensity, favoring the last-presented answer option, and remain sensitive to semantic variations like paraphrasing and combined perturbations.", "conclusion": "The study underscores the critical importance of prompt design and robustness testing when using LLMs to generate synthetic survey data."}}
{"id": "2507.07153", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07153", "abs": "https://arxiv.org/abs/2507.07153", "authors": ["Antonella Barisic Kulas", "Frano Petric", "Stjepan Bogdan"], "title": "Aerial Maritime Vessel Detection and Identification", "comment": "Preprint. ICUAS 2025", "summary": "Autonomous maritime surveillance and target vessel identification in\nenvironments where Global Navigation Satellite Systems (GNSS) are not available\nis critical for a number of applications such as search and rescue and threat\ndetection. When the target vessel is only described by visual cues and its last\nknown position is not available, unmanned aerial vehicles (UAVs) must rely\nsolely on on-board vision to scan a large search area under strict\ncomputational constraints. To address this challenge, we leverage the YOLOv8\nobject detection model to detect all vessels in the field of view. We then\napply feature matching and hue histogram distance analysis to determine whether\nany detected vessel corresponds to the target. When found, we localize the\ntarget using simple geometric principles. We demonstrate the proposed method in\nreal-world experiments during the MBZIRC2023 competition, integrated into a\nfully autonomous system with GNSS-denied navigation. We also evaluate the\nimpact of perspective on detection accuracy and localization precision and\ncompare it with the oracle approach.", "AI": {"tldr": "本文开发了一种利用视觉线索识别并定位目标船只的方法，特别适用于没有GNSS信号的环境中，该方法已通过现实世界实验进行验证。", "motivation": "在没有GNSS信号的环境下，自主海上监视和目标船只识别对搜索和救援及威胁检测等应用至关重要。该研究旨在解决仅凭视觉线索且无目标船只的最后已知位置时，无人机如何在严格的计算约束下扫描大面积区域的问题。", "method": "本文提出了一种使用YOLOv8模型检测所有可视船只，并通过特征匹配和色调直方图距离分析来识别目标船只的方法。当目标船只被识别后，使用简单的几何原理对目标进行定位。", "result": "该方法在MBZIRC2023竞赛的现实世界实验中进行了验证，并与理想情况进行了比较，评估了视角对检测准确性和定位精度的影响。", "conclusion": "研究结果表明，采用这种方法即使在GNSS受限的条件下，也能有效实现船只的识别和定位，但视角的不同会影响检测和定位的准确性。"}}
{"id": "2507.07229", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07229", "abs": "https://arxiv.org/abs/2507.07229", "authors": ["Krithika Ramesh", "Daniel Smolyak", "Zihao Zhao", "Nupoor Gandhi", "Ritu Agarwal", "Margrét Bjarnadóttir", "Anjalie Field"], "title": "SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains", "comment": null, "summary": "We present SynthTextEval, a toolkit for conducting comprehensive evaluations\nof synthetic text. The fluency of large language model (LLM) outputs has made\nsynthetic text potentially viable for numerous applications, such as reducing\nthe risks of privacy violations in the development and deployment of AI systems\nin high-stakes domains. Realizing this potential, however, requires principled\nconsistent evaluations of synthetic data across multiple dimensions: its\nutility in downstream systems, the fairness of these systems, the risk of\nprivacy leakage, general distributional differences from the source text, and\nqualitative feedback from domain experts. SynthTextEval allows users to conduct\nevaluations along all of these dimensions over synthetic data that they upload\nor generate using the toolkit's generation module. While our toolkit can be run\nover any data, we highlight its functionality and effectiveness over datasets\nfrom two high-stakes domains: healthcare and law. By consolidating and\nstandardizing evaluation metrics, we aim to improve the viability of synthetic\ntext, and in-turn, privacy-preservation in AI development.", "AI": {"tldr": "SynthTextEval工具包用于评估合成文本在下游应用中的实用性、系统公平性、隐私泄露风险、与其他文本分布差异等方面，特别适用于健康和法律领域的合成数据。这有助于提升合成文本的实用性和加强隐私保护。", "motivation": "由于大型语言模型（LLM）输出的流畅性，合成文本在减少AI系统开发和部署中的隐私泄露风险方面具有潜在价值，特别是在高风险领域。", "method": "本文介绍了SynthTextEval工具包，该工具包旨在对合成文本进行全面评估，涵盖了合成数据的多个维度：其在下游系统中的应用效果、系统的公平性、隐私泄露风险、与源文本在分布上的差异以及领域专家的定性反馈。", "result": "该工具包使得用户能够评估他们上传或使用其生成模块生成的数据的各个维度，并且特别强调了健康和法律两个高风险领域中数据集的功能和有效性。", "conclusion": "通过整合和标准化评价指标，该研究旨在提升合成文本的实用性和保持AI开发中的隐私保护。"}}
{"id": "2507.07154", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07154", "abs": "https://arxiv.org/abs/2507.07154", "authors": ["Desheng Li", "Chaoliang Liu", "Zhiyong Xiao"], "title": "CL-Polyp: A Contrastive Learning-Enhanced Network for Accurate Polyp Segmentation", "comment": null, "summary": "Accurate segmentation of polyps from colonoscopy images is crucial for the\nearly diagnosis and treatment of colorectal cancer. Most existing deep\nlearning-based polyp segmentation methods adopt an Encoder-Decoder\narchitecture, and some utilize multi-task frameworks that incorporate auxiliary\ntasks such as classification to enhance segmentation performance. However,\nthese approaches often require additional labeled data and rely on task\nsimilarity, which can limit their generalizability. To address these\nchallenges, we propose CL-Polyp, a contrastive learning-enhanced polyp\nsegmentation network. Our method leverages contrastive learning to improve the\nencoder's ability to extract discriminative features by contrasting positive\nand negative sample pairs derived from polyp images. This self-supervised\nstrategy enhances visual representation without requiring additional\nannotations. In addition, we introduce two lightweight and effective modules:\nthe Modified Atrous Spatial Pyramid Pooling (MASPP) module for better\nmulti-scale feature fusion, and the Channel Concatenate and Element Add (CA)\nmodule to fuse low-level and upsampled features for improved boundary\nreconstruction. Extensive experiments on five benchmark datasets-Kvasir-SEG,\nCVC-ClinicDB, CVC-ColonDB, CVC-300, and ETIS-demonstrate that CL-Polyp\nconsistently outperforms state-of-the-art methods. Specifically, it improves\nthe IoU metric by 0.011 and 0.020 on the Kvasir-SEG and CVC-ClinicDB datasets,\nrespectively, validating its effectiveness in clinical polyp segmentation\ntasks.", "AI": {"tldr": "本文提出了CL-Polyp，一种基于对比学习的结肠息肉分割网络，通过对比正负样本对来提高特征提取能力，并引入了两个轻量级模块来提升多尺度特征融合和边界重建效果，在多个数据集上表现优于现有方法。", "motivation": "现有息肉分割方法依赖辅助任务框架，需要额外标注数据，且受限于任务相似性，这限制了其泛化能力。本文旨在解决这些问题，提出无需额外标注的自我监督策略以改善视觉表示。", "method": "本文采用对比学习增强的编码器解码器架构，引入对比学习方法提高特征提取能力，并设计了MASPP和CA模块来提升分割效果。", "result": "实验显示，CL-Polyp在Kvasir-SEG和CVC-ClinicDB数据集上分别提高了0.011和0.020的IoU指标，表现优于现有技术。", "conclusion": "CL-Polyp展示了在无须额外标注的情况下改善息肉分割性能的能力，证明了其在临床息肉分割任务中的有效性。"}}
{"id": "2507.07248", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07248", "abs": "https://arxiv.org/abs/2507.07248", "authors": ["Minseon Kim", "Jean-Philippe Corbeil", "Alessandro Sordoni", "Francois Beaulieu", "Paul Vozila"], "title": "Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings", "comment": null, "summary": "As the performance of large language models (LLMs) continues to advance,\ntheir adoption is expanding across a wide range of domains, including the\nmedical field. The integration of LLMs into medical applications raises\ncritical safety concerns, particularly due to their use by users with diverse\nroles, e.g. patients and clinicians, and the potential for model's outputs to\ndirectly affect human health. Despite the domain-specific capabilities of\nmedical LLMs, prior safety evaluations have largely focused only on general\nsafety benchmarks. In this paper, we introduce a safety evaluation protocol\ntailored to the medical domain in both patient user and clinician user\nperspectives, alongside general safety assessments and quantitatively analyze\nthe safety of medical LLMs. We bridge a gap in the literature by building the\nPatientSafetyBench containing 466 samples over 5 critical categories to measure\nsafety from the perspective of the patient. We apply our red-teaming protocols\non the MediPhi model collection as a case study. To our knowledge, this is the\nfirst work to define safety evaluation criteria for medical LLMs through\ntargeted red-teaming taking three different points of view - patient,\nclinician, and general user - establishing a foundation for safer deployment in\nmedical domains.", "AI": {"tldr": "本文为医疗领域语言模型专门设计了一种安全评估协议，并构建了PatientSafetyBench数据集，进行红队测试，从患者、临床医生和普通用户三个视角评估模型的安全性，首次定义了医疗领域LLMs的安全评估标准。", "motivation": "随着大型语言模型（LLMs）在医学领域的应用越来越广泛，从不同角色的用户使用的安全问题变得日益关键。此前的安全评估主要集中在通用安全基准上，少有针对医疗领域安全的具体设计。", "method": "本文提出了一种专门针对医疗领域语言模型的安全评估协议，该协议从患者视角和临床医生视角出发，除了通用安全评估之外，还进行了定量分析。此外，构建了包含466个样本的PatientSafetyBench数据集，覆盖5个关键安全类别，用于从患者角度衡量安全性。并通过MediPhi模型集合进行案例研究，应用了其红队测试协议。", "result": "通过应用具体的红队测试协议对MediPhi模型集合进行分析，本文填补了这一领域的空白，首次定义了医疗领域语言模型的安全评估标准，此种标准是从患者、临床医生和普通用户三个不同的角度进行。", "conclusion": "本文的工作为在医学领域更加安全地部署语言模型奠定了基础。"}}
{"id": "2507.07157", "categories": ["cs.CV", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07157", "abs": "https://arxiv.org/abs/2507.07157", "authors": ["Arshak Rezvani", "Ali Akbari", "Kosar Sanjar Arani", "Maryam Mirian", "Emad Arasteh", "Martin J. McKeown"], "title": "Interpretable EEG-to-Image Generation with Semantic Prompts", "comment": "Actionable Interpretability Workshop (non-archival) at the 42\n  International Conference on Machine Learning", "summary": "Decoding visual experience from brain signals offers exciting possibilities\nfor neuroscience and interpretable AI. While EEG is accessible and temporally\nprecise, its limitations in spatial detail hinder image reconstruction. Our\nmodel bypasses direct EEG-to-image generation by aligning EEG signals with\nmultilevel semantic captions -- ranging from object-level to abstract themes --\ngenerated by a large language model. A transformer-based EEG encoder maps brain\nactivity to these captions through contrastive learning. During inference,\ncaption embeddings retrieved via projection heads condition a pretrained latent\ndiffusion model for image generation. This text-mediated framework yields\nstate-of-the-art visual decoding on the EEGCVPR dataset, with interpretable\nalignment to known neurocognitive pathways. Dominant EEG-caption associations\nreflected the importance of different semantic levels extracted from perceived\nimages. Saliency maps and t-SNE projections reveal semantic topography across\nthe scalp. Our model demonstrates how structured semantic mediation enables\ncognitively aligned visual decoding from EEG.", "AI": {"tldr": "研究构建了一个通过多层级语义描述媒介解码EEG信号的视觉体验的方法，实现了图像的生成，并揭示了语义在EEG信号中的分布。", "motivation": "旨在克服EEG在空间细节上的限制，通过文本中介框架实现从EEG信号到图像的解码，结合神经科学与可解释的AI。", "method": "利用大规模语言模型生成多层级语义描述（从物体级别到抽象主题），并通过对比学习将EEG信号对齐到这些描述上。在推理阶段，通过投影头检索到的描述嵌入条件化预训练的潜扩散模型进行图像生成。", "result": "该方法在EEGCVPR数据集上达到了最先进的视觉解码效果，并揭示了感知图像中不同语义级别的EEG-描述关联性的显着性。", "conclusion": "此模型展示了结构化语义中介如何实现认知对齐的视觉从EEG信号的解码，从而提供了对感知图像中语义重要性的新见解。"}}
{"id": "2507.07280", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07280", "abs": "https://arxiv.org/abs/2507.07280", "authors": ["Mariah Bradford", "Nikhil Krishnaswamy", "Nathaniel Blanchard"], "title": "The Impact of Background Speech on Interruption Detection in Collaborative Groups", "comment": "Long Paper AIED 2025", "summary": "Interruption plays a crucial role in collaborative learning, shaping group\ninteractions and influencing knowledge construction. AI-driven support can\nassist teachers in monitoring these interactions. However, most previous work\non interruption detection and interpretation has been conducted in\nsingle-conversation environments with relatively clean audio. AI agents\ndeployed in classrooms for collaborative learning within small groups will need\nto contend with multiple concurrent conversations -- in this context,\noverlapping speech will be ubiquitous, and interruptions will need to be\nidentified in other ways. In this work, we analyze interruption detection in\nsingle-conversation and multi-group dialogue settings. We then create a\nstate-of-the-art method for interruption identification that is robust to\noverlapping speech, and thus could be deployed in classrooms. Further, our work\nhighlights meaningful linguistic and prosodic information about how\ninterruptions manifest in collaborative group interactions. Our investigation\nalso paves the way for future works to account for the influence of overlapping\nspeech from multiple groups when tracking group dialog.", "AI": {"tldr": "研究开发了一种先进方法，用于在教室这种包含重叠语音的多小组对话环境中识别中断，并揭示了中断在合作学习中是如何表现的。", "motivation": "虽然先前关于中断检测和解释的研究大多在单一对话环境中进行，本研究旨在解决在包含多个并行对话的教室场景中，AI如何从重叠语音中识别中断的问题。", "method": "本研究分析了在单一对话环境和多小组对话环境中中断检测的差异，并开发了一种能够应对重叠语音的先进中断识别方法，使得该方法可以应用于教室场景。此外，研究还探讨了合作小组互动中断中具有意义的语用和韵律信息。", "result": "开发了一种先进的中断识别方法，这种方法对于重叠语音具有鲁棒性，因此能够应用于教育领域，特别是小组协作学习的教室环境。", "conclusion": "本研究提供了一个方法来识别重叠语音中的中断，为未来跟踪小组对话和考虑多重重叠语音影响的研究铺平了道路。"}}
{"id": "2507.07202", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07202", "abs": "https://arxiv.org/abs/2507.07202", "authors": ["Mohamed Elmoghany", "Ryan Rossi", "Seunghyun Yoon", "Subhojyoti Mukherjee", "Eslam Bakr", "Puneet Mathur", "Gang Wu", "Viet Dac Lai", "Nedim Lipka", "Ruiyi Zhang", "Varun Manjunatha", "Chien Nguyen", "Daksh Dangi", "Abel Salinas", "Mohammad Taesiri", "Hongjie Chen", "Xiaolei Huang", "Joe Barrow", "Nesreen Ahmed", "Hoda Eldardiry", "Namyong Park", "Yu Wang", "Jaemin Cho", "Anh Totti Nguyen", "Zhengzhong Tu", "Thien Nguyen", "Dinesh Manocha", "Mohamed Elhoseiny", "Franck Dernoncourt"], "title": "A Survey on Long-Video Storytelling Generation: Architectures, Consistency, and Cinematic Quality", "comment": null, "summary": "Despite the significant progress that has been made in video generative\nmodels, existing state-of-the-art methods can only produce videos lasting 5-16\nseconds, often labeled \"long-form videos\". Furthermore, videos exceeding 16\nseconds struggle to maintain consistent character appearances and scene layouts\nthroughout the narrative. In particular, multi-subject long videos still fail\nto preserve character consistency and motion coherence. While some methods can\ngenerate videos up to 150 seconds long, they often suffer from frame redundancy\nand low temporal diversity. Recent work has attempted to produce long-form\nvideos featuring multiple characters, narrative coherence, and high-fidelity\ndetail. We comprehensively studied 32 papers on video generation to identify\nkey architectural components and training strategies that consistently yield\nthese qualities. We also construct a comprehensive novel taxonomy of existing\nmethods and present comparative tables that categorize papers by their\narchitectural designs and performance characteristics.", "AI": {"tldr": "本文研究了32篇生成高质量长视频的论文，揭示了关键架构与策略，构建了新分类法，并提供对比表。", "motivation": "现有的视频生成模型在产生长视频时，尤其是在多角色视频中，难以保持角色一致性和动作连贯性。本研究旨在通过综合分析现有文献，解决这些问题，提升视频生成的质量与多样性。", "method": "通过对32篇视频生成论文的全面研究，我们确定了能够持续产生高质量视频的关键架构组件和训练策略。我们还构建了一个现有方法的详尽新分类法，并提供了分类表，根据其架构设计和性能特点对论文进行分类。", "result": "提出了一种关于视频生成方法的新分类法，并通过对比表分类现有研究，提高了理解和开发高质量视频生成模型的能力。", "conclusion": "我们总结了视频生成研究的关键组件和策略，提出了一个详尽的新分类法，并提供了对比表格。这些工作有助于理解如何持续生成高质量的视频。"}}
{"id": "2507.07307", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07307", "abs": "https://arxiv.org/abs/2507.07307", "authors": ["Anirban Saha Anik", "Xiaoying Song", "Elliott Wang", "Bryan Wang", "Bengisu Yarimbas", "Lingzi Hong"], "title": "Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation", "comment": null, "summary": "Large language models (LLMs) incorporated with Retrieval-Augmented Generation\n(RAG) have demonstrated powerful capabilities in generating counterspeech\nagainst misinformation. However, current studies rely on limited evidence and\noffer less control over final outputs. To address these challenges, we propose\na Multi-agent Retrieval-Augmented Framework to generate counterspeech against\nhealth misinformation, incorporating multiple LLMs to optimize knowledge\nretrieval, evidence enhancement, and response refinement. Our approach\nintegrates both static and dynamic evidence, ensuring that the generated\ncounterspeech is relevant, well-grounded, and up-to-date. Our method\noutperforms baseline approaches in politeness, relevance, informativeness, and\nfactual accuracy, demonstrating its effectiveness in generating high-quality\ncounterspeech. To further validate our approach, we conduct ablation studies to\nverify the necessity of each component in our framework. Furthermore, human\nevaluations reveal that refinement significantly enhances counterspeech quality\nand obtains human preference.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.07230", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07230", "abs": "https://arxiv.org/abs/2507.07230", "authors": ["Priyank Pathak", "Yogesh S. Rawat"], "title": "Colors See Colors Ignore: Clothes Changing ReID with Color Disentanglement", "comment": "ICCV'25 paper", "summary": "Clothes-Changing Re-Identification (CC-ReID) aims to recognize individuals\nacross different locations and times, irrespective of clothing. Existing\nmethods often rely on additional models or annotations to learn robust,\nclothing-invariant features, making them resource-intensive. In contrast, we\nexplore the use of color - specifically foreground and background colors - as a\nlightweight, annotation-free proxy for mitigating appearance bias in ReID\nmodels. We propose Colors See, Colors Ignore (CSCI), an RGB-only method that\nleverages color information directly from raw images or video frames. CSCI\nefficiently captures color-related appearance bias ('Color See') while\ndisentangling it from identity-relevant ReID features ('Color Ignore'). To\nachieve this, we introduce S2A self-attention, a novel self-attention to\nprevent information leak between color and identity cues within the feature\nspace. Our analysis shows a strong correspondence between learned color\nembeddings and clothing attributes, validating color as an effective proxy when\nexplicit clothing labels are unavailable. We demonstrate the effectiveness of\nCSCI on both image and video ReID with extensive experiments on four CC-ReID\ndatasets. We improve the baseline by Top-1 2.9% on LTCC and 5.0% on PRCC for\nimage-based ReID, and 1.0% on CCVID and 2.5% on MeVID for video-based ReID\nwithout relying on additional supervision. Our results highlight the potential\nof color as a cost-effective solution for addressing appearance bias in\nCC-ReID. Github: https://github.com/ppriyank/ICCV-CSCI-Person-ReID.", "AI": {"tldr": "提出了CSCI方法，利用颜色信息减轻重识别模型中的外观偏差，显示出比基线更高的准确性。", "motivation": "解决现有方法依赖额外模型或标注的问题，探索使用颜色作为轻量级、无需标注的代理方法。", "method": "CSCI方法和S2A自注意力机制，直接从原始图像或视频帧中利用颜色信息，以缓解外观偏差。", "result": "在四个CC-ReID数据集上进行了广泛实验，提高了图像和视频重识别的准确性。", "conclusion": "颜色是一种有效的、成本效益高的解决重识别中外观偏差的代理方法。"}}
{"id": "2507.07414", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.07414", "abs": "https://arxiv.org/abs/2507.07414", "authors": ["Fardin Rastakhiz"], "title": "GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural Networks for Text Representation", "comment": null, "summary": "Time, cost, and energy efficiency are critical considerations in\nDeep-Learning (DL), particularly when processing long texts. Transformers,\nwhich represent the current state of the art, exhibit quadratic computational\ncomplexity relative to input length, making them inefficient for extended\ndocuments. This study introduces a novel model architecture that combines Graph\nNeural Networks (GNNs) and Convolutional Neural Networks (CNNs), integrated\nwith a real-time, end-to-end graph generation mechanism. The model processes\ncompact batches of character-level inputs without requiring padding or\ntruncation. To enhance performance while maintaining high speed and efficiency,\nthe model incorporates information from Large Language Models (LLMs), such as\ntoken embeddings and sentiment polarities, through efficient dictionary\nlookups. It captures local contextual patterns using CNNs, expands local\nreceptive fields via lattice-based graph structures, and employs small-world\ngraphs to aggregate document-level information. The generated graphs exhibit\nstructural properties indicative of meaningful semantic organization, with an\naverage clustering coefficient of approximately 0.45 and an average shortest\npath length ranging between 4 and 5. The model is evaluated across multiple\ntext classification tasks, including sentiment analysis and\nnews-categorization, and is compared against state-of-the-art models.\nExperimental results confirm the proposed model's efficiency and competitive\nperformance.", "AI": {"tldr": "本文提出了一种新型的模型架构，结合了CNNs和GNNs，用于处理文本以提高效率。该模型通过实时生成图来处理字符级的输入，并在多个文本分类任务中显示出良好的效果。", "motivation": "研究的动机在于，处理长文本时，时间和能量效率是深度学习（DL）中的关键问题。转换器模型存在计算复杂度较高问题，尤其是相对于输入长度的二次复杂度，这使得它们在处理较长文档时效率低下。因此，提出了一种新的模型架构以解决此问题。", "method": "该研究提出了一种结合图神经网络（GNNs）和卷积神经网络（CNNs）的新模型架构，它与实时端到端图生成机制集成。模型处理字符级输入的小批次，无需填充或截断，并通过高效字典查找整合大型语言模型中的信息，如标记嵌入和情感极性。使用CNN捕捉局部上下文模式，利用基于晶格的图结构扩大局部接受场，并采用小世界图聚合文档级别的信息。", "result": "实验结果证实了所提出的模型在效率和性能上的竞争力。模型在多个文本分类任务（包括情感分析和新闻分类）上进行了评估，并与最先进的模型进行了比较。", "conclusion": "研究结论表明，所提出的结合CNNs和GNNs，并集成实时端到端图生成机制的模型架构，虽然在处理长文本时可以更高效，但也展示了竞争性的性能表现。"}}
{"id": "2507.07242", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07242", "abs": "https://arxiv.org/abs/2507.07242", "authors": ["Johannes Merz", "Lucien Fostier"], "title": "Automated Video Segmentation Machine Learning Pipeline", "comment": null, "summary": "Visual effects (VFX) production often struggles with slow, resource-intensive\nmask generation. This paper presents an automated video segmentation pipeline\nthat creates temporally consistent instance masks. It employs machine learning\nfor: (1) flexible object detection via text prompts, (2) refined per-frame\nimage segmentation and (3) robust video tracking to ensure temporal stability.\nDeployed using containerization and leveraging a structured output format, the\npipeline was quickly adopted by our artists. It significantly reduces manual\neffort, speeds up the creation of preliminary composites, and provides\ncomprehensive segmentation data, thereby enhancing overall VFX production\nefficiency.", "AI": {"tldr": "The paper introduces an efficient automated video segmentation pipeline leveraging machine learning, which has been successfully adopted in VFX production, cutting down manual mask creation time and boosting efficiency.", "motivation": "The motivation is to address the slow and resource-intensive process of mask generation in VFX production, aiming to reduce manual effort, speed up the initial compositing stages, and improve the overall efficiency of VFX production.", "method": "This paper proposes an automated video segmentation pipeline that uses machine learning for object detection with text prompts, per-frame segmentation, and video tracking to maintain temporal stability. The pipeline is deployed using containerization.", "result": "The pipeline significantly reduces manual labor, expedites the creation of preliminary composites, and provides thorough segmentation data, leading to improved VFX production efficiency.", "conclusion": "The automated video segmentation pipeline effectively enhances the VFX production process by automating mask generation, thereby saving time and resources."}}
{"id": "2507.07419", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07419", "abs": "https://arxiv.org/abs/2507.07419", "authors": ["Hieu Tran", "Zonghai Yao", "Won Seok Jang", "Sharmin Sultana", "Allen Chang", "Yuan Zhang", "Hong Yu"], "title": "MedReadCtrl: Personalizing medical text generation with readability-controlled instruction learning", "comment": "Equal contribution for the first two authors. arXiv admin note: text\n  overlap with arXiv:2406.09205", "summary": "Generative AI has demonstrated strong potential in healthcare, from clinical\ndecision support to patient-facing chatbots that improve outcomes. A critical\nchallenge for deployment is effective human-AI communication, where content\nmust be both personalized and understandable. We introduce MedReadCtrl, a\nreadability-controlled instruction tuning framework that enables LLMs to adjust\noutput complexity without compromising meaning. Evaluations of nine datasets\nand three tasks across medical and general domains show that MedReadCtrl\nachieves significantly lower readability instruction-following errors than\nGPT-4 (e.g., 1.39 vs. 1.59 on ReadMe, p<0.001) and delivers substantial gains\non unseen clinical tasks (e.g., +14.7 ROUGE-L, +6.18 SARI on MTSamples).\nExperts consistently preferred MedReadCtrl (71.7% vs. 23.3%), especially at low\nliteracy levels. These gains reflect MedReadCtrl's ability to restructure\nclinical content into accessible, readability-aligned language while preserving\nmedical intent, offering a scalable solution to support patient education and\nexpand equitable access to AI-enabled care.", "AI": {"tldr": "研究介绍MedReadCtrl框架，提升了生成式AI在医疗领域中人机沟通的个性化与可理解性，特别是在临床任务和低学历水平阅读需求上的表现优于GPT-4。", "motivation": "提高生成式AI在医疗领域应用时的人机沟通效率，特别是在个性化和易理解内容的生成上。", "method": "引入了MedReadCtrl，这是一种可调难度而不影响意义的可读性控制指令调优框架。", "result": "在九个数据集和三个任务中的评估显示MedReadCtrl比GPT-4有更低的可读性指令遵循错误，并且在未见的临床任务上有了巨大的提升。专家更倾向于MedReadCtrl，尤其在低学历水平用户中。", "conclusion": "MedReadCtrl能够将临床内容重组为易于理解且符合可读性要求的语言，同时保留医疗意图，为患者教育和支持AI赋能的医疗提供了可扩展解决方案。"}}
{"id": "2507.07262", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07262", "abs": "https://arxiv.org/abs/2507.07262", "authors": ["Shehreen Azad", "Yogesh S Rawat"], "title": "DisenQ: Disentangling Q-Former for Activity-Biometrics", "comment": "Accepted in ICCV 2025", "summary": "In this work, we address activity-biometrics, which involves identifying\nindividuals across diverse set of activities. Unlike traditional person\nidentification, this setting introduces additional challenges as identity cues\nbecome entangled with motion dynamics and appearance variations, making\nbiometrics feature learning more complex. While additional visual data like\npose and/or silhouette help, they often struggle from extraction inaccuracies.\nTo overcome this, we propose a multimodal language-guided framework that\nreplaces reliance on additional visual data with structured textual\nsupervision. At its core, we introduce \\textbf{DisenQ} (\\textbf{Disen}tangling\n\\textbf{Q}-Former), a unified querying transformer that disentangles\nbiometrics, motion, and non-biometrics features by leveraging structured\nlanguage guidance. This ensures identity cues remain independent of appearance\nand motion variations, preventing misidentifications. We evaluate our approach\non three activity-based video benchmarks, achieving state-of-the-art\nperformance. Additionally, we demonstrate strong generalization to complex\nreal-world scenario with competitive performance on a traditional video-based\nidentification benchmark, showing the effectiveness of our framework.", "AI": {"tldr": "本文提出了DisenQ，这是一种通过结构化语言指导来解耦特征的查询转换器，用于解决活动生物识别问题，优于依赖额外视觉数据的方法并达到了最先进的性能。", "motivation": "解决活动生物识别领域的问题，即在一系列不同活动中对个体进行识别。这个问题在身份线索与运动动力学和外观变化纠缠在一起时变得更加复杂。", "method": "提出了一种多模态语言引导框架，使用结构化文本指导来解耦生物特征、运动和非生物特征特征，而不是依赖额外的视觉数据。核心是一个统一的查询转换器DisenQ，它通过结构化语言指导来分离这些特征。", "result": "在三个基于活动的视频基准测试中，该方法达到了最先进的性能。此外，在一个传统的基于视频的身份认证基准测试中也展示了强大的通用性和竞争力。", "conclusion": "该框架通过结构化语言指导确保身份线索独立于外观和运动变化，防止错误识别，适合复杂的真实世界场景，表明了框架的有效性。"}}
{"id": "2507.07421", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07421", "abs": "https://arxiv.org/abs/2507.07421", "authors": ["Zonghai Yao", "Youxia Zhao", "Avijit Mitra", "David A. Levy", "Emily Druhl", "Jack Tsai", "Hong Yu"], "title": "SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic EHR Data", "comment": "Equal contribution for the first two authors", "summary": "Eviction is a significant yet understudied social determinants of health\n(SDoH), linked to housing instability, unemployment, and mental health. While\neviction appears in unstructured electronic health records (EHRs), it is rarely\ncoded in structured fields, limiting downstream applications. We introduce\nSynthEHR-Eviction, a scalable pipeline combining LLMs, human-in-the-loop\nannotation, and automated prompt optimization (APO) to extract eviction\nstatuses from clinical notes. Using this pipeline, we created the largest\npublic eviction-related SDoH dataset to date, comprising 14 fine-grained\ncategories. Fine-tuned LLMs (e.g., Qwen2.5, LLaMA3) trained on\nSynthEHR-Eviction achieved Macro-F1 scores of 88.8% (eviction) and 90.3% (other\nSDoH) on human validated data, outperforming GPT-4o-APO (87.8%, 87.3%),\nGPT-4o-mini-APO (69.1%, 78.1%), and BioBERT (60.7%, 68.3%), while enabling\ncost-effective deployment across various model sizes. The pipeline reduces\nannotation effort by over 80%, accelerates dataset creation, enables scalable\neviction detection, and generalizes to other information extraction tasks.", "AI": {"tldr": "开发了SynthEHR-Eviction管道，结合LLM、人机协作注释和APO，从临床记录中有效抽取驱逐状态，创建了大规模数据集，显著提高了数据标注效率和模型性能。", "motivation": "驱逐是健康的社会决定因素（SDoH），但在结构化EHR字段中很少编码。这项工作旨在解决这一问题，提供了一个可以大规模应用的解决方案。", "method": "介绍了一种称为SynthEHR-Eviction的管道，结合了大语言模型（LLM）、人机协作注释和自动提示优化（APO），用于从临床记录中抽取驱逐状态。", "result": "使用该管道创建了迄今为止最大的公共驱逐相关SDoH数据集，含有14个细粒度类别。调优后的LLM在人类验证的数据上实现了高达88.8%（驱逐）和90.3%（其他SDoH）的Macro-F1分数。", "conclusion": "该方法减少了80%以上的标注工作，加速了数据集的创建，实现了可扩展的驱逐检测，并可以推广到其他信息抽取任务。"}}
{"id": "2507.07274", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07274", "abs": "https://arxiv.org/abs/2507.07274", "authors": ["Ananya Raval", "Aravind Narayanan", "Vahid Reza Khazaie", "Shaina Raza"], "title": "LinguaMark: Do Multimodal Models Speak Fairly? A Benchmark-Based Evaluation", "comment": "Accepted at ASONAM'25", "summary": "Large Multimodal Models (LMMs) are typically trained on vast corpora of\nimage-text data but are often limited in linguistic coverage, leading to biased\nand unfair outputs across languages. While prior work has explored multimodal\nevaluation, less emphasis has been placed on assessing multilingual\ncapabilities. In this work, we introduce LinguaMark, a benchmark designed to\nevaluate state-of-the-art LMMs on a multilingual Visual Question Answering\n(VQA) task. Our dataset comprises 6,875 image-text pairs spanning 11 languages\nand five social attributes. We evaluate models using three key metrics: Bias,\nAnswer Relevancy, and Faithfulness. Our findings reveal that closed-source\nmodels generally achieve the highest overall performance. Both closed-source\n(GPT-4o and Gemini2.5) and open-source models (Gemma3, Qwen2.5) perform\ncompetitively across social attributes, and Qwen2.5 demonstrates strong\ngeneralization across multiple languages. We release our benchmark and\nevaluation code to encourage reproducibility and further research.", "AI": {"tldr": "本文介绍了LinguaMark，一个用于评估大型多模态模型在多语言视觉问答任务上的基准，测试结果显示Qwen2.5在多语言泛化方面表现突出。", "motivation": "尽管大型多模态模型通常在大量的图像-文本数据上进行训练，但它们往往在语言覆盖范围上存在限制，导致跨语言输出存在偏差和不公平问题。以前的工作虽然探索了多模态评估，但较少关注多语言能力的评估。因此，本工作旨在开发一个针对多语言VQA任务的评估基准。", "method": "介绍了一个名为LinguaMark的新基准，用于评估先进的大型多模态模型在多语言视觉问答（VQA）任务上的表现。该数据集包含6,875个跨越11种语言和五种社会属性的图像-文本对，并使用偏见、回答相关性和忠实度三个关键指标评估模型。", "result": "在LinguaMark基准测试中，闭源模型如GPT-4o和Gemini2.5，以及开源模型如Gemma3和Qwen2.5，在偏见、回答相关性和忠实度这三个关键指标上表现良好。Qwen2.5尤其在不同语言之间表现出强大的泛化能力。", "conclusion": "研究发现闭源模型总体表现最佳，闭源模型（GPT-4o 和 Gemini2.5）和开源模型（Gemma3，Qwen2.5）在社会属性方面表现均较强，特别地，Qwen2.5 在多种语言中表现出强大的泛化能力。提供基准和评估代码鼓励可重复性和进一步的研究。"}}
{"id": "2507.07439", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07439", "abs": "https://arxiv.org/abs/2507.07439", "authors": ["Matthieu Boileau", "Philippe Helluy", "Jeremy Pawlus", "Svitlana Vyetrenko"], "title": "Towards Interpretable Time Series Foundation Models", "comment": "International Conference on Machine Leaning (ICML) 2025 Workshop on\n  Foundation Models for Structured Data", "summary": "In this paper, we investigate the distillation of time series reasoning\ncapabilities into small, instruction-tuned language models as a step toward\nbuilding interpretable time series foundation models. Leveraging a synthetic\ndataset of mean-reverting time series with systematically varied trends and\nnoise levels, we generate natural language annotations using a large multimodal\nmodel and use these to supervise the fine-tuning of compact Qwen models. We\nintroduce evaluation metrics that assess the quality of the distilled reasoning\n- focusing on trend direction, noise intensity, and extremum localization - and\nshow that the post-trained models acquire meaningful interpretive capabilities.\nOur results highlight the feasibility of compressing time series understanding\ninto lightweight, language-capable models suitable for on-device or\nprivacy-sensitive deployment. This work contributes a concrete foundation\ntoward developing small, interpretable models that explain temporal patterns in\nnatural language.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.07297", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07297", "abs": "https://arxiv.org/abs/2507.07297", "authors": ["Chengfei Wu", "Ronald Seoh", "Bingxuan Li", "Liqiang Zhang", "Fengrong Han", "Dan Goldwasser"], "title": "MagiC: Evaluating Multimodal Cognition Toward Grounded Visual Reasoning", "comment": null, "summary": "Recent advances in large vision-language models have led to impressive\nperformance in visual question answering and multimodal reasoning. However, it\nremains unclear whether these models genuinely perform grounded visual\nreasoning or rely on superficial patterns and dataset biases. In this work, we\nintroduce MagiC, a comprehensive benchmark designed to evaluate grounded\nmultimodal cognition, assessing not only answer accuracy but also the quality\nof step-by-step reasoning and its alignment with relevant visual evidence. Our\nbenchmark includes approximately 5,500 weakly supervised QA examples generated\nfrom strong model outputs and 900 human-curated examples with fine-grained\nannotations, including answers, rationales, and bounding box groundings. We\nevaluate 15 vision-language models ranging from 7B to 70B parameters across\nfour dimensions: final answer correctness, reasoning validity, grounding\nfidelity, and self-correction ability. MagiC further includes diagnostic\nsettings to probe model robustness under adversarial visual cues and assess\ntheir capacity for introspective error correction. We introduce new metrics\nsuch as MagiScore and StepSense, and provide comprehensive analyses that reveal\nkey limitations and opportunities in current approaches to grounded visual\nreasoning.", "AI": {"tldr": "研究者们发布了MagiC基准测试，用于评估视觉语言模型在视觉推理任务中的表现。", "motivation": "尽管大型视觉语言模型已经在视觉问题回答和多模态推理方面表现出色，但仍不清楚这些模型是否真正进行了基于视觉的推理，还是仅仅依赖于表面模式和数据集偏差。本研究旨在评估这些模型进行基于视觉的多模态认知的能力。", "method": "本研究介绍了一个名为MagiC的全面基准测试，旨在评估基于视觉的多模态认知。该基准不仅评估答案的准确性，还评估逐步推理的质量及其与相关视觉证据的一致性。基准包含大约5,500个弱监督QA样本和900个人类策划样本，这些样本具有细粒度的注释，包括答案、推理依据和边界框定位。", "result": "研究对15个参数量从7B到70B的视觉语言模型进行了评估，从最终答案的准确性、推理的有效性、定位的真实性和自我校正能力四个维度进行。此外，还包含了诊断设置来探测模型在对抗性视觉线索下的鲁棒性，并评估其内省错误纠正能力。提出了新的评估指标MagiScore和StepSense。", "conclusion": "全面的分析揭示了当前方法在基于视觉的推理方面的关键限制和机遇。本研究献不仅提供了一个评估基准，还引入了新的评估指标，为未来的研究指明了方向。"}}
{"id": "2507.07441", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07441", "abs": "https://arxiv.org/abs/2507.07441", "authors": ["Yu Xia", "Yiran Jenny Shen", "Junda Wu", "Tong Yu", "Sungchul Kim", "Ryan A. Rossi", "Lina Yao", "Julian McAuley"], "title": "SAND: Boosting LLM Agents with Self-Taught Action Deliberation", "comment": null, "summary": "Large Language Model (LLM) agents are commonly tuned with supervised\nfinetuning on ReAct-style expert trajectories or preference optimization over\npairwise rollouts. Most of these methods focus on imitating specific expert\nbehaviors or promoting chosen reasoning thoughts and actions over rejected\nones. However, without reasoning and comparing over alternatives actions, LLM\nagents finetuned with these methods may over-commit towards seemingly plausible\nbut suboptimal actions due to limited action space exploration. To address\nthis, in this paper we propose Self-taught ActioN Deliberation (SAND)\nframework, enabling LLM agents to explicitly deliberate over candidate actions\nbefore committing to one. To tackle the challenges of when and what to\ndeliberate given large action space and step-level action evaluation, we\nincorporate self-consistency action sampling and execution-guided action\ncritique to help synthesize step-wise action deliberation thoughts using the\nbase model of the LLM agent. In an iterative manner, the deliberation\ntrajectories are then used to finetune the LLM agent itself. Evaluating on two\nrepresentative interactive agent tasks, SAND achieves an average 20%\nimprovement over initial supervised finetuning and also outperforms\nstate-of-the-art agent tuning approaches.", "AI": {"tldr": "本文提出Self-taught ActioN Deliberation（SAND）框架，通过审议候选行动的方式来改进现有大型语言模型的行为优化方法，从而提高代理在两个代表性交互任务上的性能，平均改进20%。", "motivation": "现有的大型语言模型代理通常通过监督微调或成对回放的偏好优化来调优，这可能导致代理过度依赖某些看似合理但实际上次优的行为。本文提出了一种新的方法来解决这个问题。", "method": "SAND框架被提出，以解决在大型行动空间探索不足导致的选择次优行动的问题。该框架通过对候选行动进行明确的审议，避免了简单模仿专家行为的方法可能导致的过度承诺次优行动的问题。通过自我一致性行动采样和执行导向行动批评来辅助综合逐行动审议思维。这些审议轨迹在迭代过程中被用来微调LLM代理本身。", "result": "在两个代表性交互代理任务中，SAND方法比初始的监督微调提高了20%，并且超越了最新的代理微调方法。", "conclusion": "SAND框架提高了LLM代理在行动审议和选择上的能力，并且展示了在实际任务中的有效性和优越性。"}}
{"id": "2507.07317", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07317", "abs": "https://arxiv.org/abs/2507.07317", "authors": ["Sherry X. Chen", "Yi Wei", "Luowei Zhou", "Suren Kumar"], "title": "ADIEE: Automatic Dataset Creation and Scorer for Instruction-Guided Image Editing Evaluation", "comment": "International Conference on Computer Vision (ICCV) 2025", "summary": "Recent advances in instruction-guided image editing underscore the need for\neffective automated evaluation. While Vision-Language Models (VLMs) have been\nexplored as judges, open-source models struggle with alignment, and proprietary\nmodels lack transparency and cost efficiency. Additionally, no public training\ndatasets exist to fine-tune open-source VLMs, only small benchmarks with\ndiverse evaluation schemes. To address this, we introduce ADIEE, an automated\ndataset creation approach which is then used to train a scoring model for\ninstruction-guided image editing evaluation. We generate a large-scale dataset\nwith over 100K samples and use it to fine-tune a LLaVA-NeXT-8B model modified\nto decode a numeric score from a custom token. The resulting scorer outperforms\nall open-source VLMs and Gemini-Pro 1.5 across all benchmarks, achieving a\n0.0696 (+17.24%) gain in score correlation with human ratings on AURORA-Bench,\nand improving pair-wise comparison accuracy by 4.03% (+7.21%) on GenAI-Bench\nand 4.75% (+9.35%) on AURORA-Bench, respectively, compared to the\nstate-of-the-art. The scorer can act as a reward model, enabling automated best\nedit selection and model fine-tuning. Notably, the proposed scorer can boost\nMagicBrush model's average evaluation score on ImagenHub from 5.90 to 6.43\n(+8.98%).", "AI": {"tldr": "提出了ADIEE方法来构建一个大规模数据集，并基于该数据集训练了一种新的评分模型，用于指令引导图像编辑的自动评估，该模型的性能优于所有开源VLM和Gemini-Pro 1.5。", "motivation": "当前的方法在对指令引导的图像编辑进行自动评估时面临挑战，开源VLM模型在对齐方面表现不佳，而专有模型因为缺乏透明度和高昂的成本效率不足。此外，公开的训练数据集的缺乏限制了模型的使用和改进。因此，研究团队引入了ADIEE来应对这些挑战。", "method": "我们提出了一种名为ADIEE的自动数据集创建方法来解决现有的评估难题。该方法用于创建包含超过10万样本的大规模数据集，并利用这个数据集对LLaVA-NeXT-8B模型进行微调，使其能够解码来自自定义标记的数值评分。", "result": "训练出的新模型在与人类评分的相关性上提高了17.24%，在对MagicBrush模型的评估提升了8.98%。", "conclusion": "提出的评分模型可以作为一个奖励模型，用于自动选择最佳图像编辑和模型微调，展示了其在提高图像编辑评估准确性方面的潜力。"}}
{"id": "2507.07451", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07451", "abs": "https://arxiv.org/abs/2507.07451", "authors": ["Hongzhi Zhang", "Jia Fu", "Jingyuan Zhang", "Kai Fu", "Qi Wang", "Fuzheng Zhang", "Guorui Zhou"], "title": "RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning", "comment": "https://github.com/Kwai-Klear/RLEP", "summary": "Reinforcement learning (RL) for large language models is an energy-intensive\nendeavor: training can be unstable, and the policy may gradually drift away\nfrom its pretrained weights. We present \\emph{RLEP}\\, -- \\,Reinforcement\nLearning with Experience rePlay\\, -- \\,a two-phase framework that first\ncollects verified trajectories and then replays them during subsequent\ntraining. At every update step, the policy is optimized on mini-batches that\nblend newly generated rollouts with these replayed successes. By replaying\nhigh-quality examples, RLEP steers the model away from fruitless exploration,\nfocuses learning on promising reasoning paths, and delivers both faster\nconvergence and stronger final performance. On the Qwen2.5-Math-7B base model,\nRLEP reaches baseline peak accuracy with substantially fewer updates and\nultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,\non AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our\ncode, datasets, and checkpoints are publicly available at\nhttps://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further\nresearch.", "AI": {"tldr": "RLEP框架通过重放高质量轨迹，避免无果探索，专注于有望的推理路径，加速收敛并提升最终性能，在多个数学问题数据集上超越基线性能。", "motivation": "强化学习在大规模语言模型中的训练是能耗高的任务：训练可能不稳定，策略可能逐渐偏离其预训练权重。", "method": "提出了一种名为RLEP（基于经验重放的强化学习）的两阶段框架，该框架首先收集验证过的轨迹，然后在后续训练中重放这些经验。在每个更新步骤中，策略都在由新生成轨迹和重放的成功经验混合而成的小批量数据上进行优化。通过重放高质量样本，RLEP引导模型远离无果的探索，专注于有希望的推理路径，从而实现更快的收敛和更优的最终性能。", "result": "在Qwen2.5-Math-7B基础模型上，RLEP用更少的更新次数达到基线峰值准确率，并最终超越它，在AIME-2024、AIME-2025、AMC-2023数据集上的准确率分别提高到了39.9%、22.3%、82.2%。", "conclusion": "实验结果表明RLEP框架提高了在数学问题任务上的准确率，并显著减少了训练所需的更新次数，提供代码、数据集和检查点以促进可复制性和进一步研究。"}}
{"id": "2507.07333", "categories": ["cs.CV", "I.4.9"], "pdf": "https://arxiv.org/pdf/2507.07333", "abs": "https://arxiv.org/abs/2507.07333", "authors": ["Hui Pang", "Sunil Hadap", "Violetta Shevchenko", "Rahul Suresh", "Amin Banitalebi-Dehkordi"], "title": "Scalable and Realistic Virtual Try-on Application for Foundation Makeup with Kubelka-Munk Theory", "comment": "Presented at the workshop Three questions about virtual try-on at\n  CVPR 2025", "summary": "Augmented reality is revolutionizing beauty industry with virtual try-on\n(VTO) applications, which empowers users to try a wide variety of products\nusing their phones without the hassle of physically putting on real products. A\ncritical technical challenge in foundation VTO applications is the accurate\nsynthesis of foundation-skin tone color blending while maintaining the\nscalability of the method across diverse product ranges. In this work, we\npropose a novel method to approximate well-established Kubelka-Munk (KM) theory\nfor faster image synthesis while preserving foundation-skin tone color blending\nrealism. Additionally, we build a scalable end-to-end framework for realistic\nfoundation makeup VTO solely depending on the product information available on\ne-commerce sites. We validate our method using real-world makeup images,\ndemonstrating that our framework outperforms other techniques.", "AI": {"tldr": "研究提出了一种新的方法，通过近似Kubelka-Munk理论实现了快速且逼真的粉底试妆，构建了一个可扩展的端到端框架，并在实际应用中证明了其有效性和优势。", "motivation": "这项工作的动机在于解决美容行业中虚拟试妆应用的关键技术挑战，特别是准确地合成粉底与皮肤色调的融合，并保持该方法在不同产品范围内的可扩展性。", "method": "我们提出了一种新颖的方法来近似已建立的Kubelka-Munk理论，以实现更快速的图像合成，同时保留粉底与皮肤色调的逼真融合。此外，我们构建了一个仅依赖于电子商务网站上产品信息的可扩展端到端框架，用于实现逼真的粉底试妆应用。", "result": "我们使用现实生活中的彩妆图像验证了我们的方法，证明我们的框架优于其他技术。", "conclusion": "我们的研究为实现快速且逼真的粉底试妆虚拟应用提供了一个高效的解决方案。"}}
{"id": "2507.07484", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07484", "abs": "https://arxiv.org/abs/2507.07484", "authors": ["Kaiqu Liang", "Haimin Hu", "Xuandong Zhao", "Dawn Song", "Thomas L. Griffiths", "Jaime Fernández Fisac"], "title": "Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models", "comment": "Project page, code & data: https://machine-bullshit.github.io", "summary": "Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to\nstatements made without regard to their truth value. While previous work has\nexplored large language model (LLM) hallucination and sycophancy, we propose\nmachine bullshit as an overarching conceptual framework that can allow\nresearchers to characterize the broader phenomenon of emergent loss of\ntruthfulness in LLMs and shed light on its underlying mechanisms. We introduce\nthe Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and\npropose a complementary taxonomy analyzing four qualitative forms of bullshit:\nempty rhetoric, paltering, weasel words, and unverified claims. We conduct\nempirical evaluations on the Marketplace dataset, the Political Neutrality\ndataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI\nassistants) explicitly designed to evaluate machine bullshit. Our results\ndemonstrate that model fine-tuning with reinforcement learning from human\nfeedback (RLHF) significantly exacerbates bullshit and inference-time\nchain-of-thought (CoT) prompting notably amplify specific bullshit forms,\nparticularly empty rhetoric and paltering. We also observe prevalent machine\nbullshit in political contexts, with weasel words as the dominant strategy. Our\nfindings highlight systematic challenges in AI alignment and provide new\ninsights toward more truthful LLM behavior.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.07340", "categories": ["cs.CV", "I.2; I.4; I.5; I.7"], "pdf": "https://arxiv.org/pdf/2507.07340", "abs": "https://arxiv.org/abs/2507.07340", "authors": ["Daniel A. P. Oliveira", "David Martins de Matos"], "title": "Entity Re-identification in Visual Storytelling via Contrastive Reinforcement Learning", "comment": "7 pages", "summary": "Visual storytelling systems, particularly large vision-language models,\nstruggle to maintain character and object identity across frames,\n  often failing to recognize when entities in different images represent the\nsame individuals or objects,\n  leading to inconsistent references and referential hallucinations.\n  This occurs because models lack explicit training on when to establish entity\nconnections across frames.\n  We propose a contrastive reinforcement learning approach that trains models\nto discriminate between coherent image sequences\n  and stories from unrelated images.\n  We extend the Story Reasoning dataset with synthetic negative examples to\nteach appropriate entity connection behavior.\n  We employ Direct Preference Optimization with a dual-component reward\nfunction that promotes grounding and re-identification of entities\n  in real stories while penalizing incorrect entity connections in synthetic\ncontexts.\n  Using this contrastive framework, we fine-tune Qwen Storyteller (based on\nQwen2.5-VL 7B).\n  Evaluation shows improvements in grounding mAP from 0.27 to 0.31 (+14.8%), F1\nfrom 0.35 to 0.41 (+17.1%).\n  Pronoun grounding accuracy improved across all pronoun types except ``its'',\n  and cross-frame character and object persistence increased\n  across all frame counts, with entities appearing in 5 or more frames\nadvancing from 29.3% to 33.3% (+13.7%).\n  Well-structured stories, containing the chain-of-thought and grounded story,\nincreased from 79.1% to 97.5% (+23.3%).", "AI": {"tldr": "提出了对比强化学习方法，以改进视觉叙事模型在不同帧之间建立连贯实体连接的能力，通过微调Qwen Storyteller模型，显著提升了故事连贯性、目标检测和角色物体持续性。", "motivation": "视觉叙事系统，尤其是大规模视觉语言模型，难以在不同帧中保持角色和对象身份的一致性，导致参考不一致和参考幻象。这主要是因为模型缺乏在不同帧之间建立实体连接的明确训练。", "method": "使用对比强化学习方法训练模型，以区分连贯的图像序列和不相关的图像。通过扩展Story Reasoning数据集并引入合成负面样本，训练模型正确连接实体。采用直接偏好优化，使用双组件奖励函数，推广实体在真实故事中的定位和重新识别，并对合成上下文中不正确的实体连接进行惩罚。", "result": "通过对比框架微调Qwen Storyteller模型（基于Qwen2.5-VL 7B），评估结果显示：目标检测mAP从0.27提高到0.31，F1分数从0.35提升至0.41。除“its”之外，各类代词的地改善显著，跨帧角色和物体持续性在所有帧计数中增加，特别是在5帧以上的实体持续性从29.3%增加到33.3%。结构良好的故事比例从79.1%提升至97.5%。", "conclusion": "研究显示，对比强化学习方法能够有效提升视觉叙事模型在处理跨帧实体连接方面的能力，并且在多项评估指标上均有显著改善。这为未来的视觉叙事系统的实体一致性提供了新的解决方案。"}}
{"id": "2507.07495", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07495", "abs": "https://arxiv.org/abs/2507.07495", "authors": ["Mihir Parmar", "Palash Goyal", "Xin Liu", "Yiwen Song", "Mingyang Ling", "Chitta Baral", "Hamid Palangi", "Tomas Pfister"], "title": "PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving", "comment": "15 Pages", "summary": "Recently, decomposing complex problems into simple subtasks--a crucial part\nof human-like natural planning--to solve the given problem has significantly\nboosted the performance of large language models (LLMs). However, leveraging\nsuch planning structures during post-training to boost the performance of\nsmaller open-source LLMs remains underexplored. Motivated by this, we introduce\nPLAN-TUNING, a unified post-training framework that (i) distills synthetic task\ndecompositions (termed \"planning trajectories\") from large-scale LLMs and (ii)\nfine-tunes smaller models via supervised and reinforcement-learning objectives\ndesigned to mimic these planning processes to improve complex reasoning. On\nGSM8k and the MATH benchmarks, plan-tuned models outperform strong baselines by\nan average $\\sim7\\%$. Furthermore, plan-tuned models show better generalization\ncapabilities on out-of-domain datasets, with average $\\sim10\\%$ and $\\sim12\\%$\nperformance improvements on OlympiadBench and AIME 2024, respectively. Our\ndetailed analysis demonstrates how planning trajectories improves complex\nreasoning capabilities, showing that PLAN-TUNING is an effective strategy for\nimproving task-specific performance of smaller LLMs.", "AI": {"tldr": "PLAN-TUNING, a post-training method, fine-tunes smaller LLMs to better mimic planning strategies of larger models, thereby boosting reasoning performance and generalization capabilities.", "motivation": "The primary motivation is to apply the effective problem decomposition and planning strategies used by large LLMs to smaller LLMs to boost their performance and generalization.", "method": "PLAN-TUNING is a post-training framework that first distills planning trajectories from large-scale LLMs and then fine-tunes smaller models with these trajectories to enhance reasoning skills through supervised and reinforcement learning objectives.", "result": "The plan-tuned models showed improvements of ~7% on GSM8k and MATH benchmarks and even better generalization to out-of-domain data, with gains of ~10% on OlympiadBench and ~12% on AIME 2024.", "conclusion": "PLAN-TUNING is an effective approach for enhancing smaller LLMs' performance on specific tasks by incorporating the planning strategies of larger models."}}
{"id": "2507.07374", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07374", "abs": "https://arxiv.org/abs/2507.07374", "authors": ["Haotian Wang", "Aoran Xiao", "Xiaoqin Zhang", "Meng Yang", "Shijian Lu"], "title": "PacGDC: Label-Efficient Generalizable Depth Completion with Projection Ambiguity and Consistency", "comment": "Accepted to ICCV 2025", "summary": "Generalizable depth completion enables the acquisition of dense metric depth\nmaps for unseen environments, offering robust perception capabilities for\nvarious downstream tasks. However, training such models typically requires\nlarge-scale datasets with metric depth labels, which are often labor-intensive\nto collect. This paper presents PacGDC, a label-efficient technique that\nenhances data diversity with minimal annotation effort for generalizable depth\ncompletion. PacGDC builds on novel insights into inherent ambiguities and\nconsistencies in object shapes and positions during 2D-to-3D projection,\nallowing the synthesis of numerous pseudo geometries for the same visual scene.\nThis process greatly broadens available geometries by manipulating scene scales\nof the corresponding depth maps. To leverage this property, we propose a new\ndata synthesis pipeline that uses multiple depth foundation models as scale\nmanipulators. These models robustly provide pseudo depth labels with varied\nscene scales, affecting both local objects and global layouts, while ensuring\nprojection consistency that supports generalization. To further diversify\ngeometries, we incorporate interpolation and relocation strategies, as well as\nunlabeled images, extending the data coverage beyond the individual use of\nfoundation models. Extensive experiments show that PacGDC achieves remarkable\ngeneralizability across multiple benchmarks, excelling in diverse scene\nsemantics/scales and depth sparsity/patterns under both zero-shot and few-shot\nsettings. Code: https://github.com/Wang-xjtu/PacGDC.", "AI": {"tldr": "提出PacGDC技术，通过合成几何体的多尺度版本来改进深度完成的泛化能力，减少了收集训练所需的大规模度量深度标签数据集的负担。", "motivation": "目的是通过提出PacGDC技术，以极小的注释工作量增强数据多样性，解决训练广义深度完成模型所需的具有度量深度标签的大规模数据集的收集问题。", "method": "PacGDC技术利用对2D到3D投影中物体形状和位置固有歧义和一致性的新见解来合成同一视觉场景的多个伪几何体，扩大深度图的尺度和多样性。采用多个深度基础模型作为尺度调节器，并结合插值、重定位策略和无标签图像以进一步提高几何多样性。", "result": "实验表明，PacGDC在多个基准上表现出显著的泛化能力，在各种场景语义、尺度和深度稀疏模式下表现优异，无论是零样本还是少样本设置下均出众。", "conclusion": "PacGDC技术有效解决了广义深度完成任务中注释工作量大、数据多样性不足的问题，显示了其在不同条件下的泛化能力。"}}
{"id": "2507.07498", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07498", "abs": "https://arxiv.org/abs/2507.07498", "authors": ["Keqin Bao", "Nuo Chen", "Xiaoyuan Li", "Binyuan Hui", "Bowen Yu", "Fuli Feng", "Junyang Lin", "Xiangnan He", "Dayiheng Liu"], "title": "Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems without Code", "comment": null, "summary": "Enhancing reasoning capabilities remains a central focus in the LLM reasearch\ncommunity. A promising direction involves requiring models to simulate code\nexecution step-by-step to derive outputs for given inputs. However, as code is\noften designed for large-scale systems, direct application leads to\nover-reliance on complex data structures and algorithms, even for simple cases,\nresulting in overfitting to algorithmic patterns rather than core reasoning\nstructures. To address this, we propose TeaR, which aims at teaching LLMs to\nreason better. TeaR leverages careful data curation and reinforcement learning\nto guide models in discovering optimal reasoning paths through code-related\ntasks, thereby improving general reasoning abilities. We conduct extensive\nexperiments using two base models and three long-CoT distillation models, with\nmodel sizes ranging from 1.5 billion to 32 billion parameters, and across 17\nbenchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results\nconsistently show significant performance improvements. Notably, TeaR achieves\na 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.", "AI": {"tldr": "TeaR improves LLM reasoning capabilities by teaching models to better navigate code execution and derive outputs, resulting in performance gains across numerous benchmarks.", "motivation": "The motivation is to improve the reasoning capabilities of large language models by avoiding overfitting to complex algorithms and focusing on core reasoning structures.", "method": "TeaR uses careful data curation and reinforcement learning to guide LLMs in discovering optimal reasoning paths through code-related tasks.", "result": "The results show significant performance improvements across various benchmarks and model sizes, with a notable 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.", "conclusion": "TeaR effectively enhances the reasoning skills of LLMs through its methodological approach to data curation and learning, creating a more adaptable and capable model for various reasoning tasks."}}
{"id": "2507.07379", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07379", "abs": "https://arxiv.org/abs/2507.07379", "authors": ["Hong Xu", "Shireen Y. Elhabian"], "title": "Adaptive Particle-Based Shape Modeling for Anatomical Surface Correspondence", "comment": null, "summary": "Particle-based shape modeling (PSM) is a family of approaches that\nautomatically quantifies shape variability across anatomical cohorts by\npositioning particles (pseudo landmarks) on shape surfaces in a consistent\nconfiguration. Recent advances incorporate implicit radial basis function\nrepresentations as self-supervised signals to better capture the complex\ngeometric properties of anatomical structures. However, these methods still\nlack self-adaptivity -- that is, the ability to automatically adjust particle\nconfigurations to local geometric features of each surface, which is essential\nfor accurately representing complex anatomical variability. This paper\nintroduces two mechanisms to increase surface adaptivity while maintaining\nconsistent particle configurations: (1) a novel neighborhood correspondence\nloss to enable high adaptivity and (2) a geodesic correspondence algorithm that\nregularizes optimization to enforce geodesic neighborhood consistency. We\nevaluate the efficacy and scalability of our approach on challenging datasets,\nproviding a detailed analysis of the adaptivity-correspondence trade-off and\nbenchmarking against existing methods on surface representation accuracy and\ncorrespondence metrics.", "AI": {"tldr": "The paper presents a novel approach to improve adaptivity in particle-based shape modeling by introducing a new neighborhood correspondence loss and a geodesic correspondence algorithm, which enhance the model's ability to accurately represent complex anatomical structures.", "motivation": "The motivation behind this paper is to address the lack of self-adaptivity in particle-based shape modeling for anatomical structures, aiming to more accurately represent complex anatomical variability.", "method": "This paper introduces two mechanisms to enhance adaptivity in PSM: a new neighborhood correspondence loss and a geodesic correspondence algorithm. These improvements aim to better adjust particle configurations to local geometric features without losing consistency.", "result": "The approach is evaluated on challenging datasets and shows improved adaptivity while maintaining surface representation accuracy and correspondence metrics compared to existing methods.", "conclusion": "The results suggest that the proposed methods offer a balance in adaptivity and consistency, making them suitable for representing complex anatomical variability in shape modeling."}}
{"id": "2507.07499", "categories": ["cs.CL", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2507.07499", "abs": "https://arxiv.org/abs/2507.07499", "authors": ["Hein Htet", "Amgad Ahmed Ali Ibrahim", "Yutaka Sasaki", "Ryoji Asahi"], "title": "Extracting ORR Catalyst Information for Fuel Cell from Scientific Literature", "comment": "28 pages, 12 figures, 6 tables", "summary": "The oxygen reduction reaction (ORR) catalyst plays a critical role in\nenhancing fuel cell efficiency, making it a key focus in material science\nresearch. However, extracting structured information about ORR catalysts from\nvast scientific literature remains a significant challenge due to the\ncomplexity and diversity of textual data. In this study, we propose a named\nentity recognition (NER) and relation extraction (RE) approach using DyGIE++\nwith multiple pre-trained BERT variants, including MatSciBERT and PubMedBERT,\nto extract ORR catalyst-related information from the scientific literature,\nwhich is compiled into a fuel cell corpus for materials informatics\n(FC-CoMIcs). A comprehensive dataset was constructed manually by identifying 12\ncritical entities and two relationship types between pairs of the entities. Our\nmethodology involves data annotation, integration, and fine-tuning of\ntransformer-based models to enhance information extraction accuracy. We assess\nthe impact of different BERT variants on extraction performance and investigate\nthe effects of annotation consistency. Experimental evaluations demonstrate\nthat the fine-tuned PubMedBERT model achieves the highest NER F1-score of\n82.19% and the MatSciBERT model attains the best RE F1-score of 66.10%.\nFurthermore, the comparison with human annotators highlights the reliability of\nfine-tuned models for ORR catalyst extraction, demonstrating their potential\nfor scalable and automated literature analysis. The results indicate that\ndomain-specific BERT models outperform general scientific models like BlueBERT\nfor ORR catalyst extraction.", "AI": {"tldr": "本研究提出使用DyGIE++结合多种预训练的BERT变体，包括MatSciBERT和PubMedBERT，从科学文献中提取与氧还原反应（ORR）催化剂相关的信息。实验表明，经过微调的PubMedBERT模型在命名实体识别（NER）中达到最高的F1值82.19%，而MatSciBERT在关系抽取（RE）中表现最佳，F1值为66.10%。这表明领域特定的BERT模型在ORR催化剂提取方面优于通用的科学模型。", "motivation": "ORR催化剂对于提高燃料电池效率至关重要，而如何从大量科学文献中结构化提取ORR催化剂信息是一个重大挑战。此研究旨在解决这一问题。", "method": "研究中提出了一种使用DyGIE++结合多种预训练BERT变体（包括MatSciBERT和PubMedBERT）的方法，手动构建了包含12个关键实体和两种关系类型的综合性数据集，通过数据标注、集成和模型微调来提高信息提取准确性。", "result": "实验显示，微调后的PubMedBERT模型在NER中获得了82.19%的最高F1值，MatSciBERT则在RE中得到最佳的66.10% F1值，与人工注释比较表明，微调后的模型具有可靠的ORR催化剂提取能力。", "conclusion": "研究结果表明，领域特定的BERT模型（如MatSciBERT和PubMedBERT）在ORR催化剂信息提取方面优于通用科学模型，预示着它们在自动化文献分析中的巨大潜力。"}}
{"id": "2507.07381", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07381", "abs": "https://arxiv.org/abs/2507.07381", "authors": ["Hao Xu", "Arbind Agrahari Baniya", "Sam Wells", "Mohamed Reda Bouadjenek", "Richard Dazeley", "Sunil Aryal"], "title": "Multi-Scale Attention and Gated Shifting for Fine-Grained Event Spotting in Videos", "comment": null, "summary": "Precise Event Spotting (PES) in sports videos requires frame-level\nrecognition of fine-grained actions from single-camera footage. Existing PES\nmodels typically incorporate lightweight temporal modules such as Gate Shift\nModule (GSM) or Gate Shift Fuse (GSF) to enrich 2D CNN feature extractors with\ntemporal context. However, these modules are limited in both temporal receptive\nfield and spatial adaptability. We propose a Multi-Scale Attention Gate Shift\nModule (MSAGSM) that enhances GSM with multi-scale temporal dilations and\nmulti-head spatial attention, enabling efficient modeling of both short- and\nlong-term dependencies while focusing on salient regions. MSAGSM is a\nlightweight plug-and-play module that can be easily integrated with various 2D\nbackbones. To further advance the field, we introduce the Table Tennis\nAustralia (TTA) dataset-the first PES benchmark for table tennis-containing\nover 4800 precisely annotated events. Extensive experiments across five PES\nbenchmarks demonstrate that MSAGSM consistently improves performance with\nminimal overhead, setting new state-of-the-art results.", "AI": {"tldr": "为了改进现有精确事件检测模型的时间和空间局限性，作者提出了多尺度注意门移模块（MSAGSM），并在多个基准测试中验证了其优越性能。", "motivation": "现有精确事件检测模型在处理视频中精细动作识别时的能力有限，尤其是在时间感受野和空间适应性方面。为了改进这些局限，作者提出了新的模块MSAGSM。", "method": "我们提出了一种多尺度注意门移模块（MSAGSM），该模块通过引入多尺度时间扩张和多头空间注意力机制增强了传统的Gate Shift Module（GSM），以更高效地建模短期和长期依赖关系并聚焦于显著区域。MSAGSM是一个轻量级的模块，可以轻松集成到各种二维主干网络中。", "result": "通过在五个精确事件检测基准上进行的广泛实验表明，MSAGSM在性能上实现了一致的提升，同时保持了计算上的效率，并设定了新的行业标准结果。", "conclusion": "我们提出了一个轻量级插件式模块MSAGSM，该模块通过引入多尺度时间扩张和多头空间注意力机制，提高了GSM的能力。该模块在多个基准上验证了其有效性和性能提升，并在乒乓球澳大利亚数据集等数据集上达到了新的SOTA结果。"}}
