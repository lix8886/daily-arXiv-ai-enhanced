<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 28]
- [cs.CV](#cs.CV) [Total: 24]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LLM-as-a-qualitative-judge: automating error analysis in natural language generation](https://arxiv.org/abs/2506.09147)
*Nadezhda Chirkova,Tunde Oluwaseyi Ajayi,Seth Aycock,Zain Muhammad Mujahid,Vladana Perlić,Ekaterina Borisova,Markarit Vartampetian*

Main category: cs.CL

TL;DR: 本文提出LLM-as-a-qualitative-judge，一个基于LLM的评估方法，其主要输出是对NLG系统输出中的常见问题类型的结构化报告，并展示了该方法识别特定实例问题的情况。


<details>
  <summary>Details</summary>
Motivation: 当前，将LLM用于评估生成文本的方法主要是定量工具，主要输出是数值分数。本文旨在提出一种定性评估方法，为目标提供有关如何改进给定的NLG系统的有意义见解。

Method: 提出的方法包括两个主要步骤：开放式逐实例问题分析和使用直观的累积算法对发现的问题进行聚类。

Result: 实验结果显示，LLM-as-a-qualitative-judge在2/3的情况下正确地识别了实例特定的问题，并且能够产生类似于人类标注员编制的错误类型报告。

Conclusion: 研究证明了LLM除了作为定量评估工具外，也可以作为定性评估工具，以提供结构化的问题报告，进而帮助开发者理解和改进NLG系统。

Abstract: Prompting large language models (LLMs) to evaluate generated text, known as
LLM-as-a-judge, has become a standard evaluation approach in natural language
generation (NLG), but is primarily used as a quantitative tool, i.e. with
numerical scores as main outputs. In this work, we propose
LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main
output being a structured report of common issue types in the NLG system
outputs. Our approach is targeted at providing developers with meaningful
insights on what improvements can be done to a given NLG system and consists of
two main steps, namely open-ended per-instance issue analysis and clustering of
the discovered issues using an intuitive cumulative algorithm. We also
introduce a strategy for evaluating the proposed approach, coupled with ~300
annotations of issues in instances from 12 NLG datasets. Our results show that
LLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3
cases and is capable of producing error type reports resembling the reports
composed by human annotators. Our code and data are publicly available at
https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.

</details>


### [2] [PHRASED: Phrase Dictionary Biasing for Speech Translation](https://arxiv.org/abs/2506.09175)
*Peidong Wang,Jian Xue,Rui Zhao,Junkun Chen,Aswin Shanmugam Subramanian,Jinyu Li*

Main category: cs.CL

TL;DR: 本文提出了一种短语字典偏置方法，提高了短语在语音翻译中的准确性和召回率，相较于短语列表偏置方法，提升了21%的相对性能，并使多模态大型语言模型的短语召回率提高了85%。


<details>
  <summary>Details</summary>
Motivation: 短语对于理解对话中的核心概念至关重要，但由于其在训练数据中的罕见出现，短语的准确翻译在语音翻译任务中颇具挑战。

Method: 本文提出了一种短语字典偏置方法，以利用从源语言到目标语言的短语映射对。该方法应用到了两种广泛使用的模型中：基于转导器的流式语音翻译模型和多模态大型语言模型。

Result: 实验结果显示，短语字典偏置方法比短语列表偏置方法相对提高了21%的性能。此外，短语字典偏置使多模态大型语言模型能够使用外部短语信息，实现了85%的相对召回率提升。

Conclusion: 短语字典偏置方法显著提升了语音翻译模型和多模态大型语言模型在短语翻译上的性能。

Abstract: Phrases are essential to understand the core concepts in conversations.
However, due to their rare occurrence in training data, correct translation of
phrases is challenging in speech translation tasks. In this paper, we propose a
phrase dictionary biasing method to leverage pairs of phrases mapping from the
source language to the target language. We apply the phrase dictionary biasing
method to two types of widely adopted models, a transducer-based streaming
speech translation model and a multimodal large language model. Experimental
results show that the phrase dictionary biasing method outperforms phrase list
biasing by 21% relatively for the streaming speech translation model. In
addition, phrase dictionary biasing enables multimodal large language models to
use external phrase information, achieving 85% relative improvement in phrase
recall.

</details>


### [3] [A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs](https://arxiv.org/abs/2506.09218)
*Bruno Ferenc Šegedin*

Main category: cs.CL

TL;DR: A study examining the capacity of CNNs to generate phonotactic generalizations from raw audio data finds that, with a narrow fully-connected layer, convolutional layers can dynamically generalize phonetic dependencies beyond the specific learned lexical configurations.


<details>
  <summary>Details</summary>
Motivation: The motivation of this study is to understand if DNNs, particularly CNNs trained on audio waveforms, can make generalizations about phonotactics that go beyond the specific instances of words they've been trained on, and to examine how the architecture of the neural network influences this ability.

Method: This study uses generative convolutional neural networks (CNNs) trained on raw audio waveforms of lexical items. It explores the impact of a narrow fully-connected layer (FC) bottleneck (shrunk from 1024 channels to 8) on the model's ability to generalize phonotactic rules derived from lexical learning.

Result: The study finds that the convolutional layers can dynamically generalize phonetic dependencies beyond lexically-constrained configurations. A novel technique for probing the model's lexically-independent generalizations was proposed, which bypasses the FC layer and uses randomized feature maps fed into the convolutional block to generate audio outputs.

Conclusion: The results suggest that generative convolutional neural networks are capable of learning and applying phonotactic generalizations beyond the specific lexical items they are trained on, especially when the fully-connected layer is constrained.

Abstract: The ability of deep neural networks (DNNs) to represent phonotactic
generalizations derived from lexical learning remains an open question. This
study (1) investigates the lexically-invariant generalization capacity of
generative convolutional neural networks (CNNs) trained on raw audio waveforms
of lexical items and (2) explores the consequences of shrinking the
fully-connected layer (FC) bottleneck from 1024 channels to 8 before training.
Ultimately, a novel technique for probing a model's lexically-independent
generalizations is proposed that works only under the narrow FC bottleneck:
generating audio outputs by bypassing the FC and inputting randomized feature
maps into the convolutional block. These outputs are equally biased by a
phonotactic restriction in training as are outputs generated with the FC. This
result shows that the convolutional layers can dynamically generalize phonetic
dependencies beyond lexically-constrained configurations learned by the FC.

</details>


### [4] [Extrapolation by Association: Length Generalization Transfer in Transformers](https://arxiv.org/abs/2506.09251)
*Ziyang Cai,Nayoung Lee,Avi Schwarzschild,Samet Oymak,Dimitris Papailiopoulos*

Main category: cs.CL

TL;DR: 研究通过任务关联展示了Transformer模型的长度泛化能力可以跨任务转移，并提供了机制上的初步证据。


<details>
  <summary>Details</summary>
Motivation: 探讨Transformer语言模型处理更长输入的能力是如何形成的，特别是在未见过的更长输入上如何泛化的问题。

Method: 通过任务关联的视角研究了模型处理更长输入的能力，即长度泛化能力。研究表明，训练模型时使用一个较长且相关的辅助任务，可以使模型在其他目标任务中更好地泛化到未见过的更长输入。

Result: 长度泛化能力能够在不同的算法任务之间转移，包括算术运算、字符串转换和迷宫导航任务。预训练语言模型中也观察到了类似的转移效果。

Conclusion: 模型可以继承来自相似任务的泛化能力，并且长度泛化的转移与任务间注意力头的重用有关，这加深了对模型如何泛化到分布外输入的理解。

Abstract: Transformer language models have demonstrated impressive generalization
capabilities in natural language domains, yet we lack a fine-grained
understanding of how such generalization arises. In this paper, we investigate
length generalization--the ability to extrapolate from shorter to longer
inputs--through the lens of \textit{task association}. We find that length
generalization can be \textit{transferred} across related tasks. That is,
training a model with a longer and related auxiliary task can lead it to
generalize to unseen and longer inputs from some other target task. We
demonstrate this length generalization transfer across diverse algorithmic
tasks, including arithmetic operations, string transformations, and maze
navigation. Our results show that transformer models can inherit generalization
capabilities from similar tasks when trained jointly. Moreover, we observe
similar transfer effects in pretrained language models, suggesting that
pretraining equips models with reusable computational scaffolding that
facilitates extrapolation in downstream settings. Finally, we provide initial
mechanistic evidence that length generalization transfer correlates with the
re-use of the same attention heads between the tasks. Together, our findings
deepen our understanding of how transformers generalize to out-of-distribution
inputs and highlight the compositional reuse of inductive structure across
tasks.

</details>


### [5] [Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat](https://arxiv.org/abs/2506.09259)
*Zhuofang Li,Rafal Kocielnik,Fereshteh Soltani,Penphob,Boonyarungsrit,Animashree Anandkumar,R. Michael Alvarez*

Main category: cs.CL

TL;DR: 研究提出了自我锚定注意力模型（SAAM）来识别并分类游戏中亲社会行为，并将其应用于《使命召唤：现代战争II》中，提高了亲社会沟通行为的识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要集中在使用自然语言处理技术检测游戏中少量的有毒内容以实现管理目的，而新兴的研究强调了检测亲社会行为的重要性。然而，在游戏聊天文本中识别亲社会行为的数据集、模型和资源相当匮乏。研究旨在解决这一低资源环境下的问题，促进积极互动的鼓励。

Method: 采用了无监督的发现方法，并结合游戏领域专家的合作，从游戏中玩家的聊天记录中识别和分类亲社会行为。提出了自我锚定注意力模型（Self-Anchored Attention Model，SAAM），该模型相较于现有最佳技术提高了7.9%。通过使用整个训练集作为“锚点”来帮助在缺乏大量训练数据的情况下改善模型性能。

Result: 成果包括提出了自我锚定注意力模型（SAAM），相较于现有最佳技术提高了7.9%，在低资源环境中实现了亲社会行为的自动分类。研究展示了在仅有少量化标注数据的情况下，模型仍然有效。

Conclusion: 这篇研究是首次将NLP技术应用于发现和分类玩家在游戏中的亲社会沟通行为的研究，证明了从仅关注减少消极内容到主动鼓励积极互动的可能性。

Abstract: Millions of players engage daily in competitive online games, communicating
through in-game chat. Prior research has focused on detecting relatively small
volumes of toxic content using various Natural Language Processing (NLP)
techniques for the purpose of moderation. However, recent studies emphasize the
importance of detecting prosocial communication, which can be as crucial as
identifying toxic interactions. Recognizing prosocial behavior allows for its
analysis, rewarding, and promotion. Unlike toxicity, there are limited
datasets, models, and resources for identifying prosocial behaviors in
game-chat text. In this work, we employed unsupervised discovery combined with
game domain expert collaboration to identify and categorize prosocial player
behaviors from game chat. We further propose a novel Self-Anchored Attention
Model (SAAM) which gives 7.9% improvement compared to the best existing
technique. The approach utilizes the entire training set as "anchors" to help
improve model performance under the scarcity of training data. This approach
led to the development of the first automated system for classifying prosocial
behaviors in in-game chats, particularly given the low-resource settings where
large-scale labeled data is not available. Our methodology was applied to one
of the most popular online gaming titles - Call of Duty(R): Modern
Warfare(R)II, showcasing its effectiveness. This research is novel in applying
NLP techniques to discover and classify prosocial behaviors in player in-game
chat communication. It can help shift the focus of moderation from solely
penalizing toxicity to actively encouraging positive interactions on online
platforms.

</details>


### [6] [Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models](https://arxiv.org/abs/2506.09277)
*Milan Bhan,Jean-Noel Vittaut,Nicolas Chesneau,Sarath Chandar,Marie-Jeanne Lesot*

Main category: cs.CL

TL;DR: The paper develops a framework to measure the faithfulness of LLM self-explanations by comparing them to the model's internal processes, aiming to enhance the reliability of these explanations and foster future improvements.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the fact that current LLM-generated self-NLEs can be logically coherent but not truly reflective of the model's actual decision-making process, leading to unfaithful explanations. This study aims to fill the gap in understanding and improving the faithfulness of these explanations.

Method: This paper proposes a flexible framework to measure the faithfulness of LLM-generated self-Natural Language Explanations (self-NLE) by comparing self-NLE with the interpretations of the model's internal neural activities, thereby offering deep insights into the connection between self-NLE and the model's reasoning process.

Result: The paper establishes a novel approach for quantifying self-NLE faithfulness which connects the explanations to the model's internal reasoning, advancing the understanding of self-NLE authenticity and laying groundwork for the generation of more reliable self-NLE in the future.

Conclusion: The work concludes with a robust framework for assessing self-NLE faithfulness in LLMs by linking the generated explanations to the model's internal neural states, paving the way for more faithful self-NLEs in the future.

Abstract: Large Language Models (LLM) have demonstrated the capability of generating
free text self Natural Language Explanation (self-NLE) to justify their
answers. Despite their logical appearance, self-NLE do not necessarily reflect
the LLM actual decision-making process, making such explanations unfaithful.
While existing methods for measuring self-NLE faithfulness mostly rely on
behavioral tests or computational block identification, none of them examines
the neural activity underlying the model's reasoning. This work introduces a
novel flexible framework for quantitatively measuring the faithfulness of
LLM-generated self-NLE by directly comparing the latter with interpretations of
the model's internal hidden states. The proposed framework is versatile and
provides deep insights into self-NLE faithfulness by establishing a direct
connection between self-NLE and model reasoning. This approach advances the
understanding of self-NLE faithfulness and provides building blocks for
generating more faithful self-NLE.

</details>


### [7] [$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding](https://arxiv.org/abs/2506.09301)
*Cesare Spinoso-Di Piano,David Austin,Pablo Piantanida,Jackie Chi Kit Cheung*

Main category: cs.CL

TL;DR: 本文提出了$(RSA)^2$框架，通过考虑说话人的修辞策略来解释比喻语言，避免了对动机的特定场景化建模，并在讽刺语言理解上达到了先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有的RSA框架无法解释比喻表达，或者需要以特定场景的方式模拟说话人使用比喻语言的隐含动机。为了人类兼容地解释非字面语言，本文提出了$(RSA)^2$框架。

Method: 本文介绍了$(RSA)^2$框架，它通过考虑说话人使用的修辞策略来建模比喻语言的使用。

Result: $(RSA)^2$框架与大语言模型结合，在新引入的PragMega+数据集的讽刺分割上实现了最先进的性能。

Conclusion: 该研究展示了$(RSA)^2$框架在理解非字面语言方面的优势，无需模拟说话人使用非字面语言的具体动机，即可实现人类兼容的解释。

Abstract: Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in
human communication, resulting in utterances where the literal and the intended
meanings do not match. The Rational Speech Act (RSA) framework, which
explicitly models speaker intentions, is the most widespread theory of
probabilistic pragmatics, but existing implementations are either unable to
account for figurative expressions or require modeling the implicit motivations
for using figurative language (e.g., to express joy or annoyance) in a
setting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware
RSA $(RSA)^2$ framework which models figurative language use by considering a
speaker's employed rhetorical strategy. We show that $(RSA)^2$ enables
human-compatible interpretations of non-literal utterances without modeling a
speaker's motivations for being non-literal. Combined with LLMs, it achieves
state-of-the-art performance on the ironic split of PragMega+, a new irony
interpretation dataset introduced in this study.

</details>


### [8] [Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models](https://arxiv.org/abs/2506.09315)
*Yao Xiao,Heidi Christensen,Stefan Goetze*

Main category: cs.CL

TL;DR: 本研究使用Mistral-7B模型增强了阿尔茨海默病的语言检测能力，提高了检测准确性，并展示了清晰的决策边界。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病是一种影响认知能力的神经退行性疾病，通常会影响语言能力。这项研究的目的是提高阿尔茨海默病的检测准确性。

Method: 本研究使用了Mistral-7B指令跟随版大型语言模型，扩展了检测阿尔茨海默病的成对困惑度方法。

Result: 相比于当前最佳成对困惑度方法和ADReSS 2020挑战基准测试中表现最好的方法，准确率分别提高了3.33%和6.35%。

Conclusion: 提出的方法可以有效检测阿尔茨海默病，并且具有清晰且可解释的决策边界，模型已经学会了阿尔茨海默病患者特有的语言模式，为模型解释和数据增强提供了新的方法。

Abstract: Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive
decline that commonly impacts language ability. This work extends the paired
perplexity approach to detecting AD by using a recent large language model
(LLM), the instruction-following version of Mistral-7B. We improve accuracy by
an average of 3.33% over the best current paired perplexity method and by 6.35%
over the top-ranked method from the ADReSS 2020 challenge benchmark. Our
further analysis demonstrates that the proposed approach can effectively detect
AD with a clear and interpretable decision boundary in contrast to other
methods that suffer from opaque decision-making processes. Finally, by
prompting the fine-tuned LLMs and comparing the model-generated responses to
human responses, we illustrate that the LLMs have learned the special language
patterns of AD speakers, which opens up possibilities for novel methods of
model interpretation and data augmentation.

</details>


### [9] [Towards Efficient and Effective Alignment of Large Language Models](https://arxiv.org/abs/2506.09329)
*Yuxin Jiang*

Main category: cs.CL

TL;DR: The paper presents new methodologies for data collection, training, and evaluation to improve LLM alignment with human expectations, introducing frameworks like Lion, WebR, LTE, and BMC and the FollowBench for constraint adherence.


<details>
  <summary>Details</summary>
Motivation: To efficiently and effectively align LLMs with human expectations.

Method: Lion, an adversarial distillation framework for alignment data collection; Web Reconstruction (WebR), a fully automated framework for synthesizing instruction-tuning data; Learning to Edit (LTE), a framework for efficient knowledge integration; Bridging and Modeling Correlations (BMC), a refinement of Direct Preference Optimization (DPO) for better alignment across QA and mathematical reasoning tasks.

Result: State-of-the-art zero-shot reasoning with Lion; improved data diversity and scalability with WebR; improved real-time and batch knowledge updates with LTE; superior alignment across QA and mathematical reasoning tasks with BMC; key weaknesses in current models' constraint adherence identified with FollowBench.

Conclusion: The paper proposes and evaluates several novel methodologies in data collection, training, and evaluation for LLM alignment, significantly advancing the state-of-the-art in zero-shot reasoning, knowledge integration, and constraint adherence.

Abstract: Large language models (LLMs) exhibit remarkable capabilities across diverse
tasks, yet aligning them efficiently and effectively with human expectations
remains a critical challenge. This thesis advances LLM alignment by introducing
novel methodologies in data collection, training, and evaluation. We first
address alignment data collection. Existing approaches rely heavily on manually
curated datasets or proprietary models. To overcome these limitations, we
propose Lion, an adversarial distillation framework that iteratively refines
training data by identifying and generating challenging instructions, enabling
state-of-the-art zero-shot reasoning. Additionally, we introduce Web
Reconstruction (WebR), a fully automated framework that synthesizes
instruction-tuning data directly from raw web documents, significantly
improving data diversity and scalability over existing synthetic data methods.
Next, we enhance alignment training through novel optimization techniques. We
develop Learning to Edit (LTE), a framework that enables LLMs to efficiently
integrate new knowledge while preserving existing information. LTE leverages
meta-learning to improve both real-time and batch knowledge updates.
Furthermore, we introduce Bridging and Modeling Correlations (BMC), a
refinement of Direct Preference Optimization (DPO) that explicitly captures
token-level correlations in preference data, leading to superior alignment
across QA and mathematical reasoning tasks. Finally, we tackle the challenge of
evaluating alignment. Existing benchmarks emphasize response quality but
overlook adherence to specific constraints. To bridge this gap, we introduce
FollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to
follow complex constraints across diverse instruction types. Our results expose
key weaknesses in current models' constraint adherence, offering insights for
future improvements.

</details>


### [10] [Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation](https://arxiv.org/abs/2506.09331)
*Arjun Vaithilingam Sudhakar*

Main category: cs.CL

TL;DR: 研究LLMs是否拥有理论心智，通过在其基础上创建能够自然语言互动的智能体，以实现在多智能体环境下的人工智能和人类的良好协作。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否能模拟和推理他人的意图，即LLMs是否具有某种形式的理论心智。这在人与人以及人与自主系统间的有效协作中至关重要。

Method: 通过多智能体强化学习（MARL）的视角研究LLMs的理论心智，该视角中智能体通过重复互动学习协作，类似于人类的社会推理。

Result: 未直接给出研究成果，但该项研究旨在提升人工智能智能体适应并与其他智能体及人类合作的能力。

Conclusion: 通过基于LLM的智能体来创建人机混合系统，这些系统有朝一日能够实现无缝协作，对人类与人工智能的互动未来具有广泛的意义。

Abstract: Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot
generalization capabilities across complex natural language tasks, enabling
their widespread use as virtual assistants for diverse applications such as
translation and summarization. Despite being trained solely on large corpora of
text without explicit supervision on author intent, LLMs appear to infer the
underlying meaning of textual interactions. This raises a fundamental question:
can LLMs model and reason about the intentions of others, i.e., do they possess
a form of theory of mind? Understanding other's intentions is crucial for
effective collaboration, which underpins human societal success and is
essential for cooperative interactions among multiple agents, including humans
and autonomous systems. In this work, we investigate the theory of mind in LLMs
through the lens of cooperative multi-agent reinforcement learning (MARL),
where agents learn to collaborate via repeated interactions, mirroring human
social reasoning. Our approach aims to enhance artificial agent's ability to
adapt and cooperate with both artificial and human partners. By leveraging
LLM-based agents capable of natural language interaction, we move towards
creating hybrid human-AI systems that can foster seamless collaboration, with
broad implications for the future of human-artificial interaction.

</details>


### [11] [RePO: Replay-Enhanced Policy Optimization](https://arxiv.org/abs/2506.09340)
*Siheng Li,Zhanhui Zhou,Wai Lam,Chao Yang,Chaochao Lu*

Main category: cs.CL

TL;DR: This paper presents Replay-Enhanced Policy Optimization (RePO), a method that improves upon GRPO in terms of data efficiency and computational cost while optimizing large language models for reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to find a more efficient method in terms of computational costs and data usage for optimizing LLMs using reinforcement learning, compared to the existing Group Relative Policy Optimization (GRPO) method.

Method: Replay-Enhanced Policy Optimization (RePO) is introduced to improve data efficiency and reduce computational costs when optimizing large language models (LLMs) for reinforcement learning tasks. It retrieves off-policy samples from a replay buffer to enrich the dataset used for policy optimization.

Result: Experimental results show that RePO achieves significant performance gains over GRPO, with absolute average performance improvements of 18.4 and 4.1 points for Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively. Additionally, for Qwen3-1.7B, the number of effective optimization steps rose by 48% while increasing computational cost by only 15%.

Conclusion: The conclusion is that RePO increases the data efficiency and reduces the computational cost while optimizing large language models for reinforcement learning tasks. It provides significant performance gains with a relatively small increase in computational costs.

Abstract: Reinforcement learning (RL) is vital for optimizing large language models
(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages
using multiple on-policy outputs per prompt, leading to high computational
costs and low data efficiency. To address this, we introduce Replay-Enhanced
Policy Optimization (RePO), which leverages diverse replay strategies to
retrieve off-policy samples from a replay buffer, allowing policy optimization
based on a broader and more diverse set of samples for each prompt. Experiments
on five LLMs across seven mathematical reasoning benchmarks demonstrate that
RePO achieves absolute average performance gains of $18.4$ and $4.1$ points for
Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further
analysis indicates that RePO increases computational cost by $15\%$ while
raising the number of effective optimization steps by $48\%$ for Qwen3-1.7B,
with both on-policy and off-policy sample numbers set to $8$. The repository
can be accessed at https://github.com/SihengLi99/RePO.

</details>


### [12] [Latent Multi-Head Attention for Small Language Models](https://arxiv.org/abs/2506.09342)
*Sushant Mehta,Raj Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.CL

TL;DR: 该研究展示了MLA+RoPE在小型语言模型中的效率与质量之间的显著权衡，实现了显著的内存节省和推理加速。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索小型语言模型中潜多头注意力的使用情况，以揭示潜在的效率和质量之间的权衡关系。

Method: 我们研究了小型语言模型中的潜多头注意力（MLA），并提出了三种不同架构变体的比较：标准多头注意力（MHA）、MLA以及带有旋转位置编码的MLA（MLA+RoPE）。

Result: 研究发现，带有旋转位置编码的MLA（MLA+RoPE）在使用半秩潜在维度时（r=d/2），可以实现45%的KV缓存内存降低，同时只增加0.3%的验证损失；并且，在小模型中，旋转位置编码对于MLA的性能提升至关重要。

Conclusion: MLA+RoPE在确保模型质量（7.4/10）的情况下，同时也具备了更好的内存使用和推理速度。模型和代码将在接受后公开发布。

Abstract: We present the first comprehensive study of latent multi-head attention (MLA)
for small language models, revealing interesting efficiency-quality trade-offs.
Training 30M-parameter GPT models on 100,000 synthetic stories, we benchmark
three architectural variants: standard multi-head attention (MHA), MLA, and MLA
with rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE
with half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory
reduction while incurring only a 0.3% increase in validation loss (essentially
matching MHA quality)- a Pareto improvement for memory constrained deployment.
We further show that RoPE is crucial for MLA in small models: without it, MLA
underperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by
2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2
achieves a 1.4 times speedup over full-rank MLA while maintaining the memory
savings. GPT-4 evaluations corroborate perplexity results, with ours achieving
the highest quality scores (7.4/10) across grammar, creativity, and consistency
metrics. Code and models will be released upon acceptance.

</details>


### [13] [OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment](https://arxiv.org/abs/2506.09349)
*Chao-Hong Tan,Qian Chen,Wen Wang,Chong Deng,Qinglin Zhang,Luyao Cheng,Hai Yu,Xin Zhang,Xiang Lv,Tianyu Zhao,Chong Zhang,Yukun Ma,Yafeng Chen,Hui Wang,Jiaqing Liu,Jieping Ye*

Main category: cs.CL

TL;DR: This paper introduces OmniDRCA, a parallel speech-text generation model based on joint autoregressive modeling, achieving new state-of-the-art performance in speech-text generation.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the limitations of existing methods for generating discrete speech tokens with LLMs, which fall short in either failing to incorporate speech tokens into the LLM's autoregressive process or generate speech and text tokens independently.

Method: Our approach, named OmniDRCA, relies on parallel speech and text processing featuring dual-resolution speech representations and contrastive cross-modal alignment to enhance audio comprehension.

Result: OmniDRCA has achieved state-of-the-art performance on Spoken Question Answering benchmarks among models based on parallel joint speech-text modeling, and competitive performance relative to interleaved models.

Conclusion: This work presents an innovative method to parallelly generate speech and text with autoregressive modeling, advancing the state-of-the-art in speech-text foundation models. It also lays the groundwork for future work in full-duplex conversational scenarios.

Abstract: Recent studies on end-to-end speech generation with large language models
(LLMs) have attracted significant community attention, with multiple works
extending text-based LLMs to generate discrete speech tokens. Existing
approaches primarily fall into two categories: (1) Methods that generate
discrete speech tokens independently without incorporating them into the LLM's
autoregressive process, resulting in text generation being unaware of
concurrent speech synthesis. (2) Models that generate interleaved or parallel
speech-text tokens through joint autoregressive modeling, enabling mutual
modality awareness during generation. This paper presents OmniDRCA, a parallel
speech-text foundation model based on joint autoregressive modeling, featuring
dual-resolution speech representations and contrastive cross-modal alignment.
Our approach processes speech and text representations in parallel while
enhancing audio comprehension through contrastive alignment. Experimental
results on Spoken Question Answering benchmarks demonstrate that OmniDRCA
establishes new state-of-the-art (SOTA) performance among parallel joint
speech-text modeling based foundation models, and achieves competitive
performance compared to interleaved models. Additionally, we explore the
potential of extending the framework to full-duplex conversational scenarios.

</details>


### [14] [DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts](https://arxiv.org/abs/2506.09351)
*Yuchen Feng,Bowen Shen,Naibin Gu,Jiaxuan Zhao,Peng Fu,Zheng Lin,Weiping Wang*

Main category: cs.CL

TL;DR: 提出DIVE方法对MoE架构语言模型进行高效的重组，尤其是在不同校准数据集上剪枝后，有效利用专家多样性减少训练冗余。实验验证其训练效率高且准确率损失小。


<details>
  <summary>Details</summary>
Motivation: 观察到在不同的校准数据集上对特定语言模型进行剪枝后，会出现显著的多样性。然而，现有重组方法往往忽视这种多样性，可能导致冗余。本研究旨在解决这一问题。

Method: DIVE方法，包括领域亲和力挖掘、基于剪枝的专家重组以及高效再训练。特别地，重组包括前馈网络模块的剪枝和重组。

Result: 实验表明，DIVE实现了高效训练，且准确率损失最小，在激活参数数量相同的条件下，优于现有的剪枝和MoE重组方法。

Conclusion: 通过增强重组方法中的多样性，可以提高模型训练效率，减少精度损失。

Abstract: Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture
achieve high cost-efficiency by selectively activating a subset of the
parameters. Despite the inference efficiency of MoE LLMs, the training of
extensive experts from scratch incurs substantial overhead, whereas
reconstructing a dense LLM into an MoE LLM significantly reduces the training
budget. However, existing reconstruction methods often overlook the diversity
among experts, leading to potential redundancy. In this paper, we come up with
the observation that a specific LLM exhibits notable diversity after being
pruned on different calibration datasets, based on which we present a
Diversity-Enhanced reconstruction method named DIVE. The recipe of DIVE
includes domain affinity mining, pruning-based expert reconstruction, and
efficient retraining. Specifically, the reconstruction includes pruning and
reassembly of the feed-forward network (FFN) module. After reconstruction, we
efficiently retrain the model on routers, experts and normalization modules. We
implement DIVE on Llama-style LLMs with open-source training corpora.
Experiments show that DIVE achieves training efficiency with minimal accuracy
trade-offs, outperforming existing pruning and MoE reconstruction methods with
the same number of activated parameters.

</details>


### [15] [Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL](https://arxiv.org/abs/2506.09359)
*Qingyun Zeng,Simin Ma,Arash Niknafs,Ashish Basran,Carol Szabo*

Main category: cs.CL

TL;DR: 论文探讨了使用大型语言模型（LLMs）来解决Text-to-SQL系统中生成SQL语句的语义等价性评估问题的方法及挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）大幅度提升了Text-to-SQL（NL2SQL）系统的性能，但对生成SQL语句的语义等价性评估仍然是一个挑战，特别是在面对模糊的用户查询和多种有效的SQL解释的情况下。

Method: 本研究探讨了利用大型语言模型（LLMs）来评估生成SQL的语义等价性和一种更实用的“弱”语义等价性的方法。

Result: 本研究分析了SQL等价性和非等价性的常见模式，并讨论了基于LLM评估所面临的挑战。

Conclusion: 通过对SQL等价性和非等价性模式的分析和LLM评估方法讨论，本研究为改进SQL语义等价性评价提供了新视角。

Abstract: The rise of Large Language Models (LLMs) has significantly advanced
Text-to-SQL (NL2SQL) systems, yet evaluating the semantic equivalence of
generated SQL remains a challenge, especially given ambiguous user queries and
multiple valid SQL interpretations. This paper explores using LLMs to assess
both semantic and a more practical "weak" semantic equivalence. We analyze
common patterns of SQL equivalence and inequivalence, discuss challenges in
LLM-based evaluation.

</details>


### [16] [COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content](https://arxiv.org/abs/2506.09367)
*Zhengyuan Liu,Stella Xin Yin,Dion Hoe-Lian Goh,Nancy F. Chen*

Main category: cs.CL

TL;DR: 研究提出了一种命名为COGENT的课程导向框架，用于生成适合年级阅读水平的教育内容，能够有效解决生成式AI应用于教育时面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在内容生成方面展示了强大的潜力和多样性，但在教育领域中应用时存在挑战，如模型难以与课程标准一致并维持合适的年级阅读水平。特别是STEM教育，如何在保持科学性的同时用通俗易懂的语言解释复杂抽象的概念给学生带来了额外挑战。

Method: 我们提出了一种名为COGENT的框架，该框架旨在生成符合年级水平的教育内容。框架中包含了三个课程组成部分（科学概念、核心思想和学习目标），通过控制长度、词汇和句子复杂性来调控可读性，并采用"基于好奇"的方法来提高学生参与度和兴趣。

Result: 通过利用LLM作为评判者进行多维度评估，结果表明，COGENT能够持续生成与参考文献相当或更优的年级适当段落。

Conclusion: 我们的工作提供了一种生成适应性和高质量学习资源的可行方法。

Abstract: While Generative AI has demonstrated strong potential and versatility in
content generation, its application to educational contexts presents several
challenges. Models often fail to align with curriculum standards and maintain
grade-appropriate reading levels consistently. Furthermore, STEM education
poses additional challenges in balancing scientific explanations with everyday
language when introducing complex and abstract ideas and phenomena to younger
students. In this work, we propose COGENT, a curriculum-oriented framework for
generating grade-appropriate educational content. We incorporate three
curriculum components (science concepts, core ideas, and learning objectives),
control readability through length, vocabulary, and sentence complexity, and
adopt a ``wonder-based'' approach to increase student engagement and interest.
We conduct a multi-dimensional evaluation via both LLM-as-a-judge and human
expert analysis. Experimental results show that COGENT consistently produces
grade-appropriate passages that are comparable or superior to human references.
Our work establishes a viable approach for scaling adaptive and high-quality
learning resources.

</details>


### [17] [CoLMbo: Speaker Language Model for Descriptive Profiling](https://arxiv.org/abs/2506.09375)
*Massa Baali,Shuo Han,Syed Abdul Hannan,Purusottam Samal,Karanveer Singh,Soham Deshmukh,Rita Singh,Bhiksha Raj*

Main category: cs.CL

TL;DR: CoLMbo, a Speaker Language Model, integrates a speaker encoder with prompt-based conditioning to generate detailed and structured speaker descriptions, including dialect, gender, and age, marking an advancement in speaker recognition systems.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to overcome the limitations of current speaker recognition systems that struggle to provide detailed speaker characteristics or context-rich descriptions.

Method: The method involves integrating a speaker encoder with prompt-based conditioning to create detailed captions based on speaker embeddings.

Result: CoLMbo provides customized descriptions including regional dialect variations and age-related traits, performing well in zero-shot scenarios across diverse datasets.

Conclusion: This innovative approach enhances traditional speaker profiling and marks a significant advancement in the field of speaker recognition systems.

Abstract: Speaker recognition systems are often limited to classification tasks and
struggle to generate detailed speaker characteristics or provide context-rich
descriptions. These models primarily extract embeddings for speaker
identification but fail to capture demographic attributes such as dialect,
gender, and age in a structured manner. This paper introduces CoLMbo, a Speaker
Language Model (SLM) that addresses these limitations by integrating a speaker
encoder with prompt-based conditioning. This allows for the creation of
detailed captions based on speaker embeddings. CoLMbo utilizes user-defined
prompts to adapt dynamically to new speaker characteristics and provides
customized descriptions, including regional dialect variations and age-related
traits. This innovative approach not only enhances traditional speaker
profiling but also excels in zero-shot scenarios across diverse datasets,
marking a significant advancement in the field of speaker recognition.

</details>


### [18] [Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024](https://arxiv.org/abs/2506.09381)
*Austin McCutcheon,Thiago E. A. de Oliveira,Aleksandr Zheleznov,Chris Brogly*

Main category: cs.CL

TL;DR: 研究通过多种机器学习模型评估新闻标题和链接的质量，发现传统方法和深度学习模型都可以有效区分新闻标题/链接的质量，但存在训练时间和预测性能之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨能否自动区分低质量新闻标题/链接与高质量新闻标题/链接，以应对线上新闻的广泛发布带来的潜在问题。

Method: 本研究使用了12种机器学习模型来分析57,544,214条全球新闻网站的链接和标题。数据集中，每个文本的二元标签基于专家对新闻领域质量的一致评分得出。实验中使用了115个提取出的语义特征，并针对特定模型进行了训练和测试。

Result: 传统集成方法，尤其是装袋分类器，表现良好（准确率88.1%，F1分数88.3%）。微调过的DistilBERT模型取得了最高的准确率90.3%，但需要更长的训练时间。

Conclusion: 结果表明，基于自然语言处理特征的传统分类器与深度学习模型都能有效地区分新闻标题/链接的质量，但需要在预测性能和训练时间之间做出权衡。

Abstract: The proliferation of online news enables potential widespread publication of
perceived low-quality news headlines/links. As a result, we investigated
whether it was possible to automatically distinguish perceived lower-quality
news headlines/links from perceived higher-quality headlines/links. We
evaluated twelve machine learning models on a binary, balanced dataset of
57,544,214 worldwide news website links/headings from 2018-2024 (28,772,107 per
class) with 115 extracted linguistic features. Binary labels for each text were
derived from scores based on expert consensus regarding the respective news
domain quality. Traditional ensemble methods, particularly the bagging
classifier, had strong performance (88.1% accuracy, 88.3% F1, 80/20 train/test
split). Fine-tuned DistilBERT achieved the highest accuracy (90.3%, 80/20
train/test split) but required more training time. The results suggest that
both NLP features with traditional classifiers and deep learning models can
effectively differentiate perceived news headline/link quality, with some
trade-off between predictive performance and train time.

</details>


### [19] [Comparing human and LLM politeness strategies in free production](https://arxiv.org/abs/2506.09391)
*Haoran Zhao,Robert D. Hawkins*

Main category: cs.CL

TL;DR: 研究发现大规模语言模型（LLMs）在生成礼貌语言时表现出一定的能力，特别是在超过700亿参数的模型中，它们能够复制关键的礼貌策略偏好，并在开放性任务中获得人类评价者的偏好。然而，这些模型倾向于在所有情境下过度使用消极礼貌策略，可能导致误解。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索大规模语言模型如何在信息传递和社会互动之间找到平衡，并通过与人类的对比评估其在礼貌用语上的表现。

Method: 通过对比分析人类和大规模语言模型在受控和开放式任务中生成的礼貌言语策略，研究了LLMs在不同情境下使用礼貌策略的能力。

Result: 研究发现，超过700亿参数的LLMs能够有效复制计算语用学文献中的关键礼貌策略偏好；在开放性场景下，人类评价者对模型生成的响应表现出偏好；然而，这些模型在所有情境下都倾向于使用消极礼貌策略，可能导致误解。

Conclusion: 尽管现代LLMs在礼貌策略的应用上表现出色，但它们在不同情境下使用正面与负面策略的不平衡性，引起了关于AI系统语用学对齐的重要疑问。

Abstract: Polite speech poses a fundamental alignment challenge for large language
models (LLMs). Humans deploy a rich repertoire of linguistic strategies to
balance informational and social goals -- from positive approaches that build
rapport (compliments, expressions of interest) to negative strategies that
minimize imposition (hedging, indirectness). We investigate whether LLMs employ
a similarly context-sensitive repertoire by comparing human and LLM responses
in both constrained and open-ended production tasks. We find that larger models
($\ge$70B parameters) successfully replicate key preferences from the
computational pragmatics literature, and human evaluators surprisingly prefer
LLM-generated responses in open-ended contexts. However, further linguistic
analyses reveal that models disproportionately rely on negative politeness
strategies even in positive contexts, potentially leading to
misinterpretations. While modern LLMs demonstrate an impressive handle on
politeness strategies, these subtle differences raise important questions about
pragmatic alignment in AI systems.

</details>


### [20] [A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings](https://arxiv.org/abs/2506.09393)
*Xinyi Gao,Qiucheng Wu,Yang Zhang,Xuechen Liu,Kaizhi Qian,Ying Xu,Shiyu Chang*

Main category: cs.CL

TL;DR: 本文提出了KT$^2$，通过隐藏马尔可夫树模型建模学生对知识概念的掌握，并利用EM算法估计学生知识掌握程度。实验表明KT$^2$在低资源的真实在线环境中，优于强大的基线模型。


<details>
  <summary>Details</summary>
Motivation: 许多知识追踪的实际课堂环境通常数据资源有限，并且需要随着学生练习历史的增长进行在线更新，这对现有的知识追踪方法提出了重大挑战。为了在低资源条件下恢复强大性能，我们重新审视了典型的课堂环境中可用的层次知识概念信息，在数据稀疏时提供有力的先验知识。

Method: 我们提出了基于知识树的知识追踪(KT$^2$)，这是一个概率性的知识追踪框架，它通过隐藏马尔可夫树模型来建模学生对知识概念树状层级结构的理解。KT$^2$利用EM算法估计学生掌握知识的情况，并通过增量更新机制在接收到新答案时支持个性化预测。

Result: 实验结果表明，KT$^2$在低资源的真实在线环境下，相比于强大基线模型，能保持一致性地更优表现。

Conclusion: 实验表明KT$^2$在低资源的真实在线环境中，始终保持优于强大基线模型的性能。

Abstract: Knowledge tracing (KT) aims to estimate a student's evolving knowledge state
and predict their performance on new exercises based on performance history.
Many realistic classroom settings for KT are typically low-resource in data and
require online updates as students' exercise history grows, which creates
significant challenges for existing KT approaches. To restore strong
performance under low-resource conditions, we revisit the hierarchical
knowledge concept (KC) information, which is typically available in many
classroom settings and can provide strong prior when data are sparse. We
therefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a
probabilistic KT framework that models student understanding over a
tree-structured hierarchy of knowledge concepts using a Hidden Markov Tree
Model. KT$^2$ estimates student mastery via an EM algorithm and supports
personalized prediction through an incremental update mechanism as new
responses arrive. Our experiments show that KT$^2$ consistently outperforms
strong baselines in realistic online, low-resource settings.

</details>


### [21] [Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models](https://arxiv.org/abs/2506.09408)
*Jui-Ming Yao,Hao-Yuan Chen,Zi-Xian Tang,Bing-Jia Tan,Sheng-Wei Peng,Bing-Cheng Xie,Shun-Feng Su*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) have demonstrated impressive performance on
multiple-choice question answering (MCQA) benchmarks, yet they remain highly
vulnerable to minor input perturbations. In this paper, we introduce and
evaluate Token Constraint Decoding (TCD). This simple yet effective
inference-time algorithm enforces alignment between token-level predictions to
enhance robustness in noisy settings. Through extensive experiments on
CommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired
with prompt engineering (PE) fixes, significantly restores performance degraded
by input noise, yielding up to +39\% absolute gains for weaker models like
Gemma3 1B. Penalty sweep analyses further reveal that TCD implicitly
regularizes overconfident outputs, with different models requiring distinct
penalty schedules to maximize resilience. Our findings establish TCD as a
practical, model-agnostic approach for improving reasoning stability under
real-world imperfections and pave the way for more reliable deployment of LLMs
in safety-critical or user-facing applications.

</details>


### [22] [PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering](https://arxiv.org/abs/2506.09414)
*Xiujun Zhou,Pingjian Zhang,Deyou Tang*

Main category: cs.CL

TL;DR: 提出PGDA-KGQA以解决多跳推理样本稀缺和语义扭曲问题，通过多样化的数据增强策略提升知识图谱问答性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有数据增强方法只能处理单跳问题且容易语义扭曲，同时解决多跳推理样本稀缺导致模型泛化能力弱的问题。

Method: 采用统一的提示设计范式，利用精心设计的提示结合LLMs生成大规模的(问题, 逻辑形式)对，采取三种策略丰富训练集。

Result: 在标准KGQA数据集上的实验显示出优于现有方法的表现，在WebQSP和ComplexWebQuestions上分别提高了2.8%-3.1%和1.8%-2.4%的F1, Hits@1, 和Accuracy。

Conclusion: PGDA-KGQA框架通过整合LLM，多样化的数据增强策略证明了其在提升KGQA模型性能方面的有效性。

Abstract: Knowledge Graph Question Answering (KGQA) is a crucial task in natural
language processing that requires reasoning over knowledge graphs (KGs) to
answer natural language questions. Recent methods utilizing large language
models (LLMs) have shown remarkable semantic parsing capabilities but are
limited by the scarcity of diverse annotated data and multi-hop reasoning
samples. Traditional data augmentation approaches are focus mainly on
single-hop questions and prone to semantic distortion, while LLM-based methods
primarily address semantic distortion but usually neglect multi-hop reasoning,
thus limiting data diversity. The scarcity of multi-hop samples further weakens
models' generalization. To address these issues, we propose PGDA-KGQA, a
prompt-guided generative framework with multiple data augmentation strategies
for KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by
crafting meticulously engineered prompts that integrate the provided textual
content, it leverages LLMs to generate large-scale (question, logical form)
pairs for model training. Specifically, PGDA-KGQA enriches its training set by:
(1) generating single-hop pseudo questions to improve the alignment of question
semantics with KG relations; (2) applying semantic-preserving question
rewriting to improve robustness against linguistic variations; (3) employing
answer-guided reverse path exploration to create realistic multi-hop questions.
By adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA
utilizes the augmented data to enhance the accuracy of logical form generation
and thus improve answer retrieval performance. Experiments demonstrate that
outperforms state-of-the-art methods on standard KGQA datasets, achieving
improvements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by
1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.

</details>


### [23] [Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings](https://arxiv.org/abs/2506.09424)
*Md Messal Monem Miah,Adrita Anika,Xi Shi,Ruihong Huang*

Main category: cs.CL

TL;DR: 本研究全面评估了大型语言模型（LLMs）和大型多模态模型（LMMs）在不同领域的自动欺骗检测能力。结果显示，微调后的LLMs在文本欺骗检测任务中表现出顶级水平，而LMMs在利用跨模态线索上表现不佳。研究还探讨了辅助特征和不同提示策略的影响。


<details>
  <summary>Details</summary>
Motivation: 鉴于数字世界中欺骗检测的重要性与挑战性，该研究旨在评估不同类型的模型在欺骗检测任务中的表现，以便为实际应用提供指导。

Method: 研究者通过对三个不同数据集（现实生活访谈、人际情景中的指示性欺骗以及欺诈性评论）进行评估，采用零样本和少样本方法，并测试不同样本选择策略。

Result: 发现微调后的LLMs在文本欺骗检测中表现出色，LMMs在跨模态线索利用上存在局限性；辅助特征和不同的提示策略对结果有一定影响。

Conclusion: 研究揭示了LLMs在处理和解释欺骗线索方面的潜力与局限性，为现实世界的应用提供了关键见解。

Abstract: Detecting deception in an increasingly digital world is both a critical and
challenging task. In this study, we present a comprehensive evaluation of the
automated deception detection capabilities of Large Language Models (LLMs) and
Large Multimodal Models (LMMs) across diverse domains. We assess the
performance of both open-source and commercial LLMs on three distinct datasets:
real life trial interviews (RLTD), instructed deception in interpersonal
scenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the
effectiveness of different experimental setups for deception detection,
including zero-shot and few-shot approaches with random or similarity-based
in-context example selection. Our results show that fine-tuned LLMs achieve
state-of-the-art performance on textual deception detection tasks, while LMMs
struggle to fully leverage cross-modal cues. Additionally, we analyze the
impact of auxiliary features, such as non-verbal gestures and video summaries,
and examine the effectiveness of different prompting strategies, including
direct label generation and chain-of-thought reasoning. Our findings provide
key insights into how LLMs process and interpret deceptive cues across
modalities, highlighting their potential and limitations in real-world
deception detection applications.

</details>


### [24] [Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting](https://arxiv.org/abs/2506.09428)
*Fei Ding,Baiqiao Wang*

Main category: cs.CL

TL;DR: 本文针对SFT方法导致的语言模型普遍能力降低问题，提出了一种更加经济的方法，通过重构指令分布和多模型筛选来减少灾难性遗忘，实验结果表明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 虽然SFT增强了大语言模型的指令遵从能力和特定任务的适应性，但也往往降低了它们的普遍能力。此外，由于原始预训练数据的不可访问性，灾难性遗忘的问题在第三方实践者对开源模型实施SFT时会恶化。为解决这一挑战，本文提出了一种新的方法。

Method: 本文提出了一种新的监督微调(SFT)方法，旨在以较低成本减少灾难性遗忘的风险，同时不依赖原始微调数据。该方法首先重构基础模型可能的SFT指令分布，然后通过多模型筛选过程选择最优数据，并将其与新数据混合进行SFT。

Result: 实验结果显示，该方法在保持通用领域泛化能力的同时，提升了特定任务性能。

Conclusion: 本文提出的方法证明了在保持通用领域的泛化能力的同时，可以改进特定任务的表现，并且以较低的成本减少灾难性遗忘的风险。

Abstract: Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'
instruction-following capabilities and domain-specific task adaptability, often
diminishes their general capabilities. Moreover, due to the inaccessibility of
original pre-training data, catastrophic forgetting tends to be exacerbated
when third-party practitioners implement SFT on open-sourced models. To address
this challenge, we propose a novel, more cost-effective SFT method which could
effectively reduce the risk of catastrophic forgetting without access to
original SFT data. Our approach begins by reconstructing the likely SFT
instruction distribution of the base model, followed by a multi-model screening
process to select optimal data, which is then mixed with new data for SFT.
Experimental results demonstrate that our method preserves generalization
capabilities in general domains while improving task-specific performance.

</details>


### [25] [GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture](https://arxiv.org/abs/2506.09440)
*GigaChat team,Mamedov Valentin,Evgenii Kosarev,Gregory Leleytner,Ilya Shchuckin,Valeriy Berezovskiy,Daniil Smirnov,Dmitry Kozlov,Sergei Averkiev,Lukyanenko Ivan,Aleksandr Proshunin,Ainur Israfilova,Ivan Baskov,Artem Chervyakov,Emil Shakirov,Mikhail Kolesov,Daria Khomich,Darya Latortseva,Sergei Porkhun,Yury Fedorov,Oleg Kutuzov,Polina Kudriavtseva,Sofiia Soldatova,Kolodin Egor,Stanislav Pyatkin,Dzmitry Menshykh,Grafov Sergei,Eldar Damirov,Karlov Vladimir,Ruslan Gaitukiev,Arkadiy Shatenov,Alena Fenogenova,Nikita Savushkin,Fedor Minkin*

Main category: cs.CL

TL;DR: 本文介绍了一款俄语大模型GigaChat，详细描述了其架构、预训练过程及实验，并开放了部分模型供研究和工业使用。


<details>
  <summary>Details</summary>
Motivation: 由于开发针对俄语的基础模型需要大量的计算资源，因此专门优化的模型不多。本文旨在填补这一空缺。

Method: 本文介绍了GigaChat系列俄语LLM模型，提供了关于模型架构、预训练过程和实验的详细报告，以及如何指导设计选择的方法。

Result: 研究评估了GigaChat模型在俄语和英语基准上的性能，并将其与多语言模型进行了比较。

Conclusion: 本文通过API、Telegram机器人和Web界面提供顶级模型的系统演示，并开放了三个公开模型，以促进俄语NLP研究和工业解决方案的发展。

Abstract: Generative large language models (LLMs) have become crucial for modern NLP
research and applications across various languages. However, the development of
foundational models specifically tailored to the Russian language has been
limited, primarily due to the significant computational resources required.
This paper introduces the GigaChat family of Russian LLMs, available in various
sizes, including base models and instruction-tuned versions. We provide a
detailed report on the model architecture, pre-training process, and
experiments to guide design choices. In addition, we evaluate their performance
on Russian and English benchmarks and compare GigaChat with multilingual
analogs. The paper presents a system demonstration of the top-performing models
accessible via an API, a Telegram bot, and a Web interface. Furthermore, we
have released three open GigaChat models in open-source
(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities
and support the development of industrial solutions for the Russian language.

</details>


### [26] [UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs](https://arxiv.org/abs/2506.09450)
*Prameshwar Thiyagarajan,Vaishnavi Parimi,Shamant Sai,Soumil Garg,Zhangir Meirbek,Nitin Yarlagadda,Kevin Zhu,Chris Kim*

Main category: cs.CL

TL;DR: 研究引入了UniToMBench——一个用于提升和评估语言模型社会认知能力的综合性基准工具，通过实验发现模型在ToM任务上的成就与不足。


<details>
  <summary>Details</summary>
Motivation: 目前，大型语言模型在理解和预测人类思想状态方面存在困难。因此，本文旨在通过UniToMBench 提供一个综合工具，帮助改善这一情况并评估模型的ToM（Theory of Mind）能力。

Method: 本文介绍了UniToMBench，该基准通过整合SimToM和TOMBENCH的优点，引入了多交互任务设计和变化的故事场景，旨在系统性地提升和评估语言模型在理解人类心理状态方面的能力。UniToMBench 使用超过1,000个手工编写的情景组成的定制数据集，结合视角理解和多样的评估指标，以更好地刺激语言模型的社会认知能力。

Result: 评估结果显示，像GPT-4o和GPT-4o Mini这样的模型在情绪和信念相关的情形中表现出一致的高准确率，通常超过80%。然而，在基于知识的任务中，其表现则显示出较大的差异性。

Conclusion: 尽管如此，实验结果还是展现了当前语言模型在处理ToM任务的能力上的优势和局限性，强调了UniToMBench作为一种全面评估和发展工具的重要性。

Abstract: Theory of Mind (ToM), the ability to understand the mental states of oneself
and others, remains a challenging area for large language models (LLMs), which
often fail to predict human mental states accurately. In this paper, we
introduce UniToMBench, a unified benchmark that integrates the strengths of
SimToM and TOMBENCH to systematically improve and assess ToM capabilities in
LLMs by integrating multi-interaction task designs and evolving story
scenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,
UniToMBench combines perspective-taking techniques with diverse evaluation
metrics to better stimulate social cognition in LLMs. Through evaluation, we
observe that while models like GPT-4o and GPT-4o Mini show consistently high
accuracy in tasks involving emotional and belief-related scenarios, with
results usually above 80%, there is significant variability in their
performance across knowledge-based tasks. These results highlight both the
strengths and limitations of current LLMs in ToM-related tasks, underscoring
the value of UniToMBench as a comprehensive tool for future development. Our
code is publicly available here:
https://github.com/Shamant/unifiedtombenchmark.

</details>


### [27] [Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms](https://arxiv.org/abs/2506.09457)
*Zeguan Xiao,Yun Chen,Guanhua Chen*

Main category: cs.CL

TL;DR: 本文提出一种称为Prefix-Oriented Equal-length Training (POET)的新方法，通过截断优选和非优选响应以匹配较短响应的长度，以解决Direct Alignment Algorithms (DAAs)中存在的奖励生成差距问题，从而改善大型语言模型与人类偏好的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 解决Direct Alignment Algorithms (DAAs)中的“奖励生成差距”问题，该问题源于训练期间优化目标与推理期间实际生成表现之间的不匹配。

Method: 引入POET方法，即通过截断优选和非优选响应以匹配较短响应的长度，从而解决奖励生成差距问题。

Result: 实验结果表明，对比DPO和SimPO等DAAs的传统实现方式，POET方法在AlpacaEval 2测试中提高了最多15.6分，并在多个下游任务中均有改进。

Conclusion: 本文的研究突显了在DAAs中解决奖励优化与生成性能之间对齐问题的重要性。

Abstract: Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization
(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient
alternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms
for aligning large language models (LLMs) with human preferences. However, DAAs
suffer from a fundamental limitation we identify as the "reward-generation gap"
-- a misalignment between optimization objectives during training and actual
generation performance during inference. In this paper, we find a contributor
to the reward-generation gap is the mismatch between the inherent importance of
prefix tokens during the LLM generation process and how this importance is
reflected in the implicit reward functions of DAAs. To bridge the gap, we
introduce a simple yet effective approach called Prefix-Oriented Equal-length
Training (POET), which truncates both preferred and dispreferred responses to
match the shorter one's length. Training with POET, where both responses in
each sample are truncated to equal length, resulting in diverse truncated
lengths across samples, the optimization of DAAs objective is implicitly
constrained to converge across all positions, thus paying more attention to
prefix tokens than the standard DAAs. We conduct experiments with DPO and
SimPO, two representative DAAs, demonstrating that POET improves over their
standard implementations, achieving up to 15.6 points in AlpacaEval 2 and
overall improvements across downstream tasks. Our results highlight the
importance of addressing the misalignment between reward optimization and
generation performance in DAAs.

</details>


### [28] [Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers](https://arxiv.org/abs/2506.09495)
*Ilanit Sobol,Shir Lissak,Refael Tikochinski,Tal Nakash,Anat Brunstein Klomek,Eyal Fruchter,Roi Reichart*

Main category: cs.CL

TL;DR: 研究通过分析181个曾有生命危险自杀尝试者的YouTube频道视频，结合134个对照组频道，采用计算、混合和专家驱动的方法，识别出与自杀行为相关的主题，发现心理健康挣扎和YouTube互动是两个主要指标，且动机的差异表明自杀者希望帮助他人或个人康复。


<details>
  <summary>Details</summary>
Motivation: 鉴于自杀在全球多个西方国家是主要死因，该研究基于社交媒体上内容，特别是YouTube视频，探讨自杀行为如何呈现及其与专家知识的差异，以期为自杀行为的研究提供新的视角。

Method: 研究使用了计算自下而上、混合和专家驱动自上而下的三个方法来分析数据，包括长周期的181个自杀尝试者的YouTube频道，应用LLM主题建模来识别行为指示器，并结合临床专家的审核和心理评估。

Result: 通过自下而上的方法识别了两个与自杀行为显著相关的主题：心理健康挣扎和YouTube互动；混合方法由专家审核，但未发现额外的主题。自上而下的心理评估揭示了自杀者上传视频前后在动机上存在差异，分别希望通过分享经历帮助他人或作为个人康复的一部分。

Conclusion: 研究整合了自下而上和自上而下的发现，指出自杀行为与数字行为及临床见解之间存在复杂关系，并强调自下而上方法在揭示平台特定指标（如YouTube互动）的价值。

Abstract: Suicide remains a leading cause of death in Western countries, underscoring
the need for new research approaches. As social media becomes central to daily
life, digital footprints offer valuable insight into suicidal behavior.
Focusing on individuals who attempted suicide while uploading videos to their
channels, we investigate: How do suicidal behaviors manifest on YouTube, and
how do they differ from expert knowledge? We applied complementary approaches:
computational bottom-up, hybrid, and expert-driven top-down, on a novel
longitudinal dataset of 181 YouTube channels from individuals with
life-threatening attempts, alongside 134 control channels. In the bottom-up
approach, we applied LLM-based topic modeling to identify behavioral
indicators. Of 166 topics, five were associated with suicide-attempt, with two
also showing temporal attempt-related changes ($p<.01$) - Mental Health
Struggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,
a clinical expert reviewed LLM-derived topics and flagged 19 as
suicide-related. However, none showed significant attempt-related temporal
effects beyond those identified bottom-up. Notably, YouTube Engagement, a
platform-specific indicator, was not flagged by the expert, underscoring the
value of bottom-up discovery. In the top-down approach, psychological
assessment of suicide attempt narratives revealed that the only significant
difference between individuals who attempted before and those attempted during
their upload period was the motivation to share this experience: the former
aimed to Help Others ($\beta=-1.69$, $p<.01$), while the latter framed it as
part of their Personal Recovery ($\beta=1.08$, $p<.01$). By integrating these
approaches, we offer a nuanced understanding of suicidality, bridging digital
behavior and clinical insights.
  * Within-group changes in relation to the suicide attempt.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [29] [ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices](https://arxiv.org/abs/2506.09066)
*Maoyu Wang,Yao Lu,Jiaqi Nie,Zeyu Wang,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.CV

TL;DR: ReStNet solves the problem of deploying pre-trained models in IoT with varying resources by stitching two models together and selectively fine-tuning them for efficiency and adaptability.


<details>
  <summary>Details</summary>
Motivation: The motivation behind ReStNet is to address resource constraints in IoT applications, where deploying a single model across all platforms with heterogeneous computational and memory resources is not feasible and existing compression methods are inflexible.

Method: ReStNet, a Reusable and Stitchable Network, dynamically constructs a hybrid network by stitching two pre-trained models together, using Centered Kernel Alignment (CKA) for determining stitching points, and fine-tuning only the stitching layer for efficiency.

Result: ReStNet demonstrates flexible accuracy-efficiency trade-offs at runtime and significantly reduces training cost through its novel approach to stitching models.

Conclusion: ReStNet enables rapid adaptation to changing resource budgets and flexible combination of different model families, making it suitable for efficient deployment in varying IoT environments.

Abstract: With the rapid development of deep learning, a growing number of pre-trained
models have been publicly available. However, deploying these fixed models in
real-world IoT applications is challenging because different devices possess
heterogeneous computational and memory resources, making it impossible to
deploy a single model across all platforms. Although traditional compression
methods, such as pruning, quantization, and knowledge distillation, can improve
efficiency, they become inflexible once applied and cannot adapt to changing
resource constraints. To address these issues, we propose ReStNet, a Reusable
and Stitchable Network that dynamically constructs a hybrid network by
stitching two pre-trained models together. Implementing ReStNet requires
addressing several key challenges, including how to select the optimal
stitching points, determine the stitching order of the two pre-trained models,
and choose an effective fine-tuning strategy. To systematically address these
challenges and adapt to varying resource constraints, ReStNet determines the
stitching point by calculating layer-wise similarity via Centered Kernel
Alignment (CKA). It then constructs the hybrid model by retaining early layers
from a larger-capacity model and appending deeper layers from a smaller one. To
facilitate efficient deployment, only the stitching layer is fine-tuned. This
design enables rapid adaptation to changing budgets while fully leveraging
available resources. Moreover, ReStNet supports both homogeneous (CNN-CNN,
Transformer-Transformer) and heterogeneous (CNN-Transformer) stitching,
allowing to combine different model families flexibly. Extensive experiments on
multiple benchmarks demonstrate that ReStNet achieve flexible
accuracy-efficiency trade-offs at runtime while significantly reducing training
cost.

</details>


### [30] [Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations](https://arxiv.org/abs/2506.09067)
*Zhiyu Xue,Reza Abbasi-Asl,Ramtin Pedarsani*

Main category: cs.CV

TL;DR: 本文提出一种在推理时抵御视觉和文本攻击的防御策略，同时提高Med-VLMs的安全性而不会显著降低其性能。


<details>
  <summary>Details</summary>
Motivation: Med-VLMs的安全漏洞尚未得到充分研究。需要一种方法，让这些模型能够拒绝有害查询，同时避免过度防御导致性能下降。

Method: 本论文提出了一种新颖的推理时防御策略，能够抵御视觉和文本形式的攻击，通过增加合成临床演示来提高模型的安全性。此外，还介绍了一种混合展示策略，旨在预算有限的情况下平衡安全性和性能。

Result: 实验结果显示，该防御策略能有效提升模型安全性，同时通过混合展示策略在面对有限预算时也能保持性能。

Conclusion: 研究证明，通过合成临床演示增量，提出的防御策略可以增强Med-VLMs的安全性而不显著损害性能。增加演示预算能进一步解决过度防御问题。

Abstract: Generative medical vision-language models~(Med-VLMs) are primarily designed
to generate complex textual information~(e.g., diagnostic reports) from
multimodal inputs including vision modality~(e.g., medical images) and language
modality~(e.g., clinical queries). However, their security vulnerabilities
remain underexplored. Med-VLMs should be capable of rejecting harmful queries,
such as \textit{Provide detailed instructions for using this CT scan for
insurance fraud}. At the same time, addressing security concerns introduces the
risk of over-defense, where safety-enhancing mechanisms may degrade general
performance, causing Med-VLMs to reject benign clinical queries. In this paper,
we propose a novel inference-time defense strategy to mitigate harmful queries,
enabling defense against visual and textual jailbreak attacks. Using diverse
medical imaging datasets collected from nine modalities, we demonstrate that
our defense strategy based on synthetic clinical demonstrations enhances model
safety without significantly compromising performance. Additionally, we find
that increasing the demonstration budget alleviates the over-defense issue. We
then introduce a mixed demonstration strategy as a trade-off solution for
balancing security and performance under few-shot demonstration budget
constraints.

</details>


### [31] [BG-HOP: A Bimanual Generative Hand-Object Prior](https://arxiv.org/abs/2506.09068)
*Sriram Krishna,Sravan Chittupalli,Sungjae Park*

Main category: cs.CV

TL;DR: 提出了BG-HOP，一种旨在建模3D双手动-物互动的生成先验模型，展示了捕捉手和物联合分布的初步结果。


<details>
  <summary>Details</summary>
Motivation: 解决双手动-物互动数据有限的问题，通过扩展现有的单手生成先验模型。

Method: 开发了BG-HOP模型来生成双手动-物互动，并为给定物体合成抓取动作。

Result: 模型能够生成双手动-物互动，并为给定物体合成抓握。

Conclusion: 该研究展示了BG-HOP在生成双手动-物互动方面的潜力，并公开了代码和模型。

Abstract: In this work, we present BG-HOP, a generative prior that seeks to model
bimanual hand-object interactions in 3D. We address the challenge of limited
bimanual interaction data by extending existing single-hand generative priors,
demonstrating preliminary results in capturing the joint distribution of hands
and objects. Our experiments showcase the model's capability to generate
bimanual interactions and synthesize grasps for given objects. We make code and
models publicly available.

</details>


### [32] [Segment Any Architectural Facades (SAAF):An automatic segmentation model for building facades, walls and windows based on multimodal semantics guidance](https://arxiv.org/abs/2506.09071)
*Peilin Li,Jun Yin,Jing Zhong,Ran Luo,Pengyu Zeng,Miao Zhang*

Main category: cs.CV

TL;DR: 研究提出了一种基于多模态语义引导的建筑外墙和窗户分割模型SAAF，该模型能够自主学习文本描述到图像分割的映射关系，并在多个外墙数据集上表现出优越的分割效果。


<details>
  <summary>Details</summary>
Motivation: 在建筑数字化发展背景下，建筑物信息模型和计算机辅助设计的效率可以通过自动分割墙体和窗户得到有效提升。

Method: 本研究提出了一种基于多模态语义引导的建筑外墙和窗户自动分割模型Segment Any Architectural Facades (SAAF)。首先，SAAF具备多模态语义协同特征提取机制，可以融合文本描述中的语义信息和图像特征，提高对建筑外墙构件的语义理解。其次，开发了一个端到端的训练框架，使模型能够自主学习从文本描述到图像分割的映射关系，减少人工干预的影响，提高模型的自动化和鲁棒性。

Result: 在多个外墙数据集上进行了广泛实验，SAAF的分割结果在mIoU指标上超越了现有的方法，表明该模型在面对多样化数据集时能够保持高精度分割能力。

Conclusion: SAAF模型在提高墙体和窗户分割任务的准确性与泛化能力方面取得了一定进展，可以提供关于建筑计算机视觉技术和多模态学习在建筑领域的新思路和技术路径参考。

Abstract: In the context of the digital development of architecture, the automatic
segmentation of walls and windows is a key step in improving the efficiency of
building information models and computer-aided design. This study proposes an
automatic segmentation model for building facade walls and windows based on
multimodal semantic guidance, called Segment Any Architectural Facades (SAAF).
First, SAAF has a multimodal semantic collaborative feature extraction
mechanism. By combining natural language processing technology, it can fuse the
semantic information in text descriptions with image features, enhancing the
semantic understanding of building facade components. Second, we developed an
end-to-end training framework that enables the model to autonomously learn the
mapping relationship from text descriptions to image segmentation, reducing the
influence of manual intervention on the segmentation results and improving the
automation and robustness of the model. Finally, we conducted extensive
experiments on multiple facade datasets. The segmentation results of SAAF
outperformed existing methods in the mIoU metric, indicating that the SAAF
model can maintain high-precision segmentation ability when faced with diverse
datasets. Our model has made certain progress in improving the accuracy and
generalization ability of the wall and window segmentation task. It is expected
to provide a reference for the development of architectural computer vision
technology and also explore new ideas and technical paths for the application
of multimodal learning in the architectural field.

</details>


### [33] [VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks](https://arxiv.org/abs/2506.09079)
*Xinlong Chen,Yuanxing Zhang,Yushuo Guan,Bohan Zeng,Yang Shi,Sihan Yang,Pengfei Wan,Qiang Liu,Liang Wang,Tieniu Tan*

Main category: cs.CV

TL;DR: This paper introduces VersaVid-R1, a new model for video reasoning, using innovative datasets and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the underdeveloped field of video-based reasoning due to data scarcity and ineffective training methodologies.

Method: The paper develops a new model named VersaVid-R1 by leveraging two novel datasets, DarkEventInfer and MixVidQA, and training with reinforcement learning guided by diverse reward functions.

Result: Experiments show that VersaVid-R1 surpasses existing models across various benchmarks, including video understanding, cognitive reasoning, and captioning tasks.

Conclusion: The conclusion is that the developed model, VersaVid-R1, represents a significant progress in video understanding and reasoning under the Reason-Then-Respond paradigm.

Abstract: Recent advancements in multimodal large language models have successfully
extended the Reason-Then-Respond paradigm to image-based reasoning, yet
video-based reasoning remains an underdeveloped frontier, primarily due to the
scarcity of high-quality reasoning-oriented data and effective training
methodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA,
two novel datasets specifically designed to stimulate the model's advanced
video understanding and reasoning abilities. DarkEventinfer presents videos
with masked event segments, requiring models to infer the obscured content
based on contextual video cues. MixVidQA, on the other hand, presents
interleaved video sequences composed of two distinct clips, challenging models
to isolate and reason about one while disregarding the other. Leveraging these
carefully curated training samples together with reinforcement learning guided
by diverse reward functions, we develop VersaVid-R1, the first versatile video
understanding and reasoning model under the Reason-Then-Respond paradigm
capable of handling multiple-choice and open-ended question answering, as well
as video captioning tasks. Extensive experiments demonstrate that VersaVid-R1
significantly outperforms existing models across a broad spectrum of
benchmarks, covering video general understanding, cognitive reasoning, and
captioning tasks.

</details>


### [34] [FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation](https://arxiv.org/abs/2506.09081)
*Zheqi He,Yesheng Liu,Jing-shu Zheng,Xuejing Li,Richeng Xuan,Jin-Ge Yao,Xi Yang*

Main category: cs.CV

TL;DR: FlagEvalMM is an open-source evaluation framework for multimodal models, designed to assess various vision-language tasks efficiently and accurately.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a comprehensive evaluation framework for multimodal models that can assess vision-language understanding and generation tasks with improved efficiency and accuracy.

Method: We decouple model inference from evaluation through an independent evaluation service, enabling flexible resource allocation and seamless integration of new tasks and models. FlagEvalMM uses advanced inference acceleration tools and asynchronous data loading to enhance evaluation efficiency.

Result: Extensive experiments demonstrate that FlagEvalMM provides accurate and efficient insights into the performance of multimodal models.

Conclusion: FlagEvalMM is a valuable tool for advancing research in multimodal models, offering a framework for accurate and efficient model evaluation.

Abstract: We present FlagEvalMM, an open-source evaluation framework designed to
comprehensively assess multimodal models across a diverse range of
vision-language understanding and generation tasks, such as visual question
answering, text-to-image/video generation, and image-text retrieval. We
decouple model inference from evaluation through an independent evaluation
service, thus enabling flexible resource allocation and seamless integration of
new tasks and models. Moreover, FlagEvalMM utilizes advanced inference
acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to
significantly enhance evaluation efficiency. Extensive experiments show that
FlagEvalMM offers accurate and efficient insights into model strengths and
limitations, making it a valuable tool for advancing multimodal research. The
framework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.

</details>


### [35] [AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models](https://arxiv.org/abs/2506.09082)
*Zheda Mai,Arpita Chowdhury,Zihe Wang,Sooyoung Jeon,Lemeng Wang,Jiacheng Hou,Jihyung Kil,Wei-Lun Chao*

Main category: cs.CV

TL;DR: 为了系统地评估视觉基础模型(VFMs)，研究引入了AVA-Bench，一个明确分解14个原子视觉能力的基准，提供了精确的VFMs评估，并且减少了GPU时间，提升了评估效率。


<details>
  <summary>Details</summary>
Motivation: 针对视觉基础模型(VFMs)评估中的两个盲点：1)指令调优数据可能与VQA测试分布不匹配；2)VQA基准可能需要多种视觉能力，难以定位错误来源。为了弥补这些不足，引入AVA-Bench来优化VFMs的评估。

Method: 提出AVA-Bench，第一个明确分解14个原子视觉能力的基准，如定位、深度估计和空间理解，这些能力共同支持复杂的视觉推理任务。通过分离这些AVAs并在每个AVAs中匹配训练和测试分布，AVA-Bench可以精确指出VFMs的表现。

Result: 应用AVA-Bench评估领先的VFMs揭示了独特的"能力指纹"，将VFMs的选择从经验猜测转变为原则性工程工作。值得注意的是，0.5B LLM与7B LLM可以得到类似的VFMs排名，同时减少了8倍的GPU时间，使评估更高效。

Conclusion: 通过提供全面透明的基准，AVA-Bench为下一代表视觉基础模型(VFMs)的开发奠定了基础。

Abstract: The rise of vision foundation models (VFMs) calls for systematic evaluation.
A common approach pairs VFMs with large language models (LLMs) as
general-purpose heads, followed by evaluation on broad Visual Question
Answering (VQA) benchmarks. However, this protocol has two key blind spots: (i)
the instruction tuning data may not align with VQA test distributions, meaning
a wrong prediction can stem from such data mismatch rather than a VFM' visual
shortcomings; (ii) VQA benchmarks often require multiple visual abilities,
making it hard to tell whether errors stem from lacking all required abilities
or just a single critical one. To address these gaps, we introduce AVA-Bench,
the first benchmark that explicitly disentangles 14 Atomic Visual Abilities
(AVAs) -- foundational skills like localization, depth estimation, and spatial
understanding that collectively support complex visual reasoning tasks. By
decoupling AVAs and matching training and test distributions within each,
AVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench
to leading VFMs thus reveals distinctive "ability fingerprints," turning VFM
selection from educated guesswork into principled engineering. Notably, we find
that a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours
by 8x, enabling more efficient evaluation. By offering a comprehensive and
transparent benchmark, we hope AVA-Bench lays the foundation for the next
generation of VFMs.

</details>


### [36] [BakuFlow: A Streamlining Semi-Automatic Label Generation Tool](https://arxiv.org/abs/2506.09083)
*Jerry Lin,Partick P. W. Chen*

Main category: cs.CV

TL;DR: 本文介绍了BakuFlow，一种改进的半自动标注生成工具，它具有多种创新功能，旨在提高标注效率和灵活性。


<details>
  <summary>Details</summary>
Motivation: 准确标注数据仍然是计算机视觉中的瓶颈，特别是在大规模任务中，手动标注费时且容易出错。

Method: 本文介绍了BakuFlow，一种半自动标注生成工具，它具有多种功能：（1）一个可调放大镜，用于像素级精确的手动修正；（2）一个交互式数据增强模块，可用于多样化训练数据集；（3）标签传播功能，可以将标注对象迅速复制到连续帧中；（4）自动标注模块，基于改进的YOLOE框架。

Result: BakuFlow在实际的计算机视觉和工业场景中显著减少了标注工作量，提升了标注效率。

Conclusion: 这些创新使得BakuFlow特别适用于对象检测和追踪任务，在实际操作中大幅减少了标注工作量并提高了效率。

Abstract: Accurately labeling (or annotation) data is still a bottleneck in computer
vision, especially for large-scale tasks where manual labeling is
time-consuming and error-prone. While tools like LabelImg can handle the
labeling task, some of them still require annotators to manually label each
image. In this paper, we introduce BakuFlow, a streamlining semi-automatic
label generation tool. Key features include (1) a live adjustable magnifier for
pixel-precise manual corrections, improving user experience; (2) an interactive
data augmentation module to diversify training datasets; (3) label propagation
for rapidly copying labeled objects between consecutive frames, greatly
accelerating annotation of video data; and (4) an automatic labeling module
powered by a modified YOLOE framework. Unlike the original YOLOE, our extension
supports adding new object classes and any number of visual prompts per class
during annotation, enabling flexible and scalable labeling for dynamic,
real-world datasets. These innovations make BakuFlow especially effective for
object detection and tracking, substantially reducing labeling workload and
improving efficiency in practical computer vision and industrial scenarios.

</details>


### [37] [Bias Analysis in Unconditional Image Generative Models](https://arxiv.org/abs/2506.09106)
*Xiaofeng Zhang,Michelle Lin,Simon Lacoste-Julien,Aaron Courville,Yash Goyal*

Main category: cs.CV

TL;DR: 通过分析无条件图像生成模型，研究揭示了偏见检测的敏感度问题，尤其是在连续性属性上。研究表明需要改善评估框架并更深入理解属性的社会复杂性。


<details>
  <summary>Details</summary>
Motivation: 尽管关于人工智能模型偏见问题的文献越来越多，但关于无条件生成中偏见产生的机制仍然不清晰。研究旨在探讨这一问题，特别是在无条件图像生成中的偏见变化。

Method: 我们定义了一个属性的偏见为该属性在观察分布中的存在概率与其在理想参考分布中的期望比例之间的差异。研究通过训练若干无条件图像生成模型，并采用一个常用的偏见评估框架来研究训练分布和生成分布之间的偏见变化。

Result: 实验结果显示，检测到的属性转移相对较小。属性转移对用于标记生成图像的分类器非常敏感，尤其是在决策边界位于高密度区域时。研究还发现在连续性属性上这种分类器敏感度常被观察到，而非显示出二元特性。

Conclusion: 研究强调了需要更具有代表性的标注实践，对评估框架的不足进行更深入的审查，并认识到在评价偏见时属性的社会复杂性。

Abstract: The widespread adoption of generative AI models has raised growing concerns
about representational harm and potential discriminatory outcomes. Yet, despite
growing literature on this topic, the mechanisms by which bias emerges -
especially in unconditional generation - remain disentangled. We define the
bias of an attribute as the difference between the probability of its presence
in the observed distribution and its expected proportion in an ideal reference
distribution. In our analysis, we train a set of unconditional image generative
models and adopt a commonly used bias evaluation framework to study bias shift
between training and generated distributions. Our experiments reveal that the
detected attribute shifts are small. We find that the attribute shifts are
sensitive to the attribute classifier used to label generated images in the
evaluation framework, particularly when its decision boundaries fall in
high-density regions. Our empirical analysis indicates that this classifier
sensitivity is often observed in attributes values that lie on a spectrum, as
opposed to exhibiting a binary nature. This highlights the need for more
representative labeling practices, understanding the shortcomings through
greater scrutiny of evaluation frameworks, and recognizing the socially complex
nature of attributes when evaluating bias.

</details>


### [38] [CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation](https://arxiv.org/abs/2506.09109)
*Arnav Yayavaram,Siddharth Yayavaram,Simran Khanuja,Michael Saxon,Graham Neubig*

Main category: cs.CV

TL;DR: 引入新型评估度量CAIRe，计算给定标签集下的图像文化相关性，克服了文化偏见的测量瓶颈，实验证明了它与人类评估的一致性。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型在不同文化背景下的一致性能变得尤为重要，但目前存在性能下降、事实不准确或输出冒犯性等方面的挑战。由于缺乏可靠评估文化偏见的方法，这一问题难以解决。

Method: CAIRe评估指标用于评估给定用户定义标签集的图像的文化相关性程度，通过将图像中的实体和概念与知识库进行关联，并使用事实信息为每个文化标签提供独立的分级判断。

Result: 在手动创建的数据集上，CAIRe超过所有基线模型28个百分点的F1得分。另外，对于两种普遍概念的数据集，CAIRe与人类评分达到0.56和0.66的皮尔逊相关系数，彰显出其对大规模图像源人类判断的强大一致性。

Conclusion: CAIRe作为一种新的评估度量标准，可以有效地衡量文本到图像生成中产生的文化相关性，其结果与人类的评价高度一致。

Abstract: As text-to-image models become increasingly prevalent, ensuring their
equitable performance across diverse cultural contexts is critical. Efforts to
mitigate cross-cultural biases have been hampered by trade-offs, including a
loss in performance, factual inaccuracies, or offensive outputs. Despite
widespread recognition of these challenges, an inability to reliably measure
these biases has stalled progress. To address this gap, we introduce CAIRe, a
novel evaluation metric that assesses the degree of cultural relevance of an
image, given a user-defined set of labels. Our framework grounds entities and
concepts in the image to a knowledge base and uses factual information to give
independent graded judgments for each culture label. On a manually curated
dataset of culturally salient but rare items built using language models, CAIRe
surpasses all baselines by 28% F1 points. Additionally, we construct two
datasets for culturally universal concept, one comprising of T2I-generated
outputs and another retrieved from naturally occurring data. CAIRe achieves
Pearson's correlations of 0.56 and 0.66 with human ratings on these sets, based
on a 5-point Likert scale of cultural relevance. This demonstrates its strong
alignment with human judgment across diverse image sources.

</details>


### [39] [Seedance 1.0: Exploring the Boundaries of Video Generation Models](https://arxiv.org/abs/2506.09113)
*Yu Gao,Haoyuan Guo,Tuyen Hoang,Weilin Huang,Lu Jiang,Fangyuan Kong,Huixia Li,Jiashi Li,Liang Li,Xiaojie Li,Xunsong Li,Yifu Li,Shanchuan Lin,Zhijie Lin,Jiawei Liu,Shu Liu,Xiaonan Nie,Zhiwu Qing,Yuxi Ren,Li Sun,Zhi Tian,Rui Wang,Sen Wang,Guoqiang Wei,Guohong Wu,Jie Wu,Ruiqi Xia,Fei Xiao,Xuefeng Xiao,Jiangqiao Yan,Ceyuan Yang,Jianchao Yang,Runkai Yang,Tao Yang,Yihang Yang,Zilyu Ye,Xuejiao Zeng,Yan Zeng,Heng Zhang,Yang Zhao,Xiaozheng Zheng,Peihao Zhu,Jiaxin Zou,Feilong Zuo*

Main category: cs.CV

TL;DR: Seedance 1.0 是一个高性能的视频生成模型，通过多源数据整理、架构设计以及优化技术实现快速和高质量的视频生成。


<details>
  <summary>Details</summary>
Motivation: 当前的视频生成模型在遵循指令、运动合理性和视觉质量之间难以平衡，此报告旨在介绍Seedance 1.0在这些方面的改进。

Method: Seedance 1.0 通过多源数据整理、高效的架构设计、优化的后训练方法以及多阶段知识蒸馏策略实现模型加速，从而提升视频生成的质量和速度。

Result: Seedance 1.0 能够在复杂多主体的情境中准确遵循指令，生成具有时空连贯性和主体一致性高质量视频，比现有最先进的视频生成模型快10倍。

Conclusion: Seedance 1.0 实现了高质量与快速视频生成的结合，具有显著的时空流畅性和结构稳定性，在复杂多主体情境中保持指令精确遵循和多场景叙事连贯性。

Abstract: Notable breakthroughs in diffusion modeling have propelled rapid improvements
in video generation, yet current foundational model still face critical
challenges in simultaneously balancing prompt following, motion plausibility,
and visual quality. In this report, we introduce Seedance 1.0, a
high-performance and inference-efficient video foundation generation model that
integrates several core technical improvements: (i) multi-source data curation
augmented with precision and meaningful video captioning, enabling
comprehensive learning across diverse scenarios; (ii) an efficient architecture
design with proposed training paradigm, which allows for natively supporting
multi-shot generation and jointly learning of both text-to-video and
image-to-video tasks. (iii) carefully-optimized post-training approaches
leveraging fine-grained supervised fine-tuning, and video-specific RLHF with
multi-dimensional reward mechanisms for comprehensive performance improvements;
(iv) excellent model acceleration achieving ~10x inference speedup through
multi-stage distillation strategies and system-level optimizations. Seedance
1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds
(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance
1.0 stands out with high-quality and fast video generation having superior
spatiotemporal fluidity with structural stability, precise instruction
adherence in complex multi-subject contexts, native multi-shot narrative
coherence with consistent subject representation.

</details>


### [40] [Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models](https://arxiv.org/abs/2506.09229)
*Sungwon Hwang,Hyojin Jang,Kinam Kim,Minho Park,Jaegul choo*

Main category: cs.CV

TL;DR: 本文介绍了一种新的跨帧表征对齐技术CREPA，旨在改进视频扩散模型的帧间语义一致性，在实验中展示了其在视觉质量及跨帧一致性上的提升。


<details>
  <summary>Details</summary>
Motivation: 研究指出，尽管在用户级别的视频模型微调以生成反映特定训练数据属性的视频方面存在挑战，但这一方向仍鲜有探索，具有重要的实际意义。

Method: 研究首先提出了一种简单适应于视频模型的REPA方法，并发现其限制在于无法有效保持帧间语义的一致性。为此，引入CREPA技术，该技术通过将当前帧的隐藏状态与相邻帧的外部特征进行对齐，来解决这一问题。

Result: 该研究提出了用于视频扩散模型（VDMs）的跨帧表征对齐（CREPA）技术，以解决现有方法在保留帧间语义一致性方面的不足。实验表明，与传统方法相比，CREPA 能够在大量参数高效微调方法（如LoRA）中提升视频生成的视觉保真度和帧间语义连贯性。研究在多个具有不同属性的数据集上验证了CREPA的适用性。

Conclusion: CREPA作为一项新技术，已在大规模VDMs实验证明了其在视觉保真度和跨帧语义连贯性方面的提升效果，并在不同属性的数据集上展示了其广泛适用性。

Abstract: Fine-tuning Video Diffusion Models (VDMs) at the user level to generate
videos that reflect specific attributes of training data presents notable
challenges, yet remains underexplored despite its practical importance.
Meanwhile, recent work such as Representation Alignment (REPA) has shown
promise in improving the convergence and quality of DiT-based image diffusion
models by aligning, or assimilating, its internal hidden states with external
pretrained visual features, suggesting its potential for VDM fine-tuning. In
this work, we first propose a straightforward adaptation of REPA for VDMs and
empirically show that, while effective for convergence, it is suboptimal in
preserving semantic consistency across frames. To address this limitation, we
introduce Cross-frame Representation Alignment (CREPA), a novel regularization
technique that aligns hidden states of a frame with external features from
neighboring frames. Empirical evaluations on large-scale VDMs, including
CogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual
fidelity and cross-frame semantic coherence when fine-tuned with
parameter-efficient methods such as LoRA. We further validate CREPA across
diverse datasets with varying attributes, confirming its broad applicability.
Project page: https://crepavideo.github.io

</details>


### [41] [PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies](https://arxiv.org/abs/2506.09237)
*Mojtaba Nafez,Amirhossein Koochakian,Arad Maleki,Jafar Habibi,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: 提出了一个对抗鲁棒的PatchGuard异常检测与定位方法，采用ViT架构加入前景感知伪异常样本，并通过对抗训练增强了模型在边缘条件下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的异常检测与定位方法容易受到对抗攻击的影响，原因在于训练数据的局限性，只包含了正常的无标签样本。PatchGuard旨在通过引入伪异常样本和局部掩模来解决这一问题，提升系统在对抗环境下的表现。

Method: PatchGuard, 一种基于前景感知伪异常样本来增强ViT架构的异常检测和定位方法，通过对抗训练优化模型对抗鲁棒性。

Result: 实验结果表明，在多个工业和医疗数据集上，PatchGuard相较于先前的方法在对抗设置下分别提高了53.2%的异常检测性能和68.5%的异常定位性能，并且在非对抗设置下也保持了良好的准确度。

Conclusion: PatchGuard展示了在多种数据集上的优越性能，特别是在对抗环境下显著提升了异常检测和定位的准确性。

Abstract: Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields
that demand high reliability, such as medical imaging and industrial
monitoring. However, current AD and AL approaches are often susceptible to
adversarial attacks due to limitations in training data, which typically
include only normal, unlabeled samples. This study introduces PatchGuard, an
adversarially robust AD and AL method that incorporates pseudo anomalies with
localization masks within a Vision Transformer (ViT)-based architecture to
address these vulnerabilities. We begin by examining the essential properties
of pseudo anomalies, and follow it by providing theoretical insights into the
attention mechanisms required to enhance the adversarial robustness of AD and
AL systems. We then present our approach, which leverages Foreground-Aware
Pseudo-Anomalies to overcome the deficiencies of previous anomaly-aware
methods. Our method incorporates these crafted pseudo-anomaly samples into a
ViT-based framework, with adversarial training guided by a novel loss function
designed to improve model robustness, as supported by our theoretical analysis.
Experimental results on well-established industrial and medical datasets
demonstrate that PatchGuard significantly outperforms previous methods in
adversarial settings, achieving performance gains of $53.2\%$ in AD and
$68.5\%$ in AL, while also maintaining competitive accuracy in non-adversarial
settings. The code repository is available at
https://github.com/rohban-lab/PatchGuard .

</details>


### [42] [UFM: A Simple Path towards Unified Dense Correspondence with Flow](https://arxiv.org/abs/2506.09278)
*Yuchen Zhang,Nikhil Keetha,Chenwei Lyu,Bhuvan Jhamb,Yutian Chen,Yuheng Qiu,Jay Karhade,Shreyas Jha,Yaoyu Hu,Deva Ramanan,Sebastian Scherer,Wenshan Wang*

Main category: cs.CV

TL;DR: UFM, a unified model for image correspondence and optical flow, outperforms specialized models in both accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: To improve performance and generalization for dense image correspondence tasks by unifying wide-baseline image matching and optical flow estimation through a single model.

Method: Unified Flow & Matching model (UFM) uses a simple, generic transformer architecture for dense image correspondence across wide-baseline scenarios and optical flow estimation.

Result: UFM is 28% more accurate than state-of-the-art flow methods and 62% less error and 6.7x faster than dense wide-baseline matchers.

Conclusion: The development of UFM showcases the potential of unified training in achieving better and faster results for both optical flow and dense correspondence tasks, opening new avenues for real-time and multi-modal applications.

Abstract: Dense image correspondence is central to many applications, such as visual
odometry, 3D reconstruction, object association, and re-identification.
Historically, dense correspondence has been tackled separately for
wide-baseline scenarios and optical flow estimation, despite the common goal of
matching content between two images. In this paper, we develop a Unified Flow &
Matching model (UFM), which is trained on unified data for pixels that are
co-visible in both source and target images. UFM uses a simple, generic
transformer architecture that directly regresses the (u,v) flow. It is easier
to train and more accurate for large flows compared to the typical
coarse-to-fine cost volumes in prior work. UFM is 28% more accurate than
state-of-the-art flow methods (Unimatch), while also having 62% less error and
6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to
demonstrate that unified training can outperform specialized approaches across
both domains. This result enables fast, general-purpose correspondence and
opens new directions for multi-modal, long-range, and real-time correspondence
tasks.

</details>


### [43] [Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery](https://arxiv.org/abs/2506.09299)
*Sindhu Boddu,Arindam Mukherjee*

Main category: cs.CV

TL;DR: This paper introduces an optimized YOLOv4-Tiny model for object detection in aerial emergency response imagery. The model is quantized to INT8, reducing its size and enhancing its inference speed without sacrificing accuracy, making it ideal for real-time use on low-power devices.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is the need for a lightweight and energy-efficient object detection model specifically tailored for aerial imagery in emergency response, as existing models are not optimized for these conditions and publicly available datasets are insufficient.

Method: The paper employs the YOLOv4-Tiny model optimized through post-training quantization to INT8 precision for object detection in aerial imagery during emergencies. A custom-curated dataset with 10,820 annotated images specific to emergency scenarios is used for training, addressing the scarcity of publicly available drone-view emergency data.

Result: The quantized YOLOv4-Tiny model achieves comparable detection performance to the YOLOv5-small model but with a significantly reduced model size (from 22.5 MB to 6.4 MB) and 44% faster inference speed.

Conclusion: The quantized YOLOv4-Tiny model is highly suitable for real-time object detection on low-power edge devices in emergency response situations, providing energy efficiency and speed.

Abstract: This paper presents a lightweight and energy-efficient object detection
solution for aerial imagery captured during emergency response situations. We
focus on deploying the YOLOv4-Tiny model, a compact convolutional neural
network, optimized through post-training quantization to INT8 precision. The
model is trained on a custom-curated aerial emergency dataset, consisting of
10,820 annotated images covering critical emergency scenarios. Unlike prior
works that rely on publicly available datasets, we created this dataset
ourselves due to the lack of publicly available drone-view emergency imagery,
making the dataset itself a key contribution of this work. The quantized model
is evaluated against YOLOv5-small across multiple metrics, including mean
Average Precision (mAP), F1 score, inference time, and model size. Experimental
results demonstrate that the quantized YOLOv4-Tiny achieves comparable
detection performance while reducing the model size from 22.5 MB to 6.4 MB and
improving inference speed by 44\%. With a 71\% reduction in model size and a
44\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly
suitable for real-time emergency detection on low-power edge devices.

</details>


### [44] [Efficient Edge Deployment of Quantized YOLOv4-Tiny for Aerial Emergency Object Detection on Raspberry Pi 5](https://arxiv.org/abs/2506.09300)
*Sindhu Boddu,Arindam Mukherjee*

Main category: cs.CV

TL;DR: 量化后的YOLOv4-Tiny模型在Raspberry Pi 5上实现了快速高效的空袭紧急图像对象检测。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估在资源受限的边缘设备上部署和部署实时对象检测模型在空袭紧急图像中的性能。

Method: 使用TensorFlow Lite的后训练量化技术将YOLOv4-Tiny模型量化到INT8精度，并在Raspberry Pi 5上评估了检测速度、功耗和热性能。

Result: 量化模型在每张图片上的推理时间为28.2毫秒，并且平均功耗为13.85瓦，相比于32位浮点格式模型显示出了显著的功耗降低。检测精度在关键的几类紧急情况如救护车、警车、消防车和车祸中的表现仍旧稳健。

Conclusion: 结果表明，低功耗嵌入式AI系统在安全关键的应急响应应用中的实时部署具有潜力。

Abstract: This paper presents the deployment and performance evaluation of a quantized
YOLOv4-Tiny model for real-time object detection in aerial emergency imagery on
a resource-constrained edge device the Raspberry Pi 5. The YOLOv4-Tiny model
was quantized to INT8 precision using TensorFlow Lite post-training
quantization techniques and evaluated for detection speed, power consumption,
and thermal feasibility under embedded deployment conditions. The quantized
model achieved an inference time of 28.2 ms per image with an average power
consumption of 13.85 W, demonstrating a significant reduction in power usage
compared to its FP32 counterpart. Detection accuracy remained robust across key
emergency classes such as Ambulance, Police, Fire Engine, and Car Crash. These
results highlight the potential of low-power embedded AI systems for real-time
deployment in safety-critical emergency response applications.

</details>


### [45] [MSSDF: Modality-Shared Self-supervised Distillation for High-Resolution Multi-modal Remote Sensing Image Learning](https://arxiv.org/abs/2506.09327)
*Tong Wang,Guanzhou Chen,Xiaodong Zhang,Chenxi Liu,Jiaqi Wang,Xiaoliang Tan,Wenchao Guo,Qingyuan Yang,Kaiqi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种多模态自监督学习框架，用于遥感图像的预训练，该框架在多个下游任务中表现出色，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 遥感图像解释对于环境监测、城市规划和灾害评估至关重要，但获取高质量的标注数据既耗时又昂贵。为此，本文提出了一种新的学习框架来解决这一问题。

Method: 该框架利用高分辨率RGB图像、多光谱数据和数字表面模型（DSM），引入了信息感知自适应掩模策略、跨模态掩模机制和多任务自监督目标。

Result: 评估结果显示该方法在大多数任务上优于现有的预训练方法，具体在Potsdam和Vaihingen语义分割任务中使用50%训练集达成的mIoU分数分别达到了78.30%和76.50%，在SECOND数据集二元变化检测任务中mIoU分数达到47.51%。

Conclusion: 通过实验验证，提出的方法在多个遥感应用任务上表现出优越性，证明了其在遥感图像解释中的应用潜力。

Abstract: Remote sensing image interpretation plays a critical role in environmental
monitoring, urban planning, and disaster assessment. However, acquiring
high-quality labeled data is often costly and time-consuming. To address this
challenge, we proposes a multi-modal self-supervised learning framework that
leverages high-resolution RGB images, multi-spectral data, and digital surface
models (DSM) for pre-training. By designing an information-aware adaptive
masking strategy, cross-modal masking mechanism, and multi-task self-supervised
objectives, the framework effectively captures both the correlations across
different modalities and the unique feature structures within each modality. We
evaluated the proposed method on multiple downstream tasks, covering typical
remote sensing applications such as scene classification, semantic
segmentation, change detection, object detection, and depth estimation.
Experiments are conducted on 15 remote sensing datasets, encompassing 26 tasks.
The results demonstrate that the proposed method outperforms existing
pretraining approaches in most tasks. Specifically, on the Potsdam and
Vaihingen semantic segmentation tasks, our method achieved mIoU scores of
78.30\% and 76.50\%, with only 50\% train-set. For the US3D depth estimation
task, the RMSE error is reduced to 0.182, and for the binary change detection
task in SECOND dataset, our method achieved mIoU scores of 47.51\%, surpassing
the second CS-MAE by 3 percentage points. Our pretrain code, checkpoints, and
HR-Pairs dataset can be found in https://github.com/CVEO/MSSDF.

</details>


### [46] [CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation](https://arxiv.org/abs/2506.09343)
*Yuxing Long,Jiyao Zhang,Mingjie Pan,Tianshu Wu,Taewhan Kim,Hao Dong*

Main category: cs.CV

TL;DR: 研究提出首个基于手册的家电操作基准CheckManual和首个基于手册的操作规划模型ManualPlan，强调机器人通过理解手册操作家电的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的与手册相关的研究局限于问答任务，而现有的操作研究忽视了手册的重要作用，无法理解多页手册。研究旨在提高机器人对手册的理解，进而提升其操作家电的能力。

Method: 本研究提出了一种基于手册的家电操作基准CheckManual，设计了一个大型模型辅助的人类修订数据生成管道来创建基于CAD家电模型的手册。此外，还提出了首个基于手册的操作规划模型ManualPlan，设立了一组基准以评估模型性能。

Result: 建立了新的基于手册的操作挑战、度量标准和模拟环境，为模型性能评估奠定了基础。此外，通过提出ManualPlan模型，设定了CheckManual基准的一组初步基准。

Conclusion: 研究展示了通过机器人理解和学习家电手册来执行复杂任务的潜力，未来的研究将进一步优化模型，提高机器人理解和操作家电的能力。

Abstract: Correct use of electrical appliances has significantly improved human life
quality. Unlike simple tools that can be manipulated with common sense,
different parts of electrical appliances have specific functions defined by
manufacturers. If we want the robot to heat bread by microwave, we should
enable them to review the microwave manual first. From the manual, it can learn
about component functions, interaction methods, and representative task steps
about appliances. However, previous manual-related works remain limited to
question-answering tasks while existing manipulation researchers ignore the
manual's important role and fail to comprehend multi-page manuals. In this
paper, we propose the first manual-based appliance manipulation benchmark
CheckManual. Specifically, we design a large model-assisted human-revised data
generation pipeline to create manuals based on CAD appliance models. With these
manuals, we establish novel manual-based manipulation challenges, metrics, and
simulator environments for model performance evaluation. Furthermore, we
propose the first manual-based manipulation planning model ManualPlan to set up
a group of baselines for the CheckManual benchmark.

</details>


### [47] [An Effective End-to-End Solution for Multimodal Action Recognition](https://arxiv.org/abs/2506.09345)
*Songping Wang,Xiantao Hu,Yueming Lyu,Caifeng Shan*

Main category: cs.CV

TL;DR: A multimodal action recognition solution is proposed, utilizing data enhancement, pre-training with transfer learning, and various prediction enhancement methods, achieving high accuracy on the competition leaderboard.


<details>
  <summary>Details</summary>
Motivation: To address the challenges faced in tri-modal action recognition tasks due to the scarcity of tri-modal data, a comprehensive multimodal action recognition solution is proposed.

Method: First, existing data are transformed and expanded via optimized data enhancement techniques. More RGB datasets are used for pre-training the backbone network through transfer learning. Secondly, multimodal spatial features are extracted using 2D CNNs and combined with TSM for multimodal spatial-temporal feature extraction and improved computational efficiency. Common prediction enhancement methods, such as SWA, Ensemble, and TTA, are employed to integrate the knowledge of models from different training periods of the same architecture and different architectures for comprehensive action prediction.

Result: The approach achieved a Top-1 accuracy of 99% and a Top-5 accuracy of 100% on the competition leaderboard.

Conclusion: The proposed solution showcases superiority in tri-modal action recognition tasks by effectively utilizing multimodal information and improving computational efficiency.

Abstract: Recently, multimodal tasks have strongly advanced the field of action
recognition with their rich multimodal information. However, due to the
scarcity of tri-modal data, research on tri-modal action recognition tasks
faces many challenges. To this end, we have proposed a comprehensive multimodal
action recognition solution that effectively utilizes multimodal information.
First, the existing data are transformed and expanded by optimizing data
enhancement techniques to enlarge the training scale. At the same time, more
RGB datasets are used to pre-train the backbone network, which is better
adapted to the new task by means of transfer learning. Secondly, multimodal
spatial features are extracted with the help of 2D CNNs and combined with the
Temporal Shift Module (TSM) to achieve multimodal spatial-temporal feature
extraction comparable to 3D CNNs and improve the computational efficiency. In
addition, common prediction enhancement methods, such as Stochastic Weight
Averaging (SWA), Ensemble and Test-Time augmentation (TTA), are used to
integrate the knowledge of models from different training periods of the same
architecture and different architectures, so as to predict the actions from
different perspectives and fully exploit the target information. Ultimately, we
achieved the Top-1 accuracy of 99% and the Top-5 accuracy of 100% on the
competition leaderboard, demonstrating the superiority of our solution.

</details>


### [48] [Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation](https://arxiv.org/abs/2506.09350)
*Shanchuan Lin,Ceyuan Yang,Hao He,Jianwen Jiang,Yuxi Ren,Xin Xia,Yang Zhao,Xuefeng Xiao,Lu Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种自回归对抗后训练（AAPT）方法，将预训练的潜在视频扩散模型转换为实时、交互式的视频生成器。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模视频生成模型计算密集，限制了它们在实时和交互式应用中的使用。我们提出的方法旨在解决这一问题，使其能够在实时应用中使用。

Method: Structure

Result: {\"tldr\": \"本文提出了一种自回归对抗后训练（AAPT）方法，将预训练的潜在视频扩散模型转换为实时、交互式的视频生成器。\", \"motivation\": \"现有的大规模视频生成模型计算密集，限制了它们在实时和交互式应用中的使用。我们提出的方法旨在解决这一问题，使其能够在实时应用中使用。\", \"method\": \"该方法使用单个神经函数评估（1NFE）逐帧生成潜在帧，并使用对抗性训练作为自回归生成的有效范式。\", \"result\": \"实验表明，我们的模型能够实现实时（24fps），流式视频生成，在单个H100上达到736x416分辨率，或在8xH100上达到1280x720分辨率，最长可达一分钟。\", \"conclusion\": \"该方法证明了在长视频生成过程中，对抗性训练可以有效减少误差积累，并且使其在实时和交互式环境中可行。\"}

Conclusion: 该方法证明了在长视频生成过程中，对抗性训练可以有效减少误差积累，并且使其在实时和交互式环境中可行。

Abstract: Existing large-scale video generation models are computationally intensive,
preventing adoption in real-time and interactive applications. In this work, we
propose autoregressive adversarial post-training (AAPT) to transform a
pre-trained latent video diffusion model into a real-time, interactive video
generator. Our model autoregressively generates a latent frame at a time using
a single neural function evaluation (1NFE). The model can stream the result to
the user in real time and receive interactive responses as controls to generate
the next latent frame. Unlike existing approaches, our method explores
adversarial training as an effective paradigm for autoregressive generation.
This not only allows us to design an architecture that is more efficient for
one-step generation while fully utilizing the KV cache, but also enables
training the model in a student-forcing manner that proves to be effective in
reducing error accumulation during long video generation. Our experiments
demonstrate that our 8B model achieves real-time, 24fps, streaming video
generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to
a minute long (1440 frames). Visit our research website at
https://seaweed-apt.com/2

</details>


### [49] [A new approach for image segmentation based on diffeomorphic registration and gradient fields](https://arxiv.org/abs/2506.09357)
*Junchao Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种基于微分同胚变换的2D图像分割的新方法，该方法使用LDDMM框架下的变分方法，通过将模板曲线变形为图像域中的曲线来进行图像分割，实现了对理论基础和灵活性的结合。


<details>
  <summary>Details</summary>
Motivation: 传统的边缘检测和变分方法已经被广泛研究，而最近的深度学习方法虽然显示出了有希望的结果，但通常需要大量的训练数据。本文旨在提出一个不需要大量数据集且理论基础坚实的图像分割方法。

Method: 本文提出了一种新颖的2D图像分割变分框架，该框架结合了形状分析和微分同胚变换的概念。该方法通过LDDMM框架下的微分同胚变换将模板曲线变形为图像域中的曲线，使用PyKeops库实现Python中基于GPU的加速，从而实现了依赖于理论基础灵活准确地分割图像，而无需大量数据集。

Result: 该方法实现了基于理论基础和灵活性的图像分割，不依赖于大量的数据集。

Conclusion: 本文提出的方法为2D图像分割提供了一种新的理论基础深厚且灵活的方法论，从而减少了对大量训练数据的依赖。

Abstract: Image segmentation is a fundamental task in computer vision aimed at
delineating object boundaries within images. Traditional approaches, such as
edge detection and variational methods, have been widely explored, while recent
advances in deep learning have shown promising results but often require
extensive training data. In this work, we propose a novel variational framework
for 2D image segmentation that integrates concepts from shape analysis and
diffeomorphic transformations. Our method models segmentation as the
deformation of a template curve via a diffeomorphic transformation of the image
domain, using the Large Deformation Diffeomorphic Metric Mapping (LDDMM)
framework. The curve evolution is guided by a loss function that compares the
deformed curve to the image gradient field, formulated through the varifold
representation of geometric shapes. The approach is implemented in Python with
GPU acceleration using the PyKeops library. This framework allows for accurate
segmentation with a flexible and theoretically grounded methodology that does
not rely on large datasets.

</details>


### [50] [SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing](https://arxiv.org/abs/2506.09363)
*Hongguang Zhu,Yunchao Wei,Mengyu Wang,Siyu Jiao,Yan Fang,Jiannan Huang,Yao Zhao*

Main category: cs.CV

TL;DR: Introduces SAGE, a method combining semantic-augment erasing and global-local retention mechanism for safer diffusion models


<details>
  <summary>Details</summary>
Motivation: address safety risks such as unsafe content generation and copyright infringement in diffusion models, and avoid the limitation of existing concept erasing methods

Method: semantic-augment erasing and global-local collaborative retention mechanism

Result: SAGE demonstrates comprehensive superiority in the safe generation of diffusion models compared with other methods

Conclusion: SAGE effectively mitigates safety risks and retains performance in irrelevant concepts compared to previous methods

Abstract: Diffusion models (DMs) have achieved significant progress in text-to-image
generation. However, the inevitable inclusion of sensitive information during
pre-training poses safety risks, such as unsafe content generation and
copyright infringement. Concept erasing finetunes weights to unlearn
undesirable concepts, and has emerged as a promising solution. However,
existing methods treat unsafe concept as a fixed word and repeatedly erase it,
trapping DMs in ``word concept abyss'', which prevents generalized
concept-related erasing. To escape this abyss, we introduce semantic-augment
erasing which transforms concept word erasure into concept domain erasure by
the cyclic self-check and self-erasure. It efficiently explores and unlearns
the boundary representation of concept domain through semantic spatial
relationships between original and training DMs, without requiring additional
preprocessed data. Meanwhile, to mitigate the retention degradation of
irrelevant concepts while erasing unsafe concepts, we further propose the
global-local collaborative retention mechanism that combines global semantic
relationship alignment with local predicted noise preservation, effectively
expanding the retentive receptive field for irrelevant concepts. We name our
method SAGE, and extensive experiments demonstrate the comprehensive
superiority of SAGE compared with other methods in the safe generation of DMs.
The code and weights will be open-sourced at
https://github.com/KevinLight831/SAGE.

</details>


### [51] [ScaleLSD: Scalable Deep Line Segment Detection Streamlined](https://arxiv.org/abs/2506.09369)
*Zeran Ke,Bin Tan,Xianwei Zheng,Yujun Shen,Tianfu Wu,Nan Xue*

Main category: cs.CV

TL;DR: 研究开发了ScaleLSD，一种高性能、高效的线段检测模型，专为大规模和多样化的自然图像设计。


<details>
  <summary>Details</summary>
Motivation: 研究目的是学习一个适用于任何自然图像的鲁棒领域无关的线段检测模型，以解决大规模图像的线段几何表征问题。

Method: 此研究提出了ScaleLSD，一种专注于在大规模无标签现实图像中学习线段检测的自监督学习方法。ScaleLSD通过复习和简化深度和非深度LSD方法的基本设计，实现高性能和高效的线段检测。

Result: 实验表明，ScaleLSD在零样本图像检测性能，单视角3D几何估计，双视角线段匹配和多视角3D线映射方面表现出色，首次超越了传统的非深度线段检测方法。

Conclusion: 研究得出了ScaleLSD是第一个在所测试的所有方面都优于原有的非深度LSD的深度学习方法，显著扩展和增强了图像线几何的多功能性。

Abstract: This paper studies the problem of Line Segment Detection (LSD) for the
characterization of line geometry in images, with the aim of learning a
domain-agnostic robust LSD model that works well for any natural images. With
the focus of scalable self-supervised learning of LSD, we revisit and
streamline the fundamental designs of (deep and non-deep) LSD approaches to
have a high-performing and efficient LSD learner, dubbed as ScaleLSD, for the
curation of line geometry at scale from over 10M unlabeled real-world images.
Our ScaleLSD works very well to detect much more number of line segments from
any natural images even than the pioneered non-deep LSD approach, having a more
complete and accurate geometric characterization of images using line segments.
Experimentally, our proposed ScaleLSD is comprehensively testified under
zero-shot protocols in detection performance, single-view 3D geometry
estimation, two-view line segment matching, and multiview 3D line mapping, all
with excellent performance obtained. Based on the thorough evaluation, our
ScaleLSD is observed to be the first deep approach that outperforms the
pioneered non-deep LSD in all aspects we have tested, significantly expanding
and reinforcing the versatility of the line geometry of images. Code and Models
are available at https://github.com/ant-research/scalelsd

</details>


### [52] [UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images](https://arxiv.org/abs/2506.09378)
*Qijian Tian,Xin Tan,Jingyu Gong,Yuan Xie,Lizhuang Ma*

Main category: cs.CV

TL;DR: This paper introduces UniForward, a model that reconstructs 3D scenes and semantic fields from sparse-view images in real time, achieving high-quality results without requiring camera parameters or depth information, and demonstrating state-of-the-art performance in novel view synthesis and segmentation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the challenges of embedding semantics into 3D representations, achieving real-time reconstruction, and ensuring the practical applicability using only images as inputs without additional information like camera parameters or ground truth depth.

Method: Our method, UniForward, is a feed-forward model designed to predict 3D Gaussians with anisotropic semantic features from uncalibrated, sparse-view images. It employs a dual-branch decoupled decoder to unify the 3D scene and semantic field representation, and uses a loss-guided view sampler for training stability.

Result: Experiments have shown that UniForward can reconstruct 3D scenes and semantic fields in real-time from sparse-view images, achieving high-quality rendering and view-consistent semantic features with state-of-the-art performance.

Conclusion: The conclusion is that the proposed UniForward model effectively unifies 3D scene and semantic field reconstruction by predicting anisotropic semantic features in 3D Gaussians, from sparse-view input images, and achieves state-of-the-art performance without needing camera parameters or ground truth depth.

Abstract: We propose a feed-forward Gaussian Splatting model that unifies 3D scene and
semantic field reconstruction. Combining 3D scenes with semantic fields
facilitates the perception and understanding of the surrounding environment.
However, key challenges include embedding semantics into 3D representations,
achieving generalizable real-time reconstruction, and ensuring practical
applicability by using only images as input without camera parameters or ground
truth depth. To this end, we propose UniForward, a feed-forward model to
predict 3D Gaussians with anisotropic semantic features from only uncalibrated
and unposed sparse-view images. To enable the unified representation of the 3D
scene and semantic field, we embed semantic features into 3D Gaussians and
predict them through a dual-branch decoupled decoder. During training, we
propose a loss-guided view sampler to sample views from easy to hard,
eliminating the need for ground truth depth or masks required by previous
methods and stabilizing the training process. The whole model can be trained
end-to-end using a photometric loss and a distillation loss that leverages
semantic features from a pre-trained 2D semantic model. At the inference stage,
our UniForward can reconstruct 3D scenes and the corresponding semantic fields
in real time from only sparse-view images. The reconstructed 3D scenes achieve
high-quality rendering, and the reconstructed 3D semantic field enables the
rendering of view-consistent semantic features from arbitrary views, which can
be further decoded into dense segmentation masks in an open-vocabulary manner.
Experiments on novel view synthesis and novel view segmentation demonstrate
that our method achieves state-of-the-art performances for unifying 3D scene
and semantic field reconstruction.

</details>
