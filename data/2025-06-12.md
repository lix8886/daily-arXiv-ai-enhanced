<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 66]
- [cs.CV](#cs.CV) [Total: 58]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LLM-as-a-qualitative-judge: automating error analysis in natural language generation](https://arxiv.org/abs/2506.09147)
*Nadezhda Chirkova,Tunde Oluwaseyi Ajayi,Seth Aycock,Zain Muhammad Mujahid,Vladana Perlić,Ekaterina Borisova,Markarit Vartampetian*

Main category: cs.CL

本文提出LLM-as-a-qualitative-judge，一个基于LLM的评估方法，其主要输出是对NLG系统输出中的常见问题类型的结构化报告，并展示了该方法识别特定实例问题的情况。


<details>
  <summary>Details</summary>
Motivation: 当前，将LLM用于评估生成文本的方法主要是定量工具，主要输出是数值分数。本文旨在提出一种定性评估方法，为目标提供有关如何改进给定的NLG系统的有意义见解。

Method: 提出的方法包括两个主要步骤：开放式逐实例问题分析和使用直观的累积算法对发现的问题进行聚类。

Result: 实验结果显示，LLM-as-a-qualitative-judge在2/3的情况下正确地识别了实例特定的问题，并且能够产生类似于人类标注员编制的错误类型报告。

Conclusion: 研究证明了LLM除了作为定量评估工具外，也可以作为定性评估工具，以提供结构化的问题报告，进而帮助开发者理解和改进NLG系统。

Abstract: Prompting large language models (LLMs) to evaluate generated text, known as
LLM-as-a-judge, has become a standard evaluation approach in natural language
generation (NLG), but is primarily used as a quantitative tool, i.e. with
numerical scores as main outputs. In this work, we propose
LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main
output being a structured report of common issue types in the NLG system
outputs. Our approach is targeted at providing developers with meaningful
insights on what improvements can be done to a given NLG system and consists of
two main steps, namely open-ended per-instance issue analysis and clustering of
the discovered issues using an intuitive cumulative algorithm. We also
introduce a strategy for evaluating the proposed approach, coupled with ~300
annotations of issues in instances from 12 NLG datasets. Our results show that
LLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3
cases and is capable of producing error type reports resembling the reports
composed by human annotators. Our code and data are publicly available at
https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.

</details>


### [2] [PHRASED: Phrase Dictionary Biasing for Speech Translation](https://arxiv.org/abs/2506.09175)
*Peidong Wang,Jian Xue,Rui Zhao,Junkun Chen,Aswin Shanmugam Subramanian,Jinyu Li*

Main category: cs.CL

本文提出了一种短语字典偏置方法，提高了短语在语音翻译中的准确性和召回率，相较于短语列表偏置方法，提升了21%的相对性能，并使多模态大型语言模型的短语召回率提高了85%。


<details>
  <summary>Details</summary>
Motivation: 短语对于理解对话中的核心概念至关重要，但由于其在训练数据中的罕见出现，短语的准确翻译在语音翻译任务中颇具挑战。

Method: 本文提出了一种短语字典偏置方法，以利用从源语言到目标语言的短语映射对。该方法应用到了两种广泛使用的模型中：基于转导器的流式语音翻译模型和多模态大型语言模型。

Result: 实验结果显示，短语字典偏置方法比短语列表偏置方法相对提高了21%的性能。此外，短语字典偏置使多模态大型语言模型能够使用外部短语信息，实现了85%的相对召回率提升。

Conclusion: 短语字典偏置方法显著提升了语音翻译模型和多模态大型语言模型在短语翻译上的性能。

Abstract: Phrases are essential to understand the core concepts in conversations.
However, due to their rare occurrence in training data, correct translation of
phrases is challenging in speech translation tasks. In this paper, we propose a
phrase dictionary biasing method to leverage pairs of phrases mapping from the
source language to the target language. We apply the phrase dictionary biasing
method to two types of widely adopted models, a transducer-based streaming
speech translation model and a multimodal large language model. Experimental
results show that the phrase dictionary biasing method outperforms phrase list
biasing by 21% relatively for the streaming speech translation model. In
addition, phrase dictionary biasing enables multimodal large language models to
use external phrase information, achieving 85% relative improvement in phrase
recall.

</details>


### [3] [A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs](https://arxiv.org/abs/2506.09218)
*Bruno Ferenc Šegedin*

Main category: cs.CL

A study examining the capacity of CNNs to generate phonotactic generalizations from raw audio data finds that, with a narrow fully-connected layer, convolutional layers can dynamically generalize phonetic dependencies beyond the specific learned lexical configurations.


<details>
  <summary>Details</summary>
Motivation: The motivation of this study is to understand if DNNs, particularly CNNs trained on audio waveforms, can make generalizations about phonotactics that go beyond the specific instances of words they've been trained on, and to examine how the architecture of the neural network influences this ability.

Method: This study uses generative convolutional neural networks (CNNs) trained on raw audio waveforms of lexical items. It explores the impact of a narrow fully-connected layer (FC) bottleneck (shrunk from 1024 channels to 8) on the model's ability to generalize phonotactic rules derived from lexical learning.

Result: The study finds that the convolutional layers can dynamically generalize phonetic dependencies beyond lexically-constrained configurations. A novel technique for probing the model's lexically-independent generalizations was proposed, which bypasses the FC layer and uses randomized feature maps fed into the convolutional block to generate audio outputs.

Conclusion: The results suggest that generative convolutional neural networks are capable of learning and applying phonotactic generalizations beyond the specific lexical items they are trained on, especially when the fully-connected layer is constrained.

Abstract: The ability of deep neural networks (DNNs) to represent phonotactic
generalizations derived from lexical learning remains an open question. This
study (1) investigates the lexically-invariant generalization capacity of
generative convolutional neural networks (CNNs) trained on raw audio waveforms
of lexical items and (2) explores the consequences of shrinking the
fully-connected layer (FC) bottleneck from 1024 channels to 8 before training.
Ultimately, a novel technique for probing a model's lexically-independent
generalizations is proposed that works only under the narrow FC bottleneck:
generating audio outputs by bypassing the FC and inputting randomized feature
maps into the convolutional block. These outputs are equally biased by a
phonotactic restriction in training as are outputs generated with the FC. This
result shows that the convolutional layers can dynamically generalize phonetic
dependencies beyond lexically-constrained configurations learned by the FC.

</details>


### [4] [Extrapolation by Association: Length Generalization Transfer in Transformers](https://arxiv.org/abs/2506.09251)
*Ziyang Cai,Nayoung Lee,Avi Schwarzschild,Samet Oymak,Dimitris Papailiopoulos*

Main category: cs.CL

研究通过任务关联展示了Transformer模型的长度泛化能力可以跨任务转移，并提供了机制上的初步证据。


<details>
  <summary>Details</summary>
Motivation: 探讨Transformer语言模型处理更长输入的能力是如何形成的，特别是在未见过的更长输入上如何泛化的问题。

Method: 通过任务关联的视角研究了模型处理更长输入的能力，即长度泛化能力。研究表明，训练模型时使用一个较长且相关的辅助任务，可以使模型在其他目标任务中更好地泛化到未见过的更长输入。

Result: 长度泛化能力能够在不同的算法任务之间转移，包括算术运算、字符串转换和迷宫导航任务。预训练语言模型中也观察到了类似的转移效果。

Conclusion: 模型可以继承来自相似任务的泛化能力，并且长度泛化的转移与任务间注意力头的重用有关，这加深了对模型如何泛化到分布外输入的理解。

Abstract: Transformer language models have demonstrated impressive generalization
capabilities in natural language domains, yet we lack a fine-grained
understanding of how such generalization arises. In this paper, we investigate
length generalization--the ability to extrapolate from shorter to longer
inputs--through the lens of \textit{task association}. We find that length
generalization can be \textit{transferred} across related tasks. That is,
training a model with a longer and related auxiliary task can lead it to
generalize to unseen and longer inputs from some other target task. We
demonstrate this length generalization transfer across diverse algorithmic
tasks, including arithmetic operations, string transformations, and maze
navigation. Our results show that transformer models can inherit generalization
capabilities from similar tasks when trained jointly. Moreover, we observe
similar transfer effects in pretrained language models, suggesting that
pretraining equips models with reusable computational scaffolding that
facilitates extrapolation in downstream settings. Finally, we provide initial
mechanistic evidence that length generalization transfer correlates with the
re-use of the same attention heads between the tasks. Together, our findings
deepen our understanding of how transformers generalize to out-of-distribution
inputs and highlight the compositional reuse of inductive structure across
tasks.

</details>


### [5] [Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat](https://arxiv.org/abs/2506.09259)
*Zhuofang Li,Rafal Kocielnik,Fereshteh Soltani,Penphob,Boonyarungsrit,Animashree Anandkumar,R. Michael Alvarez*

Main category: cs.CL

研究提出了自我锚定注意力模型（SAAM）来识别并分类游戏中亲社会行为，并将其应用于《使命召唤：现代战争II》中，提高了亲社会沟通行为的识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要集中在使用自然语言处理技术检测游戏中少量的有毒内容以实现管理目的，而新兴的研究强调了检测亲社会行为的重要性。然而，在游戏聊天文本中识别亲社会行为的数据集、模型和资源相当匮乏。研究旨在解决这一低资源环境下的问题，促进积极互动的鼓励。

Method: 采用了无监督的发现方法，并结合游戏领域专家的合作，从游戏中玩家的聊天记录中识别和分类亲社会行为。提出了自我锚定注意力模型（Self-Anchored Attention Model，SAAM），该模型相较于现有最佳技术提高了7.9%。通过使用整个训练集作为“锚点”来帮助在缺乏大量训练数据的情况下改善模型性能。

Result: 成果包括提出了自我锚定注意力模型（SAAM），相较于现有最佳技术提高了7.9%，在低资源环境中实现了亲社会行为的自动分类。研究展示了在仅有少量化标注数据的情况下，模型仍然有效。

Conclusion: 这篇研究是首次将NLP技术应用于发现和分类玩家在游戏中的亲社会沟通行为的研究，证明了从仅关注减少消极内容到主动鼓励积极互动的可能性。

Abstract: Millions of players engage daily in competitive online games, communicating
through in-game chat. Prior research has focused on detecting relatively small
volumes of toxic content using various Natural Language Processing (NLP)
techniques for the purpose of moderation. However, recent studies emphasize the
importance of detecting prosocial communication, which can be as crucial as
identifying toxic interactions. Recognizing prosocial behavior allows for its
analysis, rewarding, and promotion. Unlike toxicity, there are limited
datasets, models, and resources for identifying prosocial behaviors in
game-chat text. In this work, we employed unsupervised discovery combined with
game domain expert collaboration to identify and categorize prosocial player
behaviors from game chat. We further propose a novel Self-Anchored Attention
Model (SAAM) which gives 7.9% improvement compared to the best existing
technique. The approach utilizes the entire training set as "anchors" to help
improve model performance under the scarcity of training data. This approach
led to the development of the first automated system for classifying prosocial
behaviors in in-game chats, particularly given the low-resource settings where
large-scale labeled data is not available. Our methodology was applied to one
of the most popular online gaming titles - Call of Duty(R): Modern
Warfare(R)II, showcasing its effectiveness. This research is novel in applying
NLP techniques to discover and classify prosocial behaviors in player in-game
chat communication. It can help shift the focus of moderation from solely
penalizing toxicity to actively encouraging positive interactions on online
platforms.

</details>


### [6] [Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models](https://arxiv.org/abs/2506.09277)
*Milan Bhan,Jean-Noel Vittaut,Nicolas Chesneau,Sarath Chandar,Marie-Jeanne Lesot*

Main category: cs.CL

The paper develops a framework to measure the faithfulness of LLM self-explanations by comparing them to the model's internal processes, aiming to enhance the reliability of these explanations and foster future improvements.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the fact that current LLM-generated self-NLEs can be logically coherent but not truly reflective of the model's actual decision-making process, leading to unfaithful explanations. This study aims to fill the gap in understanding and improving the faithfulness of these explanations.

Method: This paper proposes a flexible framework to measure the faithfulness of LLM-generated self-Natural Language Explanations (self-NLE) by comparing self-NLE with the interpretations of the model's internal neural activities, thereby offering deep insights into the connection between self-NLE and the model's reasoning process.

Result: The paper establishes a novel approach for quantifying self-NLE faithfulness which connects the explanations to the model's internal reasoning, advancing the understanding of self-NLE authenticity and laying groundwork for the generation of more reliable self-NLE in the future.

Conclusion: The work concludes with a robust framework for assessing self-NLE faithfulness in LLMs by linking the generated explanations to the model's internal neural states, paving the way for more faithful self-NLEs in the future.

Abstract: Large Language Models (LLM) have demonstrated the capability of generating
free text self Natural Language Explanation (self-NLE) to justify their
answers. Despite their logical appearance, self-NLE do not necessarily reflect
the LLM actual decision-making process, making such explanations unfaithful.
While existing methods for measuring self-NLE faithfulness mostly rely on
behavioral tests or computational block identification, none of them examines
the neural activity underlying the model's reasoning. This work introduces a
novel flexible framework for quantitatively measuring the faithfulness of
LLM-generated self-NLE by directly comparing the latter with interpretations of
the model's internal hidden states. The proposed framework is versatile and
provides deep insights into self-NLE faithfulness by establishing a direct
connection between self-NLE and model reasoning. This approach advances the
understanding of self-NLE faithfulness and provides building blocks for
generating more faithful self-NLE.

</details>


### [7] [$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding](https://arxiv.org/abs/2506.09301)
*Cesare Spinoso-Di Piano,David Austin,Pablo Piantanida,Jackie Chi Kit Cheung*

Main category: cs.CL

本文提出了$(RSA)^2$框架，通过考虑说话人的修辞策略来解释比喻语言，避免了对动机的特定场景化建模，并在讽刺语言理解上达到了先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有的RSA框架无法解释比喻表达，或者需要以特定场景的方式模拟说话人使用比喻语言的隐含动机。为了人类兼容地解释非字面语言，本文提出了$(RSA)^2$框架。

Method: 本文介绍了$(RSA)^2$框架，它通过考虑说话人使用的修辞策略来建模比喻语言的使用。

Result: $(RSA)^2$框架与大语言模型结合，在新引入的PragMega+数据集的讽刺分割上实现了最先进的性能。

Conclusion: 该研究展示了$(RSA)^2$框架在理解非字面语言方面的优势，无需模拟说话人使用非字面语言的具体动机，即可实现人类兼容的解释。

Abstract: Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in
human communication, resulting in utterances where the literal and the intended
meanings do not match. The Rational Speech Act (RSA) framework, which
explicitly models speaker intentions, is the most widespread theory of
probabilistic pragmatics, but existing implementations are either unable to
account for figurative expressions or require modeling the implicit motivations
for using figurative language (e.g., to express joy or annoyance) in a
setting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware
RSA $(RSA)^2$ framework which models figurative language use by considering a
speaker's employed rhetorical strategy. We show that $(RSA)^2$ enables
human-compatible interpretations of non-literal utterances without modeling a
speaker's motivations for being non-literal. Combined with LLMs, it achieves
state-of-the-art performance on the ironic split of PragMega+, a new irony
interpretation dataset introduced in this study.

</details>


### [8] [Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models](https://arxiv.org/abs/2506.09315)
*Yao Xiao,Heidi Christensen,Stefan Goetze*

Main category: cs.CL

本研究使用Mistral-7B模型增强了阿尔茨海默病的语言检测能力，提高了检测准确性，并展示了清晰的决策边界。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病是一种影响认知能力的神经退行性疾病，通常会影响语言能力。这项研究的目的是提高阿尔茨海默病的检测准确性。

Method: 本研究使用了Mistral-7B指令跟随版大型语言模型，扩展了检测阿尔茨海默病的成对困惑度方法。

Result: 相比于当前最佳成对困惑度方法和ADReSS 2020挑战基准测试中表现最好的方法，准确率分别提高了3.33%和6.35%。

Conclusion: 提出的方法可以有效检测阿尔茨海默病，并且具有清晰且可解释的决策边界，模型已经学会了阿尔茨海默病患者特有的语言模式，为模型解释和数据增强提供了新的方法。

Abstract: Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive
decline that commonly impacts language ability. This work extends the paired
perplexity approach to detecting AD by using a recent large language model
(LLM), the instruction-following version of Mistral-7B. We improve accuracy by
an average of 3.33% over the best current paired perplexity method and by 6.35%
over the top-ranked method from the ADReSS 2020 challenge benchmark. Our
further analysis demonstrates that the proposed approach can effectively detect
AD with a clear and interpretable decision boundary in contrast to other
methods that suffer from opaque decision-making processes. Finally, by
prompting the fine-tuned LLMs and comparing the model-generated responses to
human responses, we illustrate that the LLMs have learned the special language
patterns of AD speakers, which opens up possibilities for novel methods of
model interpretation and data augmentation.

</details>


### [9] [Towards Efficient and Effective Alignment of Large Language Models](https://arxiv.org/abs/2506.09329)
*Yuxin Jiang*

Main category: cs.CL

The paper presents new methodologies for data collection, training, and evaluation to improve LLM alignment with human expectations, introducing frameworks like Lion, WebR, LTE, and BMC and the FollowBench for constraint adherence.


<details>
  <summary>Details</summary>
Motivation: To efficiently and effectively align LLMs with human expectations.

Method: Lion, an adversarial distillation framework for alignment data collection; Web Reconstruction (WebR), a fully automated framework for synthesizing instruction-tuning data; Learning to Edit (LTE), a framework for efficient knowledge integration; Bridging and Modeling Correlations (BMC), a refinement of Direct Preference Optimization (DPO) for better alignment across QA and mathematical reasoning tasks.

Result: State-of-the-art zero-shot reasoning with Lion; improved data diversity and scalability with WebR; improved real-time and batch knowledge updates with LTE; superior alignment across QA and mathematical reasoning tasks with BMC; key weaknesses in current models' constraint adherence identified with FollowBench.

Conclusion: The paper proposes and evaluates several novel methodologies in data collection, training, and evaluation for LLM alignment, significantly advancing the state-of-the-art in zero-shot reasoning, knowledge integration, and constraint adherence.

Abstract: Large language models (LLMs) exhibit remarkable capabilities across diverse
tasks, yet aligning them efficiently and effectively with human expectations
remains a critical challenge. This thesis advances LLM alignment by introducing
novel methodologies in data collection, training, and evaluation. We first
address alignment data collection. Existing approaches rely heavily on manually
curated datasets or proprietary models. To overcome these limitations, we
propose Lion, an adversarial distillation framework that iteratively refines
training data by identifying and generating challenging instructions, enabling
state-of-the-art zero-shot reasoning. Additionally, we introduce Web
Reconstruction (WebR), a fully automated framework that synthesizes
instruction-tuning data directly from raw web documents, significantly
improving data diversity and scalability over existing synthetic data methods.
Next, we enhance alignment training through novel optimization techniques. We
develop Learning to Edit (LTE), a framework that enables LLMs to efficiently
integrate new knowledge while preserving existing information. LTE leverages
meta-learning to improve both real-time and batch knowledge updates.
Furthermore, we introduce Bridging and Modeling Correlations (BMC), a
refinement of Direct Preference Optimization (DPO) that explicitly captures
token-level correlations in preference data, leading to superior alignment
across QA and mathematical reasoning tasks. Finally, we tackle the challenge of
evaluating alignment. Existing benchmarks emphasize response quality but
overlook adherence to specific constraints. To bridge this gap, we introduce
FollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to
follow complex constraints across diverse instruction types. Our results expose
key weaknesses in current models' constraint adherence, offering insights for
future improvements.

</details>


### [10] [Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation](https://arxiv.org/abs/2506.09331)
*Arjun Vaithilingam Sudhakar*

Main category: cs.CL

研究LLMs是否拥有理论心智，通过在其基础上创建能够自然语言互动的智能体，以实现在多智能体环境下的人工智能和人类的良好协作。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否能模拟和推理他人的意图，即LLMs是否具有某种形式的理论心智。这在人与人以及人与自主系统间的有效协作中至关重要。

Method: 通过多智能体强化学习（MARL）的视角研究LLMs的理论心智，该视角中智能体通过重复互动学习协作，类似于人类的社会推理。

Result: 未直接给出研究成果，但该项研究旨在提升人工智能智能体适应并与其他智能体及人类合作的能力。

Conclusion: 通过基于LLM的智能体来创建人机混合系统，这些系统有朝一日能够实现无缝协作，对人类与人工智能的互动未来具有广泛的意义。

Abstract: Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot
generalization capabilities across complex natural language tasks, enabling
their widespread use as virtual assistants for diverse applications such as
translation and summarization. Despite being trained solely on large corpora of
text without explicit supervision on author intent, LLMs appear to infer the
underlying meaning of textual interactions. This raises a fundamental question:
can LLMs model and reason about the intentions of others, i.e., do they possess
a form of theory of mind? Understanding other's intentions is crucial for
effective collaboration, which underpins human societal success and is
essential for cooperative interactions among multiple agents, including humans
and autonomous systems. In this work, we investigate the theory of mind in LLMs
through the lens of cooperative multi-agent reinforcement learning (MARL),
where agents learn to collaborate via repeated interactions, mirroring human
social reasoning. Our approach aims to enhance artificial agent's ability to
adapt and cooperate with both artificial and human partners. By leveraging
LLM-based agents capable of natural language interaction, we move towards
creating hybrid human-AI systems that can foster seamless collaboration, with
broad implications for the future of human-artificial interaction.

</details>


### [11] [RePO: Replay-Enhanced Policy Optimization](https://arxiv.org/abs/2506.09340)
*Siheng Li,Zhanhui Zhou,Wai Lam,Chao Yang,Chaochao Lu*

Main category: cs.CL

This paper presents Replay-Enhanced Policy Optimization (RePO), a method that improves upon GRPO in terms of data efficiency and computational cost while optimizing large language models for reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to find a more efficient method in terms of computational costs and data usage for optimizing LLMs using reinforcement learning, compared to the existing Group Relative Policy Optimization (GRPO) method.

Method: Replay-Enhanced Policy Optimization (RePO) is introduced to improve data efficiency and reduce computational costs when optimizing large language models (LLMs) for reinforcement learning tasks. It retrieves off-policy samples from a replay buffer to enrich the dataset used for policy optimization.

Result: Experimental results show that RePO achieves significant performance gains over GRPO, with absolute average performance improvements of 18.4 and 4.1 points for Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively. Additionally, for Qwen3-1.7B, the number of effective optimization steps rose by 48% while increasing computational cost by only 15%.

Conclusion: The conclusion is that RePO increases the data efficiency and reduces the computational cost while optimizing large language models for reinforcement learning tasks. It provides significant performance gains with a relatively small increase in computational costs.

Abstract: Reinforcement learning (RL) is vital for optimizing large language models
(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages
using multiple on-policy outputs per prompt, leading to high computational
costs and low data efficiency. To address this, we introduce Replay-Enhanced
Policy Optimization (RePO), which leverages diverse replay strategies to
retrieve off-policy samples from a replay buffer, allowing policy optimization
based on a broader and more diverse set of samples for each prompt. Experiments
on five LLMs across seven mathematical reasoning benchmarks demonstrate that
RePO achieves absolute average performance gains of $18.4$ and $4.1$ points for
Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further
analysis indicates that RePO increases computational cost by $15\%$ while
raising the number of effective optimization steps by $48\%$ for Qwen3-1.7B,
with both on-policy and off-policy sample numbers set to $8$. The repository
can be accessed at https://github.com/SihengLi99/RePO.

</details>


### [12] [Latent Multi-Head Attention for Small Language Models](https://arxiv.org/abs/2506.09342)
*Sushant Mehta,Raj Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.CL

该研究展示了MLA+RoPE在小型语言模型中的效率与质量之间的显著权衡，实现了显著的内存节省和推理加速。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索小型语言模型中潜多头注意力的使用情况，以揭示潜在的效率和质量之间的权衡关系。

Method: 我们研究了小型语言模型中的潜多头注意力（MLA），并提出了三种不同架构变体的比较：标准多头注意力（MHA）、MLA以及带有旋转位置编码的MLA（MLA+RoPE）。

Result: 研究发现，带有旋转位置编码的MLA（MLA+RoPE）在使用半秩潜在维度时（r=d/2），可以实现45%的KV缓存内存降低，同时只增加0.3%的验证损失；并且，在小模型中，旋转位置编码对于MLA的性能提升至关重要。

Conclusion: MLA+RoPE在确保模型质量（7.4/10）的情况下，同时也具备了更好的内存使用和推理速度。模型和代码将在接受后公开发布。

Abstract: We present the first comprehensive study of latent multi-head attention (MLA)
for small language models, revealing interesting efficiency-quality trade-offs.
Training 30M-parameter GPT models on 100,000 synthetic stories, we benchmark
three architectural variants: standard multi-head attention (MHA), MLA, and MLA
with rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE
with half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory
reduction while incurring only a 0.3% increase in validation loss (essentially
matching MHA quality)- a Pareto improvement for memory constrained deployment.
We further show that RoPE is crucial for MLA in small models: without it, MLA
underperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by
2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2
achieves a 1.4 times speedup over full-rank MLA while maintaining the memory
savings. GPT-4 evaluations corroborate perplexity results, with ours achieving
the highest quality scores (7.4/10) across grammar, creativity, and consistency
metrics. Code and models will be released upon acceptance.

</details>


### [13] [OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment](https://arxiv.org/abs/2506.09349)
*Chao-Hong Tan,Qian Chen,Wen Wang,Chong Deng,Qinglin Zhang,Luyao Cheng,Hai Yu,Xin Zhang,Xiang Lv,Tianyu Zhao,Chong Zhang,Yukun Ma,Yafeng Chen,Hui Wang,Jiaqing Liu,Jieping Ye*

Main category: cs.CL

This paper introduces OmniDRCA, a parallel speech-text generation model based on joint autoregressive modeling, achieving new state-of-the-art performance in speech-text generation.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the limitations of existing methods for generating discrete speech tokens with LLMs, which fall short in either failing to incorporate speech tokens into the LLM's autoregressive process or generate speech and text tokens independently.

Method: Our approach, named OmniDRCA, relies on parallel speech and text processing featuring dual-resolution speech representations and contrastive cross-modal alignment to enhance audio comprehension.

Result: OmniDRCA has achieved state-of-the-art performance on Spoken Question Answering benchmarks among models based on parallel joint speech-text modeling, and competitive performance relative to interleaved models.

Conclusion: This work presents an innovative method to parallelly generate speech and text with autoregressive modeling, advancing the state-of-the-art in speech-text foundation models. It also lays the groundwork for future work in full-duplex conversational scenarios.

Abstract: Recent studies on end-to-end speech generation with large language models
(LLMs) have attracted significant community attention, with multiple works
extending text-based LLMs to generate discrete speech tokens. Existing
approaches primarily fall into two categories: (1) Methods that generate
discrete speech tokens independently without incorporating them into the LLM's
autoregressive process, resulting in text generation being unaware of
concurrent speech synthesis. (2) Models that generate interleaved or parallel
speech-text tokens through joint autoregressive modeling, enabling mutual
modality awareness during generation. This paper presents OmniDRCA, a parallel
speech-text foundation model based on joint autoregressive modeling, featuring
dual-resolution speech representations and contrastive cross-modal alignment.
Our approach processes speech and text representations in parallel while
enhancing audio comprehension through contrastive alignment. Experimental
results on Spoken Question Answering benchmarks demonstrate that OmniDRCA
establishes new state-of-the-art (SOTA) performance among parallel joint
speech-text modeling based foundation models, and achieves competitive
performance compared to interleaved models. Additionally, we explore the
potential of extending the framework to full-duplex conversational scenarios.

</details>


### [14] [DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts](https://arxiv.org/abs/2506.09351)
*Yuchen Feng,Bowen Shen,Naibin Gu,Jiaxuan Zhao,Peng Fu,Zheng Lin,Weiping Wang*

Main category: cs.CL

提出DIVE方法对MoE架构语言模型进行高效的重组，尤其是在不同校准数据集上剪枝后，有效利用专家多样性减少训练冗余。实验验证其训练效率高且准确率损失小。


<details>
  <summary>Details</summary>
Motivation: 观察到在不同的校准数据集上对特定语言模型进行剪枝后，会出现显著的多样性。然而，现有重组方法往往忽视这种多样性，可能导致冗余。本研究旨在解决这一问题。

Method: DIVE方法，包括领域亲和力挖掘、基于剪枝的专家重组以及高效再训练。特别地，重组包括前馈网络模块的剪枝和重组。

Result: 实验表明，DIVE实现了高效训练，且准确率损失最小，在激活参数数量相同的条件下，优于现有的剪枝和MoE重组方法。

Conclusion: 通过增强重组方法中的多样性，可以提高模型训练效率，减少精度损失。

Abstract: Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture
achieve high cost-efficiency by selectively activating a subset of the
parameters. Despite the inference efficiency of MoE LLMs, the training of
extensive experts from scratch incurs substantial overhead, whereas
reconstructing a dense LLM into an MoE LLM significantly reduces the training
budget. However, existing reconstruction methods often overlook the diversity
among experts, leading to potential redundancy. In this paper, we come up with
the observation that a specific LLM exhibits notable diversity after being
pruned on different calibration datasets, based on which we present a
Diversity-Enhanced reconstruction method named DIVE. The recipe of DIVE
includes domain affinity mining, pruning-based expert reconstruction, and
efficient retraining. Specifically, the reconstruction includes pruning and
reassembly of the feed-forward network (FFN) module. After reconstruction, we
efficiently retrain the model on routers, experts and normalization modules. We
implement DIVE on Llama-style LLMs with open-source training corpora.
Experiments show that DIVE achieves training efficiency with minimal accuracy
trade-offs, outperforming existing pruning and MoE reconstruction methods with
the same number of activated parameters.

</details>


### [15] [Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL](https://arxiv.org/abs/2506.09359)
*Qingyun Zeng,Simin Ma,Arash Niknafs,Ashish Basran,Carol Szabo*

Main category: cs.CL

论文探讨了使用大型语言模型（LLMs）来解决Text-to-SQL系统中生成SQL语句的语义等价性评估问题的方法及挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）大幅度提升了Text-to-SQL（NL2SQL）系统的性能，但对生成SQL语句的语义等价性评估仍然是一个挑战，特别是在面对模糊的用户查询和多种有效的SQL解释的情况下。

Method: 本研究探讨了利用大型语言模型（LLMs）来评估生成SQL的语义等价性和一种更实用的“弱”语义等价性的方法。

Result: 本研究分析了SQL等价性和非等价性的常见模式，并讨论了基于LLM评估所面临的挑战。

Conclusion: 通过对SQL等价性和非等价性模式的分析和LLM评估方法讨论，本研究为改进SQL语义等价性评价提供了新视角。

Abstract: The rise of Large Language Models (LLMs) has significantly advanced
Text-to-SQL (NL2SQL) systems, yet evaluating the semantic equivalence of
generated SQL remains a challenge, especially given ambiguous user queries and
multiple valid SQL interpretations. This paper explores using LLMs to assess
both semantic and a more practical "weak" semantic equivalence. We analyze
common patterns of SQL equivalence and inequivalence, discuss challenges in
LLM-based evaluation.

</details>


### [16] [COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content](https://arxiv.org/abs/2506.09367)
*Zhengyuan Liu,Stella Xin Yin,Dion Hoe-Lian Goh,Nancy F. Chen*

Main category: cs.CL

研究提出了一种命名为COGENT的课程导向框架，用于生成适合年级阅读水平的教育内容，能够有效解决生成式AI应用于教育时面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在内容生成方面展示了强大的潜力和多样性，但在教育领域中应用时存在挑战，如模型难以与课程标准一致并维持合适的年级阅读水平。特别是STEM教育，如何在保持科学性的同时用通俗易懂的语言解释复杂抽象的概念给学生带来了额外挑战。

Method: 我们提出了一种名为COGENT的框架，该框架旨在生成符合年级水平的教育内容。框架中包含了三个课程组成部分（科学概念、核心思想和学习目标），通过控制长度、词汇和句子复杂性来调控可读性，并采用"基于好奇"的方法来提高学生参与度和兴趣。

Result: 通过利用LLM作为评判者进行多维度评估，结果表明，COGENT能够持续生成与参考文献相当或更优的年级适当段落。

Conclusion: 我们的工作提供了一种生成适应性和高质量学习资源的可行方法。

Abstract: While Generative AI has demonstrated strong potential and versatility in
content generation, its application to educational contexts presents several
challenges. Models often fail to align with curriculum standards and maintain
grade-appropriate reading levels consistently. Furthermore, STEM education
poses additional challenges in balancing scientific explanations with everyday
language when introducing complex and abstract ideas and phenomena to younger
students. In this work, we propose COGENT, a curriculum-oriented framework for
generating grade-appropriate educational content. We incorporate three
curriculum components (science concepts, core ideas, and learning objectives),
control readability through length, vocabulary, and sentence complexity, and
adopt a ``wonder-based'' approach to increase student engagement and interest.
We conduct a multi-dimensional evaluation via both LLM-as-a-judge and human
expert analysis. Experimental results show that COGENT consistently produces
grade-appropriate passages that are comparable or superior to human references.
Our work establishes a viable approach for scaling adaptive and high-quality
learning resources.

</details>


### [17] [CoLMbo: Speaker Language Model for Descriptive Profiling](https://arxiv.org/abs/2506.09375)
*Massa Baali,Shuo Han,Syed Abdul Hannan,Purusottam Samal,Karanveer Singh,Soham Deshmukh,Rita Singh,Bhiksha Raj*

Main category: cs.CL

CoLMbo, a Speaker Language Model, integrates a speaker encoder with prompt-based conditioning to generate detailed and structured speaker descriptions, including dialect, gender, and age, marking an advancement in speaker recognition systems.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to overcome the limitations of current speaker recognition systems that struggle to provide detailed speaker characteristics or context-rich descriptions.

Method: The method involves integrating a speaker encoder with prompt-based conditioning to create detailed captions based on speaker embeddings.

Result: CoLMbo provides customized descriptions including regional dialect variations and age-related traits, performing well in zero-shot scenarios across diverse datasets.

Conclusion: This innovative approach enhances traditional speaker profiling and marks a significant advancement in the field of speaker recognition systems.

Abstract: Speaker recognition systems are often limited to classification tasks and
struggle to generate detailed speaker characteristics or provide context-rich
descriptions. These models primarily extract embeddings for speaker
identification but fail to capture demographic attributes such as dialect,
gender, and age in a structured manner. This paper introduces CoLMbo, a Speaker
Language Model (SLM) that addresses these limitations by integrating a speaker
encoder with prompt-based conditioning. This allows for the creation of
detailed captions based on speaker embeddings. CoLMbo utilizes user-defined
prompts to adapt dynamically to new speaker characteristics and provides
customized descriptions, including regional dialect variations and age-related
traits. This innovative approach not only enhances traditional speaker
profiling but also excels in zero-shot scenarios across diverse datasets,
marking a significant advancement in the field of speaker recognition.

</details>


### [18] [Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024](https://arxiv.org/abs/2506.09381)
*Austin McCutcheon,Thiago E. A. de Oliveira,Aleksandr Zheleznov,Chris Brogly*

Main category: cs.CL

研究通过多种机器学习模型评估新闻标题和链接的质量，发现传统方法和深度学习模型都可以有效区分新闻标题/链接的质量，但存在训练时间和预测性能之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨能否自动区分低质量新闻标题/链接与高质量新闻标题/链接，以应对线上新闻的广泛发布带来的潜在问题。

Method: 本研究使用了12种机器学习模型来分析57,544,214条全球新闻网站的链接和标题。数据集中，每个文本的二元标签基于专家对新闻领域质量的一致评分得出。实验中使用了115个提取出的语义特征，并针对特定模型进行了训练和测试。

Result: 传统集成方法，尤其是装袋分类器，表现良好（准确率88.1%，F1分数88.3%）。微调过的DistilBERT模型取得了最高的准确率90.3%，但需要更长的训练时间。

Conclusion: 结果表明，基于自然语言处理特征的传统分类器与深度学习模型都能有效地区分新闻标题/链接的质量，但需要在预测性能和训练时间之间做出权衡。

Abstract: The proliferation of online news enables potential widespread publication of
perceived low-quality news headlines/links. As a result, we investigated
whether it was possible to automatically distinguish perceived lower-quality
news headlines/links from perceived higher-quality headlines/links. We
evaluated twelve machine learning models on a binary, balanced dataset of
57,544,214 worldwide news website links/headings from 2018-2024 (28,772,107 per
class) with 115 extracted linguistic features. Binary labels for each text were
derived from scores based on expert consensus regarding the respective news
domain quality. Traditional ensemble methods, particularly the bagging
classifier, had strong performance (88.1% accuracy, 88.3% F1, 80/20 train/test
split). Fine-tuned DistilBERT achieved the highest accuracy (90.3%, 80/20
train/test split) but required more training time. The results suggest that
both NLP features with traditional classifiers and deep learning models can
effectively differentiate perceived news headline/link quality, with some
trade-off between predictive performance and train time.

</details>


### [19] [Comparing human and LLM politeness strategies in free production](https://arxiv.org/abs/2506.09391)
*Haoran Zhao,Robert D. Hawkins*

Main category: cs.CL

研究发现大规模语言模型（LLMs）在生成礼貌语言时表现出一定的能力，特别是在超过700亿参数的模型中，它们能够复制关键的礼貌策略偏好，并在开放性任务中获得人类评价者的偏好。然而，这些模型倾向于在所有情境下过度使用消极礼貌策略，可能导致误解。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索大规模语言模型如何在信息传递和社会互动之间找到平衡，并通过与人类的对比评估其在礼貌用语上的表现。

Method: 通过对比分析人类和大规模语言模型在受控和开放式任务中生成的礼貌言语策略，研究了LLMs在不同情境下使用礼貌策略的能力。

Result: 研究发现，超过700亿参数的LLMs能够有效复制计算语用学文献中的关键礼貌策略偏好；在开放性场景下，人类评价者对模型生成的响应表现出偏好；然而，这些模型在所有情境下都倾向于使用消极礼貌策略，可能导致误解。

Conclusion: 尽管现代LLMs在礼貌策略的应用上表现出色，但它们在不同情境下使用正面与负面策略的不平衡性，引起了关于AI系统语用学对齐的重要疑问。

Abstract: Polite speech poses a fundamental alignment challenge for large language
models (LLMs). Humans deploy a rich repertoire of linguistic strategies to
balance informational and social goals -- from positive approaches that build
rapport (compliments, expressions of interest) to negative strategies that
minimize imposition (hedging, indirectness). We investigate whether LLMs employ
a similarly context-sensitive repertoire by comparing human and LLM responses
in both constrained and open-ended production tasks. We find that larger models
($\ge$70B parameters) successfully replicate key preferences from the
computational pragmatics literature, and human evaluators surprisingly prefer
LLM-generated responses in open-ended contexts. However, further linguistic
analyses reveal that models disproportionately rely on negative politeness
strategies even in positive contexts, potentially leading to
misinterpretations. While modern LLMs demonstrate an impressive handle on
politeness strategies, these subtle differences raise important questions about
pragmatic alignment in AI systems.

</details>


### [20] [A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings](https://arxiv.org/abs/2506.09393)
*Xinyi Gao,Qiucheng Wu,Yang Zhang,Xuechen Liu,Kaizhi Qian,Ying Xu,Shiyu Chang*

Main category: cs.CL

本文提出了KT$^2$，通过隐藏马尔可夫树模型建模学生对知识概念的掌握，并利用EM算法估计学生知识掌握程度。实验表明KT$^2$在低资源的真实在线环境中，优于强大的基线模型。


<details>
  <summary>Details</summary>
Motivation: 许多知识追踪的实际课堂环境通常数据资源有限，并且需要随着学生练习历史的增长进行在线更新，这对现有的知识追踪方法提出了重大挑战。为了在低资源条件下恢复强大性能，我们重新审视了典型的课堂环境中可用的层次知识概念信息，在数据稀疏时提供有力的先验知识。

Method: 我们提出了基于知识树的知识追踪(KT$^2$)，这是一个概率性的知识追踪框架，它通过隐藏马尔可夫树模型来建模学生对知识概念树状层级结构的理解。KT$^2$利用EM算法估计学生掌握知识的情况，并通过增量更新机制在接收到新答案时支持个性化预测。

Result: 实验结果表明，KT$^2$在低资源的真实在线环境下，相比于强大基线模型，能保持一致性地更优表现。

Conclusion: 实验表明KT$^2$在低资源的真实在线环境中，始终保持优于强大基线模型的性能。

Abstract: Knowledge tracing (KT) aims to estimate a student's evolving knowledge state
and predict their performance on new exercises based on performance history.
Many realistic classroom settings for KT are typically low-resource in data and
require online updates as students' exercise history grows, which creates
significant challenges for existing KT approaches. To restore strong
performance under low-resource conditions, we revisit the hierarchical
knowledge concept (KC) information, which is typically available in many
classroom settings and can provide strong prior when data are sparse. We
therefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a
probabilistic KT framework that models student understanding over a
tree-structured hierarchy of knowledge concepts using a Hidden Markov Tree
Model. KT$^2$ estimates student mastery via an EM algorithm and supports
personalized prediction through an incremental update mechanism as new
responses arrive. Our experiments show that KT$^2$ consistently outperforms
strong baselines in realistic online, low-resource settings.

</details>


### [21] [Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models](https://arxiv.org/abs/2506.09408)
*Jui-Ming Yao,Hao-Yuan Chen,Zi-Xian Tang,Bing-Jia Tan,Sheng-Wei Peng,Bing-Cheng Xie,Shun-Feng Su*

Main category: cs.CL

Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) have demonstrated impressive performance on
multiple-choice question answering (MCQA) benchmarks, yet they remain highly
vulnerable to minor input perturbations. In this paper, we introduce and
evaluate Token Constraint Decoding (TCD). This simple yet effective
inference-time algorithm enforces alignment between token-level predictions to
enhance robustness in noisy settings. Through extensive experiments on
CommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired
with prompt engineering (PE) fixes, significantly restores performance degraded
by input noise, yielding up to +39\% absolute gains for weaker models like
Gemma3 1B. Penalty sweep analyses further reveal that TCD implicitly
regularizes overconfident outputs, with different models requiring distinct
penalty schedules to maximize resilience. Our findings establish TCD as a
practical, model-agnostic approach for improving reasoning stability under
real-world imperfections and pave the way for more reliable deployment of LLMs
in safety-critical or user-facing applications.

</details>


### [22] [PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering](https://arxiv.org/abs/2506.09414)
*Xiujun Zhou,Pingjian Zhang,Deyou Tang*

Main category: cs.CL

提出PGDA-KGQA以解决多跳推理样本稀缺和语义扭曲问题，通过多样化的数据增强策略提升知识图谱问答性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有数据增强方法只能处理单跳问题且容易语义扭曲，同时解决多跳推理样本稀缺导致模型泛化能力弱的问题。

Method: 采用统一的提示设计范式，利用精心设计的提示结合LLMs生成大规模的(问题, 逻辑形式)对，采取三种策略丰富训练集。

Result: 在标准KGQA数据集上的实验显示出优于现有方法的表现，在WebQSP和ComplexWebQuestions上分别提高了2.8%-3.1%和1.8%-2.4%的F1, Hits@1, 和Accuracy。

Conclusion: PGDA-KGQA框架通过整合LLM，多样化的数据增强策略证明了其在提升KGQA模型性能方面的有效性。

Abstract: Knowledge Graph Question Answering (KGQA) is a crucial task in natural
language processing that requires reasoning over knowledge graphs (KGs) to
answer natural language questions. Recent methods utilizing large language
models (LLMs) have shown remarkable semantic parsing capabilities but are
limited by the scarcity of diverse annotated data and multi-hop reasoning
samples. Traditional data augmentation approaches are focus mainly on
single-hop questions and prone to semantic distortion, while LLM-based methods
primarily address semantic distortion but usually neglect multi-hop reasoning,
thus limiting data diversity. The scarcity of multi-hop samples further weakens
models' generalization. To address these issues, we propose PGDA-KGQA, a
prompt-guided generative framework with multiple data augmentation strategies
for KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by
crafting meticulously engineered prompts that integrate the provided textual
content, it leverages LLMs to generate large-scale (question, logical form)
pairs for model training. Specifically, PGDA-KGQA enriches its training set by:
(1) generating single-hop pseudo questions to improve the alignment of question
semantics with KG relations; (2) applying semantic-preserving question
rewriting to improve robustness against linguistic variations; (3) employing
answer-guided reverse path exploration to create realistic multi-hop questions.
By adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA
utilizes the augmented data to enhance the accuracy of logical form generation
and thus improve answer retrieval performance. Experiments demonstrate that
outperforms state-of-the-art methods on standard KGQA datasets, achieving
improvements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by
1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.

</details>


### [23] [Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings](https://arxiv.org/abs/2506.09424)
*Md Messal Monem Miah,Adrita Anika,Xi Shi,Ruihong Huang*

Main category: cs.CL

本研究全面评估了大型语言模型（LLMs）和大型多模态模型（LMMs）在不同领域的自动欺骗检测能力。结果显示，微调后的LLMs在文本欺骗检测任务中表现出顶级水平，而LMMs在利用跨模态线索上表现不佳。研究还探讨了辅助特征和不同提示策略的影响。


<details>
  <summary>Details</summary>
Motivation: 鉴于数字世界中欺骗检测的重要性与挑战性，该研究旨在评估不同类型的模型在欺骗检测任务中的表现，以便为实际应用提供指导。

Method: 研究者通过对三个不同数据集（现实生活访谈、人际情景中的指示性欺骗以及欺诈性评论）进行评估，采用零样本和少样本方法，并测试不同样本选择策略。

Result: 发现微调后的LLMs在文本欺骗检测中表现出色，LMMs在跨模态线索利用上存在局限性；辅助特征和不同的提示策略对结果有一定影响。

Conclusion: 研究揭示了LLMs在处理和解释欺骗线索方面的潜力与局限性，为现实世界的应用提供了关键见解。

Abstract: Detecting deception in an increasingly digital world is both a critical and
challenging task. In this study, we present a comprehensive evaluation of the
automated deception detection capabilities of Large Language Models (LLMs) and
Large Multimodal Models (LMMs) across diverse domains. We assess the
performance of both open-source and commercial LLMs on three distinct datasets:
real life trial interviews (RLTD), instructed deception in interpersonal
scenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the
effectiveness of different experimental setups for deception detection,
including zero-shot and few-shot approaches with random or similarity-based
in-context example selection. Our results show that fine-tuned LLMs achieve
state-of-the-art performance on textual deception detection tasks, while LMMs
struggle to fully leverage cross-modal cues. Additionally, we analyze the
impact of auxiliary features, such as non-verbal gestures and video summaries,
and examine the effectiveness of different prompting strategies, including
direct label generation and chain-of-thought reasoning. Our findings provide
key insights into how LLMs process and interpret deceptive cues across
modalities, highlighting their potential and limitations in real-world
deception detection applications.

</details>


### [24] [Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting](https://arxiv.org/abs/2506.09428)
*Fei Ding,Baiqiao Wang*

Main category: cs.CL

本文针对SFT方法导致的语言模型普遍能力降低问题，提出了一种更加经济的方法，通过重构指令分布和多模型筛选来减少灾难性遗忘，实验结果表明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 虽然SFT增强了大语言模型的指令遵从能力和特定任务的适应性，但也往往降低了它们的普遍能力。此外，由于原始预训练数据的不可访问性，灾难性遗忘的问题在第三方实践者对开源模型实施SFT时会恶化。为解决这一挑战，本文提出了一种新的方法。

Method: 本文提出了一种新的监督微调(SFT)方法，旨在以较低成本减少灾难性遗忘的风险，同时不依赖原始微调数据。该方法首先重构基础模型可能的SFT指令分布，然后通过多模型筛选过程选择最优数据，并将其与新数据混合进行SFT。

Result: 实验结果显示，该方法在保持通用领域泛化能力的同时，提升了特定任务性能。

Conclusion: 本文提出的方法证明了在保持通用领域的泛化能力的同时，可以改进特定任务的表现，并且以较低的成本减少灾难性遗忘的风险。

Abstract: Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'
instruction-following capabilities and domain-specific task adaptability, often
diminishes their general capabilities. Moreover, due to the inaccessibility of
original pre-training data, catastrophic forgetting tends to be exacerbated
when third-party practitioners implement SFT on open-sourced models. To address
this challenge, we propose a novel, more cost-effective SFT method which could
effectively reduce the risk of catastrophic forgetting without access to
original SFT data. Our approach begins by reconstructing the likely SFT
instruction distribution of the base model, followed by a multi-model screening
process to select optimal data, which is then mixed with new data for SFT.
Experimental results demonstrate that our method preserves generalization
capabilities in general domains while improving task-specific performance.

</details>


### [25] [GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture](https://arxiv.org/abs/2506.09440)
*GigaChat team,Mamedov Valentin,Evgenii Kosarev,Gregory Leleytner,Ilya Shchuckin,Valeriy Berezovskiy,Daniil Smirnov,Dmitry Kozlov,Sergei Averkiev,Lukyanenko Ivan,Aleksandr Proshunin,Ainur Israfilova,Ivan Baskov,Artem Chervyakov,Emil Shakirov,Mikhail Kolesov,Daria Khomich,Darya Latortseva,Sergei Porkhun,Yury Fedorov,Oleg Kutuzov,Polina Kudriavtseva,Sofiia Soldatova,Kolodin Egor,Stanislav Pyatkin,Dzmitry Menshykh,Grafov Sergei,Eldar Damirov,Karlov Vladimir,Ruslan Gaitukiev,Arkadiy Shatenov,Alena Fenogenova,Nikita Savushkin,Fedor Minkin*

Main category: cs.CL

本文介绍了一款俄语大模型GigaChat，详细描述了其架构、预训练过程及实验，并开放了部分模型供研究和工业使用。


<details>
  <summary>Details</summary>
Motivation: 由于开发针对俄语的基础模型需要大量的计算资源，因此专门优化的模型不多。本文旨在填补这一空缺。

Method: 本文介绍了GigaChat系列俄语LLM模型，提供了关于模型架构、预训练过程和实验的详细报告，以及如何指导设计选择的方法。

Result: 研究评估了GigaChat模型在俄语和英语基准上的性能，并将其与多语言模型进行了比较。

Conclusion: 本文通过API、Telegram机器人和Web界面提供顶级模型的系统演示，并开放了三个公开模型，以促进俄语NLP研究和工业解决方案的发展。

Abstract: Generative large language models (LLMs) have become crucial for modern NLP
research and applications across various languages. However, the development of
foundational models specifically tailored to the Russian language has been
limited, primarily due to the significant computational resources required.
This paper introduces the GigaChat family of Russian LLMs, available in various
sizes, including base models and instruction-tuned versions. We provide a
detailed report on the model architecture, pre-training process, and
experiments to guide design choices. In addition, we evaluate their performance
on Russian and English benchmarks and compare GigaChat with multilingual
analogs. The paper presents a system demonstration of the top-performing models
accessible via an API, a Telegram bot, and a Web interface. Furthermore, we
have released three open GigaChat models in open-source
(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities
and support the development of industrial solutions for the Russian language.

</details>


### [26] [UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs](https://arxiv.org/abs/2506.09450)
*Prameshwar Thiyagarajan,Vaishnavi Parimi,Shamant Sai,Soumil Garg,Zhangir Meirbek,Nitin Yarlagadda,Kevin Zhu,Chris Kim*

Main category: cs.CL

研究引入了UniToMBench——一个用于提升和评估语言模型社会认知能力的综合性基准工具，通过实验发现模型在ToM任务上的成就与不足。


<details>
  <summary>Details</summary>
Motivation: 目前，大型语言模型在理解和预测人类思想状态方面存在困难。因此，本文旨在通过UniToMBench 提供一个综合工具，帮助改善这一情况并评估模型的ToM（Theory of Mind）能力。

Method: 本文介绍了UniToMBench，该基准通过整合SimToM和TOMBENCH的优点，引入了多交互任务设计和变化的故事场景，旨在系统性地提升和评估语言模型在理解人类心理状态方面的能力。UniToMBench 使用超过1,000个手工编写的情景组成的定制数据集，结合视角理解和多样的评估指标，以更好地刺激语言模型的社会认知能力。

Result: 评估结果显示，像GPT-4o和GPT-4o Mini这样的模型在情绪和信念相关的情形中表现出一致的高准确率，通常超过80%。然而，在基于知识的任务中，其表现则显示出较大的差异性。

Conclusion: 尽管如此，实验结果还是展现了当前语言模型在处理ToM任务的能力上的优势和局限性，强调了UniToMBench作为一种全面评估和发展工具的重要性。

Abstract: Theory of Mind (ToM), the ability to understand the mental states of oneself
and others, remains a challenging area for large language models (LLMs), which
often fail to predict human mental states accurately. In this paper, we
introduce UniToMBench, a unified benchmark that integrates the strengths of
SimToM and TOMBENCH to systematically improve and assess ToM capabilities in
LLMs by integrating multi-interaction task designs and evolving story
scenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,
UniToMBench combines perspective-taking techniques with diverse evaluation
metrics to better stimulate social cognition in LLMs. Through evaluation, we
observe that while models like GPT-4o and GPT-4o Mini show consistently high
accuracy in tasks involving emotional and belief-related scenarios, with
results usually above 80%, there is significant variability in their
performance across knowledge-based tasks. These results highlight both the
strengths and limitations of current LLMs in ToM-related tasks, underscoring
the value of UniToMBench as a comprehensive tool for future development. Our
code is publicly available here:
https://github.com/Shamant/unifiedtombenchmark.

</details>


### [27] [Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms](https://arxiv.org/abs/2506.09457)
*Zeguan Xiao,Yun Chen,Guanhua Chen*

Main category: cs.CL

本文提出一种称为Prefix-Oriented Equal-length Training (POET)的新方法，通过截断优选和非优选响应以匹配较短响应的长度，以解决Direct Alignment Algorithms (DAAs)中存在的奖励生成差距问题，从而改善大型语言模型与人类偏好的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 解决Direct Alignment Algorithms (DAAs)中的“奖励生成差距”问题，该问题源于训练期间优化目标与推理期间实际生成表现之间的不匹配。

Method: 引入POET方法，即通过截断优选和非优选响应以匹配较短响应的长度，从而解决奖励生成差距问题。

Result: 实验结果表明，对比DPO和SimPO等DAAs的传统实现方式，POET方法在AlpacaEval 2测试中提高了最多15.6分，并在多个下游任务中均有改进。

Conclusion: 本文的研究突显了在DAAs中解决奖励优化与生成性能之间对齐问题的重要性。

Abstract: Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization
(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient
alternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms
for aligning large language models (LLMs) with human preferences. However, DAAs
suffer from a fundamental limitation we identify as the "reward-generation gap"
-- a misalignment between optimization objectives during training and actual
generation performance during inference. In this paper, we find a contributor
to the reward-generation gap is the mismatch between the inherent importance of
prefix tokens during the LLM generation process and how this importance is
reflected in the implicit reward functions of DAAs. To bridge the gap, we
introduce a simple yet effective approach called Prefix-Oriented Equal-length
Training (POET), which truncates both preferred and dispreferred responses to
match the shorter one's length. Training with POET, where both responses in
each sample are truncated to equal length, resulting in diverse truncated
lengths across samples, the optimization of DAAs objective is implicitly
constrained to converge across all positions, thus paying more attention to
prefix tokens than the standard DAAs. We conduct experiments with DPO and
SimPO, two representative DAAs, demonstrating that POET improves over their
standard implementations, achieving up to 15.6 points in AlpacaEval 2 and
overall improvements across downstream tasks. Our results highlight the
importance of addressing the misalignment between reward optimization and
generation performance in DAAs.

</details>


### [28] [Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers](https://arxiv.org/abs/2506.09495)
*Ilanit Sobol,Shir Lissak,Refael Tikochinski,Tal Nakash,Anat Brunstein Klomek,Eyal Fruchter,Roi Reichart*

Main category: cs.CL

研究通过分析181个曾有生命危险自杀尝试者的YouTube频道视频，结合134个对照组频道，采用计算、混合和专家驱动的方法，识别出与自杀行为相关的主题，发现心理健康挣扎和YouTube互动是两个主要指标，且动机的差异表明自杀者希望帮助他人或个人康复。


<details>
  <summary>Details</summary>
Motivation: 鉴于自杀在全球多个西方国家是主要死因，该研究基于社交媒体上内容，特别是YouTube视频，探讨自杀行为如何呈现及其与专家知识的差异，以期为自杀行为的研究提供新的视角。

Method: 研究使用了计算自下而上、混合和专家驱动自上而下的三个方法来分析数据，包括长周期的181个自杀尝试者的YouTube频道，应用LLM主题建模来识别行为指示器，并结合临床专家的审核和心理评估。

Result: 通过自下而上的方法识别了两个与自杀行为显著相关的主题：心理健康挣扎和YouTube互动；混合方法由专家审核，但未发现额外的主题。自上而下的心理评估揭示了自杀者上传视频前后在动机上存在差异，分别希望通过分享经历帮助他人或作为个人康复的一部分。

Conclusion: 研究整合了自下而上和自上而下的发现，指出自杀行为与数字行为及临床见解之间存在复杂关系，并强调自下而上方法在揭示平台特定指标（如YouTube互动）的价值。

Abstract: Suicide remains a leading cause of death in Western countries, underscoring
the need for new research approaches. As social media becomes central to daily
life, digital footprints offer valuable insight into suicidal behavior.
Focusing on individuals who attempted suicide while uploading videos to their
channels, we investigate: How do suicidal behaviors manifest on YouTube, and
how do they differ from expert knowledge? We applied complementary approaches:
computational bottom-up, hybrid, and expert-driven top-down, on a novel
longitudinal dataset of 181 YouTube channels from individuals with
life-threatening attempts, alongside 134 control channels. In the bottom-up
approach, we applied LLM-based topic modeling to identify behavioral
indicators. Of 166 topics, five were associated with suicide-attempt, with two
also showing temporal attempt-related changes ($p<.01$) - Mental Health
Struggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,
a clinical expert reviewed LLM-derived topics and flagged 19 as
suicide-related. However, none showed significant attempt-related temporal
effects beyond those identified bottom-up. Notably, YouTube Engagement, a
platform-specific indicator, was not flagged by the expert, underscoring the
value of bottom-up discovery. In the top-down approach, psychological
assessment of suicide attempt narratives revealed that the only significant
difference between individuals who attempted before and those attempted during
their upload period was the motivation to share this experience: the former
aimed to Help Others ($\beta=-1.69$, $p<.01$), while the latter framed it as
part of their Personal Recovery ($\beta=1.08$, $p<.01$). By integrating these
approaches, we offer a nuanced understanding of suicidality, bridging digital
behavior and clinical insights.
  * Within-group changes in relation to the suicide attempt.

</details>


### [29] [LLM-as-a-qualitative-judge: automating error analysis in natural language generation](https://arxiv.org/abs/2506.09147)
*Nadezhda Chirkova,Tunde Oluwaseyi Ajayi,Seth Aycock,Zain Muhammad Mujahid,Vladana Perlić,Ekaterina Borisova,Markarit Vartampetian*

Main category: cs.CL

该研究提出一种新型评估方法，通过LLM生成结构化报告，帮助开发者理解并解决NLG系统中的问题。


<details>
  <summary>Details</summary>
Motivation: 该研究受到当前LLM评估主要作为定量工具的趋势影响，其目的在于为开发者提供有关系统改进的有意义见解。

Method: 该研究提出了一种名为LLM-as-a-qualitative-judge的方法，主要输出为NLG系统输出中的常见问题类型的结构化报告。该方法由两个主要步骤组成：开环式实例问题分析和使用直观的累积算法对发现的问题进行聚类。

Result: 研究结果表明，LLM-as-a-qualitative-judge在三分之二的情况下能够正确识别实例特定的问题，并且能够生成类似于人类注释者的错误类型报告。

Conclusion: LLM-as-a-qualitative-judge方法验证了其在识别和报告NLG系统输出问题方面的有效性，为开发者提供了宝贵的反馈。

Abstract: Prompting large language models (LLMs) to evaluate generated text, known as
LLM-as-a-judge, has become a standard evaluation approach in natural language
generation (NLG), but is primarily used as a quantitative tool, i.e. with
numerical scores as main outputs. In this work, we propose
LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main
output being a structured report of common issue types in the NLG system
outputs. Our approach is targeted at providing developers with meaningful
insights on what improvements can be done to a given NLG system and consists of
two main steps, namely open-ended per-instance issue analysis and clustering of
the discovered issues using an intuitive cumulative algorithm. We also
introduce a strategy for evaluating the proposed approach, coupled with ~300
annotations of issues in instances from 12 NLG datasets. Our results show that
LLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3
cases and is capable of producing error type reports resembling the reports
composed by human annotators. Our code and data are publicly available at
https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.

</details>


### [30] [PHRASED: Phrase Dictionary Biasing for Speech Translation](https://arxiv.org/abs/2506.09175)
*Peidong Wang,Jian Xue,Rui Zhao,Junkun Chen,Aswin Shanmugam Subramanian,Jinyu Li*

Main category: cs.CL

提出了短语词典偏置方法以改善语音翻译中短语的准确翻译，此方法在两种模型中都大大提高了性能和短语召回率。


<details>
  <summary>Details</summary>
Motivation: 由于短语在训练数据中出现频率较低，这使得在语音翻译任务中正确翻译短语变得具有挑战性。因此，本研究旨在通过使用短语词典偏置方法改善短语翻译的准确性。

Method: 本研究提出了一种基于短语词典偏置的方法，以利用从源语言到目标语言的短语映射对。该方法应用于两种广泛采用的模型：基于转导器的流式语音翻译模型和多模态大语言模型。

Result: 实验结果表明，与短语列表偏置相比，短语词典偏置方法在流式语音翻译模型中提高了21%的相对性能。此外，这项方法使多模态大语言模型能够利用外部短语信息，实现了85%的相对短语召回率提升。

Conclusion: 短语词典偏置方法在改善语音翻译任务中的短语翻译质量上显示出显著优势，对于提升翻译质量和模型性能具有重要价值。

Abstract: Phrases are essential to understand the core concepts in conversations.
However, due to their rare occurrence in training data, correct translation of
phrases is challenging in speech translation tasks. In this paper, we propose a
phrase dictionary biasing method to leverage pairs of phrases mapping from the
source language to the target language. We apply the phrase dictionary biasing
method to two types of widely adopted models, a transducer-based streaming
speech translation model and a multimodal large language model. Experimental
results show that the phrase dictionary biasing method outperforms phrase list
biasing by 21% relatively for the streaming speech translation model. In
addition, phrase dictionary biasing enables multimodal large language models to
use external phrase information, achieving 85% relative improvement in phrase
recall.

</details>


### [31] [A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs](https://arxiv.org/abs/2506.09218)
*Bruno Ferenc Šegedin*

Main category: cs.CL

研究发现，通过在瓶颈全连接层大幅缩小的情况下，卷积神经网络能基于随机特征图直接生成音频输出，并显示了在原始训练数据之外动态泛化语音依存关系的能力，这说明了模型具备词汇独立的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨深度神经网络（DNN）利用词汇学习衍生的音系泛化的表示能力，并探究在增加模型压缩率（即缩小全连接层）的情况下模型的泛化性能。

Method: 本研究采用生成式卷积神经网络（CNN）对词汇项的原始音频波形进行训练，以探究在瓶颈全连接层（FC）缩小至8信道后，模型在词汇不变情况下的泛化能力。

Result: 研究结果表明，当瓶颈全连接层大幅减少时，可以直接向卷积模块输入随机特征图，以生成音频输出，这种方法与使用全连接层生成的输出同样受到训练中语音学限制的影响。这说明卷积层可以在全连接层学习的词汇限制配置之外动态泛化语音依赖关系。

Conclusion: 本研究提出了一种新颖的探测模型是否具备词汇独立泛化能力的技术，该技术只在瓶颈全连接层极窄的情况下有效，证明了模型能够在词汇约束之外泛化语音模式的能力。

Abstract: The ability of deep neural networks (DNNs) to represent phonotactic
generalizations derived from lexical learning remains an open question. This
study (1) investigates the lexically-invariant generalization capacity of
generative convolutional neural networks (CNNs) trained on raw audio waveforms
of lexical items and (2) explores the consequences of shrinking the
fully-connected layer (FC) bottleneck from 1024 channels to 8 before training.
Ultimately, a novel technique for probing a model's lexically-independent
generalizations is proposed that works only under the narrow FC bottleneck:
generating audio outputs by bypassing the FC and inputting randomized feature
maps into the convolutional block. These outputs are equally biased by a
phonotactic restriction in training as are outputs generated with the FC. This
result shows that the convolutional layers can dynamically generalize phonetic
dependencies beyond lexically-constrained configurations learned by the FC.

</details>


### [32] [Extrapolation by Association: Length Generalization Transfer in Transformers](https://arxiv.org/abs/2506.09251)
*Ziyang Cai,Nayoung Lee,Avi Schwarzschild,Samet Oymak,Dimitris Papailiopoulos*

Main category: cs.CL

研究通过任务关联分析了Transformer模型从短输入到长输入泛化的迁移现象，表明模型训练时联合学习相关的长输入任务有助于提高其在目标任务中处理长输入的能力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索Transformer模型在处理长输入时的能力是如何形成的，尤其是在短输入训练的基础上如何能够推广到更长的输入情形。

Method: 研究方法涉及通过任务关联来探索跨任务长度泛化的转移现象，作者设计了多个算法任务（如算术运算、字符转换和迷宫导航）来验证这一假设。

Result: 本文通过任务关联的角度研究了Transformer语言模型从短输入到长输入的外推能力，发现这种能力可以在相关任务之间转移。作者展示了在不同的算法任务中模型可以通过与某个长输入相关的辅助任务训练，获得对其他任务的长输入进行有效处理的能力。此外，预训练的大规模语言模型也表现出类似的任务之间转移的特性，提示这些模型在预训练时可能构建了可复用的计算框架，以促进下游任务的推断。研究还初步揭示了这种外推能力的转移与同一注意力头在不同任务中的重复使用有关。总的来说，本文加深了我们对于Transformer模型如何处理分布外输入的机制理解，并强调了任务之间归纳结构的复用性。

Conclusion: 研究结论是Transformer模型能够通过任务之间的关联学习来继承泛化能力，且预训练语言模型中的计算框架有助于在下游任务中的外推推理。注意力头的重复使用可能是这种迁移效果背后的机制。

Abstract: Transformer language models have demonstrated impressive generalization
capabilities in natural language domains, yet we lack a fine-grained
understanding of how such generalization arises. In this paper, we investigate
length generalization--the ability to extrapolate from shorter to longer
inputs--through the lens of \textit{task association}. We find that length
generalization can be \textit{transferred} across related tasks. That is,
training a model with a longer and related auxiliary task can lead it to
generalize to unseen and longer inputs from some other target task. We
demonstrate this length generalization transfer across diverse algorithmic
tasks, including arithmetic operations, string transformations, and maze
navigation. Our results show that transformer models can inherit generalization
capabilities from similar tasks when trained jointly. Moreover, we observe
similar transfer effects in pretrained language models, suggesting that
pretraining equips models with reusable computational scaffolding that
facilitates extrapolation in downstream settings. Finally, we provide initial
mechanistic evidence that length generalization transfer correlates with the
re-use of the same attention heads between the tasks. Together, our findings
deepen our understanding of how transformers generalize to out-of-distribution
inputs and highlight the compositional reuse of inductive structure across
tasks.

</details>


### [33] [Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat](https://arxiv.org/abs/2506.09259)
*Zhuofang Li,Rafal Kocielnik,Fereshteh Soltani,Penphob,Boonyarungsrit,Animashree Anandkumar,R. Michael Alvarez*

Main category: cs.CL

研究通过无监督方法与游戏领域专家协作，识别并分类游戏中玩家的亲社会行为，并提出了一个自我锚定注意力模型(SAAM)，使模型性能提高了7.9%。


<details>
  <summary>Details</summary>
Motivation: 研究的重点在于识别和分类游戏中玩家的亲社会行为，这是与有毒互动识别同样重要的工作，尤其是在标注数据稀缺的情况下。

Method: 采用了无监督发现方法，并结合游戏领域专家合作，来识别和归类游戏中玩家的亲社会行为。提出了一个新颖的自我锚定注意力模型（SAAM），该模型与现有的最佳技术相比提高了7.9％的性能。

Result: 开发了第一个用于分类游戏中亲社会行为的自动化系统，并在Call of Duty(R): Modern Warfare(R)II游戏中展示了其有效性。

Conclusion: 此研究创新性地应用NLP技术来发现和分类游戏中玩家的亲社会行为，展现了从惩罚有毒行为到鼓励积极互动的转变潜力。

Abstract: Millions of players engage daily in competitive online games, communicating
through in-game chat. Prior research has focused on detecting relatively small
volumes of toxic content using various Natural Language Processing (NLP)
techniques for the purpose of moderation. However, recent studies emphasize the
importance of detecting prosocial communication, which can be as crucial as
identifying toxic interactions. Recognizing prosocial behavior allows for its
analysis, rewarding, and promotion. Unlike toxicity, there are limited
datasets, models, and resources for identifying prosocial behaviors in
game-chat text. In this work, we employed unsupervised discovery combined with
game domain expert collaboration to identify and categorize prosocial player
behaviors from game chat. We further propose a novel Self-Anchored Attention
Model (SAAM) which gives 7.9% improvement compared to the best existing
technique. The approach utilizes the entire training set as "anchors" to help
improve model performance under the scarcity of training data. This approach
led to the development of the first automated system for classifying prosocial
behaviors in in-game chats, particularly given the low-resource settings where
large-scale labeled data is not available. Our methodology was applied to one
of the most popular online gaming titles - Call of Duty(R): Modern
Warfare(R)II, showcasing its effectiveness. This research is novel in applying
NLP techniques to discover and classify prosocial behaviors in player in-game
chat communication. It can help shift the focus of moderation from solely
penalizing toxicity to actively encouraging positive interactions on online
platforms.

</details>


### [34] [Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models](https://arxiv.org/abs/2506.09277)
*Milan Bhan,Jean-Noel Vittaut,Nicolas Chesneau,Sarath Chandar,Marie-Jeanne Lesot*

Main category: cs.CL

论文提出了一种新的框架，用于测量大语言模型生成的自我自然语言解释(self-NLE)的可信度。通过对比self-NLE和模型内部隐藏状态来改进解释的可信度评价。


<details>
  <summary>Details</summary>
Motivation: 鉴于现有的自我自然语言解释(self-NLE)方法通常不反映模型的实际决策过程，导致解释不可信，作者希望通过对比模型内部状态来改进self-NLE的可信度评价方法。

Method: 此论文提出了一种新的灵活框架，用于通过将语言模型自产生自然语言解释(self-NLE)与模型内部隐藏状态的解释直接比较，来定量测量self-NLE的可信度。

Result: 该框架的使用使得研究者能够更加深入地了解self-NLE的可信度，并为进一步生成更可靠的self-NLE提供了基础。

Conclusion: 通过建立self-NLE与模型推理之间的直接联系，这项工作提高了对self-NLE可信度的理解，并为生成更可信的self-NLE提供了构建模块。

Abstract: Large Language Models (LLM) have demonstrated the capability of generating
free text self Natural Language Explanation (self-NLE) to justify their
answers. Despite their logical appearance, self-NLE do not necessarily reflect
the LLM actual decision-making process, making such explanations unfaithful.
While existing methods for measuring self-NLE faithfulness mostly rely on
behavioral tests or computational block identification, none of them examines
the neural activity underlying the model's reasoning. This work introduces a
novel flexible framework for quantitatively measuring the faithfulness of
LLM-generated self-NLE by directly comparing the latter with interpretations of
the model's internal hidden states. The proposed framework is versatile and
provides deep insights into self-NLE faithfulness by establishing a direct
connection between self-NLE and model reasoning. This approach advances the
understanding of self-NLE faithfulness and provides building blocks for
generating more faithful self-NLE.

</details>


### [35] [$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding](https://arxiv.org/abs/2506.09301)
*Cesare Spinoso-Di Piano,David Austin,Pablo Piantanida,Jackie Chi Kit Cheung*

Main category: cs.CL

本文提出了$(RSA)^2$框架，解决了现有RSA理论不能有效建模修辞表达的问题，通过整合LLMs，实现在讽刺表达理解方面的尖端性能。


<details>
  <summary>Details</summary>
Motivation: 现有的RSA理论无法解释修辞表达，或者需要在特定环境中建模说话人使用修辞语的潜在动机。为了弥补这些不足，提出$(RSA)^2$框架。

Method: 本文介绍了$(RSA)^2$框架，该框架通过考虑说话人使用的修辞策略来建模修辞语的使用。

Result: $(RSA)^2$框架能够实现与人类相兼容的非字面意义的理解，并且在新引入的PragMega+言语解释数据集的讽刺部分达到了最先进的性能。

Conclusion: 通过对说话人修辞策略的建模，$(RSA)^2$框架在理解非字面意义方面更进一步，无需考虑说话人使用非字面语言的具体动机。

Abstract: Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in
human communication, resulting in utterances where the literal and the intended
meanings do not match. The Rational Speech Act (RSA) framework, which
explicitly models speaker intentions, is the most widespread theory of
probabilistic pragmatics, but existing implementations are either unable to
account for figurative expressions or require modeling the implicit motivations
for using figurative language (e.g., to express joy or annoyance) in a
setting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware
RSA $(RSA)^2$ framework which models figurative language use by considering a
speaker's employed rhetorical strategy. We show that $(RSA)^2$ enables
human-compatible interpretations of non-literal utterances without modeling a
speaker's motivations for being non-literal. Combined with LLMs, it achieves
state-of-the-art performance on the ironic split of PragMega+, a new irony
interpretation dataset introduced in this study.

</details>


### [36] [Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models](https://arxiv.org/abs/2506.09315)
*Yao Xiao,Heidi Christensen,Stefan Goetze*

Main category: cs.CL

本研究使用Mistral-7B语言模型改进了阿尔茨海默病的检测方法，提升了准确率，并展示了更加透明的决策过程。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病（AD）是一种影响认知功能，尤其是语言能力的神经退行性疾病。本研究旨在提高AD检测的准确性并创建更具解释性的决策模型，以超越当前方法的性能。

Method: 本研究利用Mistral-7B指令跟随版大型语言模型（LLM）扩展了用于检测阿尔茨海默病（AD）的配对困惑度方法。通过这种方法，研究提高了整体检测准确率，并展示了清晰可解释的决策边界。

Result: 相比于最佳当前配对困惑度方法，本研究的准确率提高了3.33%，相比ADReSS 2020挑战中的最佳方法，提高了6.35%。通过仔细分析模型生成的回应和人类回应的对比，研究还展示了LLMs学习到了AD患者的特殊语言模式。

Conclusion: 本研究证明了使用大型语言模型（LLM）可以有效检测阿尔茨海默病，并奠定了解释性决策边界的理论基础。这种方法为模型解释和数据增强开辟了新的可能性。

Abstract: Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive
decline that commonly impacts language ability. This work extends the paired
perplexity approach to detecting AD by using a recent large language model
(LLM), the instruction-following version of Mistral-7B. We improve accuracy by
an average of 3.33% over the best current paired perplexity method and by 6.35%
over the top-ranked method from the ADReSS 2020 challenge benchmark. Our
further analysis demonstrates that the proposed approach can effectively detect
AD with a clear and interpretable decision boundary in contrast to other
methods that suffer from opaque decision-making processes. Finally, by
prompting the fine-tuned LLMs and comparing the model-generated responses to
human responses, we illustrate that the LLMs have learned the special language
patterns of AD speakers, which opens up possibilities for novel methods of
model interpretation and data augmentation.

</details>


### [37] [Towards Efficient and Effective Alignment of Large Language Models](https://arxiv.org/abs/2506.09329)
*Yuxin Jiang*

Main category: cs.CL

The paper introduces methods for refining and collecting training data (Lion, WebR), techniques for enhancing model training (LTE, BMC), and an evaluation benchmark (FollowBench) to better align large language models with human expectations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the alignment of large language models (LLMs) with human expectations by overcoming limitations in data collection, enhancing training techniques, and developing a more accurate evaluation system.

Method: Our approach comprises three main parts: Lion, an adversarial distillation framework to refine training data by generating challenging instructions; WebR, an automated framework to synthesize instruction data from web documents, enhancing data diversity and scalability; Learning to Edit (LTE), a knowledge updating framework using meta-learning; and BMC, an optimization technique that captures token-level correlations, improving alignment in reasoning tasks.

Result: The results show state-of-the-art zero-shot reasoning with Lion, enhanced data diversity and scalability with WebR, efficient knowledge updates with LTE, and improved alignment in QA and mathematical reasoning tasks with BMC. FollowBench provides insights into models' constraint adherence weaknesses.

Conclusion: The study introduces novel methodologies to enhance the alignment of large language models with human expectations through improved data collection, training techniques, and evaluation methods, paving the way for future advancements.

Abstract: Large language models (LLMs) exhibit remarkable capabilities across diverse
tasks, yet aligning them efficiently and effectively with human expectations
remains a critical challenge. This thesis advances LLM alignment by introducing
novel methodologies in data collection, training, and evaluation. We first
address alignment data collection. Existing approaches rely heavily on manually
curated datasets or proprietary models. To overcome these limitations, we
propose Lion, an adversarial distillation framework that iteratively refines
training data by identifying and generating challenging instructions, enabling
state-of-the-art zero-shot reasoning. Additionally, we introduce Web
Reconstruction (WebR), a fully automated framework that synthesizes
instruction-tuning data directly from raw web documents, significantly
improving data diversity and scalability over existing synthetic data methods.
Next, we enhance alignment training through novel optimization techniques. We
develop Learning to Edit (LTE), a framework that enables LLMs to efficiently
integrate new knowledge while preserving existing information. LTE leverages
meta-learning to improve both real-time and batch knowledge updates.
Furthermore, we introduce Bridging and Modeling Correlations (BMC), a
refinement of Direct Preference Optimization (DPO) that explicitly captures
token-level correlations in preference data, leading to superior alignment
across QA and mathematical reasoning tasks. Finally, we tackle the challenge of
evaluating alignment. Existing benchmarks emphasize response quality but
overlook adherence to specific constraints. To bridge this gap, we introduce
FollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to
follow complex constraints across diverse instruction types. Our results expose
key weaknesses in current models' constraint adherence, offering insights for
future improvements.

</details>


### [38] [Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation](https://arxiv.org/abs/2506.09331)
*Arjun Vaithilingam Sudhakar*

Main category: cs.CL

研究大型语言模型(LLMs)是否具有理论上的思维能力，即能否理解他人的意图，并通过合作型多智能体强化学习(MARL)进行探索。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型(LLMs)在复杂的自然语言任务中表现出优秀的零样本和少样本泛化能力，但它们是否理解文本交流背后的意图还有待验证。理解他人的意图对于有效的协作至关重要。

Method: 通过合作型多智能体强化学习(MARL)探究大型语言模型(LLMs)的思维理论，其中智能体通过重复交互学会协作，这反映了人类社会推理。

Result: 研究旨在提升人工智能代理的适应和合作能力，与人工及人类伙伴共同合作。

Conclusion: 结合基于LLM的代理进行自然语言交互，研究旨在发展混合的人工智能-人类系统，实现流畅的合作，对未来人机交互有一定的启示意义。

Abstract: Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot
generalization capabilities across complex natural language tasks, enabling
their widespread use as virtual assistants for diverse applications such as
translation and summarization. Despite being trained solely on large corpora of
text without explicit supervision on author intent, LLMs appear to infer the
underlying meaning of textual interactions. This raises a fundamental question:
can LLMs model and reason about the intentions of others, i.e., do they possess
a form of theory of mind? Understanding other's intentions is crucial for
effective collaboration, which underpins human societal success and is
essential for cooperative interactions among multiple agents, including humans
and autonomous systems. In this work, we investigate the theory of mind in LLMs
through the lens of cooperative multi-agent reinforcement learning (MARL),
where agents learn to collaborate via repeated interactions, mirroring human
social reasoning. Our approach aims to enhance artificial agent's ability to
adapt and cooperate with both artificial and human partners. By leveraging
LLM-based agents capable of natural language interaction, we move towards
creating hybrid human-AI systems that can foster seamless collaboration, with
broad implications for the future of human-artificial interaction.

</details>


### [39] [RePO: Replay-Enhanced Policy Optimization](https://arxiv.org/abs/2506.09340)
*Siheng Li,Zhanhui Zhou,Wai Lam,Chao Yang,Chaochao Lu*

Main category: cs.CL

RePO enhances the efficiency and performance of reinforcement learning in optimizing large language models by utilizing a replay buffer for diverse sample retrieval, achieving significant performance gains compared to previous methods while managing computational costs.


<details>
  <summary>Details</summary>
Motivation: The motivation behind introducing RePO is to address the limitations of GRPO, which requires multiple on-policy outputs per prompt and suffers from high computational costs and low data efficiency.

Method: The paper introduces Replay-Enhanced Policy Optimization (RePO), which retrieves off-policy samples from a replay buffer to optimize the policy based on a more diverse set of samples for each prompt, aiming to enhance data efficiency and reduce computational costs compared to Group Relative Policy Optimization (GRPO).

Result: Experiments conducted on five LLMs across seven mathematical reasoning benchmarks show that RePO achieves absolute average performance gains of 18.4 and 4.1 points for Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Furthermore, RePO increases the computational cost by 15% but raises the number of effective optimization steps by 48% for Qwen3-1.7B.

Conclusion: The conclusion of the paper is that RePO is effective in improving the performance of LLMs in mathematical reasoning tasks while requiring only a small increase in computational cost, making it a promising approach for optimizing large language models.

Abstract: Reinforcement learning (RL) is vital for optimizing large language models
(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages
using multiple on-policy outputs per prompt, leading to high computational
costs and low data efficiency. To address this, we introduce Replay-Enhanced
Policy Optimization (RePO), which leverages diverse replay strategies to
retrieve off-policy samples from a replay buffer, allowing policy optimization
based on a broader and more diverse set of samples for each prompt. Experiments
on five LLMs across seven mathematical reasoning benchmarks demonstrate that
RePO achieves absolute average performance gains of $18.4$ and $4.1$ points for
Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further
analysis indicates that RePO increases computational cost by $15\%$ while
raising the number of effective optimization steps by $48\%$ for Qwen3-1.7B,
with both on-policy and off-policy sample numbers set to $8$. The repository
can be accessed at https://github.com/SihengLi99/RePO.

</details>


### [40] [Latent Multi-Head Attention for Small Language Models](https://arxiv.org/abs/2506.09342)
*Sushant Mehta,Raj Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.CL

研究展示了一种优化小型语言模型内存占用和推理速度的方法，即在潜在多头注意力机制中加入旋转位置编码，这种方法在保持高质量的同时实现了显著的内存节省和加速效果。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在通过在小型语言模型中引入潜在多头注意力机制，并结合旋转位置编码，来优化模型的内存占用，并提高推理的速度，同时保证模型的高质量。

Method: 论文探讨了小型语言模型中潜在多头注意力（MLA）的首个全面研究，并对其效率与质量之间的权衡进行了揭示。实验通过在100,000个合成故事上训练30M参数的GPT模型，对比了三种架构变体：标准的多头注意力（MHA）、MLA以及应用旋转位置编码的MLA（MLA+RoPE）。

Result: 研究发现MLA+RoPE使用半秩潜在维度（r = d/2）可以减少45%的KV缓存内存占用，而验证损失仅增加0.3%，几乎没有损失质量，是内存受限部署中的帕累托改进。此外，在小型模型中，RoPE是MLA性能的关键。MLA在应用RoPE后超过了标准注意力机制2%。在NVIDIA A100 GPU上的推理基准测试显示，采用r=d/2的MLA可以实现相对于全秩MLA的1.4倍加速，同时保持内存节省。GPT-4评估证实了困惑度结果，我们的方法在语法、创意和一致性方面取得了最高的质量分数（7.4/10）。

Conclusion: 论文得出结论，MLA+RoPE通过减少KV缓存内存的使用来优化小型语言模型，达到在不牺牲质量的情况下加速推理的目的。

Abstract: We present the first comprehensive study of latent multi-head attention (MLA)
for small language models, revealing interesting efficiency-quality trade-offs.
Training 30M-parameter GPT models on 100,000 synthetic stories, we benchmark
three architectural variants: standard multi-head attention (MHA), MLA, and MLA
with rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE
with half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory
reduction while incurring only a 0.3% increase in validation loss (essentially
matching MHA quality)- a Pareto improvement for memory constrained deployment.
We further show that RoPE is crucial for MLA in small models: without it, MLA
underperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by
2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2
achieves a 1.4 times speedup over full-rank MLA while maintaining the memory
savings. GPT-4 evaluations corroborate perplexity results, with ours achieving
the highest quality scores (7.4/10) across grammar, creativity, and consistency
metrics. Code and models will be released upon acceptance.

</details>


### [41] [OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment](https://arxiv.org/abs/2506.09349)
*Chao-Hong Tan,Qian Chen,Wen Wang,Chong Deng,Qinglin Zhang,Luyao Cheng,Hai Yu,Xin Zhang,Xiang Lv,Tianyu Zhao,Chong Zhang,Yukun Ma,Yafeng Chen,Hui Wang,Jiaqing Liu,Jieping Ye*

Main category: cs.CL

OmniDRCA模型实现了双分辨率语音表示和对比跨模态对齐的并行语音文本生成任务，达到该领域的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机在于现有方法在生成离散语音令牌时存在不足，旨在改善语音和文本生成之间的交互性和协同性。

Method: OmniDRCA采用并行的语音-文本基础模型，基于联合自回归建模，具有双分辨率语音表示和对比跨模态对齐，实现语音和文本的并行处理，并通过对比对齐来增强音频理解。

Result: 实验结果表明，OmniDRCA在口语问答基准测试中建立了并行联合语音-文本建模基础模型的最新技术水平，并与交错模型相比具有竞争力。

Conclusion: 研究结论表示，OmniDRCA模型在平行语音和文本生成中表现出色，并探索了将其框架扩展至全双工对话场景的潜力。

Abstract: Recent studies on end-to-end speech generation with large language models
(LLMs) have attracted significant community attention, with multiple works
extending text-based LLMs to generate discrete speech tokens. Existing
approaches primarily fall into two categories: (1) Methods that generate
discrete speech tokens independently without incorporating them into the LLM's
autoregressive process, resulting in text generation being unaware of
concurrent speech synthesis. (2) Models that generate interleaved or parallel
speech-text tokens through joint autoregressive modeling, enabling mutual
modality awareness during generation. This paper presents OmniDRCA, a parallel
speech-text foundation model based on joint autoregressive modeling, featuring
dual-resolution speech representations and contrastive cross-modal alignment.
Our approach processes speech and text representations in parallel while
enhancing audio comprehension through contrastive alignment. Experimental
results on Spoken Question Answering benchmarks demonstrate that OmniDRCA
establishes new state-of-the-art (SOTA) performance among parallel joint
speech-text modeling based foundation models, and achieves competitive
performance compared to interleaved models. Additionally, we explore the
potential of extending the framework to full-duplex conversational scenarios.

</details>


### [42] [DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts](https://arxiv.org/abs/2506.09351)
*Yuchen Feng,Bowen Shen,Naibin Gu,Jiaxuan Zhao,Peng Fu,Zheng Lin,Weiping Wang*

Main category: cs.CL

本文提出了一种多样性的增强重建方法DIVE，用于重构大型语言模型（LLMs），该方法在训练效率上优于现有的重构方法，且具有较小的精度权衡。


<details>
  <summary>Details</summary>
Motivation: 尽管MoE架构的大型语言模型（LLMs）在推理时效率很高，但从头开始训练大量专家会带来显著的开销，而将密集的LLMs转换为MoE LLMs可以显著减少训练预算。然而，现有的重构方法往往忽视了专家之间的多样性，可能导致冗余。因此，提出了一种多样性的增强重建方法DIVE。

Method: DIVE方法涉及领域亲和力挖掘、基于剪枝的专家重构以及高效的再训练。特别是，重构过程包括前馈网络（FFN）模块的剪枝和重组。重构后，我们对路由器、专家和归一化模块进行高效的再训练。

Result: 实验结果证明了DIVE的有效性，它能够以较小的精度权衡提高训练效率，优于现有的剪枝和MoE重构方法。

Conclusion: 实验表明，DIVE在训练效率上实现了与现有剪枝和MoE重构方法相比，在激活参数数量相同的情况下，具有较小的精度权衡，并优于它们。

Abstract: Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture
achieve high cost-efficiency by selectively activating a subset of the
parameters. Despite the inference efficiency of MoE LLMs, the training of
extensive experts from scratch incurs substantial overhead, whereas
reconstructing a dense LLM into an MoE LLM significantly reduces the training
budget. However, existing reconstruction methods often overlook the diversity
among experts, leading to potential redundancy. In this paper, we come up with
the observation that a specific LLM exhibits notable diversity after being
pruned on different calibration datasets, based on which we present a
Diversity-Enhanced reconstruction method named DIVE. The recipe of DIVE
includes domain affinity mining, pruning-based expert reconstruction, and
efficient retraining. Specifically, the reconstruction includes pruning and
reassembly of the feed-forward network (FFN) module. After reconstruction, we
efficiently retrain the model on routers, experts and normalization modules. We
implement DIVE on Llama-style LLMs with open-source training corpora.
Experiments show that DIVE achieves training efficiency with minimal accuracy
trade-offs, outperforming existing pruning and MoE reconstruction methods with
the same number of activated parameters.

</details>


### [43] [Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL](https://arxiv.org/abs/2506.09359)
*Qingyun Zeng,Simin Ma,Arash Niknafs,Ashish Basran,Carol Szabo*

Main category: cs.CL

The paper explores the use of LLMs to evaluate the semantic equivalence of SQL queries generated from natural language inputs in NL2SQL systems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the difficulty in assessing the semantic correctness of SQL outputs derived from natural language inputs using LLMs, a problem that has increased in complexity with the advent of advanced LLMs in NL2SQL systems.

Method: This paper examines the use of Large Language Models (LLMs) to evaluate the semantic equivalence of SQL queries generated by Text-to-SQL systems, considering the nuances of ambiguous natural language inputs and multiple possible SQL interpretations.

Result: Not explicitly mentioned in the abstract, but the analysis of patterns of SQL equivalence and inequivalence and the discussion of LLM-based evaluation challenges suggest insights into how LLMs can be leveraged for SQL semantic evaluation.

Conclusion: The paper concludes by underscoring the potential of LLMs in assessing SQL equivalence and inequivalence, and by highlighting the challenges and patterns of SQL equivalence in the context of LLM-based evaluation.

Abstract: The rise of Large Language Models (LLMs) has significantly advanced
Text-to-SQL (NL2SQL) systems, yet evaluating the semantic equivalence of
generated SQL remains a challenge, especially given ambiguous user queries and
multiple valid SQL interpretations. This paper explores using LLMs to assess
both semantic and a more practical "weak" semantic equivalence. We analyze
common patterns of SQL equivalence and inequivalence, discuss challenges in
LLM-based evaluation.

</details>


### [44] [COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content](https://arxiv.org/abs/2506.09367)
*Zhengyuan Liu,Stella Xin Yin,Dion Hoe-Lian Goh,Nancy F. Chen*

Main category: cs.CL

本文介绍了一个名为COGENT的课程导向框架，用于生成与年级相符的教育内容，该框架在多维度评估中取得了与人类参考相媲美或更优的结果。


<details>
  <summary>Details</summary>
Motivation: 由于生成式AI在教育教学应用中存在无法始终与课程标准及年级阅读水平保持一致的问题，尤其是在STEM教育中，如何用日常语言解释复杂抽象的概念和现象成为一个挑战。因此，本文提出了该框架，旨在解决这些问题。

Method: 我们提出了COGENT框架，该框架结合了三个课程组件（科学概念、核心理念和学习目标），并通过控制内容的长度、词汇和句子复杂度来调整可读性。同时还采用了“好奇心驱动”的方法来提高学生的学习兴趣和参与度。

Result: 实验结果显示，COGENT能够稳定地生成与年级相符的段落，这些段落在评估中表现出了与人类参考材料相当或更好的质量。

Conclusion: 本研究确立了一种有效的方法，可以扩展并提高学习资源的质量。

Abstract: While Generative AI has demonstrated strong potential and versatility in
content generation, its application to educational contexts presents several
challenges. Models often fail to align with curriculum standards and maintain
grade-appropriate reading levels consistently. Furthermore, STEM education
poses additional challenges in balancing scientific explanations with everyday
language when introducing complex and abstract ideas and phenomena to younger
students. In this work, we propose COGENT, a curriculum-oriented framework for
generating grade-appropriate educational content. We incorporate three
curriculum components (science concepts, core ideas, and learning objectives),
control readability through length, vocabulary, and sentence complexity, and
adopt a ``wonder-based'' approach to increase student engagement and interest.
We conduct a multi-dimensional evaluation via both LLM-as-a-judge and human
expert analysis. Experimental results show that COGENT consistently produces
grade-appropriate passages that are comparable or superior to human references.
Our work establishes a viable approach for scaling adaptive and high-quality
learning resources.

</details>


### [45] [CoLMbo: Speaker Language Model for Descriptive Profiling](https://arxiv.org/abs/2506.09375)
*Massa Baali,Shuo Han,Syed Abdul Hannan,Purusottam Samal,Karanveer Singh,Soham Deshmukh,Rita Singh,Bhiksha Raj*

Main category: cs.CL

本文介绍的CoLMbo说话人语言模型(SLM)通过整合说话人编码器和基于提示的条件调节，能够生成详细且上下文丰富的说话人描述，提升了传统说话人识别系统的功能。


<details>
  <summary>Details</summary>
Motivation: 现有说话人识别系统主要集中在分类任务上，难以生成详细的说话人特征或表达丰富的上下文描述。这些模型主要提取说话人的嵌入进行身份识别，但无法以结构化方式捕捉诸如方言、性别和年龄等人口统计学属性。

Method: 本研究介绍了一种名为CoLMbo的说话人语言模型(SLM)，该模型通过将说话人编码器与基于提示的调节方法结合，解决了现有系统无法生成详细的说话人特征和提供上下文丰富描述的问题。通过使用用户定义的提示来适应新的说话人特征，CoLMbo能够提供定制化的描述，包括区域方言的变体和与年龄相关的特征。

Result: 研究结果表明，CoLMbo在传统的说话人特征描述之外有所提升，并且在零样本场景中跨多种数据集表现优异，标志着在说话人识别领域有了重要的进展。

Conclusion: CoLMbo模型通过其独特的基于提示的调节机制，能够动态地生成详细的说话人描述，包括说话人的地区方言和年龄特征等。这项工作代表了说话人识别技术领域的一大进步，尤其是在零样本学习场景中的表现。

Abstract: Speaker recognition systems are often limited to classification tasks and
struggle to generate detailed speaker characteristics or provide context-rich
descriptions. These models primarily extract embeddings for speaker
identification but fail to capture demographic attributes such as dialect,
gender, and age in a structured manner. This paper introduces CoLMbo, a Speaker
Language Model (SLM) that addresses these limitations by integrating a speaker
encoder with prompt-based conditioning. This allows for the creation of
detailed captions based on speaker embeddings. CoLMbo utilizes user-defined
prompts to adapt dynamically to new speaker characteristics and provides
customized descriptions, including regional dialect variations and age-related
traits. This innovative approach not only enhances traditional speaker
profiling but also excels in zero-shot scenarios across diverse datasets,
marking a significant advancement in the field of speaker recognition.

</details>


### [46] [Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024](https://arxiv.org/abs/2506.09381)
*Austin McCutcheon,Thiago E. A. de Oliveira,Aleksandr Zheleznov,Chris Brogly*

Main category: cs.CL

本研究表明可以使用机器学习模型有效地区分新闻标题/链接的质量。Bagging分类器和DistilBERT实现了良好的性能，但训练时间有所不同。


<details>
  <summary>Details</summary>
Motivation: 随着在线新闻的增多，质量参差不齐的新闻标题/链接大量涌现。因此，研究旨在探讨是否可以自动区分质量较低的新闻标题/链接与质量较高的标题/链接。

Method: 本研究通过评估12种机器学习模型，使用包含2018-2024年间来自全球新闻网站的57,544,214条新闻链接/标题的平衡数据集，该数据集提取了115个语言特征。

Result: 研究发现，传统的集成方法，特别是Bagging分类器表现突出（准确率88.1%，F1值88.3%，基于80/20的训练/测试分割）。微调后的DistilBERT模型达到了最高的准确率（90.3%，基于80/20的训练/测试分割），但需要更长的训练时间。

Conclusion: 研究结果表明，结合传统的分类器与NLP特征，以及深度学习模型，都能够有效地对新闻标题/链接的质量进行区分，但存在训练时间和预测性能之间的权衡。

Abstract: The proliferation of online news enables potential widespread publication of
perceived low-quality news headlines/links. As a result, we investigated
whether it was possible to automatically distinguish perceived lower-quality
news headlines/links from perceived higher-quality headlines/links. We
evaluated twelve machine learning models on a binary, balanced dataset of
57,544,214 worldwide news website links/headings from 2018-2024 (28,772,107 per
class) with 115 extracted linguistic features. Binary labels for each text were
derived from scores based on expert consensus regarding the respective news
domain quality. Traditional ensemble methods, particularly the bagging
classifier, had strong performance (88.1% accuracy, 88.3% F1, 80/20 train/test
split). Fine-tuned DistilBERT achieved the highest accuracy (90.3%, 80/20
train/test split) but required more training time. The results suggest that
both NLP features with traditional classifiers and deep learning models can
effectively differentiate perceived news headline/link quality, with some
trade-off between predictive performance and train time.

</details>


### [47] [Comparing human and LLM politeness strategies in free production](https://arxiv.org/abs/2506.09391)
*Haoran Zhao,Robert D. Hawkins*

Main category: cs.CL

研究发现，尽管大型语言模型在礼貌交流策略方面表现出色，但在积极语境中过度依赖消极礼貌策略可能会导致问题。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在礼貌用语方面的问题，看它们是否能像人类一样灵活地调整语言策略来满足信息和社交的目标。

Method: 通过对比人类和大语言模型（LLMs）在受限和开放生成任务中的响应，研究LLMs是否也采用了同样基于上下文的策略。

Result: 研究表明，较大的模型（至少700亿参数）能够成功复制计算语用学文献中的关键偏好，并且人类评估者意外地更喜欢在开放性环境中由LLMs生成的回复。但是，进一步的语言分析表明，模型在积极的语境中过度依赖消极礼貌策略，可能导致误解。

Conclusion: 现代大语言模型在使用礼貌策略方面展现出了惊人的能力，但这些细微差异对AI系统的语用对齐提出了重要问题。

Abstract: Polite speech poses a fundamental alignment challenge for large language
models (LLMs). Humans deploy a rich repertoire of linguistic strategies to
balance informational and social goals -- from positive approaches that build
rapport (compliments, expressions of interest) to negative strategies that
minimize imposition (hedging, indirectness). We investigate whether LLMs employ
a similarly context-sensitive repertoire by comparing human and LLM responses
in both constrained and open-ended production tasks. We find that larger models
($\ge$70B parameters) successfully replicate key preferences from the
computational pragmatics literature, and human evaluators surprisingly prefer
LLM-generated responses in open-ended contexts. However, further linguistic
analyses reveal that models disproportionately rely on negative politeness
strategies even in positive contexts, potentially leading to
misinterpretations. While modern LLMs demonstrate an impressive handle on
politeness strategies, these subtle differences raise important questions about
pragmatic alignment in AI systems.

</details>


### [48] [A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings](https://arxiv.org/abs/2506.09393)
*Xinyi Gao,Qiucheng Wu,Yang Zhang,Xuechen Liu,Kaizhi Qian,Ying Xu,Shiyu Chang*

Main category: cs.CL

我们提出了KT$^2$，一种基于知识树的层次化隐马尔可夫模型，用于知识追踪，它在低资源的课堂环境中表现出色。


<details>
  <summary>Details</summary>
Motivation: 许多实际的课堂环境对于知识追踪来说往往是低资源的数据环境，并且需要在线更新以适应学生不断增长的练习历史，这为现有的知识追踪方法带来了很大的挑战。为了在这种低资源条件下恢复良好的性能，我们重新审视了层次化的知识概念（KC）信息，这些信息在许多课堂环境中通常可用，并且在数据稀疏时可以提供强先验。

Method: 我们提出了一种基于知识树的知识追踪方法（KT$^2$），这是一种概率性的知识追踪框架，它使用隐马尔可夫树模型来对学生理解过的一系列分级知识概念进行建模。KT$^2$通过EM算法估计学生掌握程度，并通过增量更新机制支持个性化预测。

Result: 实验结果表明，KT$^2$在实际的在线低资源环境下性能始终优于强大的基线模型。

Conclusion: 研究结论未在摘要中直接体现，但可以推断，KT$^2$框架在低资源环境中展示了强大的性能和适应性。

Abstract: Knowledge tracing (KT) aims to estimate a student's evolving knowledge state
and predict their performance on new exercises based on performance history.
Many realistic classroom settings for KT are typically low-resource in data and
require online updates as students' exercise history grows, which creates
significant challenges for existing KT approaches. To restore strong
performance under low-resource conditions, we revisit the hierarchical
knowledge concept (KC) information, which is typically available in many
classroom settings and can provide strong prior when data are sparse. We
therefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a
probabilistic KT framework that models student understanding over a
tree-structured hierarchy of knowledge concepts using a Hidden Markov Tree
Model. KT$^2$ estimates student mastery via an EM algorithm and supports
personalized prediction through an incremental update mechanism as new
responses arrive. Our experiments show that KT$^2$ consistently outperforms
strong baselines in realistic online, low-resource settings.

</details>


### [49] [Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models](https://arxiv.org/abs/2506.09408)
*Jui-Ming Yao,Hao-Yuan Chen,Zi-Xian Tang,Bing-Jia Tan,Sheng-Wei Peng,Bing-Cheng Xie,Shun-Feng Su*

Main category: cs.CL

我们提出了一种简单而有效的推理时算法，Token Constraint Decoding (TCD)，它能够提高大型语言模型在有噪声环境下的鲁棒性，特别是当与prompt engineering修复结合使用时，能显著提高性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在多项选择问题回答（MCQA）基准测试中表现出色，但它们仍容易受到输入微小扰动的影响。

Method: 我们提出了一种名为Token Constraint Decoding (TCD) 的简单但在推理时非常有效的算法。该算法通过确保token级别预测之间的对齐来提高在有噪声情况下的鲁棒性。

Result: 通过在CommonsenseQA、MMLU和MMLU-Pro数据集上的广泛实验，我们展示了TCD，特别是当与prompt engineering（PE）修复结合使用时，大幅提高了弱化模型如Gemma3 1B在输入噪声情况下性能，整体性能提升高达39%。

Conclusion: 我们的发现确定了TCD作为一种实用的模型不可知方法，可用于改进现实世界不完美情况下的推理稳定性，并为更可靠的LLMs在安全关键或面向用户的场景中的部署铺平了道路。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance on
multiple-choice question answering (MCQA) benchmarks, yet they remain highly
vulnerable to minor input perturbations. In this paper, we introduce and
evaluate Token Constraint Decoding (TCD). This simple yet effective
inference-time algorithm enforces alignment between token-level predictions to
enhance robustness in noisy settings. Through extensive experiments on
CommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired
with prompt engineering (PE) fixes, significantly restores performance degraded
by input noise, yielding up to +39\% absolute gains for weaker models like
Gemma3 1B. Penalty sweep analyses further reveal that TCD implicitly
regularizes overconfident outputs, with different models requiring distinct
penalty schedules to maximize resilience. Our findings establish TCD as a
practical, model-agnostic approach for improving reasoning stability under
real-world imperfections and pave the way for more reliable deployment of LLMs
in safety-critical or user-facing applications.

</details>


### [50] [PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering](https://arxiv.org/abs/2506.09414)
*Xiujun Zhou,Pingjian Zhang,Deyou Tang*

Main category: cs.CL

PGDA-KGQA, a prompt-guided generative framework, enhances Knowledge Graph Question Answering by introducing diverse data augmentation strategies that include generating single-hop pseudo questions, semantic-preserving question rewriting, and creating multi-hop questions through answer-guided reverse path exploration.


<details>
  <summary>Details</summary>
Motivation: The motivation behind PGDA-KGQA is to tackle the difficulty in obtaining a large amount of diverse annotated data and the scarcity of multi-hop reasoning samples, which are critical challenges for effective model training in KGQA tasks.

Method: PGDA-KGQA adopts a unified prompt-design paradigm integrating various data augmentation strategies to enhance training data. It includes generating single-hop questions for better semantic alignment, conducting semantic-preserving question rewriting for robustness, and creating multi-hop questions through answer-guided reverse path exploration.

Result: Experimental results show that PGDA-KGQA surpasses existing state-of-the-art methods on KGQA datasets like WebQSP and ComplexWebQuestions, leading to improved accuracies in F1, Hits@1, and Accuracy metrics.

Conclusion: By employing a combination of data augmentation techniques centered around prompt-guided generative strategies, PGDA-KGQA demonstrates its effectiveness in enhancing training data diversity and improving logical form generation and answer retrieval performance for KGQA.

Abstract: Knowledge Graph Question Answering (KGQA) is a crucial task in natural
language processing that requires reasoning over knowledge graphs (KGs) to
answer natural language questions. Recent methods utilizing large language
models (LLMs) have shown remarkable semantic parsing capabilities but are
limited by the scarcity of diverse annotated data and multi-hop reasoning
samples. Traditional data augmentation approaches are focus mainly on
single-hop questions and prone to semantic distortion, while LLM-based methods
primarily address semantic distortion but usually neglect multi-hop reasoning,
thus limiting data diversity. The scarcity of multi-hop samples further weakens
models' generalization. To address these issues, we propose PGDA-KGQA, a
prompt-guided generative framework with multiple data augmentation strategies
for KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by
crafting meticulously engineered prompts that integrate the provided textual
content, it leverages LLMs to generate large-scale (question, logical form)
pairs for model training. Specifically, PGDA-KGQA enriches its training set by:
(1) generating single-hop pseudo questions to improve the alignment of question
semantics with KG relations; (2) applying semantic-preserving question
rewriting to improve robustness against linguistic variations; (3) employing
answer-guided reverse path exploration to create realistic multi-hop questions.
By adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA
utilizes the augmented data to enhance the accuracy of logical form generation
and thus improve answer retrieval performance. Experiments demonstrate that
outperforms state-of-the-art methods on standard KGQA datasets, achieving
improvements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by
1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.

</details>


### [51] [Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings](https://arxiv.org/abs/2506.09424)
*Md Messal Monem Miah,Adrita Anika,Xi Shi,Ruihong Huang*

Main category: cs.CL

本研究全面评估了大语言模型（LLMs）和大跨模态模型（LMMs）在多个领域中自动检测欺骗的能力。研究表明，微调的LLMs在文本欺骗检测任务中达到了最先进的性能，而LMMs在利用跨模态线索方面仍存在问题。


<details>
  <summary>Details</summary>
Motivation: 随着数字世界的日益发展，检测欺骗成为了一个关键且具有挑战性的任务。研究的目的是评估当前模型在检测欺骗方面的能力和限制。

Method: 研究评估了开源和商业LLMs在真实生活庭审面试、受指导的社交欺骗场景和欺骗评论数据集上的效能。研究系统地分析了不同实验设置下的欺骗检测效果，包括零样本和少样本方法以及随机或基于相似性的上下文实例选择。

Result: 研究结果显示微调的LLMs在文本欺骗检测中表现最佳，而LMMs在利用跨模态线索方面表现不佳。

Conclusion: 研究表明LLMs在处理和解释跨模态欺骗线索方面具有潜在的应用价值，但也指出它们在真实世界中的应用仍然存在局限性。

Abstract: Detecting deception in an increasingly digital world is both a critical and
challenging task. In this study, we present a comprehensive evaluation of the
automated deception detection capabilities of Large Language Models (LLMs) and
Large Multimodal Models (LMMs) across diverse domains. We assess the
performance of both open-source and commercial LLMs on three distinct datasets:
real life trial interviews (RLTD), instructed deception in interpersonal
scenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the
effectiveness of different experimental setups for deception detection,
including zero-shot and few-shot approaches with random or similarity-based
in-context example selection. Our results show that fine-tuned LLMs achieve
state-of-the-art performance on textual deception detection tasks, while LMMs
struggle to fully leverage cross-modal cues. Additionally, we analyze the
impact of auxiliary features, such as non-verbal gestures and video summaries,
and examine the effectiveness of different prompting strategies, including
direct label generation and chain-of-thought reasoning. Our findings provide
key insights into how LLMs process and interpret deceptive cues across
modalities, highlighting their potential and limitations in real-world
deception detection applications.

</details>


### [52] [Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting](https://arxiv.org/abs/2506.09428)
*Fei Ding,Baiqiao Wang*

Main category: cs.CL

我们提出了一种新的、更成本效益高的SFT方法，可以有效降低灾难性遗忘的风险，同时保持模型的泛化能力和提高特定任务的表现。


<details>
  <summary>Details</summary>
Motivation: 由于SFT虽然增强了大型语言模型的指令跟随能力和特定任务适应性，但往往也会削弱它们的通用能力，同时由于原始预训练数据的不可获得性，当第三方在开源模型上进行SFT时灾难性遗忘往往会被加剧，为了应对这一挑战，我们提出了这一方法。

Method: 我们提出了一个更成本效益高的SFT方法，通过重建基础模型的可能SFT指令分布，并使用多模型筛选过程选择最佳数据，然后将这些数据与新数据混合进行SFT，以有效降低灾难性遗忘的风险。

Result: 实验结果显示，我们的方法在保持通用领域的泛化能力的同时，提高了特定任务的表现。

Conclusion: 此方法能够在无需访问原始SFT数据的情况下，有效减少灾难性遗忘的风险，并在提高特定任务表现水准的同时保持模型的泛化能力。

Abstract: Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'
instruction-following capabilities and domain-specific task adaptability, often
diminishes their general capabilities. Moreover, due to the inaccessibility of
original pre-training data, catastrophic forgetting tends to be exacerbated
when third-party practitioners implement SFT on open-sourced models. To address
this challenge, we propose a novel, more cost-effective SFT method which could
effectively reduce the risk of catastrophic forgetting without access to
original SFT data. Our approach begins by reconstructing the likely SFT
instruction distribution of the base model, followed by a multi-model screening
process to select optimal data, which is then mixed with new data for SFT.
Experimental results demonstrate that our method preserves generalization
capabilities in general domains while improving task-specific performance.

</details>


### [53] [GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture](https://arxiv.org/abs/2506.09440)
*GigaChat team,Mamedov Valentin,Evgenii Kosarev,Gregory Leleytner,Ilya Shchuckin,Valeriy Berezovskiy,Daniil Smirnov,Dmitry Kozlov,Sergei Averkiev,Lukyanenko Ivan,Aleksandr Proshunin,Ainur Israfilova,Ivan Baskov,Artem Chervyakov,Emil Shakirov,Mikhail Kolesov,Daria Khomich,Darya Latortseva,Sergei Porkhun,Yury Fedorov,Oleg Kutuzov,Polina Kudriavtseva,Sofiia Soldatova,Kolodin Egor,Stanislav Pyatkin,Dzmitry Menshykh,Grafov Sergei,Eldar Damirov,Karlov Vladimir,Ruslan Gaitukiev,Arkadiy Shatenov,Alena Fenogenova,Nikita Savushkin,Fedor Minkin*

Main category: cs.CL

本文介绍了GigaChat系列俄罗斯LLM模型及其预训练过程，详细报告了模型架构和设计选择，提供了模型在俄语和英语基准测试上的性能评估，并展示了通过API，Telegram机器人和Web界面访问的顶级模型系统演示。开源发布了三个GigaChat模型。


<details>
  <summary>Details</summary>
Motivation: 由于开发针对俄语的基础模型所需的计算资源很大，俄语特定的基础模型的发展受到限制。本文的动机是填补这一空白，提供一套专门针对俄语的大型语言模型。

Method: 本文介绍了GigaChat系列俄罗斯LLM模型，这些模型可供不同需求使用，包括基础模型和指令调整版本。详细报告了模型架构，预训练过程以及实验设计选择。

Result: 通过对俄罗斯和英语基准的性能评估，GigaChat与多语言类似模型进行了比较，并证明在API，Telegram机器人和Web界面系统演示中的顶级模型的有效性。

Conclusion: 本文推出了一系列针对俄罗斯语言的大型语言模型GigaChat，开放了三个模型的源代码，推动了NLP研究机会和基于俄语的工业解决方案的开发。

Abstract: Generative large language models (LLMs) have become crucial for modern NLP
research and applications across various languages. However, the development of
foundational models specifically tailored to the Russian language has been
limited, primarily due to the significant computational resources required.
This paper introduces the GigaChat family of Russian LLMs, available in various
sizes, including base models and instruction-tuned versions. We provide a
detailed report on the model architecture, pre-training process, and
experiments to guide design choices. In addition, we evaluate their performance
on Russian and English benchmarks and compare GigaChat with multilingual
analogs. The paper presents a system demonstration of the top-performing models
accessible via an API, a Telegram bot, and a Web interface. Furthermore, we
have released three open GigaChat models in open-source
(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities
and support the development of industrial solutions for the Russian language.

</details>


### [54] [UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs](https://arxiv.org/abs/2506.09450)
*Prameshwar Thiyagarajan,Vaishnavi Parimi,Shamant Sai,Soumil Garg,Zhangir Meirbek,Nitin Yarlagadda,Kevin Zhu,Chris Kim*

Main category: cs.CL

本文提出了一个用于评估大型语言模型ToM能力的综合基准UniToMBench，通过多交互任务和演变故事场景来系统性地评估模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在理解人类心理状态方面存在挑战，本文旨在通过开发新的评测基准来系统性评估和改进模型对此类任务的表现。

Method: 引入了UniToMBench基准测试，该测试集成了SimToM和TOMBENCH的优点，设计了多交互任务和演变故事场景来系统地改进和评估LLMs的ToM能力。通过超过1,000个手写场景构成的自定义数据集，结合视角选取技术和多样化的评估指标，更好地刺激LLMs的社会认知能力。

Result: GPT-4o和GPT-4o Mini等模型在情绪和信念相关任务中表现良好，通常准确率高于80%，但在知识相关任务上的表现则显示出较大的变异性。

Conclusion: 结果表明，尽管某些模型在涉及情绪和信念相关的任务中表现出色，但在知识相关任务上的表现则存在较大差异，这突显了UniToMBench作为未来开发全面工具的价值。

Abstract: Theory of Mind (ToM), the ability to understand the mental states of oneself
and others, remains a challenging area for large language models (LLMs), which
often fail to predict human mental states accurately. In this paper, we
introduce UniToMBench, a unified benchmark that integrates the strengths of
SimToM and TOMBENCH to systematically improve and assess ToM capabilities in
LLMs by integrating multi-interaction task designs and evolving story
scenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,
UniToMBench combines perspective-taking techniques with diverse evaluation
metrics to better stimulate social cognition in LLMs. Through evaluation, we
observe that while models like GPT-4o and GPT-4o Mini show consistently high
accuracy in tasks involving emotional and belief-related scenarios, with
results usually above 80%, there is significant variability in their
performance across knowledge-based tasks. These results highlight both the
strengths and limitations of current LLMs in ToM-related tasks, underscoring
the value of UniToMBench as a comprehensive tool for future development. Our
code is publicly available here:
https://github.com/Shamant/unifiedtombenchmark.

</details>


### [55] [Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms](https://arxiv.org/abs/2506.09457)
*Zeguan Xiao,Yun Chen,Guanhua Chen*

Main category: cs.CL

The paper identifies a 'reward-generation gap' in Direct Alignment Algorithms (DAAs) and introduces POET to improve their performance in aligning large language models with human preferences.


<details>
  <summary>Details</summary>
Motivation: To address the reward-generation gap in DAAs by improving how the importance of prefix tokens during the LLM generation process is reflected in DAA's implicit reward functions.

Method: Prefix-Oriented Equal-length Training (POET) which truncates both preferred and dispreferred responses to match the shorter one's length to optimize DAA objectives.

Result: POET improves the performance of Direct Alignment Algorithms (DAAs) like DPO and SimPO, achieving up to 15.6 points improvement on AlpacaEval 2 and improvements across various downstream tasks.

Conclusion: The study highlights the critical role of addressing the reward-generation gap in the effectiveness of DAAs for aligning large language models with human preferences.

Abstract: Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization
(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient
alternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms
for aligning large language models (LLMs) with human preferences. However, DAAs
suffer from a fundamental limitation we identify as the "reward-generation gap"
-- a misalignment between optimization objectives during training and actual
generation performance during inference. In this paper, we find a contributor
to the reward-generation gap is the mismatch between the inherent importance of
prefix tokens during the LLM generation process and how this importance is
reflected in the implicit reward functions of DAAs. To bridge the gap, we
introduce a simple yet effective approach called Prefix-Oriented Equal-length
Training (POET), which truncates both preferred and dispreferred responses to
match the shorter one's length. Training with POET, where both responses in
each sample are truncated to equal length, resulting in diverse truncated
lengths across samples, the optimization of DAAs objective is implicitly
constrained to converge across all positions, thus paying more attention to
prefix tokens than the standard DAAs. We conduct experiments with DPO and
SimPO, two representative DAAs, demonstrating that POET improves over their
standard implementations, achieving up to 15.6 points in AlpacaEval 2 and
overall improvements across downstream tasks. Our results highlight the
importance of addressing the misalignment between reward optimization and
generation performance in DAAs.

</details>


### [56] [Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers](https://arxiv.org/abs/2506.09495)
*Ilanit Sobol,Shir Lissak,Refael Tikochinski,Tal Nakash,Anat Brunstein Klomek,Eyal Fruchter,Roi Reichart*

Main category: cs.CL

研究采用多种方法分析了181个有自杀倾向的YouTube频道和134个对照组频道，发现心理健康挣扎和YouTube参与度是与自杀行为有关的重要指标。


<details>
  <summary>Details</summary>
Motivation: 自杀仍然是西方国家的主要死因之一，这项研究尝试利用社交媒体上的数字足迹来洞察自杀行为。

Method: 采用计算的自下而上、混合型和专家驱动的自上而下三种互补方法，基于181个YouTube频道的纵向数据集（包含有生命威胁的尝试者）和134个控制频道进行研究。

Result: 自下而上的方法发现心理健康挣扎和YouTube参与度与自杀尝试有关；混合方法由临床专家审核生成的话题，但没有发现更多显著关联。自上而下的方法发现，在上传期间尝试自杀的人和尝试自杀之前就上传视频的人之间的显著差异是动机不同。

Conclusion: 研究整合了多方法，提供了一个细致的自杀性理解，结合了数字行为和临床见解。

Abstract: Suicide remains a leading cause of death in Western countries, underscoring
the need for new research approaches. As social media becomes central to daily
life, digital footprints offer valuable insight into suicidal behavior.
Focusing on individuals who attempted suicide while uploading videos to their
channels, we investigate: How do suicidal behaviors manifest on YouTube, and
how do they differ from expert knowledge? We applied complementary approaches:
computational bottom-up, hybrid, and expert-driven top-down, on a novel
longitudinal dataset of 181 YouTube channels from individuals with
life-threatening attempts, alongside 134 control channels. In the bottom-up
approach, we applied LLM-based topic modeling to identify behavioral
indicators. Of 166 topics, five were associated with suicide-attempt, with two
also showing temporal attempt-related changes ($p<.01$) - Mental Health
Struggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,
a clinical expert reviewed LLM-derived topics and flagged 19 as
suicide-related. However, none showed significant attempt-related temporal
effects beyond those identified bottom-up. Notably, YouTube Engagement, a
platform-specific indicator, was not flagged by the expert, underscoring the
value of bottom-up discovery. In the top-down approach, psychological
assessment of suicide attempt narratives revealed that the only significant
difference between individuals who attempted before and those attempted during
their upload period was the motivation to share this experience: the former
aimed to Help Others ($\beta=-1.69$, $p<.01$), while the latter framed it as
part of their Personal Recovery ($\beta=1.08$, $p<.01$). By integrating these
approaches, we offer a nuanced understanding of suicidality, bridging digital
behavior and clinical insights.
  * Within-group changes in relation to the suicide attempt.

</details>


### [57] [Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning](https://arxiv.org/abs/2506.09501)
*Jiayi Yuan,Hao Li,Xinheng Ding,Wenya Xie,Yu-Jhe Li,Wentian Zhao,Kun Wan,Jing Shi,Xia Hu,Zirui Liu*

Main category: cs.CL

研究揭示了数值精度对大语言模型（LLMs）性能可重复性的影响，提出了一种新的推理管道LayerCast，以提升模型输出的稳定性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在多个领域中已成为重要组成部分，并展示了卓越的性能。然而，其性能的重现性受到系统配置更改的影响，比如评估批次大小、GPU数量和GPU版本等，这些问题在推理模型中尤其显著，因为这些因素可以影响到最终的准确性。此研究旨在调查数值精度如何影响LLMs的可重复性。

Method: 通过控制实验，研究了在不同硬件、软件和精度设置下大语言模型推理中数值精度对可重复性的影响。提出了一个轻量级推理管道LayerCast，在该管道中权重以16位精度存储，但所有计算都在FP32中进行，从而在内存效率和数值稳定性之间取得平衡。

Result: 研究发现，不同硬件、软件和精度设置可以引发显著的生成回答差异，特别是在推理模型中。例如，DeepSeek-R1-Distill-Qwen-7B模型在bfloat16精度和贪婪解码下，由于GPU数量、类型和评估批次大小的差异可以展示出高达9%的准确率差异和9000个token的回答长度差异。

Conclusion: 这是首次系统调查数值精度对大语言模型（LLMs）推理可重复性影响的工作。提出了LayerCast，一个权衡内存效率和数值稳定性的轻量级推理管道，该管道的代码在https://github.com/nanomaoli/llm_reproducibility上开放获取。

Abstract: Large Language Models (LLMs) are now integral across various domains and have
demonstrated impressive performance. Progress, however, rests on the premise
that benchmark scores are both accurate and reproducible. We demonstrate that
the reproducibility of LLM performance is fragile: changing system
configuration such as evaluation batch size, GPU count, and GPU version can
introduce significant difference in the generated responses. This issue is
especially pronounced in reasoning models, where minor rounding differences in
early tokens can cascade into divergent chains of thought, ultimately affecting
accuracy. For instance, under bfloat16 precision with greedy decoding, a
reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation
in accuracy and 9,000 tokens difference in response length due to differences
in GPU count, type, and evaluation batch size. We trace the root cause of this
variability to the non-associative nature of floating-point arithmetic under
limited numerical precision. This work presents the first systematic
investigation into how numerical precision affects reproducibility in LLM
inference. Through carefully controlled experiments across various hardware,
software, and precision settings, we quantify when and how model outputs
diverge. Our analysis reveals that floating-point precision -- while critical
for reproducibility -- is often neglected in evaluation practices. Inspired by
this, we develop a lightweight inference pipeline, dubbed LayerCast, that
stores weights in 16-bit precision but performs all computations in FP32,
balancing memory efficiency with numerical stability. Code is available at
https://github.com/nanomaoli/llm_reproducibility.

</details>


### [58] [TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding](https://arxiv.org/abs/2506.09507)
*Bingheng Wu,Jingze Shi,Yifan Wu,Nan Tang,Yuyu Luo*

Main category: cs.CL

本文提出了\textbf{\ourRoPE}方法和\textbf{\model}架构，解决了Transformer与状态空间模型集成中的位置编码不兼容问题，实现了比标准Transformer模型更快的训练和推理速度以及更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 文章旨在解决Transformer模型与状态空间模型在集成过程中存在的位置编码不一致难题，该问题导致集成模型性能不佳和不连续现象。

Method: 本文提出了一种统一的旋转位置编码（\textbf{\ourRoPE}）方法，解决了Transformer和状态空间模型之间位置编码机制不一致的问题。基于\textbf{\ourRoPE}，本文介绍了一种名为\textbf{\model}的混合架构，该架构将Transformer和SSM层在统一的位置编码方案下无缝结合。

Result: \textbf{\model}在4K序列长度下，训练和推理速度比标准Transformer模型分别快42.3%和29.5%，在语言建模基准上比标准Transformer基线高出4%以上的准确性，且比常规的Transformer或SSM具有更好的扩展性，1.3B版本的平均准确性比320M版本升高了7.22%。

Conclusion: 结果表明，统一的位置编码解决了混合模型中的位置编码不兼容问题，使得高效、高性能的长上下文建模得以实现。

Abstract: Transformers exhibit proficiency in capturing long-range dependencies,
whereas State Space Models (SSMs) facilitate linear-time sequence modeling.
Notwithstanding their synergistic potential, the integration of these
architectures presents a significant challenge, primarily attributable to a
fundamental incongruity in their respective positional encoding mechanisms:
Transformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs
leverage implicit positional representations via convolutions. This divergence
often precipitates discontinuities and suboptimal performance. To address this
impediment, we propose a unified rotary position embedding (\textbf{\ourRoPE})
methodology, thereby establishing a consistent positional encoding framework
for both self-attention and state-space components. Using this \ourRoPE, we
introduce \textbf{\model}, a hybrid architecture that coherently integrates the
Transformer and SSM layers under this unified positional encoding scheme. At a
4K sequence length, \model exhibits training and inference speeds that are
\textbf{42.3\% and 29.5\% faster}, respectively, relative to standard
Transformer models. It also delivers higher accuracy: under comparable
settings, it surpasses a Transformer baseline by over 4\% on language modeling
benchmarks. \model furthermore scales more effectively: \model-1.3B gains
\textbf{7.22\%} in average accuracy over its 320M version (versus about 6\%
gains for equivalent Transformers or SSMs). Our results show that unified
positional encoding resolves positional incompatibility in hybrid models,
enabling efficient, high-performance long-context modeling.

</details>


### [59] [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/abs/2506.09513)
*Yu Sun,Xingyu Qian,Weiwen Xu,Hao Zhang,Chenghao Xiao,Long Li,Yu Rong,Wenbing Huang,Qifeng Bai,Tingyang Xu*

Main category: cs.CL

构建了ReasonMed数据集，发现结合详细的链式思维推理与简洁答案摘要的微调策略最有效，训练出的ReasonMed-7B模型在小于10B参数的模型中设立了新基准。


<details>
  <summary>Details</summary>
Motivation: 虽然基于推理的大语言模型在数学和编程方面表现出色，但它们在知识密集型医学问答方面的能力尚未被充分探索。

Method: 通过多智能体验证和细化过程构造了ReasonMed数据集，其中包括使用Error Refiner来改进和纠正由验证器标出的易错步骤。

Result: 结合详细的Chain-of-Thought推理与简洁的答案摘要进行微调，这种策略能产生最有效的医疗推理模型。基于此策略训练的ReasonMed-7B模型为小于10B参数的模型设立了新基准，比前一个最佳模型提升了4.17%，甚至在PubMedQA上的表现超过LLaMA3.1-70B模型4.60%。

Conclusion: 研究结果表明，详细的链式思维方法与简洁的答案总结相结合是训练医疗推理模型的最佳实践。

Abstract: Though reasoning-based large language models (LLMs) have excelled in
mathematics and programming, their capabilities in knowledge-intensive medical
question answering remain underexplored. To address this, we introduce
ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality
examples distilled from 1.7 million initial reasoning paths generated by
various LLMs. ReasonMed is constructed through a \textit{multi-agent
verification and refinement process}, where we design an \textit{Error Refiner}
to enhance the reasoning paths by identifying and correcting error-prone steps
flagged by a verifier. Leveraging ReasonMed, we systematically investigate best
practices for training medical reasoning models and find that combining
detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields
the most effective fine-tuning strategy. Based on this strategy, we train
ReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the
prior best by 4.17\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\%.

</details>


### [60] [KG-Infused RAG: Augmenting Corpus-Based RAG with External Knowledge Graphs](https://arxiv.org/abs/2506.09542)
*Dingjun Wu,Yukun Yan,Zhenghao Liu,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

本文提出了KG-Infused RAG框架，它通过整合知识图谱和实施传播激活，改进了RAG系统在检索生成中的准确性和多源信息整合能力。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常依赖单一的知识源，或是非结构化的文本或是结构化的知识，并且缺乏认知启发机制来激活相关的知识。为了应对这些问题，提出了KG-Infused RAG框架。

Method: KG-Infused RAG框架将知识图谱整合到RAG系统中，实现传播激活这一认知过程，该过程能够使概念关联和推理。KG-Infused RAG检索KG事实，相应地扩展查询，并通过结合语料库片段和结构化事实来增强生成，从而实现基于语义结构的可解释、多源检索。

Result: 实验结果表明，KG-Infused RAG在五个QA基准的测试中优于普通的RAG系统，提高了3.8%到13.8%的效果。

Conclusion: 实验结果显示，KG-Infused RAG在五个问答基准上始终优于普通的RAG（提高了3.8%到13.8%）。此外，当KG-Infused RAG集成到Self-RAG中时，也带来了进一步的性能提升，证明了它作为语料库RAG方法增强模块的有效性和多功能性。

Abstract: Retrieval-Augmented Generation (RAG) improves factual accuracy by grounding
responses in external knowledge. However, existing methods typically rely on a
single source, either unstructured text or structured knowledge. Moreover, they
lack cognitively inspired mechanisms for activating relevant knowledge. To
address these issues, we propose KG-Infused RAG, a framework that integrates
KGs into RAG systems to implement spreading activation, a cognitive process
that enables concept association and inference. KG-Infused RAG retrieves KG
facts, expands the query accordingly, and enhances generation by combining
corpus passages with structured facts, enabling interpretable, multi-source
retrieval grounded in semantic structure. We further improve KG-Infused RAG via
preference learning on sampled key stages in the pipeline. Experiments on five
QA benchmarks show that KG-Infused RAG consistently outperforms vanilla RAG (by
3.8% to 13.8%). Additionally, when integrated into Self-RAG, KG-Infused RAG
brings further performance gains, demonstrating its effectiveness and
versatility as a plug-and-play enhancement module for corpus-based RAG methods.

</details>


### [61] [MEDUSA: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions](https://arxiv.org/abs/2506.09556)
*Georgios Chatzichristodoulou,Despoina Kosmopoulou,Antonios Kritikos,Anastasia Poulopoulou,Efthymios Georgiou,Athanasios Katsamanis,Vassilis Katsouros,Alexandros Potamianos*

Main category: cs.CL

本文介绍了MEDUSA，一种新的多模态情感识别框架，适用于自然条件下的情感表示，该框架在Interspeech 2025挑战中排名第一。


<details>
  <summary>Details</summary>
Motivation: 情感识别是一个具有挑战性的任务，因为人类情感具有主观性，而且在自然条件下情感的表现是不均衡的。

Method: 我们提出了MEDUSA，一个包含四个训练阶段的多模态框架，旨在解决情感识别中的类别不平衡和情感模糊问题。前两个阶段训练了一个使用DeepSER（深度跨模态变压器融合机制的扩展）的分类器集合。DeepSER结合了预训练的自监督语音和语言表示。同时采用流形MixUp进行进一步的正则化处理。最后两个阶段优化了一个可训练的元分类器，它结合了集合预测。

Result: 我们的方法在Interspeech 2025的自然条件下基于语音的情感识别挑战任务1中排名第一。

Conclusion: 我们的方法MEDUSA利用多模态训练框架有效解决了情感识别中的主观性和不平衡问题。

Abstract: SER is a challenging task due to the subjective nature of human emotions and
their uneven representation under naturalistic conditions. We propose MEDUSA, a
multimodal framework with a four-stage training pipeline, which effectively
handles class imbalance and emotion ambiguity. The first two stages train an
ensemble of classifiers that utilize DeepSER, a novel extension of a deep
cross-modal transformer fusion mechanism from pretrained self-supervised
acoustic and linguistic representations. Manifold MixUp is employed for further
regularization. The last two stages optimize a trainable meta-classifier that
combines the ensemble predictions. Our training approach incorporates human
annotation scores as soft targets, coupled with balanced data sampling and
multitask learning. MEDUSA ranked 1st in Task 1: Categorical Emotion
Recognition in the Interspeech 2025: Speech Emotion Recognition in Naturalistic
Conditions Challenge.

</details>


### [62] [Gender Bias in English-to-Greek Machine Translation](https://arxiv.org/abs/2506.09558)
*Eleni Gkovedarou,Joke Daems,Luna De Bruyne*

Main category: cs.CL

本研究调查了Google Translate和DeepL在英语到希腊语翻译中的性别偏见，发现虽然在直接定义性别的句子中表现良好，但在性别模糊的情况下离性别包容性翻译还有差距。研究还初步验证了GPT-4o作为偏见缓解工具的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着对包容性语言需求的增长，人们对机器翻译（MT）系统可能强化性别刻板印象的担忧日益增加。本研究的动机在于调查商业MT系统中的性别偏见，并探索减少这类偏见的方法。

Method: 该研究通过使用手动编写的双语数据集GendEL，包含240个性别模糊和明确的句子，来评估Google Translate和DeepL在英语到希腊语翻译中的性别偏见。此外，它还测试了GPT-4o作为减少翻译偏见工具的可能性。

Result: 研究发现，尽管两个MT系统在直接定义性别时表现良好，但在未指定性别时，它们离产生性别包容或中立翻译仍有一定差距。GPT-4o展现了一定的潜力，但对于大多数模糊情境还是存在残留偏见。

Conclusion: 研究得出结论，现有的机器翻译系统在处理性别偏见方面仍有改进空间，而GPT-4o展现出了缓解这些偏见的可能性，但仍然有待于进一步减少内部残留的性别偏见。

Abstract: As the demand for inclusive language increases, concern has grown over the
susceptibility of machine translation (MT) systems to reinforce gender
stereotypes. This study investigates gender bias in two commercial MT systems,
Google Translate and DeepL, focusing on the understudied English-to-Greek
language pair. We address three aspects of gender bias: i) male bias, ii)
occupational stereotyping, and iii) errors in anti-stereotypical translations.
Additionally, we explore the potential of prompted GPT-4o as a bias mitigation
tool that provides both gender-explicit and gender-neutral alternatives when
necessary. To achieve this, we introduce GendEL, a manually crafted bilingual
dataset of 240 gender-ambiguous and unambiguous sentences that feature
stereotypical occupational nouns and adjectives. We find persistent gender bias
in translations by both MT systems; while they perform well in cases where
gender is explicitly defined, with DeepL outperforming both Google Translate
and GPT-4o in feminine gender-unambiguous sentences, they are far from
producing gender-inclusive or neutral translations when the gender is
unspecified. GPT-4o shows promise, generating appropriate gendered and neutral
alternatives for most ambiguous cases, though residual biases remain evident.

</details>


### [63] [Towards Open Foundation Language Model and Corpus for Macedonian: A Low-Resource Language](https://arxiv.org/abs/2506.09560)
*Stefan Krsteski,Matea Tashkovska,Borjan Sazdov,Hristijan Gjoreski,Branislav Gerazov*

Main category: cs.CL

Developed resources and a state-of-the-art model for Macedonian to enhance LLM proficiency, outperforming existing models in key benchmarks and receiving positive qualitative feedback.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of LLM capabilities for underrepresented languages such as Macedonian and to enhance applications in regions where such languages are spoken.

Method: We created several resources to facilitate the adoption of Large Language Models (LLMs) for Macedonian language proficiency. This includes the creation of the largest Macedonian corpus to date, totaling 3.5B words and a conversational instruction dataset, as well as a Macedonian evaluation suite. A domestic-yak model, an 8B-parameter state-of-the-art model, was trained on these resources.

Result: The developed model, domestic-yak, outperformed all existing 8B-parameter models across various benchmarks, and showed performance on par with much larger models. It also received higher qualitative ratings from native speakers for grammatical correctness and cultural appropriateness.

Conclusion: The research advances LLM proficiency in Macedonian and sets a foundation for progress in similarly underrepresented languages. All resources were openly released to encourage further research and application advancements.

Abstract: The increase in technological adoption worldwide comes with demands for novel
tools to be used by the general population. Large Language Models (LLMs)
provide a great opportunity in this respect, but their capabilities remain
limited for low-resource languages, restricting applications in countries where
such languages are spoken. We create several resources to facilitate the
adoption of LLMs and to support research advancements for Macedonian. We
collect the largest Macedonian corpus to date, consisting of 40GB of textual
data and totaling 3.5B words. To support conversational applications, we
collect a 106k-instance instruction dataset, carefully built to be culturally
grounded. For evaluation, we construct a Macedonian evaluation suite covering
seven benchmarks. Finally, we train domestic-yak, a state-of-the-art
8B-parameter model, on our curated datasets and evaluate it against eight
baseline models using the newly constructed benchmark suite. Our model
outperforms all existing models in the 8B parameter range across all
benchmarks, and achieves performance comparable to models up to 10x larger.
Furthermore, a qualitative analysis with native speakers reveals that our model
is preferred over larger counterparts, receiving higher ratings for grammatical
correctness and cultural appropriateness. All datasets, code, and model weights
are openly released, setting a foundation for advancing LLMs in similarly
underrepresented languages. These resources are publicly available at
github.com/LVSTCK for source code, and at huggingface.co/LVSTCK for pretrained
model weights and data.

</details>


### [64] [From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies](https://arxiv.org/abs/2506.09566)
*Blaž Škrlj,Boshko Koloski,Senja Pollak,Nada Lavrač*

Main category: cs.CL

本篇调查论文分析了知识图谱(KGs)与大型语言模型(LLMs)的协同效应，分类现有方法，并强调了可扩展性、计算效率和数据质量的重要性。


<details>
  <summary>Details</summary>
Motivation: 文章旨在探讨将KGs中的结构化知识融入LLMs中以提升事实基础和推理能力的方法，并识别这一领域的关键差距。

Method: 本文通过系统分析知识图谱(KGs)与大型语言模型(LLMs)的协同作用，将现有方法分为两大类：增强LLMs的KG，以及增强KGs的LLMs。

Result: 论文识别了当前方法的关键差距，并强调了结构化知识整合的相互优势。

Conclusion: 本论文提出了未来研究方向，如神经符号结合、动态知识图谱更新、数据可靠性及伦理考量，推动智能系统管理更复杂的真实世界知识任务。

Abstract: Integrating structured knowledge from Knowledge Graphs (KGs) into Large
Language Models (LLMs) enhances factual grounding and reasoning capabilities.
This survey paper systematically examines the synergy between KGs and LLMs,
categorizing existing approaches into two main groups: KG-enhanced LLMs, which
improve reasoning, reduce hallucinations, and enable complex question
answering; and LLM-augmented KGs, which facilitate KG construction, completion,
and querying. Through comprehensive analysis, we identify critical gaps and
highlight the mutual benefits of structured knowledge integration. Compared to
existing surveys, our study uniquely emphasizes scalability, computational
efficiency, and data quality. Finally, we propose future research directions,
including neuro-symbolic integration, dynamic KG updating, data reliability,
and ethical considerations, paving the way for intelligent systems capable of
managing more complex real-world knowledge tasks.

</details>


### [65] [Memorization in Language Models through the Lens of Intrinsic Dimension](https://arxiv.org/abs/2506.09591)
*Stefan Arnold*

Main category: cs.CL

本研究发现，内在维度（ID）对语言模型的记忆化有抑制作用，特别是对于高ID序列，在过度参数化模型和稀疏曝光的情况下，记忆化的可能性更低。


<details>
  <summary>Details</summary>
Motivation: 之前的研究已经确定了上下文长度、参数大小和重复频率等因素对非预期记忆化的影响，但关于潜在结构是如何调节记忆化的速率知之甚少。本研究旨在探讨这一点。

Method: 我们使用了内在维度（ID）作为量度序列在隐空间中的结构性复杂度的几何代理，来研究其如何调节语言模型的记忆化过程。

Result: 我们发现内在维度（ID）是抑制记忆化的信号：与低ID序列相比，高ID序列不太可能被记住，尤其是在过度参数化的模型和稀疏曝光的情况下。

Conclusion: 这些发现强调了规模、曝光和复杂性在塑造记忆化过程中的相互作用。

Abstract: Language Models (LMs) are prone to memorizing parts of their data during
training and unintentionally emitting them at generation time, raising concerns
about privacy leakage and disclosure of intellectual property. While previous
research has identified properties such as context length, parameter size, and
duplication frequency, as key drivers of unintended memorization, little is
known about how the latent structure modulates this rate of memorization. We
investigate the role of Intrinsic Dimension (ID), a geometric proxy for the
structural complexity of a sequence in latent space, in modulating
memorization. Our findings suggest that ID acts as a suppressive signal for
memorization: compared to low-ID sequences, high-ID sequences are less likely
to be memorized, particularly in overparameterized models and under sparse
exposure. These findings highlight the interaction between scale, exposure, and
complexity in shaping memorization.

</details>


### [66] [Benchmarking Debiasing Methods for LLM-based Parameter Estimates](https://arxiv.org/abs/2506.09627)
*Nicolas Audinet de Pieuchon,Adel Daoud,Connor T. Jerzak,Moa Johansson,Richard Johansson*

Main category: cs.CL

Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) offer an inexpensive yet powerful way to
annotate text, but are often inconsistent when compared with experts. These
errors can bias downstream estimates of population parameters such as
regression coefficients and causal effects. To mitigate this bias, researchers
have developed debiasing methods such as Design-based Supervised Learning (DSL)
and Prediction-Powered Inference (PPI), which promise valid estimation by
combining LLM annotations with a limited number of expensive expert
annotations. Although these methods produce consistent estimates under
theoretical assumptions, it is unknown how they compare in finite samples of
sizes encountered in applied research. We make two contributions: First, we
study how each method's performance scales with the number of expert
annotations, highlighting regimes where LLM bias or limited expert labels
significantly affect results. Second, we compare DSL and PPI across a range of
tasks, finding that although both achieve low bias with large datasets, DSL
often outperforms PPI on bias reduction and empirical efficiency, but its
performance is less consistent across datasets. Our findings indicate that
there is a bias-variance tradeoff at the level of debiasing methods, calling
for more research on developing metrics for quantifying their efficiency in
finite samples.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [67] [ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices](https://arxiv.org/abs/2506.09066)
*Maoyu Wang,Yao Lu,Jiaqi Nie,Zeyu Wang,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.CV

ReStNet solves the problem of deploying pre-trained models in IoT with varying resources by stitching two models together and selectively fine-tuning them for efficiency and adaptability.


<details>
  <summary>Details</summary>
Motivation: The motivation behind ReStNet is to address resource constraints in IoT applications, where deploying a single model across all platforms with heterogeneous computational and memory resources is not feasible and existing compression methods are inflexible.

Method: ReStNet, a Reusable and Stitchable Network, dynamically constructs a hybrid network by stitching two pre-trained models together, using Centered Kernel Alignment (CKA) for determining stitching points, and fine-tuning only the stitching layer for efficiency.

Result: ReStNet demonstrates flexible accuracy-efficiency trade-offs at runtime and significantly reduces training cost through its novel approach to stitching models.

Conclusion: ReStNet enables rapid adaptation to changing resource budgets and flexible combination of different model families, making it suitable for efficient deployment in varying IoT environments.

Abstract: With the rapid development of deep learning, a growing number of pre-trained
models have been publicly available. However, deploying these fixed models in
real-world IoT applications is challenging because different devices possess
heterogeneous computational and memory resources, making it impossible to
deploy a single model across all platforms. Although traditional compression
methods, such as pruning, quantization, and knowledge distillation, can improve
efficiency, they become inflexible once applied and cannot adapt to changing
resource constraints. To address these issues, we propose ReStNet, a Reusable
and Stitchable Network that dynamically constructs a hybrid network by
stitching two pre-trained models together. Implementing ReStNet requires
addressing several key challenges, including how to select the optimal
stitching points, determine the stitching order of the two pre-trained models,
and choose an effective fine-tuning strategy. To systematically address these
challenges and adapt to varying resource constraints, ReStNet determines the
stitching point by calculating layer-wise similarity via Centered Kernel
Alignment (CKA). It then constructs the hybrid model by retaining early layers
from a larger-capacity model and appending deeper layers from a smaller one. To
facilitate efficient deployment, only the stitching layer is fine-tuned. This
design enables rapid adaptation to changing budgets while fully leveraging
available resources. Moreover, ReStNet supports both homogeneous (CNN-CNN,
Transformer-Transformer) and heterogeneous (CNN-Transformer) stitching,
allowing to combine different model families flexibly. Extensive experiments on
multiple benchmarks demonstrate that ReStNet achieve flexible
accuracy-efficiency trade-offs at runtime while significantly reducing training
cost.

</details>


### [68] [Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations](https://arxiv.org/abs/2506.09067)
*Zhiyu Xue,Reza Abbasi-Asl,Ramtin Pedarsani*

Main category: cs.CV

本文提出一种在推理时抵御视觉和文本攻击的防御策略，同时提高Med-VLMs的安全性而不会显著降低其性能。


<details>
  <summary>Details</summary>
Motivation: Med-VLMs的安全漏洞尚未得到充分研究。需要一种方法，让这些模型能够拒绝有害查询，同时避免过度防御导致性能下降。

Method: 本论文提出了一种新颖的推理时防御策略，能够抵御视觉和文本形式的攻击，通过增加合成临床演示来提高模型的安全性。此外，还介绍了一种混合展示策略，旨在预算有限的情况下平衡安全性和性能。

Result: 实验结果显示，该防御策略能有效提升模型安全性，同时通过混合展示策略在面对有限预算时也能保持性能。

Conclusion: 研究证明，通过合成临床演示增量，提出的防御策略可以增强Med-VLMs的安全性而不显著损害性能。增加演示预算能进一步解决过度防御问题。

Abstract: Generative medical vision-language models~(Med-VLMs) are primarily designed
to generate complex textual information~(e.g., diagnostic reports) from
multimodal inputs including vision modality~(e.g., medical images) and language
modality~(e.g., clinical queries). However, their security vulnerabilities
remain underexplored. Med-VLMs should be capable of rejecting harmful queries,
such as \textit{Provide detailed instructions for using this CT scan for
insurance fraud}. At the same time, addressing security concerns introduces the
risk of over-defense, where safety-enhancing mechanisms may degrade general
performance, causing Med-VLMs to reject benign clinical queries. In this paper,
we propose a novel inference-time defense strategy to mitigate harmful queries,
enabling defense against visual and textual jailbreak attacks. Using diverse
medical imaging datasets collected from nine modalities, we demonstrate that
our defense strategy based on synthetic clinical demonstrations enhances model
safety without significantly compromising performance. Additionally, we find
that increasing the demonstration budget alleviates the over-defense issue. We
then introduce a mixed demonstration strategy as a trade-off solution for
balancing security and performance under few-shot demonstration budget
constraints.

</details>


### [69] [BG-HOP: A Bimanual Generative Hand-Object Prior](https://arxiv.org/abs/2506.09068)
*Sriram Krishna,Sravan Chittupalli,Sungjae Park*

Main category: cs.CV

提出了BG-HOP，一种旨在建模3D双手动-物互动的生成先验模型，展示了捕捉手和物联合分布的初步结果。


<details>
  <summary>Details</summary>
Motivation: 解决双手动-物互动数据有限的问题，通过扩展现有的单手生成先验模型。

Method: 开发了BG-HOP模型来生成双手动-物互动，并为给定物体合成抓取动作。

Result: 模型能够生成双手动-物互动，并为给定物体合成抓握。

Conclusion: 该研究展示了BG-HOP在生成双手动-物互动方面的潜力，并公开了代码和模型。

Abstract: In this work, we present BG-HOP, a generative prior that seeks to model
bimanual hand-object interactions in 3D. We address the challenge of limited
bimanual interaction data by extending existing single-hand generative priors,
demonstrating preliminary results in capturing the joint distribution of hands
and objects. Our experiments showcase the model's capability to generate
bimanual interactions and synthesize grasps for given objects. We make code and
models publicly available.

</details>


### [70] [Segment Any Architectural Facades (SAAF):An automatic segmentation model for building facades, walls and windows based on multimodal semantics guidance](https://arxiv.org/abs/2506.09071)
*Peilin Li,Jun Yin,Jing Zhong,Ran Luo,Pengyu Zeng,Miao Zhang*

Main category: cs.CV

研究提出了一种基于多模态语义引导的建筑外墙和窗户分割模型SAAF，该模型能够自主学习文本描述到图像分割的映射关系，并在多个外墙数据集上表现出优越的分割效果。


<details>
  <summary>Details</summary>
Motivation: 在建筑数字化发展背景下，建筑物信息模型和计算机辅助设计的效率可以通过自动分割墙体和窗户得到有效提升。

Method: 本研究提出了一种基于多模态语义引导的建筑外墙和窗户自动分割模型Segment Any Architectural Facades (SAAF)。首先，SAAF具备多模态语义协同特征提取机制，可以融合文本描述中的语义信息和图像特征，提高对建筑外墙构件的语义理解。其次，开发了一个端到端的训练框架，使模型能够自主学习从文本描述到图像分割的映射关系，减少人工干预的影响，提高模型的自动化和鲁棒性。

Result: 在多个外墙数据集上进行了广泛实验，SAAF的分割结果在mIoU指标上超越了现有的方法，表明该模型在面对多样化数据集时能够保持高精度分割能力。

Conclusion: SAAF模型在提高墙体和窗户分割任务的准确性与泛化能力方面取得了一定进展，可以提供关于建筑计算机视觉技术和多模态学习在建筑领域的新思路和技术路径参考。

Abstract: In the context of the digital development of architecture, the automatic
segmentation of walls and windows is a key step in improving the efficiency of
building information models and computer-aided design. This study proposes an
automatic segmentation model for building facade walls and windows based on
multimodal semantic guidance, called Segment Any Architectural Facades (SAAF).
First, SAAF has a multimodal semantic collaborative feature extraction
mechanism. By combining natural language processing technology, it can fuse the
semantic information in text descriptions with image features, enhancing the
semantic understanding of building facade components. Second, we developed an
end-to-end training framework that enables the model to autonomously learn the
mapping relationship from text descriptions to image segmentation, reducing the
influence of manual intervention on the segmentation results and improving the
automation and robustness of the model. Finally, we conducted extensive
experiments on multiple facade datasets. The segmentation results of SAAF
outperformed existing methods in the mIoU metric, indicating that the SAAF
model can maintain high-precision segmentation ability when faced with diverse
datasets. Our model has made certain progress in improving the accuracy and
generalization ability of the wall and window segmentation task. It is expected
to provide a reference for the development of architectural computer vision
technology and also explore new ideas and technical paths for the application
of multimodal learning in the architectural field.

</details>


### [71] [VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks](https://arxiv.org/abs/2506.09079)
*Xinlong Chen,Yuanxing Zhang,Yushuo Guan,Bohan Zeng,Yang Shi,Sihan Yang,Pengfei Wan,Qiang Liu,Liang Wang,Tieniu Tan*

Main category: cs.CV

This paper introduces VersaVid-R1, a new model for video reasoning, using innovative datasets and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the underdeveloped field of video-based reasoning due to data scarcity and ineffective training methodologies.

Method: The paper develops a new model named VersaVid-R1 by leveraging two novel datasets, DarkEventInfer and MixVidQA, and training with reinforcement learning guided by diverse reward functions.

Result: Experiments show that VersaVid-R1 surpasses existing models across various benchmarks, including video understanding, cognitive reasoning, and captioning tasks.

Conclusion: The conclusion is that the developed model, VersaVid-R1, represents a significant progress in video understanding and reasoning under the Reason-Then-Respond paradigm.

Abstract: Recent advancements in multimodal large language models have successfully
extended the Reason-Then-Respond paradigm to image-based reasoning, yet
video-based reasoning remains an underdeveloped frontier, primarily due to the
scarcity of high-quality reasoning-oriented data and effective training
methodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA,
two novel datasets specifically designed to stimulate the model's advanced
video understanding and reasoning abilities. DarkEventinfer presents videos
with masked event segments, requiring models to infer the obscured content
based on contextual video cues. MixVidQA, on the other hand, presents
interleaved video sequences composed of two distinct clips, challenging models
to isolate and reason about one while disregarding the other. Leveraging these
carefully curated training samples together with reinforcement learning guided
by diverse reward functions, we develop VersaVid-R1, the first versatile video
understanding and reasoning model under the Reason-Then-Respond paradigm
capable of handling multiple-choice and open-ended question answering, as well
as video captioning tasks. Extensive experiments demonstrate that VersaVid-R1
significantly outperforms existing models across a broad spectrum of
benchmarks, covering video general understanding, cognitive reasoning, and
captioning tasks.

</details>


### [72] [FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation](https://arxiv.org/abs/2506.09081)
*Zheqi He,Yesheng Liu,Jing-shu Zheng,Xuejing Li,Richeng Xuan,Jin-Ge Yao,Xi Yang*

Main category: cs.CV

FlagEvalMM is an open-source evaluation framework for multimodal models, designed to assess various vision-language tasks efficiently and accurately.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a comprehensive evaluation framework for multimodal models that can assess vision-language understanding and generation tasks with improved efficiency and accuracy.

Method: We decouple model inference from evaluation through an independent evaluation service, enabling flexible resource allocation and seamless integration of new tasks and models. FlagEvalMM uses advanced inference acceleration tools and asynchronous data loading to enhance evaluation efficiency.

Result: Extensive experiments demonstrate that FlagEvalMM provides accurate and efficient insights into the performance of multimodal models.

Conclusion: FlagEvalMM is a valuable tool for advancing research in multimodal models, offering a framework for accurate and efficient model evaluation.

Abstract: We present FlagEvalMM, an open-source evaluation framework designed to
comprehensively assess multimodal models across a diverse range of
vision-language understanding and generation tasks, such as visual question
answering, text-to-image/video generation, and image-text retrieval. We
decouple model inference from evaluation through an independent evaluation
service, thus enabling flexible resource allocation and seamless integration of
new tasks and models. Moreover, FlagEvalMM utilizes advanced inference
acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to
significantly enhance evaluation efficiency. Extensive experiments show that
FlagEvalMM offers accurate and efficient insights into model strengths and
limitations, making it a valuable tool for advancing multimodal research. The
framework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.

</details>


### [73] [AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models](https://arxiv.org/abs/2506.09082)
*Zheda Mai,Arpita Chowdhury,Zihe Wang,Sooyoung Jeon,Lemeng Wang,Jiacheng Hou,Jihyung Kil,Wei-Lun Chao*

Main category: cs.CV

为了系统地评估视觉基础模型(VFMs)，研究引入了AVA-Bench，一个明确分解14个原子视觉能力的基准，提供了精确的VFMs评估，并且减少了GPU时间，提升了评估效率。


<details>
  <summary>Details</summary>
Motivation: 针对视觉基础模型(VFMs)评估中的两个盲点：1)指令调优数据可能与VQA测试分布不匹配；2)VQA基准可能需要多种视觉能力，难以定位错误来源。为了弥补这些不足，引入AVA-Bench来优化VFMs的评估。

Method: 提出AVA-Bench，第一个明确分解14个原子视觉能力的基准，如定位、深度估计和空间理解，这些能力共同支持复杂的视觉推理任务。通过分离这些AVAs并在每个AVAs中匹配训练和测试分布，AVA-Bench可以精确指出VFMs的表现。

Result: 应用AVA-Bench评估领先的VFMs揭示了独特的"能力指纹"，将VFMs的选择从经验猜测转变为原则性工程工作。值得注意的是，0.5B LLM与7B LLM可以得到类似的VFMs排名，同时减少了8倍的GPU时间，使评估更高效。

Conclusion: 通过提供全面透明的基准，AVA-Bench为下一代表视觉基础模型(VFMs)的开发奠定了基础。

Abstract: The rise of vision foundation models (VFMs) calls for systematic evaluation.
A common approach pairs VFMs with large language models (LLMs) as
general-purpose heads, followed by evaluation on broad Visual Question
Answering (VQA) benchmarks. However, this protocol has two key blind spots: (i)
the instruction tuning data may not align with VQA test distributions, meaning
a wrong prediction can stem from such data mismatch rather than a VFM' visual
shortcomings; (ii) VQA benchmarks often require multiple visual abilities,
making it hard to tell whether errors stem from lacking all required abilities
or just a single critical one. To address these gaps, we introduce AVA-Bench,
the first benchmark that explicitly disentangles 14 Atomic Visual Abilities
(AVAs) -- foundational skills like localization, depth estimation, and spatial
understanding that collectively support complex visual reasoning tasks. By
decoupling AVAs and matching training and test distributions within each,
AVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench
to leading VFMs thus reveals distinctive "ability fingerprints," turning VFM
selection from educated guesswork into principled engineering. Notably, we find
that a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours
by 8x, enabling more efficient evaluation. By offering a comprehensive and
transparent benchmark, we hope AVA-Bench lays the foundation for the next
generation of VFMs.

</details>


### [74] [BakuFlow: A Streamlining Semi-Automatic Label Generation Tool](https://arxiv.org/abs/2506.09083)
*Jerry Lin,Partick P. W. Chen*

Main category: cs.CV

本文介绍了BakuFlow，一种改进的半自动标注生成工具，它具有多种创新功能，旨在提高标注效率和灵活性。


<details>
  <summary>Details</summary>
Motivation: 准确标注数据仍然是计算机视觉中的瓶颈，特别是在大规模任务中，手动标注费时且容易出错。

Method: 本文介绍了BakuFlow，一种半自动标注生成工具，它具有多种功能：（1）一个可调放大镜，用于像素级精确的手动修正；（2）一个交互式数据增强模块，可用于多样化训练数据集；（3）标签传播功能，可以将标注对象迅速复制到连续帧中；（4）自动标注模块，基于改进的YOLOE框架。

Result: BakuFlow在实际的计算机视觉和工业场景中显著减少了标注工作量，提升了标注效率。

Conclusion: 这些创新使得BakuFlow特别适用于对象检测和追踪任务，在实际操作中大幅减少了标注工作量并提高了效率。

Abstract: Accurately labeling (or annotation) data is still a bottleneck in computer
vision, especially for large-scale tasks where manual labeling is
time-consuming and error-prone. While tools like LabelImg can handle the
labeling task, some of them still require annotators to manually label each
image. In this paper, we introduce BakuFlow, a streamlining semi-automatic
label generation tool. Key features include (1) a live adjustable magnifier for
pixel-precise manual corrections, improving user experience; (2) an interactive
data augmentation module to diversify training datasets; (3) label propagation
for rapidly copying labeled objects between consecutive frames, greatly
accelerating annotation of video data; and (4) an automatic labeling module
powered by a modified YOLOE framework. Unlike the original YOLOE, our extension
supports adding new object classes and any number of visual prompts per class
during annotation, enabling flexible and scalable labeling for dynamic,
real-world datasets. These innovations make BakuFlow especially effective for
object detection and tracking, substantially reducing labeling workload and
improving efficiency in practical computer vision and industrial scenarios.

</details>


### [75] [Bias Analysis in Unconditional Image Generative Models](https://arxiv.org/abs/2506.09106)
*Xiaofeng Zhang,Michelle Lin,Simon Lacoste-Julien,Aaron Courville,Yash Goyal*

Main category: cs.CV

通过分析无条件图像生成模型，研究揭示了偏见检测的敏感度问题，尤其是在连续性属性上。研究表明需要改善评估框架并更深入理解属性的社会复杂性。


<details>
  <summary>Details</summary>
Motivation: 尽管关于人工智能模型偏见问题的文献越来越多，但关于无条件生成中偏见产生的机制仍然不清晰。研究旨在探讨这一问题，特别是在无条件图像生成中的偏见变化。

Method: 我们定义了一个属性的偏见为该属性在观察分布中的存在概率与其在理想参考分布中的期望比例之间的差异。研究通过训练若干无条件图像生成模型，并采用一个常用的偏见评估框架来研究训练分布和生成分布之间的偏见变化。

Result: 实验结果显示，检测到的属性转移相对较小。属性转移对用于标记生成图像的分类器非常敏感，尤其是在决策边界位于高密度区域时。研究还发现在连续性属性上这种分类器敏感度常被观察到，而非显示出二元特性。

Conclusion: 研究强调了需要更具有代表性的标注实践，对评估框架的不足进行更深入的审查，并认识到在评价偏见时属性的社会复杂性。

Abstract: The widespread adoption of generative AI models has raised growing concerns
about representational harm and potential discriminatory outcomes. Yet, despite
growing literature on this topic, the mechanisms by which bias emerges -
especially in unconditional generation - remain disentangled. We define the
bias of an attribute as the difference between the probability of its presence
in the observed distribution and its expected proportion in an ideal reference
distribution. In our analysis, we train a set of unconditional image generative
models and adopt a commonly used bias evaluation framework to study bias shift
between training and generated distributions. Our experiments reveal that the
detected attribute shifts are small. We find that the attribute shifts are
sensitive to the attribute classifier used to label generated images in the
evaluation framework, particularly when its decision boundaries fall in
high-density regions. Our empirical analysis indicates that this classifier
sensitivity is often observed in attributes values that lie on a spectrum, as
opposed to exhibiting a binary nature. This highlights the need for more
representative labeling practices, understanding the shortcomings through
greater scrutiny of evaluation frameworks, and recognizing the socially complex
nature of attributes when evaluating bias.

</details>


### [76] [CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation](https://arxiv.org/abs/2506.09109)
*Arnav Yayavaram,Siddharth Yayavaram,Simran Khanuja,Michael Saxon,Graham Neubig*

Main category: cs.CV

引入新型评估度量CAIRe，计算给定标签集下的图像文化相关性，克服了文化偏见的测量瓶颈，实验证明了它与人类评估的一致性。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型在不同文化背景下的一致性能变得尤为重要，但目前存在性能下降、事实不准确或输出冒犯性等方面的挑战。由于缺乏可靠评估文化偏见的方法，这一问题难以解决。

Method: CAIRe评估指标用于评估给定用户定义标签集的图像的文化相关性程度，通过将图像中的实体和概念与知识库进行关联，并使用事实信息为每个文化标签提供独立的分级判断。

Result: 在手动创建的数据集上，CAIRe超过所有基线模型28个百分点的F1得分。另外，对于两种普遍概念的数据集，CAIRe与人类评分达到0.56和0.66的皮尔逊相关系数，彰显出其对大规模图像源人类判断的强大一致性。

Conclusion: CAIRe作为一种新的评估度量标准，可以有效地衡量文本到图像生成中产生的文化相关性，其结果与人类的评价高度一致。

Abstract: As text-to-image models become increasingly prevalent, ensuring their
equitable performance across diverse cultural contexts is critical. Efforts to
mitigate cross-cultural biases have been hampered by trade-offs, including a
loss in performance, factual inaccuracies, or offensive outputs. Despite
widespread recognition of these challenges, an inability to reliably measure
these biases has stalled progress. To address this gap, we introduce CAIRe, a
novel evaluation metric that assesses the degree of cultural relevance of an
image, given a user-defined set of labels. Our framework grounds entities and
concepts in the image to a knowledge base and uses factual information to give
independent graded judgments for each culture label. On a manually curated
dataset of culturally salient but rare items built using language models, CAIRe
surpasses all baselines by 28% F1 points. Additionally, we construct two
datasets for culturally universal concept, one comprising of T2I-generated
outputs and another retrieved from naturally occurring data. CAIRe achieves
Pearson's correlations of 0.56 and 0.66 with human ratings on these sets, based
on a 5-point Likert scale of cultural relevance. This demonstrates its strong
alignment with human judgment across diverse image sources.

</details>


### [77] [Seedance 1.0: Exploring the Boundaries of Video Generation Models](https://arxiv.org/abs/2506.09113)
*Yu Gao,Haoyuan Guo,Tuyen Hoang,Weilin Huang,Lu Jiang,Fangyuan Kong,Huixia Li,Jiashi Li,Liang Li,Xiaojie Li,Xunsong Li,Yifu Li,Shanchuan Lin,Zhijie Lin,Jiawei Liu,Shu Liu,Xiaonan Nie,Zhiwu Qing,Yuxi Ren,Li Sun,Zhi Tian,Rui Wang,Sen Wang,Guoqiang Wei,Guohong Wu,Jie Wu,Ruiqi Xia,Fei Xiao,Xuefeng Xiao,Jiangqiao Yan,Ceyuan Yang,Jianchao Yang,Runkai Yang,Tao Yang,Yihang Yang,Zilyu Ye,Xuejiao Zeng,Yan Zeng,Heng Zhang,Yang Zhao,Xiaozheng Zheng,Peihao Zhu,Jiaxin Zou,Feilong Zuo*

Main category: cs.CV

Seedance 1.0 是一个高性能的视频生成模型，通过多源数据整理、架构设计以及优化技术实现快速和高质量的视频生成。


<details>
  <summary>Details</summary>
Motivation: 当前的视频生成模型在遵循指令、运动合理性和视觉质量之间难以平衡，此报告旨在介绍Seedance 1.0在这些方面的改进。

Method: Seedance 1.0 通过多源数据整理、高效的架构设计、优化的后训练方法以及多阶段知识蒸馏策略实现模型加速，从而提升视频生成的质量和速度。

Result: Seedance 1.0 能够在复杂多主体的情境中准确遵循指令，生成具有时空连贯性和主体一致性高质量视频，比现有最先进的视频生成模型快10倍。

Conclusion: Seedance 1.0 实现了高质量与快速视频生成的结合，具有显著的时空流畅性和结构稳定性，在复杂多主体情境中保持指令精确遵循和多场景叙事连贯性。

Abstract: Notable breakthroughs in diffusion modeling have propelled rapid improvements
in video generation, yet current foundational model still face critical
challenges in simultaneously balancing prompt following, motion plausibility,
and visual quality. In this report, we introduce Seedance 1.0, a
high-performance and inference-efficient video foundation generation model that
integrates several core technical improvements: (i) multi-source data curation
augmented with precision and meaningful video captioning, enabling
comprehensive learning across diverse scenarios; (ii) an efficient architecture
design with proposed training paradigm, which allows for natively supporting
multi-shot generation and jointly learning of both text-to-video and
image-to-video tasks. (iii) carefully-optimized post-training approaches
leveraging fine-grained supervised fine-tuning, and video-specific RLHF with
multi-dimensional reward mechanisms for comprehensive performance improvements;
(iv) excellent model acceleration achieving ~10x inference speedup through
multi-stage distillation strategies and system-level optimizations. Seedance
1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds
(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance
1.0 stands out with high-quality and fast video generation having superior
spatiotemporal fluidity with structural stability, precise instruction
adherence in complex multi-subject contexts, native multi-shot narrative
coherence with consistent subject representation.

</details>


### [78] [Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models](https://arxiv.org/abs/2506.09229)
*Sungwon Hwang,Hyojin Jang,Kinam Kim,Minho Park,Jaegul choo*

Main category: cs.CV

本文介绍了一种新的跨帧表征对齐技术CREPA，旨在改进视频扩散模型的帧间语义一致性，在实验中展示了其在视觉质量及跨帧一致性上的提升。


<details>
  <summary>Details</summary>
Motivation: 研究指出，尽管在用户级别的视频模型微调以生成反映特定训练数据属性的视频方面存在挑战，但这一方向仍鲜有探索，具有重要的实际意义。

Method: 研究首先提出了一种简单适应于视频模型的REPA方法，并发现其限制在于无法有效保持帧间语义的一致性。为此，引入CREPA技术，该技术通过将当前帧的隐藏状态与相邻帧的外部特征进行对齐，来解决这一问题。

Result: 该研究提出了用于视频扩散模型（VDMs）的跨帧表征对齐（CREPA）技术，以解决现有方法在保留帧间语义一致性方面的不足。实验表明，与传统方法相比，CREPA 能够在大量参数高效微调方法（如LoRA）中提升视频生成的视觉保真度和帧间语义连贯性。研究在多个具有不同属性的数据集上验证了CREPA的适用性。

Conclusion: CREPA作为一项新技术，已在大规模VDMs实验证明了其在视觉保真度和跨帧语义连贯性方面的提升效果，并在不同属性的数据集上展示了其广泛适用性。

Abstract: Fine-tuning Video Diffusion Models (VDMs) at the user level to generate
videos that reflect specific attributes of training data presents notable
challenges, yet remains underexplored despite its practical importance.
Meanwhile, recent work such as Representation Alignment (REPA) has shown
promise in improving the convergence and quality of DiT-based image diffusion
models by aligning, or assimilating, its internal hidden states with external
pretrained visual features, suggesting its potential for VDM fine-tuning. In
this work, we first propose a straightforward adaptation of REPA for VDMs and
empirically show that, while effective for convergence, it is suboptimal in
preserving semantic consistency across frames. To address this limitation, we
introduce Cross-frame Representation Alignment (CREPA), a novel regularization
technique that aligns hidden states of a frame with external features from
neighboring frames. Empirical evaluations on large-scale VDMs, including
CogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual
fidelity and cross-frame semantic coherence when fine-tuned with
parameter-efficient methods such as LoRA. We further validate CREPA across
diverse datasets with varying attributes, confirming its broad applicability.
Project page: https://crepavideo.github.io

</details>


### [79] [PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies](https://arxiv.org/abs/2506.09237)
*Mojtaba Nafez,Amirhossein Koochakian,Arad Maleki,Jafar Habibi,Mohammad Hossein Rohban*

Main category: cs.CV

提出了一个对抗鲁棒的PatchGuard异常检测与定位方法，采用ViT架构加入前景感知伪异常样本，并通过对抗训练增强了模型在边缘条件下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的异常检测与定位方法容易受到对抗攻击的影响，原因在于训练数据的局限性，只包含了正常的无标签样本。PatchGuard旨在通过引入伪异常样本和局部掩模来解决这一问题，提升系统在对抗环境下的表现。

Method: PatchGuard, 一种基于前景感知伪异常样本来增强ViT架构的异常检测和定位方法，通过对抗训练优化模型对抗鲁棒性。

Result: 实验结果表明，在多个工业和医疗数据集上，PatchGuard相较于先前的方法在对抗设置下分别提高了53.2%的异常检测性能和68.5%的异常定位性能，并且在非对抗设置下也保持了良好的准确度。

Conclusion: PatchGuard展示了在多种数据集上的优越性能，特别是在对抗环境下显著提升了异常检测和定位的准确性。

Abstract: Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields
that demand high reliability, such as medical imaging and industrial
monitoring. However, current AD and AL approaches are often susceptible to
adversarial attacks due to limitations in training data, which typically
include only normal, unlabeled samples. This study introduces PatchGuard, an
adversarially robust AD and AL method that incorporates pseudo anomalies with
localization masks within a Vision Transformer (ViT)-based architecture to
address these vulnerabilities. We begin by examining the essential properties
of pseudo anomalies, and follow it by providing theoretical insights into the
attention mechanisms required to enhance the adversarial robustness of AD and
AL systems. We then present our approach, which leverages Foreground-Aware
Pseudo-Anomalies to overcome the deficiencies of previous anomaly-aware
methods. Our method incorporates these crafted pseudo-anomaly samples into a
ViT-based framework, with adversarial training guided by a novel loss function
designed to improve model robustness, as supported by our theoretical analysis.
Experimental results on well-established industrial and medical datasets
demonstrate that PatchGuard significantly outperforms previous methods in
adversarial settings, achieving performance gains of $53.2\%$ in AD and
$68.5\%$ in AL, while also maintaining competitive accuracy in non-adversarial
settings. The code repository is available at
https://github.com/rohban-lab/PatchGuard .

</details>


### [80] [UFM: A Simple Path towards Unified Dense Correspondence with Flow](https://arxiv.org/abs/2506.09278)
*Yuchen Zhang,Nikhil Keetha,Chenwei Lyu,Bhuvan Jhamb,Yutian Chen,Yuheng Qiu,Jay Karhade,Shreyas Jha,Yaoyu Hu,Deva Ramanan,Sebastian Scherer,Wenshan Wang*

Main category: cs.CV

UFM, a unified model for image correspondence and optical flow, outperforms specialized models in both accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: To improve performance and generalization for dense image correspondence tasks by unifying wide-baseline image matching and optical flow estimation through a single model.

Method: Unified Flow & Matching model (UFM) uses a simple, generic transformer architecture for dense image correspondence across wide-baseline scenarios and optical flow estimation.

Result: UFM is 28% more accurate than state-of-the-art flow methods and 62% less error and 6.7x faster than dense wide-baseline matchers.

Conclusion: The development of UFM showcases the potential of unified training in achieving better and faster results for both optical flow and dense correspondence tasks, opening new avenues for real-time and multi-modal applications.

Abstract: Dense image correspondence is central to many applications, such as visual
odometry, 3D reconstruction, object association, and re-identification.
Historically, dense correspondence has been tackled separately for
wide-baseline scenarios and optical flow estimation, despite the common goal of
matching content between two images. In this paper, we develop a Unified Flow &
Matching model (UFM), which is trained on unified data for pixels that are
co-visible in both source and target images. UFM uses a simple, generic
transformer architecture that directly regresses the (u,v) flow. It is easier
to train and more accurate for large flows compared to the typical
coarse-to-fine cost volumes in prior work. UFM is 28% more accurate than
state-of-the-art flow methods (Unimatch), while also having 62% less error and
6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to
demonstrate that unified training can outperform specialized approaches across
both domains. This result enables fast, general-purpose correspondence and
opens new directions for multi-modal, long-range, and real-time correspondence
tasks.

</details>


### [81] [Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery](https://arxiv.org/abs/2506.09299)
*Sindhu Boddu,Arindam Mukherjee*

Main category: cs.CV

This paper introduces an optimized YOLOv4-Tiny model for object detection in aerial emergency response imagery. The model is quantized to INT8, reducing its size and enhancing its inference speed without sacrificing accuracy, making it ideal for real-time use on low-power devices.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is the need for a lightweight and energy-efficient object detection model specifically tailored for aerial imagery in emergency response, as existing models are not optimized for these conditions and publicly available datasets are insufficient.

Method: The paper employs the YOLOv4-Tiny model optimized through post-training quantization to INT8 precision for object detection in aerial imagery during emergencies. A custom-curated dataset with 10,820 annotated images specific to emergency scenarios is used for training, addressing the scarcity of publicly available drone-view emergency data.

Result: The quantized YOLOv4-Tiny model achieves comparable detection performance to the YOLOv5-small model but with a significantly reduced model size (from 22.5 MB to 6.4 MB) and 44% faster inference speed.

Conclusion: The quantized YOLOv4-Tiny model is highly suitable for real-time object detection on low-power edge devices in emergency response situations, providing energy efficiency and speed.

Abstract: This paper presents a lightweight and energy-efficient object detection
solution for aerial imagery captured during emergency response situations. We
focus on deploying the YOLOv4-Tiny model, a compact convolutional neural
network, optimized through post-training quantization to INT8 precision. The
model is trained on a custom-curated aerial emergency dataset, consisting of
10,820 annotated images covering critical emergency scenarios. Unlike prior
works that rely on publicly available datasets, we created this dataset
ourselves due to the lack of publicly available drone-view emergency imagery,
making the dataset itself a key contribution of this work. The quantized model
is evaluated against YOLOv5-small across multiple metrics, including mean
Average Precision (mAP), F1 score, inference time, and model size. Experimental
results demonstrate that the quantized YOLOv4-Tiny achieves comparable
detection performance while reducing the model size from 22.5 MB to 6.4 MB and
improving inference speed by 44\%. With a 71\% reduction in model size and a
44\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly
suitable for real-time emergency detection on low-power edge devices.

</details>


### [82] [Efficient Edge Deployment of Quantized YOLOv4-Tiny for Aerial Emergency Object Detection on Raspberry Pi 5](https://arxiv.org/abs/2506.09300)
*Sindhu Boddu,Arindam Mukherjee*

Main category: cs.CV

量化后的YOLOv4-Tiny模型在Raspberry Pi 5上实现了快速高效的空袭紧急图像对象检测。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估在资源受限的边缘设备上部署和部署实时对象检测模型在空袭紧急图像中的性能。

Method: 使用TensorFlow Lite的后训练量化技术将YOLOv4-Tiny模型量化到INT8精度，并在Raspberry Pi 5上评估了检测速度、功耗和热性能。

Result: 量化模型在每张图片上的推理时间为28.2毫秒，并且平均功耗为13.85瓦，相比于32位浮点格式模型显示出了显著的功耗降低。检测精度在关键的几类紧急情况如救护车、警车、消防车和车祸中的表现仍旧稳健。

Conclusion: 结果表明，低功耗嵌入式AI系统在安全关键的应急响应应用中的实时部署具有潜力。

Abstract: This paper presents the deployment and performance evaluation of a quantized
YOLOv4-Tiny model for real-time object detection in aerial emergency imagery on
a resource-constrained edge device the Raspberry Pi 5. The YOLOv4-Tiny model
was quantized to INT8 precision using TensorFlow Lite post-training
quantization techniques and evaluated for detection speed, power consumption,
and thermal feasibility under embedded deployment conditions. The quantized
model achieved an inference time of 28.2 ms per image with an average power
consumption of 13.85 W, demonstrating a significant reduction in power usage
compared to its FP32 counterpart. Detection accuracy remained robust across key
emergency classes such as Ambulance, Police, Fire Engine, and Car Crash. These
results highlight the potential of low-power embedded AI systems for real-time
deployment in safety-critical emergency response applications.

</details>


### [83] [MSSDF: Modality-Shared Self-supervised Distillation for High-Resolution Multi-modal Remote Sensing Image Learning](https://arxiv.org/abs/2506.09327)
*Tong Wang,Guanzhou Chen,Xiaodong Zhang,Chenxi Liu,Jiaqi Wang,Xiaoliang Tan,Wenchao Guo,Qingyuan Yang,Kaiqi Zhang*

Main category: cs.CV

本文提出了一种多模态自监督学习框架，用于遥感图像的预训练，该框架在多个下游任务中表现出色，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 遥感图像解释对于环境监测、城市规划和灾害评估至关重要，但获取高质量的标注数据既耗时又昂贵。为此，本文提出了一种新的学习框架来解决这一问题。

Method: 该框架利用高分辨率RGB图像、多光谱数据和数字表面模型（DSM），引入了信息感知自适应掩模策略、跨模态掩模机制和多任务自监督目标。

Result: 评估结果显示该方法在大多数任务上优于现有的预训练方法，具体在Potsdam和Vaihingen语义分割任务中使用50%训练集达成的mIoU分数分别达到了78.30%和76.50%，在SECOND数据集二元变化检测任务中mIoU分数达到47.51%。

Conclusion: 通过实验验证，提出的方法在多个遥感应用任务上表现出优越性，证明了其在遥感图像解释中的应用潜力。

Abstract: Remote sensing image interpretation plays a critical role in environmental
monitoring, urban planning, and disaster assessment. However, acquiring
high-quality labeled data is often costly and time-consuming. To address this
challenge, we proposes a multi-modal self-supervised learning framework that
leverages high-resolution RGB images, multi-spectral data, and digital surface
models (DSM) for pre-training. By designing an information-aware adaptive
masking strategy, cross-modal masking mechanism, and multi-task self-supervised
objectives, the framework effectively captures both the correlations across
different modalities and the unique feature structures within each modality. We
evaluated the proposed method on multiple downstream tasks, covering typical
remote sensing applications such as scene classification, semantic
segmentation, change detection, object detection, and depth estimation.
Experiments are conducted on 15 remote sensing datasets, encompassing 26 tasks.
The results demonstrate that the proposed method outperforms existing
pretraining approaches in most tasks. Specifically, on the Potsdam and
Vaihingen semantic segmentation tasks, our method achieved mIoU scores of
78.30\% and 76.50\%, with only 50\% train-set. For the US3D depth estimation
task, the RMSE error is reduced to 0.182, and for the binary change detection
task in SECOND dataset, our method achieved mIoU scores of 47.51\%, surpassing
the second CS-MAE by 3 percentage points. Our pretrain code, checkpoints, and
HR-Pairs dataset can be found in https://github.com/CVEO/MSSDF.

</details>


### [84] [CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation](https://arxiv.org/abs/2506.09343)
*Yuxing Long,Jiyao Zhang,Mingjie Pan,Tianshu Wu,Taewhan Kim,Hao Dong*

Main category: cs.CV

研究提出首个基于手册的家电操作基准CheckManual和首个基于手册的操作规划模型ManualPlan，强调机器人通过理解手册操作家电的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的与手册相关的研究局限于问答任务，而现有的操作研究忽视了手册的重要作用，无法理解多页手册。研究旨在提高机器人对手册的理解，进而提升其操作家电的能力。

Method: 本研究提出了一种基于手册的家电操作基准CheckManual，设计了一个大型模型辅助的人类修订数据生成管道来创建基于CAD家电模型的手册。此外，还提出了首个基于手册的操作规划模型ManualPlan，设立了一组基准以评估模型性能。

Result: 建立了新的基于手册的操作挑战、度量标准和模拟环境，为模型性能评估奠定了基础。此外，通过提出ManualPlan模型，设定了CheckManual基准的一组初步基准。

Conclusion: 研究展示了通过机器人理解和学习家电手册来执行复杂任务的潜力，未来的研究将进一步优化模型，提高机器人理解和操作家电的能力。

Abstract: Correct use of electrical appliances has significantly improved human life
quality. Unlike simple tools that can be manipulated with common sense,
different parts of electrical appliances have specific functions defined by
manufacturers. If we want the robot to heat bread by microwave, we should
enable them to review the microwave manual first. From the manual, it can learn
about component functions, interaction methods, and representative task steps
about appliances. However, previous manual-related works remain limited to
question-answering tasks while existing manipulation researchers ignore the
manual's important role and fail to comprehend multi-page manuals. In this
paper, we propose the first manual-based appliance manipulation benchmark
CheckManual. Specifically, we design a large model-assisted human-revised data
generation pipeline to create manuals based on CAD appliance models. With these
manuals, we establish novel manual-based manipulation challenges, metrics, and
simulator environments for model performance evaluation. Furthermore, we
propose the first manual-based manipulation planning model ManualPlan to set up
a group of baselines for the CheckManual benchmark.

</details>


### [85] [An Effective End-to-End Solution for Multimodal Action Recognition](https://arxiv.org/abs/2506.09345)
*Songping Wang,Xiantao Hu,Yueming Lyu,Caifeng Shan*

Main category: cs.CV

A multimodal action recognition solution is proposed, utilizing data enhancement, pre-training with transfer learning, and various prediction enhancement methods, achieving high accuracy on the competition leaderboard.


<details>
  <summary>Details</summary>
Motivation: To address the challenges faced in tri-modal action recognition tasks due to the scarcity of tri-modal data, a comprehensive multimodal action recognition solution is proposed.

Method: First, existing data are transformed and expanded via optimized data enhancement techniques. More RGB datasets are used for pre-training the backbone network through transfer learning. Secondly, multimodal spatial features are extracted using 2D CNNs and combined with TSM for multimodal spatial-temporal feature extraction and improved computational efficiency. Common prediction enhancement methods, such as SWA, Ensemble, and TTA, are employed to integrate the knowledge of models from different training periods of the same architecture and different architectures for comprehensive action prediction.

Result: The approach achieved a Top-1 accuracy of 99% and a Top-5 accuracy of 100% on the competition leaderboard.

Conclusion: The proposed solution showcases superiority in tri-modal action recognition tasks by effectively utilizing multimodal information and improving computational efficiency.

Abstract: Recently, multimodal tasks have strongly advanced the field of action
recognition with their rich multimodal information. However, due to the
scarcity of tri-modal data, research on tri-modal action recognition tasks
faces many challenges. To this end, we have proposed a comprehensive multimodal
action recognition solution that effectively utilizes multimodal information.
First, the existing data are transformed and expanded by optimizing data
enhancement techniques to enlarge the training scale. At the same time, more
RGB datasets are used to pre-train the backbone network, which is better
adapted to the new task by means of transfer learning. Secondly, multimodal
spatial features are extracted with the help of 2D CNNs and combined with the
Temporal Shift Module (TSM) to achieve multimodal spatial-temporal feature
extraction comparable to 3D CNNs and improve the computational efficiency. In
addition, common prediction enhancement methods, such as Stochastic Weight
Averaging (SWA), Ensemble and Test-Time augmentation (TTA), are used to
integrate the knowledge of models from different training periods of the same
architecture and different architectures, so as to predict the actions from
different perspectives and fully exploit the target information. Ultimately, we
achieved the Top-1 accuracy of 99% and the Top-5 accuracy of 100% on the
competition leaderboard, demonstrating the superiority of our solution.

</details>


### [86] [Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation](https://arxiv.org/abs/2506.09350)
*Shanchuan Lin,Ceyuan Yang,Hao He,Jianwen Jiang,Yuxi Ren,Xin Xia,Yang Zhao,Xuefeng Xiao,Lu Jiang*

Main category: cs.CV

本文提出了一种自回归对抗后训练（AAPT）方法，将预训练的潜在视频扩散模型转换为实时、交互式的视频生成器。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模视频生成模型计算密集，限制了它们在实时和交互式应用中的使用。我们提出的方法旨在解决这一问题，使其能够在实时应用中使用。

Method: Structure

Result: {\"tldr\": \"本文提出了一种自回归对抗后训练（AAPT）方法，将预训练的潜在视频扩散模型转换为实时、交互式的视频生成器。\", \"motivation\": \"现有的大规模视频生成模型计算密集，限制了它们在实时和交互式应用中的使用。我们提出的方法旨在解决这一问题，使其能够在实时应用中使用。\", \"method\": \"该方法使用单个神经函数评估（1NFE）逐帧生成潜在帧，并使用对抗性训练作为自回归生成的有效范式。\", \"result\": \"实验表明，我们的模型能够实现实时（24fps），流式视频生成，在单个H100上达到736x416分辨率，或在8xH100上达到1280x720分辨率，最长可达一分钟。\", \"conclusion\": \"该方法证明了在长视频生成过程中，对抗性训练可以有效减少误差积累，并且使其在实时和交互式环境中可行。\"}

Conclusion: 该方法证明了在长视频生成过程中，对抗性训练可以有效减少误差积累，并且使其在实时和交互式环境中可行。

Abstract: Existing large-scale video generation models are computationally intensive,
preventing adoption in real-time and interactive applications. In this work, we
propose autoregressive adversarial post-training (AAPT) to transform a
pre-trained latent video diffusion model into a real-time, interactive video
generator. Our model autoregressively generates a latent frame at a time using
a single neural function evaluation (1NFE). The model can stream the result to
the user in real time and receive interactive responses as controls to generate
the next latent frame. Unlike existing approaches, our method explores
adversarial training as an effective paradigm for autoregressive generation.
This not only allows us to design an architecture that is more efficient for
one-step generation while fully utilizing the KV cache, but also enables
training the model in a student-forcing manner that proves to be effective in
reducing error accumulation during long video generation. Our experiments
demonstrate that our 8B model achieves real-time, 24fps, streaming video
generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to
a minute long (1440 frames). Visit our research website at
https://seaweed-apt.com/2

</details>


### [87] [A new approach for image segmentation based on diffeomorphic registration and gradient fields](https://arxiv.org/abs/2506.09357)
*Junchao Zhou*

Main category: cs.CV

本文提出了一种基于微分同胚变换的2D图像分割的新方法，该方法使用LDDMM框架下的变分方法，通过将模板曲线变形为图像域中的曲线来进行图像分割，实现了对理论基础和灵活性的结合。


<details>
  <summary>Details</summary>
Motivation: 传统的边缘检测和变分方法已经被广泛研究，而最近的深度学习方法虽然显示出了有希望的结果，但通常需要大量的训练数据。本文旨在提出一个不需要大量数据集且理论基础坚实的图像分割方法。

Method: 本文提出了一种新颖的2D图像分割变分框架，该框架结合了形状分析和微分同胚变换的概念。该方法通过LDDMM框架下的微分同胚变换将模板曲线变形为图像域中的曲线，使用PyKeops库实现Python中基于GPU的加速，从而实现了依赖于理论基础灵活准确地分割图像，而无需大量数据集。

Result: 该方法实现了基于理论基础和灵活性的图像分割，不依赖于大量的数据集。

Conclusion: 本文提出的方法为2D图像分割提供了一种新的理论基础深厚且灵活的方法论，从而减少了对大量训练数据的依赖。

Abstract: Image segmentation is a fundamental task in computer vision aimed at
delineating object boundaries within images. Traditional approaches, such as
edge detection and variational methods, have been widely explored, while recent
advances in deep learning have shown promising results but often require
extensive training data. In this work, we propose a novel variational framework
for 2D image segmentation that integrates concepts from shape analysis and
diffeomorphic transformations. Our method models segmentation as the
deformation of a template curve via a diffeomorphic transformation of the image
domain, using the Large Deformation Diffeomorphic Metric Mapping (LDDMM)
framework. The curve evolution is guided by a loss function that compares the
deformed curve to the image gradient field, formulated through the varifold
representation of geometric shapes. The approach is implemented in Python with
GPU acceleration using the PyKeops library. This framework allows for accurate
segmentation with a flexible and theoretically grounded methodology that does
not rely on large datasets.

</details>


### [88] [SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing](https://arxiv.org/abs/2506.09363)
*Hongguang Zhu,Yunchao Wei,Mengyu Wang,Siyu Jiao,Yan Fang,Jiannan Huang,Yao Zhao*

Main category: cs.CV

Introduces SAGE, a method combining semantic-augment erasing and global-local retention mechanism for safer diffusion models


<details>
  <summary>Details</summary>
Motivation: address safety risks such as unsafe content generation and copyright infringement in diffusion models, and avoid the limitation of existing concept erasing methods

Method: semantic-augment erasing and global-local collaborative retention mechanism

Result: SAGE demonstrates comprehensive superiority in the safe generation of diffusion models compared with other methods

Conclusion: SAGE effectively mitigates safety risks and retains performance in irrelevant concepts compared to previous methods

Abstract: Diffusion models (DMs) have achieved significant progress in text-to-image
generation. However, the inevitable inclusion of sensitive information during
pre-training poses safety risks, such as unsafe content generation and
copyright infringement. Concept erasing finetunes weights to unlearn
undesirable concepts, and has emerged as a promising solution. However,
existing methods treat unsafe concept as a fixed word and repeatedly erase it,
trapping DMs in ``word concept abyss'', which prevents generalized
concept-related erasing. To escape this abyss, we introduce semantic-augment
erasing which transforms concept word erasure into concept domain erasure by
the cyclic self-check and self-erasure. It efficiently explores and unlearns
the boundary representation of concept domain through semantic spatial
relationships between original and training DMs, without requiring additional
preprocessed data. Meanwhile, to mitigate the retention degradation of
irrelevant concepts while erasing unsafe concepts, we further propose the
global-local collaborative retention mechanism that combines global semantic
relationship alignment with local predicted noise preservation, effectively
expanding the retentive receptive field for irrelevant concepts. We name our
method SAGE, and extensive experiments demonstrate the comprehensive
superiority of SAGE compared with other methods in the safe generation of DMs.
The code and weights will be open-sourced at
https://github.com/KevinLight831/SAGE.

</details>


### [89] [ScaleLSD: Scalable Deep Line Segment Detection Streamlined](https://arxiv.org/abs/2506.09369)
*Zeran Ke,Bin Tan,Xianwei Zheng,Yujun Shen,Tianfu Wu,Nan Xue*

Main category: cs.CV

研究开发了ScaleLSD，一种高性能、高效的线段检测模型，专为大规模和多样化的自然图像设计。


<details>
  <summary>Details</summary>
Motivation: 研究目的是学习一个适用于任何自然图像的鲁棒领域无关的线段检测模型，以解决大规模图像的线段几何表征问题。

Method: 此研究提出了ScaleLSD，一种专注于在大规模无标签现实图像中学习线段检测的自监督学习方法。ScaleLSD通过复习和简化深度和非深度LSD方法的基本设计，实现高性能和高效的线段检测。

Result: 实验表明，ScaleLSD在零样本图像检测性能，单视角3D几何估计，双视角线段匹配和多视角3D线映射方面表现出色，首次超越了传统的非深度线段检测方法。

Conclusion: 研究得出了ScaleLSD是第一个在所测试的所有方面都优于原有的非深度LSD的深度学习方法，显著扩展和增强了图像线几何的多功能性。

Abstract: This paper studies the problem of Line Segment Detection (LSD) for the
characterization of line geometry in images, with the aim of learning a
domain-agnostic robust LSD model that works well for any natural images. With
the focus of scalable self-supervised learning of LSD, we revisit and
streamline the fundamental designs of (deep and non-deep) LSD approaches to
have a high-performing and efficient LSD learner, dubbed as ScaleLSD, for the
curation of line geometry at scale from over 10M unlabeled real-world images.
Our ScaleLSD works very well to detect much more number of line segments from
any natural images even than the pioneered non-deep LSD approach, having a more
complete and accurate geometric characterization of images using line segments.
Experimentally, our proposed ScaleLSD is comprehensively testified under
zero-shot protocols in detection performance, single-view 3D geometry
estimation, two-view line segment matching, and multiview 3D line mapping, all
with excellent performance obtained. Based on the thorough evaluation, our
ScaleLSD is observed to be the first deep approach that outperforms the
pioneered non-deep LSD in all aspects we have tested, significantly expanding
and reinforcing the versatility of the line geometry of images. Code and Models
are available at https://github.com/ant-research/scalelsd

</details>


### [90] [UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images](https://arxiv.org/abs/2506.09378)
*Qijian Tian,Xin Tan,Jingyu Gong,Yuan Xie,Lizhuang Ma*

Main category: cs.CV

This paper introduces UniForward, a model that reconstructs 3D scenes and semantic fields from sparse-view images in real time, achieving high-quality results without requiring camera parameters or depth information, and demonstrating state-of-the-art performance in novel view synthesis and segmentation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the challenges of embedding semantics into 3D representations, achieving real-time reconstruction, and ensuring the practical applicability using only images as inputs without additional information like camera parameters or ground truth depth.

Method: Our method, UniForward, is a feed-forward model designed to predict 3D Gaussians with anisotropic semantic features from uncalibrated, sparse-view images. It employs a dual-branch decoupled decoder to unify the 3D scene and semantic field representation, and uses a loss-guided view sampler for training stability.

Result: Experiments have shown that UniForward can reconstruct 3D scenes and semantic fields in real-time from sparse-view images, achieving high-quality rendering and view-consistent semantic features with state-of-the-art performance.

Conclusion: The conclusion is that the proposed UniForward model effectively unifies 3D scene and semantic field reconstruction by predicting anisotropic semantic features in 3D Gaussians, from sparse-view input images, and achieves state-of-the-art performance without needing camera parameters or ground truth depth.

Abstract: We propose a feed-forward Gaussian Splatting model that unifies 3D scene and
semantic field reconstruction. Combining 3D scenes with semantic fields
facilitates the perception and understanding of the surrounding environment.
However, key challenges include embedding semantics into 3D representations,
achieving generalizable real-time reconstruction, and ensuring practical
applicability by using only images as input without camera parameters or ground
truth depth. To this end, we propose UniForward, a feed-forward model to
predict 3D Gaussians with anisotropic semantic features from only uncalibrated
and unposed sparse-view images. To enable the unified representation of the 3D
scene and semantic field, we embed semantic features into 3D Gaussians and
predict them through a dual-branch decoupled decoder. During training, we
propose a loss-guided view sampler to sample views from easy to hard,
eliminating the need for ground truth depth or masks required by previous
methods and stabilizing the training process. The whole model can be trained
end-to-end using a photometric loss and a distillation loss that leverages
semantic features from a pre-trained 2D semantic model. At the inference stage,
our UniForward can reconstruct 3D scenes and the corresponding semantic fields
in real time from only sparse-view images. The reconstructed 3D scenes achieve
high-quality rendering, and the reconstructed 3D semantic field enables the
rendering of view-consistent semantic features from arbitrary views, which can
be further decoded into dense segmentation masks in an open-vocabulary manner.
Experiments on novel view synthesis and novel view segmentation demonstrate
that our method achieves state-of-the-art performances for unifying 3D scene
and semantic field reconstruction.

</details>


### [91] [ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices](https://arxiv.org/abs/2506.09066)
*Maoyu Wang,Yao Lu,Jiaqi Nie,Zeyu Wang,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.CV

ReStNet dynamically stitches two pre-trained models to adapt to varying resource constraints in IoT devices, achieving flexible accuracy-efficiency trade-offs and reduced training costs.


<details>
  <summary>Details</summary>
Motivation: The motivation behind ReStNet is to address the challenges of deploying fixed pre-trained models in IoT devices with heterogeneous resources. Traditional compression methods fail to adapt dynamically to changing resource constraints.

Method: ReStNet stitches two pre-trained models together, determining the stitching points using layer-wise similarity calculated via Centered Kernel Alignment. It retains early layers from a higher capacity model and appends deeper layers from a smaller model. Only the stitching layer is fine-tuned. It supports both homogeneous and heterogeneous stitching.

Result: Experiments show that ReStNet achieves flexible accuracy-efficiency trade-offs and significantly reduces training costs.

Conclusion: ReStNet offers a reusable and stitchable design that enables rapid adaptation to different resource constraints and supports flexible combination of various model families.

Abstract: With the rapid development of deep learning, a growing number of pre-trained
models have been publicly available. However, deploying these fixed models in
real-world IoT applications is challenging because different devices possess
heterogeneous computational and memory resources, making it impossible to
deploy a single model across all platforms. Although traditional compression
methods, such as pruning, quantization, and knowledge distillation, can improve
efficiency, they become inflexible once applied and cannot adapt to changing
resource constraints. To address these issues, we propose ReStNet, a Reusable
and Stitchable Network that dynamically constructs a hybrid network by
stitching two pre-trained models together. Implementing ReStNet requires
addressing several key challenges, including how to select the optimal
stitching points, determine the stitching order of the two pre-trained models,
and choose an effective fine-tuning strategy. To systematically address these
challenges and adapt to varying resource constraints, ReStNet determines the
stitching point by calculating layer-wise similarity via Centered Kernel
Alignment (CKA). It then constructs the hybrid model by retaining early layers
from a larger-capacity model and appending deeper layers from a smaller one. To
facilitate efficient deployment, only the stitching layer is fine-tuned. This
design enables rapid adaptation to changing budgets while fully leveraging
available resources. Moreover, ReStNet supports both homogeneous (CNN-CNN,
Transformer-Transformer) and heterogeneous (CNN-Transformer) stitching,
allowing to combine different model families flexibly. Extensive experiments on
multiple benchmarks demonstrate that ReStNet achieve flexible
accuracy-efficiency trade-offs at runtime while significantly reducing training
cost.

</details>


### [92] [Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations](https://arxiv.org/abs/2506.09067)
*Zhiyu Xue,Reza Abbasi-Asl,Ramtin Pedarsani*

Main category: cs.CV

本文提出了一种新防御策略，用于在生成式医学视觉-语言模型中处理有害查询，同时避免降级性能。实验表明，我们的策略可以提高安全性，并通过增加演示预算改善性能。此外，我们还提供了一种混合策略来应对预算约束。


<details>
  <summary>Details</summary>
Motivation: 当前生成式医学视觉-语言模型(Med-VLMs)的主要功能是从包括视觉和语言模态在内的多模态输入生成复杂的文本信息，但其安全漏洞尚未得到充分研究。这些模型需要能够拒绝有害查询，同时避免过度防御导致拒绝良性临床查询。

Method: 我们提出了一种新颖的推理时防御策略，用于缓解有害查询，可以防御视觉和文本越狱攻击。该策略基于合成临床演示，通过在多样化医学图像数据集上进行验证，展示了提高模型安全性的同时不显著降低性能。此外，我们还提出了一种混合演示策略，作为在少量演示预算约束下平衡安全性和性能的折衷方案。

Result: 我们的防御策略基于合成临床演示，使用来自九种模式的多样化医疗图像数据集进行验证，证明该策略能够增强模型的安全性而不会显著降低性能。此外，我们发现增加演示预算可以缓解过度防御问题。

Conclusion: 我们提出了一种新型推理时防御策略，该策略利用合成临床演示来缓解有害查询，可以防御视觉和文本越狱攻击。这种策略能够提高模型的安全性而不显著降低性能，并且我们还发现增加演示预算可以减轻过度防御的问题。针对少量演示预算约束下的安全性和性能平衡问题，我们提出了一种混合演示策略作为折衷方案。

Abstract: Generative medical vision-language models~(Med-VLMs) are primarily designed
to generate complex textual information~(e.g., diagnostic reports) from
multimodal inputs including vision modality~(e.g., medical images) and language
modality~(e.g., clinical queries). However, their security vulnerabilities
remain underexplored. Med-VLMs should be capable of rejecting harmful queries,
such as \textit{Provide detailed instructions for using this CT scan for
insurance fraud}. At the same time, addressing security concerns introduces the
risk of over-defense, where safety-enhancing mechanisms may degrade general
performance, causing Med-VLMs to reject benign clinical queries. In this paper,
we propose a novel inference-time defense strategy to mitigate harmful queries,
enabling defense against visual and textual jailbreak attacks. Using diverse
medical imaging datasets collected from nine modalities, we demonstrate that
our defense strategy based on synthetic clinical demonstrations enhances model
safety without significantly compromising performance. Additionally, we find
that increasing the demonstration budget alleviates the over-defense issue. We
then introduce a mixed demonstration strategy as a trade-off solution for
balancing security and performance under few-shot demonstration budget
constraints.

</details>


### [93] [BG-HOP: A Bimanual Generative Hand-Object Prior](https://arxiv.org/abs/2506.09068)
*Sriram Krishna,Sravan Chittupalli,Sungjae Park*

Main category: cs.CV

本文介绍的BG-HOP是一种生成性先验模型，用于建模三维双手与物体的交互，解决了双手交互数据有限的问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决双手交互数据有限的问题并捕捉双手与物体交互的联合分布。

Method: 通过扩展现有的单手生成先验模型，BG-HOP旨在建模三维的双手与物体的交互。

Result: 实验展示了该模型生成双手交互和为给定物体合成抓取姿势的能力。

Conclusion: 代码和模型将公开提供。

Abstract: In this work, we present BG-HOP, a generative prior that seeks to model
bimanual hand-object interactions in 3D. We address the challenge of limited
bimanual interaction data by extending existing single-hand generative priors,
demonstrating preliminary results in capturing the joint distribution of hands
and objects. Our experiments showcase the model's capability to generate
bimanual interactions and synthesize grasps for given objects. We make code and
models publicly available.

</details>


### [94] [Segment Any Architectural Facades (SAAF):An automatic segmentation model for building facades, walls and windows based on multimodal semantics guidance](https://arxiv.org/abs/2506.09071)
*Peilin Li,Jun Yin,Jing Zhong,Ran Luo,Pengyu Zeng,Miao Zhang*

Main category: cs.CV

研究提出了一种名为SAAF的模型，通过结合文本描述和图像特征，实现建筑立面墙和窗户的高精度自动分割，实验结果表明其在多个数据集上的表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在建筑的数字化发展中，建筑立面墙和窗户的自动分割是提升建筑信息模型和计算机辅助设计效率的关键步骤。

Method: 本研究提出了一种基于多模态语义引导的建筑立面墙和窗户的自动分割模型，称为SAAF（Segment Any Architectural Facades）。首先，SAAF具有多模态语义协作特征提取机制，通过结合自然语言处理技术，将文本描述中所包含的语义信息与图像特征融合起来，增强对建筑立面组件的语义理解。其次，我们开发了一种端到端的训练框架，使模型能够自主学习从文本描述到图像分割的映射关系，减少了人工干预对分割结果的影响，提高了模型的自动化程度和鲁棒性。

Result: 我们在多个建筑立面数据集上进行了广泛的实验。SAAF的分割结果在mIoU指标上优于现有的方法，表明SAAF模型在面对多种数据集时能保持较高的分割精度。

Conclusion: 本模型在提高墙和窗户分割任务的准确性和泛化能力方面取得了一定的进展。预计将为建筑计算机视觉技术的发展提供参考，也为多模态学习在建筑领域的应用探索新的思路和技术路径。

Abstract: In the context of the digital development of architecture, the automatic
segmentation of walls and windows is a key step in improving the efficiency of
building information models and computer-aided design. This study proposes an
automatic segmentation model for building facade walls and windows based on
multimodal semantic guidance, called Segment Any Architectural Facades (SAAF).
First, SAAF has a multimodal semantic collaborative feature extraction
mechanism. By combining natural language processing technology, it can fuse the
semantic information in text descriptions with image features, enhancing the
semantic understanding of building facade components. Second, we developed an
end-to-end training framework that enables the model to autonomously learn the
mapping relationship from text descriptions to image segmentation, reducing the
influence of manual intervention on the segmentation results and improving the
automation and robustness of the model. Finally, we conducted extensive
experiments on multiple facade datasets. The segmentation results of SAAF
outperformed existing methods in the mIoU metric, indicating that the SAAF
model can maintain high-precision segmentation ability when faced with diverse
datasets. Our model has made certain progress in improving the accuracy and
generalization ability of the wall and window segmentation task. It is expected
to provide a reference for the development of architectural computer vision
technology and also explore new ideas and technical paths for the application
of multimodal learning in the architectural field.

</details>


### [95] [VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks](https://arxiv.org/abs/2506.09079)
*Xinlong Chen,Yuanxing Zhang,Yushuo Guan,Bohan Zeng,Yang Shi,Sihan Yang,Pengfei Wan,Qiang Liu,Liang Wang,Tieniu Tan*

Main category: cs.CV

通过引入DarkEventInfer和MixVidQA两个数据集以及VersaVid-R1模型，研究解决了视频理解与推理能力不足的问题，显著提升了模型在多任务处理上的性能。


<details>
  <summary>Details</summary>
Motivation: 由于高质量的以推理为导向的数据和有效的训练方法较为稀缺，导致视频推理领域发展滞后。本研究旨在通过引入特定的数据集和开发新模型来改善这一问题。

Method: 提出了两个新数据集DarkEventInfer和MixVidQA，分别要求模型根据上下文视频线索推断缺失段落以及在交织的视频序列中隔离并理解片段。模型训练通过强化学习结合多样化的奖励函数来引导。

Result: 实验证明，VersaVid-R1模型在多项视频理解、认知推理和视频描述任务的基准测试中表现出色，明显超越现有模型。

Conclusion: VersaVid-R1是首款采用Reason-Then-Respond范式的多用途视频理解和推理模型，能够有效处理多项任务，并在多项基准测试中展示出优异性能。

Abstract: Recent advancements in multimodal large language models have successfully
extended the Reason-Then-Respond paradigm to image-based reasoning, yet
video-based reasoning remains an underdeveloped frontier, primarily due to the
scarcity of high-quality reasoning-oriented data and effective training
methodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA,
two novel datasets specifically designed to stimulate the model's advanced
video understanding and reasoning abilities. DarkEventinfer presents videos
with masked event segments, requiring models to infer the obscured content
based on contextual video cues. MixVidQA, on the other hand, presents
interleaved video sequences composed of two distinct clips, challenging models
to isolate and reason about one while disregarding the other. Leveraging these
carefully curated training samples together with reinforcement learning guided
by diverse reward functions, we develop VersaVid-R1, the first versatile video
understanding and reasoning model under the Reason-Then-Respond paradigm
capable of handling multiple-choice and open-ended question answering, as well
as video captioning tasks. Extensive experiments demonstrate that VersaVid-R1
significantly outperforms existing models across a broad spectrum of
benchmarks, covering video general understanding, cognitive reasoning, and
captioning tasks.

</details>


### [96] [FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation](https://arxiv.org/abs/2506.09081)
*Zheqi He,Yesheng Liu,Jing-shu Zheng,Xuejing Li,Richeng Xuan,Jin-Ge Yao,Xi Yang*

Main category: cs.CV

FlagEvalMM是一个开源评估框架，用于多模态模型的全面评估，它提升了评估效率并提供模型性能的精确洞见。


<details>
  <summary>Details</summary>
Motivation: 旨在为多模态模型提供一个全面的评估工具，促进该领域的研究和发展。

Method: 我们介绍了FlagEvalMM，一个开源评估框架，用于全面评估多模态模型在视觉-语言理解和生成任务（如视觉问答、文本到图像/视频生成、图像-文本检索）上的表现。该框架通过独立的评估服务将模型推理与评估过程解耦，从而实现灵活的资源分配和新任务、模型的无缝集成。此外，FlagEvalMM利用先进的推理加速工具（如vLLM、SGLang）和异步数据加载，显著提升了评估效率。

Result: 大量的实验表明，FlagEvalMM为模型的优势和局限性提供了准确高效的洞见，是推进多模态研究的有力工具。

Conclusion: FlagEvalMM框架公开可访问，为多模态模型的评估提供了一个高效且准确的途径。

Abstract: We present FlagEvalMM, an open-source evaluation framework designed to
comprehensively assess multimodal models across a diverse range of
vision-language understanding and generation tasks, such as visual question
answering, text-to-image/video generation, and image-text retrieval. We
decouple model inference from evaluation through an independent evaluation
service, thus enabling flexible resource allocation and seamless integration of
new tasks and models. Moreover, FlagEvalMM utilizes advanced inference
acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to
significantly enhance evaluation efficiency. Extensive experiments show that
FlagEvalMM offers accurate and efficient insights into model strengths and
limitations, making it a valuable tool for advancing multimodal research. The
framework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.

</details>


### [97] [AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models](https://arxiv.org/abs/2506.09082)
*Zheda Mai,Arpita Chowdhury,Zihe Wang,Sooyoung Jeon,Lemeng Wang,Jiacheng Hou,Jihyung Kil,Wei-Lun Chao*

Main category: cs.CV

介绍了AVA-Bench，一个新基准，用于系统地评估vision foundation models (VFMs)的14种基础视觉能力，解决了现有VQA基准测试的两个关键盲点，并表明较小的LLM（0.5B）也能提供类似的VFM排名评估，同时大大减少了计算资源的消耗。


<details>
  <summary>Details</summary>
Motivation: 目的是解决现有VFMs评估方法中存在的两个主要盲点：指令微调数据可能与VQA测试分布不一致，以及VQA基准测试往往要求多种视觉能力，难以确定错误是由所有必需能力的缺失还是由单一关键能力缺失导致。

Method: 提出了AVA-Bench, 该基准测试明确拆分了14种基础视觉能力，并确保每种能力在训练和测试数据上的分布一致性，以便准确指出VFMs在哪方面表现出色或不足，转而使VFM选择更加科学合理。

Result: 应用AVA-Bench到领先的VFMs上揭示了独特的"能力指纹"，并且发现一个较小的0.5B LLM能与一个7B LLM提供相似的VFM评价结果，但可节省8倍的GPU小时数，从而实现更高效的评估。

Conclusion: 通过提供全面透明的基准测试，AVA-Bench为进一步改进VFMs奠定了基础。

Abstract: The rise of vision foundation models (VFMs) calls for systematic evaluation.
A common approach pairs VFMs with large language models (LLMs) as
general-purpose heads, followed by evaluation on broad Visual Question
Answering (VQA) benchmarks. However, this protocol has two key blind spots: (i)
the instruction tuning data may not align with VQA test distributions, meaning
a wrong prediction can stem from such data mismatch rather than a VFM' visual
shortcomings; (ii) VQA benchmarks often require multiple visual abilities,
making it hard to tell whether errors stem from lacking all required abilities
or just a single critical one. To address these gaps, we introduce AVA-Bench,
the first benchmark that explicitly disentangles 14 Atomic Visual Abilities
(AVAs) -- foundational skills like localization, depth estimation, and spatial
understanding that collectively support complex visual reasoning tasks. By
decoupling AVAs and matching training and test distributions within each,
AVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench
to leading VFMs thus reveals distinctive "ability fingerprints," turning VFM
selection from educated guesswork into principled engineering. Notably, we find
that a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours
by 8x, enabling more efficient evaluation. By offering a comprehensive and
transparent benchmark, we hope AVA-Bench lays the foundation for the next
generation of VFMs.

</details>


### [98] [BakuFlow: A Streamlining Semi-Automatic Label Generation Tool](https://arxiv.org/abs/2506.09083)
*Jerry Lin,Partick P. W. Chen*

Main category: cs.CV

BakuFlow是一种半自动标注工具，为计算机视觉任务提供了高效的标注解决方案，特别是针对大规模任务。


<details>
  <summary>Details</summary>
Motivation: 准确的数据标注是计算机视觉中的一个瓶颈，尤其是对于大规模任务，手动标注既耗时又容易出错。

Method: 介绍了一种半自动标签生成工具BakuFlow，该工具具有实时可调放大镜、交互式数据增强模块、标签传播功能以及基于修改后的YOLOE框架的自动标签模块。

Result: 该工具通过创新功能在标注视频数据时大幅加快标注速度，并使标注过程变得更加灵活和可扩展，尤其适用于对象检测和跟踪。

Conclusion: BakuFlow在实际计算机视觉和工业场景中大大减少了标注工作量并提高了效率，特别有效地适用于对象检测和跟踪任务。

Abstract: Accurately labeling (or annotation) data is still a bottleneck in computer
vision, especially for large-scale tasks where manual labeling is
time-consuming and error-prone. While tools like LabelImg can handle the
labeling task, some of them still require annotators to manually label each
image. In this paper, we introduce BakuFlow, a streamlining semi-automatic
label generation tool. Key features include (1) a live adjustable magnifier for
pixel-precise manual corrections, improving user experience; (2) an interactive
data augmentation module to diversify training datasets; (3) label propagation
for rapidly copying labeled objects between consecutive frames, greatly
accelerating annotation of video data; and (4) an automatic labeling module
powered by a modified YOLOE framework. Unlike the original YOLOE, our extension
supports adding new object classes and any number of visual prompts per class
during annotation, enabling flexible and scalable labeling for dynamic,
real-world datasets. These innovations make BakuFlow especially effective for
object detection and tracking, substantially reducing labeling workload and
improving efficiency in practical computer vision and industrial scenarios.

</details>


### [99] [Bias Analysis in Unconditional Image Generative Models](https://arxiv.org/abs/2506.09106)
*Xiaofeng Zhang,Michelle Lin,Simon Lacoste-Julien,Aaron Courville,Yash Goyal*

Main category: cs.CV

Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The widespread adoption of generative AI models has raised growing concerns
about representational harm and potential discriminatory outcomes. Yet, despite
growing literature on this topic, the mechanisms by which bias emerges -
especially in unconditional generation - remain disentangled. We define the
bias of an attribute as the difference between the probability of its presence
in the observed distribution and its expected proportion in an ideal reference
distribution. In our analysis, we train a set of unconditional image generative
models and adopt a commonly used bias evaluation framework to study bias shift
between training and generated distributions. Our experiments reveal that the
detected attribute shifts are small. We find that the attribute shifts are
sensitive to the attribute classifier used to label generated images in the
evaluation framework, particularly when its decision boundaries fall in
high-density regions. Our empirical analysis indicates that this classifier
sensitivity is often observed in attributes values that lie on a spectrum, as
opposed to exhibiting a binary nature. This highlights the need for more
representative labeling practices, understanding the shortcomings through
greater scrutiny of evaluation frameworks, and recognizing the socially complex
nature of attributes when evaluating bias.

</details>


### [100] [CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation](https://arxiv.org/abs/2506.09109)
*Arnav Yayavaram,Siddharth Yayavaram,Simran Khanuja,Michael Saxon,Graham Neubig*

Main category: cs.CV

研究团队提出了一种新的评估指标CAIRe，用于衡量生成图像的文化相关性，与现有的基线比较展示了显著的优势，并与人类的判断保持高度一致。


<details>
  <summary>Details</summary>
Motivation: 旨在解决在文本到图像的转换模型中，跨文化偏差无法有效衡量的问题，这些问题包括性能下降、事实不准确或输出内容冒犯等问题。

Method: 介绍了一种新的评估指标CAIRe，该指标用于评估图像的文化相关性，给定用户定义的标签集，将实体和概念与知识库中的信息进行链接，并使用事实信息对每个文化标签进行独立的分级判断。

Result: 在手动整理的数据集上，CAIRe的F1值超过所有基线28%；在使用语言模型构建的文化相关但罕见项目的数据集上，CAIRe与人工评级在文化相关性方面达到了0.56和0.66的皮尔逊相关系数，基于5级Likert量表。

Conclusion: 这些结果表明，CAIRe在评估图像文化相关性的过程中具有强大的功能，并能与人类的判断保持高度一致性，适用于不同的图像来源。

Abstract: As text-to-image models become increasingly prevalent, ensuring their
equitable performance across diverse cultural contexts is critical. Efforts to
mitigate cross-cultural biases have been hampered by trade-offs, including a
loss in performance, factual inaccuracies, or offensive outputs. Despite
widespread recognition of these challenges, an inability to reliably measure
these biases has stalled progress. To address this gap, we introduce CAIRe, a
novel evaluation metric that assesses the degree of cultural relevance of an
image, given a user-defined set of labels. Our framework grounds entities and
concepts in the image to a knowledge base and uses factual information to give
independent graded judgments for each culture label. On a manually curated
dataset of culturally salient but rare items built using language models, CAIRe
surpasses all baselines by 28% F1 points. Additionally, we construct two
datasets for culturally universal concept, one comprising of T2I-generated
outputs and another retrieved from naturally occurring data. CAIRe achieves
Pearson's correlations of 0.56 and 0.66 with human ratings on these sets, based
on a 5-point Likert scale of cultural relevance. This demonstrates its strong
alignment with human judgment across diverse image sources.

</details>


### [101] [Seedance 1.0: Exploring the Boundaries of Video Generation Models](https://arxiv.org/abs/2506.09113)
*Yu Gao,Haoyuan Guo,Tuyen Hoang,Weilin Huang,Lu Jiang,Fangyuan Kong,Huixia Li,Jiashi Li,Liang Li,Xiaojie Li,Xunsong Li,Yifu Li,Shanchuan Lin,Zhijie Lin,Jiawei Liu,Shu Liu,Xiaonan Nie,Zhiwu Qing,Yuxi Ren,Li Sun,Zhi Tian,Rui Wang,Sen Wang,Guoqiang Wei,Guohong Wu,Jie Wu,Ruiqi Xia,Fei Xiao,Xuefeng Xiao,Jiangqiao Yan,Ceyuan Yang,Jianchao Yang,Runkai Yang,Tao Yang,Yihang Yang,Zilyu Ye,Xuejiao Zeng,Yan Zeng,Heng Zhang,Yang Zhao,Xiaozheng Zheng,Peihao Zhu,Jiaxin Zou,Feilong Zuo*

Main category: cs.CV

论文介绍了Seedance 1.0，一个高性能和推理高效的基础视频生成模型，改进了数据处理、模型架构设计、后训练优化和模型加速技术，在视频生成质量和速度上超越了现有的最先进模型。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型在同步平衡指令遵循、动作合理性以及视觉质量方面仍面临重大挑战。Seedance 1.0正是为了提升视频生成的各项性能而开发。

Method: Seedance 1.0采用了多个核心技术改进：(i) 多源数据整理，增强精确和有意义的视频标注，以实现跨越多种场景的全面学习；(ii) 一个高效架构设计的训练模式，支持多镜头生成，并同时学习文本-视频和图像-视频任务；(iii) 细致优化的后训练方法，利用精细监督微调和多维奖励机制的视频特异性RLHF进行综合性能改进；(iv) 优秀模型加速，通过多阶段知识提炼策略和系统级优化实现大约10倍的推理加速。

Result: Seedance 1.0能够以1080p分辨率仅用41.4秒生成5秒的视频，在NVIDIA-L20上实现了速度和质量的显著提升。

Conclusion: 相比于现有的视频生成模型，Seedance 1.0在高质量和快速视频生成方面表现更优，具备出色的时空流畅性、结构稳定性、在复杂多主体场景中的精确指令遵循性以及多镜头叙事连贯性。

Abstract: Notable breakthroughs in diffusion modeling have propelled rapid improvements
in video generation, yet current foundational model still face critical
challenges in simultaneously balancing prompt following, motion plausibility,
and visual quality. In this report, we introduce Seedance 1.0, a
high-performance and inference-efficient video foundation generation model that
integrates several core technical improvements: (i) multi-source data curation
augmented with precision and meaningful video captioning, enabling
comprehensive learning across diverse scenarios; (ii) an efficient architecture
design with proposed training paradigm, which allows for natively supporting
multi-shot generation and jointly learning of both text-to-video and
image-to-video tasks. (iii) carefully-optimized post-training approaches
leveraging fine-grained supervised fine-tuning, and video-specific RLHF with
multi-dimensional reward mechanisms for comprehensive performance improvements;
(iv) excellent model acceleration achieving ~10x inference speedup through
multi-stage distillation strategies and system-level optimizations. Seedance
1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds
(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance
1.0 stands out with high-quality and fast video generation having superior
spatiotemporal fluidity with structural stability, precise instruction
adherence in complex multi-subject contexts, native multi-shot narrative
coherence with consistent subject representation.

</details>


### [102] [Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models](https://arxiv.org/abs/2506.09229)
*Sungwon Hwang,Hyojin Jang,Kinam Kim,Minho Park,Jaegul choo*

Main category: cs.CV

This paper introduces Cross-frame Representation Alignment (CREPA), a new technique for improving semantic consistency in fine-tuned Video Diffusion Models (VDMs) across frames.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the fine-tuning of VDMs at the user level to generate videos that better reflect specific attributes of training data, which is an underexplored area with high practical relevance.

Method: Content provided discusses the adaptation of Representation Alignment (REPA) for fine-tuning Video Diffusion Models (VDMs). It identifies limitations in maintaining semantic consistency and introduces Cross-frame Representation Alignment (CREPA) as a novel regularization technique addressing this issue.

Result: Empirical evaluations show that CREPA improves visual fidelity and cross-frame semantic coherence when fine-tuned with parameter-efficient methods such as LoRA.

Conclusion: The conclusion validates the effectiveness of CREPA, showing that it not only enhances the visual quality of video generation but also improves semantic coherence across frames when fine-tuned with LoRA, and its broad applicability is confirmed across diverse datasets.

Abstract: Fine-tuning Video Diffusion Models (VDMs) at the user level to generate
videos that reflect specific attributes of training data presents notable
challenges, yet remains underexplored despite its practical importance.
Meanwhile, recent work such as Representation Alignment (REPA) has shown
promise in improving the convergence and quality of DiT-based image diffusion
models by aligning, or assimilating, its internal hidden states with external
pretrained visual features, suggesting its potential for VDM fine-tuning. In
this work, we first propose a straightforward adaptation of REPA for VDMs and
empirically show that, while effective for convergence, it is suboptimal in
preserving semantic consistency across frames. To address this limitation, we
introduce Cross-frame Representation Alignment (CREPA), a novel regularization
technique that aligns hidden states of a frame with external features from
neighboring frames. Empirical evaluations on large-scale VDMs, including
CogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual
fidelity and cross-frame semantic coherence when fine-tuned with
parameter-efficient methods such as LoRA. We further validate CREPA across
diverse datasets with varying attributes, confirming its broad applicability.
Project page: https://crepavideo.github.io

</details>


### [103] [PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies](https://arxiv.org/abs/2506.09237)
*Mojtaba Nafez,Amirhossein Koochakian,Arad Maleki,Jafar Habibi,Mohammad Hossein Rohban*

Main category: cs.CV

本文提出了一种新的对抗性鲁棒异常检测和定位方法——PatchGuard。该方法基于Vision Transformer，通过引入前景感知伪异常和新的损失函数增强模型的鲁棒性。实验表明，PatchGuard在工业和医学数据集上的对抗性性能大大提高。


<details>
  <summary>Details</summary>
Motivation: 当前的AD和AL方法易受对抗性攻击，因为训练数据通常是只包含正常样本且未标注的数据。本研究旨在提高这些系统的对抗性鲁棒性。

Method: 本研究提出了PatchGuard方法，该方法采用基于Vision Transformer (ViT)的架构，通过引入伪异常和定位掩模，以应对AD和AL系统的易受攻击性。PatchGuard利用前景感知伪异常来克服前人方法的不足，并借助新的损失函数指导对抗性训练，提高模型的鲁棒性。

Result: 实验结果表明，PatchGuard在工业和医学数据集上的实验中，在对抗性环境下，AD性能提高了53.2%，AL性能提高了68.5%。它同时也在常规环境下保持了较高的准确性。

Conclusion: PatchGuard方法通过引入前景感知伪异常和新的损失函数，显着提高了AD和AL模型的对抗性鲁棒性，同时保持了在其它方面的竞争力。

Abstract: Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields
that demand high reliability, such as medical imaging and industrial
monitoring. However, current AD and AL approaches are often susceptible to
adversarial attacks due to limitations in training data, which typically
include only normal, unlabeled samples. This study introduces PatchGuard, an
adversarially robust AD and AL method that incorporates pseudo anomalies with
localization masks within a Vision Transformer (ViT)-based architecture to
address these vulnerabilities. We begin by examining the essential properties
of pseudo anomalies, and follow it by providing theoretical insights into the
attention mechanisms required to enhance the adversarial robustness of AD and
AL systems. We then present our approach, which leverages Foreground-Aware
Pseudo-Anomalies to overcome the deficiencies of previous anomaly-aware
methods. Our method incorporates these crafted pseudo-anomaly samples into a
ViT-based framework, with adversarial training guided by a novel loss function
designed to improve model robustness, as supported by our theoretical analysis.
Experimental results on well-established industrial and medical datasets
demonstrate that PatchGuard significantly outperforms previous methods in
adversarial settings, achieving performance gains of $53.2\%$ in AD and
$68.5\%$ in AL, while also maintaining competitive accuracy in non-adversarial
settings. The code repository is available at
https://github.com/rohban-lab/PatchGuard .

</details>


### [104] [UFM: A Simple Path towards Unified Dense Correspondence with Flow](https://arxiv.org/abs/2506.09278)
*Yuchen Zhang,Nikhil Keetha,Chenwei Lyu,Bhuvan Jhamb,Yutian Chen,Yuheng Qiu,Jay Karhade,Shreyas Jha,Yaoyu Hu,Deva Ramanan,Sebastian Scherer,Wenshan Wang*

Main category: cs.CV

The paper introduces the Unified Flow & Matching (UFM) model, which uses a transformer architecture to provide a unified solution for dense correspondence tasks, outperforming specialized methods in both accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: To address the dense correspondence problem, which is crucial for various applications like visual odometry and 3D reconstruction, by providing a unified approach that works for both wide-baseline scenarios and optical flow estimation.

Method: Unified Flow & Matching model (UFM) using a generic transformer architecture to directly regress (u,v) flow.

Result: UFM outperforms state-of-the-art flow methods by 28% in accuracy, has 62% less error compared to dense wide-baseline matchers, and is 6.7x faster.

Conclusion: The paper demonstrates that unified training of dense correspondence can surpass specialized methods, offering potential for fast, general-purpose correspondence tasks and opening new research avenues for multi-modal applications.

Abstract: Dense image correspondence is central to many applications, such as visual
odometry, 3D reconstruction, object association, and re-identification.
Historically, dense correspondence has been tackled separately for
wide-baseline scenarios and optical flow estimation, despite the common goal of
matching content between two images. In this paper, we develop a Unified Flow &
Matching model (UFM), which is trained on unified data for pixels that are
co-visible in both source and target images. UFM uses a simple, generic
transformer architecture that directly regresses the (u,v) flow. It is easier
to train and more accurate for large flows compared to the typical
coarse-to-fine cost volumes in prior work. UFM is 28% more accurate than
state-of-the-art flow methods (Unimatch), while also having 62% less error and
6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to
demonstrate that unified training can outperform specialized approaches across
both domains. This result enables fast, general-purpose correspondence and
opens new directions for multi-modal, long-range, and real-time correspondence
tasks.

</details>


### [105] [Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery](https://arxiv.org/abs/2506.09299)
*Sindhu Boddu,Arindam Mukherjee*

Main category: cs.CV

This paper develops a small, power-efficient object detection model for aerial imagery during emergencies using a custom dataset and YOLOv4-Tiny quantized to INT8.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to develop a lightweight and energy-efficient approach to object detection for aerial imagery in emergency response scenarios. The authors address the need for a custom dataset as publicly available datasets lack the necessary drone-view emergency imagery.

Method: The paper utilizes the YOLOv4-Tiny model, which has been optimized via post-training quantization to INT8 precision. The model is trained using a custom dataset made up of 10,820 annotated images, tailored for emergency response aerial imagery.

Result: The quantized YOLOv4-Tiny model demonstrates comparable detection performance to YOLOv5-small, with significant improvements in model size (reduced from 22.5MB to 6.4MB) and inference speed (improved by 44%).

Conclusion: The study concludes that the quantized YOLOv4-Tiny model is well-suited for real-time emergency detection on low-power edge devices, due to its optimized model size and fast inference time.

Abstract: This paper presents a lightweight and energy-efficient object detection
solution for aerial imagery captured during emergency response situations. We
focus on deploying the YOLOv4-Tiny model, a compact convolutional neural
network, optimized through post-training quantization to INT8 precision. The
model is trained on a custom-curated aerial emergency dataset, consisting of
10,820 annotated images covering critical emergency scenarios. Unlike prior
works that rely on publicly available datasets, we created this dataset
ourselves due to the lack of publicly available drone-view emergency imagery,
making the dataset itself a key contribution of this work. The quantized model
is evaluated against YOLOv5-small across multiple metrics, including mean
Average Precision (mAP), F1 score, inference time, and model size. Experimental
results demonstrate that the quantized YOLOv4-Tiny achieves comparable
detection performance while reducing the model size from 22.5 MB to 6.4 MB and
improving inference speed by 44\%. With a 71\% reduction in model size and a
44\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly
suitable for real-time emergency detection on low-power edge devices.

</details>


### [106] [Efficient Edge Deployment of Quantized YOLOv4-Tiny for Aerial Emergency Object Detection on Raspberry Pi 5](https://arxiv.org/abs/2506.09300)
*Sindhu Boddu,Arindam Mukherjee*

Main category: cs.CV

The paper shows that a quantized YOLOv4-Tiny model can effectively perform real-time object detection with reduced power consumption on a Raspberry Pi 5, suitable for emergency response applications.


<details>
  <summary>Details</summary>
Motivation: To enhance real-time object detection capabilities for safety-critical emergency response applications, by reducing power consumption and improving thermal feasibility on a resource-constrained edge device.

Method: This paper quantizes the YOLOv4-Tiny model to INT8 precision using TensorFlow Lite for deployment on a Raspberry Pi 5, targeting real-time object detection in aerial emergency imagery.

Result: The quantized model achieved an inference time of 28.2 ms per image, with an average power consumption of 13.85 W, showing significant power savings compared to FP32. Detection accuracy is robust for emergency classes.

Conclusion: The study demonstrates the suitability of low-power embedded AI systems for real-time emergency response tasks on resource-constrained devices like the Raspberry Pi 5.

Abstract: This paper presents the deployment and performance evaluation of a quantized
YOLOv4-Tiny model for real-time object detection in aerial emergency imagery on
a resource-constrained edge device the Raspberry Pi 5. The YOLOv4-Tiny model
was quantized to INT8 precision using TensorFlow Lite post-training
quantization techniques and evaluated for detection speed, power consumption,
and thermal feasibility under embedded deployment conditions. The quantized
model achieved an inference time of 28.2 ms per image with an average power
consumption of 13.85 W, demonstrating a significant reduction in power usage
compared to its FP32 counterpart. Detection accuracy remained robust across key
emergency classes such as Ambulance, Police, Fire Engine, and Car Crash. These
results highlight the potential of low-power embedded AI systems for real-time
deployment in safety-critical emergency response applications.

</details>


### [107] [MSSDF: Modality-Shared Self-supervised Distillation for High-Resolution Multi-modal Remote Sensing Image Learning](https://arxiv.org/abs/2506.09327)
*Tong Wang,Guanzhou Chen,Xiaodong Zhang,Chenxi Liu,Jiaqi Wang,Xiaoliang Tan,Wenchao Guo,Qingyuan Yang,Kaiqi Zhang*

Main category: cs.CV

提出了一个多模态自监督学习框架，通过利用高分辨率RGB图像、多光谱数据和DSM进行预训练，提高了遥感图像解读性能，特别在多个任务中超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 遥感图像解释在环境监测、城市规划和灾害评估中起着关键作用。然而，获得高质量标记数据往往是代价高昂且耗时的。为了应对这一挑战，该研究提出了上述框架。

Method: 提出了一种多模态自监督学习框架，利用高分辨率RGB图像、多光谱数据和数字表面模型（DSM）进行预训练。通过设计信息感知自适应掩蔽策略、跨模态掩蔽机制和多任务自监督目标，该框架有效捕捉了不同模态之间的关联以及每个模态内的独特特征结构。

Result: 在15个遥感数据集上进行了实验，涵盖了26个任务，结果表明所提出的方法在大多数任务中优于现有预训练方法。具体来说，在Potsdam和Vaihingen语义分割任务中，我们的方法实现了78.30%和76.50%的mIoU，只使用50%的训练集。对于US3D深度估计任务，RMSE误差减少到0.182，对于SECOND数据集中的二元变化检测任务，我们的方法实现了47.51%的mIoU，超过了第二名CS-MAE 3个百分点。

Conclusion: 所提出的多模态自监督学习框架能够通过充分利用高分辨率RGB图像、多光谱数据和DSM数据的优势，提高遥感图像解释的性能。这一方法在多个任务中都表现出了优越性，证明了自监督学习在需要高成本数据标注的遥感图像任务中的潜力。

Abstract: Remote sensing image interpretation plays a critical role in environmental
monitoring, urban planning, and disaster assessment. However, acquiring
high-quality labeled data is often costly and time-consuming. To address this
challenge, we proposes a multi-modal self-supervised learning framework that
leverages high-resolution RGB images, multi-spectral data, and digital surface
models (DSM) for pre-training. By designing an information-aware adaptive
masking strategy, cross-modal masking mechanism, and multi-task self-supervised
objectives, the framework effectively captures both the correlations across
different modalities and the unique feature structures within each modality. We
evaluated the proposed method on multiple downstream tasks, covering typical
remote sensing applications such as scene classification, semantic
segmentation, change detection, object detection, and depth estimation.
Experiments are conducted on 15 remote sensing datasets, encompassing 26 tasks.
The results demonstrate that the proposed method outperforms existing
pretraining approaches in most tasks. Specifically, on the Potsdam and
Vaihingen semantic segmentation tasks, our method achieved mIoU scores of
78.30\% and 76.50\%, with only 50\% train-set. For the US3D depth estimation
task, the RMSE error is reduced to 0.182, and for the binary change detection
task in SECOND dataset, our method achieved mIoU scores of 47.51\%, surpassing
the second CS-MAE by 3 percentage points. Our pretrain code, checkpoints, and
HR-Pairs dataset can be found in https://github.com/CVEO/MSSDF.

</details>


### [108] [CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation](https://arxiv.org/abs/2506.09343)
*Yuxing Long,Jiyao Zhang,Mingjie Pan,Tianshu Wu,Taewhan Kim,Hao Dong*

Main category: cs.CV

本文提出了一种基于手册的家电操作基准CheckManual和一个相应模型ManualPlan，以提高机器人的家电操作能力。以前的研究要么局限于问答任务，要么忽视了手册的重要性，而本文则填补了这一空白。


<details>
  <summary>Details</summary>
Motivation: 之前的与手册相关的工作局限于问答任务，而现有的操作研究则忽视了手册的重要角色，并且无法理解多页手册的内容。本文旨在填补这一空白，通过创建基于手册的操作基准和模型，提高机器人理解并使用家电的能力。

Method: 我们提出了第一个基于手册的家用电器操作基准CheckManual。具体来说，我们设计了一个大型模型辅助的人工修订数据生成管道，根据CAD家电模型创建手册。通过这些手册，我们建立了一系列基于手册的操作挑战、评估指标和模拟环境。此外，我们还提出了第一个基于手册的操作规划模型ManualPlan，为CheckManual基准建立了一组基线模型。

Result: 虽然文中具体的实验结果未完全阐述，但通过建立CheckManual基准和ManualPlan模型，为未来的机器人理解和操作家用电器的研究奠定了基础。

Conclusion: 在本文中，我们引入了首个基于手册的家电操作基准CheckManual以及首个手册为基础的操作规划模型ManualPlan，这为我们更好地理解和操作家用机器人提供了一个新的方向。

Abstract: Correct use of electrical appliances has significantly improved human life
quality. Unlike simple tools that can be manipulated with common sense,
different parts of electrical appliances have specific functions defined by
manufacturers. If we want the robot to heat bread by microwave, we should
enable them to review the microwave manual first. From the manual, it can learn
about component functions, interaction methods, and representative task steps
about appliances. However, previous manual-related works remain limited to
question-answering tasks while existing manipulation researchers ignore the
manual's important role and fail to comprehend multi-page manuals. In this
paper, we propose the first manual-based appliance manipulation benchmark
CheckManual. Specifically, we design a large model-assisted human-revised data
generation pipeline to create manuals based on CAD appliance models. With these
manuals, we establish novel manual-based manipulation challenges, metrics, and
simulator environments for model performance evaluation. Furthermore, we
propose the first manual-based manipulation planning model ManualPlan to set up
a group of baselines for the CheckManual benchmark.

</details>


### [109] [An Effective End-to-End Solution for Multimodal Action Recognition](https://arxiv.org/abs/2506.09345)
*Songping Wang,Xiantao Hu,Yueming Lyu,Caifeng Shan*

Main category: cs.CV

A comprehensive solution for multimodal action recognition utilizes data enhancement, transfer learning, and multimodal feature extraction, achieving high accuracy on a competition leaderboard.


<details>
  <summary>Details</summary>
Motivation: Due to the scarcity of tri-modal data, the paper aims to propose a comprehensive multimodal action recognition solution to effectively utilize multimodal information.

Method: First, the data is enhanced to enlarge the training scale, and RGB datasets are used to pre-train the backbone network for transfer learning. Secondly, multimodal spatial-temporal features are extracted using 2D CNNs combined with the TSM. Additionally, SWA, Ensemble, and TTA methods are used to enhance predictions.

Result: The solution achieved the Top-1 accuracy of 99% and the Top-5 accuracy of 100% on the competition leaderboard.

Conclusion: The proposed solution demonstrates superior performance in multimodal action recognition by effectively utilizing data enhancement techniques, transfer learning, and multimodal feature extraction, leading to high accuracy results.

Abstract: Recently, multimodal tasks have strongly advanced the field of action
recognition with their rich multimodal information. However, due to the
scarcity of tri-modal data, research on tri-modal action recognition tasks
faces many challenges. To this end, we have proposed a comprehensive multimodal
action recognition solution that effectively utilizes multimodal information.
First, the existing data are transformed and expanded by optimizing data
enhancement techniques to enlarge the training scale. At the same time, more
RGB datasets are used to pre-train the backbone network, which is better
adapted to the new task by means of transfer learning. Secondly, multimodal
spatial features are extracted with the help of 2D CNNs and combined with the
Temporal Shift Module (TSM) to achieve multimodal spatial-temporal feature
extraction comparable to 3D CNNs and improve the computational efficiency. In
addition, common prediction enhancement methods, such as Stochastic Weight
Averaging (SWA), Ensemble and Test-Time augmentation (TTA), are used to
integrate the knowledge of models from different training periods of the same
architecture and different architectures, so as to predict the actions from
different perspectives and fully exploit the target information. Ultimately, we
achieved the Top-1 accuracy of 99% and the Top-5 accuracy of 100% on the
competition leaderboard, demonstrating the superiority of our solution.

</details>


### [110] [Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation](https://arxiv.org/abs/2506.09350)
*Shanchuan Lin,Ceyuan Yang,Hao He,Jianwen Jiang,Yuxi Ren,Xin Xia,Yang Zhao,Xuefeng Xiao,Lu Jiang*

Main category: cs.CV

本文提出了一种名为AAPT的方法，能够将大规模视频生成模型转变为实时、交互式的视频生成器，并通过自回归对抗性训练有效减少了生成过程中的误差累积，大幅提高了生成效率。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模视频生成模型计算密集，阻碍了其在实时和交互式应用中的应用。本文旨在解决这一问题，提高模型的实时性和交互性。

Method: 本文提出了一种称为自回归对抗后训练（AAPT）的方法，用于将预先训练好的潜视频扩散模型转换为实时交互式视频生成器。该模型能够逐帧自回归地生成潜帧，并使用单一神经函数评估（1NFE），实时向用户传输结果并接收交互反馈以生成下一帧。本文的方法探索了对抗性训练作为一种有效的自回归生成范式。

Result: 实验结果显示，本文的80亿参数模型能够在单个H100上以736x416分辨率实时实现24fps视频生成，或在8xH100上达到1280x720分辨率，生成长达1分钟（1440帧）的视频。

Conclusion: 本文的模型通过设计更高效的单步生成架构和利用KV缓存，以及采用学生强迫样式的对抗性训练，成功地减少了长期视频生成中的误差累积，并实现了高效的实时视频生成效果。

Abstract: Existing large-scale video generation models are computationally intensive,
preventing adoption in real-time and interactive applications. In this work, we
propose autoregressive adversarial post-training (AAPT) to transform a
pre-trained latent video diffusion model into a real-time, interactive video
generator. Our model autoregressively generates a latent frame at a time using
a single neural function evaluation (1NFE). The model can stream the result to
the user in real time and receive interactive responses as controls to generate
the next latent frame. Unlike existing approaches, our method explores
adversarial training as an effective paradigm for autoregressive generation.
This not only allows us to design an architecture that is more efficient for
one-step generation while fully utilizing the KV cache, but also enables
training the model in a student-forcing manner that proves to be effective in
reducing error accumulation during long video generation. Our experiments
demonstrate that our 8B model achieves real-time, 24fps, streaming video
generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to
a minute long (1440 frames). Visit our research website at
https://seaweed-apt.com/2

</details>


### [111] [A new approach for image segmentation based on diffeomorphic registration and gradient fields](https://arxiv.org/abs/2506.09357)
*Junchao Zhou*

Main category: cs.CV

这篇论文提出了一种结合形状分析和微分同胚变换的二维图像分割新框架，其使用LDDMM框架和varifold表示对图像进行分割，不需要大规模的数据集，实现了准确的分割。


<details>
  <summary>Details</summary>
Motivation: 传统的分割方法如边缘检测和变分方法已被广泛研究，虽然深度学习方法近期取得了令人满意的效果，但往往需要大量的训练数据。因此，提出此方法以解决数据需求问题并提供一个理论上各具基础的方法。

Method: 提出了一种新的二维图像分割变分框架，该框架结合了形状分析和微分同胚变换的概念。该方法将分割建模为通过对图像域进行微分同胚变换而使模板曲线变形的过程，使用大变形微分同胚度量映射（LDDMM）框架。利用varifold表示几何形状，通过将变形曲线与图像梯度场进行比较来引导曲线演化。

Result: 通过Python实现GPU加速后，该框架在准确分割方面有着灵活且理论上坚实的方法，无需依赖大规模数据集。

Conclusion: 该方法提供了理论基础并展示了在不需要大量训练数据的情况下进行准确分割的能力。

Abstract: Image segmentation is a fundamental task in computer vision aimed at
delineating object boundaries within images. Traditional approaches, such as
edge detection and variational methods, have been widely explored, while recent
advances in deep learning have shown promising results but often require
extensive training data. In this work, we propose a novel variational framework
for 2D image segmentation that integrates concepts from shape analysis and
diffeomorphic transformations. Our method models segmentation as the
deformation of a template curve via a diffeomorphic transformation of the image
domain, using the Large Deformation Diffeomorphic Metric Mapping (LDDMM)
framework. The curve evolution is guided by a loss function that compares the
deformed curve to the image gradient field, formulated through the varifold
representation of geometric shapes. The approach is implemented in Python with
GPU acceleration using the PyKeops library. This framework allows for accurate
segmentation with a flexible and theoretically grounded methodology that does
not rely on large datasets.

</details>


### [112] [SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing](https://arxiv.org/abs/2506.09363)
*Hongguang Zhu,Yunchao Wei,Mengyu Wang,Siyu Jiao,Yan Fang,Jiannan Huang,Yao Zhao*

Main category: cs.CV

SAGE提出了一种新方法，有效地解决了扩散模型在生成图像时的不安全内容和版权侵犯问题。


<details>
  <summary>Details</summary>
Motivation: 文章旨在解决扩散模型在预训练中不可避免地包含敏感信息所带来的一系列安全风险，比如生成不安全内容和侵犯版权的风险。

Method: SAGE方法包括语义增强擦除和全局-局部协同保留机制两种机制，前者能够将概念词擦除转换为概念领域擦除，通过循环自我检查和自我擦除来探索和取消概念领域的边界表示，后者则旨在减轻擦除不安全概念时对无关概念的保留退化。

Result: 该论文提出了一种名为SAGE的方法，相对于现有方法，它更有效地处理了扩散模型（DMs）在预训练中可能包含的敏感信息问题，避免了“词概念深渊”的陷阱，并且在安全生成DMs方面展示出了全面的优势。

Conclusion: SAGE通过引入语义增强擦除和全局-局部协同保留机制，证明了其在扩散模型安全生成方面的有效性。

Abstract: Diffusion models (DMs) have achieved significant progress in text-to-image
generation. However, the inevitable inclusion of sensitive information during
pre-training poses safety risks, such as unsafe content generation and
copyright infringement. Concept erasing finetunes weights to unlearn
undesirable concepts, and has emerged as a promising solution. However,
existing methods treat unsafe concept as a fixed word and repeatedly erase it,
trapping DMs in ``word concept abyss'', which prevents generalized
concept-related erasing. To escape this abyss, we introduce semantic-augment
erasing which transforms concept word erasure into concept domain erasure by
the cyclic self-check and self-erasure. It efficiently explores and unlearns
the boundary representation of concept domain through semantic spatial
relationships between original and training DMs, without requiring additional
preprocessed data. Meanwhile, to mitigate the retention degradation of
irrelevant concepts while erasing unsafe concepts, we further propose the
global-local collaborative retention mechanism that combines global semantic
relationship alignment with local predicted noise preservation, effectively
expanding the retentive receptive field for irrelevant concepts. We name our
method SAGE, and extensive experiments demonstrate the comprehensive
superiority of SAGE compared with other methods in the safe generation of DMs.
The code and weights will be open-sourced at
https://github.com/KevinLight831/SAGE.

</details>


### [113] [ScaleLSD: Scalable Deep Line Segment Detection Streamlined](https://arxiv.org/abs/2506.09369)
*Zeran Ke,Bin Tan,Xianwei Zheng,Yujun Shen,Tianfu Wu,Nan Xue*

Main category: cs.CV

该论文提出了ScaleLSD方法，实现了在海量无标记自然图像上的大规模自监督学习，有效提升了线段检测的性能和效率，是在所有测试方面都超越传统非深度学习模型的首个深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 为了在任何自然图像上学习鲁棒的、领域无关的线段检测模型，实现高效和高质量的图像几何特征提炼。

Method: 通过重新审视和简化深度与非深度方法的基本设计，开发了ScaleLSD，针对海量无标记图像的线段检测任务。

Result: 实验表明，ScaleLSD在零样本检测性能、单视图3D几何估计、双视角线段匹配和多视角3D线段映射等多个方面表现出色。

Conclusion: ScaleLSD是首个在所有测试方面均超越传统非深度学习线段检测方法的深度学习方法，显著增强了图像几何线特征的多样性和稳定性。

Abstract: This paper studies the problem of Line Segment Detection (LSD) for the
characterization of line geometry in images, with the aim of learning a
domain-agnostic robust LSD model that works well for any natural images. With
the focus of scalable self-supervised learning of LSD, we revisit and
streamline the fundamental designs of (deep and non-deep) LSD approaches to
have a high-performing and efficient LSD learner, dubbed as ScaleLSD, for the
curation of line geometry at scale from over 10M unlabeled real-world images.
Our ScaleLSD works very well to detect much more number of line segments from
any natural images even than the pioneered non-deep LSD approach, having a more
complete and accurate geometric characterization of images using line segments.
Experimentally, our proposed ScaleLSD is comprehensively testified under
zero-shot protocols in detection performance, single-view 3D geometry
estimation, two-view line segment matching, and multiview 3D line mapping, all
with excellent performance obtained. Based on the thorough evaluation, our
ScaleLSD is observed to be the first deep approach that outperforms the
pioneered non-deep LSD in all aspects we have tested, significantly expanding
and reinforcing the versatility of the line geometry of images. Code and Models
are available at https://github.com/ant-research/scalelsd

</details>


### [114] [UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images](https://arxiv.org/abs/2506.09378)
*Qijian Tian,Xin Tan,Jingyu Gong,Yuan Xie,Lizhuang Ma*

Main category: cs.CV

Researchers created a feed-forward model named UniForward that predicts 3D scenes and their semantic fields from uncalibrated, sparse-view images, achieving real-time synthesis and high-quality rendering without requiring camera parameters or ground truth depth, outperforming current methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to unify 3D scene and semantic field reconstruction by proposing a technique that overcomes the challenges of incorporating semantics into 3D representations, achieving real-time reconstruction, and ensuring practicality by only using images as input.

Method: The paper develops a model named UniForward, which is a feed-forward model designed to predict 3D Gaussians with anisotropic semantic features from sparse-view images without need for camera parameters or ground truth depth. It embeds semantic features into 3D Gaussians and uses a dual-branch decoupled decoder to predict them. For training, a loss-guided view sampler is introduced to stabilize the process without relying on ground truth depth or masks.

Result: Experiments on novel view synthesis and novel view segmentation show that the method achieves state-of-the-art performances for unifying 3D scene and semantic field reconstruction. The model can reconstruct 3D scenes with high-quality rendering and generate view-consistent semantic features, which can be decoded into dense segmentation masks.

Conclusion: This research proposes a novel approach to simultaneously reconstruct 3D scenes and corresponding semantic fields from sparse-view images through a feed-forward Gaussian Splatting model that requires no camera calibration or ground truth depth data. It demonstrates high-quality reconstruction and real-time synthesis capabilities.

Abstract: We propose a feed-forward Gaussian Splatting model that unifies 3D scene and
semantic field reconstruction. Combining 3D scenes with semantic fields
facilitates the perception and understanding of the surrounding environment.
However, key challenges include embedding semantics into 3D representations,
achieving generalizable real-time reconstruction, and ensuring practical
applicability by using only images as input without camera parameters or ground
truth depth. To this end, we propose UniForward, a feed-forward model to
predict 3D Gaussians with anisotropic semantic features from only uncalibrated
and unposed sparse-view images. To enable the unified representation of the 3D
scene and semantic field, we embed semantic features into 3D Gaussians and
predict them through a dual-branch decoupled decoder. During training, we
propose a loss-guided view sampler to sample views from easy to hard,
eliminating the need for ground truth depth or masks required by previous
methods and stabilizing the training process. The whole model can be trained
end-to-end using a photometric loss and a distillation loss that leverages
semantic features from a pre-trained 2D semantic model. At the inference stage,
our UniForward can reconstruct 3D scenes and the corresponding semantic fields
in real time from only sparse-view images. The reconstructed 3D scenes achieve
high-quality rendering, and the reconstructed 3D semantic field enables the
rendering of view-consistent semantic features from arbitrary views, which can
be further decoded into dense segmentation masks in an open-vocabulary manner.
Experiments on novel view synthesis and novel view segmentation demonstrate
that our method achieves state-of-the-art performances for unifying 3D scene
and semantic field reconstruction.

</details>


### [115] [ReID5o: Achieving Omni Multi-modal Person Re-identification in a Single Model](https://arxiv.org/abs/2506.09385)
*Jialong Zuo,Yongtai Deng,Mengdan Tan,Rui Jin,Dongyue Wu,Nong Sang,Liang Pan,Changxin Gao*

Main category: cs.CV

The paper introduces ReID5o, a multi-modal learning framework for OM-ReID, and constructs ORBench, a benchmark dataset for evaluating multi-modal ReID methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitation of existing ReID methods and datasets that do not handle multiple modalities effectively by introducing a challenging problem, Omni Multi-modal Person Re-identification (OM-ReID).

Method: The paper proposes ReID5o, a new multi-modal learning framework for person Re-identification (ReID) that allows for synergistic fusion and cross-modal alignment of any combination of modalities using a unified encoding and multi-expert routing mechanism.

Result: Extensive experiments on the new multi-modal dataset ORBench demonstrate the advancement and practicality of ReID5o, which outperforms other models on this dataset.

Conclusion: The paper contributes a novel multi-modal learning framework called ReID5o and a comprehensive multi-modal dataset ORBench, which offer a valuable resource for research in multi-modal person ReID.

Abstract: In real-word scenarios, person re-identification (ReID) expects to identify a
person-of-interest via the descriptive query, regardless of whether the query
is a single modality or a combination of multiple modalities. However, existing
methods and datasets remain constrained to limited modalities, failing to meet
this requirement. Therefore, we investigate a new challenging problem called
Omni Multi-modal Person Re-identification (OM-ReID), which aims to achieve
effective retrieval with varying multi-modal queries. To address dataset
scarcity, we construct ORBench, the first high-quality multi-modal dataset
comprising 1,000 unique identities across five modalities: RGB, infrared, color
pencil, sketch, and textual description. This dataset also has significant
superiority in terms of diversity, such as the painting perspectives and
textual information. It could serve as an ideal platform for follow-up
investigations in OM-ReID. Moreover, we propose ReID5o, a novel multi-modal
learning framework for person ReID. It enables synergistic fusion and
cross-modal alignment of arbitrary modality combinations in a single model,
with a unified encoding and multi-expert routing mechanism proposed. Extensive
experiments verify the advancement and practicality of our ORBench. A wide
range of possible models have been evaluated and compared on it, and our
proposed ReID5o model gives the best performance. The dataset and code will be
made publicly available at https://github.com/Zplusdragon/ReID5o_ORBench.

</details>


### [116] [Improving Out-of-Distribution Detection via Dynamic Covariance Calibration](https://arxiv.org/abs/2506.09399)
*Kaiyu Guo,Zijian Wang,Brian C. Lovell,Mahsa Baktashmotlagh*

Main category: cs.CV

Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Out-of-Distribution (OOD) detection is essential for the trustworthiness of
AI systems. Methods using prior information (i.e., subspace-based methods) have
shown effective performance by extracting information geometry to detect OOD
data with a more appropriate distance metric. However, these methods fail to
address the geometry distorted by ill-distributed samples, due to the
limitation of statically extracting information geometry from the training
distribution. In this paper, we argue that the influence of ill-distributed
samples can be corrected by dynamically adjusting the prior geometry in
response to new data. Based on this insight, we propose a novel approach that
dynamically updates the prior covariance matrix using real-time input features,
refining its information. Specifically, we reduce the covariance along the
direction of real-time input features and constrain adjustments to the residual
space, thus preserving essential data characteristics and avoiding effects on
unintended directions in the principal space. We evaluate our method on two
pre-trained models for the CIFAR dataset and five pre-trained models for
ImageNet-1k, including the self-supervised DINO model. Extensive experiments
demonstrate that our approach significantly enhances OOD detection across
various models. The code is released at https://github.com/workerbcd/ooddcc.

</details>


### [117] [SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation](https://arxiv.org/abs/2506.09403)
*Xinya Liu,Jianghao Wu,Tao Lu,Shaoting Zhang,Guotai Wang*

Main category: cs.CV

本文提出的一种SRPL-SFDA方法，通过测试时刻三分支强度增强、可靠伪标签选择和可靠性感知训练流程，在无源自域适应中取得了显著成效，较现有方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 在医疗图像分割模型部署到新临床中心时，域自适应（DA）至关重要，尤其是在存在显著域转换时。无源自域自适应（SFDA）能够处理隐私保护和对源域数据访问的制约问题，但面临在目标域中监督不足的挑战。

Method: 提出了一个名为SRPL-SFDA的方法，针对无源领域适应问题，该方法包括三个主要组成部分：1）测试时刻三分支强度增强（T3IE），以提高伪标签的质量并增强SAM的零样本推理能力；2）一个基于输入扰动下多个SAM输出一致性的可靠伪标签选择模块；3）使用可靠伪标签进行监督并以熵最小化方式对不可靠部分进行正则化的可靠性感知训练流程。

Result: 在两个多域医疗图像分割数据集上的实验分别验证了：1）SRPL-SFDA能够有效增强未标记目标域的伪标签质量，并通过可靠性感知训练提高SFDA性能；2）SRPL-SFDA优于现有的SFDA方法，并且其性能接近于目标域的监督训练。

Conclusion: 本文提出的方法SRPL-SFDA，针对无源自域适应问题，通过改进伪标签的质量和选择机制，提高了医疗图像分割模型在目标域的适应性能。

Abstract: Domain Adaptation (DA) is crucial for robust deployment of medical image
segmentation models when applied to new clinical centers with significant
domain shifts. Source-Free Domain Adaptation (SFDA) is appealing as it can deal
with privacy concerns and access constraints on source-domain data during
adaptation to target-domain data. However, SFDA faces challenges such as
insufficient supervision in the target domain with unlabeled images. In this
work, we propose a Segment Anything Model (SAM)-guided Reliable Pseudo-Labels
method for SFDA (SRPL-SFDA) with three key components: 1) Test-Time Tri-branch
Intensity Enhancement (T3IE) that not only improves quality of raw
pseudo-labels in the target domain, but also leads to SAM-compatible inputs
with three channels to better leverage SAM's zero-shot inference ability for
refining the pseudo-labels; 2) A reliable pseudo-label selection module that
rejects low-quality pseudo-labels based on Consistency of Multiple SAM Outputs
(CMSO) under input perturbations with T3IE; and 3) A reliability-aware training
procedure in the unlabeled target domain where reliable pseudo-labels are used
for supervision and unreliable parts are regularized by entropy minimization.
Experiments conducted on two multi-domain medical image segmentation datasets
for fetal brain and the prostate respectively demonstrate that: 1) SRPL-SFDA
effectively enhances pseudo-label quality in the unlabeled target domain, and
improves SFDA performance by leveraging the reliability-aware training; 2)
SRPL-SFDA outperformed state-of-the-art SFDA methods, and its performance is
close to that of supervised training in the target domain. The code of this
work is available online: https://github.com/HiLab-git/SRPL-SFDA.

</details>


### [118] [Synthetic Human Action Video Data Generation with Pose Transfer](https://arxiv.org/abs/2506.09411)
*Vaclav Knapp,Matyas Bohacek*

Main category: cs.CV

A method to generate synthetic human action video data using pose transfer aims to improve action recognition tasks by diversifying datasets and compensating for underrepresented groups, open-sourced with a new dataset called RANDOM People.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is the uncanny features in synthetic data that diminish its effectiveness for training in video understanding tasks such as sign language translation, gesture recognition, and human motion understanding in autonomous driving.

Method: The paper proposes a method for generating synthetic human action video data using pose transfer, specifically controllable 3D Gaussian avatar models.

Result: The proposed method improved performance in action recognition tasks when evaluated on the Toyota Smarthome and NTU RGB+D datasets, effectively adding diversity to few-shot datasets and compensating for underrepresented groups in real training data.

Conclusion: The method not only enhances synthetic data generation for human action recognition but also makes up for diversity gaps in the datasets. The researchers open-sourced the method and a related dataset named RANDOM People.

Abstract: In video understanding tasks, particularly those involving human motion,
synthetic data generation often suffers from uncanny features, diminishing its
effectiveness for training. Tasks such as sign language translation, gesture
recognition, and human motion understanding in autonomous driving have thus
been unable to exploit the full potential of synthetic data. This paper
proposes a method for generating synthetic human action video data using pose
transfer (specifically, controllable 3D Gaussian avatar models). We evaluate
this method on the Toyota Smarthome and NTU RGB+D datasets and show that it
improves performance in action recognition tasks. Moreover, we demonstrate that
the method can effectively scale few-shot datasets, making up for groups
underrepresented in the real training data and adding diverse backgrounds. We
open-source the method along with RANDOM People, a dataset with videos and
avatars of novel human identities for pose transfer crowd-sourced from the
internet.

</details>


### [119] [Noise Conditional Variational Score Distillation](https://arxiv.org/abs/2506.09416)
*Xinyu Peng,Ziyang Zheng,Yaoming Wang,Han Li,Nuowen Kan,Wenrui Dai,Chenglin Li,Junni Zou,Hongkai Xiong*

Main category: cs.CV

Noise Conditional Variational Score Distillation (NCVSD) 提出了一种将预训练扩散模型转化为生成去噪器的新方法，实现了在不同噪声水平下生成样本的快速性和质量改善。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于通过蒸馏预训练扩散模型，来构建能够快速生成样本同时保持迭代优化优势的生成去噪器。

Method: Noise Conditional Variational Score Distillation (NCVSD) 方法通过揭示无条件评分函数隐式地表征去噪后验分布的评分函数，将这一见解整合到变分评分蒸馏（VSD）框架中，从而实现在广泛噪音水平范围内，可扩展地学习能够近似采样去噪后验分布的生成去噪器。

Result: 实验结果显示，通过调节测试时的计算量，该方法超越了教师扩散模型，并且在反问题解决方面达到了创纪录的LPIPS值（用显著少于扩散方法的NFEs），同时在类别条件图像生成方面表现优异。

Conclusion: NCVSD 方法通过提升生成去噪器的快速生成能力和高质量样本生成，展示了在不同应用场景下的优越性能，尤其是在反问题解决上的表现，同时使用了比扩散方法少得多的NFEs。

Abstract: We propose Noise Conditional Variational Score Distillation (NCVSD), a novel
method for distilling pretrained diffusion models into generative denoisers. We
achieve this by revealing that the unconditional score function implicitly
characterizes the score function of denoising posterior distributions. By
integrating this insight into the Variational Score Distillation (VSD)
framework, we enable scalable learning of generative denoisers capable of
approximating samples from the denoising posterior distribution across a wide
range of noise levels. The proposed generative denoisers exhibit desirable
properties that allow fast generation while preserve the benefit of iterative
refinement: (1) fast one-step generation through sampling from pure Gaussian
noise at high noise levels; (2) improved sample quality by scaling the
test-time compute with multi-step sampling; and (3) zero-shot probabilistic
inference for flexible and controllable sampling. We evaluate NCVSD through
extensive experiments, including class-conditional image generation and inverse
problem solving. By scaling the test-time compute, our method outperforms
teacher diffusion models and is on par with consistency models of larger sizes.
Additionally, with significantly fewer NFEs than diffusion-based methods, we
achieve record-breaking LPIPS on inverse problems.

</details>


### [120] [ODG: Occupancy Prediction Using Dual Gaussians](https://arxiv.org/abs/2506.09417)
*Yunxiao Shi,Yinhao Zhu,Shizhong Han,Jisoo Jeong,Amin Ansari,Hong Cai,Fatih Porikli*

Main category: cs.CV

本文提出了一种结合BEV和稀疏点云表示的双分支3D占用预测方法ODG，它通过共享三维信息和融合两分支来改善对小物体和其他复杂物体的处理，并通过实验验证了方法的有效性和高效性。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是解决现有3D占用方法计算成本高以及BEV和稀疏点云各自存在的局限性。BEV表示难以处理小物体，而稀疏点云表达在捕捉平坦或大物体时效率低。因此，需要一个能够有效结合两种表示方法优势的方案来提高自动驾驶中的场景理解。

Method: 本文提出了一种名为ODG的新颖3D占用预测方法，它结合了BEV表示和稀疏点云表示。通过设计一个双分支结构：基于查询的稀疏点云分支和BEV分支。稀疏点云分支学习到的三维信息通过交叉注意力机制与BEV分支共享，以此来增强难以检测对象在BEV平面上的信号。最后，两个分支的输出融合，生成预测的3D占用。

Result: ODG方法在Occ3D-nuScenes和Occ3D-Waymo数据集上的实验显示，其表现优于其他方法，并且具有高效的推理速度。

Conclusion: 实验结果表明，提出的ODG方法在Occ3D-nuScenes和Occ3D-Waymo基准测试中表现出优越性，并且与最新的高效方法相比，ODG还提供了具有竞争力的推理速度。

Abstract: 3D occupancy provides fine-grained 3D geometry and semantics for scene
understanding which is critical for autonomous driving. Most existing methods,
however, carry high compute costs, requiring dense 3D feature volume and
cross-attention to effectively aggregate information. More recent works have
adopted Bird's Eye View (BEV) or sparse points as scene representation with
much reduced cost, but still suffer from their respective shortcomings. More
concretely, BEV struggles with small objects that often experience significant
information loss after being projected to the ground plane. On the other hand,
points can flexibly model little objects in 3D, but is inefficient at capturing
flat surfaces or large objects. To address these challenges, in this paper, we
present a novel 3D occupancy prediction approach, ODG, which combines BEV and
sparse points based representations. We propose a dual-branch design: a
query-based sparse points branch and a BEV branch. The 3D information learned
in the sparse points branch is shared with the BEV stream via cross-attention,
which enriches the weakened signals of difficult objects on the BEV plane. The
outputs of both branches are finally fused to generate predicted 3D occupancy.
We conduct extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo
benchmarks that demonstrate the superiority of our proposed ODG. Moreover, ODG
also delivers competitive inference speed when compared to the latest efficient
approaches.

</details>


### [121] [A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation](https://arxiv.org/abs/2506.09427)
*Yukang Feng,Jianwen Sun,Chuanhao Li,Zizhen Li,Jiaxin Ai,Fanrui Zhang,Yifan Chang,Sizhuo Zhou,Shenglin Zhang,Yu Dai,Kaipeng Zhang*

Main category: cs.CV

Introduce InterSyn and SynJudge for improving the generation of interleaved image-text outputs in LMMs, with experimental studies showing improved quality and system performance.


<details>
  <summary>Details</summary>
Motivation: To enhance LMMs' capability in generating tightly interleaved image-text outputs by creating a new, richly-instructional dataset and an evaluation tool.

Method: Large Multimodal Models (LMMs) are improved with the introduction of InterSyn, a new dataset created using the Self-Evaluation with Iterative Refinement (SEIR) method. SynJudge, an evaluation model, is also introduced to assess the quality of interleaved multimodal outputs.

Result: SEIR leads to higher quality datasets compared to unrefined processes, and LMMs trained on InterSyn show performance gains across all evaluation metrics.

Conclusion: The SEIR method and InterSyn dataset significantly improve multimodal dataset quality and performance in LMMs, confirming their utility for advancing multimodal systems.

Abstract: Recent advancements in Large Multimodal Models (LMMs) have significantly
improved multimodal understanding and generation. However, these models still
struggle to generate tightly interleaved image-text outputs, primarily due to
the limited scale, quality and instructional richness of current training
datasets. To address this, we introduce InterSyn, a large-scale multimodal
dataset constructed using our Self-Evaluation with Iterative Refinement (SEIR)
method. InterSyn features multi-turn, instruction-driven dialogues with tightly
interleaved imagetext responses, providing rich object diversity and rigorous
automated quality refinement, making it well-suited for training
next-generation instruction-following LMMs. Furthermore, to address the lack of
reliable evaluation tools capable of assessing interleaved multimodal outputs,
we introduce SynJudge, an automatic evaluation model designed to quantitatively
assess multimodal outputs along four dimensions: text content, image content,
image quality, and image-text synergy.
  Experimental studies show that the SEIR method leads to substantially higher
dataset quality compared to an otherwise identical process without refinement.
  Moreover, LMMs trained on InterSyn achieve uniform performance gains across
all evaluation metrics, confirming InterSyn's utility for advancing multimodal
systems.

</details>


### [122] [A Novel Lightweight Transformer with Edge-Aware Fusion for Remote Sensing Image Captioning](https://arxiv.org/abs/2506.09429)
*Swadhin Das,Divyansh Mundra,Priyanshu Dayal,Raksha Sharma*

Main category: cs.CV

本文提出一种轻量级的Transformer架构和边缘感知增强策略，以解决遥感图像描述中的计算复杂度高和细粒度结构特征忽略问题。实验表明，该方法在质量上优于现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的模型虽然在遥感图像描述方面表现出色，但由于高计算成本和多模态框架中单独采用Transformer编码器和解码器，其实际应用受到阻碍。同时，现有模型往往侧重于高层次的语义提取而忽略了细粒度的结构特征。

Method: 提出了一种轻量级的Transformer架构，通过减少编码器层的维度并使用GPT-2的精炼版本作为解码器来应对计算成本高的问题。此外，还引入了一种边缘感知增强策略以增强图像表示和对象边界理解。

Result: 实验结果表明，所提出的方法显著提高了描述的质量，优于现有的最先进方法。

Conclusion: 研究证明了一种轻量级Transformer架构与边缘感知增强策略在提升遥感图像描述质量方面的有效性与可行性。

Abstract: Transformer-based models have achieved strong performance in remote sensing
image captioning by capturing long-range dependencies and contextual
information. However, their practical deployment is hindered by high
computational costs, especially in multi-modal frameworks that employ separate
transformer-based encoders and decoders. In addition, existing remote sensing
image captioning models primarily focus on high-level semantic extraction while
often overlooking fine-grained structural features such as edges, contours, and
object boundaries. To address these challenges, a lightweight transformer
architecture is proposed by reducing the dimensionality of the encoder layers
and employing a distilled version of GPT-2 as the decoder. A knowledge
distillation strategy is used to transfer knowledge from a more complex teacher
model to improve the performance of the lightweight network. Furthermore, an
edge-aware enhancement strategy is incorporated to enhance image representation
and object boundary understanding, enabling the model to capture fine-grained
spatial details in remote sensing images. Experimental results demonstrate that
the proposed approach significantly improves caption quality compared to
state-of-the-art methods.

</details>


### [123] [TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision](https://arxiv.org/abs/2506.09445)
*Ayush Gupta,Anirban Roy,Rama Chellappa,Nathaniel D. Bastian,Alvaro Velasquez,Susmit Jha*

Main category: cs.CV

TOGA模型在弱监督环境下解决了视频中的时序定位问题，通过引导调优，模型可以同时生成问题答案和时间定位信息，并在多个基准数据集上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 本研究致力于解决在弱监督环境下对于视频问答的时序定位问题，即在没有时间戳标注的情况下从视频中定位相关的回答。

Method: 我们提出了一种名为TOGA的视觉语言模型，它能够在没有时间戳注释的弱监督设置下，从视频中生成开放性问题的答案并附带时间定位。通过引导调优（instruct-tune）让TOGA可以同时生成答案和时间定位。此外，我们生成伪标签以进行时间定位，并通过施加一致性约束来确保这些标签的有效性，该约束要求一个时间片段中的问答之间保持一致性。

Result: 我们在NExT-GQA（针对弱监督环境下的问题回答设计的基准测试）上评估了TOGA在时间定位问题回答任务上的表现，在MSVD-QA和ActivityNet-QA上评估了其在开放性问题回答任务上的表现。实验结果表明，TOGA在这两个任务上都达到了最先进的性能。

Conclusion: 研究结果表明，同时生成答案和时间定位能够提升视频问答的性能，并且我们的方法TOGA在多个基准数据集上取得了最先进的性能。

Abstract: We address the problem of video question answering (video QA) with temporal
grounding in a weakly supervised setup, without any temporal annotations. Given
a video and a question, we generate an open-ended answer grounded with the
start and end time. For this task, we propose TOGA: a vision-language model for
Temporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune
TOGA to jointly generate the answer and the temporal grounding. We operate in a
weakly supervised setup where the temporal grounding annotations are not
available. We generate pseudo labels for temporal grounding and ensure the
validity of these labels by imposing a consistency constraint between the
question of a grounding response and the response generated by a question
referring to the same temporal segment. We notice that jointly generating the
answers with the grounding improves performance on question answering as well
as grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For
grounded QA, we consider the NExT-GQA benchmark which is designed to evaluate
weakly supervised grounded question answering. For open-ended QA, we consider
the MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art
performance for both tasks on these benchmarks.

</details>


### [124] [Harmonizing and Merging Source Models for CLIP-based Domain Generalization](https://arxiv.org/abs/2506.09446)
*Yuhe Ding,Jian Liang,Bo Jiang,Zi Wang,Aihua Zheng,Bin Luo*

Main category: cs.CV

提出一种新型多源域泛化方法——Harmonizing and Merging (HAM)，可有效提升模型在未见领域的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在训练多源领域时存在的样本冲突和优化冲突，以提高模型在未见领域的泛化能力。

Method: 提出Harmonizing and Merging (HAM)，一种源模型合并框架，用于CLIP领域的多源泛化。该框架首先在源模型训练期间避免冲突样本，调节模型更新方向，接着使用冗余感知的历史模型合并方法整合所有源模型的知识。

Result: CLIP-based领域泛化旨在利用CLIP的强大零样本分类能力和多个源数据集，提高模型在未见领域的泛化能力。现有方法通常在多个源领域上训练单个模型以捕获领域共享信息，但该范式内在地存在两种冲突：1) 样本冲突，源于样本噪声和源之间的极端领域变化；2) 优化冲突，来自多源训练时的竞争和权衡。这些冲突阻碍了泛化并导致次优解。近期研究表明，模型合并可以有效缓解多目标优化中的竞争，提升泛化性能。受此启发，我们提出了Harmonizing and Merging (HAM)，一种基于CLIP领域的新型源模型合并框架。在源模型训练过程中，HAM丰富样本，避免冲突样本，调节所有模型的更新方向。接着引入冗余感知的历史模型合并方法，有效整合所有源模型的知识。HAM全面整合源领域信息，同时实现源模型间相互增强，最终生成具有最优泛化能力的模型。大量实验表明，我们的方法在五个常用基准数据集上表现出色，达到了最先进的性能。

Conclusion: 实验验证，HAM方法在多个基准数据集上达到先进性能，证明了其在CLIP-based领域泛化中的有效性。

Abstract: CLIP-based domain generalization aims to improve model generalization to
unseen domains by leveraging the powerful zero-shot classification capabilities
of CLIP and multiple source datasets. Existing methods typically train a single
model across multiple source domains to capture domain-shared information.
However, this paradigm inherently suffers from two types of conflicts: 1)
sample conflicts, arising from noisy samples and extreme domain shifts among
sources; and 2) optimization conflicts, stemming from competition and
trade-offs during multi-source training. Both hinder the generalization and
lead to suboptimal solutions. Recent studies have shown that model merging can
effectively mitigate the competition of multi-objective optimization and
improve generalization performance. Inspired by these findings, we propose
Harmonizing and Merging (HAM), a novel source model merging framework for
CLIP-based domain generalization. During the training process of the source
models, HAM enriches the source samples without conflicting samples, and
harmonizes the update directions of all models. Then, a redundancy-aware
historical model merging method is introduced to effectively integrate
knowledge across all source models. HAM comprehensively consolidates source
domain information while enabling mutual enhancement among source models,
ultimately yielding a final model with optimal generalization capabilities.
Extensive experiments on five widely used benchmark datasets demonstrate the
effectiveness of our approach, achieving state-of-the-art performance.

</details>
