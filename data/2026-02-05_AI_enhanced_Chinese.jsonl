{"id": "2602.03878", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.03878", "abs": "https://arxiv.org/abs/2602.03878", "authors": ["Longjie Zhao", "Ziming Hong", "Jiaxin Huang", "Runnan Chen", "Mingming Gong", "Tongliang Liu"], "title": "Intellectual Property Protection for 3D Gaussian Splatting Assets: A Survey", "comment": "A collection of relevant papers is summarized and will be continuously updated at \\url{https://github.com/tmllab/Awesome-3DGS-IP-Protection}", "summary": "3D Gaussian Splatting (3DGS) has become a mainstream representation for real-time 3D scene synthesis, enabling applications in virtual and augmented reality, robotics, and 3D content creation. Its rising commercial value and explicit parametric structure raise emerging intellectual property (IP) protection concerns, prompting a surge of research on 3DGS IP protection. However, current progress remains fragmented, lacking a unified view of the underlying mechanisms, protection paradigms, and robustness challenges. To address this gap, we present the first systematic survey on 3DGS IP protection and introduce a bottom-up framework that examines (i) underlying Gaussian-based perturbation mechanisms, (ii) passive and active protection paradigms, and (iii) robustness threats under emerging generative AI era, revealing gaps in technical foundations and robustness characterization and indicating opportunities for deeper investigation. Finally, we outline six research directions across robustness, efficiency, and protection paradigms, offering a roadmap toward reliable and trustworthy IP protection for 3DGS assets.", "AI": {"tldr": "论文对3D高斯散射（3DGS）的知识产权保护进行了全面的综述，分析了不同保护机制和技术挑战，并提出了未来的研究方向。", "motivation": "随着3DGS的商业价值上升和明确的参数结构，知识产权保护成为一个关键问题，而现有的研究在保护机制、范例和鲁棒性挑战上的进展是分散的，因此该论文旨在提供一个系统性的综述。", "method": "该论文介绍了一种自下而上的框架，用于分析3D高斯散射（3DGS）知识产权保护，包括高斯基扰动机制、被动和主动保护范式以及在新兴的生成AI时代下的鲁棒性威胁。", "result": "该论文提出了3DGS知识产权保护的综合性框架，突出研究空白，并指明了未来可能的研究途径。", "conclusion": "论文综述了3DGS知识产权保护领域，指出了技术基础和鲁棒性特征上的空白，并提出了六个研究方向，以实现对3DGS资产的可靠的、值得信赖的知识产权保护。"}}
{"id": "2602.03879", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03879", "abs": "https://arxiv.org/abs/2602.03879", "authors": ["Ali Bayeh", "Samira Sadaoui", "Malek Mouhoub"], "title": "TruKAN: Towards More Efficient Kolmogorov-Arnold Networks Using Truncated Power Functions", "comment": "23 pages, 9 figures", "summary": "To address the trade-off between computational efficiency and adherence to Kolmogorov-Arnold Network (KAN) principles, we propose TruKAN, a new architecture based on the KAN structure and learnable activation functions. TruKAN replaces the B-spline basis in KAN with a family of truncated power functions derived from k-order spline theory. This change maintains the KAN's expressiveness while enhancing accuracy and training time. Each TruKAN layer combines a truncated power term with a polynomial term and employs either shared or individual knots. TruKAN exhibits greater interpretability than other KAN variants due to its simplified basis functions and knot configurations. By prioritizing interpretable basis functions, TruKAN aims to balance approximation efficacy with transparency. We develop the TruKAN model and integrate it into an advanced EfficientNet-V2-based framework, which is then evaluated on computer vision benchmark datasets. To ensure a fair comparison, we develop various models: MLP-, KAN-, SineKAN and TruKAN-based EfficientNet frameworks and assess their training time and accuracy across small and deep architectures. The training phase uses hybrid optimization to improve convergence stability. Additionally, we investigate layer normalization techniques for all the models and assess the impact of shared versus individual knots in TruKAN. Overall, TruKAN outperforms other KAN models in terms of accuracy, computational efficiency and memory usage on the complex vision task, demonstrating advantages beyond the limited settings explored in prior KAN studies.", "AI": {"tldr": "提出TruKAN架构，用截断幂函数族替换KAN中的B样条基函数。提升了KAN模型的准确性和计算效率，并且具有更高的可解释性。", "motivation": "为了解决计算效率与KAN原则的权衡问题。TruKAN试图通过优先考虑可解释的基函数，在近似效能和透明度之间取得平衡。", "method": "提出了一种基于KAN结构和可学习激活函数的新架构TruKAN。TruKAN用从k阶样条理论中衍生出的截断幂函数族替换了KAN中的B样条基函数。这种变更保持了KAN的表达性，同时提高了准确性和训练时间。TruKAN的每一层结合了一个截断幂项和一个多项式项，并且使用共享或单独的结点。", "result": "TruKAN在复杂的视觉任务中在准确性和计算效率方面优于其他KAN模型，并且在内存使用上也有优势。", "conclusion": "TruKAN的效果超越了先前探索的有限设置中的KAN研究，特别是在复杂视觉任务上的准确性和计算效率上表现更优。"}}
{"id": "2602.03881", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03881", "abs": "https://arxiv.org/abs/2602.03881", "authors": ["Maxx Richard Rahman", "Mostafa Hammouda", "Wolfgang Maass"], "title": "DiGAN: Diffusion-Guided Attention Network for Early Alzheimer's Disease Detection", "comment": null, "summary": "Early diagnosis of Alzheimer's disease (AD) remains a major challenge due to the subtle and temporally irregular progression of structural brain changes in the prodromal stages. Existing deep learning approaches require large longitudinal datasets and often fail to model the temporal continuity and modality irregularities inherent in real-world clinical data. To address these limitations, we propose the Diffusion-Guided Attention Network (DiGAN), which integrates latent diffusion modelling with an attention-guided convolutional network. The diffusion model synthesizes realistic longitudinal neuroimaging trajectories from limited training data, enriching temporal context and improving robustness to unevenly spaced visits. The attention-convolutional layer then captures discriminative structural--temporal patterns that distinguish cognitively normal subjects from those with mild cognitive impairment and subjective cognitive decline. Experiments on synthetic and ADNI datasets demonstrate that DiGAN outperforms existing state-of-the-art baselines, showing its potential for early-stage AD detection.", "AI": {"tldr": "DiGAN结合扩散模型和注意力-卷积层，有效解决了现有深度学习方法在处理纵向神经影像数据时遇到的问题，能更准确地预测早期AD。", "motivation": "早期诊断阿尔茨海默病(AD)因其临床前期阶段结构脑变化的微妙性和时间不规则性而面临重大挑战。现有的深度学习方法需要大量的纵向数据集，且往往无法很好地建模这种时间和模态不规则性。", "method": "提出了一种称为DiGAN的扩散引导注意力网络，结合了潜在扩散模型和注意力引导的卷积网络。扩散模型从有限的训练数据中合成现实的纵向神经影像轨迹，增强了时间上下文，提高了对不规则时间间隔访问的鲁棒性。注意力-卷积层捕捉区分认知正常受试者和轻度认知障碍以及主观认知下降受试者的结构-时间模式。", "result": "在合成数据和ADNI数据集上的实验表明，DiGAN在早期AD检测方面优于现有的最先进的基线方法。", "conclusion": "通过将扩散模型与注意力-卷积网络结合，DiGAN能够有效改善早期AD的识别，显示出其在临床应用上的潜力。"}}
{"id": "2602.03882", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03882", "abs": "https://arxiv.org/abs/2602.03882", "authors": ["Haijiang Yan", "Nick Chater", "Adam Sanborn"], "title": "PriorProbe: Recovering Individual-Level Priors for Personalizing Neural Networks in Facial Expression Recognition", "comment": null, "summary": "Incorporating individual-level cognitive priors offers an important route to personalizing neural networks, yet accurately eliciting such priors remains challenging: existing methods either fail to uniquely identify them or introduce systematic biases. Here, we introduce PriorProbe, a novel elicitation approach grounded in Markov Chain Monte Carlo with People that recovers fine-grained, individual-specific priors. Focusing on a facial expression recognition task, we apply PriorProbe to individual participants and test whether integrating the recovered priors with a state-of-the-art neural network improves its ability to predict an individual's classification on ambiguous stimuli. The PriorProbe-derived priors yield substantial performance gains, outperforming both the neural network alone and alternative sources of priors, while preserving the network's inference on ground-truth labels. Together, these results demonstrate that PriorProbe provides a general and interpretable framework for personalizing deep neural networks.", "AI": {"tldr": "PriorProbe引入了一种新颖的先验提取方式，提高了深度神经网络对个体分类的预测能力。", "motivation": "研究动机在于个性化神经网络，而准确获取个体认知先验仍具挑战性。现有方法要么无法唯一识别先验，要么引入系统偏见。", "method": "PriorProbe方法基于具有人的马尔可夫链蒙特卡罗法，能够恢复个体特有的细粒度先验。", "result": "实验表明，PriorProbe获得的先验带来了显著性能提升，优于单独使用神经网络及其他先验来源，同时保留了网络对真实标签的推断能力。", "conclusion": "结果表明PriorProbe提供了一种通用且可解释的框架，用于个性化深度神经网络。"}}
{"id": "2602.03942", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03942", "abs": "https://arxiv.org/abs/2602.03942", "authors": ["Mohamed Elgaar", "Hadi Amiri"], "title": "Linguistic Blind Spots in Clinical Decision Extraction", "comment": "EACL HeaLing Workshop 2026", "summary": "Extracting medical decisions from clinical notes is a key step for clinical decision support and patient-facing care summaries. We study how the linguistic characteristics of clinical decisions vary across decision categories and whether these differences explain extraction failures. Using MedDec discharge summaries annotated with decision categories from the Decision Identification and Classification Taxonomy for Use in Medicine (DICTUM), we compute seven linguistic indices for each decision span and analyze span-level extraction recall of a standard transformer model. We find clear category-specific signatures: drug-related and problem-defining decisions are entity-dense and telegraphic, whereas advice and precaution decisions contain more narrative, with higher stopword and pronoun proportions and more frequent hedging and negation cues. On the validation split, exact-match recall is 48%, with large gaps across linguistic strata: recall drops from 58% to 24% from the lowest to highest stopword-proportion bins, and spans containing hedging or negation cues are less likely to be recovered. Under a relaxed overlap-based match criterion, recall increases to 71%, indicating that many errors are span boundary disagreements rather than complete misses. Overall, narrative-style spans--common in advice and precaution decisions--are a consistent blind spot under exact matching, suggesting that downstream systems should incorporate boundary-tolerant evaluation and extraction strategies for clinical decisions.", "AI": {"tldr": "研究分析了临床决策的语言学特征并发现语言特征的类别特定模式，建议临床决策支持系统应通过调整评估和提取策略来改善这些类别的召回性能。", "motivation": "研究旨在探讨临床决策的语义特征如何随决策类别变化，并分析这些差异是否解释了提取失败的原因。", "method": "该研究使用MedDec出院总结，这些总结根据医疗决策分类法（DICTUM）进行了决策分类标注。研究计算了每个决策片段的七个语言学指标，并分析了一个标准变压器模型在片段级别的提取召回率。", "result": "研究发现了不同决策类别之间的语言特征差异：与药物相关的决策和定义问题的决策文本密度较高且简洁；而建议和预防决策则更具有叙述性，停用词和代词的比例较高，并且经常包含犹豫和否定的表达。在验证集上，精确匹配召回率为48%。语言学特征对召回率影响显著：从停用词比例最低的分箱到最高的分箱，召回率从58%降至24%。放松匹配标准后，召回率增加到71%。", "conclusion": "研究结果表明，针对叙事风格决策片段（常见于建议和预防决策）存在共识，这表明后续系统应考虑采用容差边界评价和提取策略来覆盖临床决策。"}}
{"id": "2602.03883", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03883", "abs": "https://arxiv.org/abs/2602.03883", "authors": ["Akshansh Mishra", "Rakesh Morisetty"], "title": "Explainable Computer Vision Framework for Automated Pore Detection and Criticality Assessment in Additive Manufacturing", "comment": "6 figures", "summary": "Internal porosity remains a critical defect mode in additively manufactured components, compromising structural performance and limiting industrial adoption. Automated defect detection methods exist but lack interpretability, preventing engineers from understanding the physical basis of criticality predictions. This study presents an explainable computer vision framework for pore detection and criticality assessment in three-dimensional tomographic volumes. Sequential grayscale slices were reconstructed into volumetric datasets, and intensity-based thresholding with connected component analysis identified 500 individual pores. Each pore was characterized using geometric descriptors including size, aspect ratio, extent, and spatial position relative to the specimen boundary. A pore interaction network was constructed using percentile-based Euclidean distance criteria, yielding 24,950 inter-pore connections. Machine learning models predicted pore criticality scores from extracted features, and SHAP analysis quantified individual feature contributions. Results demonstrate that normalized surface distance dominates model predictions, contributing more than an order of magnitude greater importance than all other descriptors. Pore size provides minimal influence, while geometric parameters show negligible impact. The strong inverse relationship between surface proximity and criticality reveals boundary-driven failure mechanisms. This interpretable framework enables transparent defect assessment and provides actionable insights for process optimization and quality control in additive manufacturing.", "AI": {"tldr": "本研究提出了一个可解释的计算机视觉框架，用于三维成像体积中的孔隙检测和重要性评估。研究指出重要性分数的预测主要取决于孔隙相对于表面的距离，揭示了边界驱动的失效机制。", "motivation": "研究动机在于现有的自动化缺陷检测方法缺乏可解释性，这使得工程师无法理解对关键性的预测背后的物理基础。", "method": "本研究提出了一个可解释的计算机视觉框架，用于三维成像体积中的孔隙检测和重要性评估。该方法包括将灰度图像序列重建为体积数据集，使用基于强度的阈值和连接组件分析识别孔隙，并使用几何描述符对每个孔隙进行表征。此外，构建了孔隙相互作用网络，并使用机器学习模型预测孔隙的重要性分数，运用SHAP分析量化每个特征的贡献。", "result": "研究中的方法能够识别500个单独的孔隙，并且构建出24,950个孔隙连接。机器学习模型使用从孔隙特征中提取的信息来预测孔隙的重要性分数，而SHAP分析显示，对表面距离的归一化在预测中起主导作用。", "conclusion": "研究结果表明，靠近表面的重要性分数主导模型的预测，其重要性比所有其他描述符高出一个数量级，而尺寸提供最小的影响，几何参数的影响可以忽视。这种强逆相关关系揭示了边界驱动的失效机制。这一可解释框架能促进透明的缺陷评估并为三维打印过程优化和质量控制提供行动建议。"}}
{"id": "2602.03962", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03962", "abs": "https://arxiv.org/abs/2602.03962", "authors": ["Erik Saule", "Kalpathi Subramanian", "Razvan Bunescu"], "title": "Automatic Classification of Pedagogical Materials against CS Curriculum Guidelines", "comment": null, "summary": "Professional societies often publish curriculum guidelines to help programs align their content to international standards. In Computer Science, the primary standard is published by ACM and IEEE and provide detailed guidelines for what should be and could be included in a Computer Science program.\n  While very helpful, it remains difficult for program administrators to assess how much of the guidelines is being covered by a CS program. This is in particular due to the extensiveness of the guidelines, containing thousands of individual items. As such, it is time consuming and cognitively demanding to audit every course to confidently mark everything that is actually being covered. Our preliminary work indicated that it takes about a day of work per course.\n  In this work, we propose using Natural Language Processing techniques to accelerate the process. We explore two kinds of techniques, the first relying on traditional tools for parsing, tagging, and embeddings, while the second leverages the power of Large Language Models. We evaluate the application of these techniques to classify a corpus of pedagogical materials and show that we can meaningfully classify documents automatically.", "AI": {"tldr": "本文探讨了两种技术来加速计算机科学课程对国际标准的覆盖情况的评估过程。", "motivation": "尽管 ACM 和 IEEE 提供了详细的计算机科学课程指导，但课程管理者难以评估这些指导中有多少内容被课程覆盖，因为这些指导包含了很多具体条目，使得评估过程既耗时又耗费脑力。", "method": "本文提出了使用自然语言处理技术来加速评估过程，探索了两种技术：一是传统的解析、标注和嵌入方法，二是利用大型语言模型的优势。", "result": "研究结果表明，可以使用上述自然语言处理技术对教学材料进行自动分类。", "conclusion": "研究证明，通过使用自然语言处理技术，可以有效加快对计算机科学课程是否符合国际标准的评估过程。"}}
{"id": "2602.03890", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.03890", "abs": "https://arxiv.org/abs/2602.03890", "authors": ["Xindan Zhang", "Weilong Yan", "Yufei Shi", "Xuerui Qiu", "Tao He", "Ying Li", "Ming Li", "Hehe Fan"], "title": "4DPC$^2$hat: Towards Dynamic Point Cloud Understanding with Failure-Aware Bootstrapping", "comment": null, "summary": "Point clouds provide a compact and expressive representation of 3D objects, and have recently been integrated into multimodal large language models (MLLMs). However, existing methods primarily focus on static objects, while understanding dynamic point cloud sequences remains largely unexplored. This limitation is mainly caused by the lack of large-scale cross-modal datasets and the difficulty of modeling motions in spatio-temporal contexts. To bridge this gap, we present 4DPC$^2$hat, the first MLLM tailored for dynamic point cloud understanding. To this end, we construct a large-scale cross-modal dataset 4DPC$^2$hat-200K via a meticulous two-stage pipeline consisting of topology-consistent 4D point construction and two-level captioning. The dataset contains over 44K dynamic object sequences, 700K point cloud frames, and 200K curated question-answer (QA) pairs, supporting inquiries about counting, temporal relationship, action, spatial relationship, and appearance. At the core of the framework, we introduce a Mamba-enhanced temporal reasoning MLLM to capture long-range dependencies and dynamic patterns among a point cloud sequence. Furthermore, we propose a failure-aware bootstrapping learning strategy that iteratively identifies model deficiencies and generates targeted QA supervision to continuously strengthen corresponding reasoning capabilities. Extensive experiments demonstrate that our 4DPC$^2$hat significantly improves action understanding and temporal reasoning compared with existing models, establishing a strong foundation for 4D dynamic point cloud understanding.", "AI": {"tldr": "本研究构建了名为4DPC$^2$hat的多模态语言模型，以解决现有模型在动态点云序列理解方面的不足。通过提出的大规模跨模态数据集4DPC$^2$hat-200K以及增强的时间推理方法，研究显著提高了动作理解和时间推理能力。", "motivation": "现有的方法主要集中在静态物体上，对于动态点云序列的理解研究较少。这种不足主要是由于缺乏大规模跨模态数据集以及在时空上下文中建模运动的困难。", "method": "本文提出了一种名为4DPC$^2$hat的大型多模态语言模型，专门用于动态点云的理解。研究构建了一个大规模的跨模态数据集4DPC$^2$hat-200K，该数据集通过两个步骤的管道生成：拓扑一致的4D点构建和两层次的标注。该模型利用增强的时间推理技术来捕捉点云序列中的长范围依赖和动态模式。此外，研究还提出了一种故障感知自举学习策略，该策略能够迭代地识别模型的缺陷，并生成有针对性的问答监督来增强相应的推理能力。", "result": "实验结果表明，该模型在动作理解和时间推理方面显著优于现有的模型，为4D动态点云的理解奠定了坚实的基础。", "conclusion": "研究提出了多模态语言模型4DPC$^2$hat，特别适用于动态点云的理解，并证明了其在动作理解和时间推理方面的能力。"}}
{"id": "2602.03979", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03979", "abs": "https://arxiv.org/abs/2602.03979", "authors": ["Ariel Kwiatkowski", "Natasha Butt", "Ismail Labiad", "Julia Kempe", "Yann Ollivier"], "title": "Likelihood-Based Reward Designs for General LLM Reasoning", "comment": null, "summary": "Fine-tuning large language models (LLMs) on reasoning benchmarks via reinforcement learning requires a specific reward function, often binary, for each benchmark. This comes with two potential limitations: the need to design the reward, and the potentially sparse nature of binary rewards. Here, we systematically investigate rewards derived from the probability or log-probability of emitting the reference answer (or any other prompt continuation present in the data), which have the advantage of not relying on specific verifiers and being available at scale. Several recent works have advocated for the use of similar rewards (e.g., VeriFree, JEPO, RLPR, NOVER). We systematically compare variants of likelihood-based rewards with standard baselines, testing performance both on standard mathematical reasoning benchmarks, and on long-form answers where no external verifier is available. We find that using the log-probability of the reference answer as the reward for chain-of-thought (CoT) learning is the only option that performs well in all setups. This reward is also consistent with the next-token log-likelihood loss used during pretraining. In verifiable settings, log-probability rewards bring comparable or better success rates than reinforcing with standard binary rewards, and yield much better perplexity. In non-verifiable settings, they perform on par with SFT. On the other hand, methods based on probability, such as VeriFree, flatline on non-verifiable settings due to vanishing probabilities of getting the correct answer. Overall, this establishes log-probability rewards as a viable method for CoT fine-tuning, bridging the short, verifiable and long, non-verifiable answer settings.", "AI": {"tldr": "通过系统比较，发现使用参考答案的对数概率作为奖励信号进行链式思考学习的方法在多种设定下表现良好，特别是在可验证和不可验证的情况下都能取得较好的效果，显示出对数概率奖励是一种有效的细调方法。", "motivation": "克服以往奖励函数设计复杂和稀疏二进制奖励的问题，探索基于似然概率的奖励信号在语言模型中进行链式思考学习的有效性。", "method": "系统地比较了基于似然概率的奖励信号，使用标准基准测试数学推理能力和长文本答案的情况。", "result": "发现使用参考答案的对数概率作为奖励信号的方法在所有设定下具有良好的表现，特别是在没有外部验证的情况下表现良好。而基于概率的方法在不可验证的条件下表现不佳。", "conclusion": "对数概率奖励在链式思考的微调中是一种有效的方法，能够跨越简短可验证和长文本不可验证答案的设定。"}}
{"id": "2602.03892", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.03892", "abs": "https://arxiv.org/abs/2602.03892", "authors": ["Jinxing Zhou", "Yanghao Zhou", "Yaoting Wang", "Zongyan Han", "Jiaqi Ma", "Henghui Ding", "Rao Muhammad Anwer", "Hisham Cholakkal"], "title": "Audit After Segmentation: Reference-Free Mask Quality Assessment for Language-Referred Audio-Visual Segmentation", "comment": null, "summary": "Language-referred audio-visual segmentation (Ref-AVS) aims to segment target objects described by natural language by jointly reasoning over video, audio, and text. Beyond generating segmentation masks, providing rich and interpretable diagnoses of mask quality remains largely underexplored. In this work, we introduce Mask Quality Assessment in the Ref-AVS context (MQA-RefAVS), a new task that evaluates the quality of candidate segmentation masks without relying on ground-truth annotations as references at inference time. Given audio-visual-language inputs and each provided segmentation mask, the task requires estimating its IoU with the unobserved ground truth, identifying the corresponding error type, and recommending an actionable quality-control decision. To support this task, we construct MQ-RAVSBench, a benchmark featuring diverse and representative mask error modes that span both geometric and semantic issues. We further propose MQ-Auditor, a multimodal large language model (MLLM)-based auditor that explicitly reasons over multimodal cues and mask information to produce quantitative and qualitative mask quality assessments. Extensive experiments demonstrate that MQ-Auditor outperforms strong open-source and commercial MLLMs and can be integrated with existing Ref-AVS systems to detect segmentation failures and support downstream segmentation improvement. Data and codes will be released at https://github.com/jasongief/MQA-RefAVS.", "AI": {"tldr": "这篇文章介绍了一个新的任务MQA-RefAVS，旨在评估语言指引的视音频分割掩码的质量。它构建了MQ-RAVSBench基准，并提出了MQ-Auditor模型来评估掩码质量，实验显示其优于现有的MLLM模型。", "motivation": "在描述性的自然语言指引下通过共同分析视频、音频和文本，切分目标对象的Ref-AVS领域中，提供丰富的可解释的掩码质量诊断仍然很少被探索。因此，引入了MQA-RefAVS，其目的是在推断时无需依赖地面真实标注的情况下，评估候选分割掩码的质量。", "method": "提出了MQ-Auditor，这是一种基于多模态大型语言模型（MLLM）的审计器，能够明确地结合多模态线索和分割掩码信息，以生成定量和定性的掩码质量评估。", "result": "实验结果显示MQ-Auditor优于开源和商业的MLLM模型，可以检测分割失败，并支持下游的分割改进。", "conclusion": "MQ-Auditor能够准确评估分割掩码的质量，并能够推荐质量控制的决策，增强了Ref-AVS系统的有效性和鲁棒性。此外，其结果可以通过开源数据和代码获得验证。"}}
{"id": "2602.03980", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03980", "abs": "https://arxiv.org/abs/2602.03980", "authors": ["Vsevolod Kapatsinski"], "title": "Transformers perform adaptive partial pooling", "comment": "6 pages, submitted to the annual meeting of the Cognitive Science Society", "summary": "Because language is creative, any reasonable language model must generalize, deciding what to say in novel contexts by using information from similar contexts. But what about contexts that are not novel but merely infrequent? In hierarchical regression, the model's predictions for behavior in a context are affected by observations from other similar contexts to the extent that 1) the current context is infrequent and 2) different contexts behave similarly. This is called adaptive partial pooling of evidence. This paper shows that next-word predictions of a transformer (GPT2) are increasingly unaffected by observations from outside the current context across epochs of training (the amount of pooling reduces with training), and that the extent of pooling is affected by context frequency, context number (type frequency) and context variability in a similar way to hierarchical regression. These characteristics of learning in transformers are argued to be realistic on both rational and empirical grounds.", "AI": {"tldr": "The paper investigates how transformers, like GPT2, adapt their prediction strategies based on context, mimicking human behavior by generalizing from similar, yet infrequent scenarios, and observes a decrease in reliance on external context data as training progresses.", "motivation": "The motivation behind this research is to understand how language models, particularly transformers, handle infrequent contexts in language tasks. This understanding is critical for improving the ability of such models to generalize effectively.", "method": "The paper employs a transformer model, specifically GPT2, to analyze next-word prediction patterns and examines how these predictions are influenced by observations from similar contexts. It compares the transformer's learning characteristics with those of hierarchical regression, focusing on factors like context frequency, number of contexts, and variability.", "result": "The study reveals that as the transformer (GPT2) undergoes more epochs of training, its next-word predictions become less reliant on data from outside the immediate context, a phenomenon the authors call a reduction in adaptive partial pooling. The impact of context frequency, context number, and variability on this pooling process mirrors findings in hierarchical regression.", "conclusion": "The behavior of transformers in learning from context similarities and infrequency seems to align with more general patterns observed in learning mechanisms, making their learning characteristics both logically sound and consistent with empirical evidence."}}
{"id": "2602.03893", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.03893", "abs": "https://arxiv.org/abs/2602.03893", "authors": ["Yibing Wang", "Shuang Li", "Tingting Huang", "Yu Zhang", "Chulhong Kim", "Seongwook Choi", "Changhui Li"], "title": "GPAIR: Gaussian-Kernel-Based Ultrafast 3D Photoacoustic Iterative Reconstruction", "comment": null, "summary": "Although the iterative reconstruction (IR) algorithm can substantially correct reconstruction artifacts in photoacoustic (PA) computed tomography (PACT), it suffers from long reconstruction times, especially for large-scale three-dimensional (3D) imaging in which IR takes hundreds of seconds to hours. The computing burden severely limits the practical applicability of IR algorithms. In this work, we proposed an ultrafast IR method for 3D PACT, called Gaussian-kernel-based Ultrafast 3D Photoacoustic Iterative Reconstruction (GPAIR), which achieves orders-of-magnitude acceleration in computing. GPAIR transforms traditional spatial grids with continuous isotropic Gaussian kernels. By deriving analytical closed-form expression for pressure waves and implementing powerful GPU-accelerated differentiable Triton operators, GPAIR demonstrates extraordinary ultrafast sub-second reconstruction speed for 3D targets containing 8.4 million voxels in animal experiments. This revolutionary ultrafast image reconstruction enables near-real-time large-scale 3D PA reconstruction, significantly advancing 3D PACT toward clinical applications.", "AI": {"tldr": "An ultrafast IR method for 3D PACT, called GPAIR, is proposed to significantly reduce computing times.", "motivation": "To overcome the long reconstruction time issue of the traditional iterative reconstruction (IR) algorithm in 3D PACT.", "method": "Gaussian-kernel-based Ultrafast 3D Photoacoustic Iterative Reconstruction (GPAIR) method for 3D PACT is proposed.", "result": "GPAIR method achieves ultrafast sub-second reconstruction speed for 3D targets, greatly accelerating the process and making it practical for clinical use.", "conclusion": "The introduction of GPAIR enables near-real-time 3D PA reconstruction, pushing 3D PACT closer to practical clinical applications."}}
{"id": "2602.04033", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.04033", "abs": "https://arxiv.org/abs/2602.04033", "authors": ["Jindřich Libovický"], "title": "On the Credibility of Evaluating LLMs using Survey Questions", "comment": "Accepted to the Workshop on Multilingual and Multicultural Evaluation at EACL 2026, 12 pages, 2 figures", "summary": "Recent studies evaluate the value orientation of large language models (LLMs) using adapted social surveys, typically by prompting models with survey questions and comparing their responses to average human responses. This paper identifies limitations in this methodology that, depending on the exact setup, can lead to both underestimating and overestimating the similarity of value orientation. Using the World Value Survey in three languages across five countries, we demonstrate that prompting methods (direct vs. chain-of-thought) and decoding strategies (greedy vs. sampling) significantly affect results. To assess the interaction between answers, we introduce a novel metric, self-correlation distance. This metric measures whether LLMs maintain consistent relationships between answers across different questions, as humans do. This indicates that even a high average agreement with human data, when considering LLM responses independently, does not guarantee structural alignment in responses. Additionally, we reveal a weak correlation between two common evaluation metrics, mean-squared distance and KL divergence, which assume that survey answers are independent of each other. For future research, we recommend CoT prompting, sampling-based decoding with dozens of samples, and robust analysis using multiple metrics, including self-correlation distance.", "AI": {"tldr": "本文探讨了大型语言模型价值观评估中方法论的局限性，并提出了一种新的评估标准--自我相关距离。", "motivation": "研究指出现有通过改编社会调查评估大型语言模型价值观的方法存在局限性，并可能导致低估或高估模型与人类价值观的相似度。", "method": "研究采用世界价值观调查，在三种语言和五个国家中评估了大型语言模型（LLMs）的价值观取向，并比较了直接提示与思维链提示方法以及贪婪与采样解码策略的效果。", "result": "研究结果显示提示方法和解码策略对结果有显著影响，并引入了一种新的评估标准--自我相关距离，以评估回答之间的相互作用。", "conclusion": "研究发现即使大型语言模型在与人类调查数据的平均一致性高，但并不保证其回答之间具有结构性的一致性，且两种常见评估指标之间相关性较弱。建议未来研究采用思维链提示、多采样解码策略并使用包括自我相关距离在内的多重评估指标。"}}
