<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 14]
- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Direct Token Optimization: A Self-contained Approach to Large Language Model Unlearning](https://arxiv.org/abs/2510.00125)
*Hong kyu Lee,Ruixuan Liu,Li Xiong*

Main category: cs.CL

> 本文提出了一种名为直接令牌优化（DTO）的自包含机器遗忘方法，该方法提高了遗忘质量和保持了模型性能，且不依赖外部资源。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大型语言模型的遗忘方法通常依赖外部资源如辅助语言模型、保留数据集或甚至商业AI服务，这在实践中往往是不切实际的，并且可能会引入额外的隐私风险。

**Method:** 提出了一种名为直接令牌优化（DTO）的新自包含机器遗忘方法，该方法通过直接优化令牌级别的目标，消除了对外部资源的需求。

**Result:** 实验结果表明，所提出的DTO在多个基准数据集上的遗忘质量比最新的基线方法提高了多达16.8倍，同时保持了可比的模型实用性。

**Conclusion:** DTO方法通过直接优化令牌级别的目标，实现了消除外部资源依赖，提高了遗忘质量和保持模型实用性。

**Abstract:** Machine unlearning is an emerging technique that removes the influence of a
subset of training data (forget set) from a model without full retraining, with
applications including privacy protection, content moderation, and model
correction. The key challenge lies in ensuring that the model completely
forgets the knowledge of the forget set without compromising its overall
utility. Existing unlearning methods for large language models (LLMs) often
utilize auxiliary language models, retain datasets, or even commercial AI
services for effective unlearning and maintaining the model utility. However,
dependence on these external resources is often impractical and could
potentially introduce additional privacy risks. In this work, we propose direct
token optimization (DTO), a novel self-contained unlearning approach for LLMs
that directly optimizes the token level objectives and eliminates the need for
external resources. Given a sequence to unlearn, we identify two categories of
tokens: target tokens, which capture critical knowledge for unlearning, and the
remaining non-target tokens, which are crucial for maintaining the model
utility. The former are used to optimize the unlearning objective, while the
latter serve to preserve the model's performance. The experimental results show
that the proposed DTO achieves up to 16.8$\times$ improvement in forget quality
on several benchmark datasets than the latest baselines while maintaining a
comparable level of model utility.

</details>


### [2] [TAMA: Tool-Augmented Multimodal Agent for Procedural Activity Understanding](https://arxiv.org/abs/2510.00161)
*Kimihiro Hasegawa,Wiradee Imrattanatrai,Masaki Asada,Ken Fukuda,Teruko Mitamura*

Main category: cs.CL

> 本文介绍了一种创新的程序性活动理解框架TAMA，该框架使用无需训练的多媒体工具在多模态情况下工作，提升了与视觉语言模型结合后的性能表现。

<details>
  <summary>Details</summary>

**Motivation:** 论文旨在解决专为程序性活动助手开发的系统尚不成熟的问题，目标是支撑人类在日常生活和专业环境中进行程序性活动的理解与辅助。

**Method:** 我们提出了一种名为TAMA（Tool-Augmented Multimodal Agent）的框架，该框架能够在无需训练的情况下通过使用多媒体工具进行交错的多模态推理。

**Result:** 实验结果表明，与GPT-5和MiMo-VL等视觉语言模型结合时，我们的方法能提升性能。另外，消融研究亦证实了框架的两个关键特征：多媒体返回工具和代理灵活工具选择的有效性。

**Conclusion:** 我们的框架和实验结果促进了图像思考范式的应用，不仅局限于视频和多模态任务领域，还促进了程序性活动助手的发展。

**Abstract:** Procedural activity assistants potentially support humans in a variety of
settings, from our daily lives, e.g., cooking or assembling flat-pack
furniture, to professional situations, e.g., manufacturing or biological
experiments. Despite its potential use cases, the system development tailored
for such an assistant is still underexplored. In this paper, we propose a novel
framework, called TAMA, a Tool-Augmented Multimodal Agent, for procedural
activity understanding. TAMA enables interleaved multimodal reasoning by making
use of multimedia-returning tools in a training-free setting. Our experimental
result on the multimodal procedural QA dataset, ProMQA-Assembly, shows that our
approach can improve the performance of vision-language models, especially
GPT-5 and MiMo-VL. Furthermore, our ablation studies provide empirical support
for the effectiveness of two features that characterize our framework,
multimedia-returning tools and agentic flexible tool selection. We believe our
proposed framework and experimental results facilitate the thinking with images
paradigm for video and multimodal tasks, let alone the development of
procedural activity assistants.

</details>


### [3] [DRBench: A Realistic Benchmark for Enterprise Deep Research](https://arxiv.org/abs/2510.00172)
*Amirhossein Abaskohi,Tianyi Chen,Miguel Muñoz-Mármol,Curtis Fox,Amrutha Varshini Ramesh,Étienne Marcotte,Xing Han Lù,Nicolas Chapados,Spandana Gella,Christopher Pal,Alexandre Drouin,Issam H. Laradji*

Main category: cs.CL

> 本文提出了DRBench，一个针对企业环境中复杂开放性深度研究任务的AI代理评估基准。

<details>
  <summary>Details</summary>

**Motivation:** 传统的基准测试专注于简单问题或仅限于网络查询，但并不能完全捕捉到现实世界中企业环境下复杂的问题解决过程。因此，我们需要一个能评估AI代理在多步骤查询和寻找异构数据源中相关信息能力的基准。

**Method:** 我们提出了DRBench，这是一个用于评估企业环境中AI代理在复杂开放性深度研究任务上的基准测试工具。DRBench通过多步骤查询评估代理，这些查询需要从公共网络和私人公司知识库中识别支持性事实。每个任务都是基于现实的用户角色和企业情境设计的，涵盖了包括生产力软件、云文件系统、电子邮件、聊天对话和开放网络在内的异构搜索空间。任务通过一个精心设计的合成管道生成，并经由人工验证。代理的评估基于它们召回相关信息、保持事实准确性以及生成连贯且结构良好的报告的能力。

**Result:** 我们发布了跨越10个领域的15个深度研究任务，如销售、网络安全和合规性。通过评估来自开源和闭源模型（如GPT、Llama和Qwen）的多样DR代理以及不同的DR策略，我们展示了DRBench的有效性，指出了这些模型和策略的优势、劣势以及推进企业深度研究的关键路径。

**Conclusion:** DRBench作为一个全面的、针对企业深度研究任务的评估框架，强调了复杂信息整合能力的需求，为提升AI代理在企业应用场景中的性能提供了指导。

**Abstract:** We introduce DRBench, a benchmark for evaluating AI agents on complex,
open-ended deep research tasks in enterprise settings. Unlike prior benchmarks
that focus on simple questions or web-only queries, DRBench evaluates agents on
multi-step queries (for example, ``What changes should we make to our product
roadmap to ensure compliance with this standard?") that require identifying
supporting facts from both the public web and private company knowledge base.
Each task is grounded in realistic user personas and enterprise context,
spanning a heterogeneous search space that includes productivity software,
cloud file systems, emails, chat conversations, and the open web. Tasks are
generated through a carefully designed synthesis pipeline with
human-in-the-loop verification, and agents are evaluated on their ability to
recall relevant insights, maintain factual accuracy, and produce coherent,
well-structured reports. We release 15 deep research tasks across 10 domains,
such as Sales, Cybersecurity, and Compliance. We demonstrate the effectiveness
of DRBench by evaluating diverse DR agents across open- and closed-source
models (such as GPT, Llama, and Qwen) and DR strategies, highlighting their
strengths, weaknesses, and the critical path for advancing enterprise deep
research. Code is available at https://github.com/ServiceNow/drbench.

</details>


### [4] [PrimeX: A Dataset of Worldview, Opinion, and Explanation](https://arxiv.org/abs/2510.00174)
*Rik Koncel-Kedziorski,Brihi Joshi,Tim Paek*

Main category: cs.CL

> 本研究开发了PrimeX数据集，以研究个人信念在增强语言模型用户对齐中的作用，发现信念解释和世界观能够促进语言模型的个性化。

<details>
  <summary>Details</summary>

**Motivation:** 随着语言模型的应用越来越广泛，更好地表示个人用户对模型的需求也在增加。本研究试图探究个人信念系统是否存在可以用于增强语言模型与用户对齐的信息。

**Method:** 通过开发PrimeX数据集，该数据集包含了来自858名美国居民的公共意见调查数据，以及两个额外的个人信念信息来源：受访者撰写的具体意见的理由，以及评估受访者世界观的原始世界信念调查。

**Result:** 初步分析结果表明，信念解释和世界观对于个性化语言模型具有价值。此外，PrimeX中的额外信念信息可以对自然语言处理社区和心理学研究社区产生积极作用。

**Conclusion:** 研究表明，PrimeX数据集中的信念信息可以为个性化语言模型提供帮助，并为NLP和心理学研究开辟新的研究途径。

**Abstract:** As the adoption of language models advances, so does the need to better
represent individual users to the model. Are there aspects of an individual's
belief system that a language model can utilize for improved alignment?
Following prior research, we investigate this question in the domain of opinion
prediction by developing PrimeX, a dataset of public opinion survey data from
858 US residents with two additional sources of belief information: written
explanations from the respondents for why they hold specific opinions, and the
Primal World Belief survey for assessing respondent worldview. We provide an
extensive initial analysis of our data and show the value of belief
explanations and worldview for personalizing language models. Our results
demonstrate how the additional belief information in PrimeX can benefit both
the NLP and psychological research communities, opening up avenues for further
study.

</details>


### [5] [Personalized Reasoning: Just-In-Time Personalization and Why LLMs Fail At It](https://arxiv.org/abs/2510.00177)
*Shuyue Stella Li,Avinandan Bose,Faeze Brahman,Simon Shaolei Du,Pang Wei Koh,Maryam Fazel,Yulia Tsvetkov*

Main category: cs.CL

> PREFDISCO 方法被提出以帮助语言模型理解和应对个性化的推理需求，特别是当在没有用户交互历史的情况下需要即时满足个性化需求时。发现 29%的个性化推理尝试未达到理想效果。

<details>
  <summary>Details</summary>

**Motivation:** 当前的语言模型开发框架无法有效应对即时应用场景中用户个性化需求，尤其是在冷启动条件或隐私限制下的情况下。这种挑战要求模型不仅仅是解决任务正确，还需要精确地匹配用户的特定偏好。

**Method:** 当前的大语言模型(LLM)开发将任务解决和偏好对齐视为两个独立的挑战，先优化客观正确性，然后对齐众多人类偏好。然而，在面向人类的应用中，仅解决问题是不够的，如果响应与用户需求不匹配，则会失败。在冷启动条件或隐私限制下，即时场景中没有之前的用户交互历史，这一挑战更为严重。LLM 需要识别它们对用户偏好的未知部分，通过提问策略性地获取偏好值，然后相应地调整它们的推理过程和响应。我们引入了一个名为 PREFDISCO 的评估方法，该方法可以将静态基准转化为基于心理学的具有稀疏偏好的人设的互动个性化任务。框架创建了情景，其中相同的问题需要根据用户背景来不同的推理链，因为最优解释方法取决于个人专业知识和偏好，同时保持事实准确性。评估前沿模型的性能显示，29.0% 的个人化尝试表现出比通用响应更差的偏好对齐，而通用响应也无法很好地满足单个用户的需求。这些发现表明个性化推理需要专门开发而不应自然产生。PREFDISCO 建立了个性化推理作为研究前沿可度量的领域，并揭示了当前 LLM 交互能力的基本限制，为开发教育、医疗保健和技术领域的个性化系统奠定了基础，这些领域的个性化至关重要。

**Result:** 评估显示，21 个前沿 LLM 模型在 10 个任务中表现不理想，表现出个性化推理需要专门开发，而不仅仅是自然演化。

**Conclusion:** 研究突出了个性化推理作为语言模型研究的新领域，强调了当前 LLM 的交互能力存在基本限制。PREFDISCO 为未来在教育、医疗保健和技术领域的个性化系统发展提供了基础。

**Abstract:** Current large language model (LLM) development treats task-solving and
preference alignment as separate challenges, optimizing first for objective
correctness, then for alignment to aggregated human preferences. This paradigm
fails in human-facing applications where solving a problem correctly is
insufficient if the response mismatches the user's needs. This challenge
intensifies in just-in-time scenarios where no prior user interaction history
exists due to cold-start conditions or privacy constraints. LLMs need to
identify what they don't know about user preferences, strategically elicit
preference values through questioning, then adapt their reasoning processes and
responses accordingly -- a complicated chain of cognitive processes which we
term personalized reasoning. We introduce PREFDISCO, an evaluation methodology
that transforms static benchmarks into interactive personalization tasks using
psychologically-grounded personas with sparse preferences. Our framework
creates scenarios where identical questions require different reasoning chains
depending on user context, as optimal explanation approaches vary by individual
expertise and preferences while maintaining factual accuracy. Evaluation of 21
frontier models across 10 tasks reveals 29.0% of naive personalization attempts
produce worse preference alignment than generic responses, yet generic
responses also fail to serve individual user needs effectively. These findings
suggest personalized reasoning requires dedicated development rather than
emerging naturally. PREFDISCO establishes personalized reasoning as a
measurable research frontier and reveals fundamental limitations in current
LLMs' interactive capabilities, providing a foundation for developing systems
that can adapt to individual users in education, healthcare, and technical
domains where personalization is critical.

</details>


### [6] [BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses](https://arxiv.org/abs/2510.00232)
*Xin Xu,Xunzhi He,Churan Zhi,Ruizhe Chen,Julian McAuley,Zexue He*

Main category: cs.CL

> 研究者提出了BiasFreeBench，一个用于比较八种主流去偏方法的统一评价基准，同时引入了Bias-Free Score来衡量模型回复的公平性和安全性，旨在解决现有研究的一致性和现实差距问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有的去偏方法存在着因评估方法不同而导致的不一致问题，并且现有的评估方式忽视了现实使用场景的差异。因此，为了提供更加统一和贴近实际使用情况的评估方式，引入了BiasFreeBench和Bias-Free Score，为目的在此。

**Method:** 现有的研究在评估大型语言模型（LLMs）的去偏方法时使用了多样化的基线和指标，导致了不一致的对比结果。此外，这些评估大多依赖于LLMs对偏见和非偏见上下文概率的比较，忽视了与真实使用场景之间的差距。为了能够在去偏方法之间进行一致的评估并填补这一差距，研究人员引入了BiasFreeBench，这是一个实证基准，全面比较了八种主流的去偏技巧（包括四种基于提示的方法和四种基于训练的方法），这八种方法在两个测试场景（多选题QA和开放性的多轮QA）中进行比较，通过重组现有数据集进行统一的查询-响应设置。另外，引入了一个针对回复的指标，即Bias-Free Score，用于衡量LLM回复是否公平、安全、以及反刻板印象的程度。去偏性能会根据几个关键维度进行系统比较和分析：提示法与训练法的范式、模型大小、以及不同训练策略向未见过的偏见类型推广的一般化能力。

**Result:** 此次研究比较了八种主流的去偏方法（四种基于提示和四种基于训练的方法），提出通过Bias-Free Score来衡量模型回复的质量，并展示了一套可以系统地比较和分析去偏效果的方法。

**Conclusion:** 通过BiasFreeBench和Bias-Free Score的引入，研究者希望提供一个统一且接近实际应用场景的评估环境，以鼓励更加公平和安全的LLM模型的发展。同时，作者还计划公开此基准，以促进行业内对去偏研究的统一性和协作性。

**Abstract:** Existing studies on bias mitigation methods for large language models (LLMs)
use diverse baselines and metrics to evaluate debiasing performance, leading to
inconsistent comparisons among them. Moreover, their evaluations are mostly
based on the comparison between LLMs' probabilities of biased and unbiased
contexts, which ignores the gap between such evaluations and real-world use
cases where users interact with LLMs by reading model responses and expect fair
and safe outputs rather than LLMs' probabilities. To enable consistent
evaluation across debiasing methods and bridge this gap, we introduce
BiasFreeBench, an empirical benchmark that comprehensively compares eight
mainstream bias mitigation techniques (covering four prompting-based and four
training-based methods) on two test scenarios (multi-choice QA and open-ended
multi-turn QA) by reorganizing existing datasets into a unified query-response
setting. We further introduce a response-level metric, Bias-Free Score, to
measure the extent to which LLM responses are fair, safe, and
anti-stereotypical. Debiasing performances are systematically compared and
analyzed across key dimensions: the prompting vs. training paradigm, model
size, and generalization of different training strategies to unseen bias types.
We will publicly release our benchmark, aiming to establish a unified testbed
for bias mitigation research.

</details>


### [7] [TASER: Translation Assessment via Systematic Evaluation and Reasoning](https://arxiv.org/abs/2510.00255)
*Monishwaran Maheswaran,Marco Carini,Christian Federmann,Tony Diaz*

Main category: cs.CL

> 本研究开发了TASER，一种通过利用大型推理模型（LRMs）进行自动化翻译质量评估的指标，在多个评估任务中表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 这项研究的动机是为了克服现行自动化评估指标的问题，通过引入系统、分步骤的评估方法，利用LRMs的显式推理能力来提升翻译质量评估的准确性和透明度。

**Method:** 我们提出了TASER（通过系统评估和推理进行翻译评估）这一指标，该指标利用大型推理模型（LRMs）来进行自动化翻译质量评估。TASER利用LRMs的显式推理能力，进行系统化的、分步骤的评估。

**Result:** TASER在WMT24 Metrics Shared Task中的系统级评估中，在基于参考和非基于参考设置下均取得了最高的软成对准确性，并在段落层面的非基于参考评估中表现优异。

**Conclusion:** 研究证明，大型推理模型在翻译质量评估方面比传统语言模型有明显的进步，结合了更高的准确性和透明性，可以应用于多种语言对。

**Abstract:** We introduce TASER (Translation Assessment via Systematic Evaluation and
Reasoning), a metric that uses Large Reasoning Models (LRMs) for automated
translation quality assessment. TASER harnesses the explicit reasoning
capabilities of LRMs to conduct systematic, step-by-step evaluation of
translation quality. We evaluate TASER on the WMT24 Metrics Shared Task across
both reference-based and reference-free scenarios, demonstrating
state-of-the-art performance. In system-level evaluation, TASER achieves the
highest soft pairwise accuracy in both reference-based and reference-free
settings, outperforming all existing metrics. At the segment level, TASER
maintains competitive performance with our reference-free variant ranking as
the top-performing metric among all reference-free approaches. Our experiments
reveal that structured prompting templates yield superior results with LRMs
compared to the open-ended approaches that proved optimal for traditional LLMs.
We evaluate o3, a large reasoning model from OpenAI, with varying reasoning
efforts, providing insights into the relationship between reasoning depth and
evaluation quality. The explicit reasoning process in LRMs offers
interpretability and visibility, addressing a key limitation of existing
automated metrics. Our results demonstrate that Large Reasoning Models show a
measurable advancement in translation quality assessment, combining improved
accuracy with transparent evaluation across diverse language pairs.

</details>


### [8] [Retrieval-Augmented Generation for Electrocardiogram-Language Models](https://arxiv.org/abs/2510.00261)
*Xiaoyu Song,William Han,Tony Chen,Chaojing Duan,Michael A. Rosenberg,Emerson Liu,Ding Zhao*

Main category: cs.CL

> 我们开发了首个开源RAG管道，用于生成性心电图语言模型，提高了模型的可信度和生成结果，已在三个公开数据集上验证了这一效果。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有研究中缺乏针对ELMs的RAG管道设计的系统性研究及开源实现，本研究旨在填补这一空白，并改善生成模型的可信赖性和生成质量。

**Method:** 本研究提出了首个开源的检索增强生成（RAG）管道，用于生成性心电图语言模型（ELMs），并通过基线和消融研究来评估其在自然语言生成（NLG）方面的能力。

**Result:** 实验表明，在多个公共数据集上，使用RAG的ELM模型在生成性能方面优于未使用RAG的基线模型。

**Conclusion:** 实验结果表明，在加入RAG后，ELMs在自然语言生成任务中的性能有了显著的提升，此研究还强调了ELM设计时需要考虑的关键点。

**Abstract:** Interest in generative Electrocardiogram-Language Models (ELMs) is growing,
as they can produce textual responses conditioned on ECG signals and textual
queries. Unlike traditional classifiers that output label probabilities, ELMs
are more versatile, supporting domain-specific tasks (e.g., waveform analysis,
diagnosis, prognosis) as well as general tasks (e.g., open-ended questions,
dialogue). Retrieval-Augmented Generation (RAG), widely used in Large Language
Models (LLMs) to ground LLM outputs in retrieved knowledge, helps reduce
hallucinations and improve natural language generation (NLG). However, despite
its promise, no open-source implementation or systematic study of RAG pipeline
design for ELMs currently exists. To address this gap, we present the first
open-source RAG pipeline for ELMs, along with baselines and ablation studies
for NLG. Experiments on three public datasets show that ELMs with RAG
consistently improves performance over non-RAG baselines and highlights key ELM
design considerations. Our code is available at:
https://github.com/willxxy/ECG-Bench.

</details>


### [9] [Judging with Confidence: Calibrating Autoraters to Preference Distributions](https://arxiv.org/abs/2510.00263)
*Zhuohang Li,Xiaowei Li,Chengyu Huang,Guowang Li,Katayoon Goshvadi,Bo Dai,Dale Schuurmans,Paul Zhou,Hamid Palangi,Yiwen Song,Palash Goyal,Murat Kantarcioglu,Bradley A. Malin,Yuan Xue*

Main category: cs.CL

> 提出了一个通用框架，用于校准与目标人群定义的偏好完整分布相匹配的概率自动化评分者。

<details>
  <summary>Details</summary>

**Motivation:** 自动化评分者（autoraters）的可靠性受限于训练时使用离散偏好标签导致的单一真实情况，这在任务往往是主观、模棱两可或细微差别时构成挑战。为了提高可靠性，自动化评分者必须学习包含目标人群定义的偏好完整分布。

**Method:** 我们提出了一种用于校准概率自动化评分者的通用框架，该框架可以根据给定的偏好分布进行调整。针对不同的数据条件，我们提出了两种学习方法：1）针对密集的概率标签的直接监督微调；2）针对稀疏的二进制标签的强化学习方法。

**Result:** 实验结果表明，采用分布匹配目标微调的自动化评分者能产生更符合目标偏好分布的概率预测，具有更好的校准效果以及显著降低的位置偏差，同时保留了在客观任务上的性能。

**Conclusion:** 该研究提出的方法改善了自动化评分者与人类价值观的对齐问题，通过学习完整偏好的分布来提高自动化评分者的可靠性和一致性。

**Abstract:** The alignment of large language models (LLMs) with human values increasingly
relies on using other LLMs as automated judges, or ``autoraters''. However,
their reliability is limited by a foundational issue: they are trained on
discrete preference labels, forcing a single ground truth onto tasks that are
often subjective, ambiguous, or nuanced. We argue that a reliable autorater
must learn to model the full distribution of preferences defined by a target
population. In this paper, we propose a general framework for calibrating
probabilistic autoraters to any given preference distribution. We formalize the
problem and present two learning methods tailored to different data conditions:
1) a direct supervised fine-tuning for dense, probabilistic labels, and 2) a
reinforcement learning approach for sparse, binary labels. Our empirical
results show that finetuning autoraters with a distribution-matching objective
leads to verbalized probability predictions that are better aligned with the
target preference distribution, with improved calibration and significantly
lower positional bias, all while preserving performance on objective tasks.

</details>


### [10] [Efficient Layer-wise LLM Fine-tuning for Revision Intention Prediction](https://arxiv.org/abs/2510.00268)
*Zhexiong Liu,Diane Litman*

Main category: cs.CL

> 论文提出了一种新的参数高效的微调方案IR-Tuning，能够在无需大量标注数据的情况下，提高大型语言模型在文本修订分类任务上的性能和效率。

<details>
  <summary>Details</summary>

**Motivation:** 虽然大型语言模型在生成任务中表现出色，但它们在文本分类上的潜力尚未充分挖掘，特别是在需要注释数据的文本修订任务中。由于相关标注耗时且成本高，本文旨在通过介绍一种新的微调框架解决这一问题。

**Method:** 本文提出了IR-Tuning，一种分层参数高效微调（PEFT）框架，该框架通过基于梯度范数分布动态选择重要层进行微调，同时冻结冗余层以减少参数量。

**Result:** 该论文提出了一种新的微调框架IR-Tuning，用于解决大型语言模型在文本修订分类任务上面临的挑战。该框架通过动态选择重要层进行参数高效的微调，同时冻结冗余层，从而在减少标注数据需求的同时，实现快速收敛、低GPU内存消耗和在小规模修订语料库上的有效性。实验结果显示，IR-Tuning在不同的文本修订任务上超越了几种分层参数高效微调的基准方法。

**Conclusion:** 实验结果表明，IR-Tuning在各种文本修订任务上效果超越了几种分层PEFT基准方法，同时具有快速收敛、低GPU内存消耗和在少量修订语料上的有效性。

**Abstract:** Large Language Models (LLMs) have shown extraordinary success across various
text generation tasks; however, their potential for simple yet essential text
classification remains underexplored, as LLM pre-training tends to emphasize
generation over classification. While LLMs with instruction tuning can
transform classification into a generation task, they often struggle to
categorize nuanced texts. One such example is text revision, which involves
nuanced edits between pairs of texts. Although simply fine-tuning LLMs for
revision classification seems plausible, it requires a large amount of revision
annotations, which are exceptionally expensive and scarce in the community. To
address this issue, we introduce a plug-and-play layer-wise parameter-efficient
fine-tuning (PEFT) framework, i.e., IR-Tuning, which fine-tunes a subset of
important LLM layers that are dynamically selected based on their gradient norm
distribution, while freezing those of redundant layers. Extensive experiments
suggest that IR-Tuning surpasses several layer-wise PEFT baselines over diverse
text revisions, while achieving fast convergence, low GPU memory consumption,
and effectiveness on small revision corpora.

</details>


### [11] [SafePassage: High-Fidelity Information Extraction with Black Box LLMs](https://arxiv.org/abs/2510.00276)
*Joe Barrow,Raj Patel,Misha Kharkovski,Ben Davies,Ryan Schmitt*

Main category: cs.CL

> 本论文提出了一种新的信息提取方法SafePassage，解决了黑盒大语言模型在信息提取中可能出现的问题，能有效减少幻觉并保持高精度。

<details>
  <summary>Details</summary>

**Motivation:** 黑盒大型语言模型（LLMs）使信息提取变得容易配置，但难以信任。为了防止信息提取不基于文档的情况，引入了"安全段落"的概念。

**Method:** 介绍了一种名为SafePassage的三步流水线，用来生成既基于文档又能与提取信息保持一致的安全段落。该流程包括：(1) LLM提取器，从文档中生成结构化的实体及其上下文；(2) 字符串全局对齐器；(3) 分数模型。

**Result:** 实验结果表明，在信息提取任务中，这三个部分协同使用可以将幻觉率减少高达85%，且不会显著增加误判非幻觉的比例。此外，小规模任务特定样例上微调的变压器编码器在识别不安全段落方面可以优于LLM评分模型。

**Conclusion:** SafePassage流水线和人类提取质量判断之间有高度的一致性，这意味着该流水线可以用于评估LLM。同时，实验表明微调的变压器编码器在标记不安全段落方面表现优于LLM模型。

**Abstract:** Black box large language models (LLMs) make information extraction (IE) easy
to configure, but hard to trust. Unlike traditional information extraction
pipelines, the information "extracted" is not guaranteed to be grounded in the
document. To prevent this, this paper introduces the notion of a "safe
passage": context generated by the LLM that is both grounded in the document
and consistent with the extracted information. This is operationalized via a
three-step pipeline, SafePassage, which consists of: (1) an LLM extractor that
generates structured entities and their contexts from a document, (2) a
string-based global aligner, and (3) a scoring model. Results show that using
these three parts in conjunction reduces hallucinations by up to 85% on
information extraction tasks with minimal risk of flagging non-hallucinations.
High agreement between the SafePassage pipeline and human judgments of
extraction quality mean that the pipeline can be dually used to evaluate LLMs.
Surprisingly, results also show that using a transformer encoder fine-tuned on
a small number of task-specific examples can outperform an LLM scoring model at
flagging unsafe passages. These annotations can be collected in as little as
1-2 hours.

</details>


### [12] [ReEvalMed: Rethinking Medical Report Evaluation by Aligning Metrics with Real-World Clinical Judgment](https://arxiv.org/abs/2510.00280)
*Ruochen Li,Jun Li,Bailiang Jian,Kun Yuan,Youxiang Zhu*

Main category: cs.CL

> 本文揭示了现有自动医学报告生成质量评估方法的缺陷，并提出了一种基于临床的Meta-Evaluation框架，旨在构建更可靠、更具临床相关性的评估方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的自动生成医学报告的评估指标虽然能给出高分，但却无法赢得医生的信任。这种差距暴露了当前评估方法在判断生成报告质量时存在的根本问题。

**Method:** 本文提出了一个基于临床的Meta-Evaluation框架，用于重新思考和评估当前自动医学报告生成质量的评估指标。该框架以临床一致性及关键指标能力等临床标准为依据，旨在提高生成报告的临床可靠性。

**Result:** 研究使用了一个细化的数据集，该数据集中包含了真实的医学报告和改写后的报告对，并标注了错误类型、临床重要性标签和解释。通过系统评估现有指标，揭示了它们在解释临床语义方面的局限性，如无法辨别临床重要的错误、过度惩罚无害的变化以及严重性不同错误的一致性差等问题。

**Conclusion:** 本文通过一个基于临床的Meta-Evaluation框架，明确了改进现有自动生成医学报告的评估方法的方向。

**Abstract:** Automatically generated radiology reports often receive high scores from
existing evaluation metrics but fail to earn clinicians' trust. This gap
reveals fundamental flaws in how current metrics assess the quality of
generated reports. We rethink the design and evaluation of these metrics and
propose a clinically grounded Meta-Evaluation framework. We define clinically
grounded criteria spanning clinical alignment and key metric capabilities,
including discrimination, robustness, and monotonicity. Using a fine-grained
dataset of ground truth and rewritten report pairs annotated with error types,
clinical significance labels, and explanations, we systematically evaluate
existing metrics and reveal their limitations in interpreting clinical
semantics, such as failing to distinguish clinically significant errors,
over-penalizing harmless variations, and lacking consistency across error
severity levels. Our framework offers guidance for building more clinically
reliable evaluation methods.

</details>


### [13] [o-MEGA: Optimized Methods for Explanation Generation and Analysis](https://arxiv.org/abs/2510.00288)
*Ľuboš Kriš,Jaroslav Kopčan,Qiwei Peng,Andrej Ridzik,Marcel Veselý,Martin Tamajka*

Main category: cs.CL

> 本文提出了o-mega工具，优化可解释AI方法，提高事实核查系统在语义匹配领域的透明度和可信度。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于Transformer语言模型在NLP领域的广泛应用带来了关于模型透明度和可信度的重大挑战，本文旨在解决选择最优解释方法的难题。

**Method:** 本文介绍了一种名为o-mega的超参数优化工具，该工具旨在自动识别在语义匹配领域中最有效的可解释AI方法及其配置。

**Result:** 通过使用精心挑选的社交媒体帖子与反驳声明配对的数据集进行评估，该工具展示了在自动化事实核查系统的透明度方面的改善。

**Conclusion:** o-mega能显著提高错误信息检测等关键应用中声明匹配模型的可解释性，从而为更可信和透明的AI系统做出贡献。

**Abstract:** The proliferation of transformer-based language models has revolutionized NLP
domain while simultaneously introduced significant challenges regarding model
transparency and trustworthiness. The complexity of achieving explainable
systems in this domain is evidenced by the extensive array of explanation
methods and evaluation metrics developed by researchers. To address the
challenge of selecting optimal explainability approaches, we present
\textbf{\texttt{o-mega}}, a hyperparameter optimization tool designed to
automatically identify the most effective explainable AI methods and their
configurations within the semantic matching domain. We evaluate o-mega on a
post-claim matching pipeline using a curated dataset of social media posts
paired with refuting claims. Our tool systematically explores different
explainable methods and their hyperparameters, demonstrating improved
transparency in automated fact-checking systems. As a result, such automated
optimization of explanation methods can significantly enhance the
interpretability of claim-matching models in critical applications such as
misinformation detection, contributing to more trustworthy and transparent AI
systems.

</details>


### [14] [CORTEX: Collaborative LLM Agents for High-Stakes Alert Triage](https://arxiv.org/abs/2510.00311)
*Bowen Wei,Yuan Shen Tay,Howard Liu,Jinhao Pan,Kun Luo,Ziwei Zhu,Chris Jordan*

Main category: cs.CL

> 我们提出CORTEX，一个多智能体LLM架构，用于处理企业环境下的高风险警报分类，减少假阳性，并提高了调查质量。

<details>
  <summary>Details</summary>

**Motivation:** 传统的检测管道是脆弱且缺乏上下文的，而最近基于LLM的方法通常依赖单一模型来解释日志、检索上下文以及最终决定告警，这种方法对于嘈杂的企业数据难以处理，并且透明度有限。

**Method:** 我们提出了一种名为CORTEX的多智能体LLM架构，用于高风险的警报分类。该架构包含专门的智能体，它们能协同工作处理实际证据：行为分析智能体检查活动序列，证据收集智能体查询外部系统，推理智能体将发现综合为可审计的决策。

**Result:** 在不同的企业场景中，CORTEX显著减少了假阳性和提高了调查质量，优于最先进的单一智能体LLM方法。

**Conclusion:** CORTEX在处理企业环境中高风险警报分类方面表现出色，降低了假阳性警报的数量，并提高了调查质量。

**Abstract:** Security Operations Centers (SOCs) are overwhelmed by tens of thousands of
daily alerts, with only a small fraction corresponding to genuine attacks. This
overload creates alert fatigue, leading to overlooked threats and analyst
burnout. Classical detection pipelines are brittle and context-poor, while
recent LLM-based approaches typically rely on a single model to interpret logs,
retrieve context, and adjudicate alerts end-to-end -- an approach that
struggles with noisy enterprise data and offers limited transparency. We
propose CORTEX, a multi-agent LLM architecture for high-stakes alert triage in
which specialized agents collaborate over real evidence: a behavior-analysis
agent inspects activity sequences, evidence-gathering agents query external
systems, and a reasoning agent synthesizes findings into an auditable decision.
To support training and evaluation, we release a dataset of fine-grained SOC
investigations from production environments, capturing step-by-step analyst
actions and linked tool outputs. Across diverse enterprise scenarios, CORTEX
substantially reduces false positives and improves investigation quality over
state-of-the-art single-agent LLMs.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [15] [Hybrid Deep Learning for Hyperspectral Single Image Super-Resolution](https://arxiv.org/abs/2510.00033)
*Usman Muhammad,Jorma Laaksonen*

Main category: cs.CV

> 本文介绍了一种新的模型SSUF，它能够提高高光谱图像的空间分辨率和光谱保真度，并通过实验验证了该模型在减少复杂度的同时保持了高性能。

<details>
  <summary>Details</summary>

**Motivation:** 提出SSUF的目的在于解决高光谱单图像超分辨率（SISR）的难题，即在恢复精细空间细节的同时保持广泛的波长范围内的光谱保真度，这限制了常规深度学习模型的性能。

**Method:** 提出了一种新的模块Spectral-Spatial Unmixing Fusion (SSUF)，可以无缝集成到标准的2D卷积架构中，以提升空间分辨率和光谱完整性。SSUF结合了光谱解混与光谱-空间特征提取，指导基于ResNet的卷积神经网络进行改进的重建。另外，还提出了一种自定义的空谱梯度损失函数，整合了均方误差和空谱梯度组件，以鼓励精确重建空间和光谱特征。

**Result:** 实验在三个公开的遥感高光谱数据集上进行，验证了所提出的混合深度学习模型在减少模型复杂性的同时实现了竞争性的性能。

**Conclusion:** 该研究通过提出SSUF模块和自定义的空谱梯度损失函数，改进了高光谱单图像超分辨率的性能，并在这种改进基础上减少了模型的复杂性。

**Abstract:** Hyperspectral single image super-resolution (SISR) is a challenging task due
to the difficulty of restoring fine spatial details while preserving spectral
fidelity across a wide range of wavelengths, which limits the performance of
conventional deep learning models. To address this challenge, we introduce
Spectral-Spatial Unmixing Fusion (SSUF), a novel module that can be seamlessly
integrated into standard 2D convolutional architectures to enhance both spatial
resolution and spectral integrity. The SSUF combines spectral unmixing with
spectral--spatial feature extraction and guides a ResNet-based convolutional
neural network for improved reconstruction. In addition, we propose a custom
Spatial-Spectral Gradient Loss function that integrates mean squared error with
spatial and spectral gradient components, encouraging accurate reconstruction
of both spatial and spectral features. Experiments on three public remote
sensing hyperspectral datasets demonstrate that the proposed hybrid deep
learning model achieves competitive performance while reducing model
complexity.

</details>


### [16] [Review of Hallucination Understanding in Large Language and Vision Models](https://arxiv.org/abs/2510.00034)
*Zhengyi Ho,Siyuan Liang,Dacheng Tao*

Main category: cs.CV

> 研究提供了全面描述图像和文本幻觉现象的框架，揭示幻觉背后的可预测模式和偏差，为开发更强大的生成AI系统提供了解决问题的基石。

<details>
  <summary>Details</summary>

**Motivation:** 幻觉现象不仅会导致传播错误信息，还可能导致财务和运营上的危害，构建一个全面理解幻觉的框架十分有必要。

**Method:** 通过构建一个统一的多层次框架来描述图像和文本幻觉，此框架涵盖了多种应用场景，并采用任务-模式交织的方法来促进对幻觉的综合理解。研究揭示幻觉通常源于数据分布中的可预测模式和继承偏差。

**Result:** 调查研究表明，幻觉往往源自数据分布中的可预测模式和继承偏差，提供了一个坚实的基础来开发更强大且有效的方法解决现实世界生成AI系统的幻觉问题。

**Conclusion:** 本调查提供了一个全面理解幻觉现象的基础，有助于支持在生成AI系统中开发更强大且有效的方法。

**Abstract:** The widespread adoption of large language and vision models in real-world
applications has made urgent the need to address hallucinations -- instances
where models produce incorrect or nonsensical outputs. These errors can
propagate misinformation during deployment, leading to both financial and
operational harm. Although much research has been devoted to mitigating
hallucinations, our understanding of it is still incomplete and fragmented.
Without a coherent understanding of hallucinations, proposed solutions risk
mitigating surface symptoms rather than underlying causes, limiting their
effectiveness and generalizability in deployment. To tackle this gap, we first
present a unified, multi-level framework for characterizing both image and text
hallucinations across diverse applications, aiming to reduce conceptual
fragmentation. We then link these hallucinations to specific mechanisms within
a model's lifecycle, using a task-modality interleaved approach to promote a
more integrated understanding. Our investigations reveal that hallucinations
often stem from predictable patterns in data distributions and inherited
biases. By deepening our understanding, this survey provides a foundation for
developing more robust and effective solutions to hallucinations in real-world
generative AI systems.

</details>


### [17] [On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations](https://arxiv.org/abs/2510.00037)
*Jianing Guo,Zhenhong Wu,Chang Tu,Yiyao Ma,Xiangqi Kong,Zhiqian Liu,Jiaming Ji,Shuning Zhang,Yuanpei Chen,Kai Chen,Xianglong Liu,Qi Dou,Yaodong Yang,Huijie Zhao,Weifeng Lv,Simin Li*

Main category: cs.CV

> 提出了一种新的VLA模型RobustVLA，它在输入和输出方面都获得了更好的鲁棒性，显著提高了在实际应用中的性能。

<details>
  <summary>Details</summary>

**Motivation:** 在VLA模型中，对于实际世界中产生的多模态扰动的鲁棒性对于模型部署至关重要。当前的VLA方法专注于简单的视觉干扰，忽略了在动作、指令、环境和观测中产生的更广泛的多模态扰动。为了提高VLA模型的多模态鲁棒性，提出了RobustVLA。

**Method:** 提出了一种针对VLA输入和输出鲁棒性的方法RobustVLA。在输出鲁棒性方面，进行了离线鲁棒优化，对抗最坏的动作噪声以最大化流匹配目标的不匹配度。这可以看作是对抗训练、标签平滑和离群值惩罚。在输入鲁棒性方面，强加了动作在保持任务语义的输入变化中的一致性。为了处理多个扰动，通过将鲁棒性建模为多臂强盗问题，并应用上置信界算法来自动识别最具危害性的噪声。

**Result:** 在LIBERO实验中，RobustVLA在所有17种扰动下相对于基线分别取得了pi0架构12.6%，OpenVLA架构10.4%的绝对增益，并且推理速度比现有的视觉鲁棒VLA快50.6倍。在四种模态的扰动下，应用于具有有限示范的真实FR5机器人时，绝对增益达到了65.6%。

**Conclusion:** 实验证明，所提出的RobustVLA模型在对抗输入和输出扰动方面增强了鲁棒性，并在综合多个扰动的情况下表现更为优越。

**Abstract:** In Vision-Language-Action (VLA) models, robustness to real-world
perturbations is critical for deployment. Existing methods target simple visual
disturbances, overlooking the broader multi-modal perturbations that arise in
actions, instructions, environments, and observations. Here, we first evaluate
the robustness of mainstream VLAs under 17 perturbations across four
modalities. We find (1) actions as the most fragile modality, (2) Existing
visual-robust VLA do not gain robustness in other modality, and (3) pi0
demonstrates superior robustness with a diffusion-based action head. To build
multi-modal robust VLAs, we propose RobustVLA against perturbations in VLA
inputs and outputs. For output robustness, we perform offline robust
optimization against worst-case action noise that maximizes mismatch in flow
matching objective. This can be seen as adversarial training, label smoothing,
and outlier penalization. For input robustness, we enforce consistent actions
across input variations that preserve task semantics. To account for multiple
perturbations, we formulate robustness as a multi-armed bandit problem and
apply an upper confidence bound algorithm to automatically identify the most
harmful noise. Experiments on LIBERO demonstrate our RobustVLA delivers
absolute gains over baselines of 12.6% on the pi0 backbone and 10.4% on the
OpenVLA backbone across all 17 perturbations, achieving 50.6x faster inference
than existing visual-robust VLAs, and a 10.4% gain under mixed perturbations.
Our RobustVLA is particularly effective on real-world FR5 robot with limited
demonstrations, showing absolute gains by 65.6% under perturbations of four
modalities.

</details>


### [18] [Uncovering Intrinsic Capabilities: A Paradigm for Data Curation in Vision-Language Models](https://arxiv.org/abs/2510.00040)
*Junjie Li,Ziao Wang,Jianghong Ma,Xiaofeng Zhang*

Main category: cs.CV

> CADC框架调整了从任务特定启发式到内在能力分析的策划过程，即使使用减少了95%的数据量，仍能超越全数据训练在多模态基准上的表现。

<details>
  <summary>Details</summary>

**Motivation:** 强大的VLMs模型面临着指令调优困难的问题，特别是在减少指令调优数据集预算时性能下降。传统启发式策略将模型视为黑盒，忽视了指导学习的潜在能力。

**Method:** CADC框架通过基于梯度的学习轨迹在无监督的情况下发现内在能力，并根据影响力估计将训练数据归因于这些能力。通过平衡选择和分阶段排序来策划能力感知的课程，从而将指令调优从任务特定的启发式方法转变为内在能力分析。

**Result:** 使用仅仅5％的原始数据，CADC在多模态基准测试上超越了全数据训练。

**Conclusion:** 结果表明，内在能力是模型学习的基本构建模块，CADC作为指令数据策划的主要范例得到了确立。

**Abstract:** Large vision-language models (VLMs) achieve strong benchmark performance, but
controlling their behavior through instruction tuning remains difficult.
Reducing the budget of instruction tuning dataset often causes regressions, as
heuristic strategies treat models as black boxes and overlook the latent
capabilities that govern learning. We introduce Capability-Attributed Data
Curation (CADC), a framework that shifts curation from task-specific heuristics
to intrinsic capability analysis. CADC discovers intrinsic capabilities in an
unsupervised manner from gradient-based learning trajectories, attributes
training data to these capabilities via influence estimation, and curates
capability-aware curricula through balanced selection and staged sequencing.
This transforms black-box instruction tuning into a controllable,
capability-driven process. With as little as 5% of the original data, CADC
surpasses full-data training on multimodal benchmarks. These results validate
intrinsic capabilities as the fundamental building blocks of model learning and
establish CADC as a principle paradigm for instruction data curation.

</details>


### [19] [Culture In a Frame: C$^3$B as a Comic-Based Benchmark for Multimodal Culturally Awareness](https://arxiv.org/abs/2510.00041)
*Yuchen Song,Andong Chen,Wenxin Zhu,Kehai Chen,Xuefeng Bai,Muyun Yang,Tiejun Zhao*

Main category: cs.CV

> 提出了一种新型的CMC交叉文化基准C$^3$B，以提高MLLMs的文化意识能力评估，通过包含多种文化、多任务和多语言的高难度任务来挑战当前的MLLMs。

<details>
  <summary>Details</summary>

**Motivation:** 当前基准存在任务设计难度提高不足和跨语言任务缺乏的问题。此外，基准往往使用现实世界的图像，每个图像通常只包含一种文化，这使得基准对MLLMs来说相对较容易。基于这些问题，我们提出了C$^3$B。

**Method:** C$^3$B (Comics Cross-Cultural Benchmark)是一种新型的多元文化、多任务和多语言的文化意识能力评估基准。它包含了超过2000张图像和超过18000个问答对，基于三种难度逐渐增加的任务构建，从基本的视觉识别到更高级的文化冲突理解，最后是文化内容生成。

**Result:** 评估了11个开放源码的MLLMs的性能，显示MLLMs与人类的性能之间存在显著差距。这表明C$^3$B对当前的MLLMs提出了实质性的挑战。

**Conclusion:** C$^3$B展示出对于当前MLLMs来说是一个重大的挑战，鼓励未来的研究提高MLLMs的文化意识能力。

**Abstract:** Cultural awareness capabilities has emerged as a critical capability for
Multimodal Large Language Models (MLLMs). However, current benchmarks lack
progressed difficulty in their task design and are deficient in cross-lingual
tasks. Moreover, current benchmarks often use real-world images. Each
real-world image typically contains one culture, making these benchmarks
relatively easy for MLLMs. Based on this, we propose C$^3$B ($\textbf{C}$omics
$\textbf{C}$ross-$\textbf{C}$ultural $\textbf{B}$enchmark), a novel
multicultural, multitask and multilingual cultural awareness capabilities
benchmark. C$^3$B comprises over 2000 images and over 18000 QA pairs,
constructed on three tasks with progressed difficulties, from basic visual
recognition to higher-level cultural conflict understanding, and finally to
cultural content generation. We conducted evaluations on 11 open-source MLLMs,
revealing a significant performance gap between MLLMs and human performance.
The gap demonstrates that C$^3$B poses substantial challenges for current
MLLMs, encouraging future research to advance the cultural awareness
capabilities of MLLMs.

</details>


### [20] [Beyond the Prompt: Gender Bias in Text-to-Image Models, with a Case Study on Hospital Professions](https://arxiv.org/abs/2510.00045)
*Franck Vandewiele,Remi Synave,Samuel Delepoulle,Remi Cozot*

Main category: cs.CV

> 研究发现，这些模型在生成护士时大多呈现女性，外科医生则主要呈现男性。性别刻板印象不仅系统化而且因模型而异，提示词的选择对性别表现有重要影响。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是探讨六种最先进的开源文本到图像模型（HunyuanImage 2.1、HiDream-I1-dev、Qwen-Image、FLUX.1-dev、Stable-Diffusion 3.5 Large、Stable-Diffusion-XL）在性别表现上的系统性职业刻板印象。

**Method:** 本研究通过精心设计的提示词来生成与五个医院相关职业（心脏病学家、医院主管、护士、救护人员、外科医生）相关的100张图像，并结合五个肖像描述符（“”、商务、中性、美学、美丽）。

**Result:** 研究揭示了所有模型生成的护士形象均为女性，而外科医生形象主要是男性。Qwen-Image 和 SDXL 模型表现出严格男性主导，HiDream-I1-dev 结果混杂，FLUX.1-dev 大多数角色趋向女性。不同的肖像描述符也调制了性别平衡，例如商务描述符强化男性形象，而美丽描述符则更有利于女性形象。

**Conclusion:** 研究结果表明，性别偏见在文本到图像模型中既具有系统性又因模型而异。提示词的选择在塑造人口统计结果中发挥着关键作用，进一步强调了开发偏见意识设计、平衡的默认设置以及用户指导的重要性，以防止存在于生成人工智能中的职业刻板印象的强化。

**Abstract:** Text-to-image (TTI) models are increasingly used in professional,
educational, and creative contexts, yet their outputs often embed and amplify
social biases. This paper investigates gender representation in six
state-of-the-art open-weight models: HunyuanImage 2.1, HiDream-I1-dev,
Qwen-Image, FLUX.1-dev, Stable-Diffusion 3.5 Large, and Stable-Diffusion-XL.
Using carefully designed prompts, we generated 100 images for each combination
of five hospital-related professions (cardiologist, hospital director, nurse,
paramedic, surgeon) and five portrait qualifiers ("", corporate, neutral,
aesthetic, beautiful).
  Our analysis reveals systematic occupational stereotypes: all models produced
nurses exclusively as women and surgeons predominantly as men. However,
differences emerge across models: Qwen-Image and SDXL enforce rigid male
dominance, HiDream-I1-dev shows mixed outcomes, and FLUX.1-dev skews female in
most roles. HunyuanImage 2.1 and Stable-Diffusion 3.5 Large also reproduce
gender stereotypes but with varying degrees of sensitivity to prompt
formulation. Portrait qualifiers further modulate gender balance, with terms
like corporate reinforcing male depictions and beautiful favoring female ones.
Sensitivity varies widely: Qwen-Image remains nearly unaffected, while
FLUX.1-dev, SDXL, and SD3.5 show strong prompt dependence.
  These findings demonstrate that gender bias in TTI models is both systematic
and model-specific. Beyond documenting disparities, we argue that prompt
wording plays a critical role in shaping demographic outcomes. The results
underscore the need for bias-aware design, balanced defaults, and user guidance
to prevent the reinforcement of occupational stereotypes in generative AI.

</details>


### [21] [Reinforcement Learning-Based Prompt Template Stealing for Text-to-Image Models](https://arxiv.org/abs/2510.00046)
*Xiaotian Zou*

Main category: cs.CV

> 本研究介绍了RLStealer，一种基于强化学习的提示逆向框架，能够从少量示例图像中恢复提示模板，这揭示了提示交易市场中的安全隐患。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于提示交易市场的兴起及其未被充分研究的安全风险，本研究指出窃取提示本身的威胁，并提出一种有效的解决方案。

**Method:** RLStealer将模板窃取视为一个顺序决策问题，利用多个基于相似性的反馈信号作为奖励函数来探索提示空间。

**Result:** 实验表明，RLStealer达到了最先进的性能，并将攻击成本降低到现有基线要求的13%以下。

**Conclusion:** 本研究强调了提示交易市场中存在的紧急安全威胁，并为开发保护标准奠定了基础。

**Abstract:** Multimodal Large Language Models (MLLMs) have transformed text-to-image
workflows, allowing designers to create novel visual concepts with
unprecedented speed. This progress has given rise to a thriving prompt trading
market, where curated prompts that induce trademark styles are bought and sold.
Although commercially attractive, prompt trading also introduces a largely
unexamined security risk: the prompts themselves can be stolen.
  In this paper, we expose this vulnerability and present RLStealer, a
reinforcement learning based prompt inversion framework that recovers its
template from only a small set of example images. RLStealer treats template
stealing as a sequential decision making problem and employs multiple
similarity based feedback signals as reward functions to effectively explore
the prompt space. Comprehensive experiments on publicly available benchmarks
demonstrate that RLStealer gets state-of-the-art performance while reducing the
total attack cost to under 13% of that required by existing baselines. Our
further analysis confirms that RLStealer can effectively generalize across
different image styles to efficiently steal unseen prompt templates. Our study
highlights an urgent security threat inherent in prompt trading and lays the
groundwork for developing protective standards in the emerging MLLMs
marketplace.

</details>


### [22] [Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations](https://arxiv.org/abs/2510.00047)
*Sihao Ding,Santosh Vasa,Aditi Ramadwar*

Main category: cs.CV

> 我们提出了一个完全自动化的验证程序（EDCT），用于检测视觉语言模型生成的解释是否与模型预测的因果因素一致，结果表明存在显著的不一致，并提供了相应的审核工件。

<details>
  <summary>Details</summary>

**Motivation:** 视觉语言模型（VLMs）经常产生听起来可信但并不能反映预测因果因素的流畅的自然语言解释。这种可信度和一致性之间的差距带来了技术和管理上的风险。为了缓解这些问题，我们提出了 EDCT 方法。

**Method:** 我们提出了解释驱动的反事实测试（EDCT），这是一种完全自动化的验证程序，用于验证目标视觉语言模型（VLMs）。给定一张图片和一个问题，EDCT 会执行以下步骤：(1) 获得模型的答案和自然语言解释（NLE），(2) 将 NLE 解析成可检测的视觉概念，(3) 通过生成性图像修复生成针对性的反事实修改，(4) 使用语言模型辅助分析答案和解释的变化来计算反事实一致性分数（CCS）。

**Result:** 在 120 个策划的 OK-VQA 示例和多个 VLMs 上，EDCT 揭示了显著的一致性差距，并提供了监管部门对齐的审核工件，指出当引用的概念失败因果测试时的情景。

**Conclusion:** EDCT 方法可以有效地检测视觉语言模型解释中的因果一致性问题，并提供监管者的审计工件，有助于理解和改善这些模型的表现。

**Abstract:** Vision-Language Models (VLMs) often produce fluent Natural Language
Explanations (NLEs) that sound convincing but may not reflect the causal
factors driving predictions. This mismatch of plausibility and faithfulness
poses technical and governance risks. We introduce Explanation-Driven
Counterfactual Testing (EDCT), a fully automated verification procedure for a
target VLM that treats the model's own explanation as a falsifiable hypothesis.
Given an image-question pair, EDCT: (1) obtains the model's answer and NLE, (2)
parses the NLE into testable visual concepts, (3) generates targeted
counterfactual edits via generative inpainting, and (4) computes a
Counterfactual Consistency Score (CCS) using LLM-assisted analysis of changes
in both answers and explanations. Across 120 curated OK-VQA examples and
multiple VLMs, EDCT uncovers substantial faithfulness gaps and provides
regulator-aligned audit artifacts indicating when cited concepts fail causal
tests.

</details>


### [23] [HiDe: Rethinking The Zoom-IN method in High Resolution MLLMs via Hierarchical Decoupling](https://arxiv.org/abs/2510.00054)
*Xianjie Liu,Yiman Hu,Yixiong Zou,Liang Wu,Jian Xu,Bo Zheng*

Main category: cs.CV

> 提出了HiDe框架，通过Token-wise Attention Decoupling和Layout-Preserving Decoupling技术解决MLLMs在高分辨率图像上的性能瓶颈问题，并在多个基准测试中取得了最优结果。

<details>
  <summary>Details</summary>

**Motivation:** 虽然MLLMs在视觉理解任务上取得了显著进展，但在高分辨率图像上表现不佳，传统的'放大'策略未能从根本上解决问题。研究提出一个新的问题视角，并设计了解决方案。

**Method:** HiDe框架利用Token-wise Attention Decoupling来识别关键信息并精确对齐目标视觉区域，使用Layout-Preserving Decoupling从背景中分离这些区域，重建一个紧凑的、保留必要空间布局的图像表示。

**Result:** HiDe在V*Bench, HRBench4K和HRBench8K上达到了当前的最佳性能，并优化了内存使用。

**Conclusion:** HiDe框架利用一种无训练的方法成功解决了MLLMs在处理高分辨率图像时的关键问题，并且在性能和资源利用率上表现优异。

**Abstract:** Multimodal Large Language Models (MLLMs) have made significant strides in
visual understanding tasks. However, their performance on high-resolution
images remains suboptimal. While existing approaches often attribute this
limitation to perceptual constraints and argue that MLLMs struggle to recognize
small objects, leading them to use "zoom in" strategies for better detail, our
analysis reveals a different cause: the main issue is not object size, but
rather caused by complex background interference. We systematically analyze
this "zoom in" operation through a series of decoupling experiments and propose
the Hierarchical Decoupling Framework (HiDe), a training-free framework that
uses Token-wise Attention Decoupling (TAD) to decouple the question tokens and
identify the key information tokens, then leverages their attention weights to
achieve precise alignment with the target visual regions. Subsequently, it
employs Layout-Preserving Decoupling (LPD) to decouple these regions from the
background and reconstructs a compact representation that preserves essential
spatial layouts while eliminating background interference. HiDe sets a new SOTA
on V*Bench, HRBench4K, and HRBench8K, boosting Qwen2.5-VL 7B and InternVL3 8B
to SOTA (92.1% and 91.6% on V*Bench), even surpassing RL methods. After
optimization, HiDe uses 75% less memory than the previous training-free
approach. Code is provided in https://github.com/Tennine2077/HiDe.

</details>


### [24] [FSDENet: A Frequency and Spatial Domains based Detail Enhancement Network for Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2510.00059)
*Jiahao Fu,Yinfeng Yu,Liejun Wang*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** To fully leverage spatial information for remote sensing image segmentation
and address semantic edge ambiguities caused by grayscale variations (e.g.,
shadows and low-contrast regions), we propose the Frequency and Spatial Domains
based Detail Enhancement Network (FSDENet). Our framework employs spatial
processing methods to extract rich multi-scale spatial features and
fine-grained semantic details. By effectively integrating global and
frequency-domain information through the Fast Fourier Transform (FFT) in global
mappings, the model's capability to discern global representations under
grayscale variations is significantly strengthened. Additionally, we utilize
Haar wavelet transform to decompose features into high- and low-frequency
components, leveraging their distinct sensitivity to edge information to refine
boundary segmentation. The model achieves dual-domain synergy by integrating
spatial granularity with frequency-domain edge sensitivity, substantially
improving segmentation accuracy in boundary regions and grayscale transition
zones. Comprehensive experimental results demonstrate that FSDENet achieves
state-of-the-art (SOTA) performance on four widely adopted datasets: LoveDA,
Vaihingen, Potsdam, and iSAID.

</details>
