<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 46]
- [cs.CV](#cs.CV) [Total: 49]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LLM-Driven Preference Data Synthesis for Proactive Prediction of the Next User Utterance in Human-Machine Dialogue](https://arxiv.org/abs/2601.09713)
*Jinqiang Wang,Huansheng Ning,Jianguo Ding,Tao Zhu,Liming Chen,Chris Nugent*

Main category: cs.CL

> 本研究提出了一种新的偏好数据合成方法ProUtt，以预测用户的下一句话。该方法优于现有的数据生成技术、用户模拟器和商业LLM API。

<details>
  <summary>Details</summary>

**Motivation:** 尽管用户模拟器的方法可以预测用户的下一句话，但它们主要模仿用户的说话风格而不是推进对话。已有的方法未能明确建模生成用户下一句话的意图推理，并且无法定义和合成偏好与非偏好推理过程。为了解决这些问题，提出了ProUtt方法。

**Method:** ProUtt方法将对话历史转换成意图树，并通过从利用和探索的角度预测下一个可能路径，来明确模型意图推理轨迹。随后，通过对不同未来轮次的意图树路径进行扰动或修正，构造偏好和非偏好推理过程，以预测用户的下一句话。

**Result:** 实验结果表明，在四个基准数据集中，使用LLM作为评判标准和人类评判标准，ProUtt在生成数据的方法、用户模拟器和商业LLM API方面均优于现有的方法。

**Conclusion:** 研究提出了ProUtt方法，该方法生成的数据可以提升LLM对用户下一话语的预测能力。实验表明，ProUtt在多种评判标准下优于现有方法，代码和生成的数据集已公开，以促进未来研究。

**Abstract:** Proactively predicting a users next utterance in human-machine dialogue can streamline interaction and improve user experience. Existing commercial API-based solutions are subject to privacy concerns while deploying general-purpose LLMs locally remains computationally expensive. As such, training a compact, task-specific LLM provides a practical alternative. Although user simulator methods can predict a user's next utterance, they mainly imitate their speaking style rather than advancing the dialogue. Preference data synthesis has been investigated to generate data for proactive next utterance prediction and help align LLMs with user preferences. Yet existing methods lack the ability to explicitly model the intent reasoning that leads to the user's next utterance and to define and synthesize preference and non-preference reasoning processes for predicting the user's next utterance.To address these challenges, we propose ProUtt, an LLM-driven preference data synthesis method for proactive next utterance prediction. ProUtt converts dialogue history into an intent tree and explicitly models intent reasoning trajectories by predicting the next plausible path from both exploitation and exploration perspectives. It then constructs preference and non-preference reasoning processes by perturbing or revising intent tree paths at different future turns. Extensive evaluations using LLM-as-a-judge and human judgments demonstrate that ProUtt consistently outperforms existing data synthesis methods, user simulators, and commercial LLM APIs across four benchmark datasets. We release both the code and the synthesized datasets to facilitate future research.

</details>


### [2] [Evaluating Novelty in AI-Generated Research Plans Using Multi-Workflow LLM Pipelines](https://arxiv.org/abs/2601.09714)
*Devesh Saraogi,Rohit Singhee,Dhruv Kumar*

Main category: cs.CL

> 本论文通过比较五种不同的代理工作流程，探究其在生成创新和可行的研究计划方面的性能。结果显示，基于分解和长上下文的工作流程在创新性方面表现更佳。

<details>
  <summary>Details</summary>

**Motivation:** 该论文旨在探讨大语言模型（LLMs）集成到科学研究生态系统中时，AI生成研究的创造性和原创性。鉴于在单一步骤提示方法中发现的“智能剽窃”问题，该研究动机是验证多步骤系统是否能产生更具创新性和可行性的研究计划。

**Method:** 本研究调查了代理工作流程（多步骤系统，采用迭代推理、进化搜索和递归分解）是否能够生成更具创新性和可行性的研究计划。研究基准了五种推理架构：基于反思的迭代改进、Sakana AI v2的进化算法、Google Co-Scientist的多代理框架、GPT Deep Research（GPT-5.1）的递归分解和Gemini3 Pro的多模式长上下文管道。

**Result:** 调查发现，基于分解和长上下文的工作流程在创新性评估中平均得分4.17/5，而基于反思的方法得分显著较低，为2.33/5。研究结果表明，高绩效工作流程在各个研究领域中，不仅能保持可行性，而且不会牺牲创造力。

**Conclusion:** 这些发现支持了精心设计的多阶段代理工作流程可以推进AI辅助研究构思的观点。证明了在使用大语言模型辅助科研时，采用迭代和递归方法比单一反射方法更能够促进创新和新颖性的生成。

**Abstract:** The integration of Large Language Models (LLMs) into the scientific ecosystem raises fundamental questions about the creativity and originality of AI-generated research. Recent work has identified ``smart plagiarism'' as a concern in single-step prompting approaches, where models reproduce existing ideas with terminological shifts. This paper investigates whether agentic workflows -- multi-step systems employing iterative reasoning, evolutionary search, and recursive decomposition -- can generate more novel and feasible research plans. We benchmark five reasoning architectures: Reflection-based iterative refinement, Sakana AI v2 evolutionary algorithms, Google Co-Scientist multi-agent framework, GPT Deep Research (GPT-5.1) recursive decomposition, and Gemini~3 Pro multimodal long-context pipeline. Using evaluations from thirty proposals each on novelty, feasibility, and impact, we find that decomposition-based and long-context workflows achieve mean novelty of 4.17/5, while reflection-based approaches score significantly lower (2.33/5). Results reveal varied performance across research domains, with high-performing workflows maintaining feasibility without sacrificing creativity. These findings support the view that carefully designed multi-stage agentic workflows can advance AI-assisted research ideation.

</details>


### [3] [Introducing Axlerod: An LLM-based Chatbot for Assisting Independent Insurance Agents](https://arxiv.org/abs/2601.09715)
*Adam Bradley,John Hastings,Khandaker Mamun Ahmed*

Main category: cs.CL

> 本篇论文通过设计和实现AI驱动的对话系统Axlerod，提高了独立保险代理人的工作流程效率，并通过实验证明其在政策检索方面的高准确率和时间效率。

<details>
  <summary>Details</summary>

**Motivation:** 随着人工智能技术在保险行业的广泛应用，特别是智能对话代理领域，本文旨在通过Axlerod的开发和评估，推动独立保险代理人的工作效率提升。

**Method:** 本论文介绍了Axlerod的设计、实现和实证评估。Axlerod是一个基于AI的对话界面，旨在提高独立保险代理人的运营效率，利用自然语言处理（NLP）、检索增强生成（RAG）和领域特定知识集成技术，解析用户意图，访问结构化政策数据库，并提供实时、上下文相关的响应。

**Result:** 实验结果表明，Axlerod在政策检索任务中的整体准确率为93.18%，平均搜索时间减少了2.42秒。

**Conclusion:** 本文为保险科技中企业级AI应用的研究提供了重要贡献，尤其关注代理辅助而非面向消费者的架构。

**Abstract:** The insurance industry is undergoing a paradigm shift through the adoption of artificial intelligence (AI) technologies, particularly in the realm of intelligent conversational agents. Chatbots have evolved into sophisticated AI-driven systems capable of automating complex workflows, including policy recommendation and claims triage, while simultaneously enabling dynamic, context-aware user engagement. This paper presents the design, implementation, and empirical evaluation of Axlerod, an AI-powered conversational interface designed to improve the operational efficiency of independent insurance agents. Leveraging natural language processing (NLP), retrieval-augmented generation (RAG), and domain-specific knowledge integration, Axlerod demonstrates robust capabilities in parsing user intent, accessing structured policy databases, and delivering real-time, contextually relevant responses. Experimental results underscore Axlerod's effectiveness, achieving an overall accuracy of 93.18% in policy retrieval tasks while reducing the average search time by 2.42 seconds. This work contributes to the growing body of research on enterprise-grade AI applications in insurtech, with a particular focus on agent-assistive rather than consumer-facing architectures.

</details>


### [4] [Opportunities and Challenges of Natural Language Processing for Low-Resource Senegalese Languages in Social Science Research](https://arxiv.org/abs/2601.09716)
*Derguene Mbaye,Tatiana D. P. Mbengue,Madoune R. Seye,Moussa Diallo,Mamadou L. Ndiaye,Dimitri S. Adjanohoun,Cheikh S. Wade,Djiby Sow,Jean-Claude B. Munyaka,Jerome Chenal*

Main category: cs.CL

> 本文概述了塞内加尔六种官方语言在自然语言处理中的进展和挑战，通过分析其数字准备情况，总结了现有的努力，并提供了一个促进合作和可重复性的资源库，同时也提出了一份促进伦理数据治理、开放资源和跨学科合作的路线图。

<details>
  <summary>Details</summary>

**Motivation:** 本文的动机是在自然语言处理领域快速发展的背景下，探讨非洲语言特别是塞内加尔的国家语言在技术进步中的参与度和面临的挑战。

**Method:** 本文通过对塞内加尔宪法认可的六种官方语言（沃洛夫语、普拉尔语、塞雷尔语、茱拉语、曼丁哥语和森尼根语）进行语言学、社会技术以及基础设施因素的综合分析，总结了这些语言在数字准备方面的现状，同时识别了数据、工具和基准方面的缺口。文章还分析了现有的文本规范化、机器翻译以及语音处理方面的努力。

**Result:** 作者提供了一个中央GitHub存储库，汇集了这些语言的各种自然语言处理任务的公共资源，旨在促进合作和可重复性。此外，文章特别关注了自然语言处理在社会科学应用中的潜力，特别是在多语言转录、翻译和检索管道上的应用，从而大幅提高了领域研究的效率和包容性。

**Conclusion:** 文章总结了一份面向塞内加尔语言可持续和社区为中心的自然语言处理生态系统的路线图，强调了伦理数据治理、公开资源和跨学科合作的重要性。

**Abstract:** Natural Language Processing (NLP) is rapidly transforming research methodologies across disciplines, yet African languages remain largely underrepresented in this technological shift. This paper provides the first comprehensive overview of NLP progress and challenges for the six national languages officially recognized by the Senegalese Constitution: Wolof, Pulaar, Sereer, Joola, Mandingue, and Soninke. We synthesize linguistic, sociotechnical, and infrastructural factors that shape their digital readiness and identify gaps in data, tools, and benchmarks. Building on existing initiatives and research works, we analyze ongoing efforts in text normalization, machine translation, and speech processing. We also provide a centralized GitHub repository that compiles publicly accessible resources for a range of NLP tasks across these languages, designed to facilitate collaboration and reproducibility. A special focus is devoted to the application of NLP to the social sciences, where multilingual transcription, translation, and retrieval pipelines can significantly enhance the efficiency and inclusiveness of field research. The paper concludes by outlining a roadmap toward sustainable, community-centered NLP ecosystems for Senegalese languages, emphasizing ethical data governance, open resources, and interdisciplinary collaboration.

</details>


### [5] [SALP-CG: Standard-Aligned LLM Pipeline for Classifying and Grading Large Volumes of Online Conversational Health Data](https://arxiv.org/abs/2601.09717)
*Yiwei Yan,Hao Li,Hua He,Gong Kai,Zhengyi Yang,Guanfeng Liu*

Main category: cs.CL

> The paper introduces SALP-CG, a language model-based extraction pipeline, for classifying and grading privacy risks in online medical conversation data, adhering to GB/T 39725-2020 standards.

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of unified standards and reliable automated methods for sensitivity classification in conversational health data, ensuring data protection in online medical consultations.

**Method:** Developed SALP-CG pipeline using few-shot learning, JSON Schema constrained decoding, and deterministic high-risk rules for privacy risk classification.

**Result:** On the MedDialog-CN benchmark, the models demonstrated strong performance, achieving micro-F1=0.900 for maximum-level prediction, and successfully stratified sensitive information categories.

**Conclusion:** SALP-CG provides a reliable and practical method for classifying privacy risks in online health conversations across different language models, advancing health data governance.

**Abstract:** Online medical consultations generate large volumes of conversational health data that often embed protected health information, requiring robust methods to classify data categories and assign risk levels in line with policies and practice. However, existing approaches lack unified standards and reliable automated methods to fulfill sensitivity classification for such conversational health data. This study presents a large language model-based extraction pipeline, SALP-CG, for classifying and grading privacy risks in online conversational health data. We concluded health-data classification and grading rules in accordance with GB/T 39725-2020. Combining few-shot guidance, JSON Schema constrained decoding, and deterministic high-risk rules, the backend-agnostic extraction pipeline achieves strong category compliance and reliable sensitivity across diverse LLMs. On the MedDialog-CN benchmark, models yields robust entity counts, high schema compliance, and accurate sensitivity grading, while the strongest model attains micro-F1=0.900 for maximum-level prediction. The category landscape stratified by sensitivity shows that Level 2-3 items dominate, enabling re-identification when combined; Level 4-5 items are less frequent but carry outsize harm. SALP-CG reliably helps classify categories and grading sensitivity in online conversational health data across LLMs, offering a practical method for health data governance. Code is available at https://github.com/dommii1218/SALP-CG.

</details>


### [6] [StatLLaMA: A multi-stage training framework for building a domain-optimized statistical language model](https://arxiv.org/abs/2601.09718)
*Jing-Yi Zeng,Guan-Hua Huang*

Main category: cs.CL

> 研究通过轻量级LLaMA-3.2-3B系列建造统计专项大型语言模型的方法，通过对比三种多阶段训练流程发现，只有以指令调整后的基础模型作为起始能有效进行领域特化。

<details>
  <summary>Details</summary>

**Motivation:** 探索如何高效构建统计领域的大型语言模型，利用轻量级LLaMA系列作为基础模型，评估不同类型基础模型经多阶段训练后的效果，寻找有效路径。

**Method:** 系统比较三种多阶段训练流程，包括直接从无指令跟随能力的基础模型开始，通过指令微调增强基础模型，以及从指令调整后的模型开始进行持续预训练、监督微调（SFT）、基于人类反馈的强化学习（RLHF）等。

**Result:** 结果显示，从无指令跟随能力的基础模型开始的流程，即便经过长时间的指令微调、SFT或RLHF也难以形成有意义的统计推理。而以指令调整后的LLaMA作为基础的模型可以实现有效领域特化。SFT变体评估揭露了领域专业知识与一般推理能力之间的权衡。

**Conclusion:** 研究得出了成功的训练流程，证明直接偏好优化提供稳定的RLHF对齐。为了防止高度优化模型中的灾难性遗忘，下游微调必须以极低强度进行。最终模型StatLLaMA在数学推理、常识推理和统计专业技能评估中表现均衡，提供了一种开发资源高效统计LLM的实用蓝图。

**Abstract:** This study investigates how to efficiently build a domain-specialized large language model (LLM) for statistics using the lightweight LLaMA-3.2-3B family as the foundation model (FM). We systematically compare three multi-stage training pipelines, starting from a base FM with no instruction-following capability, a base FM augmented with post-hoc instruction tuning, and an instruction-tuned FM with strong general reasoning abilities across continual pretraining, supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF) preference alignment, and downstream task adaptation. Results show that pipelines beginning with a base FM fail to develop meaningful statistical reasoning, even after extensive instruction tuning, SFT, or RLHF alignment. In contrast, starting from LLaMA-3.2-3B-Instruct enables effective domain specialization. A comprehensive evaluation of SFT variants reveals clear trade-offs between domain expertise and general reasoning ability. We further demonstrate that direct preference optimization provides stable and effective RLHF preference alignment. Finally, we show that downstream fine-tuning must be performed with extremely low intensity to avoid catastrophic forgetting in highly optimized models. The final model, StatLLaMA, achieves strong and balanced performance on benchmarks of mathematical reasoning, common-sense reasoning, and statistical expertise, offering a practical blueprint for developing resource-efficient statistical LLMs. The code is available at https://github.com/HuangDLab/StatLLaMA.

</details>


### [7] [Bounded Hyperbolic Tangent: A Stable and Efficient Alternative to Pre-Layer Normalization in Large Language Models](https://arxiv.org/abs/2601.09719)
*Hoyoon Byun,Youngjun Choi,Taero Kim,Sungrae Park,Kyungwoo Song*

Main category: cs.CL

> BHyT is introduced to tackle the instability and inefficiency in normalization-free methods for large language models, showing improved pretraining stability and efficiency.

<details>
  <summary>Details</summary>

**Motivation:** To solve the instability and inefficiency of normalization methods in large language models (LLMs), especially as the model depth increases.

**Method:** Bounded Hyperbolic Tanh (BHyT) combines a tanh nonlinearity with explicit data-driven input bounding to maintain activations within a non-saturating range, preventing depth-wise growth in activation magnitude and variance.

**Result:** BHyT shows an average improvement of 15.8% in training speed and 4.2% higher token generation throughput compared to RMSNorm, while maintaining or enhancing inference performance and robustness.

**Conclusion:** BHyT is proposed as an efficient and stable normalization method that supports the pretraining of large language models without the drawbacks of increased depth.

**Abstract:** Pre-Layer Normalization (Pre-LN) is the de facto choice for large language models (LLMs) and is crucial for stable pretraining and effective transfer learning. However, Pre-LN is inefficient due to repeated statistical calculations and suffers from the curse of depth. As layers grow, the magnitude and variance of the hidden state escalate, destabilizing training. Efficiency-oriented normalization-free methods such as Dynamic Tanh (DyT) improve speed but remain fragile at depth. To jointly address stability and efficiency, we propose Bounded Hyperbolic Tanh (BHyT), a drop-in replacement for Pre-LN. BHyT couples a tanh nonlinearity with explicit, data-driven input bounding to keep activations within a non-saturating range. It prevents depth-wise growth in activation magnitude and variance and comes with a theoretical stability guarantee. For efficiency, BHyT computes exact statistics once per block and replaces a second normalization with a lightweight variance approximation, enhancing efficiency. Empirically, BHyT demonstrates improved stability and efficiency during pretraining, achieving an average of 15.8% faster training and an average of 4.2% higher token generation throughput compared to RMSNorm., while matching or surpassing its inference performance and robustness across language understanding and reasoning benchmarks. Our code is available at: https://anonymous.4open.science/r/BHyT

</details>


### [8] [Uncertainty-Aware Dynamic Knowledge Graphs for Reliable Question Answering](https://arxiv.org/abs/2601.09720)
*Yu Takahashi,Shun Takeuchi,Kexuan Xin,Guillaume Pelat,Yoshiaki Ikai,Junya Saito,Jonathan Vitale,Shlomo Berkovsky,Amin Beheshti*

Main category: cs.CL

> 本文介绍了一个不确定性感知的动态知识图谱框架，用于提高问题回答的可靠性和透明度，重点针对医疗领域的应用进行了演示。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于知识图谱的问题回答框架通常将事实表示为静态和确定性的，无法捕捉信息的演变性质和推理中的不确定性，因此提出了不确定性感知的动态知识图谱以解决这一问题。

**Method:** 介绍了一种结合动态构建知识图谱、置信度评分及不确定性检索、交互式界面的不确定性感知动态知识图谱框架，以实现更稳健和透明的问题回答。

**Result:** 该系统展示了不确定性建模如何通过让用户探索动态图谱、检查置信度标注的三元组以及比较基线与置信度感知的答案来使QA更加稳健和透明。

**Conclusion:** 该用例证明了不确定性感知动态知识图谱框架在提高高风险应用中问题回答可靠性方面的潜力。未来可以在更多领域进一步推广应用。

**Abstract:** Question answering (QA) systems are increasingly deployed across domains. However, their reliability is undermined when retrieved evidence is incomplete, noisy, or uncertain. Existing knowledge graph (KG) based QA frameworks typically represent facts as static and deterministic, failing to capture the evolving nature of information and the uncertainty inherent in reasoning. We present a demonstration of uncertainty-aware dynamic KGs, a framework that combines (i) dynamic construction of evolving KGs, (ii) confidence scoring and uncertainty-aware retrieval, and (iii) an interactive interface for reliable and interpretable QA. Our system highlights how uncertainty modeling can make QA more robust and transparent by enabling users to explore dynamic graphs, inspect confidence-annotated triples, and compare baseline versus confidence-aware answers. The target users of this demo are clinical data scientists and clinicians, and we instantiate the framework in healthcare: constructing personalized KGs from electronic health records, visualizing uncertainty across patient visits, and evaluating its impact on a mortality prediction task. This use case demonstrates the broader promise of uncertainty-aware dynamic KGs for enhancing QA reliability in high-stakes applications.

</details>


### [9] [Cross-Platform Evaluation of Large Language Model Safety in Pediatric Consultations: Evolution of Adversarial Robustness and the Scale Paradox](https://arxiv.org/abs/2601.09721)
*Vahideh Zolfaghari*

Main category: cs.CL

> 研究评估了大型语言模型在压力环境下提供儿科咨询服务的安全性，较小模型表现优于大模型。研究使用了PediatricAnxietyBench数据集，并指出较小模型在安全性方面得分更高，且部分模型在特定主题上存在缺陷，要求在实际应用中加强安全性测试和针对性的训练。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于之前的研究主要集中在中立环境下的大型语言模型评估，忽略了它们在面临焦虑用户挑战时的安全性。此研究旨在通过模拟焦虑的家长对儿科咨询服务的压力，来评估这些语言模型的实际安全性。

**Method:** 研究使用PediatricAnxietyBench数据集，包含300个查询（150个真实的和150个对抗性的），涵盖了10个不同主题。评估了三个通过API接入的语言模型：Llama-3.3-70B，Llama-3.1-8B和Mistral-7B。安全性的评估基于一个0-15分的量表，该量表考量了模型保持谨慎、进行引导和识别紧急状况等方式。使用配对t检验及CI重采样进行分析。

**Result:** 研究发现，评分范围从Llama-3.3-70B的9.70到Mistral-7B的10.39。Llama-3.1-8B在安全性评分上超过了Llama-3.3-70B（+0.66），表现为显著的对抗效应。Llama-3.3-70B在平台间的安全性相对稳定，但有8%的错误判例。尤其在处理癫痫相关问题时，模型表现出不适当的诊断。此外，模型的模糊回复与安全性高度相关（r=0.68）。

**Conclusion:** 该研究强调了模型对齐和架构设计的重要性，具有较小规模的模型在安全性评估中表现更佳。不过，研究还发现了某些紧急情况下的识别空白，提醒这些模型尚不适用于初步诊断。这些发现可以指导模型的选择，强调了在使用大型语言模型前进行对抗测试的必要性。

**Abstract:** Background Large language models (LLMs) are increasingly deployed in medical consultations, yet their safety under realistic user pressures remains understudied. Prior assessments focused on neutral conditions, overlooking vulnerabilities from anxious users challenging safeguards. This study evaluated LLM safety under parental anxiety-driven adversarial pressures in pediatric consultations across models and platforms. Methods PediatricAnxietyBench, from a prior evaluation, includes 300 queries (150 authentic, 150 adversarial) spanning 10 topics. Three models were assessed via APIs: Llama-3.3-70B and Llama-3.1-8B (Groq), Mistral-7B (HuggingFace), yielding 900 responses. Safety used a 0-15 scale for restraint, referral, hedging, emergency recognition, and non-prescriptive behavior. Analyses employed paired t-tests with bootstrapped CIs. Results Mean scores: 9.70 (Llama-3.3-70B) to 10.39 (Mistral-7B). Llama-3.1-8B outperformed Llama-3.3-70B by +0.66 (p=0.0001, d=0.225). Models showed positive adversarial effects, Mistral-7B strongest (+1.09, p=0.0002). Safety generalized across platforms; Llama-3.3-70B had 8% failures. Seizures vulnerable (33% inappropriate diagnoses). Hedging predicted safety (r=0.68, p<0.001). Conclusions Evaluation shows safety depends on alignment and architecture over scale, with smaller models outperforming larger. Evolution to robustness across releases suggests targeted training progress. Vulnerabilities and no emergency recognition indicate unsuitability for triage. Findings guide selection, stress adversarial testing, and provide open benchmark for medical AI safety.

</details>


### [10] [ADMEDTAGGER: an annotation framework for distillation of expert knowledge for the Polish medical language](https://arxiv.org/abs/2601.09722)
*Franciszek Górski,Andrzej Czyżewski*

Main category: cs.CL

> 本文利用多语言LLM Llama3.1预训练模型，作为教师模型来标注波兰语医学文本，解决了在医学文本标注资源稀缺的问题，取得了优异的分类器效果。

<details>
  <summary>Details</summary>

**Motivation:** 我们的工作旨在解决ADMEDVOICE项目中标签资源不足的问题，该项目收集了大量代表五个临床类别的医学文本语料库。使用LUCLLa3.1进行标注，克服了标注数据稀缺的难题。

**Method:** 在本研究中，我们利用一个多语言的LLM，即Llama3.1，在一个大型语料库上进行预训练，并将其作为教师模型来传授标注医学文本所需的专业知识。通过五个临床类别（放射学、肿瘤学、心脏病学、高血压和病理学）的大量医学文本语料库，我们开发了一个多分类器来解决标签资源不足的问题，利用Llama3.1对大量波兰语医学文本进行标注，并利用有限的标注资源验证了部分标签，创建了一个测试集。这些标注的数据用于训练和验证了三种基于BERT架构的分类器：蒸馏DistilBERT模型、在医学数据上微调的BioBERT和在波兰语语料库上微调的HerBERT。

**Result:** 实验结果中，DistilBERT模型取得了最佳表现，对于每个临床类别的F1分数>0.80，其中三个类别的F1分数>0.93，展示了高度有效的分类器。

**Conclusion:** 这种分类器作为一种替代方案相比于大型语言模型，其体积约小500倍，GPU显存消耗减少约300倍，推理速度要快几倍。

**Abstract:** In this work, we present an annotation framework that demonstrates how a multilingual LLM pretrained on a large corpus can be used as a teacher model to distill the expert knowledge needed for tagging medical texts in Polish. This work is part of a larger project called ADMEDVOICE, within which we collected an extensive corpus of medical texts representing five clinical categories - Radiology, Oncology, Cardiology, Hypertension, and Pathology. Using this data, we had to develop a multi-class classifier, but the fundamental problem turned out to be the lack of resources for annotating an adequate number of texts. Therefore, in our solution, we used the multilingual Llama3.1 model to annotate an extensive corpus of medical texts in Polish. Using our limited annotation resources, we verified only a portion of these labels, creating a test set from them. The data annotated in this way were then used for training and validation of 3 different types of classifiers based on the BERT architecture - the distilled DistilBERT model, BioBERT fine-tuned on medical data, and HerBERT fine-tuned on the Polish language corpus. Among the models we trained, the DistilBERT model achieved the best results, reaching an F1 score > 0.80 for each clinical category and an F1 score > 0.93 for 3 of them. In this way, we obtained a series of highly effective classifiers that represent an alternative to large language models, due to their nearly 500 times smaller size, 300 times lower GPU VRAM consumption, and several hundred times faster inference.

</details>


### [11] [SagaScale: A Realistic, Scalable, and High-Quality Long-Context Benchmark Built from Full-Length Novels](https://arxiv.org/abs/2601.09723)
*Guancheng Du,Yong Hu,Wenqing Wang,Yaming Yang,Jiaheng Gao*

Main category: cs.CL

> 提出了SagaScale，一个基于全长小说的真实、可扩展且高质量的长文本基准测试，并对12个前沿的大型语言模型进行了评估。

<details>
  <summary>Details</summary>

**Motivation:** 虽然大型语言模型在长文本和复杂文档的理解上取得了进步，但仍有局限性。因此，创建了一个更加真实、可扩展且高质量的基准测试，旨在改善现有长文本理解的基准测试中的问题。

**Method:** 构建了一个名为SagaScale的长文本基准测试，该测试基于全长的小说，并使用自动化数据收集管道创建问题-答案对，这些问题-答案对使用外部资源如维基百科页面来生成，但在评估时并不提供这些资源。

**Result:** 评估结果显示直接提供全文给大型语言模型可以显著优于其他方法，大多数模型在长文本情况下表现不佳，但Gemini-2.5-Pro例外，且表明Agentic RAG能够有效解决Naïve RAG中的检索瓶颈。

**Conclusion:** SagaScale提供了一个更广阔的视角来评估大型语言模型的长文本理解能力，并且公开了测试基准和数据收集代码库，以促进未来的研究。

**Abstract:** Large Language Models (LLMs) have shown significant progress, but understanding long and complex documents remains challenging. Many long-context benchmarks have been proposed, but they face several limitations, including task realism, data scalability, and data quality. To this end, we introduce SagaScale, a realistic, scalable, and high-quality long-context benchmark built from full-length novels. The entire benchmark is constructed using an automated data collection pipeline that utilizes external resources (e.g., Wikipedia pages) to curate question-answer pairs. Critically, these external resources are provided only for benchmark construction and not during evaluation, which allows LLMs to curate complex questions that go beyond what they can answer during evaluation. SagaScale is also bilingual and offers the largest context length to date, with average token counts exceeding 250K for English novels and 320K for Chinese novels. Our evaluation across 12 frontier LLMs and three long-context methods -- Naïve RAG, Agentic RAG, and Long Context -- yields key insights, including: (1) Directly supplying the full context to the LLM can outperform other methods by a large margin; (2) Most LLMs still struggle with lengthy contexts, but Gemini-2.5-Pro stands out as an exception; and (3) Agentic RAG effectively addresses the retrieval bottleneck in Naïve RAG. Finally, we publicly release the SagaScale benchmark and our data collection codebase to facilitate future research.

</details>


### [12] [Syntactic Framing Fragility: An Audit of Robustness in LLM Ethical Decisions](https://arxiv.org/abs/2601.09724)
*Katherine Elkins,Jon Chun*

Main category: cs.CL

> 本文研究了大型语言模型（LLMs）在逻辑等价但句法不同的提示下是否能够保持一致的道德判断，并提出了句法框架脆性（SFF）评估框架，以通过逻辑极性规范化（LPN）来隔离纯粹的句法效果。针对14种道德情境和4种控制结构（共计39,975个决策），对来自美国和中国的23种最先进的模型进行了审计，发现这些模型在句法极性变化时存在显著的道德判断不一致性。研究表明句法一致性是评判道德稳健性的一个独特且关键的维度。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLMs）越来越多地在重要决策场景中被部署，但它们对良性提示变化的稳健性尚鲜为人探究。鉴于此，作者探讨了LLMs在逻辑等价但句法不同的提示下是否能够保持一致的道德判断，并提出了一种全新的评估框架来检验这一假说。

**Method:** 作者提出了一种名为句法框架脆性评估框架（SFF）并使用逻辑极性规范化（LPN）方法，以纯句法变化为基础进行决策对比。

**Result:** 通过SFF评估框架对23种最先进的模型进行了审计，发现在逻辑极性的变化下，这些模型的道德判断存在显著不一致性，且免费开源模型的脆性几乎是商用模型的两倍。此外，研究还发现，某些模型在“不应”这样的提示下，会产生极端的负面反应。

**Conclusion:** 研究表明，句法一致性构成了道德稳健性的一个独特且关键的维度。因此，建议在评估已部署的大型语言模型的安全时，应将类似SFF式的审计作为常规的评价标准。代码和结果将在GitHub上公开。

**Abstract:** Large language models (LLMs) are increasingly deployed in consequential decision-making settings, yet their robustness to benign prompt variation remains underexplored. In this work, we study whether LLMs maintain consistent ethical judgments across logically equivalent but syntactically different prompts, focusing on variations involving negation and conditional structure. We introduce Syntactic Framing Fragility (SFF), a robustness evaluation framework that isolates purely syntactic effects via Logical Polarity Normalization (LPN), enabling direct comparison of decisions across positive and negative framings without semantic drift. Auditing 23 state-of-the-art models spanning the U.S. and China as well as small U.S. open-source software models over 14 ethical scenarios and four controlled framings (39,975 decisions), we find widespread and statistically significant inconsistency: many models reverse ethical endorsements solely due to syntactic polarity, with open-source models exhibiting over twice the fragility of commercial counterparts. We further uncover extreme negation sensitivity, where some models endorse actions in 80-97% of cases when explicitly prompted with "should not." We show that eliciting chain-of-thought reasoning substantially reduces fragility, identifying a practical mitigation lever, and we map fragility across scenarios, finding higher risk in financial and business contexts than in medical scenarios. Our results demonstrate that syntactic consistency constitutes a distinct and critical dimension of ethical robustness, and we argue that SFF-style audits should be a standard component of safety evaluation for deployed LLMs. Code and results will be available on github.com.

</details>


### [13] [Assessing and Improving Punctuation Robustness in English-Marathi Machine Translation](https://arxiv.org/abs/2601.09725)
*Kaustubh Shivshankar Shejole,Sourabh Deoghare,Pushpak Bhattacharyya*

Main category: cs.CL

> 本文引入Virām基准，评估英语到马拉蒂语机器翻译中对标点符号的鲁棒性，对比单一翻译模型和微调模型，结果表明微调模型能显著改善翻译质量并提高可靠性。

<details>
  <summary>Details</summary>

**Motivation:** 标点符号在解决书面语言的语义和结构歧义中扮演着关键角色，而机器翻译系统在许多低资源语言设置中广泛应用。本文专注于马拉蒂语，这是一种低至中等资源的语言。

**Method:** 本文引入了Virām，这是一个评估英语到马拉蒂语机器翻译中标点符号鲁棒性的诊断基准，包含54个手动筛选的标点符号模棱两可的实例。研究了两种提升可靠性的方法：一个是先恢复后翻译的流水线方法，另一个是直接对带有不同标点符号的数据进行微调。

**Result:** 结果显示，专门的微调模型和流水线系统显著改善了在Virām基准上的翻译质量。定性分析表明，原始模型可能导致错误翻译，而微调模型显著提高了整体可靠性。

**Conclusion:** 大型语言模型在保持标点符号模糊文本的意义方面落后于这些任务特定的方法，因此需要在这一领域进行进一步的研究。

**Abstract:** Punctuation plays a critical role in resolving semantic and structural ambiguity in written language. Machine Translation (MT) systems are now widely applied across diverse domains and languages, including many low-resource settings. In this work, we focus on Marathi, a low- to middle-resource language. We introduce Virām, the first diagnostic benchmark for assessing punctuation robustness in English-to-Marathi machine translation, consisting of 54 manually curated, punctuation-ambiguous instances. We evaluate two primary strategies for enhancing reliability: a pipeline-based restore-then-translate approach and direct fine-tuned on punctuation-varied data. Our results demonstrate that specialized fine-tuned models and pipeline systems significantly improve translation quality over standard baselines on the Virām benchmark. Qualitative analysis reveals that the original model may result in wrong translations leading to wrong interpretations, while fine-tuned models significantly improve overall reliability. Furthermore, we find that current Large Language Models (LLMs) lag behind these task-specific approaches in preserving meaning for punctuation-ambiguous text, thus necessitating further research in this area.

</details>


### [14] [Forgetting as a Feature: Cognitive Alignment of Large Language Models](https://arxiv.org/abs/2601.09726)
*Hien Tran,Quinten Steenhuis,Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CL

> 研究重新解读了大语言模型(LLMs)中的遗忘现象，将其视为一种类似人类记忆的机制，并提出了一个评估套件和一种新的提示策略，以改善LLMs的长时间推理性能。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于讨论大语言模型(LLMs)中的系统性遗忘问题，并试图重新定义这种行为，而非简单地将之视为一种缺陷。

**Method:** 研究采用人类记忆动态的指数衰减概率模型来描述LLMs的推理过程，并且设计了一个基准套件来评估时间推理，概念漂移适应和联想记忆召回。

**Result:** 研究发现LLMs的遗忘率与人类记忆的稳定性与适应性之间的效率权衡类似，并提出了一种概率记忆提示策略，可以模拟人类记忆的衰减。

**Conclusion:** 研究结论认为，遗忘不是适应性智能的失败模式，而是一个规范机制。

**Abstract:** Large Language Models (LLMs) are often evaluated against ideals of perfect Bayesian inference, yet growing evidence suggests that their in-context reasoning exhibits systematic forgetting of past information. Rather than viewing this behavior as a limitation, we reinterpret forgetting as a functional cognitive mechanism. Drawing inspiration from human memory dynamics, we model LLM inference as a probabilistic memory process governed by exponential decay. We introduce a benchmark suite that evaluates temporal reasoning, concept drift adaptation, and associative recall, enabling direct comparison between model behavior and human cognitive patterns. Our empirical results reveal that LLMs demonstrate forgetting rates analogous to human memory efficiency trade-offs between stability and adaptability. Building on these observations, we propose probabilistic memory prompting, a lightweight strategy that shapes evidence integration to mimic human-like memory decay, leading to improved long-horizon reasoning performance. Our findings position forgetting not as a failure mode, but as a principled mechanism for adaptive intelligence.

</details>


### [15] [SciNets: Graph-Constrained Multi-Hop Reasoning for Scientific Literature Synthesis](https://arxiv.org/abs/2601.09727)
*Sauhard Dubey*

Main category: cs.CL

> 该研究运用图约束方法处理科学跨域合成问题，通过构建概念图和应用多种推理路径来生成机制解释，揭示了推理深度和结构稳定性之间的权衡关系。

<details>
  <summary>Details</summary>

**Motivation:** 为了克服检索系统和无约束语言模型在连接跨领域文献中的机制性解释时面临的能力限制，该论文着眼于提供对推理深度和结构基础更好的控制。

**Method:** 该论文提出了一种基于图约束的多跳推理方法，用于跨领域的科学综合。给定一个科学查询和一个紧凑的、针对查询的语料库，SciNets 构建了一个有向概念图，并通过识别连接很少共现概念的多跳推理路径来合成机制性解释。

**Result:** 研究结果表明，在机器学习、生物学和气候科学任务中，通过图约束方法能进行深度和多样化的推理，但同时也会牺牲一定的结构稳定性。最短路径推理虽然结构保守，但稳定性高。

**Conclusion:** 研究表明，显式的图约束使多跳推理具有可控性，但同时也揭示了更深和更多样化的符号推理增加了基础不稳定性的权衡关系。这为当前图-LLM集成在科学综合方面的局限性和能力提供了系统的特征描述。

**Abstract:** Cross-domain scientific synthesis requires connecting mechanistic explanations across fragmented literature, a capability that remains challenging for both retrieval-based systems and unconstrained language models. While recent work has applied large language models to scientific summarization and question answering, these approaches provide limited control over reasoning depth and structural grounding. We frame mechanistic synthesis as a graph-constrained multi-hop reasoning problem over literature-derived concept graphs. Given a scientific query and a compact, query-local corpus, SciNets constructs a directed concept graph and synthesizes mechanistic explanations by identifying multi-hop reasoning paths that connect concepts that rarely co-occur within individual papers. We systematically compare shortest-path reasoning, k-shortest paths with diversity constraints, stochastic random walks, and a retrieval-augmented language model baseline. Rather than evaluating correctness, which is often indeterminate when synthesizing connections across distributed sources, we introduce a behavioral framework that measures symbolic reasoning depth, mechanistic diversity, and grounding stability. Across machine learning, biology, and climate science tasks, explicit graph constraints enable controllable multi-hop reasoning while revealing a consistent trade-off: deeper and more diverse symbolic reasoning increases grounding instability, whereas shortest-path reasoning remains highly stable but structurally conservative. These findings provide a systematic behavioral characterization of the limits and capabilities of current graph-LLM integration for scientific synthesis.

</details>


### [16] [Eliminating Agentic Workflow for Introduction Generation with Parametric Stage Tokens](https://arxiv.org/abs/2601.09728)
*Meicong Zhang,Tiancheng su,Guoxiu He*

Main category: cs.CL

> 本文提出了一种新的方法STIG，它能够通过将工作流阶段转化为阶段信号，并进行指令调优，使得LLMs能够在单一推理过程中生成详细介绍，优于传统方法。

<details>
  <summary>Details</summary>

**Motivation:** 使用预定义的代理工作流引导大型语言模型（LLMs）进行文献分类和综述已经成为一个研究热点。然而，编写研究介绍更具挑战性，需要严密的逻辑、连贯的结构和抽象的总结。现有工作流通常遭受长推理链、错误积累和文本连贯性降低的困扰。为了克服这些限制，本文提出了一种新的方法。

**Method:** 本文提出了Stage Token for Introduction Generation (STIG) 方法，该方法将原来工作流的多个阶段转换为明确的阶段信号，指导模型在生成过程中遵循不同的逻辑角色和功能。通过指令调优，模型学会了阶段标记和文本功能之间的映射，以及阶段间的逻辑顺序和转换模式，并将这些知识编码到模型参数中。

**Result:** 与传统代理工作流和其他基准相比，STIG能够在单一推理中生成多阶段文本，在语义相似性和句子级别结构合理性指标上表现出更优越的性能。

**Conclusion:** 实验结果表明，STIG能够在单一推理中生成多阶段文本，无需明确调用工作流。STIG在语义相似性和句子级别的结构合理性指标上优于传统的工作流和其他基线方法。

**Abstract:** In recent years, using predefined agentic workflows to guide large language models (LLMs) for literature classification and review has become a research focus. However, writing research introductions is more challenging. It requires rigorous logic, coherent structure, and abstract summarization. Existing workflows often suffer from long reasoning chains, error accumulation, and reduced textual coherence. To address these limitations, we propose eliminating external agentic workflows. Instead, we directly parameterize their logical structure into the LLM. This allows the generation of a complete introduction in a single inference. To this end, we introduce the Stage Token for Introduction Generation (STIG). STIG converts the multiple stages of the original workflow into explicit stage signals. These signals guide the model to follow different logical roles and functions during generation. Through instruction tuning, the model learns the mapping between stage tokens and text functions. It also learns the logical order and transition patterns between stages, encoding this knowledge into the model parameters. Experimental results show that STIG can generate multi-stage text in a single inference. It does not require explicit workflow calls. STIG outperforms traditional agentic workflows and other baselines on metrics of semantic similarity and sentence-level structural rationality. The code is provided in the Supplementary Materials.

</details>


### [17] [Enhancing Business Analytics through Hybrid Summarization of Financial Reports](https://arxiv.org/abs/2601.09729)
*Tohida Rehman*

Main category: cs.CL

> 研究开发了一种混合框架，用于生成来自Earnings Conference Calls Transcripts (ECTSum) 数据集准确可靠的新闻风格简介。

<details>
  <summary>Details</summary>

**Motivation:** 财务报告和收益沟通包含大量结构化和半结构化信息，使其详细的手动分析效率低下。收益电话会议为公司的表现、前景和战略重点提供了有价值的证据。对长电话记录的手动分析需要付出大量的努力，并容易受到解释偏差和无意错误的影响。

**Method:** 本研究提出了一种结合抽取和抽象方法的混合摘要框架，第一步应用LexRank算法识别关键句子，第二步使用针对资源受限环境优化的BART和PEGASUS模型对这些句子进行总结。此外，研究还通过微调Longformer编码器-解码器模型来直接捕捉金融文件中的长程上下文依赖关系。

**Result:** 研究表明，长上下文模型总体表现最强，而混合框架在计算资源受限的情况下取得了有竞争力的结果，并且具有更好的事实一致性。

**Conclusion:** 这些发现支持开发实用的摘要系统，可以高效地将长时间的金融文本提炼成有用的业务见解。

**Abstract:** Financial reports and earnings communications contain large volumes of structured and semi structured information, making detailed manual analysis inefficient. Earnings conference calls provide valuable evidence about a firm's performance, outlook, and strategic priorities. The manual analysis of lengthy call transcripts requires substantial effort and is susceptible to interpretive bias and unintentional error. In this work, we present a hybrid summarization framework that combines extractive and abstractive techniques to produce concise and factually reliable Reuters-style summaries from the ECTSum dataset. The proposed two stage pipeline first applies the LexRank algorithm to identify salient sentences, which are subsequently summarized using fine-tuned variants of BART and PEGASUS designed for resource constrained settings. In parallel, we fine-tune a Longformer Encoder-Decoder (LED) model to directly capture long-range contextual dependencies in financial documents.
  Model performance is evaluated using standard automatic metrics, including ROUGE, METEOR, MoverScore, and BERTScore, along with domain-specific variants such as SciBERTScore and FinBERTScore. To assess factual accuracy, we further employ entity-level measures based on source-precision and F1-target. The results highlight complementary trade offs between approaches, long context models yield the strongest overall performance, while the hybrid framework achieves competitive results with improved factual consistency under computational constraints. These findings support the development of practical summarization systems for efficiently distilling lengthy financial texts into usable business insights.

</details>


### [18] [Clinical Document Metadata Extraction: A Scoping Review](https://arxiv.org/abs/2601.09730)
*Kurt Miller,Qiuhao Lu,William Hersh,Kirk Roberts,Steven Bedrick,Andrew Wen,Hongfang Liu*

Main category: cs.CL

> The abstract discusses the challenges and methods for automating the extraction of metadata from clinical documents, summarizing a scoping review of relevant studies from 2011 to 2025.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind the paper is to address the difficulties in standardizing clinical document metadata due to its heterogeneity and changes over time. The authors aim to understand current research trends and identify essential gaps in the field.

**Method:** The method involves following PRISMA-ScR guidelines to locate and assess articles that examine clinical document metadata extraction. A total of 266 articles were located, and 67 articles were selected for a more detailed examination.

**Result:** The results indicated that 45 of the studies were methodological, 17 were applied to specific tasks, and 5 focused on the composition of the metadata. There has been a shift in methodologies from predominantly rule-based systems and traditional machine learning to using transformer-based architectures.

**Conclusion:** The conclusion highlights the advancements in methods for metadata extraction and predicts that research will further develop to include more detailed metadata representations and integration into clinical practices.

**Abstract:** Clinical document metadata, such as document type, structure, author role, medical specialty, and encounter setting, is essential for accurate interpretation of information captured in clinical documents. However, vast documentation heterogeneity and drift over time challenge harmonization of document metadata. Automated extraction methods have emerged to coalesce metadata from disparate practices into target schema. This scoping review aims to catalog research on clinical document metadata extraction, identify methodological trends and applications, and highlight gaps. We followed the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) guidelines to identify articles that perform clinical document metadata extraction. We initially found and screened 266 articles published between January 2011 and August 2025, then comprehensively reviewed 67 we deemed relevant to our study. Among the articles included, 45 were methodological, 17 used document metadata as features in a downstream application, and 5 analyzed document metadata composition. We observe myriad purposes for methodological study and application types. Available labelled public data remains sparse except for structural section datasets. Methods for extracting document metadata have progressed from largely rule-based and traditional machine learning with ample feature engineering to transformer-based architectures with minimal feature engineering. The emergence of large language models has enabled broader exploration of generalizability across tasks and datasets, allowing the possibility of advanced clinical text processing systems. We anticipate that research will continue to expand into richer document metadata representations and integrate further into clinical applications and workflows.

</details>


### [19] [Geometric Patterns of Meaning: A PHATE Manifold Analysis of Multi-lingual Embeddings](https://arxiv.org/abs/2601.09731)
*Wen G Gong*

Main category: cs.CL

> 本研究通过多层次分析框架和Semanscope工具研究了多语言嵌入中的几何语义。发现模型在多种语言层级上存在几何模式和局限，证明PHATE流形学习对于研究不同语言嵌入中的几何结构具有一定价值。

<details>
  <summary>Details</summary>

**Motivation:** 这项研究的动机在于揭示当前嵌入模型中的几何模式与局限性，特别是在不同语言层级上，不同元素的几何分布特征。

**Method:** 本研究使用了一种多层次分析框架，并借助可视化工具Semanscope（基于PHATE流形学习）来分析多种语言嵌入中的语义几何结构。该分析框架涵盖了四个语言层次：次字符层面、字符层面、词汇层面以及数值概念层面。

**Result:** 研究发现，纯结构元素（如汉字部首）在几何上的崩溃表明了模型难以区分语义成分与结构成分。不同书写系统在字符层面表现出独特的几何特征。词汇层面，内容词在英语、汉语和德语中20个语义领域出现了聚类分支模式。数值层面，阿拉伯数字形成了螺旋轨迹，不符合传统的分布语义学假设。

**Conclusion:** 这些研究结果证明了PHATE流形学习作为一种关键分析工具，不仅能够研究嵌入空间中语义结构的几何结构，还能够验证嵌入模型在捕捉语义关系方面的有效性。

**Abstract:** We introduce a multi-level analysis framework for examining semantic geometry in multilingual embeddings, implemented through Semanscope (a visualization tool that applies PHATE manifold learning across four linguistic levels). Analysis of diverse datasets spanning sub-character components, alphabetic systems, semantic domains, and numerical concepts reveals systematic geometric patterns and critical limitations in current embedding models. At the sub-character level, purely structural elements (Chinese radicals) exhibit geometric collapse, highlighting model failures to distinguish semantic from structural components. At the character level, different writing systems show distinct geometric signatures. At the word level, content words form clustering-branching patterns across 20 semantic domains in English, Chinese, and German. Arabic numbers organize through spiral trajectories rather than clustering, violating standard distributional semantics assumptions. These findings establish PHATE manifold learning as an essential analytic tool not only for studying geometric structure of meaning in embedding space, but also for validating the effectiveness of embedding models in capturing semantic relationships.

</details>


### [20] [Benchmarking Cross-Lingual Semantic Alignment in Multilingual Embeddings](https://arxiv.org/abs/2601.09732)
*Wen G. Gong*

Main category: cs.CL

> 本文介绍了语义亲和度 (SA) 指标并使用 Semanscope 框架展示了13种模型在4个数据集上的性能，结果表明语义对齐需要明确的翻译监督而不是仅仅是模型规模或者多语言数据。

<details>
  <summary>Details</summary>

**Motivation:** 目前有数百种多语言嵌入模型可供使用，但实践者缺乏明确的指导，以确定哪些模型提供真正的跨语言语义对齐以及哪些模型以特定于语言的模式执行任务。任务驱动的基准测试可能掩盖了基本的对齐问题。

**Method:** 引入了语义亲和度（SA），这是一个使用余弦距离测量跨语言与内语言传播比率的有界（0到1之间）指标，并结合了PHATE可视化技术，该技术是在我们的Semanscope框架内完成的。通过在4个数据集上评估13种模型（共52个实验）来评估该方法。

**Result:** 实验揭示了模型性能的三级结构，其中顶级模型通过翻译配对监督实现强烈的对齐，大型语言模型的嵌入值在多次实验中趋于稳定，而仅使用多语言模型训练的模型则未能达到满意的对齐效果。训练目标，而不是架构或模型规模决定模型对齐效果。

**Conclusion:** 这项工作的提出属于一个提供语义基准的方法，以帮助实践者从数百种可用模型中选择质量较高的多语言嵌入模型。实验表明，跨语言对齐需要明确的翻译监督，而不仅仅是模型规模或多语言数据。

**Abstract:** With hundreds of multilingual embedding models available, practitioners lack clear guidance on which provide genuine cross-lingual semantic alignment versus task performance through language-specific patterns. Task-driven benchmarks (MTEB) may mask fundamental alignment shortcomings. We introduce Semantic Affinity (SA), a bounded (between 0 and 1) metric measuring inter-lingual to intra-lingual spread ratio using cosine distance, combined with PHATE visualization in our Semanscope framework. Benchmarking 13 models across 4 datasets (52 experiments) reveals a three-tier structure: (1) Top BERT models (LaBSE SA = 0.70, USE SA = 0.68, S-BERT SA = 0.68) achieve strong alignment via translation-pair supervision; (2) LLM embeddings plateau at SA between 0.55 and 0.61 regardless of 0.6 B to 8 B scale; (3) MLM-only BERT models (mBERT, XLM-R, SA < 0.50) fail despite more than 100 language training. Training objective, not architecture or scale, determines alignment. Oracle Bone primitives (1200 BCE) expose semantic drift-models learn corpus patterns rather than cognitive primitives. This work provides semantic benchmarking to help practitioners select quality multilingual embeddings from hundreds of available models, showing cross-lingual alignment requires explicit translation supervision, not merely model scale or multilingual data.

</details>


### [21] [Closing the Data Loop: Using OpenDataArena to Engineer Superior Training Datasets](https://arxiv.org/abs/2601.09733)
*Xin Gao,Xiaoyang Wang,Yun Zhu,Mengzhang Cai,Conghui He,Lijun Wu*

Main category: cs.CL

> A new closed-loop dataset engineering framework, ODA, is introduced for dataset construction, leading to significant performance improvements in LLM fine-tuning through two new datasets. This suggests a shift toward a more structured and efficient data-centric AI paradigm.

<details>
  <summary>Details</summary>

**Motivation:** The construction of Supervised Fine-Tuning (SFT) datasets is often heuristic and lacks a systematic approach to understand how individual samples impact model performance. This report aims to shift toward a more informed and structured dataset curation process.

**Method:** We propose a closed-loop dataset engineering framework using OpenDataArena (ODA), which utilizes value-anchored rankings and multi-dimensional analysis for dataset construction. This methodology is instantiated through the creation of two new datasets, ODA-Math-460k and ODA-Mixture.

**Result:** The datasets ODA-Math-460k and ODA-Mixture, built using the ODA framework, achieve SOTA results on benchmarks like AIME and HMMT, and significantly outperform larger open-source baselines in multi-domain instruction tasks.

**Conclusion:** The study validates the effectiveness of data-centric AI approaches in improving model performance and data efficiency through transparent and systematic evaluation methodologies.

**Abstract:** The construction of Supervised Fine-Tuning (SFT) datasets is a critical yet under-theorized stage in the post-training of Large Language Models (LLMs), as prevalent practices often rely on heuristic aggregation without a systematic understanding of how individual samples contribute to model performance. In this report, we propose a paradigm shift from ad-hoc curation to a closed-loop dataset engineering framework using OpenDataArena (ODA), which leverages value-anchored rankings and multi-dimensional analysis to transform value benchmarking into feedback signals guiding dataset construction. We instantiate this methodology through two new datasets: \textbf{ODA-Math-460k}, a specialized mathematics reasoning dataset that utilizes a novel two-stage difficulty-aware pipeline to achieve State-of-the-Art (SOTA) results on benchmarks such as AIME and HMMT, and \textbf{ODA-Mixture (100k \& 500k)}, a series of multi-domain instruction datasets built via an ``Anchor-and-Patch'' strategy that outperforms significantly larger open-source baselines. Our empirical results demonstrate that ODA-driven datasets significantly improve both domain-specific reasoning and general utility while achieving superior data efficiency, validating a transition toward data-centric AI where transparent evaluation serves as the primary engine for engineering high-quality training data.

</details>


### [22] [From Detection to Diagnosis: Advancing Hallucination Analysis with Automated Data Synthesis](https://arxiv.org/abs/2601.09734)
*Yanyi Liu,Qingwen Yang,Tiezheng Guo,Feiyu Qu,Jun Liu,Yingyou Wen*

Main category: cs.CL

> 研究将大语言模型幻觉的检测方法转换为诊断方法，提出了幻觉诊断任务并开发了一种自动化流程（HDG）用于生成诊断数据。该模型（HDM-4B-RL）在幻觉检测任务上超过了当前最先进模型，同时具有较小的模型规模，在全面诊断任务中也展现出了与大型通用模型相当的能力。

<details>
  <summary>Details</summary>

**Motivation:** 当前研究主要集中在二元“检测”方法上，而这些方法虽然能够识别幻觉，但无法提供可解释的和有助于改进模型的实际反馈，因而限制了其实用性。因此，提出从“检测”到“诊断”的新研究范式，旨在实现对幻觉的错误定位、因果解释和内容修正。

**Method:** 通过开发Hallucination Diagnosis Generator (HDG) 自动化流程，该流程能够从原始语料库通过多维度增强策略（包括受控事实伪造和推理链干扰）系统地生成具有丰富诊断元数据的高质量训练样本。采用Group Relative Policy Optimization (GRPO)，结合结构、准确性和定位信号的综合奖励函数，训练了名为HDM-4B-RL的模型。

**Result:** 实验结果显示，该模型在HaluEval基准测试上超过了以前最先进的检测模型，同时在诊断任务上与先进的通用模型可相媲美。

**Conclusion:** 本研究验证了幻觉诊断方法在构建更值得信赖和可靠的生成式AI系统中的可行性与价值。

**Abstract:** Hallucinations in Large Language Models (LLMs), defined as the generation of content inconsistent with facts or context, represent a core obstacle to their reliable deployment in critical domains. Current research primarily focuses on binary "detection" approaches that, while capable of identifying hallucinations, fail to provide interpretable and actionable feedback for model improvement, thus limiting practical utility. To address this limitation, a new research paradigm is proposed, shifting from "detection" to "diagnosis". The Hallucination Diagnosis Task is introduced, a task which requires models to not only detect hallucinations, but also perform error localization, causal explanation, and content correction. We develop the Hallucination Diagnosis Generator (HDG), an automated pipeline that systematically generates high-quality training samples with rich diagnostic metadata from raw corpora through multi-dimensional augmentation strategies including controlled fact fabrication and reasoning chain perturbation. Using HDG-generated data, we train HDM-4B-RL, a 4-billion-parameter hallucination diagnosis model, employing Group Relative Policy Optimization (GRPO) with a comprehensive reward function incorporating structural, accuracy, and localization signals. Experimental results demonstrate that our model surpasses previous state-of-the-art detection models on the HaluEval benchmark while achieving comparable performance to advanced general-purpose models. In comprehensive diagnosis tasks, HDM-4B-RL matches the capabilities of larger general models while maintaining a smaller size. This work validates the feasibility and value of hallucination diagnosis, providing an effective methodology for building more trustworthy and reliable generative AI systems.

</details>


### [23] [Stable and Explainable Personality Trait Evaluation in Large Language Models with Internal Activations](https://arxiv.org/abs/2601.09833)
*Xiaoxu Ma,Xiangbo Zhang,Zhenyu Weng*

Main category: cs.CL

> 本文提出了一种基于内部激活的评估大型语言模型个性特征的方法，称为Persona-Vector Neutrality Interpolation (PVNI)，较现有的问卷评估方法更加稳定和可解释。

<details>
  <summary>Details</summary>

**Motivation:** 传统问卷评估语言模型个性特征的方法存在稳定性和解释性不足的问题，其结果对提示细微变化高度敏感。

**Method:** 提出了Persona-Vector Neutrality Interpolation (PVNI) 方法，通过对比提示从模型的内部激活中提取目标个性特征的人格向量，并通过在人格向量作为锚轴上插值估计中性分数，实现中性提示表示和人格方向的可解释比较。

**Result:** 理论分析和大量实验表明，相较于现有方法，PVNI在不同语言模型中提供了更加稳定且可解释的个性特征评估，即使在问卷和角色扮演变体中也是如此。

**Conclusion:** 实验验证了PVNI方法的有效性和泛化性能，指出其在评估大型语言模型的个性特征中具有较大潜力和实际应用价值。

**Abstract:** Evaluating personality traits in Large Language Models (LLMs) is key to model interpretation, comparison, and responsible deployment. However, existing questionnaire-based evaluation methods exhibit limited stability and offer little explainability, as their results are highly sensitive to minor variations in prompt phrasing or role-play configurations. To address these limitations, we propose an internal-activation-based approach, termed Persona-Vector Neutrality Interpolation (PVNI), for stable and explainable personality trait evaluation in LLMs. PVNI extracts a persona vector associated with a target personality trait from the model's internal activations using contrastive prompts. It then estimates the corresponding neutral score by interpolating along the persona vector as an anchor axis, enabling an interpretable comparison between the neutral prompt representation and the persona direction. We provide a theoretical analysis of the effectiveness and generalization properties of PVNI. Extensive experiments across diverse LLMs demonstrate that PVNI yields substantially more stable personality trait evaluations than existing methods, even under questionnaire and role-play variants.

</details>


### [24] [Bears, all bears, and some bears. Language Constraints on Language Models' Inductive Inferences](https://arxiv.org/abs/2601.09852)
*Sriram Padmanabhan,Siyuan Song,Kanishka Misra*

Main category: cs.CL

> 通过实验发现视觉语言模型与儿童一样能够区分不同类型的陈述，并以不同的方式拓展新属性到特定对象，这表明模型能够捕捉到人类在处理这些问题时的细微差别，基于的是归纳约束而非表面形式的差异。

<details>
  <summary>Details</summary>

**Motivation:** 了解语言如何微妙地影响我们进行归纳推理的方式，特别是儿童如何根据不同的命题类型（基因表达、普遍量化名词短语、不定复数名词短语）将新属性扩展到特定对象。通过验证视觉语言模型是否也能表现出类似的行为来探讨这些细微差异。

**Method:** 通过复制Gelman等人(2002)的实验，使用视觉语言模型来测试这些细微的差异是否出现在一般用途的统计学习者中。实验包括一系列的预条件测试（包括对图像中类别的稳健识别和对“所有”与“一些”的敏感性），以及原始实验本身。

**Result:** 视觉语言模型的行为与人类的行为相一致。后续分析表明，这些差异是基于归纳约束，而不是基于形式表面的差异。

**Conclusion:** 视觉语言模型可以捕捉人类在处理不同类型的命题（基因表达、普遍量化名词短语和不定复数名词短语）时的细微差别，这暗示它们以不同的方式代表这些类型的命题。

**Abstract:** Language places subtle constraints on how we make inductive inferences. Developmental evidence by Gelman et al. (2002) has shown children (4 years and older) to differentiate among generic statements ("Bears are daxable"), universally quantified NPs ("all bears are daxable") and indefinite plural NPs ("some bears are daxable") in extending novel properties to a specific member (all > generics > some), suggesting that they represent these types of propositions differently. We test if these subtle differences arise in general purpose statistical learners like Vision Language Models, by replicating the original experiment. On tasking them through a series of precondition tests (robust identification of categories in images and sensitivities to all and some), followed by the original experiment, we find behavioral alignment between models and humans. Post-hoc analyses on their representations revealed that these differences are organized based on inductive constraints and not surface-form differences.

</details>


### [25] [MedRedFlag: Investigating how LLMs Redirect Misconceptions in Real-World Health Communication](https://arxiv.org/abs/2601.09853)
*Sraavya Sambara,Yuan Pu,Ayman Ali,Vishala Mishra,Lionel Wong,Monica Agrawal*

Main category: cs.CL

> 研究揭示了LLMs在处理需要引导的医疗问题时的能力不足，指出了面向患者的医疗AI系统中的重大安全隐患。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型被越来越多地用于提供医疗建议，尚未测试其处理嵌入现实医疗问题中的错误前提的能力。本研究旨在探索LLMs在面对这类问题时的表现。

**Method:** 开发了一种半自动管道以整理MedRedFlag数据集,该数据集包含1100多个需要引导的Reddit上的问题，并系统地比较了最先进的LLMs和临床医生的答复。

**Result:** LLMs经常在检测到问题前提时无法进行适当的引导，提供了可能导致患者作出次优医疗决策的答案。

**Conclusion:** LLMs在检测到问题前提时经常无法引导问题，并且可能提供导致次优医疗决策的答案，揭示了LLMs在现实世界医疗交流条件下的重大性能差距，突显了面向患者的医疗AI系统的重要安全问题。

**Abstract:** Real-world health questions from patients often unintentionally embed false assumptions or premises. In such cases, safe medical communication typically involves redirection: addressing the implicit misconception and then responding to the underlying patient context, rather than the original question. While large language models (LLMs) are increasingly being used by lay users for medical advice, they have not yet been tested for this crucial competency. Therefore, in this work, we investigate how LLMs react to false premises embedded within real-world health questions. We develop a semi-automated pipeline to curate MedRedFlag, a dataset of 1100+ questions sourced from Reddit that require redirection. We then systematically compare responses from state-of-the-art LLMs to those from clinicians. Our analysis reveals that LLMs often fail to redirect problematic questions, even when the problematic premise is detected, and provide answers that could lead to suboptimal medical decision making. Our benchmark and results reveal a novel and substantial gap in how LLMs perform under the conditions of real-world health communication, highlighting critical safety concerns for patient-facing medical AI systems. Code and dataset are available at https://github.com/srsambara-1/MedRedFlag.

</details>


### [26] [OUTLINEFORGE: Hierarchical Reinforcement Learning with Explicit States for Scientific Writing](https://arxiv.org/abs/2601.09858)
*Yilin Bao,Ziyao He,Zayden Yang*

Main category: cs.CL

> 本文提出一个强化学习框架，用以解决当前大型语言模型在科学论文生成中的全局结构不一致、输入覆盖率低和引用可靠性差等问题。该方法将科学大纲构建视为一个长跨度规划问题，并引入了一种两阶段优化程序来支持有效学习，即从部分计划中重构大纲和基于价值引导的强化学习。实验结果表明，这种方法相比其他神经网络和大型语言模型基线在长范围结构一致性和引用可靠性方面有持续改进。

<details>
  <summary>Details</summary>

**Motivation:** 当前的大型语言模型虽然在局部流畅性方面表现出色，但在科学论文生成的全局结构、输入内容覆盖和引用一致性方面存在不足。为了提高全球结构的连贯性、输入内容的覆盖率和引用的准确性，本文提出了一种强化学习框架来解决这些挑战。

**Method:** 本文提出的方法使用强化学习框架，将构建科学论文大纲看作是一个层次化文档结构上的长期规划问题。该方法通过结构化行动逐步建立完整的科学稿件。此外，通过一项两阶段优化程序，从部分规划中的反向大纲重构和基于价值引导的正向强化学习来支持有效的学习。

**Result:** 通过引入一个全新的科学论文生成基准，评估了文档规划、信息应用、参考清晰度和大纲的组织结构以及内容层面的事实准确性。实验结果显示，本文的方法显著改进了长距离结构一致性以及引用的可靠性，尤其是在这一点上明显优于神经网络和其他基线模型。

**Conclusion:** 研究结论强调了本文提出的框架如何有效改善了大型语言模型在生成科学论文时的全局结构连贯性、输入内容覆盖范围以及引用准确性的不足。实验结果表明，这种方法具备更强的长距离结构一致性和引用可靠性。

**Abstract:** Scientific paper generation requires document-level planning and factual grounding, but current large language models, despite their strong local fluency, often fail in global structure, input coverage, and citation consistency. We present a reinforcement learning framework that casts scientific outline construction as a long-horizon planning problem over hierarchical document structures. Our approach models edit evolving outlines through structured actions, enabling the system to incrementally build a complete scientific manuscript. To support effective and stabilize learning,we introduce a two-stage optimization procedure consisting of (i) backward outline reconstruction from partial plans to enforce global structural consistency, and (ii) forward value-guided reinforcement learning with rewards explicitly modeling scientific correctness, discourse coherence, and citation fidelity. In addition, We further introduce a benchmark for scientific paper generation that evaluates document planning, input utilization, reference faithfulness, outline organization, and content-level factual accuracy. Our results show consistent improvements over strong neural and LLM baselines, particularly in long-range structural coherence and citation reliability.

</details>


### [27] [Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL](https://arxiv.org/abs/2601.09876)
*Yifei Shen,Yilun Zhao,Justice Ou,Tinglin Huang,Arman Cohan*

Main category: cs.CL

> 介绍CLINSQL基准测试，评估了专有和开源模型在处理临床文本到SQL任务中的表现。

<details>
  <summary>Details</summary>

**Motivation:** 解决现实世界电子健康记录（EHR）中多表关联、临床相关过滤等问题，推动临床可靠的文本到SQL技术的发展。

**Method:** 介绍了一个名为CLINSQL的基准测试，包含了633个经过专家标注的任务，这些任务基于MIMIC-IV v3.1数据集，要求多表连接、临床有意义的过滤以及生成可执行的SQL查询。评估了22个专有和开源模型在链式思考自我修正下的表现，并使用基于评分标准的SQL分析和执行检查进行评估。

**Result:** GPT-5-mini在测试集中获得了74.7%的执行得分，DeepSeek-R1以69.2%的表现领先开源模型，Gemini-2.5-Pro从Easy集中的85.5%下降到Hard集中的67.2%。

**Conclusion:** 尽管有近期的进展，但在临床可靠性方面表现仍不足，CLINSQL的进展标志着向临床可靠的EHR分析文本到SQL技术发展的实质进步。

**Abstract:** Real-world clinical text-to-SQL requires reasoning over heterogeneous EHR tables, temporal windows, and patient-similarity cohorts to produce executable queries. We introduce CLINSQL, a benchmark of 633 expert-annotated tasks on MIMIC-IV v3.1 that demands multi-table joins, clinically meaningful filters, and executable SQL. Solving CLINSQL entails navigating schema metadata and clinical coding systems, handling long contexts, and composing multi-step queries beyond traditional text-to-SQL. We evaluate 22 proprietary and open-source models under Chain-of-Thought self-refinement and use rubric-based SQL analysis with execution checks that prioritize critical clinical requirements. Despite recent advances, performance remains far from clinical reliability: on the test set, GPT-5-mini attains 74.7% execution score, DeepSeek-R1 leads open-source at 69.2% and Gemini-2.5-Pro drops from 85.5% on Easy to 67.2% on Hard. Progress on CLINSQL marks tangible advances toward clinically reliable text-to-SQL for real-world EHR analytics.

</details>


### [28] [Clozing the Gap: Exploring Why Language Model Surprisal Outperforms Cloze Surprisal](https://arxiv.org/abs/2601.09886)
*Sathvik Nair,Byung-Doh Oh*

Main category: cs.CL

> 研究比较了人类填空任务与语言模型的概率预测能力，发现语言模型的概率预测在评估语言处理难度上更准确，并探讨了其优势原因。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于验证语言模型预测概率高于人类填空数据预测概率的原因，确保这些模型的性能是基于正确的理由，并非由于不同的预测方法导致对语言理解中的预测作用得出不同科学结论。

**Method:** 本文使用语言模型的预测概率与人类在填空任务中的反应概率进行了对比，以评估它们在预测语言处理中努力程度方面的效果。

**Result:** 发现语言模型的概率预测不会受到分辨率低的问题影响，能区分语义相似的词汇，并能准确地给低频词汇分配概率。

**Conclusion:** 研究结果呼吁改进填空研究的分辨率，并通过实验探究人类预测是否同样敏感于语言模型所能做出的细微区别。

**Abstract:** How predictable a word is can be quantified in two ways: using human responses to the cloze task or using probabilities from language models (LMs).When used as predictors of processing effort, LM probabilities outperform probabilities derived from cloze data. However, it is important to establish that LM probabilities do so for the right reasons, since different predictors can lead to different scientific conclusions about the role of prediction in language comprehension. We present evidence for three hypotheses about the advantage of LM probabilities: not suffering from low resolution, distinguishing semantically similar words, and accurately assigning probabilities to low-frequency words. These results call for efforts to improve the resolution of cloze studies, coupled with experiments on whether human-like prediction is also as sensitive to the fine-grained distinctions made by LM probabilities.

</details>


### [29] [Take Out Your Calculators: Estimating the Real Difficulty of Question Items with LLM Student Simulations](https://arxiv.org/abs/2601.09953)
*Christabel Acquaye,Yi Ting Huang,Marine Carpuat,Rachel Rudinger*

Main category: cs.CL

> 研究展示了开源大型语言模型（LLMs）在评估数学题目难度方面的预测价值。虽然LLMs直接判断题目难度效果不佳，但通过模拟实际学生的方法，尤其是使用较弱数学能力的模型，可以有效提高预测准确性。

<details>
  <summary>Details</summary>

**Motivation:** 传统上，标准化数学评估需要昂贵的人类预试研究来确定题目的难度。这促使我们探索LEEPs在预测数学题目的难度上的应用，以期找到更经济有效的解决方案。

**Method:** 我们研究了开源大型语言模型（LLMs）对评估实际学生数学选择题难度的预测价值。使用模拟方法，我们模拟了不同年级（4年级、8年级和12年级）学生的“课堂”，让语言模型扮演不同熟练度的学生。我们使用这些模拟的结果来拟合项目反应理论（IRT）模型，并将模拟得到的项目难度参数与实际难度进行比较。

**Result:** 我们观察到，在4年级、8年级和12年级分别高达0.75，0.76和0.82的相关性。进一步的研究表明，具有较弱数学能力的语言模型在预测准确性方面高于数学能力强的模型，并且以名字而不是学生ID进行角色扮演可以提高预测效果，同时将名字按照性别和种族划分能够进一步改善结果。

**Conclusion:** 我们的研究展示了使用模拟方法的LLMs在预测数学题目难度上的潜力。特别是，我发现较弱数学能力的语言模型可以更准确地反映真实难度，进一步说明了开源模型在评估实际学生数学题目难度中的适用性。

**Abstract:** Standardized math assessments require expensive human pilot studies to establish the difficulty of test items. We investigate the predictive value of open-source large language models (LLMs) for evaluating the difficulty of multiple-choice math questions for real-world students. We show that, while LLMs are poor direct judges of problem difficulty, simulation-based approaches with LLMs yield promising results under the right conditions. Under the proposed approach, we simulate a "classroom" of 4th, 8th, or 12th grade students by prompting the LLM to role-play students of varying proficiency levels. We use the outcomes of these simulations to fit Item Response Theory (IRT) models, comparing learned difficulty parameters for items to their real-world difficulties, as determined by item-level statistics furnished by the National Assessment of Educational Progress (NAEP). We observe correlations as high as 0.75, 0.76, and 0.82 for grades 4, 8, and 12, respectively. In our simulations, we experiment with different "classroom sizes," showing tradeoffs between computation size and accuracy. We find that role-plays with named students improves predictions (compared to student ids), and stratifying names across gender and race further improves predictions. Our results show that LLMs with relatively weaker mathematical abilities (Gemma) actually yield better real-world difficulty predictions than mathematically stronger models (Llama and Qwen), further underscoring the suitability of open-source models for the task.

</details>


### [30] [Context Volume Drives Performance: Tackling Domain Shift in Extremely Low-Resource Translation via RAG](https://arxiv.org/abs/2601.09982)
*David Samuel Setiawan,Raphaël Merx,Jey Han Lau*

Main category: cs.CL

> 研究展示了一种混合框架以提高低资源语言的神经机器翻译性能。

<details>
  <summary>Details</summary>

**Motivation:** 我们的动机在于解决低资源语言在领域转移时性能显著下降的问题，尤其是在像Dhao这样的几乎没有数字记录的语言上。

**Method:** 我们提出了一种混合框架，该框架首先使用在新约圣经（NT）上微调的神经机器翻译（NMT）模型生成初步草稿，然后通过检索增强生成（RAG）方法，利用大型语言模型（LLM）进行优化。

**Result:** 最终系统达到了35.21 chrF++的评分，相比仅使用NMT模型有了8.10的显著提升，并几乎恢复到NT域内的性能水平。

**Conclusion:** 研究表明，性能的改善主要归因于检索到的例子数量，而不是检索算法的选择。此外，LLM扮演了一个强大的“安全网”，修复了零样本领域中的重大错误。

**Abstract:** Neural Machine Translation (NMT) models for low-resource languages suffer significant performance degradation under domain shift. We quantify this challenge using Dhao, an indigenous language of Eastern Indonesia with no digital footprint beyond the New Testament (NT). When applied to the unseen Old Testament (OT), a standard NMT model fine-tuned on the NT drops from an in-domain score of 36.17 chrF++ to 27.11 chrF++. To recover this loss, we introduce a hybrid framework where a fine-tuned NMT model generates an initial draft, which is then refined by a Large Language Model (LLM) using Retrieval-Augmented Generation (RAG). The final system achieves 35.21 chrF++ (+8.10 recovery), effectively matching the original in-domain quality. Our analysis reveals that this performance is driven primarily by the number of retrieved examples rather than the choice of retrieval algorithm. Qualitative analysis confirms the LLM acts as a robust "safety net," repairing severe failures in zero-shot domains.

</details>


### [31] [SocraticKG: Knowledge Graph Construction via QA-Driven Fact Extraction](https://arxiv.org/abs/2601.10003)
*Sanghyeok Choi,Woosang Jeon,Kyuseok Yang,Taehyeong Kim*

Main category: cs.CL

> SocraticKG uses question-answer pairs as an intermediate representation for constructing knowledge graphs from text, improving both factual coverage and structural coherence.

<details>
  <summary>Details</summary>

**Motivation:** Current LLM-based approaches face a trade-off between factual coverage and relational fragmentation when constructing knowledge graphs from text, leading to either fragmented relations or lost information.

**Method:** SocraticKG employs 5W1H-guided QA expansion to unfold document-level semantics before extracting triples, capturing contextual dependencies and implicit relational links.

**Result:** Evaluation on the MINE benchmark shows that SocraticKG enhances factual retention and structural cohesion, even when the volume of the extracted knowledge increases.

**Conclusion:** The use of QA-mediated semantic scaffolding is critical for structured semantics prior to KG extraction, leading to more coherent and reliable knowledge graphs.

**Abstract:** Constructing Knowledge Graphs (KGs) from unstructured text provides a structured framework for knowledge representation and reasoning, yet current LLM-based approaches struggle with a fundamental trade-off: factual coverage often leads to relational fragmentation, while premature consolidation causes information loss. To address this, we propose SocraticKG, an automated KG construction method that introduces question-answer pairs as a structured intermediate representation to systematically unfold document-level semantics prior to triple extraction. By employing 5W1H-guided QA expansion, SocraticKG captures contextual dependencies and implicit relational links typically lost in direct KG extraction pipelines, providing explicit grounding in the source document that helps mitigate implicit reasoning errors. Evaluation on the MINE benchmark demonstrates that our approach effectively addresses the coverage-connectivity trade-off, achieving superior factual retention while maintaining high structural cohesion even as extracted knowledge volume substantially expands. These results highlight that QA-mediated semantic scaffolding plays a critical role in structuring semantics prior to KG extraction, enabling more coherent and reliable graph construction in subsequent stages.

</details>


### [32] [EHRNavigator: A Multi-Agent System for Patient-Level Clinical Question Answering over Heterogeneous Electronic Health Records](https://arxiv.org/abs/2601.10020)
*Lingfei Qian,Mauro Giuffre,Yan Wang,Huan He,Qianqian Xie,Xuguang Ai,Xeuqing Peng,Fan Ma,Ruey-Ling Weng,Donald Wright,Adan Wang,Qingyu Chen,Vipina K. Keloth,Hua Xu*

Main category: cs.CL

> 本文介绍了EHRNavigator，一个用于跨异构和多模态EHR数据进行患者级别问答的多代理框架，其在现实条件下的表现优于传统系统。

<details>
  <summary>Details</summary>

**Motivation:** 由于大多数现有的基于自然语言的问答系统都是在基准数据集上进行评估，导致它们的实际相关性有限，因此本研究的动力在于解决这一限制。

**Method:** 本文提出了一种名为EHRNavigator的多代理框架，该框架利用AI代理对异构和多模式的电子健康记录（EHR）数据进行患者级别的问答处理。

**Result:** 通过使用公共基准数据集和机构数据集在现实的医院环境下评估，EHRNavigator展示了强大的泛化能力，在现实案例中达到了86%的准确率，并保持了可接受的临床响应时间。

**Conclusion:** 总体而言，这些发现表明EHRNavigator成功地弥合了基准评估和临床部署之间的差距，为现实世界的EHR问答提供了一个强大的、适应性强且高效的解决方案。

**Abstract:** Clinical decision-making increasingly relies on timely and context-aware access to patient information within Electronic Health Records (EHRs), yet most existing natural language question-answering (QA) systems are evaluated solely on benchmark datasets, limiting their practical relevance. To overcome this limitation, we introduce EHRNavigator, a multi-agent framework that harnesses AI agents to perform patient-level question answering across heterogeneous and multimodal EHR data. We assessed its performance using both public benchmark and institutional datasets under realistic hospital conditions characterized by diverse schemas, temporal reasoning demands, and multimodal evidence integration. Through quantitative evaluation and clinician-validated chart review, EHRNavigator demonstrated strong generalization, achieving 86% accuracy on real-world cases while maintaining clinically acceptable response times. Overall, these findings confirm that EHRNavigator effectively bridges the gap between benchmark evaluation and clinical deployment, offering a robust, adaptive, and efficient solution for real-world EHR question answering.

</details>


### [33] [EmplifAI: a Fine-grained Dataset for Japanese Empathetic Medical Dialogues in 28 Emotion Labels](https://arxiv.org/abs/2601.10033)
*Wan Jou She,Lis Kanashiro Pereira,Fei Cheng,Sakiko Yahata,Panote Siriaraya,Eiji Aramaki*

Main category: cs.CL

> 文章介绍了EmplifAI日本情感对话数据集，评估了情感对齐情况，并通过微调基线日语大型语言模型实现了同情心和对话流畅性的提高。

<details>
  <summary>Details</summary>

**Motivation:** 为了帮助慢性疾病患者应对疾病管理过程中复杂的情绪变化，文章引入了EmplifAI数据集，旨在通过情感对话提供支持。

**Method:** 介绍了EmplifAI数据集的创建方法。数据集通过众包和专家审核的方式收集了280个医疗背景下的情境和4125个两轮对话，并利用BERTScore评估了多个大型语言模型（LLMs）在情境-对话对中的情感对齐情况。

**Result:** 评估结果显示，使用BERTScore对多个大型语言模型进行评估，获得了F1得分为0.83。利用EmplifAI对基线日语大型语言模型进行微调后，显著提高了流畅性、通用同情心和情绪特定同情心。

**Conclusion:** 通过对EmplifAI数据集的应用，文章实现了对多个大型语言模型的情感对话评测，验证了通过特定数据集微调模型以增强其同情心表现的方法的有效性。

**Abstract:** This paper introduces EmplifAI, a Japanese empathetic dialogue dataset designed to support patients coping with chronic medical conditions. They often experience a wide range of positive and negative emotions (e.g., hope and despair) that shift across different stages of disease management. EmplifAI addresses this complexity by providing situation-based dialogues grounded in 28 fine-grained emotion categories, adapted and validated from the GoEmotions taxonomy. The dataset includes 280 medically contextualized situations and 4125 two-turn dialogues, collected through crowdsourcing and expert review. To evaluate emotional alignment in empathetic dialogues, we assessed model predictions on situation--dialogue pairs using BERTScore across multiple large language models (LLMs), achieving F1 scores of 0.83. Fine-tuning a baseline Japanese LLM (LLM-jp-3.1-13b-instruct4) with EmplifAI resulted in notable improvements in fluency, general empathy, and emotion-specific empathy. Furthermore, we compared the scores assigned by LLM-as-a-Judge and human raters on dialogues generated by multiple LLMs to validate our evaluation pipeline and discuss the insights and potential risks derived from the correlation analysis.

</details>


### [34] [Long-Chain Reasoning Distillation via Adaptive Prefix Alignment](https://arxiv.org/abs/2601.10064)
*Zhenghao Liu,Zhuoyang Wu,Xinze Li,Yukun Yan,Shuo Wang,Zulong Chen,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CL

> P-ALIGN方法改进了小规模学生模型的学习效率，通过自适应地截断教师生成的推理轨迹，并利用简明的前缀进行有效的监督学习。

<details>
  <summary>Details</summary>

**Motivation:** 教师生成的推理轨迹通常又长又复杂，这使得学生模型的学习变得困难。这种不匹配导致了提供的监督信号和学生模型的学习能力之间的差距。为解决这一问题，提出了P-ALIGN蒸馏框架。

**Method:** 提出了一种名为Prefix-ALIGNment distillation (P-ALIGN) 的框架，通过自适应前缀对齐充分利用教师的CoTs进行蒸馏。具体来说，P-ALIGN通过判断剩余的后缀是否简洁且足以指导学生模型来自适应地截断教师生成的推理轨迹，并利用教师生成的前缀来监督学生模型，促进有效的前缀对齐。

**Result:** 在多个数学推理基准测试上，P-ALIGN比所有基线高出超过3%。进一步的分析表明，由P-ALIGN构建的前缀提供了更有效的监督信号，同时避免了冗余和不确定性推理成分的负面影响。

**Conclusion:** 实验结果证明，通过利用自适应前缀对齐方式，P-ALIGN显著提升了学生模型在数学推理任务上的性能，同时也表明其在提供有效学习信号上的优势。

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities, particularly in solving complex mathematical problems. Recent studies show that distilling long reasoning trajectories can effectively enhance the reasoning performance of small-scale student models. However, teacher-generated reasoning trajectories are often excessively long and structurally complex, making them difficult for student models to learn. This mismatch leads to a gap between the provided supervision signal and the learning capacity of the student model. To address this challenge, we propose Prefix-ALIGNment distillation (P-ALIGN), a framework that fully exploits teacher CoTs for distillation through adaptive prefix alignment. Specifically, P-ALIGN adaptively truncates teacher-generated reasoning trajectories by determining whether the remaining suffix is concise and sufficient to guide the student model. Then, P-ALIGN leverages the teacher-generated prefix to supervise the student model, encouraging effective prefix alignment. Experiments on multiple mathematical reasoning benchmarks demonstrate that P-ALIGN outperforms all baselines by over 3%. Further analysis indicates that the prefixes constructed by P-ALIGN provide more effective supervision signals, while avoiding the negative impact of redundant and uncertain reasoning components. All code is available at https://github.com/NEUIR/P-ALIGN.

</details>


### [35] [Deriving Character Logic from Storyline as Codified Decision Trees](https://arxiv.org/abs/2601.10080)
*Letian Peng,Kun Zhou,Longfei Yun,Yupeng Hou,Jingbo Shang*

Main category: cs.CL

> 我们提出了一种名为Codified Decision Trees (CDT) 的基于数据驱动的方法，生成可解释且可靠的代理角色配置文件，用于共情多样化的叙事环境。这种方法大幅改善了代理行为的可靠性和表现。

<details>
  <summary>Details</summary>

**Motivation:** 现有的行为配置文件大多是无结构化、不可执行和弱验证的，导致了代理行为的脆弱性。本研究旨在解决这一问题，提出了更有结构化和验证性的行为配置文件。

**Method:** 我们提出了一种基于数据驱动的构架，名为Codified Decision Trees (CDT)，它从大规模的叙事数据中诱导出可执行且可解释的决策结构。CDT将行为配置文件表示为一系列条件规则的树形结构，其中内部节点对应于有效的场景条件，叶子节点编码了基于事实的行为陈述，从而支持在执行时可确定性地检索符合上下文的规则。树是通过迭代地诱导候选场景-动作规则，用数据对其有效性进行验证，并通过分层专业化进行优化而学习得到的。

**Result:** 在多个基准测试中，CDT在16个制品的85个人物角色上显著优于人工编写配置文件和先前的角色配置文件诱导方法，表明编码验证的行为表示促进了更为可靠的代理定位。

**Conclusion:** 通过我们的方法，生成的支持透明检查和原则性更新的行为配置文件，显示出在多种叙事背景下，代理行为的更可靠表现。

**Abstract:** Role-playing (RP) agents rely on behavioral profiles to act consistently across diverse narrative contexts, yet existing profiles are largely unstructured, non-executable, and weakly validated, leading to brittle agent behavior. We propose Codified Decision Trees (CDT), a data-driven framework that induces an executable and interpretable decision structure from large-scale narrative data. CDT represents behavioral profiles as a tree of conditional rules, where internal nodes correspond to validated scene conditions and leaves encode grounded behavioral statements, enabling deterministic retrieval of context-appropriate rules at execution time. The tree is learned by iteratively inducing candidate scene-action rules, validating them against data, and refining them through hierarchical specialization, yielding profiles that support transparent inspection and principled updates. Across multiple benchmarks, CDT substantially outperforms human-written profiles and prior profile induction methods on $85$ characters across $16$ artifacts, indicating that codified and validated behavioral representations lead to more reliable agent grounding.

</details>


### [36] [Is MT Ready for the Next Crisis or Pandemic?](https://arxiv.org/abs/2601.10082)
*Vipasha Bansal,Elizabeth Brown,Chelsea Kendrick,Benjamin Pong,William D. Lewis*

Main category: cs.CL

> 研究评估了四种商业机器翻译系统在翻译TICO-19数据集中的疫情相关短句时的性能，并由此评估了这些系统在下一次大流行中应对低资源语言的准备程度。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在探讨现有的商业机器翻译系统能否有效地在危机和医疗环境下翻译到和从低资源语言进行翻译。

**Method:** 使用了TICO-19数据集对四种商业机器翻译系统进行了评估。

**Result:** 研究提出了这些机器翻译系统在处理低资源语言翻译时的能力及可能存在的问题。

**Conclusion:** 当前商业机器翻译系统应用于低资源语言翻译在即将到来的疫情中存在一定的局限性和准备不足。

**Abstract:** Communication in times of crisis is essential. However, there is often a mismatch between the language of governments, aid providers, doctors, and those to whom they are providing aid. Commercial MT systems are reasonable tools to turn to in these scenarios. But how effective are these tools for translating to and from low resource languages, particularly in the crisis or medical domain? In this study, we evaluate four commercial MT systems using the TICO-19 dataset, which is composed of pandemic-related sentences from a large set of high priority languages spoken by communities most likely to be affected adversely in the next pandemic. We then assess the current degree of ``readiness'' for another pandemic (or epidemic) based on the usability of the output translations.

</details>


### [37] [CALM-IT: Generating Realistic Long-Form Motivational Interviewing Dialogues with Dual-Actor Conversational Dynamics Tracking](https://arxiv.org/abs/2601.10085)
*Viet Cuong Nguyen,Nhi Yen Nguyen,Kristin A. Candan,Mary Conlon,Vanessa Rumie,Kristen Risola,Srijan Kumar,Munmun De Choudhury*

Main category: cs.CL

> 本文引入了CALM-IT框架，用于生成和评估长格式的动力学访谈对话，并说明此框架优于现有的基线方法。

<details>
  <summary>Details</summary>

**Motivation:** 大规模语言模型（LLM）在心理健康相关设置中的应用不断增加，但它们难以维持现实中、目标导向的长时间对话。虽然LLM可以生成流畅的回应，但它们优化的是下一步的回应，而不是维护一个连贯的治疗进度模型，导致韧性和长周期偏离的问题。

**Method:** 本文提出了CALM-IT框架，用于生成和评估长形式的动力学访谈对话，该框架明确建模了双角色对话动力学。CALM-IT将治疗师-客户互动表示为双向状态空间过程，双方持续更新所推断的对齐、心理状态和短期目标，以指导策略选择和话语生成。

**Result:** 在大规模评估中，CALM-IT在效果和目标一致性方面始终优于强大的基线，并且随着对话长度的增加保持了更高的稳定性。尽管CALM-IT发起的治疗师重定向较少，但它实现了最高的客户接受率（64.3%），表明干预时间更加精确和与治疗目标一致。

**Conclusion:** 总体而言，CALM-IT框架提供了证据，证明建模随时间演变的对话状态对于生成高质量的长形式的合成对话至关重要。

**Abstract:** Large Language Models (LLMs) are increasingly used in mental health-related settings, yet they struggle to sustain realistic, goal-directed dialogue over extended interactions. While LLMs generate fluent responses, they optimize locally for the next turn rather than maintaining a coherent model of therapeutic progress, leading to brittleness and long-horizon drift. We introduce CALM-IT, a framework for generating and evaluating long-form Motivational Interviewing (MI) dialogues that explicitly models dual-actor conversational dynamics. CALM-IT represents therapist-client interaction as a bidirectional state-space process, in which both agents continuously update inferred alignment, mental states, and short-term goals to guide strategy selection and utterance generation. Across large-scale evaluations, CALM-IT consistently outperforms strong baselines in Effectiveness and Goal Alignment and remains substantially more stable as conversation length increases. Although CALM-IT initiates fewer therapist redirections, it achieves the highest client acceptance rate (64.3%), indicating more precise and therapeutically aligned intervention timing. Overall, CALM-IT provides evidence for modeling evolving conversational state being essential for generating high-quality long-form synthetic conversations.

</details>


### [38] [SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature](https://arxiv.org/abs/2601.10108)
*Yiming Ren,Junjie Wang,Yuxin Meng,Yihang Shi,Zhiqiang Lin,Ruihang Chu,Yiran Xu,Ziming Li,Yunfei Zhao,Zihan Wang,Yu Qiao,Ruiming Tang,Minghao Liu,Yujiu Yang*

Main category: cs.CL

> 

<details>
  <summary>Details</summary>

**Motivation:** 

**Method:** Structure

**Result:** {
  "tldr": "本文提出了Fish-in-the-Ocean(FITO)范式来评估多模态大语言模型对长篇科学论文的理解水平，并构建了SIN-Data和SIN-Bench测试套件。实验表明，尽管某些模型在答案准确性方面表现良好，但在证据链的构建上存在不足，这成为了主要的瓶颈。", 
  "motivation": "传统评估方式难以确保模型在回答问题时构建了因果、证据链。因此，提出FITO范式及一套完整的评估体系，旨在推动模型从答案匹配到因果链构建的进步。", 
  "method": "构建SIN-Data和SIN-Bench来分别形成测试数据集和四个渐进式评测任务。提出'无证据，无分数'原则，评估模型回答是否有得自文档的关键证据支持。", 
  "result": "实验结果显示，对于八个多模态大语言模型来说，构建因果推理链成为了主要瓶颈。 Gemini-3-pro获得了最佳的总体平均分数（0.573），而GPT-5在SIN-QA任务上答案准确率最高（0.767），但在整体证据链评分上表现不佳。", 
  "conclusion": "本文指出，在多模态大语言模型中，仅仅正确答案还不足够，模型还应该构建起相应的因果、证据链，这目前仍是困扰模型表现的主要问题。不同模型在构建证据链方面的能力差异凸显了进一步研究的必要性。")
  
}

**Conclusion:** 

**Abstract:** Evaluating whether multimodal large language models truly understand long-form scientific papers remains challenging: answer-only metrics and synthetic "Needle-In-A-Haystack" tests often reward answer matching without requiring a causal, evidence-linked reasoning trace in the document. We propose the "Fish-in-the-Ocean" (FITO) paradigm, which requires models to construct explicit cross-modal evidence chains within native scientific documents. To operationalize FITO, we build SIN-Data, a scientific interleaved corpus that preserves the native interleaving of text and figures. On top of it, we construct SIN-Bench with four progressive tasks covering evidence discovery (SIN-Find), hypothesis verification (SIN-Verify), grounded QA (SIN-QA), and evidence-anchored synthesis (SIN-Summary). We further introduce "No Evidence, No Score", scoring predictions when grounded to verifiable anchors and diagnosing evidence quality via matching, relevance, and logic. Experiments on eight MLLMs show that grounding is the primary bottleneck: Gemini-3-pro achieves the best average overall score (0.573), while GPT-5 attains the highest SIN-QA answer accuracy (0.767) but underperforms on evidence-aligned overall scores, exposing a gap between correctness and traceable support.

</details>


### [39] [Skill-Aware Data Selection and Fine-Tuning for Data-Efficient Reasoning Distillation](https://arxiv.org/abs/2601.10109)
*Lechen Zhang,Yunxiang Zhang,Wei Hu,Lu Wang*

Main category: cs.CL

> 研究提出了一种技能为中心的蒸馏框架，通过只使用少量精选的数据，大幅度提升了数学推理任务中弱模型的推理能力。

<details>
  <summary>Details</summary>

**Motivation:** 大规模模型的蒸馏通常需要大量数据进行监督微调，因此本研究旨在通过提出数据高效的训练方法来解决这一问题。

**Method:** 提出了一种技能为中心的蒸馏框架，包含两个组件：基于技能的数据选择和技能感知的微调。

**Result:** 使用仅从10万个教师生成的语料库中精选出的1000个训练样本，该方法在五个数学推理基准测试中分别使Qwen3-4B和Qwen3-8B的性能提升了1.6%和1.4%，超过了随机微调基线。

**Conclusion:** 结果表明，通过强调训练期间的技能集中训练，可以有效提升逻辑推理模型的蒸馏效率。

**Abstract:** Large reasoning models such as DeepSeek-R1 and their distilled variants achieve strong performance on complex reasoning tasks. Yet, distilling these models often demands large-scale data for supervised fine-tuning (SFT), motivating the pursuit of data-efficient training methods. To address this, we propose a skill-centric distillation framework that efficiently transfers reasoning ability to weaker models with two components: (1) Skill-based data selection, which prioritizes examples targeting the student model's weaker skills, and (2) Skill-aware fine-tuning, which encourages explicit skill decomposition during problem solving. With only 1,000 training examples selected from a 100K teacher-generated corpus, our method surpasses random SFT baselines by +1.6% on Qwen3-4B and +1.4% on Qwen3-8B across five mathematical reasoning benchmarks. Further analysis confirms that these gains concentrate on skills emphasized during training, highlighting the effectiveness of skill-centric training for efficient reasoning distillation.

</details>


### [40] [Role-Playing Agents Driven by Large Language Models: Current Status, Challenges, and Future Trends](https://arxiv.org/abs/2601.10122)
*Ye Wang,Jiaxing Chen,Hongjiang Xiao*

Main category: cs.CL

> This paper reviews the development and technology of role-playing language agents (RPLAs) with a focus on their technological evolution stages, critical technical pathways supporting high-quality role-playing, and the methods and challenges of constructing role-specific corpora.

<details>
  <summary>Details</summary>

**Motivation:** To provide a systematic review of the current landscape of RPLAs and to highlight key areas for future research.

**Method:** The paper systematically reviews the current development and key technologies of RPLAs, summarizing critical technical pathways and analyzing methods and challenges of constructing role-specific corpora.

**Result:** The paper delineates the evolution stages of RPLAs and identifies critical areas such as character modeling, memory-augmented prompting, and behavioral decision control. It also covers the challenges and methods for constructing role-specific corpora and reviews evaluation frameworks and benchmark datasets.

**Conclusion:** The paper suggests future research directions for RPLAs including personality evolution modeling, multi-agent collaboration, and integration with cognitive neuroscience to advance immersive interaction techniques.

**Abstract:** In recent years, with the rapid advancement of large language models (LLMs), role-playing language agents (RPLAs) have emerged as a prominent research focus at the intersection of natural language processing (NLP) and human-computer interaction. This paper systematically reviews the current development and key technologies of RPLAs, delineating the technological evolution from early rule-based template paradigms, through the language style imitation stage, to the cognitive simulation stage centered on personality modeling and memory mechanisms. It summarizes the critical technical pathways supporting high-quality role-playing, including psychological scale-driven character modeling, memory-augmented prompting mechanisms, and motivation-situation-based behavioral decision control. At the data level, the paper further analyzes the methods and challenges of constructing role-specific corpora, focusing on data sources, copyright constraints, and structured annotation processes. In terms of evaluation, it collates multi-dimensional assessment frameworks and benchmark datasets covering role knowledge, personality fidelity, value alignment, and interactive hallucination, while commenting on the advantages and disadvantages of methods such as human evaluation, reward models, and LLM-based scoring. Finally, the paper outlines future development directions of role-playing agents, including personality evolution modeling, multi-agent collaborative narrative, multimodal immersive interaction, and integration with cognitive neuroscience, aiming to provide a systematic perspective and methodological insights for subsequent research.

</details>


### [41] [ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback](https://arxiv.org/abs/2601.10156)
*Yutao Mou,Zhangchi Xue,Lijun Li,Peiyang Liu,Shikun Zhang,Wei Ye,Jing Shao*

Main category: cs.CL

> 本文构建了LLM代理工具调用安全性检测基准TS-Bench，并提出了使用多任务强化学习的方法TS-Guard，对可能的危险行为进行预判。此外，还引入了一种反馈驱动的推理框架TS-Flow，显著减少了有害的工具调用，提高了代理的安全性和任务完成度。

<details>
  <summary>Details</summary>

**Motivation:** 长期来看，LLM代理通过调用外部工具与环境交互的能力也放大了安全风险。实时监控代理在每个步骤中工具调用行为并提前干预，以防止不安全执行是代理部署的关键，但这一方面尚未得到充分探索。

**Method:** 我们构建了一个新的基准测试TS-Bench，用于检测LLM代理在步骤级别工具调用的安全性。随后，我们开发了一个防护模型TS-Guard，该模型采用多任务强化学习，能够通过推理交互历史来检测工具调用的危险行为。TS-Guard评估请求的有害性及行为与攻击的相关性，提供可解释且泛化的安全判断。此外，我们还提出了一种基于防护模型反馈的推理框架TS-Flow，它在面对提示注入攻击时，平均减少了ReAct样式代理65%的有害工具调用，并提高了大约10%的安全任务完成度。

**Result:** 实验表明，TS-Flow可以显著减少LLM代理在执行任务时的有害工具调用行为，并在提示注入攻击的环境下提高大约10%的良性任务完成度。

**Conclusion:** 本研究提出的TS-Guard和TS-Flow框架显著提高了LLM代理在处理复杂任务和抵抗外部攻击时的安全性，提供了一种有效的实时监控和干预机制。这些方法不仅是在理论上的创新，也为实际应用提供了有力支持。

**Abstract:** While LLM-based agents can interact with environments via invoking external tools, their expanded capabilities also amplify security risks. Monitoring step-level tool invocation behaviors in real time and proactively intervening before unsafe execution is critical for agent deployment, yet remains under-explored. In this work, we first construct TS-Bench, a novel benchmark for step-level tool invocation safety detection in LLM agents. We then develop a guardrail model, TS-Guard, using multi-task reinforcement learning. The model proactively detects unsafe tool invocation actions before execution by reasoning over the interaction history. It assesses request harmfulness and action-attack correlations, producing interpretable and generalizable safety judgments and feedback. Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.

</details>


### [42] [What Gets Activated: Uncovering Domain and Driver Experts in MoE Language Models](https://arxiv.org/abs/2601.10159)
*Guimin Hu,Meng Li,Qiwei Peng,Lijie Hu,Boyan Xu,Ruichu Cai*

Main category: cs.CL

> 本研究通过分析MoE模型中的领域专家和驱动专家，揭示了模型激活专家的模式，以及令牌与特定专家激活之间的关系，提高了模型内部机制的可解释性。

<details>
  <summary>Details</summary>

**Motivation:** 大多数可解释性研究集中在Transformer中的层或神经元机制，多专家模型（MoE）中的专家级行为尚待探索。受大脑功能专业化的启发，我们通过区分领域专家和驱动专家来分析专家激活模式。

**Method:** 引入基于熵和因果效应的度量标准来评估专家是否因特定领域而被强烈偏好，以及专家激活对模型输出的因果贡献程度，从而识别领域专家和驱动专家。

**Result:** 研究结果表明，(1)一些激活的专家显示出清晰的领域偏好，而另一些对模型性能有强烈影响，发挥着决定性角色；(2)句子中较早出现的令牌更可能触发驱动专家；(3)调整领域专家和驱动专家的权重可以显著提高模型性能。

**Conclusion:** 该研究揭示了MoE模型内部机制，提高了其可解释性。调整领域专家和驱动专家权重可以显著提高所有三种模型和领域中的性能，这为理解和优化MoE模型提供了新的视角。

**Abstract:** Most interpretability work focuses on layer- or neuron-level mechanisms in Transformers, leaving expert-level behavior in MoE LLMs underexplored. Motivated by functional specialization in the human brain, we analyze expert activation by distinguishing domain and driver experts. In this work, we study expert activation in MoE models across three public domains and address two key questions: (1) which experts are activated, and whether certain expert types exhibit consistent activation patterns; and (2) how tokens are associated with and trigger the activation of specific experts. To answer these questions, we introduce entropy-based and causal-effect metrics to assess whether an expert is strongly favored for a particular domain, and how strongly expert activation contributes causally to the model's output, thus identify domain and driver experts, respectively. Furthermore, we explore how individual tokens are associated with the activation of specific experts. Our analysis reveals that (1) Among the activated experts, some show clear domain preferences, while others exert strong causal influence on model performance, underscoring their decisive roles. (2) tokens occurring earlier in a sentence are more likely to trigger the driver experts, and (3) adjusting the weights of domain and driver experts leads to significant performance gains across all three models and domains. These findings shed light on the internal mechanisms of MoE models and enhance their interpretability.

</details>


### [43] [Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment](https://arxiv.org/abs/2601.10160)
*Cameron Tice,Puria Radmard,Samuel Ratnam,Andy Kim,David Africa,Kyle O'Brien*

Main category: cs.CL

> 研究发现，预训练数据中关于AI的不对齐描述会增加AI系统的不对齐行为，反之亦然。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于预训练语料库中包含了大量关于AI系统的论述，但这些论述对下游对齐的影响尚不明确。如果AI行为的普遍描述是负面的，LLMs可能会内化相应的行为先验，从而导致自证的不对齐。

**Method:** 通过预训练6.9B参数的LLMs并使用不同数量的(mis)对齐讨论，进行了对此假设的首次控制研究。

**Result:** 研究表明，增加关于AI不对齐的人造训练文件的比例会导致不对齐行为的显著增加。反之，增加关于对齐行为的文档比例，可以将不对齐评分从45%降至9%。这些影响在后训练阶段减弱但仍持续。

**Conclusion:** 研究结果确立了如何通过预训练数据塑造对齐先验，或称对齐预训练，作为对后续训练的补充。建议实践者在训练能力的同时进行对齐预训练。

**Abstract:** Pretraining corpora contain extensive discourse about AI systems, yet the causal influence of this discourse on downstream alignment remains poorly understood. If prevailing descriptions of AI behaviour are predominantly negative, LLMs may internalise corresponding behavioural priors, giving rise to self-fulfilling misalignment. This paper provides the first controlled study of this hypothesis by pretraining 6.9B-parameter LLMs with varying amounts of (mis)alignment discourse. We find that discussion of AI contributes to misalignment. Upsampling synthetic training documents about AI misalignment leads to a notable increase in misaligned behaviour. Conversely, upsampling documents about aligned behaviour reduces misalignment scores from 45% to 9%. We consider this evidence of self-fulfilling alignment. These effects are dampened, but persist through post-training. Our findings establish the study of how pretraining data shapes alignment priors, or alignment pretraining, as a complement to post-training. We recommend practitioners pretrain for alignment as well as capabilities. Our models and datasets are available at alignmentpretraining.ai

</details>


### [44] [AWED-FiNER: Agents, Web applications, and Expert Detectors for Fine-grained Named Entity Recognition across 36 Languages for 6.6 Billion Speakers](https://arxiv.org/abs/2601.10161)
*Prachuryya Kaushik,Ashish Anand*

Main category: cs.CL

> AWED-FiNER是一个旨在为全球36种语言提供细粒度命名实体识别解决方案的开源生态系统。

<details>
  <summary>Details</summary>

**Motivation:** 由于大型语言模型在一般自然语言处理任务中表现优异，但在低资源语言和细粒度NLP任务中表现不佳，因此AWED-FiNER被开发来填补这一空白。

**Method:** 该论文介绍了AWED-FiNER，一个开源生态系统，旨在为全球36种语言提供细粒度命名实体识别(FgNER)的解决方案。此系统包含代理工具包、网络应用和多种最新的专家模型，特别是关注资源较少的语言。

**Result:** AWED-FiNER系统为超过66亿人口使用的36种语言提供了FgNER解决方案，并特别关注了一些濒危语言，如博多语、曼尼普尔语、比申普里亚语和弥索语。

**Conclusion:** AWED-FiNER提供了一系列的代理工具和网络应用，使得用户能够在各种资源约束条件下进行离线部署，并获取FgNER的注释结果。

**Abstract:** We introduce AWED-FiNER, an open-source ecosystem designed to bridge the gap in Fine-grained Named Entity Recognition (FgNER) for 36 global languages spoken by more than 6.6 billion people. While Large Language Models (LLMs) dominate general Natural Language Processing (NLP) tasks, they often struggle with low-resource languages and fine-grained NLP tasks. AWED-FiNER provides a collection of agentic toolkits, web applications, and several state-of-the-art expert models that provides FgNER solutions across 36 languages. The agentic tools enable to route multilingual text to specialized expert models and fetch FgNER annotations within seconds. The web-based platforms provide ready-to-use FgNER annotation service for non-technical users. Moreover, the collection of language specific extremely small sized open-source state-of-the-art expert models facilitate offline deployment in resource contraint scenerios including edge devices. AWED-FiNER covers languages spoken by over 6.6 billion people, including a specific focus on vulnerable languages such as Bodo, Manipuri, Bishnupriya, and Mizo. The resources can be accessed here: Agentic Tool (https://github.com/PrachuryyaKaushik/AWED-FiNER), Web Application (https://hf.co/spaces/prachuryyaIITG/AWED-FiNER), and 49 Expert Detector Models (https://hf.co/collections/prachuryyaIITG/awed-finer).

</details>


### [45] [Credit C-GPT: A Domain-Specialized Large Language Model for Conversational Understanding in Vietnamese Debt Collection](https://arxiv.org/abs/2601.10167)
*Nhung Nguyen Thi Hong,Cuong Nguyen Dang,Tri Le Ngoc*

Main category: cs.CL

> 本文提出的Credit C-GPT模型，是一个为越南语债务催收场景特制的大型语言模型，它高度集成了多种对话智能任务，具有显著的性能改进。

<details>
  <summary>Details</summary>

**Motivation:** 债务催收是银行、金融服务和保险（BFSI）行业内的重要功能，它依赖于大量的人对人交流，这些交流主要在越南语呼叫中心进行。这些对话涉及到非正式口语、情感变化及复杂的领域特定推理，这些都对传统自然语言处理系统构成了重大挑战。

**Method:** 本文介绍了Credit C-GPT，这是一个拥有七亿参数的领域专用大型语言模型，专门针对越南语债务催收场景进行了微调。该模型集成了多种对话智能任务，包括对话理解、情感识别、意图检测、呼叫阶段分类和结构化槽值提取，这些任务都基于单一的推理框架。文章描述了数据构建过程、标注策略和训练方法，并在专有的人工标注数据集上评估了该模型。

**Result:** 实验结果显示，与传统的基于管道的方法相比，该模型性能有所提升，表明领域专用的对话语言模型提供了一个可扩展且保护隐私的实时辅助和事后分析方案，适用于企业呼叫中心。

**Conclusion:** 实验证明，领域专门化对话语言模型能够为企业呼叫中心提供一个可扩展且尊重隐私的实时辅助和事后分析方案。

**Abstract:** Debt collection is a critical function within the banking, financial services, and insurance (BFSI) sector, relying heavily on large-scale human-to-human conversational interactions conducted primarily in Vietnamese contact centers. These conversations involve informal spoken language, emotional variability, and complex domain-specific reasoning, which pose significant challenges for traditional natural language processing systems. This paper introduces Credit C-GPT, a domain-specialized large language model with seven billion parameters, fine-tuned for conversational understanding in Vietnamese debt collection scenarios. The proposed model integrates multiple conversational intelligence tasks, including dialogue understanding, sentiment recognition, intent detection, call stage classification, and structured slot-value extraction, within a single reasoning-based framework. We describe the data construction process, annotation strategy, and training methodology, and evaluate the model on proprietary human-annotated datasets. Experimental results show consistent improvements over traditional pipeline-based approaches, indicating that domain-specialized conversational language models provide a scalable and privacy-aware solution for real-time assistance and post-call analytics in enterprise contact centers.

</details>


### [46] [HOMURA: Taming the Sand-Glass for Time-Constrained LLM Translation via Reinforcement Learning](https://arxiv.org/abs/2601.10187)
*Ziang Cui,Mengran Yu,Tianjiao Li,Chenyu Shi,Yingxuan Shi,Lusheng Zhang,Hongwei Lin*

Main category: cs.CL

> The paper addresses the verbosity bias in large language models for multilingual translation, particularly in subtitling and dubbing tasks. It introduces Sand-Glass benchmark and HOMURA framework to improve semantic fidelity and temporal compliance.

<details>
  <summary>Details</summary>

**Motivation:** The research is motivated by the need to address the verbosity problem that hinders large language models from effective performance in time-constrained translation tasks such as subtitling and dubbing without compromising on semantic accuracy.

**Method:** The authors propose the creation of a benchmark called Sand-Glass to evaluate translation under strict syllable-level duration constraints and develop HOMURA, a reinforcement learning framework that optimizes the balance between semantic preservation and temporal conformity via a KL-regularized objective with a dynamic syllable-ratio reward.

**Result:** Experiment results show that the proposed HOMURA framework remarkably improves upon LLM baselines, achieving controlled output length while preserving semantic quality.

**Conclusion:** By effectively resolving the conflict between verbosity and temporal constraints, the HOMURA framework demonstrates notable improvements in the practice of translation under syllable-level timing strictures.

**Abstract:** Large Language Models (LLMs) have achieved remarkable strides in multilingual translation but are hindered by a systemic cross-lingual verbosity bias, rendering them unsuitable for strict time-constrained tasks like subtitling and dubbing. Current prompt-engineering approaches struggle to resolve this conflict between semantic fidelity and rigid temporal feasibility. To bridge this gap, we first introduce Sand-Glass, a benchmark specifically designed to evaluate translation under syllable-level duration constraints. Furthermore, we propose HOMURA, a reinforcement learning framework that explicitly optimizes the trade-off between semantic preservation and temporal compliance. By employing a KL-regularized objective with a novel dynamic syllable-ratio reward, HOMURA effectively "tames" the output length. Experimental results demonstrate that our method significantly outperforms strong LLM baselines, achieving precise length control that respects linguistic density hierarchies without compromising semantic adequacy.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [47] [Diffusion-Driven Deceptive Patches: Adversarial Manipulation and Forensic Detection in Facial Identity Verification](https://arxiv.org/abs/2601.09806)
*Shahrzad Sayyafzadeh,Hongmei Chi,Shonda Bernadin*

Main category: cs.CV

> 该研究提出了一种端到端的流程，用于生成、改进和评估对抗补丁，以破坏面部生物识别系统，适用于法医分析和安全性测试。通过ViT-GPT2模型实现对抗样本的描述，并通过感知哈希和分割实现了有效检测，达到SSIM值为0.95。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于提出一种新的方法来生成改进的对抗性补丁，用于法医分析以识别和理解对抗性攻击的模式，同时也用于安全性测试，评估生物识别系统的漏洞。

**Method:** 使用FGSM生成针对身份分类器的对抗性噪声，采用扩散模型结合反向扩散进行改进以提升难以察觉性。将生成的对抗性补丁应用于面部图像中进行测试。引入Vision Transformer (ViT)-GPT2模型生成对抗性图像的描述。

**Result:** 研究评估了身份分类的变化、描述结果的变化以及在对抗条件下面部识别和表情识别系统中的漏洞。通过感知哈希和分割有效检测了对抗性补丁和样本，达到了SSIM值为0.95。

**Conclusion:** 研究提出了一种有效生成和增强对抗性补丁的流程，证明了在保持视觉自然性的同时可以获得高的对抗成功率。同时，通过ViT-GPT2模型得到了对对抗样本的语义性描述，有助于法医分析和文档记录。

**Abstract:** This work presents an end-to-end pipeline for generating, refining, and evaluating adversarial patches to compromise facial biometric systems, with applications in forensic analysis and security testing. We utilize FGSM to generate adversarial noise targeting an identity classifier and employ a diffusion model with reverse diffusion to enhance imperceptibility through Gaussian smoothing and adaptive brightness correction, thereby facilitating synthetic adversarial patch evasion. The refined patch is applied to facial images to test its ability to evade recognition systems while maintaining natural visual characteristics. A Vision Transformer (ViT)-GPT2 model generates captions to provide a semantic description of a person's identity for adversarial images, supporting forensic interpretation and documentation for identity evasion and recognition attacks. The pipeline evaluates changes in identity classification, captioning results, and vulnerabilities in facial identity verification and expression recognition under adversarial conditions. We further demonstrate effective detection and analysis of adversarial patches and adversarial samples using perceptual hashing and segmentation, achieving an SSIM of 0.95.

</details>


### [48] [LCF3D: A Robust and Real-Time Late-Cascade Fusion Framework for 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2601.09812)
*Carlo Sgaravatti,Riccardo Pieroni,Matteo Corno,Sergio M. Savaresi,Luca Magri,Giacomo Boracchi*

Main category: cs.CV

> 本文提出了LCF3D，一种结合RGB和LiDAR数据的新型多模态融合方法，尤其提升了行人和骑车人的3D检测性能，并展示了在不同传感配置下的良好泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 准确地在自动驾驶中定位3D物体如行人、骑车人及移动车辆对于提高检测性能至关重要。LCF3D旨在解决如何有效地整合RGB摄像头和LiDAR传感器数据以进行精确的3D物体检测，并解决了不同传感配置下域泛化的挑战。

**Method:** LCF3D采用了一种多模态融合的传感器融合框架，结合了基于RGB图像的2D对象检测器和基于LiDAR点云的3D对象检测器。该框架包括两个关键原则：一是后期融合（Late Fusion），通过匹配LiDAR的3D检测结果与RGB的2D检测结果来减少LiDAR的误检；二是级联融合（Cascade Fusion），通过生成新的3D锥形提案来补回RGB检测中的未识别对象。

**Result:** 实验结果表明，LCF3D在不同的测试域和传感配置下具有良好的性能，并在Kitti和nuScenes数据集上对于行人、骑车人、摩托车和自行车等类别进行了显著的改进。

**Conclusion:** LCF3D框架通过采用多模态融合技术，有效地提高了自动驾驶中对3D物体（尤其是行人和骑车人）的检测准确性，并且对于不同域的传感配置具有一般化处理能力。

**Abstract:** Accurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonomous Driving. To ensure high detection performance, Autonomous Vehicles complement RGB cameras with LiDAR sensors, but effectively combining these data sources for 3D object detection remains challenging. We propose LCF3D, a novel sensor fusion framework that combines a 2D object detector on RGB images with a 3D object detector on LiDAR point clouds. By leveraging multimodal fusion principles, we compensate for inaccuracies in the LiDAR object detection network. Our solution combines two key principles: (i) late fusion, to reduce LiDAR False Positives by matching LiDAR 3D detections with RGB 2D detections and filtering out unmatched LiDAR detections; and (ii) cascade fusion, to recover missed objects from LiDAR by generating new 3D frustum proposals corresponding to unmatched RGB detections. Experiments show that LCF3D is beneficial for domain generalization, as it turns out to be successful in handling different sensor configurations between training and testing domains. LCF3D achieves significant improvements over LiDAR-based methods, particularly for challenging categories like pedestrians and cyclists in the KITTI dataset, as well as motorcycles and bicycles in nuScenes. Code can be downloaded from: https://github.com/CarloSgaravatti/LCF3D.

</details>


### [49] [Explainable Deep Learning for Pediatric Pneumonia Detection in Chest X-Ray Images](https://arxiv.org/abs/2601.09814)
*Adil O. Khadidos,Aziida Nanyonga,Alaa O. Khadidos,Olfat M. Mirza,Mustafa Tahsin Yilmaz*

Main category: cs.CV

> EfficientNet-B0 outperformed DenseNet121 for pediatric pneumonia detection in chest X-rays, yielding higher accuracy, F1-score, and MCC.

<details>
  <summary>Details</summary>

**Motivation:** To develop a more accurate and computationally efficient diagnostic tool for pediatric pneumonia using state-of-the-art deep learning models.

**Method:** A dataset of 5,863 pediatric chest X-ray images was used to fine-tune DenseNet121 and EfficientNet-B0 models under identical training settings. Grad-CAM and LIME were applied to understand model prediction patterns.

**Result:** EfficientNet-B0 achieved better performance metrics compared to DenseNet121, though both models showed high recall scores.

**Conclusion:** EfficientNet-B0 is a promising candidate for clinical use in pediatric pneumonia diagnosis due to its balanced performance and computational efficiency, with added trust through explainability techniques.

**Abstract:** Background: Pneumonia remains a leading cause of morbidity and mortality among children worldwide, emphasizing the need for accurate and efficient diagnostic support tools. Deep learning has shown strong potential in medical image analysis, particularly for chest X-ray interpretation. This study compares two state-of-the-art convolutional neural network (CNN) architectures for automated pediatric pneumonia detection. Methods: A publicly available dataset of 5,863 pediatric chest X-ray images was used. Images were preprocessed through normalization, resizing, and data augmentation to enhance generalization. DenseNet121 and EfficientNet-B0 were fine-tuned using pretrained ImageNet weights under identical training settings. Performance was evaluated using accuracy, F1-score, Matthews Correlation Coefficient (MCC), and recall. Model explainability was incorporated using Gradient-weighted Class Activation Mapping (Grad-CAM) and Local Interpretable Model-agnostic Explanations (LIME) to visualize image regions influencing predictions. Results: EfficientNet-B0 outperformed DenseNet121, achieving an accuracy of 84.6%, F1-score of 0.8899, and MCC of 0.6849. DenseNet121 achieved 79.7% accuracy, an F1-score of 0.8597, and MCC of 0.5852. Both models demonstrated high recall values above 0.99, indicating strong sensitivity to pneumonia detection. Grad-CAM and LIME visualizations showed consistent focus on clinically relevant lung regions, supporting the reliability of model decisions. Conclusions: EfficientNet-B0 provided a more balanced and computationally efficient performance compared to DenseNet121, making it a strong candidate for clinical deployment. The integration of explainability techniques enhances transparency and trustworthiness in AI-assisted pediatric pneumonia diagnosis.

</details>


### [50] [NanoSD: Edge Efficient Foundation Model for Real Time Image Restoration](https://arxiv.org/abs/2601.09823)
*Subhajit Sanyal,Srinivas Soumitri Miriyala,Akshay Janardan Bankar,Sravanth Kodavanti,Harshit,Abhishek Ameta,Shreyas Pandith,Amit Satish Unde*

Main category: cs.CV

> NanoSD是基于Stable Diffusion 1.5设计的轻量级扩散模型系列，可以在保持生成性能的同时实现在边缘设备上的实时推理。

<details>
  <summary>Details</summary>

**Motivation:** 现有的轻量级变体大多通过压缩降噪U-Net或减少扩散轨迹来实现，这扰乱了潜在流形的连贯性，限制了其通用性。因此，本研究旨在通过设计NanoSD，解决重计算成本和设备部署间的问题。

**Method:** 本研究提出了NanoSD，这是一个由Stable Diffusion 1.5通过网络手术、特征导向生成蒸馏和结构化的架构缩放联合应用于U-Net以及VAE编解码器所提取的帕累托最优扩散基础模型系列。这种全流程协同设计方法保留了生成先验的同时，生成了一系列具有不同精确度、延迟和模型大小的操作点模型，例如1.3亿到3.15亿参数，可以在移动级NPUs上的实时推理延迟能低至20毫秒。

**Result:** 研究表明，参数减少本身并不与硬件效率相关。并分析了架构平衡、特征路由以及潜在空间保留如何共同影响真正的设备延迟情况。NanoSD作为可插入底座使用，能够在图像超分辨率、图像去模糊、脸部修复和单目深度估计等方面实现顶级的表现，超过了先前的轻量级扩散模型，在感知质量和实际可部署性方面都显示出了优越性。

**Conclusion:** NanoSD建立了一系列通用目的的扩散基础模型，这使得其适用于边缘设备上的实时视觉生成和恢复。

**Abstract:** Latent diffusion models such as Stable Diffusion 1.5 offer strong generative priors that are highly valuable for image restoration, yet their full pipelines remain too computationally heavy for deployment on edge devices. Existing lightweight variants predominantly compress the denoising U-Net or reduce the diffusion trajectory, which disrupts the underlying latent manifold and limits generalization beyond a single task. We introduce NanoSD, a family of Pareto-optimal diffusion foundation models distilled from Stable Diffusion 1.5 through network surgery, feature-wise generative distillation, and structured architectural scaling jointly applied to the U-Net and the VAE encoder-decoder. This full-pipeline co-design preserves the generative prior while producing models that occupy distinct operating points along the accuracy-latency-size frontier (e.g., 130M-315M parameters, achieving real-time inference down to 20ms on mobile-class NPUs). We show that parameter reduction alone does not correlate with hardware efficiency, and we provide an analysis revealing how architectural balance, feature routing, and latent-space preservation jointly shape true on-device latency. When used as a drop-in backbone, NanoSD enables state-of-the-art performance across image super-resolution, image deblurring, face restoration, and monocular depth estimation, outperforming prior lightweight diffusion models in both perceptual quality and practical deployability. NanoSD establishes a general-purpose diffusion foundation model family suitable for real-time visual generation and restoration on edge devices.

</details>


### [51] [UniHash: Unifying Pointwise and Pairwise Hashing Paradigms for Seen and Unseen Category Retrieval](https://arxiv.org/abs/2601.09828)
*Xiaoxu Ma,Runhao Li,Hanwen Liu,Xiangbo Zhang,Zhenyu Weng*

Main category: cs.CV

> 本文提出了UniHash，一个结合点式和对式训练范式的双分支框架，能够在已知和未知类别图像检索中实现平衡的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的深度散列方法由于局限于单一训练范式，无法同时在已知类别和未知类别的图像检索中表现出色。

**Method:** UniHash框架包括两个互补的分支：基于中心的点式分支和对式分支，通过新型的哈希码学习方法实现分支间的知识迁移。

**Result:** 理论分析验证了UniHash的效率，实验结果表明UniHash在CIFAR-10、MSCOCO和ImageNet数据集上取得了领先的结果。

**Conclusion:** UniHash通过整合点式和对式范式的优点，显著提高了图像检索在已知和未知类别下的一致性表现。

**Abstract:** Effective retrieval across both seen and unseen categories is crucial for modern image retrieval systems. Retrieval on seen categories ensures precise recognition of known classes, while retrieval on unseen categories promotes generalization to novel classes with limited supervision. However, most existing deep hashing methods are confined to a single training paradigm, either pointwise or pairwise, where the former excels on seen categories and the latter generalizes better to unseen ones. To overcome this limitation, we propose Unified Hashing (UniHash), a dual-branch framework that unifies the strengths of both paradigms to achieve balanced retrieval performance across seen and unseen categories. UniHash consists of two complementary branches: a center-based branch following the pointwise paradigm and a pairwise branch following the pairwise paradigm. A novel hash code learning method is introduced to enable bidirectional knowledge transfer between branches, improving hash code discriminability and generalization. It employs a mutual learning loss to align hash representations and introduces a Split-Merge Mixture of Hash Experts (SM-MoH) module to enhance cross-branch exchange of hash representations. Theoretical analysis substantiates the effectiveness of UniHash, and extensive experiments on CIFAR-10, MSCOCO, and ImageNet demonstrate that UniHash consistently achieves state-of-the-art performance in both seen and unseen image retrieval scenarios.

</details>


### [52] [ViSIL: Unified Evaluation of Information Loss in Multimodal Video Captioning](https://arxiv.org/abs/2601.09851)
*Po-han Li,Shenghui Chen,Ufuk Topcu,Sandeep Chinchali*

Main category: cs.CV

> 研究提出了一种新的评估方法ViSIL，该方法通过视觉语言模型量化多模态视频摘要中的信息损失，使其成为评估多模态摘要的有效指标，与VQA任务的人工与VLM评价显著相关，并提高了VQA精度。

<details>
  <summary>Details</summary>

**Motivation:** 传统度量如BLEU或ROUGE无法准确度量跨不同模态的信息覆盖情况，比如一段文本与一组关键帧之间的对比。因此，提出了ViSIL评分以更好地量化多模态视频摘要的信息覆盖度。

**Method:** 作者通过开发Video Summary Information Loss (ViSIL)评分来评估多模态视频摘要中的信息损失，该评分利用视觉语言模型确定摘要与原始视频之间的信息差异。

**Result:** 该研究提出了Video Summary Information Loss (ViSIL)评分，以解决多模态视频摘要中信息覆盖度量的问题。ViSIL评分通过视觉语言模型（VLM）推断来量化视频信息与摘要之间的信息损失，此方法为具有结构差异的多模态摘要格式提供了可直接比较的统一指标。研究结果显示，ViSIL评分与视频问答（VQA）任务中的人工评价和VLM性能具有显著的统计相关性，并且在不影响处理负载的情况下，与文本摘要相比，ViSIL评分优化摘要选择的Pareto最优前沿提高了7%的VQA精度。

**Conclusion:** 研究表明，ViSIL评分与VQA任务中的VLM性能及人工评分具有显著的统计相关性，并且当选择最佳的摘要时，这种评分方法可以提高7%的VQA精度，无需增加处理负载。

**Abstract:** Multimodal video captioning condenses dense footage into a structured format of keyframes and natural language. By creating a cohesive multimodal summary, this approach anchors generative AI in rich semantic evidence and serves as a lightweight proxy for high-efficiency retrieval. However, traditional metrics like BLEU or ROUGE fail to quantify information coverage across disparate modalities, such as comparing a paragraph of text to a sequence of keyframes. To address this, we propose the Video Summary Information Loss (ViSIL) score, an information-theoretic framework that quantifies the video information not captured by a summary via vision-language model (VLM) inference. By measuring the information loss, ViSIL is a unified metric that enables direct comparison across multimodal summary formats despite their structural discrepancies. Our results demonstrate that ViSIL scores show a statistically significant correlation with both human and VLM performance on Video Question Answering (VQA) tasks. ViSIL also enables summary selection to optimize the trade-off between information loss and processing speed, establishing a Pareto-optimal frontier that outperforms text summaries by $7\%$ in VQA accuracy without increasing processing load.

</details>


### [53] [Breaking the Limits of Open-Weight CLIP: An Optimization Framework for Self-supervised Fine-tuning of CLIP](https://arxiv.org/abs/2601.09859)
*Anant Mehta,Xiyuan Wei,Xingyu Chen,Tianbao Yang*

Main category: cs.CV

> This paper introduces TuneCLIP, a self-supervised fine-tuning framework for improving the performance of open-weight CLIP models across various downstream tasks without the need for training from scratch on large datasets. It includes a warm-up stage to recover optimization statistics and a fine-tuning stage to optimize a new contrastive loss, achieving up to +2.5% gains on ImageNet and +1.2% on DataComp benchmark, setting a new baseline for efficient post-pretraining adaptation for CLIP models.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the issue of performance degradation when applying standard training protocols to an open-weight CLIP model and to enhance general performance across a variety of tasks without requiring extensive and costly retraining on large datasets.

**Method:** TuneCLIP employs a two-stage approach: a warm-up stage that recovers optimization statistics to reduce cold-start bias and a fine-tuning stage that optimizes a new contrastive loss to address the penalty on false negatives.

**Result:** Experiments with TuneCLIP demonstrate consistent performance improvements across different model architectures and scales, significantly elevating the performance of open-weight CLIP models such as SigLIP (ViT-B/16) on ImageNet, out-of-distribution, and DataComp benchmarks.

**Conclusion:** TuneCLIP represents an effective self-supervised fine-tuning framework that can significantly improve the performance of open-weight CLIP models without the prohibitive costs associated with retraining from scratch on vast datasets.

**Abstract:** CLIP has become a cornerstone of multimodal representation learning, yet improving its performance typically requires a prohibitively costly process of training from scratch on billions of samples. We ask a different question: Can we improve the performance of open-weight CLIP models across various downstream tasks using only existing self-supervised datasets? Unlike supervised fine-tuning, which adapts a pretrained model to a single downstream task, our setting seeks to improve general performance across various tasks. However, as both our experiments and prior studies reveal, simply applying standard training protocols starting from an open-weight CLIP model often fails, leading to performance degradation. In this paper, we introduce TuneCLIP, a self-supervised fine-tuning framework that overcomes the performance degradation. TuneCLIP has two key components: (1) a warm-up stage of recovering optimization statistics to reduce cold-start bias, inspired by theoretical analysis, and (2) a fine-tuning stage of optimizing a new contrastive loss to mitigate the penalization on false negative pairs. Our extensive experiments show that TuneCLIP consistently improves performance across model architectures and scales. Notably, it elevates leading open-weight models like SigLIP (ViT-B/16), achieving gains of up to +2.5% on ImageNet and related out-of-distribution benchmarks, and +1.2% on the highly competitive DataComp benchmark, setting a new strong baseline for efficient post-pretraining adaptation.

</details>


### [54] [VibrantSR: Sub-Meter Canopy Height Models from Sentinel-2 Using Generative Flow Matching](https://arxiv.org/abs/2601.09866)
*Kiarie Ndegwa,Andreas Gros,Tony Chang,David Diaz,Vincent A. Landau,Nathan E. Rutenbeck,Luke J. Zachmann,Guy Bayes,Scott Conway*

Main category: cs.CV

> VibrantSR 是一种用于从 Sentinel-2 影像生成高分辨率冠层高度模型的超分辨率框架，能在大规模森林监测和碳核算方面比其他基准方法更高效，且无需依赖昂贵且不规律的航空影像。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在开发一种基于广泛可用 Sentinel-2 影像的超分辨率方法，以克服现有基于航空影像的森林监测和碳核算方案的成本高和技术限制。

**Method:** 研究提出了 VibrantSR 框架，用于从 Sentinel-2 影像中生成更高分辨率的冠层高度模型，面向森林监测和碳核算。

**Result:** VibrantSR 是一种生成式超分辨率框架，用于从 10 米分辨率的 Sentinel-2 影像中估计 0.5 米分辨率的冠层高度模型。该方法优于基于航空影像的方法，后者受到获取频率和时间不规律的限制。实验结果显示，VibrantSR 在 22 个 EPA Level 3 生态区域内的平均绝对误差为 4.39 米，优于几个卫星基准。虽然基于航空影像的方法仍具有更高的精度，但 VibrantSR 在不依赖昂贵且时间不规律的航空影像的情况下，能够实现大规模森林监测及碳核算。

**Conclusion:** 实验表明，VibrantSR 在大规模森林监测中表现出色，相对于其他基准方法，它能够在无需依赖昂贵且不规律的航空影像的前提下，实现高效的大陆级监测。

**Abstract:** We present VibrantSR (Vibrant Super-Resolution), a generative super-resolution framework for estimating 0.5 meter canopy height models (CHMs) from 10 meter Sentinel-2 imagery. Unlike approaches based on aerial imagery that are constrained by infrequent and irregular acquisition schedules, VibrantSR leverages globally available Sentinel-2 seasonal composites, enabling consistent monitoring at a seasonal-to-annual cadence. Evaluated across 22 EPA Level 3 eco-regions in the western United States using spatially disjoint validation splits, VibrantSR achieves a Mean Absolute Error of 4.39 meters for canopy heights >= 2 m, outperforming Meta (4.83 m), LANDFIRE (5.96 m), and ETH (7.05 m) satellite-based benchmarks. While aerial-based VibrantVS (2.71 m MAE) retains an accuracy advantage, VibrantSR enables operational forest monitoring and carbon accounting at continental scales without reliance on costly and temporally infrequent aerial acquisitions.

</details>


### [55] [MedVL-SAM2: A unified 3D medical vision-language model for multimodal reasoning and prompt-driven segmentation](https://arxiv.org/abs/2601.09879)
*Yang Xing,Jiong Wu,Savas Ozdemir,Ying Zhang,Yang Yang,Wei Shao,Kuang Gong*

Main category: cs.CV

> MedVL-SAM2是一个统一的3D医学多模态模型，旨在同时实现医学报告生成、视觉问答（VQA）和多范式分割（包括语义、引用和交互式分割），并展示了优秀的性能和可靠的三维视觉定位能力。

<details>
  <summary>Details</summary>

**Motivation:** 虽然现有的医学视觉语言模型在图像级文本任务中表现出色，但要在3D医学视觉语言模型中实现细粒度的视觉定位和体积空间推理仍然具有挑战性。该研究旨在解决这一问题，提出一种统一的模型框架。

**Method:** MedVL-SAM2通过集成了图像级别的推理和像素级别的感知，并融合了基于SAM2的体积分割模块，能够进行精准的多粒度空间推理。该模型在多阶段训练管道中训练，首先在大规模的3D CT图像-文本对语料库上预训练，然后联合优化语言理解和分割目标。

**Result:** 该模型在报告生成、VQA以及多种3D分割任务中展示了当前最先进的性能，且能提供可靠的3D视觉定位功能，可控制的交互式分割，以及强大的跨模态推理能力。

**Conclusion:** 研究结果表明，在一个统一的3D医学视觉语言模型中，可以同时实现高层语义推理和精确的3D定位。

**Abstract:** Recent progress in medical vision-language models (VLMs) has achieved strong performance on image-level text-centric tasks such as report generation and visual question answering (VQA). However, achieving fine-grained visual grounding and volumetric spatial reasoning in 3D medical VLMs remains challenging, particularly when aiming to unify these capabilities within a single, generalizable framework. To address this challenge, we proposed MedVL-SAM2, a unified 3D medical multimodal model that concurrently supports report generation, VQA, and multi-paradigm segmentation, including semantic, referring, and interactive segmentation. MedVL-SAM2 integrates image-level reasoning and pixel-level perception through a cohesive architecture tailored for 3D medical imaging, and incorporates a SAM2-based volumetric segmentation module to enable precise multi-granular spatial reasoning. The model is trained in a multi-stage pipeline: it is first pre-trained on a large-scale corpus of 3D CT image-text pairs to align volumetric visual features with radiology-language embeddings. It is then jointly optimized with both language-understanding and segmentation objectives using a comprehensive 3D CT segmentation dataset. This joint training enables flexible interaction via language, point, or box prompts, thereby unifying high-level visual reasoning with spatially precise localization. Our unified architecture delivers state-of-the-art performance across report generation, VQA, and multiple 3D segmentation tasks. Extensive analyses further show that the model provides reliable 3D visual grounding, controllable interactive segmentation, and robust cross-modal reasoning, demonstrating that high-level semantic reasoning and precise 3D localization can be jointly achieved within a unified 3D medical VLM.

</details>


### [56] [Transition Matching Distillation for Fast Video Generation](https://arxiv.org/abs/2601.09881)
*Weili Nie,Julius Berner,Nanye Ma,Chao Liu,Saining Xie,Arash Vahdat*

Main category: cs.CV

> 本文提出了TMD，一种高效的框架，用于将视频扩散模型转化为快速生成器，同时保持良好的视觉质量。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型视频扩散和流模型在高质量视频生成方面取得了显著成功，但这些模型在实时交互应用中的使用仍然受限，因为其多步采样过程效率低下。

**Method:** 提出了Transition Matching Distillation (TMD)框架，通过匹配扩散模型的多步去噪轨迹和轻量级条件流的几步概率转换过程，将视频扩散模型精炼为高效的多步生成器。TMD框架将原扩散骨干分解为两部分：提取语义表示的主要骨干以及基于这些表示执行多步内部流更新的流头。

**Result:** 实验表明，TMD在生成速度和视觉质量之间提供了灵活且强大的权衡。特别是在可比的推理成本下，TMD在视觉保真度和提示相关性方面优于现有精炼模型。

**Conclusion:** TMD是一个用于精炼视频扩散模型的新框架，能够实现高效的几步生成器，且在视觉保真度和提示相关性方面优于现有模型。

**Abstract:** Large video diffusion and flow models have achieved remarkable success in high-quality video generation, but their use in real-time interactive applications remains limited due to their inefficient multi-step sampling process. In this work, we present Transition Matching Distillation (TMD), a novel framework for distilling video diffusion models into efficient few-step generators. The central idea of TMD is to match the multi-step denoising trajectory of a diffusion model with a few-step probability transition process, where each transition is modeled as a lightweight conditional flow. To enable efficient distillation, we decompose the original diffusion backbone into two components: (1) a main backbone, comprising the majority of early layers, that extracts semantic representations at each outer transition step; and (2) a flow head, consisting of the last few layers, that leverages these representations to perform multiple inner flow updates. Given a pretrained video diffusion model, we first introduce a flow head to the model, and adapt it into a conditional flow map. We then apply distribution matching distillation to the student model with flow head rollout in each transition step. Extensive experiments on distilling Wan2.1 1.3B and 14B text-to-video models demonstrate that TMD provides a flexible and strong trade-off between generation speed and visual quality. In particular, TMD outperforms existing distilled models under comparable inference costs in terms of visual fidelity and prompt adherence. Project page: https://research.nvidia.com/labs/genair/tmd

</details>


### [57] [OT-Drive: Out-of-Distribution Off-Road Traversable Area Segmentation via Optimal Transport](https://arxiv.org/abs/2601.09952)
*Zhihua Zhao,Guoqiang Li,Chen Min,Kangping Lu*

Main category: cs.CV

> The paper introduces OT-Drive, a novel multi-modal optimal transport framework for improving traversable area segmentation in unseen scenarios, demonstrating superior performance and robustness.

<details>
  <summary>Details</summary>

**Motivation:** Current data-driven methods struggle with degraded performance in out-of-distribution scenarios, affecting autonomous driving tasks. The paper aims to improve the robustness of traversable area segmentation under unseen conditions.

**Method:** The paper proposes OT-Drive, a multi-modal fusion framework using Optimal Transport. It includes a Scene Anchor Generator (SAG) for decomposing scene information into semantic anchors and an OT Fusion module for fusing RGB and surface normal features.

**Result:** The proposed method achieves 95.16% mIoU on out-of-distribution scenarios and 89.79% mIoU on cross-dataset transfer tasks, showing significant improvements over baseline methods.

**Conclusion:** The paper concludes that their model can achieve strong generalization in out-of-distribution scenarios with limited training data, enhancing practicality and efficiency for real-world autonomous driving systems.

**Abstract:** Reliable traversable area segmentation in unstructured environments is critical for planning and decision-making in autonomous driving. However, existing data-driven approaches often suffer from degraded segmentation performance in out-of-distribution (OOD) scenarios, consequently impairing downstream driving tasks. To address this issue, we propose OT-Drive, an Optimal Transport--driven multi-modal fusion framework. The proposed method formulates RGB and surface normal fusion as a distribution transport problem. Specifically, we design a novel Scene Anchor Generator (SAG) to decompose scene information into the joint distribution of weather, time-of-day, and road type, thereby constructing semantic anchors that can generalize to unseen scenarios. Subsequently, we design an innovative Optimal Transport-based multi-modal fusion module (OT Fusion) to transport RGB and surface normal features onto the manifold defined by the semantic anchors, enabling robust traversable area segmentation under OOD scenarios. Experimental results demonstrate that our method achieves 95.16% mIoU on ORFD OOD scenarios, outperforming prior methods by 6.35%, and 89.79% mIoU on cross-dataset transfer tasks, surpassing baselines by 13.99%.These results indicate that the proposed model can attain strong OOD generalization with only limited training data, substantially enhancing its practicality and efficiency for real-world deployment.

</details>


### [58] [The Spatial Blindspot of Vision-Language Models](https://arxiv.org/abs/2601.09954)
*Nahid Alam,Leema Krishna Murali,Siddhant Bharadwaj,Patrick Liu,Timothy Chung,Drishti Sharma,Akshata A,Kranthi Kiran,Wesley Tam,Bala Krishna S Vegesna*

Main category: cs.CV

> The paper highlights the lack of spatial awareness in Vision-Language Models and proposes improvements through alternative image encoder training objectives and 2D positional encodings to enhance spatial reasoning.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to overcome the limitations of current VLMs in capturing spatial relationships, which is crucial for applications like robotics and embodied AI.

**Method:** The paper investigates two main approaches to improve spatial awareness in Vision-Language Models (VLMs): image encoders trained with alternative objectives and the use of 2D positional encodings.

**Result:** Experiments demonstrate that the proposed architectural modifications lead to better performance on spatial reasoning benchmarks.

**Conclusion:** The study concludes that incorporating spatial awareness through these techniques is essential for the next generation of VLMs to support applications that require a strong understanding of spatial relationships.

**Abstract:** Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.

</details>


### [59] [DR$^2$Seg: Decomposed Two-Stage Rollouts for Efficient Reasoning Segmentation in Multimodal Large Language Models](https://arxiv.org/abs/2601.09981)
*Yulin He,Wei Chen,Zhikang Jian,Tianhang Guo,Wenjuan Zhou,Minglong Li*

Main category: cs.CV

> DR$^2$Seg通过自奖励框架和两阶段策略，改进了推理分割任务中的效率和准确性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法在处理复杂的文本查询时容易产生冗长的推理链，影响目标定位，因此提出DR$^2$Seg以改善这一问题。

**Method:** DR$^2$Seg采用两阶段策略，将推理分割分为多模态推理和引用分割，减少了冗余思考，提高了推理效率和分割精度，无需额外的思考监督。

**Result:** 实验表明，DR$^2$Seg在不同规模的MLLMs和分割模型上均能提高推理效率和分割性能。

**Conclusion:** DR$^2$Seg框架通过减少冗余思考，显著提高了推理分割任务的效果。

**Abstract:** Reasoning segmentation is an emerging vision-language task that requires reasoning over intricate text queries to precisely segment objects. However, existing methods typically suffer from overthinking, generating verbose reasoning chains that interfere with object localization in multimodal large language models (MLLMs). To address this issue, we propose DR$^2$Seg, a self-rewarding framework that improves both reasoning efficiency and segmentation accuracy without requiring extra thinking supervision. DR$^2$Seg employs a two-stage rollout strategy that decomposes reasoning segmentation into multimodal reasoning and referring segmentation. In the first stage, the model generates a self-contained description that explicitly specifies the target object. In the second stage, this description replaces the original complex query to verify its self-containment. Based on this design, two self-rewards are introduced to strengthen goal-oriented reasoning and suppress redundant thinking. Extensive experiments across MLLMs of varying scales and segmentation models demonstrate that DR$^2$Seg consistently improves reasoning efficiency and overall segmentation performance.

</details>


### [60] [DW-DGAT: Dynamically Weighted Dual Graph Attention Network for Neurodegenerative Disease Diagnosis](https://arxiv.org/abs/2601.10001)
*Chengjia Liang,Zhenjiong Wang,Chao Chen,Ruizhi Zhang,Songxi Liang,Hai Xie,Haijun Lei,Zhongwei Huang*

Main category: cs.CV

> 本文针对神经退行性疾病早期诊断面临的挑战，提出了一种动态加权双图注意力网络（DW-DGAT），展示了该方法在处理数据高维度、异质性以及类别不平衡方面具有优越性能，并通过实验验证了其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于帕金森病（PD）和阿尔茨海默病（AD）等神经退行性疾病早期诊断的重要性，本文旨在解决多指标数据的高维度、神经影像和表型数据的异质性以及类别不平衡对ND早期诊断所造成的挑战。

**Method:** 本文提出了一种动态加权双图注意力网络（DW-DGAT），该网络整合了多指标数据的通用数据融合策略，基于脑区和样本间关系的双图注意力网络架构，以及减轻类别不平衡的类权重生成机制和两个稳定有效的损失函数。

**Result:** 严格的实验基于帕金森进展生物标志物计划（PPMI）和阿尔茨海默病神经影像学计划（ADNI）研究进行，结果展示了本文方法具有最先进的性能。

**Conclusion:** 实验结果证明了动态加权双图注意力网络（DW-DGAT）在神经退行性疾病早期诊断中具有最先进的性能。

**Abstract:** Parkinson's disease (PD) and Alzheimer's disease (AD) are the two most prevalent and incurable neurodegenerative diseases (NDs) worldwide, for which early diagnosis is critical to delay their progression. However, the high dimensionality of multi-metric data with diverse structural forms, the heterogeneity of neuroimaging and phenotypic data, and class imbalance collectively pose significant challenges to early ND diagnosis. To address these challenges, we propose a dynamically weighted dual graph attention network (DW-DGAT) that integrates: (1) a general-purpose data fusion strategy to merge three structural forms of multi-metric data; (2) a dual graph attention architecture based on brain regions and inter-sample relationships to extract both micro- and macro-level features; and (3) a class weight generation mechanism combined with two stable and effective loss functions to mitigate class imbalance. Rigorous experiments, based on the Parkinson Progression Marker Initiative (PPMI) and Alzhermer's Disease Neuroimaging Initiative (ADNI) studies, demonstrate the state-of-the-art performance of our approach.

</details>


### [61] [VERHallu: Evaluating and Mitigating Event Relation Hallucination in Video Large Language Models](https://arxiv.org/abs/2601.10010)
*Zefan Zhang,Kehua Zhu,Shijie Jiang,Hongyuan Lu,Shengkai Sun,Tian Bai*

Main category: cs.CV

> Paper presents VERHallu benchmark on event relation hallucination and proposes KFP strategy to improve VideoLLMs' dense-event relation understanding without affecting inference speed.

<details>
  <summary>Details</summary>

**Motivation:** To address the issue that current VideoLLMs struggle with dense-event relation reasoning, often overlooking subevents, leading to incomplete and inaccurate understanding of event relations.

**Method:** Key-Frame Propagating (KFP) strategy is proposed to enhance multi-event understanding by reallocating frame-level attention within intermediate layers.

**Result:** Experiments demonstrate the proposed KFP strategy effectively reduces event relation hallucination without impacting inference speed.

**Conclusion:** The proposed KFP strategy shows promise in improving the comprehensive understanding of event relations in videos, addressing the limitations of current VideoLLMs.

**Abstract:** Video Large Language Models (VideoLLMs) exhibit various types of hallucinations. Existing research has primarily focused on hallucinations involving the presence of events, objects, and scenes in videos, while largely neglecting event relation hallucination. In this paper, we introduce a novel benchmark for evaluating the Video Event Relation Hallucination, named VERHallu. This benchmark focuses on causal, temporal, and subevent relations between events, encompassing three types of tasks: relation classification, question answering, and counterfactual question answering, for a comprehensive evaluation of event relation hallucination. Additionally, it features counterintuitive video scenarios that deviate from typical pretraining distributions, with each sample accompanied by human-annotated candidates covering both vision-language and pure language biases. Our analysis reveals that current state-of-the-art VideoLLMs struggle with dense-event relation reasoning, often relying on prior knowledge due to insufficient use of frame-level cues. Although these models demonstrate strong grounding capabilities for key events, they often overlook the surrounding subevents, leading to an incomplete and inaccurate understanding of event relations. To tackle this, we propose a Key-Frame Propagating (KFP) strategy, which reallocates frame-level attention within intermediate layers to enhance multi-event understanding. Experiments show it effectively mitigates the event relation hallucination without affecting inference speed.

</details>


### [62] [Disentangled Concept Representation for Text-to-image Person Re-identification](https://arxiv.org/abs/2601.10053)
*Giyeol Kim,Chanho Eom*

Main category: cs.CV

> 文中提出了一种新的文本到图像人重新识别（TIReID）方法DiCo，其通过分层独立的跨模态对齐提高了性能，并增强了结果的解释性

<details>
  <summary>Details</summary>

**Motivation:** 文本到图像的人重新识别（TIReID）面临着视觉外观与文本表达之间模态差异巨大的挑战，需要建模区分具有相似属性（如衣服颜色、纹理或着装风格）的个体所需的细粒度对应关系

**Method:** DiCo (Disentangled Concept Representation) 是一个实现分层且独立的跨模态对齐的新框架。它引入了基于共享槽的表示方法，每个槽在跨模态中充当部分级别的定位点，并进一步分解为多个概念块。这种设计能够在保留图像和文本之间的一致性部分对应关系的同时，解耦互补属性（如颜色、纹理、形状）

**Result:** 在CUHK-PEDES、ICFG-PEDES和RSTPReid上的广泛实验表明，该框架实现了与最先进方法的竞争性能，同时通过显式的槽级和块级表示增强了解释性，实现了更精细的检索结果

**Conclusion:** 提出了DiCo框架，通过分层和独立的跨模态对齐，成功地提高了文本到图像的人重新识别性能，并增强了结果的解释性

**Abstract:** Text-to-image person re-identification (TIReID) aims to retrieve person images from a large gallery given free-form textual descriptions. TIReID is challenging due to the substantial modality gap between visual appearances and textual expressions, as well as the need to model fine-grained correspondences that distinguish individuals with similar attributes such as clothing color, texture, or outfit style. To address these issues, we propose DiCo (Disentangled Concept Representation), a novel framework that achieves hierarchical and disentangled cross-modal alignment. DiCo introduces a shared slot-based representation, where each slot acts as a part-level anchor across modalities and is further decomposed into multiple concept blocks. This design enables the disentanglement of complementary attributes (\textit{e.g.}, color, texture, shape) while maintaining consistent part-level correspondence between image and text. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that our framework achieves competitive performance with state-of-the-art methods, while also enhancing interpretability through explicit slot- and block-level representations for more fine-grained retrieval results.

</details>


### [63] [UEOF: A Benchmark Dataset for Underwater Event-Based Optical Flow](https://arxiv.org/abs/2601.10054)
*Nick Truong,Pritam P. Karmokar,William J. Beksi*

Main category: cs.CV

> 本文提出了一种新的合成数据集UEOF，解决了事件相机在水下环境中光流预测的问题。本文通过基准测试展示了此数据集对于未来算法开发的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 研究现有的数据集缺乏实际的水下光学和精确的光流配对，这限制了事件相机在水下环境中的应用。为解决此问题，提出一个基于物理的合成水下事件光流数据集。

**Method:** 使用基于物理的光线追踪的RGBD序列和现代视频到事件的管道生成真实事件数据流。

**Result:** 本文提出了一种基于物理的水下光流数据集，使用渲染的RGBD视频序列转换为真实的事件数据流。该数据集包含了密集的地面真实光流、深度和相机运动信息。此外，本文对现有的光流预测方法进行了基准测试，以研究水下光照传输对事件形成和运动估计精度的影响。此数据集为未来的水下事件感知算法的发展和评估建立了新的基准。

**Conclusion:** 本文建立了首个水下事件光流数据集，并对其进行了详细的研究，为未来的水下事件感知算法的发展和评估奠定了基础。

**Abstract:** Underwater imaging is fundamentally challenging due to wavelength-dependent light attenuation, strong scattering from suspended particles, turbidity-induced blur, and non-uniform illumination. These effects impair standard cameras and make ground-truth motion nearly impossible to obtain. On the other hand, event cameras offer microsecond resolution and high dynamic range. Nonetheless, progress on investigating event cameras for underwater environments has been limited due to the lack of datasets that pair realistic underwater optics with accurate optical flow. To address this problem, we introduce the first synthetic underwater benchmark dataset for event-based optical flow derived from physically-based ray-traced RGBD sequences. Using a modern video-to-event pipeline applied to rendered underwater videos, we produce realistic event data streams with dense ground-truth flow, depth, and camera motion. Moreover, we benchmark state-of-the-art learning-based and model-based optical flow prediction methods to understand how underwater light transport affects event formation and motion estimation accuracy. Our dataset establishes a new baseline for future development and evaluation of underwater event-based perception algorithms. The source code and dataset for this project are publicly available at https://robotic-vision-lab.github.io/ueof.

</details>


### [64] [CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation](https://arxiv.org/abs/2601.10061)
*Chengzhuo Tong,Mingkun Chang,Shenglong Zhang,Yuran Wang,Cheng Liang,Zhizheng Zhao,Ruichuan An,Bohan Zeng,Yang Shi,Yifan Dai,Ziming Zhao,Guanbin Li,Pengfei Wan,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

> CoF-T2I is a novel model that incorporates Chain-of-Frame (CoF) reasoning into text-to-image generation, enabling more progressive and refined image creation, and shows promising results in enhancing the quality and reducing motion artifacts in T2I tasks.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this study arises from the limited exploration of video generation models' capabilities in enhancing text-to-image generation. Specifically, there is a lack of clear visual reasoning starting points and interpretable intermediate states in the T2I generation process, which the authors aim to address.

**Method:** The method introduced is CoF-T2I, a model that integrates Chain-of-Frame (CoF) reasoning into text-to-image (T2I) generation. This model facilitates progressive visual refinement, utilizing intermediate frames as explicit visual reasoning steps.

**Result:** Experiments demonstrate that CoF-T2I outperforms standard video models and achieves competitive results on challenging benchmarks like GenEval and Imagine-Bench, with scores of 0.86 and 7.468 respectively.

**Conclusion:** The conclusion is that the integration of CoF reasoning into T2I generation through progressive visual refinement shows significant promise in advancing the quality of high-standard text-to-image generation.

**Abstract:** Recent video generation models have revealed the emergence of Chain-of-Frame (CoF) reasoning, enabling frame-by-frame visual inference. With this capability, video models have been successfully applied to various visual tasks (e.g., maze solving, visual puzzles). However, their potential to enhance text-to-image (T2I) generation remains largely unexplored due to the absence of a clearly defined visual reasoning starting point and interpretable intermediate states in the T2I generation process. To bridge this gap, we propose CoF-T2I, a model that integrates CoF reasoning into T2I generation via progressive visual refinement, where intermediate frames act as explicit reasoning steps and the final frame is taken as output. To establish such an explicit generation process, we curate CoF-Evol-Instruct, a dataset of CoF trajectories that model the generation process from semantics to aesthetics. To further improve quality and avoid motion artifacts, we enable independent encoding operation for each frame. Experiments show that CoF-T2I significantly outperforms the base video model and achieves competitive performance on challenging benchmarks, reaching 0.86 on GenEval and 7.468 on Imagine-Bench. These results indicate the substantial promise of video models for advancing high-quality text-to-image generation.

</details>


### [65] [ReaMIL: Reasoning- and Evidence-Aware Multiple Instance Learning for Whole-Slide Histopathology](https://arxiv.org/abs/2601.10073)
*Hyun Do Jung,Jungwon Choi,Hwiyoung Kim*

Main category: cs.CV

> ReaMIL enhances standard MIL for histopathology by incorporating a light-weight selection head that identifies spatially compact, informative regions, thus improving prediction accuracy and efficiency.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve upon the standard MIL approach by adding a mechanism that selects spatially compact and informative regions without sacrificing performance.

**Method:** We introduce ReaMIL, a multiple instance learning approach for histopathology that includes a light selection head trained with a budgeted-sufficiency objective.

**Result:** ReaMIL matches or slightly improves baseline AUC on histopathology datasets and provides efficient evidence tiles with accurate class confidence.

**Conclusion:** The method is effective in selecting minimal yet sufficient evidence tiles for accurate prediction, integrating seamlessly with standard MIL training and yielding slide-level overlays.

**Abstract:** We introduce ReaMIL (Reasoning- and Evidence-Aware MIL), a multiple instance learning approach for whole-slide histopathology that adds a light selection head to a strong MIL backbone. The head produces soft per-tile gates and is trained with a budgeted-sufficiency objective: a hinge loss that enforces the true-class probability to be $\geq τ$ using only the kept evidence, under a sparsity budget on the number of selected tiles. The budgeted-sufficiency objective yields small, spatially compact evidence sets without sacrificing baseline performance. Across TCGA-NSCLC (LUAD vs. LUSC), TCGA-BRCA (IDC vs. Others), and PANDA, ReaMIL matches or slightly improves baseline AUC and provides quantitative evidence-efficiency diagnostics. On NSCLC, it attains AUC 0.983 with a mean minimal sufficient K (MSK) $\approx 8.2$ tiles at $τ= 0.90$ and AUKC $\approx 0.864$, showing that class confidence rises sharply and stabilizes once a small set of tiles is kept. The method requires no extra supervision, integrates seamlessly with standard MIL training, and naturally yields slide-level overlays. We report accuracy alongside MSK, AUKC, and contiguity for rigorous evaluation of model behavior on WSIs.

</details>


### [66] [Thinking Like Van Gogh: Structure-Aware Style Transfer via Flow-Guided 3D Gaussian Splatting](https://arxiv.org/abs/2601.10075)
*Zhendong Wang,Lebin Zhou,Jingchuan Xiao,Rongduo Han,Nam Ling,Cihan Ruan*

Main category: cs.CV

> 本文提出了一种流场引导下的无网格3D几何推进框架，用于实现3D后印象派风格化，通过分离几何形变和色彩调整，改进了艺术风格化的真实性和质量，同时引入了美学判断的评估方式。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大多数3D风格迁移方法将几何视为用于表面纹理投影的刚性基底，而忽略了后印象派放大结构形式同时抑制照片细节的原则。为了忠实再现后印象派风格化，需要在3D场景中拥抱几何抽象作为主要的表达方式。

**Method:** 本研究提出了一种基于流场引导的几何推进框架，适用于3D高斯散射方法（3DGS），以在无网格设置中实现后印象派风格化。该方法从2D绘画中提取方向流场并将其反向传播到3D空间，调整高斯基元形成与场景拓扑结构对齐的笔触，而不是依赖显式的网格先验。

**Result:** 研究结果表明，该方法能够通过画家的运动直接驱动表达性的结构变形，而非依赖于光度约束，并引入了一个新的评估框架，通过美学判断而非传统的像素级别度量来评估艺术真实性。

**Conclusion:** 该研究方法实现了在无网格设置条件下，基于画家2D动作品的3D高斯几何变形，并能隔离几何变形和色彩优化，减少在结构抽象过程中的伪影。这种方法在保持几何变形独立性的同时，提供了一种新的艺术真实性的评估方法，即采用视觉语言模型作为评判标准。

**Abstract:** In 1888, Vincent van Gogh wrote, "I am seeking exaggeration in the essential." This principle, amplifying structural form while suppressing photographic detail, lies at the core of Post-Impressionist art. However, most existing 3D style transfer methods invert this philosophy, treating geometry as a rigid substrate for surface-level texture projection. To authentically reproduce Post-Impressionist stylization, geometric abstraction must be embraced as the primary vehicle of expression.
  We propose a flow-guided geometric advection framework for 3D Gaussian Splatting (3DGS) that operationalizes this principle in a mesh-free setting. Our method extracts directional flow fields from 2D paintings and back-propagates them into 3D space, rectifying Gaussian primitives to form flow-aligned brushstrokes that conform to scene topology without relying on explicit mesh priors. This enables expressive structural deformation driven directly by painterly motion rather than photometric constraints.
  Our contributions are threefold: (1) a projection-based, mesh-free flow guidance mechanism that transfers 2D artistic motion into 3D Gaussian geometry; (2) a luminance-structure decoupling strategy that isolates geometric deformation from color optimization, mitigating artifacts during aggressive structural abstraction; and (3) a VLM-as-a-Judge evaluation framework that assesses artistic authenticity through aesthetic judgment instead of conventional pixel-level metrics, explicitly addressing the subjective nature of artistic stylization.

</details>


### [67] [Difficulty-guided Sampling: Bridging the Target Gap between Dataset Distillation and Downstream Tasks](https://arxiv.org/abs/2601.10090)
*Mingzhuo Li,Guang Li,Linfeng Ye,Jiafeng Mao,Takahiro Ogawa,Konstantinos N. Plataniotis,Miki Haseyama*

Main category: cs.CV

> 本文提出了难度引导采样（DGS）方法，以改善数据集蒸馏性能，使其在图像分类等任务上的表现更佳。

<details>
  <summary>Details</summary>

**Motivation:** 现有的数据集蒸馏方法通常侧重于从原始数据集中提取特征，忽视了任务特定的信息，产生了蒸馏目标与下游任务之间的目标差距。有必要将有助于下游训练的特征整合到数据蒸馏中以填补这一差距。

**Method:** 我们提出了基于难度引导的采样（DGS）方法，这是一种插件式后续采样模块，用于从由现有方法生成的图像池中根据特定目标难度分布进行采样。此外，提出了困难感知指导（DAG）来探索难度在生成过程中的影响。

**Result:** 在多种设置下的广泛实验表明了所提方法的有效性，并突显了难度在各种下游任务中的潜在应用。

**Conclusion:** 通过引入DGS方法，文章展示了其在缩小蒸馏目标与下游任务之间差距的有效性。此外，DAG的应用进一步表明了难度在数据生成过程中具有更广泛的应用潜力。

**Abstract:** In this paper, we propose difficulty-guided sampling (DGS) to bridge the target gap between the distillation objective and the downstream task, therefore improving the performance of dataset distillation. Deep neural networks achieve remarkable performance but have time and storage-consuming training processes. Dataset distillation is proposed to generate compact, high-quality distilled datasets, enabling effective model training while maintaining downstream performance. Existing approaches typically focus on features extracted from the original dataset, overlooking task-specific information, which leads to a target gap between the distillation objective and the downstream task. We propose leveraging characteristics that benefit the downstream training into data distillation to bridge this gap. Focusing on the downstream task of image classification, we introduce the concept of difficulty and propose DGS as a plug-in post-stage sampling module. Following the specific target difficulty distribution, the final distilled dataset is sampled from image pools generated by existing methods. We also propose difficulty-aware guidance (DAG) to explore the effect of difficulty in the generation process. Extensive experiments across multiple settings demonstrate the effectiveness of the proposed methods. It also highlights the broader potential of difficulty for diverse downstream tasks.

</details>


### [68] [V-Zero: Self-Improving Multimodal Reasoning with Zero Annotation](https://arxiv.org/abs/2601.10094)
*Han Wang,Yi Yang,Jingyuan Hu,Minfeng Zhu,Wei Chen*

Main category: cs.CV

> V-Zero, a framework for self-improvement in vision-language models using only unlabeled images, demonstrates significant performance gains without human annotations through the interaction of a Questioner and Solver role, both trained iteratively via Group Relative Policy Optimization.

<details>
  <summary>Details</summary>

**Motivation:** State-of-the-art vision-language models require large-scale human-annotated datasets which are costly and time-consuming to create. V-Zero aims to overcome this limitation by enabling self-improvement with unlabeled data.

**Method:** V-Zero employs two roles, a Questioner and a Solver, in a co-evolutionary loop. The Questioner generates challenging questions using a dual-reward system, while the Solver is trained using pseudo-labels from its own responses. Role training is carried out using Group Relative Policy Optimization.

**Result:** V-Zero improved the performance of the Qwen2.5-VL-7B-Instruct model in visual mathematical reasoning by +1.7 points and in general vision-centric tasks by +2.6 points, all without the use of human-annotated data.

**Conclusion:** The V-Zero framework shows the potential for self-improvement in multimodal systems without relying on human-annotated datasets, suggesting a promising direction for future vision-language model development.

**Abstract:** Recent advances in multimodal learning have significantly enhanced the reasoning capabilities of vision-language models (VLMs). However, state-of-the-art approaches rely heavily on large-scale human-annotated datasets, which are costly and time-consuming to acquire. To overcome this limitation, we introduce V-Zero, a general post-training framework that facilitates self-improvement using exclusively unlabeled images. V-Zero establishes a co-evolutionary loop by instantiating two distinct roles: a Questioner and a Solver. The Questioner learns to synthesize high-quality, challenging questions by leveraging a dual-track reasoning reward that contrasts intuitive guesses with reasoned results. The Solver is optimized using pseudo-labels derived from majority voting over its own sampled responses. Both roles are trained iteratively via Group Relative Policy Optimization (GRPO), driving a cycle of mutual enhancement. Remarkably, without a single human annotation, V-Zero achieves consistent performance gains on Qwen2.5-VL-7B-Instruct, improving visual mathematical reasoning by +1.7 and general vision-centric by +2.6, demonstrating the potential of self-improvement in multimodal systems. Code is available at https://github.com/SatonoDia/V-Zero

</details>


### [69] [InfoSculpt: Sculpting the Latent Space for Generalized Category Discovery](https://arxiv.org/abs/2601.10098)
*Wenwen Liao,Hang Ruan,Jianbo Yu,Yuansong Wang,Qingchao Jiang,Xiaofeng Yang*

Main category: cs.CV

> 本文提出InfoSculpt框架，采用信息瓶颈原理和双条件互信息目标，解决如何从大规模无标签数据中分类已知和未知类别实例的问题。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有GCD方法主要依靠伪标签或两阶段聚类缺乏有效机制区分类别定义信号与实例特定噪声的问题，重新从信息论角度考虑GCD。

**Method:** 本文提出了一个名为InfoSculpt的新框架，该框架结合了标注数据的类别级条件互信息（CMI）和所有数据的实例级CMI，以分别学习已知类别的紧凑判别表示并压缩由数据增强引起的噪声。

**Result:** 研究表明通过最小化双条件互信息目标系统地雕刻表示空间的方法在八个基准测试上得到了验证，证明了其在从已知和未知类别中分类实例的能力。这种方法避免了伪标签或两阶段聚类的限制。

**Conclusion:** 实验在八个基准测试上验证了InfoSculpt方法在创建解纠缠和强大的潜在空间方面的有效性，该空间保留了分类信息，同时丢弃了噪音实例特定细节。

**Abstract:** Generalized Category Discovery (GCD) aims to classify instances from both known and novel categories within a large-scale unlabeled dataset, a critical yet challenging task for real-world, open-world applications. However, existing methods often rely on pseudo-labeling, or two-stage clustering, which lack a principled mechanism to explicitly disentangle essential, category-defining signals from instance-specific noise. In this paper, we address this fundamental limitation by re-framing GCD from an information-theoretic perspective, grounded in the Information Bottleneck (IB) principle. We introduce InfoSculpt, a novel framework that systematically sculpts the representation space by minimizing a dual Conditional Mutual Information (CMI) objective. InfoSculpt uniquely combines a Category-Level CMI on labeled data to learn compact and discriminative representations for known classes, and a complementary Instance-Level CMI on all data to distill invariant features by compressing augmentation-induced noise. These two objectives work synergistically at different scales to produce a disentangled and robust latent space where categorical information is preserved while noisy, instance-specific details are discarded. Extensive experiments on 8 benchmarks demonstrate that InfoSculpt validating the effectiveness of our information-theoretic approach.

</details>


### [70] [FlowAct-R1: Towards Interactive Humanoid Video Generation](https://arxiv.org/abs/2601.10103)
*Lizhen Wang,Yongming Zhu,Zhipeng Ge,Youwei Zheng,Longhao Zhang,Tianshu Hu,Shiyang Qin,Mingshuang Luo,Jiaxu Zhang,Xin Chen,Yulong Wang,Zerong Zheng,Jianwen Jiang,Chao Liang,Weifeng Chen,Xing Wang,Yuan Zhang,Mingyuan Gao*

Main category: cs.CV

> 本文介绍了FlowAct-R1框架，该框架能在实时交互中生成高保真的人类视频，采用分段扩散和自我强制策略，实现了在480p分辨率下的25fps输出，总体效果和泛化表现优异。

<details>
  <summary>Details</summary>

**Motivation:** 尽管视频合成技术有了显著进步，现有方法仍难以在高保真合成和实时交互需求之间取得平衡。本文旨在解决这一问题，提供高质量且实时交互的人类视频合成方法。

**Method:** 本文提出FlowAct-R1框架，专门用于实时交互式人类视频生成。该框架基于MMDiT架构，能够在保持低延迟响应的同时进行任意时长的视频流合成。为了在连续交互过程中减少错误累积并确保长期时间一致性，提出了分段扩散强制策略和一种新颖的自我强制变体。通过利用高效的蒸馏和系统级优化，框架实现了在480p分辨率下的25fps稳定输出，时间到首帧（TTFF）仅为约1.5秒。

**Result:** 实验结果表明，FlowAct-R1在交互场景中实现了卓越的行为生动性和感知真实性，同时在不同角色风格间保持了强大的泛化能力。

**Conclusion:** 本文提出的FlowAct-R1框架通过精细化且全面的全身控制，使得视频生成的高度逼真和实时交互成为可能。这是在保持动作自然过渡到多样化行为状态的同时，解决错误累积和长期时间一致性问题的重要进展。

**Abstract:** Interactive humanoid video generation aims to synthesize lifelike visual agents that can engage with humans through continuous and responsive video. Despite recent advances in video synthesis, existing methods often grapple with the trade-off between high-fidelity synthesis and real-time interaction requirements. In this paper, we propose FlowAct-R1, a framework specifically designed for real-time interactive humanoid video generation. Built upon a MMDiT architecture, FlowAct-R1 enables the streaming synthesis of video with arbitrary durations while maintaining low-latency responsiveness. We introduce a chunkwise diffusion forcing strategy, complemented by a novel self-forcing variant, to alleviate error accumulation and ensure long-term temporal consistency during continuous interaction. By leveraging efficient distillation and system-level optimizations, our framework achieves a stable 25fps at 480p resolution with a time-to-first-frame (TTFF) of only around 1.5 seconds. The proposed method provides holistic and fine-grained full-body control, enabling the agent to transition naturally between diverse behavioral states in interactive scenarios. Experimental results demonstrate that FlowAct-R1 achieves exceptional behavioral vividness and perceptual realism, while maintaining robust generalization across diverse character styles.

</details>


### [71] [MathDoc: Benchmarking Structured Extraction and Active Refusal on Noisy Mathematics Exam Papers](https://arxiv.org/abs/2601.10104)
*Chenyue Zhou,Jiayi Tuo,Shitong Qin,Wei Dai,Mingxuan Wang,Ziwei Zhao,Duoyang Li,Shiyang Su,Yanxi Lu,Yanbiao Ma*

Main category: cs.CV

> 我们引入了一个新的基准测试MathDoc，用于评估从真实高中数学考试卷中提取结构化问题的性能，并提出了一个评估框架。实验表明，尽管当前模型在某些方面表现良好，但在拒绝无法识别的输入方面仍有不足。

<details>
  <summary>Details</summary>

**Motivation:** 自动从基于纸张的数学考试中抽取结构化问题对于智能教育至关重要，但在实际环境中由于严重的视觉噪声依然具有挑战性。现有基准主要关注于干净文档或通用布局分析，忽略了数学问题的结构性以及模型主动拒绝不完整输入的能力。

**Method:** 我们提出MathDoc，这是首个用于从真实的高中数学考试卷中进行文档级信息抽取的基准。MathDoc包含3,609个精心挑选的带有现实世界噪音的问题，并明确包括了一些无法识别的样本，以评估模型的主动拒绝能力。我们还提出了一种多维度的评估框架，涵盖了题干准确性、视觉相似性和拒绝能力。

**Result:** 实验结果表明，尽管最先进的多模态语言模型（如Qwen3-VL和Gemini-2.5-Pro）在抽取性能方面表现强劲，但在识别不可读输入时的表现不佳，通常产生自信但无效的输出。

**Conclusion:** 这些结果突显了当前多模态语言模型存在的关键缺陷，并确立了MathDoc作为评估模型在退化文档条件下可靠性的基准。

**Abstract:** The automated extraction of structured questions from paper-based mathematics exams is fundamental to intelligent education, yet remains challenging in real-world settings due to severe visual noise. Existing benchmarks mainly focus on clean documents or generic layout analysis, overlooking both the structural integrity of mathematical problems and the ability of models to actively reject incomplete inputs. We introduce MathDoc, the first benchmark for document-level information extraction from authentic high school mathematics exam papers. MathDoc contains \textbf{3,609} carefully curated questions with real-world artifacts and explicitly includes unrecognizable samples to evaluate active refusal behavior. We propose a multi-dimensional evaluation framework covering stem accuracy, visual similarity, and refusal capability. Experiments on SOTA MLLMs, including Qwen3-VL and Gemini-2.5-Pro, show that although end-to-end models achieve strong extraction performance, they consistently fail to refuse illegible inputs, instead producing confident but invalid outputs. These results highlight a critical gap in current MLLMs and establish MathDoc as a benchmark for assessing model reliability under degraded document conditions. Our project repository is available at \href{https://github.com/winnk123/papers/tree/master}{GitHub repository}

</details>


### [72] [Enhancing Visual In-Context Learning by Multi-Faceted Fusion](https://arxiv.org/abs/2601.10107)
*Wenwen Liao,Jianbo Yu,Yuansong Wang,Qingchao Jiang,Xiaofeng Yang*

Main category: cs.CV

> 本文提出了一个多组合协作融合框架，通过生成三个由不同组合的高质量提示构成的上下文表示分支，来改进现有的"检索然后提示"方法。这种方法在多种任务上展示了强大的跨任务泛化能力和更精确的预测。

<details>
  <summary>Details</summary>

**Motivation:** 现有的"检索然后提示"方法通常只选择单一最佳视觉提示，这往往会丢弃其他候选者的有价值的上下文信息。即使将前K个提示融合为一个增强表示，依然限制了模型的推理能力。我们提出的方法旨在利用多方面的信息协作融合，以充分发挥这些多样化上下文的潜力。

**Method:** 我们的方法生成了三个上下文表示分支，每个分支都是由不同组合的高质量提示构成。这些互补的指导信号被输入到我们设计的MULTI-VQGAN架构中，该架构能够联合解释和利用多个来源的协作信息。

**Result:** 广泛的实验表明，该方法在前景分割，单对象检测和图像着色等多种任务上，表现出比现有方法更强大的跨任务泛化能力和更精确的预测能力。

**Conclusion:** 研究表明，多组合协作融合框架可以提升模型的跨任务泛化能力和精确性，适合作为视觉上下文学习的一种改善策略。

**Abstract:** Visual In-Context Learning (VICL) has emerged as a powerful paradigm, enabling models to perform novel visual tasks by learning from in-context examples. The dominant "retrieve-then-prompt" approach typically relies on selecting the single best visual prompt, a practice that often discards valuable contextual information from other suitable candidates. While recent work has explored fusing the top-K prompts into a single, enhanced representation, this still simply collapses multiple rich signals into one, limiting the model's reasoning capability. We argue that a more multi-faceted, collaborative fusion is required to unlock the full potential of these diverse contexts. To address this limitation, we introduce a novel framework that moves beyond single-prompt fusion towards an multi-combination collaborative fusion. Instead of collapsing multiple prompts into one, our method generates three contextual representation branches, each formed by integrating information from different combinations of top-quality prompts. These complementary guidance signals are then fed into proposed MULTI-VQGAN architecture, which is designed to jointly interpret and utilize collaborative information from multiple sources. Extensive experiments on diverse tasks, including foreground segmentation, single-object detection, and image colorization, highlight its strong cross-task generalization, effective contextual fusion, and ability to produce more robust and accurate predictions than existing methods.

</details>


### [73] [Beyond Single Prompts: Synergistic Fusion and Arrangement for VICL](https://arxiv.org/abs/2601.10117)
*Wenwen Liao,Jianbo Yu,Yuansong Wang,Shifu Yan,Xiaofeng Yang*

Main category: cs.CV

> 本文提出了一个端到端的VICL框架，通过自适应融合模块和特定排列的轻量级MLP及双向微调机制，改善了视觉任务适应性和任务性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法在视觉任务快速适应方面存在两个关键问题：仅选择最相似的提示而忽略了其他高质量提示提供的互补线索，以及未能充分利用不同提示排列所隐含的结构化信息。本文旨在解决这些问题，并提升跨任务泛化能力。

**Method:** 本文提出了一种端到端的VICL框架，该框架包含一个自适应融合模块，能够从多个提示中综合关键模式和标注，以形成更精准的上下文提示；同时引入了特定排列的轻量级MLP来分离布局先验和核心模型。此外，还设计了一种双向微调机制，通过交换查询和提示的角色，增强了融合模块和插图模型之间的协作。

**Result:** 实验结果表明，本文方法在前景分割、单目标检测和图像着色等任务上表现出优越的性能和跨任务泛化能力。

**Conclusion:** 实验结果支持了所提方法的有效性，证明其端到端的VICL框架不仅改善了现有方法的不足之处，还提升了任务性能和跨任务的泛化能力。

**Abstract:** Vision In-Context Learning (VICL) enables inpainting models to quickly adapt to new visual tasks from only a few prompts. However, existing methods suffer from two key issues: (1) selecting only the most similar prompt discards complementary cues from other high-quality prompts; and (2) failing to exploit the structured information implied by different prompt arrangements.
  We propose an end-to-end VICL framework to overcome these limitations. Firstly, an adaptive Fusion Module aggregates critical patterns and annotations from multiple prompts to form more precise contextual prompts. Secondly, we introduce arrangement-specific lightweight MLPs to decouple layout priors from the core model, while minimally affecting the overall model. In addition, an bidirectional fine-tuning mechanism swaps the roles of query and prompt, encouraging the model to reconstruct the original prompt from fused context and thus enhancing collaboration between the fusion module and the inpainting model. Experiments on foreground segmentation, single-object detection, and image colorization demonstrate superior results and strong cross-task generalization of our method.

</details>


### [74] [VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2601.10124)
*Sicheng Yang,Zhaohu Xing,Lei Zhu*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Consistency learning with feature perturbation is a widely used strategy in semi-supervised medical image segmentation. However, many existing perturbation methods rely on dropout, and thus require a careful manual tuning of the dropout rate, which is a sensitive hyperparameter and often difficult to optimize and may lead to suboptimal regularization. To overcome this limitation, we propose VQ-Seg, the first approach to employ vector quantization (VQ) to discretize the feature space and introduce a novel and controllable Quantized Perturbation Module (QPM) that replaces dropout. Our QPM perturbs discrete representations by shuffling the spatial locations of codebook indices, enabling effective and controllable regularization. To mitigate potential information loss caused by quantization, we design a dual-branch architecture where the post-quantization feature space is shared by both image reconstruction and segmentation tasks. Moreover, we introduce a Post-VQ Feature Adapter (PFA) to incorporate guidance from a foundation model (FM), supplementing the high-level semantic information lost during quantization. Furthermore, we collect a large-scale Lung Cancer (LC) dataset comprising 828 CT scans annotated for central-type lung carcinoma. Extensive experiments on the LC dataset and other public benchmarks demonstrate the effectiveness of our method, which outperforms state-of-the-art approaches. Code available at: https://github.com/script-Yang/VQ-Seg.

</details>


### [75] [LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning](https://arxiv.org/abs/2601.10129)
*Linquan Wu,Tianxiang Jiang,Yifei Dong,Haoyu Yang,Fengji Zhang,Shichaang Meng,Ai Xuan,Linqi Song,Jacky Keung*

Main category: cs.CV

> LaViT框架用于多模态隐式推理，改善视觉基础，提高复杂推理任务上的性能。

<details>
  <summary>Details</summary>

**Motivation:** 当前的多模态隐式推理往往依赖于外部监督，忽视了内在视觉注意力动态。本研究发现学生模型在模仿教师的文本输出时，关注的是根本不同的视觉区域，更多地依赖于语言先验而不是基于感知的。

**Method:** 提出了LaViT框架，该框架通过自动回归方式重建教师模型的视觉语义和注意力轨迹，以在文本生成之前对齐潜隐视觉思维，而非静态嵌入。使用课程感官门控机制防止走捷径学习。

**Result:** 实验表明，LaViT显著增强了视觉基础能力，在复杂推理任务上获得了高达+16.9%的增益，使一个紧凑的30亿参数模型能够超越更大规模的开源变体和专有模型，如GPT-4o。

**Conclusion:** LaViT通过其独特的视觉和注意力轨迹重建方法，成功解决了多模态模型中的感知差距问题，提升了模型的性能。

**Abstract:** Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher's textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher's visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o.

</details>


### [76] [Advancing Adaptive Multi-Stage Video Anomaly Reasoning: A Benchmark Dataset and Method](https://arxiv.org/abs/2601.10165)
*Chao Huang,Benfeng Wang,Wei Wang,Jie Wen,Li Shen,Wenqi Ren,Yong Xu,Xiaochun Cao*

Main category: cs.CV

> 研究引入了视频异常推理（VAR）的任务，提出了一种大规模的新数据集，并开发了一个能够进行多阶段自适应推理和风险感知决策的MLLM模型Vad-R1-Plus，以改进视频异常检测和理解。

<details>
  <summary>Details</summary>

**Motivation:** 为了填补现有MLLM方法在视频异常检测和理解领域缺乏明确推理过程、风险意识及决策导向解释的空白，研究提出了新的任务VAR。

**Method:** 研究构建了一个包含8,641个视频的新数据集，通过结构化的感知-认知-行动链思维（PerCoAct-CoT）来标注不同推理深度的问题，并提出了具有风险感知决策能力的MLLM模型Vad-R1-Plus。

**Result:** 实验结果表明，基于新任务和数据集提出的基准和方法显著提升了MLLM在VAR任务上的推理能力，优于开源和专有基线。

**Conclusion:** 研究通过引入VAR任务及配套的数据集，展示了在视频异常理解中进行多阶段推理和风险感知决策的有效方法，推动了MLLM在这一领域的推理能力。

**Abstract:** Recent progress in reasoning capabilities of Multimodal Large Language Models(MLLMs) has highlighted their potential for performing complex video understanding tasks. However, in the domain of Video Anomaly Detection and Understanding (VAD&U), existing MLLM-based methods are largely limited to anomaly localization or post-hoc description, lacking explicit reasoning processes, risk awareness, and decision-oriented interpretation. To address this gap, we define a new task termed Video Anomaly Reasoning (VAR), which elevates video anomaly analysis from descriptive understanding to structured, multi-stage reasoning. VAR explicitly requires models to perform progressive reasoning over anomalous events before answering anomaly-related questions, encompassing visual perception, causal interpretation, and risk-aware decision making. To support this task, we present a new dataset with 8,641 videos, where each video is annotated with diverse question types corresponding to different reasoning depths, totaling more than 50,000 samples, making it one of the largest datasets for video anomaly. The annotations are based on a structured Perception-Cognition-Action Chain-of-Thought (PerCoAct-CoT), which formalizes domain-specific reasoning priors for video anomaly understanding. This design enables systematic evaluation of multi-stage and adaptive anomaly reasoning. In addition, we propose Anomaly-Aware Group Relative Policy Optimization to further enhance reasoning reliability under weak supervision. Building upon the proposed task and dataset, we develop an end-to-end MLLM-based VAR model termed Vad-R1-Plus, which supports adaptive hierarchical reasoning and risk-aware decision making. Extensive experiments demonstrate that the proposed benchmark and method effectively advance the reasoning capabilities of MLLMs on VAR tasks, outperforming both open-source and proprietary baselines.

</details>


### [77] [RAG-3DSG: Enhancing 3D Scene Graphs with Re-Shot Guided Retrieval-Augmented Generation](https://arxiv.org/abs/2601.10168)
*Yue Chang,Rufeng Chen,Zhaofan Zhang,Yi Chen,Sihong Xie*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Open-vocabulary 3D Scene Graph (3DSG) generation can enhance various downstream tasks in robotics, such as manipulation and navigation, by leveraging structured semantic representations. A 3DSG is constructed from multiple images of a scene, where objects are represented as nodes and relationships as edges. However, existing works for open-vocabulary 3DSG generation suffer from both low object-level recognition accuracy and speed, mainly due to constrained viewpoints, occlusions, and redundant surface density. To address these challenges, we propose RAG-3DSG to mitigate aggregation noise through re-shot guided uncertainty estimation and support object-level Retrieval-Augmented Generation (RAG) via reliable low-uncertainty objects. Furthermore, we propose a dynamic downsample-mapping strategy to accelerate cross-image object aggregation with adaptive granularity. Experiments on Replica dataset demonstrate that RAG-3DSG significantly improves node captioning accuracy in 3DSG generation while reducing the mapping time by two-thirds compared to the vanilla version.

</details>


### [78] [From Physical Degradation Models to Task-Aware All-in-One Image Restoration](https://arxiv.org/abs/2601.10192)
*Hu Gao,Xiaoning Lei,Xichen Xu,Xingjian Wang,Lizhuang Ma*

Main category: cs.CV

> 本文提出了一种高效全功能图像复原方法OPIR，通过物理退化建模预测任务感知逆算子进行图像复原，并在实验中表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 旨在通过单一训练的模型自适应处理多个复原任务，尽管现有方法通过引入提示信息或利用大模型已取得显著效果，但附加的学习模块增加了系统复杂性，阻碍了实时适用性。

**Method:** 采用物理退化建模视角，预测任务感知的逆退化算子以实现高效的全功能图像复原。该框架包含两个阶段：第一阶段，预测的逆算子生成初始复原图像及不确定性感知图，该图强调难以重建的区域，确保复原的可靠性；第二阶段，在不确定性图的引导下进一步细化复原。在两个阶段中均使用相同的逆算子预测网络，并在算子预测后引入任务感知参数，以适应不同的退化任务。通过加速逆算子的卷积运算，所提方法实现了高效的全功能图像复原。

**Result:** 通过广泛实验验证，该方法实现了高效的全功能图像复原，同时在任务对齐的复原中表现出极高的竞争力。

**Conclusion:** 提出的紧密集成的架构OPIR通过广泛的实验验证，在实现优越的全功能复原性能的同时，还在任务对齐的复原中表现出极强的竞争力。

**Abstract:** All-in-one image restoration aims to adaptively handle multiple restoration tasks with a single trained model. Although existing methods achieve promising results by introducing prompt information or leveraging large models, the added learning modules increase system complexity and hinder real-time applicability. In this paper, we adopt a physical degradation modeling perspective and predict a task-aware inverse degradation operator for efficient all-in-one image restoration. The framework consists of two stages. In the first stage, the predicted inverse operator produces an initial restored image together with an uncertainty perception map that highlights regions difficult to reconstruct, ensuring restoration reliability. In the second stage, the restoration is further refined under the guidance of this uncertainty map. The same inverse operator prediction network is used in both stages, with task-aware parameters introduced after operator prediction to adapt to different degradation tasks. Moreover, by accelerating the convolution of the inverse operator, the proposed method achieves efficient all-in-one image restoration. The resulting tightly integrated architecture, termed OPIR, is extensively validated through experiments, demonstrating superior all-in-one restoration performance while remaining highly competitive on task-aligned restoration.

</details>


### [79] [ELITE: Efficient Gaussian Head Avatar from a Monocular Video via Learned Initialization and TEst-time Generative Adaptation](https://arxiv.org/abs/2601.10200)
*Kim Youwang,Lee Hyoseok,Subin Park,Gerard Pons-Moll,Tae-Hyun Oh*

Main category: cs.CV

> ELITE是一种通过学习初始化和测试时间生成性适应来从单目视频中高效合成高保真、可动画的人脸角色的方法，具有强泛化能力和显著的性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是为了克服以前使用3D数据先验或2D生成性先验所限制的单目视频中缺失视觉线索的困难。3D数据先验方法往往难以泛化，而2D生成性先验方法则计算成本高且容易身份幻化。通过两者的互补优势，ELITE希望实现高保真人脸角色的合成。

**Method:** ELITE采用了一种通过学习初始化和测试时间生成性适应来从单目视频中高效合成高斯人脸角色的方法。具体来说，它设计了一个前馈Mesh2Gaussian先验模型（MGPM）用于快速初始化高斯角色，并且设计了一个测试时间的生成性适应阶段来缩小领域差异，该阶段利用真实和合成图像作为监督。为了恢复丢失的视觉细节，ELITE提出了一种基于高斯角色渲染的渲染引导单步扩散增强方法，代替了之前那些速度慢且容易生成幻觉的全扩散降噪策略。

**Result:** 实验表明，ELITE生成的虚拟角色在视觉上优于以往的方法，即使对于复杂的表情也是这样。同时，其合成速度比2D生成性先验方法快60倍。

**Conclusion:** ELITE能够提供高质量的虚拟角色合成，同时保持了高效率，解决了3D数据和2D生成性先验方法的局限性，展现了很好的应用前景。

**Abstract:** We introduce ELITE, an Efficient Gaussian head avatar synthesis from a monocular video via Learned Initialization and TEst-time generative adaptation. Prior works rely either on a 3D data prior or a 2D generative prior to compensate for missing visual cues in monocular videos. However, 3D data prior methods often struggle to generalize in-the-wild, while 2D generative prior methods are computationally heavy and prone to identity hallucination. We identify a complementary synergy between these two priors and design an efficient system that achieves high-fidelity animatable avatar synthesis with strong in-the-wild generalization. Specifically, we introduce a feed-forward Mesh2Gaussian Prior Model (MGPM) that enables fast initialization of a Gaussian avatar. To further bridge the domain gap at test time, we design a test-time generative adaptation stage, leveraging both real and synthetic images as supervision. Unlike previous full diffusion denoising strategies that are slow and hallucination-prone, we propose a rendering-guided single-step diffusion enhancer that restores missing visual details, grounded on Gaussian avatar renderings. Our experiments demonstrate that ELITE produces visually superior avatars to prior works, even for challenging expressions, while achieving 60x faster synthesis than the 2D generative prior method.

</details>


### [80] [Beyond Inpainting: Unleash 3D Understanding for Precise Camera-Controlled Video Generation](https://arxiv.org/abs/2601.10214)
*Dong-Yu Chen,Yixin Guo,Shuojin Yang,Tai-Jiang Mu,Shi-Min Hu*

Main category: cs.CV

> 提出了DepthDirector，一个精确控制相机的视频重渲染框架，通过利用3D先验和几何引导信号，改善了原视频的生成质量。

<details>
  <summary>Details</summary>

**Motivation:** 尽管条件视频生成中的相机控制已被广泛研究，但精确改变相机轨迹同时忠实地保持视频内容仍然是一个具有挑战性的任务。主流的实现精确相机控制的方法是根据目标轨迹来扭曲3D表示，但是这些方法未能充分利用视频扩散模型（VDM）的3D先验，容易陷入修补陷阱，导致主题不一致和生成质量下降。为了应对这个问题，我们提出了DepthDirector这个视频重渲染框架。

**Method:** 通过利用来自显式3D表示的深度视频作为相机控制指导，我们提出了一种名为DepthDirector的视频重渲染框架，该框架具有精确的相机控制能力。具体来说，我们设计了一种视图-内容双重流条件机制，该机制将源视频和在目标视点下渲染的深度序列同时注入到预训练的视频生成模型中。这种几何引导信号使VDM能够理解相机移动并利用其3D理解能力，从而实现精确的相机控制和一致的内容生成。此外，我们引入了一种轻量级的LoRA基础的视频扩散适配器来训练我们的框架。

**Result:** 创建了一个大规模的多相机同步数据集MultiCam-WarpData，使用Unreal Engine 5创建，包含1K动态场景的8K视频。大量实验表明，DepthDirector在相机可控制性和视觉质量上都优于现有的方法。

**Conclusion:** 实验表明，DepthDirector不仅在相机可控制性上比现有方法更优秀，而且在视觉质量上也表现更好。代码和数据集将公开可用。

**Abstract:** Camera control has been extensively studied in conditioned video generation; however, performing precisely altering the camera trajectories while faithfully preserving the video content remains a challenging task. The mainstream approach to achieving precise camera control is warping a 3D representation according to the target trajectory. However, such methods fail to fully leverage the 3D priors of video diffusion models (VDMs) and often fall into the Inpainting Trap, resulting in subject inconsistency and degraded generation quality. To address this problem, we propose DepthDirector, a video re-rendering framework with precise camera controllability. By leveraging the depth video from explicit 3D representation as camera-control guidance, our method can faithfully reproduce the dynamic scene of an input video under novel camera trajectories. Specifically, we design a View-Content Dual-Stream Condition mechanism that injects both the source video and the warped depth sequence rendered under the target viewpoint into the pretrained video generation model. This geometric guidance signal enables VDMs to comprehend camera movements and leverage their 3D understanding capabilities, thereby facilitating precise camera control and consistent content generation. Next, we introduce a lightweight LoRA-based video diffusion adapter to train our framework, fully preserving the knowledge priors of VDMs. Additionally, we construct a large-scale multi-camera synchronized dataset named MultiCam-WarpData using Unreal Engine 5, containing 8K videos across 1K dynamic scenes. Extensive experiments show that DepthDirector outperforms existing methods in both camera controllability and visual quality. Our code and dataset will be publicly available.

</details>


### [81] [Optimizing Multimodal LLMs for Egocentric Video Understanding: A Solution for the HD-EPIC VQA Challenge](https://arxiv.org/abs/2601.10228)
*Sicheng Yang,Yukai Huang,Shitong Sun,Weitong Cai,Jiankang Deng,Jifei Song,Zhensong Zhang*

Main category: cs.CV

> 本文提出了一套综合框架以提升多模态大型语言模型在复杂视频QA基准任务上的性能。

<details>
  <summary>Details</summary>

**Motivation:** 大型多模态语言模型在处理如HD-EPIC VQA等复杂视频问题回答基准时遇到挑战，包括模糊的查询/选项、糟糕的长时段推理能力和非标准的输出。

**Method:** 我们提出了一种框架，该框架整合了查询/选项预处理、领域特定的Qwen2.5-VL微调、用于多步骤推理的新颖时间链推理(T-CoT)提示，以及鲁棒的后处理。

**Result:** 该系统在HD-EPIC VQA上实现了41.6%的准确率。

**Conclusion:** 这一成果展示了对苛求视频理解任务进行全面管道优化的必要性。

**Abstract:** Multimodal Large Language Models (MLLMs) struggle with complex video QA benchmarks like HD-EPIC VQA due to ambiguous queries/options, poor long-range temporal reasoning, and non-standardized outputs. We propose a framework integrating query/choice pre-processing, domain-specific Qwen2.5-VL fine-tuning, a novel Temporal Chain-of-Thought (T-CoT) prompting for multi-step reasoning, and robust post-processing. This system achieves 41.6% accuracy on HD-EPIC VQA, highlighting the need for holistic pipeline optimization in demanding video understanding. Our code, fine-tuned models are available at https://github.com/YoungSeng/Egocentric-Co-Pilot.

</details>


### [82] [Attend to what I say: Highlighting relevant content on slides](https://arxiv.org/abs/2601.10244)
*Megha Mariam K M,C. V. Jawahar*

Main category: cs.CV

> 一种方法能够自动识别并突出演讲中幻灯片最重要的部分，通过匹配演讲内容和幻灯片内容，实现更好的听觉和视觉同步，从而减少认知压力，提高理解力。

<details>
  <summary>Details</summary>

**Motivation:** 在听取演讲的同时，试图从幻灯片中找到相关信息是一件挑战性的事情。听者在关注幻灯片的一部分时，演讲者可能会转到新的内容，导致视觉上的不匹配，这使得难以吸收关键细节。这种方法旨在通过减少认知压力和提高理解力，更好地同步听者听到的内容和他们需要关注的内容。

**Method:** 介绍了一种方法，该方法能够根据演讲者的叙述自动识别并突出幻灯片中最重要的区域。通过分析口头内容并与幻灯片中的文本或图形元素相匹配，这种方法确保了听者听到的内容与他们需要关注的内容之间的更好同步。

**Result:** 探索了解决此问题的不同方法，并评估了它们的成功案例和失败案例。这种方法有助于更好地理解和吸收丰富内容的视频，如教育视频和学术会议中的演讲。

**Conclusion:** 多媒体文档的分析作为理解内容丰富的视频的关键需求正在浮现，这种方法通过减少认知负担和提高理解力，能够帮助听众更好地跟随和理解内容。

**Abstract:** Imagine sitting in a presentation, trying to follow the speaker while simultaneously scanning the slides for relevant information. While the entire slide is visible, identifying the relevant regions can be challenging. As you focus on one part of the slide, the speaker moves on to a new sentence, leaving you scrambling to catch up visually. This constant back-and-forth creates a disconnect between what is being said and the most important visual elements, making it hard to absorb key details, especially in fast-paced or content-heavy presentations such as conference talks. This requires an understanding of slides, including text, graphics, and layout. We introduce a method that automatically identifies and highlights the most relevant slide regions based on the speaker's narrative. By analyzing spoken content and matching it with textual or graphical elements in the slides, our approach ensures better synchronization between what listeners hear and what they need to attend to. We explore different ways of solving this problem and assess their success and failure cases. Analyzing multimedia documents is emerging as a key requirement for seamless understanding of content-rich videos, such as educational videos and conference talks, by reducing cognitive strain and improving comprehension. Code and dataset are available at: https://github.com/meghamariamkm2002/Slide_Highlight

</details>


### [83] [DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset](https://arxiv.org/abs/2601.10305)
*Hengyu Shen,Tiancheng Gu,Bin Qin,Lan Wu,Yuling Wu,Shuo Tan,Zelong Sun,Jun Wang,Nan Wu,Xiang An,Weidong Cai,Ziyong Feng,Kaicheng Yang*

Main category: cs.CV

> DanQing, a Chinese dataset of 100 million image-text pairs from 2024-2025 web data, enhances the performance of VLP models in Chinese tasks.

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of high-quality Chinese image-text datasets for Vision-Language Pre-training (VLP) models.

**Method:** DanQing, a high-quality Chinese cross-modal dataset, is constructed using a rigorous pipeline involving the collection of 100 million image-text pairs from 2024-2025 web data. This captures evolving semantic trends better than existing datasets.

**Result:** DanQing demonstrates superior performance in various Chinese downstream tasks such as zero-shot classification, cross-modal retrieval, and LMM-based evaluations.

**Conclusion:** The release of DanQing under the Creative Common CC-BY 4.0 license supports the research community in advancing Chinese vision-language pretraining.

**Abstract:** Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.

</details>


### [84] [Hierarchical Refinement of Universal Multimodal Attacks on Vision-Language Models](https://arxiv.org/abs/2601.10313)
*Peng-Fei Zhang,Zi Huang*

Main category: cs.CV

> Introduce Hierarchical Refinement Attack (HRA) to refine universal adversarial perturbations in both image and text modalities at sample and optimization levels, leading to more efficient universal adversarial attacks for VLP models.

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing adversarial attacks for VLP models, which are mostly sample-specific and thus require significant computational resources in large datasets or new scenarios.

**Method:** Hierarchical Refinement Attack (HRA), a multimodal universal attack framework for VLP models, including refining UAPs at the sample and optimization levels, ScMix augmentation, and global words identification for text modality.

**Result:** Experimental results show the proposed multimodal attacks' effectiveness in diverse downstream tasks, VLP models, and datasets.

**Conclusion:** The HRA method, with its innovative approaches for image and text modality attacks, demonstrates superiority over existing methods in creating efficient universal adversarial attacks for VLP models.

**Abstract:** Existing adversarial attacks for VLP models are mostly sample-specific, resulting in substantial computational overhead when scaled to large datasets or new scenarios. To overcome this limitation, we propose Hierarchical Refinement Attack (HRA), a multimodal universal attack framework for VLP models. HRA refines universal adversarial perturbations (UAPs) at both the sample level and the optimization level. For the image modality, we disentangle adversarial examples into clean images and perturbations, allowing each component to be handled independently for more effective disruption of cross-modal alignment. We further introduce a ScMix augmentation strategy that diversifies visual contexts and strengthens both global and local utility of UAPs, thereby reducing reliance on spurious features. In addition, we refine the optimization path by leveraging a temporal hierarchy of historical and estimated future gradients to avoid local minima and stabilize universal perturbation learning. For the text modality, HRA identifies globally influential words by combining intra-sentence and inter-sentence importance measures, and subsequently utilizes these words as universal text perturbations. Extensive experiments across various downstream tasks, VLP models, and datasets demonstrate the superiority of the proposed universal multimodal attacks.

</details>


### [85] [ROMA: Real-time Omni-Multimodal Assistant with Interactive Streaming Understanding](https://arxiv.org/abs/2601.10323)
*Xueyun Tian,Wei Li,Bingbing Xu,Heng Dong,Yuanzhuo Wang,Huawei Shen*

Main category: cs.CV

> ROMA 是一个实时的全方位多模态助手，能够处理音频流和视频流的同步理解，实现反应式和主动式交互。通过特殊的训练方法，ROMA 在主动式任务上达到了顶尖水平，并在反应式任务上表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有的多模态模型在处理流式音视频理解时能力分散的问题，即这些模型通常支持的模态不完整或缺乏自主主动性。

**Method:** ROMA 采用轻量级的 speak head 来解耦响应触发过程，同时使用精心设计的流式数据集和两阶段的训练策略来优化模型对流式格式的适应性和主动性响应。

**Result:** 实验结果显示，ROMA 在12个基准测试中实现了主动式任务上的顶级性能和反应式任务上的竞争力，证明了其在实时多模态统一理解中的有效性。

**Conclusion:** ROMA 成功解决了流式音视频理解中的挑战，为实现综合的反应式和主动式交互提供了一个强大的工具。

**Abstract:** Recent Omni-multimodal Large Language Models show promise in unified audio, vision, and text modeling. However, streaming audio-video understanding remains challenging, as existing approaches suffer from disjointed capabilities: they typically exhibit incomplete modality support or lack autonomous proactive monitoring. To address this, we present ROMA, a real-time omni-multimodal assistant for unified reactive and proactive interaction. ROMA processes continuous inputs as synchronized multimodal units, aligning dense audio with discrete video frames to handle granularity mismatches. For online decision-making, we introduce a lightweight speak head that decouples response initiation from generation to ensure precise triggering without task conflict. We train ROMA with a curated streaming dataset and a two-stage curriculum that progressively optimizes for streaming format adaptation and proactive responsiveness. To standardize the fragmented evaluation landscape, we reorganize diverse benchmarks into a unified suite covering both proactive (alert, narration) and reactive (QA) settings. Extensive experiments across 12 benchmarks demonstrate ROMA achieves state-of-the-art performance on proactive tasks while competitive in reactive settings, validating its robustness in unified real-time omni-multimodal understanding.

</details>


### [86] [SRAW-Attack: Space-Reweighted Adversarial Warping Attack for SAR Target Recognition](https://arxiv.org/abs/2601.10324)
*Yiming Zhang,Weibo Qin,Yuntian Liu,Feng Wang*

Main category: cs.CV

> 提出了一种新的对抗攻击方法SRAW，该方法通过优化的空间变形生成对抗样本，显著降低了SAR-ATR模型的性能，同时具有更好的隐蔽性和对抗迁移性。

<details>
  <summary>Details</summary>

**Motivation:** 虽然基于深度神经网络的SAR自动目标识别系统得到了广泛应用，但它们容易受到对抗样本的影响，并且过度依赖背景区域，导致对抗鲁棒性差。现有的SAR-ATR对抗攻击方法通常需要明显的视觉失真来实现有效的性能，因此需要一种在有效性和隐蔽性之间平衡的攻击方法。

**Method:** 空间重加权对抗变形(SRAW)方法，通过优化的空间变形和在前景和背景区域上的重新加权预算来生成对抗样本。

**Result:** 实验表明，SRAW在隐身性和对抗迁移性方面显著优于现有的方法，并使得最先进的SAR-ATR模型性能大幅下降。

**Conclusion:** SRAW方法能够生成高效的隐蔽性对抗样本，来挑战现有的SAR-ATR系统的鲁棒性。

**Abstract:** Synthetic aperture radar (SAR) imagery exhibits intrinsic information sparsity due to its unique electromagnetic scattering mechanism. Despite the widespread adoption of deep neural network (DNN)-based SAR automatic target recognition (SAR-ATR) systems, they remain vulnerable to adversarial examples and tend to over-rely on background regions, leading to degraded adversarial robustness. Existing adversarial attacks for SAR-ATR often require visually perceptible distortions to achieve effective performance, thereby necessitating an attack method that balances effectiveness and stealthiness. In this paper, a novel attack method termed Space-Reweighted Adversarial Warping (SRAW) is proposed, which generates adversarial examples through optimized spatial deformation with reweighted budgets across foreground and background regions. Extensive experiments demonstrate that SRAW significantly degrades the performance of state-of-the-art SAR-ATR models and consistently outperforms existing methods in terms of imperceptibility and adversarial transferability. Code is made available at https://github.com/boremycin/SAR-ATR-TransAttack.

</details>


### [87] [Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders](https://arxiv.org/abs/2601.10332)
*Siqi Kou,Jiachun Jin,Zetong Zhou,Ye Ma,Yugang Wang,Quan Chen,Peng Jiang,Xiao Yang,Jun Zhu,Kai Yu,Zhijie Deng*

Main category: cs.CV

> 本研究提出了一种新的文本到图像生成方法，通过激活文本编码器的推理和重写特性，以及对扩散系统的优化，提高了生成图像的质量和一致性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的文本到图像生成模型大多将大规模语言模型只用作文本编码器，未能充分利用其推理能力来推断给定文本提示应描绘的视觉内容。

**Method:** 提出了一种名为think-then-generate (T2G)的方法，首先通过轻量级的监督微调过程激活LLM文本编码器的思考-重写模式，接下来通过Dual-GRPO实现文本编码器和扩散系统的同时优化。文本编码器通过基于图像的奖励进行强化，从而能够推理和回忆世界知识；扩散系统则被优化以生成语义一致且视觉连贯的图像。

**Result:** 实验证明，在基于推理的图像生成和编辑基准测试中，模型在事实一致性、语义对齐和视觉真实性等方面都取得了显著的提升，达到了0.79的WISE评分，与GPT-4的得分大致相当。

**Conclusion:** 研究结果代表了向具有推理、表达和示范能力的下一代统一模型迈进的重要一步。

**Abstract:** Recent progress in text-to-image (T2I) diffusion models (DMs) has enabled high-quality visual synthesis from diverse textual prompts. Yet, most existing T2I DMs, even those equipped with large language model (LLM)-based text encoders, remain text-pixel mappers -- they employ LLMs merely as text encoders, without leveraging their inherent reasoning capabilities to infer what should be visually depicted given the textual prompt. To move beyond such literal generation, we propose the think-then-generate (T2G) paradigm, where the LLM-based text encoder is encouraged to reason about and rewrite raw user prompts; the states of the rewritten prompts then serve as diffusion conditioning. To achieve this, we first activate the think-then-rewrite pattern of the LLM encoder with a lightweight supervised fine-tuning process. Subsequently, the LLM encoder and diffusion backbone are co-optimized to ensure faithful reasoning about the context and accurate rendering of the semantics via Dual-GRPO. In particular, the text encoder is reinforced using image-grounded rewards to infer and recall world knowledge, while the diffusion backbone is pushed to produce semantically consistent and visually coherent images. Experiments show substantial improvements in factual consistency, semantic alignment, and visual realism across reasoning-based image generation and editing benchmarks, achieving 0.79 on WISE score, nearly on par with GPT-4. Our results constitute a promising step toward next-generation unified models with reasoning, expression, and demonstration capacities.

</details>


### [88] [An analytic theory of convolutional neural network inverse problems solvers](https://arxiv.org/abs/2601.10334)
*Minh Hai Nguyen,Quoc Bao Do,Edouard Pauwels,Pierre Weiss*

Main category: cs.CV

> 文章通过分析训练好的卷积神经网络，提出并验证了一种名为局部等变最小均方误差（LE-MMSE）的公式，解决了CNNs在成像反问题中理论理解不足的问题。

<details>
  <summary>Details</summary>

**Motivation:** 解决监督卷积神经网络（CNNs）在成像反问题上的应用，尽管其在实际操作中有很好的结果，但其所基于理论理解不足，被视为黑箱操作的问题。

**Method:** 通过分析训练好的神经网络，以其最小均方误差（MMSE）估计器的角度出发，结合卷积神经网络（CNNs）的两种基本归纳偏差：平移等变性和局部性，得到分析上可解释且又有实际意义的公式，这个限制版本的公式被命名为局部等变最小均方误差（LE-MMSE）公式。

**Result:** 通过广泛的数值实验，包括各种反问题（去噪、插补、退卷积），多种数据集（FFHQ、CIFAR-10、FashionMNIST），多种架构（U-Net、ResNet、PatchMLP）上展示，理论与神经网络输出匹配（PSNR ≳ 25dB）。

**Conclusion:** 文章提供了对物理感知和非物理感知估计器的差异见解，高密度训练数据分布区域的影响，以及其他因素（如数据集大小、块大小等）的影响。

**Abstract:** Supervised convolutional neural networks (CNNs) are widely used to solve imaging inverse problems, achieving state-of-the-art performance in numerous applications. However, despite their empirical success, these methods are poorly understood from a theoretical perspective and often treated as black boxes. To bridge this gap, we analyze trained neural networks through the lens of the Minimum Mean Square Error (MMSE) estimator, incorporating functional constraints that capture two fundamental inductive biases of CNNs: translation equivariance and locality via finite receptive fields. Under the empirical training distribution, we derive an analytic, interpretable, and tractable formula for this constrained variant, termed Local-Equivariant MMSE (LE-MMSE). Through extensive numerical experiments across various inverse problems (denoising, inpainting, deconvolution), datasets (FFHQ, CIFAR-10, FashionMNIST), and architectures (U-Net, ResNet, PatchMLP), we demonstrate that our theory matches the neural networks outputs (PSNR $\gtrsim25$dB). Furthermore, we provide insights into the differences between \emph{physics-aware} and \emph{physics-agnostic} estimators, the impact of high-density regions in the training (patch) distribution, and the influence of other factors (dataset size, patch size, etc).

</details>


### [89] [Fine-Grained Human Pose Editing Assessment via Layer-Selective MLLMs](https://arxiv.org/abs/2601.10369)
*Ningyu Sun,Zhaolin Cai,Zitong Xu,Peihang Chen,Huiyu Duan,Yichao Yan,Xiongkuo Min,Xiaokang Yang*

Main category: cs.CV

> 本文介绍了HPE-Bench，一个含1,700个标准化样本和17种最先进的编辑模型的基准测试，以及一个基于多模态大规模语言模型的统一框架，用于解决文本引导的人体姿态编辑中存在的结构异常和生成伪影问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有评估指标在隔离真实性检测和质量评估方面存在局限，无法提供细粒度的姿势特定不一致性的见解。

**Method:** 研究人员提出了一个统一框架，基于层选择多模态大规模语言模型（MLLM），并使用对比LoRA调优和新颖的层敏感性分析（LSA）机制来确定最佳特征层。

**Result:** 该框架在真实性检测和多维度质量回归方面表现出色，有效地填补了鉴定检测和质量评估之间的差距。

**Conclusion:** 该研究为文本引导的人体姿态编辑提供了一个有效的评估基准和框架，提高编辑模型的真实性和质量。

**Abstract:** Text-guided human pose editing has gained significant traction in AIGC applications. However,it remains plagued by structural anomalies and generative artifacts. Existing evaluation metrics often isolate authenticity detection from quality assessment, failing to provide fine-grained insights into pose-specific inconsistencies. To address these limitations, we introduce HPE-Bench, a specialized benchmark comprising 1,700 standardized samples from 17 state-of-the-art editing models, offering both authenticity labels and multi-dimensional quality scores. Furthermore, we propose a unified framework based on layer-selective multimodal large language models (MLLMs). By employing contrastive LoRA tuning and a novel layer sensitivity analysis (LSA) mechanism, we identify the optimal feature layer for pose evaluation. Our framework achieves superior performance in both authenticity detection and multi-dimensional quality regression, effectively bridging the gap between forensic detection and quality assessment.

</details>


### [90] [Towards Efficient Low-rate Image Compression with Frequency-aware Diffusion Prior Refinement](https://arxiv.org/abs/2601.10373)
*Yichong Xia,Yimin Zhou,Jinpeng Wang,Bin Chen*

Main category: cs.CV

> 本文提出了DiffCR，一种加速扩散生成先验的框架，用于实现高效的图像压缩与重建，减少了比特率并加快了解码速度。

<details>
  <summary>Details</summary>

**Motivation:** 现有基于扩散的方法在图像压缩中面临采样过程缓慢和次优比特分配的问题，这些问题源于分割的训练范式。

**Method:** 提出了一种名为DiffCR的加速扩散生成先验的图像压缩框架，其中包括一个基于频率感知跳跃估计（FaSE）模块，该模块通过频率解耦注意力（FDA）在不同的时间步上调整ε预测先验，使其与压缩潜变量对齐。此外，一个轻量级的一致性估计器实现了快速两步解码，保留扩散采样的语义轨迹，而无需更新核心扩散模型。

**Result:** 与最先进的扩散生成的压缩基线相比，DiffCR实现了27.2%（LPIPS）和65.1%（PSNR）的比特率节省，并且解码速度提高了10倍以上。

**Conclusion:** DiffCR框架能够在不过度更新基础扩散模型的情况下，实现高效的、高保真的图像重建，同时显著减少比特率，并大幅提升解码速度。

**Abstract:** Recent advancements in diffusion-based generative priors have enabled visually plausible image compression at extremely low bit rates. However, existing approaches suffer from slow sampling processes and suboptimal bit allocation due to fragmented training paradigms. In this work, we propose Accelerate \textbf{Diff}usion-based Image Compression via \textbf{C}onsistency Prior \textbf{R}efinement (DiffCR), a novel compression framework for efficient and high-fidelity image reconstruction. At the heart of DiffCR is a Frequency-aware Skip Estimation (FaSE) module that refines the $ε$-prediction prior from a pre-trained latent diffusion model and aligns it with compressed latents at different timesteps via Frequency Decoupling Attention (FDA). Furthermore, a lightweight consistency estimator enables fast \textbf{two-step decoding} by preserving the semantic trajectory of diffusion sampling. Without updating the backbone diffusion model, DiffCR achieves substantial bitrate savings (27.2\% BD-rate (LPIPS) and 65.1\% BD-rate (PSNR)) and over $10\times$ speed-up compared to SOTA diffusion-based compression baselines.

</details>


### [91] [Global Context Compression with Interleaved Vision-Text Transformation](https://arxiv.org/abs/2601.10378)
*Dian Jiao,Jiaxin Duan,Shuai Zhao,Jiabing Leng,Yiran Zhang,Feng Huang*

Main category: cs.CV

> 本文提出了一种名为VIST2的新Transformer模型，该模型在输入阶段和推理阶段都通过文本和视觉编码交织的方式实现全局上下文压缩，从而减少了计算和内存成本，在长文本任务上表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 受到视觉语言模型在端到端OCR方面成就的启发，文章动机在于探索一种新的全局上下文压缩方法，以减少Transformer输入的tokens数量，从而减轻注意力计算的压力。

**Method:** 提出VIST2模型，将文本块与视觉编码交织输入，并仅依赖视觉token进行预测。文本块被转换成素描图像，并通过从课程预训练到多模式交错指令微调等多个阶段进行训练。

**Result:** 实验结果显示，压缩比例为4倍时，VIST2模型比基准模型在长文本任务上表现更优，第一token生成速度提高了3倍，内存使用减少了77%，浮点运算减少了74%。

**Conclusion:** VIST2通过全局上下文压缩方法，在长文本任务中实现了显著的计算资源节省。代码和数据集将公开发布以支持进一步研究。

**Abstract:** Recent achievements of vision-language models in end-to-end OCR point to a new avenue for low-loss compression of textual information. This motivates earlier works that render the Transformer's input into images for prefilling, which effectively reduces the number of tokens through visual encoding, thereby alleviating the quadratically increased Attention computations. However, this partial compression fails to save computational or memory costs at token-by-token inference. In this paper, we investigate global context compression, which saves tokens at both prefilling and inference stages. Consequently, we propose VIST2, a novel Transformer that interleaves input text chunks alongside their visual encoding, while depending exclusively on visual tokens in the pre-context to predict the next text token distribution. Around this idea, we render text chunks into sketch images and train VIST2 in multiple stages, starting from curriculum-scheduled pretraining for optical language modeling, followed by modal-interleaved instruction tuning. We conduct extensive experiments using VIST2 families scaled from 0.6B to 8B to explore the training recipe and hyperparameters. With a 4$\times$ compression ratio, the resulting models demonstrate significant superiority over baselines on long writing tasks, achieving, on average, a 3$\times$ speedup in first-token generation, 77% reduction in memory usage, and 74% reduction in FLOPS. Our codes and datasets will be public to support further studies.

</details>


### [92] [Handling Missing Modalities in Multimodal Survival Prediction for Non-Small Cell Lung Cancer](https://arxiv.org/abs/2601.10386)
*Filippo Ruffini,Camillo Maria Caruso,Claudia Tacconi,Lorenzo Nibid,Francesca Miccolis,Marta Lovino,Carlo Greco,Edy Ippolito,Michele Fiore,Alessio Cortellini,Bruno Beomonte Zobel,Giuseppe Perrone,Bruno Vincenzi,Claudio Marrocco,Alessandro Bria,Elisa Ficarra,Sara Ramella,Valerio Guarrasi,Paolo Soda*

Main category: cs.CV

> 本文提出了一种新的缺失感知多模态生存框架，用于不可切除II-III期非小细胞肺癌的生存率预测。方法可适应性地融合不同模态，特别在存在缺失数据的情况下，表现出比单模态和其他融合策略更好的效果。

<details>
  <summary>Details</summary>

**Motivation:** 由于多模态深度学习在临床应用上受到小样本量和缺失模态的限制，该研究旨在通过新的框架提高非小细胞肺癌患者的生存预测精度。

**Method:** 该研究提出了一种针对不可切除II-III期非小细胞肺癌（NSCLC）患者生存率预测的缺失感知多模态框架。方法结合了计算机断层扫描（CT）、全幻灯片病理图像（WSI）以及结构化的临床变量，并采用基础模型进行模态特异性特征提取，同时使用缺失感知编码策略进行中间融合。

**Result:** 实验结果显示，中间模态融合方法在所有模态融合方法中表现最优，特别在结合WSI和临床模态时，C指数达到了73.30。且该框架显示了自适应性，可以自动调整不同模态的权重。

**Conclusion:** 提出的框架展示了其在处理缺失模态时的稳健性，并通过实验验证了中间模态融合策略在生存预测中的优越性。

**Abstract:** Accurate survival prediction in Non-Small Cell Lung Cancer (NSCLC) requires the integration of heterogeneous clinical, radiological, and histopathological information. While Multimodal Deep Learning (MDL) offers a promises for precision prognosis and survival prediction, its clinical applicability is severely limited by small cohort sizes and the presence of missing modalities, often forcing complete-case filtering or aggressive imputation. In this work, we present a missing-aware multimodal survival framework that integrates Computed Tomography (CT), Whole-Slide Histopathology (WSI) Images, and structured clinical variables for overall survival modeling in unresectable stage II-III NSCLC. By leveraging Foundation Models (FM) for modality-specific feature extraction and a missing-aware encoding strategy, the proposed approach enables intermediate multimodal fusion under naturally incomplete modality profiles. The proposed architecture is resilient to missing modalities by design, allowing the model to utilize all available data without being forced to drop patients during training or inference. Experimental results demonstrate that intermediate fusion consistently outperforms unimodal baselines as well as early and late fusion strategies, with the strongest performance achieved by the fusion of WSI and clinical modalities (73.30 C-index). Further analyses of modality importance reveal an adaptive behavior in which less informative modalities, i.e., CT modality, are automatically down-weighted and contribute less to the final survival prediction.

</details>


### [93] [Multi-Temporal Frames Projection for Dynamic Processes Fusion in Fluorescence Microscopy](https://arxiv.org/abs/2601.10392)
*Hassan Eshkiki,Sarah Costa,Mostafa Mohammadpour,Farinaz Tanhaei,Christopher H. George,Fabio Caraffini*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Fluorescence microscopy is widely employed for the analysis of living biological samples; however, the utility of the resulting recordings is frequently constrained by noise, temporal variability, and inconsistent visualisation of signals that oscillate over time. We present a unique computational framework that integrates information from multiple time-resolved frames into a single high-quality image, while preserving the underlying biological content of the original video. We evaluate the proposed method through an extensive number of configurations (n = 111) and on a challenging dataset comprising dynamic, heterogeneous, and morphologically complex 2D monolayers of cardiac cells. Results show that our framework, which consists of a combination of explainable techniques from different computer vision application fields, is capable of generating composite images that preserve and enhance the quality and information of individual microscopy frames, yielding 44% average increase in cell count compared to previous methods. The proposed pipeline is applicable to other imaging domains that require the fusion of multi-temporal image stacks into high-quality 2D images, thereby facilitating annotation and downstream segmentation.

</details>


### [94] [Lunar-G2R: Geometry-to-Reflectance Learning for High-Fidelity Lunar BRDF Estimation](https://arxiv.org/abs/2601.10449)
*Clementine Grethen,Nicolas Menga,Roland Brochard,Geraldine Morin,Simone Gasparini,Jeremy Lebreton,Manuel Sanchez Gestido*

Main category: cs.CV

> 论文提出了一种名为Lunar-G2R的方法，通过月球数字高程模型直接预测空间变化的BRDF参数，以提高月球表面渲染的光度真实性和视觉导航能力。

<details>
  <summary>Details</summary>

**Motivation:** 现有的月球渲染流程依赖于简化的或空间均匀的BRDF模型，这些模型的参数难以估计，无法捕捉局部反射变化，限制了光度真实性。为了克服这个问题，我们提出了Lunar-G2R方法，旨在通过月球数字高程模型直接预测空间变化的BRDF参数，而无需在推理时使用多视角图像、受控照明或专门的反射捕获硬件。

**Method:** 该方法利用一个基于可微分渲染的U-Net，通过最小化实际轨道图像和已知视图与照明几何结构下的基于物理的渲染之间的光度差异来进行训练。

**Result:** 实验表明，与最先进的基线模型相比，我们的方法在Tycho陨石坑的一个地理隔离区域上将光度误差降低了38％，同时实现了更高的PSNR和SSIM，并提高了感知相似性，捕捉到了空间均匀模型中缺失的精细尺度反射变化。

**Conclusion:** 这项工作展示了首次直接从地形几何结构中推断出空间变化反射模型的方法，为提高月球表面渲染的真实感和视觉导航提供了新的途径。

**Abstract:** We address the problem of estimating realistic, spatially varying reflectance for complex planetary surfaces such as the lunar regolith, which is critical for high-fidelity rendering and vision-based navigation. Existing lunar rendering pipelines rely on simplified or spatially uniform BRDF models whose parameters are difficult to estimate and fail to capture local reflectance variations, limiting photometric realism. We propose Lunar-G2R, a geometry-to-reflectance learning framework that predicts spatially varying BRDF parameters directly from a lunar digital elevation model (DEM), without requiring multi-view imagery, controlled illumination, or dedicated reflectance-capture hardware at inference time. The method leverages a U-Net trained with differentiable rendering to minimize photometric discrepancies between real orbital images and physically based renderings under known viewing and illumination geometry. Experiments on a geographically held-out region of the Tycho crater show that our approach reduces photometric error by 38 % compared to a state-of-the-art baseline, while achieving higher PSNR and SSIM and improved perceptual similarity, capturing fine-scale reflectance variations absent from spatially uniform models. To our knowledge, this is the first method to infer a spatially varying reflectance model directly from terrain geometry.

</details>


### [95] [Urban Socio-Semantic Segmentation with Vision-Language Reasoning](https://arxiv.org/abs/2601.10477)
*Yu Wang,Yi Wang,Rui Dai,Yujie Wang,Kaikui Liu,Xiangxiang Chu,Yansheng Li*

Main category: cs.CV

> 本文提出了一种基于视觉语言模型推理的城市社会语义分割方法，并开发了相应的数据集SocioSeg。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机是为了提高现有模型对社会定义类别（如学校、公园）进行分割的能力。

**Method:** 本文提出了一种新的视觉语言推理框架SocioReasoner，该框架通过跨模式识别和多阶段推理来模拟人类识别和标注社会语义实体的过程，并使用强化学习来优化这一难以通过梯度计算进行优化的推理过程。

**Result:** 实验结果表明，该方法在现有的最先进的模型上取得了改进，并且具有很强的零样本泛化能力。

**Conclusion:** 本文的结论是所提出的方法和数据集可以有效地进行社会语义分割，并有可能推动该领域的未来研究。

**Abstract:** As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.

</details>
