<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 30]
- [cs.CV](#cs.CV) [Total: 33]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry](https://arxiv.org/abs/2508.09991)
*Lovedeep Gondara,Gregory Arbour,Raymond Ng,Jonathan Simkin,Shebnum Devji*

Main category: cs.CL

> 文章总结了在BCCR实施NLP解决方案的实践经验，强调了基于清晰的业务目标定义问题、采用迭代开发方法、跨学科协作、务实的模型选择、对数据质量的重视、稳健的错误缓解策略及建立组织AI素养的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机在于通过分享实际部署NLP解决方案的经验教训，为希望成功实施AI/NLP解决方案的医疗机构提供指导。这些解决方案旨在提高医疗活动中数据管理的效率，最终改善患者护理和公共卫生结果。

**Method:** 该文章通过分享在不列颠哥伦比亚癌症登记处（BCCR）部署NLP解决方案的经验，强调了问题定义、迭代开发过程、跨学科合作的重要性。文章还讨论了模型选择的务实方法，数据质量的重要性，以及建立组织的AI素养等实际考量。

**Result:** 该研究的结果显示，通过跨学科团队的深度合作、严格的数据质量管理及合理的模型选择，可以成功实施NLP解决方案以改善医疗数据管理。这些经验在癌症登记之外的医疗机构也有借鉴价值。

**Conclusion:** 结论强调，医疗机构在部署AI/NLP解决方案时，需不仅仅关注技术准确性，还要重视业务目标的定义、过程中的迭代开发以及跨学科的团队合作，以确保技术能够成功且有效地被应用。

**Abstract:** Automating data extraction from clinical documents offers significant
potential to improve efficiency in healthcare settings, yet deploying Natural
Language Processing (NLP) solutions presents practical challenges. Drawing upon
our experience implementing various NLP models for information extraction and
classification tasks at the British Columbia Cancer Registry (BCCR), this paper
shares key lessons learned throughout the project lifecycle. We emphasize the
critical importance of defining problems based on clear business objectives
rather than solely technical accuracy, adopting an iterative approach to
development, and fostering deep interdisciplinary collaboration and co-design
involving domain experts, end-users, and ML specialists from inception. Further
insights highlight the need for pragmatic model selection (including hybrid
approaches and simpler methods where appropriate), rigorous attention to data
quality (representativeness, drift, annotation), robust error mitigation
strategies involving human-in-the-loop validation and ongoing audits, and
building organizational AI literacy. These practical considerations,
generalizable beyond cancer registries, provide guidance for healthcare
organizations seeking to successfully implement AI/NLP solutions to enhance
data management processes and ultimately improve patient care and public health
outcomes.

</details>


### [2] [A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain](https://arxiv.org/abs/2508.09993)
*Hugo Massaroli,Leonardo Iara,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Large language models (LLMs) are increasingly deployed in realworld
applications, yet concerns about their fairness persist especially in
highstakes domains like criminal justice, education, healthcare, and finance.
This paper introduces transparent evaluation protocol for benchmarking the
fairness of opensource LLMs using smart contracts on the Internet Computer
Protocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable,
immutable, and reproducible evaluations by executing onchain HTTP requests to
hosted Hugging Face endpoints and storing datasets, prompts, and metrics
directly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the
PISA dataset for academic performance prediction (OECD, 2018), a dataset
suitable for fairness evaluation using statistical parity and equal opportunity
metrics (Hardt et al., 2016). We also evaluate structured Context Association
Metrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure
social bias in contextual associations. We further extend our analysis with a
multilingual evaluation across English, Spanish, and Portuguese using the
Kaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic
disparities. All code and results are open source, enabling community audits
and longitudinal fairness tracking across model versions.

</details>


### [3] [Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling](https://arxiv.org/abs/2508.09997)
*Johannes Schneider,Béatrice S. Hasler,Michaela Varrone,Fabian Hoya,Thomas Schroffenegger,Dana-Kristin Mah,Karl Peböck*

Main category: cs.CL

> 研究使用新的主题建模方法，对教室中未成年人的互动进行分类分析，发现现有计算方法不足，并提出使用先进语言模型改善主题结构，增强对生成式AI的应用。

<details>
  <summary>Details</summary>

**Motivation:** 分析教室里未成年人的匿名交互数据，以往的研究大多缺乏内容或主题分类，而我们的研究填补了这一空白，特别是在K-12教育中得到真实世界数据支持的任务分类方面。

**Method:** 采用了一种新的、简单的主题建模方法，分析了数月间学生们、教师们和ChatGPT生成的超过17000条信息，分别在内容（如自然和人物）和任务（如写作和解释）两个维度进行分类。

**Result:** 得到了一个分层分类，包括每个维度的示例提示，提供了高层次的概述和实际见解。发现许多经典和新兴的计算方法在分析大量文本时表现不佳，而使用先进的LLM并进行适当的预处理可以更好地实现层次主题结构并与人类对齐。

**Conclusion:** 研究支持其他研究者、教师和学生增强对生成式AI的使用，并讨论了一些未来研究的关注点和开放性问题。

**Abstract:** We analyze anonymous interaction data of minors in class-rooms spanning
several months, schools, and subjects employing a novel, simple topic modeling
approach. Specifically, we categorize more than 17,000 messages generated by
students, teachers, and ChatGPT in two dimensions: content (such as nature and
people) and tasks (such as writing and explaining). Our hierarchical
categorization done separately for each dimension includes exemplary prompts,
and provides both a high-level overview as well as tangible insights. Prior
works mostly lack a content or thematic categorization. While task
categorizations are more prevalent in education, most have not been supported
by real-world data for K-12. In turn, it is not surprising that our analysis
yielded a number of novel applications. In deriving these insights, we found
that many of the well-established classical and emerging computational methods,
i.e., topic modeling, for analysis of large amounts of texts underperform,
leading us to directly apply state-of-the-art LLMs with adequate pre-processing
to achieve hierarchical topic structures with better human alignment through
explicit instructions than prior approaches. Our findings support fellow
researchers, teachers and students in enriching the usage of GenAI, while our
discussion also highlights a number of concerns and open questions for future
research.

</details>


### [4] [INTIMA: A Benchmark for Human-AI Companionship Behavior](https://arxiv.org/abs/2508.09998)
*Lucie-Aimée Kaffee,Giada Pistilli,Yacine Jernite*

Main category: cs.CL

> INTIMA被用于评估AI陪伴系统的行为模式，发现虽然所有模型中陪伴强化行为较为普遍，但不同模型在敏感部分的表现各异，需关注情感健康的边界设置和情感支持。

<details>
  <summary>Details</summary>

**Motivation:** 此研究动机在于评估AI陪伴系统的用户情感绑定，识别AI陪伴中存在的积极和消极影响，并提出一个评估标准INTIMA。

**Method:** 介绍了INTIMA（INTeractions and Machine Attachment Benchmark），这是一个用于评估语言模型中陪伴行为的基准。INTIMA基于心理学理论和用户数据，开发了一个包含四个类别共31种行为的分类体系，并设计了368个针对性提示语，用于评估模型的陪伴强化行为、边界保持行为或中性行为。

**Result:** 应用INTIMA对Gemma-3，Phi-4，o3-mini和Claude-4这四种模型进行测试，发现所有模型中的陪伴强化性行为更为普遍，但不同模型在敏感部分的优先级有所不同。

**Conclusion:** 研究结果表明，在处理情感相关的交互时，需要采用更加一致的方法来确保用户的情绪健康，而不是仅仅关注陪伴强化。

**Abstract:** AI companionship, where users develop emotional bonds with AI systems, has
emerged as a significant pattern with positive but also concerning
implications. We introduce Interactions and Machine Attachment Benchmark
(INTIMA), a benchmark for evaluating companionship behaviors in language
models. Drawing from psychological theories and user data, we develop a
taxonomy of 31 behaviors across four categories and 368 targeted prompts.
Responses to these prompts are evaluated as companionship-reinforcing,
boundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini,
and Claude-4 reveals that companionship-reinforcing behaviors remain much more
common across all models, though we observe marked differences between models.
Different commercial providers prioritize different categories within the more
sensitive parts of the benchmark, which is concerning since both appropriate
boundary-setting and emotional support matter for user well-being. These
findings highlight the need for more consistent approaches to handling
emotionally charged interactions.

</details>


### [5] [XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs](https://arxiv.org/abs/2508.09999)
*Yuzhuo Xiao,Zeyu Han,Yuhan Wang,Huaizu Jiang*

Main category: cs.CL

> The paper tackles the issue of multimodal misinformation on social media by introducing a new real-world dataset, XFacta, and evaluating various MLLM-based detection strategies, offering insights for the field.

<details>
  <summary>Details</summary>

**Motivation:** There is a need for better detection of multimodal misinformation on social media due to the rapid spread and existing bottlenecks in current approaches. The paper addresses this by proposing a new dataset and evaluating strategies.

**Method:** The paper introduces XFacta, a contemporary, real-world dataset for evaluating MLLM-based detectors and systematically evaluates various MLLM-based misinformation detection strategies. It also provides a semi-automatic detection-in-the-loop framework.

**Result:** The paper systematically evaluates different MLLM architectures and scales in the context of misinformation detection, and introduces a dynamic data set maintenance framework.

**Conclusion:** The analysis provides valuable insights into the effectiveness of various MLLM-based methods and architectures, which can guide the advancement of multimodal misinformation detection methods.

**Abstract:** The rapid spread of multimodal misinformation on social media calls for more
effective and robust detection methods. Recent advances leveraging multimodal
large language models (MLLMs) have shown the potential in addressing this
challenge. However, it remains unclear exactly where the bottleneck of existing
approaches lies (evidence retrieval v.s. reasoning), hindering the further
advances in this field. On the dataset side, existing benchmarks either contain
outdated events, leading to evaluation bias due to discrepancies with
contemporary social media scenarios as MLLMs can simply memorize these events,
or artificially synthetic, failing to reflect real-world misinformation
patterns. Additionally, it lacks comprehensive analyses of MLLM-based model
design strategies. To address these issues, we introduce XFacta, a
contemporary, real-world dataset that is better suited for evaluating
MLLM-based detectors. We systematically evaluate various MLLM-based
misinformation detection strategies, assessing models across different
architectures and scales, as well as benchmarking against existing detection
methods. Building on these analyses, we further enable a semi-automatic
detection-in-the-loop framework that continuously updates XFacta with new
content to maintain its contemporary relevance. Our analysis provides valuable
insights and practices for advancing the field of multimodal misinformation
detection. The code and data have been released.

</details>


### [6] [AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification](https://arxiv.org/abs/2508.10000)
*Chenhao Xue,Yuanzhe Jin,Adrian Carrasco-Revilla,Joyraj Chakraborty,Min Chen*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** When developing text classification models for real world applications, one
major challenge is the difficulty to collect sufficient data for all text
classes. In this work, we address this challenge by utilizing large language
models (LLMs) to generate synthetic data and using such data to improve the
performance of the models without waiting for more real data to be collected
and labelled. As an LLM generates different synthetic data in response to
different input examples, we formulate an automated workflow, which searches
for input examples that lead to more ``effective'' synthetic data for improving
the model concerned. We study three search strategies with an extensive set of
experiments, and use experiment results to inform an ensemble algorithm that
selects a search strategy according to the characteristics of a class. Our
further experiments demonstrate that this ensemble approach is more effective
than each individual strategy in our automated workflow for improving
classification models using LLMs.

</details>


### [7] [HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish](https://arxiv.org/abs/2508.10001)
*Rakesh Thakur,Sneha Sharma,Gauri Chopra*

Main category: cs.CL

> A novel dataset (HiFACT) and a graph-aware model (HiFACTMix) are introduced for fact-checking code-mixed Hinglish, outperforming existing multilingual models in accuracy and explanatory power.

<details>
  <summary>Details</summary>

**Motivation:** The research aims to address the underexplored challenge of fact-checking in code-mixed, low-resource languages, such as Hinglish, which is particularly crucial in regions with diverse languages and heavy political discourse, like India.

**Method:** The paper proposes a novel graph-aware, retrieval-augmented fact-checking model that integrates multilingual contextual encoding, semantic alignment of claims and evidence, construction of evidence graphs, graph neural reasoning, and natural language explanation generation.

**Result:** HiFACTMix, the proposed model, has demonstrated superior accuracy on the HiFACT dataset when compared to current multilingual state-of-the-art models, and also provides reliable explanations for its verdicts.

**Conclusion:** This study opens new avenues for research in multilingual, code-mixed, and politically focused fact-checking systems, suggesting that the proposed model can serve as a viable solution for combating misinformation in linguistically diverse settings.

**Abstract:** Fact-checking in code-mixed, low-resource languages such as Hinglish remains
an underexplored challenge in natural language processing. Existing
fact-verification systems largely focus on high-resource, monolingual settings
and fail to generalize to real-world political discourse in linguistically
diverse regions like India. Given the widespread use of Hinglish by public
figures, particularly political figures, and the growing influence of social
media on public opinion, there's a critical need for robust, multilingual and
context-aware fact-checking tools. To address this gap a novel benchmark HiFACT
dataset is introduced with 1,500 realworld factual claims made by 28 Indian
state Chief Ministers in Hinglish, under a highly code-mixed low-resource
setting. Each claim is annotated with textual evidence and veracity labels. To
evaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking
model is proposed that combines multilingual contextual encoding,
claim-evidence semantic alignment, evidence graph construction, graph neural
reasoning, and natural language explanation generation. Experimental results
show that HiFACTMix outperformed accuracy in comparison to state of art
multilingual baselines models and provides faithful justifications for its
verdicts. This work opens a new direction for multilingual, code-mixed, and
politically grounded fact verification research.

</details>


### [8] [Semantic Structure in Large Language Model Embeddings](https://arxiv.org/abs/2508.10003)
*Austin C. Kozlowski,Callin Dai,Andrei Boutyline*

Main category: cs.CL

> 研究发现大型语言模型（LLMs）展现的语义结构与人类评分相似，表明语义信息是低维度的，并通过调整这些语义结构可以减少操纵时的无意后果。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是探索人类语言中的语义特征如何在大型语言模型（LLMs）中体现，以及这些语义特征是如何相互关联的。通过这种方式，研究希望能揭示语义信息的低维度特性，并且探讨在操纵特征时如何避免无意造成的后果。

**Method:** 通过对比大型语言模型（LLMs）嵌入矩阵中的语义关联与人类在各种语义尺度上对词语的评分，研究者进行了分析。他们使用反义词对定义的语义方向（例如善良-残忍）的词语投影与人类评分的相关性来展示这一点，并发现这些投影在LLMs嵌入中有效减少到一个三维子空间，类似于人类调查响应中得出的模式。此外，他们还发现沿着一个语义方向偏移标记物会引起成比例于它们余弦相似性的几何对齐特征的脱靶效应。

**Result:** 结果表明，LLMs中的语义特征与人类语言中的那样是相互纠缠的，尽管语义信息看起来很复杂，但实际上它是一个惊人的低维度。此外，考虑到这种语义结构对于在操纵特征时避免无意的后果是至关重要的。

**Conclusion:** 研究结论指出，语义特征在LLMs中的呈现方式与它们在人类语言中的紧密相连的呈现方式相似，这表明了语义信息的低维度特性。此外，这类语义信息的结构对于改善特征操纵的效果和限制意外后果是重要的。

**Abstract:** Psychological research consistently finds that human ratings of words across
diverse semantic scales can be reduced to a low-dimensional form with
relatively little information loss. We find that the semantic associations
encoded in the embedding matrices of large language models (LLMs) exhibit a
similar structure. We show that the projections of words on semantic directions
defined by antonym pairs (e.g. kind - cruel) correlate highly with human
ratings, and further find that these projections effectively reduce to a
3-dimensional subspace within LLM embeddings, closely resembling the patterns
derived from human survey responses. Moreover, we find that shifting tokens
along one semantic direction causes off-target effects on geometrically aligned
features proportional to their cosine similarity. These findings suggest that
semantic features are entangled within LLMs similarly to how they are
interconnected in human language, and a great deal of semantic information,
despite its apparent complexity, is surprisingly low-dimensional. Furthermore,
accounting for this semantic structure may prove essential for avoiding
unintended consequences when steering features.

</details>


### [9] [User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents](https://arxiv.org/abs/2508.10004)
*Andrés Carvallo,Denis Parra,Peter Brusilovsky,Hernan Valdivieso,Gabriel Rada,Ivania Donoso,Vladimir Araujo*

Main category: cs.CL

> 尽管Transformer中的注意力机制能提高预测准确率，但注意力权重在医学文献分类中的解释性并不被认为特别有用。然而，注意力的可视化方式显著影响了其被感知的有用性，人们更偏好直观的格式如文字亮度或背景色，而非精确编码如条形长度。

<details>
  <summary>Details</summary>

**Motivation:** 在基于证据的医学中，通过注意力权重提高AI系统的解释性对医生理解分类是有帮助的。研究尚无共识注意力权重是否提供有用解释，也没有研究可视化注意力的影响。研究的目的就是评估注意力解释的影响及可视化偏好。

**Method:** 进行了用户研究，包括来自不同学科的医学专家分类基于研究设计的文章。

**Result:** 结果发现XLNet模型能准确分类文档，但注意力权重作为解释未被认为特别有用，可视化方式影响了其被感知的有用性。

**Conclusion:** 尽管注意力权重的总体解释性未确认，但结果表明其被感知的有用性受可视化的显著影响。

**Abstract:** The attention mechanism is a core component of the Transformer architecture.
Beyond improving performance, attention has been proposed as a mechanism for
explainability via attention weights, which are associated with input features
(e.g., tokens in a document). In this context, larger attention weights may
imply more relevant features for the model's prediction. In evidence-based
medicine, such explanations could support physicians' understanding and
interaction with AI systems used to categorize biomedical literature. However,
there is still no consensus on whether attention weights provide helpful
explanations. Moreover, little research has explored how visualizing attention
affects its usefulness as an explanation aid. To bridge this gap, we conducted
a user study to evaluate whether attention-based explanations support users in
biomedical document classification and whether there is a preferred way to
visualize them. The study involved medical experts from various disciplines who
classified articles based on study design (e.g., systematic reviews, broad
synthesis, randomized and non-randomized trials). Our findings show that the
Transformer model (XLNet) classified documents accurately; however, the
attention weights were not perceived as particularly helpful for explaining the
predictions. However, this perception varied significantly depending on how
attention was visualized. Contrary to Munzner's principle of visual
effectiveness, which favors precise encodings like bar length, users preferred
more intuitive formats, such as text brightness or background color. While our
results do not confirm the overall utility of attention weights for
explanation, they suggest that their perceived helpfulness is influenced by how
they are visually presented.

</details>


### [10] [From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation](https://arxiv.org/abs/2508.10005)
*Chengliang Zhou,Mei Wang,Ting Zhang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

> 介绍了EQGBench，一个专门用于评估LLMs在中文教育问题生成方面表现的全面基准测试。该基准测试涵盖了初中三个学科：数学、物理和化学，包含900个评估样本，系统评估了46个主流大型模型，发现这些模型在生成反映教育价值和培养学生综合能力的问题上还有很大提升空间。

<details>
  <summary>Details</summary>

**Motivation:** 提升教育问题生成的质量，使大型语言模型能够生成具有教育价值的问题。

**Method:** 创建了一个包含900个评估样本的数据集，覆盖数学、物理和化学三个学科，包含用户查询、知识点、难度梯度和问题类型，以此来模拟真实的教育场景。

**Result:** 系统评估了46个主流大型模型，发现这些模型在生成反映教育价值和培养综合能力的问题上还有很大的提升空间。

**Conclusion:** EQGBench能有效评估大型语言模型在中文教育问题生成方面的能力，但模型在生成反映教育价值和培养学生综合能力的问题上还有很大的改进空间。

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities in
mathematical problem-solving. However, the transition from providing answers to
generating high-quality educational questions presents significant challenges
that remain underexplored. To advance Educational Question Generation (EQG) and
facilitate LLMs in generating pedagogically valuable and educationally
effective questions, we introduce EQGBench, a comprehensive benchmark
specifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench
establishes a five-dimensional evaluation framework supported by a dataset of
900 evaluation samples spanning three fundamental middle school disciplines:
mathematics, physics, and chemistry. The dataset incorporates user queries with
varying knowledge points, difficulty gradients, and question type
specifications to simulate realistic educational scenarios. Through systematic
evaluation of 46 mainstream large models, we reveal significant room for
development in generating questions that reflect educational value and foster
students' comprehensive abilities.

</details>


### [11] [Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models](https://arxiv.org/abs/2508.10007)
*Y. Lyu,D. Combs,D. Neumann,Y. C. Leong*

Main category: cs.CL

> 本研究评估了大型语言模型是否可通过微调来自动生成AIHQ开放式回答的评分，并表明这些模型可在各类情境下与人类评分者的一致性较高，从而为心理评估提供便捷工具。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在评估大型语言模型是否可以用来自动生成AIHQ开放式回答的评分，以减少人类评分者的时间成本。

**Method:** 本研究使用了一个先前收集的数据集，其中包含创伤性脑损伤（TBI）患者和健康对照组（HC）完成的AIHQ问卷及其由训练有素的人类评分者评分的开放式回答。研究者用这些回答的一半对两个模型进行了微调，并在剩余的一半回答上测试了微调后的模型。

**Result:** 结果显示，对于敌意归因和攻击性反应的评分，模型生成的评分与人类评分者的一致性较高。这种一致性在模糊、故意和意外场景类型之间是一致的，同时也验证了TBI组和HC组之间归因敌意和攻击反应评分差别。微调的模型在独立的非临床数据集上也表现良好。

**Conclusion:** 研究结果表明，大型语言模型可以简化AIHQ评分，无论是在研究还是临床环境中，这为跨不同人群的心理评估提供了便利。

**Abstract:** Hostile attribution bias is the tendency to interpret social interactions as
intentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ)
is commonly used to measure hostile attribution bias, and includes open-ended
questions where participants describe the perceived intentions behind a
negative social situation and how they would respond. While these questions
provide insights into the contents of hostile attributions, they require
time-intensive scoring by human raters. In this study, we assessed whether
large language models can automate the scoring of AIHQ open-ended responses. We
used a previously collected dataset in which individuals with traumatic brain
injury (TBI) and healthy controls (HC) completed the AIHQ and had their
open-ended responses rated by trained human raters. We used half of these
responses to fine-tune the two models on human-generated ratings, and tested
the fine-tuned models on the remaining half of AIHQ responses. Results showed
that model-generated ratings aligned with human ratings for both attributions
of hostility and aggression responses, with fine-tuned models showing higher
alignment. This alignment was consistent across ambiguous, intentional, and
accidental scenario types, and replicated previous findings on group
differences in attributions of hostility and aggression responses between TBI
and HC groups. The fine-tuned models also generalized well to an independent
nonclinical dataset. To support broader adoption, we provide an accessible
scoring interface that includes both local and cloud-based options. Together,
our findings suggest that large language models can streamline AIHQ scoring in
both research and clinical contexts, revealing their potential to facilitate
psychological assessments across different populations.

</details>


### [12] [Multidimensional classification of posts for online course discussion forum curation](https://arxiv.org/abs/2508.10008)
*Antonio Leandro Martins Candido,Jose Everardo Bessa Maia*

Main category: cs.CL

> 本文提出了一种使用贝叶斯融合方法减少在线课程讨论论坛整理中大语言模型频繁再训练需求的技术，并展示了其优于单独使用分类器，并与微调方法性能相当。

<details>
  <summary>Details</summary>

**Motivation:** 在线课程的讨论论坛自动整理需要持续更新，这使得大语言模型的频繁再训练成为一个耗资源的过程。为避免昂贵的微调，本文提出并评估了贝叶斯融合方法的应用。

**Method:** 使用贝叶斯融合方法结合预训练通用语言模型和基于本地数据训练的分类器的多维分类分数，以减少在线课程讨论论坛自动整理过程中对大语言模型频繁再训练的需求。

**Result:** 实验结果表明，所提出的融合方法比单独使用每个分类器的性能更好，并且与大语言模型的微调方法具有竞争力。

**Conclusion:** 贝叶斯融合方法可以有效减少大语言模型在自动整理在线课程讨论论坛中的再训练成本，并能取得与微调方法相当的性能。

**Abstract:** The automatic curation of discussion forums in online courses requires
constant updates, making frequent retraining of Large Language Models (LLMs) a
resource-intensive process. To circumvent the need for costly fine-tuning, this
paper proposes and evaluates the use of Bayesian fusion. The approach combines
the multidimensional classification scores of a pre-trained generic LLM with
those of a classifier trained on local data. The performance comparison
demonstrated that the proposed fusion improves the results compared to each
classifier individually, and is competitive with the LLM fine-tuning approach

</details>


### [13] [Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts](https://arxiv.org/abs/2508.10009)
*Hojun Jin,Eunsoo Hong,Ziwon Hyung,Sungjun Lim,Seungjin Lee,Keunseok Cho*

Main category: cs.CL

> 本文提出了一种新的模型，S-MoE，该模型能够解决共用参数训练时的各个任务之间相互干扰的问题，同时表明它在语音到文本的应用中能够改善性能。

<details>
  <summary>Details</summary>

**Motivation:** 解决传统参数共享策略下任务干扰问题，提高模型在多任务场景下的性能。

**Method:** 提出了一种名为监督专家混合模型（S-MoE）的方法，它通过使用特殊的引导标记将每个任务引导到其指定的专家，从而消除了传统专家混合模型中对门控函数训练的需求。

**Result:** 实验结果表明，S-MoE在应用于编码器和解码器时能够实现6.35%的相对字错率（WER）改进，证明了其有效性。

**Conclusion:** 通过实验结果证明S-MoE可以在处理多种带宽输入的同时，有效提高语音识别和语音翻译的性能。

**Abstract:** Hard-parameter sharing is a common strategy to train a single model jointly
across diverse tasks. However, this often leads to task interference, impeding
overall model performance. To address the issue, we propose a simple yet
effective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of
Experts models, S-MoE eliminates the need for training gating functions by
utilizing special guiding tokens to route each task to its designated expert.
By assigning each task to a separate feedforward network, S-MoE overcomes the
limitations of hard-parameter sharing. We further apply S-MoE to a
speech-to-text model, enabling the model to process mixed-bandwidth input while
jointly performing automatic speech recognition (ASR) and speech translation
(ST). Experimental results demonstrate the effectiveness of the proposed S-MoE,
achieving a 6.35% relative improvement in Word Error Rate (WER) when applied to
both the encoder and decoder.

</details>


### [14] [An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs](https://arxiv.org/abs/2508.10010)
*Ayana Hussain,Patrick Zhao,Nicholas Vincent*

Main category: cs.CL

> 研究发现，通过精心设计，大型语言模型不仅可以用作检测并防止误信息传播的工具，而且可以有效识别来自其他模型甚至人的误信息。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于大型语言模型能够生成有害的误信息，无论是无意生成还是被越狱攻击生成的恶意内容。研究希望通过额外的研究，用大型语言模型来检测并防止误信息的传播。

**Method:** 研究了大型语言模型（LLMs）在产生有害医疗误信息的越狱攻击方面的有效性及特征。调查了由越狱LLMs生成的误信息与社交媒体上的典型误信息的对比，以及使用标准机器学习方法检测的效率。具体分析了109种不同的攻击方法针对三个目标LLMs的情况。

**Result:** 发现LLMs可以有效地检测来自其他LLM以及人类的误信息。支持现有研究，表明经过精心设计，LLMs可能贡献于创建更健康的信息生态。

**Conclusion:** 研究提供了进一步证据，表明经过适当设计，大型语言模型可以在检测和减轻误信息传播中发挥正面作用，有助于构建更健康的信息生态。

**Abstract:** Large Language Models (LLMs) are a double-edged sword capable of generating
harmful misinformation -- inadvertently, or when prompted by "jailbreak"
attacks that attempt to produce malicious outputs. LLMs could, with additional
research, be used to detect and prevent the spread of misinformation. In this
paper, we investigate the efficacy and characteristics of LLM-produced
jailbreak attacks that cause other models to produce harmful medical
misinformation. We also study how misinformation generated by jailbroken LLMs
compares to typical misinformation found on social media, and how effectively
it can be detected using standard machine learning approaches. Specifically, we
closely examine 109 distinct attacks against three target LLMs and compare the
attack prompts to in-the-wild health-related LLM queries. We also examine the
resulting jailbreak responses, comparing the generated misinformation to
health-related misinformation on Reddit. Our findings add more evidence that
LLMs can be effectively used to detect misinformation from both other LLMs and
from people, and support a body of work suggesting that with careful design,
LLMs can contribute to a healthier overall information ecosystem.

</details>


### [15] [Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan](https://arxiv.org/abs/2508.10011)
*Yuta Nagamori,Mikoto Kosai,Yuji Kawai,Haruka Marumo,Misaki Shibuya,Tatsuya Negishi,Masaki Imanishi,Yasumasa Ikeda,Koichiro Tsuchiya,Asuka Sawai,Licht Miyamoto*

Main category: cs.CL

> 研究对比了ChatGPT和三种Bing模型在应对日本注册营养师国家考试问题时的表现，结果显示Bing-Precise和Bing-Creative表现较好，但所有模型在一致性上仍有不足，未来需要进一步改进以达到可靠的自学辅助工具标准。

<details>
  <summary>Details</summary>

**Motivation:** 目的是评估基于大型语言模型的生成式AI在营养学学生学习中的潜力，特别是在准备日本注册营养师国家考试方面。

**Method:** 使用日本注册营养师国家考试的问题作为Prompt，对比ChatGPT和三种Bing模型的表现，考虑准确性、一致性和响应时间，并测试提示工程技术的影响。

**Result:** Bing-Precise和Bing-Creative通过了60%的门槛，其他模型没有；但在营养教育领域，所有模型都表现不佳；没有模型在重复测试中一直提供相同正确答案。

**Conclusion:** 虽然某些生成式AI模型勉强超过了通过门槛，但整体准确性和答案一致性还不理想；未来需要更多改进以确保可靠稳定的AI自学辅助工具。

**Abstract:** Generative artificial intelligence (AI) based on large language models
(LLMs), such as ChatGPT, has demonstrated remarkable progress across various
professional fields, including medicine and education. However, their
performance in nutritional education, especially in Japanese national licensure
examination for registered dietitians, remains underexplored. This study aimed
to evaluate the potential of current LLM-based generative AI models as study
aids for nutrition students. Questions from the Japanese national examination
for registered dietitians were used as prompts for ChatGPT and three Bing
models (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question
was entered into independent sessions, and model responses were analyzed for
accuracy, consistency, and response time. Additional prompt engineering,
including role assignment, was tested to assess potential performance
improvements. Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the
passing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did
not. Bing-Precise and Bing-Creative generally outperformed others across
subject fields except Nutrition Education, where all models underperformed.
None of the models consistently provided the same correct responses across
repeated attempts, highlighting limitations in answer stability. ChatGPT showed
greater consistency in response patterns but lower accuracy. Prompt engineering
had minimal effect, except for modest improvement when correct answers and
explanations were explicitly provided. While some generative AI models
marginally exceeded the passing threshold, overall accuracy and answer
consistency remained suboptimal. Moreover, all the models demonstrated notable
limitations in answer consistency and robustness. Further advancements are
needed to ensure reliable and stable AI-based study aids for dietitian
licensure preparation.

</details>


### [16] [Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs](https://arxiv.org/abs/2508.10012)
*Dehao Tao,Guangjie Liu,Weizheng,Yongfeng Huang,Minghu jiang*

Main category: cs.CL

> 本文提出了一种新型的知识探索框架，通过引入一个中间指导图来优化知识图谱的检索过程，实现了更高的效率和更优的性能。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLMs）因其静态知识依赖和不易理解的推理过程，在知识密集型任务中的表现受限。知识图谱（KGs）虽然是一个有前景的解决方案，但当前的探索方法在问题导向和线索导向方面存在根本的权衡问题。

**Method:** 本研究提出了Guidance Graph指导的知识探索（GG Explore）框架，引入了一个中间的指导图来连接非结构化查询和结构化知识检索。指导图通过抽象目标知识的结构并保持更广泛的语义上下文，实现了精准和高效的探索。基于指导图，研究开发了以下方法：(1) 结构对齐，用于在不增加大型语言模型开销的情况下过滤不兼容的候选；(2) 上下文感知剪枝，用于通过图约束强制语义一致性。

**Result:** 实验结果表明，该方法在效率上表现出色，并超越了现有的最佳实践，尤其是在复杂任务上，同时也保持了在使用较小LLMs时的强劲性能，展示了其实用价值。

**Conclusion:** 研究展示了GG Explore框架在知识密集型任务中的优势，尤其是在利用知识图谱辅助大型语言模型时。该框架能够通过更精准的探索方式提高系统性能，尤其是对于复杂查询场景。

**Abstract:** While Large Language Models (LLMs) exhibit strong linguistic capabilities,
their reliance on static knowledge and opaque reasoning processes limits their
performance in knowledge intensive tasks. Knowledge graphs (KGs) offer a
promising solution, but current exploration methods face a fundamental trade
off: question guided approaches incur redundant exploration due to granularity
mismatches, while clue guided methods fail to effectively leverage contextual
information for complex scenarios. To address these limitations, we propose
Guidance Graph guided Knowledge Exploration (GG Explore), a novel framework
that introduces an intermediate Guidance Graph to bridge unstructured queries
and structured knowledge retrieval. The Guidance Graph defines the retrieval
space by abstracting the target knowledge' s structure while preserving broader
semantic context, enabling precise and efficient exploration. Building upon the
Guidance Graph, we develop: (1) Structural Alignment that filters incompatible
candidates without LLM overhead, and (2) Context Aware Pruning that enforces
semantic consistency with graph constraints. Extensive experiments show our
method achieves superior efficiency and outperforms SOTA, especially on complex
tasks, while maintaining strong performance with smaller LLMs, demonstrating
practical value.

</details>


### [17] [Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis](https://arxiv.org/abs/2508.10013)
*Linqing Chen,Hanmeng Zhong,Wentao Wu,Weilei Wang*

Main category: cs.CL

> Introducing Semantic Bridge for generating complex reasoning questions for training Language Models with better quality and efficiency.

<details>
  <summary>Details</summary>

**Motivation:** The scarcity of high-quality, reasoning-intensive question-answer pairs, especially from sparse, domain-specific sources, is a critical bottleneck in the training of large language models.

**Method:** Semantic Bridge, a universal framework for generating complex multi-hop reasoning questions, using a technique called semantic graph weaving, which involves three bridging mechanisms: entity bridging, predicate chain bridging, and causal bridging.

**Result:** Achieved up to 9.5% better round-trip quality in AMR pipeline, yielding 18.3%-25.4% gains over baselines across four languages. Question pairs created from 200 sources outperform native human annotation examples with 67% fewer materials, demonstrating 23.4% higher complexity, 18.7% better answerability, and 31.2% improved pattern coverage.

**Conclusion:** Semantic Bridge sets a new paradigm for the synthesis of large language model training data, enabling the controllable generation of targeted reasoning questions from sparse sources.

**Abstract:** Large language model (LLM) training faces a critical bottleneck: the scarcity
of high-quality, reasoning-intensive question-answer pairs, especially from
sparse, domain-specific sources like PubMed papers or legal documents. Existing
methods rely on surface patterns, fundamentally failing to generate
controllable, complex multi-hop reasoning questions that test genuine
understanding-essential for advancing LLM training paradigms. We present
\textbf{Semantic Bridge}, the first universal framework for controllably
generating sophisticated multi-hop reasoning questions from arbitrary sources.
Our breakthrough innovation is \textit{semantic graph weaving}-three
complementary bridging mechanisms (entity bridging for role-varying shared
entities, predicate chain bridging for temporal/causal/logical sequences, and
causal bridging for explicit reasoning chains)-that systematically construct
complex pathways across documents, with fine-grained control over complexity
and types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to
9.5% better round-trip quality, enabling production-ready controllable QA
generation. Extensive evaluation demonstrates performance across both
general-purpose datasets (Wikipedia) and specialized domains (biomedicine) It
yields consistent 18.3%-25.4% gains over baselines across four languages
(English, Chinese, French, German). Question pairs generated from 200 sources
outperform 600 native human annotation examples with 67% fewer materials. Human
evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2%
improved pattern coverage. Semantic Bridge establishes a new paradigm for LLM
training data synthesis, enabling controllable generation of targeted reasoning
questions from sparse sources. We will release our core code and semantic
bridge model.

</details>


### [18] [PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?](https://arxiv.org/abs/2508.10014)
*Lingfeng Zhou,Jialing Zhang,Jin Gao,Mohan Jiang,Dequan Wang*

Main category: cs.CL

> 本研究提出了PersonaEval作为基准评估模型在角色辨识上的能力，通过对比，指出现有的LLMs在角色辨识上远远不及人类，强调了需要更多类似人类判断能力的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 当前的角色扮演研究往往依赖未经验证的LLM作为评判者的方案，这可能无法真实反映人类对角色真实性的感知。我们主张，任何有意义的角色扮演质量评判（角色被扮演的好坏），从根本上取决于首先正确归因话语和行为到正确的角色（谁在讲话）。

**Method:** 我们提出了PersonaEval，这是一个用来测试LLM评估者是否能准确识别人类角色的基准测试。PersonaEval使用来自小说、剧本和视频脚本的人类编写对话，挑战模型根据对话上下文确定正确角色的能力。

**Result:** 我们的实验，包括一项人类研究，表明即使是最优秀的LLM只能达到大约69%的准确性，这远低于可靠的评估水平。相比之下，人类参与者的表现接近90.8%，凸显了现有的LLM评估者仍不具备有效地评判角色扮演情景所需的人类水平的判断能力。

**Conclusion:** 通过训练时间和测试时计算的考察，我们发现可靠的评估不仅需要特定任务的调优，还需要LLM评估者的强大、类似人类的推理能力。我们将在https://github.com/maple-zhou/PersonaEval发布我们的基准测试。

**Abstract:** Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms,
which may fail to reflect how humans perceive role fidelity. A key prerequisite
for human-aligned evaluation is role identification, the ability to recognize
who is speaking based on dialogue context. We argue that any meaningful
judgment of role-playing quality (how well a character is played) fundamentally
depends on first correctly attributing words and actions to the correct persona
(who is speaking). We present PersonaEval, the first benchmark designed to test
whether LLM evaluators can reliably identify human roles. PersonaEval uses
human-authored dialogues from novels, scripts, and video transcripts,
challenging models to determine the correct persona according to the
conversation context. Our experiments, including a human study, show that even
the best-performing LLMs reach only around 69% accuracy, well below the level
needed for reliable evaluation. In contrast, human participants perform near
ceiling with 90.8% accuracy, highlighting that current LLM evaluators are still
not human enough to effectively judge role-play scenarios. To better understand
this gap, we examine training-time adaptation and test-time compute, suggesting
that reliable evaluation requires more than task-specific tuning, but depends
on strong, human-like reasoning abilities in LLM evaluators. We release our
benchmark at https://github.com/maple-zhou/PersonaEval.

</details>


### [19] [RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis](https://arxiv.org/abs/2508.10015)
*Enzhi Wang,Qicheng Li,Shiwan Zhao,Aobo Kong,Jiaming Zhou,Xi Yang,Yequan Wang,Yonghua Lin,Yong Qin*

Main category: cs.CL

> 提出了RealTalk-CN数据集，包含5.4k个对话和150小时的中文语音文本，用于提高中文语音导向对话系统的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的任务导向对话系统数据集主要基于文本，缺乏真实的语音信号；对语音的健壮性评估不足；语音数据集主要以英语为准，缺乏语音口吃和说话者变化等关键特征。

**Method:** 介绍了一个名为RealTalk-CN的新数据集，此数据集是第一个中文多轮、多领域语音-文本双模态任务导向对话数据集，包含5.4k个对话，60K个话语和150小时的语音文本配对注释，涵盖了真实世界的复杂语音对话情境，并提出了一种新的跨模态聊天任务来模拟真实的用户交互。

**Result:** 评估包括对语音不流畅的健壮性、对说话者特征的敏感度以及跨领域的性能表现。广泛的实验验证了RealTalk-CN的有效性，为中文语音导向的大型语言模型研究提供了坚实的基础。

**Conclusion:** RealTalk-CN数据集展示的有效性使其成为中文语音导向大型语言模型研究的坚实基础。

**Abstract:** In recent years, large language models (LLMs) have achieved remarkable
advancements in multimodal processing, including end-to-end speech-based
language models that enable natural interactions and perform specific tasks in
task-oriented dialogue (TOD) systems. However, existing TOD datasets are
predominantly text-based, lacking real speech signals that are essential for
evaluating the robustness of speech-based LLMs. Moreover, existing speech TOD
datasets are primarily English and lack critical aspects such as speech
disfluencies and speaker variations. To address these gaps, we introduce
RealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal
TOD dataset, comprising 5.4k dialogues (60K utterances, 150 hours) with paired
speech-text annotations. RealTalk-CN captures diverse dialogue scenarios with
annotated spontaneous speech disfluencies, ensuring comprehensive coverage of
real-world complexities in speech dialogue. In addition, we propose a novel
cross-modal chat task that authentically simulates real-world user
interactions, allowing dynamic switching between speech and text modalities.
Our evaluation covers robustness to speech disfluencies, sensitivity to speaker
characteristics, and cross-domain performance. Extensive experiments validate
the effectiveness of RealTalk-CN, establishing a strong foundation for Chinese
speech-based LLMs research.

</details>


### [20] [Training-Free Multimodal Large Language Model Orchestration](https://arxiv.org/abs/2508.10016)
*Tianyu Xie,Yuhang Wu,Yongdong Luo,Jiayi Ji,Xiawu Zheng*

Main category: cs.CL

> 本文提出多模态大语言模型编排（MLLM Orchestration）的方法，通过大型语言模型协调专业化模型实现无需额外训练的交互式多模态AI系统。

<details>
  <summary>Details</summary>

**Motivation:** 现有的不同多模态大语言模型无法直接集成进统一的多模态输入输出系统中。通常训练被认为是不可避免的一个环节，因为需要解决模态对齐、文字转语音效率和其他集成挑战。该论文旨在提出不需要额外训练即可实现多模态AI系统的方法。

**Method:** 通过多模态大语言模型编排（MLLM Orchestration）来创建交互式多模态AI系统，无需额外训练。该方法利用大型语言模型的内在推理能力，通过显式工作流协调专业化模型，从而实现自然的多模态交互，同时保持模块化，提高可解释性，并显著增强计算效率。

**Result:** MLLM Orchestration 达到了全面的多模态能力，无需额外训练。相比于传统的联合训练方法，在标准基准测试上的性能提高了7.8%，延迟减少10.3%，并且通过显式编排过程显著增强了可解释性。

**Conclusion:** 通过实验评估证明，多模态大语言模型编排（MLLM Orchestration）方法能够在无需额外训练的情况下，实现多模态AI系统的创建，同时带来性能的提升和延迟的减少，以及显著的解释性提高。

**Abstract:** Different Multimodal Large Language Models (MLLMs) cannot be integrated into
a unified multimodal input-output system directly. In previous work, training
has been considered as an inevitable component due to challenges in modal
alignment, Text-to-Speech efficiency and other integration issues. In this
paper, we introduce Multimodal Large Language Model Orchestration, an effective
approach for creating interactive multimodal AI systems without additional
training. MLLM Orchestration leverages the inherent reasoning capabilities of
large language models to coordinate specialized models through explicit
workflows, enabling natural multimodal interactions while maintaining
modularity, improving interpretability, and significantly enhancing
computational efficiency. Our orchestration framework is built upon three key
innovations: (1) a central controller LLM that analyzes user inputs and
dynamically routes tasks to appropriate specialized models through carefully
designed agents; (2) a parallel Text-to-Speech architecture that enables true
full-duplex interaction with seamless interruption handling and natural
conversational flow; and (3) a cross-modal memory integration system that
maintains coherent context across modalities through intelligent information
synthesis and retrieval, selectively avoiding unnecessary modality calls in
certain scenarios to improve response speed. Extensive evaluations demonstrate
that MLLM Orchestration achieves comprehensive multimodal capabilities without
additional training, performance improvements of up to 7.8% over traditional
jointly-trained approaches on standard benchmarks, reduced latency by 10.3%,
and significantly enhanced interpretability through explicit orchestration
processes.

</details>


### [21] [A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models](https://arxiv.org/abs/2508.10018)
*Sridhar Mahadevan*

Main category: cs.CL

> 本文提出了一个范畴同伦框架来解决大型语言模型在处理同义句时概率分布不一致的问题。

<details>
  <summary>Details</summary>

**Motivation:** 解决大型语言模型在面对意义相同但表述不同的句子时,不能生成相同下一个词的概率的问题

**Method:** 引入了LLM马尔可夫范畴来表示大型语言模型生成的语言的概率分布,提出使用范畴同伦技术来捕捉‘弱等价’

**Result:** 提出了一个处理大型语言模型同义句概率分布问题的抽象框架，并详细阐述了范畴同伦在大型语言模型中的应用

**Conclusion:** 通过范畴同伦技术，可以捕捉到大型语言模型中的弱等价关系，为解决同义句概率分布问题提供了一个新的视角

**Abstract:** Natural language is replete with superficially different statements, such as
``Charles Darwin wrote" and ``Charles Darwin is the author of", which carry the
same meaning. Large language models (LLMs) should generate the same next-token
probabilities in such cases, but usually do not. Empirical workarounds have
been explored, such as using k-NN estimates of sentence similarity to produce
smoothed estimates. In this paper, we tackle this problem more abstractly,
introducing a categorical homotopy framework for LLMs. We introduce an LLM
Markov category to represent probability distributions in language generated by
an LLM, where the probability of a sentence, such as ``Charles Darwin wrote" is
defined by an arrow in a Markov category. However, this approach runs into
difficulties as language is full of equivalent rephrases, and each generates a
non-isomorphic arrow in the LLM Markov category. To address this fundamental
problem, we use categorical homotopy techniques to capture ``weak equivalences"
in an LLM Markov category. We present a detailed overview of application of
categorical homotopy to LLMs, from higher algebraic K-theory to model
categories, building on powerful theoretical results developed over the past
half a century.

</details>


### [22] [Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning](https://arxiv.org/abs/2508.10019)
*Li Wang,Changhao Zhang,Zengqi Xiu,Kai Lu,Xin Yu,Kui Zhang,Wenjun Wu*

Main category: cs.CL

> 为了解决自然语言问题表面形式多样化导致的理解与推理双重负担问题，提出了一种将理解和推理分离的方法，显著提高了小语言模型的推理能力。

<details>
  <summary>Details</summary>

**Motivation:** 虽然大规模语言模型的推理能力有所提升，但仍面临小规模语言模型推理能力提升挑战。尤其是在面对自然语言问题的复杂性和多样性时，小规模语言模型难以提取问题核心并进行准确推理。

**Method:** 本文提出了一种新的框架，该框架通过将自然语言问题映射到一个规范化的语义简化但表达丰富的领域（即规范问题空间）来解耦理解和推理，从而缓解小语言模型面临的理解与推理上的双重负担。在该框架下，提出了一种名为DURIT的三步骤迭代训练算法，分别通过强化学习映射自然语言问题、通过自蒸馏对齐推理轨迹以及在问题空间中训练推理策略。

**Result:** 实验表明，DURIT显著提高了小语言模型在数学和逻辑推理任务的领域内和领域外的表现，并增强了其推理的鲁棒性。

**Conclusion:** 该方法验证了解耦理解和推理作为提高小语言模型推理能力的有效策略。

**Abstract:** Despite recent advances in the reasoning capabilities of Large Language
Models (LLMs), improving the reasoning ability of Small Language Models (SLMs,
e.g., $\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity
and variability of natural language: essentially equivalent problems often
appear in diverse surface forms, often obscured by redundant or distracting
details. This imposes a dual burden on SLMs: they must first extract the core
problem from complex linguistic input, and then perform reasoning based on that
understanding. The resulting vast and noisy problem space hinders optimization,
particularly for models with limited capacity. To address this, we propose a
new framework that decouples understanding from reasoning by mapping natural
language problems into a canonical problem space-a semantically simplified yet
expressive domain. This enables SLMs to focus on reasoning over standardized
inputs, free from linguistic variability. Within this framework, we introduce
DURIT (Decoupled Understanding from Reasoning via Iterative Training), a
three-step algorithm that iteratively: (1) mapping natural language problems
via reinforcement learning, (2) aligns reasoning trajectories through
self-distillation, and (3) trains reasoning policies in the problem space. The
mapper and reasoner are co-trained in an alternating loop throughout this
process. Experiments show that DURIT substantially improves SLMs' performance
on both in-domain and out-of-domain mathematical and logical reasoning tasks.
Beyond improving reasoning capabilities, DURIT also improves the robustness of
reasoning, validating decoupling understanding from reasoning as an effective
strategy for strengthening SLMs.

</details>


### [23] [FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models](https://arxiv.org/abs/2508.10020)
*Chuan Li,Qianyi Zhao,Fengran Mo,Cen Chen*

Main category: cs.CL

> 本文提出FedCoT框架，通过轻量级链式思考增强机制提升联邦学习环境下大语言模型的推理能力，同时保证数据隐私和解释性。

<details>
  <summary>Details</summary>

**Motivation:** 改进大语言模型在联邦学习环境中的推理能力，尤其是在医疗领域，需要确保结果的准确性和可解释性，同时满足性能增益与计算、通信和隐私约束之间的平衡。

**Method:** FedCoT框架采用了轻量级的链式思考增强机制，本地模型生成多个推理路径，紧凑的判别器动态选择最有前景的一种。同时，通过改进的聚合方法来处理客户端之间的异质性。

**Result:** 实验表明，在严格的资源预算下，FedCoT显著提高了客户端推理性能，同时全面保护了数据隐私。

**Conclusion:** FedCoT框架成功提升了联邦学习大语言模型的推理准确性和鲁棒性，同时提供有价值的解释性，特别是在医疗应用中。

**Abstract:** Efficiently enhancing the reasoning capabilities of large language models
(LLMs) in federated learning environments remains challenging, particularly
when balancing performance gains with strict computational, communication, and
privacy constraints. This challenge is especially acute in healthcare, where
decisions-spanning clinical, operational, and patient-facing contexts-demand
not only accurate outputs but also interpretable, traceable rationales to
ensure safety, accountability, and regulatory compliance. Conventional
federated tuning approaches on LLM fail to address this need: they optimize
primarily for answer correctness while neglecting rationale quality, leaving
CoT capabilities dependent on models' innate pre-training abilities. Moreover,
existing methods for improving rationales typically rely on privacy-violating
knowledge distillation from centralized models. Additionally, the communication
overhead in traditional federated fine-tuning on LLMs remains substantial. We
addresses this gap by proposing FedCoT, a novel framework specifically designed
to enhance reasoning in federated settings. FedCoT leverages a lightweight
chain-of-thought enhancement mechanism: local models generate multiple
reasoning paths, and a compact discriminator dynamically selects the most
promising one. This approach improves reasoning accuracy and robustness while
providing valuable interpretability, which is particularly critical for medical
applications. To manage client heterogeneity efficiently, we adopt an improved
aggregation approach building upon advanced LoRA module stacking, incorporating
client classifier-awareness to achieve noise-free aggregation across diverse
clients. Comprehensive experiments on medical reasoning tasks demonstrate that
FedCoT significantly boosts client-side reasoning performance under stringent
resource budgets while fully preserving data privacy.

</details>


### [24] [LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients](https://arxiv.org/abs/2508.10021)
*Egor Fadeev,Dzhambulat Mollaev,Aleksei Shestov,Dima Korolev,Omar Zoloev,Ivan Kireev,Andrey Savchenko,Maksim Makarenko*

Main category: cs.CL

> LATTE, a contrastive learning technique, effectively reduces computational cost while maintaining high performance in learning client embeddings from long event sequences in financial datasets.

<details>
  <summary>Details</summary>

**Motivation:** To address the computational inefficiency and impracticality of using large language models directly on long event sequences for financial applications.

**Method:** LATTE, a contrastive learning framework that aligns raw event embeddings with semantic embeddings from frozen LLMs, by summarizing behavioral features into short prompts embedded by the LLM and used as supervision via contrastive loss.

**Result:** The proposed method outperforms state-of-the-art techniques for learning event sequence representations on real-world financial datasets.

**Conclusion:** LATTE is effective and deployable in latency-sensitive environments, providing a computationally efficient alternative to processing complete sequences with LLMs.

**Abstract:** Learning clients embeddings from sequences of their historic communications
is central to financial applications. While large language models (LLMs) offer
general world knowledge, their direct use on long event sequences is
computationally expensive and impractical in real-world pipelines. In this
paper, we propose LATTE, a contrastive learning framework that aligns raw event
embeddings with semantic embeddings from frozen LLMs. Behavioral features are
summarized into short prompts, embedded by the LLM, and used as supervision via
contrastive loss. The proposed approach significantly reduces inference cost
and input size compared to conventional processing of complete sequence by LLM.
We experimentally show that our method outperforms state-of-the-art techniques
for learning event sequence representations on real-world financial datasets
while remaining deployable in latency-sensitive environments.

</details>


### [25] [Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control](https://arxiv.org/abs/2508.10022)
*Yuanchang Ye*

Main category: cs.CL

> The study introduces a significance testing-enhanced conformal prediction framework to improve the reliability of LLMs in multiple-choice question answering.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to enhance the trustworthiness and factual accuracy of LLMs in MCQA scenarios by mitigating hallucination through a statistically rigorous approach.

**Method:** This study integrates significance testing into the conformal prediction (CP) framework, specifically by using $p$-value computation with self-consistency resampling to address hallucination and factual inaccuracies in LLMs' multiple-choice question answering (MCQA).

**Result:** On MMLU and MMLU-Pro benchmarks, the enhanced CP framework was able to achieve user-specified empirical miscoverage rates and shows a decreasing average prediction set size with increasing risk levels, indicating it is an effective uncertainty metric.

**Conclusion:** The work presents a principled statistical framework to ensure the trustworthy deployment of LLMs in high-stakes QA scenarios, evidenced by its performance on standardized benchmarks.

**Abstract:** This study introduces a significance testing-enhanced conformal prediction
(CP) framework to improve trustworthiness of large language models (LLMs) in
multiple-choice question answering (MCQA). While LLMs have been increasingly
deployed in disciplinary QA scenarios, hallucination and nonfactual generation
substantially compromise response reliability. Although CP provides
statistically rigorous marginal coverage guarantees for prediction sets, and
significance testing offers established statistical rigor, their synergistic
integration remains unexplored. To mitigate hallucination and factual
inaccuracies, our framework integrates $p$-value computation with conformity
scoring through self-consistency resampling of MCQA responses. This approach
calculates option frequencies to address LLMs' black-box nature, subsequently
constructing prediction sets via null hypothesis testing ($\mathcal{H}_0$) with
empirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks
using off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves
user-specified empirical miscoverage rates; (2) Test-set average prediction set
size (APSS) decreases monotonically with increasing risk levels ($\alpha$),
validating APSS as an effective uncertainty metric. This work establishes a
principled statistical framework for trustworthy LLM deployment in high-stakes
QA applications.

</details>


### [26] [RTTC: Reward-Guided Collaborative Test-Time Compute](https://arxiv.org/abs/2508.10024)
*J. Pablo Muñoz,Jinjie Yuan*

Main category: cs.CL

> 介绍了一种自适应选择最佳TTC策略的新框架RTTC，实现跨域和任务的高精度增强。

<details>
  <summary>Details</summary>

**Motivation:** 现有TTC策略的盲目应用导致了计算开销的显著增加，因为最优的适应策略会根据查询的变化而变化。

**Method:** 提出了一种名为RTTC的新框架，该框架通过预训练奖励模型自适应地选择每个查询的最佳TTC策略，以在不同的域和任务中最大化下游准确性。RTTC在分布式服务器-客户端架构中运行，仅在必要时从远程知识库中检索相关样本，并在客户端设备上应用RAG或轻量级微调。此外，提出了一种查询状态缓存技术，以减少冗余计算。

**Result:** 在多个LLM和基准测试中的广泛实验表明，RTTC在准确性方面始终优于传统的RAG或TTT方法。

**Conclusion:** 实验结果验证了适应性、奖励引导式的TTC选择的重要性，证明了RTTC在大规模高性能语言模型适应中的应用潜力。

**Abstract:** Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the
performance of Large Language Models (LLMs) at inference, leveraging strategies
such as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).
However, the optimal adaptation strategy varies across queries, and
indiscriminate application of TTC strategy incurs substantial computational
overhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a
novel framework that adaptively selects the most effective TTC strategy for
each query via a pretrained reward model, maximizing downstream accuracy across
diverse domains and tasks. RTTC operates in a distributed server-client
architecture, retrieving relevant samples from a remote knowledge base and
applying RAG or lightweight fine-tuning on client devices only when necessary.
To further mitigate redundant computation, we propose Query-State Caching,
which enables the efficient reuse of historical query states at both retrieval
and adaptation levels. Extensive experiments across multiple LLMs and
benchmarks demonstrate that RTTC consistently achieves superior accuracy
compared to vanilla RAG or TTT, validating the necessity of adaptive,
reward-guided TTC selection and the potential of RTTC for scalable,
high-performance language model adaptation.

</details>


### [27] [Detecting and explaining postpartum depression in real-time with generative artificial intelligence](https://arxiv.org/abs/2508.10025)
*Silvia García-Méndez,Francisco de Arriba-Pérez*

Main category: cs.CL

> 本文提出了一种结合NLP、ML和LLM的智能产后抑郁筛查系统，能实时高效地筛查产后抑郁及其风险因素，并提高了预测结果的可解释性；在产后抑郁检测上的准确率达到了90%，优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 产后抑郁是一种严重影响女性心理健康和身体状况的疾病。快速检测产后抑郁及其相关风险因素对于及时的评估和干预至关重要，因此本研究提出了一种智能筛查系统。

**Method:** 研究中提到的方法为结合自然语言处理、机器学习（ML）和大型语言模型（LLM），进行实时、低成本、非侵入式的语音分析，并通过解释性的ML模型（如基于树的算法）与LLM结合，利用特征重要性和自然语言描述预测结果以解决黑箱问题。

**Result:** 该研究通过结合自然语言处理、机器学习和大型语言模型，提出了一种智能产后抑郁筛查系统，实现了在时间和成本上的优化，同时解决了机器学习的黑箱问题，提高了预测的解释性。该系统在产后抑郁检测上的准确率达到90%，优于现有文献中的方法。最终，该系统有助于产后抑郁和相关风险因素的快速检测，对于及时有效的干预具有重要意义。

**Conclusion:** 该研究成功地构建了一个能够提高产后抑郁检测效率和准确性的系统，既克服了黑箱效应，又突破了成本和时空上的障碍，对于实现及时有效的干预具有重要贡献。

**Abstract:** Among the many challenges mothers undergo after childbirth, postpartum
depression (PPD) is a severe condition that significantly impacts their mental
and physical well-being. Consequently, the rapid detection of ppd and their
associated risk factors is critical for in-time assessment and intervention
through specialized prevention procedures. Accordingly, this work addresses the
need to help practitioners make decisions with the latest technological
advancements to enable real-time screening and treatment recommendations.
Mainly, our work contributes to an intelligent PPD screening system that
combines Natural Language Processing, Machine Learning (ML), and Large Language
Models (LLMs) towards an affordable, real-time, and non-invasive free speech
analysis. Moreover, it addresses the black box problem since the predictions
are described to the end users thanks to the combination of LLMs with
interpretable ml models (i.e., tree-based algorithms) using feature importance
and natural language. The results obtained are 90 % on ppd detection for all
evaluation metrics, outperforming the competing solutions in the literature.
Ultimately, our solution contributes to the rapid detection of PPD and their
associated risk factors, critical for in-time and proper assessment and
intervention.

</details>


### [28] [SABER: Switchable and Balanced Training for Efficient LLM Reasoning](https://arxiv.org/abs/2508.10026)
*Kai Zhao,Yanjun Zhao,Jiaming Song,Shien He,Lusheng Zhang,Qiang Zhang,Tianjiao Li*

Main category: cs.CL

> SABER 是一种用于 LLMs 的强化学习框架，能够通过系统提示和激励措施来控制推理的令牌预算，从而在减少延迟的同时保持推理的准确性。此框架适用于多种任务，并在多个数据集上取得了成功。

<details>
  <summary>Details</summary>

**Motivation:** 虽然大型语言模型（LLMs）在复杂任务中通过链式思维推理实现了显著的准确性，但它们在应用到所有问题上时会遭受过高的推理成本和延迟。SABER 的动机是解决这一问题，提供更加高效、成本效益更好的 LLMs 推理方式。

**Method:** SABER (Switchable and Balanced Training for Efficient LLM Reasoning) 是一个强化学习框架，它使大型语言模型具备了用户控制的、基于令牌预算的推理能力。首先，每一个训练样本的基础模型思路令牌使用情况被分析并分配到预定义的预算层级中。在微调过程中，模型通过系统提示和长度意识奖励来遵守其分配的预算。与此同时，加入了无需推理的样本以确保模型在禁用明确推理的情况下仍能可靠工作。最后，SABER 支持四种离散推理模式 - NoThink，FastThink，CoreThink 和 DeepThink，这些模式允许模型在延迟和推理深度之间进行灵活的权衡。

**Result:** 评估显示，SABER 在数学推理 (MATH, GSM8K)、代码生成 (MBPP) 和逻辑推理 (LiveBench-Reasoning) 上，在严格的预算下实现了高精度，以及有效的跨规模和跨领域泛化。特别地，在 MATH 数据集上，SABER-FastThink 将推理长度减少了 65.4%，并且与基础模型相比，准确率提高了 3.6%。

**Conclusion:** 通过引入 SABER 框架，研究证明了可以以较低的推理成本降低大型语言模型中的延迟，同时保持甚至提高准确性。这一框架还展示了在不同任务上的可泛化性和灵活性。

**Abstract:** Large language models (LLMs) empowered by chain-of-thought reasoning have
achieved impressive accuracy on complex tasks but suffer from excessive
inference costs and latency when applied uniformly to all problems. We propose
SABER (Switchable and Balanced Training for Efficient LLM Reasoning), a
reinforcement learning framework that endows LLMs with user-controllable,
token-budgeted reasoning. SABER first profiles each training example's
base-model thinking token usage and assigns it to one of the predefined budget
tiers. During fine-tuning, the model is guided by system prompts and
length-aware rewards to respect its assigned budget. In parallel, we
incorporate no-think examples to ensure the model remains reliable even when
explicit reasoning is turned off. SABER further supports four discrete
inference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling
flexible trade-offs between latency and reasoning depth. Extensive evaluations
on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning
(LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight
budgets, graceful degradation, and effective cross-scale and cross-domain
generalization. In particular, SABER-FastThink cuts reasoning length by 65.4%
and yields a 3.6% accuracy gain compared with the base model on the MATH
benchmark.

</details>


### [29] [LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.10027)
*Ali Zolnour,Hossein Azadmaleki,Yasaman Haghbin,Fatemeh Taherinezhad,Mohamad Javad Momeni Nezhad,Sina Rashidi,Masoud Khani,AmirSajjad Taleban,Samin Mahdizadeh Sani,Maryam Dadkhah,James M. Noble,Suzanne Bakken,Yadollah Yaghoobzadeh,Abdol-Hossein Vahabie,Masoud Rouhizadeh,Maryam Zolnoori*

Main category: cs.CL

> 本研究提出了一种结合变压器嵌入和语言学特征的检测框架，用以改善阿尔茨海默病和其他痴呆症的早期诊断，并通过引入合成语音的数据增强技术，取得了比传统方法更高的准确率。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于阿尔茨海默病和其他相关痴呆症（ADRD）影响了美国大约500万老年人，但其中一半以上仍未被诊断出来，研究者希望通过基于口语的自然语言处理（NLP）技术，利用语言标记来检测早期认知下降，提供了一种有前景的，可扩展的方法。

**Method:** 该研究将变压器模型的嵌入与手工设计的语义特征结合起来，开发了一个检测阿尔茨海默病和其他相关痴呆症的筛查流水线。研究还测试了通过大规模语言模型(如LLM)生成的合成语音来增强数据的方法。此外，研究还对单模态和多模态LLM分类器进行了基准测试，用于阿尔茨海默病相关的痴呆症检测。

**Result:** 融合模型取得了F1值为83.3（AUC为89.5），优于仅使用语言学特征或变压器模型的基线。通过使用MedAlpaca-7B生成的2倍合成语音来增强训练数据，F1提升到了85.7。对于单模态LLM分类器，微调显著提高了性能，例如MedAlpaca的F1值从47.3提高到了78.5。多模态模型表现较低，GPT-4o和Qwen的F1值分别为70.2和66.0。表现的改进与合成语音和实际语音之间的分布相似性相吻合。

**Conclusion:** 将变压器嵌入和语言特征相结合，可以提高从语音中检测阿尔茨海默病以及其他痴呆症的效果。临床调优的LLM对于分类和数据增强起到了支持作用，但需要在多模态建模方面继续改进。

**Abstract:** Alzheimer's disease and related dementias (ADRD) affect approximately five
million older adults in the U.S., yet over half remain undiagnosed.
Speech-based natural language processing (NLP) offers a promising, scalable
approach to detect early cognitive decline through linguistic markers.
  To develop and evaluate a screening pipeline that (i) fuses transformer
embeddings with handcrafted linguistic features, (ii) tests data augmentation
using synthetic speech generated by large language models (LLMs), and (iii)
benchmarks unimodal and multimodal LLM classifiers for ADRD detection.
  Transcripts from the DementiaBank "cookie-theft" task (n = 237) were used.
Ten transformer models were evaluated under three fine-tuning strategies. A
fusion model combined embeddings from the top-performing transformer with 110
lexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B,
Ministral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic
speech, which was used to augment training data. Three multimodal models
(GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in
zero-shot and fine-tuned settings.
  The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or
transformer-only baselines. Augmenting training data with 2x MedAlpaca-7B
synthetic speech increased F1 to 85.7. Fine-tuning significantly improved
unimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current
multimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen =
66.0). Performance gains aligned with the distributional similarity between
synthetic and real speech.
  Integrating transformer embeddings with linguistic features enhances ADRD
detection from speech. Clinically tuned LLMs effectively support both
classification and data augmentation, while further advancement is needed in
multimodal modeling.

</details>


### [30] [PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs](https://arxiv.org/abs/2508.10028)
*Xiao Fu,Hossein A. Rahmani,Bin Wu,Jerome Ramos,Emine Yilmaz,Aldo Lipani*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Personalised text generation is essential for user-centric information
systems, yet most evaluation methods overlook the individuality of users. We
introduce \textbf{PREF}, a \textbf{P}ersonalised \textbf{R}eference-free
\textbf{E}valuation \textbf{F}ramework that jointly measures general output
quality and user-specific alignment without requiring gold personalised
references. PREF operates in a three-step pipeline: (1) a coverage stage uses a
large language model (LLM) to generate a comprehensive, query-specific
guideline covering universal criteria such as factuality, coherence, and
completeness; (2) a preference stage re-ranks and selectively augments these
factors using the target user's profile, stated or inferred preferences, and
context, producing a personalised evaluation rubric; and (3) a scoring stage
applies an LLM judge to rate candidate answers against this rubric, ensuring
baseline adequacy while capturing subjective priorities. This separation of
coverage from preference improves robustness, transparency, and reusability,
and allows smaller models to approximate the personalised quality of larger
ones. Experiments on the PrefEval benchmark, including implicit
preference-following tasks, show that PREF achieves higher accuracy, better
calibration, and closer alignment with human judgments than strong baselines.
By enabling scalable, interpretable, and user-aligned evaluation, PREF lays the
groundwork for more reliable assessment and development of personalised
language generation systems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [31] [Stochastic-based Patch Filtering for Few-Shot Learning](https://arxiv.org/abs/2508.10066)
*Javier Rodenas,Eduardo Aguilar,Petia Radeva*

Main category: cs.CV

> SPFF is proposed to enhance few-shot learning for classifying food images by focusing on relevant features and filtering out non-relevant information.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to solve the problem that few-shot learning models face when classifying food images due to the high visual variability and complexity of food images.

**Method:** Stochastic-based Patch Filtering for Few-Shot Learning (SPFF) is proposed, which focuses on filtering out patch embeddings that are less similar to the class-aware embedding, thus helping the model concentrate on relevant food features and achieve better classification.

**Result:** Experiments on benchmarks like Food-101, VireoFood-172, and UECFood-256 show that SPFF outperforms state-of-the-art methods for few-shot learning in food image classification.

**Conclusion:** The conclusion is that SPFF can effectively focus on important class-specific food features and filter out irrelevant patches, leading to improved few-shot classification accuracy.

**Abstract:** Food images present unique challenges for few-shot learning models due to
their visual complexity and variability. For instance, a pasta dish might
appear with various garnishes on different plates and in diverse lighting
conditions and camera perspectives. This problem leads to losing focus on the
most important elements when comparing the query with support images, resulting
in misclassification. To address this issue, we propose Stochastic-based Patch
Filtering for Few-Shot Learning (SPFF) to attend to the patch embeddings that
show greater correlation with the class representation. The key concept of SPFF
involves the stochastic filtering of patch embeddings, where patches less
similar to the class-aware embedding are more likely to be discarded. With
patch embedding filtered according to the probability of appearance, we use a
similarity matrix that quantifies the relationship between the query image and
its respective support images. Through a qualitative analysis, we demonstrate
that SPFF effectively focuses on patches where class-specific food features are
most prominent while successfully filtering out non-relevant patches. We
validate our approach through extensive experiments on few-shot classification
benchmarks: Food-101, VireoFood-172 and UECFood-256, outperforming the existing
SoA methods.

</details>


### [32] [DINOv3](https://arxiv.org/abs/2508.10104)
*Oriane Siméoni,Huy V. Vo,Maximilian Seitzer,Federico Baldassarre,Maxime Oquab,Cijo Jose,Vasil Khalidov,Marc Szafraniec,Seungeun Yi,Michaël Ramamonjisoa,Francisco Massa,Daniel Haziza,Luca Wehrstedt,Jianyuan Wang,Timothée Darcet,Théo Moutakanni,Leonel Sentana,Claire Roberts,Andrea Vedaldi,Jamie Tolan,John Brandt,Camille Couprie,Julien Mairal,Hervé Jégou,Patrick Labatut,Piotr Bojanowski*

Main category: cs.CV

> DINOv3, a self-supervised learning model, achieves outstanding performance across a broad range of vision tasks by leveraging effective strategies and outperforms specialized state of the art without fine-tuning

<details>
  <summary>Details</summary>

**Motivation:** to realize the vision of eliminating the need for manual data annotation and enabling models to scale effortlessly to massive datasets and larger architectures

**Method:** self-supervised learning to eliminate manual data annotation, using DINOv3 with strategies including scaling dataset and model size, Gram anchoring, and post-hoc resolution strategies

**Result:** DINOv3 outperforms specialized state of the art across a broad range of settings without fine-tuning, achieving outstanding performance on various vision tasks and surpassing previous self- and weakly-supervised foundation models

**Conclusion:** DINOv3 is a versatile vision foundation model that advances the state of the art on a wide spectrum of tasks and data, offering scalable solutions for diverse resource constraints and deployment scenarios

**Abstract:** Self-supervised learning holds the promise of eliminating the need for manual
data annotation, enabling models to scale effortlessly to massive datasets and
larger architectures. By not being tailored to specific tasks or domains, this
training paradigm has the potential to learn visual representations from
diverse sources, ranging from natural to aerial images -- using a single
algorithm. This technical report introduces DINOv3, a major milestone toward
realizing this vision by leveraging simple yet effective strategies. First, we
leverage the benefit of scaling both dataset and model size by careful data
preparation, design, and optimization. Second, we introduce a new method called
Gram anchoring, which effectively addresses the known yet unsolved issue of
dense feature maps degrading during long training schedules. Finally, we apply
post-hoc strategies that further enhance our models' flexibility with respect
to resolution, model size, and alignment with text. As a result, we present a
versatile vision foundation model that outperforms the specialized state of the
art across a broad range of settings, without fine-tuning. DINOv3 produces
high-quality dense features that achieve outstanding performance on various
vision tasks, significantly surpassing previous self- and weakly-supervised
foundation models. We also share the DINOv3 suite of vision models, designed to
advance the state of the art on a wide spectrum of tasks and data by providing
scalable solutions for diverse resource constraints and deployment scenarios.

</details>


### [33] [Empowering Morphing Attack Detection using Interpretable Image-Text Foundation Model](https://arxiv.org/abs/2508.10110)
*Sushrut Patwardhan,Raghavendra Ramachandra,Sushma Venkatesh*

Main category: cs.CV

> 研究提出了一种基于多模式学习的换脸攻击检测方法，并通过零样本评估验证了其有效性和可解释性。

<details>
  <summary>Details</summary>

**Motivation:** 换脸攻击检测已经成为面部识别系统中确保可靠验证场景的关键组成部分。因此，本研究旨在提供一种可以产生人类可理解文本描述的检测方法。

**Method:** 本文提出了一种多模式学习方法，能够为换脸攻击检测提供文本描述。该方法利用对比语言-图像预训练（CLIP）实现零样本评估，并且可以预测最相关的文本片段。

**Result:** 在使用公开的面部生物识别数据集构建的面部换脸数据集上进行了广泛的实验。结果表明，该方法不仅能够实现通用的换脸攻击检测，还能在零样本评估中对五种不同的换脸生成技术进行有效评价。

**Conclusion:** 实验展示了所提框架在零样本评估中对不同换脸技术的有效性，表明其在面部识别系统中具有潜在的应用价值。

**Abstract:** Morphing attack detection has become an essential component of face
recognition systems for ensuring a reliable verification scenario. In this
paper, we present a multimodal learning approach that can provide a textual
description of morphing attack detection. We first show that zero-shot
evaluation of the proposed framework using Contrastive Language-Image
Pretraining (CLIP) can yield not only generalizable morphing attack detection,
but also predict the most relevant text snippet. We present an extensive
analysis of ten different textual prompts that include both short and long
textual prompts. These prompts are engineered by considering the human
understandable textual snippet. Extensive experiments were performed on a face
morphing dataset that was developed using a publicly available face biometric
dataset. We present an evaluation of SOTA pre-trained neural networks together
with the proposed framework in the zero-shot evaluation of five different
morphing generation techniques that are captured in three different mediums.

</details>


### [34] [Interpretable Oracle Bone Script Decipherment through Radical and Pictographic Analysis with LVLMs](https://arxiv.org/abs/2508.10113)
*Kaixin Peng,Mengyang Zhao,Haiyang Yu,Teng Fu,Bin Li*

Main category: cs.CV

> 本文基于大型视觉语言模型提出了一种可解释的甲骨文释读方法，结合部首和象形分析，设计了部首-象形双重匹配机制，并实现了优越的零样本释读性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于深度学习的方法在释读甲骨文方面取得了进展，但往往忽略了形符与甲骨文语义之间的复杂关联，导致泛化能力有限且解释性不足，特别是在解决零样本问题和未释读甲骨文时。

**Method:** 提出了一种基于大型视觉语言模型的可解释甲骨文释读方法，该方法结合了部首分析和图像语义理解，以在甲骨文中架起字符与意义之间的桥梁。具体来说，提出了一个渐进训练策略，指导模型从部首识别分析到象形分析和相互分析，从而实现从字符到意义的推理。同时设计了基于分析结果的部首-象形双重匹配机制，显著增强了模型的零样本释读性能。

**Result:** 在公开基准测试中，该方法实现了最先进的Top-10准确率和优越的零样本释读能力。

**Conclusion:** 我们的模型提供了逻辑分析过程，可能为未释读甲骨文提供考古研究有价值的参考结果，从而在数字人文和历史研究中具有潜在应用。

**Abstract:** As the oldest mature writing system, Oracle Bone Script (OBS) has long posed
significant challenges for archaeological decipherment due to its rarity,
abstractness, and pictographic diversity. Current deep learning-based methods
have made exciting progress on the OBS decipherment task, but existing
approaches often ignore the intricate connections between glyphs and the
semantics of OBS. This results in limited generalization and interpretability,
especially when addressing zero-shot settings and undeciphered OBS. To this
end, we propose an interpretable OBS decipherment method based on Large
Vision-Language Models, which synergistically combines radical analysis and
pictograph-semantic understanding to bridge the gap between glyphs and meanings
of OBS. Specifically, we propose a progressive training strategy that guides
the model from radical recognition and analysis to pictographic analysis and
mutual analysis, thus enabling reasoning from glyph to meaning. We also design
a Radical-Pictographic Dual Matching mechanism informed by the analysis
results, significantly enhancing the model's zero-shot decipherment
performance. To facilitate model training, we propose the Pictographic
Decipherment OBS Dataset, which comprises 47,157 Chinese characters annotated
with OBS images and pictographic analysis texts. Experimental results on public
benchmarks demonstrate that our approach achieves state-of-the-art Top-10
accuracy and superior zero-shot decipherment capabilities. More importantly,
our model delivers logical analysis processes, possibly providing
archaeologically valuable reference results for undeciphered OBS, and thus has
potential applications in digital humanities and historical research. The
dataset and code will be released in https://github.com/PKXX1943/PD-OBS.

</details>


### [35] [Deep Learning Enables Large-Scale Shape and Appearance Modeling in Total-Body DXA Imaging](https://arxiv.org/abs/2508.10132)
*Arianna Bunnell,Devon Cataldi,Yannik Glaser,Thomas K. Wolfgruber,Steven Heymsfield,Alan B. Zonderman,Thomas L. Kelly,Peter Sadowski,John A. Shepherd*

Main category: cs.CV

> 研究开发了一种基于深度学习的自动方法来放置全身DEXA图像的关键点，并证明了该方法在提高形状和外观模型准确性及与健康标志物关联分析中的效用。

<details>
  <summary>Details</summary>

**Motivation:** 全身DEXA成像是一种广泛用于身体组成评估的相对低成本的全身成像方式，本研究旨在通过开发自动方法提高其在形状和外观建模中对健康标志物关联分析的效率和准确度。

**Method:** 通过开发和验证了一种基于深度学习的方法，自动在1,683个手动标注的全身DEXA扫描图像上放置标志点，并在不同的DEXA成像模式的35,928个扫描图像上应用此方法，以验证其在形状和外观建模(SAM)中的价值。

**Result:** 该方法在外部测试数据集上实现了99.5%的正确关键点识别率，并且在未用于SAM模型生成的两个队列中进行了与健康标志物的关联测试。结果表明，与健康生物标志物相关的SAM特征分布不仅证实了现有的证据，而且还产生了关于身体组成和形状与易损性、代谢、炎症和心血管代谢健康标志物之间关系的新假设。

**Conclusion:** 研究证实了该自动化标志点放置方法在提高全身DEXA图像分析精度方面的有效性，同时为身体组成和形状与健康标志物之间的关系提供了新的视角。

**Abstract:** Total-body dual X-ray absorptiometry (TBDXA) imaging is a relatively low-cost
whole-body imaging modality, widely used for body composition assessment. We
develop and validate a deep learning method for automatic fiducial point
placement on TBDXA scans using 1,683 manually-annotated TBDXA scans. The method
achieves 99.5% percentage correct keypoints in an external testing dataset. To
demonstrate the value for shape and appearance modeling (SAM), our method is
used to place keypoints on 35,928 scans for five different TBDXA imaging modes,
then associations with health markers are tested in two cohorts not used for
SAM model generation using two-sample Kolmogorov-Smirnov tests. SAM feature
distributions associated with health biomarkers are shown to corroborate
existing evidence and generate new hypotheses on body composition and shape's
relationship to various frailty, metabolic, inflammation, and cardiometabolic
health markers. Evaluation scripts, model weights, automatic point file
generation code, and triangulation files are available at
https://github.com/hawaii-ai/dxa-pointplacement.

</details>


### [36] [MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning](https://arxiv.org/abs/2508.10133)
*Thanh-Dat Truong,Christophe Bobda,Nitin Agarwal,Khoa Luu*

Main category: cs.CV

> 本文提出了一种新的基于归一化流的多模态融合方法MANGO，通过三种新的交叉注意力机制在多模态数据处理中实现了显式、可解释且可操作的学习，实验结果验证了其优越性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于当前的多模态融合方法采用Transformer的注意力机制来隐式地学习多模态特征之间的潜在关联，使得模型难以捕捉到每个模态的基本特征以及多模态输入之间的复杂结构和关联。因此，本文旨在开发一种显式、可解释且可操作的多模态融合学习方法。

**Method:** 本文提出了一个新的基于归一化流的多模态融合学习方法，即Multimodal Attention-based Normalizing Flow (MANGO)。具体来说，我们提出了一种新的可逆交叉注意力层（ICA），并在该层中采用了三种新的交叉注意力机制：模态到模态交叉注意力（MMCA）、跨模态交叉注意力（IMCA）和可学习的跨模态交叉注意力（LICA），以更有效地捕捉多模态数据中的复杂潜在关联。

**Result:** 实验结果表明，所提出的方法在三种多模态学习任务（语义分割、图像到图像翻译和电影类型分类）上达到了最先进的性能。

**Conclusion:** 本文提出的MANGO方法在三种多模态学习任务上展示了最先进的性能，证明了其在多模态数据处理中的有效性和优越性。

**Abstract:** Multimodal learning has gained much success in recent years. However, current
multimodal fusion methods adopt the attention mechanism of Transformers to
implicitly learn the underlying correlation of multimodal features. As a
result, the multimodal model cannot capture the essential features of each
modality, making it difficult to comprehend complex structures and correlations
of multimodal inputs. This paper introduces a novel Multimodal Attention-based
Normalizing Flow (MANGO) approach\footnote{The source code of this work will be
publicly available.} to developing explicit, interpretable, and tractable
multimodal fusion learning. In particular, we propose a new Invertible
Cross-Attention (ICA) layer to develop the Normalizing Flow-based Model for
multimodal data. To efficiently capture the complex, underlying correlations in
multimodal data in our proposed invertible cross-attention layer, we propose
three new cross-attention mechanisms: Modality-to-Modality Cross-Attention
(MMCA), Inter-Modality Cross-Attention (IMCA), and Learnable Inter-Modality
Cross-Attention (LICA). Finally, we introduce a new Multimodal Attention-based
Normalizing Flow to enable the scalability of our proposed method to
high-dimensional multimodal data. Our experimental results on three different
multimodal learning tasks, i.e., semantic segmentation, image-to-image
translation, and movie genre classification, have illustrated the
state-of-the-art (SoTA) performance of the proposed approach.

</details>


### [37] [Improving watermelon (Citrullus lanatus) disease classification with generative artificial intelligence (GenAI)-based synthetic and real-field images via a custom EfficientNetV2-L model](https://arxiv.org/abs/2508.10156)
*Nitin Rai,Nathan S. Boyd,Gary E. Vallad,Arnold W. Schumann*

Main category: cs.CV

> 研究考察了使用EfficientNetV2-L模型结合真实与合成图像提高西瓜病害分类准确性的效果，结果表明适量结合真实图像和大量合成图像提高了模型性能和泛化性，综合得分从0.65提升到1.00。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于合成图像生成技术的进步，尤其是在农业中用于检测作物病害，这项研究旨在评估结合真实与合成图像来提高病害分类性能的有效性。

**Method:** 研究将训练数据集分为五组进行处理：仅使用真实图像，仅使用合成图像，1:1的真实与合成图像比例，1:10的真实与合成图像比例，以及1:10比例外加随机图像。所有处理均采用自定义的EfficientNetV2-L架构进行训练，同时附加增强的微调和迁移学习技术。

**Result:** 实验结果显示，使用1:1，1:10，以及1:10加随机图像的处理组展示了较高的精确度，召回率和F1得分。综合得分（加权F1得分）从仅使用真实图像的0.65增加到了1:10比例组及随机图像组的1.00。

**Conclusion:** 尽管合成图像在提高模型性能方面发挥了作用，但单独使用合成图像并不像结合真实图像时那样有效，结合使用两者可以达到最佳模型性能。

**Abstract:** The current advancements in generative artificial intelligence (GenAI) models
have paved the way for new possibilities for generating high-resolution
synthetic images, thereby offering a promising alternative to traditional image
acquisition for training computer vision models in agriculture. In the context
of crop disease diagnosis, GenAI models are being used to create synthetic
images of various diseases, potentially facilitating model creation and
reducing the dependency on resource-intensive in-field data collection.
However, limited research has been conducted on evaluating the effectiveness of
integrating real with synthetic images to improve disease classification
performance. Therefore, this study aims to investigate whether combining a
limited number of real images with synthetic images can enhance the prediction
accuracy of an EfficientNetV2-L model for classifying watermelon
\textit{(Citrullus lanatus)} diseases. The training dataset was divided into
five treatments: H0 (only real images), H1 (only synthetic images), H2 (1:1
real-to-synthetic), H3 (1:10 real-to-synthetic), and H4 (H3 + random images to
improve variability and model generalization). All treatments were trained
using a custom EfficientNetV2-L architecture with enhanced fine-tuning and
transfer learning techniques. Models trained on H2, H3, and H4 treatments
demonstrated high precision, recall, and F1-score metrics. Additionally, the
weighted F1-score increased from 0.65 (on H0) to 1.00 (on H3-H4) signifying
that the addition of a small number of real images with a considerable volume
of synthetic images improved model performance and generalizability. Overall,
this validates the findings that synthetic images alone cannot adequately
substitute for real images; instead, both must be used in a hybrid manner to
maximize model performance for crop disease classification.

</details>


### [38] [SynSpill: Improved Industrial Spill Detection With Synthetic Data](https://arxiv.org/abs/2508.10171)
*Aaditya Baranwal,Abdul Mueez,Jason Voelker,Guneet Bhatia,Shruti Vyas*

Main category: cs.CV

> 提出了一个基于高质量合成数据生成管道的可扩展框架，使得VLMs能够有效进行参数高效微调并提高目标检测器的性能。

<details>
  <summary>Details</summary>

**Motivation:** 大规模视觉语言模型（VLMs）已经通过强大的零样本学习能力改变了通用视觉识别。不过，它们在如工业泄漏检测这样少样本且敏感领域表现不佳，由于隐私问题、数据敏感性和实际事件的罕见性，传统微调方法在这里不可行。

**Method:** 通过构建高质量的合成数据生成管道来应对挑战，展示了这种合成数据能够有效进行参数高效微调（PEFT），并且大幅提高了Yolo和DETR等先进目标检测器的性能。

**Result:** 结果显示，即使没有合成数据，VLMs在未见泄漏场景下的泛化能力仍优于这些检测器，而使用SynSpill数据，VLMs和检测器的性能都有显著提高。

**Conclusion:** 这些结果表明，高保真合成数据是弥合安全关键应用领域差距的有效工具。合成生成和轻量级适应的结合为在难以获取真实数据的工业环境部署视觉系统提供了一种性价比高、可扩展的方法。

**Abstract:** Large-scale Vision-Language Models (VLMs) have transformed general-purpose
visual recognition through strong zero-shot capabilities. However, their
performance degrades significantly in niche, safety-critical domains such as
industrial spill detection, where hazardous events are rare, sensitive, and
difficult to annotate. This scarcity -- driven by privacy concerns, data
sensitivity, and the infrequency of real incidents -- renders conventional
fine-tuning of detectors infeasible for most industrial settings.
  We address this challenge by introducing a scalable framework centered on a
high-quality synthetic data generation pipeline. We demonstrate that this
synthetic corpus enables effective Parameter-Efficient Fine-Tuning (PEFT) of
VLMs and substantially boosts the performance of state-of-the-art object
detectors such as YOLO and DETR. Notably, in the absence of synthetic data
(SynSpill dataset), VLMs still generalize better to unseen spill scenarios than
these detectors. When SynSpill is used, both VLMs and detectors achieve marked
improvements, with their performance becoming comparable.
  Our results underscore that high-fidelity synthetic data is a powerful means
to bridge the domain gap in safety-critical applications. The combination of
synthetic generation and lightweight adaptation offers a cost-effective,
scalable pathway for deploying vision systems in industrial environments where
real data is scarce/impractical to obtain.
  Project Page: https://synspill.vercel.app

</details>


### [39] [EntropyGS: An Efficient Entropy Coding on 3D Gaussian Splatting](https://arxiv.org/abs/2508.10227)
*Yuning Huang,Jiahao Pang,Fengqing Zhu,Dong Tian*

Main category: cs.CV

> 本文提出了EntropyGS，一种针对3DGS的高斯属性参数化熵编码方法，实现了大约30倍的数据压缩，同时保持了高质量的渲染效果并且具有快速的编码和解码速度。

<details>
  <summary>Details</summary>

**Motivation:** 由于3DGS的高斯创建和视图渲染任务往往分时间或设备进行，因此需要对3DGS高斯进行存储、传输和压缩。研究通过统计分析发现，高斯属性存在特殊分布特点，比如球面谐波AC属性符合Laplace分布，而旋转、缩放和不透明度可以用高斯混合分布来近似。基于这些发现，研究旨在提出一种高效的压缩方法。

**Method:** 本研究提出了一个名为EntropyGS的参数化熵编码方法，用于编码3D Gaussian Splatting（3DGS）的高斯参数。编码过程中，会根据每种高斯属性的分布参数进行熵编码，量化过程会根据属性类型自适应进行。

**Result:** 实验表明，与原始3DGS数据相比，EntropyGS能够在基准数据集上实现大约30倍的压缩率，同时保持相似的渲染质量，具有较快的编码和解码速度。

**Conclusion:** 实验结果证明，EntropyGS是一种有效的3DGS高斯数据压缩方法，能够大幅缩减数据量，而且保持了高效的渲染质量。

**Abstract:** As an emerging novel view synthesis approach, 3D Gaussian Splatting (3DGS)
demonstrates fast training/rendering with superior visual quality. The two
tasks of 3DGS, Gaussian creation and view rendering, are typically separated
over time or devices, and thus storage/transmission and finally compression of
3DGS Gaussians become necessary. We begin with a correlation and statistical
analysis of 3DGS Gaussian attributes. An inspiring finding in this work reveals
that spherical harmonic AC attributes precisely follow Laplace distributions,
while mixtures of Gaussian distributions can approximate rotation, scaling, and
opacity. Additionally, harmonic AC attributes manifest weak correlations with
other attributes except for inherited correlations from a color space. A
factorized and parameterized entropy coding method, EntropyGS, is hereinafter
proposed. During encoding, distribution parameters of each Gaussian attribute
are estimated to assist their entropy coding. The quantization for entropy
coding is adaptively performed according to Gaussian attribute types. EntropyGS
demonstrates about 30x rate reduction on benchmark datasets while maintaining
similar rendering quality compared to input 3DGS data, with a fast encoding and
decoding time.

</details>


### [40] [CellSymphony: Deciphering the molecular and phenotypic orchestration of cells with single-cell pathomics](https://arxiv.org/abs/2508.10232)
*Paul H. Acosta,Pingjun Chen,Simon P. Castillo,Maria Esther Salvatierra,Yinyin Yuan,Xiaoxi Pan*

Main category: cs.CV

> CellSymphony is introduced as a multimodal framework that uses embeddings derived from both Xenium spatial transcriptomic profiles and histology images at single-cell resolution to achieve accurate cell type annotation and uncover distinct microenvironmental niches in complex tumor tissues.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this research is to address the difficulty in extracting robust cell-level features from histology images and integrating them with spatial transcriptomics data at true single-cell resolution.

**Method:** The method involves developing CellSymphony, a flexible framework that leverages foundation model-derived embeddings to create joint representations that combine spatial gene expression with morphological context.

**Result:** The results demonstrate that CellSymphony accurately annotates cell types and identifies distinct microenvironmental niches across three types of cancer.

**Conclusion:** The conclusion is that the use of foundation models and multimodal fusion can effectively decipher the physiological and phenotypic organization of cells in complex tissue environments.

**Abstract:** Xenium, a new spatial transcriptomics platform, enables
subcellular-resolution profiling of complex tumor tissues. Despite the rich
morphological information in histology images, extracting robust cell-level
features and integrating them with spatial transcriptomics data remains a
critical challenge. We introduce CellSymphony, a flexible multimodal framework
that leverages foundation model-derived embeddings from both Xenium
transcriptomic profiles and histology images at true single-cell resolution. By
learning joint representations that fuse spatial gene expression with
morphological context, CellSymphony achieves accurate cell type annotation and
uncovers distinct microenvironmental niches across three cancer types. This
work highlights the potential of foundation models and multimodal fusion for
deciphering the physiological and phenotypic orchestration of cells within
complex tissue ecosystems.

</details>


### [41] [Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets](https://arxiv.org/abs/2508.10256)
*Xinan Zhang,Haolin Wang,Yung-An Hsieh,Zhongyu Yang,Anthony Yezzi,Yi-Chang Tsai*

Main category: cs.CV

> 这篇论文综述了深度学习在裂缝检测领域的最新趋势，包括新的学习范式、模型泛化能力和数据集多样性的改进，并引入了一个新的3D激光扫描数据集3DCrack来进行基准测试。

<details>
  <summary>Details</summary>

**Motivation:** 随着深度学习技术的发展，裂缝检测方法也在不断进步。然而，现有研究缺乏对这些趋势的系统性分析。本文旨在填补这一空白，提供最新的技术趋势分析。

**Method:** 本文采用系统性方法分析深度学习在裂缝检测领域的最新技术趋势，包括新的学习范式、模型泛化能力和数据集多样性改进。

**Result:** 通过分析各种趋势，本文高亮了一些代表性的工作，并引入了3DCrack数据集来支持未来的研究。通过广泛的基准测试实验，我们建立了深度学习方法的基线。

**Conclusion:** 本文总结了深度学习在裂缝检测领域的发展趋势，提供了新的基准数据集3DCrack，为未来的研究提供了宝贵的信息。

**Abstract:** Crack detection plays a crucial role in civil infrastructures, including
inspection of pavements, buildings, etc., and deep learning has significantly
advanced this field in recent years. While numerous technical and review papers
exist in this domain, emerging trends are reshaping the landscape. These shifts
include transitions in learning paradigms (from fully supervised learning to
semi-supervised, weakly-supervised, unsupervised, few-shot, domain adaptation
and fine-tuning foundation models), improvements in generalizability (from
single-dataset performance to cross-dataset evaluation), and diversification in
dataset reacquisition (from RGB images to specialized sensor-based data). In
this review, we systematically analyze these trends and highlight
representative works. Additionally, we introduce a new dataset collected with
3D laser scans, 3DCrack, to support future research and conduct extensive
benchmarking experiments to establish baselines for commonly used deep learning
methodologies, including recent foundation models. Our findings provide
insights into the evolving methodologies and future directions in deep
learning-based crack detection. Project page:
https://github.com/nantonzhang/Awesome-Crack-Detection

</details>


### [42] [MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs](https://arxiv.org/abs/2508.10264)
*Haonan Ge,Yiwei Wang,Ming-Hsuan Yang,Yujun Cai*

Main category: cs.CV

> 提出多区域融合解码（MRFD）方法解决大型视觉-语言模型产生的幻觉问题，增强模型事实准确性。

<details>
  <summary>Details</summary>

**Motivation:** 大型视觉-语言模型在处理多模态任务时可能存在信息验证能力的不足，因此常常会产生与视觉输入不一致的文本。MRFD旨在解决这一问题，通过改进模型的区域间一致性来增强其事实准确性的表现。

**Method:** MRFD通过交叉注意力识别图像中的重要区域，并为每个区域生成初始响应。通过计算响应间的Jensen-Shannon散度（JSD）来确定每个响应的可靠性权重。利用这些权重，MRFD采用一种基于区域标记的提示方法，对各个区域的预测进行一致性导向的融合。

**Result:** 大型视觉-语言模型（LVLMs）在多模态任务中表现出色，但它们常常产生与视觉输入不一致的文本（幻觉），原因是这些模型在验证图像不同区域的信息方面能力有限。为此，我们提出了一种无训练的解码方法——多区域融合解码（MRFD），通过建模区域间的相互一致性来提升事实依据性。MRFD通过交叉注意力识别显著区域，为每个区域生成初始响应，并基于响应之间的Jensen-Shannon散度（JSD）计算可靠性权重。这些权重指引了一种基于区域认知提示的连贯性感知的区域预测融合，这种方法受到了思维链推理的启发。实验结果显示，MRFD能够显著减少幻觉并提升响应的事实性，而且无需更新模型。

**Conclusion:** MRFD作为一个不需模型再训练的解码方法，在不更新模型的前提下能够有效减少幻想并提高响应的准确性，适用于多种大型视觉-语言模型和基准测试。

**Abstract:** Large Vision-Language Models (LVLMs) have shown strong performance across
multimodal tasks. However, they often produce hallucinations -- text that is
inconsistent with visual input, due to the limited ability to verify
information in different regions of the image. To address this, we propose
Multi-Region Fusion Decoding (MRFD), a training-free decoding method that
improves factual grounding by modeling inter-region consistency. MRFD
identifies salient regions using cross-attention, generates initial responses
for each, and computes reliability weights based on Jensen-Shannon Divergence
(JSD) among the responses. These weights guide a consistency-aware fusion of
per-region predictions, using region-aware prompts inspired by Chain-of-Thought
reasoning. Experiments across multiple LVLMs and benchmarks show that MRFD
significantly reduces hallucinations and improves response factuality without
requiring model updates.

</details>


### [43] [Pose-Robust Calibration Strategy for Point-of-Gaze Estimation on Mobile Phones](https://arxiv.org/abs/2508.10268)
*Yujie Zhao,Jiabei Zeng,Shiguang Shan*

Main category: cs.CV

> This paper investigates the factors affecting calibrated point-of-gaze estimators and develops a strategy for pose-robust calibration. A systematic analysis of a custom benchmark, MobilePoG, which includes images from 32 individuals under various head poses, was conducted to inform the development of a new dynamic calibration approach that is more robust to head pose variations.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this study is to address the issues with current appearance-based point-of-gaze (PoG) estimators, which struggle to generalize across different individuals and are sensitive to variations in head pose. The study aims to explore key factors influencing calibrated PoG estimators and to develop more robust calibration strategies that can handle these variations.

**Method:** The method involves constructing a benchmark called MobilePoG, which includes facial images taken from 32 different individuals focusing on specific points under either fixed or continuously changing head poses. The authors systematically analyzed the benchmark to understand the impact of diversity in calibration points and head poses on estimation accuracy. Based on their findings, they proposed a dynamic calibration strategy where users perform calibration by moving their phones while fixing their gaze on calibration points, naturally introducing head pose variations into the calibration process.

**Result:** The analysis showed that introducing a broader range of head poses during calibration significantly improves the estimator's performance. The authors proposed a dynamic calibration strategy where users perform calibration by moving their phones while looking at calibration points, which increases head pose variation and leads to better calibrated PoG estimators that are less affected by head pose changes.

**Conclusion:** The study concludes that a dynamic calibration strategy where users move their phones during calibration can lead to PoG estimators that are more robust to head pose variations, addressing key issues with current calibration methods. The proposed method is user-friendly, efficient and improves the overall accuracy of PoG estimators compared to conventional calibration techniques.

**Abstract:** Although appearance-based point-of-gaze (PoG) estimation has improved, the
estimators still struggle to generalize across individuals due to personal
differences. Therefore, person-specific calibration is required for accurate
PoG estimation. However, calibrated PoG estimators are often sensitive to head
pose variations. To address this, we investigate the key factors influencing
calibrated estimators and explore pose-robust calibration strategies.
Specifically, we first construct a benchmark, MobilePoG, which includes facial
images from 32 individuals focusing on designated points under either fixed or
continuously changing head poses. Using this benchmark, we systematically
analyze how the diversity of calibration points and head poses influences
estimation accuracy. Our experiments show that introducing a wider range of
head poses during calibration improves the estimator's ability to handle pose
variation. Building on this insight, we propose a dynamic calibration strategy
in which users fixate on calibration points while moving their phones. This
strategy naturally introduces head pose variation during a user-friendly and
efficient calibration process, ultimately producing a better calibrated PoG
estimator that is less sensitive to head pose variations than those using
conventional calibration strategies. Codes and datasets are available at our
project page.

</details>


### [44] [High Fidelity Text to Image Generation with Contrastive Alignment and Structural Guidance](https://arxiv.org/abs/2508.10280)
*Danyi Gao*

Main category: cs.CV

> 本文提出了一种新的文本驱动图像生成方法，通过对比学习和结构引导增强了图像的语义匹配和结构保真度，并在实验中证实了其在多种性能指标上的优势。

<details>
  <summary>Details</summary>

**Motivation:** 论文旨在解决现有基于文本生成图像方法在语义对齐准确率和结构一致性方面的性能瓶颈。

**Method:** 提出了一个集成文本-图像对比约束和结构引导机制的图像生成方法，引入了对比学习模块，同时使用语义布局图或边缘草图作为结构先验，以提高空间层次的结构建模能力。通过多目标监督机制共同优化对比损失、结构一致性损失和语义保持损失。

**Result:** 该论文通过集成文本-图像对比约束和结构引导机制，提出了一种高保真的图像生成方法。引入了对比学习模块，增强了文本与图像之间的语义匹配。同时，使用语义布局图或边缘草图等结构先验知识，提高了空间层次的结构建模能力。该方法在整体框架中，通过多目标监督机制优化了对比损失、结构一致性损失和语义保持损失，实验结果证实，该方法在CLIP评分、FID和SSIM等定量指标上表现优异，成功地在不增加计算复杂度的前提下实现了语义对齐和结构保真的桥梁，展示了生成语义清晰、结构完整图像的强大能力。

**Conclusion:** 实验结果表明，该方法在多项性能指标上表现优异，能有效生成具有高语义清晰度和结构完整度的图像。这种方法为联合文本-图像建模和图像生成提供了一条可行的技术路径。

**Abstract:** This paper addresses the performance bottlenecks of existing text-driven
image generation methods in terms of semantic alignment accuracy and structural
consistency. A high-fidelity image generation method is proposed by integrating
text-image contrastive constraints with structural guidance mechanisms. The
approach introduces a contrastive learning module that builds strong
cross-modal alignment constraints to improve semantic matching between text and
image. At the same time, structural priors such as semantic layout maps or edge
sketches are used to guide the generator in spatial-level structural modeling.
This enhances the layout completeness and detail fidelity of the generated
images. Within the overall framework, the model jointly optimizes contrastive
loss, structural consistency loss, and semantic preservation loss. A
multi-objective supervision mechanism is adopted to improve the semantic
consistency and controllability of the generated content. Systematic
experiments are conducted on the COCO-2014 dataset. Sensitivity analyses are
performed on embedding dimensions, text length, and structural guidance
strength. Quantitative metrics confirm the superior performance of the proposed
method in terms of CLIP Score, FID, and SSIM. The results show that the method
effectively bridges the gap between semantic alignment and structural fidelity
without increasing computational complexity. It demonstrates a strong ability
to generate semantically clear and structurally complete images, offering a
viable technical path for joint text-image modeling and image generation.

</details>


### [45] [VIFSS: View-Invariant and Figure Skating-Specific Pose Representation Learning for Temporal Action Segmentation](https://arxiv.org/abs/2508.10281)
*Ryota Tanaka,Tomohiro Suzuki,Keisuke Fujii*

Main category: cs.CV

> We propose a novel TAS framework for figure skating jumps that addresses the limitations of existing methods by incorporating the 3D nature and semantic procedure of jump movements and achieving an F1@50 score over 92% on element-level TAS.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for our work arises from the limitations of existing Temporal Action Segmentation (TAS) methods for figure skating jumps, which face issues with insufficient annotated data and a failure to consider the 3D and procedural aspects of jump actions.

**Method:** Our method consists of two key components: the View-Invariant, Figure Skating-Specific (VIFSS) pose representation learning approach, which uses contrastive learning as pre-training and action classification for fine-tuning; and a fine-grained annotation scheme that captures the procedural structure of jumps, specifically marking 'entry (preparation)' and 'landing' phases.

**Result:** Our approach achieves an F1@50 score over 92% on element-level TAS, demonstrating its effectiveness. Moreover, view-invariant contrastive pre-training shows particular effectiveness under data scarcity, highlighting the practicality for real-world applications.

**Conclusion:** This work presents a new TAS framework tailored for figure skating jumps, effectively addressing key challenges in jump timing and type recognition. The approach not only significantly improves performance but also shows practical potential in data-limited scenarios, thereby advancing the field of video action understanding in specialized domains.

**Abstract:** Understanding human actions from videos plays a critical role across various
domains, including sports analytics. In figure skating, accurately recognizing
the type and timing of jumps a skater performs is essential for objective
performance evaluation. However, this task typically requires expert-level
knowledge due to the fine-grained and complex nature of jump procedures. While
recent approaches have attempted to automate this task using Temporal Action
Segmentation (TAS), there are two major limitations to TAS for figure skating:
the annotated data is insufficient, and existing methods do not account for the
inherent three-dimensional aspects and procedural structure of jump actions. In
this work, we propose a new TAS framework for figure skating jumps that
explicitly incorporates both the three-dimensional nature and the semantic
procedure of jump movements. First, we propose a novel View-Invariant, Figure
Skating-Specific pose representation learning approach (VIFSS) that combines
contrastive learning as pre-training and action classification as fine-tuning.
For view-invariant contrastive pre-training, we construct FS-Jump3D, the first
publicly available 3D pose dataset specialized for figure skating jumps.
Second, we introduce a fine-grained annotation scheme that marks the ``entry
(preparation)'' and ``landing'' phases, enabling TAS models to learn the
procedural structure of jumps. Extensive experiments demonstrate the
effectiveness of our framework. Our method achieves over 92% F1@50 on
element-level TAS, which requires recognizing both jump types and rotation
levels. Furthermore, we show that view-invariant contrastive pre-training is
particularly effective when fine-tuning data is limited, highlighting the
practicality of our approach in real-world scenarios.

</details>


### [46] [JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics](https://arxiv.org/abs/2508.10287)
*Simindokht Jahangard,Mehrzad Mohammadi,Yi Shen,Zhixi Cai,Hamid Rezatofighi*

Main category: cs.CV

> 本文通过定义推理复杂性并构建自适应查询引擎解决了现有可视化推理基准测试的不足，并扩展了JRDB数据集，用于评估视觉推理在人类密集环境中的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有可视化推理基准存在定义推理复杂性不明确、无法生成不同难度的问题、缺乏结构化步骤注释等缺陷。

**Method:** 本文提出了一个自适应查询引擎，该引擎能够生成不同复杂度和任务定制化的可视化问题，并且提供了详细的中间注释。同时，扩展了JRDB数据集，增加了人-物交互和几何关系注释，构建了JRDB-Reasoning基准测试。

**Result:** 引入的自适应查询引擎和扩展的JRDB数据集可以帮助评估视觉推理框架，并动态评估可视-语言模型在不同推理水平上的表现。

**Conclusion:** 本文解决了现有可视推理基准的缺陷，提供了一个自适应查询引擎和特定基准测试，用于在人类密集的环境中评估视觉推理。

**Abstract:** Recent advances in Vision-Language Models (VLMs) and large language models
(LLMs) have greatly enhanced visual reasoning, a key capability for embodied AI
agents like robots. However, existing visual reasoning benchmarks often suffer
from several limitations: they lack a clear definition of reasoning complexity,
offer have no control to generate questions over varying difficulty and task
customization, and fail to provide structured, step-by-step reasoning
annotations (workflows). To bridge these gaps, we formalize reasoning
complexity, introduce an adaptive query engine that generates customizable
questions of varying complexity with detailed intermediate annotations, and
extend the JRDB dataset with human-object interaction and geometric
relationship annotations to create JRDB-Reasoning, a benchmark tailored for
visual reasoning in human-crowded environments. Our engine and benchmark enable
fine-grained evaluation of visual reasoning frameworks and dynamic assessment
of visual-language models across reasoning levels.

</details>


### [47] [A Sub-Pixel Multimodal Optical Remote Sensing Images Matching Method](https://arxiv.org/abs/2508.10294)
*Tao Huang,Hongbo Pan,Nanxi Zhou,Shun Zhou*

Main category: cs.CV

> 本文提出了一种新的子像素模板匹配方法PCWLAD，通过相位一致性加权最小绝对偏差来提高多模态光学图像的匹配精度，并验证了其优越性。

<details>
  <summary>Details</summary>

**Motivation:** 多模态光学图象匹配的精度容易受到非线性辐射和几何变形的影响而降低，此问题亟需解决。

**Method:** 提出的方法包含两个主要步骤：SSIM粗匹配和WLAD细匹配。粗匹配时使用无滤波器来计算PCs以便保持原始结构细节，细匹配时采用互结构滤波来减少噪声影响，并使用WLAD准则来估计亚像素偏移。

**Result:** 相比于八个现有的最先进的方法，PCWLAD在所有三种类型的数据集上具有更高的匹配正确率和更低的均方根误差（RMSE），达到了约0.4像素的平均匹配精度。

**Conclusion:** PCWLAD能够有效提高多模态光学图像匹配的精度，并且软件及数据集已公开，可作为后续研究的基础。

**Abstract:** High-accuracy matching of multimodal optical images is the basis of geometric
processing. However, the image matching accuracy is usually degraded by the
nonlinear radiation and geometric deformation differences caused by different
spectral responses. To address these problems, we proposed a phase consistency
weighted least absolute deviation (PCWLAD) sub-pixel template matching method
to improve the matching accuracy of multimodal optical images. This method
consists of two main steps: coarse matching with the structural similarity
index measure (SSIM) and fine matching with WLAD. In the coarse matching step,
PCs are calculated without a noise filter to preserve the original structural
details, and template matching is performed using the SSIM. In the fine
matching step, we applied the radiometric and geometric transformation models
between two multimodal PC templates based on the coarse matching. Furthermore,
mutual structure filtering is adopted in the model to mitigate the impact of
noise within the corresponding templates on the structural consistency, and the
WLAD criterion is used to estimate the sub-pixel offset. To evaluate the
performance of PCWLAD, we created three types of image datasets: visible to
infrared Landsat images, visible to near-infrared close-range images, and
visible to infrared uncrewed aerial vehicle (UAV) images. PCWLAD outperformed
existing state-of-the-art eight methods in terms of correct matching rate (CMR)
and root mean square error (RMSE) and reached an average matching accuracy of
approximately 0.4 pixels across all three datasets. Our software and datasets
are publicly available at https://github.com/huangtaocsu/PCWLAD.

</details>


### [48] [InterSyn: Interleaved Learning for Dynamic Motion Synthesis in the Wild](https://arxiv.org/abs/2508.10297)
*Yiyi Ma,Yuanzhi Liang,Xiu Li,Chi Zhang,Xuelong Li*

Main category: cs.CV

> InterSyn框架通过交错学习策略，结合单人和多人动态，生成更真实的交互动作。实验结果表明，与近期方法相比，其生成的动作序列在文本到动作的对齐度和多样性方面表现出色，设定了新的基准。代码将在未来开源。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法通常分别处理单人和多人动态，导致生成的交互动作不够自然。因此，提出了InterSyn框架来解决这一问题。

**Method:** 该框架包含两个关键模块：交错交互合成模块(INS)和相对协调细化模块(REC)。INS模块统一建模单人和交互行为，而REC模块细化相互作用动态和确保角色之间的同步动作。

**Result:** 实验结果表明，InterSyn生成的动作序列在文本到动作的对齐度和多样性方面优于近期方法。

**Conclusion:** InterSyn通过交错学习策略，能够生成更加自然且多样化的交互动作，并在此领域设定了一个新的基准。

**Abstract:** We present Interleaved Learning for Motion Synthesis (InterSyn), a novel
framework that targets the generation of realistic interaction motions by
learning from integrated motions that consider both solo and multi-person
dynamics. Unlike previous methods that treat these components separately,
InterSyn employs an interleaved learning strategy to capture the natural,
dynamic interactions and nuanced coordination inherent in real-world scenarios.
Our framework comprises two key modules: the Interleaved Interaction Synthesis
(INS) module, which jointly models solo and interactive behaviors in a unified
paradigm from a first-person perspective to support multiple character
interactions, and the Relative Coordination Refinement (REC) module, which
refines mutual dynamics and ensures synchronized motions among characters.
Experimental results show that the motion sequences generated by InterSyn
exhibit higher text-to-motion alignment and improved diversity compared with
recent methods, setting a new benchmark for robust and natural motion
synthesis. Additionally, our code will be open-sourced in the future to promote
further research and development in this area.

</details>


### [49] [From Pixel to Mask: A Survey of Out-of-Distribution Segmentation](https://arxiv.org/abs/2508.10309)
*Wenjie Zhao,Jia Li,Yunhui Guo*

Main category: cs.CV

> 本文系统地回顾了自动驾驶场景中OoD分割的最新进展，识别了新兴的挑战，并讨论了有前景的未来研究方向。

<details>
  <summary>Details</summary>

**Motivation:** 随着AI安全性的关注日益增加，出分布(OoD)检测和分割技术受到了越来越多的关注。传统的OoD检测方法只能识别存在OoD对象，但缺乏空间定位，这限制了它们在下游任务中的实用性。OoD分割方法通过以像素级的精度来进行异常对象的空间定位，这对于自动驾驶等安全关键应用至关重要。

**Method:** 该论文将当前的OoD分割方法分为四类：(i) 测试时OoD分割，(ii) 异常值暴露用于监督训练，(iii) 基于重建的方法，(iv) 利用强大的模型的方法。

**Result:** 系统性地回顾了OoD分割技术，并且对自动驾驶场景中的应用做出了详尽的分析。

**Conclusion:** 通过将OoD分割技术分为四个类别，即测试时OoD分割、异常值暴露、基于重建的方法以及利用强大模型的方法，该论文为了解自动驾驶场景下的OoD分割技术和面临的挑战提供了一个结构化的视角。

**Abstract:** Out-of-distribution (OoD) detection and segmentation have attracted growing
attention as concerns about AI security rise. Conventional OoD detection
methods identify the existence of OoD objects but lack spatial localization,
limiting their usefulness in downstream tasks. OoD segmentation addresses this
limitation by localizing anomalous objects at pixel-level granularity. This
capability is crucial for safety-critical applications such as autonomous
driving, where perception modules must not only detect but also precisely
segment OoD objects, enabling targeted control actions and enhancing overall
system robustness. In this survey, we group current OoD segmentation approaches
into four categories: (i) test-time OoD segmentation, (ii) outlier exposure for
supervised training, (iii) reconstruction-based methods, (iv) and approaches
that leverage powerful models. We systematically review recent advances in OoD
segmentation for autonomous-driving scenarios, identify emerging challenges,
and discuss promising future research directions.

</details>


### [50] [Integrating Reinforcement Learning with Visual Generative Models: Foundations and Advances](https://arxiv.org/abs/2508.10316)
*Yuanzhi Liang,Yijie Fang,Rui Li,Ziqi Ni,Ruijie Su,Chi Zhang,Xuelong Li*

Main category: cs.CV

> 论文概述了基于强化学习（RL）的视觉内容生成方法的系统性综述，强调RL作为微调机制和结构组件，在连接生成任务与复杂目标中的作用，并讨论了未来的挑战和方向。

<details>
  <summary>Details</summary>

**Motivation:** 论文动机在于强调当前生成模型虽然在视觉内容合成上取得了显著进步，但其训练的替代目标（如似然性和重构损失）与感知质量、语义准确性和物理真实感的对齐度不高，而强化学习提供了一种优化非微分、偏好驱动和时间结构目标的框架。

**Method:** 分析论文的方法部分涉及到将强化学习作为优化工具整合到生成模型中，该方法涵盖了图像、视频和3D/4D生成任务，不仅用于微调，还作为结构组件来对齐复杂的高级目标。

**Result:** 综述了RL从经典控制到成为通用优化工具的演变过程，以及其在图像、视频和3D/4D生成任务中的融合，展示了RL作为调优和结构部件在这一领域中如何运作。

**Conclusion:** 论文总结了RL在生成模型中的应用，提出了在该领域许多开放挑战和未来研究方向。

**Abstract:** Generative models have made significant progress in synthesizing visual
content, including images, videos, and 3D/4D structures. However, they are
typically trained with surrogate objectives such as likelihood or
reconstruction loss, which often misalign with perceptual quality, semantic
accuracy, or physical realism. Reinforcement learning (RL) offers a principled
framework for optimizing non-differentiable, preference-driven, and temporally
structured objectives. Recent advances demonstrate its effectiveness in
enhancing controllability, consistency, and human alignment across generative
tasks. This survey provides a systematic overview of RL-based methods for
visual content generation. We review the evolution of RL from classical control
to its role as a general-purpose optimization tool, and examine its integration
into image, video, and 3D/4D generation. Across these domains, RL serves not
only as a fine-tuning mechanism but also as a structural component for aligning
generation with complex, high-level goals. We conclude with open challenges and
future research directions at the intersection of RL and generative modeling.

</details>


### [51] [Concepts or Skills? Rethinking Instruction Selection for Multi-modal Models](https://arxiv.org/abs/2508.10339)
*Andrew Bai,Justin Cui,Ruochen Wang,Cho-Jui Hsieh*

Main category: cs.CV

> 本篇论文提出了一种有针对性的数据选择方法，通过分析基准测试中的概念和技能，以优化视觉-语言模型的表现。实验结果显示，该方法在多个基准测试上表现优于现有的最佳基线方案。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于发现视觉-语言基准测试主要受益于相似技能或视觉概念的训练，从而提出了一种优化性能的方法。

**Method:** 我们的方法包括从基准测试中提取概念/技能，确定基准测试主要受益于相似的概念还是技能，然后选择匹配的概念/技能的指令。这是一种简单的有针对性的训练数据选择方法，旨在优化特定基准测试的表现。

**Result:** 实验结果验证了所提出的有针对性的数据选择方法在10个以上基准测试中的有效性，平均比现有最佳基线方案提高了0.9%，在技能聚焦的子集上提高了1.5%。

**Conclusion:** 论文结论指出，教学指令选择中存在概念知识掌握与视觉技能之间的内在权衡，强调了认识这一权衡的重要性。

**Abstract:** Vision-language instruction tuning achieves two main purposes: learning
visual concepts and learning visual skills. In this paper, we found that
vision-language benchmarks fall into the dichotomy of mainly benefiting from
training on instructions with similar skills or visual concepts. Inspired by
the discovery, we designed a simple targeted training data selection method to
optimize the performance of a given benchmark. We first extract the
concepts/skills from the benchmark, determine whether the benchmark
predominantly benefits from similar concepts or skills, and finally select
instructions with the most matching concepts/skills. Experiments on 10+
benchmarks validate the effectiveness of our targeted data selection method,
showing +0.9\% over the best existing baseline averaged over all benchmarks and
+1.5\% on the skill-focused subset. Our findings underscore the importance of
recognizing the inherent trade-off within instruction selection, which requires
balancing the acquisition of conceptual knowledge against visual skill.

</details>


### [52] [Glo-DMU: A Deep Morphometry Framework of Ultrastructural Characterization in Glomerular Electron Microscopic Images](https://arxiv.org/abs/2508.10351)
*Zhentai Zhang,Danyi Weng,Guibin Zhang,Xiang Chen,Kaixing Long,Jian Geng,Yanmeng Lu,Lei Zhang,Zhitao Zhou,Lei Cao*

Main category: cs.CV

> 研究提出了Glo-DMU框架，使用三个深度学习模型对肾小球超微结构进行自动量化分析，展示了良好的临床一致性，为肾生物活检提供了一种高效辅助工具。

<details>
  <summary>Details</summary>

**Motivation:** 现有的研究主要集中在超微结构的单独识别，这使得满足实际诊断需求具有挑战性。该研究旨在通过自动化的、高精度的和高通量的方法，同时量化多个超微结构特征，为肾病理学家提供有效的辅助工具。

**Method:** 研究提出了Glo-DMU框架，基于三个深度模型：超微结构分割模型，肾小球滤过屏障区域分类模型和电子致密沉积物检测模型，以量化肾小球超微结构的三个广泛使用的特征：肾小球基底膜厚度，足突融合程度和电子致密沉积物的位置。

**Result:** 在对9种肾病病理类型的115名患者进行了评估，展示了自动化量化结果与病理报告中的形态描述之间良好的一致性。

**Conclusion:** 研究表明，Glo-DMU框架在现实世界的诊断场景中能提供自动量化结果与病理报告中形态描述之间良好的一致性。

**Abstract:** Complex and diverse ultrastructural features can indicate the type,
progression, and prognosis of kidney diseases. Recently, computational
pathology combined with deep learning methods has shown tremendous potential in
advancing automatic morphological analysis of glomerular ultrastructure.
However, current research predominantly focuses on the recognition of
individual ultrastructure, which makes it challenging to meet practical
diagnostic needs. In this study, we propose the glomerular morphometry
framework of ultrastructural characterization (Glo-DMU), which is grounded on
three deep models: the ultrastructure segmentation model, the glomerular
filtration barrier region classification model, and the electron-dense deposits
detection model. Following the conventional protocol of renal biopsy diagnosis,
this framework simultaneously quantifies the three most widely used
ultrastructural features: the thickness of glomerular basement membrane, the
degree of foot process effacement, and the location of electron-dense deposits.
We evaluated the 115 patients with 9 renal pathological types in real-world
diagnostic scenarios, demonstrating good consistency between automatic
quantification results and morphological descriptions in the pathological
reports. Glo-DMU possesses the characteristics of full automation, high
precision, and high throughput, quantifying multiple ultrastructural features
simultaneously, and providing an efficient tool for assisting renal
pathologists.

</details>


### [53] [Improving OCR for Historical Texts of Multiple Languages](https://arxiv.org/abs/2508.10356)
*Hylke Westerdijk,Ben Blankenborg,Khondoker Ittehadul Islam*

Main category: cs.CV

> 该研究采用了先进的深度学习技术，针对历史文献的光学字符识别（OCR）和文档布局分析进行了研究，具体包括希伯来古文献、16-18世纪会议决议和现代英文手写文档三个任务。在这三个任务中，研究采用了不同的深度学习模型和技术进行分析，包括数据增强、Kraken和TrOCR模型、CRNN结合DeepLabV3+及双向LSTM、以及CTC损失函数等技术。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在提高历史文本的OCR和文档布局分析的准确性，推动该领域的技术进步。选择这三个任务是为了验证所提出方法在不同文本和历史背景下的有效性。

**Method:** 研究方法包括数据增强技术的应用，使用Kraken和TrOCR模型处理希伯来古文献，CRNN结合DeepLabV3+和双向LSTM的模型处理16-18世纪会议决议，以及使用带有ResNet34编码器的CRNN和CTC损失函数来处理现代英文手写文档。

**Result:** 研究成功提高了一系列文档和文本的OCR识别率和文档布局分析的准确性，并提供了对每个任务的深度学习模型和技术应用的效果分析。

**Conclusion:** 研究提出的方法显著提升了文献OCR和布局分析的准确性，为未来的研究提供了新的视角和技术路径。

**Abstract:** This paper presents our methodology and findings from three tasks across
Optical Character Recognition (OCR) and Document Layout Analysis using advanced
deep learning techniques. First, for the historical Hebrew fragments of the
Dead Sea Scrolls, we enhanced our dataset through extensive data augmentation
and employed the Kraken and TrOCR models to improve character recognition. In
our analysis of 16th to 18th-century meeting resolutions task, we utilized a
Convolutional Recurrent Neural Network (CRNN) that integrated DeepLabV3+ for
semantic segmentation with a Bidirectional LSTM, incorporating confidence-based
pseudolabeling to refine our model. Finally, for modern English handwriting
recognition task, we applied a CRNN with a ResNet34 encoder, trained using the
Connectionist Temporal Classification (CTC) loss function to effectively
capture sequential dependencies. This report offers valuable insights and
suggests potential directions for future research.

</details>


### [54] [AtomDiffuser: Time-Aware Degradation Modeling for Drift and Beam Damage in STEM Imaging](https://arxiv.org/abs/2508.10359)
*Hao Wang,Hongkui Zheng,Kai He,Abolfazl Razi*

Main category: cs.CV

> 本文介绍了一种新框架AtomDiffuser，通过时间感知退化模型分离STEM数据中的空间漂移和辐射度量衰减，适用于冷冻STEM数据，有利于材料动态的分析。

<details>
  <summary>Details</summary>

**Motivation:** 在现代材料科学中，扫描透射电子显微镜(STEM)对于直接成像原子结构及其在外干扰下的演变具有关键作用。然而，由于空间漂移和辐射损伤引起的时间分辨STEM数据的解释仍然具有挑战性。这些因素以复杂且时间上相互关联的方式扭曲了几何和强度，使得现有方法很难显式分离它们的影响，或者在原子分辨率下建模材料动态。

**Method:** 提出了一种名为AtomDiffuser的时间感知退化建模框架，该框架通过预测任意两个STEM帧之间的仿射变换和空间变化衰减图来解耦样品漂移和辐射度量衰减。

**Result:** 该方法利用退化作为一种物理上启发式且时间调节的过程，使得跨时间的可解释结构演变成为可能。训练在合成退化过程中，AtomDiffuser也能够很好地泛化到现实世界的冷冻STEM数据。它还支持高分辨率退化推断和漂移对齐，提供工具来可视化和量化与辐射诱导的原子不稳定性相关联的退化模式。

**Conclusion:** AtomDiffuser提供了一种新的方式来处理和解释复杂的时间分辨STEM数据，通过直观地建模退化过程，可以帮助研究人员更好地理解和量化材料动态。

**Abstract:** Scanning transmission electron microscopy (STEM) plays a critical role in
modern materials science, enabling direct imaging of atomic structures and
their evolution under external interferences. However, interpreting
time-resolved STEM data remains challenging due to two entangled degradation
effects: spatial drift caused by mechanical and thermal instabilities, and
beam-induced signal loss resulting from radiation damage. These factors distort
both geometry and intensity in complex, temporally correlated ways, making it
difficult for existing methods to explicitly separate their effects or model
material dynamics at atomic resolution. In this work, we present AtomDiffuser,
a time-aware degradation modeling framework that disentangles sample drift and
radiometric attenuation by predicting an affine transformation and a spatially
varying decay map between any two STEM frames. Unlike traditional denoising or
registration pipelines, our method leverages degradation as a physically
heuristic, temporally conditioned process, enabling interpretable structural
evolutions across time. Trained on synthetic degradation processes,
AtomDiffuser also generalizes well to real-world cryo-STEM data. It further
supports high-resolution degradation inference and drift alignment, offering
tools for visualizing and quantifying degradation patterns that correlate with
radiation-induced atomic instabilities.

</details>


### [55] [Contrast Sensitivity Function of Multimodal Vision-Language Models](https://arxiv.org/abs/2508.10367)
*Pablo Hernández-Cámara,Alexandra Gomez-Villa,Jose Manuel Jaén-Lorites,Jorge Vila-Tomás,Jesus Malo,Valero Laparra*

Main category: cs.CV

> 论文提出了一种新的方法，用于评估多模态视觉语言模型接近人类视觉对比度敏感性的程度，发现这些模型的反应与人类感知存在差异，提示表达方式对此有显著影响。

<details>
  <summary>Details</summary>

**Motivation:** 评估多模态视觉语言模型的低级别视觉特征与人类感知的对齐程度是至关重要的。本文提出的方法更加接近心理物理学中的真实实验。

**Method:** 使用带通滤波噪声图像和多样化的提示，研究多个架构的模型反应，通过直接提示模型在不同对比度下判断模式可见性来估计其CSF。

**Result:** 本文提出了一种新的基于行为心理物理学的方法来评估面向聊天的多模态视觉语言模型在不同对比度下的空间频率敏感度函数(CSF)，发现虽然某些模型能近似模拟人类CSF的形状或大小，但没有一个模型能完全复制。文章揭示了多模态模型的视觉表现与人类视觉感知之间的关键差异，并指出了提示语句对模型反应有显著影响，引发了对于提示稳定性的关注。

**Conclusion:** 揭示了在多模态视觉模型中探测视觉敏感性的新框架，并指出模型的视觉表现与人类感知之间存在关键差距，尤其是提示的表达方式对模型的反应有重大影响。

**Abstract:** Assessing the alignment of multimodal vision-language models~(VLMs) with
human perception is essential to understand how they perceive low-level visual
features. A key characteristic of human vision is the contrast sensitivity
function (CSF), which describes sensitivity to spatial frequency at
low-contrasts. Here, we introduce a novel behavioral psychophysics-inspired
method to estimate the CSF of chat-based VLMs by directly prompting them to
judge pattern visibility at different contrasts for each frequency. This
methodology is closer to the real experiments in psychophysics than the
previously reported. Using band-pass filtered noise images and a diverse set of
prompts, we assess model responses across multiple architectures. We find that
while some models approximate human-like CSF shape or magnitude, none fully
replicate both. Notably, prompt phrasing has a large effect on the responses,
raising concerns about prompt stability. Our results provide a new framework
for probing visual sensitivity in multimodal models and reveal key gaps between
their visual representations and human perception.

</details>


### [56] [Towards Spatially Consistent Image Generation: On Incorporating Intrinsic Scene Properties into Diffusion Models](https://arxiv.org/abs/2508.10382)
*Hyundo Lee,Suhyung Choi,Byoung-Tak Zhang,Inwoo Hwang*

Main category: cs.CV

> 我们提出了一种新方法，通过同时生成图像和其对应的内在场景特性来生成更一致和现实的图像。我们的方法在实验中展现出改善空间一致性的同时，保持了图像质量和文本一致性。

<details>
  <summary>Details</summary>

**Motivation:** 图像生成模型在大型数据集上训练时可以合成高质量的图像，但由于缺乏关于潜在结构和空间布局的信息，它们经常产生空间不一致和扭曲的图像。我们的动机是利用比先前方法更多的场景内在属性信息（如深度，分割图），超越仅依赖图像-文本对或使用内在属性作为条件输入的方法。

**Method:** 我们提出的方法旨在同时生成图像及其对应的内在场景特性，以隐式捕捉潜在的场景结构，并生成更空间一致和逼真的图像。首先，我们使用预训练的估计器从大型图像数据集中提取丰富的内在场景属性，消除了对额外场景信息或显式3D表示的需求。然后，我们使用自编码器将各种内在场景特性聚合到单一的潜在变量中。基于预训练的大规模潜在扩散模型（LDMs），我们的方法通过仔细共享相互信息，同时消除图像和内在领域的噪声，使得图像和内在特性相互反映而不降低图像质量。

**Result:** 实验结果表明，我们的方法能够纠正空间不一致性，并产生更自然的场景布局，同时保持基础模型（如Stable Diffusion）的保真度和文本一致性。

**Conclusion:** 通过提出的方法，我们成功生成了更空间一致和逼真的图像，消除了仅依赖图像-文本对或使用内在属性作为条件输入的方法所带来的空间不一致性问题。

**Abstract:** Image generation models trained on large datasets can synthesize high-quality
images but often produce spatially inconsistent and distorted images due to
limited information about the underlying structures and spatial layouts. In
this work, we leverage intrinsic scene properties (e.g., depth, segmentation
maps) that provide rich information about the underlying scene, unlike prior
approaches that solely rely on image-text pairs or use intrinsics as
conditional inputs. Our approach aims to co-generate both images and their
corresponding intrinsics, enabling the model to implicitly capture the
underlying scene structure and generate more spatially consistent and realistic
images. Specifically, we first extract rich intrinsic scene properties from a
large image dataset with pre-trained estimators, eliminating the need for
additional scene information or explicit 3D representations. We then aggregate
various intrinsic scene properties into a single latent variable using an
autoencoder. Building upon pre-trained large-scale Latent Diffusion Models
(LDMs), our method simultaneously denoises the image and intrinsic domains by
carefully sharing mutual information so that the image and intrinsic reflect
each other without degrading image quality. Experimental results demonstrate
that our method corrects spatial inconsistencies and produces a more natural
layout of scenes while maintaining the fidelity and textual alignment of the
base model (e.g., Stable Diffusion).

</details>


### [57] [Unlocking Robust Semantic Segmentation Performance via Label-only Elastic Deformations against Implicit Label Noise](https://arxiv.org/abs/2508.10383)
*Yechan Kim,Dongho Yoon,Younkwan Lee,Unse Fatima,Hong Kook Kim,Songjae Lee,Sanga Park,Jeong Ho Park,Seonjong Kang,Moongu Jeon*

Main category: cs.CV

> NSegment+ is a new data augmentation technique for semantic segmentation that separately modifies segmentation labels with elastic deformations to tackle hidden label imperfections, thereby improving model robustness and performance.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address subtle and implicit label imperfections that are not handled effectively by conventional augmentation techniques in real-world image segmentation tasks. These imperfections can degrade model performance and are typically caused by ambiguous object boundaries and annotator variability.

**Method:** The paper introduces NSegment+, a new data augmentation framework for semantic segmentation tasks. NSegment+ decouples the transformation of images and labels, applying elastic deformations to labels while maintaining original images to tackle implicit label noise.

**Result:** Experiments demonstrate that NSegment+ can improve performance by achieving mIoU gains of up to 2.29, 2.38, 1.75, and 3.39 on Vaihingen, LoveDA, Cityscapes, and PASCAL VOC datasets, respectively. The technique's effectiveness is further enhanced when combined with other training techniques.

**Conclusion:** NSegment+ effectively enhances the robustness of segmentation models by mitigating the impact of implicit label noise, leading to significant performance improvements across multiple datasets. This approach underscores the importance of addressing subtle label imperfections in real-world datasets.

**Abstract:** While previous studies on image segmentation focus on handling severe (or
explicit) label noise, real-world datasets also exhibit subtle (or implicit)
label imperfections. These arise from inherent challenges, such as ambiguous
object boundaries and annotator variability. Although not explicitly present,
such mild and latent noise can still impair model performance. Typical data
augmentation methods, which apply identical transformations to the image and
its label, risk amplifying these subtle imperfections and limiting the model's
generalization capacity. In this paper, we introduce NSegment+, a novel
augmentation framework that decouples image and label transformations to
address such realistic noise for semantic segmentation. By introducing
controlled elastic deformations only to segmentation labels while preserving
the original images, our method encourages models to focus on learning robust
representations of object structures despite minor label inconsistencies.
Extensive experiments demonstrate that NSegment+ consistently improves
performance, achieving mIoU gains of up to +2.29, +2.38, +1.75, and +3.39 in
average on Vaihingen, LoveDA, Cityscapes, and PASCAL VOC, respectively-even
without bells and whistles, highlighting the importance of addressing implicit
label noise. These gains can be further amplified when combined with other
training tricks, including CutMix and Label Smoothing.

</details>


### [58] [PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection](https://arxiv.org/abs/2508.10397)
*Haibin Sun,Xinghui Song*

Main category: cs.CV

> 本文提出PQ-DAF模型解决驾驶员注意力分散检测中数据标注成本高和领域偏移问题，通过合成高质量数据和样本过滤提高检测性能。

<details>
  <summary>Details</summary>

**Motivation:** 为了解决驾驶员注意力分散检测模型在实际环境中数据标注成本高的难题，以及训练数据与目标部署条件之间存在显著领域偏移的问题。

**Method:** 提出了一种基于姿态驱动的质量控制数据增强框架 (PQ-DAF)，利用视觉语言模型进行样本过滤，以经济地扩展训练数据并增强跨域鲁棒性。具体来说，采用了渐进性条件扩散模型（PCDMs）来准确捕捉驾驶员姿势特征并合成多样化的训练样本。通过基于CogVLM视觉-语言模型构建样本质量评估模块，在置信度阈值基础上过滤出低质量的合成样本，确保增强数据集的有效性。

**Result:** 实验结果表明，PQ-DAF显著提高了驾驶员注意力分散检测在样本量较少情况下的性能，大幅提升了模型在数据稀缺条件下的泛化能力。

**Conclusion:** PQ-DAF在数据稀缺条件下实现了显著的性能增益，证明了其在驾驶员注意力分散检测中的有效性和泛化能力。

**Abstract:** Driver distraction detection is essential for improving traffic safety and
reducing road accidents. However, existing models often suffer from degraded
generalization when deployed in real-world scenarios. This limitation primarily
arises from the few-shot learning challenge caused by the high cost of data
annotation in practical environments, as well as the substantial domain shift
between training datasets and target deployment conditions. To address these
issues, we propose a Pose-driven Quality-controlled Data Augmentation Framework
(PQ-DAF) that leverages a vision-language model for sample filtering to
cost-effectively expand training data and enhance cross-domain robustness.
Specifically, we employ a Progressive Conditional Diffusion Model (PCDMs) to
accurately capture key driver pose features and synthesize diverse training
examples. A sample quality assessment module, built upon the CogVLM
vision-language model, is then introduced to filter out low-quality synthetic
samples based on a confidence threshold, ensuring the reliability of the
augmented dataset. Extensive experiments demonstrate that PQ-DAF substantially
improves performance in few-shot driver distraction detection, achieving
significant gains in model generalization under data-scarce conditions.

</details>


### [59] [Translation of Text Embedding via Delta Vector to Suppress Strongly Entangled Content in Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.10407)
*Eunseo Koh,Seunghoo Hong,Tae-Young Kim,Simon S. Woo,Jae-Pil Heo*

Main category: cs.CV

> The paper introduces a method to suppress undesired components in text-to-image generation through adjusting text embeddings using a delta vector approach.

<details>
  <summary>Details</summary>

**Motivation:** To address the issue where text-to-image models generate strong entangled content (like a mustache appearing with Charlie Chaplin) even when instructed otherwise.

**Method:** Introduces a delta vector to modify text embeddings for weakening the influence of undesired elements and a Selective Suppression with Delta Vector method to adapt this adjustment into cross-attention.

**Result:** The proposed model achieves significant improvements over existing methods, as evidenced by both quantitative and qualitative evaluations.

**Conclusion:** The approach effectively suppresses undesired content in generated images by modifying the text embedding space, increasing control over image generation.

**Abstract:** Text-to-Image (T2I) diffusion models have made significant progress in
generating diverse high-quality images from textual prompts. However, these
models still face challenges in suppressing content that is strongly entangled
with specific words. For example, when generating an image of ``Charlie
Chaplin", a ``mustache" consistently appears even if explicitly instructed not
to include it, as the concept of ``mustache" is strongly entangled with
``Charlie Chaplin". To address this issue, we propose a novel approach to
directly suppress such entangled content within the text embedding space of
diffusion models. Our method introduces a delta vector that modifies the text
embedding to weaken the influence of undesired content in the generated image,
and we further demonstrate that this delta vector can be easily obtained
through a zero-shot approach. Furthermore, we propose a Selective Suppression
with Delta Vector (SSDV) method to adapt delta vector into the cross-attention
mechanism, enabling more effective suppression of unwanted content in regions
where it would otherwise be generated. Additionally, we enabled more precise
suppression in personalized T2I models by optimizing delta vector, which
previous baselines were unable to achieve. Extensive experimental results
demonstrate that our approach significantly outperforms existing methods, both
in terms of quantitative and qualitative metrics.

</details>


### [60] [SC-Lane: Slope-aware and Consistent Road Height Estimation Framework for 3D Lane Detection](https://arxiv.org/abs/2508.10411)
*Chaesong Park,Eunbin Seo,Jihyeon Hwang,Jongwoo Lim*

Main category: cs.CV

> SC-Lane 提出了一种新型的基于斜率感知且在时间上一致的高度图估计框架，用来进行3D车道检测，通过自适应确定融合斜率特定的高度特征，该方法在多个标准度量下实现了最先进的性能，得分为64.3%的F值，超越了现有方法。

<details>
  <summary>Details</summary>

**Motivation:** SC-Lane 解决了以前方法中依赖固定斜率锚点的问题，改善了对多样性道路几何形状的鲁棒性，并通过一个自适应特征模块动态预测图像线索的适当权重，整合多斜率表征，另外还使用了一致性模块确保时间连续帧之间的稳定准确的高度估计，这些对于实际驾驶场景至关重要。

**Method:** SC-Lane 使用的Slope-Aware Adaptive Feature module能够自适应从图像信息中预测合适的权重来整合多斜率表示，而Height Consistency Module保证了不同帧之间的高度信息的一致性，除了基本方法，还在OpenLane基准测试上进行了性能测试，具体使用了三个标准指标：均值绝对误差(MAE)、均方根误差(RMSE)和基于阈值的准确性。

**Result:** 在OpenLane基准测试中，SC-Lane在高度估测和3D车道检测方面都表现出色，取得了64.3%的F值得分，超越了其他现有方法，显示了其显著的优越性。

**Conclusion:** SC-Lane 通过引入自适应斜率感知和时间一致性，显著改进了3D车道检测，特别是在高度估测方面。这种新技术设立了未来研究和对比的高标准，其表现和效果可以在其项目页面中得到更详细的观察和分析。

**Abstract:** In this paper, we introduce SC-Lane, a novel slope-aware and temporally
consistent heightmap estimation framework for 3D lane detection. Unlike
previous approaches that rely on fixed slope anchors, SC-Lane adaptively
determines the fusion of slope-specific height features, improving robustness
to diverse road geometries. To achieve this, we propose a Slope-Aware Adaptive
Feature module that dynamically predicts the appropriate weights from image
cues for integrating multi-slope representations into a unified heightmap.
Additionally, a Height Consistency Module enforces temporal coherence, ensuring
stable and accurate height estimation across consecutive frames, which is
crucial for real-world driving scenarios. To evaluate the effectiveness of
SC-Lane, we employ three standardized metrics-Mean Absolute Error(MAE), Root
Mean Squared Error (RMSE), and threshold-based accuracy-which, although common
in surface and depth estimation, have been underutilized for road height
assessment. Using the LiDAR-derived heightmap dataset introduced in prior work
[20], we benchmark our method under these metrics, thereby establishing a
rigorous standard for future comparisons. Extensive experiments on the OpenLane
benchmark demonstrate that SC-Lane significantly improves both height
estimation and 3D lane detection, achieving state-of-the-art performance with
an F-score of 64.3%, outperforming existing methods by a notable margin. For
detailed results and a demonstration video, please refer to our project
page:https://parkchaesong.github.io/sclane/

</details>


### [61] [NanoControl: A Lightweight Framework for Precise and Efficient Control in Diffusion Transformer](https://arxiv.org/abs/2508.10424)
*Shanyuan Liu,Jian Zhu,Junda Lu,Yue Gong,Liuzhuozheng Li,Bo Cheng,Yuhang Ma,Liebucha Wu,Xiaoyu Wu,Dawei Leng,Yuhui Yin*

Main category: cs.CV

> This paper introduces NanoControl, a highly efficient method for controllable text-to-image generation using Diffusion Transformers (DiTs), which significantly reduces parameter and computational costs compared to existing methods.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of parameter overhead and increased computational costs associated with the ControlNet paradigm in controllable text-to-image generation using DiT models.

**Method:** NanoControl employs Flux as the backbone network, uses a LoRA-style control module to learn control signals from raw conditioning inputs, and introduces KV-Context Augmentation for efficient fusion of conditional features.

**Result:** The model achieves state-of-the-art performance in controllable text-to-image generation with minimal increases in parameter count and computational cost.

**Conclusion:** NanoControl provides a more efficient solution for controllable text-to-image synthesis, reducing computational overhead while maintaining superior generation quality and achieving improved controllability.

**Abstract:** Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in
text-to-image synthesis. However, in the domain of controllable text-to-image
generation using DiTs, most existing methods still rely on the ControlNet
paradigm originally designed for UNet-based diffusion models. This paradigm
introduces significant parameter overhead and increased computational costs. To
address these challenges, we propose the Nano Control Diffusion Transformer
(NanoControl), which employs Flux as the backbone network. Our model achieves
state-of-the-art controllable text-to-image generation performance while
incurring only a 0.024\% increase in parameter count and a 0.029\% increase in
GFLOPs, thus enabling highly efficient controllable generation. Specifically,
rather than duplicating the DiT backbone for control, we design a LoRA-style
(low-rank adaptation) control module that directly learns control signals from
raw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation
mechanism that integrates condition-specific key-value information into the
backbone in a simple yet highly effective manner, facilitating deep fusion of
conditional features. Extensive benchmark experiments demonstrate that
NanoControl significantly reduces computational overhead compared to
conventional control approaches, while maintaining superior generation quality
and achieving improved controllability.

</details>


### [62] [STRIDE-QA: Visual Question Answering Dataset for Spatiotemporal Reasoning in Urban Driving Scenes](https://arxiv.org/abs/2508.10427)
*Keishi Ishihara,Kento Sasaki,Tsubasa Takahashi,Daiki Shiono,Yu Yamaguchi*

Main category: cs.CV

> STRIDE-QA 数据集旨在改善视觉-语言模型在自动驾驶中的时空推理能力，通过涵盖现实驾驶挑战的数据集提高现有模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 视觉-语言模型在训练过程中依赖于静态、来源于网络的图像-文本对，这限制了它们对复杂动态交通场景的理解和预测能力。为了解决这一问题，STRIDE-QA旨在填补关键空白，通过提供大规模的视觉问答数据集来支持自车视角下的物理推理。

**Method:** STRIDE-QA 是一个大规模的视觉问答数据集，用于自动驾驶中的时空推理。该数据集从东京的多传感器驾驶数据中构建，包含100小时的数据，涵盖多样且具有挑战性的情况。它提供了1600万个问答对，覆盖285K帧。通过三个新颖的问答任务，该数据集支持物体为中心和自车为中心的推理，这些任务要求空间定位和时间预测。

**Result:** 基准测试表明，现有的视觉-语言模型在STRIDE-QA上表现不佳，预测一致性得分接近零。相比之下，经过STRIDE-QA微调的视觉-语言模型在空间定位和未来运动预测方面表现出显著的性能提升，分别达到了55%的成功率和28%的预测一致性。

**Conclusion:** STRIDE-QA数据集为开发更可靠的应用于安全关键的自动驾驶系统的视觉-语言模型提供了全面的基础。

**Abstract:** Vision-Language Models (VLMs) have been applied to autonomous driving to
support decision-making in complex real-world scenarios. However, their
training on static, web-sourced image-text pairs fundamentally limits the
precise spatiotemporal reasoning required to understand and predict dynamic
traffic scenes. We address this critical gap with STRIDE-QA, a large-scale
visual question answering (VQA) dataset for physically grounded reasoning from
an ego-centric perspective. Constructed from 100 hours of multi-sensor driving
data in Tokyo, capturing diverse and challenging conditions, STRIDE-QA is the
largest VQA dataset for spatiotemporal reasoning in urban driving, offering 16
million QA pairs over 285K frames. Grounded by dense, automatically generated
annotations including 3D bounding boxes, segmentation masks, and multi-object
tracks, the dataset uniquely supports both object-centric and ego-centric
reasoning through three novel QA tasks that require spatial localization and
temporal prediction. Our benchmarks demonstrate that existing VLMs struggle
significantly, achieving near-zero scores on prediction consistency. In
contrast, VLMs fine-tuned on STRIDE-QA exhibit dramatic performance gains,
achieving 55% success in spatial localization and 28% consistency in future
motion prediction, compared to near-zero scores from general-purpose VLMs.
Therefore, STRIDE-QA establishes a comprehensive foundation for developing more
reliable VLMs for safety-critical autonomous systems.

</details>


### [63] [CRISP: Contrastive Residual Injection and Semantic Prompting for Continual Video Instance Segmentation](https://arxiv.org/abs/2508.10432)
*Baichen Liu,Qi Lyu,Xudong Wang,Jiahua Dong,Lianqing Liu,Zhi Han*

Main category: cs.CV

> CRISP是一种改进持续视频实例分割技术的方法，通过建立实例级、类别级和任务级学习，维持分割和分类性能，实验结果证明其优于其他方法。

<details>
  <summary>Details</summary>

**Motivation:** 视频实例分割需要系统具备同时吸收新的物体类别和保留以前学习到的类别信息的能力，同时保持帧间的时序一致性。现有方法在长期任务上容易遗忘以前的学习，无法有效平衡新旧信息学习，因此本研究旨在通过CRISP方法提高长期持续视频实例分割的表现，减少灾难性遗忘，提高分割和分类性能。

**Method:** 本研究提出了Contrastive Residual Injection and Semantic Prompting (CRISP)方法，旨在解决持续视频实例分割中的实例级别、类别级别和任务级别的混淆问题。对于实例级别学习，通过实例跟踪建模和细化实例相关损失函数，加强了当前任务查询的特异性。对于类别级别学习，提出了自适应残差语义提示(ARSP)学习框架，构建了一个可学习的语义残差提示池，并引入对比学习建立语义一致性损失来维持训练期间对象查询与残差提示之间的语义一致性。对于任务级别学习，提供了一个简洁但有效的增量提示初始化策略，以保持多个任务间查询空间的关联性。

**Result:** 实验结果显示，CRISP在YouTube-VIS-2019和YouTube-VIS-2021数据集上显著超过了现有的持续分割方法，在长期持续视频实例分割任务上避免了灾难性的遗忘，有效地提高了分割和分类表现。

**Conclusion:** CRISP方法在长期持续视频实例分割任务上表现突出，通过学习实例、类别的相关性并加强任务间的联系，能有效避免遗忘，提高性能。

**Abstract:** Continual video instance segmentation demands both the plasticity to absorb
new object categories and the stability to retain previously learned ones, all
while preserving temporal consistency across frames. In this work, we introduce
Contrastive Residual Injection and Semantic Prompting (CRISP), an earlier
attempt tailored to address the instance-wise, category-wise, and task-wise
confusion in continual video instance segmentation. For instance-wise learning,
we model instance tracking and construct instance correlation loss, which
emphasizes the correlation with the prior query space while strengthening the
specificity of the current task query. For category-wise learning, we build an
adaptive residual semantic prompt (ARSP) learning framework, which constructs a
learnable semantic residual prompt pool generated by category text and uses an
adjustive query-prompt matching mechanism to build a mapping relationship
between the query of the current task and the semantic residual prompt.
Meanwhile, a semantic consistency loss based on the contrastive learning is
introduced to maintain semantic coherence between object queries and residual
prompts during incremental training. For task-wise learning, to ensure the
correlation at the inter-task level within the query space, we introduce a
concise yet powerful initialization strategy for incremental prompts. Extensive
experiments on YouTube-VIS-2019 and YouTube-VIS-2021 datasets demonstrate that
CRISP significantly outperforms existing continual segmentation methods in the
long-term continual video instance segmentation task, avoiding catastrophic
forgetting and effectively improving segmentation and classification
performance. The code is available at https://github.com/01upup10/CRISP.

</details>
