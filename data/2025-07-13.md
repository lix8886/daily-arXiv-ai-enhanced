<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 12]
- [cs.CV](#cs.CV) [Total: 16]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs](https://arxiv.org/abs/2507.07186)
*Itay Itzhak,Yonatan Belinkov,Gabriel Stanovsky*

Main category: cs.CL

> 研究通过因果实验方法发现大型语言模型中的认知偏差主要由预训练决定，而不仅仅是微调效果。这项研究提供了理解和评估大型语言模型偏差的新视角。

<details>
  <summary>Details</summary>

**Motivation:** 以前的工作发现这些偏差在不同的模型间有所差异，并且可以通过指令微调来放大。然而，目前尚不清楚这些偏差差异是否源于预训练、微调，甚至是由于训练随机性引起的随机噪声。因此，研究旨在解决这一问题。

**Method:** 提出了一种两阶段的因果实验方法来分离这些因素。首先，使用不同的随机种子多次微调模型，以研究训练随机性对超过30种认知偏差的影响。其次，引入了跨调优方法，即将指令数据集在模型之间交换，以孤立偏差来源。

**Result:** 研究发现，虽然训练随机性引入了一些变异性，但偏差主要由预训练决定：具有相同预训练基础模型的模型表现出比仅共享微调数据的模型更相似的偏差模式。

**Conclusion:** 这些见解表明，理解微调模型中的偏差需要考虑其预训练起源，而不仅仅是微调效果。这一视角可以指导未来为了开发评估和缓解大型语言模型偏差的策略而做出的努力。

**Abstract:** Large language models (LLMs) exhibit cognitive biases -- systematic
tendencies of irrational decision-making, similar to those seen in humans.
Prior work has found that these biases vary across models and can be amplified
by instruction tuning. However, it remains unclear if these differences in
biases stem from pretraining, finetuning, or even random noise due to training
stochasticity. We propose a two-step causal experimental approach to
disentangle these factors. First, we finetune models multiple times using
different random seeds to study how training randomness affects over $30$
cognitive biases. Second, we introduce \emph{cross-tuning} -- swapping
instruction datasets between models to isolate bias sources. This swap uses
datasets that led to different bias patterns, directly testing whether biases
are dataset-dependent. Our findings reveal that while training randomness
introduces some variability, biases are mainly shaped by pretraining: models
with the same pretrained backbone exhibit more similar bias patterns than those
sharing only finetuning data. These insights suggest that understanding biases
in finetuned models requires considering their pretraining origins beyond
finetuning effects. This perspective can guide future efforts to develop
principled strategies for evaluating and mitigating bias in LLMs.

</details>


### [2] [Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses](https://arxiv.org/abs/2507.07188)
*Jens Rupprecht,Georg Ahnert,Markus Strohmaier*

Main category: cs.CL

> 研究发现九种不同的大语言模型在应对世界价值观调查问题的多样性扰动时表现出一致性“最近效应偏差”，表明使用LLMs生成调查数据时需考虑题目的设计和测试其稳健性。

<details>
  <summary>Details</summary>

**Motivation:** 这项研究的动机来自于大语言模型（LLMs）在社会科学研究中的广泛使用，但它们的可靠性和对已知响应偏差的易感性却不甚明了。

**Method:** 此研究通过在世界价值观调查（WVS）的问题上测试九种不同的大语言模型（LLMs），并对其问题表述和答案选项结构应用了11种不同的扰动进行了综合测试，模拟了超过167,000个访谈场景，以检验LLMs在规范性调查环境中的回答稳健性。

**Result:** 研究揭示了所有测试模型在面临扰动时都表现出脆弱性，并且都存在一定程度上的“最近效应偏差”，倾向于选择最后展示的答案选项。尽管较大模型通常更稳健，所有模型仍然对语义上的变体（如同义句）和组合的扰动敏感。

**Conclusion:** 通过应用一整套扰动，这项研究揭示了LLMs部分地与人类识别的调查响应偏见对齐。这强调了使用LLMs生成合成调查数据时，题目的设计和鲁棒性测试的重要性。

**Abstract:** Large Language Models (LLMs) are increasingly used as proxies for human
subjects in social science surveys, but their reliability and susceptibility to
known response biases are poorly understood. This paper investigates the
response robustness of LLMs in normative survey contexts -- we test nine
diverse LLMs on questions from the World Values Survey (WVS), applying a
comprehensive set of 11 perturbations to both question phrasing and answer
option structure, resulting in over 167,000 simulated interviews. In doing so,
we not only reveal LLMs' vulnerabilities to perturbations but also reveal that
all tested models exhibit a consistent \textit{recency bias} varying in
intensity, disproportionately favoring the last-presented answer option. While
larger models are generally more robust, all models remain sensitive to
semantic variations like paraphrasing and to combined perturbations. By
applying a set of perturbations, we reveal that LLMs partially align with
survey response biases identified in humans. This underscores the critical
importance of prompt design and robustness testing when using LLMs to generate
synthetic survey data.

</details>


### [3] [SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains](https://arxiv.org/abs/2507.07229)
*Krithika Ramesh,Daniel Smolyak,Zihao Zhao,Nupoor Gandhi,Ritu Agarwal,Margrét Bjarnadóttir,Anjalie Field*

Main category: cs.CL

> 简要总结了该分析的目的和工具包的主要功能。

<details>
  <summary>Details</summary>

**Motivation:** 分析此内容的动机在于理解SynthTextEval工具包对增强合成文本在实际场景中应用性及隐私保护的贡献。

**Method:** 使用了Structure函数来对论文摘要进行结构化分析，提取了主要内容。

**Result:** <tool_call>
{{"name": "Structure", "arguments": {"tldr": "SynthTextEval 是一个用于对合成文本进行全面评估的工具包，旨在提升AI系统开发中的隐私保护能力。", "motivation": "随着大型语言模型在生成流畅合成文本方面的进展，该工具包旨在通过一致且全面的评估方法来提高这类文本在各个领域的适用性及隐私保护。", "method": "该工具包能够从多个维度评估合成文本：下游系统的实用性、公平性、隐私泄露风险、与源文本的分布差异以及领域专家的定性反馈。", "result": "通过两个高风险领域的实例（医疗和法律），展示了工具包的有效性。", "conclusion": "该工具包通过标准化评估指标，旨在提高合成文本在高风险领域如医疗、法务中的应用性和隐私保护性能。"}}}
</tool_call>

**Conclusion:** 成功分析并提取了SynthTextEval工具包的动机、方法、结果和结论。

**Abstract:** We present SynthTextEval, a toolkit for conducting comprehensive evaluations
of synthetic text. The fluency of large language model (LLM) outputs has made
synthetic text potentially viable for numerous applications, such as reducing
the risks of privacy violations in the development and deployment of AI systems
in high-stakes domains. Realizing this potential, however, requires principled
consistent evaluations of synthetic data across multiple dimensions: its
utility in downstream systems, the fairness of these systems, the risk of
privacy leakage, general distributional differences from the source text, and
qualitative feedback from domain experts. SynthTextEval allows users to conduct
evaluations along all of these dimensions over synthetic data that they upload
or generate using the toolkit's generation module. While our toolkit can be run
over any data, we highlight its functionality and effectiveness over datasets
from two high-stakes domains: healthcare and law. By consolidating and
standardizing evaluation metrics, we aim to improve the viability of synthetic
text, and in-turn, privacy-preservation in AI development.

</details>


### [4] [Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings](https://arxiv.org/abs/2507.07248)
*Minseon Kim,Jean-Philippe Corbeil,Alessandro Sordoni,Francois Beaulieu,Paul Vozila*

Main category: cs.CL

> 该论文提出了一种专门针对医疗领域的大型语言模型的安全性评估协议，并从患者和医生的角度进行了量化分析，填补了该领域研究的空白。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大型语言模型在广泛领域中被采用，尤其是在医疗领域，但是这些模型的安全性评估大多集中在通用的安全基准上，缺乏医疗领域的针对性评估。这篇论文旨在填补这一研究空白。

**Method:** 此论文引入了一个专门针对医疗领域的安全性评估协议，并从患者和临床医生的角度进行了量化分析。此外，作者构建了一个名为PatientSafetyBench的数据集，包含466个样本，跨越5个关键类别，以从患者的角度衡量安全性。

**Result:** 通过应用针对MediPhi模型集合的红线对抗协议（red-teaming protocols），此研究为医疗领域的大型语言模型的安全性评估奠定了基础。

**Conclusion:** 该工作定义了医疗大型语言模型的安全性评估标准，通过目标明确的红线对抗方法，从患者、临床医生和一般用户三个不同角度出发，为医疗领域的安全部署奠定了基础。

**Abstract:** As the performance of large language models (LLMs) continues to advance,
their adoption is expanding across a wide range of domains, including the
medical field. The integration of LLMs into medical applications raises
critical safety concerns, particularly due to their use by users with diverse
roles, e.g. patients and clinicians, and the potential for model's outputs to
directly affect human health. Despite the domain-specific capabilities of
medical LLMs, prior safety evaluations have largely focused only on general
safety benchmarks. In this paper, we introduce a safety evaluation protocol
tailored to the medical domain in both patient user and clinician user
perspectives, alongside general safety assessments and quantitatively analyze
the safety of medical LLMs. We bridge a gap in the literature by building the
PatientSafetyBench containing 466 samples over 5 critical categories to measure
safety from the perspective of the patient. We apply our red-teaming protocols
on the MediPhi model collection as a case study. To our knowledge, this is the
first work to define safety evaluation criteria for medical LLMs through
targeted red-teaming taking three different points of view - patient,
clinician, and general user - establishing a foundation for safer deployment in
medical domains.

</details>


### [5] [The Impact of Background Speech on Interruption Detection in Collaborative Groups](https://arxiv.org/abs/2507.07280)
*Mariah Bradford,Nikhil Krishnaswamy,Nathaniel Blanchard*

Main category: cs.CL

> 研究开发了一种新的中断检测技术，能够在多组对话的复杂情况下有效识别中断，适用于教室环境。

<details>
  <summary>Details</summary>

**Motivation:** 中断在协作学习中起着至关重要的作用，大多数关于中断检测和解释的工作是在单对话环境中进行的。为了开发出适用于包含重叠语音的多组对话环境的中断检测方法，本研究应运而生。

**Method:** 本研究分析了在单对话和多组对话设置中的中断检测。研究团队开发了一种能够抵御重叠语音干扰的中断识别新技术，并且对协作小组互动中中断的表达方式进行了有意义的语言和韵律信息的研究。

**Result:** 研究结果表明，新技术可以有效地识别重叠语音中的中断，为教室内的AI支持提供了可能性。此外，研究还揭示了多组对话中的重叠语音对跟踪组对话的影响。

**Conclusion:** 本研究开发了一种可以抵御重叠语音干扰的中断识别技术，并探讨了多组对话中断在现实中的可能应用，为未来的研究奠定了基础。

**Abstract:** Interruption plays a crucial role in collaborative learning, shaping group
interactions and influencing knowledge construction. AI-driven support can
assist teachers in monitoring these interactions. However, most previous work
on interruption detection and interpretation has been conducted in
single-conversation environments with relatively clean audio. AI agents
deployed in classrooms for collaborative learning within small groups will need
to contend with multiple concurrent conversations -- in this context,
overlapping speech will be ubiquitous, and interruptions will need to be
identified in other ways. In this work, we analyze interruption detection in
single-conversation and multi-group dialogue settings. We then create a
state-of-the-art method for interruption identification that is robust to
overlapping speech, and thus could be deployed in classrooms. Further, our work
highlights meaningful linguistic and prosodic information about how
interruptions manifest in collaborative group interactions. Our investigation
also paves the way for future works to account for the influence of overlapping
speech from multiple groups when tracking group dialog.

</details>


### [6] [Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation](https://arxiv.org/abs/2507.07307)
*Anirban Saha Anik,Xiaoying Song,Elliott Wang,Bryan Wang,Bengisu Yarimbas,Lingzi Hong*

Main category: cs.CL

> A Multi-agent Retrieval-Augmented Framework using multiple LLMs is proposed to improve the quality of counterspeech against health misinformation.

<details>
  <summary>Details</summary>

**Motivation:** Current methods for generating counterspeech against misinformation lack both control over the output and an effective mechanism to integrate up-to-date knowledge, motivated the development of a new approach.

**Method:** Our approach integrates a Multi-agent Retrieval-Augmented Framework that utilizes multiple large language models (LLMs) to improve knowledge retrieval and response refinement in countering health misinformation.

**Result:** Ablation studies and human evaluations show the necessity and effectiveness of each component in the proposed framework, indicating that refinement significantly enhances the quality of the generated counterspeech.

**Conclusion:** The proposed Multi-agent Retrieval-Augmented Framework effectively generates high-quality counterspeech that is polite, relevant, informative, and factually accurate against health misinformation.

**Abstract:** Large language models (LLMs) incorporated with Retrieval-Augmented Generation
(RAG) have demonstrated powerful capabilities in generating counterspeech
against misinformation. However, current studies rely on limited evidence and
offer less control over final outputs. To address these challenges, we propose
a Multi-agent Retrieval-Augmented Framework to generate counterspeech against
health misinformation, incorporating multiple LLMs to optimize knowledge
retrieval, evidence enhancement, and response refinement. Our approach
integrates both static and dynamic evidence, ensuring that the generated
counterspeech is relevant, well-grounded, and up-to-date. Our method
outperforms baseline approaches in politeness, relevance, informativeness, and
factual accuracy, demonstrating its effectiveness in generating high-quality
counterspeech. To further validate our approach, we conduct ablation studies to
verify the necessity of each component in our framework. Furthermore, human
evaluations reveal that refinement significantly enhances counterspeech quality
and obtains human preference.

</details>


### [7] [GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural Networks for Text Representation](https://arxiv.org/abs/2507.07414)
*Fardin Rastakhiz*

Main category: cs.CL

> A novel model combining GNNs and CNNs is proposed for more efficient processing of long texts, incorporating real-time graph generation and LLM data via dictionary lookups, demonstrating competitive performance in text classification tasks.

<details>
  <summary>Details</summary>

**Motivation:** The motivation stems from the inefficiency of current state-of-the-art models like Transformers when processing extended documents due to quadratic computational complexity relative to input length. The goal is to improve time, cost, and energy efficiency without sacrificing performance.

**Method:** This paper proposes a new model architecture that integrates Graph Neural Networks (GNNs) and Convolutional Neural Networks (CNNs) to process long texts more efficiently. It utilizes a real-time, end-to-end graph generation method and incorporates information from Large Language Models (LLMs) using efficient dictionary lookups.

**Result:** The model's structural properties, such as an average clustering coefficient of approximately 0.45 and an average shortest path length between 4 and 5, indicate meaningful semantic organization. Experimental results validate the model’s efficiency and competitive performance across various text classification tasks like sentiment analysis and news categorization.

**Conclusion:** The proposed model architecture, by efficiently processing unsupplemented text inputs and maintaining competitive performance in text classification tasks, provides a promising solution to the computational inefficiencies faced by existing models in processing long documents.

**Abstract:** Time, cost, and energy efficiency are critical considerations in
Deep-Learning (DL), particularly when processing long texts. Transformers,
which represent the current state of the art, exhibit quadratic computational
complexity relative to input length, making them inefficient for extended
documents. This study introduces a novel model architecture that combines Graph
Neural Networks (GNNs) and Convolutional Neural Networks (CNNs), integrated
with a real-time, end-to-end graph generation mechanism. The model processes
compact batches of character-level inputs without requiring padding or
truncation. To enhance performance while maintaining high speed and efficiency,
the model incorporates information from Large Language Models (LLMs), such as
token embeddings and sentiment polarities, through efficient dictionary
lookups. It captures local contextual patterns using CNNs, expands local
receptive fields via lattice-based graph structures, and employs small-world
graphs to aggregate document-level information. The generated graphs exhibit
structural properties indicative of meaningful semantic organization, with an
average clustering coefficient of approximately 0.45 and an average shortest
path length ranging between 4 and 5. The model is evaluated across multiple
text classification tasks, including sentiment analysis and
news-categorization, and is compared against state-of-the-art models.
Experimental results confirm the proposed model's efficiency and competitive
performance.

</details>


### [8] [MedReadCtrl: Personalizing medical text generation with readability-controlled instruction learning](https://arxiv.org/abs/2507.07419)
*Hieu Tran,Zonghai Yao,Won Seok Jang,Sharmin Sultana,Allen Chang,Yuan Zhang,Hong Yu*

Main category: cs.CL

> 研究通过MedReadCtrl框架，提高了人工智能生成医疗内容的可读性和专业性，显著减少了可读性相关的指令执行错误，专家评价优于GPT-4，并且特别适合低识字水平用户的理解。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于解决AI在医疗领域部署时遇到的人机沟通挑战，尤其是如何使AI生成的内容既个性化又易于理解。为了解决这一问题，提出了MedReadCtrl框架。

**Method:** 研究介绍了一种名为MedReadCtrl的可读性控制指令调优框架，该框架能够使语言模型根据需要调整输出内容的复杂度，但不会牺牲对内容意义的理解。

**Result:** 研究展示了MedReadCtrl框架在提升医疗内容可读性控制方面的优势，对比GPT-4，在多个评估数据集和任务中表现更佳，特别是在临床任务中。专家也更倾向于使用MedReadCtrl的输出内容，特别是在低识字水平中的应用。这表明MedReadCtrl能够将临床内容重构为更易于理解的语言，同时保持医疗信息的准确性，为提升患者教育和扩大AI医疗普及提供了可扩展的解决方案。

**Conclusion:** 总的来说，MedReadCtrl提供了一种可扩展的解决方案，能够将复杂的医疗内容重构为易于理解的语言，同时保留其医疗意图，有利于患者教育和扩大AI医疗的平等访问。

**Abstract:** Generative AI has demonstrated strong potential in healthcare, from clinical
decision support to patient-facing chatbots that improve outcomes. A critical
challenge for deployment is effective human-AI communication, where content
must be both personalized and understandable. We introduce MedReadCtrl, a
readability-controlled instruction tuning framework that enables LLMs to adjust
output complexity without compromising meaning. Evaluations of nine datasets
and three tasks across medical and general domains show that MedReadCtrl
achieves significantly lower readability instruction-following errors than
GPT-4 (e.g., 1.39 vs. 1.59 on ReadMe, p<0.001) and delivers substantial gains
on unseen clinical tasks (e.g., +14.7 ROUGE-L, +6.18 SARI on MTSamples).
Experts consistently preferred MedReadCtrl (71.7% vs. 23.3%), especially at low
literacy levels. These gains reflect MedReadCtrl's ability to restructure
clinical content into accessible, readability-aligned language while preserving
medical intent, offering a scalable solution to support patient education and
expand equitable access to AI-enabled care.

</details>


### [9] [SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic EHR Data](https://arxiv.org/abs/2507.07421)
*Zonghai Yao,Youxia Zhao,Avijit Mitra,David A. Levy,Emily Druhl,Jack Tsai,Hong Yu*

Main category: cs.CL

> 研究提出了一种新的管道（SynthEHR-Eviction），用于提取电子健康记录中的驱逐信息，解决了驱逐作为社会健康决定因素信息提取的问题，且显著提高了提取精度，减少了标注工作量。

<details>
  <summary>Details</summary>

**Motivation:** 驱逐是一个重要的但被忽视的社会健康决定因素，与住房不稳定、失业和心理健康相关。尽管在非结构化的电子健康记录中存在驱逐信息，但由于缺乏在结构化字段中的编码，这些信息在下游应用中难以利用。

**Method:** 通过结合大型语言模型（LLM）、人类反馈循环注释和自动化提示优化（APO），提出了一种可扩展的名为SynthEHR-Eviction的管道，用于从临床记录中提取驱逐状态信息。

**Result:** 该研究创建了迄今最大的公共驱逐相关社会健康决定因子数据集，并通过微调大型语言模型（如Qwen2.5、LLaMA3）实现了88.8%（驱逐状态）和90.3%（其他SDoH）的Macro-F1评分。

**Conclusion:** 研究证明了SynthEHR-Eviction管道的有效性，不仅大大降低了标注工作量，加速了数据集的创建，还能够应用于其他信息提取任务，实现成本效益的部署。

**Abstract:** Eviction is a significant yet understudied social determinants of health
(SDoH), linked to housing instability, unemployment, and mental health. While
eviction appears in unstructured electronic health records (EHRs), it is rarely
coded in structured fields, limiting downstream applications. We introduce
SynthEHR-Eviction, a scalable pipeline combining LLMs, human-in-the-loop
annotation, and automated prompt optimization (APO) to extract eviction
statuses from clinical notes. Using this pipeline, we created the largest
public eviction-related SDoH dataset to date, comprising 14 fine-grained
categories. Fine-tuned LLMs (e.g., Qwen2.5, LLaMA3) trained on
SynthEHR-Eviction achieved Macro-F1 scores of 88.8% (eviction) and 90.3% (other
SDoH) on human validated data, outperforming GPT-4o-APO (87.8%, 87.3%),
GPT-4o-mini-APO (69.1%, 78.1%), and BioBERT (60.7%, 68.3%), while enabling
cost-effective deployment across various model sizes. The pipeline reduces
annotation effort by over 80%, accelerates dataset creation, enables scalable
eviction detection, and generalizes to other information extraction tasks.

</details>


### [10] [Towards Interpretable Time Series Foundation Models](https://arxiv.org/abs/2507.07439)
*Matthieu Boileau,Philippe Helluy,Jeremy Pawlus,Svitlana Vyetrenko*

Main category: cs.CL

> 研究展示了通过合成时间序列数据及大型多模态模型生成的自然语言注释微调Qwen模型的可行性，验证了将时间序列理解压缩到轻量级模型中的可能性。

<details>
  <summary>Details</summary>

**Motivation:** 推动构建可解释的时间序列基础模型的发展，展示小型语言模型如何压缩并理解复杂的时间序列数据。

**Method:** 研究采用合成的均值回归时间序列数据集，并利用大型多模态模型生成自然语言注释以监督紧凑型Qwen模型的微调。采用的评估指标包括趋势方向、噪声强度和极值定位等。

**Result:** 该研究探讨了将时间序列推理能力提炼到小型指令调整语言模型中的可能性，为构建可解释的时间序列基础模型迈出了重要的一步。研究通过合成的均值回归时间序列数据集，利用大型多模态模型生成自然语言注释，从而监督紧凑型Qwen模型的微调过程。评估指标集中在趋势方向、噪声强度和极值定位等方面，证明了这些模型确实获得了有意义的解释能力。研究结果表明，时间序列的理解可以压缩到轻量级的语言模型中，适合于设备上的或隐私敏感环境下的部署。这项工作为开发可以以自然语言解释时间模式的小型、可解释模型奠定了坚实的基础。

**Conclusion:** 小型、语言能力较强的模型能够获得有意义的解释能力，压缩时间序列理解到轻量级模型中的可行性得到了验证。这为开发可解释的、能够用自然语言表达时间模式的模型铺平了道路。

**Abstract:** In this paper, we investigate the distillation of time series reasoning
capabilities into small, instruction-tuned language models as a step toward
building interpretable time series foundation models. Leveraging a synthetic
dataset of mean-reverting time series with systematically varied trends and
noise levels, we generate natural language annotations using a large multimodal
model and use these to supervise the fine-tuning of compact Qwen models. We
introduce evaluation metrics that assess the quality of the distilled reasoning
- focusing on trend direction, noise intensity, and extremum localization - and
show that the post-trained models acquire meaningful interpretive capabilities.
Our results highlight the feasibility of compressing time series understanding
into lightweight, language-capable models suitable for on-device or
privacy-sensitive deployment. This work contributes a concrete foundation
toward developing small, interpretable models that explain temporal patterns in
natural language.

</details>


### [11] [SAND: Boosting LLM Agents with Self-Taught Action Deliberation](https://arxiv.org/abs/2507.07441)
*Yu Xia,Yiran Jenny Shen,Junda Wu,Tong Yu,Sungchul Kim,Ryan A. Rossi,Lina Yao,Julian McAuley*

Main category: cs.CL

> 提出SAND框架以提升LLM代理在宽广行动空间中的决策能力，通过对候选行动进行思考与比较，解决了LLM代理过早确定行动的局限，实验显示SAND比初版监督微调高出20%的表现，超越了现有代理微调方法。

<details>
  <summary>Details</summary>

**Motivation:** 解决大语言模型代理在有限行动空间探索中可能会过早地选择看似合理但实际上是次优行动的问题。

**Method:** SAND框架，允许LLM代理在执行之前对候选行动进行明确的思考。该框架通过自我一致性行动采样和执行指导行动评估来解决何时及何物进行行动评估的问题，并通过迭代方式使用这些思考轨迹来微调LLM代理本身。

**Result:** 在两个具有代表性的交互式代理任务上，SAND达到了比初版监督微调平均提高20%的性能，且超越了现有的代理微调方法。

**Conclusion:** SAND框架能够通过扩展LLM代理在行动之前进行详尽的思考与评估，显著提高了代理的决策质量，超越了现存的方法。

**Abstract:** Large Language Model (LLM) agents are commonly tuned with supervised
finetuning on ReAct-style expert trajectories or preference optimization over
pairwise rollouts. Most of these methods focus on imitating specific expert
behaviors or promoting chosen reasoning thoughts and actions over rejected
ones. However, without reasoning and comparing over alternatives actions, LLM
agents finetuned with these methods may over-commit towards seemingly plausible
but suboptimal actions due to limited action space exploration. To address
this, in this paper we propose Self-taught ActioN Deliberation (SAND)
framework, enabling LLM agents to explicitly deliberate over candidate actions
before committing to one. To tackle the challenges of when and what to
deliberate given large action space and step-level action evaluation, we
incorporate self-consistency action sampling and execution-guided action
critique to help synthesize step-wise action deliberation thoughts using the
base model of the LLM agent. In an iterative manner, the deliberation
trajectories are then used to finetune the LLM agent itself. Evaluating on two
representative interactive agent tasks, SAND achieves an average 20%
improvement over initial supervised finetuning and also outperforms
state-of-the-art agent tuning approaches.

</details>


### [12] [RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning](https://arxiv.org/abs/2507.07451)
*Hongzhi Zhang,Jia Fu,Jingyuan Zhang,Kai Fu,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.CL

> RLEP, a two-phase framework for reinforcement learning, improves convergence and performance of large language models by replaying verified trajectories.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of training instability and policy drift in reinforcement learning for large language models by focusing on promising learning paths and fostering faster convergence.

**Method:** Reinforcement Learning with Experience rePlay (RLEP), a two-phase framework that involves collecting verified trajectories and replaying them during subsequent training to optimize the policy on blended mini-batches of new and replayed rollouts.

**Result:** RLEP improved baseline peak accuracy and surpassed it on various benchmarks: AIME-2024 from 38.2% to 39.9%, AIME-2025 from 19.8% to 22.3%, and AMC-2023 from 77.0% to 82.2%.

**Conclusion:** RLEP demonstrates superior performance in reinforcement learning for large language models by enhancing convergence speed and final accuracy, and the research materials are available for further study.

**Abstract:** Reinforcement learning (RL) for large language models is an energy-intensive
endeavor: training can be unstable, and the policy may gradually drift away
from its pretrained weights. We present \emph{RLEP}\, -- \,Reinforcement
Learning with Experience rePlay\, -- \,a two-phase framework that first
collects verified trajectories and then replays them during subsequent
training. At every update step, the policy is optimized on mini-batches that
blend newly generated rollouts with these replayed successes. By replaying
high-quality examples, RLEP steers the model away from fruitless exploration,
focuses learning on promising reasoning paths, and delivers both faster
convergence and stronger final performance. On the Qwen2.5-Math-7B base model,
RLEP reaches baseline peak accuracy with substantially fewer updates and
ultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,
on AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our
code, datasets, and checkpoints are publicly available at
https://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further
research.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [13] [Multi-level Mixture of Experts for Multimodal Entity Linking](https://arxiv.org/abs/2507.07108)
*Zhiwei Hu,Víctor Gutiérrez-Basulto,Zhiliang Xiang,Ru Li,Jeff Z. Pan*

Main category: cs.CV

> 本文提出了一种多层级专家混合模型（MMoE），用于解决多模态实体链接中的两个问题：提及的模糊性和动态选择模态内容的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 多模态实体链接（MEL）旨在链接多模态上下文中模糊的提及和知识库中的实体。而现有的MEL方法忽视了提及模糊性和动态模态内容选择的问题。

**Method:** MMoE模型包含四个部分：描述感知提及增强模块，多模态特征提取模块，内部层级专家混合模块，以及层级间专家混合模块。

**Result:** 实验结果表明，MMoE模型在多模态实体链接任务上表现出色，超越了现有的先进方法。

**Conclusion:** MMoE模型通过有效的模态内容处理机制解决了提及模糊性和模态重要性选择问题，展示了其在MEL任务中的优越性。

**Abstract:** Multimodal Entity Linking (MEL) aims to link ambiguous mentions within
multimodal contexts to associated entities in a multimodal knowledge base.
Existing approaches to MEL introduce multimodal interaction and fusion
mechanisms to bridge the modality gap and enable multi-grained semantic
matching. However, they do not address two important problems: (i) mention
ambiguity, i.e., the lack of semantic content caused by the brevity and
omission of key information in the mention's textual context; (ii) dynamic
selection of modal content, i.e., to dynamically distinguish the importance of
different parts of modal information. To mitigate these issues, we propose a
Multi-level Mixture of Experts (MMoE) model for MEL. MMoE has four components:
(i) the description-aware mention enhancement module leverages large language
models to identify the WikiData descriptions that best match a mention,
considering the mention's textual context; (ii) the multimodal feature
extraction module adopts multimodal feature encoders to obtain textual and
visual embeddings for both mentions and entities; (iii)-(iv) the intra-level
mixture of experts and inter-level mixture of experts modules apply a switch
mixture of experts mechanism to dynamically and adaptively select features from
relevant regions of information. Extensive experiments demonstrate the
outstanding performance of MMoE compared to the state-of-the-art. MMoE's code
is available at: https://github.com/zhiweihu1103/MEL-MMoE.

</details>


### [14] [CoPT: Unsupervised Domain Adaptive Segmentation using Domain-Agnostic Text Embeddings](https://arxiv.org/abs/2507.07125)
*Cristina Mata,Kanchana Ranasinghe,Michael S. Ryoo*

Main category: cs.CV

> 本文提出了一个新的CoPT方法，该方法利用领域无关的文本嵌入来改进无监督领域适应在图像分割任务上的性能，并展示了在多个基准上的优越性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大规模视觉-语言表征学习取得了进展，但现有的UDA方法在分割任务上尚未充分利用文本的领域无关特性。本文旨在通过引入新的方法来改进UDA在分割任务中的性能。

**Method:** 本文提出了一种基于协方差的像素-文本损失（CoPT），该方法利用领域无关的文本嵌入来学习图像分割编码器中的领域不变特征。文本嵌入通过我们的LLM领域模板过程生成，该过程中使用LLM生成源域和目标域的描述，并将这些描述输入到一个冻结的CLIP模型中进行组合。

**Result:** 在四个基准实验中，采用CoPT训练的模型在UDA分割任务上达到了新的性能最优。

**Conclusion:** 通过实验验证了CoPT方法在无监督领域适应语义分割任务上的有效性，其优于之前的方法。代码可在https://github.com/cfmata/CoPT找到。

**Abstract:** Unsupervised domain adaptation (UDA) involves learning class semantics from
labeled data within a source domain that generalize to an unseen target domain.
UDA methods are particularly impactful for semantic segmentation, where
annotations are more difficult to collect than in image classification. Despite
recent advances in large-scale vision-language representation learning, UDA
methods for segmentation have not taken advantage of the domain-agnostic
properties of text. To address this, we present a novel Covariance-based
Pixel-Text loss, CoPT, that uses domain-agnostic text embeddings to learn
domain-invariant features in an image segmentation encoder. The text embeddings
are generated through our LLM Domain Template process, where an LLM is used to
generate source and target domain descriptions that are fed to a frozen CLIP
model and combined. In experiments on four benchmarks we show that a model
trained using CoPT achieves the new state of the art performance on UDA for
segmentation. The code can be found at https://github.com/cfmata/CoPT.

</details>


### [15] [Image Can Bring Your Memory Back: A Novel Multi-Modal Guided Attack against Image Generation Model Unlearning](https://arxiv.org/abs/2507.07139)
*Renyang Liu,Guanlin Li,Tianwei Zhang,See-Kiong Ng*

Main category: cs.CV

> 本文提出了一种名为Recall的新对抗框架，专门用于评估和破坏去学习后的图像生成模型的鲁棒性，且实验结果表明其在多个方面优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的去学习技术在面对多模态对抗输入时的稳健性和有效性尚未得到充分探索，为了填补这一空白并揭示当前去学习机制中的潜在漏洞，本研究提出了Recall框架。

**Method:** 提出了一种名为Recall的新对抗框架，专门用于破坏去学习（unlearn）后的图像生成模型的鲁棒性。与现有的主要依赖于对抗文本提示的方法不同，Recall利用了扩散模型的多模态条件能力，通过单个语义相关的参考图像优化对抗图像提示。

**Result:** 实验结果表明，Recall在对抗有效性、计算效率和原始文本提示的语义保真度方面都优于现有的基线方法，而且评估了十种最先进的去学习方法。

**Conclusion:** 研究揭示了当前去学习机制中的关键漏洞，强调了确保生成模型的安全性和可靠性的解决方案的必要性。

**Abstract:** Recent advances in image generation models (IGMs), particularly
diffusion-based architectures such as Stable Diffusion (SD), have markedly
enhanced the quality and diversity of AI-generated visual content. However,
their generative capability has also raised significant ethical, legal, and
societal concerns, including the potential to produce harmful, misleading, or
copyright-infringing content. To mitigate these concerns, machine unlearning
(MU) emerges as a promising solution by selectively removing undesirable
concepts from pretrained models. Nevertheless, the robustness and effectiveness
of existing unlearning techniques remain largely unexplored, particularly in
the presence of multi-modal adversarial inputs.
  To bridge this gap, we propose Recall, a novel adversarial framework
explicitly designed to compromise the robustness of unlearned IGMs. Unlike
existing approaches that predominantly rely on adversarial text prompts, Recall
exploits the intrinsic multi-modal conditioning capabilities of diffusion
models by efficiently optimizing adversarial image prompts with guidance from a
single semantically relevant reference image. Extensive experiments across ten
state-of-the-art unlearning methods and diverse tasks show that Recall
consistently outperforms existing baselines in terms of adversarial
effectiveness, computational efficiency, and semantic fidelity with the
original textual prompt. These findings reveal critical vulnerabilities in
current unlearning mechanisms and underscore the need for more robust solutions
to ensure the safety and reliability of generative models. Code and data are
publicly available at \textcolor{blue}{https://github.com/ryliu68/RECALL}.

</details>


### [16] [Explainable Artificial Intelligence in Biomedical Image Analysis: A Comprehensive Survey](https://arxiv.org/abs/2507.07148)
*Getamesay Haile Dagnaw,Yanming Zhu,Muhammad Hassan Maqsood,Wencheng Yang,Xingshuai Dong,Xuefei Yin,Alan Wee-Chung Liew*

Main category: cs.CV

> 本综述论文系统地分类和分析了解释型AI技术，特别关注于生物医学影像分析。提出了一种针对不同影像类型的模式中心分类体系，并探讨了多模态学习和视觉与语言模型在解释型生物医学AI中的角色。总结了评估指标和开源工具，讨论了未来方向。

<details>
  <summary>Details</summary>

**Motivation:** 解释型人工智能(XAI)在生物医学影像分析中变得越来越重要，以促进深度学习模型的透明度、信任度和临床应用。现有的一些综述缺乏对成像模式的关注，忽略了多模态和视觉语言范式中的最新进展，并且提供有限的实际指导。

**Method:** 本论文采用了系统性分类的方法，对XAI技术进行了详细的分析，特别是针对不同的医学成像类型，提出了一个模式中心的分类体系。此外，还讨论了多模态学习和视觉语言模型在可解释的生物医学AI中的角色。

**Result:** 论文总结了广泛使用的评估指标和开源框架，并对持续存在的挑战和未来的研究方向提出了批判性的讨论。

**Conclusion:** 该综述为促进解释型深度学习在生物医学影像分析中的应用提供了及时而深入的基础。

**Abstract:** Explainable artificial intelligence (XAI) has become increasingly important
in biomedical image analysis to promote transparency, trust, and clinical
adoption of DL models. While several surveys have reviewed XAI techniques, they
often lack a modality-aware perspective, overlook recent advances in multimodal
and vision-language paradigms, and provide limited practical guidance. This
survey addresses this gap through a comprehensive and structured synthesis of
XAI methods tailored to biomedical image analysis.We systematically categorize
XAI methods, analyzing their underlying principles, strengths, and limitations
within biomedical contexts. A modality-centered taxonomy is proposed to align
XAI methods with specific imaging types, highlighting the distinct
interpretability challenges across modalities. We further examine the emerging
role of multimodal learning and vision-language models in explainable
biomedical AI, a topic largely underexplored in previous work. Our
contributions also include a summary of widely used evaluation metrics and
open-source frameworks, along with a critical discussion of persistent
challenges and future directions. This survey offers a timely and in-depth
foundation for advancing interpretable DL in biomedical image analysis.

</details>


### [17] [Robust Multimodal Large Language Models Against Modality Conflict](https://arxiv.org/abs/2507.07151)
*Zongmeng Zhang,Wengang Zhou,Jie Zhao,Houqiang Li*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Despite the impressive capabilities of multimodal large language models
(MLLMs) in vision-language tasks, they are prone to hallucinations in
real-world scenarios. This paper investigates the hallucination phenomenon in
MLLMs from the perspective of modality conflict. Unlike existing works focusing
on the conflicts between model responses and inputs, we study the inherent
conflicts in inputs from different modalities that place MLLMs in a dilemma and
directly lead to hallucinations. We formally define the modality conflict and
construct a dataset named Multimodal Modality Conflict (MMMC) to simulate this
phenomenon in vision-language tasks. Three methods based on prompt engineering,
supervised fine-tuning, and reinforcement learning are proposed to alleviate
the hallucination caused by modality conflict. Extensive experiments are
conducted on the MMMC dataset to analyze the merits and demerits of these
methods. Our results show that the reinforcement learning method achieves the
best performance in mitigating the hallucination under modality conflict, while
the supervised fine-tuning method shows promising and stable performance. Our
work sheds light on the unnoticed modality conflict that leads to
hallucinations and provides more insights into the robustness of MLLMs.

</details>


### [18] [Aerial Maritime Vessel Detection and Identification](https://arxiv.org/abs/2507.07153)
*Antonella Barisic Kulas,Frano Petric,Stjepan Bogdan*

Main category: cs.CV

> 本文提出了一个基于YOLOv8、特征匹配和色调直方图分析的无GNSS环境下自主识别海上目标船只的方法，并通过MBZIRC2023竞赛中的实验进行验证。

<details>
  <summary>Details</summary>

**Motivation:** 在GNSS不可用的情况下，自主海上监视和目标船只识别对于搜索和救援以及威胁检测等多个应用至关重要。

**Method:** 本文使用YOLOv8模型检测视野中的所有船只，并通过特征匹配和色调直方图分析确定哪些检测到的船只与目标相符。定位目标时使用简单的几何原理。

**Result:** 该方法在MBZIRC2023竞赛中集成到一个完全自主系统，并进行了无GNSS导航的实验验证。还评估了视角对检测精度和定位精度的影响，并与理想方法进行了比较。

**Conclusion:** 实验结果表明，提出的方法能够在无GNSS环境下有效识别并定位目标船只，为搜索和救援及威胁检测提供了可能。

**Abstract:** Autonomous maritime surveillance and target vessel identification in
environments where Global Navigation Satellite Systems (GNSS) are not available
is critical for a number of applications such as search and rescue and threat
detection. When the target vessel is only described by visual cues and its last
known position is not available, unmanned aerial vehicles (UAVs) must rely
solely on on-board vision to scan a large search area under strict
computational constraints. To address this challenge, we leverage the YOLOv8
object detection model to detect all vessels in the field of view. We then
apply feature matching and hue histogram distance analysis to determine whether
any detected vessel corresponds to the target. When found, we localize the
target using simple geometric principles. We demonstrate the proposed method in
real-world experiments during the MBZIRC2023 competition, integrated into a
fully autonomous system with GNSS-denied navigation. We also evaluate the
impact of perspective on detection accuracy and localization precision and
compare it with the oracle approach.

</details>


### [19] [CL-Polyp: A Contrastive Learning-Enhanced Network for Accurate Polyp Segmentation](https://arxiv.org/abs/2507.07154)
*Desheng Li,Chaoliang Liu,Zhiyong Xiao*

Main category: cs.CV

> 本文提出了一种新的息肉分割网络CL-Polyp，通过对比学习提高特征判别性并增强视觉表示，同时引入了两个模块优化分割性能，实验结果表明该方法在多个基准数据集上优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 大多数现有的基于深度学习的息肉分割方法采用编码器-解码器架构，或者利用多任务框架（如分类）来提高分割性能。然而，这些方法通常需要额外的标注数据并依赖任务相似性，这可能限制了它们的通用性。为了应对这些挑战，提出了CL-Polyp。

**Method:** 提出了一种基于对比学习增强的息肉分割网络CL-Polyp，通过对比息肉图像中正负样本对来提高编码器提取判别性特征的能力。同时引入了两个轻量级有效的模块：改进的空洞空间金字塔池化（MASPP）模块以实现更好的多尺度特征融合，以及通道连接和元素相加（CA）模块来融合低级和上采样的特征以提升边界重建。

**Result:** 在五个基准数据集（Kvasir-SEG、CVC-ClinicDB、CVC-ColonDB、CVC-300和ETIS）上的大量实验证明，CL-Polyp始终优于最先进的方法，在Kvasir-SEG和CVC-ClinicDB数据集上的IoU指标分别提升了0.011和0.020。

**Conclusion:** CL-Polyp在临床息肉分割任务中显示出了有效性，展示了其在改进息肉分割性能上的潜力。

**Abstract:** Accurate segmentation of polyps from colonoscopy images is crucial for the
early diagnosis and treatment of colorectal cancer. Most existing deep
learning-based polyp segmentation methods adopt an Encoder-Decoder
architecture, and some utilize multi-task frameworks that incorporate auxiliary
tasks such as classification to enhance segmentation performance. However,
these approaches often require additional labeled data and rely on task
similarity, which can limit their generalizability. To address these
challenges, we propose CL-Polyp, a contrastive learning-enhanced polyp
segmentation network. Our method leverages contrastive learning to improve the
encoder's ability to extract discriminative features by contrasting positive
and negative sample pairs derived from polyp images. This self-supervised
strategy enhances visual representation without requiring additional
annotations. In addition, we introduce two lightweight and effective modules:
the Modified Atrous Spatial Pyramid Pooling (MASPP) module for better
multi-scale feature fusion, and the Channel Concatenate and Element Add (CA)
module to fuse low-level and upsampled features for improved boundary
reconstruction. Extensive experiments on five benchmark datasets-Kvasir-SEG,
CVC-ClinicDB, CVC-ColonDB, CVC-300, and ETIS-demonstrate that CL-Polyp
consistently outperforms state-of-the-art methods. Specifically, it improves
the IoU metric by 0.011 and 0.020 on the Kvasir-SEG and CVC-ClinicDB datasets,
respectively, validating its effectiveness in clinical polyp segmentation
tasks.

</details>


### [20] [Interpretable EEG-to-Image Generation with Semantic Prompts](https://arxiv.org/abs/2507.07157)
*Arshak Rezvani,Ali Akbari,Kosar Sanjar Arani,Maryam Mirian,Emad Arasteh,Martin J. McKeown*

Main category: cs.CV

> 研究介绍了一种新的文本中介框架，用于从EEG信号中解码图像，该框架利用EEG信号与多级语义描述的对齐，基于对比学习和潜在扩散模型实现了最先进的视觉解码效果。

<details>
  <summary>Details</summary>

**Motivation:** 由于EEG在空间细节上的限制，直接从EEG信号生成图像比较困难。该研究旨在通过文本中介的方法来提高从EEG信号解码视觉体验的效果，以期在神经科学和可解释AI领域取得突破。

**Method:** 本研究开发了一种通过对比学习将EEG信号与多级语义描述对齐的模型。首先，EEG信号通过变压器编码器映射到由大语言模型生成的描述上。接着，在推断过程中，通过检索的描述嵌入来调节预训练的潜在扩散模型生成图像。

**Result:** 实验结果表明，此文本中介框架在EEGCVPR数据集上实现了最先进的视觉解码效果，并展示了与已知神经认知通路的良好一致性。

**Conclusion:** 本研究表明，通过结构化的语义中介，可以实现与认知相一致的EEG视觉解码。研究还揭示了不同语义层次的重要性及在头皮上的语义地形。

**Abstract:** Decoding visual experience from brain signals offers exciting possibilities
for neuroscience and interpretable AI. While EEG is accessible and temporally
precise, its limitations in spatial detail hinder image reconstruction. Our
model bypasses direct EEG-to-image generation by aligning EEG signals with
multilevel semantic captions -- ranging from object-level to abstract themes --
generated by a large language model. A transformer-based EEG encoder maps brain
activity to these captions through contrastive learning. During inference,
caption embeddings retrieved via projection heads condition a pretrained latent
diffusion model for image generation. This text-mediated framework yields
state-of-the-art visual decoding on the EEGCVPR dataset, with interpretable
alignment to known neurocognitive pathways. Dominant EEG-caption associations
reflected the importance of different semantic levels extracted from perceived
images. Saliency maps and t-SNE projections reveal semantic topography across
the scalp. Our model demonstrates how structured semantic mediation enables
cognitively aligned visual decoding from EEG.

</details>


### [21] [A Survey on Long-Video Storytelling Generation: Architectures, Consistency, and Cinematic Quality](https://arxiv.org/abs/2507.07202)
*Mohamed Elmoghany,Ryan Rossi,Seunghyun Yoon,Subhojyoti Mukherjee,Eslam Bakr,Puneet Mathur,Gang Wu,Viet Dac Lai,Nedim Lipka,Ruiyi Zhang,Varun Manjunatha,Chien Nguyen,Daksh Dangi,Abel Salinas,Mohammad Taesiri,Hongjie Chen,Xiaolei Huang,Joe Barrow,Nesreen Ahmed,Hoda Eldardiry,Namyong Park,Yu Wang,Jaemin Cho,Anh Totti Nguyen,Zhengzhong Tu,Thien Nguyen,Dinesh Manocha,Mohamed Elhoseiny,Franck Dernoncourt*

Main category: cs.CV

> 研究了怎样生成高质量的长视频，并构建了一个方法分类系统。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有方法在生成长时间视频时维持角色一致性、场景布局一致性和高时间多样性的问题。

**Method:** 全面研究了32篇视频生成论文，提出一种新的方法分类系统，并提供了分类和性能特征对比表。

**Result:** 通过对32篇视频生成论文的全面研究，识别出可生成高质量长视频的关键架构组件和训练策略，构建了一个综合的现有方法分类系统，并用对比表分类论文。

**Conclusion:** 解决了生成长时间视频时的质量问题，即角色一致性、场景布局一致性和高时间多样性的问题。

**Abstract:** Despite the significant progress that has been made in video generative
models, existing state-of-the-art methods can only produce videos lasting 5-16
seconds, often labeled "long-form videos". Furthermore, videos exceeding 16
seconds struggle to maintain consistent character appearances and scene layouts
throughout the narrative. In particular, multi-subject long videos still fail
to preserve character consistency and motion coherence. While some methods can
generate videos up to 150 seconds long, they often suffer from frame redundancy
and low temporal diversity. Recent work has attempted to produce long-form
videos featuring multiple characters, narrative coherence, and high-fidelity
detail. We comprehensively studied 32 papers on video generation to identify
key architectural components and training strategies that consistently yield
these qualities. We also construct a comprehensive novel taxonomy of existing
methods and present comparative tables that categorize papers by their
architectural designs and performance characteristics.

</details>


### [22] [Colors See Colors Ignore: Clothes Changing ReID with Color Disentanglement](https://arxiv.org/abs/2507.07230)
*Priyank Pathak,Yogesh S. Rawat*

Main category: cs.CV

> CSCI uses color information for robust, clothing-invariant person re-identification without needing additional supervision, demonstrating improved performance across multiple datasets.

<details>
  <summary>Details</summary>

**Motivation:** To propose a lightweight, annotation-free proxy to mitigate appearance bias in ReID models, overcoming the resource-intensive reliance on additional models or annotations seen in existing methods.

**Method:** CSCI, an RGB-only method using color information directly from raw images or video frames to capture color-related appearance bias while disentangling it from identity-relevant ReID features, employing S2A self-attention to prevent feature information leak.

**Result:** Results on four CC-ReID datasets show significant improvements over baselines: Top-1 2.9% on LTCC, 5.0% on PRCC for image-based ReID, and 1.0% on CCVID, 2.5% on MeVID for video-based ReID.

**Conclusion:** Color serves as an effective, cost-effective proxy for clothing attributes in clothing-changing re-identification, validating its application in improving ReID across various settings.

**Abstract:** Clothes-Changing Re-Identification (CC-ReID) aims to recognize individuals
across different locations and times, irrespective of clothing. Existing
methods often rely on additional models or annotations to learn robust,
clothing-invariant features, making them resource-intensive. In contrast, we
explore the use of color - specifically foreground and background colors - as a
lightweight, annotation-free proxy for mitigating appearance bias in ReID
models. We propose Colors See, Colors Ignore (CSCI), an RGB-only method that
leverages color information directly from raw images or video frames. CSCI
efficiently captures color-related appearance bias ('Color See') while
disentangling it from identity-relevant ReID features ('Color Ignore'). To
achieve this, we introduce S2A self-attention, a novel self-attention to
prevent information leak between color and identity cues within the feature
space. Our analysis shows a strong correspondence between learned color
embeddings and clothing attributes, validating color as an effective proxy when
explicit clothing labels are unavailable. We demonstrate the effectiveness of
CSCI on both image and video ReID with extensive experiments on four CC-ReID
datasets. We improve the baseline by Top-1 2.9% on LTCC and 5.0% on PRCC for
image-based ReID, and 1.0% on CCVID and 2.5% on MeVID for video-based ReID
without relying on additional supervision. Our results highlight the potential
of color as a cost-effective solution for addressing appearance bias in
CC-ReID. Github: https://github.com/ppriyank/ICCV-CSCI-Person-ReID.

</details>


### [23] [Automated Video Segmentation Machine Learning Pipeline](https://arxiv.org/abs/2507.07242)
*Johannes Merz,Lucien Fostier*

Main category: cs.CV

> 本文提出了一种自动化的视频分割流水线，采用机器学习技术，通过文本提示进行灵活的目标检测、逐帧图像分割和视频跟踪，以生成具有时间一致性的实例掩膜，显著减少了人工工作量，加快了初步合成的创建，并提高了整体视觉效果生产效率。

<details>
  <summary>Details</summary>

**Motivation:** 目的一方面是解决视觉效果生产中慢速、资源密集的掩膜生成问题，另一方面是为艺术家提供一个可以快速采用且能提高工作效率的自动化解决方案。

**Method:** 该方法利用机器学习进行灵活的对象检测、精确的逐帧图像分割和稳健的视频跟踪，通过容器化部署和采用结构化的输出格式来实现一个自动化的视频分割流水线。

**Result:** 该流水线降低了对人工的需求，加速了初步合成的创建过程，并提供了全面的分割数据。

**Conclusion:** 这个自动化的视频分割流水线能够生成时间一致性的实例掩膜，显著提高了视觉效果生产的效率。

**Abstract:** Visual effects (VFX) production often struggles with slow, resource-intensive
mask generation. This paper presents an automated video segmentation pipeline
that creates temporally consistent instance masks. It employs machine learning
for: (1) flexible object detection via text prompts, (2) refined per-frame
image segmentation and (3) robust video tracking to ensure temporal stability.
Deployed using containerization and leveraging a structured output format, the
pipeline was quickly adopted by our artists. It significantly reduces manual
effort, speeds up the creation of preliminary composites, and provides
comprehensive segmentation data, thereby enhancing overall VFX production
efficiency.

</details>


### [24] [DisenQ: Disentangling Q-Former for Activity-Biometrics](https://arxiv.org/abs/2507.07262)
*Shehreen Azad,Yogesh S Rawat*

Main category: cs.CV

> 研究一种新的活动生物特征识别方法，通过多模态语言引导框架，利用结构化文本指导解缠生物特征和运动信息，实现个体识别，且在多个基准测试上表现优异。

<details>
  <summary>Details</summary>

**Motivation:** 这项研究旨在解决活动生物特征识别的问题，即在一系列不同的活动中识别个体。与传统的人脸识别相比，这种方法在身份线索中增添了运动动态和外观变异，增加了生物特征学习的复杂性。

**Method:** 提出了一种多模态语言引导框架，通过结构化的文本监督来取代对额外视觉数据的依赖。其核心组件是DisenQ（解缠Q-Former），这是一个统一的查询转换器，它通过结构化语言指导来分离生物特征、运动和非生物特征信息。

**Result:** 评估结果显示，该方法在三个活动视频基准上实现了最先进的性能，并在传统视频识别基准上展示了良好的泛化能力，证明了框架的有效性。

**Conclusion:** 研究展示了一种创新的方法，通过结构化的文本指导来解缠视频序列中的生物特征、运动和非生物特征信息，实现了对不同活动中的个体高效准确的识别，并且在现实世界复杂场景中的泛化表现良好。

**Abstract:** In this work, we address activity-biometrics, which involves identifying
individuals across diverse set of activities. Unlike traditional person
identification, this setting introduces additional challenges as identity cues
become entangled with motion dynamics and appearance variations, making
biometrics feature learning more complex. While additional visual data like
pose and/or silhouette help, they often struggle from extraction inaccuracies.
To overcome this, we propose a multimodal language-guided framework that
replaces reliance on additional visual data with structured textual
supervision. At its core, we introduce \textbf{DisenQ} (\textbf{Disen}tangling
\textbf{Q}-Former), a unified querying transformer that disentangles
biometrics, motion, and non-biometrics features by leveraging structured
language guidance. This ensures identity cues remain independent of appearance
and motion variations, preventing misidentifications. We evaluate our approach
on three activity-based video benchmarks, achieving state-of-the-art
performance. Additionally, we demonstrate strong generalization to complex
real-world scenario with competitive performance on a traditional video-based
identification benchmark, showing the effectiveness of our framework.

</details>


### [25] [LinguaMark: Do Multimodal Models Speak Fairly? A Benchmark-Based Evaluation](https://arxiv.org/abs/2507.07274)
*Ananya Raval,Aravind Narayanan,Vahid Reza Khazaie,Shaina Raza*

Main category: cs.CV

> 该研究介绍了LinguaMark基准，用于评估最先进的LMM在多语言视觉问答任务上的表现，发现闭源模型总体表现最佳，开源模型也有竞争力，尤其是Qwen2.5在多种语言上有出色表现。

<details>
  <summary>Details</summary>

**Motivation:** 研究多模态模型在多语言方面的表现，发现它们通常在语言覆盖率上有所限制，并提出了一个专门的评估基准来解决这一问题。

**Method:** 创建了一个包含6875个图像文本对的数据集，涵盖了11种语言和5种社会属性，使用偏见度、答案相关性和忠实度三个关键指标来评估模型表现。

**Result:** 研究发现闭源模型总体表现最好，开源模型如Qwen2.5在多种语言上展示了很强的泛化能力。

**Conclusion:** 研究结果表明，LMM在多语言处理能力方面仍有改进空间，特别是多模态模型在跨语种表现上的研究和改进具有重要意义。

**Abstract:** Large Multimodal Models (LMMs) are typically trained on vast corpora of
image-text data but are often limited in linguistic coverage, leading to biased
and unfair outputs across languages. While prior work has explored multimodal
evaluation, less emphasis has been placed on assessing multilingual
capabilities. In this work, we introduce LinguaMark, a benchmark designed to
evaluate state-of-the-art LMMs on a multilingual Visual Question Answering
(VQA) task. Our dataset comprises 6,875 image-text pairs spanning 11 languages
and five social attributes. We evaluate models using three key metrics: Bias,
Answer Relevancy, and Faithfulness. Our findings reveal that closed-source
models generally achieve the highest overall performance. Both closed-source
(GPT-4o and Gemini2.5) and open-source models (Gemma3, Qwen2.5) perform
competitively across social attributes, and Qwen2.5 demonstrates strong
generalization across multiple languages. We release our benchmark and
evaluation code to encourage reproducibility and further research.

</details>


### [26] [MagiC: Evaluating Multimodal Cognition Toward Grounded Visual Reasoning](https://arxiv.org/abs/2507.07297)
*Chengfei Wu,Ronald Seoh,Bingxuan Li,Liqiang Zhang,Fengrong Han,Dan Goldwasser*

Main category: cs.CV

> 研究开发了MagiC基准测试，用于全面评估视觉-语言模型的基于视觉的推理能力，指标包括答案的正确性、推理质量、视觉证据定位准确性及自我更正能力。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于验证大型视觉-语言模型是否真正具备基础的视觉推理能力，还是仅仅依赖于浅层模式和数据集偏见。

**Method:** 该论文介绍了MagiC基准测试，用于评估多模态认知，特别是视觉推理的质量，不仅关注答题的准确性，还评估逐步推理的质量和其对相关视觉证据的依赖性。该基准包含了约5500个由强模型输出生成的弱监督问答示例，以及900个人类编写的示例，附有详细的注释，包括答案、推理依据和边界框定位。

**Result:** 测试了参数从7B到70B的15种视觉-语言模型，评估了最终答案的正确性、推理的有效性、定位的准确性以及自我校正能力。同时，还引入了新的评估指标如MagiScore和StepSense，并提供了当前方法在基于视觉推理的关键限制和机遇方面的全面分析。

**Conclusion:** 该研究揭示了现有基于视觉推理方法的关键局限性，提供了新的分析方向和改进机会。

**Abstract:** Recent advances in large vision-language models have led to impressive
performance in visual question answering and multimodal reasoning. However, it
remains unclear whether these models genuinely perform grounded visual
reasoning or rely on superficial patterns and dataset biases. In this work, we
introduce MagiC, a comprehensive benchmark designed to evaluate grounded
multimodal cognition, assessing not only answer accuracy but also the quality
of step-by-step reasoning and its alignment with relevant visual evidence. Our
benchmark includes approximately 5,500 weakly supervised QA examples generated
from strong model outputs and 900 human-curated examples with fine-grained
annotations, including answers, rationales, and bounding box groundings. We
evaluate 15 vision-language models ranging from 7B to 70B parameters across
four dimensions: final answer correctness, reasoning validity, grounding
fidelity, and self-correction ability. MagiC further includes diagnostic
settings to probe model robustness under adversarial visual cues and assess
their capacity for introspective error correction. We introduce new metrics
such as MagiScore and StepSense, and provide comprehensive analyses that reveal
key limitations and opportunities in current approaches to grounded visual
reasoning.

</details>


### [27] [ADIEE: Automatic Dataset Creation and Scorer for Instruction-Guided Image Editing Evaluation](https://arxiv.org/abs/2507.07317)
*Sherry X. Chen,Yi Wei,Luowei Zhou,Suren Kumar*

Main category: cs.CV

> 本研究引入了ADIEE，一种自动数据集创建方法，用于训练评估工具，以改进通过指令引导的图像编辑评估效果。通过生成包含超过100K样本的数据集并微调LLaVA-NeXT-8B模型，所得评分器在多个基准测试中表现优于开源和专有视觉语言模型，显著提高了与人类评分的相关性和成对比较准确度。

<details>
  <summary>Details</summary>

**Motivation:** 当前，由指令引导的图像编辑过程中，有效的自动化评估方法显得尤为重要。但现有的开源视觉语言模型存在对齐问题，而专有模型则面临透明度低和成本高的问题。此外，没有公开的训练数据集来微调开源视觉语言模型。因此，本研究旨在引入一种方法以改善自动化评估工具的性能。

**Method:** 研究者采取了创建一个大规模数据集的方法，并训练了一个评分模型用于评估指令引导的图像编辑。该数据集包含超过100K样本，并使用这个数据集来微调一个LLaVA-NeXT-8B模型，以解码来自自定义token的数值评分。

**Result:** 所创建的评分器在多个基准测试中均优于所有开源视觉语言模型和专有模型，并取得了显著的成绩。与Human-AURORA-Bench评分呈现出更强的相关性，并分别在GenAI-Bench和AURORA-Bench的成对比较准确度上有了大的改进。此外，还能作为奖励模式，促进自动最佳编辑选择和模型微调。

**Conclusion:** 通过提出和实施ADIEE方法及其评分器，本研究归显地改善了现有自动评估工具在指令引导的图像编辑中的性能，作为奖励模型提升了MagicBrush模型在ImagenHub上的平均评分表现。

**Abstract:** Recent advances in instruction-guided image editing underscore the need for
effective automated evaluation. While Vision-Language Models (VLMs) have been
explored as judges, open-source models struggle with alignment, and proprietary
models lack transparency and cost efficiency. Additionally, no public training
datasets exist to fine-tune open-source VLMs, only small benchmarks with
diverse evaluation schemes. To address this, we introduce ADIEE, an automated
dataset creation approach which is then used to train a scoring model for
instruction-guided image editing evaluation. We generate a large-scale dataset
with over 100K samples and use it to fine-tune a LLaVA-NeXT-8B model modified
to decode a numeric score from a custom token. The resulting scorer outperforms
all open-source VLMs and Gemini-Pro 1.5 across all benchmarks, achieving a
0.0696 (+17.24%) gain in score correlation with human ratings on AURORA-Bench,
and improving pair-wise comparison accuracy by 4.03% (+7.21%) on GenAI-Bench
and 4.75% (+9.35%) on AURORA-Bench, respectively, compared to the
state-of-the-art. The scorer can act as a reward model, enabling automated best
edit selection and model fine-tuning. Notably, the proposed scorer can boost
MagicBrush model's average evaluation score on ImagenHub from 5.90 to 6.43
(+8.98%).

</details>


### [28] [Scalable and Realistic Virtual Try-on Application for Foundation Makeup with Kubelka-Munk Theory](https://arxiv.org/abs/2507.07333)
*Hui Pang,Sunil Hadap,Violetta Shevchenko,Rahul Suresh,Amin Banitalebi-Dehkordi*

Main category: cs.CV

> 本研究提出了一个新颖的方法，通过近似Kubelka-Munk理论，实现快速而真实的粉底肤色融合效果，并构建了一个可扩展的端到端框架，适用于电商平台上的真实粉底虚拟试妆。

<details>
  <summary>Details</summary>

**Motivation:** 随着增强现实技术在美妆行业中的应用，虚拟试妆正变得愈发重要。然而，准确地合成粉底与肤色的融合效果并保持多种产品范围内的一致性是一项技术挑战。

**Method:** 研究人员采取了一种近似Kubelka-Munk理论的方法来快速合成图像，同时保持粉底与肤色融合的真实感。他们构建了一个完全依赖电商平台产品信息的可扩展的端到端框架。

**Result:** 通过使用真实世界的化妆图像进行验证，研究结果显示该框架在合成粉底试妆效果上优于其他方法。

**Conclusion:** 研究成功开发出一个能够实现实时、高质量的粉底虚拟试妆的方法和技术框架。

**Abstract:** Augmented reality is revolutionizing beauty industry with virtual try-on
(VTO) applications, which empowers users to try a wide variety of products
using their phones without the hassle of physically putting on real products. A
critical technical challenge in foundation VTO applications is the accurate
synthesis of foundation-skin tone color blending while maintaining the
scalability of the method across diverse product ranges. In this work, we
propose a novel method to approximate well-established Kubelka-Munk (KM) theory
for faster image synthesis while preserving foundation-skin tone color blending
realism. Additionally, we build a scalable end-to-end framework for realistic
foundation makeup VTO solely depending on the product information available on
e-commerce sites. We validate our method using real-world makeup images,
demonstrating that our framework outperforms other techniques.

</details>
