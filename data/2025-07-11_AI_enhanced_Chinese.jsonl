{"id": "2507.07186", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07186", "abs": "https://arxiv.org/abs/2507.07186", "authors": ["Itay Itzhak", "Yonatan Belinkov", "Gabriel Stanovsky"], "title": "Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs", "comment": "CoLM 2025", "summary": "Large language models (LLMs) exhibit cognitive biases -- systematic\ntendencies of irrational decision-making, similar to those seen in humans.\nPrior work has found that these biases vary across models and can be amplified\nby instruction tuning. However, it remains unclear if these differences in\nbiases stem from pretraining, finetuning, or even random noise due to training\nstochasticity. We propose a two-step causal experimental approach to\ndisentangle these factors. First, we finetune models multiple times using\ndifferent random seeds to study how training randomness affects over $30$\ncognitive biases. Second, we introduce \\emph{cross-tuning} -- swapping\ninstruction datasets between models to isolate bias sources. This swap uses\ndatasets that led to different bias patterns, directly testing whether biases\nare dataset-dependent. Our findings reveal that while training randomness\nintroduces some variability, biases are mainly shaped by pretraining: models\nwith the same pretrained backbone exhibit more similar bias patterns than those\nsharing only finetuning data. These insights suggest that understanding biases\nin finetuned models requires considering their pretraining origins beyond\nfinetuning effects. This perspective can guide future efforts to develop\nprincipled strategies for evaluating and mitigating bias in LLMs.", "AI": {"tldr": "本文研究了大型语言模型中的认知偏见来源，通过实验揭示偏见主要由预训练决定，同时考虑训练随机性的影响。这些发现有助于未来减少偏见的策略开发。", "motivation": "此前的研究发现，这些偏见在不同的模型间有所不同，并且可以通过指令调优来放大。然而，尚不清楚这些偏见差别是源自预训练、微调，还是仅由于训练中的随机性。", "method": "本研究提出了一个两步的因果实验方法来解开这些因素。第一步，使用不同的随机种子多次微调模型，以研究训练随机性对30多种认知偏见的影响。第二步，引入“交叉调优”——交换模型之间的指令数据集，以隔离偏见的来源。这一交换使用的是导致不同偏见模式的数据集，直接测试偏见是否依赖于数据集。", "result": "研究结果表明，虽然训练的随机性引入了一些可变性，但偏见主要由预训练决定：具有相同预训练基础的模型表现出的偏见模式比仅有相同微调数据的模型更为相似。", "conclusion": "这一发现表明，理解微调模型中的偏见需要考虑其预训练起源，而不仅仅是微调效果。此视角可以引导未来的努力，发展评估和减少LLMs中偏见的原则策略。"}}
{"id": "2507.07188", "categories": ["cs.CL", "cs.AI", "cs.CY", "J.4"], "pdf": "https://arxiv.org/pdf/2507.07188", "abs": "https://arxiv.org/abs/2507.07188", "authors": ["Jens Rupprecht", "Georg Ahnert", "Markus Strohmaier"], "title": "Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses", "comment": "18 pages, 17 figures", "summary": "Large Language Models (LLMs) are increasingly used as proxies for human\nsubjects in social science surveys, but their reliability and susceptibility to\nknown response biases are poorly understood. This paper investigates the\nresponse robustness of LLMs in normative survey contexts -- we test nine\ndiverse LLMs on questions from the World Values Survey (WVS), applying a\ncomprehensive set of 11 perturbations to both question phrasing and answer\noption structure, resulting in over 167,000 simulated interviews. In doing so,\nwe not only reveal LLMs' vulnerabilities to perturbations but also reveal that\nall tested models exhibit a consistent \\textit{recency bias} varying in\nintensity, disproportionately favoring the last-presented answer option. While\nlarger models are generally more robust, all models remain sensitive to\nsemantic variations like paraphrasing and to combined perturbations. By\napplying a set of perturbations, we reveal that LLMs partially align with\nsurvey response biases identified in humans. This underscores the critical\nimportance of prompt design and robustness testing when using LLMs to generate\nsynthetic survey data.", "AI": {"tldr": "该研究显示，大型语言模型（LLMs）在用于社交科学调查时存在反应偏差，包括近期偏见，并且对于微调问题表述有敏感反应，提示了在使用LLMs生成合成数据时需要注意模型的反应稳健性。", "motivation": "该研究旨在探讨大型语言模型在规范调查情境中的反应稳健性，特别是它们对已知响应偏差的倾向，因为以前这方面的理解十分有限。", "method": "本研究测试了九种不同的大型语言模型（LLMs）在世界价值观调查（WVS）问题上的反应，通过改变问题表述和答案选项结构，应用了11种不同的干扰，并进行了超过167,000次模拟访谈。", "result": "研究揭示了模型对干扰的脆弱性以及所有测试模型中存在的一致性近期偏见，所有模型对语义变化和复合干扰仍然敏感。", "conclusion": "通过引入一系列干扰性测试，研究发现LLMs的部分人类调查响应偏差特征，强调了在使用LLMs生成合成调查数据时，妥善设计提示词和进行稳健性测试的重要性。"}}
{"id": "2507.07229", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07229", "abs": "https://arxiv.org/abs/2507.07229", "authors": ["Krithika Ramesh", "Daniel Smolyak", "Zihao Zhao", "Nupoor Gandhi", "Ritu Agarwal", "Margrét Bjarnadóttir", "Anjalie Field"], "title": "SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains", "comment": null, "summary": "We present SynthTextEval, a toolkit for conducting comprehensive evaluations\nof synthetic text. The fluency of large language model (LLM) outputs has made\nsynthetic text potentially viable for numerous applications, such as reducing\nthe risks of privacy violations in the development and deployment of AI systems\nin high-stakes domains. Realizing this potential, however, requires principled\nconsistent evaluations of synthetic data across multiple dimensions: its\nutility in downstream systems, the fairness of these systems, the risk of\nprivacy leakage, general distributional differences from the source text, and\nqualitative feedback from domain experts. SynthTextEval allows users to conduct\nevaluations along all of these dimensions over synthetic data that they upload\nor generate using the toolkit's generation module. While our toolkit can be run\nover any data, we highlight its functionality and effectiveness over datasets\nfrom two high-stakes domains: healthcare and law. By consolidating and\nstandardizing evaluation metrics, we aim to improve the viability of synthetic\ntext, and in-turn, privacy-preservation in AI development.", "AI": {"tldr": "本文提出了SynthTextEval工具包，用于对合成文本进行多维度的评估，以提高合成文本的可行性和隐私保护。", "motivation": "大型语言模型输出的流畅性使得合成文本在诸如减少AI系统开发和部署过程中隐私侵犯风险等众多应用中变得可行，但实现这一潜力需要对合成数据进行全面评估。", "method": "我们提出了SynthTextEval，这是一个用于全面评估合成文本的工具包。该工具包可以评估合成数据在下游系统中的效用、系统的公平性、隐私泄露的风险、与原始文本的分布差异以及领域专家的定性反馈。", "result": "SynthTextEval展示了其在医疗保健和法律这两个高风险领域的功能和有效性，通过整合和标准化评估指标来提高合成文本的可行性，进而改善AI开发中的隐私保护。", "conclusion": "通过使用SynthTextEval，我们能够改善合成文本的可行性和AI开发中的隐私保护。"}}
{"id": "2507.07248", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07248", "abs": "https://arxiv.org/abs/2507.07248", "authors": ["Minseon Kim", "Jean-Philippe Corbeil", "Alessandro Sordoni", "Francois Beaulieu", "Paul Vozila"], "title": "Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings", "comment": null, "summary": "As the performance of large language models (LLMs) continues to advance,\ntheir adoption is expanding across a wide range of domains, including the\nmedical field. The integration of LLMs into medical applications raises\ncritical safety concerns, particularly due to their use by users with diverse\nroles, e.g. patients and clinicians, and the potential for model's outputs to\ndirectly affect human health. Despite the domain-specific capabilities of\nmedical LLMs, prior safety evaluations have largely focused only on general\nsafety benchmarks. In this paper, we introduce a safety evaluation protocol\ntailored to the medical domain in both patient user and clinician user\nperspectives, alongside general safety assessments and quantitatively analyze\nthe safety of medical LLMs. We bridge a gap in the literature by building the\nPatientSafetyBench containing 466 samples over 5 critical categories to measure\nsafety from the perspective of the patient. We apply our red-teaming protocols\non the MediPhi model collection as a case study. To our knowledge, this is the\nfirst work to define safety evaluation criteria for medical LLMs through\ntargeted red-teaming taking three different points of view - patient,\nclinician, and general user - establishing a foundation for safer deployment in\nmedical domains.", "AI": {"tldr": "本文介绍了一个专门针对医疗领域的大型语言模型安全评估协议，并从患者和临床医生的角度进行了详细的安全性量分析。通过构建PatientSafetyBench数据集并应用针对性的红队测试方法，研究了MediPhi模型的安全性。", "motivation": "大型语言模型在医疗领域的广泛应用提高了对其安全性的要求。现有的安全性评估大多关注于通用的安全基准，而忽视了医疗领域特有的风险和考量。", "method": "构建了一个包含466个样本的PatientSafetyBench数据集，覆盖5个关键的安全评估类别，从患者和临床医生的不同视角进行安全性评估。", "result": "通过红队测试方法对MediPhi模型进行评估，提出了医疗语言模型安全性评估的新标准，并从患者、临床医生和一般用户三个角度进行考量。", "conclusion": "这项工作首次定义了医疗领域大型语言模型的安全性评估标准，为医疗领域的安全部署奠定了基础。"}}
{"id": "2507.07108", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.07108", "abs": "https://arxiv.org/abs/2507.07108", "authors": ["Zhiwei Hu", "Víctor Gutiérrez-Basulto", "Zhiliang Xiang", "Ru Li", "Jeff Z. Pan"], "title": "Multi-level Mixture of Experts for Multimodal Entity Linking", "comment": "Accepted at KDD 2025", "summary": "Multimodal Entity Linking (MEL) aims to link ambiguous mentions within\nmultimodal contexts to associated entities in a multimodal knowledge base.\nExisting approaches to MEL introduce multimodal interaction and fusion\nmechanisms to bridge the modality gap and enable multi-grained semantic\nmatching. However, they do not address two important problems: (i) mention\nambiguity, i.e., the lack of semantic content caused by the brevity and\nomission of key information in the mention's textual context; (ii) dynamic\nselection of modal content, i.e., to dynamically distinguish the importance of\ndifferent parts of modal information. To mitigate these issues, we propose a\nMulti-level Mixture of Experts (MMoE) model for MEL. MMoE has four components:\n(i) the description-aware mention enhancement module leverages large language\nmodels to identify the WikiData descriptions that best match a mention,\nconsidering the mention's textual context; (ii) the multimodal feature\nextraction module adopts multimodal feature encoders to obtain textual and\nvisual embeddings for both mentions and entities; (iii)-(iv) the intra-level\nmixture of experts and inter-level mixture of experts modules apply a switch\nmixture of experts mechanism to dynamically and adaptively select features from\nrelevant regions of information. Extensive experiments demonstrate the\noutstanding performance of MMoE compared to the state-of-the-art. MMoE's code\nis available at: https://github.com/zhiweihu1103/MEL-MMoE.", "AI": {"tldr": "本文提出了一种多级专家混合模型(MMoE)，用于解决多模态实体链接中的指代模糊和动态选择模态内容的问题。", "motivation": "现有的多模态实体链接方法缺乏处理指代模糊和区分模态信息重要性的能力，本文旨在通过MMoE模型来改进这些问题。", "method": "MMoE模型包含四个部分：描述感知的提及增强模块、多模态特征提取模块、多层次专家混合模块，通过开关混合专家机制动态地选择信息区域的特征。", "result": "实验表明MMoE模型在多模态实体链接任务上表现优于现有方法。", "conclusion": "MMoE模型通过多级专家混合机制和描述感知的提及增强模块提高了多模态实体链接的性能。"}}
{"id": "2507.07280", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07280", "abs": "https://arxiv.org/abs/2507.07280", "authors": ["Mariah Bradford", "Nikhil Krishnaswamy", "Nathaniel Blanchard"], "title": "The Impact of Background Speech on Interruption Detection in Collaborative Groups", "comment": "Long Paper AIED 2025", "summary": "Interruption plays a crucial role in collaborative learning, shaping group\ninteractions and influencing knowledge construction. AI-driven support can\nassist teachers in monitoring these interactions. However, most previous work\non interruption detection and interpretation has been conducted in\nsingle-conversation environments with relatively clean audio. AI agents\ndeployed in classrooms for collaborative learning within small groups will need\nto contend with multiple concurrent conversations -- in this context,\noverlapping speech will be ubiquitous, and interruptions will need to be\nidentified in other ways. In this work, we analyze interruption detection in\nsingle-conversation and multi-group dialogue settings. We then create a\nstate-of-the-art method for interruption identification that is robust to\noverlapping speech, and thus could be deployed in classrooms. Further, our work\nhighlights meaningful linguistic and prosodic information about how\ninterruptions manifest in collaborative group interactions. Our investigation\nalso paves the way for future works to account for the influence of overlapping\nspeech from multiple groups when tracking group dialog.", "AI": {"tldr": "研究了单对话和多组对话环境中的中断检测，开发了能够应对重叠语音情况的中断识别方法，揭示了协作小组互动中的语言学和韵律信息。", "motivation": "大多数中断检测研究是在单对话环境和相对纯净的音频背景下进行的，而实际的小组协作学习环境中存在重叠语音，需要有效的方法来识别中断。", "method": "通过对比单对话环境和多小组对话环境中的中断检测，开发了一种针对重叠语音情况的先进中断识别方法。", "result": "创建了能够应对重叠语音情况的先进中断识别方法，为进一步研究奠定了基础。", "conclusion": "该方法在包含多个小组的对话环境中可以有效识别中断，并揭示了中断在协作小组互动中的语言学和韵律信息。"}}
{"id": "2507.07125", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.07125", "abs": "https://arxiv.org/abs/2507.07125", "authors": ["Cristina Mata", "Kanchana Ranasinghe", "Michael S. Ryoo"], "title": "CoPT: Unsupervised Domain Adaptive Segmentation using Domain-Agnostic Text Embeddings", "comment": "ECCV 2024", "summary": "Unsupervised domain adaptation (UDA) involves learning class semantics from\nlabeled data within a source domain that generalize to an unseen target domain.\nUDA methods are particularly impactful for semantic segmentation, where\nannotations are more difficult to collect than in image classification. Despite\nrecent advances in large-scale vision-language representation learning, UDA\nmethods for segmentation have not taken advantage of the domain-agnostic\nproperties of text. To address this, we present a novel Covariance-based\nPixel-Text loss, CoPT, that uses domain-agnostic text embeddings to learn\ndomain-invariant features in an image segmentation encoder. The text embeddings\nare generated through our LLM Domain Template process, where an LLM is used to\ngenerate source and target domain descriptions that are fed to a frozen CLIP\nmodel and combined. In experiments on four benchmarks we show that a model\ntrained using CoPT achieves the new state of the art performance on UDA for\nsegmentation. The code can be found at https://github.com/cfmata/CoPT.", "AI": {"tldr": "markdown", "motivation": "markdown", "method": "markdown", "result": "{\n  \"tldr\": \"本文提出了一种基于协方差的像素文本损失(CoPT)，利用领域无关的文本嵌入来学习图像分割编码器中的领域不变特征，从而改进无监督领域适应(UDA)的性能，特别是在语义分割任务中。实验结果表明，使用CoPT训练的模型在四个基准测试中达到了UDA语义分割的新技术水平。\",\n  \"motivation\": \"无监督领域适应方法在语义分割任务中的进步有限，主要是因为没有充分利用文本描述的领域无关特性。通过结合大模型生成的源领域和目标领域的描述，以及CLIP模型生成的文本嵌入，本文旨在提高UDA在语义分割中的性能。\",\n  \"method\": \"本文提出了一种新的方法，即基于协方差的像素文本损失(CoPT)。该方法利用领域无关的文本嵌入来学习领域不变的特征。文本嵌入是通过一个大语言模型(LLM)生成的源领域和目标领域的描述转换的，这些描述通过一个冻结的CLIP模型进行处理和结合。\",\n  \"result\": \"通过在四个基准测试上的实验，本文方法表现出了在无监督领域适应(UDA)的语义分割中的新技术水平，证明了所提出的方法的有效性和优越性。\",\n  \"conclusion\": \"实验表明，基于协方差的像素文本损失(CoPT)对于提高无监督领域适应在语义分割任务中的性能是非常有效的，具体结果在多个数据集上得到了验证。\n}", "conclusion": "markdown"}}
{"id": "2507.07307", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07307", "abs": "https://arxiv.org/abs/2507.07307", "authors": ["Anirban Saha Anik", "Xiaoying Song", "Elliott Wang", "Bryan Wang", "Bengisu Yarimbas", "Lingzi Hong"], "title": "Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation", "comment": null, "summary": "Large language models (LLMs) incorporated with Retrieval-Augmented Generation\n(RAG) have demonstrated powerful capabilities in generating counterspeech\nagainst misinformation. However, current studies rely on limited evidence and\noffer less control over final outputs. To address these challenges, we propose\na Multi-agent Retrieval-Augmented Framework to generate counterspeech against\nhealth misinformation, incorporating multiple LLMs to optimize knowledge\nretrieval, evidence enhancement, and response refinement. Our approach\nintegrates both static and dynamic evidence, ensuring that the generated\ncounterspeech is relevant, well-grounded, and up-to-date. Our method\noutperforms baseline approaches in politeness, relevance, informativeness, and\nfactual accuracy, demonstrating its effectiveness in generating high-quality\ncounterspeech. To further validate our approach, we conduct ablation studies to\nverify the necessity of each component in our framework. Furthermore, human\nevaluations reveal that refinement significantly enhances counterspeech quality\nand obtains human preference.", "AI": {"tldr": "本文提出了一种多代理检索增强框架，用于应对健康领域的虚假信息，该框架比现有方法在多个关键指标上表现更优，并通过消融研究和人类评估验证了其有效性和必要性。", "motivation": "现有的研究依赖于有限的证据，并且对最终输出的控制较少。为了应对这些挑战，我们提出了这一新框架。", "method": "我们提出了一种多代理检索增强框架，用于对抗健康领域的虚假信息，该框架集成了多个大型语言模型以优化知识检索、证据增强和响应细化。这种方法结合了静态和动态证据，以确保生成的对抗言论是相关的、有根据的和最新的。", "result": "我们的方法在礼貌性、相关性、信息量和事实准确性方面超过了基线方法，证明了其在生成高质量对抗言论方面的有效性。我们还进行了消融研究以验证框架中每个组件的必要性，并通过人类评估表明细化过程显著提升了对抗言论的质量。", "conclusion": "我们的研究表明，多代理检索增强框架在生成高质量的对抗言论方面是有效的，特别是在礼貌性、相关性、信息量和事实准确性方面。该框架能够有效地整合和利用动态及静态证据，验证了其在处理健康领域的虚假信息中的潜力。human评估进一步证实，该框架对于生成高质量的对抗言论至关重要。"}}
{"id": "2507.07139", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07139", "abs": "https://arxiv.org/abs/2507.07139", "authors": ["Renyang Liu", "Guanlin Li", "Tianwei Zhang", "See-Kiong Ng"], "title": "Image Can Bring Your Memory Back: A Novel Multi-Modal Guided Attack against Image Generation Model Unlearning", "comment": null, "summary": "Recent advances in image generation models (IGMs), particularly\ndiffusion-based architectures such as Stable Diffusion (SD), have markedly\nenhanced the quality and diversity of AI-generated visual content. However,\ntheir generative capability has also raised significant ethical, legal, and\nsocietal concerns, including the potential to produce harmful, misleading, or\ncopyright-infringing content. To mitigate these concerns, machine unlearning\n(MU) emerges as a promising solution by selectively removing undesirable\nconcepts from pretrained models. Nevertheless, the robustness and effectiveness\nof existing unlearning techniques remain largely unexplored, particularly in\nthe presence of multi-modal adversarial inputs.\n  To bridge this gap, we propose Recall, a novel adversarial framework\nexplicitly designed to compromise the robustness of unlearned IGMs. Unlike\nexisting approaches that predominantly rely on adversarial text prompts, Recall\nexploits the intrinsic multi-modal conditioning capabilities of diffusion\nmodels by efficiently optimizing adversarial image prompts with guidance from a\nsingle semantically relevant reference image. Extensive experiments across ten\nstate-of-the-art unlearning methods and diverse tasks show that Recall\nconsistently outperforms existing baselines in terms of adversarial\neffectiveness, computational efficiency, and semantic fidelity with the\noriginal textual prompt. These findings reveal critical vulnerabilities in\ncurrent unlearning mechanisms and underscore the need for more robust solutions\nto ensure the safety and reliability of generative models. Code and data are\npublicly available at \\textcolor{blue}{https://github.com/ryliu68/RECALL}.", "AI": {"tldr": "The paper introduces Recall, an adversarial framework that targets vulnerabilities in unlearning mechanisms of image generation models by using optimized adversarial image prompts.", "motivation": "The motivation is to address the ethical and societal risks associated with image generation models by evaluating and improving the effectiveness of machine unlearning techniques, particularly against multi-modal adversarial inputs.", "method": "Recall exploits the multi-modal conditioning of diffusion models by optimizing adversarial image prompts guided by a relevant reference image, contrasting with existing methods that mainly use adversarial text prompts.", "result": "Experiment results demonstrate Recall's superior performance in degrading unlearned image generation models across several dimensions, indicating critical vulnerabilities in current unlearning strategies.", "conclusion": "The findings highlight the need for more robust unlearning mechanisms within image generation models to ensure their safe application and reliability."}}
{"id": "2507.07414", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.07414", "abs": "https://arxiv.org/abs/2507.07414", "authors": ["Fardin Rastakhiz"], "title": "GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural Networks for Text Representation", "comment": null, "summary": "Time, cost, and energy efficiency are critical considerations in\nDeep-Learning (DL), particularly when processing long texts. Transformers,\nwhich represent the current state of the art, exhibit quadratic computational\ncomplexity relative to input length, making them inefficient for extended\ndocuments. This study introduces a novel model architecture that combines Graph\nNeural Networks (GNNs) and Convolutional Neural Networks (CNNs), integrated\nwith a real-time, end-to-end graph generation mechanism. The model processes\ncompact batches of character-level inputs without requiring padding or\ntruncation. To enhance performance while maintaining high speed and efficiency,\nthe model incorporates information from Large Language Models (LLMs), such as\ntoken embeddings and sentiment polarities, through efficient dictionary\nlookups. It captures local contextual patterns using CNNs, expands local\nreceptive fields via lattice-based graph structures, and employs small-world\ngraphs to aggregate document-level information. The generated graphs exhibit\nstructural properties indicative of meaningful semantic organization, with an\naverage clustering coefficient of approximately 0.45 and an average shortest\npath length ranging between 4 and 5. The model is evaluated across multiple\ntext classification tasks, including sentiment analysis and\nnews-categorization, and is compared against state-of-the-art models.\nExperimental results confirm the proposed model's efficiency and competitive\nperformance.", "AI": {"tldr": "This paper presents a new model that integrates CNNs and GNNs with a real-time graph generation mechanism to efficiently process long texts. The model shows competitive performance in text classification tasks and is more efficient than currently state-of-the-art transformers.", "motivation": "The motivation for this study arises from the need for more efficient and time-effective models capable of handling long texts. Current state-of-the-art models, like transformers, exhibit computational complexity that is unsuitable for processing extended documents.", "method": "The paper introduces a new model architecture that combines Graph Neural Networks (GNNs) and Convolutional Neural Networks (CNNs) for processing long texts efficiently. This model uses real-time, end-to-end graph generation and utilizes character-level inputs without padding or truncation, thus achieving better performance and efficiency.", "result": "The model captures local contextual patterns with CNNs and expands local receptive fields using lattice-based graph structures. It also employs small-world graphs to aggregate document-level information. Experimental results show that the proposed model is efficient and performs well in text classification tasks.", "conclusion": "The new model architecture, combining GNNs and CNNs with real-time graph generation, offers a competitive alternative to the existing quadratic computational complexity models used for long text processing. It achieves high performance and efficiency."}}
{"id": "2507.07148", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07148", "abs": "https://arxiv.org/abs/2507.07148", "authors": ["Getamesay Haile Dagnaw", "Yanming Zhu", "Muhammad Hassan Maqsood", "Wencheng Yang", "Xingshuai Dong", "Xuefei Yin", "Alan Wee-Chung Liew"], "title": "Explainable Artificial Intelligence in Biomedical Image Analysis: A Comprehensive Survey", "comment": null, "summary": "Explainable artificial intelligence (XAI) has become increasingly important\nin biomedical image analysis to promote transparency, trust, and clinical\nadoption of DL models. While several surveys have reviewed XAI techniques, they\noften lack a modality-aware perspective, overlook recent advances in multimodal\nand vision-language paradigms, and provide limited practical guidance. This\nsurvey addresses this gap through a comprehensive and structured synthesis of\nXAI methods tailored to biomedical image analysis.We systematically categorize\nXAI methods, analyzing their underlying principles, strengths, and limitations\nwithin biomedical contexts. A modality-centered taxonomy is proposed to align\nXAI methods with specific imaging types, highlighting the distinct\ninterpretability challenges across modalities. We further examine the emerging\nrole of multimodal learning and vision-language models in explainable\nbiomedical AI, a topic largely underexplored in previous work. Our\ncontributions also include a summary of widely used evaluation metrics and\nopen-source frameworks, along with a critical discussion of persistent\nchallenges and future directions. This survey offers a timely and in-depth\nfoundation for advancing interpretable DL in biomedical image analysis.", "AI": {"tldr": "本文系统地分类并分析了适用于生物医学图像分析的XAI方法，提出了针对不同成像模态的分类方法，探讨了多模态学习和视觉语言模型在该领域的重要性，并讨论了持续的挑战和未来的研究方向。", "motivation": "本文旨在填补现有XAI技术综述中忽略的模态感知视角、多模态和视觉-语言范式的最近进展，以及提供有限的实际指导的空白。", "method": "本文通过系统地分类XAI方法，分析了这些方法的基本原理、优势和局限性，特别是在生物医学成像背景下的应用。此外，本文提出了以模态为中心的分类方法，将XAI方法与其特定的成像类型相结合，并特别关注多模态学习和视觉语言模型在可解释的生物医学AI中的作用，这是以前研究中较少探讨的主题。", "result": "通过对XAI方法的全面结构化综合，提出了一个以模态为中心的分类法，总结了广泛使用的评估指标和开源框架，讨论了现有挑战及未来方向。", "conclusion": "本文提供了一个及时且深入的基础，有助于推动生物医学图像分析中解释性深度学习的发展。"}}
{"id": "2507.07419", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07419", "abs": "https://arxiv.org/abs/2507.07419", "authors": ["Hieu Tran", "Zonghai Yao", "Won Seok Jang", "Sharmin Sultana", "Allen Chang", "Yuan Zhang", "Hong Yu"], "title": "MedReadCtrl: Personalizing medical text generation with readability-controlled instruction learning", "comment": "Equal contribution for the first two authors. arXiv admin note: text\n  overlap with arXiv:2406.09205", "summary": "Generative AI has demonstrated strong potential in healthcare, from clinical\ndecision support to patient-facing chatbots that improve outcomes. A critical\nchallenge for deployment is effective human-AI communication, where content\nmust be both personalized and understandable. We introduce MedReadCtrl, a\nreadability-controlled instruction tuning framework that enables LLMs to adjust\noutput complexity without compromising meaning. Evaluations of nine datasets\nand three tasks across medical and general domains show that MedReadCtrl\nachieves significantly lower readability instruction-following errors than\nGPT-4 (e.g., 1.39 vs. 1.59 on ReadMe, p<0.001) and delivers substantial gains\non unseen clinical tasks (e.g., +14.7 ROUGE-L, +6.18 SARI on MTSamples).\nExperts consistently preferred MedReadCtrl (71.7% vs. 23.3%), especially at low\nliteracy levels. These gains reflect MedReadCtrl's ability to restructure\nclinical content into accessible, readability-aligned language while preserving\nmedical intent, offering a scalable solution to support patient education and\nexpand equitable access to AI-enabled care.", "AI": {"tldr": "研究介绍了一种名为MedReadCtrl的可调整语言复杂度的框架，以提升人机沟通的有效性，尤其在医学领域。评估显示其在多个指标上优于GPT-4，专家也对其高度偏好。", "motivation": "在医疗保健领域部署生成式AI的关键挑战之一是有效的人机沟通，这要求内容必须既个性化又易于理解。", "method": "引入了MedReadCtrl，这是一种可控制可读性的指令微调框架，使大语言模型在不损失意义的前提下调整输出的复杂度。", "result": "评估了医疗和一般领域的九个数据集和三个任务，结果表明MedReadCtrl的可读性指令遵循错误显著低于GPT-4，并在未见过的临床任务上取得了显著提升。专家们也更偏好MedReadCtrl。", "conclusion": "这些成果反映出MedReadCtrl可以将临床内容重新结构化为易于理解的语言，同时保留医疗意图，为患者教育和扩大AI技术支持的医疗的公平访问提供了一个可扩展的解决方案。"}}
{"id": "2507.07151", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07151", "abs": "https://arxiv.org/abs/2507.07151", "authors": ["Zongmeng Zhang", "Wengang Zhou", "Jie Zhao", "Houqiang Li"], "title": "Robust Multimodal Large Language Models Against Modality Conflict", "comment": "ICML 2025", "summary": "Despite the impressive capabilities of multimodal large language models\n(MLLMs) in vision-language tasks, they are prone to hallucinations in\nreal-world scenarios. This paper investigates the hallucination phenomenon in\nMLLMs from the perspective of modality conflict. Unlike existing works focusing\non the conflicts between model responses and inputs, we study the inherent\nconflicts in inputs from different modalities that place MLLMs in a dilemma and\ndirectly lead to hallucinations. We formally define the modality conflict and\nconstruct a dataset named Multimodal Modality Conflict (MMMC) to simulate this\nphenomenon in vision-language tasks. Three methods based on prompt engineering,\nsupervised fine-tuning, and reinforcement learning are proposed to alleviate\nthe hallucination caused by modality conflict. Extensive experiments are\nconducted on the MMMC dataset to analyze the merits and demerits of these\nmethods. Our results show that the reinforcement learning method achieves the\nbest performance in mitigating the hallucination under modality conflict, while\nthe supervised fine-tuning method shows promising and stable performance. Our\nwork sheds light on the unnoticed modality conflict that leads to\nhallucinations and provides more insights into the robustness of MLLMs.", "AI": {"tldr": "研究揭示了多模态大型语言模型中的模态冲突现象，提出了缓解措施，并通过实验验证了这些方法的有效性。", "motivation": "研究多模态大型语言模型在现实世界场景中由于模态冲突导致的幻象现象，探究输入来自不同模态的固有冲突如何使得模型陷入困境从而直接导致幻觉。", "method": "提出了基于提示工程、监督微调和强化学习的三种方法来缓解由模态冲突引起的幻象。", "result": "实验显示强化学习方法在缓解模态冲突条件下幻象方面表现最好，而监督微调方法表现出稳定且有潜力的表现。", "conclusion": "该研究为理解多模态大型语言模型中的模态冲突现象提供了新的视角，并为提高这些模型的鲁棒性提出了见解。"}}
{"id": "2507.07421", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07421", "abs": "https://arxiv.org/abs/2507.07421", "authors": ["Zonghai Yao", "Youxia Zhao", "Avijit Mitra", "David A. Levy", "Emily Druhl", "Jack Tsai", "Hong Yu"], "title": "SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic EHR Data", "comment": "Equal contribution for the first two authors", "summary": "Eviction is a significant yet understudied social determinants of health\n(SDoH), linked to housing instability, unemployment, and mental health. While\neviction appears in unstructured electronic health records (EHRs), it is rarely\ncoded in structured fields, limiting downstream applications. We introduce\nSynthEHR-Eviction, a scalable pipeline combining LLMs, human-in-the-loop\nannotation, and automated prompt optimization (APO) to extract eviction\nstatuses from clinical notes. Using this pipeline, we created the largest\npublic eviction-related SDoH dataset to date, comprising 14 fine-grained\ncategories. Fine-tuned LLMs (e.g., Qwen2.5, LLaMA3) trained on\nSynthEHR-Eviction achieved Macro-F1 scores of 88.8% (eviction) and 90.3% (other\nSDoH) on human validated data, outperforming GPT-4o-APO (87.8%, 87.3%),\nGPT-4o-mini-APO (69.1%, 78.1%), and BioBERT (60.7%, 68.3%), while enabling\ncost-effective deployment across various model sizes. The pipeline reduces\nannotation effort by over 80%, accelerates dataset creation, enables scalable\neviction detection, and generalizes to other information extraction tasks.", "AI": {"tldr": "该研究提出SynthEHR-Eviction用于提取电子健康记录（EHRs）中的驱逐记录，实现高效的数据抽取和低成本部署，同时创建了大规模公共数据集，提高了驱逐信息及其他SDoH信息提取的准确性。", "motivation": "驱逐作为一种重要的健康社会决定因素（SDoH）至今研究不足。它与住房不稳定性、失业和精神健康问题相关联。驱逐记录虽然存在于未结构化的电子健康记录中，但在结构化字段中极少编码，限制了其后续应用。为了应对这些问题，该研究提出了SynthEHR-Eviction。", "method": "该研究使用了一种名为SynthEHR-Eviction的可扩展管道，结合了大语言模型（LLMs）、人类在回路中注释和自动化提示优化（APO），用于从临床记录中提取驱逐状态。该系统包括LLMs处理未结构化的电子健康记录中的驱逐信息，以及通过人机协作优化模型和标记的工作流程。", "result": "研究团队利用该管道创建了迄今为止最大的公共驱逐相关信息提取SDoH数据集，其中包括14个细粒度类别。经过微调的大型语言模型（例如Qwen2.5、LLaMA3）在人工验证的数据上实现了88.8%（驱逐）和90.3%（其他SDoH）的宏F1分数，优于GPT-4o-APO（87.8%，87.3%）、GPT-4o-mini-APO（69.1%，78.1%）和BioBERT（60.7%，68.3%）。", "conclusion": "该研究提出的SynthEHR-Eviction管道显著减少了标注工作量，加速了数据集的创建，并为驱逐信息的可扩展检测提供了可能，同时还可以泛化应用于其他信息提取任务。这种方法不仅提升了行业内的数据挖掘效率，也扩大了对SDoH关注的研究领域。"}}
{"id": "2507.07153", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07153", "abs": "https://arxiv.org/abs/2507.07153", "authors": ["Antonella Barisic Kulas", "Frano Petric", "Stjepan Bogdan"], "title": "Aerial Maritime Vessel Detection and Identification", "comment": "Preprint. ICUAS 2025", "summary": "Autonomous maritime surveillance and target vessel identification in\nenvironments where Global Navigation Satellite Systems (GNSS) are not available\nis critical for a number of applications such as search and rescue and threat\ndetection. When the target vessel is only described by visual cues and its last\nknown position is not available, unmanned aerial vehicles (UAVs) must rely\nsolely on on-board vision to scan a large search area under strict\ncomputational constraints. To address this challenge, we leverage the YOLOv8\nobject detection model to detect all vessels in the field of view. We then\napply feature matching and hue histogram distance analysis to determine whether\nany detected vessel corresponds to the target. When found, we localize the\ntarget using simple geometric principles. We demonstrate the proposed method in\nreal-world experiments during the MBZIRC2023 competition, integrated into a\nfully autonomous system with GNSS-denied navigation. We also evaluate the\nimpact of perspective on detection accuracy and localization precision and\ncompare it with the oracle approach.", "AI": {"tldr": "The paper discusses an autonomous maritime surveillance system using UAVs with on-board vision for target vessel identification in GNSS-denied environments. It uses YOLOv8 for vessel detection, followed by feature matching and hue histogram analysis for target identification, and evaluates the system's performance under various perspectives during the MBZIRC2023 competition.", "motivation": "The motivation of the paper is to provide an autonomous solution for surveillance and target identification in areas where GPS signals are unreliable or unavailable, which is crucial for several applications including search and rescue and threat detection.", "method": "The paper utilizes the YOLOv8 object detection model for identifying all vessels and uses additional techniques like feature matching and hue histogram analysis to confirm the target vessel. A geometric approach is then used to localize the target.", "result": "The method is evaluated through real-world experiments during the MBZIRC2023 competition, demonstrating its capabilities in a fully autonomous system. The impact of perspective on detection accuracy and localization precision is also analyzed.", "conclusion": "The proposed method effectively accomplishes target vessel identification and localization in GNSS-denied environments, showing promising results from its implementation in autonomous UAV systems tested in real-world conditions."}}
{"id": "2507.07439", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07439", "abs": "https://arxiv.org/abs/2507.07439", "authors": ["Matthieu Boileau", "Philippe Helluy", "Jeremy Pawlus", "Svitlana Vyetrenko"], "title": "Towards Interpretable Time Series Foundation Models", "comment": "International Conference on Machine Leaning (ICML) 2025 Workshop on\n  Foundation Models for Structured Data", "summary": "In this paper, we investigate the distillation of time series reasoning\ncapabilities into small, instruction-tuned language models as a step toward\nbuilding interpretable time series foundation models. Leveraging a synthetic\ndataset of mean-reverting time series with systematically varied trends and\nnoise levels, we generate natural language annotations using a large multimodal\nmodel and use these to supervise the fine-tuning of compact Qwen models. We\nintroduce evaluation metrics that assess the quality of the distilled reasoning\n- focusing on trend direction, noise intensity, and extremum localization - and\nshow that the post-trained models acquire meaningful interpretive capabilities.\nOur results highlight the feasibility of compressing time series understanding\ninto lightweight, language-capable models suitable for on-device or\nprivacy-sensitive deployment. This work contributes a concrete foundation\ntoward developing small, interpretable models that explain temporal patterns in\nnatural language.", "AI": {"tldr": "研究显示，通过微调小型语言模型，可以实现对时间序列数据的解释性理解，适合设备端或注重隐私的部署。", "motivation": "探索将时间序列推理能力蒸馏到小型指令调整语言模型中，以实现可解释的时间序列基础模型。", "method": "利用合成的均值回复时间序列数据集，通过一个大型多模态模型生成自然语言注释，并用这些注释监督紧凑型Qwen模型的微调。", "result": "研究表明，微调后的模型获得了有意义的解释能力，证明了将时间序列理解压缩到轻量级语言模型中的可行性。", "conclusion": "本研究为发展能够以自然语言解释时间模式的小型可解释模型提供了实际基础。"}}
{"id": "2507.07154", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07154", "abs": "https://arxiv.org/abs/2507.07154", "authors": ["Desheng Li", "Chaoliang Liu", "Zhiyong Xiao"], "title": "CL-Polyp: A Contrastive Learning-Enhanced Network for Accurate Polyp Segmentation", "comment": null, "summary": "Accurate segmentation of polyps from colonoscopy images is crucial for the\nearly diagnosis and treatment of colorectal cancer. Most existing deep\nlearning-based polyp segmentation methods adopt an Encoder-Decoder\narchitecture, and some utilize multi-task frameworks that incorporate auxiliary\ntasks such as classification to enhance segmentation performance. However,\nthese approaches often require additional labeled data and rely on task\nsimilarity, which can limit their generalizability. To address these\nchallenges, we propose CL-Polyp, a contrastive learning-enhanced polyp\nsegmentation network. Our method leverages contrastive learning to improve the\nencoder's ability to extract discriminative features by contrasting positive\nand negative sample pairs derived from polyp images. This self-supervised\nstrategy enhances visual representation without requiring additional\nannotations. In addition, we introduce two lightweight and effective modules:\nthe Modified Atrous Spatial Pyramid Pooling (MASPP) module for better\nmulti-scale feature fusion, and the Channel Concatenate and Element Add (CA)\nmodule to fuse low-level and upsampled features for improved boundary\nreconstruction. Extensive experiments on five benchmark datasets-Kvasir-SEG,\nCVC-ClinicDB, CVC-ColonDB, CVC-300, and ETIS-demonstrate that CL-Polyp\nconsistently outperforms state-of-the-art methods. Specifically, it improves\nthe IoU metric by 0.011 and 0.020 on the Kvasir-SEG and CVC-ClinicDB datasets,\nrespectively, validating its effectiveness in clinical polyp segmentation\ntasks.", "AI": {"tldr": "本文提出了CL-Polyp，一种基于对比学习增强的息肉分割网络，通过对比正负样本对来提升编码器提取判别特征的能力，同时引入了MASPP和CA模块来改善多尺度特征融合和边界重建。实验表明，CL-Polyp在五个基准数据集上均优于现有方法。", "motivation": "现有的息肉分割方法依赖于辅助任务并需要额外标签，这限制了其在不同任务间的广泛应用性。", "method": "本文提出了CL-Polyp，利用对比学习来增强特征提取，引入了MASPP和CA模块，分别用于多尺度特征融合和边界重建。", "result": "在五个基准数据集（Kvasir-SEG, CVC-ClinicDB, CVC-ColonDB, CVC-300, ETIS）上的实验表明，CL-Polyp优于现有的方法，具体而言，在Kvasir-SEG和CVC-ClinicDB数据集上，IoU分别提高了0.011和0.020。", "conclusion": "CL-Polyp作为对比学习增强的息肉分割模型，通过改进特征提取、多尺度特征融合和边界重建，提升了息肉分割的性能，对临床息肉分割任务有效。"}}
{"id": "2507.07441", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07441", "abs": "https://arxiv.org/abs/2507.07441", "authors": ["Yu Xia", "Yiran Jenny Shen", "Junda Wu", "Tong Yu", "Sungchul Kim", "Ryan A. Rossi", "Lina Yao", "Julian McAuley"], "title": "SAND: Boosting LLM Agents with Self-Taught Action Deliberation", "comment": null, "summary": "Large Language Model (LLM) agents are commonly tuned with supervised\nfinetuning on ReAct-style expert trajectories or preference optimization over\npairwise rollouts. Most of these methods focus on imitating specific expert\nbehaviors or promoting chosen reasoning thoughts and actions over rejected\nones. However, without reasoning and comparing over alternatives actions, LLM\nagents finetuned with these methods may over-commit towards seemingly plausible\nbut suboptimal actions due to limited action space exploration. To address\nthis, in this paper we propose Self-taught ActioN Deliberation (SAND)\nframework, enabling LLM agents to explicitly deliberate over candidate actions\nbefore committing to one. To tackle the challenges of when and what to\ndeliberate given large action space and step-level action evaluation, we\nincorporate self-consistency action sampling and execution-guided action\ncritique to help synthesize step-wise action deliberation thoughts using the\nbase model of the LLM agent. In an iterative manner, the deliberation\ntrajectories are then used to finetune the LLM agent itself. Evaluating on two\nrepresentative interactive agent tasks, SAND achieves an average 20%\nimprovement over initial supervised finetuning and also outperforms\nstate-of-the-art agent tuning approaches.", "AI": {"tldr": "该研究提出了一种新的SAND框架，通过自我教学行动审议，解决了大语言模型代理在行动选择中的次优决策问题，实现了显著性能提升。", "motivation": "大多数当前的方法关注于模仿特定的专家行为或促进被选择的推理思想和行动，而忽略了对备选行动的较为全面的探索。这样的限制导致LLM代理可能过于倾向于那些看似可能但实际上次优的行动。因此，提出一个新的框架来改善当前的策略显得十分必要。", "method": "大型语言模型（LLM）代理通常通过监督微调或成对滚动的偏好优化进行调整。为解决仅模仿特定专家行为或选择性促进推理思维而忽略子优化动作的问题，本文提出了一种自我教学行动审议（SAND）框架，使LLM代理能够显式地审议候选行动。为了解决在大规模行动空间和行动评估中的审议挑战，我们融入了自我一致性行动采样和执行引导的行动评估，以帮助合成逐步的行动审议思维。通过迭代方式使用审议轨迹对LLM代理进行微调，SAND方法在两个代表性交互代理任务中实现了平均20%的改进，超越了最先进的代理调整方法。", "result": "实验结果表明，与初始监督微调相比，SAND框架在两个代表性交互代理任务上获得了平均20%的性能提升，并优于最先进的代理调优方法。", "conclusion": "SAND框架通过使LLM代理能显式审议候选行动，解决了选择次优行动的问题，并在测试任务中表现出优于现有方法的表现。"}}
{"id": "2507.07157", "categories": ["cs.CV", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07157", "abs": "https://arxiv.org/abs/2507.07157", "authors": ["Arshak Rezvani", "Ali Akbari", "Kosar Sanjar Arani", "Maryam Mirian", "Emad Arasteh", "Martin J. McKeown"], "title": "Interpretable EEG-to-Image Generation with Semantic Prompts", "comment": "Actionable Interpretability Workshop (non-archival) at the 42\n  International Conference on Machine Learning", "summary": "Decoding visual experience from brain signals offers exciting possibilities\nfor neuroscience and interpretable AI. While EEG is accessible and temporally\nprecise, its limitations in spatial detail hinder image reconstruction. Our\nmodel bypasses direct EEG-to-image generation by aligning EEG signals with\nmultilevel semantic captions -- ranging from object-level to abstract themes --\ngenerated by a large language model. A transformer-based EEG encoder maps brain\nactivity to these captions through contrastive learning. During inference,\ncaption embeddings retrieved via projection heads condition a pretrained latent\ndiffusion model for image generation. This text-mediated framework yields\nstate-of-the-art visual decoding on the EEGCVPR dataset, with interpretable\nalignment to known neurocognitive pathways. Dominant EEG-caption associations\nreflected the importance of different semantic levels extracted from perceived\nimages. Saliency maps and t-SNE projections reveal semantic topography across\nthe scalp. Our model demonstrates how structured semantic mediation enables\ncognitively aligned visual decoding from EEG.", "AI": {"tldr": "本文提出了一种使用文本中介框架进行认知对齐的视觉解码方法，该方法通过对EEG信号与多层级语义字幕的对齐实现了从EEG到图像的先进生成。", "motivation": "虽然EEG在时间和可访问性方面有优势，但其空间细节的局限性阻碍了图像重建。本文的方法通过绕过直接的EEG到图像生成，改进了从EEG信号中解码视觉体验的效果。", "method": "使用大型语言模型生成多层级语义字幕（从对象层面到抽象主题）来与EEG信号对齐，通过对比学习将脑活动映射到这些字幕，利用transformer为基础的EEG编码器。在推理过程中，通过投射头检索到的字幕嵌入条件化预训练的潜在扩散模型来生成图像。", "result": "该文本中介框架在EEGCVPR数据集上实现了最先进的视觉解码，揭示了不同的语义层级在感知图像中的重要性，并且通过显着性图和t-SNE投影展示了头皮上的语义地形。", "conclusion": "研究表明结构化的语义中介能更有效地实现从EEG到视觉的解码，揭示了神经认知路径的解释性对齐。"}}
{"id": "2507.07451", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07451", "abs": "https://arxiv.org/abs/2507.07451", "authors": ["Hongzhi Zhang", "Jia Fu", "Jingyuan Zhang", "Kai Fu", "Qi Wang", "Fuzheng Zhang", "Guorui Zhou"], "title": "RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning", "comment": "https://github.com/Kwai-Klear/RLEP", "summary": "Reinforcement learning (RL) for large language models is an energy-intensive\nendeavor: training can be unstable, and the policy may gradually drift away\nfrom its pretrained weights. We present \\emph{RLEP}\\, -- \\,Reinforcement\nLearning with Experience rePlay\\, -- \\,a two-phase framework that first\ncollects verified trajectories and then replays them during subsequent\ntraining. At every update step, the policy is optimized on mini-batches that\nblend newly generated rollouts with these replayed successes. By replaying\nhigh-quality examples, RLEP steers the model away from fruitless exploration,\nfocuses learning on promising reasoning paths, and delivers both faster\nconvergence and stronger final performance. On the Qwen2.5-Math-7B base model,\nRLEP reaches baseline peak accuracy with substantially fewer updates and\nultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,\non AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our\ncode, datasets, and checkpoints are publicly available at\nhttps://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further\nresearch.", "AI": {"tldr": "RLEP框架通过收集验证轨迹并回放这些轨迹，优化大型语言模型的强化学习，实现更快的收敛速度和更好的最终性能。", "motivation": "传统的强化学习方法对于大型语言模型来说能耗高且不稳定，本研究希望通过提出的新框架优化这一问题。", "method": "研究设计了RLEP框架，包括收集验证轨迹和回放这些轨迹的两个阶段，优化新生成的滚动数据和回放成功轨迹的混合方法。", "result": "该研究提出了RLEP框架，通过两个阶段收集验证轨迹并随后在其上进行回放，从而提高了强化学习中大型语言模型的速度和最终性能。RLEP通过回放高质量的轨迹，避免了无效探索，加快了模型的收敛速度，且最终表现超过了基线。实验表明，RLEP在不同数据集上均展示了更强的性能。", "conclusion": "RLEP框架在保持并改善大型语言模型的实际性能的同时，提升了强化学习的效率和效果，对于研究和应用都有着重要意义。"}}
{"id": "2507.07202", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07202", "abs": "https://arxiv.org/abs/2507.07202", "authors": ["Mohamed Elmoghany", "Ryan Rossi", "Seunghyun Yoon", "Subhojyoti Mukherjee", "Eslam Bakr", "Puneet Mathur", "Gang Wu", "Viet Dac Lai", "Nedim Lipka", "Ruiyi Zhang", "Varun Manjunatha", "Chien Nguyen", "Daksh Dangi", "Abel Salinas", "Mohammad Taesiri", "Hongjie Chen", "Xiaolei Huang", "Joe Barrow", "Nesreen Ahmed", "Hoda Eldardiry", "Namyong Park", "Yu Wang", "Jaemin Cho", "Anh Totti Nguyen", "Zhengzhong Tu", "Thien Nguyen", "Dinesh Manocha", "Mohamed Elhoseiny", "Franck Dernoncourt"], "title": "A Survey on Long-Video Storytelling Generation: Architectures, Consistency, and Cinematic Quality", "comment": null, "summary": "Despite the significant progress that has been made in video generative\nmodels, existing state-of-the-art methods can only produce videos lasting 5-16\nseconds, often labeled \"long-form videos\". Furthermore, videos exceeding 16\nseconds struggle to maintain consistent character appearances and scene layouts\nthroughout the narrative. In particular, multi-subject long videos still fail\nto preserve character consistency and motion coherence. While some methods can\ngenerate videos up to 150 seconds long, they often suffer from frame redundancy\nand low temporal diversity. Recent work has attempted to produce long-form\nvideos featuring multiple characters, narrative coherence, and high-fidelity\ndetail. We comprehensively studied 32 papers on video generation to identify\nkey architectural components and training strategies that consistently yield\nthese qualities. We also construct a comprehensive novel taxonomy of existing\nmethods and present comparative tables that categorize papers by their\narchitectural designs and performance characteristics.", "AI": {"tldr": "本文探讨了视频生成的关键架构和训练策略，并对现有方法进行了分类，以提高生成长视频的质量，解决帧冗余和时间多样性的挑战。", "motivation": "尽管视频生成模型取得了显著进展，但现有最先进的方法只能生成持续5-16秒的视频，而且长时间视频难以保持角色外观和场景布局的一致性，特别是多角色长视频更难以保持角色一致性和动作连贯性。此外，虽然一些方法能生成长达150秒的视频，但它们常常存在帧冗余和低时间多样性的问题。本文试图通过研究解决这些问题。", "method": "本文通过对32篇视频生成论文的全面研究，旨在识别出能够生成高质量长视频的关键架构组件和训练策略。此外，本文还构建了一个新的分类法，并给出了比较表格，对现有方法按其架构设计和性能特点进行分类。", "result": "无具体结果，因摘要仅描述了研究动机、方法和目标，并未展示具体研究结果。", "conclusion": "通过构建全面的论文分类法和比较表格，本文为未来长视频生成研究提供了宝贵的参考框架，指出了关键的架构组件和训练策略。"}}
{"id": "2507.07484", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07484", "abs": "https://arxiv.org/abs/2507.07484", "authors": ["Kaiqu Liang", "Haimin Hu", "Xuandong Zhao", "Dawn Song", "Thomas L. Griffiths", "Jaime Fernández Fisac"], "title": "Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models", "comment": "Project page, code & data: https://machine-bullshit.github.io", "summary": "Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to\nstatements made without regard to their truth value. While previous work has\nexplored large language model (LLM) hallucination and sycophancy, we propose\nmachine bullshit as an overarching conceptual framework that can allow\nresearchers to characterize the broader phenomenon of emergent loss of\ntruthfulness in LLMs and shed light on its underlying mechanisms. We introduce\nthe Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and\npropose a complementary taxonomy analyzing four qualitative forms of bullshit:\nempty rhetoric, paltering, weasel words, and unverified claims. We conduct\nempirical evaluations on the Marketplace dataset, the Political Neutrality\ndataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI\nassistants) explicitly designed to evaluate machine bullshit. Our results\ndemonstrate that model fine-tuning with reinforcement learning from human\nfeedback (RLHF) significantly exacerbates bullshit and inference-time\nchain-of-thought (CoT) prompting notably amplify specific bullshit forms,\nparticularly empty rhetoric and paltering. We also observe prevalent machine\nbullshit in political contexts, with weasel words as the dominant strategy. Our\nfindings highlight systematic challenges in AI alignment and provide new\ninsights toward more truthful LLM behavior.", "AI": {"tldr": "该论文提出machine bullshit的概念，引入Bullshit Index量化机器模型的bullshit，并发现模型的微调和推理时的chain-of-thought提示会加剧此类问题，指出了AI对齐过程中系统的挑战。", "motivation": "该论文提出了将machine bullshit作为研究LLM中更广泛的脱离真实性现象的概念框架，并探索其潜在机制，目的是为研究人员提供一种分析复杂问题的新方法。", "method": "该论文引入了“Bullshit Index”来量化大语言模型（LLM）对真理的漠视，并提出了一个分析四种质性形式的bullshit（空洞的修辞、佯装、含糊其辞和未经验证的声明）的补充分类法。同时，进行了Marketplace数据集、政治中立数据集和新的BullshitEval基准（2400种情境，涵盖100个AI助手）的实证评估，后者专门设计用于评估机器bullshit。", "result": "实证评估结果表明，利用来自人类反馈的强化学习进行模型微调显著加剧了bullshit，推理阶段的chain-of-thought提示显著放大了特定形式的bullshit，尤其是在政治情景下含糊其辞的表现最为突出。", "conclusion": "研究结果揭示了系统性的AI对齐挑战，并为进一步实现更真实的LLM行为提供了新的见解。"}}
{"id": "2507.07230", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07230", "abs": "https://arxiv.org/abs/2507.07230", "authors": ["Priyank Pathak", "Yogesh S. Rawat"], "title": "Colors See Colors Ignore: Clothes Changing ReID with Color Disentanglement", "comment": "ICCV'25 paper", "summary": "Clothes-Changing Re-Identification (CC-ReID) aims to recognize individuals\nacross different locations and times, irrespective of clothing. Existing\nmethods often rely on additional models or annotations to learn robust,\nclothing-invariant features, making them resource-intensive. In contrast, we\nexplore the use of color - specifically foreground and background colors - as a\nlightweight, annotation-free proxy for mitigating appearance bias in ReID\nmodels. We propose Colors See, Colors Ignore (CSCI), an RGB-only method that\nleverages color information directly from raw images or video frames. CSCI\nefficiently captures color-related appearance bias ('Color See') while\ndisentangling it from identity-relevant ReID features ('Color Ignore'). To\nachieve this, we introduce S2A self-attention, a novel self-attention to\nprevent information leak between color and identity cues within the feature\nspace. Our analysis shows a strong correspondence between learned color\nembeddings and clothing attributes, validating color as an effective proxy when\nexplicit clothing labels are unavailable. We demonstrate the effectiveness of\nCSCI on both image and video ReID with extensive experiments on four CC-ReID\ndatasets. We improve the baseline by Top-1 2.9% on LTCC and 5.0% on PRCC for\nimage-based ReID, and 1.0% on CCVID and 2.5% on MeVID for video-based ReID\nwithout relying on additional supervision. Our results highlight the potential\nof color as a cost-effective solution for addressing appearance bias in\nCC-ReID. Github: https://github.com/ppriyank/ICCV-CSCI-Person-ReID.", "AI": {"tldr": "提出一种以颜色为代理的服装变化重识别方法CSCI，该方法没有依赖额外的模型或标注，通过RGB信息直接从原始数据中分离出颜色和身份特征，显著提升了ReID性能。", "motivation": "解决现有的服装变化重识别方法依赖额外模型或标注，资源消耗大的问题。探索颜色作为对抗重识别模型外观偏差的轻量级、无标注代理。", "method": "Colors See, Colors Ignore (CSCI), 使用RGB信息直接从原始图像或视频帧中捕获颜色相关外观偏差，同时隔离与身份相关的重识别特征。为防止特征空间中颜色和身份特征之间的信息泄露，引入了S2A自注意力机制。", "result": "在四个服装变化重识别数据集上进行了广泛的实验，证明了CSCI的有效性，相比于基准，在图像基重识别上提高了LTCC数据集Top-1 2.9%，PRCC数据集Top-1 5.0%，在视频基重识别上提高了CCVID数据集1.0%，MeVID数据集2.5%，且无需依赖额外的监督。", "conclusion": "证明了颜色作为当显式服装标签不可用时的有效代理，并展示了颜色作为一种成本效益高的解决方案，在处理CC-ReID中的外观偏差方面的潜力。"}}
{"id": "2507.07495", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07495", "abs": "https://arxiv.org/abs/2507.07495", "authors": ["Mihir Parmar", "Palash Goyal", "Xin Liu", "Yiwen Song", "Mingyang Ling", "Chitta Baral", "Hamid Palangi", "Tomas Pfister"], "title": "PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving", "comment": "15 Pages", "summary": "Recently, decomposing complex problems into simple subtasks--a crucial part\nof human-like natural planning--to solve the given problem has significantly\nboosted the performance of large language models (LLMs). However, leveraging\nsuch planning structures during post-training to boost the performance of\nsmaller open-source LLMs remains underexplored. Motivated by this, we introduce\nPLAN-TUNING, a unified post-training framework that (i) distills synthetic task\ndecompositions (termed \"planning trajectories\") from large-scale LLMs and (ii)\nfine-tunes smaller models via supervised and reinforcement-learning objectives\ndesigned to mimic these planning processes to improve complex reasoning. On\nGSM8k and the MATH benchmarks, plan-tuned models outperform strong baselines by\nan average $\\sim7\\%$. Furthermore, plan-tuned models show better generalization\ncapabilities on out-of-domain datasets, with average $\\sim10\\%$ and $\\sim12\\%$\nperformance improvements on OlympiadBench and AIME 2024, respectively. Our\ndetailed analysis demonstrates how planning trajectories improves complex\nreasoning capabilities, showing that PLAN-TUNING is an effective strategy for\nimproving task-specific performance of smaller LLMs.", "AI": {"tldr": "PLAN-TUNING 是一种后训练框架，它将大型语言模型（LLM）的合成任务分解（即“规划轨迹”）提炼出来，然后通过监督学习和强化学习目标微调较小的模型，以模仿这些规划过程并提高复杂推理能力。实验结果表明，这种方法在多个基准测试中表现出色，展示了比强基线高出约7%的性能，并具有更好的泛化能力。", "motivation": "虽然将复杂问题分解为简单子任务对于提高大型语言模型的性能有很大帮助，但却很少有研究利用这种规划结构来提升小型的开源语言模型的性能。因此，提出 PLAN-TUNING 来填补这一领域的空白。", "method": "PLAN-TUNING 方法包括两大部分：一是从大规模语言模型中提炼出合成任务分解（规划轨迹），二是通过设计促进模仿这些规划过程的监督学习和强化学习目标来微调小型模型。", "result": "在 GSM8k 和 MATH 基准测试中，经 PLAN-TUNING 微调的模型比强基线高出约7%的性能；在 OlympiadBench 和 AIME 2024 测试集上，性能提升分别为约10%和12%，验证了 PLAN-TUNING 对于改进小型语言模型的特定任务性能是有效的。", "conclusion": "通过详细的分析显示，规划轨迹对提升复杂推理能力有益，并且 PLAN-TUNING 是一种有效策略，能够更好地解决复杂的问题，特别是在提高小型语言模型的特定任务性能方面取得了显著效果。"}}
{"id": "2507.07242", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07242", "abs": "https://arxiv.org/abs/2507.07242", "authors": ["Johannes Merz", "Lucien Fostier"], "title": "Automated Video Segmentation Machine Learning Pipeline", "comment": null, "summary": "Visual effects (VFX) production often struggles with slow, resource-intensive\nmask generation. This paper presents an automated video segmentation pipeline\nthat creates temporally consistent instance masks. It employs machine learning\nfor: (1) flexible object detection via text prompts, (2) refined per-frame\nimage segmentation and (3) robust video tracking to ensure temporal stability.\nDeployed using containerization and leveraging a structured output format, the\npipeline was quickly adopted by our artists. It significantly reduces manual\neffort, speeds up the creation of preliminary composites, and provides\ncomprehensive segmentation data, thereby enhancing overall VFX production\nefficiency.", "AI": {"tldr": "论文描述了一种自动化的视频分割管道，涉及到机器学习、容器化部署和时间一致的实例遮罩，显著提升了VFX生产的效率。", "motivation": "视觉效果（VFX）制作过程中，通常会遇到生成蒙版过程缓慢且资源消耗大的问题。本论文就是为了解决这些问题提出的。", "method": "自动化的视频分割管道采用机器学习技术来创建时间一致的实例蒙版。具体使用了三个关键步骤：（1）通过文本提示灵活的对象检测，（2）每帧图像的精确分割，（3）确保时间稳定性的鲁棒视频跟踪。部署使用了容器化技术，并采用了结构化的输出格式。", "result": "该管道被艺术家迅速采纳，显著减少了人工工作量，加速了初步合成的创建，并提供了全面的分割数据，从而提高了整体VFX生产的效率。", "conclusion": "提出的方法有效地解决了VFX制作中面临的慢速和耗资源的问题，通过自动化的视频分割管道，提高了生产和制作效率。"}}
{"id": "2507.07498", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07498", "abs": "https://arxiv.org/abs/2507.07498", "authors": ["Keqin Bao", "Nuo Chen", "Xiaoyuan Li", "Binyuan Hui", "Bowen Yu", "Fuli Feng", "Junyang Lin", "Xiangnan He", "Dayiheng Liu"], "title": "Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems without Code", "comment": null, "summary": "Enhancing reasoning capabilities remains a central focus in the LLM reasearch\ncommunity. A promising direction involves requiring models to simulate code\nexecution step-by-step to derive outputs for given inputs. However, as code is\noften designed for large-scale systems, direct application leads to\nover-reliance on complex data structures and algorithms, even for simple cases,\nresulting in overfitting to algorithmic patterns rather than core reasoning\nstructures. To address this, we propose TeaR, which aims at teaching LLMs to\nreason better. TeaR leverages careful data curation and reinforcement learning\nto guide models in discovering optimal reasoning paths through code-related\ntasks, thereby improving general reasoning abilities. We conduct extensive\nexperiments using two base models and three long-CoT distillation models, with\nmodel sizes ranging from 1.5 billion to 32 billion parameters, and across 17\nbenchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results\nconsistently show significant performance improvements. Notably, TeaR achieves\na 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.", "AI": {"tldr": "论文提出了 TeaR 方法，通过数据整理和强化学习提高 LLM 的推理能力，实验结果显示显著的性能提升，特别是在某些模型上的大幅性能改进。", "motivation": "论文动机在于改进大型语言模型（LLM）的推理能力。通常，模型在模拟代码执行时容易过拟合到算法模式，而不是核心推理结构。", "method": "TeaR 提出了一种通过仔细的数据整理和强化学习来指导模型发现与代码相关的任务中的最优推理路径的方法，从而提高模型的总体推理能力。", "result": "实验结果表明，在跨越数学、知识、代码和逻辑推理17个基准测试中，使用不同大小基础模型和长CoT蒸馏模型的测试中，TeaR 方法表现出了显著的性能提升，如在 Qwen2.5-7B 上提升了 35.9%，在 R1-Distilled-7B 上提升了 5.9%。", "conclusion": "TeaR 方法通过优化数据整理和应用强化学习指导模型发现最优推理路径，显著提高了大型语言模型在多种推理任务上的表现。"}}
{"id": "2507.07262", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07262", "abs": "https://arxiv.org/abs/2507.07262", "authors": ["Shehreen Azad", "Yogesh S Rawat"], "title": "DisenQ: Disentangling Q-Former for Activity-Biometrics", "comment": "Accepted in ICCV 2025", "summary": "In this work, we address activity-biometrics, which involves identifying\nindividuals across diverse set of activities. Unlike traditional person\nidentification, this setting introduces additional challenges as identity cues\nbecome entangled with motion dynamics and appearance variations, making\nbiometrics feature learning more complex. While additional visual data like\npose and/or silhouette help, they often struggle from extraction inaccuracies.\nTo overcome this, we propose a multimodal language-guided framework that\nreplaces reliance on additional visual data with structured textual\nsupervision. At its core, we introduce \\textbf{DisenQ} (\\textbf{Disen}tangling\n\\textbf{Q}-Former), a unified querying transformer that disentangles\nbiometrics, motion, and non-biometrics features by leveraging structured\nlanguage guidance. This ensures identity cues remain independent of appearance\nand motion variations, preventing misidentifications. We evaluate our approach\non three activity-based video benchmarks, achieving state-of-the-art\nperformance. Additionally, we demonstrate strong generalization to complex\nreal-world scenario with competitive performance on a traditional video-based\nidentification benchmark, showing the effectiveness of our framework.", "AI": {"tldr": "本文提出DisenQ框架，通过结构化的语言指导来解缠特征，实现了更准确的活动生物特征识别，并在三个视频基准测试中达到了最先进的性能。", "motivation": "本文旨在解决活动生物特征识别问题，即在不同活动下识别个体，这比传统的个人识别更加复杂，因为身份线索与运动动力学和外观变化交织在一起。", "method": "本文提出了一种多模态语言引导框架，使用结构化的文本监督代替依赖额外的视觉数据。核心是一个名为DisenQ（解缠Q-Former）的统一查询变压器，它通过结构化的语言指导来解缠生物特征、运动和非生物特征特征，确保身份线索独立于外观和运动变化，从而防止误识别。", "result": "实验结果表明，该框架在三个基于活动的视频基准测试中获得了最先进的性能，并且在传统视频识别基准测试中展示了良好的泛化能力。", "conclusion": "该研究证明了所提出的框架在活动生物特征识别任务中的有效性，并展示了其实现身份线索独立于外观和运动变化的能力。"}}
{"id": "2507.07499", "categories": ["cs.CL", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2507.07499", "abs": "https://arxiv.org/abs/2507.07499", "authors": ["Hein Htet", "Amgad Ahmed Ali Ibrahim", "Yutaka Sasaki", "Ryoji Asahi"], "title": "Extracting ORR Catalyst Information for Fuel Cell from Scientific Literature", "comment": "28 pages, 12 figures, 6 tables", "summary": "The oxygen reduction reaction (ORR) catalyst plays a critical role in\nenhancing fuel cell efficiency, making it a key focus in material science\nresearch. However, extracting structured information about ORR catalysts from\nvast scientific literature remains a significant challenge due to the\ncomplexity and diversity of textual data. In this study, we propose a named\nentity recognition (NER) and relation extraction (RE) approach using DyGIE++\nwith multiple pre-trained BERT variants, including MatSciBERT and PubMedBERT,\nto extract ORR catalyst-related information from the scientific literature,\nwhich is compiled into a fuel cell corpus for materials informatics\n(FC-CoMIcs). A comprehensive dataset was constructed manually by identifying 12\ncritical entities and two relationship types between pairs of the entities. Our\nmethodology involves data annotation, integration, and fine-tuning of\ntransformer-based models to enhance information extraction accuracy. We assess\nthe impact of different BERT variants on extraction performance and investigate\nthe effects of annotation consistency. Experimental evaluations demonstrate\nthat the fine-tuned PubMedBERT model achieves the highest NER F1-score of\n82.19% and the MatSciBERT model attains the best RE F1-score of 66.10%.\nFurthermore, the comparison with human annotators highlights the reliability of\nfine-tuned models for ORR catalyst extraction, demonstrating their potential\nfor scalable and automated literature analysis. The results indicate that\ndomain-specific BERT models outperform general scientific models like BlueBERT\nfor ORR catalyst extraction.", "AI": {"tldr": "本研究提出了一种基于多个BERT变体的NER和RE方法来从科学文献中提取ORR催化剂的信息，实验结果表明领域特定的BERT模型在提取精度上表现更为优异。", "motivation": "氧还原反应（ORR）催化剂对提高燃料电池效率至关重要，而从大量的科学文献中提取结构化的ORR催化剂信息依然是一项重大挑战，这是材料科学研究的关键焦点。", "method": "提出了一种基于DyGIE++的命名实体识别(NER)和关系抽取(RE)方法，利用多种预训练的BERT变体（包括MatSciBERT和PubMedBERT）从科学文献中提取ORR催化剂相关的信息，这些信息被整合到燃料电池语料库（FC-CoMIcs）中用于材料信息学研究。", "result": "实验评估表明，微调后的PubMedBERT模型实现了最高的NER F1分数82.19%，而MatSciBERT模型则在RE上获得了最佳的F1分数66.10%。", "conclusion": "结果表明，领域特定的BERT模型在ORR催化剂提取方面优于一般科学模型，如BlueBERT，这展示了它们在大规模自动化文献分析中的潜力。"}}
{"id": "2507.07274", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07274", "abs": "https://arxiv.org/abs/2507.07274", "authors": ["Ananya Raval", "Aravind Narayanan", "Vahid Reza Khazaie", "Shaina Raza"], "title": "LinguaMark: Do Multimodal Models Speak Fairly? A Benchmark-Based Evaluation", "comment": "Accepted at ASONAM'25", "summary": "Large Multimodal Models (LMMs) are typically trained on vast corpora of\nimage-text data but are often limited in linguistic coverage, leading to biased\nand unfair outputs across languages. While prior work has explored multimodal\nevaluation, less emphasis has been placed on assessing multilingual\ncapabilities. In this work, we introduce LinguaMark, a benchmark designed to\nevaluate state-of-the-art LMMs on a multilingual Visual Question Answering\n(VQA) task. Our dataset comprises 6,875 image-text pairs spanning 11 languages\nand five social attributes. We evaluate models using three key metrics: Bias,\nAnswer Relevancy, and Faithfulness. Our findings reveal that closed-source\nmodels generally achieve the highest overall performance. Both closed-source\n(GPT-4o and Gemini2.5) and open-source models (Gemma3, Qwen2.5) perform\ncompetitively across social attributes, and Qwen2.5 demonstrates strong\ngeneralization across multiple languages. We release our benchmark and\nevaluation code to encourage reproducibility and further research.", "AI": {"tldr": "研究提出了LinguaMark基准，用于评估大模型在多语言视觉问答任务上的表现，发现闭源模型性能最高，开源模型也有较强的推广能力。", "motivation": "尽管大型多模态模型（LMMs）通常是在大量的图像-文本数据上训练的，但它们在语言覆盖范围上往往有限，导致在不同语言间的输出存在偏见和不公平。之前的工作探索了多模态评估，但很少重视评估多语言能力。", "method": "本研究介绍了LinguaMark基准测试，旨在评估最先进的大模型在多语言视觉问答任务上的表现。该数据集包含6875个图像-文本对，涵盖了11种语言和五个社会属性。使用三个关键指标（偏见、答案相关性和忠实性）评估模型表现。", "result": "研究发现闭源模型（GPT-4o和Gemini2.5）通常在整体性能上表现最佳，开源模型（Gemma3，Qwen2.5）在社会属性上也表现得很有竞争力，特别是Qwen2.5在多语言上有较强的泛化能力。", "conclusion": "发布了基准测试和评估代码，鼓励可重复性和进一步研究。"}}
{"id": "2507.07505", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07505", "abs": "https://arxiv.org/abs/2507.07505", "authors": ["Varin Sikka", "Vishal Sikka"], "title": "Hallucination Stations: On Some Basic Limitations of Transformer-Based Language Models", "comment": "6 pages; to be submitted to AAAI-26 after reviews", "summary": "With widespread adoption of transformer-based language models in AI, there is\nsignificant interest in the limits of LLMs capabilities, specifically so-called\nhallucinations, occurrences in which LLMs provide spurious, factually incorrect\nor nonsensical information when prompted on certain subjects. Furthermore,\nthere is growing interest in agentic uses of LLMs - that is, using LLMs to\ncreate agents that act autonomously or semi-autonomously to carry out various\ntasks, including tasks with applications in the real world. This makes it\nimportant to understand the types of tasks LLMs can and cannot perform. We\nexplore this topic from the perspective of the computational complexity of LLM\ninference. We show that LLMs are incapable of carrying out computational and\nagentic tasks beyond a certain complexity, and further that LLMs are incapable\nof verifying the accuracy of tasks beyond a certain complexity. We present\nexamples of both, then discuss some consequences of this work.", "AI": {"tldr": "该论文探讨了基于变压器的语言模型（LLM）在执行任务及验证任务准确性方面的计算复杂性限制。结果表明，LLM无法处理超过一定复杂度的计算和代理任务，并且也无法验证这些复杂任务的准确性。", "motivation": "随着变压器式语言模型在AI应用中的普及，研究其能力局限，尤其是生成虚假、错误信息的问题变得尤为重要。尤其是当使用LLM创建自主或半自主代理以执行各种任务时，了解哪些任务LLM能处理、哪些不能变得越来越关键。", "method": "文章从LLM推理的计算复杂度角度出发，探讨了LLM处理任务及验证准确性时的局限性。", "result": "研究表明，LLM无法执行超过一定复杂度的计算和代理任务，同样，也无法验证这些复杂任务的准确性。", "conclusion": "通过提供具体例子和讨论工作后果，文章指出了了解基于变压器的语言模型执行复杂任务和验证准确性能力局限的重要性。"}}
{"id": "2507.07297", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07297", "abs": "https://arxiv.org/abs/2507.07297", "authors": ["Chengfei Wu", "Ronald Seoh", "Bingxuan Li", "Liqiang Zhang", "Fengrong Han", "Dan Goldwasser"], "title": "MagiC: Evaluating Multimodal Cognition Toward Grounded Visual Reasoning", "comment": null, "summary": "Recent advances in large vision-language models have led to impressive\nperformance in visual question answering and multimodal reasoning. However, it\nremains unclear whether these models genuinely perform grounded visual\nreasoning or rely on superficial patterns and dataset biases. In this work, we\nintroduce MagiC, a comprehensive benchmark designed to evaluate grounded\nmultimodal cognition, assessing not only answer accuracy but also the quality\nof step-by-step reasoning and its alignment with relevant visual evidence. Our\nbenchmark includes approximately 5,500 weakly supervised QA examples generated\nfrom strong model outputs and 900 human-curated examples with fine-grained\nannotations, including answers, rationales, and bounding box groundings. We\nevaluate 15 vision-language models ranging from 7B to 70B parameters across\nfour dimensions: final answer correctness, reasoning validity, grounding\nfidelity, and self-correction ability. MagiC further includes diagnostic\nsettings to probe model robustness under adversarial visual cues and assess\ntheir capacity for introspective error correction. We introduce new metrics\nsuch as MagiScore and StepSense, and provide comprehensive analyses that reveal\nkey limitations and opportunities in current approaches to grounded visual\nreasoning.", "AI": {"tldr": "MagiC是一个新的评估基准，用于评估视觉语言模型的视觉推理水平，揭示了这些模型在推理准确性、定位和自我纠正能力上的限制。", "motivation": "动机在于评估当前视觉语言模型是否真正进行了基于视觉证据的推理，而非依赖浅层模式和数据偏差。", "method": "通过介绍MagiC这一全面评估基准，来测试视觉语言模型的视觉推理能力。该基准包含约5,500个来自强模型输出的弱监督问答示例和900个人类策划的例子，其中包含答案、理由和边界框标注。", "result": "评估了15个视觉语言模型在最终答案准确性、推理有效性、定位准确性以及自我纠正能力上的表现。", "conclusion": "引入了新的评估指标，如MagiScore和StepSense，揭示了现有方法在基于视觉推理上的关键限制和机会。"}}
{"id": "2507.07509", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.07509", "abs": "https://arxiv.org/abs/2507.07509", "authors": ["Yuanchen Shi", "Longyin Zhang", "Fang Kong"], "title": "Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset and a Co-Evolving Multi-Agent System", "comment": "10pages,8 figures", "summary": "The growing need for psychological support due to increasing pressures has\nexposed the scarcity of relevant datasets, particularly in non-English\nlanguages. To address this, we propose a framework that leverages limited\nreal-world data and expert knowledge to fine-tune two large language models:\nDialog Generator and Dialog Modifier. The Generator creates large-scale\npsychological counseling dialogues based on predefined paths, which guide\nsystem response strategies and user interactions, forming the basis for\neffective support. The Modifier refines these dialogues to align with\nreal-world data quality. Through both automated and manual review, we construct\nthe Chinese Psychological support Dialogue Dataset (CPsDD), containing 68K\ndialogues across 13 groups, 16 psychological problems, 13 causes, and 12\nsupport focuses. Additionally, we introduce the Comprehensive Agent Dialogue\nSupport System (CADSS), where a Profiler analyzes user characteristics, a\nSummarizer condenses dialogue history, a Planner selects strategies, and a\nSupporter generates empathetic responses. The experimental results of the\nStrategy Prediction and Emotional Support Conversation (ESC) tasks demonstrate\nthat CADSS achieves state-of-the-art performance on both CPsDD and ESConv\ndatasets.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.07317", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07317", "abs": "https://arxiv.org/abs/2507.07317", "authors": ["Sherry X. Chen", "Yi Wei", "Luowei Zhou", "Suren Kumar"], "title": "ADIEE: Automatic Dataset Creation and Scorer for Instruction-Guided Image Editing Evaluation", "comment": "International Conference on Computer Vision (ICCV) 2025", "summary": "Recent advances in instruction-guided image editing underscore the need for\neffective automated evaluation. While Vision-Language Models (VLMs) have been\nexplored as judges, open-source models struggle with alignment, and proprietary\nmodels lack transparency and cost efficiency. Additionally, no public training\ndatasets exist to fine-tune open-source VLMs, only small benchmarks with\ndiverse evaluation schemes. To address this, we introduce ADIEE, an automated\ndataset creation approach which is then used to train a scoring model for\ninstruction-guided image editing evaluation. We generate a large-scale dataset\nwith over 100K samples and use it to fine-tune a LLaVA-NeXT-8B model modified\nto decode a numeric score from a custom token. The resulting scorer outperforms\nall open-source VLMs and Gemini-Pro 1.5 across all benchmarks, achieving a\n0.0696 (+17.24%) gain in score correlation with human ratings on AURORA-Bench,\nand improving pair-wise comparison accuracy by 4.03% (+7.21%) on GenAI-Bench\nand 4.75% (+9.35%) on AURORA-Bench, respectively, compared to the\nstate-of-the-art. The scorer can act as a reward model, enabling automated best\nedit selection and model fine-tuning. Notably, the proposed scorer can boost\nMagicBrush model's average evaluation score on ImagenHub from 5.90 to 6.43\n(+8.98%).", "AI": {"tldr": "提出ADIEE，一种用于指导图像编辑评价的自动化数据集创建方法，生成大样本数据集训练LLaVA-NeXT-8B模型，该评分器在各种基准测试中超越现有模型，显著提高了相应的评分相关性和准确率。", "motivation": "为解决图像编辑指导中自动评估的有效性问题，现有的开源VLM模型难以对齐，专有模型透明度低且成本效益差，缺乏可训练数据集。", "method": "引入ADIEE，一种自动数据集创建方法，用于训练图像编辑评价评分模型。用超过10万条样本训练LLaVA-NeXT-8B模型，并修改它从自定义标记中解码出数值分数。", "result": "评分器超越了所有开源VLMs和Gemini-Pro 1.5，与人类评估评分的相关性提高了17.24%，在GenAI-Bench和AURORA-Bench基准测试中提高了4.03%和4.75%的配对比较准确率。", "conclusion": "该评分器在AURORA-Bench和GenAI-Bench基准测试中的分数与人类评价的相关性提高了17.24%和7.21%，并提高了MagicBrush模型在ImagenHub上的平均评估分数8.98%。"}}
{"id": "2507.07518", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07518", "abs": "https://arxiv.org/abs/2507.07518", "authors": ["Mikey Elmers", "Koji Inoue", "Divesh Lala", "Tatsuya Kawahara"], "title": "Triadic Multi-party Voice Activity Projection for Turn-taking in Spoken Dialogue Systems", "comment": "Accepted to Interspeech 2025", "summary": "Turn-taking is a fundamental component of spoken dialogue, however\nconventional studies mostly involve dyadic settings. This work focuses on\napplying voice activity projection (VAP) to predict upcoming turn-taking in\ntriadic multi-party scenarios. The goal of VAP models is to predict the future\nvoice activity for each speaker utilizing only acoustic data. This is the first\nstudy to extend VAP into triadic conversation. We trained multiple models on a\nJapanese triadic dataset where participants discussed a variety of topics. We\nfound that the VAP trained on triadic conversation outperformed the baseline\nfor all models but that the type of conversation affected the accuracy. This\nstudy establishes that VAP can be used for turn-taking in triadic dialogue\nscenarios. Future work will incorporate this triadic VAP turn-taking model into\nspoken dialogue systems.", "AI": {"tldr": "本研究首次将VAP技术应用于三人间对话场景以预测语音活动，实验表明此方法优于基线模型，但对话类型影响准确性。", "motivation": "传统的语音对话研究大多集中在二元设置上，本研究旨在通过应用语音活动投影技术来扩展到三人间的对话场景，以预测即将进行的轮流说话。", "method": "本研究首次将语音活动投影（VAP）技术应用于三人间的多角色对话场景中，通过仅使用声学数据来预测未来的说话活动。", "result": "研究结果表明，在三人间对话场景中训练的VAP模型在所有模型上均优于基线模型，但对话类型影响了准确性。", "conclusion": "这项研究证明了VAP技术可用于三人间对话场景中的轮流说话预测，未来的工作将把这种三人间VAP轮流说话模型整合进语音对话系统中。"}}
{"id": "2507.07333", "categories": ["cs.CV", "I.4.9"], "pdf": "https://arxiv.org/pdf/2507.07333", "abs": "https://arxiv.org/abs/2507.07333", "authors": ["Hui Pang", "Sunil Hadap", "Violetta Shevchenko", "Rahul Suresh", "Amin Banitalebi-Dehkordi"], "title": "Scalable and Realistic Virtual Try-on Application for Foundation Makeup with Kubelka-Munk Theory", "comment": "Presented at the workshop Three questions about virtual try-on at\n  CVPR 2025", "summary": "Augmented reality is revolutionizing beauty industry with virtual try-on\n(VTO) applications, which empowers users to try a wide variety of products\nusing their phones without the hassle of physically putting on real products. A\ncritical technical challenge in foundation VTO applications is the accurate\nsynthesis of foundation-skin tone color blending while maintaining the\nscalability of the method across diverse product ranges. In this work, we\npropose a novel method to approximate well-established Kubelka-Munk (KM) theory\nfor faster image synthesis while preserving foundation-skin tone color blending\nrealism. Additionally, we build a scalable end-to-end framework for realistic\nfoundation makeup VTO solely depending on the product information available on\ne-commerce sites. We validate our method using real-world makeup images,\ndemonstrating that our framework outperforms other techniques.", "AI": {"tldr": "研究开发了一种新的方法来提高AR底妆虚拟试用应用的图像合成速度，同时保持颜色融合的真实感，并验证了其有效性。", "motivation": "鉴于AR技术在美容行业中的应用，尤其是在底妆虚拟试用中的应用，本研究旨在解决快速、准确地合成底妆与肤色色调融合的技术难题，同时确保方法的可扩展性。", "method": "提出了一种近似Kubelka-Munk理论的新方法，用于加快图像合成速度，同时保持底妆与肤色的自然融合。此外，我们构建了一个仅依赖电商平台产品信息的实际底妆虚拟试用的端到端框架。", "result": "通过使用真实妆容图像进行了验证，该框架在底妆虚拟试用方面优于其他技术。", "conclusion": "本研究提出的方法和框架，为美妆行业提供了更快速、更真实的底妆虚拟试用解决方案。"}}
{"id": "2507.07539", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07539", "abs": "https://arxiv.org/abs/2507.07539", "authors": ["Akram Elbouanani", "Evan Dufraisse", "Aboubacar Tuo", "Adrian Popescu"], "title": "CEA-LIST at CheckThat! 2025: Evaluating LLMs as Detectors of Bias and Opinion in Text", "comment": "Notebook for the CheckThat! Lab at CLEF 2025", "summary": "This paper presents a competitive approach to multilingual subjectivity\ndetection using large language models (LLMs) with few-shot prompting. We\nparticipated in Task 1: Subjectivity of the CheckThat! 2025 evaluation\ncampaign. We show that LLMs, when paired with carefully designed prompts, can\nmatch or outperform fine-tuned smaller language models (SLMs), particularly in\nnoisy or low-quality data settings. Despite experimenting with advanced prompt\nengineering techniques, such as debating LLMs and various example selection\nstrategies, we found limited benefit beyond well-crafted standard few-shot\nprompts. Our system achieved top rankings across multiple languages in the\nCheckThat! 2025 subjectivity detection task, including first place in Arabic\nand Polish, and top-four finishes in Italian, English, German, and multilingual\ntracks. Notably, our method proved especially robust on the Arabic dataset,\nlikely due to its resilience to annotation inconsistencies. These findings\nhighlight the effectiveness and adaptability of LLM-based few-shot learning for\nmultilingual sentiment tasks, offering a strong alternative to traditional\nfine-tuning, particularly when labeled data is scarce or inconsistent.", "AI": {"tldr": "本研究展示了大型语言模型在少量示例提示的辅助下，能够有效地进行多语言主观性检测，并在CheckThat! 2025评估活动中多种语言任务中取得优异成绩。", "motivation": "动机在于探索在多语言环境下使用少量示例提示的大型语言模型进行主观性检测的竞争力。", "method": "使用大量语言模型（LLMs）和少量示例提示的方法进行多语言主观性检测。参与了CheckThat! 2025评估活动的任务1：主观性评估。通过精心设计的提示，展示了大型语言模型能够匹敌或超越经过微调的小型语言模型（SLMs），特别是在面对嘈杂或质量较低的数据时。在尝试了高级提示工程技术后，如辩论型LLMs和各种示例选择策略，发现并没有显著提升超过精心设计的标准少量示例提示的效果。", "result": "我们的系统在CheckThat! 2025主观性检测任务中多种语言的表现均为顶级，其中包括阿拉伯语和波兰语的第一名，以及意大利语、英语、德语和多语言项目的前四名。在阿拉伯语数据集上，该方法表现出特别的鲁棒性，可能是因为更能适应标注不一致的情况。", "conclusion": "这些发现强调了基于LLM的少量示例学习方法在多语言情感任务中的有效性与适应性，提供了一种强大的替代方案，特别是当标注数据稀缺或不一致时。"}}
{"id": "2507.07340", "categories": ["cs.CV", "I.2; I.4; I.5; I.7"], "pdf": "https://arxiv.org/pdf/2507.07340", "abs": "https://arxiv.org/abs/2507.07340", "authors": ["Daniel A. P. Oliveira", "David Martins de Matos"], "title": "Entity Re-identification in Visual Storytelling via Contrastive Reinforcement Learning", "comment": "7 pages", "summary": "Visual storytelling systems, particularly large vision-language models,\nstruggle to maintain character and object identity across frames,\n  often failing to recognize when entities in different images represent the\nsame individuals or objects,\n  leading to inconsistent references and referential hallucinations.\n  This occurs because models lack explicit training on when to establish entity\nconnections across frames.\n  We propose a contrastive reinforcement learning approach that trains models\nto discriminate between coherent image sequences\n  and stories from unrelated images.\n  We extend the Story Reasoning dataset with synthetic negative examples to\nteach appropriate entity connection behavior.\n  We employ Direct Preference Optimization with a dual-component reward\nfunction that promotes grounding and re-identification of entities\n  in real stories while penalizing incorrect entity connections in synthetic\ncontexts.\n  Using this contrastive framework, we fine-tune Qwen Storyteller (based on\nQwen2.5-VL 7B).\n  Evaluation shows improvements in grounding mAP from 0.27 to 0.31 (+14.8%), F1\nfrom 0.35 to 0.41 (+17.1%).\n  Pronoun grounding accuracy improved across all pronoun types except ``its'',\n  and cross-frame character and object persistence increased\n  across all frame counts, with entities appearing in 5 or more frames\nadvancing from 29.3% to 33.3% (+13.7%).\n  Well-structured stories, containing the chain-of-thought and grounded story,\nincreased from 79.1% to 97.5% (+23.3%).", "AI": {"tldr": "本研究通过对比强化学习方法解决视觉叙事系统中实体根植和重新识别的挑战，提升了代词根植准确性和故事构建的连贯性。", "motivation": "视觉叙事系统，尤其是在大型视觉语言模型中，难以在不同帧间维护角色和对象的身份，这导致了不一致的引用和实体引用幻觉。", "method": "提出了一种对比强化学习方法，该方法训练模型区分连贯的图像序列和故事与不相关的图像，并扩展了Story Reasoning数据集，加入了合成的负面例子来教授适当的实体连接行为。采用直接偏好优化，奖励函数由两个部分组成，一个促进现实故事中实体的根植和重新识别，另一个则在合成上下文中对错误的实体连接进行惩罚。", "result": "通过这种对比框架，微调了Qwen Storyteller（基于Qwen2.5-VL 7B）。评估显示在实体根植mAP从0.27提升到0.31（+14.8%），F1从0.35提升到0.41（+17.1%）。代词根植准确性在所有代词类型中均有提升，除了“its”之外。跨帧角色和对象的持久性在所有帧数中均有增加，特别是在5帧或更多的实体从29.3%提升到33.3%（+13.7%）。结构良好的故事（包括连贯的思想链和根植的故事）从79.1%提升到97.5%（+23.3%）。", "conclusion": "研究结果表明，通过引入合成负面例子和双组件奖励函数来改进训练框架后，模型在实体根植、代词根植和故事构建中的性能显著提升。"}}
{"id": "2507.07543", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.07543", "abs": "https://arxiv.org/abs/2507.07543", "authors": ["Chen Amiraz", "Yaroslav Fyodorov", "Elad Haramaty", "Zohar Karnin", "Liane Lewin-Eytan"], "title": "The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English Corpora", "comment": null, "summary": "Cross-lingual retrieval-augmented generation (RAG) is a critical capability\nfor retrieving and generating answers across languages. Prior work in this\ncontext has mostly focused on generation and relied on benchmarks derived from\nopen-domain sources, most notably Wikipedia. In such settings, retrieval\nchallenges often remain hidden due to language imbalances, overlap with\npretraining data, and memorized content. To address this gap, we study\nArabic-English RAG in a domain-specific setting using benchmarks derived from\nreal-world corporate datasets. Our benchmarks include all combinations of\nlanguages for the user query and the supporting document, drawn independently\nand uniformly at random. This enables a systematic study of multilingual\nretrieval behavior.\n  Our findings reveal that retrieval is a critical bottleneck in cross-lingual\ndomain-specific scenarios, with significant performance drops occurring when\nthe user query and supporting document languages differ. A key insight is that\nthese failures stem primarily from the retriever's difficulty in ranking\ndocuments across languages. Finally, we propose a simple retrieval strategy\nthat addresses this source of failure by enforcing equal retrieval from both\nlanguages, resulting in substantial improvements in cross-lingual and overall\nperformance. These results highlight meaningful opportunities for improving\nmultilingual retrieval, particularly in practical, real-world RAG applications.", "AI": {"tldr": "针对阿拉伯语-英语的跨语言RAG，使用企业数据集进行特定领域的研究发现，检索是跨语言特定场景中的关键瓶颈，提出通过强制从两种语言等比例检索文档来改善性能。", "motivation": "先前的工作主要集中于生成方面，并使用主要来自维基百科的通用领域基准。在这种情况下，由于语言不平衡、与预训练数据的重叠以及内容记忆等因素，检索挑战往往被掩盖。为了解决这一问题，本研究提出了一种新的方法。", "method": "研究采用来自实际企业数据集的基准，在特定领域背景下研究阿拉伯语-英语的跨语言检索增强生成（RAG）。基准涵盖了用户查询和辅助文档语言的所有组合，这些组合独立且均匀随机抽取。这种设置使得可以系统地研究多语言检索行为。", "result": "研究表明，当用户查询与辅助文档语言不同时，跨语言特定领域的检索和生成性能显著下降。研究还提出了通过强制从两种语言中以相等概率检索文档的新策略，以解决这一问题，从而显著提高了跨语言和整体性能。", "conclusion": "这一发现强调了在实际的RAG应用程序中改善多语言检索方面具有重要机会。"}}
{"id": "2507.07374", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07374", "abs": "https://arxiv.org/abs/2507.07374", "authors": ["Haotian Wang", "Aoran Xiao", "Xiaoqin Zhang", "Meng Yang", "Shijian Lu"], "title": "PacGDC: Label-Efficient Generalizable Depth Completion with Projection Ambiguity and Consistency", "comment": "Accepted to ICCV 2025", "summary": "Generalizable depth completion enables the acquisition of dense metric depth\nmaps for unseen environments, offering robust perception capabilities for\nvarious downstream tasks. However, training such models typically requires\nlarge-scale datasets with metric depth labels, which are often labor-intensive\nto collect. This paper presents PacGDC, a label-efficient technique that\nenhances data diversity with minimal annotation effort for generalizable depth\ncompletion. PacGDC builds on novel insights into inherent ambiguities and\nconsistencies in object shapes and positions during 2D-to-3D projection,\nallowing the synthesis of numerous pseudo geometries for the same visual scene.\nThis process greatly broadens available geometries by manipulating scene scales\nof the corresponding depth maps. To leverage this property, we propose a new\ndata synthesis pipeline that uses multiple depth foundation models as scale\nmanipulators. These models robustly provide pseudo depth labels with varied\nscene scales, affecting both local objects and global layouts, while ensuring\nprojection consistency that supports generalization. To further diversify\ngeometries, we incorporate interpolation and relocation strategies, as well as\nunlabeled images, extending the data coverage beyond the individual use of\nfoundation models. Extensive experiments show that PacGDC achieves remarkable\ngeneralizability across multiple benchmarks, excelling in diverse scene\nsemantics/scales and depth sparsity/patterns under both zero-shot and few-shot\nsettings. Code: https://github.com/Wang-xjtu/PacGDC.", "AI": {"tldr": "本文介绍了PacGDC，一种能有效提升数据多样性并减少标注工作的深度补全技术，通过尺度操纵及多种合成方法，实现了在不同场景下的优秀泛化效果。", "motivation": "针对通用深度补全模型通常需要大量带有深度标签的数据集且这些数据集的收集过程费时费力的问题，本研究提出了一种标签高效的技术来增强数据多样性并减少标注工作量。", "method": "通过利用场景中物体形状和位置在2D到3D投影过程中的内在不确定性和一致性，PacGDC能合成同一视觉场景下的多种伪几何数据，大大丰富了可用的几何数据。此外，通过多个深度基础模型进行尺度操纵，结合插值和重新定位策略以及未标注图像，来进一步提高几何多样性。", "result": "实验结果表明，PacGDC在多个基准测试中展现出良好的泛化能力，在不同的场景语义/尺度以及深度稀疏/模式下均表现出色，不论是零样本还是少样本设置。", "conclusion": "PacGDC作为一种标签高效的深度补全技术，通过引入尺度操纵策略以及多种数据合成方法，能在减少标注成本的同时提升模型的泛化性能。"}}
{"id": "2507.07562", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07562", "abs": "https://arxiv.org/abs/2507.07562", "authors": ["Jierun Chen", "Tiezheng Yu", "Haoli Bai", "Lewei Yao", "Jiannan Wu", "Kaican Li", "Fei Mi", "Chaofan Tao", "Lei Zhu", "Manyi Zhang", "Xiaohui Li", "Lu Hou", "Lifeng Shang", "Qun Liu"], "title": "The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs", "comment": null, "summary": "Large vision-language models (VLMs) increasingly adopt post-training\ntechniques such as long chain-of-thought (CoT) supervised fine-tuning (SFT) and\nreinforcement learning (RL) to elicit sophisticated reasoning. While these\nmethods exhibit synergy in language-only models, their joint effectiveness in\nVLMs remains uncertain. We present a systematic investigation into the distinct\nroles and interplay of long-CoT SFT and RL across multiple multimodal reasoning\nbenchmarks. We find that SFT improves performance on difficult questions by\nin-depth, structured reasoning, but introduces verbosity and degrades\nperformance on simpler ones. In contrast, RL promotes generalization and\nbrevity, yielding consistent improvements across all difficulty levels, though\nthe improvements on the hardest questions are less prominent compared to SFT.\nSurprisingly, combining them through two-staged, interleaved, or progressive\ntraining strategies, as well as data mixing and model merging, all fails to\nproduce additive benefits, instead leading to trade-offs in accuracy, reasoning\nstyle, and response length. This ``synergy dilemma'' highlights the need for\nmore seamless and adaptive approaches to unlock the full potential of combined\npost-training techniques for reasoning VLMs.", "AI": {"tldr": "研究了长链式细思监督微调与强化学习在视觉语言模型中的作用及其结合存在的局限性。", "motivation": "尽管这些方法在仅包含语言的模型中展现出协同效果，但它们在视觉语言模型（VLM）中的综合有效性尚不明确。", "method": "通过系统性的研究探讨了长链式细思（CoT）监督微调（SFT）与强化学习（RL）在多种多模态推理基准上的独特作用及其相互作用。", "result": "发现SFT通过深入、结构化的推理提升对难题的回答性能，但带来冗长的回复并降低对简单问题的回答性能。而RL促进了可泛化、简洁的回答风格，但在最难的问题上改进不如SFT明显。结合两种技术并未产生叠加的效果，反而在精度、推理风格和回复长度等方面产生了权衡。", "conclusion": "这种“协同困境”揭示了需要开发更无缝且自适应的方法，以充分利用联合后续训练技术为推理VLM解锁全部潜力。"}}
{"id": "2507.07379", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07379", "abs": "https://arxiv.org/abs/2507.07379", "authors": ["Hong Xu", "Shireen Y. Elhabian"], "title": "Adaptive Particle-Based Shape Modeling for Anatomical Surface Correspondence", "comment": null, "summary": "Particle-based shape modeling (PSM) is a family of approaches that\nautomatically quantifies shape variability across anatomical cohorts by\npositioning particles (pseudo landmarks) on shape surfaces in a consistent\nconfiguration. Recent advances incorporate implicit radial basis function\nrepresentations as self-supervised signals to better capture the complex\ngeometric properties of anatomical structures. However, these methods still\nlack self-adaptivity -- that is, the ability to automatically adjust particle\nconfigurations to local geometric features of each surface, which is essential\nfor accurately representing complex anatomical variability. This paper\nintroduces two mechanisms to increase surface adaptivity while maintaining\nconsistent particle configurations: (1) a novel neighborhood correspondence\nloss to enable high adaptivity and (2) a geodesic correspondence algorithm that\nregularizes optimization to enforce geodesic neighborhood consistency. We\nevaluate the efficacy and scalability of our approach on challenging datasets,\nproviding a detailed analysis of the adaptivity-correspondence trade-off and\nbenchmarking against existing methods on surface representation accuracy and\ncorrespondence metrics.", "AI": {"tldr": "本文提出了两种机制以增强粒子配置对表面的适应性，并在保持一致性的同时提升局部几何特征的适应能力。", "motivation": "现有的基于粒子的形状建模方法虽然在捕捉解剖结构的复杂几何特征上取得了进展，但仍缺乏自适应性，即自动调整粒子配置以适应每个表面局部几何特征的能力，这对准确表示复杂的解剖变异至关重要。", "method": "引入了两种机制来增强表面适应性同时保持粒子配置的一致性：(1) 一种新颖的邻域对应损失，以实现高适应性；(2) 一种测地线对应算法，用于在优化过程中强制执行测地线邻域一致性。", "result": "在具有挑战性的数据集上评估了所提方法的效果和可扩展性，详细分析了适应性-对应权衡，并与现有方法在表面表示准确性和对应度量上进行了基准测试。", "conclusion": "本文的方法对复杂的表面表示准确性以及对应度量进行了基准测试，展示了其在适应性-对应权衡中的出色表现。"}}
{"id": "2507.07572", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07572", "abs": "https://arxiv.org/abs/2507.07572", "authors": ["Yupu Liang", "Yaping Zhang", "Zhiyang Zhang", "Yang Zhao", "Lu Xiang", "Chengqing Zong", "Yu Zhou"], "title": "Single-to-mix Modality Alignment with Multimodal Large Language Model for Document Image Machine Translation", "comment": "Accepted by ACL 2025 Main", "summary": "Document Image Machine Translation (DIMT) aims to translate text within\ndocument images, facing generalization challenges due to limited training data\nand the complex interplay between visual and textual information. To address\nthese challenges, we introduce M4Doc, a novel single-to-mix modality alignment\nframework leveraging Multimodal Large Language Models (MLLMs). M4Doc aligns an\nimage-only encoder with the multimodal representations of an MLLM, pre-trained\non large-scale document image datasets. This alignment enables a lightweight\nDIMT model to learn crucial visual-textual correlations during training. During\ninference, M4Doc bypasses the MLLM, maintaining computational efficiency while\nbenefiting from its multimodal knowledge. Comprehensive experiments demonstrate\nsubstantial improvements in translation quality, especially in cross-domain\ngeneralization and challenging document image scenarios.", "AI": {"tldr": "M4Doc, a novel DIMT framework, uses alignment with MLLMs to achieve better translation quality and cross-domain generalization.", "motivation": "To improve the generalization of DIMT models with limited training data and to balance the complex interaction between visual and textual elements.", "method": "M4Doc aligns an image-only encoder with the multimodal representations of an MLLM pre-trained on large-scale document image datasets to learn visual-textual correlations.", "result": "Experiments show significant improvements in translation quality and robustness, particularly in cross-domain generalization and handling difficult document images.", "conclusion": "The M4Doc approach facilitates a lightweight model to achieve high translation quality while maintaining computational efficiency."}}
{"id": "2507.07381", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07381", "abs": "https://arxiv.org/abs/2507.07381", "authors": ["Hao Xu", "Arbind Agrahari Baniya", "Sam Wells", "Mohamed Reda Bouadjenek", "Richard Dazeley", "Sunil Aryal"], "title": "Multi-Scale Attention and Gated Shifting for Fine-Grained Event Spotting in Videos", "comment": null, "summary": "Precise Event Spotting (PES) in sports videos requires frame-level\nrecognition of fine-grained actions from single-camera footage. Existing PES\nmodels typically incorporate lightweight temporal modules such as Gate Shift\nModule (GSM) or Gate Shift Fuse (GSF) to enrich 2D CNN feature extractors with\ntemporal context. However, these modules are limited in both temporal receptive\nfield and spatial adaptability. We propose a Multi-Scale Attention Gate Shift\nModule (MSAGSM) that enhances GSM with multi-scale temporal dilations and\nmulti-head spatial attention, enabling efficient modeling of both short- and\nlong-term dependencies while focusing on salient regions. MSAGSM is a\nlightweight plug-and-play module that can be easily integrated with various 2D\nbackbones. To further advance the field, we introduce the Table Tennis\nAustralia (TTA) dataset-the first PES benchmark for table tennis-containing\nover 4800 precisely annotated events. Extensive experiments across five PES\nbenchmarks demonstrate that MSAGSM consistently improves performance with\nminimal overhead, setting new state-of-the-art results.", "AI": {"tldr": "本文提出了一种多尺度注意力门移模块（MSAGSM）用于增强运动视频中细粒度动作识别的精确事件捕捉，引入了首个乒乓球精确事件捕捉基准（Table Tennis Australia，TTA数据集），在多个基准测试中表现优异，提高了性能并降低了计算成本。", "motivation": "现有的精确事件捕捉（PES）模型在处理运动视频中的细粒度动作识别时，使用轻量级时间模块（例如GSM或GSF）来增强2D卷积神经网络特征提取器。然而，这些模块在时间接收域和空间适应性方面有限。为了解决这些问题，我们提出了MSAGSM。", "method": "我们提出了一种多尺度注意力门移模块（MSAGSM），该模块通过引入多尺度时间膨胀和多头空间注意力来增强现有门移模块，以更有效地建模短期和长期的时间依赖性并聚焦于显著区域。MSAGSM是一个轻量级的可插拔模块，可以与各种2D骨干网络轻松集成。", "result": "我们引入了首个乒乓球精确事件捕捉基准表（Table Tennis Australia，TTA），该数据集包含超过4800个精细标注的事件。在五个PES基准上的广泛实验表明，MSAGSM在性能提升上表现一致，并且只需极小的计算成本，刷新了最新的技术水平。", "conclusion": "MSAGSM通过多尺度时间膨胀和空间注意力机制提高了时间依赖性的建模效率，与多种2D骨干网络兼容，展示了其在改善PES性能上的应用潜力和优势。"}}
{"id": "2507.07586", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07586", "abs": "https://arxiv.org/abs/2507.07586", "authors": ["Cooper Doyle"], "title": "Bayesian Discrete Diffusion Beats Autoregressive Perplexity", "comment": "12 pages, 2 figures, 2 tables", "summary": "We reveal a hidden Bayesian core of discrete-diffusion language models by\nshowing that the expected denoiser output under the forward masking\ndistribution recovers the exact posterior over clean tokens. Under minimal\nassumptions, Monte Carlo marginalization over K independent corruptions\nconverges to this posterior at rate O(1/sqrt(K)), yielding a simple proof of\nconsistency and finite-sample error bounds. Building on this insight, we\nintroduce a lightweight inference-time ensemble that averages K\nmask-and-denoise passes to obtain posterior-aware token probabilities and\nuncertainty estimates at no extra training cost. On WikiText-2, our method\nachieves test perplexity 8.8 with K=8, versus 20.3 for GPT-2 Small, despite\nusing a model of comparable size. Code is available at\nhttps://github.com/mercury0100/bayesradd.", "AI": {"tldr": "The paper uncovers a Bayesian aspect of discrete-diffusion language models and uses this to develop a lightweight ensemble method for better token probabilities and uncertainty estimates.", "motivation": "The motivation is to improve the performance of language models by revealing their Bayesian core and using this insight to enhance inference-time processing.", "method": "We introduce a lightweight inference-time ensemble method that averages K mask-and-denoise passes to obtain posterior-aware token probabilities and uncertainty estimates, leveraging the hidden Bayesian core of discrete-diffusion language models.", "result": "The method achieves a test perplexity of 8.8 with K=8 on WikiText-2, which is a significant improvement over GPT-2 Small, despite using a model of comparable size.", "conclusion": "The paper demonstrates that by leveraging a Bayesian perspective, one can improve the performance of discrete-diffusion language models with minimal additional computational cost."}}
{"id": "2507.07393", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07393", "abs": "https://arxiv.org/abs/2507.07393", "authors": ["Jinseong Kim", "Junghoon Song", "Gyeongseon Baek", "Byeongjoon Noh"], "title": "KeyRe-ID: Keypoint-Guided Person Re-Identification using Part-Aware Representation in Videos", "comment": "10 pages, 2 figures,", "summary": "We propose \\textbf{KeyRe-ID}, a keypoint-guided video-based person\nre-identification framework consisting of global and local branches that\nleverage human keypoints for enhanced spatiotemporal representation learning.\nThe global branch captures holistic identity semantics through\nTransformer-based temporal aggregation, while the local branch dynamically\nsegments body regions based on keypoints to generate fine-grained, part-aware\nfeatures. Extensive experiments on MARS and iLIDS-VID benchmarks demonstrate\nstate-of-the-art performance, achieving 91.73\\% mAP and 97.32\\% Rank-1 accuracy\non MARS, and 96.00\\% Rank-1 and 100.0\\% Rank-5 accuracy on iLIDS-VID. The code\nfor this work will be publicly available on GitHub upon publication.", "AI": {"tldr": "提出了KeyRe-ID，一个基于关键点的视频人物重识别框架，在多个基准测试中显示了优异性能。", "motivation": "视频人物重识别中，现有方法在处理复杂的遮挡、视角变化和光照条件下表现不佳，因此提出了结合关键点和Transformer技术的框架以改善时空表示学习。", "method": "我们提出了KeyRe-ID，一个基于关键点引导的视频人物重识别框架，包括全局分支和局部分支，利用人体关键点进行增强的时空表示学习。全局分支通过基于Transformer的时间聚合来捕捉整体身份语义，而局部分支则基于关键点动态分割身体区域，生成细粒度、部分感知的特征。", "result": "在MARS和iLIDS-VID基准测试上的广泛实验展示了该方法的先进性能。在MARS上达到了91.73% mAP和97.32% Rank-1准确率，在iLIDS-VID上达到了96.00% Rank-1和100.0% Rank-5准确率。", "conclusion": "实验显示了KeyRe-ID框架在处理视频人物重识别任务中的有效性，尤其是通过关键点和Transformer来增强时空特征。该方法实现了在MARS和iLIDS-VID基准上的最优性能。"}}
{"id": "2507.07630", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07630", "abs": "https://arxiv.org/abs/2507.07630", "authors": ["Joyeeta Datta", "Niclas Doll", "Qusai Ramadan", "Zeyd Boukhers"], "title": "Exploring the Limits of Model Compression in LLMs: A Knowledge Distillation Study on QA Tasks", "comment": "Accepted four publication at the 26th Meeting of the Special Interest\n  on Discourse and Dialogue", "summary": "Large Language Models (LLMs) have demonstrated outstanding performance across\na range of NLP tasks, however, their computational demands hinder their\ndeployment in real-world, resource-constrained environments. This work\ninvestigates the extent to which LLMs can be compressed using Knowledge\nDistillation (KD) while maintaining strong performance on Question Answering\n(QA) tasks. We evaluate student models distilled from the Pythia and Qwen2.5\nfamilies on two QA benchmarks, SQuAD and MLQA, under zero-shot and one-shot\nprompting conditions. Results show that student models retain over 90% of their\nteacher models' performance while reducing parameter counts by up to 57.1%.\nFurthermore, one-shot prompting yields additional performance gains over\nzero-shot setups for both model families. These findings underscore the\ntrade-off between model efficiency and task performance, demonstrating that KD,\ncombined with minimal prompting, can yield compact yet capable QA systems\nsuitable for resource-constrained applications.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.07394", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07394", "abs": "https://arxiv.org/abs/2507.07394", "authors": ["Zhimin Zhang", "Bi'an Du", "Caoyuan Ma", "Zheng Wang", "Wei Hu"], "title": "Behave Your Motion: Habit-preserved Cross-category Animal Motion Transfer", "comment": null, "summary": "Animal motion embodies species-specific behavioral habits, making the\ntransfer of motion across categories a critical yet complex task for\napplications in animation and virtual reality. Existing motion transfer\nmethods, primarily focused on human motion, emphasize skeletal alignment\n(motion retargeting) or stylistic consistency (motion style transfer), often\nneglecting the preservation of distinct habitual behaviors in animals. To\nbridge this gap, we propose a novel habit-preserved motion transfer framework\nfor cross-category animal motion. Built upon a generative framework, our model\nintroduces a habit-preservation module with category-specific habit encoder,\nallowing it to learn motion priors that capture distinctive habitual\ncharacteristics. Furthermore, we integrate a large language model (LLM) to\nfacilitate the motion transfer to previously unobserved species. To evaluate\nthe effectiveness of our approach, we introduce the DeformingThings4D-skl\ndataset, a quadruped dataset with skeletal bindings, and conduct extensive\nexperiments and quantitative analyses, which validate the superiority of our\nproposed model.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.07634", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07634", "abs": "https://arxiv.org/abs/2507.07634", "authors": ["Abhinav Java", "Srivathsan Koundinyan", "Nagarajan Natarajan", "Amit Sharma"], "title": "FrugalRAG: Learning to retrieve and reason for multi-hop QA", "comment": "Accepted at ICML Workshop: Efficient Systems for Foundation Models", "summary": "We consider the problem of answering complex questions, given access to a\nlarge unstructured document corpus. The de facto approach to solving the\nproblem is to leverage language models that (iteratively) retrieve and reason\nthrough the retrieved documents, until the model has sufficient information to\ngenerate an answer. Attempts at improving this approach focus on\nretrieval-augmented generation (RAG) metrics such as accuracy and recall and\ncan be categorized into two types: (a) fine-tuning on large question answering\n(QA) datasets augmented with chain-of-thought traces, and (b) leveraging\nRL-based fine-tuning techniques that rely on question-document relevance\nsignals. However, efficiency in the number of retrieval searches is an equally\nimportant metric, which has received less attention. In this work, we show\nthat: (1) Large-scale fine-tuning is not needed to improve RAG metrics,\ncontrary to popular claims in recent literature. Specifically, a standard ReAct\npipeline with improved prompts can outperform state-of-the-art methods on\nbenchmarks such as HotPotQA. (2) Supervised and RL-based fine-tuning can help\nRAG from the perspective of frugality, i.e., the latency due to number of\nsearches at inference time. For example, we show that we can achieve\ncompetitive RAG metrics at nearly half the cost (in terms of number of\nsearches) on popular RAG benchmarks, using the same base model, and at a small\ntraining cost (1000 examples).", "AI": {"tldr": "The study suggests that significant enhancement in RAG metrics can be achieved with a minimal amount of additional training data and improved prompts, challenging the notion that large-scale fine-tuning is necessary.", "motivation": "The goal is to find a more efficient way of improving RAG metrics such as accuracy while reducing the number of retrieval searches required for a satisfactory answer.", "method": "We explore the efficiency of retrieval-augmented generation (RAG) for answering complex questions by comparing the effects of large-scale fine-tuning and improved prompting techniques. This is done without relying on extensive retraining as other works suggest, using benchmarks like HotPotQA to assess performance.", "result": "The research demonstrates that a standard ReAct pipeline with enhanced prompts can match or surpass state-of-the-art RAG performance, with fewer retrieval searches and a reduced training dataset.", "conclusion": "Efficiency in RAG can be improved significantly by focusing on prompting techniques rather than solely on large-scale training, thus suggesting alternative paths for future development in question answering systems."}}
{"id": "2507.07395", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07395", "abs": "https://arxiv.org/abs/2507.07395", "authors": ["Yongtang Bao", "Chengjie Tang", "Yuze Wang", "Haojie Li"], "title": "Seg-Wild: Interactive Segmentation based on 3D Gaussian Splatting for Unconstrained Image Collections", "comment": null, "summary": "Reconstructing and segmenting scenes from unconstrained photo collections\nobtained from the Internet is a novel but challenging task. Unconstrained photo\ncollections are easier to get than well-captured photo collections. These\nunconstrained images suffer from inconsistent lighting and transient\nocclusions, which makes segmentation challenging. Previous segmentation methods\ncannot address transient occlusions or accurately restore the scene's lighting\nconditions. Therefore, we propose Seg-Wild, an interactive segmentation method\nbased on 3D Gaussian Splatting for unconstrained image collections, suitable\nfor in-the-wild scenes. We integrate multi-dimensional feature embeddings for\neach 3D Gaussian and calculate the feature similarity between the feature\nembeddings and the segmentation target to achieve interactive segmentation in\nthe 3D scene. Additionally, we introduce the Spiky 3D Gaussian Cutter (SGC) to\nsmooth abnormal 3D Gaussians. We project the 3D Gaussians onto a 2D plane and\ncalculate the ratio of 3D Gaussians that need to be cut using the SAM mask. We\nalso designed a benchmark to evaluate segmentation quality in in-the-wild\nscenes. Experimental results demonstrate that compared to previous methods,\nSeg-Wild achieves better segmentation results and reconstruction quality. Our\ncode will be available at https://github.com/Sugar0725/Seg-Wild.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.07640", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07640", "abs": "https://arxiv.org/abs/2507.07640", "authors": ["Haotan Guo", "Jianfei He", "Jiayuan Ma", "Hongbin Na", "Zimu Wang", "Haiyang Zhang", "Qi Chen", "Wei Wang", "Zijing Shi", "Tao Shen", "Ling Chen"], "title": "Lost in Pronunciation: Detecting Chinese Offensive Language Disguised by Phonetic Cloaking Replacement", "comment": "In progress", "summary": "Phonetic Cloaking Replacement (PCR), defined as the deliberate use of\nhomophonic or near-homophonic variants to hide toxic intent, has become a major\nobstacle to Chinese content moderation. While this problem is well-recognized,\nexisting evaluations predominantly rely on rule-based, synthetic perturbations\nthat ignore the creativity of real users. We organize PCR into a four-way\nsurface-form taxonomy and compile \\ours, a dataset of 500 naturally occurring,\nphonetically cloaked offensive posts gathered from the RedNote platform.\nBenchmarking state-of-the-art LLMs on this dataset exposes a serious weakness:\nthe best model reaches only an F1-score of 0.672, and zero-shot\nchain-of-thought prompting pushes performance even lower. Guided by error\nanalysis, we revisit a Pinyin-based prompting strategy that earlier studies\njudged ineffective and show that it recovers much of the lost accuracy. This\nstudy offers the first comprehensive taxonomy of Chinese PCR, a realistic\nbenchmark that reveals current detectors' limits, and a lightweight mitigation\ntechnique that advances research on robust toxicity detection.", "AI": {"tldr": "研究定义并分类了中文中的音韵伪装替换（PCR）现象，使用一个自然产生的数据集评估了现有模型的表现，发现它们存在严重不足，并提出了一种基于拼音的提示策略来提高检测效果。", "motivation": "研究者观察到，现有的评测大多依赖于基于规则的、合成扰动，忽视了真实的用户创造力，这成为了中文内容审核的主要障碍。本研究旨在解决这一问题，并提出了一个对现有检测器限制的现实基准。", "method": "研究者们通过创建一个四分类的表面形式分类法，并收集了一个包含500个自然发生的音韵伪装攻击帖子的数据集来定义音韵伪装替换（PCR）现象。他们还评估了最先进的语言模型对此数据集的性能，并探讨了一种基于拼音的提示策略。", "result": "研究表明，即使是最好的语言模型在检测这种攻击时的F1分数也只有0.672，而零样本链式思考提示的效果更差。研究发现，基于拼音的提示策略可以显著提高检测准确性。", "conclusion": "该研究提供了中文PCR的首个全面分类法、一个揭示现有检测器限制的现实基准，以及一种轻量级的缓解技术，从而推动了对稳健毒性检测的研究。"}}
{"id": "2507.07410", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07410", "abs": "https://arxiv.org/abs/2507.07410", "authors": ["Xinan Zhang", "Muhammad Zubair Irshad", "Anthony Yezzi", "Yi-Chang Tsai", "Zsolt Kira"], "title": "EscherNet++: Simultaneous Amodal Completion and Scalable View Synthesis through Masked Fine-Tuning and Enhanced Feed-Forward 3D Reconstruction", "comment": null, "summary": "We propose EscherNet++, a masked fine-tuned diffusion model that can\nsynthesize novel views of objects in a zero-shot manner with amodal completion\nability. Existing approaches utilize multiple stages and complex pipelines to\nfirst hallucinate missing parts of the image and then perform novel view\nsynthesis, which fail to consider cross-view dependencies and require redundant\nstorage and computing for separate stages. Instead, we apply masked fine-tuning\nincluding input-level and feature-level masking to enable an end-to-end model\nwith the improved ability to synthesize novel views and conduct amodal\ncompletion. In addition, we empirically integrate our model with other\nfeed-forward image-to-mesh models without extra training and achieve\ncompetitive results with reconstruction time decreased by 95%, thanks to its\nability to synthesize arbitrary query views. Our method's scalable nature\nfurther enhances fast 3D reconstruction. Despite fine-tuning on a smaller\ndataset and batch size, our method achieves state-of-the-art results, improving\nPSNR by 3.9 and Volume IoU by 0.28 on occluded tasks in 10-input settings,\nwhile also generalizing to real-world occluded reconstruction.", "AI": {"tldr": "本文提出了一种名为EscherNet++的新模型，该模型能够在零样本方式下合成新颖视角的物体。", "motivation": "已有方法通过多个阶段和复杂管道来先排除图像的缺失部分，然后进行新颖视角合成，忽视了跨视角依赖问题，且需要额外的存储和计算。", "method": "该研究采用了带mask的微调方法，包括输入级和特征级mask，使模型具备端到端合成新颖视角和进行无模态补全的能力。", "result": "该方法在遮挡任务中，PSNR提升了3.9，Volume IoU提升了0.28，在10输入设置下达到最先进的结果，并且能够泛化到现实世界的遮挡重建。", "conclusion": "EscherNet++展示了在零样本方式下合成新颖视角物体的强大能力，并且通过减少95%的重建时间提高了效率。"}}
{"id": "2507.07653", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07653", "abs": "https://arxiv.org/abs/2507.07653", "authors": ["Andrew D. Foland"], "title": "An Automated Length-Aware Quality Metric for Summarization", "comment": null, "summary": "This paper proposes NOrmed Index of Retention (NOIR), a quantitative\nobjective metric for evaluating summarization quality of arbitrary texts that\nrelies on both the retention of semantic meaning and the summary length\ncompression. This gives a measure of how well the recall-compression tradeoff\nis managed, the most important skill in summarization. Experiments demonstrate\nthat NOIR effectively captures the token-length / semantic retention tradeoff\nof a summarizer and correlates to human perception of sumarization quality.\nUsing a language model-embedding to measure semantic similarity, it provides an\nautomated alternative for assessing summarization quality without relying on\ntime-consuming human-generated reference summaries. The proposed metric can be\napplied to various summarization tasks, offering an automated tool for\nevaluating and improving summarization algorithms, summarization prompts, and\nsynthetically-generated summaries.", "AI": {"tldr": "提出NOIR指标，基于语义保留和长度压缩来评估摘要质量，与人工感知相关，可应用于各种摘要任务。", "motivation": "旨在提供一种不需要依赖耗时的人工参考摘要而自动评估摘要质量的方法，适应各种摘要任务。", "method": "提出了一种名为NOrmed Index of Retention (NOIR)的指标，该指标基于语义意义的保持和摘要长度的压缩来定量评估任意文本的摘要质量。", "result": "实验表明NOIR能够有效捕捉生成器的token长度/语义保留权衡，并且与人类对摘要质量的感知相关。", "conclusion": "NOIR可以作为一种自动工具，用于评估并改进摘要算法、摘要提示和人工生成的摘要。"}}
{"id": "2507.07415", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07415", "abs": "https://arxiv.org/abs/2507.07415", "authors": ["Xinyao Yu", "Hao Sun", "Zeyu Ling", "Ziwei Niu", "Zhenjia Bai", "Rui Qin", "Yen-Wei Chen", "Lanfen Lin"], "title": "EPIC: Efficient Prompt Interaction for Text-Image Classification", "comment": "arXiv admin note: substantial text overlap with arXiv:2401.14856", "summary": "In recent years, large-scale pre-trained multimodal models (LMMs) generally\nemerge to integrate the vision and language modalities, achieving considerable\nsuccess in multimodal tasks, such as text-image classification. The growing\nsize of LMMs, however, results in a significant computational cost for\nfine-tuning these models for downstream tasks. Hence, prompt-based interaction\nstrategy is studied to align modalities more efficiently. In this context, we\npropose a novel efficient prompt-based multimodal interaction strategy, namely\nEfficient Prompt Interaction for text-image Classification (EPIC).\nSpecifically, we utilize temporal prompts on intermediate layers, and integrate\ndifferent modalities with similarity-based prompt interaction, to leverage\nsufficient information exchange between modalities. Utilizing this approach,\nour method achieves reduced computational resource consumption and fewer\ntrainable parameters (about 1\\% of the foundation model) compared to other\nfine-tuning strategies. Furthermore, it demonstrates superior performance on\nthe UPMC-Food101 and SNLI-VE datasets, while achieving comparable performance\non the MM-IMDB dataset.", "AI": {"tldr": "提出了一种新的高效提示方式，用于文本图像分类任务，与其它微调策略相比，该方法计算资源消耗更少，参数量更少，同时在某些数据集上表现更优。", "motivation": "大型预训练多模态模型在多模态任务上取得了显著成功，但其量级的增长导致了微调这些模型的计算成本增加。因此，研究了基于提示的交互策略，以更高效地对齐模态。", "method": "提出了名为Efficient Prompt Interaction for text-image Classification（EPIC）的新型高效提示方式。通过在中间层利用时间提示，集成基于相似度的提示交互来优化不同模态的信息交换，减少计算资源消耗和可训练参数。", "result": "相较于其它微调策略，该方法减少了计算资源消费和可训练参数，并在UPMC-Food101和SNLI-VE数据集上表现出色，同时在MM-IMDB数据集上表现出相当的性能。", "conclusion": "该方法通过集成基于相似度的提示交互，有效减少了计算资源消耗和参数量，同时改善了在特定数据集上的性能表现，为解决大型预训练多模态模型在微调时的计算成本提供了有效方案。"}}
{"id": "2507.07694", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07694", "abs": "https://arxiv.org/abs/2507.07694", "authors": ["Chuanyang Zheng", "Jiankai Sun", "Yihang Gao", "Yuehao Wang", "Peihao Wang", "Jing Xiong", "Liliang Ren", "Hao Cheng", "Janardhan Kulkarni", "Yelong Shen", "Atlas Wang", "Mac Schwager", "Anderson Schneider", "Xiaodong Liu", "Jianfeng Gao"], "title": "SAS: Simulated Attention Score", "comment": "Tech Report", "summary": "The attention mechanism is a core component of the Transformer architecture.\nVarious methods have been developed to compute attention scores, including\nmulti-head attention (MHA), multi-query attention, group-query attention and so\non. We further analyze the MHA and observe that its performance improves as the\nnumber of attention heads increases, provided the hidden size per head remains\nsufficiently large. Therefore, increasing both the head count and hidden size\nper head with minimal parameter overhead can lead to significant performance\ngains at a low cost. Motivated by this insight, we introduce Simulated\nAttention Score (SAS), which maintains a compact model size while simulating a\nlarger number of attention heads and hidden feature dimension per head. This is\nachieved by projecting a low-dimensional head representation into a\nhigher-dimensional space, effectively increasing attention capacity without\nincreasing parameter count. Beyond the head representations, we further extend\nthe simulation approach to feature dimension of the key and query embeddings,\nenhancing expressiveness by mimicking the behavior of a larger model while\npreserving the original model size. To control the parameter cost, we also\npropose Parameter-Efficient Attention Aggregation (PEAA). Comprehensive\nexperiments on a variety of datasets and tasks demonstrate the effectiveness of\nthe proposed SAS method, achieving significant improvements over different\nattention variants.", "AI": {"tldr": "论文提出了一种新颖的注意力机制方法SAS，能够在不显著增加参数数量的情况下模拟更多的注意力头和增加隐藏特征维度，实验表明该方法在不同的数据集和任务上都获得了显著的性能提升。", "motivation": "通过观察多头注意力机制（MHA）发现，随着注意力头数量的增加，其性能得到提升，前提是每个头的隐藏大小足够大。因此，增加头数量和隐藏大小可以带来显著性能提升而不增加模型参数数量。基于此，提出了SAS方法。", "method": "文中提出了一种名为模拟注意分数（SAS）的方法，该方法通过将低维头表示投影到高维空间来保持紧凑的模型大小同时模拟更多的注意头和每个头的隐藏特征维度。此外，该方法还扩展到键和查询嵌入的特征维度模拟，以增强表达能力。为了控制参数成本，文中还提出了参数高效注意力聚合（PEAA）的方法。", "result": "在各种数据集和任务上的综合实验表明，提出的SAS方法在不同的注意力变体中实现了显著的改进。", "conclusion": "通过对多头注意力机制的研究，提出了一种模拟注意分数（SAS）的方法，该方法在保持模型大小不变的情况下，增强了模型的注意力能力和表达能力，从而在不增加模型参数数量的前提下实现了性能的提升。"}}
{"id": "2507.07424", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07424", "abs": "https://arxiv.org/abs/2507.07424", "authors": ["Jingjing Jiang", "Chao Ma", "Xurui Song", "Hanwang Zhang", "Jun Luo"], "title": "Corvid: Improving Multimodal Large Language Models Towards Chain-of-Thought Reasoning", "comment": "ICCV 2025", "summary": "Recent advancements in multimodal large language models (MLLMs) have\ndemonstrated exceptional performance in multimodal perception and\nunderstanding. However, leading open-source MLLMs exhibit significant\nlimitations in complex and structured reasoning, particularly in tasks\nrequiring deep reasoning for decision-making and problem-solving. In this work,\nwe present Corvid, an MLLM with enhanced chain-of-thought (CoT) reasoning\ncapabilities. Architecturally, Corvid incorporates a hybrid vision encoder for\ninformative visual representation and a meticulously designed connector\n(GateMixer) to facilitate cross-modal alignment. To enhance Corvid's CoT\nreasoning capabilities, we introduce MCoT-Instruct-287K, a high-quality\nmultimodal CoT instruction-following dataset, refined and standardized from\ndiverse public reasoning sources. Leveraging this dataset, we fine-tune Corvid\nwith a two-stage CoT-formatted training approach to progressively enhance its\nstep-by-step reasoning abilities. Furthermore, we propose an effective\ninference-time scaling strategy that enables Corvid to mitigate over-reasoning\nand under-reasoning through self-verification. Extensive experiments\ndemonstrate that Corvid outperforms existing o1-like MLLMs and state-of-the-art\nMLLMs with similar parameter scales, with notable strengths in mathematical\nreasoning and science problem-solving. Project page:\nhttps://mm-vl.github.io/corvid.", "AI": {"tldr": "本文介绍了Corvid，一种具备增强链式推理能力的多模态大型语言模型。实验结果表明其在数学推理和科学问题解决上表现优于同类模型。", "motivation": "多模态大型语言模型在多模态感知和理解方面表现出色，但在复杂和结构化推理任务上存在明显局限性，特别是在需要深入推理的决策和问题解决任务中。本研究旨在解决这一问题，提高模型的推理能力。", "method": "Corvid采用混合视觉编码器和精心设计的跨模态对齐连接器GateMixer来提高链式思考（CoT）推理能力。通过使用MCoT-Instruct-287K这一高质量的多模态CoT指令跟随数据集，并采用两阶段的CoT格式训练方法进行微调，逐步提升了其逐步推理能力。此外，Corvid还采用了一个有效的推理时间扩展策略来解决过度推理和推理不足的问题。", "result": "实验表明，Corvid在数学推理和科学问题解决方面表现出色，超过了现有参数规模相似的多模态大型语言模型。", "conclusion": "Corvid通过改进架构设计和训练策略，在提高模型推理能力方面取得了显著的进展，特别是在数学推理和科学问题解决任务上具有显著优势。"}}
{"id": "2507.07695", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07695", "abs": "https://arxiv.org/abs/2507.07695", "authors": ["Hruday Markondapatnaikuni", "Basem Suleiman", "Abdelkarim Erradi", "Shijing Chen"], "title": "KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM question-answering capabilities", "comment": "21 pages, 14 figures", "summary": "Fine-tuning is an immensely resource-intensive process when retraining Large\nLanguage Models (LLMs) to incorporate a larger body of knowledge. Although many\nfine-tuning techniques have been developed to reduce the time and computational\ncost involved, the challenge persists as LLMs continue to grow in size and\ncomplexity. To address this, a new approach to knowledge expansion in LLMs is\nneeded. Retrieval-Augmented Generation (RAG) offers one such alternative by\nstoring external knowledge in a database and retrieving relevant chunks to\nsupport question answering. However, naive implementations of RAG face\nsignificant limitations in scalability and answer accuracy. This paper\nintroduces KeyKnowledgeRAG (K2RAG), a novel framework designed to overcome\nthese limitations. Inspired by the divide-and-conquer paradigm, K2RAG\nintegrates dense and sparse vector search, knowledge graphs, and text\nsummarization to improve retrieval quality and system efficiency. The framework\nalso includes a preprocessing step that summarizes the training data,\nsignificantly reducing the training time. K2RAG was evaluated using the\nMultiHopRAG dataset, where the proposed pipeline was trained on the document\ncorpus and tested on a separate evaluation set. Results demonstrated notable\nimprovements over common naive RAG implementations. K2RAG achieved the highest\nmean answer similarity score of 0.57, and reached the highest third quartile\n(Q3) similarity of 0.82, indicating better alignment with ground-truth answers.\nIn addition to improved accuracy, the framework proved highly efficient. The\nsummarization step reduced the average training time of individual components\nby 93%, and execution speed was up to 40% faster than traditional knowledge\ngraph-based RAG systems. K2RAG also demonstrated superior scalability,\nrequiring three times less VRAM than several naive RAG implementations tested\nin this study.", "AI": {"tldr": "K2RAG是一种新型框架，克服了RAG的局限性，提高检索质量和系统效率，同时提高了精度和效率，并表现出更好的可扩展性。", "motivation": "尽管已经开发了多种减少训练时间的方法，但随着LLMs的规模和复杂性增加，它们在重新训练时仍然非常耗费资源。因此，需要一种新的知识扩展方法。", "method": "K2RAG整合了稠密向量搜索、稀疏向量搜索、知识图谱和文本总结，以提高检索质量和系统效率，包括训练数据的预处理步骤，以显著减少训练时间。", "result": "K2RAG在MultiHopRAG数据集上的表现优于常见的RAG实现，达到最高的答案相似度平均值0.57和第三分位数0.82。", "conclusion": "K2RAG不仅在答案准确性和执行速度上有所改进，同时显著减少了所需的VRAM，提高了系统效率和数据处理能力。"}}
{"id": "2507.07435", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07435", "abs": "https://arxiv.org/abs/2507.07435", "authors": ["Yuqi Cheng", "Yihan Sun", "Hui Zhang", "Weiming Shen", "Yunkang Cao"], "title": "Towards High-Resolution 3D Anomaly Detection: A Scalable Dataset and Real-Time Framework for Subtle Industrial Defects", "comment": "14 pages, 8figures", "summary": "In industrial point cloud analysis, detecting subtle anomalies demands\nhigh-resolution spatial data, yet prevailing benchmarks emphasize\nlow-resolution inputs. To address this disparity, we propose a scalable\npipeline for generating realistic and subtle 3D anomalies. Employing this\npipeline, we developed MiniShift, the inaugural high-resolution 3D anomaly\ndetection dataset, encompassing 2,577 point clouds, each with 500,000 points\nand anomalies occupying less than 1\\% of the total. We further introduce\nSimple3D, an efficient framework integrating Multi-scale Neighborhood\nDescriptors (MSND) and Local Feature Spatial Aggregation (LFSA) to capture\nintricate geometric details with minimal computational overhead, achieving\nreal-time inference exceeding 20 fps. Extensive evaluations on MiniShift and\nestablished benchmarks demonstrate that Simple3D surpasses state-of-the-art\nmethods in both accuracy and speed, highlighting the pivotal role of\nhigh-resolution data and effective feature aggregation in advancing practical\n3D anomaly detection.", "AI": {"tldr": "本文为工业点云分析中检测细微异常引入了高分辨率的3D异常检测数据集MiniShift，并提出了一个高效的检测框架Simple3D，该框架在速度和精度上超越了现有方法。", "motivation": "当前的基准数据集主要用于低分辨率的输入，而这并不适用于需要高分辨率数据的工业点云异常检测。因此，需要一种新的高分辨率数据集和检测框架来解决这个问题。", "method": "我们提出了一种可扩展的生成高分辨率3D数据的管道，以及一个名为MiniShift的新数据集。同时，我们设计了一个简单的框架Simple3D，该框架结合了多尺度邻域描述符（MSND）和局部特征空间聚合（LFSA），以高效的方式捕捉几何细节，实现实时检测。", "result": "实验结果显示，Simple3D框架在MiniShift数据集和现有的基准数据集上都表现出了优于现有方法的准确性和速度。", "conclusion": "这表明高分辨率数据和有效的特征聚合对于提升3D异常检测的实际应用具有关键性的作用。"}}
{"id": "2507.07700", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.07700", "abs": "https://arxiv.org/abs/2507.07700", "authors": ["Dominykas Seputis", "Yongkang Li", "Karsten Langerak", "Serghei Mihailov"], "title": "Rethinking the Privacy of Text Embeddings: A Reproducibility Study of \"Text Embeddings Reveal (Almost) As Much As Text\"", "comment": "This paper has been accepted for oral presentation in the\n  reproducibility track at RecSys 2025", "summary": "Text embeddings are fundamental to many natural language processing (NLP)\ntasks, extensively applied in domains such as recommendation systems and\ninformation retrieval (IR). Traditionally, transmitting embeddings instead of\nraw text has been seen as privacy-preserving. However, recent methods such as\nVec2Text challenge this assumption by demonstrating that controlled decoding\ncan successfully reconstruct original texts from black-box embeddings. The\nunexpectedly strong results reported by Vec2Text motivated us to conduct\nfurther verification, particularly considering the typically non-intuitive and\nopaque structure of high-dimensional embedding spaces. In this work, we\nreproduce the Vec2Text framework and evaluate it from two perspectives: (1)\nvalidating the original claims, and (2) extending the study through targeted\nexperiments. First, we successfully replicate the original key results in both\nin-domain and out-of-domain settings, with only minor discrepancies arising due\nto missing artifacts, such as model checkpoints and dataset splits.\nFurthermore, we extend the study by conducting a parameter sensitivity\nanalysis, evaluating the feasibility of reconstructing sensitive inputs (e.g.,\npasswords), and exploring embedding quantization as a lightweight privacy\ndefense. Our results show that Vec2Text is effective under ideal conditions,\ncapable of reconstructing even password-like sequences that lack clear\nsemantics. However, we identify key limitations, including its sensitivity to\ninput sequence length. We also find that Gaussian noise and quantization\ntechniques can mitigate the privacy risks posed by Vec2Text, with quantization\noffering a simpler and more widely applicable solution. Our findings emphasize\nthe need for caution in using text embeddings and highlight the importance of\nfurther research into robust defense mechanisms for NLP systems.", "AI": {"tldr": "该研究通过验证Vec2Text框架和扩展实验，探讨了文本嵌入的隐私风险，指出虽然Vec2Text在理想条件下能有效重构原始文本，但存在一些弱点，并提出高斯噪声和量化技术作为可能的隐私防御手段。", "motivation": "动机是验证Vec2Text的研究结果及其隐私保护能力，考虑到高维嵌入空间通常不直观且不透明。", "method": "研究方法包括重复Vec2Text的实验，以及通过参数敏感性分析、敏感输入重构测试和嵌入量化实验来扩展研究。", "result": "结果显示Vec2Text在理想条件下能有效重构包括密码在内的文本序列，但它受到输入序列长度等限制。", "conclusion": "结论强调在使用文本嵌入时需要谨慎，并强调需要进一步研究以构建更强大的NLP系统隐私防御机制。"}}
{"id": "2507.07443", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07443", "abs": "https://arxiv.org/abs/2507.07443", "authors": ["Ling Zhou", "Runtian Yuan", "Yi Liu", "Yuejie Zhang", "Rui Feng", "Shang Gao"], "title": "Dual Semantic-Aware Network for Noise Suppressed Ultrasound Video Segmentation", "comment": null, "summary": "Ultrasound imaging is a prevalent diagnostic tool known for its simplicity\nand non-invasiveness. However, its inherent characteristics often introduce\nsubstantial noise, posing considerable challenges for automated lesion or organ\nsegmentation in ultrasound video sequences. To address these limitations, we\npropose the Dual Semantic-Aware Network (DSANet), a novel framework designed to\nenhance noise robustness in ultrasound video segmentation by fostering mutual\nsemantic awareness between local and global features. Specifically, we\nintroduce an Adjacent-Frame Semantic-Aware (AFSA) module, which constructs a\nchannel-wise similarity matrix to guide feature fusion across adjacent frames,\neffectively mitigating the impact of random noise without relying on\npixel-level relationships. Additionally, we propose a Local-and-Global\nSemantic-Aware (LGSA) module that reorganizes and fuses temporal unconditional\nlocal features, which capture spatial details independently at each frame, with\nconditional global features that incorporate temporal context from adjacent\nframes. This integration facilitates multi-level semantic representation,\nsignificantly improving the model's resilience to noise interference. Extensive\nevaluations on four benchmark datasets demonstrate that DSANet substantially\noutperforms state-of-the-art methods in segmentation accuracy. Moreover, since\nour model avoids pixel-level feature dependencies, it achieves significantly\nhigher inference FPS than video-based methods, and even surpasses some\nimage-based models. Code can be found in\n\\href{https://github.com/ZhouL2001/DSANet}{DSANet}", "AI": {"tldr": "研究提出了一种称为DSANet的新框架，用于增强超声视频分割的噪声鲁棒性，展示了在精度和速度上的显著优势。", "motivation": "超声成像作为一种无创诊断工具，因其固有的特点会引入大量噪声，这对自动化病变或器官分割带来了挑战。为了应对这些挑战而提出该框架。", "method": "提出了一种名为Dual Semantic-Aware Network (DSANet) 的新框架来增强超声视频分割的噪声鲁棒性。该框架引入了Adjacent-Frame Semantic-Aware (AFSA) 模块和Local-and-Global Semantic-Aware (LGSA) 模块，通过引导相邻帧之间的特征融合来减轻随机噪声的影响，同时保持对空间细节和时间上下文的关注。", "result": "在四个基准数据集上的广泛评估表明，DSANet 在分割精度上显著优于现有方法。此外，该模型的推理速度也显著高于基于视频的方法，甚至超过了某些基于图像的模型。", "conclusion": "通过集成局部和全局特征的多级语义表征，DSANet 能够有效应对噪声干扰，实现更高的分割精度和更快的推理速度。"}}
{"id": "2507.07725", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07725", "abs": "https://arxiv.org/abs/2507.07725", "authors": ["Zhijin Dong"], "title": "Not All Preferences are What You Need for Post-Training: Selective Alignment Strategy for Preference Optimization", "comment": null, "summary": "Post-training alignment of large language models (LLMs) is a critical\nchallenge, as not all tokens contribute equally to model performance. This\npaper introduces a selective alignment strategy that prioritizes high-impact\ntokens within preference pairs, leveraging token-level log-probability\ndifferences between the current policy and a reference model. By focusing on\nthese informative tokens, our approach reduces computational overhead and\nenhances alignment fidelity. We further explore the role of reference model\nquality, demonstrating that stronger reference models significantly improve\ntoken selection accuracy and overall optimization effectiveness. Comprehensive\nexperiments on benchmarks such as Arena-Hard and MT-Bench validate the\nsuperiority of our Selective-DPO method over standard DPO and\ndistillation-based baselines. Our findings highlight the importance of\ntoken-level optimization and reference model selection in advancing preference\nalignment for LLMs. The code is available at\nhttps://github.com/Dongzhijin/SDPO.", "AI": {"tldr": "本论文介绍了一种选择性对齐策略，专注于高影响力的标记，以提高对齐的保真度并减少计算负担。实验表明，这个Selective-DPO方法优于现有的方法。", "motivation": "训练后期模型的对齐是一个关键挑战，因为并非所有标记对模型性能都有同等贡献。", "method": "此论文提出了一种选择性对齐策略，该策略优先考虑偏好对中高影响的标记，并利用当前策略和参考模型之间的标记级对数概率差异。通过聚焦这些信息量大的标记，该方法减少了计算开销并增强了对齐的保真度。", "result": "在诸如Arena-Hard和MT-Bench的基准测试上进行的全面实验验证了Selective-DPO方法优于标准DPO和基于蒸馏的基线。", "conclusion": "研究发现强调了标记级优化和参考模型选择在推进LLM偏好对齐中的重要性。"}}
{"id": "2507.07453", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07453", "abs": "https://arxiv.org/abs/2507.07453", "authors": ["M. A. Rasel", "Sameem Abdul Kareem", "Zhenli Kwan", "Shin Shen Yong", "Unaizah Obaidellah"], "title": "Bluish Veil Detection and Lesion Classification using Custom Deep Learnable Layers with Explainable Artificial Intelligence (XAI)", "comment": "Accepted version. Published in Computers in Biology and Medicine, 14\n  June 2024. DOI: 10.1016/j.compbiomed.2024.108758", "summary": "Melanoma, one of the deadliest types of skin cancer, accounts for thousands\nof fatalities globally. The bluish, blue-whitish, or blue-white veil (BWV) is a\ncritical feature for diagnosing melanoma, yet research into detecting BWV in\ndermatological images is limited. This study utilizes a non-annotated skin\nlesion dataset, which is converted into an annotated dataset using a proposed\nimaging algorithm based on color threshold techniques on lesion patches and\ncolor palettes. A Deep Convolutional Neural Network (DCNN) is designed and\ntrained separately on three individual and combined dermoscopic datasets, using\ncustom layers instead of standard activation function layers. The model is\ndeveloped to categorize skin lesions based on the presence of BWV. The proposed\nDCNN demonstrates superior performance compared to conventional BWV detection\nmodels across different datasets. The model achieves a testing accuracy of\n85.71% on the augmented PH2 dataset, 95.00% on the augmented ISIC archive\ndataset, 95.05% on the combined augmented (PH2+ISIC archive) dataset, and\n90.00% on the Derm7pt dataset. An explainable artificial intelligence (XAI)\nalgorithm is subsequently applied to interpret the DCNN's decision-making\nprocess regarding BWV detection. The proposed approach, coupled with XAI,\nsignificantly improves the detection of BWV in skin lesions, outperforming\nexisting models and providing a robust tool for early melanoma diagnosis.", "AI": {"tldr": "研究提出了一种基于颜色阈值技术将非标注皮肤病变数据集转换为标注数据集的方法，并设计了一种深度卷积神经网络（DCNN），用于识别皮肤病变中的蓝白膜（BWV），其在多个数据集上的测试准确率均超过85%，优于现有模型。", "motivation": "鉴于蓝白膜在诊断黑色素瘤中的重要性及当前对其在皮肤病学图像中检测研究的不足，该研究旨在开发更优的BWV检测方法以提高早期诊断效率。", "method": "研究使用颜色阈值技术将非标注皮肤病变数据集转换为标注数据集，并设计一种使用自定义层替代标准激活函数层的深度卷积神经网络（DCNN）。", "result": "DCNN模型在增强后的不同数据集上达到较高的测试准确率，包括PH2数据集上的85.71%，ISIC归档数据集上的95.00%，结合增强的PH2+ISIC归档数据集上的95.05%，以及Derm7pt数据集上的90.00%。", "conclusion": "该研究提出的DCNN模型结合可解释人工智能算法在皮肤病变中识别BWV方面表现出色，比现有模型更加优越，为早期黑色素瘤诊断提供了有力工具。"}}
{"id": "2507.07741", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.07741", "abs": "https://arxiv.org/abs/2507.07741", "authors": ["Maha Tufail Agro", "Atharva Kulkarni", "Karima Kadaoui", "Zeerak Talat", "Hanan Aldarmaki"], "title": "Code-Switching in End-to-End Automatic Speech Recognition: A Systematic Literature Review", "comment": null, "summary": "Motivated by a growing research interest into automatic speech recognition\n(ASR), and the growing body of work for languages in which code-switching (CS)\noften occurs, we present a systematic literature review of code-switching in\nend-to-end ASR models. We collect and manually annotate papers published in\npeer reviewed venues. We document the languages considered, datasets, metrics,\nmodel choices, and performance, and present a discussion of challenges in\nend-to-end ASR for code-switching. Our analysis thus provides insights on\ncurrent research efforts and available resources as well as opportunities and\ngaps to guide future research.", "AI": {"tldr": "本文提供了一个关于代码切换在端到端语音识别模型中系统文献回顾的摘要，分析了涉及的语言、数据集、度量方法、模型选择和性能，讨论了在端到端代码切换语音识别中的挑战。", "motivation": "受到自动语音识别（ASR）研究兴趣的增长，以及在经常发生代码切换（CS）的语言中工作的不断增多的驱动，作者开展了这项研究。", "method": "收集并手动标注发表在同行评审会议上的论文，文档中包括考虑的语言、数据集、度量方法、模型选择和性能。", "result": "提供了关于当前研究工作和可用资源以及未来研究机会和缺口的见解。", "conclusion": "这项分析揭示了在端到端代码切换语音识别研究领域的现状和未来的发展方向。"}}
{"id": "2507.07460", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07460", "abs": "https://arxiv.org/abs/2507.07460", "authors": ["Jeonghoon Song", "Sunghun Kim", "Jaegyun Im", "Byeongjoon Noh"], "title": "Objectomaly: Objectness-Aware Refinement for OoD Segmentation with Structural Consistency and Boundary Precision", "comment": null, "summary": "Out-of-Distribution (OoD) segmentation is critical for safety-sensitive\napplications like autonomous driving. However, existing mask-based methods\noften suffer from boundary imprecision, inconsistent anomaly scores within\nobjects, and false positives from background noise. We propose\n\\textbf{\\textit{Objectomaly}}, an objectness-aware refinement framework that\nincorporates object-level priors. Objectomaly consists of three stages: (1)\nCoarse Anomaly Scoring (CAS) using an existing OoD backbone, (2)\nObjectness-Aware Score Calibration (OASC) leveraging SAM-generated instance\nmasks for object-level score normalization, and (3) Meticulous Boundary\nPrecision (MBP) applying Laplacian filtering and Gaussian smoothing for contour\nrefinement. Objectomaly achieves state-of-the-art performance on key OoD\nsegmentation benchmarks, including SMIYC AnomalyTrack/ObstacleTrack and\nRoadAnomaly, improving both pixel-level (AuPRC up to 96.99, FPR$_{95}$ down to\n0.07) and component-level (F1$-$score up to 83.44) metrics. Ablation studies\nand qualitative results on real-world driving videos further validate the\nrobustness and generalizability of our method. Code will be released upon\npublication.", "AI": {"tldr": "Objectomaly 是一种对象意识增强的框架，处理 OoD 分割问题，通过三阶段的框架，在多个基准测试中显示了优越的性能和鲁棒性。", "motivation": "该论文的动机在于解决现有基于掩码的方法在 OoD 分割中的边界不精确、对象内的异常评分不一致以及来自背景噪声的假阳性等问题，尤其是在自动驾驶等领域中需要高度安全性的应用。", "method": "Objectomaly 方法包含三个阶段：1) 使用现有的 OoD 背骨网络进行粗略异常评分 (Coarse Anomaly Scoring, CAS)，2) 利用 SAM 生成的实例掩码进行对象级分数校准 (Objectness-Aware Score Calibration, OASC)，3) 使用拉普拉斯滤波和高斯平滑进行轮廓精修 (Meticulous Boundary Precision, MBP).", "result": "Objectomaly 在几个关键的 OoD 分割基准上实现了最先进的性能，包括 SMIYC AnomalyTrack/ObstacleTrack 和 RoadAnomaly，提升了像素级（AuPRC 最高达到 96.99，FPR95 最低达到 0.07）和组件级（F1 分数最高达到 83.44）的评估指标。", "conclusion": "通过消融研究和在真实驾驶视频上的定性结果，证明了 Objectomaly 方法的鲁棒性和通用性。"}}
{"id": "2507.07748", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07748", "abs": "https://arxiv.org/abs/2507.07748", "authors": ["Peizhang Shao", "Linrui Xu", "Jinxi Wang", "Wei Zhou", "Xingyu Wu"], "title": "When Large Language Models Meet Law: Dual-Lens Taxonomy, Technical Advances, and Ethical Governance", "comment": null, "summary": "This paper establishes the first comprehensive review of Large Language\nModels (LLMs) applied within the legal domain. It pioneers an innovative dual\nlens taxonomy that integrates legal reasoning frameworks and professional\nontologies to systematically unify historical research and contemporary\nbreakthroughs. Transformer-based LLMs, which exhibit emergent capabilities such\nas contextual reasoning and generative argumentation, surmount traditional\nlimitations by dynamically capturing legal semantics and unifying evidence\nreasoning. Significant progress is documented in task generalization, reasoning\nformalization, workflow integration, and addressing core challenges in text\nprocessing, knowledge integration, and evaluation rigor via technical\ninnovations like sparse attention mechanisms and mixture-of-experts\narchitectures. However, widespread adoption of LLM introduces critical\nchallenges: hallucination, explainability deficits, jurisdictional adaptation\ndifficulties, and ethical asymmetry. This review proposes a novel taxonomy that\nmaps legal roles to NLP subtasks and computationally implements the Toulmin\nargumentation framework, thus systematizing advances in reasoning, retrieval,\nprediction, and dispute resolution. It identifies key frontiers including\nlow-resource systems, multimodal evidence integration, and dynamic rebuttal\nhandling. Ultimately, this work provides both a technical roadmap for\nresearchers and a conceptual framework for practitioners navigating the\nalgorithmic future, laying a robust foundation for the next era of legal\nartificial intelligence. We have created a GitHub repository to index the\nrelevant papers: https://github.com/Kilimajaro/LLMs_Meet_Law.", "AI": {"tldr": "该论文为大语言模型在法律领域的应用提供了一个综合性的回顾和前景展望，同时也指出了采用LLM所面临的挑战，包括幻觉问题、可解释性缺陷、司法区域适应性和伦理不对称问题。最终，该著作提供了一份技术路线图和概念框架，为研究者和从业者导航法律人工智能的算法未来奠定了坚实的基础。", "motivation": "该论文首次全面回顾了大语言模型（LLM）在法律领域的应用，并提出了一种创新的双重视角分类法，将法律推理框架和专业术语结合起来，系统性地统一了历史研究和最新成果。", "method": "通过利用基于Transformer的LLM，这些模型展示了如情境推理和生成性论证等新兴能力，克服了传统限制，动态捕捉法律语义并统一证据推理。", "result": "文中记录了任务泛化、推理形式化、流程集成以及解决文本处理、知识整合和评估严谨性方面核心挑战的显著进步，通过诸如稀疏注意力机制和专家混合架构等技术创新来实现。", "conclusion": "该篇综述提出了一个将法律角色映射到NLP子任务并计算实现Toulmin论证框架的新分类法，从而系统化地推动了在推理、检索、预测和争端解决方面的进步。指出了关键前沿领域，包括低资源系统、多模态证据整合和动态反驳处理。"}}
{"id": "2507.07464", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07464", "abs": "https://arxiv.org/abs/2507.07464", "authors": ["Chang-Hwan Son"], "title": "Degradation-Agnostic Statistical Facial Feature Transformation for Blind Face Restoration in Adverse Weather Conditions", "comment": null, "summary": "With the increasing deployment of intelligent CCTV systems in outdoor\nenvironments, there is a growing demand for face recognition systems optimized\nfor challenging weather conditions. Adverse weather significantly degrades\nimage quality, which in turn reduces recognition accuracy. Although recent face\nimage restoration (FIR) models based on generative adversarial networks (GANs)\nand diffusion models have shown progress, their performance remains limited due\nto the lack of dedicated modules that explicitly address weather-induced\ndegradations. This leads to distorted facial textures and structures. To\naddress these limitations, we propose a novel GAN-based blind FIR framework\nthat integrates two key components: local Statistical Facial Feature\nTransformation (SFFT) and Degradation-Agnostic Feature Embedding (DAFE). The\nlocal SFFT module enhances facial structure and color fidelity by aligning the\nlocal statistical distributions of low-quality (LQ) facial regions with those\nof high-quality (HQ) counterparts. Complementarily, the DAFE module enables\nrobust statistical facial feature extraction under adverse weather conditions\nby aligning LQ and HQ encoder representations, thereby making the restoration\nprocess adaptive to severe weather-induced degradations. Experimental results\ndemonstrate that the proposed degradation-agnostic SFFT model outperforms\nexisting state-of-the-art FIR methods based on GAN and diffusion models,\nparticularly in suppressing texture distortions and accurately reconstructing\nfacial structures. Furthermore, both the SFFT and DAFE modules are empirically\nvalidated in enhancing structural fidelity and perceptual quality in face\nrestoration under challenging weather scenarios.", "AI": {"tldr": "论文提出了一种基于GAN的盲面部图像恢复框架，包括局部统计面部特征变换模块和降质不可知特征嵌入模块，有效提高了恶劣天气下的面部识别准确度。", "motivation": "随着智能CCTV系统的普及，对恶劣天气条件下性能优化的面部识别系统的需求日益增长。现有模型在应对天气引起的降质方面存在局限性，导致面部纹理和结构的扭曲。", "method": "提出了一个结合局部统计面部特征变换(SFFT)和降质不可知特征嵌入(DAFE)模块的新型GAN框架，SFFT模块通过低质量（LQ）和高质量（HQ）面部区域之间的局部统计分布对齐改进面部结构和颜色保真度；DAFE模块通过LQ和HQ编码器表示的对齐提高恶劣天气下的面部特征提取效果。", "result": "实验结果表明，提出的降质不可知SFFT模型在抑制纹理扭曲方面和准确重建面部结构方面优于现有的GAN和扩散模型，特别是极大改善了恶劣天气下的面部图像恢复效果。", "conclusion": "所提技术和模块在恶劣天气场景中增强结构性保真度和感知质量方面得到了验证，表明了方法的有效性。"}}
{"id": "2507.07803", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.07803", "abs": "https://arxiv.org/abs/2507.07803", "authors": ["Shoutao Guo", "Xiang Li", "Shaolei Zhang", "Mengge Liu", "Wei Chen", "Yang Feng"], "title": "StreamUni: Achieving Streaming Speech Translation with a Unified Large Speech-Language Model", "comment": "The code is at https://github.com/ictnlp/StreamUni; The model is at\n  https://huggingface.co/ICTNLP/StreamUni-Phi4", "summary": "Streaming speech translation (StreamST) requires determining appropriate\ntiming, known as policy, to generate translations while continuously receiving\nsource speech inputs, balancing low latency with high translation quality.\nHowever, existing StreamST methods typically operate on sentence-level speech\nsegments, referred to as simultaneous speech translation (SimulST). In\npractice, they require collaboration with segmentation models to accomplish\nStreamST, where the truncated speech segments constrain SimulST models to make\npolicy decisions and generate translations based on limited contextual\ninformation. Moreover, SimulST models struggle to learn effective policies due\nto the complexity of speech inputs and cross-lingual generation. To address\nthese challenges, we propose StreamUni, which achieves StreamST through a\nunified Large Speech-Language Model (LSLM). Specifically, StreamUni\nincorporates speech Chain-of-Thought (CoT) in guiding the LSLM to generate\nmulti-stage outputs. Leveraging these multi-stage outputs, StreamUni\nsimultaneously accomplishes speech segmentation, policy decision, and\ntranslation generation, completing StreamST without requiring massive\npolicy-specific training. Additionally, we propose a streaming CoT training\nmethod that enhances low-latency policy decisions and generation capabilities\nusing limited CoT data. Experiments demonstrate that our approach achieves\nstate-of-the-art performance on StreamST tasks.", "AI": {"tldr": "本文提出了一种新的流式语音翻译方法StreamUni，利用统一的大型语音语言模型来改进现有方法中存在的限制，实验显示其性能优越。", "motivation": "文章旨在提出一种新的方法来解决现有流式语音翻译方法在实时性和翻译质量之间的平衡问题，特别是由于语音分段限制了翻译模型的策略决策和生成能力。", "method": "StreamUni采用统一的大型语音语言模型(LSLM)，通过语音链条思维(CoT)引导多阶段输出来实现流式语音翻译(StreamST)。这种方法解决了传统的SimulST模型在学习有效策略方面的难题，避免了对外部分词模型的依赖，从而减少了限制并提高了翻译质量。", "result": "实验表明，这种方法在流式语音翻译任务上达到了最先进的性能。", "conclusion": "通过引入StreamUni和相应的流水线CoT训练方法，研究在没有大量特定策略训练的情况下实现了高效的低延迟策略决策和生成能力，证明了其方法的有效性和先进性。"}}
{"id": "2507.07483", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07483", "abs": "https://arxiv.org/abs/2507.07483", "authors": ["Qiangqiang Wu", "Yi Yu", "Chenqi Kong", "Ziquan Liu", "Jia Wan", "Haoliang Li", "Alex C. Kot", "Antoni B. Chan"], "title": "Temporal Unlearnable Examples: Preventing Personal Video Data from Unauthorized Exploitation by Object Tracking", "comment": "Accepted by ICCV 2025", "summary": "With the rise of social media, vast amounts of user-uploaded videos (e.g.,\nYouTube) are utilized as training data for Visual Object Tracking (VOT).\nHowever, the VOT community has largely overlooked video data-privacy issues, as\nmany private videos have been collected and used for training commercial models\nwithout authorization. To alleviate these issues, this paper presents the first\ninvestigation on preventing personal video data from unauthorized exploitation\nby deep trackers. Existing methods for preventing unauthorized data use\nprimarily focus on image-based tasks (e.g., image classification), directly\napplying them to videos reveals several limitations, including inefficiency,\nlimited effectiveness, and poor generalizability. To address these issues, we\npropose a novel generative framework for generating Temporal Unlearnable\nExamples (TUEs), and whose efficient computation makes it scalable for usage on\nlarge-scale video datasets. The trackers trained w/ TUEs heavily rely on\nunlearnable noises for temporal matching, ignoring the original data structure\nand thus ensuring training video data-privacy. To enhance the effectiveness of\nTUEs, we introduce a temporal contrastive loss, which further corrupts the\nlearning of existing trackers when using our TUEs for training. Extensive\nexperiments demonstrate that our approach achieves state-of-the-art performance\nin video data-privacy protection, with strong transferability across VOT\nmodels, datasets, and temporal matching tasks.", "AI": {"tldr": "This paper presents a new method for protecting personal video data privacy in visual object tracking by generating Temporal Unlearnable Examples (TUEs) and applying a temporal contrastive loss to improve privacy protection effectiveness.", "motivation": "The paper addresses the overlooked data privacy concerns in the use of personal videos for training visual object tracking models, focusing on the inadequacy of existing methods that primarily target image-based tasks.", "method": "This paper proposes a novel generative framework for generating Temporal Unlearnable Examples (TUEs) to protect personal video data privacy in visual object tracking tasks, complemented by a temporal contrastive loss to enhance the effectiveness of these TUEs.", "result": "The experiments show that the proposed approach achieves state-of-the-art performance in protecting video data privacy with strong transferability across visual object tracking models, datasets, and tasks.", "conclusion": "The introduction of TUEs and a temporal contrastive loss significantly advances the protection of video data privacy in visual object tracking, overcoming the limitations of existing methods."}}
{"id": "2507.07808", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07808", "abs": "https://arxiv.org/abs/2507.07808", "authors": ["Sara Candussio", "Gaia Saveri", "Gabriele Sarti", "Luca Bortolussi"], "title": "Bridging Logic and Learning: Decoding Temporal Logic Embeddings via Transformers", "comment": "16 pages, 3 figures, to be published in ECML-PKDD", "summary": "Continuous representations of logic formulae allow us to integrate symbolic\nknowledge into data-driven learning algorithms. If such embeddings are\nsemantically consistent, i.e. if similar specifications are mapped into nearby\nvectors, they enable continuous learning and optimization directly in the\nsemantic space of formulae. However, to translate the optimal continuous\nrepresentation into a concrete requirement, such embeddings must be invertible.\nWe tackle this issue by training a Transformer-based decoder-only model to\ninvert semantic embeddings of Signal Temporal Logic (STL) formulae. STL is a\npowerful formalism that allows us to describe properties of signals varying\nover time in an expressive yet concise way. By constructing a small vocabulary\nfrom STL syntax, we demonstrate that our proposed model is able to generate\nvalid formulae after only 1 epoch and to generalize to the semantics of the\nlogic in about 10 epochs. Additionally, the model is able to decode a given\nembedding into formulae that are often simpler in terms of length and nesting\nwhile remaining semantically close (or equivalent) to gold references. We show\nthe effectiveness of our methodology across various levels of training formulae\ncomplexity to assess the impact of training data on the model's ability to\neffectively capture the semantic information contained in the embeddings and\ngeneralize out-of-distribution. Finally, we deploy our model for solving a\nrequirement mining task, i.e. inferring STL specifications that solve a\nclassification task on trajectories, performing the optimization directly in\nthe semantic space.", "AI": {"tldr": "研究提出一种使用Transformer模型逆向转换STL公式嵌入的方法，以实现从连续表示到具体逻辑规范的转换，适用于要求挖掘任务，尤其在信号时间属性描述方面。", "motivation": "使连续表示转化为具体的逻辑公式，以便在公式语义空间中进行连续学习和优化。", "method": "使用基于Transformer的仅解码器模型来逆向转换信号时态逻辑（STL）公式语义嵌入，通过从STL语法构造的小词汇表生成有效公式。", "result": "模型经过仅1个轮次训练后即能生成有效公式，大约10轮次后能掌握逻辑语义，且解码出的公式往往更简洁并尽量保持与参考公式语义等价。", "conclusion": "该模型在不同复杂度训练公式水平上的测试展示了其从嵌入中捕获语义信息并推广到新分布的能力，证明了在直接解决基于轨迹的分类任务的逻辑要求挖掘任务上的有效性。"}}
{"id": "2507.07487", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07487", "abs": "https://arxiv.org/abs/2507.07487", "authors": ["Jiaxu Wan", "Xu Wang", "Mengwei Xie", "Xinyuan Chang", "Xinran Liu", "Zheng Pan", "Mu Xu", "Ding Yuan"], "title": "Driving by Hybrid Navigation: An Online HD-SD Map Association Framework and Benchmark for Autonomous Vehicles", "comment": "23 pages, 10 figures, 9 tables", "summary": "Autonomous vehicles rely on global standard-definition (SD) maps for\nroad-level route planning and online local high-definition (HD) maps for\nlane-level navigation. However, recent work concentrates on construct online HD\nmaps, often overlooking the association of global SD maps with online HD maps\nfor hybrid navigation, making challenges in utilizing online HD maps in the\nreal world. Observing the lack of the capability of autonomous vehicles in\nnavigation, we introduce \\textbf{O}nline \\textbf{M}ap \\textbf{A}ssociation, the\nfirst benchmark for the association of hybrid navigation-oriented online maps,\nwhich enhances the planning capabilities of autonomous vehicles. Based on\nexisting datasets, the OMA contains 480k of roads and 260k of lane paths and\nprovides the corresponding metrics to evaluate the performance of the model.\nAdditionally, we propose a novel framework, named Map Association Transformer,\nas the baseline method, using path-aware attention and spatial attention\nmechanisms to enable the understanding of geometric and topological\ncorrespondences. The code and dataset can be accessed at\nhttps://github.com/WallelWan/OMA-MAT.", "AI": {"tldr": "在线地图关联（OMA）是首个用于混合导航在线地图关联的基准测试，旨在提升自动驾驶车辆的路径规划能力。该框架包含48万条道路与26万条车道路径，提出了一种新的基于路径和空间注意力机制的方法，名为地图关联转换器（MAT）。此方法能够支持几何和拓扑关系的理解。代码和数据集可在指定GitHub仓库获取。", "motivation": "观察到近期研究专注于生成在线高精度地图，但忽略了与标准地图的关联对混合导航的实际应用挑战。因此，引入了在线地图关联基准，以解决自动驾驶车辆在导航能力上的局限。", "method": "Structure", "result": "{\n  \"tldr\": \"在线地图关联（OMA）是首个用于混合导航在线地图关联的基准测试，旨在提升自动驾驶车辆的路径规划能力。该框架包含48万条道路与26万条车道路径，提出了一种新的基于路径和空间注意力机制的方法，名为地图关联转换器（MAT）。此方法能够支持几何和拓扑关系的理解。代码和数据集可在指定GitHub仓库获取。\", \n  \"motivation\": \"观察到近期研究专注于生成在线高精度地图，但忽略了与标准地图的关联对混合导航的实际应用挑战。因此，引入了在线地图关联基准，以解决自动驾驶车辆在导航能力上的局限。\", \n  \"method\": \"引入了名为地图关联转换器的新框架，采用了路径感知注意力和空间注意力机制来增强模型对地图几何和拓扑关系的理解能力。\", \n  \"conclusion\": \"该项目提供了一个详实的基准测试（OMA）和新的基准框架（MAT），有助于推动自动驾驶车辆地图融合及规划技术的发展。\", \n  \"result\": \"OMA提供了丰富的道路与路径数据，并通过相应的评估指标测试模型性能。MAT框架展现出路径关联的理解能力，为混合导航提供了关键技术基础。\"}\n}", "conclusion": "该项目提供了一个详实的基准测试（OMA）和新的基准框架（MAT），有助于推动自动驾驶车辆地图融合及规划技术的发展。"}}
{"id": "2507.07810", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07810", "abs": "https://arxiv.org/abs/2507.07810", "authors": ["Nhi Hoai Doan", "Tatsuya Hiraoka", "Kentaro Inui"], "title": "Understanding and Controlling Repetition Neurons and Induction Heads in In-Context Learning", "comment": null, "summary": "This paper investigates the relationship between large language models'\n(LLMs) ability to recognize repetitive input patterns and their performance on\nin-context learning (ICL). In contrast to prior work that has primarily focused\non attention heads, we examine this relationship from the perspective of skill\nneurons, specifically repetition neurons. Our experiments reveal that the\nimpact of these neurons on ICL performance varies depending on the depth of the\nlayer in which they reside. By comparing the effects of repetition neurons and\ninduction heads, we further identify strategies for reducing repetitive outputs\nwhile maintaining strong ICL capabilities.", "AI": {"tldr": "本文通过技能神经元来探讨大型语言模型识别重复输入模式的能力与其在上下文学习中的表现之间的关系，并提出减少重复输出的方法。", "motivation": "本研究的动机是探索并理解大型语言模型识别重复模式的能力如何影响其在上下文学习任务中的表现，并试图找到减少重复输出同时保持强大上下文学习能力的方法。", "method": "研究着重于通过技能神经元，特别是重复神经元，来分析大型语言模型(LLMs)识别重复输入模式的能力与其在上下文学习(ICL)中的表现之间的关系。不同于以前集中关注注意力头的工作，本研究通过实验展示了这些神经元对ICL性能的影响取决于它们所在的层数深度。", "result": "研究结果显示，重复神经元对ICL性能的影响与其所在神经网络层的深度有关。通过比较重复神经元和归纳头的影响，提出了一些减少重复输出而维持强大ICL能力的策略。", "conclusion": "研究结论指出，重复神经元在不同深度会影响模型的上下文学习性能。通过精确控制重复神经元的激活状态，可以在减少模型重复输出的同时保持或提高其上下文学习能力。"}}
{"id": "2507.07496", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07496", "abs": "https://arxiv.org/abs/2507.07496", "authors": ["Marie-Christine Pali", "Christina Schwaiger", "Malik Galijasevic", "Valentin K. Ladenhauf", "Stephanie Mangesius", "Elke R. Gizewski"], "title": "Semi-supervised learning and integration of multi-sequence MR-images for carotid vessel wall and plaque segmentation", "comment": null, "summary": "The analysis of carotid arteries, particularly plaques, in multi-sequence\nMagnetic Resonance Imaging (MRI) data is crucial for assessing the risk of\natherosclerosis and ischemic stroke. In order to evaluate metrics and radiomic\nfeatures, quantifying the state of atherosclerosis, accurate segmentation is\nimportant. However, the complex morphology of plaques and the scarcity of\nlabeled data poses significant challenges. In this work, we address these\nproblems and propose a semi-supervised deep learning-based approach designed to\neffectively integrate multi-sequence MRI data for the segmentation of carotid\nartery vessel wall and plaque. The proposed algorithm consists of two networks:\na coarse localization model identifies the region of interest guided by some\nprior knowledge on the position and number of carotid arteries, followed by a\nfine segmentation model for precise delineation of vessel walls and plaques. To\neffectively integrate complementary information across different MRI sequences,\nwe investigate different fusion strategies and introduce a multi-level\nmulti-sequence version of U-Net architecture. To address the challenges of\nlimited labeled data and the complexity of carotid artery MRI, we propose a\nsemi-supervised approach that enforces consistency under various input\ntransformations. Our approach is evaluated on 52 patients with\narteriosclerosis, each with five MRI sequences. Comprehensive experiments\ndemonstrate the effectiveness of our approach and emphasize the role of fusion\npoint selection in U-Net-based architectures. To validate the accuracy of our\nresults, we also include an expert-based assessment of model performance. Our\nfindings highlight the potential of fusion strategies and semi-supervised\nlearning for improving carotid artery segmentation in data-limited MRI\napplications.", "AI": {"tldr": "本文提出了一种半监督深度学习方法用于颈动脉及其斑块的精准分割，采用多层级多序列的U-Net架构，并通过不同融合点的选择提高了MRI数据分割的准确性。实验结果显示了方法的有效性。", "motivation": "研究的动机在于分析颈动脉斑块并通过多序列MRI数据进行准确的分割，这对于评估动脉粥样硬化和缺血性中风的风险至关重要。然而，复杂的斑块形态和欠缺的标注数据带来了挑战。", "method": "该研究提出了一种半监督深度学习方法，用于整合多序列MRI数据以进行颈动脉壁及其斑块的分割。该算法由两个网络组成：粗略定位模型和精细分割模型。粗略定位模型用于基于先前的知识识别感兴趣区域，而精细分割模型则用于精确划定血管壁和斑块。研究还探讨了不同融合策略，并提出了一种多层级多序列的U-Net架构版本。为了解决标注数据有限和颈动脉MRI复杂度的问题，研究提出了一个半监督方法，通过在不同的输入变换下保持一致性来提高准确性。", "result": "方法在52名患有动脉粥样硬化的患者的数据集上进行了评估，每个患者都有五种MRI序列影像。实验结果显示了该方法的有效性和在U-Net架构中融合点选择的重要性。研究还包含了基于专家的模型性能评估。", "conclusion": "研究结论表明，融合策略和半监督学习对于数据有限条件下的颈动脉分割具有潜在的提升作用。"}}
{"id": "2507.07817", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07817", "abs": "https://arxiv.org/abs/2507.07817", "authors": ["Anwoy Chatterjee", "H S V N S Kowndinya Renduchintala", "Sumit Bhatia", "Tanmoy Chakraborty"], "title": "On the Effect of Instruction Tuning Loss on Generalization", "comment": "Transactions of the Association for Computational Linguistics (TACL)", "summary": "Instruction Tuning has emerged as a pivotal post-training paradigm that\nenables pre-trained language models to better follow user instructions. Despite\nits significance, little attention has been given to optimizing the loss\nfunction used. A fundamental, yet often overlooked, question is whether the\nconventional auto-regressive objective - where loss is computed only on\nresponse tokens, excluding prompt tokens - is truly optimal for instruction\ntuning. In this work, we systematically investigate the impact of\ndifferentially weighting prompt and response tokens in instruction tuning loss,\nand propose Weighted Instruction Tuning (WIT) as a better alternative to\nconventional instruction tuning. Through extensive experiments on five language\nmodels of different families and scale, three finetuning datasets of different\nsizes, and five diverse evaluation benchmarks, we show that the standard\ninstruction tuning loss often yields suboptimal performance and limited\nrobustness to input prompt variations. We find that a low-to-moderate weight\nfor prompt tokens coupled with a moderate-to-high weight for response tokens\nyields the best-performing models across settings and also serve as better\nstarting points for the subsequent preference alignment training. These\nfindings highlight the need to reconsider instruction tuning loss and offer\nactionable insights for developing more robust and generalizable models. Our\ncode is open-sourced at https://github.com/kowndinya-renduchintala/WIT.", "AI": {"tldr": "文章提出了一种新的指令调整方法WIT，该方法通过适当地调整提示和响应令牌在损失函数中的权重来改进模型性能和鲁棒性。实验结果表明，WIT比传统的方法更优。", "motivation": "现有的指令调整方法通常忽略了对损失函数的优化，特别是仅针对响应令牌计算损失的方法的有效性。研究动机在于探讨通过重新考虑损失函数的设计来提升模型性能和鲁棒性的可能性。", "method": "研究中系统性地调查了对提示令牌和响应令牌在指令调整损失中施加不同权重的影响，并提出了一种新的方法——加权指令调整（Weighted Instruction Tuning, WIT）。通过实验测试了该方法在不同规模和家族的语言模型、不同大小的细调数据集和五个不同的评估基准上的效果。", "result": "研究分析了当前指令调整（Instruction Tuning）中损失函数的优化问题，提出加权指令调整（Weighted Instruction Tuning, WIT）方法，通过实验验证了传统的仅对响应令牌计算损失的方法存在性能不足和对输入提示变化鲁棒性有限的问题。实验表明，对提示令牌和响应令牌赋予适当的权重可以提高模型的性能和鲁棒性。", "conclusion": "研究表明标准的指令调整损失往往会导致次优的性能和对输入提示变化鲁棒性不足。通过对提示令牌赋予较低到中等的权重，同时对响应令牌赋予中等到较高的权重，可以得到性能最佳的模型。这表明重新设计指令调整的损失函数是必要的，并为开发更强大、更具通用性的语言模型提供了有用的看法。"}}
{"id": "2507.07510", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07510", "abs": "https://arxiv.org/abs/2507.07510", "authors": ["Binxu Li", "Minkai Xu", "Meihua Dang", "Stefano Ermon"], "title": "Divergence Minimization Preference Optimization for Diffusion Model Alignment", "comment": "24 pages, 8 figures", "summary": "Diffusion models have achieved remarkable success in generating realistic and\nversatile images from text prompts. Inspired by the recent advancements of\nlanguage models, there is an increasing interest in further improving the\nmodels by aligning with human preferences. However, we investigate alignment\nfrom a divergence minimization perspective and reveal that existing preference\noptimization methods are typically trapped in suboptimal mean-seeking\noptimization. In this paper, we introduce Divergence Minimization Preference\nOptimization (DMPO), a novel and principled method for aligning diffusion\nmodels by minimizing reverse KL divergence, which asymptotically enjoys the\nsame optimization direction as original RL. We provide rigorous analysis to\njustify the effectiveness of DMPO and conduct comprehensive experiments to\nvalidate its empirical strength across both human evaluations and automatic\nmetrics. Our extensive results show that diffusion models fine-tuned with DMPO\ncan consistently outperform or match existing techniques, specifically\noutperforming all existing diffusion alignment baselines by at least 64.6% in\nPickScore across all evaluation datasets, demonstrating the method's\nsuperiority in aligning generative behavior with desired outputs. Overall, DMPO\nunlocks a robust and elegant pathway for preference alignment, bridging\nprincipled theory with practical performance in diffusion models.", "AI": {"tldr": "本文提出了DMPO方法，通过最小化逆KL散度进行扩散模型对齐，有效超越现有技术。", "motivation": "受到最近语言模型进展的启发，我们希望通过与人类偏好一致来进一步改进模型。通过从散度最小化的角度研究对齐，我们发现现有的偏好优化方法通常陷入次优的均值寻找优化中。", "method": "我们的方法名为Divergence Minimization Preference Optimization (DMPO)，这是一种新颖且原理明确的方法，通过最小化逆KL散度来对齐扩散模型，其优化方向与原始RL渐近相同。", "result": "我们的实验结果全面证实了DMPO在人类评价和自动指标两方面的有效性，特别是在PickScore评估数据集上，DMPO比现有扩散模型基线高出至少64.6%，展示了其将生成行为与所需输出对齐的优势。", "conclusion": "总体而言，DMPO为扩散模型的偏好对齐解锁了稳健而优雅的路径，既具有坚实的理论基础，又提供了优异的实际表现。"}}
{"id": "2507.07824", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07824", "abs": "https://arxiv.org/abs/2507.07824", "authors": ["Gianluca Vico", "Jindřinch Libovický"], "title": "Conditional Unigram Tokenization with Parallel Data", "comment": "21 pages, 4 figures, submitted to Tokenization Workshop (TokShop) at\n  ICML 2025", "summary": "We introduce conditional unigram tokenization, a novel approach that extends\nunigram tokenization by conditioning target token probabilities on\nsource-language tokens from parallel data. Given a fixed source tokenizer, our\nmethod learns a target tokenizer that maximizes cross-lingual semantic\nalignment. We evaluate our tokenizer on four language pairs across different\nfamilies and resource levels, examining intrinsic properties and downstream\nperformance on machine translation and language modeling. While our conditional\ntokenizer maintains comparable statistical properties to standard unigram\ntokenizers, results are mixed: we observe no improvements in machine\ntranslation quality, but find consistent perplexity reductions in language\nmodeling. We hypothesize that quadratic scaling of conditional probability\nestimation with respect to the vocabulary size creates a data efficiency\nbottleneck. Our findings suggest that alternative parameterizations may be\nnecessary for practical cross-lingual tokenization.", "AI": {"tldr": "本文介绍了一种条件单词素化的创新技术，该技术在一定条件下显现出了改善语言模型性能的潜力，但在机器翻译上的表现则没有显著改进。", "motivation": "我们引入一种新的条件单词素化方法，目的是探索如何增强跨语言任务中的语义对齐效果。", "method": "我们提出了一种新的方法——条件单词素化，这种方法通过利用平行数据中源语言词素来调整目标语言词素的概率，从而扩展了单词素化。给定一个固定的源语言词素器，我们的方法学习一个目标语言词素器，旨在最大化跨语言语义对齐。", "result": "我们在四个不同语言系列和资源水平的语言对上评估了我们的词素化方法，观察了单词词素器的内在性质及其在机器翻译和语言模型中的下游性能。虽然我们的条件单词词素器的统计性质与标准单词素器相当，但结果是混合的：我们没有观察到机器翻译质量的提高，但在语言模型中发现了持续的困惑度降低。我们假设条件概率估计与词汇表大小呈二次关系扩大了数据效率瓶颈。", "conclusion": "研究结果表明，利用条件单词词素化进行跨语言任务并非总是理想的解决方案，应当考虑其他参数化方式来提高实践中的效率。"}}
