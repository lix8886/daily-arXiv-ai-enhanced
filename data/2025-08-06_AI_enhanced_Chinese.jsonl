{"id": "2508.02806", "categories": ["cs.CV", "cs.LG", "I.2.10; I.4.8; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.02806", "abs": "https://arxiv.org/abs/2508.02806", "authors": ["Zongyou Yang", "Jonathan Loo"], "title": "PyCAT4: A Hierarchical Vision Transformer-based Framework for 3D Human Pose Estimation", "comment": "10 pages, 20 figures", "summary": "Recently, a significant improvement in the accuracy of 3D human pose\nestimation has been achieved by combining convolutional neural networks (CNNs)\nwith pyramid grid alignment feedback loops. Additionally, innovative\nbreakthroughs have been made in the field of computer vision through the\nadoption of Transformer-based temporal analysis architectures. Given these\nadvancements, this study aims to deeply optimize and improve the existing Pymaf\nnetwork architecture. The main innovations of this paper include: (1)\nIntroducing a Transformer feature extraction network layer based on\nself-attention mechanisms to enhance the capture of low-level features; (2)\nEnhancing the understanding and capture of temporal signals in video sequences\nthrough feature temporal fusion techniques; (3) Implementing spatial pyramid\nstructures to achieve multi-scale feature fusion, effectively balancing feature\nrepresentations differences across different scales. The new PyCAT4 model\nobtained in this study is validated through experiments on the COCO and 3DPW\ndatasets. The results demonstrate that the proposed improvement strategies\nsignificantly enhance the network's detection capability in human pose\nestimation, further advancing the development of human pose estimation\ntechnology.", "AI": {"tldr": "", "motivation": "", "method": "Structure", "result": "{\n  \"tldr\": \"本文通过改进Pymaf网络结构，引入基于自注意力机制的Transformer特征提取层、时序特征融合技术和空间金字塔结构，提出了PyCAT4模型，并在COCO和3DPW数据集上验证了改进策略显著提升了人体姿态估计的性能。\",\n  \"motivation\": \"已有研究表明，结合卷积神经网络与金字塔网格对齐反馈环路在3D人体姿态估计中取得显著进展。同时，基于Transformer的时间分析架构也为计算机视觉领域带来了创新突破。因此，本文旨在深入优化Pymaf网络架构。\",\n  \"method\": \"主要改进包括：引入基于自注意力机制的Transformer特征提取层以增强低层次特征捕捉；通过特征时序融合技术增强对视频序列中时序信号的理解和捕捉；应用空间金字塔结构实现多尺度特征融合，有效平衡不同尺度特征表示差异。\",\n  \"result\": \"实验在COCO和3DPW数据集上证明了提出的改进策略显著增强了网络在人体姿态估计中的检测能力。\",\n  \"conclusion\": \"研究提出的PyCAT4模型显著提升了人体姿态估计技术的发展。\n}", "conclusion": ""}}
{"id": "2508.02807", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02807", "abs": "https://arxiv.org/abs/2508.02807", "authors": ["Tongchun Zuo", "Zaiyu Huang", "Shuliang Ning", "Ente Lin", "Chao Liang", "Zerong Zheng", "Jianwen Jiang", "Yuan Zhang", "Mingyuan Gao", "Xin Dong"], "title": "DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework", "comment": "18 pages, 12 figures", "summary": "Video virtual try-on (VVT) technology has garnered considerable academic\ninterest owing to its promising applications in e-commerce advertising and\nentertainment. However, most existing end-to-end methods rely heavily on scarce\npaired garment-centric datasets and fail to effectively leverage priors of\nadvanced visual models and test-time inputs, making it challenging to\naccurately preserve fine-grained garment details and maintain temporal\nconsistency in unconstrained scenarios. To address these challenges, we propose\nDreamVVT, a carefully designed two-stage framework built upon Diffusion\nTransformers (DiTs), which is inherently capable of leveraging diverse unpaired\nhuman-centric data to enhance adaptability in real-world scenarios. To further\nleverage prior knowledge from pretrained models and test-time inputs, in the\nfirst stage, we sample representative frames from the input video and utilize a\nmulti-frame try-on model integrated with a vision-language model (VLM), to\nsynthesize high-fidelity and semantically consistent keyframe try-on images.\nThese images serve as complementary appearance guidance for subsequent video\ngeneration. \\textbf{In the second stage}, skeleton maps together with\nfine-grained motion and appearance descriptions are extracted from the input\ncontent, and these along with the keyframe try-on images are then fed into a\npretrained video generation model enhanced with LoRA adapters. This ensures\nlong-term temporal coherence for unseen regions and enables highly plausible\ndynamic motions. Extensive quantitative and qualitative experiments demonstrate\nthat DreamVVT surpasses existing methods in preserving detailed garment content\nand temporal stability in real-world scenarios. Our project page\nhttps://virtu-lab.github.io/", "AI": {"tldr": "提出DreamVVT框架，解决现有VVT方法在保持精细衣物细节和时间一致性上的困难，实验证明其优于现有方法。", "motivation": "解决现有结束VVT方法依赖稀有配对衣物数据集、未能利用先进视觉模型和测试时输入先验知识的问题，以实现在不受约束情况下准确保持精细衣物细节并保持时间一致性。", "method": "提出DreamVVT，一个两阶段框架：第一阶段利用多帧试穿模型生成关键帧试穿衣图，第二阶段使用增强的LoRA适配器视频生成模型处理关键帧和输入内容，以确保长期时间和动态连贯性。", "result": "Video虚拟试衣(VVT)技术因其在电子商务广告和娱乐中的潜在应用而受到学术界的广泛关注。然而，现有的端到端方法通常依赖于稀有的配对衣物数据集，并且未能充分利用先进视觉模型和测试时输入的先验知识，这导致在不受约束的情况下难以准确保持精细的衣物细节并保持时间一致性。为了解决这些挑战，我们提出了DreamVVT，这是一个基于Diffusion Transformers (DiTs)的精心设计的两阶段框架，能够利用多样化的未配对的人体数据来增强在实际场景中的适应性。在第一阶段，我们从输入视频中采样代表性帧，并利用集成了视觉语言模型(VLM)的多帧试穿模型，生成高保真度和语义一致的关键帧试穿衣图。这些图像作为后续视频生成的补充外观指导。**在第二阶段**，从输入内容中提取骨架图和精细的动作及外观描述，与关键帧试穿衣图一起输入到增强LoRA适配器的预训练视频生成模型中。这确保了长期的时间连贯性，并为未见区域启用高度可信的动态动作。广泛的定量和定性实验证明，DreamVVT在实际情况下优于现有的方法，能够保持详细的衣物内容和时间稳定性。", "conclusion": "通过广泛的实验，证明了DreamVVT相比于现有方法能够更好地保持详细的衣物内容和时间稳定性。"}}
{"id": "2508.02829", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02829", "abs": "https://arxiv.org/abs/2508.02829", "authors": ["Adam Colton"], "title": "Elucidating the Role of Feature Normalization in IJEPA", "comment": null, "summary": "In the standard image joint embedding predictive architecture (IJEPA),\nfeatures at the output of the teacher encoder are layer normalized (LN) before\nserving as a distillation target for the student encoder and predictor. We\npropose that this feature normalization disrupts the natural energy hierarchy\nof visual tokens, where high-energy tokens (those with larger L2 norms) encode\nsemantically important image regions. LN forces all features to have identical\nL2 norms, effectively equalizing their energies and preventing the model from\nprioritizing semantically rich regions. We find that IJEPA models trained with\nfeature LN exhibit loss maps with significant checkerboard-like artifacts. We\npropose that feature LN be replaced with a DynTanh activation as the latter\nbetter preserves token energies and allows high-energy tokens to greater\ncontribute to the prediction loss. We show that IJEPA trained with feature\nDynTanh exhibits a longer-tailed loss distribution and fixes the checkerboard\nartifacts in the loss map. Our empirical results show that our simple\nmodification improves ImageNet linear probe accuracy from 38% to 42.7% for\nViT-Small and reduces RMSE by 0.08 on NYU Depth V2 monocular depth estimation.\nThese results suggest that preserving natural token energies is crucial for\neffective self-supervised visual representation learning.", "AI": {"tldr": "Replacing layer normalization with DynTanh in IJEPA improves self-supervised learning performance in image classification and depth estimation by preserving the natural energy hierarchy of visual tokens.", "motivation": "The motivation is to address the issue that LN disrupts the natural energy hierarchy of visual tokens, which is crucial for effective self-supervised visual representation learning, leading to checkerboard-like artifacts in the loss maps of IJEPA models.", "method": "The paper proposes replacing the layer normalization (LN) in the standard image joint embedding predictive architecture (IJEPA) with a DynTanh activation to preserve the natural energy hierarchy of visual tokens, allowing semantically rich regions to be prioritized.", "result": "The proposed method exhibits a longer-tailed loss distribution, fixes checkerboard artifacts in the loss map, and improves ImageNet linear probe accuracy from 38% to 42.7% for ViT-Small and reduces RMSE by 0.08 on NYU Depth V2 monocular depth estimation.", "conclusion": "Preserving the natural energy hierarchy of visual tokens by using DynTanh instead of LN in IJEPA can improve the model's performance in tasks such as image classification and depth estimation."}}
{"id": "2508.02831", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02831", "abs": "https://arxiv.org/abs/2508.02831", "authors": ["Mikołaj Zieliński", "Krzysztof Byrski", "Tomasz Szczepanik", "Przemysław Spurek"], "title": "GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing", "comment": null, "summary": "Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently\ntransformed 3D scene representation and rendering. NeRF achieves high-fidelity\nnovel view synthesis by learning volumetric representations through neural\nnetworks, but its implicit encoding makes editing and physical interaction\nchallenging. In contrast, GS represents scenes as explicit collections of\nGaussian primitives, enabling real-time rendering, faster training, and more\nintuitive manipulation. This explicit structure has made GS particularly\nwell-suited for interactive editing and integration with physics-based\nsimulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural\nRadiance Fields Interactive Editing), a hybrid model that combines the\nphotorealistic rendering quality of NeRF with the editable and structured\nrepresentation of GS. Instead of using spherical harmonics for appearance\nmodeling, we assign each Gaussian a trainable feature embedding. These\nembeddings are used to condition a NeRF network based on the k nearest\nGaussians to each query point. To make this conditioning efficient, we\nintroduce Ray-Traced Gaussian Proximity Search (RT-GPS), a fast nearest\nGaussian search based on a modified ray-tracing pipeline. We also integrate a\nmulti-resolution hash grid to initialize and update Gaussian features.\nTogether, these components enable real-time, locality-aware editing: as\nGaussian primitives are repositioned or modified, their interpolated influence\nis immediately reflected in the rendered output. By combining the strengths of\nimplicit and explicit representations, GENIE supports intuitive scene\nmanipulation, dynamic interaction, and compatibility with physical simulation,\nbridging the gap between geometry-based editing and neural rendering. The code\ncan be found under (https://github.com/MikolajZielinski/genie)", "AI": {"tldr": "GENIE模型结合NeRF和GS的优势，实现了高质量渲染及实时、直观的3D场景编辑。", "motivation": "解决NeRF内在表达下的编辑和物理交互挑战，同时利用GS在实时渲染和直观操作的优势，通过结合二者的优点来实现更有效的3D场景表示和渲染。", "method": "GENIE结合NeRF的高质量渲染能力和GS的可编辑性强结构表示。它用trainable feature embedding代替球谐函数进行外观建模，并基于RT-GPS实现高效的NeRF网络条件化。同时，利用multi-resolution hash grid初始化和更新Gaussian特征。", "result": "GENIE模型实现了实时局部感知编辑，作为Gaussian primitives重新定位或修改时，其插值影响会立即反映在渲染输出中。这使得GENIE能够支持直观的场景操作和动态交互，同时兼容物理仿真。", "conclusion": "GENIE模型在融合显式和隐式表示方法的同时，实现了高质量渲染、实时编辑和动态交互，为神经渲染和几何编辑桥接了优势，揭示了未来3D场景表示技术的发展方向。"}}
{"id": "2508.02808", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02808", "abs": "https://arxiv.org/abs/2508.02808", "authors": ["Radhika Dua", "Young Joon", "Kwon", "Siddhant Dogra", "Daniel Freedman", "Diana Ruan", "Motaz Nashawaty", "Danielle Rigau", "Daniel Alexander Alber", "Kang Zhang", "Kyunghyun Cho", "Eric Karl Oermann"], "title": "Clinically Grounded Agent-based Report Evaluation: An Interpretable Metric for Radiology Report Generation", "comment": null, "summary": "Radiological imaging is central to diagnosis, treatment planning, and\nclinical decision-making. Vision-language foundation models have spurred\ninterest in automated radiology report generation (RRG), but safe deployment\nrequires reliable clinical evaluation of generated reports. Existing metrics\noften rely on surface-level similarity or behave as black boxes, lacking\ninterpretability. We introduce ICARE (Interpretable and Clinically-grounded\nAgent-based Report Evaluation), an interpretable evaluation framework\nleveraging large language model agents and dynamic multiple-choice question\nanswering (MCQA). Two agents, each with either the ground-truth or generated\nreport, generate clinically meaningful questions and quiz each other. Agreement\non answers captures preservation and consistency of findings, serving as\ninterpretable proxies for clinical precision and recall. By linking scores to\nquestion-answer pairs, ICARE enables transparent, and interpretable assessment.\nClinician studies show ICARE aligns significantly more with expert judgment\nthan prior metrics. Perturbation analyses confirm sensitivity to clinical\ncontent and reproducibility, while model comparisons reveal interpretable error\npatterns.", "AI": {"tldr": "本文提出了ICARE框架，用于可解释的放射学报告评估，改善了自动化报告生成的安全性。", "motivation": "自动化放射学报告生成需要可靠的临床评估，但现有度量标准往往依赖于表面相似性或作为黑箱，缺乏可解释性。", "method": "提出了ICARE（可解释的基于代理的临床报告评估框架），该框架使用大型语言模型代理和动态多项选择问答（MCQA）来生成和回答具有临床意义的问题，以评估生成的报告是否能保持并一致地传达临床发现。", "result": "临床研究表明，ICARE与专家判断的相关性远高于先前的指标。分析还确认了其对临床内容的敏感性和可重复性，并揭示了可解释的错误模式。", "conclusion": "通过将分数与问题-答案对联系起来，ICARE实现了透明和可解释的评估，为自动化放射学报告生成提供了一种可靠的度量标准。"}}
{"id": "2508.02844", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02844", "abs": "https://arxiv.org/abs/2508.02844", "authors": ["Anghong Du", "Nay Aung", "Theodoros N. Arvanitis", "Stefan K. Piechnik", "Joao A C Lima", "Steffen E. Petersen", "Le Zhang"], "title": "RefineSeg: Dual Coarse-to-Fine Learning for Medical Image Segmentation", "comment": null, "summary": "High-quality pixel-level annotations of medical images are essential for\nsupervised segmentation tasks, but obtaining such annotations is costly and\nrequires medical expertise. To address this challenge, we propose a novel\ncoarse-to-fine segmentation framework that relies entirely on coarse-level\nannotations, encompassing both target and complementary drawings, despite their\ninherent noise. The framework works by introducing transition matrices in order\nto model the inaccurate and incomplete regions in the coarse annotations. By\njointly training on multiple sets of coarse annotations, it progressively\nrefines the network's outputs and infers the true segmentation distribution,\nachieving a robust approximation of precise labels through matrix-based\nmodeling. To validate the flexibility and effectiveness of the proposed method,\nwe demonstrate the results on two public cardiac imaging datasets, ACDC and\nMSCMRseg, and further evaluate its performance on the UK Biobank dataset.\nExperimental results indicate that our approach surpasses the state-of-the-art\nweakly supervised methods and closely matches the fully supervised approach.", "AI": {"tldr": "提出了一种仅依赖粗标注的医学图像分割框架，通过转换矩阵处理不准确的区域，实验表明该方法超越了现有的弱监督方法，并接近全监督方法的效果。", "motivation": "高质量的医学图像像素级标注对监督分割任务至关重要，但成本高昂且需要医学专业知识。为此，我们提出了这一新框架，仅使用粗尺度的标注，包括目标和互补绘制，即使它们包含噪声。", "method": "我们提出了一种基于粗标注的从粗糙到精细的分割框架，通过引入转换矩阵来处理粗标注中的不准确和不完整区域，该框架利用多个粗标注集进行联合训练，逐步细化网络的输出，从而推断出真正的分割分布。", "result": "在ACDC、MSCMRseg和UK Biobank三个公开心脏成像数据集上的实验结果表明，所提出的方法在性能上超越了现有的弱监督方法，并接近全监督方法的效果。", "conclusion": "本研究介绍的从粗糙到精细的分割框架展示了其灵活性和有效性，通过仅使用粗标注就能实现接近全监督方法的精确度。"}}
{"id": "2508.02853", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02853", "abs": "https://arxiv.org/abs/2508.02853", "authors": ["Yinuo Xu", "Veronica Derricks", "Allison Earl", "David Jurgens"], "title": "Modeling Annotator Disagreement with Demographic-Aware Experts and Synthetic Perspectives", "comment": "28 pages, 17 figures", "summary": "We present an approach to modeling annotator disagreement in subjective NLP\ntasks through both architectural and data-centric innovations. Our model,\nDEM-MoE (Demographic-Aware Mixture of Experts), routes inputs to expert\nsubnetworks based on annotator demographics, enabling it to better represent\nstructured, group-level variation compared to prior models. DEM-MoE\nconsistently performs competitively across demographic groups, and shows\nespecially strong results on datasets with high annotator disagreement. To\naddress sparse demographic coverage, we test whether LLM-generated synthetic\nannotations via zero-shot persona prompting can be used for data imputation. We\nshow these synthetic judgments align moderately well with human annotations on\nour data and offer a scalable way to potentially enrich training data. We then\npropose and evaluate approaches for blending real and synthetic data using\nstrategies tailored to dataset structure. We find that the optimal strategies\ndepend on dataset structure. Together, these contributions improve the\nrepresentation of diverse perspectives.", "AI": {"tldr": "本文提出了一种新的模型DEM-MoE，通过专家子网络路由和合成注释改进了对注释者分歧的建模效果，使得模型能更好地处理多样化的视角。", "motivation": "本文旨在通过架构和数据层面的创新建模主观NLP任务中的注释者分歧，以更好地处理群体层面的结构化变异性。", "method": "DEM-MoE模型通过基于注释者人口统计学特征的专家子网络路由，以更好地表示结构化和群体层面的变化，优于之前的模型。此外，通过使用大规模语言模型生成的合成注释进行零样本角色提示来测试稀疏人口统计学覆盖的数据填补方法。", "result": "DEM-MoE在各个人口统计组中表现一致，并在注释者意见分歧较大的数据集上表现出特别好的效果。合成注释与人类注释有中等程度的对齐，并为扩充训练数据提供了可扩展的方式。", "conclusion": "这些贡献一起提高了对多样化观点的表示。并且，不同数据集结构的最佳混合真实和合成数据策略有所不同。"}}
{"id": "2508.02858", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02858", "abs": "https://arxiv.org/abs/2508.02858", "authors": ["Tianheng Zhu", "Yiheng Feng"], "title": "MIDAR: Mimicking LiDAR Detection for Traffic Applications with a Lightweight Plug-and-Play Model", "comment": "18 pages, 9 figures", "summary": "As autonomous driving (AD) technology advances, increasing research has\nfocused on leveraging cooperative perception (CP) data collected from multiple\nAVs to enhance traffic applications. Due to the impracticality of large-scale\nreal-world AV deployments, simulation has become the primary approach in most\nstudies. While game-engine-based simulators like CARLA generate high-fidelity\nraw sensor data (e.g., LiDAR point clouds) which can be used to produce\nrealistic detection outputs, they face scalability challenges in multi-AV\nscenarios. In contrast, microscopic traffic simulators such as SUMO scale\nefficiently but lack perception modeling capabilities. To bridge this gap, we\npropose MIDAR, a LiDAR detection mimicking model that approximates realistic\nLiDAR detections using vehicle-level features readily available from\nmicroscopic traffic simulators. Specifically, MIDAR predicts true positives\n(TPs) and false negatives (FNs) from ideal LiDAR detection results based on the\nspatial layouts and dimensions of surrounding vehicles. A Refined Multi-hop\nLine-of-Sight (RM-LoS) graph is constructed to encode the occlusion\nrelationships among vehicles, upon which MIDAR employs a GRU-enhanced APPNP\narchitecture to propagate features from the ego AV and occluding vehicles to\nthe prediction target. MIDAR achieves an AUC of 0.909 in approximating the\ndetection results generated by CenterPoint, a mainstream 3D LiDAR detection\nmodel, on the nuScenes AD dataset. Two CP-based traffic applications further\nvalidate the necessity of such realistic detection modeling, particularly for\ntasks requiring accurate individual vehicle observations (e.g., position,\nspeed, lane index). As demonstrated in the applications, MIDAR can be\nseamlessly integrated into traffic simulators and trajectory datasets and will\nbe open-sourced upon publication.", "AI": {"tldr": "提出了一种称为MIDAR的模拟模型，它使用微观交通模拟器中容易获得的车辆特征来近似生成现实的LiDAR检测。研究结果表明，MIDAR在模拟高保真LiDAR检测方面效果显著。", "motivation": "尽管基于游戏引擎的模拟器如CARLA可以生成高保真的原始传感器数据，但它们在多AV场景中面临可扩展性挑战。而像SUMO这样的微观交通模拟器虽然高效，但缺乏感知建模能力。本研究旨在填补这一空白。", "method": "提出了一种名为MIDAR的LiDAR检测模拟模型，该模型基于微观交通模拟器中易于获得的车辆级特征，近似生成现实的LiDAR检测。MIDAR利用增强的GRU-APPNP架构通过形成精简的多跳视线图（RM-LoS）来传播特征，从而预测理想LiDAR检测结果中的真阳性(TPs)和假阴性(FNs)。", "result": "在nuScenes自动驾驶数据集上，MIDAR能够以0.909的AUC值近似中心点（CenterPoint）生成的检测结果，这是一款广泛使用的3D LiDAR检测模型。通过两个基于CP的交通应用验证了这种现实检测建模的必要性，尤其是在需要精确车辆个体观测的任务中。", "conclusion": "MIDAR可以无缝集成到交通模拟器和轨迹数据集中，并将在发表后开源。"}}
{"id": "2508.02872", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02872", "abs": "https://arxiv.org/abs/2508.02872", "authors": ["Giovanni Cherubin", "Andrew Paverd"], "title": "Highlight & Summarize: RAG without the jailbreaks", "comment": null, "summary": "Preventing jailbreaking and model hijacking of Large Language Models (LLMs)\nis an important yet challenging task. For example, when interacting with a\nchatbot, malicious users can input specially crafted prompts to cause the LLM\nto generate undesirable content or perform a completely different task from its\nintended purpose. Existing mitigations for such attacks typically rely on\nhardening the LLM's system prompt or using a content classifier trained to\ndetect undesirable content or off-topic conversations. However, these\nprobabilistic approaches are relatively easy to bypass due to the very large\nspace of possible inputs and undesirable outputs. In this paper, we present and\nevaluate Highlight & Summarize (H&S), a new design pattern for\nretrieval-augmented generation (RAG) systems that prevents these attacks by\ndesign. The core idea is to perform the same task as a standard RAG pipeline\n(i.e., to provide natural language answers to questions, based on relevant\nsources) without ever revealing the user's question to the generative LLM. This\nis achieved by splitting the pipeline into two components: a highlighter, which\ntakes the user's question and extracts relevant passages (\"highlights\") from\nthe retrieved documents, and a summarizer, which takes the highlighted passages\nand summarizes them into a cohesive answer. We describe several possible\ninstantiations of H&S and evaluate their generated responses in terms of\ncorrectness, relevance, and response quality. Surprisingly, when using an\nLLM-based highlighter, the majority of H&S responses are judged to be better\nthan those of a standard RAG pipeline.", "AI": {"tldr": "本论文提出了一种名为Highlight & Summarize (H&S) 的设计模式，用于增强检索增强生成系统的安全性，通过设计防止恶意用户通过精心设计的提示来让大语言模型生成不期望的内容或偏离其主要任务。此设计确保用户的问题永远不会暴露给生成模型。", "motivation": "防止大规模语言模型被破解和遭受模型劫持是一项重要却充满挑战的任务。现有的缓解措施容易被绕过，因此作者提出了新的设计模式旨在通过设计来防止这些攻击。", "method": "论文提出的方法是将系统分为两个组件：高亮组件和摘要组件，前者针对用户问题从检索文档中提取相关段落，后者则将提取的段落总结成一个连贯的答案。这种方法只需标准RAG系统来提供基于相关来源的自然语言答案，但不会暴露用户的问题。", "result": "作者评估了H&S模式的多个实例，结果表明大多数H&S的回应被评估为比标准RAG管道的回应更优，尤其是当高亮组件使用LLM时表现尤为出色。", "conclusion": "此设计模式通过防止直接向生成模型暴露用户的问题线索，提供了对于防止L层模型遭受恶意利用和模型调制的新方法。"}}
{"id": "2508.02871", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02871", "abs": "https://arxiv.org/abs/2508.02871", "authors": ["J. Alex Hurt", "Trevor M. Bajkowski", "Grant J. Scott", "Curt H. Davis"], "title": "Evaluation and Analysis of Deep Neural Transformers and Convolutional Neural Networks on Modern Remote Sensing Datasets", "comment": null, "summary": "In 2012, AlexNet established deep convolutional neural networks (DCNNs) as\nthe state-of-the-art in CV, as these networks soon led in visual tasks for many\ndomains, including remote sensing. With the publication of Visual Transformers,\nwe are witnessing the second modern leap in computational vision, and as such,\nit is imperative to understand how various transformer-based neural networks\nperform on satellite imagery. While transformers have shown high levels of\nperformance in natural language processing and CV applications, they have yet\nto be compared on a large scale to modern remote sensing data. In this paper,\nwe explore the use of transformer-based neural networks for object detection in\nhigh-resolution electro-optical satellite imagery, demonstrating\nstate-of-the-art performance on a variety of publicly available benchmark data\nsets. We compare eleven distinct bounding-box detection and localization\nalgorithms in this study, of which seven were published since 2020, and all\neleven since 2015. The performance of five transformer-based architectures is\ncompared with six convolutional networks on three state-of-the-art opensource\nhigh-resolution remote sensing imagery datasets ranging in size and complexity.\nFollowing the training and evaluation of thirty-three deep neural models, we\nthen discuss and analyze model performance across various feature extraction\nmethodologies and detection algorithms.", "AI": {"tldr": "本文研究了基于变压器的神经网络在高分辨遥感图像目标检测中的应用，并在三个公开数据集上，比较了基于变压器的系统与传统卷积网络的表现。", "motivation": "随着视觉变压器的出现，计算视觉领域迈出了第二个现代飞跃，本研究旨在探讨基于变压器的神经网络在卫星图像处理中的表现，特别是远距感测数据上的大范围比较还未有人尝试。", "method": "文章中讨论了基于变压器的神经网络和卷积神经网络在高分辨率遥感图像目标检测中的应用。研究中考察了11种边界框检测和定位算法，并且比较了5种变压器架构和6种卷积网络在3个公开的高分辨率遥感数据集上的表现。", "result": "通过训练和评估33个深度神经模型，研究展示了基于变压器的系统在多个高分辨率遥感数据集上表现出相当的竞争性，证实了其在目标检测中的优越性能。", "conclusion": "研究表明，基于变压器的神经网络在高分辨率遥感图象目标检测中能够实现高性能，提供了可能超越了传统卷积网络的新方法。"}}
{"id": "2508.02885", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02885", "abs": "https://arxiv.org/abs/2508.02885", "authors": ["Elliot Murphy", "Rohan Venkatesh", "Edward Khokhlovich", "Andrey Vyshedskiy"], "title": "Merge-based syntax is mediated by distinct neurocognitive mechanisms: A clustering analysis of comprehension abilities in 84,000 individuals with language deficits across nine languages", "comment": null, "summary": "In the modern language sciences, the core computational operation of syntax,\n'Merge', is defined as an operation that combines two linguistic units (e.g.,\n'brown', 'cat') to form a categorized structure ('brown cat', a Noun Phrase).\nThis can then be further combined with additional linguistic units based on\nthis categorial information, respecting non-associativity such that abstract\ngrouping is respected. Some linguists have embraced the view that Merge is an\nelementary, indivisible operation that emerged in a single evolutionary step.\nFrom a neurocognitive standpoint, different mental objects constructed by Merge\nmay be supported by distinct mechanisms: (1) simple command constructions\n(e.g., \"eat apples\"); (2) the merging of adjectives and nouns (\"red boat\"); and\n(3) the merging of nouns with spatial prepositions (\"laptop behind the sofa\").\nHere, we systematically investigate participants' comprehension of sentences\nwith increasing levels of syntactic complexity. Clustering analyses revealed\nbehavioral evidence for three distinct structural types, which we discuss as\npotentially emerging at different developmental stages and subject to selective\nimpairment. While a Merge-based syntax may still have emerged suddenly in\nevolutionary time, responsible for the structured symbolic turn our species\ntook, different cognitive mechanisms seem to underwrite the processing of\nvarious types of Merge-based objects.", "AI": {"tldr": "本研究通过参与者对复杂句子的理解分析了三种不同结构类型的神经认知机制，提示这些结构可能分期出现并受不同机制支持。", "motivation": "研究动机在于探讨Merge作为基本操作在神经认知层面是如何工作的，并考察不同类型的Merge操作是否由不同的认知机制支持。", "method": "本研究通过系统调查参与者对不同语法复杂程度句子的理解，利用聚类分析来揭示行为证据，分为三种不同的结构类型。", "result": "聚类分析揭示了三种不同的结构类型，这些类型可能在不同的发展阶段出现，并且可能受到选择性损伤的影响。", "conclusion": "尽管基于Merge的语法可能在进化中迅速出现，但不同类型的Merge结构可能在发展阶段中分期出现，并且受不同认知机制的影响。"}}
{"id": "2508.02890", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02890", "abs": "https://arxiv.org/abs/2508.02890", "authors": ["Rongxin Jiang", "Robert Long", "Chenghao Gu", "Mingrui Yan"], "title": "VisuCraft: Enhancing Large Vision-Language Models for Complex Visual-Guided Creative Content Generation via Structured Information Extraction", "comment": null, "summary": "This paper introduces VisuCraft, a novel framework designed to significantly\nenhance the capabilities of Large Vision-Language Models (LVLMs) in complex\nvisual-guided creative content generation. Existing LVLMs often exhibit\nlimitations in maintaining high visual fidelity, genuine creativity, and\nprecise adherence to nuanced user instructions when generating long-form texts.\nVisuCraft addresses these challenges by integrating a multimodal structured\ninformation extractor (E) and a dynamic prompt generation module (G). The\nextractor distills fine-grained visual attributes from input images into a\nrich, structured representation, which the dynamic prompt module then combines\nwith user instructions to create highly optimized prompts for underlying LVLMs\n(e.g., LLaVA, InstructBLIP). Evaluated on the self-constructed\nImageStoryGen-500K dataset using VisuGen Metrics (Visual Grounding, Creativity,\nand Instruction Adherence), VisuCraft consistently outperforms baseline LVLMs\nacross tasks like story generation and poetry composition. Our results\ndemonstrate remarkable improvements, particularly in creativity and instruction\nadherence, validating VisuCraft's effectiveness in producing imaginative,\nvisually grounded, and user-aligned long-form creative text. This work unlocks\nnew potential for LVLMs in sophisticated creative AI applications.", "AI": {"tldr": "VisuCraft is a novel framework that integrates a multimodal structured information extractor and dynamic prompt generation module to enhance LVLMs, improving their performance in tasks like story generation and poetry composition.", "motivation": "to significantly enhance the capabilities of LVLMs in complex visual-guided creative content generation, addressing limitations in existing LVLMs.", "method": "integrates a multimodal structured information extractor (E) and a dynamic prompt generation module (G) to enhance LVLMs.", "result": "VisuCraft outperforms baseline LVLMs in creativity and instruction adherence when generating long-form texts such as stories and poetry.", "conclusion": "VisuCraft demonstrates remarkable improvements in producing imaginative, visually grounded, and user-aligned long-form creative text, unlocking new potential for LVLMs in sophisticated creative AI applications."}}
{"id": "2508.02886", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02886", "abs": "https://arxiv.org/abs/2508.02886", "authors": ["Wenjie Luo", "Ruocheng Li", "Shanshan Zhu", "Julian Perry"], "title": "Coherent Multimodal Reasoning with Iterative Self-Evaluation for Vision-Language Models", "comment": null, "summary": "Despite significant advancements, current large language models (LLMs) and\nvision-language models (LVLMs) continue to struggle with complex, multi-step,\ncross-modal common sense reasoning tasks, often exhibiting a lack of\n\"deliberative thinking.\" They tend to rely on superficial associations rather\nthan deep, chained inference, particularly when integrating visual information\nwith abstract concepts. To address this, we propose the Coherent Multimodal\nReasoning Framework (CMRF), a novel approach that enhances LVLMs' common sense\nreasoning capabilities through an iterative, self-evaluating inference\nmechanism. CMRF mimics human problem-solving by decomposing complex queries,\ngenerating step-by-step inferences, and self-correcting errors. Our framework\nintegrates three key modules: a Reasoning Decomposition Unit (RDU) for breaking\ndown problems into sub-questions, a Contextual Inference Engine (CIE) for\ncontextual inference, and a Coherence Assessment Module (CAM) for evaluating\nlogical consistency and confidence. Coupled with an Adaptive Iterative\nRefinement strategy, CMRF systematically refines its reasoning paths. Built\nupon LLaVA-1.6-34B and trained on a novel Multimodal Daily Activity Reasoning\n(MDAR) dataset, CMRF achieves state-of-the-art performance among open-source\nLVLMs on challenging benchmarks like VCR, A-OKVQA, and DailyLife-MRC. It\nattains an average accuracy of 69.4%, surpassing the best open-source baseline\nby +2.4 percentage points, with particular strength in complex reasoning\nscenarios. Extensive ablation studies and human evaluations confirm the\ncritical contributions of each module and the effectiveness of iterative\nrefinement in fostering more coherent and accurate reasoning.", "AI": {"tldr": "研究通过提出一种全新的连贯多模态推理框架（CMRF），旨在解决现有语言和视觉-语言模型在复杂推理任务中表现不佳的问题。该框架结合了多种不同的模块和迭代自我评估的策略，实现了先进的基准测试性能。", "motivation": "由于现有的大语言模型（LLMs）和视觉-语言模型（LVLMs）在处理复杂多步骤、跨模态常识推理任务时表现不佳，常常缺乏“深思熟虑”的能力，倾向于依靠浅层关联而不是深层链式推理，特别是将视觉信息与抽象概念融合时。因此，研究提出的目的是增强这些模型的推理能力。", "method": "研究提出了一种名为连贯多模态推理框架（CMRF）的新方法，该方法通过迭代自我评估推理机制来增强视觉-语言模型的常识推理能力。CMRF 模仿人类解决问题的方式，将复杂查询分解成子问题，生成逐步推理，并自我纠正错误。该框架集成了三个关键模块：推理分解单元（RDU）、上下文推理引擎（CIE）和连贯性评估模块（CAM），并结合自适应迭代细化策略，系统地改进其推理路径。", "result": "基于 LLaVA-1.6-34B，并在新的多模态日常活动推理（MDAR）数据集上训练的 CMRF，在 VCR、A-OKVQA 和 DailyLife-MRC 等具有挑战性的基准测试中取得了最先进的性能。其平均准确率为 69.4%，超过最佳开源基线 +2.4 个百分点，在复杂的推理情景中表现出特别强的性能。", "conclusion": "研究表明，CMRF 通过集成的推理模块和迭代自我评估机制有效提高了视觉-语言模型的复杂多模态推理能力，并在多个基准测试中表现出优于其他开源基线的性能。"}}
{"id": "2508.02903", "categories": ["cs.CV", "68T07", "I.4.9; I.2.10"], "pdf": "https://arxiv.org/pdf/2508.02903", "abs": "https://arxiv.org/abs/2508.02903", "authors": ["Mehrdad Moradi", "Kamran Paynabar"], "title": "RDDPM: Robust Denoising Diffusion Probabilistic Model for Unsupervised Anomaly Segmentation", "comment": "10 pages, 5 figures. Accepted to the ICCV 2025 Workshop on\n  Vision-based Industrial InspectiON (VISION)", "summary": "Recent advancements in diffusion models have demonstrated significant success\nin unsupervised anomaly segmentation. For anomaly segmentation, these models\nare first trained on normal data; then, an anomalous image is noised to an\nintermediate step, and the normal image is reconstructed through backward\ndiffusion. Unlike traditional statistical methods, diffusion models do not rely\non specific assumptions about the data or target anomalies, making them\nversatile for use across different domains. However, diffusion models typically\nassume access to normal data for training, limiting their applicability in\nrealistic settings. In this paper, we propose novel robust denoising diffusion\nmodels for scenarios where only contaminated (i.e., a mix of normal and\nanomalous) unlabeled data is available. By casting maximum likelihood\nestimation of the data as a nonlinear regression problem, we reinterpret the\ndenoising diffusion probabilistic model through a regression lens. Using robust\nregression, we derive a robust version of denoising diffusion probabilistic\nmodels. Our novel framework offers flexibility in constructing various robust\ndiffusion models. Our experiments show that our approach outperforms current\nstate of the art diffusion models, for unsupervised anomaly segmentation when\nonly contaminated data is available. Our method outperforms existing\ndiffusion-based approaches, achieving up to 8.08\\% higher AUROC and 10.37\\%\nhigher AUPRC on MVTec datasets. The implementation code is available at:\nhttps://github.com/mehrdadmoradi124/RDDPM", "AI": {"tldr": "This paper presents a novel robust denoising diffusion probabilistic model specifically designed to handle contaminated datasets for unsupervised anomaly segmentation, outperforming current state-of-the-art models in experiments.", "motivation": "The motivation for this paper is to address the limitation of traditional diffusion models which require access to clean, normal data for training. This is not always possible in realistic settings where only contaminated, unlabeled data is available.", "method": "By interpreting the denoising diffusion probabilistic model as a nonlinear regression problem, the authors use robust regression techniques to develop a robust denoising diffusion probabilistic model that can effectively handle contaminated datasets containing both normal and anomalous data.", "result": "Experiments show the proposed robust denoising diffusion probabilistic model outperforms existing diffusion models when dealing with contaminated data, achieving up to 8.08% higher AUROC and 10.37% higher AUPRC on MVTec datasets.", "conclusion": "The work demonstrates that robust denoising diffusion probabilistic models can be successfully applied to unsupervised anomaly segmentation when only contaminated data is accessible, offering improvements over current diffusion-based methods."}}
{"id": "2508.02901", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02901", "abs": "https://arxiv.org/abs/2508.02901", "authors": ["Osama Khalid", "Sanvesh Srivastava", "Padmini Srinivasan"], "title": "SLIM-LLMs: Modeling of Style-Sensory Language RelationshipsThrough Low-Dimensional Representations", "comment": null, "summary": "Sensorial language -- the language connected to our senses including vision,\nsound, touch, taste, smell, and interoception, plays a fundamental role in how\nwe communicate experiences and perceptions. We explore the relationship between\nsensorial language and traditional stylistic features, like those measured by\nLIWC, using a novel Reduced-Rank Ridge Regression (R4) approach. We demonstrate\nthat low-dimensional latent representations of LIWC features r = 24 effectively\ncapture stylistic information for sensorial language prediction compared to the\nfull feature set (r = 74). We introduce Stylometrically Lean Interpretable\nModels (SLIM-LLMs), which model non-linear relationships between these style\ndimensions. Evaluated across five genres, SLIM-LLMs with low-rank LIWC features\nmatch the performance of full-scale language models while reducing parameters\nby up to 80%.", "AI": {"tldr": "研究采用R4方法探索感官语言与风格特征关系，开发SLIM-LLMs，证明了利用低维度LIWC特征预测感官语言的有效性，且降低了80%的参数量。", "motivation": "研究感官语言如何影响我们的交流体验与感知，并评估在降低参数量的同时保持高性能的语言模型的有效性。", "method": "使用了降低秩的岭回归（R4）方法来探索感官语言与传统风格特征之间的关系，并引入了SLIM-LLMs（简约可解释模型）来模拟这些风格维度之间的非线性关系。", "result": "低维潜在表示（r = 24）的LIWC特征在预测感官语言方面比全特征集（r = 74）更有效地捕捉风格信息，且SLIM-LLMs在降低参数量达80%的同时性能未减。", "conclusion": "证明了简约的风格测定模型可以在多种文体中有效模拟感官语言，同时减少模型参数量。"}}
{"id": "2508.02905", "categories": ["cs.CV", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.02905", "abs": "https://arxiv.org/abs/2508.02905", "authors": ["Mahnoor Fatima Saad", "Ziad Al-Halah"], "title": "How Would It Sound? Material-Controlled Multimodal Acoustic Profile Generation for Indoor Scenes", "comment": "Accepted to ICCV 2025. Project Page:\n  https://mahnoor-fatima-saad.github.io/m-capa.html", "summary": "How would the sound in a studio change with a carpeted floor and acoustic\ntiles on the walls? We introduce the task of material-controlled acoustic\nprofile generation, where, given an indoor scene with specific audio-visual\ncharacteristics, the goal is to generate a target acoustic profile based on a\nuser-defined material configuration at inference time. We address this task\nwith a novel encoder-decoder approach that encodes the scene's key properties\nfrom an audio-visual observation and generates the target Room Impulse Response\n(RIR) conditioned on the material specifications provided by the user. Our\nmodel enables the generation of diverse RIRs based on various material\nconfigurations defined dynamically at inference time. To support this task, we\ncreate a new benchmark, the Acoustic Wonderland Dataset, designed for\ndeveloping and evaluating material-aware RIR prediction methods under diverse\nand challenging settings. Our results demonstrate that the proposed model\neffectively encodes material information and generates high-fidelity RIRs,\noutperforming several baselines and state-of-the-art methods.", "AI": {"tldr": "研究介绍了一种基于用户定义材料配置生成室内声学特性的方法，提出了一种新颖的编码器-解码器模型，并创建了一个新的基准数据集用于评估该方法的性能，结果显示其优于多种基线方法和现有技术。", "motivation": "探索声学模拟的新方法，特别是在不同材料配置下生成室内声学特性的能力。这些信息对于理解材料如何影响音频的传播和音质至关重要。", "method": "使用了一种新颖的编码器-解码器模型，该模型可以从音视频观察中编码场景的关键特性，并根据用户提供的材料规格生成目标房间脉冲响应（RIR）。", "result": "提出的方法能够有效地根据不同的材料配置生成高质量的RIR，优于多种基线和最先进的方法。", "conclusion": "该研究为精确建模不同材料的声学影响提供了一个新工具，有助于改进室内声学环境的模拟与控制。"}}
{"id": "2508.02931", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02931", "abs": "https://arxiv.org/abs/2508.02931", "authors": ["Shengqi Li", "Amarnath Gupta"], "title": "Can LLMs Generate High-Quality Task-Specific Conversations?", "comment": null, "summary": "This paper introduces a parameterization framework for controlling\nconversation quality in large language models. We explore nine key parameters\nacross six dimensions that enable precise specification of dialogue properties.\nThrough experiments with state-of-the-art LLMs, we demonstrate that\nparameter-based control produces statistically significant differences in\ngenerated conversation properties. Our approach addresses challenges in\nconversation generation, including topic coherence, knowledge progression,\ncharacter consistency, and control granularity. The framework provides a\nstandardized method for conversation quality control with applications in\neducation, therapy, customer service, and entertainment. Future work will focus\non implementing additional parameters through architectural modifications and\ndeveloping benchmark datasets for evaluation.", "AI": {"tldr": "论文提出了一种参数化框架，以精确控制大型语言模型的对话质量，并展示了这一方法的有效性和广泛应用前景。", "motivation": "该研究旨在解决对话生成中的挑战，包括主题连贯性、知识进程、人物一致性以及控制粒度。通过参数化的方法，可以更好地控制对话的质量。", "method": "此论文介绍了一种用于控制大型语言模型对话质量的参数化框架。该框架探索了六个维度上的九个关键参数，以精确指定对话属性。", "result": "通过与最先进的LLMs的实验，研究证明，基于参数的控制能够产生对话属性的统计显著差异。", "conclusion": "该框架为对话质量控制提供了一种标准化的方法，具有教育、治疗、客户服务和娱乐等多个应用场景。未来研究将集中在通过架构修改实现更多参数，并开发基准数据集进行评估。"}}
{"id": "2508.02917", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02917", "abs": "https://arxiv.org/abs/2508.02917", "authors": ["Vebjørn Haug Kåsene", "Pierre Lison"], "title": "Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces", "comment": "This paper has been accepted to ICNSLP 2025", "summary": "Vision-and-Language Navigation (VLN) refers to the task of enabling\nautonomous robots to navigate unfamiliar environments by following natural\nlanguage instructions. While recent Large Vision-Language Models (LVLMs) have\nshown promise in this task, most current VLM systems rely on models\nspecifically designed and optimized for navigation, leaving the potential of\noff-the-shelf LVLMs underexplored. Furthermore, while older VLN approaches used\nlow-level action spaces with egocentric views and atomic actions (such as \"turn\nleft\" or \"move forward\"), newer models tend to favor panoramic action spaces\nwith discrete navigable viewpoints. This paper investigates (1) whether\noff-the-shelf LVLMs (fine-tuned without architectural modifications or\nsimulator-based training) can effectively support VLN tasks and (2) whether\nsuch models can support both low-level and panoramic action paradigms. To this\nend, we fine-tune the open-source model Qwen2.5-VL-3B-Instruct on the\nRoom-to-Room (R2R) dataset and evaluate its empirical performance across both\nlow-level and panoramic action spaces. The best resulting model achieves a 41%\nsuccess rate on the R2R test set, demonstrating that while off-the-shelf LVLMs\ncan learn to perform Vision-and-Language Navigation, they still lag behind\nmodels specifically designed for this task.", "AI": {"tldr": "研究评估了现成的大型视觉语言模型（LVLMs）在视觉语言导航（VLN）任务中的性能，发现它们可以在一定程度上执行该任务，但性能不及专门设计的模型。", "motivation": "探讨现成的大型视觉语言模型（LVLMs）是否可以在无需架构修改或模拟训练的情况下有效支持视觉语言导航（VLN）任务，并且是否可以支持低级别和全景动作范式。", "method": "研究使用了开源模型Qwen2.5-VL-3B-Instruct，并在Room-to-Room (R2R) 数据集上进行微调，以评估其在低级别和全景动作空间中的性能表现。", "result": "最佳模型在R2R测试集上达到了41%的成功率，表明现成的LVLMs可以学习执行视觉语言导航任务，但仍然落后于专门为该任务设计的模型。", "conclusion": "现成的LVLMs具有支持VLN任务的潜力，但它们的成功率仍然低于专门为VLN设计的模型。未来的工作可能需要探索架构修改或增加模拟训练以提升性能。"}}
{"id": "2508.02997", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02997", "abs": "https://arxiv.org/abs/2508.02997", "authors": ["Sri Durga Sai Sowmya Kadali", "Evangelos E. Papalexakis"], "title": "CoCoTen: Detecting Adversarial Inputs to Large Language Models through Latent Space Features of Contextual Co-occurrence Tensors", "comment": null, "summary": "The widespread use of Large Language Models (LLMs) in many applications marks\na significant advance in research and practice. However, their complexity and\nhard-to-understand nature make them vulnerable to attacks, especially\njailbreaks designed to produce harmful responses. To counter these threats,\ndeveloping strong detection methods is essential for the safe and reliable use\nof LLMs. This paper studies this detection problem using the Contextual\nCo-occurrence Matrix, a structure recognized for its efficacy in data-scarce\nenvironments. We propose a novel method leveraging the latent space\ncharacteristics of Contextual Co-occurrence Matrices and Tensors for the\neffective identification of adversarial and jailbreak prompts. Our evaluations\nshow that this approach achieves a notable F1 score of 0.83 using only 0.5% of\nlabeled prompts, which is a 96.6% improvement over baselines. This result\nhighlights the strength of our learned patterns, especially when labeled data\nis scarce. Our method is also significantly faster, speedup ranging from 2.3 to\n128.4 times compared to the baseline models. To support future research and\nreproducibility, we have made our implementation publicly available.", "AI": {"tldr": "The paper introduces a novel method for the detection of adversarial and jailbreak prompts in large language models using Contextual Co-occurrence Matrices and Tensors, achieving a high F1 score with minimal labeled data and faster computation time compared to baseline models.", "motivation": "To enhance the security and reliability of large language models by developing a robust method for detecting harmful or adversarial prompts, especially in the context of scarce labeled data.", "method": "Structure", "result": "The proposed method achieved a notable F1 score of 0.83 using only 0.5% of labeled prompts, showing a 96.6% improvement over baseline methods in terms of accuracy and speedup of computations ranging from 2.3 to 128.4 times.", "conclusion": "The paper demonstrates that Contextual Co-occurrence Matrices and Tensors can effectively identify adversarial prompts with minimal labeled data and enhanced speed, providing a significant advancement in the security of large language models."}}
{"id": "2508.02923", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02923", "abs": "https://arxiv.org/abs/2508.02923", "authors": ["Minh-Hai Nguyen", "Edouard Pauwels", "Pierre Weiss"], "title": "How Diffusion Prior Landscapes Shape the Posterior in Blind Deconvolution", "comment": null, "summary": "The Maximum A Posteriori (MAP) estimation is a widely used framework in blind\ndeconvolution to recover sharp images from blurred observations. The estimated\nimage and blur filter are defined as the maximizer of the posterior\ndistribution. However, when paired with sparsity-promoting image priors, MAP\nestimation has been shown to favors blurry solutions, limiting its\neffectiveness. In this paper, we revisit this result using diffusion-based\npriors, a class of models that capture realistic image distributions. Through\nan empirical examination of the prior's likelihood landscape, we uncover two\nkey properties: first, blurry images tend to have higher likelihoods; second,\nthe landscape contains numerous local minimizers that correspond to natural\nimages. Building on these insights, we provide a theoretical analysis of the\nblind deblurring posterior. This reveals that the MAP estimator tends to\nproduce sharp filters (close to the Dirac delta function) and blurry solutions.\nHowever local minimizers of the posterior, which can be obtained with gradient\ndescent, correspond to realistic, natural images, effectively solving the blind\ndeconvolution problem. Our findings suggest that overcoming MAP's limitations\nrequires good local initialization to local minima in the posterior landscape.\nWe validate our analysis with numerical experiments, demonstrating the\npractical implications of our insights for designing improved priors and\noptimization techniques.", "AI": {"tldr": "本文通过经验分析扩散模型先验的似然景观，揭示了MAP估计在盲去模糊中得到模糊解的原因，并通过理论分析和实验证明，局部极小值对应于真实自然图像，可通过适当的初始化找到这些局部极小值来改善盲去模糊效果。", "motivation": "尽管MAP估计被判别图像先验广泛应用于盲去卷积中，但当采用促进稀疏性的判别图像先验时，MAP倾向于产生模糊的结果。本文希望通过使用扩散模型先验，重新审视这一结果，并找到克服MAP局限性的方法。", "method": "通过经验分析扩散模型的似然景观，发现高模糊图像具有更高的似然性，并且景观中存在许多对应于自然图像的局部极小值。基于此，对盲去模糊的后验概率进行了理论分析，揭示了MAP估计器倾向于产生锐化过的滤波器（接近狄拉克delta函数）和模糊解。局部后验极小值，可以通过梯度下降获得，对应于真实且自然的图像，有效解决了盲去模糊问题。", "result": "研究表明，通过优化初始值可以克服MAP估计的局限性，使算法达到局部极小值，实现有效的盲去模糊。数值实验验证了分析结果，表明这些见解对于设计更优的先验模型和优化技术具有实际意义。", "conclusion": "MAP估计在盲去模糊问题中难以获得清晰的重建图像。然而，通过分析扩散模型的后验概率的似然景观，表明局部极小值对应于自然图像，且可通过梯度下降算法找到这些局部极小值。这些结论对改进先验和优化策略具有重要指导意义。"}}
{"id": "2508.03037", "categories": ["cs.CL", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.03037", "abs": "https://arxiv.org/abs/2508.03037", "authors": ["Ariya Mukherjee-Gandhi", "Oliver Muellerklein"], "title": "When Algorithms Meet Artists: Topic Modeling the AI-Art Debate, 2013-2025", "comment": "18 pages, 5 figures, 5 tables", "summary": "As generative AI continues to reshape artistic production and alternate modes\nof human expression, artists whose livelihoods are most directly affected have\nraised urgent concerns about consent, transparency, and the future of creative\nlabor. However, the voices of artists are often marginalized in dominant public\nand scholarly discourse. This study presents a twelve-year analysis, from 2013\nto 2025, of English-language discourse surrounding AI-generated art. It draws\nfrom 439 curated 500-word excerpts sampled from opinion articles, news reports,\nblogs, legal filings, and spoken-word transcripts. Through a reproducible\nmethodology, we identify five stable thematic clusters and uncover a\nmisalignment between artists' perceptions and prevailing media narratives. Our\nfindings highlight how the use of technical jargon can function as a subtle\nform of gatekeeping, often sidelining the very issues artists deem most urgent.\nOur work provides a BERTopic-based methodology and a multimodal baseline for\nfuture research, alongside a clear call for deeper, transparency-driven\nengagement with artist perspectives in the evolving AI-creative landscape.", "AI": {"tldr": "研究通过分析关于AI生成艺术的多年英语讨论，揭示了艺术家与公众叙述之间的差异，强调了对艺术家观点更深层次交流的必要。", "motivation": "探讨人工智能生成艺术引发的同意、透明度和创意劳动未来等紧迫问题，并关注在这个话题中常常被边缘化的艺术家的声音。", "method": "使用BERTopic方法分析了2013年至2025年间的439个英文文本摘录，内容涉及意见文章、新闻报道、博客、法律文件和演讲记录。", "result": "识别出五个稳定的主题群集，并揭示了艺术家视角与主流媒介叙事之间的错位。", "conclusion": "强调技术术语作为一种微妙的守门机制，往往忽视艺术家认为最紧迫的问题。呼吁在不断演化的AI创意领域，通过透明驱动的参与，更深入地了解艺术家的观点。"}}
{"id": "2508.02927", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02927", "abs": "https://arxiv.org/abs/2508.02927", "authors": ["Srikanth Muralidharan", "Heitor R. Medeiros", "Masih Aminbeidokhti", "Eric Granger", "Marco Pedersoli"], "title": "Infrared Object Detection with Ultra Small ConvNets: Is ImageNet Pretraining Still Useful?", "comment": null, "summary": "Many real-world applications require recognition models that are robust to\ndifferent operational conditions and modalities, but at the same time run on\nsmall embedded devices, with limited hardware. While for normal size models,\npre-training is known to be very beneficial in accuracy and robustness, for\nsmall models, that can be employed for embedded and edge devices, its effect is\nnot clear. In this work, we investigate the effect of ImageNet pretraining on\nincreasingly small backbone architectures (ultra-small models, with $<$1M\nparameters) with respect to robustness in downstream object detection tasks in\nthe infrared visual modality. Using scaling laws derived from standard object\nrecognition architectures, we construct two ultra-small backbone families and\nsystematically study their performance. Our experiments on three different\ndatasets reveal that while ImageNet pre-training is still useful, beyond a\ncertain capacity threshold, it offers diminishing returns in terms of\nout-of-distribution detection robustness. Therefore, we advise practitioners to\nstill use pre-training and, when possible avoid too small models as while they\nmight work well for in-domain problems, they are brittle when working\nconditions are different.", "AI": {"tldr": "研究发现对于参数量小于1M的超小型模型，ImageNet预训练虽然仍旧能带来性能提升，但超过某个容量点后，它在提升领域外鲁棒性方面的效果明显减弱。建议实践中仍使用预训练，但也要避免过于小型的模型。", "motivation": "在现实世界的应用中，需要具有强大适应性和鲁棒性的中小型模型，能够运行在嵌入式设备上。尽管已知预训练对于正常大小的模型是很有帮助的，但对于小型模型来说，其作用尚不明确。本研究旨在探究ImageNet预训练对于提高小型模型在红外视觉模态下游目标检测任务中的鲁棒性的影响。", "method": "构建了两种超小型主干网络家族，并系统地研究了它们的表现。这些网络家族基于从标准对象识别架构中推导出的缩放定律。通过在三个不同的数据集上进行实验，评估了ImageNet预训练对这些小型模型的影响。", "result": "", "conclusion": "ImageNet预训练虽然对小型模型依然有用，但是超过一定容量阈值后，它在处理领域外检测鲁棒性方面的收益会逐渐减小。因此作者建议实践者仍然可以使用预训练，但在可能的情况下，避免使用过于小型的模型，因为这些模型虽然可能对领域内问题有良好表现，但在条件不同时会显得不稳定。"}}
{"id": "2508.03098", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03098", "abs": "https://arxiv.org/abs/2508.03098", "authors": ["Haoran Wang", "Xiongxiao Xu", "Baixiang Huang", "Kai Shu"], "title": "Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language Models in Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances the factual accuracy of large\nlanguage models (LLMs) by conditioning outputs on external knowledge sources.\nHowever, when retrieval involves private or sensitive data, RAG systems are\nsusceptible to extraction attacks that can leak confidential information\nthrough generated responses. We propose Privacy-Aware Decoding (PAD), a\nlightweight, inference-time defense that adaptively injects calibrated Gaussian\nnoise into token logits during generation. PAD integrates confidence-based\nscreening to selectively protect high-risk tokens, efficient sensitivity\nestimation to minimize unnecessary noise, and context-aware noise calibration\nto balance privacy with generation quality. A \\renyi Differential Privacy (RDP)\naccountant rigorously tracks cumulative privacy loss, enabling explicit\nper-response $(\\varepsilon, \\delta)$-DP guarantees for sensitive outputs.\nUnlike prior approaches requiring retraining or corpus-level filtering, PAD is\nmodel-agnostic and operates entirely at decoding time with minimal\ncomputational overhead. Experiments on three real-world datasets demonstrate\nthat PAD substantially reduces private information leakage while preserving\nresponse utility, outperforming existing retrieval- and post-processing-based\ndefenses. Our work takes an important step toward mitigating privacy risks in\nRAG via decoding strategies, paving the way for universal and scalable privacy\nsolutions in sensitive domains. Our code is available:\nhttps://github.com/wang2226/PAD.", "AI": {"tldr": "本文提出了一种轻量、推理时的防御策略 Privacy-Aware Decoding (PAD)，在生成私密信息时通过向 token 概率分布中注入噪音来保护隐私，降低了私密信息泄露的风险，同时保持了生成质量。", "motivation": "该论文旨在解决 Retrieval-Augmented Generation (RAG) 在处理私密或敏感数据时，容易发生通过生成的响应泄露私密信息的问题。", "method": "Privacy-Aware Decoding (PAD) 方法通过在生成过程中向 token 的概率分布中注入校准的高斯噪声，以防御提取攻击。PAD 包含基于信心的筛选来保护高风险 token，有效估算敏感性以最小化不必要的噪声，以及基于上下文的噪声校准来平衡隐私和生成质量。", "result": "实验结果显示，PAD 能够显著减少私密信息的泄露，同时保持响应的效用，优于现有的基于检索和后期处理的防御方法。", "conclusion": "通过解码策略缓解隐私风险，PAD 为敏感领域提供了通用和可扩展的隐私解决方案的途径。"}}
{"id": "2508.02944", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02944", "abs": "https://arxiv.org/abs/2508.02944", "authors": ["Chenxu Zhang", "Zenan Li", "Hongyi Xu", "You Xie", "Xiaochen Zhao", "Tianpei Gu", "Guoxian Song", "Xin Chen", "Chao Liang", "Jianwen Jiang", "Linjie Luo"], "title": "X-Actor: Emotional and Expressive Long-Range Portrait Acting from Audio", "comment": "Project Page at https://byteaigc.github.io/X-Actor/", "summary": "We present X-Actor, a novel audio-driven portrait animation framework that\ngenerates lifelike, emotionally expressive talking head videos from a single\nreference image and an input audio clip. Unlike prior methods that emphasize\nlip synchronization and short-range visual fidelity in constrained speaking\nscenarios, X-Actor enables actor-quality, long-form portrait performance\ncapturing nuanced, dynamically evolving emotions that flow coherently with the\nrhythm and content of speech. Central to our approach is a two-stage decoupled\ngeneration pipeline: an audio-conditioned autoregressive diffusion model that\npredicts expressive yet identity-agnostic facial motion latent tokens within a\nlong temporal context window, followed by a diffusion-based video synthesis\nmodule that translates these motions into high-fidelity video animations. By\noperating in a compact facial motion latent space decoupled from visual and\nidentity cues, our autoregressive diffusion model effectively captures\nlong-range correlations between audio and facial dynamics through a\ndiffusion-forcing training paradigm, enabling infinite-length emotionally-rich\nmotion prediction without error accumulation. Extensive experiments demonstrate\nthat X-Actor produces compelling, cinematic-style performances that go beyond\nstandard talking head animations and achieves state-of-the-art results in\nlong-range, audio-driven emotional portrait acting.", "AI": {"tldr": "提出了X-Actor，一种生成逼真表情的讲话人像视频的音频驱动框架，通过结合自回归扩散模型和视频合成模块，实现了长时长、情感丰富的面部动画效果。", "motivation": "之前的面部动画方法在强调唇部同步和短时视觉保真度的同时，没有很好地处理大规模动态情感表达。X-Actor的目标是生成高质量的面部动画，能够情感丰富且在长时间内保持连贯性。", "method": "X-Actor采用两阶段的解耦生成流水线，首先通过音频条件的自回归扩散模型，在长时序上下文中生成表情丰富但不具身份特性的面部动作潜在标记，随后通过基于扩散的视频合成模块把这些动作转化为高保真的视频动画。", "result": "实验表明，X-Actor能够在长时间内生成具有丰富情感的面部动作预测，且效果优于现有方法，能够产生超越一般讲话人像动画的视觉表演。", "conclusion": "X-Actor通过其独特的长时序上下文音频驱动面部动作预测能力和基于扩散的视频合成方法，成功实现了高质量、长时长的情感丰富的面部动画生成。"}}
{"id": "2508.03110", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03110", "abs": "https://arxiv.org/abs/2508.03110", "authors": ["Zizhong Li", "Haopeng Zhang", "Jiawei Zhang"], "title": "Token-Level Precise Attack on RAG: Searching for the Best Alternatives to Mislead Generation", "comment": null, "summary": "While large language models (LLMs) have achieved remarkable success in\nproviding trustworthy responses for knowledge-intensive tasks, they still face\ncritical limitations such as hallucinations and outdated knowledge. To address\nthese issues, the retrieval-augmented generation (RAG) framework enhances LLMs\nwith access to external knowledge via a retriever, enabling more accurate and\nreal-time outputs about the latest events. However, this integration brings new\nsecurity vulnerabilities: the risk that malicious content in the external\ndatabase can be retrieved and used to manipulate model outputs. Although prior\nwork has explored attacks on RAG systems, existing approaches either rely\nheavily on access to the retriever or fail to jointly consider both retrieval\nand generation stages, limiting their effectiveness, particularly in black-box\nscenarios. To overcome these limitations, we propose Token-level Precise Attack\non the RAG (TPARAG), a novel framework that targets both white-box and\nblack-box RAG systems. TPARAG leverages a lightweight white-box LLM as an\nattacker to generate and iteratively optimize malicious passages at the token\nlevel, ensuring both retrievability and high attack success in generation.\nExtensive experiments on open-domain QA datasets demonstrate that TPARAG\nconsistently outperforms previous approaches in retrieval-stage and end-to-end\nattack effectiveness. These results further reveal critical vulnerabilities in\nRAG pipelines and offer new insights into improving their robustness.", "AI": {"tldr": "文章提出了一个新的攻击框架TPARAG，专门针对RAG系统中的检索和生成阶段进行攻击，以解决现有方法依赖检索器或不能同步考虑两阶段的问题，并展示了其有效性和RAG系统的潜在安全性问题。", "motivation": "尽管之前的工作探讨了对RAG系统的攻击，但这些方法要么严重依赖于对检索器的访问，要么无法同时考虑检索和生成阶段，这在黑盒场景中尤其限制了它们的有效性。为了解决这些限制，提出了TPARAG。", "method": "提出了一种名为TPARAG的新框架，该框架利用轻量级白盒LLM作为攻击者，生成并迭代优化恶意文本，在检索和生成阶段都实现高成功率的攻击。", "result": "实验结果表明，TPARAG在检索阶段和端到端攻击效果上都优于之前的方法，揭示了RAG流水线中的关键漏洞，并为提高其鲁棒性提供了新的见解。", "conclusion": "TPARAG框架能够有效针对白盒和黑盒RAG系统，展示了一种更为精确和高效的攻击机制，这对理解和增强RAG系统的安全性具有重要意义。"}}
{"id": "2508.02967", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02967", "abs": "https://arxiv.org/abs/2508.02967", "authors": ["Dawei Zhang", "Xiaojie Guo"], "title": "Towards Robust Image Denoising with Scale Equivariance", "comment": null, "summary": "Despite notable advances in image denoising, existing models often struggle\nto generalize beyond in-distribution noise patterns, particularly when\nconfronted with out-of-distribution (OOD) conditions characterized by spatially\nvariant noise. This generalization gap remains a fundamental yet underexplored\nchallenge. In this work, we investigate \\emph{scale equivariance} as a core\ninductive bias for improving OOD robustness. We argue that incorporating\nscale-equivariant structures enables models to better adapt from training on\nspatially uniform noise to inference on spatially non-uniform degradations.\nBuilding on this insight, we propose a robust blind denoising framework\nequipped with two key components: a Heterogeneous Normalization Module (HNM)\nand an Interactive Gating Module (IGM). HNM stabilizes feature distributions\nand dynamically corrects features under varying noise intensities, while IGM\nfacilitates effective information modulation via gated interactions between\nsignal and feature paths. Extensive evaluations demonstrate that our model\nconsistently outperforms state-of-the-art methods on both synthetic and\nreal-world benchmarks, especially under spatially heterogeneous noise. Code\nwill be made publicly available.", "AI": {"tldr": "本文研究了尺度等方差性作为改善分布外（OOD）鲁棒性的核心归纳偏置，并提出了一种鲁棒的盲去噪框架，该框架分别通过异构归一化模块（HNM）和交互门控模块（IGM）来适应空间均匀和非均匀噪声。", "motivation": "尽管图像去噪已经取得了显著进展，但现有模型在推广至分布外（OOD）噪声模式方面仍然面临挑战，特别是在空间变异噪声条件下。作者旨在探索尺度等方差性作为核心归纳偏置，以提高 OOD 稳健性。", "method": "本文提出了一个鲁棒的盲去噪框架，其包含两个关键组件：异构归一化模块（HNM）和交互门控模块（IGM）。HNM 稳定了特征分布，并在不同噪声强度下动态校正特征，而 IGM 通过信号路径和特征路径之间的门控交互促进有效信息调制。", "result": "模型在合成数据和真实数据上的广泛评估中表现出色，特别是在空间异质噪声条件下。", "conclusion": "实验证明，该模型在合成和真实世界基准上都优于最先进的方法，特别是在空间异质噪声环境中。"}}
{"id": "2508.03112", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.03112", "abs": "https://arxiv.org/abs/2508.03112", "authors": ["Motaz Saad", "David Langlois", "Kamel Smaili"], "title": "Cross-lingual Opinions and Emotions Mining in Comparable Documents", "comment": "16 pages, 5 figures", "summary": "Comparable texts are topic-aligned documents in multiple languages that are\nnot direct translations. They are valuable for understanding how a topic is\ndiscussed across languages. This research studies differences in sentiments and\nemotions across English-Arabic comparable documents. First, texts are annotated\nwith sentiment and emotion labels. We apply a cross-lingual method to label\ndocuments with opinion classes (subjective/objective), avoiding reliance on\nmachine translation. To annotate with emotions (anger, disgust, fear, joy,\nsadness, surprise), we manually translate the English WordNet-Affect (WNA)\nlexicon into Arabic, creating bilingual emotion lexicons used to label the\ncomparable corpora. We then apply a statistical measure to assess the agreement\nof sentiments and emotions in each source-target document pair. This comparison\nis especially relevant when the documents originate from different sources. To\nour knowledge, this aspect has not been explored in prior literature. Our study\nincludes English-Arabic document pairs from Euronews, BBC, and Al-Jazeera\n(JSC). Results show that sentiment and emotion annotations align when articles\ncome from the same news agency and diverge when they come from different ones.\nThe proposed method is language-independent and generalizable to other language\npairs.", "AI": {"tldr": "该研究通过跨语言的情感和情绪标注方法，对比了英语-阿拉伯语新闻中对同一主题的不同讨论方式和情绪差异，方法具有语言独立性和通用性。", "motivation": "该研究旨在探讨跨语言的比较文本在情感和情绪表达上的差异，这些文本虽然在主题上一致，但并非直接翻译。这有助于理解同一主题在不同语言中的讨论方式。", "method": "该研究首先对文本进行情感和情绪标签的标注。采用了一种跨语言的方法来对文件进行主观/客观的意见分类，而不依赖机器翻译。为了进行情绪标注（包括愤怒、厌恶、恐惧、喜悦、悲伤、惊讶），手动将英语的WordNet情感词典（WNA）翻译成阿拉伯语词典，创建了一个多语言的情感词典用于标签标注。接着，使用统计方法来评估每一对源-目标文档在情感和情绪上的一致性。", "result": "研究结果表明，当文章来自同一新闻机构时，情感和情绪标签之间呈现出一致性；反之，如果文章来自不同的机构，则会出现分歧。", "conclusion": "该方法是语言独立的，可以推广到其他语言对。"}}
{"id": "2508.02973", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02973", "abs": "https://arxiv.org/abs/2508.02973", "authors": ["Alakh Desai", "Nuno Vasconcelos"], "title": "Diffusion Models with Adaptive Negative Sampling Without External Resources", "comment": null, "summary": "Diffusion models (DMs) have demonstrated an unparalleled ability to create\ndiverse and high-fidelity images from text prompts. However, they are also\nwell-known to vary substantially regarding both prompt adherence and quality.\nNegative prompting was introduced to improve prompt compliance by specifying\nwhat an image must not contain. Previous works have shown the existence of an\nideal negative prompt that can maximize the odds of the positive prompt. In\nthis work, we explore relations between negative prompting and classifier-free\nguidance (CFG) to develop a sampling procedure, {\\it Adaptive Negative Sampling\nWithout External Resources} (ANSWER), that accounts for both positive and\nnegative conditions from a single prompt. This leverages the internal\nunderstanding of negation by the diffusion model to increase the odds of\ngenerating images faithful to the prompt. ANSWER is a training-free technique,\napplicable to any model that supports CFG, and allows for negative grounding of\nimage concepts without an explicit negative prompts, which are lossy and\nincomplete. Experiments show that adding ANSWER to existing DMs outperforms the\nbaselines on multiple benchmarks and is preferred by humans 2x more over the\nother methods.", "AI": {"tldr": "本文提出了ANSWER技术，通过分类器无关引导改进扩散模型的采样过程，提高了图像生成的质量和准确性，无需训练且优于其他方法。", "motivation": "尽管扩散模型能够从文本提示生成多样化和高质量的图像，但它们在保真度和质量方面存在很大差异。因此，引入了负提示以增强图像对提示的依从性，该方法旨在改进这一过程。", "method": "ANSWER方法通过探索负提示与分类器无关引导之间的关系来改进扩散模型的采样过程，无需外部资源即可同时考虑正负条件，从而提高图像生成的质量和准确性。", "result": "实验结果表明，在多个基准上，ANSWER与现有的扩散模型结合使用优于基线方法，并且人类更偏好该方法，其偏好程度是其他方法的两倍。", "conclusion": "ANSWER是一种无需训练的技术，可以应用于任何支持分类器无关引导的模型，它通过利用扩散模型的内部否定理解来增加生成图像对提示的保真度。"}}
{"id": "2508.03137", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03137", "abs": "https://arxiv.org/abs/2508.03137", "authors": ["Ge Shi", "Kaiyu Huang", "Guochen Feng"], "title": "Long Story Generation via Knowledge Graph and Literary Theory", "comment": null, "summary": "The generation of a long story consisting of several thousand words is a\nsub-task in the field of long text generation~(LTG). Previous research has\naddressed this challenge through outline-based generation, which employs a\nmulti-stage method for generating outlines into stories. However, this approach\nsuffers from two common issues: almost inevitable theme drift caused by the\nloss of memory of previous outlines, and tedious plots with incoherent logic\nthat are less appealing to human readers.\n  In this paper, we propose the multi-agent Story Generator structure to\nimprove the multi-stage method, using large language models~(LLMs) as the core\ncomponents of agents. To avoid theme drift, we introduce a memory storage model\ncomprising two components: a long-term memory storage that identifies the most\nimportant memories, thereby preventing theme drift; and a short-term memory\nstorage that retains the latest outlines from each generation round. To\nincorporate engaging elements into the story, we design a story theme obstacle\nframework based on literary narratology theory that introduces uncertain\nfactors and evaluation criteria to generate outline. This framework calculates\nthe similarity of the former storyline and enhances the appeal of the story by\nbuilding a knowledge graph and integrating new node content. Additionally, we\nestablish a multi-agent interaction stage to simulate writer-reader interaction\nthrough dialogue and revise the story text according to feedback, to ensure it\nremains consistent and logical. Evaluations against previous methods\ndemonstrate that our approach can generate higher-quality long stories.", "AI": {"tldr": "本文提出了一种基于多智能体的故事生成结构，改进了现有的多阶段方法，使用大型语言模型作为核心组件，引入了记忆存储模型和故事情节障碍框架，以防止主题漂移并增加故事的魅力。实验表明，该方法能生成质量更高的长故事。", "motivation": "先前的研究通过基于概要的生成方法，使用多阶段方法将概要转化为故事，但是这种方法存在主题漂移和故事情节不连贯的现象，这样的故事对人类读者的吸引力较低。因此，提出了改进的多阶段故事生成方法。", "method": "通过多智能体故事生成结构来改善多阶段方法，采用大型语言模型作为代理的核心组件。为了防止主题漂移，介绍了包含两个组件的记忆存储模型：长期记忆存储，识别最重要的记忆，从而防止主题漂移；短期记忆存储，保留在每一轮生成中的最新概要。为了将引人入胜的元素融入故事中，基于文学叙事理论设计了一个故事情节障碍框架，引入不确定因素和评估标准来生成概要。此外，建立了多智能体交互阶段，通过对话模拟作者与读者的互动，根据反馈修订故事文本，以确保其保持一致性和逻辑性。", "result": "实验结果表明，与先前的方法相比，本文提出的方法能够生成质量更高的长故事。", "conclusion": "本文所提出的方法通过引入记忆模块和故事情节障碍框架，有效防止了主题漂移，并增强故事的连贯性和逻辑性，从而生成质量更高的长故事。"}}
{"id": "2508.02978", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02978", "abs": "https://arxiv.org/abs/2508.02978", "authors": ["Yusaku Takama", "Ning Ding", "Tatsuya Yokota", "Toru Tamaki"], "title": "Separating Shared and Domain-Specific LoRAs for Multi-Domain Learning", "comment": "9 pages", "summary": "Existing architectures of multi-domain learning have two types of adapters:\nshared LoRA for all domains and domain-specific LoRA for each particular\ndomain. However, it remains unclear whether this structure effectively captures\ndomain-specific information. In this paper, we propose a method that ensures\nthat shared and domain-specific LoRAs exist in different subspaces;\nspecifically, the column and left null subspaces of the pre-trained weights. We\napply the proposed method to action recognition with three datasets (UCF101,\nKinetics400, and HMDB51) and demonstrate its effectiveness in some cases along\nwith the analysis of the dimensions of LoRA weights.", "AI": {"tldr": "本文提出了一种新方法，确保共享和特定领域的LoRA存在于不同的子空间中，并在动作识别方面进行了实验，证明了这种方法的有效性。", "motivation": "现有的多领域学习架构有两种类型的适配器：所有领域共享的LoRA和每个特定领域的领域特定LoRA。然而，这种结构是否能有效捕捉特定领域的信息尚不清楚。", "method": "我们提出了一种方法，确保共享和特定领域的LoRA存在于不同的子空间中，具体而言，是在预训练权重的列子空间和左零子空间中。", "result": "我们通过将所提出的方法应用于动作识别实验，使用三个数据集（UCF101, Kinetics400, 和HMDB51）展现出了方法的有效性。", "conclusion": "通过分析LoRA权重的空间维度，证明所提出的方法能够有效在某些情况下提升模型的能力。"}}
{"id": "2508.03140", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03140", "abs": "https://arxiv.org/abs/2508.03140", "authors": ["Junyao Yang", "Jianwei Wang", "Huiping Zhuang", "Cen Chen", "Ziqian Zeng"], "title": "RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior", "comment": "15 pages, 7 figures", "summary": "Large Language Models (LLMs) with long chain-of-thought (CoT) capability,\ntermed Reasoning Models, demonstrate superior intricate problem-solving\nabilities through multi-step long CoT reasoning. To create a dual-capability\nmodel with long CoT capability and domain-specific knowledge without\nsubstantial computational and data costs, model merging emerges as a highly\nresource-efficient method. However, significant challenges lie in merging\ndomain-specific LLMs with long CoT ones since nowadays merging methods suffer\nfrom reasoning capability degradation, even gibberish output and output\ncollapse. To overcome this, we introduce RCP-Merging: Merging Long\nChain-of-Thought Models with Domain-Specific Models by Considering Reasoning\nCapability as Prior, a novel merging framework designed to integrate\ndomain-specific LLMs with long CoT capability, meanwhile maintaining model\nperformance in the original domain. Treating reasoning model weights as\nfoundational prior, our method utilizes a reasoning capability indicator to\npreserve core long CoT capability model weights while selectively merging\nessential domain-specific weights. We conducted extensive experiments on\nQwen2.5-7B, Llama3.1-8B, and Qwen2.5-1.5B models in BioMedicine and Finance\ndomains. Our results show that RCP-Merging successfully merges a reasoning\nmodel with domain-specific ones, improving domain task performance by 9.5% and\n9.2% over state-of-the-art methods, without significantly harming the original\nlong CoT reasoning capability.", "AI": {"tldr": "This paper introduces RCP-Merging, a method for effectively merging long CoT models with domain-specific models, which improves task performance in specific domains without degrading reasoning capabilities.", "motivation": "To create a dual-capability model with long CoT capability and domain-specific knowledge without substantial computational and data costs, overcoming the challenges of reasoning capability degradation during the merging process.", "method": "RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior, a novel merging framework designed to integrate domain-specific LLMs with long CoT capability, maintaining model performance in the original domain.", "result": "The results show that RCP-Merging successfully merges a reasoning model with domain-specific ones, improving domain task performance by 9.5% and 9.2% over state-of-the-art methods, without significantly harming the original long CoT reasoning capability.", "conclusion": "RCP-Merging is an effective method to merge reasoning models with domain-specific models while maintaining the original model's reasoning capabilities, leading to improved performance in domain tasks."}}
{"id": "2508.02981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02981", "abs": "https://arxiv.org/abs/2508.02981", "authors": ["Takuya Sugimoto", "Ning Ding", "Toru Tamaki"], "title": "MoExDA: Domain Adaptation for Edge-based Action Recognition", "comment": "7 pages", "summary": "Modern action recognition models suffer from static bias, leading to reduced\ngeneralization performance. In this paper, we propose MoExDA, a lightweight\ndomain adaptation between RGB and edge information using edge frames in\naddition to RGB frames to counter the static bias issue. Experiments\ndemonstrate that the proposed method effectively suppresses static bias with a\nlower computational cost, allowing for more robust action recognition than\nprevious approaches.", "AI": {"tldr": "提出MoExDA方法以减轻RGB动作识别模型中的静态偏置问题，实验验证了其有效性。", "motivation": "现代动作识别模型受静态偏置影响，导致泛化性能下降。", "method": "MoExDA, 一种轻量级的RGB和边缘信息之间的领域适应方法，通过在RGB帧的基础上增加边缘帧来应对静态偏置问题。", "result": "实验表明，所提方法能有效抑制静态偏置，并且计算成本更低，相比之前的方法有更稳健的动作识别效果。", "conclusion": "MoExDA方法能在保持较低计算成本的同时，有效改善动作识别的静态偏置问题，展现出更稳健的识别效果。"}}
{"id": "2508.03178", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03178", "abs": "https://arxiv.org/abs/2508.03178", "authors": ["Chenyang Wang", "Liang Wen", "Shousheng Jia", "Xiangzheng Zhang", "Liang Xu"], "title": "Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following", "comment": "12 pages, 10 figures, 7 tables", "summary": "While advancements in the reasoning abilities of LLMs have significantly\nenhanced their performance in solving mathematical problems, coding tasks, and\ngeneral puzzles, their effectiveness in accurately adhering to instructions\nremains inconsistent, particularly with more complex directives. Our\ninvestigation identifies lazy reasoning during the thinking stage as the\nprimary factor contributing to poor instruction adherence. To mitigate this\nissue, we propose a comprehensive framework designed to enable rigorous\nreasoning processes involving preview and self-checking, essential for\nsatisfying strict instruction constraints. Specifically, we first generate\ninstructions with complex constraints and apply a filtering process to obtain\nvalid prompts, resulting in three distinct prompt datasets categorized as hard,\neasy, and pass. Then, we employ rejection sampling on the pass prompts to\ncurate a small yet high-quality dataset, enabling a cold-start initialization\nof the model and facilitating its adaptation to effective reasoning patterns.\nSubsequently, we employ an entropy-preserving supervised fine-tuning\n(Entropy-SFT) strategy coupled with token-wise entropy-adaptive (TEA-RL)\nreinforcement learning guided by rule-based dense rewards. This approach\nencourages the model to transform its reasoning mechanism, ultimately fostering\ngeneralizable reasoning abilities that encompass preview and self-checking.\nExtensive experiments conducted on instruction-following benchmarks demonstrate\nremarkable performance improvements across various model scales. Notably, our\nLight-IF-32B model surpasses both larger open-source models such as DeepSeek-R1\nand closed-source models like Doubao-1.6.", "AI": {"tldr": "研究指出惰性推理是导致模型在处理复杂指令时表现不稳定的主要原因，并提出了一套框架解决这个问题，该框架通过一系列措施改善了模型的推理机制和指令遵循性，实验结果表明性能显著提升。", "motivation": "发现大型语言模型在处理复杂指令时表现不稳定，推理步骤中的惰性推理导致了指令遵守性差。", "method": "提出一个全面的框架，以促进严格的推理过程，其中包括预览和自我检查，这有助于满足严格的指令约束条件。该框架包括生成具有复杂约束的指令，并通过过滤过程获得有效的提示，在此基础上形成三个不同的数据集，再通过拒绝采样技术选取高质量的提示数据集进行模型的冷启动初始化及适应有效的推理模式。此外，还采用了保持熵的监督微调(Entropy-SFT)策略及基于规则的密集奖励引导的令牌自适应熵强化学习(TEA-RL)。", "result": "对指令遵循基准进行的广泛实验表明了显著的性能改进，特别是在各种模型规模上均有改进，其Light-IF-32B模型在性能上超越了DeepSeek-R1和Doubao-1.6。", "conclusion": "实验结果表明，借助这一方法，模型的推理机制将得到改善，能够更好地遵循指令。实验在各种指令遵循基准上展示了显著的性能提升，Light-IF-32B模型在性能上超越了多个大型开源模型和封闭源代码模型。"}}
{"id": "2508.02987", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02987", "abs": "https://arxiv.org/abs/2508.02987", "authors": ["Zachary Yahn", "Selim Furkan Tekin", "Fatih Ilhan", "Sihao Hu", "Tiansheng Huang", "Yichang Xu", "Margaret Loper", "Ling Liu"], "title": "Adversarial Attention Perturbations for Large Object Detection Transformers", "comment": "ICCV 2025", "summary": "Adversarial perturbations are useful tools for exposing vulnerabilities in\nneural networks. Existing adversarial perturbation methods for object detection\nare either limited to attacking CNN-based detectors or weak against\ntransformer-based detectors. This paper presents an Attention-Focused Offensive\nGradient (AFOG) attack against object detection transformers. By design, AFOG\nis neural-architecture agnostic and effective for attacking both large\ntransformer-based object detectors and conventional CNN-based detectors with a\nunified adversarial attention framework. This paper makes three original\ncontributions. First, AFOG utilizes a learnable attention mechanism that\nfocuses perturbations on vulnerable image regions in multi-box detection tasks,\nincreasing performance over non-attention baselines by up to 30.6%. Second,\nAFOG's attack loss is formulated by integrating two types of feature loss\nthrough learnable attention updates with iterative injection of adversarial\nperturbations. Finally, AFOG is an efficient and stealthy adversarial\nperturbation method. It probes the weak spots of detection transformers by\nadding strategically generated and visually imperceptible perturbations which\ncan cause well-trained object detection models to fail. Extensive experiments\nconducted with twelve large detection transformers on COCO demonstrate the\nefficacy of AFOG. Our empirical results also show that AFOG outperforms\nexisting attacks on transformer-based and CNN-based object detectors by up to\n83% with superior speed and imperceptibility. Code is available at\nhttps://github.com/zacharyyahn/AFOG.", "AI": {"tldr": "This paper presents AFOG, an effective adversarial attack method that targets both CNN and transformer-based object detectors, highlighting vulnerabilities with minimal and imperceptible perturbations.", "motivation": "To develop a unified method that effectively attacks both transformer-based and CNN-based object detectors, filling the gap in existing adversarial techniques.", "method": "Attention-Focused Offensive Gradient (AFOG) attack against object detection transformers, using a learnable attention mechanism to focus perturbations on vulnerable image areas.", "result": "Extensive experiments on twelve large detection transformers on COCO show AFOG outperforms existing attacks on both transformer and CNN-based detectors by up to 83%, with better speed and imperceptibility.", "conclusion": "AFOG provides a significant advancement in adversarial attacks against modern object detection systems by offering an effective, unified approach that is also efficient and stealthy."}}
{"id": "2508.03181", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03181", "abs": "https://arxiv.org/abs/2508.03181", "authors": ["Lukas Pätz", "Moritz Beyer", "Jannik Späth", "Lasse Bohlen", "Patrick Zschech", "Mathias Kraus", "Julian Rosenberger"], "title": "Analyzing German Parliamentary Speeches: A Machine Learning Approach for Topic and Sentiment Classification", "comment": "Accepted at 20th International Conference on Wirtschaftsinformatik\n  (WI25); September 2025, M\\\"unster, Germany", "summary": "This study investigates political discourse in the German parliament, the\nBundestag, by analyzing approximately 28,000 parliamentary speeches from the\nlast five years. Two machine learning models for topic and sentiment\nclassification were developed and trained on a manually labeled dataset. The\nmodels showed strong classification performance, achieving an area under the\nreceiver operating characteristic curve (AUROC) of 0.94 for topic\nclassification (average across topics) and 0.89 for sentiment classification.\nBoth models were applied to assess topic trends and sentiment distributions\nacross political parties and over time. The analysis reveals remarkable\nrelationships between parties and their role in parliament. In particular, a\nchange in style can be observed for parties moving from government to\nopposition. While ideological positions matter, governing responsibilities also\nshape discourse. The analysis directly addresses key questions about the\nevolution of topics, sentiment dynamics, and party-specific discourse\nstrategies in the Bundestag.", "AI": {"tldr": "本研究使用机器学习模型分析德意志联邦议会的约28000次演讲，模型显示出很强的分类性能，并揭示了政党角色变化时的话语风格显著关系。", "motivation": "研究动机是探索德国联邦议院（Bundestag）中政党之间的主题趋势和情绪分布，特别是揭示不同政党在政府和反对派角色之间变动时的风格变化。", "method": "本研究通过分析过去五年中大约28000次德国联邦议院的演讲来研究政治话语。开发了两个用于主题和情绪分类的机器学习模型，并在一个手动标注的数据集上训练了这些模型。", "result": "开发的模型在主题分类上达到了平均AUROC为0.94，在情绪分类上达到了0.89，表现优异。分析表明，政党的意识形态立场确实重要，但执政责任同样影响其话语风格。", "conclusion": "本研究直接解决有关德意志联邦议会中的主题变迁、情绪动态以及政党特定话语策略的关键问题。"}}
{"id": "2508.03006", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03006", "abs": "https://arxiv.org/abs/2508.03006", "authors": ["Fan Yang", "Yihao Huang", "Jiayi Zhu", "Ling Shi", "Geguang Pu", "Jin Song Dong", "Kailong Wang"], "title": "Seeing It Before It Happens: In-Generation NSFW Detection for Diffusion-Based Text-to-Image Models", "comment": "8 pages", "summary": "Diffusion-based text-to-image (T2I) models enable high-quality image\ngeneration but also pose significant risks of misuse, particularly in producing\nnot-safe-for-work (NSFW) content. While prior detection methods have focused on\nfiltering prompts before generation or moderating images afterward, the\nin-generation phase of diffusion models remains largely unexplored for NSFW\ndetection. In this paper, we introduce In-Generation Detection (IGD), a simple\nyet effective approach that leverages the predicted noise during the diffusion\nprocess as an internal signal to identify NSFW content. This approach is\nmotivated by preliminary findings suggesting that the predicted noise may\ncapture semantic cues that differentiate NSFW from benign prompts, even when\nthe prompts are adversarially crafted. Experiments conducted on seven NSFW\ncategories show that IGD achieves an average detection accuracy of 91.32% over\nnaive and adversarial NSFW prompts, outperforming seven baseline methods.", "AI": {"tldr": "The paper introduces IGD, a technique using the predicted noise during diffusion to detect NSFW content, achieving high accuracy across various categories.", "motivation": "The motivation behind this paper is to address the challenge of detecting NSFW content during the image generation phase of diffusion models, which has not been explored much before.", "method": "In-Generation Detection (IGD) leverages the predicted noise during the diffusion process to identify NSFW content, aiming to detect NSFW even in adversarially crafted prompts.", "result": "Experiments show IGD achieves an average detection accuracy of 91.32% over naive and adversarial NSFW prompts, exceeding seven baseline methods.", "conclusion": "IGD demonstrates strong effectiveness in detecting NSFW content during the generation phase of diffusion-based T2I models, providing a novel internal signal approach for this purpose."}}
{"id": "2508.03199", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03199", "abs": "https://arxiv.org/abs/2508.03199", "authors": ["Muhammed Saeed", "Shaina Raza", "Ashmal Vayani", "Muhammad Abdul-Mageed", "Ali Emami", "Shady Shehata"], "title": "Beyond Content: How Grammatical Gender Shapes Visual Representation in Text-to-Image Models", "comment": null, "summary": "Research on bias in Text-to-Image (T2I) models has primarily focused on\ndemographic representation and stereotypical attributes, overlooking a\nfundamental question: how does grammatical gender influence visual\nrepresentation across languages? We introduce a cross-linguistic benchmark\nexamining words where grammatical gender contradicts stereotypical gender\nassociations (e.g., ``une sentinelle'' - grammatically feminine in French but\nreferring to the stereotypically masculine concept ``guard''). Our dataset\nspans five gendered languages (French, Spanish, German, Italian, Russian) and\ntwo gender-neutral control languages (English, Chinese), comprising 800 unique\nprompts that generated 28,800 images across three state-of-the-art T2I models.\nOur analysis reveals that grammatical gender dramatically influences image\ngeneration: masculine grammatical markers increase male representation to 73\\%\non average (compared to 22\\% with gender-neutral English), while feminine\ngrammatical markers increase female representation to 38\\% (compared to 28\\% in\nEnglish). These effects vary systematically by language resource availability\nand model architecture, with high-resource languages showing stronger effects.\nOur findings establish that language structure itself, not just content, shapes\nAI-generated visual outputs, introducing a new dimension for understanding bias\nand fairness in multilingual, multimodal systems.", "AI": {"tldr": "研究发现了语法性别对AI生成图像中性别表示的显著影响，显示了理解多语言和多模态系统公平性问题的新角度。", "motivation": "研究动机在于探索语法性别如何影响视觉表现，特别是在不同语言中表现出的性格局部代表情况，填补了现有研究的空白。", "method": "采用了包含800个独特提示词的数据集，涵盖了五种有性别的语言和两种无性别的控制语言，分析了三种最先进的文本生成图像模型生成的图像。", "result": "该研究揭示了在多语言和多模态系统中，语法性别对AI生成的视觉输出有显著影响，提出了理解系统偏差和公平性的新维度。研究采用跨语言基准测试，跨越五种有性别区分的语言和两种没有性别区分的语言，分析了三种最先进文本到图像模型生成的28,800张图像，展示了语法性别对生成图像中性别表示的显著影响。", "conclusion": "研究结论指出语言结构本身会塑造AI生成的视觉输出，这为理解和解决这些系统中的偏差和公平性问题提供了新的研究维度。"}}
{"id": "2508.03007", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03007", "abs": "https://arxiv.org/abs/2508.03007", "authors": ["Xinhui Li", "Xiaojie Guo"], "title": "Multi-Granularity Feature Calibration via VFM for Domain Generalized Semantic Segmentation", "comment": null, "summary": "Domain Generalized Semantic Segmentation (DGSS) aims to improve the\ngeneralization ability of models across unseen domains without access to target\ndata during training. Recent advances in DGSS have increasingly exploited\nvision foundation models (VFMs) via parameter-efficient fine-tuning strategies.\nHowever, most existing approaches concentrate on global feature fine-tuning,\nwhile overlooking hierarchical adaptation across feature levels, which is\ncrucial for precise dense prediction. In this paper, we propose\nMulti-Granularity Feature Calibration (MGFC), a novel framework that performs\ncoarse-to-fine alignment of VFM features to enhance robustness under domain\nshifts. Specifically, MGFC first calibrates coarse-grained features to capture\nglobal contextual semantics and scene-level structure. Then, it refines\nmedium-grained features by promoting category-level feature discriminability.\nFinally, fine-grained features are calibrated through high-frequency spatial\ndetail enhancement. By performing hierarchical and granularity-aware\ncalibration, MGFC effectively transfers the generalization strengths of VFMs to\nthe domain-specific task of DGSS. Extensive experiments on benchmark datasets\ndemonstrate that our method outperforms state-of-the-art DGSS approaches,\nhighlighting the effectiveness of multi-granularity adaptation for the semantic\nsegmentation task of domain generalization.", "AI": {"tldr": "本文提出了一种新的框架MGFC，通过多层次特征校准来提高视觉基础模型在不同领域下的语义分割任务中的适应性。", "motivation": "现有的域泛化语义分割方法大多集中在全局特征调整，忽略了层次化适应在特征级别上对密集预测任务的重要性。", "method": "MGFC方法通过从粗到细对视觉基础模型特征进行对齐，先校准粗粒度特征以捕获全局语义和场景结构，接着细化中粒度特征提升分类特征的判别性，最后通过增强高频空间细节校准细粒度特征。", "result": "实验结果表明，MGFC在基准数据集上超过了现有的DGSS方法。", "conclusion": "多粒度适应策略显著提升了语义分割任务的域泛化性能。"}}
{"id": "2508.03204", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03204", "abs": "https://arxiv.org/abs/2508.03204", "authors": ["Abhirup Sinha", "Pritilata Saha", "Tithi Saha"], "title": "Current State in Privacy-Preserving Text Preprocessing for Domain-Agnostic NLP", "comment": "To be published in the Proceedings of Die Studierendenkonferenz\n  Informatik (SKILL) 2024", "summary": "Privacy is a fundamental human right. Data privacy is protected by different\nregulations, such as GDPR. However, modern large language models require a huge\namount of data to learn linguistic variations, and the data often contains\nprivate information. Research has shown that it is possible to extract private\ninformation from such language models. Thus, anonymizing such private and\nsensitive information is of utmost importance. While complete anonymization may\nnot be possible, a number of different pre-processing approaches exist for\nmasking or pseudonymizing private information in textual data. This report\nfocuses on a few of such approaches for domain-agnostic NLP tasks.", "AI": {"tldr": "本文探讨了几种用于遮盖或伪匿名化文本数据中私人信息的预处理方法，特别聚焦于与领域无关的自然语言处理任务。", "motivation": "隐私是一项基本的人权。在需要大量数据来学习语言变化的现代大型语言模型中，保护数据隐私变得尤为重要，因为这些数据往往包含私人信息。研究表明，有可能从这些语言模型中提取私人信息。因此，匿名化这些私人和敏感信息至关重要。", "method": "探讨了几种在文本数据中遮盖或伪匿名化私人信息的预处理方法，特别关注于与领域无关的自然语言处理任务。", "result": "虽然完全匿名化可能无法实现，但本文报告了几种对与领域无关的自然语言处理任务中私人信息遮盖或伪匿名化的预处理方法。", "conclusion": "该研究强调了在数据处理阶段对私人和敏感信息进行保护的重要性，并建议使用特定的预处理方法来达到这一目的。"}}
{"id": "2508.03009", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03009", "abs": "https://arxiv.org/abs/2508.03009", "authors": ["Xuyi Yang", "Wenhao Zhang", "Hongbo Jin", "Lin Liu", "Hongbo Xu", "Yongwei Nie", "Fei Yu", "Fei Ma"], "title": "Enhancing Long Video Question Answering with Scene-Localized Frame Grouping", "comment": null, "summary": "Current Multimodal Large Language Models (MLLMs) often perform poorly in long\nvideo understanding, primarily due to resource limitations that prevent them\nfrom processing all video frames and their associated information. Efficiently\nextracting relevant information becomes a challenging task. Existing frameworks\nand evaluation tasks focus on identifying specific frames containing core\nobjects from a large number of irrelevant frames, which does not align with the\npractical needs of real-world applications. To address this issue, we propose a\nnew scenario under the video question-answering task, SceneQA, which emphasizes\nscene-based detail perception and reasoning abilities. And we develop the LVSQA\ndataset to support the SceneQA task, which is built upon carefully selected\nvideos from LVBench and contains a new collection of question-answer pairs to\npromote a more fair evaluation of MLLMs' scene perception abilities in long\nvideos. Inspired by human cognition, we introduce a novel method called SLFG.\nThe core idea of SLFG is to combine individual frames into semantically\ncoherent scene frames. By leveraging scene localization methods and dynamic\nframe reassembly mechanisms, SLFG significantly enhances the understanding\ncapabilities of existing MLLMs in long videos. SLFG requires no modification to\nthe original model architecture and boasts excellent plug-and-play usability.\nExperimental results show that this method performs exceptionally well in\nseveral long video benchmark tests. Code and dataset will be released at\nhttp://www.slfg.pkuzwh.cn.", "AI": {"tldr": "提出了一种新的视频问答任务场景SceneQA和相应的LVSQA数据集，以及一种增强MLLMs在长视频理解能力的方法SLFG，该方法结合场景定位和动态帧重组机制，显著提升了长视频理解能力。", "motivation": "当前的多模态大型语言模型在长视频理解方面表现不佳，主要是由于资源限制不能处理所有的视频帧及其相关信息，现有的框架和评估任务侧重于从大量不必要的帧中识别包含核心对象的具体帧，这不适用于实际应用。为了应对这一问题，提出了一种新的视频问答任务场景，即SceneQA，重点在于场景细节感知和推理能力。", "method": "SLFG (Semantic Localized Frame Grouping) 方法，该方法通过结合场景定位技术和动态帧重组机制，将独立帧合并为语义连贯的场景帧，以增强现有MLLMs在长视频理解中的能力。", "result": "实验结果表明，SLFG方法在多个长视频基准测试中表现出色。", "conclusion": "SLFG方法无需修改原始模型架构，具有良好的可插拔性。代码和数据集将在http://www.slfg.pkuzwh.cn上发布。"}}
{"id": "2508.03211", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03211", "abs": "https://arxiv.org/abs/2508.03211", "authors": ["Pablo J. Diego-Simón", "Emmanuel Chemla", "Jean-Rémi King", "Yair Lakretz"], "title": "Probing Syntax in Large Language Models: Successes and Remaining Challenges", "comment": null, "summary": "The syntactic structures of sentences can be readily read-out from the\nactivations of large language models (LLMs). However, the ``structural probes''\nthat have been developed to reveal this phenomenon are typically evaluated on\nan indiscriminate set of sentences. Consequently, it remains unclear whether\nstructural and/or statistical factors systematically affect these syntactic\nrepresentations. To address this issue, we conduct an in-depth analysis of\nstructural probes on three controlled benchmarks. Our results are three-fold.\nFirst, structural probes are biased by a superficial property: the closer two\nwords are in a sentence, the more likely structural probes will consider them\nas syntactically linked. Second, structural probes are challenged by linguistic\nproperties: they poorly represent deep syntactic structures, and get interfered\nby interacting nouns or ungrammatical verb forms. Third, structural probes do\nnot appear to be affected by the predictability of individual words. Overall,\nthis work sheds light on the current challenges faced by structural probes.\nProviding a benchmark made of controlled stimuli to better evaluate their\nperformance.", "AI": {"tldr": "研究对语言模型中句法结构的表示开展了深入分析，揭示了结构探针的挑战，强调了句法表示的表层属性和语言属性的影响，同时也为结构探针提供了更好的评估基准。", "motivation": "研究团队发现，目前使用的结构探针是在非选择性的句子集上进行评估的，这使得系统因素是否影响这些句法表示尚不清楚。", "method": "采用结构探针在三个控制基准上进行深入分析，以探讨句法结构表示的问题。", "result": "1. 结构探针偏向于一个表面属性：句子中的两个词越接近，结构探针越可能认为它们在句法上是链接的。2. 结构探针受到语言属性的挑战：它们无法很好地表示深层句法结构，并且受到互相作用的名词或不合语法的动词形式的干扰。3. 结构探针似乎不受个别词语可预测性的影响。", "conclusion": "这项工作揭示了当前结构探针面临的挑战，并提供了一个由控制刺激构成的基准，以更好地评估它们的性能。"}}
{"id": "2508.03017", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03017", "abs": "https://arxiv.org/abs/2508.03017", "authors": ["Liheng Zhang", "Weihao Yu", "Zubo Lu", "Haozhi Gu", "Jin Huang"], "title": "SA-3DGS: A Self-Adaptive Compression Method for 3D Gaussian Splatting", "comment": "9 pages, 7 figures. Under review at AAAI 2026", "summary": "Recent advancements in 3D Gaussian Splatting have enhanced efficient and\nhigh-quality novel view synthesis. However, representing scenes requires a\nlarge number of Gaussian points, leading to high storage demands and limiting\npractical deployment. The latest methods facilitate the compression of Gaussian\nmodels but struggle to identify truly insignificant Gaussian points in the\nscene, leading to a decline in subsequent Gaussian pruning, compression\nquality, and rendering performance. To address this issue, we propose SA-3DGS,\na method that significantly reduces storage costs while maintaining rendering\nquality. SA-3DGS learns an importance score to automatically identify the least\nsignificant Gaussians in scene reconstruction, thereby enabling effective\npruning and redundancy reduction. Next, the importance-aware clustering module\ncompresses Gaussians attributes more accurately into the codebook, improving\nthe codebook's expressive capability while reducing model size. Finally, the\ncodebook repair module leverages contextual scene information to repair the\ncodebook, thereby recovering the original Gaussian point attributes and\nmitigating the degradation in rendering quality caused by information loss.\nExperimental results on several benchmark datasets show that our method\nachieves up to 66x compression while maintaining or even improving rendering\nquality. The proposed Gaussian pruning approach is not only adaptable to but\nalso improves other pruning-based methods (e.g., LightGaussian), showcasing\nexcellent performance and strong generalization ability.", "AI": {"tldr": "SA-3DGS：通过重要性评分剪枝和压缩高斯点，提高渲染质量和压缩效率，实现实用性的大幅提升。", "motivation": "虽然现有方法可以压缩高斯模型，但在识别真正不重要的高斯点方面存在困难，导致后续的高斯剪枝、压缩质量和渲染性能下降。为解决这一问题，提出了SA-3DGS方法，以显著降低存储成本，同时保持渲染质量。", "method": "SA-3DGS, 通过学习重要性评分来自动识别最小化高斯点，实现有效的剪枝和冗余减少。重要性感知聚类模块将高斯属性更准确地压缩到代码本中，提高代码本的表现力，减少模型大小。代码本修复模块利用场景上下文信息修复代码本，恢复原始高斯点属性，降低因信息丢失带来的渲染质量退化。", "result": "实验结果表明，该方法能够在保持或改善渲染质量的同时，实现高达66倍的压缩率。提出的高斯剪枝方法不仅适用于其他剪枝方法（如LightGaussian），还展示了优秀的性能和强大的泛化能力。", "conclusion": "SA-3DGS方法通过学习重要性评分进行有效的高斯点剪枝和压缩，同时使用代码本修复模块恢复渲染质量，能够在大幅压缩模型大小的同时保持或改善渲染质量。"}}
{"id": "2508.03240", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03240", "abs": "https://arxiv.org/abs/2508.03240", "authors": ["Mutaz Ayesh", "Nicolás Gutiérrez-Rolón", "Fernando Alva-Manchego"], "title": "CardiffNLP at CLEARS-2025: Prompting Large Language Models for Plain Language and Easy-to-Read Text Rewriting", "comment": null, "summary": "This paper details the CardiffNLP team's contribution to the CLEARS shared\ntask on Spanish text adaptation, hosted by IberLEF 2025. The shared task\ncontained two subtasks and the team submitted to both. Our team took an\nLLM-prompting approach with different prompt variations. While we initially\nexperimented with LLaMA-3.2, we adopted Gemma-3 for our final submission, and\nlanded third place in Subtask 1 and second place in Subtask 2. We detail our\nnumerous prompt variations, examples, and experimental results.", "AI": {"tldr": "CardiffNLP团队使用基于LLM提示的方法参与了西班牙语文本改编的共享任务，在两个子任务中分别获得了第三名和第二名。", "motivation": "参与由IberLEF 2025举办的西班牙语文本改编共享任务，该任务包含两个子任务。", "method": "采用了基于LLM提示的方法，尝试了不同的提示变化。最初使用了LLaMA-3.2，最终提交时使用了Gemma-3模型。", "result": "在子任务1中获得了第三名，在子任务2中获得第二名。", "conclusion": "详细的介绍了各种提示变化，示例及实验结果。"}}
{"id": "2508.03034", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03034", "abs": "https://arxiv.org/abs/2508.03034", "authors": ["Qi Xie", "Yongjia Ma", "Donglin Di", "Xuehao Gao", "Xun Yang"], "title": "MoCA: Identity-Preserving Text-to-Video Generation via Mixture of Cross Attention", "comment": null, "summary": "Achieving ID-preserving text-to-video (T2V) generation remains challenging\ndespite recent advances in diffusion-based models. Existing approaches often\nfail to capture fine-grained facial dynamics or maintain temporal identity\ncoherence. To address these limitations, we propose MoCA, a novel Video\nDiffusion Model built on a Diffusion Transformer (DiT) backbone, incorporating\na Mixture of Cross-Attention mechanism inspired by the Mixture-of-Experts\nparadigm. Our framework improves inter-frame identity consistency by embedding\nMoCA layers into each DiT block, where Hierarchical Temporal Pooling captures\nidentity features over varying timescales, and Temporal-Aware Cross-Attention\nExperts dynamically model spatiotemporal relationships. We further incorporate\na Latent Video Perceptual Loss to enhance identity coherence and fine-grained\ndetails across video frames. To train this model, we collect CelebIPVid, a\ndataset of 10,000 high-resolution videos from 1,000 diverse individuals,\npromoting cross-ethnicity generalization. Extensive experiments on CelebIPVid\nshow that MoCA outperforms existing T2V methods by over 5% across Face\nsimilarity.", "AI": {"tldr": "MoCA, a novel Video Diffusion Model with a special attention mechanism and hierarchical temporal pooling, outperforms current text-to-video generation methods by enhancing identity coherence and fine-grained details.", "motivation": "The motivation behind this study is to address the challenge of maintaining identity coherence and capturing fine-grained facial dynamics in text-to-video (T2V) generation where existing models are still lacking, despite recent advances.", "method": "The paper proposes MoCA, a Video Diffusion Model built on a Diffusion Transformer (DiT) backbone. It incorporates a Mixture of Cross-Attention mechanism and applies Hierarchical Temporal Pooling and Temporal-Aware Cross-Attention Experts to improve inter-frame identity consistency and fine-grained facial dynamics. A Latent Video Perceptual Loss is also used to enhance identity coherence and fine-grained details.", "result": "Experiments on a newly collected dataset, CelebIPVid, demonstrate MoCA's superior performance over existing T2V methods in terms of identity coherence and fine-grained facial dynamics.", "conclusion": "MoCA outperforms existing text-to-video (T2V) methods by more than 5% in Face similarity measurements, indicating better performance in maintaining temporal identity coherence and capturing fine-grained facial dynamics."}}
{"id": "2508.03247", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.03247", "abs": "https://arxiv.org/abs/2508.03247", "authors": ["Shintaro Sakai", "Jisun An", "Migyeong Kang", "Haewoon Kwak"], "title": "Somatic in the East, Psychological in the West?: Investigating Clinically-Grounded Cross-Cultural Depression Symptom Expression in LLMs", "comment": null, "summary": "Prior clinical psychology research shows that Western individuals with\ndepression tend to report psychological symptoms, while Eastern individuals\nreport somatic ones. We test whether Large Language Models (LLMs), which are\nincreasingly used in mental health, reproduce these cultural patterns by\nprompting them with Western or Eastern personas. Results show that LLMs largely\nfail to replicate the patterns when prompted in English, though prompting in\nmajor Eastern languages (i.e., Chinese, Japanese, and Hindi) improves alignment\nin several configurations. Our analysis pinpoints two key reasons for this\nfailure: the models' low sensitivity to cultural personas and a strong,\nculturally invariant symptom hierarchy that overrides cultural cues. These\nfindings reveal that while prompt language is important, current\ngeneral-purpose LLMs lack the robust, culture-aware capabilities essential for\nsafe and effective mental health applications.", "AI": {"tldr": "研究发现，当以东方语言（中文、日语和印地语）提示时，LLMs 更能识别文化差异，但总体上仍未能很好地再现心理疾病症状的文化差异，揭示了这些模型缺乏文化感知能力。", "motivation": "鉴于LLMs在心理健康领域的应用越来越普及，研究旨在测试这些模型是否能够识别并再现心理疾病症状的文化差异，即西方人倾向于报告心理症状，而东方人则报告身体症状。", "method": "通过向大型语言模型（LLMs）提供西方或东方的人物设定来测试这些模型是否会在心理健康领域重现这些文化模式。", "result": "实验结果显示，当以英语进行提示时，LLMs 大体上未能重现这些模式，然而在以主要的东方语言（即中文、日语和印地语）提示时，尽管在某些设置中有所改善，但仍未能完全对齐。", "conclusion": "研究揭示了，虽然提示语的语言很重要，但目前的通用 LLM 缺乏健壮的文化感知能力，这对于心理健康的运用是安全和有效的。"}}
{"id": "2508.03039", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.03039", "abs": "https://arxiv.org/abs/2508.03039", "authors": ["Yiran Meng", "Junhong Ye", "Wei Zhou", "Guanghui Yue", "Xudong Mao", "Ruomei Wang", "Baoquan Zhao"], "title": "VideoForest: Person-Anchored Hierarchical Reasoning for Cross-Video Question Answering", "comment": null, "summary": "Cross-video question answering presents significant challenges beyond\ntraditional single-video understanding, particularly in establishing meaningful\nconnections across video streams and managing the complexity of multi-source\ninformation retrieval. We introduce VideoForest, a novel framework that\naddresses these challenges through person-anchored hierarchical reasoning. Our\napproach leverages person-level features as natural bridge points between\nvideos, enabling effective cross-video understanding without requiring\nend-to-end training. VideoForest integrates three key innovations: 1) a\nhuman-anchored feature extraction mechanism that employs ReID and tracking\nalgorithms to establish robust spatiotemporal relationships across multiple\nvideo sources; 2) a multi-granularity spanning tree structure that\nhierarchically organizes visual content around person-level trajectories; and\n3) a multi-agent reasoning framework that efficiently traverses this\nhierarchical structure to answer complex cross-video queries. To evaluate our\napproach, we develop CrossVideoQA, a comprehensive benchmark dataset\nspecifically designed for person-centric cross-video analysis. Experimental\nresults demonstrate VideoForest's superior performance in cross-video reasoning\ntasks, achieving 71.93% accuracy in person recognition, 83.75% in behavior\nanalysis, and 51.67% in summarization and reasoning, significantly\noutperforming existing methods. Our work establishes a new paradigm for\ncross-video understanding by unifying multiple video streams through\nperson-level features, enabling sophisticated reasoning across distributed\nvisual information while maintaining computational efficiency.", "AI": {"tldr": "VideoForest是一个通过人为主体的层次化推理方法来解决跨视频理解问题的新型框架，基于人为主体的特征来联系多个视频，通过多粒度跨度树结构和多代理推理框架达到高效理解，并在跨视频理解任务中表现出色。", "motivation": "跨视频问答比传统的单视频理解更具挑战性，特别是在建立视频流间的有意义联系和管理多源信息检索的复杂性方面。", "method": "通过人为主体的层次化推理方法，VideoForest框架解决了跨视频理解的挑战，包括使用ReID和跟踪算法进行人为主体的特征提取，建立多视频源间稳定的空间时间关系；基于人为主体轨迹的多粒度跨度树结构；以及高效遍历此层次结构的多代理推理框架。", "result": "在跨视频理解任务中，VideoForest显示出了卓越的性能，达到了71.93%的人脸识别，83.75%的行为分析和51.67%的总结和推理的准确率，显著优于现有方法。", "conclusion": "通过将多视频流统一到人为主体的特征中，VideoForest提供了一个新的跨视频理解范式，可以对分布式的视觉信息进行复杂推理，同时保持计算效率。"}}
{"id": "2508.03250", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03250", "abs": "https://arxiv.org/abs/2508.03250", "authors": ["Deborah Dore", "Elena Cabrio", "Serena Villata"], "title": "RooseBERT: A New Deal For Political Language Modelling", "comment": null, "summary": "The increasing amount of political debates and politics-related discussions\ncalls for the definition of novel computational methods to automatically\nanalyse such content with the final goal of lightening up political\ndeliberation to citizens. However, the specificity of the political language\nand the argumentative form of these debates (employing hidden communication\nstrategies and leveraging implicit arguments) make this task very challenging,\neven for current general-purpose pre-trained Language Models. To address this\nissue, we introduce a novel pre-trained Language Model for political discourse\nlanguage called RooseBERT. Pre-training a language model on a specialised\ndomain presents different technical and linguistic challenges, requiring\nextensive computational resources and large-scale data. RooseBERT has been\ntrained on large political debate and speech corpora (8K debates, each composed\nof several sub-debates on different topics) in English. To evaluate its\nperformances, we fine-tuned it on four downstream tasks related to political\ndebate analysis, i.e., named entity recognition, sentiment analysis, argument\ncomponent detection and classification, and argument relation prediction and\nclassification. Our results demonstrate significant improvements over\ngeneral-purpose Language Models on these four tasks, highlighting how\ndomain-specific pre-training enhances performance in political debate analysis.\nWe release the RooseBERT language model for the research community.", "AI": {"tldr": "RooseBERT, a specialized pre-trained Language Model for political discourse, shows improved performance over general-purpose models in analyzing political debates.", "motivation": "The complexity and specificity of political language necessitate specialized computational methods to better analyze political debates and provide insights to citizens.", "method": "We introduce RooseBERT, a pre-trained Language Model for political discourse, trained on large political debate and speech corpora. The model is fine-tuned for four downstream tasks including named entity recognition, sentiment analysis, argument component detection and classification, and argument relation prediction and classification.", "result": "Fine-tuning RooseBERT on four downstream tasks related to political debate analysis showed significant improvements over general-purpose Language Models.", "conclusion": "RooseBERT demonstrates significant advancements in the analysis of political debates, proving the value of domain-specific pre-training in enhancing model performance."}}
{"id": "2508.03050", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03050", "abs": "https://arxiv.org/abs/2508.03050", "authors": ["Zeyu Zhu", "Weijia Wu", "Mike Zheng Shou"], "title": "Multi-human Interactive Talking Dataset", "comment": "9 pages, 4 figures, 4 tables", "summary": "Existing studies on talking video generation have predominantly focused on\nsingle-person monologues or isolated facial animations, limiting their\napplicability to realistic multi-human interactions. To bridge this gap, we\nintroduce MIT, a large-scale dataset specifically designed for multi-human\ntalking video generation. To this end, we develop an automatic pipeline that\ncollects and annotates multi-person conversational videos. The resulting\ndataset comprises 12 hours of high-resolution footage, each featuring two to\nfour speakers, with fine-grained annotations of body poses and speech\ninteractions. It captures natural conversational dynamics in multi-speaker\nscenario, offering a rich resource for studying interactive visual behaviors.\nTo demonstrate the potential of MIT, we furthur propose CovOG, a baseline model\nfor this novel task. It integrates a Multi-Human Pose Encoder (MPE) to handle\nvarying numbers of speakers by aggregating individual pose embeddings, and an\nInteractive Audio Driver (IAD) to modulate head dynamics based on\nspeaker-specific audio features. Together, these components showcase the\nfeasibility and challenges of generating realistic multi-human talking videos,\nestablishing MIT as a valuable benchmark for future research. The code is\navalibale at: https://github.com/showlab/Multi-human-Talking-Video-Dataset.", "AI": {"tldr": "The paper presents MIT, a new dataset for multi-human talking video generation, and introduces CovOG, a baseline model that establishes this as a valuable benchmark for future research.", "motivation": "The primary motivation is to address the limitation of existing studies that focus on single-person monologues or isolated facial animations, thereby expanding the applicability to more realistic multi-human interactions.", "method": "The paper introduces a dataset called MIT for multi-human talking video generation, which includes an automatic pipeline for collecting and annotating multi-person conversational videos. Furthermore, it proposes a baseline model called CovOG, integrating Multi-Human Pose Encoder (MPE) and Interactive Audio Driver (IAD).", "result": "The MIT dataset comprises 12 hours of high-resolution footage of 2-4 speakers with fine-grained annotations, demonstrating interactive visual behaviors. The CovOG model effectively generates multi-human talking videos.", "conclusion": "The MIT dataset and the CovOG model together demonstrate the potential and challenges of generating realistic multi-human talking videos, setting a benchmark for future research in this area."}}
{"id": "2508.03259", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03259", "abs": "https://arxiv.org/abs/2508.03259", "authors": ["Duzhen Zhang", "Chenxing Li", "Jiahua Dong", "Qi Liu", "Dong Yu"], "title": "Exploring Stability-Plasticity Trade-offs for Continual Named Entity Recognition", "comment": "Accepted by IEEE/ACM Transactions on Audio, Speech and Language\n  Processing", "summary": "Continual Named Entity Recognition (CNER) is an evolving field that focuses\non sequentially updating an existing model to incorporate new entity types.\nPrevious CNER methods primarily utilize Knowledge Distillation (KD) to preserve\nprior knowledge and overcome catastrophic forgetting, strictly ensuring that\nthe representations of old and new models remain consistent. Consequently, they\noften impart the model with excessive stability (i.e., retention of old\nknowledge) but limited plasticity (i.e., acquisition of new knowledge). To\naddress this issue, we propose a Stability-Plasticity Trade-off (SPT) method\nfor CNER that balances these aspects from both representation and weight\nperspectives. From the representation perspective, we introduce a pooling\noperation into the original KD, permitting a level of plasticity by\nconsolidating representation dimensions. From the weight perspective, we\ndynamically merge the weights of old and new models, strengthening old\nknowledge while maintaining new knowledge. During this fusion, we implement a\nweight-guided selective mechanism to prioritize significant weights. Moreover,\nwe develop a confidence-based pseudo-labeling approach for the current\nnon-entity type, which predicts entity types using the old model to handle the\nsemantic shift of the non-entity type, a challenge specific to CNER that has\nlargely been ignored by previous methods. Extensive experiments across ten CNER\nsettings on three benchmark datasets demonstrate that our SPT method surpasses\nprevious CNER approaches, highlighting its effectiveness in achieving a\nsuitable stability-plasticity trade-off.", "AI": {"tldr": "本文提出了一种新的CNER方法，通过调整模型的表示和权重方面的稳定性和可塑性，解决了之前方法存在的过度稳定性和不足的学习能力问题，实验结果表明该方法优于以往的方法。", "motivation": "解决之前CNER方法中模型对旧知识过稳定性而对新知识适应性不足的问题，实现记忆与学习新知识之间的平衡。", "method": "SPT方法通过调整表示和权重来平衡模型的记忆和学习新知识的能力。在表示层面，通过在原始知识蒸馏中引入池化操作来增加模型可塑性；在权重层面，通过动态合并老模型和新模型的权重来增强旧知识的同时保持新知识。此外，对于CNER特有的非实体类型语义漂移挑战，提出了基于置信度的伪标签方法来预测实体类型。", "result": "在三个基准数据集上进行的十种CNER设置的实验表明，SPT方法优于以往的CNER方法，证明了其在实现合适稳定性和可塑性之间的平衡方面的有效性。", "conclusion": "本文提出的方法成功地调整了模型的记忆和学习新知识的能力，提升了CNER性能。"}}
{"id": "2508.03055", "categories": ["cs.CV", "cs.AI", "I.4.8"], "pdf": "https://arxiv.org/pdf/2508.03055", "abs": "https://arxiv.org/abs/2508.03055", "authors": ["Hyebin Cho", "Jaehyup Lee"], "title": "Uncertainty-Guided Face Matting for Occlusion-Aware Face Transformation", "comment": "Accepted to ACM MM 2025. 9 pages, 8 figures, 6 tables", "summary": "Face filters have become a key element of short-form video content, enabling\na wide array of visual effects such as stylization and face swapping. However,\ntheir performance often degrades in the presence of occlusions, where objects\nlike hands, hair, or accessories obscure the face. To address this limitation,\nwe introduce the novel task of face matting, which estimates fine-grained alpha\nmattes to separate occluding elements from facial regions. We further present\nFaceMat, a trimap-free, uncertainty-aware framework that predicts high-quality\nalpha mattes under complex occlusions. Our approach leverages a two-stage\ntraining pipeline: a teacher model is trained to jointly estimate alpha mattes\nand per-pixel uncertainty using a negative log-likelihood (NLL) loss, and this\nuncertainty is then used to guide the student model through spatially adaptive\nknowledge distillation. This formulation enables the student to focus on\nambiguous or occluded regions, improving generalization and preserving semantic\nconsistency. Unlike previous approaches that rely on trimaps or segmentation\nmasks, our framework requires no auxiliary inputs making it well-suited for\nreal-time applications. In addition, we reformulate the matting objective by\nexplicitly treating skin as foreground and occlusions as background, enabling\nclearer compositing strategies. To support this task, we newly constructed\nCelebAMat, a large-scale synthetic dataset specifically designed for\nocclusion-aware face matting. Extensive experiments show that FaceMat\noutperforms state-of-the-art methods across multiple benchmarks, enhancing the\nvisual quality and robustness of face filters in real-world, unconstrained\nvideo scenarios. The source code and CelebAMat dataset are available at\nhttps://github.com/hyebin-c/FaceMat.git", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.03262", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03262", "abs": "https://arxiv.org/abs/2508.03262", "authors": ["Junhyuk Choi", "Hyeonchu Park", "Haemin Lee", "Hyebeen Shin", "Hyun Joung Jin", "Bugeun Kim"], "title": "Pay What LLM Wants: Can LLM Simulate Economics Experiment with 522 Real-human Persona?", "comment": "Preprint", "summary": "Recent advances in Large Language Models (LLMs) have generated significant\ninterest in their capacity to simulate human-like behaviors, yet most studies\nrely on fictional personas rather than actual human data. We address this\nlimitation by evaluating LLMs' ability to predict individual economic\ndecision-making using Pay-What-You-Want (PWYW) pricing experiments with real\n522 human personas. Our study systematically compares three state-of-the-art\nmultimodal LLMs using detailed persona information from 522 Korean participants\nin cultural consumption scenarios. We investigate whether LLMs can accurately\nreplicate individual human choices and how persona injection methods affect\nprediction performance. Results reveal that while LLMs struggle with precise\nindividual-level predictions, they demonstrate reasonable group-level\nbehavioral tendencies. Also, we found that commonly adopted prompting\ntechniques are not much better than naive prompting methods; reconstruction of\npersonal narrative nor retrieval augmented generation have no significant gain\nagainst simple prompting method. We believe that these findings can provide the\nfirst comprehensive evaluation of LLMs' capabilities on simulating economic\nbehavior using real human data, offering empirical guidance for persona-based\nsimulation in computational social science.", "AI": {"tldr": "研究评估了大型语言模型在预测个体经济决策上的能力，发现虽然这些模型在个体水平的预测上困难，但在群体水平上表现出一定的行为倾向，并且常见的提示技术并不比简单的提示方法效果好。", "motivation": "大多数关于LLMs的研究依赖于虚构的人设，而不是实际的人类数据。此研究旨在通过利用真实的人设数据评估LLMs在经济决策模拟中的能力，并系统比较了三种最先进多模态LLMs。", "method": "研究使用了522个韩国参与者的详细人设信息，在文化消费场景下进行Pay-What-You-Want（PWYW）定价实验，调查了LLMs对个体人类选择的准确复制能力及人设注入方法对预测性能的影响。", "result": "结果表明虽然LLMs在个体层面的预测上存在困难，但在群体行为倾向上表现合理。同时，发现常用的提示技巧在效果上并没有比简单的提示方法有显著优势。", "conclusion": "研究为基于真实人类数据模拟经济行为的LLMs能力提供了首个全面评估，为计算社会科学中基于人设的模拟提供了实证指导。"}}
{"id": "2508.03060", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03060", "abs": "https://arxiv.org/abs/2508.03060", "authors": ["Lekang Wen", "Jing Xiao", "Liang Liao", "Jiajun Chen", "Mi Wang"], "title": "CHARM: Collaborative Harmonization across Arbitrary Modalities for Modality-agnostic Semantic Segmentation", "comment": null, "summary": "Modality-agnostic Semantic Segmentation (MaSS) aims to achieve robust scene\nunderstanding across arbitrary combinations of input modality. Existing methods\ntypically rely on explicit feature alignment to achieve modal homogenization,\nwhich dilutes the distinctive strengths of each modality and destroys their\ninherent complementarity. To achieve cooperative harmonization rather than\nhomogenization, we propose CHARM, a novel complementary learning framework\ndesigned to implicitly align content while preserving modality-specific\nadvantages through two components: (1) Mutual Perception Unit (MPU), enabling\nimplicit alignment through window-based cross-modal interaction, where\nmodalities serve as both queries and contexts for each other to discover\nmodality-interactive correspondences; (2) A dual-path optimization strategy\nthat decouples training into Collaborative Learning Strategy (CoL) for\ncomplementary fusion learning and Individual Enhancement Strategy (InE) for\nprotected modality-specific optimization. Experiments across multiple datasets\nand backbones indicate that CHARM consistently outperform the baselines, with\nsignificant increment on the fragile modalities. This work shifts the focus\nfrom model homogenization to harmonization, enabling cross-modal\ncomplementarity for true harmony in diversity.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.03275", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03275", "abs": "https://arxiv.org/abs/2508.03275", "authors": ["Jiahao Zhao"], "title": "LECTOR: LLM-Enhanced Concept-based Test-Oriented Repetition for Adaptive Spaced Learning", "comment": "15 pages, 4 figures, 1 table", "summary": "Spaced repetition systems are fundamental to efficient learning and memory\nretention, but existing algorithms often struggle with semantic interference\nand personalized adaptation. We present LECTOR (\\textbf{L}LM-\\textbf{E}nhanced\n\\textbf{C}oncept-based \\textbf{T}est-\\textbf{O}riented \\textbf{R}epetition), a\nnovel adaptive scheduling algorithm specifically designed for test-oriented\nlearning scenarios, particularly language examinations where success rate is\nparamount. LECTOR leverages large language models for semantic analysis while\nincorporating personalized learning profiles, addressing the critical challenge\nof semantic confusion in vocabulary learning by utilizing LLM-powered semantic\nsimilarity assessment and integrating it with established spaced repetition\nprinciples. Our comprehensive evaluation against six baseline algorithms\n(SSP-MMC, SM2, HLR, FSRS, ANKI, THRESHOLD) across 100 simulated learners over\n100 days demonstrates significant improvements: LECTOR achieves a 90.2\\%\nsuccess rate compared to 88.4\\% for the best baseline (SSP-MMC), representing a\n2.0\\% relative improvement. The algorithm shows particular strength in handling\nsemantically similar concepts, reducing confusion-induced errors while\nmaintaining computational efficiency. Our results establish LECTOR as a\npromising direction for intelligent tutoring systems and adaptive learning\nplatforms.", "AI": {"tldr": "本文提出了一种名为LECTOR的自适应学习算法，该算法在语言学习测试方面实现了更高的成功率达到90.2%，比现有的最好算法提高了2.0%。", "motivation": "现有的间隔重复算法经常在语义干扰和个性化适应方面遇到困难。本文针对这一问题，提出了一种新的算法LECTOR，旨在提高学习效率和记忆保留。", "method": "LECTOR是一种新型的自适应计划算法，专门用于测试导向的学习场景，特别是语言考试，重点是成功的通过率。该算法利用大型语言模型进行语义分析，并结合个性化学习档案，通过利用LLM支持的语义相似性评估来解决词汇学习中的语义混淆问题，并与现有的间隔重复原则相结合。", "result": "通过对六个基线算法（SSP-MMC，SM2，HLR，FSRS，ANKI，THRESHOLD）进行综合评估，结果显示LECTOR实现了90.2％的成功率，比最好的基线算法SSP-MMC提高了2.0％。", "conclusion": "LECTOR在处理语义相似概念方面表现出色，减少了语义混淆引起的错误，同时保持了计算效率。研究结果确立了LECTOR作为智能辅导系统和自适应学习平台的有前途的方向。"}}
{"id": "2508.03064", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03064", "abs": "https://arxiv.org/abs/2508.03064", "authors": ["Trinh Quoc Nguyen", "Oky Dicky Ardiansyah Prima", "Katsuyoshi Hotta"], "title": "CORE-ReID: Comprehensive Optimization and Refinement through Ensemble fusion in Domain Adaptation for person re-identification", "comment": null, "summary": "This study introduces a novel framework, \"Comprehensive Optimization and\nRefinement through Ensemble Fusion in Domain Adaptation for Person\nRe-identification (CORE-ReID)\", to address an Unsupervised Domain Adaptation\n(UDA) for Person Re-identification (ReID). The framework utilizes CycleGAN to\ngenerate diverse data that harmonizes differences in image characteristics from\ndifferent camera sources in the pre-training stage. In the fine-tuning stage,\nbased on a pair of teacher-student networks, the framework integrates\nmulti-view features for multi-level clustering to derive diverse pseudo labels.\nA learnable Ensemble Fusion component that focuses on fine-grained local\ninformation within global features is introduced to enhance learning\ncomprehensiveness and avoid ambiguity associated with multiple pseudo-labels.\nExperimental results on three common UDAs in Person ReID demonstrate\nsignificant performance gains over state-of-the-art approaches. Additional\nenhancements, such as Efficient Channel Attention Block and Bidirectional Mean\nFeature Normalization mitigate deviation effects and adaptive fusion of global\nand local features using the ResNet-based model, further strengthening the\nframework. The proposed framework ensures clarity in fusion features, avoids\nambiguity, and achieves high ac-curacy in terms of Mean Average Precision,\nTop-1, Top-5, and Top-10, positioning it as an advanced and effective solution\nfor the UDA in Person ReID. Our codes and models are available at\nhttps://github.com/TrinhQuocNguyen/CORE-ReID.", "AI": {"tldr": "CORE-ReID, a novel framework introduced for UDA in Person Re-identification, significantly enhances performance in terms of methods such as CycleGAN, teacher-student networks, and an Ensemble Fusion component aiding in achieving clarity in feature fusion and high accuracy for re-identification.", "motivation": "The aim of the research is to enhance the performance of UDA in Person Re-identification by addressing the issue of different data characteristics from different camera sources and by deriving effective pseudo labels to achieve high accuracy in re-identification tasks.", "method": "The paper introduces CORE-ReID, a framework designed for Unsupervised Domain Adaptation (UDA) in Person Re-identification (ReID). It employs CycleGAN in the pre-training phase for simulating diverse data from different camera angles. Two stages of teachers-student network learning and a novel Ensemble Fusion component that utilizes fine-grained local information alongside global features are implemented for successful adaptation and comprehensive learning. Enhancements include Efficient Channel Attention Block and Bidirectional Mean Feature Normalization.", "result": "The experimental results indicate a notable improvement over state-of-the-art approaches in terms of Mean Average Precision and other top-k measures.", "conclusion": "The proposed framework showcases a significant advancement in the UDA for Person ReID, ensuring clear feature fusion, avoiding ambiguity, and achieving high accuracy in re-identification tasks, positioning it as an effective solution for the UDA problem in Person ReID."}}
{"id": "2508.03276", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03276", "abs": "https://arxiv.org/abs/2508.03276", "authors": ["Terra Blevins", "Susanne Schmalwieser", "Benjamin Roth"], "title": "Do language models accommodate their users? A study of linguistic convergence", "comment": null, "summary": "While large language models (LLMs) are generally considered proficient in\ngenerating language, how similar their language usage is to that of humans\nremains understudied. In this paper, we test whether models exhibit linguistic\nconvergence, a core pragmatic element of human language communication, asking:\ndo models adapt, or converge, to the linguistic patterns of their user? To\nanswer this, we systematically compare model completions of exisiting dialogues\nto the original human responses across sixteen language models, three dialogue\ncorpora, and a variety of stylometric features. We find that models strongly\nconverge to the conversation's style, often significantly overfitting relative\nto the human baseline. While convergence patterns are often feature-specific,\nwe observe consistent shifts in convergence across modeling settings, with\ninstruction-tuned and larger models converging less than their pretrained\ncounterparts. Given the differences between human and model convergence\npatterns, we hypothesize that the underlying mechanisms for these behaviors are\nvery different.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.03069", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03069", "abs": "https://arxiv.org/abs/2508.03069", "authors": ["Bo Zhang", "Yifan Zhang", "Shuo Yan", "Yu Bai", "Zheng Zhang", "Wu Liu", "Xiuzhuang Zhou", "Wendong Wang"], "title": "SSFMamba: Symmetry-driven Spatial-Frequency Feature Fusion for 3D Medical Image Segmentation", "comment": null, "summary": "In light of the spatial domain's limited capacity for modeling global context\nin 3D medical image segmentation, emerging approaches have begun to incorporate\nfrequency domain representations. However, straightforward feature extraction\nstrategies often overlook the unique properties of frequency domain\ninformation, such as conjugate symmetry. They also fail to account for the\nfundamental differences in data distribution between the spatial and frequency\ndomains, which can ultimately dilute or obscure the complementary strengths\nthat frequency-based representations offer. In this paper, we propose SSFMamba,\na Mamba based Symmetry-driven Spatial-Frequency feature fusion network for 3D\nmedical image segmentation. SSFMamba employs a complementary dual-branch\narchitecture that extracts features from both the spatial and frequency\ndomains, and leverages a Mamba block to fuse these heterogeneous features to\npreserve global context while reinforcing local details. In the frequency\ndomain branch, we harness Mamba's exceptional capability to extract global\ncontextual information in conjunction with the synergistic effect of frequency\ndomain features to further enhance global modeling. Moreover, we design a 3D\nmulti-directional scanning mechanism to strengthen the fusion of local and\nglobal cues. Extensive experiments on the BraTS2020 and BraTS2023 datasets\ndemonstrate that our approach consistently outperforms state-of-the-art methods\nacross various evaluation metrics.", "AI": {"tldr": "提出了SSFMamba，一种基于Mamba驱动的对称空间-频率特征融合网络，用于3D医学图像分割。", "motivation": "由于空间域在3D医学图像分割中对全局上下文建模的容量有限，新兴的方法开始结合频率域表示。然而，常规的特征提取策略常常忽略频率域信息的独特性质，如共轭对称性，以及空间域和频率域数据分布的基本差异，这可能最终会稀释或掩盖基于频率表示的互补优势。", "method": "SSFMamba采用了一种互补的双分支架构，分别从空间域和频率域提取特征，并利用Mamba块融合这些异构特征，以保留全局上下文并强化局部细节。在频率域分支中，通过Mamba强大的提取全局上下文信息的能力，以及频率域特征的协同效应，进一步增强全局建模能力。此外，设计了一种3D多方向扫描机制来加强局部和全局线索的融合。", "result": "在BraTS2020和BraTS2023数据集上的广泛实验表明，该方法在各种评估指标上始终优于现有的最先进的方法。", "conclusion": "SSFMamba通过融合空间域和频率域特征，能够有效地保留全局上下文并强化局部细节，从而在3D医学图像分割中表现优于现有方法。"}}
{"id": "2508.03292", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03292", "abs": "https://arxiv.org/abs/2508.03292", "authors": ["Shahed Masoudian", "Gustavo Escobedo", "Hannah Strauss", "Markus Schedl"], "title": "Investigating Gender Bias in LLM-Generated Stories via Psychological Stereotypes", "comment": "Under Review", "summary": "As Large Language Models (LLMs) are increasingly used across different\napplications, concerns about their potential to amplify gender biases in\nvarious tasks are rising. Prior research has often probed gender bias using\nexplicit gender cues as counterfactual, or studied them in sentence completion\nand short question answering tasks. These formats might overlook more implicit\nforms of bias embedded in generative behavior of longer content. In this work,\nwe investigate gender bias in LLMs using gender stereotypes studied in\npsychology (e.g., aggressiveness or gossiping) in an open-ended task of\nnarrative generation. We introduce a novel dataset called StereoBias-Stories\ncontaining short stories either unconditioned or conditioned on (one, two, or\nsix) random attributes from 25 psychological stereotypes and three task-related\nstory endings. We analyze how the gender contribution in the overall story\nchanges in response to these attributes and present three key findings: (1)\nWhile models, on average, are highly biased towards male in unconditioned\nprompts, conditioning on attributes independent from gender stereotypes\nmitigates this bias. (2) Combining multiple attributes associated with the same\ngender stereotype intensifies model behavior, with male ones amplifying bias\nand female ones alleviating it. (3) Model biases align with psychological\nground-truth used for categorization, and alignment strength increases with\nmodel size. Together, these insights highlight the importance of\npsychology-grounded evaluation of LLMs.", "AI": {"tldr": "研究通过StereoBias-Stories数据集探讨LLM中的性别偏见，发现未加条件提示时模型有显著的男性偏见，但通过特定属性条件设定能改变偏见强度，且偏见模式与心理学数据一致，这一发现提示了心理学评估的重要性。", "motivation": "随着大型语言模型在各种应用程序中的广泛应用，对它们可能放大性别偏见的担忧增加。以往的偏见研究往往使用明确的性别提示作为反事实或者仅限于句子生成和简短问题回答任务，这可能忽视了更隐蔽的偏见。为了弥补这一局限，这项工作采用了心理学中关于性别刻板印象的研究，通过开放式叙事生成任务对性别偏见进行了调查。", "method": "本研究通过引入一个名为StereoBias-Stories的新数据集来探讨LLM中的性别偏见问题，该数据集包含未加条件或经过随机属性条件的简短故事，这些属性来自25种心理学性别刻板印象和三个与任务相关的结局。研究重点分析了这些属性对整个故事中性别贡献的影响。", "result": "研究得出了三个关键发现：(1) LL模型在未加条件提示时通常存在明显的男性偏见，但通过非性别刻板印象的属性进行条件设定可以减轻这种偏见。(2) 结合与同一性别刻板印象相关的多个属性会加强模型的行为，男性属性会加重偏见，女性属性则会减轻偏见。(3) 模型的偏见与用于分类的心理学事实一致，且一致性随模型大小的增加而增强。", "conclusion": "研究结果表明，心理学基础的评估对于理解大型语言模型的性别偏见至关重要。"}}
{"id": "2508.03077", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03077", "abs": "https://arxiv.org/abs/2508.03077", "authors": ["Anran Wu", "Long Peng", "Xin Di", "Xueyuan Dai", "Chen Wu", "Yang Wang", "Xueyang Fu", "Yang Cao", "Zheng-Jun Zha"], "title": "RobustGS: Unified Boosting of Feedforward 3D Gaussian Splatting under Low-Quality Conditions", "comment": null, "summary": "Feedforward 3D Gaussian Splatting (3DGS) overcomes the limitations of\noptimization-based 3DGS by enabling fast and high-quality reconstruction\nwithout the need for per-scene optimization. However, existing feedforward\napproaches typically assume that input multi-view images are clean and\nhigh-quality. In real-world scenarios, images are often captured under\nchallenging conditions such as noise, low light, or rain, resulting in\ninaccurate geometry and degraded 3D reconstruction. To address these\nchallenges, we propose a general and efficient multi-view feature enhancement\nmodule, RobustGS, which substantially improves the robustness of feedforward\n3DGS methods under various adverse imaging conditions, enabling high-quality 3D\nreconstruction. The RobustGS module can be seamlessly integrated into existing\npretrained pipelines in a plug-and-play manner to enhance reconstruction\nrobustness. Specifically, we introduce a novel component, Generalized\nDegradation Learner, designed to extract generic representations and\ndistributions of multiple degradations from multi-view inputs, thereby\nenhancing degradation-awareness and improving the overall quality of 3D\nreconstruction. In addition, we propose a novel semantic-aware state-space\nmodel. It first leverages the extracted degradation representations to enhance\ncorrupted inputs in the feature space. Then, it employs a semantic-aware\nstrategy to aggregate semantically similar information across different views,\nenabling the extraction of fine-grained cross-view correspondences and further\nimproving the quality of 3D representations. Extensive experiments demonstrate\nthat our approach, when integrated into existing methods in a plug-and-play\nmanner, consistently achieves state-of-the-art reconstruction quality across\nvarious types of degradations.", "AI": {"tldr": "This paper presents a novel module, RobustGS, aimed at improving the robustness of feedforward 3D Gaussian Splatting for high-quality 3D reconstruction under real-world imaging conditions, achieving state-of-the-art results.", "motivation": "The motivation behind this paper is to address the limitations of existing feedforward 3D Gaussian Splatting methods when faced with images of poor quality due to real-world challenges such as noise, low light, or rain, which lead to inaccurate reconstructions.", "method": "Our method introduces a multi-view feature enhancement module named RobustGS, designed to improve the robustness of feedforward 3D Gaussian Splatting under various adverse imaging conditions. It includes a Generalized Degradation Learner to extract generic degradation representations and a semantic-aware state-space model to enhance and aggregate information.", "result": "Experiments show that the proposed approach significantly enhances 3D reconstruction quality across a range of degradation types when integrated into existing methods.", "conclusion": "The paper concludes that the introduced RobustGS module effectively improves the robustness of 3D Gaussian Splatting and can be applied in a plug-and-play manner to enhance the reconstruction quality of existing models under various adverse imaging conditions."}}
{"id": "2508.03294", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03294", "abs": "https://arxiv.org/abs/2508.03294", "authors": ["Leonidas Zotos", "Ivo Pascal de Jong", "Matias Valdenegro-Toro", "Andreea Ioana Sburlea", "Malvina Nissim", "Hedderik van Rijn"], "title": "NLP Methods May Actually Be Better Than Professors at Estimating Question Difficulty", "comment": "10 pages, 2 figures, accepted at the 2nd International Workshop on AI\n  in Society, Education and Educational Research (AISEER)", "summary": "Estimating the difficulty of exam questions is essential for developing good\nexams, but professors are not always good at this task. We compare various\nLarge Language Model-based methods with three professors in their ability to\nestimate what percentage of students will give correct answers on True/False\nexam questions in the areas of Neural Networks and Machine Learning. Our\nresults show that the professors have limited ability to distinguish between\neasy and difficult questions and that they are outperformed by directly asking\nGemini 2.5 to solve this task. Yet, we obtained even better results using\nuncertainties of the LLMs solving the questions in a supervised learning\nsetting, using only 42 training samples. We conclude that supervised learning\nusing LLM uncertainty can help professors better estimate the difficulty of\nexam questions, improving the quality of assessment.", "AI": {"tldr": "本研究比较了大型语言模型和教授在估计考试题目难度上的能力，发现语言模型尤其是其不确定性信息可用于预测题目难度，从而帮助教育者开发更优质考试。", "motivation": "教授们在估计考试题目难度上往往不够准确，因此本研究旨在通过比较语言模型和教授们的估计结果，探索更有效的题目难度评估方法。", "method": "通过比较大型语言模型与三位教授对神经网络和机器学习领域中的真假题目的正确回答百分比估计能力，来评估教授们在题目难度估计上的表现。此外，研究还采用监督学习方法，利用语言模型解决题目时的不确定性，在仅有42个训练样本的情况下获得了更好的结果。", "result": "教授们在区分题目难度方面的表现有限，而直接询问Gemini 2.5语言模型来估算题目难度的方法优于教授的估计。采用监督学习方法，利用语言模型解决题目时的不确定性可以获得更好的结果。", "conclusion": "研究表明，利用语言模型的不确定性进行监督学习可以帮助教授更准确地估计考试题目的难度，从而提高评估的质量。"}}
{"id": "2508.03079", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03079", "abs": "https://arxiv.org/abs/2508.03079", "authors": ["Zaiying Zhao", "Toshihiko Yamasaki"], "title": "Exploring Fairness across Fine-Grained Attributes in Large Vision-Language Models", "comment": "Accepted to the Responsible Generative AI (ReGenAI) Workshop, CVPR\n  2025", "summary": "The rapid expansion of applications using Large Vision-Language Models\n(LVLMs), such as GPT-4o, has raised significant concerns about their fairness.\nWhile existing studies primarily focus on demographic attributes such as race\nand gender, fairness across a broader range of attributes remains largely\nunexplored. In this study, we construct an open-set knowledge base of bias\nattributes leveraging Large Language Models (LLMs) and evaluate the fairness of\nLVLMs across finer-grained attributes. Our experimental results reveal that\nLVLMs exhibit biased outputs across a diverse set of attributes and further\ndemonstrate that cultural, environmental, and behavioral factors have a more\npronounced impact on LVLM decision-making than traditional demographic\nattributes.", "AI": {"tldr": "研究利用大规模语言模型构建知识库，评估大规模视觉语言模型在多种属性上的公平性，发现文化、环境和行为因素对模型决策有显著影响。", "motivation": "当前研究主要集中在种族和性别等人际属性上的公平性问题，对于更广泛属性上的公平性探讨较少。本研究旨在填补这一空白。", "method": "我们利用大规模语言模型构建了一个开放集知识库，来识别偏见属性，并利用这些属性评估大规模视觉语言模型在更细粒度属性上的公平性。", "result": "实验结果显示大规模视觉语言模型在多样化的属性上表现出有偏见的结果，文化、环境和行为因素对模型决策的影响比传统的种族和性别等人口统计属性更为显著。", "conclusion": "研究强调，要全面理解LVLMs的公平性问题，需要考虑文化、环境和行为等更为广泛的属性。"}}
{"id": "2508.03296", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03296", "abs": "https://arxiv.org/abs/2508.03296", "authors": ["Anqi Li", "Wenwei Jin", "Jintao Tong", "Pengda Qin", "Weijia Li", "Guo Lu"], "title": "Towards Trustworthy Multimodal Moderation via Policy-Aligned Reasoning and Hierarchical Labeling", "comment": null, "summary": "Social platforms have revolutionized information sharing, but also\naccelerated the dissemination of harmful and policy-violating content. To\nensure safety and compliance at scale, moderation systems must go beyond\nefficiency and offer accuracy and interpretability. However, current approaches\nlargely rely on noisy, label-driven learning, lacking alignment with moderation\nrules and producing opaque decisions that hinder human review. Therefore, we\npropose Hierarchical Guard (Hi-Guard), a multimodal moderation framework that\nintroduces a new policy-aligned decision paradigm. The term \"Hierarchical\"\nreflects two key aspects of our system design: (1) a hierarchical moderation\npipeline, where a lightweight binary model first filters safe content and a\nstronger model handles fine-grained risk classification; and (2) a hierarchical\ntaxonomy in the second stage, where the model performs path-based\nclassification over a hierarchical taxonomy ranging from coarse to fine-grained\nlevels. To ensure alignment with evolving moderation policies, Hi-Guard\ndirectly incorporates rule definitions into the model prompt. To further\nenhance structured prediction and reasoning, we introduce a multi-level\nsoft-margin reward and optimize with Group Relative Policy Optimization (GRPO),\npenalizing semantically adjacent misclassifications and improving explanation\nquality. Extensive experiments and real-world deployment demonstrate that\nHi-Guard achieves superior classification accuracy, generalization, and\ninterpretability, paving the way toward scalable, transparent, and trustworthy\ncontent safety systems. Code is available at:\nhttps://github.com/lianqi1008/Hi-Guard.", "AI": {"tldr": "Hi-Guard, a hierarchical moderation framework, aligns with policy rules, improves interpretability, and achieves better classification accuracy, making social platform moderation more transparent and trustworthy.", "motivation": "To address the challenges of efficient, accurate, and interpretable moderation on social platforms, especially with evolving policies.", "method": "A hierarchical moderation framework named Hi-Guard is proposed, which includes a hierarchical moderation pipeline and a hierarchical taxonomy for classification, ensuring policy alignment and improving interpretability.", "result": "Hi-Guard shows superior classification accuracy, generalization, and interpretability in extensive experiments and real-world deployment.", "conclusion": "Hi-Guard introduces a new paradigm that ensures alignment with moderation rules and produces transparent decisions, contributing to scalable, transparent, and trustworthy content safety systems."}}
{"id": "2508.03081", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03081", "abs": "https://arxiv.org/abs/2508.03081", "authors": ["Bo Zhang", "Xu Xinan", "Shuo Yan", "Yu Bai", "Zheng Zhang", "Wufan Wang", "Wendong Wang"], "title": "Contrastive Cross-Bag Augmentation for Multiple Instance Learning-based Whole Slide Image Classification", "comment": null, "summary": "Recent pseudo-bag augmentation methods for Multiple Instance Learning\n(MIL)-based Whole Slide Image (WSI) classification sample instances from a\nlimited number of bags, resulting in constrained diversity. To address this\nissue, we propose Contrastive Cross-Bag Augmentation ($C^2Aug$) to sample\ninstances from all bags with the same class to increase the diversity of\npseudo-bags. However, introducing new instances into the pseudo-bag increases\nthe number of critical instances (e.g., tumor instances). This increase results\nin a reduced occurrence of pseudo-bags containing few critical instances,\nthereby limiting model performance, particularly on test slides with small\ntumor areas. To address this, we introduce a bag-level and group-level\ncontrastive learning framework to enhance the discrimination of features with\ndistinct semantic meanings, thereby improving model performance. Experimental\nresults demonstrate that $C^2Aug$ consistently outperforms state-of-the-art\napproaches across multiple evaluation metrics.", "AI": {"tldr": "提出对比交叉包增强($C^2Aug$)方法，通过增加包间多样性并引入对比学习框架，提升了MIL在WSI分类中的性能。", "motivation": "现有的伪包增强方法由于从有限数量的包中抽样，导致多样性受限。", "method": "我们提出了对比交叉包增强（$C^2Aug$）方法，通过从具有相同类别的所有包中采样实例来增加伪包的多样性。此外，为了解决引入更多关键实例后模型性能受限的问题，我们引入了包级别和组级别的对比学习框架，以增强具有不同语义意义特征的区分性。", "result": "实验结果表明，$C^2Aug$ 在多个评估指标上始终优于最先进的方法。", "conclusion": "通过增加包间多样性和引入对比学习框架，$C^2Aug$ 能够有效地提高模型性能，特别是在测试图像肿瘤区域较小的情况下表现更佳。"}}
{"id": "2508.03333", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03333", "abs": "https://arxiv.org/abs/2508.03333", "authors": ["Zhende Song", "Shengji Tang", "Peng Ye", "Jiayuan Fan", "Tao Chen"], "title": "CTTS: Collective Test-Time Scaling", "comment": null, "summary": "Test-time scaling (TTS) has emerged as a promising research field for\nenhancing the effectiveness of large language models (LLMs) without extra\ntraining. However, most existing approaches, e.g., Best-of-N and\nSelf-Consistency rely on a single agent interacting with a reward model\n(SA-SR), constrained by limited capabilities of a single test-time scaling\n(STTS) paradigm. On the other hand, recent works demonstrate that\ncollective-agent methods can break through the upper bound of single-agent\nsystems by orchestrating diverse models. Thus, in this paper, we take a first\nstep towards exploring Collective Test-Time Scaling (CTTS). Consider the\ndifferent interaction types of single and multiple models, we design three\nprimary paradigms to investigate the optimal paradigm of CTTS: (1) single agent\nto multiple reward models (SA-MR); (2) multiple agents to single reward model\n(MA-SR); and (3) multiple agents to multiple reward models (MA-MR). Extensive\nexperiments demonstrate that MA-MR consistently achieves the best performance.\nBased on this, we propose a novel framework named CTTS-MM that effectively\nleverages both multi-agent and multi-reward-model collaboration for enhanced\ninference. Specifically, for multi-agent collaboration, we propose an Agent\nCollaboration Search (ACS), which searches for the most effective combination\nof LLM agents from a large candidate pool; for multi-reward-model\ncollaboration, we propose Mixture of Reword Models (MoR), which consists of a\ncurated question pool and a Prior Reward model Ensemble Selection (PRES) to\nselect the optimal combinations of reward models via Pair-wise Reward Ranking\n(PRR) metric. Experiments across seven mainstream benchmarks demonstrate that\nthe proposed CTTS-MM consistently obtains superior performance. Code will be\nreleased at https://github.com/magent4aci/CTTS-MM.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.03094", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03094", "abs": "https://arxiv.org/abs/2508.03094", "authors": ["Jiantao Tan", "Peixian Ma", "Kanghao Chen", "Zhiming Dai", "Ruixuan Wang"], "title": "Augmenting Continual Learning of Diseases with LLM-Generated Visual Concepts", "comment": null, "summary": "Continual learning is essential for medical image classification systems to\nadapt to dynamically evolving clinical environments. The integration of\nmultimodal information can significantly enhance continual learning of image\nclasses. However, while existing approaches do utilize textual modality\ninformation, they solely rely on simplistic templates with a class name,\nthereby neglecting richer semantic information. To address these limitations,\nwe propose a novel framework that harnesses visual concepts generated by large\nlanguage models (LLMs) as discriminative semantic guidance. Our method\ndynamically constructs a visual concept pool with a similarity-based filtering\nmechanism to prevent redundancy. Then, to integrate the concepts into the\ncontinual learning process, we employ a cross-modal image-concept attention\nmodule, coupled with an attention loss. Through attention, the module can\nleverage the semantic knowledge from relevant visual concepts and produce\nclass-representative fused features for classification. Experiments on medical\nand natural image datasets show our method achieves state-of-the-art\nperformance, demonstrating the effectiveness and superiority of our method. We\nwill release the code publicly.", "AI": {"tldr": "A novel continual learning framework for medical image classification that uses visual concepts from large language models to provide semantic guidance, showing state-of-the-art performance.", "motivation": "To enhance continual learning for medical image classification systems by integrating richer semantic information from visual concepts generated by LLMs, going beyond simple class name templates used in current methods.", "method": "Our method dynamically constructs a visual concept pool using a similarity-based filtering mechanism to prevent redundancy and employs a cross-modal image-concept attention module to integrate visual concepts into continual learning, with an attention loss.", "result": "Experiments on medical and natural image datasets demonstrate state-of-the-art performance, proving the effectiveness of the method.", "conclusion": "The proposed framework demonstrates the superiority of combining multimodal information through semantic guidance provided by visual concepts, achieving better continual learning performance."}}
{"id": "2508.03358", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.03358", "abs": "https://arxiv.org/abs/2508.03358", "authors": ["Tiago G Canário", "Catarina Duarte", "Flávio L. Pinheiro", "João L. M. Pereira"], "title": "Taggus: An Automated Pipeline for the Extraction of Characters' Social Networks from Portuguese Fiction Literature", "comment": "24 pages, 5 Figures, 4 Tables", "summary": "Automatically identifying characters and their interactions from fiction\nbooks is, arguably, a complex task that requires pipelines that leverage\nmultiple Natural Language Processing (NLP) methods, such as Named Entity\nRecognition (NER) and Part-of-speech (POS) tagging. However, these methods are\nnot optimized for the task that leads to the construction of Social Networks of\nCharacters. Indeed, the currently available methods tend to underperform,\nespecially in less-represented languages, due to a lack of manually annotated\ndata for training. Here, we propose a pipeline, which we call Taggus, to\nextract social networks from literary fiction works in Portuguese. Our results\nshow that compared to readily available State-of-the-Art tools -- off-the-shelf\nNER tools and Large Language Models (ChatGPT) -- the resulting pipeline, which\nuses POS tagging and a combination of heuristics, achieves satisfying results\nwith an average F1-Score of $94.1\\%$ in the task of identifying characters and\nsolving for co-reference and $75.9\\%$ in interaction detection. These\nrepresent, respectively, an increase of $50.7\\%$ and $22.3\\%$ on results\nachieved by the readily available State-of-the-Art tools. Further steps to\nimprove results are outlined, such as solutions for detecting relationships\nbetween characters. Limitations on the size and scope of our testing samples\nare acknowledged. The Taggus pipeline is publicly available to encourage\ndevelopment in this field for the Portuguese language.2", "AI": {"tldr": "研究提供了一种名为Taggus的管道，专门用于从葡萄牙语文本中识别和构建角色社交网络，性能优于现成的NLP工具。", "motivation": "现有的自然语言处理方法在构建角色社交网络方面的效果不佳，特别是在数据标注不足的语言中。研究者提出这项研究以优化此过程。", "method": "文中提出了一种名为Taggus的管道，用于从葡萄牙语文学小说作品中抽取社交网络。该管道结合了词性标注和若干启发式方法来识别角色和解决共指问题。", "result": "相较于现成的最先进工具（包括实体名称识别工具和大型语言模型），Taggus在识别字符和解决共指问题上的F1值为94.1%，在交互检测上的F1值为75.9%，分别提高了50.7%和22.3%。", "conclusion": "尽管实验样本在规模和范围上有限，但Taggus展示了在葡萄牙语文本处理上的潜力。研究提出了未来改进的方向，比如更准确地检测角色关系，并公开了该管道以推动相关领域的进一步发展。"}}
{"id": "2508.03100", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03100", "abs": "https://arxiv.org/abs/2508.03100", "authors": ["Yogesh Kulkarni", "Pooyan Fazli"], "title": "AVATAR: Reinforcement Learning to See, Hear, and Reason Over Video", "comment": null, "summary": "Multimodal reasoning over long-horizon video is challenging due to the need\nfor precise spatiotemporal fusion and alignment across modalities. While recent\nmethods such as Group Relative Policy Optimization (GRPO) have shown promise in\nthis domain, they suffer from three key limitations: (1) data inefficiency from\ntheir on-policy design, (2) a vanishing advantage problem, where identical or\nnear-identical rewards within a group eliminate the learning signal by\nproducing zero-valued advantages, and (3) uniform credit assignment that fails\nto emphasize critical reasoning steps. We introduce AVATAR (Audio-Video Agent\nfor Alignment and Reasoning), a framework that addresses these limitations\nthrough two core components: (1) an off-policy training architecture that\nimproves sample efficiency and resolves vanishing advantages by reusing past\nexperiences with greater reward diversity, and (2) Temporal Advantage Shaping\n(TAS), a novel credit assignment strategy that upweights key reasoning phases\nduring learning. AVATAR achieves strong performance across various benchmarks,\noutperforming the Qwen2.5-Omni baseline by +5.4on MMVU, +4.9 on OmniBench, and\n+4.5 on Video-Holmes, while demonstrating over 35% higher sample efficiency.", "AI": {"tldr": "AVATAR框架解决了多模式长时视频推理中的几个关键问题，包括数据效率低下、优势消亡问题以及均匀的信用分配，通过离线训练架构和时间优势塑造策略提高了性能和样本效率。", "motivation": "鉴于多模式长时视频推理的挑战，比如对精确的时空融合和跨模式对齐的需求，该研究旨在解决现有方法，尤其是GRPO，在数据使用效率、优势消亡及均匀信用分配上的不足。", "method": "AVATAR采用了一种离线训练架构，提高了样本效率，并通过重用过去的经验解决了优势消亡问题。同时，它引入了时间优势塑造（TAS）策略，该策略在学习过程中强调关键推理阶段的权重。", "result": "AVATAR在多种基准测试中表现出色，相比Qwen2.5-Omni基线模型在MMVU上提升了5.4%,在Omnibench上提升了4.9%,在Video-Holmes上提升了4.5%，并展示了超过35%的样本效率提升。", "conclusion": "AVATAR框架通过改进的训练架构和时间优势塑造策略，在多模式长时视频推理任务中解决了关键问题，显著提升了性能和样本效率。"}}
{"id": "2508.03363", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03363", "abs": "https://arxiv.org/abs/2508.03363", "authors": ["Haotian Wu", "Bo Xu", "Yao Shu", "Menglin Yang", "Chengwei Qin"], "title": "Thinking with Nothinking Calibration: A New In-Context Learning Paradigm in Reasoning Large Language Models", "comment": null, "summary": "Reasoning large language models (RLLMs) have recently demonstrated remarkable\ncapabilities through structured and multi-step reasoning. While prior research\nhas primarily focused on improving their training and inference strategies,\ntheir potential for in-context learning (ICL) remains largely underexplored. To\nfill this gap, we propose Thinking with Nothinking Calibration (JointThinking),\na new ICL paradigm that leverages the structured difference between two\nreasoning modes, i.e., Thinking and Nothinking, to improve reasoning accuracy.\nSpecifically, our method prompts the model to generate two answers in parallel:\none in Thinking mode and the other in Nothinking mode. A second round of\nThinking is triggered only when the two initial responses are inconsistent,\nusing a single prompt that incorporates the original question and both\ncandidate answers. Since such disagreement occurs infrequently (e.g., only 6\\%\nin GSM8K), our method performs just one round of reasoning in most cases,\nresulting in minimal latency overhead. Extensive experiments across multiple\nreasoning benchmarks demonstrate that JointThinking significantly outperforms\nfew-shot chain-of-thought (CoT) and majority voting with improved answer\nrobustness. Moreover, It achieves comparable in-distribution performance to\ntraining-based SOTA method, while substantially outperforming on\nout-of-distribution tasks. We further conduct a systematic analysis of the\ncalibration mechanism, showing that leveraging different reasoning modes\nconsistently lowers the error rate and highlights the value of structural\nthinking diversity. Additionally, we observe that the performance gap between\nactual and ideal reasoning narrows as model size increases in the second round\nof thinking, indicating the strong scalability of our approach. Finally, we\ndiscuss current limitations and outline promising directions for future ICL\nresearch in RLLMs.", "AI": {"tldr": "本文提出了一种名为JointThinking的新方法，通过利用思考与无思考两种模式间的差异来提升推理型大规模语言模型的准确性和鲁棒性。方法仅在初始响应不同时进行额外推理，从而有效提升了性能，同时减少了推理延迟。", "motivation": "该研究旨在探索推理型大规模语言模型的上下文学习潜力。虽然之前的研究主要集中在改善这些模型的训练和推断策略上，但对模型在上下文学习方面的能力的关注不多。", "method": "本文提出了一种新的上下文学习范式，称为思维无思维校准（JointThinking）。该方法通过利用两种推理模式（思考模式和无思考模式）之间的结构差异来提高推理准确率。具体来说，该方法提示模型并行生成两种模式下的答案，只有当两个初始响应不一致时，才触发第二次思考。", "result": "通过多任务推理基准测试，JointThinking显著优于少样本链式思考（CoT）和多数投票法，在答案的鲁棒性上得到了提升，同时还可达到与现有的基于训练的SOTA方法类似的效果，而在分布外任务上则显著优于其他方法。实验还揭示了利用不同的推理模式可以持续降低错误率，并显示出结构化思维多样性的价值。此外，实验还表明，随着模型规模的增加，实际情况下的推理与理想推理之间的性能差距在第二次思考时缩小，表明了该方法良好的可扩展性。", "conclusion": "作者讨论了该方法当前的局限性，并展望了在推理型大规模语言模型上下文学习领域未来的研究方向，强调了该方法在提高模型推理能力方面的潜在价值。"}}
{"id": "2508.03102", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03102", "abs": "https://arxiv.org/abs/2508.03102", "authors": ["Tianjiao Jiang", "Zhen Zhang", "Yuhang Liu", "Javen Qinfeng Shi"], "title": "Causal Disentanglement and Cross-Modal Alignment for Enhanced Few-Shot Learning", "comment": null, "summary": "Few-shot learning (FSL) often requires effective adaptation of models using\nlimited labeled data. However, most existing FSL methods rely on entangled\nrepresentations, requiring the model to implicitly recover the unmixing process\nto obtain disentangled representations using only limited supervision, which\nhinders effective adaptation. Recent theoretical studies show that multimodal\ncontrastive learning methods, such as CLIP, can disentangle latent\nrepresentations up to linear transformations. In light of this, we propose the\nCausal CLIP Adapter (CCA), a novel framework that explicitly disentangles\nvisual features extracted from CLIP using unsupervised Independent Component\nAnalysis (ICA). This removes the need to learn the unmixing process from the\nlabeled data, thereby reducing the number of trainable parameters and\nmitigating overfitting. Taking a step further, while ICA can obtain visual\ndisentangled representations, it may also disrupt CLIP's intra- and inter-modal\nalignment. To counteract this, CCA further leverages CLIP's inherent\ncross-modal alignment by enhancing it in two ways: unidirectionally, through\nfine-tuning a CLIP-based text classifier, and bidirectionally, via a\ncross-attention mechanism that enriches visual and textual representations\nthrough mutual interaction. Both unimodal and cross-modal classification\noutputs can be effectively combined linearly to improve classification\naccuracy. Extensive experiments on 11 benchmark datasets demonstrate that our\nmethod consistently outperforms state-of-the-art approaches in terms of\nfew-shot performance and robustness to distributional shifts, while maintaining\ncomputational efficiency. Code will be available at\nhttps://github.com/tianjiao-j/CCA.", "AI": {"tldr": "The paper introduces the Causal CLIP Adapter (CCA), a method that disentangles CLIP's visual features using ICA and enhances cross-modal alignment, demonstrating superior few-shot performance and robustness on 11 benchmark datasets.", "motivation": "Unlike most FSL methods which rely on entangled representations, CCA disentangles latent representations via ICA, making effective use of limited labeled data possible.", "method": "The Causal CLIP Adapter (CCA) uses unsupervised Independent Component Analysis (ICA) to disentangle visual features from CLIP, reducing the number of trainable parameters and mitigating overfitting. It enhances the CLIP's intrinsic cross-modal alignment using a unidirectional text classifier and a bidirectional cross-attention mechanism.", "result": "Experiments on 11 benchmark datasets show that the CCA approach outperforms state-of-the-art methods in few-shot learning and is robust to distributional shifts.", "conclusion": "CCA provides a robust and efficient approach to few-shot learning, offering an improvement over existing methods by disentangling representations and enhancing cross-modal alignment."}}
{"id": "2508.03399", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03399", "abs": "https://arxiv.org/abs/2508.03399", "authors": ["Eliseo Bao", "Anxo Pérez", "Javier Parapar"], "title": "ReDSM5: A Reddit Dataset for DSM-5 Depression Detection", "comment": "Accepted as a resource paper at CIKM 2025", "summary": "Depression is a pervasive mental health condition that affects hundreds of\nmillions of individuals worldwide, yet many cases remain undiagnosed due to\nbarriers in traditional clinical access and pervasive stigma. Social media\nplatforms, and Reddit in particular, offer rich, user-generated narratives that\ncan reveal early signs of depressive symptomatology. However, existing\ncomputational approaches often label entire posts simply as depressed or not\ndepressed, without linking language to specific criteria from the DSM-5, the\nstandard clinical framework for diagnosing depression. This limits both\nclinical relevance and interpretability. To address this gap, we introduce\nReDSM5, a novel Reddit corpus comprising 1484 long-form posts, each\nexhaustively annotated at the sentence level by a licensed psychologist for the\nnine DSM-5 depression symptoms. For each label, the annotator also provides a\nconcise clinical rationale grounded in DSM-5 methodology. We conduct an\nexploratory analysis of the collection, examining lexical, syntactic, and\nemotional patterns that characterize symptom expression in social media\nnarratives. Compared to prior resources, ReDSM5 uniquely combines\nsymptom-specific supervision with expert explanations, facilitating the\ndevelopment of models that not only detect depression but also generate\nhuman-interpretable reasoning. We establish baseline benchmarks for both\nmulti-label symptom classification and explanation generation, providing\nreference results for future research on detection and interpretability.", "AI": {"tldr": "本论文提出了ReDSM5，这是一个包含Reddit上长篇帖的多标签抑郁症状注释数据集。它结合了针对每个症状的专家解释，旨在帮助开发不仅能够检测抑郁，还能够生成人类可理解推理的模型。", "motivation": "尽管抑郁症影响全球数百万人，但由于传统临床访问和广泛存在的污名化问题，许多病例未能得到诊断。现有的计算方法通常只是将帖子简单地分为抑郁或非抑郁两类，而未能将其语言与DSM-5中的具体标准相联系。这限制了临床相关性和解释性。", "method": "本研究创建了一个名为ReDSM5的新Reddit数据集，包含1484篇长篇帖子，由持证心理学家从九个DSM-5抑郁症状的角度进行句子级详尽标注。此外，对于每个标签，标注者还会提供基于DSM-5方法的简要临床理由。", "result": "研究进行了数据集的探索性分析，探讨了社交媒体叙事中症状表达的词汇、句法和情感模式。确立了多标签症状分类和解释生成的基准测试，为未来的检测和解释研究提供了参考结果。", "conclusion": "ReDSM5的独特之处在于它结合了症状特定的监督与专家解释，促进了模型的开发，使得模型不仅能够检测抑郁症，还能生成人类可理解的推理。该数据集为未来研究定义了基准。"}}
{"id": "2508.03118", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03118", "abs": "https://arxiv.org/abs/2508.03118", "authors": ["Heng Jia", "Linchao Zhu", "Na Zhao"], "title": "H3R: Hybrid Multi-view Correspondence for Generalizable 3D Reconstruction", "comment": "ICCV 2025", "summary": "Despite recent advances in feed-forward 3D Gaussian Splatting, generalizable\n3D reconstruction remains challenging, particularly in multi-view\ncorrespondence modeling. Existing approaches face a fundamental trade-off:\nexplicit methods achieve geometric precision but struggle with ambiguous\nregions, while implicit methods provide robustness but suffer from slow\nconvergence. We present H3R, a hybrid framework that addresses this limitation\nby integrating volumetric latent fusion with attention-based feature\naggregation. Our framework consists of two complementary components: an\nefficient latent volume that enforces geometric consistency through epipolar\nconstraints, and a camera-aware Transformer that leverages Pl\\\"ucker\ncoordinates for adaptive correspondence refinement. By integrating both\nparadigms, our approach enhances generalization while converging 2$\\times$\nfaster than existing methods. Furthermore, we show that spatial-aligned\nfoundation models (e.g., SD-VAE) substantially outperform semantic-aligned\nmodels (e.g., DINOv2), resolving the mismatch between semantic representations\nand spatial reconstruction requirements. Our method supports variable-number\nand high-resolution input views while demonstrating robust cross-dataset\ngeneralization. Extensive experiments show that our method achieves\nstate-of-the-art performance across multiple benchmarks, with significant PSNR\nimprovements of 0.59 dB, 1.06 dB, and 0.22 dB on the RealEstate10K, ACID, and\nDTU datasets, respectively. Code is available at\nhttps://github.com/JiaHeng-DLUT/H3R.", "AI": {"tldr": "The paper introduces H3R, a hybrid 3D reconstruction framework that combines volumetric latent fusion with attention-based feature aggregation, improving efficiency and performance over existing methods.", "motivation": "The motivation is to tackle the challenges in 3D reconstruction, specifically in multi-view correspondence modeling, by addressing the fundamental trade-off between explicit and implicit methods.", "method": "The method involves an efficient latent volume for enforcing geometric consistency and a camera-aware Transformer using Pl\"ucker coordinates for adaptive correspondence refinement.", "result": "H3R converges twice as fast as existing methods and achieves state-of-the-art performance with significant PSNR improvements on multiple benchmarks.", "conclusion": "The study concludes that H3R offers an effective solution to 3D reconstruction, demonstrating robust performance and cross-dataset generalization."}}
{"id": "2508.03420", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.03420", "abs": "https://arxiv.org/abs/2508.03420", "authors": ["Bing Wang", "Ximing Li", "Yiming Wang", "Changchun Li", "Jiaxu Cui", "Renchu Guan", "Bo Yang"], "title": "Variety Is the Spice of Life: Detecting Misinformation with Dynamic Environmental Representations", "comment": "Accepted by CIKM 2025. 11 pages, 4 figures. Code:\n  https://github.com/wangbing1416/MISDER", "summary": "The proliferation of misinformation across diverse social media platforms has\ndrawn significant attention from both academic and industrial communities due\nto its detrimental effects. Accordingly, automatically distinguishing\nmisinformation, dubbed as Misinformation Detection (MD), has become an\nincreasingly active research topic. The mainstream methods formulate MD as a\nstatic learning paradigm, which learns the mapping between the content, links,\nand propagation of news articles and the corresponding manual veracity labels.\nHowever, the static assumption is often violated, since in real-world\nscenarios, the veracity of news articles may vacillate within the dynamically\nevolving social environment. To tackle this problem, we propose a novel\nframework, namely Misinformation detection with Dynamic Environmental\nRepresentations (MISDER). The basic idea of MISDER lies in learning a social\nenvironmental representation for each period and employing a temporal model to\npredict the representation for future periods. In this work, we specify the\ntemporal model as the LSTM model, continuous dynamics equation, and pre-trained\ndynamics system, suggesting three variants of MISDER, namely MISDER-LSTM,\nMISDER-ODE, and MISDER-PT, respectively. To evaluate the performance of MISDER,\nwe compare it to various MD baselines across 2 prevalent datasets, and the\nexperimental results can indicate the effectiveness of our proposed model.", "AI": {"tldr": "The paper presents MISDER, a framework that dynamically tracks and predicts the changing veracity of news articles over time, addressing the limitations of existing static approaches to misinformation detection.", "motivation": "The motivation behind this research is the critical issue of misinformation across various social media platforms and the damage caused by its proliferation. The paper aims to improve the detection of misinformation by moving beyond the constraints of static learning methods, which understate the reality of an evolving social media environment impacting the credibility of news.", "method": "The paper proposes a novel framework called Misinformation Detection with Dynamic Environmental Representations (MISDER). This framework aims to address the limitations of static learning models by incorporating a dynamic social environment and using temporal models to predict the fluctuating veracity of news articles. Three variants of MISDER are explored: MISDER-LSTM, MISDER-ODE, and MISDER-PT.", "result": "The experimental section of the paper shows that the novel MISDER framework, along with its variants (MISDER-LSTM, MISDER-ODE, MISDER-PT), outperforms the existing static approaches in detecting misinformation, showcasing the benefits of incorporating a dynamic perspective in the modeling.", "conclusion": "The effectiveness of the proposed MISDER framework, including its three variants, is demonstrated through comparative experiments against various baselines across two datasets, indicating better performance of the dynamic approach over the static methods in misinformation detection."}}
{"id": "2508.03127", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03127", "abs": "https://arxiv.org/abs/2508.03127", "authors": ["Sai Ma", "Zhuang Li", "John A Taylor"], "title": "Landsat30-AU: A Vision-Language Dataset for Australian Landsat Imagery", "comment": null, "summary": "Vision language models (VLMs) that enable natural language interaction with\nsatellite imagery can democratize Earth observation by accelerating expert\nworkflows, making data accessible to non-specialists, and enabling planet-scale\nautomation. However, existing datasets focus mainly on short-term,\nhigh-resolution imagery from a limited number of satellites, overlooking\nlow-resolution, multi-satellite, long-term archives, such as Landsat, that are\nessential for affordable and bias-robust global monitoring. We address this gap\nwith Landsat30-AU, a large-scale vision-language dataset built from 30-meter\nresolution imagery collected by four Landsat satellites (5, 7, 8, and 9) over\nAustralia, spanning more than 36 years. The dataset includes two components:\nLandsat30-AU-Cap, containing 196,262 image-caption pairs, and Landsat30-AU-VQA,\ncomprising 17,725 human-verified visual question answering (VQA) samples across\neight remote sensing domains. Both datasets are curated through a bootstrapped\npipeline that leverages generic VLMs with iterative refinement and human\nverification to ensure quality. Our evaluation of eight VLMs on our benchmark\nreveals that off-the-shelf models struggle to understand satellite imagery. The\nopen-source remote-sensing VLM EarthDial achieves only 0.07 SPIDEr in\ncaptioning and a VQA accuracy of 0.48, highlighting the limitations of current\napproaches. Encouragingly, lightweight fine-tuning of Qwen2.5-VL-7B on\nLandsat30-AU improves captioning performance from 0.11 to 0.31 SPIDEr and\nboosts VQA accuracy from \\textbf{0.74} to 0.87. Code and data are available at\nhttps://github.com/papersubmit1/landsat30-au.", "AI": {"tldr": "论文构建了Landsat30-AU数据集，用于提升现有视觉语言模型在长时间跨度、低分辨率卫星图像的理解能力，研究表明，通过轻量级微调现有VLM模型可以显著提升其在特定任务上的性能。", "motivation": "现有的数据集主要关注短期、高分辨率的卫星数据，忽略了对于长期监测更具成本效益和减少偏差的低分辨率、多卫星数据集的重要性。", "method": "提出了Landsat30-AU数据集，包括Landsat30-AU-Cap和Landsat30-AU-VQA两个部分，使用了自举管道结合人机协作保证数据质量。", "result": "开源遥感视觉语言模型EarthDial在描述和VQA任务上的表现较差，但对Qwen2.5-VL-7B进行轻量级微调后，其在Landsat30-AU上的描述和VQA性能均有提升。", "conclusion": "研究展示了现有VLM模型在处理卫星图像方面的局限性，并通过专注于低分辨率、长期存档的多卫星数据集Landsat30-AU，改善了模型在视觉语言任务上的性能。"}}
{"id": "2508.03440", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03440", "abs": "https://arxiv.org/abs/2508.03440", "authors": ["Junhong Wu", "Jinliang Lu", "Zixuan Ren", "Ganqiang Hu", "Zhi Wu", "Dai Dai", "Hua Wu"], "title": "LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models", "comment": "10 pages, 7 figures, working in progress", "summary": "Human cognition naturally engages with abstract and fluid concepts, whereas\nexisting reasoning models often rely on generating discrete tokens, potentially\nconstraining their expressive capabilities. Recent advancements aim to address\nthis limitation by enabling large language models (LLMs) to generate soft,\nabstract tokens, thus facilitating reasoning within a continuous concept space.\nThis paper explores the `Soft Thinking' capabilities of various LLMs by\nexamining the models' internal behavior using a suite of probing techniques.\nContrary to the common belief that Soft Thinking enables the simultaneous\nexploration of diverse reasoning paths, our findings reveal that LLMs\npredominantly rely on the most influential component of the soft inputs during\nsubsequent decoding steps. This reliance hinders the exploration of different\nreasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding,\nobscuring the advantage of transmitting more information through Soft Tokens.\nTo tackle this issue, we explore sampling strategies to introduce\n\\emph{randomness}, employing methods such as Dirichlet resampling and the\nGumbel-Softmax trick. Our experiments demonstrate that incorporating randomness\ncan alleviate the limitations of vanilla approaches and unleash the potential\nof Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate\nrandomness with controlled smoothness, resulting in superior performance across\neight reasoning benchmarks.", "AI": {"tldr": "论文探索了大型语言模型（LLMs）的软思考能力，并发现这些模型主要依赖于软输入中最具影响力的部分。为了解决这一问题，通过实验引入随机性，尤其是在采用Gumbel-Softmax技巧后，模型的表现有了显著提升。", "motivation": "动机在于解决现有LLMs依赖离散token，限制其表达能力的问题。通过探索'软思考'能力，希望改善模型在连续概念空间中的推理能力。", "method": "此论文使用了一组探测技术来分析各种大型语言模型（LLMs）在'软思考'能力方面的内部行为。为了应对软思考的局限性，研究者还探索了引入随机性的采样策略，如Dirichlet重采样和Gumbel-Softmax技巧。", "result": "实验结果表明，引入随机性可以在一定程度上减轻传统方法的局限性，Gumbel-Softmax技巧因为能够提供足够的随机性同时保持可控的平滑性，因此在实验中表现突出。", "conclusion": "研究得出结论认为，通过引入适当的随机采样策略尤其是Gumbel-Softmax技巧，能够显著改善模型在八项推理基准上的表现，扩展了软思考的能力。"}}
{"id": "2508.03132", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03132", "abs": "https://arxiv.org/abs/2508.03132", "authors": ["Arion Zimmermann", "Soon-Jo Chung", "Fred Hadaegh"], "title": "COFFEE: A Shadow-Resilient Real-Time Pose Estimator for Unknown Tumbling Asteroids using Sparse Neural Networks", "comment": "in Proc. 75th Int. Astronautical Congress (IAC-24), Milan, Italy,\n  Oct. 2024", "summary": "The accurate state estimation of unknown bodies in space is a critical\nchallenge with applications ranging from the tracking of space debris to the\nshape estimation of small bodies. A necessary enabler to this capability is to\nfind and track features on a continuous stream of images. Existing methods,\nsuch as SIFT, ORB and AKAZE, achieve real-time but inaccurate pose estimates,\nwhereas modern deep learning methods yield higher quality features at the cost\nof more demanding computational resources which might not be available on\nspace-qualified hardware. Additionally, both classical and data-driven methods\nare not robust to the highly opaque self-cast shadows on the object of\ninterest. We show that, as the target body rotates, these shadows may lead to\nlarge biases in the resulting pose estimates. For these objects, a bias in the\nreal-time pose estimation algorithm may mislead the spacecraft's state\nestimator and cause a mission failure, especially if the body undergoes a\nchaotic tumbling motion. We present COFFEE, the Celestial Occlusion Fast\nFEature Extractor, a real-time pose estimation framework for asteroids designed\nto leverage prior information on the sun phase angle given by sun-tracking\nsensors commonly available onboard spacecraft. By associating salient contours\nto their projected shadows, a sparse set of features are detected, invariant to\nthe motion of the shadows. A Sparse Neural Network followed by an\nattention-based Graph Neural Network feature matching model are then jointly\ntrained to provide a set of correspondences between successive frames. The\nresulting pose estimation pipeline is found to be bias-free, more accurate than\nclassical pose estimation pipelines and an order of magnitude faster than other\nstate-of-the-art deep learning pipelines on synthetic data as well as on\nrenderings of the tumbling asteroid Apophis.", "AI": {"tldr": "摘要：研究提出COFFEE来解决现有方法无法准确估计空间目标的姿态问题，特别是对于高度不透明自投射阴影物体，COFFEE在合成数据和阿波菲斯小行星模拟数据上实现了无偏、比传统方法更准确且比其他深度学习方法快一个数量级的效果。", "motivation": "动机：解决现有方法（如SIFT，ORB和AKAZE）的不准确性和现代深度学习方法在计算资源消耗上的高要求问题，尤其是处理在目标物体转动时造成的大偏差估计问题。这些问题可能导致航天器状态估计错误，尤其是面对混沌滚动运动时。", "method": "方法：提出了COFFEE（Celestial Occlusion Fast FEature Extractor），这是一种专门针对小行星实时姿态估计的框架。该框架能够利用空间探测器上的太阳跟踪传感器提供的太阳相位角的先验信息，通过关联显著轮廓与其投影阴影来检测一组稀疏特征，这些特征对于阴影的运动是不变的。接着使用稀疏神经网络和基于注意力的图神经网络特征匹配模型进行联合训练，旨在提供连续帧之间的对应关系。", "result": "结果：COFFEE方法展示了在合成数据和对直接翻转的小行星阿波菲斯的渲染数据上的优势，提供了无偏、更准确的实时姿态评估结果，并且比其他最先进的深度学习管道处理速度快一个数量级。", "conclusion": "结论：COFFEE提供了一种解决方案，成功解决了阴影运动问题导致的偏差姿态估计问题，同时在效果和速度上超越了传统方法和其他深度学习方法。"}}
