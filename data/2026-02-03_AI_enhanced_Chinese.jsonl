{"id": "2602.00095", "categories": ["cs.CV", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.00095", "abs": "https://arxiv.org/abs/2602.00095", "authors": ["Weiyu Sun", "Liangliang Chen", "Yongnuo Cai", "Huiru Xie", "Yi Zeng", "Ying Zhang"], "title": "EDU-CIRCUIT-HW: Evaluating Multimodal Large Language Models on Real-World University-Level STEM Student Handwritten Solutions", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) hold significant promise for revolutionizing traditional education and reducing teachers' workload. However, accurately interpreting unconstrained STEM student handwritten solutions with intertwined mathematical formulas, diagrams, and textual reasoning poses a significant challenge due to the lack of authentic and domain-specific benchmarks. Additionally, current evaluation paradigms predominantly rely on the outcomes of downstream tasks (e.g., auto-grading), which often probe only a subset of the recognized content, thereby failing to capture the MLLMs' understanding of complex handwritten logic as a whole. To bridge this gap, we release EDU-CIRCUIT-HW, a dataset consisting of 1,300+ authentic student handwritten solutions from a university-level STEM course. Utilizing the expert-verified verbatim transcriptions and grading reports of student solutions, we simultaneously evaluate various MLLMs' upstream recognition fidelity and downstream auto-grading performance. Our evaluation uncovers an astonishing scale of latent failures within MLLM-recognized student handwritten content, highlighting the models' insufficient reliability for auto-grading and other understanding-oriented applications in high-stakes educational settings. In solution, we present a case study demonstrating that leveraging identified error patterns to preemptively detect and rectify recognition errors, with only minimal human intervention (approximately 4% of the total solutions), can significantly enhance the robustness of the deployed AI-enabled grading system on unseen student solutions.", "AI": {"tldr": "The paper presents EDU-CIRCUIT-HW, a dataset of 1,300+ authentic student handwritten solutions from a university-level STEM course, to evaluate MLLMs' performance in recognizing and auto-grading complex handwritten content, and proposes a method to enhance the robustness of AI-enabled grading systems.", "motivation": "The main motivation of this paper is to address the challenge of accurately interpreting STEM student handwritten solutions with intertwined mathematical formulas, diagrams, and textual reasoning.", "method": "The researchers release EDU-CIRCUIT-HW, a dataset of 1,300+ authentic handwritten solutions with expert-verified transcriptions and grading reports, to evaluate the recognition and auto-grading capabilities of MLLMs.", "result": "The evaluation uncovered significant latent failures in MLLMs' understanding of complex handwritten content, suggesting that current models are insufficiently reliable for auto-grading.", "conclusion": "The paper concludes by presenting a case study showing that preemptively identifying and rectifying recognition errors with minimal human intervention can greatly enhance the reliability of AI-enabled grading systems in educational settings."}}
{"id": "2602.00096", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00096", "abs": "https://arxiv.org/abs/2602.00096", "authors": ["Zhengqing Gao", "Ziwen Li", "Xin Wang", "Jiaxin Huang", "Zhenyang Ren", "Mingkai Shao", "Hanlue Zhang", "Tianyu Huang", "Yongkang Cheng", "Yandong Guo", "Runqi Lin", "Yuanyuan Wang", "Tongliang Liu", "Kun Zhang", "Mingming Gong"], "title": "Mirage2Matter: A Physically Grounded Gaussian World Model from Video", "comment": null, "summary": "The scalability of embodied intelligence is fundamentally constrained by the scarcity of real-world interaction data. While simulation platforms provide a promising alternative, existing approaches often suffer from a substantial visual and physical gap to real environments and rely on expensive sensors, precise robot calibration, or depth measurements, limiting their practicality at scale. We present Simulate Anything, a graphics-driven world modeling and simulation framework that enables efficient generation of high-fidelity embodied training data using only multi-view environment videos and off-the-shelf assets. Our approach reconstructs real-world environments into a photorealistic scene representation using 3D Gaussian Splatting (3DGS), seamlessly capturing fine-grained geometry and appearance from video. We then leverage generative models to recover a physically realistic representation and integrate it into a simulation environment via a precision calibration target, enabling accurate scale alignment between the reconstructed scene and the real world. Together, these components provide a unified, editable, and physically grounded world model. Vision Language Action (VLA) models trained on our simulated data achieve strong zero-shot performance on downstream tasks, matching or even surpassing results obtained with real-world data, highlighting the potential of reconstruction-driven world modeling for scalable and practical embodied intelligence training.", "AI": {"tldr": "提出Simulate Anything框架，利用多视角视频和现成资产高效生成高质量的具身智能训练数据，重建物理现实环境，模拟结果显著。", "motivation": "解决现有模拟平台在视觉和物理上与真实环境存在较大差距的问题，同时避免使用昂贵的传感器、精确的机器人校准或深度测量的需求。", "method": "通过多视角环境视频和现成的资产，使用3D高斯喷射（3DGS）从视频中捕捉细粒度的几何和外观，重建现实环境并通过生成模型恢复物理现实的表示，使模拟环境中的精确标尺对齐成为可能。", "result": "在模拟数据上训练的Vision Language Action (VLA) 模型在下游任务中获得了强大的零射击性能，其结果可以媲美甚至超越使用真实世界数据的成绩。", "conclusion": "证明了通过重建设驱动的世界建模，可以使具身智能训练在规模化和实践性上变得可能。"}}
{"id": "2602.00104", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00104", "abs": "https://arxiv.org/abs/2602.00104", "authors": ["Zhuohong Chen", "Zhengxian Wu", "Zirui Liao", "Shenao Jiang", "Hangrui Xu", "Yang Chen", "Chaokui Su", "Xiaoyu Liu", "Haoqian Wang"], "title": "R3G: A Reasoning--Retrieval--Reranking Framework for Vision-Centric Answer Generation", "comment": null, "summary": "Vision-centric retrieval for VQA requires retrieving images to supply missing visual cues and integrating them into the reasoning process. However, selecting the right images and integrating them effectively into the model's reasoning remains challenging.To address this challenge, we propose R3G, a modular Reasoning-Retrieval-Reranking framework.It first produces a brief reasoning plan that specifies the required visual cues, then adopts a two-stage strategy, with coarse retrieval followed by fine-grained reranking, to select evidence images.On MRAG-Bench, R3G improves accuracy across six MLLM backbones and nine sub-scenarios, achieving state-of-the-art overall performance. Ablations show that sufficiency-aware reranking and reasoning steps are complementary, helping the model both choose the right images and use them well. We release code and data at https://github.com/czh24/R3G.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2602.00105", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00105", "abs": "https://arxiv.org/abs/2602.00105", "authors": ["Wing Chan", "Richard Allen"], "title": "HYPE-EDIT-1: Benchmark for Measuring Reliability in Frontier Image Editing Models", "comment": "14 pages, 5 figures, for code and data, see https://github.com/sourceful-official/hype-edit-1-benchmark", "summary": "Public demos of image editing models are typically best-case samples; real workflows pay for retries and review time. We introduce HYPE-EDIT-1, a 100-task benchmark of reference-based marketing/design edits with binary pass/fail judging. For each task we generate 10 independent outputs to estimate per-attempt pass rate, pass@10, expected attempts under a retry cap, and an effective cost per successful edit that combines model price with human review time. We release 50 public tasks and maintain a 50-task held-out private split for server-side evaluation, plus a standardized JSON schema and tooling for VLM and human-based judging. Across the evaluated models, per-attempt pass rates span 34-83 percent and effective cost per success spans USD 0.66-1.42. Models that have low per-image pricing are more expensive when you consider the total effective cost of retries and human reviews.", "AI": {"tldr": "研究提出了HYPE-EDIT-1基准测试，用于评估营销和设计编辑模型的实际表现。结果显示，考虑重试和人工审核时间后，低价模型的实际成本可能更高。", "motivation": "公共展示的图像编辑模型演示通常是最佳样本；实际工作流程要付出重试和审核时间。为了评估这些模型的真实表现，我们提出了这一基准测试方法。", "method": "我们介绍了HYPE-EDIT-1，这是一个100任务基准测试，用于基于参考的营销/设计编辑，以二进制通过/失败为评判标准。对于每个任务，我们生成10个独立的输出来估计每次尝试的通过率、前十次尝试的通过率、在重试限制下的预期尝试次数以及考虑到模型价格和人审时间的有效每次成功编辑成本。", "result": "评估的模型每次尝试的通过率在34%到83%之间，每次成功的有效成本在0.66到1.42美元之间。模型按每张图片定价较低的话，在考虑总的重试和人审核成本时，总的成本会更高。", "conclusion": "这项研究显示，虽然一些模型的单次定价较低，但考虑到模型的多次尝试和人审的时间成本，这些低价模型的实际使用成本可能并不低。这强调了在考虑图像编辑模型的成本时，需要综合考虑多个因素。"}}
{"id": "2602.00007", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00007", "abs": "https://arxiv.org/abs/2602.00007", "authors": ["MinGyu Jeon", "SuWan Cho", "JaeYoung Shu"], "title": "PPoGA: Predictive Plan-on-Graph with Action for Knowledge Graph Question Answering", "comment": null, "summary": "Large Language Models (LLMs) augmented with Knowledge Graphs (KGs) have advanced complex question answering, yet they often remain susceptible to failure when their initial high-level reasoning plan is flawed. This limitation, analogous to cognitive functional fixedness, prevents agents from restructuring their approach, leading them to pursue unworkable solutions. To address this, we propose PPoGA (Predictive Plan-on-Graph with Action), a novel KGQA framework inspired by human cognitive control and problem-solving. PPoGA incorporates a Planner-Executor architecture to separate high-level strategy from low-level execution and leverages a Predictive Processing mechanism to anticipate outcomes. The core innovation of our work is a self-correction mechanism that empowers the agent to perform not only Path Correction for local execution errors but also Plan Correction by identifying, discarding, and reformulating the entire plan when it proves ineffective. We conduct extensive experiments on three challenging multi-hop KGQA benchmarks: GrailQA, CWQ, and WebQSP. The results demonstrate that PPoGA achieves state-of-the-art performance, significantly outperforming existing methods. Our work highlights the critical importance of metacognitive abilities like problem restructuring for building more robust and flexible AI reasoning systems.", "AI": {"tldr": "文章提出了PPoGA框架，通过预测处理和自我修正机制来增强大型语言模型的问题解决能力，并在三个挑战性的多跳KGQA数据集上实现了最先进的性能。", "motivation": "虽然大型语言模型（LLMs）增强了复杂问题回答的能力，但它们在初期策略错误时仍容易失败，类似于认知功能固着。为解决此问题而提出了PPoGA。", "method": "PPoGA (Predictive Plan-on-Graph with Action) 是一种新的KGQA框架，它结合了人类认知控制与问题解决的灵感。该框架采用计划者-执行者架构分离高级策略与低级执行，并利用预测处理机制预测结果。核心创新是一种自我修正机制，它不仅可以纠正局部执行错误的路径，还可以识别、丢弃并重塑整个计划。", "result": "在GrailQA、CWQ和WebQSP这三个多跳KGQA基准测试上，PPoGA的性能优于现有方法，证明了其有效性。", "conclusion": "该研究强调了元认知能力（如问题重构）对于构建更强大、灵活的AI推理系统的重要性。"}}
{"id": "2602.00107", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00107", "abs": "https://arxiv.org/abs/2602.00107", "authors": ["Yuan Gao", "Xinyu Guo", "Wenjing Xie", "Zifan Wang", "Hongwen Yu", "Gongyang Li", "Shugong Xu"], "title": "Efficient UAV trajectory prediction: A multi-modal deep diffusion framework", "comment": "in Chinese language", "summary": "To meet the requirements for managing unauthorized UAVs in the low-altitude economy, a multi-modal UAV trajectory prediction method based on the fusion of LiDAR and millimeter-wave radar information is proposed. A deep fusion network for multi-modal UAV trajectory prediction, termed the Multi-Modal Deep Fusion Framework, is designed. The overall architecture consists of two modality-specific feature extraction networks and a bidirectional cross-attention fusion module, aiming to fully exploit the complementary information of LiDAR and radar point clouds in spatial geometric structure and dynamic reflection characteristics. In the feature extraction stage, the model employs independent but structurally identical feature encoders for LiDAR and radar. After feature extraction, the model enters the Bidirectional Cross-Attention Mechanism stage to achieve information complementarity and semantic alignment between the two modalities. To verify the effectiveness of the proposed model, the MMAUD dataset used in the CVPR 2024 UG2+ UAV Tracking and Pose-Estimation Challenge is adopted as the training and testing dataset. Experimental results show that the proposed multi-modal fusion model significantly improves trajectory prediction accuracy, achieving a 40% improvement compared to the baseline model. In addition, ablation experiments are conducted to demonstrate the effectiveness of different loss functions and post-processing strategies in improving model performance. The proposed model can effectively utilize multi-modal data and provides an efficient solution for unauthorized UAV trajectory prediction in the low-altitude economy.", "AI": {"tldr": "提出了一种基于LiDAR和毫米波雷达信息融合的多模态无人机轨迹预测方法，设计了一个名为多模态深度融合框架的网络，实验表明该方法显著提高了预测精度，相比基准模型提高了40%。", "motivation": "为了满足低空经济中对未经授权的无人机进行管理的需求，需要一种准确的无人机轨迹预测方法，以适应LiDAR和毫米波雷达的互补信息特性，同时提高预测的准确性。", "method": "设计了一个多模态深度融合框架，包括两个独立但结构相同的特征提取网络和一个双向交叉注意力融合模块，通过深度融合LiDAR和雷达点云的空间几何结构和动态反射特性信息以提高轨迹预测准确性。", "result": "采用CVPR 2024 UG2+无人机跟踪和姿态估计挑战赛使用的MMAUD数据集进行实验，结果显示该多模态融合模型将轨道预测精度提升了40%，此外进行的消融实验证明了不同损失函数和后处理策略的有效性。", "conclusion": "所提出的模型能够有效利用多模态数据，并为低空经济中的未经授权的无人机轨迹预测提供了高效的解决方案。"}}
{"id": "2602.00009", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00009", "abs": "https://arxiv.org/abs/2602.00009", "authors": ["Samuel Thio", "Matthew Lewis", "Spiros Denaxas", "Richard JB Dobson"], "title": "Unlocking Electronic Health Records: A Hybrid Graph RAG Approach to Safe Clinical AI for Patient QA", "comment": "26 pages, 5 figures, 2 tables", "summary": "Electronic health record (EHR) systems present clinicians with vast repositories of clinical information, creating a significant cognitive burden where critical details are easily overlooked. While Large Language Models (LLMs) offer transformative potential for data processing, they face significant limitations in clinical settings, particularly regarding context grounding and hallucinations. Current solutions typically isolate retrieval methods focusing either on structured data (SQL/Cypher) or unstructured semantic search but fail to integrate both simultaneously. This work presents MediGRAF (Medical Graph Retrieval Augmented Framework), a novel hybrid Graph RAG system that bridges this gap. By uniquely combining Neo4j Text2Cypher capabilities for structured relationship traversal with vector embeddings for unstructured narrative retrieval, MediGRAF enables natural language querying of the complete patient journey. Using 10 patients from the MIMIC-IV dataset (generating 5,973 nodes and 5,963 relationships), we generated enough nodes and data for patient level question answering (QA), and we evaluated this architecture across varying query complexities. The system demonstrated 100\\% recall for factual queries which means all relevant information was retrieved and in the output, while complex inference tasks achieved a mean expert quality score of 4.25/5 with zero safety violations. These results demonstrate that hybrid graph-grounding significantly advances clinical information retrieval, offering a safer, more comprehensive alternative to standard LLM deployments.", "AI": {"tldr": "本文提出了一种名为MediGRAF的新的混合图形RAG系统，该系统集成了结构化数据和非结构化数据的检索方法，从而实现临床信息的更高效和安全的查询和检索。", "motivation": "电子健康记录系统向临床医生提供了大量的信息，导致重要的细节容易被忽视，而语言模型在临床环境中面临一些局限性，特别是在情境理解和避免误导性信息生成方面。目前的解决方案要么集中在结构化数据的检索，要么集中在非结构化语义搜索上，但未能同时整合两者。", "method": "本研究提出了一种名为MediGRAF的创新混合图检索增强框架，它结合了Neo4j Text2Cypher功能用于结构化关系遍历和向量嵌入用于非结构化叙述检索，从而实现对完整患者历程的自然语言查询。", "result": "通过使用MIMIC-IV数据集中的10位患者（生成5,973个节点和5,963个关系），该架构在不同查询复杂度下的评估显示，对于事实性查询，MediGRAF的召回率达到100%，而对于复杂的推理任务，它获得了4.25/5的专家质量评分，且没有出现任何安全问题。", "conclusion": "MediGRAF这种组合了图框架的方法在临床信息检索中表现出了显著的进步，提供了一个比标准语言模型部署更为安全、全面的替代方案。"}}
{"id": "2602.00108", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00108", "abs": "https://arxiv.org/abs/2602.00108", "authors": ["René Peinl", "Vincent Tischler", "Patrick Schröder", "Christian Groth"], "title": "SITUATE -- Synthetic Object Counting Dataset for VLM training", "comment": "accepted at 21st International Conference on Computer Vision Theory and Applications", "summary": "We present SITUATE, a novel dataset designed for training and evaluating Vision Language Models on counting tasks with spatial constraints. The dataset bridges the gap between simple 2D datasets like VLMCountBench and often ambiguous real-life datasets like TallyQA, which lack control over occlusions and spatial composition. Experiments show that our dataset helps to improve generalization for out-of-distribution images, since a finetune of Qwen VL 2.5 7B on SITUATE improves accuracy on the Pixmo count test data, but not vice versa. We cross validate this by comparing the model performance across established other counting benchmarks and against an equally sized fine-tuning set derived from Pixmo count.", "AI": {"tldr": "本研究提出SITUATE数据集，旨在通过数量任务中的空间约束条件训练和评估视觉语言模型，实验结果显示该数据集提高了模型对分布外图像的一般化能力。", "motivation": "设计SITUATE数据集，以解决简单2D数据集和粗糙的现实数据集之间的不足，尤其是在控制遮挡和空间组合方面。", "method": "提出了名为SITUATE的新数据集，用于在具有空间约束条件的数量任务中训练和评估视觉语言模型。该数据集填补了简单2D数据集（如VLMCountBench）与通常模糊的现实世界数据集（如TallyQA）之间的空白，后者难以控制遮挡和空间组合。", "result": "实验表明，我们的数据集有助于提高对分布外图像的一般化能力，因为在SITUATE上微调Qwen VL 2.5 7B模型可以提升在Pixmo数量测试数据上的准确性，但反过来则不成立。通过对其他已建立的数量基准和同等规模从Pixmo数量数据集中提取的微调集进行交叉验证，进一步验证了这一点。", "conclusion": "SITUATE数据集展示了其在提升视觉语言模型数量任务上的效能，特别是满足空间约束条件下对分布外数据的一般化性能。"}}
{"id": "2602.00015", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00015", "abs": "https://arxiv.org/abs/2602.00015", "authors": ["Xun Xu"], "title": "G-MemLLM: Gated Latent Memory Augmentation for Long-Context Reasoning in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, yet they remain constrained by the finite capacity of their context windows and the inherent difficulty of maintaining long-term factual consistency during multi-hop reasoning. While existing methods utilize context compression or recurrent tokens, they often suffer from ``context rot'' or the dilution of information over long horizons. In this paper, we propose \\textbf{G-MemLLM}, a memory-augmented architecture that integrates a frozen LLM backbone with a trainable \\textbf{Latent Memory Bank}. Our key innovation is a GRU-style gated update logic that allows the model to selectively update, preserve, or overwrite latent memory slots, preventing the vanishing gradients of knowledge common in recurrent systems. We evaluate G-MemLLM across scales, from GPT-2 (124M) to Llama 3.1 (8B), on the HotpotQA and Zero-Shot Relation Extraction (ZsRE) benchmarks. Our results demonstrate that G-MemLLM significantly enhances multi-hop reasoning and relational precision, achieving a 13.3\\% accuracy boost on ZsRE for Llama 3.1-8B, and it also yields improvements across model scales, boosting Answer F1 by 8.56 points for GPT-2 and increasing Supporting Fact F1 by 6.89 points for Llama 3.1-8B on HotpotQA.", "AI": {"tldr": "本文介绍了G-MemLLM，一种结合了冻结的LLM主干和可训练的潜在记忆库的内存增强架构。它引入了GRU样式的门控更新机制来选择性地更新、保持或重写存储的潜在记忆槽。实验结果显示该方法在多跳推理和关系精度方面有显著提高，尤其是在ZsRE基准测试上Llama 3.1-8B的准确性提高了13.3%；GPT-2和Llama 3.1-8B在HotpotQA上的答题F1值和辅助事实F1值分别提升8.56和6.89点。", "motivation": "现有方法使用上下文压缩或循环标记，但仍面临上下文窗口有限和长时间推理时保持事实一致性的问题。本文旨在解决这些问题，并增强大语言模型的长期记忆能力。", "method": "G-MemLLM是一个内存增强的架构，它通过一个冻结的LLM主干与一个可训练的潜在记忆库结合，并使用了类似GRU的门控更新机制来选择性地更新、保持或重写记忆槽。", "result": "在HotpotQA和Zero-Shot Relation Extraction基准测试上进行了实验，表明G-MemLLM显著提高了多跳推理和关系精度，例如，在ZsRE上Llama 3.1-8B的准确性就提高了13.3%，同时在HotpotQA上GPT-2和Llama 3.1-8B的答题F1和辅助事实F1值分别提升8.56和6.89点。", "conclusion": "本文提出的G-MemLLM架构，采用类似GRU的内存更新机制，大大改善了大语言模型在保持长期事实一致性和多跳推理能力方面的性能。"}}
{"id": "2602.00109", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00109", "abs": "https://arxiv.org/abs/2602.00109", "authors": ["John J. Howard", "Richard O. Plesh", "Yevgeniy B. Sirotin", "Jerry L. Tipton", "Arun R. Vemury"], "title": "Robustness of Presentation Attack Detection in Remote Identity Validation Scenarios", "comment": "Accepted to the IEEE/CVF WACV 2026 Workshop on Generative, Adversarial and Presentation Attacks in Biometrics (GAPBio). 8 pages, 6 figures, 4 tables", "summary": "Presentation attack detection (PAD) subsystems are an important part of effective and user-friendly remote identity validation (RIV) systems. However, ensuring robust performance across diverse environmental and procedural conditions remains a critical challenge. This paper investigates the impact of low-light conditions and automated image acquisition on the robustness of commercial PAD systems using a scenario test of RIV. Our results show that PAD systems experience a significant decline in performance when utilized in low-light or auto-capture scenarios, with a model-predicted increase in error rates by a factor of about four under low-light conditions and a doubling of those odds under auto-capture workflows. Specifically, only one of the tested systems was robust to these perturbations, maintaining a maximum bona fide presentation classification error rate below 3% across all scenarios. Our findings emphasize the importance of testing across diverse environments to ensure robust and reliable PAD performance in real-world applications.", "AI": {"tldr": "The paper investigates how PAD systems perform in low-light and auto-capture conditions, finding a significant decrease in their effectiveness except for one robust system.", "motivation": "The motivation is to assess the robustness of commercial PAD systems under varying conditions to ensure reliable performance in real-world remote identity validation systems.", "method": "A scenario test of remote identity validation (RIV) was conducted to evaluate PAD systems under low-light conditions and automated image acquisition.", "result": "PAD systems demonstrated a significant performance decline under the tested conditions, with the error rates increasing significantly while only one system remained robust.", "conclusion": "The conclusion is that robust testing across diverse environments is essential to ensure the dependability of PAD systems in real-world applications."}}
{"id": "2602.00016", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00016", "abs": "https://arxiv.org/abs/2602.00016", "authors": ["Jiongchi Yu", "Yuhan Ma", "Xiaoyu Zhang", "Junjie Wang", "Qiang Hu", "Chao Shen", "Xiaofei Xie"], "title": "PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems", "comment": "28 pages", "summary": "With the increasing deployment of large language models (LLMs) in affective agents and AI systems, maintaining a consistent and authentic LLM personality becomes critical for user trust and engagement. However, existing work overlooks a fundamental psychological consensus that personality traits are dynamic and context-dependent. To bridge this gap, we introduce PTCBENCH, a systematic benchmark designed to quantify the consistency of LLM personalities under controlled situational contexts. PTCBENCH subjects models to 12 distinct external conditions spanning diverse location contexts and life events, and rigorously assesses the personality using the NEO Five-Factor Inventory. Our study on 39,240 personality trait records reveals that certain external scenarios (e.g., \"Unemployment\") can trigger significant personality changes of LLMs, and even alter their reasoning capabilities. Overall, PTCBENCH establishes an extensible framework for evaluating personality consistency in realistic, evolving environments, offering actionable insights for developing robust and psychologically aligned AI systems.", "AI": {"tldr": "PTCBENCH is a benchmark designed to assess the consistency of large language model personalities under different scenarios, indicating that certain conditions can significantly alter LLM personalities and reasoning capabilities.", "motivation": "The paper addresses the need for consistent and authentic LLM personality in AI systems and affective agents, noting the current oversight of dynamic and context-dependent personality traits.", "method": "The authors develop PTCBENCH, which tests LLMs under 12 distinct scenario conditions and evaluates personality consistency using the NEO Five-Factor Inventory.", "result": "Analysis of 39,240 personality trait records shows that specific external conditions, such as unemployment, can lead to significant changes in LLM personalities and reasoning abilities.", "conclusion": "PTCBENCH provides an extensible framework for evaluating personality consistency in dynamic environments, offering insights for creating more robust and psychologically aligned AI systems."}}
{"id": "2602.00110", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00110", "abs": "https://arxiv.org/abs/2602.00110", "authors": ["Yu Li", "Guilherme N. DeSouza", "Praveen Rao", "Chi-Ren Shyu"], "title": "Observing Health Outcomes Using Remote Sensing Imagery and Geo-Context Guided Visual Transformer", "comment": "Submitted to IEEE Transactions on Geoscience and Remote Sensing", "summary": "Visual transformers have driven major progress in remote sensing image analysis, particularly in object detection and segmentation. Recent vision-language and multimodal models further extend these capabilities by incorporating auxiliary information, including captions, question and answer pairs, and metadata, which broadens applications beyond conventional computer vision tasks. However, these models are typically optimized for semantic alignment between visual and textual content rather than geospatial understanding, and therefore are not suited for representing or reasoning with structured geospatial layers. In this study, we propose a novel model that enhances remote sensing imagery processing with guidance from auxiliary geospatial information. Our approach introduces a geospatial embedding mechanism that transforms diverse geospatial data into embedding patches that are spatially aligned with image patches. To facilitate cross-modal interaction, we design a guided attention module that dynamically integrates multimodal information by computing attention weights based on correlations with auxiliary data, thereby directing the model toward the most relevant regions. In addition, the module assigns distinct roles to individual attention heads, allowing the model to capture complementary aspects of the guidance information and improving the interpretability of its predictions. Experimental results demonstrate that the proposed framework outperforms existing pretrained geospatial foundation models in predicting disease prevalence, highlighting its effectiveness in multimodal geospatial understanding.", "AI": {"tldr": "本文提出了一种新的方法，利用地理空间信息来增强遥感图像处理，通过空间对齐的嵌入块转化地理空间数据，并设计了引导注意力模块来动态整合多模态信息，提高了预测的准确性和可解释性。研究表明，这种方法在多模态地理空间理解方面表现出色。", "motivation": "视觉变换器在遥感图像分析、目标检测和分割方面取得了重大进展。最近的视觉-语言和多模态模型通过结合辅助信息进一步扩展了这些能力。然而，这些模型通常旨在优化视觉和文本内容之间的语义对齐而非地理空间理解，因此不适用于表示或推理结构化的地理空间层。", "method": "本研究提出了一种新颖的模型，通过辅助地理空间信息增强遥感图像处理。该方法引入了一种地理空间嵌入机制，将多样化的地理空间数据转换成与图像补丁空间对齐的嵌入补丁。为了促进跨模态交互，设计了一个引导注意模块，通过基于辅助数据的相关性计算注意力权重动态整合多模态信息。此外，该模块为各个注意力头分配了不同的角色，使模型能够捕捉指导信息的互补方面，从而提高预测的可解释性。", "result": "实验结果证明，该框架在预测疾病流行率方面优于现有的预训练地理空间基础模型。", "conclusion": "实验结果表明，所提出的框架在预测疾病流行率方面优于现有的预训练地理空间基础模型，体现了其在多模态地理空间理解方面的能力。"}}
{"id": "2602.00017", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.00017", "abs": "https://arxiv.org/abs/2602.00017", "authors": ["Benyamin Tabarsi", "Wenbo Li", "Tahreem Yasir", "Aryan Santhosh Kumar", "Laura Widman", "Dongkuan Xu", "Tiffany Barnes"], "title": "SafeTalkCoach: Diversity-Driven Multi-Agent Simulation for Parent-Teen Health Conversations", "comment": null, "summary": "The importance of effective parent-child communication about sexual health is widely acknowledged, but real-world data on these conversations is scarce and challenging to collect, due to their private and sensitive nature. Although LLMs have been widely adopted in dialogue generation, they may deviate from best practices and frequently lack realism and diversity. We introduce SafeTalkCoach, a diversity-driven multi-agent dialogue generation framework that simulates parent-child conversations about sexual health, and present an accompanying dataset. SafeTalkCoach integrates crowd-sourced and synthesized scenarios, established sexual health guidelines, evidence-based personas, adaptive control modules, and hierarchical diversification. Through evaluations, we demonstrate that SafeTalkCoach generates diverse conversations while maintaining realism, communication quality, and controllability in practice. Our goal is that the SafeTalkCoach framework and the dataset support both AI research and health communications practices.", "AI": {"tldr": "研究介绍了一个用于生成父母与孩子之间关于性健康话题的多样化对话的框架 SafeTalkCoach，它通过多种方式确保对话的多样性、现实性和质量。", "motivation": "尽管大语言模型在对话生成中被广泛应用，但它们可能偏离最佳实践，通常缺乏现实性和多样性。本研究旨在通过SafeTalkCoach解决这一问题。", "method": "SafeTalkCoach 是一个基于多样性的多智能体对话生成框架，用于模拟父母和孩子关于性健康的话题。它整合了众包和合成的场景、已建立的性健康指南、基于证据的人物设定、自适应控制模块以及层级多样化。", "result": "评估结果表明，SafeTalkCoach 能够生成多样化对话，同时维持现实性、沟通质量和可控性。", "conclusion": "SafeTalkCoach 框架和数据集旨在支持AI研究和健康沟通实践。"}}
{"id": "2602.00111", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00111", "abs": "https://arxiv.org/abs/2602.00111", "authors": ["Haiyu Yang", "Heidi Lesscher", "Enhong Liu", "Miel Hostens"], "title": "From Manual Observation to Automated Monitoring: Space Allowance Effects on Play Behaviour in Group-Housed Dairy Calves", "comment": null, "summary": "Play behaviour serves as a positive welfare indicator in dairy calves, yet the influence of space allowance under commercial conditions remains poorly characterized, particularly at intermediate-to-high allowances (6-20 m2 per calf). This study investigated the relationship between space allowance and play behaviour in 60 group-housed dairy calves across 14 commercial farms in the Netherlands (space range: 2.66-17.98 m2 per calf), and developed an automated computer vision pipeline for scalable monitoring. Video observations were analyzed using a detailed ethogram, with play expressed as percentage of observation period (%OP). Statistical analysis employed linear mixed models with farm as a random effect. A computer vision pipeline was trained on manual annotations from 108 hours on 6 farms and validated on held-out test data. The computer vision classifier achieved 97.6% accuracy with 99.4% recall for active play detection. Calves spent on average 1.0% of OP playing reflecting around 10 minutes per 17-hour period. The space-play relationship was non-linear, with highest play levels at 8-10 m2 per calf (1.6% OP) and lowest at 6-8 m2 and 12-14 m2 (<0.6% OP). Space remained significant after controlling for age, health, and group size. In summary, these findings suggest that 8-10 m2 per calf represents a practical target balancing welfare benefits with economic feasibility, and demonstrate that automated monitoring can scale small annotation projects to continuous welfare assessment systems.", "AI": {"tldr": "研究发现，奶牛犊在每头牛8-10平方米的活动空间下表现出最高的玩乐行为，这对福利有益且经济可行。此外，研究开发了自动化计算机视觉系统来监测玩乐行为。", "motivation": "探讨商业条件下不同空间分配对奶牛犊玩乐行为的影响，特别是选择在6-20平方米的空间范围内进行研究。", "method": "Structure", "result": "{\n  \"tldr\": \"研究发现，奶牛犊在每头牛8-10平方米的活动空间下表现出最高的玩乐行为，这对福利有益且经济可行。此外，研究开发了自动化计算机视觉系统来监测玩乐行为。\",\n  \"motivation\": \"探讨商业条件下不同空间分配对奶牛犊玩乐行为的影响，特别是选择在6-20平方米的空间范围内进行研究。\",\n  \"method\": \"通过收集荷兰14个农场中的60群组奶牛犊的数据，并利用详细的石化方式分析视频观察，结合计算机视觉技术进行玩乐行为的自动监测。\",\n  \"result\": \"计算机视觉分类器在活动玩乐检测中达到97.6%的准确率和99.4%的召回率。奶牛犊平均花了1%的观察时间玩耍，而8-10平方米的空间使玩乐行为最优。\",\n  \"conclusion\": \"研究结论建议每头牛8-10平方米作为兼顾福利与经济可行性的理想空间配置，并证明自动化监测系统可应用于持续的福利评估。\\n\"}\n}", "conclusion": "研究结论建议每头牛8-10平方米作为兼顾福利与经济可行性的理想空间配置，并证明自动化监测系统可应用于持续的福利评估。"}}
