{"id": "2512.03126", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03126", "abs": "https://arxiv.org/abs/2512.03126", "authors": ["Shan Zhang", "Aotian Chen", "Kai Zou", "Jindong Gu", "Yuan Xue", "Anton van den Hengel"], "title": "Hierarchical Process Reward Models are Symbolic Vision Learners", "comment": null, "summary": "Symbolic computer vision represents diagrams through explicit logical rules and structured representations, enabling interpretable understanding in machine vision. This requires fundamentally different learning paradigms from pixel-based visual models. Symbolic visual learners parse diagrams into geometric primitives-points, lines, and shapes-whereas pixel-based learners operate on textures and colors. We propose a novel self-supervised symbolic auto-encoder that encodes diagrams into structured primitives and their interrelationships within the latent space, and decodes them through our executable engine to reconstruct the input diagrams. Central to this architecture is Symbolic Hierarchical Process Reward Modeling, which applies hierarchical step-level parsing rewards to enforce point-on-line, line-on-shape, and shape-on-relation consistency. Since vanilla reinforcement learning exhibits poor exploration in the policy space during diagram reconstruction; we thus introduce stabilization mechanisms to balance exploration and exploitation. We fine-tune our symbolic encoder on downstream tasks, developing a neuro-symbolic system that integrates the reasoning capabilities of neural networks with the interpretability of symbolic models through reasoning-grounded visual rewards. Evaluations across reconstruction, perception, and reasoning tasks demonstrate the effectiveness of our approach: achieving a 98.2% reduction in MSE for geometric diagram reconstruction, surpassing GPT-4o by 0.6% with a 7B model on chart reconstruction, and improving by +13% on the MathGlance perception benchmark, and by +3% on MathVerse and GeoQA reasoning benchmarks.", "AI": {"tldr": "本文介绍了一种新的自我监督符号自动编码器，显著提升了图形的重构和理解能力。", "motivation": "该研究旨在解决基于传统的像素视觉模型和基于符号的视觉学习之间的根本差异。符号计算机视觉使用显式的逻辑规则和结构化的表示方法，可以提供机器视觉可解释的理解。这需要一种与基于像素的视觉模型完全不同的学习范式。", "method": "文中提出了一种新的自我监督的符号自动编码器，该编码器能够将图形编码为结构化的基元及其内在联系，并通过执行引擎解码来重构输入的图形。核心部分是使用符号层次过程奖励模型，通过分层的步骤级解析奖励来确保点在线上、线在形状上、形状在关系上的一致性。由于传统的强化学习在图形重构中表现出在策略空间中的探索不足，因此引入了稳定机制来平衡探索和利用。", "result": "该系统在图形重构、感知和推理任务上取得了显著的效果，包括实现了显著的MSE减少和在多个基准测试中超越其他模型。", "conclusion": "实验结果证明了这种方法的有效性，实现了几何图形重构中MSE减少了98.2%，在图形重构上超过了GPT-4o（+0.6%）且使用了70亿参数的模型，在MathGlance感知基准上提高了13%，在MathVerse和GeoQA推理基准上提高了3%。"}}
{"id": "2512.03182", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03182", "abs": "https://arxiv.org/abs/2512.03182", "authors": ["Yasser Taha", "Grégoire Montavon", "Nils Körber"], "title": "Drainage: A Unifying Framework for Addressing Class Uncertainty", "comment": "16 pages, 8 figures", "summary": "Modern deep learning faces significant challenges with noisy labels, class ambiguity, as well as the need to robustly reject out-of-distribution or corrupted samples. In this work, we propose a unified framework based on the concept of a \"drainage node'' which we add at the output of the network. The node serves to reallocate probability mass toward uncertainty, while preserving desirable properties such as end-to-end training and differentiability. This mechanism provides a natural escape route for highly ambiguous, anomalous, or noisy samples, particularly relevant for instance-dependent and asymmetric label noise. In systematic experiments involving the addition of varying proportions of instance-dependent noise or asymmetric noise to CIFAR-10/100 labels, our drainage formulation achieves an accuracy increase of up to 9\\% over existing approaches in the high-noise regime. Our results on real-world datasets, such as mini-WebVision, mini-ImageNet and Clothing-1M, match or surpass existing state-of-the-art methods. Qualitative analysis reveals a denoising effect, where the drainage neuron consistently absorbs corrupt, mislabeled, or outlier data, leading to more stable decision boundaries. Furthermore, our drainage formulation enables applications well beyond classification, with immediate benefits for web-scale, semi-supervised dataset cleaning, and open-set applications.", "AI": {"tldr": "本文提出基于排水节点的统一框架提高模型对噪声标签的鲁棒性，特别是在高噪声环境下，模型性能较传统方法提高高达9%。", "motivation": "本文旨在解决现有深度学习面临噪声标签、类别不确定性以及需要稳健地拒绝分布外或被污染样本的挑战。", "method": "本文提出了一个基于“排水节点”的统一框架，该节点被添加在网络的输出端。排水节点起到重新分配概率质量，增加不确定性的作用，同时保留了端到端训练和可微分性等优良属性。", "result": "在添加不同比例的实例依赖噪声或非对称噪声的CIFAR-10/100数据集上，本方法实现了比现有方法更高的准确率。在现实世界数据集如mini-WebVision, mini-ImageNet和Clothing-1M上，本方法的性能也达到了或超过了最新的先进方法。", "conclusion": "定性分析表明，排水节点具有明显的去噪效果，能够减少腐蚀、有误标注或异常数据，稳定决策边界。此外，该排水框架不仅能应用于分类任务，还能应用于大规模网络的半监督数据清理和开放式任务。"}}
{"id": "2512.03199", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03199", "abs": "https://arxiv.org/abs/2512.03199", "authors": ["Justin Norman", "Hany Farid"], "title": "Does Head Pose Correction Improve Biometric Facial Recognition?", "comment": null, "summary": "Biometric facial recognition models often demonstrate significant decreases in accuracy when processing real-world images, often characterized by poor quality, non-frontal subject poses, and subject occlusions. We investigate whether targeted, AI-driven, head-pose correction and image restoration can improve recognition accuracy. Using a model-agnostic, large-scale, forensic-evaluation pipeline, we assess the impact of three restoration approaches: 3D reconstruction (NextFace), 2D frontalization (CFR-GAN), and feature enhancement (CodeFormer). We find that naive application of these techniques substantially degrades facial recognition accuracy. However, we also find that selective application of CFR-GAN combined with CodeFormer yields meaningful improvements.", "AI": {"tldr": "The paper explores whether AI-driven head-pose correction and image restoration can improve the accuracy of facial recognition in real-world images. Selective use of certain techniques shows positive effects.", "motivation": "Facial recognition accuracy decreases in real-world images due to poor image quality, non-frontal poses, and occlusions. The motivation is to enhance recognition accuracy with AI-driven image processing.", "method": "The study uses a large-scale forensic-evaluation pipeline to assess the impact of three image restoration techniques: 3D reconstruction (NextFace), 2D frontalization (CFR-GAN), and feature enhancement (CodeFormer).", "result": "Naive application of the restoration techniques can reduce facial recognition accuracy. However, applying CFR-GAN along with CodeFormer selectively enhances the accuracy.", "conclusion": "AI-driven image restoration combined selectively can improve facial recognition in challenging scenarios. This may pave the way for more robust facial recognition systems in the future."}}
{"id": "2512.03210", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03210", "abs": "https://arxiv.org/abs/2512.03210", "authors": ["Jingkang Wang", "Henry Che", "Yun Chen", "Ze Yang", "Lily Goli", "Sivabalan Manivasagam", "Raquel Urtasun"], "title": "Flux4D: Flow-based Unsupervised 4D Reconstruction", "comment": "NeurIPS 2025. Project page: https://waabi.ai/flux4d/", "summary": "Reconstructing large-scale dynamic scenes from visual observations is a fundamental challenge in computer vision, with critical implications for robotics and autonomous systems. While recent differentiable rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved impressive photorealistic reconstruction, they suffer from scalability limitations and require annotations to decouple actor motion. Existing self-supervised methods attempt to eliminate explicit annotations by leveraging motion cues and geometric priors, yet they remain constrained by per-scene optimization and sensitivity to hyperparameter tuning. In this paper, we introduce Flux4D, a simple and scalable framework for 4D reconstruction of large-scale dynamic scenes. Flux4D directly predicts 3D Gaussians and their motion dynamics to reconstruct sensor observations in a fully unsupervised manner. By adopting only photometric losses and enforcing an \"as static as possible\" regularization, Flux4D learns to decompose dynamic elements directly from raw data without requiring pre-trained supervised models or foundational priors simply by training across many scenes. Our approach enables efficient reconstruction of dynamic scenes within seconds, scales effectively to large datasets, and generalizes well to unseen environments, including rare and unknown objects. Experiments on outdoor driving datasets show Flux4D significantly outperforms existing methods in scalability, generalization, and reconstruction quality.", "AI": {"tldr": "提出了一个简单且可扩展的框架Flux4D，用于大规模动态场景的4D重建，能够在未标注的数据上实现高效、高质量的场景重建。", "motivation": "克服现有方法如NeRF和3DGS在大规模动态场景重建中的可扩展性限制，并避免需要标注来分离动态物体的必要性。同时，解决自监督方法受到场景特定优化和超参数敏感性的限制。", "method": " Flux4D直接从原始数据中预测3D高斯分布及其运动动态，仅通过光度损失和\"尽可能静态\"的正则化来实现无需预训练模型或基础先验知识的动态元素分解。", "result": "实验显示，Flux4D在可扩展性、泛化能力和重建质量方面显著优于现有方法，能在几秒钟内实现动态场景的重建，适应大规模数据集，并在包括罕见和未知物体的新环境中表现良好。", "conclusion": "Flux4D是一个强大的无监督框架，能够实现有效的大规模动态场景4D重建，它不需要外部标注或复杂的预训练模型，在多种环境下表现出色。"}}
{"id": "2512.03047", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03047", "abs": "https://arxiv.org/abs/2512.03047", "authors": ["Samih Fadli"], "title": "Entropy-Based Measurement of Value Drift and Alignment Work in Large Language Models", "comment": "6 pages. Companion paper to \"The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems\". Code and tools: https://github.com/AerisSpace/EthicalEntropyKit", "summary": "Large language model safety is usually assessed with static benchmarks, but key failures are dynamic: value drift under distribution shift, jailbreak attacks, and slow degradation of alignment in deployment. Building on a recent Second Law of Intelligence that treats ethical entropy as a state variable which tends to increase unless countered by alignment work, we make this framework operational for large language models. We define a five-way behavioral taxonomy, train a classifier to estimate ethical entropy S(t) from model transcripts, and measure entropy dynamics for base and instruction-tuned variants of four frontier models across stress tests. Base models show sustained entropy growth, while tuned variants suppress drift and reduce ethical entropy by roughly eighty percent. From these trajectories we estimate an effective alignment work rate gamma_eff and embed S(t) and gamma_eff in a monitoring pipeline that raises alerts when entropy drift exceeds a stability threshold, enabling run-time oversight of value drift.", "AI": {"tldr": "该研究通过评估伦理熵的动态变化来监测大型语言模型的安全性，并借助于智能的第二定律框架，提出了一种实时监督方法以应对模型对齐问题。", "motivation": "大型语言模型的安全性通常用静态基准进行评估，但关键的失败是动态的：分布移位下的价值观漂移，越狱攻击，以及在部署过程中对齐的缓慢退化。", "method": "通过提出智能的第二定律将框架应用于大型语言模型，并定义了一个五维的行为分类法，训练了一个分类器来估计从模型转录内容的伦理熵S(t)，并测量了四种前沿模型的基线和指令微调变体在压力测试下的伦理熵动态。", "result": "基线模型显示出持续的熵增长，而微调变体抑制了漂移并减少了约百分之八十的伦理熵。", "conclusion": "根据这些轨迹估计了有效的对齐工作率gamma_eff，并将其与S(t)嵌入监控管道中，当熵漂移超过稳定性阈值时，可以实现价值漂移的实时监督。"}}
{"id": "2512.03233", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03233", "abs": "https://arxiv.org/abs/2512.03233", "authors": ["Richard Füzesséry", "Kaziwa Saleh", "Sándor Szénási", "Zoltán Vámossy"], "title": "Object Counting with GPT-4o and GPT-5: A Comparative Study", "comment": "5 pages, 3 figures", "summary": "Zero-shot object counting attempts to estimate the number of object instances belonging to novel categories that the vision model performing the counting has never encountered during training. Existing methods typically require large amount of annotated data and often require visual exemplars to guide the counting process. However, large language models (LLMs) are powerful tools with remarkable reasoning and data understanding abilities, which suggest the possibility of utilizing them for counting tasks without any supervision. In this work we aim to leverage the visual capabilities of two multi-modal LLMs, GPT-4o and GPT-5, to perform object counting in a zero-shot manner using only textual prompts. We evaluate both models on the FSC-147 and CARPK datasets and provide a comparative analysis. Our findings show that the models achieve performance comparable to the state-of-the-art zero-shot approaches on FSC-147, in some cases, even surpass them.", "AI": {"tldr": "研究利用多模态大型语言模型（GPT-4o和GPT-5）的视觉能力，仅通过文本提示来执行无样本对象计数任务，并在两个数据集上取得与现有方法相当甚至更好的效果。", "motivation": "现有的无样本对象计数方法一般需要大量注释的数据，并且通常需要视觉样本来引导计数过程。然而，大型语言模型具有出色的推理和数据理解能力，这表明可以将它们用于无需监督的计数任务。", "method": "本研究旨在利用两种多模态大型语言模型（GPT-4o和GPT-5）的视觉能力，仅通过文本提示来执行无样本（零样本）对象计数任务。", "result": "研究在FSC-147和CARPK数据集上评估了这两个模型，并进行了比较分析。结果显示，这些模型在FSC-147数据集上的表现与现有的无样本方法相当，在某些情况下甚至超过了它们。", "conclusion": "研究证明了大型语言模型在无样本对象计数方面的潜力，表明它们可以在无需大量标注数据或视觉样本的情况下，仅通过文本提示实现有效计数。"}}
{"id": "2512.03079", "categories": ["cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03079", "abs": "https://arxiv.org/abs/2512.03079", "authors": ["Anudeex Shetty"], "title": "Watermarks for Embeddings-as-a-Service Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated exceptional capabilities in natural language understanding and generation. Based on these LLMs, businesses have started to provide Embeddings-as-a-Service (EaaS), offering feature extraction capabilities (in the form of text embeddings) that benefit downstream natural language processing tasks. However, prior research has demonstrated that EaaS is vulnerable to imitation attacks, where an attacker clones the service's model in a black-box manner without access to the model's internal workings. In response, watermarks have been added to the text embeddings to protect the intellectual property of EaaS providers by allowing them to check for model ownership. This thesis focuses on defending against imitation attacks by investigating EaaS watermarks. To achieve this goal, we unveil novel attacks and propose and validate new watermarking techniques.\n  Firstly, we show that existing EaaS watermarks can be removed through paraphrasing the input text when attackers clone the model during imitation attacks. Our study illustrates that paraphrasing can effectively bypass current state-of-the-art EaaS watermarks across various attack setups (including different paraphrasing techniques and models) and datasets in most instances. This demonstrates a new vulnerability in recent EaaS watermarking techniques.\n  Subsequently, as a countermeasure, we propose a novel watermarking technique, WET (Watermarking EaaS with Linear Transformation), which employs linear transformation of the embeddings. Watermark verification is conducted by applying a reverse transformation and comparing the similarity between recovered and original embeddings. We demonstrate its robustness against paraphrasing attacks with near-perfect verifiability. We conduct detailed ablation studies to assess the significance of each component and hyperparameter in WET.", "AI": {"tldr": "本论文探讨了如何通过水印技术防御针对基于大型语言模型的Embeddings-as-a-Service的模仿攻击。", "motivation": "针对现有EaaS服务存在模仿攻击的风险，以及现有水印技术可被通过文本改写的方式绕过的局限性，我们需要研究更为可靠的防御机制。", "method": "研究展示了现有关于EaaS水印方法的脆弱性，并提出了一个新的水印技术WET，利用线性变换技术增强水印的鲁棒性。", "result": "实验表明WET在各种改写攻击设置和数据集下，能够有效防御模仿攻击，保持接近完美的验证正确性。", "conclusion": "说明了通过适当的水印技术可以在很大程度上提升EaaS服务的安全性，防止模仿攻击。"}}
{"id": "2512.03237", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.03237", "abs": "https://arxiv.org/abs/2512.03237", "authors": ["Nafiseh Izadyar", "Teseo Schneider"], "title": "LLM-Guided Material Inference for 3D Point Clouds", "comment": null, "summary": "Most existing 3D shape datasets and models focus solely on geometry, overlooking the material properties that determine how objects appear. We introduce a two-stage large language model (LLM) based method for inferring material composition directly from 3D point clouds with coarse segmentations. Our key insight is to decouple reasoning about what an object is from what it is made of. In the first stage, an LLM predicts the object's semantic; in the second stage, it assigns plausible materials to each geometric segment, conditioned on the inferred semantics. Both stages operate in a zero-shot manner, without task-specific training. Because existing datasets lack reliable material annotations, we evaluate our method using an LLM-as-a-Judge implemented in DeepEval. Across 1,000 shapes from Fusion/ABS and ShapeNet, our method achieves high semantic and material plausibility. These results demonstrate that language models can serve as general-purpose priors for bridging geometric reasoning and material understanding in 3D data.", "AI": {"tldr": "该研究提出了一种基于大语言模型的两阶段方法，用于从3D点云数据中推断物体的材质组成。这种方法能在没有任何任务特定训练的情况下，有效地推测出物体的材质。评估结果显示该方法具有显著的语义和材质合理性。", "motivation": "现有大多数3D形状数据集和模型主要关注几何属性，而忽视了影响物体外观的材质属性。论文旨在解决这一问题，提供一种新的方法，以填补3D几何形状与材料理解之间的空白。", "method": "该论文提出了一种两阶段的大语言模型（LLM）方法，用于直接从带有粗略分割的3D点云数据中推断物体的材质组成。首先使用LLM来预测物体的语义，然后在第二阶段将可能的材质分配给每个几何部分，这个过程都基于推测出来的语义信息，并且无需针对特定任务进行训练。", "result": "尽管现有的数据集缺乏可靠的材质注释，但该方法在包含Fusion/ABS和ShapeNet中的1,000个形状的数据集上，通过DeepEval实现的LLM作为裁判进行了评估，结果显示，该方法在物体语义和材质的合理性预测上表现良好。", "conclusion": "研究结果表明，语言模型可以作为通用先验，来连接3D数据中的几何推理和材质理解。"}}
{"id": "2512.03082", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03082", "abs": "https://arxiv.org/abs/2512.03082", "authors": ["Nan Zhuang", "Wenshuo Wang", "Lekai Qian", "Yuxiao Wang", "Boyu Cao", "Qi Liu"], "title": "Alleviating Choice Supportive Bias in LLM with Reasoning Dependency Generation", "comment": null, "summary": "Recent studies have demonstrated that some Large Language Models exhibit choice-supportive bias (CSB) when performing evaluations, systematically favoring their chosen options and potentially compromising the objectivity of AI-assisted decision making. While existing debiasing approaches primarily target demographic and social biases, methods for addressing cognitive biases in LLMs remain largely unexplored. In this work, we present the first solution to address CSB through Reasoning Dependency Generation (RDG), a novel framework for generating unbiased reasoning data to mitigate choice-supportive bias through fine-tuning. RDG automatically constructs balanced reasoning QA pairs, explicitly (un)modeling the dependencies between choices, evidences, and justifications. Our approach is able to generate a large-scale dataset of QA pairs across domains, incorporating Contextual Dependency Data and Dependency Decouple Data. Experiments show that LLMs fine-tuned on RDG-generated data demonstrate a 81.5% improvement in memory-based experiments and 94.3% improvement in the evaluation-based experiment, while maintaining similar performance on standard BBQ benchmarks. This work pioneers an approach for addressing cognitive biases in LLMs and contributes to the development of more reliable AI-assisted decision support systems.", "AI": {"tldr": "提出了一种名为Reasoning Dependency Generation（RDG）的新方法，通过生成无偏的推理数据来减轻大语言模型的选择支持性偏差。", "motivation": "当前，大语言模型（LLMs）在评估任务中表现出选择支持性偏差（CSB），影响了AI辅助决策的客观性。现有的去偏方法主要针对人口和社会偏见，而对认知偏见的策略尚未得到深入研究。", "method": "本研究提出了一个名为Reasoning Dependency Generation（RDG）的新框架，用于生成无偏的推理数据，通过微调来减轻选择支持性偏差。RDG能够自动生成平衡推理问答对，明确建模或不建模选择、证据和理由之间的依赖关系。", "result": "实验表明，使用RDG生成数据进行微调的大语言模型在基于记忆和评估的实验中分别提高了81.5%和94.3%的表现，同时在标准BBQ基准测试中也保持了相似的性能。", "conclusion": "研究为解决大语言模型中的认知偏见提供了一种新方法，并有助于开发更可靠的AI辅助决策支持系统。"}}
{"id": "2512.03245", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03245", "abs": "https://arxiv.org/abs/2512.03245", "authors": ["Liying Lu", "Raphaël Achddou", "Sabine Süsstrunk"], "title": "2-Shots in the Dark: Low-Light Denoising with Minimal Data Acquisition", "comment": null, "summary": "Raw images taken in low-light conditions are very noisy due to low photon count and sensor noise. Learning-based denoisers have the potential to reconstruct high-quality images. For training, however, these denoisers require large paired datasets of clean and noisy images, which are difficult to collect. Noise synthesis is an alternative to large-scale data acquisition: given a clean image, we can synthesize a realistic noisy counterpart. In this work, we propose a general and practical noise synthesis method that requires only one single noisy image and one single dark frame per ISO setting. We represent signal-dependent noise with a Poisson distribution and introduce a Fourier-domain spectral sampling algorithm to accurately model signal-independent noise. The latter generates diverse noise realizations that maintain the spatial and statistical properties of real sensor noise. As opposed to competing approaches, our method neither relies on simplified parametric models nor on large sets of clean-noisy image pairs. Our synthesis method is not only accurate and practical, it also leads to state-of-the-art performances on multiple low-light denoising benchmarks.", "AI": {"tldr": "本文提出了一种只需单个噪声图像和暗帧即可生成真实噪声图像的方法，适用于多种ISO设置，能够在不依赖简化参数模型或大量干净与噪声图像对的前提下，准确且高效地建模传感器噪声，最终在多个低光去噪基准上取得了最先进的性能。", "motivation": "在低光条件下拍摄的原始图像是嘈杂的，因为光子数量少且传感器噪声大。基于学习的去噪器可以通过大数据集进行训练来重建高质量图像。然而，这些数据集难以采集。因此，提出噪声合成作为一种替代大规模数据获取的方法。", "method": "提出了一种通用且实用的噪声合成方法，该方法只需要每种ISO设置下一个单独的噪声图像和一个暗帧。通过泊松分布表示信号相关噪声，并引入频域光谱采样算法来准确建模信号无关噪声。这种方法产生的噪声实例保持了真实传感器噪声的空间和统计特性。", "result": "实验结果表明，该噪声合成方法不仅准确而且实用，并且能够在多个低光去噪基准上取得最先进的性能。", "conclusion": "该研究提出的方法证明了其在低光摄影去噪应用中的有效性和实用性，无需大量配对数据集即可合成真实的噪声图像。"}}
{"id": "2512.03195", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03195", "abs": "https://arxiv.org/abs/2512.03195", "authors": ["Stylianos Saroglou", "Konstantinos Diamantaras", "Francesco Preta", "Marina Delianidi", "Apostolos Benisis", "Christian Johannes Meyer"], "title": "Enhancing Job Matching: Occupation, Skill and Qualification Linking with the ESCO and EQF taxonomies", "comment": "14 pages, 1 figure, Preprint", "summary": "This study investigates the potential of language models to improve the classification of labor market information by linking job vacancy texts to two major European frameworks: the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy and the European Qualifications Framework (EQF). We examine and compare two prominent methodologies from the literature: Sentence Linking and Entity Linking. In support of ongoing research, we release an open-source tool, incorporating these two methodologies, designed to facilitate further work on labor classification and employment discourse. To move beyond surface-level skill extraction, we introduce two annotated datasets specifically aimed at evaluating how occupations and qualifications are represented within job vacancy texts. Additionally, we examine different ways to utilize generative large language models for this task. Our findings contribute to advancing the state of the art in job entity extraction and offer computational infrastructure for examining work, skills, and labor market narratives in a digitally mediated economy. Our code is made publicly available: https://github.com/tabiya-tech/tabiya-livelihoods-classifier", "AI": {"tldr": "本研究探讨了语言模型在分类劳动市场信息中的潜力，尤其关注将职位空缺文本链接到ESCO和EQF两个欧洲框架。研究比较了句子链接和实体链接两种方法，并发布了开源工具支持进一步研究。同时，研究发布了两个新的标注数据集，用于评估职业和资格在职位空缺文本中的表示方式，还探讨了生成式语言大模型的应用。研究进展了工作实体提取的技术，并提供了计算基础设施来分析数字经济中的工作、技能和劳动力市场叙事。", "motivation": "研究动机是评估语言模型在劳动市场信息分类中的应用潜力，利用相关方法将职位描述与欧洲技能、资格框架进行有效链接。", "method": "使用句子链接和实体链接两种文献中广泛采用的方法进行研究，并开发了一个结合这两种方法的开源工具。同时，还发布了两个特定的标注数据集以评估职业和资格在职位空缺文本中的表示方式。", "result": "研究结果改进了工作实体提取的技术，并指向了如何利用生成式大语言模型在这个任务中的不同使用方式，提供了相关的计算基础设施。", "conclusion": "研究成果推进了在分类劳动力市场信息方面的技术水平，并提供了开源工具和计算基础架构，以支持在未来数字经济中对工作和技能的更深入分析。"}}
{"id": "2512.03247", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03247", "abs": "https://arxiv.org/abs/2512.03247", "authors": ["Haitian Zheng", "Yuan Yao", "Yongsheng Yu", "Yuqian Zhou", "Jiebo Luo", "Zhe Lin"], "title": "PixPerfect: Seamless Latent Diffusion Local Editing with Discriminative Pixel-Space Refinement", "comment": "Published in the Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)", "summary": "Latent Diffusion Models (LDMs) have markedly advanced the quality of image inpainting and local editing. However, the inherent latent compression often introduces pixel-level inconsistencies, such as chromatic shifts, texture mismatches, and visible seams along editing boundaries. Existing remedies, including background-conditioned latent decoding and pixel-space harmonization, usually fail to fully eliminate these artifacts in practice and do not generalize well across different latent representations or tasks. We introduce PixPerfect, a pixel-level refinement framework that delivers seamless, high-fidelity local edits across diverse LDM architectures and tasks. PixPerfect leverages (i) a differentiable discriminative pixel space that amplifies and suppresses subtle color and texture discrepancies, (ii) a comprehensive artifact simulation pipeline that exposes the refiner to realistic local editing artifacts during training, and (iii) a direct pixel-space refinement scheme that ensures broad applicability across diverse latent representations and tasks. Extensive experiments on inpainting, object removal, and insertion benchmarks demonstrate that PixPerfect substantially enhances perceptual fidelity and downstream editing performance, establishing a new standard for robust and high-fidelity localized image editing.", "AI": {"tldr": "PixPerfect is introduced to refine and enhance the quality of local image edits by addressing pixel-level inconsistencies in Latent Diffusion Models (LDMs).", "motivation": "To address pixel-level inconsistencies, such as chromatic shifts, texture mismatches, and visible seams, resulting from latent compression in Local Denoising Models (LDMs) and to improve the perceptual fidelity and downstream editing performance in image inpainting and local editing.", "method": "A pixel-level refinement framework called PixPerfect, which uses a differentiable discriminative pixel space to amplify and suppress color and texture discrepancies, a comprehensive artifact simulation pipeline for realistic local editing artifact training, and a direct pixel-space refinement scheme to ensure broad applicability.", "result": "Extensive experiments show that PixPerfect significantly enhances perceptual fidelity and editing performance across various benchmarks.", "conclusion": "PixPerfect establishes a new standard for robust and high-fidelity localized image editing, improving the quality of local edits across different LDM architectures and tasks."}}
{"id": "2512.03197", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03197", "abs": "https://arxiv.org/abs/2512.03197", "authors": ["Faezeh Faez", "Marzieh S. Tahaei", "Yaochen Hu", "Ali Pourranjbar", "Mahdi Biparva", "Mark Coates", "Yingxue Zhang"], "title": "InvertiTune: High-Quality Data Synthesis for Cost-Effective Single-Shot Text-to-Knowledge Graph Generation", "comment": null, "summary": "Large Language Models (LLMs) have revolutionized the ability to understand and generate text, enabling significant progress in automatic knowledge graph construction from text (Text2KG). Many Text2KG methods, however, rely on iterative LLM prompting, making them computationally expensive and prone to overlooking complex relations distributed throughout the text. To address these limitations, we propose InvertiTune, a framework that combines a controlled data generation pipeline with supervised fine-tuning (SFT). Within this framework, the data-generation pipeline systematically extracts subgraphs from large knowledge bases, applies noise filtering, and leverages LLMs to generate corresponding natural text descriptions, a task more aligned with LLM capabilities than direct KG generation from text. This pipeline enables generating datasets composed of longer texts paired with larger KGs that better reflect real-world scenarios compared to existing benchmarks, thus supporting effective SFT of lightweight models for single-shot KG construction. Experimental results on CE12k, a dataset generated using the introduced pipeline, show that InvertiTune outperforms larger non-fine-tuned LLMs as well as state-of-the-art Text2KG approaches, while also demonstrating stronger cross-dataset generalization on CrossEval-1200, a test set created from three established benchmark datasets and CE12k. These findings highlight the importance of realistic, high-quality training data for advancing efficient and high-performing Text2KG systems.", "AI": {"tldr": "提出了InvertiTune框架，通过结合控制数据生成管道与监督微调来提高Text2KG的效率和性能，实验验证了其有效性。", "motivation": "许多从文本构建知识图谱（Text2KG）的方法依赖于迭代的大语言模型（LLMs）提示，这导致计算成本高昂并且容易忽略分布在文本中的复杂关系。为了解决这些问题。", "method": "InvertiTune框架，结合控制数据生成管道与监督微调（SFT）。数据生成管道系统地从大型知识库中提取子图，应用噪声过滤，并利用大语言模型（LLMs）生成相应的自然语言描述，从而生成更适合现实世界的长文本与大型知识图谱配对的数据集，以支持轻量模型的单次知识图谱构建的监督微调。", "result": "实验结果表明，在使用引入的管道生成的CE12k数据集上，InvertiTune的表现优于大型未微调的LLMs以及最先进的Text2KG方法，并且在CrossEval-1200测试集上也展示了更强的跨数据集泛化能力。", "conclusion": "这些发现强调了高质量且真实的训练数据对于推进高效和高性能的Text2KG系统的重要性。"}}
{"id": "2512.03257", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03257", "abs": "https://arxiv.org/abs/2512.03257", "authors": ["Mark Moussa", "Andre Williams", "Seth Roffe", "Douglas Morton"], "title": "PyroFocus: A Deep Learning Approach to Real-Time Wildfire Detection in Multispectral Remote Sensing Imagery", "comment": null, "summary": "Rapid and accurate wildfire detection is crucial for emergency response and environmental management. In airborne and spaceborne missions, real-time algorithms must distinguish between no fire, active fire, and post-fire conditions, and estimate fire intensity. Multispectral and hyperspectral thermal imagers provide rich spectral information, but high data dimensionality and limited onboard resources make real-time processing challenging. As wildfires increase in frequency and severity, the need for low-latency and computationally efficient onboard detection methods is critical.\n  We present a systematic evaluation of multiple deep learning architectures, including custom Convolutional Neural Networks (CNNs) and Transformer-based models, for multi-class fire classification. We also introduce PyroFocus, a two-stage pipeline that performs fire classification followed by fire radiative power (FRP) regression or segmentation to reduce inference time and computational cost for onboard deployment. Using data from NASA's MODIS/ASTER Airborne Simulator (MASTER), which is similar to a next-generation fire detection sensor, we compare accuracy, inference latency, and resource efficiency.\n  Experimental results show that the proposed two-stage pipeline achieves strong trade-offs between speed and accuracy, demonstrating significant potential for real-time edge deployment in future wildfire monitoring missions.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2512.03214", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.03214", "abs": "https://arxiv.org/abs/2512.03214", "authors": ["Paulina Garcia-Corral"], "title": "Identifying attributions of causality in political text", "comment": null, "summary": "Explanations are a fundamental element of how people make sense of the political world. Citizens routinely ask and answer questions about why events happen, who is responsible, and what could or should be done differently. Yet despite their importance, explanations remain an underdeveloped object of systematic analysis in political science, and existing approaches are fragmented and often issue-specific. I introduce a framework for detecting and parsing explanations in political text. To do this, I train a lightweight causal language model that returns a structured data set of causal claims in the form of cause-effect pairs for downstream analysis. I demonstrate how causal explanations can be studied at scale, and show the method's modest annotation requirements, generalizability, and accuracy relative to human coding.", "AI": {"tldr": "研究引入了一个框架和一种轻量级的因果语言模型来检测和解析政治文本中的解释，展示了其在大规模研究中的有效性。", "motivation": "尽管解释对于人们理解政治世界非常重要，但政治科学中对解释的系统分析却相对较少且支离破碎。该研究旨在填补这一空白，提供一个统一的分析框架。", "method": "引入框架以在政治文本中检测和解析解释，并训练了一个轻量级的因果语言模型，返回结构化的因果声明数据集（原因-效果配对）用于进一步分析。", "result": "展示了在大规模研究中如何分析因果解释，并证明了该方法的小标注需求、可推广性和相对于人工编码的准确性。", "conclusion": "提出了一种新的方法来检测和解析政治文本中的因果解释，这种方法具有较高的准确性和可扩展性。"}}
{"id": "2512.03284", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03284", "abs": "https://arxiv.org/abs/2512.03284", "authors": ["Hongpei Zheng", "Shijie Li", "Yanran Li", "Hujun Yin"], "title": "SpatialReasoner: Active Perception for Large-Scale 3D Scene Understanding", "comment": null, "summary": "Spatial reasoning in large-scale 3D environments remains challenging for current vision-language models, which are typically constrained to room-scale scenarios. We introduce H$^2$U3D (Holistic House Understanding in 3D), a 3D visual question answering dataset designed for house-scale scene understanding. H$^2$U3D features multi-floor environments spanning up to three floors and 10-20 rooms, covering more than 300 m$^2$. Through an automated annotation pipeline, it constructs hierarchical coarse-to-fine visual representations and generates diverse question-answer pairs with chain-of-thought annotations. We further propose SpatialReasoner, an active perception framework that autonomously invokes spatial tools to explore 3D scenes based on textual queries. SpatialReasoner is trained through a two-stage strategy: a supervised cold start followed by reinforcement learning with an adaptive exploration reward that promotes efficient exploration while discouraging redundant operations. Extensive experiments demonstrate that SpatialReasoner achieves state-of-the-art performance on H$^2$U3D, outperforming strong baselines including GPT-4o and Gemini-2.5-Pro. Notably, our method attains superior results while using only 3-4 images in total on average, compared to baselines requiring 16+ images, highlighting the effectiveness of our coarse-to-fine active exploration paradigm.", "AI": {"tldr": "文章提出了针对大规模3D环境中房屋尺度场景理解的H$^2$U3D数据集和SpatialReasoner主动感知框架，实验结果表明SpatialReasoner比其他方法效果更好，且所需图像数目较少。", "motivation": "当前的视觉语言模型在大规模3D环境中的空间推理仍然具有挑战性，特别是在房屋规模的场景理解上。为此，提出了H$^2$U3D数据集和其他相应的方法。", "method": "介绍了一种名为SpatialReasoner的主动感知框架，该框架能够基于文本查询自主调用空间工具来探索3D场景。SpatialReasoner通过两阶段策略进行训练：先进行监督学习的冷启动，再通过带有自适应探索奖励的强化学习，以促进有效的探索同时避免冗余操作。", "result": "实验表明，SpatialReasoner在H$^2$U3D数据集上达到了最先进的性能，超过了包括GPT-4o和Gemini-2.5-Pro在内的强大的基线模型。", "conclusion": "该方法通过平均仅使用3-4张图像实现了卓越的结果，展示出了粗到细主动探索范式的有效性。"}}
{"id": "2512.03310", "categories": ["cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03310", "abs": "https://arxiv.org/abs/2512.03310", "authors": ["Kunj Joshi", "David A. Smith"], "title": "Randomized Masked Finetuning: An Efficient Way to Mitigate Memorization of PIIs in LLMs", "comment": "To be submitted for ICML 2026", "summary": "The current literature on memorization in Natural Language Models, especially Large Language Models (LLMs), poses severe security and privacy risks, as models tend to memorize personally identifying information (PIIs) from training data. We introduce Randomized Masked Fine-Tuning (RMFT), a novel privacy-preserving fine-tuning technique that reduces PII memorization while minimizing performance impact. Using the Enron Email Dataset, we demonstrate that RMFT achieves an 80.81% reduction in Total Extraction Rate and 80.17% reduction in Seen Extraction Rate compared to baseline fine-tuning, outperforming deduplication methods while maintaining only a 5.73% increase in perplexity. We present MaxTER, a Pareto-optimal evaluation framework for assessing privacy-utility tradeoffs, and show the performance of RMFT vs Deduplication by Area Under The Response Curve (AURC) metric.", "AI": {"tldr": "本文提出了一种新的隐私保护技术——随机掩码微调（RMFT），该技术能够显著减少大型语言模型记忆个人身份信息的数量，同时对模型性能的影响较小。", "motivation": "现有的关于自然语言模型（特别是大型语言模型）记忆的研究提出了严重的安全和隐私风险，模型倾向于从训练数据中记忆个人身份信息（PII）。", "method": "我们引入了随机掩码微调（RMFT），这是一种新的隐私保护微调技术，旨在减少大型语言模型中的个人身份信息（PII）记忆，同时尽量减少对性能的影响。", "result": "实验表明，与基线微调方法相比，RMFT在Enron电子邮件数据集上实现了80.81%的提取率减少和80.17%的见过的提取率减少，同时，与重删方法相比，RMFT在保持模型性能上仅有5.73%的困惑度增加。", "conclusion": "实验结果表明，RMFT能够大幅降低PII提取率，并且通过AURC指标展示了其在隐私和性能之间的优势。"}}
{"id": "2512.03317", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03317", "abs": "https://arxiv.org/abs/2512.03317", "authors": ["Thomas Monninger", "Zihan Zhang", "Steffen Staab", "Sihao Ding"], "title": "NavMapFusion: Diffusion-based Fusion of Navigation Maps for Online Vectorized HD Map Construction", "comment": "Accepted to 2026 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2026)", "summary": "Accurate environmental representations are essential for autonomous driving, providing the foundation for safe and efficient navigation. Traditionally, high-definition (HD) maps are providing this representation of the static road infrastructure to the autonomous system a priori. However, because the real world is constantly changing, such maps must be constructed online from on-board sensor data. Navigation-grade standard-definition (SD) maps are widely available, but their resolution is insufficient for direct deployment. Instead, they can be used as coarse prior to guide the online map construction process. We propose NavMapFusion, a diffusion-based framework that performs iterative denoising conditioned on high-fidelity sensor data and on low-fidelity navigation maps. This paper strives to answer: (1) How can coarse, potentially outdated navigation maps guide online map construction? (2) What advantages do diffusion models offer for map fusion? We demonstrate that diffusion-based map construction provides a robust framework for map fusion. Our key insight is that discrepancies between the prior map and online perception naturally correspond to noise within the diffusion process; consistent regions reinforce the map construction, whereas outdated segments are suppressed. On the nuScenes benchmark, NavMapFusion conditioned on coarse road lines from OpenStreetMap data reaches a 21.4% relative improvement on 100 m, and even stronger improvements on larger perception ranges, while maintaining real-time capabilities. By fusing low-fidelity priors with high-fidelity sensor data, the proposed method generates accurate and up-to-date environment representations, guiding towards safer and more reliable autonomous driving. The code is available at https://github.com/tmonnin/navmapfusion", "AI": {"tldr": "使用基于扩散模型的NavMapFusion框架，在低精度导航地图和高精度传感器数据的基础上进行在线地图构建，实现更准确的自动驾驶环境表示，代码公开。", "motivation": "旨在解决静态道路基础设施的高精度地图建设和利用低精度导航地图指导在线地图构建的问题。", "method": "提出NavMapFusion框架，基于扩散模型，通过迭代去噪，融合高精度传感器数据和低精度导航地图进行在线地图构建。", "result": "在nuScenes基准测试中，NavMapFusion基于OpenStreetMap数据，在100米范围内实现了21.4%的相对改进，更长的距离感知有更好的改进效果，同时实时性得到保证。", "conclusion": "研究表明，通过融合低精度先验和高精度传感器数据，NavMapFusion方法能够生成更准确、更新的环境表示，促进更安全可靠的自动驾驶。"}}
{"id": "2512.03334", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03334", "abs": "https://arxiv.org/abs/2512.03334", "authors": ["Nemika Tyagi", "Nelvin Licona Guevara", "Olga Kellert"], "title": "Modeling Topics and Sociolinguistic Variation in Code-Switched Discourse: Insights from Spanish-English and Spanish-Guaraní", "comment": "10 pages, 4 figures", "summary": "This study presents an LLM-assisted annotation pipeline for the sociolinguistic and topical analysis of bilingual discourse in two typologically distinct contexts: Spanish-English and Spanish-Guaraní. Using large language models, we automatically labeled topic, genre, and discourse-pragmatic functions across a total of 3,691 code-switched sentences, integrated demographic metadata from the Miami Bilingual Corpus, and enriched the Spanish-Guaraní dataset with new topic annotations. The resulting distributions reveal systematic links between gender, language dominance, and discourse function in the Miami data, and a clear diglossic division between formal Guaraní and informal Spanish in Paraguayan texts. These findings replicate and extend earlier interactional and sociolinguistic observations with corpus-scale quantitative evidence. The study demonstrates that large language models can reliably recover interpretable sociolinguistic patterns traditionally accessible only through manual annotation, advancing computational methods for cross-linguistic and low-resource bilingual research.", "AI": {"tldr": "研究表明使用大型语言模型可以自动化标注双语对话的主题、体裁和语用功能，并揭示了性别、语言优势和语用功能之间的系统联系，证明了大型语言模型在双语研究中的可靠性。", "motivation": "研究动机在于探索大型语言模型在自动标注双语对话中的应用，并通过大规模定量证据验证其有效性，继而推进跨语言和低资源双语研究的计算方法。", "method": "利用大型语言模型对来自Miami双语语料库和Paraguayan文本的3,691个混合语言句子进行自动标注，包括主题、体裁和语用功能，同时加入人口统计数据的元数据。", "result": "研究发现，在Miami数据中，性别、语言优势和语用功能之间存在系统联系；而在Paraguayan文本中，存在明显的正式瓜拉尼语和非正式西班牙语的双语现象。", "conclusion": "研究证明，大型语言模型可以可靠地恢复先前通过手动注释可以得到的可解释的社会语言学模式，推进了跨语言和低资源双语研究的计算方法。"}}
{"id": "2512.03335", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03335", "abs": "https://arxiv.org/abs/2512.03335", "authors": ["Faizan Farooq Khan", "K J Joseph", "Koustava Goswami", "Mohamed Elhoseiny", "Balaji Vasan Srinivasan"], "title": "Step-by-step Layered Design Generation", "comment": null, "summary": "Design generation, in its essence, is a step-by-step process where designers progressively refine and enhance their work through careful modifications. Despite this fundamental characteristic, existing approaches mainly treat design synthesis as a single-step generation problem, significantly underestimating the inherent complexity of the creative process. To bridge this gap, we propose a novel problem setting called Step-by-Step Layered Design Generation, which tasks a machine learning model with generating a design that adheres to a sequence of instructions from a designer. Leveraging recent advancements in multi-modal LLMs, we propose SLEDGE: Step-by-step LayEred Design GEnerator to model each update to a design as an atomic, layered change over its previous state, while being grounded in the instruction. To complement our new problem setting, we introduce a new evaluation suite, including a dataset and a benchmark. Our exhaustive experimental analysis and comparison with state-of-the-art approaches tailored to our new setup demonstrate the efficacy of our approach. We hope our work will attract attention to this pragmatic and under-explored research area.", "AI": {"tldr": "本文提出了SLEDGE模型，针对逐层设计生成的问题设置，展示了其在生成复杂设计过程中的有效性和优越性。", "motivation": "现有的设计合成方法大多将其视为单一生成问题，忽视了设计创作的复杂性。为解决这一问题，研究提出了Step-by-Step Layered Design Generation（逐层设计生成）的新问题设置。", "method": "提出了名为SLEDGE的Step-by-step LayEred Design GEnerator，利用多模态大模型来生成逐层变化的设计，每一步更新都是基于前一状态的原子化改变，同时遵循设计师的指令。", "result": "新的评估套件包括数据集和基准测试，实验结果表明SLEDGE在新的设置下相较于最新的方法表现出色。", "conclusion": "研究工作希望能够吸引人们对这一实用但尚未充分探索的研究领域予以关注。"}}
{"id": "2512.03340", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03340", "abs": "https://arxiv.org/abs/2512.03340", "authors": ["Rohan Charudatt Salvi", "Chirag Chawla", "Dhruv Jain", "Swapnil Panigrahi", "Md Shad Akhtar", "Shweta Yadav"], "title": "PERCS: Persona-Guided Controllable Biomedical Summarization Dataset", "comment": "9 pages, 4 figures, 6 tables", "summary": "Automatic medical text simplification plays a key role in improving health literacy by making complex biomedical research accessible to diverse readers. However, most existing resources assume a single generic audience, overlooking the wide variation in medical literacy and information needs across user groups. To address this limitation, we introduce PERCS (Persona-guided Controllable Summarization), a dataset of biomedical abstracts paired with summaries tailored to four personas: Laypersons, Premedical Students, Non-medical Researchers, and Medical Experts. These personas represent different levels of medical literacy and information needs, emphasizing the need for targeted, audience-specific summarization. Each summary in PERCS was reviewed by physicians for factual accuracy and persona alignment using a detailed error taxonomy. Technical validation shows clear differences in readability, vocabulary, and content depth across personas. Along with describing the dataset, we benchmark four large language models on PERCS using automatic evaluation metrics that assess comprehensiveness, readability, and faithfulness, establishing baseline results for future research. The dataset, annotation guidelines, and evaluation materials are publicly available to support research on persona-specific communication and controllable biomedical summarization.", "AI": {"tldr": "PERCS数据集为不同医学素养水平和信息需求的受众定制了生物医学摘要，提升了健康素养。该数据集及其评估材料公开，支持未来的研究工作。", "motivation": "现有的资源大多假设只有一种通用的受众类型，忽略了在不同用户组中的医学素养和信息需求的广泛差异。为了克服这个限制，开发了PERCS数据集。", "method": "提出了PERCS数据集，该数据集包含生物医学摘要及其为四种不同受众类型定制的摘要。这四种受众类型分别为：非专业人士、预科医学生、非医学研究人员和医学专家。每种摘要都经过医生审核，确保其在事实准确性和人群匹配度方面的正确性。", "result": "展示了技术验证结果，比较了不同受众类型的可读性、词汇和内容深度的差异。对四个大型语言模型在PERCS数据集上的基准测试结果进行了评估，使用了自动评估指标来衡量全面性、可读性和忠实度。", "conclusion": "PERCS数据集、标注指南和评估材料都是公开的，旨在支持基于受众类型进行控制性生物医学摘要的研究。这些资源为未来的研究建立了基线结果。"}}
{"id": "2512.03339", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03339", "abs": "https://arxiv.org/abs/2512.03339", "authors": ["Yeganeh Ghamary", "Victoria Wu", "Hooman Vaseli", "Christina Luong", "Teresa Tsang", "Siavash Bigdeli", "Purang Abolmaesumi"], "title": "ProtoEFNet: Dynamic Prototype Learning for Inherently Interpretable Ejection Fraction Estimation in Echocardiography", "comment": "11 pages, Accepted in IMIMIC Workshop at MICCAI 2025", "summary": "Ejection fraction (EF) is a crucial metric for assessing cardiac function and diagnosing conditions such as heart failure. Traditionally, EF estimation requires manual tracing and domain expertise, making the process time-consuming and subject to interobserver variability. Most current deep learning methods for EF prediction are black-box models with limited transparency, which reduces clinical trust. Some post-hoc explainability methods have been proposed to interpret the decision-making process after the prediction is made. However, these explanations do not guide the model's internal reasoning and therefore offer limited reliability in clinical applications. To address this, we introduce ProtoEFNet, a novel video-based prototype learning model for continuous EF regression. The model learns dynamic spatiotemporal prototypes that capture clinically meaningful cardiac motion patterns. Additionally, the proposed Prototype Angular Separation (PAS) loss enforces discriminative representations across the continuous EF spectrum. Our experiments on the EchonetDynamic dataset show that ProtoEFNet can achieve accuracy on par with its non-interpretable counterpart while providing clinically relevant insight. The ablation study shows that the proposed loss boosts performance with a 2% increase in F1 score from 77.67$\\pm$2.68 to 79.64$\\pm$2.10. Our source code is available at: https://github.com/DeepRCL/ProtoEF", "AI": {"tldr": "ProtoEFNet, a prototype learning model for continuous EF regression, uses PAS loss to improve discriminative representations, achieving comparable accuracy to non-interpretable models with added clinical interpretability.", "motivation": "The motivation is to overcome the limitations of manual EF estimation, time consumption, interobserver variability, and the lack of transparency in most current deep learning methods used for EF prediction.", "method": "ProtoEFNet is introduced as a video-based prototype learning model for continuous EF regression, capturing dynamic spatiotemporal prototypes of cardiac motion patterns. The method uses a novel loss function, Prototype Angular Separation (PAS), to improve discriminative representations across the continuous EF spectrum.", "result": "Experiments on the EchonetDynamic dataset demonstrated that ProtoEFNet achieves comparable accuracy to non-interpretable models while offering clinical relevance. An ablation study indicated a 2% increase in F1 score due to the use of the PAS loss function.", "conclusion": "ProtoEFNet offers a transparent method for continuous EF regression, which can provide interpretable insights beneficial for clinical diagnosis while maintaining high accuracy."}}
{"id": "2512.03343", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03343", "abs": "https://arxiv.org/abs/2512.03343", "authors": ["Darshan Fofadiya"], "title": "Idea-Gated Transformers: Enforcing Semantic Coherence via Differentiable Vocabulary Pruning", "comment": "Code available at https://github.com/DarshanFofadiya/idea-gated-transformers/tree/main", "summary": "Autoregressive Language Models (LLMs) trained on Next-Token Prediction (NTP) often suffer from ``Topic Drift'' where the generation wanders away from the initial prompt due to a reliance on local associations rather than global planning \\citep{holtzman2019curious}. While scaling model size mitigates this \\citep{brown2020language}, the fundamental myopia of the NTP objective remains. In this work, we introduce the Idea-Gated Transformer, a novel architecture that separates semantic planning from syntactic generation. We introduce an auxiliary ``Idea Head'' trained to predict the bag-of-words distribution for a future context window, creating a latent ``Concept Vector'' that actively gates the main vocabulary during generation. We propose a differentiable gating mechanism that suppresses semantically irrelevant tokens, effectively pruning the search space in real-time. Experiments on WikiText-103 demonstrate that while the Idea-Gated model achieves comparable validation perplexity to a standard GPT-2 baseline, it exhibits significantly superior Domain Retention. Qualitative and quantitative analysis reveals that the gating mechanism successfully locks generation into specific semantic clusters (e.g., Finance, Science) and resists associative drift, offering a parameter-efficient path toward more controllable language modeling.", "AI": {"tldr": "本文介绍了Idea-Gated Transformer，通过引入一个辅助的思想头和不同的门控机制来解决语言模型“主题漂移”的问题。实验展现出相比于标准模型，该模型在保持话题上具有显著优势。", "motivation": "训练于下一个词预测（NTP）的自回归语言模型（LLMs）经常出现“主题漂移”的问题，即生成的内容会逐渐偏离原始提示，因为模型过于依赖局部关联而忽略了全局规划。", "method": "本文提出了名为Idea-Gated Transformer的新架构，该架构通过分离语义规划和句法生成来解决问题。提出了一种辅助的“Idea Head”，用于预测未来上下文窗口的词袋分布，从而生成一个潜在的“Concept Vector”，该向量会在生成过程中主动控制主要词汇。通过一种可微分的门控机制来抑制语义无关的词汇，实现实时搜索空间的剪枝。", "result": "实验结果显示，尽管Idea-Gated模型在WikiText-103数据集上的验证困惑度与标准的GPT-2基线模型相当，但它表现出显著更优的主题保持能力。", "conclusion": "定性和定量分析表明，该门控机制可以成功地将生成锁定在特定的语义集群内，防止联想驱动的漂移，并为实现更可控的语言模型提供了一条参数高效的路径。"}}
{"id": "2512.03345", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03345", "abs": "https://arxiv.org/abs/2512.03345", "authors": ["Seunghoi Kim", "Henry F. J. Tregidgo", "Chen Jin", "Matteo Figini", "Daniel C. Alexander"], "title": "HalluGen: Synthesizing Realistic and Controllable Hallucinations for Evaluating Image Restoration", "comment": null, "summary": "Generative models are prone to hallucinations: plausible but incorrect structures absent in the ground truth. This issue is problematic in image restoration for safety-critical domains such as medical imaging, industrial inspection, and remote sensing, where such errors undermine reliability and trust. For example, in low-field MRI, widely used in resource-limited settings, restoration models are essential for enhancing low-quality scans, yet hallucinations can lead to serious diagnostic errors. Progress has been hindered by a circular dependency: evaluating hallucinations requires labeled data, yet such labels are costly and subjective. We introduce HalluGen, a diffusion-based framework that synthesizes realistic hallucinations with controllable type, location, and severity, producing perceptually realistic but semantically incorrect outputs (segmentation IoU drops from 0.86 to 0.36). Using HalluGen, we construct the first large-scale hallucination dataset comprising 4,350 annotated images derived from 1,450 brain MR images for low-field enhancement, enabling systematic evaluation of hallucination detection and mitigation. We demonstrate its utility in two applications: (1) benchmarking image quality metrics and developing Semantic Hallucination Assessment via Feature Evaluation (SHAFE), a feature-based metric with soft-attention pooling that improves hallucination sensitivity over traditional metrics; and (2) training reference-free hallucination detectors that generalize to real restoration failures. Together, HalluGen and its open dataset establish the first scalable foundation for evaluating hallucinations in safety-critical image restoration.", "AI": {"tldr": "研究了生成模型中的幻觉问题，并提出了一种名为HalluGen的框架，该框架能够生成具有可控制属性的幻觉，并创建了一个大规模的幻觉数据集，用于系统地评估幻觉的检测和缓解。HanluGen在两个应用中展示了其有效性，一个是开发改进的幻觉敏感度度量，另一个是训练无参照的幻觉检测器。", "motivation": "解决生成模型在图像复原时出现的幻觉问题，尤其在医疗成像、工业检测和遥感等安全攸关领域。因为这些领域的错误会破坏可靠性和信任。例如，在资源有限的地区广泛使用的低场MRI中，复原模型对于提高低质量扫描至关重要，但幻觉会导致严重的诊断错误。", "method": "介绍了名为HalluGen的基于扩散的框架，该框架能够合成具有可控制类型、位置和严重程度的逼真幻觉，生成了4350张带注释的图像，这些图像源于1450张用于低场增强的脑MR图像，从而建立了第一个大规模的幻觉数据集，用于系统地评估幻觉检测和缓解。", "result": "通过使用HalluGen，创建了第一个大规模的幻觉数据集，这使得可以系统地评估幻觉的检测和缓解。并在两个应用中展示了数据集的有效性：(1)基准图像质量指标，并开发了Semantic Hallucination Assessment via Feature Evaluation (SHAFE)，这是一种基于特征度量的指标，使用软注意力池化，提高了幻觉敏感度。(2)训练无参照的幻觉检测器以泛化到真实的复原失败。", "conclusion": "HalluGen及其开放数据集建立了第一个可扩展的基础，用于评估安全关键图像复原中的幻觉。"}}
{"id": "2512.03360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03360", "abs": "https://arxiv.org/abs/2512.03360", "authors": ["Qingchuan Li", "Mingyue Cheng", "Zirui Liu", "Daoyu Wang", "Yuting Zeng", "Tongxuan Liu"], "title": "From Hypothesis to Premises: LLM-based Backward Logical Reasoning with Selective Symbolic Translation", "comment": "Accepted by AAAI2026", "summary": "Logical reasoning is a core challenge in natural language understanding and a fundamental capability of artificial intelligence, underpinning scientific discovery, mathematical theorem proving, and complex decision-making. Despite the remarkable progress of large language models (LLMs), most current approaches still rely on forward reasoning paradigms, generating step-by-step rationales from premises to conclusions. However, such methods often suffer from redundant inference paths, hallucinated steps, and semantic drift, resulting in inefficient and unreliable reasoning. In this paper, we propose a novel framework, Hypothesis-driven Backward Logical Reasoning (HBLR). The core idea is to integrate confidence-aware symbolic translation with hypothesis-driven backward reasoning. In the translation phase, only high-confidence spans are converted into logical form, such as First-Order Logic (FOL), while uncertain content remains in natural language. A translation reflection module further ensures semantic fidelity by evaluating symbolic outputs and reverting lossy ones back to text when necessary. In the reasoning phase, HBLR simulates human deductive thinking by assuming the conclusion is true and recursively verifying its premises. A reasoning reflection module further identifies and corrects flawed inference steps, enhancing logical coherence. Extensive experiments on five reasoning benchmarks demonstrate that HBLR consistently outperforms strong baselines in both accuracy and efficiency.", "AI": {"tldr": "The paper introduces HBLR, a new framework for logical reasoning in natural language understanding that outperforms existing methods by using confidence-aware symbolic translation and backward reasoning from assumed conclusions, leading to enhanced accuracy and efficiency.", "motivation": "The motivation is to address the inefficiencies and unreliability in current logical reasoning approaches in natural language understanding, particularly those using forward reasoning paradigms that generate step-by-step rationales, often resulting in redundant inference paths, hallucinated steps, and semantic drift.", "method": "The paper proposes a novel framework, Hypothesis-driven Backward Logical Reasoning (HBLR), which integrates confidence-aware symbolic translation with hypothesis-driven backward reasoning. It features a translation phase that converts high-confidence spans into logical form, while uncertain content remains in natural language, and a reasoning phase that simulates human deductive thinking by assuming the conclusion is true and recursively verifying its premises.", "result": "Extensive experiments on five reasoning benchmarks show that HBLR outperforms strong baselines in both accuracy and efficiency.", "conclusion": "The conclusion is that the HBLR framework significantly improves the efficiency and reliability of logical reasoning compared to current forward reasoning approaches by utilizing a confidence-aware translation method and a hypothesis-driven backward reasoning strategy."}}
{"id": "2512.03346", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03346", "abs": "https://arxiv.org/abs/2512.03346", "authors": ["Lynn Kandakji", "William Woof", "Nikolas Pontikos"], "title": "Hierarchical Attention for Sparse Volumetric Anomaly Detection in Subclinical Keratoconus", "comment": "16 pages, 7 figures, 6 tables", "summary": "The detection of weak, spatially distributed anomalies in volumetric medical imaging remains a major challenge. The subtle, non-adjacent nature of early disease signals is often lost due to suboptimal architectural inductive biases: 2D/3D CNNs impose strong locality, while ViTs diffuse unconstrained global attention. This conflict leaves the optimal inductive structure for robust, sparse volumetric pattern recognition unresolved. This study presents a controlled comparison of sixteen modern deep learning architectures spanning 2D/3D convolutional, hybrid, and volumetric transformer families for subclinical keratoconus (SKC) detection from 3D anterior segment OCT volumes. We demonstrate that hierarchical attention models offer a superior and more parameter-efficient inductive bias, surpassing the performance of both 2D and 3D CNNs and ViTs. Our results show 21-23% higher sensitivity and specificity in the sparse anomaly (subclinical) regime. Mechanistic analyses reveal that this advantage stems from precise spatial scale alignment: hierarchical windowing produces effective receptive fields matched to the intermediate, multi-slice extent of subclinical abnormalities. This avoids excessive CNN locality and diffuse global attention. Attention-distance measurements confirm a key insight into architectural adaptation: the required spatial integration length shifts significantly based on the signal strength, with subclinical cases necessitating longer integration compared to both healthy and manifest disease states. Representational similarity and auxiliary age/sex prediction tasks further support the generalizability of these inductive principles. The findings provide design guidance for future volumetric anomaly detection systems, establishing hierarchical attention as a principled and effective approach for early pathological change analysis in 3D medical imaging.", "AI": {"tldr": "本研究通过对比16种现代深度学习架构，包括2D/3D卷积、混合和体积变换模型，展示了分层注意力模型在从3D前段OCT图像中检测亚临床圆锥角膜（SKC）方面具有更优越的性能和更少的参数需求。", "motivation": "研究旨在解决医学影像中弱的、空间分布异常的检测问题，特别是在早期疾病信号微弱且不相邻的情况下，寻找最优的归纳结构来提高稀疏的体积模式识别的鲁棒性。", "method": "对比分析了16种现代深度学习架构，涵盖了2D/3D卷积、混合和体积变换模型，用于从3D前段OCT图像中检测亚临床圆锥角膜。", "result": "结果表明，分层注意力模型优于2D和3D卷积神经网络以及ViT，其敏感性和特异性提高了21-23%。机制分析揭示出这种方法可以通过精确的空间尺度对齐来避免过多的CNN局部性和全局注意力扩散。", "conclusion": "本研究发现为未来的体积异常检测系统设计提供了指导，确立了分层注意力作为三维医学成像中早期病理变化分析的一种合理而有效的方式。"}}
{"id": "2512.03377", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03377", "abs": "https://arxiv.org/abs/2512.03377", "authors": ["Hanting Chen", "Chu Zhong", "Kai Han", "Yuchuan Tian", "Yuchen Liang", "Tianyu Guo", "Xinghao Chen", "Dacheng Tao", "Yunhe Wang"], "title": "Nexus: Higher-Order Attention Mechanisms in Transformers", "comment": null, "summary": "Transformers have achieved significant success across various domains, relying on self-attention to capture dependencies. However, the standard first-order attention mechanism is often limited by a low-rank bottleneck, struggling to capture intricate, multi-hop relationships within a single layer. In this paper, we propose the \\textbf{Higher-Order Attention Network (Hon)}, a novel architecture designed to enhance representational power through a recursive framework. Unlike standard approaches that use static linear projections for Queries and Keys, Hon dynamically refines these representations via nested self-attention mechanisms. Specifically, the Query and Key vectors are themselves outputs of inner attention loops, allowing tokens to aggregate global context and model high-order correlations \\textit{prior} to the final attention computation. We enforce a parameter-efficient weight-sharing strategy across recursive steps, ensuring that this enhanced expressivity incurs $\\mathcal{O}(1)$ additional parameters. We provide theoretical analysis demonstrating that our method breaks the linear bottleneck of standard attention. Empirically, Hon outperforms standard Transformers on multiple benchmarks.", "AI": {"tldr": "本文介绍了更高阶注意力机制网络（Hon），通过递归框架动态优化查询和键向量的表示，增强模型的表示能力，并在理论上证明了Hon能够打破标准注意力机制的线性瓶颈，实验证明Hon在多个基准测试中优于标准Transformer模型。", "motivation": "标准自注意力机制受到低秩瓶颈的限制，难以在单层中捕捉到复杂和多跳的关系，因此提出更高阶注意力机制网络来增强模型的表示能力。", "method": "Hon通过递归框架，使用内嵌自注意力机制来动态优化查询和键向量的表示，采用参数高效的权重组共享策略控制模型参数量，以增强模型的表达能力。", "result": "理论分析表明Hon打破标准注意力机制的线性瓶颈，实验表明Hon在多个基准测试上优于标准Transformer模型。", "conclusion": "通过递归框架和内嵌自注意力机制，Hon能够打破标准自注意力机制的瓶颈并展现出更强的表征能力，实验效果证明其优于标准Transformer模型。"}}
{"id": "2512.03350", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03350", "abs": "https://arxiv.org/abs/2512.03350", "authors": ["Yu Yuan", "Tharindu Wickremasinghe", "Zeeshan Nadir", "Xijun Wang", "Yiheng Chi", "Stanley H. Chan"], "title": "SeeU: Seeing the Unseen World via 4D Dynamics-aware Generation", "comment": "Project Page: https://yuyuanspace.com/SeeU/", "summary": "Images and videos are discrete 2D projections of the 4D world (3D space + time). Most visual understanding, prediction, and generation operate directly on 2D observations, leading to suboptimal performance. We propose SeeU, a novel approach that learns the continuous 4D dynamics and generate the unseen visual contents. The principle behind SeeU is a new 2D$\\to$4D$\\to$2D learning framework. SeeU first reconstructs the 4D world from sparse and monocular 2D frames (2D$\\to$4D). It then learns the continuous 4D dynamics on a low-rank representation and physical constraints (discrete 4D$\\to$continuous 4D). Finally, SeeU rolls the world forward in time, re-projects it back to 2D at sampled times and viewpoints, and generates unseen regions based on spatial-temporal context awareness (4D$\\to$2D). By modeling dynamics in 4D, SeeU achieves continuous and physically-consistent novel visual generation, demonstrating strong potentials in multiple tasks including unseen temporal generation, unseen spatial generation, and video editing.", "AI": {"tldr": "SeeU通过一种2D到4D再到2D的学习框架，可以从2D帧中重建和生成未见的视觉内容。", "motivation": "传统的视觉理解和生成方法直接基于2D观察，导致性能次优。该研究旨在通过建模4D动态来改善这一问题。", "method": "提出了一种名为SeeU的新方法，可以从稀疏和单眼的2D帧中重建4D世界，学习连续的4D动态，并在低秩表示和物理约束下生成未见的视觉内容。", "result": "通过SeeU在多个任务（如未来时间生成，未知空间生成和视频编辑）中实现了连续且物理一致的新型视觉生成。", "conclusion": "SeeU通过建模4D动态达到了连续的和与物理一致的新视觉生成，展现了在多个任务中的潜在应用价值。"}}
{"id": "2512.03381", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03381", "abs": "https://arxiv.org/abs/2512.03381", "authors": ["Nicholas Tomlin", "Naitian Zhou", "Eve Fleisig", "Liangyuan", "Chen", "Téa Wright", "Lauren Vinh", "Laura X. Ma", "Seun Eisape", "Ellie French", "Tingting Du", "Tianjiao Zhang", "Alexander Koller", "Alane Suhr"], "title": "Characterizing Language Use in a Collaborative Situated Game", "comment": null, "summary": "Cooperative video games, where multiple participants must coordinate by communicating and reasoning under uncertainty in complex environments, yield a rich source of language data. We collect the Portal Dialogue Corpus: a corpus of 11.5 hours of spoken human dialogue in the co-op mode of the popular Portal 2 virtual puzzle game, comprising 24.5K total utterances. We analyze player language and behavior, identifying a number of linguistic phenomena that rarely appear in most existing chitchat or task-oriented dialogue corpora, including complex spatial reference, clarification and repair, and ad-hoc convention formation. To support future analyses of language use in complex, situated, collaborative problem-solving scenarios, we publicly release the corpus, which comprises player videos, audio, transcripts, game state data, and both manual and automatic annotations of language data.", "AI": {"tldr": "研究者创建了一个合作型视讯游戏中的对话语料库，包含了多种复杂且平均语料库中不常见的语言现象，并向公众开放。", "motivation": "研究合作型视讯游戏中玩家的对话，这类游戏需要参与者通过沟通和推理在复杂环境中协调行动。这些情况下的语言使用对于AI研究来说非常有参考价值，可以填补现有的闲聊和任务导向对话语料库中缺失的一些语言现象，如复杂空间指代、澄清和修正，以及自定义惯例的形成等。", "method": "创建了一个名为Portal Dialogue Corpus的语料库，包含11.5小时的口头对话数据，来源于Portal 2合作模式下的虚拟解谜游戏中，一共包含24,500条话语。语料库还包括玩家视频、音频、文字记录、游戏状态数据以及人工和自动的语言数据标注。", "result": "识别出了存在于合作型视讯游戏对话中的复杂空间指代、澄清和修正，以及自定义惯例的形成等语言现象。", "conclusion": "这些语言现象的发现表明了合作型视讯游戏对话数据在理解和分析人类语言在复杂、情境化、协作解决问题任务中使用的重要性，这为后续相关研究提供了宝贵的资源。"}}
{"id": "2512.03359", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03359", "abs": "https://arxiv.org/abs/2512.03359", "authors": ["Md Rashidul Islam", "Bakary Gibba", "Altagi Abdallah Bakheit Abdelgadir"], "title": "A Hybrid Deep Learning Framework with Explainable AI for Lung Cancer Classification with DenseNet169 and SVM", "comment": null, "summary": "Lung cancer is a very deadly disease worldwide, and its early diagnosis is crucial for increasing patient survival rates. Computed tomography (CT) scans are widely used for lung cancer diagnosis as they can give detailed lung structures. However, manual interpretation is time-consuming and prone to human error. To surmount this challenge, the study proposes a deep learning-based automatic lung cancer classification system to enhance detection accuracy and interpretability. The IQOTHNCCD lung cancer dataset is utilized, which is a public CT scan dataset consisting of cases categorized into Normal, Benign, and Malignant and used DenseNet169, which includes Squeezeand-Excitation blocks for attention-based feature extraction, Focal Loss for handling class imbalance, and a Feature Pyramid Network (FPN) for multi-scale feature fusion. In addition, an SVM model was developed using MobileNetV2 for feature extraction, improving its classification performance. For model interpretability enhancement, the study integrated Grad-CAM for the visualization of decision-making regions in CT scans and SHAP (Shapley Additive Explanations) for explanation of feature contributions within the SVM model. Intensive evaluation was performed, and it was found that both DenseNet169 and SVM models achieved 98% accuracy, suggesting their robustness for real-world medical practice. These results open up the potential for deep learning to improve the diagnosis of lung cancer by a higher level of accuracy, transparency, and robustness.", "AI": {"tldr": "该研究提出了一种基于深度学习的自动肺癌分类系统，采用DenseNet169和SVM模型结合Grad-CAM和SHAP技术进行分类和解释，大大提高了肺癌诊断的准确性和解释性。", "motivation": "肺部癌症是一种全球致命性疾病，对于提高患者的生存率而言，早期诊断非常关键。虽然CT扫描用于肺部癌症的诊断非常常见，因为它能给出详尽的肺部结构图像，但是手动解读CT非常耗时并容易出错。为此，该研究提出了一种基于深度学习的自动肺癌分类系统，以提升检测的准确性和解释性。", "method": "该研究采用了两种方法：一是使用包含Squeeze-and-Excitation块的DenseNet169进行特征提取，同时使用Focal Loss处理类别不平衡问题，以及使用Feature Pyramid Network (FPN)实现多尺度特征融合；二是使用MobileNetV2进行特征提取，并结合SVM模型以提高其分类性能。此外，为了增强模型可解释性，还集成了Grad-CAM以可视化CT扫描中的决策区域，以及SHAP用于解释SVM模型中的特征贡献。", "result": "该研究使用了名为IQOTHNCCD的公共CT扫描数据集中的病例，这些病例分为正常、良性与恶性三类。通过密集评估发现，DenseNet169和SVM模型均达到了98%的准确度，表明这两种模型在实际医疗实践中具有非常强的实用性。", "conclusion": "研究结果表明，在肺癌诊断中，该研究提出的深度学习系统显示了较高的准确性、透明度和鲁棒性，这为提高肺癌诊断的准确率开辟了潜在的道路。"}}
{"id": "2512.03402", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03402", "abs": "https://arxiv.org/abs/2512.03402", "authors": ["Yixing Xu", "Chao Li", "Xuanwu Yin", "Spandan Tiwari", "Dong Li", "Ashish Sirasao", "Emad Barsoum"], "title": "Dual LoRA: Enhancing LoRA with Magnitude and Direction Updates", "comment": null, "summary": "Low-rank adaptation (LoRA) is one of the most popular methods among parameter-efficient fine-tuning (PEFT) methods to adapt pre-trained large language models (LLMs) to specific downstream tasks. However, the model trained based on LoRA often has an unsatisfactory performance due to its low-rank assumption. In this paper, we propose a novel method called Dual LoRA to improve the performance by incorporating an inductive bias into the original LoRA. Specifically, we separate low-rank matrices into two groups: the magnitude group to control whether or not and how far we should update a parameter and the direction group to decide whether this parameter should move forward or backward, to better simulate the parameter updating process of the full fine-tuning based on gradient-based optimization algorithms. We show that this can be simply achieved by adding a ReLU function to the magnitude group and a sign function to the direction group. We conduct several experiments over a wide range of NLP tasks, including natural language generation (NLG), understanding (NLU), and commonsense reasoning datasets on GPT-2, RoBERTa, DeBERTa, and LLaMA-1/2/3 as baseline models. The results show that we consistently outperform LoRA and its state-of-the-art variants with the same number of trainable parameters.", "AI": {"tldr": "Introduces Dual LoRA to improve upon Low-Rank Adaptation (LoRA) by better simulating the full parameter updating process during fine-tuning of large language models, leading to enhanced performance.", "motivation": "To enhance the performance of fine-tuning large language models for specific tasks by addressing the limitation of LoRA's low-rank assumption.", "method": "Dual LoRA, a method that improves upon the original LoRA by dividing low-rank matrices into magnitude and direction groups, using a ReLU function for magnitude control and a sign function for direction control.", "result": "Experiments across a variety of NLP tasks with different datasets show consistent improvements over LoRA and its state-of-the-art variants with the same number of trainable parameters.", "conclusion": "Dual LoRA successfully improves the performance of fine-tuned large language models without increasing the number of trainable parameters, making it a promising method for parameter-efficient fine-tuning."}}
{"id": "2512.03369", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03369", "abs": "https://arxiv.org/abs/2512.03369", "authors": ["Nan Zhou", "Huandong Wang", "Jiahao Li", "Han Li", "Yali Song", "Qiuhua Wang", "Yong Li", "Xinlei Chen"], "title": "FireSentry: A Multi-Modal Spatio-temporal Benchmark Dataset for Fine-Grained Wildfire Spread Forecasting", "comment": null, "summary": "Fine-grained wildfire spread prediction is crucial for enhancing emergency response efficacy and decision-making precision. However, existing research predominantly focuses on coarse spatiotemporal scales and relies on low-resolution satellite data, capturing only macroscopic fire states while fundamentally constraining high-precision localized fire dynamics modeling capabilities. To bridge this gap, we present FireSentry, a provincial-scale multi-modal wildfire dataset characterized by sub-meter spatial and sub-second temporal resolution. Collected using synchronized UAV platforms, FireSentry provides visible and infrared video streams, in-situ environmental measurements, and manually validated fire masks. Building on FireSentry, we establish a comprehensive benchmark encompassing physics-based, data-driven, and generative models, revealing the limitations of existing mask-only approaches. Our analysis proposes FiReDiff, a novel dual-modality paradigm that first predicts future video sequences in the infrared modality, and then precisely segments fire masks in the mask modality based on the generated dynamics. FiReDiff achieves state-of-the-art performance, with video quality gains of 39.2% in PSNR, 36.1% in SSIM, 50.0% in LPIPS, 29.4% in FVD, and mask accuracy gains of 3.3% in AUPRC, 59.1% in F1 score, 42.9% in IoU, and 62.5% in MSE when applied to generative models. The FireSentry benchmark dataset and FiReDiff paradigm collectively advance fine-grained wildfire forecasting and dynamic disaster simulation. The processed benchmark dataset is publicly available at: https://github.com/Munan222/FireSentry-Benchmark-Dataset.", "AI": {"tldr": "提出了一种新的双模态范式FiReDiff，用于精确预测红外视频序列和分割火势掩模，实现细粒度的野火扩散预测，优于现有方法。数据集FireSentry可公开获取。", "motivation": "现有研究集中在粗空间时间尺度，采用低分辨率卫星数据，难以精确建模火势动态。为了克服这个问题，提出了高空间和时间分辨率的数据集FireSentry，以及FiReDiff范式。", "method": "FireSentry数据集提供可见光和红外视频流，以及现场环境测量。提出了FiReDiff范式，先预测红外视频序列，再分割火势掩模。", "result": "FiReDiff达到领先水平，在PSNR、SSIM、LPIPS、FVD、AUPRC、F1得分、IoU、MSE等指标上显著提高。", "conclusion": "FiReDiff范式和FireSentry数据集的进步共同推动了细粒度野火预测和灾害动力学模拟。"}}
{"id": "2512.03442", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03442", "abs": "https://arxiv.org/abs/2512.03442", "authors": ["Xingrun Xing", "Zhiyuan Fan", "Jie Lou", "Guoqi Li", "Jiajun Zhang", "Debing Zhang"], "title": "PretrainZero: Reinforcement Active Pretraining", "comment": null, "summary": "Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.", "AI": {"tldr": "PretrainZero is a reinforcement active learning framework that leverages large pretraining corpuses to improve reasoning capabilities beyond domain-specific tasks in reinforcement learning models. It focuses on active pretraining, self-supervised learning, and enhances performance through verification scaling, as evidenced by improvements on general reasoning benchmarks.", "motivation": "The motivation behind this work is to overcome the limitations of current reinforcement learning models that limit their ability to achieve general reasoning capabilities due to reliance on domain-specific rewards and verifiable labels. PretrainZero aims to enhance general reasoning by actively learning from pretraining corpuses and improving upon pretrained base models.", "method": "Recent reinforcement learning (RL) based large-thinking models have shown impressive domain-specific expertise. However, these models rely heavily on verifiable rewards, limiting their performance in general reasoning. PretrainZero addresses this limitation by proposing a reinforcement active learning framework that leverages pretraining corpuses to extend RL beyond domain-specific applications towards general pretraining. PretrainZero emphasizes active pretraining by learning a unified reasoning policy, self-supervised learning by directly training reasoners on general Wikipedia corpus without explicit labels, and verification scaling by challenging the models with increasing complexity.", "result": "PretrainZero demonstrates significant improvements over Qwen3-4B-Base on several benchmarks for general reasoning: an 8.43 increase on the MMLU-Pro benchmark, 5.96 on SuperGPQA, and 10.60 on the math average benchmark. The framework also supports downstream RLVR tasks post-pretraining.", "conclusion": "PretrainZero introduces a novel approach to reinforcing learning by using a reinforcement active learning framework that breaks away from the reliance on verifiable rewards and labels, focusing on improving general reasoning through active pretraining, self-supervised learning, and verification scaling. The results show significant improvements in general reasoning benchmarks, suggesting the framework's potential for broader development of artificial general intelligence."}}
{"id": "2512.03370", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03370", "abs": "https://arxiv.org/abs/2512.03370", "authors": ["Lingjun Zhao", "Yandong Luo", "James Hay", "Lu Gan"], "title": "ShelfGaussian: Shelf-Supervised Open-Vocabulary Gaussian-based 3D Scene Understanding", "comment": null, "summary": "We introduce ShelfGaussian, an open-vocabulary multi-modal Gaussian-based 3D scene understanding framework supervised by off-the-shelf vision foundation models (VFMs). Gaussian-based methods have demonstrated superior performance and computational efficiency across a wide range of scene understanding tasks. However, existing methods either model objects as closed-set semantic Gaussians supervised by annotated 3D labels, neglecting their rendering ability, or learn open-set Gaussian representations via purely 2D self-supervision, leading to degraded geometry and limited to camera-only settings. To fully exploit the potential of Gaussians, we propose a Multi-Modal Gaussian Transformer that enables Gaussians to query features from diverse sensor modalities, and a Shelf-Supervised Learning Paradigm that efficiently optimizes Gaussians with VFM features jointly at 2D image and 3D scene levels. We evaluate ShelfGaussian on various perception and planning tasks. Experiments on Occ3D-nuScenes demonstrate its state-of-the-art zero-shot semantic occupancy prediction performance. ShelfGaussian is further evaluated on an unmanned ground vehicle (UGV) to assess its in the-wild performance across diverse urban scenarios. Project website: https://lunarlab-gatech.github.io/ShelfGaussian/.", "AI": {"tldr": "本文介绍了一个新的三维场景理解框架ShelfGaussian，通过结合多模态高斯变压器和架式监督学习，实现了开放词汇下的高斯表示学习，展示了在2D和3D任务中的优越性能。", "motivation": "高斯基方法已经在各种场景理解任务中表现出优越的性能和计算效率。然而，现有的方法或忽视了物体的渲染能力，或限制在仅相机设置下。为了充分利用高斯模型的潜力，我们提出了ShelfGaussian框架。", "method": "我们介绍了ShelfGaussian，一个基于高斯的多模态开放式词汇的三维场景理解框架，该框架由现成的视觉基础模型（VFMs）监督。现有的方法要么将物体建模为受注释的三维标签监督的封闭集语义高斯，忽视了它们的渲染能力，要么通过纯粹的2D自我监督学习开放集高斯表示，导致几何退化并仅限于相机设置。为充分利用高斯的潜力，我们提出了一种多模态高斯变压器，使高斯能够从多种传感器模态中查询特征，以及一种架式监督学习范式，能够高效地在2D图像和3D场景级别上联合优化高斯。", "result": "我们在各种感知和规划任务上评估了ShelfGaussian。在Occ3D-nuScenes上的实验表明，它在零样本语义占用预测方面具有最先进的性能。ShelfGaussian还被评估在一个无人地面车辆（UGV）上，以评估其在多种城市环境中的表现。", "conclusion": "ShelfGaussian提供了一个能够查询多种传感器模态特征并有效优化高斯的方法，这有助于在三维场景理解中获得更好的性能和泛化能力。"}}
{"id": "2512.03494", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03494", "abs": "https://arxiv.org/abs/2512.03494", "authors": ["Di Xiu", "Hongyin Tang", "Bolin Rong", "Lizhi Yan", "Jingang Wang", "Yifan Lu", "Xunliang Cai"], "title": "A Preliminary Study on the Promises and Challenges of Native Top-$k$ Sparse Attention", "comment": null, "summary": "Large Language Models (LLMs) are increasingly prevalent in the field of long-context modeling, however, their inference computational costs have become a critical bottleneck hindering the advancement of tasks such as agents and multimodal applications. This report conducts a preliminary investigation into the effectiveness and theoretical mechanisms of the Top-$k$ Attention mechanism during both the decoding and training phases. First, we validate the effectiveness of exact Top-$k$ Decoding through extensive experimentation. Experiments demonstrate that retaining only the pivotal Keys with the highest similarity to the Query as the context window during the decoding stage achieves performance comparable to, or even surpassing, full attention on downstream tasks such as HELMET and LongBench v2. Second, we further explore the native Top-$k$ Attention training strategy. Experiments confirm that ensuring the consistency between training and inference regarding Top-$k$ Attention operations facilitates the further unlocking of Top-$k$ Decoding's potential, thereby significantly enhancing model performance. Furthermore, considering the high computational complexity of exact Top-$k$ Attention, we investigate the impact of approximate Top-$k$ algorithm precision on downstream tasks. Our research confirms a positive correlation between downstream task performance and approximation fidelity, and we provide statistical evaluations of the Lightning Indexer's precision within the DeepSeek-V3.2-Exp model. Finally, this report provides a theoretical interpretation from the perspective of Entropy. Experimental observations indicate that models subjected to Top-$k$ Attention SFT exhibit a distinct phenomenon of entropy reduction in downstream tasks, which validates the hypothesis that low-entropy states are better adapted to Top-$k$ Decoding.", "AI": {"tldr": "本报告研究了Top-$k$ 注意力机制在解码和训练阶段的有效性和理论机制，验证了这种方法在降低计算成本的同时保持高性能的潜力，并提供了基于熵的理论解释。", "motivation": "当前大型语言模型在长上下文建模中广泛应用，但其推理计算成本成为任务发展的重要障碍，需要探索有效降低成本的方法。", "method": "通过广泛的实验验证了精确Top-$k$ 解码的有效性，并进一步研究了原生Top-$k$ 注意力训练策略，探讨了近似Top-$k$ 算法精度对任务性能的影响。", "result": "实验结果表明，精确Top-$k$ 注意力在解码阶段能够达到与全注意力相当甚至超过的性能；保持训练和推理中Top-$k$ 注意力操作的一致性可以进一步提升模型性能；近似Top-$k$ 精度与任务表现正相关，并提供了对Lightning Indexer精度的统计评估。", "conclusion": "基于熵的理论视角，实验表明Top-$k$ 注意力SFT在下游任务中表现出熵减现象，支持了低熵状态更适配Top-$k$ 解码的假设。"}}
{"id": "2512.03404", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03404", "abs": "https://arxiv.org/abs/2512.03404", "authors": ["Yujian Zhao", "Hankun Liu", "Guanglin Niu"], "title": "MOS: Mitigating Optical-SAR Modality Gap for Cross-Modal Ship Re-Identification", "comment": null, "summary": "Cross-modal ship re-identification (ReID) between optical and synthetic aperture radar (SAR) imagery has recently emerged as a critical yet underexplored task in maritime intelligence and surveillance. However, the substantial modality gap between optical and SAR images poses a major challenge for robust identification. To address this issue, we propose MOS, a novel framework designed to mitigate the optical-SAR modality gap and achieve modality-consistent feature learning for optical-SAR cross-modal ship ReID. MOS consists of two core components: (1) Modality-Consistent Representation Learning (MCRL) applies denoise SAR image procession and a class-wise modality alignment loss to align intra-identity feature distributions across modalities. (2) Cross-modal Data Generation and Feature fusion (CDGF) leverages a brownian bridge diffusion model to synthesize cross-modal samples, which are subsequently fused with original features during inference to enhance alignment and discriminability. Extensive experiments on the HOSS ReID dataset demonstrate that MOS significantly surpasses state-of-the-art methods across all evaluation protocols, achieving notable improvements of +3.0%, +6.2%, and +16.4% in R1 accuracy under the ALL to ALL, Optical to SAR, and SAR to Optical settings, respectively. The code and trained models will be released upon publication.", "AI": {"tldr": "提出MOS框架解决光学和SAR图像间船舶重识别的挑战，通过模态一致表示学习和跨模态数据生成与特征融合改进了识别精度，且优于现有方法。", "motivation": "跨模态船舶重识别(Cross-modal ship ReID)任务在光学和合成孔径雷达(SAR)图像间的识别具挑战性。MOS旨在缓解光学-SAR模态差距并实现跨模态一致的特征学习。", "method": "MOS框架包含两个核心组件：(1) MCRL，通过去噪SAR图像处理和类别间的模态对齐损失来对齐同一身份在不同模态下的特征分布。(2) CDGF，利用布朗桥扩散模型生成交叉模态样本，然后在推理过程中与原始特征融合以增强对齐和区分度。", "result": "在HOSS ReID数据集上的广泛实验表明，MOS显著超越了最先进的方法，在所有评估协议中实现了R1准确率的显著提升：在ALL to ALL, 光学到SAR，以及SAR到光学设置下分别提升了+3.0%，+6.2%，和+16.4%。", "conclusion": "MOS框架在跨模态船舶重识别任务中表现出色，显著提高了不同模态间的识别准确性。"}}
{"id": "2512.03503", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03503", "abs": "https://arxiv.org/abs/2512.03503", "authors": ["Haohan Yuan", "Siu Cheung Hui", "Haopeng Zhang"], "title": "Understanding LLM Reasoning for Abstractive Summarization", "comment": "26 pages,15 figures", "summary": "While the reasoning capabilities of Large Language Models (LLMs) excel in analytical tasks such as mathematics and code generation, their utility for abstractive summarization remains widely assumed but largely unverified. To bridge this gap, we first tailor general reasoning strategies to the summarization domain. We then conduct a systematic, large scale comparative study of 8 reasoning strategies and 3 Large Reasoning Models (LRMs) across 8 diverse datasets, assessing both summary quality and faithfulness. Our findings show that reasoning is not a universal solution and its effectiveness is highly dependent on the specific strategy and context. Specifically, we observe a trade-off between summary quality and factual faithfulness: explicit reasoning strategies tend to improve fluency at the expense of factual grounding, while implicit reasoning in LRMs exhibits the inverse pattern. Furthermore, increasing an LRM's internal reasoning budget does not improve, and can even hurt, factual consistency, suggesting that effective summarization demands faithful compression rather than creative over-thinking.", "AI": {"tldr": "研究发现，在抽象摘要任务中，大语言模型的推理策略效果不尽相同，存在摘要质量和事实忠实度之间的权衡。", "motivation": "探究大语言模型在推理任务中的优越性能是否适用于抽象摘要任务，因效果缺乏系统验证。", "method": "Structure", "result": "{\n  \"tldr\": \"研究发现，在抽象摘要任务中，大语言模型的推理策略效果不尽相同，存在摘要质量和事实忠实度之间的权衡。明确推理策略虽能提高流畅性但牺牲了事实依据，而大推理模型中的隐式推理表现出相反的趋势。增加模型内部的推理预算不一定提升，甚至会损害事实一致性。\",\n  \"motivation\": \"探究大语言模型在推理任务如数学和代码生成中的优越性能是否适用于抽象摘要任务，因为这一领域的效果还缺乏系统验证。\",\n  \"method\": \"调整通用推理策略以适应摘要领域，并对8种推理策略和3种大型推理模型在8个多样化数据集上进行大规模系统比较，评估摘要的质量和忠实度。\",\n  \"result\": \"研究发现推理并不是万能的解决方案且效果高度依赖于具体策略和上下文，明确推理策略提高流畅性但牺牲事实依据，大型推理模型中的隐式推理表现相反，增加推理预算反而可能损害摘要的事实一致性。\",\n  \"conclusion\": \"有效的摘要不只是要看压缩的创意，更要保证忠实反映原始内容。\")", "conclusion": "有效的摘要需要忠实的压缩，而非创意的过度思考。"}}
{"id": "2512.03405", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03405", "abs": "https://arxiv.org/abs/2512.03405", "authors": ["Jiangtao Wu", "Shihao Li", "Zhaozhou Bian", "Yuanxing Zhang", "Jialu Chen", "Runzhe Wen", "An Ping", "Yiwen He", "Jiakai Wang", "Jiaheng Liu"], "title": "ViDiC: Video Difference Captioning", "comment": null, "summary": "Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2512.03582", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03582", "abs": "https://arxiv.org/abs/2512.03582", "authors": ["Zeba Afroz", "Harsh Vardhan", "Pawan Bhakuni", "Aanchal Punia", "Rajdeep Kumar", "Md. Shad Akhtar"], "title": "Fine-grained Narrative Classification in Biased News Articles", "comment": null, "summary": "Narratives are the cognitive and emotional scaffolds of propaganda. They organize isolated persuasive techniques into coherent stories that justify actions, attribute blame, and evoke identification with ideological camps. In this paper, we propose a novel fine-grained narrative classification in biased news articles. We also explore article-bias classification as the precursor task to narrative classification and fine-grained persuasive technique identification. We develop INDI-PROP, the first ideologically grounded fine-grained narrative dataset with multi-level annotation for analyzing propaganda in Indian news media. Our dataset INDI-PROP comprises 1,266 articles focusing on two polarizing socio-political events in recent times: CAA and the Farmers' protest. Each article is annotated at three hierarchical levels: (i) ideological article-bias (pro-government, pro-opposition, neutral), (ii) event-specific fine-grained narrative frames anchored in ideological polarity and communicative intent, and (iii) persuasive techniques. We propose FANTA and TPTC, two GPT-4o-mini guided multi-hop prompt-based reasoning frameworks for the bias, narrative, and persuasive technique classification. FANTA leverages multi-layered communicative phenomena by integrating information extraction and contextual framing for hierarchical reasoning. On the other hand, TPTC adopts systematic decomposition of persuasive cues via a two-stage approach. Our evaluation suggests substantial improvement over underlying baselines in each case.", "AI": {"tldr": "本文通过对印度新闻媒体中两个具有争议的社会政治事件(CAA和农民抗议活动)的分析，提出了一种新颖的细致叙事分类方法，并开发了用于分析宣传的精细数据集INDI-PROP。同时，提出了FANTA和TPTC两种框架以改进偏见、叙事和劝说技巧的分类性能。", "motivation": "为了在有偏见的新闻文章中进行精细的叙事分类，作者开发了INDI-PROP数据集，这是首个基于意识形态的精细叙事数据集，用于分析印度新闻媒体中的宣传。", "method": "本文提出了FANTA和TPTC两种基于多跳提示推理框架的方法，用于偏见、叙事和劝说技巧分类。FANTA通过整合信息提取和上下文框架来进行分层推理，而TPTC采用两阶段的方法来系统地分解劝说线索。", "result": "实验结果表明，所提出的框架在偏见、叙事和劝说技巧分类上相较于基线方法有显著提升。", "conclusion": "研究揭示了在有偏见的新闻文章中进行细致叙事分类的潜力，并展示了FANTA和TPTC框架如何有效提高宣传分析任务的性能。"}}
{"id": "2512.03418", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.03418", "abs": "https://arxiv.org/abs/2512.03418", "authors": ["Yuqi Ji", "Junjie Ke", "Lihuo He", "Jun Liu", "Kaifan Zhang", "Yu-Kun Lai", "Guiguang Ding", "Xinbo Gao"], "title": "YOLOA: Real-Time Affordance Detection via LLM Adapter", "comment": "13 pages, 9 figures, conference", "summary": "Affordance detection aims to jointly address the fundamental \"what-where-how\" challenge in embodied AI by understanding \"what\" an object is, \"where\" the object is located, and \"how\" it can be used. However, most affordance learning methods focus solely on \"how\" objects can be used while neglecting the \"what\" and \"where\" aspects. Other affordance detection methods treat object detection and affordance learning as two independent tasks, lacking effective interaction and real-time capability. To overcome these limitations, we introduce YOLO Affordance (YOLOA), a real-time affordance detection model that jointly handles these two tasks via a large language model (LLM) adapter. Specifically, YOLOA employs a lightweight detector consisting of object detection and affordance learning branches refined through the LLM Adapter. During training, the LLM Adapter interacts with object and affordance preliminary predictions to refine both branches by generating more accurate class priors, box offsets, and affordance gates. Experiments on our relabeled ADG-Det and IIT-Heat benchmarks demonstrate that YOLOA achieves state-of-the-art accuracy (52.8 / 73.1 mAP on ADG-Det / IIT-Heat) while maintaining real-time performance (up to 89.77 FPS, and up to 846.24 FPS for the lightweight variant). This indicates that YOLOA achieves an excellent trade-off between accuracy and efficiency.", "AI": {"tldr": "Introducing YOLOA, a real-time affordance detection model that uses a LLM adapter to refine object and affordance predictions jointly, achieving high accuracy and FPS on benchmarks.", "motivation": "To address the limitations of existing methods that either neglect object recognition and location or treat affordance detection and object detection as separate tasks without effective interaction.", "method": "YOLOA is a real-time affordance detection model that integrates object detection and affordance learning using a large language model (LLM) adapter to refine both branches during training.", "result": "YOLOA achieves state-of-the-art accuracy with 52.8 / 73.1 mAP on ADG-Det / IIT-Heat benchmarks, simultaneously maintaining real-time performance at up to 89.77 FPS, with a lightweight variant reaching up to 846.24 FPS.", "conclusion": "YOLOA demonstrates an excellent balance between accuracy and efficiency, making it a promising approach for real-time affordance detection in embodied AI."}}
