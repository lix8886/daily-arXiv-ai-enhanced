<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 7]
- [cs.CV](#cs.CV) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Inconsistent Affective Reaction: Sentiment of Perception and Opinion in Urban Environments](https://arxiv.org/abs/2510.07359)
*Jingfei Huang,Han Tu*

Main category: cs.CL

> 本研究通过分析包含百度和腾讯街景图像及微博社交文本的数据集，提出了新的方法来识别和阐明北京市二环内的感知与意见的情感不一致性，并通过不同技术手段分析和可视化情感反应趋势，揭示了情感随时间变化与城市元素的关联。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在探索社交媒体对城市感知的影响，以及提出新的方法来理解城市不同区域的情感变化和情感不一致性的起因。

**Method:** 本研究构建了一个结合140,750张百度和腾讯街景图像及984,024条微博文本数据的集合，使用了一种综合对象检测和自然语言处理技术的反应指数来分类情感，并利用回归分析、图像分割和基于土地使用分布的词频进行分析。

**Result:** 研究表明，情感反应感知趋势图显示出正向情感变得更均匀，而意见情感反应趋势图则显示了更极端的变化。情感反应随时间变化与密布的建筑物和行人数量有显著关联。

**Conclusion:** 本研究的不一致情感图谱揭示了人类对城市区域的感知和意见的情感差异，并为城市更新策略提供潜在解释和方向。

**Abstract:** The ascension of social media platforms has transformed our understanding of
urban environments, giving rise to nuanced variations in sentiment reaction
embedded within human perception and opinion, and challenging existing
multidimensional sentiment analysis approaches in urban studies. This study
presents novel methodologies for identifying and elucidating sentiment
inconsistency, constructing a dataset encompassing 140,750 Baidu and Tencent
Street view images to measure perceptions, and 984,024 Weibo social media text
posts to measure opinions. A reaction index is developed, integrating object
detection and natural language processing techniques to classify sentiment in
Beijing Second Ring for 2016 and 2022. Classified sentiment reaction is
analysed and visualized using regression analysis, image segmentation, and word
frequency based on land-use distribution to discern underlying factors. The
perception affective reaction trend map reveals a shift toward more evenly
distributed positive sentiment, while the opinion affective reaction trend map
shows more extreme changes. Our mismatch map indicates significant disparities
between the sentiments of human perception and opinion of urban areas over the
years. Changes in sentiment reactions have significant relationships with
elements such as dense buildings and pedestrian presence. Our inconsistent maps
present perception and opinion sentiments before and after the pandemic and
offer potential explanations and directions for environmental management, in
formulating strategies for urban renewal.

</details>


### [2] [Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation](https://arxiv.org/abs/2510.07414)
*Mufei Li,Dongqi Fu,Limei Wang,Si Zhang,Hanqing Zeng,Kaan Sancak,Ruizhong Qiu,Haoyu Wang,Xiaoxin He,Xavier Bresson,Yinglong Xia,Chonglin Sun,Pan Li*

Main category: cs.CL

> 研究构建了HaystackCraft基准测试，模拟现实世界中的噪声长上下文对长上下文大型语言模型的影响，发现更强的密集检索策略会引入更多挑战性的分散注意力内容，但图基重排序能提高检索效果并减少有害的分散注意力。

<details>
  <summary>Details</summary>

**Motivation:** 当前的长上下文大型语言模型在合成的“针在稻草堆”测试中表现良好，但忽略了来自偏差检索和智能工作流程的噪声上下文。作者认为需要进行“稻草堆工程”，以构建能真实反映关键现实因素的噪声长上下文，以测试模型对长上下文的健壮性。

**Method:** 构建HaystackCraft基准测试，该测试基于完整的英文维基百科超链接网络，包含多跳问题。通过多种检索策略（稀疏、密集、混合以及图基）来评估它们如何影响分散注意力内容的构成、排序以及对大型语言模型性能的影响。此外，该测试还将NIAH（针在稻草堆）扩展到了动态的、依赖语言模型的设置中，模拟操作流程中模型调整查询、反思过去的推理、决定何时停止的过程。

**Result:** 实验显示，虽然更强的密集检索策略会引入更具挑战性的分散注意力内容，但图基重排序同时提高了检索的有效性并减少了更多有害的分散注意力。在智能测试中，即使是GPT-5这样的先进模型也遭受来自自生分散注意力的级联失败或难以实现早期停止。

**Conclusion:** 这些结果突显了在智能长上下文推理中面临的持续挑战，并确立了HaystackCraft作为未来进展重要测试平台的地位。

**Abstract:** Modern long-context large language models (LLMs) perform well on synthetic
"needle-in-a-haystack" (NIAH) benchmarks, but such tests overlook how noisy
contexts arise from biased retrieval and agentic workflows. We argue that
haystack engineering is necessary to construct noisy long contexts that
faithfully capture key real-world factors -- distraction from heterogeneous
biased retrievers and cascading errors in agentic workflows -- to test models'
long-context robustness. We instantiate it through HaystackCraft, a new NIAH
benchmark built on the full English Wikipedia hyperlink network with multi-hop
questions. HaystackCraft evaluates how heterogeneous retrieval strategies
(e.g., sparse, dense, hybrid, and graph-based) affect distractor composition,
haystack ordering, and downstream LLM performance. HaystackCraft further
extends NIAH to dynamic, LLM-dependent settings that simulate agentic
operations, where models refine queries, reflect on their past reasonings, and
decide when to stop. Experiments with 15 long-context models show that (1)
while stronger dense retrievers can introduce more challenging distractors,
graph-based reranking simultaneously improves retrieval effectiveness and
mitigates more harmful distractors; (2) in agentic tests, even advanced models
like Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated
distractors or struggle to perform early stops. These results highlight
persistent challenges in agentic long-context reasoning and establish
HaystackCraft as a valuable testbed for future progress.

</details>


### [3] [Lemma Dilemma: On Lemma Generation Without Domain- or Language-Specific Training Data](https://arxiv.org/abs/2510.07434)
*Olia Toporkov,Alan Akbik,Rodrigo Agerri*

Main category: cs.CL

> 通过对12种不同形态复杂度的语言进行实验，尽管编码器在领域外设置中仍然具有竞争力，但现有的大语言模型（LLMs）通过直接生成引词而无需预先微调，在大多数语言中达到了最先进的性能。

<details>
  <summary>Details</summary>

**Motivation:** 虽然大型语言模型已经在许多NLP任务上展示了其竞争力，但还没有证据显示它们在上下文引词任务上的效果如何。本文旨在研究LLMs在该任务上的能力，并将其与传统方法进行比较。

**Method:** 我们通过比较最新一代的大语言模型（LLMs）在上下文化引词（lemmatization）任务中的表现与传统全监督方法的表现来进行实证研究。具体来说，我们主要探讨在目标领域或语言中没有监督训练数据的情况下的表现，对比了(i) 仅编码器的监督方法（在领域外进行微调） 和 (ii) 跨语言方法，与直接使用LLMs上下文生成引词的表现。

**Result:** 实验结果表明，尽管基于编码器的方法在使用黄金数据进行领域外微调时仍然具有竞争力，但现有的LLMs通过直接生成引词且无需预先微调，在大多数语言中都取得了最先进的结果。

**Conclusion:** 研究表明，最新的大型语言模型在没有目标领域或语言的训练数据时，也能通过少量示例直接生成引词来达到优越的表现，这显示出它们在NLP任务中潜在的强大能力。

**Abstract:** Lemmatization is the task of transforming all words in a given text to their
dictionary forms. While large language models (LLMs) have demonstrated their
ability to achieve competitive results across a wide range of NLP tasks, there
is no prior evidence of how effective they are in the contextual lemmatization
task. In this paper, we empirically investigate the capacity of the latest
generation of LLMs to perform in-context lemmatization, comparing it to the
traditional fully supervised approach. In particular, we consider the setting
in which supervised training data is not available for a target domain or
language, comparing (i) encoder-only supervised approaches, fine-tuned
out-of-domain, and (ii) cross-lingual methods, against direct in-context lemma
generation with LLMs. Our experimental investigation across 12 languages of
different morphological complexity finds that, while encoders remain
competitive in out-of-domain settings when fine-tuned on gold data, current
LLMs reach state-of-the-art results for most languages by directly generating
lemmas in-context without prior fine-tuning, provided just with a few examples.
Data and code available upon publication:
https://github.com/oltoporkov/lemma-dilemma

</details>


### [4] [LASER: An LLM-based ASR Scoring and Evaluation Rubric](https://arxiv.org/abs/2510.07437)
*Amruta Parulekar,Preethi Jyothi*

Main category: cs.CL

> 提出了一个基于大语言模型的评分标准LASER，该标准在评估自动语音识别系统时，能够更好地考量语言的细微差异。实验表明，该方法具有较高的准确性和可靠性。

<details>
  <summary>Details</summary>

**Motivation:** 传统的诸如词错误率(WER)的自动语音识别(ASR)评估指标倾向于对不显著改变句子语义的形态和句法细微差别不公平地处罚。引入LASER是为了更公正地评估这些细微差别。

**Method:** 开发了一种基于大规模语言模型（LLM）的评分标准LASER，该标准利用了LLM的上下文学习能力，通过详细示例的提示进行学习。此外，还展示了如何对较小的LLM（如Llama 3）进行微调，使其能够根据参考文本和ASR预测生成的词对样例来预测应施加何种惩罚。

**Result:** 使用Gemini 2.5 Pro的Hindi LASER评分获得了与人类标注非常高的94%的相关性评分。Hindi示例提示也有效地分析了其他印度语言（如Marathi、Kannada和Malayalam）中的错误。较小的LLM（如Llama 3）的微调预测准确性达到了接近89%。

**Conclusion:** 研究表明，基于LLM的评分系统可以有效地评估ASR系统，特别是在语言细微差别方面。此外，这种系统不仅可以应用于Hindi，还可以应用于其他印度语言，显示了其广泛的适用性。

**Abstract:** Standard ASR evaluation metrics like Word Error Rate (WER) tend to unfairly
penalize morphological and syntactic nuances that do not significantly alter
sentence semantics. We introduce an LLM-based scoring rubric LASER that
leverages state-of-the-art LLMs' in-context learning abilities to learn from
prompts with detailed examples. Hindi LASER scores using Gemini 2.5 Pro
achieved a very high correlation score of 94% with human annotations. Hindi
examples in the prompt were also effective in analyzing errors in other Indian
languages such as Marathi, Kannada and Malayalam. We also demonstrate how a
smaller LLM like Llama 3 can be finetuned on word-pair examples derived from
reference and ASR predictions to predict what kind of penalty should be applied
with close to 89% accuracy.

</details>


### [5] [Meaningful Pose-Based Sign Language Evaluation](https://arxiv.org/abs/2510.07453)
*Zifan Jiang,Colin Leong,Amit Moryossef,Anne Göhring,Annette Rios,Oliver Cory,Maksym Ivashechkin,Neha Tarigopula,Biao Zhang,Rico Sennrich,Sarah Ebling*

Main category: cs.CL

> TL;DR：通过自动和人类评估手段综合考查了 sign language 人工姿态的评价方法，发现不同指标在场景下的权衡，发布了评估工具包。

<details>
  <summary>Details</summary>

**Motivation:** 动机：改善 sign language 在人工姿态表示中的评估，提供一套全面的评估指标，帮助研究人员和开发者更好地理解不同指标在不同场景下的表现。

**Method:** 方法：研究涵盖了基于关键点距离测量、基于嵌入和基于反翻译的指标，采用自动元评估和人类相关研究来综合分析这些方法的有效性。

**Result:** 内容摘要：我们呈献了一项关于有意义地评估以人类骨骼姿态形式出现的 sign language 发音的综合研究。该研究涵盖了基于关键点距离、基于嵌入的和基于反翻译的指标。我们通过自动元评估签名级别检索和跨不同 sign language 的文本到姿态翻译的人类相关研究，展示了不同指标在不同场景中的权衡。我们的发现以及开源姿态评估工具包为开发和评估 sign language 翻译或生成系统提供了实用且可重现的方法。

**Conclusion:** 结论：研究表明不同指标在不同场景下的表现有所不同，提供了一套实用的评估方法并通过开源工具包推动了 sign language 翻译和生成系统的开发。

**Abstract:** We present a comprehensive study on meaningfully evaluating sign language
utterances in the form of human skeletal poses. The study covers keypoint
distance-based, embedding-based, and back-translation-based metrics. We show
tradeoffs between different metrics in different scenarios through automatic
meta-evaluation of sign-level retrieval and a human correlation study of
text-to-pose translation across different sign languages. Our findings and the
open-source pose-evaluation toolkit provide a practical and reproducible way of
developing and evaluating sign language translation or generation systems.

</details>


### [6] [Populism Meets AI: Advancing Populism Research with LLMs](https://arxiv.org/abs/2510.07458)
*Eduardo Ryô Tamaki,Yujin J. Jung,Julia Chatterley,Grant Mitchell,Semir Dzebo,Cristóbal Sandoval,Levente Littvay,Kirk A. Hawkins*

Main category: cs.CL

> 研究通过模仿人类编码训练过程的特定领域提示策略，使语言模型能够实现与专家人类编码者相媲美的分类准确性，成功测量民粹主义的复杂和情境敏感方面。

<details>
  <summary>Details</summary>

**Motivation:** 传统的基于文本分析策略在建立民粹主义理念内容测量的基础方面发挥了关键作用，但这些方法成本高昂、耗时且难以在多种语言、情境和大规模语料库中扩展。本研究旨在开发一种更高效的方法来测量民粹主义的理念内容。

**Method:** 通过基于准则和范例引导的连贯性思考（CoT）提示方法，模仿人类编码者的训练过程，利用Global Populism Database（GPD），一个全球领导人演讲的全面数据集，该数据集根据民粹主义的程度进行了注释。通过使用此数据集，对多个专有和开放权重的模型进行测试以复制GPD中的分数。

**Result:** 实验结果表明，此种领域特定的提示策略可以使得语言模型达到与专家人类评分者相当的分类准确率，表现出其在处理民粹主义复杂且情境相关方面的能力。

**Conclusion:** 基于准则和范例引导的连贯性思考（CoT）提示方法可以作为传统文本分析策略的一种有效替代，有助于构建更高效和准确的民粹主义理念内容测量方法。

**Abstract:** Measuring the ideational content of populism remains a challenge. Traditional
strategies based on textual analysis have been critical for building the
field's foundations and providing a valid, objective indicator of populist
framing. Yet these approaches are costly, time consuming, and difficult to
scale across languages, contexts, and large corpora. Here we present the
results from a rubric and anchor guided chain of thought (CoT) prompting
approach that mirrors human coder training. By leveraging the Global Populism
Database (GPD), a comprehensive dataset of global leaders' speeches annotated
for degrees of populism, we replicate the process used to train human coders by
prompting the LLM with an adapted version of the same documentation to guide
the model's reasoning. We then test multiple proprietary and open weight models
by replicating scores in the GPD. Our findings reveal that this domain specific
prompting strategy enables the LLM to achieve classification accuracy on par
with expert human coders, demonstrating its ability to navigate the nuanced,
context sensitive aspects of populism.

</details>


### [7] [MAPRO: Recasting Multi-Agent Prompt Optimization as Maximum a Posteriori Inference](https://arxiv.org/abs/2510.07475)
*Zheyuan Zhang,Lin Ge,Hongjiang Li,Weicheng Zhu,Chuxu Zhang,Yanfang Ye*

Main category: cs.CL

> 论文提出了MAPRO框架，用于解决多智能体系统中提示优化的难题，主要通过使用语言引导的最大乘积信念传播算法来解决优化问题，并且通过选择性更新代理提示来改进系统的性能。

<details>
  <summary>Details</summary>

**Motivation:** 尽管多智能体系统（MAS）能够通过协调专业化角色超越单个智能体的性能，但在设计有效的MAS时，仍然面临提示敏感性和系统复杂度导致的不稳定性挑战。为了应对这些挑战，该论文提出了MAPRO框架，专门解决多智能体的提示优化问题。

**Method:** 该论文提出的MAPRO框架通过四个阶段将多智能体系统的提示优化问题转化为后验概率最大的推断问题，并利用语言引导的最大乘积信念传播算法来解决。为了处理信用分配问题，MAPRO采用了基于拓扑结构的精细机制，结合执行反馈和下游责任反馈，选择性地更新智能体的提示信息，逐步聚合到一组协调的智能体特定的提示策略。

**Result:** 在各种任务的基准测试中，MAPRO达到了最先进的性能，一直超越手动设计的基线以及最近的自动化方法。

**Conclusion:** MAPRO不仅实现了更好的性能，而且其基于MAP的公式还为未来构建更可靠和原理性的多智能体系统提供了普遍的指导方针。

**Abstract:** Large language models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, and LLM-based agents further extend these abilities to various
practical workflows. While recent progress shows that multi-agent systems (MAS)
can outperform single agents by coordinating specialized roles, designing
effective MAS remains difficult due to prompt sensitivity and the compounded
instability MAS creates. To cope with the challenge, recent efforts in
automated prompt design have reduced manual effort. However, multi-agent prompt
optimization remains largely unexplored. Challenges like exponentially
expanding search space and ambiguous credit assignment together make systematic
design intractable without principled methods. Therefore, we introduce
M}ulti-Agent PRompt Optimization (MAPRO), a four-stage framework that first
formulates MAS prompt optimization as a Maximum a Posteriori (MAP) inference
problem and solves it using a language-guided variant of max-product belief
propagation algorithm. To address credit assignment and updates the system
iteratively, MAPRO employs a topology-aware refinement mechanism that
integrates execution feedback and downstream blames to selectively update agent
prompts. Through this process, MAPRO progressively converges to a coordinated
set of agent-specific prompt policies. Across benchmarks in various tasks,
MAPRO achieves state-of-the-art performance, consistently surpassing manually
engineered baselines and recent automated alternatives. Beyond performance, our
MAP-based formulation also delivers general guidelines for building more
reliable and principled multi-agent systems in the future

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [Enhancing Maritime Object Detection in Real-Time with RT-DETR and Data Augmentation](https://arxiv.org/abs/2510.07346)
*Nader Nemati*

Main category: cs.CV

> 文章介绍了一种基于RT-DETR的实时海上物体检测系统，通过多种技术手段解决了小目标检测和图像增强的问题，并提供了一个完整的Python检测流程。

<details>
  <summary>Details</summary>

**Motivation:** 解决海上物体检测中存在的目标尺寸小和真实RGB数据标注不足的问题。

**Method:** 使用RT-DETR基础，结合多尺度特征融合、不确定性最小化查询选择、智能权重策略和数据增强技术来优化海上物体检测。

**Result:** 此论文提出了一种基于RT-DETR的实时海上物体检测系统，通过使用增强的合成图像来弥补真实RGB数据标注的不足。系统结合了多尺度特征融合、不确定性最小化查询选择和智能权重策略应对真实与合成样本之间的视觉差异。该设计保留了DETR的端到端集合预测优势，并允许在推理时调整速度和准确度之间的平衡。此外，还通过数据增强技术平衡了数据集中的各类物体，提高了系统的鲁棒性和准确性。论文中详细介绍了一个完整的Python海上物体检测流程，并验证了每个模块的功能以及系统在极端光照或海况环境下的表现。

**Conclusion:** 通过结合多种技术创新，提出了一种能够实时检测海上物体的系统，具有较好的鲁棒性和准确性，并提供了一个完整的Python实现流程。

**Abstract:** Maritime object detection faces essential challenges due to the small target
size and limitations of labeled real RGB data. This paper will present a
real-time object detection system based on RT-DETR, enhanced by employing
augmented synthetic images while strictly evaluating on real data. This study
employs RT-DETR for the maritime environment by combining multi-scale feature
fusion, uncertainty-minimizing query selection, and smart weight between
synthetic and real training samples. The fusion module in DETR enhances the
detection of small, low-contrast vessels, query selection focuses on the most
reliable proposals, and the weighting strategy helps reduce the visual gap
between synthetic and real domains. This design preserves DETR's refined
end-to-end set prediction while allowing users to adjust between speed and
accuracy at inference time. Data augmentation techniques were also used to
balance the different classes of the dataset to improve the robustness and
accuracy of the model. Regarding this study, a full Python robust maritime
detection pipeline is delivered that maintains real-time performance even under
practical limits. It also verifies how each module contributes, and how the
system handles failures in extreme lighting or sea conditions. This study also
includes a component analysis to quantify the contribution of each
architectural module and explore its interactions.

</details>


### [9] [DynamicEval: Rethinking Evaluation for Dynamic Text-to-Video Synthesis](https://arxiv.org/abs/2510.07441)
*Nithin C. Babu,Aniruddha Mahapatra,Harsh Rangwani,Rajiv Soundararajan,Kuldeep Kulkarni*

Main category: cs.CV

> 本文旨在通过引入DynamicEval，以解决现有T2V评估基准中的局限性，即缺乏对动态摄像机运动的评估，提出新的背景和前景一致性评估指标，提高人类偏好的预测准确性。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有的T2V评估基准无法充分评测动态摄像机运动下视频的质量，且在视频级别上未能充分进行评估，作者旨在改进这些问题，使模型评估更加全面。

**Method:** 内容介绍了针对现有文本到视频（T2V）评估基准存在的局限性，提出了一个新的评估指标DynamicEval，该基准着重于动态摄像机运动并配以人类对视频的评价。此外，还提出了两种新的评估视频质量的关键维度的指标：背景场景一致性指标和前景目标一致性指标。

**Result:** 经实验显示，所提出的背景和前景一致性评估指标在视频级别和模型级别的人类偏好预测中的准确性有了明显的提升。

**Conclusion:** DynamicEval作为一个新的评估基准，可以更全面地评估在动态摄像机运动条件下的视频生成模型。

**Abstract:** Existing text-to-video (T2V) evaluation benchmarks, such as VBench and
EvalCrafter, suffer from two limitations. (i) While the emphasis is on
subject-centric prompts or static camera scenes, camera motion essential for
producing cinematic shots and existing metrics under dynamic motion are largely
unexplored. (ii) These benchmarks typically aggregate video-level scores into a
single model-level score for ranking generative models. Such aggregation,
however, overlook video-level evaluation, which is vital to selecting the
better video among the candidate videos generated for a given prompt. To
address these gaps, we introduce DynamicEval, a benchmark consisting of
systematically curated prompts emphasizing dynamic camera motion, paired with
45k human annotations on video pairs from 3k videos generated by ten T2V
models. DynamicEval evaluates two key dimensions of video quality: background
scene consistency and foreground object consistency. For background scene
consistency, we obtain the interpretable error maps based on the Vbench motion
smoothness metric. We observe that while the Vbench motion smoothness metric
shows promising alignment with human judgments, it fails in two cases:
occlusions/disocclusions arising from camera and foreground object movements.
Building on this, we propose a new background consistency metric that leverages
object error maps to correct two failure cases in a principled manner. Our
second innovation is the introduction of a foreground consistency metric that
tracks points and their neighbors within each object instance to assess object
fidelity. Extensive experiments demonstrate that our proposed metrics achieve
stronger correlations with human preferences at both the video level and the
model level (an improvement of more than 2% points), establishing DynamicEval
as a more comprehensive benchmark for evaluating T2V models under dynamic
camera motion.

</details>


### [10] [Provably Accelerated Imaging with Restarted Inertia and Score-based Image Priors](https://arxiv.org/abs/2510.07470)
*Marien Renaud,Julien Hermant,Deliang Wei,Yu Sun*

Main category: cs.CV

> RISP 是一种结合快速收敛和高质量图像恢复的算法，它在 RED 的基础上加入了重启惯性策略，同时保持了基于评分的图像先验，从而在不依赖图像先验函数的凸性的情况下实现了更快的收敛速度，并在多种图像逆问题中得到了验证。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于解决现有算法在图像恢复过程中快速收敛和高质量恢复两个方面难以兼顾的问题，现有方法如正则化去噪 (RED) 通常集中在设计复杂的图像先验来改善重建质量，而收敛加速则依赖于策略性手段。

**Method:** 提出了一种新的算法 RISP，该算法在 RED 的基础上增加了重启惯性策略，同时保持了基于评分的图像先验，以获得快速收敛速度和高质量的图像重建。

**Result:** 证明 RISP 达到了比 RED 更快的静止点收敛速度，并且在多种图像逆问题中实验验证了 RISP 可以实现快速收敛的同时获得高质量的重建结果。

**Conclusion:** RISP 通过引入重启惯性，实现了既快速收敛又高质图像恢复的目标，克服了 RED 等现有算法在收敛速度上的限制，并展示了其在各类图像逆问题中的实用性。

**Abstract:** Fast convergence and high-quality image recovery are two essential features
of algorithms for solving ill-posed imaging inverse problems. Existing methods,
such as regularization by denoising (RED), often focus on designing
sophisticated image priors to improve reconstruction quality, while leaving
convergence acceleration to heuristics. To bridge the gap, we propose Restarted
Inertia with Score-based Priors (RISP) as a principled extension of RED. RISP
incorporates a restarting inertia for fast convergence, while still allowing
score-based image priors for high-quality reconstruction. We prove that RISP
attains a faster stationary-point convergence rate than RED, without requiring
the convexity of the image prior. We further derive and analyze the associated
continuous-time dynamical system, offering insight into the connection between
RISP and the heavy-ball ordinary differential equation (ODE). Experiments
across a range of imaging inverse problems demonstrate that RISP enables fast
convergence while achieving high-quality reconstructions.

</details>
