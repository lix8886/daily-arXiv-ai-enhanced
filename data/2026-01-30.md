<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 12]
- [cs.CV](#cs.CV) [Total: 16]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents](https://arxiv.org/abs/2601.20975)
*Nikita Gupta,Riju Chatterjee,Lukas Haas,Connie Tao,Andrew Wang,Chang Liu,Hidekazu Oiwa,Elena Gribovskaya,Jan Ackermann,John Blitzer,Sasha Goldshtein,Dipanjan Das*

Main category: cs.CL

> DeepSearchQA是一个用于评估智能代理复杂信息检索能力的基准测试，展示了现有模型在特定任务中的局限性，并指出未来改进的方向。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于评估现有智能代理在复杂搜索和信息整合任务上的不足，并为未来的研发提供诊断工具。

**Method:** 本文介绍了一个名为DeepSearchQA的基准测试，它包含900个提示，用于评估智能代理在17个不同领域中进行多层次信息检索任务的能力。该测试强调系统化整理破碎信息、去重和实体解析、以及在开放搜索空间中判断停止标准的能力。

**Result:** 对当前最先进的代理模型进行全面评估后，发现它们在平衡高召回率和精准度方面存在问题。这些问题包括提前停止搜索（过少检索）和过分宽泛的低置信度答案以提高看似更高的召回率。

**Conclusion:** 研究表明当前代理设计中存在提升空间，DeepSearchQA测试为未来研究提供了诊断工具，推动研发更稳健的深度检索能力。

**Abstract:** We introduce DeepSearchQA, a 900-prompt benchmark for evaluating agents on difficult multi-step information-seeking tasks across 17 different fields. Unlike traditional benchmarks that target single answer retrieval or broad-spectrum factuality, DeepSearchQA features a dataset of challenging, handcrafted tasks designed to evaluate an agent's ability to execute complex search plans to generate exhaustive answer lists. This shift in design explicitly tests three critical, yet under-evaluated capabilities: 1) systematic collation of fragmented information from disparate sources, 2) de-duplication and entity resolution to ensure precision, and 3) the ability to reason about stopping criteria within an open-ended search space. Each task is structured as a causal chain, where discovering information for one step is dependent on the successful completion of the previous one, stressing long-horizon planning and context retention. All tasks are grounded in the open web with objectively verifiable answer sets. Our comprehensive evaluation of state-of-the-art agent architectures reveals significant performance limitations: even the most advanced models struggle to balance high recall with precision. We observe distinct failure modes ranging from premature stopping (under-retrieval) to hedging behaviors, where agents cast an overly wide net of low-confidence answers to artificially boost recall. These findings highlight critical headroom in current agent designs and position DeepSearchQA as an essential diagnostic tool for driving future research toward more robust, deep-research capabilities.

</details>


### [2] [asr_eval: Algorithms and tools for multi-reference and streaming speech recognition evaluation](https://arxiv.org/abs/2601.20992)
*Oleg Sedukhin,Andrey Kostin*

Main category: cs.CL

> 本文提出了改进的语音识别评估方法，包括字符串对齐算法及新的测试数据集DiverseSpeech-Ru，同时开发了评估流式语音识别及多转录对比工具。

<details>
  <summary>Details</summary>

**Motivation:** 传统的语音识别评估方法存在一些缺陷，特别是在非拉丁语等具有丰富词形变化的语言上。

**Method:** 提出一种改进的字符串对齐算法，支持多参考标注、任意长度插入和更好的词对齐，同时收集了长格式的野生俄罗斯语音测试集DiverseSpeech-Ru。

**Result:** 实验显示模型通常会适应数据集特定的标注，而造成度量提升的错觉。基于改进的词对齐，开发了流式语音识别评估工具和多个转录对比工具。

**Conclusion:** 本文提出了语音识别评估的新方法和工具，并计划将代码公开。

**Abstract:** We propose several improvements to the speech recognition evaluation. First, we propose a string alignment algorithm that supports both multi-reference labeling, arbitrary-length insertions and better word alignment. This is especially useful for non-Latin languages, those with rich word formation, to label cluttered or longform speech. Secondly, we collect a novel test set DiverseSpeech-Ru of longform in-the-wild Russian speech with careful multi-reference labeling. We also perform multi-reference relabeling of popular Russian tests set and study fine-tuning dynamics on its corresponding train set. We demonstrate that the model often adopts to dataset-specific labeling, causing an illusion of metric improvement. Based on the improved word alignment, we develop tools to evaluate streaming speech recognition and to align multiple transcriptions to compare them visually. Additionally, we provide uniform wrappers for many offline and streaming speech recognition models. Our code will be made publicly available.

</details>


### [3] [UrduBench: An Urdu Reasoning Benchmark using Contextually Ensembled Translations with Human-in-the-Loop](https://arxiv.org/abs/2601.21000)
*Muhammad Ali Shafique,Areej Mehboob,Layba Fiaz,Muhammad Usman Qadeer,Hamza Farooq*

Main category: cs.CL

> 本文提出了一种用于乌尔都语推理评估的翻译框架，并将其应用于乌尔都语基准的创建，分析了LLMs在不同任务上的性能，并指出语言稳定性对推理至关重要。

<details>
  <summary>Details</summary>

**Motivation:** 乌尔都语推理评估受到机器翻译敏感性和更重视通用语言任务而不是推理基准的限制。本文旨在开发乌尔都语的理由基准。

**Method:** 本文提出了一种包含多人参与验证的上下文集成翻译框架，该框架利用多个翻译系统开发乌尔都语推理基准，同时保持上下文和结构的完整性。

**Result:** 研究发现多步和符号推理任务在乌尔都语中具有挑战性，稳定的语言对齐对强大的推理能力至关重要。

**Conclusion:** 本文建立了一个可扩展的方法用于乌尔都语标准推理评估，提供了关于多语言推理失败的实证洞察。此实验设置也广泛适用于其他低资源语言。

**Abstract:** Recent advances in large language models (LLMs) have led to strong reasoning capabilities; however, evaluating such models in low-resource languages remains challenging due to the lack of standardized benchmarks. In particular, Urdu reasoning evaluation has been limited by the sensitivity of machine translation and an emphasis on general language tasks rather than reasoning benchmarks. In this paper, we propose a contextually ensembled translation framework with human-in-the-loop validation that leverages multiple translation systems to develop Urdu reasoning benchmarks while preserving contextual and structural integrity. Using this framework, we translate widely adopted reasoning and question-answering benchmarks, including MGSM, MATH-500, CommonSenseQA, and OpenBookQA, into Urdu, collectively referred to as UrduBench, and conduct a comprehensive evaluation of both reasoning-oriented and instruction-tuned LLMs across multiple prompting strategies. Our analysis reveals performance differences across (1) four datasets, (2) five task difficulty levels, (3) diverse model architectures, (4) multiple model scaling settings, and (5) language consistency tests. We find that multi-step and symbolic reasoning tasks pose significant challenges in Urdu, and that stable language alignment is a critical prerequisite for robust reasoning. Overall, our work establishes a scalable methodology for standardized reasoning evaluation in Urdu and provides empirical insights into multilingual reasoning failures. This experimental setup is also broadly applicable to other low-resource languages. The code and datasets will be publicly released.

</details>


### [4] [Position-invariant Fine-tuning of Speech Enhancement Models with Self-supervised Speech Representations](https://arxiv.org/abs/2601.21084)
*Amit Meghanani,Thomas Hain*

Main category: cs.CL

> This paper investigates integrating front-end speech enhancement models with SSL-based models for noisy conditions, finding the soft-DTW-based approach to be superior in speed and performance, highlighting the necessity of position-invariant fine-tuning strategies.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the general limitation of self-supervised representation fine-tuning, particularly the problem of positional embeddings in SSL models.

**Method:** Two strategies are considered: (1) zero-padding, previously explored in SSL pre-training but here examined in the fine-tuning setting, and (2) speed perturbations with a soft-DTW loss.

**Result:** Experiments show that the soft-DTW-based approach achieves faster convergence and improved downstream performance.

**Conclusion:** The importance of position-invariant fine-tuning in SSL-based speech modelling is underscored by the results.

**Abstract:** Integrating front-end speech enhancement (SE) models with self-supervised learning (SSL)-based speech models is effective for downstream tasks in noisy conditions. SE models are commonly fine-tuned using SSL representations with mean squared error (MSE) loss between enhanced and clean speech. However, MSE is prone to exploiting positional embeddings in SSL models, allowing the objective to be minimised through positional correlations instead of content-related information. This work frames the problem as a general limitation of self-supervised representation fine-tuning and investigates it through representation-guided SE. Two strategies are considered: (1) zero-padding, previously explored in SSL pre-training but here examined in the fine-tuning setting, and (2) speed perturbations with a soft-DTW loss. Experiments show that the soft-DTW-based approach achieves faster convergence and improved downstream performance, underscoring the importance of position-invariant fine-tuning in SSL-based speech modelling.

</details>


### [5] [ChunkWise LoRA: Adaptive Sequence Partitioning for Memory-Efficient Low-Rank Adaptation and Accelerated LLM Inference](https://arxiv.org/abs/2601.21109)
*Ketan Thakkar,Maitreyi Chatterjee,Ramasubramanian Balasubramanian,Achyuthan Jootoo,Rajendra Ugrani*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Recent advances in low-rank adaptation (LoRA) have enabled efficient fine-tuning of large language models (LLMs) with minimal additional parameters. However, existing LoRA methods apply static rank configurations uniformly across all input tokens, ignoring variation in token complexity and computational requirements. In this work, we propose ChunkWise LoRA, a dynamic and adaptive approach that partitions sequences into variable-length chunks based on token complexity and assigns each chunk a tailored low-rank configuration. Our system introduces a runtime scheduler that estimates token difficulty, performs adaptive chunking, and selects per-chunk LoRA rank and scaling using a rank-ladder mechanism. To preserve output consistency, we further introduce a boundary-safe composition module and integrate policy-driven KV-cache strategies. Experiments on benchmark datasets such as Wikitext-103 and SQuAD demonstrate that ChunkWise LoRA achieves up to 34\% lower latency and 38% memory reduction compared to baseline LoRA, while maintaining or improving task performance metrics like BLEU, EM, and perplexity. The proposed framework remains fully compatible with existing transformer architectures and inference frameworks, providing a practical solution for real-world deployment of parameter-efficient LLMs.

</details>


### [6] [Multi-task Code LLMs: Data Mix or Model Merge?](https://arxiv.org/abs/2601.21115)
*Mingzhi Zhu,Boris Sobolev,Rahul Krishna,Raju Pavuluri,Stacy Patterson,Michele Merler*

Main category: cs.CL

> 研究比较了在小型多任务代码语言模型中，数据混合和模型合并两种方法的效果。实验结果显示，对于大模型，模型合并方法表现更好；而对于小模型，数据混合更为有效。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在探索在代理框架中部署小型专业化代码LLM时的高效多任务学习策略，同时也考虑到了性能、约束和成本之间的平衡。

**Method:** 本文探讨了两种创建小型多任务代码LLM的方法：数据混合和模型合并，并比较了它们在跨两个模型家族（Qwen Coder 和 DeepSeek Coder）的两个规模（2B和7B参数）上的表现，特别是在代码生成和代码总结任务上。

**Result:** 实验结果显示，对于较大的模型规模，模型合并方法在所有任务上表现出色，特别是Qwen Coder 2.5 7B模型在HumanEval基准测试中达到了92.7%的Pass@1，超过其任务特定单独微调模型的表现；而在较小规模的模型中，数据混合方法更为有效。

**Conclusion:** 研究结果表明，通过仔细的合并和混合策略，可以有效结合任务特定的能力，且不会有明显的性能下降，这使得它们在资源受限的部署场景中成为理想的选择。

**Abstract:** Recent research advocates deploying smaller, specialized code LLMs in agentic frameworks alongside frontier models, sparking interest in efficient strategies for multi-task learning that balance performance, constraints, and costs. We compare two approaches for creating small, multi-task code LLMs: data mixing versus model merging. We conduct extensive experiments across two model families (Qwen Coder and DeepSeek Coder) at two scales (2B and 7B parameters), fine-tuning them for code generation and code summarization tasks. Our evaluation on HumanEval, MBPP, and CodeXGlue benchmarks reveals that model merging achieves the best overall performance at larger scale across model families, retaining 96% of specialized model performance on code generation tasks while maintaining summarization capabilities. Notably, merged models can even surpass individually fine-tuned models, with our best configuration of Qwen Coder 2.5 7B model achieving 92.7% Pass@1 on HumanEval compared to 90.9% for its task-specific fine-tuned equivalent. At a smaller scale we find instead data mixing to be a preferred strategy. We further introduce a weight analysis technique to understand how different tasks affect model parameters and their implications for merging strategies. The results suggest that careful merging and mixing strategies can effectively combine task-specific capabilities without significant performance degradation, making them ideal for resource-constrained deployment scenarios.

</details>


### [7] [Large Language Models Naively Recover Ethnicity from Individual Records](https://arxiv.org/abs/2601.21132)
*Noah Dasanaike*

Main category: cs.CL

> 该研究展示了大型语言模型（LLM）能够仅通过名字推断出种族背景，其准确率超过贝叶斯改进姓名地理编码（BISG）技术，且不需要额外的训练数据。使用佛罗里达州和北卡罗来纳州选民名单的分层样本进行了测试，准确率高达84.7%，优于BISG（68.2%）。此外，这种方法还能减少收入偏见，并适用于多个国家和不同的分类标准，如宗教、种姓等。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于展示大型语言模型在无需额外训练数据的情况下，能否超越传统方法（如BISG）对种族背景进行分类的能力，并试图验证该方法在减少收入偏见和其他国家应用的可行性。

**Method:** 研究使用了六个不同的语言模型，包括Gemini 3 Flash, GPT-4o以及开源模型DeepSeek v3.2和GLM-4.7，对佛罗里达州和北卡罗来纳州选民名单的分层样本进行了种族分类的测试。为了增强推理能力，还测试了包括如党派注册等元数据的影响。

**Result:** 研究结果显示，LLM在预测种族背景上表现出色，准确率高达84.7%，并能有效减轻收入偏见。研究还通过其他国家的选民登记数据、印度立法者和土地记录等样本验证了这一方法的适用性，准确率分别为64.3%、99.2%和74.0%。

**Conclusion:** 结论指出，大型语言模型在种族分类上展现出了优于传统方法的广泛应用潜力，不仅是测试了美国的数据，在其他国家的数据上也能达到预期的准确率，而且还能降低收入引起的偏见。对于大规模应用来说，使用小规模变压器模型，并在此模型上进行微调可以超越BISG，同时成本低，便于本地部署。

**Abstract:** I demonstrate that large language models can infer ethnicity from names with accuracy exceeding that of Bayesian Improved Surname Geocoding (BISG) without additional training data, enabling inference outside the United States and to contextually appropriate classification categories. Using stratified samples from Florida and North Carolina voter files with self-reported race, LLM-based classification achieves up to 84.7% accuracy, outperforming BISG (68.2%) on balanced samples. I test six models including Gemini 3 Flash, GPT-4o, and open-source alternatives such as DeepSeek v3.2 and GLM-4.7. Enabling extended reasoning can improve accuracy by 1-3 percentage points, though effects vary across contexts; including metadata such as party registration reaches 86.7%. LLM classification also reduces the income bias inherent in BISG, where minorities in wealthier neighborhoods are systematically misclassified as White. I further validate using Lebanese voter registration with religious sect (64.3% accuracy), Indian MPs from reserved constituencies (99.2%), and Indian land records with caste classification (74.0%). Aggregate validation across India, Uganda, Nepal, Armenia, Chile, and Costa Rica using original full-count voter rolls demonstrates that the method recovers known population distributions where naming conventions are distinctive. For large-scale applications, small transformer models fine-tuned on LLM labels exceed BISG accuracy while enabling local deployment at no cost.

</details>


### [8] [EnsembleLink: Accurate Record Linkage Without Training Data](https://arxiv.org/abs/2601.21138)
*Noah Dasanaike*

Main category: cs.CL

> EnsembleLink is a record linkage method that achieves high accuracy without training labels by using pre-trained language models, which is efficient and runs locally.

<details>
  <summary>Details</summary>

**Motivation:** Record linkage is crucial for social science research, but existing methods have issues with accuracy or require extensive training data. There's a need for a robust, high-accuracy method that doesn't rely on labeled training data.

**Method:** Structure

**Result:** EnsembleLink outperforms or matches other methods on various benchmarks without the need for extensive labeling, and completes tasks efficiently.

**Conclusion:** EnsembleLink provides a significant advancement in record linkage by offering high accuracy, semantic understanding, and operational efficiency without requiring training labels.

**Abstract:** Record linkage, the process of matching records that refer to the same entity across datasets, is essential to empirical social science but remains methodologically underdeveloped. Researchers treat it as a preprocessing step, applying ad hoc rules without quantifying the uncertainty that linkage errors introduce into downstream analyses. Existing methods either achieve low accuracy or require substantial labeled training data. I present EnsembleLink, a method that achieves high accuracy without any training labels. EnsembleLink leverages pre-trained language models that have learned semantic relationships (e.g., that "South Ozone Park" is a neighborhood in "New York City" or that "Lutte ouvriere" refers to the Trotskyist "Workers' Struggle" party) from large text corpora. On benchmarks spanning city names, person names, organizations, multilingual political parties, and bibliographic records, EnsembleLink matches or exceeds methods requiring extensive labeling. The method runs locally on open-source models, requiring no external API calls, and completes typical linkage tasks in minutes.

</details>


### [9] [Output-Space Search: Targeting LLM Generations in a Frozen Encoder-Defined Output Space](https://arxiv.org/abs/2601.21169)
*Tobias Materzok*

Main category: cs.CL

> 该研究介绍了一种新的生成方法Output-Space Search（OS-Search），这种方法通过目标搜索方式提升了文本和代码生成的质量和多样性，并在目标函数优化上表现出了有效性。

<details>
  <summary>Details</summary>

**Motivation:** 该论文的动机是解决大型语言模型生成过程中存在的固有时序依赖性和低多样性的问题。OS-Search打破了传统的逐个生成token的方法，开辟了一个新的策略，即目标搜索。通过这种方式可以提升生成内容的质量和多样性。

**Method:** 我们介绍了一种名为输出空间搜索（Output-Space Search，简称OS-Search）的方法。该方法将大型语言模型（LLM）生成的过程转化为以目标点为终点的搜索。通过在一个冻结的编码器定义的三维输出空间Z中选择一个目标点z*，并使用一个基于检索的策略在标准自回归解码中生成接近z*坐标的输出。这种策略是在序列级强化学习（sequence-level RL）中训练的。这种方法能够使搜索做到平行化并在Z空间中进行黑箱优化，而且不需要依赖路径的逐个标记/程序搜索。

**Result:** 在故事生成方面，通过对Z（文本）空间的扫查，所得到的LLM评分的多样性比提示链接方法提高了3.1倍。在代码生成方面，通过对Z（代码）空间的贝叶斯优化，即使在匹配推理预算的情况下，也能提升一个从控制器中隐藏的目标函数。

**Conclusion:** 研究结果表明，相较于传统方法，Output-Space Search不仅在文本和代码生成中的多样性有了显著提升，而且在目标函数优化方面表现出了有效性和鲁棒性。这种方法对未来大型语言模型的生成以及优化具有重要的参考价值。

**Abstract:** We introduce Output-Space Search (OS-Search), which turns LLM generation into endpoint search. An outer loop selects a target z* in a frozen encoder-defined 3D output space Z, and a retrieval-grounded policy trained with sequence-level RL generates outputs whose coordinates land near z* under standard autoregressive decoding. This enables parallel sweeps and black-box optimization in Z without path-dependent token/program search. On stories, sweeping Z (text) yields 3.1x higher LLM-scored diversity than prompt-chaining. On code, Bayesian optimization over Z (code) improves an objective withheld from the controller under matched inference budgets while preserving validity.

</details>


### [10] [From Linear Input to Hierarchical Structure: Function Words as Statistical Cues for Language Learning](https://arxiv.org/abs/2601.21191)
*Xiulin Yang,Heidi Getz,Ethan Gotlieb Wilcox*

Main category: cs.CL

> 研究探讨了线性输入中哪些统计条件支持学习层次结构，特别是通过分析功能词的分布特性，发现高频次和可靠的语言结构关联性对于神经学习者掌握语言更为关键。

<details>
  <summary>Details</summary>

**Motivation:** 了解哪些统计条件能够支持从线性输入中学习层次结构，特别是探索功能词在语言习得中的作用。

**Method:** 跨语言语料分析，结合反事实语言建模和消除实验，研究不同语言变体的习得情况。

**Result:** 在186种语言中验证了功能词的三大特性普遍存在；揭示高频次和结构关联性对语言习得更重要；不同的学习条件会导致对功能词的依赖程度系统性不同。

**Conclusion:** 功能词在语言习得中起到关键作用，但不同内部机制可导致相同习得表现。

**Abstract:** What statistical conditions support learning hierarchical structure from linear input? In this paper, we address this question by focusing on the statistical distribution of function words. Function words have long been argued to play a crucial role in language acquisition due to their distinctive distributional properties, including high frequency, reliable association with syntactic structure, and alignment with phrase boundaries. We use cross-linguistic corpus analysis to first establish that all three properties are present across 186 studied languages. Next, we use a combination of counterfactual language modeling and ablation experiments to show that language variants preserving all three properties are more easily acquired by neural learners, with frequency and structural association contributing more strongly than boundary alignment. Follow-up probing and ablation analyses further reveal that different learning conditions lead to systematically different reliance on function words, indicating that similar performance can arise from distinct internal mechanisms.

</details>


### [11] [Scaling Embeddings Outperforms Scaling Experts in Language Models](https://arxiv.org/abs/2601.21204)
*Hong Liu,Jiaqi Zhang,Chao Wang,Xing Hu,Linkun Lyu,Jiaqi Sun,Xurui Yang,Bo Wang,Fengcun Li,Yulei Qian,Lingtong Si,Yerui Sun,Rumei Li,Peng Pei,Yuchen Xie,Xunliang Cai*

Main category: cs.CL

> 本文研究了嵌入扩展在大型语言模型中的应用，提出了一种有效方法以解决现有模型的性能瓶颈问题，并展示了LongCat-Flash-Lite模型的优越性能。

<details>
  <summary>Details</summary>

**Motivation:** 随着专家混合架构成为大型语言模型稀疏性扩展的标准，它们逐渐面临收益递减和系统级瓶颈问题。

**Method:** 通过综合分析和实验，本文探讨了嵌入扩展作为大型语言模型稀疏性扩展的强大且正交维度。

**Result:** 研究系统地阐述了影响这种方法有效性的关键架构因素，并通过集成定制的系统优化和推测解码，成功将该稀疏性转化为实际推理加速。

**Conclusion:** 指导这些见解，本文介绍了LongCat-Flash-Lite，一个685亿参数的模型，它不仅超越了参数等效的MoE基准模型，还在代理和编码领域表现出色。

**Abstract:** While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.

</details>


### [12] [Multilingual Dysarthric Speech Assessment Using Universal Phone Recognition and Language-Specific Phonemic Contrast Modeling](https://arxiv.org/abs/2601.21205)
*Eunjung Yeo,Julie M. Liss,Visar Berisha,David R. Mortensen*

Main category: cs.CL

> A multilingual framework for assessing speech intelligibility, combining universal and language-specific phonological assessments, is developed and tested across multiple languages, demonstrating its clinical significance in measuring dysarthria-related speech degradation.

<details>
  <summary>Details</summary>

**Motivation:** The growing prevalence of neurological disorders associated with dysarthria drives the necessity for automated intelligibility assessment methods that can be applied across languages, as most existing methods are either language-specific or do not account for language-specific intelligibility factors.

**Method:** We present a multilingual phoneme-production assessment framework that integrates universal phone recognition with language-specific phoneme interpretation through contrastive phonological feature distances for phone-to-phoneme mapping and sequence alignment.

**Result:** The analysis conducted on English, Spanish, Italian, and Tamil demonstrates that the proposed framework can generate three metrics: phoneme error rate (PER), phonological feature error rate (PFER), and a new measure called phoneme coverage (PhonCov). PER benefits from both mapping and alignment, PFER is enhanced by alignment alone, and PhonCov is improved through mapping.

**Conclusion:** The framework captures clinically significant patterns of intelligibility decline that align with established observations of dysarthric speech, showcasing its potential as a comprehensive tool for assessing speech intelligibility in multiple languages.

**Abstract:** The growing prevalence of neurological disorders associated with dysarthria motivates the need for automated intelligibility assessment methods that are applicalbe across languages. However, most existing approaches are either limited to a single language or fail to capture language-specific factors shaping intelligibility. We present a multilingual phoneme-production assessment framework that integrates universal phone recognition with language-specific phoneme interpretation using contrastive phonological feature distances for phone-to-phoneme mapping and sequence alignment. The framework yields three metrics: phoneme error rate (PER), phonological feature error rate (PFER), and a newly proposed alignment-free measure, phoneme coverage (PhonCov). Analysis on English, Spanish, Italian, and Tamil show that PER benefits from the combination of mapping and alignment, PFER from alignment alone, and PhonCov from mapping. Further analyses demonstrate that the proposed framework captures clinically meaningful patterns of intelligibility degradation consistent with established observations of dysarthric speech.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [13] [MA-LipNet: Multi-Dimensional Attention Networks for Robust Lipreading](https://arxiv.org/abs/2601.20881)
*Matteo Rossi*

Main category: cs.CV

> 本文提出了一种新的唇读技术MA-LipNet，通过三个注意力模块处理视频特征，显著提高了唇读的准确率。

<details>
  <summary>Details</summary>

**Motivation:** 由于口型和肢体语言的微妙性，现有的唇读技术经常面临特征区分度低和泛化能力差的问题。本文旨在解决这些挑战。

**Method:** 提出了一种称为Multi-Attention Lipreading Network (MA-LipNet) 的新技术，主要通过顺序应用三个注意力模块处理视频特征，这包括Channel Attention、Joint Spatial-Temporal Attention 和 Separate Spatial-Temporal Attention。

**Result:** MA-LipNet在CMLR和GRID数据集上的实验显示，其显著降低了字符错误率和单词错误率，证明了其效果和优越性。

**Conclusion:** 这项工作强调了多维度特征优化在鲁棒的视觉语音识别中的重要性。

**Abstract:** Lipreading, the technology of decoding spoken content from silent videos of lip movements, holds significant application value in fields such as public security. However, due to the subtle nature of articulatory gestures, existing lipreading methods often suffer from limited feature discriminability and poor generalization capabilities. To address these challenges, this paper delves into the purification of visual features from temporal, spatial, and channel dimensions. We propose a novel method named Multi-Attention Lipreading Network(MA-LipNet). The core of MA-LipNet lies in its sequential application of three dedicated attention modules. Firstly, a \textit{Channel Attention (CA)} module is employed to adaptively recalibrate channel-wise features, thereby mitigating interference from less informative channels. Subsequently, two spatio-temporal attention modules with distinct granularities-\textit{Joint Spatial-Temporal Attention (JSTA)} and \textit{Separate Spatial-Temporal Attention (SSTA)}-are leveraged to suppress the influence of irrelevant pixels and video frames. The JSTA module performs a coarse-grained filtering by computing a unified weight map across the spatio-temporal dimensions, while the SSTA module conducts a more fine-grained refinement by separately modeling temporal and spatial attentions. Extensive experiments conducted on the CMLR and GRID datasets demonstrate that MA-LipNet significantly reduces the Character Error Rate (CER) and Word Error Rate (WER), validating its effectiveness and superiority over several state-of-the-art methods. Our work highlights the importance of multi-dimensional feature refinement for robust visual speech recognition.

</details>


### [14] [Non-Markov Multi-Round Conversational Image Generation with History-Conditioned MLLMs](https://arxiv.org/abs/2601.20911)
*Haochen Zhang,Animesh Sinha,Felix Juefei-Xu,Haoyu Ma,Kunpeng Li,Zhipeng Fan,Meng Dong,Xiaoliang Dai,Tingbo Hou,Peizhao Zhang,Zecheng He*

Main category: cs.CV

> 研究开发了非马尔可夫多轮图像生成的技术，提高了图像生成模型在多轮对话中的性能。

<details>
  <summary>Details</summary>

**Motivation:** 当前大多数多轮基准测试和训练方法都是马尔可夫式的，忽视了长时间的历史信息。本研究针对用户可引用早期状态、撤销更改或引用多轮前引入的实体的更具有挑战性的非马尔可夫环境进行了探索。

**Method:** 本研究提出了处理非马尔可夫多轮互动的方法，包括非马尔可夫多轮数据构造策略（如回滚样式编辑以回溯早期视觉状态和基于名字的多轮个性化策略），一种基于历史的训练和推理框架以及缓存机制来防止多轮身份漂移，以及高保真图像重构和可编辑个性化的方法改进。

**Result:** 研究表明，针对非马尔可夫互动进行明确训练，可以大幅提高多轮一致性和指令遵循性，同时保持强大的单轮编辑和个性化能力。

**Conclusion:** 本论文证明通过专门为非马尔可夫互动设计的模型能够显著提高多轮图像生成任务中的精度和一致性。

**Abstract:** Conversational image generation requires a model to follow user instructions across multiple rounds of interaction, grounded in interleaved text and images that accumulate as chat history. While recent multimodal large language models (MLLMs) can generate and edit images, most existing multi-turn benchmarks and training recipes are effectively Markov: the next output depends primarily on the most recent image, enabling shortcut solutions that ignore long-range history. In this work we formalize and target the more challenging non-Markov setting, where a user may refer back to earlier states, undo changes, or reference entities introduced several rounds ago. We present (i) non-Markov multi-round data construction strategies, including rollback-style editing that forces retrieval of earlier visual states and name-based multi-round personalization that binds names to appearances across rounds; (ii) a history-conditioned training and inference framework with token-level caching to prevent multi-round identity drift; and (iii) enabling improvements for high-fidelity image reconstruction and editable personalization, including a reconstruction-based DiT detokenizer and a multi-stage fine-tuning curriculum. We demonstrate that explicitly training for non-Markov interactions yields substantial improvements in multi-round consistency and instruction compliance, while maintaining strong single-round editing and personalization.

</details>


### [15] [Text controllable PET denoising](https://arxiv.org/abs/2601.20990)
*Xuehua Ye,Hongxu Yang,Adam J. Schwarz*

Main category: cs.CV

> A text-guided denoising method for enhancing PET images by utilizing features from a pretrained CLIP model with a U-Net denoising model, showing significant improvements in image quality.

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of complicated noise in PET images that can obscure critical diagnostic information.

**Method:** The paper proposes a text-guided denoising method utilizing features from a pretrained CLIP model with a U-Net based denoising model to enhance PET images.

**Result:** Experimental results demonstrate significant improvements in both qualitative and quantitative assessments of the denoised PET images.

**Conclusion:** The proposed model's flexibility shows potential for handling more complex denoising demands or reducing the acquisition time.

**Abstract:** Positron Emission Tomography (PET) imaging is a vital tool in medical diagnostics, offering detailed insights into molecular processes within the human body. However, PET images often suffer from complicated noise, which can obscure critical diagnostic information. The quality of the PET image is impacted by various factors including scanner hardware, image reconstruction, tracer properties, dose/count level, and acquisition time. In this study, we propose a novel text-guided denoising method capable of enhancing PET images across a wide range of count levels within a single model. The model utilized the features from a pretrained CLIP model with a U-Net based denoising model. Experimental results demonstrate that the proposed model leads significant improvements in both qualitative and quantitative assessments. The flexibility of the model shows the potential for helping more complicated denoising demands or reducing the acquisition time.

</details>


### [16] [Low performing pixel correction in computed tomography with unrolled network and synthetic data training](https://arxiv.org/abs/2601.20995)
*Hongxu Yang,Levente Lippenszky,Edina Timko,Lehel Ferenczi,Gopal Avinash*

Main category: cs.CV

> 研究提出了一种无需真实世界训练数据的双域法，利用合成数据有效校正CT中的LPP伪影。

<details>
  <summary>Details</summary>

**Motivation:** 现有的LPP纠正方法需要昂贵的特定数据集，且只关注图像域或投影域，忽视CT几何中的内在相关性。

**Method:** 采用基于合成数据的无卷展双域方法，利用合成数据中的LPP内在相关性来纠正伪影。

**Result:** 本研究旨在解决CT成像中低性能像素(LPP)导致的环状和条纹伪影问题。已有的解决方法主要集中在图像域或投影域校正，这些方法依赖于昂贵的训练数据集，并忽略CT几何学中的内在相关性。研究人员提出了一种基于合成数据的无卷展双域方法来校正LPP伪影，该方法不需使用真实临床数据，通过自然图像生成的合成数据来训练模型。实验表明，在模拟1-2%中心附近探测器缺陷的情况下，该方法显著优于现有技术。研究结果表明，我们的解决方案不仅消除了模型训练数据收集的成本，还可以适应不同的扫描仪设置。

**Conclusion:** 研究提出的方法可以有效且低成本地解决因CT探测器的LPP导致的伪影问题，具有广泛的应用潜力。

**Abstract:** Low performance pixels (LPP) in Computed Tomography (CT) detectors would lead to ring and streak artifacts in the reconstructed images, making them clinically unusable. In recent years, several solutions have been proposed to correct LPP artifacts, either in the image domain or in the sinogram domain using supervised deep learning methods. However, these methods require dedicated datasets for training, which are expensive to collect. Moreover, existing approaches focus solely either on image-space or sinogram-space correction, ignoring the intrinsic correlations from the forward operation of the CT geometry. In this work, we propose an unrolled dual-domain method based on synthetic data to correct LPP artifacts. Specifically, the intrinsic correlations of LPP between the sinogram and image domains are leveraged through synthetic data generated from natural images, enabling the trained model to correct artifacts without requiring any real-world clinical data. In experiments simulating 1-2% detectors defect near the isocenter, the proposed method outperformed the state-of-the-art approaches by a large margin. The results indicate that our solution can correct LPP artifacts without the cost of data collection for model training, and it is adaptable to different scanner settings for software-based applications.

</details>


### [17] [AI-based Prediction of Biochemical Recurrence from Biopsy and Prostatectomy Samples](https://arxiv.org/abs/2601.21022)
*Andrea Camilloni,Chiara Micoli,Nita Mulliqi,Erik Everett Palm,Thorgerdur Palsdottir,Kelvin Szolnoky,Xiaoyi Ji,Sol Erika Boman,Andrea Discacciati,Henrik Grönberg,Lars Egevad,Tobias Nordström,Kimmo Kartasalo,Martin Eklund*

Main category: cs.CV

> 

<details>
  <summary>Details</summary>

**Motivation:** 

**Method:** 

**Result:** {
  "tldr": "使用基于AI的模型在诊断性前列腺活检切片上预测生化复发风险，并在多个队列中验证了其有效性和泛化能力，增加了临床变量后提升了预后价值，优于传统的CAPRA-S评分系统。",
  "motivation": "当前预测前列腺癌生化复发的工具精度不高，迫切需要一种更精准的方法来帮助治疗决策。",
  "method": "在STHLM3队列的活检切片上训练了一种基于AI的模型，使用了基础模型和注意力机制的多实例学习，然后在其他三个前列腺切除队列上测试了该模型的泛化能力。",
  "result": "该图像基方法在三个外部队列上分别达成了0.64、0.70和0.70的五年时间依赖AUC，集成临床变量提升了预后价值，并能够显著地进行风险分层。模型优于传统的CAPRA-S评分系统。",
  "conclusion": "活检训练的组织病理学AI模型在不同类型样本间可以泛化，支持术前和术后的决策制定，不过未来研究需要对比AI多模式方法和简单预测模型的附加值。",
  "function": "Structure", 
  "args": {"content": ""}}

**Conclusion:** 

**Abstract:** Biochemical recurrence (BCR) after radical prostatectomy (RP) is a surrogate marker for aggressive prostate cancer with adverse outcomes, yet current prognostic tools remain imprecise. We trained an AI-based model on diagnostic prostate biopsy slides from the STHLM3 cohort (n = 676) to predict patient-specific risk of BCR, using foundation models and attention-based multiple instance learning. Generalizability was assessed across three external RP cohorts: LEOPARD (n = 508), CHIMERA (n = 95), and TCGA-PRAD (n = 379). The image-based approach achieved 5-year time-dependent AUCs of 0.64, 0.70, and 0.70, respectively. Integrating clinical variables added complementary prognostic value and enabled statistically significant risk stratification. Compared with guideline-based CAPRA-S, AI incrementally improved postoperative prognostication. These findings suggest biopsy-trained histopathology AI can generalize across specimen types to support preoperative and postoperative decision making, but the added value of AI-based multimodal approaches over simpler predictive models should be critically scrutinized in further studies.

</details>


### [18] [BadDet+: Robust Backdoor Attacks for Object Detection](https://arxiv.org/abs/2601.21066)
*Kealan Dunnett,Reza Arablouei,Dimity Miller,Volkan Dedeoglu,Raja Jurdak*

Main category: cs.CV

> 研究提出了一种名为BadDet+的后门攻击方法，它能在物体检测中表现出位置和尺度不变性，并增强了对现实世界环境中的物理鲁棒性。这一研究揭示了物体检测中的显著漏洞，并强调了需要专门的防御措施。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于检测的后门攻击方法存在依赖不现实假设和缺乏物理验证的问题。研究旨在解决这些问题，并更好地理解后门攻击对物体检测的影响。

**Method:** BadDet+，一个基于惩罚的方法，它统一了区域误分类攻击（RMA）和对象消失攻击（ODA）。通过使用对数障碍惩罚来抑制触发输入的真实类预测，实现了位置和尺度不变性以及增强了物理健壮性。

**Result:** 与现有的RMA和ODA基线相比，BadDet+在转化为实际世界的基准上表现出更优的合成到物理的转移，并且保持了干净的性能。此外，从理论上证实，该惩罚方法在一个特定的触发特征子空间中工作，可靠地引发攻击而不会破坏标准推理。

**Conclusion:** 研究突显了对象检测中显著的漏洞，并强调了需要专门的防御措施。

**Abstract:** Backdoor attacks pose a severe threat to deep learning, yet their impact on object detection remains poorly understood compared to image classification. While attacks have been proposed, we identify critical weaknesses in existing detection-based methods, specifically their reliance on unrealistic assumptions and a lack of physical validation. To bridge this gap, we introduce BadDet+, a penalty-based framework that unifies Region Misclassification Attacks (RMA) and Object Disappearance Attacks (ODA). The core mechanism utilizes a log-barrier penalty to suppress true-class predictions for triggered inputs, resulting in (i) position and scale invariance, and (ii) enhanced physical robustness. On real-world benchmarks, BadDet+ achieves superior synthetic-to-physical transfer compared to existing RMA and ODA baselines while preserving clean performance. Theoretical analysis confirms the proposed penalty acts within a trigger-specific feature subspace, reliably inducing attacks without degrading standard inference. These results highlight significant vulnerabilities in object detection and the necessity for specialized defenses.

</details>


### [19] [Towards Mitigating Modality Bias in Vision-Language Models for Temporal Action Localization](https://arxiv.org/abs/2601.21078)
*Jiaqi Li,Guangming Wang,Shuntian Zheng,Minzhe Ni,Xiaoman Lu,Guanghui Ye,Yu Guan*

Main category: cs.CV

> ActionVLM旨在解决时间动作定位中的模态偏见问题，通过保持视觉为主，语言辅助的方法，提高了模型的表现能力，在THUMOS14数据集上mAP提升了最高3.2%。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法倾向于过度强调语言先验，从而影响了视觉的表现效果。为解决这个问题，ActionVLM旨在保持视觉作为主要信号的同时，自适应地利用语言，从而有效解决模态偏差的问题。

**Method:** ActionVLM提出了一种视觉-语言聚合框架，通过引入一个去偏重加权模块来估算语言优势（即语言相对于仅使用视觉的预测的增量效益），并动态地重新加权语言模态。此外，还采用了一种残差聚合策略，将语言视为一种补充性优化，而非主要驱动力。

**Result:** 在THUMOS14数据集上的实验结果表明，ActionVLM相比于现有最佳技术，mAP提升了最高3.2%。

**Conclusion:** 实验表明，ActionVLM能够有效减轻模态偏差，减少对语言先验的信任，提升在时间动作定位上表现，且在THUMOS14数据集上达到了新的最佳性能。

**Abstract:** Temporal Action Localization (TAL) requires identifying both the boundaries and categories of actions in untrimmed videos. While vision-language models (VLMs) offer rich semantics to complement visual evidence, existing approaches tend to overemphasize linguistic priors at the expense of visual performance, leading to a pronounced modality bias. We propose ActionVLM, a vision-language aggregation framework that systematically mitigates modality bias in TAL. Our key insight is to preserve vision as the dominant signal while adaptively exploiting language only when beneficial. To this end, we introduce (i) a debiasing reweighting module that estimates the language advantage-the incremental benefit of language over vision-only predictions-and dynamically reweights language modality accordingly, and (ii) a residual aggregation strategy that treats language as a complementary refinement rather than the primary driver. This combination alleviates modality bias, reduces overconfidence from linguistic priors, and strengthens temporal reasoning. Experiments on THUMOS14 show that our model outperforms state-of-the-art by up to 3.2% mAP.

</details>


### [20] [Shape of Thought: Progressive Object Assembly via Visual Chain-of-Thought](https://arxiv.org/abs/2601.21081)
*Yu Huo,Siyu Zhang,Kun Zeng,Haoyue Liu,Owen Lee,Junlin Chen,Yuquan Lu,Yifu Guo,Yaodong Liang,Xiaoying Tang*

Main category: cs.CV

> 文章提出了一种新框架Shape-of-Thought，用于文本到图像生成中的透明、过程监督的组合生成，并引入了相关数据集和基准测试。

<details>
  <summary>Details</summary>

**Motivation:** 多模态文本到图像生成模型在视觉保真度方面表现很强，但在组合结构约束（尤其是生成数字能力、属性绑定和部件级关系）下仍显脆弱。该论文旨在解决这些挑战。

**Method:** 提出了一种名为Shape-of-Thought (SoT)的视觉CoT框架，该框架能够在没有外部引擎的情况下通过连贯的2D投影逐步组装形状。SoT训练了一个统一的多模态自回归模型，以生成交错的文本计划和渲染的中间状态，帮助模型捕捉形状组装逻辑，而无需生成显式的几何表示。

**Result:** 在SoT-26K上微调后，组件计数的精度达到88.4%，结构拓扑的精度达到84.8%，超过了仅基于文本的基线模型约20%。

**Conclusion:** SoT建立了一种新的透明、过程监管的组合生成范式，代码可在指定链接获得，数据集将在接受后公开。

**Abstract:** Multimodal models for text-to-image generation have achieved strong visual fidelity, yet they remain brittle under compositional structural constraints-notably generative numeracy, attribute binding, and part-level relations. To address these challenges, we propose Shape-of-Thought (SoT), a visual CoT framework that enables progressive shape assembly via coherent 2D projections without external engines at inference time. SoT trains a unified multimodal autoregressive model to generate interleaved textual plans and rendered intermediate states, helping the model capture shape-assembly logic without producing explicit geometric representations. To support this paradigm, we introduce SoT-26K, a large-scale dataset of grounded assembly traces derived from part-based CAD hierarchies, and T2S-CompBench, a benchmark for evaluating structural integrity and trace faithfulness. Fine-tuning on SoT-26K achieves 88.4% on component numeracy and 84.8% on structural topology, outperforming text-only baselines by around 20%. SoT establishes a new paradigm for transparent, process-supervised compositional generation. The code is available at https://anonymous.4open.science/r/16FE/. The SoT-26K dataset will be released upon acceptance.

</details>


### [21] [An AI Framework for Microanastomosis Motion Assessment](https://arxiv.org/abs/2601.21120)
*Yan Meng,Eduardo J. Torres-Rodríguez,Marcelle Altshuler,Nishanth Gowda,Arhum Naeem,Recai Yilmaz,Omar Arnaout,Daniel A. Donoho*

Main category: cs.CV

> 本文提出一种用于自动评估显微外科手术中吻合手术器械操作技能的AI系统，该系统整合了四个核心模块，实验结果表明该框架具有较高精度。

<details>
  <summary>Details</summary>

**Motivation:** 由于显微外科手术需要极其高精度和精细的技巧，因此对高效、标准化的评估方法需求迫切。以往的评估方法依赖于专家主观判断，存在评分者间差异、缺乏标准化评估标准、易受认知偏见影响等弊端。本文旨在解决这些问题，提出准确、可靠、可自动化的评估系统。

**Method:** 本论文提出了一种用于自动评估显微吻合手术器械操作技能的AI框架。该系统整合了四个核心组件：基于You Only Look Once (YOLO)架构的器械检测模块、基于Deep Simple Online and Realtime Tracking (DeepSORT)开发的器械跟踪模块、使用形状描述符的器械尖端定位模块以及基于专家标注数据训练的监督分类模块。

**Result:** 该AI框架的实验结果表明其有效性，实现了97%的器械检测精度，平均精度均值(mAP)为96%，通过50%到95%的Intersection over Union (IoU)阈值进行测量。

**Conclusion:** 结果表明，提出的AI框架有效，能够以一致性和可扩展性评估显微吻合手术的器械操作技能。

**Abstract:** Proficiency in microanastomosis is a fundamental competency across multiple microsurgical disciplines. These procedures demand exceptional precision and refined technical skills, making effective, standardized assessment methods essential. Traditionally, the evaluation of microsurgical techniques has relied heavily on the subjective judgment of expert raters. They are inherently constrained by limitations such as inter-rater variability, lack of standardized evaluation criteria, susceptibility to cognitive bias, and the time-intensive nature of manual review. These shortcomings underscore the urgent need for an objective, reliable, and automated system capable of assessing microsurgical performance with consistency and scalability. To bridge this gap, we propose a novel AI framework for the automated assessment of microanastomosis instrument handling skills. The system integrates four core components: (1) an instrument detection module based on the You Only Look Once (YOLO) architecture; (2) an instrument tracking module developed from Deep Simple Online and Realtime Tracking (DeepSORT); (3) an instrument tip localization module employing shape descriptors; and (4) a supervised classification module trained on expert-labeled data to evaluate instrument handling proficiency. Experimental results demonstrate the effectiveness of the framework, achieving an instrument detection precision of 97%, with a mean Average Precision (mAP) of 96%, measured by Intersection over Union (IoU) thresholds ranging from 50% to 95% (mAP50-95).

</details>


### [22] [Bidirectional Cross-Perception for Open-Vocabulary Semantic Segmentation in Remote Sensing Imagery](https://arxiv.org/abs/2601.21159)
*Jianzheng Wang,Huan Ni*

Main category: cs.CV

> 提出了一种用于无训练开放词汇语义分割的SDCI方法，通过多个模块优化遥感图像的语义分割性能，实验表明其优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的无训练开放词汇语义分割（OVSS）方法难以满足高分辨率遥感图像对几何定位和语义预测的高要求，因此提出了SDCI方法以解决这一问题。

**Method:** SDCI方法基于空间正则化的双分支协作推理框架，包括交叉模型注意力融合模块（CAF）进行特征编码阶段的合作推理，双向交叉图扩散优化模块（BCDR）通过迭代随机游走扩散增强分割得分的可靠性，并采用基于凸优化的超级像素协作预测机制（CSCP）进一步优化物体边界。

**Result:** 实验表明，SDCI方法在多个遥感语义分割基准上比现有的无训练开放词汇语义分割方法性能更好，并通过对传统基于对象的遥感图像分析方法的有效性的进一步确认，验证了其性能。

**Conclusion:** SDCI方法通过综合多种机制，显著提高了无训练开放词汇遥感图像语义分割的性能。此外，基于超级像素结构的传统遥感图像分析方法在深度学习框架中仍然有效。

**Abstract:** High-resolution remote sensing imagery is characterized by densely distributed land-cover objects and complex boundaries, which places higher demands on both geometric localization and semantic prediction. Existing training-free open-vocabulary semantic segmentation (OVSS) methods typically fuse CLIP and vision foundation models (VFMs) using "one-way injection" and "shallow post-processing" strategies, making it difficult to satisfy these requirements. To address this issue, we propose a spatial-regularization-aware dual-branch collaborative inference framework for training-free OVSS, termed SDCI. First, during feature encoding, SDCI introduces a cross-model attention fusion (CAF) module, which guides collaborative inference by injecting self-attention maps into each other. Second, we propose a bidirectional cross-graph diffusion refinement (BCDR) module that enhances the reliability of dual-branch segmentation scores through iterative random-walk diffusion. Finally, we incorporate low-level superpixel structures and develop a convex-optimization-based superpixel collaborative prediction (CSCP) mechanism to further refine object boundaries. Experiments on multiple remote sensing semantic segmentation benchmarks demonstrate that our method achieves better performance than existing approaches. Moreover, ablation studies further confirm that traditional object-based remote sensing image analysis methods leveraging superpixel structures remain effective within deep learning frameworks. Code: https://github.com/yu-ni1989/SDCI.

</details>


### [23] [Enhancing Underwater Light Field Images via Global Geometry-aware Diffusion Process](https://arxiv.org/abs/2601.21179)
*Yuji Lin,Qian Zhao,Zongsheng Yue,Junhui Hou,Deyu Meng*

Main category: cs.CV

> 本研究提出了一种名为GeoDiff-LF的新框架，通过扩散模型和光场几何结构相结合以提升水下4-D光场图像的质量，有效减轻了水下场景中的色彩失真，显著提升了图像处理的视觉和定量性能。

<details>
  <summary>Details</summary>

**Motivation:** 旨在通过4-D光场成像获取高质量的水下图像，解决水下图像质量差的问题。

**Method:** 提出了一种基于扩散模型的框架GeoDiff-LF，以增强4-D光场成像的质量。该框架有三个关键改进：1）修改的U-Net架构，包含卷积和注意力适配器，以建模几何线索；2）使用张量分解和渐进加权的几何引导损失函数，以正则化全局结构；3）优化的采样策略，包含噪声预测，以提高效率。

**Result:** 实验表明，该框架在视觉保真度和定量性能上均优于现有方法，推动了水下图像增强技术的最新进展。

**Conclusion:** GeoDiff-LF框架通过结合扩散先验和光场几何，有效地减少了水下图像中的色彩失真，超越了现有方法的表现。

**Abstract:** This work studies the challenging problem of acquiring high-quality underwater images via 4-D light field (LF) imaging. To this end, we propose GeoDiff-LF, a novel diffusion-based framework built upon SD-Turbo to enhance underwater 4-D LF imaging by leveraging its spatial-angular structure. GeoDiff-LF consists of three key adaptations: (1) a modified U-Net architecture with convolutional and attention adapters to model geometric cues, (2) a geometry-guided loss function using tensor decomposition and progressive weighting to regularize global structure, and (3) an optimized sampling strategy with noise prediction to improve efficiency. By integrating diffusion priors and LF geometry, GeoDiff-LF effectively mitigates color distortion in underwater scenes. Extensive experiments demonstrate that our framework outperforms existing methods across both visual fidelity and quantitative performance, advancing the state-of-the-art in enhancing underwater imaging. The code will be publicly available at https://github.com/linlos1234/GeoDiff-LF.

</details>


### [24] [FRISM: Fine-Grained Reasoning Injection via Subspace-Level Model Merging for Vision-Language Models](https://arxiv.org/abs/2601.21187)
*Chenyu Huang,Peng Ye,Xudong Tan,Jinhan Mu,Shenghe Zheng,Li Shen,Tao Chen*

Main category: cs.CV

> The paper introduces FRISM, a framework that uses SVD on Large Reasoning Models and adaptive tuning of subspaces to enhance the reasoning capabilities of Vision-Language Models without affecting their visual performance. It achieves state-of-the-art results across multiple benchmarks.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to solve the problem that current methods can only operate at a coarse-grained layer level, leading to a trade-off between enhancing reasoning capabilities and preserving visual capabilities of Vision-Language Models.

**Method:** FRISM (Fine-grained Reasoning Injection via Subspace-level model Merging) is proposed to enhance the reasoning capabilities of Vision-Language Models by decomposing Large Reasoning Models task vectors through SVD and tuning the scaling coefficients of each subspace adaptively. A label-free self-distillation learning strategy is also introduced.

**Result:** FRISM consistently achieves state-of-the-art performance across various visual reasoning benchmarks, effectively enhancing reasoning capabilities without sacrificing the model's original visual capabilities.

**Conclusion:** The study concludes that FRISM provides an effective and fine-grained method to merge Large Reasoning Models into Vision-Language Models, significantly improving their reasoning performance while maintaining high visual recognition accuracy.

**Abstract:** Efficiently enhancing the reasoning capabilities of Vision-Language Models (VLMs) by merging them with Large Reasoning Models (LRMs) has emerged as a promising direction. However, existing methods typically operate at a coarse-grained layer level, which often leads to a trade-off between injecting reasoning capabilities and preserving visual capabilities. To address this limitation, we propose {FRISM} (Fine-grained Reasoning Injection via Subspace-level model Merging), a fine-grained reasoning injection framework based on subspace-level model merging. Observing that reasoning capabilities are encoded in distinct subspaces, FRISM decomposes LRM task vectors via Singular Value Decomposition (SVD) and adaptively tunes the scaling coefficients of each subspace through learning to realize fine-grained reasoning injection. Furthermore, we introduce a label-free self-distillation learning strategy with a dual-objective optimization using common vision-language perception datasets. Extensive experiments demonstrate that FRISM effectively improves reasoning capabilities without compromising the model's original visual capabilities by consistently achieving state-of-the-art performance across diverse visual reasoning benchmarks.

</details>


### [25] [Generative Recall, Dense Reranking: Learning Multi-View Semantic IDs for Efficient Text-to-Video Retrieval](https://arxiv.org/abs/2601.21193)
*Zecheng Zhao,Zhi Chen,Zi Huang,Shazia Sadiq,Tong Chen*

Main category: cs.CV

> 提出GRDR方法解决生成式检索中的语义歧义和跨模态偏差问题，提高视频召回质量，同时大幅度减少了资源消耗。

<details>
  <summary>Details</summary>

**Motivation:** 传统的基于稠密检索的技术在计算和存储上会随着语料库的增大而增大，不适用于大规模实时应用程序。尽管生成式检索（GR）提供近乎恒定的推理和存储复杂度，且能够快速排除无关候选视频，但其因语义歧义和跨模态偏差问题影响了召回质量。因此，需要一种新方法来提高召回质量并保持高效率。

**Method:** 提出了一种名为GRDR（生成式召回和稠密重排序）的新方法，该方法通过使用查询引导的多视角编码器为每个视频分配多个语义ID，并通过共享代码本联合训练编码器和生成式检索器，以此作为文本和视频之间的语义桥梁。在推理阶段，通过字典树约束的解码生成紧凑的候选集，并通过稠密模型进行细粒度匹配以进行重排序。

**Result:** 实验结果表明，在不同的TVR基准数据集上，GRDR在准确性方面与强大的稠密检索方法相当，同时将索引存储减少了10倍，并且提高了300倍的全语料库检索速度。

**Conclusion:** GRDR方法通过引入多视角语义标识符和联合训练机制，有效解决了传统生成式检索中的语义歧义和跨模态偏差问题，从而显著提升了召回阶段的候选视频质量，并在保持高精度的同时大幅减少了计算资源和存储资源的消耗。

**Abstract:** Text-to-Video Retrieval (TVR) is essential in video platforms. Dense retrieval with dual-modality encoders leads in accuracy, but its computation and storage scale poorly with corpus size. Thus, real-time large-scale applications adopt two-stage retrieval, where a fast recall model gathers a small candidate pool, which is reranked by an advanced dense retriever. Due to hugely reduced candidates, the reranking model can use any off-the-shelf dense retriever without hurting efficiency, meaning the recall model bounds two-stage TVR performance. Recently, generative retrieval (GR) replaces dense video embeddings with discrete semantic IDs and retrieves by decoding text queries into ID tokens. GR offers near-constant inference and storage complexity, and its semantic IDs capture high-level video features via quantization, making it ideal for quickly eliminating irrelevant candidates during recall. However, as a recall model in two-stage TVR, GR suffers from (i) semantic ambiguity, where each video satisfies diverse queries but is forced into one semantic ID; and (ii) cross-modal misalignment, as semantic IDs are solely derived from visual features without text supervision. We propose Generative Recall and Dense Reranking (GRDR), designing a novel GR method to uplift recalled candidate quality. GRDR assigns multiple semantic IDs to each video using a query-guided multi-view tokenizer exposing diverse semantic access paths, and jointly trains the tokenizer and generative retriever via a shared codebook to cast semantic IDs as the semantic bridge between texts and videos. At inference, trie-constrained decoding generates a compact candidate set reranked by a dense model for fine-grained matching. Experiments on TVR benchmarks show GRDR matches strong dense retrievers in accuracy while reducing index storage by an order of magnitude and accelerating up to 300$\times$ in full-corpus retrieval.

</details>


### [26] [Thinker: A vision-language foundation model for embodied intelligence](https://arxiv.org/abs/2601.21199)
*Baiyu Pan,Daqin Luo,Junpeng Yang,Jiyuan Wang,Yixuan Zhang,Hailin Shi,Jichao Jiao*

Main category: cs.CV

> Thinker模型旨在解决大视觉语言模型在机器人领域面临的问题，通过优化的数据集和输入方法，显著提升了视频理解和任务规划的能力。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大视觉语言模型在应用于机器人领域时遇到了一些问题，这些问题对人类而言简单，但模型容易出错，如第三人称和第一人称视角的混淆及在时间推理中忽视视频结尾的信息。

**Method:** 通过构建针对机器人感知和推理的大规模数据集，以及采用一种将关键帧和完整视频序列同时作为输入的简单有效方法，来解决大视觉语言模型在机器人领域面临的挑战。

**Result:** 该模型在任务规划领域中最常用的两个基准数据集上取得了最先进的结果。

**Conclusion:** Thinker模型通过专门的数据集和联合输入关键帧及完整视频序列的方法，大幅提升了视频理解的能力，并在相关基准测试中取得了先进成果。

**Abstract:** When large vision-language models are applied to the field of robotics, they encounter problems that are simple for humans yet error-prone for models. Such issues include confusion between third-person and first-person perspectives and a tendency to overlook information in video endings during temporal reasoning. To address these challenges, we propose Thinker, a large vision-language foundation model designed for embodied intelligence. We tackle the aforementioned issues from two perspectives. Firstly, we construct a large-scale dataset tailored for robotic perception and reasoning, encompassing ego-view videos, visual grounding, spatial understanding, and chain-of-thought data. Secondly, we introduce a simple yet effective approach that substantially enhances the model's capacity for video comprehension by jointly incorporating key frames and full video sequences as inputs. Our model achieves state-of-the-art results on two of the most commonly used benchmark datasets in the field of task planning.

</details>


### [27] [LAMP: Learning Universal Adversarial Perturbations for Multi-Image Tasks via Pre-trained Models](https://arxiv.org/abs/2601.21220)
*Alvi Md Ishmam,Najibul Haque Sarker,Zaber Ibn Abdul Hakim,Chris Thomas*

Main category: cs.CV

> This paper presents LAMP, a black-box technique for creating universal adversarial perturbations that undermine the performance of multi-image Multimodal Large Language Models (MLLMs) by disrupting their ability to effectively process multiple images.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the unexplored vulnerabilities of multi-image Multimodal Large Language Models (MLLMs) by introducing a black-box adversarial attack method, which is more practical for real-world applications compared to the traditional white-box attack methods.

**Method:** LAMP, a black-box method for learning Universal Adversarial Perturbations (UAPs) targeting multi-image MLLMs, is introduced. It employs an attention-based constraint for preventing effective aggregation of information across images, a cross-image contagious constraint for spreading adversarial effects, and an index-attention suppression loss for position-invariant attack.

**Result:** Experimental results demonstrate that LAMP outperforms state-of-the-art (SOTA) baselines, achieving high attack success rates across various vision-language tasks and models.

**Conclusion:** The conclusion is that LAMP effectively exposes vulnerabilities in multi-image MLLMs, outperforming current adversarial attacks methods, and highlights the need for further research into robustness enhancements in such models.

**Abstract:** Multimodal Large Language Models (MLLMs) have achieved remarkable performance across vision-language tasks. Recent advancements allow these models to process multiple images as inputs. However, the vulnerabilities of multi-image MLLMs remain unexplored. Existing adversarial attacks focus on single-image settings and often assume a white-box threat model, which is impractical in many real-world scenarios. This paper introduces LAMP, a black-box method for learning Universal Adversarial Perturbations (UAPs) targeting multi-image MLLMs. LAMP applies an attention-based constraint that prevents the model from effectively aggregating information across images. LAMP also introduces a novel cross-image contagious constraint that forces perturbed tokens to influence clean tokens, spreading adversarial effects without requiring all inputs to be modified. Additionally, an index-attention suppression loss enables a robust position-invariant attack. Experimental results show that LAMP outperforms SOTA baselines and achieves the highest attack success rates across multiple vision-language tasks and models.

</details>


### [28] [PTQ4ARVG: Post-Training Quantization for AutoRegressive Visual Generation Models](https://arxiv.org/abs/2601.21238)
*Xuewen Liu,Zhikai Li,Jing Zhang,Mengjuan Chen,Qingyi Gu*

Main category: cs.CV

> 本文提出了PTQ4ARVG，一个无训练的量化框架，旨在解决ARVG模型的量化难题。通过实验验证了该框架在保持性能的同时，可以实现高效的量化。

<details>
  <summary>Details</summary>

**Motivation:** 由于在ARVG上应用量化方法仍然很大程度上未被探索，现有的量化方法很难有效应用于ARVG模型。我们探索了这个问题，并识别了三个关键挑战：(1)通道级别的严重异常值；(2)标记级别的高度动态激活；(3)样本级别的分布信息不匹配。

**Method:** 我们提出了PTQ4ARVG，一个无训练的后训练量化框架，包含三个组成部分：(1) Gain-Projected Scaling (GPS) 可以通过泰勒级数扩展量化损失来量化激活-权重量化的增益，并推导出最优缩放因子；(2) 静态Token-Wise量化（STWQ）利用ARVG的固有特性来解决Token-wise的方差；(3) 分布引导校准（DGC）选择对分布熵贡献最大的样本，消除样本级别的分布不匹配。

**Result:** 实验显示，PTQ4ARVG框架可以有效量化ARVG家族模型到8位和6-bit，同时保持竞争力的性能。

**Conclusion:** 我们提出的PTQ4ARVG框架可以在减少模型大小和计算延迟的同时，保持强大的性能。我们的实验证明，这一框架能有效地将ARVG模型量化至8位和6位，同时维持有竞争力的性能。

**Abstract:** AutoRegressive Visual Generation (ARVG) models retain an architecture compatible with language models, while achieving performance comparable to diffusion-based models. Quantization is commonly employed in neural networks to reduce model size and computational latency. However, applying quantization to ARVG remains largely underexplored, and existing quantization methods fail to generalize effectively to ARVG models. In this paper, we explore this issue and identify three key challenges: (1) severe outliers at channel-wise level, (2) highly dynamic activations at token-wise level, and (3) mismatched distribution information at sample-wise level. To these ends, we propose PTQ4ARVG, a training-free post-training quantization (PTQ) framework consisting of: (1) Gain-Projected Scaling (GPS) mitigates the channel-wise outliers, which expands the quantization loss via a Taylor series to quantify the gain of scaling for activation-weight quantization, and derives the optimal scaling factor through differentiation.(2) Static Token-Wise Quantization (STWQ) leverages the inherent properties of ARVG, fixed token length and position-invariant distribution across samples, to address token-wise variance without incurring dynamic calibration overhead.(3) Distribution-Guided Calibration (DGC) selects samples that contribute most to distributional entropy, eliminating the sample-wise distribution mismatch. Extensive experiments show that PTQ4ARVG can effectively quantize the ARVG family models to 8-bit and 6-bit while maintaining competitive performance. Code is available at http://github.com/BienLuky/PTQ4ARVG .

</details>
