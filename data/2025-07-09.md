<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 26]
- [cs.CV](#cs.CV) [Total: 22]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [TokenShapley: Token Level Context Attribution with Shapley Value](https://arxiv.org/abs/2507.05261)
*Yingtai Xiao,Yuqing Zhu,Sirat Samyoun,Wanrong Zhang,Jiachen T. Wang,Jian Du*

Main category: cs.CL

> 本文提出了TokenShapley，一种新的令牌级归因方法，解决了现有方法在关键词级别的归因不足，展示了其在实验中的卓越性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的归因方法在句子级别上表现良好，但在关键词级别上（如数字、年份或名字）归因时存在不足，为此提出了TokenShapley方法。

**Method:** TokenShapley结合了基于Shapley值的数据归因和基于KNN的检索技术，以提供精细化的令牌级归因方法。

**Result:** TokenShapley在四个基准测试中表现出色，在令牌级归因准确率上比最先进的模型提高了11-23%。

**Conclusion:** TokenShapley通过结合Shapley值和KNN检索，成功解决了令牌级别归因的问题，并在实验中取得了良好的性能。

**Abstract:** Large language models (LLMs) demonstrate strong capabilities in in-context
learning, but verifying the correctness of their generated responses remains a
challenge. Prior work has explored attribution at the sentence level, but these
methods fall short when users seek attribution for specific keywords within the
response, such as numbers, years, or names. To address this limitation, we
propose TokenShapley, a novel token-level attribution method that combines
Shapley value-based data attribution with KNN-based retrieval techniques
inspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed
datastore for contextual retrieval and computing Shapley values to quantify
token importance, TokenShapley provides a fine-grained data attribution
approach. Extensive evaluations on four benchmarks show that TokenShapley
outperforms state-of-the-art baselines in token-level attribution, achieving an
11-23% improvement in accuracy.

</details>


### [2] [User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in LLMs](https://arxiv.org/abs/2507.05266)
*Sougata Saha,Monojit Choudhury*

Main category: cs.CL

> 本文提出了一种新的框架，用于通过用户行为预测来衡量大语言模型的泛化能力，这种方法相比于传统知识检索和推理任务更为理想，实验结果表明GPT-4o在电影和音乐推荐数据集中表现最佳但所有模型仍存在较大改进空间。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于当前训练大型语言模型时数据污染问题日益严峻，本文认为传统的知识检索和推理任务并不适合作为衡量其泛化能力的标准，因而寻找一个替代方案十分必要。

**Method:** 本文基于用户行为预测提出了一种新的框架，并在电影和音乐推荐数据集上使用了GPT-4o、GPT-4o-mini和Llama-3.1-8B-Instruct进行了测试。

**Result:** 实验结果验证了新框架的有效性，其中GPT-4o的表现优于GPT-4o-mini和Llama，但各模型均有改进余地，尤其是Llama模型。

**Conclusion:** 通过用户行为预测衡量大型语言模型的泛化能力是一个合理、可扩展且鲁棒性的选择，能够提供比传统分数更好的见解。

**Abstract:** Measuring the generalization ability of Large Language Models (LLMs) is
challenging due to data contamination. As models grow and computation becomes
cheaper, ensuring tasks and test cases are unseen during training phases will
become nearly impossible. We argue that knowledge-retrieval and reasoning tasks
are not ideal for measuring generalization, as LLMs are not trained for
specific tasks. Instead, we propose user behavior prediction, also a key aspect
of personalization, as a theoretically sound, scalable, and robust alternative.
We introduce a novel framework for this approach and test it on movie and music
recommendation datasets for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct.
Results align with our framework's predictions, showing GPT-4o outperforms
GPT-4o-mini and Llama, though all models have much room for improvement,
especially Llama.

</details>


### [3] [An Adaptive Supervised Contrastive Learning Framework for Implicit Sexism Detection in Digital Social Networks](https://arxiv.org/abs/2507.05271)
*Mohammad Zia Ur Rehman,Aditya Shah,Nagendra Kumar*

Main category: cs.CL

> 本研究提出一种新型的隐性性别歧视检测方法ASCEND，运用创新的对比学习和多特征增强技术，在多个数据集上展现出比现有方法更高的捕获能力。

<details>
  <summary>Details</summary>

**Motivation:** 社交媒体全球化的趋势加剧了仇恨内容的传播，尤其是常被传统检测方法忽视的隐性性别歧视。

**Method:** 一种自适应监督对比学习框架（ASCEND），使用基于阈值的对比学习来精炼嵌入空间，并结合词级别注意力模块及情感、情绪和毒性特征来提高隐性性别歧视语言的检测准确率。

**Result:** 实验结果表明，ASCEND在EXIST2021和MLSC数据集上显著优于现有方法，平均Macro F1值分别提高了9.86%，29.63%和32.51%，显示出其在捕捉隐性性别歧视的细微线索上的有效性。

**Conclusion:** ASCEND框架能够更准确地检测社交媒体中的隐性性别歧视，并且比现有方法有显著的性能提升。

**Abstract:** The global reach of social media has amplified the spread of hateful content,
including implicit sexism, which is often overlooked by conventional detection
methods. In this work, we introduce an Adaptive Supervised Contrastive lEarning
framework for implicit sexism detectioN (ASCEND). A key innovation of our
method is the incorporation of threshold-based contrastive learning: by
computing cosine similarities between embeddings, we selectively treat only
those sample pairs as positive if their similarity exceeds a learnable
threshold. This mechanism refines the embedding space by robustly pulling
together representations of semantically similar texts while pushing apart
dissimilar ones, thus reducing false positives and negatives. The final
classification is achieved by jointly optimizing a contrastive loss with a
cross-entropy loss. Textual features are enhanced through a word-level
attention module. Additionally, we employ sentiment, emotion, and toxicity
features. Evaluations on the EXIST2021 and MLSC datasets demonstrate that
ASCEND significantly outperforms existing methods, with average Macro F1
improvements of 9.86%, 29.63%, and 32.51% across multiple tasks, highlighting
its efficacy in capturing the subtle cues of implicit sexist language.

</details>


### [4] [Beyond classical and contemporary models: a transformative ai framework for student dropout prediction in distance learning using rag, prompt engineering, and cross-modal fusion](https://arxiv.org/abs/2507.05285)
*Miloud Mihoubi,Meriem Zerkouk,Belkacem Chikhaoui*

Main category: cs.CL

> 本文提出了一种新的AI框架，通过情感分析、提示工程和注意力融合来改善对远程学习的学生辍学情况的预测，取得了显著的效果。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在解决远程学习中的学生辍学问题，传统机器学习模型难以捕捉到学生互动中的未结构化情感和上下文因素。

**Method:** 该论文介绍了一个创新型的AI框架，通过三个协同创新来重新定义辍学预测：用于领域特定情感分析的检索增强生成（RAG），用于解析学术压力的提示工程，以及跨模式注意力融合，以动态对齐文本、行为和社会人口统计信息。

**Result:** 该框架在一个包含4423名学生的纵向数据集上进行了评估，实现了89%的准确率和0.88的F1分数，比常规模型提高了7%，并将假阴性减少了21%。

**Conclusion:** 该工作弥合了预测分析与可操作性教学之间的差距，为全球教育系统提供了一种可扩展的解决方案，以降低辍学的风险。

**Abstract:** Student dropout in distance learning remains a critical challenge, with
profound societal and economic consequences. While classical machine learning
models leverage structured socio-demographic and behavioral data, they often
fail to capture the nuanced emotional and contextual factors embedded in
unstructured student interactions. This paper introduces a transformative AI
framework that redefines dropout prediction through three synergistic
innovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment
analysis, prompt engineering to decode academic stressors, and cross-modal
attention fusion to dynamically align textual, behavioral, and
socio-demographic insights. By grounding sentiment analysis in a curated
knowledge base of pedagogical content, our RAG-enhanced BERT model interprets
student comments with unprecedented contextual relevance, while optimized
prompts isolate indicators of academic distress (e.g., "isolation," "workload
anxiety"). A cross-modal attention layer then fuses these insights with
temporal engagement patterns, creating holistic risk profiles. Evaluated on a
longitudinal dataset of 4 423 students, the framework achieves 89% accuracy and
an F1-score of 0.88, outperforming conventional models by 7% and reducing false
negatives by 21%. Beyond prediction, the system generates interpretable
interventions by retrieving contextually aligned strategies (e.g., mentorship
programs for isolated learners). This work bridges the gap between predictive
analytics and actionable pedagogy, offering a scalable solution to mitigate
dropout risks in global education systems

</details>


### [5] [LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review](https://arxiv.org/abs/2507.05319)
*Cheng Yuan,Xinkai Rui,Yongqi Fan,Yawei Fan,Boyang Zhong,Jiacheng Wang,Weiyan Zhang,Tong Ruan*

Main category: cs.CL

> LCDS系统通过文本相似性计算和逻辑规则，解决了大语言模型在生成出院总结时的信息来源追溯问题，生成了更可靠的出院总结，有助于系统的持续改进。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于大语言模型在自动生成出院总结时仍然面临产生不准确内容或无源信息的问题，尤其是在电子医疗记录通常包含长文本数据的情况下，这使得内容的来源追溯更具挑战性。因此，提出LCDS系统来应对这些挑战。

**Method:** LCDS系统通过计算EMR和出院总结之间的文本相似性来构建来源映射表，限制总结内容的范围，并整合一系列逻辑规则生成可信的银质出院总结。系统支持对生成内容的来源追溯。

**Result:** 该研究通过构建EMR和出院总结间的文本相似性表，以及整合逻辑规则，提出了LCDS系统，以减少大语言模型在生成出院总结过程中出现的错误信息生成和信息来源难以追溯问题。系统在不同临床领域生成了更可靠的出院总结，并支持生成内容的来源追溯，方便专家审查。最终，通过黄金出院总结记录进行LLMs的增量微调。

**Conclusion:** LCDS系统能够生成更可靠和来源清晰的出院总结，并允许专家高效审核、反馈和纠正错误。同时，生成的黄金出院总结用于LLMs的增量微调。

**Abstract:** Despite the remarkable performance of Large Language Models (LLMs) in
automated discharge summary generation, they still suffer from hallucination
issues, such as generating inaccurate content or fabricating information
without valid sources. In addition, electronic medical records (EMRs) typically
consist of long-form data, making it challenging for LLMs to attribute the
generated content to the sources. To address these challenges, we propose LCDS,
a Logic-Controlled Discharge Summary generation system. LCDS constructs a
source mapping table by calculating textual similarity between EMRs and
discharge summaries to constrain the scope of summarized content. Moreover,
LCDS incorporates a comprehensive set of logical rules, enabling it to generate
more reliable silver discharge summaries tailored to different clinical fields.
Furthermore, LCDS supports source attribution for generated content, allowing
experts to efficiently review, provide feedback, and rectify errors. The
resulting golden discharge summaries are subsequently recorded for incremental
fine-tuning of LLMs. Our project and demo video are in the GitHub repository
https://github.com/ycycyc02/LCDS.

</details>


### [6] [MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents](https://arxiv.org/abs/2507.05330)
*Ming Gong,Xucheng Huang,Chenghan Yang,Xianhan Peng,Haoxin Wang,Yang Liu,Ling Jiang*

Main category: cs.CL

> 本文介绍了MindFlow，一个开源的电子商务多模态大语言模型代理，展示了在复杂场景下的显著优势。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大语言模型在电子商务客户服务中的新应用方面取得了进展，但它们在复杂的多模态场景中的能力仍然受限。为了解决这一问题，提出了MindFlow。

**Method:** 本文介绍了MindFlow，这是第一个开源的针对电子商务的多模态大语言模型代理，它基于CoALA框架构建，集成了记忆、决策和行动模块，并采用了一种模块化的"MLLM-as-Tool"策略来进行有效的视觉-文本推理。

**Result:** 通过线上A/B测试和基于模拟的消融实验，MindFlow在处理复杂查询、提升用户满意度和减少运营成本方面表现出显著的优势，在实际部署中观察到相对提升了93.53%的表现。

**Conclusion:** MindFlow是第一个针对电子商务的开源多模态大语言模型代理，展示了在处理复杂查询、提升用户满意度和减少运营成本方面的显著优势。

**Abstract:** Recent advances in large language models (LLMs) have enabled new applications
in e-commerce customer service. However, their capabilities remain constrained
in complex, multimodal scenarios. We present MindFlow, the first open-source
multimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it
integrates memory, decision-making, and action modules, and adopts a modular
"MLLM-as-Tool" strategy for effect visual-textual reasoning. Evaluated via
online A/B testing and simulation-based ablation, MindFlow demonstrates
substantial gains in handling complex queries, improving user satisfaction, and
reducing operational costs, with a 93.53% relative improvement observed in
real-world deployments.

</details>


### [7] [LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks](https://arxiv.org/abs/2507.05346)
*William Fleshman,Benjamin Van Durme*

Main category: cs.CL

> 研究提出了一种无需额外训练或数据访问的新方法LAG，用于高效地选择和组合大量专家知识，其在知识密集型任务中性能优越，并且可以与其他方法兼容使用。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于专门针对特定任务和领域的微调语言模型专家数量的增加，需要有效的方法来选择和组合这些专家。

**Method:** 我们提出了一种名为LoRA-Augmented Generation (LAG)的方法，用于在不需要额外训练或数据访问的情况下，在每词和每层的基础上高效地过滤、检索和应用专家知识。

**Result:** 我们在各种知识密集型任务上评估了LAG，其性能优于现有的无数据方法，并且展示了与检索增强生成(RAG)等其他解决方案的兼容性。

**Conclusion:** LAG可以作为现有无数据方法的改进方案，并且在有额外数据的情况下也能与其他方法兼容，展示了其广泛的应用潜力。

**Abstract:** The proliferation of fine-tuned language model experts for specific tasks and
domains signals the need for efficient selection and combination methods. We
propose LoRA-Augmented Generation (LAG) for leveraging large libraries of
knowledge and task-specific LoRA adapters. LAG requires no additional training
or access to data, and efficiently filters, retrieves, and applies experts on a
per-token and layer basis. We evaluate LAG on various knowledge-intensive
tasks, achieving superior performance over existing data-free methods. We
explore scenarios where additional data is available, demonstrating LAG's
compatibility with alternative solutions such as retrieval-augmented generation
(RAG).

</details>


### [8] [On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study](https://arxiv.org/abs/2507.05362)
*Riccardo Alberghi,Elizaveta Demyanenko,Luca Biggio,Luca Saglietti*

Main category: cs.CL

> 研究发现，大型语言模型在面对更困难的问题时，增加计算和使用系统、增量的方式进行推理可以提高推理能力。使用较长但有效的轨迹训练模型，即使这些轨迹并不高效，也能带来更好的泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于探讨增加测试时计算量和系统的、增量式的推理方式（类比于人类问题解决过程）对大语言模型推理能力的影响，特别是这两种因素如何独立影响模型性能。

**Method:** 通过基于分层图中最小路径任务的受控设置，研究如何提高大型语言模型中的推理能力。具体来说，训练了解码器唯一的变压器模型，使用自定义分词器，通过最优的自下而上的动态规划轨迹与涉及回溯的较长但有效的轨迹进行比较。

**Result:** 令人惊讶地发现，在相同的训练标记预算下，使用低效轨迹训练的模型在未见过的图上表现更好，这并不单纯是由于轨迹长度增加的作用，而是因模型对下一个标记预测的信心增加所致。

**Conclusion:** 研究表明，使用长度较长但连贯且局部递增的推理轨迹进行训练，可以使训练信号更容易优化，进而提升模型泛化能力。这表明，模型的性能提升与模型对下一标记预测的信心密切相关。

**Abstract:** Recent advances in natural language processing highlight two key factors for
improving reasoning in large language models (LLMs): (i) allocating more
test-time compute tends to help on harder problems but often introduces
redundancy in the reasoning trace, and (ii) compute is most effective when
reasoning is systematic and incremental, forming structured chains of thought
(CoTs) akin to human problem-solving. To study these factors in isolation, we
introduce a controlled setting based on shortest-path tasks in layered graphs.
We train decoder-only transformers on question-trace-answer triples using a
custom tokenizer, comparing models trained on optimal bottom-up dynamic
programming traces with those trained on longer, valid traces involving
backtracking. Surprisingly, with the same training-token budget, models trained
on inefficient traces generalize better to unseen graphs. This benefit is not
due to length alone-injecting arbitrary redundancy into reasoning traces fails
to help and can even hurt performance. Instead, we find that generalization
correlates with the model's confidence in next-token prediction, suggesting
that long, coherent, and locally incremental traces make the training signal
easier to optimize.

</details>


### [9] [EduCoder: An Open-Source Annotation System for Education Transcript Data](https://arxiv.org/abs/2507.05385)
*Guanzhong Pan,Mei Tan,Hyunji Nam,Lucía Langlois,James Malamut,Liliana Deonizio,Dorottya Demszky*

Main category: cs.CL

> 介绍了一种名为EduCoder的领域特定工具，用于支持教育对话的语句级标注，解决教育对话转录编码复杂性的问题，同时提供多种标注类型和标注者标注结果比较功能。

<details>
  <summary>Details</summary>

**Motivation:** 虽然有许多适用于NLP和定性研究的通用文本标注工具，但针对教育对话转录的复杂性，这些工具仍然不足。开发EduCoder是为了应对定义复杂的编码手册、支持开放性和分类编码等挑战。

**Method:** 设计了一个名为EduCoder的领域特定工具，旨在支持教育对话的语句级标注。EduCoder平台允许研究人员和领域专家基于观察数据合作定义复杂的编码手册，同时支持分类和开放式标注类型，并结合上下文材料。此外，它还提供多个标注者的标注结果的并排比较功能，以提高数据可靠性。

**Result:** EduCoder解决了教育对话转录中编码复杂性的问题，平台开源，提供了演示视频。

**Conclusion:** EduCoder是一个开放源码的平台，旨在通过提供包括分类和开放式标注以及其他特性在内的功能，帮助解决教育对话转录编码的复杂性问题，并通过提供多个标注者的标注结果比较来提高数据可靠性。

**Abstract:** We introduce EduCoder, a domain-specialized tool designed to support
utterance-level annotation of educational dialogue. While general-purpose text
annotation tools for NLP and qualitative research abound, few address the
complexities of coding education dialogue transcripts -- with diverse
teacher-student and peer interactions. Common challenges include defining
codebooks for complex pedagogical features, supporting both open-ended and
categorical coding, and contextualizing utterances with external features, such
as the lesson's purpose and the pedagogical value of the instruction. EduCoder
is designed to address these challenges by providing a platform for researchers
and domain experts to collaboratively define complex codebooks based on
observed data. It incorporates both categorical and open-ended annotation types
along with contextual materials. Additionally, it offers a side-by-side
comparison of multiple annotators' responses, allowing comparison and
calibration of annotations with others to improve data reliability. The system
is open-source, with a demo video available.

</details>


### [10] [The Generalization Ridge: Information Flow in Natural Language Generation](https://arxiv.org/abs/2507.05387)
*Ruidi Chang,Chunyuan Deng,Hanjie Chen*

Main category: cs.CL

> 研究探讨了Transformer模型中间层在泛化能力中的作用，提出了InfoRidge框架，并通过实验揭示了一致的非单调趋势，即泛化能力形式在中上层形成脊谷，然后在最后几层下降。

<details>
  <summary>Details</summary>

**Motivation:** 尽管基于Transformer的语言模型在自然语言生成任务中取得了最先进的表现，但其内部合成任务相关信息的机制尚未充分理解。现有研究指出，与最终层相比，中间层往往产生更有泛化能力的表示，但这种泛化能力是如何出现并传播的仍然是未知的。

**Method:** 提出InfoRidge框架，通过评估不同深度隐藏表示与目标输出之间的互信息，来表征任务相关信息在模型训练过程中的流向。实验中使用了残差缩放系数（应用于每个残差块的可训练标量参数）作为功能探针，用于评估各Transformer层的重要性。

**Result:** 实验结果显示，预测信息在中上层达到峰值，形成泛化脊谷，然后在最后几层中下降，显示从泛化到记忆的转变。进一步研究表明，当分布发生变化时，模型会减少对最后一层的权重，而越来越依赖泛化脊谷层。

**Conclusion:** 研究揭示了Transformer内部机制的新见解，强调了中间层在支持泛化中的关键作用。

**Abstract:** Transformer-based language models have achieved state-of-the-art performance
in natural language generation (NLG) tasks, yet their internal mechanisms for
synthesizing task-relevant information remain insufficiently understood. While
prior studies suggest that intermediate layers often yield more generalizable
representations than final layers, how this generalization ability emerges and
propagates across layers during training remains unclear. To address this gap,
we propose InfoRidge, an information-theoretic framework, to characterize how
predictive information-the mutual information between hidden representations
and target outputs-varies across depth. Estimating this quantity enables us to
trace the flow of task-relevant information throughout the model during
training. Our experiments across various models and datasets reveal a
consistent non-monotonic trend: predictive information peaks in upper-middle
layers-forming a generalization ridge-before declining in final layers,
reflecting a transition between generalization and memorization. To further
investigate this phenomenon, we introduce residual scaling
coefficients-trainable scalar parameters applied to each residual block-which
serve as functional probes for assessing the relative importance of individual
transformer layers. These coefficients reveal that, under distribution shift,
models downweight final layers and increasingly rely on ridge layers,
highlighting their role in generalization. Together, these findings offer new
insights into the internal mechanisms of transformers and underscore the
critical role of intermediate layers in supporting generalization.

</details>


### [11] [Controlling What You Share: Assessing Language Model Adherence to Privacy Preferences](https://arxiv.org/abs/2507.05391)
*Guillem Ramírez,Alexandra Birch,Ivan Titov*

Main category: cs.CL

> 本文提出了一种新的隐私框架，利用本地模型和隐私配置文件保护用户数据，即便在向外部模型发送查询时也能确保隐私。

<details>
  <summary>Details</summary>

**Motivation:** 为了使用户能够控制自己的数据而不必完全暴露给服务提供商，该研究引入了一种隐私保护的新方法。

**Method:** 研究构建了一个隐私框架，其中本地模型会根据用户的隐私配置文件重写查询内容，以隐藏用户定义的敏感信息。此外，还介绍了一个名为PEEP的多语言数据集，用于支持此项研究。

**Result:** 该研究提出了一种隐私框架，通过隐私配置文件来控制用户数据的暴露情况。框架利用本地模型根据简单的自然语言指令重写查询内容，仅隐藏用户认为敏感的信息，然后发送给外部模型，从而在保护隐私的同时维持性能。实验表明，轻量级的语言模型能够部分遵循这些指令，但也存在一些挑战，强调了需要开发能更好地理解和遵循用户定义隐私偏好的模型。

**Conclusion:** 轻量级语言模型在遵循隐私配置文件方面取得了一定的效果，但仍有改进空间，未来需要研究更能够理解和遵守用户隐私偏好的模型。

**Abstract:** Large language models (LLMs) are primarily accessed via commercial APIs, but
this often requires users to expose their data to service providers. In this
paper, we explore how users can stay in control of their data by using privacy
profiles: simple natural language instructions that say what should and should
not be revealed. We build a framework where a local model uses these
instructions to rewrite queries, only hiding details deemed sensitive by the
user, before sending them to an external model, thus balancing privacy with
performance. To support this research, we introduce PEEP, a multilingual
dataset of real user queries annotated to mark private content and paired with
synthetic privacy profiles. Our experiments with lightweight LLMs show they can
follow these instructions to some extent, but also face consistent challenges,
highlighting the need for models that better understand and comply with
user-defined privacy preferences.

</details>


### [12] [Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning](https://arxiv.org/abs/2507.05418)
*Jaedong Hwang,Kumar Tanmay,Seok-Jin Lee,Ayush Agrawal,Hamid Palangi,Kumar Ayush,Ila Fiete,Paul Pu Liang*

Main category: cs.CL

> 本文针对大型语言模型在多语言推理中存在的问题，提出GeoFact-X基准测试和BRIDGE训练方法，提高了模型在多语言情境下的推理准确性。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLMs）在数学、事实问答和代码生成等领域取得了显著成果，但在这些任务中的多语言推理能力尚未充分发展。特别是在低资源语言如斯瓦希里语或泰语中，LLMs经常误解提示或默认用英语推理。这种对高资源语言的隐性偏见损害了事实准确性、可解释性和信任度。现有的多语言基准测试仅关注最终答案，忽视了模型是否实际上是用目标语言进行推理。为了解决这一问题，引入了GeoFact-X。

**Method:** GeoFact-X是一个基于地理的多语言事实推理基准测试，包含五种语言的注解推理轨迹：英语、印地语、日语、斯瓦希里语和泰语。此外，提出了BRIDGE，这是一种新的训练方法，它在监督微调和测试时间强化学习中使用语言一致性奖励来引导推理与输入语言一致。

**Result:** 作者开发了一个自动评估协议，使用LLM作为裁判来评估答案的正确性和推理轨迹的质量和语言一致性，这对超越表面级别的度量标准提供了细致和可扩展的分析。

**Conclusion:** 实验结果表明，BRIDGE显著增强了多语言推理的准确性，表明具有推理感知的多语言强化学习对于强大的跨语言泛化至关重要。

**Abstract:** Large Language Models (LLMs) have achieved strong performance in domains like
mathematics, factual QA, and code generation, yet their multilingual reasoning
capabilities in these tasks remain underdeveloped. Especially for low-resource
languages such as Swahili or Thai, LLMs can often misinterpret prompts or
default to reasoning in English. This implicit bias toward high-resource
languages undermines factual accuracy, interpretability, and trust. Current
multilingual benchmarks focus only on final answers, overlooking whether models
actually reason in the target language. To address this gap, we introduce
GeoFact-X, a geography-based multilingual factual reasoning benchmark with
annotated reasoning traces in five languages: English, Hindi, Japanese,
Swahili, and Thai. We further propose BRIDGE, a novel training method that
guides supervised fine-tuning and test-time reinforcement learning with a
language-consistency reward to align reasoning with the input language.
Finally, we develop an automatic evaluation protocol using LLM-as-a-judge to
assess answer correctness and the quality and language consistency of reasoning
traces, enabling nuanced and scalable analysis beyond surface-level metrics.
Our results show that BRIDGE significantly enhances multilingual reasoning
fidelity, demonstrating that reasoning-aware multilingual reinforcement
learning is crucial for robust cross-lingual generalization.
https://jd730.github.io/projects/GeoFact-X_BRIDGE

</details>


### [13] ["Lost-in-the-Later": Framework for Quantifying Contextual Grounding in Large Language Models](https://arxiv.org/abs/2507.05424)
*Yufei Tao,Adam Hiatt,Rahul Seetharaman,Ameeta Agrawal*

Main category: cs.CL

> 论文探讨了大型语言模型如何处理上下文知识与参数化知识，发现模型存在“后期未受关注”的现象，并提出一些改进背景利用的方法。

<details>
  <summary>Details</summary>

**Motivation:** 探索大型语言模型如何优先和整合上下文和参数化知识，这两者能力已经被识别但其具体机制尚未被充分研究。

**Method:** 介绍了一种名为CoPE的新评估框架，该框架可以系统性地在不同模型和语言中衡量上下文知识（CK）和参数化知识（PK）。使用了英文、西班牙语和丹麦语的MultiWikiAtomic数据集，分析了大型语言模型（LLMs）如何整合背景，优先考虑信息以及在开放式问答中如何纳入参数化知识。

**Result:** 发现了一个我们称之为“后期未受关注”的现象，即LLMs倾向于忽略或降级化处理在给定背景中出现较晚的信息。进一步发现，无论是推理模型还是通过链式思想提示的非推理模型，都比不经过链式思想提示的非推理模型更少使用背景。链式思想提示导致了更低的回忆率和更短的回答，从而导致背景理解降低。提出了基于提示的方法来有效利用输入背景，研究表明CK引导的提示可以提高事实依据，减少杜撰的现象。

**Conclusion:** 通过CoPE分析，开发了新的方法来提高语言模型对输入背景的利用效率，从而提升了模型的事实依据并减少了杜撰的情况。

**Abstract:** Large language models are capable of leveraging both contextual and
parametric knowledge but how they prioritize and integrate these sources
remains underexplored. We introduce CoPE, a novel evaluation framework that
systematically measures contextual knowledge (CK) and parametric knowledge (PK)
across models and languages. Using our MultiWikiAtomic dataset in English,
Spanish, and Danish, we analyze how large language models (LLMs) integrate
context, prioritize information, and incorporate PK in open-ended question
answering. Our analysis uncovers a phenomenon we call lost-in-the-later, where
LLMs tend to overlook or deprioritize information that appears later in a given
context, revealing a strong positional bias that affects contextual grounding.
We further find that reasoning models, as well as non-reasoning models prompted
with chain-of-thought (CoT), use context even less than non-reasoning models
without CoT and fail to mitigate the lost-in-the-later effect. CoT prompting,
in particular, results in lower recall and shorter responses, leading to
degraded contextual grounding. Based on these insights, we design prompt-based
methods to effectively leverage input context. A case study applying CoPE to
summarization demonstrates that CK-informed prompting improves factual
grounding and reduces hallucination.

</details>


### [14] [Gendered Divides in Online Discussions about Reproductive Rights](https://arxiv.org/abs/2507.05443)
*Ashwin Rao,Sze Yuh Nina Wang,Kristina Lerman*

Main category: cs.CL

> 通过分析近1000万条推特上的堕胎相关信息，研究表明性别和地理位置在堕胎议题中起着决定性作用，特别是在保守地区，性别差异更为显著，与意识形态无关。Dobbs文件泄露事件更加强了女性在那些堕胎权利受到威胁地区的在线讨论。

<details>
  <summary>Details</summary>

**Motivation:** 尽管堕胎问题的意识形态分歧已经被广泛记录，但性別以及地方社会政治环境如何塑造公众话语的研究较少。本文旨在填补这一空白，探讨性别在堕胎态度中的作用。

**Method:** 本研究通过对推特上近1000万条与堕胎相关的帖子进行分析，这些帖子来自带有推断性别、意识形态和地理位置的用户。研究主要考察性别是如何在地方社会政治背景下影响堕胎态度和情感表达的。

**Result:** 性别在塑造堕胎态度和情感表达上扮演了重要角色，尤其是保守地区，这一作用独立于意识形态。研究发现，在某些地区，性別差异在堕胎态度上变得更加明显，特别是在Dobbs文件泄露后。该事件加剧了女性在堕胎权利受威胁地区的在线参与。

**Conclusion:** 堕胎讨论不仅是意识形态的两极分化，还深受性别和地理位置的影响，在机构发生动荡时，身份认同在塑造政治表达方面发挥了核心作用。

**Abstract:** The U.S. Supreme Court's 2022 ruling in Dobbs v. Jackson Women's Health
Organization marked a turning point in the national debate over reproductive
rights. While the ideological divide over abortion is well documented, less is
known about how gender and local sociopolitical contexts interact to shape
public discourse. Drawing on nearly 10 million abortion-related posts on X
(formerly Twitter) from users with inferred gender, ideology and location, we
show that gender significantly moderates abortion attitudes and emotional
expression, particularly in conservative regions, and independently of
ideology. This creates a gender gap in abortion attitudes that grows more
pronounced in conservative regions. The leak of the Dobbs draft opinion further
intensified online engagement, disproportionately mobilizing pro-abortion women
in areas where access was under threat. These findings reveal that abortion
discourse is not only ideologically polarized but also deeply structured by
gender and place, highlighting the central role of identity in shaping
political expression during moments of institutional disruption.

</details>


### [15] [PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically Distant Language Pairs](https://arxiv.org/abs/2507.05444)
*Sana Kang,Myeongseok Gwon,Su Young Kwon,Jaewook Lee,Andrew Lan,Bhiksha Raj,Rita Singh*

Main category: cs.CL

> 本文提出了一种名为PhoniTale的跨语言助记符生成系统，旨在帮助非英语母语者通过语音相似性获取英语词汇的助记符，并进行了有效性测试。

<details>
  <summary>Details</summary>

**Motivation:** 考虑到第二语言学习者在学习英韩等语言时，由于语音和结构不匹配，词汇学习难度增大。尤其是针对非英语母语者学习英语的场景，以往的研究较少，因此作者提出使用大型语言模型生成关键词助记符以辅助词汇学习。

**Method:** 本文介绍了一个名为PhoniTale的跨语言助记符生系统，该系统基于语音相似性检索L1关键词序列，并利用大型语言模型生成助记符，以帮助非英语母语者学习英语词汇。

**Result:** 作者通过自动化评估指标和人类评估，将PhoniTale的输出与人类生成的助记符以及之前的自动化方法进行了比较。结果显示，PhoniTale的表现与人类生成的助记符相当。

**Conclusion:** 测试结果表明，PhoniTale生成的助记符在效果上与人类手工创造的助记符相差无几，但在助记符的质量和生成方法上还有改进的空间。

**Abstract:** Vocabulary acquisition poses a significant challenge for second-language (L2)
learners, especially when learning typologically distant languages such as
English and Korean, where phonological and structural mismatches complicate
vocabulary learning. Recently, large language models (LLMs) have been used to
generate keyword mnemonics by leveraging similar keywords from a learner's
first language (L1) to aid in acquiring L2 vocabulary. However, most of this
research has focused on native English speakers learning other languages,
rather than the reverse. In this paper, we present PhoniTale, a novel
cross-lingual mnemonic generation system that retrieves L1 keyword sequence
based on phonological similarity and uses LLMs to generate mnemonics. We
evaluate PhoniTale using both automated metrics and human evaluations,
comparing its output to mnemonics created by humans and by previous automated
approaches. To assess practical effectiveness, we also conduct a short-term
recall test measuring mnemonic helpfulness. Our findings show that PhoniTale
performs comparably to human-authored mnemonics. We also highlight key areas
for future improvement in mnemonic quality and methodology.

</details>


### [16] [On the Semantics of Large Language Models](https://arxiv.org/abs/2507.05448)
*Martin Schuele*

Main category: cs.CL

> 研究通过分析LLMs内部机制和生成的语言表示，结合弗雷格和罗素的语义理论，对LLMs的语义理解能力有了更细致的认识。

<details>
  <summary>Details</summary>

**Motivation:** 探讨LLMs在多大程度上真正理解语言这一争议的问题。

**Method:** 通过研究大型语言模型（如ChatGPT）的内部运作及其生成的语言表示，并借鉴弗雷格和罗素的经典语义理论，以探究LLMs在词汇和句子层面的语义能力。

**Result:** 对LLMs的潜在语义能力有了更加细致的理解。

**Conclusion:** 研究揭示了LLMs在词汇和句子层级上的语义能力。

**Abstract:** Large Language Models (LLMs) such as ChatGPT demonstrated the potential to
replicate human language abilities through technology, ranging from text
generation to engaging in conversations. However, it remains controversial to
what extent these systems truly understand language. We examine this issue by
narrowing the question down to the semantics of LLMs at the word and sentence
level. By examining the inner workings of LLMs and their generated
representation of language and by drawing on classical semantic theories by
Frege and Russell, we get a more nuanced picture of the potential semantic
capabilities of LLMs.

</details>


### [17] [ModelCitizens:Representing Community Voices in Online Safety](https://arxiv.org/abs/2507.05455)
*Ashima Suvarna,Christina Chance,Hamid Palangi,Sophie Hao,Thomas Hartvigsen,Saadia Gabriel*

Main category: cs.CL

> Structure

<details>
  <summary>Details</summary>

**Motivation:** Structure

**Method:** Structure

**Result:** {
  "tldr": "研究提出MODELCITIZENS数据集和基于此数据集微调的语言模型，以提高自动识别有毒言论的能力，强调了基于社区视角的重要性。",
  "motivation": "现存有毒语言检测模型在单一真实性的训练数据和忽视社群感知下表现不佳，研究旨在改善这一状况。",
  "method": "构建了包含了6.8K社交媒体帖子和40K多样化身份群体的毒性注释的MODELCITIZENS数据集，并使用大型语言模型生成对话场景来丰富数据集。基于此数据集微调了LLAMACITIZEN-8B 和 GEMMACITIZEN-12B模型。",
  "result": "在MODELCITIZENS数据集上，现有的顶级检测工具表现次于研究团队微调的模型，显示出新模型的优越性。",
  "conclusion": "研究强调了根据社区意见进行注释和模型开发对于包容性内容调节的重要性。",
  "paper_content": "Automatic toxic language detection is critical for creating safe, inclusive online spaces. However, it is a highly subjective task, with perceptions of toxic language shaped by community norms and lived experience. Existing toxicity detection models are typically trained on annotations that collapse diverse annotator perspectives into a single ground truth, erasing important context-specific notions of toxicity such as reclaimed language. To address this, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K toxicity annotations across diverse identity groups. To capture the role of conversational context on toxicity, typical of social media posts, we augment MODELCITIZENS posts with LLM-generated conversational scenarios. State-of-the-art toxicity detection tools (e.g. OpenAI Moderation API, GPT-o4-mini) underperform on MODELCITIZENS, with further degradation on context-augmented posts. Finally, we release LLAMACITIZEN-8B and GEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS, which outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our findings highlight the importance of community-informed annotation and modeling for inclusive content moderation. "
}

**Conclusion:** Structure

**Abstract:** Automatic toxic language detection is critical for creating safe, inclusive
online spaces. However, it is a highly subjective task, with perceptions of
toxic language shaped by community norms and lived experience. Existing
toxicity detection models are typically trained on annotations that collapse
diverse annotator perspectives into a single ground truth, erasing important
context-specific notions of toxicity such as reclaimed language. To address
this, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K
toxicity annotations across diverse identity groups. To capture the role of
conversational context on toxicity, typical of social media posts, we augment
MODELCITIZENS posts with LLM-generated conversational scenarios.
State-of-the-art toxicity detection tools (e.g. OpenAI Moderation API,
GPT-o4-mini) underperform on MODELCITIZENS, with further degradation on
context-augmented posts. Finally, we release LLAMACITIZEN-8B and
GEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS,
which outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our
findings highlight the importance of community-informed annotation and modeling
for inclusive content moderation.

</details>


### [18] [Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications](https://arxiv.org/abs/2507.05517)
*Jean-Philippe Corbeil,Asma Ben Abacha,George Michalopoulos,Phillip Swazinna,Miguel Del-Agua,Jerome Tremblay,Akila Jeeson Daniel,Cari Bader,Kevin Cho,Pooja Krishnan,Nathan Bodenstab,Thomas Lin,Wenxuan Teng,Francois Beaulieu,Paul Vozila*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Large language models (LLMs) such as GPT-4o and o1 have demonstrated strong
performance on clinical natural language processing (NLP) tasks across multiple
medical benchmarks. Nonetheless, two high-impact NLP tasks - structured tabular
reporting from nurse dictations and medical order extraction from
doctor-patient consultations - remain underexplored due to data scarcity and
sensitivity, despite active industry efforts. Practical solutions to these
real-world clinical tasks can significantly reduce the documentation burden on
healthcare providers, allowing greater focus on patient care. In this paper, we
investigate these two challenging tasks using private and open-source clinical
datasets, evaluating the performance of both open- and closed-weight LLMs, and
analyzing their respective strengths and limitations. Furthermore, we propose
an agentic pipeline for generating realistic, non-sensitive nurse dictations,
enabling structured extraction of clinical observations. To support further
research in both areas, we release SYNUR and SIMORD, the first open-source
datasets for nurse observation extraction and medical order extraction.

</details>


### [19] [Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS](https://arxiv.org/abs/2507.05557)
*Alex ZH Dou,Zhongwei Wan,Dongfei Cui,Xin Wang,Jing Xiong,Haokun Lin,Chaofan Tao,Shen Yan,Mi Zhang*

Main category: cs.CL

> R2-LLMs是一种新颖的分层检索增强推理框架，通过双层检索方法提升了LLM的推理时性能，无需进行更复杂的训练数据准备。

<details>
  <summary>Details</summary>

**Motivation:** 为了在不需要从更先进的模型中获取链式思维（CoT）训练数据的情况下提升大规模语言模型（LLMs）的推理时增强效果，提出了R2-LLMs框架。

**Method:** R2-LLMs通过双层次检索增强推理时的泛化能力。粗层次上，提取复杂推理问题的抽象模板并检索相似的问题-答案对来促进高层次上下文学习；细层次上，在蒙特卡洛树搜索（MCTS）期间，从参考数学问题数据集中高效检索类似中间解决方案，利用过程奖励模型（PRM）进行评分。

**Result:** 实验证明，与基线模型相比，使用LLaMA-3.1-8B在MATH500、GSM8K和OlympiadBench-TO数据集上的表现提高了最高16%。

**Conclusion:** 该方法提升了复杂推理任务的准确性，使得推理时的性能显著提高。

**Abstract:** Test-time scaling has emerged as a promising paradigm in language modeling,
leveraging additional computational resources at inference time to enhance
model performance. In this work, we introduce R2-LLMs, a novel and versatile
hierarchical retrieval-augmented reasoning framework designed to improve
test-time scaling in large language models (LLMs) without requiring
distillation from more advanced models to obtain chain-of-thought (CoT)
training data. R2-LLMs enhances inference-time generalization by integrating
dual-level retrieval-based in-context learning: (1) At the coarse level, our
approach extracts abstract templates from complex reasoning problems and
retrieves similar problem-answer pairs to facilitate high-level in-context
learning; (2) At the fine level, during Monte Carlo Tree Search (MCTS), R2-LLMs
efficiently retrieves analogous intermediate solution steps from reference
mathematical problem datasets, refining step-wise reasoning with the aid of a
process reward model (PRM) for scoring. R2-LLMs is a robust hierarchical
reasoning-augmentation method that enhances in-context-level reasoning while
seamlessly integrating with step-level tree search methods. Utilizing PRM, it
refines both candidate generation and decision-making for improved reasoning
accuracy. Empirical evaluations on the MATH500, GSM8K, and OlympiadBench-TO
datasets achieve substantial relative improvement with an increase of up to 16%
using LLaMA-3.1-8B compared to the baselines, showcasing the effectiveness of
our approach in complex reasoning tasks.

</details>


### [20] [Self-Review Framework for Enhancing Instruction Following Capability of LLM](https://arxiv.org/abs/2507.05598)
*Sihyun Park*

Main category: cs.CL

> Re5框架通过一系列优化措施，实现了在资源效率和输出质量之间的平衡，有效提高了语言模型指令遵守能力。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在解决语言模型在遵守复杂指令方面的局限性，同时减少资源消耗，并避免由于过度修订导致的输出质量下降。

**Method:** Re5框架通过提取任务和约束组件，执行结构性评估，应用细粒度的约束特定内容评估并进行选择性修订，以提高指令遵守性能并保持生成内容的质量。

**Result:** 实验结果显示，Re5框架使用少量数据即可达到与基于GPT-4o-mini的数据训练的模型相似的指令遵守性能，维持了64.24%的响应质量胜率。

**Conclusion:** 研究验证了Re5框架是一种增强指令遵守性能的有效方法，同时在几乎无需外部监督的情况下保持了输出质量。

**Abstract:** Various techniques have been proposed to improve large language models (LLMs)
adherence to formatting and instruction constraints. One of the most effective
approaches involves utilizing high-quality data generated by powerful models.
However, such models often fail to fully comply with complex instructions in a
single generation. To address this limitation, iterative revision methods have
been introduced. Nevertheless, as the number of data points and revision
iterations increases, the associated monetary costs grow significantly. As a
resource-efficient alternative, methods have been proposed that leverage
high-performance evaluation tools to compensate for the limited self-evaluation
capabilities of open-source LLMs. However, these approaches often lead to a
degradation in output quality due to excessive revision. To overcome these
challenges, we propose Re5, a self-evaluation and revision framework designed
to enhance instruction-following performance while preserving the quality of
the generated content. Re5 extracts task and constraint components from user
instructions, performs structural evaluations to prevent error accumulation,
and applies fine-grained constraint-specific content evaluations followed by
selective revisions. This process ensures precise and quality-preserving
improvements. The final high-quality outputs are used for alignment tuning,
enabling long-term alignment improvements through a data-centric iterative
refinement loop. Experimental results demonstrate that Re5 achieves
instruction-following performance comparable to models trained on data
generated by GPT-4o-mini, a high-performance model, even with a small amount of
data while maintaining response quality with a 64.24%-win rate over the
non-revised initial responses. These results validate Re5 as an efficient and
effective solution for enhancing instruction adherence with minimal external
supervision.

</details>


### [21] [Flipping Knowledge Distillation: Leveraging Small Models' Expertise to Enhance LLMs in Text Matching](https://arxiv.org/abs/2507.05617)
*Mingzhe Li,Jing Xiang,Qishen Zhang,Kaiyang Wan,Xiuying Chen*

Main category: cs.CL

> 文章提出了一种新的反向知识蒸馏方法，使大型语言模型从小型语言模型学习，通过MCL方法提升模型在特定任务中的性能。

<details>
  <summary>Details</summary>

**Motivation:** 激励在于结合小型模型在特定任务中的专业化优势和大型语言模型的丰富语义理解。传统知识蒸馏使得大型模型的知识转移到小型模型，但本文探讨了如何反向操作，以利用小型模型优化后的特定领域效果来提升大型模型的性能。

**Method:** 文章提出了一种反向知识蒸馏方法，其中大型语言模型（LLM）从小型语言模型（SLM）学习。通过重新解释LLM为编码器-解码器模式并使用LoRA技术，解决了LLM与小型编码器模型之间的架构差异。在训练过程中，编码器生成压缩表示及其相似度，这些表示与老师模型生成的相似度通过提出的Margin-aware Contrastive Learning (MCL)方法对齐。MCL确保了对正例和负例相似性的准确校准，并自适应地处理内部差异。

**Result:** 实验证明，通过本文提出的方法，大型语言模型在金融和医疗健康基准以及实际应用中的性能得到显著提升，且该方法已经被完全部署到在线环境中。

**Conclusion:** 研究表明反向知识蒸馏可以通过使用一个表现良好的小型模型来有效提升大型语言模型的性能，这对于提高特定任务的效果具有重要意义。

**Abstract:** Knowledge distillation typically involves transferring knowledge from a Large
Language Model (LLM) to a Smaller Language Model (SLM). However, in tasks such
as text matching, fine-tuned smaller models often yield more effective
domain-specific representations, as they focus on optimizing the similarity of
input pairs. To leverage both the specialized strengths of small models and the
rich semantic understanding of LLMs, we introduce a flipped knowledge
distillation paradigm, where LLM learns from SLM. Specifically, we address the
architectural gap between decoder-only LLMs and smaller encoder-based models by
reinterpreting LLMs in an encoder-decoder manner using LoRA. The encoder
generates compressed representations, while the decoder maps them to the output
space. During training, the encoder produces representations and their
similarities, which are then aligned with the similarity scores produced by the
teacher, using our proposed Margin-aware Contrastive Learning (MCL) approach.
The MCL ensures accurate similarity for both positive and negative pairs, and
adaptively handles the internal differences within positive and negative
samples. Our paradigm requires only a reasonably good-performing SLM, allowing
the LLM to achieve improved performance. Experiments on financial and
healthcare benchmarks, as well as real-world applications, confirm its
effectiveness, and the model has been fully deployed in an online environment.

</details>


### [22] [SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression](https://arxiv.org/abs/2507.05633)
*Yiqiao Jin,Kartik Sharma,Vineeth Rakesh,Yingtong Dou,Menghai Pan,Mahashweta Das,Srijan Kumar*

Main category: cs.CL

> SARA是一个统一的RAG框架，通过结合自然语言文本片段与语义压缩向量，在限制上下文长度的情况下提升局部精度和全局知识覆盖。它在9个数据集和5个开源LLMs上表现出显著的改进。

<details>
  <summary>Details</summary>

**Motivation:** 现有的检索增强生成（RAG）方法在扩展大型语言模型（LLMs）时面临挑战，即受限的有效上下文长度和检索文档的冗余性。仅基于压缩的方法能减少输入大小，但往往会丢弃对事实准确度至关重要的细粒度细节。SARA框架旨在解决这些问题。

**Method:** SARA框架结合了自然语言文本片段与语义压缩向量，以在受限的上下文预算下平衡局部精度和全局知识覆盖。该框架从两个互补层次表示上下文：细粒度的自然语言片段，保留关键实体和数值；紧凑的可解释向量，总结高层语义。迭代证据选择模块使用压缩向量对上下文进行动态重排序。

**Result:** 实验结果表明，在9个数据集和5个跨越Mistral、Llama和Gemma三个模型家族的开源LLMs上，SARA在答案相关性、正确性和语义相似性上分别提升了+17.71、+13.72和+15.53。这些结果证明了SARA框架的有效性和优势。

**Conclusion:** SARA框架展示了将文本和压缩表示集成起来对于实现稳健、上下文高效的RAG的重要性。该方法不仅提高了答案的相关性和正确性，还改善了语义相似性。

**Abstract:** Retrieval-augmented Generation (RAG) extends large language models (LLMs)
with external knowledge but faces key challenges: restricted effective context
length and redundancy in retrieved documents. Pure compression-based approaches
reduce input size but often discard fine-grained details essential for factual
accuracy. We propose SARA, a unified RAG framework that balances local
precision and global knowledge coverage under tight context budgets. SARA
combines natural-language text snippets with semantic compression vectors to
jointly enhance context efficiency and answer correctness. It represents
contexts at two complementary levels: 1) fine-grained natural-language spans
that preserve critical entities and numerical values, and 2) compact,
interpretable vectors that summarize high-level semantics. An iterative
evidence-selection module employs the compression vectors for dynamic reranking
of contexts. Across 9 datasets and 5 open-source LLMs spanning 3 model families
(Mistral, Llama, and Gemma), SARA consistently improves answer relevance
(+17.71), answer correctness (+13.72), and semantic similarity (+15.53),
demonstrating the importance of integrating textual and compressed
representations for robust, context-efficient RAG.

</details>


### [23] [ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?](https://arxiv.org/abs/2507.05639)
*Haoxin Wang,Xianhan Peng,Xucheng Huang,Yizhe Huang,Ming Gong,Chenghan Yang,Yang Liu,Ling Jiang*

Main category: cs.CL

> 论文提出ECom-Bench，这是首个针对具有多模态能力的LLM代理在电子商务客户服务领域表现的评估框架，显示出在复杂场景下当前模型面临较大挑战。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于没有专门针对具有多模态能力的LLM代理在电子商务客户服务领域表现的评估框架，论文的动机是填补这一研究空白，并为该领域的进一步研究和发展提供基础。

**Method:** 本论文介绍了ECom-Bench，该框架用于评估具有多模态能力的LLM代理在电子商务客户服务领域的表现。ECom-Bench的特点是基于从真实电子商务客户互动中收集的人设信息的动态用户模拟，以及一个源于真实电子商务对话的任务数据集。

**Result:** 通过使用ECom-Bench，研究发现即使是像GPT-4o这样的先进模型在复杂电子商务场景下的表现也只是10-20%的通过率，突显了这些场景的挑战性。

**Conclusion:** 论文结论指出，ECom-Bench提供了一个严格的评估基准，有助于识别现有模型的局限性以及未来改进的方向。代码和数据在发布后将开源，以促进更广泛的研究合作。

**Abstract:** In this paper, we introduce ECom-Bench, the first benchmark framework for
evaluating LLM agent with multimodal capabilities in the e-commerce customer
support domain. ECom-Bench features dynamic user simulation based on persona
information collected from real e-commerce customer interactions and a
realistic task dataset derived from authentic e-commerce dialogues. These
tasks, covering a wide range of business scenarios, are designed to reflect
real-world complexities, making ECom-Bench highly challenging. For instance,
even advanced models like GPT-4o achieve only a 10-20% pass^3 metric in our
benchmark, highlighting the substantial difficulties posed by complex
e-commerce scenarios. Upon publication, the code and data will be open-sourced
to facilitate further research and development in this domain.

</details>


### [24] [Smoothie-Qwen: Post-Hoc Smoothing to Reduce Language Bias in Multilingual LLMs](https://arxiv.org/abs/2507.05686)
*SeungWon Ji,Jungyup Lee,Jemin Kim,Sang Park,SeungJae Lee*

Main category: cs.CL

> Smoothie-Qwen is a lightweight, post-hoc method that mitigates language bias in multilingual large language models without requiring retraining.

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of language confusion in multilingual large language models, where responses are often generated in a dominant language, regardless of the prompt's language.

**Method:** Smoothie-Qwen, a lightweight post-hoc method that selectively adjusts token-level output probabilities to mitigate language bias without retraining.

**Result:** The method reduces unintended Chinese output by over 95% while preserving task accuracy on multilingual benchmarks.

**Conclusion:** This work provides a practical and efficient solution for enhancing the language controllability of LLMs, making them more reliable for global applications.

**Abstract:** Multilingual large language models (LLMs) often exhibit language confusion, a
tendency to generate responses in a dominant language irrespective of the
prompt's language. To address this, we propose Smoothie-Qwen, a lightweight,
post-hoc method that mitigates language bias without retraining. This technique
selectively adjusts token-level output probabilities to effectively suppress
undesired language generation. Applied to the Qwen model, our method reduces
unintended Chinese output by over 95% while preserving task accuracy on
multilingual benchmarks. This work provides a practical and efficient solution
for enhancing the language controllability of LLMs, making them more reliable
for global applications.

</details>


### [25] [Agentic-R1: Distilled Dual-Strategy Reasoning](https://arxiv.org/abs/2507.05707)
*Weihua Du,Pranjal Aggarwal,Sean Welleck,Yiming Yang*

Main category: cs.CL

> 提出DualDistill框架及Agentic-R1模型，旨在通过多策略蒸馏方法提高计算和逻辑任务的推理准确性和效率。

<details>
  <summary>Details</summary>

**Motivation:** 当前的长链思维模型在数学推理方面表现出色，但依赖于缓慢且易出错的自然语言跟踪。而工具增强型代理通过代码执行处理算术问题，但对于复杂的逻辑任务则不如人意。研究目的是解决这些问题。

**Method:** 介绍了一个微调框架DualDistill，该框架从多个教师模型中提炼出互补的推理策略，以训练出一个统一的学生模型Agentic-R1。该模型能够动态选择每项查询的最佳策略，对于算术和算法问题调用工具，而对于抽象问题则使用基于文本的推理。

**Result:** 使用这种方法训练的模型在多种任务中提高了准确性，包括计算密集型任务标准基准测试，展示了多策略蒸馏在实现稳健和有效推理方面的有效性。

**Conclusion:** 研究证明了多策略蒸馏方法的有效性，通过该方法训练的模型在多种任务中表现出更稳健和高效的推理性能。项目代码已开源。

**Abstract:** Current long chain-of-thought (long-CoT) models excel at mathematical
reasoning but rely on slow and error-prone natural language traces.
Tool-augmented agents address arithmetic via code execution, but often falter
on complex logical tasks. We introduce a fine-tuning framework, DualDistill,
that distills complementary reasoning strategies from multiple teachers into a
unified student model. Using this approach, we train Agentic-R1, which
dynamically selects the optimal strategy for each query, invoking tools for
arithmetic and algorithmic problems, and using text-based reasoning for
abstract ones. Our method improves accuracy across a range of tasks, including
both computation-intensive and standard benchmarks, demonstrating the
effectiveness of multi-strategy distillation in achieving robust and efficient
reasoning. Our project is available at https://github.com/StigLidu/DualDistill

</details>


### [26] [DRAGON: Dynamic RAG Benchmark On News](https://arxiv.org/abs/2507.05713)
*Fedor Chernogorskii,Sergei Averkiev,Liliya Kudraleeva,Zaven Martirosian,Maria Tikhonova,Valentin Malykh,Alena Fenogenova*

Main category: cs.CL

> DRAGON是一项针对俄语RAG系统的动态评估基准，适用于不断更新的新闻语料库。该基准使用知识图谱自动进行问题生成，并提供完整的评估框架和公共排行榜。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于现有的针对英语的RAG基准较多，但其他语言如俄语的评估资源稀缺且静态，无法捕捉实际部署中动态变化的特点，本研究旨在填补这一空白。

**Method:** 提出DRAGON（动态RAG基准测试——新闻），这是首个用于评估俄语环境中RAG系统在不断变化的新闻语料上的动态基准。该基准基于定期更新的俄语新闻和公共文档语料库，支持对检索器和生成器组件的全面评估。使用从语料库构建的知识图谱自动生成问题，能够提取四种核心问题类型，这些类型与不同的子图模式对齐。

**Result:** 推出了一个完整的评估框架，包括自动问题生成管道、评估脚本及基准数据，这些也许可以用于其他语言和多语言环境。同时，启动了公共排行榜以鼓励社区参与和比较。

**Conclusion:** DRAGON的提出促进了俄语环境中RAG系统的评估，提供了动态的评价方法，并以开放资源的形式释放了评估框架，为多语言环境的RAG系统评估提供了可能。

**Abstract:** Retrieval-Augmented Generation (RAG) is a widely adopted approach for
improving the factuality of large language models (LLMs) by incorporating
external knowledge at inference time. Although there exist multiple RAG
benchmarks for English, evaluation resources for other languages, including
Russian, remain scarce and static, failing to capture the dynamic nature of
real-world deployments.
  In this work, we present DRAGON (Dynamic RAG Benchmark On News), the first
dynamic benchmark for evaluating RAG systems in Russian on a changing news
corpora. DRAGON is built upon a regularly updated corpus of Russian news and
public documents and supports comprehensive evaluation of both the retriever
and generator components. Question generation is performed automatically with
the use of Knowledge Graph constructed from the corpus and enables the
extraction of four core question types aligned with distinct subgraph patterns.
We release a complete evaluation framework comprising the pipeline for
automatic question generation, evaluation scripts, which are potentially
reusable for other languages and multilingual settings, and benchmark data. We
also launch a public leaderboard to encourage community participation and
comparison.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [27] [Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)](https://arxiv.org/abs/2507.05300)
*Nicholas Merchant,Haitz Sáez de Ocáriz Borde,Andrei Cristian Popescu,Carlos Garcia Jurado Suarez*

Main category: cs.CV

> 为了解决现有的文本转图像模型中由于数据集原因而导致的提示词一致性问题，研究中引入了一种新的带有结构化标题的数据集Re-LAION-Caption 19M，并展示了它能够提升模型的文本-图像一致性。

<details>
  <summary>Details</summary>

**Motivation:** 文本转图像生成模型在大规模数据集（如LAION-5B）上训练时往往难以有效处理复杂的提示词。这使得用户需要依赖复杂的提示词工程以获得期望的输出。通过该研究，旨在通过训练中的结构化标题来改进这些问题并提升模型的可控性。

**Method:** 通过在训练中强制一致的标题结构来提高模型的可控性和一致性。引入了包含1900万张1024x1024图片和基于Mistral 7B Instruct的LLaVA-Next模型生成的标题的Re-LAION-Caption 19M数据集。每个标题遵循四个部分模板：主题、场景、美学和摄影细节。使用结构化和随机打乱的标题对PixArt-$\Sigma$和Stable Diffusion 2进行了微调。

**Result:** 结构化标题版本在使用视觉问答模型评价时，持续显示出更高的文本-图像一致性分数。

**Conclusion:** 结构化标题的使用可以显著提高模型的可控性和一致性。

**Abstract:** We argue that generative text-to-image models often struggle with prompt
adherence due to the noisy and unstructured nature of large-scale datasets like
LAION-5B. This forces users to rely heavily on prompt engineering to elicit
desirable outputs. In this work, we propose that enforcing a consistent caption
structure during training can significantly improve model controllability and
alignment. We introduce Re-LAION-Caption 19M, a high-quality subset of
Re-LAION-5B, comprising 19 million 1024x1024 images with captions generated by
a Mistral 7B Instruct-based LLaVA-Next model. Each caption follows a four-part
template: subject, setting, aesthetics, and camera details. We fine-tune
PixArt-$\Sigma$ and Stable Diffusion 2 using both structured and randomly
shuffled captions, and show that structured versions consistently yield higher
text-image alignment scores using visual question answering (VQA) models. The
dataset is publicly available at
https://huggingface.co/datasets/supermodelresearch/Re-LAION-Caption19M.

</details>


### [28] [CorrDetail: Visual Detail Enhanced Self-Correction for Face Forgery Detection](https://arxiv.org/abs/2507.05302)
*Binjia Zhou,Hengrui Lou,Lizhe Chen,Haoyuan Li,Dawei Luo,Shuai Chen,Jie Lei,Zunlei Feng,Yijun Bei*

Main category: cs.CV

> 文章介绍了一种改进的框架CorrDetail，通过视觉细节增强和自我修正机制提高面部伪造检测的准确性和解释能力。

<details>
  <summary>Details</summary>

**Motivation:** 针对现有伪造检测技术解释伪造细节能力不足或出现幻觉的问题，提出一种改进方案。

**Method:** CorrDetail框架通过改进的自我修正机制，利用视觉细节增强模块来提高面部伪造检测的准确性，同时结合融合决策策略来增强模型的辨别能力。

**Result:** 实验结果显示，CorrDetail不仅在性能上达到了最新技术的水平，而且能够在准确识别伪造细节的同时展现出强大的泛化能力。

**Conclusion:** CorrDetail框架通过融合视觉细节增强和自我修正机制，提高了面部伪造检测技术的解释能力和准确性。

**Abstract:** With the swift progression of image generation technology, the widespread
emergence of facial deepfakes poses significant challenges to the field of
security, thus amplifying the urgent need for effective deepfake
detection.Existing techniques for face forgery detection can broadly be
categorized into two primary groups: visual-based methods and multimodal
approaches. The former often lacks clear explanations for forgery details,
while the latter, which merges visual and linguistic modalities, is more prone
to the issue of hallucinations.To address these shortcomings, we introduce a
visual detail enhanced self-correction framework, designated CorrDetail, for
interpretable face forgery detection. CorrDetail is meticulously designed to
rectify authentic forgery details when provided with error-guided questioning,
with the aim of fostering the ability to uncover forgery details rather than
yielding hallucinated responses. Additionally, to bolster the reliability of
its findings, a visual fine-grained detail enhancement module is incorporated,
supplying CorrDetail with more precise visual forgery details. Ultimately, a
fusion decision strategy is devised to further augment the model's
discriminative capacity in handling extreme samples, through the integration of
visual information compensation and model bias reduction.Experimental results
demonstrate that CorrDetail not only achieves state-of-the-art performance
compared to the latest methodologies but also excels in accurately identifying
forged details, all while exhibiting robust generalization capabilities.

</details>


### [29] [YOLO-APD: Enhancing YOLOv8 for Robust Pedestrian Detection on Complex Road Geometries](https://arxiv.org/abs/2507.05376)
*Aquino Joctum,John Kandiri*

Main category: cs.CV

> 本文介绍了YOLO-APD，一种针对复杂路况（如S型弯道）行人检测优化的深度学习架构。该架构基于YOLOv8并引入了多项技术创新，提高了检测精度和效率，同时保持了实时处理能力。

<details>
  <summary>Details</summary>

**Motivation:** 标准基于RGB摄像头的方法在复杂路况下的行人检测性能有限，需要一种更健壮的检测系统以提高自动驾驶车辆在复杂环境中的感知能力。

**Method:** YOLO-APD架构集成了多项关键技术，包括无参数的SimAM注意机制、计算高效的C3Ghost模块、增强多尺度特征池化的SimSPPF模块、Mish激活函数以及网络瓶颈中优秀的特征融合IGD模块。此外，还利用车辆转向动力学进行自适应区域处理。

**Result:** 在模拟复杂场景的CARLA数据集上的评估结果显示，YOLO-APD达到了77.7%的mAP@0.5:0.95和超过96%的行人召回率，显著超越了包括YOLOv8在内的基准模型，同时保持了100 FPS的实时处理能力。

**Conclusion:** 本研究为开发基于低成本传感器的高度准确、高效和适应性强的感知系统提供了重要贡献，有助于提升自动驾驶车辆在复杂驾驶环境中的安全性和可靠性。

**Abstract:** Autonomous vehicle perception systems require robust pedestrian detection,
particularly on geometrically complex roadways like Type-S curved surfaces,
where standard RGB camera-based methods face limitations. This paper introduces
YOLO-APD, a novel deep learning architecture enhancing the YOLOv8 framework
specifically for this challenge. YOLO-APD integrates several key architectural
modifications: a parameter-free SimAM attention mechanism, computationally
efficient C3Ghost modules, a novel SimSPPF module for enhanced multi-scale
feature pooling, the Mish activation function for improved optimization, and an
Intelligent Gather & Distribute (IGD) module for superior feature fusion in the
network's neck. The concept of leveraging vehicle steering dynamics for
adaptive region-of-interest processing is also presented. Comprehensive
evaluations on a custom CARLA dataset simulating complex scenarios demonstrate
that YOLO-APD achieves state-of-the-art detection accuracy, reaching 77.7%
mAP@0.5:0.95 and exceptional pedestrian recall exceeding 96%, significantly
outperforming baseline models, including YOLOv8. Furthermore, it maintains
real-time processing capabilities at 100 FPS, showcasing a superior balance
between accuracy and efficiency. Ablation studies validate the synergistic
contribution of each integrated component. Evaluation on the KITTI dataset
confirms the architecture's potential while highlighting the need for domain
adaptation. This research advances the development of highly accurate,
efficient, and adaptable perception systems based on cost-effective sensors,
contributing to enhanced safety and reliability for autonomous navigation in
challenging, less-structured driving environments.

</details>


### [30] [Foreground-aware Virtual Staining for Accurate 3D Cell Morphological Profiling](https://arxiv.org/abs/2507.05383)
*Alexandr A. Kalinin,Paula Llanos,Theresa Maria Sommer,Giovanni Sestini,Xinhai Hou,Jonathan Z. Sexton,Xiang Wan,Ivo D. Dinov,Brian D. Athey,Nicolas Rivron,Anne E. Carpenter,Beth Cimini,Shantanu Singh,Matthew J. O'Meara*

Main category: cs.CV

> Spotlight，一种新的虚拟染色方法，通过聚焦在相关细胞结构上提升虚拟染色的生物信号表达精度，优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 本论文指出现有虚拟染色方法训练依赖的损失函数对待每个像素都同样处理，这会导致背景噪声和伪影的重现，而非专注于生物信号。从而提出Spotlight方法以解决这一问题。

**Method:** Spotlight方法采用基于直方图的前景估计来屏蔽像素级损失，并使用软阈值化预测来计算Dice损失，以此实现形态感知学习。

**Result:** 笯的内容似乎丢失了，请您提供有效的内容以便我进行分析。

**Conclusion:** 实验表明，应用于3D基准数据集时，Spotlight能够改善形态表示同时保持像素级准确性，生成更适合用于分割和特征分析的虚拟染色图。

**Abstract:** Microscopy enables direct observation of cellular morphology in 3D, with
transmitted-light methods offering low-cost, minimally invasive imaging and
fluorescence microscopy providing specificity and contrast. Virtual staining
combines these strengths by using machine learning to predict fluorescence
images from label-free inputs. However, training of existing methods typically
relies on loss functions that treat all pixels equally, thus reproducing
background noise and artifacts instead of focusing on biologically meaningful
signals. We introduce Spotlight, a simple yet powerful virtual staining
approach that guides the model to focus on relevant cellular structures.
Spotlight uses histogram-based foreground estimation to mask pixel-wise loss
and to calculate a Dice loss on soft-thresholded predictions for shape-aware
learning. Applied to a 3D benchmark dataset, Spotlight improves morphological
representation while preserving pixel-level accuracy, resulting in virtual
stains better suited for downstream tasks such as segmentation and profiling.

</details>


### [31] [From General to Specialized: The Need for Foundational Models in Agriculture](https://arxiv.org/abs/2507.05390)
*Vishal Nedungadi,Xingguo Xiong,Aike Potze,Ron Van Bree,Tao Lin,Marc Rußwurm,Ioannis N. Athanasiadis*

Main category: cs.CV

> 该研究探索基础模型在农业中的应用潜力，评估通用基础模型在农业任务中的表现，并强调需要开发专门的农业基础模型。

<details>
  <summary>Details</summary>

**Motivation:** 随着全球人口的增长和气候变化的加剧，粮食安全问题日益凸显。研究旨在探索如何利用基础模型的最新进展应对农业可持续发展的挑战。

**Method:** 研究首先定义了一个农业领域的基础模型需求框架，然后评估了两个通用基础模型在特定农业任务中的效果。

**Result:** 此次研究主要探讨了基础模型在农业领域的应用潜力，尤其是在监测和解决农业相关问题如作物类型识别、作物生长阶段估计以及作物产量预测等方面。研究构建了一个理想的农业基础模型（CropFM）需求框架，并评估了两个通用基础模型在三个农业特定任务的表现。研究强调了开发专门针对农业的基础模型的必要性。

**Conclusion:** 现有研究证明，基础模型有助于解决农业领域的挑战，但仍需专门开发适用于农业任务的模型以满足特定需求。

**Abstract:** Food security remains a global concern as population grows and climate change
intensifies, demanding innovative solutions for sustainable agricultural
productivity. Recent advances in foundation models have demonstrated remarkable
performance in remote sensing and climate sciences, and therefore offer new
opportunities for agricultural monitoring. However, their application in
challenges related to agriculture-such as crop type mapping, crop phenology
estimation, and crop yield estimation-remains under-explored. In this work, we
quantitatively evaluate existing foundational models to assess their
effectivity for a representative set of agricultural tasks. From an
agricultural domain perspective, we describe a requirements framework for an
ideal agricultural foundation model (CropFM). We then survey and compare
existing general-purpose foundational models in this framework and empirically
evaluate two exemplary of them in three representative agriculture specific
tasks. Finally, we highlight the need for a dedicated foundational model
tailored specifically to agriculture.

</details>


### [32] [Enhancing Underwater Images Using Deep Learning with Subjective Image Quality Integration](https://arxiv.org/abs/2507.05393)
*Jose M. Montero,Jose-Luis Lisani*

Main category: cs.CV

> This study uses deep learning to enhance underwater images, leveraging human assessments in the training phase, which results in substantial image quality improvements as measured by various metrics and visual assessment.

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to improve the quality of underwater images using a deep learning approach by integrating human subjective assessments into the training process.

**Method:** Our method involves first training a classifier network to distinguish between high- and low-quality underwater images, and then training GANs to refine low-quality images based on various enhancement criteria, such as color fidelity and image sharpness.

**Result:** The proposed model achieves significant improvements in underwater image quality, as demonstrated by both quantitative metrics (PSNR, SSIM, UIQM) and qualitative analysis.

**Conclusion:** Incorporating enhancement criteria such as color fidelity and image sharpness into the training of GANs can effectively enhance the quality of underwater images.

**Abstract:** Recent advances in deep learning, particularly neural networks, have
significantly impacted a wide range of fields, including the automatic
enhancement of underwater images. This paper presents a deep learning-based
approach to improving underwater image quality by integrating human subjective
assessments into the training process. To this end, we utilize publicly
available datasets containing underwater images labeled by experts as either
high or low quality. Our method involves first training a classifier network to
distinguish between high- and low-quality images. Subsequently, generative
adversarial networks (GANs) are trained using various enhancement criteria to
refine the low-quality images. The performance of the GAN models is evaluated
using quantitative metrics such as PSNR, SSIM, and UIQM, as well as through
qualitative analysis. Results demonstrate that the proposed model --
particularly when incorporating criteria such as color fidelity and image
sharpness -- achieves substantial improvements in both perceived and measured
image quality.

</details>


### [33] [pFedMMA: Personalized Federated Fine-Tuning with Multi-Modal Adapter for Vision-Language Models](https://arxiv.org/abs/2507.05394)
*Sajjad Ghiasvand,Mahnoosh Alizadeh,Ramtin Pedarsani*

Main category: cs.CV

> 该研究提出了一种新的个性化联邦学习框架pFedMMA，利用多模态适配器在视觉语言任务中实现了个性化和泛化能力的优良权衡。

<details>
  <summary>Details</summary>

**Motivation:** Vision-Language Models (VLMs)如CLIP在零样本和少量样本设置中展现了出色的泛化能力，但要有效地适应去中心化和异质数据仍然具挑战性。尽管prompt tuning作为个性化联邦学习中的流行参数有效方法，但现有方法往往以牺牲泛化能力为代价，特别在未见过的类或领域上表现不佳。

**Method:** 该研究提出了pFedMMA，一种利用多模态适配器的个性化联邦学习框架，用于视觉语言任务。每个适配器包含特定模态的上投影层和下投影层，以及一个全局共享的投影，用于对齐跨模态特征。该框架采用了非对称优化策略，允许客户端根据个性化数据分布进行局部适应，同时共同训练共享投影以改善全局泛化性。该设计是通信高效的，因为在轮次期间只交换共享组件。

**Result:** 通过在包括领域分布和标签转移场景在内的十一项不同数据集上的广泛实验，研究显示pFedMMA在个性化和泛化之间的权衡上实现了最先进表现，超过了近期的联邦prompt tuning方法。

**Conclusion:** 研究证明了pFedMMA在视觉语言任务中的有效性和高效性，特别是在跨多个数据集的个性化和泛化性能上达到了最先进水平。

**Abstract:** Vision-Language Models (VLMs) like CLIP have demonstrated remarkable
generalization in zero- and few-shot settings, but adapting them efficiently to
decentralized, heterogeneous data remains a challenge. While prompt tuning has
emerged as a popular parameter-efficient approach in personalized federated
learning, existing methods often sacrifice generalization in favor of
personalization, struggling particularly on unseen classes or domains. In this
work, we propose pFedMMA, the first personalized federated learning framework
that leverages multi-modal adapters for vision-language tasks. Each adapter
contains modality-specific up- and down-projection layers alongside a globally
shared projection that aligns cross-modal features. Our asymmetric optimization
strategy allows clients to locally adapt to personalized data distributions
while collaboratively training the shared projection to improve global
generalization. This design is also communication-efficient, as only the shared
component is exchanged during rounds. Through extensive experiments across
eleven datasets, including domain- and label-shift scenarios, we show that
pFedMMA achieves state-of-the-art trade-offs between personalization and
generalization, outperforming recent federated prompt tuning methods. The code
is available at https://github.com/sajjad-ucsb/pFedMMA.

</details>


### [34] [Neural-Driven Image Editing](https://arxiv.org/abs/2507.05397)
*Pengfei Zhou,Jie Xia,Xiaopeng Peng,Wangbo Zhao,Zilong Ye,Zekai Li,Suorong Yang,Jiadong Pan,Yuanxiang Chen,Ziqiao Wang,Kai Wang,Qian Zheng,Xiaojun Chang,Gang Pan,Shurong Dong,Kaipeng Zhang,Yang You*

Main category: cs.CV

> LoongX利用多种神经生理信号（如EEG、fNIRS、PPG和头部运动信号）进行无手操作图像编辑，通过CS3模块和DGF模块将这些信号融合，实现了接近文本驱动方法的性能，并且在结合语音信号时表现更优。

<details>
  <summary>Details</summary>

**Motivation:** 传统图像编辑依靠手动输入，对有运动控制或语言能力限制的用户来说是一项挑战。通过利用脑机接口和生成模型，LoongX旨在使图像编辑更加直观和易于访问。

**Method:** LoongX采用跨尺度状态空间(CS3)模块和动态门控融合(DGF)模块将多模态神经信号融合，通过对比学习预先训练编码器将认知状态与语义意图对齐，使用扩散转换器进行微调将融合的特征与编辑语义对齐。

**Result:** 实验结果表明，LoongX的性能与文本驱动的方法相当，即使得CLIP-I和DINO接近甚至超过，而将神经信号与语音结合使用时，LoongX表现更好，优于CLIP-T。

**Conclusion:** 这项研究展示了神经驱动的生成模型在使图像编辑更加易用和直观方面的潜力，同时也为进一步的认知驱动创造性技术研究提供了新的方向。

**Abstract:** Traditional image editing typically relies on manual prompting, making it
labor-intensive and inaccessible to individuals with limited motor control or
language abilities. Leveraging recent advances in brain-computer interfaces
(BCIs) and generative models, we propose LoongX, a hands-free image editing
approach driven by multimodal neurophysiological signals. LoongX utilizes
state-of-the-art diffusion models trained on a comprehensive dataset of 23,928
image editing pairs, each paired with synchronized electroencephalography
(EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography
(PPG), and head motion signals that capture user intent. To effectively address
the heterogeneity of these signals, LoongX integrates two key modules. The
cross-scale state space (CS3) module encodes informative modality-specific
features. The dynamic gated fusion (DGF) module further aggregates these
features into a unified latent space, which is then aligned with edit semantics
via fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train
the encoders using contrastive learning to align cognitive states with semantic
intentions from embedded natural language. Extensive experiments demonstrate
that LoongX achieves performance comparable to text-driven methods (CLIP-I:
0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural
signals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results
highlight the promise of neural-driven generative models in enabling
accessible, intuitive image editing and open new directions for
cognitive-driven creative technologies. Datasets and code will be released to
support future work and foster progress in this emerging area.

</details>


### [35] [Motion Generation: A Survey of Generative Approaches and Benchmarks](https://arxiv.org/abs/2507.05419)
*Aliasghar Khani,Arianna Rampini,Bruno Roy,Larasika Nadela,Noa Kaplan,Evan Atherton,Derek Cheung,Jacky Bibliowicz*

Main category: cs.CV

> 本文综述了2023年以来顶级会议中的运动生成方法，详细分析了生成策略、架构、条件机制、评估指标和数据集，旨在帮助研究从业者清晰比较并识别领域挑战。

<details>
  <summary>Details</summary>

**Motivation:** 随着GAN、自编码器、自回归模型和基于扩散的技术等多样化的建模范式的引入，运动生成领域的快速发展产生了对全面和结构化审查的需求，特别是从所用生成方法的角度进行的审查。

**Method:** 本文通过对生成策略的深入分类来评估运动生成方法，重点分析了2023年以来顶级会议中的论文，探讨了架构原则、条件机制和生成设置，并总结了文献中使用的评估指标和数据集。

**Result:** 本文提供了一个详细的运动生成方法综述，涵盖了最新的研究进展，使研究人员和从业者能够更清晰地进行比较，并识别出未解决的挑战。

**Conclusion:** 此综述旨在为运动生成领域内不断发展的研究和实践提供及时且基础性的参考文献。

**Abstract:** Motion generation, the task of synthesizing realistic motion sequences from
various conditioning inputs, has become a central problem in computer vision,
computer graphics, and robotics, with applications ranging from animation and
virtual agents to human-robot interaction. As the field has rapidly progressed
with the introduction of diverse modeling paradigms including GANs,
autoencoders, autoregressive models, and diffusion-based techniques, each
approach brings its own advantages and limitations. This growing diversity has
created a need for a comprehensive and structured review that specifically
examines recent developments from the perspective of the generative approach
employed.
  In this survey, we provide an in-depth categorization of motion generation
methods based on their underlying generative strategies. Our main focus is on
papers published in top-tier venues since 2023, reflecting the most recent
advancements in the field. In addition, we analyze architectural principles,
conditioning mechanisms, and generation settings, and compile a detailed
overview of the evaluation metrics and datasets used across the literature. Our
objective is to enable clearer comparisons and identify open challenges,
thereby offering a timely and foundational reference for researchers and
practitioners navigating the rapidly evolving landscape of motion generation.

</details>


### [36] [Mastering Regional 3DGS: Locating, Initializing, and Editing with Diverse 2D Priors](https://arxiv.org/abs/2507.05426)
*Lanqing Guo,Yufei Wang,Hezhen Hu,Yan Zheng,Yeying Jin,Siyu Huang,Zhangyang Wang*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Many 3D scene editing tasks focus on modifying local regions rather than the
entire scene, except for some global applications like style transfer, and in
the context of 3D Gaussian Splatting (3DGS), where scenes are represented by a
series of Gaussians, this structure allows for precise regional edits, offering
enhanced control over specific areas of the scene; however, the challenge lies
in the fact that 3D semantic parsing often underperforms compared to its 2D
counterpart, making targeted manipulations within 3D spaces more difficult and
limiting the fidelity of edits, which we address by leveraging 2D diffusion
editing to accurately identify modification regions in each view, followed by
inverse rendering for 3D localization, then refining the frontal view and
initializing a coarse 3DGS with consistent views and approximate shapes derived
from depth maps predicted by a 2D foundation model, thereby supporting an
iterative, view-consistent editing process that gradually enhances structural
details and textures to ensure coherence across perspectives. Experiments
demonstrate that our method achieves state-of-the-art performance while
delivering up to a $4\times$ speedup, providing a more efficient and effective
approach to 3D scene local editing.

</details>


### [37] [OpenWorldSAM: Extending SAM2 for Universal Image Segmentation with Language Prompts](https://arxiv.org/abs/2507.05427)
*Shiting Xiao,Rishabh Kabra,Yuhang Li,Donghyun Lee,Joao Carreira,Priyadarshini Panda*

Main category: cs.CV

> 研究提出OpenWorldSAM框架，通过结合轻量级视觉语言模型和预训练组件，实现了高效、灵活的开放词汇对象分割，展示出领先性能。

<details>
  <summary>Details</summary>

**Motivation:** 当前基于开放性语言提示的对象分割仍是挑战，需要模型将文本语义转化为精确的空间遮罩，同时处理多样和未见过的类别。

**Method:** 我们的方法通过整合轻量级视觉语言模型（VLM）提取的多模态嵌入，扩展了Segment Anything Model v2 (SAM2)框架，适应开放词汇场景。主要包含四大原则：统一提示、高效性、实例意识和泛化能力。

**Result:** 实验表明，OpenWorldSAM在多个基准测试中达到了最先进的开放词汇语义、实例和全景分割性能。

**Conclusion:** OpenWorldSAM框架展示了处理不同范围提示的能力，并且在未见过的类别上表现出强大的零样本泛化能力。

**Abstract:** The ability to segment objects based on open-ended language prompts remains a
critical challenge, requiring models to ground textual semantics into precise
spatial masks while handling diverse and unseen categories. We present
OpenWorldSAM, a framework that extends the prompt-driven Segment Anything Model
v2 (SAM2) to open-vocabulary scenarios by integrating multi-modal embeddings
extracted from a lightweight vision-language model (VLM). Our approach is
guided by four key principles: i) Unified prompting: OpenWorldSAM supports a
diverse range of prompts, including category-level and sentence-level language
descriptions, providing a flexible interface for various segmentation tasks.
ii) Efficiency: By freezing the pre-trained components of SAM2 and the VLM, we
train only 4.5 million parameters on the COCO-stuff dataset, achieving
remarkable resource efficiency. iii) Instance Awareness: We enhance the model's
spatial understanding through novel positional tie-breaker embeddings and
cross-attention layers, enabling effective segmentation of multiple instances.
iv) Generalization: OpenWorldSAM exhibits strong zero-shot capabilities,
generalizing well on unseen categories and an open vocabulary of concepts
without additional training. Extensive experiments demonstrate that
OpenWorldSAM achieves state-of-the-art performance in open-vocabulary semantic,
instance, and panoptic segmentation across multiple benchmarks, including
ADE20k, PASCAL, ScanNet, and SUN-RGBD.

</details>


### [38] [Robotic System with AI for Real Time Weed Detection, Canopy Aware Spraying, and Droplet Pattern Evaluation](https://arxiv.org/abs/2507.05432)
*Inayat Rasool,Pappu Kumar Yadav,Amee Parmar,Hasan Mirzakhaninafchi,Rikesh Budhathoki,Zain Ul Abideen Usmani,Supriya Paudel,Ivan Perez Olivera,Eric Jone*

Main category: cs.CV

> 开发了一种基于视觉和AI的可变喷洒系统，以检测杂草、估算冠层大小并动态调整喷嘴工作状态。系统使用YOLO11n和YOLO11n-seg模型，并通过Arduino Uno接口控制喷嘴。室内试验表明系统具备根据冠层大小调整喷洒量的能力。

<details>
  <summary>Details</summary>

**Motivation:** 现代农业中，农药的过量和均匀施用增加了投入成本，导致环境污染并促进了抗药性杂草的进化。为了应对这些挑战，开发了一个能够进行杂草检测、冠层大小估算以及动态调整喷嘴工作的视觉引导AI系统。

**Method:** 该系统集成了YOLO11n和YOLO11n-seg深度学习模型，运行在NVIDIA Jetson Orin Nano硬件平台上，使用Arduino Uno基线圈接口来控制喷嘴。

**Result:** 在室内使用15个不同大小的芋儿树进行试验，YOLO11n模型取得的平均精度(mAP@50)为0.98，精度为0.99，召回率接近1.0。YOLO11n-seg模型的mAP@50为0.48，精度为0.55，召回率为0.52。验证表明系统的喷水覆盖率与冠层大小相关，展示了系统调整喷洒强度的能力。

**Conclusion:** 结果证明，结合实时深度学习与低成本嵌入式硬件进行选择性农药喷洒的可能性。未来的研究将专注于识别南达科他州三种常见杂草，并在大豆和玉米生产系统中进行进一步验证。

**Abstract:** Uniform and excessive herbicide application in modern agriculture contributes
to increased input costs, environmental pollution, and the emergence of
herbicide resistant weeds. To address these challenges, we developed a vision
guided, AI-driven variable rate sprayer system capable of detecting weed
presence, estimating canopy size, and dynamically adjusting nozzle activation
in real time. The system integrates lightweight YOLO11n and YOLO11n-seg deep
learning models, deployed on an NVIDIA Jetson Orin Nano for onboard inference,
and uses an Arduino Uno-based relay interface to control solenoid actuated
nozzles based on canopy segmentation results. Indoor trials were conducted
using 15 potted Hibiscus rosa sinensis plants of varying canopy sizes to
simulate a range of weed patch scenarios. The YOLO11n model achieved a mean
average precision (mAP@50) of 0.98, with a precision of 0.99 and a recall close
to 1.0. The YOLO11n-seg segmentation model achieved a mAP@50 of 0.48, precision
of 0.55, and recall of 0.52. System performance was validated using water
sensitive paper, which showed an average spray coverage of 24.22% in zones
where canopy was present. An upward trend in mean spray coverage from 16.22%
for small canopies to 21.46% and 21.65% for medium and large canopies,
respectively, demonstrated the system's capability to adjust spray output based
on canopy size in real time. These results highlight the potential of combining
real time deep learning with low-cost embedded hardware for selective herbicide
application. Future work will focus on expanding the detection capabilities to
include three common weed species in South Dakota: water hemp (Amaranthus
tuberculatus), kochia (Bassia scoparia), and foxtail (Setaria spp.), followed
by further validation in both indoor and field trials within soybean and corn
production systems.

</details>


### [39] [Driving as a Diagnostic Tool: Scenario-based Cognitive Assessment in Older Drivers From Driving Video](https://arxiv.org/abs/2507.05463)
*Md Zahid Hasan,Guillermo Basulto-Elias,Jun Ha Chang,Sahuna Hallmark,Matthew Rizzo,Anuj Sharma,Soumik Sarkar*

Main category: cs.CV

> 通过自然驾驶视频和大规模视觉模型，本研究旨在分析老年人的驾驶行为，识别与MCI和AD相关的认知状态，以期实现早期认知衰退检测并为功能减退设计出早期干预措施。

<details>
  <summary>Details</summary>

**Motivation:** 由于目前诊断方法耗时且成本高，认知衰退（包括阿尔茨海默病和轻度认知障碍）经常被漏诊。此研究旨在通过分析车内系统捕捉的真实驾驶行为，实现早期识别认知衰退、减轻老化人口中认知衰退的社会和经济负担。

**Method:** 利用大规模视觉模型和自然驾驶视频来分析驾驶员行为，识别认知状态并预测疾病进展。该方法建立在车辆可作为“诊断工具”的基础上，通过分析驾驶员的实际驾驶行为来提取与功能减退和MCI、AD临床特征相关的“数字指纹”。

**Result:** 通过大规模视觉模型从老年人的日常驾驶模式中提取有意义的信息，早期检测认知衰退。该方法能够识别功能障碍的早期警告信号，有助于早干预策略。

**Conclusion:** 此研究推进了早期检测技术的发展，支持创建可扩展且非侵入性的监测系统，以减轻认知衰退对老龄化社会的影响。

**Abstract:** We introduce scenario-based cognitive status identification in older drivers
from Naturalistic driving videos and large vision models. In recent times,
cognitive decline, including Alzheimer's disease (AD) and mild cognitive
impairment (MCI), is often underdiagnosed due to the time-consuming and costly
nature of current diagnostic methods. By analyzing real-world driving behavior
captured through in-vehicle systems, this research aims to extract "digital
fingerprints" that correlate with functional decline and clinical features of
MCI and AD. Moreover, modern large vision models can draw meaningful insights
from everyday driving patterns of older patients to early detect cognitive
decline. We propose a framework that uses large vision models and naturalistic
driving videos to analyze driver behavior, classify cognitive status and
predict disease progression. We leverage the strong relationship between
real-world driving behavior as an observation of the current cognitive status
of the drivers where the vehicle can be utilized as a "diagnostic tool". Our
method identifies early warning signs of functional impairment, contributing to
proactive intervention strategies. This work enhances early detection and
supports the development of scalable, non-invasive monitoring systems to
mitigate the growing societal and economic burden of cognitive decline in the
aging population.

</details>


### [40] [Cloud Diffusion Part 1: Theory and Motivation](https://arxiv.org/abs/2507.05496)
*Andrew Randono*

Main category: cs.CV

> 论文提出了利用规模不变性噪音配置文件改进的Cloud Diffusion Model，以期提升图像生成技术的性能。

<details>
  <summary>Details</summary>

**Motivation:** 随着图像生成技术的发展，基于扩散模型的技术因逐步向图像中添加噪音并训练模型从中分离信号而受到关注。但经典的扩散模型使用的是白噪音，而真实世界中的自然图像拥有规模不变性，两者之间存在差异，因此研究提出了新的模型Cloud Diffusion Model来解决这个问题。

**Method:** 论文的创新点在于采用了与自然图像特性更匹配的规模不变性噪音配置文件来替代传统的白噪音。新模型在细节处理和模型控制方面可能带来改进。

**Result:** 该论文提出了一种称为'Cloud Diffusion Model'的图像生成模型。该模型利用与自然图像规模不变性相关的噪音配置文件，替代传统的白噪音。研究认为，这样的模型可以提高推理速度、改善高频细节，并增强可控性。在未来的工作中，作者将通过构建和训练Cloud Diffusion Model并将其与经典白噪声扩散模型进行比较来验证这一观点。

**Conclusion:** 研究结果表明，Cloud Diffusion Model作为一种更贴近自然图像特性的模型，能够加速推理，改进图像细节，并更容易控制。

**Abstract:** Diffusion models for image generation function by progressively adding noise
to an image set and training a model to separate out the signal from the noise.
The noise profile used by these models is white noise -- that is, noise based
on independent normal distributions at each point whose mean and variance is
independent of the scale. By contrast, most natural image sets exhibit a type
of scale invariance in their low-order statistical properties characterized by
a power-law scaling. Consequently, natural images are closer (in a quantifiable
sense) to a different probability distribution that emphasizes large scale
correlations and de-emphasizes small scale correlations. These scale invariant
noise profiles can be incorporated into diffusion models in place of white
noise to form what we will call a ``Cloud Diffusion Model". We argue that these
models can lead to faster inference, improved high-frequency details, and
greater controllability. In a follow-up paper, we will build and train a Cloud
Diffusion Model that uses scale invariance at a fundamental level and compare
it to classic, white noise diffusion models.

</details>


### [41] [LoomNet: Enhancing Multi-View Image Generation via Latent Space Weaving](https://arxiv.org/abs/2507.05499)
*Giulio Federico,Fabio Carrara,Claudio Gennaro,Giuseppe Amato,Marco Di Benedetto*

Main category: cs.CV

> LoomNet是一种可以高效生成高质量连贯多视图的新架构，实验表明其表现优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 从单一图像生成一致的多视图图像仍然是具有挑战性的。缺乏空间一致性会影响表面重构中3D网格的质量。本研究旨在解决这一问题。

**Method:** 我们提出了一种名为LoomNet的新多视图扩散架构，该架构通过多次并行应用相同的扩散模型来协同构建和利用共享的潜在空间以实现视图一致性。对于每个视角特定的推断，都会生成一个编码来表示它自己的假设的视图，该编码会被投影到三个正交平面上。对于每一个平面，所有视图的编码都被融合成一个综合的平面。这些综合平面随后被处理以传播信息和插补缺失的区域，从而将假设合并成一个统一、连贯的解释。

**Result:** LoomNet能够在短短15秒内生成16个高质量且连贯的视图。实验表明，与最先进的方法相比，LoomNet在图像质量和重构指标上都有优异表现，同时也展示了其创造性，能够从同一个输入生成多种合理的新视图。

**Conclusion:** 通过引入LoomNet，该研究成功地提高了多视图生成的连贯性和创造性，展现了其在高质量图像生成方面相比现有方法的优势。

**Abstract:** Generating consistent multi-view images from a single image remains
challenging. Lack of spatial consistency often degrades 3D mesh quality in
surface reconstruction. To address this, we propose LoomNet, a novel multi-view
diffusion architecture that produces coherent images by applying the same
diffusion model multiple times in parallel to collaboratively build and
leverage a shared latent space for view consistency. Each viewpoint-specific
inference generates an encoding representing its own hypothesis of the novel
view from a given camera pose, which is projected onto three orthogonal planes.
For each plane, encodings from all views are fused into a single aggregated
plane. These aggregated planes are then processed to propagate information and
interpolate missing regions, combining the hypotheses into a unified, coherent
interpretation. The final latent space is then used to render consistent
multi-view images. LoomNet generates 16 high-quality and coherent views in just
15 seconds. In our experiments, LoomNet outperforms state-of-the-art methods on
both image quality and reconstruction metrics, also showing creativity by
producing diverse, plausible novel views from the same input.

</details>


### [42] [Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model](https://arxiv.org/abs/2507.05513)
*Mengyao Xu,Gabriel Moreira,Ronay Ak,Radek Osmulski,Yauhen Babakhin,Zhiding Yu,Benedikt Schifferer,Even Oldridge*

Main category: cs.CV

> We introduce llama-nemoretriever-colembed, a text-image retrieval model that modifies NVIDIA's Eagle2 VLM and integrates ColBERT-style mechanisms, achieving state-of-the-art performance on ViDoRe benchmarks.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to meet the increasing demand for retrieval systems that work across different modalities by creating a unified text-image retrieval model.

**Method:** Our method modifies the NVIDIA Eagle2 Vision-Language model (VLM) by replacing causal attention with bidirectional attention and integrating a ColBERT-style late interaction mechanism, alongside a two-stage training strategy.

**Result:** The 3B model variant achieves state-of-the-art performance, leading both ViDoRe V1 and V2 leaderboards with NDCG@5 scores of 91.0 and 63.5 respectively.

**Conclusion:** Our approach provides superior retrieval accuracy across multiple benchmarks, but there are trade-offs in storage and efficiency which we analyze comprehensively.

**Abstract:** Motivated by the growing demand for retrieval systems that operate across
modalities, we introduce llama-nemoretriever-colembed, a unified text-image
retrieval model that delivers state-of-the-art performance across multiple
benchmarks. We release two model variants, 1B and 3B. The 3B model achieves
state of the art performance, scoring NDCG@5 91.0 on ViDoRe V1 and 63.5 on
ViDoRe V2, placing first on both leaderboards as of June 27, 2025.
  Our approach leverages the NVIDIA Eagle2 Vision-Language model (VLM),
modifies its architecture by replacing causal attention with bidirectional
attention, and integrates a ColBERT-style late interaction mechanism to enable
fine-grained multimodal retrieval in a shared embedding space. While this
mechanism delivers superior retrieval accuracy, it introduces trade-offs in
storage and efficiency. We provide a comprehensive analysis of these
trade-offs. Additionally, we adopt a two-stage training strategy to enhance the
model's retrieval capabilities.

</details>


### [43] [Simulating Refractive Distortions and Weather-Induced Artifacts for Resource-Constrained Autonomous Perception](https://arxiv.org/abs/2507.05536)
*Moseli Mots'oehli,Feimei Chen,Hok Wai Chan,Itumeleng Tlali,Thulani Babeli,Kyungim Baek,Huaijin Chen*

Main category: cs.CV

> 研究了一种适用于非洲驾驶场景的图像增强管道，包括折射和天气效果的模拟，以助于低成本自动驾驶算法的数据增强，并提供了相关的工具和基准测试结果。

<details>
  <summary>Details</summary>

**Motivation:** 自动驾驶汽车数据集在发展中国家尤其是非洲的多样化的城市、农村和未铺设的道路中的匮乏，是低资源环境下的感知研究的一大障碍。

**Method:** 我们提出了一种程序化增强管道，该管道能够增强低成本单目行车记录仪的录像，添加逼真的折射扭曲和由天气引起的特征，这些特征特别适用于具有挑战性的非洲驾驶场景。我们的折射模块模拟了来自低质量镜头和空气湍流的光学效果，包括镜头扭曲、Perlin噪声、薄板样条(TPS)和散度自由(不可压缩)变形。天气模块则添加了均匀的雾、非均匀的雾和镜头眩光。

**Result:** 为了建立一个基准，我们提供了三种图像恢复模型的基线性能。我们发布了一套畸变工具包、增强的数据集分割以及基准测试结果，以此来支持非洲欠代表性环境中感知研究的发展。

**Conclusion:** 可以通过低成本采集的数据和我们提供的工具提高自动驾驶算法在非洲地区典型环境中的性能，促进相关技术的发展。

**Abstract:** The scarcity of autonomous vehicle datasets from developing regions,
particularly across Africa's diverse urban, rural, and unpaved roads, remains a
key obstacle to robust perception in low-resource settings. We present a
procedural augmentation pipeline that enhances low-cost monocular dashcam
footage with realistic refractive distortions and weather-induced artifacts
tailored to challenging African driving scenarios. Our refractive module
simulates optical effects from low-quality lenses and air turbulence, including
lens distortion, Perlin noise, Thin-Plate Spline (TPS), and divergence-free
(incompressible) warps. The weather module adds homogeneous fog, heterogeneous
fog, and lens flare. To establish a benchmark, we provide baseline performance
using three image restoration models. To support perception research in
underrepresented African contexts, without costly data collection, labeling, or
simulation, we release our distortion toolkit, augmented dataset splits, and
benchmark results.

</details>


### [44] [ReLayout: Integrating Relation Reasoning for Content-aware Layout Generation with Multi-modal Large Language Models](https://arxiv.org/abs/2507.05568)
*Jiaxu Tian,Xuehui Yu,Yaoxing Wang,Pan Wang,Guangqian Guo,Shan Gao*

Main category: cs.CV

> ReLayout, a novel method using relation-CoT, enhances layout annotations through explicit relation definitions and introduces a layout prototype rebalance sampler to generate more structured and diverse layouts.

<details>
  <summary>Details</summary>

**Motivation:** To improve the structural coherence and diversity of automatically generated layouts using LLMs by addressing their limitations in interpreting spatial relationships.

**Method:** Content-aware layout using relation-CoT to enhance spatial relationships in layout generation.

**Result:** Experimental results show that ReLayout outperforms previous methods by producing more structurally coherent and diverse layouts that align better with human aesthetics.

**Conclusion:** ReLayout is an effective solution for generating aesthetically coherent and diverse layouts, overcoming the limitations of existing LLM-based layout methods.

**Abstract:** Content-aware layout aims to arrange design elements appropriately on a given
canvas to convey information effectively. Recently, the trend for this task has
been to leverage large language models (LLMs) to generate layouts
automatically, achieving remarkable performance. However, existing LLM-based
methods fail to adequately interpret spatial relationships among visual themes
and design elements, leading to structural and diverse problems in layout
generation. To address this issue, we introduce ReLayout, a novel method that
leverages relation-CoT to generate more reasonable and aesthetically coherent
layouts by fundamentally originating from design concepts. Specifically, we
enhance layout annotations by introducing explicit relation definitions, such
as region, salient, and margin between elements, with the goal of decomposing
the layout into smaller, structured, and recursive layouts, thereby enabling
the generation of more structured layouts. Furthermore, based on these defined
relationships, we introduce a layout prototype rebalance sampler, which defines
layout prototype features across three dimensions and quantifies distinct
layout styles. This sampler addresses uniformity issues in generation that
arise from data bias in the prototype distribution balance process. Extensive
experimental results verify that ReLayout outperforms baselines and can
generate structural and diverse layouts that are more aligned with human
aesthetics and more explainable.

</details>


### [45] [Multi-Modal Face Anti-Spoofing via Cross-Modal Feature Transitions](https://arxiv.org/abs/2507.05575)
*Jun-Xiong Chong,Fang-Yu Hsu,Ming-Tsung Hsu,Yi-Ting Lin,Kai-Heng Chien,Chiou-Ting Hsu,Pei-Kai Huang*

Main category: cs.CV

> 提出了CTNet来解决多模态人脸识别中活体检测的问题。通过学习跨模态特征转换的一致性和不一致性来检测活体和伪造人脸，同时处理缺失模态的问题。该方法在多个协议中表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 动机来源于单模态中真实人脸样本之间的视觉差异通常远小于伪造样本。此外，真实样本之间的跨模态特征转换比真实-伪造之间的转换更一致。这些特性被用来指导CTNet的设计。

**Method:** 提出了一种新的跨模态转换指导网络（CTNet）来解决多模态人脸识别中活体检测的挑战。首先，通过学习单模态中真实人脸样本之间的跨模态特征转换来构建一个泛化的特征空间。接着，通过学习真实和伪造样本之间的跨模态特征转换不一致性来有效检测推理阶段的OOD攻击。为了进一步解决模态缺失的问题，提出学习从RGB模式中获取额外的红外和深度特征作为辅助模态。

**Result:** 实验结果显示，提出的CTNet在大多数协议下优于以前的双分类多模态活体检测方法。

**Conclusion:** 提出的CTNet在多模态活体检测任务中优于之前的两分类方法，尤其在处理分布不一致和模态缺失的情况下表现更好。

**Abstract:** Multi-modal face anti-spoofing (FAS) aims to detect genuine human presence by
extracting discriminative liveness cues from multiple modalities, such as RGB,
infrared (IR), and depth images, to enhance the robustness of biometric
authentication systems. However, because data from different modalities are
typically captured by various camera sensors and under diverse environmental
conditions, multi-modal FAS often exhibits significantly greater distribution
discrepancies across training and testing domains compared to single-modal FAS.
Furthermore, during the inference stage, multi-modal FAS confronts even greater
challenges when one or more modalities are unavailable or inaccessible. In this
paper, we propose a novel Cross-modal Transition-guided Network (CTNet) to
tackle the challenges in the multi-modal FAS task. Our motivation stems from
that, within a single modality, the visual differences between live faces are
typically much smaller than those of spoof faces. Additionally, feature
transitions across modalities are more consistent for the live class compared
to those between live and spoof classes. Upon this insight, we first propose
learning consistent cross-modal feature transitions among live samples to
construct a generalized feature space. Next, we introduce learning the
inconsistent cross-modal feature transitions between live and spoof samples to
effectively detect out-of-distribution (OOD) attacks during inference. To
further address the issue of missing modalities, we propose learning
complementary infrared (IR) and depth features from the RGB modality as
auxiliary modalities. Extensive experiments demonstrate that the proposed CTNet
outperforms previous two-class multi-modal FAS methods across most protocols.

</details>


### [46] [Semi-Supervised Defect Detection via Conditional Diffusion and CLIP-Guided Noise Filtering](https://arxiv.org/abs/2507.05588)
*Shuai Li,Shihan Chen,Wanru Geng,Zhaohua Xu,Xiaolu Liu,Can Dong,Zhen Tian,Changlin Chen*

Main category: cs.CV

> 该论文介绍了基于条件扩散模型（DSYM）的半监督缺陷检测框架，以提高工业质量检查中的数据效率和精度，实验结果表明该方法在标注数据较少的情况下也能达到较高的检测精度。

<details>
  <summary>Details</summary>

**Motivation:** 传统的缺陷检测方法依赖手动检查或早期图像处理算法，存在效率低、成本高和鲁棒性差的问题，因此提出了一个新的半监督方法以解决这些问题。

**Method:** 框架采用了带有伪标签的未标注数据协同训练和阶段联合优化策略，使用标签数据进行初始训练，并通过生成伪标签引入未标注数据，实现多尺度伪缺陷样本的生成和标签污染的缓解。

**Result:** 实验结果表明，与传统监督方法相比，该框架可以达到78.4%的mAP@0.5，在只有40%的标注数据情况下，仍能达到75.1%的mAP@0.5。

**Conclusion:** 该研究提供了一种高精度、低依赖标签的解决方案，适用于工业质量检查场景，代码已在GitHub开源。

**Abstract:** In the realm of industrial quality inspection, defect detection stands as a
critical component, particularly in high-precision, safety-critical sectors
such as automotive components aerospace, and medical devices. Traditional
methods, reliant on manual inspection or early image processing algorithms,
suffer from inefficiencies, high costs, and limited robustness. This paper
introduces a semi-supervised defect detection framework based on conditional
diffusion (DSYM), leveraging a two-stage collaborative training mechanism and a
staged joint optimization strategy. The framework utilizes labeled data for
initial training and subsequently incorporates unlabeled data through the
generation of pseudo-labels. A conditional diffusion model synthesizes
multi-scale pseudo-defect samples, while a CLIP cross-modal feature-based noise
filtering mechanism mitigates label contamination. Experimental results on the
NEU-DET dataset demonstrate a 78.4% mAP@0.5 with the same amount of labeled
data as traditional supervised methods, and 75.1% mAP@0.5 with only 40% of the
labeled data required by the original supervised model, showcasing significant
advantages in data efficiency. This research provides a high-precision,
low-labeling-dependent solution for defect detection in industrial quality
inspection scenarios. The work of this article has been open-sourced at
https://github.com/cLin-c/Semisupervised-DSYM.

</details>


### [47] [GSVR: 2D Gaussian-based Video Representation for 800+ FPS with Hybrid Deformation Field](https://arxiv.org/abs/2507.05594)
*Zhizhuo Pang,Zhihui Ke,Xiaobo Zhou,Tie Qiu*

Main category: cs.CV

> GSVR, a 2D Gaussian-based video representation, is introduced to improve decoding speed while maintaining high video quality. It reaches 800+ FPS and 35+ PSNR on Bunny with a 2-second training time per frame, outperforming existing methods in speed and efficiency and showing promise in interpolation and compression tasks.

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing video representation methods with convolutional networks, which feature slow decoding speeds and long training times, GSVR aims to offer a faster, more efficient alternative that maintains high video quality.

**Method:** GSVR uses a 2D Gaussian-based representation, a hybrid deformation field combining tri-plane and polynomial motion patterns, and a Dynamic-aware Time Slicing strategy to improve upon conventional video representation methods. It also employs quantization-aware fine-tuning and image codecs for compression.

**Result:** Experiments confirm that GSVR not only converges significantly faster and has 10x the decoding speed of existing methods but also performs comparably in video interpolation and outperforms NeRV in video compression.

**Conclusion:** The proposed GSVR method offers a substantial improvement over existing video representation methods with increased decoding speed, lower training time, and efficient compression. It presents a promising approach for future video technology advancements.

**Abstract:** Implicit neural representations for video have been recognized as a novel and
promising form of video representation. Existing works pay more attention to
improving video reconstruction quality but little attention to the decoding
speed. However, the high computation of convolutional network used in existing
methods leads to low decoding speed. Moreover, these convolution-based video
representation methods also suffer from long training time, about 14 seconds
per frame to achieve 35+ PSNR on Bunny. To solve the above problems, we propose
GSVR, a novel 2D Gaussian-based video representation, which achieves 800+ FPS
and 35+ PSNR on Bunny, only needing a training time of $2$ seconds per frame.
Specifically, we propose a hybrid deformation field to model the dynamics of
the video, which combines two motion patterns, namely the tri-plane motion and
the polynomial motion, to deal with the coupling of camera motion and object
motion in the video. Furthermore, we propose a Dynamic-aware Time Slicing
strategy to adaptively divide the video into multiple groups of pictures(GOP)
based on the dynamic level of the video in order to handle large camera motion
and non-rigid movements. Finally, we propose quantization-aware fine-tuning to
avoid performance reduction after quantization and utilize image codecs to
compress Gaussians to achieve a compact representation. Experiments on the
Bunny and UVG datasets confirm that our method converges much faster than
existing methods and also has 10x faster decoding speed compared to other
methods. Our method has comparable performance in the video interpolation task
to SOTA and attains better video compression performance than NeRV.

</details>


### [48] [PaddleOCR 3.0 Technical Report](https://arxiv.org/abs/2507.05595)
*Cheng Cui,Ting Sun,Manhui Lin,Tingquan Gao,Yubo Zhang,Jiaxuan Liu,Xueqing Wang,Zelun Zhang,Changda Zhou,Hongen Liu,Yue Zhang,Wenyu Lv,Kui Huang,Yichao Zhang,Jing Zhang,Jun Zhang,Yi Liu,Dianhai Yu,Yanjun Ma*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** This technical report introduces PaddleOCR 3.0, an Apache-licensed
open-source toolkit for OCR and document parsing. To address the growing demand
for document understanding in the era of large language models, PaddleOCR 3.0
presents three major solutions: (1) PP-OCRv5 for multilingual text recognition,
(2) PP-StructureV3 for hierarchical document parsing, and (3) PP-ChatOCRv4 for
key information extraction. Compared to mainstream vision-language models
(VLMs), these models with fewer than 100 million parameters achieve competitive
accuracy and efficiency, rivaling billion-parameter VLMs. In addition to
offering a high-quality OCR model library, PaddleOCR 3.0 provides efficient
tools for training, inference, and deployment, supports heterogeneous hardware
acceleration, and enables developers to easily build intelligent document
applications.

</details>
