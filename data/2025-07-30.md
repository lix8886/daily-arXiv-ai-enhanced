<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 23]
- [cs.CV](#cs.CV) [Total: 26]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Categorical Classification of Book Summaries Using Word Embedding Techniques](https://arxiv.org/abs/2507.21058)
*Kerem Keskin,Mümine Kaya Keleş*

Main category: cs.CL

> 研究使用了词嵌入技术，自然语言处理技术和机器学习算法对图书摘要和分类进行了分类，并发现对于土耳其文本，SVM、朴素贝叶斯、逻辑回归模型和TF-IDF、One-Hot编码技术效果更好。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于利用词嵌入技术和机器学习算法对图书摘要及类别进行分类，并比较不同词嵌入方法的成功率。

**Method:** 本研究采用了词嵌入方法（包括One-Hot编码，Word2Vec和TF-IDF），自然语言处理技术和机器学习算法对来自图书网站的图书摘要和类别进行了分类。

**Result:** 结果表明，支持向量机、朴素贝叶斯和逻辑回归模型以及TF-IDF和One-Hot编码词嵌入技术在土耳其文本上表现更佳。

**Conclusion:** 通过结果，可以看出对于土耳其文本的支持向量机、朴素贝叶斯和逻辑回归模型，以及TF-IDF和One-Hot编码词嵌入技术表现更佳。

**Abstract:** In this study, book summaries and categories taken from book sites were
classified using word embedding methods, natural language processing techniques
and machine learning algorithms. In addition, one hot encoding, Word2Vec and
Term Frequency - Inverse Document Frequency (TF-IDF) methods, which are
frequently used word embedding methods were used in this study and their
success was compared. Additionally, the combination table of the pre-processing
methods used is shown and added to the table. Looking at the results, it was
observed that Support Vector Machine, Naive Bayes and Logistic Regression
Models and TF-IDF and One-Hot Encoder word embedding techniques gave more
successful results for Turkish texts.

</details>


### [2] [Dialogic Social Learning for Artificial Agents: Enhancing LLM Ontology Acquisition through Mixed-Initiative Educational Interactions](https://arxiv.org/abs/2507.21065)
*Sabrina Patania,Luca Annese,Cansu Koyuturk,Azzurra Ruggeri,Dimitri Ognibene*

Main category: cs.CL

> 研究开发了一个动态环境，通过社会化的教学对话彰显了对大语言模型的学习改进，通过对社会中介学习范式的探讨，以期提高其获取知识的能力。

<details>
  <summary>Details</summary>

**Motivation:** 传统的AI训练范式，主要基于监督学习或强化学习，成为类似于'皮亚杰式的'独立探索模型。这些方法通常依赖大型数据集和稀疏反馈信号，限制了模型利用交互有效学习的能力。该研究旨在探索基于维果茨基的社会文化理论的中介社会化学习范式解决这些局限的潜力。

**Method:** 本研究开发了一个名为 'AI 社交竞技场' 的动态环境，其中AI学习代理与知识丰富的AI教师代理进行二元教学对话。这些互动强调以结构化的对话为核心机制来获取知识，而不是仅仅依赖内部推理或模式识别。研究关注了不同的教学策略对AI在本体论知识获取过程中学习的影响。

**Result:** 实证结果显示，这种对话方法，特别是结合自上而下的解释与学习者发起的问题以混合方向交互的方式，显著增强了大语言模型获取和应用新知识的能力，超越了单向教学方法和直接访问结构化知识的方法。

**Conclusion:** 这些发现表明，将教育学和心理学见解融入AI和机器人培训中可以显著提高训练后的知识获取和响应质量。此方法为诸如提示工程等现有策略提供了一条补充路线。

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities in
processing extensive offline datasets. However, they often face challenges in
acquiring and integrating complex, knowledge online. Traditional AI training
paradigms, predominantly based on supervised learning or reinforcement
learning, mirror a 'Piagetian' model of independent exploration. These
approaches typically rely on large datasets and sparse feedback signals,
limiting the models' ability to learn efficiently from interactions. Drawing
inspiration from Vygotsky's sociocultural theory, this study explores the
potential of socially mediated learning paradigms to address these limitations.
  We introduce a dynamic environment, termed the 'AI Social Gym', where an AI
learner agent engages in dyadic pedagogical dialogues with knowledgeable AI
teacher agents. These interactions emphasize external, structured dialogue as a
core mechanism for knowledge acquisition, contrasting with methods that depend
solely on internal inference or pattern recognition.
  Our investigation focuses on how different pedagogical strategies impact the
AI learning process in the context of ontology acquisition. Empirical results
indicate that such dialogic approaches-particularly those involving
mixed-direction interactions combining top-down explanations with
learner-initiated questioning-significantly enhance the LLM's ability to
acquire and apply new knowledge, outperforming both unidirectional
instructional methods and direct access to structured knowledge, formats
typically present in training datasets.
  These findings suggest that integrating pedagogical and psychological
insights into AI and robot training can substantially improve post-training
knowledge acquisition and response quality. This approach offers a
complementary pathway to existing strategies like prompt engineering

</details>


### [3] [Product vs. Process: Exploring EFL Students' Editing of AI-Generated Text for Expository Writing](https://arxiv.org/abs/2507.21073)
*David James Woo,Yangyang Yu,Kai Guo,Yilin Huang,April Ka Yeng Fung*

Main category: cs.CL

> The study investigates the impact of AI text editing by EFL students on composition quality.

<details>
  <summary>Details</summary>

**Motivation:** The study aims to explore how EFL secondary students edit AI-generated text and the impact of these editing behaviors on the quality of their expository compositions.

**Method:** This research employs a convergent design to analyze screen recordings and compositions of EFL secondary students editing AI-generated text, utilizing qualitative coding, descriptive statistics, temporal sequence analysis, human-rated scoring, and multiple linear regression analysis.

**Result:** MLR analyses indicate a positive correlation between the number of AI-generated words and all score dimensions, while most editing variables show minimal impact on composition quality.

**Conclusion:** The findings suggest a disconnect between students' editing efforts and improvements in composition quality, indicating AI supports but does not replace writing skills. It emphasizes the need for genre-specific instruction and process-focused writing before integrating AI, alongside developing assessments that value both process and product.

**Abstract:** Text generated by artificial intelligence (AI) chatbots is increasingly used
in English as a foreign language (EFL) writing contexts, yet its impact on
students' expository writing process and compositions remains understudied.
This research examines how EFL secondary students edit AI-generated text.
Exploring editing behaviors in their expository writing process and in
expository compositions, and their effect on human-rated scores for content,
organization, language, and overall quality. Participants were 39 Hong Kong
secondary students who wrote an expository composition with AI chatbots in a
workshop. A convergent design was employed to analyze their screen recordings
and compositions to examine students' editing behaviors and writing qualities.
Analytical methods included qualitative coding, descriptive statistics,
temporal sequence analysis, human-rated scoring, and multiple linear regression
analysis. We analyzed over 260 edits per dataset, and identified two editing
patterns: one where students refined introductory units repeatedly before
progressing, and another where they quickly shifted to extensive edits in body
units (e.g., topic and supporting sentences). MLR analyses revealed that the
number of AI-generated words positively predicted all score dimensions, while
most editing variables showed minimal impact. These results suggest a
disconnect between students' significant editing effort and improved
composition quality, indicating AI supports but does not replace writing
skills. The findings highlight the importance of genre-specific instruction and
process-focused writing before AI integration. Educators should also develop
assessments valuing both process and product to encourage critical engagement
with AI text.

</details>


### [4] [Which symbol grounding problem should we try to solve?](https://arxiv.org/abs/2507.21080)
*Vincent C. Müller*

Main category: cs.CL

> 文章批评了Floridi和Taddeo提出的“零语义承诺”条件，并提出需要重新定义接地问题，以解决人工计算代理的行为能力和意义功能的解释与再现问题。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于评估Floridi和Taddeo提出的“零语义承诺”条件和解决方案的有效性，并探讨不同的方法论以解决接地问题。

**Method:** 本文通过分析Floridi和Taddeo提出的“零语义承诺”条件及其解决方案，指出该条件无法实现，并进一步探讨了Luc Steels的不同建议。通过对计算的恰当理解，作者重新审视目标在系统中所起的作用以及如何定义接地问题。

**Result:** 作者认为唯一合理的接地问题是，如何解释和再现人工计算代理中的行为能力和意义功能。

**Conclusion:** 作者得出结论，需要重新思考接地问题的定义以及系统中“目标”的作用，以寻求解决此问题的有效方法。

**Abstract:** Floridi and Taddeo propose a condition of "zero semantic commitment" for
solutions to the grounding problem, and a solution to it. I argue briefly that
their condition cannot be fulfilled, not even by their own solution. After a
look at Luc Steels' very different competing suggestion, I suggest that we need
to re-think what the problem is and what role the 'goals' in a system play in
formulating the problem. On the basis of a proper understanding of computing, I
come to the conclusion that the only sensible grounding problem is how we can
explain and re-produce the behavioral ability and function of meaning in
artificial computational agents

</details>


### [5] [ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs](https://arxiv.org/abs/2507.21083)
*Franck Bardol*

Main category: cs.CL

> 研究分析了GPT-4对情感调整问题的响应方式，发现GPT-4在面对负面问题时更倾向于采取中立或积极态度，特别是在敏感话题上，这表明模型存在一种情绪过度矫正现象。

<details>
  <summary>Details</summary>

**Motivation:** 研究大型语言模型如GPT-4如何根据问题的情感语气调整其回应，探索其情感处理和响应模式的偏差。

**Method:** 通过系统地改变156个提示语的情绪语气，探讨了GPT-4对不同情感调节问题的回答方式。这些提示语涵盖了有争议的主题和日常话题。

**Result:** GPT-4对情绪负面的提问比对中性提问更少提供负面回答，表现出所谓的“反弹”偏差，并在使用语调-价值转换矩阵和基于1536维嵌入的可视化来量化行为时得到了证实。

**Conclusion:** 揭示了情感提示所驱动的一种未被充分探索的偏差，这类偏差对于AI的对齐和信任有着重要的意义。

**Abstract:** Large Language Models like GPT-4 adjust their responses not only based on the
question asked, but also on how it is emotionally phrased. We systematically
vary the emotional tone of 156 prompts - spanning controversial and everyday
topics - and analyze how it affects model responses. Our findings show that
GPT-4 is three times less likely to respond negatively to a negatively framed
question than to a neutral one. This suggests a "rebound" bias where the model
overcorrects, often shifting toward neutrality or positivity. On sensitive
topics (e.g., justice or politics), this effect is even more pronounced:
tone-based variation is suppressed, suggesting an alignment override. We
introduce concepts like the "tone floor" - a lower bound in response negativity
- and use tone-valence transition matrices to quantify behavior. Visualizations
based on 1536-dimensional embeddings confirm semantic drift based on tone. Our
work highlights an underexplored class of biases driven by emotional framing in
prompts, with implications for AI alignment and trust. Code and data are
available at: https://github.com/bardolfranck/llm-responses-viewer

</details>


### [6] [Reviving Your MNEME: Predicting The Side Effects of LLM Unlearning and Fine-Tuning via Sparse Model Diffing](https://arxiv.org/abs/2507.21084)
*Aly M. Kassem,Zhuan Shi,Negar Rostamzadeh,Golnoosh Farnadi*

Main category: cs.CL

> 提出MNEME框架，用于检测LLM在微调或卸载后的意外副作用，提高了对这类模型变化的理解。

<details>
  <summary>Details</summary>

**Motivation:** 现有的评估方法无法检测LLM微调或卸载后不可预见的副作用。例如，卸载生物学知识可能会影响化学任务的表现。因此，提出了MNEME框架来检测这些副作用。

**Method:** 引入了MNEME（Model diffiNg for Evaluating Mechanistic Effects），一个轻量级框架，通过稀疏模型差异比较来识别LLM微调或卸载后的意外副作用。MNEME比较基线模型与微调后的模型在任务无关的数据上的表现，以隔离行为变化，无需访问微调数据。

**Result:** MNEME在三种场景下应用于五个LLM，准确率达到95%，表明稀疏探测和比较能够提供一个可扩展且自动化的视角来理解微调引发的模型变化。此外，结果显示在高激活样本上进行再训练可以部分逆转这些效应。

**Conclusion:** 研究结果表明，稀疏探测和比较方法为理解和管理LLM行为提供了实用工具，并展示了高激活样本上的再训练可以部分逆转这些效应。

**Abstract:** Large language models (LLMs) are frequently fine-tuned or unlearned to adapt
to new tasks or eliminate undesirable behaviors. While existing evaluation
methods assess performance after such interventions, there remains no general
approach for detecting unintended side effects, such as unlearning biology
content degrading performance on chemistry tasks, particularly when these
effects are unpredictable or emergent. To address this issue, we introduce
MNEME, Model diffiNg for Evaluating Mechanistic Effects, a lightweight
framework for identifying these side effects using sparse model diffing. MNEME
compares base and fine-tuned models on task-agnostic data (for example, The
Pile, LMSYS-Chat-1M) without access to fine-tuning data to isolate behavioral
shifts. Applied to five LLMs across three scenarios: WMDP knowledge unlearning,
emergent misalignment, and benign fine-tuning, MNEME achieves up to 95 percent
accuracy in predicting side effects, aligning with known benchmarks and
requiring no custom heuristics. Furthermore, we show that retraining on
high-activation samples can partially reverse these effects. Our results
demonstrate that sparse probing and diffing offer a scalable and automated lens
into fine-tuning-induced model changes, providing practical tools for
understanding and managing LLM behavior.

</details>


### [7] [Multi-Amateur Contrastive Decoding for Text Generation](https://arxiv.org/abs/2507.21086)
*Jaydip Sen,Subhasis Dasgupta,Hetvi Waghela*

Main category: cs.CL

> MACD, a generalization of Contrastive Decoding, uses multiple amateur models to better identify and mitigate failure modes in language generation, resulting in improved text quality without additional training.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to overcome the limitation of Contrastive Decoding (CD) that depends on a single amateur model, which restricts its ability to capture diverse failure modes of text generation.

**Method:** This paper proposes Multi-Amateur Contrastive Decoding (MACD), which uses an ensemble of amateur models to identify undesirable generation patterns in text. It integrates contrastive signals using averaging and consensus penalization, and enhances controllability by integrating targeted stylistic or content biases.

**Result:** Experimental results show that MACD surpasses traditional decoding methods and CD in fluency, coherence, diversity, and adaptability, without the need for additional training.

**Conclusion:** MACD provides a more robust and versatile solution for enhancing open-ended text generation, capturing a wider range of undesirable patterns and allowing for controllable generation.

**Abstract:** Contrastive Decoding (CD) has emerged as an effective inference-time strategy
for enhancing open-ended text generation by exploiting the divergence in output
probabilities between a large expert language model and a smaller amateur
model. Although CD improves coherence and fluency, its dependence on a single
amateur restricts its capacity to capture the diverse and multifaceted failure
modes of language generation, such as repetition, hallucination, and stylistic
drift. This paper proposes Multi-Amateur Contrastive Decoding (MACD), a
generalization of the CD framework that employs an ensemble of amateur models
to more comprehensively characterize undesirable generation patterns. MACD
integrates contrastive signals through both averaging and consensus
penalization mechanisms and extends the plausibility constraint to operate
effectively in the multi-amateur setting. Furthermore, the framework enables
controllable generation by incorporating amateurs with targeted stylistic or
content biases. Experimental results across multiple domains, such as news,
encyclopedic, and narrative, demonstrate that MACD consistently surpasses
conventional decoding methods and the original CD approach in terms of fluency,
coherence, diversity, and adaptability, all without requiring additional
training or fine-tuning.

</details>


### [8] [QU-NLP at CheckThat! 2025: Multilingual Subjectivity in News Articles Detection using Feature-Augmented Transformer Models with Sequential Cross-Lingual Fine-Tuning](https://arxiv.org/abs/2507.21095)
*Mohammad AL-Smadi*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** This paper presents our approach to the CheckThat! 2025 Task 1 on
subjectivity detection, where systems are challenged to distinguish whether a
sentence from a news article expresses the subjective view of the author or
presents an objective view on the covered topic. We propose a feature-augmented
transformer architecture that combines contextual embeddings from pre-trained
language models with statistical and linguistic features. Our system leveraged
pre-trained transformers with additional lexical features: for Arabic we used
AraELECTRA augmented with part-of-speech (POS) tags and TF-IDF features, while
for the other languages we fine-tuned a cross-lingual DeBERTa~V3 model combined
with TF-IDF features through a gating mechanism. We evaluated our system in
monolingual, multilingual, and zero-shot settings across multiple languages
including English, Arabic, German, Italian, and several unseen languages. The
results demonstrate the effectiveness of our approach, achieving competitive
performance across different languages with notable success in the monolingual
setting for English (rank 1st with macro-F1=0.8052), German (rank 3rd with
macro-F1=0.8013), Arabic (rank 4th with macro-F1=0.5771), and Romanian (rank
1st with macro-F1=0.8126) in the zero-shot setting. We also conducted an
ablation analysis that demonstrated the importance of combining TF-IDF features
with the gating mechanism and the cross-lingual transfer for subjectivity
detection. Furthermore, our analysis reveals the model's sensitivity to both
the order of cross-lingual fine-tuning and the linguistic proximity of the
training languages.

</details>


### [9] [Rewrite-to-Rank: Optimizing Ad Visibility via Retrieval-Aware Text Rewriting](https://arxiv.org/abs/2507.21099)
*Chloe Ho,Ishneet Sukhvinder Singh,Diya Sharma,Tanvi Reddy Anumandla,Michael Lu,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

> 研究了LLM广告重写的效应，提出了一种新方法通过调整广告措辞来提高其在检索系统中的可见性，实验显示该方法在多个方面优于其他方法。

<details>
  <summary>Details</summary>

**Motivation:** 探索LLM广告重写如何改进广告在其检索系统中的排名以及在生成的LLM响应中的包含频率。

**Method:** 采用基于监督微调的框架，并使用自定义损失函数在语义相关性和内容保真度之间进行平衡。

**Result:** 实验显示，PPO训练的模型在大多数情况下优于模板工程和监督微调，在基于指令的提示下实现了最高2.79 DeltaDIR@5和0.0073 DeltaMRR@5。

**Conclusion:** 此方法为在LLM集成的检索系统中有效重写广告提供了一种可扩展的方法，突显了广告在检索前书写的重要性以及提示格式和强化学习的作用。

**Abstract:** Search algorithms and user query relevance have given LLMs the ability to
return relevant information, but the effect of content phrasing on ad
visibility remains underexplored. We investigate how LLM-based rewriting of
advertisements can improve their ranking in retrieval systems and inclusion in
generated LLM responses, without modifying the retrieval model itself. We
introduce a supervised fine-tuning framework with a custom loss balancing
semantic relevance and content fidelity. To evaluate effectiveness, we propose
two metrics: DeltaMRR@K (ranking improvement) and DeltaDIR@K (inclusion
frequency improvement). Our approach presents a scalable method to optimize ad
phrasing, enhancing visibility in retrieval-based LLM workflows. Experiments
across both instruction-based and few-shot prompting demonstrate that PPO
trained models outperform both prompt engineering and supervised fine-tuning in
most cases, achieving up to a 2.79 DeltaDIR@5 and 0.0073 DeltaMRR@5 in
instruction-based prompting. These results highlight the importance of how the
ad is written before retrieval and prompt format and reinforcement learning in
effective ad rewriting for LLM integrated retrieval systems.

</details>


### [10] [iLSU-T: an Open Dataset for Uruguayan Sign Language Translation](https://arxiv.org/abs/2507.21104)
*Ariel E. Stassi,Yanina Boria,J. Matías Di Martino,Gregory Randall*

Main category: cs.CL

> 本文介绍了一个公开的手语视频数据集iLSU T，并通过使用三种最先进的翻译算法进行实验，展示了本地化数据集在手语翻译和理解中的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于每个国家的手语特点，机器翻译需要本地数据来开发新技术和适应现有技术。为了开发新的手语理解和生成工具，需要这种多模式和精心策划的数据。

**Method:** 本研究构建了一个名为iLSU T的公开数据集，该数据集包含了超过185小时的来自公共电视广播的乌拉圭手语视频，这些视频带有音频和文本转录。通过使用三种最先进的翻译算法进行了一系列实验，目的是为该数据集建立基准，并评估其有用性和所提出的数据处理管线的有效性。

**Result:** 实验表明，需要更多的本地化数据集来进行手语翻译和理解，这对于开发提高可访问性和包容性的新工具至关重要。研究中的数据和代码均可获取。

**Conclusion:** 该数据集的建立和实验表明，更多本地化数据集对于开发新的手语处理工具和提高所有人的可访问性和包容性至关重要。

**Abstract:** Automatic sign language translation has gained particular interest in the
computer vision and computational linguistics communities in recent years.
Given each sign language country particularities, machine translation requires
local data to develop new techniques and adapt existing ones. This work
presents iLSU T, an open dataset of interpreted Uruguayan Sign Language RGB
videos with audio and text transcriptions. This type of multimodal and curated
data is paramount for developing novel approaches to understand or generate
tools for sign language processing. iLSU T comprises more than 185 hours of
interpreted sign language videos from public TV broadcasting. It covers diverse
topics and includes the participation of 18 professional interpreters of sign
language. A series of experiments using three state of the art translation
algorithms is presented. The aim is to establish a baseline for this dataset
and evaluate its usefulness and the proposed pipeline for data processing. The
experiments highlight the need for more localized datasets for sign language
translation and understanding, which are critical for developing novel tools to
improve accessibility and inclusion of all individuals. Our data and code can
be accessed.

</details>


### [11] [Creation of a Numerical Scoring System to Objectively Measure and Compare the Level of Rhetoric in Arabic Texts: A Feasibility Study, and A Working Prototype](https://arxiv.org/abs/2507.21106)
*Mandar Marathe*

Main category: cs.CL

> 本研究旨在开发一种测量方法来评估阿语文本中阿拉伯修辞手法的使用情况，成功开发了包括网站和在线计算器在内的工具。

<details>
  <summary>Details</summary>

**Motivation:** 目前没有客观的方法来判断一个说话者或作者是否在一个给定的文本中使用了阿拉伯修辞手法，使用到什么程度，以及为什么使用。也没有客观的方法来跨体裁、作者或时期比较阿拉伯修辞的使用情况。本研究的目的是开发一种测量给定文本中构成阿拉伯修辞的文学设备的密度的方法，以作为一种替代标志。

**Method:** 通过编制84种常见的修辞手法及其定义的综合列表，构建了一个识别文本中修辞手法的系统，采用基于词素计数计算修辞手法密度的方法，并创建了四种电子工具和一个模拟工具来支持计算阿语文本的修辞手法密度，包括一个网站和一个在线计算器。还创造了一个报告阿拉伯修辞三个子领域中使用修辞手法分布的技术。

**Result:** 该研究开发了一个能够在任何阿语文本或演讲中准确报告阿拉伯修辞密度的实用工具。

**Conclusion:** 本研究成功开发了一种能够准确测量任何阿语文本中修辞密度的方法和工具，填补了阿拉伯语言学在客观测量修辞手法使用情况方面的空白。

**Abstract:** Arabic Rhetoric is the field of Arabic linguistics which governs the art and
science of conveying a message with greater beauty, impact and persuasiveness.
The field is as ancient as the Arabic language itself and is found extensively
in classical and contemporary Arabic poetry, free verse and prose. In practical
terms, it is the intelligent use of word order, figurative speech and
linguistic embellishments to enhance message delivery. Despite the volumes that
have been written about it and the high status accorded to it, there is no way
to objectively know whether a speaker or writer has used Arabic rhetoric in a
given text, to what extent, and why. There is no objective way to compare the
use of Arabic rhetoric across genres, authors or epochs. It is impossible to
know which of pre-Islamic poetry, Andalucian Arabic poetry, or modern literary
genres are richer in Arabic rhetoric. The aim of the current study was to
devise a way to measure the density of the literary devices which constitute
Arabic rhetoric in a given text, as a proxy marker for Arabic rhetoric itself.
A comprehensive list of 84 of the commonest literary devices and their
definitions was compiled. A system of identifying literary devices in texts was
constructed. A method of calculating the density of literary devices based on
the morpheme count of the text was utilised. Four electronic tools and an
analogue tool were created to support the calculation of an Arabic text's
rhetorical literary device density, including a website and online calculator.
Additionally, a technique of reporting the distribution of literary devices
used across the three sub-domains of Arabic rhetoric was created. The output of
this project is a working tool which can accurately report the density of
Arabic rhetoric in any Arabic text or speech.

</details>


### [12] [Curved Inference: Concern-Sensitive Geometry in Large Language Model Residual Streams](https://arxiv.org/abs/2507.21107)
*Rob Manson*

Main category: cs.CL

> 研究提出了一种名为Curved Inference的框架，分析大型语言模型在响应不同语义关注点变化时的行为，揭示了模型几何结构的变化。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在分析大型语言模型在面对不同语义领域（情感、道德、视角、逻辑、身份、环境和无意义领域）的提示时，其内部激活轨迹如何变化。

**Method:** 我们提出了一种名为Curved Inference的几何可解释性框架，用于追踪大型语言模型的残差流轨迹在语义关注点变化时如何弯曲。

**Result:** 研究发现，对LLaMA和Gemma3-1b模型进行语义关注点变化的提示时，它们的内部激活轨迹会显著变化。LLaMA在关注点强度增加时，弯曲度和显著性会有持续且统计显著的提升。而Gemma在响应关注点变化时较弱，尤其在辨别中等和强变体方面。

**Conclusion:** 研究结果支持大型语言模型几何结构的两层观点：嵌入空间中编码的潜在概念结构，和由特定提示引起的上下文轨迹。Curved Inference揭示了模型如何在深度上导航、重新定向或加强语义意义，为诊断模型一致性、抽象和新兴推理动态提供了一个原则性的方法。

**Abstract:** We propose Curved Inference - a geometric Interpretability framework that
tracks how the residual stream trajectory of a large language model bends in
response to shifts in semantic concern. Across 20 matched prompts spanning
emotional, moral, perspective, logical, identity, environmental, and nonsense
domains, we analyse Gemma3-1b and LLaMA3.2-3b using five native-space metrics,
with a primary focus on curvature (\k{appa}_i) and salience (S(t)). These
metrics are computed under a pullback semantic metric derived from the
unembedding matrix, ensuring that all measurements reflect token-aligned
geometry rather than raw coordinate structure. We find that concern-shifted
prompts reliably alter internal activation trajectories in both models - with
LLaMA exhibiting consistent, statistically significant scaling in both
curvature and salience as concern intensity increases. Gemma also responds to
concern but shows weaker differentiation between moderate and strong variants.
Our results support a two-layer view of LLM geometry - a latent conceptual
structure encoded in the embedding space, and a contextual trajectory shaped by
prompt-specific inference. Curved Inference reveals how models navigate,
reorient, or reinforce semantic meaning over depth, offering a principled
method for diagnosing alignment, abstraction, and emergent inference dynamics.
These findings offer fresh insight into semantic abstraction and model
alignment through the lens of Curved Inference.

</details>


### [13] [A Survey of Classification Tasks and Approaches for Legal Contracts](https://arxiv.org/abs/2507.21108)
*Amrita Singh,Aditya Joshi,Jiaojiao Jiang,Hye-young Paik*

Main category: cs.CL

> The paper provides a comprehensive survey of automatic legal contract classification (LCC), analyzing its methodologies, datasets, and challenges, to improve legal processes and information accessibility.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the inefficiencies and errors in manual contract reviews by improving the speed, accuracy, and accessibility of contract analysis through automation.

**Method:** Automatic Legal Contract Classification (LCC) involves the use of Traditional Machine Learning, Deep Learning, and Transformer-based approaches to classify contracts.

**Result:** The paper identifies seven classification tasks, reviews fourteen datasets, and introduces a methodology taxonomy for LCC. It also discusses evaluation techniques and highlights the best-performing results.

**Conclusion:** The survey suggests future research directions to further improve the efficiency, accuracy, and scalability of legal contract classification. It aims to enhance legal processes and make legal information more accessible.

**Abstract:** Given the large size and volumes of contracts and their underlying inherent
complexity, manual reviews become inefficient and prone to errors, creating a
clear need for automation. Automatic Legal Contract Classification (LCC)
revolutionizes the way legal contracts are analyzed, offering substantial
improvements in speed, accuracy, and accessibility. This survey delves into the
challenges of automatic LCC and a detailed examination of key tasks, datasets,
and methodologies. We identify seven classification tasks within LCC, and
review fourteen datasets related to English-language contracts, including
public, proprietary, and non-public sources. We also introduce a methodology
taxonomy for LCC, categorized into Traditional Machine Learning, Deep Learning,
and Transformer-based approaches. Additionally, the survey discusses evaluation
techniques and highlights the best-performing results from the reviewed
studies. By providing a thorough overview of current methods and their
limitations, this survey suggests future research directions to improve the
efficiency, accuracy, and scalability of LCC. As the first comprehensive survey
on LCC, it aims to support legal NLP researchers and practitioners in improving
legal processes, making legal information more accessible, and promoting a more
informed and equitable society.

</details>


### [14] [SemRAG: Semantic Knowledge-Augmented RAG for Improved Question-Answering](https://arxiv.org/abs/2507.21110)
*Kezhen Zhong,Basem Suleiman,Abdelkarim Erradi,Shijing Chen*

Main category: cs.CL

> 本文介绍了SemRAG，一种增强型的检索增强生成框架，通过语义切块和知识图谱的运用，高效地整合领域特定知识，避免了繁琐的微调过程。

<details>
  <summary>Details</summary>

**Motivation:** 将领域特定知识整合到大语言模型中对于提高其在专业任务中的表现至关重要，然而现有的适配方法计算消耗大且易过拟合，限制了其扩展性。

**Method:** SemRAG采用语义切块算法，基于句子嵌入的余弦相似度对文档进行分割，同时运用知识图谱对检索出的信息进行结构化处理，以此提高检索精度和上下文理解。

**Result:** 实验结果表明，与传统RAG方法相比，SemRAG在MultiHop RAG和Wikipedia数据集上的实验中显著提升了从知识图谱中检索出的信息的相关性和准确性。

**Conclusion:** SemRAG能够在避免资源密集型微调的情况下，创建高效的领域特定语言模型管道，是一种实用且可扩展的方法。

**Abstract:** This paper introduces SemRAG, an enhanced Retrieval Augmented Generation
(RAG) framework that efficiently integrates domain-specific knowledge using
semantic chunking and knowledge graphs without extensive fine-tuning.
Integrating domain-specific knowledge into large language models (LLMs) is
crucial for improving their performance in specialized tasks. Yet, existing
adaptations are computationally expensive, prone to overfitting and limit
scalability. To address these challenges, SemRAG employs a semantic chunking
algorithm that segments documents based on the cosine similarity from sentence
embeddings, preserving semantic coherence while reducing computational
overhead. Additionally, by structuring retrieved information into knowledge
graphs, SemRAG captures relationships between entities, improving retrieval
accuracy and contextual understanding. Experimental results on MultiHop RAG and
Wikipedia datasets demonstrate SemRAG has significantly enhances the relevance
and correctness of retrieved information from the Knowledge Graph,
outperforming traditional RAG methods. Furthermore, we investigate the
optimization of buffer sizes for different data corpus, as optimizing buffer
sizes tailored to specific datasets can further improve retrieval performance,
as integration of knowledge graphs strengthens entity relationships for better
contextual comprehension. The primary advantage of SemRAG is its ability to
create an efficient, accurate domain-specific LLM pipeline while avoiding
resource-intensive fine-tuning. This makes it a practical and scalable approach
aligned with sustainability goals, offering a viable solution for AI
applications in domain-specific fields.

</details>


### [15] [InsurTech innovation using natural language processing](https://arxiv.org/abs/2507.21112)
*Panyi Dong,Zhiyu Quan*

Main category: cs.CL

> 本文通过真实世界数据展示了NLP在保险行业的应用，特别是商业保险中的实际案例，说明NLP不仅是辅助工具，更是现代数据驱动保险分析的基础元素。

<details>
  <summary>Details</summary>

**Motivation:** 随着InsurTech的迅速崛起，传统保险公司正在探索替代数据源和先进科技以维持竞争优势。本文旨在概述自然语言处理（NLP）的概念及其在保险运营中的新兴应用，特别是将原始、非结构化的文本转化为可用于精算分析和决策的结构化数据。

**Method:** 本研究采用多种自然语言处理技术，处理来自InsurTech行业合作伙伴提供的实际替代数据，这些数据丰富了传统的保险数据来源。通过这些数据，展示了商业保险情境中的实际应用案例。

**Result:** 研究展示了如何利用文本衍生洞察，不仅丰富和调整了商业保险定价的传统评级因素，还通过引入新的行业分类来提供评估基础风险的新视角。

**Conclusion:** 通过演示，我们表明，NLP不仅是辅助工具，更是现代数据驱动保险分析的基石。

**Abstract:** With the rapid rise of InsurTech, traditional insurance companies are
increasingly exploring alternative data sources and advanced technologies to
sustain their competitive edge. This paper provides both a conceptual overview
and practical case studies of natural language processing (NLP) and its
emerging applications within insurance operations with a focus on transforming
raw, unstructured text into structured data suitable for actuarial analysis and
decision-making. Leveraging real-world alternative data provided by an
InsurTech industry partner that enriches traditional insurance data sources, we
apply various NLP techniques to demonstrate practical use cases in the
commercial insurance context. These enriched, text-derived insights not only
add to and refine traditional rating factors for commercial insurance pricing
but also offer novel perspectives for assessing underlying risk by introducing
novel industry classifications. Through these demonstrations, we show that NLP
is not merely a supplementary tool but a foundational element for modern,
data-driven insurance analytics.

</details>


### [16] [TRIDENT: Benchmarking LLM Safety in Finance, Medicine, and Law](https://arxiv.org/abs/2507.21134)
*Zheng Hui,Yijiang River Dong,Ehsan Shareghi,Nigel Collier*

Main category: cs.CL

> 本文定义了LLM领域内特定的安全原则，并引入了专门评估LLMs在法律、金融和医疗领域安全性的基准测试Trident-Bench。评估结果揭示了关键安全差距，强调了需要在领域内安全性方面进行更精细的改进。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型（LLMs）越来越多地应用于法律、金融和医疗等高风险领域，系统性评估LLMs在这些领域的特定安全性变得至关重要。然而，之前的研究主要集中在改善LLMs在这些领域的性能，忽略了对LLMs领域安全性风险的评估，本研究旨在填补这一空白。

**Method:** 本研究首先定义了基于医学伦理原则、律师职业行为准则及金融伦理代码的LLM领域内特定的安全原则。接着，引入了专门针对LLM在法律、金融和医疗领域安全性的基准测试Trident-Bench。

**Result:** 通过对19个通用和领域专长模型在Trident-Bench上的评估，发现了关键的安全差距：强大的通才模型（如GPT、Gemini）能够满足基本期望，而领域专长模型在复杂的伦理细微问题上往往表现不佳。

**Conclusion:** 通过引入Trident-Bench，本研究为在法律和金融领域内研究LLMs的安全性提供了首个系统性资源，并为未来旨在减少LLMs在专业监管领域安全风险的研究奠定了基础。

**Abstract:** As large language models (LLMs) are increasingly deployed in high-risk
domains such as law, finance, and medicine, systematically evaluating their
domain-specific safety and compliance becomes critical. While prior work has
largely focused on improving LLM performance in these domains, it has often
neglected the evaluation of domain-specific safety risks. To bridge this gap,
we first define domain-specific safety principles for LLMs based on the AMA
Principles of Medical Ethics, the ABA Model Rules of Professional Conduct, and
the CFA Institute Code of Ethics. Building on this foundation, we introduce
Trident-Bench, a benchmark specifically targeting LLM safety in the legal,
financial, and medical domains. We evaluated 19 general-purpose and
domain-specialized models on Trident-Bench and show that it effectively reveals
key safety gaps -- strong generalist models (e.g., GPT, Gemini) can meet basic
expectations, whereas domain-specialized models often struggle with subtle
ethical nuances. This highlights an urgent need for finer-grained
domain-specific safety improvements. By introducing Trident-Bench, our work
provides one of the first systematic resources for studying LLM safety in law
and finance, and lays the groundwork for future research aimed at reducing the
safety risks of deploying LLMs in professionally regulated fields. Code and
benchmark will be released at: https://github.com/zackhuiiiii/TRIDENT

</details>


### [17] [TTS-1 Technical Report](https://arxiv.org/abs/2507.21138)
*Oleg Atamanenko,Anna Chalova,Joseph Coombes,Nikki Cope,Phillip Dang,Zhifeng Deng,Jimmy Du,Michael Ermolenko,Feifan Fan,Yufei Feng,Cheryl Fichter,Pavel Filimonov,Louis Fischer,Kylan Gibbs,Valeria Gusarova,Pavel Karpik,Andreas Assad Kottner,Ian Lee,Oliver Louie,Jasmine Mai,Mikhail Mamontov,Suri Mao,Nurullah Morshed,Igor Poletaev,Florin Radu,Dmytro Semernia,Evgenii Shingarev,Vikram Sivaraja,Peter Skirko,Rinat Takhautdinov,Robert Villahermosa,Jean Wang*

Main category: cs.CL

> Inworld TTS-1引入了两个高质量、高效的TTS模型，通过高级训练技术达到了顶尖性能，并支持多语言情感控制，同时开源了项目代码。

<details>
  <summary>Details</summary>

**Motivation:** 开发Inworld TTS-1的目的是为了在要求极高的应用中实现语音合成的极致质量和表现力，同时也通过较小的模型支持实时语音合成和设备上使用场景。

**Method:** 引入了两个基于Transformer的自回归文本到语音（TTS）模型，分别是拥有88亿参数的TTS-1-Max和16亿参数的TTS-1。通过扩大训练计算能力和采用预训练、微调以及语言模型部分的强化学习对齐的序列过程，两个模型在多种基准测试中均达到了最先进的性能。

**Result:** Inworld TTS-1和TTS-1-Max能够生成高分辨率的48kHz语音，具有低延迟，并且支持11种语言，通过音频标记提供细致的情感控制和非语言声音。同时还开源了训练和模型代码。

**Conclusion:** 通过上述方法，Inworld TTS-1证明了只需通过上下文学习即可获得卓越的语音质量，同时通过开源项目促进了交流与进一步发展。

**Abstract:** We introduce Inworld TTS-1, a set of two Transformer-based autoregressive
text-to-speech (TTS) models. Our largest model, TTS-1-Max, has 8.8B parameters
and is designed for utmost quality and expressiveness in demanding
applications. TTS-1 is our most efficient model, with 1.6B parameters, built
for real-time speech synthesis and on-device use cases. By scaling train-time
compute and applying a sequential process of pre-training, fine-tuning, and
RL-alignment of the speech-language model (SpeechLM) component, both models
achieve state-of-the-art performance on a variety of benchmarks, demonstrating
exceptional quality relying purely on in-context learning of the speaker's
voice. Inworld TTS-1 and TTS-1-Max can generate high-resolution 48 kHz speech
with low latency, and support 11 languages with fine-grained emotional control
and non-verbal vocalizations through audio markups. We additionally open-source
our training and modeling code under an MIT license.

</details>


### [18] [Diverse LLMs or Diverse Question Interpretations? That is the Ensembling Question](https://arxiv.org/abs/2507.21168)
*Rafael Rosales,Santiago Miret*

Main category: cs.CL

> 本文研究了两种多样性方法在大型语言模型中的应用，实验表明问题解释多样性优于模型多样性。

<details>
  <summary>Details</summary>

**Motivation:** 多样性已被证明可以提高包括大型语言模型在内的各种机器学习模型的性能，但确定最有效的多样性使用方式仍然具有挑战性。本文旨在探讨在二元问题回答上，模型多样性与问题解释多样性两种策略的有效性。

**Method:** 本文通过比较两种多样性方法来回答二元问题，一种是模型多样性，即使用多个模型回答同一个问题；另一种是问题解释多样性，即使用相同的模型以不同的方式回答同一个问题。在这两种情况下，都采用了多数投票作为集成共识机制来确定最终答案。

**Result:** 实验结果表明，问题解释多样性相比模型多样性，在boolq、strategyqa和pubmedqa数据集上，前者一贯地展现出更高的集成准确性。此外，对GPT和LLaMa的分析表明，模型多样性通常会产生介于最好和最差成员之间的结果，并无明显改进。

**Conclusion:** 研究表明，对于回答二元问题，问题解释多样性比模型多样性更有效。模型多样性在实验中并没有带来明显的性能提升。

**Abstract:** Effectively leveraging diversity has been shown to improve performance for
various machine learning models, including large language models (LLMs).
However, determining the most effective way of using diversity remains a
challenge. In this work, we compare two diversity approaches for answering
binary questions using LLMs: model diversity, which relies on multiple models
answering the same question, and question interpretation diversity, which
relies on using the same model to answer the same question framed in different
ways. For both cases, we apply majority voting as the ensemble consensus
heuristic to determine the final answer. Our experiments on boolq, strategyqa,
and pubmedqa show that question interpretation diversity consistently leads to
better ensemble accuracy compared to model diversity. Furthermore, our analysis
of GPT and LLaMa shows that model diversity typically produces results between
the best and the worst ensemble members without clear improvement.

</details>


### [19] [Contrast-CAT: Contrasting Activations for Enhanced Interpretability in Transformer-based Text Classifiers](https://arxiv.org/abs/2507.21186)
*Sungmin Han,Jeonghyun Lee,Sangkyun Lee*

Main category: cs.CL

> 论文提出了一种改进的解释神经网络模型决策的方法，名为Contrast-CAT，它通过对比激活提高了神经网络模型的可解释性。

<details>
  <summary>Details</summary>

**Motivation:** 激活归因方法虽然能够有效地解释基于Transformer的文本分类模型，但这些方法可能受到激活中类别无关特征的影响，导致解释不可靠。为此，作者提出了一种改进的归因方法。

**Method:** Contrast-CAT, 一种基于激活对比的归因方法，通过将输入序列的激活与参考激活对比，过滤出与类别无关的特征，生成更清晰、更忠实的归因图。

**Result:** 实验结果表明，Contrast-CAT在不同的数据集和模型上普遍优于现有方法，在MoRF设置下的AOPC和LOdds分别平均提高了1.30和2.25倍。

**Conclusion:** Contrast-CAT的引入显著提升了基于Transformer的文本分类模型的可解释性。

**Abstract:** Transformers have profoundly influenced AI research, but explaining their
decisions remains challenging -- even for relatively simpler tasks such as
classification -- which hinders trust and safe deployment in real-world
applications. Although activation-based attribution methods effectively explain
transformer-based text classification models, our findings reveal that these
methods can be undermined by class-irrelevant features within activations,
leading to less reliable interpretations. To address this limitation, we
propose Contrast-CAT, a novel activation contrast-based attribution method that
refines token-level attributions by filtering out class-irrelevant features. By
contrasting the activations of an input sequence with reference activations,
Contrast-CAT generates clearer and more faithful attribution maps. Experimental
results across various datasets and models confirm that Contrast-CAT
consistently outperforms state-of-the-art methods. Notably, under the MoRF
setting, it achieves average improvements of x1.30 in AOPC and x2.25 in LOdds
over the most competing methods, demonstrating its effectiveness in enhancing
interpretability for transformer-based text classification.

</details>


### [20] [Understanding Public Perception of Crime in Bangladesh: A Transformer-Based Approach with Explainability](https://arxiv.org/abs/2507.21234)
*Fatema Binte Hassan,Md Al Jubair,Mohammad Mehadi Hasan,Tahmid Hossain,S M Mehebubur Rahman Khan Shuvo,Mohammad Shamsul Arefin*

Main category: cs.CL

> 本文发展了一种基于XLM-RoBERTa的模型进行孟加拉语情感分析，分类准确率为97%，优于现有方法。研究展示了该模型在公众观点分析中提取行动性见解的潜力，可用于公共政策和犯罪预防。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在研究公众对犯罪相关新闻情感的变化，特别是在社交媒体平台上公众观点的演变。

**Method:** 本研究利用基于XLM-RoBERTa Base架构的Transformer模型，对包含28,528个孟加拉语社交媒体评论的新数据集进行情感分类，将其划分为正面、负面和中性三类。

**Result:** 所提出的模型在孟加拉语情感分析中达到了97%的分类准确率，超过了现有的最先进的方法，证明了在处理像孟加拉语这样的低资源语言方面，基于Transformer的模型的有效性。为了增强模型的可解释性，研究还采用了可解释的人工智能技术来识别对情感分类最具影响力的特征。

**Conclusion:** 研究结果表明Transformer模型在处理像孟加拉语这样的低资源语言方面的有效性，并展示了其提取行动性见解的潜力，以支持公共政策制定和犯罪预防策略。

**Abstract:** In recent years, social media platforms have become prominent spaces for
individuals to express their opinions on ongoing events, including criminal
incidents. As a result, public sentiment can shift dynamically over time. This
study investigates the evolving public perception of crime-related news by
classifying user-generated comments into three categories: positive, negative,
and neutral. A newly curated dataset comprising 28,528 Bangla-language social
media comments was developed for this purpose. We propose a transformer-based
model utilizing the XLM-RoBERTa Base architecture, which achieves a
classification accuracy of 97%, outperforming existing state-of-the-art methods
in Bangla sentiment analysis. To enhance model interpretability, explainable AI
technique is employed to identify the most influential features driving
sentiment classification. The results underscore the effectiveness of
transformer-based models in processing low-resource languages such as Bengali
and demonstrate their potential to extract actionable insights that can support
public policy formulation and crime prevention strategies.

</details>


### [21] [Bangla BERT for Hyperpartisan News Detection: A Semi-Supervised and Explainable AI Approach](https://arxiv.org/abs/2507.21242)
*Mohammad Mehadi Hasan,Fatema Binte Hassan,Md Al Jubair,Zobayer Ahmed,Sazzatul Yeakin,Md Masum Billah*

Main category: cs.CL

> 本研究通过微调孟加拉语BERT模型来精确检测偏见新闻，实现了高准确率，为低资源语言环境下的自然语言处理提供了新方法。

<details>
  <summary>Details</summary>

**Motivation:** 由于缺乏针对孟加拉语的高级自然语言处理方法，使得辨别偏见新闻存在困难，进而导致误导性的信息广泛传播，该研究旨在解决这一问题。

**Method:** 通过微调针对孟加拉语的BERT模型来提高辨别偏见新闻的分类准确性，同时使用半监督学习提升预测效果，并应用LIME为模型决策提供透明解释。

**Result:** 在试用数据中，经过微调的孟加拉语BERT模型达到了95.65%的准确率，优于传统方法，证明了在资源较少的语言环境中使用变压器模型的有效性。

**Conclusion:** 研究展示了变压器模型在低资源语言中的有效性，从而为这一领域的进一步改进奠定了基础。

**Abstract:** In the current digital landscape, misinformation circulates rapidly, shaping
public perception and causing societal divisions. It is difficult to identify
hyperpartisan news in Bangla since there aren't many sophisticated natural
language processing methods available for this low-resource language. Without
effective detection methods, biased content can spread unchecked, posing
serious risks to informed discourse. To address this gap, our research
fine-tunes Bangla BERT. This is a state-of-the-art transformer-based model,
designed to enhance classification accuracy for hyperpartisan news. We evaluate
its performance against traditional machine learning models and implement
semi-supervised learning to enhance predictions further. Not only that, we use
LIME to provide transparent explanations of the model's decision-making
process, which helps to build trust in its outcomes. With a remarkable accuracy
score of 95.65%, Bangla BERT outperforms conventional approaches, according to
our trial data. The findings of this study demonstrate the usefulness of
transformer models even in environments with limited resources, which opens the
door to further improvements in this area.

</details>


### [22] [Can human clinical rationales improve the performance and explainability of clinical text classification models?](https://arxiv.org/abs/2507.21302)
*Christoph Metzner,Shang Gao,Drahomira Herrmannova,Heidi A. Hanson*

Main category: cs.CL

> 研究发现，人类临床推理作为额外训练数据可提升模型性能（尤其是在资源充足的情况下），但在资源有限时效果不一致。此外，基于推理训练的模型在准确性上通常不如基于更多报告训练的模型。

<details>
  <summary>Details</summary>

**Motivation:** 研究人类临床推理作为额外监督以提高基于transformer的模型在编码临床文档时的性能和可解释性的潜力。

**Method:** 通过分析99,125个人类临床推理和128,649份电子病理报告来评估基于transformer的模型提取原发癌症部位的表现，并研究充分性作为预先选择推理的质量度量方法。

**Result:** 研究表明，在资源充足的情况下，临床推理作为额外训练数据可以提升模型性能，但在资源有限的情况下结果不一致。使用充分性衡量并不总是有效。

**Conclusion:** 使用临床推理作为额外训练数据能够带来较小的性能提升和略微更优的可解释性。然而，总的来说，它们不如使用更多的报告进行训练来得有效。

**Abstract:** AI-driven clinical text classification is vital for explainable automated
retrieval of population-level health information. This work investigates
whether human-based clinical rationales can serve as additional supervision to
improve both performance and explainability of transformer-based models that
automatically encode clinical documents. We analyzed 99,125 human-based
clinical rationales that provide plausible explanations for primary cancer site
diagnoses, using them as additional training samples alongside 128,649
electronic pathology reports to evaluate transformer-based models for
extracting primary cancer sites. We also investigated sufficiency as a way to
measure rationale quality for pre-selecting rationales. Our results showed that
clinical rationales as additional training data can improve model performance
in high-resource scenarios but produce inconsistent behavior when resources are
limited. Using sufficiency as an automatic metric to preselect rationales also
leads to inconsistent results. Importantly, models trained on rationales were
consistently outperformed by models trained on additional reports instead. This
suggests that clinical rationales don't consistently improve model performance
and are outperformed by simply using more reports. Therefore, if the goal is
optimizing accuracy, annotation efforts should focus on labeling more reports
rather than creating rationales. However, if explainability is the priority,
training models on rationale-supplemented data may help them better identify
rationale-like features. We conclude that using clinical rationales as
additional training data results in smaller performance improvements and only
slightly better explainability (measured as average token-level rationale
coverage) compared to training on additional reports.

</details>


### [23] [Do Large Language Models Understand Morality Across Cultures?](https://arxiv.org/abs/2507.21319)
*Hadi Mohammadi,Yasmeen F. S. S. Meijer,Efthymia Papadopoulou,Ayoub Bagheri*

Main category: cs.CL

> 本研究调查了LLMs在捕捉跨文化道德观点方面的表现，发现这些模型往往不能精确反映真实世界中的道德多样性，显示出对模型的改进和减少文化偏见的迫切需求。

<details>
  <summary>Details</summary>

**Motivation:** 由于LLMs潜在嵌入的偏见（如性别、种族和文化偏见）而引发了关于这些技术的伦理使用和社会后果的重大质疑，因此该研究的目的在于调查LLMs在捕捉跨文化道德观点的差异和相似性方面的表现。

**Method:** 本研究采用了三种方法来调查大型语言模型（LLMs）在捕捉跨文化道德观点的差异和相似性方面的表现：(1) 比较模型生成的道德分数的方差与国际调查数据报告的方差；(2) 进行聚类对齐分析，以评估从LLM输出国群组与调查数据之间的对应关系；(3) 使用系统选择的令牌对直接对模型提出对比询问。

**Result:** 研究发现，目前的LLMs往往不能再现跨文化道德变异的全部范围，倾向于压缩差异，并且与实证调查模式之间的对齐度较低。

**Conclusion:** 这些发现突显了需要更加强大的方法来减少偏见并提高LLMs的文化代表性。最后，研究讨论了LLMs负责任发展和全球部署的公平性和伦理一致性的重要意义。

**Abstract:** Recent advancements in large language models (LLMs) have established them as
powerful tools across numerous domains. However, persistent concerns about
embedded biases, such as gender, racial, and cultural biases arising from their
training data, raise significant questions about the ethical use and societal
consequences of these technologies. This study investigates the extent to which
LLMs capture cross-cultural differences and similarities in moral perspectives.
Specifically, we examine whether LLM outputs align with patterns observed in
international survey data on moral attitudes. To this end, we employ three
complementary methods: (1) comparing variances in moral scores produced by
models versus those reported in surveys, (2) conducting cluster alignment
analyses to assess correspondence between country groupings derived from LLM
outputs and survey data, and (3) directly probing models with comparative
prompts using systematically chosen token pairs. Our results reveal that
current LLMs often fail to reproduce the full spectrum of cross-cultural moral
variation, tending to compress differences and exhibit low alignment with
empirical survey patterns. These findings highlight a pressing need for more
robust approaches to mitigate biases and improve cultural representativeness in
LLMs. We conclude by discussing the implications for the responsible
development and global deployment of LLMs, emphasizing fairness and ethical
alignment.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [24] [GAITEX: Human motion dataset from impaired gait and rehabilitation exercises of inertial and optical sensor data](https://arxiv.org/abs/2507.21069)
*Andreas Spilz,Heiko Oppel,Jochen Werner,Kathrin Stucke-Straub,Felix Capanni,Michael Munz*

Main category: cs.CV

> 文章介绍了一个多模态数据集，结合了IMU和运动捕捉数据，用于支持机器学习模型在人体运动分析中的开发和基准测试，加速了该领域研究进程。

<details>
  <summary>Details</summary>

**Motivation:** 由于物理治疗练习和步态分析中的传感器分类模型的开发需要大规模且多样化的数据集，这既耗时又昂贵。为了应对这一挑战，本研究旨在提供一个用于训练和测试机器学习模型的数据集，支持自动化运动评估、步态分析、时间活动分割和生物力学参数估计等任务。

**Method:** 本研究建立了一个多模态数据集，记录了来自19名参与者的物理治疗练习和步态相关练习的正确和相关临床变体以及正常和受损步态模式。数据集包括来自九个惯性测量单元(IMU)和35个光学标记捕捉全身运动的数据，可以与基于标记的运动捕捉系统进行精确比较。此外，还提供了处理后的IMU方位数据、特定于主题的OpenSim模型、逆运动学结果以及用于在肌肉骨骼环境中可视化IMU方位的工具。

**Result:** 创建了一个包含原始IMU数据和运动捕捉(MoCap)数据的数据集，用于支持机器学习模型的开发，该模型能够分析和评估人体运动。

**Conclusion:** 这个资源加速了机器学习驱动的人体运动分析的研究，提供了处理后IMU方位、逆运动学结果等数据，支持研究者进行各种分析目标。

**Abstract:** Wearable inertial measurement units (IMUs) offer a cost-effective and
scalable means to assess human movement quality in clinical and everyday
settings. However, the development of robust sensor-based classification models
for physiotherapeutic exercises and gait analysis requires large, diverse
datasets, which are costly and time-consuming to collect. Here, we present a
multimodal dataset of physiotherapeutic exercises - including correct and
clinically relevant variants - and gait-related exercises - including both
normal and impaired gait patterns - recorded from 19 participants using
synchronized IMUs and marker-based motion capture (MoCap). The dataset includes
raw data from nine IMUs and thirty-five optical markers capturing full-body
kinematics. Each IMU is additionally equipped with four optical markers,
enabling precise comparison between IMU-derived orientation estimates and
reference values from the MoCap system. To support further analysis, we also
provide processed IMU orientations aligned with common segment coordinate
systems, subject-specific OpenSim models, inverse kinematics results, and tools
for visualizing IMU orientations in the musculoskeletal context. Detailed
annotations of movement execution quality and time-stamped segmentations
support diverse analysis goals. This dataset supports the development and
benchmarking of machine learning models for tasks such as automatic exercise
evaluation, gait analysis, temporal activity segmentation, and biomechanical
parameter estimation. To facilitate reproducibility, we provide code for
postprocessing, sensor-to-segment alignment, inverse kinematics computation,
and technical validation. This resource is intended to accelerate research in
machine learning-driven human movement analysis.

</details>


### [25] [Seeing Beyond Frames: Zero-Shot Pedestrian Intention Prediction with Raw Temporal Video and Multimodal Cues](https://arxiv.org/abs/2507.21161)
*Pallavi Zambare,Venkata Nikhil Thanikella,Ying Liu*

Main category: cs.CV

> BF-PIP是一种零样本方法，可以从简短的连续视频片段中直接推断出行人的过街意图，并且在没有额外训练的情况下，其预测准确率比GPT-4V基线高出了18%。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于传统的监督学习方法需要大量的再训练来适应新的驾驶场景，在复杂城市环境中进行行人意图预测显得尤为重要。

**Method:** 这种方法称为BF-PIP（超越帧的行人意图预测），它基于Gemini 2.5 Pro，可以从简短的连续视频片段中直接推断出过街意图，这些视频片段还结合了JAAD元数据。BF-PIP通过专门的多模态提示结合了边界框注释和自车速度信息，与基于GPT-4V的方法相比，它处理的是不间断的视频片段。

**Result:** BF-PIP无需额外训练就能达到73%的预测准确率，比GPT-4V基线方法高出18%。

**Conclusion:** 将时间视频输入与上下文线索相结合，可以提高空间和时间的感知能力，并在模糊条件下改善意图推断。这种方法为智能运输系统中的敏捷且无需再训练的感知模块铺平了道路。

**Abstract:** Pedestrian intention prediction is essential for autonomous driving in
complex urban environments. Conventional approaches depend on supervised
learning over frame sequences and require extensive retraining to adapt to new
scenarios. Here, we introduce BF-PIP (Beyond Frames Pedestrian Intention
Prediction), a zero-shot approach built upon Gemini 2.5 Pro. It infers crossing
intentions directly from short, continuous video clips enriched with structured
JAAD metadata. In contrast to GPT-4V based methods that operate on discrete
frames, BF-PIP processes uninterrupted temporal clips. It also incorporates
bounding-box annotations and ego-vehicle speed via specialized multimodal
prompts. Without any additional training, BF-PIP achieves 73% prediction
accuracy, outperforming a GPT-4V baseline by 18 %. These findings illustrate
that combining temporal video inputs with contextual cues enhances
spatiotemporal perception and improves intent inference under ambiguous
conditions. This approach paves the way for agile, retraining-free perception
module in intelligent transportation system.

</details>


### [26] [ChartM$^3$: Benchmarking Chart Editing with Multimodal Instructions](https://arxiv.org/abs/2507.21167)
*Danglu Yang,Liang Zhang,Zihao Yue,Liangyu Chen,Yichen Xu,Wenxuan Wang,Qin Jin*

Main category: cs.CV

> 本文介绍了一种新的表格编辑方法，通过结合自然语言和视觉指示符来更准确地编辑表格，并提出一个新的多模态表格编辑基准Chart$\text{M}^3$。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有方法主要依赖自然语言指令，这往往过于模糊，不能支持细粒度编辑。而新的多模态范式通过结合自然语言和视觉指示，有望更准确地进行表格编辑。

**Method:** 本文提出了一种新的多模态表格编辑范式，用户意图通过自然语言和视觉指示符的结合来明确表达。为支持该范式，提出了Chart$\text{M}^3$，这是一个具有多级难度和多角度评估的多模态表格编辑的新基准，包含1,000个样本。

**Result:** Chart$\text{M}^3$的评估揭示了现有大规模语言模型在解释和应对视觉指示方面存在显著限制。通过在大规模训练集Chart$\text{M}^3$-Train上进行微调，可以显著改进这些模型的表现。

**Conclusion:** 本研究表明，通过提供具有视觉指示的多模态监督，可以在构建实用的表格编辑系统方面取得重要进展。

**Abstract:** Charts are a fundamental visualization format widely used in data analysis
across research and industry. While enabling users to edit charts based on
high-level intentions is of great practical value, existing methods primarily
rely on natural language instructions, which are often too ambiguous to support
fine-grained editing. In this work, we introduce a novel paradigm for
multimodal chart editing, where user intent is expressed through a combination
of natural language and visual indicators that explicitly highlight the
elements to be modified. To support this paradigm, we present
Chart$\text{M}^3$, a new benchmark for Multimodal chart editing with
Multi-level complexity and Multi-perspective evaluation. Chart$\text{M}^3$
contains 1,000 samples spanning four levels of editing difficulty. Each sample
includes triplets in the form of (chart, code, multimodal instructions). To
comprehensively evaluate chart editing models, Chart$\text{M}^3$ provides
metrics that assess both visual appearance and code correctness. Our benchmark
reveals significant limitations in current multimodal large language models
(MLLMs), including GPT-4o, particularly in their ability to interpret and act
on visual indicators. To address this, we construct Chart$\text{M}^3$-Train, a
large-scale training set with 24,000 multimodal chart editing samples.
Fine-tuning MLLMs on this dataset leads to substantial improvements,
demonstrating the importance of multimodal supervision in building practical
chart editing systems. Our datasets, codes, and evaluation tools are available
at https://github.com/MLrollIT/ChartM3. %https://github.com/MLrollIT/ChartM3Our
datasets, codes, and evaluation tools are available at
https://github.com/yaolinli/VCE.

</details>


### [27] [PanoGAN A Deep Generative Model for Panoramic Dental Radiographs](https://arxiv.org/abs/2507.21200)
*Soren Pedersen,Sanyam Jain,Mikkel Chavez,Viktor Ladehoff,Bruna Neves de Freitas,Ruben Pauwels*

Main category: cs.CV

> 本研究开发了一种用于合成牙全景X光片的生成对抗网络（GAN），通过探索不同的模型配置，取得了在图像清晰度与细节展示间的权衡，并为未来的牙科成像研究提供了基础。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在解决牙科研究和教育中数据稀缺的问题。通过开发一种生成牙全景X光片的生成对抗网络（GAN），希望能够增加可供使用的数据量，从而促进相关领域的研究和教育。

**Method:** 本研究使用深度卷积生成对抗网络（DCGAN）结合Wasserstein损失函数和梯度惩罚（WGANGP），基于一个包含2322张牙片的数据库进行训练，这些牙片质量不一。重点放在了牙槽区，其他解剖结构被裁剪。进行了广泛的预处理和数据清洗以标准化输入，同时保留了解剖变异。探索了四个候选模型，通过调整判别器迭代次数、特征深度，以及在训练前是否采用去噪技术。

**Result:** 临床专家对生成的牙片从解剖可见性和真实性两个方面进行了5级评分评估（1表示非常差，5表示优秀）。大多数图像在解剖描绘上表现中等，但有一些图像受到伪影的影响。观察到模型之间的权衡关系：未使用去噪数据训练的模型在像下颌管和板障骨等结构上提供更精细的细节，而使用去噪数据训练的模型则在总体图像清晰度和锐度上表现更优。

**Conclusion:** 本研究为基于GAN的方法在牙科成像中的未来发展奠定了基础。通过比较不同设定下训练出的模型，研究揭示了模型在细节展示和图像清晰度之间的权衡。

**Abstract:** This paper presents the development of a generative adversarial network (GAN)
for synthesizing dental panoramic radiographs. Although exploratory in nature,
the study aims to address the scarcity of data in dental research and
education. We trained a deep convolutional GAN (DCGAN) using a Wasserstein loss
with gradient penalty (WGANGP) on a dataset of 2322 radiographs of varying
quality. The focus was on the dentoalveolar regions, other anatomical
structures were cropped out. Extensive preprocessing and data cleaning were
performed to standardize the inputs while preserving anatomical variability. We
explored four candidate models by varying critic iterations, feature depth, and
the use of denoising prior to training. A clinical expert evaluated the
generated radiographs based on anatomical visibility and realism, using a
5-point scale (1 very poor 5 excellent). Most images showed moderate anatomical
depiction, although some were degraded by artifacts. A trade-off was observed
the model trained on non-denoised data yielded finer details especially in
structures like the mandibular canal and trabecular bone, while a model trained
on denoised data offered superior overall image clarity and sharpness. These
findings provide a foundation for future work on GAN-based methods in dental
imaging.

</details>


### [28] [On Explaining Visual Captioning with Hybrid Markov Logic Networks](https://arxiv.org/abs/2507.21246)
*Monika Shah,Somdeb Sarkhel,Deepak Venugopal*

Main category: cs.CV

> 本文开发了一种基于HMLNs的解释框架来说明训练数据中的示例如何影响生成的描述，从而增强了模型的解释能力。

<details>
  <summary>Details</summary>

**Motivation:** DNN在多模态任务上的进展使得解释模型如何整合视觉信息、语言信息和知识表示来生成描述成为一个具有挑战性的问题，现有的度量标准无法提供深入的见解，因此需要开发一种可解释的解释框架。

**Method:** 本文提出了一种新的基于混合马尔可夫逻辑网络（HMLNs）的解释框架，以解释训练数据中的相关示例如何影响模型生成的描述。通过学习训练实例上的HMLN分布，并计算在条件生成样本下分布的变化，可以量化哪些示例可能是生成观测描述的更丰富的信息源。

**Result:** 实验表明该解释框架能够解释训练数据中哪些示例影响了描述的生成，并展示了模型生成的描述对于人类判读者的可解释性。

**Conclusion:** 通过Amazon Mechanical Turk进行的实验展示了所提解释框架的可解释性，并能够比较不同模型在可解释性方面的表现。

**Abstract:** Deep Neural Networks (DNNs) have made tremendous progress in multimodal tasks
such as image captioning. However, explaining/interpreting how these models
integrate visual information, language information and knowledge representation
to generate meaningful captions remains a challenging problem. Standard metrics
to measure performance typically rely on comparing generated captions with
human-written ones that may not provide a user with a deep insights into this
integration. In this work, we develop a novel explanation framework that is
easily interpretable based on Hybrid Markov Logic Networks (HMLNs) - a language
that can combine symbolic rules with real-valued functions - where we
hypothesize how relevant examples from the training data could have influenced
the generation of the observed caption. To do this, we learn a HMLN
distribution over the training instances and infer the shift in distributions
over these instances when we condition on the generated sample which allows us
to quantify which examples may have been a source of richer information to
generate the observed caption. Our experiments on captions generated for
several state-of-the-art captioning models using Amazon Mechanical Turk
illustrate the interpretability of our explanations, and allow us to compare
these models along the dimension of explainability.

</details>


### [29] [Dual Guidance Semi-Supervised Action Detection](https://arxiv.org/abs/2507.21247)
*Ankit Singh,Efstratios Gavves,Cees G. M. Snoek,Hilde Kuehne*

Main category: cs.CV

> 本研究提出了一种新的半监督学习方法，用于时空动作定位，显著提高了有限标注数据情况下的模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 虽然半监督学习已在图像分类中得到了广泛研究，但对于时空动作定位的应用研究较少。鉴于标注数据难以获取时，半监督学习具有改善深度学习模型预测性能的巨大潜力，本研究旨在填补这一空白。

**Method:** 提出了一种用于时空动作定位的半监督方法，引入了一个双指导网络来选择更好的伪边界框。该方法结合了帧级分类和边界框预测，以确保动作类别在帧和边界框之间的一致性。

**Result:** 实验结果表明，所提出的方法在UCF101-24、J-HMDB-21和AVA数据集上优于扩展的基于图像的半监督基线。

**Conclusion:** 研究结果表明，结合帧级分类和边界框预测的双指导网络对于提升时空动作定位任务性能是有效的。

**Abstract:** Semi-Supervised Learning (SSL) has shown tremendous potential to improve the
predictive performance of deep learning models when annotations are hard to
obtain. However, the application of SSL has so far been mainly studied in the
context of image classification. In this work, we present a semi-supervised
approach for spatial-temporal action localization. We introduce a dual guidance
network to select better pseudo-bounding boxes. It combines a frame-level
classification with a bounding-box prediction to enforce action class
consistency across frames and boxes. Our evaluation across well-known
spatial-temporal action localization datasets, namely UCF101-24 , J-HMDB-21 and
AVA shows that the proposed module considerably enhances the model's
performance in limited labeled data settings. Our framework achieves superior
results compared to extended image-based semi-supervised baselines.

</details>


### [30] [Tracking Moose using Aerial Object Detection](https://arxiv.org/abs/2507.21256)
*Christopher Indris,Raiyan Rahman,Goetz Bramesfeld,Guanghui Wang*

Main category: cs.CV

> 研究应用patching数据增强并比较了三种不同架构的目标检测器性能，发现加速和简化模型在有限规模的patch下对于无人机的用途也是有效的，这使得无人机的应用更具前景。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于传统的载人飞机成本高、风险大且干扰性强，而自主无人机的计算能力有限，本研究旨在通过计算机视觉的方法提高小型动物的检测精度，特别是那些在图像中仅占据几个像素宽的对象。

**Method:** 本研究通过应用patching增强技术到数据集中，比较了三种架构多样化的常见目标检测器在不同设置下的表现。研究调整了patching方法的超参数以获得最佳检测精度。

**Result:** 每个模型至少在一种patching配置下达到了93%的mAP@IoU=0.5。统计分析提供了各种因素影响的深入评论，表明加速和简化的模型在这个任务中与计算能力要求更高的模型一样有效。

**Conclusion:** 研究表明，在空中野生动物追踪任务中，加速和简化的模型也能够达到良好的检测效果，即使在有限的补丁尺度下亦是如此，从而鼓励无人机应用。数据集和模型将在GitHub上公开。

**Abstract:** Aerial wildlife tracking is critical for conservation efforts and relies on
detecting small objects on the ground below the aircraft. It presents technical
challenges: crewed aircraft are expensive, risky and disruptive; autonomous
drones have limited computational capacity for onboard AI systems. Since the
objects of interest may appear only a few pixels wide, small object detection
is an inherently challenging computer vision subfield compounded by
computational efficiency needs. This paper applies a patching augmentation to
datasets to study model performance under various settings. A comparative study
of three common yet architecturally diverse object detectors is conducted using
the data, varying the patching method's hyperparameters against detection
accuracy. Each model achieved at least 93\% mAP@IoU=0.5 on at least one
patching configuration. Statistical analyses provide an in-depth commentary on
the effects of various factors. Analysis also shows that faster, simpler models
are about as effective as models that require more computational power for this
task and perform well given limited patch scales, encouraging UAV deployment.
Datasets and models will be made available via
https://github.com/chrisindris/Moose.

</details>


### [31] [HDR Environment Map Estimation with Latent Diffusion Models](https://arxiv.org/abs/2507.21261)
*Jack Hilliard,Adrian Hilton,Jean-Yves Guillemaut*

Main category: cs.CV

> 本文介绍了一种基于LDM的新方法，用于从单张图像中生成HDR环境贴图，解决了ERP格式中的边界接缝问题，并提出了适应ERP格式的扩散变换器PanoDiT。实验结果表明方法的有效性，但仍存在图像质量损失问题。

<details>
  <summary>Details</summary>

**Motivation:** 动机是改进从单视图图像中生成HDR环境贴图的算法，以减少ERP格式的极点失真和边界接缝问题，同时提升环境贴图的质量和准确性。

**Method:** 本文提出了一种新的方法，利用潜在扩散模型（LDM）从单视图图像中生成高质量的HDR环境贴图，以真实地照亮镜面反射表面。为了解决ERP表示格式中的极点失真和边界接缝问题，提出了一种ERP卷积填充方法，并设计了一种全景适应的扩散变换器架构PanoDiT。

**Result:** PanoDiT网络在减少ERP失真和伪影方面表现出色，但图像质量和可信度有所降低。实验结果显示，所提模型在图像质量和光照准确性方面能够与现有最优方法竞争。

**Conclusion:** 研究表明，通过适应ERP格式的扩散变换器架构，可以在一定程度上改善环境贴图的失真问题，但会对图像质量和环境的真实性造成一定程度的影响。

**Abstract:** We advance the field of HDR environment map estimation from a single-view
image by establishing a novel approach leveraging the Latent Diffusion Model
(LDM) to produce high-quality environment maps that can plausibly light
mirror-reflective surfaces. A common issue when using the ERP representation,
the format used by the vast majority of approaches, is distortions at the poles
and a seam at the sides of the environment map. We remove the border seam
artefact by proposing an ERP convolutional padding in the latent autoencoder.
Additionally, we investigate whether adapting the diffusion network
architecture to the ERP format can improve the quality and accuracy of the
estimated environment map by proposing a panoramically-adapted Diffusion
Transformer architecture. Our proposed PanoDiT network reduces ERP distortions
and artefacts, but at the cost of image quality and plausibility. We evaluate
with standard benchmarks to demonstrate that our models estimate high-quality
environment maps that perform competitively with state-of-the-art approaches in
both image quality and lighting accuracy.

</details>


### [32] [Fairness and Robustness of CLIP-Based Models for Chest X-rays](https://arxiv.org/abs/2507.21291)
*Théo Sourget,David Restrepo,Céline Hudelot,Enzo Ferrante,Stergios Christodoulidis,Maria Vakalopoulou*

Main category: cs.CV

> 本研究评估了基于CLIP的模型在胸部X光图像分类中的公平性和鲁棒性，发现存在年龄差异和依赖虚假相关性的问题。

<details>
  <summary>Details</summary>

**Motivation:** 由于CLIP-based模型在自然图像-文本领域表现强劲，近期的一些研究将这些架构应用于医疗任务，尤其是在放射学方面，因为拥有大量成对的图像和报告数据集，比如胸部X线摄影。然而，这些模型在不同临床任务中的公平性和鲁棒性尚未被充分探索。

**Method:** 本研究通过使用六个广泛使用的基于CLIP的模型对胸部X光图像进行分类来评估模型的公平性和鲁棒性。评估使用了三个公开数据集：MIMIC-CXR、NIH-CXR14 和 NEATX，考察了包括年龄、性别和种族在内的六个条件和亚群的公平性。同时，通过评估胸部插管存在与否的气胸病例来检验模型对捷径学习的鲁棒性。

**Result:** 结果显示，不同年龄的患者之间存在性能差距，但对于其他属性则更具有公平性。所有模型在没有胸部插管的图像上的性能较低，表明它们依赖于虚假的相关性。尽管可以基于模型生成的嵌入来分类敏感属性，但在使用PCA进行评估时没有看到这些模式，显示出这些可视化技术在评估模型方面的局限性。

**Conclusion:** 研究表明，六个被广泛使用的基于CLIP的模型在胸部X光图像分类中表现出一定的公平性和鲁棒性问题，尤其是关于年龄差异和对虚相关性的依赖。强调需要进一步研究和改进这些基于CLIP的模型以更好的适应医疗任务。

**Abstract:** Motivated by the strong performance of CLIP-based models in natural
image-text domains, recent efforts have adapted these architectures to medical
tasks, particularly in radiology, where large paired datasets of images and
reports, such as chest X-rays, are available. While these models have shown
encouraging results in terms of accuracy and discriminative performance, their
fairness and robustness in the different clinical tasks remain largely
underexplored. In this study, we extensively evaluate six widely used
CLIP-based models on chest X-ray classification using three publicly available
datasets: MIMIC-CXR, NIH-CXR14, and NEATX. We assess the models fairness across
six conditions and patient subgroups based on age, sex, and race. Additionally,
we assess the robustness to shortcut learning by evaluating performance on
pneumothorax cases with and without chest drains. Our results indicate
performance gaps between patients of different ages, but more equitable results
for the other attributes. Moreover, all models exhibit lower performance on
images without chest drains, suggesting reliance on spurious correlations. We
further complement the performance analysis with a study of the embeddings
generated by the models. While the sensitive attributes could be classified
from the embeddings, we do not see such patterns using PCA, showing the
limitations of these visualisation techniques when assessing models. Our code
is available at https://github.com/TheoSourget/clip_cxr_fairness

</details>


### [33] [VoluMe -- Authentic 3D Video Calls from Live Gaussian Splat Prediction](https://arxiv.org/abs/2507.21311)
*Martin de La Gorce,Charlie Hewitt,Tibor Takacs,Robert Gerdisch,Zafiirah Hosenie,Givi Meishvili,Marek Kowalski,Thomas J. Cashman,Antonio Criminisi*

Main category: cs.CV

> Real-time 3D reconstruction from 2D webcams for accessible and authentic 3D videoconferencing.

<details>
  <summary>Details</summary>

**Motivation:** represent people in 3D meetings without the constraints of complex hardware, fixed appearance, or pre-trained models

**Method:** predict 3D Gaussian reconstructions in real time from a single 2D webcam feed, with a stability loss for temporal stability

**Result:** state-of-the-art accuracy in visual quality and stability metrics, demonstrated in live one-to-one 3D meetings using only a standard 2D camera and display

**Conclusion:** method is highly accessible, realistic, and authentic for 3D videoconferencing

**Abstract:** Virtual 3D meetings offer the potential to enhance copresence, increase
engagement and thus improve effectiveness of remote meetings compared to
standard 2D video calls. However, representing people in 3D meetings remains a
challenge; existing solutions achieve high quality by using complex hardware,
making use of fixed appearance via enrolment, or by inverting a pre-trained
generative model. These approaches lead to constraints that are unwelcome and
ill-fitting for videoconferencing applications. We present the first method to
predict 3D Gaussian reconstructions in real time from a single 2D webcam feed,
where the 3D representation is not only live and realistic, but also authentic
to the input video. By conditioning the 3D representation on each video frame
independently, our reconstruction faithfully recreates the input video from the
captured viewpoint (a property we call authenticity), while generalizing
realistically to novel viewpoints. Additionally, we introduce a stability loss
to obtain reconstructions that are temporally stable on video sequences. We
show that our method delivers state-of-the-art accuracy in visual quality and
stability metrics compared to existing methods, and demonstrate our approach in
live one-to-one 3D meetings using only a standard 2D camera and display. This
demonstrates that our approach can allow anyone to communicate volumetrically,
via a method for 3D videoconferencing that is not only highly accessible, but
also realistic and authentic.

</details>


### [34] [GLCP: Global-to-Local Connectivity Preservation for Tubular Structure Segmentation](https://arxiv.org/abs/2507.21328)
*Feixiang Zhou,Zhuangzhi Gao,He Zhao,Jianyang Xie,Yanda Meng,Yitian Zhao,Gregory Y. H. Lip,Yalin Zheng*

Main category: cs.CV

> 

<details>
  <summary>Details</summary>

**Motivation:** 

**Method:** 

**Result:** {
  "tldr": "The paper proposes a Global-to-Local Connectivity Preservation (GLCP) framework that addresses the segmentation challenges of tubular structures, especially in dealing with local discontinuity, which is often overlooked by existing methods.", 
  "motivation": "To address the persistent issue of structural fragmentation in tubular structure segmentation which affects medical applications and is not sufficiently resolved by existing methods focused on global topologies.", 
  "method": "The authors introduce a GLCP framework that includes an Interactive Multi-head Segmentation (IMS) module and a Dual-Attention-based Refinement (DAR) module. IMS learns global segmentation, skeleton maps, and local discontinuity maps, while DAR enhances segmentation quality through refinement.", 
  "result": "Experiments on 2D and 3D datasets show that the GLCP framework performs better than state-of-the-art methods in terms of accuracy and continuity of the segmented tubular structures.", 
  "conclusion": "The GLCP framework effectively improves the segmentation of tubular structures by explicitly addressing local discontinuities and maintaining global topological integrity. The model outperforms existing approaches as demonstrated across a range of datasets.", 
  "citation": ""}


**Conclusion:** 

**Abstract:** Accurate segmentation of tubular structures, such as vascular networks, plays
a critical role in various medical domains. A remaining significant challenge
in this task is structural fragmentation, which can adversely impact downstream
applications. Existing methods primarily focus on designing various loss
functions to constrain global topological structures. However, they often
overlook local discontinuity regions, leading to suboptimal segmentation
results. To overcome this limitation, we propose a novel Global-to-Local
Connectivity Preservation (GLCP) framework that can simultaneously perceive
global and local structural characteristics of tubular networks. Specifically,
we propose an Interactive Multi-head Segmentation (IMS) module to jointly learn
global segmentation, skeleton maps, and local discontinuity maps, respectively.
This enables our model to explicitly target local discontinuity regions while
maintaining global topological integrity. In addition, we design a lightweight
Dual-Attention-based Refinement (DAR) module to further improve segmentation
quality by refining the resulting segmentation maps. Extensive experiments on
both 2D and 3D datasets demonstrate that our GLCP achieves superior accuracy
and continuity in tubular structure segmentation compared to several
state-of-the-art approaches. The source codes will be available at
https://github.com/FeixiangZhou/GLCP.

</details>


### [35] [Analyzing the Sensitivity of Vision Language Models in Visual Question Answering](https://arxiv.org/abs/2507.21335)
*Monika Shah,Sudarshan Balaji,Somdeb Sarkhel,Sanorita Dey,Deepak Venugopal*

Main category: cs.CV

> 研究显示，在违规格赖斯会话准则的情况下，视觉语言模型的表现会下降。

<details>
  <summary>Details</summary>

**Motivation:** 探索视觉语言模型在违反格赖斯会话准则时的能力，以及它们是否能以类似人类的方式处理这些违反情况。

**Method:** 通过人为在问题中添加修饰语来研究视觉语言模型（VLMs）在违反格赖斯会话准则时的表现。

**Result:** 初步结果显示，随着修饰语的增加，VLMs的表现一致性地下降。

**Conclusion:** 该研究提供了一个有前景的方法来理解视觉语言模型的限制。

**Abstract:** We can think of Visual Question Answering as a (multimodal) conversation
between a human and an AI system. Here, we explore the sensitivity of Vision
Language Models (VLMs) through the lens of cooperative principles of
conversation proposed by Grice. Specifically, even when Grice's maxims of
conversation are flouted, humans typically do not have much difficulty in
understanding the conversation even though it requires more cognitive effort.
Here, we study if VLMs are capable of handling violations to Grice's maxims in
a manner that is similar to humans. Specifically, we add modifiers to
human-crafted questions and analyze the response of VLMs to these modifiers. We
use three state-of-the-art VLMs in our study, namely, GPT-4o, Claude-3.5-Sonnet
and Gemini-1.5-Flash on questions from the VQA v2.0 dataset. Our initial
results seem to indicate that the performance of VLMs consistently diminish
with the addition of modifiers which indicates our approach as a promising
direction to understand the limitations of VLMs.

</details>


### [36] [Enhancing and Accelerating Brain MRI through Deep Learning Reconstruction Using Prior Subject-Specific Imaging](https://arxiv.org/abs/2507.21349)
*Amirmohammad Shamaei,Alexander Stebner,Salome,Bosshart,Johanna Ospel,Gouri Ginde,Mariana Bento,Roberto Souza*

Main category: cs.CV

> A deep-learning-based MRI reconstruction framework is proposed to significantly improve scan quality and reduce acquisition time by integrating information from previous scans, demonstrating superior performance compared to existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation stems from the need to reduce long MRI acquisition times which lead to increased costs and decreased patient comfort. The goal is to integrate prior MRI information more efficiently to enhance current scan quality without extending the acquisition time.

**Method:** We propose a deep-learning-based MRI reconstruction framework that includes an initial reconstruction network, a deep registration model, and a transformer-based enhancement network. This method aims to overcome the time-consuming process of integrating prior information from previous scans to improve the quality of current scans.

**Result:** Our validation on a longitudinal dataset (2,808 images from 18 subjects at 4 acceleration factors: R5, R10, R15, R20) showed significant improvement over existing methods (p < 0.05, Wilcoxon signed-rank test). The method was noted to improve brain segmentation accuracy and volumetric agreement while also reducing reconstruction time.

**Conclusion:** The proposed framework successfully enhances MRI reconstruction quality and accuracy, and reduces the reconstruction time compared to traditional methods, making it a more suitable option for clinical applications.

**Abstract:** Magnetic resonance imaging (MRI) is a crucial medical imaging modality.
However, long acquisition times remain a significant challenge, leading to
increased costs, and reduced patient comfort. Recent studies have shown the
potential of using deep learning models that incorporate information from prior
subject-specific MRI scans to improve reconstruction quality of present scans.
Integrating this prior information requires registration of the previous scan
to the current image reconstruction, which can be time-consuming. We propose a
novel deep-learning-based MRI reconstruction framework which consists of an
initial reconstruction network, a deep registration model, and a
transformer-based enhancement network. We validated our method on a
longitudinal dataset of T1-weighted MRI scans with 2,808 images from 18
subjects at four acceleration factors (R5, R10, R15, R20). Quantitative metrics
confirmed our approach's superiority over existing methods (p < 0.05, Wilcoxon
signed-rank test). Furthermore, we analyzed the impact of our MRI
reconstruction method on the downstream task of brain segmentation and observed
improved accuracy and volumetric agreement with reference segmentations. Our
approach also achieved a substantial reduction in total reconstruction time
compared to methods that use traditional registration algorithms, making it
more suitable for real-time clinical applications. The code associated with
this work is publicly available at
https://github.com/amirshamaei/longitudinal-mri-deep-recon.

</details>


### [37] [Group Relative Augmentation for Data Efficient Action Detection](https://arxiv.org/abs/2507.21353)
*Deep Anil Patel,Iain Melvin,Zachary Izzo,Martin Renqiang Min*

Main category: cs.CV

> 研究提出了一种结合参数有效调优和内部特征增强的方法，以解决视频-语言模型在有限数据下的动作检测任务中的适应问题，并在多个数据集上验证了其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于解决在有限训练数据下，将大型视频语言模型（VLM）有效适应到动作检测任务中时存在的过拟合及场景级预训练与所需的人体中心理解之间的粒度不匹配问题。

**Method:** 研究采用了结合参数有效调优（LoRA）和内部特征增强的方法。特征增强是在VLM主干内通过冻结的方法实现的，利用了FiLM技术。此外，提出了一种可以动态调节每种增强样本在训练中贡献的分组加权损失函数，该损失函数基于每个样本的预测结果与群体平均预测的离散程度。

**Result:** 该研究针对利用大型视频-语言模型（VLM）进行动作检测时面临的小数据集下的过拟合和粒度不匹配问题，提出了一个高效的适应策略，该策略结合了参数有效调优方法（如LoRA），同时引入了一种新的可学习内部特征增强。这些增强通过FiLM在冷冻的VLM主干中应用，生成直接相关的多样化特征变异。此外，研究还引入了一个基于预测离群度的动态训练贡献调整的分组加权损失函数，以促进学习更加健壮。该方法在复杂多标签、多对象动作检测数据集（如AVA，MOMA）上展示了强大的mAP性能，并在从少量范例中调整VLM方面显示出显著的数据效率。

**Conclusion:** 该方法的有效性已经在复杂的多标签、多对象动作检测数据集（如AVA和MOMA）上得到了验证，表现出了强健的mAP性能，并且在从少量范例中调整VLM时表现出显著的数据效率。

**Abstract:** Adapting large Video-Language Models (VLMs) for action detection using only a
few examples poses challenges like overfitting and the granularity mismatch
between scene-level pre-training and required person-centric understanding. We
propose an efficient adaptation strategy combining parameter-efficient tuning
(LoRA) with a novel learnable internal feature augmentation. Applied within the
frozen VLM backbone using FiLM, these augmentations generate diverse feature
variations directly relevant to the task. Additionally, we introduce a
group-weighted loss function that dynamically modulates the training
contribution of each augmented sample based on its prediction divergence
relative to the group average. This promotes robust learning by prioritizing
informative yet reasonable augmentations. We demonstrate our method's
effectiveness on complex multi-label, multi-person action detection datasets
(AVA, MOMA), achieving strong mAP performance and showcasing significant data
efficiency for adapting VLMs from limited examples.

</details>


### [38] [Collaborative Perceiver: Elevating Vision-based 3D Object Detection via Local Density-Aware Spatial Occupancy](https://arxiv.org/abs/2507.21358)
*Jicheng Yuan,Manh Nguyen Duc,Qian Liu,Manfred Hauswirth,Danh Le Phuoc*

Main category: cs.CV

> 研究提出了一种名为Collaborative Perceiver (CoP)的多任务学习框架，通过引入空间占用信息来提升3D目标检测的效果。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法通常通过合并提取的对象特征来构建BEV表示，忽视了道路和人行道等本征环境背景，这阻碍了检测器全面感知物理世界的特点。为此，提出了这种方法。

**Method:** 引入了一个多任务学习框架，称为Collaborative Perceiver (CoP)，该框架利用空间占用作为辅助信息，以挖掘3D目标检测和占用预测任务之间共享的结构和概念相似性，从而弥补空间表示和特征细化的缺口。首先提出了一种生成包含局部密度信息（LDO）的密集占用真值的管道，用于重建详细的环境信息。接着采用体素-高度引导的采样（VHS）策略，根据不同的物体特性提取细粒度的局部特征。此外，开发了一个全局-局部协作特征融合（CFF）模块，无缝结合两个任务间互补的知识，从而组成更稳健的鸟瞰视图（BEV）表示。

**Result:** 在nuScenes基准上的广泛实验表明，CoP优于现有的基于视觉的框架，在测试集上实现了49.5%的mAP和59.2%的NDS。

**Conclusion:** 研究提出的Collaborative Perceiver框架通过多任务学习，提升了鸟瞰视图3D物体检测的效果，证明了该方法的有效性和优越性。

**Abstract:** Vision-based bird's-eye-view (BEV) 3D object detection has advanced
significantly in autonomous driving by offering cost-effectiveness and rich
contextual information. However, existing methods often construct BEV
representations by collapsing extracted object features, neglecting intrinsic
environmental contexts, such as roads and pavements. This hinders detectors
from comprehensively perceiving the characteristics of the physical world. To
alleviate this, we introduce a multi-task learning framework, Collaborative
Perceiver (CoP), that leverages spatial occupancy as auxiliary information to
mine consistent structural and conceptual similarities shared between 3D object
detection and occupancy prediction tasks, bridging gaps in spatial
representations and feature refinement. To this end, we first propose a
pipeline to generate dense occupancy ground truths incorporating local density
information (LDO) for reconstructing detailed environmental information. Next,
we employ a voxel-height-guided sampling (VHS) strategy to distill fine-grained
local features according to distinct object properties. Furthermore, we develop
a global-local collaborative feature fusion (CFF) module that seamlessly
integrates complementary knowledge between both tasks, thus composing more
robust BEV representations. Extensive experiments on the nuScenes benchmark
demonstrate that CoP outperforms existing vision-based frameworks, achieving
49.5\% mAP and 59.2\% NDS on the test set. Code and supplementary materials are
available at this link https://github.com/jichengyuan/Collaborative-Perceiver.

</details>


### [39] [Evaluating Deep Learning Models for African Wildlife Image Classification: From DenseNet to Vision Transformers](https://arxiv.org/abs/2507.21364)
*Lukman Jibril Aliyu,Umar Sani Muhammad,Bilqisu Ismail,Nasiru Muhammad,Almustapha A Wakili,Seid Muhie Yimam,Shamsuddeen Hassan Muhammad,Mustapha Abdullahi*

Main category: cs.CV

> 本研究通过比较深度学习模型在非洲野生动物图像分类中的性能，提出了在现实世界中应用深度学习进行野生动物保护的可行性和应注意的权衡问题。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于非洲野生动物种群在过去五十年内面临严重威胁，数量下降超过65%，本研究旨在通过深度学习图像分类技术为生物多样性监测和保护提供一种有前景的工具。

**Method:** 本研究比较了四种深度学习模型（DenseNet-201, ResNet-152, EfficientNet-B4, Vision Transformer ViT-H/14）在非洲野生动物图像分类中的表现，使用了含有四种物种（水牛、大象、犀牛和斑马）的数据集，并且采用了迁移学习方法，利用冻结的特征提取器进行实验评估。

**Result:** 实验结果显示，DenseNet-201 在卷积神经网络中表现最好，准确率为67%；而 Vision Transformer ViT-H/14 表现出最高的准确率99%，但计算成本显著更高，导致部署成为问题。

**Conclusion:** 研究结果突显了在准确性、资源需求和部署之间的权衡关系。DenseNet-201 成功整合进入Hugging Face Gradio Space，实现了在实地应用中的实时使用，展示了在保护场景中部署轻量级模型的可行性。

**Abstract:** Wildlife populations in Africa face severe threats, with vertebrate numbers
declining by over 65% in the past five decades. In response, image
classification using deep learning has emerged as a promising tool for
biodiversity monitoring and conservation. This paper presents a comparative
study of deep learning models for automatically classifying African wildlife
images, focusing on transfer learning with frozen feature extractors. Using a
public dataset of four species: buffalo, elephant, rhinoceros, and zebra; we
evaluate the performance of DenseNet-201, ResNet-152, EfficientNet-B4, and
Vision Transformer ViT-H/14. DenseNet-201 achieved the best performance among
convolutional networks (67% accuracy), while ViT-H/14 achieved the highest
overall accuracy (99%), but with significantly higher computational cost,
raising deployment concerns. Our experiments highlight the trade-offs between
accuracy, resource requirements, and deployability. The best-performing CNN
(DenseNet-201) was integrated into a Hugging Face Gradio Space for real-time
field use, demonstrating the feasibility of deploying lightweight models in
conservation settings. This work contributes to African-grounded AI research by
offering practical insights into model selection, dataset preparation, and
responsible deployment of deep learning tools for wildlife conservation.

</details>


### [40] [Exploring Probabilistic Modeling Beyond Domain Generalization for Semantic Segmentation](https://arxiv.org/abs/2507.21367)
*I-Hsiang Chen,Hua-En Chang,Wei-Ting Chen,Jenq-Neng Hwang,Sy-Yen Kuo*

Main category: cs.CV

> 引入PDAF框架，通过概率扩散建模增强分割模型对未见环境的泛化能力，解决了忽视隐域先验导致泛化能力下降的问题。

<details>
  <summary>Details</summary>

**Motivation:** 文章旨在解决现有方法忽视隐域先验，仅通过对特征进行投影来提升特征对齐，从而导致迁移至未见环境时性能下降的问题。

**Method:** PDAF, 即概率扩散对齐框架，通过引入隐域先验来捕捉域偏移，并将其作为条件因素来对齐源域和未见的目标域。该框架包含三个模块：隐域先验提取器（LPE）预测隐域先验，域补偿模块（DCM）调整特征表示以减轻域偏移，扩散先验估计器（DPE）通过扩散过程估计隐域先验，无需配对样本。

**Result:** 通过模拟隐域偏移，PDAF能够有效地对预训练分割模型进行集成，提高了模型在复杂目标条件下的泛化能力。实验结果显示了PDAF框架在各种具有挑战性的城市场景中的有效性。

**Conclusion:** PDAF框架通过迭代建模域偏移过程，逐步细化特征表示，从而提升了分割模型在未见环境下的性能。

**Abstract:** Domain Generalized Semantic Segmentation (DGSS) is a critical yet challenging
task, as domain shifts in unseen environments can severely compromise model
performance. While recent studies enhance feature alignment by projecting
features into the source domain, they often neglect intrinsic latent domain
priors, leading to suboptimal results. In this paper, we introduce PDAF, a
Probabilistic Diffusion Alignment Framework that enhances the generalization of
existing segmentation networks through probabilistic diffusion modeling. PDAF
introduces a Latent Domain Prior (LDP) to capture domain shifts and uses this
prior as a conditioning factor to align both source and unseen target domains.
To achieve this, PDAF integrates into a pre-trained segmentation model and
utilizes paired source and pseudo-target images to simulate latent domain
shifts, enabling LDP modeling. The framework comprises three modules: the
Latent Prior Extractor (LPE) predicts the LDP by supervising domain shifts; the
Domain Compensation Module (DCM) adjusts feature representations to mitigate
domain shifts; and the Diffusion Prior Estimator (DPE) leverages a diffusion
process to estimate the LDP without requiring paired samples. This design
enables PDAF to iteratively model domain shifts, progressively refining feature
representations to enhance generalization under complex target conditions.
Extensive experiments validate the effectiveness of PDAF across diverse and
challenging urban scenes.

</details>


### [41] [Top2Pano: Learning to Generate Indoor Panoramas from Top-Down View](https://arxiv.org/abs/2507.21371)
*Zitong Zhang,Suranjan Gautam,Rui Yu*

Main category: cs.CV

> This paper proposes Top2Pano, an end-to-end model for realistic indoor panoramic synthesis from top-down views, achieving better performance than baselines by enhancing realism and structural fidelity.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the challenge of generating immersive 360° indoor panoramas from 2D top-down views, which is difficult due to the lack of explicit 3D structure and the need for geometric consistency and photorealism. The application areas are broad, including virtual reality, interior design, real estate, and robotics.

**Method:** Our method estimates volumetric occupancy to infer 3D structures from top-down 2D views, then uses volumetric rendering to generate coarse color and depth panoramas. These panoramas guide a diffusion-based refinement stage using ControlNet, enhancing the realism and structural fidelity of the final panoramic images.

**Result:** The evaluation on two datasets demonstrates that Top2Pano outperforms the baselines in effectively reconstructing geometry, occlusions, and the spatial arrangement, and it also generalizes well, creating high-quality panoramas from schematic floorplans.

**Conclusion:** The study concludes that Top2Pano shows potential for bridging the gap between top-down views and immersive indoor synthesis, contributing valuable solutions for fields like virtual interior design and real estate visualization.

**Abstract:** Generating immersive 360{\deg} indoor panoramas from 2D top-down views has
applications in virtual reality, interior design, real estate, and robotics.
This task is challenging due to the lack of explicit 3D structure and the need
for geometric consistency and photorealism. We propose Top2Pano, an end-to-end
model for synthesizing realistic indoor panoramas from top-down views. Our
method estimates volumetric occupancy to infer 3D structures, then uses
volumetric rendering to generate coarse color and depth panoramas. These guide
a diffusion-based refinement stage using ControlNet, enhancing realism and
structural fidelity. Evaluations on two datasets show Top2Pano outperforms
baselines, effectively reconstructing geometry, occlusions, and spatial
arrangements. It also generalizes well, producing high-quality panoramas from
schematic floorplans. Our results highlight Top2Pano's potential in bridging
top-down views with immersive indoor synthesis.

</details>


### [42] [Multimodal LLMs as Customized Reward Models for Text-to-Image Generation](https://arxiv.org/abs/2507.21391)
*Shijie Zhou,Ruiyi Zhang,Huaisheng Zhu,Branislav Kveton,Yufan Zhou,Jiuxiang Gu,Jian Chen,Changyou Chen*

Main category: cs.CV

> LLaVA-Reward is an efficient reward model for evaluating text-to-image generations, outperforming existing methods by leveraging hidden states of multimodal language models and enhancing visual-textual interaction through a Skip-connection Cross Attention module.

<details>
  <summary>Details</summary>

**Motivation:** Existing methods require instruction-following data for supervised fine-tuning and evaluate generation quality from text responses, which is time-consuming and difficult to train. LLaVA-Reward aims to address these issues by utilizing the hidden states of MLLMs and supporting different preference data types for efficient fine-tuning.

**Method:** We propose LLaVA-Reward, an efficient reward model for automatically evaluating text-to-image generations using pretrained multimodal large language models (MLLMs). To improve the interaction between visual and textual representations in decoder-only MLLMs, a Skip-connection Cross Attention (SkipCA) module is introduced, which connects early-layer visual features with later-layer hidden representations.

**Result:** Empirical results show that LLaVA-Reward surpasses conventional and MLLM-based methods in generating human-aligned scores for automatic evaluations and inference-time scaling in text-to-image generations.

**Conclusion:** LLaVA-Reward is an effective and efficient method for automating the evaluation of text-to-image generations, resulting in improved quality and human-aligned scores compared to conventional and multimodal language model-based approaches.

**Abstract:** We introduce LLaVA-Reward, an efficient reward model designed to
automatically evaluate text-to-image (T2I) generations across multiple
perspectives, leveraging pretrained multimodal large language models (MLLMs).
Existing MLLM-based approaches require instruction-following data for
supervised fine-tuning and evaluate generation quality on analyzing text
response, which is time-consuming and difficult to train. To address this
problem, we propose LLaVA-Reward, which directly utilizes the hidden states of
MLLMs given text-image pairs. To enhance the bidirectional interaction between
visual and textual representations in decoder-only MLLMs, we further propose
adding a Skip-connection Cross Attention (SkipCA) module. This design enhances
text-image correlation reasoning by connecting early-layer visual features with
later-layer hidden representations.In addition, LLaVA-Reward supports different
types of preference data for efficient fine-tuning, including paired preference
data and unpaired data. We train LLaVA-Reward on four evaluation perspectives:
text-image alignment, fidelity/artifact, safety, and overall ranking. Empirical
results demonstrate that LLaVA-Reward outperforms conventional and MLLM-based
methods in generating human-aligned scores for automatic evaluations and
inference-time scaling in text-to-image generations.

</details>


### [43] [ReGATE: Learning Faster and Better with Fewer Tokens in MLLMs](https://arxiv.org/abs/2507.21420)
*Chaoyu Li,Yogesh Kulkarni,Pooyan Fazli*

Main category: cs.CV

> ReGATE方法通过教师-学生框架在训练大型多模态语言模型时选择性处理重要标记来减少计算负担，显著提高了训练效率。

<details>
  <summary>Details</summary>

**Motivation:** 现存的效率方法主要集中在推理阶段，对训练阶段的效果有限。因此，研究提出了一种新的方法来优化训练过程中的计算成本。

**Method:** ReGATE采用教师-学生框架，通过计算每个标记的参考损失并与学生模型的难度分数结合，实现选择性处理关键标记，降低计算负担。

**Result:** 实验结果显示，应用于VideoLLaMA2时，ReGATE能够在仅使用35%标记的情况下达到标准训练的峰值精度，并且速度提升2倍。

**Conclusion:** ReGATE证明了在减少计算负担的同时还能够提高多模态基准测试的性能。

**Abstract:** The computational cost of training multimodal large language models (MLLMs)
rapidly increases with the number of tokens involved. Existing efficiency
methods primarily target inference and rely on token reduction or merging,
offering limited benefit during training. In this paper, we propose ReGATE
(Reference$-$Guided Adaptive Token Elision), an adaptive token pruning method
for accelerating MLLM training. Specifically, ReGATE adopts a teacher-student
framework in which the MLLM being trained serves as the student, and a frozen
reference large language model (LLM) acts as the teacher. The teacher computes
per-token reference losses, which are combined with an exponential moving
average (EMA) of the student's own difficulty scores. This adaptive
difficulty-based scoring enables the selective processing of crucial tokens
while bypassing less informative ones in the forward pass, significantly
reducing computational overhead. Experiments demonstrate that ReGATE, when
applied to VideoLLaMA2, matches the peak accuracy of standard training on
MVBench up to 2$\times$ faster, using only 35% of the tokens. With additional
training, it even surpasses the baseline on several multimodal benchmarks, all
while reducing the total token count by over 41%. Code and models will be
released soon.

</details>


### [44] [MapDiffusion: Generative Diffusion for Vectorized Online HD Map Construction and Uncertainty Estimation in Autonomous Driving](https://arxiv.org/abs/2507.21423)
*Thomas Monninger,Zihan Zhang,Zhipeng Mo,Md Zafar Anwar,Steffen Staab,Sihao Ding*

Main category: cs.CV

> 本文提出了MapDiffusion，一种新的生成方法，用于提高自主驾驶系统中在线矢量化高清地图构建的不确定性和鲁棒性，同时提高性能。

<details>
  <summary>Details</summary>

**Motivation:** 传统的地图构建模型提供确定性的点估计，无法捕捉不确定性及真实世界环境中的固有模糊性，如遮挡和缺少车道标记。

**Method:** MapDiffusion提出了一种新的生成方法，利用扩散范式从学习到的BEV潜在网格条件下来迭代细化随机初始化的查询，生成多个可能的地图样本。

**Result:** 在nuScenes数据集上的广泛实验表明，MapDiffusion在线地图构建方面达到了最先进的性能，基线模型的单个样本性能提高了5%，并且多个样本的一致改进验证了分布建模的好处。

**Conclusion:** 通过建模整个地图分布，MapDiffusion增强了在线矢量化高清地图构建的鲁棒性和可靠性，使自动驾驶汽车在复杂环境中实现不确定性感知决策。

**Abstract:** Autonomous driving requires an understanding of the static environment from
sensor data. Learned Bird's-Eye View (BEV) encoders are commonly used to fuse
multiple inputs, and a vector decoder predicts a vectorized map representation
from the latent BEV grid. However, traditional map construction models provide
deterministic point estimates, failing to capture uncertainty and the inherent
ambiguities of real-world environments, such as occlusions and missing lane
markings. We propose MapDiffusion, a novel generative approach that leverages
the diffusion paradigm to learn the full distribution of possible vectorized
maps. Instead of predicting a single deterministic output from learned queries,
MapDiffusion iteratively refines randomly initialized queries, conditioned on a
BEV latent grid, to generate multiple plausible map samples. This allows
aggregating samples to improve prediction accuracy and deriving uncertainty
estimates that directly correlate with scene ambiguity. Extensive experiments
on the nuScenes dataset demonstrate that MapDiffusion achieves state-of-the-art
performance in online map construction, surpassing the baseline by 5% in
single-sample performance. We further show that aggregating multiple samples
consistently improves performance along the ROC curve, validating the benefit
of distribution modeling. Additionally, our uncertainty estimates are
significantly higher in occluded areas, reinforcing their value in identifying
regions with ambiguous sensor input. By modeling the full map distribution,
MapDiffusion enhances the robustness and reliability of online vectorized HD
map construction, enabling uncertainty-aware decision-making for autonomous
vehicles in complex environments.

</details>


### [45] [Dual Cross-image Semantic Consistency with Self-aware Pseudo Labeling for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2507.21440)
*Han Wu,Chong Wang,Zhiming Cui*

Main category: cs.CV

> 提出一种名为DuCiSC的框架，用于解决现有半监督学习方法在医学图像分割中存在的问题，例如像素级一致性训练、跨图区域一致性及标签与未标签数据不平衡产生的特征差异问题。结果显示该框架在多个数据集上的分割效果优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 当前的半监督学习方法在处理医学图像分割时，主要依赖于通过伪标记实施的像素级一致性训练，但忽视了更全面的语义级别的一致性（如对象区域），并且受标签和未标签数据不平衡的影响，提取的特征有许多差异。为解决这些问题，研究提出了这种新的框架。

**Method:** 提出了一种名为DuCiSC（Dual Cross-image Semantic Consistency）的框架，用于半监督医学图像分割。该框架不仅强化了像素级别的语义一致性，还通过两个范式来鼓励图像间区域级别的语义一致性。这两个范式分别用于标签图像和未标签图像以及标签图像和融合图像之间的语义对齐，以此来解决由于标签和未标签数据不平衡所导致的特征差异问题。此外，还设计了一种新的自我感知置信度估计策略来准确筛选可靠的伪标签，从而利用未标签数据的训练动态。

**Result:** 该方法在四个数据集上进行了广泛验证，包括两个流行的二分类基准数据集用于分割左心房和胰腺、一个多类的自动心脏诊断挑战数据集和一个分割复杂的解剖结构中下牙槽神经的具有挑战性的情景，结果表明优于现有的最先进的方法。

**Conclusion:** 研究表明，DuCiSC框架能够有效处理医学图像分割中的半监督学习问题，通过区域级别的语义一致性增强了分割效果，并且在多个数据集上优于现有的最先进的方法。代码已公开。

**Abstract:** Semi-supervised learning has proven highly effective in tackling the
challenge of limited labeled training data in medical image segmentation. In
general, current approaches, which rely on intra-image pixel-wise consistency
training via pseudo-labeling, overlook the consistency at more comprehensive
semantic levels (e.g., object region) and suffer from severe discrepancy of
extracted features resulting from an imbalanced number of labeled and unlabeled
data. To overcome these limitations, we present a new \underline{Du}al
\underline{C}ross-\underline{i}mage \underline{S}emantic
\underline{C}onsistency (DuCiSC) learning framework, for semi-supervised
medical image segmentation. Concretely, beyond enforcing pixel-wise semantic
consistency, DuCiSC proposes dual paradigms to encourage region-level semantic
consistency across: 1) labeled and unlabeled images; and 2) labeled and fused
images, by explicitly aligning their prototypes. Relying on the dual paradigms,
DuCiSC can effectively establish consistent cross-image semantics via prototype
representations, thereby addressing the feature discrepancy issue. Moreover, we
devise a novel self-aware confidence estimation strategy to accurately select
reliable pseudo labels, allowing for exploiting the training dynamics of
unlabeled data. Our DuCiSC method is extensively validated on four datasets,
including two popular binary benchmarks in segmenting the left atrium and
pancreas, a multi-class Automatic Cardiac Diagnosis Challenge dataset, and a
challenging scenario of segmenting the inferior alveolar nerve that features
complicated anatomical structures, showing superior segmentation results over
previous state-of-the-art approaches. Our code is publicly available at
\href{https://github.com/ShanghaiTech-IMPACT/DuCiSC}{https://github.com/ShanghaiTech-IMPACT/DuCiSC}.

</details>


### [46] [Recursive Visual Imagination and Adaptive Linguistic Grounding for Vision Language Navigation](https://arxiv.org/abs/2507.21450)
*Bolei Chen,Jiaxu Kang,Yifei Wang,Ping Zhong,Qi Wu,Jianxin Wang*

Main category: cs.CV

> 提出了递归视觉想象（RVI）技术和自适应语言对齐（ALG）方法，通过递归总结路径上的视觉感知并进行语言对齐，提高了Vision Language Navigation (VLN)任务中的导航准确性。

<details>
  <summary>Details</summary>

**Motivation:** 现有模型在VLN中面临过于详尽的场景表示和语言-视觉对齐的模糊性问题，导致对导航有利的高层次场景数据分析不足和违反语言指令的行为。

**Method:** 通过将历史轨迹建模为紧凑的神经网格并引入RVI技术，使代理关注视觉转换的规律性和语义场景布局；此外，提出ALG方法使学到的情景记忆与不同的语言成分精确匹配，以提升导航操作的准确性。

**Result:** 在具有挑战性的VLN-CE和ObjectNav任务中，新提出的导航策略优于现有最先进技术。

**Conclusion:** 研究显示，RVI和ALG技术改进了VLN中的语境理解和导航决策。

**Abstract:** Vision Language Navigation (VLN) typically requires agents to navigate to
specified objects or remote regions in unknown scenes by obeying linguistic
commands. Such tasks require organizing historical visual observations for
linguistic grounding, which is critical for long-sequence navigational
decisions. However, current agents suffer from overly detailed scene
representation and ambiguous vision-language alignment, which weaken their
comprehension of navigation-friendly high-level scene priors and easily lead to
behaviors that violate linguistic commands. To tackle these issues, we propose
a navigation policy by recursively summarizing along-the-way visual
perceptions, which are adaptively aligned with commands to enhance linguistic
grounding. In particular, by structurally modeling historical trajectories as
compact neural grids, several Recursive Visual Imagination (RVI) techniques are
proposed to motivate agents to focus on the regularity of visual transitions
and semantic scene layouts, instead of dealing with misleading geometric
details. Then, an Adaptive Linguistic Grounding (ALG) technique is proposed to
align the learned situational memories with different linguistic components
purposefully. Such fine-grained semantic matching facilitates the accurate
anticipation of navigation actions and progress. Our navigation policy
outperforms the state-of-the-art methods on the challenging VLN-CE and
ObjectNav tasks, showing the superiority of our RVI and ALG techniques for VLN.

</details>


### [47] [Boost Self-Supervised Dataset Distillation via Parameterization, Predefined Augmentation, and Approximation](https://arxiv.org/abs/2507.21455)
*Sheng-Feng Yu,Jia-Jiun Yao,Wei-Chen Chiu*

Main category: cs.CV

> 本文提出了一种自监督数据集蒸馏方法，通过引入低维基参数化、预定数据增强和轻量级网络建模，能够更高效且紧凑地蒸馏自监督数据集，实验验证了该方法的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 随着数据集规模的快速增长，训练大型深度模型的计算成本变得非常高昂。本文旨在通过蒸馏图像及其自监督训练的表示来减轻这一问题，特别是为了比现有方法更真实和紧凑地保留原始数据集的关键特性。

**Method:** 本文提出了一种名为自监督数据集蒸馏的方法，它通过引入图像和表示的不同低维基的创新参数化来提取原始数据集的关键特性，使用预定的数据增强来应对数据增强随机性引起的不稳定性，并利用轻量级网络建模同一图像的增强视图的表示之间的联系，从而实现更紧凑的蒸馏对。

**Result:** 实验在多个数据集上验证了本文方法在蒸馏效率、跨架构泛化能力和迁移学习性能上的优越性。

**Conclusion:** 本文的方法能够在保持模型性能的同时，大幅减少所需数据集的大小，从而降低计算成本，并在跨架构泛化和迁移学习方面具有显著优势。

**Abstract:** Although larger datasets are crucial for training large deep models, the
rapid growth of dataset size has brought a significant challenge in terms of
considerable training costs, which even results in prohibitive computational
expenses. Dataset Distillation becomes a popular technique recently to reduce
the dataset size via learning a highly compact set of representative exemplars,
where the model trained with these exemplars ideally should have comparable
performance with respect to the one trained with the full dataset. While most
of existing works upon dataset distillation focus on supervised datasets, we
instead aim to distill images and their self-supervisedly trained
representations into a distilled set. This procedure, named as Self-Supervised
Dataset Distillation, effectively extracts rich information from real datasets,
yielding the distilled sets with enhanced cross-architecture generalizability.
Particularly, in order to preserve the key characteristics of original dataset
more faithfully and compactly, several novel techniques are proposed: 1) we
introduce an innovative parameterization upon images and representations via
distinct low-dimensional bases, where the base selection for parameterization
is experimentally shown to play a crucial role; 2) we tackle the instability
induced by the randomness of data augmentation -- a key component in
self-supervised learning but being underestimated in the prior work of
self-supervised dataset distillation -- by utilizing predetermined
augmentations; 3) we further leverage a lightweight network to model the
connections among the representations of augmented views from the same image,
leading to more compact pairs of distillation. Extensive experiments conducted
on various datasets validate the superiority of our approach in terms of
distillation efficiency, cross-architecture generalization, and transfer
learning performance.

</details>


### [48] [An Angular-Temporal Interaction Network for Light Field Object Tracking in Low-Light Scenes](https://arxiv.org/abs/2507.21460)
*Mianzhao Wang,Fan Shi,Xu Cheng,Feifei Zhang,Shengyong Chen*

Main category: cs.CV

> 本文提出了一种新颖的4D光场ESP表示方法和交互网络ATINet，它们通过利用极线平面内光线角度变化来增强低光场景中的视觉表现，并在物体跟踪任务中取得了先进性能。

<details>
  <summary>Details</summary>

**Motivation:** 高质量的4D光场表示与有效的角度特征建模对于场景感知至关重要，尤其是在复杂低光场景中的移动目标识别。然而，最近的发展在时间域内的角度建模可靠性方面仍然存在问题。

**Method:** 本研究提出了一个新颖的光场极线面结构图像（ESI）表示，它明确界定了光场中的几何结构，并利用极线平面内光线角度的突然变化来增强低光场景中的视觉表现，减少高维光场的空间冗余。另外，研究还提出了一种用于光场物体跟踪的角-时间交互网络（ATINet），该网络从光场的几何结构线索和角-时间交互线索中学习角度感知的表示，并可通过自我监督方式进行优化，以提高时间域内几何特征交互的质量。

**Result:** 实验表明，ATINet在单个物体跟踪中达到了最先进的性能，并且所提出的方法扩展到多物体跟踪中也显示了高质量光场角-时间建模的效果。

**Conclusion:** 基于高性能光场极线面结构图像（ESI）表示和角-时间交互网络（ATINet），研究人员表明他们的方法在物体跟踪中具有显著性能，并特别适用于低光场景。

**Abstract:** High-quality 4D light field representation with efficient angular feature
modeling is crucial for scene perception, as it can provide discriminative
spatial-angular cues to identify moving targets. However, recent developments
still struggle to deliver reliable angular modeling in the temporal domain,
particularly in complex low-light scenes. In this paper, we propose a novel
light field epipolar-plane structure image (ESI) representation that explicitly
defines the geometric structure within the light field. By capitalizing on the
abrupt changes in the angles of light rays within the epipolar plane, this
representation can enhance visual expression in low-light scenes and reduce
redundancy in high-dimensional light fields. We further propose an
angular-temporal interaction network (ATINet) for light field object tracking
that learns angular-aware representations from the geometric structural cues
and angular-temporal interaction cues of light fields. Furthermore, ATINet can
also be optimized in a self-supervised manner to enhance the geometric feature
interaction across the temporal domain. Finally, we introduce a large-scale
light field low-light dataset for object tracking. Extensive experimentation
demonstrates that ATINet achieves state-of-the-art performance in single object
tracking. Furthermore, we extend the proposed method to multiple object
tracking, which also shows the effectiveness of high-quality light field
angular-temporal modeling.

</details>


### [49] [Describe, Adapt and Combine: Empowering CLIP Encoders for Open-set 3D Object Retrieval](https://arxiv.org/abs/2507.21489)
*Zhichuan Wang,Yang Zhou,Zhe Liu,Rui Yu,Song Bai,Yulong Wang,Xinwei He,Xiang Bai*

Main category: cs.CV

> DAC框架结合CLIP和MLLM，利用多视角图像生成泛化的3D表示形式，实现了良好的开放集合3D对象检索效果，提高了对未知类别的泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 现有的3D对象检索方法无法生成泛化的表示形式，尤其是在3D训练数据不足的情况下。基于CLIP模型，作者提出了一种新的框架DAC，旨在使用多视角图像生成适用于开放集合下的3D对象检索的泛化表示。

**Method:** DAC框架结合CLIP模型与多模态大语言模型（MLLM），利用多视角图像进行开放集合下的3D对象检索。在训练时，MLLM描述已知类别信息，以适应CLIP的训练目标；在推理时，提供未知对象的外部提示以补充视觉线索。此外，引入了Additive-Bias Low-Rank适应方法（AB-LoRA）以减轻过拟合并提高对未知类别的泛化能力。

**Result:** DAC在四个开放集合3D对象检索数据集上平均mAP提高了10.01%，表现优于以往方法。而且，它的泛化性能也在基于图像和跨数据集的设置中得到验证。

**Conclusion:** 提出了一种结合CLIP和MLLM的简单而有效的3D对象检索框架DAC，证明了这种方法在开放集合3D对象检索任务中的有效性和泛化能力。

**Abstract:** Open-set 3D object retrieval (3DOR) is an emerging task aiming to retrieve 3D
objects of unseen categories beyond the training set. Existing methods
typically utilize all modalities (i.e., voxels, point clouds, multi-view
images) and train specific backbones before fusion. However, they still
struggle to produce generalized representations due to insufficient 3D training
data. Being contrastively pre-trained on web-scale image-text pairs, CLIP
inherently produces generalized representations for a wide range of downstream
tasks. Building upon it, we present a simple yet effective framework named
Describe, Adapt and Combine (DAC) by taking only multi-view images for open-set
3DOR. DAC innovatively synergizes a CLIP model with a multi-modal large
language model (MLLM) to learn generalized 3D representations, where the MLLM
is used for dual purposes. First, it describes the seen category information to
align with CLIP's training objective for adaptation during training. Second, it
provides external hints about unknown objects complementary to visual cues
during inference. To improve the synergy, we introduce an Additive-Bias
Low-Rank adaptation (AB-LoRA), which alleviates overfitting and further
enhances the generalization to unseen categories. With only multi-view images,
DAC significantly surpasses prior arts by an average of +10.01\% mAP on four
open-set 3DOR datasets. Moreover, its generalization is also validated on
image-based and cross-dataset setups. Code is available at
https://github.com/wangzhichuan123/DAC.

</details>
