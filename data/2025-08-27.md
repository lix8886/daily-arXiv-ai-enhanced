<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 20]
- [cs.CV](#cs.CV) [Total: 14]
- [eess.IV](#eess.IV) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Semantic Attractors and the Emergence of Meaning: Towards a Teleological Model of AGI](https://arxiv.org/abs/2508.18290)
*Hans-Joachim Rudolph*

Main category: cs.CL

> 本文提出了一种通过递归张量转换形成的新型语义模型，这个模型可以通过复数空间中的循环操作模拟复杂的语言现象，而不再是依赖统计预测。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在探索一种新的语义处理方式，不依赖于现有的统计预测模型，而是通过递归张量转换的方式来构建新的含义。这要求一个全新的认知架构--不仅仅预测语言，而是塑造语言。

**Method:** 本文提出了一种基于复值意义空间中的语义吸引子的理论框架，与基于统计性下一个词预测的现有变压器模型相区别。该模型中，含义不是通过概率推断获得，而是通过递归张量转换形成。利用虚数单位i的循环操作，描述了一个能模拟讽刺、同音词和歧义的旋转语义结构。模型的核心是一个语义吸引子，它作为意向性代理（微维坦）作用于模拟过程，引导含义向稳定、清晰和表达深度转变。

**Result:** 该模型能够通过语义吸引子作用于语言的递归张量转换，通过意向上引导含义向稳定、清晰发展，提出了语义变换的新数学模型。

**Conclusion:** 真正含义的出现并不是来自模拟，而是通过向语义一致性递归收敛的结果，这需要一个新的、设计用于塑造语言而不是仅仅预测它的认知架构。

**Abstract:** This essay develops a theoretical framework for a semantic Artificial General
Intelligence (AGI) based on the notion of semantic attractors in complex-valued
meaning spaces. Departing from current transformer-based language models, which
operate on statistical next-token prediction, we explore a model in which
meaning is not inferred probabilistically but formed through recursive
tensorial transformation. Using cyclic operations involving the imaginary unit
\emph{i}, we describe a rotational semantic structure capable of modeling
irony, homonymy, and ambiguity. At the center of this model, however, is a
semantic attractor -- a teleological operator that, unlike statistical
computation, acts as an intentional agent (Microvitum), guiding meaning toward
stability, clarity, and expressive depth. Conceived in terms of gradient flows,
tensor deformations, and iterative matrix dynamics, the attractor offers a
model of semantic transformation that is not only mathematically suggestive,
but also philosophically significant. We argue that true meaning emerges not
from simulation, but from recursive convergence toward semantic coherence, and
that this requires a fundamentally new kind of cognitive architecture -- one
designed to shape language, not just predict it.

</details>


### [2] [LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions](https://arxiv.org/abs/2508.18321)
*Maojia Song,Tej Deep Pala,Weisheng Jin,Amir Zadeh,Chuan Li,Dorien Herremans,Soujanya Poria*

Main category: cs.CL

> 本文研究了在多代理系统中，大型语言模型如何建立信任、抵制错误信息并整合同伴输入，以期实现集体智能。提出了一个名为KAIROS的基准测试，结果显示GRPO策略表现最佳但降低了对社交影响的鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 虽然之前的工作集中在一致性偏差上，但本文进一步探讨了LLM如何根据过去的印象建立信任、抵制错误信息并整合与同伴的交互，这些都是在复杂社交动态下实现集体智能的关键因素。

**Method:** 我们提出了KAIROS，一个模拟问答竞赛中具有不同可靠性的同伴代理的基准测试，可以精细控制专家-新手角色、嘈杂的群体和对抗的同伴等条件。LLM接收历史交互和当前同伴的响应，以系统性地研究信任、同伴行动和自信如何影响决策。

**Result:** 实验结果显示，结合多代理背景、基于结果的奖励以及无约束推理的组相对策略优化（GRPO）在多个模型中表现出最佳的整体性能，但也会降低对社会影响的鲁棒性。

**Conclusion:** 根据研究结果，我们发现多代理背景下结合基于结果的奖励以及无约束推理的组相对策略优化可以达到最优性能，但同时也发现这种方法降低了模型对社会影响的抵抗力。

**Abstract:** Large language models (LLMs) are increasingly deployed in multi-agent systems
(MAS) as components of collaborative intelligence, where peer interactions
dynamically shape individual decision-making. Although prior work has focused
on conformity bias, we extend the analysis to examine how LLMs form trust from
previous impressions, resist misinformation, and integrate peer input during
interaction, key factors for achieving collective intelligence under complex
social dynamics. We present KAIROS, a benchmark simulating quiz contests with
peer agents of varying reliability, offering fine-grained control over
conditions such as expert-novice roles, noisy crowds, and adversarial peers.
LLMs receive both historical interactions and current peer responses, allowing
systematic investigation into how trust, peer action, and self-confidence
influence decisions. As for mitigation strategies, we evaluate prompting,
supervised fine-tuning, and reinforcement learning, Group Relative Policy
Optimisation (GRPO), across multiple models. Our results reveal that GRPO with
multi-agent context combined with outcome-based rewards and unconstrained
reasoning achieves the best overall performance, but also decreases the
robustness to social influence compared to Base models. The code and datasets
are available at: https://github.com/declare-lab/KAIROS.

</details>


### [3] [Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective](https://arxiv.org/abs/2508.18328)
*Masudul Hasan Masud Bhuiyan,Matteo Varvello,Yasir Zaki,Cristian-Alexandru Staicu*

Main category: cs.CL

> 本文提出了LangCrUX数据集并分析了多语言网页的无障碍性挑战，发现了无障碍提示与可视内容语言多样性不符的问题，并提出了一个语言感知的无障碍性测试工具Kizuki。

<details>
  <summary>Details</summary>

**Motivation:** 由于缺乏关于多语言网页内容的综合数据集，大规模研究这种问题的能力受到了限制。为了填补这一空白，作者引入了LangCrUX数据集。

**Method:** 通过创建LangCrUX数据集，该数据集包含120,000个网站和12种主要使用非拉丁字母的语言，作者进行了多语言网页无障碍性分析，并提出了Kizuki，一种语言感知的自动化无障碍性测试扩展程序。

**Result:** 分析发现无障碍提示往往不能反映可视内容的语言多样性，这降低了屏幕阅读器的有效性，并限制了网络无障碍性。

**Conclusion:** 研究表明，支持多种语言的网站无障碍提示未能充分反映内容的语言多样性，Kizuki的提出为解决语言不一致性提供了一个工具。

**Abstract:** English is the predominant language on the web, powering nearly half of the
world's top ten million websites. Support for multilingual content is
nevertheless growing, with many websites increasingly combining English with
regional or native languages in both visible content and hidden metadata. This
multilingualism introduces significant barriers for users with visual
impairments, as assistive technologies like screen readers frequently lack
robust support for non-Latin scripts and misrender or mispronounce non-English
text, compounding accessibility challenges across diverse linguistic contexts.
Yet, large-scale studies of this issue have been limited by the lack of
comprehensive datasets on multilingual web content. To address this gap, we
introduce LangCrUX, the first large-scale dataset of 120,000 popular websites
across 12 languages that primarily use non-Latin scripts. Leveraging this
dataset, we conduct a systematic analysis of multilingual web accessibility and
uncover widespread neglect of accessibility hints. We find that these hints
often fail to reflect the language diversity of visible content, reducing the
effectiveness of screen readers and limiting web accessibility. We finally
propose Kizuki, a language-aware automated accessibility testing extension to
account for the limited utility of language-inconsistent accessibility hints.

</details>


### [4] [Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models](https://arxiv.org/abs/2508.18381)
*Yuchun Fan,Yilin Wang,Yongyu Mu,Lei Huang,Bei Li,Xiaocheng Feng,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

> 通过识别和微调浅层中与多语言理解相关联的语言特异性神经元激活，PLAST方法显著提高了大视觉语言模型（LVLMs）的多语言能力，并且仅调整了14%的参数。

<details>
  <summary>Details</summary>

**Motivation:** 解决大视觉语言模型在多语言理解能力上的不平衡问题。

**Method:** 提出PLAST方法，通过精确的语言特异性层微调来提高模型的多语言能力，包括识别涉及多语言理解的层和使用问题翻译对调整这些层。

**Result:** 实验结果表明，PLAST在MM-Bench和MMMB基准上的表现优异，展示了模型多语言能力的提升，并且有效提升了处理低资源语言和复杂视觉推理任务的泛化能力。

**Conclusion:** PLAST通过聚焦于语言特异性神经元激活并微调涉及多语言理解的深层，以高效的方式增强了LVLMs的多语言能力，且泛化性强。

**Abstract:** Large vision-language models (LVLMs) have demonstrated exceptional
capabilities in understanding visual information with human languages but also
exhibit an imbalance in multilingual capabilities. In this work, we delve into
the multilingual working pattern of LVLMs and identify a salient correlation
between the multilingual understanding ability of LVLMs and language-specific
neuron activations in shallow layers. Building on this insight, we introduce
PLAST, a training recipe that achieves efficient multilingual enhancement for
LVLMs by Precise LAnguage-Specific layers fine-Tuning. PLAST first identifies
layers involved in multilingual understanding by monitoring language-specific
neuron activations. These layers are then precisely fine-tuned with
question-translation pairs to achieve multilingual alignment. Our empirical
results on MM-Bench and MMMB demonstrate that PLAST effectively improves the
multilingual capabilities of LVLMs and achieves significant efficiency with
only 14% of the parameters tuned. Further analysis reveals that PLAST can be
generalized to low-resource and complex visual reasoning tasks, facilitating
the language-specific visual information engagement in shallow layers.

</details>


### [5] [Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails](https://arxiv.org/abs/2508.18384)
*Kellen Tan Cheng,Anna Lisa Gentile,Chad DeLuca,Guang-Jie Ren*

Main category: cs.CL

> 本文提出了一种名为backprompting的方法，用于生成仿真构建健康建议守门员所需的训练数据，在与现行技术的比较中显示了显著性能优势。

<details>
  <summary>Details</summary>

**Motivation:** 开发和维护稳健的检测器在风险缓解方面面临诸多挑战，其中最大的挑战之一是获得用于训练高质量标签数据，尤其是在部署LLM之前。本文旨在解决这一问题。

**Method:** 提出了一种称为backprompting的方法，用于生成类似于真实LLM输出的带有标签的数据，特别是在健康建议守门员技术开发方面。此外，该方法结合了稀疏的人机协作聚类技术来标注生成的数据，创建与原始数据集相当但类似真实LLM输出的平行语料库。

**Result:** 该技术应用在了最难且最微妙的守门员领域之一，即识别LLM输出中的健康建议，并且显示出优于其他解决方案的表现。

**Conclusion:** 通过将现有数据集与合成样本融合，该研究产生了一种健壮的训练数据集，其提出的检测器能够在健康建议识别上，甚至拥有比GPT-4少400倍参数的情况下，性能提升最高3.73%。

**Abstract:** The pervasiveness of large language models (LLMs) in enterprise settings has
also brought forth a significant amount of risks associated with their usage.
Guardrails technologies aim to mitigate this risk by filtering LLMs'
input/output text through various detectors. However, developing and
maintaining robust detectors faces many challenges, one of which is the
difficulty in acquiring production-quality labeled data on real LLM outputs
prior to deployment. In this work, we propose backprompting, a simple yet
intuitive solution to generate production-like labeled data for health advice
guardrails development. Furthermore, we pair our backprompting method with a
sparse human-in-the-loop clustering technique to label the generated data. Our
aim is to construct a parallel corpus roughly representative of the original
dataset yet resembling real LLM output. We then infuse existing datasets with
our synthetic examples to produce robust training data for our detector. We
test our technique in one of the most difficult and nuanced guardrails: the
identification of health advice in LLM output, and demonstrate improvement
versus other solutions. Our detector is able to outperform GPT-4o by up to
3.73%, despite having 400x less parameters.

</details>


### [6] [Integral Transformer: Denoising Attention, Not Too Much Not Too Little](https://arxiv.org/abs/2508.18387)
*Ivan Kobyzev,Abbas Ghaddar,Dingtao Hu,Boxing Chen*

Main category: cs.CL

> 本论文介绍了一种名为Integral Transformer的新自我注意力机制，该机制在减少注意力噪声的同时保留有用的特殊标记信息，在多种语言基准测试中表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法如Cog Attention和差分Transformer通过引入负注意力分数来减少无关紧要的标记（如特殊标记和标点符号）对注意力的影响，但可能存在丢弃有用信息的风险。因此，提出了Integral Transformer来减少注意力噪声，同时保留有用信息。

**Method:** 本论文提出了Integral Transformer，这是一种新的自我注意力机制，通过整合来自logit分布的信号来减少注意力噪声。这种方法在减少噪声的同时保留了对模型性能至关重要的特殊标记的贡献。

**Result:** 实验表明，相较于传统的、Cog的和差分注意力变体，本论文的方法在知识和推理语言基准测试上表现更佳。

**Conclusion:** 分析表明，在Transformer的较低层使用传统的自我注意力可以提高性能，而Integral Transformer在高层有效地平衡了注意力分布并减少了排名崩溃。

**Abstract:** Softmax self-attention often assigns disproportionate weight to semantically
uninformative tokens such as special tokens and punctuation, a phenomenon known
as attention noise. While recent methods like Cog Attention and the
Differential Transformer have addressed this by introducing negative attention
scores, they risk discarding useful information. In this paper, we propose the
Integral Transformer, a novel self-attention mechanism that denoises attention
by integrating signals sampled from the logit distribution. Our approach
mitigates noise while preserving the contributions of special tokens critical
for model performance. Extensive experiments demonstrate that our model
outperforms vanilla, Cog, and Differential attention variants on
well-established knowledge and reasoning language benchmarks. Moreover, our
analysis reveals that employing vanilla self-attention in the lower Transformer
layers enhances performance and that the Integral Transformer effectively
balances attention distributions and reduces rank collapse in upper layers.

</details>


### [7] [Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning](https://arxiv.org/abs/2508.18395)
*Jeong-seok Oh,Jay-yoon Lee*

Main category: cs.CL

> 本文提出了LSC方法，通过可学习的嵌入选择最语义一致的回答，适用于短答案和长答案，同时保持了低计算开销和良好校准的信心估计。

<details>
  <summary>Details</summary>

**Motivation:** 概率解码在大语言模型（LLMs）中往往会产生不一致的回答，特别是在复杂或长形式的问题上。现有的方法例如SC, USC, WUCS对于某种形式的回答存在不足，因此本文提出了LSC来解决这些问题。

**Method:** 提出了一种名为潜在自我一致性（Latent Self-Consistency，LSC）的方法，该方法通过可学习的标记嵌入来选择最语义一致的回答。通过轻量级的生成摘要标记，推理时间仅增加不到1%，且无需更改模型架构。

**Result:** 在6个短格式和5个长格式推理基准测试（例如MATH、MMLU、TruthfulQA）上，LSC在所有短格式和长格式基准上平均都超过了SC、USC和WUCS，同时保持了可忽略的计算开销。此外，LSC还提供了良好校准的信心估计，在两种答案格式下均保持了较低的预期校准误差。

**Conclusion:** LSC作为一种实用的共识选择方法，能够在各种答案格式下可靠地工作，同时提供了算法定校的信心估计。

**Abstract:** Probabilistic decoding in Large Language Models (LLMs) often yields
inconsistent outputs, particularly on complex or long-form questions.
Self-Consistency (SC) mitigates this for short-form QA by majority voting over
exact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram
Consistency Score (WUCS) extend to long-form responses but lose accuracy on
short-form benchmarks.
  We introduce Latent Self-Consistency (LSC), which selects the most
semantically consistent response using learnable token embeddings. A
lightweight forward generation of summary tokens increases inference time by
less than 1% and requires no changes to the model architecture.
  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU,
TruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form
ones on average, while maintaining negligible computational overhead. These
results position LSC as a practical consistency-selection method that works
reliably across answer formats. Additionally, LSC provides well-calibrated
confidence estimates, maintaining low Expected Calibration Error across both
answer formats.

</details>


### [8] [Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering](https://arxiv.org/abs/2508.18407)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Michal Spiegel,Josef Kuchař*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** A majority of recent work in AI assesses models' generalization capabilities
through the lens of performance on out-of-distribution (OOD) datasets. Despite
their practicality, such evaluations build upon a strong assumption: that OOD
evaluations can capture and reflect upon possible failures in a real-world
deployment.
  In this work, we challenge this assumption and confront the results obtained
from OOD evaluations with a set of specific failure modes documented in
existing question-answering (QA) models, referred to as a reliance on spurious
features or prediction shortcuts.
  We find that different datasets used for OOD evaluations in QA provide an
estimate of models' robustness to shortcuts that have a vastly different
quality, some largely under-performing even a simple, in-distribution
evaluation. We partially attribute this to the observation that spurious
shortcuts are shared across ID+OOD datasets, but also find cases where a
dataset's quality for training and evaluation is largely disconnected. Our work
underlines limitations of commonly-used OOD-based evaluations of
generalization, and provides methodology and recommendations for evaluating
generalization within and beyond QA more robustly.

</details>


### [9] [How Reliable are LLMs for Reasoning on the Re-ranking task?](https://arxiv.org/abs/2508.18444)
*Nafis Tanveer Islam,Zhiming Zhao*

Main category: cs.CL

> 本文探讨了不同的训练方法如何影响LLMs在重新排名任务中的语义理解，并研究了这些模型是否能生成更具有信息量的文字推理来应对透明度挑战和数据限制，通过使用环境和地球科学领域的小规模数据集来重新排名内容，并分析可解释性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管LLMs在语义理解能力上有所提高，并且表现出对人类价值观的更强意识和对齐，但这一改进是以牺牲透明度为代价的。为了理解重新排名背后的推理并为用户提供解释性说明，深入理解LLMs的内在工作原理是必要的。此外，在用户参与度低、排名数据不足的新开发系统中，准确进行重新排名依然是一个挑战。

**Method:** 本文通过使用来自环境和地球科学领域的相对较小的排名数据集来重新排名检索到的内容，分析了不同的训练方法如何影响LLMs在重新排名任务中的语义理解，并探讨这些模型是否能够生成更具有信息量的文字推理以克服有限训练数据的透明度挑战。此外，本文还分析了可解释的信息，以了解重新排名是否可以通过可解释性来进行推理。

**Result:** 本文发现某些训练方法相较于其他的训练方法展现出更好的可解释性，这意味着并不是所有的训练方法都能使模型学习到准确的语义理解，而是获取到了一些抽象的知识以优化评估，从而引发了关于LLMs的真实可靠性的疑问。

**Conclusion:** 通过研究不同的训练方法对LLMs在重新排名任务中的语义理解的影响，本文揭示了某些训练方法的可解释性优于其他方法，但是这些模型是否能生成更多的文字推理来解决透明度挑战和有限训练数据的限制仍然需要进一步研究。

**Abstract:** With the improving semantic understanding capability of Large Language Models
(LLMs), they exhibit a greater awareness and alignment with human values, but
this comes at the cost of transparency. Although promising results are achieved
via experimental analysis, an in-depth understanding of the LLM's internal
workings is unavoidable to comprehend the reasoning behind the re-ranking,
which provides end users with an explanation that enables them to make an
informed decision. Moreover, in newly developed systems with limited user
engagement and insufficient ranking data, accurately re-ranking content remains
a significant challenge. While various training methods affect the training of
LLMs and generate inference, our analysis has found that some training methods
exhibit better explainability than others, implying that an accurate semantic
understanding has not been learned through all training methods; instead,
abstract knowledge has been gained to optimize evaluation, which raises
questions about the true reliability of LLMs. Therefore, in this work, we
analyze how different training methods affect the semantic understanding of the
re-ranking task in LLMs and investigate whether these models can generate more
informed textual reasoning to overcome the challenges of transparency or LLMs
and limited training data. To analyze the LLMs for re-ranking tasks, we utilize
a relatively small ranking dataset from the environment and the Earth science
domain to re-rank retrieved content. Furthermore, we also analyze the
explainable information to see if the re-ranking can be reasoned using
explainability.

</details>


### [10] [Integrating gender inclusivity into large language models via instruction tuning](https://arxiv.org/abs/2508.18466)
*Alina Wróblewska,Bartosz Żuk*

Main category: cs.CL

> 本研究通过调整LLMs使波兰语中的性别偏见得到缓解，从而实现性别平衡。研究展示了如何使用数据集IPIS对LLMs进行微调，以实现波兰语性别包容性语言生成。

<details>
  <summary>Details</summary>

**Motivation:** 由于波兰语中历史和政治惯例使得阳性形式在指代男性、女性和混合性别群体时占主导地位，因此经过训练的大语言模型（LLMs）会继承并强化这种阳性偏见，生成性别失衡的输出。此研究旨在缓解波兰语语言生成中的性别偏见问题。

**Method:** 该研究通过使用IPIS数据集对大语言模型（LLMs）进行微调来解决波兰语中的性别偏见问题。IPIS数据集包含人工编写的波兰语性别包容性校对内容和波兰语到英语的翻译指令。研究设计了一个带有明确波兰语性别包容性指南的系统提示。

**Result:** 他们对多语言LLMs（Llama-8B, Mistral-7B 和 Mistral-Nemo）和特定于波兰语的LLMs（Bielik和PLLuM）进行了IPIS调整。

**Conclusion:** 提出的方法旨在将性别包容性作为这些模型的基本特征，为缓解波兰语语言生成中的性别偏见提供了系统性解决方案。

**Abstract:** Imagine a language with masculine, feminine, and neuter grammatical genders,
yet, due to historical and political conventions, masculine forms are
predominantly used to refer to men, women and mixed-gender groups. This is the
reality of contemporary Polish. A social consequence of this unfair linguistic
system is that large language models (LLMs) trained on Polish texts inherit and
reinforce this masculine bias, generating gender-imbalanced outputs. This study
addresses this issue by tuning LLMs using the IPIS dataset, a collection of
human-crafted gender-inclusive proofreading in Polish and Polish-to-English
translation instructions. Grounded in a theoretical linguistic framework, we
design a system prompt with explicit gender-inclusive guidelines for Polish. In
our experiments, we IPIS-tune multilingual LLMs (Llama-8B, Mistral-7B and
Mistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM). Our approach aims to
integrate gender inclusivity as an inherent feature of these models, offering a
systematic solution to mitigate gender bias in Polish language generation.

</details>


### [11] [Principled Detection of Hallucinations in Large Language Models via Multiple Testing](https://arxiv.org/abs/2508.18473)
*Jiawei Li,Akshayaa Magesh,Venugopal V. Veeravalli*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** While Large Language Models (LLMs) have emerged as powerful foundational
models to solve a variety of tasks, they have also been shown to be prone to
hallucinations, i.e., generating responses that sound confident but are
actually incorrect or even nonsensical. In this work, we formulate the problem
of detecting hallucinations as a hypothesis testing problem and draw parallels
to the problem of out-of-distribution detection in machine learning models. We
propose a multiple-testing-inspired method to solve the hallucination detection
problem, and provide extensive experimental results to validate the robustness
of our approach against state-of-the-art methods.

</details>


### [12] [COMET-poly: Machine Translation Metric Grounded in Other Candidates](https://arxiv.org/abs/2508.18549)
*Maike Züfle,Vilém Zouhar,Tu Anh Dinh,Felipe Maia Polo,Jan Niehues,Mrinmaya Sachan*

Main category: cs.CL

> Two new machine translation evaluation metrics, COMET-polycand and COMET-polyic, that use additional translation information have been developed and shown to significantly improve performance over existing metrics.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind developing these metrics is that while humans assess translations in the broader context of multiple alternatives, automated metrics typically consider only the source sentence and a single translation. This could be affecting their performance.

**Method:** The paper proposes two automated metrics, COMET-polycand and COMET-polyic, which incorporate additional information for evaluating machine translation quality. COMET-polycand uses alternative translations of the same source sentence, while COMET-polyic utilizes translations of similar source texts and their human-labeled quality scores.

**Result:** The study found that adding a single additional translation in COMET-polycand improves the evaluation (Kendall's tau-b correlation improved from 0.079 to 0.118), with further gains from more translations. Incorporating retrieved examples in COMET-polyic also showed improved evaluation results (Kendall's tau-b correlation improved from 0.079 to 0.116).

**Conclusion:** The paper concludes that introducing additional information beyond a single translation can significantly improve the performance of machine translation metrics, likely because it more closely mimics the human evaluation process.

**Abstract:** Automated metrics for machine translation attempt to replicate human
judgment. Unlike humans, who often assess a translation in the context of
multiple alternatives, these metrics typically consider only the source
sentence and a single translation. This discrepancy in the evaluation setup may
negatively impact the performance of automated metrics. We propose two
automated metrics that incorporate additional information beyond the single
translation. COMET-polycand uses alternative translations of the same source
sentence to compare and contrast with the translation at hand, thereby
providing a more informed assessment of its quality. COMET-polyic, inspired by
retrieval-based in-context learning, takes in translations of similar source
texts along with their human-labeled quality scores to guide the evaluation. We
find that including a single additional translation in COMET-polycand improves
the segment-level metric performance (0.079 to 0.118 Kendall's tau-b
correlation), with further gains when more translations are added.
Incorporating retrieved examples in COMET-polyic yields similar improvements
(0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly.

</details>


### [13] [The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation](https://arxiv.org/abs/2508.18569)
*Girish A. Koushik,Fatemeh Nazarieh,Katherine Birch,Shenbin Qian,Diptesh Kanojia*

Main category: cs.CL

> 本文提出一种自评估框架用于生成视觉隐喻，展示了无需训练与训练基础方法的有效性，证明了结构提示和轻量级强化学习在适度计算下可有效完成隐喻对齐。

<details>
  <summary>Details</summary>

**Motivation:** 文本的主要动机是解决隐喻生成任务中的语言理解挑战，该任务需要绑定源概念与目标概念，同时保持意义和视觉一致性。

**Method:** 本文提出了一种自评估的视觉隐喻生成框架，专注于隐喻的对齐。该框架包含两个新的方法：一个无需训练的流程，显式地将提示分解为源-目标-意义(S-T-M)映射来合成图像；另一个则是基于训练的方法，利用作者提出的自我评估奖励模式来改进对齐，无需大规模重训练。

**Result:** 在保留的测试集上，无需训练的方法在分解、CLIP和意义对齐（MA）分数上超越了强大的基线模型（GPT-4o, Imagen）。用户研究还表明，参与者总体上更偏好GPT-4o，但本文提出的无需训练方法在开源方法中表现最好，并在抽象隐喻上略超Imagen。

**Conclusion:** 本文表明，结构提示和轻量级强化学习在适度计算下可有效完成隐喻对齐，剩余的差距似乎是由于审美和采样所引起的。

**Abstract:** Visual metaphor generation is a challenging task that aims to generate an
image given an input text metaphor. Inherently, it needs language understanding
to bind a source concept with a target concept, in a way that preserves meaning
while ensuring visual coherence. We propose a self-evaluating visual metaphor
generation framework that focuses on metaphor alignment. Our self-evaluation
approach combines existing metrics with our newly proposed metaphor
decomposition score and a meaning alignment (MA) metric. Within this setup, we
explore two novel approaches: a training-free pipeline that explicitly
decomposes prompts into source-target-meaning (S-T-M) mapping for image
synthesis, and a complementary training-based pipeline that improves alignment
using our proposed self-evaluation reward schema, without any large-scale
retraining. On the held-out test set, the training-free approach surpasses
strong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores,
with the training-based approach close behind. We evaluate our framework output
using a user-facing study, and observed that participants preferred GPT-4o
overall, while our training-free pipeline led open-source methods and edged
Imagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or
more abstract metaphors, with closed models excelling on short, concrete cases;
we also observe sensitivity to sampler settings. Overall, structured prompting
and lightweight RL perform metaphor alignment well under modest compute, and
remaining gaps to human preference appear driven by aesthetics and sampling.

</details>


### [14] [What do language models model? Transformers, automata, and the format of thought](https://arxiv.org/abs/2508.18598)
*Colin Klein*

Main category: cs.CL

> 论文通过对比人类语言能力和Transformer架构的计算格式，论证大型语言模型主要是训练数据集的模型，但其作为语言生成工具的潜力不应被低估。

<details>
  <summary>Details</summary>

**Motivation:** 探讨大型语言模型的本质：它们是人类能力的反映，还是训练数据集的模型？

**Method:** 通过分析transformer架构的计算不变性来论证大型语言模型的线性处理格式与人类语言能力的超线性计算格式之间的差异，并结合Liu等人的理论来解释模型如何学习语言技术的机制。

**Result:** 论证了大型语言模型本质上是训练数据集的模型，并提出了模型如何作为语言生成工具的积极解读。

**Conclusion:** 虽然大型语言模型不同于人类语言处理方式，但它们同样学习了生成语言的技术，在给定适当上下文时，能够产生新的语言，这并不完全是贬低性的解读。

**Abstract:** What do large language models actually model? Do they tell us something about
human capacities, or are they models of the corpus we've trained them on? I
give a non-deflationary defence of the latter position. Cognitive science tells
us that linguistic capabilities in humans rely supralinear formats for
computation. The transformer architecture, by contrast, supports at best a
linear formats for processing. This argument will rely primarily on certain
invariants of the computational architecture of transformers. I then suggest a
positive story about what transformers are doing, focusing on Liu et al.
(2022)'s intriguing speculations about shortcut automata. I conclude with why I
don't think this is a terribly deflationary story. Language is not (just) a
means for expressing inner state but also a kind of 'discourse machine' that
lets us make new language given appropriate context. We have learned to use
this technology in one way; LLMs have also learned to use it too, but via very
different means.

</details>


### [15] [A New NMT Model for Translating Clinical Texts from English to Spanish](https://arxiv.org/abs/2508.18607)
*Rumeng Li,Xun Wang,Hong Yu*

Main category: cs.CL

> 本文提出NOOV系统，该系统结合了双语词典和短语查找表，以解决EHR翻译任务中的未知单词和重复单词问题，提高了翻译质量和流畅度。

<details>
  <summary>Details</summary>

**Motivation:** 由于缺乏平行对齐语料库和丰富的未知单词，将电子健康记录(EHR)叙述从英语翻译成西班牙语是一个重要的但具有挑战性的临床任务。

**Method:** NOOV (No OOV)系统结合了从平行语料库中自动学习的双语词典和从大型生物医学知识资源中提取的短语查找表，解决了神经机器翻译中的未知单词问题和单词重复问题，提高了翻译中的短语生成质量。

**Result:** 评估表明，NOOV系统能更好地生成EHR翻译，提升了翻译的准确性和流畅度。

**Conclusion:** NOOV系统在仅使用少量领域内平行对齐语料库进行训练的情况下，提高了EHR翻译的质量，特别是在解决未知单词问题和单词重复问题方面。

**Abstract:** Translating electronic health record (EHR) narratives from English to Spanish
is a clinically important yet challenging task due to the lack of a
parallel-aligned corpus and the abundant unknown words contained. To address
such challenges, we propose \textbf{NOOV} (for No OOV), a new neural machine
translation (NMT) system that requires little in-domain parallel-aligned corpus
for training. NOOV integrates a bilingual lexicon automatically learned from
parallel-aligned corpora and a phrase look-up table extracted from a large
biomedical knowledge resource, to alleviate both the unknown word problem and
the word-repeat challenge in NMT, enhancing better phrase generation of NMT
systems. Evaluation shows that NOOV is able to generate better translation of
EHR with improvement in both accuracy and fluency.

</details>


### [16] [Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models](https://arxiv.org/abs/2508.18609)
*Chenxi Zhou,Pengfei Cao,Jiang Li,Jun Zhao,Kang Liu*

Main category: cs.CL

> This paper investigates the impact of post-training quantization on LLMs by establishing task-stratified scaling laws, providing insights for developing knowledge-aware quantization strategies.

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the gaps in understanding how post-training quantization impacts diverse LLM knowledge capabilities and to establish scaling laws that incorporate crucial PTQ-specific parameters and task-specific sensitivities.

**Method:** This paper develops a unified quantitative framework incorporating model size, effective bit-width, calibration set size, and group size to disentangle LLM knowledge into memorization and utilization capabilities.

**Result:** The central finding reveals that knowledge memorization exhibits markedly greater sensitivity to variations in effective bit-width, calibration set size, and model size compared to the more robust knowledge utilization.

**Conclusion:** The findings offer a fine-grained understanding of PTQ's impact on LLMs and guide the development of strategies for preserving targeted cognitive functions in quantized models.

**Abstract:** Large language models (LLMs) present significant deployment challenges due to
their scale, with post-training quantization (PTQ) emerging as a practical
compression solution. However, a comprehensive understanding of how PTQ
precisely impacts diverse LLM knowledge capabilities remains elusive, and
existing scaling laws for quantized models often overlook crucial PTQ-specific
parameters and task-specific sensitivities. This paper addresses these gaps by
conducting an extensive empirical investigation to establish task-stratified
scaling laws. We disentangle LLM knowledge into memorization and utilization
capabilities and develop a unified quantitative framework that incorporates
model size, effective bit-width, calibration set size, and group size. Our
central finding reveals that knowledge memorization exhibits markedly greater
sensitivity to variations in effective bit-width, calibration set size, and
model size compared to the more robust knowledge utilization. These findings
offer a fine-grained understanding of PTQ's impact and provide guidance for
developing knowledge-aware quantization strategies that can better preserve
targeted cognitive functions.

</details>


### [17] [Thinking Before You Speak: A Proactive Test-time Scaling Approach](https://arxiv.org/abs/2508.18648)
*Cong Li,Wenchang Chai,Hejun Wu,Yan Pan,Pengxu Wei,Liang Lin*

Main category: cs.CL

> 提出了一种名为THYS的方法，以生成洞察力的方式解决了大型语言模型在复杂推理任务中的缺陷问题。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLMs）在处理如数学这样复杂的推理任务时，通常表现出缺陷，这种缺陷被称为人类推理模式与LLMs培训数据中的模式之间的差异造成的。

**Method:** 提出了一种名为Thinking Before You Speak (TBYS)的推理框架，并设计了一条自动收集和过滤上下文示例的管道，用于生成洞察力（insights），这减少了人工标注工作和微调的负担。

**Result:** 实验验证了TBYS在具有挑战性的数学数据集上的有效性。

**Conclusion:** 该项目提供了一种名为TBYS的框架，通过生成洞察力改善了复杂推理任务的效果，并在具有挑战性的数学数据集上验证了其有效性。

**Abstract:** Large Language Models (LLMs) often exhibit deficiencies with complex
reasoning tasks, such as maths, which we attribute to the discrepancy between
human reasoning patterns and those presented in the LLMs' training data. When
dealing with complex problems, humans tend to think carefully before expressing
solutions. However, they often do not articulate their inner thoughts,
including their intentions and chosen methodologies. Consequently, critical
insights essential for bridging reasoning steps may be absent in training data
collected from human sources. To bridge this gap, we proposes inserting
\emph{insight}s between consecutive reasoning steps, which review the status
and initiate the next reasoning steps. Unlike prior prompting strategies that
rely on a single or a workflow of static prompts to facilitate reasoning,
\emph{insight}s are \emph{proactively} generated to guide reasoning processes.
We implement our idea as a reasoning framework, named \emph{Thinking Before You
Speak} (TBYS), and design a pipeline for automatically collecting and filtering
in-context examples for the generation of \emph{insight}s, which alleviates
human labeling efforts and fine-tuning overheads. Experiments on challenging
mathematical datasets verify the effectiveness of TBYS. Project website:
https://gitee.com/jswrt/TBYS

</details>


### [18] [Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models](https://arxiv.org/abs/2508.18651)
*Chenxu Yang,Qingyi Si,Zheng Lin*

Main category: cs.CL

> 提出一种名为Collaborative Decoding (CoDe) 的新方法，通过动态整合使用和不使用外部知识生成的输出概率，解决了大语言模型在事实准确性和表达性之间的权衡问题，并通过全面实验验证了其有效性和通用性。

<details>
  <summary>Details</summary>

**Motivation:** 目前的大语言模型在无缝整合知识的同时难以保持事实准确性和表达性，人类却能自然地拥有这些能力。这次工作的动机是解决这一限制，打破事实准确性和表达性之间的权衡。

**Method:** 提出了一种名为Collaborative Decoding (CoDe) 的新方法，该方法通过动态整合使用和不使用外部知识生成的输出概率，打破了事实准确性和表达性之间的权衡。这种方法根据分布差异和模型信心指导，选择性地激活模型内部参数中的相关且可靠的表达。此外，还引入了一个知识感知的重新排序机制，以防止过度依赖先前的参数知识，并确保适当利用提供的外部信息。

**Result:** 通过全面的实验，他们的即插即用CoDe框架在提高多种大语言模型的事实准确性的同时，不会牺牲表达性，并且在多种评估指标上表现出优越的性能，证实了其有效性和通用性。

**Conclusion:** 研究得出结论，即插即用的CoDe框架在提高大语言模型的事实准确性的同时不会牺牲其表达性，并在各种评估指标上均表现出色，证明了CoDe方法的有效性和广泛适用性。

**Abstract:** Grounding responses in external knowledge represents an effective strategy
for mitigating hallucinations in Large Language Models (LLMs). However, current
LLMs struggle to seamlessly integrate knowledge while simultaneously
maintaining faithfulness (or fidelity) and expressiveness, capabilities that
humans naturally possess. This limitation results in outputs that either lack
support from external knowledge, thereby compromising faithfulness, or appear
overly verbose and unnatural, thus sacrificing expressiveness. In this work, to
break the trade-off between faithfulness and expressiveness, we propose
Collaborative Decoding (CoDe), a novel approach that dynamically integrates
output probabilities generated with and without external knowledge. This
integration is guided by distribution divergence and model confidence, enabling
the selective activation of relevant and reliable expressions from the model's
internal parameters. Furthermore, we introduce a knowledge-aware reranking
mechanism that prevents over-reliance on prior parametric knowledge while
ensuring proper utilization of provided external information. Through
comprehensive experiments, our plug-and-play CoDe framework demonstrates
superior performance in enhancing faithfulness without compromising
expressiveness across diverse LLMs and evaluation metrics, validating both its
effectiveness and generalizability.

</details>


### [19] [Emotion Omni: Enabling Empathetic Speech Response Generation through Large Language Models](https://arxiv.org/abs/2508.18655)
*Haoyu Wang,Guangyan Zhang,Jiale Chen,Jingyu Li,Yuehai Wang,Yiwen Guo*

Main category: cs.CL

> 本文提出了一种名为Emotion Omni的新模型架构，用于生成具有同理心的语音回复，同时构建了一个包含20万情感对话的数据集支持同理心语音助手的开发。

<details>
  <summary>Details</summary>

**Motivation:** 随着语音大语言模型的发展，用户可以直接通过语音与助手进行交互。然而，现有的模型大多只是将回复内容转换成语音，而未能完全理解用户查询中嵌入的丰富情感和副语言提示，这在提升人机交互体验方面显得尤为重要。我们的动机在于寻求一种方法来开发一个能够利用有限数据生成同理心回复的语音大语言模型，而无需大规模训练。

**Method:** 我们提出了一种名为Emotion Omni的新模型架构，用于理解用户语音输入中的情感内容，并生成具有同理心的语音回复。此外，我们基于开源的TTS框架开发了一个数据生成管道，以构建一个包含20万情感对话的数据集，以支持同理心语音助手的构建。

**Result:** 通过提出的新模型架构和数据生成方式，解决了在有限数据和计算资源的情况下开发能够生成同理心回复的语音大语言模型的挑战。

**Conclusion:** 通过构建Emotion Omni模型和20万情感对话数据集，我们证明了在有限数据和计算资源条件下，仍可以开发能够生成同理心回复的语音大语言模型。这为进一步改善人机交互体验提供了重要的技术支持。

**Abstract:** With the development of speech large language models (speech LLMs), users can
now interact directly with assistants via speech. However, most existing models
simply convert the response content into speech without fully understanding the
rich emotional and paralinguistic cues embedded in the user's query. In many
cases, the same sentence can have different meanings depending on the emotional
expression. Furthermore, emotional understanding is essential for improving
user experience in human-machine interaction. Currently, most speech LLMs with
empathetic capabilities are trained on massive datasets. This approach requires
vast amounts of data and significant computational resources. Therefore, a key
challenge lies in how to develop a speech LLM capable of generating empathetic
responses with limited data and without the need for large-scale training. To
address this challenge, we propose Emotion Omni, a novel model architecture
designed to understand the emotional content of user speech input and generate
empathetic speech responses. Additionally, we developed a data generation
pipeline based on an open-source TTS framework to construct a 200k emotional
dialogue dataset, which supports the construction of an empathetic speech
assistant. The demos are available at https://w311411.github.io/omni_demo/

</details>


### [20] [Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum](https://arxiv.org/abs/2508.18673)
*Xinglong Yang,Quan Feng,Zhongying Pan,Xiang Chen,Yu Tian,Wentong Li,Shuofei Qiao,Yuxia Geng,Xingyu Zhao,Sheng-Jun Huang*

Main category: cs.CL

> The paper proposes a novel framework inspired by the pedagogical principle of 'tailored teaching with balanced difficulty' to improve the effectiveness of Multimodal Chain-of-Thought (MCoT) prompting and develop a difficulty-balanced sampling strategy that ensures diversity across both dimensions.

<details>
  <summary>Details</summary>

**Motivation:** The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often limited by the use of randomly or manually selected examples. These examples fail to account for both model-specific knowledge distributions and the intrinsic complexity of the tasks, resulting in suboptimal and unstable model performance.

**Method:** Structure

**Result:** {
  "tldr": "本文提出了一种名为"定制教学与平衡难度"的框架来改进多模态链式思考提示效果，并通过结合模型感知难度和样本内在复杂度信号实现了采样策略的优化，大大减少了由随机抽样引起的表现不一致问题。",
  "motivation": "多模态链式思考提示的效果受限于随机或手动选择的示例，这些示例没有考虑到模型特定的知识分布和任务内在的复杂性，导致了模型表现不佳且不稳定。为此，本文提出了一种新的框架来解决这一问题。",
  "method": "本文采用了"定制教学与平衡难度"的框架，并将提示选择重新定义为提示课程设计问题。该框架利用了两个互补的信号：模型感知难度和样本内在复杂度，以确保选择的提示示例在两个维度上都是多样化的。",
  "result": "实验结果表明，与随机抽样相比，本文的方法能够在五个挑战性基准上以及多种流行多模态大语言模型上取得显著和一致的改进，并大幅度减少由随机抽样引起的表现不一致。",
  "conclusion": "实验证明了本文方法的有效性，通过联合分析模型感知难度和样本内在复杂度，我们提出的采样策略可以提供一种原则化和稳健的方式来提升多模态推理能力。",
  "tool_calls": []
}

**Conclusion:** Experiments demonstrate that the proposed method yields substantial and consistent improvements and greatly reduces performance discrepancies caused by random sampling, providing a principled and robust approach for enhancing multimodal reasoning.

**Abstract:** The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often
limited by the use of randomly or manually selected examples. These examples
fail to account for both model-specific knowledge distributions and the
intrinsic complexity of the tasks, resulting in suboptimal and unstable model
performance. To address this, we propose a novel framework inspired by the
pedagogical principle of "tailored teaching with balanced difficulty". We
reframe prompt selection as a prompt curriculum design problem: constructing a
well ordered set of training examples that align with the model's current
capabilities. Our approach integrates two complementary signals: (1)
model-perceived difficulty, quantified through prediction disagreement in an
active learning setup, capturing what the model itself finds challenging; and
(2) intrinsic sample complexity, which measures the inherent difficulty of each
question-image pair independently of any model. By jointly analyzing these
signals, we develop a difficulty-balanced sampling strategy that ensures the
selected prompt examples are diverse across both dimensions. Extensive
experiments conducted on five challenging benchmarks and multiple popular
Multimodal Large Language Models (MLLMs) demonstrate that our method yields
substantial and consistent improvements and greatly reduces performance
discrepancies caused by random sampling, providing a principled and robust
approach for enhancing multimodal reasoning.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [21] [Towards Training-Free Underwater 3D Object Detection from Sonar Point Clouds: A Comparison of Traditional and Deep Learning Approaches](https://arxiv.org/abs/2508.18293)
*M. Salman Shaukat,Yannik Käckenmeister,Sebastian Bader,Thomas Kirste*

Main category: cs.CV

> 本文探讨了在缺乏真实训练数据的情况下，如何实现可靠的水下三维目标检测。通过结合物理基础的声纳模拟生成合成训练数据和基于模型的模板匹配系统的方法，实现了在真实声纳数据上检测的高精度，挑战了关于数据依赖深度学习的传统观点，并为缺乏数据的水下环境中的自主导航、海洋考古和离岸基础设施监测开辟了新途径。

<details>
  <summary>Details</summary>

**Motivation:** 本文面临的动机是解决水下环境中获取足够标注声纳数据的难题，尤其是依赖深度学习技术的背景下，希望通过新方法实现无需真实训练数据的检测，填补该领域的空白。

**Method:** 本文采用两种方法：使用物理建模生成的合成训练数据来训练深度网络，以及一种利用目标物几何先验结构的稳健模型匹配系统。

**Result:** 实验结果显示，在使用合成数据训练下，神经网络在模拟场景中的检测精度达到了98% mAP，但应用到真实声纳数据时下降到40% mAP。相比之下，模板匹配方法在不需要任何训练的情况下，在真实数据中保持了83% mAP的检测精度。

**Conclusion:** 本文的结论是，即使面对数据稀缺的挑战，也可以通过模板匹配的方式实现高精度的水下三维目标检测，这对于深度学习依赖大量训练数据的方法提出了挑战，并强调了在缺乏数据的环境中的潜在应用价值。

**Abstract:** Underwater 3D object detection remains one of the most challenging frontiers
in computer vision, where traditional approaches struggle with the harsh
acoustic environment and scarcity of training data. While deep learning has
revolutionized terrestrial 3D detection, its application underwater faces a
critical bottleneck: obtaining sufficient annotated sonar data is prohibitively
expensive and logistically complex, often requiring specialized vessels, expert
surveyors, and favorable weather conditions. This work addresses a fundamental
question: Can we achieve reliable underwater 3D object detection without
real-world training data? We tackle this challenge by developing and comparing
two paradigms for training-free detection of artificial structures in multibeam
echo-sounder point clouds. Our dual approach combines a physics-based sonar
simulation pipeline that generates synthetic training data for state-of-the-art
neural networks, with a robust model-based template matching system that
leverages geometric priors of target objects. Evaluation on real bathymetry
surveys from the Baltic Sea reveals surprising insights: while neural networks
trained on synthetic data achieve 98% mean Average Precision (mAP) on simulated
scenes, they drop to 40% mAP on real sonar data due to domain shift.
Conversely, our template matching approach maintains 83% mAP on real data
without requiring any training, demonstrating remarkable robustness to acoustic
noise and environmental variations. Our findings challenge conventional wisdom
about data-hungry deep learning in underwater domains and establish the first
large-scale benchmark for training-free underwater 3D detection. This work
opens new possibilities for autonomous underwater vehicle navigation, marine
archaeology, and offshore infrastructure monitoring in data-scarce environments
where traditional machine learning approaches fail.

</details>


### [22] [MobileDenseAttn:A Dual-Stream Architecture for Accurate and Interpretable Brain Tumor Detection](https://arxiv.org/abs/2508.18294)
*Shudipta Banik,Muna Das,Trapa Banik,Md. Ehsanul Haque*

Main category: cs.CV

> 本研究介绍了一个名为MobileDenseAttn的模型，该模型结合了MobileNetV2和DenseNet201的优点，展示了在脑肿瘤检测中的高准确度和计算效率。研究证明，该模型相比于基线模型VGG19在准确度上有3.67%的提升，训练时间减少了39.3%。

<details>
  <summary>Details</summary>

**Motivation:** 手动分析脑肿瘤检测费时且容易出错，现有的方法存在泛化能力差、计算效率低、不透明且缺乏解释性的问题。因此，需要开发一种能够克服这些问题的新型模型，提高脑肿瘤检测的准确性和工作效率。

**Method:** MobileDenseAttn模型结合了MobileNetV2和DenseNet201的双流结构，用于逐步提高特征表示、计算效率和通过GradCAM提供的视觉解释。模型基于特征级融合，并在增强后的数据集上训练，数据集包含6,020个MRI扫描，涵盖了胶质瘤、脑膜瘤、垂体瘤和正常样本。

**Result:** 在严格的5折交叉验证协议下，MobileDenseAttn模型展示出了99.75%的训练准确率，98.35%的测试准确率和一个稳定的F1分数0.9835。此外，GradCAM热图清晰地显示出肿瘤受影响的部位，提高了模型的可解释性。

**Conclusion:** MobileDenseAttn模型被定位为一个高效、高性能且具有高度解释性的模型，其在实际临床应用中有很大的潜力，可以帮助识别现实生活中的脑肿瘤。

**Abstract:** The detection of brain tumor in MRI is an important aspect of ensuring timely
diagnostics and treatment; however, manual analysis is commonly long and
error-prone. Current approaches are not universal because they have limited
generalization to heterogeneous tumors, are computationally inefficient, are
not interpretable, and lack transparency, thus limiting trustworthiness. To
overcome these issues, we introduce MobileDenseAttn, a fusion model of dual
streams of MobileNetV2 and DenseNet201 that can help gradually improve the
feature representation scale, computing efficiency, and visual explanations via
GradCAM. Our model uses feature level fusion and is trained on an augmented
dataset of 6,020 MRI scans representing glioma, meningioma, pituitary tumors,
and normal samples. Measured under strict 5-fold cross-validation protocols,
MobileDenseAttn provides a training accuracy of 99.75%, a testing accuracy of
98.35%, and a stable F1 score of 0.9835 (95% CI: 0.9743 to 0.9920). The
extensive validation shows the stability of the model, and the comparative
analysis proves that it is a great advancement over the baseline models (VGG19,
DenseNet201, MobileNetV2) with a +3.67% accuracy increase and a 39.3% decrease
in training time compared to VGG19. The GradCAM heatmaps clearly show
tumor-affected areas, offering clinically significant localization and
improving interpretability. These findings position MobileDenseAttn as an
efficient, high performance, interpretable model with a high probability of
becoming a clinically practical tool in identifying brain tumors in the real
world.

</details>


### [23] [Can VLMs Recall Factual Associations From Visual References?](https://arxiv.org/abs/2508.18297)
*Dhananjay Ashok,Ashutosh Chaubey,Hirona J. Arai,Jonathan May,Jesse Thomason*

Main category: cs.CV

> 研究揭示了视觉语言模型在处理视觉信息时的能力缺陷，并开发了探测器来识别模型性能不佳的情况，这有助于提升模型的预测精度和减少错误的风险。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于揭示VLMs在处理视觉信息时面临的挑战，特别是在图像表示与内部知识关联方面的问题。

**Method:** 通过受控实验，研究人员识别到了视觉语言模型（VLMs）在多模态-grounding方面的系统性缺陷。实验表明VLMs在依赖视觉参考时，其事实知识的召回能力显著下降。

**Result:** 研究发现，当VLMs依靠图像来识别实体时，它们的事实知识召回能力会下降到原来的一半，并且这种情况下的内部状态表现出独特模式。对这些内部状态进行探测可以以超过92%的准确率识别系统不可靠的情况。

**Conclusion:** 纠正这一系统性且可检测的缺陷对语言-grounding研究至关重要，作者提供了后续研究的方向。

**Abstract:** Through a controlled study, we identify a systematic deficiency in the
multimodal grounding of Vision Language Models (VLMs). While VLMs can recall
factual associations when provided a textual reference to an entity; their
ability to do so is significantly diminished when the reference is visual
instead. Forcing VLMs to rely on image representations of an entity halves
their ability to recall factual knowledge, suggesting that VLMs struggle to
link their internal knowledge of an entity with its image representation. We
show that such linking failures are correlated with the expression of distinct
patterns in model internal states, and that probes on these internal states
achieve over 92% accuracy at flagging cases where the VLM response is
unreliable. These probes can be applied, without retraining, to identify when a
VLM will fail to correctly answer a question that requires an understanding of
multimodal input. When used to facilitate selective prediction on a visual
question answering task, the probes increase coverage by 7.87% (absolute) while
also reducing the risk of error by 0.9% (absolute). Addressing the systematic,
detectable deficiency is an important avenue in language grounding, and we
provide informed recommendations for future directions.

</details>


### [24] [SERES: Semantic-aware neural reconstruction from sparse views](https://arxiv.org/abs/2508.18314)
*Bo Xu,Yuhu Guo,Yuchao Wang,Wenting Wang,Yeung Yam,Charlie C. L. Wang,Xinyi Le*

Main category: cs.CV

> This paper introduces semantic-aware neural reconstruction to create precise 3D models from few images, enhancing accuracy with semantic information and novel regularization, significantly outperforming baselines.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the reconstruction of 3D high-fidelity models from sparse images by addressing common issues such as radiance and shape ambiguities.

**Method:** We propose a semantic-aware neural reconstruction method to generate 3D high-fidelity models from sparse images. We address the issue of severe radiance ambiguity by enriching neural implicit representations with patch-based semantic logits. Additionally, a novel regularization based on geometric primitive masks is used to reduce shape ambiguity.

**Result:** The performance of our method is validated through experimental evaluation. Our approach reduces the average chamfer distances on the DTU dataset by 44% for SparseNeuS and 20% for VolRecon. When used as a plugin, the average error is reduced by 69% for NeuS and 68% for Neuralangelo.

**Conclusion:** The novel method proposed shows significant improvements in 3D model reconstruction accuracy when compared to existing baselines, offering an effective solution for generating high-fidelity 3D models from sparse images.

**Abstract:** We propose a semantic-aware neural reconstruction method to generate 3D
high-fidelity models from sparse images. To tackle the challenge of severe
radiance ambiguity caused by mismatched features in sparse input, we enrich
neural implicit representations by adding patch-based semantic logits that are
optimized together with the signed distance field and the radiance field. A
novel regularization based on the geometric primitive masks is introduced to
mitigate shape ambiguity. The performance of our approach has been verified in
experimental evaluation. The average chamfer distances of our reconstruction on
the DTU dataset can be reduced by 44% for SparseNeuS and 20% for VolRecon. When
working as a plugin for those dense reconstruction baselines such as NeuS and
Neuralangelo, the average error on the DTU dataset can be reduced by 69% and
68% respectively.

</details>


### [25] [Automated Landfill Detection Using Deep Learning: A Comparative Study of Lightweight and Custom Architectures with the AerialWaste Dataset](https://arxiv.org/abs/2508.18315)
*Nowshin Sharmily,Rusab Sarmun,Muhammad E. H. Chowdhury,Mir Hamidul Hussain,Saad Bin Abul Kashem,Molla E Majid,Amith Khandakar*

Main category: cs.CV

> The paper addresses the issue of illegal landfills using deep learning techniques and a newly developed AerialWaste Dataset, achieving high accuracy in binary classification.

<details>
  <summary>Details</summary>

**Motivation:** The motivation stems from the global hazardous threat posed by illegal landfills and the challenge in manually identifying them, making deep learning an effective means to address this issue.

**Method:** The study uses a dataset of 10434 images from the Lombardy region of Italy and leverages lightweight deep learning models like Mobilenetv2, Googlenet, Densenet, and MobileVit to train and validate the dataset. An ensemble model approach is used to improve performance.

**Result:** The ensemble model achieved 92.33% accuracy, 92.67% precision, 92.33% sensitivity, 92.41% F1 score, and 92.71% specificity in binary classification.

**Conclusion:** The research demonstrates that lightweight deep learning models can effectively detect illegal landfills when used in an ensemble model, offering a scalable solution to this significant environmental issue.

**Abstract:** Illegal landfills are posing as a hazardous threat to people all over the
world. Due to the arduous nature of manually identifying the location of
landfill, many landfills go unnoticed by authorities and later cause dangerous
harm to people and environment. Deep learning can play a significant role in
identifying these landfills while saving valuable time, manpower and resources.
Despite being a burning concern, good quality publicly released datasets for
illegal landfill detection are hard to find due to security concerns. However,
AerialWaste Dataset is a large collection of 10434 images of Lombardy region of
Italy. The images are of varying qualities, collected from three different
sources: AGEA Orthophotos, WorldView-3, and Google Earth. The dataset contains
professionally curated, diverse and high-quality images which makes it
particularly suitable for scalable and impactful research. As we trained
several models to compare results, we found complex and heavy models to be
prone to overfitting and memorizing training data instead of learning patterns.
Therefore, we chose lightweight simpler models which could leverage general
features from the dataset. In this study, Mobilenetv2, Googlenet, Densenet,
MobileVit and other lightweight deep learning models were used to train and
validate the dataset as they achieved significant success with less
overfitting. As we saw substantial improvement in the performance using some of
these models, we combined the best performing models and came up with an
ensemble model. With the help of ensemble and fusion technique, binary
classification could be performed on this dataset with 92.33% accuracy, 92.67%
precision, 92.33% sensitivity, 92.41% F1 score and 92.71% specificity.

</details>


### [26] [Structures Meet Semantics: Multimodal Fusion via Graph Contrastive Learning](https://arxiv.org/abs/2508.18322)
*Jiangfeng Sun,Sihao He,Zhonghong Ou,Meina Song*

Main category: cs.CV

> SSU框架提出了一种新的多模态情感分析方法，通过动态构建模态特定图和跨模态语义锁定来提高多模态表示的质量。在CMU-MOSI和CMU-MOSEI基准数据集上的实验验证了SSU框架在减少计算开销的同时达到最优性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的多模态融合方法常常忽略了模态特定的结构依赖关系和语义不一致，限制了它们的质量、解释性和鲁棒性。为了解决这些问题，研究人员提出了SSU框架。

**Method:** SSU框架通过利用语言语法为文本构建模态特定图，并使用轻量级文本引导的注意力机制为声学和视觉模态构建，以捕捉详细的模内关系和语义交互。此外，SSU还引入了一个语义锚点，该锚点由全局文本语义派生出来，作为跨模态对齐中心，有效地调和了异质语义空间。此外，SSU开发了一个多视角对比学习目标，促进跨模态和模内视角的辨别性、语义一致性和结构连贯性。

**Result:** SSU在CMU-MOSI和CMU-MOSEI两个基准数据集上取得了优于现有方法的结果，并且在减少计算开销的同时保持了高解释性和捕捉细微情感模式的能力。

**Conclusion:** 研究表明，SSU框架通过综合考虑模态特定的结构信息和跨模态语义对齐，可以提高多模态情感分析的质量和解释性，并且在减少计算开销的同时达到最优性能。

**Abstract:** Multimodal sentiment analysis (MSA) aims to infer emotional states by
effectively integrating textual, acoustic, and visual modalities. Despite
notable progress, existing multimodal fusion methods often neglect
modality-specific structural dependencies and semantic misalignment, limiting
their quality, interpretability, and robustness. To address these challenges,
we propose a novel framework called the Structural-Semantic Unifier (SSU),
which systematically integrates modality-specific structural information and
cross-modal semantic grounding for enhanced multimodal representations.
Specifically, SSU dynamically constructs modality-specific graphs by leveraging
linguistic syntax for text and a lightweight, text-guided attention mechanism
for acoustic and visual modalities, thus capturing detailed intra-modal
relationships and semantic interactions. We further introduce a semantic
anchor, derived from global textual semantics, that serves as a cross-modal
alignment hub, effectively harmonizing heterogeneous semantic spaces across
modalities. Additionally, we develop a multiview contrastive learning objective
that promotes discriminability, semantic consistency, and structural coherence
across intra- and inter-modal views. Extensive evaluations on two widely used
benchmark datasets, CMU-MOSI and CMU-MOSEI, demonstrate that SSU consistently
achieves state-of-the-art performance while significantly reducing
computational overhead compared to prior methods. Comprehensive qualitative
analyses further validate SSU's interpretability and its ability to capture
nuanced emotional patterns through semantically grounded interactions.

</details>


### [27] [FastAvatar: Instant 3D Gaussian Splatting for Faces from Single Unconstrained Poses](https://arxiv.org/abs/2508.18389)
*Hao Liang,Zhixuan Ge,Ashish Tiwari,Soumendu Majee,G. M. Dilshan Godaliyadda,Ashok Veeraraghavan,Guha Balakrishnan*

Main category: cs.CV

> FastAvatar是一个能够从任意角度的单张人脸图像在近实时（<10ms）生成3D高斯散射模型的高效框架。与现有方法相比，它在重建质量和速度上都有显著提升，并支持实时身份插值和属性编辑功能，适用于消费级和交互式系统中的高质量3D头像应用。

<details>
  <summary>Details</summary>

**Motivation:** 为了开发一种快速且高质量的3D人脸重建方法来满足消费者和交互式系统的需求，尤其是在身份插值和属性编辑方面的实时性能。

**Method:** FastAvatar利用创新的编解码器神经网络设计，首先从多视角面部捕捉的数据集中构建3DGS模板模型；其次将输入的人脸图像编码为身份特定且不受姿势影响的潜在嵌入，并解码该嵌入以预测模板3DGS模型中每个高斯的结构和外观参数的残差。

**Result:** FastAvatar显著提高了现有3DGS方法的重建质量，并且速度比现有方法快1000倍。它还支持实时身份插值和属性编辑，这是现有方法无法实现的。

**Conclusion:** 通过结合卓越的重建质量和速度，FastAvatar扩大了3DGS在消费者和交互式系统中的应用范围，提高了该技术的实用性和功能。

**Abstract:** We present FastAvatar, a pose-invariant, feed-forward framework that can
generate a 3D Gaussian Splatting (3DGS) model from a single face image from an
arbitrary pose in near-instant time (<10ms). FastAvatar uses a novel
encoder-decoder neural network design to achieve both fast fitting and identity
preservation regardless of input pose. First, FastAvatar constructs a 3DGS face
``template'' model from a training dataset of faces with multi-view captures.
Second, FastAvatar encodes the input face image into an identity-specific and
pose-invariant latent embedding, and decodes this embedding to predict
residuals to the structural and appearance parameters of each Gaussian in the
template 3DGS model. By only inferring residuals in a feed-forward fashion,
model inference is fast and robust. FastAvatar significantly outperforms
existing feed-forward face 3DGS methods (e.g., GAGAvatar) in reconstruction
quality, and runs 1000x faster than per-face optimization methods (e.g.,
FlashAvatar, GaussianAvatars and GASP). In addition, FastAvatar's novel latent
space design supports real-time identity interpolation and attribute editing
which is not possible with any existing feed-forward 3DGS face generation
framework. FastAvatar's combination of excellent reconstruction quality and
speed expands the scope of 3DGS for photorealistic avatar applications in
consumer and interactive systems.

</details>


### [28] [Securing Face and Fingerprint Templates in Humanitarian Biometric Systems](https://arxiv.org/abs/2508.18415)
*Giuseppe Stragapede,Sam Merrick,Vedrana Krivokuća Hahn,Justin Sukaitis,Vincent Graf Narbel*

Main category: cs.CV

> A mobile biometric system using PolyProtect for face and fingerprint embeddings is analyzed, showing promise for secure operations in humanitarian scenarios.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to enhance the efficiency of operations in humanitarian and emergency settings while protecting the privacy and safety of biometric data subjects.

**Method:** Content presents a mobile biometric system for humanitarian scenarios with a focus on the use of PolyProtect for face embeddings and fingerprint biometrics, utilizing a biometric template protection scheme to address privacy and security risks.

**Result:** The research evaluates PolyProtect with promising results in verification and identification accuracy, irreversibility, and unlinkability when applied to facial and fingerprint biometrics.

**Conclusion:** The study concludes with positive outcomes for the use of PolyProtect in humanitarian contexts, indicating its potential application in ensuring secure and efficient operations.

**Abstract:** In humanitarian and emergency scenarios, the use of biometrics can
dramatically improve the efficiency of operations, but it poses risks for the
data subjects, which are exacerbated in contexts of vulnerability. To address
this, we present a mobile biometric system implementing a biometric template
protection (BTP) scheme suitable for these scenarios. After rigorously
formulating the functional, operational, and security and privacy requirements
of these contexts, we perform a broad comparative analysis of the BTP
landscape. PolyProtect, a method designed to operate on neural network face
embeddings, is identified as the most suitable method due to its effectiveness,
modularity, and lightweight computational burden. We evaluate PolyProtect in
terms of verification and identification accuracy, irreversibility, and
unlinkability, when this BTP method is applied to face embeddings extracted
using EdgeFace, a novel state-of-the-art efficient feature extractor, on a
real-world face dataset from a humanitarian field project in Ethiopia.
Moreover, as PolyProtect promises to be modality-independent, we extend its
evaluation to fingerprints. To the best of our knowledge, this is the first
time that PolyProtect has been evaluated for the identification scenario and
for fingerprint biometrics. Our experimental results are promising, and we plan
to release our code

</details>


### [29] [Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?](https://arxiv.org/abs/2508.18421)
*Fatemeh Ziaeetabar*

Main category: cs.CV

> 论文提出视觉基础模型应集成动态关系图以增强其在复杂任务中的表现，通过实例证明集成图推理模块的优势，并提出了未来研究方向。

<details>
  <summary>Details</summary>

**Motivation:** FMs在需要明确实体、角色和时空关系推理的任务上存在持续的局限性，而这些关系能力对于细粒度的人类活动识别、第一人称视频理解和多模态医学影像分析是至关重要的。

**Method:** 建议未来视觉基础模型（FMs）应集成显式关系接口，具体为动态关系图（其拓扑和边语义由输入和任务上下文推断得出）。

**Result:** 研究表明，通过在FMs中增加轻量级、上下文自适应的图推理模块，能够提高细粒度语义准确性、鲁棒性、可解释性和计算效率。同时，这样的混合模型实现了良好的内存和硬件效率，使得在实际资源限制下部署成为可能。

**Conclusion:** 提出了FM图混合模型的研究议程，包括学习动态图构建、多层级关系推理（例如，在行为理解中的部分-对象-场景或医疗成像中的区域-器官）、跨模态融合以及直接检测结构化视觉任务中关系能力的评估协议。

**Abstract:** Vision foundation models (FMs) have become the predominant architecture in
computer vision, providing highly transferable representations learned from
large-scale, multimodal corpora. Nonetheless, they exhibit persistent
limitations on tasks that require explicit reasoning over entities, roles, and
spatio-temporal relations. Such relational competence is indispensable for
fine-grained human activity recognition, egocentric video understanding, and
multimodal medical image analysis, where spatial, temporal, and semantic
dependencies are decisive for performance. We advance the position that
next-generation FMs should incorporate explicit relational interfaces,
instantiated as dynamic relational graphs (graphs whose topology and edge
semantics are inferred from the input and task context). We illustrate this
position with cross-domain evidence from recent systems in human manipulation
action recognition and brain tumor segmentation, showing that augmenting FMs
with lightweight, context-adaptive graph-reasoning modules improves
fine-grained semantic fidelity, out of distribution robustness,
interpretability, and computational efficiency relative to FM only baselines.
Importantly, by reasoning sparsely over semantic nodes, such hybrids also
achieve favorable memory and hardware efficiency, enabling deployment under
practical resource constraints. We conclude with a targeted research agenda for
FM graph hybrids, prioritizing learned dynamic graph construction, multi-level
relational reasoning (e.g., part object scene in activity understanding, or
region organ in medical imaging), cross-modal fusion, and evaluation protocols
that directly probe relational competence in structured vision tasks.

</details>


### [30] [LPLC: A Dataset for License Plate Legibility Classification](https://arxiv.org/abs/2508.18425)
*Lucas Wojcik,Gabriel E. Lima,Valfride Nascimento,Eduil Nascimento Jr.,Rayson Laroca,David Menotti*

Main category: cs.CV

> 研究提出LPLC数据集用于车牌可读性分类，测试结果显示任务难度较高，说明该领域需进一步研究。

<details>
  <summary>Details</summary>

**Motivation:** 为了解决车牌识别中不可读车牌的识别挑战，尤其是那些需要增强识别性能和计算效率的情况，本研究建立了一个专门的数据集。

**Method:** 本研究提出一种新的车辆牌照可读性分类数据集(LPLC数据集)，包含10,210张车辆图像和12,687个标注的车牌，用于分辨车牌是否需要使用超分辨率重建等方法进行增强。该数据集涵盖多种车型、光照条件及图像质量等级。通过细粒度注释，包括车辆级和车牌级遮挡标注，四种可读性类别（完美、良好、较差、不可读）及字符标签，排除不可读车牌。

**Result:** 采用三种图像识别网络(ViT、ResNet和YOLO)作为基准模型进行分类测试，试图判断车牌图像是否足够好、是否需要超分辨率处理或根本无法恢复，整体F1分数均低于80%。

**Conclusion:** 采用的几种基准模型F1分数均低于80%，突出显示了任务的挑战性，强调了在该领域进行深入研究的必要性。LPLC数据集供公开研究使用。

**Abstract:** Automatic License Plate Recognition (ALPR) faces a major challenge when
dealing with illegible license plates (LPs). While reconstruction methods such
as super-resolution (SR) have emerged, the core issue of recognizing these
low-quality LPs remains unresolved. To optimize model performance and
computational efficiency, image pre-processing should be applied selectively to
cases that require enhanced legibility. To support research in this area, we
introduce a novel dataset comprising 10,210 images of vehicles with 12,687
annotated LPs for legibility classification (the LPLC dataset). The images span
a wide range of vehicle types, lighting conditions, and camera/image quality
levels. We adopt a fine-grained annotation strategy that includes vehicle- and
LP-level occlusions, four legibility categories (perfect, good, poor, and
illegible), and character labels for three categories (excluding illegible
LPs). As a benchmark, we propose a classification task using three image
recognition networks to determine whether an LP image is good enough, requires
super-resolution, or is completely unrecoverable. The overall F1 score, which
remained below 80% for all three baseline models (ViT, ResNet, and YOLO),
together with the analyses of SR and LP recognition methods, highlights the
difficulty of the task and reinforces the need for further research. The
proposed dataset is publicly available at
https://github.com/lmlwojcik/lplc-dataset.

</details>


### [31] [CLARIFY: A Specialist-Generalist Framework for Accurate and Lightweight Dermatological Visual Question Answering](https://arxiv.org/abs/2508.18430)
*Aranya Saha,Tanvir Ahmed Khan,Ismam Nur Swapnil,Mohammad Ariful Haque*

Main category: cs.CV

> CLARIFY是用于皮肤病视觉问答任务的专才-通才框架，结合了轻量级的领域训练图像分类器和强大的压缩式对话视觉语言模型，提高了诊断准确性，同时降低了计算成本。

<details>
  <summary>Details</summary>

**Motivation:** 虽然视觉语言模型在医疗任务中表现出巨大潜力，但它们的一般用途性质限制了专业诊断的准确性，且其大规模带来了实际临床部署中的推理成本问题。为此，CLARIFY框架旨在解决这些挑战。

**Method:** CLARIFY采用了一个专才-通才框架，包括一个轻量级的领域训练图像分类器（专才）和一个强大的压缩式对话视觉语言模型（通才）。专才提供快速且高准确率的诊断预测，而通才生成自然语言解释。专才的预测直接引导通才的推理，并通过知识图谱检索模块保证回应的准确性和可靠性。

**Result:** 实验结果表明，与最强基线相比较，CLARIFY在诊断准确性上提高了18%，同时将平均VRAM需求和延迟分别降低了至少20%和5%。

**Conclusion:** 实验证明，专才-通才系统为构建轻量级、具有可信度且临床可行的AI系统提供了一种实用且强大的范例。

**Abstract:** Vision-language models (VLMs) have shown significant potential for medical
tasks; however, their general-purpose nature can limit specialized diagnostic
accuracy, and their large size poses substantial inference costs for real-world
clinical deployment. To address these challenges, we introduce CLARIFY, a
Specialist-Generalist framework for dermatological visual question answering
(VQA). CLARIFY combines two components: (i) a lightweight, domain-trained image
classifier (the Specialist) that provides fast and highly accurate diagnostic
predictions, and (ii) a powerful yet compressed conversational VLM (the
Generalist) that generates natural language explanations to user queries. In
our framework, the Specialist's predictions directly guide the Generalist's
reasoning, focusing it on the correct diagnostic path. This synergy is further
enhanced by a knowledge graph-based retrieval module, which grounds the
Generalist's responses in factual dermatological knowledge, ensuring both
accuracy and reliability. This hierarchical design not only reduces diagnostic
errors but also significantly improves computational efficiency. Experiments on
our curated multimodal dermatology dataset demonstrate that CLARIFY achieves an
18\% improvement in diagnostic accuracy over the strongest baseline, a
fine-tuned, uncompressed single-line VLM, while reducing the average VRAM
requirement and latency by at least 20\% and 5\%, respectively. These results
indicate that a Specialist-Generalist system provides a practical and powerful
paradigm for building lightweight, trustworthy, and clinically viable AI
systems.

</details>


### [32] [VQualA 2025 Challenge on Face Image Quality Assessment: Methods and Results](https://arxiv.org/abs/2508.18445)
*Sizhuo Ma,Wei-Ting Chen,Qiang Gao,Jian Wang,Chris Wei Zhou,Wei Sun,Weixia Zhang,Linhan Cao,Jun Jia,Xiangyang Zhu,Dandan Zhu,Xiongkuo Min,Guangtao Zhai,Baoying Chen,Xiongwei Xiao,Jishen Zeng,Wei Wu,Tiexuan Lou,Yuchen Tan,Chunyi Song,Zhiwei Xu,MohammadAli Hamidi,Hadi Amirpour,Mingyin Bai,Jiawang Du,Zhenyu Jiang,Zilong Lu,Ziguan Cui,Zongliang Gan,Xinpeng Li,Shiqi Jiang,Chenhui Li,Changbo Wang,Weijun Yuan,Zhan Li,Yihang Chen,Yifan Deng,Ruting Deng,Zhanglu Chen,Boyang Yao,Shuling Zheng,Feng Zhang,Zhiheng Fu,Abhishek Joshi,Aman Agarwal,Rakhil Immidisetti,Ajay Narasimha Mopidevi,Vishwajeet Shukla,Hao Yang,Ruikun Zhang,Liyuan Pan,Kaixin Deng,Hang Ouyang,Fan yang,Zhizun Luo,Zhuohang Shi,Songning Lai,Weilin Ruan,Yutao Yue*

Main category: cs.CV

> The VQualA 2025 Challenge on Face Image Quality Assessment (FIQA) aimed to develop efficient models under strict resource constraints to predict the quality of degraded face images.

<details>
  <summary>Details</summary>

**Motivation:** Real-world conditions often degrade face images, impacting their usability in applications. Thus, the challenge sought to improve the assessment of face image quality under realistic degradations.

**Method:** Structure

**Result:** The challenge involved 127 participants and 1519 submissions, focusing on creating lightweight models for face image quality assessment that could handle images with various resolutions and realistic degradations.

**Conclusion:** The challenge highlighted the development of efficient and accurate models for FIQA, advancing methods that could handle real-world image degradations, contributing to more practical and effective quality assessment techniques for face images.

**Abstract:** Face images play a crucial role in numerous applications; however, real-world
conditions frequently introduce degradations such as noise, blur, and
compression artifacts, affecting overall image quality and hindering subsequent
tasks. To address this challenge, we organized the VQualA 2025 Challenge on
Face Image Quality Assessment (FIQA) as part of the ICCV 2025 Workshops.
Participants created lightweight and efficient models (limited to 0.5 GFLOPs
and 5 million parameters) for the prediction of Mean Opinion Scores (MOS) on
face images with arbitrary resolutions and realistic degradations. Submissions
underwent comprehensive evaluations through correlation metrics on a dataset of
in-the-wild face images. This challenge attracted 127 participants, with 1519
final submissions. This report summarizes the methodologies and findings for
advancing the development of practical FIQA approaches.

</details>


### [33] [Context-Aware Zero-Shot Anomaly Detection in Surveillance Using Contrastive and Predictive Spatiotemporal Modeling](https://arxiv.org/abs/2508.18463)
*Md. Rashid Shahriar Khan,Md. Abrar Hasan,Mohammod Tareq Aziz Justice*

Main category: cs.CV

> 本文提出了一种新的上下文感知零样本异常检测框架，结合了多个模型来识别监控视频中的异常行为，无需在训练中使用异常示例。

<details>
  <summary>Details</summary>

**Motivation:** 由于异常行为在监控视频中难以预测且具有上下文依赖性，因此检测异常行为具有挑战性。为了解决这一问题，本文提出了一个零样本异常检测的方法，以克服在训练期间难以获取足够的异常示例的问题。

**Method:** 本文提出了一种新的上下文感知的零样本异常检测框架，用于识别异常事件，而无需在训练期间暴露于异常示例。该混合架构结合了TimeSformer、DPC和CLIP来建模时空动态和语义上下文。TimeSformer作为视觉骨干提取丰富的时空特性，DPC预测未来的表示形式来识别时间偏差。此外，一个基于CLIP的语义流通过上下文特定的文本提示实现概念级别的异常检测。这些组件使用InfoNCE和CPC损失联合训练，将视觉输入与其时间语义表示对齐。一个上下文门控机制进一步通过调制预测以场景感知线索或全局视频特征来增强决策。通过整合预测建模与视觉语言理解，该系统可以推广到复杂的环境中先前未见过的行为。

**Result:** 该框架将时间推理与零样本异常检测中的语义上下文结合在一起。该方法在没有异常示例的情况下识别异常事件，扩展了零样本异常检测在监控中的应用。

**Conclusion:** 该研究实现了在没有异常示例的情况下识别异常事件的目标，同时利用视觉语言理解增强了决策的准确性。

**Abstract:** Detecting anomalies in surveillance footage is inherently challenging due to
their unpredictable and context-dependent nature. This work introduces a novel
context-aware zero-shot anomaly detection framework that identifies abnormal
events without exposure to anomaly examples during training. The proposed
hybrid architecture combines TimeSformer, DPC, and CLIP to model spatiotemporal
dynamics and semantic context. TimeSformer serves as the vision backbone to
extract rich spatial-temporal features, while DPC forecasts future
representations to identify temporal deviations. Furthermore, a CLIP-based
semantic stream enables concept-level anomaly detection through
context-specific text prompts. These components are jointly trained using
InfoNCE and CPC losses, aligning visual inputs with their temporal and semantic
representations. A context-gating mechanism further enhances decision-making by
modulating predictions with scene-aware cues or global video features. By
integrating predictive modeling with vision-language understanding, the system
can generalize to previously unseen behaviors in complex environments. This
framework bridges the gap between temporal reasoning and semantic context in
zero-shot anomaly detection for surveillance. The code for this research has
been made available at
https://github.com/NK-II/Context-Aware-ZeroShot-Anomaly-Detection-in-Surveillance.

</details>


### [34] [DoGFlow: Self-Supervised LiDAR Scene Flow via Cross-Modal Doppler Guidance](https://arxiv.org/abs/2508.18506)
*Ajinkya Khoche,Qingwen Zhang,Yixi Cai,Sina Sharif Mansouri,Patric Jensfelt*

Main category: cs.CV

> DoGFlow是一种新型的自我监督框架，可以从雷达数据中获取伪标签并应用于激光雷达数据中，以实现3D场景流估计，大大提高了自我监督方法的性能和数据标注的效率。

<details>
  <summary>Details</summary>

**Motivation:** 准确的3D场景流估计对于自动驾驶系统在动态环境中安全导航至关重要，但生成大规模的手动注释数据集却是一个很大的瓶颈。现有自我监督方法难以在长距离和恶劣天气场景中达到监督学习方法的性能，而监督学习方法依赖昂贵的人工标注，难以扩展。

**Method:** 本研究提出了DoGFlow框架，通过跨模态标签传递的方法，从4D雷达多普勒测量中实时计算运动伪标签，并将其转移到激光雷达（LiDAR）域，以恢复完整的3D对象运动。

**Result:** 在具有挑战性的MAN TruckScenes数据集上，DoGFlow显著优于现有的自我监督方法，并提高了标签效率，使激光雷达骨架能够仅用10%的真实标注数据达到超过90%的完全监督性能。

**Conclusion:** DoGFlow通过跨模态标签传递方法，无需任何手动标注就能实现高质量的3D场景流估计，从而解决了现有自我监督方法在长距离和恶劣天气条件下性能不足的问题。

**Abstract:** Accurate 3D scene flow estimation is critical for autonomous systems to
navigate dynamic environments safely, but creating the necessary large-scale,
manually annotated datasets remains a significant bottleneck for developing
robust perception models. Current self-supervised methods struggle to match the
performance of fully supervised approaches, especially in challenging
long-range and adverse weather scenarios, while supervised methods are not
scalable due to their reliance on expensive human labeling. We introduce
DoGFlow, a novel self-supervised framework that recovers full 3D object motions
for LiDAR scene flow estimation without requiring any manual ground truth
annotations. This paper presents our cross-modal label transfer approach, where
DoGFlow computes motion pseudo-labels in real-time directly from 4D radar
Doppler measurements and transfers them to the LiDAR domain using dynamic-aware
association and ambiguity-resolved propagation. On the challenging MAN
TruckScenes dataset, DoGFlow substantially outperforms existing self-supervised
methods and improves label efficiency by enabling LiDAR backbones to achieve
over 90% of fully supervised performance with only 10% of the ground truth
data. For more details, please visit https://ajinkyakhoche.github.io/DogFlow/

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [35] [Federative ischemic stroke segmentation as alternative to overcome domain-shift multi-institution challenges](https://arxiv.org/abs/2508.18296)
*Edgar Rangel,Fabio Martinez*

Main category: eess.IV

> 本文开发了一个协作框架，用于分割DWI上的缺血性中风病变，通过共享知识提高了模型的泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 由于患者的人口统计数据、扫描仪供应商和专家注释的不同，DWI确定中风病变存在高度变异性，现有的计算支持方法通常局限于单一机构的数据，缺乏泛化性。

**Method:** 该论文提出了一种协作框架，用于通过共享深度中心无关表示知识来分割DWI序列中的缺血性中风病变，并通过14个模拟的医疗中心和2031项研究对其进行了验证。

**Result:** FedAvg模型在所有中心的总体DSC为0.71±0.24，AVD为5.29±22.74，ALD为2.16±3.60，LF1为0.70±0.26，优于集中式模型和其他联邦学习规则。

**Conclusion:** 模型展示了强大的泛化性能，且在不同病变类别中表现一致，在分布外中心也展示了可靠的表现。

**Abstract:** Stroke is the second leading cause of death and the third leading cause of
disability worldwide. Clinical guidelines establish diffusion resonance imaging
(DWI, ADC) as the standard for localizing, characterizing, and measuring
infarct volume, enabling treatment support and prognosis. Nonetheless, such
lesion analysis is highly variable due to different patient demographics,
scanner vendors, and expert annotations. Computational support approaches have
been key to helping with the localization and segmentation of lesions. However,
these strategies are dedicated solutions that learn patterns from only one
institution, lacking the variability to generalize geometrical lesions shape
models. Even worse, many clinical centers lack sufficient labeled samples to
adjust these dedicated solutions. This work developed a collaborative framework
for segmenting ischemic stroke lesions in DWI sequences by sharing knowledge
from deep center-independent representations. From 14 emulated healthcare
centers with 2031 studies, the FedAvg model achieved a general DSC of $0.71 \pm
0.24$, AVD of $5.29 \pm 22.74$, ALD of $2.16 \pm 3.60$ and LF1 of $0.70 \pm
0.26$ over all centers, outperforming both the centralized and other federated
rules. Interestingly, the model demonstrated strong generalization properties,
showing uniform performance across different lesion categories and reliable
performance in out-of-distribution centers (with DSC of $0.64 \pm 0.29$ and AVD
of $4.44 \pm 8.74$ without any additional training).

</details>


### [36] [Analise de Desaprendizado de Maquina em Modelos de Classificacao de Imagens Medicas](https://arxiv.org/abs/2508.18509)
*Andreza M. C. Falcao,Filipe R. Cordeiro*

Main category: eess.IV

> 研究了SalUn无学习模型在医学图像分类数据集上的表现，发现其性能接近完全重新训练，为医疗应用提供了高效的无学习解决方案。

<details>
  <summary>Details</summary>

**Motivation:** 机器无学习旨在从预训练模型中删除私人或敏感数据，同时保持模型的稳健性。尽管近期有所进展，但这种技术尚未在医学图像分类中得到探索。

**Method:** 评估了SalUn无学习模型在PathMNIST、OrganAMNIST和BloodMNIST数据集上的表现，并分析了数据增强对无学习质量的影响。

**Result:** 结果显示SalUn的表现接近于完全重新训练，表明它是一个有效的解决方案。

**Conclusion:** SalUn模型在医疗应用中提供了一种有效的无学习解决方案，其性能接近完全重新训练。

**Abstract:** Machine unlearning aims to remove private or sensitive data from a
pre-trained model while preserving the model's robustness. Despite recent
advances, this technique has not been explored in medical image classification.
This work evaluates the SalUn unlearning model by conducting experiments on the
PathMNIST, OrganAMNIST, and BloodMNIST datasets. We also analyse the impact of
data augmentation on the quality of unlearning. Results show that SalUn
achieves performance close to full retraining, indicating an efficient solution
for use in medical applications.

</details>


### [37] [A Deep Learning Application for Psoriasis Detection](https://arxiv.org/abs/2508.18528)
*Anna Milani,Fábio S. da Silva,Elloá B. Guedes,Ricardo Rios*

Main category: eess.IV

> 研究比较了三种卷积神经网络模型在银屑病皮肤图像分类中的性能，结果显示Inception v3模型表现最佳。

<details>
  <summary>Details</summary>

**Motivation:** 目的是比较三种卷积神经网络模型在皮肤病变图像分类中的性能，为银屑病的准确诊断提供支持。

**Method:** 本文通过比较ResNet50、Inception v3和VGG19三个卷积神经网络模型在皮肤病变（银屑病）图像分类上的性能来进行研究。

**Result:** 研究结果显示Inception v3模型在准确率和F1-Score（97.5% ± 0.2）方面表现良好，因此可以作为支持银屑病诊断的有力工具。

**Conclusion:** 结论表明，Inception v3模型在银屑病的皮肤病变图像分类任务中，具有较高的诊断价值。

**Abstract:** In this paper a comparative study of the performance of three Convolutional
Neural Network models, ResNet50, Inception v3 and VGG19 for classification of
skin images with lesions affected by psoriasis is presented. The images used
for training and validation of the models were obtained from specialized
platforms. Some techniques were used to adjust the evaluation metrics of the
neural networks. The results found suggest the model Inception v3 as a valuable
tool for supporting the diagnosis of psoriasis. This is due to its satisfactory
performance with respect to accuracy and F1-Score (97.5% ${\pm}$ 0.2).

</details>
