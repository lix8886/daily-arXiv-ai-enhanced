{"id": "2506.10005", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.GR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.10005", "abs": "https://arxiv.org/abs/2506.10005", "authors": ["Sridhar S", "Nithin A", "Shakeel Rifath", "Vasantha Raj"], "title": "Multimodal Cinematic Video Synthesis Using Text-to-Image and Audio Generation Models", "comment": "10 pages, seven figures about Multimodal Cinematic Video Synthesis\n  Using Text-to-Image and Audio Generation Models", "summary": "Advances in generative artificial intelligence have altered multimedia\ncreation, allowing for automatic cinematic video synthesis from text inputs.\nThis work describes a method for creating 60-second cinematic movies\nincorporating Stable Diffusion for high-fidelity image synthesis, GPT-2 for\nnarrative structuring, and a hybrid audio pipeline using gTTS and\nYouTube-sourced music. It uses a five-scene framework, which is augmented by\nlinear frame interpolation, cinematic post-processing (e.g., sharpening), and\naudio-video synchronization to provide professional-quality results. It was\ncreated in a GPU-accelerated Google Colab environment using Python 3.11. It has\na dual-mode Gradio interface (Simple and Advanced), which supports resolutions\nof up to 1024x768 and frame rates of 15-30 FPS. Optimizations such as CUDA\nmemory management and error handling ensure reliability. The experiments\ndemonstrate outstanding visual quality, narrative coherence, and efficiency,\nfurthering text-to-video synthesis for creative, educational, and industrial\napplications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4ece\u6587\u672c\u8f93\u5165\u81ea\u52a8\u751f\u621060\u79d2\u7535\u5f71\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e86Stable Diffusion\u3001GPT-2\u548c\u6df7\u5408\u97f3\u9891\u7ba1\u9053\u6280\u672f\uff0c\u63d0\u4f9b\u4e86\u4e13\u4e1a\u7684\u89c6\u9891\u8d28\u91cf\u548c\u6545\u4e8b\u8fde\u8d2f\u6027\u3002", "motivation": "\u9274\u4e8e\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u8fdb\u6b65\uff0c\u8fd9\u9879\u5de5\u4f5c\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u5148\u8fdb\u7684\u56fe\u50cf\u3001\u53d9\u8ff0\u548c\u97f3\u9891\u6280\u672f\uff0c\u4ece\u6587\u672c\u8f93\u5165\u521b\u5efa\u5177\u6709\u9ad8\u8d28\u91cf\u7684\u52a8\u6001\u89c6\u9891\uff0c\u4ee5\u652f\u6301\u521b\u610f\u3001\u6559\u80b2\u548c\u5de5\u4e1a\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5305\u62ec\u5229\u7528Stable Diffusion\u8fdb\u884c\u9ad8\u8d28\u91cf\u56fe\u50cf\u5408\u6210\uff0cGPT-2\u6784\u5efa\u53d9\u8ff0\u7ed3\u6784\uff0c\u4ee5\u53ca\u7ed3\u5408\u4f5c\u6587\u8bed\u97f3\u5408\u6210\u548cYouTube\u97f3\u4e50\u7684\u6df7\u5408\u97f3\u9891\u7ba1\u9053\uff0c\u901a\u8fc7\u7ebf\u6027\u5e27\u63d2\u5165\u3001\u540e\u671f\u5236\u4f5c\u548c\u97f3\u9891\u89c6\u9891\u540c\u6b65\u5728Google Colab\u4e0a\u5b9e\u73b0\u6b64\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u6545\u4e8b\u8fde\u8d2f\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5df2\u5728\u7b80\u5355\u7684GUI\u548c\u9ad8\u7ea7\u6a21\u5f0f\u7684\u4e0d\u540c\u5206\u8fa8\u7387\u548c\u5e27\u7387\u4e0b\u8fdb\u884c\u4e86\u6d4b\u8bd5\u548c\u4f18\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u751f\u6210\u6587\u672c\u5230\u89c6\u9891\u5408\u6210\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u5e94\u7528\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u521b\u610f\u89c6\u9891\u5236\u4f5c\u3001\u6559\u80b2\u8d44\u6e90\u5f00\u53d1\u548c\u5546\u4e1a\u5185\u5bb9\u521b\u4f5c\u3002"}}
{"id": "2506.10082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.10082", "abs": "https://arxiv.org/abs/2506.10082", "authors": ["Chenjian Gao", "Lihe Ding", "Xin Cai", "Zhanpeng Huang", "Zibin Wang", "Tianfan Xue"], "title": "LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning", "comment": "12 pages", "summary": "Video editing using diffusion models has achieved remarkable results in\ngenerating high-quality edits for videos. However, current methods often rely\non large-scale pretraining, limiting flexibility for specific edits.\nFirst-frame-guided editing provides control over the first frame, but lacks\nflexibility over subsequent frames. To address this, we propose a mask-based\nLoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video\n(I2V) models for flexible video editing. Our approach preserves background\nregions while enabling controllable edits propagation. This solution offers\nefficient and adaptable video editing without altering the model architecture.\nTo better steer this process, we incorporate additional references, such as\nalternate viewpoints or representative scene states, which serve as visual\nanchors for how content should unfold. We address the control challenge using a\nmask-driven LoRA tuning strategy that adapts a pre-trained image-to-video model\nto the editing context. The model must learn from two distinct sources: the\ninput video provides spatial structure and motion cues, while reference images\noffer appearance guidance. A spatial mask enables region-specific learning by\ndynamically modulating what the model attends to, ensuring that each area draws\nfrom the appropriate source. Experimental results show our method achieves\nsuperior video editing performance compared to state-of-the-art methods.", "AI": {"tldr": "A new method uses mask-based LoRA tuning to adaptively edit videos, offering control over content propagation without changing the model's architecture.", "motivation": "The motivation is to provide more flexibility in video editing beyond the limitations of existing methods, which often require large-scale pretraining and lack control over edits beyond the first frame.", "method": "Our approach involves a mask-based LoRA tuning method that allows the adaptation of pre-trained Image-to-Video models for specific video edits, preserving background regions while allowing for controllable alterations.", "result": "Experimental results demonstrate superior performance over current state-of-the-art methods.", "conclusion": "The mask-based LoRA tuning method enables efficient and adaptable video editing, providing enhanced control over video content propagation."}}
{"id": "2506.10084", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.10084", "abs": "https://arxiv.org/abs/2506.10084", "authors": ["Bin Guo", "John H. L. Hansen"], "title": "DeepTraverse: A Depth-First Search Inspired Network for Algorithmic Visual Understanding", "comment": "NeurIPS 2025", "summary": "Conventional vision backbones, despite their success, often construct\nfeatures through a largely uniform cascade of operations, offering limited\nexplicit pathways for adaptive, iterative refinement. This raises a compelling\nquestion: can principles from classical search algorithms instill a more\nalgorithmic, structured, and logical processing flow within these networks,\nleading to representations built through more interpretable, perhaps\nreasoning-like decision processes? We introduce DeepTraverse, a novel vision\narchitecture directly inspired by algorithmic search strategies, enabling it to\nlearn features through a process of systematic elucidation and adaptive\nrefinement distinct from conventional approaches. DeepTraverse operationalizes\nthis via two key synergistic components: recursive exploration modules that\nmethodically deepen feature analysis along promising representational paths\nwith parameter sharing for efficiency, and adaptive calibration modules that\ndynamically adjust feature salience based on evolving global context. The\nresulting algorithmic interplay allows DeepTraverse to intelligently construct\nand refine feature patterns. Comprehensive evaluations across a diverse suite\nof image classification benchmarks show that DeepTraverse achieves highly\ncompetitive classification accuracy and robust feature discrimination, often\noutperforming conventional models with similar or larger parameter counts. Our\nwork demonstrates that integrating such algorithmic priors provides a\nprincipled and effective strategy for building more efficient, performant, and\nstructured vision backbones.", "AI": {"tldr": "DeepTraverse\u662f\u57fa\u4e8e\u7ecf\u5178\u641c\u7d22\u7b97\u6cd5\u8bbe\u8ba1\u7684\u65b0\u89c6\u89c9\u67b6\u6784\uff0c\u5b83\u901a\u8fc7\u9012\u5f52\u63a2\u7d22\u548c\u81ea\u9002\u5e94\u6821\u51c6\u6765\u6784\u5efa\u548c\u4f18\u5316\u7279\u5f81\u3002\u5b9e\u9a8c\u8868\u660e\u5728\u4fdd\u6301\u6216\u8d85\u8fc7\u4f20\u7edf\u6a21\u578b\u7684\u6027\u80fd\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u7279\u5f81\u5206\u7c7b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u901a\u8fc7\u4e00\u7cfb\u5217\u64cd\u4f5c\u6784\u9020\u7279\u5f81\uff0c\u5e76\u4e14\u7f3a\u4e4f\u660e\u786e\u7684\u9002\u5e94\u6027\u548c\u8fed\u4ee3\u4fee\u6b63\u7684\u8def\u5f84\u3002\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u8bd5\u56fe\u901a\u8fc7\u5f15\u5165\u7ecf\u5178\u641c\u7d22\u7b97\u6cd5\u7684\u539f\u5219\u6765\u6539\u8fdb\u8fd9\u4e00\u70b9\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u540d\u4e3aDeepTraverse\u7684\u65b0\u89c6\u89c9\u67b6\u6784\uff0c\u7075\u611f\u6765\u81ea\u7ecf\u5178\u7684\u641c\u7d22\u7b97\u6cd5\u3002DeepTraverse\u5305\u62ec\u4e24\u4e2a\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff1a\u9012\u5f52\u63a2\u7d22\u6a21\u5757\uff0c\u901a\u8fc7\u5171\u4eab\u53c2\u6570\u4e0d\u65ad\u6df1\u5316\u7279\u5f81\u5206\u6790\uff1b\u81ea\u9002\u5e94\u6821\u51c6\u6a21\u5757\uff0c\u6839\u636e\u4e0d\u65ad\u53d8\u5316\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u52a8\u6001\u8c03\u6574\u7279\u5f81\u7684\u91cd\u8981\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cDeepTraverse\u5728\u591a\u6837\u5316\u7684\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u5728\u7279\u5f81\u533a\u5206\u5ea6\u65b9\u9762\u901a\u5e38\u4f18\u4e8e\u53c2\u6570\u91cf\u7c7b\u4f3c\u751a\u81f3\u66f4\u591a\u7684\u4f20\u7edf\u6a21\u578b\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u96c6\u6210\u7b97\u6cd5\u6027\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u6784\u5efa\u51fa\u66f4\u9ad8\u6548\u3001\u6027\u80fd\u66f4\u597d\u4e14\u7ed3\u6784\u66f4\u6e05\u6670\u7684\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u3002"}}
{"id": "2506.10085", "categories": ["cs.CV", "cs.AI", "I.2.6; I.2.9; I.2.10"], "pdf": "https://arxiv.org/pdf/2506.10085", "abs": "https://arxiv.org/abs/2506.10085", "authors": ["Christos Ziakas", "Alessandra Russo"], "title": "Test-Time Adaptation for Generalizable Task Progress Estimation", "comment": "pages, 2 figures, accepted to the 2nd Workshop on Test-Time\n  Adaptation: Putting Updates to the Test (PUT) at 42nd International\n  Conference on Machine Learning (ICML), Vancouver, Canada, 2025", "summary": "We propose a test-time adaptation method that enables a progress estimation\nmodel to adapt online to the visual and temporal context of test trajectories\nby optimizing a learned self-supervised objective. To this end, we introduce a\ngradient-based meta-learning strategy to train the model on expert visual\ntrajectories and their natural language task descriptions, such that test-time\nadaptation improves progress estimation relying on semantic content over\ntemporal order. Our test-time adaptation method generalizes from a single\ntraining environment to diverse out-of-distribution tasks, environments, and\nembodiments, outperforming the state-of-the-art in-context learning approach\nusing autoregressive vision-language models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u57fa\u4e8e\u8bed\u4e49\u5185\u5bb9\u7684\u8fdb\u5ea6\u4f30\u8ba1\uff0c\u8be5\u65b9\u6cd5\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u6539\u8fdb\u73b0\u6709\u7684\u8fdb\u5ea6\u4f30\u8ba1\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u9002\u5e94\u89c6\u89c9\u548c\u65f6\u95f4\u4e0a\u4e0b\u6587\u7684\u53d8\u5316\uff0c\u4ece\u800c\u63d0\u9ad8\u5728\u5206\u5e03\u5916\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u4f7f\u8fdb\u5ea6\u4f30\u8ba1\u6a21\u578b\u901a\u8fc7\u4f18\u5316\u4e00\u4e2a\u5b66\u4e60\u7684\u81ea\u76d1\u7763\u76ee\u6807\uff0c\u5728\u7ebf\u9002\u5e94\u6d4b\u8bd5\u8f68\u8ff9\u7684\u89c6\u89c9\u548c\u65f6\u95f4\u4e0a\u4e0b\u6587\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u5143\u5b66\u4e60\u7b56\u7565\uff0c\u7528\u4e8e\u8bad\u7ec3\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u4ece\u4e13\u5bb6\u7684\u89c6\u89c9\u8f68\u8ff9\u53ca\u5176\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u4e2d\u5b66\u4e60\uff0c\u4ece\u800c\u5728\u6d4b\u8bd5\u65f6\u9002\u5e94\u6027\u6539\u8fdb\u57fa\u4e8e\u8bed\u4e49\u5185\u5bb9\u800c\u4e0d\u662f\u65f6\u95f4\u987a\u5e8f\u7684\u8fdb\u5ea6\u4f30\u8ba1\u3002", "result": "\u6211\u4eec\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u80fd\u591f\u4ece\u5355\u4e2a\u8bad\u7ec3\u73af\u5883\u6cdb\u5316\u5230\u591a\u6837\u5316\u7684\u5206\u5e03\u5916\u4efb\u52a1\u3001\u73af\u5883\u548c\u8eab\u4f53\u5f62\u6001\u4e0a\uff0c\u8d85\u8d8a\u4e86\u4f7f\u7528\u81ea\u56de\u5f52\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u5f53\u524d\u6700\u5148\u8fdb\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u8bed\u4e49\u5185\u5bb9\u7684\u7406\u89e3\uff0c\u8fd8\u5c55\u73b0\u4e86\u5728\u591a\u6837\u5316\u7684\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u548c\u73af\u5883\u4e2d\u7684\u4f18\u8d8a\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2506.10019", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10019", "abs": "https://arxiv.org/abs/2506.10019", "authors": ["Tian Lan", "Yang-Hao Zhou", "Zi-Ao Ma", "Fanshu Sun", "Rui-Qing Sun", "Junyu Luo", "Rong-Cheng Tu", "Heyan Huang", "Chen Xu", "Zhijing Wu", "Xian-Ling Mao"], "title": "A Survey of Automatic Evaluation Methods on Text, Visual and Speech Generations", "comment": null, "summary": "Recent advances in deep learning have significantly enhanced generative AI\ncapabilities across text, images, and audio. However, automatically evaluating\nthe quality of these generated outputs presents ongoing challenges. Although\nnumerous automatic evaluation methods exist, current research lacks a\nsystematic framework that comprehensively organizes these methods across text,\nvisual, and audio modalities. To address this issue, we present a comprehensive\nreview and a unified taxonomy of automatic evaluation methods for generated\ncontent across all three modalities; We identify five fundamental paradigms\nthat characterize existing evaluation approaches across these domains. Our\nanalysis begins by examining evaluation methods for text generation, where\ntechniques are most mature. We then extend this framework to image and audio\ngeneration, demonstrating its broad applicability. Finally, we discuss\npromising directions for future research in cross-modal evaluation\nmethodologies.", "AI": {"tldr": "\u8bba\u6587\u6982\u8ff0\uff1a\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8de8\u6a21\u6001\u7684\u751f\u6210\u5185\u5bb9\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u7684\u5168\u9762\u56de\u987e\u548c\u7edf\u4e00\u5206\u7c7b\u6cd5\uff0c\u9002\u7528\u4e8e\u6587\u672c\u3001\u56fe\u50cf\u548c\u97f3\u9891\u751f\u6210\u9886\u57df\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\u6846\u67b6\u7684\u95ee\u9898\u3002", "motivation": "\u672c\u6587\u7684\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u81ea\u52a8\u8bc4\u4f30\u751f\u6210\u5185\u5bb9\u8d28\u91cf\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6587\u672c\u3001\u56fe\u50cf\u548c\u97f3\u9891\u751f\u6210\u9886\u57df\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5206\u6790\u8bba\u6587\u4e2d\u7684\u65b9\u6cd5\u90e8\u5206\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8de8\u6a21\u6001\u751f\u6210\u5185\u5bb9\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u7684\u5168\u9762\u56de\u987e\u548c\u7edf\u4e00\u5206\u7c7b\u6cd5\u3002", "result": "\u672c\u6587\u786e\u7acb\u4e86\u4e00\u4e2a\u7528\u4e8e\u8de8\u6a21\u6001\u751f\u6210\u5185\u5bb9\u7684\u8bc4\u4f30\u65b9\u6cd5\u7684\u5168\u9762\u56de\u987e\u548c\u7edf\u4e00\u5206\u7c7b\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u6269\u5c55\u5230\u4e86\u6587\u672c\u3001\u56fe\u50cf\u548c\u97f3\u9891\u751f\u6210\u9886\u57df\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u662f\u63d0\u51fa\u4e86\u4e00\u4e2a\u9002\u7528\u4e8e\u6587\u672c\u3001\u56fe\u50cf\u548c\u97f3\u9891\u751f\u6210\u5185\u5bb9\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u786e\u5b9a\u4e86\u4e94\u79cd\u57fa\u7840\u8303\u5f0f\u6765\u63cf\u8ff0\u8fd9\u4e9b\u9886\u57df\u7684\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u8de8\u6a21\u6001\u8bc4\u4f30\u65b9\u6cd5\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2506.10100", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.10100", "abs": "https://arxiv.org/abs/2506.10100", "authors": ["Yantai Yang", "Yuhao Wang", "Zichen Wen", "Luo Zhongwei", "Chang Zou", "Zhipeng Zhang", "Chuan Wen", "Linfeng Zhang"], "title": "EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark.", "AI": {"tldr": "EfficientVLA\u662f\u4e00\u79cd\u7528\u4e8e\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u9ad8\u6548\u63a8\u7406\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u4e09\u79cd\u7b56\u7565\u7cfb\u7edf\u6027\u5730\u6d88\u9664\u5197\u4f59\uff0c\u5b9e\u73b0\u52a0\u901f\u548c\u51cf\u5c11\u8ba1\u7b97\u91cf\u7684\u76ee\u6807\u3002", "motivation": "\u9488\u5bf9\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b(VLA)\u9ad8\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u7684\u74f6\u9888\u95ee\u9898\uff0c\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u5f80\u5f80\u53ea\u9488\u5bf9\u5b64\u7acb\u7684\u6548\u7387\u95ee\u9898\uff0c\u65e0\u6cd5\u5168\u9762\u89e3\u51b3\u6574\u4e2aVLA\u6d41\u6c34\u7ebf\u4e2d\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEfficientVLA\u7684\u52a0\u901f\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4e09\u79cd\u7b56\u7565\u7cfb\u7edf\u5730\u6d88\u9664\u5197\u4f59\uff1a1. \u6839\u636e\u5c42\u95f4\u5197\u4f59\u6027\u5206\u6790\u5bf9\u8bed\u8a00\u6a21\u5757\u8fdb\u884c\u526a\u679d\u64cd\u4f5c\uff1b2. \u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u7b56\u7565\u4f18\u5316\u89c6\u89c9\u5904\u7406\u8def\u5f84\uff0c\u9009\u62e9\u4e00\u7ec4\u7d27\u51d1\u3001\u591a\u6837\u5316\u7684\u89c6\u89c9\u6807\u8bb0\uff1b3. \u5728\u8fed\u4ee3\u6269\u6563\u52a8\u4f5c\u5934\u4e2d\u7f13\u5b58\u5e76\u590d\u7528\u5173\u952e\u4e2d\u95f4\u7279\u5f81\uff0c\u4ee5\u51cf\u5c11\u65f6\u95f4\u4e0a\u7684\u8ba1\u7b97\u5197\u4f59\u3002", "result": "\u5c06\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u6807\u51c6VLA\u6a21\u578bCogACT\uff0c\u83b7\u5f97\u4e861.93\u500d\u7684\u63a8\u7406\u52a0\u901f\uff0cFLOPs\u964d\u4f4e\u523028.9%\uff0c\u5728SIMPLER\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6210\u529f\u7387\u4e0b\u964d\u4ec50.6%\u3002", "conclusion": "EfficientVLA\u6846\u67b6\u8bc1\u660e\u4e86\u5176\u80fd\u591f\u6709\u6548\u5730\u51cf\u5c11VLA\u6a21\u578b\u4e2d\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u6210\u529f\u7387\uff0c\u6709\u52a9\u4e8e\u8fd9\u4e9b\u6a21\u578b\u5728\u5b9e\u9645\u4e2d\u7684\u90e8\u7f72\u3002"}}
{"id": "2506.10055", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.10055", "abs": "https://arxiv.org/abs/2506.10055", "authors": ["Dingfeng Shi", "Jingyi Cao", "Qianben Chen", "Weichen Sun", "Weizhen Li", "Hongxuan Lu", "Fangchen Dong", "Tianrui Qin", "King Zhu", "Minghao Yang", "Jian Yang", "Ge Zhang", "Jiaheng Liu", "Changwang Zhang", "Jun Wang", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "title": "TaskCraft: Automated Generation of Agentic Tasks", "comment": null, "summary": "Agentic tasks, which require multi-step problem solving with autonomy, tool\nuse, and adaptive reasoning, are becoming increasingly central to the\nadvancement of NLP and AI. However, existing instruction data lacks tool\ninteraction, and current agentic benchmarks rely on costly human annotation,\nlimiting their scalability. We introduce \\textsc{TaskCraft}, an automated\nworkflow for generating difficulty-scalable, multi-tool, and verifiable agentic\ntasks with execution trajectories. TaskCraft expands atomic tasks using\ndepth-based and width-based extensions to create structurally and\nhierarchically complex challenges. Empirical results show that these tasks\nimprove prompt optimization in the generation workflow and enhance supervised\nfine-tuning of agentic foundation models. We present a large-scale synthetic\ndataset of approximately 36,000 tasks with varying difficulty to support future\nresearch on agent tuning and evaluation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TaskCraft\uff0c\u4e00\u79cd\u81ea\u52a8\u751f\u6210\u667a\u80fd\u4efb\u52a1\u7684\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u6539\u8fdbNLP\u548cAI\u4e2d\u7684\u591a\u6b65\u9aa4\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "motivation": "\u9ad8\u9636\u4efb\u52a1\u9700\u8981\u81ea\u4e3b\u3001\u5de5\u5177\u4f7f\u7528\u548c\u9002\u5e94\u6027\u63a8\u7406\u7b49\u591a\u6b65\u9aa4\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u5b83\u4eec\u8d8a\u6765\u8d8a\u6210\u4e3a\u4fc3\u8fdbNLP\u548cAI\u53d1\u5c55\u7684\u6838\u5fc3\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u6307\u4ee4\u6570\u636e\u7f3a\u4e4f\u5de5\u5177\u4ea4\u4e92\uff0c\u800c\u73b0\u6709\u7684\u9ad8\u9636\u4efb\u52a1\u57fa\u51c6\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002", "method": "\u5f15\u5165\u4e86TaskCraft\uff0c\u8fd9\u662f\u4e00\u4e2a\u53ef\u4ee5\u81ea\u52a8\u751f\u6210\u96be\u5ea6\u53ef\u8c03\u3001\u591a\u5de5\u5177\u534f\u4f5c\u4e14\u53ef\u4ee5\u9a8c\u8bc1\u7684\u667a\u80fd\u4efb\u52a1\u7684\u5de5\u4f5c\u6d41\u3002TaskCraft\u901a\u8fc7\u57fa\u4e8e\u6df1\u5ea6\u548c\u5bbd\u5ea6\u7684\u6269\u5c55\u6765\u6269\u5c55\u539f\u5b50\u4efb\u52a1\uff0c\u521b\u9020\u51fa\u7ed3\u6784\u590d\u6742\u548c\u5c42\u6b21\u5316\u7684\u6311\u6218\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8fd9\u4e9b\u4efb\u52a1\u53ef\u4ee5\u6539\u5584\u751f\u6210\u5de5\u4f5c\u6d41\u4e2d\u7684\u63d0\u793a\u4f18\u5316\uff0c\u5e76\u63d0\u9ad8\u667a\u80fd\u57fa\u7840\u6a21\u578b\u7684\u76d1\u7763\u5fae\u8c03\u6548\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5305\u542b\u5927\u7ea636,000\u4e2a\u4e0d\u540c\u96be\u5ea6\u4efb\u52a1\u7684\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u672a\u6765\u7684\u667a\u80fd\u8c03\u4f18\u548c\u8bc4\u4f30\u7814\u7a76\u3002"}}
{"id": "2506.10117", "categories": ["cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.10117", "abs": "https://arxiv.org/abs/2506.10117", "authors": ["Klim Kireev", "Ana-Maria Cre\u0163u", "Raphael Meier", "Sarah Adel Bargal", "Elissa Redmiles", "Carmela Troncoso"], "title": "A Manually Annotated Image-Caption Dataset for Detecting Children in the Wild", "comment": "14 pages, 6 figures", "summary": "Platforms and the law regulate digital content depicting minors (defined as\nindividuals under 18 years of age) differently from other types of content.\nGiven the sheer amount of content that needs to be assessed, machine\nlearning-based automation tools are commonly used to detect content depicting\nminors. To our knowledge, no dataset or benchmark currently exists for\ndetecting these identification methods in a multi-modal environment. To fill\nthis gap, we release the Image-Caption Children in the Wild Dataset (ICCWD), an\nimage-caption dataset aimed at benchmarking tools that detect depictions of\nminors. Our dataset is richer than previous child image datasets, containing\nimages of children in a variety of contexts, including fictional depictions and\npartially visible bodies. ICCWD contains 10,000 image-caption pairs manually\nlabeled to indicate the presence or absence of a child in the image. To\ndemonstrate the possible utility of our dataset, we use it to benchmark three\ndifferent detectors, including a commercial age estimation system applied to\nimages. Our results suggest that child detection is a challenging task, with\nthe best method achieving a 75.3% true positive rate. We hope the release of\nour dataset will aid in the design of better minor detection methods in a wide\nrange of scenarios.", "AI": {"tldr": "\u8bba\u6587\u53d1\u5e03\u4e86\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\u96c6ICCWD\uff0c\u7528\u4e8e\u68c0\u6d4b\u56fe\u50cf\u4e2d\u7684\u672a\u6210\u5e74\u4eba\uff0c\u5e76\u8868\u660e\u513f\u7ae5\u68c0\u6d4b\u662f\u4e00\u4e2a\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002", "motivation": "\u5e73\u53f0\u548c\u6cd5\u5f8b\u5bf9\u672a\u6210\u5e74\u4eba\uff08\u5b9a\u4e49\u4e3a18\u5c81\u4ee5\u4e0b\u7684\u4e2a\u4eba\uff09\u7684\u5185\u5bb9\u4e0e\u5176\u4ed6\u7c7b\u578b\u7684\u5185\u5bb9\u7684\u76d1\u7ba1\u6709\u6240\u4e0d\u540c\u3002\u9274\u4e8e\u9700\u8981\u8bc4\u4f30\u7684\u5185\u5bb9\u91cf\u5de8\u5927\uff0c\u901a\u5e38\u4f7f\u7528\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u5de5\u5177\u6765\u68c0\u6d4b\u63cf\u7ed8\u672a\u6210\u5e74\u4eba\u7684\u5185\u5bb9\u3002\u8be5\u8bba\u6587\u65e8\u5728\u586b\u8865\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u6ca1\u6709\u7528\u4e8e\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\u68c0\u6d4b\u8fd9\u4e9b\u8bc6\u522b\u65b9\u6cd5\u7684\u7a7a\u767d\u3002", "method": "\u8be5\u8bba\u6587\u53d1\u5e03\u4e86\u540d\u4e3aImage-Caption Children in the Wild Dataset (ICCWD) \u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u68c0\u6d4b\u672a\u6210\u5e74\u4eba\u63cf\u7ed8\u7684\u5de5\u5177\u3002\u6570\u636e\u96c6\u5305\u542b10,000\u4e2a\u56fe\u50cf-\u6807\u9898\u5bf9\uff0c\u624b\u52a8\u6807\u6ce8\u4ee5\u6307\u793a\u56fe\u50cf\u4e2d\u662f\u5426\u5b58\u5728\u513f\u7ae5\u3002", "result": "\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u5bf9\u4e09\u79cd\u4e0d\u540c\u7684\u68c0\u6d4b\u5668\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec\u5e94\u7528\u4e8e\u56fe\u50cf\u7684\u5546\u4e1a\u5e74\u9f84\u4f30\u7b97\u7cfb\u7edf\u3002\u7ed3\u679c\u663e\u793a\uff0c\u513f\u7ae5\u68c0\u6d4b\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u6700\u597d\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e8675.3%\u7684\u771f\u6b63\u4f8b\u7387\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u7684\u53d1\u5e03\u6709\u52a9\u4e8e\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u672a\u6210\u5e74\u4eba\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u9002\u7528\u4e8e\u5404\u79cd\u573a\u666f\u3002"}}
{"id": "2506.10077", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.10077", "abs": "https://arxiv.org/abs/2506.10077", "authors": ["Christopher J. Agostino", "Quan Le Thien", "Molly Apsel", "Denizhan Pak", "Elina Lesyk", "Ashabari Majumdar"], "title": "A quantum semantic framework for natural language processing", "comment": "12 pages, 2 figures, accepted submission to Quantum AI and NLP 2025", "summary": "Semantic degeneracy represents a fundamental property of natural language\nthat extends beyond simple polysemy to encompass the combinatorial explosion of\npotential interpretations that emerges as semantic expressions increase in\ncomplexity. Large Language Models (LLMs) and other modern NLP systems face\ninherent limitations precisely because they operate within natural language\nitself, making them subject to the same interpretive constraints imposed by\nsemantic degeneracy. In this work, we argue using Kolmogorov complexity that as\nan expression's complexity grows, the likelihood of any interpreting agent\n(human or LLM-powered AI) recovering the single intended meaning vanishes. This\ncomputational intractability suggests the classical view that linguistic forms\npossess meaning in and of themselves is flawed. We alternatively posit that\nmeaning is instead actualized through an observer-dependent interpretive act.\nTo test this, we conducted a semantic Bell inequality test using diverse LLM\nagents as ``computational cognitive systems'' to interpret ambiguous word pairs\nunder varied contextual settings. Across several independent experiments, we\nfound average CHSH expectation values ranging from 1.2 to 2.8, with several\nruns yielding values (e.g., 2.3-2.4) that significantly violate the classical\nboundary ($|S|\\leq2$). This demonstrates that linguistic interpretation under\nambiguity can exhibit non-classical contextuality, consistent with results from\nhuman cognition experiments. These results inherently imply that classical\nfrequentist-based analytical approaches for natural language are necessarily\nlossy. Instead, we propose that Bayesian-style repeated sampling approaches can\nprovide more practically useful and appropriate characterizations of linguistic\nmeaning in context.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u81ea\u7136\u8bed\u8a00\u4e2d\u7684\u8bed\u4e49\u9000\u5316\u73b0\u8c61\uff0c\u6307\u51fa\u5176\u5bf9\u4e8e\u8bed\u8a00\u6a21\u578b\u548cNLP\u7cfb\u7edf\u89e3\u91ca\u80fd\u529b\u7684\u9650\u5236\u3002\u901a\u8fc7\u8bed\u4e49\u8d1d\u5c14\u4e0d\u7b49\u5f0f\u6d4b\u8bd5\uff0c\u5f97\u51fa\u8bed\u8a00\u89e3\u91ca\u5728\u6a21\u68f1\u4e24\u53ef\u7684\u4e0a\u4e0b\u6587\u4e2d\u8868\u73b0\u51fa\u975e\u7ecf\u5178\u7684\u4e0a\u4e0b\u6587\u6027\uff0c\u610f\u5473\u7740\u4f20\u7edf\u7684\u57fa\u4e8e\u9891\u7387\u7684\u65b9\u6cd5\u5728\u5206\u6790\u81ea\u7136\u8bed\u8a00\u65f6\u4f1a\u4e22\u5931\u4fe1\u606f\u3002\u4f5c\u8005\u5efa\u8bae\u91c7\u7528\u8d1d\u53f6\u65af\u91cd\u590d\u91c7\u6837\u7684\u65b9\u6cd5\u6765\u66f4\u597d\u63cf\u8ff0\u8bed\u8a00\u610f\u4e49\u3002", "motivation": "\u63a2\u8ba8\u81ea\u7136\u8bed\u8a00\u7684\u8bed\u4e49\u9000\u5316\u73b0\u8c61\u5bf9\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5176\u4ed6\u73b0\u4ee3NLP\u7cfb\u7edf\u6765\u8bf4\uff0c\u6784\u6210\u4e86\u56fa\u6709\u7684\u5c40\u9650\u6027\u3002\u56e0\u4e3a\u5b83\u4eec\u64cd\u4f5c\u5728\u81ea\u7136\u8bed\u8a00\u4e4b\u5185\uff0c\u540c\u6837\u53d7\u5230\u8bed\u4e49\u9000\u5316\u5e26\u6765\u7684\u89e3\u91ca\u9650\u5236\u3002", "method": "\u901a\u8fc7Kolmogorov\u590d\u6742\u6027\u7406\u8bba\u8bba\u8bc1\uff0c\u968f\u7740\u8868\u8fbe\u590d\u6742\u5ea6\u7684\u589e\u52a0\uff0c\u4efb\u4f55\u89e3\u91ca\u4ee3\u7406\uff08\u4eba\u7c7b\u6216LLM\u9a71\u52a8\u7684AI\uff09\u6062\u590d\u5355\u4e00\u9884\u671f\u542b\u4e49\u7684\u53ef\u80fd\u6027\u4f1a\u6d88\u5931\u3002\u4e3a\u6b64\uff0c\u8fdb\u884c\u4e86\u8bed\u4e49\u8d1d\u5c14\u4e0d\u7b49\u5f0f\u6d4b\u8bd5\uff0c\u4f7f\u7528\u4e0d\u540cLLM\u4ee3\u7406\u4f5c\u4e3a\u2018\u8ba1\u7b97\u8ba4\u77e5\u7cfb\u7edf\u2019\u6765\u89e3\u91ca\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u8bbe\u7f6e\u4e0b\u7684\u6a21\u68f1\u4e24\u53ef\u7684\u8bcd\u5bf9\u3002", "result": "\u5728\u591a\u4e2a\u72ec\u7acb\u5b9e\u9a8c\u4e2d\uff0c\u5e73\u5747CHSH\u671f\u671b\u503c\u8303\u56f4\u4ece1.2\u52302.8\uff0c\u6709\u4e9b\u8fd0\u884c\u7ed3\u679c\uff08\u4f8b\u59822.3-2.4\uff09\u663e\u8457\u8fdd\u53cd\u4e86\u7ecf\u5178\u754c\u9650(|S|\u22642)\u3002\u8fd9\u8868\u660e\u5728\u6a21\u68f1\u4e24\u53ef\u662f\u7684\u8bed\u5883\u4e0b\uff0c\u8bed\u8a00\u89e3\u91ca\u53ef\u4ee5\u8868\u73b0\u51fa\u975e\u7ecf\u5178\u7684\u4e0a\u4e0b\u6587\u6027\uff0c\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u5b9e\u9a8c\u7684\u7ed3\u679c\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7531\u4e8e\u8bed\u4e49\u9000\u5316\u73b0\u8c61\u7684\u5b58\u5728\uff0c\u81ea\u7136\u8bed\u8a00\u7684\u610f\u4e49\u4e0d\u662f\u56fa\u5b9a\u7684\uff0c\u800c\u662f\u901a\u8fc7\u89c2\u5bdf\u8005\u4f9d\u8d56\u7684\u89e3\u91ca\u884c\u4e3a\u5b9e\u73b0\u7684\u3002\u56e0\u6b64\uff0c\u63d0\u51fa\u4f7f\u7528\u8d1d\u53f6\u65af\u91c7\u6837\u65b9\u6cd5\u6765\u66f4\u9002\u5408\u5730\u8868\u5f81\u8bed\u8a00\u610f\u4e49\u3002"}}
{"id": "2506.10119", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10119", "abs": "https://arxiv.org/abs/2506.10119", "authors": ["Natanael Lucena", "F\u00e1bio S. da Silva", "Ricardo Rios"], "title": "Detec\u00e7\u00e3o da Psor\u00edase Utilizando Vis\u00e3o Computacional: Uma Abordagem Comparativa Entre CNNs e Vision Transformers", "comment": "12 pages, in Portuguese language, 2 figures, 2 tables, and 4\n  formulas. To be published in the Proceedings of the LII Brazilian Integrated\n  Software and Hardware Seminar 2025 (SEMISH 2025)", "summary": "This paper presents a comparison of the performance of Convolutional Neural\nNetworks (CNNs) and Vision Transformers (ViTs) in the task of multi-classifying\nimages containing lesions of psoriasis and diseases similar to it. Models\npre-trained on ImageNet were adapted to a specific data set. Both achieved high\npredictive metrics, but the ViTs stood out for their superior performance with\nsmaller models. Dual Attention Vision Transformer-Base (DaViT-B) obtained the\nbest results, with an f1-score of 96.4%, and is recommended as the most\nefficient architecture for automated psoriasis detection. This article\nreinforces the potential of ViTs for medical image classification tasks.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6bd4\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u89c6\u89c9\u53d8\u6362\u5668\uff08ViT\uff09\u5728\u591a\u5206\u7c7b\u94f6\u5c51\u75c5\u53ca\u5176\u7c7b\u4f3c\u75be\u75c5\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u6a21\u578b\u5728ImageNet\u4e0a\u9884\u8bad\u7ec3\uff0c\u5e76\u9002\u5e94\u7279\u5b9a\u6570\u636e\u96c6\u3002ViT\u56e0\u5176\u5728\u8f83\u5c0f\u6a21\u578b\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u800c\u7a81\u51fa\u3002\u53cc\u6ce8\u610f\u529b\u89c6\u89c9\u53d8\u6362\u5668-\u57fa\u7840\uff08DaViT-B\uff09\u6548\u679c\u6700\u4f73\uff0cF1\u5206\u6570\u4e3a96.4%\uff0c\u63a8\u8350\u4e3a\u81ea\u52a8\u94f6\u5c51\u75c5\u68c0\u6d4b\u6700\u6709\u6548\u7684\u67b6\u6784\u3002", "motivation": "\u63a2\u8ba8\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u4e0e\u89c6\u89c9\u53d8\u6362\u5668\uff08ViT\uff09\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u76f8\u5bf9\u6027\u80fd\uff0c\u7279\u522b\u662f\u9488\u5bf9\u94f6\u5c51\u75c5\u53ca\u5176\u76f8\u4f3c\u75be\u75c5\u7684\u81ea\u52a8\u68c0\u6d4b\u3002", "method": "\u91c7\u7528\u9884\u8bad\u7ec3\u5728ImageNet\u4e0a\u7684CNN\u548cViT\u6a21\u578b\uff0c\u5e76\u9002\u5e94\u7279\u5b9a\u7684\u76ae\u80a4\u75c5\u56fe\u50cf\u6570\u636e\u96c6\u3002\u6d4b\u8bd5\u76f8\u5bf9\u6027\u80fd\uff0c\u6bd4\u8f83\u5404\u7c7b\u6a21\u578b\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u4e24\u79cd\u6a21\u578b\u90fd\u53d6\u5f97\u4e86\u8f83\u9ad8\u7684\u9884\u6d4b\u6307\u6807\uff0c\u4f46ViT\u4ee5\u5176\u66f4\u5c0f\u7684\u6a21\u578b\u5c3a\u5bf8\u5b9e\u73b0\u4e86\u66f4\u4f18\u6548\u679c\u3002\u5176\u4e2d\uff0c\u53cc\u6ce8\u610f\u529b\u89c6\u89c9\u53d8\u6362\u5668-\u57fa\u7840\uff08DaViT-B\uff09\u8fbe\u5230\u4e8696.4%\u7684F1\u5206\u6570\uff0c\u6027\u80fd\u6700\u4f73\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cViT\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5982\u94f6\u5c51\u75c5\u8fd9\u7c7b\u76ae\u80a4\u75c5\u56fe\u50cf\u7684\u81ea\u52a8\u68c0\u6d4b\u4e2d\uff0c\u5c55\u793a\u4e86\u4f18\u4e8eCNN\u7684\u6027\u80fd\u3002"}}
{"id": "2506.10086", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.10086", "abs": "https://arxiv.org/abs/2506.10086", "authors": ["Christodoulos Constantinides", "Shuxin Lin", "Nianjun Zhou", "Dhaval Patel"], "title": "Chat-of-Thought: Collaborative Multi-Agent System for Generating Domain Specific Information", "comment": null, "summary": "This paper presents a novel multi-agent system called Chat-of-Thought,\ndesigned to facilitate the generation of Failure Modes and Effects Analysis\n(FMEA) documents for industrial assets. Chat-of-Thought employs multiple\ncollaborative Large Language Model (LLM)-based agents with specific roles,\nleveraging advanced AI techniques and dynamic task routing to optimize the\ngeneration and validation of FMEA tables. A key innovation in this system is\nthe introduction of a Chat of Thought, where dynamic, multi-persona-driven\ndiscussions enable iterative refinement of content. This research explores the\napplication domain of industrial equipment monitoring, highlights key\nchallenges, and demonstrates the potential of Chat-of-Thought in addressing\nthese challenges through interactive, template-driven workflows and\ncontext-aware agent collaboration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChat-of-Thought\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u4f18\u5316\u5de5\u4e1a\u8d44\u4ea7\u7684FMEA\u6587\u6863\u751f\u6210\u8fc7\u7a0b\u3002\u7cfb\u7edf\u901a\u8fc7\u591a\u89d2\u8272\u9a71\u52a8\u7684\u8ba8\u8bba\u548c\u9ad8\u7ea7AI\u6280\u672f\u63d0\u9ad8\u4e86\u6587\u6863\u751f\u6210\u548c\u9a8c\u8bc1\u7684\u6548\u7387\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5de5\u4e1a\u8bbe\u5907\u76d1\u6d4b\u7684\u5e94\u7528\u9886\u57df\uff0c\u7a81\u51fa\u5173\u952e\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u4ea4\u4e92\u5f0f\u3001\u6a21\u677f\u9a71\u52a8\u7684\u5de5\u4f5c\u6d41\u7a0b\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4ee3\u7406\u534f\u4f5c\uff0c\u5c55\u793aChat-of-Thought\u7cfb\u7edf\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChat-of-Thought\u7684\u65b0\u9896\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u65e8\u5728\u4e3a\u5de5\u4e1a\u8d44\u4ea7\u751f\u6210\u6545\u969c\u6a21\u5f0f\u4e0e\u5f71\u54cd\u5206\u6790\uff08FMEA\uff09\u6587\u6863\u3002\u8be5\u7cfb\u7edf\u91c7\u7528\u591a\u4e2a\u5177\u6709\u7279\u5b9a\u89d2\u8272\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\uff0c\u5e76\u5229\u7528\u9ad8\u7ea7AI\u6280\u672f\u548c\u52a8\u6001\u4efb\u52a1\u8def\u7531\u6765\u4f18\u5316FMEA\u8868\u683c\u7684\u751f\u6210\u548c\u9a8c\u8bc1\u3002\u5176\u4e2d\u4e00\u4e2a\u5173\u952e\u521b\u65b0\u70b9\u662f\u63d0\u51fa\u4e86\u57fa\u4e8e\u52a8\u6001\u591a\u89d2\u8272\u9a71\u52a8\u8ba8\u8bba\u7684\u601d\u8003\u5bf9\u8bdd\uff08Chat of Thought\uff09\u673a\u5236\uff0c\u4f7f\u5185\u5bb9\u80fd\u591f\u901a\u8fc7\u8fed\u4ee3\u65b9\u5f0f\u5f97\u5230\u5b8c\u5584\u3002", "result": "\u867d\u7136\u6458\u8981\u4e2d\u6ca1\u6709\u5177\u4f53\u5217\u51fa\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u7814\u7a76\u8868\u660eChat-of-Thought\u7cfb\u7edf\u80fd\u591f\u901a\u8fc7\u5176\u72ec\u7279\u7684\u591a\u4ee3\u7406\u534f\u4f5c\u67b6\u6784\u548c\u601d\u8003\u5bf9\u8bdd\u673a\u5236\u5728\u751f\u6210FMEA\u6587\u6863\u4e0a\u53d6\u5f97\u663e\u8457\u8fdb\u6b65\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u8868\u660eChat-of-Thought\u7cfb\u7edf\u5728\u89e3\u51b3\u5de5\u4e1a\u76d1\u6d4b\u9886\u57df\u7684\u6311\u6218\u4e0a\u6709\u8f83\u5927\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u751f\u6210FMEA\u6587\u6863\u53ca\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4ee3\u7406\u534f\u4f5c\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u65b9\u9762\u3002"}}
{"id": "2506.10128", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10128", "abs": "https://arxiv.org/abs/2506.10128", "authors": ["Xiyao Wang", "Zhengyuan Yang", "Chao Feng", "Yongyuan Liang", "Yuhang Zhou", "Xiaoyu Liu", "Ziyi Zang", "Ming Li", "Chung-Ching Lin", "Kevin Lin", "Linjie Li", "Furong Huang", "Lijuan Wang"], "title": "ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs", "comment": null, "summary": "Reinforcement learning (RL) has shown great effectiveness for fine-tuning\nlarge language models (LLMs) using tasks that are challenging yet easily\nverifiable, such as math reasoning or code generation. However, extending this\nsuccess to visual perception in vision-language models (VLMs) has been impeded\nby the scarcity of vision-centric tasks that are simultaneously challenging and\nunambiguously verifiable. To this end, we introduce ViCrit (Visual Caption\nHallucination Critic), an RL proxy task that trains VLMs to localize a subtle,\nsynthetic visual hallucination injected into paragraphs of human-written image\ncaptions. Starting from a 200-word captions, we inject a single, subtle visual\ndescription error-altering a few words on objects, attributes, counts, or\nspatial relations-and task the model to pinpoint the corrupted span given the\nimage and the modified caption. This formulation preserves the full perceptual\ndifficulty while providing a binary, exact-match reward that is easy to compute\nand unambiguous. Models trained with the ViCrit Task exhibit substantial gains\nacross a variety of VL benchmarks. Crucially, the improvements transfer beyond\nnatural-image training data to abstract image reasoning and visual math,\nshowing promises of learning to perceive rather than barely memorizing seen\nobjects. To facilitate evaluation, we further introduce ViCrit-Bench, a\ncategory-balanced diagnostic benchmark that systematically probes perception\nerrors across diverse image domains and error types. Together, our results\ndemonstrate that fine-grained hallucination criticism is an effective and\ngeneralizable objective for enhancing visual perception in VLMs.", "AI": {"tldr": "\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aViCrit\u7684\u65b0\u4efb\u52a1\uff0c\u4ee5\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u9ad8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u3002\u8be5\u4efb\u52a1\u5728\u5176\u8bbe\u8ba1\u4e0a\u5b9e\u73b0\u4e86\u611f\u77e5\u96be\u5ea6\u548c\u53ef\u9a8c\u8bc1\u6027\u7684\u5e73\u8861\uff0c\u5e76\u5728\u591a\u4e2a\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u4e86\u5176\u6709\u6548\u6027\u548c\u6cdb\u5316\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u89c6\u89c9\u611f\u77e5\u4efb\u52a1\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u96be\u4ee5\u9a8c\u8bc1\u7684\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u7684\u4efb\u52a1\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u3002", "method": "\u5f15\u5165\u4e86ViCrit\uff08\u89c6\u89c9\u63cf\u8ff0\u5e7b\u89c9\u6279\u8bc4\uff09\uff0c\u4e00\u4e2aRL\u4ee3\u7406\u4efb\u52a1\uff0c\u8be5\u4efb\u52a1\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5b9a\u4f4d\u6bb5\u843d\u4e2d\u4eba\u5de5\u5f15\u5165\u7684\u7ec6\u5c0f\u89c6\u89c9\u5e7b\u89c9\u3002\u901a\u8fc7\u4fee\u6539\u4eba\u7c7b\u7f16\u5199\u7684\u56fe\u50cf\u63cf\u8ff0\u4e2d\u7684\u5355\u8bcd\u3001\u5c5e\u6027\u3001\u6570\u91cf\u6216\u7a7a\u95f4\u5173\u7cfb\uff0c\u4f7f\u6a21\u578b\u5728\u7ed9\u5b9a\u56fe\u50cf\u548c\u4fee\u6539\u540e\u7684\u63cf\u8ff0\u7684\u60c5\u51b5\u4e0b\u627e\u5230\u9519\u8bef\u90e8\u5206\u3002", "result": "\u4f7f\u7528ViCrit\u4efb\u52a1\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u63d0\u5347\uff0c\u8fd9\u79cd\u63d0\u5347\u4e0d\u4ec5\u5c40\u9650\u4e8e\u81ea\u7136\u56fe\u50cf\uff0c\u8fd8\u6269\u5c55\u5230\u4e86\u62bd\u8c61\u56fe\u50cf\u63a8\u7406\u548c\u89c6\u89c9\u6570\u5b66\u4e0a\uff0c\u5c55\u793a\u4e86\u5b66\u4e60\u611f\u77e5\u800c\u975e\u5355\u7eaf\u8bb0\u5fc6\u7684\u6f5c\u529b\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u7684\u5e7b\u89c9\u6279\u8bc4\u662f\u4e00\u4e2a\u6709\u6548\u4e14\u901a\u7528\u7684\u76ee\u6807\uff0c\u53ef\u4ee5\u63d0\u9ad8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u3002"}}
