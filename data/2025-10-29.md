<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 6]
- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Evaluating Long-Term Memory for Long-Context Question Answering](https://arxiv.org/abs/2510.23730)
*Alessandra Terranova,Björn Ross,Alexandra Birch*

Main category: cs.CL

> 研究通过LoCoMo基准测试了不同类型的记忆系统在长时间对话任务中的有效性，发现带有记忆增强的方法可以大幅减少token使用同时保持准确性。小型基础模型从检索增强生成中受益最多，而强大的指令调优推理模型则从回溯和更复杂代理语义记忆中获益。特别是，情景记忆有助于LLM意识到自身知识的局限性。

<details>
  <summary>Details</summary>

**Motivation:** 探索哪种类型的记忆系统能够最有效地提升大语言模型在长时间对话中的表现。

**Method:** 使用LoCoMo基准测试全上下文提示、检索增强生成等几种记忆增强方法。

**Result:** 记忆增强的方法可以减少90%的token使用量，并保持准确性。小型基础模型从检索增强生成中收益最大，而强大的指令调优推理模型则从情景学习与更复杂的代理语义记忆中收益。

**Conclusion:** 记忆增强的方法对于提高大语言模型的效率和准确性是必要的，模型能力越强则应该采用越复杂的记忆架构。情景记忆可以提升LLM自我意识，认识到其知识的局限性。

**Abstract:** In order for large language models to achieve true conversational continuity
and benefit from experiential learning, they need memory. While research has
focused on the development of complex memory systems, it remains unclear which
types of memory are most effective for long-context conversational tasks. We
present a systematic evaluation of memory-augmented methods using LoCoMo, a
benchmark of synthetic long-context dialogues annotated for question-answering
tasks that require diverse reasoning strategies. We analyse full-context
prompting, semantic memory through retrieval-augmented generation and agentic
memory, episodic memory through in-context learning, and procedural memory
through prompt optimization. Our findings show that memory-augmented approaches
reduce token usage by over 90% while maintaining competitive accuracy. Memory
architecture complexity should scale with model capability, with small
foundation models benefitting most from RAG, and strong instruction-tuned
reasoning model gaining from episodic learning through reflections and more
complex agentic semantic memory. In particular, episodic memory can help LLMs
recognise the limits of their own knowledge.

</details>


### [2] [BitSkip: An Empirical Analysis of Quantization and Early Exit Composition](https://arxiv.org/abs/2510.23766)
*Ramshankar Bhuvaneswaran,Handan Liu*

Main category: cs.CL

> BitSkip-V1，一个简单的8位量化模型，没有Hadamard变换，展示出超过复杂模型的性能，甚至能在质量上与全精度基线竞争，同时带来显著的速度提升。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于个体技术如极限量化和动态路由的益处已经得到了充分的文档记录，但它们组合效果的理解依然不足。通过探索这些问题，作者希望增进对该领域的理解。

**Method:** 此论文介绍了BitSkip，一个用于系统性探索这些技术间交互的混合架构框架。它探讨了一个简单的8位量化的模型而没有Hadamard变换（BitSkip-V1）的表现。

**Result:** 研究发现，BitSkip-V1不仅胜过更为复杂的4位和通过Hadamard变换增强的模型，还在质量（困惑度为1.13对1.19）上与全精度基线竞争。引入Hadamard变换，即使是8位精度，也会使性能恶化超过37000%。BitSkip-V1配方展示了出色的早期退出特性，第18层提供最佳的32.5%速度提升，只需4%的质量损失。

**Conclusion:** 论文得出结论，简单8位量化的BitSkip-V1模型在性能上优于更复杂的模型，并展现出优秀的早期退出特性和速度优势，提供了对大型语言模型架构的有趣见解。

**Abstract:** The pursuit of efficient Large Language Models (LLMs) has led to increasingly
complex techniques like extreme quantization and dynamic routing. While
individual benefits of these methods are well-documented, their compositional
effects remain poorly understood. This paper introduces BitSkip, a hybrid
architectural framework for systematically exploring these interactions.
Counter-intuitively, our findings reveal that a simple 8-bit quantized model
without Hadamard transform (BitSkip-V1) not only outperforms its more complex
4-bit and Hadamard-enhanced counterparts but also competes the full-precision
baseline in quality (perplexity of 1.13 vs 1.19) . The introduction of Hadamard
transforms, even at 8-bit precision, catastrophically degraded performance by
over 37,000%, tracing fundamental training instability. Our BitSkip-V1 recipe
demonstrates superior early-exit characteristics, with layer 18 providing
optimal 32.5% speed gain for minimal 4% quality loss.

</details>


### [3] [Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural Processing of Figurative Language](https://arxiv.org/abs/2510.23828)
*Mena Attia,Aashiq Muhamed,Mai Alkhamissi,Thamar Solorio,Mona Diab*

Main category: cs.CL

> 该研究评估了大型语言模型（LLMs）处理具有文化背景的语言的能力，特别是理解和实用寓意表达的能力。在阿拉伯语和英语中设计了上下文理解、实用和象征意义解释的评估任务，并评估了22个开源和闭源LLMs。结果表明LLMs在处理包含文化细微差别的寓意语言方面存在挑战。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是了解大型语言模型在处理包含文化细微差别的语言方面的表现，特别是理解和实用寓意表达的能力。

**Method:** 研究设计了包括上下文理解、实用和象征意义解释在内的评估任务，针对埃及阿拉伯语的成语、多方言阿拉伯语的谚语和英语谚语进行了评估。

**Result:** 结果显示，对于阿拉伯语谚语，准确率比英语谚语低4.29%；对于埃及成语，准确率比阿拉伯谚语低10.28%。在实用任务中，准确率比理解任务低14.07%，但提供上下文成语句子会提高准确率。模型在象征意义理解上的表现有限，与人工注释者最高只能达到85.58%的一致性。

**Conclusion:** 研究发现寓意语言可以用来诊断文化推理能力，尽管LLMs能解释某些寓意的意义，但在使用这些意义上仍面临挑战。研究还发布了Kinayat数据集，为评估埃及阿拉伯语成语的寓意理解与实用提供了支持。

**Abstract:** We present a comprehensive evaluation of the ability of large language models
(LLMs) to process culturally grounded language, specifically to understand and
pragmatically use figurative expressions that encode local knowledge and
cultural nuance. Using figurative language as a proxy for cultural nuance and
local knowledge, we design evaluation tasks for contextual understanding,
pragmatic use, and connotation interpretation in Arabic and English. We
evaluate 22 open- and closed-source LLMs on Egyptian Arabic idioms,
multidialectal Arabic proverbs, and English proverbs. Our results show a
consistent hierarchy: the average accuracy for Arabic proverbs is 4.29% lower
than for English proverbs, and performance for Egyptian idioms is 10.28% lower
than for Arabic proverbs. For the pragmatic use task, accuracy drops by 14.07%
relative to understanding, though providing contextual idiomatic sentences
improves accuracy by 10.66%. Models also struggle with connotative meaning,
reaching at most 85.58% agreement with human annotators on idioms with 100%
inter-annotator agreement. These findings demonstrate that figurative language
serves as an effective diagnostic for cultural reasoning: while LLMs can often
interpret figurative meaning, they face challenges in using it appropriately.
To support future research, we release Kinayat, the first dataset of Egyptian
Arabic idioms designed for both figurative understanding and pragmatic use
evaluation.

</details>


### [4] [How Pragmatics Shape Articulation: A Computational Case Study in STEM ASL Discourse](https://arxiv.org/abs/2510.23842)
*Saki Imai,Lee Kezar,Laurel Aichler,Mert Inan,Erin Walker,Alicia Wooten,Lorna Quandt,Malihe Alikhani*

Main category: cs.CL

> 我们针对ASL中STEM话题对话收集数据，分析了双人互动与独白中手势的时空变化，发现互动手势比独立手势更短，模型在识别STEM词汇时，参与者之间的协调程度有显著变化。

<details>
  <summary>Details</summary>

**Motivation:** 大多数最先进的手语模型都是使用口译员数据或孤立词汇进行训练的，这忽视了自然对话中所具有的变化性。而人类交流会根据上下文和交流者通过时空变化和发音风格进行动态调整。特别是在教育环境中，教师和学生会使用新的词汇，这方面的研究有所欠缺。

**Method:** 我们收集了一个美国手语（ASL）STEM对话的动态捕捉数据集，以实现双人互动性手语、单人讲座和翻译文章之间的定量比较。通过连续的运动特征，我们在重复出现STEM词汇时，将对话特定的反复动作与个人努力的减少分开。

**Result:** 我们发现，平均而言，对话中的手势比孤立的手势短24.6%-44.6%，在独白情景中没有这种显著的减少。评估了手势嵌入模型在识别STEM手势方面的能力，并模拟参与者在其间随着时间变得协调的程度。

**Conclusion:** 这项研究将语言分析和计算建模结合起来，探讨了实际语用如何影响手语的表达以及其在手语技术中的表现形式。

**Abstract:** Most state-of-the-art sign language models are trained on interpreter or
isolated vocabulary data, which overlooks the variability that characterizes
natural dialogue. However, human communication dynamically adapts to contexts
and interlocutors through spatiotemporal changes and articulation style. This
specifically manifests itself in educational settings, where novel vocabularies
are used by teachers, and students. To address this gap, we collect a motion
capture dataset of American Sign Language (ASL) STEM (Science, Technology,
Engineering, and Mathematics) dialogue that enables quantitative comparison
between dyadic interactive signing, solo signed lecture, and interpreted
articles. Using continuous kinematic features, we disentangle dialogue-specific
entrainment from individual effort reduction and show spatiotemporal changes
across repeated mentions of STEM terms. On average, dialogue signs are
24.6%-44.6% shorter in duration than the isolated signs, and show significant
reductions absent in monologue contexts. Finally, we evaluate sign embedding
models on their ability to recognize STEM signs and approximate how entrained
the participants become over time. Our study bridges linguistic analysis and
computational modeling to understand how pragmatics shape sign articulation and
its representation in sign language technologies.

</details>


### [5] [CRADLE Bench: A Clinician-Annotated Benchmark for Multi-Faceted Mental Health Crisis and Safety Risk Detection](https://arxiv.org/abs/2510.23845)
*Grace Byun,Rebecca Lipschutz,Sean T. Minton,Abigail Lott,Jinho D. Choi*

Main category: cs.CL

> 本文提出了CRADLE BENCH，一个涵盖多种危机类型及时间标签的基准测试，通过自动标注的语料库训练模型，并通过微调提供了六种互补的危机检测模型。

<details>
  <summary>Details</summary>

**Motivation:** 本文的动机是解决语言模型在检测如自杀倾向、性侵、家庭暴力、儿童虐待和性骚扰等精神健康危机情况下可靠性不足的问题。如果不能可靠地识别这些危机情况，将会产生严重的后果。

**Method:** 本文引入了CRADLE BENCH，这是一个多方面的危机检测基准，涵盖了七种类型的危机情况，并首次结合了时间标签。为了训练该模型，使用了大约4K个用多种语言模型的多数票集成自动标注的训练语料库，这比单一模型标注的效果更好。

**Result:** 通过对共识和一致同意的子集进行微调，本文提供了六种危机检测模型，它们在不同的同意标准下进行了互补训练，从而提高了危机检测的准确性和可靠性。

**Conclusion:** CRADLE BENCH作为多方面的危机检测基准，通过共识和一致同意机制提供了多种互补的危机检测模型，不仅提升了危机检测的可靠性，还丰富了对危机识别的研究。

**Abstract:** Detecting mental health crisis situations such as suicide ideation, rape,
domestic violence, child abuse, and sexual harassment is a critical yet
underexplored challenge for language models. When such situations arise during
user--model interactions, models must reliably flag them, as failure to do so
can have serious consequences. In this work, we introduce CRADLE BENCH, a
benchmark for multi-faceted crisis detection. Unlike previous efforts that
focus on a limited set of crisis types, our benchmark covers seven types
defined in line with clinical standards and is the first to incorporate
temporal labels. Our benchmark provides 600 clinician-annotated evaluation
examples and 420 development examples, together with a training corpus of
around 4K examples automatically labeled using a majority-vote ensemble of
multiple language models, which significantly outperforms single-model
annotation. We further fine-tune six crisis detection models on subsets defined
by consensus and unanimous ensemble agreement, providing complementary models
trained under different agreement criteria.

</details>


### [6] [Temporal Blindness in Multi-Turn LLM Agents: Misaligned Tool Use vs. Human Time Perception](https://arxiv.org/abs/2510.23853)
*Yize Cheng,Arshia Soltani Moakhar,Chenrui Fan,Kazem Faghih,Parsa Hosseini,Wenxiao Wang,Soheil Feizi*

Main category: cs.CL

> 文章通过设计TicToc-v1测试集研究大型语言模型的时间盲视问题，并发现即使加入了时间戳，多数模型的表现也远未达到理想水平，强调了针对人类时间感知进行特定后训练对齐的需求。

<details>
  <summary>Details</summary>

**Motivation:** 文章探讨了大型语言模型代理在多轮对话中处理时间敏感任务时存在的关键问题——时间盲视，即模型默认操作基于静态上下文，无法根据实际时间流逝做出决策。为研究这一挑战，提出了TicToc-v1测试集，以考察时间信息对于决策的影响。

**Method:** 引入TicToc-v1测试集，涵盖34个对时间敏感的不同场景的多轮对话轨迹。向对话消息添加时间戳，以此提供时间上下文，并根据人类偏好将样本分为两个子集：一个子集表明人类倾向于依赖之前的观察结果，另一个子集则倾向于需要新的工具调用。通过人机偏好对比分析LLM在不同时间间隔下的工具调用决策。

**Result:** 分析结果表明，缺乏时间信息时，多数模型的表现仅略高于随机水平，最佳对齐率仅为60%以上。而增加时间戳虽有助于性能稍有提升，特别是对较大模型而言，但仍达不到显著改善，最佳表现仅能达到65%。简单的基于提示的对齐方式效果有限。

**Conclusion:** 研究结论指出现有模型在动态环境中的工具调用与人类时间感知之间的对齐问题，并强调在多轮对话语言模型训练后进行特定的后训练对齐的必要性。

**Abstract:** Large language model agents are increasingly used in multi-turn
conversational settings to interact with and execute tasks in dynamic
environments. However, a key limitation is their temporal blindness: they, by
default, operate with a stationary context, failing to account for the
real-world time elapsed between messages. This becomes a critical liability
when an agent must decide whether to invoke a tool based on how much time has
passed since the last observation. Without temporal awareness, agents often
either over-rely on previous context (skipping necessary tool calls), or
under-rely on it (unnecessarily repeating tool calls). To study this challenge,
we introduce TicToc-v1, a test set of multi-turn user-agent trajectories across
34 scenarios with varying time sensitivity. Each trajectory ends with a user
question, where the need for a tool call depends on the amount of time elapsed
since the last message. To give LLMs temporal context, we augment dialogue
messages with explicit timestamps, bridging the gap between static dialogue and
evolving environments. We then collected human preferences for these samples,
creating two subsets: one where humans preferred relying on the previous
observation (prefer-noTool), and another where they preferred a new tool call
(prefer-Tool). We evaluated how well LLM tool-calling decisions align with
human preferences under varying time intervals on TicToc-v1. Our analysis show
that without time information, most models perform only slightly better than
random, with the top alignment rate being just over 60%. While adding
timestamps leads to a slight improvement, particularly for larger models, the
improvement is modest, peaking at around 65%. We also show that naive,
prompt-based alignment have limited effectiveness. Our findings highlight the
need for specific post-training alignment to align multi-turn LLM tool use with
human temporal perception.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [7] [Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices](https://arxiv.org/abs/2510.23775)
*Aryan Mathur,Asaduddin Ahmed,Pushti Amit Vasoya,Simeon Kandan Sonar,Yasir Z,Madesh Kuppusamy*

Main category: cs.CV

> 本研究开发了一个用于检测图像真实性的解释系统，结合了轻量级卷积分类器和视觉语言模型，实现了快速准确的伪迹检测与定位。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于AI生成图像现实性提高所带来的验证图像真实性挑战，本研究旨在构建一个可解释的真实度检测系统。

**Method:** 使用轻量级卷积分类器“Faster-Than-Lies”结合视觉语言模型Qwen2-VL-7B来对32x32图像中的伪迹进行分类、定位和解释。使用自编码器重构错误图生成伪迹定位热图以增强可解释性，并将70种视觉伪迹类型分为八大类，为每个检测到的异常生成可解释的文本描述。

**Result:** 该模型在扩展的CiFAKE数据集上获得了96.5%的准确率，此数据集还包括对抗性扰动，并且在8核CPU上的推理时间为175毫秒，支持部署在本地或边缘设备上。

**Conclusion:** 这项工作展示了通过结合视觉和语言推理技术来实现低分辨率图像的真实度检测，并展示了其在法医鉴定、工业检查以及社交媒体监管等跨域应用的潜力。

**Abstract:** The increasing realism of AI-generated imagery poses challenges for verifying
visual authenticity. We present an explainable image authenticity detection
system that combines a lightweight convolutional classifier
("Faster-Than-Lies") with a Vision-Language Model (Qwen2-VL-7B) to classify,
localize, and explain artifacts in 32x32 images. Our model achieves 96.5%
accuracy on the extended CiFAKE dataset augmented with adversarial
perturbations and maintains an inference time of 175ms on 8-core CPUs, enabling
deployment on local or edge devices. Using autoencoder-based reconstruction
error maps, we generate artifact localization heatmaps, which enhance
interpretability for both humans and the VLM. We further categorize 70 visual
artifact types into eight semantic groups and demonstrate explainable text
generation for each detected anomaly. This work highlights the feasibility of
combining visual and linguistic reasoning for interpretable authenticity
detection in low-resolution imagery and outlines potential cross-domain
applications in forensics, industrial inspection, and social media moderation.

</details>


### [8] [CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting](https://arxiv.org/abs/2510.23785)
*Md Tanvir Hossain,Akif Islam,Mohd Ruhul Ameen*

Main category: cs.CV

> 本研究提出了CountFormer模型，采用DINOv2基础模型来改进物体计数，实现了性能卓越的类无关物体计数，尤其在复杂结构场景中表现好。

<details>
  <summary>Details</summary>

**Motivation:** 人类能够通过识别视觉重复和结构关系而非依靠类别身份来轻松计数不同的物体，然而现有的大多数计数模型在面对复杂形状、内部对称性或组件重叠的物体时会计数错误。

**Method:** 引入了CountFormer，这是一个基于Transformer的框架，用于学习识别重复和结构一致性来进行类无关的对象计数。该模型基于CounTR架构，将原视觉编码器替换为自监督基础模型DINOv2，该基础模型可以生成更丰富且空间一致的特征表示。此外，模型还引入位置嵌入融合，在解码为密度图之前保留几何关系，解码采用轻量级卷积解码器。

**Result:** 在FSC-147数据集上进行评估，该模型的表现与当前最先进的方法相当，并且在结构复杂或密集的场景中表现出更高的准确性。

**Conclusion:** 研究表明，使用DINOv2等基础模型的集成可以让计数系统接近人类的结构感知能力，迈向真正普遍且无示例的计数范式。

**Abstract:** Humans can effortlessly count diverse objects by perceiving visual repetition
and structural relationships rather than relying on class identity. However,
most existing counting models fail to replicate this ability; they often
miscount when objects exhibit complex shapes, internal symmetry, or overlapping
components. In this work, we introduce CountFormer, a transformer-based
framework that learns to recognize repetition and structural coherence for
class-agnostic object counting. Built upon the CounTR architecture, our model
replaces its visual encoder with the self-supervised foundation model DINOv2,
which produces richer and spatially consistent feature representations. We
further incorporate positional embedding fusion to preserve geometric
relationships before decoding these features into density maps through a
lightweight convolutional decoder. Evaluated on the FSC-147 dataset, our model
achieves performance comparable to current state-of-the-art methods while
demonstrating superior accuracy on structurally intricate or densely packed
scenes. Our findings indicate that integrating foundation models such as DINOv2
enables counting systems to approach human-like structural perception,
advancing toward a truly general and exemplar-free counting paradigm.

</details>


### [9] [A geometric and deep learning reproducible pipeline for monitoring floating anthropogenic debris in urban rivers using in situ cameras](https://arxiv.org/abs/2510.23798)
*Gauthier Grimmer,Romain Wenger,Clément Flint,Germain Forestier,Gilles Rixhon,Valentin Chardon*

Main category: cs.CV

> 本文提出了一种基于固定摄像头和深度学习的河中漂浮垃圾持续监测框架，并强调了数据集创建的重要性，最终证明了该方法在城市水环境中的可行性。

<details>
  <summary>Details</summary>

**Motivation:** 由于河中漂浮的人为垃圾泛滥成为了一个迫在眉睫的环境问题，对生物多样性、水质和人类活动如航行和娱乐产生了负面影响，因此研究旨在解决这一问题。

**Method:** 本研究提出了一种利用固定在位摄像头和深度学习技术对河水中漂浮垃圾进行持续量化和监测的新框架。同时，本方法还识别出最适合复杂环境的深度学习模型，在准确性和推断速度方面进行了测试。此外，还实现了一个几何模型，从2D图像中估算检测对象的实际大小，并考虑了相机的内在和外在特性。

**Result:** 研究结果强调了数据集构成协议的重要性，特别是关于负图像的集成和时间泄漏的考虑。实验发现了一种结合投影几何和回归修正的可量化的方法用于估算物体的度量。

**Conclusion:** 研究展示了结合投影几何和回归校正进行度量对象估算的可行性，为城市水环境的坚固、低成本和自动化监测系统的发展铺平了道路。

**Abstract:** The proliferation of floating anthropogenic debris in rivers has emerged as a
pressing environmental concern, exerting a detrimental influence on
biodiversity, water quality, and human activities such as navigation and
recreation. The present study proposes a novel methodological framework for the
monitoring the aforementioned waste, utilising fixed, in-situ cameras. This
study provides two key contributions: (i) the continuous quantification and
monitoring of floating debris using deep learning and (ii) the identification
of the most suitable deep learning model in terms of accuracy and inference
speed under complex environmental conditions. These models are tested in a
range of environmental conditions and learning configurations, including
experiments on biases related to data leakage. Furthermore, a geometric model
is implemented to estimate the actual size of detected objects from a 2D image.
This model takes advantage of both intrinsic and extrinsic characteristics of
the camera. The findings of this study underscore the significance of the
dataset constitution protocol, particularly with respect to the integration of
negative images and the consideration of temporal leakage. In conclusion, the
feasibility of metric object estimation using projective geometry coupled with
regression corrections is demonstrated. This approach paves the way for the
development of robust, low-cost, automated monitoring systems for urban aquatic
environments.

</details>


### [10] [RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution of Rare-Earth Features](https://arxiv.org/abs/2510.23816)
*Forouzan Fallah,Wenwen Li,Chia-Yu Hsu,Hyunho Lee,Yezhou Yang*

Main category: cs.CV

> RareFlow是一个物理感知的遥感图像超级分辨率框架，特别针对分布外数据进行了优化。该框架通过门控控制网络保留几何细节，并通过文本提示合成复杂特征。实验结果表明，RareFlow输出的图像保真度接近真值图像，且优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 超级分辨率（SR）在遥感图像下通常在分布外（OOD）条件下表现不佳，尤其是在罕见的地质特征和多样化的传感器数据中，导致生成的图像在视觉上看起来合理但在物理上不准确。

**Method:** RareFlow采用了一种双条件架构，包含一个门控控制网络（Gated ControlNet），用于保留低分辨率输入中的细粒度几何细节，并通过文本提示提供语义引导来合成复杂特征。为了确保物理上的合理性，RareFlow引入了一种多方面的损失函数，保证输出在光谱和辐射度方面与传感器特性保持一致。此外，该框架通过采用随机前向传播方法量化其自身的预测不确定性，输出的方差可以识别不熟悉的输入，减少特征幻觉。

**Result:** 在一项针对多传感器卫星图像的新建立的基准数据集上的验证显示，在盲测中，地球物理专家认为RareFlow生成的输出接近地面真值图像的保真度，明显优于现有的最先进的基线模型。

**Conclusion:** RareFlow为数据稀缺的科学领域提供了一个高保真合成的稳健框架，并为在严重领域偏移下的可控生成提供了新的范式。

**Abstract:** Super-resolution (SR) for remote sensing imagery often fails under
out-of-distribution (OOD) conditions, such as rare geomorphic features captured
by diverse sensors, producing visually plausible but physically inaccurate
results. We present RareFlow, a physics-aware SR framework designed for OOD
robustness. RareFlow's core is a dual-conditioning architecture. A Gated
ControlNet preserves fine-grained geometric fidelity from the low-resolution
input, while textual prompts provide semantic guidance for synthesizing complex
features. To ensure physically sound outputs, we introduce a multifaceted loss
function that enforces both spectral and radiometric consistency with sensor
properties. Furthermore, the framework quantifies its own predictive
uncertainty by employing a stochastic forward pass approach; the resulting
output variance directly identifies unfamiliar inputs, mitigating feature
hallucination. We validate RareFlow on a new, curated benchmark of multi-sensor
satellite imagery. In blind evaluations, geophysical experts rated our model's
outputs as approaching the fidelity of ground truth imagery, significantly
outperforming state-of-the-art baselines. This qualitative superiority is
corroborated by quantitative gains in perceptual metrics, including a nearly
40\% reduction in FID. RareFlow provides a robust framework for high-fidelity
synthesis in data-scarce scientific domains and offers a new paradigm for
controlled generation under severe domain shift.

</details>


### [11] [TRELLISWorld: Training-Free World Generation from Object Generators](https://arxiv.org/abs/2510.23880)
*Hanke Chen,Yuan Liu,Minchen Li*

Main category: cs.CV

> 本文提出了一种基于文本驱动的3D场景生成方法，使用扩散模型作为模块生成器，解决了现有方法的局限性，使得场景可以大规模合成并支持全方位视图。

<details>
  <summary>Details</summary>

**Motivation:** 现有的3D场景生成方法通常受到单一对象生成的限制，需要特定领域的训练，或者不支持360度全方位视图。本研究旨在提出一种无需训练的方法，以改善这些问题。

**Method:** 通过重新定位通用的文本到3D对象扩散模型作为模块化瓷砖生成器，将场景生成重新定义为多瓷砖去噪问题。这种方法可以独立生成和无缝融合重叠的3D区域，从而能够合成大型连贯场景，同时保持局部语义控制。

**Result:** 研究展示了一种支持多样化场景布局、高效生成和灵活编辑的方法，为通用的、语言驱动的3D场景构建奠定了简单而强大的基础。

**Conclusion:** 该方法无需场景级数据集或重新训练，依赖极小的启发式规则，继承了对象级先验的知识，为3D场景构建提供了一种高效灵活的方法。

**Abstract:** Text-driven 3D scene generation holds promise for a wide range of
applications, from virtual prototyping to AR/VR and simulation. However,
existing methods are often constrained to single-object generation, require
domain-specific training, or lack support for full 360-degree viewability. In
this work, we present a training-free approach to 3D scene synthesis by
repurposing general-purpose text-to-3D object diffusion models as modular tile
generators. We reformulate scene generation as a multi-tile denoising problem,
where overlapping 3D regions are independently generated and seamlessly blended
via weighted averaging. This enables scalable synthesis of large, coherent
scenes while preserving local semantic control. Our method eliminates the need
for scene-level datasets or retraining, relies on minimal heuristics, and
inherits the generalization capabilities of object-level priors. We demonstrate
that our approach supports diverse scene layouts, efficient generation, and
flexible editing, establishing a simple yet powerful foundation for
general-purpose, language-driven 3D scene construction.

</details>


### [12] [Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2510.23894)
*Jinxin Zhou,Jiachen Jiang,Zhihui Zhu*

Main category: cs.CV

> LHT-CLIP 通过分析CLIP模型的层、头部和标记级别视觉区分度，提出了无训练方法改善语义分割性能，达到新标杆。

<details>
  <summary>Details</summary>

**Motivation:** 解决CLIP模型在语义分割中的表现欠佳，这归因于其在图像级别预训练目标和像素级别视觉理解之间的错位。旨在克服现有方法由于继承了之前层的全局对齐偏差而导致的分割性能不佳的问题。

**Method:** 本文提出了一种名为LHT-CLIP的无训练框架，该框架通过综合分析揭示了三个关键见解：(i) 最终层加强了图像-文本对齐但牺牲了视觉区分度；(ii) 注意力头的子集表现出一致的强视觉区分度；(iii) 异常标记表现出稀疏且一致的激活模式。基于这些发现，提出了三种互补技术：语义空间重加权，选择性头部增强和异常标记替换，以有效恢复视觉区分度并提高分割性能。

**Result:** 通过无需额外训练或调整大量超参数的策略，LHT-CLIP系统性地提升了视觉区分度，并显著提高分割效果，在多样化的情况下取得了最佳性能。

**Conclusion:** 实验表明，LHT-CLIP在8个常见语义分割基准测试中取得了最先进的性能，展示其实用性和有效性，具有实际部署潜力。

**Abstract:** Extending CLIP models to semantic segmentation remains challenging due to the
misalignment between their image-level pre-training objectives and the
pixel-level visual understanding required for dense prediction. While prior
efforts have achieved encouraging results by reorganizing the final layer and
features, they often inherit the global alignment bias of preceding layers,
leading to suboptimal segmentation performance. In this work, we propose
LHT-CLIP, a novel training-free framework that systematically exploits the
visual discriminability of CLIP across layer, head, and token levels. Through
comprehensive analysis, we reveal three key insights: (i) the final layers
primarily strengthen image-text alignment with sacrifice of visual
discriminability (e.g., last 3 layers in ViT-B/16 and 8 layers in ViT-L/14),
partly due to the emergence of anomalous tokens; (ii) a subset of attention
heads (e.g., 10 out of 144 in ViT-B/16) display consistently strong visual
discriminability across datasets; (iii) abnormal tokens display sparse and
consistent activation pattern compared to normal tokens. Based on these
findings, we propose three complementary techniques: semantic-spatial
reweighting, selective head enhancement, and abnormal token replacement to
effectively restore visual discriminability and improve segmentation
performance without any additional training, auxiliary pre-trained networks, or
extensive hyperparameter tuning. Extensive experiments on 8 common semantic
segmentation benchmarks demonstrate that LHT-CLIP achieves state-of-the-art
performance across diverse scenarios, highlighting its effectiveness and
practicality for real-world deployment.

</details>


### [13] [DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning](https://arxiv.org/abs/2510.23907)
*Eddison Pham,Prisha Priyadarshini,Adrian Maliackel,Kanishk Bandi,Cristian Meo,Kevin Zhu*

Main category: cs.CV

> DynaStride是用于生成连贯场景级别字幕的流水线，无需手动场景分割。它可以自适应地捕捉关键过渡，并生成更有时间连贯性和信息量的字幕，从而提升教学内容的生成效果。

<details>
  <summary>Details</summary>

**Motivation:** 场景级别的字幕生成可以增强教学视频的学习效果，因为这需要理解视觉线索和时间结构。然而，如果字幕不能够捕获这些结构，可能会导致缺乏连贯性和质量，进而产生困惑并削弱视频的教育意图。为了填补这一空白，提出了DynaStride。

**Method:** 通过使用YouCookII数据集的场景注释，DynaStride执行自适应帧采样和多模态窗口化，以捕捉每个场景内的关键过渡。接着，采用多模态思维链过程生成多个动作-对象对，然后通过动态步幅窗口选择算法对这些对进行优化和融合，从而实现视觉语义和时间推理的一体化说明性字幕生成。

**Result:** 实证评估结果显示，与强大的基准模型相比，DynaStride在N-gram指标和语义相似性度量上均有显著提高。定性分析表明，DynaStride生成的字幕在时间连贯性和信息量方面都有优势。

**Conclusion:** DynaStride提供了一种提高AI生成的教学内容质量的新方向，为未来的研究指出了一个有希望的路径。

**Abstract:** Scene-level captioning in instructional videos can enhance learning by
requiring an understanding of both visual cues and temporal structure. By
aligning visual cues with textual guidance, this understanding supports
procedural learning and multimodal reasoning, providing a richer context for
skill acquisition. However, captions that fail to capture this structure may
lack coherence and quality, which can create confusion and undermine the
video's educational intent. To address this gap, we introduce DynaStride, a
pipeline to generate coherent, scene-level captions without requiring manual
scene segmentation. Using the YouCookII dataset's scene annotations, DynaStride
performs adaptive frame sampling and multimodal windowing to capture key
transitions within each scene. It then employs a multimodal chain-of-thought
process to produce multiple action-object pairs, which are refined and fused
using a dynamic stride window selection algorithm that adaptively balances
temporal context and redundancy. The final scene-level caption integrates
visual semantics and temporal reasoning in a single instructional caption.
Empirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,
demonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and
semantic similarity measures (BERTScore, CLIPScore). Qualitative analyses
further show that DynaStride produces captions that are more temporally
coherent and informative, suggesting a promising direction for improving
AI-powered instructional content generation.

</details>


### [14] [TurboPortrait3D: Single-step diffusion-based fast portrait novel-view synthesis](https://arxiv.org/abs/2510.23929)
*Emily Kim,Julieta Martinez,Timur Bagautdinov,Jessica Hodgins*

Main category: cs.CV

> 本文介绍了TurboPortrait3D，一种用于人像3D新视角合成的方法，结合图像扩散模型，以提升生成的3D表示的质量同时保持低延迟。

<details>
  <summary>Details</summary>

**Motivation:** 现有的图像到3D模型虽然可以生成可渲染的3D表示，但是往往存在视觉伪影、细节不足和难以完全保留主体身份的缺陷。而图像扩散模型虽然可以生成高质量的图像，但由于计算成本高，且不具备3D基础，不能直接产生多视角一致的输出。

**Method:** 我们的方法名为TurboPortrait3D，用于生成3D人像的新视角合成。该方法利用图像扩散模型提升现有的图像到3D模型质量，同时保持低延迟和三维感知能力。此方法首先通过单个正脸图像输入，应用图像至3D头像生成管道获取初始的3D表示及相应的噪点渲染结果。接着，我们将这些噪点渲染输入给一个扩散模型，该模型以输入图像为条件，专门训练用于以多视角一致的方式修正渲染结果。此外，我们提出了一种新的训练策略，包括在大量合成多视角数据上预训练，然后在高质量真实图像上进行微调。

**Result:** 实验结果表明，我们的方法在人像新视角合成中，无论是在定性分析还是定量分析上，都有优异的表现，并且在效率上也有提升。

**Conclusion:** 结论部分指出，通过此方法，我们能够以高质量和低延迟的方式生成高质量的3D人像多视角合成，并显示出优于当前技术水平的结果。

**Abstract:** We introduce TurboPortrait3D: a method for low-latency novel-view synthesis
of human portraits. Our approach builds on the observation that existing
image-to-3D models for portrait generation, while capable of producing
renderable 3D representations, are prone to visual artifacts, often lack of
detail, and tend to fail at fully preserving the identity of the subject. On
the other hand, image diffusion models excel at generating high-quality images,
but besides being computationally expensive, are not grounded in 3D and thus
are not directly capable of producing multi-view consistent outputs. In this
work, we demonstrate that image-space diffusion models can be used to
significantly enhance the quality of existing image-to-avatar methods, while
maintaining 3D-awareness and running with low-latency. Our method takes a
single frontal image of a subject as input, and applies a feedforward
image-to-avatar generation pipeline to obtain an initial 3D representation and
corresponding noisy renders. These noisy renders are then fed to a single-step
diffusion model which is conditioned on input image(s), and is specifically
trained to refine the renders in a multi-view consistent way. Moreover, we
introduce a novel effective training strategy that includes pre-training on a
large corpus of synthetic multi-view data, followed by fine-tuning on
high-quality real images. We demonstrate that our approach both qualitatively
and quantitatively outperforms current state-of-the-art for portrait novel-view
synthesis, while being efficient in time.

</details>


### [15] [PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors](https://arxiv.org/abs/2510.23930)
*Xirui Jin,Renbiao Jin,Boying Li,Danping Zou,Wenxian Yu*

Main category: cs.CV

> 提出了一种新的室内场景三维表面重建方法PlanarGS，该方法基于3DGS，通过引入平面和几何先验监督，解决了大量低纹理区域导致的几何重建不稳定问题，实验表明方法优于现有最佳方法。

<details>
  <summary>Details</summary>

**Motivation:** 三维高斯喷点（3DGS）作为新颖视图合成的有效表示方法，已经实现了令人印象深刻的视觉质量，但在以大面积低纹理区域为主导的场景中，如室内环境，常用的光度损失用于优化3DGS时会导致几何模糊并妨碍高质量3D表面的恢复。

**Method:** 我们的方法PlanarGS是一个基于3D高斯喷点（3DGS）的框架，专为室内场景重建设计。具体来说，我们设计了一个用于语言提示平面先验（LP3）的流程，该流程利用预训练的视觉-语言分割模型，并通过多视角融合和几何先验来精炼区域建议。框架中的3D高斯点通过额外的两项进行优化：一项是平面先验监督，以确保平面一致性；另一项是几何先验监督，引导高斯点朝向深度和法线线索调整。

**Result:** 我们在标准的室内数据集上进行了广泛实验，结果显示PlanarGS能够重建精确且详细的3D表面，相较于现有方法，整体性能有显著的提高。

**Conclusion:** 我们的工作引入了一种新的3DGS框架（PlanarGS），该框架解决了室内环境中大面积低纹理区域难以精确重建的问题。通过引入平面和几何先验监督，我们的方法在重建3D表面时表现出更高的精度和细节度，达标记现时最佳方法的性能。

**Abstract:** Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an
efficient representation for novel-view synthesis, achieving impressive visual
quality. However, in scenes dominated by large and low-texture regions, common
in indoor environments, the photometric loss used to optimize 3DGS yields
ambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome
this limitation, we introduce PlanarGS, a 3DGS-based framework tailored for
indoor scene reconstruction. Specifically, we design a pipeline for
Language-Prompted Planar Priors (LP3) that employs a pretrained vision-language
segmentation model and refines its region proposals via cross-view fusion and
inspection with geometric priors. 3D Gaussians in our framework are optimized
with two additional terms: a planar prior supervision term that enforces planar
consistency, and a geometric prior supervision term that steers the Gaussians
toward the depth and normal cues. We have conducted extensive experiments on
standard indoor benchmarks. The results show that PlanarGS reconstructs
accurate and detailed 3D surfaces, consistently outperforming state-of-the-art
methods by a large margin. Project page: https://planargs.github.io

</details>


### [16] [Adaptive Training of INRs via Pruning and Densification](https://arxiv.org/abs/2510.23943)
*Diana Aldana,João Paulo Lima,Daniel Csillag,Daniel Perazzo,Haoan Feng,Luiz Velho,Tiago Novello*

Main category: cs.CV

> AIRe提出了一种自适应训练方案来改进隐式神经表示(INRs)，该方案通过神经元剪枝和输入频率密集化来优化模型结构，减少参数冗余并提高重建质量。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法在选择输入频率和架构以及管理参数冗余方面依赖于启发式方法和繁重的超参数优化，本文旨在通过AIRe自适应训练方案解决这些问题。

**Method:** AIRe采用自适应训练方案，通过神经元剪枝机制避免冗余，并通过输入频率密集化提升表示能力。剪枝阶段识别对模型贡献较小的神经元，并通过有针对性的权重衰减将它们的信息转移到剩余的神经元上，然后进行结构化剪枝。密集化阶段则在信号拟合不足的频谱区域增加输入频率，扩展表示基础。

**Result:** 实验结果表明，AIRe在图像和SDF(Signed Distance Fields)上减少模型大小的同时保持或改进了重建质量。

**Conclusion:** AIRe通过改进的训练方案，不仅减少了模型大小，而且保持或提升了重建质量，因此是一种有效的隐式神经表示方法。

**Abstract:** Encoding input coordinates with sinusoidal functions into multilayer
perceptrons (MLPs) has proven effective for implicit neural representations
(INRs) of low-dimensional signals, enabling the modeling of high-frequency
details. However, selecting appropriate input frequencies and architectures
while managing parameter redundancy remains an open challenge, often addressed
through heuristics and heavy hyperparameter optimization schemes. In this
paper, we introduce AIRe ($\textbf{A}$daptive $\textbf{I}$mplicit neural
$\textbf{Re}$presentation), an adaptive training scheme that refines the INR
architecture over the course of optimization. Our method uses a neuron pruning
mechanism to avoid redundancy and input frequency densification to improve
representation capacity, leading to an improved trade-off between network size
and reconstruction quality. For pruning, we first identify less-contributory
neurons and apply a targeted weight decay to transfer their information to the
remaining neurons, followed by structured pruning. Next, the densification
stage adds input frequencies to spectrum regions where the signal underfits,
expanding the representational basis. Through experiments on images and SDFs,
we show that AIRe reduces model size while preserving, or even improving,
reconstruction quality. Code and pretrained models will be released for public
use.

</details>
