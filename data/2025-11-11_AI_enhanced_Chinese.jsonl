{"id": "2511.05516", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.05516", "abs": "https://arxiv.org/abs/2511.05516", "authors": ["Canxiang Yan", "Chunxiang Jin", "Dawei Huang", "Haibing Yu", "Han Peng", "Hui Zhan", "Jie Gao", "Jing Peng", "Jingdong Chen", "Jun Zhou", "Kaimeng Ren", "Ming Yang", "Mingxue Yang", "Qiang Xu", "Qin Zhao", "Ruijie Xiong", "Shaoxiong Lin", "Xuezhi Wang", "Yi Yuan", "Yifei Wu", "Yongjie Lyu", "Zhengyu He", "Zhihao Qiu", "Zhiqiang Fang", "Ziyuan Huang"], "title": "Ming-UniAudio: Speech LLM for Joint Understanding, Generation and Editing with Unified Representation", "comment": "32 pages, 8 figures", "summary": "Existing speech models suffer from competing requirements on token representations by understanding and generation tasks. This discrepancy in representation prevents speech language models from performing instruction-based free-form editing. To solve this challenge, we introduce a novel framework that unifies speech understanding, generation, and editing. The core of our unified model is a unified continuous speech tokenizer MingTok-Audio, the first continuous tokenizer to effectively integrate semantic and acoustic features, which makes it suitable for both understanding and generation tasks. Based on this unified continuous audio tokenizer, we developed the speech language model Ming-UniAudio, which achieved a balance between generation and understanding capabilities. Ming-UniAudio sets new state-of-the-art (SOTA) records on 8 out of 12 metrics on the ContextASR benchmark. Notably, for Chinese voice cloning, it achieves a highly competitive Seed-TTS-WER of 0.95. Leveraging this foundational model, we further trained a dedicated speech editing model Ming-UniAudio-Edit, the first speech language model that enables universal, free-form speech editing guided solely by natural language instructions, handling both semantic and acoustic modifications without timestamp condition. To rigorously assess the editing capability and establish a foundation for future research, we introduce Ming-Freeform-Audio-Edit, the first comprehensive benchmark tailored for instruction-based free-form speech editing, featuring diverse scenarios and evaluation dimensions spanning semantic correctness, acoustic quality, and instruction alignment. We open-sourced the continuous audio tokenizer, the unified foundational model, and the free-form instruction-based editing model to facilitate the development of unified audio understanding, generation, and manipulation.", "AI": {"tldr": "提出了一个统一的框架，解决了现有语音模型在理解和生成任务中表示差异的问题，并实现了基于自然语言指令的自由形式语音编辑。", "motivation": "解决现有语音模型在理解和生成任务中表示差异的问题，使模型能够进行基于自然语言指令的自由形式编辑。", "method": "开发了一种名为MingTok-Audio的统一连续语音标记器，可同时适用于理解和生成任务，基于此开发了Ming-UniAudio模型，能够平衡生成和理解能力，从而实现Ming-UniAudio-Edit语音编辑模型。", "result": "Ming-UniAudio在ContextASR基准测试中的12项指标中有8项达到了新的SOTA，对于中文语音克隆，它达到了0.95的Seed-TTS-WER。", "conclusion": "展示了模型在语音理解和生成上的效果，并通过新推出的Ming-Freeform-Audio-Edit数据集证明了模型在语音编辑上的能力和多样性。开源了工具和模型以促进统一音频理解和生成的发展。"}}
{"id": "2511.05518", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05518", "abs": "https://arxiv.org/abs/2511.05518", "authors": ["Myeongseob Ko", "Nikhil Reddy Billa", "Adam Nguyen", "Charles Fleming", "Ming Jin", "Ruoxi Jia"], "title": "Retracing the Past: LLMs Emit Training Data When They Get Lost", "comment": "The 2025 Conference on Empirical Methods in Natural Language Processing", "summary": "The memorization of training data in large language models (LLMs) poses significant privacy and copyright concerns. Existing data extraction methods, particularly heuristic-based divergence attacks, often exhibit limited success and offer limited insight into the fundamental drivers of memorization leakage. This paper introduces Confusion-Inducing Attacks (CIA), a principled framework for extracting memorized data by systematically maximizing model uncertainty. We empirically demonstrate that the emission of memorized text during divergence is preceded by a sustained spike in token-level prediction entropy. CIA leverages this insight by optimizing input snippets to deliberately induce this consecutive high-entropy state. For aligned LLMs, we further propose Mismatched Supervised Fine-tuning (SFT) to simultaneously weaken their alignment and induce targeted confusion, thereby increasing susceptibility to our attacks. Experiments on various unaligned and aligned LLMs demonstrate that our proposed attacks outperform existing baselines in extracting verbatim and near-verbatim training data without requiring prior knowledge of the training data. Our findings highlight persistent memorization risks across various LLMs and offer a more systematic method for assessing these vulnerabilities.", "AI": {"tldr": "对于大规模语言模型中的训练数据记忆问题，本文提出了一种新的攻击框架Confusion-Inducing Attacks（CIA）以及Mismatched Supervised Fine-tuning（SFT）方法，以系统地探索和评估语言模型的记忆泄露问题。", "motivation": "论文的目标是解决大规模语言模型中的训练数据记忆问题，这可能导致隐私和版权泄露的风险。现有数据提取方法成功率有限，无法提供关于记忆泄露根本原因的深入理解。现有方法主要基于启发式的分歧攻击，而本文提出了一种更系统的攻击框架来解决这个问题。", "method": "该论文介绍了Confusion-Inducing Attacks（CIA）框架，通过系统地最大化模型的不确定性来抽取模型记住的数据。CIA框架通过优化输入片段，有目的地诱导连续的高熵状态。对于对齐的语言模型，论文还提出了一种名为Mismatched Supervised Fine-tuning（SFT）的方法，这种方法同时减弱模型的对齐性并引入定向的混淆，以提高模型对CIA攻击的易感性。", "result": "实验显示，在不同对齐和未对齐的语言模型中，本论文提出的攻击方法优于现有的基线方法，在提取准确和近乎准确的训练数据方面表现出色。", "conclusion": "实验结果表明，本文提出的攻击方法能够成功提取到原文本和近似原文本的训练数据，而不需要事先知道训练数据的具体内容。这表明了不同语言模型中存在持续的记忆风险，并提供了一种系统的方法来评估这些弱点。"}}
{"id": "2511.05532", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.05532", "abs": "https://arxiv.org/abs/2511.05532", "authors": ["Rufan Zhang", "Lin Zhang", "Xianghang Mi"], "title": "Beyond One-Size-Fits-All: Personalized Harmful Content Detection with In-Context Learning", "comment": null, "summary": "The proliferation of harmful online content--e.g., toxicity, spam, and negative sentiment--demands robust and adaptable moderation systems. However, prevailing moderation systems are centralized and task-specific, offering limited transparency and neglecting diverse user preferences--an approach ill-suited for privacy-sensitive or decentralized environments. We propose a novel framework that leverages in-context learning (ICL) with foundation models to unify the detection of toxicity, spam, and negative sentiment across binary, multi-class, and multi-label settings. Crucially, our approach enables lightweight personalization, allowing users to easily block new categories, unblock existing ones, or extend detection to semantic variations through simple prompt-based interventions--all without model retraining. Extensive experiments on public benchmarks (TextDetox, UCI SMS, SST2) and a new, annotated Mastodon dataset reveal that: (i) foundation models achieve strong cross-task generalization, often matching or surpassing task-specific fine-tuned models; (ii) effective personalization is achievable with as few as one user-provided example or definition; and (iii) augmenting prompts with label definitions or rationales significantly enhances robustness to noisy, real-world data. Our work demonstrates a definitive shift beyond one-size-fits-all moderation, establishing ICL as a practical, privacy-preserving, and highly adaptable pathway for the next generation of user-centric content safety systems. To foster reproducibility and facilitate future research, we publicly release our code on GitHub and the annotated Mastodon dataset on Hugging Face.", "AI": {"tldr": "研究提出了一种基于ICL和基础模型的新型框架，用于统一检测文本毒性、垃圾邮件和负面情绪。该框架能够进行跨任务的一般化，并支持高效的个性化设置，用户可以使用极少的例子或定义来调整系统的检测能力。这种框架为未来的用户中心内容安全系统提供了实用的、隐私保护的、高度可适应的解决方案。", "motivation": "目前的文本内容审核系统通常是中心化和任务特定的，缺少透明度并且忽视了用户的多样性需求，这种系统不适合隐私敏感或去中心化的环境。为了解决这些问题，提出了本研究。", "method": "我们提出了一种利用基础模型和上下文学习（ICL）来统一检测文本毒性、垃圾邮件和负面情绪的新型框架。该框架能够在二元分类、多类分类和多标签设置中工作，并且可以实现轻量级个性化设置，用户可以通过简单的提示干预来轻松地阻止新类别、解除现有类别的屏蔽或扩展到语义变体而无需重训模型。", "result": "实验结果表明：（i）基础模型在跨任务的一般化上表现出色，通常与任务特定的微调模型持平或超越；（ii）通过仅一个用户提供的示例或定义即可实现有效的个人化；（iii）通过使用标签定义或推理扩充提示可以显著提高在噪声现实数据上的鲁棒性。", "conclusion": "研究表明，基础模型在跨任务的一般化中有很强的表现，具有匹配甚至超越特定任务微调模型的能力。此外，还可以通过使用简单的用户提示来进行有效的个性化设置，同时增加提示中的标签定义或推理可以显著提高在噪声现实数据上的鲁棒性。"}}
{"id": "2511.05533", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.05533", "abs": "https://arxiv.org/abs/2511.05533", "authors": ["Bharathi Kannan Nithyanantham", "Tobias Sesterhenn", "Ashwin Nedungadi", "Sergio Peral Garijo", "Janis Zenkner", "Christian Bartelt", "Stefan Lüdtke"], "title": "MCP4IFC: IFC-Based Building Design Using Large Language Models", "comment": null, "summary": "Bringing generative AI into the architecture, engineering and construction (AEC) field requires systems that can translate natural language instructions into actions on standardized data models. We present MCP4IFC, a comprehensive open-source framework that enables Large Language Models (LLMs) to directly manipulate Industry Foundation Classes (IFC) data through the Model Context Protocol (MCP). The framework provides a set of BIM tools, including scene querying tools for information retrieval, predefined functions for creating and modifying common building elements, and a dynamic code-generation system that combines in-context learning with retrieval-augmented generation (RAG) to handle tasks beyond the predefined toolset. Experiments demonstrate that an LLM using our framework can successfully perform complex tasks, from building a simple house to querying and editing existing IFC data. Our framework is released as open-source to encourage research in LLM-driven BIM design and provide a foundation for AI-assisted modeling workflows. Our code is available at https://show2instruct.github.io/mcp4ifc/.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.05509", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05509", "abs": "https://arxiv.org/abs/2511.05509", "authors": ["Joel Valdivia Ortega", "Lorenz Lamm", "Franziska Eckardt", "Benedikt Schworm", "Marion Jasnin", "Tingying Peng"], "title": "Randomized-MLP Regularization Improves Domain Adaptation and Interpretability in DINOv2", "comment": null, "summary": "Vision Transformers (ViTs), such as DINOv2, achieve strong performance across domains but often repurpose low-informative patch tokens in ways that reduce the interpretability of attention and feature maps. This challenge is especially evident in medical imaging, where domain shifts can degrade both performance and transparency. In this paper, we introduce Randomized-MLP (RMLP) regularization, a contrastive learning-based method that encourages more semantically aligned representations. We use RMLPs when fine-tuning DINOv2 to both medical and natural image modalities, showing that it improves or maintains downstream performance while producing more interpretable attention maps. We also provide a mathematical analysis of RMLPs, offering insights into its role in enhancing ViT-based models and advancing our understanding of contrastive learning.", "AI": {"tldr": "通过引入RMLP正则化，论文实现了ViT模型在医学和自然图像上的高性能和更解释性，并提供了RMLP的数学分析。", "motivation": "由于低信息补丁标记的复用减少了ViT（如DINOv2）注意力和特征图的可解释性，尤其是在医学图像领域，这篇论文旨在解决该问题，通过引入RMLP来提高模型的性能和可解释性。", "method": "论文提出了一种基于对比学习的RMLP正则化方法，旨在提升ViT模型的性能和解释性。", "result": "论文介绍了一种名为随机化多层感知器（RMLP）的正则化方法，该方法基于对比学习，旨在鼓励更具语义对齐的表示。在微调DINOv2模型应用于医学和自然图像模态时，RMLP不仅改善或维持了下游性能，还生成了更可解释的注意力图。此外，论文还提供了对RMLP的数学分析，为增强基于ViT的模型的理解及对比学习进展提供了洞见。", "conclusion": "论文证实了RMLP正则化在提高DINOv2模型性能和可解释性方面的有效性，并通过数学分析提供了对RMLP作用机制的深入理解。"}}
{"id": "2511.05534", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.05534", "abs": "https://arxiv.org/abs/2511.05534", "authors": ["Kunxi Li", "Yufan Xiong", "Zhonghua Jiang", "Yiyun Zhou", "Zhaode Wang", "Chengfei Lv", "Shengyu Zhang"], "title": "FlowMM: Cross-Modal Information Flow Guided KV Cache Merging for Efficient Multimodal Context Inference", "comment": null, "summary": "Traditional KV cache eviction strategies, which discard less critical KV-pairs based on attention scores, often degrade generation quality, causing context loss or hallucinations. Recent efforts shift toward KV merging, merging eviction tokens with retention tokens based on similarity. However, in multimodal scenarios, distributional biases across modality tokens and attentional biases in cross-modal interactions limit its effectiveness. This work introduces FlowMM, an adaptive framework for cross-modal information flow-guided multimodal KV cache merging. FlowMM leverages cross-modal information flow to dynamically apply layer-specific merging strategies, capturing modality-specific patterns while preserving contextual integrity. Furthermore, we introduce a sensitivity-adaptive token matching mechanism that jointly evaluates token similarity and task-critical sensitivity, merging low-risk tokens while safeguarding high-sensitivity ones. Extensive experiments across diverse leading MLLMs show that FlowMM reduces KV cache memory by 80% to 95% and decoding latency by 1.3-1.8x, while maintaining competitive task performance.", "AI": {"tldr": "文章提出了FlowMM框架，通过跨模态信息流指导KV缓存的合并，解决了多模态场景中的挑战，显著减少了KV缓存的内存和解码延迟，同时保持任务性能。", "motivation": "传统的KV缓存淘汰策略基于注意力分数丢弃不太关键的KV对，这通常会降低生成质量，导致上下文丢失或幻觉。尽管最近的研究转向基于相似性的KV合并，但是对于多模态场景，模态令牌之间的分布偏差和跨模态交互中的注意力偏差限制了其效果。", "method": "FlowMM是一个适应性框架，它利用跨模态信息流来动态应用层特定的合并策略，能够捕捉模态特定的模式同时保持上下文完整性。此外，它还引入了一种敏感度自适应的令牌匹配机制，可以同时评估令牌相似性和任务关键敏感度，合并低风险令牌同时保护高敏感度的令牌。", "result": "在多种领先的多模态大语言模型上的广泛实验表明，FlowMM可以将KV缓存内存减少80%到95%，并将解码延迟减少1.3-1.8倍，同时保持竞争性的任务性能。", "conclusion": "FlowMM通过动态层特定KV缓存合并策略和敏感度自适应令牌匹配机制成功提高了多模态场景下的模型效率和性能，同时减少了内存占用和解码延迟。"}}
{"id": "2511.05540", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05540", "abs": "https://arxiv.org/abs/2511.05540", "authors": ["Shiyao Sang"], "title": "Token Is All You Need: Cognitive Planning through Sparse Intent Alignment", "comment": "6 pages, 2 figures. Preprint exploring a new cognitive paradigm for autonomous planning", "summary": "We challenge the long-standing assumption that exhaustive scene modeling is required for high-performance end-to-end autonomous driving (E2EAD). Unlike world-model approaches that rely on computationally intensive future scene generation or vision-language-action (VLA) systems constrained by Markov assumptions, we show that a minimal set of semantically rich tokens is sufficient for effective planning. Experiments on the nuPlan benchmark (720 scenarios, over 11,000 samples) using perception-informed BEV representations yield three key findings: (1) even without future prediction, our sparse representation achieves 0.548 m ADE, comparable to or surpassing prior methods reporting around 0.75 m on nuScenes; (2) conditioning trajectory decoding on predicted future tokens reduces ADE to 0.479 m, a 12.6% improvement over current-state baselines; and (3) explicit reconstruction loss offers no benefit and may degrade performance under reliable perception inputs. Notably, we observe the emergence of temporal fuzziness, where the model adaptively attends to task-relevant semantics rather than aligning rigidly to fixed timestamps, providing a cognitive advantage for planning under uncertainty. Our \"token is all you need\" principle marks a paradigm shift from reconstructing the world to understanding it, laying a foundation for cognitively inspired systems that plan through imagination rather than reaction.", "AI": {"tldr": "研究证明在自动驾驶中，不需要复杂的场景模型，少量丰富的语义标记就足以进行有效地规划，而且能够在对未来的预测中取得比传统方法更好的性能。", "motivation": "该研究旨在挑战现有自动驾驶领域中需要对场景进行详尽建模的传统观点，证明了少量的语义信息足以完成有效的规划。", "method": "本研究提出了一种基于少量语义丰富的标记进行有效规划的方法，这种方法不需要对未来场景进行详细的生成或预测，而是使用了更简洁的BEV表示方法。", "result": "在nuPlan基准上进行了实验，实验结果表明使用预测的未来标记进行轨迹解码可以将ADE减少到0.479m，相较于之前的方法有了12.6%的提升。", "conclusion": "研究表明，通过理解和利用少量的语义标记，可以实现有效的自动驾驶规划，这标志着从对世界的重建向对世界的理解迈进的范式转变。"}}
{"id": "2511.05535", "categories": ["cs.CL", "cs.DB", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.05535", "abs": "https://arxiv.org/abs/2511.05535", "authors": ["Trivikram Satharasi", "S Sitharama Iyengar"], "title": "Future of AI Models: A Computational perspective on Model collapse", "comment": "Submitted to Springer Nature. Code Available at https://github.com/t-satharasi/AI-Modal-Collapse-Code-for-Reproduction.git", "summary": "Artificial Intelligence, especially Large Language Models (LLMs), has transformed domains such as software engineering, journalism, creative writing, academia, and media (Naveed et al. 2025; arXiv:2307.06435). Diffusion models like Stable Diffusion generate high-quality images and videos from text. Evidence shows rapid expansion: 74.2% of newly published webpages now contain AI-generated material (Ryan Law 2025), 30-40% of the active web corpus is synthetic (Spennemann 2025; arXiv:2504.08755), 52% of U.S. adults use LLMs for writing, coding, or research (Staff 2025), and audits find AI involvement in 18% of financial complaints and 24% of press releases (Liang et al. 2025). The underlying neural architectures, including Transformers (Vaswani et al. 2023; arXiv:1706.03762), RNNs, LSTMs, GANs, and diffusion networks, depend on large, diverse, human-authored datasets (Shi & Iyengar 2019). As synthetic content dominates, recursive training risks eroding linguistic and semantic diversity, producing Model Collapse (Shumailov et al. 2024; arXiv:2307.15043; Dohmatob et al. 2024; arXiv:2402.07712). This study quantifies and forecasts collapse onset by examining year-wise semantic similarity in English-language Wikipedia (filtered Common Crawl) from 2013 to 2025 using Transformer embeddings and cosine similarity metrics. Results reveal a steady rise in similarity before public LLM adoption, likely driven by early RNN/LSTM translation and text-normalization pipelines, though modest due to a smaller scale. Observed fluctuations reflect irreducible linguistic diversity, variable corpus size across years, finite sampling error, and an exponential rise in similarity after the public adoption of LLM models. These findings provide a data-driven estimate of when recursive AI contamination may significantly threaten data richness and model generalization.", "AI": {"tldr": "研究通过分析2013年至2025年英语维基百科中逐年语义相似性的变化，预测递归AI训练可能导致的语言模型崩溃的时间点，发现公众采用LLM模型后相似性呈指数级增长。", "motivation": "鉴于合成内容的主导地位，研究旨在量化和预测由递归训练导致的语言和语义多样性下降问题，即模型崩溃。", "method": "此研究通过观察2013年至2025年间英语维基百科（经过过滤的Common Crawl数据）中逐年语义相似性的变化，使用Transformer嵌入和余弦相似性度量来量化并预测模型崩溃的起始点。", "result": "研究发现，在公众采用LLM模型之前，相似性有稳定上升，这主要是早期RNN/LSTM翻译和文本规范化管线驱动的结果，尽管幅度不大。公众采用LLM模型后，相似性呈指数级上升，这反映出递归AI污染可能威胁到数据的丰富性和模型的泛化能力。", "conclusion": "研究为递归AI训练何时可能显著威胁到数据的丰富性和模型的泛化能力提供了基于数据的估计。这一趋势意味着需要采取措施来防止模型崩溃，保护语言和语义的多样性。"}}
{"id": "2511.05547", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05547", "abs": "https://arxiv.org/abs/2511.05547", "authors": ["Advait Thakur", "Khushi Khanchandani", "Akshita Shetty", "Chaitravi Reddy", "Ritisa Behera"], "title": "Automated Invoice Data Extraction: Using LLM and OCR", "comment": "10 pages, 3 figures", "summary": "Conventional Optical Character Recognition (OCR) systems are challenged by\nvariant invoice layouts, handwritten text, and low- quality scans, which are\noften caused by strong template dependencies that restrict their flexibility\nacross different document structures and layouts. Newer solutions utilize\nadvanced deep learning models such as Convolutional Neural Networks (CNN) as\nwell as Transformers, and domain-specific models for better layout analysis and\naccuracy across various sections over varied document types. Large Language\nModels (LLMs) have revolutionized extraction pipelines at their core with\nsophisticated entity recognition and semantic comprehension to support complex\ncontextual relationship mapping without direct programming specification.\nVisual Named Entity Recognition (NER) capabilities permit extraction from\ninvoice images with greater contextual sensitivity and much higher accuracy\nrates than older approaches. Existing industry best practices utilize hybrid\narchitectures that blend OCR technology and LLM for maximum scalability and\nminimal human intervention. This work introduces a holistic Artificial\nIntelligence (AI) platform combining OCR, deep learning, LLMs, and graph\nanalytics to achieve unprecedented extraction quality and consistency.", "AI": {"tldr": "本文介绍了一种结合OCR、深度学习、LLMs和图分析的人工智能平台，以实现前所未有的提取质量和一致性。", "motivation": "传统的OCR系统受限于模板依赖，无法灵活应对不同的文档结构和布局，而新兴的解决方案利用先进的深度学习模型，如CNN和Transformer，结合LLMs的实体识别和语义理解能力，旨在提高不同文档类型下的布局分析和准确率。", "method": "该工作提出了一种结合OCR技术、深度学习模型、LLMs以及图分析的综合性AI平台，侧重于提高提取质量和一致性。", "result": "与传统方法相比，新平台提高了从发票图像中提取信息的上下文敏感性和准确性。", "conclusion": "通过使用深度学习架构和LLMs结合的混合结构，该平台能够提供最大化的可扩展性和最低限度的人工干预，实现文档信息提取的最优化。"}}
{"id": "2511.05541", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05541", "abs": "https://arxiv.org/abs/2511.05541", "authors": ["Usha Bhalla", "Alex Oesterling", "Claudio Mayrink Verdun", "Himabindu Lakkaraju", "Flavio P. Calmon"], "title": "Temporal Sparse Autoencoders: Leveraging the Sequential Nature of Language for Interpretability", "comment": "23 Pages, 10 figures", "summary": "Translating the internal representations and computations of models into concepts that humans can understand is a key goal of interpretability. While recent dictionary learning methods such as Sparse Autoencoders (SAEs) provide a promising route to discover human-interpretable features, they suffer from a variety of problems, including a systematic failure to capture the rich conceptual information that drives linguistic understanding. Instead, they exhibit a bias towards shallow, token-specific, or noisy features, such as \"the phrase 'The' at the start of sentences\". In this work, we propose that this is due to a fundamental issue with how dictionary learning methods for LLMs are trained. Language itself has a rich, well-studied structure spanning syntax, semantics, and pragmatics; however, current unsupervised methods largely ignore this linguistic knowledge, leading to poor feature discovery that favors superficial patterns over meaningful concepts. We focus on a simple but important aspect of language: semantic content has long-range dependencies and tends to be smooth over a sequence, whereas syntactic information is much more local. Building on this insight, we introduce Temporal Sparse Autoencoders (T-SAEs), which incorporate a novel contrastive loss encouraging consistent activations of high-level features over adjacent tokens. This simple yet powerful modification enables SAEs to disentangle semantic from syntactic features in a self-supervised manner. Across multiple datasets and models, T-SAEs recover smoother, more coherent semantic concepts without sacrificing reconstruction quality. Strikingly, they exhibit clear semantic structure despite being trained without explicit semantic signal, offering a new pathway for unsupervised interpretability in language models.", "AI": {"tldr": "本文提出了Temporal Sparse Autoencoders (T-SAEs) 来改进现有的Sparse Autoencoders (SAEs)，以更好地捕捉语言的语义信息，而不是停留在浅层的单词或模式。T-SAEs 通过引入对比损失来促进相邻词汇之间的高级特征保持一致性，从而实现语义与句法特征的分离。", "motivation": "现有的字典学习方法如稀疏自动编码器 (SAEs) 在发现人类可理解的特征上存在问题，如倾向于捕捉表面模式而非有意义的概念。本研究旨在解决这个问题，特别是考虑到语言具有丰富的结构。", "method": "提出了Temporal Sparse Autoencoders (T-SAEs)，增加了对比损失以保证语义特征的一致性，并结合语义信息长距离依赖的特点进行改进。", "result": "T-SAEs 恢复了更平滑、连贯的语义概念，而在多个数据集和模型上没有牺牲重建质量。", "conclusion": "T-SAEs 通过无监督的方式展示了语义的结构，为语言模型提供了新的路径，以获得更好的无监督可解释性。"}}
{"id": "2511.05551", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05551", "abs": "https://arxiv.org/abs/2511.05551", "authors": ["Qiaojie Zheng", "Jiucai Zhang", "Xiaoli Zhang"], "title": "In-Context-Learning-Assisted Quality Assessment Vision-Language Models for Metal Additive Manufacturing", "comment": "8 pages, 8 figures", "summary": "Vision-based quality assessment in additive manufacturing often requires dedicated machine learning models and application-specific datasets. However, data collection and model training can be expensive and time-consuming. In this paper, we leverage vision-language models' (VLMs') reasoning capabilities to assess the quality of printed parts and introduce in-context learning (ICL) to provide VLMs with necessary application-specific knowledge and demonstration samples. This method eliminates the requirement for large application-specific datasets for training models. We explored different sampling strategies for ICL to search for the optimal configuration that makes use of limited samples. We evaluated these strategies on two VLMs, Gemini-2.5-flash and Gemma3:27b, with quality assessment tasks in wire-laser direct energy deposition processes. The results show that ICL-assisted VLMs can reach quality classification accuracies similar to those of traditional machine learning models while requiring only a minimal number of samples. In addition, unlike traditional classification models that lack transparency, VLMs can generate human-interpretable rationales to enhance trust. Since there are no metrics to evaluate their interpretability in manufacturing applications, we propose two metrics, knowledge relevance and rationale validity, to evaluate the quality of VLMs' supporting rationales. Our results show that ICL-assisted VLMs can address application-specific tasks with limited data, achieving relatively high accuracy while also providing valid supporting rationales for improved decision transparency.", "AI": {"tldr": "通过使用视觉-语言模型和上下文学习，实现用少量样本进行高质量的3D打印部件评估，同时提供可解释的决策理由。", "motivation": "减少数据收集和模型训练的成本和时间，提高模型的可解释性，使决策过程更透明。", "method": "使用视觉-语言模型和上下文学习，探索不同的抽样策略以实现最少样本配置，同时提出新的可解释性评估指标。", "result": "在仅使用少量样本的情况下，所提方法达到了与传统机器学习模型相似的分类准确性，并提供了有效的解释理由。", "conclusion": "上下文学习辅助的视觉-语言模型能够很好地处理特定应用任务，实现较高的准确率和决策透明度。"}}
{"id": "2511.05553", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05553", "abs": "https://arxiv.org/abs/2511.05553", "authors": ["Xinyan Cai", "Shiguang Wu", "Dafeng Chi", "Yuzheng Zhuang", "Xingyue Quan", "Jianye Hao", "Qiang Guan"], "title": "EVLP:Learning Unified Embodied Vision-Language Planner with Reinforced Supervised Fine-Tuning", "comment": null, "summary": "In complex embodied long-horizon manipulation tasks, effective task decomposition and execution require synergistic integration of textual logical reasoning and visual-spatial imagination to ensure efficient and accurate operation. Current methods fail to adopt a unified generation framework for multimodal planning, lead to inconsistent in multimodal planning. To address this challenge, we present \\textbf{EVLP (Embodied Vision-Language Planner)}, an innovative multimodal unified generation framework that jointly models linguistic reasoning and visual generation. Our approach achieves multimodal planning for long-horizon tasks through a novel training pipeline incorporating dynamic pretraining and reinforced alignment. Our core innovations consist of three key components: \\textbf{1) Unified Multimodal Generation Framework}: For understanding, We integrate semantic information with spatial features to provide comprehensive visual perception. For generation, we directly learn the joint distribution of discrete images for one-step visual synthesis, enabling coordinated language-visual modeling through learnable cross-modal attention mechanisms. \\textbf{2) Dynamic Perception Pretraining}: We propose a bidirectional dynamic alignment strategy employing inverse dynamics tasks and forward dynamics tasks, effectively strengthening multimodal correlations within a unified feature space. \\textbf{3) Reinforced Supervised Fine-Tuning}: While conducting instruction-based fine-tuning in the unified generation space, we construct a reinforce loss to align the spatial logic between textual actions and generated images, enabling the model to acquire spatio-awared multimodal planning capabilities.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.05578", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.05578", "abs": "https://arxiv.org/abs/2511.05578", "authors": ["Preston Firestone", "Shubham Ugare", "Gagandeep Singh", "Sasa Misailovic"], "title": "UTF-8 Plumbing: Byte-level Tokenizers Unavoidably Enable LLMs to Generate Ill-formed UTF-8", "comment": "COLM 2025", "summary": "Subword tokenization segments input text according to a pre-defined vocabulary to feed it into a language model; the language model, in turn, generates a sequence made from this same vocabulary. The members of the vocabulary can be built of code points or bytes. Using code points means that all members of the vocabulary are valid UTF-8 characters. However, it also requires thousands of initial members to achieve acceptable coverage of inputs. Beginning with bytes, on the contrary, avoids out-of-vocabulary errors with only 256 initial members of the vocabulary, but the members of the vocabulary and sequences of them are not guaranteed to be valid UTF-8. Sequences that are not valid UTF-8 break code that assumes its input to be valid UTF-8. Applications of language models must account for the breakage thereby introduced. In this paper, we formalize tokenization using monoid theory and prove that tokenizers whose vocabularies contain tokens that are ill-formed UTF-8 can always produce sequences that are ill-formed UTF-8. We demonstrate formally that attempting to incrementally convert tokens back to a string and interpret the results as UTF-8 gives different results than converting the whole sequence of tokens at once. This formal result predicts real-world bugs: we evaluate mitigations for the problem identified and provide case studies of major foundation models, serving engines, and constrained generation systems.", "AI": {"tldr": "研究了子词分词方式对语言模型输入的影响，分析了使用字节作为初始词汇成员的问题并提出了解决方案。", "motivation": "解决子词分词过程中由于不符合UTF-8格式引起的代码中断问题，保证语言模型的稳定性和准确性。", "method": "使用幺半群理论对子词分词方式进行形式化，并证明词汇表中包含不符合UTF-8格式的标记的分词器总是会产生不符合UTF-8格式的序列。", "result": "证明了增量地将标记转换回字符串并将其解释为UTF-8与一次性转换整个标记序列的结果不同，预测了现实世界中的错误。", "conclusion": "通过对主要的基础模型、服务引擎和约束生成系统的评估，展示了解决不符合UTF-8字符序列问题的方法的有效性。"}}
{"id": "2511.05554", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05554", "abs": "https://arxiv.org/abs/2511.05554", "authors": ["Chenping Pei", "Fadi Dornaika", "Jingjun Bi"], "title": "MCFCN: Multi-View Clustering via a Fusion-Consensus Graph Convolutional Network", "comment": null, "summary": "Existing Multi-view Clustering (MVC) methods based on subspace learning focus on consensus representation learning while neglecting the inherent topological structure of data. Despite the integration of Graph Neural Networks (GNNs) into MVC, their input graph structures remain susceptible to noise interference. Methods based on Multi-view Graph Refinement (MGRC) also have limitations such as insufficient consideration of cross-view consistency, difficulty in handling hard-to-distinguish samples in the feature space, and disjointed optimization processes caused by graph construction algorithms. To address these issues, a Multi-View Clustering method via a Fusion-Consensus Graph Convolutional Network (MCFCN) is proposed. The network learns the consensus graph of multi-view data in an end-to-end manner and learns effective consensus representations through a view feature fusion model and a Unified Graph Structure Adapter (UGA). It designs Similarity Matrix Alignment Loss (SMAL) and Feature Representation Alignment Loss (FRAL). With the guidance of consensus, it optimizes view-specific graphs, preserves cross-view topological consistency, promotes the construction of intra-class edges, and realizes effective consensus representation learning with the help of GCN to improve clustering performance. MCFCN demonstrates state-of-the-art performance on eight multi-view benchmark datasets, and its effectiveness is verified by extensive qualitative and quantitative implementations. The code will be provided at https://github.com/texttao/MCFCN.", "AI": {"tldr": "提出了一种新的多视角聚类方法（MCFCN），其基于融合共识图卷积网络，能有效提升多视角数据的聚类效果。", "motivation": "现有的基于子空间学习的多视角聚类方法忽略了数据的固有拓扑结构，即使引入了图神经网络（GNNs），其输入图结构仍易受噪声干扰，而多视角图优化（MGRC）方法则存在跨视角一致性不足、难以处理特征空间中难以区分的样本以及由图构造算法导致的优化过程割裂等问题。", "method": "MCFCN 提出了一种通过融合共识图卷积网络（Fusion-Consensus Graph Convolutional Network）来优化多视角聚类的方法，该方法包括视图特征融合模型和统一图结构适配器（UGA），并通过相似矩阵对齐损失（SMAL）和特征表示对齐损失（FRAL）来优化视图特定的图结构，以实现有效的共识表示学习。", "result": "MCFCN 在八个多视角基准数据集上展现了最先进的性能，并通过大量的定性和定量实验验证了其有效性。", "conclusion": "该研究为多视角聚类提供了一种新的解决方案，通过融合共识图卷积网络和相应的损失函数设计，MCFCN 实现了跨视角的一致性保持和分类内部边的构建，从而提高了聚类性能。"}}
{"id": "2511.05650", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05650", "abs": "https://arxiv.org/abs/2511.05650", "authors": ["Yichen Wang", "Chenghao Yang", "Tenghao Huang", "Muhao Chen", "Jonathan May", "Mina Lee"], "title": "Optimizing Diversity and Quality through Base-Aligned Model Collaboration", "comment": "52 pages, 16 figures", "summary": "Alignment has greatly improved large language models (LLMs)' output quality at the cost of diversity, yielding highly similar outputs across generations. We propose Base-Aligned Model Collaboration (BACo), an inference-time token-level model collaboration framework that dynamically combines a base LLM with its aligned counterpart to optimize diversity and quality. Inspired by prior work (Fei et al., 2025), BACo employs routing strategies that determine, at each token, from which model to decode based on next-token prediction uncertainty and predicted contents' semantic role. Prior diversity-promoting methods, such as retraining, prompt engineering, and multi-sampling methods, improve diversity but often degrade quality or require costly decoding or post-training. In contrast, BACo achieves both high diversity and quality post hoc within a single pass, while offering strong controllability. We explore a family of routing strategies, across three open-ended generation tasks and 13 metrics covering diversity and quality, BACo consistently surpasses state-of-the-art inference-time baselines. With our best router, BACo achieves a 21.3% joint improvement in diversity and quality. Human evaluations also mirror these improvements. The results suggest that collaboration between base and aligned models can optimize and control diversity and quality.", "AI": {"tldr": "提出BACo方法，在保持高质量输出的情况下增加多样性，显著优于现有方法，通过动态选择基础模型或其对齐版本来实现。", "motivation": "该研究旨在解决对齐带来的多样性下降问题，提高语言模型输出的质量同时保持多样性。", "method": "BACo方法通过在推理阶段动态结合基础语言模型及其对齐版本来优化多样性与质量，具体通过路由策略决定在每个词的位置从哪个模型解码，这基于下一个词的不确定性预测和预测内容的语义角色。", "result": "BACo在三种开放式生成任务和13个涵盖多样性和质量的指标上超越了现有的推理时间基线方法。在最佳路由器下，BACo在多样性和质量上实现21.3%的联合提升。", "conclusion": "研究表明，基础模型和对齐模型之间的协作可以优化并控制多样性和质量。"}}
{"id": "2511.05557", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05557", "abs": "https://arxiv.org/abs/2511.05557", "authors": ["Jiayuan Wang", "Q. M. Jonathan Wu", "Ning Zhang", "Katsuya Suto", "Lei Zhong"], "title": "Compressing Multi-Task Model for Autonomous Driving via Pruning and Knowledge Distillation", "comment": null, "summary": "Autonomous driving systems rely on panoptic perception to jointly handle object detection, drivable area segmentation, and lane line segmentation. Although multi-task learning is an effective way to integrate these tasks, its increasing model parameters and complexity make deployment on on-board devices difficult. To address this challenge, we propose a multi-task model compression framework that combines task-aware safe pruning with feature-level knowledge distillation. Our safe pruning strategy integrates Taylor-based channel importance with gradient conflict penalty to keep important channels while removing redundant and conflicting channels. To mitigate performance degradation after pruning, we further design a task head-agnostic distillation method that transfers intermediate backbone and encoder features from a teacher to a student model as guidance. Experiments on the BDD100K dataset demonstrate that our compressed model achieves a 32.7% reduction in parameters while segmentation performance shows negligible accuracy loss and only a minor decrease in detection (-1.2% for Recall and -1.8% for mAP50) compared to the teacher. The compressed model still runs at 32.7 FPS in real-time. These results show that combining pruning and knowledge distillation provides an effective compression solution for multi-task panoptic perception.", "AI": {"tldr": "本文提出了一个结合任务感知安全剪枝和特征级知识蒸馏的多任务模型压缩框架，有效解决了自动驾驶系统中多任务学习参数多、复杂度高的问题，并在BDD100K数据集上验证了此方法的实用性与有效性。", "motivation": "多任务学习虽然是一种将目标检测、可行驶区域分割和车道线分割任务集成的有效方法，但随着模型参数和复杂度的增加，这使得在车载设备上的部署变得困难。为了解决这一问题，本文提出了一个新的解决方案。", "method": "本文提出了一种多任务模型压缩框架，结合任务感知的安全剪枝和特征级知识蒸馏，以应对自动驾驶系统中多任务学习参数增多和复杂度增加的问题。安全剪枝策略结合了基于Taylor的通道重要性和梯度冲突惩罚，以保留重要通道并去除冗余和冲突的通道。为了缓解剪枝后的性能下降，设计了一种任务头无关的知识蒸馏方法，将教师模型的中间骨干和编码器特征传递给学生模型作为指导。", "result": "实验结果表明，在BDD100K数据集上，本文压缩后的模型参数减少了32.7%，分割性能几乎没有损失，检测性能（召回率和mAP50）仅略有下降（分别下降1.2%和1.8%）。压缩后的模型仍然能够以32.7 FPS实时运行。", "conclusion": "这些结果表明，结合剪枝和知识蒸馏为多任务全景感知提供了一种有效的压缩方案。"}}
{"id": "2511.05722", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05722", "abs": "https://arxiv.org/abs/2511.05722", "authors": ["Zheng Du", "Hao Kang", "Song Han", "Tushar Krishna", "Ligeng Zhu"], "title": "OckBench: Measuring the Efficiency of LLM Reasoning", "comment": null, "summary": "Large language models such as GPT-4, Claude 3, and the Gemini series have improved automated reasoning and code generation. However, existing benchmarks mainly focus on accuracy and output quality, and they ignore an important factor: decoding token efficiency. In real systems, generating 10,000 tokens versus 100,000 tokens leads to large differences in latency, cost, and energy. In this work, we introduce OckBench, a model-agnostic and hardware-agnostic benchmark that evaluates both accuracy and token count for reasoning and coding tasks. Through experiments comparing multiple open- and closed-source models, we uncover that many models with comparable accuracy differ wildly in token consumption, revealing that efficiency variance is a neglected but significant axis of differentiation. We further demonstrate Pareto frontiers over the accuracy-efficiency plane and argue for an evaluation paradigm shift: we should no longer treat tokens as \"free\" to multiply. OckBench provides a unified platform for measuring, comparing, and guiding research in token-efficient reasoning. Our benchmarks are available at https://ockbench.github.io/ .", "AI": {"tldr": "本文提出OckBench，一个评估模型推理和代码生成任务效率的新基准，强调了对令牌使用效率的重视。", "motivation": "研究的动机是认识到现有评估基准中，令牌效率尚未得到足够重视，而实际上令牌消耗对系统性能比如延迟、成本和能源消耗有重要影响。目的在于提出一个新的评估架构，提醒研究人员不要将令牌视为‘廉价’资源。", "method": "研究方法包括开发OckBench基准，用于评价模型在推理和编程任务上的准确度和令牌使用数。进一步，通过实验比较多个开源和闭源模型，以揭示模型间的令牌效率差异。", "result": "研究表明，尽管大型语言模型如GPT-4, Claude 3 和 Gemini 系列在自动化推理和代码生成方面有所提升，但现有的评估基准主要集中在准确度和输出质量上，忽略了解码令牌效率这一重要因素。通过引入OckBench，该研究提出并验证了一个既考虑准确度又关注令牌数量的推理和编码任务评估基准，揭示了在令牌使用效率上的差异。", "conclusion": "研究结论论证了模型效率评估的重要性，提出了一个通过OckBench进行令牌效率和精度综合评价的方法，呼吁评估模型不应忽略令牌效率。"}}
{"id": "2511.05561", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05561", "abs": "https://arxiv.org/abs/2511.05561", "authors": ["Jiali Gao", "Taoran Liu", "Hongfei Ye", "Jianjun Chen"], "title": "FilletRec: A Lightweight Graph Neural Network with Intrinsic Features for Automated Fillet Recognition", "comment": null, "summary": "Automated recognition and simplification of fillet features in CAD models is critical for CAE analysis, yet it remains an open challenge. Traditional rule-based methods lack robustness, while existing deep learning models suffer from poor generalization and low accuracy on complex fillets due to their generic design and inadequate training data. To address these issues, this paper proposes an end-to-end, data-driven framework specifically for fillet features. We first construct and release a large-scale, diverse benchmark dataset for fillet recognition to address the inadequacy of existing data. Based on it, we propose FilletRec, a lightweight graph neural network. The core innovation of this network is its use of pose-invariant intrinsic geometric features, such as curvature, enabling it to learn more fundamental geometric patterns and thereby achieve high-precision recognition of complex geometric topologies. Experiments show that FilletRec surpasses state-of-the-art methods in both accuracy and generalization, while using only 0.2\\%-5.4\\% of the parameters of baseline models, demonstrating high model efficiency. Finally, the framework completes the automated workflow from recognition to simplification by integrating an effective geometric simplification algorithm.", "AI": {"tldr": "本文提出了一种专门用于识别和简化CAD模型倒圆角特征的端到端数据驱动框架，通过提出FilletRec，实现了在准确性和泛化能力上的显著提升，并且具有高模型效率。", "motivation": "本文的研究动机在于解决CAD模型自动化倒圆角特征识别和简化的问题，这是CAE分析中的关键问题。传统的基于规则的方法缺乏鲁棒性，而现有的深度学习模型由于其通用设计和训练数据不足，在处理复杂倒圆角时表现出较差的一般化能力和低准确度。", "method": "本文提出了一种端到端的数据驱动框架，专门用于识别和简化CAD模型中的倒圆角特征。该框架包括构建并发布一个大规模且多样的倒圆角识别基准数据集，以及提出FilletRec，一种轻量级的图神经网络。FilletRec的核心创新是利用姿态不变的内在几何特征，如曲率，从而能够学习基础几何模式并实现复杂几何结构的高精度识别。", "result": "实验结果显示，FilletRec在准确性和泛化能力方面超过了现有的最先进方法，且参数量仅为基准模型的0.2%-5.4%，展现出高模型效率。", "conclusion": "总之，本文提出的框架包括构建倒圆角识别数据集和轻量级图神经网络FilletRec，展示了高效准确识别CAD模型中复杂倒圆角特征的能力，同时通过集成几何简化算法完成了识别到简化的自动化工作流程。"}}
{"id": "2511.05743", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.05743", "abs": "https://arxiv.org/abs/2511.05743", "authors": ["Kerem Sahin", "Sheridan Feucht", "Adam Belfki", "Jannik Brinkmann", "Aaron Mueller", "David Bau", "Chris Wendler"], "title": "In-Context Learning Without Copying", "comment": null, "summary": "Induction heads are attention heads that perform inductive copying by matching patterns from earlier context and copying their continuations verbatim. As models develop induction heads, they often experience a sharp drop in training loss, a phenomenon cited as evidence that induction heads may serve as a prerequisite for more complex in-context learning (ICL) capabilities. In this work, we ask whether transformers can still acquire ICL capabilities when inductive copying is suppressed. We propose Hapax, a setting where we omit the loss contribution of any token that can be correctly predicted by induction heads. Despite a significant reduction in inductive copying, performance on abstractive ICL tasks (i.e., tasks where the answer is not contained in the input context) remains comparable and surpasses the vanilla model on 13 of 21 tasks, even though 31.7\\% of tokens are omitted from the loss. Furthermore, our model achieves lower loss values on token positions that cannot be predicted correctly by induction heads. Mechanistic analysis further shows that models trained with Hapax develop fewer and weaker induction heads but still preserve ICL capabilities. Taken together, our findings indicate that inductive copying is not essential for learning abstractive ICL mechanisms.", "AI": {"tldr": "通过Hapax设置，即使显著减少归纳复制，模型仍然可以保持上下文学习能力，且在很多抽象任务中表现优于标准模型，表明归纳复制不是学习抽象上下文机制的必需条件。", "motivation": "研究探讨了变压器在抑制归纳复制的情况下是否仍然可以获得上下文学习能力，挑战了归纳头作为复杂上下文学习能力先决条件的观点。", "method": "通过提出Hapax设置，该设置省略了可以被归纳头正确预测的任何标记的损失贡献，研究了抑制归纳复制时变压器是否仍然可以获得上下文学习能力。", "result": "即使减少了归纳复制，Hapax模型在抽象上下文学习任务上的表现仍与标准模型相当，甚至在21项任务中的13项上优于标准模型，尽管省略了31.7%的标记的损失。此外，Hapax模型在无法被归纳头正确预测的标记位置上的损失值降低。", "conclusion": "综合来看，研究发现，归纳复制对于学习抽象上下文学习机制来说并不是必不可少的。"}}
{"id": "2511.05564", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05564", "abs": "https://arxiv.org/abs/2511.05564", "authors": ["Yang Liu", "Boan Chen", "Xiaoguang Zhu", "Jing Liu", "Peng Sun", "Wei Zhou"], "title": "M2S2L: Mamba-based Multi-Scale Spatial-temporal Learning for Video Anomaly Detection", "comment": "IEEE VCIP 2025", "summary": "Video anomaly detection (VAD) is an essential task in the image processing community with prospects in video surveillance, which faces fundamental challenges in balancing detection accuracy with computational efficiency. As video content becomes increasingly complex with diverse behavioral patterns and contextual scenarios, traditional VAD approaches struggle to provide robust assessment for modern surveillance systems. Existing methods either lack comprehensive spatial-temporal modeling or require excessive computational resources for real-time applications. In this regard, we present a Mamba-based multi-scale spatial-temporal learning (M2S2L) framework in this paper. The proposed method employs hierarchical spatial encoders operating at multiple granularities and multi-temporal encoders capturing motion dynamics across different time scales. We also introduce a feature decomposition mechanism to enable task-specific optimization for appearance and motion reconstruction, facilitating more nuanced behavioral modeling and quality-aware anomaly assessment. Experiments on three benchmark datasets demonstrate that M2S2L framework achieves 98.5%, 92.1%, and 77.9% frame-level AUCs on UCSD Ped2, CUHK Avenue, and ShanghaiTech respectively, while maintaining efficiency with 20.1G FLOPs and 45 FPS inference speed, making it suitable for practical surveillance deployment.", "AI": {"tldr": "The M2S2L framework combines multi-scale spatial-temporal modeling with feature decomposition for appearance and motion reconstruction. It achieves high accuracy across multiple datasets with efficient performance, making it suitable for real-time video surveillance.", "motivation": "Modern VAD methods struggle with the complexity of video content and have issues with either poor spatial-temporal modeling or excessive computational demands. The M2S2L framework aims to address these challenges by providing robust, efficient video anomaly detection suitable for real-time applications.", "method": "The proposed M2S2L framework uses hierarchical spatial encoders with multi-scale operating granularities and multi-temporal encoders capturing motion dynamics at various time scales. It also introduces a feature decomposition mechanism for task-specific optimization of appearance and motion reconstruction.", "result": "The M2S2L framework achieved high frame-level AUCs of 98.5%, 92.1%, and 77.9% on the UCSD Ped2, CUHK Avenue, and ShanghaiTech datasets, respectively. Additionally, it demonstrated efficiency with 20.1G FLOPs and 45 FPS inference speed.", "conclusion": "The M2S2L framework is capable of delivering both high accuracy and efficiency in video anomaly detection, making it suitable for practical deployment in surveillance systems."}}
{"id": "2511.05752", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.05752", "abs": "https://arxiv.org/abs/2511.05752", "authors": ["Xiangchen Song", "Yulin Huang", "Jinxu Guo", "Yuchen Liu", "Yaxuan Luan"], "title": "Multi-Scale Feature Fusion and Graph Neural Network Integration for Text Classification with Large Language Models", "comment": null, "summary": "This study investigates a hybrid method for text classification that integrates deep feature extraction from large language models, multi-scale fusion through feature pyramids, and structured modeling with graph neural networks to enhance performance in complex semantic contexts. First, the large language model captures contextual dependencies and deep semantic representations of the input text, providing a rich feature foundation for subsequent modeling. Then, based on multi-level feature representations, the feature pyramid mechanism effectively integrates semantic features of different scales, balancing global information and local details to construct hierarchical semantic expressions. Furthermore, the fused features are transformed into graph representations, and graph neural networks are employed to capture latent semantic relations and logical dependencies in the text, enabling comprehensive modeling of complex interactions among semantic units. On this basis, the readout and classification modules generate the final category predictions. The proposed method demonstrates significant advantages in robustness alignment experiments, outperforming existing models on ACC, F1-Score, AUC, and Precision, which verifies the effectiveness and stability of the framework. This study not only constructs an integrated framework that balances global and local information as well as semantics and structure, but also provides a new perspective for multi-scale feature fusion and structured semantic modeling in text classification tasks.", "AI": {"tldr": "研究提出了一种结合深度特征提取、多尺度融合和结构化图模型的混合方法，以提升复杂语义场景下的文本分类性能，并在实验中展示了优于现有模型的效果。", "motivation": "研究旨在应对复杂语义场景下的文本分类挑战，通过综合运用深度学习、多尺度融合和图结构化建模来改进分类方法。", "method": "该研究的方法包括：1. 使用大型语言模型提取文本的深层语义特性；2. 利用特征金字塔实现多尺度的语义特征融合；3. 将融合特征转化为图表示，并用图神经网络捕捉潜在的语义关系及逻辑依赖，以进行全面的语义单元互动模型构建。", "result": "研究提出的模型在鲁棒性对齐实验中表现优异，在准确性（ACC）、F1值、AUC和精确度等多个指标上超过了现有模型。", "conclusion": "该研究不仅构建了一个平衡全局与局部信息、语义与结构的集成框架，同时为多尺度特征融合和文本分类任务中的结构化语义建模提供了新视角。"}}
{"id": "2511.05565", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05565", "abs": "https://arxiv.org/abs/2511.05565", "authors": ["Shreyan Ganguly", "Angona Biswas", "Jaydeep Rade", "Md Hasibul Hasan Hasib", "Nabila Masud", "Nitish Singla", "Abhipsa Dash", "Ushashi Bhattacharjee", "Aditya Balu", "Anwesha Sarkar", "Adarsh Krishnamurthy", "Soumik Sarkar"], "title": "In-Context Adaptation of VLMs for Few-Shot Cell Detection in Optical Microscopy", "comment": null, "summary": "Foundation vision-language models (VLMs) excel on natural images, but their utility for biomedical microscopy remains underexplored. In this paper, we investigate how in-context learning enables state-of-the-art VLMs to perform few-shot object detection when large annotated datasets are unavailable, as is often the case with microscopic images. We introduce the Micro-OD benchmark, a curated collection of 252 images specifically curated for in-context learning, with bounding-box annotations spanning 11 cell types across four sources, including two in-lab expert-annotated sets. We systematically evaluate eight VLMs under few-shot conditions and compare variants with and without implicit test-time reasoning tokens. We further implement a hybrid Few-Shot Object Detection (FSOD) pipeline that combines a detection head with a VLM-based few-shot classifier, which enhances the few-shot performance of recent VLMs on our benchmark. Across datasets, we observe that zero-shot performance is weak due to the domain gap; however, few-shot support consistently improves detection, with marginal gains achieved after six shots. We observe that models with reasoning tokens are more effective for end-to-end localization, whereas simpler variants are more suitable for classifying pre-localized crops. Our results highlight in-context adaptation as a practical path for microscopy, and our benchmark provides a reproducible testbed for advancing open-vocabulary detection in biomedical imaging.", "AI": {"tldr": "研究语境学习如何帮助先进的VLMs在缺少大规模标注数据的生物医学显微镜图像中执行少量样本物体检测，提出了专为语境学习设计的基准Micro-OD，并提出了一种混合FSOD流水线，实验证明语境适应法能够改善显微镜领域的物体检测效果。", "motivation": "视觉语言模型（VLMs）在自然图像上表现出色，但对于生物医学显微镜图像的用途仍处于探索阶段，尤其是在大规模的标注数据集不可用的情况下。", "method": "研究了语境学习如何使先进的视觉语言模型（VLMs）在大规模注释数据集不可用的情况下进行少量样本物体检测，提出了Micro-OD基准，用于少量样本物体检测的系统性评价，并实现了一个混合少量样本物体检测（FSOD）流水线。", "result": "观察到零样本性能因领域差异而较弱，但少量样本支持一贯改善检测，带有推理标记的模型对于端到端定位更有效，而简单的变体对于分类预先定位的作物更适宜。", "conclusion": "实验结果表明，语境适应是显微镜领域的一种实际路径，而研究提出的基准为促进生物医学成像中的开放词汇检测提供了可重复的测试平台。"}}
{"id": "2511.05759", "categories": ["cs.CL", "cs.AI", "cs.FL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05759", "abs": "https://arxiv.org/abs/2511.05759", "authors": ["Marcelo Arenas", "Pablo Barceló", "Luis Cofré", "Alexander Kozachinskiy"], "title": "Language Generation: Complexity Barriers and Implications for Learning", "comment": null, "summary": "Kleinberg and Mullainathan showed that, in principle, language generation is always possible: with sufficiently many positive examples, a learner can eventually produce sentences indistinguishable from those of a target language. However, the existence of such a guarantee does not speak to its practical feasibility. In this work, we show that even for simple and well-studied language families -- such as regular and context-free languages -- the number of examples required for successful generation can be extraordinarily large, and in some cases not bounded by any computable function. These results reveal a substantial gap between theoretical possibility and efficient learnability. They suggest that explaining the empirical success of modern language models requires a refined perspective -- one that takes into account structural properties of natural language that make effective generation possible in practice.", "AI": {"tldr": "研究揭示了语言生成理论可能性与实际学习效率之间的巨大差距，指出解释现代语言模型的成功需要新的视角。", "motivation": "理论上语言生成是可行的，但这项工作探讨了这一理论可能性与实际学习效率之间的巨大差距。", "method": "通过分析简单且研究充分的语言族，如正则语言和上下文自由语言，来评估成功生成所需的示例数量。", "result": "发现在某些情况下，所需示例数量大到无法由任何可计算函数界定。", "conclusion": "现代语言模型的实际成功需要一个更精炼的视角，考虑使自然语言的有效生成成为可能的结构性质。"}}
{"id": "2511.05566", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05566", "abs": "https://arxiv.org/abs/2511.05566", "authors": ["Yao Zhang", "Souza Leite Clayton", "Yu Xiao"], "title": "Efficient Online Continual Learning in Sensor-Based Human Activity Recognition", "comment": "13 pages", "summary": "Machine learning models for sensor-based human activity recognition (HAR) are\nexpected to adapt post-deployment to recognize new activities and different\nways of performing existing ones. To address this need, Online Continual\nLearning (OCL) mechanisms have been proposed, allowing models to update their\nknowledge incrementally as new data become available while preserving\npreviously acquired information. However, existing OCL approaches for\nsensor-based HAR are computationally intensive and require extensive labeled\nsamples to represent new changes. Recently, pre-trained model-based (PTM-based)\nOCL approaches have shown significant improvements in performance and\nefficiency for computer vision applications. These methods achieve strong\ngeneralization capabilities by pre-training complex models on large datasets,\nfollowed by fine-tuning on downstream tasks for continual learning. However,\napplying PTM-based OCL approaches to sensor-based HAR poses significant\nchallenges due to the inherent heterogeneity of HAR datasets and the scarcity\nof labeled data in post-deployment scenarios. This paper introduces PTRN-HAR,\nthe first successful application of PTM-based OCL to sensor-based HAR. Unlike\nprior PTM-based OCL approaches, PTRN-HAR pre-trains the feature extractor using\ncontrastive loss with a limited amount of data. This extractor is then frozen\nduring the streaming stage. Furthermore, it replaces the conventional dense\nclassification layer with a relation module network. Our design not only\nsignificantly reduces the resource consumption required for model training\nwhile maintaining high performance, but also improves data efficiency by\nreducing the amount of labeled data needed for effective continual learning, as\ndemonstrated through experiments on three public datasets, outperforming the\nstate-of-the-art. The code can be found here:\nhttps://anonymous.4open.science/r/PTRN-HAR-AF60/", "AI": {"tldr": "本研究介绍了PTRN-HAR，首次将预训练模型基在线连续学习应用于传感器驱动的人类活动识别，并通过三个公开数据集验证了其优越性。", "motivation": "现有的在线连续学习方法对于传感器驱动的人类活动识别而言，计算成本高且需要大量的标记样本。因此，本研究旨在提高效率和性能，以减少模型训练所需的资源并降低标记数据需求。", "method": "研究提出了PTRN-HAR，通过对比损失在有限的数据上预训练特征提取器，并冻结提取器进行逐步学习。同时，替换了传统的密集分类层，使用关系模块网络。", "result": "实验表明，PTRN-HAR在三个公开数据集上表现优于现有方法，并成功减少了用于有效连续学习的任务所需标记数据的数量。", "conclusion": "PTRN-HAR代表了预训练模型基在线连续学习方法在传感器驱动的人类活动识别中的首次成功应用，通过改进的架构设计，它优化了资源消耗并增强了数据效率。"}}
{"id": "2511.05784", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05784", "abs": "https://arxiv.org/abs/2511.05784", "authors": ["Yaxuan Wang", "Chris Yuhao Liu", "Quan Liu", "Jinglong Pang", "Wei Wei", "Yujia Bao", "Yang Liu"], "title": "DRAGON: Guard LLM Unlearning in Context via Negative Detection and Reasoning", "comment": "Please refer to the NeurIPS 2025 submission: https://openreview.net/forum?id=FNuul0hlin; The paper has been accepted to the ICML 2025 MUGen Workshop: https://openreview.net/forum?id=ET24oKP23c", "summary": "Unlearning in Large Language Models (LLMs) is crucial for protecting private data and removing harmful knowledge. Most existing approaches rely on fine-tuning to balance unlearning efficiency with general language capabilities. However, these methods typically require training or access to retain data, which is often unavailable in real world scenarios. Although these methods can perform well when both forget and retain data are available, few works have demonstrated equivalent capability in more practical, data-limited scenarios. To overcome these limitations, we propose Detect-Reasoning Augmented GeneratiON (DRAGON), a systematic, reasoning-based framework that utilizes in-context chain-of-thought (CoT) instructions to guard deployed LLMs before inference. Instead of modifying the base model, DRAGON leverages the inherent instruction-following ability of LLMs and introduces a lightweight detection module to identify forget-worthy prompts without any retain data. These are then routed through a dedicated CoT guard model to enforce safe and accurate in-context intervention. To robustly evaluate unlearning performance, we introduce novel metrics for unlearning performance and the continual unlearning setting. Extensive experiments across three representative unlearning tasks validate the effectiveness of DRAGON, demonstrating its strong unlearning capability, scalability, and applicability in practical scenarios.", "AI": {"tldr": "本文提出DRAGON框架，通过轻量级的检测模块和链式思维引导机制，实现在不访问保留数据的情况下有效擦除大语言模型中的私人或有害数据。", "motivation": "大多数现有方法依赖于微调来平衡遗忘效率和通用语言能力，通常需要训练或访问保留数据。然而，在现实世界场景中这些数据往往是不可用的。DRAGON旨在克服这些限制，展示在实用、数据受限场景下的同等能力。", "method": "提出了一种基于推理的系统性框架Detect-Reasoning Augmented GeneratiON (DRAGON)，该框架利用情境中的链式思维（CoT）指令来保护部署的LLM。DRAGON 不修改基础模型，而是引入了一个轻量级的检测模块，在没有保留数据的情况下识别需要遗忘的提示。这些提示随后会通过专门的CoT守卫模型进行处理，以确保安全的上下文干预。", "result": "在三个代表性的遗忘任务中进行了广泛实验，验证了DRAGON的有效性，展示了其强大的遗忘能力、可扩展性和在实际场景中的适用性。", "conclusion": "DRAGON展示了一种在不修改基础模型的前提下有效实现模型遗忘的方法，具有可扩展性和实用性。"}}
{"id": "2511.05567", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05567", "abs": "https://arxiv.org/abs/2511.05567", "authors": ["Shin Kamada", "Takumi Ichimura"], "title": "Automatic Extraction of Road Networks by using Teacher-Student Adaptive Structural Deep Belief Network and Its Application to Landslide Disaster", "comment": null, "summary": "An adaptive structural learning method of Restricted Boltzmann Machine (RBM) and Deep Belief Network (DBN) has been developed as one of prominent deep learning models. The neuron generation-annihilation algorithm in RBM and layer generation algorithm in DBN make an optimal network structure for given input during the learning. In this paper, our model is applied to an automatic recognition method of road network system, called RoadTracer. RoadTracer can generate a road map on the ground surface from aerial photograph data. A novel method of RoadTracer using the Teacher-Student based ensemble learning model of Adaptive DBN is proposed, since the road maps contain many complicated features so that a model with high representation power to detect should be required. The experimental results showed the detection accuracy of the proposed model was improved from 40.0\\% to 89.0\\% on average in the seven major cities among the test dataset. In addition, we challenged to apply our method to the detection of available roads when landslide by natural disaster is occurred, in order to rapidly obtain a way of transportation. For fast inference, a small size of the trained model was implemented on a small embedded edge device as lightweight deep learning. We reported the detection results for the satellite image before and after the rainfall disaster in Japan.", "AI": {"tldr": "A new method using adaptive DBN for RoadTracer demonstrates improved road detection accuracy and fast post-disaster inference on small devices.", "motivation": "The motivation is to improve the accuracy of road network recognition from aerial images and to enable efficient post-disaster road network assessment by utilizing a high representation power model and lightweight deep learning implementation.", "method": "The paper proposes an adaptive structural learning method for RBM and DBN, which includes a neuron generation-annihilation algorithm for RBM and a layer generation algorithm for DBN. It also introduces the RoadTracer method which uses a Teacher-Student based ensemble learning model of Adaptive DBN to generate road maps from aerial photograph data.", "result": "The proposed model shows an improvement in detection accuracy from 40.0% to 89.0% on average in seven major cities. Additionally, it was successfully applied for rapid post-disaster road availability detection on embedded edge devices.", "conclusion": "The paper concludes that the adaptive learning method for DBN is effective for automatic road network system recognition and post-disaster analysis, showing high accuracy and fast inference capability on embedded devices."}}
{"id": "2511.05852", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05852", "abs": "https://arxiv.org/abs/2511.05852", "authors": ["Yinjie Cheng", "Paul Youssef", "Christin Seifert", "Jörg Schlötterer", "Zhixue Zhao"], "title": "Quantifying Edits Decay in Fine-tuned LLMs", "comment": "Under review at ICLR 2026", "summary": "Knowledge editing has emerged as a lightweight alternative to retraining for correcting or injecting specific facts in large language models (LLMs). Meanwhile, fine-tuning remains the default operation for adapting LLMs to new domains and tasks. Despite their widespread adoption, these two post-training interventions have been studied in isolation, leaving open a crucial question: if we fine-tune an edited model, do the edits survive? This question is motivated by two practical scenarios: removing covert or malicious edits, and preserving beneficial edits. If fine-tuning impairs edits as shown in Figure 1, current KE methods become less useful, as every fine-tuned model would require re-editing, which significantly increases the cost; if edits persist, fine-tuned models risk propagating hidden malicious edits, raising serious safety concerns. To this end, we systematically quantify edits decay after fine-tuning, investigating how fine-tuning affects knowledge editing. We evaluate two state-of-the-art editing methods (MEMIT, AlphaEdit) and three fine-tuning approaches (full-parameter, LoRA, DoRA) across five LLMs and three datasets, yielding 232 experimental configurations. Our results show that edits decay after fine-tuning, with survival varying across configurations, e.g., AlphaEdit edits decay more than MEMIT edits. Further, we propose selective-layer fine-tuning and find that fine-tuning edited layers only can effectively remove edits, though at a slight cost to downstream performance. Surprisingly, fine-tuning non-edited layers impairs more edits than full fine-tuning. Overall, our study establishes empirical baselines and actionable strategies for integrating knowledge editing with fine-tuning, and underscores that evaluating model editing requires considering the full LLM application pipeline.", "AI": {"tldr": "微调会影响模型中的知识编辑效果，研究系统地量化了这一情况，并提出了策略来更好地结合知识编辑和微调过程。", "motivation": "研究探讨了知识编辑和微调结合时编辑是否能保留的问题。这是由两个实际场景所驱动的：一是需要移除隐藏的恶意编辑信息，二是需要保持有益的编辑信息。如果微调破坏了编辑内容，那么当前的知识编辑方法将大大贬值。", "method": "研究系统量化了知识编辑在微调后的衰减情况，评估了两种知识编辑方法（MEMIT和AlphaEdit）和三种微调方法（全参数微调，LoRA和DoRA）在五个语言模型和三个数据集上的效果。", "result": "研究结果表明，微调会导致知识编辑的衰减，不同配置下的编辑效果会有所不同，例如AlphaEdit编辑相比MEMIT编辑衰减更严重。研究还发现对已编辑层进行微调能够有效删除编辑内容，但会对下游任务性能有轻微影响。此外，对未编辑层进行微调会比全微调更严重地影响编辑效果。", "conclusion": "研究确定了将知识编辑和微调结合起来的实证基线和实用策略，并强调评估模型编辑时必须考虑整个大型语言模型应用的流水线。"}}
{"id": "2511.05570", "categories": ["cs.CV", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05570", "abs": "https://arxiv.org/abs/2511.05570", "authors": ["Milad Malekzadeh", "Elias Willberg", "Jussi Torkko", "Silviya Korpilo", "Kamyar Hasanzadeh", "Olle Järv", "Tuuli Toivonen"], "title": "Do Street View Imagery and Public Participation GIS align: Comparative Analysis of Urban Attractiveness", "comment": null, "summary": "As digital tools increasingly shape spatial planning practices, understanding how different data sources reflect human experiences of urban environments is essential. Street View Imagery (SVI) and Public Participation GIS (PPGIS) represent two prominent approaches for capturing place-based perceptions that can support urban planning decisions, yet their comparability remains underexplored. This study investigates the alignment between SVI-based perceived attractiveness and residents' reported experiences gathered via a city-wide PPGIS survey in Helsinki, Finland. Using participant-rated SVI data and semantic image segmentation, we trained a machine learning model to predict perceived attractiveness based on visual features. We compared these predictions to PPGIS-identified locations marked as attractive or unattractive, calculating agreement using two sets of strict and moderate criteria. Our findings reveal only partial alignment between the two datasets. While agreement (with a moderate threshold) reached 67% for attractive and 77% for unattractive places, agreement (with a strict threshold) dropped to 27% and 29%, respectively. By analysing a range of contextual variables, including noise, traffic, population presence, and land use, we found that non-visual cues significantly contributed to mismatches. The model failed to account for experiential dimensions such as activity levels and environmental stressors that shape perceptions but are not visible in images. These results suggest that while SVI offers a scalable and visual proxy for urban perception, it cannot fully substitute the experiential richness captured through PPGIS. We argue that both methods are valuable but serve different purposes; therefore, a more integrated approach is needed to holistically capture how people perceive urban environments.", "AI": {"tldr": "本研究通过比较街景图像（SVI）和公众参与GIS（PPGIS）这两种城市感知数据源，发现SVI在预测城市地区吸引力方面与居民报告的一致性有限。非视觉因素是产生差异的重要原因。两种方法各有优势，但需要整合使用以全面捕捉城市感知。", "motivation": "研究动机在于探索街景图像（SVI）与公众参与GIS（PPGIS）在反映城市环境感知方面的一致性，这两种方法分别代表了捕捉基于地点感知的不同途径，这些感知可以支持城市规划决策。然而，不同数据源之间的可比性尚未得到充分探讨。", "method": "研究方法包括使用参与者评级的街景图像（SVI）数据和语义图像分割，训练机器学习模型基于视觉特征预测感知吸引力，并通过两个严格和宽松的标准比较模型预测与PPGIS识别的具有吸引力或不具吸引力的地点。", "result": "研究结果显示出SVI感知吸引力与居民PPGIS调查结果之间仅存在部分一致。宽松标准下一致性分别为有吸引力地点67%，不具吸引力地点77%，而严格标准下则分别降至27%和29%。通过分析非视觉线索如噪音、交通等，发现这些因素对不匹配程度有显著贡献。", "conclusion": "研究结论为SVI提供了可扩展和视觉化的城市感知代理，但不能完全替代PPGIS捕捉到的体验丰富性。两种方法各自有其价值但也存在着用途上的差异，我们需要一种更综合的方法来全面捕捉人们的都市环境感知方式。"}}
{"id": "2511.05901", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05901", "abs": "https://arxiv.org/abs/2511.05901", "authors": ["Rui Yang", "Matthew Yu Heng Wong", "Huitao Li", "Xin Li", "Wentao Zhu", "Jingchi Liao", "Kunyu Yu", "Jonathan Chong Kai Liew", "Weihao Xuan", "Yingjian Chen", "Yuhe Ke", "Jasmine Chiat Ling Ong", "Douglas Teodoro", "Chuan Hong", "Daniel Shi Wei Ting", "Nan Liu"], "title": "Retrieval-Augmented Generation in Medicine: A Scoping Review of Technical Implementations, Clinical Applications, and Ethical Considerations", "comment": null, "summary": "The rapid growth of medical knowledge and increasing complexity of clinical practice pose challenges. In this context, large language models (LLMs) have demonstrated value; however, inherent limitations remain. Retrieval-augmented generation (RAG) technologies show potential to enhance their clinical applicability. This study reviewed RAG applications in medicine. We found that research primarily relied on publicly available data, with limited application in private data. For retrieval, approaches commonly relied on English-centric embedding models, while LLMs were mostly generic, with limited use of medical-specific LLMs. For evaluation, automated metrics evaluated generation quality and task performance, whereas human evaluation focused on accuracy, completeness, relevance, and fluency, with insufficient attention to bias and safety. RAG applications were concentrated on question answering, report generation, text summarization, and information extraction. Overall, medical RAG remains at an early stage, requiring advances in clinical validation, cross-linguistic adaptation, and support for low-resource settings to enable trustworthy and responsible global use.", "AI": {"tldr": "研究综述了RAG技术在医疗领域中的应用现状，指出了潜在的发展方向与改进领域。", "motivation": "该研究的动机是应对医学知识快速增加和临床实践日益复杂所带来的挑战。", "method": "分析的方法主要包括对检索增强生成（RAG）技术在医疗领域的应用进行综述。", "result": "研究发现RAG技术主要基于公开数据进行，多依赖于英语中心嵌入模型以及通用大语言模型，应用范围集中在问答、报告生成、文本摘要和信息提取上，但存在评估不足等问题。", "conclusion": "总体而言，RAG技术在医疗领域的应用仍处于初期阶段，需在临床验证、跨语言适应与资源短缺环境支持方面进一步发展，以实现全球范围内可靠且负责任的应用。"}}
{"id": "2511.05571", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05571", "abs": "https://arxiv.org/abs/2511.05571", "authors": ["Xiaofei Wang", "Stephen Price", "Chao Li"], "title": "C3-Diff: Super-resolving Spatial Transcriptomics via Cross-modal Cross-content Contrastive Diffusion Modelling", "comment": null, "summary": "The rapid advancement of spatial transcriptomics (ST), i.e., spatial gene expressions, has made it possible to measure gene expression within original tissue, enabling us to discover molecular mechanisms. However, current ST platforms frequently suffer from low resolution, limiting the in-depth understanding of spatial gene expression. Super-resolution approaches promise to enhance ST maps by integrating histology images with gene expressions of profiled tissue spots. However, it remains a challenge to model the interactions between histology images and gene expressions for effective ST enhancement. This study presents a cross-modal cross-content contrastive diffusion framework, called C3-Diff, for ST enhancement with histology images as guidance. In C3-Diff, we firstly analyze the deficiency of traditional contrastive learning paradigm, which is then refined to extract both modal-invariant and content-invariant features of ST maps and histology images. Further, to overcome the problem of low sequencing sensitivity in ST maps, we perform nosing-based information augmentation on the surface of feature unit hypersphere. Finally, we propose a dynamic cross-modal imputation-based training strategy to mitigate ST data scarcity. We tested C3-Diff by benchmarking its performance on four public datasets, where it achieves significant improvements over competing methods. Moreover, we evaluate C3-Diff on downstream tasks of cell type localization, gene expression correlation and single-cell-level gene expression prediction, promoting AI-enhanced biotechnology for biomedical research and clinical applications. Codes are available at https://github.com/XiaofeiWang2018/C3-Diff.", "AI": {"tldr": "This study proposes C3-Diff, a novel framework for enhancing spatial transcriptomics data using histology images as guidance. By addressing limitations of traditional contrastive learning and incorporating noise-based augmentation and dynamic imputation strategies, C3-Diff achieves significant improvements in data resolution and downstream analysis.", "motivation": "The motivation behind this study is to improve the resolution and understanding of spatial gene expression maps in spatial transcriptomics, which are often deficient due to low resolution and limited histology data integration techniques. The study aims to address these challenges to push the boundaries of molecular mechanism discovery.", "method": "This study introduces C3-Diff, a cross-modal, cross-content contrastive diffusion framework designed to enhance spatial transcriptomics (ST) maps using histology images as a reference. It addresses the limitations of traditional contrastive learning by extracting both modal-invariant and content-invariant features, performing noise-based information augmentation to handle low sequencing sensitivity, and implementing a dynamic cross-modal imputation-based training strategy.", "result": "The experiments on four public datasets demonstrated that C3-Diff outperforms competing methods in enhancing spatial transcriptomic data. Additionally, C3-Diff performed well in downstream tasks, including cell type localization, gene expression correlation analysis, and single-cell-level gene expression prediction.", "conclusion": "C3-Diff has been shown to enhance spatial transcriptomic data effectively, achieving significant performance improvements over existing methods when tested on public datasets. The method also shows promising outcomes in downstream tasks such as cell type localization, gene expression correlation, and single-cell-level gene expression prediction, enhancing applicability for biomedical research and clinical use."}}
{"id": "2511.05913", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05913", "abs": "https://arxiv.org/abs/2511.05913", "authors": ["Hongtao Wang", "Renchi Yang", "Wenqing Lin"], "title": "NILC: Discovering New Intents with LLM-assisted Clustering", "comment": null, "summary": "New intent discovery (NID) seeks to recognize both new and known intents from unlabeled user utterances, which finds prevalent use in practical dialogue systems. Existing works towards NID mainly adopt a cascaded architecture, wherein the first stage focuses on encoding the utterances into informative text embeddings beforehand, while the latter is to group similar embeddings into clusters (i.e., intents), typically by K-Means. However, such a cascaded pipeline fails to leverage the feedback from both steps for mutual refinement, and, meanwhile, the embedding-only clustering overlooks nuanced textual semantics, leading to suboptimal performance. To bridge this gap, this paper proposes NILC, a novel clustering framework specially catered for effective NID. Particularly, NILC follows an iterative workflow, in which clustering assignments are judiciously updated by carefully refining cluster centroids and text embeddings of uncertain utterances with the aid of large language models (LLMs). Specifically, NILC first taps into LLMs to create additional semantic centroids for clusters, thereby enriching the contextual semantics of the Euclidean centroids of embeddings. Moreover, LLMs are then harnessed to augment hard samples (ambiguous or terse utterances) identified from clusters via rewriting for subsequent cluster correction. Further, we inject supervision signals through non-trivial techniques seeding and soft must links for more accurate NID in the semi-supervised setting. Extensive experiments comparing NILC against multiple recent baselines under both unsupervised and semi-supervised settings showcase that NILC can achieve significant performance improvements over six benchmark datasets of diverse domains consistently.", "AI": {"tldr": "文章提出了一个名为NILC的新颖聚类框架，用于解决新意图发现中的问题。该方法通过迭代更新聚类分配并利用大型语言模型（LLMs）提高聚类和文本嵌入的准确性，在多个数据集上展示了优于现有方法的性能。", "motivation": "现有新意图发现（NID）方法采用级联架构，这种方法忽略了两个阶段之间的反馈机制，且纯基于嵌入的聚类方法忽略了细微的文本语义，导致性能欠佳。", "method": "NILC采用迭代工作流程，利用大型语言模型（LLMs）对不确定的语音片段的聚类分配和文本嵌入进行仔细调整。首先使用LLMs创建额外的语义聚类中心，以丰富嵌入欧几里得中心的上下文语义。此外，通过重写从聚类中识别出的难以判别的样本（如模糊或简洁的语句）进一步增强聚类修正。", "result": "实验表明，NILC在无监督和半监督设置下的多个基准数据集上与多个最近的基线相比，能够实现显著的性能提升。", "conclusion": "NILC框架的有效展示了通过迭代流程和大型语言模型增强的新意图发现（NID）技术可以实现更好的性能，特别是在处理难以判别的样本时。"}}
{"id": "2511.05573", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05573", "abs": "https://arxiv.org/abs/2511.05573", "authors": ["Ziyang Liu", "Kevin Valencia", "Justin Cui"], "title": "Video Text Preservation with Synthetic Text-Rich Videos", "comment": null, "summary": "While Text-To-Video (T2V) models have advanced rapidly, they continue to struggle with generating legible and coherent text within videos. In particular, existing models often fail to render correctly even short phrases or words and previous attempts to address this problem are computationally expensive and not suitable for video generation. In this work, we investigate a lightweight approach to improve T2V diffusion models using synthetic supervision. We first generate text-rich images using a text-to-image (T2I) diffusion model, then animate them into short videos using a text-agnostic image-to-video (I2v) model. These synthetic video-prompt pairs are used to fine-tune Wan2.1, a pre-trained T2V model, without any architectural changes. Our results show improvement in short-text legibility and temporal consistency with emerging structural priors for longer text. These findings suggest that curated synthetic data and weak supervision offer a practical path toward improving textual fidelity in T2V generation.", "AI": {"tldr": "我们提出了一种轻量级方法，通过合成监督改进T2V扩散模型，通过生成富含文本的图像，并将它们动画化为视频，以用于微调预训练模型。这种方法提高了短文本的可读性和时序一致性。", "motivation": "虽然文本到视频（T2V）模型已经迅速发展，但它们仍然难以生成清晰且连贯的视频文本。现有的模型往往无法正确渲染短语或单词，而之前试图解决这个问题的方法计算成本高昂且不适合生成视频。", "method": "我们首先使用文本到图像（T2I）扩散模型生成富含文本的图像，然后使用不依赖文本的图像到视频（I2V）模型将这些图像动画化为短视频。使用这些合成视频-提示对来微调预训练的T2V模型Wan2.1，无需进行架构上的任何改动。", "result": "我们的结果表明，在短文本的可读性和时序一致性方面有所改善，并且随着结构先验的出现，对于更长的文本也表现出改进。", "conclusion": "这些结果表明，精心策划的合成数据和弱监督提供了一条实用的方法，可以提高文本在T2V生成中的保真度。"}}
{"id": "2511.05921", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05921", "abs": "https://arxiv.org/abs/2511.05921", "authors": ["Ankan Mullick", "Sukannya Purkayastha", "Saransh Sharma", "Pawan Goyal", "Niloy Ganguly"], "title": "IDALC: A Semi-Supervised Framework for Intent Detection and Active Learning based Correction", "comment": "Paper accepted in IEEE Transactions on Artificial Intelligence (October 2025)", "summary": "Voice-controlled dialog systems have become immensely popular due to their ability to perform a wide range of actions in response to diverse user queries. These agents possess a predefined set of skills or intents to fulfill specific user tasks. But every system has its own limitations. There are instances where, even for known intents, if any model exhibits low confidence, it results in rejection of utterances that necessitate manual annotation. Additionally, as time progresses, there may be a need to retrain these agents with new intents from the system-rejected queries to carry out additional tasks. Labeling all these emerging intents and rejected utterances over time is impractical, thus calling for an efficient mechanism to reduce annotation costs. In this paper, we introduce IDALC (Intent Detection and Active Learning based Correction), a semi-supervised framework designed to detect user intents and rectify system-rejected utterances while minimizing the need for human annotation. Empirical findings on various benchmark datasets demonstrate that our system surpasses baseline methods, achieving a 5-10% higher accuracy and a 4-8% improvement in macro-F1. Remarkably, we maintain the overall annotation cost at just 6-10% of the unlabelled data available to the system. The overall framework of IDALC is shown in Fig. 1", "AI": {"tldr": "本文介绍了IDALC框架，该框架能在减少人工标注成本的同时提高语音控制对话系统的性能，优于基线方法，且标注成本大幅降低。", "motivation": "语音控制对话系统在应对用户查询时表现出强大的能力，但仍有未被覆盖的意图需求，人工标注所有新出现的意图和被拒绝的语句成本极高，因而需要一种有效机制来降低标注成本。", "method": "IDALC (Intent Detection and Active Learning based Correction) 是一个半监督框架，旨在识别用户意图，并修正系统拒绝的语句，同时尽量减少人工标注的需求。", "result": "在多个基准数据集上，IDALC系统比基线方法表现更优，准确率提高了5-10%，宏观F1值提高了4-8%，总体标注成本保持在未标注数据的6-10%以内。", "conclusion": "IDALC框架在降低标注成本的同时，提升了语音控制对话系统的检测准确性和修正能力。"}}
{"id": "2511.05574", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05574", "abs": "https://arxiv.org/abs/2511.05574", "authors": ["Stanislav Selitskiy"], "title": "Elements of Active Continuous Learning and Uncertainty Self-Awareness: a Narrow Implementation for Face and Facial Expression Recognition", "comment": null, "summary": "Reflection on one's thought process and making corrections to it if there exists dissatisfaction in its performance is, perhaps, one of the essential traits of intelligence. However, such high-level abstract concepts mandatory for Artificial General Intelligence can be modelled even at the low level of narrow Machine Learning algorithms. Here, we present the self-awareness mechanism emulation in the form of a supervising artificial neural network (ANN) observing patterns in activations of another underlying ANN in a search for indications of the high uncertainty of the underlying ANN and, therefore, the trustworthiness of its predictions. The underlying ANN is a convolutional neural network (CNN) ensemble employed for face recognition and facial expression tasks. The self-awareness ANN has a memory region where its past performance information is stored, and its learnable parameters are adjusted during the training to optimize the performance. The trustworthiness verdict triggers the active learning mode, giving elements of agency to the machine learning algorithm that asks for human help in high uncertainty and confusion conditions.", "AI": {"tldr": "提出了一种自我意识仿真机制，通过监督人工神经网络观察底层人工神经网络的不确定性和预测可信度，并通过主动学习模式在高不确定性时寻求人类帮助。", "motivation": "反思自己的思维过程并在性能令人不满意时进行修正大概是智能的核心特征之一。然而，这种对未来通用人工智能必不可少的高层次抽象概念甚至可以在狭窄的机器学习算法低层进行建模。", "method": "这里提出了自我意识机制的仿真，形式为监督人工神经网络（ANN），用于观察另一个底层ANN的激活模式，以寻找底层ANN高不确定性的迹象，从而判断其预测的可信度。底层ANN是一个用于人脸识别和面部表情任务的卷积神经网络（CNN）集合。自我意识的ANN有一个存储过去表现信息的记忆区域，其可学习参数在训练过程中进行调整以优化性能。", "result": "可信赖性裁决触发主动学习模式，使机器学习算法在高不确定性和混乱条件下请求人类帮助，因此赋予算法主动权。", "conclusion": "此方法为在机器学习算法中实现自我意识的特性提供了概念验证，展示了在未来实现通用人工智能的一种可能路径。"}}
{"id": "2511.05933", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05933", "abs": "https://arxiv.org/abs/2511.05933", "authors": ["Renfei Zhang", "Manasa Kaniselvan", "Niloofar Mireshghallah"], "title": "Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs", "comment": "`", "summary": "Reinforcement learning (RL) is often credited with improving language model reasoning and generalization at the expense of degrading memorized knowledge. We challenge this narrative by observing that RL-enhanced models consistently outperform their base and supervised fine-tuned (SFT) counterparts on pure knowledge recall tasks, particularly those requiring traversal of hierarchical, structured knowledge (e.g., medical codes). We hypothesize these gains stem not from newly acquired data, but from improved procedural skills in navigating and searching existing knowledge hierarchies within the model parameters. To support this hypothesis, we show that structured prompting, which explicitly guides SFTed models through hierarchical traversal, recovers most of the performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We further find that while prompting improves final-answer accuracy, RL-enhanced models retain superior ability to recall correct procedural paths on deep-retrieval tasks. Finally our layer-wise internal activation analysis reveals that while factual representations (e.g., activations for the statement \"code 57.95 refers to urinary infection\") maintain high cosine similarity between SFT and RL models, query representations (e.g., \"what is code 57.95\") diverge noticeably, indicating that RL primarily transforms how models traverse knowledge rather than the knowledge representation itself.", "AI": {"tldr": "研究表明，增强学习提高了模型在复杂知识结构中的导航和检索能力，即使是在纯知识回溯任务上，与基础模型相比仍保持优势。", "motivation": "挑战强化学习恶化语言模型知识记忆能力的观念，揭示强化学习对模型搜索和导航现有知识结构技能的提升作用。", "method": "通过对比增强模型与基础模型及监督微调模型在纯知识回溯任务上的表现，提出假设强化学习改进了模型在嵌套知识结构中的导航和搜索技能，并通过结构化提示实验和技术分析来验证这一假设。", "result": "结构化提示恢复了大部分性能差距，但仍无法完全实现RL改进模型在深度检索任务中正确程序路径回溯的能力。此外，逐层内部激活分析表明，RL改变了模型如何遍历知识的方式，而不是知识本身的表现形式。", "conclusion": "研究表明强化学习不仅没有牺牲模型的知识记忆能力，反而提高了其在复杂知识结构中的检索能力和导航技能。"}}
{"id": "2511.05575", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05575", "abs": "https://arxiv.org/abs/2511.05575", "authors": ["Weston Bondurant", "Arkaprava Sinha", "Hieu Le", "Srijan Das", "Stephanie Schuckers"], "title": "DiffSwap++: 3D Latent-Controlled Diffusion for Identity-Preserving Face Swapping", "comment": null, "summary": "Diffusion-based approaches have recently achieved strong results in face swapping, offering improved visual quality over traditional GAN-based methods. However, even state-of-the-art models often suffer from fine-grained artifacts and poor identity preservation, particularly under challenging poses and expressions. A key limitation of existing approaches is their failure to meaningfully leverage 3D facial structure, which is crucial for disentangling identity from pose and expression. In this work, we propose DiffSwap++, a novel diffusion-based face-swapping pipeline that incorporates 3D facial latent features during training. By guiding the generation process with 3D-aware representations, our method enhances geometric consistency and improves the disentanglement of facial identity from appearance attributes. We further design a diffusion architecture that conditions the denoising process on both identity embeddings and facial landmarks, enabling high-fidelity and identity-preserving face swaps. Extensive experiments on CelebA, FFHQ, and CelebV-Text demonstrate that DiffSwap++ outperforms prior methods in preserving source identity while maintaining target pose and expression. Additionally, we introduce a biometric-style evaluation and conduct a user study to further validate the realism and effectiveness of our approach. Code will be made publicly available at https://github.com/WestonBond/DiffSwapPP", "AI": {"tldr": "DiffSwap++通过整合3D面部结构特征的扩散模型，改进了人脸交换的质量和身份保持性。", "motivation": "现有的基于扩散模型的人脸交换方法在保持身份和处理复杂姿态和表情方面表现不佳。通过融合3D人脸特征，DiffSwap++旨在解决这些问题，以提高图像质量和身份保存效果。", "method": "DiffSwap++ 提出了一种新的基于扩散模型的人脸交换方法，该方法在训练过程中融入了3D人脸潜在特征，以提高几何一致性和面部身份与外观属性的分离度。该方法还在去噪过程中基于身份嵌入和面部特征点进行条件控制，以实现高保真度和身份保持的脸部交换。", "result": "在CelebA、FFHQ和CelebV-Text数据集上的广泛实验显示，DiffSwap++在保持源身份的同时，维持目标的姿态和表情方面优于先前的方法。", "conclusion": "通过采用3D感知的表示方法，DiffSwap++在人脸交换中增强了几何一致性并促进了面部身份与姿态和表情之间的分离。实验和用户研究验证了该方法的逼真性和有效性。"}}
{"id": "2511.05969", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05969", "abs": "https://arxiv.org/abs/2511.05969", "authors": ["Anton Kolonin", "Anna Arinicheva"], "title": "Interpretable Recognition of Cognitive Distortions in Natural Language Texts", "comment": "9 pages, 4 figures", "summary": "We propose a new approach to multi-factor classification of natural language texts based on weighted structured patterns such as N-grams, taking into account the heterarchical relationships between them, applied to solve such a socially impactful problem as the automation of detection of specific cognitive distortions in psychological care, relying on an interpretable, robust and transparent artificial intelligence model. The proposed recognition and learning algorithms improve the current state of the art in this field. The improvement is tested on two publicly available datasets, with significant improvements over literature-known F1 scores for the task, with optimal hyper-parameters determined, having code and models available for future use by the community.", "AI": {"tldr": "本文提出了一种基于加权N-gram模式的多因素分类方法，解决了认知扭曲自动检测的问题，实现了优于现有方法的表现。", "motivation": "研究动机在于解决心理护理中检测认知扭曲的自动化问题，依赖于透明且可解释的人工智能模型来改善当前研究状态。", "method": "本研究提出了一种基于加权结构模式（如N-gram）的多因素自然语言文本分类新方法，该方法考虑了这些模式之间的异层关系，应用于心理护理中的特定认知扭曲自动检测这一具有社会影响的问题。通过可解释性强、鲁棒且透明的人工智能模型来实现这一目标。", "result": "通过这种方法，研究在两个公开的数据集上显著提升了已知的F1分数，并确定了最优的超参数。", "conclusion": "所提出的方法在检测特定认知扭曲任务上表现出改进，并且提供了可供社区未来使用的代码和模型。"}}
{"id": "2511.05590", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05590", "abs": "https://arxiv.org/abs/2511.05590", "authors": ["Yoojin Oh", "Junhyug Noh"], "title": "Beyond Softmax: Dual-Branch Sigmoid Architecture for Accurate Class Activation Maps", "comment": "Accepted at BMVC 2025", "summary": "Class Activation Mapping (CAM) and its extensions have become indispensable tools for visualizing the evidence behind deep network predictions. However, by relying on a final softmax classifier, these methods suffer from two fundamental distortions: additive logit shifts that arbitrarily bias importance scores, and sign collapse that conflates excitatory and inhibitory features. We propose a simple, architecture-agnostic dual-branch sigmoid head that decouples localization from classification. Given any pretrained model, we clone its classification head into a parallel branch ending in per-class sigmoid outputs, freeze the original softmax head, and fine-tune only the sigmoid branch with class-balanced binary supervision. At inference, softmax retains recognition accuracy, while class evidence maps are generated from the sigmoid branch -- preserving both magnitude and sign of feature contributions. Our method integrates seamlessly with most CAM variants and incurs negligible overhead. Extensive evaluations on fine-grained tasks (CUB-200-2011, Stanford Cars) and WSOL benchmarks (ImageNet-1K, OpenImages30K) show improved explanation fidelity and consistent Top-1 Localization gains -- without any drop in classification accuracy. Code is available at https://github.com/finallyupper/beyond-softmax.", "AI": {"tldr": "This paper presents a dual-branch network design that separates localization from classification to provide unbiased and more accurate class evidence mapping in deep learning without affecting classification accuracy.", "motivation": "The motivation behind this paper is to address the issues of bias in importance scores and the loss of feature contribution sign in methods like CAM by proposing a more accurate way of visualizing class evidence in deep network predictions.", "method": "Our method involves creating a dual-branch network where one branch retains the softmax classifier for maintaining classification accuracy, and the other branch, a sigmoid classifier, is used for generating class evidence maps without the biases present in softmax.", "result": "The evaluations on fine-grained tasks and WSOL benchmarks demonstrate that this method improves explanation fidelity and localization accuracy without a decrease in classification performance.", "conclusion": "The conclusion is that the proposed architecture-agnostic dual-branch sigmoid head achieves better visualization of class evidence by avoiding the biases inherent to the softmax classifier used in traditional CAM approaches."}}
{"id": "2511.05993", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05993", "abs": "https://arxiv.org/abs/2511.05993", "authors": ["Renren Jin", "Pengzhi Gao", "Yuqi Ren", "Zhuowen Han", "Tongxuan Zhang", "Wuwei Huang", "Wei Liu", "Jian Luan", "Deyi Xiong"], "title": "Revisiting Entropy in Reinforcement Learning for Large Reasoning Models", "comment": "16 pages, 11 figures, 3 tables", "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a predominant approach for enhancing the reasoning capabilities of large language models (LLMs). However, the entropy of LLMs usually collapses during RLVR training, causing premature convergence to suboptimal local minima and hinder further performance improvement. Although various approaches have been proposed to mitigate entropy collapse, a comprehensive study of entropy in RLVR remains lacking. To address this gap, we conduct extensive experiments to investigate the entropy dynamics of LLMs trained with RLVR and analyze how model entropy correlates with response diversity, calibration, and performance across various benchmarks. Our findings reveal that the number of off-policy updates, the diversity of training data, and the clipping thresholds in the optimization objective are critical factors influencing the entropy of LLMs trained with RLVR. Moreover, we theoretically and empirically demonstrate that tokens with positive advantages are the primary contributors to entropy collapse, and that model entropy can be effectively regulated by adjusting the relative loss weights of tokens with positive and negative advantages during training.", "AI": {"tldr": "本文分析了用RLVR训练的LLM中的熵动态，发现改进模型熵和性能的关键因素，并提出通过调整特定标记的损失权重来控制熵的方法。", "motivation": "研究动机在于填补在RLVR背景下对熵进行全面研究的空白，解决由于熵坍缩导致的过早收敛于次优局部最小值的问题，阻碍了进一步性能改进。", "method": "本研究通过广泛的实验来调查用RLVR训练的LLM的熵动态，并分析模型熵与响应多样性、校准以及在各种基准上的表现之间的相关性。研究方法包括调整训练参数，如脱策更新次数、训练数据多样性、优化目标中的裁剪阈值，以及在训练过程中调整具有正负面优势的标记的相对损失权重。", "result": "研究表明，脱策更新次数、训练数据多样性、优化目标中的裁剪阈值是影响用RLVR训练的LLM的熵的关键因素。研究还表明，具有正优势的标记是熵坍缩的主要贡献者，通过调整训练中具有正负优势的标记的相对损失权重，可以有效控制模型熵。", "conclusion": "通过理论和实证研究，揭示了用RLVR增强的LLM中熵的相关因素，并提出了通过调整具有正负优势的标记的相对损失权重可以有效控制熵的新见解。"}}
{"id": "2511.05600", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05600", "abs": "https://arxiv.org/abs/2511.05600", "authors": ["Soumyajit Maity", "Pranjal Kamboj", "Sneha Maity", "Rajat Singh", "Sankhadeep Chatterjee"], "title": "Google-MedGemma Based Abnormality Detection in Musculoskeletal radiographs", "comment": "Proceedings of ICICT 2026, London, Springer (Forthcoming, February 2026; Accepted for Publication)", "summary": "This paper proposes a MedGemma-based framework for automatic abnormality detection in musculoskeletal radiographs. Departing from conventional autoencoder and neural network pipelines, the proposed method leverages the MedGemma foundation model, incorporating a SigLIP-derived vision encoder pretrained on diverse medical imaging modalities. Preprocessed X-ray images are encoded into high-dimensional embeddings using the MedGemma vision backbone, which are subsequently passed through a lightweight multilayer perceptron for binary classification. Experimental assessment reveals that the MedGemma-driven classifier exhibits strong performance, exceeding conventional convolutional and autoencoder-based metrics. Additionally, the model leverages MedGemma's transfer learning capabilities, enhancing generalization and optimizing feature engineering. The integration of a modern medical foundation model not only enhances representation learning but also facilitates modular training strategies such as selective encoder block unfreezing for efficient domain adaptation. The findings suggest that MedGemma-powered classification systems can advance clinical radiograph triage by providing scalable and accurate abnormality detection, with potential for broader applications in automated medical image analysis.\n  Keywords: Google MedGemma, MURA, Medical Image, Classification.", "AI": {"tldr": "研究提出了一种使用MedGemma框架改进肌肉骨骼放射图异常检测的方法，该方法表现良好，并具备迁移学习能力。", "motivation": "提出一种基于MedGemma框架的肌肉骨骼放射图异常自动检测方法，以改进传统自编码器和神经网络管道的性能。", "method": "通过将MedGemma视觉骨干用于高维嵌入预处理X光图像，并将其通过轻量级多层感知器进行二元分类。", "result": "实验表明，MedGemma驱动的分类器性能优异，超过了传统的卷积和自编码器指标。此外，模型利用MedGemma的迁移学习能力，增强了泛化和特征工程。", "conclusion": "发现表明，基于MedGemma的分类系统可以通过提供可扩展和准确的异常检测来推进临床放射分类，并可能在自动化医学图像分析中具有更广泛的应用。"}}
{"id": "2511.06000", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.06000", "abs": "https://arxiv.org/abs/2511.06000", "authors": ["Favour Yahdii Aghaebe", "Tanefa Apekey", "Elizabeth Williams", "Nafise Sadat Moosavi"], "title": "LLMs Do Not See Age: Assessing Demographic Bias in Automated Systematic Review Synthesis", "comment": "Accepted at AACL 2025", "summary": "Clinical interventions often hinge on age: medications and procedures safe for adults may be harmful to children or ineffective for older adults. However, as language models are increasingly integrated into biomedical evidence synthesis workflows, it remains uncertain whether these systems preserve such crucial demographic distinctions. To address this gap, we evaluate how well state-of-the-art language models retain age-related information when generating abstractive summaries of biomedical studies. We construct DemogSummary, a novel age-stratified dataset of systematic review primary studies, covering child, adult, and older adult populations. We evaluate three prominent summarisation-capable LLMs, Qwen (open-source), Longformer (open-source) and GPT-4.1 Nano (proprietary), using both standard metrics and a newly proposed Demographic Salience Score (DSS), which quantifies age-related entity retention and hallucination. Our results reveal systematic disparities across models and age groups: demographic fidelity is lowest for adult-focused summaries, and under-represented populations are more prone to hallucinations. These findings highlight the limitations of current LLMs in faithful and bias-free summarisation and point to the need for fairness-aware evaluation frameworks and summarisation pipelines in biomedical NLP.", "AI": {"tldr": "研究发现，当前的大型语言模型在生成摘要时存在年龄相关的局限性，尤其是成人摘要的人口统计保真度最低，而代表性不足的人群更容易出现虚构。强调了需要在生物医学自然语言处理中开发更公正的评估框架和摘要生成管道。", "motivation": "虽然语言模型被越来越多地整合到生物医学证据合成工作流程中，但这些系统是否能够保留像年龄这样的重要人口特征仍旧不确定。为了弥补这一差距，研究了大量的语言模型在生成生物医学研究的抽象摘要时是否保留了年龄相关信息。", "method": "构建了一个名为DemogSummary的新数据集，该数据集依据年龄分层，涵盖了儿童、成人和老年成人人群的系统综述初级研究。评估了三个具有摘要生成能力的大型语言模型：Qwen（开源）、Longformer（开源）和GPT-4.1 Nano（专有），使用标准评估指标和新提出的Demographic Salience Score（DSS），该指标量化了年龄相关实体的保留和虚构情况。", "result": "发现模型在生成成人聚焦摘要时，人口统计学保真度最低，而代表性不足的人群更容易出现虚构。", "conclusion": "这些发现揭示了当前大型语言模型在忠实且无偏见的摘要生成方面的局限性，并指出了在生物医学自然语言处理中需要公正的评估框架和摘要生成管道。"}}
{"id": "2511.05604", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05604", "abs": "https://arxiv.org/abs/2511.05604", "authors": ["Subash Gautam", "Alejandro Vargas-Uscategui", "Peter King", "Hans Lohr", "Alireza Bab-Hadiashar", "Ivan Cole", "Ehsan Asadi"], "title": "In-process 3D Deviation Mapping and Defect Monitoring (3D-DM2) in High Production-rate Robotic Additive Manufacturing", "comment": null, "summary": "Additive manufacturing (AM) is an emerging digital manufacturing technology to produce complex and freeform objects through a layer-wise deposition. High deposition rate robotic AM (HDRRAM) processes, such as cold spray additive manufacturing (CSAM), offer significantly increased build speeds by delivering large volumes of material per unit time. However, maintaining shape accuracy remains a critical challenge, particularly due to process instabilities in current open-loop systems. Detecting these deviations as they occur is essential to prevent error propagation, ensure part quality, and minimize post-processing requirements. This study presents a real-time monitoring system to acquire and reconstruct the growing part and directly compares it with a near-net reference model to detect the shape deviation during the manufacturing process. The early identification of shape inconsistencies, followed by segmenting and tracking each deviation region, paves the way for timely intervention and compensation to achieve consistent part quality.", "AI": {"tldr": "该研究旨在解决冷喷涂增材制造过程中形状精度保持的难题，并提出实时监控系统来检测形状偏差并进行干预补偿。", "motivation": "当前的开放式系统在保持形状精度方面存在关键挑战，尤其是由于工艺不稳定导致的偏差，这会引发误差传播和增加后处理需求，因此需要实时检测这些偏差。", "method": "本研究提出了一种实时监控系统，可以获取并重建正在增材制造中的零件，并直接将其与近净参考模型进行比较，以检测制造过程中的形状偏差。", "result": "通过早期识别形状不一致，分段并跟踪每个偏差区域，为及时干预和补偿以实现一致的零件质量提供了方法。", "conclusion": "实时监测系统对于实现高质量零件至关重要，它通过早期检测并及时补偿形状偏差，提升了工艺稳定性和零件质量。"}}
{"id": "2511.06023", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.06023", "abs": "https://arxiv.org/abs/2511.06023", "authors": ["Deng Yixuan", "Ji Xiaoqiang"], "title": "Multi-Reward GRPO Fine-Tuning for De-biasing Large Language Models: A Study Based on Chinese-Context Discrimination Data", "comment": null, "summary": "Large Language Models (LLMs) often exhibit implicit biases and discriminatory tendencies that reflect underlying social stereotypes. While recent alignment techniques such as RLHF and DPO have mitigated some of these issues, they remain limited in addressing culturally specific and multi-dimensional forms of discrimination. This paper proposes a Multi-Reward Group Relative Policy Optimization (GRPO) framework to fine-tune LLMs toward ethical and bias-free behavior. Our approach constructs a synthetic English-language dataset derived from Chinese-context discrimination categories, including regional, ethnic, and occupational biases. Each instance is paired with both neutral and biased responses to train a reward model based on DeBERTa-v3, which provides multi-dimensional reward signals capturing fairness, neutrality, and linguistic quality. The trained reward model then guides GRPO fine-tuning to optimize model outputs along these ethical dimensions. Experimental results demonstrate significant reductions in bias intensity and improved alignment with non-discriminatory standards without compromising fluency or informativeness. This study highlights the effectiveness of GRPO-based multi-reward optimization for de-biasing LLMs and offers a replicable framework for cultural-contextual ethical alignment.", "AI": {"tldr": "本文提出了一种名为Multi-Reward Group Relative Policy Optimization (GRPO) 的框架，用于微调大型语言模型（LLMs），使这些模型展现出更公平和无偏的行为。", "motivation": "尽管最近的对齐技术如RLHF和DPO缓解了一些偏见问题，但它们在解决文化和多维形式的歧视方面仍然存在局限。本文旨在提出一种更有效的方法来解决这些问题。", "method": "本研究构建了一个基于中文背景歧视类别的合成英文数据集，采用了DeBERTa-v3模型进行训练，并提供多维度的奖励信号，以此指导GRPO框架对模型进行调整。", "result": "实验结果表明，使用GRPO方法能够显著降低偏见强度，并且在不损害流畅性和信息性的情况下改善了与非歧视标准的一致性。", "conclusion": "本文展示了GRPO框架在去除LLMs偏见的有效性，并为文化情景的伦理对齐提供了一个可复制的框架。"}}
{"id": "2511.05609", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05609", "abs": "https://arxiv.org/abs/2511.05609", "authors": ["Ziying Li", "Xuequan Lu", "Xinkui Zhao", "Guanjie Cheng", "Shuiguang Deng", "Jianwei Yin"], "title": "Walking the Schrödinger Bridge: A Direct Trajectory for Text-to-3D Generation", "comment": "NeurIPS 2025; https://github.com/emmaleee789/TraCe.git", "summary": "Recent advancements in optimization-based text-to-3D generation heavily rely on distilling knowledge from pre-trained text-to-image diffusion models using techniques like Score Distillation Sampling (SDS), which often introduce artifacts such as over-saturation and over-smoothing into the generated 3D assets. In this paper, we address this essential problem by formulating the generation process as learning an optimal, direct transport trajectory between the distribution of the current rendering and the desired target distribution, thereby enabling high-quality generation with smaller Classifier-free Guidance (CFG) values. At first, we theoretically establish SDS as a simplified instance of the Schrödinger Bridge framework. We prove that SDS employs the reverse process of an Schrödinger Bridge, which, under specific conditions (e.g., a Gaussian noise as one end), collapses to SDS's score function of the pre-trained diffusion model. Based upon this, we introduce Trajectory-Centric Distillation (TraCe), a novel text-to-3D generation framework, which reformulates the mathematically trackable framework of Schrödinger Bridge to explicitly construct a diffusion bridge from the current rendering to its text-conditioned, denoised target, and trains a LoRA-adapted model on this trajectory's score dynamics for robust 3D optimization. Comprehensive experiments demonstrate that TraCe consistently achieves superior quality and fidelity to state-of-the-art techniques.", "AI": {"tldr": "提出TraCe方法，通过构造扩散桥梁解决3D生成中的质量缺陷，实验结果显示优于现有技术。", "motivation": "解决基于优化的文本到3D生成技术中引入的如过度饱和和过度平滑等缺陷，特别是在使用Score Distillation Sampling（SDS）等技术时。", "method": "通过将生成过程建模为学习当前渲染与目标分布之间最优直接传输轨迹，提出Trajectory-Centric Distillation（TraCe）框架，该框架基于Schrödinger Bridge的数学框架显式构建扩散桥梁，对LoRA适配模型进行训练，以提高3D优化的鲁棒性。", "result": "实验结果表明，TraCe在质量和保真度方面优于最先进的技术。", "conclusion": "TraCe框架为改进文本到3D生成的质量提供了有效方法，展示了在未来3D生成中的潜力。"}}
{"id": "2511.06048", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06048", "abs": "https://arxiv.org/abs/2511.06048", "authors": ["Xinyuan Yan", "Shusen Liu", "Kowshik Thopalli", "Bei Wang"], "title": "Visual Exploration of Feature Relationships in Sparse Autoencoders with Curated Concepts", "comment": "8 pages (5 main paper+3 refernce), 2 figures, pulished at Mechanistic Interpretability Workshop at NeurIPS 2025", "summary": "Sparse autoencoders (SAEs) have emerged as a powerful tool for uncovering interpretable features in large language models (LLMs) through the sparse directions they learn. However, the sheer number of extracted directions makes comprehensive exploration intractable. While conventional embedding techniques such as UMAP can reveal global structure, they suffer from limitations including high-dimensional compression artifacts, overplotting, and misleading neighborhood distortions. In this work, we propose a focused exploration framework that prioritizes curated concepts and their corresponding SAE features over attempts to visualize all available features simultaneously. We present an interactive visualization system that combines topology-based visual encoding with dimensionality reduction to faithfully represent both local and global relationships among selected features. This hybrid approach enables users to investigate SAE behavior through targeted, interpretable subsets, facilitating deeper and more nuanced analysis of concept representation in latent space.", "AI": {"tldr": "开发了一种交互式可视化系统，专注于稀疏自动编码器学习的特征的解释，通过结合拓扑可视化编码和降维方法来解决高维数据的可视化挑战。", "motivation": "传统的嵌入技术如UMAP等，虽然可以揭示全局结构，但仍存在高维压缩伪影、过绘和误导性的邻域扭曲等问题。同时，由于稀疏自动编码器（SAE）生成的方向数量庞大，全面探索变得不可行。", "method": "提出了一种聚焦探索框架，该框架优先考虑策划的概念及其相应的稀疏自动编码器特征，而不是尝试同时可视化所有可用特征。通过结合基于拓扑的可视化编码与降维技术，系统可以真实地表示选定特征之间的局部和全局关系，从而支持对潜在空间中概念表示进行更深入和细致的分析。", "result": "该方法能够忠实表达选中特征的局部和全局的相关性，使得用户可以通过有针对性的、可解释的子集调查SAE的行为。", "conclusion": "这种方法支持对潜在空间中概念表示进行深入和细致的分析。"}}
{"id": "2511.05611", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05611", "abs": "https://arxiv.org/abs/2511.05611", "authors": ["Shuaikang Zhu", "Yang Yang", "Chen Sun"], "title": "Pose-Aware Multi-Level Motion Parsing for Action Quality Assessment", "comment": null, "summary": "Human pose serves as a cornerstone of action quality assessment (AQA), where subtle spatial-temporal variations in pose often distinguish excellence from mediocrity. In high-level competitions, these nuanced differences become decisive factors in scoring. In this paper, we propose a novel multi-level motion parsing framework for AQA based on enhanced spatial-temporal pose features. On the first level, the Action-Unit Parser is designed with the help of pose extraction to achieve precise action segmentation and comprehensive local-global pose representations. On the second level, Motion Parser is used by spatial-temporal feature learning to capture pose changes and appearance details for each action-unit. Meanwhile, some special conditions other than body-related will impact action scoring, like water splash in diving. In this work, we design an additional Condition Parser to offer users more flexibility in their choices. Finally, Weight-Adjust Scoring Module is introduced to better accommodate the diverse requirements of various action types and the multi-scale nature of action-units. Extensive evaluations on large-scale diving sports datasets demonstrate that our multi-level motion parsing framework achieves state-of-the-art performance in both action segmentation and action scoring tasks.", "AI": {"tldr": "本文提出了一种基于增强时空姿态特征的多级动作解析框架，用于动作质量评估。该框架包括姿态单元解析器、动作解析器、条件解析器和加权评分模块。在大规模跳水运动数据集上的评估显示，框架在动作分割和评分任务上达到了最先进的性能。", "motivation": "文章旨在解决高水准比赛中动作质量评估中对细微时空差异的区分问题，这些细微差异是决定评分的关键因素。", "method": "Structure", "result": "{\"tldr\": \"本文提出了一种基于增强时空姿态特征的多级动作解析框架，用于动作质量评估。该框架包括姿态单元解析器、动作解析器、条件解析器和加权评分模块。在大规模跳水运动数据集上的评估显示，框架在动作分割和评分任务上达到了最先进的性能。\", \"motivation\": \"文章旨在解决高水准比赛中动作质量评估中对细微时空差异的区分问题，这些细微差异是决定评分的关键因素。\", \"method\": \"提出一种多级动作解析框架，包括姿态单元解析器进行精确动作分割和综合姿态表示，动作解析器用于捕捉姿态变化和细节。此外还包括一个条件解析器和加权评分模块。\", \"result\": \"框架在大规模跳水数据集上的评估显示其在动作分割和评分任务上的优越性能。\", \"conclusion\": \"验证了在高水准动作评分中使用增强时空姿态特征的重要性，所提出的多级动作解析框架达到了最先进的评估结果。\"}", "conclusion": "验证了在高水准动作评分中使用增强时空姿态特征的重要性，所提出的多级动作解析框架达到了最先进的评估结果。"}}
{"id": "2511.06051", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.06051", "abs": "https://arxiv.org/abs/2511.06051", "authors": ["Mahmoud El-Bahnasawi"], "title": "Efficient Hate Speech Detection: A Three-Layer LoRA-Tuned BERTweet Framework", "comment": "13 pages, 2 figures", "summary": "This paper addresses the critical challenge of developing computationally efficient hate speech detection systems that maintain competitive performance while being practical for real-time deployment. We propose a novel three-layer framework that combines rule-based pre-filtering with a parameter-efficient LoRA-tuned BERTweet model and continuous learning capabilities. Our approach achieves 0.85 macro F1 score - representing 94% of the performance of state-of-the-art large language models like SafePhi (Phi-4 based) while using a base model that is 100x smaller (134M vs 14B parameters). Compared to traditional BERT-based approaches with similar computational requirements, our method demonstrates superior performance through strategic dataset unification and optimized fine-tuning. The system requires only 1.87M trainable parameters (1.37% of full fine-tuning) and trains in approximately 2 hours on a single T4 GPU, making robust hate speech detection accessible in resource-constrained environments while maintaining competitive accuracy for real-world deployment.", "AI": {"tldr": "The paper presents a computationally efficient and lightweight hate speech detection system, which reaches 0.85 macro F1 score with 134M parameters, achieving 94% performance of advanced models while training quickly on a T4 GPU, making it ideal for real-world deployment.", "motivation": "The motivation behind this work is the need to develop robust, yet lightweight and practical, real-time hate speech detection systems. The focus is on balancing high performance with the ability to function efficiently in environments with limited computational resources.", "method": "The paper proposes a three-layer framework for hate speech detection that includes rule-based pre-filtering, a parameter-efficient LoRA-tuned BERTweet model, and continuous learning mechanisms. The aim is to achieve high performance while remaining computationally efficient for real-time use.", "result": "The framework achieved a 0.85 macro F1 score, which is 94% of the performance of complex language models like SafePhi but with a significantly smaller base model size of 134M parameters versus 14B. It only requires 1.87M trainable parameters and completes training in about 2 hours on a T4 GPU, demonstrating superior performance compared to traditional BERT-based approaches under similar computational constraints.", "conclusion": "The conclusion highlights the effectiveness of the proposed method in delivering competitive performance in hate speech detection while using fewer parameters and faster training times than its counterparts, making it a suitable solution for real-world applications, especially in resource-constrained scenarios."}}
{"id": "2511.05616", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05616", "abs": "https://arxiv.org/abs/2511.05616", "authors": ["Connor Dunlop", "Matthew Zheng", "Kavana Venkatesh", "Pinar Yanardag"], "title": "Personalized Image Editing in Text-to-Image Diffusion Models via Collaborative Direct Preference Optimization", "comment": "Published at NeurIPS'25 Main Conference", "summary": "Text-to-image (T2I) diffusion models have made remarkable strides in generating and editing high-fidelity images from text. Yet, these models remain fundamentally generic, failing to adapt to the nuanced aesthetic preferences of individual users. In this work, we present the first framework for personalized image editing in diffusion models, introducing Collaborative Direct Preference Optimization (C-DPO), a novel method that aligns image edits with user-specific preferences while leveraging collaborative signals from like-minded individuals. Our approach encodes each user as a node in a dynamic preference graph and learns embeddings via a lightweight graph neural network, enabling information sharing across users with overlapping visual tastes. We enhance a diffusion model's editing capabilities by integrating these personalized embeddings into a novel DPO objective, which jointly optimizes for individual alignment and neighborhood coherence. Comprehensive experiments, including user studies and quantitative benchmarks, demonstrate that our method consistently outperforms baselines in generating edits that are aligned with user preferences.", "AI": {"tldr": "This paper presents C-DPO, a method for personalized image editing in diffusion models that encodes users as preference graph nodes, uses a GNN to learn user embeddings, and integrates these into a DPO objective to align edits with user preferences.", "motivation": "The motivation behind this work is to address the limitation of text-to-image (T2I) diffusion models, which fail to adapt to the individual aesthetic preferences of users. The aim is to introduce personalized editing capabilities to these models.", "method": "Our method introduces Collaborative Direct Preference Optimization (C-DPO) for personalized image editing in diffusion models. It encodes users as nodes in a preference graph and learns embeddings with a graph neural network to share information among users with similar tastes. These embeddings are integrated into a DPO objective to jointly optimize for user preference alignment and neighborhood coherence.", "result": "Experiments, including user studies and quantitative benchmarks, show that the proposed method outperforms baselines in generating image edits aligned with specific user preferences.", "conclusion": "The conclusion is that the proposed method of integrating personalized user embeddings into the DPO objective significantly improves the alignment of image edits with user preferences compared to baseline methods."}}
{"id": "2511.06057", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.06057", "abs": "https://arxiv.org/abs/2511.06057", "authors": ["Bingbing Wang", "Zhengda Jin", "Bin Liang", "Jing Li", "Ruifeng Xu"], "title": "ReMoD: Rethinking Modality Contribution in Multimodal Stance Detection via Dual Reasoning", "comment": null, "summary": "Multimodal Stance Detection (MSD) is a crucial task for understanding public opinion on social media. Existing work simply fuses information from various modalities to learn stance representations, overlooking the varying contributions of stance expression from different modalities. Therefore, stance misunderstanding noises may be drawn into the stance learning process due to the risk of learning errors by rough modality combination. To address this, we get inspiration from the dual-process theory of human cognition and propose **ReMoD**, a framework that **Re**thinks **Mo**dality contribution of stance expression through a **D**ual-reasoning paradigm. ReMoD integrates *experience-driven intuitive reasoning* to capture initial stance cues with *deliberate reflective reasoning* to adjust for modality biases, refine stance judgments, and thereby dynamically weight modality contributions based on their actual expressive power for the target stance. Specifically, the intuitive stage queries the Modality Experience Pool (MEP) and Semantic Experience Pool (SEP) to form an initial stance hypothesis, prioritizing historically impactful modalities. This hypothesis is then refined in the reflective stage via two reasoning chains: Modality-CoT updates MEP with adaptive fusion strategies to amplify relevant modalities, while Semantic-CoT refines SEP with deeper contextual insights of stance semantics. These dual experience structures are continuously refined during training and recalled at inference to guide robust and context-aware stance decisions. Extensive experiments on the public MMSD benchmark demonstrate that our ReMoD significantly outperforms most baseline models and exhibits strong generalization capabilities.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.05617", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05617", "abs": "https://arxiv.org/abs/2511.05617", "authors": ["Pouya Shiri", "Amirali Baniasadi"], "title": "Convolutional Fully-Connected Capsule Network (CFC-CapsNet): A Novel and Fast Capsule Network", "comment": null, "summary": "A Capsule Network (CapsNet) is a relatively new classifier and one of the possible successors of Convolutional Neural Networks (CNNs). CapsNet maintains the spatial hierarchies between the features and outperforms CNNs at classifying images including overlapping categories. Even though CapsNet works well on small-scale datasets such as MNIST, it fails to achieve a similar level of performance on more complicated datasets and real applications. In addition, CapsNet is slow compared to CNNs when performing the same task and relies on a higher number of parameters. In this work, we introduce Convolutional Fully-Connected Capsule Network (CFC-CapsNet) to address the shortcomings of CapsNet by creating capsules using a different method. We introduce a new layer (CFC layer) as an alternative solution to creating capsules. CFC-CapsNet produces fewer, yet more powerful capsules resulting in higher network accuracy. Our experiments show that CFC-CapsNet achieves competitive accuracy, faster training and inference and uses less number of parameters on the CIFAR-10, SVHN and Fashion-MNIST datasets compared to conventional CapsNet.", "AI": {"tldr": "CFC-CapsNet addresses the limitations of CapsNet by creating fewer, more powerful capsules leading to higher accuracy and efficiency.", "motivation": "CapsNet has limitations such as poor performance on complex datasets and slower speed compared to CNNs. CFC-CapsNet aims to improve network accuracy and reduce computational costs.", "method": "CFC-CapsNet introduces a new layer (CFC layer) that creates fewer but more powerful capsules compared to CapsNet.", "result": "On datasets such as CIFAR-10, SVHN, and Fashion-MNIST, CFC-CapsNet demonstrates higher accuracy and faster training and inference compared to traditional CapsNet, while using fewer parameters.", "conclusion": "The experiment results indicate that CFC-CapsNet is a competitive choice compared to the standard CapsNet in terms of performance, speed, and parameter efficiency."}}
{"id": "2511.06067", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.06067", "abs": "https://arxiv.org/abs/2511.06067", "authors": ["Haoyue Yang", "Xuanle Zhao", "Yujie Liu", "Zhuojun Zou", "Kailin Lyu", "Changchun Zhou", "Yao Zhu", "Jie Hao"], "title": "Automating Hardware Design and Verification from Architectural Papers via a Neural-Symbolic Graph Framework", "comment": "Preprint Version, Work in Progress", "summary": "The reproduction of hardware architectures from academic papers remains a significant challenge due to the lack of publicly available source code and the complexity of hardware description languages (HDLs). To this end, we propose \\textbf{ArchCraft}, a Framework that converts abstract architectural descriptions from academic papers into synthesizable Verilog projects with register-transfer level (RTL) verification. ArchCraft introduces a structured workflow, which uses formal graphs to capture the Architectural Blueprint and symbols to define the Functional Specification, translating unstructured academic papers into verifiable, hardware-aware designs. The framework then generates RTL and testbench (TB) code decoupled via these symbols to facilitate verification and debugging, ultimately reporting the circuit's Power, Area, and Performance (PPA). Moreover, we propose the first benchmark, \\textbf{ArchSynthBench}, for synthesizing hardware from architectural descriptions, with a complete set of evaluation indicators, 50 project-level circuits, and around 600 circuit blocks. We systematically assess ArchCraft on ArchSynthBench, where the experiment results demonstrate the superiority of our proposed method, surpassing direct generation methods and the VerilogCoder framework in both paper understanding and code completion. Furthermore, evaluation and physical implementation of the generated executable RTL code show that these implementations meet all timing constraints without violations, and their performance metrics are consistent with those reported in the original papers.", "AI": {"tldr": "ArchCraft是一个将学术论文中的抽象架构描述转化为可综合的Verilog项目的框架，通过形式化的图和符号定义架构蓝图和功能规范，生成RTL和测试平台代码，提供Power、Area和Performance评估。ArchSynthBench作为首个针对架构描述硬件综合的基准集，用于评估ArchCraft的有效性。实验结果表明ArchCraft优于直接生成方法和VerilogCoder框架。", "motivation": "由于源代码缺乏和硬件描述语言的复杂性，重现实验硬件架构从学术论文仍然是一项挑战。", "method": "ArchCraft采用结构化工作流，使用形式化的图记录架构蓝图和符号定义功能规范，将非结构化的学术论文转化为可验证的硬件感知设计。框架生成解耦的RTL和测试平台（TB）代码，以便利验证和调试，从而报告电路的P、A和P指标。", "result": "本研究提出了ArchSynthBench基准集进行系统评估，实验结果显示ArchCraft在理解论文和代码生成两方面均超越直接生成方法和VerilogCoder框架。此外，评估生成的RTL代码的执行性能显示，所实现的电路性能指标与原始论文报告一致，并满足所有时序约束。", "conclusion": "ArchCraft框架有力地解决了根据学术论文实现硬件架构的挑战，提供了从架构描述到RTL代码生成的完整流程，并通过ArchSynthBench展示了其优越性。"}}
{"id": "2511.05622", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05622", "abs": "https://arxiv.org/abs/2511.05622", "authors": ["Nicholas Babey", "Tiffany Gu", "Yiheng Li", "Cristian Meo", "Kevin Zhu"], "title": "Grounding Foundational Vision Models with 3D Human Poses for Robust Action Recognition", "comment": "Accepted at NeurIPS 2025 SpaVLE, for code see https://github.com/nbabey20/groundactrec , 9 pages, 1 figure", "summary": "For embodied agents to effectively understand and interact within the world around them, they require a nuanced comprehension of human actions grounded in physical space. Current action recognition models, often relying on RGB video, learn superficial correlations between patterns and action labels, so they struggle to capture underlying physical interaction dynamics and human poses in complex scenes. We propose a model architecture that grounds action recognition in physical space by fusing two powerful, complementary representations: V-JEPA 2's contextual, predictive world dynamics and CoMotion's explicit, occlusion-tolerant human pose data. Our model is validated on both the InHARD and UCF-19-Y-OCC benchmarks for general action recognition and high-occlusion action recognition, respectively. Our model outperforms three other baselines, especially within complex, occlusive scenes. Our findings emphasize a need for action recognition to be supported by spatial understanding instead of statistical pattern recognition.", "AI": {"tldr": "提出了一种新的模型架构，通过融合V-JEPA 2和CoMotion的数据，提高了在复杂遮挡场景中的动作识别性能。", "motivation": "当前的动作识别模型主要依赖RGB视频，学习的是图案和动作标签之间的表面关系，难以捕捉到基础的物理交互动态和人类姿态，尤其在复杂场景中表现不佳。因此需要一种新的模型提高其在复杂遮挡场景中的性能。", "method": "我们提出了一种新的模型架构，通过融合V-JEPA 2的上下文预测世界动态信息和CoMotion的显式遮挡容忍人体姿态数据来进行基于物理空间的动作识别。这种方法旨在捕捉复杂场景中的人类动作和姿态信息。", "result": "该模型在InHARD和UCF-19-Y-OCC数据集上进行了验证，前者用于一般动作识别，后者用于高遮挡动作识别。实验结果表明，与三个基准模型相比，我们的模型在复杂遮挡场景中表现优异。", "conclusion": "研究结果强调了动作识别需要依赖空间理解，而非仅仅是统计模式识别，这对提高模型在复杂场景下的动作识别性能至关重要。"}}
{"id": "2511.06073", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.06073", "abs": "https://arxiv.org/abs/2511.06073", "authors": ["Simeon Emanuilov", "Richard Ackermann"], "title": "Stemming Hallucination in Language Models Using a Licensing Oracle", "comment": "23 pages, 4 figures, 8 tables. Introduces the Licensing Oracle, an architectural solution for eliminating hallucinations in language models through formal SHACL validation against knowledge graphs. All datasets and models are available at https://huggingface.co/collections/s-emanuilov/licensing-oracle-experiments", "summary": "Language models exhibit remarkable natural language generation capabilities but remain prone to hallucinations, generating factually incorrect information despite producing syntactically coherent responses. This study introduces the Licensing Oracle, an architectural solution designed to stem hallucinations in LMs by enforcing truth constraints through formal validation against structured knowledge graphs. Unlike statistical approaches that rely on data scaling or fine-tuning, the Licensing Oracle embeds a deterministic validation step into the model's generative process, ensuring that only factually accurate claims are made. We evaluated the effectiveness of the Licensing Oracle through experiments comparing it with several state-of-the-art methods, including baseline language model generation, fine-tuning for factual recall, fine-tuning for abstention behavior, and retrieval-augmented generation (RAG). Our results demonstrate that although RAG and fine-tuning improve performance, they fail to eliminate hallucinations. In contrast, the Licensing Oracle achieved perfect abstention precision (AP = 1.0) and zero false answers (FAR-NE = 0.0), ensuring that only valid claims were generated with 89.1% accuracy in factual responses. This work shows that architectural innovations, such as the Licensing Oracle, offer a necessary and sufficient solution for hallucinations in domains with structured knowledge representations, offering guarantees that statistical methods cannot match. Although the Licensing Oracle is specifically designed to address hallucinations in fact-based domains, its framework lays the groundwork for truth-constrained generation in future AI systems, providing a new path toward reliable, epistemically grounded models.", "AI": {"tldr": "The Licensing Oracle is proposed as an architectural solution to reduce factual hallucinations in language models. It performs better than statistical methods, ensuring high accuracy in factual responses by validating claims against structured knowledge graphs.", "motivation": "Language models, while capable of producing coherent text, often generate factually incorrect information. This paper aims to reduce such hallucinations by proposing a deterministic validation method embedded within the model's architecture.", "method": "This study introduces the Licensing Oracle, an architectural solution for curtailing hallucinations in language models. The approach embeds a formal validation step against structured knowledge graphs, ensuring that only factually accurate claims are made.", "result": "Experiments show that the Licensing Oracle achieved 100% abstention precision and zero false answers, generating a 89.1% accuracy in factual responses. This is superior to statistical methods, which reduce but do not eliminate hallucinations.", "conclusion": "The research concludes that architectural innovations like the Licensing Oracle provide a robust solution to factual inaccuracy in language models, setting a foundational framework for future truth-constrained AI systems."}}
{"id": "2511.05623", "categories": ["cs.CV", "cs.LG", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.05623", "abs": "https://arxiv.org/abs/2511.05623", "authors": ["Mariafrancesca Patalano", "Giovanna Capizzi", "Kamran Paynabar"], "title": "Registration-Free Monitoring of Unstructured Point Cloud Data via Intrinsic Geometrical Properties", "comment": null, "summary": "Modern sensing technologies have enabled the collection of unstructured point cloud data (PCD) of varying sizes, which are used to monitor the geometric accuracy of 3D objects. PCD are widely applied in advanced manufacturing processes, including additive, subtractive, and hybrid manufacturing. To ensure the consistency of analysis and avoid false alarms, preprocessing steps such as registration and mesh reconstruction are commonly applied prior to monitoring. However, these steps are error-prone, time-consuming and may introduce artifacts, potentially affecting monitoring outcomes. In this paper, we present a novel registration-free approach for monitoring PCD of complex shapes, eliminating the need for both registration and mesh reconstruction. Our proposal consists of two alternative feature learning methods and a common monitoring scheme. Feature learning methods leverage intrinsic geometric properties of the shape, captured via the Laplacian and geodesic distances. In the monitoring scheme, thresholding techniques are used to further select intrinsic features most indicative of potential out-of-control conditions. Numerical experiments and case studies highlight the effectiveness of the proposed approach in identifying different types of defects.", "AI": {"tldr": "本文提出了一种无需注册和网格重建的新型点云数据监控方法，通过利用形状的内在几何属性来识别3D对象的潜在缺陷。", "motivation": "传统的点云数据分析前需要进行注册和网格重建，这些步骤容易出错且耗时，可能导致监控结果偏差。因此，本文旨在提出一种更为高效和精确的监控方法。", "method": "该方法包括两种特征学习方法和一个通用监控方案。特征学习方法基于形状的内在几何属性，如拉普拉斯和测地距离。监控方案中使用阈值技术选出最有可能表示出问题的特征。", "result": "数值实验和案例研究证明了该方法在识别不同类型缺陷方面的有效性。", "conclusion": "本文提供的无需注册和网格重建的方法是一种有效的点云数据监控方案，能够在检测复杂形状3D对象的几何准确性中发挥作用。"}}
{"id": "2511.06086", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06086", "abs": "https://arxiv.org/abs/2511.06086", "authors": ["Saurabh Page", "Advait Joshi", "S. S. Sonawane"], "title": "MuonAll: Muon Variant for Efficient Finetuning of Large Language Models", "comment": null, "summary": "Muon optimizer has demonstrated robust results in pretraining of language models but its performance in finetuning of existing public pretrained models is not yet explored. Currently, Muon is used along with AdamW introducing a scope of improvement for adopting all parameters inside Muon. We introduce MuonAll, which incorporates all the parameters inside Muon by transforming into 2D matrices. We conduct extensive finetuning experiments across publicly available language models with model sizes upto half billion parameters. Muon and MuonAll perform at par with AdamW across major benchmarks, highlighting their effectiveness as alternative optimizers. We open-source the distributed implementations of Muon and MuonAll, available at https://github.com/Saurabh750/optimizer", "AI": {"tldr": "MuonAll, an enhanced version of Muon optimizer, achieves comparable performance to AdamW in fine-tuning large language models, and both optimizers' implementations are open-sourced.", "motivation": "To explore the potential of the Muon optimizer for fine-tuning large pre-trained models, as its ability is yet to be fully investigated in this context.", "method": "MuonAll iteratively refines Muon by incorporating all its parameters in 2D matrices, suitable for fine-tuning large existing models.", "result": "MuonAll and Muon fine-tuning performance is on par with AdamW across a variety of language models as demonstrated by extensive experiments.", "conclusion": "MuonAll, alongside the original Muon, can effectively be used as alternative optimizers for fine-tuning large pre-trained language models."}}
{"id": "2511.05681", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05681", "abs": "https://arxiv.org/abs/2511.05681", "authors": ["Sina Malakouti", "Boqing Gong", "Adriana Kovashka"], "title": "Culture in Action: Evaluating Text-to-Image Models through Social Activities", "comment": null, "summary": "Text-to-image (T2I) diffusion models achieve impressive photorealism by training on large-scale web data, but models inherit cultural biases and fail to depict underrepresented regions faithfully. Existing cultural benchmarks focus mainly on object-centric categories (e.g., food, attire, and architecture), overlooking the social and daily activities that more clearly reflect cultural norms. Few metrics exist for measuring cultural faithfulness. We introduce CULTIVate, a benchmark for evaluating T2I models on cross-cultural activities (e.g., greetings, dining, games, traditional dances, and cultural celebrations). CULTIVate spans 16 countries with 576 prompts and more than 19,000 images, and provides an explainable descriptor-based evaluation framework across multiple cultural dimensions, including background, attire, objects, and interactions. We propose four metrics to measure cultural alignment, hallucination, exaggerated elements, and diversity. Our findings reveal systematic disparities: models perform better for global north countries than for the global south, with distinct failure modes across T2I systems. Human studies confirm that our metrics correlate more strongly with human judgments than existing text-image metrics.", "AI": {"tldr": "本文提出了CULTIVate基准测试，用于评估文本到图像扩散模型在跨文化活动中的表现，并引入了四个度量标准来衡量文化一致性、幻觉、夸张元素和多样性。研究发现，模型在全球北方国家的表现优于全球南方国家。", "motivation": "现有文化基准主要集中在对象类别上，忽略了许多日常活动，这些活动更能反映文化规范。缺乏测量文化真实性的指数。", "method": "CULTIVate包括16个国家的576个提示和超过19,000张图像，涵盖背景、服饰、物品和互动等多个文化维度。提出了四个量化指标。", "result": "研究揭示了模型在评估不同地区时存在系统的差异性，全球北方国家的性能优于全球南方国家。", "conclusion": "人类研究表明，该研究采用的新指标与人类判断的相关性比现有的文本图像指标更强。"}}
{"id": "2511.06125", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.06125", "abs": "https://arxiv.org/abs/2511.06125", "authors": ["Nathan Scales", "Nathanael Schärli", "Olivier Bousquet"], "title": "Evaluation of retrieval-based QA on QUEST-LOFT", "comment": null, "summary": "Despite the popularity of retrieval-augmented generation (RAG) as a solution for grounded QA in both academia and industry, current RAG methods struggle with questions where the necessary information is distributed across many documents or where retrieval needs to be combined with complex reasoning. Recently, the LOFT study has shown that this limitation also applies to approaches based on long-context language models, with the QUEST benchmark exhibiting particularly large headroom. In this paper, we provide an in-depth analysis of the factors contributing to the poor performance on QUEST-LOFT, publish updated numbers based on a thorough human evaluation, and demonstrate that RAG can be optimized to significantly outperform long-context approaches when combined with a structured output format containing reasoning and evidence, optionally followed by answer re-verification.", "AI": {"tldr": "本文分析了RAG方法在QUEST-LOFT基准测试中的表现不佳的因素，通过人类评估发布了更新的数据，并展示了优化后的RAG结合结构化输出与答案重新验证可以显著超越基于长上下文的语言模型方法。", "motivation": "当前的检索增强生成（RAG）方法在知识分散在多个文档中的问题或需要结合复杂推理时表现不佳。LOFT研究也表明，基于长上下文语言模型的方法在QUEST基准上表现不佳。", "method": "我们从LOFT研究中深入分析了导致在QUEST-LOFT基准上表现不佳的因素，并进行了详细的人员评估以发布更新的数据。同时，我们展示了如何通过结合结构化输出格式（包括推理和证据）对RAG进行优化，甚至可选添加答案重新验证，使RAG的表现显著超越基于长上下文语言模型的方法。", "result": "通过详细的人类评估和优化后的RAG结合结构化输出及可选的答案重新验证，显著提高了RAG在QUEST-LOFT基准测试中的表现，超越了长上下文语言模型的方法。", "conclusion": "优化的RAG结构，包括添加推理和证据的结构化输出及可选的答案重新验证过程，可以显著提升在复杂推理问题中的表现，并优于长上下文语言模型方法。"}}
{"id": "2511.05682", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05682", "abs": "https://arxiv.org/abs/2511.05682", "authors": ["Yujin Potter", "Zhun Wang", "Nicholas Crispino", "Kyle Montgomery", "Alexander Xiong", "Ethan Y. Chang", "Francesco Pinto", "Yuqi Chen", "Rahul Gupta", "Morteza Ziyadi", "Christos Christodoulopoulos", "Bo Li", "Chenguang Wang", "Dawn Song"], "title": "VMDT: Decoding the Trustworthiness of Video Foundation Models", "comment": "NeurIPS 2025 Datasets & Benchmarks", "summary": "As foundation models become more sophisticated, ensuring their trustworthiness becomes increasingly critical; yet, unlike text and image, the video modality still lacks comprehensive trustworthiness benchmarks. We introduce VMDT (Video-Modal DecodingTrust), the first unified platform for evaluating text-to-video (T2V) and video-to-text (V2T) models across five key trustworthiness dimensions: safety, hallucination, fairness, privacy, and adversarial robustness. Through our extensive evaluation of 7 T2V models and 19 V2T models using VMDT, we uncover several significant insights. For instance, all open-source T2V models evaluated fail to recognize harmful queries and often generate harmful videos, while exhibiting higher levels of unfairness compared to image modality models. In V2T models, unfairness and privacy risks rise with scale, whereas hallucination and adversarial robustness improve -- though overall performance remains low. Uniquely, safety shows no correlation with model size, implying that factors other than scale govern current safety levels. Our findings highlight the urgent need for developing more robust and trustworthy video foundation models, and VMDT provides a systematic framework for measuring and tracking progress toward this goal. The code is available at https://sunblaze-ucb.github.io/VMDT-page/.", "AI": {"tldr": "VMDT是一个评估文本到视频和视频到文本模型的统一平台，涵盖了五个关键评估维度。研究揭示，开源的T2V模型无法识别有害查询并常生成有害视频，而V2T模型中不公平和隐私风险随着规模增大而增加。", "motivation": "随着基础模型变得更加复杂，确保它们的可信度变得越来越重要;然而，与文本和图像不同，视频领域仍然缺乏全面的可信度基准。", "method": "引入VMDT（视频模式解码信任），这是第一个统一的平台，用于评估文本到视频(T2V)和视频到文本(V2T)模型的信任度，涵盖了五个关键信任维度：安全性、幻觉、公平性、隐私和对抗鲁棒性。", "result": "所有评估过的开源T2V模型都无法识别有害查询并常生成有害视频，但在公平性上表现比图像模型差。V2T模型中，随着规模的增大，不公平和隐私风险增加，但幻觉和对抗鲁棒性有所改善，虽然整体性能依然低下。", "conclusion": "研究结果强调发展更稳健和值得信任的视频基础模型的紧迫性，VMDT提供了一个系统性的框架来衡量和跟踪朝此目标的进步。"}}
{"id": "2511.06146", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.06146", "abs": "https://arxiv.org/abs/2511.06146", "authors": ["Akshar Tumu", "Varad Shinde", "Parisa Kordjamshidi"], "title": "Referring Expressions as a Lens into Spatial Language Grounding in Vision-Language Models", "comment": "Accepted at IJCNLP-AACL 2025", "summary": "Spatial Reasoning is an important component of human cognition and is an area in which the latest Vision-language models (VLMs) show signs of difficulty. The current analysis works use image captioning tasks and visual question answering. In this work, we propose using the Referring Expression Comprehension task instead as a platform for the evaluation of spatial reasoning by VLMs. This platform provides the opportunity for a deeper analysis of spatial comprehension and grounding abilities when there is 1) ambiguity in object detection, 2) complex spatial expressions with a longer sentence structure and multiple spatial relations, and 3) expressions with negation ('not'). In our analysis, we use task-specific architectures as well as large VLMs and highlight their strengths and weaknesses in dealing with these specific situations. While all these models face challenges with the task at hand, the relative behaviors depend on the underlying models and the specific categories of spatial semantics (topological, directional, proximal, etc.). Our results highlight these challenges and behaviors and provide insight into research gaps and future directions.", "AI": {"tldr": "本文提出使用指向表达式理解任务作为评估视觉语言模型(VLMs)空间推理能力的平台，分析了模型在对象检测模糊、复杂空间表达和否定表达上的表现。", "motivation": "随着最新的视觉语言模型显示出在空间推理上的困难，研究团队希望找到一个能够更深入地分析模型空间理解和定位能力的平台。", "method": "采用指向表达式理解任务来评估模型的空间推理能力，并且使用特定的任务架构和大型视觉语言模型进行分析。", "result": "所有模型在任务中都面临挑战，但具体表现取决于模型的类型以及空间语义类别。", "conclusion": "这一研究揭示了模型在特定任务中的挑战和行为规律，为未来的研究指明了方向。"}}
{"id": "2511.05702", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05702", "abs": "https://arxiv.org/abs/2511.05702", "authors": ["Yehyun Suh", "Lin Li", "Aric Plumley", "Chaochao Zhou", "Daniel Moyer", "Kongbin Kang"], "title": "Pedicle Screw Pairing and Registration for Screw Pose Estimation from Dual C-arm Images Using CAD Models", "comment": null, "summary": "Accurate matching of pedicle screws in both anteroposterior (AP) and lateral (LAT) images is critical for successful spinal decompression and stabilization during surgery. However, establishing screw correspondence, especially in LAT views, remains a significant clinical challenge. This paper introduces a method to address pedicle screw correspondence and pose estimation from dual C-arm images. By comparing screw combinations, the approach demonstrates consistent accuracy in both pairing and registration tasks. The method also employs 2D-3D alignment with screw CAD 3D models to accurately pair and estimate screw pose from dual views. Our results show that the correct screw combination consistently outperforms incorrect pairings across all test cases, even prior to registration. After registration, the correct combination further enhances alignment between projections and images, significantly reducing projection error. This approach shows promise for improving surgical outcomes in spinal procedures by providing reliable feedback on screw positioning.", "AI": {"tldr": "The paper presents a method for accurately matching and estimating the pose of pedicle screws in both AP and LAT views using dual C-arm images, which enhances surgical outcomes in spinal procedures by providing reliable feedback on screw positioning.", "motivation": "The motivation is to address the clinical challenge of establishing screw correspondence, especially in lateral views, which is crucial for successful spinal decompression and stabilization during surgery.", "method": "The method involves comparing screw combinations and using 2D-3D alignment with screw CAD 3D models to accurately pair and estimate screw pose from dual views.", "result": "The results indicate that the correct screw combination consistently outperforms incorrect pairings across all test cases, with registration further enhancing alignment and significantly reducing projection error.", "conclusion": "The conclusion is that the proposed approach shows promise for improving the accuracy of pedicle screw placement during spinal surgery, thereby enhancing surgical outcomes."}}
{"id": "2511.06183", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.06183", "abs": "https://arxiv.org/abs/2511.06183", "authors": ["Ryuhei Miyazato", "Ting-Ruen Wei", "Xuyang Wu", "Hsin-Tai Wu", "Kei Harada"], "title": "BookAsSumQA: An Evaluation Framework for Aspect-Based Book Summarization via Question Answering", "comment": null, "summary": "Aspect-based summarization aims to generate summaries that highlight specific aspects of a text, enabling more personalized and targeted summaries. However, its application to books remains unexplored due to the difficulty of constructing reference summaries for long text. To address this challenge, we propose BookAsSumQA, a QA-based evaluation framework for aspect-based book summarization. BookAsSumQA automatically generates aspect-specific QA pairs from a narrative knowledge graph to evaluate summary quality based on its question-answering performance. Our experiments using BookAsSumQA revealed that while LLM-based approaches showed higher accuracy on shorter texts, RAG-based methods become more effective as document length increases, making them more efficient and practical for aspect-based book summarization.", "AI": {"tldr": "提出了BookAsSumQA框架以解决长文本的方面式摘要评估问题，实验显示在长文档中RAG方法优于LLM方法。", "motivation": "目标是生成能够突出文本特定方面的摘要，使摘要更加个性化和针对性。但由于难以构建长文本的参考摘要，这种方法在书籍上的应用尚待探索。", "method": "通过构建基于QA的评估框架BookAsSumQA来解决长文本摘要中的应用难题，该框架能够自动生成面向特定方面的问答对，以评价摘要的质量。", "result": "实验使用BookAsSumQA发现，随着文档长度的增长，RAG方法的效率和实用性超过了LLM方法。", "conclusion": "实验表明，尽管基于LLM的方法在短文本上表现更优，但随着文档长度的增加，基于RAG的方法变得更加有效，因此更适合用于面向特定方面的书籍摘要。"}}
{"id": "2511.05705", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.05705", "abs": "https://arxiv.org/abs/2511.05705", "authors": ["David Acuna", "Chao-Han Huck Yang", "Yuntian Deng", "Jaehun Jung", "Ximing Lu", "Prithviraj Ammanabrolu", "Hyunwoo Kim", "Yuan-Hong Liao", "Yejin Choi"], "title": "Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale", "comment": "Project Page: https://nvlabs.github.io/LongGroundedThoughts/", "summary": "Recent progress in multimodal reasoning has been driven largely by undisclosed datasets and proprietary data synthesis recipes, leaving open questions about how to systematically build large-scale, vision-centric reasoning datasets, particularly for tasks that go beyond visual math. In this work, we introduce a new reasoning data generation framework spanning diverse skills and levels of complexity with over 1M high-quality synthetic vision-centric questions. The dataset also includes preference data and instruction prompts supporting both offline and online RL. Our synthesis framework proceeds in two stages: (1) scale; and (2) complexity. Reasoning traces are then synthesized through a two-stage process that leverages VLMs and reasoning LLMs, producing CoT traces for VLMs that capture the richness and diverse cognitive behaviors found in frontier reasoning models. Remarkably, we show that finetuning Qwen2.5-VL-7B on our data outperforms all open-data baselines across all evaluated vision-centric benchmarks, and even surpasses strong closed-data models such as MiMo-VL-7B-RL on V* Bench, CV-Bench and MMStar-V. Perhaps most surprising, despite being entirely vision-centric, our data transfers positively to text-only reasoning (MMLU-Pro) and audio reasoning (MMAU), demonstrating its effectiveness. Similarly, despite not containing videos or embodied visual data, we observe notable gains when evaluating on a single-evidence embodied QA benchmark (NiEH). Finally, we use our data to analyze the entire VLM post-training pipeline. Our empirical analysis highlights that (i) SFT on high-quality data with non-linear reasoning traces is essential for effective online RL, (ii) staged offline RL matches online RL's performance while reducing compute demands, and (iii) careful SFT on high quality data can substantially improve out-of-domain, cross-modality transfer.", "AI": {"tldr": "本文介绍了一种生成大规模视觉推理数据的方法，这种数据集能够应用于离线和在线强化学习，并且还发现这种数据集能在不同领域的推理中产生积极的迁移效果，如文本推理和音频推理。", "motivation": "当前的多模态推理研究大多依赖于未公开的数据集和私有的数据合成方法，本文旨在探讨如何系统地构建大规模、以视觉为中心的推理数据集。", "method": "本文提出了一种新的用于生成大规模、视觉中心推理数据集的框架。该框架分为两阶段：规模扩展和复杂度提升，并通过预训练的视觉语言模型和推理语言模型生成详细的推理过程。", "result": "通过对Qwen2.5-VL-7B进行数据微调，模型在所有评估的视觉推理基准测试上超越了所有基于开放数据的基线模型，甚至超过了封闭数据模型。", "conclusion": "本文的数据集不仅适用于视觉推理任务，还能在文本和音频推理中表现出色，且对模型的训练流程有重要的启示，尤其是强调高质量数据对在线强化学习的支持作用。"}}
{"id": "2511.06190", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.06190", "abs": "https://arxiv.org/abs/2511.06190", "authors": ["Sangmook Lee", "Dohyung Kim", "Hyukhun Koh", "Nakyeong Yang", "Kyomin Jung"], "title": "Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning", "comment": "7 pages, 5 figures", "summary": "Recent advances in Large Language Models (LLMs) - particularly model scaling and test-time techniques - have greatly enhanced the reasoning capabilities of language models at the expense of higher inference costs. To lower inference costs, prior works train router models or deferral mechanisms that allocate easy queries to a small, efficient model, while forwarding harder queries to larger, more expensive models. However, these trained router models often lack robustness under domain shifts and require expensive data synthesis techniques such as Monte Carlo rollouts to obtain sufficient ground-truth routing labels for training. In this work, we propose Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning (STEER), a domain-agnostic framework that performs fine-grained, step-level routing between smaller and larger LLMs without utilizing external models. STEER leverages confidence scores from the smaller model's logits prior to generating a reasoning step, so that the large model is invoked only when necessary. Extensive evaluations using different LLMs on a diverse set of challenging benchmarks across multiple domains such as Mathematical Reasoning, Multi-Hop QA, and Planning tasks indicate that STEER achieves competitive or enhanced accuracy while reducing inference costs (up to +20% accuracy with 48% less FLOPs compared to solely using the larger model on AIME), outperforming baselines that rely on trained external modules. Our results establish model-internal confidence as a robust, domain-agnostic signal for model routing, offering a scalable pathway for efficient LLM deployment.", "AI": {"tldr": "本文提出了一种名为STEER的新方法，通过使用小模型的置信得分来进行决策，从而在不增加外部模型的情况下，实现大规模语言模型的高效推理。这种方法在不同任务中表现出良好的性能，并能够显著减少计算资源的消耗。", "motivation": "大规模语言模型虽提升了语言模型的推理能力，但同时也导致了较高的推理成本。为解决这一问题，本文旨在提出一种方法，能够在不同的领域和任务中有效地分配推理资源，以减少总的成本。", "method": "本文提出了一种名为STEER的方法来降低推理成本，它是一种无须外部模型的领域无关框架，能够对小规模和大规模语言模型进行细粒度的、按步骤的路由。STEER主要依赖于小模型的置信得分来判断是否需要调用大规模模型。", "result": "实验结果显示，STEER在处理各种具有挑战性的任务时，不仅提高了准确率，而且也成功降低了推理成本。例如，在AIME任务上，比起仅使用大规模模型，它提升了20%的准确性并减少了48%的浮点运算。", "conclusion": "研究结果证明了利用模型内部的置信得分进行路由是一种强大且通用的信号，它为进一步实现大规模语言模型的高效部署提供了可能。"}}
{"id": "2511.05731", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05731", "abs": "https://arxiv.org/abs/2511.05731", "authors": ["Xing Yao", "Ahana Gangopadhyay", "Hsi-Ming Chang", "Ravi Soni"], "title": "Towards Better Ultrasound Video Segmentation Foundation Model: An Empirical study on SAM2 Finetuning from Data Perspective", "comment": null, "summary": "Ultrasound (US) video segmentation remains a challenging problem due to strong inter- and intra-dataset variability, motion artifacts, and limited annotated data. Although foundation models such as Segment Anything Model 2 (SAM2) demonstrate strong zero-shot and prompt-guided segmentation capabilities, their performance deteriorates substantially when transferred to medical imaging domains. Current adaptation studies mainly emphasize architectural modifications, while the influence of data characteristics and training regimes has not been systematically examined. In this study, we present a comprehensive, data-centric investigation of SAM2 adaptation for ultrasound video segmentation. We analyze how training-set size, video duration, and augmentation schemes affect adaptation performance under three paradigms: task-specific fine-tuning, intermediate adaptation, and multi-task joint training, across five SAM2 variants and multiple prompting modes. We further design six ultrasound-specific augmentations, assessing their effect relative to generic strategies. Experiments on three representative ultrasound datasets reveal that data scale and temporal context play a more decisive role than model architecture or initialization. Moreover, joint training offers an efficient compromise between modality alignment and task specialization. This work aims to provide empirical insights for developing efficient, data-aware adaptation pipelines for SAM2 in ultrasound video analysis.", "AI": {"tldr": "本研究对SAM2在超声视频分割领域的适应性进行了从数据角度的全面研究，发现数据规模和时间上下文比模型结构更重要，并提出了一种有效的联合训练策略。", "motivation": "尽管基础模型如SAM2在零样本和提示引导分割方面表现出强大的能力，但其性能在转移到医学图像领域时显著下降。当前的适配研究主要强调架构上的修改，而数据特性和训练制度的影响则未被系统地研究。这项研究旨在提供从数据角度出发的有效适配策略的实证见解。", "method": "该研究从数据的角度出发，综合调查了SAM2模型在超声视频分割上的适应性。研究分析了训练集大小、视频时长和数据增强方案这三个因素如何在任务特定微调、中间适应和多任务联合训练三种范式下影响适应性能，涵盖了五个SAM2变体和多种提示模式。此外，研究还设计了六种超声特定的数据增强方案，并评估了它们相对于通用策略的效果。", "result": "实验结果表明，在三个代表性的超声数据集上，数据规模和时间上下文比模型结构或初始化更能决定性能。此外，联合训练在模态对齐和任务专业化之间提供了有效的平衡。", "conclusion": "该研究为开发高效的、数据敏感的SAM2超声视频分析的适应管道提供了实证见解。"}}
{"id": "2511.06215", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06215", "abs": "https://arxiv.org/abs/2511.06215", "authors": ["Puzhen Su", "Yongzhu Miao", "Chunxi Guo", "Jintao Tang", "Shasha Li", "Ting Wang"], "title": "Explicit Knowledge-Guided In-Context Learning for Early Detection of Alzheimer's Disease", "comment": "This paper was accepted by IEEE BIBM 2025 conference", "summary": "Detecting Alzheimer's Disease (AD) from narrative transcripts remains a challenging task for large language models (LLMs), particularly under out-of-distribution (OOD) and data-scarce conditions. While in-context learning (ICL) provides a parameter-efficient alternative to fine-tuning, existing ICL approaches often suffer from task recognition failure, suboptimal demonstration selection, and misalignment between label words and task objectives, issues that are amplified in clinical domains like AD detection. We propose Explicit Knowledge In-Context Learners (EK-ICL), a novel framework that integrates structured explicit knowledge to enhance reasoning stability and task alignment in ICL. EK-ICL incorporates three knowledge components: confidence scores derived from small language models (SLMs) to ground predictions in task-relevant patterns, parsing feature scores to capture structural differences and improve demo selection, and label word replacement to resolve semantic misalignment with LLM priors. In addition, EK-ICL employs a parsing-based retrieval strategy and ensemble prediction to mitigate the effects of semantic homogeneity in AD transcripts. Extensive experiments across three AD datasets demonstrate that EK-ICL significantly outperforms state-of-the-art fine-tuning and ICL baselines. Further analysis reveals that ICL performance in AD detection is highly sensitive to the alignment of label semantics and task-specific context, underscoring the importance of explicit knowledge in clinical reasoning under low-resource conditions.", "AI": {"tldr": "本文针对大规模语言模型在阿尔茨海默病（AD）检测中的难题提出了一种新的ICL框架EK-ICL，通过引入显式知识来解决现存的ICL方法问题，实验表明该方法显著优于现有技术。", "motivation": "现有的in-context学习方法在阿尔茨海默病检测等临床领域面临任务识别失败、次优示例选择和标签词与任务目标不匹配等问题，尤其是在资源稀缺条件下。", "method": "提出了一种名为Explicit Knowledge In-Context Learners（EK-ICL）的新框架，该框架在in-context学习中整合了结构化显式知识，以增强推理稳定性和任务对齐。EK-ICL集成了三个知识组件：小语言模型（SLMs）得出的信心分数来确保预测与任务相关，解析特征分数来捕捉结构性差异并改善演示选择，以及标签词替换来解决与大型语言模型先验知识的语义不对齐。此外，EK-ICL采用了基于解析的检索策略和集成预测来缓解AD转录文本中的语义同质性影响。", "result": "在三个AD数据集上的广泛实验表明，EK-ICL显著优于最先进的微调和ICL基线。", "conclusion": "实验进一步揭示，ICL在AD检测中的表现对标签语义与任务特定背景的对齐度高度敏感，这强调了在低资源条件下临床推理中使用显式知识的重要性。"}}
{"id": "2511.05760", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05760", "abs": "https://arxiv.org/abs/2511.05760", "authors": ["Mateo Ortiz", "Juan Olmos", "Fabio Martínez"], "title": "A Second-Order Attention Mechanism For Prostate Cancer Segmentation and Detection in Bi-Parametric MRI", "comment": "Accepted at the 28th Iberoamerican Congress on Pattern Recognition (CIARP 2025). To appear in Lecture Notes in Computer Science (LNCS), Springer", "summary": "The detection of clinically significant prostate cancer lesions (csPCa) from biparametric magnetic resonance imaging (bp-MRI) has emerged as a noninvasive imaging technique for improving accurate diagnosis. Nevertheless, the analysis of such images remains highly dependent on the subjective expert interpretation. Deep learning approaches have been proposed for csPCa lesions detection and segmentation, but they remain limited due to their reliance on extensively annotated datasets. Moreover, the high lesion variability across prostate zones poses additional challenges, even for expert radiologists. This work introduces a second-order geometric attention (SOGA) mechanism that guides a dedicated segmentation network, through skip connections, to detect csPCa lesions. The proposed attention is modeled on the Riemannian manifold, learning from symmetric positive definitive (SPD) representations. The proposed mechanism was integrated into standard U-Net and nnU-Net backbones, and was validated on the publicly available PI-CAI dataset, achieving an Average Precision (AP) of 0.37 and an Area Under the ROC Curve (AUC-ROC) of 0.83, outperforming baseline networks and attention-based methods. Furthermore, the approach was evaluated on the Prostate158 dataset as an independent test cohort, achieving an AP of 0.37 and an AUC-ROC of 0.75, confirming robust generalization and suggesting discriminative learned representations.", "AI": {"tldr": "This work introduces a second-order geometric attention mechanism for detecting clinically significant prostate cancer lesions from biparametric MRI images, which outperformed baseline methods on publicly available datasets.", "motivation": "The analysis of biparametric MRI images for detecting clinically significant prostate cancer lesions relies heavily on subjective expert interpretation, and deep learning approaches have limitations due to the need for extensively annotated datasets and the high variability of lesions.", "method": "A second-order geometric attention mechanism (SOGA), modeled on the Riemannian manifold using symmetric positive definite representations, is introduced. It guides the segmentation process in U-Net and nnU-Net backbones.", "result": "The method achieved an AP of 0.37 and an AUC-ROC of 0.83 on the PI-CAI dataset and an AP of 0.37 and an AUC-ROC of 0.75 on the Prostate158 dataset, outperforming baseline and attention-based methods.", "conclusion": "The proposed SOGA mechanism demonstrates robust generalization and the ability to provide discriminative learned representations for the detection of clinically significant prostate cancer lesions from biparametric MRI images."}}
{"id": "2511.06222", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.06222", "abs": "https://arxiv.org/abs/2511.06222", "authors": ["Yue Huang", "Xiangqi Wang", "Xiangliang Zhang"], "title": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization", "comment": "Accepted by AAAI 2026 (Oral)", "summary": "In high-stakes scenarios-such as self-harm, legal, or medical queries-LLMs must be both trustworthy and helpful. However, these goals often conflict. We propose priority alignment, a new alignment paradigm that enforces a strict \"trustworthy-before-helpful\" ordering: optimization of helpfulness is conditioned on first meeting trustworthy thresholds (e.g., harmlessness or honesty). To realize this, we introduce Self-Priority Alignment (SPA)-a fully unsupervised framework that generates diverse responses, self-evaluates them and refines them by the model itself, and applies dual-criterion denoising to remove inconsistency and control variance. From this, SPA constructs lexicographically ordered preference pairs and fine-tunes the model using an uncertainty-weighted alignment loss that emphasizes high-confidence, high-gap decisions. Experiments across multiple benchmarks show that SPA improves helpfulness without compromising safety, outperforming strong baselines while preserving general capabilities. Our results demonstrate that SPA provides a scalable and interpretable alignment strategy for critical LLM applications.", "AI": {"tldr": "本文提出了优先级对齐（SPA）框架，在确保可信的前提下提高大语言模型的帮助性，实验表明这种方法可以在不牺牲安全性的前提下提高有用性。", "motivation": "在高风险场景（如自残、法律或医疗查询）中，大语言模型需要同时可信和有用，但这两个目标往往相冲突。本文旨在解决这一问题。", "method": "本文提出了优先级对齐(Self-Priority Alignment, SPA)，这是一种全新的对齐范式，通过无监督框架自动生成多样化的响应、自我评估并优化这些响应，并应用双标准去噪方法来去除不一致性和控制方差，从而构建字典序排列的偏好对，并采用不确定性加权对齐损失进行微调，以强调高置信度和显著差异的决策。", "result": "实验结果显示，SPA能有效提高模型的有用性而不影响安全性，表现优于强基线，同时保持了模型的通用能力。", "conclusion": "本文结果证明了SPA为关键大语言模型应用提供了一个可扩展且可解释的对齐策略。"}}
{"id": "2511.05772", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05772", "abs": "https://arxiv.org/abs/2511.05772", "authors": ["B. Mederos", "J. Mejía", "A. Medina-Reyes", "Y. Espinosa-Almeyda", "J. D. Díaz-Roman", "I. Rodríguez-Mederos", "M. Mejía-Carreon", "F. Gonzalez-Lopez"], "title": "Sign language recognition from skeletal data using graph and recurrent neural networks", "comment": "15 pages, 2 figures", "summary": "This work presents an approach for recognizing isolated sign language gestures using skeleton-based pose data extracted from video sequences. A Graph-GRU temporal network is proposed to model both spatial and temporal dependencies between frames, enabling accurate classification. The model is trained and evaluated on the AUTSL (Ankara university Turkish sign language) dataset, achieving high accuracy. Experimental results demonstrate the effectiveness of integrating graph-based spatial representations with temporal modeling, providing a scalable framework for sign language recognition. The results of this approach highlight the potential of pose-driven methods for sign language understanding.", "AI": {"tldr": "本研究提出了一种基于骨架姿态数据识别孤立手势的方法，使用Graph-GRU时序网络来同时模拟空间和时间依赖关系，获得了高精度的分类结果。", "motivation": "解决现有方法在识别孤立手语手势上的不足，提高识别精度。", "method": "利用Graph-GRU网络模型来捕捉视频序列中的空间和时间依赖关系。", "result": "在AUTSL数据集上的实验表明，该方法可以有效地将基于图形的空陈述已被截断，请看完整版以获取全面信息。间表示与时间建模相结合，实现了高精度的识别。", "conclusion": "研究证明了基于姿态驱动的方法对于手语理解的潜力。"}}
{"id": "2511.06230", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06230", "abs": "https://arxiv.org/abs/2511.06230", "authors": ["Juntao Li", "Haobin Yuan", "Ling Luo", "Tengxiao Lv", "Yan Jiang", "Fan Wang", "Ping Zhang", "Huiyi Lv", "Jian Wang", "Yuanyuan Sun", "Hongfei Lin"], "title": "Overview of CHIP 2025 Shared Task 2: Discharge Medication Recommendation for Metabolic Diseases Based on Chinese Electronic Health Records", "comment": null, "summary": "Discharge medication recommendation plays a critical role in ensuring treatment continuity, preventing readmission, and improving long-term management for patients with chronic metabolic diseases. This paper present an overview of the CHIP 2025 Shared Task 2 competition, which aimed to develop state-of-the-art approaches for automatically recommending appro-priate discharge medications using real-world Chinese EHR data. For this task, we constructed CDrugRed, a high-quality dataset consisting of 5,894 de-identified hospitalization records from 3,190 patients in China. This task is challenging due to multi-label nature of medication recommendation, het-erogeneous clinical text, and patient-specific variability in treatment plans. A total of 526 teams registered, with 167 and 95 teams submitting valid results to the Phase A and Phase B leaderboards, respectively. The top-performing team achieved the highest overall performance on the final test set, with a Jaccard score of 0.5102, F1 score of 0.6267, demonstrating the potential of advanced large language model (LLM)-based ensemble systems. These re-sults highlight both the promise and remaining challenges of applying LLMs to medication recommendation in Chinese EHRs. The post-evaluation phase remains open at https://tianchi.aliyun.com/competition/entrance/532411/.", "AI": {"tldr": "本文概述了CHIP 2025共享任务2竞赛，旨在通过使用真实中国的电子健康记录数据开发最先进的自动出院用药推荐方法。顶级团队展示了高级大语言模型(如LLM)支持的集成系统在中文EHR上应用药物推荐的潜力。", "motivation": "出院用药推荐在确保治疗连续性、防止再入院和改善慢性代谢病患者的长期管理中起着关键作用。本文旨在通过构建竞赛任务来推动采用最先进的方法，使用真实世界的中国电子健康记录数据来自动推荐合适的出院用药。", "method": "本研究构建了一个名为CDrugRed的高质量数据集，该数据集包含了来自中国3,190名患者的5,894份去识别化的住院记录，并提出了一个自动推荐出院用药的竞赛任务。竞赛吸引了526支队伍注册，最终有167支和95支队伍提交了有效结果，分别参与了A阶段和B阶段的比赛。", "result": "最终，表现最好的团队在所有测试集中取得了最高的成绩，Jaccard得分为0.5102，F1得分为0.6267，表明LLM模型在中文电子病历的药物推荐上既有潜力也存在挑战。", "conclusion": "该竞赛展示了使用大规模语言模型在处理多标记用药推荐、异构临床文本以及基于患者的治疗方案差异等挑战方面的成果。然而，研究结果也提醒我们仍有待解决的问题。"}}
{"id": "2511.05782", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05782", "abs": "https://arxiv.org/abs/2511.05782", "authors": ["Lalit Maurya", "Honghai Liu", "Reyer Zwiggelaar"], "title": "TCSA-UDA: Text-Driven Cross-Semantic Alignment for Unsupervised Domain Adaptation in Medical Image Segmentation", "comment": null, "summary": "Unsupervised domain adaptation for medical image segmentation remains a significant challenge due to substantial domain shifts across imaging modalities, such as CT and MRI. While recent vision-language representation learning methods have shown promise, their potential in UDA segmentation tasks remains underexplored. To address this gap, we propose TCSA-UDA, a Text-driven Cross-Semantic Alignment framework that leverages domain-invariant textual class descriptions to guide visual representation learning. Our approach introduces a vision-language covariance cosine loss to directly align image encoder features with inter-class textual semantic relations, encouraging semantically meaningful and modality-invariant feature representations. Additionally, we incorporate a prototype alignment module that aligns class-wise pixel-level feature distributions across domains using high-level semantic prototypes. This mitigates residual category-level discrepancies and enhances cross-modal consistency. Extensive experiments on challenging cross-modality cardiac, abdominal, and brain tumor segmentation benchmarks demonstrate that our TCSA-UDA framework significantly reduces domain shift and consistently outperforms state-of-the-art UDA methods, establishing a new paradigm for integrating language-driven semantics into domain-adaptive medical image analysis.", "AI": {"tldr": "TCSA-UDA uses textual class descriptions and alignment mechanisms to improve unsupervised domain adaptation in medical image segmentation across different imaging modalities.", "motivation": "To tackle unsupervised domain adaptation (UDA) challenges in medical image segmentation, especially the significant domain shifts across imaging modalities like CT and MRI.", "method": "Our approach, TCSA-UDA, introduces a Text-driven Cross-Semantic Alignment framework leveraging domain-invariant textual class descriptions. It includes a vision-language covariance cosine loss to align image encoder features with textual semantics and a prototype alignment module to enhance cross-modal consistency.", "result": "Experiments on cardiac, abdominal, and brain tumor segmentation show TCSA-UDA effectively reduces domain shift and surpasses current UDA methods.", "conclusion": "TCSA-UDA establishes a new method for integrating language-driven semantics into domain-adaptive medical image analysis, improving cross-modality segmentation performance."}}
{"id": "2511.06234", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06234", "abs": "https://arxiv.org/abs/2511.06234", "authors": ["Mojtaba Noghabaei"], "title": "Analyzing and Mitigating Negation Artifacts using Data Augmentation for Improving ELECTRA-Small Model Accuracy", "comment": null, "summary": "Pre-trained models for natural language inference (NLI) often achieve high performance on benchmark datasets by using spurious correlations, or dataset artifacts, rather than understanding language touches such as negation. In this project, we investigate the performance of an ELECTRA-small model fine-tuned on the Stanford Natural Language Inference (SNLI) dataset, focusing on its handling of negation. Through analysis, we identify that the model struggles with correctly classifying examples containing negation. To address this, we augment the training data with contrast sets and adversarial examples emphasizing negation. Our results demonstrate that this targeted data augmentation improves the model's accuracy on negation-containing examples without adversely affecting overall performance, therefore mitigating the identified dataset artifact.", "AI": {"tldr": "通过在训练数据中添加对比集和强调否定的对抗性示例，研究发现ELECTRA-small模型在否定相关的自然语言推理任务上性能得到提升，同时整体性能未受影响。", "motivation": "探讨ELECTRA-small模型在SNLI数据集上的表现，尤其是处理否定信息的能力，以期改进模型在否定相关的任务上的准确性。", "method": "通过分析模型对包含否定信息的数据的处理能力，然后在训练数据中加入强调否定的对比集和对抗性示例。", "result": "针对性的数据增强策略提高了模型在包含否定信息数据上的分类准确性，而整体性能没有下降。", "conclusion": "这种方法可以缓解数据集中的特定偏差问题，比如否定问题，从而提高模型的真实理解能力。"}}
{"id": "2511.05795", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05795", "abs": "https://arxiv.org/abs/2511.05795", "authors": ["Xuqing Geng", "Lei Su", "Zhongwei Bian", "Zewen Sun", "Jiaxuan Wen", "Jie Tian", "Yang Du"], "title": "Position-Prior-Guided Network for System Matrix Super-Resolution in Magnetic Particle Imaging", "comment": "accepted as oral presentation at EMBC 2025", "summary": "Magnetic Particle Imaging (MPI) is a novel medical imaging modality. One of the established methods for MPI reconstruction is based on the System Matrix (SM). However, the calibration of the SM is often time-consuming and requires repeated measurements whenever the system parameters change. Current methodologies utilize deep learning-based super-resolution (SR) techniques to expedite SM calibration; nevertheless, these strategies do not fully exploit physical prior knowledge associated with the SM, such as symmetric positional priors. Consequently, we integrated positional priors into existing frameworks for SM calibration. Underpinned by theoretical justification, we empirically validated the efficacy of incorporating positional priors through experiments involving both 2D and 3D SM SR methods.", "AI": {"tldr": "本文提出了结合位置先验知识的方法加速MPI系统矩阵的校准，并通过实验验证其有效性。", "motivation": "由于现有的系统矩阵校准过程耗时且在系统参数变化时需要重复测量，当前的方法通过深度学习超分辨率技术加速SM校准，但没有充分利用物理上的先验知识。本文旨在解决这个问题。", "method": "本文提出了一种在系统矩阵校准中融合位置先验知识的方法，这种方法在理论上得到了验证，并通过2D和3D系统矩阵超分辨率方法的实验进行了有效性检验。", "result": "通过实验证明了在系统矩阵校准中融入位置先验知识的有效性。", "conclusion": "实验结果表明，结合位置先验知识的系统矩阵超分辨率方法相对于现有方法具有更好的性能。"}}
{"id": "2511.06344", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06344", "abs": "https://arxiv.org/abs/2511.06344", "authors": ["Zhirui Zhang", "Changhua Pei", "Tianyi Gao", "Zhe Xie", "Yibo Hao", "Zhaoyang Yu", "Longlong Xu", "Tong Xiao", "Jing Han", "Dan Pei"], "title": "TimeSense:Making Large Language Models Proficient in Time-Series Analysis", "comment": null, "summary": "In the time-series domain, an increasing number of works combine text with temporal data to leverage the reasoning capabilities of large language models (LLMs) for various downstream time-series understanding tasks. This enables a single model to flexibly perform tasks that previously required specialized models for each domain. However, these methods typically rely on text labels for supervision during training, biasing the model toward textual cues while potentially neglecting the full temporal features. Such a bias can lead to outputs that contradict the underlying time-series context. To address this issue, we construct the EvalTS benchmark, comprising 10 tasks across three difficulty levels, from fundamental temporal pattern recognition to complex real-world reasoning, to evaluate models under more challenging and realistic scenarios. We also propose TimeSense, a multimodal framework that makes LLMs proficient in time-series analysis by balancing textual reasoning with a preserved temporal sense. TimeSense incorporates a Temporal Sense module that reconstructs the input time-series within the model's context, ensuring that textual reasoning is grounded in the time-series dynamics. Moreover, to enhance spatial understanding of time-series data, we explicitly incorporate coordinate-based positional embeddings, which provide each time point with spatial context and enable the model to capture structural dependencies more effectively. Experimental results demonstrate that TimeSense achieves state-of-the-art performance across multiple tasks, and it particularly outperforms existing methods on complex multi-dimensional time-series reasoning tasks.", "AI": {"tldr": "TimeSense is introduced to balance textual reasoning with temporal features in time-series analysis, using a Temporal Sense module and coordinate-based positional embeddings.", "motivation": "The motivation is to address the limitations of current methods that rely heavily on textual cues, which can lead to outputs that contradict the underlying time-series context. The goal is to improve time-series analysis under challenging and realistic scenarios.", "method": "This paper proposes TimeSense, a multimodal framework that balances textual reasoning with temporal features in time-series analysis. It includes a Temporal Sense module for reconstructing time-series within the model's context and uses coordinate-based positional embeddings for enhanced spatial understanding.", "result": "Experimental results show that TimeSense achieves state-of-the-art performance across multiple tasks and performs particularly well on complex multi-dimensional time-series reasoning tasks.", "conclusion": "The proposed TimeSense framework improves the proficiency of large language models in time-series analysis by better integrating temporal data with textual information, leading to superior performance in various tasks compared to previous methods."}}
{"id": "2511.05803", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05803", "abs": "https://arxiv.org/abs/2511.05803", "authors": ["Lalit Maurya", "Honghai Liu", "Reyer Zwiggelaar"], "title": "MACMD: Multi-dilated Contextual Attention and Channel Mixer Decoding for Medical Image Segmentation", "comment": null, "summary": "Medical image segmentation faces challenges due to variations in anatomical structures. While convolutional neural networks (CNNs) effectively capture local features, they struggle with modeling long-range dependencies. Transformers mitigate this issue with self-attention mechanisms but lack the ability to preserve local contextual information. State-of-the-art models primarily follow an encoder-decoder architecture, achieving notable success. However, two key limitations remain: (1) Shallow layers, which are closer to the input, capture fine-grained details but suffer from information loss as data propagates through deeper layers. (2) Inefficient integration of local details and global context between the encoder and decoder stages. To address these challenges, we propose the MACMD-based decoder, which enhances attention mechanisms and facilitates channel mixing between encoder and decoder stages via skip connections. This design leverages hierarchical dilated convolutions, attention-driven modulation, and a cross channel-mixing module to capture long-range dependencies while preserving local contextual details, essential for precise medical image segmentation. We evaluated our approach using multiple transformer encoders on both binary and multi-organ segmentation tasks. The results demonstrate that our method outperforms state-of-the-art approaches in terms of Dice score and computational efficiency, highlighting its effectiveness in achieving accurate and robust segmentation performance. The code available at https://github.com/lalitmaurya47/MACMD", "AI": {"tldr": "本文通过MACMD解码器改进了医学图像分割模型，利用跨通道混合模块提高了分割性能，有效结合了局部细节与长距离依赖关系。", "motivation": "解决医学图像分割中的长期依赖关系建模难题，改善细粒度信息在深层网络中的保留，并优化局部细节与全局上下文在编码器与解码器阶段的整合。", "method": "提出了MACMD解码器，通过跨通道混合模块、注意力驱动调节和层次空洞卷积，解决了编码器-解码器架构中细粒度信息丢失和局部细节与全局上下文集成不充分的问题。", "result": "实验结果表明，通过多个Transformer编码器进行评估，新的方法在Dice分数和计算效率上均优于现有模型，显示出在医学图像分割上的准确性和鲁棒性。", "conclusion": "MACMD解码器在医学图像分割问题上的性能优于当前最优方法，表明其在保留局部细节同时捕捉长距离依赖关系方面的有效性。"}}
{"id": "2511.06391", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06391", "abs": "https://arxiv.org/abs/2511.06391", "authors": ["Irina Proskurina", "Marc-Antoine Carpentier", "Julien Velcin"], "title": "HatePrototypes: Interpretable and Transferable Representations for Implicit and Explicit Hate Speech Detection", "comment": null, "summary": "Optimization of offensive content moderation models for different types of hateful messages is typically achieved through continued pre-training or fine-tuning on new hate speech benchmarks. However, existing benchmarks mainly address explicit hate toward protected groups and often overlook implicit or indirect hate, such as demeaning comparisons, calls for exclusion or violence, and subtle discriminatory language that still causes harm. While explicit hate can often be captured through surface features, implicit hate requires deeper, full-model semantic processing. In this work, we question the need for repeated fine-tuning and analyze the role of HatePrototypes, class-level vector representations derived from language models optimized for hate speech detection and safety moderation. We find that these prototypes, built from as few as 50 examples per class, enable cross-task transfer between explicit and implicit hate, with interchangeable prototypes across benchmarks. Moreover, we show that parameter-free early exiting with prototypes is effective for both hate types. We release the code, prototype resources, and evaluation scripts to support future research on efficient and transferable hate speech detection.", "AI": {"tldr": "本文探讨了在不同类型的仇恨言论检测模型间转移的问题，通过使用HatePrototypes，无需频繁微调模型，就可以实现显式和隐式仇恨言论的有效检测。", "motivation": "现有的仇恨言论基准主要集中在明确针对受保护群体的仇恨言论上，而忽视了隐含的或间接的仇恨言论，例如贬低比较、要求排斥或暴力、以及细微的歧视性语言。", "method": "本文使用了HatePrototypes，即从用于仇恨言论检测和安全审查的优化语言模型中派生的类别级向量表示，来分析显式仇恨与隐式仇恨之间的跨任务转移。", "result": "研究发现，这些原型可以从每个类别中仅使用50个示例构建，在显式和隐式仇恨之间实现跨任务转移，并且在不同基准之间可以互换。此外，参数无关的早期退出策略在两种仇恨类型中都有效。", "conclusion": "研究结果表明，HatePrototypes可以在不同类型的仇恨言论检测中实现有效的跨任务转移，并且不需要额外的微调，减少了模型的复杂性和计算成本。"}}
{"id": "2511.05818", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05818", "abs": "https://arxiv.org/abs/2511.05818", "authors": ["Yuchen Su", "Zhineng Chen", "Yongkun Du", "Zuxuan Wu", "Hongtao Xie", "Yu-Gang Jiang"], "title": "LRANet++: Low-Rank Approximation Network for Accurate and Efficient Text Spotting", "comment": null, "summary": "End-to-end text spotting aims to jointly optimize text detection and recognition within a unified framework. Despite significant progress, designing an accurate and efficient end-to-end text spotter for arbitrary-shaped text remains largely unsolved. We identify the primary bottleneck as the lack of a reliable and efficient text detection method. To address this, we propose a novel parameterized text shape method based on low-rank approximation for precise detection and a triple assignment detection head to enable fast inference. Specifically, unlike other shape representation methods that employ data-irrelevant parameterization, our data-driven approach derives a low-rank subspace directly from labeled text boundaries. To ensure this process is robust against the inherent annotation noise in this data, we utilize a specialized recovery method based on an $\\ell_1$-norm formulation, which accurately reconstructs the text shape with only a few key orthogonal vectors. By exploiting the inherent shape correlation among different text contours, our method achieves consistency and compactness in shape representation. Next, the triple assignment scheme introduces a novel architecture where a deep sparse branch (for stabilized training) is used to guide the learning of an ultra-lightweight sparse branch (for accelerated inference), while a dense branch provides rich parallel supervision. Building upon these advancements, we integrate the enhanced detection module with a lightweight recognition branch to form an end-to-end text spotting framework, termed LRANet++, capable of accurately and efficiently spotting arbitrary-shaped text. Extensive experiments on several challenging benchmarks demonstrate the superiority of LRANet++ compared to state-of-the-art methods. Code will be available at: https://github.com/ychensu/LRANet-PP.git", "AI": {"tldr": "本文通过提出一个基于低秩近似的新参数化文本形状方法解决了任意形状文本检测的挑战，并开发了具有三重分支架构的检测头，名为LRANet++，在多种复杂场景测试中表现优异。", "motivation": "现有方法在处理任意形状文本的检测和识别时仍面临挑战，主要瓶颈是缺乏可靠和高效的文本检测方法。为此，研究者们打算设计一种新的参数化文本形状方法以用于精准检测，并配合快速推断的三重指派检测头解决上述问题。", "method": "本方法提出了基于低秩近似的新参数化文本形状方法，直接通过标注文本边界数据推导低秩子空间，以克服标注噪声带来的影响。此外，通过一种新颖的三重指派架构，结合深度稀疏分支以稳定训练、超轻量级稀疏分支以加速推断和密集分支以提供丰富并行监督。", "result": "经过大量实验的验证，新提出的LRANet++在多个具有挑战性的基准数据集上展示了优于现有最佳方法的性能。", "conclusion": "研究表明，通过先进的检测方法和架构设计，LRANet++能够高效且精确地识别任意形状的文本，有效提升端到端文本检测识别的能力。"}}
{"id": "2511.06402", "categories": ["cs.CL", "cs.CY", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.06402", "abs": "https://arxiv.org/abs/2511.06402", "authors": ["Lionel Z. Wang", "Shihan Ben", "Yulu Huang", "Simeng Qing"], "title": "SugarTextNet: A Transformer-Based Framework for Detecting Sugar Dating-Related Content on Social Media with Context-Aware Focal Loss", "comment": "This paper is accepted by HICSS 2026", "summary": "Sugar dating-related content has rapidly proliferated on mainstream social media platforms, giving rise to serious societal and regulatory concerns, including commercialization of intimate relationships and the normalization of transactional relationships.~Detecting such content is highly challenging due to the prevalence of subtle euphemisms, ambiguous linguistic cues, and extreme class imbalance in real-world data.~In this work, we present SugarTextNet, a novel transformer-based framework specifically designed to identify sugar dating-related posts on social media.~SugarTextNet integrates a pretrained transformer encoder, an attention-based cue extractor, and a contextual phrase encoder to capture both salient and nuanced features in user-generated text.~To address class imbalance and enhance minority-class detection, we introduce Context-Aware Focal Loss, a tailored loss function that combines focal loss scaling with contextual weighting.~We evaluate SugarTextNet on a newly curated, manually annotated dataset of 3,067 Chinese social media posts from Sina Weibo, demonstrating that our approach substantially outperforms traditional machine learning models, deep learning baselines, and large language models across multiple metrics.~Comprehensive ablation studies confirm the indispensable role of each component.~Our findings highlight the importance of domain-specific, context-aware modeling for sensitive content detection, and provide a robust solution for content moderation in complex, real-world scenarios.", "AI": {"tldr": "SugarTextNet利用先进的深度学习框架和新的损失函数，有效提高了糖约会相关内容检测的性能。", "motivation": "由于社交媒体平台上糖约会相关内容的激增，引发了对商业化亲密关系和交易关系常态化的社会及监管担忧。糖约会相关内容的检测具有挑战性，因为存在细微的隐喻、模棱两可的语言线索以及现实世界数据中的极端类别不平衡。", "method": "SugarTextNet采用了一个基于transformer的框架，集成了预训练的transformer编码器、基于注意力的提示提取器和上下文短语编码器，以捕获用户生成文本中的显著特征和细微差别。还引入了上下文感知的Focal Loss，以解决类别不平衡问题并提高少数类别的检测能力。", "result": "在由3,067个新浪微博帖子组成的新建立、手动标注的数据集上评估SugarTextNet，结果表明该方法在多个度量指标上显著优于传统机器学习模型、深度学习基线和大语言模型。", "conclusion": "糖约会相关内容检测的重要性和领域特定、上下文感知建模的价值得到了强调，为复杂现实世界场景中的内容审核提供了强大的解决方案。"}}
{"id": "2511.05832", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05832", "abs": "https://arxiv.org/abs/2511.05832", "authors": ["Yunge Li", "Lanyu Xu"], "title": "Hilbert-Guided Block-Sparse Local Attention", "comment": null, "summary": "The quadratic compute and memory costs of global self-attention severely limit its use in high-resolution images. Local attention reduces complexity by restricting attention to neighborhoods. Block-sparse kernels can further improve the efficiency of local attention, but conventional local attention patterns often fail to deliver significant speedups because tokens within a window are not contiguous in the 1D sequence. This work proposes a novel method for constructing windows and neighborhoods based on the Hilbert curve. Image tokens are first reordered along a Hilbert curve, and windows and neighborhoods are then formed on the reordered 1D sequence. From a block-sparse perspective, this strategy significantly increases block sparsity and can be combined with existing block-sparse kernels to improve the efficiency of 2D local attention. Experiments show that the proposed Hilbert Window Attention and Hilbert Slide Attention can accelerate window attention and slide attention by about $4\\times$ and $18\\times$, respectively. To assess practicality, the strategy is instantiated as the Hilbert Window Transformer and the Hilbert Neighborhood Transformer, both of which achieve end-to-end speedups with minimal accuracy loss. Overall, combining Hilbert-guided local attention with block-sparse kernels offers a general and practical approach to enhancing the efficiency of 2D local attention for images. The code is available at https://github.com/Yunge6666/Hilbert-Local-Attention.", "AI": {"tldr": "本文提出基于希尔伯特曲线的窗口和邻域构建策略，提高了2D局部注意力的效率，分别加速了窗口注意力和滑动注意力约4倍和18倍，且在终端应用中实现了效率与精度的平衡。", "motivation": "全局自注意力的二次计算和内存成本严重限制了其在高分辨率图像中的使用。局部注意力降至局部小区间提升了复杂度，然而传统的局部注意力模式往往无法显著加速，因为窗口内的标记在1D序列中不是连续的。为了提高局部注意力的效率，本文提出新的窗口构造方法。", "method": "本文提出了一种基于希尔伯特曲线构造窗口和邻域的新方法。首先将图像标记沿希尔伯特曲线重新排序，然后在重新排序的一维序列上建立窗口和邻域。从块稀疏的角度来看，这一策略显著提高了块稀疏性，并且可以与现有的块稀疏核结合，提高二维局部注意力的效率。", "result": "实验结果表明，提出的希尔伯特窗口注意力和希尔伯特滑动注意力分别加速窗口注意力和滑动注意力约4倍和18倍。同时，应用实例Hilbert Window Transformer和Hilbert Neighborhood Transformer均实现了终端加速，准确度损失小。", "conclusion": "研究证明结合基于希尔伯特曲线的局部注意力和块稀疏核提供了一般且实用的方法可提升图像2D局部注意力的效率。"}}
{"id": "2511.06418", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.06418", "abs": "https://arxiv.org/abs/2511.06418", "authors": ["Sunil Mohan", "Theofanis Karaletsos"], "title": "How Well Do LLMs Understand Drug Mechanisms? A Knowledge + Reasoning Evaluation Dataset", "comment": "An earlier version of this paper appears in IEEE FLLM 2025. GitHub: https://github.com/czi-ai/DrugMechCounterfactuals", "summary": "Two scientific fields showing increasing interest in pre-trained large language models (LLMs) are drug development / repurposing, and personalized medicine. For both, LLMs have to demonstrate factual knowledge as well as a deep understanding of drug mechanisms, so they can recall and reason about relevant knowledge in novel situations. Drug mechanisms of action are described as a series of interactions between biomedical entities, which interlink into one or more chains directed from the drug to the targeted disease. Composing the effects of the interactions in a candidate chain leads to an inference about whether the drug might be useful or not for that disease. We introduce a dataset that evaluates LLMs on both factual knowledge of known mechanisms, and their ability to reason about them under novel situations, presented as counterfactuals that the models are unlikely to have seen during training. Using this dataset, we show that o4-mini outperforms the 4o, o3, and o3-mini models from OpenAI, and the recent small Qwen3-4B-thinking model closely matches o4-mini's performance, even outperforming it in some cases. We demonstrate that the open world setting for reasoning tasks, which requires the model to recall relevant knowledge, is more challenging than the closed world setting where the needed factual knowledge is provided. We also show that counterfactuals affecting internal links in the reasoning chain present a much harder task than those affecting a link from the drug mentioned in the prompt.", "AI": {"tldr": "论文探讨了大型预训练语言模型（LLMs）在药物研发与个性化医疗领域的应用，介绍了评估LLMs事实知识及推理能力的数据集，并展示了不同模型在此数据集上的表现。", "motivation": "许多科学领域对大型预训练语言模型在药物研发与个性化医疗方面的应用表现出浓厚兴趣，但要求这些模型不仅要具备事实知识，还要理解复杂的药物作用机制。因此，作者希望通过建立新的评估数据集来检验这些模型的效果。", "method": "研究中使用了评估药物机制知识及推理能力的数据集，该数据集包含模型训练时未见的反事实情景，用以评估LLMs在开放世界情境中的表现。", "result": "实验结果显示，o4-mini模型在评估中表现优于4o、o3以及o3-mini等模型，且较小的Qwen3-4B-thinking模型也有竞争力的表现，在某些情况下甚至超越了o4-mini。", "conclusion": "研究得出结论，不同模型在处理涉及内部链接的反事实推理任务上存在较大差异，涉及药物链内部链接的推理比仅涉及链上药物的推理更复杂。"}}
{"id": "2511.05833", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05833", "abs": "https://arxiv.org/abs/2511.05833", "authors": ["Taixi Chen", "Yiu-ming Cheung"], "title": "TYrPPG: Uncomplicated and Enhanced Learning Capability rPPG for Remote Heart Rate Estimation", "comment": "The 6th International Workshop on AI for Social Good in the Connected World (AI4SG)@ IEEE WI-IAT 2025", "summary": "Remote photoplethysmography (rPPG) can remotely extract physiological signals from RGB video, which has many advantages in detecting heart rate, such as low cost and no invasion to patients. The existing rPPG model is usually based on the transformer module, which has low computation efficiency. Recently, the Mamba model has garnered increasing attention due to its efficient performance in natural language processing tasks, demonstrating potential as a substitute for transformer-based algorithms. However, the Mambaout model and its variants prove that the SSM module, which is the core component of the Mamba model, is unnecessary for the vision task. Therefore, we hope to prove the feasibility of using the Mambaout-based module to remotely learn the heart rate. Specifically, we propose a novel rPPG algorithm called uncomplicated and enhanced learning capability rPPG (TYrPPG). This paper introduces an innovative gated video understanding block (GVB) designed for efficient analysis of RGB videos. Based on the Mambaout structure, this block integrates 2D-CNN and 3D-CNN to enhance video understanding for analysis. In addition, we propose a comprehensive supervised loss function (CSL) to improve the model's learning capability, along with its weakly supervised variants. The experiments show that our TYrPPG can achieve state-of-the-art performance in commonly used datasets, indicating its prospects and superiority in remote heart rate estimation. The source code is available at https://github.com/Taixi-CHEN/TYrPPG.", "AI": {"tldr": "A new rPPG approach, called TYrPPG, is proposed, which leverages a GVB and CSL for efficient and effective remote heart rate estimation.", "motivation": "To overcome the computational inefficiencies of existing rPPG models and prove the potential of the Mambaout-based module for extracting heart rate from RGB videos.", "method": "This paper proposes an innovative rPPG algorithm named TYrPPG, incorporating a GVB based on Mambaout structure, which integrates 2D-CNN and 3D-CNN to enhance video analysis, and a CSL loss function to boost learning capability.", "result": "Experiments demonstrate that TYrPPG achieves state-of-the-art performance in heart rate estimation on commonly used datasets.", "conclusion": "The proposed model, TYrPPG, shows promising results for remote heart rate estimation, indicating its potential for practical applications and future research."}}
{"id": "2511.06427", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.06427", "abs": "https://arxiv.org/abs/2511.06427", "authors": ["Lifeng Han", "David Lindevelt", "Sander Puts", "Erik van Mulligen", "Suzan Verberne"], "title": "Dutch Metaphor Extraction from Cancer Patients' Interviews and Forum Data using LLMs and Human in the Loop", "comment": "Ongoing project report, on behalf of 4D PICTURE https://4dpicture.eu/", "summary": "Metaphors and metaphorical language (MLs) play an important role in healthcare communication between clinicians, patients, and patients' family members. In this work, we focus on Dutch language data from cancer patients. We extract metaphors used by patients using two data sources: (1) cancer patient storytelling interview data and (2) online forum data, including patients' posts, comments, and questions to professionals. We investigate how current state-of-the-art large language models (LLMs) perform on this task by exploring different prompting strategies such as chain of thought reasoning, few-shot learning, and self-prompting. With a human-in-the-loop setup, we verify the extracted metaphors and compile the outputs into a corpus named HealthQuote.NL. We believe the extracted metaphors can support better patient care, for example shared decision making, improved communication between patients and clinicians, and enhanced patient health literacy. They can also inform the design of personalized care pathways. We share prompts and related resources at https://github.com/aaronlifenghan/HealthQuote.NL", "AI": {"tldr": "本文聚焦于从荷兰语的数据中提取癌症患者所使用的比喻性语言，使用最先进语言模型并结合多种提示策略，最终创建HealthQuote.NL语料库以支持改善医疗服务。", "motivation": "癌症患者与临床医生、患者家人之间使用比喻性语言在医疗沟通中起到了重要作用。研究目的是通过提取这些比喻性语言，以改善患者护理、增强患者健康素养及设计个性化护理路径。", "method": "本研究通过两种数据源提取癌症患者使用的比喻性语言：(1)癌症患者叙述访谈数据；(2)在线论坛数据，包括患者对专业人士发布的帖子、评论和提问。通过探索不同提示策略如链式思维推理、少样本学习和自提提示语句来评估当前最先进大语言模型在这个任务上的表现。", "result": "在人类介入的设置下，验证了提取出的比喻并整理成一个名为HealthQuote.NL的语料库。", "conclusion": "通过本研究提取的比喻性语言可以支持更优质的医疗护理，例如，共享决策制定、改善患者与临床医生之间的沟通并提升患者健康素养。同时，这些语言也能为个性化护理路径的设计提供信息。"}}
{"id": "2511.05841", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05841", "abs": "https://arxiv.org/abs/2511.05841", "authors": ["Changqing Gong", "Huafeng Qin", "Mounim A. El-Yacoubi"], "title": "Understanding Cross Task Generalization in Handwriting-Based Alzheimer's Screening via Vision Language Adaptation", "comment": null, "summary": "Alzheimer's disease is a prevalent neurodegenerative disorder for which early detection is critical. Handwriting-often disrupted in prodromal AD-provides a non-invasive and cost-effective window into subtle motor and cognitive decline. Existing handwriting-based AD studies, mostly relying on online trajectories and hand-crafted features, have not systematically examined how task type influences diagnostic performance and cross-task generalization. Meanwhile, large-scale vision language models have demonstrated remarkable zero or few-shot anomaly detection in natural images and strong adaptability across medical modalities such as chest X-ray and brain MRI. However, handwriting-based disease detection remains largely unexplored within this paradigm. To close this gap, we introduce a lightweight Cross-Layer Fusion Adapter framework that repurposes CLIP for handwriting-based AD screening. CLFA implants multi-level fusion adapters within the visual encoder to progressively align representations toward handwriting-specific medical cues, enabling prompt-free and efficient zero-shot inference. Using this framework, we systematically investigate cross-task generalization-training on a specific handwriting task and evaluating on unseen ones-to reveal which task types and writing patterns most effectively discriminate AD. Extensive analyses further highlight characteristic stroke patterns and task-level factors that contribute to early AD identification, offering both diagnostic insights and a benchmark for handwriting-based cognitive assessment.", "AI": {"tldr": "本文提出了一种基于CLIP的跨层融合适配器框架，用于基于手写的阿尔茨海默病筛查。该框架能够高效地进行无提示、零样本推理，并系统地调查了任务类型对筛查性能的影响。", "motivation": "既往的研究主要依赖于在线轨迹和手工特征提取，但未系统调查任务类型对诊断性能和跨任务泛化能力的影响。此外，大规模视觉语言模型在自然图像和医学模态中显示出良好的零样本异常检测能力，但在手写疾病检测中研究较少。为了填补这一空白，我们提出了该框架。", "method": "我们引入了一种轻量级的跨层融合适配器框架(Cross-Layer Fusion Adapter, CLFA)，该框架利用CLIP进行基于手写的阿尔茨海默病筛查。CLFA在视觉编码器中嵌入了多级融合适配器，逐步对齐特定于手写医疗特征的表示，从而能够进行无提示且高效的零样本推理。", "result": "通过该框架，我们系统地研究了跨任务泛化，揭示了哪些任务类型和书写模式最能有效区分阿尔茨海默病，进一步突出了特征笔画模式和任务级别因素，这些都是早期阿尔茨海默病识别中的重要诊断信息和基准。", "conclusion": "该研究不仅为早期阿尔茨海默病的筛查提供了有价值的诊断见解，还为基于手书的认知评估提供了一个基准。研究结果表明，在无提示且高效的零样本框架下进行手写疾病检测是可行的，为未来的相关研究奠定了基础。"}}
{"id": "2511.06441", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06441", "abs": "https://arxiv.org/abs/2511.06441", "authors": ["Mayank Saini", "Arit Kumar Bishwas"], "title": "Towards Resource-Efficient Multimodal Intelligence: Learned Routing among Specialized Expert Models", "comment": "15 pages, 4 figures", "summary": "As AI moves beyond text, large language models (LLMs) increasingly power vision, audio, and document understanding; however, their high inference costs hinder real-time, scalable deployment. Conversely, smaller open-source models offer cost advantages but struggle with complex or multimodal queries. We introduce a unified, modular framework that intelligently routes each query - textual, multimodal, or complex - to the most fitting expert model, using a learned routing network that balances cost and quality. For vision tasks, we employ a two-stage open-source pipeline optimized for efficiency and reviving efficient classical vision components where they remain SOTA for sub-tasks. On benchmarks such as Massive Multitask Language Understanding (MMLU) and Visual Question Answering (VQA), we match or exceed the performance of always-premium LLM (monolithic systems with one model serving all query types) performance, yet reduce the reliance on costly models by over 67%. With its extensible, multi-agent orchestration, we deliver high-quality, resource-efficient AI at scale.", "AI": {"tldr": "一种新的AI框架通过智能分配复杂或多样化的查询到合适的模型来降低高昂的计算成本，达到了与大型语言模型相当的性能，但将对昂贵模型的依赖减少了67%以上。", "motivation": "尽管大型语言模型在视觉、音频和文档理解方面日益强大，但高昂的推理成本阻碍了其实时部署和可扩展性。相比之下，较小的开源模型虽然具有成本优势，但在处理复杂或多模态查询时表现不佳。因此，需要一种能平衡成本和质量，且适用于多种查询类型的新型框架。", "method": "该研究设计了一种基于学习路由网络的统一、模块化框架，该框架能够根据查询类型智能地将任务分配给最适合的专家模型。对于视觉任务，采用了一个高效率的开源两阶段管道，采用经典计算机视觉组件处理子任务。", "result": "研究提出了一种统一且模块化的框架，该框架使用学习路由网络将文本、多模态或复杂的查询智能地定向到最合适的专家模型，以平衡成本和质量。在视觉任务中，使用了高效率的经典视觉组件优化后的两阶段开源管道。该框架在MMLU和VQA基准测试中达到了与大型语言模型（专用于处理所有查询类型）相当或更好的性能，同时对昂贵模型的依赖降低了超过67%。因此，实现了高质量、资源高效的大规模人工智能。", "conclusion": "研究提出的框架在主流基准测试中达到了与大型语言模型相当或更好的性能，同时大幅降低了使用高昂模型的需求，证明了其在高效、高质量的人工智能应用中具有扩展性。"}}
{"id": "2511.05844", "categories": ["cs.CV", "cs.AI", "cs.IT", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.05844", "abs": "https://arxiv.org/abs/2511.05844", "authors": ["Seyed Alireza Javid", "Amirhossein Bagheri", "Nuria González-Prelcic"], "title": "Enhancing Diffusion Model Guidance through Calibration and Regularization", "comment": "Accepted from NeurIPS 2025 Workshop on Structured Probabilistic Inference & Generative Modeling. Code available at https://github.com/ajavid34/guided-info-diffusion", "summary": "Classifier-guided diffusion models have emerged as a powerful approach for conditional image generation, but they suffer from overconfident predictions during early denoising steps, causing the guidance gradient to vanish. This paper introduces two complementary contributions to address this issue. First, we propose a differentiable calibration objective based on the Smooth Expected Calibration Error (Smooth ECE), which improves classifier calibration with minimal fine-tuning and yields measurable improvements in Frechet Inception Distance (FID). Second, we develop enhanced sampling guidance methods that operate on off-the-shelf classifiers without requiring retraining. These include tilted sampling with batch-level reweighting, adaptive entropy-regularized sampling to preserve diversity, and a novel f-divergence-based sampling strategy that strengthens class-consistent guidance while maintaining mode coverage. Experiments on ImageNet 128x128 demonstrate that our divergence-regularized guidance achieves an FID of 2.13 using a ResNet-101 classifier, improving upon existing classifier-guided diffusion methods while requiring no diffusion model retraining. The results show that principled calibration and divergence-aware sampling provide practical and effective improvements for classifier-guided diffusion.", "AI": {"tldr": "本文解决了分类器引导扩散模型中由于过分自信的预测所导致的早期去噪梯度消失的问题，引入了两种改进方法，其中包含基于平滑期望校准误差的可微校准目标以及几种不同的采样方法，从而改善了模型表现，实验结果显示可大幅度提高FID性能。", "motivation": "分类器引导扩散模型，尽管是用于条件图像生成的强大方法之一，但在早期去噪步骤中过度自信的预测导致了指导梯度的消失。", "method": "本文提出了两种互补的方法来解决分类器引导扩散模型在早期去噪步骤中过度自信预测的问题，导致引导梯度消失。首先，引入了一个基于平滑期望校准误差（Smooth ECE）的可微校准目标，以改善分类器的校准，并且仅需最少的微调，从而大大提高了Frechet Inception Distance (FID)。其次，开发了一些增强的采样指导方法，这些方法可以在现成的分类器上操作，并不需要重新训练。这些方法包括具有批次级重加权的倾斜抽样、保留多样性的自适应熵正则化抽样，以及一种新的f-散度抽样策略，该策略可以增强类别一致性的指导，同时保持模式覆盖率。", "result": "在ImageNet 128x128上的实验表明，提出的发散正则化指导在FID上实现了2.13的结果，使用的是ResNet-101分类器，优于现有的分类器引导扩散模型方法，并且不需要扩散模型的重新训练。", "conclusion": "结果表明，通过基于发散的采样和校准，可以对分类器引导扩散模型提供有效的改进，而不需要对扩散模型进行重新训练。"}}
{"id": "2511.06446", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06446", "abs": "https://arxiv.org/abs/2511.06446", "authors": ["Bohan Yu", "Wei Huang", "Kang Liu"], "title": "SR-KI: Scalable and Real-Time Knowledge Integration into LLMs via Supervised Attention", "comment": "Accepted by AAAI 2026", "summary": "This paper proposes SR-KI, a novel approach for integrating real-time and large-scale structured knowledge bases (KBs) into large language models (LLMs). SR-KI begins by encoding KBs into key-value pairs using a pretrained encoder, and injects them into LLMs' KV cache. Building on this representation, we employ a two-stage training paradigm: first locating a dedicated retrieval layer within the LLM, and then applying an attention-based loss at this layer to explicitly supervise attention toward relevant KB entries. Unlike traditional retrieval-augmented generation methods that rely heavily on the performance of external retrievers and multi-stage pipelines, SR-KI supports end-to-end inference by performing retrieval entirely within the models latent space. This design enables efficient compression of injected knowledge and facilitates dynamic knowledge updates. Comprehensive experiments demonstrate that SR-KI enables the integration of up to 40K KBs into a 7B LLM on a single A100 40GB GPU, and achieves strong retrieval performance, maintaining over 98% Recall@10 on the best-performing task and exceeding 88% on average across all tasks. Task performance on question answering and KB ID generation also demonstrates that SR-KI maintains strong performance while achieving up to 99.75% compression of the injected KBs.", "AI": {"tldr": "SR-KI是一种用于将大规模实时知识图谱整合到大语言模型中的方法，它通过内部检索和两阶段训练范式实现端到端的推理，实验表明它可以有效压缩知识并在多个任务上保持高性能。", "motivation": "SR-KI的设计目的是为了能够高效地将大规模实时知识图谱整合到大语言模型中，不同于传统的检索增强生成方法依赖外部检索器和多阶段的流水线，SR-KI能够在模型内部完成知识整合，实现高效的压缩和更新。", "method": "SR-KI采用了一种新颖的方法，通过预训练的编码器将知识图谱编码为键值对，并将其注入到大模型的KV缓存中。该方法采用两阶段训练范式：首先在大模型中定位一个专用的检索层，然后在该层应用基于注意力的损失，以明确地引导注意力指向相关的知识图谱条目。SR-KI可以在模型的潜在空间内完成检索，从而支持端到端的推理。", "result": "实验表明，SR-KI可以在7B模型上整合多达40K的知识图谱，性能非常突出，能够保持98%以上的Recall@10和超过88%的平均召回率。同时，SR-KI在问题回答和知识图谱ID生成任务上也表现出色，即使压缩率高达99.75%也能保持强大的性能。", "conclusion": "SR-KI通过其独特的设计，有效地整合了大规模实时知识图谱到大语言模型中，实现了高效的压缩和更新，同时保持了高性能的检索性能和任务表现。这为大语言模型的应用拓展提供了新的可能性。"}}
{"id": "2511.05853", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05853", "abs": "https://arxiv.org/abs/2511.05853", "authors": ["Bingyang Guo", "Qiang Zuo", "Ruiyun Yu"], "title": "Point Cloud Segmentation of Integrated Circuits Package Substrates Surface Defects Using Causal Inference: Dataset Construction and Methodology", "comment": null, "summary": "The effective segmentation of 3D data is crucial for a wide range of industrial applications, especially for detecting subtle defects in the field of integrated circuits (IC). Ceramic package substrates (CPS), as an important electronic material, are essential in IC packaging owing to their superior physical and chemical properties. However, the complex structure and minor defects of CPS, along with the absence of a publically available dataset, significantly hinder the development of CPS surface defect detection. In this study, we construct a high-quality point cloud dataset for 3D segmentation of surface defects in CPS, i.e., CPS3D-Seg, which has the best point resolution and precision compared to existing 3D industrial datasets. CPS3D-Seg consists of 1300 point cloud samples under 20 product categories, and each sample provides accurate point-level annotations. Meanwhile, we conduct a comprehensive benchmark based on SOTA point cloud segmentation algorithms to validate the effectiveness of CPS3D-Seg. Additionally, we propose a novel 3D segmentation method based on causal inference (CINet), which quantifies potential confounders in point clouds through Structural Refine (SR) and Quality Assessment (QA) Modules. Extensive experiments demonstrate that CINet significantly outperforms existing algorithms in both mIoU and accuracy.", "AI": {"tldr": "We introduce CPS3D-Seg, a high-quality 3D point cloud dataset for CPS surface defect segmentation, and propose a new 3D segmentation method, CINet, which outperforms existing algorithms.", "motivation": "The lack of publicly available datasets and the complex structure of Ceramic Package Substrates (CPS) with minor defects have hindered the development of CPS surface defect detection. To address this, we constructed the CPS3D-Seg dataset and proposed CINet for 3D segmentation.", "method": "We propose a novel 3D segmentation method called CINet based on causal inference with Structural Refine (SR) and Quality Assessment (QA) modules to quantify potential confounders in point clouds.", "result": "Experiments show that our proposed CINet significantly outperforms existing algorithms in terms of mean Intersection over Union (mIoU) and accuracy.", "conclusion": "The CPS3D-Seg dataset and our proposed CINet method have the potential to significantly advance the identification and resolution of surface defects in CPS, enhancing the quality control in IC packaging industry."}}
{"id": "2511.06497", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06497", "abs": "https://arxiv.org/abs/2511.06497", "authors": ["Quang Phuoc Nguyen", "David Anugraha", "Felix Gaschi", "Jun Bin Cheng", "En-Shiun Annie Lee"], "title": "Rethinking what Matters: Effective and Robust Multilingual Realignment for Low-Resource Languages", "comment": "Accepted to IJCNLP-AACL 2025", "summary": "Realignment is a promising strategy to improve cross-lingual transfer in multilingual language models. However, empirical results are mixed and often unreliable, particularly for typologically distant or low-resource languages (LRLs) compared to English. Moreover, word realignment tools often rely on high-quality parallel data, which can be scarce or noisy for many LRLs. In this work, we conduct an extensive empirical study to investigate whether realignment truly benefits from using all available languages, or if strategically selected subsets can offer comparable or even improved cross-lingual transfer, and study the impact on LRLs. Our controlled experiments show that realignment can be particularly effective for LRLs and that using carefully selected, linguistically diverse subsets can match full multilingual alignment, and even outperform it for unseen LRLs. This indicates that effective realignment does not require exhaustive language coverage and can reduce data collection overhead, while remaining both efficient and robust when guided by informed language selection.", "AI": {"tldr": "研究发现，精心选择的多语言子集在语言重对齐中能够有效提升低资源语言的跨语言迁移效果，甚至优于全多语言对齐。", "motivation": "现有语言重对齐策略在不同类型和资源稀缺的语言上效果不稳定且依赖高质量平行数据。", "method": "进行了一系列控制实验，对比使用不同语言子集进行重对齐的效果。", "result": "精心挑选的多语言子集能够达到全面多语言对齐的效果，尤其在低资源语言上表现优异。", "conclusion": "语言重对齐的有效性并不一定依赖于广泛的多语言覆盖，而是可以通过精明的语言选择策略实现高效和鲁棒的跨语言迁移。"}}
{"id": "2511.05865", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.05865", "abs": "https://arxiv.org/abs/2511.05865", "authors": ["Viet Nguyen", "Vishal M. Patel"], "title": "CGCE: Classifier-Guided Concept Erasure in Generative Models", "comment": "24 pages, 15 figures", "summary": "Recent advancements in large-scale generative models have enabled the creation of high-quality images and videos, but have also raised significant safety concerns regarding the generation of unsafe content. To mitigate this, concept erasure methods have been developed to remove undesirable concepts from pre-trained models. However, existing methods remain vulnerable to adversarial attacks that can regenerate the erased content. Moreover, achieving robust erasure often degrades the model's generative quality for safe, unrelated concepts, creating a difficult trade-off between safety and performance. To address this challenge, we introduce Classifier-Guided Concept Erasure (CGCE), an efficient plug-and-play framework that provides robust concept erasure for diverse generative models without altering their original weights. CGCE uses a lightweight classifier operating on text embeddings to first detect and then refine prompts containing undesired concepts. This approach is highly scalable, allowing for multi-concept erasure by aggregating guidance from several classifiers. By modifying only unsafe embeddings at inference time, our method prevents harmful content generation while preserving the model's original quality on benign prompts. Extensive experiments show that CGCE achieves state-of-the-art robustness against a wide range of red-teaming attacks. Our approach also maintains high generative utility, demonstrating a superior balance between safety and performance. We showcase the versatility of CGCE through its successful application to various modern T2I and T2V models, establishing it as a practical and effective solution for safe generative AI.", "AI": {"tldr": "The paper presents Classifier-Guided Concept Erasure (CGCE), a scalable and robust method for concept erasure in generative models, ensuring safety without compromising generative quality.", "motivation": "To enhance safety in generative models, reducing the risk of producing harmful content while maintaining high-quality output for safe concepts.", "method": "The method introduces CGCE, a plug-and-play framework using lightweight classifiers on text embeddings to detect and refine prompts containing undesired concepts.", "result": "Experiments demonstrate that CGCE effectively erases concepts and resists adversarial attacks, achieving a better balance between safety and generative performance than existing methods.", "conclusion": "CGCE is a versatile solution that provides efficient and robust concept erasure, making it a practical tool for ensuring the safety and high quality of outputs in various modern text-to-image and text-to-video models."}}
{"id": "2511.06516", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.06516", "abs": "https://arxiv.org/abs/2511.06516", "authors": ["Amit LeVi", "Raz Lapid", "Rom Himelstein", "Yaniv Nemcovsky", "Ravid Shwartz Ziv", "Avi Mendelson"], "title": "You Had One Job: Per-Task Quantization Using LLMs' Hidden Representations", "comment": null, "summary": "Large Language Models (LLMs) excel across diverse tasks, yet many applications require only limited capabilities, making large variants inefficient in memory and latency. Existing approaches often combine distillation and quantization, but most post-training quantization (PTQ) methods are task-agnostic, ignoring how task-specific signals are distributed across layers. In this work, we propose to use hidden representations that encode task-salient signals as a guideline for quantization. In order to fully utilize our innovative idea, this paper compares two new task-aware PTQ methods: Task-Aware Quantization (TAQ), which allocates bitwidths using task-conditioned statistics from hidden activations, and TAQO, which allocates precision based on direct layer sensitivity tests. From a small calibration set, these approaches identify task-relevant layers, preserving their precision while aggressively quantizing the rest. This yields stable task sensitivity profiles and efficient task-specialized models. Across models, TAQ and TAQO outperform the baselines; TAQ leads on Phi-4, while TAQO leads on Llama-3.1, Qwen3, and Qwen2.5. For instances, on Phi-4 it achieves 42.33 EM / 50.81 F1, far surpassing Activation-aware Weight Quantization (AWQ) (2.25 / 7.07), while remaining within < 1.0% of the original accuracy at lower average precision.", "AI": {"tldr": "本文提出了一种基于任务相关信号指导的最新量化方法，以提升模型在特定任务上的性能和效率。TAQ 和 TAQO 方法在多个模型上优于传统的量化方法，实现更高的精度，并且在精度损失小于 1% 的情况下，做到更高效的任务特定模型。", "motivation": "大型语言模型在多种任务上表现出色，但许多应用仅需要有限的能力，使得大型模型在内存和延迟上变得低效。现有的方法通常结合了蒸馏和量化，然而大多数后训练量化（PTQ）方法都是任务无关的，忽视了任务特定信号如何在各个层之间分布。因此，该论文试图通过利用隐藏表示中的任务相关信号作为量化指导，提高模型的效率和任务特定的性能。", "method": "此论文提出了两种新的方法，Task-Aware Quantization (TAQ) 和 TAQO，用于任务相关的后训练量化方法。TAQ 方法利用隐藏层激活的任务条件统计数据来分配位宽，而 TAQO 方法则是基于直接的层敏感性测试来进行精度分配。这些方法从一个小的校准集中识别出任务相关的层，保持这些层的精度，而对其他部分进行激进的量化。", "result": "实验结果表明，这两种方法在不同的模型上都优于基线方法。TAQ 在 Phi-4 模型上表现最佳，而 TAQO 在 Llama-3.1, Qwen3, 和 Qwen2.5 模型上表现最佳。例如，在 Phi-4 模型上，TAQ 达到了 42.33 EM / 50.81 F1，远超传统激活感知权重量化（AWQ）的 (2.25 / 7.07)，同时保持低于原始精度的 1.0% 以内。", "conclusion": "论文得出结论，基于任务感知的量化方法（TAQ 和 TAQO）能够有效提高模型在特定任务上的性能，同时降低内存和延迟，这对于那些只需要一部分功能的应用来说是更为高效的解决方案。"}}
{"id": "2511.05866", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05866", "abs": "https://arxiv.org/abs/2511.05866", "authors": ["Suresh Nehra", "Aupendu Kar", "Jayanta Mukhopadhyay", "Prabir Kumar Biswas"], "title": "Light-Field Dataset for Disparity Based Depth Estimation", "comment": "This paper has been accepted to ACM ICVGIP 2025", "summary": "A Light Field (LF) camera consists of an additional two-dimensional array of micro-lenses placed between the main lens and sensor, compared to a conventional camera. The sensor pixels under each micro-lens receive light from a sub-aperture of the main lens. This enables the image sensor to capture both spatial information and the angular resolution of a scene point. This additional angular information is used to estimate the depth of a 3-D scene. The continuum of virtual viewpoints in light field data enables efficient depth estimation using Epipolar Line Images (EPIs) with robust occlusion handling. However, the trade-off between angular information and spatial information is very critical and depends on the focal position of the camera. To design, develop, implement, and test novel disparity-based light field depth estimation algorithms, the availability of suitable light field image datasets is essential. In this paper, a publicly available light field image dataset is introduced and thoroughly described. We have also demonstrated the effect of focal position on the disparity of a 3-D point as well as the shortcomings of the currently available light field dataset. The proposed dataset contains 285 light field images captured using a Lytro Illum LF camera and 13 synthetic LF images. The proposed dataset also comprises a synthetic dataset with similar disparity characteristics to those of a real light field camera. A real and synthetic stereo light field dataset is also created by using a mechanical gantry system and Blender. The dataset is available at https://github.com/aupendu/light-field-dataset.", "AI": {"tldr": "本论文提出并详细描述了一个新的光场图像数据集，包括285张真实光场图像和13张合成图像，并分析了焦距位置的影响及现有数据集的局限性。", "motivation": "论文旨在解决光场深度估计算法开发过程中所需的数据集问题，提供一个适合研究的光场图像数据集。同时，论文分析了焦距位置对三维点的视差影响以及当前可用的光场数据集的不足。", "method": "本论文介绍并详细描述了一个公开可用的光场图像数据集，该数据集包含285张使用Lytro Illum光场摄像机捕捉到的光场图像及13张合成的光场图像。此外，还提出了一种通过机械龙门系统和Blender创建的真实与合成立体光场数据集。", "result": "提出的数据集包含了使用Lytro Illum光场摄像机捕捉到的285张光场图像及13张合成光场图像，此外还包含了与实际光场相机具有类似视差特征的合成数据集。真实和合成的立体光场数据集是通过机械龙门系统和Blender创建的。", "conclusion": "论文提供了一个包含285张真实光场图像和13张合成图像的数据集以支持光场深度估计算法的研究，同时指出焦距位置对视差有重要影响。"}}
{"id": "2511.06530", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.06530", "abs": "https://arxiv.org/abs/2511.06530", "authors": ["Xiaonan Luo", "Yue Huang", "Ping He", "Xiangliang Zhang"], "title": "Better Datasets Start From RefineLab: Automatic Optimization for High-Quality Dataset Refinement", "comment": null, "summary": "High-quality Question-Answer (QA) datasets are foundational for reliable Large Language Model (LLM) evaluation, yet even expert-crafted datasets exhibit persistent gaps in domain coverage, misaligned difficulty distributions, and factual inconsistencies. The recent surge in generative model-powered datasets has compounded these quality challenges. In this work, we introduce RefineLab, the first LLM-driven framework that automatically refines raw QA textual data into high-quality datasets under a controllable token-budget constraint. RefineLab takes a set of target quality attributes (such as coverage and difficulty balance) as refinement objectives, and performs selective edits within a predefined token budget to ensure practicality and efficiency. In essence, RefineLab addresses a constrained optimization problem: improving the quality of QA samples as much as possible while respecting resource limitations. With a set of available refinement operations (e.g., rephrasing, distractor replacement), RefineLab takes as input the original dataset, a specified set of target quality dimensions, and a token budget, and determines which refinement operations should be applied to each QA sample. This process is guided by an assignment module that selects optimal refinement strategies to maximize overall dataset quality while adhering to the budget constraint. Experiments demonstrate that RefineLab consistently narrows divergence from expert datasets across coverage, difficulty alignment, factual fidelity, and distractor quality. RefineLab pioneers a scalable, customizable path to reproducible dataset design, with broad implications for LLM evaluation.", "AI": {"tldr": "RefineLab自动优化QA数据集，提高其质量并解决领域覆盖、难度分布和事实一致性问题，同时在给定的token预算内操作。", "motivation": "高质量的QA数据集对于评估大规模语言模型至关重要，但即使专家编制的数据集也存在缺陷。RefineLab旨在通过自动化的手段提高数据集的质量。", "method": "RefineLab是一个LLM驱动的框架，根据目标质量属性（如覆盖面和难度平衡）进行选择性编辑，并在预定义的token预算下操作。", "result": "实验表明，RefineLab能有效地减少与专家数据集之间的偏差，在覆盖面、难度一致性、事实精度和干扰项质量等方面表现出色。", "conclusion": "RefineLab提供了一种可扩展、可定制的方法来设计可再现的数据集，并为大规模语言模型评估带来了广泛的积极影响。"}}
{"id": "2511.05876", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05876", "abs": "https://arxiv.org/abs/2511.05876", "authors": ["Jian Zhu", "Xin Zou", "Jun Sun", "Cheng Luo", "Lei Liu", "Lingfang Zeng", "Ning Zhang", "Bian Wu", "Chang Tang", "Lirong Dai"], "title": "MoEGCL: Mixture of Ego-Graphs Contrastive Representation Learning for Multi-View Clustering", "comment": "AAAI'2026 oral paper", "summary": "In recent years, the advancement of Graph Neural Networks (GNNs) has significantly propelled progress in Multi-View Clustering (MVC). However, existing methods face the problem of coarse-grained graph fusion. Specifically, current approaches typically generate a separate graph structure for each view and then perform weighted fusion of graph structures at the view level, which is a relatively rough strategy. To address this limitation, we present a novel Mixture of Ego-Graphs Contrastive Representation Learning (MoEGCL). It mainly consists of two modules. In particular, we propose an innovative Mixture of Ego-Graphs Fusion (MoEGF), which constructs ego graphs and utilizes a Mixture-of-Experts network to implement fine-grained fusion of ego graphs at the sample level, rather than the conventional view-level fusion. Additionally, we present the Ego Graph Contrastive Learning (EGCL) module to align the fused representation with the view-specific representation. The EGCL module enhances the representation similarity of samples from the same cluster, not merely from the same sample, further boosting fine-grained graph representation. Extensive experiments demonstrate that MoEGCL achieves state-of-the-art results in deep multi-view clustering tasks. The source code is publicly available at https://github.com/HackerHyper/MoEGCL.", "AI": {"tldr": "Proposes MoEGCL for fine-grained multi-view clustering by constructing ego graphs and utilizing fine-grained fusion at the sample level.", "motivation": "To address the issue of coarse-grained graph fusion in Multi-View Clustering (MVC) by proposing a more precise fusion mechanism at the sample level.", "method": "Mixture of Ego-Graphs Contrastive Representation Learning (MoEGCL) consisting of Mixture of Ego-Graphs Fusion (MoEGF) and Ego Graph Contrastive Learning (EGCL). MoEGF constructs ego graphs for fine-grained fusion at the sample level. EGCL enhances representation similarity.", "result": "MoEGCL achieves state-of-the-art results in deep multi-view clustering tasks.", "conclusion": "The proposed MoEGCL method improves upon existing multi-view clustering approaches by offering a fine-grained fusion strategy, leading to more accurate clustering."}}
{"id": "2511.06531", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06531", "abs": "https://arxiv.org/abs/2511.06531", "authors": ["Oluwadara Kalejaiye", "Luel Hagos Beyene", "David Ifeoluwa Adelani", "Mmekut-Mfon Gabriel Edet", "Aniefon Daniel Akpan", "Eno-Abasi Urua", "Anietie Andy"], "title": "Ibom NLP: A Step Toward Inclusive Natural Language Processing for Nigeria's Minority Languages", "comment": "Accepted at IJCNLP-AACL", "summary": "Nigeria is the most populous country in Africa with a population of more than 200 million people. More than 500 languages are spoken in Nigeria and it is one of the most linguistically diverse countries in the world. Despite this, natural language processing (NLP) research has mostly focused on the following four languages: Hausa, Igbo, Nigerian-Pidgin, and Yoruba (i.e <1% of the languages spoken in Nigeria). This is in part due to the unavailability of textual data in these languages to train and apply NLP algorithms. In this work, we introduce ibom -- a dataset for machine translation and topic classification in four Coastal Nigerian languages from the Akwa Ibom State region: Anaang, Efik, Ibibio, and Oro. These languages are not represented in Google Translate or in major benchmarks such as Flores-200 or SIB-200. We focus on extending Flores-200 benchmark to these languages, and further align the translated texts with topic labels based on SIB-200 classification dataset. Our evaluation shows that current LLMs perform poorly on machine translation for these languages in both zero-and-few shot settings. However, we find the few-shot samples to steadily improve topic classification with more shots.", "AI": {"tldr": "The paper introduces a dataset named ibom for machine translation and topic classification in four Coastal Nigerian languages: Anaang, Efik, Ibibio, and Oro, showing that current LLMs perform poorly on machine translation but improve in topic classification with more few-shot samples.", "motivation": "The motivation behind this paper is to address the scarcity of textual data for NLP research in the linguistically diverse country of Nigeria, specifically focusing on four languages not represented in major benchmarks or Google Translate.", "method": "The authors created the ibom dataset for machine translation and topic classification in Anaang, Efik, Ibibio, and Oro, and aligned translated texts with topic labels based on the SIB-200 classification dataset.", "result": "Current large language models perform poorly on machine translation tasks in a zero-and-few shot settings for the introduced languages, but show improving performance in few-shot topic classification tasks as more samples are provided.", "conclusion": "The ibom dataset addresses the lack of training data and linguistic diversity in NLP research for Nigerian languages, highlighting the need for further benchmark inclusion of more languages and the potential for improvement in classification tasks with more data."}}
{"id": "2511.05890", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05890", "abs": "https://arxiv.org/abs/2511.05890", "authors": ["Ziqing Ma", "Chang Yang", "Zhichang Guo", "Yao Li"], "title": "Towards Frequency-Adaptive Learning for SAR Despeckling", "comment": "13 pages, 14 figures,9 tables", "summary": "Synthetic Aperture Radar (SAR) images are inherently corrupted by speckle noise, limiting their utility in high-precision applications. While deep learning methods have shown promise in SAR despeckling, most methods employ a single unified network to process the entire image, failing to account for the distinct speckle statistics associated with different spatial physical characteristics. It often leads to artifacts, blurred edges, and texture distortion. To address these issues, we propose SAR-FAH, a frequency-adaptive heterogeneous despeckling model based on a divide-and-conquer architecture. First, wavelet decomposition is used to separate the image into frequency sub-bands carrying different intrinsic characteristics. Inspired by their differing noise characteristics, we design specialized sub-networks for different frequency components. The tailored approach leverages statistical variations across frequencies, improving edge and texture preservation while suppressing noise. Specifically, for the low-frequency part, denoising is formulated as a continuous dynamic system via neural ordinary differential equations, ensuring structural fidelity and sufficient smoothness that prevents artifacts. For high-frequency sub-bands rich in edges and textures, we introduce an enhanced U-Net with deformable convolutions for noise suppression and enhanced features. Extensive experiments on synthetic and real SAR images validate the superior performance of the proposed model in noise removal and structural preservation.", "AI": {"tldr": "A novel despeckling method for SAR images named SAR-FAH, which uses wavelet decomposition to handle different frequency components with tailored networks, showing superior results in noise removal and structural preservation.", "motivation": "Current deep learning methods for SAR despeckling often use a single network, ignoring distinct speckle statistics and leading to artifacts, blurred edges, and texture distortion. This paper aims to address these issues to improve the utility of SAR images in high-precision applications.", "method": "The paper proposes SAR-FAH, a frequency-adaptive heterogeneous despeckling model utilizing wavelet decomposition to divide the image into frequency sub-bands. Neural ordinary differential equations are applied to low-frequency components for structural fidelity, while an enhanced U-Net with deformable convolutions handles high-frequency sub-bands to preserve edges and textures.", "result": "Extensive experiments on synthetic and real SAR images demonstrate that SAR-FAH outperforms existing methods in terms of noise removal while preserving structural, edge, and texture details.", "conclusion": "The proposed SAR-FAH model effectively improves the despeckling process by leveraging frequency-specific despeckling strategies, leading to better noise removal and structural preservation in SAR images."}}
{"id": "2511.06571", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06571", "abs": "https://arxiv.org/abs/2511.06571", "authors": ["Haiyan Zhao", "Zirui He", "Fan Yang", "Ali Payani", "Mengnan Du"], "title": "Rep2Text: Decoding Full Text from a Single LLM Token Representation", "comment": "15 pages, 7 figures, 4 tables", "summary": "Large language models (LLMs) have achieved remarkable progress across diverse tasks, yet their internal mechanisms remain largely opaque. In this work, we address a fundamental question: to what extent can the original input text be recovered from a single last-token representation within an LLM? We propose Rep2Text, a novel framework for decoding full text from last-token representations. Rep2Text employs a trainable adapter that projects a target model's internal representations into the embedding space of a decoding language model, which then autoregressively reconstructs the input text. Experiments on various model combinations (Llama-3.1-8B, Gemma-7B, Mistral-7B-v0.1, Llama-3.2-3B) demonstrate that, on average, over half of the information in 16-token sequences can be recovered from this compressed representation while maintaining strong semantic integrity and coherence. Furthermore, our analysis reveals an information bottleneck effect: longer sequences exhibit decreased token-level recovery while preserving strong semantic integrity. Besides, our framework also demonstrates robust generalization to out-of-distribution medical data.", "AI": {"tldr": "本文提出Rep2Text框架，通过可训练的适配器将内部表示投影到解码器语言模型中以自回归方式重构输入文本，实验表明可以从压缩表示中恢复出50%以上的信息并保持语义连贯性。", "motivation": "在本文中，我们致力于回答一个基本问题：在大规模语言模型中，能否从单个最后一个标记的表示中恢复出原始输入文本的程度有多大？", "method": "我们提出了一种名为Rep2Text的新框架，用于从最后一个标记的表示中解码完整文本。Rep2Text采用可训练的适配器，将目标模型的内部表示投影到解码器语言模型的嵌入空间中，然后自回归地重构输入文本。", "result": "实验发现，平均而言，可以从中压碎的表示中恢复出16个标记序列中的超过一半的信息，同时保持强大的语义完整性和连贯性。此外，我们的框架还能很好地推广到非分布的医疗数据上。", "conclusion": "通过我们的实验展示，文本信息虽然在长度上受到压缩，但在语义上的完整性没有受到太大损失。这表明Rep2Text框架在保持文本语义的同时，能够有效地恢复压缩后的文本。"}}
{"id": "2511.05893", "categories": ["cs.CV", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.05893", "abs": "https://arxiv.org/abs/2511.05893", "authors": ["Hongxia Li", "Ying Ji", "Yongxin Dong", "Yuehua Feng"], "title": "Hybrid second-order gradient histogram based global low-rank sparse regression for robust face recognition", "comment": null, "summary": "Low-rank sparse regression models have been widely applied in the field of face recognition. To further address the challenges caused by complex occlusions and illumination variations, this paper proposes a Hybrid Second-Order Gradient Histogram based Global Low-Rank Sparse Regression (H2H-GLRSR) model. Specifically, a novel feature descriptor called the Hybrid Second-Order Gradient Histogram (H2H) is first designed to more effectively characterize the local structural features of facial images. Then, this descriptor is integrated with the Sparse Regularized Nuclear Norm based Matrix Regression (SR$\\_$NMR). Moreover, a global low-rank constraint is imposed on the residual matrix, enabling the model to better capture the global correlations inherent in structured noise. Experimental results demonstrate that the proposed method significantly outperforms existing regression-based classification approaches under challenging scenarios involving occlusions, illumination changes, and unconstrained environments.", "AI": {"tldr": "提出了一种名为H2H-GLRSR的新模型，融合了新颖的特征描述符H2H和基于稀疏正则化核范数的矩阵回归方法，对包含复杂遮挡和光照变化的面部识别场景进行了有效处理。", "motivation": "为了解决复杂遮挡和光照变化造成的面部识别挑战，提高在这些情况下识别的准确性。", "method": "设计了一种新的特征描述符H2H，用于更有效地刻画面部图像的局部结构特征，并将其与SR_NMR相结合，同时加入全局低秩约束。", "result": "实验表明该方法在复杂的遮挡和光照变化环境下显著优于现有的回归分类方法。", "conclusion": "H2H-GLRSR模型在复杂遮挡和光照变化的情况下表现出优秀的面部识别性能。"}}
{"id": "2511.06582", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06582", "abs": "https://arxiv.org/abs/2511.06582", "authors": ["Jacob Si", "Mike Qu", "Michelle Lee", "Yingzhen Li"], "title": "TabRAG: Tabular Document Retrieval via Structured Language Representations", "comment": "NeurIPS 2025 AI4Tab", "summary": "Ingesting data for Retrieval-Augmented Generation (RAG) involves either fine-tuning the embedding model directly on the target corpus or parsing documents for embedding model encoding. The former, while accurate, incurs high computational hardware requirements, while the latter suffers from suboptimal performance when extracting tabular data. In this work, we address the latter by presenting TabRAG, a parsing-based RAG pipeline designed to tackle table-heavy documents via structured language representations. TabRAG outperforms existing popular parsing-based methods for generation and retrieval. Code is available at https://github.com/jacobyhsi/TabRAG.", "AI": {"tldr": "Introduces TabRAG, a parsing-based RAG method designed for handling table-heavy documents, showing better performance than existing methods.", "motivation": "To improve the performance of Parsing-based RAG on table-heavy documents, addressing the suboptimal performance issue.", "method": "Parsing-based RAG pipeline designed to tackle table-heavy documents via structured language representations.", "result": "TabRAG outperforms existing popular parsing-based methods for generation and retrieval.", "conclusion": "TabRAG is effective for dealing with table-heavy documents and outperforms existing parsing-based RAG methods in terms of generation and retrieval."}}
{"id": "2511.05894", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05894", "abs": "https://arxiv.org/abs/2511.05894", "authors": ["Fei Yu", "Quan Deng", "Shengeng Tang", "Yuehua Li", "Lechao Cheng"], "title": "Open-World 3D Scene Graph Generation for Retrieval-Augmented Reasoning", "comment": "Accepted by AAAI 2026", "summary": "Understanding 3D scenes in open-world settings poses fundamental challenges for vision and robotics, particularly due to the limitations of closed-vocabulary supervision and static annotations. To address this, we propose a unified framework for Open-World 3D Scene Graph Generation with Retrieval-Augmented Reasoning, which enables generalizable and interactive 3D scene understanding. Our method integrates Vision-Language Models (VLMs) with retrieval-based reasoning to support multimodal exploration and language-guided interaction. The framework comprises two key components: (1) a dynamic scene graph generation module that detects objects and infers semantic relationships without fixed label sets, and (2) a retrieval-augmented reasoning pipeline that encodes scene graphs into a vector database to support text/image-conditioned queries. We evaluate our method on 3DSSG and Replica benchmarks across four tasks-scene question answering, visual grounding, instance retrieval, and task planning-demonstrating robust generalization and superior performance in diverse environments. Our results highlight the effectiveness of combining open-vocabulary perception with retrieval-based reasoning for scalable 3D scene understanding.", "AI": {"tldr": "A unified framework for Open-World 3D Scene Graph Generation with Retrieval-Augmented Reasoning is proposed to enable generalizable and interactive 3D scene understanding.", "motivation": "The motivation is to address the fundamental challenges in understanding 3D scenes in open-world settings, particularly due to the limitations of closed-vocabulary supervision and static annotations, by proposing a unified framework that enables generalizable and interactive 3D scene understanding.", "method": "Our method integrates Vision-Language Models (VLMs) with retrieval-based reasoning to support multimodal exploration and language-guided interaction, comprising a dynamic scene graph generation module that detects objects and infers semantic relationships without fixed label sets, and a retrieval-augmented reasoning pipeline that encodes scene graphs into a vector database to support text/image-conditioned queries.", "result": "Our method demonstrates robust generalization and superior performance in diverse environments across four tasks - scene question answering, visual grounding, instance retrieval, and task planning.", "conclusion": "The effectiveness of combining open-vocabulary perception with retrieval-based reasoning for scalable 3D scene understanding is highlighted by the results on 3DSSG and Replica benchmarks."}}
{"id": "2511.06592", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.06592", "abs": "https://arxiv.org/abs/2511.06592", "authors": ["Zhi Rui Tam", "Yun-Nung Chen"], "title": "MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making", "comment": null, "summary": "As large language models transition from text-based interfaces to audio interactions in clinical settings, they might introduce new vulnerabilities through paralinguistic cues in audio. We evaluated these models on 170 clinical cases, each synthesized into speech from 36 distinct voice profiles spanning variations in age, gender, and emotion. Our findings reveal a severe modality bias: surgical recommendations for audio inputs varied by as much as 35% compared to identical text-based inputs, with one model providing 80% fewer recommendations. Further analysis uncovered age disparities of up to 12% between young and elderly voices, which persisted in most models despite chain-of-thought prompting. While explicit reasoning successfully eliminated gender bias, the impact of emotion was not detected due to poor recognition performance. These results demonstrate that audio LLMs are susceptible to making clinical decisions based on a patient's voice characteristics rather than medical evidence, a flaw that risks perpetuating healthcare disparities. We conclude that bias-aware architectures are essential and urgently needed before the clinical deployment of these models.", "AI": {"tldr": "研究发现大型语言模型在临床环境中可能基于患者的语音特征而非医学证据做出临床决策，这可能导致医疗不平等。需要在部署这些模型之前研发出具有偏见意识的架构。", "motivation": "随着大型语言模型从基于文本的界面转向临床环境中的音频交互，它们可能会通过音频中的副语言线索引入新的漏洞。因此，作者评估了这些模型在临床环境中的表现，以揭示潜在的偏见问题。", "method": "作者对170个临床病例进行了评估，每个病例被合成成36种不同语音配置文件的语音，涵盖年龄、性别和情绪的变化。", "result": "", "conclusion": "结论是临床环境中使用的音频语言模型容易受到患者语音特征的影响，致使临床决策偏差，存在引发医疗不平等的风险。因此，迫切需要开发偏见意识架构。"}}
{"id": "2511.05898", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05898", "abs": "https://arxiv.org/abs/2511.05898", "authors": ["Zhaoyang Wang", "Dong Wang"], "title": "GABFusion: Rethinking Feature Fusion for Low-Bit Quantization of Multi-Task Networks", "comment": "9 pages,6 figures", "summary": "Despite the effectiveness of quantization-aware training (QAT) in compressing deep neural networks, its performance on multi-task architectures often degrades significantly due to task-specific feature discrepancies and gradient conflicts. To address these challenges, we propose Gradient-Aware Balanced Feature Fusion (GABFusion), which dynamically balances gradient magnitudes and fuses task-specific features in a quantization-friendly manner. We further introduce Attention Distribution Alignment (ADA), a feature-level distillation strategy tailored for quantized models. Our method demonstrates strong generalization across network architectures and QAT algorithms, with theoretical guarantees on gradient bias reduction. Extensive experiments demonstrate that our strategy consistently enhances a variety of QAT methods across different network architectures and bit-widths. On PASCAL VOC and COCO datasets, the proposed approach achieves average mAP improvements of approximately 3.3% and 1.6%, respectively. When applied to YOLOv5 under 4-bit quantization, our method narrows the accuracy gap with the full-precision model to only 1.7% on VOC, showcasing its effectiveness in preserving performance under low-bit constraints. Notably, the proposed framework is modular, easy to integrate, and compatible with any existing QAT technique-enhancing the performance of quantized models without requiring modifications to the original network architecture.", "AI": {"tldr": "本文提出了GABFusion和ADA以解决QAT在多任务网络上的性能下降问题，实验表明该方法可跨网络架构和QAT算法提升性能，并且该方法易集成兼容现有技术。", "motivation": "为了解决量化感知训练（QAT）在多任务架构上由于任务特定特征差异和梯度冲突而导致性能下降的问题。", "method": "我们提出了Gradient-Aware Balanced Feature Fusion (GABFusion) 动态平衡梯度幅度并融合任务特定特征，同时引入了Attention Distribution Alignment (ADA) 特征级别的蒸馏策略，提高量化模型表现。", "result": "实验表明，我们的策略可以提升多种QAT方法在不同网络架构和比特宽度下的性能。在PASCAL VOC和COCO数据集上，分别取得了平均mAP提升了约3.3%和1.6%。应用于4位量化YOLOv5时，与全精度模型相比，精度差距仅缩小至1.7%。", "conclusion": "该框架是模块化的，容易集成并且兼容任何现有的QAT技术，无需修改原网络架构，即可提升量化模型的性能。"}}
{"id": "2511.06601", "categories": ["cs.CL", "cs.FL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.06601", "abs": "https://arxiv.org/abs/2511.06601", "authors": ["Zi-Niu Wu"], "title": "Duality-based Mode Operations and Pyramid Multilayer Mapping for Rhetorical Modes", "comment": null, "summary": "Rhetorical modes are useful in both academic and non-academic writing, and can be subjects to be studied within linguistic research and computational modeling. Establishing a conceptual bridge among these domains could enable each to benefit from the others. This paper proposes duality-based mode operations (split-unite, forward-backward, expansion-reduction and orthogonal dualities) to expand the set of rhetorical modes, introducing generated modes like combination and generalization, thereby enhancing epistemic diversity across multiple applications. It further presents a pyramid multilayer mapping framework (e.g., three layers from the rhetorical model layer, to cognitive layer, and to epistemic layers) that reduces the resulting cognitive complexity. The degrees of expressive diversity and complexity reduction are quantified through binomial combinatorics and Shannon entropy analysis. A Marginal Rhetorical Bit (MRB) is identified, permitting the definition of a rhetorical-scalable parameter that measures expressive growth speed in bits per stage. A direct entropy measure shows that hierarchical selection over smaller subsets markedly reduces choice uncertainty compared with flat selection across all modes. These considerations appear to transform static and non-measurable rhetorical taxonomies into more dynamic and more measurable systems for discourse design. From this work, it would be possible to identify a pathway for future AI systems to operate not only on language tokens but on layered rhetorical reasoning structures, bridging linguistic, pedagogical, academic, and computational research", "AI": {"tldr": "论文提出了新的修辞模式运算，增强认知多样性，并提出分层映射框架降低认知复杂性，量化了表达多样性与复杂性，建立边际修辞位参数。这可能会为未来AI系统提供处理分层修辞推理结构的方法，实现修辞系统从静态、不可衡量向动态、可衡量转变。", "motivation": "修辞模式在学术和非学术写作中都很有用，可以作为语言研究和计算模型的主题。建立这样一个跨领域的概念桥梁可以帮助彼此受益。", "method": "提出四种二元性质的操作——分割-统一，正向-逆向，扩展-简化和正交二元性，以此扩展修辞模式集合，并生成新的模式，如组合和概括。通过这一方法，可增强多个应用中的认知多样性。此外，还提出了一种分层映射框架（三个层次：修辞模型层、认知层和认知层）来降低因此产生的认知复杂度。", "result": "通过二项式组合和香农熵分析量化表达多样性和复杂性减少的程度。确定了边际修辞位（MRB），允许定义一个可衡量修辞增长速率的参数。根据直接熵度量，分层选择较小的子集相比在整个模式中进行平铺选择明显降低选择不确定性。", "conclusion": "这项工作可能为未来的AI系统提供一条途径，使其不仅能够处理语言标记，还能够处理分层的修辞推理结构，以及连接语言学、教学法、学术和计算研究。"}}
