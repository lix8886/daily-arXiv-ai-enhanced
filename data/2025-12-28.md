<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 23]
- [cs.CV](#cs.CV) [Total: 19]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Uncovering Competency Gaps in Large Language Models and Their Benchmarks](https://arxiv.org/abs/2512.20638)
*Matyas Bohacek,Nino Scherrer,Nicholas Dufour,Thomas Leung,Christoph Bregler,Stephanie C. Y. Chan*

Main category: cs.CL

> 本文提出了一种新的方法，利用稀疏自动编码器(SAEs)自动发现大型语言模型(LLMs)内部表示的“模型差距”和标准基准中的“基准差距”。该方法基于提取SAE概念激活并计算跨基准数据的显著性加权性能分数，实现了对模型性能的评估和基准之间的比较，并展示了方法在两个开源模型和十个基准测试中的应用效果，揭示了模型在特定概念上的弱点以及基准测试的覆盖不平衡问题。

<details>
  <summary>Details</summary>

**Motivation:** 当前，对于大型语言模型的评估严重依赖标准化的基准测试，这些基准测试提供了一些给定能力的汇总指标，但汇总指标可能掩盖模型不足的地方和基准测试本身覆盖不均衡的问题。本文旨在解决这些不足，提出了基于稀疏自动编码器的方法。

**Method:** Structure

**Result:** 通过应用本文提出的方法，发现这些模型在与奉承行为相对的概念上（例如礼貌地拒绝请求或设定边界）和与安全讨论相关的概念上表现较差，同时发现很多评估基准过度代表性服从、权威或指令执行的概念，而缺乏核心概念的体现。该方法通过提供概念层面的分解，揭示了模型评分的原因和基准应该如何改进。

**Conclusion:** 本文提出的方法提供了一种基于表示的方法来评估模型和基准，方法能够揭示模型的弱点并指出基准测试的不平衡问题。这种方法与传统的汇总指标相辅相成，能够揭示模型在特定概念上的表现原因并揭示基准测试的潜在方向。

**Abstract:** The evaluation of large language models (LLMs) relies heavily on standardized benchmarks. These benchmarks provide useful aggregated metrics for a given capability, but those aggregated metrics can obscure (i) particular sub-areas where the LLMs are weak ("model gaps") and (ii) imbalanced coverage in the benchmarks themselves ("benchmark gaps"). We propose a new method that uses sparse autoencoders (SAEs) to automatically uncover both types of gaps. By extracting SAE concept activations and computing saliency-weighted performance scores across benchmark data, the method grounds evaluation in the model's internal representations and enables comparison across benchmarks. As examples demonstrating our approach, we applied the method to two popular open-source models and ten benchmarks. We found that these models consistently underperformed on concepts that stand in contrast to sycophantic behaviors (e.g., politely refusing a request or asserting boundaries) and concepts connected to safety discussions. These model gaps align with observations previously surfaced in the literature; our automated, unsupervised method was able to recover them without manual supervision. We also observed benchmark gaps: many of the evaluated benchmarks over-represented concepts related to obedience, authority, or instruction-following, while missing core concepts that should fall within their intended scope. In sum, our method offers a representation-grounded approach to evaluation, enabling concept-level decomposition of benchmark scores. Rather than replacing conventional aggregated metrics, CG complements them by providing a concept-level decomposition that can reveal why a model scored as it did and how benchmarks could evolve to better reflect their intended scope. Code is available at https://competency-gaps.github.io.

</details>


### [2] [SA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention](https://arxiv.org/abs/2512.20724)
*Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CL

> 介绍SA-DiffuSeq框架，通过集成稀疏注意力机制来解决长文本生成中的计算成本和内存开销问题，提升长期文档处理的效率和质量。

<details>
  <summary>Details</summary>

**Motivation:** 解决基于扩散模型的长文本生成中存在的计算和内存问题，提高对于长文档的处理能力。

**Method:** 引入SA-DiffuSeq框架，利用稀疏注意力机制减少计算复杂度，维持语义连贯性和生成质量，使用一个适用于稀疏注意力动力学的软吸收状态来稳定扩散路径并加快序列重建。

**Result:** 实验表明，SA-DiffuSeq在训练效率和采样速度方面超过了现有基准，尤其在处理长序列时性能更强。

**Conclusion:** 研究结果表明，将结构化稀疏性整合到扩散模型中是实现高效长文本生成的一种有前景的方向。

**Abstract:** Diffusion based approaches to long form text generation suffer from prohibitive computational cost and memory overhead as sequence length increases. We introduce SA-DiffuSeq, a diffusion framework that integrates sparse attention to fundamentally improve scalability for long document modeling. By selectively allocating attention within the diffusion process, SA-DiffuSeq significantly reduces computational complexity while maintaining semantic coherence and generation quality. A key component of our method is a soft absorbing state tailored to sparse attention dynamics, which stabilizes diffusion trajectories and accelerates sequence reconstruction. This design improves sampling efficiency and enhances precision in long range dependency modeling. Extensive experiments demonstrate that SA-DiffuSeq consistently surpasses state of the art diffusion baselines in both training efficiency and sampling speed, with especially strong gains on extended sequences. These properties make SA-DiffuSeq well suited for demanding long form applications such as scientific writing, large scale code generation, and multi turn long context dialogue. Overall, our results indicate that incorporating structured sparsity into diffusion models is a promising direction for efficient and expressive long text generation.

</details>


### [3] [TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior](https://arxiv.org/abs/2512.20757)
*Gül Sena Altıntaş,Malikeh Ehghaghi,Brian Lester,Fengyuan Liu,Wanru Zhao,Marco Ciccone,Colin Raffel*

Main category: cs.CL

> 研究提出TokSuite，通过控制变量的方法来研究分词器对语言模型的影响，揭示了不同分词器的性能差异。

<details>
  <summary>Details</summary>

**Motivation:** 解决分词器对语言模型性能和行为的重要影响难以量化的问题，提供一个专门用于研究分词器影响的研究工具和基准测试。

**Method:** 通过训练使用不同分词器但其他条件相同的14个模型，以及创建一个新的基准测试来衡量模型面对真实世界扰动时的表现，以研究分词器对语言模型的影响。

**Result:** 提出了TokSuite，这是一个包含模型集合和基准测试的工具，能够有效地分离和分析不同分词器对语言模型的影响。

**Conclusion:** TokSuite支持一系列新颖的发现，阐明了各种流行的分词器各自的优缺点。

**Abstract:** Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers.

</details>


### [4] [Adversarial Training for Failure-Sensitive User Simulation in Mental Health Dialogue Optimization](https://arxiv.org/abs/2512.20773)
*Ziyi Zhu,Olivier Tieleman,Caitlin A. Stamatis,Luka Smyth,Thomas D. Hull,Daniel R. Cahn,Matteo Malgaroli*

Main category: cs.CL

> 研究提出了一种对抗性框架来改善用户模拟器的真实性，以更好地暴露心理健康支持聊天机器人中的失败模式。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于在培训和评估面向任务的对话系统（TOD）时，生成能够准确复制人类行为的模拟器是一个挑战，我们开发了一种对抗性训练框架来解决这个问题。

**Method:** 我们提出了一种对抗性训练框架，通过一个生成器（用户模拟器）和一个判别器之间的竞争动态，迭代地改进用户模拟器的真实感。

**Result:** 我们的方法表明，经过微调的模拟器在揭示系统问题方面显着优于零样本基线模型，对抗性训练进一步增强了多样性、分布一致性以及预测有效性。

**Conclusion:** 这些结果显示了在心理健康支持领域，通过对抗性训练创建逼真的用户模拟器是一个有前景的方法，可以实现在部署前快速、可靠且成本效益高的系统评估。

**Abstract:** Realistic user simulation is crucial for training and evaluating task-oriented dialogue (TOD) systems, yet creating simulators that accurately replicate human behavior remains challenging. A key property of effective simulators is their ability to expose failure modes of the systems they evaluate. We present an adversarial training framework that iteratively improves user simulator realism through a competitive dynamic between a generator (user simulator) and a discriminator. Applied to mental health support chatbots, our approach demonstrates that fine-tuned simulators dramatically outperform zero-shot base models at surfacing system issues, and adversarial training further enhances diversity, distributional alignment, and predictive validity. The resulting simulator achieves a strong correlation between simulated and real failure occurrence rates across diverse chatbot configurations while maintaining low distributional divergence of failure modes. Discriminator accuracy decreases drastically after three adversarial iterations, suggesting improved realism. These results provide evidence that adversarial training is a promising approach for creating realistic user simulators in mental health support TOD domains, enabling rapid, reliable, and cost-effective system evaluation before deployment.

</details>


### [5] [Large Language Models Approach Expert Pedagogical Quality in Math Tutoring but Differ in Instructional and Linguistic Profiles](https://arxiv.org/abs/2512.20780)
*Ramatu Oiza Abdulsalam,Segun Aroyehun*

Main category: cs.CL

> 研究比较了专家人类导师、新手人类导师和大型语言模型的教学策略和语言特性，发现大型语言模型在教学质量上与专家相似，但在策略和语言特征上存在差异。

<details>
  <summary>Details</summary>

**Motivation:** 探讨大型语言模型生成数学辅导回应的教学行为与专家人类实践的匹配程度。

**Method:** 采用了一种受控、逐轮次的比较方法，其中专家人类导师、新手人类导师和多个大型语言模型对同一组数学补救对话轮次作出回应。研究者们分析了教学策略和语言特性的特点，包括重新陈述和重构、追求准确度、词汇多样性、可读性、礼貌和代理。

**Result:** 大型语言模型在感知教学质量方面达到了专家水平，但在教学和语言特征上与专家人类导师存在系统性差异。具体而言，大型语言模型倾向于较少使用专家人类导师的特点性重新陈述和重构策略，同时产生更长、词汇更多样、更礼貌的回复。统计分析显示，重新陈述和重构、词汇多样性以及追求准确度与感知教学质量呈正相关，而更具代理性和礼貌的语言则与感知教学质量呈负相关。

**Conclusion:** 最近的大型语言模型表现出的教学质量与专家人类导师相当，但依赖的教学和语言策略不同。这些发现强调了当评估人类导师和智能辅导系统之间的辅导回应时，分析教学策略和语言特性的价值。

**Abstract:** Recent work has explored the use of large language models for generating tutoring responses in mathematics, yet it remains unclear how closely their instructional behavior aligns with expert human practice. We examine this question using a controlled, turn-level comparison in which expert human tutors, novice human tutors, and multiple large language models respond to the same set of math remediation conversation turns. We examine both instructional strategies and linguistic characteristics of tutoring responses, including restating and revoicing, pressing for accuracy, lexical diversity, readability, politeness, and agency. We find that large language models approach expert levels of perceived pedagogical quality on average but exhibit systematic differences in their instructional and linguistic profiles. In particular, large language models tend to underuse restating and revoicing strategies characteristic of expert human tutors, while producing longer, more lexically diverse, and more polite responses. Statistical analyses show that restating and revoicing, lexical diversity, and pressing for accuracy are positively associated with perceived pedagogical quality, whereas higher levels of agentic and polite language are negatively associated. Overall, recent large language models exhibit levels of perceived pedagogical quality comparable to expert human tutors, while relying on different instructional and linguistic strategies. These findings underscore the value of analyzing instructional strategies and linguistic characteristics when evaluating tutoring responses across human tutors and intelligent tutoring systems.

</details>


### [6] [Investigating Model Editing for Unlearning in Large Language Models](https://arxiv.org/abs/2512.20794)
*Shariqah Hossain,Lalana Kagal*

Main category: cs.CL

> 论文探讨了使用模型编辑算法实现机器遗忘的方法，显示了在特定情况下，这些方法可能会更有效。

<details>
  <summary>Details</summary>

**Motivation:** 机器遗忘旨在从模型中移除不必要的信息，但许多方法对于含有大量参数的大型语言模型效率低或无法完全移除所需信息而不损害保留的知识。

**Method:** 该论文探索了模型编辑算法ROME、IKE和WISE，并为遗忘设置设计了新的编辑目标。

**Result:** 研究显示，在某些情况下，模型编辑方法在遗忘的质量方面可以超越传统的遗忘方法。

**Conclusion:** 与传统遗忘技术一样，它们在不损害整个模型性能的情况下，难以封装需要遗忘的内容范围。

**Abstract:** Machine unlearning aims to remove unwanted information from a model, but many methods are inefficient for LLMs with large numbers of parameters or fail to fully remove the intended information without degrading performance on knowledge that should be retained. Model editing algorithms solve a similar problem of changing information in models, but they focus on redirecting inputs to a new target rather than removing that information altogether. In this work, we explore the editing algorithms ROME, IKE, and WISE and design new editing targets for an unlearning setting. Through this investigation, we show that model editing approaches can exceed baseline unlearning methods in terms of quality of forgetting depending on the setting. Like traditional unlearning techniques, they struggle to encapsulate the scope of what is to be unlearned without damage to the overall model performance.

</details>


### [7] [Measuring Mechanistic Independence: Can Bias Be Removed Without Erasing Demographics?](https://arxiv.org/abs/2512.20796)
*Zhengyang Shan,Aaron Mueller*

Main category: cs.CL

> 研究调查了独立的人口统计学偏见机制与语言模型中一般人口统计学识别的关系，通过干预任务特定机制，可以在不损害模型核心能力的同时减少偏见。

<details>
  <summary>Details</summary>

**Motivation:** 研究独立的人口统计学偏见机制与语言模型中的一般人口统计学识别之间的关系。

**Method:** 通过一个多任务评估设置，我们将人口统计学特征与姓名、职业和教育水平相关联，以此来测量模型在去除偏见的同时是否能保留人口统计学检测能力。我们比较了基于归因和基于相关性的方法来定位偏见特征。

**Result:** 在Gemma-2-9B中，使用有目标的稀疏自动编码器特征消融可以减少偏见而不会降低识别性能，基于归因的消融可以缓解种族和性别职业偏见，同时保持姓名识别准确性，而基于相关性的消融对教育偏见更为有效。

**Conclusion:** 总体结果表明，人口统计学偏见源自任务特定机制，而非绝对的人口统计学标记，机制性的推理时间干预可以实现精准去偏见而不损害模型核心能力。

**Abstract:** We investigate how independent demographic bias mechanisms are from general demographic recognition in language models. Using a multi-task evaluation setup where demographics are associated with names, professions, and education levels, we measure whether models can be debiased while preserving demographic detection capabilities. We compare attribution-based and correlation-based methods for locating bias features. We find that targeted sparse autoencoder feature ablations in Gemma-2-9B reduce bias without degrading recognition performance: attribution-based ablations mitigate race and gender profession stereotypes while preserving name recognition accuracy, whereas correlation-based ablations are more effective for education bias. Qualitative analysis further reveals that removing attribution features in education tasks induces ``prior collapse'', thus increasing overall bias. This highlights the need for dimension-specific interventions. Overall, our results show that demographic bias arises from task-specific mechanisms rather than absolute demographic markers, and that mechanistic inference-time interventions can enable surgical debiasing without compromising core model capabilities.

</details>


### [8] [Semantic Deception: When Reasoning Models Can't Compute an Addition](https://arxiv.org/abs/2512.20812)
*Nathaniël de Leeuw,Marceau Nahon,Mathis Reymond,Raja Chatila,Mehdi Khamassi*

Main category: cs.CL

> 研究揭示了LLMs在符号推理任务上的局限性，表明它们过依赖于表面语义，这引发了在决策情境中不可靠的推理能力的伦理和社会担忧。

<details>
  <summary>Details</summary>

**Motivation:** 为了调查LLMs在处理陌生符号时的推理能力，特别是它们是否能够维持符号抽象或是依赖学到的语义关联。

**Method:** 通过引入一个实验框架来测试LLMs处理和操控陌生符号的能力，重新定义标准数字和数学运算符使用新的符号，并要求LLMs解决以这种改变后的符号表示的简单计算任务。这个框架用于探究LLMs在存在误导性语义关联的情况下的表现，这些语义关联来自于符号的形式或特定的上下文。

**Result:** 通过四个不同LLMs的实验结果显示，语义线索极大影响了LLMs在符号推理任务上的表现，撕褪了模型仅依赖于表面语义的倾向，即使在看似正确执行指令时语义线索也能干扰其基本能力。

**Conclusion:** 实验表明，在简单任务中，语义线索可以显著降低LLMs的推理性能。这揭示了当前LLMs在符号操作能力上的局限性，并强调了它们倾向于过度依赖表面语义，即使在遵循指令的情况下，语义线索依然影响基本能力。这表明LLMs可能在重要决策情境中失败，这些情境要求稳健的符号推理而不应该被模型训练中继承的语义关联所影响。

**Abstract:** Large language models (LLMs) are increasingly used in situations where human values are at stake, such as decision-making tasks that involve reasoning when performed by humans. We investigate the so-called reasoning capabilities of LLMs over novel symbolic representations by introducing an experimental framework that tests their ability to process and manipulate unfamiliar symbols. We introduce semantic deceptions: situations in which symbols carry misleading semantic associations due to their form, such as being embedded in specific contexts, designed to probe whether LLMs can maintain symbolic abstraction or whether they default to exploiting learned semantic associations. We redefine standard digits and mathematical operators using novel symbols, and task LLMs with solving simple calculations expressed in this altered notation. The objective is: (1) to assess LLMs' capacity for abstraction and manipulation of arbitrary symbol systems; (2) to evaluate their ability to resist misleading semantic cues that conflict with the task's symbolic logic. Through experiments with four LLMs we show that semantic cues can significantly deteriorate reasoning models' performance on very simple tasks. They reveal limitations in current LLMs' ability for symbolic manipulations and highlight a tendency to over-rely on surface-level semantics, suggesting that chain-of-thoughts may amplify reliance on statistical correlations. Even in situations where LLMs seem to correctly follow instructions, semantic cues still impact basic capabilities. These limitations raise ethical and societal concerns, undermining the widespread and pernicious tendency to attribute reasoning abilities to LLMs and suggesting how LLMs might fail, in particular in decision-making contexts where robust symbolic reasoning is essential and should not be compromised by residual semantic associations inherited from the model's training.

</details>


### [9] [EssayCBM: Rubric-Aligned Concept Bottleneck Models for Transparent Essay Grading](https://arxiv.org/abs/2512.20817)
*Kumar Satvik Chaudhary,Chengshuai Zhao,Fan Zhang,Yung Hin Tse,Garima Agrawal,Yuli Deng,Huan Liu*

Main category: cs.CL

> EssayCBM是一种注重可解释性的自动作文评估框架，通过专门的预测头评估八个写作概念的分数，形成透明的瓶颈，并使用轻量级网络计算最终分数，提供了可操作的概念级反馈。

<details>
  <summary>Details</summary>

**Motivation:** 自动化评分系统的评估过程对教育者和学生来说仍然是一个挑战，尤其是在大型语言模型作为黑箱的情况下。

**Method:** EssayCBM不直接从文本预测成绩，而是通过编码器上的专门预测头评估八个写作概念，如论点清晰度和证据使用。这些概念分数形成透明的瓶颈，轻量级网络仅使用这些概念计算最终评分。

**Result:** EssayCBM在性能上与黑盒模型相当，同时通过直观的网络界面提供可操作的概念级反馈。

**Conclusion:** 该框架允许教师调整概念预测并即时查看更新的成绩，支持问责制的人工干预评估过程。

**Abstract:** Understanding how automated grading systems evaluate essays remains a significant challenge for educators and students, especially when large language models function as black boxes. We introduce EssayCBM, a rubric-aligned framework that prioritizes interpretability in essay assessment. Instead of predicting grades directly from text, EssayCBM evaluates eight writing concepts, such as Thesis Clarity and Evidence Use, through dedicated prediction heads on an encoder. These concept scores form a transparent bottleneck, and a lightweight network computes the final grade using only concepts. Instructors can adjust concept predictions and instantly view the updated grade, enabling accountable human-in-the-loop evaluation. EssayCBM matches black-box performance while offering actionable, concept-level feedback through an intuitive web interface.

</details>


### [10] [MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs](https://arxiv.org/abs/2512.20822)
*Zhan Qu,Michael Färber*

Main category: cs.CL

> 该研究提出了MediEval框架和CoRFu方法，有效提高了大型语言模型在医疗领域的可靠性和安全性，特别是在评估和优化知识合理性和上下文一致性方面。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLMs）逐渐应用于医学领域，但其应用受到可靠性和安全性问题的限制。现有的评估方法要么单独测试纯粹的医学知识，要么评估患者层面的推理而不验证正确性，这留下了一个关键的空白。

**Method:** 该论文引入了MediEval基准测试，结合MIMIC-IV电子健康记录与UMLS等生物医学词汇表构建的统一知识库，生成真实患者情境下的医疗事实性和反事实性语句，以系统评估语言模型在医疗知识准确性和上下文一致性方面的表现。

**Result:** 研究者使用该框架识别了当前专有、开源和领域特定的LLMs中存在的关键失败模式，包括虚假支持和真理反转，并提出Counterfactual Risk-Aware Fine-tuning (CoRFu) 方法来减轻这些风险，实现了准确性和安全性上的提高。

**Conclusion:** 研究表明，通过MediEval框架和CoRFu方法，可以显著提升LLMs在医疗领域的准确性和安全性。

**Abstract:** Large Language Models (LLMs) are increasingly applied to medicine, yet their adoption is limited by concerns over reliability and safety. Existing evaluations either test factual medical knowledge in isolation or assess patient-level reasoning without verifying correctness, leaving a critical gap. We introduce MediEval, a benchmark that links MIMIC-IV electronic health records (EHRs) to a unified knowledge base built from UMLS and other biomedical vocabularies. MediEval generates diverse factual and counterfactual medical statements within real patient contexts, enabling systematic evaluation across a 4-quadrant framework that jointly considers knowledge grounding and contextual consistency. Using this framework, we identify critical failure modes, including hallucinated support and truth inversion, that current proprietary, open-source, and domain-specific LLMs frequently exhibit. To address these risks, we propose Counterfactual Risk-Aware Fine-tuning (CoRFu), a DPO-based method with an asymmetric penalty targeting unsafe confusions. CoRFu improves by +16.4 macro-F1 points over the base model and eliminates truth inversion errors, demonstrating both higher accuracy and substantially greater safety.

</details>


### [11] [Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning](https://arxiv.org/abs/2512.20848)
*NVIDIA,:,Aaron Blakeman,Aaron Grattafiori,Aarti Basant,Abhibha Gupta,Abhinav Khattar,Adi Renduchintala,Aditya Vavre,Akanksha Shukla,Akhiad Bercovich,Aleksander Ficek,Aleksandr Shaposhnikov,Alex Kondratenko,Alexander Bukharin,Alexandre Milesi,Ali Taghibakhshi,Alisa Liu,Amelia Barton,Ameya Sunil Mahabaleshwarkar,Amir Klein,Amit Zuker,Amnon Geifman,Amy Shen,Anahita Bhiwandiwalla,Andrew Tao,Ann Guan,Anubhav Mandarwal,Arham Mehta,Ashwath Aithal,Ashwin Poojary,Asif Ahamed,Asma Kuriparambil Thekkumpate,Ayush Dattagupta,Banghua Zhu,Bardiya Sadeghi,Barnaby Simkin,Ben Lanir,Benedikt Schifferer,Besmira Nushi,Bilal Kartal,Bita Darvish Rouhani,Boris Ginsburg,Brandon Norick,Brandon Soubasis,Branislav Kisacanin,Brian Yu,Bryan Catanzaro,Carlo del Mundo,Chantal Hwang,Charles Wang,Cheng-Ping Hsieh,Chenghao Zhang,Chenhan Yu,Chetan Mungekar,Chintan Patel,Chris Alexiuk,Christopher Parisien,Collin Neale,Damon Mosk-Aoyama,Dan Su,Dane Corneil,Daniel Afrimi,Daniel Rohrer,Daniel Serebrenik,Daria Gitman,Daria Levy,Darko Stosic,David Mosallanezhad,Deepak Narayanan,Dhruv Nathawani,Dima Rekesh,Dina Yared,Divyanshu Kakwani,Dong Ahn,Duncan Riach,Dusan Stosic,Edgar Minasyan,Edward Lin,Eileen Long,Eileen Peters Long,Elena Lantz,Ellie Evans,Elliott Ning,Eric Chung,Eric Harper,Eric Tramel,Erick Galinkin,Erik Pounds,Evan Briones,Evelina Bakhturina,Faisal Ladhak,Fay Wang,Fei Jia,Felipe Soares,Feng Chen,Ferenc Galko,Frankie Siino,Gal Hubara Agam,Ganesh Ajjanagadde,Gantavya Bhatt,Gargi Prasad,George Armstrong,Gerald Shen,Gorkem Batmaz,Grigor Nalbandyan,Haifeng Qian,Harsh Sharma,Hayley Ross,Helen Ngo,Herman Sahota,Hexin Wang,Himanshu Soni,Hiren Upadhyay,Huizi Mao,Huy C Nguyen,Huy Q Nguyen,Iain Cunningham,Ido Shahaf,Igor Gitman,Ilya Loshchilov,Ivan Moshkov,Izzy Putterman,Jan Kautz,Jane Polak Scowcroft,Jared Casper,Jatin Mitra,Jeffrey Glick,Jenny Chen,Jesse Oliver,Jian Zhang,Jiaqi Zeng,Jie Lou,Jimmy Zhang,Jining Huang,Joey Conway,Joey Guman,John Kamalu,Johnny Greco,Jonathan Cohen,Joseph Jennings,Joyjit Daw,Julien Veron Vialard,Junkeun Yi,Jupinder Parmar,Kai Xu,Kan Zhu,Kari Briski,Katherine Cheung,Katherine Luna,Keshav Santhanam,Kevin Shih,Kezhi Kong,Khushi Bhardwaj,Krishna C. Puvvada,Krzysztof Pawelec,Kumar Anik,Lawrence McAfee,Laya Sleiman,Leon Derczynski,Li Ding,Lucas Liebenwein,Luis Vega,Maanu Grover,Maarten Van Segbroeck,Maer Rodrigues de Melo,Makesh Narsimhan Sreedhar,Manoj Kilaru,Maor Ashkenazi,Marc Romeijn,Mark Cai,Markus Kliegl,Maryam Moosaei,Matvei Novikov,Mehrzad Samadi,Melissa Corpuz,Mengru Wang,Meredith Price,Michael Boone,Michael Evans,Miguel Martinez,Mike Chrzanowski,Mohammad Shoeybi,Mostofa Patwary,Nabin Mulepati,Natalie Hereth,Nave Assaf,Negar Habibi,Neta Zmora,Netanel Haber,Nicola Sessions,Nidhi Bhatia,Nikhil Jukar,Nikki Pope,Nikolai Ludwig,Nima Tajbakhsh,Nirmal Juluru,Oleksii Hrinchuk,Oleksii Kuchaiev,Olivier Delalleau,Oluwatobi Olabiyi,Omer Ullman Argov,Ouye Xie,Parth Chadha,Pasha Shamis,Pavlo Molchanov,Pawel Morkisz,Peter Dykas,Peter Jin,Pinky Xu,Piotr Januszewski,Pranav Prashant Thombre,Prasoon Varshney,Pritam Gundecha,Qing Miao,Rabeeh Karimi Mahabadi,Ran El-Yaniv,Ran Zilberstein,Rasoul Shafipour,Rich Harang,Rick Izzo,Rima Shahbazyan,Rishabh Garg,Ritika Borkar,Ritu Gala,Riyad Islam,Roger Waleffe,Rohit Watve,Roi Koren,Ruoxi Zhang,Russell J. Hewett,Ryan Prenger,Ryan Timbrook,Sadegh Mahdavi,Sahil Modi,Samuel Kriman,Sanjay Kariyappa,Sanjeev Satheesh,Saori Kaji,Satish Pasumarthi,Sean Narentharen,Sean Narenthiran,Seonmyeong Bak,Sergey Kashirsky,Seth Poulos,Shahar Mor,Shanmugam Ramasamy,Shantanu Acharya,Shaona Ghosh,Sharath Turuvekere Sreenivas,Shelby Thomas,Shiqing Fan,Shreya Gopal,Shrimai Prabhumoye,Shubham Pachori,Shubham Toshniwal,Shuoyang Ding,Siddharth Singh,Simeng Sun,Smita Ithape,Somshubra Majumdar,Soumye Singhal,Stefania Alborghetti,Stephen Ge,Sugam Dipak Devare,Sumeet Kumar Barua,Suseella Panguluri,Suyog Gupta,Sweta Priyadarshi,Syeda Nahida Akter,Tan Bui,Teodor-Dumitru Ene,Terry Kong,Thanh Do,Tijmen Blankevoort,Tom Balough,Tomer Asida,Tomer Bar Natan,Tugrul Konuk,Twinkle Vashishth,Udi Karpas,Ushnish De,Vahid Noorozi,Vahid Noroozi,Venkat Srinivasan,Venmugil Elango,Vijay Korthikanti,Vitaly Kurin,Vitaly Lavrukhin,Wanli Jiang,Wasi Uddin Ahmad,Wei Du,Wei Ping,Wenfei Zhou,Will Jennings,William Zhang,Wojciech Prazuch,Xiaowei Ren,Yashaswi Karnati,Yejin Choi,Yev Meyer,Yi-Fu Wu,Yian Zhang,Ying Lin,Yonatan Geifman,Yonggan Fu,Yoshi Subara,Yoshi Suhara,Yubo Gao,Zach Moshe,Zhen Dong,Zihan Liu,Zijia Chen,Zijie Yan*

Main category: cs.CL

> Nemotron 3 Nano 30B-A3B是一个混合的Mamba-Transformer语言模型，其在提高准确性的同时，推理吞吐量较之前提升了3.3倍。

<details>
  <summary>Details</summary>

**Motivation:** 为了提供一个在推理吞吐量和准确性上优于现有模型的新型语言模型。

**Method:** 提出了一种混合的Mixture-of-Experts Mamba-Transformer语言模型Nemotron 3 Nano 30B-A3B。该模型经过25万亿个文本标记的预训练，其中包括相对于Nemotron 2新增的3万亿个独特标记。预训练后，还进行了监督微调和大规模的RL训练。

**Result:** Nemotron 3 Nano 模型的准确性高于Nemotron 2 Nano，并且在推理过程中的参数激活量减少了一半。该模型在流行的基准测试中比其他类似规模的开源模型（如GPT-OSS-20B和Qwen3-30B-A3B-Thinking-2507）具有更高的准确性，推理吞吐量提升了3.3倍。

**Conclusion:** Nemotron 3 Nano 30B-A3B在保持较高准确性的同时，显著提升了推理吞吐量，具有更强的代理、推理和对话能力，并支持长达1M标记的上下文长度。

**Abstract:** We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.

</details>


### [12] [How important is Recall for Measuring Retrieval Quality?](https://arxiv.org/abs/2512.20854)
*Shelly Schwartz,Oleg Vasilyev,Randy Sawaya*

Main category: cs.CL

> 论文提出了一种新的方法来评估几种检索策略，即使在不知道所有相关文档的情况下也能有效进行。

<details>
  <summary>Details</summary>

**Motivation:** 在真实检索设置中，大型和不断发展的知识库的总相关文档数量通常是未知的，因此无法计算召回率。

**Method:** 本研究通过测量检索质量指标与LLM生成的响应质量之间的相关性来评估几种已建立的策略。实验在具有相对较少相关文档（2-15个）的多个数据集上进行。

**Result:** 研究提出了一种简单的检索质量度量方法，该方法在不知道相关文档总数的情况下表现良好。

**Conclusion:** 研究表明，在不了解相关文档总数的情况下，使用提出的简单检索质量度量方法能够有效地评估检索策略。

**Abstract:** In realistic retrieval settings with large and evolving knowledge bases, the total number of documents relevant to a query is typically unknown, and recall cannot be computed. In this paper, we evaluate several established strategies for handling this limitation by measuring the correlation between retrieval quality metrics and LLM-based judgments of response quality, where responses are generated from the retrieved documents. We conduct experiments across multiple datasets with a relatively low number of relevant documents (2-15). We also introduce a simple retrieval quality measure that performs well without requiring knowledge of the total number of relevant documents.

</details>


### [13] [NVIDIA Nemotron 3: Efficient and Open Intelligence](https://arxiv.org/abs/2512.20856)
*NVIDIA,:,Aaron Blakeman,Aaron Grattafiori,Aarti Basant,Abhibha Gupta,Abhinav Khattar,Adi Renduchintala,Aditya Vavre,Akanksha Shukla,Akhiad Bercovich,Aleksander Ficek,Aleksandr Shaposhnikov,Alex Kondratenko,Alexander Bukharin,Alexandre Milesi,Ali Taghibakhshi,Alisa Liu,Amelia Barton,Ameya Sunil Mahabaleshwarkar,Amir Klein,Amit Zuker,Amnon Geifman,Amy Shen,Anahita Bhiwandiwalla,Andrew Tao,Anjulie Agrusa,Ankur Verma,Ann Guan,Anubhav Mandarwal,Arham Mehta,Ashwath Aithal,Ashwin Poojary,Asif Ahamed,Asit Mishra,Asma Kuriparambil Thekkumpate,Ayush Dattagupta,Banghua Zhu,Bardiya Sadeghi,Barnaby Simkin,Ben Lanir,Benedikt Schifferer,Besmira Nushi,Bilal Kartal,Bita Darvish Rouhani,Boris Ginsburg,Brandon Norick,Brandon Soubasis,Branislav Kisacanin,Brian Yu,Bryan Catanzaro,Carlo del Mundo,Chantal Hwang,Charles Wang,Cheng-Ping Hsieh,Chenghao Zhang,Chenhan Yu,Chetan Mungekar,Chintan Patel,Chris Alexiuk,Christopher Parisien,Collin Neale,Cyril Meurillon,Damon Mosk-Aoyama,Dan Su,Dane Corneil,Daniel Afrimi,Daniel Lo,Daniel Rohrer,Daniel Serebrenik,Daria Gitman,Daria Levy,Darko Stosic,David Mosallanezhad,Deepak Narayanan,Dhruv Nathawani,Dima Rekesh,Dina Yared,Divyanshu Kakwani,Dong Ahn,Duncan Riach,Dusan Stosic,Edgar Minasyan,Edward Lin,Eileen Long,Eileen Peters Long,Elad Segal,Elena Lantz,Ellie Evans,Elliott Ning,Eric Chung,Eric Harper,Eric Tramel,Erick Galinkin,Erik Pounds,Evan Briones,Evelina Bakhturina,Evgeny Tsykunov,Faisal Ladhak,Fay Wang,Fei Jia,Felipe Soares,Feng Chen,Ferenc Galko,Frank Sun,Frankie Siino,Gal Hubara Agam,Ganesh Ajjanagadde,Gantavya Bhatt,Gargi Prasad,George Armstrong,Gerald Shen,Gorkem Batmaz,Grigor Nalbandyan,Haifeng Qian,Harsh Sharma,Hayley Ross,Helen Ngo,Herbert Hum,Herman Sahota,Hexin Wang,Himanshu Soni,Hiren Upadhyay,Huizi Mao,Huy C Nguyen,Huy Q Nguyen,Iain Cunningham,Ido Galil,Ido Shahaf,Igor Gitman,Ilya Loshchilov,Itamar Schen,Itay Levy,Ivan Moshkov,Izik Golan,Izzy Putterman,Jan Kautz,Jane Polak Scowcroft,Jared Casper,Jatin Mitra,Jeffrey Glick,Jenny Chen,Jesse Oliver,Jian Zhang,Jiaqi Zeng,Jie Lou,Jimmy Zhang,Jinhang Choi,Jining Huang,Joey Conway,Joey Guman,John Kamalu,Johnny Greco,Jonathan Cohen,Joseph Jennings,Joyjit Daw,Julien Veron Vialard,Junkeun Yi,Jupinder Parmar,Kai Xu,Kan Zhu,Kari Briski,Katherine Cheung,Katherine Luna,Keith Wyss,Keshav Santhanam,Kevin Shih,Kezhi Kong,Khushi Bhardwaj,Kirthi Shankar,Krishna C. Puvvada,Krzysztof Pawelec,Kumar Anik,Lawrence McAfee,Laya Sleiman,Leon Derczynski,Li Ding,Lizzie Wei,Lucas Liebenwein,Luis Vega,Maanu Grover,Maarten Van Segbroeck,Maer Rodrigues de Melo,Mahdi Nazemi,Makesh Narsimhan Sreedhar,Manoj Kilaru,Maor Ashkenazi,Marc Romeijn,Marcin Chochowski,Mark Cai,Markus Kliegl,Maryam Moosaei,Matt Kulka,Matvei Novikov,Mehrzad Samadi,Melissa Corpuz,Mengru Wang,Meredith Price,Michael Andersch,Michael Boone,Michael Evans,Miguel Martinez,Mikail Khona,Mike Chrzanowski,Minseok Lee,Mohammad Dabbah,Mohammad Shoeybi,Mostofa Patwary,Nabin Mulepati,Najeeb Nabwani,Natalie Hereth,Nave Assaf,Negar Habibi,Neta Zmora,Netanel Haber,Nicola Sessions,Nidhi Bhatia,Nikhil Jukar,Nikki Pope,Nikolai Ludwig,Nima Tajbakhsh,Nir Ailon,Nirmal Juluru,Nishant Sharma,Oleksii Hrinchuk,Oleksii Kuchaiev,Olivier Delalleau,Oluwatobi Olabiyi,Omer Ullman Argov,Omri Puny,Oren Tropp,Ouye Xie,Parth Chadha,Pasha Shamis,Paul Gibbons,Pavlo Molchanov,Pawel Morkisz,Peter Dykas,Peter Jin,Pinky Xu,Piotr Januszewski,Pranav Prashant Thombre,Prasoon Varshney,Pritam Gundecha,Przemek Tredak,Qing Miao,Qiyu Wan,Rabeeh Karimi Mahabadi,Rachit Garg,Ran El-Yaniv,Ran Zilberstein,Rasoul Shafipour,Rich Harang,Rick Izzo,Rima Shahbazyan,Rishabh Garg,Ritika Borkar,Ritu Gala,Riyad Islam,Robert Hesse,Roger Waleffe,Rohit Watve,Roi Koren,Ruoxi Zhang,Russell Hewett,Russell J. Hewett,Ryan Prenger,Ryan Timbrook,Sadegh Mahdavi,Sahil Modi,Samuel Kriman,Sangkug Lim,Sanjay Kariyappa,Sanjeev Satheesh,Saori Kaji,Satish Pasumarthi,Saurav Muralidharan,Sean Narentharen,Sean Narenthiran,Seonmyeong Bak,Sergey Kashirsky,Seth Poulos,Shahar Mor,Shanmugam Ramasamy,Shantanu Acharya,Shaona Ghosh,Sharath Turuvekere Sreenivas,Shelby Thomas,Shiqing Fan,Shreya Gopal,Shrimai Prabhumoye,Shubham Pachori,Shubham Toshniwal,Shuoyang Ding,Siddharth Singh,Simeng Sun,Smita Ithape,Somshubra Majumdar,Soumye Singhal,Stas Sergienko,Stefania Alborghetti,Stephen Ge,Sugam Dipak Devare,Sumeet Kumar Barua,Suseella Panguluri,Suyog Gupta,Sweta Priyadarshi,Syeda Nahida Akter,Tan Bui,Teodor-Dumitru Ene,Terry Kong,Thanh Do,Tijmen Blankevoort,Tim Moon,Tom Balough,Tomer Asida,Tomer Bar Natan,Tomer Ronen,Tugrul Konuk,Twinkle Vashishth,Udi Karpas,Ushnish De,Vahid Noorozi,Vahid Noroozi,Venkat Srinivasan,Venmugil Elango,Victor Cui,Vijay Korthikanti,Vinay Rao,Vitaly Kurin,Vitaly Lavrukhin,Vladimir Anisimov,Wanli Jiang,Wasi Uddin Ahmad,Wei Du,Wei Ping,Wenfei Zhou,Will Jennings,William Zhang,Wojciech Prazuch,Xiaowei Ren,Yashaswi Karnati,Yejin Choi,Yev Meyer,Yi-Fu Wu,Yian Zhang,Yigong Qin,Ying Lin,Yonatan Geifman,Yonggan Fu,Yoshi Subara,Yoshi Suhara,Yubo Gao,Zach Moshe,Zhen Dong,Zhongbo Zhu,Zihan Liu,Zijia Chen,Zijie Yan*

Main category: cs.CL

> Nemotron 3系列模型包括Nano、Super和Ultra三种，采用了创新的LatentMoE方法和混合架构，支持长上下文处理和高精度推理。即将公开模型权重及相关软件、数据等资源。

<details>
  <summary>Details</summary>

**Motivation:** 开发Nemotron 3系列，旨在提供强大的代理、推理和对话能力，兼顾高性能输出和长上下文处理。

**Method:** Nemotron 3系列模型采用混合Mamba-Transformer架构，包括Nano、Super和Ultra三种型号。Super和Ultra使用NVFP4训练，并创新性地采用LatentMoE方法以提高模型质量，并配备MTP层以加快文本生成速度。所有模型通过多环境强化学习后训练，支持推理、多步工具使用及细粒度推理预算控制。

**Result:** Nano型号在准确性上超越了同类模型，并且在推理方面保持了极高的成本效益。Super型号优化了协作代理和高负载工作负载，如IT票务自动化。Ultra型号则达到最先进的准确性和推理性能。

**Conclusion:** Nemotron 3系列模型展示了卓越的性能和灵活性，适用于从低成本高效推理到高精度和高性能推理的各种应用场景。

**Abstract:** We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.

</details>


### [14] [Architectural Trade-offs in Small Language Models Under Compute Constraints](https://arxiv.org/abs/2512.20877)
*Shivraj Singh Bhatti*

Main category: cs.CL

> 本文通过逐步添加神经网络组件来分析小型语言模型在计算资源约束下的性能。实验表明，注意力机制即使在小型模型中也优于MLP，但模型复杂度增加需配合适当优化。

<details>
  <summary>Details</summary>

**Motivation:** 研究在严格计算约束下的小型语言模型中的架构选择和训练预算如何相互作用以决定其性能。

**Method:** 我们从一个线性下一词预测模型开始，逐步引入非线性、自注意力机制和多层Transformer架构，并在Tiny Shakespeare字符级建模、Penn Treebank和WikiText-2的词级建模中评估每一阶段的表现。我们通过测试负对数似然值（NLL）、参数量和近似的训练FLOPs来比较模型，以衡量准确性与效率之间的权衡。

**Result:** 基于注意力的模型即使在小规模上也能在每FLOP的效率上超过MLP，而增加深度或上下文范围若没有足够的优化可能反而会损害性能。此外，我们在旋转位置嵌入（RoPE）上的进一步检查发现，大型语言模型中成功的架构技术并不一定适用于小型模型。

**Conclusion:** 我们的研究结果表明，即使是小规模模型，基于注意力的模型在每FLOP效率上也超过了MLP，显示出其在资源受限环境中的优势，但模型复杂度提高需伴随优化策略，否则可能适得其反。此外，像旋转位置嵌入这样的大型模型成功策略不一定适用于小型模型。

**Abstract:** We present a systematic empirical study of small language models under strict compute constraints, analyzing how architectural choices and training budget interact to determine performance. Starting from a linear next-token predictor, we progressively introduce nonlinearities, self-attention, and multi-layer transformer architectures, evaluating each on character-level modeling of Tiny Shakespeare and word-level modeling of Penn Treebank (PTB) and WikiText-2. We compare models using test negative log-likelihood (NLL), parameter count, and approximate training FLOPs to characterize accuracy-efficiency trade-offs. Our results show that attention-based models dominate MLPs in per-FLOP efficiency even at small scale, while increasing depth or context without sufficient optimization can degrade performance. We further examine rotary positional embeddings (RoPE), finding that architectural techniques successful in large language models do not necessarily transfer to small-model regimes.

</details>


### [15] [Where Did This Sentence Come From? Tracing Provenance in LLM Reasoning Distillation](https://arxiv.org/abs/2512.20908)
*Kaiyuan Liu,Shaotian Yan,Rui Miao,Bing Wang,Chen Shen,Jun Zhang,Jieping Ye*

Main category: cs.CL

> 本文提出了一种新的跨模型推理蒸馏源追踪框架，用于分析和改进学生模型在测试时的行为和泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 现有的推理蒸馏方法缺乏对蒸馏模型能力来源的详细分析，这款新的框架是为了分析学生模型在测试时是否能够保持与教师模型一致的行为，以及其泛化能力。

**Method:** 本文介绍了一种跨模型推理蒸馏源追踪框架，通过比较教师模型、原始学生模型和蒸馏模型在相同上下文中的预测概率来分类每个动作的来源，并基于此提出了一种受教师指导的数据选择方法。

**Result:** 通过对多个代表性教师模型和多种学生模型的验证，证明了该框架的有效性，突出了其在推理蒸馏中的潜力。

**Conclusion:** 本文证明了其源追踪框架的实用性和教师指导数据选择方法的有效性，为推理蒸馏的研究提供了新的见解和工具。

**Abstract:** Reasoning distillation has attracted increasing attention. It typically leverages a large teacher model to generate reasoning paths, which are then used to fine-tune a student model so that it mimics the teacher's behavior in training contexts. However, previous approaches have lacked a detailed analysis of the origins of the distilled model's capabilities. It remains unclear whether the student can maintain consistent behaviors with the teacher in novel test-time contexts, or whether it regresses to its original output patterns, raising concerns about the generalization of distillation models. To analyse this question, we introduce a cross-model Reasoning Distillation Provenance Tracing framework. For each action (e.g., a sentence) produced by the distilled model, we obtain the predictive probabilities assigned by the teacher, the original student, and the distilled model under the same context. By comparing these probabilities, we classify each action into different categories. By systematically disentangling the provenance of each action, we experimentally demonstrate that, in test-time contexts, the distilled model can indeed generate teacher-originated actions, which correlate with and plausibly explain observed performance on distilled model. Building on this analysis, we further propose a teacher-guided data selection method. Unlike prior approach that rely on heuristics, our method directly compares teacher-student divergences on the training data, providing a principled selection criterion. We validate the effectiveness of our approach across multiple representative teacher models and diverse student models. The results highlight the utility of our provenance-tracing framework and underscore its promise for reasoning distillation. We hope to share Reasoning Distillation Provenance Tracing and our insights into reasoning distillation with the community.

</details>


### [16] [Foundation Model-based Evaluation of Neuropsychiatric Disorders: A Lifespan-Inclusive, Multi-Modal, and Multi-Lingual Study](https://arxiv.org/abs/2512.20948)
*Zhongren Dong,Haotian Guo,Weixiang Xu,Huan Zhao,Zixing Zhang*

Main category: cs.CL

> FEND框架结合了多种语言的语音和文本数据，评估在检测神经精神疾病（如AD、抑郁和ASD）时的效果。结果显示，对AD和抑郁的检测效果优于对ASD的检测。此外，研究发现模态不平衡问题和跨语料库的一致性对模型性能有显著影响。

<details>
  <summary>Details</summary>

**Motivation:** 尽管多模态方法在神经精神疾病的诊断中展现出潜力，但仍面临着多语言泛化难题和缺乏统一评估框架的挑战。本研究旨在通过FEND框架解决这些问题，以便为多语言和多任务设置中的疾病评估提供更好的支持。

**Method:** 本研究提出了一种名为FEND（基于Foundation模型的神经精神疾病评估）的综合多模态框架，该框架结合了语音和文本模态，用于检测阿尔茨海默病（AD）、抑郁和自闭症谱系障碍（ASD），涵盖了整个人群的生命阶段。FEND利用13个多语言数据集，涵盖了英语、汉语、希腊语、法语和荷兰语，系统性地评估了多模态融合的性能表现。

**Result:** 研究结果显示，多模态方法在AD和抑郁检测中表现出色，但对于自闭症谱系障碍的检测表现较差，推测是由于数据集异质性的原因。另外，模态不平衡也是个显著问题，多模态融合未能超越最佳单模态模型。跨语料库实验表明，在任务和语言一致性场景中性能稳健，而在多语言和任务异质性设置中性能下降显著。

**Conclusion:** 通过提供广泛的基准和详细的性能影响因素分析，FEND前瞻性地推进了自动化、全生命周期覆盖和多语言神经精神疾病评估领域。研究呼吁其他研究者采用FEND框架进行公平的比较和可重复的研究工作。

**Abstract:** Neuropsychiatric disorders, such as Alzheimer's disease (AD), depression, and autism spectrum disorder (ASD), are characterized by linguistic and acoustic abnormalities, offering potential biomarkers for early detection. Despite the promise of multi-modal approaches, challenges like multi-lingual generalization and the absence of a unified evaluation framework persist. To address these gaps, we propose FEND (Foundation model-based Evaluation of Neuropsychiatric Disorders), a comprehensive multi-modal framework integrating speech and text modalities for detecting AD, depression, and ASD across the lifespan. Leveraging 13 multi-lingual datasets spanning English, Chinese, Greek, French, and Dutch, we systematically evaluate multi-modal fusion performance. Our results show that multi-modal fusion excels in AD and depression detection but underperforms in ASD due to dataset heterogeneity. We also identify modality imbalance as a prevalent issue, where multi-modal fusion fails to surpass the best mono-modal models. Cross-corpus experiments reveal robust performance in task- and language-consistent scenarios but noticeable degradation in multi-lingual and task-heterogeneous settings. By providing extensive benchmarks and a detailed analysis of performance-influencing factors, FEND advances the field of automated, lifespan-inclusive, and multi-lingual neuropsychiatric disorder assessment. We encourage researchers to adopt the FEND framework for fair comparisons and reproducible research.

</details>


### [17] [Neural Probe-Based Hallucination Detection for Large Language Models](https://arxiv.org/abs/2512.20949)
*Shize Liang,Hongzhi Wang*

Main category: cs.CL

> 提出了一种基于神经网络的token级别幻觉检测框架，使用轻量级MLP探针进行非线性建模，并展示了比现有方法更好的性能。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在文本生成和知识问答任务中表现出色，但其在高风险领域应用受限，因为它们易于生成幻觉内容。当前基于不确定性估计和外部知识检索的幻觉检测方法存在局限性，而本文旨在提高幻觉检测的准确性和效率。

**Method:** 通过冻结语言模型参数并使用轻量级MLP探针来执行高阶隐藏状态的非线性建模，从而构建了一个基于神经网络的token级别的幻觉检测框架。设计了多目标联合损失函数以增强检测稳定性和语义消歧能力。此外，建立了一个层位置-探针性能响应模型，使用贝叶斯优化自动搜索最优的探针插入层，以实现更好的训练结果。

**Result:** 实验结果表明，MLP探针在LongFact、HealthBench和TriviaQA数据集上显著优于最先进的方法，在准确性、召回率和低误报条件下的检测能力方面表现更好。

**Conclusion:** 本文提出的MLP探针方法在token级别的幻觉检测上具有显著优势，证明了非线性建模在提高幻觉内容检测性能的有效性。

**Abstract:** Large language models(LLMs) excel at text generation and knowledge question-answering tasks, but they are prone to generating hallucinated content, severely limiting their application in high-risk domains. Current hallucination detection methods based on uncertainty estimation and external knowledge retrieval suffer from the limitation that they still produce erroneous content at high confidence levels and rely heavily on retrieval efficiency and knowledge coverage. In contrast, probe methods that leverage the model's hidden-layer states offer real-time and lightweight advantages. However, traditional linear probes struggle to capture nonlinear structures in deep semantic spaces.To overcome these limitations, we propose a neural network-based framework for token-level hallucination detection. By freezing language model parameters, we employ lightweight MLP probes to perform nonlinear modeling of high-level hidden states. A multi-objective joint loss function is designed to enhance detection stability and semantic disambiguity. Additionally, we establish a layer position-probe performance response model, using Bayesian optimization to automatically search for optimal probe insertion layers and achieve superior training results.Experimental results on LongFact, HealthBench, and TriviaQA demonstrate that MLP probes significantly outperform state-of-the-art methods in accuracy, recall, and detection capability under low false-positive conditions.

</details>


### [18] [MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment](https://arxiv.org/abs/2512.20950)
*Mohammad Mahdi Abootorabi,Alireza Ghahramani Kure,Mohammadali Mohammadkhani,Sina Elahimanesh,Mohammad Ali Ali Panah*

Main category: cs.CL

> The paper presents TriAligner, a system for multilingual and crosslingual fact-checked claim retrieval, using a dual-encoder architecture with contrastive learning, which shows better performance in fact-checking than existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the growing issue of misinformation by developing a more effective system for fact-checking, particularly in a multilingual context.

**Method:** Our method, TriAligner, uses a dual-encoder architecture with contrastive learning and includes native and English translations across different modalities to retrieve claims in multiple languages.

**Result:** The system demonstrates significant improvements in retrieval accuracy and fact-checking performance over baseline methods on both monolingual and crosslingual benchmarks.

**Conclusion:** The conclusion is that TriAligner effectively improves the accuracy of claim retrieval across languages, which is crucial in combating misinformation.

**Abstract:** This paper presents our system for SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval. In an era where misinformation spreads rapidly, effective fact-checking is increasingly critical. We introduce TriAligner, a novel approach that leverages a dual-encoder architecture with contrastive learning and incorporates both native and English translations across different modalities. Our method effectively retrieves claims across multiple languages by learning the relative importance of different sources in alignment. To enhance robustness, we employ efficient data preprocessing and augmentation using large language models while incorporating hard negative sampling to improve representation learning. We evaluate our approach on monolingual and crosslingual benchmarks, demonstrating significant improvements in retrieval accuracy and fact-checking performance over baselines.

</details>


### [19] [Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models](https://arxiv.org/abs/2512.20954)
*Xiang Zhang,Jiaqi Wei,Yuejin Yang,Zijie Qiu,Yuhan Chen,Zhiqiang Gao,Muhammad Abdul-Mageed,Laks V. S. Lakshmanan,Wanli Ouyang,Chenyu You,Siqi Sun*

Main category: cs.CL

> 本文探讨了将链式思考（CoT）提示应用于蛋白质语言模型的局限性，并通过引入反射预训练方法克服这一问题，从而提高了模型的推理能力和自我纠正能力。

<details>
  <summary>Details</summary>

**Motivation:** 研究表明，由于蛋白质语言模型的标记空间表达能力有限，无法将链式思考（CoT）提示应用于类似蛋白质和RNA语言模型的非自然语言领域。因此，本论文旨在通过增强表达能力，提高生物序列模型的推理能力。

**Method:** 本研究提出了语言表达能力的概念，定义为一种语言使用其标记和语法规则编码信息的能力，并引入了反射预训练（reflection pretraining）的概念，首次在生物序列模型中应用。该方法通过生成辅助的“思考标记”来实现中间推理，这些标记超越了简单的答案标记。

**Result:** 该研究的理论部分表明，扩展的标记集显著提高了生物语言的表达能力，从而改善了模型的整体推理能力。实验结果则展示了该预训练方法能使蛋白质模型实现自我纠正，相较于标准预训练方法，性能有所提升。

**Conclusion:** 引入反射预训练，并通过生成辅助的“思考标记”来增强蛋白质语言模型的中间推理能力，实现了在蛋白质和RNA语言模型上的CoT提示的应用，从而提高了模型的性能。

**Abstract:** Chain-of-Thought (CoT) prompting has significantly advanced task-solving capabilities in natural language processing with large language models. Unlike standard prompting, CoT encourages the model to generate intermediate reasoning steps, non-answer tokens, that help guide the model toward more accurate final outputs. These intermediate steps enable more complex reasoning processes such as error correction, memory management, future planning, and self-reflection. However, applying CoT to non-natural language domains, such as protein and RNA language models, is not yet possible, primarily due to the limited expressiveness of their token spaces (e.g., amino acid tokens). In this work, we propose and define the concept of language expressiveness: the ability of a given language, using its tokens and grammar, to encode information. We show that the limited expressiveness of protein language severely restricts the applicability of CoT-style reasoning. To overcome this, we introduce reflection pretraining, for the first time in a biological sequence model, which enables the model to engage in intermediate reasoning through the generation of auxiliary "thinking tokens" beyond simple answer tokens. Theoretically, we demonstrate that our augmented token set significantly enhances biological language expressiveness, thereby improving the overall reasoning capacity of the model. Experimentally, our pretraining approach teaches protein models to self-correct and leads to substantial performance gains compared to standard pretraining.

</details>


### [20] [Automatic Replication of LLM Mistakes in Medical Conversations](https://arxiv.org/abs/2512.20983)
*Oleksii Proniakin,Diego Fajardo,Ruslan Nazarenko,Razvan Marinescu*

Main category: cs.CL

> 本文介绍了MedMistake，一个能够自动提取大型语言模型在医患对话中所犯错误并转化为单次问答对数据集的管道系统。该数据集包含了GPT-5和Gemini 2.5 Pro在内的多个前沿语言模型未能准确回答的3,390个问答对。医疗专家验证了子集中的211个问题，并用此进行了最终评估。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在医学领域评估时往往需要人工复制特定错误，这既费时又繁琐。MedMistake系统的引入是为了自动化这一过程，使其更加高效和易于操作。

**Method:** MedMistake系统通过复杂医患对话生成数据，使用两个大型语言模型评委评估对话中的错误，并简化错误为单次问答对。

**Result:** MedMistake-All包含了3,390个大型语言模型无法正确回答的问答对，其中的211个问答对被医疗专家验证用于最终评估中。评估结果显示GPT，Claude和Grok系列模型表现最佳。

**Conclusion:** MedMistake系统为医学生态的语言模型评估提供了一种自动化的方法，使得跨多个维度评估语言模型的性能变得更加便捷。评估结果显示了某些模型在特定环境下具有更好的性能表现。

**Abstract:** Large language models (LLMs) are increasingly evaluated in clinical settings using multi-dimensional rubrics which quantify reasoning quality, safety, and patient-centeredness. Yet, replicating specific mistakes in other LLM models is not straightforward and often requires manual effort. We introduce MedMistake, an automatic pipeline that extracts mistakes LLMs make in patient-doctor conversations and converts them into a benchmark of single-shot QA pairs. Our pipeline (1) creates complex, conversational data between an LLM patient and LLM doctor, (2) runs an evaluation with a committee of 2 LLM judges across a variety of dimensions and (3) creates simplified single-shot QA scenarios from those mistakes. We release MedMistake-All, a dataset of 3,390 single-shot QA pairs where GPT-5 and Gemini 2.5 Pro are currently failing to answer correctly, as judged by two LLM judges. We used medical experts to validate a subset of 211/3390 questions (MedMistake-Bench), which we used to run a final evaluation of 12 frontier LLMs: Claude Opus 4.5, Claude Sonnet 4.5, DeepSeek-Chat, Gemini 2.5 Pro, Gemini 3 Pro, GPT-4o, GPT-5, GPT-5.1, GPT-5.2, Grok 4, Grok 4.1, Mistral Large. We found that GPT models, Claude and Grok obtained the best performance on MedMistake-Bench. We release both the doctor-validated benchmark (MedMistake-Bench), as well as the full dataset (MedMistake-All) at https://huggingface.co/datasets/TheLumos/MedicalMistakeBenchmark.

</details>


### [21] [Distilling the Essence: Efficient Reasoning Distillation via Sequence Truncation](https://arxiv.org/abs/2512.21002)
*Wei-Rui Chen,Vignesh Kothapalli,Ata Fatahibaarzi,Hejian Sang,Shao Tang,Qingquan Song,Zhipeng Wang,Muhammad Abdul-Mageed*

Main category: cs.CL

> 本文通过研究如何在不同段落之间分配监督来优化知识蒸馏的过程，发现仅考虑早期推理标记可以有效减少训练成本，同时保持较高性能。

<details>
  <summary>Details</summary>

**Motivation:** 知识蒸馏过程在处理包含提示、链式思维、答案的长序列上需要大量的计算资源。为了提高效率，本文研究如何通过改变监督的分配来优化这个过程。

**Method:** 本文研究了如何在不同的段落（P，CoT，A）之间分配监督影响学生表现。实验表明，仅在CoT部分进行选择性知识蒸馏是有效的，前提是提示（P）和答案（A）的信息已经被包含在CoT中。基于这一洞察，本文建立了一种截断协议，以量化计算量和质量之间的权衡关系。

**Result:** 实验发现，仅使用每个训练序列的前50%的标记进行训练，可以在数学基准测试中平均保持约94%的完整序列性能，同时训练时间、内存使用量和浮点运算次数都减少了大约50%。

**Conclusion:** 研究表明，通过优先考虑早期的推理标记，可以在减少计算资源使用的同时保持较高的模型性能，这对于优化推理知识蒸馏提供了有效的工具。

**Abstract:** Distilling the reasoning capabilities from a large language model (LLM) to a smaller student model often involves training on substantial amounts of reasoning data. However, distillation over lengthy sequences with prompt (P), chain-of-thought (CoT), and answer (A) segments makes the process computationally expensive. In this work, we investigate how the allocation of supervision across different segments (P, CoT, A) affects student performance. Our analysis shows that selective knowledge distillation over only the CoT tokens can be effective when the prompt and answer information is encompassed by it. Building on this insight, we establish a truncation protocol to quantify computation-quality tradeoffs as a function of sequence length. We observe that training on only the first $50\%$ of tokens of every training sequence can retain, on average, $\approx94\%$ of full-sequence performance on math benchmarks while reducing training time, memory usage, and FLOPs by about $50\%$ each. These findings suggest that reasoning distillation benefits from prioritizing early reasoning tokens and provides a simple lever for computation-quality tradeoffs. Codes are available at https://github.com/weiruichen01/distilling-the-essence.

</details>


### [22] [Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy](https://arxiv.org/abs/2512.21017)
*Xiaofeng Shi,Qian Kou,Yuduo Li,Hua Zhou*

Main category: cs.CL

> 本文提出了一种新的两阶段微调方法SFTKey，旨在解决传统监督微调中对长链式思考序列过度关注的问题，通过实验验证了该方法的有效性和准确性提升。

<details>
  <summary>Details</summary>

**Motivation:** 传统监督微调模型在处理链式思考(CoT)序列时可能过于关注过长的序列，而减少对短但关键的部分的关注，即最终答案。这直接影响了任务的成功率和评估质量。

**Method:** 我们提出了一种名为SFTKey的两阶段训练方案。第一阶段使用传统的监督微调以确保输出格式的正确性；第二阶段仅对关键部分进行微调，以提高其准确性。

**Result:** 在多项基准测试和模型家族中的广泛实验表明，SFTKey相比于传统SFT方法准确率平均提高了超过5%，同时保持了生成正确格式的能力。

**Conclusion:** 这项研究通过明确平衡链式思考学习和对答案相关标记的额外优化，推进了大规模语言模型的微调方法。

**Abstract:** With the rapid advancement of Large Language Models (LLMs), the Chain-of-Thought (CoT) component has become significant for complex reasoning tasks. However, in conventional Supervised Fine-Tuning (SFT), the model could allocate disproportionately more attention to CoT sequences with excessive length. This reduces focus on the much shorter but essential Key portion-the final answer, whose correctness directly determines task success and evaluation quality. To address this limitation, we propose SFTKey, a two-stage training scheme. In the first stage, conventional SFT is applied to ensure proper output format, while in the second stage, only the Key portion is fine-tuned to improve accuracy. Extensive experiments across multiple benchmarks and model families demonstrate that SFTKey achieves an average accuracy improvement exceeding 5\% over conventional SFT, while preserving the ability to generate correct formats. Overall, this study advances LLM fine-tuning by explicitly balancing CoT learning with additional optimization on answer-relevant tokens.

</details>


### [23] [Semantic Refinement with LLMs for Graph Representations](https://arxiv.org/abs/2512.21106)
*Safal Thapaliya,Zehong Wang,Jiazheng Li,Ziming Li,Yanfang Ye,Chuxu Zhang*

Main category: cs.CL

> 通过数据自适应方法处理结构-语义异质性，提出DAS框架，结合GNN和LLM来进行图表示学习，以适应不同结构和语义特征的图数据。

<details>
  <summary>Details</summary>

**Motivation:** 大多数现有方法解决结构-语义异质性问题的方向是在模型层面逐步注入新的归纳偏差，但这种方法受限于实际世界图数据的开放和多样性。因此，本工作从数据的角度出发，将节点语义视为任务自适应变量。

**Method:** 提出了一种数据自适应语义优化框架（DAS），该框架在一个闭环反馈系统中结合了一个固定的图神经网络（GNN）和一个大型语言模型（LLM）。GNN提供隐式监督信号以引导LLM的语义优化，并且优化后的语义被反馈回来以更新同一个图学习器。

**Result:** 实验结果表明，在结构主导的图上有一致的改进，同时在语义丰富的图上保持竞争力，证明了在结构-语义异质性下基于数据的方法的有效性。

**Conclusion:** 数据自适应的语义调整在面对结构-语义异质性挑战时表现出色，证明了一种有效的数据自适应框架能够在不同类型的图数据间优化性能。

**Abstract:** Graph-structured data exhibit substantial heterogeneity in where their predictive signals originate: in some domains, node-level semantics dominate, while in others, structural patterns play a central role. This structure-semantics heterogeneity implies that no graph learning model with a fixed inductive bias can generalize optimally across diverse graph domains. However, most existing methods address this challenge from the model side by incrementally injecting new inductive biases, which remains fundamentally limited given the open-ended diversity of real-world graphs. In this work, we take a data-centric perspective and treat node semantics as a task-adaptive variable. We propose a Data-Adaptive Semantic Refinement framework DAS for graph representation learning, which couples a fixed graph neural network (GNN) and a large language model (LLM) in a closed feedback loop. The GNN provides implicit supervisory signals to guide the semantic refinement of LLM, and the refined semantics are fed back to update the same graph learner. We evaluate our approach on both text-rich and text-free graphs. Results show consistent improvements on structure-dominated graphs while remaining competitive on semantics-rich graphs, demonstrating the effectiveness of data-centric semantic adaptation under structure-semantics heterogeneity.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [24] [VL4Gaze: Unleashing Vision-Language Models for Gaze Following](https://arxiv.org/abs/2512.20735)
*Shijing Wang,Chaoqun Cui,Yaping Huang,Hyung Jin Chang,Yihua Cheng*

Main category: cs.CV

> 本文介绍了VL4Gaze，这是一个全新的大规模基准，用于评估和提升视觉语言模型(VLMs)对人类凝视的理解能力。通过四个互补任务，研究显示VLMs在没有特定任务监督的情况下难以准确推断凝视的语义和空间定位。

<details>
  <summary>Details</summary>

**Motivation:** 现有的视觉语言模型在理解人类凝视方面存在不足，而人类凝视为理解注意力、意图和社会互动提供了关键线索。因此，本文旨在填补这一研究空白。

**Method:** 提出VL4Gaze，包含489K个自动生成的问题-答案对和124K张图像。通过视觉问答(VQA)问题的四个互补任务来评估凝视理解能力。

**Result:** 研究显示，即使在大规模的VLMs中，如果不针对特定任务进行监督，它们也很难可靠地推断凝视的语义和空间位置。训练在VL4Gaze上带来了所有任务上的重大且一致改进。

**Conclusion:** 本文强调了针对特定任务的多任务监督在开发视觉语言模型凝视理解能力中的重要性。还将发布该数据集和代码，以支持在此方向上的进一步研究和发展。

**Abstract:** Human gaze provides essential cues for interpreting attention, intention, and social interaction in visual scenes, yet gaze understanding remains largely unexplored in current vision-language models (VLMs). While recent VLMs achieve strong scene-level reasoning across a range of visual tasks, there exists no benchmark that systematically evaluates or trains them for gaze interpretation, leaving open the question of whether gaze understanding can emerge from general-purpose vision-language pre-training. To address this gap, we introduce VL4Gaze, the first large-scale benchmark designed to investigate, evaluate, and unlock the potential of VLMs for gaze understanding. VL4Gaze contains 489K automatically generated question-answer pairs across 124K images and formulates gaze understanding as a unified VQA problem through four complementary tasks: (1) gaze object description, (2) gaze direction description, (3) gaze point location, and (4) ambiguous question recognition. We comprehensively evaluate both commercial and open-source VLMs under in-context learning and fine-tuning settings. The results show that even large-scale VLMs struggle to reliably infer gaze semantics and spatial localization without task-specific supervision. In contrast, training on VL4Gaze brings substantial and consistent improvements across all tasks, highlighting the importance of targeted multi-task supervision for developing gaze understanding capabilities in VLMs. We will release the dataset and code to support further research and development in this direction.

</details>


### [25] [TrashDet: Iterative Neural Architecture Search for Efficient Waste Detection](https://arxiv.org/abs/2512.20746)
*Tony Tran,Bin Hu*

Main category: cs.CV

> The paper introduces TrashDets, a family of TinyML-friendly trash detection models designed for edge and IoT devices. Through a specialized neural architecture search method, it achieves state-of-the-art performance with significant efficiency improvements over existing models.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to develop efficient trash detection systems for resource-constrained edge devices by optimizing models under TinyML constraints, aiming to improve accuracy and reduce energy consumption, latency, and power usage compared to existing detectors.

**Method:** The method involves an iterative, hardware-aware neural architecture search targeting edge devices, employing a Once-for-All-style ResDets supernet and a population passthrough mechanism, incorporating an accuracy predictor to stabilize and optimize search.

**Result:** The TrashDet family spans models from 1.2M to 30.5M parameters, offering scalable detection options with mAP50 values between 11.4 and 19.5 on the TACO dataset's five-class subset. For the MAX78002 microcontroller, TrashDet-ResNet and TrashDet-MBNet variants demonstrate strong performance, achieving notable improvements in efficiency measures.

**Conclusion:** The paper concludes that the TrashDet models, with their efficient architecture and optimized performance for TinyML environments, represent a significant advancement in the field, providing scalable solutions for resource-constrained edge and IoT devices while significantly outperforming baseline detectors in terms of efficiency and accuracy.

**Abstract:** This paper addresses trash detection on the TACO dataset under strict TinyML constraints using an iterative hardware-aware neural architecture search framework targeting edge and IoT devices. The proposed method constructs a Once-for-All-style ResDets supernet and performs iterative evolutionary search that alternates between backbone and neck/head optimization, supported by a population passthrough mechanism and an accuracy predictor to reduce search cost and improve stability. This framework yields a family of deployment-ready detectors, termed TrashDets. On a five-class TACO subset (paper, plastic, bottle, can, cigarette), the strongest variant, TrashDet-l, achieves 19.5 mAP50 with 30.5M parameters, improving accuracy by up to 3.6 mAP50 over prior detectors while using substantially fewer parameters. The TrashDet family spans 1.2M to 30.5M parameters with mAP50 values between 11.4 and 19.5, providing scalable detector options for diverse TinyML deployment budgets on resource-constrained hardware. On the MAX78002 microcontroller with the TrashNet dataset, two specialized variants, TrashDet-ResNet and TrashDet-MBNet, jointly dominate the ai87-fpndetector baseline, with TrashDet-ResNet achieving 7525~$μ$J energy per inference at 26.7 ms latency and 37.45 FPS, and TrashDet-MBNet improving mAP50 by 10.2%; together they reduce energy consumption by up to 88%, latency by up to 78%, and average power by up to 53% compared to existing TinyML detectors.

</details>


### [26] [OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective](https://arxiv.org/abs/2512.20770)
*Markus Gross,Sai B. Matha,Aya Fahmy,Rui Song,Daniel Cremers,Henri Meess*

Main category: cs.CV

> 本文提出OccuFly，一个基于相机的空中SSC基准数据集，解决了缺少LiDAR数据的挑战，通过一个无LiDAR的数据生成框架，利用3D重建技术和自动化标签转移技术来减少繁重的3D标注工作。

<details>
  <summary>Details</summary>

**Motivation:** 现有的SSC研究大多集中在地面场景，空中场景的研究较少，且使用LiDAR传感器生成数据存在许多限制，如法规、质量和能量约束以及从高空位置获得的点云稀疏性。因此，本文提出了一个基于相机的空中SSC数据集及数据生成框架，以适应现代无人驾驶飞行器的广泛使用。

**Method:** 作者创建了OccuFly数据集，并引入了一个LiDAR-free的数据生成方法。该方法使用传统的3D重建技术和自动化标签转移技术，将2D掩码提升到重建的点云中，来减少3D标注的工作量。

**Result:** OccuFly数据集覆盖了城市、工业和农村场景，并提供了22种语义类别。同时，通过现有的研究基础设施对状态-of-the-art方法进行了基准测试，并指出了特定于高空视角的几个挑战。

**Conclusion:** OccuFly作为第一个空中SSC基准，提供了一个新的研究视角。通过提出LiDAR-free的数据生成框架，这篇文章推动了基于相机的3D场景理解的研究进展。

**Abstract:** Semantic Scene Completion (SSC) is crucial for 3D perception in mobile robotics, as it enables holistic scene understanding by jointly estimating dense volumetric occupancy and per-voxel semantics. Although SSC has been widely studied in terrestrial domains such as autonomous driving, aerial scenarios like autonomous flying remain largely unexplored, thereby limiting progress on downstream applications. Furthermore, LiDAR sensors represent the primary modality for SSC data generation, which poses challenges for most uncrewed aerial vehicles (UAVs) due to flight regulations, mass and energy constraints, and the sparsity of LiDAR-based point clouds from elevated viewpoints. To address these limitations, we introduce OccuFly, the first real-world, camera-based aerial SSC benchmark, captured at altitudes of 50m, 40m, and 30m during spring, summer, fall, and winter. OccuFly covers urban, industrial, and rural scenarios, provides 22 semantic classes, and the data format adheres to established conventions to facilitate seamless integration with existing research. Crucially, we propose a LiDAR-free data generation framework based on camera modality, which is ubiquitous on modern UAVs. By utilizing traditional 3D reconstruction, our framework automates label transfer by lifting a subset of annotated 2D masks into the reconstructed point cloud, thereby substantially minimizing manual 3D annotation effort. Finally, we benchmark the state-of-the-art on OccuFly and highlight challenges specific to elevated viewpoints, yielding a comprehensive vision benchmark for holistic aerial 3D scene understanding.

</details>


### [27] [NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts](https://arxiv.org/abs/2512.20783)
*Raja Mallina,Bryar Shareef*

Main category: cs.CV

> NullBUS is a proposed framework for BUS segmentation that utilizes a mix of image-only and multimodal data by incorporating nullable prompts, achieving high performance even when metadata is missing.

<details>
  <summary>Details</summary>

**Motivation:** The lack of reliable metadata in public BUS datasets limits the robustness of segmentation models trained on small multimodal subsets.

**Method:** NullBUS introduces nullable prompts as learnable null embeddings with presence masks, allowing the model to use text when available but fall back to image-only data when metadata is absent.

**Result:** When evaluated on a pool of three public BUS datasets, NullBUS achieved a mean IoU of 0.8568 and a mean Dice of 0.9103.

**Conclusion:** NullBUS demonstrates state-of-the-art performance in BUS segmentation under mixed prompt availability.

**Abstract:** Breast ultrasound (BUS) segmentation provides lesion boundaries essential for computer-aided diagnosis and treatment planning. While promptable methods can improve segmentation performance and tumor delineation when text or spatial prompts are available, many public BUS datasets lack reliable metadata or reports, constraining training to small multimodal subsets and reducing robustness. We propose NullBUS, a multimodal mixed-supervision framework that learns from images with and without prompts in a single model. To handle missing text, we introduce nullable prompts, implemented as learnable null embeddings with presence masks, enabling fallback to image-only evidence when metadata are absent and the use of text when present. Evaluated on a unified pool of three public BUS datasets, NullBUS achieves a mean IoU of 0.8568 and a mean Dice of 0.9103, demonstrating state-of-the-art performance under mixed prompt availability.

</details>


### [28] [Learning to Sense for Driving: Joint Optics-Sensor-Model Co-Design for Semantic Segmentation](https://arxiv.org/abs/2512.20815)
*Reeshad Khan amd John Gauch*

Main category: cs.CV

> This paper introduces a co-designed RAW-to-task pipeline for autonomous driving that integrates optics, sensors, and semantic segmentation networks, achieving better performance and deployability over conventional systems.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to overcome the limitations of traditional autonomous driving pipelines which ignore machine semantics and are prone to sensor artifacts. By unifying optics, sensors, and networks, the system is designed to better serve machine vision tasks.

**Method:** Traditional autonomous driving systems separate camera design from the perception process, but this paper proposes a co-design framework that integrates optics, sensor modeling, and segmentation networks to optimize the RAW-to-task pipeline. This framework includes learnable color filter arrays and noise simulations, tailored specifically for segmentation tasks.

**Result:** Evaluations on KITTI-360 indicate significant improvement in mIoU compared to traditional methods, especially for challenging conditions. The model is also compact and can run at high frame rates, making it practical for deployment on mobile devices.

**Conclusion:** The research demonstrates that full-stack co-optimization for autonomous systems enhances both efficiency and reliability of perception tasks, particularly for edge applications. This can be seen through visual and quantitative improvements in semantic segmentation performance.

**Abstract:** Traditional autonomous driving pipelines decouple camera design from downstream perception, relying on fixed optics and handcrafted ISPs that prioritize human viewable imagery rather than machine semantics. This separation discards information during demosaicing, denoising, or quantization, while forcing models to adapt to sensor artifacts. We present a task-driven co-design framework that unifies optics, sensor modeling, and lightweight semantic segmentation networks into a single end-to-end RAW-to-task pipeline. Building on DeepLens[19], our system integrates realistic cellphone-scale lens models, learnable color filter arrays, Poisson-Gaussian noise processes, and quantization, all optimized directly for segmentation objectives. Evaluations on KITTI-360 show consistent mIoU improvements over fixed pipelines, with optics modeling and CFA learning providing the largest gains, especially for thin or low-light-sensitive classes. Importantly, these robustness gains are achieved with a compact ~1M-parameter model running at ~28 FPS, demonstrating edge deployability. Visual and quantitative analyses further highlight how co-designed sensors adapt acquisition to semantic structure, sharpening boundaries and maintaining accuracy under blur, noise, and low bit-depth. Together, these findings establish full-stack co-optimization of optics, sensors, and networks as a principled path toward efficient, reliable, and deployable perception in autonomous systems.

</details>


### [29] [CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images](https://arxiv.org/abs/2512.20833)
*Vidit Agrawal,John Peters,Tyler N. Thompson,Mohammad Vali Sanian,Chau Pham,Nikita Moshkov,Arshad Kazi,Aditya Pillai,Jack Freeman,Byunguk Kang,Samouil L. Farhi,Ernest Fraenkel,Ron Stewart,Lassi Paavolainen,Bryan A. Plummer,Juan C. Caicedo*

Main category: cs.CV

> CHAMMI-75 facilitates the creation of adaptable cellular morphology models that can work with diverse microscopy image types, enhancing the effectiveness of multi-channel bioimaging tasks.

<details>
  <summary>Details</summary>

**Motivation:** The goal of the paper is to address the limitation of existing cellular morphology models that are typically trained with a single microscopy imaging type and cannot be reused across studies due to differences in technical specifications or out-of-distribution experimental conditions.

**Method:** The paper presents CHAMMI-75, an open-access dataset of heterogeneous, multi-channel microscopy images from 75 diverse biological studies. This resource is designed to investigate cellular morphology models that are channel-adaptive and can process any microscopy image type.

**Result:** Training with CHAMMI-75 significantly improves the performance in multi-channel bioimaging tasks compared to single-type models due to the high diversity in microscopy modalities.

**Conclusion:** The work paves the way for the development of next-generation cellular morphology models, enabling broader applicability across different biological studies.

**Abstract:** Quantifying cell morphology using images and machine learning has proven to be a powerful tool to study the response of cells to treatments. However, models used to quantify cellular morphology are typically trained with a single microscopy imaging type. This results in specialized models that cannot be reused across biological studies because the technical specifications do not match (e.g., different number of channels), or because the target experimental conditions are out of distribution. Here, we present CHAMMI-75, an open access dataset of heterogeneous, multi-channel microscopy images from 75 diverse biological studies. We curated this resource from publicly available sources to investigate cellular morphology models that are channel-adaptive and can process any microscopy image type. Our experiments show that training with CHAMMI-75 can improve performance in multi-channel bioimaging tasks primarily because of its high diversity in microscopy modalities. This work paves the way to create the next generation of cellular morphology models for biological studies.

</details>


### [30] [Input-Adaptive Visual Preprocessing for Efficient Fast Vision-Language Model Inference](https://arxiv.org/abs/2512.20839)
*Putu Indah Githa Cahyani,Komang David Dananjaya Suartana,Novanto Yudistira*

Main category: cs.CV

> The paper presents an adaptive visual preprocessing method to enhance the efficiency of Vision-Language Models by dynamically adjusting input resolution and spatial coverage based on image content, achieving significant improvements in inference time and token count without modifying the VLM architecture

<details>
  <summary>Details</summary>

**Motivation:** to reduce redundant computation for visually simple inputs and improve the efficiency of Vision-Language Models (VLMs), especially for high-resolution visual inputs

**Method:** adaptive visual preprocessing method that dynamically adjusts input resolution and spatial coverage based on image content characteristics, combining content-aware image analysis, adaptive resolution selection, and content-aware cropping

**Result:** reduces per-image inference time by over 50%, lowers mean full generation time, and achieves more than 55% reduction in visual token count compared to the baseline pipeline

**Conclusion:** input-aware preprocessing is an effective and lightweight strategy for improving deployment-oriented efficiency of vision-language models

**Abstract:** Vision-Language Models (VLMs) have demonstrated strong performance on multimodal reasoning tasks, but their deployment remains challenging due to high inference latency and computational cost, particularly when processing high-resolution visual inputs. While recent architectures such as FastVLM improve efficiency through optimized vision encoders, existing pipelines still rely on static visual preprocessing, leading to redundant computation for visually simple inputs. In this work, we propose an adaptive visual preprocessing method that dynamically adjusts input resolution and spatial coverage based on image content characteristics. The proposed approach combines content-aware image analysis, adaptive resolution selection, and content-aware cropping to reduce visual redundancy prior to vision encoding. Importantly, the method is integrated with FastVLM without modifying its architecture or requiring retraining. We evaluate the proposed method on a subset of the DocVQA dataset in an inference-only setting, focusing on efficiency-oriented metrics. Experimental results show that adaptive preprocessing reduces per-image inference time by over 50\%, lowers mean full generation time, and achieves a consistent reduction of more than 55\% in visual token count compared to the baseline pipeline. These findings demonstrate that input-aware preprocessing is an effective and lightweight strategy for improving deployment-oriented efficiency of vision-language models. To facilitate reproducibility, our implementation is provided as a fork of the FastVLM repository, incorporating the files for the proposed method, and is available at https://github.com/kmdavidds/mlfastlm.

</details>


### [31] [ALIVE: An Avatar-Lecture Interactive Video Engine with Content-Aware Retrieval for Real-Time Interaction](https://arxiv.org/abs/2512.20858)
*Md Zabirul Islam,Md Motaleb Hossen Manik,Ge Wang*

Main category: cs.CV

> 论文介绍了一个名为ALIVE的系统，通过整合基于本地硬件运行的数字人生成讲座、内容敏感检索和实时互动功能，实现了从被动观看到动态互动学习的转变，有效提升了录制讲座的教育价值。

<details>
  <summary>Details</summary>

**Motivation:** 论文的动机源自于传统视频lecture缺乏即时澄清的机制，导致学习者在遇到困惑时需要进行额外搜索。同时，基于大型语言模型和神经网络avatar的交互式学习系统往往不能很好地与讲座内容结合，依赖云端服务，或是检索解释与avatar输出分离。ALIVE系统旨在改善这些问题，提供一种隐私保护下的统一交互学习体验。

**Method:** 该论文提出了ALIVE系统，它通过结合自动生成讲座的数字人、内容感知检索机制以及实时多模式交互功能，将传统的单向视频讲座转变为具有实时互动性的动态学习体验。该系统利用本地硬件运行，可以通过语音或文本提出问题，并获得基于文本或数字人形式的回答。为了保持响应速度，ALIVE采用了轻量级嵌入模型、FAISS检索算法和分段数字人合成技术，实现了嵌入式预加载。

**Result:** 研究结果表明，ALIVE系统能够在医学成像课程的全流程演示中，准确地提供内容敏感的实时支持，验证了其检索准确性、延时特性和用户体验。

**Conclusion:** 结论是，通过多模态AI技术结合内容感知检索和本地部署，ALIVE系统能够大大提高录制lecture的教学价值，为下一代交互式学习环境提供了可扩展的发展路径。

**Abstract:** Traditional lecture videos offer flexibility but lack mechanisms for real-time clarification, forcing learners to search externally when confusion arises. Recent advances in large language models and neural avatars provide new opportunities for interactive learning, yet existing systems typically lack lecture awareness, rely on cloud-based services, or fail to integrate retrieval and avatar-delivered explanations in a unified, privacy-preserving pipeline.
  We present ALIVE, an Avatar-Lecture Interactive Video Engine that transforms passive lecture viewing into a dynamic, real-time learning experience. ALIVE operates fully on local hardware and integrates (1) Avatar-delivered lecture generated through ASR transcription, LLM refinement, and neural talking-head synthesis; (2) A content-aware retrieval mechanism that combines semantic similarity with timestamp alignment to surface contextually relevant lecture segments; and (3) Real-time multimodal interaction, enabling students to pause the lecture, ask questions through text or voice, and receive grounded explanations either as text or as avatar-delivered responses.
  To maintain responsiveness, ALIVE employs lightweight embedding models, FAISS-based retrieval, and segmented avatar synthesis with progressive preloading. We demonstrate the system on a complete medical imaging course, evaluate its retrieval accuracy, latency characteristics, and user experience, and show that ALIVE provides accurate, content-aware, and engaging real-time support.
  ALIVE illustrates how multimodal AI-when combined with content-aware retrieval and local deployment-can significantly enhance the pedagogical value of recorded lectures, offering an extensible pathway toward next-generation interactive learning environments.

</details>


### [32] [Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images](https://arxiv.org/abs/2512.20866)
*Haotian Lv,Chao Li,Jiangbo Dai,Yuhui Zhang,Zepeng Fan,Yiqiu Tan,Dawei Wang,Binglei Xie*

Main category: cs.CV

> 本文提出了一种三维管道智能检测框架，通过集成深度学习优化策略与3D GPR的物理特性，解决了多视图特征弱相关性、小规模目标识别精度低和复杂场景下的鲁棒性不足等问题，为地下管道的智能识别与定位提供了一种高效可靠的新型技术框架。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决使用3D GPR在地下管道检测中存在的多视图特征弱相关性、小规模目标识别精度低和复杂场景下的鲁棒性不足等问题。

**Method:** 基于B/C/D-Scan三视图联合分析策略，提出了三维管道三视图特征评估方法。通过将FDTD方法获得的正向模拟结果与实际测量数据进行交叉验证来建立该方法。接着，提出了DCO-YOLO框架，该框架在原始YOLOv11算法中集成了DySample、CGLU和OutlookAttention跨维度关联机制，显著提高了对小型管道边缘特征的提取能力。此外，提出了3D-DIoU空间特征匹配算法，该算法通过整合三维几何约束和中心距离惩罚项实现多视图标注的自动化关联。

**Result:** 该方法在复杂多管道场景中实现了96.2%的准确率、93.3%的召回率和96.7%的平均精度均值，分别比基线模型高出2.0%、2.1%和0.9%。

**Conclusion:** 本文所提出的方法为复杂多管道场景中基于深度学习的地下管道智能检测提供了一种有效且可靠的方法，证明了其在复杂场景中的优越性能和鲁棒性。

**Abstract:** To address the issues of weak correlation between multi-view features, low recognition accuracy of small-scale targets, and insufficient robustness in complex scenarios in underground pipeline detection using 3D GPR, this paper proposes a 3D pipeline intelligent detection framework. First, based on a B/C/D-Scan three-view joint analysis strategy, a three-dimensional pipeline three-view feature evaluation method is established by cross-validating forward simulation results obtained using FDTD methods with actual measurement data. Second, the DCO-YOLO framework is proposed, which integrates DySample, CGLU, and OutlookAttention cross-dimensional correlation mechanisms into the original YOLOv11 algorithm, significantly improving the small-scale pipeline edge feature extraction capability. Furthermore, a 3D-DIoU spatial feature matching algorithm is proposed, which integrates three-dimensional geometric constraints and center distance penalty terms to achieve automated association of multi-view annotations. The three-view fusion strategy resolves inherent ambiguities in single-view detection. Experiments based on real urban underground pipeline data show that the proposed method achieves accuracy, recall, and mean average precision of 96.2%, 93.3%, and 96.7%, respectively, in complex multi-pipeline scenarios, which are 2.0%, 2.1%, and 0.9% higher than the baseline model. Ablation experiments validated the synergistic optimization effect of the dynamic feature enhancement module and Grad-CAM++ heatmap visualization demonstrated that the improved model significantly enhanced its ability to focus on pipeline geometric features. This study integrates deep learning optimization strategies with the physical characteristics of 3D GPR, offering an efficient and reliable novel technical framework for the intelligent recognition and localization of underground pipelines.

</details>


### [33] [NeRV360: Neural Representation for 360-Degree Videos with a Viewport Decoder](https://arxiv.org/abs/2512.20871)
*Daichi Arai,Kyohei Unno,Yasuko Sugito,Yuichi Kusakabe*

Main category: cs.CV

> NeRV360, an implicit neural representation framework for 360-degree videos, focuses on user-selected viewports, integrating viewport extraction into decoding and using a spatial-temporal affine transform module. This results in significant memory savings and faster decoding speed without compromising image quality.

<details>
  <summary>Details</summary>

**Motivation:** To address the high memory consumption and slow decoding speed associated with applying NeRV to high-resolution 360-degree videos, which are impractical for real-time applications.

**Method:** NeRV360 proposes an end-to-end framework that decodes only the user-selected viewport. It integrates viewport extraction into the decoding process and introduces a spatial-temporal affine transform module for conditional decoding based on viewpoint and time.

**Result:** Experiments on 6K-resolution videos show that NeRV360 achieves a 7-fold reduction in memory consumption and a 2.5-fold increase in decoding speed compared to prior work, HNeRV, while maintaining superior image quality measured by objective metrics.

**Conclusion:** NeRV360 effectively reduces memory usage and improves decoding speed for high-resolution 360-degree videos, making real-time applications possible, and it preserves better image quality compared to conventional methods.

**Abstract:** Implicit neural representations for videos (NeRV) have shown strong potential for video compression. However, applying NeRV to high-resolution 360-degree videos causes high memory usage and slow decoding, making real-time applications impractical. We propose NeRV360, an end-to-end framework that decodes only the user-selected viewport instead of reconstructing the entire panoramic frame. Unlike conventional pipelines, NeRV360 integrates viewport extraction into decoding and introduces a spatial-temporal affine transform module for conditional decoding based on viewpoint and time. Experiments on 6K-resolution videos show that NeRV360 achieves a 7-fold reduction in memory consumption and a 2.5-fold increase in decoding speed compared to HNeRV, a representative prior work, while delivering better image quality in terms of objective metrics.

</details>


### [34] [Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification](https://arxiv.org/abs/2512.20892)
*Tingfeng Xian,Wenlve Zhou,Zhiheng Zhou,Zhelin Li*

Main category: cs.CV

> This paper introduces a new method called Domain Representation Injection (DRI) for enhancing Cross-Modality Ship Re-Identification (CMS Re-ID) tasks. By injecting domain-specific representations into a frozen Vision Foundation Model, the method achieves SOTA performance with minimal trainable parameters, demonstrating its effectiveness and efficiency, particularly on the HOSS-ReID dataset.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the challenge of modality discrepancies in Cross-Modality Ship Re-Identification (CMS Re-ID), which hinders the tracking of maritime targets in various conditions. The paper seeks to optimize the feature space for limited-capacity models and enhance the performance of existing PEFT methods.

**Method:** Our method proposes a novel Parameter-Efficient Fine-Tuning (PEFT) strategy called Domain Representation Injection (DRI). It utilizes a Vision Foundation Model (VFM) frozen to preserve general knowledge and a lightweight, learnable Offset Encoder to extract domain-specific representations. These representations are adaptively transformed and injected into the intermediate layers via additive fusion, thereby reshaping the feature distribution to suit the downstream task efficiently with minimal trainable parameters.

**Result:** Experimental results showcase the effectiveness of the proposed method, achieving State-of-the-Art (SOTA) performance for CMS Re-ID tasks. On the HOSS-ReID dataset, the method achieves mAP scores of 57.9% and 60.5% with relatively small parameter counts of 1.54M and 7.05M, respectively.

**Conclusion:** The paper concludes that the proposed Domain Representation Injection (DRI) method is superior in efficiency and performance for CMS Re-ID tasks, capable of achieving high accuracy with fewer parameters, thereby being more practical and cost-effective for real-world applications.

**Abstract:** Cross-Modality Ship Re-Identification (CMS Re-ID) is critical for achieving all-day and all-weather maritime target tracking, yet it is fundamentally challenged by significant modality discrepancies. Mainstream solutions typically rely on explicit modality alignment strategies; however, this paradigm heavily depends on constructing large-scale paired datasets for pre-training. To address this, grounded in the Platonic Representation Hypothesis, we explore the potential of Vision Foundation Models (VFMs) in bridging modality gaps. Recognizing the suboptimal performance of existing generic Parameter-Efficient Fine-Tuning (PEFT) methods that operate within the weight space, particularly on limited-capacity models, we shift the optimization perspective to the feature space and propose a novel PEFT strategy termed Domain Representation Injection (DRI). Specifically, while keeping the VFM fully frozen to maximize the preservation of general knowledge, we design a lightweight, learnable Offset Encoder to extract domain-specific representations rich in modality and identity attributes from raw inputs. Guided by the contextual information of intermediate features at different layers, a Modulator adaptively transforms these representations. Subsequently, they are injected into the intermediate layers via additive fusion, dynamically reshaping the feature distribution to adapt to the downstream task without altering the VFM's pre-trained weights. Extensive experimental results demonstrate the superiority of our method, achieving State-of-the-Art (SOTA) performance with minimal trainable parameters. For instance, on the HOSS-ReID dataset, we attain 57.9\% and 60.5\% mAP using only 1.54M and 7.05M parameters, respectively. The code is available at https://github.com/TingfengXian/DRI.

</details>


### [35] [DGSAN: Dual-Graph Spatiotemporal Attention Network for Pulmonary Nodule Malignancy Prediction](https://arxiv.org/abs/2512.20898)
*Xiao Yu,Zhaojie Fang,Guanyu Zhou,Yin Shen,Huoling Luo,Ye Li,Ahmed Elazab,Xiang Wan,Ruiquan Ge,Changmiao Wang*

Main category: cs.CV

> This paper introduces a Dual-Graph Spatiotemporal Attention Network for enhancing the early detection and diagnosis of pulmonary nodules through better multimodal information fusion, achieving higher accuracy and efficiency.

<details>
  <summary>Details</summary>

**Motivation:** The motivation stems from the need for more effective multimodal information fusion methods in the early detection and diagnosis of pulmonary nodules, as existing methods are limited.

**Method:** Our approach involves developing a Global-Local Feature Encoder, Dual-Graph Construction method, and a Hierarchical Cross-Modal Graph Fusion Module to refine the integration of multimodal information, all working together within a Dual-Graph Spatiotemporal Attention Network (DGSAN).

**Result:** Experiments on the NLST-cmst and CSTL-derived datasets show that the DGSAN substantially improves the accuracy of pulmonary nodule classification compared to existing methods.

**Conclusion:** The study concludes that the proposed DGSAN significantly outperforms state-of-the-art methods in classifying pulmonary nodules with high accuracy and computational efficiency.

**Abstract:** Lung cancer continues to be the leading cause of cancer-related deaths globally. Early detection and diagnosis of pulmonary nodules are essential for improving patient survival rates. Although previous research has integrated multimodal and multi-temporal information, outperforming single modality and single time point, the fusion methods are limited to inefficient vector concatenation and simple mutual attention, highlighting the need for more effective multimodal information fusion. To address these challenges, we introduce a Dual-Graph Spatiotemporal Attention Network, which leverages temporal variations and multimodal data to enhance the accuracy of predictions. Our methodology involves developing a Global-Local Feature Encoder to better capture the local, global, and fused characteristics of pulmonary nodules. Additionally, a Dual-Graph Construction method organizes multimodal features into inter-modal and intra-modal graphs. Furthermore, a Hierarchical Cross-Modal Graph Fusion Module is introduced to refine feature integration. We also compiled a novel multimodal dataset named the NLST-cmst dataset as a comprehensive source of support for related research. Our extensive experiments, conducted on both the NLST-cmst and curated CSTL-derived datasets, demonstrate that our DGSAN significantly outperforms state-of-the-art methods in classifying pulmonary nodules with exceptional computational efficiency.

</details>


### [36] [Benchmarking and Enhancing VLM for Compressed Image Understanding](https://arxiv.org/abs/2512.20901)
*Zifu Zhang,Tongda Xu,Siqi Li,Shengxi Li,Yue Zhang,Mai Xu,Yan Wang*

Main category: cs.CV

> 本文介绍了一个评估VLM在压缩图像上的新基准，并提出了一种提升VLM性能的适配器。

<details>
  <summary>Details</summary>

**Motivation:** 随着视觉语言模型的发展及其应用需求的增长，降低模型输入图像的带宽成了一个重要的问题，因此需要研究VLM如何处理低码率压缩图像的问题。

**Method:** 我们提出了一种通用的视觉语言模型适配器来提升模型在已压缩图像上的性能，并通过一个包含超过一百万张压缩图像的全面基准测试来评估视觉语言模型（VLM）在不同压缩条件下的表现。

**Result:** 实验结果表明，提出的适配器可以将VLM在不同编码器和码率下的性能提升10%到30%。

**Conclusion:** 我们通过实验展示了适配器的有效性和适用性，并为未来的模型和应用提供了一个重要的基准。

**Abstract:** With the rapid development of Vision-Language Models (VLMs) and the growing demand for their applications, efficient compression of the image inputs has become increasingly important. Existing VLMs predominantly digest and understand high-bitrate compressed images, while their ability to interpret low-bitrate compressed images has yet to be explored by far. In this paper, we introduce the first comprehensive benchmark to evaluate the ability of VLM against compressed images, varying existing widely used image codecs and diverse set of tasks, encompassing over one million compressed images in our benchmark. Next, we analyse the source of performance gap, by categorising the gap from a) the information loss during compression and b) generalisation failure of VLM. We visualize these gaps with concrete examples and identify that for compressed images, only the generalization gap can be mitigated. Finally, we propose a universal VLM adaptor to enhance model performance on images compressed by existing codecs. Consequently, we demonstrate that a single adaptor can improve VLM performance across images with varying codecs and bitrates by 10%-30%. We believe that our benchmark and enhancement method provide valuable insights and contribute toward bridging the gap between VLMs and compressed images.

</details>


### [37] [PanoGrounder: Bridging 2D and 3D with Panoramic Scene Representations for VLM-based 3D Visual Grounding](https://arxiv.org/abs/2512.20907)
*Seongmin Jung,Seongho Choi,Gunwoo Jeon,Minsu Cho,Jongwoo Lim*

Main category: cs.CV

> 提出了PanoGrounder框架，结合全景表示与2D视觉语言模型进行3D视觉接地任务，性能优于现有方法，具有良好的泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 传统的监督模型依赖显式的3D几何信息，但在可泛化的视觉语言模型面前表现较弱，且缺乏大规模的3D视觉语言数据集。因此，需要一种可以更好地泛化到未经见过的数据集和语言表述变化的方法来填补语言理解与3D场景推理之间的空白。

**Method:** 我们提出了一种名为PanoGrounder的3D视觉接地框架，该框架通过结合多模态全景表示与预训练的2D视觉语言模型来进行强有力的视觉-语言推理。该模型利用带有3D语义和几何特征的全景渲染图作为2D和3D之间的中间表示，并能够直接输入到VLM中进行推理，同时保留了全景视角下的长距离物体关系。整个过程分为三个阶段：选取紧凑的全景视点、在每个全景图上进行文本查询配对，以及融合各个视角的预测结果生成最终的3D边界框。

**Result:** 实验结果表明，我们的方法在ScanRefer和Nr3D数据集上取得了最先进的性能，并且展示了它在未见过的3D数据集和文本改写中的优越泛化能力。

**Conclusion:** 通过结合多模态数据和预训练模型的强大推理能力，PanoGrounder展示了一种创新的方法解决3D视觉接地问题，为未来在机器人和视觉语言感知领域的研究奠定了基础。

**Abstract:** 3D Visual Grounding (3DVG) is a critical bridge from vision-language perception to robotics, requiring both language understanding and 3D scene reasoning. Traditional supervised models leverage explicit 3D geometry but exhibit limited generalization, owing to the scarcity of 3D vision-language datasets and the limited reasoning capabilities compared to modern vision-language models (VLMs). We propose PanoGrounder, a generalizable 3DVG framework that couples multi-modal panoramic representation with pretrained 2D VLMs for strong vision-language reasoning. Panoramic renderings, augmented with 3D semantic and geometric features, serve as an intermediate representation between 2D and 3D, and offer two major benefits: (i) they can be directly fed to VLMs with minimal adaptation and (ii) they retain long-range object-to-object relations thanks to their 360-degree field of view. We devise a three-stage pipeline that places a compact set of panoramic viewpoints considering the scene layout and geometry, grounds a text query on each panoramic rendering with a VLM, and fuses per-view predictions into a single 3D bounding box via lifting. Our approach achieves state-of-the-art results on ScanRefer and Nr3D, and demonstrates superior generalization to unseen 3D datasets and text rephrasings.

</details>


### [38] [Self-supervised Multiplex Consensus Mamba for General Image Fusion](https://arxiv.org/abs/2512.20921)
*Yingying Wang,Rongjin Zhuang,Hui Zheng,Xuanhua He,Ke Cao,Xiaotong Tu,Xinghao Ding*

Main category: cs.CV

> 本文提出了SMC-Mamba框架，通过MAFE模块和MCCM模块以及BSCL损失函数实现了多模态信息的高效整合，增强了跨多种任务的图像融合效果。

<details>
  <summary>Details</summary>

**Motivation:** 传统的任务特定技术主要关注多模态信息的整合，而对于通用图像融合而言，需要解决广泛的任务并提升性能同时不增加复杂度。

**Method:** 提出了SMC-Mamba框架，包括MAFE模块和MCCM模块，以及BSCL损失函数。MAFE模块通过自适应门控和空间-通道及频率-旋转扫描来保留细节和增强全局表示。MCCM模块通过多模态专家的动态协作达成共识以整合信息，BSCL则在不增加计算量的同时保留高频率信息并提升下游任务性能。

**Result:** 在红外可见光、医学、多焦点和多曝光融合以及下游视觉任务中，该方法优于现有的最先进的图像融合算法。

**Conclusion:** 实验结果证实了所提方法在多种图像融合任务中优于现有的方法，证明了其在提升图像质量和下游视觉任务中的有效性。

**Abstract:** Image fusion integrates complementary information from different modalities to generate high-quality fused images, thereby enhancing downstream tasks such as object detection and semantic segmentation. Unlike task-specific techniques that primarily focus on consolidating inter-modal information, general image fusion needs to address a wide range of tasks while improving performance without increasing complexity. To achieve this, we propose SMC-Mamba, a Self-supervised Multiplex Consensus Mamba framework for general image fusion. Specifically, the Modality-Agnostic Feature Enhancement (MAFE) module preserves fine details through adaptive gating and enhances global representations via spatial-channel and frequency-rotational scanning. The Multiplex Consensus Cross-modal Mamba (MCCM) module enables dynamic collaboration among experts, reaching a consensus to efficiently integrate complementary information from multiple modalities. The cross-modal scanning within MCCM further strengthens feature interactions across modalities, facilitating seamless integration of critical information from both sources. Additionally, we introduce a Bi-level Self-supervised Contrastive Learning Loss (BSCL), which preserves high-frequency information without increasing computational overhead while simultaneously boosting performance in downstream tasks. Extensive experiments demonstrate that our approach outperforms state-of-the-art (SOTA) image fusion algorithms in tasks such as infrared-visible, medical, multi-focus, and multi-exposure fusion, as well as downstream visual tasks.

</details>


### [39] [Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting](https://arxiv.org/abs/2512.20927)
*Yoonwoo Jeong,Cheng Sun,Frank Wang,Minsu Cho,Jaesung Choe*

Main category: cs.CV

> 提出Quantile Rendering (Q-Render)和Gaussian Splatting Network (GS-Net)，在保持高保真度的同时，实现了高维特征的高效处理和实时渲染。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法采用了代码本或特征压缩技术解决高效渲染高维特征的问题，但却导致了信息损失，从而降低了分割质量。

**Method:** 引入了Quantile Rendering (Q-Render) 技术，这是一种新型的3D高斯渲染策略，可以在处理高维特征时维持高保真度。并且提出了Gaussian Splatting Network (GS-Net)，这是一般化的3D神经网络，可以预测高斯特征。

**Result:** 实验表明，与现有最先进的方法相比，该框架在ScanNet和LeRF数据集上性能更优，并且能够实现实时渲染，对512-D特征映射的速度提高了约43.7倍。

**Conclusion:** Q-Render和GS-Net在处理高维特征时更高效，保持了高保真度，并且实现了显著的速度提升。代码将会公开提供。

**Abstract:** Recent advancements in computer vision have successfully extended Open-vocabulary segmentation (OVS) to the 3D domain by leveraging 3D Gaussian Splatting (3D-GS). Despite this progress, efficiently rendering the high-dimensional features required for open-vocabulary queries poses a significant challenge. Existing methods employ codebooks or feature compression, causing information loss, thereby degrading segmentation quality. To address this limitation, we introduce Quantile Rendering (Q-Render), a novel rendering strategy for 3D Gaussians that efficiently handles high-dimensional features while maintaining high fidelity. Unlike conventional volume rendering, which densely samples all 3D Gaussians intersecting each ray, Q-Render sparsely samples only those with dominant influence along the ray. By integrating Q-Render into a generalizable 3D neural network, we also propose Gaussian Splatting Network (GS-Net), which predicts Gaussian features in a generalizable manner. Extensive experiments on ScanNet and LeRF demonstrate that our framework outperforms state-of-the-art methods, while enabling real-time rendering with an approximate ~43.7x speedup on 512-D feature maps. Code will be made publicly available.

</details>


### [40] [Transductive Visual Programming: Evolving Tool Libraries from Experience for Spatial Reasoning](https://arxiv.org/abs/2512.20934)
*Shengguang Wu,Xiaohan Wang,Yuhui Zhang,Hao Zhu,Serena Yeung-Levy*

Main category: cs.CV

> Transductive Visual Programming (TVP) is a novel framework that enhances spatial reasoning in 3D scenes through a continuous process of tool creation and refinement based on experience, outperforming existing models in various benchmarks.

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current vision-language models in handling precise geometric calculations in 3D spatial reasoning, by improving the method of visual programming to effectively solve such problems.

**Method:** TVP builds an Example Library through solving problems with basic tools and then creates an evolving Tool Library by abstracting higher-level, reusable tools from the experiential solutions found in the Example Library.

**Result:** TVP demonstrates superior performance relative to other models, including GPT-4o and the previous best visual programming system, on the Omni3D-Bench and SpatialScore-Hard benchmarks.

**Conclusion:** Experience-driven transductive tool creation in TVP provides a robust method for creating evolving visual programming agents capable of addressing complex spatial reasoning tasks with high generalization ability.

**Abstract:** Spatial reasoning in 3D scenes requires precise geometric calculations that challenge vision-language models. Visual programming addresses this by decomposing problems into steps calling specialized tools, yet existing methods rely on either fixed toolsets or speculative tool induction before solving problems, resulting in suboptimal programs and poor utilization of induced tools. We present Transductive Visual Programming (TVP), a novel framework that builds new tools from its own experience rather than speculation. TVP first solves problems using basic tools while accumulating experiential solutions into an Example Library, then abstracts recurring patterns from these programs into reusable higher-level tools for an evolving Tool Library. This allows TVP to tackle new problems with increasingly powerful tools learned from experience. On Omni3D-Bench, TVP achieves state-of-the-art performance, outperforming GPT-4o by 22% and the previous best visual programming system by 11%. Our transductively learned tools are used 5x more frequently as core program dependency than inductively created ones, demonstrating more effective tool discovery and reuse. The evolved tools also show strong generalization to unseen spatial tasks, achieving superior performance on benchmarks from SpatialScore-Hard collection without any testset-specific modification. Our work establishes experience-driven transductive tool creation as a powerful paradigm for building self-evolving visual programming agents that effectively tackle challenging spatial reasoning tasks. We release our code at https://transductive-visualprogram.github.io/.

</details>


### [41] [Reasoning-Driven Amodal Completion: Collaborative Agents and Perceptual Evaluation](https://arxiv.org/abs/2512.20936)
*Hongxing Fan,Shuyu Zhao,Jiayang Ao,Lu Sheng*

Main category: cs.CV

> A novel Collaborative Multi-Agent Reasoning Framework for amodal completion that decouples semantic planning from visual synthesis, ensuring coherent single-pass synthesis and evaluated with a new human-aligned metric, MAC-Score.

<details>
  <summary>Details</summary>

**Motivation:** To overcome the challenges of maintaining semantic consistency and structural integrity in amodal completion while addressing issues of inference instability and error accumulation in progressive approaches.

**Method:** The method involves a Collaborative Multi-Agent Reasoning Framework, where specialized agents are used for semantic planning through upfront reasoning. It includes a Verification Agent for correction in the semantic planning phase and a Diverse Hypothesis Generator for handling ambiguities of invisible regions.

**Result:** The framework outperforms state-of-the-art methods across multiple datasets and includes a new evaluation metric, MAC-Score, which aligns with human judgment for assessing inferred invisible content.

**Conclusion:** The developed framework effectively solves issues of inference instability, error accumulation, and ambiguity management in amodal completion, providing a superior approach in generating coherent visible and invisible object parts.

**Abstract:** Amodal completion, the task of inferring invisible object parts, faces significant challenges in maintaining semantic consistency and structural integrity. Prior progressive approaches are inherently limited by inference instability and error accumulation. To tackle these limitations, we present a Collaborative Multi-Agent Reasoning Framework that explicitly decouples Semantic Planning from Visual Synthesis. By employing specialized agents for upfront reasoning, our method generates a structured, explicit plan before pixel generation, enabling visually and semantically coherent single-pass synthesis. We integrate this framework with two critical mechanisms: (1) a self-correcting Verification Agent that employs Chain-of-Thought reasoning to rectify visible region segmentation and identify residual occluders strictly within the Semantic Planning phase, and (2) a Diverse Hypothesis Generator that addresses the ambiguity of invisible regions by offering diverse, plausible semantic interpretations, surpassing the limited pixel-level variations of standard random seed sampling. Furthermore, addressing the limitations of traditional metrics in assessing inferred invisible content, we introduce the MAC-Score (MLLM Amodal Completion Score), a novel human-aligned evaluation metric. Validated against human judgment and ground truth, these metrics establish a robust standard for assessing structural completeness and semantic consistency with visible context. Extensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods across multiple datasets. Our project is available at: https://fanhongxing.github.io/remac-page.

</details>


### [42] [Beyond Artifacts: Real-Centric Envelope Modeling for Reliable AI-Generated Image Detection](https://arxiv.org/abs/2512.20937)
*Ruiqi Liu,Yi Han,Zhengbo Zhang,Liwei Yao,Zhiyuan Yan,Jialiang Shen,ZhiJin Chen,Boyi Sun,Lubin Weng,Jing Dong,Yan Wang,Shu Wu*

Main category: cs.CV

> REM方法通过自我重构中的特征级扰动生成近似真实样本，并学习包围真实图像流形的边界，解决了现有检测器面对链退化时的不足，在RealChain基准上表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 现有的检测器通常过度拟合于生成器特定的伪影，并且在现实世界退化的情形下变得极其敏感。随着生成架构的发展和图像经历多轮跨平台分享和后处理(链退化)，这些伪影线索变得过时且难以检测，因此需要一种新的方法来解决这个问题。

**Method:** REM提出了一种新的范式，通过在自我重构中引入特征级扰动生成近似真实的样本，并采用跨域一致性包络估计器来学习包围真实图像流形的边界，以实现从学习生成器伪影向建模真实图像鲁棒分布的转换。

**Result:** 通过在REM上的八个基准评估，平均提高了7.5%以上，且在极度退化的RealChain基准上保持了出色的泛化能力，为在现实世界条件下检测合成图像奠定了坚实基础。

**Conclusion:** REM为现实世界条件下检测合成图像提供了一种新颖且有效的方法，通过RealChain基准测量，证明其比现有方法更具鲁棒性和泛化能力。

**Abstract:** The rapid progress of generative models has intensified the need for reliable and robust detection under real-world conditions. However, existing detectors often overfit to generator-specific artifacts and remain highly sensitive to real-world degradations. As generative architectures evolve and images undergo multi-round cross-platform sharing and post-processing (chain degradations), these artifact cues become obsolete and harder to detect. To address this, we propose Real-centric Envelope Modeling (REM), a new paradigm that shifts detection from learning generator artifacts to modeling the robust distribution of real images. REM introduces feature-level perturbations in self-reconstruction to generate near-real samples, and employs an envelope estimator with cross-domain consistency to learn a boundary enclosing the real image manifold. We further build RealChain, a comprehensive benchmark covering both open-source and commercial generators with simulated real-world degradation. Across eight benchmark evaluations, REM achieves an average improvement of 7.5% over state-of-the-art methods, and notably maintains exceptional generalization on the severely degraded RealChain benchmark, establishing a solid foundation for synthetic image detection under real-world conditions. The code and the RealChain benchmark will be made publicly available upon acceptance of the paper.

</details>
