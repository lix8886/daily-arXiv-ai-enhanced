{"id": "2510.08589", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08589", "abs": "https://arxiv.org/abs/2510.08589", "authors": ["Nirmal Elamon", "Rouzbeh Davoudi"], "title": "Beyond CNNs: Efficient Fine-Tuning of Multi-Modal LLMs for Object Detection on Low-Data Regimes", "comment": null, "summary": "The field of object detection and understanding is rapidly evolving, driven\nby advances in both traditional CNN-based models and emerging multi-modal large\nlanguage models (LLMs). While CNNs like ResNet and YOLO remain highly effective\nfor image-based tasks, recent transformer-based LLMs introduce new capabilities\nsuch as dynamic context reasoning, language-guided prompts, and holistic scene\nunderstanding. However, when used out-of-the-box, the full potential of LLMs\nremains underexploited, often resulting in suboptimal performance on\nspecialized visual tasks. In this work, we conduct a comprehensive comparison\nof fine-tuned traditional CNNs, zero-shot pre-trained multi-modal LLMs, and\nfine-tuned multi-modal LLMs on the challenging task of artificial text overlay\ndetection in images. A key contribution of our study is demonstrating that LLMs\ncan be effectively fine-tuned on very limited data (fewer than 1,000 images) to\nachieve up to 36% accuracy improvement, matching or surpassing CNN-based\nbaselines that typically require orders of magnitude more data. By exploring\nhow language-guided models can be adapted for precise visual understanding with\nminimal supervision, our work contributes to the broader effort of bridging\nvision and language, offering novel insights into efficient cross-modal\nlearning strategies. These findings highlight the adaptability and data\nefficiency of LLM-based approaches for real-world object detection tasks and\nprovide actionable guidance for applying multi-modal transformers in\nlow-resource visual environments. To support continued progress in this area,\nwe have made the code used to fine-tune the models available in our GitHub,\nenabling future improvements and reuse in related applications.", "AI": {"tldr": "研究展示了通过少量数据微调多模态LLM可显著提升图像中人工文本叠加检测的精度，并展示了这种方法的数据效率和适应性。", "motivation": "探索语言引导模型在需要少量监督的精确视觉理解任务中的应用，并展示多模态LLM的潜力。", "method": "比较了微调的传统CNN、零样本预训练多模态LLM和微调的多模态LLM在图像中人工文本叠加检测任务上的表现。", "result": "在少量数据（少于1000张图像）微调下，LLM的精度提升了多达36%，与需要大量数据的传统CNN基线相比不相上下或超越。", "conclusion": "多模态LLM方法在真实世界对象检测任务中展现出了高度的数据效率和适应性，为在资源匮乏的视觉环境中应用多模态变换器提供了实用指南。"}}
{"id": "2510.08617", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08617", "abs": "https://arxiv.org/abs/2510.08617", "authors": ["Saumya B"], "title": "Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation", "comment": "Code and results available at\n  https://github.com/Saumya4321/2d-brain-tumor-segmentation", "summary": "Brain tumor segmentation is crucial for diagnosis and treatment planning, yet\nchallenges such as class imbalance and limited model generalization continue to\nhinder progress. This work presents a reproducible evaluation of U-Net\nsegmentation performance on brain tumor MRI using focal loss and basic data\naugmentation strategies. Experiments were conducted on a publicly available MRI\ndataset, focusing on focal loss parameter tuning and assessing the impact of\nthree data augmentation techniques: horizontal flip, rotation, and scaling. The\nU-Net with focal loss achieved a precision of 90%, comparable to\nstate-of-the-art results. By making all code and results publicly available,\nthis study establishes a transparent, reproducible baseline to guide future\nresearch on augmentation strategies and loss function design in brain tumor\nsegmentation.", "AI": {"tldr": "本文通过使用焦点损失(focal loss)和基本数据扩增策略对U-Net在脑肿瘤MRI图像分割性能进行评估，对焦点损失参数进行调优，并评估了三种数据增广技术的影响。实验结果表明U-Net配合焦点损失达到了90%的精度，与最先进的结果相当。", "motivation": "由于类别不平衡和模型泛化能力有限等挑战，脑肿瘤分割的研究遇到了阻碍。本文试图通过可重复的方式评估U-Net模型在使用焦点损失和数据扩增策略时的性能，为未来的脑肿瘤分割研究提供一个透明的研究基准。", "method": "本文使用U-Net进行脑肿瘤分割，结合使用焦点损失和数据扩增方法如水平翻转、旋转和缩放。实验在公开的MRI数据集上进行。", "result": "实验结果表明，使用焦点损失的U-Net模型达到了90%的精度，这一结果与当前最先进的研究水平相当。", "conclusion": "本文通过透明且可重复的方式建立了U-Net在脑肿瘤分割研究中的基准，尤其在数据扩增策略和损失函数设计方面，为未来的研究提供了指导意义。"}}
{"id": "2510.08625", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08625", "abs": "https://arxiv.org/abs/2510.08625", "authors": ["Hyeonggeun Han", "Sehwan Kim", "Hyungjun Joo", "Sangwoo Hong", "Jungwoo Lee"], "title": "Adjusting Initial Noise to Mitigate Memorization in Text-to-Image Diffusion Models", "comment": null, "summary": "Despite their impressive generative capabilities, text-to-image diffusion\nmodels often memorize and replicate training data, prompting serious concerns\nover privacy and copyright. Recent work has attributed this memorization to an\nattraction basin-a region where applying classifier-free guidance (CFG) steers\nthe denoising trajectory toward memorized outputs-and has proposed deferring\nCFG application until the denoising trajectory escapes this basin. However,\nsuch delays often result in non-memorized images that are poorly aligned with\nthe input prompts, highlighting the need to promote earlier escape so that CFG\ncan be applied sooner in the denoising process. In this work, we show that the\ninitial noise sample plays a crucial role in determining when this escape\noccurs. We empirically observe that different initial samples lead to varying\nescape times. Building on this insight, we propose two mitigation strategies\nthat adjust the initial noise-either collectively or individually-to find and\nutilize initial samples that encourage earlier basin escape. These approaches\nsignificantly reduce memorization while preserving image-text alignment.", "AI": {"tldr": "研究发现通过调整初始噪声可以使文本转图像扩散模型早期脱离吸引盆地，从而减少记忆训练数据，并保持图像与文本的一致性。", "motivation": "文本到图像扩散模型经常复制训练数据，这引发对隐私和版权的担忧。最近的研究将其归因于吸引盆地，在这个区域内，无类别引导将去噪轨迹导向记忆输出。延迟应用无类别引导会导致图像与输入提示不匹配，因此需要找到一种促进快速脱离盆地的方法，以便稍后应用无类别引导。", "method": "本研究提出了两种调整初始噪声的方法，旨在帮助扩散模型更快地脱离吸引盆地，从而减少对训练数据的记忆效应。这两种方法可以集体或个别地调整初始噪声，以便找到鼓励早期脱离盆地的样本。", "result": "研究发现，不同的初始噪声样本会导致不同的脱离时间，并提出的方法显著减少了记忆效应，同时保持了图像与文本的对齐。", "conclusion": "通过调整初始噪声来促进扩散过程中的早期脱离盆地，可以有效地减少文本到图像扩散模型的记忆效应，同时保持生成图像与输入文本的一致性。"}}
{"id": "2510.08588", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08588", "abs": "https://arxiv.org/abs/2510.08588", "authors": ["Ritesh Mehta"], "title": "Enhancing Biomedical Named Entity Recognition using GLiNER-BioMed with Targeted Dictionary-Based Post-processing for BioASQ 2025 task 6", "comment": "Paper published to CLEF 2025 CEUR-WS", "summary": "Biomedical Named Entity Recognition (BioNER), task6 in BioASQ (A challenge in\nlarge-scale biomedical semantic indexing and question answering), is crucial\nfor extracting information from scientific literature but faces hurdles such as\ndistinguishing between similar entity types like genes and chemicals. This\nstudy evaluates the GLiNER-BioMed model on a BioASQ dataset and introduces a\ntargeted dictionary-based post-processing strategy to address common\nmisclassifications. While this post-processing approach demonstrated notable\nimprovement on our development set, increasing the micro F1-score from a\nbaseline of 0.79 to 0.83, this enhancement did not generalize to the blind test\nset, where the post-processed model achieved a micro F1-score of 0.77 compared\nto the baselines 0.79. We also discuss insights gained from exploring\nalternative methodologies, including Conditional Random Fields. This work\nhighlights the potential of dictionary-based refinement for pre-trained BioNER\nmodels but underscores the critical challenge of overfitting to development\ndata and the necessity of ensuring robust generalization for real-world\napplicability.", "AI": {"tldr": "研究提出了针对GLiNER-BioMed模型的后处理策略，在开发集上表现改善，但在测试集上未能保持效果，强调了不过度拟合开发数据的重要性。", "motivation": "由于生物医学命名实体识别（BioNER）在从科学文献中提取信息方面至关重要，但面临着区分基因和化学物质等相似实体类型等挑战，因此提出此项研究。", "method": "研究评估了GLiNER-BioMed模型在BioASQ数据集上的性能，并提出了一种基于目标字典的后处理策略来解决常见的错误分类问题。", "result": "后处理方法在开发数据集上显著提高了微F1分数，从基线的0.79增加到0.83，但在盲测数据集中，后处理模型的微F1分数下降到0.77，对比基线无明显改善。", "conclusion": "这一工作突显了基于字典的精调对于预训练BioNER模型的巨大潜力，但同时也强调了避免过度拟合到开发数据并保证模型稳健泛化至现实世界应用的重要性。"}}
{"id": "2510.08628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08628", "abs": "https://arxiv.org/abs/2510.08628", "authors": ["Siiri Leppälampi", "Sonja M. Hyrynsalmi", "Erno Vanhala"], "title": "The Digital Mirror: Gender Bias and Occupational Stereotypes in AI-Generated Images", "comment": null, "summary": "Generative AI offers vast opportunities for creating visualisations, such as\ngraphics, videos, and images. However, recent studies around AI-generated\nvisualisations have primarily focused on the creation process and image\nquality, overlooking representational biases. This study addresses this gap by\ntesting representation biases in AI-generated pictures in an occupational\nsetting and evaluating how two AI image generator tools, DALL-E 3 and Ideogram,\ncompare. Additionally, the study discusses topics such as ageing and emotions\nin AI-generated images. As AI image tools are becoming more widely used,\naddressing and mitigating harmful gender biases becomes essential to ensure\ndiverse representation in media and professional settings. In this study, over\n750 AI-generated images of occupations were prompted. The thematic analysis\nresults revealed that both DALL-E 3 and Ideogram reinforce traditional gender\nstereotypes in AI-generated images, although to varying degrees. These findings\nemphasise that AI visualisation tools risk reinforcing narrow representations.\nIn our discussion section, we propose suggestions for practitioners,\nindividuals and researchers to increase representation when generating images\nwith visible genders.", "AI": {"tldr": "研究比较了DALL-E 3和Ideogram两种AI工具在职业场景中的表现偏差情况，发现它们都存在不同程度地强化性别刻板印象的风险，研究提出了有关如何增加性别多样性的建议。", "motivation": "此研究旨在弥补当前关于AI生成视觉内容研究中的一个空白，即忽视了表现偏差问题。研究关注于职业场景中的AI生成图片，并特别讨论了AI生成图像中关于老化和情绪的问题。随着AI图像工具的广泛使用，研究强调了必须解决和缓解这些有害的性别偏见，以确保在媒体和专业环境中实现多样化的表现。", "method": "研究通过比较两种AI图像生成工具DALL-E 3和Ideogram，测试了AI生成图片中表现偏差的存在。研究者们生成了超过750张职业场景的AI图像并进行了主题分析。", "result": "主题分析显示，DALL-E 3和Ideogram都在不同程度上强化了传统性别刻板印象。", "conclusion": "研究结果强调了AI可视化工具可能持续强化狭窄表现的风险。在讨论部分，研究者提出了建议，以帮助从业者、个人和研究者在生成具有可见性别的图像时增加代表性。"}}
{"id": "2510.08592", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08592", "abs": "https://arxiv.org/abs/2510.08592", "authors": ["Shahriar Kabir Nahin", "Hadi Askari", "Muhao Chen", "Anshuman Chhabra"], "title": "Less Diverse, Less Safe: The Indirect But Pervasive Risk of Test-Time Scaling in Large Language Models", "comment": null, "summary": "Test-Time Scaling (TTS) improves LLM reasoning by exploring multiple\ncandidate responses and then operating over this set to find the best output. A\ntacit premise behind TTS is that sufficiently diverse candidate pools enhance\nreliability. In this work, we show that this assumption in TTS introduces a\npreviously unrecognized failure mode. When candidate diversity is curtailed,\neven by a modest amount, TTS becomes much more likely to produce unsafe\noutputs. We present a reference-guided diversity reduction protocol (RefDiv)\nthat serves as a diagnostic attack to stress test TTS pipelines. Through\nextensive experiments across four open-source models (Qwen3, Mistral, Llama3.1,\nGemma3) and two widely used TTS strategies (Monte Carlo Tree Search and\nBest-of-N), constraining diversity consistently signifies the rate at which TTS\nproduces unsafe results. The effect is often stronger than that produced by\nprompts directly with high adversarial intent scores. This observed phenomenon\nalso transfers across TTS strategies and to closed-source models (e.g. OpenAI\no3 and Gemini-2.5-Pro), thus indicating that this is a general and extant\nproperty of TTS rather than a model-specific artifact. Additionally, we find\nthat numerous widely used safety guardrail classifiers (e.g. Llama-Guard and\nOpenAI Moderation API), are unable to flag the adversarial input prompts\ngenerated by RefDiv, demonstrating that existing defenses offer limited\nprotection against this diversity-driven failure mode. Through this work, we\nhope to motivate future research on designing robust TTS strategies that are\nboth effective and secure against diversity-targeted stress tests as\nillustrated by RefDiv.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.08629", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08629", "abs": "https://arxiv.org/abs/2510.08629", "authors": ["Jort Vincenti", "Metod Jazbec", "Guoxuan Xia"], "title": "Dynamic Mixture-of-Experts for Visual Autoregressive Model", "comment": null, "summary": "Visual Autoregressive Models (VAR) offer efficient and high-quality image\ngeneration but suffer from computational redundancy due to repeated Transformer\ncalls at increasing resolutions. We introduce a dynamic Mixture-of-Experts\nrouter integrated into VAR. The new architecture allows to trade compute for\nquality through scale-aware thresholding. This thresholding strategy balances\nexpert selection based on token complexity and resolution, without requiring\nadditional training. As a result, we achieve 20% fewer FLOPs, 11% faster\ninference and match the image quality achieved by the dense baseline.", "AI": {"tldr": "通过集成动态专家混合路由机制到视觉自回归（VAR）模型，我们通过权衡计算量与图像生成质量，实现了更高的效率和相近的图像质量。", "motivation": "虽然视觉自回归模型提供了高效的图像生成，但是它们因在增加分辨率时重复调用变换器而存在计算冗余的问题。为此，我们提出了一个解决方案来减少这种冗余。", "method": "我们引入了一种集成到视觉自回归模型（VAR）中的动态专家混合路由机制。该新架构通过尺度感知阈值策略在计算量与生成质量之间进行权衡，这种策略根据token复杂度和分辨率来平衡专家选择，且不需要额外训练。", "result": "最终，我们实现了20%的FLOPs减少，推理速度提高了11%，并且达到了与密集基线相同水平的图像质量。", "conclusion": "通过引入尺度感知阈值策略，我们有效减少计算冗余，加速了图像生成过程，同时保持了高图像质量标准，展示了该方法在提升图像生成效率方面的潜力。"}}
{"id": "2510.08593", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.08593", "abs": "https://arxiv.org/abs/2510.08593", "authors": ["Yuxin Li", "Eng Siong Chng", "Cuntai Guan"], "title": "Hierarchical Self-Supervised Representation Learning for Depression Detection from Speech", "comment": null, "summary": "Speech-based depression detection (SDD) is a promising, non-invasive\nalternative to traditional clinical assessments. However, it remains limited by\nthe difficulty of extracting meaningful features and capturing sparse,\nheterogeneous depressive cues over time. Pretrained self-supervised learning\n(SSL) models such as WavLM provide rich, multi-layer speech representations,\nyet most existing SDD methods rely only on the final layer or search for a\nsingle best-performing one. These approaches often overfit to specific datasets\nand fail to leverage the full hierarchical structure needed to detect subtle\nand persistent depression signals.\n  To address this challenge, we propose HAREN-CTC, a novel architecture that\nintegrates multi-layer SSL features using cross-attention within a multitask\nlearning framework, combined with Connectionist Temporal Classification loss to\nhandle sparse temporal supervision. HAREN-CTC comprises two key modules: a\nHierarchical Adaptive Clustering module that reorganizes SSL features into\ncomplementary embeddings, and a Cross-Modal Fusion module that models\ninter-layer dependencies through cross-attention. The CTC objective enables\nalignment-aware training, allowing the model to track irregular temporal\npatterns of depressive speech cues.\n  We evaluate HAREN-CTC under both an upper-bound setting with standard data\nsplits and a generalization setting using five-fold cross-validation. The model\nachieves state-of-the-art macro F1-scores of 0.81 on DAIC-WOZ and 0.82 on\nMODMA, outperforming prior methods across both evaluation scenarios.", "AI": {"tldr": "HAREN-CTC, a novel speech-based depression detection system using multi-layer SSL, cross-attention, and CTC loss, achieves state-of-the-art results on standard datasets.", "motivation": "The motivation stems from the limitations of existing speech-based depression detection methods, which are restricted by difficulty in extracting meaningful features and capturing sparse depressive cues over time.", "method": "The paper introduces HAREN-CTC, an architecture that integrates multi-layer SSL features via cross-attention in a multitask framework, along with CTC loss for sparse temporal supervision. It has two main components: a Hierarchical Adaptive Clustering module and a Cross-Modal Fusion module.", "result": "In evaluations, HAREN-CTC achieved the best macro F1-scores of 0.81 on DAIC-WOZ and 0.82 on MODMA, outperforming past approaches in both data splits and five-fold cross-validation settings.", "conclusion": "The research concludes on the effectiveness of their proposed HAREN-CTC model in overcoming the limitations of current methods, demonstrating superior performance in detecting depression through speech analysis."}}
{"id": "2510.08631", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08631", "abs": "https://arxiv.org/abs/2510.08631", "authors": ["Hanieh Shojaei Miandashti", "Claus Brenner"], "title": "Out-of-Distribution Detection in LiDAR Semantic Segmentation Using Epistemic Uncertainty from Hierarchical GMMs", "comment": null, "summary": "In addition to accurate scene understanding through precise semantic\nsegmentation of LiDAR point clouds, detecting out-of-distribution (OOD)\nobjects, instances not encountered during training, is essential to prevent the\nincorrect assignment of unknown objects to known classes. While supervised OOD\ndetection methods depend on auxiliary OOD datasets, unsupervised methods avoid\nthis requirement but typically rely on predictive entropy, the entropy of the\npredictive distribution obtained by averaging over an ensemble or multiple\nposterior weight samples. However, these methods often conflate epistemic\n(model) and aleatoric (data) uncertainties, misclassifying ambiguous in\ndistribution regions as OOD. To address this issue, we present an unsupervised\nOOD detection approach that employs epistemic uncertainty derived from\nhierarchical Bayesian modeling of Gaussian Mixture Model (GMM) parameters in\nthe feature space of a deep neural network. Without requiring auxiliary data or\nadditional training stages, our approach outperforms existing uncertainty-based\nmethods on the SemanticKITTI dataset, achieving an 18\\% improvement in AUROC,\n22\\% increase in AUPRC, and 36\\% reduction in FPR95 (from 76\\% to 40\\%),\ncompared to the predictive entropy approach used in prior works.", "AI": {"tldr": "本文提出了一种新的无监督的OOD检测方法，该方法能够明确地区分模型不确定性和数据不确定性，而无需额外的数据集或训练阶段，显著提升在SemanticKITTI数据集上的检测性能。", "motivation": "该论文的动机是解决现有的无监督检测方法通常将模型不确定性和数据不确定性混合，导致对歧义的仪器内区域误分类为OOD问题。此外，本研究旨在不依赖辅助数据集的前提下，提升模型检测OOD的性能。", "method": "我们的方法是使用从深度神经网络特征空间中的高斯混合模型（GMM）参数的层次贝叶斯建模中得出的模型不确定性来进行无监督的OOD检测。与依赖于预测熵的方法不同，我们的方法能够区分模型不确定性（贝叶斯不确定性）和数据不确定性（数据散度），这样可以更准确地识别出分布式外的数据。", "result": "与先前工作中使用的预测熵方法相比，我们的方法在SemanticKITTI数据集上实现了18%的AUROC改进，22%的AUPRC增加，以及36%的FPR95减少（从76%降至40%）。", "conclusion": "结论是，该方法通过区分模型不确定性和数据不确定性，在未见过的数据检测上取得了显著的性能提升，显示出在自动驾驶等场景中的应用潜力。"}}
{"id": "2510.08595", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08595", "abs": "https://arxiv.org/abs/2510.08595", "authors": ["V. S. Raghu Parupudi"], "title": "Systematic Diagnosis of Brittle Reasoning in Large Language Models", "comment": "Submitted to NEURIPS-2025 MATHAI workshop", "summary": "A central question in artificial intelligence is the extent to which machine\nlearning models comprehend mathematics. To address this, we propose a novel\nframework for measuring mathematical reasoning that moves beyond standard\nbenchmarks to diagnose specific failure points. Our method first generates\nstructured, step-by-step reasoning from gpt-3.5-turbo on the GSM8K dataset. We\nthen use a more capable analyst model, gpt-4o-mini, to categorize errors and,\ncrucially, perform an unsupervised clustering of every reasoning sentence to\nidentify emergent \"reasoning modes.\" This analysis reveals a cognitive profile\nwith a stark, nonhuman-like brittleness: while the model achieves near-perfect\naccuracy on procedural modes like sequential calculation, its performance on\nmodes requiring combinatorial reasoning with restrictions plummets. By\nidentifying and quantifying the reliability of these distinct reasoning skills,\nour work provides a more granular method to evaluate mathematical comprehension\nand offers a precise roadmap for developing new capabilities and more reliable\nfuture applications.", "AI": {"tldr": "研究提出一种框架评估AI模型在数学推理上的实际能力，发现模型在特定的推理模式上表现出非人类般的脆弱性，并提供了开发更可靠AI应用的指引。", "motivation": "研究人工智能中机器学习模型理解和处理数学问题的能力，特别是想要了解模型在不同数学推理类型上的表现和弱点。", "method": "本研究提出了一种新的数学推理衡量框架，该框架超越了标准基准，能够诊断具体的失败点。首先，通过gpt-3.5-turbo对GSM8K数据集进行结构化的、逐步推理的生成。然后，使用更强大的分析模型gpt-4o-mini对错误进行分类，并对每个推理句子进行无监督聚类分析，以识别出现的“推理模式”。", "result": "分析揭示了一种非人类般的认知脆弱性：模型在顺序计算等程序模式上表现出近乎完美的准确率，但在需要组合推理和限制的问题上则表现得很差。", "conclusion": "本工作提供了一种更细致的方法来评估数学理解能力，为开发新的能力和未来的可靠应用提供了精准的路线图。"}}
{"id": "2510.08635", "categories": ["cs.CV", "cs.AI", "I.2"], "pdf": "https://arxiv.org/pdf/2510.08635", "abs": "https://arxiv.org/abs/2510.08635", "authors": ["Conor McCarthy", "Loes Quirijnen", "Jan Peter van Zandwijk", "Zeno Geradts", "Marcel Worring"], "title": "Hi-OSCAR: Hierarchical Open-set Classifier for Human Activity Recognition", "comment": "Accepted at ACM on Interactive, Mobile, Wearable and Ubiquitous\n  Technologies (IMWUT)", "summary": "Within Human Activity Recognition (HAR), there is an insurmountable gap\nbetween the range of activities performed in life and those that can be\ncaptured in an annotated sensor dataset used in training. Failure to properly\nhandle unseen activities seriously undermines any HAR classifier's reliability.\nAdditionally within HAR, not all classes are equally dissimilar, some\nsignificantly overlap or encompass other sub-activities. Based on these\nobservations, we arrange activity classes into a structured hierarchy. From\nthere, we propose Hi-OSCAR: a Hierarchical Open-set Classifier for Activity\nRecognition, that can identify known activities at state-of-the-art accuracy\nwhile simultaneously rejecting unknown activities. This not only enables\nopen-set classification, but also allows for unknown classes to be localized to\nthe nearest internal node, providing insight beyond a binary \"known/unknown\"\nclassification. To facilitate this and future open-set HAR research, we\ncollected a new dataset: NFI_FARED. NFI_FARED contains data from multiple\nsubjects performing nineteen activities from a range of contexts, including\ndaily living, commuting, and rapid movements, which is fully public and\navailable for download.", "AI": {"tldr": "提出了一个层级开放设置分类器（Hi-OSCAR）以识别已知活动并拒绝未知活动，并构建了一个新的数据集NFI_FARED来支持这项研究。", "motivation": "解决日常生活活动与训练集中标注的传感器数据活动之间的不可逾越差距，同时考虑到HAR（人类活动识别）中各类活动之间的相似性。", "method": "通过构建活动类别的层级结构来处理已知和未知活动的识别问题，并提出Hi-OSCAR（层级开放设置分类器用于活动识别）以实现状态-of-the-art精确识别已知活动，同时拒绝未知活动。", "result": "达到了已知活动识别的状态-of-the-art准确率，同时可以将未知活动定位到最接近的内部节点。", "conclusion": "Hi-OSCAR不仅实现了开放集合分类，而且还通过收集的新数据集（NFI_FARED）推动了开放设置HAR研究的未来。"}}
{"id": "2510.08596", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08596", "abs": "https://arxiv.org/abs/2510.08596", "authors": ["V. S. Raghu Parupudi"], "title": "Confidence, Not Perplexity: A Better Metric for the Creative Era of LLMs", "comment": "Submitted to AACL-IJCNLP 2025 (Eval4NLP)", "summary": "Reference-free metrics like self-perplexity are strongly biased against\ncreative text generation. We propose the Confidence Score (CS), derived from a\nmodel's output probability distribution, as a less biased alternative.\nExperiments on gpt-4o-mini show that while fluency-based metrics prefer novel\nresponses in 0\\% of cases on 99 creative prompts, our CS does so 19% of the\ntime, a statistically significant difference (95% CI for difference: [11.1%,\n27.3%]). We also show that CS effectively distinguishes between easy, medium,\nand hard tasks, confirmed by non-overlapping confidence intervals. The\nConfidence Score thus mitigates the creativity bias of traditional metrics\nwhile retaining their core evaluative strengths, offering a more balanced\nassessment for modern LLMs.", "AI": {"tldr": "研究提出了一种新的评价指标——置信度分数(CS)，它能更客观地评估生成文本的创造性，特别是在鉴别新奇响应方面优于现有指标。", "motivation": "现有的无参考评价指标，如自困惑度(self-perplexity)，存在对创新文本生成的偏差。研究动机在于寻找一种更为公正的评价方式。", "method": "提出了一种基于模型输出概率分布的置信度分数(Confidence Score, CS)，作为评估生成文本的替代指标。", "result": "实验结果显示，相较于传统的基于流畅度的评价指标，CS在99个创意提示中能更频繁地偏爱新颖的响应（19% vs 0%），且该差异具有统计学意义（95%置信区间：[11.1%，27.3%]）。CS还可以有效区分不同难度的任务。", "conclusion": "CS解决了传统评估指标的创新偏见问题，同时保留了评估的主要优点，为现代语言模型提供了更平衡的评估。"}}
{"id": "2510.08637", "categories": ["cs.CV", "physics.med-ph", "94A12, 62H30, 68T10", "I.5.4; I.4.7; J.3"], "pdf": "https://arxiv.org/pdf/2510.08637", "abs": "https://arxiv.org/abs/2510.08637", "authors": ["Mostafa Mohammadpour", "Mehdi Zekriyapanah Gashti", "Yusif S. Gasimov"], "title": "Detection of high-frequency oscillations using time-frequency analysis", "comment": "17 pages, 7 figures", "summary": "High-frequency oscillations (HFOs) are a new biomarker for identifying the\nepileptogenic zone. Mapping HFO-generating regions can improve the precision of\nresection sites in patients with refractory epilepsy. However, detecting HFOs\nremains challenging, and their clinical features are not yet fully defined.\nVisual identification of HFOs is time-consuming, labor-intensive, and\nsubjective. As a result, developing automated methods to detect HFOs is\ncritical for research and clinical use. In this study, we developed a novel\nmethod for detecting HFOs in the ripple and fast ripple frequency bands (80-500\nHz). We validated it using both controlled datasets and data from epilepsy\npatients. Our method employs an unsupervised clustering technique to categorize\nevents extracted from the time-frequency domain using the S-transform. The\nproposed detector differentiates HFOs events from spikes, background activity,\nand artifacts. Compared to existing detectors, our method achieved a\nsensitivity of 97.67%, a precision of 98.57%, and an F-score of 97.78% on the\ncontrolled dataset. In epilepsy patients, our results showed a stronger\ncorrelation with surgical outcomes, with a ratio of 0.73 between HFOs rates in\nresected versus non-resected contacts. The study confirmed previous findings\nthat HFOs are promising biomarkers of epileptogenicity in epileptic patients.\nRemoving HFOs, especially fast ripple, leads to seizure freedom, while\nremaining HFOs lead to seizure recurrence.", "AI": {"tldr": "A novel method for detecting HFOs (High-frequency oscillations) using unsupervised clustering and S-transform shows high sensitivity, precision, and F-score, correlating positively with surgical outcomes in epilepsy patients.", "motivation": "HFOs are critical biomarkers for the epileptogenic zone but are challenging to detect. This research aims to develop an automated method for detecting HFOs in different frequency bands for clinical use.", "method": "We validated our novel HFO detection method using both controlled datasets and patient data. The method uses unsupervised clustering with S-transform to distinguish HFOs from other activities.", "result": "Our method achieved high sensitivity (97.67%), precision (98.57%), and an F-score of 97.78%. In epilepsy patients, a strong correlation (ratio of 0.73) with surgical outcomes was found.", "conclusion": "The study confirms that removing HFOs, particularly fast ripples, correlates with seizure freedom, validating HFOs as biomarkers for epileptogenicity."}}
{"id": "2510.08600", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08600", "abs": "https://arxiv.org/abs/2510.08600", "authors": ["Devleena Das", "Rajeev Patwari", "Ashish Sirasao"], "title": "Recover-LoRA: Data-Free Accuracy Recovery of Degraded Language Models via Low-Rank Adaptation", "comment": "Accepted to EMNLP 2025 Industry Track", "summary": "Inference optimizations such as quantization, pruning, format and datatype\nconversion, model export, and serialization can lead to functional degradations\nin language model task performance. While most efforts on performance recovery\nfor deployment focus on robust quantization techniques, we focus on recovering\nmodel accuracies from any sources that degrade model weights, such as improper\nmodel serialization. In this work, we propose Recover-LoRA, a lightweight and\ndataset agnostic method to recover accuracy in degraded models. Recover-LoRA\nuses synthetic data and logit distillation to learn LoRA adapters on selective\nlayers that facilitate aligning the degraded model to its full precision model.\nWe investigate the utility of Recover-LoRA across a diverse set of small\nlanguage models (SLMs), including models with varying attention architectures,\nmulti-head attention (MHA) and group-query attention (GQA), as well as several\nevaluation datasets. Our results show that Recover-LoRA recovers model\naccuracies by 5-17% on MHA and GQA SLMs.", "AI": {"tldr": "提出了Recover-LoRA方法，通过合成数据和logit蒸馏恢复小语言模型的性能。", "motivation": "论文旨在解决由模型序列化不当等来源引起的模型权重功能退化的问题，而不仅仅是集中在健壮的量化技术上来恢复部署性能。", "method": "Recover-LoRA是使用合成数据和logit蒸馏来在选择的层上学习LoRA适配器，从而将退化的模型与其全精度模型对齐的轻量级且数据集无关的方法。", "result": "实验证明Recover-LoRA在多项评估数据集上对不同的小型语言模型恢复了模型准确度，提高了5-17%。", "conclusion": "研究表明，Recover-LoRA在多头注意力(MHA)和分组查询注意力(GQA)的小型语言模型(SLMs)中恢复了5-17%的模型准确性。"}}
{"id": "2510.08638", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08638", "abs": "https://arxiv.org/abs/2510.08638", "authors": ["Thomas Fel", "Binxu Wang", "Michael A. Lepori", "Matthew Kowal", "Andrew Lee", "Randall Balestriero", "Sonia Joseph", "Ekdeep S. Lubana", "Talia Konkle", "Demba Ba", "Martin Wattenberg"], "title": "Into the Rabbit Hull: From Task-Relevant Concepts in DINO to Minkowski Geometry", "comment": null, "summary": "DINOv2 is routinely deployed to recognize objects, scenes, and actions; yet\nthe nature of what it perceives remains unknown. As a working baseline, we\nadopt the Linear Representation Hypothesis (LRH) and operationalize it using\nSAEs, producing a 32,000-unit dictionary that serves as the interpretability\nbackbone of our study, which unfolds in three parts.\n  In the first part, we analyze how different downstream tasks recruit concepts\nfrom our learned dictionary, revealing functional specialization:\nclassification exploits \"Elsewhere\" concepts that fire everywhere except on\ntarget objects, implementing learned negations; segmentation relies on boundary\ndetectors forming coherent subspaces; depth estimation draws on three distinct\nmonocular depth cues matching visual neuroscience principles.\n  Following these functional results, we analyze the geometry and statistics of\nthe concepts learned by the SAE. We found that representations are partly dense\nrather than strictly sparse. The dictionary evolves toward greater coherence\nand departs from maximally orthogonal ideals (Grassmannian frames). Within an\nimage, tokens occupy a low dimensional, locally connected set persisting after\nremoving position. These signs suggest representations are organized beyond\nlinear sparsity alone.\n  Synthesizing these observations, we propose a refined view: tokens are formed\nby combining convex mixtures of archetypes (e.g., a rabbit among animals, brown\namong colors, fluffy among textures). This structure is grounded in Gardenfors'\nconceptual spaces and in the model's mechanism as multi-head attention produces\nsums of convex mixtures, defining regions bounded by archetypes. We introduce\nthe Minkowski Representation Hypothesis (MRH) and examine its empirical\nsignatures and implications for interpreting vision-transformer\nrepresentations.", "AI": {"tldr": "本文通过对DINOv2模型部署不同下游任务时感知内容的探究，深入分析了模型内部的线性表示假设（LRH）机制，并提出了一种新的表示假设（MRH），展示了视觉变换器模型中词典表示的特征和结构。", "motivation": "本文旨在揭示DINOv2这样的视觉模型所感知内容的本质，尤其是在不同任务下的表现。通过探究模型内部的概念表示方式，希望能理解视觉模型如何处理和解释视觉信息。", "method": "本文采用了线性表示假设（LRH）作为工作基线，并使用SAEs（稀疏自动编码器）实现这一假设，创建了一个包含32,000个单元的词典以作为研究的可解释性基础。研究分为三个部分，首先是分析不同下游任务如何从学到的词典中调用概念，其次是分析词典的概念结构和统计特性，最后是提出一种新的表示假设（Minkowski Representation Hypothesis, MRH）并探讨其实证特征和对未来理解视觉变换器表示的意义。", "result": "研究揭示了代表性的功能特殊化，以及在逻辑结构上有别于先前的线性稀疏表示的观点，呈现出一种新的表示结构，证实词典中元素在局部具有连接性且部分表示是密集化的，而不是严格的稀疏化。", "conclusion": "本文提出了Minkowski表示假设（MRH）来更精确地描述DINOv2的词典表示方式，这表明模型的表示形式是由各种原型的凸组合构成，这种结构基于概念空间和模型中多头注意力机制产生凸组合和定义由原型界定的区域的思想。"}}
{"id": "2510.08601", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.08601", "abs": "https://arxiv.org/abs/2510.08601", "authors": ["Aneesh Jonelagadda", "Christina Hahn", "Haoze Zheng", "Salvatore Penachio"], "title": "Mnemosyne: An Unsupervised, Human-Inspired Long-Term Memory Architecture for Edge-Based LLMs", "comment": "12 pages, 4 figures", "summary": "Long-term memory is essential for natural, realistic dialogue. However,\ncurrent large language model (LLM) memory systems rely on either brute-force\ncontext expansion or static retrieval pipelines that fail on edge-constrained\ndevices. We introduce Mnemosyne, an unsupervised, human-inspired long-term\nmemory architecture designed for edge-based LLMs. Our approach uses\ngraph-structured storage, modular substance and redundancy filters, memory\ncommitting and pruning mechanisms, and probabilistic recall with temporal decay\nand refresh processes modeled after human memory. Mnemosyne also introduces a\nconcentrated \"core summary\" efficiently derived from a fixed-length subset of\nthe memory graph to capture the user's personality and other domain-specific\nlong-term details such as, using healthcare application as an example,\npost-recovery ambitions and attitude towards care. Unlike existing\nretrieval-augmented methods, Mnemosyne is designed for use in longitudinal\nhealthcare assistants, where repetitive and semantically similar but temporally\ndistinct conversations are limited by naive retrieval. In experiments with\nlongitudinal healthcare dialogues, Mnemosyne demonstrates the highest win rate\nof 65.8% in blind human evaluations of realism and long-term memory capability\ncompared to a baseline RAG win rate of 31.1%. Mnemosyne also achieves current\nhighest LoCoMo benchmark scores in temporal reasoning and single-hop retrieval\ncompared to other same-backboned techniques. Further, the average overall score\nof 54.6% was second highest across all methods, beating commonly used Mem0 and\nOpenAI baselines among others. This demonstrates that improved factual recall,\nenhanced temporal reasoning, and much more natural user-facing responses can be\nfeasible with an edge-compatible and easily transferable unsupervised memory\narchitecture.", "AI": {"tldr": "The paper introduces Mnemosyne, an unsupervised, human-inspired long-term memory architecture for edge-based language models, showing superior performance in longitudinal healthcare dialogues compared to existing methods.", "motivation": "Current LLM memory systems, which rely on brute-force context expansion or static retrieval pipelines, fail on edge-constrained devices. There's a need for a more efficient and effective long-term memory solution for edge-based language models.", "method": "Our approach uses graph-structured storage, modular substance and redundancy filters, memory committing and pruning mechanisms, and probabilistic recall with temporal decay and refresh processes modeled after human memory.", "result": "In experiments with longitudinal healthcare dialogues, Mnemosyne demonstrates the highest win rate of 65.8% in blind human evaluations of realism and long-term memory capability, outperforming the baseline RAG win rate of 31.1%. It also achieves the highest LoCoMo benchmark scores in temporal reasoning and single-hop retrieval.", "conclusion": "The research demonstrates that the introduction of Mnemosyne significantly enhances factual recall, temporal reasoning, and naturalness of user-facing responses in longitudinal healthcare assistants, which may be feasible with an unsupervised memory architecture that's compatible with edge devices."}}
{"id": "2510.08653", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08653", "abs": "https://arxiv.org/abs/2510.08653", "authors": ["Zhe Dong", "Yuzhe Sun", "Haochen Jiang", "Tianzhu Liu", "Yanfeng Gu"], "title": "PhyDAE: Physics-Guided Degradation-Adaptive Experts for All-in-One Remote Sensing Image Restoration", "comment": null, "summary": "Remote sensing images inevitably suffer from various degradation factors\nduring acquisition, including atmospheric interference, sensor limitations, and\nimaging conditions. These complex and heterogeneous degradations pose severe\nchallenges to image quality and downstream interpretation tasks. Addressing\nlimitations of existing all-in-one restoration methods that overly rely on\nimplicit feature representations and lack explicit modeling of degradation\nphysics, this paper proposes Physics-Guided Degradation-Adaptive Experts\n(PhyDAE). The method employs a two-stage cascaded architecture transforming\ndegradation information from implicit features into explicit decision signals,\nenabling precise identification and differentiated processing of multiple\nheterogeneous degradations including haze, noise, blur, and low-light\nconditions. The model incorporates progressive degradation mining and\nexploitation mechanisms, where the Residual Manifold Projector (RMP) and\nFrequency-Aware Degradation Decomposer (FADD) comprehensively analyze\ndegradation characteristics from manifold geometry and frequency perspectives.\nPhysics-aware expert modules and temperature-controlled sparse activation\nstrategies are introduced to enhance computational efficiency while ensuring\nimaging physics consistency. Extensive experiments on three benchmark datasets\n(MD-RSID, MD-RRSHID, and MDRS-Landsat) demonstrate that PhyDAE achieves\nsuperior performance across all four restoration tasks, comprehensively\noutperforming state-of-the-art methods. Notably, PhyDAE substantially improves\nrestoration quality while achieving significant reductions in parameter count\nand computational complexity, resulting in remarkable efficiency gains compared\nto mainstream approaches and achieving optimal balance between performance and\nefficiency. Code is available at https://github.com/HIT-SIRS/PhyDAE.", "AI": {"tldr": "提出了一种称为Physics-Guided Degradation-Adaptive Experts (PhyDAE) 的方法，通过两阶段级联架构，能够精准识别和处理多种异构退化因素，并通过引入物理感知专家模块和温度控制稀疏激活策略，提高了计算效率同时保持了成像物理一致性。实验表明，PhyDAE 在四个恢复任务上表现出色，同时参数量和计算复杂度显著降低。", "motivation": "现有的全功能恢复方法过于依赖隐式特征表示，缺乏对退化物理的显式建模，难以准确处理复杂的异质退化因素。", "method": "PhyDAE 采用两阶段级联架构，将退化信息从隐式特征转变为显式决策信号。该模型通过引入Residual Manifold Projector (RMP) 和Frequency-Aware Degradation Decomposer (FADD)，从流形几何和频率两个角度分析退化特征。同时采用物理感知专家模块和温度控制稀疏激活策略来增强计算效率，保证成像物理一致性。", "result": "在三个基准数据集 (MD-RSID, MD-RRSHID, 和MDRS-Landsat) 的广泛实验表明，PhyDAE 在所有四个恢复任务上表现出色，参数量和计算复杂度显著降低。", "conclusion": "PhyDAE 在图像恢复质量上表现出色，并且在参数量和计算复杂度方面优于主流方法，实现了性能和效率的最优平衡。"}}
