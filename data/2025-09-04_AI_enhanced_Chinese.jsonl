{"id": "2509.02659", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02659", "abs": "https://arxiv.org/abs/2509.02659", "authors": ["Zilong Guo", "Yi Luo", "Long Sha", "Dongxu Wang", "Panqu Wang", "Chenyang Xu", "Yi Yang"], "title": "2nd Place Solution for CVPR2024 E2E Challenge: End-to-End Autonomous Driving Using Vision Language Model", "comment": "2nd place in CVPR 2024 End-to-End Driving at Scale Challenge", "summary": "End-to-end autonomous driving has drawn tremendous attention recently. Many\nworks focus on using modular deep neural networks to construct the end-to-end\narchi-tecture. However, whether using powerful large language models (LLM),\nespecially multi-modality Vision Language Models (VLM) could benefit the\nend-to-end driving tasks remain a question. In our work, we demonstrate that\ncombining end-to-end architectural design and knowledgeable VLMs yield\nimpressive performance on the driving tasks. It is worth noting that our method\nonly uses a single camera and is the best camera-only solution across the\nleaderboard, demonstrating the effectiveness of vision-based driving approach\nand the potential for end-to-end driving tasks.", "AI": {"tldr": "本研究展示将端到端架构设计与知识丰富的多模态视觉语言模型相结合，在仅使用单目摄像头的情况下，实现了驾驶任务上的出色表现，证明了基于视觉的驾驶方法的有效性和端到端驾驶任务的潜力。", "motivation": "尽管许多研究集中于使用模块化的深度神经网络构建端到端架构，但是否可以利用强大的大语言模型，尤其是多模态视觉语言模型，来提升驾驶任务的端到端性能，仍然是一个待解答的问题。", "method": "研究结合了端到端架构设计和知识丰富的多模态视觉语言模型以提升驾驶任务的表现。", "result": "该研究显示，仅使用单目摄像头的情况下，所提出的方法在排行榜上成为了最优的单摄像头解决方案。", "conclusion": "实验结果证明了基于视觉的驾驶方法的有效性以及多模态视觉语言模型应用于端到端驾驶任务的潜力。"}}
{"id": "2509.02807", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02807", "abs": "https://arxiv.org/abs/2509.02807", "authors": ["Mennatullah Siam"], "title": "PixFoundation 2.0: Do Video Multi-Modal LLMs Use Motion in Visual Grounding?", "comment": "Work under review in NeurIPS 2025 with the title \"Are we using Motion\n  in Referring Segmentation? A Motion-Centric Evaluation\"", "summary": "Multi-modal large language models (MLLMs) have shown impressive\ngeneralization across tasks using images and text modalities. While their\nextension to video has enabled tasks such as video question answering and video\ncaptioning, their pixel-level visual grounding abilities are less studied. In\nthis work, we raise the pertinent question of whether motion is used in\npixel-level visual grounding and whether video MLLMs can segment objects based\non natural language expressions describing their motion patterns. We identify\nthe shortcomings in the current benchmarks, where we show that a single frame\ncan often suffice for capturing the motion referring expression without any\ntemporal reasoning. To address this, we introduce four motion-centric probing\ntechniques, particularly designed for the visual grounding task, to study video\nMLLMs' ability to identify true motion from a fake one and their ability to\ngrasp the motion order. Consequently, we provide a motion-centric benchmark,\nMoCentric-Bench. It ensures that video MLLMs are evaluated towards leveraging\nthe interaction between motion and language rather than being dominated by\nstatic appearance cues emphasized in existing visual grounding datasets. We\nfurther establish strong single-image baselines that are on par with or\noutperform prior methods. Finally, we explore simple motion-centric adaptation\ntechniques that provide state-of-the-art performance on our MoCentric-Bench.\nOur motion-centric benchmark, evaluation and findings challenge future models\nto improve dense spatiotemporal grounding and pixel-level understanding within\nvideos. Code and datasets will be made publicly available at\nhttps://github.com/MSiam/PixFoundation-2.0.git.", "AI": {"tldr": "研究提出了一种专门针对视觉定位任务的运动中心探测技术，设计了运动中心基准测试MoCentric-Bench，并展示运动预测技术在基准测试中的高级性能。", "motivation": "当前的基准测试存在不足，单一帧可以足够捕获运动描述表达，而不进行任何时间推理。为了解决这个问题，该研究提出了一种专门针对视觉定位任务的运动中心探测技术。", "method": "研究识别了现有基准测试的不足，引入了四种运动中心探测技术，设计了运动中心基准测试MoCentric-Bench，为此，还建立了强大的单图像基线，并探索了简单的运动中心适应技术。", "result": "基准测试MoCentric-Bench确保视觉多模态大语言模型评估侧重于利用运动和语言的交互作用，而不是受现有视觉定位数据集中强调的静态外观线索的影响。简单运动中心适应技术在MoCentric-Bench上提供了最先进的性能。", "conclusion": "该研究提出了一个运动中心基准测试MoCentric-Bench，挑战未来模型提高视频中的密集时空定位和像素级理解能力，并展示了在该基准上的顶级性能。"}}
{"id": "2509.02851", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02851", "abs": "https://arxiv.org/abs/2509.02851", "authors": ["Sadra Saremi", "Amirhossein Ahmadkhan Kordbacheh"], "title": "Multi-Scale Deep Learning for Colon Histopathology: A Hybrid Graph-Transformer Approach", "comment": null, "summary": "Colon cancer also known as Colorectal cancer, is one of the most malignant\ntypes of cancer worldwide. Early-stage detection of colon cancer is highly\ncrucial to prevent its deterioration. This research presents a hybrid\nmulti-scale deep learning architecture that synergizes capsule networks, graph\nattention mechanisms, transformer modules, and residual learning to advance\ncolon cancer classification on the Lung and Colon Cancer Histopathological\nImage Dataset (LC25000) dataset. The proposed model in this paper utilizes the\nHG-TNet model that introduces a hybrid architecture that joins strength points\nin transformers and convolutional neural networks to capture multi-scale\nfeatures in histopathological images. Mainly, a transformer branch extracts\nglobal contextual bonds by partitioning the image into patches by\nconvolution-based patch embedding and then processing these patches through a\ntransformer encoder. Analogously, a dedicated CNN branch captures fine-grained,\nlocal details through successive Incorporation these diverse features, combined\nwith a self-supervised rotation prediction objective, produce a robust\ndiagnostic representation that surpasses standard architectures in performance.\nResults show better performance not only in accuracy or loss function but also\nin these algorithms by utilizing capsule networks to preserve spatial orders\nand realize how each element individually combines and forms whole structures.", "AI": {"tldr": "研究提出了一种结合胶囊网络、图注意力机制、变压器模块和残差学习的多尺度深度学习架构，以提高LC25000数据集上结肠癌的分类性能，方法中的HG-TNet模型展示了超越标准架构的表现。", "motivation": "结肠癌是一种全球性的恶性癌症，早期检测对于预防其恶化至关重要。这项研究是为了开发一种新的架构来提高结肠癌分类的性能，以尽早准确检测结肠癌。", "method": "论文中提出的方法使用了称为HG-TNet的混合架构，该架构结合了变压器和卷积神经网络的优点，以捕获组织学图像中的多尺度特征。变压器分支通过卷积基础的补丁嵌入成块地分割图像，而CNN分支捕获细粒度的局部细节。这些不同的特征通过自监督旋转预测目标进行组合。", "result": "研究结果显示，该模型不仅在准确度或损失函数方面表现出色，而且通过利用胶囊网络来保持空间顺序，并识别每个元素是如何个别组合并形成整体结构的，也优于传统的架构。", "conclusion": "研究表明，提出的基于HG-TNet的新型混合架构在结肠病理图像分类任务上表现出卓越的性能，特别是在维持空间顺序和识别整体结构方面优于传统方法。"}}
{"id": "2509.02898", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02898", "abs": "https://arxiv.org/abs/2509.02898", "authors": ["Armin Saadat", "Nima Hashemi", "Hooman Vaseli", "Michael Y. Tsang", "Christina Luong", "Michiel Van de Panne", "Teresa S. M. Tsang", "Purang Abolmaesumi"], "title": "PRECISE-AS: Personalized Reinforcement Learning for Efficient Point-of-Care Echocardiography in Aortic Stenosis Diagnosis", "comment": "To be published in MICCAI 2025", "summary": "Aortic stenosis (AS) is a life-threatening condition caused by a narrowing of\nthe aortic valve, leading to impaired blood flow. Despite its high prevalence,\naccess to echocardiography (echo), the gold-standard diagnostic tool, is often\nlimited due to resource constraints, particularly in rural and underserved\nareas. Point-of-care ultrasound (POCUS) offers a more accessible alternative\nbut is restricted by operator expertise and the challenge of selecting the most\nrelevant imaging views. To address this, we propose a reinforcement learning\n(RL)-driven active video acquisition framework that dynamically selects each\npatient's most informative echo videos. Unlike traditional methods that rely on\na fixed set of videos, our approach continuously evaluates whether additional\nimaging is needed, optimizing both accuracy and efficiency. Tested on data from\n2,572 patients, our method achieves 80.6% classification accuracy while using\nonly 47% of the echo videos compared to a full acquisition. These results\ndemonstrate the potential of active feature acquisition to enhance AS\ndiagnosis, making echocardiographic assessments more efficient, scalable, and\npersonalized. Our source code is available at:\nhttps://github.com/Armin-Saadat/PRECISE-AS.", "AI": {"tldr": "本研究开发了一种强化学习驱动的主动视频采集框架，用于主动脉瓣狭窄的诊断，实现了高效、可扩展和个性化的超声心动图评估，试验结果表明可在使用更少视频片段的情况下达到80.6%的分类准确率。", "motivation": "鉴于超声心动图诊断在资源受限地区的可获得性受限以及即时护理超声（POCUS）受限于操作者经验和难以选择最相关的成像视图，本研究旨在提高主动特征采集在主动脉瓣狭窄诊断中的效率和可扩展性，使之更加个性化。", "method": "本研究提出了一种基于强化学习（RL）的主动视频采集框架，用于动态选择每个患者的最有信息量的超声心动图视图，从而优化准确性和效率，不同于依赖固定视频集的传统方法。", "result": "该方法在2,572名患者的数据上测试时，实现了80.6%的分类准确率，且只需使用常规采集47%的超声心动图视频，证明了主动特征采集在主动脉瓣狭窄诊断中的有效性。", "conclusion": "研究证明了主动特征采集方法在提高主动脉瓣狭窄诊断效率、可扩展性和个性化方面的潜力，实现了高效的超声心动图评估。"}}
{"id": "2509.02785", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02785", "abs": "https://arxiv.org/abs/2509.02785", "authors": ["Jusheng Zhang", "Yijia Fan", "Kaitong Cai", "Zimeng Huang", "Xiaofei Sun", "Jian Wang", "Chengpei Tang", "Keze Wang"], "title": "DrDiff: Dynamic Routing Diffusion with Hierarchical Attention for Breaking the Efficiency-Quality Trade-off", "comment": "Accepted 2025 EMNLP (MainConference)", "summary": "This paper introduces DrDiff, a novel framework for long-text generation that\novercomes the efficiency-quality trade-off through three core technologies.\nFirst, we design a dynamic expert scheduling mechanism that intelligently\nallocates computational resources during the diffusion process based on text\ncomplexity, enabling more efficient handling of text generation tasks of\nvarying difficulty. Second, we introduce a Hierarchical Sparse Attention (HSA)\nmechanism that adaptively adjusts attention patterns according to a variety of\ninput lengths, reducing computational complexity from O($n^2$) to O($n$) while\nmaintaining model performance. Finally, we propose a soft absorption guidance\noptimization strategy that combines with DPM-solver++ to reduce diffusion\nsteps, significantly improving generation speed. Comprehensive experiments on\nvarious long-text generation benchmarks demonstrate the superiority of our\nDrDiff over the existing SOTA methods.", "AI": {"tldr": "DrDiff is a novel, efficient long-text generation framework overcoming existing efficiency-quality trade-offs with advanced scheduling, attention, and optimization mechanisms.", "motivation": "The motivation for this research is to overcome the efficiency-quality trade-off in long-text generation by improving both computational efficiency and model performance.", "method": "This paper introduces DrDiff, which incorporates three core technologies: a dynamic expert scheduling mechanism, a Hierarchical Sparse Attention (HSA) mechanism, and a soft absorption guidance optimization strategy combined with DPM-solver++.", "result": "Comprehensive experiments on various long-text generation benchmarks show that DrDiff outperforms existing state-of-the-art methods.", "conclusion": "DrDiff, with its innovative technologies, sets a new standard for efficiency and quality in long-text generation, outpacing current state-of-the-art techniques."}}
{"id": "2509.02902", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02902", "abs": "https://arxiv.org/abs/2509.02902", "authors": ["Muhammad Shahbaz", "Shaurya Agarwal"], "title": "LiGuard: A Streamlined Open-Source Framework for Rapid & Interactive Lidar Research", "comment": null, "summary": "There is a growing interest in the development of lidar-based autonomous\nmobility and Intelligent Transportation Systems (ITS). To operate and research\non lidar data, researchers often develop code specific to application niche.\nThis approach leads to duplication of efforts across studies that, in many\ncases, share multiple methodological steps such as data input/output (I/O),\npre/post processing, and common algorithms in multi-stage solutions. Moreover,\nslight changes in data, algorithms, and/or research focus may force major\nrevisions in the code. To address these challenges, we present LiGuard, an\nopen-source software framework that allows researchers to: 1) rapidly develop\ncode for their lidar-based projects by providing built-in support for data I/O,\npre/post processing, and commonly used algorithms, 2) interactively\nadd/remove/reorder custom algorithms and adjust their parameters, and 3)\nvisualize results for classification, detection, segmentation, and tracking\ntasks. Moreover, because it creates all the code files in structured\ndirectories, it allows easy sharing of entire projects or even the individual\ncomponents to be reused by other researchers. The effectiveness of LiGuard is\ndemonstrated via case studies.", "AI": {"tldr": "文章介绍了一种名为LiGuard的开源软件框架，旨在为激光雷达数据的研究和处理提供标准化支持，减少重复工作并促进代码复用。", "motivation": "激光雷达自主移动和智能交通系统（ITS）的发展引起了越来越多的兴趣。然而，在研究激光雷达数据时，研究人员经常为特定的应用领域开发专门的代码，这导致了重复工作，并且数据、算法或研究重点的微小变化可能会迫使重大代码修订。因此，开发LiGuard框架来解决这些问题。", "method": "该论文提出了一种名为LiGuard的开源软件框架，用于支持激光雷达数据的处理。该框架提供了数据输入/输出（I/O）、预处理/后处理和常用算法的支持，允许用户以交互方式添加/移除/重新排序自定义算法，调整其参数，并可视化分类、检测、分割和跟踪任务的结果。此外，该框架将所有代码文件创建在结构化的目录中，便于研究人员分享整个项目或复用单个组件。", "result": "通过案例研究验证了LiGuard的有效性。", "conclusion": "LiGuard软件框架通过提供标准化的支持以及便于代码复用的结构化目录系统，促进了激光雷达相关研究的效率和协作。"}}
{"id": "2509.02830", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.02830", "abs": "https://arxiv.org/abs/2509.02830", "authors": ["Pu Wang", "Shinji Watanabe", "Hugo Van hamme"], "title": "SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR", "comment": "Accepted by IEEE ASRU 2025", "summary": "Parameter-efficient fine-tuning (PEFT) has emerged as a scalable solution for\nadapting large foundation models. While low-rank adaptation (LoRA) is widely\nused in speech applications, its state-of-the-art variants, e.g., VeRA, DoRA,\nPiSSA, and SVFT, are developed mainly for language and vision tasks, with\nlimited validation in speech. This work presents the first comprehensive\nintegration and benchmarking of these PEFT methods within ESPnet. We further\nintroduce structured SVD-guided (SSVD) fine-tuning, which selectively rotates\ninput-associated right singular vectors while keeping output-associated vectors\nfixed to preserve semantic mappings. This design enables robust domain\nadaptation with minimal trainable parameters and improved efficiency. We\nevaluate all methods on domain-shifted speech recognition tasks, including\nchild speech and dialectal variation, across model scales from 0.1B to 2B. All\nimplementations are released in ESPnet to support reproducibility and future\nwork.", "AI": {"tldr": "本研究探索了低秩适应的各种变体在语音任务中的应用，并引入了一种新的SSVD微调方法，实现了高效的领域适应。", "motivation": "虽然低秩适应在语音应用程序中被广泛应用，但其先进变体主要为语言和视觉任务开发，且在语音任务上有验证不足的问题。本研究动机是解决该问题，推动语音领域高效适应技术的发展。", "method": "本研究全面整合并评估了低秩适应（LoRA）的各种先进变体，如VeRA、DoRA、PiSSA和SVFT，并首次将其应用于ESPnet语音任务中。此外，研究中引入了一种基于结构化奇异值分解（SSVD）的精细微调方法，该方法通过选择性地旋转与输入相关的奇异向量，同时固定与输出相关的奇异向量，以最小的可训练参数实现稳健的领域适应，有效提高效率。", "result": "所有方法均在领域转移的语音识别任务上进行了评估，包括儿童语音和方言变化，涵盖模型规模从0.1B到2B。", "conclusion": "所有实现都已发布在ESPnet中，以支持可重复研究和未来的工作。研究成果展示了SSVD方法与其他PEFT方法在语音领域适应方面的有效性和优势。"}}
{"id": "2509.02903", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02903", "abs": "https://arxiv.org/abs/2509.02903", "authors": ["Muhammad Shahbaz", "Shaurya Agarwal"], "title": "PercepTwin: Modeling High-Fidelity Digital Twins for Sim2Real LiDAR-based Perception for Intelligent Transportation Systems", "comment": null, "summary": "LiDAR-based perception in intelligent transportation systems (ITS), for tasks\nsuch as object detection, tracking, and semantic and instance segmentation, is\npredominantly solved by deep neural network models which often require\nlarge-scale labeled datasets during training to achieve generalization.\nHowever, creating these datasets is costly. time consuming and require human\nlabor before the datasets are ready for training models. This hinders\nscalability of the LiDAR-based perception systems in ITS. Sim2Real learning\noffers scalable alternative, however, its effectiveness is dependent on the\nfidelity of the source simulation(s) to real-world, in terms of environment\nstructure, actor dynamics, and sensor emulations. In response, this paper\nintroduces a rigorous and reproducible methodology for creating large-scale,\nhigh-quality synthetic datasets using High-Fidelity Digital Twins (HiFi DTs).\nThe proposed workflow outlines the steps, tools, and best practices for\ndigitally replicating real-world environments, encompassing static geometry\nmodeling, road infrastructure replication, and dynamic traffic scenario\ngeneration. Leveraging open-source and readily available resources such as\nsatellite imagery and OpenStreetMap data, alongside specific sensor\nconfigurations, this paper provides practical, detailed guidance for\nconstructing robust synthetic environments. These environments subsequently\nfacilitate scalable, cost-effective, and diverse dataset generation, forming a\nreliable foundation for robust Sim2Real learning.", "AI": {"tldr": "This paper presents a methodology for creating large-scale, high-quality synthetic datasets with High-Fidelity Digital Twins for LiDAR-based perception in ITS, providing a cost-effective solution to overcome the limitations in dataset creation.", "motivation": "The motivation is to circumvent the costly and time-consuming process of creating large-scale labeled real-world datasets for training deep learning models, which is necessary for LiDAR-based perception tasks, and to leverage the potential of Sim2Real learning.", "method": "The paper introduces a methodology for creating scalable and high-quality synthetic datasets using High-Fidelity Digital Twins (HiFi DTs) for LiDAR-based perception systems in ITS.", "result": "The result is a workflow that outlines the steps, tools, and best practices for generating synthetic environments with high fidelity to the real-world, enabling diverse and cost-effective dataset generation.", "conclusion": "The conclusion is that the proposed methodology can create a reliable foundation for robust Sim2Real learning, allowing for scalable and efficient deployment of LiDAR-based perceptual systems in ITS."}}
{"id": "2509.02834", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02834", "abs": "https://arxiv.org/abs/2509.02834", "authors": ["Gustavo Bonil", "João Gondim", "Marina dos Santos", "Simone Hashiguti", "Helena Maia", "Nadia Silva", "Helio Pedrini", "Sandra Avila"], "title": "Clustering Discourses: Racial Biases in Short Stories about Women Generated by Large Language Models", "comment": "12 pages, 3 figures. Accepted at STIL @ BRACIS 2025", "summary": "This study investigates how large language models, in particular LLaMA\n3.2-3B, construct narratives about Black and white women in short stories\ngenerated in Portuguese. From 2100 texts, we applied computational methods to\ngroup semantically similar stories, allowing a selection for qualitative\nanalysis. Three main discursive representations emerge: social overcoming,\nancestral mythification and subjective self-realization. The analysis uncovers\nhow grammatically coherent, seemingly neutral texts materialize a crystallized,\ncolonially structured framing of the female body, reinforcing historical\ninequalities. The study proposes an integrated approach, that combines machine\nlearning techniques with qualitative, manual discourse analysis.", "AI": {"tldr": "本研究使用LLaMA 3.2-3B模型生成的葡萄牙语短篇小说，通过计算方法分析文本，并得出三种主要的叙事模式，揭示了文本中的隐性殖民结构和性别不平等，提出了一种新的综合分析方法。", "motivation": "研究动机在于探究大型语言模型在葡萄牙语短篇小说中对黑人和白人女性构建叙事的方式，并从中发现可能存在的隐性偏见和不平等。", "method": "本研究采用计算方法对2100篇文本进行分组，以选出适合定性分析的短故事。研究中使用了大型语言模型LLaMA 3.2-3B。", "result": "研究结果揭示了三种主要的叙事表现形式：社会克服、祖先神话化和个人自我实现。此外，结果显示，尽管文本在语法上连贯且看似中立，但它们实际上强化了对女性身体的殖民化结构描述，体现了历史上的不平等。", "conclusion": "本研究提出了一种结合机器学习技术和定性的人工话语分析的综合方法，以更好地理解文本中复杂的性别和种族叙事。"}}
{"id": "2509.02904", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02904", "abs": "https://arxiv.org/abs/2509.02904", "authors": ["Muhammad Shahbaz", "Shaurya Agarwal"], "title": "High-Fidelity Digital Twins for Bridging the Sim2Real Gap in LiDAR-Based ITS Perception", "comment": null, "summary": "Sim2Real domain transfer offers a cost-effective and scalable approach for\ndeveloping LiDAR-based perception (e.g., object detection, tracking,\nsegmentation) in Intelligent Transportation Systems (ITS). However, perception\nmodels trained in simulation often under perform on real-world data due to\ndistributional shifts. To address this Sim2Real gap, this paper proposes a\nhigh-fidelity digital twin (HiFi DT) framework that incorporates real-world\nbackground geometry, lane-level road topology, and sensor-specific\nspecifications and placement. We formalize the domain adaptation challenge\nunderlying Sim2Real learning and present a systematic method for constructing\nsimulation environments that yield in-domain synthetic data. An off-the-shelf\n3D object detector is trained on HiFi DT-generated synthetic data and evaluated\non real data. Our experiments show that the DT-trained model outperforms the\nequivalent model trained on real data by 4.8%. To understand this gain, we\nquantify distributional alignment between synthetic and real data using\nmultiple metrics, including Chamfer Distance (CD), Maximum Mean Discrepancy\n(MMD), Earth Mover's Distance (EMD), and Fr'echet Distance (FD), at both\nraw-input and latent-feature levels. Results demonstrate that HiFi DTs\nsubstantially reduce domain shift and improve generalization across diverse\nevaluation scenarios. These findings underscore the significant role of digital\ntwins in enabling reliable, simulation-based LiDAR perception for real-world\nITS applications.", "AI": {"tldr": "本文提出了一种高保真数字孪生框架，来解决LiDAR感知中的仿真到现实世界性能差距问题。实验显示，模型在合成数据上的表现优于真实数据，性能提升显著。", "motivation": "由于在仿真中训练的感知模型通常在实际数据上表现不佳，因此本文旨在通过提出HiFi DT框架来解决Sim2Real间隙问题，从而提高基于LiDAR的感知（例如对象检测、跟踪、分割）在智能运输系统（ITS）中的性能。", "method": "本文提出了一个高保真数字孪生（HiFi DT）框架，它结合了现实世界的背景几何、车道级道路拓扑结构以及特定传感器的规格和放置位置。该框架旨在解决仿真到现实世界（Sim2Real）学习中的领域适应挑战，并提供了一种系统的方法来构建能够生成符合领域数据的仿真环境。", "result": "实验表明，在HiFi DT生成的合成数据上训练的模型比在真实数据上训练的等效模型表现更好，性能提高了4.8%。此外，通过使用包括Chamfer Distance、Maximum Mean Discrepancy、Earth Mover's Distance和Fr'echet Distance在内的多种度量方法，验证了HiFi DT显著降低了领域偏移，并提升了泛化能力。", "conclusion": "研究结果强调了数字孪生在使基于仿真的LiDAR感知在现实世界的ITS应用中更为可靠方面的重要作用。这些发现为通过仿真提升基于LiDAR的ITS感知性能提供了新的视角。"}}
{"id": "2509.02855", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.02855", "abs": "https://arxiv.org/abs/2509.02855", "authors": ["Hyunji Nam", "Lucia Langlois", "James Malamut", "Mei Tan", "Dorottya Demszky"], "title": "IDEAlign: Comparing Large Language Models to Human Experts in Open-ended Interpretive Annotations", "comment": "10 pages, 9 pages for appendix", "summary": "Large language models (LLMs) are increasingly applied to open-ended,\ninterpretive annotation tasks, such as thematic analysis by researchers or\ngenerating feedback on student work by teachers. These tasks involve free-text\nannotations requiring expert-level judgments grounded in specific objectives\n(e.g., research questions or instructional goals). Evaluating whether\nLLM-generated annotations align with those generated by expert humans is\nchallenging to do at scale, and currently, no validated, scalable measure of\nsimilarity in ideas exists. In this paper, we (i) introduce the scalable\nevaluation of interpretive annotation by LLMs as a critical and understudied\ntask, (ii) propose IDEAlgin, an intuitive benchmarking paradigm for capturing\nexpert similarity ratings via a \"pick-the-odd-one-out\" triplet judgment task,\nand (iii) evaluate various similarity metrics, including vector-based ones\n(topic models, embeddings) and LLM-as-a-judge via IDEAlgin, against these human\nbenchmarks. Applying this approach to two real-world educational datasets\n(interpretive analysis and feedback generation), we find that vector-based\nmetrics largely fail to capture the nuanced dimensions of similarity meaningful\nto experts. Prompting LLMs via IDEAlgin significantly improves alignment with\nexpert judgments (9-30% increase) compared to traditional lexical and\nvector-based metrics. These results establish IDEAlgin as a promising paradigm\nfor evaluating LLMs against open-ended expert annotations at scale, informing\nresponsible deployment of LLMs in education and beyond.", "AI": {"tldr": "研究提出并评估了IDEAlgin，这是一个用于评估LLM生成注释与人类专家生成注释之间相似性的基准测试范式。经验证明，与传统的度量方法相比，IDEAlgin能够更有效地评估LLM与专家的一致性，并提供了提升性能的方法。", "motivation": "大规模语言模型（LLMs）在开放式、解释性注释任务中的应用越来越广泛，这类任务包括研究人员的主题分析和教师对学生作业的反馈等。这些任务涉及需要基于特定目标（例如研究问题或教学目标）进行专家判断的自由文本注释。目前尚无有效、可扩展的方法来衡量LLM生成的注释与人类专家生成的注释之间的相似性。因此，研究提出了IDEAlgin作为衡量LLM生成注释与专家注释之间相似性的新基准。", "method": "提出了IDEAlgin这一基准测试范式，采用“三元组判断任务”的方式来捕捉专家的人类相似性评分，并评估了包括基于向量的方法（如主题模型和嵌入式方法）和利用LLM作为判断者的相似性度量。", "result": "实验结果表明，传统的词汇和基于向量的度量方法难以捕捉专家对相似性的微妙理解和评估，而经由IDEAlgin设计的LLM判断方式能够显著提高与专家判断的一致性，在两个实际的教育数据集上提高了一致性，提升了9-30个百分点。", "conclusion": "结果表明IDEAlgin作为一个评价大规模语言模型与开放式专家注释之间相似性的基准范式是切实可行的，其意在指导LLM在教育及其他领域的负责任使用。"}}
{"id": "2509.02918", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02918", "abs": "https://arxiv.org/abs/2509.02918", "authors": ["Midhat Urooj", "Ayan Banerjee", "Farhat Shaikh", "Kuntal Thakur", "Sandeep Gupta"], "title": "Single Domain Generalization in Diabetic Retinopathy: A Neuro-Symbolic Learning Approach", "comment": "Accepted in ANSyA 2025: 1st International Workshop on Advanced\n  Neuro-Symbolic Applications", "summary": "Domain generalization remains a critical challenge in medical imaging, where\nmodels trained on single sources often fail under real-world distribution\nshifts. We propose KG-DG, a neuro-symbolic framework for diabetic retinopathy\n(DR) classification that integrates vision transformers with expert-guided\nsymbolic reasoning to enable robust generalization across unseen domains. Our\napproach leverages clinical lesion ontologies through structured, rule-based\nfeatures and retinal vessel segmentation, fusing them with deep visual\nrepresentations via a confidence-weighted integration strategy. The framework\naddresses both single-domain generalization (SDG) and multi-domain\ngeneralization (MDG) by minimizing the KL divergence between domain embeddings,\nthereby enforcing alignment of high-level clinical semantics. Extensive\nexperiments across four public datasets (APTOS, EyePACS, Messidor-1,\nMessidor-2) demonstrate significant improvements: up to a 5.2% accuracy gain in\ncross-domain settings and a 6% improvement over baseline ViT models. Notably,\nour symbolic-only model achieves a 63.67% average accuracy in MDG, while the\ncomplete neuro-symbolic integration achieves the highest accuracy compared to\nexisting published baselines and benchmarks in challenging SDG scenarios.\nAblation studies reveal that lesion-based features (84.65% accuracy)\nsubstantially outperform purely neural approaches, confirming that symbolic\ncomponents act as effective regularizers beyond merely enhancing\ninterpretability. Our findings establish neuro-symbolic integration as a\npromising paradigm for building clinically robust, and domain-invariant medical\nAI systems.", "AI": {"tldr": "本文提出了KG-DG，一种用于糖尿病视网膜病变分类的神经符号框架，通过融合视觉变换器和专家引导的符号推理以增强在未见领域上的泛化能力。", "motivation": "在医疗影像领域，模型往往在数据分布变化时表现不佳。目的是通过结合视觉和符号处理，来提高模型的跨领域泛化能力。", "method": "框架使用临床病变本体和视网膜血管分割的结构化、基于规则的特征，并通过置信度加权集成策略与深度视觉表示相结合。通过最小化域嵌入之间的KL散度来实现跨单领域和多领域的泛化。", "result": "实验显示，在跨域设置下，准确率提高了5.2%，对比基线ViT模型提高了6%。通过融合全部神经符号达到最高准确率。", "conclusion": "研究表明，神经符号集成是构建临床鲁棒和领域不变的医疗AI系统的一个有前景的方法。"}}
