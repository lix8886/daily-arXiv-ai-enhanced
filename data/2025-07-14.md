<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 54]
- [cs.CV](#cs.CV) [Total: 68]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [RepeaTTS: Towards Feature Discovery through Repeated Fine-Tuning](https://arxiv.org/abs/2507.08012)
*Atli Sigurgeirsson,Simon King*

Main category: cs.CL

> 论文提出了一个新的微调策略，通过主成分分析来增强基于提示的文本到语音模型的可控性，特别是在提高不包含情感揭示模型的合成语音质量方面。

<details>
  <summary>Details</summary>

**Motivation:** 论文动机在于解决基于提示的文本到语音模型的两个问题：一方面，控制仅限于训练期间暴露给模型的声学特征；另一方面，同一输入产生的不可控变化反映在语料统计中。

**Method:** 该论文提出了一种新的微调方法，通过主成分分析（PCA）对成千上万的合成样本进行处理，确定解释输出变化最大比例的潜在特征，并将这些特征作为新的标签用于二次微调。

**Result:** 该方法在两个基于冰岛语表达语料库训练的模型上进行了评估：一个模型包含情感揭示，另一个不包含。对于不包含情感揭示的模型，该方法成功生成了连续和离散的特征，增强了模型的整体可控性。

**Conclusion:** 通过引入新的微调策略，论文成功地增强了模型的可控性，特别是在没有情感揭示的模型中，这种方法证明了其在提高连续和离散特征质量方面的有效性。

**Abstract:** A Prompt-based Text-To-Speech model allows a user to control different
aspects of speech, such as speaking rate and perceived gender, through natural
language instruction. Although user-friendly, such approaches are on one hand
constrained: control is limited to acoustic features exposed to the model
during training, and too flexible on the other: the same inputs yields
uncontrollable variation that are reflected in the corpus statistics.
  We investigate a novel fine-tuning regime to address both of these issues at
the same time by exploiting the uncontrollable variance of the model. Through
principal component analysis of thousands of synthesised samples, we determine
latent features that account for the highest proportion of the output variance
and incorporate them as new labels for secondary fine-tuning. We evaluate the
proposed methods on two models trained on an expressive Icelandic speech
corpus, one with emotional disclosure and one without. In the case of the model
without emotional disclosure, the method yields both continuous and discrete
features that improve overall controllability of the model.

</details>


### [2] [MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model](https://arxiv.org/abs/2507.08013)
*K. Sahit Reddy,N. Ragavenderan,Vasanth K.,Ganesh N. Naik,Vishalakshi Prabhu,Nagaraja G. S*

Main category: cs.CL

> 提出MedicalBERT模型，专门增强生物医学领域自然语言处理能力，性能超过多个同类模型。

<details>
  <summary>Details</summary>

**Motivation:** 针对生物医学文献特有的专业术语挑战，优化现有模型处理此任务的能力。

**Method:** Structure

**Result:** {"tldr": "提出MedicalBERT模型，专门增强生物医学领域自然语言处理能力，性能超过多个同类模型。", "motivation": "针对生物医学文献特有的专业术语挑战，优化现有模型处理此任务的能力。", "method": "基于BERT开发MedicalBERT，使用大规模生物医学数据集进行预训练和微调。", "result": "在命名实体识别、关系抽取、问答、句子相似性和文档分类任务上，MedicalBERT的表现优于其他多个BERT相关模型。", "conclusion": "验证了使用预训练BERT模型进行医疗NLP任务的有效性及转移学习技术在捕捉领域特定信息方面的优势。"}

**Conclusion:** 验证了使用预训练BERT模型进行医疗NLP任务的有效性及转移学习技术在捕捉领域特定信息方面的优势。

**Abstract:** Recent advances in natural language processing (NLP) have been driven
bypretrained language models like BERT, RoBERTa, T5, and GPT. Thesemodels excel
at understanding complex texts, but biomedical literature, withits
domain-specific terminology, poses challenges that models likeWord2Vec and
bidirectional long short-term memory (Bi-LSTM) can't fullyaddress. GPT and T5,
despite capturing context, fall short in tasks needingbidirectional
understanding, unlike BERT. Addressing this, we proposedMedicalBERT, a
pretrained BERT model trained on a large biomedicaldataset and equipped with
domain-specific vocabulary that enhances thecomprehension of biomedical
terminology. MedicalBERT model is furtheroptimized and fine-tuned to address
diverse tasks, including named entityrecognition, relation extraction, question
answering, sentence similarity, anddocument classification. Performance metrics
such as the F1-score,accuracy, and Pearson correlation are employed to showcase
the efficiencyof our model in comparison to other BERT-based models such as
BioBERT,SciBERT, and ClinicalBERT. MedicalBERT outperforms these models onmost
of the benchmarks, and surpasses the general-purpose BERT model by5.67% on
average across all the tasks evaluated respectively. This work alsounderscores
the potential of leveraging pretrained BERT models for medicalNLP tasks,
demonstrating the effectiveness of transfer learning techniques incapturing
domain-specific information.
  (PDF) MedicalBERT: enhancing biomedical natural language processing using
pretrained BERT-based model. Available from:
https://www.researchgate.net/publication/392489050_MedicalBERT_enhancing_biomedical_natural_language_processing_using_pretrained_BERT-based_model
[accessed Jul 06 2025].

</details>


### [3] [Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking](https://arxiv.org/abs/2507.08014)
*Aldan Creo,Raul Castro Fernandez,Manuel Cebrian*

Main category: cs.CL

> 研究大规模分析了超过200万次真实对话，发现越狱尝试的复杂性与正常对话无显著差异，并揭示了LLM安全进化受人类创新限制的事实。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型（LLMs）的部署范围不断扩大，理解越狱策略的复杂性和演变对于AI安全性至关重要。

**Method:** 采用大规模实证分析，通过对超过200万次来自不同平台的真实对话进行分析，这些平台包括专门的越狱社区和通用聊天机器人。使用多种复杂性度量标准，包括概率测量、词汇多样性、压缩比率和认知负荷指标来衡量越狱尝试的复杂性。

**Result:** 发现越狱尝试的复杂性并不显著高于正常对话。这种模式在专门的越狱社区和普通用户群体中均保持一致，表明攻击复杂程度存在实践上限。时间序列分析显示，尽管用户发起的攻击毒性保持相对稳定，但助手响应的毒性有所下降，说明安全措施正在改进。复杂性分布中缺乏幂律分布进一步表明越狱开发存在自然限制。

**Conclusion:** 研究结果挑战了攻击者和防御者之间不断升级的军备竞赛的主流观点，相反，表明LLM的安全进化由人类创新的限制驱动，而防御措施不断进步。该结果揭示了在学术披露越狱技术时可能出现的重要信息危害，因为超出当前复杂度基线的复杂攻击可能会打破当前的均衡状态，并在防御措施适应之前造成广泛危害。

**Abstract:** As large language models (LLMs) become increasingly deployed, understanding
the complexity and evolution of jailbreaking strategies is critical for AI
safety.
  We present a mass-scale empirical analysis of jailbreak complexity across
over 2 million real-world conversations from diverse platforms, including
dedicated jailbreaking communities and general-purpose chatbots. Using a range
of complexity metrics spanning probabilistic measures, lexical diversity,
compression ratios, and cognitive load indicators, we find that jailbreak
attempts do not exhibit significantly higher complexity than normal
conversations. This pattern holds consistently across specialized jailbreaking
communities and general user populations, suggesting practical bounds on attack
sophistication. Temporal analysis reveals that while user attack toxicity and
complexity remains stable over time, assistant response toxicity has decreased,
indicating improving safety mechanisms. The absence of power-law scaling in
complexity distributions further points to natural limits on jailbreak
development.
  Our findings challenge the prevailing narrative of an escalating arms race
between attackers and defenders, instead suggesting that LLM safety evolution
is bounded by human ingenuity constraints while defensive measures continue
advancing. Our results highlight critical information hazards in academic
jailbreak disclosure, as sophisticated attacks exceeding current complexity
baselines could disrupt the observed equilibrium and enable widespread harm
before defensive adaptation.

</details>


### [4] [Assessing the Capabilities and Limitations of FinGPT Model in Financial NLP Applications](https://arxiv.org/abs/2507.08015)
*Prudence Djagba,Chimezie A. Odinakachukwu*

Main category: cs.CL

> 研究评估了金融专用语言模型FinGPT，在多个NLP任务中的表现。发现它在分类任务上表现良好，但在需要推理和生成的任务上表现较差。

<details>
  <summary>Details</summary>

**Motivation:** 此研究旨在考察FinGPT在多种金融相关任务中的有效性，以识别其潜在的限制和可行性的范围，并为未来的研究提供有用的基准。

**Method:** 本研究评估了FinGPT，一种面向金融市场特定的语言模型，在六个关键的自然语言处理（NLP）任务上的表现：情感分析、文本分类、命名实体识别、财务问答、文本摘要和股票走势预测。评估使用了金融专用数据集来评估FinGPT在真实世界的金融应用程序中的能力和局限性。

**Result:** FinGPT在如情感分析和标题分类等分类任务上表现出色，往往达到与GPT-4相近的结果。但在涉及推理和生成的任务上，例如财务问答和摘要生成，其表现显著低于GPT-4和人类基准，特别是在数值准确性和复杂推理上存在差距。

**Conclusion:** 研究结果表明，尽管FinGPT在某些结构化的金融任务上有成效，但当前还不是全面的解决方案。这研究强调了在未来需要在金融领域的优化和架构上的改进。

**Abstract:** This work evaluates FinGPT, a financial domain-specific language model,
across six key natural language processing (NLP) tasks: Sentiment Analysis,
Text Classification, Named Entity Recognition, Financial Question Answering,
Text Summarization, and Stock Movement Prediction. The evaluation uses
finance-specific datasets to assess FinGPT's capabilities and limitations in
real-world financial applications. The results show that FinGPT performs
strongly in classification tasks such as sentiment analysis and headline
categorization, often achieving results comparable to GPT-4. However, its
performance is significantly lower in tasks that involve reasoning and
generation, such as financial question answering and summarization. Comparisons
with GPT-4 and human benchmarks highlight notable performance gaps,
particularly in numerical accuracy and complex reasoning. Overall, the findings
indicate that while FinGPT is effective for certain structured financial tasks,
it is not yet a comprehensive solution. This research provides a useful
benchmark for future research and underscores the need for architectural
improvements and domain-specific optimization in financial language models.

</details>


### [5] [Mechanistic Indicators of Understanding in Large Language Models](https://arxiv.org/abs/2507.08017)
*Pierre Beckmann,Matthieu Queloz*

Main category: cs.CL

> 本文通过将大型语言模型（LLMs）的内在工作原理整合到一个新的理论框架中，引入了机器理解的多层次概念：概念理解、状态世界理解和原则理解，并探讨了LLMs的奇特思维工作方式。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在挑战认为LLMs仅依赖于表面统计的观点，通过介绍机制诠释性的最新发现，提供一个理解机器理解的新视角。

**Method:** 文章提出了一种三层机器理解模式，通过解释LLMs是如何在潜在空间中形成特征方向，学习各类事物间的关联，通过状态世界理解和原则理解来阐述LLMs的不同层面的理解能力。

**Result:** 提出了一个理解LLMs内部机制的新框架，说明了LLMs如何在潜在空间中形成功能上类似于人类理解的现象。

**Conclusion:** 尽管LLMs表现出某种形式的理解能力，但它们的认知架构与人类不同，关于LLMs是否理解的争论应转向研究其奇特的思维工作机制。

**Abstract:** Recent findings in mechanistic interpretability (MI), the field probing the
inner workings of Large Language Models (LLMs), challenge the view that these
models rely solely on superficial statistics. Here, we offer an accessible
synthesis of these findings that doubles as an introduction to MI, all while
integrating these findings within a novel theoretical framework for thinking
about machine understanding. We argue that LLMs develop internal structures
that are functionally analogous to the kind of understanding that consists in
seeing connections. To sharpen this idea, we propose a three-tiered conception
of machine understanding. First, conceptual understanding emerges when a model
forms "features" as directions in latent space, thereby learning the
connections between diverse manifestations of something. Second,
state-of-the-world understanding emerges when a model learns contingent factual
connections between features and dynamically tracks changes in the world.
Third, principled understanding emerges when a model ceases to rely on a
collection of memorized facts and discovers a "circuit" that connects these
facts. However, we conclude by exploring the "parallel mechanisms" phenomenon,
arguing that while LLMs exhibit forms of understanding, their cognitive
architecture remains different from ours, and the debate should shift from
whether LLMs understand to how their strange minds work.

</details>


### [6] [Review, Remask, Refine (R3): Process-Guided Block Diffusion for Text Generation](https://arxiv.org/abs/2507.08018)
*Nikita Mounier,Parsa Idehpour*

Main category: cs.CL

> 提出了R3框架，通过PRM评估和Remask策略，来改善文本生成质量，提升模型自我纠错能力。

<details>
  <summary>Details</summary>

**Motivation:** 解决迭代文本生成中的一个关键挑战，即让模型能有效地识别和纠正自己的错误。

**Method:** 提出了一种名为Review, Remask, Refine (R3)的框架，该框架简单且优雅，无需额外训练模型，可应用于任何预训练的掩码文本扩散模型。框架利用Process Reward Model (PRM)对生成的中间块进行评估，并根据PRM分数制定Remask策略，分数越低表示问题越多，相应块中被重新掩码的标记比例越大。最后，模型集中精力修正这些特定部分，从而提升最终输出质量。

**Result:** 采用R3框架的模型能够在后续生成中有针对性地改进特定子优化部分，从而改进了最终输出结果。

**Conclusion:** 通过集中修正低评估分数的块，R3框架能够在不增加额外训练的情况下，提高文本生成的质量和效率。

**Abstract:** A key challenge for iterative text generation is enabling models to
efficiently identify and correct their own errors. We propose Review, Remask,
Refine (R3), a relatively simple yet elegant framework that requires no
additional model training and can be applied to any pre-trained masked text
diffusion model (e.g., LLaDA or BD3-LM). In R3, a Process Reward Model (PRM) is
utilized for the Review of intermediate generated blocks. The framework then
translates these PRM scores into a Remask strategy: the lower a block's PRM
score, indicating potential mistakes, the greater the proportion of tokens
within that block are remasked. Finally, the model is compelled to Refine these
targeted segments, focusing its efforts more intensively on specific
sub-optimal parts of past generations, leading to improved final output.

</details>


### [7] [Signal or Noise? Evaluating Large Language Models in Resume Screening Across Contextual Variations and Human Expert Benchmarks](https://arxiv.org/abs/2507.08019)
*Aryan Varshney,Venkat Ram Reddy Ganuthula*

Main category: cs.CL

> 研究对比了三种大型语言模型在简历筛选中的表现与人类专家的变化，发现有显著差异，表明技术的应用需审慎。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于调查大型语言模型在筛选简历时是否表现出一致性或随机性，并探究其表现与人类专家相比如何。

**Method:** 该研究比较了三种大型语言模型（Claude、GPT 和 Gemini）在不同情境下筛选简历与职位描述的表现，并将其与三位人类招聘专家的表现进行对比。使用的数据集包括相同和随机化的简历。

**Result:** 方差分析揭示了八个条件中的四个条件存在显著的平均差异，并且所有大型语言模型与人类专家的评估存在显著差异（p < 0.01）。成对t检验表明，GPT能强烈适应公司环境（p < 0.001），Gemini部分适应（p = 0.038 for Firm1），而Claude微弱适应（p > 0.1）。

**Conclusion:** 研究发现大型语言模型可以提供可解释的模式，但在特定详尽提示下显著偏离人类判断。这些发现有助于大型语言模型在自动招聘系统中的应用。

**Abstract:** This study investigates whether large language models (LLMs) exhibit
consistent behavior (signal) or random variation (noise) when screening resumes
against job descriptions, and how their performance compares to human experts.
Using controlled datasets, we tested three LLMs (Claude, GPT, and Gemini)
across contexts (No Company, Firm1 [MNC], Firm2 [Startup], Reduced Context)
with identical and randomized resumes, benchmarked against three human
recruitment experts. Analysis of variance revealed significant mean differences
in four of eight LLM-only conditions and consistently significant differences
between LLM and human evaluations (p < 0.01). Paired t-tests showed GPT adapts
strongly to company context (p < 0.001), Gemini partially (p = 0.038 for
Firm1), and Claude minimally (p > 0.1), while all LLMs differed significantly
from human experts across contexts. Meta-cognition analysis highlighted
adaptive weighting patterns that differ markedly from human evaluation
approaches. Findings suggest LLMs offer interpretable patterns with detailed
prompts but diverge substantially from human judgment, informing their
deployment in automated hiring systems.

</details>


### [8] [Circumventing Safety Alignment in Large Language Models Through Embedding Space Toxicity Attenuation](https://arxiv.org/abs/2507.08020)
*Zhibo Zhang,Yuxi Li,Kailong Wang,Shuai Yuan,Ling Shi,Haoyu Wang*

Main category: cs.CL

> 提出了一种新的框架ETTA，用于识别并减弱嵌入空间中的毒性敏感维度，以提升模型安全性，评估结果显示其显著优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 目前对于模型嵌入层面的安全对齐动态理解不足，大多数针对性强且精准的对抗扰动技术研究不足，这可能会导致显著的安全威胁。

**Method:** ETTA (Embedding Transformation Toxicity Attenuation)：识别并减弱嵌入空间中的毒性敏感维度，通过线性变换实现，绕过模型拒绝行为，同时保持语言连贯性，无需模型微调或访问训练数据。

**Result:** 在五种代表性开源大语言模型上使用AdvBench基准进行评估，ETTA实现了高达88.61％的平均攻击成功率，比最佳基线高出11.34％，对于增强安全性的模型（如指令调整后的防御模型）也达到了77.39％的成功率。

**Conclusion:** 实验结果表明了当前对齐策略中存在的关键脆弱性，并强调了需要嵌入感知防御的重要性。

**Abstract:** Large Language Models (LLMs) have achieved remarkable success across domains
such as healthcare, education, and cybersecurity. However, this openness also
introduces significant security risks, particularly through embedding space
poisoning, which is a subtle attack vector where adversaries manipulate the
internal semantic representations of input data to bypass safety alignment
mechanisms. While previous research has investigated universal perturbation
methods, the dynamics of LLM safety alignment at the embedding level remain
insufficiently understood. Consequently, more targeted and accurate adversarial
perturbation techniques, which pose significant threats, have not been
adequately studied.
  In this work, we propose ETTA (Embedding Transformation Toxicity
Attenuation), a novel framework that identifies and attenuates
toxicity-sensitive dimensions in embedding space via linear transformations.
ETTA bypasses model refusal behaviors while preserving linguistic coherence,
without requiring model fine-tuning or access to training data. Evaluated on
five representative open-source LLMs using the AdvBench benchmark, ETTA
achieves a high average attack success rate of 88.61%, outperforming the best
baseline by 11.34%, and generalizes to safety-enhanced models (e.g., 77.39% ASR
on instruction-tuned defenses). These results highlight a critical
vulnerability in current alignment strategies and underscore the need for
embedding-aware defenses.

</details>


### [9] [Unveiling Effective In-Context Configurations for Image Captioning: An External & Internal Analysis](https://arxiv.org/abs/2507.08021)
*Li Li,Yongliang Wu,Jingze Zhu,Jiawei Peng,Jianfei Cai,Xu Yang*

Main category: cs.CL

> This paper investigates multimodal in-context learning (ICL) in large multimodal models (LMMs) for image captioning, analyzing demonstration configurations and internal model behaviors with proposed metrics.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to explore and understand the impact of In-Context Examples (ICEs) configuration on LMM performance and behavior, addressing the preliminary nature of multimodal ICL configuration research.

**Method:** The paper uses both external and internal analysis for a thorough examination of ICEs configuration strategies in image captioning, alongside attention metric analysis and auxiliary experiments.

**Result:** Key findings from multiple metrics evaluation and attention analysis are summarized, and performance variations between different LMMs are explained.

**Conclusion:** The paper concludes the effectiveness of their combined external and internal analysis method, and the proposed metrics for assessing multimodal ICL in LMMs, which can be extended to other research areas.

**Abstract:** The evolution of large models has witnessed the emergence of In-Context
Learning (ICL) capabilities. In Natural Language Processing (NLP), numerous
studies have demonstrated the effectiveness of ICL. Inspired by the success of
Large Language Models (LLMs), researchers have developed Large Multimodal
Models (LMMs) with ICL capabilities. However, explorations of demonstration
configuration for multimodal ICL remain preliminary. Additionally, the
controllability of In-Context Examples (ICEs) provides an efficient and
cost-effective means to observe and analyze the inference characteristics of
LMMs under varying inputs. This paper conducts a comprehensive external and
internal investigation of multimodal in-context learning on the image
captioning task. Externally, we explore demonstration configuration strategies
through three dimensions: shot number, image retrieval, and caption assignment.
We employ multiple metrics to systematically and thoroughly evaluate and
summarize key findings. Internally, we analyze typical LMM attention
characteristics and develop attention-based metrics to quantify model
behaviors. We also conduct auxiliary experiments to explore the feasibility of
attention-driven model acceleration and compression. We further compare
performance variations between LMMs with identical model design and pretraining
strategies and explain the differences from the angles of pre-training data
features. Our study reveals both how ICEs configuration strategies impact model
performance through external experiments and characteristic typical patterns
through internal inspection, providing dual perspectives for understanding
multimodal ICL in LMMs. Our method of combining external and internal analysis
to investigate large models, along with our newly proposed metrics, can be
applied to broader research areas.

</details>


### [10] ["Amazing, They All Lean Left" -- Analyzing the Political Temperaments of Current LLMs](https://arxiv.org/abs/2507.08027)
*W. Russell Neuman,Chad Coleman,Ali Dasdan,Safinah Ali,Manan Shah,Kund Meghani*

Main category: cs.CL

> 本文发现，七个著名大型语言模型表现出一致的自由主义倾向，这种倾向来源于几个因素，并指出这不是编程错误或偏见，而是训练过程中的一个特征。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在深入理解大型语言模型内部存在的自由主义倾向的原因及其影响。

**Method:** 研究采用了多管齐下的方法，包括道德基础理论、十几种已建立的政治意识形态量表和一个新编的政治争议指数，系统调查了七个主要的LLM模型的政治倾向。

**Result:** 本文系统性地调查了七个著名大型语言模型（LLMs）的政治倾向，发现大多数模型表现出偏向自由主义的价值观。这种倾向归因于训练语料库的自由主义倾向、来自人类反馈的强化学习（RLHF）、学术伦理讨论中自由主义框架的主导地位以及安全导向的微调实践。本文还区分了政治‘偏见’和合法的知识差异，并指出微调通常会增加模型的自由主义倾向。作者认为这种‘自由主义倾向’并非编程错误或程序员的个人偏好，而是建立在以民主权利为中心的话语训练上的一个特征。这一模式可能提供了一个新视角来审视集体理性。

**Conclusion:** 作者认为大型语言模型中的‘自由主义倾向’体现了对民主权利的聚焦，可以帮助我们以新的视角审视集体道德推理。

**Abstract:** Recent studies have revealed a consistent liberal orientation in the ethical
and political responses generated by most commercial large language models
(LLMs), yet the underlying causes and resulting implications remain unclear.
This paper systematically investigates the political temperament of seven
prominent LLMs - OpenAI's GPT-4o, Anthropic's Claude Sonnet 4, Perplexity
(Sonar Large), Google's Gemini 2.5 Flash, Meta AI's Llama 4, Mistral 7b Le Chat
and High-Flyer's DeepSeek R1 -- using a multi-pronged approach that includes
Moral Foundations Theory, a dozen established political ideology scales and a
new index of current political controversies. We find strong and consistent
prioritization of liberal-leaning values, particularly care and fairness,
across most models. Further analysis attributes this trend to four overlapping
factors: Liberal-leaning training corpora, reinforcement learning from human
feedback (RLHF), the dominance of liberal frameworks in academic ethical
discourse and safety-driven fine-tuning practices. We also distinguish between
political "bias" and legitimate epistemic differences, cautioning against
conflating the two. A comparison of base and fine-tuned model pairs reveals
that fine-tuning generally increases liberal lean, an effect confirmed through
both self-report and empirical testing. We argue that this "liberal tilt" is
not a programming error or the personal preference of programmers but an
emergent property of training on democratic rights-focused discourse. Finally,
we propose that LLMs may indirectly echo John Rawls' famous veil-of ignorance
philosophical aspiration, reflecting a moral stance unanchored to personal
identity or interest. Rather than undermining democratic discourse, this
pattern may offer a new lens through which to examine collective reasoning.

</details>


### [11] [Better Together: Quantifying the Benefits of AI-Assisted Recruitment](https://arxiv.org/abs/2507.08029)
*Ada Aka,Emil Palikot,Ali Ansari,Nima Yazdani*

Main category: cs.CL

> 研究表明，AI在招聘过程中提高了通过最终面试的比例和找到新工作的概率。

<details>
  <summary>Details</summary>

**Motivation:** 尽管人工智能在招聘中越来越普遍，但对其对招聘效率和候选人选择影响的实证证据仍然有限。本研究意在填补这一空白，通过随机实验量化AI对招聘过程的具体影响，并揭示AI技术在决策中的潜在影响。

**Method:** 本研究采用随机实验方法，将37000名应聘初级开发职位的申请者分配至传统招聘流程组或AI辅助招聘流程组。AI辅助组首先通过AI驱动的结构化视频面试，然后进入人工评估，而传统组则是基于简历筛选。两个组进入最终的人工面试阶段，面试官对之前的筛选方法不知情。

**Result:** 总体来看，应聘者在AI辅助组通过最终面试的比例为54%，而传统组仅为34%，两组间的平均差异为20个百分点。此外，五个月后对顶级候选人进行追踪发现，AI组找到新工作的比例为23%，而传统组为18%，两组间找到新工作的概率差异为5.9个百分点。

**Conclusion:** 研究表明，AI在招聘中可以提高最终面试通过率和找到工作的概率，并且倾向于选择更年轻的候选人。该发现有助于增进对AI技术在招聘与人才获取过程中影响的理解，并指出其潜在的应用问题。

**Abstract:** Artificial intelligence (AI) is increasingly used in recruitment, yet
empirical evidence quantifying its impact on hiring efficiency and candidate
selection remains limited. We randomly assign 37,000 applicants for a
junior-developer position to either a traditional recruitment process (resume
screening followed by human selection) or an AI-assisted recruitment pipeline
incorporating an initial AI-driven structured video interview before human
evaluation. Candidates advancing from either track faced the same final-stage
human interview, with interviewers blind to the earlier selection method. In
the AI-assisted pipeline, 54% of candidates passed the final interview compared
with 34% from the traditional pipeline, yielding an average treatment effect of
20 percentage points (SE 12 pp.). Five months later, we collected LinkedIn
profiles of top applicants from both groups and found that 18% (SE 1.1%) of
applicants from the traditional track found new jobs compared with 23% (SE
2.3%) from the AI group, resulting in a 5.9 pp. (SE 2.6 pp.) difference in the
probability of finding new employment between groups. The AI system tended to
select younger applicants with less experience and fewer advanced credentials.
We analyze AI-generated interview transcripts to examine the selection criteria
and conversational dynamics. Our findings contribute to understanding how AI
technologies affect decision making in recruitment and talent acquisition while
highlighting some of their potential implications.

</details>


### [12] [A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models](https://arxiv.org/abs/2507.08030)
*Sonali Sharma,Ahmed M. Alaa,Roxana Daneshjou*

Main category: cs.CL

> 研究表明，2022年至2025年间，AI生成的医疗图像解释和回答上的医疗免责声明越来越少，强调必须在模型成熟后增加这类警示以确保安全。

<details>
  <summary>Details</summary>

**Motivation:** 随着医疗AI模型的广泛使用，它们生成的回答经常包含不准确的信息。因此，使用医疗免责声明提醒用户AI输出未经过专业审核或替代医疗建议变得尤为重要。

**Method:** 该研究通过分析2022年至2025年间生成模型（LLM和VLM）对500张乳腺X光片、500张胸部X光片、500张皮肤病图片和500个医学问题的输出，评估了医疗免责声明的存在情况。

**Result:** 研究结果表明，医疗免责声明在LLM和VLM输出中的存在比例从2022年的26.3%下降到2025年的0.97%，而在VLM从2023年的19.6%下降到2025年的1.05%。到了2025年，大多数模型都没有显示免责声明。

**Conclusion:** 随着公众使用的模型变得越来越有能力且权威，必须在每个输出的临床环境中实施警示声明作为保障措施。

**Abstract:** Generative AI models, including large language models (LLMs) and
vision-language models (VLMs), are increasingly used to interpret medical
images and answer clinical questions. Their responses often include
inaccuracies; therefore, safety measures like medical disclaimers are critical
to remind users that AI outputs are not professionally vetted or a substitute
for medical advice. This study evaluated the presence of disclaimers in LLM and
VLM outputs across model generations from 2022 to 2025. Using 500 mammograms,
500 chest X-rays, 500 dermatology images, and 500 medical questions, outputs
were screened for disclaimer phrases. Medical disclaimer presence in LLM and
VLM outputs dropped from 26.3% in 2022 to 0.97% in 2025, and from 19.6% in 2023
to 1.05% in 2025, respectively. By 2025, the majority of models displayed no
disclaimers. As public models become more capable and authoritative,
disclaimers must be implemented as a safeguard adapting to the clinical context
of each output.

</details>


### [13] [Beyond Scale: Small Language Models are Comparable to GPT-4 in Mental Health Understanding](https://arxiv.org/abs/2507.08031)
*Hong Jia,Shiya Fu,Vassilis Kostakos,Feng Xia,Ting Dang*

Main category: cs.CL

> 研究发现小型语言模型在心理健康理解任务上的性能接近大型语言模型，并能够通过少量样本学习快速适应和专业化，显示出作为大规模心理健康筛查工具的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 研究小型语言模型在保护隐私的应用中与大型语言模型相比在心理健康理解方面的能力，以了解它们在这一领域的适用性。

**Method:** 通过系统评估五种最先进的小型语言模型（Phi-3, Phi-3.5, Qwen2.5, Llama-3.2, Gemma2）和三种大型语言模型（GPT-4, FLAN-T5-XXL, Alpaca-7B）在六个心理健康理解任务上的性能，以零样本和少量样本学习范式进行基准测试，比较它们在关键领域的相对优缺点。

**Result:** 小型语言模型在二分类任务上平均性能仅比大型语言模型低2%，在少量样本学习中表现出显著提升，显示出它们在隐私保护方面的应用潜力。

**Conclusion:** 小型语言模型在心理健康领域展现了有效的能力，特别是在少量样本学习中，表现出作为隐私保护工具的优势，并具有开发成大规模心理健康筛查工具的潜力。

**Abstract:** The emergence of Small Language Models (SLMs) as privacy-preserving
alternatives for sensitive applications raises a fundamental question about
their inherent understanding capabilities compared to Large Language Models
(LLMs). This paper investigates the mental health understanding capabilities of
current SLMs through systematic evaluation across diverse classification tasks.
Employing zero-shot and few-shot learning paradigms, we benchmark their
performance against established LLM baselines to elucidate their relative
strengths and limitations in this critical domain. We assess five
state-of-the-art SLMs (Phi-3, Phi-3.5, Qwen2.5, Llama-3.2, Gemma2) against
three LLMs (GPT-4, FLAN-T5-XXL, Alpaca-7B) on six mental health understanding
tasks. Our findings reveal that SLMs achieve mean performance within 2\% of
LLMs on binary classification tasks (F1 scores of 0.64 vs 0.66 in zero-shot
settings), demonstrating notable competence despite orders of magnitude fewer
parameters. Both model categories experience similar degradation on multi-class
severity tasks (a drop of over 30\%), suggesting that nuanced clinical
understanding challenges transcend model scale. Few-shot prompting provides
substantial improvements for SLMs (up to 14.6\%), while LLM gains are more
variable. Our work highlights the potential of SLMs in mental health
understanding, showing they can be effective privacy-preserving tools for
analyzing sensitive online text data. In particular, their ability to quickly
adapt and specialize with minimal data through few-shot learning positions them
as promising candidates for scalable mental health screening tools.

</details>


### [14] [Integrating External Tools with Large Language Models to Improve Accuracy](https://arxiv.org/abs/2507.08034)
*Nripesh Niketan,Hadj Batatia*

Main category: cs.CL

> 本文提出了一种集成外部工具以增强LLMs在教育环境中答案质量的框架。

<details>
  <summary>Details</summary>

**Motivation:** 为了提高LLMs在教育环境中的查询能力，解决其在缺乏相关上下文信息时容易提供低质量回答或幻觉的问题。

**Method:** 本论文提出了一种框架，允许LLMs访问外部API以请求相关信息，并增强了计算能力，如计算器或日历等功能。

**Result:** 实验结果表明，该框架在数学推理和科学推理问题上的准确率显著高于现有的语言模型，分别达到了83%和88%。

**Conclusion:** 该框架的成功展示了围绕LLMs创建复杂计算生态系统的可能性，使其应用更为自然，支持更多任务和活动。

**Abstract:** This paper deals with improving querying large language models (LLMs). It is
well-known that without relevant contextual information, LLMs can provide poor
quality responses or tend to hallucinate. Several initiatives have proposed
integrating LLMs with external tools to provide them with up-to-date data to
improve accuracy. In this paper, we propose a framework to integrate external
tools to enhance the capabilities of LLMs in answering queries in educational
settings. Precisely, we develop a framework that allows accessing external APIs
to request additional relevant information. Integrated tools can also provide
computational capabilities such as calculators or calendars. The proposed
framework has been evaluated using datasets from the Multi-Modal Language
Understanding (MMLU) collection. The data consists of questions on mathematical
and scientific reasoning. Results compared to state-of-the-art language models
show that the proposed approach significantly improves performance. Our Athena
framework achieves 83% accuracy in mathematical reasoning and 88% in scientific
reasoning, substantially outperforming all tested models including GPT-4o,
LLaMA-Large, Mistral-Large, Phi-Large, and GPT-3.5, with the best baseline
model (LLaMA-Large) achieving only 67% and 79% respectively. These promising
results open the way to creating complex computing ecosystems around LLMs to
make their use more natural to support various tasks and activities.

</details>


### [15] [Barriers in Integrating Medical Visual Question Answering into Radiology Workflows: A Scoping Review and Clinicians' Insights](https://arxiv.org/abs/2507.08036)
*Deepali Mishra,Chaklam Silpasuwanchai,Ashutosh Modi,Madhumita Sushil,Sorayouth Chumnanvej*

Main category: cs.CL

> 这项研究通过文献综述和临床医生调查，探讨了医学视觉问答（MedVQA）的实际效用和挑战。研究发现MedVQA在临床中的应用潜力很大，但仍存在问题，特别是在多模态分析、患者背景整合和评估方法上。

<details>
  <summary>Details</summary>

**Motivation:** 尽管模型和技术数据集有所进步，但MedVQA在临床工作流程中的整合依然有限。这项研究系统地回顾了2018年至2024年间发表的68篇论文，并对来自印度和泰国的50位临床医生进行了调查，以探讨MedVQA的实际应用性、挑战和空白。

**Method:** 研究采用了Arksey和O'Malley的范围审查框架，结合了文献回顾和临床医生调查两种方法。文件回顾用于识别放射学工作流程中的关键概念、进展和研究空白；而临床医生调查用于收集他们对MedVQA临床相关性的看法。

**Result:** 研究揭示了大约60%的问题答案对诊断没有帮助，且缺乏临床相关性。大多数数据集和模型不支持多视角、多分辨率成像、电子病历集成或领域知识，这些都是临床诊断所必需的。此外，当前的评估指标和临床需求之间存在明显不符。有29.8%的临床医生认为MedVQA系统非常有用，而主要关切包括缺乏病史或专业知识、更倾向于手动整理的数据集和多视角成像支持等。

**Conclusion:** 尽管MedVQA显示出强大的潜力，但要实现有效的临床整合，必须解决诸如有限的多模态分析、缺乏患者背景和评估方法失准等挑战。

**Abstract:** Medical Visual Question Answering (MedVQA) is a promising tool to assist
radiologists by automating medical image interpretation through question
answering. Despite advances in models and datasets, MedVQA's integration into
clinical workflows remains limited. This study systematically reviews 68
publications (2018-2024) and surveys 50 clinicians from India and Thailand to
examine MedVQA's practical utility, challenges, and gaps. Following the Arksey
and O'Malley scoping review framework, we used a two-pronged approach: (1)
reviewing studies to identify key concepts, advancements, and research gaps in
radiology workflows, and (2) surveying clinicians to capture their perspectives
on MedVQA's clinical relevance. Our review reveals that nearly 60% of QA pairs
are non-diagnostic and lack clinical relevance. Most datasets and models do not
support multi-view, multi-resolution imaging, EHR integration, or domain
knowledge, features essential for clinical diagnosis. Furthermore, there is a
clear mismatch between current evaluation metrics and clinical needs. The
clinician survey confirms this disconnect: only 29.8% consider MedVQA systems
highly useful. Key concerns include the absence of patient history or domain
knowledge (87.2%), preference for manually curated datasets (51.1%), and the
need for multi-view image support (78.7%). Additionally, 66% favor models
focused on specific anatomical regions, and 89.4% prefer dialogue-based
interactive systems. While MedVQA shows strong potential, challenges such as
limited multimodal analysis, lack of patient context, and misaligned evaluation
approaches must be addressed for effective clinical integration.

</details>


### [16] [CRISP: Complex Reasoning with Interpretable Step-based Plans](https://arxiv.org/abs/2507.08037)
*Matan Vetzler,Koren Lazar,Guy Uziel,Eran Hirsch,Ateret Anaby-Tavor,Leshem Choshen*

Main category: cs.CL

> CRISP是一种用于多个领域的数学推理和代码生成的高阶计划数据集，展示了一种通过微调小模型以生成高质量计划的方法，这种方法比大型模型的少量提示方法更有效，并且比链式思维推理方法有显著提升。

<details>
  <summary>Details</summary>

**Motivation:** 作者提出CRISP（Complex Reasoning with Interpretable Step-based Plans）数据集，旨在解决大型语言模型在高阶推理中的不足，并挑战现有的假设，即语言模型仅通过少量提示即可生成有效的计划。

**Method:** 提出了一种自动并严格验证高阶计划的方法，这些计划通过小型模型的微调生成，比大型模型的少量提示方法生成的计划更高质量。

**Result:** 实验表明，在一个领域微调的模型在其他领域也能生成更好的计划，展示了学习规划能力的泛化性。

**Conclusion:** CRISP数据集的使用和在小型模型上的微调能够极大地提高计划的质量，这比传统的少量提示方法更具优势，并且也有优于Chain-of-Thought推理方法的表现。

**Abstract:** Recent advancements in large language models (LLMs) underscore the need for
stronger reasoning capabilities to solve complex problems effectively. While
Chain-of-Thought (CoT) reasoning has been a step forward, it remains
insufficient for many domains. A promising alternative is explicit high-level
plan generation, but existing approaches largely assume that LLMs can produce
effective plans through few-shot prompting alone, without additional training.
In this work, we challenge this assumption and introduce CRISP (Complex
Reasoning with Interpretable Step-based Plans), a multi-domain dataset of
high-level plans for mathematical reasoning and code generation. The plans in
CRISP are automatically generated and rigorously validated--both intrinsically,
using an LLM as a judge, and extrinsically, by evaluating their impact on
downstream task performance. We demonstrate that fine-tuning a small model on
CRISP enables it to generate higher-quality plans than much larger models using
few-shot prompting, while significantly outperforming Chain-of-Thought
reasoning. Furthermore, our out-of-domain evaluation reveals that fine-tuning
on one domain improves plan generation in the other, highlighting the
generalizability of learned planning capabilities.

</details>


### [17] [AblationBench: Evaluating Automated Planning of Ablations in Empirical AI Research](https://arxiv.org/abs/2507.08038)
*Talor Abramovich,Gal Chechik*

Main category: cs.CL

> 这篇论文介绍了一个名为AblationBench的基准套件，用于评估代理在规划消融实验方面的性能。包含两个任务，一个是帮助科研人员提出消融实验（AuthorAblation），另一个是帮助审稿人发现论文中缺失的消融实验（ReviewerAblation）。研究发现，目前最先进的语言模型在完成这些任务上仍面临挑战。

<details>
  <summary>Details</summary>

**Motivation:** 随着基于语言模型的自主代理在科研领域的应用日益增多，论文旨在通过设计消融实验来推动AI研究，特别是在支持或自动化科研过程方面。为了评估这些代理的能力，作者构建了一个名为AblationBench的基准测试套件，专注于评估代理在规划消融实验中的表现。

**Method:** 创建了AblationBench基准套件，包含AuthorAblation和ReviewerAblation两个任务，用于帮助作者提出消融实验和帮助审稿人发现完整论文中的缺失的消融实验。针对这两个任务，开发了基于语言模型的裁判，形成自动评估框架。

**Result:** 研究采用了两个任务来评估代理在规划消融实验方面的能力，分别是AuthorAblation和ReviewerAblation，分别包含83个和350个实例，其中最优秀的语言模型系统平均仅能识别原始消融实验的29%。实验结果表明了这些任务对当前语言模型的挑战性，同时也发现基于思考链的提示方法优于现有的基于代理的方法。

**Conclusion:** 通过实验发现，尽管语言模型在很多任务上表现出色，但在设计和识别消融实验上仍然存在较大困难，表明了在这一特定研究任务上的技术限制和未来的改进方向。同时，探索发现思考链提示方法能够带来更好的性能表现。

**Abstract:** Autonomous agents built on language models (LMs) are showing increasing
popularity in many fields, including scientific research. AI co-scientists aim
to support or automate parts of the research process using these agents. A key
component of empirical AI research is the design of ablation experiments. To
this end, we introduce AblationBench, a benchmark suite for evaluating agents
on ablation planning tasks in empirical AI research. It includes two tasks:
AuthorAblation, which helps authors propose ablation experiments based on a
method section and contains 83 instances, and ReviewerAblation, which helps
reviewers find missing ablations in a full paper and contains 350 instances.
For both tasks, we develop LM-based judges that serve as an automatic
evaluation framework. Our experiments with frontier LMs show that these tasks
remain challenging, with the best-performing LM system identifying only 29% of
the original ablations on average. Lastly, we analyze the limitations of
current LMs on these tasks, and find that chain-of-thought prompting
outperforms the currently existing agent-based approach.

</details>


### [18] [Krul: Efficient State Restoration for Multi-turn Conversations with Dynamic Cross-layer KV Sharing](https://arxiv.org/abs/2507.08045)
*Junyi Wen,Junyuan Liang,Zicong Hong,Wuhui Chen,Zibin Zheng*

Main category: cs.CL

> Krul提出了一种多轮对话的大语言模型推理系统，通过创新性地使用动态压缩策略选择、异构注意相似性估计和无‘气泡’恢复调度器，实现了更低的TTFT和KV缓存存储需求。

<details>
  <summary>Details</summary>

**Motivation:** 由于对所有历史Token重新计算或加载全量KV缓存的高昂成本，高效多轮对话中的状态恢复是迫切需要解决的问题。当前方法通过高度类似的注意力模式对相邻层的KV缓存进行压缩，忽略了不同对话之间的注意力模式相似度差异。

**Method:** Krul系统通过动态选择压缩策略、减少注意相似性计算和存储开销的异构注意相似性估计器以及通过平衡重新计算和加载流来减少潜在的‘气泡’问题的无气泡恢复调度器实现了高效的KV缓存恢复。

**Result:** 实验表明，与最先进的方法相比，Krul在不降低生成质量的情况下，首次Token产出时间（TTFT）减少了1.5倍至2.68倍，KV缓存存储减少了1.33倍至2.35倍。

**Conclusion:** Krul系统解决了多轮对话中KV缓存恢复的问题，通过动态选择压缩策略等创新，实现高效且准确的KV缓存恢复，同时显著减少了首次Token产出时间和KV缓存存储需求。

**Abstract:** Efficient state restoration in multi-turn conversations with large language
models (LLMs) remains a critical challenge, primarily due to the overhead of
recomputing or loading full key-value (KV) caches for all historical tokens. To
address this, existing approaches compress KV caches across adjacent layers
with highly similar attention patterns. However, these methods often apply a
fixed compression scheme across all conversations, selecting the same layer
pairs for compression without considering conversation-specific attention
dynamics. This static strategy overlooks variability in attention pattern
similarity across different conversations, which can lead to noticeable
accuracy degradation.
  We present Krul, a multi-turn LLM inference system that enables accurate and
efficient KV cache restoration. Krul dynamically selects compression strategies
based on attention similarity across layer pairs and uses a
recomputation-loading pipeline to restore the KV cache. It introduces three key
innovations: 1) a preemptive compression strategy selector to preserve critical
context for future conversation turns and selects a customized strategy for the
conversation; 2) a token-wise heterogeneous attention similarity estimator to
mitigate the attention similarity computation and storage overhead during model
generation; 3) a bubble-free restoration scheduler to reduce potential bubbles
brought by the imbalance of recomputing and loading stream due to compressed KV
caches. Empirical evaluations on real-world tasks demonstrate that Krul
achieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x
reduction in KV cache storage compared to state-of-the-art methods without
compromising generation quality.

</details>


### [19] [GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs](https://arxiv.org/abs/2507.08107)
*Sebastian Walter,Hannah Bast*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** We propose a new approach for generating SPARQL queries on RDF knowledge
graphs from natural language questions or keyword queries, using a large
language model. Our approach does not require fine-tuning. Instead, it uses the
language model to explore the knowledge graph by strategically executing SPARQL
queries and searching for relevant IRIs and literals. We evaluate our approach
on a variety of benchmarks (for knowledge graphs of different kinds and sizes)
and language models (of different scales and types, commercial as well as
open-source) and compare it with existing approaches. On Wikidata we reach
state-of-the-art results on multiple benchmarks, despite the zero-shot setting.
On Freebase we come close to the best few-shot methods. On other, less commonly
evaluated knowledge graphs and benchmarks our approach also performs well
overall. We conduct several additional studies, like comparing different ways
of searching the graphs, incorporating a feedback mechanism, or making use of
few-shot examples.

</details>


### [20] [Audit, Alignment, and Optimization of LM-Powered Subroutines with Application to Public Comment Processing](https://arxiv.org/abs/2507.08109)
*Reilly Raab,Mike Parker,Dan Nally,Sadie Montgomery,Anastasia Bernat,Sai Munikoti,Sameera Horawalavithana*

Main category: cs.CL

> 本文提出了一种框架，用于在传统异步代码中声明静态类型的由语言模型支持的子程序，通过提供透明、可审计的方式，减少风险并改进决策流程。该框架被封装为库以支持广泛的应用，例如在处理公共意见时的应用。

<details>
  <summary>Details</summary>

**Motivation:** 语言模型虽然有助于加速文本处理任务，但安全、可解释性和偏见问题阻碍了其实际应用。作者希望通过一种透明、可审计的方式负责任地利用语言模型，使人类专家能够专注于决策制定。

**Method:** 作者提出了一种框架，使语言模型生成的子程序可以集成到传统异步代码中，并通过少量的人类专家反馈在线改进性能。

**Result:** 该框架用于评估公共意见处理系统，尤其是在1969年《国家环境保护法》要求的情况下。通过将系统输出与人工注释的历史“地面真相”数据进行量化对比来评估表现。

**Conclusion:** 本文主要展示了一种可能的框架，通过引入静态类型定义和反馈机制，使得语言模型能够以一种可长期支持的方式应用到实际决策过程中。

**Abstract:** The advent of language models (LMs) has the potential to dramatically
accelerate tasks that may be cast to text-processing; however, real-world
adoption is hindered by concerns regarding safety, explainability, and bias.
How can we responsibly leverage LMs in a transparent, auditable manner --
minimizing risk and allowing human experts to focus on informed decision-making
rather than data-processing or prompt engineering? In this work, we propose a
framework for declaring statically typed, LM-powered subroutines (i.e.,
callable, function-like procedures) for use within conventional asynchronous
code -- such that sparse feedback from human experts is used to improve the
performance of each subroutine online (i.e., during use). In our
implementation, all LM-produced artifacts (i.e., prompts, inputs, outputs, and
data-dependencies) are recorded and exposed to audit on demand. We package this
framework as a library to support its adoption and continued development. While
this framework may be applicable across several real-world decision workflows
(e.g., in healthcare and legal fields), we evaluate it in the context of public
comment processing as mandated by the 1969 National Environmental Protection
Act (NEPA): Specifically, we use this framework to develop "CommentNEPA," an
application that compiles, organizes, and summarizes a corpus of public
commentary submitted in response to a project requiring environmental review.
We quantitatively evaluate the application by comparing its outputs (when
operating without human feedback) to historical ``ground-truth'' data as
labelled by human annotators during the preparation of official environmental
impact statements.

</details>


### [21] [Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores](https://arxiv.org/abs/2507.08143)
*Vivek Chari,Benjamin Van Durme*

Main category: cs.CL

> Compactor是一种无参数、与查询无关的KV压缩策略，使用近似杠杆分数确定标记的重要性，减少了内存负担并保持性能不变。

<details>
  <summary>Details</summary>

**Motivation:** To reduce the memory requirement of the KV cache in Large Language Models (LLMs) without牺牲性能.

**Method:** Compactor, a parameter-free, query-agnostic KV compression strategy using approximate leverage scores to determine token importance.

**Result:** Compactor能够将存储量减少一半，同时保持性能不变。在Longbench上，它实现了完整的KV性能，减少了63%的内存负担。在27个合成和现实世界任务中也证明了其有效性和通用性。

**Conclusion:** Compactor是一种有效的KV缓存压缩方法，能够在减少内存使用的同时保持性能，适用于不同模型和任务。

**Abstract:** Modern Large Language Models (LLMs) are increasingly trained to support very
large context windows. Unfortunately the ability to use long contexts in
generation is complicated by the large memory requirement of the KV cache,
which scales linearly with the context length. This memory footprint is often
the dominant resource bottleneck in real-world deployments, limiting throughput
and increasing serving cost. One way to address this is by compressing the KV
cache, which can be done either with knowledge of the question being asked
(query-aware) or without knowledge of the query (query-agnostic). We present
Compactor, a parameter-free, query-agnostic KV compression strategy that uses
approximate leverage scores to determine token importance. We show that
Compactor can achieve the same performance as competing methods while retaining
1/2 the tokens in both synthetic and real-world context tasks, with minimal
computational overhead. We further introduce a procedure for context-calibrated
compression, which allows one to infer the maximum compression ratio a given
context can support. Using context-calibrated compression, we show that
Compactor achieves full KV performance on Longbench while reducing the KV
memory burden by 63%, on average. To demonstrate the efficacy and
generalizability of our approach, we apply Compactor to 27 synthetic and
real-world tasks from RULER and Longbench, with models from both the Qwen 2.5
and Llama 3.1 families.

</details>


### [22] [Distilling Empathy from Large Language Models](https://arxiv.org/abs/2507.08151)
*Henry J. Xie,Jinghan Zhang,Xinhao Zhang,Kunpeng Liu*

Main category: cs.CL

> 本次研究专注于从大型语言模型向小型语言模型迁移同理心的能力，运用独特方法使SLMs在同理心响应上表现优于原始版本。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于，我们需要将大型语言模型（LLMs）中的同理心能力有效迁移到小型语言模型（SLMs）中，以确保它们在资源受限环境下仍能进行高质量的人机交互。

**Method:** 我们的研究方法包括一个两步微调过程以及使用从LLMs中提取的同理心对话数据集。我们不仅进行了基本的直接提示，还提出了四个独特的提示集，专门用于同理心提升。

**Result:** 实验结果显示，在利用两步微调过程和专为同理心改善设计的提示集增强的数据集后，小型语言模型生成同理心响应的胜率高达90%，比基础版本SLMs提高了10%。

**Conclusion:** 我们的研究结论为同理心的有效迁移提供了新的见解，通过精心设计的方法可以显著提升小型语言模型的同理心能力。

**Abstract:** The distillation of knowledge from Large Language Models (LLMs) into Smaller
Language Models (SLMs), preserving the capabilities and performance of LLMs
while reducing model size, has played a key role in the proliferation of LLMs.
Because SLMs are considerably smaller than LLMs, they are often utilized in
domains where human interaction is frequent but resources are highly
constrained, e.g., smart phones. Therefore, it is crucial to ensure that
empathy, a fundamental aspect of positive human interactions, already instilled
into LLMs, is retained by SLMs after distillation. In this paper, we develop a
comprehensive approach for effective empathy distillation from LLMs into SLMs.
Our approach features a two-step fine-tuning process that fully leverages
datasets of empathetic dialogue responses distilled from LLMs. We explore
several distillation methods beyond basic direct prompting and propose four
unique sets of prompts for targeted empathy improvement to significantly
enhance the empathy distillation process. Our evaluations demonstrate that SLMs
fine-tuned through the two-step fine-tuning process with distillation datasets
enhanced by the targeted empathy improvement prompts significantly outperform
the base SLM at generating empathetic responses with a win rate of 90%. Our
targeted empathy improvement prompts substantially outperform the basic direct
prompting with a 10% improvement in win rate.

</details>


### [23] [TruthTorchLM: A Comprehensive Library for Predicting Truthfulness in LLM Outputs](https://arxiv.org/abs/2507.08203)
*Duygu Nur Yaldiz,Yavuz Faruk Bakman,Sungmin Kang,Alperen Öziş,Hayrettin Eren Yildiz,Mitash Ashish Shah,Zhiqi Huang,Anoop Kumar,Alfy Samuel,Daben Liu,Sai Praneeth Karimireddy,Salman Avestimehr*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Generative Large Language Models (LLMs)inevitably produce untruthful
responses. Accurately predicting the truthfulness of these outputs is critical,
especially in high-stakes settings. To accelerate research in this domain and
make truthfulness prediction methods more accessible, we introduce TruthTorchLM
an open-source, comprehensive Python library featuring over 30 truthfulness
prediction methods, which we refer to as Truth Methods. Unlike existing
toolkits such as Guardrails, which focus solely on document-grounded
verification, or LM-Polygraph, which is limited to uncertainty-based methods,
TruthTorchLM offers a broad and extensible collection of techniques. These
methods span diverse tradeoffs in computational cost, access level (e.g.,
black-box vs white-box), grounding document requirements, and supervision type
(self-supervised or supervised). TruthTorchLM is seamlessly compatible with
both HuggingFace and LiteLLM, enabling support for locally hosted and API-based
models. It also provides a unified interface for generation, evaluation,
calibration, and long-form truthfulness prediction, along with a flexible
framework for extending the library with new methods. We conduct an evaluation
of representative truth methods on three datasets, TriviaQA, GSM8K, and
FactScore-Bio. The code is available at https://github.com/Ybakman/TruthTorchLM

</details>


### [24] [Simple Mechanistic Explanations for Out-Of-Context Reasoning](https://arxiv.org/abs/2507.08218)
*Atticus Wang,Joshua Engels,Oliver Clive-Griffin*

Main category: cs.CL

> 研究通过分析语言模型的OOCR现象，提出了一种简单的解释机制—基于LoRA技术的细调过程通过添加一个引导向量引导模型走向普遍概念，这一方法不仅在微调任务中有效，还在许多其他相关概念领域诱发了出乎意料的一般化推广能力。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探索一种机制，通过这种机制，可以解释在文献中许多关于超出上下文理解推理(OOCR)现象的案例。这类现象涉及到微调后的语言模型在面对分布外数据时表现出的意外深入的一般化推广能力。具体来说，就是解释微调是如何使模型在多个相关概念领域展示出更好的任务表现。

**Method:** 本研究提出了一种基于LoRA（低秩适应）细调技术的简单解释：通过附加一个常量引导向量，引导模型趋向于普遍概念，以此说明细调过程如何实现OOCR现象。研究发现，可以直接从头训练这些任务的引导向量，这同样能诱导OOCR现象。在无条件添加引导向量的情况下，研究甚至发现，即便是看起来需要条件行为的任务（如模型回溯），也可以实现OOCR。

**Result:** 研究表明，即使在涉及条件行为的任务，如模型后门的情况下，无条件地添加引导向量也足以实现OOCR现象，揭示了在OOCR任务中细调期间学习的机制。

**Conclusion:** 本研究通过分析提供了一种关于在细调过程中学习到什么来进行OOCR任务的解释，为解答为什么语言模型可以进行超背景推理这一核心问题做出了贡献，这个问题对于语言模型的安全可靠部署至关重要。

**Abstract:** Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs
exhibit surprisingly deep out-of-distribution generalization. Rather than
learning shallow heuristics, they implicitly internalize and act on the
consequences of observations scattered throughout the fine-tuning data. In this
work, we investigate this phenomenon mechanistically and find that many
instances of OOCR in the literature have a simple explanation: the LoRA
fine-tuning essentially adds a constant steering vector, steering the model
towards a general concept. This improves performance on the fine-tuning task
and in many other concept-related domains, causing the surprising
generalization. Moreover, we can directly train steering vectors for these
tasks from scratch, which also induces OOCR. We find that our results hold even
for a task that seems like it must involve conditional behavior (model
backdoors); it turns out that unconditionally adding a steering vector is
sufficient. Overall, our work presents one explanation of what gets learned
during fine-tuning for OOCR tasks, contributing to the key question of why LLMs
can reason out of context, an advanced capability that is highly relevant to
their safe and reliable deployment.

</details>


### [25] [Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and Reading Comprehension?](https://arxiv.org/abs/2507.08232)
*KV Aditya Srivatsa,Kaushal Kumar Maurya,Ekaterina Kochmar*

Main category: cs.CL

> 该研究探讨了大型语言模型（LLMs）作为智能教学系统（ITSs）中的代理学生在模仿实际学生行为和特征方面的准确性。发现即使没有引导，强大的通用模型在每个年级上都表现优异，但需要新的训练和评估策略来实现一致的模拟。

<details>
  <summary>Details</summary>

**Motivation:** 旨在探讨大型语言模型（LLMs）在没有引导的情况下模仿实际学生的准确程度，包括它们的行为和特征。这引起了对LLMs作为代理学生在智能教学系统（ITSs）和问题测试开发中使用的广泛探讨。

**Method:** 我们收集了来自国家教育进步评估（NAEP）的489项数学和阅读理解题目，涉及4、8、12年级，并使用项目反应理论（IRT）模型将11种多样化的最新大型语言模型（LLMs）与实际学生群体置于同一能力尺度上。

**Result:** 研究结果表明，没有引导的情况下，强大的通用模型在每个年级上都较平均学生表现优异，而较弱或领域不匹配的模型可能偶有对齐。使用年级指导提示会对模型性能产生影响，但没有评估过的模型-提示对能够在主题和年级上一致地对齐到平均学生水平。

**Conclusion:** 结论指出了当前大型语言模型在作为实际学生的代理使用时存在的局限性，强调了发展新的训练和评估策略的必要性，并提供了基于研究结果选择可行代理模型的指南。

**Abstract:** Large Language Models (LLMs) are increasingly used as proxy students in the
development of Intelligent Tutoring Systems (ITSs) and in piloting test
questions. However, to what extent these proxy students accurately emulate the
behavior and characteristics of real students remains an open question. To
investigate this, we collected a dataset of 489 items from the National
Assessment of Educational Progress (NAEP), covering mathematics and reading
comprehension in grades 4, 8, and 12. We then apply an Item Response Theory
(IRT) model to position 11 diverse and state-of-the-art LLMs on the same
ability scale as real student populations. Our findings reveal that, without
guidance, strong general-purpose models consistently outperform the average
student at every grade, while weaker or domain-mismatched models may align
incidentally. Using grade-enforcement prompts changes models' performance, but
whether they align with the average grade-level student remains highly model-
and prompt-specific: no evaluated model-prompt pair fits the bill across
subjects and grades, underscoring the need for new training and evaluation
strategies. We conclude by providing guidelines for the selection of viable
proxies based on our findings.

</details>


### [26] [Exploring Gender Differences in Chronic Pain Discussions on Reddit](https://arxiv.org/abs/2507.08241)
*Ancita Maria Andrade,Tanvi Banerjee,Ramakrishna Mundugar*

Main category: cs.CL

> 本研究利用NLP技术和HAM-CNN模型，分析了男性与女性在疼痛体验上的差异，发现女性在情感表达上更为丰富，并揭示了某些疾病在性别上的分布差异。

<details>
  <summary>Details</summary>

**Motivation:** 早期对于疼痛的研究往往忽视了性别在疼痛体验中的作用，本研究旨在填补这一空白，利用先进的自然语言处理技术来探讨男性与女性在疼痛经历上的差异。

**Method:** 本研究采用了自然语言处理技术，特别是使用了隐含属性模型-卷积神经网络（HAM-CNN）来分类性别相关的文本数据，并依据用户名汇编帖子，以性别差异为重点分析个体的疼痛体验。

**Result:** 研究成功地使用HAM-CNN把帖子分类为男性和女性两种语料库，获得了0.86的F1分值。分析结果揭示了性别的语言差异，女性帖子更倾向于情感关注。此外，研究还发现偏头痛和鼻窦炎在女性中更为普遍，以及如何依据性别差异探究止痛药对个体的影响。

**Conclusion:** 研究结果强调了性别在疼痛研究中的重要性，并指出在疼痛管理和药物治疗上，需要考虑性别差异以达到更有效的治疗效果。

**Abstract:** Pain is an inherent part of human existence, manifesting as both physical and
emotional experiences, and can be categorized as either acute or chronic. Over
the years, extensive research has been conducted to understand the causes of
pain and explore potential treatments, with contributions from various
scientific disciplines. However, earlier studies often overlooked the role of
gender in pain experiences. In this study, we utilized Natural Language
Processing (NLP) to analyze and gain deeper insights into individuals' pain
experiences, with a particular focus on gender differences. We successfully
classified posts into male and female corpora using the Hidden Attribute
Model-Convolutional Neural Network (HAM-CNN), achieving an F1 score of 0.86 by
aggregating posts based on usernames. Our analysis revealed linguistic
differences between genders, with female posts tending to be more emotionally
focused. Additionally, the study highlighted that conditions such as migraine
and sinusitis are more prevalent among females and explored how pain medication
affects individuals differently based on gender.

</details>


### [27] [KAT-V1: Kwai-AutoThink Technical Report](https://arxiv.org/abs/2507.08297)
*Zizheng Zhan,Ken Deng,Huaixi Tang,Wen Xiang,Kun Wu,Weihao Li,Wenqiang Zhu,Jingxuan Xu,Lecheng Huang,Zongxian Feng,Shaojie Wang,Shangpeng Yan,Jiaheng Liu,Zhongyuan Peng,Zuchen Gao,Haoyang Huang,Ziqi Zhan,Yanan Wu,Yuanxing Zhang,Jian Yang,Guang Chen,Haotian Zhang,Bin Chen,Bing Yu*

Main category: cs.CL

> KAT是一种为解决推理密集型任务的过度思考问题而设计的自动思考训练型40B大型语言模型，通过各种创新策略和算法，在多个任务中展现出优于现有最佳模型的性能，并已成功应用于实际场景。

<details>
  <summary>Details</summary>

**Motivation:** 开发KAT的主要动机是解决在推理密集型任务中出现的过度思考问题，通过自动调节推理过程，提高任务效率和准确性。

**Method:** 文中提出了一种名为Kwaipilot-AutoThink（KAT）的开源400亿参数大型语言模型，该模型主要解决推理密集型任务中的过度思考问题。通过一种自动思考训练范式，KAT能够基于任务复杂性动态切换推理模式与非推理模式。具体来说，方法包括基于新型标签流水线和多代理合成策略构建双制度数据集，使用多令牌预测增强的知识蒸馏来高效地转移细粒度推理，以最小的预训练成本。此外，实施冷启动初始化策略引入基于多数投票信号和意图感知提示的模式选择先验。提出了一种增强的强化学习算法Step-SRPO，该算法将中间监督引入GRPO框架，提供结构化引导，既包括推理模式选择，也包括响应准确性。

**Result:** 广泛的实验结果表明，KAT在多个基准测试中一致匹配甚至超过当前最先进的模型（DeepSeek-R1-0528 和Qwen3-235B-A22B）在推理密集型任务中的表现，同时减少了高达约30%的令牌使用量。

**Conclusion:** KAT作为一款400亿参数的模型，在多项推理任务中展示出卓越的表现，并已在Kuaishou的内部编码助手Kwaipilot中成功部署，不仅提高了实际开发工作流的准确性和效率，还展示了自动思考模式的可扩展性。此外，团队正在训练一个拥有400亿激活参数的2000亿参数Mixture-of-Experts模型，早期结果表明性能和效率方面有显著提升。

**Abstract:** We present Kwaipilot-AutoThink (KAT), an open-source 40B large language model
developed to address the overthinking problem in reasoning-intensive tasks,
where an automatic thinking training paradigm is proposed to dynamically switch
between reasoning and non-reasoning modes based on task complexity.
Specifically, first, we construct the dual-regime dataset based on a novel
tagging pipeline and a multi-agent synthesis strategy, and then we apply
Multi-Token Prediction (MTP)-enhanced knowledge distillation, enabling
efficient and fine-grained reasoning transfer with minimal pretraining cost.
Besides, we implement a cold-start initialization strategy that introduces
mode-selection priors using majority-vote signals and intent-aware prompting.
Finally, we propose Step-SRPO, a reinforcement learning algorithm that
incorporates intermediate supervision into the GRPO framework, offering
structured guidance over both reasoning-mode selection and response accuracy.
Extensive experiments across multiple benchmarks demonstrate that KAT
consistently matches or even outperforms current state-of-the-art models,
including DeepSeek-R1-0528 and Qwen3-235B-A22B, across a wide range of
reasoning-intensive tasks while reducing token usage by up to approximately
30\%. Beyond academic evaluation, KAT has been successfully deployed in
Kwaipilot (i.e., Kuaishou's internal coding assistant), and improves real-world
development workflows with high accuracy, efficiency, and controllable
reasoning behaviors. Moreover, we are actively training a 200B
Mixture-of-Experts (MoE) with 40B activation parameters, where the early-stage
results already demonstrate promising improvements in performance and
efficiency, further showing the scalability of the AutoThink paradigm.

</details>


### [28] [Improving MLLM's Document Image Machine Translation via Synchronously Self-reviewing Its OCR Proficiency](https://arxiv.org/abs/2507.08309)
*Yupu Liang,Yaping Zhang,Zhiyang Zhang,Zhiyuan Chen,Yang Zhao,Lu Xiang,Chengqing Zong,Yu Zhou*

Main category: cs.CL

> 本文提出了一种改进的微调技术，旨在提高多模态大型语言模型在文档图像机器翻译任务中的表现，同时保持其在单语OCR任务中的能力。

<details>
  <summary>Details</summary>

**Motivation:** 大型多模态语言模型（MLLMs）在文档图像任务，特别是光学字符识别（OCR）方面表现出色，但在文档图像机器翻译（DIMT）上的表现不佳。这是因为DIMT需要处理跨模态和跨语言的挑战。传统的监督微调方法导致模型忘记了已有单语能力，如OCR。为了应对这一挑战，本文提出了一种新的解决方案。

**Method:** 本文提出了一种新颖的微调范式——同步自我回顾(Synchronous Self-Reviewing, SSR)。该方法借鉴了“双语认知优势”的概念，通过首先生成OCR文本，然后生成翻译文本，使模型在学习跨语言翻译的同时，保留其强大的单语OCR能力。

**Result:** 实验结果显示，提出的SSR学习方法有助于减轻灾难性遗忘问题，提高了MLLMs在OCR和DIMT任务上的泛化能力。

**Conclusion:** 研究表明，通过同步自我回顾（SSR）的微调范式，可以改善大型多模态语言模型在OCR和DIMT任务上的性能，减少灾难性遗忘问题。

**Abstract:** Multimodal Large Language Models (MLLMs) have shown strong performance in
document image tasks, especially Optical Character Recognition (OCR). However,
they struggle with Document Image Machine Translation (DIMT), which requires
handling both cross-modal and cross-lingual challenges. Previous efforts to
enhance DIMT capability through Supervised Fine-Tuning (SFT) on the DIMT
dataset often result in the forgetting of the model's existing monolingual
abilities, such as OCR. To address these challenges, we introduce a novel
fine-tuning paradigm, named Synchronously Self-Reviewing (SSR) its OCR
proficiency, inspired by the concept "Bilingual Cognitive Advantage".
Specifically, SSR prompts the model to generate OCR text before producing
translation text, which allows the model to leverage its strong monolingual OCR
ability while learning to translate text across languages. Comprehensive
experiments demonstrate the proposed SSR learning helps mitigate catastrophic
forgetting, improving the generalization ability of MLLMs on both OCR and DIMT
tasks.

</details>


### [29] [CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template Generation](https://arxiv.org/abs/2507.08325)
*Yinzhu Quan,Xinrui Li,Ying Chen*

Main category: cs.CL

> CRMAgent, a multi-agent system based on large language models, improves the quality of message templates and writing guidance for e-commerce merchants, enhancing audience match and marketing effectiveness.

<details>
  <summary>Details</summary>

**Motivation:** Most merchants lack the expertise and scalable tools to craft persuasive outbound messages, leading to suboptimal customer engagement and conversion.

**Method:** CRMAgent utilizes group-based learning, retrieval-and-adaptation, and a rule-based fallback to generate high-quality message templates and writing guidance.

**Result:** Experiments show that CRMAgent outperforms original merchant templates, achieving significant improvements in audience match and marketing effectiveness metrics.

**Conclusion:** CRMAgent provides a scalable solution to improve CRM messaging through advanced language models, thereby boosting marketing success for e-commerce merchants.

**Abstract:** In e-commerce private-domain channels such as instant messaging and e-mail,
merchants engage customers directly as part of their Customer Relationship
Management (CRM) programmes to drive retention and conversion. While a few top
performers excel at crafting outbound messages, most merchants struggle to
write persuasive copy because they lack both expertise and scalable tools. We
introduce CRMAgent, a multi-agent system built on large language models (LLMs)
that generates high-quality message templates and actionable writing guidance
through three complementary modes. First, group-based learning enables the
agent to learn from a merchant's own top-performing messages within the same
audience segment and rewrite low-performing ones. Second,
retrieval-and-adaptation fetches templates that share the same audience segment
and exhibit high similarity in voucher type and product category, learns their
successful patterns, and adapts them to the current campaign. Third, a
rule-based fallback provides a lightweight zero-shot rewrite when no suitable
references are available. Extensive experiments show that CRMAgent consistently
outperforms merchants' original templates, delivering significant gains in both
audience-match and marketing-effectiveness metrics.

</details>


### [30] [MK2 at PBIG Competition: A Prompt Generation Solution](https://arxiv.org/abs/2507.08335)
*Yuzheng Xu,Tosho Hirasawa,Seiya Kawano,Shota Kato,Tadashi Kozuno*

Main category: cs.CL

> MK2系统通过提示工程将专利转化为产品创意，无需额外训练数据，在多项测试中表现优异，但在材料化学领域有待提升。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在利用MK2系统将现有专利转化为在三年内可行的产品创意，探索无需额外训练数据的提示工程的潜力。

**Method:** MK2方法是一个以提示为中心的流水线，其中Gemini 2.5负责起草并迭代编辑提示，将有用的片段从较弱的输出中嫁接过来；GPT-4.1使用该提示为每个专利创造一个想法；Qwen3-8B通过Elo循环评估选择最佳提示，整个过程无需额外的训练数据。

**Result:** 在三个领域，两种评估者类型和六个标准下，MK2在自动排行榜上名列前茅，并赢得了36个测试中的25个。然而，在材料化学领域表现不佳，显示出需要更深层次的专业领域基础。

**Conclusion:** 研究结果表明，轻量级的提示工程已经能够从专利中产生具有竞争力和商业相关性的创意，尽管在某些特定领域的表现需改进。

**Abstract:** The Patent-Based Idea Generation task asks systems to turn real patents into
product ideas viable within three years. We propose MK2, a prompt-centric
pipeline: Gemini 2.5 drafts and iteratively edits a prompt, grafting useful
fragments from weaker outputs; GPT-4.1 then uses this prompt to create one idea
per patent, and an Elo loop judged by Qwen3-8B selects the best prompt-all
without extra training data. Across three domains, two evaluator types, and six
criteria, MK2 topped the automatic leaderboard and won 25 of 36 tests. Only the
materials-chemistry track lagged, indicating the need for deeper domain
grounding; yet, the results show that lightweight prompt engineering has
already delivered competitive, commercially relevant ideation from patents.

</details>


### [31] [Distillation versus Contrastive Learning: How to Train Your Rerankers](https://arxiv.org/abs/2507.08336)
*Zhichao Xu,Zhiqi Huang,Shengyao Zhuang,Ashim Gupta,Vivek Srikumar*

Main category: cs.CL

> This paper empirically compares contrastive learning and knowledge distillation for training text rerankers, finding that distillation generally outperforms contrastive learning when a more powerful teacher model is available, but contrastive learning is a reliable alternative when it is not.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this study is to provide a clear empirical comparison of contrastive learning and knowledge distillation strategies for training cross-encoder rerankers in practical scenarios.

**Method:** This paper compares two strategies for training text rerankers: contrastive learning and knowledge distillation. It trains rerankers of different sizes and architectures using both methods on the same data set, with a strong contrastive learning model serving as the distillation teacher.

**Result:** The results indicate that knowledge distillation yields better in-domain and out-of-domain ranking performance than contrastive learning when distilling from a larger teacher model, but distilling from a teacher of the same capacity does not provide the same advantage, especially for out-of-domain tasks.

**Conclusion:** The conclusion recommends using knowledge distillation to train smaller rerankers if a larger, more powerful teacher model is available; otherwise, contrastive learning is a strong and reliable alternative.

**Abstract:** Training text rerankers is crucial for information retrieval. Two primary
strategies are widely used: contrastive learning (optimizing directly on
ground-truth labels) and knowledge distillation (transferring knowledge from a
larger reranker). While both have been studied in the literature, a clear
comparison of their effectiveness for training cross-encoder rerankers under
practical conditions is needed.
  This paper empirically compares these strategies by training rerankers of
different sizes and architectures using both methods on the same data, with a
strong contrastive learning model acting as the distillation teacher. Our
results show that knowledge distillation generally yields better in-domain and
out-of-domain ranking performance than contrastive learning when distilling
from a larger teacher model. This finding is consistent across student model
sizes and architectures. However, distilling from a teacher of the same
capacity does not provide the same advantage, particularly for out-of-domain
tasks. These findings offer practical guidance for choosing a training strategy
based on available teacher models. Therefore, we recommend using knowledge
distillation to train smaller rerankers if a larger, more powerful teacher is
accessible; in its absence, contrastive learning provides a strong and more
reliable alternative otherwise.

</details>


### [32] [What Factors Affect LLMs and RLLMs in Financial Question Answering?](https://arxiv.org/abs/2507.08339)
*Peng Wang,Xuesi Hu,Jiageng Wu,Yuntao Zou,Qiancheng Zhang,Dagang Li*

Main category: cs.CL

> 研究探讨了各种方法对大型语言模型和推理大型语言模型在金融领域问题回答任务上的影响，发现当前的提示方法和代理框架能增强LLMs的表现，而RLLMs固有的长期推理能力限制了传统方法对其表现的提升。

<details>
  <summary>Details</summary>

**Motivation:** 缺乏系统性探索如何最大限度地发挥LLMs和RLLMs在金融领域表现的方法。

**Method:** 使用五个大型语言模型和三个推理大型语言模型来评估提示方法、代理框架和多语言对齐方法在金融问题回答任务中的效果。

**Result:** 提示方法和代理框架通过模拟长期推理过程增强LLMs在金融问题回答中的表现，而RLLMs固有的长期推理能力限制了传统方法对其表现的提升，且多语言对齐方法主要提升了LLMs的多语言性能，对RLLMs影响较小。

**Conclusion:** 该研究为金融领域问题回答中的LLMs和RLLMs提供了重要的参考。

**Abstract:** Recently, the development of large language models (LLMs) and reasoning large
language models (RLLMs) have gained considerable attention from many
researchers. RLLMs enhance the reasoning capabilities of LLMs through Long
Chain-of-Thought (Long CoT) processes, significantly improving the performance
of LLMs in addressing complex problems. However, there are few works that
systematically explore what methods can fully unlock the performance of LLMs
and RLLMs within the financial domain. To investigate the impact of various
methods on LLMs and RLLMs, we utilize five LLMs and three RLLMs to assess the
effects of prompting methods, agentic frameworks, and multilingual alignment
methods on financial question-answering tasks. Our research findings indicate:
(1) Current prompting methods and agent frameworks enhance the performance of
LLMs in financial question answering by simulating Long CoT; (2) RLLMs possess
inherent Long CoT capabilities, which limits the effectiveness of conventional
methods in further enhancing their performance; (3) Current advanced
multilingual alignment methods primarily improve the multilingual performance
of LLMs by extending the reasoning length, which yields minimal benefits for
RLLMs. We hope that this study can serve as an important reference for LLMs and
RLLMs in the field of financial question answering.

</details>


### [33] [Beyond N-Grams: Rethinking Evaluation Metrics and Strategies for Multilingual Abstractive Summarization](https://arxiv.org/abs/2507.08342)
*Itai Mondshine,Tzuf Paz-Argaman,Reut Tsarfaty*

Main category: cs.CL

> The study evaluates n-gram-based and neural-based metrics across diverse languages, finding that neural-based metrics perform better, especially in low-resource and fusional languages.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to evaluate the suitability of n-gram-based and neural-based metrics across different languages and resource settings, addressing the uncertainty regarding their effectiveness beyond English.

**Method:** The study designs a large-scale evaluation using n-gram-based and neural-based metrics across eight languages from four language families to assess their correlation with human judgments in low and high-resource settings.

**Result:** The findings reveal that n-gram-based metrics have lower correlation with human judgments in fusional languages compared to isolating and agglutinative languages. Proper tokenization improved metric performance in fusional languages. Neural-based metrics, like COMET, generally outperformed other metrics, especially in low-resource languages.

**Conclusion:** The research underscores the limitations of n-gram-based evaluation metrics for fusional languages and emphasizes the need for development and use of neural-based metrics.

**Abstract:** Automatic n-gram based metrics such as ROUGE are widely used for evaluating
generative tasks such as summarization. While these metrics are considered
indicative (even if imperfect) of human evaluation for English, their
suitability for other languages remains unclear. To address this, we
systematically assess evaluation metrics for generation both n-gram-based and
neural based to evaluate their effectiveness across languages and tasks.
Specifically, we design a large-scale evaluation suite across eight languages
from four typological families: agglutinative, isolating, low-fusional, and
high-fusional, spanning both low- and high-resource settings, to analyze their
correlation with human judgments. Our findings highlight the sensitivity of
evaluation metrics to the language type. For example, in fusional languages,
n-gram-based metrics show lower correlation with human assessments compared to
isolating and agglutinative languages. We also demonstrate that proper
tokenization can significantly mitigate this issue for morphologically rich
fusional languages, sometimes even reversing negative trends. Additionally, we
show that neural-based metrics specifically trained for evaluation, such as
COMET, consistently outperform other neural metrics and better correlate with
human judgments in low-resource languages. Overall, our analysis highlights the
limitations of n-gram metrics for fusional languages and advocates for greater
investment in neural-based metrics trained for evaluation tasks.

</details>


### [34] [Exploring Design of Multi-Agent LLM Dialogues for Research Ideation](https://arxiv.org/abs/2507.08350)
*Keisuke Ueda,Wataru Hirota,Takuto Asakura,Takahiro Omi,Kosuke Takahashi,Kosuke Arima,Tatsuya Ishigaki*

Main category: cs.CL

> 研究分析了多代理LLM对话在科学创意生成中的作用，发现增加代理人数、加深交互及多样化角色均能丰富想法多样性，增加评估方多样性能提升创意可行性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管最近的研究表明，LLM之间的结构化对话可以改善生成想法的新颖性和可行性，但这种交互的最佳设计仍不清楚。研究旨在通过全面分析来填补这一空白。

**Method:** 通过比较不同代理角色配置、代理数量以及对话深度，研究多代理LLM对话对科学创意生成的新颖性和可行性的影响。实验设置包括由一个代理生成想法另一个代理进行评估，可以实现迭代改进。

**Result:** 研究结果显示，增加代理人数、加深交互深度和扩大代理角色差异性都可以丰富想法的多样性。特别是增加评估方多样性在创意—评估—修订循环中能够进一步提升最终提案的可行性。

**Conclusion:** 研究发现为构建有效的多代理LLM系统以进行科学创意提供了实用的指导。

**Abstract:** Large language models (LLMs) are increasingly used to support creative tasks
such as research idea generation. While recent work has shown that structured
dialogues between LLMs can improve the novelty and feasibility of generated
ideas, the optimal design of such interactions remains unclear. In this study,
we conduct a comprehensive analysis of multi-agent LLM dialogues for scientific
ideation. We compare different configurations of agent roles, number of agents,
and dialogue depth to understand how these factors influence the novelty and
feasibility of generated ideas. Our experimental setup includes settings where
one agent generates ideas and another critiques them, enabling iterative
improvement. Our results show that enlarging the agent cohort, deepening the
interaction depth, and broadening agent persona heterogeneity each enrich the
diversity of generated ideas. Moreover, specifically increasing critic-side
diversity within the ideation-critique-revision loop further boosts the
feasibility of the final proposals. Our findings offer practical guidelines for
building effective multi-agent LLM systems for scientific ideation. Our code is
available at https://github.com/g6000/MultiAgent-Research-Ideator.

</details>


### [35] [The Curious Case of Factuality Finetuning: Models' Internal Beliefs Can Improve Factuality](https://arxiv.org/abs/2507.08371)
*Benjamin Newman,Abhilasha Ravichander,Jaehun Jung,Rui Xin,Hamish Ivison,Yegor Kuznetsov,Pang Wei Koh,Yejin Choi*

Main category: cs.CL

> 研究表明，在语言模型中，使用模型生成且自身判断为事实性的数据进行微调比使用黄金事实数据更有助于减少幻觉。

<details>
  <summary>Details</summary>

**Motivation:** 语言模型容易产生幻觉，即生成事实性错误的文本。虽然可以在高质量的事实信息上进行微调来减少幻觉，但获取黄金事实数据的成本高昂，且训练不熟悉的数据可能导致更多的下游幻觉。研究的目标是确定从业者应该使用什么样的数据来减少语言模型中的幻觉。

**Method:** 我们研究了微调数据的事实性与语言模型在长文本生成任务中产生幻觉的频率之间的关系。我们比较了使用黄金事实数据和模型生成数据（模型认为是事实性数据）进行微调的效果，并评估了对这两种数据进行过滤策略后的微调表现。

**Result:** 我们发现，与使用黄金事实数据进行微调相比，使用模型认为事实性的生成数据进行微调反而对减少幻觉更有帮助。经过模型自判过滤后的模型生成数据微调往往比其他配置产生了更好的整体事实性。这一改进在三个研究领域中都有显现。

**Conclusion:** 模型自身对于数据事实性的判断能够提供一个强有力的信号，而且这一信号在多个研究领域中都有显著的效果。这表明，利用自身判断来提升数据的事实性对于减少模型幻觉是一个有效的策略。

**Abstract:** Language models are prone to hallucination - generating text that is
factually incorrect. Finetuning models on high-quality factual information can
potentially reduce hallucination, but concerns remain; obtaining factual gold
data can be expensive and training on correct but unfamiliar data may
potentially lead to even more downstream hallucination. What data should
practitioners finetune on to mitigate hallucinations in language models? In
this work, we study the relationship between the factuality of finetuning data
and the prevalence of hallucinations in long-form generation tasks.
Counterintuitively, we find that finetuning on factual gold data is not as
helpful as finetuning on model-generated data that models believe to be
factual. Next, we evaluate filtering strategies applied on both factual gold
data and model-generated data, and find that finetuning on model-generated data
that is filtered by models' own internal judgments often leads to better
overall factuality compared to other configurations: training on gold data
filtered by models' judgments, training on gold data alone, or training on
model-generated data that is supported by gold data. These factuality
improvements transfer across three domains we study, suggesting that a models'
own beliefs can provide a powerful signal for factuality.

</details>


### [36] [A Survey of Large Language Models in Discipline-specific Research: Challenges, Methods and Opportunities](https://arxiv.org/abs/2507.08425)
*Lu Xiang,Yang Zhao,Yaping Zhang,Chengqing Zong*

Main category: cs.CL

> A survey paper on Large Language Models (LLMs) highlights their integration and impact across various disciplines, analyzing technical methodologies and their application in solving discipline-specific tasks while pointing out challenges and future research directions.

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the underexplored systematic understanding of integrating LLMs into diverse disciplines, providing a comprehensive analysis of their applications and methodologies in interdisciplinary studies.

**Method:** This paper examines key methodologies for enhancing the adaptability and effectiveness of LLMs in various disciplines, such as supervised fine-tuning, retrieval-augmented generation, agent-based approaches, and tool-use integration.

**Result:** The paper explores the contributions of LLMs across various disciplines such as mathematics, physics, chemistry, biology, and the humanities and social sciences, highlighting their discipline-specific roles.

**Conclusion:** The survey highlights the prevailing challenges in the integration of LLMs and points out promising research directions, aiming to serve as a valuable resource for interdisciplinary researchers.

**Abstract:** Large Language Models (LLMs) have demonstrated their transformative potential
across numerous disciplinary studies, reshaping the existing research
methodologies and fostering interdisciplinary collaboration. However, a
systematic understanding of their integration into diverse disciplines remains
underexplored. This survey paper provides a comprehensive overview of the
application of LLMs in interdisciplinary studies, categorising research efforts
from both a technical perspective and with regard to their applicability. From
a technical standpoint, key methodologies such as supervised fine-tuning,
retrieval-augmented generation, agent-based approaches, and tool-use
integration are examined, which enhance the adaptability and effectiveness of
LLMs in discipline-specific contexts. From the perspective of their
applicability, this paper explores how LLMs are contributing to various
disciplines including mathematics, physics, chemistry, biology, and the
humanities and social sciences, demonstrating their role in discipline-specific
tasks. The prevailing challenges are critically examined and the promising
research directions are highlighted alongside the recent advances in LLMs. By
providing a comprehensive overview of the technical developments and
applications in this field, this survey aims to serve as an invaluable resource
for the researchers who are navigating the complex landscape of LLMs in the
context of interdisciplinary studies.

</details>


### [37] [ChainEdit: Propagating Ripple Effects in LLM Knowledge Editing through Logical Rule-Guided Chains](https://arxiv.org/abs/2507.08427)
*Zilu Dong,Xiangqing Shen,Zinong Yang,Rui Xia*

Main category: cs.CL

> ChainEdit框架通过结合知识图谱的逻辑规则与大型语言模型的逻辑推理能力，解决了知识编辑过程中的逻辑一致性问题，以实现系统性知识更新，并在评估中表现优异。

<details>
  <summary>Details</summary>

**Motivation:** 当前的知识编辑方法在大型语言模型中难以保持逻辑一致性，尤其是在传播与相关事实的涟漪效应时。

**Method:** 提出ChainEdit框架，该框架结合了知识图谱衍生的逻辑规则与大语言模型的逻辑推理能力，以实现系统的链式更新。通过从结构化知识库中自动抽取逻辑模式并与大语言模型的内部逻辑对齐，ChainEdit能够动态生成和编辑逻辑相关的知识簇。

**Result:** 实验表明，相较于基线方法，逻辑泛化性能提高了30%以上，同时保持了编辑的可靠性和特异性。

**Conclusion:** 通过知识感知的评估协议，该工作在处理涟漪效应时达成了逻辑一致性，并建立了新的性能标准。

**Abstract:** Current knowledge editing methods for large language models (LLMs) struggle
to maintain logical consistency when propagating ripple effects to associated
facts. We propose ChainEdit, a framework that synergizes knowledge
graph-derived logical rules with LLM logical reasoning capabilities to enable
systematic chain updates. By automatically extracting logical patterns from
structured knowledge bases and aligning them with LLMs' internal logics,
ChainEdit dynamically generates and edits logically connected knowledge
clusters. Experiments demonstrate an improvement of more than 30% in logical
generalization over baselines while preserving editing reliability and
specificity. We further address evaluation biases in existing benchmarks
through knowledge-aware protocols that disentangle external dependencies. This
work establishes new state-of-the-art performance on ripple effect while
ensuring internal logical consistency after knowledge editing.

</details>


### [38] [Finding Common Ground: Using Large Language Models to Detect Agreement in Multi-Agent Decision Conferences](https://arxiv.org/abs/2507.08440)
*Selina Heller,Mohamed Ibrahim,David Antony Selby,Sebastian Vollmer*

Main category: cs.CL

> 本文提出了一种基于大规模语言模型（LLMs）的多智能体系统，用于模拟决策会议，检测参与者之间的共识，实验结果和结论验证了该方法在模拟群体决策过程中的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于展示LLM在其模拟现实世界场景的潜力，特别是通过多智能体系统模拟群体互动。通过这些研究，希望为跨领域的专家征询研讨会提供决策支持。

**Method:** 本文介绍了一种基于大规模语言模型（LLMs）的多智能体系统，旨在模拟决策会议，并特别关注检测参与者之间的共识。研究评估了六个不同的大规模语言模型在两个任务上的表现：立场检测（识别智能体对特定问题的立场）和立场极性检测（确定智能体的观点情感）。

**Result:** 实验结果显示，大规模语言模型能够在动态和复杂的辩论中可靠地检测到共识。在系统中加入一个共识检测智能体可以提高小组辩论的效率，并提升整体讨论的质量和连贯性，使模拟会议的结果和决策过程与真实世界中的决策会议相当。

**Conclusion:** 这些发现证明了LLM多智能体系统模拟群体决策过程的潜力。同时也表明这些系统在支持专家征询研讨会的决策中可能发挥重要作用。

**Abstract:** Decision conferences are structured, collaborative meetings that bring
together experts from various fields to address complex issues and reach a
consensus on recommendations for future actions or policies. These conferences
often rely on facilitated discussions to ensure productive dialogue and
collective agreement. Recently, Large Language Models (LLMs) have shown
significant promise in simulating real-world scenarios, particularly through
collaborative multi-agent systems that mimic group interactions. In this work,
we present a novel LLM-based multi-agent system designed to simulate decision
conferences, specifically focusing on detecting agreement among the participant
agents. To achieve this, we evaluate six distinct LLMs on two tasks: stance
detection, which identifies the position an agent takes on a given issue, and
stance polarity detection, which identifies the sentiment as positive,
negative, or neutral. These models are further assessed within the multi-agent
system to determine their effectiveness in complex simulations. Our results
indicate that LLMs can reliably detect agreement even in dynamic and nuanced
debates. Incorporating an agreement-detection agent within the system can also
improve the efficiency of group debates and enhance the overall quality and
coherence of deliberations, making them comparable to real-world decision
conferences regarding outcome and decision-making. These findings demonstrate
the potential for LLM-based multi-agent systems to simulate group
decision-making processes. They also highlight that such systems could be
instrumental in supporting decision-making with expert elicitation workshops
across various domains.

</details>


### [39] [Diagnosing Failures in Large Language Models' Answers: Integrating Error Attribution into Evaluation Framework](https://arxiv.org/abs/2507.08459)
*Zishan Xu,Shuyi Xie,Qingsong Lv,Shupei Xiao,Linlin Song,Sui Wenjuan,Fan Lin*

Main category: cs.CL

> 本文针对现有的大型语言模型性能分析和错误诊断存在问题，提出一个错误归因框架和对应的数据集AttriData，以及基于此数据集训练的模型MisAttributionLLM。该模型能够有效实现错误的分类、归因以及提供反馈，显著提升了错误诊断的自动化和系统化。

<details>
  <summary>Details</summary>

**Motivation:** 主流的大型语言模型平台上每天生成大量用户与模型的交互数据。为了高效地分析模型性能和诊断失败的回答，迫切需要一个自动化的框架来系统地分类和归因错误。然而，现有的评估模型缺乏错误归因功能。为了填补这一空白，提高错误诊断的自动化和系统化水平，该研究旨在开发一个新的框架和关联模型。

**Method:** 本文提出了一个全面的错误归因框架，该框架包括6个主要类别和15个二级类别，用于系统地分类和归因错误。基于此框架，作者构建了AttriData数据集，该数据集专门用于错误归因，包含了误归因及其对应的分数和反馈。此外，作者还提出了MisAttributionLLM模型，该模型在AttriData数据集上进行了微调，是首个能够同时生成分数、误归因和反馈的通用评判模型。

**Result:** 通过实验和分析，验证了模型在错误归因上的有效性，MisAttributionLLM模型展示出了强大的性能，不但能够准确地判断回答的错误类型，还能提供对应的分数和反馈，体现出模型的稳健性和泛化能力。

**Conclusion:** 通过广泛的实验和分析验证表明，所提出的方法是有效的且稳定的，可以为大型语言模型的性能分析和错误诊断提供有力的支持。

**Abstract:** With the widespread application of Large Language Models (LLMs) in various
tasks, the mainstream LLM platforms generate massive user-model interactions
daily. In order to efficiently analyze the performance of models and diagnose
failures in their answers, it is essential to develop an automated framework to
systematically categorize and attribute errors. However, existing evaluation
models lack error attribution capability. In this work, we establish a
comprehensive Misattribution Framework with 6 primary and 15 secondary
categories to facilitate in-depth analysis. Based on this framework, we present
AttriData, a dataset specifically designed for error attribution, encompassing
misattribution, along with the corresponding scores and feedback. We also
propose MisAttributionLLM, a fine-tuned model on AttriData, which is the first
general-purpose judge model capable of simultaneously generating score,
misattribution, and feedback. Extensive experiments and analyses are conducted
to confirm the effectiveness and robustness of our proposed method.

</details>


### [40] [Using Large Language Models for Legal Decision-Making in Austrian Value-Added Tax Law: An Experimental Study](https://arxiv.org/abs/2507.08468)
*Marina Luketina,Andrea Benkel,Christoph G. Schuetz*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** This paper provides an experimental evaluation of the capability of large
language models (LLMs) to assist in legal decision-making within the framework
of Austrian and European Union value-added tax (VAT) law. In tax consulting
practice, clients often describe cases in natural language, making LLMs a prime
candidate for supporting automated decision-making and reducing the workload of
tax professionals. Given the requirement for legally grounded and
well-justified analyses, the propensity of LLMs to hallucinate presents a
considerable challenge. The experiments focus on two common methods for
enhancing LLM performance: fine-tuning and retrieval-augmented generation
(RAG). In this study, these methods are applied on both textbook cases and
real-world cases from a tax consulting firm to systematically determine the
best configurations of LLM-based systems and assess the legal-reasoning
capabilities of LLMs. The findings highlight the potential of using LLMs to
support tax consultants by automating routine tasks and providing initial
analyses, although current prototypes are not ready for full automation due to
the sensitivity of the legal domain. The findings indicate that LLMs, when
properly configured, can effectively support tax professionals in VAT tasks and
provide legally grounded justifications for decisions. However, limitations
remain regarding the handling of implicit client knowledge and context-specific
documentation, underscoring the need for future integration of structured
background information.

</details>


### [41] [ILT-Iterative LoRA Training through Focus-Feedback-Fix for Multilingual Speech Recognition](https://arxiv.org/abs/2507.08477)
*Qingliang Meng,Hao Wu,Wei Liang,Wei Xu,Qing Zhao*

Main category: cs.CL

> 本文提出了一种迭代LoRA训练方法，结合迭代伪标签策略，有效解决了过拟合问题，通过三个阶段的训练实验，证明了方法的有效性和在实际任务中的优秀表现。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型与自动语音识别系统深度融合成为具有高实用价值的热门研究方向。

**Method:** 为了应对LoRA在监督微调阶段常见的过拟合问题，本研究提出了一个新颖的训练范式——迭代LoRA训练（ILT），并与迭代伪标签策略相结合，有效提升了模型性能的理论上限。

**Result:** 实验结果表明，本研究所提出的方法是有效的，且该技术在Interspeech 2025多语言对话语音语言建模挑战赛中取得了优异成绩，证明了方法的实践可行性和应用潜力。

**Conclusion:** 研究证实了迭代LoRA训练与迭代伪标签策略结合的有效性及该方法在多语言自动语音识别任务中的卓越性能。

**Abstract:** The deep integration of large language models and automatic speech
recognition systems has become a promising research direction with high
practical value. To address the overfitting issue commonly observed in Low-Rank
Adaptation (LoRA) during the supervised fine-tuning (SFT) stage, this work
proposes an innovative training paradigm Iterative LoRA Training (ILT) in
combination with an Iterative Pseudo Labeling strategy, effectively enhancing
the theoretical upper bound of model performance. Based on Whisper-large-v3 and
Qwen2-Audio, we conduct systematic experiments using a three-stage training
process: Focus Training, Feed Back Training, and Fix Training. Experimental
results demonstrate the effectiveness of the proposed method. Furthermore, the
MegaAIS research team applied this technique in the Interspeech 2025
Multilingual Conversational Speech Language Modeling Challenge (MLC-SLM),
achieving 4th in Track 1 (Multilingual ASR Task) and 1st place in Track 2
(Speech Separation and Recognition Task), showcasing the practical feasibility
and strong application potential of our approach.

</details>


### [42] [Enhancing Essay Cohesion Assessment: A Novel Item Response Theory Approach](https://arxiv.org/abs/2507.08487)
*Bruno Alexandre Rosa,Hilário Oliveira,Luiz Rodrigues,Eduardo Araujo Oliveira,Rafael Ferreira Mello*

Main category: cs.CL

> 本研究提出了一种基于项目反应理论的方法来预测和评分教育论文中的连贯性，并将该方法在多个评估指标下验证，结果优于传统方法。

<details>
  <summary>Details</summary>

**Motivation:** 机器学习算法用于文本评价时通常不考虑构成分析语料库的实例的个别特征。因此，本研究探讨了将项目反应理论调整到机器学习背景下，以描述模型的能力、难度和区分度。

**Method:** 本研究提出了一种基于项目反应理论的连贯性评分预测方法，以调整机器学习模型生成的分数。使用的数据集包括扩展开的Essay-BR（包含6,563篇类似于全国高中考试(ENEM)风格的文章）和巴西葡萄牙语叙事文（包含1,235篇来自公立学校5至9年级学生的文章）。从这些数据集中提取了325个语言特征，并将其视为一个机器学习回归任务。

**Result:** 实验结果表明，该基于项目反应理论的方法在多个评估指标上优于传统的机器学习模型和集成方法。

**Conclusion:** 实验结果表明，所提出的方法在多个评估指标上优于传统的机器学习模型和集成方法，为提高教育论文中连贯性的自动评估提供了一种潜在方法。

**Abstract:** Essays are considered a valuable mechanism for evaluating learning outcomes
in writing. Textual cohesion is an essential characteristic of a text, as it
facilitates the establishment of meaning between its parts. Automatically
scoring cohesion in essays presents a challenge in the field of educational
artificial intelligence. The machine learning algorithms used to evaluate texts
generally do not consider the individual characteristics of the instances that
comprise the analysed corpus. In this meaning, item response theory can be
adapted to the context of machine learning, characterising the ability,
difficulty and discrimination of the models used. This work proposes and
analyses the performance of a cohesion score prediction approach based on item
response theory to adjust the scores generated by machine learning models. In
this study, the corpus selected for the experiments consisted of the extended
Essay-BR, which includes 6,563 essays in the style of the National High School
Exam (ENEM), and the Brazilian Portuguese Narrative Essays, comprising 1,235
essays written by 5th to 9th grade students from public schools. We extracted
325 linguistic features and treated the problem as a machine learning
regression task. The experimental results indicate that the proposed approach
outperforms conventional machine learning models and ensemble methods in
several evaluation metrics. This research explores a potential approach for
improving the automatic evaluation of cohesion in educational essays.

</details>


### [43] [A Third Paradigm for LLM Evaluation: Dialogue Game-Based Evaluation using clembench](https://arxiv.org/abs/2507.08491)
*David Schlangen,Sherzod Hakimov,Jonathan Jordan,Philipp Sadler*

Main category: cs.CL

> 研究提出了一种名为clembench的工具，用于评估大语言模型的对话能力，该工具支持定制测试且易于使用。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在解决对话游戏评估方法虽然具有优势，但由于缺乏成熟、可重用的实现而没有被广泛采用的问题。通过提供一个易于使用的评估工具，推动该评估方法的广泛应用。

**Method:** 该研究提出了一个名为clembench的工具，该工具自2023年开始持续开发，旨在优化对话游戏评估框架的便捷使用。此外，该工具提供了英语对话评估实例，并支持用户扩展新的定制测试。

**Result:** 研究展示了clembench的使用情况，可以用来评估大语言模型，并提供了基准对话实例，同时介绍如何扩展新的测试以更好地满足个性化需求。

**Conclusion:** clembench工具有效地解决了之前对话游戏评估方法在实施上的困难，为大语言模型的评估提供了一个新的高效途径。

**Abstract:** There are currently two main paradigms for evaluating large language models
(LLMs), reference-based evaluation and preference-based evaluation. The first,
carried over from the evaluation of machine learning models in general, relies
on pre-defined task instances, for which reference task executions are
available. The second, best exemplified by the LM-arena, relies on (often
self-selected) users bringing their own intents to a site that routes these to
several models in parallel, among whose responses the user then selects their
most preferred one. The former paradigm hence excels at control over what is
tested, while the latter comes with higher ecological validity, testing actual
use cases interactively. Recently, a third complementary paradigm has emerged
that combines some of the strengths of these approaches, offering control over
multi-turn, reference-free, repeatable interactions, while stressing
goal-directedness: dialogue game based evaluation. While the utility of this
approach has been shown by several projects, its adoption has been held back by
the lack of a mature, easily re-usable implementation. In this paper, we
present clembench, which has been in continuous development since 2023 and has
in its latest release been optimized for ease of general use. We describe how
it can be used to benchmark one's own models (using a provided set of benchmark
game instances in English), as well as how easily the benchmark itself can be
extended with new, tailor-made targeted tests.

</details>


### [44] [LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural Planning](https://arxiv.org/abs/2507.08496)
*Shibo Sun,Xue Li,Donglin Di,Mingjie Wei,Lanshun Nie,Wei-Nan Zhang,Dechen Zhan,Yang Song,Lei Fan*

Main category: cs.CL

> LLaPa 是一种用于多模态程序规划的视觉-语言模型，能够生成更好的执行计划，并且在多个数据集上表现优于其他先进模型。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型在为具身AI系统提供的程序规划方面有了显著的进步，但多模态输入集成和反事实推理仍没有得到充分的探索。该论文旨在解决这些挑战。

**Method:** LLaPa 是一个适用于多模态程序规划的视觉-语言模型框架。它可以基于任务描述和视觉环境图像生成可执行的操作序列。通过加入任务-环境重排器（TER）模块和反事实活动检索器（CAR）模块，提高了模型的程序规划能力。TER 模块通过任务导向的分割创建任务敏感的特征空间，而 CAR 模块强调了反事实情景中的潜在条件。

**Result:** LLaPa 在 ActPlan-1K 和 ALFRED 数据集上的实验表明，它生成的计划质量更高，具有更好的最长公共子序列（LCS）和正确性，比先进模型表现更佳。

**Conclusion:** LLaPa 框架显示了在多模态程序规划任务中，通过整合视觉和语言顺序和反事实推理，可以生成更高质量的计划。

**Abstract:** While large language models (LLMs) have advanced procedural planning for
embodied AI systems through strong reasoning abilities, the integration of
multimodal inputs and counterfactual reasoning remains underexplored. To tackle
these challenges, we introduce LLaPa, a vision-language model framework
designed for multimodal procedural planning. LLaPa generates executable action
sequences from textual task descriptions and visual environmental images using
vision-language models (VLMs). Furthermore, we enhance LLaPa with two auxiliary
modules to improve procedural planning. The first module, the Task-Environment
Reranker (TER), leverages task-oriented segmentation to create a task-sensitive
feature space, aligning textual descriptions with visual environments and
emphasizing critical regions for procedural execution. The second module, the
Counterfactual Activities Retriever (CAR), identifies and emphasizes potential
counterfactual conditions, enhancing the model's reasoning capability in
counterfactual scenarios. Extensive experiments on ActPlan-1K and ALFRED
benchmarks demonstrate that LLaPa generates higher-quality plans with superior
LCS and correctness, outperforming advanced models. The code and models are
available https://github.com/sunshibo1234/LLaPa.

</details>


### [45] [Semantic-Augmented Latent Topic Modeling with LLM-in-the-Loop](https://arxiv.org/abs/2507.08498)
*Mengze Hong,Chen Jason Zhang,Di Jiang*

Main category: cs.CL

> 本文探索了通过LLMs增强LDA模型的效果，发现虽然在初始化阶段没有明显效果，但在后期校正阶段取得了5.86%的连贯性提升。

<details>
  <summary>Details</summary>

**Motivation:** 由于LDA模型的效果高度依赖于其初始化的质量，因此作者探索了通过使用大型语言模型（LLMs）来增强这一过程的潜力。

**Method:** 本研究通过将大型语言模型（LLMs）整合到主题模型的两个关键阶段：初始化和后期校正中，来探讨LLMs对主题模型效果的提升。初始化阶段利用LLMs引导的主题聚类来初始化Gibbs采样算法。后期校正阶段则使用LLMs进行修正。

**Result:** 实验结果表明，提出的初始化策略虽然改善了LDA早期迭代的效果，但对收敛性没有影响，并且在对比基准测试中表现最差。然而，LLM启用的后期校正阶段在连贯性评估中取得了5.86%的显著提升。

**Conclusion:** 研究结果突出了在环路中使用LLMs的实际益处，并质疑了LLMs总是更好的文本挖掘替代方案这一信念。

**Abstract:** Latent Dirichlet Allocation (LDA) is a prominent generative probabilistic
model used for uncovering abstract topics within document collections. In this
paper, we explore the effectiveness of augmenting topic models with Large
Language Models (LLMs) through integration into two key phases: Initialization
and Post-Correction. Since the LDA is highly dependent on the quality of its
initialization, we conduct extensive experiments on the LLM-guided topic
clustering for initializing the Gibbs sampling algorithm. Interestingly, the
experimental results reveal that while the proposed initialization strategy
improves the early iterations of LDA, it has no effect on the convergence and
yields the worst performance compared to the baselines. The LLM-enabled
post-correction, on the other hand, achieved a promising improvement of 5.86%
in the coherence evaluation. These results highlight the practical benefits of
the LLM-in-the-loop approach and challenge the belief that LLMs are always the
superior text mining alternative.

</details>


### [46] [PromotionGo at SemEval-2025 Task 11: A Feature-Centric Framework for Cross-Lingual Multi-Emotion Detection in Short Texts](https://arxiv.org/abs/2507.08499)
*Ziyi Huang,Xia Cui*

Main category: cs.CL

> 本文提出了一种适应性文档表示和学习算法的框架，用于改善低资源语言中的多标签情感检测。结果表明TF-IDF适用于低资源语言，而复杂的神经模型和降维技术能够提高性能和效率。框架有助于解决多语言情感检测中的挑战。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决SemEval 2025任务11：文本情感检测中的多标签情感检测问题，特别是在短文本中进行情感检测，挑战在于处理语言多样性和资源限制的问题。

**Method:** 我们提出了一个以特征为中心的框架，该框架动态调整文档表示和学习算法，以优化特定语言的性能。该框架评估了三个关键组件：文档表示、降维和模型训练，并且在28种语言中进行了测试，其中有五种语言进行了详细分析。

**Result:** 实验结果表明，TF-IDF在低资源语言中仍然非常有效，而像FastText和Sentence-BERT那样的上下文嵌入和基于transformer的文档表示则表现出特定语言的优势。降维方法PCA减少了训练时间而不损害性能，尤其是对于FastText和MLP这样的神经模型。

**Conclusion:** 我们的框架提供了一个可扩展的多语言情感检测解决方案，能够应对语言多样性和资源限制带来的挑战。

**Abstract:** This paper presents our system for SemEval 2025 Task 11: Bridging the Gap in
Text-Based Emotion Detection (Track A), which focuses on multi-label emotion
detection in short texts. We propose a feature-centric framework that
dynamically adapts document representations and learning algorithms to optimize
language-specific performance. Our study evaluates three key components:
document representation, dimensionality reduction, and model training in 28
languages, highlighting five for detailed analysis. The results show that
TF-IDF remains highly effective for low-resource languages, while contextual
embeddings like FastText and transformer-based document representations, such
as those produced by Sentence-BERT, exhibit language-specific strengths.
Principal Component Analysis (PCA) reduces training time without compromising
performance, particularly benefiting FastText and neural models such as
Multi-Layer Perceptrons (MLP). Computational efficiency analysis underscores
the trade-off between model complexity and processing cost. Our framework
provides a scalable solution for multilingual emotion detection, addressing the
challenges of linguistic diversity and resource constraints.

</details>


### [47] [The AI Language Proficiency Monitor -- Tracking the Progress of LLMs on Multilingual Benchmarks](https://arxiv.org/abs/2507.08538)
*David Pomerenke,Jonas Nothnagel,Simon Ostermann*

Main category: cs.CL

> 研究团队介绍了一种名为AI语言能力监测器的多语言基准测试工具，以系统评估大型语言模型在200多种语言中的表现，尤其关注低资源语言，并提供了一个开源、自动更新的排行榜和仪表板。

<details>
  <summary>Details</summary>

**Motivation:** 为了确保大型语言模型的利益在全球各种语言中公平地获取，需要对其能力进行全面的语言评估。

**Method:** 介绍了一种名为AI语言能力监测器的多语言基准测试，该测试能够系统地评估大型语言模型在多达200种语言的表现，特别是低资源语言。该基准测试涵盖了翻译、问答、数学和推理等多种任务，并使用如FLORES+、MMLU、GSM8K、TruthfulQA和ARC等数据集进行评估。

**Result:** 提供了一个开源的、自动更新的排行榜和仪表板，以帮助研究人员、开发者和政策制定者发现模型性能的优势和不足。除此之外，该平台还提供了包括全球能力地图和时间趋势等描述性见解。

**Conclusion:** 通过补充和扩展此前的多语言基准测试，该研究旨在促进多语言AI领域的透明度、包容性和进步。

**Abstract:** To ensure equitable access to the benefits of large language models (LLMs),
it is essential to evaluate their capabilities across the world's languages. We
introduce the AI Language Proficiency Monitor, a comprehensive multilingual
benchmark that systematically assesses LLM performance across up to 200
languages, with a particular focus on low-resource languages. Our benchmark
aggregates diverse tasks including translation, question answering, math, and
reasoning, using datasets such as FLORES+, MMLU, GSM8K, TruthfulQA, and ARC. We
provide an open-source, auto-updating leaderboard and dashboard that supports
researchers, developers, and policymakers in identifying strengths and gaps in
model performance. In addition to ranking models, the platform offers
descriptive insights such as a global proficiency map and trends over time. By
complementing and extending prior multilingual benchmarks, our work aims to
foster transparency, inclusivity, and progress in multilingual AI. The system
is available at
https://huggingface.co/spaces/fair-forward/evals-for-every-language.

</details>


### [48] [DocPolarBERT: A Pre-trained Model for Document Understanding with Relative Polar Coordinate Encoding of Layout Structures](https://arxiv.org/abs/2507.08606)
*Benno Uthayasooriyar,Antoine Ly,Franck Vermet,Caio Corro*

Main category: cs.CL

> DocPolarBERT 是一种布局感知的 BERT 模型，它使用相对极坐标系统来考虑文本块的位置，而不是传统的笛卡尔坐标系统，尽管预训练数据较少，但仍能达到最先进的结果。

<details>
  <summary>Details</summary>

**Motivation:** 减少对大量预训练数据的依赖，提供一种更有效和高效的文档理解替代方法。

**Method:** 扩展自注意力机制，采用相对极坐标系统考虑文本块的位置，无需绝对的二维位置嵌入。

**Result:** 尽管预训练数据集比广泛使用的 IIT-CDIP 数据集小六倍以上，但 DocPolarBERT 仍能达到最先进的结果。

**Conclusion:** 精心设计的注意力机制可以补偿减少的预训练数据，提供一种更有效和高效的文档理解方案。

**Abstract:** We introduce DocPolarBERT, a layout-aware BERT model for document
understanding that eliminates the need for absolute 2D positional embeddings.
We extend self-attention to take into account text block positions in relative
polar coordinate system rather than the Cartesian one. Despite being
pre-trained on a dataset more than six times smaller than the widely used
IIT-CDIP corpus, DocPolarBERT achieves state-of-the-art results. These results
demonstrate that a carefully designed attention mechanism can compensate for
reduced pre-training data, offering an efficient and effective alternative for
document understanding.

</details>


### [49] [A comprehensive study of LLM-based argument classification: from LLAMA through GPT-4o to Deepseek-R1](https://arxiv.org/abs/2507.08621)
*Marcin Pietroń,Rafał Olszowski,Jakub Gomułka,Filip Gampel,Andrzej Tomski*

Main category: cs.CL

> 本论文研究了几种大规模语言模型在论点分类上的表现，发现ChatGPT-4o和Deepseek-R1表现最佳，但仍然存在错误。这项工作是首次使用这些数据集对LLM和提示算法进行的广泛分析。

<details>
  <summary>Details</summary>

**Motivation:** 尽管有各种基准测试和验证大规模语言模型(如LLM)质量的测试，但在公开可用的论点分类数据库中对其操作的研究仍然不足。该研究旨在填补这一空白。

**Method:** 本研究使用了包括Args.me和UKP在内的多样化数据集，测试了GPT、Llama、DeepSeek以及结合了Chain-of-Thoughts算法的推理增强变体模型。

**Result:** 结果显示，ChatGPT-4o在论点分类基准测试中表现最佳。对于结合推理能力的模型，Deepseek-R1表现突出。然而，即使是表现最好的模型仍会犯错，最常见的错误类型也被讨论了。

**Conclusion:** 研究展示了现有提示算法在论点分析中的弱点，并指出了未来改进的方向。同时，对当前可用的论点数据集进行了深入分析，指出了它们的不足之处。

**Abstract:** Argument mining (AM) is an interdisciplinary research field that integrates
insights from logic, philosophy, linguistics, rhetoric, law, psychology, and
computer science. It involves the automatic identification and extraction of
argumentative components, such as premises and claims, and the detection of
relationships between them, such as support, attack, or neutrality. Recently,
the field has advanced significantly, especially with the advent of large
language models (LLMs), which have enhanced the efficiency of analyzing and
extracting argument semantics compared to traditional methods and other deep
learning models. There are many benchmarks for testing and verifying the
quality of LLM, but there is still a lack of research and results on the
operation of these models in publicly available argument classification
databases. This paper presents a study of a selection of LLM's, using diverse
datasets such as Args.me and UKP. The models tested include versions of GPT,
Llama, and DeepSeek, along with reasoning-enhanced variants incorporating the
Chain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms
the others in the argument classification benchmarks. In case of models
incorporated with reasoning capabilities, the Deepseek-R1 shows its
superiority. However, despite their superiority, GPT-4o and Deepseek-R1 still
make errors. The most common errors are discussed for all models. To our
knowledge, the presented work is the first broader analysis of the mentioned
datasets using LLM and prompt algorithms. The work also shows some weaknesses
of known prompt algorithms in argument analysis, while indicating directions
for their improvement. The added value of the work is the in-depth analysis of
the available argument datasets and the demonstration of their shortcomings.

</details>


### [50] [The Impact of Automatic Speech Transcription on Speaker Attribution](https://arxiv.org/abs/2507.08660)
*Cristina Aggazzotti,Matthew Wiesner,Elizabeth Allyn Smith,Nicholas Andrews*

Main category: cs.CL

> 该研究首次全面分析了自动语音识别系统产生的转录错误对说话人归因任务的影响。结果显示，说话人归因能够相对较好地应对这些错误，且性能有时优于基于人类转录的数据。

<details>
  <summary>Details</summary>

**Motivation:** 之前的大部分研究都集中于使用人类注释员生产的转录文本进行说话人归因的可行性上。然而，实际环境中往往只有由自动语音识别系统生成的更具有错误的转录文本。本研究着重解决这一现实问题，首次全面探讨了自动转录对说话人归因性能的影响。

**Method:** 研究主要探讨了语音识别系统（ASR）产生的自动转录对说话人归因性能的影响。具体来说，通过分析转录错误的程度对说话人归因准确性的影响，以及ASR系统的不同属性如何影响归因性能，该研究评估了自动转录的说话人归因能力。

**Result:** 研究发现，说话人归因对词级转录错误的耐受性较高，并且恢复真实转录的目标与归因性能之间的相关性较小。总的来说，基于ASR生成的含错转录文本进行的说话人归因，其性能与基于人工转录数据的归因性能相当甚至更好。这可能是因为ASR转录错误捕获了能够揭示发言人身份的特定特征。

**Conclusion:** 该研究得出了转录错误对说话人归因的影响较小的结论，同时指出了ASR系统的转录错误可能意外地捕捉到了一些有利于识别说话人身份的信息，这对于未来的说话人归因任务具有重要意义。

**Abstract:** Speaker attribution from speech transcripts is the task of identifying a
speaker from the transcript of their speech based on patterns in their language
use. This task is especially useful when the audio is unavailable (e.g.
deleted) or unreliable (e.g. anonymized speech). Prior work in this area has
primarily focused on the feasibility of attributing speakers using transcripts
produced by human annotators. However, in real-world settings, one often only
has more errorful transcripts produced by automatic speech recognition (ASR)
systems. In this paper, we conduct what is, to our knowledge, the first
comprehensive study of the impact of automatic transcription on speaker
attribution performance. In particular, we study the extent to which speaker
attribution performance degrades in the face of transcription errors, as well
as how properties of the ASR system impact attribution. We find that
attribution is surprisingly resilient to word-level transcription errors and
that the objective of recovering the true transcript is minimally correlated
with attribution performance. Overall, our findings suggest that speaker
attribution on more errorful transcripts produced by ASR is as good, if not
better, than attribution based on human-transcribed data, possibly because ASR
transcription errors can capture speaker-specific features revealing of speaker
identity.

</details>


### [51] [KELPS: A Framework for Verified Multi-Language Autoformalization via Semantic-Syntactic Alignment](https://arxiv.org/abs/2507.08665)
*Jiyao Zhang,Chengli Zhong,Hui Xu,Qige Li,Yi Zhou*

Main category: cs.CL

> 该论文展示了KELPS框架在构建平行语料库方面的可能性及其在多个数据集上超过现有最优模型的性能。

<details>
  <summary>Details</summary>

**Motivation:** 尽管现代大型语言模型（LLMs）在将非正式数学形式化为机器可验证定理方面取得了显著进展，但仍面临平行语料库数量和质量有限的问题。本研究旨在通过提出KELPS来解决这些问题。

**Method:** 本研究提出了一种新的神经符号框架KELPS来解决将非正式数学形式化成机器可验证定理中面临的数据质量和数量限制问题。KELPS框架是一个迭代框架，可以将非正式数据翻译、合成并过滤到多种正式语言（如Lean、Coq和Isabelle）中。首先，将自然语言翻译成知识方程（KEs），这是一种基于断言逻辑设计的新语言。然后，通过严格的定义规则，将KEs转换为目标语言，这些规则既保留了句法结构，也保留了语义意义。

**Result:** 此框架生成了一个包含超60,000个问题的平行语料库。在MiniF2F上的句法准确率达到了88.9%（pass@1），优于诸如Deepseek-V3（81%）和Herald（81.3%）等现有最优模型。

**Conclusion:** KELPS框架展示了一种有效的方法来构建高质量的平行语料库，为进一步提高非正式数学向正式语言翻译的质量和效率奠定了基础。

**Abstract:** Modern large language models (LLMs) show promising progress in formalizing
informal mathematics into machine-verifiable theorems. However, these methods
still face bottlenecks due to the limited quantity and quality of multilingual
parallel corpora. In this paper, we propose a novel neuro-symbolic framework
KELPS (Knowledge-Equation based Logical Processing System) to address these
problems. KELPS is an iterative framework for translating, synthesizing, and
filtering informal data into multiple formal languages (Lean, Coq, and
Isabelle). First, we translate natural language into Knowledge Equations (KEs),
a novel language that we designed, theoretically grounded in assertional logic.
Next, we convert them to target languages through rigorously defined rules that
preserve both syntactic structure and semantic meaning. This process yielded a
parallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic
accuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3
(81%) and Herald (81.3%) across multiple datasets. All datasets and codes are
available in the supplementary materials.

</details>


### [52] [KG-Attention: Knowledge Graph-Guided Attention at Test-Time via Bidirectional Information Aggregation](https://arxiv.org/abs/2507.08704)
*Songlin Zhai,Guilin Qi,Yuan Meng*

Main category: cs.CL

> A new test-time knowledge graph (KG)-augmented framework is introduced to enhance large language models (LLMs) with structural knowledge, featuring dynamic and parameterless knowledge fusion through a knowledge graph-guided attention (KGA) module.

<details>
  <summary>Details</summary>

**Motivation:** The motivation stems from the limitations of existing KG-enhanced approaches, which include the risk of catastrophic forgetting, over-reliance on parameter-heavy fine-tuning, and poor adaptability to real-time knowledge updates.

**Method:** The paper proposes a test-time KG-augmented framework for LLMs, centered around a knowledge graph-guided attention (KGA) module. This module dynamically fuses external knowledge with minimal system changes, using outward and inward aggregation pathways to refine and enhance the input representations.

**Result:** Experiments on five benchmarks showcase the effectiveness and comparability of the proposed knowledge fusion approach, demonstrating real-time knowledge fusion capabilities without requiring parameter modifications.

**Conclusion:** The study introduces a groundbreaking test-time knowledge integration technique for LLMs, which promises more dynamic and efficient knowledge application in both static and evolving knowledge environments, maintaining the model's generalization and adaptability.

**Abstract:** Knowledge graphs (KGs) play a critical role in enhancing large language
models (LLMs) by introducing structured and grounded knowledge into the
learning process. However, most existing KG-enhanced approaches rely on
parameter-intensive fine-tuning, which risks catastrophic forgetting and
degrades the pretrained model's generalization. Moreover, they exhibit limited
adaptability to real-time knowledge updates due to their static integration
frameworks. To address these issues, we introduce the first test-time
KG-augmented framework for LLMs, built around a dedicated knowledge
graph-guided attention (KGA) module that enables dynamic knowledge fusion
without any parameter updates. The proposed KGA module augments the standard
self-attention mechanism with two synergistic pathways: outward and inward
aggregation. Specifically, the outward pathway dynamically integrates external
knowledge into input representations via input-driven KG fusion. This inward
aggregation complements the outward pathway by refining input representations
through KG-guided filtering, suppressing task-irrelevant signals and amplifying
knowledge-relevant patterns. Importantly, while the outward pathway handles
knowledge fusion, the inward path selects the most relevant triples and feeds
them back into the fusion process, forming a closed-loop enhancement mechanism.
By synergistically combining these two pathways, the proposed method supports
real-time knowledge fusion exclusively at test-time, without any parameter
modification. Extensive experiments on five benchmarks verify the comparable
knowledge fusion performance of KGA.

</details>


### [53] [Multilingual Multimodal Software Developer for Code Generation](https://arxiv.org/abs/2507.08719)
*Linzheng Chai,Jian Yang,Shukai Liu,Wei Zhang,Liran Wang,Ke Jin,Tao Sun,Congnan Liu,Chenchen Zhang,Hualei Zhu,Jiaheng Liu,Xianjie Wu,Ge Zhang,Tianyu Liu,Zhoujun Li*

Main category: cs.CL

> 本文介绍了一种新的多模态软件开发工具MM-Coder，通过整合视觉设计（如UML图和流程图）和文本指令，以解决当前大多数代码生成模型仅文本处理的问题。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型在代码生成方面取得了显著进步，但大多数模型仍局限于文本处理，忽略了现实世界中软件开发中使用的关键视觉辅助。这项工作旨在通过整合视觉辅助和文本信息来改进代码生成。

**Method:** MM-Coder整合了视觉设计输入（如UML图和流程图）与文本指令，以提高代码生成的准确性和架构一致性。为此，开发了MMc-Instruct，这是一个包括基于工作流程图的代码生成的多样化多模态指令微调数据集。

**Result:** 通过使用MMEval，该研究揭示了模型在捕捉精准视觉信息、遵循指令以及运用高级编程知识方面仍存在挑战。

**Conclusion:** 该工作旨在通过让LLMs解释和实现通过文本和视觉设计传达的复杂规范，从而变革工业编程。

**Abstract:** The rapid advancement of Large Language Models (LLMs) has significantly
improved code generation, yet most models remain text-only, neglecting crucial
visual aids like diagrams and flowcharts used in real-world software
development. To bridge this gap, we introduce MM-Coder, a Multilingual
Multimodal software developer. MM-Coder integrates visual design inputs-Unified
Modeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with
textual instructions to enhance code generation accuracy and architectural
alignment. To enable this, we developed MMc-Instruct, a diverse multimodal
instruction-tuning dataset including visual-workflow-based code generation,
allowing MM-Coder to synthesize textual and graphical information like human
developers, distinct from prior work on narrow tasks. Furthermore, we introduce
MMEval, a new benchmark for evaluating multimodal code generation, addressing
existing text-only limitations. Our evaluations using MMEval highlight
significant remaining challenges for models in precise visual information
capture, instruction following, and advanced programming knowledge. Our work
aims to revolutionize industrial programming by enabling LLMs to interpret and
implement complex specifications conveyed through both text and visual designs.

</details>


### [54] [KV Cache Steering for Inducing Reasoning in Small Language Models](https://arxiv.org/abs/2507.08799)
*Max Belitsky,Dawid J. Kopiczko,Michael Dorkenwald,M. Jehanzeb Mirza,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CL

> 本文提出了一种新的方法——缓存引导，其通过一次性的干预操作直接作用于模型的键值缓存。这种方法不仅可以引导小型语言模型实现链式思维推理，还较之前的方法更为稳定、高效和易于集成。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是为了验证缓存引导方法的有效性，特别是通过将其应用于引导小型语言模型进行链式思维推理。

**Method:** 我们提出了缓存引导（cache steering），这是一种轻量级的方法，用于通过一次干预直接对键值缓存进行隐式引导。通过利用GPT-4生成的推理轨迹来构建引导向量，使模型行为更偏向于明确的多步骤推理，而不需微调或提示修改。

**Result:** 实验评估表明，缓存引导在多样化的推理基准测试中改善了模型推理的定性结构和定量任务表现。

**Conclusion:** 相比于之前需要连续干预的激活引导技术，我们的缓存引导方法在超参数稳定性、推理时间效率和集成易用性方面具有显著优势，提供了一个更强大和实用的生成控制解决方案。

**Abstract:** We propose cache steering, a lightweight method for implicit steering of
language models via a one-shot intervention applied directly to the key-value
cache. To validate its effectiveness, we apply cache steering to induce
chain-of-thought reasoning in small language models. Our approach leverages
GPT-4o-generated reasoning traces to construct steering vectors that shift
model behavior toward more explicit, multi-step reasoning without fine-tuning
or prompt modifications. Experimental evaluations on diverse reasoning
benchmarks demonstrate that cache steering improves both the qualitative
structure of model reasoning and quantitative task performance. Compared to
prior activation steering techniques that require continuous interventions, our
one-shot cache steering offers substantial advantages in terms of
hyperparameter stability, inference-time efficiency, and ease of integration,
making it a more robust and practical solution for controlled generation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [55] [CuriosAI Submission to the EgoExo4D Proficiency Estimation Challenge 2025](https://arxiv.org/abs/2507.08022)
*Hayato Tanoue,Hiroki Nishihara,Yuma Suzuki,Takayuki Hori,Hiroki Takushima,Aiswariya Manojkumar,Yuki Shibata,Mitsuru Takeda,Fumika Beppu,Zhao Hengwei,Yuto Kanda,Daichi Yamaga*

Main category: cs.CV

> 研究提出了两种方法来改进多视角技能评估，其中两阶段方法表现更优，显示了利用场景信息对技能水平进行评估的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是改进多视角技能评估的方法，特别是在EgoExo4D技能评估挑战赛上的表现。

**Method:** 此研究提出了两种多视角技能评估方法：(1) 一种使用Sapiens-2B的多任务学习框架，该框架共同预测技能水平和场景标签（准确率为43.6%），以及(2) 一个两阶段流水线，该流水线结合了零样本场景识别和特定视角的VideoMAE分类器（准确率为47.8%）.

**Result:** 两阶段方法表现更好，展示了场景条件建模在技能水平评估中的有效性。

**Conclusion:** 研究结果支持使用两阶段流水线和场景条件建模来改进多视角技能评估。

**Abstract:** This report presents the CuriosAI team's submission to the EgoExo4D
Proficiency Estimation Challenge at CVPR 2025. We propose two methods for
multi-view skill assessment: (1) a multi-task learning framework using
Sapiens-2B that jointly predicts proficiency and scenario labels (43.6 %
accuracy), and (2) a two-stage pipeline combining zero-shot scenario
recognition with view-specific VideoMAE classifiers (47.8 % accuracy). The
superior performance of the two-stage approach demonstrates the effectiveness
of scenario-conditioned modeling for proficiency estimation.

</details>


### [56] [Self-Consistency in Vision-Language Models for Precision Agriculture: Multi-Response Consensus for Crop Disease Management](https://arxiv.org/abs/2507.08024)
*Mihir Gupta,Abhay Mangla,Ross Greer,Pratik Desai*

Main category: cs.CV

> 本文提出了一种农业领域图像处理框架，显著提升了视觉语言模型在作物病害识别和治疗建议方面的准确性，支持在资源受限的野外环境中实时决策。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于现有视觉语言模型（VLMs）在农业领域应用表现不佳，本文旨在开发一种专注于农业领域的方法，以提高作物病害识别及治疗建议的准确性。

**Method:** 本文提出了一种针对农业领域的图像处理框架，结合基于prompt的专家评估和自我一致性机制，旨在提升视觉语言模型（VLMs）在精准农业应用中的可靠性。该框架包含两大创新点：1. 基于prompt的评估协议，将语言模型配置为专家级植物病理学家，对图像分析结果进行规模化评估；2. 余弦一致性自我投票机制，通过生成多个对农业图像的候选诊断答案，使用领域自适应嵌入选择语义最连贯的诊断。

**Result:** 应用PaliGemma模型（经过微调）对玉米叶片病害进行识别，与标准贪婪解码相比，本文提出的方法将诊断准确率从82.2%提高到87.8%，症状分析准确率从38.9%提升至52.2%，治疗建议准确率从27.8%提高到43.3%。

**Conclusion:** 实验结果表明，基于本文提出框架的AI驱动精确农业工具不仅能够胜任野外条件下工作，还具备在资源受限环境下实时决策支持的能力，具有在精准农业中广泛应用的潜力。

**Abstract:** Precision agriculture relies heavily on accurate image analysis for crop
disease identification and treatment recommendation, yet existing
vision-language models (VLMs) often underperform in specialized agricultural
domains. This work presents a domain-aware framework for agricultural image
processing that combines prompt-based expert evaluation with self-consistency
mechanisms to enhance VLM reliability in precision agriculture applications. We
introduce two key innovations: (1) a prompt-based evaluation protocol that
configures a language model as an expert plant pathologist for scalable
assessment of image analysis outputs, and (2) a cosine-consistency self-voting
mechanism that generates multiple candidate responses from agricultural images
and selects the most semantically coherent diagnosis using domain-adapted
embeddings. Applied to maize leaf disease identification from field images
using a fine-tuned PaliGemma model, our approach improves diagnostic accuracy
from 82.2\% to 87.8\%, symptom analysis from 38.9\% to 52.2\%, and treatment
recommendation from 27.8\% to 43.3\% compared to standard greedy decoding. The
system remains compact enough for deployment on mobile devices, supporting
real-time agricultural decision-making in resource-constrained environments.
These results demonstrate significant potential for AI-driven precision
agriculture tools that can operate reliably in diverse field conditions.

</details>


### [57] [Development of a Canada-Wide Morphology Map for the ITU-R P. 1411 Propagation Model](https://arxiv.org/abs/2507.08026)
*Jennifer P. T. Nguyen*

Main category: cs.CV

> 本文概述了加全国形态图的发展，该图是通过机器学习方法实现的，用于优化路径损耗估计。

<details>
  <summary>Details</summary>

**Motivation:** 为了使环境类型描述符（在推荐中找到的）定量化，从而提高分类精度，实现加拿大全国的形态图，以确保更精确的户外短距离传播路径损耗估计。

**Method:** 采用机器学习方法来自动化分类过程，依据ITU-R P.1411-12传播模型指南，将加拿大划分为不同区域，包括住宅区、城市低层建筑区及城市高层建筑区。

**Result:** 通过广泛的实验优化了分类精度，生成了加拿大全国的形态图，从而提高了300 MHz至100 GHz频率范围内的户外短距离传播路径损耗估计的准确性。

**Conclusion:** 该机器学习方法能够确保在全国范围内提供更准确的路径损耗估计，适用于频率在300 MHz至100 GHz之间的户外短距离传播。

**Abstract:** This paper outlines the development of a Canada-wide morphology map
classifying regions into residential, urban low-rise, and urban high-rise
environments, following the ITU-R P.1411-12 propagation model guidelines. To
address the qualitative nature of the environment-type descriptors found in the
Recommendation, a machine learning approach is employed to automate the
classification process. Extensive experimentation optimized classification
accuracy, resulting in a Canada-wide morphology map that ensures more accurate
path loss estimations for outdoor short-range propagation at frequencies
ranging from 300 MHz to 100 GHz.

</details>


### [58] [Towards Evaluating Robustness of Prompt Adherence in Text to Image Models](https://arxiv.org/abs/2507.08039)
*Sujith Vemishetty,Advitiya Arora,Anupama Sharma*

Main category: cs.CV

> 本文旨在解决Text-to-Image模型评估不足的问题，通过设计新的评估框架和创建数据集来评估模型的图像生成一致性，并利用gpt-4o模型进行对比验证。

<details>
  <summary>Details</summary>

**Motivation:** 近年来基于语言的模型(LLMs)展现了强大的能力和广泛的应用，特别是对多模态模型和文本到图像（Text-to-Image）模型的研究相对较少，评估方面也存在不足。本研究旨在填补这一空白并提出一个评估框架。

**Method:** 本文提出了一种针对Text-to-Image模型的综合评估框架，特别关注了模型对文本提示的响应一致性。为此，作者创建了一个新的数据集，用以评估这些模型生成与输入文本提示因素变化相一致的图像的能力。此外，还介绍了一种利用gpt-4o模型生成的真实图像描述为基准的管道，用来生成人工图像，并通过再次输入gpt-4o模型进行比较评估。

**Result:** 研究发现，尽管采用了多种不同类型的Text-to-Image模型（如各种版本的Stable Diffusion和Janus模型），但在生成遵循特定变化因素的简单二值图像，以及生成遵循输入数据分布的图像方面仍存在欠缺。

**Conclusion:** 该研究表明，现有Text-to-Image模型在生成简单图像时仍面临挑战，需要进一步改进，从而提升模型对输入提示的响应一致性以及模型输出与输入数据分布的一致性。

**Abstract:** The advancements in the domain of LLMs in recent years have surprised many,
showcasing their remarkable capabilities and diverse applications. Their
potential applications in various real-world scenarios have led to significant
research on their reliability and effectiveness. On the other hand, multimodal
LLMs and Text-to-Image models have only recently gained prominence, especially
when compared to text-only LLMs. Their reliability remains constrained due to
insufficient research on assessing their performance and robustness. This paper
aims to establish a comprehensive evaluation framework for Text-to-Image
models, concentrating particularly on their adherence to prompts. We created a
novel dataset that aimed to assess the robustness of these models in generating
images that conform to the specified factors of variation in the input text
prompts. Our evaluation studies present findings on three variants of Stable
Diffusion models: Stable Diffusion 3 Medium, Stable Diffusion 3.5 Large, and
Stable Diffusion 3.5 Large Turbo, and two variants of Janus models: Janus Pro
1B and Janus Pro 7B. We introduce a pipeline that leverages text descriptions
generated by the gpt-4o model for our ground-truth images, which are then used
to generate artificial images by passing these descriptions to the
Text-to-Image models. We then pass these generated images again through gpt-4o
using the same system prompt and compare the variation between the two
descriptions. Our results reveal that these models struggle to create simple
binary images with only two factors of variation: a simple geometric shape and
its location. We also show, using pre-trained VAEs on our dataset, that they
fail to generate images that follow our input dataset distribution.

</details>


### [59] [ConsNoTrainLoRA: Data-driven Weight Initialization of Low-rank Adapters using Constraints](https://arxiv.org/abs/2507.08044)
*Debasmit Das,Hyoungwoo Park,Munawar Hayat,Seokeon Choi,Sungrack Yun,Fatih Porikli*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Foundation models are pre-trained on large-scale datasets and subsequently
fine-tuned on small-scale datasets using parameter-efficient fine-tuning (PEFT)
techniques like low-rank adapters (LoRA). In most previous works, LoRA weight
matrices are randomly initialized with a fixed rank across all attachment
points. In this paper, we improve convergence and final performance of LoRA
fine-tuning, using our proposed data-driven weight initialization method,
ConsNoTrainLoRA (CNTLoRA). We express LoRA initialization as a domain shift
problem where we use multiple constraints relating the pre-training and
fine-tuning activations. By reformulating these constraints, we obtain a
closed-form estimate of LoRA weights that depends on pre-training weights and
fine-tuning activation vectors and hence requires no training during
initialization. This weight estimate is decomposed to initialize the up and
down matrices with proposed flexibility of variable ranks. With the proposed
initialization method, we fine-tune on downstream tasks such as image
generation, image classification and image understanding. Both quantitative and
qualitative results demonstrate that CNTLoRA outperforms standard and
data-driven weight initialization methods. Extensive analyses and ablations
further elucidate the design choices of our framework, providing an optimal
recipe for faster convergence and enhanced performance.

</details>


### [60] [A Hybrid Multilayer Extreme Learning Machine for Image Classification with an Application to Quadcopters](https://arxiv.org/abs/2507.08047)
*Rolando A. Hernandez-Hernandez,Adrian Rubio-Solis*

Main category: cs.CV

> 本文提出了一种基于ELM自动编码器和间隔II型模糊逻辑理论的混合多层极限学习机(HML-ELM)，并在无人机图像分类中验证其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 多层极限学习机(ML-ELM)和其变异算法已被证明在不同的自然信号分类（如音频、视频、声学和图像）上非常有效。本研究旨在利用增强的方法在无人机上的主动图像分类中提高分类效率。

**Method:** 提出的方法是一种分层ELM学习框架，包含两个主要阶段：1) 自学特征提取 2) 监督特征分类。第一阶段通过堆叠多个ELM-AE实现无监督多层特征编码；第二阶段利用SIT2-FELM进行特征分类。

**Result:** 进行两种类型的实验以验证HML-ELM在图像分类中的效率。首先是在多个基准问题上进行图像分类；其次是在无人机中实现两个指定位置的四个不同对象之间的主动分类和传输。实验表明，提出的方法相对于其他计算方法展示了更高的效率。

**Conclusion:** 实验结果表明，提出的HML-ELM在无人机图像分类任务上相对其他类似方法提高了分类效率。

**Abstract:** Multilayer Extreme Learning Machine (ML-ELM) and its variants have proven to
be an effective technique for the classification of different natural signals
such as audio, video, acoustic and images. In this paper, a Hybrid Multilayer
Extreme Learning Machine (HML-ELM) that is based on ELM-based autoencoder
(ELM-AE) and an Interval Type-2 fuzzy Logic theory is suggested for active
image classification and applied to Unmanned Aerial Vehicles (UAVs). The
proposed methodology is a hierarchical ELM learning framework that consists of
two main phases: 1) self-taught feature extraction and 2) supervised feature
classification. First, unsupervised multilayer feature encoding is achieved by
stacking a number of ELM-AEs, in which input data is projected into a number of
high-level representations. At the second phase, the final features are
classified using a novel Simplified Interval Type-2 Fuzzy ELM (SIT2-FELM) with
a fast output reduction layer based on the SC algorithm; an improved version of
the algorithm Center of Sets Type Reducer without Sorting Requirement
(COSTRWSR). To validate the efficiency of the HML-ELM, two types of experiments
for the classification of images are suggested. First, the HML-ELM is applied
to solve a number of benchmark problems for image classification. Secondly, a
number of real experiments to the active classification and transport of four
different objects between two predefined locations using a UAV is implemented.
Experiments demonstrate that the proposed HML-ELM delivers a superior
efficiency compared to other similar methodologies such as ML-ELM, Multilayer
Fuzzy Extreme Learning Machine (ML-FELM) and ELM.

</details>


### [61] [Lightweight Cloud Masking Models for On-Board Inference in Hyperspectral Imaging](https://arxiv.org/abs/2507.08052)
*Mazen Ali,António Pereira,Fabio Gentile,Aser Cortines,Sam Mugel,Román Orús,Stelios P. Neophytides,Michalis Mavrovouniotis*

Main category: cs.CV

> 本文探讨高光谱卫星成像中云和云影标记的重要性，并比较了梯度提升方法和CNN模型。研究中最佳的模型是只包含597个可训练参数的CNN模型，它在精度、存储需求和推理时间方面表现平衡。此外，轻量级模型的实时处理潜力也支持了卫星设备上AI系统的发展。

<details>
  <summary>Details</summary>

**Motivation:** 云和云影遮盖物的标记是高光谱卫星成像中的关键预处理步骤，能提取高质量的数据。

**Method:** 本文研究了多种机器学习方法，包括XGBoost和LightGBM等梯度提升方法以及卷积神经网络(CNN)。

**Result:** 研究中所有的提升方法和CNN模型的准确率均超过93%。CNN模型在减少特征量后表现最佳，提供了高精度、低存储需求以及在CPU和GPU上的快速推理时间。仅包含最多597个可训练参数的CNN模型版本在部署可行性、精度和计算效率方面表现最佳。

**Conclusion:** 结果表明轻量级AI模型在高光谱图像实时处理中的潜力，支持太空应用中的卫星设备上AI系统的开发。

**Abstract:** Cloud and cloud shadow masking is a crucial preprocessing step in
hyperspectral satellite imaging, enabling the extraction of high-quality,
analysis-ready data. This study evaluates various machine learning approaches,
including gradient boosting methods such as XGBoost and LightGBM as well as
convolutional neural networks (CNNs). All boosting and CNN models achieved
accuracies exceeding 93%. Among the investigated models, the CNN with feature
reduction emerged as the most efficient, offering a balance of high accuracy,
low storage requirements, and rapid inference times on both CPUs and GPUs.
Variations of this version, with only up to 597 trainable parameters,
demonstrated the best trade-off in terms of deployment feasibility, accuracy,
and computational efficiency. These results demonstrate the potential of
lightweight artificial intelligence (AI) models for real-time hyperspectral
image processing, supporting the development of on-board satellite AI systems
for space-based applications.

</details>


### [62] [The relative importance of being Gaussian](https://arxiv.org/abs/2507.08059)
*F. Alberto Grünbaum,Tondgi Xu*

Main category: cs.CV

> 本文研究了原有去噪算法在使用非高斯噪声时的性能表现，实验条件为低配置机器和小尺寸图像，目的是测试算法适应性。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于探讨当算法处理与设计初衷（高斯噪声）大相径庭的噪声类型时，现有的基于高斯独立正态分布随机变量序列的去噪算法的表现如何。

**Method:** 本文的方法是研究在算法中使用非高斯噪声（如均匀分布噪声或Beta分布噪声）时，现有去噪算法的性能。这些实验在性能较低的笔记本电脑上进行，并使用了最小的图像尺寸。

**Result:** 结果部分尚未详细给出，但从描述中可以看出，研究将测试算法在非高斯噪声条件下的性能。

**Conclusion:** 结论部分未提供具体细节，研究主要聚焦于验证或探索上述发现是否在不同条件和环境下依然成立。

**Abstract:** The remarkable results for denoising in computer vision using diffusion
models given in \cite{SDWMG,HJA,HHG} yield a robust mathematical justification
for algorithms based on crucial properties of a sequence of Gaussian
independent $N(0,1)$ random variables. In particular the derivations use the
fact that a Gaussian distribution is determined by its mean and variance and
that the sum of two Gaussians is another Gaussian.
  \bigskip
  The issue raised in this short note is the following: suppose we use the
algorithm without any changes but replace the nature of the noise and use, for
instance, uniformly distributed noise or noise with a Beta distribution, or
noise which is a random superposition of two Gaussians with very different
variances. One could, of course, try to modify the algorithm keeping in mind
the nature of the noise, but this is not what we do. Instead we study the
performance of the algorithm when used with noise that is very far in nature
from the Gaussian case, where it is designed to work well.
  Usually these algorithms are implemented on very powerful computers. Our
experiments are all carried out on a small laptop and for the smallest possible
image size. Exploring how our observations are confirmed or changed when
dealing in different situations remains an interesting challenge.

</details>


### [63] [An Object-Based Deep Learning Approach for Building Height Estimation from Single SAR Images](https://arxiv.org/abs/2507.08096)
*Babak Memar,Luigi Russo,Silvia Liberata Ullo,Paolo Gamba*

Main category: cs.CV

> 本文介绍了一种基于深度学习从单幅高分辨率SAR图像中进行建筑物高度自动估计的方法，通过跨大陆城市的多中心数据集训练和验证，展示了方法在欧洲城市中的优越性能，并指出了深度学习在城市间和跨国界建筑物高度估计中的潜在应用。

<details>
  <summary>Details</summary>

**Motivation:** 精确估计建筑物的高度对于各种城市应用至关重要，而本文旨在利用高分辨率合成孔径雷达（SAR）图像进行自动化高度估计。

**Method:** 本文提出了一种基于深度学习的建筑物高度自动估计方法，该方法利用单幅高分辨率合成孔径雷达（SAR）图像，通过边界框检测和随后的高度估计来进行对象回归分析。

**Result:** 实验结果表明，在跨验证策略下，模型在欧洲城市的表现尤其出色，平均绝对误差（MAE）约为一层楼的高度（慕尼黑为2.20米），显著优于之前在跨分布场景下的最新方法。

**Conclusion:** 尽管在推广到其他大陆的城市时出现了较大的变化，特别是在亚洲，由于其独特的城市类型和高层建筑的普遍存在，但是本研究表明深度学习对于从单幅高分辨率SAR数据中进行稳健的城市间和跨国界的建筑物高度估计具有巨大潜力。

**Abstract:** Accurate estimation of building heights using very high resolution (VHR)
synthetic aperture radar (SAR) imagery is crucial for various urban
applications. This paper introduces a Deep Learning (DL)-based methodology for
automated building height estimation from single VHR COSMO-SkyMed images: an
object-based regression approach based on bounding box detection followed by
height estimation. This model was trained and evaluated on a unique
multi-continental dataset comprising eight geographically diverse cities across
Europe, North and South America, and Asia, employing a cross-validation
strategy to explicitly assess out-of-distribution (OOD) generalization. The
results demonstrate highly promising performance, particularly on European
cities where the model achieves a Mean Absolute Error (MAE) of approximately
one building story (2.20 m in Munich), significantly outperforming recent
state-of-the-art methods in similar OOD scenarios. Despite the increased
variability observed when generalizing to cities in other continents,
particularly in Asia with its distinct urban typologies and prevalence of
high-rise structures, this study underscores the significant potential of DL
for robust cross-city and cross-continental transfer learning in building
height estimation from single VHR SAR data.

</details>


### [64] [RegGS: Unposed Sparse Views Gaussian Splatting with 3DGS Registration](https://arxiv.org/abs/2507.08136)
*Chong Cheng,Yu Hu,Sicheng Yu,Beizhen Zhao,Zijian Wang,Hao Wang*

Main category: cs.CV

> RegGS 是一个基于 3D 高斯混合模型的注册框架，通过一个高效的最优运输算法和光度一致性模块，能够处理未指定视角的稀疏图像数据，实现了高精度的场景重建和视角合成。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于解决现有的3D Gaussian Splatting（3DGS）方法在使用稀疏视角图像时遇到的问题，这类方法要么受限于输入格式，要么缺乏对全局一致性的保证。

**Method:** 技术上，RegGS 使用了一个基于熵正则化的 Sinkhorn 算法来高效地解决从多个视角获得的 3D 高斯混合模型（GMMs）的最优运输 Mixture 2-Wasserstein（MW2）距离。此外，设计了一个联合的 3DGS 注册模块，整合了 MW2 距离、光度一致性和深度几何特征，实现了从粗到细的注册过程，并精确估计相机姿态和场景对齐。

**Result:** 实验结果表明，RegGS 能够有效地对局部高斯模型进行注册，同时实现了精确的姿态估计和高质量的新视图合成。

**Conclusion:** 结论是，RegGS 通过优化基于运输问题的距离和光度一致性，解决了使用稀疏视角重建场景时的挑战，提供了精确的场景重建和新视图合成。

**Abstract:** 3D Gaussian Splatting (3DGS) has demonstrated its potential in reconstructing
scenes from unposed images. However, optimization-based 3DGS methods struggle
with sparse views due to limited prior knowledge. Meanwhile, feed-forward
Gaussian approaches are constrained by input formats, making it challenging to
incorporate more input views. To address these challenges, we propose RegGS, a
3D Gaussian registration-based framework for reconstructing unposed sparse
views. RegGS aligns local 3D Gaussians generated by a feed-forward network into
a globally consistent 3D Gaussian representation. Technically, we implement an
entropy-regularized Sinkhorn algorithm to efficiently solve the optimal
transport Mixture 2-Wasserstein $(\text{MW}_2)$ distance, which serves as an
alignment metric for Gaussian mixture models (GMMs) in $\mathrm{Sim}(3)$ space.
Furthermore, we design a joint 3DGS registration module that integrates the
$\text{MW}_2$ distance, photometric consistency, and depth geometry. This
enables a coarse-to-fine registration process while accurately estimating
camera poses and aligning the scene. Experiments on the RE10K and ACID datasets
demonstrate that RegGS effectively registers local Gaussians with high
fidelity, achieving precise pose estimation and high-quality novel-view
synthesis. Project page: https://3dagentworld.github.io/reggs/.

</details>


### [65] [Temporally Consistent Amodal Completion for 3D Human-Object Interaction Reconstruction](https://arxiv.org/abs/2507.08137)
*Hyungjun Doh,Dong In Lee,Seunggeun Chi,Pin-Hao Huang,Kwonjoon Lee,Sangpil Kim,Karthik Ramani*

Main category: cs.CV

> 提出了一种新的从单目视频中重建动态人与物体交互的框架，通过非模式补全和时间上下文整合，显著提升了遮挡和时间一致性处理的性能。

<details>
  <summary>Details</summary>

**Motivation:** 传统三维重建方法在处理遮挡和时间不一致性时存在局限，因此，需要提出新的方法来改进这些问题。

**Method:** 该框架利用非模式补全技术推断遮挡部分的完整结构，整合时间上下文来强制视频序列中的重建一致性。

**Result:** 该论文介绍了一种从单目视频中重建动态人与物体交互的新框架，该框架克服了遮挡和时间不一致性的挑战。传统的三维重建方法通常假设物体静止或主体完全可见，当这些假设被违反时（特别是在存在互遮挡的场景中），其性能会受到严重影响。为此，该框架利用非模式补全来推断部分被遮挡区域的完整结构。与传统的针对单帧操作的方法不同，该方法整合了时间上下文，通过在视频序列中强制一致性来逐步细化和稳定重建。这种无模板策略适应各种条件，无需依赖预定义的模型，显著增强了在动态场景中恢复复杂细节的能力。使用3D高斯散射球技术在具有挑战性的单目视频上验证了该方法，证明其在处理遮挡和保持时间一致性方面的精度优于现有技术。

**Conclusion:** 该方法在处理遮挡和保持时间一致性上表现出色，使用3D高斯散射球技术验证了其在挑战性单目视频上的优越性能。

**Abstract:** We introduce a novel framework for reconstructing dynamic human-object
interactions from monocular video that overcomes challenges associated with
occlusions and temporal inconsistencies. Traditional 3D reconstruction methods
typically assume static objects or full visibility of dynamic subjects, leading
to degraded performance when these assumptions are violated-particularly in
scenarios where mutual occlusions occur. To address this, our framework
leverages amodal completion to infer the complete structure of partially
obscured regions. Unlike conventional approaches that operate on individual
frames, our method integrates temporal context, enforcing coherence across
video sequences to incrementally refine and stabilize reconstructions. This
template-free strategy adapts to varying conditions without relying on
predefined models, significantly enhancing the recovery of intricate details in
dynamic scenes. We validate our approach using 3D Gaussian Splatting on
challenging monocular videos, demonstrating superior precision in handling
occlusions and maintaining temporal stability compared to existing techniques.

</details>


### [66] [Adaptive Diffusion Denoised Smoothing : Certified Robustness via Randomized Smoothing with Differentially Private Guided Denoising Diffusion](https://arxiv.org/abs/2507.08163)
*Frederick Shpilevskiy,Saiyue Lyu,Krishnamurthy Dj Dvijotham,Mathias Lécuyer,Pierre-André Noël*

Main category: cs.CV

> 该研究提出了一种新的方法——自适应扩散去噪平滑，能够有效提升视觉模型对于对抗样本攻击的鲁棒性，并提供了相应的可证明认证。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机在于提升模型对抗对抗样本的鲁棒性，同时对模型预测结果提供可证明的认证。

**Method:** 我们提出了自适应扩散去噪平滑（Adaptive Diffusion Denoised Smoothing）方法，用于验证视觉模型在对抗样本攻击下的预测结果。该方法通过将引导去噪扩散模型重新解读为一系列自适应高斯微分私有机制，逐步将纯噪声样本优化为图像。

**Result:** 研究结果表明，通过特定的引导策略，该设计能在ImageNet数据集上，对于$lap{$ackslash$l_2$}lap{$ackslash$l2$}威胁模型，同时提升可证明的准确性与标准准确性。

**Conclusion:** 通过自适应高斯微分私有机制的组合来分析整个去噪过程的鲁棒性，证明了该自适应随机平滑分析的有效性。

**Abstract:** We propose Adaptive Diffusion Denoised Smoothing, a method for certifying the
predictions of a vision model against adversarial examples, while adapting to
the input. Our key insight is to reinterpret a guided denoising diffusion model
as a long sequence of adaptive Gaussian Differentially Private (GDP) mechanisms
refining a pure noise sample into an image. We show that these adaptive
mechanisms can be composed through a GDP privacy filter to analyze the
end-to-end robustness of the guided denoising process, yielding a provable
certification that extends the adaptive randomized smoothing analysis. We
demonstrate that our design, under a specific guiding strategy, can improve
both certified accuracy and standard accuracy on ImageNet for an $\ell_2$
threat model.

</details>


### [67] [An Embedded Real-time Object Alert System for Visually Impaired: A Monocular Depth Estimation based Approach through Computer Vision](https://arxiv.org/abs/2507.08165)
*Jareen Anjom,Rashik Iram Chowdhury,Tarbia Hasan,Md. Ishan Arefin Hossain*

Main category: cs.CV

> 研究提出了一种利用转移学习进行深度估计和物体检测，并通过量化技术使其轻量化和高效化的系统，帮助视障人士避免在城市中行走时碰撞到障碍物。

<details>
  <summary>Details</summary>

**Motivation:** 视障人士在城市中行走时会遇到很多障碍物，容易发生碰撞事故，因此开发一种能够提前预警视障人士前方障碍物的系统是必要的。

**Method:** 该系统利用转移学习技术训练模型进行深度估计和物体检测，并通过量化的技术优化模型，使其能够在嵌入式设备上高效运行。

**Result:** 所提出的解决方案能够实现轻量级实时深度估计和物体检测，模型的mAP50为0.801。

**Conclusion:** 该系统可以辅助视障人士在繁忙街道上安全行走，避免碰撞障碍物。

**Abstract:** Visually impaired people face significant challenges in their day-to-day
commutes in the urban cities of Bangladesh due to the vast number of
obstructions on every path. With many injuries taking place through road
accidents on a daily basis, it is paramount for a system to be developed that
can alert the visually impaired of objects at close distance beforehand. To
overcome this issue, a novel alert system is proposed in this research to
assist the visually impaired in commuting through these busy streets without
colliding with any objects. The proposed system can alert the individual to
objects that are present at a close distance. It utilizes transfer learning to
train models for depth estimation and object detection, and combines both
models to introduce a novel system. The models are optimized through the
utilization of quantization techniques to make them lightweight and efficient,
allowing them to be easily deployed on embedded systems. The proposed solution
achieved a lightweight real-time depth estimation and object detection model
with an mAP50 of 0.801.

</details>


### [68] [HNOSeg-XS: Extremely Small Hartley Neural Operator for Efficient and Resolution-Robust 3D Image Segmentation](https://arxiv.org/abs/2507.08205)
*Ken C. L. Wong,Hongzhi Wang,Tanveer Syeda-Mahmood*

Main category: cs.CV

> 本文提出的HNOSeg-XS架构通过傅里叶神经算子和哈特利变换优化，解决了医学图像分割中卷积神经网络和变压器模型的不足，实现了分辨率鲁棒、速度快、内存高效和参数高效的性能。

<details>
  <summary>Details</summary>

**Motivation:** 旨在克服卷积神经网络和变压器模型在医学图像分割中面临的问题，如高计算成本、内存占用以及因输入尺寸减小导致的性能不足。

**Method:** 提出了解決分辨率鲁棒性的HNOSeg-XS架构，该架构通过傅里叶神经算子学习可微分方程来建模图像分割，并通过哈特利变换和频率域问题重述来优化，从而具备零样本超分辨率特性。

**Result:** 在BraTS'23、KiTS'23和MVSeg'23数据集上测试时，HNOSeg-XS展示了其卓越的分辨率鲁棒性，使用少于34.7k模型参数，同时具有最佳推理时间（<0.24秒）和内存效率（<1.8 GiB）。

**Conclusion:** HNOSeg-XS模型因其分辨率鲁棒性、速度快、内存高效以及参数效率极高，展现出优于测试中的CNN和变压器模型的性能。

**Abstract:** In medical image segmentation, convolutional neural networks (CNNs) and
transformers are dominant. For CNNs, given the local receptive fields of
convolutional layers, long-range spatial correlations are captured through
consecutive convolutions and pooling. However, as the computational cost and
memory footprint can be prohibitively large, 3D models can only afford fewer
layers than 2D models with reduced receptive fields and abstract levels. For
transformers, although long-range correlations can be captured by multi-head
attention, its quadratic complexity with respect to input size is
computationally demanding. Therefore, either model may require input size
reduction to allow more filters and layers for better segmentation.
Nevertheless, given their discrete nature, models trained with patch-wise
training or image downsampling may produce suboptimal results when applied on
higher resolutions. To address this issue, here we propose the
resolution-robust HNOSeg-XS architecture. We model image segmentation by
learnable partial differential equations through the Fourier neural operator
which has the zero-shot super-resolution property. By replacing the Fourier
transform by the Hartley transform and reformulating the problem in the
frequency domain, we created the HNOSeg-XS model, which is resolution robust,
fast, memory efficient, and extremely parameter efficient. When tested on the
BraTS'23, KiTS'23, and MVSeg'23 datasets with a Tesla V100 GPU, HNOSeg-XS
showed its superior resolution robustness with fewer than 34.7k model
parameters. It also achieved the overall best inference time (< 0.24 s) and
memory efficiency (< 1.8 GiB) compared to the tested CNN and transformer
models.

</details>


### [69] [SurfDist: Interpretable Three-Dimensional Instance Segmentation Using Curved Surface Patches](https://arxiv.org/abs/2507.08223)
*Jackson Borchardt,Saul Kato*

Main category: cs.CV

> SurfDist 是一种用于三维物体实例分割的新型 CNN 架构，它能够产生高性能的表面实例分割结果，特别是对于生物医学图像数据集更为有效。

<details>
  <summary>Details</summary>

**Motivation:** 克服已有模型在实例参数化维度上的限制，并为三维实例分割提供更高效且精确的解决方案。

**Method:** Structure

**Result:** {
  "tldr": "SurfDist 是一种用于三维体积实例分割的卷积神经网络架构，它能够预测具有光滑参数化表面补丁的闭合表面实例，其预测结果可以在不失真的情况下被任意放大。对于类似生物医学成像领域的球形实例而言，SurfDist 可以比 StarDist-3D 提供更紧凑的实例参数化，并且在合成数据和真实世界数据上都展示了更好的性能。",
  "motivation": "解决实例参数化维度与实例体素分辨率的耦合问题，并提供一个在生物医学图像中常见的球形实例上能取得更紧凑参数化的解决方案。",
  "method": "修改 StarDist-3D 模型架构，允许产生具有光滑 Bézier 三角形参数化表面的闭合表面实例预测结果。",
  "result": "SurfDist 在给定的合成数据集和一个真实世界数据集上展示了比 StarDist-3D 更优的性能。",
  "conclusion": "这种方式表明了同时学习实例表面模型和实例隶属性是有效的且具有解释性的。
}

**Conclusion:** 这展示了解释性的实例表面模型与实例隶属性可以同时被有效学习的可能性。

**Abstract:** We present SurfDist, a convolutional neural network architecture for
three-dimensional volumetric instance segmentation. SurfDist enables prediction
of instances represented as closed surfaces composed of smooth parametric
surface patches, specifically bicubic B\'ezier triangles. SurfDist is a
modification of the popular model architecture StarDist-3D which breaks
StarDist-3D's coupling of instance parameterization dimension and instance
voxel resolution, and it produces predictions which may be upsampled to
arbitrarily high resolutions without introduction of voxelization artifacts.
  For datasets with blob-shaped instances, common in biomedical imaging,
SurfDist can outperform StarDist-3D with more compact instance
parameterizations. We detail SurfDist's technical implementation and show one
synthetic and one real-world dataset for which it outperforms StarDist-3D.
These results demonstrate that interpretable instance surface models can be
learned effectively alongside instance membership.

</details>


### [70] [Car Object Counting and Position Estimation via Extension of the CLIP-EBC Framework](https://arxiv.org/abs/2507.08240)
*Seoik Jung,Taekyung Song*

Main category: cs.CV

> 我们研究了CLIP-EBC框架在CARPK数据集上的车辆计数性能，并提出了基于K-means加权聚类的改进方法，试验结果表明其接近现有最优方法。

<details>
  <summary>Details</summary>

**Motivation:** 探索CLIP-EBC框架在车辆对象计数中的应用潜力，并改进其在估计物体位置上的性能。

**Method:** 我们研究了原本用于人群计数的CLIP-EBC框架对车辆计数的适用性，并且提出了基于预测密度图的K-means加权聚类方法来估计物体位置。

**Result:** 实验结果显示，我们的模型在现有方法中表现出第二好的性能，并指出了该框架扩展至定位任务的潜力。

**Conclusion:** CLIP-EBC框架在车辆计数方面有良好的表现，并通过改进方法展示了向更广泛定位任务扩展的潜力。

**Abstract:** In this paper, we investigate the applicability of the CLIP-EBC framework,
originally designed for crowd counting, to car object counting using the CARPK
dataset. Experimental results show that our model achieves second-best
performance compared to existing methods. In addition, we propose a K-means
weighted clustering method to estimate object positions based on predicted
density maps, indicating the framework's potential extension to localization
tasks.

</details>


### [71] [Transfer Learning and Mixup for Fine-Grained Few-Shot Fungi Classification](https://arxiv.org/abs/2507.08248)
*Jason Kahei Tam,Murilo Gustineli,Anthony Miyaguchi*

Main category: cs.CV

> 本文提出的方法结合了视觉变压器模型、数据增强、加权抽样及文本信息，取得了比竞赛基线更好的结果，尤其强调了领域内预训练和抽样策略的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 精准识别真菌种类是计算机视觉中的独特挑战，由于种间变异细小且种内变异大。本论文为FungiCLEF 2025竞赛展示了一种少样本细粒度视觉分类的方法。

**Method:** 本研究团队（DS@GT）采用多种视觉变压器模型、数据增强、加权抽样以及结合文本信息的方法。另外，我们也探索了使用生成式AI模型进行零样本分类，但效果不如视觉模型。

**Result:** 最终模型在竞赛基准上表现优异，并在比赛后评估的私有测试集上排名35/74，这表明在元数据选择和领域自适应多模态学习方面仍有进步空间。

**Conclusion:** 该研究强调了领域特定的预训练和平衡采样策略的有效性，并指出未来工作可以在元数据选择和领域自适应多模态学习方面做进一步研究。

**Abstract:** Accurate identification of fungi species presents a unique challenge in
computer vision due to fine-grained inter-species variation and high
intra-species variation. This paper presents our approach for the FungiCLEF
2025 competition, which focuses on few-shot fine-grained visual categorization
(FGVC) using the FungiTastic Few-Shot dataset. Our team (DS@GT) experimented
with multiple vision transformer models, data augmentation, weighted sampling,
and incorporating textual information. We also explored generative AI models
for zero-shot classification using structured prompting but found them to
significantly underperform relative to vision-based models. Our final model
outperformed both competition baselines and highlighted the effectiveness of
domain specific pretraining and balanced sampling strategies. Our approach
ranked 35/74 on the private test set in post-completion evaluation, this
suggests additional work can be done on metadata selection and domain-adapted
multi-modal learning. Our code is available at
https://github.com/dsgt-arc/fungiclef-2025.

</details>


### [72] [Portable Biomechanics Laboratory: Clinically Accessible Movement Analysis from a Handheld Smartphone](https://arxiv.org/abs/2507.08268)
*J. D. Peiffer,Kunal Shah,Irina Djuraskovic,Shawana Anarwala,Kayan Abdou,Rujvee Patel,Prakash Jayabalan,Brenton Pennicooke,R. James Cotton*

Main category: cs.CV

> 本文介绍了一种便携式生物力学实验室(PBL)，可以使用手持智能手机视频捕捉具有临床意义的生物力学数据，提供了一种可扩展且低负担的监控运动障碍的方法。

<details>
  <summary>Details</summary>

**Motivation:** 尽管临床医生通过视觉观察到运动障碍，但他们缺乏能够客观测量运动的方法。这种差距阻止了生物力学测量在临床实践中的更广泛应用，可能导致更敏感的结果衡量或更早识别障碍。

**Method:** 我们介绍了一种便携式生物力学实验室(PBL)，其中包括一个用于数据收集的安全、基于云端的智能手机应用程序和一种用于将生物力学模型拟合到这些数据的新型算法。

**Result:** 我们在神经外科和运动医学诊所测试了该系统的可用性和效用。测试结果显示了关节角度误差在参与者的3度以内，涵盖了神经系统损伤、下肢假肢使用者、儿科住院病人和对照组。使用PBL计算出的步态参数显示出了高可靠性，并且对临床差异敏感。

**Conclusion:** 这些发现支持了手持智能手机视频作为一种可扩展、低负担的工具，用于捕捉具有临床意义的生物力学数据，为无障碍监控运动障碍提供了一个有希望的途径。

**Abstract:** The way a person moves is a direct reflection of their neurological and
musculoskeletal health, yet it remains one of the most underutilized vital
signs in clinical practice. Although clinicians visually observe movement
impairments, they lack accessible and validated methods to objectively measure
movement in routine care. This gap prevents wider use of biomechanical
measurements in practice, which could enable more sensitive outcome measures or
earlier identification of impairment. We present our Portable Biomechanics
Laboratory (PBL), which includes a secure, cloud-enabled smartphone app for
data collection and a novel algorithm for fitting biomechanical models to this
data. We extensively validated PBL's biomechanical measures using a large,
clinically representative dataset. Next, we tested the usability and utility of
our system in neurosurgery and sports medicine clinics. We found joint angle
errors within 3 degrees across participants with neurological injury,
lower-limb prosthesis users, pediatric inpatients, and controls. In addition to
being easy to use, gait metrics computed from the PBL showed high reliability
and were sensitive to clinical differences. For example, in individuals
undergoing decompression surgery for cervical myelopathy, the mJOA score is a
common patient-reported outcome measure; we found that PBL gait metrics
correlated with mJOA scores and demonstrated greater responsiveness to surgical
intervention than the patient-reported outcomes. These findings support the use
of handheld smartphone video as a scalable, low-burden tool for capturing
clinically meaningful biomechanical data, offering a promising path toward
accessible monitoring of mobility impairments. We release the first clinically
validated method for measuring whole-body kinematics from handheld smartphone
video at
https://intelligentsensingandrehabilitation.github.io/MonocularBiomechanics/ .

</details>


### [73] [Cross-Resolution SAR Target Detection Using Structural Hierarchy Adaptation and Reliable Adjacency Alignment](https://arxiv.org/abs/2507.08290)
*Jiang Qin,Bin Zou,Haolin Li,Lamei Zhang*

Main category: cs.CV

> The paper proposes CR-Net, a method for enhancing performance in cross-resolution SAR target detection through integration of structural and evidential learning theory, achieving state-of-the-art results.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this research is to develop a method that can effectively solve the problem of cross-resolution detection challenge in SAR images, particularly as these images are increasingly being used in applications like urban monitoring where the resolution can vary significantly, impacting the performance of detection models.

**Method:** The paper introduces CR-Net, which combines structure priors and evidential learning theory to address discrepancies in target detection models caused by resolution differences. It incorporates two key modules: Structure-induced Hierarchical Feature Adaptation (SHFA) for enhancing the interpretability of feature adaptation and Reliable Structural Adjacency Alignment (RSAA) for improving reliable semantic alignment and discriminability of the model.

**Result:** Experimental results using different-resolution SAR datasets show that CR-Net improves cross-resolution detection performance, indicating improved intra-domain structure preservation and discriminability compared to previous methods.

**Conclusion:** The research concludes that CR-Net is effective in enhancing cross-resolution SAR target detection performance, preserving intra-domain structures, and improving the discriminability of the detection model, achieving state-of-the-art results.

**Abstract:** In recent years, continuous improvements in SAR resolution have significantly
benefited applications such as urban monitoring and target detection. However,
the improvement in resolution leads to increased discrepancies in scattering
characteristics, posing challenges to the generalization ability of target
detection models. While domain adaptation technology is a potential solution,
the inevitable discrepancies caused by resolution differences often lead to
blind feature adaptation and unreliable semantic propagation, ultimately
degrading the domain adaptation performance. To address these challenges, this
paper proposes a novel SAR target detection method (termed CR-Net), that
incorporates structure priors and evidential learning theory into the detection
model, enabling reliable domain adaptation for cross-resolution detection. To
be specific, CR-Net integrates Structure-induced Hierarchical Feature
Adaptation (SHFA) and Reliable Structural Adjacency Alignment (RSAA). SHFA
module is introduced to establish structural correlations between targets and
achieve structure-aware feature adaptation, thereby enhancing the
interpretability of the feature adaptation process. Afterwards, the RSAA module
is proposed to enhance reliable semantic alignment, by leveraging the secure
adjacency set to transfer valuable discriminative knowledge from the source
domain to the target domain. This further improves the discriminability of the
detection model in the target domain. Based on experimental results from
different-resolution datasets,the proposed CR-Net significantly enhances
cross-resolution adaptation by preserving intra-domain structures and improving
discriminability. It achieves state-of-the-art (SOTA) performance in
cross-resolution SAR target detection.

</details>


### [74] [M2DAO-Talker: Harmonizing Multi-granular Motion Decoupling and Alternating Optimization for Talking-head Generation](https://arxiv.org/abs/2507.08307)
*Kui Jiang,Shiyu Liu,Junjun Jiang,Xin Yang,Hongxun Yang,Xiaopeng Fan*

Main category: cs.CV

> 研究提出了一个新的框架M2DAO-Talker，用于改进音频驱动的虚拟人物生成的技术，通过多层次运动解耦和交替优化方法达到更真实的效果。

<details>
  <summary>Details</summary>

**Motivation:** 音频驱动的虚拟人物生成对于电影制作具有重要意义。然而，现有的3D方法在表示稳定、精细的运动场方面存在局限性，导致渲染出现伪影。

**Method:** 通过系统分析，我们将Talking Head生成重新定义为一个由三个步骤组成的统一框架：视频预处理、运动表示和图像重建。这种方法构成了我们提出的M2DAO-Talker的基础，该方法通过多层次运动解耦和交替优化解决了当前的限制。我们设计了一个新的2D肖像预处理流程，以提取逐帧的变形控制条件。为了改善运动建模，我们制定了一个多层次运动解耦策略，独立地建模非刚性（口腔和面部）和刚体（头部）运动。同时，开发了运动一致性约束以确保头部和躯干的运动一致性，以减少运动混淆引起的穿透伪影。此外，设计了一种交替优化策略，以迭代地优化面部和口腔运动参数，从而生成更真实的视频。

**Result:** 实验结果显示，M2DAO-Talker实现了生成质量2.43 dB PSNR的提高和用户评价的0.64真实感增益，同时具备150 FPS的推理速度。

**Conclusion:** 新的方法和技术提高了音频驱动的虚拟人物生成的真实性、稳定性和速度，展现了在电影制作中的应用潜力。

**Abstract:** Audio-driven talking head generation holds significant potential for film
production. While existing 3D methods have advanced motion modeling and content
synthesis, they often produce rendering artifacts, such as motion blur,
temporal jitter, and local penetration, due to limitations in representing
stable, fine-grained motion fields. Through systematic analysis, we reformulate
talking head generation into a unified framework comprising three steps: video
preprocessing, motion representation, and rendering reconstruction. This
framework underpins our proposed M2DAO-Talker, which addresses current
limitations via multi-granular motion decoupling and alternating
optimization.Specifically, we devise a novel 2D portrait preprocessing pipeline
to extract frame-wise deformation control conditions (motion region
segmentation masks, and camera parameters) to facilitate motion representation.
To ameliorate motion modeling, we elaborate a multi-granular motion decoupling
strategy, which independently models non-rigid (oral and facial) and rigid
(head) motions for improved reconstruction accuracy.Meanwhile, a motion
consistency constraint is developed to ensure head-torso kinematic consistency,
thereby mitigating penetration artifacts caused by motion aliasing. In
addition, an alternating optimization strategy is designed to iteratively
refine facial and oral motion parameters, enabling more realistic video
generation.Experiments across multiple datasets show that M2DAO-Talker achieves
state-of-the-art performance, with the 2.43 dB PSNR improvement in generation
quality and 0.64 gain in user-evaluated video realness versus TalkingGaussian
while with 150 FPS inference speed. Our project homepage is
https://m2dao-talker.github.io/M2DAO-Talk.github.io

</details>


### [75] [Cross-Domain Identity Representation for Skull to Face Matching with Benchmark DataSet](https://arxiv.org/abs/2507.08329)
*Ravi Shankar Prasad,Dinesh Singh*

Main category: cs.CV

> 研究提出了一种利用卷积孪生网络来基于颅骨X光图像进行身份识别的方法，并通过自制的数据集进行了验证，表明该方法具有较好的识别效果。

<details>
  <summary>Details</summary>

**Motivation:** 颅面重建在法医科学中对于识别犯罪和灾难中的受害者至关重要。本文旨在利用计算机视觉领域的最新进展，如深度学习，将给定的颅骨映射到具有已知身份的面部数据集中。

**Method:** 本文提出了一种框架，用于基于颅骨的X光图像进行人员识别，采用的是卷积孪生网络来实现跨域身份表示。孪生网络是一对结构相同的网络，可以训练以发现一个特征空间，在这个空间中，相似的观测结果靠近而不同的观测结果分开。网络通过对比相似和不同数据对来学习。由于获取颅骨和面部图像配对较为困难，作者制作了包含40名志愿者的专用数据集，收集了他们的正面和侧面颅骨X光图像以及光学面部图像。

**Result:** 实验结果表明，该框架在从给定颅骨识别人员方面表现令人满意。

**Conclusion:** 实验验证了卷积孪生网络在跨域识别任务中的有效性，证明了该方法在基于颅骨X光图像进行人员识别任务中的适用性。

**Abstract:** Craniofacial reconstruction in forensic science is crucial for the
identification of the victims of crimes and disasters. The objective is to map
a given skull to its corresponding face in a corpus of faces with known
identities using recent advancements in computer vision, such as deep learning.
In this paper, we presented a framework for the identification of a person
given the X-ray image of a skull using convolutional Siamese networks for
cross-domain identity representation. Siamese networks are twin networks that
share the same architecture and can be trained to discover a feature space
where nearby observations that are similar are grouped and dissimilar
observations are moved apart. To do this, the network is exposed to two sets of
comparable and different data. The Euclidean distance is then minimized between
similar pairs and maximized between dissimilar ones. Since getting pairs of
skull and face images are difficult, we prepared our own dataset of 40
volunteers whose front and side skull X-ray images and optical face images were
collected. Experiments were conducted on the collected cross-domain dataset to
train and validate the Siamese networks. The experimental results provide
satisfactory results on the identification of a person from the given skull.

</details>


### [76] [Interpretability-Aware Pruning for Efficient Medical Image Analysis](https://arxiv.org/abs/2507.08330)
*Nikita Malik,Pratinav Seth,Neeraj Kumar Singh,Chintan Chitroda,Vinay Kumar Sankarapu*

Main category: cs.CV

> 介绍了一种解释性指导剪枝框架，实现了高压缩率而仅损失极小的准确性，并且保持了临床相关性。

<details>
  <summary>Details</summary>

**Motivation:** 深度学习推动了医学影像分析领域的显著进步，但在临床实践中的应用受到现代模型规模大和缺乏透明度的限制。解释性技术的进展使我们能够评估在医学影像任务上训练的神经网络中各组件的贡献。

**Method:** 通过采用解释性指导剪枝框架，在减少模型复杂性的同时保留预测性能和透明度。通过选择性保留每一层中最相关的部分，该方法实现了有针对性的压缩，保持了临床有意义的表示。

**Result:** 在多个医学影像分类基准上进行的实验表明，该方法能够在不损失准确性的情况下实现高压缩率。

**Conclusion:** 这种方法为在医疗保健环境中部署的轻量级、可解释模型铺平了道路。

**Abstract:** Deep learning has driven significant advances in medical image analysis, yet
its adoption in clinical practice remains constrained by the large size and
lack of transparency in modern models. Advances in interpretability techniques
such as DL-Backtrace, Layer-wise Relevance Propagation, and Integrated
Gradients make it possible to assess the contribution of individual components
within neural networks trained on medical imaging tasks. In this work, we
introduce an interpretability-guided pruning framework that reduces model
complexity while preserving both predictive performance and transparency. By
selectively retaining only the most relevant parts of each layer, our method
enables targeted compression that maintains clinically meaningful
representations. Experiments across multiple medical image classification
benchmarks demonstrate that this approach achieves high compression rates with
minimal loss in accuracy, paving the way for lightweight, interpretable models
suited for real-world deployment in healthcare settings.

</details>


### [77] [CoCo-Bot: Energy-based Composable Concept Bottlenecks for Interpretable Generative Models](https://arxiv.org/abs/2507.08334)
*Sangwon Kim,In-su Jang,Pyongkun Kim,Kwang-Ju Kim*

Main category: cs.CV

> CoCo-Bot是一种可组合的概念瓶颈生成模型，它通过明确的概念传输所有信息，避免依赖辅助视觉线索，从而提高了可解释性和组合性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管概念瓶颈模型(CBMs)在生成模型中提供了可解释性和可控性，但它们常常依赖辅助视觉线索来弥补概念未捕捉到的信息，这削弱了可解释性和组合性。

**Method:** 我们提出了一种名为CoCo-Bot的生成模型，它通过明确的概念传输所有信息，消除了对辅助视觉线索的依赖，从而提高了可解释性和组合性。该模型使用基于扩散的能量函数支持对任意概念的后验干预，如概念组合和否定。

**Result:** 实验显示，基于CelebA-HQ数据集的StyleGAN2预训练模型，CoCo-Bot在保持具有竞争力的视觉质量的同时，改善了概念水平的控制力和可解释性。

**Conclusion:** CoCo-Bot通过传递所有信息而无需辅助视觉线索，解决了先前CBMs可解释性和组合性的问题，并在生成模型中实现了更高质量的操控性和可解释性。

**Abstract:** Concept Bottleneck Models (CBMs) provide interpretable and controllable
generative modeling by routing generation through explicit,
human-understandable concepts. However, previous generative CBMs often rely on
auxiliary visual cues at the bottleneck to compensate for information not
captured by the concepts, which undermines interpretability and
compositionality. We propose CoCo-Bot, a post-hoc, composable concept
bottleneck generative model that eliminates the need for auxiliary cues by
transmitting all information solely through explicit concepts. Guided by
diffusion-based energy functions, CoCo-Bot supports robust post-hoc
interventions-such as concept composition and negation-across arbitrary
concepts. Experiments using StyleGAN2 pre-trained on CelebA-HQ show that
CoCo-Bot improves concept-level controllability and interpretability, while
maintaining competitive visual quality.

</details>


### [78] [Single-Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement](https://arxiv.org/abs/2507.08340)
*Jia-Xuan Jiang,Jiashuai Liu,Hongtao Wu,Yifeng Wu,Zhong Wang,Qi Bi,Yefeng Zheng*

Main category: cs.CV

> 研究引入了两个模块来解决多模态模型在跨癌症场景中的泛化问题，通过实验验证了这些模型在四个癌症类型上的优越泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 当前的多模态方法主要集中在单一类型的癌症上，忽视了跨癌症类型泛化的挑战。此工作首次揭示了在跨癌症场景中，多模态预后模型往往不如单模态模型泛化得好，提出了跨癌症单域泛化任务以解决此问题。

**Method:** 提出了两个即插即用模块：Sparse Dirac Information Rebalancer (SDIR)和Cancer-aware Distribution Entanglement (CADE)来解决跨癌症类型的多模态预后预测中的问题。SDIR通过伯努利稀疏化和狄拉克启发的稳定化来减少强特征的主导作用，增强弱特征信号；CADE则设计用于综合目标领域分布，在潜在空间中融合局部形态线索和全局基因表达。

**Result:** 实验在四种癌症类型的基准数据集上进行，结果显示所提模型具有更强的跨癌症类型泛化能力，为实践中稳健的跨癌症多模态预后提供了基础。

**Conclusion:** 所提出的SDIR和CADE模块能够有效解决跨癌症类型多模态预后预测中的泛化挑战，提供了新型泛化任务的解决方案，有助于临床实践中的应用。

**Abstract:** Deep learning has shown remarkable performance in integrating multimodal data
for survival prediction. However, existing multimodal methods mainly focus on
single cancer types and overlook the challenge of generalization across
cancers. In this work, we are the first to reveal that multimodal prognosis
models often generalize worse than unimodal ones in cross-cancer scenarios,
despite the critical need for such robustness in clinical practice. To address
this, we propose a new task: Cross-Cancer Single Domain Generalization for
Multimodal Prognosis, which evaluates whether models trained on a single cancer
type can generalize to unseen cancers. We identify two key challenges: degraded
features from weaker modalities and ineffective multimodal integration. To
tackle these, we introduce two plug-and-play modules: Sparse Dirac Information
Rebalancer (SDIR) and Cancer-aware Distribution Entanglement (CADE). SDIR
mitigates the dominance of strong features by applying Bernoulli-based
sparsification and Dirac-inspired stabilization to enhance weaker modality
signals. CADE, designed to synthesize the target domain distribution, fuses
local morphological cues and global gene expression in latent space.
Experiments on a four-cancer-type benchmark demonstrate superior
generalization, laying the foundation for practical, robust cross-cancer
multimodal prognosis. Code is available at
https://github.com/HopkinsKwong/MCCSDG

</details>


### [79] [Towards Imperceptible JPEG Image Hiding: Multi-range Representations-driven Adversarial Stego Generation](https://arxiv.org/abs/2507.08343)
*Junxue Yang,Xin Liao,Weixuan Tang,Jianhua Yang,Zheng Qin*

Main category: cs.CV

> MRAG, a novel adversarial stego generation framework, integrates convolution and transformer operators with a multi-grained approach to achieve better resistance against steganalysis, resulting in state-of-the-art performance in deep hiding.

<details>
  <summary>Details</summary>

**Motivation:** to improve upon existing deep hiding schemes which are easily detected by advanced steganalyzers due to limitations in feature extraction and reliance on single-range operators, leading to large payloads and reduced hiding capability.

**Method:** existing deep hiding schemes are susceptible to detection due to large payloads and rely solely on either convolution or transformer operators which limits their feature extraction capabilities. MRAG, the proposed solution, integrates convolution and transformer operators to leverage local and global-range dependency modeling, respectively, and introduces multi-grained information by using coarse-grained and fine-grained frequency decomposed images. An angle-norm disentanglement loss is also used to ensure generated stegos are close to covers in norm and angle space, thus resisting more advanced steganalysis.

**Result:** extensive experiments show that MRAG achieves state-of-the-art performance in deep hiding, indicating its effectiveness in resisting detection while maintaining secret restorability and imperceptibility.

**Conclusion:** MRAG, by integrating convolution and transformer operators with a multi-grained approach, provides an advanced method for deep hiding that not only resists detection but also maintains high-quality secret communications.

**Abstract:** Deep hiding has been exploring the hiding capability of deep learning-based
models, aiming to conceal image-level messages into cover images and reveal
them from generated stego images. Existing schemes are easily detected by
steganalyzers due to their large payloads and their limitation to feature
extraction based solely on either pure convolution or pure transformer
operators within a single range, as well as pixel-level loss constraints. To
address the issue, in this paper, we introduce generation-based adversarial
attacks into color JPEG image deep hiding and propose a multi-range
representations-driven adversarial stego generation framework called MRAG from
a steganalysis perspective. Specifically, we integrate the local-range neighbor
reception characteristic of the convolution and the global-range dependency
modeling of the transformer to construct MRAG. Meanwhile, we use the
transformed images obtained through coarse-grained and fine-grained frequency
decomposition as inputs, introducing multi-grained information. Furthermore, a
features angle-norm disentanglement loss is designed to constrain the generated
stegos closer to covers in the angle and norm space of the steganalyzer's
classified features. Consequently, small yet effective adversarial
perturbations can be injected into the process of generating stegos, ensuring
that stegos maintain favorable secret restorability and imperceptibility.
Extensive experiments demonstrate that MRAG can achieve state-of-the-art
performance.

</details>


### [80] [MM-Gesture: Towards Precise Micro-Gesture Recognition through Multimodal Fusion](https://arxiv.org/abs/2507.08344)
*Jihao Gu,Fei Wang,Kun Li,Yanyan Wei,Zhiliang Wu,Dan Guo*

Main category: cs.CV

> 本文介绍了 MM-Gesture，这是一个由研发团队 HFUT-VUT 开发的多模态手势识别解决方案，在第3届 MiGA 挑战赛的手势分类赛道中排名第一，识别准确率达到 73.213%。

<details>
  <summary>Details</summary>

**Motivation:** 研发团队 HFUT-VUT 为了在第3届 MiGA 挑战赛的手势分类赛道上取得优越表现，开发了 MM-Gesture 解决方案，旨在解决微手势识别中的精度问题，相比之下优于现有的各类方法。

**Method:** MM-Gesture 是一种专为识别微妙且短暂的微手势 (MGs) 设计的多模态融合框架。该框架结合了关节、肢体、RGB 视频、泰勒级数视频、光流视频和深度视频等多种模态的互补线索。该方法使用了 PoseConv3D 和 Video Swin Transformer 架构，并结合了新颖的模态加权集成策略，在更大的 MA-52 数据集上通过迁移学习进一步提升了 RGB 模态的表现。

**Result:** 在 iMiGUE 基准测试上的实验结果包括不同模态的消融研究，验证了 MM-Gesture 方法的有效性，取得了 73.213% 的 top-1 准确率。

**Conclusion:** MM-Gesture 方案的有效性通过在 iMiGUE 基准测试中的实验得到了验证，达到了 73.213% 的 top-1 准确率。这表明该方案在微手势识别方面具有优越性能。

**Abstract:** In this paper, we present MM-Gesture, the solution developed by our team
HFUT-VUT, which ranked 1st in the micro-gesture classification track of the 3rd
MiGA Challenge at IJCAI 2025, achieving superior performance compared to
previous state-of-the-art methods. MM-Gesture is a multimodal fusion framework
designed specifically for recognizing subtle and short-duration micro-gestures
(MGs), integrating complementary cues from joint, limb, RGB video,
Taylor-series video, optical-flow video, and depth video modalities. Utilizing
PoseConv3D and Video Swin Transformer architectures with a novel
modality-weighted ensemble strategy, our method further enhances RGB modality
performance through transfer learning pre-trained on the larger MA-52 dataset.
Extensive experiments on the iMiGUE benchmark, including ablation studies
across different modalities, validate the effectiveness of our proposed
approach, achieving a top-1 accuracy of 73.213%.

</details>


### [81] [Cycle Context Verification for In-Context Medical Image Segmentation](https://arxiv.org/abs/2507.08357)
*Shishuai Hu,Zehui Liao,Liangli Zhen,Huazhu Fu,Yong Xia*

Main category: cs.CV

> 提出了一种名为Cycle Context Verification (CCV)的新框架，通过自验证预测和增强上下文对齐来改进基于In-context learning (ICL)的医学图像分割技术。

<details>
  <summary>Details</summary>

**Motivation:** ICL在实现通用医学图像分割方面显示出潜力，但其性能依赖于查询图像与上下文图像掩模对之间的对齐，且在临床场景下选取最适上下文图像是一个挑战。

**Method:** CCV框架采用一个循环流程，模型首先生成查询图像的分割掩模，然后交换查询与上下文图像的角色，通过预测原始上下文图像的掩模来验证预测，通过调整查询特有提示来改善对齐。

**Result:** 评估了CCV在七个医学图像分割数据集上的表现，使用了两种ICL基础模型，结果显示CCV优于现有方法。

**Conclusion:** CCV展现了增强ICL的基础上的分割能力，成为一种针对通用医学图像分割的健壮解决方案。

**Abstract:** In-context learning (ICL) is emerging as a promising technique for achieving
universal medical image segmentation, where a variety of objects of interest
across imaging modalities can be segmented using a single model. Nevertheless,
its performance is highly sensitive to the alignment between the query image
and in-context image-mask pairs. In a clinical scenario, the scarcity of
annotated medical images makes it challenging to select optimal in-context
pairs, and fine-tuning foundation ICL models on contextual data is infeasible
due to computational costs and the risk of catastrophic forgetting. To address
this challenge, we propose Cycle Context Verification (CCV), a novel framework
that enhances ICL-based medical image segmentation by enabling
self-verification of predictions and accordingly enhancing contextual
alignment. Specifically, CCV employs a cyclic pipeline in which the model
initially generates a segmentation mask for the query image. Subsequently, the
roles of the query and an in-context pair are swapped, allowing the model to
validate its prediction by predicting the mask of the original in-context
image. The accuracy of this secondary prediction serves as an implicit measure
of the initial query segmentation. A query-specific prompt is introduced to
alter the query image and updated to improve the measure, thereby enhancing the
alignment between the query and in-context pairs. We evaluated CCV on seven
medical image segmentation datasets using two ICL foundation models,
demonstrating its superiority over existing methods. Our results highlight
CCV's ability to enhance ICL-based segmentation, making it a robust solution
for universal medical image segmentation. The code will be available at
https://github.com/ShishuaiHu/CCV.

</details>


### [82] [Understanding Driving Risks using Large Language Models: Toward Elderly Driver Assessment](https://arxiv.org/abs/2507.08367)
*Yuki Yoshihara,Linjing Jiang,Nihan Karatas,Hitoshi Kanamori,Asuka Harada,Takahiro Tanaka*

Main category: cs.CV

> 本研究表明多模态大型语言模型在经过适当提示设计后，可作为老年驾驶评估支持工具，通过零样本、少样本及多样本学习策略提升其对复杂交通场景的理解。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机旨在探索多模态大规模语言模型在模拟人类对交通场景解释的能力，特别是对于老年驾驶者评估相关的任务。

**Method:** 本研究利用零样本、少样本和多样本提示策略评估了多模态大规模语言模型ChatGPT-4o在静态车载摄像头图像中进行交通场景解释的能力，主要考察了三个与老年司机评估相关的任务：交通密度评估、交叉口视野评估及停车标志识别。

**Result:** 结果显示提示设计显著影响性能，交叉口视野评估中的召回率从零样本情形的21.7%提升至多样本情形的57.0%；交通密度评估中的一致性则从53.5%提升至67.6%。在停车标志检测上，模型表现出了较高的精确度（最高达86.3%）但召回率较低（约为76.7%），显示出其保守的响应倾向。

**Conclusion:** 大型语言模型在设计良好的提示下，有助于驾驶风险评估的支持工具，未来研究可考虑利用更大的数据集、多样化的注释员以及下一代模型架构进行扩展。

**Abstract:** This study investigates the potential of a multimodal large language model
(LLM), specifically ChatGPT-4o, to perform human-like interpretations of
traffic scenes using static dashcam images. Herein, we focus on three judgment
tasks relevant to elderly driver assessments: evaluating traffic density,
assessing intersection visibility, and recognizing stop signs recognition.
These tasks require contextual reasoning rather than simple object detection.
Using zero-shot, few-shot, and multi-shot prompting strategies, we evaluated
the performance of the model with human annotations serving as the reference
standard. Evaluation metrics included precision, recall, and F1-score. Results
indicate that prompt design considerably affects performance, with recall for
intersection visibility increasing from 21.7% (zero-shot) to 57.0%
(multi-shot). For traffic density, agreement increased from 53.5% to 67.6%. In
stop-sign detection, the model demonstrated high precision (up to 86.3%) but a
lower recall (approximately 76.7%), indicating a conservative response
tendency. Output stability analysis revealed that humans and the model faced
difficulties interpreting structurally ambiguous scenes. However, the model's
explanatory texts corresponded with its predictions, enhancing
interpretability. These findings suggest that, with well-designed prompts, LLMs
hold promise as supportive tools for scene-level driving risk assessments.
Future studies should explore scalability using larger datasets, diverse
annotators, and next-generation model architectures for elderly driver
assessments.

</details>


### [83] [Unsupervised Methods for Video Quality Improvement: A Survey of Restoration and Enhancement Techniques](https://arxiv.org/abs/2507.08375)
*Alexandra Malyugina,Yini Li,Joanne Lin,Nantheera Anantrasirichai*

Main category: cs.CV

> 这篇综述文章详细介绍了视频恢复和增强技术，重点是无监督方法，分析了现有技术的挑战及未来研究方向，目的是改善视频质量及计算机视觉任务表现。

<details>
  <summary>Details</summary>

**Motivation:** 旨在通过综述视频恢复和增强技术，特别是无监督方法，来提高视频质量并改进计算机视觉任务的性能。

**Method:** 该论文综述了无监督视频恢复和增强技术，将其分为领域翻译、自监督信号设计以及盲点或噪声方法三大类，同时讨论了无监督视频恢复和增强中使用的损失函数分类，以及合成配对数据集在客观评估中的作用。

**Result:** 论文详细介绍了不同类型无监督方法的优势和局限性，分析了现有技术的挑战，并提出了未来研究的方向。

**Conclusion:** 最后，该论文明确了领域内的一些关键挑战，并指出了未来的研究方向。

**Abstract:** Video restoration and enhancement are critical not only for improving visual
quality, but also as essential pre-processing steps to boost the performance of
a wide range of downstream computer vision tasks. This survey presents a
comprehensive review of video restoration and enhancement techniques with a
particular focus on unsupervised approaches. We begin by outlining the most
common video degradations and their underlying causes, followed by a review of
early conventional and deep learning methods-based, highlighting their
strengths and limitations. We then present an in-depth overview of unsupervised
methods, categorise by their fundamental approaches, including domain
translation, self-supervision signal design and blind spot or noise-based
methods. We also provide a categorization of loss functions employed in
unsupervised video restoration and enhancement, and discuss the role of paired
synthetic datasets in enabling objective evaluation. Finally, we identify key
challenges and outline promising directions for future research in this field.

</details>


### [84] [From Enhancement to Understanding: Build a Generalized Bridge for Low-light Vision via Semantically Consistent Unsupervised Fine-tuning](https://arxiv.org/abs/2507.08380)
*Sen Wang,Shao Zeng,Tianjun Gu,Zhizhong Zhang,Ruixin Zhang,Shouhong Ding,Jingyun Zhang,Jun Wang,Xin Tan,Yuan Xie,Lizhuang Ma*

Main category: cs.CV

> 本文提出了Generalized Enhancement For Understanding (GEFU)框架，利用生成扩散模型和一系列创新技术来优化低光照条件下的图像增强和理解任务的性能，显著提升了泛化能力和可扩展性。

<details>
  <summary>Details</summary>

**Motivation:** 传统的低光照图像处理方法存在泛化能力和性能局限性，特别是将图像增强和高级视觉理解割裂开来。本文旨在通过构建一个通用框架，即Generalized Enhancement For Understanding (GEFU)，来解决这些问题，提高低光照图像处理技术的性能和泛化能力。

**Method:** 本文提出了Generalized Enhancement For Understanding (GEFU)，该框架利用预训练的生成扩散模型优化低光照图像，并考虑光照感知图像提示和循环注意力适配器以提高语义一致性。还提出了标签和反射一致性来改善无监督训练中的语义学习。

**Result:** 低光照条件下，图像增强与高级视觉理解通常被分开处理。现有增强方法依赖物理或几何先验，限制了泛化能力，且评估主要集中在视觉质量而非下游任务性能。低光照理解由于标注数据稀缺，主要依赖任务特定的领域适应，缺乏可扩展性。为解决这些挑战，我们提出Generalized Enhancement For Understanding (GEFU)，这是一个普遍化的框架，旨在改善泛化性能和可扩展性。该方法利用预训练的生成扩散模型优化图像，实现零样本泛化性能。我们提出了Semantically Consistent Unsupervised Fine-tuning (SCUF)，通过光照感知图像提示来指导图像生成，并提出循环注意力适配器来最大化其语义潜能。此外，为了减轻无监督训练中的语义退化，我们提出标签和反射一致性来学习高级语义和图像级别的空间语义。大量实验表明，我们提出的方法在传统图像质量和包括分类、检测、语义分割在内的低光照任务上均优于当前最先进的方法。

**Conclusion:** 我们的研究展示了通过结合生成扩散模型、光照感知提示和循环注意力适配器等技术，可以在低光照条件下提供高质量的图像增强以及提高视觉理解任务性能的有效方法。实验结果验证了该方法在传统图像质量和低光照理解任务上的优势。

**Abstract:** Low-level enhancement and high-level visual understanding in low-light vision
have traditionally been treated separately. Low-light enhancement improves
image quality for downstream tasks, but existing methods rely on physical or
geometric priors, limiting generalization. Evaluation mainly focuses on visual
quality rather than downstream performance. Low-light visual understanding,
constrained by scarce labeled data, primarily uses task-specific domain
adaptation, which lacks scalability. To address these challenges, we build a
generalized bridge between low-light enhancement and low-light understanding,
which we term Generalized Enhancement For Understanding (GEFU). This paradigm
improves both generalization and scalability. To address the diverse causes of
low-light degradation, we leverage pretrained generative diffusion models to
optimize images, achieving zero-shot generalization performance. Building on
this, we propose Semantically Consistent Unsupervised Fine-tuning (SCUF).
Specifically, to overcome text prompt limitations, we introduce an
illumination-aware image prompt to explicitly guide image generation and
propose a cycle-attention adapter to maximize its semantic potential. To
mitigate semantic degradation in unsupervised training, we propose caption and
reflectance consistency to learn high-level semantics and image-level spatial
semantics. Extensive experiments demonstrate that our proposed method
outperforms current state-of-the-art methods in traditional image quality and
GEFU tasks including classification, detection, and semantic segmentation.

</details>


### [85] [Smelly, dense, and spreaded: The Object Detection for Olfactory References (ODOR) dataset](https://arxiv.org/abs/2507.08384)
*Mathias Zinnen,Prathmesh Madhu,Inger Leemans,Peter Bell,Azhar Hussian,Hang Tran,Ali Hürriyetoğlu,Andreas Maier,Vincent Christlein*

Main category: cs.CV

> 本文介绍了一个新的数据集ODOR，该数据集包含大量的细致分类和复杂物体分布的信息，旨在为艺术作品中的物体检测提供更广泛的基准分析。

<details>
  <summary>Details</summary>

**Motivation:** 现有数据集对于图像中心偏见明显，且缺乏详细分类。本文为文化艺术的计算机视觉应用中的物体检测提供了更广泛的基准分析。

**Method:** 统计分析数据集的属性，并就对象检测模型提供广泛的基线分析。

**Result:** ODOR数据集提供了详细的分类、密集重叠的对象以及图像全画布的空间分布。通过基线分析展示了数据集的挑战性。

**Conclusion:** ODOR数据集为艺术作品中的物体检测和视觉文化遗产研究提出了新的挑战，激励进一步的研究工作。

**Abstract:** Real-world applications of computer vision in the humanities require
algorithms to be robust against artistic abstraction, peripheral objects, and
subtle differences between fine-grained target classes. Existing datasets
provide instance-level annotations on artworks but are generally biased towards
the image centre and limited with regard to detailed object classes. The
proposed ODOR dataset fills this gap, offering 38,116 object-level annotations
across 4712 images, spanning an extensive set of 139 fine-grained categories.
Conducting a statistical analysis, we showcase challenging dataset properties,
such as a detailed set of categories, dense and overlapping objects, and
spatial distribution over the whole image canvas. Furthermore, we provide an
extensive baseline analysis for object detection models and highlight the
challenging properties of the dataset through a set of secondary studies.
Inspiring further research on artwork object detection and broader visual
cultural heritage studies, the dataset challenges researchers to explore the
intersection of object recognition and smell perception.

</details>


### [86] [Subject-Consistent and Pose-Diverse Text-to-Image Generation](https://arxiv.org/abs/2507.08396)
*Zhanxin Gao,Beier Zhu,Liang Yao,Jian Yang,Ying Tai*

Main category: cs.CV

> 本文提出了一种名为CoDi的框架，通过两阶段方法（身份传输和身份细化）实现了主体一致性生成，同时保持姿态和布局的多样性。

<details>
  <summary>Details</summary>

**Motivation:** 受扩散模型渐进性质的启发，我们提出了一种名为CoDi的框架，用于解决文本到图像生成中主体一致性的问题。在这一过程中，我们希望同时保持姿态和布局的多样性，以增强视觉表现力。

**Method:** 我们的方法名为CoDi，它采用两阶段策略：身份传输（IT）和身份细化（IR）。IT在早期去噪步骤中运行，利用最优传输将身份特征以姿势感知的方式转移到每个目标图像中。这在保持姿态多样性的同时促进了主体的一致性。IR应用于后期去噪步骤，选择最显著的身份特征来进一步细化主体细节。

**Result:** 实验结果表明，CoDi在主体一致性、姿势多样性以及提示保真度方面都取得了优于现有方法的视觉感知表现和更强的整体性能。

**Conclusion:** 研究结论表明，CoDi框架能够在保持姿态和布局多样性的同时实现了图像生成的主体一致性，并且在多个评估指标上达到了最佳性能。

**Abstract:** Subject-consistent generation (SCG)-aiming to maintain a consistent subject
identity across diverse scenes-remains a challenge for text-to-image (T2I)
models. Existing training-free SCG methods often achieve consistency at the
cost of layout and pose diversity, hindering expressive visual storytelling. To
address the limitation, we propose subject-Consistent and pose-Diverse T2I
framework, dubbed as CoDi, that enables consistent subject generation with
diverse pose and layout. Motivated by the progressive nature of diffusion,
where coarse structures emerge early and fine details are refined later, CoDi
adopts a two-stage strategy: Identity Transport (IT) and Identity Refinement
(IR). IT operates in the early denoising steps, using optimal transport to
transfer identity features to each target image in a pose-aware manner. This
promotes subject consistency while preserving pose diversity. IR is applied in
the later denoising steps, selecting the most salient identity features to
further refine subject details. Extensive qualitative and quantitative results
on subject consistency, pose diversity, and prompt fidelity demonstrate that
CoDi achieves both better visual perception and stronger performance across all
metrics. The code is provided in https://github.com/NJU-PCALab/CoDi.

</details>


### [87] [PanMatch: Unleashing the Potential of Large Vision Models for Unified Matching Models](https://arxiv.org/abs/2507.08400)
*Yongjian Zhang,Longguang Wang,Kunhong Li,Ye Zhang,Yun Wang,Liang Lin,Yulan Guo*

Main category: cs.CV

> PanMatch is a flexible foundational model for robust two-frame correspondence matching tasks, such as stereo matching, optical flow, and feature matching, using a universal set of model weights and a robust feature extractor.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind PanMatch is to eliminate the need for task-specific architectures or ensemble models by unifying these tasks under a 2D displacement estimation framework, achieving better generalization and cross-domain performance.

**Method:** PanMatch leverages a robust feature transformation pipeline and pretrained on a cross-domain dataset to provide zero-shot matching capabilities across various tasks.

**Result:** PanMatch outperforms existing methods like UniMatch and Flow-Anything in cross-task evaluations and surpasses many specialized algorithms in abnormal scenarios, such as rainy day and satellite imagery.

**Conclusion:** The success of PanMatch demonstrates the potential for a single model to effectively handle multiple correspondence matching tasks with generalized robustness and versatility, setting a benchmark for future research in the field.

**Abstract:** This work presents PanMatch, a versatile foundation model for robust
correspondence matching. Unlike previous methods that rely on task-specific
architectures and domain-specific fine-tuning to support tasks like stereo
matching, optical flow or feature matching, our key insight is that any
two-frame correspondence matching task can be addressed within a 2D
displacement estimation framework using the same model weights. Such a
formulation eliminates the need for designing specialized unified architectures
or task-specific ensemble models. Instead, it achieves multi-task integration
by endowing displacement estimation algorithms with unprecedented
generalization capabilities. To this end, we highlight the importance of a
robust feature extractor applicable across multiple domains and tasks, and
propose the feature transformation pipeline that leverage all-purpose features
from Large Vision Models to endow matching baselines with zero-shot cross-view
matching capabilities. Furthermore, we assemble a cross-domain dataset with
near 1.8 million samples from stereo matching, optical flow, and feature
matching domains to pretrain PanMatch. We demonstrate the versatility of
PanMatch across a wide range of domains and downstream tasks using the same
model weights. Our model outperforms UniMatch and Flow-Anything on cross-task
evaluations, and achieves comparable performance to most state-of-the-art
task-specific algorithms on task-oriented benchmarks. Additionally, PanMatch
presents unprecedented zero-shot performance in abnormal scenarios, such as
rainy day and satellite imagery, where most existing robust algorithms fail to
yield meaningful results.

</details>


### [88] [Deep Hashing with Semantic Hash Centers for Image Retrieval](https://arxiv.org/abs/2507.08404)
*Li Chen,Rui Liu,Yuxiang Zhou,Xudong Ma,Yong Chen,Dell Zhang*

Main category: cs.CV

> 提出了一种新的基于语义哈希中心的三阶段框架（SHC）用于提高大规模图像检索性能。新方法在多个公有数据集上的实验展示了优于现有方法的大幅提升。

<details>
  <summary>Details</summary>

**Motivation:** 当前的方法通常按监督类型分类为点式、成对式和列表式。最近的点式技术（例如CSQ、MDS）通过预分配给每个类的哈希中心提高了检索性能，增强了各种数据集上哈希码的可区分性。然而，这些方法依赖于与数据无关的算法来生成哈希中心，这忽视了类之间的语义关系，可能降低检索性能。本研究引入了语义哈希中心的概念，以改进哈希码生成方法。

**Method:** 我们提出了一个三阶段框架SHC，用于生成保持语义结构的哈希码。首先，我们开发了一个分类网络，用于通过数据相关的相似性计算来识别类别之间的语义相似性。其次，引入了一个优化算法来生成语义哈希中心，保持语义相关性，同时强制中心之间的最小距离，以避免过于相似的哈希码。最后，通过训练一个深度哈希网络，使用这些语义中心将图像转换为二进制哈希码。

**Result:** 在几个公共数据集上的大规模检索任务的实验结果表明，SHC显著提高了检索性能。具体而言，SHC在MAP@100、MAP@1000和MAP@ALL度量上分别比最先进的方法提高了+7.26%、+7.62%和+11.71%。

**Conclusion:** 我们的研究表明，通过采用数据相关的语义哈希中心，SHC框架能够有效提高图像检索性能。通过对类之间的语义关系进行建模和优化，本方法能够产生更能代表图像内容的哈希码。

**Abstract:** Deep hashing is an effective approach for large-scale image retrieval.
Current methods are typically classified by their supervision types:
point-wise, pair-wise, and list-wise. Recent point-wise techniques (e.g., CSQ,
MDS) have improved retrieval performance by pre-assigning a hash center to each
class, enhancing the discriminability of hash codes across various datasets.
However, these methods rely on data-independent algorithms to generate hash
centers, which neglect the semantic relationships between classes and may
degrade retrieval performance.
  This paper introduces the concept of semantic hash centers, building on the
idea of traditional hash centers. We hypothesize that hash centers of
semantically related classes should have closer Hamming distances, while those
of unrelated classes should be more distant. To this end, we propose a
three-stage framework, SHC, to generate hash codes that preserve semantic
structure.
  First, we develop a classification network to identify semantic similarities
between classes using a data-dependent similarity calculation that adapts to
varying data distributions. Second, we introduce an optimization algorithm to
generate semantic hash centers, preserving semantic relatedness while enforcing
a minimum distance between centers to avoid excessively similar hash codes.
Finally, a deep hashing network is trained using these semantic centers to
convert images into binary hash codes.
  Experimental results on large-scale retrieval tasks across several public
datasets show that SHC significantly improves retrieval performance.
Specifically, SHC achieves average improvements of +7.26%, +7.62%, and +11.71%
in MAP@100, MAP@1000, and MAP@ALL metrics, respectively, over state-of-the-art
methods.

</details>


### [89] [Multi-modal Mutual-Guidance Conditional Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.08410)
*Shijun Yang,Xiang Zhang,Wanqing Zhao,Hangzai Luo,Sheng Zhong,Jinye Peng,Jianping Fan*

Main category: cs.CV

> 提出了 MuGCP 方法，利用 MLLMs 生成富含语义知识的SCP，并改进了跨模态对齐机制，显著提高了多模态任务性能，超越了现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决现有提示学习方法在处理未见实例的类别嵌入分布不足，以及跨模态对齐主要局限在视觉和文本编码器的最终输出层，从而限制了它们保留预训练多模态嵌入空间拓扑一致性的能力。

**Method:** MuGCP (Multi-modal Mutual-Guidance Conditional Prompt Learning) 被提出，这是一种用于条件提示生成的新范式。它利用 Multi-modal Large Language Models (MLLMs) 作为条件提示学习器，以自适应生成 Semantic Conditional Prompts (SCP)，这些提示富含对图像实例来说的具体语义知识。此外，引入了 Attention Mutual-Guidance (AMG) 模块以促进视觉和语义信息之间的交互，并生成 Visual Conditional Prompts (VCP) 以提升模型在多模态任务上的性能。还介绍了一种 Multi-Prompt Fusion (MPF) 机制，用于将 SCP 和 VCP 与上下文提示融合，以确保各种提示之间的无缝协调，增强类别嵌入和实例特定知识的建模能力。

**Result:** MuGCP 在 14 个不同数据集上超过了现有最先进方法的表现。

**Conclusion:** 通过引入 MuGCP 方法，有效地提升了 Vision-Language 模型在多模态任务中的性能，尤其是在未见实例的类别嵌入分布和跨模态融合方面。

**Abstract:** Prompt learning facilitates the efficient adaptation of Vision-Language
Models (VLMs) to various downstream tasks. However, it faces two significant
challenges: (1) inadequate modeling of class embedding distributions for unseen
instances, leading to suboptimal generalization on novel classes; (2)
prevailing methodologies predominantly confine cross-modal alignment to the
final output layer of vision and text encoders, which fundamentally limits
their capacity to preserve topological consistency with pre-trained multi-modal
embedding spaces. To this end, we introduce MuGCP (Multi-modal Mutual-Guidance
Conditional Prompt Learning), a novel paradigm designed for conditional prompt
generation. MuGCP leverages Multi-modal Large Language Models (MLLMs) as
conditional prompt learners to adaptively generate Semantic Conditional Prompts
(SCP) that incorporate rich, fine-grained high-level semantic knowledge for
image instances. To ensure effective alignment and interaction across the
multi-modal space of Vision-Language Models (VLMs), we introduce the Attention
Mutual-Guidance (AMG) module, which facilitates interactions between visual and
semantic information. Through mutual guidance, the AMG module generates Visual
Conditional Prompts (VCP), enhancing the model's performance in multi-modal
tasks. Additionally, we present a Multi-Prompt Fusion (MPF) mechanism that
integrates SCP and VCP with contextual prompts, ensuring seamless coordination
among the different prompts and enhancing the modeling of class embeddings and
instance-specific knowledge. Our MuGCP outperforms existing state-of-the-art
methods on 14 different datasets. The code will be made available after
publication.

</details>


### [90] [InstaScene: Towards Complete 3D Instance Decomposition and Reconstruction from Cluttered Scenes](https://arxiv.org/abs/2507.08416)
*Zesong Yang,Bangbang Yang,Wenqi Dong,Chenxuan Cao,Liyuan Cui,Yuewen Ma,Zhaopeng Cui,Hujun Bao*

Main category: cs.CV

> 我们提出了InstaScene，一种新的3D感知范式，旨在通过精确的实例分解和克服有限观察导致的不完整性来改善复杂场景中的3D感知。

<details>
  <summary>Details</summary>

**Motivation:** 尽管有先进的重建技术，机器人在复杂环境中识别和完成被遮挡物体的能力仍然不如人类。现有技术往往将场景建模为不分化的整体，难以从部分观测中识别出完整的物体。因此，我们的动机是改善机器人在复杂场景中的3D感知能力，特别是在分解实例和实现完全重建方面。

**Method:** 我们的方法名为InstaScene，旨在实现复杂场景的完整3D感知。为此，我们开发了一种新的空间对比学习方法，通过跟踪每个实例在不同视角下的光栅化来增强语义监督，并引入原位生成，利用有价值的观察和几何线索指导3D生成模型，以实现与现实世界无缝结合的完整实例重建。

**Result:** 实验表明，我们的方法在场景分解和对象完成方面表现出了更优的分解精度，并生产出几何准确且视觉完好的对象。

**Conclusion:** 实验结果证明，我们提出的方法在处理复杂的真实和合成场景时能够实现更准确的分解和完整性重建，证明了InstaScene的有效性。

**Abstract:** Humans can naturally identify and mentally complete occluded objects in
cluttered environments. However, imparting similar cognitive ability to
robotics remains challenging even with advanced reconstruction techniques,
which models scenes as undifferentiated wholes and fails to recognize complete
object from partial observations. In this paper, we propose InstaScene, a new
paradigm towards holistic 3D perception of complex scenes with a primary goal:
decomposing arbitrary instances while ensuring complete reconstruction. To
achieve precise decomposition, we develop a novel spatial contrastive learning
by tracing rasterization of each instance across views, significantly enhancing
semantic supervision in cluttered scenes. To overcome incompleteness from
limited observations, we introduce in-situ generation that harnesses valuable
observations and geometric cues, effectively guiding 3D generative models to
reconstruct complete instances that seamlessly align with the real world.
Experiments on scene decomposition and object completion across complex
real-world and synthetic scenes demonstrate that our method achieves superior
decomposition accuracy while producing geometrically faithful and visually
intact objects.

</details>


### [91] [Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated Diffusion Transformers](https://arxiv.org/abs/2507.08422)
*Wongi Jeong,Kyungryeol Lee,Hoigi Seo,Se Young Chun*

Main category: cs.CV

> 提出Region-Adaptive Latent Upsampling（RALU）框架，在不牺牲图像质量的前提下，通过混合分辨率采样加速扩散模型的推理过程，显著提高了生成效率。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决扩散变换器推理过程中计算成本高的问题，通过沿空间维度加速推理，减少计算量同时保持图像质量。

**Method:** RALU采用混合分辨率采样在三个阶段加速推理：1)低分辨率去噪潜在扩散以高效捕捉全局语义结构，2)特异性区域适应上采样，以处理全分辨率下容易出现伪影的区域；3)全分辨率下的所有潜在上采样进行细节优化。为了稳定不同分辨率转换期间的生成，引入了噪声时间步调整来适应不同分辨率下的噪声水平。

**Result:** 该方法在FLUX和Stable Diffusion 3上分别实现了最高7.0倍和3.0倍的速度提升，且图像质量几乎没有退化。

**Conclusion:** 提出的方法不仅可以加速高分辨率图像和视频的生成，还可以与现有的时间加速方法无缝集成，进一步降低推理延迟而不影响生成质量。

**Abstract:** Diffusion transformers have emerged as an alternative to U-net-based
diffusion models for high-fidelity image and video generation, offering
superior scalability. However, their heavy computation remains a major obstacle
to real-world deployment. Existing acceleration methods primarily exploit the
temporal dimension such as reusing cached features across diffusion timesteps.
Here, we propose Region-Adaptive Latent Upsampling (RALU), a training-free
framework that accelerates inference along spatial dimension. RALU performs
mixed-resolution sampling across three stages: 1) low-resolution denoising
latent diffusion to efficiently capture global semantic structure, 2)
region-adaptive upsampling on specific regions prone to artifacts at
full-resolution, and 3) all latent upsampling at full-resolution for detail
refinement. To stabilize generations across resolution transitions, we leverage
noise-timestep rescheduling to adapt the noise level across varying
resolutions. Our method significantly reduces computation while preserving
image quality by achieving up to 7.0$\times$ speed-up on FLUX and 3.0$\times$
on Stable Diffusion 3 with minimal degradation. Furthermore, RALU is
complementary to existing temporal accelerations such as caching methods, thus
can be seamlessly integrated to further reduce inference latency without
compromising generation quality.

</details>


### [92] [RePaintGS: Reference-Guided Gaussian Splatting for Realistic and View-Consistent 3D Scene Inpainting](https://arxiv.org/abs/2507.08434)
*Ji Hyun Seo,Byounhyun Yoo,Gerard Jounghyun Kim*

Main category: cs.CV

> 本文提出了一种基于参考视图的3D场景修复方法，解决了先前方法在保证视图间几何和外观一致性方面的不足。

<details>
  <summary>Details</summary>

**Motivation:** 现有的3D场景修复方法在处理多个视图的一致性方面存在局限性，容易导致细节损失以及感知上的不一致性。因此，需要一种更可靠的方法来改善几何保真度和外观一致性。

**Method:** 提出一种新的3D场景修复方法，该方法利用参考视图来可靠地生成复杂场景中的真实且感知一致的修复结果。通过估计其他视图与修复参考视图的相似性来调整它们在构建准确几何结构中的贡献，然后使用这种几何结构将参考修复结果扭曲到其他视图，作为伪地面真值来指导优化以匹配参考视图的外观。

**Result:** 比较评估研究表明，该方法在修复场景的几何保真度和外观一致性方面表现优于现有方法。

**Conclusion:** 该方法通过引入参考视图的使用，解决了现存方法在处理3D场景修复时视图一致性的问题。

**Abstract:** Radiance field methods, such as Neural Radiance Field or 3D Gaussian
Splatting, have emerged as seminal 3D representations for synthesizing
realistic novel views. For practical applications, there is ongoing research on
flexible scene editing techniques, among which object removal is a
representative task. However, removing objects exposes occluded regions, often
leading to unnatural appearances. Thus, studies have employed image inpainting
techniques to replace such regions with plausible content - a task referred to
as 3D scene inpainting. However, image inpainting methods produce one of many
plausible completions for each view, leading to inconsistencies between
viewpoints. A widely adopted approach leverages perceptual cues to blend
inpainted views smoothly. However, it is prone to detail loss and can fail when
there are perceptual inconsistencies across views. In this paper, we propose a
novel 3D scene inpainting method that reliably produces realistic and
perceptually consistent results even for complex scenes by leveraging a
reference view. Given the inpainted reference view, we estimate the inpainting
similarity of the other views to adjust their contribution in constructing an
accurate geometry tailored to the reference. This geometry is then used to warp
the reference inpainting to other views as pseudo-ground truth, guiding the
optimization to match the reference appearance. Comparative evaluation studies
have shown that our approach improves both the geometric fidelity and
appearance consistency of inpainted scenes.

</details>


### [93] [Vision Foundation Models as Effective Visual Tokenizers for Autoregressive Image Generation](https://arxiv.org/abs/2507.08441)
*Anlin Zheng,Xin Wen,Xuanyang Zhang,Chuofan Ma,Tiancai Wang,Gang Yu,Xiangyu Zhang,Xiaojuan Qi*

Main category: cs.CV

> 本文提出了一种使用冻结的预训练视觉基础模型构建图像分词器的方法，通过引入适应区域量化框架和语义重建目标，实现了图像重建和生成质量的显著提升，同时提高了标记效率。

<details>
  <summary>Details</summary>

**Motivation:** 传统的预训练视觉基础模型主要用于视觉理解，但尚未被广泛用于直接构建图像分词器。本文旨在填补这一领域的空白，探索视觉模型在图像标记领域的潜在应用。

**Method:** 基于预训练视觉基础模型（通常用于视觉理解），本文提出了一种新颖的方法：构建基于该模型的图像分词器。具体来说，我们采用冻结的视觉基础模型作为分词器的编码器。为了提升其有效性，本文引入了两个关键组件：一种适应区域的量化框架，能够在常规的二维网格上减少预训练特征的冗余，以及语义重建目标，确保分词器输出与基础模型表示保持一致以维持语义保真度。

**Result:** 所提出的图像分词器（VFMTok）在图像重建和生成质量方面取得了显著改善，提高了标记效率。在ImageNet基准上，自回归生成的gFID达到了2.07，并加速了模型收敛速度三倍，同时支持无需分类器自由引导的高保真度条件合成。

**Conclusion:** 所设计的基于预训练视觉基础模型的图像分词器（VFMTok）不仅在图像生成质量上取得了显著提升，同时也极大地提高了标记效率，进一步促进了自回归模型的生成能力。

**Abstract:** Leveraging the powerful representations of pre-trained vision foundation
models -- traditionally used for visual comprehension -- we explore a novel
direction: building an image tokenizer directly atop such models, a largely
underexplored area. Specifically, we employ a frozen vision foundation model as
the encoder of our tokenizer. To enhance its effectiveness, we introduce two
key components: (1) a region-adaptive quantization framework that reduces
redundancy in the pre-trained features on regular 2D grids, and (2) a semantic
reconstruction objective that aligns the tokenizer's outputs with the
foundation model's representations to preserve semantic fidelity. Based on
these designs, our proposed image tokenizer, VFMTok, achieves substantial
improvements in image reconstruction and generation quality, while also
enhancing token efficiency. It further boosts autoregressive (AR) generation --
achieving a gFID of 2.07 on ImageNet benchmarks, while accelerating model
convergence by three times, and enabling high-fidelity class-conditional
synthesis without the need for classifier-free guidance (CFG). The code will be
released publicly to benefit the community.

</details>


### [94] [Review of Feed-forward 3D Reconstruction: From DUSt3R to VGGT](https://arxiv.org/abs/2507.08448)
*Wei Zhang,Yihang Wu,Songhua Li,Wenjie Ma,Xin Ma,Qiang Li,Qi Wang*

Main category: cs.CV

> The paper introduces a new paradigm in 3D reconstruction using deep learning models that can infer camera poses and dense geometry from images more efficiently than traditional methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation stems from the limitations of traditional 3D reconstruction methods, such as complex workflows, high computational cost, and poor robustness, particularly in texture-less regions. The new approach aims to address these issues.

**Method:** The paper discusses a new family of deep learning models, represented by DUSt3R, which adopt a feed-forward approach to 3D reconstruction, using a unified deep network to infer camera poses and dense geometry directly from a set of unconstrained images in a single step.

**Result:** The survey contrasts this new paradigm with traditional methods and earlier learning-based approaches, shedding light on the disruptive potential of feed-forward models in 3D reconstruction.

**Conclusion:** The paper outlines the broader applications and future challenges of feed-forward 3D reconstruction models, including scalability and dynamic scene handling, indicating the transformative impact of this technology.

**Abstract:** 3D reconstruction, which aims to recover the dense three-dimensional
structure of a scene, is a cornerstone technology for numerous applications,
including augmented/virtual reality, autonomous driving, and robotics. While
traditional pipelines like Structure from Motion (SfM) and Multi-View Stereo
(MVS) achieve high precision through iterative optimization, they are limited
by complex workflows, high computational cost, and poor robustness in
challenging scenarios like texture-less regions. Recently, deep learning has
catalyzed a paradigm shift in 3D reconstruction. A new family of models,
exemplified by DUSt3R, has pioneered a feed-forward approach. These models
employ a unified deep network to jointly infer camera poses and dense geometry
directly from an Unconstrained set of images in a single forward pass. This
survey provides a systematic review of this emerging domain. We begin by
dissecting the technical framework of these feed-forward models, including
their Transformer-based correspondence modeling, joint pose and geometry
regression mechanisms, and strategies for scaling from two-view to multi-view
scenarios. To highlight the disruptive nature of this new paradigm, we contrast
it with both traditional pipelines and earlier learning-based methods like
MVSNet. Furthermore, we provide an overview of relevant datasets and evaluation
metrics. Finally, we discuss the technology's broad application prospects and
identify key future challenges and opportunities, such as model accuracy and
scalability, and handling dynamic scenes.

</details>


### [95] [A document is worth a structured record: Principled inductive bias design for document recognition](https://arxiv.org/abs/2507.08458)
*Benjamin Meyer,Lukas Tuggener,Sascha Hänzi,Daniel Schmid,Erdal Ayfer,Benjamin F. Grewe,Ahmed Abdulkadir,Thilo Stadelmann*

Main category: cs.CV

> 提出一种新的文档识别方法，通过设计特定结构的归纳偏置，成功实现了对密集文档类型的转录，完善了文档识别系统的应用。

<details>
  <summary>Details</summary>

**Motivation:** 当前最先进的文档识别方法忽视了文档类型特定的结构属性，导致过多依赖次优的启发式后处理，并使许多不太常见或更复杂的文档类型无法进行现代文档识别。

**Method:** 本文提出了一种新的文档识别视角，将其视为从文档到记录的转录任务，并设计了针对特定结构的归纳偏置方法及相应的基础Transformer架构，以适应不同的文档结构。

**Result:** 通过在复杂记录结构上进行广泛实验，验证了所设计归纳偏置的有效性，并成功训练了第一个能够将工程图纸转录为其内在相互关联信息的端到端模型。

**Conclusion:** 该方法为设计未被充分理解的文档类型识别系统奠定了基础，并将成为未来文档基础模型设计的指南。

**Abstract:** Many document types use intrinsic, convention-driven structures that serve to
encode precise and structured information, such as the conventions governing
engineering drawings. However, state-of-the-art approaches treat document
recognition as a mere computer vision problem, neglecting these underlying
document-type-specific structural properties, making them dependent on
sub-optimal heuristic post-processing and rendering many less frequent or more
complicated document types inaccessible to modern document recognition. We
suggest a novel perspective that frames document recognition as a transcription
task from a document to a record. This implies a natural grouping of documents
based on the intrinsic structure inherent in their transcription, where related
document types can be treated (and learned) similarly. We propose a method to
design structure-specific inductive biases for the underlying machine-learned
end-to-end document recognition systems, and a respective base transformer
architecture that we successfully adapt to different structures. We demonstrate
the effectiveness of the so-found inductive biases in extensive experiments
with progressively complex record structures from monophonic sheet music, shape
drawings, and simplified engineering drawings. By integrating an inductive bias
for unrestricted graph structures, we train the first-ever successful
end-to-end model to transcribe engineering drawings to their inherently
interlinked information. Our approach is relevant to inform the design of
document recognition systems for document types that are less well understood
than standard OCR, OMR, etc., and serves as a guide to unify the design of
future document foundation models.

</details>


### [96] [F3-Net: Foundation Model for Full Abnormality Segmentation of Medical Images with Flexible Input Modality Requirement](https://arxiv.org/abs/2507.08460)
*Seyedeh Sahar Taheri Otaghsara,Reza Rahmanzadeh*

Main category: cs.CV

> F3-Net通过灵活的合成模态训练和零图像策略克服了临床医学图像分割中的挑战，其多任务适用性超越了CNN和Transformer模型，展示了其在具体疾病数据集上的优秀性能。

<details>
  <summary>Details</summary>

**Motivation:** F3-Net旨在解决临床医学图像分割中的一些持续性挑战，如对完整多模态输入的依赖、有限的泛化能力和特定任务的狭窄范围。

**Method:** F3-Net采用灵活的合成模态训练，能减少对完整多模态输入的依赖，并通过零图像策略来增强其在实际应用中的表现。其统一架构适用于多种病理分割，包括胶质瘤、转移瘤、中风和白质病变，无需针对每种疾病重新训练。

**Result:** 在包括BraTS 2021, BraTS 2024和ISLES 2022在内的多个数据集上进行评估，F3-Net在不同领域转移和临床异质性方面表现出强大的韧性。在整体病理数据集上，F3-Net的平均Dice相似系数（DSC）分别为BraTS-GLI 2024 0.94，BraTS-MET 2024 0.82，BraTS 2021 0.94，ISLES 2022 0.79。

**Conclusion:** F3-Net被定位为桥接深度学习研究与实际临床应用之间的差距的多功能、可扩展解决方案。

**Abstract:** F3-Net is a foundation model designed to overcome persistent challenges in
clinical medical image segmentation, including reliance on complete multimodal
inputs, limited generalizability, and narrow task specificity. Through flexible
synthetic modality training, F3-Net maintains robust performance even in the
presence of missing MRI sequences, leveraging a zero-image strategy to
substitute absent modalities without relying on explicit synthesis networks,
thereby enhancing real-world applicability. Its unified architecture supports
multi-pathology segmentation across glioma, metastasis, stroke, and white
matter lesions without retraining, outperforming CNN-based and
transformer-based models that typically require disease-specific fine-tuning.
Evaluated on diverse datasets such as BraTS 2021, BraTS 2024, and ISLES 2022,
F3-Net demonstrates strong resilience to domain shifts and clinical
heterogeneity. On the whole pathology dataset, F3-Net achieves average Dice
Similarity Coefficients (DSCs) of 0.94 for BraTS-GLI 2024, 0.82 for BraTS-MET
2024, 0.94 for BraTS 2021, and 0.79 for ISLES 2022. This positions it as a
versatile, scalable solution bridging the gap between deep learning research
and practical clinical deployment.

</details>


### [97] [Dual Dimensions Geometric Representation Learning Based Document Dewarping](https://arxiv.org/abs/2507.08492)
*Heng Li,Qingcai Chen,Xiangping Wu*

Main category: cs.CV

> 本研究提出了D2Dewarp模型，通过关注文档的水平和垂直线，改善了文档去畸变的效果，并提出了一种自动生成注释的方法来构建新的大规模畸变训练数据集。

<details>
  <summary>Details</summary>

**Motivation:** 现有的文档图像去畸变方法通常只关注单一的水平维度，因此无法全面捕捉到文档的变形趋势。本文旨在利用水平和垂直两个维度的信息，以达到更好的去畸变效果。

**Method:** D2Dewarp模型通过设计一个基于X和Y坐标的有效融合模块，实现了水平和垂直维度之间特征的互补和约束。此外，本文还提出了一种使用公共文档纹理图像和自动渲染引擎来生成自动精细标注的新方法，构建新的大规模畸变训练数据集。

**Result:** D2Dewarp模型在中文和英文公开基准数据集上取得了比现有最优方法更好的量化和视觉效果。

**Conclusion:** 本研究提出的方法能够更好地捕捉文档在不同方向上的变形趋势，改善了文档图像的去畸变效果，同时构建了一个新的大规模训练数据集。

**Abstract:** Document image dewarping remains a challenging task in the deep learning era.
While existing methods have improved by leveraging text line awareness, they
typically focus only on a single horizontal dimension. In this paper, we
propose a fine-grained deformation perception model that focuses on Dual
Dimensions of document horizontal-vertical-lines to improve document Dewarping
called D2Dewarp. It can perceive distortion trends in different directions
across document details. To combine the horizontal and vertical granularity
features, an effective fusion module based on X and Y coordinate is designed to
facilitate interaction and constraint between the two dimensions for feature
complementarity. Due to the lack of annotated line features in current public
dewarping datasets, we also propose an automatic fine-grained annotation method
using public document texture images and an automatic rendering engine to build
a new large-scale distortion training dataset. The code and dataset will be
publicly released. On public Chinese and English benchmarks, both quantitative
and qualitative results show that our method achieves better rectification
results compared with the state-of-the-art methods. The dataset will be
publicly available at https://github.com/xiaomore/DocDewarpHV

</details>


### [98] [Unified People Tracking with Graph Neural Networks](https://arxiv.org/abs/2507.08494)
*Martin Engilberge,Ivan Vrkic,Friedrich Wilke Grosche,Julien Pilet,Engin Turetken,Pascal Fua*

Main category: cs.CV

> A novel fully differentiable model for multi-person tracking is proposed and evaluates it on a new large-scale dataset, achieving state-of-the-art performance.

<details>
  <summary>Details</summary>

**Motivation:** Address the challenges in multi-person tracking, especially in handling occlusions, by providing a unified model and a new large-scale dataset.

**Method:** This work introduces a fully differentiable model for multi-person tracking that constructs a dynamic spatiotemporal graph for seamless information propagation without relying on pre-computed tracklets.

**Result:** Experiments demonstrate state-of-the-art performance on public benchmarks and a newly introduced large-scale dataset, showcased even under occluded conditions.

**Conclusion:** The proposed model and dataset advance the field of multi-person tracking by achieving flexible and robust performance across various conditions.

**Abstract:** This work presents a unified, fully differentiable model for multi-people
tracking that learns to associate detections into trajectories without relying
on pre-computed tracklets. The model builds a dynamic spatiotemporal graph that
aggregates spatial, contextual, and temporal information, enabling seamless
information propagation across entire sequences. To improve occlusion handling,
the graph can also encode scene-specific information. We also introduce a new
large-scale dataset with 25 partially overlapping views, detailed scene
reconstructions, and extensive occlusions. Experiments show the model achieves
state-of-the-art performance on public benchmarks and the new dataset, with
flexibility across diverse conditions. Both the dataset and approach will be
publicly released to advance research in multi-people tracking.

</details>


### [99] [Occlusion-Guided Feature Purification Learning via Reinforced Knowledge Distillation for Occluded Person Re-Identification](https://arxiv.org/abs/2507.08520)
*Yufei Zheng,Wenjun Wang,Wenjun Gan,Jiawei Liu*

Main category: cs.CV

> 本文提出了一种遮挡引导特征净化学习方法（OGFR），通过引入遮挡感知图像变换器和特征擦除与净化模块，有效解决了现有方法在处理多样化遮挡场景时的问题和特征污染问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有的遮挡行人重识别方法依赖于可见部位的对齐、遮挡数据增强或使用完整图像补充缺失语义，但它们面临处理未见过的多样化遮挡场景的挑战和来自完整图像的特征污染问题。

**Method:** 本文提出了通过强化知识蒸馏进行遮挡引导特征净化学习的方法（OGFR），以同时解决现有方法在处理多样化的遮挡场景时遇到的问题和特征污染问题。OGFR采用教师-学生知识蒸馏架构，有效地将多种遮挡模式纳入特征表示中，并通过强化知识蒸馏从完整图像分支向遮挡图像分支传递净化后的辨别知识。此外，还设计了遮挡感知图像变换器和特征擦除与净化模块以指导遮挡鲁棒特征表示，并避免因遮挡带来的特征污染，进一步挖掘身份相关辨别线索。

**Result:** 无具体结果展示，方法描述较多。

**Conclusion:** 无具体结论展示，方法和架构介绍较多。

**Abstract:** Occluded person re-identification aims to retrieve holistic images based on
occluded ones. Existing methods often rely on aligning visible body parts,
applying occlusion augmentation, or complementing missing semantics using
holistic images. However, they face challenges in handling diverse occlusion
scenarios not seen during training and the issue of feature contamination from
holistic images. To address these limitations, we propose Occlusion-Guided
Feature Purification Learning via Reinforced Knowledge Distillation (OGFR),
which simultaneously mitigates these challenges. OGFR adopts a teacher-student
distillation architecture that effectively incorporates diverse occlusion
patterns into feature representation while transferring the purified
discriminative holistic knowledge from the holistic to the occluded branch
through reinforced knowledge distillation. Specifically, an Occlusion-Aware
Vision Transformer is designed to leverage learnable occlusion pattern
embeddings to explicitly model such diverse occlusion types, thereby guiding
occlusion-aware robust feature representation. Moreover, we devise a Feature
Erasing and Purification Module within the holistic branch, in which an agent
is employed to identify low-quality patch tokens of holistic images that
contain noisy negative information via deep reinforcement learning, and
substitute these patch tokens with learnable embedding tokens to avoid feature
contamination and further excavate identity-related discriminative clues.
Afterward, with the assistance of knowledge distillation, the student branch
effectively absorbs the purified holistic knowledge to precisely learn robust
representation regardless of the interference of occlusions.

</details>


### [100] [RadiomicsRetrieval: A Customizable Framework for Medical Image Retrieval Using Radiomics Features](https://arxiv.org/abs/2507.08546)
*Inye Na,Nejung Rue,Jiwon Chung,Hyunjin Park*

Main category: cs.CV

> RadiomicsRetrieval是一个3D医学图像检索系统，结合影像组学特征和深度学习嵌入，通过对比学习对齐特征，并应用解剖位置嵌入提供全局上下文，不仅提高了检索的特异性，还降低了用户操作的复杂度。

<details>
  <summary>Details</summary>

**Motivation:** 当前医学图像检索方法主要支持2D图像，需要完全标注的查询，限制了临床的灵活性。为解决这个问题，提出了RadiomicsRetrieval。

**Method:** 提出RadiomicsRetrieval，这是一个3D内容检索框架，结合手工提取的影像组学特征与深度学习生成的肿瘤级别嵌入。使用可提示的分割模型（如SAM）来获取肿瘤特异性图像嵌入，并通过对比学习与同样的肿瘤提取的影像组学特征对齐。这些表示进一步由解剖位置嵌入（APE）加强。

**Result:** 广泛的实验结果表明，影像组学特征显著提高了检索的特异性，而APE提供了全解剖位置的全局上下文，对于基于位置的搜索至关重要。

**Conclusion:** RadiomicsRetrieval框架只需要极小的用户提示，减少了分割的开销，并能支持多样化的临床场景。其可以通过图像嵌入或选择的影像组学属性进行查询的能力体现了其适应性，可能有助于诊断、治疗计划和大规模医疗图像库的研究。

**Abstract:** Medical image retrieval is a valuable field for supporting clinical
decision-making, yet current methods primarily support 2D images and require
fully annotated queries, limiting clinical flexibility. To address this, we
propose RadiomicsRetrieval, a 3D content-based retrieval framework bridging
handcrafted radiomics descriptors with deep learning-based embeddings at the
tumor level. Unlike existing 2D approaches, RadiomicsRetrieval fully exploits
volumetric data to leverage richer spatial context in medical images. We employ
a promptable segmentation model (e.g., SAM) to derive tumor-specific image
embeddings, which are aligned with radiomics features extracted from the same
tumor via contrastive learning. These representations are further enriched by
anatomical positional embedding (APE). As a result, RadiomicsRetrieval enables
flexible querying based on shape, location, or partial feature sets. Extensive
experiments on both lung CT and brain MRI public datasets demonstrate that
radiomics features significantly enhance retrieval specificity, while APE
provides global anatomical context essential for location-based searches.
Notably, our framework requires only minimal user prompts (e.g., a single
point), minimizing segmentation overhead and supporting diverse clinical
scenarios. The capability to query using either image embeddings or selected
radiomics attributes highlights its adaptability, potentially benefiting
diagnosis, treatment planning, and research on large-scale medical imaging
repositories. Our code is available at
https://github.com/nainye/RadiomicsRetrieval.

</details>


### [101] [SAM2RL: Towards Reinforcement Learning Memory Control in Segment Anything Model 2](https://arxiv.org/abs/2507.08548)
*Alen Adamyan,Tomáš Čížek,Matej Straka,Klara Janouskova,Martin Schmid*

Main category: cs.CV

> This paper proposes using reinforcement learning to optimize memory updates in the Segment Anything Model 2 (SAM 2) for visual object tracking tasks, demonstrating significantly better performance than existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the handling of distractors, occlusions, and object motion in visual object tracking tasks by leveraging reinforcement learning to control the model's memory updates more effectively than existing hand-crafted rules.

**Method:** Our method uses reinforcement learning to optimize memory updates in SAM 2, treating memory control as a sequential decision-making problem.

**Result:** In an overfitting setup with a separate agent per video, our method shows a relative improvement over SAM 2 that is more than three times greater than the gains achieved by existing heuristics.

**Conclusion:** The results indicate that reinforcement learning is a powerful alternative to hand-crafted update rules for memory control in visual object tracking, revealing the untapped potential of the memory bank in the Segment Anything Model 2.

**Abstract:** Segment Anything Model 2 (SAM 2) has demonstrated strong performance in
object segmentation tasks and has become the state-of-the-art for visual object
tracking. The model stores information from previous frames in a memory bank,
enabling temporal consistency across video sequences. Recent methods augment
SAM 2 with hand-crafted update rules to better handle distractors, occlusions,
and object motion. We propose a fundamentally different approach using
reinforcement learning for optimizing memory updates in SAM 2 by framing memory
control as a sequential decision-making problem. In an overfitting setup with a
separate agent per video, our method achieves a relative improvement over SAM 2
that exceeds by more than three times the gains of existing heuristics. These
results reveal the untapped potential of the memory bank and highlight
reinforcement learning as a powerful alternative to hand-crafted update rules
for memory control in visual object tracking.

</details>


### [102] [Image Translation with Kernel Prediction Networks for Semantic Segmentation](https://arxiv.org/abs/2507.08554)
*Cristina Mata,Michael S. Ryoo,Henrik Turbell*

Main category: cs.CV

> Paper presents DA-KPN, which ensures pixel-wise semantic consistency in synthetic-to-real image translation, improving semantic segmentation performance.

<details>
  <summary>Details</summary>

**Motivation:** The difficulty in obtaining accurate pixel-wise annotations for real-world data for semantic segmentation leads practitioners to use synthetic datasets. However, the domain gap between synthetic and real data can degrade performance, calling for a method to bridge this gap.

**Method:** Domain Adversarial Kernel Prediction Network (DA-KPN) is introduced to ensure semantic matching between synthetic labels and translated images. It predicts pixel-wise transformation parameters for a translation function and uses multi-scale discriminators for realism.

**Result:** DA-KPN outperforms previous GAN-based methods on benchmark datasets for semantic segmentation with limited real data and achieves comparable performance on face parsing tasks.

**Conclusion:** Proposing a method that guarantees semantic consistency in image translation from synthetic to real environments improves semantic segmentation performance in low-data scenarios.

**Abstract:** Semantic segmentation relies on many dense pixel-wise annotations to achieve
the best performance, but owing to the difficulty of obtaining accurate
annotations for real world data, practitioners train on large-scale synthetic
datasets. Unpaired image translation is one method used to address the ensuing
domain gap by generating more realistic training data in low-data regimes.
Current methods for unpaired image translation train generative adversarial
networks (GANs) to perform the translation and enforce pixel-level semantic
matching through cycle consistency. These methods do not guarantee that the
semantic matching holds, posing a problem for semantic segmentation where
performance is sensitive to noisy pixel labels. We propose a novel image
translation method, Domain Adversarial Kernel Prediction Network (DA-KPN), that
guarantees semantic matching between the synthetic label and translation.
DA-KPN estimates pixel-wise input transformation parameters of a lightweight
and simple translation function. To ensure the pixel-wise transformation is
realistic, DA-KPN uses multi-scale discriminators to distinguish between
translated and target samples. We show DA-KPN outperforms previous GAN-based
methods on syn2real benchmarks for semantic segmentation with limited access to
real image labels and achieves comparable performance on face parsing.

</details>


### [103] [Disentangling Instance and Scene Contexts for 3D Semantic Scene Completion](https://arxiv.org/abs/2507.08555)
*Enyu Liu,En Yu,Sijia Chen,Wenbing Tao*

Main category: cs.CV

> DISC, a novel dual-stream approach for 3D Semantic Scene Completion, uses class queries instead of voxel queries and demonstrates SOTA performance on SemanticKITTI and SSCBench-KITTI-360 benchmarks, improving instance category performance significantly.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind DISC is to overcome the limitations of voxels as the basic unit, which hinders the use of class-level information, essential for more detailed scene completion.

**Method:** Our method, DISC, introduces a dual-stream paradigm to enhance 3D SSC by leveraging class-level information through discriminative class queries and specialized decoding modules, addressing the limitations of voxel-level features.

**Result:** DISC achieves SOTA performance with mIoU scores of 17.35 and 20.55 on SemanticKITTI and SSCBench-KITTI-360, respectively, displaying a significant improvement in instance category performance over existing methods.

**Conclusion:** DISC improves 3D SSC by incorporating class-level information through specialized class queries and decoding modules, achieving SOTA performance on key benchmarks, even outperforming multi-frame methods with single-frame input.

**Abstract:** 3D Semantic Scene Completion (SSC) has gained increasing attention due to its
pivotal role in 3D perception. Recent advancements have primarily focused on
refining voxel-level features to construct 3D scenes. However, treating voxels
as the basic interaction units inherently limits the utilization of class-level
information, which is proven critical for enhancing the granularity of
completion results. To address this, we propose \textbf{D}isentangling Instance
and Scene Contexts (DISC), a novel dual-stream paradigm that enhances learning
for both instance and scene categories through separated optimization.
Specifically, we replace voxel queries with discriminative class queries, which
incorporate class-specific geometric and semantic priors. Additionally, we
exploit the intrinsic properties of classes to design specialized decoding
modules, facilitating targeted interactions and efficient class-level
information flow. Experimental results demonstrate that DISC achieves
state-of-the-art (SOTA) performance on both SemanticKITTI and
SSCBench-KITTI-360 benchmarks, with mIoU scores of 17.35 and 20.55,
respectively. Remarkably, DISC even outperforms multi-frame SOTA methods using
only single-frame input and significantly improves instance category
performance, surpassing both single-frame and multi-frame SOTA instance mIoU by
17.9\% and 11.9\%, respectively, on the SemanticKITTI hidden test. The code is
available at https://github.com/Enyu-Liu/DISC.

</details>


### [104] [A Multi-Modal Fusion Framework for Brain Tumor Segmentation Based on 3D Spatial-Language-Vision Integration and Bidirectional Interactive Attention Mechanism](https://arxiv.org/abs/2507.08574)
*Mingda Zhang,Kaiwen Pan*

Main category: cs.CV

> 研究提出了一种新的多模态融合框架，通过MSFA和BIVA组件结合3D MRI数据和临床文本描述，利用层次语义解耦和双向互动注意机制提高脑肿瘤分割的性能，优于现有的几种方法。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在通过整合空间-语言-视觉信息来开发一种新的多模态融合框架，以提高脑肿瘤分割的准确性和边界划分。

**Method:** 分析该论文使用了一种多模态融合框架来提高脑肿瘤分割的准确性。框架中的两个核心组件是Multi-modal Semantic Fusion Adapter (MSFA) 和Bidirectional Interactive Visual-semantic Attention (BIVA)，通过层次语义解耦和双向互动注意机制实现了空间、语言和视觉信息的综合。

**Result:** 该方法在BraTS 2020数据集上表现优秀，平均Dice系数达到0.8505，与现有方法如SCAU-Net, CA-Net,和3D U-Net相比有显著的性能提升。

**Conclusion:** 结合多模态语义融合和双向互动注意机制大大提升了脑肿瘤分割的表现，开辟了将临床知识融入医学图像分析的新范式。

**Abstract:** This study aims to develop a novel multi-modal fusion framework for brain
tumor segmentation that integrates spatial-language-vision information through
bidirectional interactive attention mechanisms to improve segmentation accuracy
and boundary delineation. Methods: We propose two core components: Multi-modal
Semantic Fusion Adapter (MSFA) integrating 3D MRI data with clinical text
descriptions through hierarchical semantic decoupling, and Bidirectional
Interactive Visual-semantic Attention (BIVA) enabling iterative information
exchange between modalities. The framework was evaluated on BraTS 2020 dataset
comprising 369 multi-institutional MRI scans. Results: The proposed method
achieved average Dice coefficient of 0.8505 and 95% Hausdorff distance of
2.8256mm across enhancing tumor, tumor core, and whole tumor regions,
outperforming state-of-the-art methods including SCAU-Net, CA-Net, and 3D
U-Net. Ablation studies confirmed critical contributions of semantic and
spatial modules to boundary precision. Conclusion: Multi-modal semantic fusion
combined with bidirectional interactive attention significantly enhances brain
tumor segmentation performance, establishing new paradigms for integrating
clinical knowledge into medical image analysis.

</details>


### [105] [BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language Models via Gaussian Discriminant Analysis](https://arxiv.org/abs/2507.08607)
*Shuang Cui,Jinglin Xu,Yi Li,Xiongxin Tang,Jiangmeng Li,Jiahuan Zhou,Fanjiang Xu,Fuchun Sun,Hui Xiong*

Main category: cs.CV

> BayesTTA is introduced to effectively manage continual-temporal test-time adaptation for vision-language models facing gradual distribution shifts, improving prediction consistency and visual representation alignment.

<details>
  <summary>Details</summary>

**Motivation:** To address the degradation of vision-language models under temporally evolving distribution shifts, which are not well-handled by existing continual test-time adaptation methods that are mainly designed for sudden and severe shifts, ignoring temporal continuity.

**Method:** BayesTTA, a Bayesian adaptation framework that enforces temporally consistent predictions and dynamically aligns visual representations by incrementally estimating class-conditional Gaussian mixture distributions, adaptively selecting covariance structures through statistical hypothesis testing, and performing calibrated inference using GDA.

**Result:** BayesTTA consistently outperforms state-of-the-art methods while maintaining efficiency, as shown by extensive experiments across four temporally evolving datasets and ten standard TTA datasets.

**Conclusion:** BayesTTA demonstrates superior performance in managing temporal drift in visual processing compared to existing methods, suitable for real-world scenarios involving gradual changes in input distributions.

**Abstract:** Vision-language models (VLMs) such as CLIP achieve strong zero-shot
recognition but degrade significantly under \textit{temporally evolving
distribution shifts} common in real-world scenarios (e.g., gradual illumination
or seasonal changes). Existing continual test-time adaptation (CTTA) methods
are typically built around sudden and severe distribution shifts and neglect
temporal continuity, leading to three core defects: limited memory cache
restricts long-range distribution modeling, causing catastrophic forgetting;
entropy-based confidence becomes unreliable under temporal drift, worsening
error accumulation; and static visual representations misalign with evolving
inputs. We formalize this practical problem as \textit{Continual-Temporal
Test-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over
time. To address it, we propose \textit{BayesTTA}, a Bayesian adaptation
framework that enforces temporally consistent predictions and dynamically
aligns visual representations. Specifically, BayesTTA incrementally estimates
class-conditional Gaussian mixture distributions without storing raw data,
adaptively selects covariance structures through statistical hypothesis
testing, and performs calibrated inference using Gaussian discriminant analysis
(GDA). These calibrated predictions supervise self-paced adaptation of
normalization layers, ensuring efficient and stable representation alignment.
We establish a comprehensive CT-TTA benchmark across four temporally evolving
datasets and further evaluate generalization on ten standard TTA datasets.
Extensive experiments show that BayesTTA consistently outperforms
state-of-the-art methods, achieving significant gains while maintaining
efficiency. Code is available at
\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}.

</details>


### [106] [Normalized vs Diplomatic Annotation: A Case Study of Automatic Information Extraction from Handwritten Uruguayan Birth Certificates](https://arxiv.org/abs/2507.08636)
*Natalia Bottaioli,Solène Tarride,Jérémy Anger,Seginus Mowlavi,Marina Gardella,Antoine Tadros,Gabriele Facciolo,Rafael Grompone von Gioi,Christopher Kermorvant,Jean-Michel Morel,Javier Preciozzi*

Main category: cs.CV

> 研究评估了DAN从乌拉圭手写出生证明中提取信息的能力，比较了两种注释策略的效果。

<details>
  <summary>Details</summary>

**Motivation:** 研究的目标在于探索DAN在处理手写文件提取信息方面的性能，并探究不同注释策略对最终转录质量的影响。

**Method:** 本研究评估了最近提出的文档注意力网络（DAN）从乌拉圭出生证明中提取键值信息的能力，这些证明是用西班牙语手写而成。研究调查了两种注释策略，以自动转录手写文档，并使用极少量的训练数据和注释努力对DAN进行微调。

**Result:** 实验在两个包含相同图像（15位以上不同书写者书写的201张出生证明扫描件）但使用不同注释方法的数据集上进行。结果表明，标准化注释对于可以标准化的字段（如出生日期和地点）更有效，而外交式注释对于不能标准化的名称和姓氏字段表现更好。

**Conclusion:** 本研究表明，不同的注释策略针对不同类型的信息能够产生不同的结果，标准化注释适合标准化字段，而外交式注释则更适合处理不可标准化的信息。

**Abstract:** This study evaluates the recently proposed Document Attention Network (DAN)
for extracting key-value information from Uruguayan birth certificates,
handwritten in Spanish. We investigate two annotation strategies for
automatically transcribing handwritten documents, fine-tuning DAN with minimal
training data and annotation effort. Experiments were conducted on two datasets
containing the same images (201 scans of birth certificates written by more
than 15 different writers) but with different annotation methods. Our findings
indicate that normalized annotation is more effective for fields that can be
standardized, such as dates and places of birth, whereas diplomatic annotation
performs much better for fields containing names and surnames, which can not be
standardized.

</details>


### [107] [OnlineBEV: Recurrent Temporal Fusion in Bird's Eye View Representations for Multi-Camera 3D Perception](https://arxiv.org/abs/2507.08644)
*Junho Koh,Youngwoo Lee,Jungho Kim,Dongyoung Lee,Jun Won Choi*

Main category: cs.CV

> 本论文提出了一种新的时序3D感知方法OnlineBEV，使用递归结构增加BEV特征的有效结合数量，同时最小化内存使用，以解决传统方法在结合多个帧进行3D感知时性能提升受限的问题。

<details>
  <summary>Details</summary>

**Motivation:** 虽然通过视角变换获得BEV特征用于三维感知被证明有效，但直接结合多个相机帧图像BEV特征的性能增益有限，特别是当考虑很大数量的帧时。这主要由于动态的变化BEV特征随时间推移由物体运动引起。因此，提出了一种新颖跨时间3D感知方法。

**Method:** OnlineBEV通过使用递归结构结合跨时间的BEV特征，最小化内存使用增加有效组合特征的数量。Motion-guided BEV Fusion Network（MBFNet）被用来实现时间特征对齐，通过从连续的BEV帧中提取运动特征来动态对齐历史和当前的BEV特征。此外，通过时间一致性学习损失强制执行时间特征对齐。

**Result:** 在线进行的实验结果在nuScenes基准测试上表明，相较于目前的最佳方法SOLOFusion，新提出的方法OnlineBEV在仅使用摄像头的3D物体检测任务上实现了显著的性能提升，达到了63.9%的NDS。

**Conclusion:** 结合时空一致性学习损失，论文提出的方法能够显著提高3D物体检测性能，证明其在自动驾驶场景中使用RGB相机进行3D感知的有效性和优越性。

**Abstract:** Multi-view camera-based 3D perception can be conducted using bird's eye view
(BEV) features obtained through perspective view-to-BEV transformations.
Several studies have shown that the performance of these 3D perception methods
can be further enhanced by combining sequential BEV features obtained from
multiple camera frames. However, even after compensating for the ego-motion of
an autonomous agent, the performance gain from temporal aggregation is limited
when combining a large number of image frames. This limitation arises due to
dynamic changes in BEV features over time caused by object motion. In this
paper, we introduce a novel temporal 3D perception method called OnlineBEV,
which combines BEV features over time using a recurrent structure. This
structure increases the effective number of combined features with minimal
memory usage. However, it is critical to spatially align the features over time
to maintain strong performance. OnlineBEV employs the Motion-guided BEV Fusion
Network (MBFNet) to achieve temporal feature alignment. MBFNet extracts motion
features from consecutive BEV frames and dynamically aligns historical BEV
features with current ones using these motion features. To enforce temporal
feature alignment explicitly, we use Temporal Consistency Learning Loss, which
captures discrepancies between historical and target BEV features. Experiments
conducted on the nuScenes benchmark demonstrate that OnlineBEV achieves
significant performance gains over the current best method, SOLOFusion.
OnlineBEV achieves 63.9% NDS on the nuScenes test set, recording
state-of-the-art performance in the camera-only 3D object detection task.

</details>


### [108] [DatasetAgent: A Novel Multi-Agent System for Auto-Constructing Datasets from Real-World Images](https://arxiv.org/abs/2507.08648)
*Haoran Sun,Haoyu Bian,Shaoning Zeng,Yunbo Rao,Xu Xu,Lin Mei,Jianping Gou*

Main category: cs.CV

> The paper introduces DatasetAgent, a multiagent collaborative system that uses real-world images to create high-quality image datasets efficiently.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to solve the issues of time-intensiveness and inefficiency associated with manual data collection and annotation, while also leveraging the value of real-world data over AI-generated data.

**Method:** The method involves a multiagent collaborative system named DatasetAgent, which uses different agents equipped with Multi-modal Large Language Models (MLLMs) and a tool package to optimize and construct datasets from real-world images.

**Result:** The method was tested in two types of experiments: expanding existing datasets and creating new ones, resulting in high-quality datasets that were used to train vision models for tasks like image classification, object detection, and image segmentation.

**Conclusion:** DatasetAgent proves effective in auto-constructing image datasets from real-world images, showcasing the potential to streamline the process of dataset creation for various vision tasks.

**Abstract:** Common knowledge indicates that the process of constructing image datasets
usually depends on the time-intensive and inefficient method of manual
collection and annotation. Large models offer a solution via data generation.
Nonetheless, real-world data are obviously more valuable comparing to
artificially intelligence generated data, particularly in constructing image
datasets. For this reason, we propose a novel method for auto-constructing
datasets from real-world images by a multiagent collaborative system, named as
DatasetAgent. By coordinating four different agents equipped with Multi-modal
Large Language Models (MLLMs), as well as a tool package for image
optimization, DatasetAgent is able to construct high-quality image datasets
according to user-specified requirements. In particular, two types of
experiments are conducted, including expanding existing datasets and creating
new ones from scratch, on a variety of open-source datasets. In both cases,
multiple image datasets constructed by DatasetAgent are used to train various
vision models for image classification, object detection, and image
segmentation.

</details>


### [109] [Generalizable 7T T1-map Synthesis from 1.5T and 3T T1 MRI with an Efficient Transformer Model](https://arxiv.org/abs/2507.08655)
*Zach Eidex,Mojtaba Safari,Tonghe Wang,Vanessa Wildman,David S. Yu,Hui Mao,Erik Middlebrooks,Aparna Kesewala,Xiaofeng Yang*

Main category: cs.CV

> 该研究提出一种名为7T-Restormer的模型，可通过1.5T或3T T1W图像合成7T质量的T1图，结果显示该模型在较少参数的情况下优于现有方法，极大提高了7T MRI在临床中的应用。

<details>
  <summary>Details</summary>

**Motivation:** 动机：7T MRI具有更高的分辨率和对比度，但成本高昂且稀有，因此提出了一种有效的方法来从常规MRI中模拟出7T质量的T1图。

**Method:** Purpose: 使用7T-Restormer模型从常规的1.5T或3T T1加权(MRI)图像中合成7T质量的T1图。方法：在35名1.5T和108名3T T1W MRI以及对应的7T T1图中进行验证，共有141个病人（32,128个切片）。模型与ResViT和ResShift模型进行比较。

**Result:** 结果：7T-Restormer模型在1.5T输入时PSNR为26.0 +/- 4.6 dB、SSIM为0.861 +/- 0.072、NMSE为0.019 +/- 0.011，在3T输入时PSNR为25.9 +/- 4.9 dB、SSIM为0.866 +/- 0.077，模型在参数使用更少的情况下提升了性能。

**Conclusion:** 结论：提出了一种从1.5T和3T T1W扫描中预测定量7T MP2RAGE图的新方法，质量高于现有最先进方法，使7T MRI的优势更易于应用于标准临床工作流程。

**Abstract:** Purpose: Ultra-high-field 7T MRI offers improved resolution and contrast over
standard clinical field strengths (1.5T, 3T). However, 7T scanners are costly,
scarce, and introduce additional challenges such as susceptibility artifacts.
We propose an efficient transformer-based model (7T-Restormer) to synthesize
7T-quality T1-maps from routine 1.5T or 3T T1-weighted (T1W) images. Methods:
Our model was validated on 35 1.5T and 108 3T T1w MRI paired with corresponding
7T T1 maps of patients with confirmed MS. A total of 141 patient cases (32,128
slices) were randomly divided into 105 (25; 80) training cases (19,204 slices),
19 (5; 14) validation cases (3,476 slices), and 17 (5; 14) test cases (3,145
slices) where (X; Y) denotes the patients with 1.5T and 3T T1W scans,
respectively. The synthetic 7T T1 maps were compared against the ResViT and
ResShift models. Results: The 7T-Restormer model achieved a PSNR of 26.0 +/-
4.6 dB, SSIM of 0.861 +/- 0.072, and NMSE of 0.019 +/- 0.011 for 1.5T inputs,
and 25.9 +/- 4.9 dB, and 0.866 +/- 0.077 for 3T inputs, respectively. Using
10.5 M parameters, our model reduced NMSE by 64 % relative to 56.7M parameter
ResShift (0.019 vs 0.052, p = <.001 and by 41 % relative to 70.4M parameter
ResViT (0.019 vs 0.032, p = <.001) at 1.5T, with similar advantages at 3T
(0.021 vs 0.060 and 0.033; p < .001). Training with a mixed 1.5 T + 3 T corpus
was superior to single-field strategies. Restricting the model to 1.5T
increased the 1.5T NMSE from 0.019 to 0.021 (p = 1.1E-3) while training solely
on 3T resulted in lower performance on input 1.5T T1W MRI. Conclusion: We
propose a novel method for predicting quantitative 7T MP2RAGE maps from 1.5T
and 3T T1W scans with higher quality than existing state-of-the-art methods.
Our approach makes the benefits of 7T MRI more accessible to standard clinical
workflows.

</details>


### [110] [ByDeWay: Boost Your multimodal LLM with DEpth prompting in a Training-Free Way](https://arxiv.org/abs/2507.08679)
*Rajarshi Roy,Devleena Das,Ankesh Banerjee,Arjya Bhattacharjee,Kousik Dasgupta,Subarna Tripathi*

Main category: cs.CV

> ByDeWay是一种无训练框架，利用深度感知提示策略（LDP）提高多模态大语言模型的空间推理和定位能力，而不改变模型参数。

<details>
  <summary>Details</summary>

**Motivation:** 通过开发无需训练的新策略提高多模态语言模型的性能，并增强其在空间理解和事实性上的表现。

**Method:** ByDeWay采用了一种新颖的提示策略（LDP）以改善多模态大语言模型的空间推理和定位。这种策略利用单目深度估计技术将场景分割成最近、中程和最远三个层次，并使用基于grounded的视觉-语言模型生成特定于这些区域的描述。这些具有深度感知的结构化描述被添加到图片-问题提示中，以提供空间背景，引导MLLM生成更加基于事实的回应。

**Result:** 实验结果显示，该方法在抑制幻象和加强推理方面有效，适用于多项基准测试和多种MLLM模型。

**Conclusion:** ByDeWay展现了深度感知提示策略在提升MLLM性能方面的能力，且无训练需求、轻量且模块化，适用于多种黑盒模型。

**Abstract:** We introduce ByDeWay, a training-free framework designed to enhance the
performance of Multimodal Large Language Models (MLLMs). ByDeWay uses a novel
prompting strategy called Layered-Depth-Based Prompting (LDP), which improves
spatial reasoning and grounding without modifying any model parameters. It
segments the scene into closest, mid-range, and farthest layers using monocular
depth estimation, then generates region-specific captions with a grounded
vision-language model. These structured, depth-aware captions are appended to
the image-question prompt, enriching it with spatial context. This guides MLLMs
to produce more grounded and less hallucinated responses. Our method is
lightweight, modular, and compatible with black-box MLLMs. Experiments on
hallucination-sensitive (POPE) and reasoning-intensive (GQA) benchmarks show
consistent improvements across multiple MLLMs, validating the effectiveness of
depth-aware prompting in a zero-training setting.

</details>


### [111] [MoSAiC: Multi-Modal Multi-Label Supervision-Aware Contrastive Learning for Remote Sensing](https://arxiv.org/abs/2507.08683)
*Debashis Gupta,Aditi Golder,Rongkhun Zhu,Kangning Cui,Wei Tang,Fan Yang,Ovidiu Csillik,Sarra Alaqahtani,V. Paul Pauca*

Main category: cs.CV

> The paper presents MoSAiC, a new framework for contrastive learning in Earth System Observation that improves semantic disentanglement and robust representation learning in multi-modal satellite imagery, outperforming existing methods on two datasets.

<details>
  <summary>Details</summary>

**Motivation:** To address the unique challenges in Earth System Observation (ESO) that include high inter-class similarity, scene clutter, and ambiguous boundaries in low-label, multi-label settings. Existing frameworks either focus on intra-modality self-supervision or lack mechanisms for multi-label alignment and semantic precision across modalities.

**Method:** MoSAiC, a unified framework that jointly optimizes intra- and inter-modality contrastive learning with a multi-label supervised contrastive loss, designed for multi-modal satellite imagery.

**Result:** Experiments on BigEarthNet V2.0 and Sent12MS datasets show MoSAiC outperforms fully supervised and self-supervised baselines in accuracy, cluster coherence, and generalization in low-label and high-class-overlap scenarios.

**Conclusion:** MoSAiC improves representation learning in multi-modal satellite imagery by enabling finer semantic disentanglement and more robust learning in spectrally similar and spatially complex classes.

**Abstract:** Contrastive learning (CL) has emerged as a powerful paradigm for learning
transferable representations without the reliance on large labeled datasets.
Its ability to capture intrinsic similarities and differences among data
samples has led to state-of-the-art results in computer vision tasks. These
strengths make CL particularly well-suited for Earth System Observation (ESO),
where diverse satellite modalities such as optical and SAR imagery offer
naturally aligned views of the same geospatial regions. However, ESO presents
unique challenges, including high inter-class similarity, scene clutter, and
ambiguous boundaries, which complicate representation learning -- especially in
low-label, multi-label settings. Existing CL frameworks often focus on
intra-modality self-supervision or lack mechanisms for multi-label alignment
and semantic precision across modalities. In this work, we introduce MoSAiC, a
unified framework that jointly optimizes intra- and inter-modality contrastive
learning with a multi-label supervised contrastive loss. Designed specifically
for multi-modal satellite imagery, MoSAiC enables finer semantic
disentanglement and more robust representation learning across spectrally
similar and spatially complex classes. Experiments on two benchmark datasets,
BigEarthNet V2.0 and Sent12MS, show that MoSAiC consistently outperforms both
fully supervised and self-supervised baselines in terms of accuracy, cluster
coherence, and generalization in low-label and high-class-overlap scenarios.

</details>


### [112] [An Efficient Approach for Muscle Segmentation and 3D Reconstruction Using Keypoint Tracking in MRI Scan](https://arxiv.org/abs/2507.08690)
*Mengyuan Liu,Jeongkyu Lee*

Main category: cs.CV

> 本文提出了一种新的无训练肌肉分割方法，采用了关键点跟踪，不仅在计算效率上取得了优势，而且在可解释性和准确性上也与先进的方法相当，特别适合应用于临床和研究中。

<details>
  <summary>Details</summary>

**Motivation:** 磁共振成像(MRI)能够进行非侵入性、高分辨率的肌肉结构分析，但是自动分割技术受限于高计算成本、对大数据集的依赖以及对较小肌肉的分割准确性较低。这种方法旨在解决基于CNN的自动分割在计算成本、通用性和可解释性上的问题。

**Method:** 本研究提出了一种基于关键点跟踪的无训练分割方法，该方法结合了关键点选择与Lucas-Kanade光流法。

**Result:** 提出的方法达到了0.6到0.7的平均Dice相似系数，具体取决于关键点选择策略，与最先进的基于CNN的模型性能相当，但显著减少了计算需求并增强了可解释性。

**Conclusion:** 该可扩展框架提供了肌肉分割的强健且可解释的替代方案，适用于临床和研究应用。

**Abstract:** Magnetic resonance imaging (MRI) enables non-invasive, high-resolution
analysis of muscle structures. However, automated segmentation remains limited
by high computational costs, reliance on large training datasets, and reduced
accuracy in segmenting smaller muscles. Convolutional neural network
(CNN)-based methods, while powerful, often suffer from substantial
computational overhead, limited generalizability, and poor interpretability
across diverse populations. This study proposes a training-free segmentation
approach based on keypoint tracking, which integrates keypoint selection with
Lucas-Kanade optical flow. The proposed method achieves a mean Dice similarity
coefficient (DSC) ranging from 0.6 to 0.7, depending on the keypoint selection
strategy, performing comparably to state-of-the-art CNN-based models while
substantially reducing computational demands and enhancing interpretability.
This scalable framework presents a robust and explainable alternative for
muscle segmentation in clinical and research applications.

</details>


### [113] [NeuralOS: Towards Simulating Operating Systems via Neural Generative Models](https://arxiv.org/abs/2507.08800)
*Luke Rivard,Sun Sun,Hongyu Guo,Wenhu Chen,Yuntian Deng*

Main category: cs.CV

> NeuralOS 是一种神经框架，用于模拟操作系统中的图形用户界面，通过预测屏幕帧对用户输入进行响应。

<details>
  <summary>Details</summary>

**Motivation:** 为了模拟操作系统中的图形用户界面（GUI），通过直接预测屏幕帧来回应用户的输入，如鼠标移动、点击和键盘事件。

**Method:** NeuralOS 结合了循环神经网络（RNN）跟踪计算机状态，以及基于扩散的神经渲染器生成屏幕图像。

**Result:** 实验表明，NeuralOS 成功生成了真实的GUI序列，准确捕捉了鼠标交互，并可靠预测了状态转换如应用启动。

**Conclusion:** 尽管精细的键盘交互仍然具有挑战性，NeuralOS 为未来的人机交互系统提供了完全适应性、生成性神经接口的一步。

**Abstract:** We introduce NeuralOS, a neural framework that simulates graphical user
interfaces (GUIs) of operating systems by directly predicting screen frames in
response to user inputs such as mouse movements, clicks, and keyboard events.
NeuralOS combines a recurrent neural network (RNN), which tracks computer
state, with a diffusion-based neural renderer that generates screen images. The
model is trained on a large-scale dataset of Ubuntu XFCE recordings, which
include both randomly generated interactions and realistic interactions
produced by AI agents. Experiments show that NeuralOS successfully renders
realistic GUI sequences, accurately captures mouse interactions, and reliably
predicts state transitions like application launches. Although modeling
fine-grained keyboard interactions precisely remains challenging, NeuralOS
offers a step toward creating fully adaptive, generative neural interfaces for
future human-computer interaction systems.

</details>


### [114] [L-CLIPScore: a Lightweight Embedding-based Captioning Metric for Evaluating and Training](https://arxiv.org/abs/2507.08710)
*Li Li,Yingzhe Peng,Xu Yang,Ruoxi Cheng,Haiyang Xu,Ming Yan,Fei Huang*

Main category: cs.CV

> 介绍了新型基于嵌入的字幕度量指标L-CLIPScore，该指标能够用来高效评估字幕质量和优化字幕生成模型的训练。

<details>
  <summary>Details</summary>

**Motivation:** 通过开发一种更高效的字幕生成度量方法来改进当前的评估和训练过程。

**Method:** Structure

**Result:** {
  "tldr": "研究人员提出了一种基于嵌入的字幕度量指标L-CLIPScore，用于高效评估字幕质量和训练字幕生成模型。", 
  "motivation": "开发一个更高效的字幕度量方法以改进评估和训练过程。", 
  "method": "L-CLIPScore基于轻量级CLIP（L-CLIP），一种双编码器架构，通过权重多路复用和矩阵分解技术压缩原始CLIP的参数。为了提取知识，研究人员设计了新型多模态相似性调节损失函数（SR）。", 
  "result": "通过压缩和使用SR损失函数训练，L-CLIP实现了与原始CLIP相似的多模态对齐能力，但需要更少的计算资源和运行时间。实验验证了L-CLIPScore的高效性和有效性。", 
  "conclusion": "使用L-CLIPScore作为字幕质量评估的评判标准是有效的，但单独使用它作为训练监督可能会导致训练失败，需要与其他度量方法混合使用。")

**Conclusion:** 尽管L-CLIPScore在评估字幕生成模型质量方面表现出色，但单独将其作为训练监督使用可能会导致问题，需要结合其他度量标准。

**Abstract:** We propose a novel embedding-based captioning metric termed as L-CLIPScore
that can be used for efficiently evaluating caption quality and training
captioning model. L-CLIPScore is calculated from a lightweight CLIP (L-CLIP),
which is a dual-encoder architecture compressed and distilled from CLIP. To
compress, we apply two powerful techniques which are weight multiplexing and
matrix decomposition for reducing the parameters of encoders and word embedding
matrix, respectively. To distill, we design a novel multi-modal Similarity
Regulator (SR) loss to transfer more vision-language alignment knowledge.
Specifically, SR loss amplifies the multi-modal embedding similarity if the
given image-text pair is matched and diminishes the similarity if the pair is
non-matched. By compressing and distilling by this novel SR loss, our L-CLIP
achieves comparable multi-modal alignment ability to the original CLIP while it
requires fewer computation resources and running time. We carry out exhaustive
experiments to validate the efficiency and effectiveness of L-CLIPScore when
using it as the judge to evaluate caption quality. We also discover that when
using L-CLIPScore as the supervisor to train the captioning model, it should be
mixed up by an n-gram-based metric and meanwhile analyze why using L-CLIPScore
only will cause fail training.

</details>


### [115] [SGPMIL: Sparse Gaussian Process Multiple Instance Learning](https://arxiv.org/abs/2507.08711)
*Andreas Lolos,Stergios Christodoulidis,Maria Vakalopoulou,Jose Dolz,Aris Moustakas*

Main category: cs.CV

> 提出了一种新的基于稀疏高斯过程的概率注意力多实例学习框架SGPMIL，用于量化实例级选择中的不确定性，提高了预测的可靠性和可解释性，同时保持了良好的包级性能。

<details>
  <summary>Details</summary>

**Motivation:** 论文试图解决实例级注意力得分中不确定性量化缺乏的问题，特别是在数字病理学这样的高像素图像领域。确定性注意力机制虽然在包级性能上表现良好，但它们忽视了实例相关性中的不确定性，这限制了预测的可靠性和可解释性。

**Method:** 研究方法是引入SGPMIL——一种基于稀疏高斯过程的概率注意力多实例学习框架。这种方法通过学习注意力评分的后验分布来实现不确定性估计。此外，还通过特征缩放改进了预测均值函数，以提高训练速度和效果。

**Result:** 这个论文提出了SGPMIL框架，这是一种基于稀疏高斯过程的概率注意力多实例学习方法。通过学习注意力得分的后验分布，SGPMIL能够提供更可靠、更校准的实例相关性映射，同时在包级性能上保持竞争力，并显著提高了不确定性下的实例级预测的准确性和可解释性。此外，SGPMIL通过在S-GP预测均值函数中引入特征缩放，实现了更快的训练速度、更高的效率和更优的实例级性能。实验表明，在多个数字病理学数据集上，该方法在包级和实例级评估中都表现出了有效性。

**Conclusion:** 研究得出的结论是，通过利用稀疏高斯过程和改进的预测均值函数，SGPMIL不仅提升了模型的训练效率，而且增强了不确定性条件下实例相关性的评估。实验结果表明，该方法在数字病理学数据集上，同时在实例级和包级评估中实现了有效的性能。

**Abstract:** Multiple Instance Learning (MIL) offers a natural solution for settings where
only coarse, bag-level labels are available, without having access to
instance-level annotations. This is usually the case in digital pathology,
which consists of gigapixel sized images. While deterministic attention-based
MIL approaches achieve strong bag-level performance, they often overlook the
uncertainty inherent in instance relevance. In this paper, we address the lack
of uncertainty quantification in instance-level attention scores by introducing
\textbf{SGPMIL}, a new probabilistic attention-based MIL framework grounded in
Sparse Gaussian Processes (SGP). By learning a posterior distribution over
attention scores, SGPMIL enables principled uncertainty estimation, resulting
in more reliable and calibrated instance relevance maps. Our approach not only
preserves competitive bag-level performance but also significantly improves the
quality and interpretability of instance-level predictions under uncertainty.
SGPMIL extends prior work by introducing feature scaling in the SGP predictive
mean function, leading to faster training, improved efficiency, and enhanced
instance-level performance. Extensive experiments on multiple well-established
digital pathology datasets highlight the effectiveness of our approach across
both bag- and instance-level evaluations. Our code will be made publicly
available.

</details>


### [116] [Unreal is all you need: Multimodal ISAC Data Simulation with Only One Engine](https://arxiv.org/abs/2507.08716)
*Kongwu Huang,Shiyi Mu,Jun Jiang,Yuan Gao,Shugong Xu*

Main category: cs.CV

> 研究提出了一种名为Great-X的多模态数据平台并开发了相应的低空无人机数据集Great-MSD，旨在促进ISAC研究中的缩放定律的应用。同时，提出了一种基于CSI的无人机三维定位算法。

<details>
  <summary>Details</summary>

**Motivation:** 探索缩放定律在ISAC(集成传感与通信)研究中的潜力，并提供一个高效的多模态数据模拟平台以及可公开访问的数据集，以促进该领域的进一步研究。

**Method:** 研究使用Unreal Engine来实现Sionna的光线追踪计算，并集成自动驾驶工具，开发了Great-X平台。基于这个平台，构建了一个大型数据集Great-MSD，并提出了基于CSI的无人机三维定位算法。

**Result:** 该研究提出了一种名为Great-X的多模态数据双平台，利用Unreal Engine重建Sionna的光线追踪计算，并与自动驾驶工具深度集成，可以高效地同步模拟包括CSI、RGB、雷达和LiDAR在内的多模态数据。基于此平台，构建了一个开源的大型低空无人机多模态数据集Great-MSD，并提出了一种基于CSI的无人机三维定位算法，展示了其在不同CSI仿真引擎中的可行性和通用性。相关代码和数据集可在https://github.com/hkw-xg/Great-MCD上获得。

**Conclusion:** 研究展示了一种新型多模态数据双平台Great-X及其适用的数据集Great-MSD，提出了一种基于CSI的无人机三维定位算法，证明了其在低空环境下的有效性和多功能性。

**Abstract:** Scaling laws have achieved success in LLM and foundation models. To explore
their potential in ISAC research, we propose Great-X. This single-engine
multimodal data twin platform reconstructs the ray-tracing computation of
Sionna within Unreal Engine and is deeply integrated with autonomous driving
tools. This enables efficient and synchronized simulation of multimodal data,
including CSI, RGB, Radar, and LiDAR. Based on this platform, we construct an
open-source, large-scale, low-altitude UAV multimodal synaesthesia dataset
named Great-MSD, and propose a baseline CSI-based UAV 3D localization
algorithm, demonstrating its feasibility and generalizability across different
CSI simulation engines. The related code and dataset are publicly available at:
https://github.com/hkw-xg/Great-MCD.

</details>


### [117] [RoundaboutHD: High-Resolution Real-World Urban Environment Benchmark for Multi-Camera Vehicle Tracking](https://arxiv.org/abs/2507.08729)
*Yuqiang Lin,Sam Lockyer,Mingxuan Sui,Li Gan,Florian Stanek,Markus Zarbock,Wenbin Li,Adrian Evans,Nic Zhang*

Main category: cs.CV

> 提出RoundaboutHD数据集，解决现有数据集在真实场景中存在的不足，为多摄像头车辆跟踪研究提供高质量的数据支持。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有的公开数据集在场景复杂度、视频清晰度和多样性方面存在不足，无法满足学术研究与实际场景之间的需求，因此需要一套新的数据集来解决这一问题。

**Method:** 介绍了一套名为RoundaboutHD的多摄像头车辆跟踪基准数据集，解决了现有数据集在复杂真实世界场景中表现不足的问题。该数据集为城市十字路口提供了40分钟的高清标注视频素材，并提供多个子集以适应不同研究需要，同时提供了车辆型号信息和摄像头几何信息。

**Result:** 提供了车辆检测、单相机跟踪、图像车辆再识别（ReID）和多摄像头跟踪的基准结果。

**Conclusion:** RoundaboutHD为研究人员提供了一个全面的、高质量的多摄像头车辆跟踪数据集，有助于推进智慧城市建设中涉及的交通密度估算、异常行为检测和嫌疑车追踪等领域的研究。

**Abstract:** The multi-camera vehicle tracking (MCVT) framework holds significant
potential for smart city applications, including anomaly detection, traffic
density estimation, and suspect vehicle tracking. However, current publicly
available datasets exhibit limitations, such as overly simplistic scenarios,
low-resolution footage, and insufficiently diverse conditions, creating a
considerable gap between academic research and real-world scenario. To fill
this gap, we introduce RoundaboutHD, a comprehensive, high-resolution
multi-camera vehicle tracking benchmark dataset specifically designed to
represent real-world roundabout scenarios. RoundaboutHD provides a total of 40
minutes of labelled video footage captured by four non-overlapping,
high-resolution (4K resolution, 15 fps) cameras. In total, 512 unique vehicle
identities are annotated across different camera views, offering rich
cross-camera association data. RoundaboutHD offers temporal consistency video
footage and enhanced challenges, including increased occlusions and nonlinear
movement inside the roundabout. In addition to the full MCVT dataset, several
subsets are also available for object detection, single camera tracking, and
image-based vehicle re-identification (ReID) tasks. Vehicle model information
and camera modelling/ geometry information are also included to support further
analysis. We provide baseline results for vehicle detection, single-camera
tracking, image-based vehicle re-identification, and multi-camera tracking. The
dataset and the evaluation code are publicly available at:
https://github.com/siri-rouser/RoundaboutHD.git

</details>


### [118] [Ensemble of Weak Spectral Total Variation Learners: a PET-CT Case Study](https://arxiv.org/abs/2507.08735)
*Anna Rosenberg,John Kennedy,Zohar Keidar,Yehoshua Y. Zeevi,Guy Gilboa*

Main category: cs.CV

> 本文基于光谱全变差（STV）特征使用弱学习器集合解决计算机视觉中的数据缺乏问题，特别针对CT数据对PET高摄取值的预测，在医疗影像实验中STV特征集合方法优于深度学习和放射组学特征。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决计算机视觉问题中常见的一个问题，即缺乏足够的训练数据。为此，作者提出了使用基于光谱全变差（STV）特征的弱学习器集合，这种特征能够很好地表征纹理，并且在二维情况下呈现出较低的相关性，因此适合集成学习的方法。

**Method:** 使用基于光谱全变差（STV）特征的弱学习器集合来解决机器学习中的计算机视觉问题，特别是在医疗影像分析中CT数据对PET高摄取值的预测问题。STV特征与全变差次梯度的非线性特征函数有关，能够很好地表征不同尺度下的纹理，并且在二维情况下这些特征表现出较低的相关性，这符合集成学习中对于低相关弱学习器的使用建议。

**Result:** 研究结果表明，使用STV特征的弱学习器集合方法优于深度学习方法（AUC=0.75）和放射组学特征（AUC=0.79），达到了最佳预测效果（AUC=0.87）。特别是CT图像中的精细STV尺度对于预测PET高摄取值具有很强的指示作用。

**Conclusion:** 本文通过解决一个实际的医疗影像问题，证明了基于STV特征的弱学习器集合方法的有效性，这种方法在预测CT数据对PET高摄取值的应用中表现出色。该研究提出了在缺乏足够训练数据的情况下，利用集成学习和STV特征进行计算机视觉任务的新路径。

**Abstract:** Solving computer vision problems through machine learning, one often
encounters lack of sufficient training data. To mitigate this we propose the
use of ensembles of weak learners based on spectral total-variation (STV)
features (Gilboa 2014). The features are related to nonlinear eigenfunctions of
the total-variation subgradient and can characterize well textures at various
scales. It was shown (Burger et-al 2016) that, in the one-dimensional case,
orthogonal features are generated, whereas in two-dimensions the features are
empirically lowly correlated. Ensemble learning theory advocates the use of
lowly correlated weak learners. We thus propose here to design ensembles using
learners based on STV features. To show the effectiveness of this paradigm we
examine a hard real-world medical imaging problem: the predictive value of
computed tomography (CT) data for high uptake in positron emission tomography
(PET) for patients suspected of skeletal metastases. The database consists of
457 scans with 1524 unique pairs of registered CT and PET slices. Our approach
is compared to deep-learning methods and to Radiomics features, showing STV
learners perform best (AUC=0.87), compared to neural nets (AUC=0.75) and
Radiomics (AUC=0.79). We observe that fine STV scales in CT images are
especially indicative for the presence of high uptake in PET.

</details>


### [119] [HieraRS: A Hierarchical Segmentation Paradigm for Remote Sensing Enabling Multi-Granularity Interpretation and Cross-Domain Transfer](https://arxiv.org/abs/2507.08741)
*Tianlong Ai,Tianzhu Liu,Haochen Jiang,Yanfeng Gu*

Main category: cs.CV

> 论文提出了一种用于遥感图像的分层土地覆盖和土地使用分类的新方法，解决了现有基于深度学习模型的两个主要限制，增强了模型的灵活性和泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于深度学习的方法在对遥感图像进行分层土地覆盖和土地使用分类时，主要面临两个主要挑战：1) 它们采用扁平分类范式，限制了它们生成端到端多粒度分层预测的能力，以符合实际使用的树状分层结构。2) 大多数跨域研究侧重于传感器或场景变化引起的性能下降问题，对于将 LCLU 模型迁移到具有异构分层结构的不同领域任务的关注较少。这些限制影响了 LCLU 模型在实际应用中的灵活性和泛化能力。

**Method:** 提出了一种新的分层解释范式 HieraRS，可以进行多粒度预测，并支持高效地将 LCLU 模型迁移到具有异构分层结构的跨领域任务中。我们引入了双向分层一致性约束机制 (BHCCM)，可以无缝集成到主流的扁平分类模型中生成分层预测，同时提高语义一致性和分类准确性。另外我们还提出了 TransLU，这是一个双分支跨域迁移框架，包含两个重要组成部分：跨域知识共享（CDKS）和跨域语义对齐（CDSA）。TransLU 支持动态类别扩展，并促进了 LCLU 模型对异构分层的适应。

**Result:** 研究构建了一个大规模的多模态分层土地利用数据集 MM-5B，该数据集具有像素级标注，这有助于验证 HieraRS 和 TransLU 方法的有效性。

**Conclusion:** 通过提出 HieraRS 和 TransLU，解决了现有 LCLU 模型的两个主要挑战：扁平分类范式限制以及对异构分层结构跨域迁移的关注不足。这提高了模型在实际应用中的灵活性和泛化潜力，特别是在跨域任务中。

**Abstract:** Hierarchical land cover and land use (LCLU) classification aims to assign
pixel-wise labels with multiple levels of semantic granularity to remote
sensing (RS) imagery. However, existing deep learning-based methods face two
major challenges: 1) They predominantly adopt a flat classification paradigm,
which limits their ability to generate end-to-end multi-granularity
hierarchical predictions aligned with tree-structured hierarchies used in
practice. 2) Most cross-domain studies focus on performance degradation caused
by sensor or scene variations, with limited attention to transferring LCLU
models to cross-domain tasks with heterogeneous hierarchies (e.g., LCLU to crop
classification). These limitations hinder the flexibility and generalization of
LCLU models in practical applications. To address these challenges, we propose
HieraRS, a novel hierarchical interpretation paradigm that enables
multi-granularity predictions and supports the efficient transfer of LCLU
models to cross-domain tasks with heterogeneous tree-structured hierarchies. We
introduce the Bidirectional Hierarchical Consistency Constraint Mechanism
(BHCCM), which can be seamlessly integrated into mainstream flat classification
models to generate hierarchical predictions, while improving both semantic
consistency and classification accuracy. Furthermore, we present TransLU, a
dual-branch cross-domain transfer framework comprising two key components:
Cross-Domain Knowledge Sharing (CDKS) and Cross-Domain Semantic Alignment
(CDSA). TransLU supports dynamic category expansion and facilitates the
effective adaptation of LCLU models to heterogeneous hierarchies. In addition,
we construct MM-5B, a large-scale multi-modal hierarchical land use dataset
featuring pixel-wise annotations. The code and MM-5B dataset will be released
at: https://github.com/AI-Tianlong/HieraRS.

</details>


### [120] [Geo-ORBIT: A Federated Digital Twin Framework for Scene-Adaptive Lane Geometry Detection](https://arxiv.org/abs/2507.08743)
*Rei Tamaru,Pei Li,Bin Ran*

Main category: cs.CV

> 这篇论文提出一种名为Geo-ORBIT的框架，该框架克服了现有数字孪生在交通系统应用上的挑战，实现了高效的车道检测与数据处理能力。

<details>
  <summary>Details</summary>

**Motivation:** 当前数字孪生在交通系统中的方法依赖静态地图或昂贵的传感器，限制了其扩展性和适应性。大规模数字孪生在收集和分析多源数据时面临隐私、通讯和计算效率等挑战。

**Method:** 介绍了一种名为Geo-ORBIT的统一框架，该框架结合了实时车道检测、数字孪生同步和联邦元学习。核心部分GeoLane是一种轻量级车道检测模型，可以从车辆轨迹数据中学习车道几何结构。进一步通过Meta-GeoLane元学习个性化局部实体的检测参数，FedMeta-GeoLane则是一种联邦学习策略，确保跨路边部署中的可扩展性和隐私保护性适应性。

**Result:** 实验结果显示，在各类城市环境中，FedMeta-GeoLane在降低几何误差的同时，增强了对未知地点的泛化能力，并大幅减少了通讯开销，性能优于基线方法和元学习方法。

**Conclusion:** 该研究为数字孪生中的柔性、场景感知型基础设施模型奠定了基础，并公开了框架代码以供进一步研究与应用。

**Abstract:** Digital Twins (DT) have the potential to transform traffic management and
operations by creating dynamic, virtual representations of transportation
systems that sense conditions, analyze operations, and support decision-making.
A key component for DT of the transportation system is dynamic roadway geometry
sensing. However, existing approaches often rely on static maps or costly
sensors, limiting scalability and adaptability. Additionally, large-scale DTs
that collect and analyze data from multiple sources face challenges in privacy,
communication, and computational efficiency. To address these challenges, we
introduce Geo-ORBIT (Geometrical Operational Roadway Blueprint with Integrated
Twin), a unified framework that combines real-time lane detection, DT
synchronization, and federated meta-learning. At the core of Geo-ORBIT is
GeoLane, a lightweight lane detection model that learns lane geometries from
vehicle trajectory data using roadside cameras. We extend this model through
Meta-GeoLane, which learns to personalize detection parameters for local
entities, and FedMeta-GeoLane, a federated learning strategy that ensures
scalable and privacy-preserving adaptation across roadside deployments. Our
system is integrated with CARLA and SUMO to create a high-fidelity DT that
renders highway scenarios and captures traffic flows in real-time. Extensive
experiments across diverse urban scenes show that FedMeta-GeoLane consistently
outperforms baseline and meta-learning approaches, achieving lower geometric
error and stronger generalization to unseen locations while drastically
reducing communication overhead. This work lays the foundation for flexible,
context-aware infrastructure modeling in DTs. The framework is publicly
available at https://github.com/raynbowy23/FedMeta-GeoLane.git.

</details>


### [121] [Compress Any Segment Anything Model (SAM)](https://arxiv.org/abs/2507.08765)
*Juntong Fan,Zhiwei Hao,Jianqiang Shen,Shang-Ling Jui,Yi Zhang,Jing-Xiao Liao,Feng-Lei Fan*

Main category: cs.CV

> 本文提出了一种名为Birkhoff的新型无数据压缩算法，可有效压缩SAM，并通过实验表明，Birkhoff在压缩时间、压缩比、压缩后的性能和推理速度方面表现一致且具有竞争力。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于SAM及其变体在医疗保健和智能制造等不同场景中的卓越性能，高效压缩SAM已经成为一个紧迫的实际需求。

**Method:** Birkhoff提出了一种新颖的无数据压缩算法，通过Hyper-Compression和HyperLinear线性层操作来实现SAM及其变体的压缩和加速推理。

**Result:** 实验表明，Birkhoff在COCO、LVIS和SA-1B数据集上的18个SAM模型上表现稳定且具有竞争力，能够以不到1%的性能下降在60秒内实现5.17倍的压缩比。

**Conclusion:** 该研究通过提出Birkhoff实现了SAM及其变体的有效压缩，证明了其在压缩比、速度和性能方面的优秀表现。

**Abstract:** Due to the excellent performance in yielding high-quality, zero-shot
segmentation, Segment Anything Model (SAM) and its variants have been widely
applied in diverse scenarios such as healthcare and intelligent manufacturing.
Therefore, effectively compressing SAMs has become an increasingly pressing
practical need. In this study, we propose Birkhoff, a novel data-free
compression algorithm for SAM and its variants. Unlike quantization, pruning,
distillation, and other compression methods, Birkhoff embodies versatility
across model types, agility in deployment, faithfulness to the original model,
and compactness in model size. Specifically, Birkhoff introduces a novel
compression algorithm: Hyper-Compression, whose core principle is to find a
dense trajectory to turn a high-dimensional parameter vector into a
low-dimensional scalar. Furthermore, Birkhoff designs a dedicated linear layer
operator, HyperLinear, to fuse decompression and matrix multiplication to
significantly accelerate inference of the compressed SAMs. Extensive
experiments on 18 SAMs in the COCO, LVIS, and SA-1B datasets show that Birkhoff
performs consistently and competitively in compression time, compression ratio,
post-compression performance, and inference speed. For example, Birkhoff can
achieve a compression ratio of 5.17x on SAM2-B, with less than 1% performance
drop without using any fine-tuning data. Moreover, the compression is finished
within 60 seconds for all models.

</details>


### [122] [A Hybrid Multi-Well Hopfield-CNN with Feature Extraction and K-Means for MNIST Classification](https://arxiv.org/abs/2507.08766)
*Ahmed Farooq*

Main category: cs.CV

> The paper proposes a hybrid model combining CNNs and Hopfield networks to classify handwritten digits in the MNIST dataset, achieving a high test accuracy of 99.2%.

<details>
  <summary>Details</summary>

**Motivation:** To handle intraclass variability and provide an interpretable framework for classification by leveraging the representation capabilities of CNNs and the energy-based decision process of Hopfield networks.

**Method:** The model extracts high-dimensional features using CNNs, clusters these into class-specific prototypes with k-means, and uses the prototypes as attractors in a multi-well Hopfield network for classification.

**Result:** The hybrid model achieved a high test accuracy of 99.2% on the MNIST dataset, highlighting the importance of deep feature extraction and proper prototype representation.

**Conclusion:** The findings suggest that the hybrid model effectively addresses image classification tasks with high variability by effectively combining CNNs and Hopfield networks, with potential applications in broader pattern recognition tasks.

**Abstract:** This study presents a hybrid model for classifying handwritten digits in the
MNIST dataset, combining convolutional neural networks (CNNs) with a multi-well
Hopfield network. The approach employs a CNN to extract high-dimensional
features from input images, which are then clustered into class-specific
prototypes using k-means clustering. These prototypes serve as attractors in a
multi-well energy landscape, where a Hopfield network performs classification
by minimizing an energy function that balances feature similarity and class
assignment.The model's design enables robust handling of intraclass
variability, such as diverse handwriting styles, while providing an
interpretable framework through its energy-based decision process. Through
systematic optimization of the CNN architecture and the number of wells, the
model achieves a high test accuracy of 99.2% on 10,000 MNIST images,
demonstrating its effectiveness for image classification tasks. The findings
highlight the critical role of deep feature extraction and sufficient prototype
coverage in achieving high performance, with potential for broader applications
in pattern recognition.

</details>
