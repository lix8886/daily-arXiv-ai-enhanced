{"id": "2508.04797", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04797", "abs": "https://arxiv.org/abs/2508.04797", "authors": ["Mohab Kishawy", "Ali Abdellatif Hussein", "Jun Chen"], "title": "RetinexDual: Retinex-based Dual Nature Approach for Generalized Ultra-High-Definition Image Restoration", "comment": null, "summary": "Advancements in image sensing have elevated the importance of\nUltra-High-Definition Image Restoration (UHD IR). Traditional methods, such as\nextreme downsampling or transformation from the spatial to the frequency\ndomain, encounter significant drawbacks: downsampling induces irreversible\ninformation loss in UHD images, while our frequency analysis reveals that pure\nfrequency-domain approaches are ineffective for spatially confined image\nartifacts, primarily due to the loss of degradation locality. To overcome these\nlimitations, we present RetinexDual, a novel Retinex theory-based framework\ndesigned for generalized UHD IR tasks. RetinexDual leverages two complementary\nsub-networks: the Scale-Attentive maMBA (SAMBA) and the Frequency Illumination\nAdaptor (FIA). SAMBA, responsible for correcting the reflectance component,\nutilizes a coarse-to-fine mechanism to overcome the causal modeling of mamba,\nwhich effectively reduces artifacts and restores intricate details. On the\nother hand, FIA ensures precise correction of color and illumination\ndistortions by operating in the frequency domain and leveraging the global\ncontext provided by it. Evaluating RetinexDual on four UHD IR tasks, namely\nderaining, deblurring, dehazing, and Low-Light Image Enhancement (LLIE), shows\nthat it outperforms recent methods qualitatively and quantitatively. Ablation\nstudies demonstrate the importance of employing distinct designs for each\nbranch in RetinexDual, as well as the effectiveness of its various components.", "AI": {"tldr": "RetinexDual是一种新的UHD图像恢复框架，针对传统方法的局限性，通过互补的子网络结构提供更有效的图像恢复解决方案。", "motivation": "传统的极端下采样或空间到频率域转换方法在UHD图像恢复中存在明显缺陷。下采样会导致信息不可逆损失，而仅采用频率域方法未能有效处理局部图像失真。RetinexDual旨在克服这些限制。", "method": "RetinexDual采用基于Retinex理论的框架，包含两个互补子网络：Scale-Attentive maMBA（SAMBA）和Frequency Illumination Adaptor（FIA）。SAMBA通过粗到细的机制来修正反射成分，而FIA在频率域中操作，利用全局上下文信息精确修正颜色和照明失真。", "result": "在四种UHD图像恢复任务（去雨、去模糊、去雾和低光图像增强）的评估中，RetinexDual的性能优于近期方法，无论是定性还是定量评估。", "conclusion": "实验结果表明，RetinexDual的两个分支的设计和各个组件的有效性为其在UHD图像恢复任务上优于其他方法做出了重要贡献。"}}
{"id": "2508.04801", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04801", "abs": "https://arxiv.org/abs/2508.04801", "authors": ["Trong-Thuan Nguyen", "Viet-Tham Huynh", "Thao Thi Phuong Dao", "Ha Nguyen Thi", "Tien To Vu Thuy", "Uyen Hanh Tran", "Tam V. Nguyen", "Thanh Dinh Le", "Minh-Triet Tran"], "title": "ACM Multimedia Grand Challenge on ENT Endoscopy Analysis", "comment": null, "summary": "Automated analysis of endoscopic imagery is a critical yet underdeveloped\ncomponent of ENT (ear, nose, and throat) care, hindered by variability in\ndevices and operators, subtle and localized findings, and fine-grained\ndistinctions such as laterality and vocal-fold state. In addition to\nclassification, clinicians require reliable retrieval of similar cases, both\nvisually and through concise textual descriptions. These capabilities are\nrarely supported by existing public benchmarks. To this end, we introduce\nENTRep, the ACM Multimedia 2025 Grand Challenge on ENT endoscopy analysis,\nwhich integrates fine-grained anatomical classification with image-to-image and\ntext-to-image retrieval under bilingual (Vietnamese and English) clinical\nsupervision. Specifically, the dataset comprises expert-annotated images,\nlabeled for anatomical region and normal or abnormal status, and accompanied by\ndual-language narrative descriptions. In addition, we define three benchmark\ntasks, standardize the submission protocol, and evaluate performance on public\nand private test splits using server-side scoring. Moreover, we report results\nfrom the top-performing teams and provide an insight discussion.", "AI": {"tldr": "本文介绍了ENTRep挑战赛，以解决ENT内窥镜图像自动化分析中的关键问题，强调其基准任务、提交协议标准化以及使用服务器端评分的公共和私人测试拆分的性能评估。", "motivation": "内窥镜图像的自动化分析在ENT护理中是一个重要但尚未充分开发的组成部分。这项工作受到了设备和操作员差异，以及细微和局部发现，特别是细微区别如侧别性和声带状态的挑战。现有公开基准很少支持分类以外的能力，例如检索类似病例，无论是在视觉上还是通过简洁的文本描述。", "method": "本文介绍了ENTRep，这是ACM Multimedia 2025的ENT内窥镜分析大赛，集成了细粒度的解剖结构分类与图像到图像和文本到图像的检索功能，并配备了双语（越南语和英语）临床监督。", "result": "文中报道了顶尖参赛队伍的表现，并提供了深入的讨论。", "conclusion": "通过ENTRep挑战赛，该研究强调了在ENT内窥镜图像的分析中，实现细粒度的解剖结构分类和双语临床监督下的图像及文本检索的重要性。"}}
{"id": "2508.04816", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04816", "abs": "https://arxiv.org/abs/2508.04816", "authors": ["Sriram Mandalika", "Lalitha V"], "title": "CoMAD: A Multiple-Teacher Self-Supervised Distillation Framework", "comment": "8 Pages, 2 Figures", "summary": "Numerous self-supervised learning paradigms, such as contrastive learning and\nmasked image modeling, learn powerful representations from unlabeled data but\nare typically pretrained in isolation, overlooking complementary insights and\nyielding large models that are impractical for resource-constrained deployment.\nTo overcome these challenges, we introduce Consensus-oriented Masked\nDistillation (CoMAD), a lightweight, parameter-free framework that unifies\nknowledge from multiple current state-of-the-art self-supervised Vision\nTransformers into a compact student network. CoMAD distills from three\npretrained ViT-Base teachers, MAE, MoCo v3, and iBOT, each offering distinct\nsemantic and contextual priors. Rather than naively averaging teacher outputs,\nwe apply asymmetric masking: the student sees only 25 percent of patches while\neach teacher receives a progressively lighter, unique mask, forcing the student\nto interpolate missing features under richer contexts. Teacher embeddings are\naligned to the student's space via a linear adapter and layer normalization,\nthen fused through our joint consensus gating, which weights each token by\ncombining cosine affinity with inter-teacher agreement. The student is trained\nwith dual-level KL divergence on visible tokens and reconstructed feature maps,\ncapturing both local and global structure. On ImageNet-1K, CoMAD's ViT-Tiny\nachieves 75.4 percent Top-1, an increment of 0.4 percent over the previous\nstate-of-the-art. In dense-prediction transfers, it attains 47.3 percent mIoU\non ADE20K, and 44.5 percent box average precision and 40.5 percent mask average\nprecision on MS-COCO, establishing a new state-of-the-art in compact SSL\ndistillation.", "AI": {"tldr": "研究提出了一种轻量级、无参数框架CoMAD，它将多个自监督视觉变压器的知识合并到一个紧凑的学生网络中，适用于资源有限的部署，同时保证性能。", "motivation": "许多自监督学习范式，如对比学习和掩码图像建模，可以从未标记的数据中学习强大的表征，但它们通常是在孤立的情况下进行预训练的，忽略了一致的见解，并产生大型模型，适用于资源有限的部署的模型，从而产生这些挑战。因此引入了CoMAD框架。", "method": "提出了一种名为共识导向掩码蒸馏（CoMAD）的轻量级、无参数框架，它将多个当前最先进的自监督视觉变压器的知识统一到一个紧凑的学生网络中。CoMAD从三个预训练的ViT-Base教师模型（MAE、MoCo v3和iBOT）中蒸馏知识，并采用非对称掩码策略，学生只能看到25%的补丁，而每个教师接收不断变轻的独特掩码，这迫使学生在更丰富的上下文中插补缺失的特征。教师嵌入通过线性适配器和层归一化对齐到学生的空间中，然后通过联合共识门融合。学生通过具有双层KL散度的训练来捕捉局部和全局结构，对可见的补丁和重建的特征图进行训练。", "result": "在ImageNet-1K上，CoMAD的ViT-Tiny达到了75.4%的top-1准确率，比之前最先进的方法提高了0.4%。在密集预测转移中，它在ADE20K上达到了47.3%的mIoU，在MS-COCO上达到了44.5%的框平均精度和40.5%的掩码平均精度，建立了紧凑SSL蒸馏的新水平。", "conclusion": "该研究表明，通过多源知识蒸馏，可以创建一个轻量级、性能优越的自监督学习模型，无需参数调整，适用于资源受限环境。"}}
{"id": "2508.04818", "categories": ["cs.CV", "eess.IV", "stat.ML", "62H35, 68T07, 62M40, 68T45", "I.2.6; I.2.10; I.4.6; I.4.8; I.5.1; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.04818", "abs": "https://arxiv.org/abs/2508.04818", "authors": ["Mehrdad Moradi", "Marco Grasso", "Bianca Maria Colosimo", "Kamran Paynabar"], "title": "Single-Step Reconstruction-Free Anomaly Detection and Segmentation via Diffusion Models", "comment": "9 pages, 8 figures, 2 tables. Submitted to an IEEE conference", "summary": "Generative models have demonstrated significant success in anomaly detection\nand segmentation over the past decade. Recently, diffusion models have emerged\nas a powerful alternative, outperforming previous approaches such as GANs and\nVAEs. In typical diffusion-based anomaly detection, a model is trained on\nnormal data, and during inference, anomalous images are perturbed to a\npredefined intermediate step in the forward diffusion process. The\ncorresponding normal image is then reconstructed through iterative reverse\nsampling.\n  However, reconstruction-based approaches present three major challenges: (1)\nthe reconstruction process is computationally expensive due to multiple\nsampling steps, making real-time applications impractical; (2) for complex or\nsubtle patterns, the reconstructed image may correspond to a different normal\npattern rather than the original input; and (3) Choosing an appropriate\nintermediate noise level is challenging because it is application-dependent and\noften assumes prior knowledge of anomalies, an assumption that does not hold in\nunsupervised settings.\n  We introduce Reconstruction-free Anomaly Detection with Attention-based\ndiffusion models in Real-time (RADAR), which overcomes the limitations of\nreconstruction-based anomaly detection. Unlike current SOTA methods that\nreconstruct the input image, RADAR directly produces anomaly maps from the\ndiffusion model, improving both detection accuracy and computational\nefficiency. We evaluate RADAR on real-world 3D-printed material and the\nMVTec-AD dataset. Our approach surpasses state-of-the-art diffusion-based and\nstatistical machine learning models across all key metrics, including accuracy,\nprecision, recall, and F1 score. Specifically, RADAR improves F1 score by 7% on\nMVTec-AD and 13% on the 3D-printed material dataset compared to the next best\nmodel.\n  Code available at: https://github.com/mehrdadmoradi124/RADAR", "AI": {"tldr": "本文介绍了一种名为RADAR的新方法，它是一种基于注意力的扩散模型，适用于实时无重构的异常检测，提高了检测的准确性与效率，并在多个数据集上超过现有的SOTA方法。", "motivation": "传统基于重构的异常检测方法面临几个主要挑战：计算成本高、复杂模式的图像重构可能导致与原图像不同的正常模式以及选择合适的中间噪声水平困难。为此，作者提出了一种新的方法以克服这些挑战。", "method": "本文提出了一种名为RADAR的无重构异常检测方法，该方法基于注意力扩散模型，能够直接从扩散模型中生成异常图，从而提高了检测效率和准确性。不同于现有的SOTA方法，RADAR不需要重构输入图像，因此能够实现实时应用。", "result": "RADAR方法在MVTec-AD数据集和3D打印材料数据集上的多项关键指标测试中超越了状态-of-the-art的扩散模型和统计机器学习模型，尤其是在F1评分上分别提高了7%和13%。", "conclusion": "RADAR方法有效解决了传统重构方法存在的计算成本高、图像重构可能导致与原图像不同的正常模式及选择合适的中间噪声水平的问题，实现了更高的检测准确性和实时性。"}}
{"id": "2508.04795", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.04795", "abs": "https://arxiv.org/abs/2508.04795", "authors": ["Thomas Thebaud", "Yen-Ju Lu", "Matthew Wiesner", "Peter Viechnicki", "Najim Dehak"], "title": "Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM", "comment": "Accepted in the 2025 IEEE Automatic Speech Recognition and\n  Understanding Workshop", "summary": "In dialogue transcription pipelines, Large Language Models (LLMs) are\nfrequently employed in post-processing to improve grammar, punctuation, and\nreadability. We explore a complementary post-processing step: enriching\ntranscribed dialogues by adding metadata tags for speaker characteristics such\nas age, gender, and emotion. Some of the tags are global to the entire\ndialogue, while some are time-variant. Our approach couples frozen audio\nfoundation models, such as Whisper or WavLM, with a frozen LLAMA language model\nto infer these speaker attributes, without requiring task-specific fine-tuning\nof either model. Using lightweight, efficient connectors to bridge audio and\nlanguage representations, we achieve competitive performance on speaker\nprofiling tasks while preserving modularity and speed. Additionally, we\ndemonstrate that a frozen LLAMA model can compare x-vectors directly, achieving\nan Equal Error Rate of 8.8% in some scenarios.", "AI": {"tldr": "论文探索了一种通过元数据标签补充对话转录的方法，以增强说话者的特征属性，使用音频和语言模型结合的方法，在保持高效的同时取得了良好的说话者特征识别效果。", "motivation": "研究动机在于通过增强对话转录中的元数据标签（如说话者年龄、性别和情绪）来提高对话质量，而不是仅仅依靠大型语言模型进行后处理以改善语法规则、标点和可读性。", "method": "该方法结合了冻结的音频基础模型（如Whisper或WavLM）和冻结的LLAMA语言模型，来推断这些说话者属性，且无需对任何模型进行特定任务的微调。通过轻量级、高效的连接器来桥接音频和语言表示。", "result": "{야매 요약}", "conclusion": "研究结论显示这种方法在说话者特征识别任务上取得了有竞争力的表现，同时保持了模块化和速度。此外，还表明冻结的LLAMA模型可以直接比较x-vectors，在某些场景下等误率（Equal Error Rate）为8.8%。"}}
{"id": "2508.04827", "categories": ["cs.CV", "68T05, 68T07", "I.2.10; I.5.1; I.4.8; J.4"], "pdf": "https://arxiv.org/pdf/2508.04827", "abs": "https://arxiv.org/abs/2508.04827", "authors": ["Chirag Seth", "Divya Naiken", "Keyan Lin"], "title": "A deep learning approach to track eye movements based on events", "comment": null, "summary": "This research project addresses the challenge of accurately tracking eye\nmovements during specific events by leveraging previous research. Given the\nrapid movements of human eyes, which can reach speeds of 300{\\deg}/s, precise\neye tracking typically requires expensive and high-speed cameras. Our primary\nobjective is to locate the eye center position (x, y) using inputs from an\nevent camera. Eye movement analysis has extensive applications in consumer\nelectronics, especially in VR and AR product development. Therefore, our\nultimate goal is to develop an interpretable and cost-effective algorithm using\ndeep learning methods to predict human attention, thereby improving device\ncomfort and enhancing overall user experience. To achieve this goal, we\nexplored various approaches, with the CNN\\_LSTM model proving most effective,\nachieving approximately 81\\% accuracy. Additionally, we propose future work\nfocusing on Layer-wise Relevance Propagation (LRP) to further enhance the\nmodel's interpretability and predictive performance.", "AI": {"tldr": "研究利用事件相机和深度学习方法开发了一种成本效益高的眼球追踪算法，以定位眼睛中心位置并预测人类注意力，以改善消费电子产品的用户体验。主要使用了CNN_LSTM模型，达到了约81%的准确率。", "motivation": "鉴于眼球运动速度非常快，传统精确眼球跟踪通常需要昂贵和高速相机。该研究针对这一挑战，旨在开发低成本、高效率的眼球追踪技术，以进一步提升消费电子产品（特别是VR和AR产品）的用户体验。", "method": "本研究利用了事件相机输入，并探索了不同的模型方法，模型中以CNN_LSTM模型表现最为出色，达到了大约81%的准确率。未来还会采用Layer-wise Relevance Propagation（LRP）技术来提升模型的可解释性和预测能力。", "result": "该研究旨在利用事件相机输入来准确跟踪特定事件期间的眼球运动，目标是定位眼睛中心位置（x，y）。由于眼球运动速度可达到300°/s，通常需要昂贵且高速的相机才能实现精确的眼球跟踪。本研究利用深度学习方法，特别是CNN_LSTM模型，实现了约81%的准确率，目标是开发一个可解释且成本效益较高的算法来预测人类注意力，从而改善设备舒适性和用户体验。未来工作将集中在使用Layer-wise Relevance Propagation (LRP) 进一步增强模型的可解释性和预测性能。", "conclusion": "本研究通过深度学习方法，特别是CNN_LSTM模型，实现了低成本、高效率的眼球追踪算法，达到了81%的准确率，有助于改善消费电子产品（尤其是VR和AR产品）的用户体验。未来的研究将继续提升该模型的可解释性和预测性能。"}}
{"id": "2508.04796", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04796", "abs": "https://arxiv.org/abs/2508.04796", "authors": ["Negar Foroutan", "Clara Meister", "Debjit Paul", "Joel Niklaus", "Sina Ahmadi", "Antoine Bosselut", "Rico Sennrich"], "title": "Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization", "comment": null, "summary": "Tokenization is the first -- and often least scrutinized -- step of most NLP\npipelines. Standard algorithms for learning tokenizers rely on frequency-based\nobjectives, which favor languages dominant in the training data and\nconsequently leave lower-resource languages with tokenizations that are\ndisproportionately longer, morphologically implausible, or even riddled with\n<UNK> placeholders. This phenomenon ultimately amplifies computational and\nfinancial inequalities between users from different language backgrounds. To\nremedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of\nthe widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes\nthe compression gain of the currently worst-compressed language, trading a\nsmall amount of global compression for cross-lingual parity. We find\nempirically that Parity-aware BPE leads to more equitable token counts across\nlanguages, with negligible impact on global compression rate and no substantial\neffect on language-model performance in downstream tasks.", "AI": {"tldr": "提出了Parity-aware BPE算法，以确保低资源语言的标记化质量和高资源语言相当，实现跨语言的公平性，对于语言模型的全局性能基本没有影响。", "motivation": "标准的分词算法依赖于基于频率的目标，这有利于训练数据中占主导地位的语言，并且会导致低资源语言的标记化问题。这种现象加剧了来自不同语言背景的用户之间的计算和财务不平等。为了改善这种情况，提出了新的分词方法。", "method": "提出了一种名为Parity-aware Byte Pair Encoding (BPE) 的变体，该算法在每个合并步骤中最大化当前压缩率最差的语言的压缩增益，以换取少量的全局压缩率，从而实现跨语言的均衡。", "result": "实验证明，Parity-aware BPE 能够在不影响全局压缩率和下游任务语言模型性能的情况下，实现跨语言更均衡的标记数量。", "conclusion": "Parity-aware BPE 能够在不显著影响全局压缩率和语言模型性能的前提下，实现跨语言更均衡的标记化。这一发现表明在NLP的初期阶段也需要关注公平性。"}}
