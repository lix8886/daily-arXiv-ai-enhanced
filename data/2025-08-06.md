<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 35]
- [cs.CV](#cs.CV) [Total: 39]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Clinically Grounded Agent-based Report Evaluation: An Interpretable Metric for Radiology Report Generation](https://arxiv.org/abs/2508.02808)
*Radhika Dua,Young Joon,Kwon,Siddhant Dogra,Daniel Freedman,Diana Ruan,Motaz Nashawaty,Danielle Rigau,Daniel Alexander Alber,Kang Zhang,Kyunghyun Cho,Eric Karl Oermann*

Main category: cs.CL

> 本文提出了ICARE框架，用于可解释的放射学报告评估，改善了自动化报告生成的安全性。

<details>
  <summary>Details</summary>

**Motivation:** 自动化放射学报告生成需要可靠的临床评估，但现有度量标准往往依赖于表面相似性或作为黑箱，缺乏可解释性。

**Method:** 提出了ICARE（可解释的基于代理的临床报告评估框架），该框架使用大型语言模型代理和动态多项选择问答（MCQA）来生成和回答具有临床意义的问题，以评估生成的报告是否能保持并一致地传达临床发现。

**Result:** 临床研究表明，ICARE与专家判断的相关性远高于先前的指标。分析还确认了其对临床内容的敏感性和可重复性，并揭示了可解释的错误模式。

**Conclusion:** 通过将分数与问题-答案对联系起来，ICARE实现了透明和可解释的评估，为自动化放射学报告生成提供了一种可靠的度量标准。

**Abstract:** Radiological imaging is central to diagnosis, treatment planning, and
clinical decision-making. Vision-language foundation models have spurred
interest in automated radiology report generation (RRG), but safe deployment
requires reliable clinical evaluation of generated reports. Existing metrics
often rely on surface-level similarity or behave as black boxes, lacking
interpretability. We introduce ICARE (Interpretable and Clinically-grounded
Agent-based Report Evaluation), an interpretable evaluation framework
leveraging large language model agents and dynamic multiple-choice question
answering (MCQA). Two agents, each with either the ground-truth or generated
report, generate clinically meaningful questions and quiz each other. Agreement
on answers captures preservation and consistency of findings, serving as
interpretable proxies for clinical precision and recall. By linking scores to
question-answer pairs, ICARE enables transparent, and interpretable assessment.
Clinician studies show ICARE aligns significantly more with expert judgment
than prior metrics. Perturbation analyses confirm sensitivity to clinical
content and reproducibility, while model comparisons reveal interpretable error
patterns.

</details>


### [2] [Modeling Annotator Disagreement with Demographic-Aware Experts and Synthetic Perspectives](https://arxiv.org/abs/2508.02853)
*Yinuo Xu,Veronica Derricks,Allison Earl,David Jurgens*

Main category: cs.CL

> 本文提出了一种新的模型DEM-MoE，通过专家子网络路由和合成注释改进了对注释者分歧的建模效果，使得模型能更好地处理多样化的视角。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在通过架构和数据层面的创新建模主观NLP任务中的注释者分歧，以更好地处理群体层面的结构化变异性。

**Method:** DEM-MoE模型通过基于注释者人口统计学特征的专家子网络路由，以更好地表示结构化和群体层面的变化，优于之前的模型。此外，通过使用大规模语言模型生成的合成注释进行零样本角色提示来测试稀疏人口统计学覆盖的数据填补方法。

**Result:** DEM-MoE在各个人口统计组中表现一致，并在注释者意见分歧较大的数据集上表现出特别好的效果。合成注释与人类注释有中等程度的对齐，并为扩充训练数据提供了可扩展的方式。

**Conclusion:** 这些贡献一起提高了对多样化观点的表示。并且，不同数据集结构的最佳混合真实和合成数据策略有所不同。

**Abstract:** We present an approach to modeling annotator disagreement in subjective NLP
tasks through both architectural and data-centric innovations. Our model,
DEM-MoE (Demographic-Aware Mixture of Experts), routes inputs to expert
subnetworks based on annotator demographics, enabling it to better represent
structured, group-level variation compared to prior models. DEM-MoE
consistently performs competitively across demographic groups, and shows
especially strong results on datasets with high annotator disagreement. To
address sparse demographic coverage, we test whether LLM-generated synthetic
annotations via zero-shot persona prompting can be used for data imputation. We
show these synthetic judgments align moderately well with human annotations on
our data and offer a scalable way to potentially enrich training data. We then
propose and evaluate approaches for blending real and synthetic data using
strategies tailored to dataset structure. We find that the optimal strategies
depend on dataset structure. Together, these contributions improve the
representation of diverse perspectives.

</details>


### [3] [Highlight & Summarize: RAG without the jailbreaks](https://arxiv.org/abs/2508.02872)
*Giovanni Cherubin,Andrew Paverd*

Main category: cs.CL

> 本论文提出了一种名为Highlight & Summarize (H&S) 的设计模式，用于增强检索增强生成系统的安全性，通过设计防止恶意用户通过精心设计的提示来让大语言模型生成不期望的内容或偏离其主要任务。此设计确保用户的问题永远不会暴露给生成模型。

<details>
  <summary>Details</summary>

**Motivation:** 防止大规模语言模型被破解和遭受模型劫持是一项重要却充满挑战的任务。现有的缓解措施容易被绕过，因此作者提出了新的设计模式旨在通过设计来防止这些攻击。

**Method:** 论文提出的方法是将系统分为两个组件：高亮组件和摘要组件，前者针对用户问题从检索文档中提取相关段落，后者则将提取的段落总结成一个连贯的答案。这种方法只需标准RAG系统来提供基于相关来源的自然语言答案，但不会暴露用户的问题。

**Result:** 作者评估了H&S模式的多个实例，结果表明大多数H&S的回应被评估为比标准RAG管道的回应更优，尤其是当高亮组件使用LLM时表现尤为出色。

**Conclusion:** 此设计模式通过防止直接向生成模型暴露用户的问题线索，提供了对于防止L层模型遭受恶意利用和模型调制的新方法。

**Abstract:** Preventing jailbreaking and model hijacking of Large Language Models (LLMs)
is an important yet challenging task. For example, when interacting with a
chatbot, malicious users can input specially crafted prompts to cause the LLM
to generate undesirable content or perform a completely different task from its
intended purpose. Existing mitigations for such attacks typically rely on
hardening the LLM's system prompt or using a content classifier trained to
detect undesirable content or off-topic conversations. However, these
probabilistic approaches are relatively easy to bypass due to the very large
space of possible inputs and undesirable outputs. In this paper, we present and
evaluate Highlight & Summarize (H&S), a new design pattern for
retrieval-augmented generation (RAG) systems that prevents these attacks by
design. The core idea is to perform the same task as a standard RAG pipeline
(i.e., to provide natural language answers to questions, based on relevant
sources) without ever revealing the user's question to the generative LLM. This
is achieved by splitting the pipeline into two components: a highlighter, which
takes the user's question and extracts relevant passages ("highlights") from
the retrieved documents, and a summarizer, which takes the highlighted passages
and summarizes them into a cohesive answer. We describe several possible
instantiations of H&S and evaluate their generated responses in terms of
correctness, relevance, and response quality. Surprisingly, when using an
LLM-based highlighter, the majority of H&S responses are judged to be better
than those of a standard RAG pipeline.

</details>


### [4] [Merge-based syntax is mediated by distinct neurocognitive mechanisms: A clustering analysis of comprehension abilities in 84,000 individuals with language deficits across nine languages](https://arxiv.org/abs/2508.02885)
*Elliot Murphy,Rohan Venkatesh,Edward Khokhlovich,Andrey Vyshedskiy*

Main category: cs.CL

> 本研究通过参与者对复杂句子的理解分析了三种不同结构类型的神经认知机制，提示这些结构可能分期出现并受不同机制支持。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探讨Merge作为基本操作在神经认知层面是如何工作的，并考察不同类型的Merge操作是否由不同的认知机制支持。

**Method:** 本研究通过系统调查参与者对不同语法复杂程度句子的理解，利用聚类分析来揭示行为证据，分为三种不同的结构类型。

**Result:** 聚类分析揭示了三种不同的结构类型，这些类型可能在不同的发展阶段出现，并且可能受到选择性损伤的影响。

**Conclusion:** 尽管基于Merge的语法可能在进化中迅速出现，但不同类型的Merge结构可能在发展阶段中分期出现，并且受不同认知机制的影响。

**Abstract:** In the modern language sciences, the core computational operation of syntax,
'Merge', is defined as an operation that combines two linguistic units (e.g.,
'brown', 'cat') to form a categorized structure ('brown cat', a Noun Phrase).
This can then be further combined with additional linguistic units based on
this categorial information, respecting non-associativity such that abstract
grouping is respected. Some linguists have embraced the view that Merge is an
elementary, indivisible operation that emerged in a single evolutionary step.
From a neurocognitive standpoint, different mental objects constructed by Merge
may be supported by distinct mechanisms: (1) simple command constructions
(e.g., "eat apples"); (2) the merging of adjectives and nouns ("red boat"); and
(3) the merging of nouns with spatial prepositions ("laptop behind the sofa").
Here, we systematically investigate participants' comprehension of sentences
with increasing levels of syntactic complexity. Clustering analyses revealed
behavioral evidence for three distinct structural types, which we discuss as
potentially emerging at different developmental stages and subject to selective
impairment. While a Merge-based syntax may still have emerged suddenly in
evolutionary time, responsible for the structured symbolic turn our species
took, different cognitive mechanisms seem to underwrite the processing of
various types of Merge-based objects.

</details>


### [5] [Coherent Multimodal Reasoning with Iterative Self-Evaluation for Vision-Language Models](https://arxiv.org/abs/2508.02886)
*Wenjie Luo,Ruocheng Li,Shanshan Zhu,Julian Perry*

Main category: cs.CL

> 研究通过提出一种全新的连贯多模态推理框架（CMRF），旨在解决现有语言和视觉-语言模型在复杂推理任务中表现不佳的问题。该框架结合了多种不同的模块和迭代自我评估的策略，实现了先进的基准测试性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有的大语言模型（LLMs）和视觉-语言模型（LVLMs）在处理复杂多步骤、跨模态常识推理任务时表现不佳，常常缺乏“深思熟虑”的能力，倾向于依靠浅层关联而不是深层链式推理，特别是将视觉信息与抽象概念融合时。因此，研究提出的目的是增强这些模型的推理能力。

**Method:** 研究提出了一种名为连贯多模态推理框架（CMRF）的新方法，该方法通过迭代自我评估推理机制来增强视觉-语言模型的常识推理能力。CMRF 模仿人类解决问题的方式，将复杂查询分解成子问题，生成逐步推理，并自我纠正错误。该框架集成了三个关键模块：推理分解单元（RDU）、上下文推理引擎（CIE）和连贯性评估模块（CAM），并结合自适应迭代细化策略，系统地改进其推理路径。

**Result:** 基于 LLaVA-1.6-34B，并在新的多模态日常活动推理（MDAR）数据集上训练的 CMRF，在 VCR、A-OKVQA 和 DailyLife-MRC 等具有挑战性的基准测试中取得了最先进的性能。其平均准确率为 69.4%，超过最佳开源基线 +2.4 个百分点，在复杂的推理情景中表现出特别强的性能。

**Conclusion:** 研究表明，CMRF 通过集成的推理模块和迭代自我评估机制有效提高了视觉-语言模型的复杂多模态推理能力，并在多个基准测试中表现出优于其他开源基线的性能。

**Abstract:** Despite significant advancements, current large language models (LLMs) and
vision-language models (LVLMs) continue to struggle with complex, multi-step,
cross-modal common sense reasoning tasks, often exhibiting a lack of
"deliberative thinking." They tend to rely on superficial associations rather
than deep, chained inference, particularly when integrating visual information
with abstract concepts. To address this, we propose the Coherent Multimodal
Reasoning Framework (CMRF), a novel approach that enhances LVLMs' common sense
reasoning capabilities through an iterative, self-evaluating inference
mechanism. CMRF mimics human problem-solving by decomposing complex queries,
generating step-by-step inferences, and self-correcting errors. Our framework
integrates three key modules: a Reasoning Decomposition Unit (RDU) for breaking
down problems into sub-questions, a Contextual Inference Engine (CIE) for
contextual inference, and a Coherence Assessment Module (CAM) for evaluating
logical consistency and confidence. Coupled with an Adaptive Iterative
Refinement strategy, CMRF systematically refines its reasoning paths. Built
upon LLaVA-1.6-34B and trained on a novel Multimodal Daily Activity Reasoning
(MDAR) dataset, CMRF achieves state-of-the-art performance among open-source
LVLMs on challenging benchmarks like VCR, A-OKVQA, and DailyLife-MRC. It
attains an average accuracy of 69.4%, surpassing the best open-source baseline
by +2.4 percentage points, with particular strength in complex reasoning
scenarios. Extensive ablation studies and human evaluations confirm the
critical contributions of each module and the effectiveness of iterative
refinement in fostering more coherent and accurate reasoning.

</details>


### [6] [SLIM-LLMs: Modeling of Style-Sensory Language RelationshipsThrough Low-Dimensional Representations](https://arxiv.org/abs/2508.02901)
*Osama Khalid,Sanvesh Srivastava,Padmini Srinivasan*

Main category: cs.CL

> 研究采用R4方法探索感官语言与风格特征关系，开发SLIM-LLMs，证明了利用低维度LIWC特征预测感官语言的有效性，且降低了80%的参数量。

<details>
  <summary>Details</summary>

**Motivation:** 研究感官语言如何影响我们的交流体验与感知，并评估在降低参数量的同时保持高性能的语言模型的有效性。

**Method:** 使用了降低秩的岭回归（R4）方法来探索感官语言与传统风格特征之间的关系，并引入了SLIM-LLMs（简约可解释模型）来模拟这些风格维度之间的非线性关系。

**Result:** 低维潜在表示（r = 24）的LIWC特征在预测感官语言方面比全特征集（r = 74）更有效地捕捉风格信息，且SLIM-LLMs在降低参数量达80%的同时性能未减。

**Conclusion:** 证明了简约的风格测定模型可以在多种文体中有效模拟感官语言，同时减少模型参数量。

**Abstract:** Sensorial language -- the language connected to our senses including vision,
sound, touch, taste, smell, and interoception, plays a fundamental role in how
we communicate experiences and perceptions. We explore the relationship between
sensorial language and traditional stylistic features, like those measured by
LIWC, using a novel Reduced-Rank Ridge Regression (R4) approach. We demonstrate
that low-dimensional latent representations of LIWC features r = 24 effectively
capture stylistic information for sensorial language prediction compared to the
full feature set (r = 74). We introduce Stylometrically Lean Interpretable
Models (SLIM-LLMs), which model non-linear relationships between these style
dimensions. Evaluated across five genres, SLIM-LLMs with low-rank LIWC features
match the performance of full-scale language models while reducing parameters
by up to 80%.

</details>


### [7] [Can LLMs Generate High-Quality Task-Specific Conversations?](https://arxiv.org/abs/2508.02931)
*Shengqi Li,Amarnath Gupta*

Main category: cs.CL

> 论文提出了一种参数化框架，以精确控制大型语言模型的对话质量，并展示了这一方法的有效性和广泛应用前景。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在解决对话生成中的挑战，包括主题连贯性、知识进程、人物一致性以及控制粒度。通过参数化的方法，可以更好地控制对话的质量。

**Method:** 此论文介绍了一种用于控制大型语言模型对话质量的参数化框架。该框架探索了六个维度上的九个关键参数，以精确指定对话属性。

**Result:** 通过与最先进的LLMs的实验，研究证明，基于参数的控制能够产生对话属性的统计显著差异。

**Conclusion:** 该框架为对话质量控制提供了一种标准化的方法，具有教育、治疗、客户服务和娱乐等多个应用场景。未来研究将集中在通过架构修改实现更多参数，并开发基准数据集进行评估。

**Abstract:** This paper introduces a parameterization framework for controlling
conversation quality in large language models. We explore nine key parameters
across six dimensions that enable precise specification of dialogue properties.
Through experiments with state-of-the-art LLMs, we demonstrate that
parameter-based control produces statistically significant differences in
generated conversation properties. Our approach addresses challenges in
conversation generation, including topic coherence, knowledge progression,
character consistency, and control granularity. The framework provides a
standardized method for conversation quality control with applications in
education, therapy, customer service, and entertainment. Future work will focus
on implementing additional parameters through architectural modifications and
developing benchmark datasets for evaluation.

</details>


### [8] [CoCoTen: Detecting Adversarial Inputs to Large Language Models through Latent Space Features of Contextual Co-occurrence Tensors](https://arxiv.org/abs/2508.02997)
*Sri Durga Sai Sowmya Kadali,Evangelos E. Papalexakis*

Main category: cs.CL

> The paper introduces a novel method for the detection of adversarial and jailbreak prompts in large language models using Contextual Co-occurrence Matrices and Tensors, achieving a high F1 score with minimal labeled data and faster computation time compared to baseline models.

<details>
  <summary>Details</summary>

**Motivation:** To enhance the security and reliability of large language models by developing a robust method for detecting harmful or adversarial prompts, especially in the context of scarce labeled data.

**Method:** Structure

**Result:** The proposed method achieved a notable F1 score of 0.83 using only 0.5% of labeled prompts, showing a 96.6% improvement over baseline methods in terms of accuracy and speedup of computations ranging from 2.3 to 128.4 times.

**Conclusion:** The paper demonstrates that Contextual Co-occurrence Matrices and Tensors can effectively identify adversarial prompts with minimal labeled data and enhanced speed, providing a significant advancement in the security of large language models.

**Abstract:** The widespread use of Large Language Models (LLMs) in many applications marks
a significant advance in research and practice. However, their complexity and
hard-to-understand nature make them vulnerable to attacks, especially
jailbreaks designed to produce harmful responses. To counter these threats,
developing strong detection methods is essential for the safe and reliable use
of LLMs. This paper studies this detection problem using the Contextual
Co-occurrence Matrix, a structure recognized for its efficacy in data-scarce
environments. We propose a novel method leveraging the latent space
characteristics of Contextual Co-occurrence Matrices and Tensors for the
effective identification of adversarial and jailbreak prompts. Our evaluations
show that this approach achieves a notable F1 score of 0.83 using only 0.5% of
labeled prompts, which is a 96.6% improvement over baselines. This result
highlights the strength of our learned patterns, especially when labeled data
is scarce. Our method is also significantly faster, speedup ranging from 2.3 to
128.4 times compared to the baseline models. To support future research and
reproducibility, we have made our implementation publicly available.

</details>


### [9] [When Algorithms Meet Artists: Topic Modeling the AI-Art Debate, 2013-2025](https://arxiv.org/abs/2508.03037)
*Ariya Mukherjee-Gandhi,Oliver Muellerklein*

Main category: cs.CL

> 研究通过分析关于AI生成艺术的多年英语讨论，揭示了艺术家与公众叙述之间的差异，强调了对艺术家观点更深层次交流的必要。

<details>
  <summary>Details</summary>

**Motivation:** 探讨人工智能生成艺术引发的同意、透明度和创意劳动未来等紧迫问题，并关注在这个话题中常常被边缘化的艺术家的声音。

**Method:** 使用BERTopic方法分析了2013年至2025年间的439个英文文本摘录，内容涉及意见文章、新闻报道、博客、法律文件和演讲记录。

**Result:** 识别出五个稳定的主题群集，并揭示了艺术家视角与主流媒介叙事之间的错位。

**Conclusion:** 强调技术术语作为一种微妙的守门机制，往往忽视艺术家认为最紧迫的问题。呼吁在不断演化的AI创意领域，通过透明驱动的参与，更深入地了解艺术家的观点。

**Abstract:** As generative AI continues to reshape artistic production and alternate modes
of human expression, artists whose livelihoods are most directly affected have
raised urgent concerns about consent, transparency, and the future of creative
labor. However, the voices of artists are often marginalized in dominant public
and scholarly discourse. This study presents a twelve-year analysis, from 2013
to 2025, of English-language discourse surrounding AI-generated art. It draws
from 439 curated 500-word excerpts sampled from opinion articles, news reports,
blogs, legal filings, and spoken-word transcripts. Through a reproducible
methodology, we identify five stable thematic clusters and uncover a
misalignment between artists' perceptions and prevailing media narratives. Our
findings highlight how the use of technical jargon can function as a subtle
form of gatekeeping, often sidelining the very issues artists deem most urgent.
Our work provides a BERTopic-based methodology and a multimodal baseline for
future research, alongside a clear call for deeper, transparency-driven
engagement with artist perspectives in the evolving AI-creative landscape.

</details>


### [10] [Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language Models in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.03098)
*Haoran Wang,Xiongxiao Xu,Baixiang Huang,Kai Shu*

Main category: cs.CL

> 本文提出了一种轻量、推理时的防御策略 Privacy-Aware Decoding (PAD)，在生成私密信息时通过向 token 概率分布中注入噪音来保护隐私，降低了私密信息泄露的风险，同时保持了生成质量。

<details>
  <summary>Details</summary>

**Motivation:** 该论文旨在解决 Retrieval-Augmented Generation (RAG) 在处理私密或敏感数据时，容易发生通过生成的响应泄露私密信息的问题。

**Method:** Privacy-Aware Decoding (PAD) 方法通过在生成过程中向 token 的概率分布中注入校准的高斯噪声，以防御提取攻击。PAD 包含基于信心的筛选来保护高风险 token，有效估算敏感性以最小化不必要的噪声，以及基于上下文的噪声校准来平衡隐私和生成质量。

**Result:** 实验结果显示，PAD 能够显著减少私密信息的泄露，同时保持响应的效用，优于现有的基于检索和后期处理的防御方法。

**Conclusion:** 通过解码策略缓解隐私风险，PAD 为敏感领域提供了通用和可扩展的隐私解决方案的途径。

**Abstract:** Retrieval-Augmented Generation (RAG) enhances the factual accuracy of large
language models (LLMs) by conditioning outputs on external knowledge sources.
However, when retrieval involves private or sensitive data, RAG systems are
susceptible to extraction attacks that can leak confidential information
through generated responses. We propose Privacy-Aware Decoding (PAD), a
lightweight, inference-time defense that adaptively injects calibrated Gaussian
noise into token logits during generation. PAD integrates confidence-based
screening to selectively protect high-risk tokens, efficient sensitivity
estimation to minimize unnecessary noise, and context-aware noise calibration
to balance privacy with generation quality. A \renyi Differential Privacy (RDP)
accountant rigorously tracks cumulative privacy loss, enabling explicit
per-response $(\varepsilon, \delta)$-DP guarantees for sensitive outputs.
Unlike prior approaches requiring retraining or corpus-level filtering, PAD is
model-agnostic and operates entirely at decoding time with minimal
computational overhead. Experiments on three real-world datasets demonstrate
that PAD substantially reduces private information leakage while preserving
response utility, outperforming existing retrieval- and post-processing-based
defenses. Our work takes an important step toward mitigating privacy risks in
RAG via decoding strategies, paving the way for universal and scalable privacy
solutions in sensitive domains. Our code is available:
https://github.com/wang2226/PAD.

</details>


### [11] [Token-Level Precise Attack on RAG: Searching for the Best Alternatives to Mislead Generation](https://arxiv.org/abs/2508.03110)
*Zizhong Li,Haopeng Zhang,Jiawei Zhang*

Main category: cs.CL

> 文章提出了一个新的攻击框架TPARAG，专门针对RAG系统中的检索和生成阶段进行攻击，以解决现有方法依赖检索器或不能同步考虑两阶段的问题，并展示了其有效性和RAG系统的潜在安全性问题。

<details>
  <summary>Details</summary>

**Motivation:** 尽管之前的工作探讨了对RAG系统的攻击，但这些方法要么严重依赖于对检索器的访问，要么无法同时考虑检索和生成阶段，这在黑盒场景中尤其限制了它们的有效性。为了解决这些限制，提出了TPARAG。

**Method:** 提出了一种名为TPARAG的新框架，该框架利用轻量级白盒LLM作为攻击者，生成并迭代优化恶意文本，在检索和生成阶段都实现高成功率的攻击。

**Result:** 实验结果表明，TPARAG在检索阶段和端到端攻击效果上都优于之前的方法，揭示了RAG流水线中的关键漏洞，并为提高其鲁棒性提供了新的见解。

**Conclusion:** TPARAG框架能够有效针对白盒和黑盒RAG系统，展示了一种更为精确和高效的攻击机制，这对理解和增强RAG系统的安全性具有重要意义。

**Abstract:** While large language models (LLMs) have achieved remarkable success in
providing trustworthy responses for knowledge-intensive tasks, they still face
critical limitations such as hallucinations and outdated knowledge. To address
these issues, the retrieval-augmented generation (RAG) framework enhances LLMs
with access to external knowledge via a retriever, enabling more accurate and
real-time outputs about the latest events. However, this integration brings new
security vulnerabilities: the risk that malicious content in the external
database can be retrieved and used to manipulate model outputs. Although prior
work has explored attacks on RAG systems, existing approaches either rely
heavily on access to the retriever or fail to jointly consider both retrieval
and generation stages, limiting their effectiveness, particularly in black-box
scenarios. To overcome these limitations, we propose Token-level Precise Attack
on the RAG (TPARAG), a novel framework that targets both white-box and
black-box RAG systems. TPARAG leverages a lightweight white-box LLM as an
attacker to generate and iteratively optimize malicious passages at the token
level, ensuring both retrievability and high attack success in generation.
Extensive experiments on open-domain QA datasets demonstrate that TPARAG
consistently outperforms previous approaches in retrieval-stage and end-to-end
attack effectiveness. These results further reveal critical vulnerabilities in
RAG pipelines and offer new insights into improving their robustness.

</details>


### [12] [Cross-lingual Opinions and Emotions Mining in Comparable Documents](https://arxiv.org/abs/2508.03112)
*Motaz Saad,David Langlois,Kamel Smaili*

Main category: cs.CL

> 该研究通过跨语言的情感和情绪标注方法，对比了英语-阿拉伯语新闻中对同一主题的不同讨论方式和情绪差异，方法具有语言独立性和通用性。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在探讨跨语言的比较文本在情感和情绪表达上的差异，这些文本虽然在主题上一致，但并非直接翻译。这有助于理解同一主题在不同语言中的讨论方式。

**Method:** 该研究首先对文本进行情感和情绪标签的标注。采用了一种跨语言的方法来对文件进行主观/客观的意见分类，而不依赖机器翻译。为了进行情绪标注（包括愤怒、厌恶、恐惧、喜悦、悲伤、惊讶），手动将英语的WordNet情感词典（WNA）翻译成阿拉伯语词典，创建了一个多语言的情感词典用于标签标注。接着，使用统计方法来评估每一对源-目标文档在情感和情绪上的一致性。

**Result:** 研究结果表明，当文章来自同一新闻机构时，情感和情绪标签之间呈现出一致性；反之，如果文章来自不同的机构，则会出现分歧。

**Conclusion:** 该方法是语言独立的，可以推广到其他语言对。

**Abstract:** Comparable texts are topic-aligned documents in multiple languages that are
not direct translations. They are valuable for understanding how a topic is
discussed across languages. This research studies differences in sentiments and
emotions across English-Arabic comparable documents. First, texts are annotated
with sentiment and emotion labels. We apply a cross-lingual method to label
documents with opinion classes (subjective/objective), avoiding reliance on
machine translation. To annotate with emotions (anger, disgust, fear, joy,
sadness, surprise), we manually translate the English WordNet-Affect (WNA)
lexicon into Arabic, creating bilingual emotion lexicons used to label the
comparable corpora. We then apply a statistical measure to assess the agreement
of sentiments and emotions in each source-target document pair. This comparison
is especially relevant when the documents originate from different sources. To
our knowledge, this aspect has not been explored in prior literature. Our study
includes English-Arabic document pairs from Euronews, BBC, and Al-Jazeera
(JSC). Results show that sentiment and emotion annotations align when articles
come from the same news agency and diverge when they come from different ones.
The proposed method is language-independent and generalizable to other language
pairs.

</details>


### [13] [Long Story Generation via Knowledge Graph and Literary Theory](https://arxiv.org/abs/2508.03137)
*Ge Shi,Kaiyu Huang,Guochen Feng*

Main category: cs.CL

> 本文提出了一种基于多智能体的故事生成结构，改进了现有的多阶段方法，使用大型语言模型作为核心组件，引入了记忆存储模型和故事情节障碍框架，以防止主题漂移并增加故事的魅力。实验表明，该方法能生成质量更高的长故事。

<details>
  <summary>Details</summary>

**Motivation:** 先前的研究通过基于概要的生成方法，使用多阶段方法将概要转化为故事，但是这种方法存在主题漂移和故事情节不连贯的现象，这样的故事对人类读者的吸引力较低。因此，提出了改进的多阶段故事生成方法。

**Method:** 通过多智能体故事生成结构来改善多阶段方法，采用大型语言模型作为代理的核心组件。为了防止主题漂移，介绍了包含两个组件的记忆存储模型：长期记忆存储，识别最重要的记忆，从而防止主题漂移；短期记忆存储，保留在每一轮生成中的最新概要。为了将引人入胜的元素融入故事中，基于文学叙事理论设计了一个故事情节障碍框架，引入不确定因素和评估标准来生成概要。此外，建立了多智能体交互阶段，通过对话模拟作者与读者的互动，根据反馈修订故事文本，以确保其保持一致性和逻辑性。

**Result:** 实验结果表明，与先前的方法相比，本文提出的方法能够生成质量更高的长故事。

**Conclusion:** 本文所提出的方法通过引入记忆模块和故事情节障碍框架，有效防止了主题漂移，并增强故事的连贯性和逻辑性，从而生成质量更高的长故事。

**Abstract:** The generation of a long story consisting of several thousand words is a
sub-task in the field of long text generation~(LTG). Previous research has
addressed this challenge through outline-based generation, which employs a
multi-stage method for generating outlines into stories. However, this approach
suffers from two common issues: almost inevitable theme drift caused by the
loss of memory of previous outlines, and tedious plots with incoherent logic
that are less appealing to human readers.
  In this paper, we propose the multi-agent Story Generator structure to
improve the multi-stage method, using large language models~(LLMs) as the core
components of agents. To avoid theme drift, we introduce a memory storage model
comprising two components: a long-term memory storage that identifies the most
important memories, thereby preventing theme drift; and a short-term memory
storage that retains the latest outlines from each generation round. To
incorporate engaging elements into the story, we design a story theme obstacle
framework based on literary narratology theory that introduces uncertain
factors and evaluation criteria to generate outline. This framework calculates
the similarity of the former storyline and enhances the appeal of the story by
building a knowledge graph and integrating new node content. Additionally, we
establish a multi-agent interaction stage to simulate writer-reader interaction
through dialogue and revise the story text according to feedback, to ensure it
remains consistent and logical. Evaluations against previous methods
demonstrate that our approach can generate higher-quality long stories.

</details>


### [14] [RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior](https://arxiv.org/abs/2508.03140)
*Junyao Yang,Jianwei Wang,Huiping Zhuang,Cen Chen,Ziqian Zeng*

Main category: cs.CL

> This paper introduces RCP-Merging, a method for effectively merging long CoT models with domain-specific models, which improves task performance in specific domains without degrading reasoning capabilities.

<details>
  <summary>Details</summary>

**Motivation:** To create a dual-capability model with long CoT capability and domain-specific knowledge without substantial computational and data costs, overcoming the challenges of reasoning capability degradation during the merging process.

**Method:** RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior, a novel merging framework designed to integrate domain-specific LLMs with long CoT capability, maintaining model performance in the original domain.

**Result:** The results show that RCP-Merging successfully merges a reasoning model with domain-specific ones, improving domain task performance by 9.5% and 9.2% over state-of-the-art methods, without significantly harming the original long CoT reasoning capability.

**Conclusion:** RCP-Merging is an effective method to merge reasoning models with domain-specific models while maintaining the original model's reasoning capabilities, leading to improved performance in domain tasks.

**Abstract:** Large Language Models (LLMs) with long chain-of-thought (CoT) capability,
termed Reasoning Models, demonstrate superior intricate problem-solving
abilities through multi-step long CoT reasoning. To create a dual-capability
model with long CoT capability and domain-specific knowledge without
substantial computational and data costs, model merging emerges as a highly
resource-efficient method. However, significant challenges lie in merging
domain-specific LLMs with long CoT ones since nowadays merging methods suffer
from reasoning capability degradation, even gibberish output and output
collapse. To overcome this, we introduce RCP-Merging: Merging Long
Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning
Capability as Prior, a novel merging framework designed to integrate
domain-specific LLMs with long CoT capability, meanwhile maintaining model
performance in the original domain. Treating reasoning model weights as
foundational prior, our method utilizes a reasoning capability indicator to
preserve core long CoT capability model weights while selectively merging
essential domain-specific weights. We conducted extensive experiments on
Qwen2.5-7B, Llama3.1-8B, and Qwen2.5-1.5B models in BioMedicine and Finance
domains. Our results show that RCP-Merging successfully merges a reasoning
model with domain-specific ones, improving domain task performance by 9.5% and
9.2% over state-of-the-art methods, without significantly harming the original
long CoT reasoning capability.

</details>


### [15] [Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following](https://arxiv.org/abs/2508.03178)
*Chenyang Wang,Liang Wen,Shousheng Jia,Xiangzheng Zhang,Liang Xu*

Main category: cs.CL

> 研究指出惰性推理是导致模型在处理复杂指令时表现不稳定的主要原因，并提出了一套框架解决这个问题，该框架通过一系列措施改善了模型的推理机制和指令遵循性，实验结果表明性能显著提升。

<details>
  <summary>Details</summary>

**Motivation:** 发现大型语言模型在处理复杂指令时表现不稳定，推理步骤中的惰性推理导致了指令遵守性差。

**Method:** 提出一个全面的框架，以促进严格的推理过程，其中包括预览和自我检查，这有助于满足严格的指令约束条件。该框架包括生成具有复杂约束的指令，并通过过滤过程获得有效的提示，在此基础上形成三个不同的数据集，再通过拒绝采样技术选取高质量的提示数据集进行模型的冷启动初始化及适应有效的推理模式。此外，还采用了保持熵的监督微调(Entropy-SFT)策略及基于规则的密集奖励引导的令牌自适应熵强化学习(TEA-RL)。

**Result:** 对指令遵循基准进行的广泛实验表明了显著的性能改进，特别是在各种模型规模上均有改进，其Light-IF-32B模型在性能上超越了DeepSeek-R1和Doubao-1.6。

**Conclusion:** 实验结果表明，借助这一方法，模型的推理机制将得到改善，能够更好地遵循指令。实验在各种指令遵循基准上展示了显著的性能提升，Light-IF-32B模型在性能上超越了多个大型开源模型和封闭源代码模型。

**Abstract:** While advancements in the reasoning abilities of LLMs have significantly
enhanced their performance in solving mathematical problems, coding tasks, and
general puzzles, their effectiveness in accurately adhering to instructions
remains inconsistent, particularly with more complex directives. Our
investigation identifies lazy reasoning during the thinking stage as the
primary factor contributing to poor instruction adherence. To mitigate this
issue, we propose a comprehensive framework designed to enable rigorous
reasoning processes involving preview and self-checking, essential for
satisfying strict instruction constraints. Specifically, we first generate
instructions with complex constraints and apply a filtering process to obtain
valid prompts, resulting in three distinct prompt datasets categorized as hard,
easy, and pass. Then, we employ rejection sampling on the pass prompts to
curate a small yet high-quality dataset, enabling a cold-start initialization
of the model and facilitating its adaptation to effective reasoning patterns.
Subsequently, we employ an entropy-preserving supervised fine-tuning
(Entropy-SFT) strategy coupled with token-wise entropy-adaptive (TEA-RL)
reinforcement learning guided by rule-based dense rewards. This approach
encourages the model to transform its reasoning mechanism, ultimately fostering
generalizable reasoning abilities that encompass preview and self-checking.
Extensive experiments conducted on instruction-following benchmarks demonstrate
remarkable performance improvements across various model scales. Notably, our
Light-IF-32B model surpasses both larger open-source models such as DeepSeek-R1
and closed-source models like Doubao-1.6.

</details>


### [16] [Analyzing German Parliamentary Speeches: A Machine Learning Approach for Topic and Sentiment Classification](https://arxiv.org/abs/2508.03181)
*Lukas Pätz,Moritz Beyer,Jannik Späth,Lasse Bohlen,Patrick Zschech,Mathias Kraus,Julian Rosenberger*

Main category: cs.CL

> 本研究使用机器学习模型分析德意志联邦议会的约28000次演讲，模型显示出很强的分类性能，并揭示了政党角色变化时的话语风格显著关系。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是探索德国联邦议院（Bundestag）中政党之间的主题趋势和情绪分布，特别是揭示不同政党在政府和反对派角色之间变动时的风格变化。

**Method:** 本研究通过分析过去五年中大约28000次德国联邦议院的演讲来研究政治话语。开发了两个用于主题和情绪分类的机器学习模型，并在一个手动标注的数据集上训练了这些模型。

**Result:** 开发的模型在主题分类上达到了平均AUROC为0.94，在情绪分类上达到了0.89，表现优异。分析表明，政党的意识形态立场确实重要，但执政责任同样影响其话语风格。

**Conclusion:** 本研究直接解决有关德意志联邦议会中的主题变迁、情绪动态以及政党特定话语策略的关键问题。

**Abstract:** This study investigates political discourse in the German parliament, the
Bundestag, by analyzing approximately 28,000 parliamentary speeches from the
last five years. Two machine learning models for topic and sentiment
classification were developed and trained on a manually labeled dataset. The
models showed strong classification performance, achieving an area under the
receiver operating characteristic curve (AUROC) of 0.94 for topic
classification (average across topics) and 0.89 for sentiment classification.
Both models were applied to assess topic trends and sentiment distributions
across political parties and over time. The analysis reveals remarkable
relationships between parties and their role in parliament. In particular, a
change in style can be observed for parties moving from government to
opposition. While ideological positions matter, governing responsibilities also
shape discourse. The analysis directly addresses key questions about the
evolution of topics, sentiment dynamics, and party-specific discourse
strategies in the Bundestag.

</details>


### [17] [Beyond Content: How Grammatical Gender Shapes Visual Representation in Text-to-Image Models](https://arxiv.org/abs/2508.03199)
*Muhammed Saeed,Shaina Raza,Ashmal Vayani,Muhammad Abdul-Mageed,Ali Emami,Shady Shehata*

Main category: cs.CL

> 研究发现了语法性别对AI生成图像中性别表示的显著影响，显示了理解多语言和多模态系统公平性问题的新角度。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探索语法性别如何影响视觉表现，特别是在不同语言中表现出的性格局部代表情况，填补了现有研究的空白。

**Method:** 采用了包含800个独特提示词的数据集，涵盖了五种有性别的语言和两种无性别的控制语言，分析了三种最先进的文本生成图像模型生成的图像。

**Result:** 该研究揭示了在多语言和多模态系统中，语法性别对AI生成的视觉输出有显著影响，提出了理解系统偏差和公平性的新维度。研究采用跨语言基准测试，跨越五种有性别区分的语言和两种没有性别区分的语言，分析了三种最先进文本到图像模型生成的28,800张图像，展示了语法性别对生成图像中性别表示的显著影响。

**Conclusion:** 研究结论指出语言结构本身会塑造AI生成的视觉输出，这为理解和解决这些系统中的偏差和公平性问题提供了新的研究维度。

**Abstract:** Research on bias in Text-to-Image (T2I) models has primarily focused on
demographic representation and stereotypical attributes, overlooking a
fundamental question: how does grammatical gender influence visual
representation across languages? We introduce a cross-linguistic benchmark
examining words where grammatical gender contradicts stereotypical gender
associations (e.g., ``une sentinelle'' - grammatically feminine in French but
referring to the stereotypically masculine concept ``guard''). Our dataset
spans five gendered languages (French, Spanish, German, Italian, Russian) and
two gender-neutral control languages (English, Chinese), comprising 800 unique
prompts that generated 28,800 images across three state-of-the-art T2I models.
Our analysis reveals that grammatical gender dramatically influences image
generation: masculine grammatical markers increase male representation to 73\%
on average (compared to 22\% with gender-neutral English), while feminine
grammatical markers increase female representation to 38\% (compared to 28\% in
English). These effects vary systematically by language resource availability
and model architecture, with high-resource languages showing stronger effects.
Our findings establish that language structure itself, not just content, shapes
AI-generated visual outputs, introducing a new dimension for understanding bias
and fairness in multilingual, multimodal systems.

</details>


### [18] [Current State in Privacy-Preserving Text Preprocessing for Domain-Agnostic NLP](https://arxiv.org/abs/2508.03204)
*Abhirup Sinha,Pritilata Saha,Tithi Saha*

Main category: cs.CL

> 本文探讨了几种用于遮盖或伪匿名化文本数据中私人信息的预处理方法，特别聚焦于与领域无关的自然语言处理任务。

<details>
  <summary>Details</summary>

**Motivation:** 隐私是一项基本的人权。在需要大量数据来学习语言变化的现代大型语言模型中，保护数据隐私变得尤为重要，因为这些数据往往包含私人信息。研究表明，有可能从这些语言模型中提取私人信息。因此，匿名化这些私人和敏感信息至关重要。

**Method:** 探讨了几种在文本数据中遮盖或伪匿名化私人信息的预处理方法，特别关注于与领域无关的自然语言处理任务。

**Result:** 虽然完全匿名化可能无法实现，但本文报告了几种对与领域无关的自然语言处理任务中私人信息遮盖或伪匿名化的预处理方法。

**Conclusion:** 该研究强调了在数据处理阶段对私人和敏感信息进行保护的重要性，并建议使用特定的预处理方法来达到这一目的。

**Abstract:** Privacy is a fundamental human right. Data privacy is protected by different
regulations, such as GDPR. However, modern large language models require a huge
amount of data to learn linguistic variations, and the data often contains
private information. Research has shown that it is possible to extract private
information from such language models. Thus, anonymizing such private and
sensitive information is of utmost importance. While complete anonymization may
not be possible, a number of different pre-processing approaches exist for
masking or pseudonymizing private information in textual data. This report
focuses on a few of such approaches for domain-agnostic NLP tasks.

</details>


### [19] [Probing Syntax in Large Language Models: Successes and Remaining Challenges](https://arxiv.org/abs/2508.03211)
*Pablo J. Diego-Simón,Emmanuel Chemla,Jean-Rémi King,Yair Lakretz*

Main category: cs.CL

> 研究对语言模型中句法结构的表示开展了深入分析，揭示了结构探针的挑战，强调了句法表示的表层属性和语言属性的影响，同时也为结构探针提供了更好的评估基准。

<details>
  <summary>Details</summary>

**Motivation:** 研究团队发现，目前使用的结构探针是在非选择性的句子集上进行评估的，这使得系统因素是否影响这些句法表示尚不清楚。

**Method:** 采用结构探针在三个控制基准上进行深入分析，以探讨句法结构表示的问题。

**Result:** 1. 结构探针偏向于一个表面属性：句子中的两个词越接近，结构探针越可能认为它们在句法上是链接的。2. 结构探针受到语言属性的挑战：它们无法很好地表示深层句法结构，并且受到互相作用的名词或不合语法的动词形式的干扰。3. 结构探针似乎不受个别词语可预测性的影响。

**Conclusion:** 这项工作揭示了当前结构探针面临的挑战，并提供了一个由控制刺激构成的基准，以更好地评估它们的性能。

**Abstract:** The syntactic structures of sentences can be readily read-out from the
activations of large language models (LLMs). However, the ``structural probes''
that have been developed to reveal this phenomenon are typically evaluated on
an indiscriminate set of sentences. Consequently, it remains unclear whether
structural and/or statistical factors systematically affect these syntactic
representations. To address this issue, we conduct an in-depth analysis of
structural probes on three controlled benchmarks. Our results are three-fold.
First, structural probes are biased by a superficial property: the closer two
words are in a sentence, the more likely structural probes will consider them
as syntactically linked. Second, structural probes are challenged by linguistic
properties: they poorly represent deep syntactic structures, and get interfered
by interacting nouns or ungrammatical verb forms. Third, structural probes do
not appear to be affected by the predictability of individual words. Overall,
this work sheds light on the current challenges faced by structural probes.
Providing a benchmark made of controlled stimuli to better evaluate their
performance.

</details>


### [20] [CardiffNLP at CLEARS-2025: Prompting Large Language Models for Plain Language and Easy-to-Read Text Rewriting](https://arxiv.org/abs/2508.03240)
*Mutaz Ayesh,Nicolás Gutiérrez-Rolón,Fernando Alva-Manchego*

Main category: cs.CL

> CardiffNLP团队使用基于LLM提示的方法参与了西班牙语文本改编的共享任务，在两个子任务中分别获得了第三名和第二名。

<details>
  <summary>Details</summary>

**Motivation:** 参与由IberLEF 2025举办的西班牙语文本改编共享任务，该任务包含两个子任务。

**Method:** 采用了基于LLM提示的方法，尝试了不同的提示变化。最初使用了LLaMA-3.2，最终提交时使用了Gemma-3模型。

**Result:** 在子任务1中获得了第三名，在子任务2中获得第二名。

**Conclusion:** 详细的介绍了各种提示变化，示例及实验结果。

**Abstract:** This paper details the CardiffNLP team's contribution to the CLEARS shared
task on Spanish text adaptation, hosted by IberLEF 2025. The shared task
contained two subtasks and the team submitted to both. Our team took an
LLM-prompting approach with different prompt variations. While we initially
experimented with LLaMA-3.2, we adopted Gemma-3 for our final submission, and
landed third place in Subtask 1 and second place in Subtask 2. We detail our
numerous prompt variations, examples, and experimental results.

</details>


### [21] [Somatic in the East, Psychological in the West?: Investigating Clinically-Grounded Cross-Cultural Depression Symptom Expression in LLMs](https://arxiv.org/abs/2508.03247)
*Shintaro Sakai,Jisun An,Migyeong Kang,Haewoon Kwak*

Main category: cs.CL

> 研究发现，当以东方语言（中文、日语和印地语）提示时，LLMs 更能识别文化差异，但总体上仍未能很好地再现心理疾病症状的文化差异，揭示了这些模型缺乏文化感知能力。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于LLMs在心理健康领域的应用越来越普及，研究旨在测试这些模型是否能够识别并再现心理疾病症状的文化差异，即西方人倾向于报告心理症状，而东方人则报告身体症状。

**Method:** 通过向大型语言模型（LLMs）提供西方或东方的人物设定来测试这些模型是否会在心理健康领域重现这些文化模式。

**Result:** 实验结果显示，当以英语进行提示时，LLMs 大体上未能重现这些模式，然而在以主要的东方语言（即中文、日语和印地语）提示时，尽管在某些设置中有所改善，但仍未能完全对齐。

**Conclusion:** 研究揭示了，虽然提示语的语言很重要，但目前的通用 LLM 缺乏健壮的文化感知能力，这对于心理健康的运用是安全和有效的。

**Abstract:** Prior clinical psychology research shows that Western individuals with
depression tend to report psychological symptoms, while Eastern individuals
report somatic ones. We test whether Large Language Models (LLMs), which are
increasingly used in mental health, reproduce these cultural patterns by
prompting them with Western or Eastern personas. Results show that LLMs largely
fail to replicate the patterns when prompted in English, though prompting in
major Eastern languages (i.e., Chinese, Japanese, and Hindi) improves alignment
in several configurations. Our analysis pinpoints two key reasons for this
failure: the models' low sensitivity to cultural personas and a strong,
culturally invariant symptom hierarchy that overrides cultural cues. These
findings reveal that while prompt language is important, current
general-purpose LLMs lack the robust, culture-aware capabilities essential for
safe and effective mental health applications.

</details>


### [22] [RooseBERT: A New Deal For Political Language Modelling](https://arxiv.org/abs/2508.03250)
*Deborah Dore,Elena Cabrio,Serena Villata*

Main category: cs.CL

> RooseBERT, a specialized pre-trained Language Model for political discourse, shows improved performance over general-purpose models in analyzing political debates.

<details>
  <summary>Details</summary>

**Motivation:** The complexity and specificity of political language necessitate specialized computational methods to better analyze political debates and provide insights to citizens.

**Method:** We introduce RooseBERT, a pre-trained Language Model for political discourse, trained on large political debate and speech corpora. The model is fine-tuned for four downstream tasks including named entity recognition, sentiment analysis, argument component detection and classification, and argument relation prediction and classification.

**Result:** Fine-tuning RooseBERT on four downstream tasks related to political debate analysis showed significant improvements over general-purpose Language Models.

**Conclusion:** RooseBERT demonstrates significant advancements in the analysis of political debates, proving the value of domain-specific pre-training in enhancing model performance.

**Abstract:** The increasing amount of political debates and politics-related discussions
calls for the definition of novel computational methods to automatically
analyse such content with the final goal of lightening up political
deliberation to citizens. However, the specificity of the political language
and the argumentative form of these debates (employing hidden communication
strategies and leveraging implicit arguments) make this task very challenging,
even for current general-purpose pre-trained Language Models. To address this
issue, we introduce a novel pre-trained Language Model for political discourse
language called RooseBERT. Pre-training a language model on a specialised
domain presents different technical and linguistic challenges, requiring
extensive computational resources and large-scale data. RooseBERT has been
trained on large political debate and speech corpora (8K debates, each composed
of several sub-debates on different topics) in English. To evaluate its
performances, we fine-tuned it on four downstream tasks related to political
debate analysis, i.e., named entity recognition, sentiment analysis, argument
component detection and classification, and argument relation prediction and
classification. Our results demonstrate significant improvements over
general-purpose Language Models on these four tasks, highlighting how
domain-specific pre-training enhances performance in political debate analysis.
We release the RooseBERT language model for the research community.

</details>


### [23] [Exploring Stability-Plasticity Trade-offs for Continual Named Entity Recognition](https://arxiv.org/abs/2508.03259)
*Duzhen Zhang,Chenxing Li,Jiahua Dong,Qi Liu,Dong Yu*

Main category: cs.CL

> 本文提出了一种新的CNER方法，通过调整模型的表示和权重方面的稳定性和可塑性，解决了之前方法存在的过度稳定性和不足的学习能力问题，实验结果表明该方法优于以往的方法。

<details>
  <summary>Details</summary>

**Motivation:** 解决之前CNER方法中模型对旧知识过稳定性而对新知识适应性不足的问题，实现记忆与学习新知识之间的平衡。

**Method:** SPT方法通过调整表示和权重来平衡模型的记忆和学习新知识的能力。在表示层面，通过在原始知识蒸馏中引入池化操作来增加模型可塑性；在权重层面，通过动态合并老模型和新模型的权重来增强旧知识的同时保持新知识。此外，对于CNER特有的非实体类型语义漂移挑战，提出了基于置信度的伪标签方法来预测实体类型。

**Result:** 在三个基准数据集上进行的十种CNER设置的实验表明，SPT方法优于以往的CNER方法，证明了其在实现合适稳定性和可塑性之间的平衡方面的有效性。

**Conclusion:** 本文提出的方法成功地调整了模型的记忆和学习新知识的能力，提升了CNER性能。

**Abstract:** Continual Named Entity Recognition (CNER) is an evolving field that focuses
on sequentially updating an existing model to incorporate new entity types.
Previous CNER methods primarily utilize Knowledge Distillation (KD) to preserve
prior knowledge and overcome catastrophic forgetting, strictly ensuring that
the representations of old and new models remain consistent. Consequently, they
often impart the model with excessive stability (i.e., retention of old
knowledge) but limited plasticity (i.e., acquisition of new knowledge). To
address this issue, we propose a Stability-Plasticity Trade-off (SPT) method
for CNER that balances these aspects from both representation and weight
perspectives. From the representation perspective, we introduce a pooling
operation into the original KD, permitting a level of plasticity by
consolidating representation dimensions. From the weight perspective, we
dynamically merge the weights of old and new models, strengthening old
knowledge while maintaining new knowledge. During this fusion, we implement a
weight-guided selective mechanism to prioritize significant weights. Moreover,
we develop a confidence-based pseudo-labeling approach for the current
non-entity type, which predicts entity types using the old model to handle the
semantic shift of the non-entity type, a challenge specific to CNER that has
largely been ignored by previous methods. Extensive experiments across ten CNER
settings on three benchmark datasets demonstrate that our SPT method surpasses
previous CNER approaches, highlighting its effectiveness in achieving a
suitable stability-plasticity trade-off.

</details>


### [24] [Pay What LLM Wants: Can LLM Simulate Economics Experiment with 522 Real-human Persona?](https://arxiv.org/abs/2508.03262)
*Junhyuk Choi,Hyeonchu Park,Haemin Lee,Hyebeen Shin,Hyun Joung Jin,Bugeun Kim*

Main category: cs.CL

> 研究评估了大型语言模型在预测个体经济决策上的能力，发现虽然这些模型在个体水平的预测上困难，但在群体水平上表现出一定的行为倾向，并且常见的提示技术并不比简单的提示方法效果好。

<details>
  <summary>Details</summary>

**Motivation:** 大多数关于LLMs的研究依赖于虚构的人设，而不是实际的人类数据。此研究旨在通过利用真实的人设数据评估LLMs在经济决策模拟中的能力，并系统比较了三种最先进多模态LLMs。

**Method:** 研究使用了522个韩国参与者的详细人设信息，在文化消费场景下进行Pay-What-You-Want（PWYW）定价实验，调查了LLMs对个体人类选择的准确复制能力及人设注入方法对预测性能的影响。

**Result:** 结果表明虽然LLMs在个体层面的预测上存在困难，但在群体行为倾向上表现合理。同时，发现常用的提示技巧在效果上并没有比简单的提示方法有显著优势。

**Conclusion:** 研究为基于真实人类数据模拟经济行为的LLMs能力提供了首个全面评估，为计算社会科学中基于人设的模拟提供了实证指导。

**Abstract:** Recent advances in Large Language Models (LLMs) have generated significant
interest in their capacity to simulate human-like behaviors, yet most studies
rely on fictional personas rather than actual human data. We address this
limitation by evaluating LLMs' ability to predict individual economic
decision-making using Pay-What-You-Want (PWYW) pricing experiments with real
522 human personas. Our study systematically compares three state-of-the-art
multimodal LLMs using detailed persona information from 522 Korean participants
in cultural consumption scenarios. We investigate whether LLMs can accurately
replicate individual human choices and how persona injection methods affect
prediction performance. Results reveal that while LLMs struggle with precise
individual-level predictions, they demonstrate reasonable group-level
behavioral tendencies. Also, we found that commonly adopted prompting
techniques are not much better than naive prompting methods; reconstruction of
personal narrative nor retrieval augmented generation have no significant gain
against simple prompting method. We believe that these findings can provide the
first comprehensive evaluation of LLMs' capabilities on simulating economic
behavior using real human data, offering empirical guidance for persona-based
simulation in computational social science.

</details>


### [25] [LECTOR: LLM-Enhanced Concept-based Test-Oriented Repetition for Adaptive Spaced Learning](https://arxiv.org/abs/2508.03275)
*Jiahao Zhao*

Main category: cs.CL

> 本文提出了一种名为LECTOR的自适应学习算法，该算法在语言学习测试方面实现了更高的成功率达到90.2%，比现有的最好算法提高了2.0%。

<details>
  <summary>Details</summary>

**Motivation:** 现有的间隔重复算法经常在语义干扰和个性化适应方面遇到困难。本文针对这一问题，提出了一种新的算法LECTOR，旨在提高学习效率和记忆保留。

**Method:** LECTOR是一种新型的自适应计划算法，专门用于测试导向的学习场景，特别是语言考试，重点是成功的通过率。该算法利用大型语言模型进行语义分析，并结合个性化学习档案，通过利用LLM支持的语义相似性评估来解决词汇学习中的语义混淆问题，并与现有的间隔重复原则相结合。

**Result:** 通过对六个基线算法（SSP-MMC，SM2，HLR，FSRS，ANKI，THRESHOLD）进行综合评估，结果显示LECTOR实现了90.2％的成功率，比最好的基线算法SSP-MMC提高了2.0％。

**Conclusion:** LECTOR在处理语义相似概念方面表现出色，减少了语义混淆引起的错误，同时保持了计算效率。研究结果确立了LECTOR作为智能辅导系统和自适应学习平台的有前途的方向。

**Abstract:** Spaced repetition systems are fundamental to efficient learning and memory
retention, but existing algorithms often struggle with semantic interference
and personalized adaptation. We present LECTOR (\textbf{L}LM-\textbf{E}nhanced
\textbf{C}oncept-based \textbf{T}est-\textbf{O}riented \textbf{R}epetition), a
novel adaptive scheduling algorithm specifically designed for test-oriented
learning scenarios, particularly language examinations where success rate is
paramount. LECTOR leverages large language models for semantic analysis while
incorporating personalized learning profiles, addressing the critical challenge
of semantic confusion in vocabulary learning by utilizing LLM-powered semantic
similarity assessment and integrating it with established spaced repetition
principles. Our comprehensive evaluation against six baseline algorithms
(SSP-MMC, SM2, HLR, FSRS, ANKI, THRESHOLD) across 100 simulated learners over
100 days demonstrates significant improvements: LECTOR achieves a 90.2\%
success rate compared to 88.4\% for the best baseline (SSP-MMC), representing a
2.0\% relative improvement. The algorithm shows particular strength in handling
semantically similar concepts, reducing confusion-induced errors while
maintaining computational efficiency. Our results establish LECTOR as a
promising direction for intelligent tutoring systems and adaptive learning
platforms.

</details>


### [26] [Do language models accommodate their users? A study of linguistic convergence](https://arxiv.org/abs/2508.03276)
*Terra Blevins,Susanne Schmalwieser,Benjamin Roth*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** While large language models (LLMs) are generally considered proficient in
generating language, how similar their language usage is to that of humans
remains understudied. In this paper, we test whether models exhibit linguistic
convergence, a core pragmatic element of human language communication, asking:
do models adapt, or converge, to the linguistic patterns of their user? To
answer this, we systematically compare model completions of exisiting dialogues
to the original human responses across sixteen language models, three dialogue
corpora, and a variety of stylometric features. We find that models strongly
converge to the conversation's style, often significantly overfitting relative
to the human baseline. While convergence patterns are often feature-specific,
we observe consistent shifts in convergence across modeling settings, with
instruction-tuned and larger models converging less than their pretrained
counterparts. Given the differences between human and model convergence
patterns, we hypothesize that the underlying mechanisms for these behaviors are
very different.

</details>


### [27] [Investigating Gender Bias in LLM-Generated Stories via Psychological Stereotypes](https://arxiv.org/abs/2508.03292)
*Shahed Masoudian,Gustavo Escobedo,Hannah Strauss,Markus Schedl*

Main category: cs.CL

> 研究通过StereoBias-Stories数据集探讨LLM中的性别偏见，发现未加条件提示时模型有显著的男性偏见，但通过特定属性条件设定能改变偏见强度，且偏见模式与心理学数据一致，这一发现提示了心理学评估的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型在各种应用程序中的广泛应用，对它们可能放大性别偏见的担忧增加。以往的偏见研究往往使用明确的性别提示作为反事实或者仅限于句子生成和简短问题回答任务，这可能忽视了更隐蔽的偏见。为了弥补这一局限，这项工作采用了心理学中关于性别刻板印象的研究，通过开放式叙事生成任务对性别偏见进行了调查。

**Method:** 本研究通过引入一个名为StereoBias-Stories的新数据集来探讨LLM中的性别偏见问题，该数据集包含未加条件或经过随机属性条件的简短故事，这些属性来自25种心理学性别刻板印象和三个与任务相关的结局。研究重点分析了这些属性对整个故事中性别贡献的影响。

**Result:** 研究得出了三个关键发现：(1) LL模型在未加条件提示时通常存在明显的男性偏见，但通过非性别刻板印象的属性进行条件设定可以减轻这种偏见。(2) 结合与同一性别刻板印象相关的多个属性会加强模型的行为，男性属性会加重偏见，女性属性则会减轻偏见。(3) 模型的偏见与用于分类的心理学事实一致，且一致性随模型大小的增加而增强。

**Conclusion:** 研究结果表明，心理学基础的评估对于理解大型语言模型的性别偏见至关重要。

**Abstract:** As Large Language Models (LLMs) are increasingly used across different
applications, concerns about their potential to amplify gender biases in
various tasks are rising. Prior research has often probed gender bias using
explicit gender cues as counterfactual, or studied them in sentence completion
and short question answering tasks. These formats might overlook more implicit
forms of bias embedded in generative behavior of longer content. In this work,
we investigate gender bias in LLMs using gender stereotypes studied in
psychology (e.g., aggressiveness or gossiping) in an open-ended task of
narrative generation. We introduce a novel dataset called StereoBias-Stories
containing short stories either unconditioned or conditioned on (one, two, or
six) random attributes from 25 psychological stereotypes and three task-related
story endings. We analyze how the gender contribution in the overall story
changes in response to these attributes and present three key findings: (1)
While models, on average, are highly biased towards male in unconditioned
prompts, conditioning on attributes independent from gender stereotypes
mitigates this bias. (2) Combining multiple attributes associated with the same
gender stereotype intensifies model behavior, with male ones amplifying bias
and female ones alleviating it. (3) Model biases align with psychological
ground-truth used for categorization, and alignment strength increases with
model size. Together, these insights highlight the importance of
psychology-grounded evaluation of LLMs.

</details>


### [28] [NLP Methods May Actually Be Better Than Professors at Estimating Question Difficulty](https://arxiv.org/abs/2508.03294)
*Leonidas Zotos,Ivo Pascal de Jong,Matias Valdenegro-Toro,Andreea Ioana Sburlea,Malvina Nissim,Hedderik van Rijn*

Main category: cs.CL

> 本研究比较了大型语言模型和教授在估计考试题目难度上的能力，发现语言模型尤其是其不确定性信息可用于预测题目难度，从而帮助教育者开发更优质考试。

<details>
  <summary>Details</summary>

**Motivation:** 教授们在估计考试题目难度上往往不够准确，因此本研究旨在通过比较语言模型和教授们的估计结果，探索更有效的题目难度评估方法。

**Method:** 通过比较大型语言模型与三位教授对神经网络和机器学习领域中的真假题目的正确回答百分比估计能力，来评估教授们在题目难度估计上的表现。此外，研究还采用监督学习方法，利用语言模型解决题目时的不确定性，在仅有42个训练样本的情况下获得了更好的结果。

**Result:** 教授们在区分题目难度方面的表现有限，而直接询问Gemini 2.5语言模型来估算题目难度的方法优于教授的估计。采用监督学习方法，利用语言模型解决题目时的不确定性可以获得更好的结果。

**Conclusion:** 研究表明，利用语言模型的不确定性进行监督学习可以帮助教授更准确地估计考试题目的难度，从而提高评估的质量。

**Abstract:** Estimating the difficulty of exam questions is essential for developing good
exams, but professors are not always good at this task. We compare various
Large Language Model-based methods with three professors in their ability to
estimate what percentage of students will give correct answers on True/False
exam questions in the areas of Neural Networks and Machine Learning. Our
results show that the professors have limited ability to distinguish between
easy and difficult questions and that they are outperformed by directly asking
Gemini 2.5 to solve this task. Yet, we obtained even better results using
uncertainties of the LLMs solving the questions in a supervised learning
setting, using only 42 training samples. We conclude that supervised learning
using LLM uncertainty can help professors better estimate the difficulty of
exam questions, improving the quality of assessment.

</details>


### [29] [Towards Trustworthy Multimodal Moderation via Policy-Aligned Reasoning and Hierarchical Labeling](https://arxiv.org/abs/2508.03296)
*Anqi Li,Wenwei Jin,Jintao Tong,Pengda Qin,Weijia Li,Guo Lu*

Main category: cs.CL

> Hi-Guard, a hierarchical moderation framework, aligns with policy rules, improves interpretability, and achieves better classification accuracy, making social platform moderation more transparent and trustworthy.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of efficient, accurate, and interpretable moderation on social platforms, especially with evolving policies.

**Method:** A hierarchical moderation framework named Hi-Guard is proposed, which includes a hierarchical moderation pipeline and a hierarchical taxonomy for classification, ensuring policy alignment and improving interpretability.

**Result:** Hi-Guard shows superior classification accuracy, generalization, and interpretability in extensive experiments and real-world deployment.

**Conclusion:** Hi-Guard introduces a new paradigm that ensures alignment with moderation rules and produces transparent decisions, contributing to scalable, transparent, and trustworthy content safety systems.

**Abstract:** Social platforms have revolutionized information sharing, but also
accelerated the dissemination of harmful and policy-violating content. To
ensure safety and compliance at scale, moderation systems must go beyond
efficiency and offer accuracy and interpretability. However, current approaches
largely rely on noisy, label-driven learning, lacking alignment with moderation
rules and producing opaque decisions that hinder human review. Therefore, we
propose Hierarchical Guard (Hi-Guard), a multimodal moderation framework that
introduces a new policy-aligned decision paradigm. The term "Hierarchical"
reflects two key aspects of our system design: (1) a hierarchical moderation
pipeline, where a lightweight binary model first filters safe content and a
stronger model handles fine-grained risk classification; and (2) a hierarchical
taxonomy in the second stage, where the model performs path-based
classification over a hierarchical taxonomy ranging from coarse to fine-grained
levels. To ensure alignment with evolving moderation policies, Hi-Guard
directly incorporates rule definitions into the model prompt. To further
enhance structured prediction and reasoning, we introduce a multi-level
soft-margin reward and optimize with Group Relative Policy Optimization (GRPO),
penalizing semantically adjacent misclassifications and improving explanation
quality. Extensive experiments and real-world deployment demonstrate that
Hi-Guard achieves superior classification accuracy, generalization, and
interpretability, paving the way toward scalable, transparent, and trustworthy
content safety systems. Code is available at:
https://github.com/lianqi1008/Hi-Guard.

</details>


### [30] [CTTS: Collective Test-Time Scaling](https://arxiv.org/abs/2508.03333)
*Zhende Song,Shengji Tang,Peng Ye,Jiayuan Fan,Tao Chen*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Test-time scaling (TTS) has emerged as a promising research field for
enhancing the effectiveness of large language models (LLMs) without extra
training. However, most existing approaches, e.g., Best-of-N and
Self-Consistency rely on a single agent interacting with a reward model
(SA-SR), constrained by limited capabilities of a single test-time scaling
(STTS) paradigm. On the other hand, recent works demonstrate that
collective-agent methods can break through the upper bound of single-agent
systems by orchestrating diverse models. Thus, in this paper, we take a first
step towards exploring Collective Test-Time Scaling (CTTS). Consider the
different interaction types of single and multiple models, we design three
primary paradigms to investigate the optimal paradigm of CTTS: (1) single agent
to multiple reward models (SA-MR); (2) multiple agents to single reward model
(MA-SR); and (3) multiple agents to multiple reward models (MA-MR). Extensive
experiments demonstrate that MA-MR consistently achieves the best performance.
Based on this, we propose a novel framework named CTTS-MM that effectively
leverages both multi-agent and multi-reward-model collaboration for enhanced
inference. Specifically, for multi-agent collaboration, we propose an Agent
Collaboration Search (ACS), which searches for the most effective combination
of LLM agents from a large candidate pool; for multi-reward-model
collaboration, we propose Mixture of Reword Models (MoR), which consists of a
curated question pool and a Prior Reward model Ensemble Selection (PRES) to
select the optimal combinations of reward models via Pair-wise Reward Ranking
(PRR) metric. Experiments across seven mainstream benchmarks demonstrate that
the proposed CTTS-MM consistently obtains superior performance. Code will be
released at https://github.com/magent4aci/CTTS-MM.

</details>


### [31] [Taggus: An Automated Pipeline for the Extraction of Characters' Social Networks from Portuguese Fiction Literature](https://arxiv.org/abs/2508.03358)
*Tiago G Canário,Catarina Duarte,Flávio L. Pinheiro,João L. M. Pereira*

Main category: cs.CL

> 研究提供了一种名为Taggus的管道，专门用于从葡萄牙语文本中识别和构建角色社交网络，性能优于现成的NLP工具。

<details>
  <summary>Details</summary>

**Motivation:** 现有的自然语言处理方法在构建角色社交网络方面的效果不佳，特别是在数据标注不足的语言中。研究者提出这项研究以优化此过程。

**Method:** 文中提出了一种名为Taggus的管道，用于从葡萄牙语文学小说作品中抽取社交网络。该管道结合了词性标注和若干启发式方法来识别角色和解决共指问题。

**Result:** 相较于现成的最先进工具（包括实体名称识别工具和大型语言模型），Taggus在识别字符和解决共指问题上的F1值为94.1%，在交互检测上的F1值为75.9%，分别提高了50.7%和22.3%。

**Conclusion:** 尽管实验样本在规模和范围上有限，但Taggus展示了在葡萄牙语文本处理上的潜力。研究提出了未来改进的方向，比如更准确地检测角色关系，并公开了该管道以推动相关领域的进一步发展。

**Abstract:** Automatically identifying characters and their interactions from fiction
books is, arguably, a complex task that requires pipelines that leverage
multiple Natural Language Processing (NLP) methods, such as Named Entity
Recognition (NER) and Part-of-speech (POS) tagging. However, these methods are
not optimized for the task that leads to the construction of Social Networks of
Characters. Indeed, the currently available methods tend to underperform,
especially in less-represented languages, due to a lack of manually annotated
data for training. Here, we propose a pipeline, which we call Taggus, to
extract social networks from literary fiction works in Portuguese. Our results
show that compared to readily available State-of-the-Art tools -- off-the-shelf
NER tools and Large Language Models (ChatGPT) -- the resulting pipeline, which
uses POS tagging and a combination of heuristics, achieves satisfying results
with an average F1-Score of $94.1\%$ in the task of identifying characters and
solving for co-reference and $75.9\%$ in interaction detection. These
represent, respectively, an increase of $50.7\%$ and $22.3\%$ on results
achieved by the readily available State-of-the-Art tools. Further steps to
improve results are outlined, such as solutions for detecting relationships
between characters. Limitations on the size and scope of our testing samples
are acknowledged. The Taggus pipeline is publicly available to encourage
development in this field for the Portuguese language.2

</details>


### [32] [Thinking with Nothinking Calibration: A New In-Context Learning Paradigm in Reasoning Large Language Models](https://arxiv.org/abs/2508.03363)
*Haotian Wu,Bo Xu,Yao Shu,Menglin Yang,Chengwei Qin*

Main category: cs.CL

> 本文提出了一种名为JointThinking的新方法，通过利用思考与无思考两种模式间的差异来提升推理型大规模语言模型的准确性和鲁棒性。方法仅在初始响应不同时进行额外推理，从而有效提升了性能，同时减少了推理延迟。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在探索推理型大规模语言模型的上下文学习潜力。虽然之前的研究主要集中在改善这些模型的训练和推断策略上，但对模型在上下文学习方面的能力的关注不多。

**Method:** 本文提出了一种新的上下文学习范式，称为思维无思维校准（JointThinking）。该方法通过利用两种推理模式（思考模式和无思考模式）之间的结构差异来提高推理准确率。具体来说，该方法提示模型并行生成两种模式下的答案，只有当两个初始响应不一致时，才触发第二次思考。

**Result:** 通过多任务推理基准测试，JointThinking显著优于少样本链式思考（CoT）和多数投票法，在答案的鲁棒性上得到了提升，同时还可达到与现有的基于训练的SOTA方法类似的效果，而在分布外任务上则显著优于其他方法。实验还揭示了利用不同的推理模式可以持续降低错误率，并显示出结构化思维多样性的价值。此外，实验还表明，随着模型规模的增加，实际情况下的推理与理想推理之间的性能差距在第二次思考时缩小，表明了该方法良好的可扩展性。

**Conclusion:** 作者讨论了该方法当前的局限性，并展望了在推理型大规模语言模型上下文学习领域未来的研究方向，强调了该方法在提高模型推理能力方面的潜在价值。

**Abstract:** Reasoning large language models (RLLMs) have recently demonstrated remarkable
capabilities through structured and multi-step reasoning. While prior research
has primarily focused on improving their training and inference strategies,
their potential for in-context learning (ICL) remains largely underexplored. To
fill this gap, we propose Thinking with Nothinking Calibration (JointThinking),
a new ICL paradigm that leverages the structured difference between two
reasoning modes, i.e., Thinking and Nothinking, to improve reasoning accuracy.
Specifically, our method prompts the model to generate two answers in parallel:
one in Thinking mode and the other in Nothinking mode. A second round of
Thinking is triggered only when the two initial responses are inconsistent,
using a single prompt that incorporates the original question and both
candidate answers. Since such disagreement occurs infrequently (e.g., only 6\%
in GSM8K), our method performs just one round of reasoning in most cases,
resulting in minimal latency overhead. Extensive experiments across multiple
reasoning benchmarks demonstrate that JointThinking significantly outperforms
few-shot chain-of-thought (CoT) and majority voting with improved answer
robustness. Moreover, It achieves comparable in-distribution performance to
training-based SOTA method, while substantially outperforming on
out-of-distribution tasks. We further conduct a systematic analysis of the
calibration mechanism, showing that leveraging different reasoning modes
consistently lowers the error rate and highlights the value of structural
thinking diversity. Additionally, we observe that the performance gap between
actual and ideal reasoning narrows as model size increases in the second round
of thinking, indicating the strong scalability of our approach. Finally, we
discuss current limitations and outline promising directions for future ICL
research in RLLMs.

</details>


### [33] [ReDSM5: A Reddit Dataset for DSM-5 Depression Detection](https://arxiv.org/abs/2508.03399)
*Eliseo Bao,Anxo Pérez,Javier Parapar*

Main category: cs.CL

> 本论文提出了ReDSM5，这是一个包含Reddit上长篇帖的多标签抑郁症状注释数据集。它结合了针对每个症状的专家解释，旨在帮助开发不仅能够检测抑郁，还能够生成人类可理解推理的模型。

<details>
  <summary>Details</summary>

**Motivation:** 尽管抑郁症影响全球数百万人，但由于传统临床访问和广泛存在的污名化问题，许多病例未能得到诊断。现有的计算方法通常只是将帖子简单地分为抑郁或非抑郁两类，而未能将其语言与DSM-5中的具体标准相联系。这限制了临床相关性和解释性。

**Method:** 本研究创建了一个名为ReDSM5的新Reddit数据集，包含1484篇长篇帖子，由持证心理学家从九个DSM-5抑郁症状的角度进行句子级详尽标注。此外，对于每个标签，标注者还会提供基于DSM-5方法的简要临床理由。

**Result:** 研究进行了数据集的探索性分析，探讨了社交媒体叙事中症状表达的词汇、句法和情感模式。确立了多标签症状分类和解释生成的基准测试，为未来的检测和解释研究提供了参考结果。

**Conclusion:** ReDSM5的独特之处在于它结合了症状特定的监督与专家解释，促进了模型的开发，使得模型不仅能够检测抑郁症，还能生成人类可理解的推理。该数据集为未来研究定义了基准。

**Abstract:** Depression is a pervasive mental health condition that affects hundreds of
millions of individuals worldwide, yet many cases remain undiagnosed due to
barriers in traditional clinical access and pervasive stigma. Social media
platforms, and Reddit in particular, offer rich, user-generated narratives that
can reveal early signs of depressive symptomatology. However, existing
computational approaches often label entire posts simply as depressed or not
depressed, without linking language to specific criteria from the DSM-5, the
standard clinical framework for diagnosing depression. This limits both
clinical relevance and interpretability. To address this gap, we introduce
ReDSM5, a novel Reddit corpus comprising 1484 long-form posts, each
exhaustively annotated at the sentence level by a licensed psychologist for the
nine DSM-5 depression symptoms. For each label, the annotator also provides a
concise clinical rationale grounded in DSM-5 methodology. We conduct an
exploratory analysis of the collection, examining lexical, syntactic, and
emotional patterns that characterize symptom expression in social media
narratives. Compared to prior resources, ReDSM5 uniquely combines
symptom-specific supervision with expert explanations, facilitating the
development of models that not only detect depression but also generate
human-interpretable reasoning. We establish baseline benchmarks for both
multi-label symptom classification and explanation generation, providing
reference results for future research on detection and interpretability.

</details>


### [34] [Variety Is the Spice of Life: Detecting Misinformation with Dynamic Environmental Representations](https://arxiv.org/abs/2508.03420)
*Bing Wang,Ximing Li,Yiming Wang,Changchun Li,Jiaxu Cui,Renchu Guan,Bo Yang*

Main category: cs.CL

> The paper presents MISDER, a framework that dynamically tracks and predicts the changing veracity of news articles over time, addressing the limitations of existing static approaches to misinformation detection.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is the critical issue of misinformation across various social media platforms and the damage caused by its proliferation. The paper aims to improve the detection of misinformation by moving beyond the constraints of static learning methods, which understate the reality of an evolving social media environment impacting the credibility of news.

**Method:** The paper proposes a novel framework called Misinformation Detection with Dynamic Environmental Representations (MISDER). This framework aims to address the limitations of static learning models by incorporating a dynamic social environment and using temporal models to predict the fluctuating veracity of news articles. Three variants of MISDER are explored: MISDER-LSTM, MISDER-ODE, and MISDER-PT.

**Result:** The experimental section of the paper shows that the novel MISDER framework, along with its variants (MISDER-LSTM, MISDER-ODE, MISDER-PT), outperforms the existing static approaches in detecting misinformation, showcasing the benefits of incorporating a dynamic perspective in the modeling.

**Conclusion:** The effectiveness of the proposed MISDER framework, including its three variants, is demonstrated through comparative experiments against various baselines across two datasets, indicating better performance of the dynamic approach over the static methods in misinformation detection.

**Abstract:** The proliferation of misinformation across diverse social media platforms has
drawn significant attention from both academic and industrial communities due
to its detrimental effects. Accordingly, automatically distinguishing
misinformation, dubbed as Misinformation Detection (MD), has become an
increasingly active research topic. The mainstream methods formulate MD as a
static learning paradigm, which learns the mapping between the content, links,
and propagation of news articles and the corresponding manual veracity labels.
However, the static assumption is often violated, since in real-world
scenarios, the veracity of news articles may vacillate within the dynamically
evolving social environment. To tackle this problem, we propose a novel
framework, namely Misinformation detection with Dynamic Environmental
Representations (MISDER). The basic idea of MISDER lies in learning a social
environmental representation for each period and employing a temporal model to
predict the representation for future periods. In this work, we specify the
temporal model as the LSTM model, continuous dynamics equation, and pre-trained
dynamics system, suggesting three variants of MISDER, namely MISDER-LSTM,
MISDER-ODE, and MISDER-PT, respectively. To evaluate the performance of MISDER,
we compare it to various MD baselines across 2 prevalent datasets, and the
experimental results can indicate the effectiveness of our proposed model.

</details>


### [35] [LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models](https://arxiv.org/abs/2508.03440)
*Junhong Wu,Jinliang Lu,Zixuan Ren,Ganqiang Hu,Zhi Wu,Dai Dai,Hua Wu*

Main category: cs.CL

> 论文探索了大型语言模型（LLMs）的软思考能力，并发现这些模型主要依赖于软输入中最具影响力的部分。为了解决这一问题，通过实验引入随机性，尤其是在采用Gumbel-Softmax技巧后，模型的表现有了显著提升。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于解决现有LLMs依赖离散token，限制其表达能力的问题。通过探索'软思考'能力，希望改善模型在连续概念空间中的推理能力。

**Method:** 此论文使用了一组探测技术来分析各种大型语言模型（LLMs）在'软思考'能力方面的内部行为。为了应对软思考的局限性，研究者还探索了引入随机性的采样策略，如Dirichlet重采样和Gumbel-Softmax技巧。

**Result:** 实验结果表明，引入随机性可以在一定程度上减轻传统方法的局限性，Gumbel-Softmax技巧因为能够提供足够的随机性同时保持可控的平滑性，因此在实验中表现突出。

**Conclusion:** 研究得出结论认为，通过引入适当的随机采样策略尤其是Gumbel-Softmax技巧，能够显著改善模型在八项推理基准上的表现，扩展了软思考的能力。

**Abstract:** Human cognition naturally engages with abstract and fluid concepts, whereas
existing reasoning models often rely on generating discrete tokens, potentially
constraining their expressive capabilities. Recent advancements aim to address
this limitation by enabling large language models (LLMs) to generate soft,
abstract tokens, thus facilitating reasoning within a continuous concept space.
This paper explores the `Soft Thinking' capabilities of various LLMs by
examining the models' internal behavior using a suite of probing techniques.
Contrary to the common belief that Soft Thinking enables the simultaneous
exploration of diverse reasoning paths, our findings reveal that LLMs
predominantly rely on the most influential component of the soft inputs during
subsequent decoding steps. This reliance hinders the exploration of different
reasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding,
obscuring the advantage of transmitting more information through Soft Tokens.
To tackle this issue, we explore sampling strategies to introduce
\emph{randomness}, employing methods such as Dirichlet resampling and the
Gumbel-Softmax trick. Our experiments demonstrate that incorporating randomness
can alleviate the limitations of vanilla approaches and unleash the potential
of Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate
randomness with controlled smoothness, resulting in superior performance across
eight reasoning benchmarks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [36] [PyCAT4: A Hierarchical Vision Transformer-based Framework for 3D Human Pose Estimation](https://arxiv.org/abs/2508.02806)
*Zongyou Yang,Jonathan Loo*

Main category: cs.CV

> 

<details>
  <summary>Details</summary>

**Motivation:** 

**Method:** Structure

**Result:** {
  "tldr": "本文通过改进Pymaf网络结构，引入基于自注意力机制的Transformer特征提取层、时序特征融合技术和空间金字塔结构，提出了PyCAT4模型，并在COCO和3DPW数据集上验证了改进策略显著提升了人体姿态估计的性能。",
  "motivation": "已有研究表明，结合卷积神经网络与金字塔网格对齐反馈环路在3D人体姿态估计中取得显著进展。同时，基于Transformer的时间分析架构也为计算机视觉领域带来了创新突破。因此，本文旨在深入优化Pymaf网络架构。",
  "method": "主要改进包括：引入基于自注意力机制的Transformer特征提取层以增强低层次特征捕捉；通过特征时序融合技术增强对视频序列中时序信号的理解和捕捉；应用空间金字塔结构实现多尺度特征融合，有效平衡不同尺度特征表示差异。",
  "result": "实验在COCO和3DPW数据集上证明了提出的改进策略显著增强了网络在人体姿态估计中的检测能力。",
  "conclusion": "研究提出的PyCAT4模型显著提升了人体姿态估计技术的发展。
}

**Conclusion:** 

**Abstract:** Recently, a significant improvement in the accuracy of 3D human pose
estimation has been achieved by combining convolutional neural networks (CNNs)
with pyramid grid alignment feedback loops. Additionally, innovative
breakthroughs have been made in the field of computer vision through the
adoption of Transformer-based temporal analysis architectures. Given these
advancements, this study aims to deeply optimize and improve the existing Pymaf
network architecture. The main innovations of this paper include: (1)
Introducing a Transformer feature extraction network layer based on
self-attention mechanisms to enhance the capture of low-level features; (2)
Enhancing the understanding and capture of temporal signals in video sequences
through feature temporal fusion techniques; (3) Implementing spatial pyramid
structures to achieve multi-scale feature fusion, effectively balancing feature
representations differences across different scales. The new PyCAT4 model
obtained in this study is validated through experiments on the COCO and 3DPW
datasets. The results demonstrate that the proposed improvement strategies
significantly enhance the network's detection capability in human pose
estimation, further advancing the development of human pose estimation
technology.

</details>


### [37] [DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework](https://arxiv.org/abs/2508.02807)
*Tongchun Zuo,Zaiyu Huang,Shuliang Ning,Ente Lin,Chao Liang,Zerong Zheng,Jianwen Jiang,Yuan Zhang,Mingyuan Gao,Xin Dong*

Main category: cs.CV

> 提出DreamVVT框架，解决现有VVT方法在保持精细衣物细节和时间一致性上的困难，实验证明其优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有结束VVT方法依赖稀有配对衣物数据集、未能利用先进视觉模型和测试时输入先验知识的问题，以实现在不受约束情况下准确保持精细衣物细节并保持时间一致性。

**Method:** 提出DreamVVT，一个两阶段框架：第一阶段利用多帧试穿模型生成关键帧试穿衣图，第二阶段使用增强的LoRA适配器视频生成模型处理关键帧和输入内容，以确保长期时间和动态连贯性。

**Result:** Video虚拟试衣(VVT)技术因其在电子商务广告和娱乐中的潜在应用而受到学术界的广泛关注。然而，现有的端到端方法通常依赖于稀有的配对衣物数据集，并且未能充分利用先进视觉模型和测试时输入的先验知识，这导致在不受约束的情况下难以准确保持精细的衣物细节并保持时间一致性。为了解决这些挑战，我们提出了DreamVVT，这是一个基于Diffusion Transformers (DiTs)的精心设计的两阶段框架，能够利用多样化的未配对的人体数据来增强在实际场景中的适应性。在第一阶段，我们从输入视频中采样代表性帧，并利用集成了视觉语言模型(VLM)的多帧试穿模型，生成高保真度和语义一致的关键帧试穿衣图。这些图像作为后续视频生成的补充外观指导。**在第二阶段**，从输入内容中提取骨架图和精细的动作及外观描述，与关键帧试穿衣图一起输入到增强LoRA适配器的预训练视频生成模型中。这确保了长期的时间连贯性，并为未见区域启用高度可信的动态动作。广泛的定量和定性实验证明，DreamVVT在实际情况下优于现有的方法，能够保持详细的衣物内容和时间稳定性。

**Conclusion:** 通过广泛的实验，证明了DreamVVT相比于现有方法能够更好地保持详细的衣物内容和时间稳定性。

**Abstract:** Video virtual try-on (VVT) technology has garnered considerable academic
interest owing to its promising applications in e-commerce advertising and
entertainment. However, most existing end-to-end methods rely heavily on scarce
paired garment-centric datasets and fail to effectively leverage priors of
advanced visual models and test-time inputs, making it challenging to
accurately preserve fine-grained garment details and maintain temporal
consistency in unconstrained scenarios. To address these challenges, we propose
DreamVVT, a carefully designed two-stage framework built upon Diffusion
Transformers (DiTs), which is inherently capable of leveraging diverse unpaired
human-centric data to enhance adaptability in real-world scenarios. To further
leverage prior knowledge from pretrained models and test-time inputs, in the
first stage, we sample representative frames from the input video and utilize a
multi-frame try-on model integrated with a vision-language model (VLM), to
synthesize high-fidelity and semantically consistent keyframe try-on images.
These images serve as complementary appearance guidance for subsequent video
generation. \textbf{In the second stage}, skeleton maps together with
fine-grained motion and appearance descriptions are extracted from the input
content, and these along with the keyframe try-on images are then fed into a
pretrained video generation model enhanced with LoRA adapters. This ensures
long-term temporal coherence for unseen regions and enables highly plausible
dynamic motions. Extensive quantitative and qualitative experiments demonstrate
that DreamVVT surpasses existing methods in preserving detailed garment content
and temporal stability in real-world scenarios. Our project page
https://virtu-lab.github.io/

</details>


### [38] [Elucidating the Role of Feature Normalization in IJEPA](https://arxiv.org/abs/2508.02829)
*Adam Colton*

Main category: cs.CV

> Replacing layer normalization with DynTanh in IJEPA improves self-supervised learning performance in image classification and depth estimation by preserving the natural energy hierarchy of visual tokens.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the issue that LN disrupts the natural energy hierarchy of visual tokens, which is crucial for effective self-supervised visual representation learning, leading to checkerboard-like artifacts in the loss maps of IJEPA models.

**Method:** The paper proposes replacing the layer normalization (LN) in the standard image joint embedding predictive architecture (IJEPA) with a DynTanh activation to preserve the natural energy hierarchy of visual tokens, allowing semantically rich regions to be prioritized.

**Result:** The proposed method exhibits a longer-tailed loss distribution, fixes checkerboard artifacts in the loss map, and improves ImageNet linear probe accuracy from 38% to 42.7% for ViT-Small and reduces RMSE by 0.08 on NYU Depth V2 monocular depth estimation.

**Conclusion:** Preserving the natural energy hierarchy of visual tokens by using DynTanh instead of LN in IJEPA can improve the model's performance in tasks such as image classification and depth estimation.

**Abstract:** In the standard image joint embedding predictive architecture (IJEPA),
features at the output of the teacher encoder are layer normalized (LN) before
serving as a distillation target for the student encoder and predictor. We
propose that this feature normalization disrupts the natural energy hierarchy
of visual tokens, where high-energy tokens (those with larger L2 norms) encode
semantically important image regions. LN forces all features to have identical
L2 norms, effectively equalizing their energies and preventing the model from
prioritizing semantically rich regions. We find that IJEPA models trained with
feature LN exhibit loss maps with significant checkerboard-like artifacts. We
propose that feature LN be replaced with a DynTanh activation as the latter
better preserves token energies and allows high-energy tokens to greater
contribute to the prediction loss. We show that IJEPA trained with feature
DynTanh exhibits a longer-tailed loss distribution and fixes the checkerboard
artifacts in the loss map. Our empirical results show that our simple
modification improves ImageNet linear probe accuracy from 38% to 42.7% for
ViT-Small and reduces RMSE by 0.08 on NYU Depth V2 monocular depth estimation.
These results suggest that preserving natural token energies is crucial for
effective self-supervised visual representation learning.

</details>


### [39] [GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing](https://arxiv.org/abs/2508.02831)
*Mikołaj Zieliński,Krzysztof Byrski,Tomasz Szczepanik,Przemysław Spurek*

Main category: cs.CV

> GENIE模型结合NeRF和GS的优势，实现了高质量渲染及实时、直观的3D场景编辑。

<details>
  <summary>Details</summary>

**Motivation:** 解决NeRF内在表达下的编辑和物理交互挑战，同时利用GS在实时渲染和直观操作的优势，通过结合二者的优点来实现更有效的3D场景表示和渲染。

**Method:** GENIE结合NeRF的高质量渲染能力和GS的可编辑性强结构表示。它用trainable feature embedding代替球谐函数进行外观建模，并基于RT-GPS实现高效的NeRF网络条件化。同时，利用multi-resolution hash grid初始化和更新Gaussian特征。

**Result:** GENIE模型实现了实时局部感知编辑，作为Gaussian primitives重新定位或修改时，其插值影响会立即反映在渲染输出中。这使得GENIE能够支持直观的场景操作和动态交互，同时兼容物理仿真。

**Conclusion:** GENIE模型在融合显式和隐式表示方法的同时，实现了高质量渲染、实时编辑和动态交互，为神经渲染和几何编辑桥接了优势，揭示了未来3D场景表示技术的发展方向。

**Abstract:** Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently
transformed 3D scene representation and rendering. NeRF achieves high-fidelity
novel view synthesis by learning volumetric representations through neural
networks, but its implicit encoding makes editing and physical interaction
challenging. In contrast, GS represents scenes as explicit collections of
Gaussian primitives, enabling real-time rendering, faster training, and more
intuitive manipulation. This explicit structure has made GS particularly
well-suited for interactive editing and integration with physics-based
simulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural
Radiance Fields Interactive Editing), a hybrid model that combines the
photorealistic rendering quality of NeRF with the editable and structured
representation of GS. Instead of using spherical harmonics for appearance
modeling, we assign each Gaussian a trainable feature embedding. These
embeddings are used to condition a NeRF network based on the k nearest
Gaussians to each query point. To make this conditioning efficient, we
introduce Ray-Traced Gaussian Proximity Search (RT-GPS), a fast nearest
Gaussian search based on a modified ray-tracing pipeline. We also integrate a
multi-resolution hash grid to initialize and update Gaussian features.
Together, these components enable real-time, locality-aware editing: as
Gaussian primitives are repositioned or modified, their interpolated influence
is immediately reflected in the rendered output. By combining the strengths of
implicit and explicit representations, GENIE supports intuitive scene
manipulation, dynamic interaction, and compatibility with physical simulation,
bridging the gap between geometry-based editing and neural rendering. The code
can be found under (https://github.com/MikolajZielinski/genie)

</details>


### [40] [RefineSeg: Dual Coarse-to-Fine Learning for Medical Image Segmentation](https://arxiv.org/abs/2508.02844)
*Anghong Du,Nay Aung,Theodoros N. Arvanitis,Stefan K. Piechnik,Joao A C Lima,Steffen E. Petersen,Le Zhang*

Main category: cs.CV

> 提出了一种仅依赖粗标注的医学图像分割框架，通过转换矩阵处理不准确的区域，实验表明该方法超越了现有的弱监督方法，并接近全监督方法的效果。

<details>
  <summary>Details</summary>

**Motivation:** 高质量的医学图像像素级标注对监督分割任务至关重要，但成本高昂且需要医学专业知识。为此，我们提出了这一新框架，仅使用粗尺度的标注，包括目标和互补绘制，即使它们包含噪声。

**Method:** 我们提出了一种基于粗标注的从粗糙到精细的分割框架，通过引入转换矩阵来处理粗标注中的不准确和不完整区域，该框架利用多个粗标注集进行联合训练，逐步细化网络的输出，从而推断出真正的分割分布。

**Result:** 在ACDC、MSCMRseg和UK Biobank三个公开心脏成像数据集上的实验结果表明，所提出的方法在性能上超越了现有的弱监督方法，并接近全监督方法的效果。

**Conclusion:** 本研究介绍的从粗糙到精细的分割框架展示了其灵活性和有效性，通过仅使用粗标注就能实现接近全监督方法的精确度。

**Abstract:** High-quality pixel-level annotations of medical images are essential for
supervised segmentation tasks, but obtaining such annotations is costly and
requires medical expertise. To address this challenge, we propose a novel
coarse-to-fine segmentation framework that relies entirely on coarse-level
annotations, encompassing both target and complementary drawings, despite their
inherent noise. The framework works by introducing transition matrices in order
to model the inaccurate and incomplete regions in the coarse annotations. By
jointly training on multiple sets of coarse annotations, it progressively
refines the network's outputs and infers the true segmentation distribution,
achieving a robust approximation of precise labels through matrix-based
modeling. To validate the flexibility and effectiveness of the proposed method,
we demonstrate the results on two public cardiac imaging datasets, ACDC and
MSCMRseg, and further evaluate its performance on the UK Biobank dataset.
Experimental results indicate that our approach surpasses the state-of-the-art
weakly supervised methods and closely matches the fully supervised approach.

</details>


### [41] [MIDAR: Mimicking LiDAR Detection for Traffic Applications with a Lightweight Plug-and-Play Model](https://arxiv.org/abs/2508.02858)
*Tianheng Zhu,Yiheng Feng*

Main category: cs.CV

> 提出了一种称为MIDAR的模拟模型，它使用微观交通模拟器中容易获得的车辆特征来近似生成现实的LiDAR检测。研究结果表明，MIDAR在模拟高保真LiDAR检测方面效果显著。

<details>
  <summary>Details</summary>

**Motivation:** 尽管基于游戏引擎的模拟器如CARLA可以生成高保真的原始传感器数据，但它们在多AV场景中面临可扩展性挑战。而像SUMO这样的微观交通模拟器虽然高效，但缺乏感知建模能力。本研究旨在填补这一空白。

**Method:** 提出了一种名为MIDAR的LiDAR检测模拟模型，该模型基于微观交通模拟器中易于获得的车辆级特征，近似生成现实的LiDAR检测。MIDAR利用增强的GRU-APPNP架构通过形成精简的多跳视线图（RM-LoS）来传播特征，从而预测理想LiDAR检测结果中的真阳性(TPs)和假阴性(FNs)。

**Result:** 在nuScenes自动驾驶数据集上，MIDAR能够以0.909的AUC值近似中心点（CenterPoint）生成的检测结果，这是一款广泛使用的3D LiDAR检测模型。通过两个基于CP的交通应用验证了这种现实检测建模的必要性，尤其是在需要精确车辆个体观测的任务中。

**Conclusion:** MIDAR可以无缝集成到交通模拟器和轨迹数据集中，并将在发表后开源。

**Abstract:** As autonomous driving (AD) technology advances, increasing research has
focused on leveraging cooperative perception (CP) data collected from multiple
AVs to enhance traffic applications. Due to the impracticality of large-scale
real-world AV deployments, simulation has become the primary approach in most
studies. While game-engine-based simulators like CARLA generate high-fidelity
raw sensor data (e.g., LiDAR point clouds) which can be used to produce
realistic detection outputs, they face scalability challenges in multi-AV
scenarios. In contrast, microscopic traffic simulators such as SUMO scale
efficiently but lack perception modeling capabilities. To bridge this gap, we
propose MIDAR, a LiDAR detection mimicking model that approximates realistic
LiDAR detections using vehicle-level features readily available from
microscopic traffic simulators. Specifically, MIDAR predicts true positives
(TPs) and false negatives (FNs) from ideal LiDAR detection results based on the
spatial layouts and dimensions of surrounding vehicles. A Refined Multi-hop
Line-of-Sight (RM-LoS) graph is constructed to encode the occlusion
relationships among vehicles, upon which MIDAR employs a GRU-enhanced APPNP
architecture to propagate features from the ego AV and occluding vehicles to
the prediction target. MIDAR achieves an AUC of 0.909 in approximating the
detection results generated by CenterPoint, a mainstream 3D LiDAR detection
model, on the nuScenes AD dataset. Two CP-based traffic applications further
validate the necessity of such realistic detection modeling, particularly for
tasks requiring accurate individual vehicle observations (e.g., position,
speed, lane index). As demonstrated in the applications, MIDAR can be
seamlessly integrated into traffic simulators and trajectory datasets and will
be open-sourced upon publication.

</details>


### [42] [Evaluation and Analysis of Deep Neural Transformers and Convolutional Neural Networks on Modern Remote Sensing Datasets](https://arxiv.org/abs/2508.02871)
*J. Alex Hurt,Trevor M. Bajkowski,Grant J. Scott,Curt H. Davis*

Main category: cs.CV

> 本文研究了基于变压器的神经网络在高分辨遥感图像目标检测中的应用，并在三个公开数据集上，比较了基于变压器的系统与传统卷积网络的表现。

<details>
  <summary>Details</summary>

**Motivation:** 随着视觉变压器的出现，计算视觉领域迈出了第二个现代飞跃，本研究旨在探讨基于变压器的神经网络在卫星图像处理中的表现，特别是远距感测数据上的大范围比较还未有人尝试。

**Method:** 文章中讨论了基于变压器的神经网络和卷积神经网络在高分辨率遥感图像目标检测中的应用。研究中考察了11种边界框检测和定位算法，并且比较了5种变压器架构和6种卷积网络在3个公开的高分辨率遥感数据集上的表现。

**Result:** 通过训练和评估33个深度神经模型，研究展示了基于变压器的系统在多个高分辨率遥感数据集上表现出相当的竞争性，证实了其在目标检测中的优越性能。

**Conclusion:** 研究表明，基于变压器的神经网络在高分辨率遥感图象目标检测中能够实现高性能，提供了可能超越了传统卷积网络的新方法。

**Abstract:** In 2012, AlexNet established deep convolutional neural networks (DCNNs) as
the state-of-the-art in CV, as these networks soon led in visual tasks for many
domains, including remote sensing. With the publication of Visual Transformers,
we are witnessing the second modern leap in computational vision, and as such,
it is imperative to understand how various transformer-based neural networks
perform on satellite imagery. While transformers have shown high levels of
performance in natural language processing and CV applications, they have yet
to be compared on a large scale to modern remote sensing data. In this paper,
we explore the use of transformer-based neural networks for object detection in
high-resolution electro-optical satellite imagery, demonstrating
state-of-the-art performance on a variety of publicly available benchmark data
sets. We compare eleven distinct bounding-box detection and localization
algorithms in this study, of which seven were published since 2020, and all
eleven since 2015. The performance of five transformer-based architectures is
compared with six convolutional networks on three state-of-the-art opensource
high-resolution remote sensing imagery datasets ranging in size and complexity.
Following the training and evaluation of thirty-three deep neural models, we
then discuss and analyze model performance across various feature extraction
methodologies and detection algorithms.

</details>


### [43] [VisuCraft: Enhancing Large Vision-Language Models for Complex Visual-Guided Creative Content Generation via Structured Information Extraction](https://arxiv.org/abs/2508.02890)
*Rongxin Jiang,Robert Long,Chenghao Gu,Mingrui Yan*

Main category: cs.CV

> VisuCraft is a novel framework that integrates a multimodal structured information extractor and dynamic prompt generation module to enhance LVLMs, improving their performance in tasks like story generation and poetry composition.

<details>
  <summary>Details</summary>

**Motivation:** to significantly enhance the capabilities of LVLMs in complex visual-guided creative content generation, addressing limitations in existing LVLMs.

**Method:** integrates a multimodal structured information extractor (E) and a dynamic prompt generation module (G) to enhance LVLMs.

**Result:** VisuCraft outperforms baseline LVLMs in creativity and instruction adherence when generating long-form texts such as stories and poetry.

**Conclusion:** VisuCraft demonstrates remarkable improvements in producing imaginative, visually grounded, and user-aligned long-form creative text, unlocking new potential for LVLMs in sophisticated creative AI applications.

**Abstract:** This paper introduces VisuCraft, a novel framework designed to significantly
enhance the capabilities of Large Vision-Language Models (LVLMs) in complex
visual-guided creative content generation. Existing LVLMs often exhibit
limitations in maintaining high visual fidelity, genuine creativity, and
precise adherence to nuanced user instructions when generating long-form texts.
VisuCraft addresses these challenges by integrating a multimodal structured
information extractor (E) and a dynamic prompt generation module (G). The
extractor distills fine-grained visual attributes from input images into a
rich, structured representation, which the dynamic prompt module then combines
with user instructions to create highly optimized prompts for underlying LVLMs
(e.g., LLaVA, InstructBLIP). Evaluated on the self-constructed
ImageStoryGen-500K dataset using VisuGen Metrics (Visual Grounding, Creativity,
and Instruction Adherence), VisuCraft consistently outperforms baseline LVLMs
across tasks like story generation and poetry composition. Our results
demonstrate remarkable improvements, particularly in creativity and instruction
adherence, validating VisuCraft's effectiveness in producing imaginative,
visually grounded, and user-aligned long-form creative text. This work unlocks
new potential for LVLMs in sophisticated creative AI applications.

</details>


### [44] [RDDPM: Robust Denoising Diffusion Probabilistic Model for Unsupervised Anomaly Segmentation](https://arxiv.org/abs/2508.02903)
*Mehrdad Moradi,Kamran Paynabar*

Main category: cs.CV

> This paper presents a novel robust denoising diffusion probabilistic model specifically designed to handle contaminated datasets for unsupervised anomaly segmentation, outperforming current state-of-the-art models in experiments.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this paper is to address the limitation of traditional diffusion models which require access to clean, normal data for training. This is not always possible in realistic settings where only contaminated, unlabeled data is available.

**Method:** By interpreting the denoising diffusion probabilistic model as a nonlinear regression problem, the authors use robust regression techniques to develop a robust denoising diffusion probabilistic model that can effectively handle contaminated datasets containing both normal and anomalous data.

**Result:** Experiments show the proposed robust denoising diffusion probabilistic model outperforms existing diffusion models when dealing with contaminated data, achieving up to 8.08% higher AUROC and 10.37% higher AUPRC on MVTec datasets.

**Conclusion:** The work demonstrates that robust denoising diffusion probabilistic models can be successfully applied to unsupervised anomaly segmentation when only contaminated data is accessible, offering improvements over current diffusion-based methods.

**Abstract:** Recent advancements in diffusion models have demonstrated significant success
in unsupervised anomaly segmentation. For anomaly segmentation, these models
are first trained on normal data; then, an anomalous image is noised to an
intermediate step, and the normal image is reconstructed through backward
diffusion. Unlike traditional statistical methods, diffusion models do not rely
on specific assumptions about the data or target anomalies, making them
versatile for use across different domains. However, diffusion models typically
assume access to normal data for training, limiting their applicability in
realistic settings. In this paper, we propose novel robust denoising diffusion
models for scenarios where only contaminated (i.e., a mix of normal and
anomalous) unlabeled data is available. By casting maximum likelihood
estimation of the data as a nonlinear regression problem, we reinterpret the
denoising diffusion probabilistic model through a regression lens. Using robust
regression, we derive a robust version of denoising diffusion probabilistic
models. Our novel framework offers flexibility in constructing various robust
diffusion models. Our experiments show that our approach outperforms current
state of the art diffusion models, for unsupervised anomaly segmentation when
only contaminated data is available. Our method outperforms existing
diffusion-based approaches, achieving up to 8.08\% higher AUROC and 10.37\%
higher AUPRC on MVTec datasets. The implementation code is available at:
https://github.com/mehrdadmoradi124/RDDPM

</details>


### [45] [How Would It Sound? Material-Controlled Multimodal Acoustic Profile Generation for Indoor Scenes](https://arxiv.org/abs/2508.02905)
*Mahnoor Fatima Saad,Ziad Al-Halah*

Main category: cs.CV

> 研究介绍了一种基于用户定义材料配置生成室内声学特性的方法，提出了一种新颖的编码器-解码器模型，并创建了一个新的基准数据集用于评估该方法的性能，结果显示其优于多种基线方法和现有技术。

<details>
  <summary>Details</summary>

**Motivation:** 探索声学模拟的新方法，特别是在不同材料配置下生成室内声学特性的能力。这些信息对于理解材料如何影响音频的传播和音质至关重要。

**Method:** 使用了一种新颖的编码器-解码器模型，该模型可以从音视频观察中编码场景的关键特性，并根据用户提供的材料规格生成目标房间脉冲响应（RIR）。

**Result:** 提出的方法能够有效地根据不同的材料配置生成高质量的RIR，优于多种基线和最先进的方法。

**Conclusion:** 该研究为精确建模不同材料的声学影响提供了一个新工具，有助于改进室内声学环境的模拟与控制。

**Abstract:** How would the sound in a studio change with a carpeted floor and acoustic
tiles on the walls? We introduce the task of material-controlled acoustic
profile generation, where, given an indoor scene with specific audio-visual
characteristics, the goal is to generate a target acoustic profile based on a
user-defined material configuration at inference time. We address this task
with a novel encoder-decoder approach that encodes the scene's key properties
from an audio-visual observation and generates the target Room Impulse Response
(RIR) conditioned on the material specifications provided by the user. Our
model enables the generation of diverse RIRs based on various material
configurations defined dynamically at inference time. To support this task, we
create a new benchmark, the Acoustic Wonderland Dataset, designed for
developing and evaluating material-aware RIR prediction methods under diverse
and challenging settings. Our results demonstrate that the proposed model
effectively encodes material information and generates high-fidelity RIRs,
outperforming several baselines and state-of-the-art methods.

</details>


### [46] [Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces](https://arxiv.org/abs/2508.02917)
*Vebjørn Haug Kåsene,Pierre Lison*

Main category: cs.CV

> 研究评估了现成的大型视觉语言模型（LVLMs）在视觉语言导航（VLN）任务中的性能，发现它们可以在一定程度上执行该任务，但性能不及专门设计的模型。

<details>
  <summary>Details</summary>

**Motivation:** 探讨现成的大型视觉语言模型（LVLMs）是否可以在无需架构修改或模拟训练的情况下有效支持视觉语言导航（VLN）任务，并且是否可以支持低级别和全景动作范式。

**Method:** 研究使用了开源模型Qwen2.5-VL-3B-Instruct，并在Room-to-Room (R2R) 数据集上进行微调，以评估其在低级别和全景动作空间中的性能表现。

**Result:** 最佳模型在R2R测试集上达到了41%的成功率，表明现成的LVLMs可以学习执行视觉语言导航任务，但仍然落后于专门为该任务设计的模型。

**Conclusion:** 现成的LVLMs具有支持VLN任务的潜力，但它们的成功率仍然低于专门为VLN设计的模型。未来的工作可能需要探索架构修改或增加模拟训练以提升性能。

**Abstract:** Vision-and-Language Navigation (VLN) refers to the task of enabling
autonomous robots to navigate unfamiliar environments by following natural
language instructions. While recent Large Vision-Language Models (LVLMs) have
shown promise in this task, most current VLM systems rely on models
specifically designed and optimized for navigation, leaving the potential of
off-the-shelf LVLMs underexplored. Furthermore, while older VLN approaches used
low-level action spaces with egocentric views and atomic actions (such as "turn
left" or "move forward"), newer models tend to favor panoramic action spaces
with discrete navigable viewpoints. This paper investigates (1) whether
off-the-shelf LVLMs (fine-tuned without architectural modifications or
simulator-based training) can effectively support VLN tasks and (2) whether
such models can support both low-level and panoramic action paradigms. To this
end, we fine-tune the open-source model Qwen2.5-VL-3B-Instruct on the
Room-to-Room (R2R) dataset and evaluate its empirical performance across both
low-level and panoramic action spaces. The best resulting model achieves a 41%
success rate on the R2R test set, demonstrating that while off-the-shelf LVLMs
can learn to perform Vision-and-Language Navigation, they still lag behind
models specifically designed for this task.

</details>


### [47] [How Diffusion Prior Landscapes Shape the Posterior in Blind Deconvolution](https://arxiv.org/abs/2508.02923)
*Minh-Hai Nguyen,Edouard Pauwels,Pierre Weiss*

Main category: cs.CV

> 本文通过经验分析扩散模型先验的似然景观，揭示了MAP估计在盲去模糊中得到模糊解的原因，并通过理论分析和实验证明，局部极小值对应于真实自然图像，可通过适当的初始化找到这些局部极小值来改善盲去模糊效果。

<details>
  <summary>Details</summary>

**Motivation:** 尽管MAP估计被判别图像先验广泛应用于盲去卷积中，但当采用促进稀疏性的判别图像先验时，MAP倾向于产生模糊的结果。本文希望通过使用扩散模型先验，重新审视这一结果，并找到克服MAP局限性的方法。

**Method:** 通过经验分析扩散模型的似然景观，发现高模糊图像具有更高的似然性，并且景观中存在许多对应于自然图像的局部极小值。基于此，对盲去模糊的后验概率进行了理论分析，揭示了MAP估计器倾向于产生锐化过的滤波器（接近狄拉克delta函数）和模糊解。局部后验极小值，可以通过梯度下降获得，对应于真实且自然的图像，有效解决了盲去模糊问题。

**Result:** 研究表明，通过优化初始值可以克服MAP估计的局限性，使算法达到局部极小值，实现有效的盲去模糊。数值实验验证了分析结果，表明这些见解对于设计更优的先验模型和优化技术具有实际意义。

**Conclusion:** MAP估计在盲去模糊问题中难以获得清晰的重建图像。然而，通过分析扩散模型的后验概率的似然景观，表明局部极小值对应于自然图像，且可通过梯度下降算法找到这些局部极小值。这些结论对改进先验和优化策略具有重要指导意义。

**Abstract:** The Maximum A Posteriori (MAP) estimation is a widely used framework in blind
deconvolution to recover sharp images from blurred observations. The estimated
image and blur filter are defined as the maximizer of the posterior
distribution. However, when paired with sparsity-promoting image priors, MAP
estimation has been shown to favors blurry solutions, limiting its
effectiveness. In this paper, we revisit this result using diffusion-based
priors, a class of models that capture realistic image distributions. Through
an empirical examination of the prior's likelihood landscape, we uncover two
key properties: first, blurry images tend to have higher likelihoods; second,
the landscape contains numerous local minimizers that correspond to natural
images. Building on these insights, we provide a theoretical analysis of the
blind deblurring posterior. This reveals that the MAP estimator tends to
produce sharp filters (close to the Dirac delta function) and blurry solutions.
However local minimizers of the posterior, which can be obtained with gradient
descent, correspond to realistic, natural images, effectively solving the blind
deconvolution problem. Our findings suggest that overcoming MAP's limitations
requires good local initialization to local minima in the posterior landscape.
We validate our analysis with numerical experiments, demonstrating the
practical implications of our insights for designing improved priors and
optimization techniques.

</details>


### [48] [Infrared Object Detection with Ultra Small ConvNets: Is ImageNet Pretraining Still Useful?](https://arxiv.org/abs/2508.02927)
*Srikanth Muralidharan,Heitor R. Medeiros,Masih Aminbeidokhti,Eric Granger,Marco Pedersoli*

Main category: cs.CV

> 研究发现对于参数量小于1M的超小型模型，ImageNet预训练虽然仍旧能带来性能提升，但超过某个容量点后，它在提升领域外鲁棒性方面的效果明显减弱。建议实践中仍使用预训练，但也要避免过于小型的模型。

<details>
  <summary>Details</summary>

**Motivation:** 在现实世界的应用中，需要具有强大适应性和鲁棒性的中小型模型，能够运行在嵌入式设备上。尽管已知预训练对于正常大小的模型是很有帮助的，但对于小型模型来说，其作用尚不明确。本研究旨在探究ImageNet预训练对于提高小型模型在红外视觉模态下游目标检测任务中的鲁棒性的影响。

**Method:** 构建了两种超小型主干网络家族，并系统地研究了它们的表现。这些网络家族基于从标准对象识别架构中推导出的缩放定律。通过在三个不同的数据集上进行实验，评估了ImageNet预训练对这些小型模型的影响。

**Result:** 

**Conclusion:** ImageNet预训练虽然对小型模型依然有用，但是超过一定容量阈值后，它在处理领域外检测鲁棒性方面的收益会逐渐减小。因此作者建议实践者仍然可以使用预训练，但在可能的情况下，避免使用过于小型的模型，因为这些模型虽然可能对领域内问题有良好表现，但在条件不同时会显得不稳定。

**Abstract:** Many real-world applications require recognition models that are robust to
different operational conditions and modalities, but at the same time run on
small embedded devices, with limited hardware. While for normal size models,
pre-training is known to be very beneficial in accuracy and robustness, for
small models, that can be employed for embedded and edge devices, its effect is
not clear. In this work, we investigate the effect of ImageNet pretraining on
increasingly small backbone architectures (ultra-small models, with $<$1M
parameters) with respect to robustness in downstream object detection tasks in
the infrared visual modality. Using scaling laws derived from standard object
recognition architectures, we construct two ultra-small backbone families and
systematically study their performance. Our experiments on three different
datasets reveal that while ImageNet pre-training is still useful, beyond a
certain capacity threshold, it offers diminishing returns in terms of
out-of-distribution detection robustness. Therefore, we advise practitioners to
still use pre-training and, when possible avoid too small models as while they
might work well for in-domain problems, they are brittle when working
conditions are different.

</details>


### [49] [X-Actor: Emotional and Expressive Long-Range Portrait Acting from Audio](https://arxiv.org/abs/2508.02944)
*Chenxu Zhang,Zenan Li,Hongyi Xu,You Xie,Xiaochen Zhao,Tianpei Gu,Guoxian Song,Xin Chen,Chao Liang,Jianwen Jiang,Linjie Luo*

Main category: cs.CV

> 提出了X-Actor，一种生成逼真表情的讲话人像视频的音频驱动框架，通过结合自回归扩散模型和视频合成模块，实现了长时长、情感丰富的面部动画效果。

<details>
  <summary>Details</summary>

**Motivation:** 之前的面部动画方法在强调唇部同步和短时视觉保真度的同时，没有很好地处理大规模动态情感表达。X-Actor的目标是生成高质量的面部动画，能够情感丰富且在长时间内保持连贯性。

**Method:** X-Actor采用两阶段的解耦生成流水线，首先通过音频条件的自回归扩散模型，在长时序上下文中生成表情丰富但不具身份特性的面部动作潜在标记，随后通过基于扩散的视频合成模块把这些动作转化为高保真的视频动画。

**Result:** 实验表明，X-Actor能够在长时间内生成具有丰富情感的面部动作预测，且效果优于现有方法，能够产生超越一般讲话人像动画的视觉表演。

**Conclusion:** X-Actor通过其独特的长时序上下文音频驱动面部动作预测能力和基于扩散的视频合成方法，成功实现了高质量、长时长的情感丰富的面部动画生成。

**Abstract:** We present X-Actor, a novel audio-driven portrait animation framework that
generates lifelike, emotionally expressive talking head videos from a single
reference image and an input audio clip. Unlike prior methods that emphasize
lip synchronization and short-range visual fidelity in constrained speaking
scenarios, X-Actor enables actor-quality, long-form portrait performance
capturing nuanced, dynamically evolving emotions that flow coherently with the
rhythm and content of speech. Central to our approach is a two-stage decoupled
generation pipeline: an audio-conditioned autoregressive diffusion model that
predicts expressive yet identity-agnostic facial motion latent tokens within a
long temporal context window, followed by a diffusion-based video synthesis
module that translates these motions into high-fidelity video animations. By
operating in a compact facial motion latent space decoupled from visual and
identity cues, our autoregressive diffusion model effectively captures
long-range correlations between audio and facial dynamics through a
diffusion-forcing training paradigm, enabling infinite-length emotionally-rich
motion prediction without error accumulation. Extensive experiments demonstrate
that X-Actor produces compelling, cinematic-style performances that go beyond
standard talking head animations and achieves state-of-the-art results in
long-range, audio-driven emotional portrait acting.

</details>


### [50] [Towards Robust Image Denoising with Scale Equivariance](https://arxiv.org/abs/2508.02967)
*Dawei Zhang,Xiaojie Guo*

Main category: cs.CV

> 本文研究了尺度等方差性作为改善分布外（OOD）鲁棒性的核心归纳偏置，并提出了一种鲁棒的盲去噪框架，该框架分别通过异构归一化模块（HNM）和交互门控模块（IGM）来适应空间均匀和非均匀噪声。

<details>
  <summary>Details</summary>

**Motivation:** 尽管图像去噪已经取得了显著进展，但现有模型在推广至分布外（OOD）噪声模式方面仍然面临挑战，特别是在空间变异噪声条件下。作者旨在探索尺度等方差性作为核心归纳偏置，以提高 OOD 稳健性。

**Method:** 本文提出了一个鲁棒的盲去噪框架，其包含两个关键组件：异构归一化模块（HNM）和交互门控模块（IGM）。HNM 稳定了特征分布，并在不同噪声强度下动态校正特征，而 IGM 通过信号路径和特征路径之间的门控交互促进有效信息调制。

**Result:** 模型在合成数据和真实数据上的广泛评估中表现出色，特别是在空间异质噪声条件下。

**Conclusion:** 实验证明，该模型在合成和真实世界基准上都优于最先进的方法，特别是在空间异质噪声环境中。

**Abstract:** Despite notable advances in image denoising, existing models often struggle
to generalize beyond in-distribution noise patterns, particularly when
confronted with out-of-distribution (OOD) conditions characterized by spatially
variant noise. This generalization gap remains a fundamental yet underexplored
challenge. In this work, we investigate \emph{scale equivariance} as a core
inductive bias for improving OOD robustness. We argue that incorporating
scale-equivariant structures enables models to better adapt from training on
spatially uniform noise to inference on spatially non-uniform degradations.
Building on this insight, we propose a robust blind denoising framework
equipped with two key components: a Heterogeneous Normalization Module (HNM)
and an Interactive Gating Module (IGM). HNM stabilizes feature distributions
and dynamically corrects features under varying noise intensities, while IGM
facilitates effective information modulation via gated interactions between
signal and feature paths. Extensive evaluations demonstrate that our model
consistently outperforms state-of-the-art methods on both synthetic and
real-world benchmarks, especially under spatially heterogeneous noise. Code
will be made publicly available.

</details>


### [51] [Diffusion Models with Adaptive Negative Sampling Without External Resources](https://arxiv.org/abs/2508.02973)
*Alakh Desai,Nuno Vasconcelos*

Main category: cs.CV

> 本文提出了ANSWER技术，通过分类器无关引导改进扩散模型的采样过程，提高了图像生成的质量和准确性，无需训练且优于其他方法。

<details>
  <summary>Details</summary>

**Motivation:** 尽管扩散模型能够从文本提示生成多样化和高质量的图像，但它们在保真度和质量方面存在很大差异。因此，引入了负提示以增强图像对提示的依从性，该方法旨在改进这一过程。

**Method:** ANSWER方法通过探索负提示与分类器无关引导之间的关系来改进扩散模型的采样过程，无需外部资源即可同时考虑正负条件，从而提高图像生成的质量和准确性。

**Result:** 实验结果表明，在多个基准上，ANSWER与现有的扩散模型结合使用优于基线方法，并且人类更偏好该方法，其偏好程度是其他方法的两倍。

**Conclusion:** ANSWER是一种无需训练的技术，可以应用于任何支持分类器无关引导的模型，它通过利用扩散模型的内部否定理解来增加生成图像对提示的保真度。

**Abstract:** Diffusion models (DMs) have demonstrated an unparalleled ability to create
diverse and high-fidelity images from text prompts. However, they are also
well-known to vary substantially regarding both prompt adherence and quality.
Negative prompting was introduced to improve prompt compliance by specifying
what an image must not contain. Previous works have shown the existence of an
ideal negative prompt that can maximize the odds of the positive prompt. In
this work, we explore relations between negative prompting and classifier-free
guidance (CFG) to develop a sampling procedure, {\it Adaptive Negative Sampling
Without External Resources} (ANSWER), that accounts for both positive and
negative conditions from a single prompt. This leverages the internal
understanding of negation by the diffusion model to increase the odds of
generating images faithful to the prompt. ANSWER is a training-free technique,
applicable to any model that supports CFG, and allows for negative grounding of
image concepts without an explicit negative prompts, which are lossy and
incomplete. Experiments show that adding ANSWER to existing DMs outperforms the
baselines on multiple benchmarks and is preferred by humans 2x more over the
other methods.

</details>


### [52] [Separating Shared and Domain-Specific LoRAs for Multi-Domain Learning](https://arxiv.org/abs/2508.02978)
*Yusaku Takama,Ning Ding,Tatsuya Yokota,Toru Tamaki*

Main category: cs.CV

> 本文提出了一种新方法，确保共享和特定领域的LoRA存在于不同的子空间中，并在动作识别方面进行了实验，证明了这种方法的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的多领域学习架构有两种类型的适配器：所有领域共享的LoRA和每个特定领域的领域特定LoRA。然而，这种结构是否能有效捕捉特定领域的信息尚不清楚。

**Method:** 我们提出了一种方法，确保共享和特定领域的LoRA存在于不同的子空间中，具体而言，是在预训练权重的列子空间和左零子空间中。

**Result:** 我们通过将所提出的方法应用于动作识别实验，使用三个数据集（UCF101, Kinetics400, 和HMDB51）展现出了方法的有效性。

**Conclusion:** 通过分析LoRA权重的空间维度，证明所提出的方法能够有效在某些情况下提升模型的能力。

**Abstract:** Existing architectures of multi-domain learning have two types of adapters:
shared LoRA for all domains and domain-specific LoRA for each particular
domain. However, it remains unclear whether this structure effectively captures
domain-specific information. In this paper, we propose a method that ensures
that shared and domain-specific LoRAs exist in different subspaces;
specifically, the column and left null subspaces of the pre-trained weights. We
apply the proposed method to action recognition with three datasets (UCF101,
Kinetics400, and HMDB51) and demonstrate its effectiveness in some cases along
with the analysis of the dimensions of LoRA weights.

</details>


### [53] [MoExDA: Domain Adaptation for Edge-based Action Recognition](https://arxiv.org/abs/2508.02981)
*Takuya Sugimoto,Ning Ding,Toru Tamaki*

Main category: cs.CV

> 提出MoExDA方法以减轻RGB动作识别模型中的静态偏置问题，实验验证了其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 现代动作识别模型受静态偏置影响，导致泛化性能下降。

**Method:** MoExDA, 一种轻量级的RGB和边缘信息之间的领域适应方法，通过在RGB帧的基础上增加边缘帧来应对静态偏置问题。

**Result:** 实验表明，所提方法能有效抑制静态偏置，并且计算成本更低，相比之前的方法有更稳健的动作识别效果。

**Conclusion:** MoExDA方法能在保持较低计算成本的同时，有效改善动作识别的静态偏置问题，展现出更稳健的识别效果。

**Abstract:** Modern action recognition models suffer from static bias, leading to reduced
generalization performance. In this paper, we propose MoExDA, a lightweight
domain adaptation between RGB and edge information using edge frames in
addition to RGB frames to counter the static bias issue. Experiments
demonstrate that the proposed method effectively suppresses static bias with a
lower computational cost, allowing for more robust action recognition than
previous approaches.

</details>


### [54] [Adversarial Attention Perturbations for Large Object Detection Transformers](https://arxiv.org/abs/2508.02987)
*Zachary Yahn,Selim Furkan Tekin,Fatih Ilhan,Sihao Hu,Tiansheng Huang,Yichang Xu,Margaret Loper,Ling Liu*

Main category: cs.CV

> This paper presents AFOG, an effective adversarial attack method that targets both CNN and transformer-based object detectors, highlighting vulnerabilities with minimal and imperceptible perturbations.

<details>
  <summary>Details</summary>

**Motivation:** To develop a unified method that effectively attacks both transformer-based and CNN-based object detectors, filling the gap in existing adversarial techniques.

**Method:** Attention-Focused Offensive Gradient (AFOG) attack against object detection transformers, using a learnable attention mechanism to focus perturbations on vulnerable image areas.

**Result:** Extensive experiments on twelve large detection transformers on COCO show AFOG outperforms existing attacks on both transformer and CNN-based detectors by up to 83%, with better speed and imperceptibility.

**Conclusion:** AFOG provides a significant advancement in adversarial attacks against modern object detection systems by offering an effective, unified approach that is also efficient and stealthy.

**Abstract:** Adversarial perturbations are useful tools for exposing vulnerabilities in
neural networks. Existing adversarial perturbation methods for object detection
are either limited to attacking CNN-based detectors or weak against
transformer-based detectors. This paper presents an Attention-Focused Offensive
Gradient (AFOG) attack against object detection transformers. By design, AFOG
is neural-architecture agnostic and effective for attacking both large
transformer-based object detectors and conventional CNN-based detectors with a
unified adversarial attention framework. This paper makes three original
contributions. First, AFOG utilizes a learnable attention mechanism that
focuses perturbations on vulnerable image regions in multi-box detection tasks,
increasing performance over non-attention baselines by up to 30.6%. Second,
AFOG's attack loss is formulated by integrating two types of feature loss
through learnable attention updates with iterative injection of adversarial
perturbations. Finally, AFOG is an efficient and stealthy adversarial
perturbation method. It probes the weak spots of detection transformers by
adding strategically generated and visually imperceptible perturbations which
can cause well-trained object detection models to fail. Extensive experiments
conducted with twelve large detection transformers on COCO demonstrate the
efficacy of AFOG. Our empirical results also show that AFOG outperforms
existing attacks on transformer-based and CNN-based object detectors by up to
83% with superior speed and imperceptibility. Code is available at
https://github.com/zacharyyahn/AFOG.

</details>


### [55] [Seeing It Before It Happens: In-Generation NSFW Detection for Diffusion-Based Text-to-Image Models](https://arxiv.org/abs/2508.03006)
*Fan Yang,Yihao Huang,Jiayi Zhu,Ling Shi,Geguang Pu,Jin Song Dong,Kailong Wang*

Main category: cs.CV

> The paper introduces IGD, a technique using the predicted noise during diffusion to detect NSFW content, achieving high accuracy across various categories.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the challenge of detecting NSFW content during the image generation phase of diffusion models, which has not been explored much before.

**Method:** In-Generation Detection (IGD) leverages the predicted noise during the diffusion process to identify NSFW content, aiming to detect NSFW even in adversarially crafted prompts.

**Result:** Experiments show IGD achieves an average detection accuracy of 91.32% over naive and adversarial NSFW prompts, exceeding seven baseline methods.

**Conclusion:** IGD demonstrates strong effectiveness in detecting NSFW content during the generation phase of diffusion-based T2I models, providing a novel internal signal approach for this purpose.

**Abstract:** Diffusion-based text-to-image (T2I) models enable high-quality image
generation but also pose significant risks of misuse, particularly in producing
not-safe-for-work (NSFW) content. While prior detection methods have focused on
filtering prompts before generation or moderating images afterward, the
in-generation phase of diffusion models remains largely unexplored for NSFW
detection. In this paper, we introduce In-Generation Detection (IGD), a simple
yet effective approach that leverages the predicted noise during the diffusion
process as an internal signal to identify NSFW content. This approach is
motivated by preliminary findings suggesting that the predicted noise may
capture semantic cues that differentiate NSFW from benign prompts, even when
the prompts are adversarially crafted. Experiments conducted on seven NSFW
categories show that IGD achieves an average detection accuracy of 91.32% over
naive and adversarial NSFW prompts, outperforming seven baseline methods.

</details>


### [56] [Multi-Granularity Feature Calibration via VFM for Domain Generalized Semantic Segmentation](https://arxiv.org/abs/2508.03007)
*Xinhui Li,Xiaojie Guo*

Main category: cs.CV

> 本文提出了一种新的框架MGFC，通过多层次特征校准来提高视觉基础模型在不同领域下的语义分割任务中的适应性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的域泛化语义分割方法大多集中在全局特征调整，忽略了层次化适应在特征级别上对密集预测任务的重要性。

**Method:** MGFC方法通过从粗到细对视觉基础模型特征进行对齐，先校准粗粒度特征以捕获全局语义和场景结构，接着细化中粒度特征提升分类特征的判别性，最后通过增强高频空间细节校准细粒度特征。

**Result:** 实验结果表明，MGFC在基准数据集上超过了现有的DGSS方法。

**Conclusion:** 多粒度适应策略显著提升了语义分割任务的域泛化性能。

**Abstract:** Domain Generalized Semantic Segmentation (DGSS) aims to improve the
generalization ability of models across unseen domains without access to target
data during training. Recent advances in DGSS have increasingly exploited
vision foundation models (VFMs) via parameter-efficient fine-tuning strategies.
However, most existing approaches concentrate on global feature fine-tuning,
while overlooking hierarchical adaptation across feature levels, which is
crucial for precise dense prediction. In this paper, we propose
Multi-Granularity Feature Calibration (MGFC), a novel framework that performs
coarse-to-fine alignment of VFM features to enhance robustness under domain
shifts. Specifically, MGFC first calibrates coarse-grained features to capture
global contextual semantics and scene-level structure. Then, it refines
medium-grained features by promoting category-level feature discriminability.
Finally, fine-grained features are calibrated through high-frequency spatial
detail enhancement. By performing hierarchical and granularity-aware
calibration, MGFC effectively transfers the generalization strengths of VFMs to
the domain-specific task of DGSS. Extensive experiments on benchmark datasets
demonstrate that our method outperforms state-of-the-art DGSS approaches,
highlighting the effectiveness of multi-granularity adaptation for the semantic
segmentation task of domain generalization.

</details>


### [57] [Enhancing Long Video Question Answering with Scene-Localized Frame Grouping](https://arxiv.org/abs/2508.03009)
*Xuyi Yang,Wenhao Zhang,Hongbo Jin,Lin Liu,Hongbo Xu,Yongwei Nie,Fei Yu,Fei Ma*

Main category: cs.CV

> 提出了一种新的视频问答任务场景SceneQA和相应的LVSQA数据集，以及一种增强MLLMs在长视频理解能力的方法SLFG，该方法结合场景定位和动态帧重组机制，显著提升了长视频理解能力。

<details>
  <summary>Details</summary>

**Motivation:** 当前的多模态大型语言模型在长视频理解方面表现不佳，主要是由于资源限制不能处理所有的视频帧及其相关信息，现有的框架和评估任务侧重于从大量不必要的帧中识别包含核心对象的具体帧，这不适用于实际应用。为了应对这一问题，提出了一种新的视频问答任务场景，即SceneQA，重点在于场景细节感知和推理能力。

**Method:** SLFG (Semantic Localized Frame Grouping) 方法，该方法通过结合场景定位技术和动态帧重组机制，将独立帧合并为语义连贯的场景帧，以增强现有MLLMs在长视频理解中的能力。

**Result:** 实验结果表明，SLFG方法在多个长视频基准测试中表现出色。

**Conclusion:** SLFG方法无需修改原始模型架构，具有良好的可插拔性。代码和数据集将在http://www.slfg.pkuzwh.cn上发布。

**Abstract:** Current Multimodal Large Language Models (MLLMs) often perform poorly in long
video understanding, primarily due to resource limitations that prevent them
from processing all video frames and their associated information. Efficiently
extracting relevant information becomes a challenging task. Existing frameworks
and evaluation tasks focus on identifying specific frames containing core
objects from a large number of irrelevant frames, which does not align with the
practical needs of real-world applications. To address this issue, we propose a
new scenario under the video question-answering task, SceneQA, which emphasizes
scene-based detail perception and reasoning abilities. And we develop the LVSQA
dataset to support the SceneQA task, which is built upon carefully selected
videos from LVBench and contains a new collection of question-answer pairs to
promote a more fair evaluation of MLLMs' scene perception abilities in long
videos. Inspired by human cognition, we introduce a novel method called SLFG.
The core idea of SLFG is to combine individual frames into semantically
coherent scene frames. By leveraging scene localization methods and dynamic
frame reassembly mechanisms, SLFG significantly enhances the understanding
capabilities of existing MLLMs in long videos. SLFG requires no modification to
the original model architecture and boasts excellent plug-and-play usability.
Experimental results show that this method performs exceptionally well in
several long video benchmark tests. Code and dataset will be released at
http://www.slfg.pkuzwh.cn.

</details>


### [58] [SA-3DGS: A Self-Adaptive Compression Method for 3D Gaussian Splatting](https://arxiv.org/abs/2508.03017)
*Liheng Zhang,Weihao Yu,Zubo Lu,Haozhi Gu,Jin Huang*

Main category: cs.CV

> SA-3DGS：通过重要性评分剪枝和压缩高斯点，提高渲染质量和压缩效率，实现实用性的大幅提升。

<details>
  <summary>Details</summary>

**Motivation:** 虽然现有方法可以压缩高斯模型，但在识别真正不重要的高斯点方面存在困难，导致后续的高斯剪枝、压缩质量和渲染性能下降。为解决这一问题，提出了SA-3DGS方法，以显著降低存储成本，同时保持渲染质量。

**Method:** SA-3DGS, 通过学习重要性评分来自动识别最小化高斯点，实现有效的剪枝和冗余减少。重要性感知聚类模块将高斯属性更准确地压缩到代码本中，提高代码本的表现力，减少模型大小。代码本修复模块利用场景上下文信息修复代码本，恢复原始高斯点属性，降低因信息丢失带来的渲染质量退化。

**Result:** 实验结果表明，该方法能够在保持或改善渲染质量的同时，实现高达66倍的压缩率。提出的高斯剪枝方法不仅适用于其他剪枝方法（如LightGaussian），还展示了优秀的性能和强大的泛化能力。

**Conclusion:** SA-3DGS方法通过学习重要性评分进行有效的高斯点剪枝和压缩，同时使用代码本修复模块恢复渲染质量，能够在大幅压缩模型大小的同时保持或改善渲染质量。

**Abstract:** Recent advancements in 3D Gaussian Splatting have enhanced efficient and
high-quality novel view synthesis. However, representing scenes requires a
large number of Gaussian points, leading to high storage demands and limiting
practical deployment. The latest methods facilitate the compression of Gaussian
models but struggle to identify truly insignificant Gaussian points in the
scene, leading to a decline in subsequent Gaussian pruning, compression
quality, and rendering performance. To address this issue, we propose SA-3DGS,
a method that significantly reduces storage costs while maintaining rendering
quality. SA-3DGS learns an importance score to automatically identify the least
significant Gaussians in scene reconstruction, thereby enabling effective
pruning and redundancy reduction. Next, the importance-aware clustering module
compresses Gaussians attributes more accurately into the codebook, improving
the codebook's expressive capability while reducing model size. Finally, the
codebook repair module leverages contextual scene information to repair the
codebook, thereby recovering the original Gaussian point attributes and
mitigating the degradation in rendering quality caused by information loss.
Experimental results on several benchmark datasets show that our method
achieves up to 66x compression while maintaining or even improving rendering
quality. The proposed Gaussian pruning approach is not only adaptable to but
also improves other pruning-based methods (e.g., LightGaussian), showcasing
excellent performance and strong generalization ability.

</details>


### [59] [MoCA: Identity-Preserving Text-to-Video Generation via Mixture of Cross Attention](https://arxiv.org/abs/2508.03034)
*Qi Xie,Yongjia Ma,Donglin Di,Xuehao Gao,Xun Yang*

Main category: cs.CV

> MoCA, a novel Video Diffusion Model with a special attention mechanism and hierarchical temporal pooling, outperforms current text-to-video generation methods by enhancing identity coherence and fine-grained details.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this study is to address the challenge of maintaining identity coherence and capturing fine-grained facial dynamics in text-to-video (T2V) generation where existing models are still lacking, despite recent advances.

**Method:** The paper proposes MoCA, a Video Diffusion Model built on a Diffusion Transformer (DiT) backbone. It incorporates a Mixture of Cross-Attention mechanism and applies Hierarchical Temporal Pooling and Temporal-Aware Cross-Attention Experts to improve inter-frame identity consistency and fine-grained facial dynamics. A Latent Video Perceptual Loss is also used to enhance identity coherence and fine-grained details.

**Result:** Experiments on a newly collected dataset, CelebIPVid, demonstrate MoCA's superior performance over existing T2V methods in terms of identity coherence and fine-grained facial dynamics.

**Conclusion:** MoCA outperforms existing text-to-video (T2V) methods by more than 5% in Face similarity measurements, indicating better performance in maintaining temporal identity coherence and capturing fine-grained facial dynamics.

**Abstract:** Achieving ID-preserving text-to-video (T2V) generation remains challenging
despite recent advances in diffusion-based models. Existing approaches often
fail to capture fine-grained facial dynamics or maintain temporal identity
coherence. To address these limitations, we propose MoCA, a novel Video
Diffusion Model built on a Diffusion Transformer (DiT) backbone, incorporating
a Mixture of Cross-Attention mechanism inspired by the Mixture-of-Experts
paradigm. Our framework improves inter-frame identity consistency by embedding
MoCA layers into each DiT block, where Hierarchical Temporal Pooling captures
identity features over varying timescales, and Temporal-Aware Cross-Attention
Experts dynamically model spatiotemporal relationships. We further incorporate
a Latent Video Perceptual Loss to enhance identity coherence and fine-grained
details across video frames. To train this model, we collect CelebIPVid, a
dataset of 10,000 high-resolution videos from 1,000 diverse individuals,
promoting cross-ethnicity generalization. Extensive experiments on CelebIPVid
show that MoCA outperforms existing T2V methods by over 5% across Face
similarity.

</details>


### [60] [VideoForest: Person-Anchored Hierarchical Reasoning for Cross-Video Question Answering](https://arxiv.org/abs/2508.03039)
*Yiran Meng,Junhong Ye,Wei Zhou,Guanghui Yue,Xudong Mao,Ruomei Wang,Baoquan Zhao*

Main category: cs.CV

> VideoForest是一个通过人为主体的层次化推理方法来解决跨视频理解问题的新型框架，基于人为主体的特征来联系多个视频，通过多粒度跨度树结构和多代理推理框架达到高效理解，并在跨视频理解任务中表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 跨视频问答比传统的单视频理解更具挑战性，特别是在建立视频流间的有意义联系和管理多源信息检索的复杂性方面。

**Method:** 通过人为主体的层次化推理方法，VideoForest框架解决了跨视频理解的挑战，包括使用ReID和跟踪算法进行人为主体的特征提取，建立多视频源间稳定的空间时间关系；基于人为主体轨迹的多粒度跨度树结构；以及高效遍历此层次结构的多代理推理框架。

**Result:** 在跨视频理解任务中，VideoForest显示出了卓越的性能，达到了71.93%的人脸识别，83.75%的行为分析和51.67%的总结和推理的准确率，显著优于现有方法。

**Conclusion:** 通过将多视频流统一到人为主体的特征中，VideoForest提供了一个新的跨视频理解范式，可以对分布式的视觉信息进行复杂推理，同时保持计算效率。

**Abstract:** Cross-video question answering presents significant challenges beyond
traditional single-video understanding, particularly in establishing meaningful
connections across video streams and managing the complexity of multi-source
information retrieval. We introduce VideoForest, a novel framework that
addresses these challenges through person-anchored hierarchical reasoning. Our
approach leverages person-level features as natural bridge points between
videos, enabling effective cross-video understanding without requiring
end-to-end training. VideoForest integrates three key innovations: 1) a
human-anchored feature extraction mechanism that employs ReID and tracking
algorithms to establish robust spatiotemporal relationships across multiple
video sources; 2) a multi-granularity spanning tree structure that
hierarchically organizes visual content around person-level trajectories; and
3) a multi-agent reasoning framework that efficiently traverses this
hierarchical structure to answer complex cross-video queries. To evaluate our
approach, we develop CrossVideoQA, a comprehensive benchmark dataset
specifically designed for person-centric cross-video analysis. Experimental
results demonstrate VideoForest's superior performance in cross-video reasoning
tasks, achieving 71.93% accuracy in person recognition, 83.75% in behavior
analysis, and 51.67% in summarization and reasoning, significantly
outperforming existing methods. Our work establishes a new paradigm for
cross-video understanding by unifying multiple video streams through
person-level features, enabling sophisticated reasoning across distributed
visual information while maintaining computational efficiency.

</details>


### [61] [Multi-human Interactive Talking Dataset](https://arxiv.org/abs/2508.03050)
*Zeyu Zhu,Weijia Wu,Mike Zheng Shou*

Main category: cs.CV

> The paper presents MIT, a new dataset for multi-human talking video generation, and introduces CovOG, a baseline model that establishes this as a valuable benchmark for future research.

<details>
  <summary>Details</summary>

**Motivation:** The primary motivation is to address the limitation of existing studies that focus on single-person monologues or isolated facial animations, thereby expanding the applicability to more realistic multi-human interactions.

**Method:** The paper introduces a dataset called MIT for multi-human talking video generation, which includes an automatic pipeline for collecting and annotating multi-person conversational videos. Furthermore, it proposes a baseline model called CovOG, integrating Multi-Human Pose Encoder (MPE) and Interactive Audio Driver (IAD).

**Result:** The MIT dataset comprises 12 hours of high-resolution footage of 2-4 speakers with fine-grained annotations, demonstrating interactive visual behaviors. The CovOG model effectively generates multi-human talking videos.

**Conclusion:** The MIT dataset and the CovOG model together demonstrate the potential and challenges of generating realistic multi-human talking videos, setting a benchmark for future research in this area.

**Abstract:** Existing studies on talking video generation have predominantly focused on
single-person monologues or isolated facial animations, limiting their
applicability to realistic multi-human interactions. To bridge this gap, we
introduce MIT, a large-scale dataset specifically designed for multi-human
talking video generation. To this end, we develop an automatic pipeline that
collects and annotates multi-person conversational videos. The resulting
dataset comprises 12 hours of high-resolution footage, each featuring two to
four speakers, with fine-grained annotations of body poses and speech
interactions. It captures natural conversational dynamics in multi-speaker
scenario, offering a rich resource for studying interactive visual behaviors.
To demonstrate the potential of MIT, we furthur propose CovOG, a baseline model
for this novel task. It integrates a Multi-Human Pose Encoder (MPE) to handle
varying numbers of speakers by aggregating individual pose embeddings, and an
Interactive Audio Driver (IAD) to modulate head dynamics based on
speaker-specific audio features. Together, these components showcase the
feasibility and challenges of generating realistic multi-human talking videos,
establishing MIT as a valuable benchmark for future research. The code is
avalibale at: https://github.com/showlab/Multi-human-Talking-Video-Dataset.

</details>


### [62] [Uncertainty-Guided Face Matting for Occlusion-Aware Face Transformation](https://arxiv.org/abs/2508.03055)
*Hyebin Cho,Jaehyup Lee*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Face filters have become a key element of short-form video content, enabling
a wide array of visual effects such as stylization and face swapping. However,
their performance often degrades in the presence of occlusions, where objects
like hands, hair, or accessories obscure the face. To address this limitation,
we introduce the novel task of face matting, which estimates fine-grained alpha
mattes to separate occluding elements from facial regions. We further present
FaceMat, a trimap-free, uncertainty-aware framework that predicts high-quality
alpha mattes under complex occlusions. Our approach leverages a two-stage
training pipeline: a teacher model is trained to jointly estimate alpha mattes
and per-pixel uncertainty using a negative log-likelihood (NLL) loss, and this
uncertainty is then used to guide the student model through spatially adaptive
knowledge distillation. This formulation enables the student to focus on
ambiguous or occluded regions, improving generalization and preserving semantic
consistency. Unlike previous approaches that rely on trimaps or segmentation
masks, our framework requires no auxiliary inputs making it well-suited for
real-time applications. In addition, we reformulate the matting objective by
explicitly treating skin as foreground and occlusions as background, enabling
clearer compositing strategies. To support this task, we newly constructed
CelebAMat, a large-scale synthetic dataset specifically designed for
occlusion-aware face matting. Extensive experiments show that FaceMat
outperforms state-of-the-art methods across multiple benchmarks, enhancing the
visual quality and robustness of face filters in real-world, unconstrained
video scenarios. The source code and CelebAMat dataset are available at
https://github.com/hyebin-c/FaceMat.git

</details>


### [63] [CHARM: Collaborative Harmonization across Arbitrary Modalities for Modality-agnostic Semantic Segmentation](https://arxiv.org/abs/2508.03060)
*Lekang Wen,Jing Xiao,Liang Liao,Jiajun Chen,Mi Wang*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Modality-agnostic Semantic Segmentation (MaSS) aims to achieve robust scene
understanding across arbitrary combinations of input modality. Existing methods
typically rely on explicit feature alignment to achieve modal homogenization,
which dilutes the distinctive strengths of each modality and destroys their
inherent complementarity. To achieve cooperative harmonization rather than
homogenization, we propose CHARM, a novel complementary learning framework
designed to implicitly align content while preserving modality-specific
advantages through two components: (1) Mutual Perception Unit (MPU), enabling
implicit alignment through window-based cross-modal interaction, where
modalities serve as both queries and contexts for each other to discover
modality-interactive correspondences; (2) A dual-path optimization strategy
that decouples training into Collaborative Learning Strategy (CoL) for
complementary fusion learning and Individual Enhancement Strategy (InE) for
protected modality-specific optimization. Experiments across multiple datasets
and backbones indicate that CHARM consistently outperform the baselines, with
significant increment on the fragile modalities. This work shifts the focus
from model homogenization to harmonization, enabling cross-modal
complementarity for true harmony in diversity.

</details>


### [64] [CORE-ReID: Comprehensive Optimization and Refinement through Ensemble fusion in Domain Adaptation for person re-identification](https://arxiv.org/abs/2508.03064)
*Trinh Quoc Nguyen,Oky Dicky Ardiansyah Prima,Katsuyoshi Hotta*

Main category: cs.CV

> CORE-ReID, a novel framework introduced for UDA in Person Re-identification, significantly enhances performance in terms of methods such as CycleGAN, teacher-student networks, and an Ensemble Fusion component aiding in achieving clarity in feature fusion and high accuracy for re-identification.

<details>
  <summary>Details</summary>

**Motivation:** The aim of the research is to enhance the performance of UDA in Person Re-identification by addressing the issue of different data characteristics from different camera sources and by deriving effective pseudo labels to achieve high accuracy in re-identification tasks.

**Method:** The paper introduces CORE-ReID, a framework designed for Unsupervised Domain Adaptation (UDA) in Person Re-identification (ReID). It employs CycleGAN in the pre-training phase for simulating diverse data from different camera angles. Two stages of teachers-student network learning and a novel Ensemble Fusion component that utilizes fine-grained local information alongside global features are implemented for successful adaptation and comprehensive learning. Enhancements include Efficient Channel Attention Block and Bidirectional Mean Feature Normalization.

**Result:** The experimental results indicate a notable improvement over state-of-the-art approaches in terms of Mean Average Precision and other top-k measures.

**Conclusion:** The proposed framework showcases a significant advancement in the UDA for Person ReID, ensuring clear feature fusion, avoiding ambiguity, and achieving high accuracy in re-identification tasks, positioning it as an effective solution for the UDA problem in Person ReID.

**Abstract:** This study introduces a novel framework, "Comprehensive Optimization and
Refinement through Ensemble Fusion in Domain Adaptation for Person
Re-identification (CORE-ReID)", to address an Unsupervised Domain Adaptation
(UDA) for Person Re-identification (ReID). The framework utilizes CycleGAN to
generate diverse data that harmonizes differences in image characteristics from
different camera sources in the pre-training stage. In the fine-tuning stage,
based on a pair of teacher-student networks, the framework integrates
multi-view features for multi-level clustering to derive diverse pseudo labels.
A learnable Ensemble Fusion component that focuses on fine-grained local
information within global features is introduced to enhance learning
comprehensiveness and avoid ambiguity associated with multiple pseudo-labels.
Experimental results on three common UDAs in Person ReID demonstrate
significant performance gains over state-of-the-art approaches. Additional
enhancements, such as Efficient Channel Attention Block and Bidirectional Mean
Feature Normalization mitigate deviation effects and adaptive fusion of global
and local features using the ResNet-based model, further strengthening the
framework. The proposed framework ensures clarity in fusion features, avoids
ambiguity, and achieves high ac-curacy in terms of Mean Average Precision,
Top-1, Top-5, and Top-10, positioning it as an advanced and effective solution
for the UDA in Person ReID. Our codes and models are available at
https://github.com/TrinhQuocNguyen/CORE-ReID.

</details>


### [65] [SSFMamba: Symmetry-driven Spatial-Frequency Feature Fusion for 3D Medical Image Segmentation](https://arxiv.org/abs/2508.03069)
*Bo Zhang,Yifan Zhang,Shuo Yan,Yu Bai,Zheng Zhang,Wu Liu,Xiuzhuang Zhou,Wendong Wang*

Main category: cs.CV

> 提出了SSFMamba，一种基于Mamba驱动的对称空间-频率特征融合网络，用于3D医学图像分割。

<details>
  <summary>Details</summary>

**Motivation:** 由于空间域在3D医学图像分割中对全局上下文建模的容量有限，新兴的方法开始结合频率域表示。然而，常规的特征提取策略常常忽略频率域信息的独特性质，如共轭对称性，以及空间域和频率域数据分布的基本差异，这可能最终会稀释或掩盖基于频率表示的互补优势。

**Method:** SSFMamba采用了一种互补的双分支架构，分别从空间域和频率域提取特征，并利用Mamba块融合这些异构特征，以保留全局上下文并强化局部细节。在频率域分支中，通过Mamba强大的提取全局上下文信息的能力，以及频率域特征的协同效应，进一步增强全局建模能力。此外，设计了一种3D多方向扫描机制来加强局部和全局线索的融合。

**Result:** 在BraTS2020和BraTS2023数据集上的广泛实验表明，该方法在各种评估指标上始终优于现有的最先进的方法。

**Conclusion:** SSFMamba通过融合空间域和频率域特征，能够有效地保留全局上下文并强化局部细节，从而在3D医学图像分割中表现优于现有方法。

**Abstract:** In light of the spatial domain's limited capacity for modeling global context
in 3D medical image segmentation, emerging approaches have begun to incorporate
frequency domain representations. However, straightforward feature extraction
strategies often overlook the unique properties of frequency domain
information, such as conjugate symmetry. They also fail to account for the
fundamental differences in data distribution between the spatial and frequency
domains, which can ultimately dilute or obscure the complementary strengths
that frequency-based representations offer. In this paper, we propose SSFMamba,
a Mamba based Symmetry-driven Spatial-Frequency feature fusion network for 3D
medical image segmentation. SSFMamba employs a complementary dual-branch
architecture that extracts features from both the spatial and frequency
domains, and leverages a Mamba block to fuse these heterogeneous features to
preserve global context while reinforcing local details. In the frequency
domain branch, we harness Mamba's exceptional capability to extract global
contextual information in conjunction with the synergistic effect of frequency
domain features to further enhance global modeling. Moreover, we design a 3D
multi-directional scanning mechanism to strengthen the fusion of local and
global cues. Extensive experiments on the BraTS2020 and BraTS2023 datasets
demonstrate that our approach consistently outperforms state-of-the-art methods
across various evaluation metrics.

</details>


### [66] [RobustGS: Unified Boosting of Feedforward 3D Gaussian Splatting under Low-Quality Conditions](https://arxiv.org/abs/2508.03077)
*Anran Wu,Long Peng,Xin Di,Xueyuan Dai,Chen Wu,Yang Wang,Xueyang Fu,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

> This paper presents a novel module, RobustGS, aimed at improving the robustness of feedforward 3D Gaussian Splatting for high-quality 3D reconstruction under real-world imaging conditions, achieving state-of-the-art results.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the limitations of existing feedforward 3D Gaussian Splatting methods when faced with images of poor quality due to real-world challenges such as noise, low light, or rain, which lead to inaccurate reconstructions.

**Method:** Our method introduces a multi-view feature enhancement module named RobustGS, designed to improve the robustness of feedforward 3D Gaussian Splatting under various adverse imaging conditions. It includes a Generalized Degradation Learner to extract generic degradation representations and a semantic-aware state-space model to enhance and aggregate information.

**Result:** Experiments show that the proposed approach significantly enhances 3D reconstruction quality across a range of degradation types when integrated into existing methods.

**Conclusion:** The paper concludes that the introduced RobustGS module effectively improves the robustness of 3D Gaussian Splatting and can be applied in a plug-and-play manner to enhance the reconstruction quality of existing models under various adverse imaging conditions.

**Abstract:** Feedforward 3D Gaussian Splatting (3DGS) overcomes the limitations of
optimization-based 3DGS by enabling fast and high-quality reconstruction
without the need for per-scene optimization. However, existing feedforward
approaches typically assume that input multi-view images are clean and
high-quality. In real-world scenarios, images are often captured under
challenging conditions such as noise, low light, or rain, resulting in
inaccurate geometry and degraded 3D reconstruction. To address these
challenges, we propose a general and efficient multi-view feature enhancement
module, RobustGS, which substantially improves the robustness of feedforward
3DGS methods under various adverse imaging conditions, enabling high-quality 3D
reconstruction. The RobustGS module can be seamlessly integrated into existing
pretrained pipelines in a plug-and-play manner to enhance reconstruction
robustness. Specifically, we introduce a novel component, Generalized
Degradation Learner, designed to extract generic representations and
distributions of multiple degradations from multi-view inputs, thereby
enhancing degradation-awareness and improving the overall quality of 3D
reconstruction. In addition, we propose a novel semantic-aware state-space
model. It first leverages the extracted degradation representations to enhance
corrupted inputs in the feature space. Then, it employs a semantic-aware
strategy to aggregate semantically similar information across different views,
enabling the extraction of fine-grained cross-view correspondences and further
improving the quality of 3D representations. Extensive experiments demonstrate
that our approach, when integrated into existing methods in a plug-and-play
manner, consistently achieves state-of-the-art reconstruction quality across
various types of degradations.

</details>


### [67] [Exploring Fairness across Fine-Grained Attributes in Large Vision-Language Models](https://arxiv.org/abs/2508.03079)
*Zaiying Zhao,Toshihiko Yamasaki*

Main category: cs.CV

> 研究利用大规模语言模型构建知识库，评估大规模视觉语言模型在多种属性上的公平性，发现文化、环境和行为因素对模型决策有显著影响。

<details>
  <summary>Details</summary>

**Motivation:** 当前研究主要集中在种族和性别等人际属性上的公平性问题，对于更广泛属性上的公平性探讨较少。本研究旨在填补这一空白。

**Method:** 我们利用大规模语言模型构建了一个开放集知识库，来识别偏见属性，并利用这些属性评估大规模视觉语言模型在更细粒度属性上的公平性。

**Result:** 实验结果显示大规模视觉语言模型在多样化的属性上表现出有偏见的结果，文化、环境和行为因素对模型决策的影响比传统的种族和性别等人口统计属性更为显著。

**Conclusion:** 研究强调，要全面理解LVLMs的公平性问题，需要考虑文化、环境和行为等更为广泛的属性。

**Abstract:** The rapid expansion of applications using Large Vision-Language Models
(LVLMs), such as GPT-4o, has raised significant concerns about their fairness.
While existing studies primarily focus on demographic attributes such as race
and gender, fairness across a broader range of attributes remains largely
unexplored. In this study, we construct an open-set knowledge base of bias
attributes leveraging Large Language Models (LLMs) and evaluate the fairness of
LVLMs across finer-grained attributes. Our experimental results reveal that
LVLMs exhibit biased outputs across a diverse set of attributes and further
demonstrate that cultural, environmental, and behavioral factors have a more
pronounced impact on LVLM decision-making than traditional demographic
attributes.

</details>


### [68] [Contrastive Cross-Bag Augmentation for Multiple Instance Learning-based Whole Slide Image Classification](https://arxiv.org/abs/2508.03081)
*Bo Zhang,Xu Xinan,Shuo Yan,Yu Bai,Zheng Zhang,Wufan Wang,Wendong Wang*

Main category: cs.CV

> 提出对比交叉包增强($C^2Aug$)方法，通过增加包间多样性并引入对比学习框架，提升了MIL在WSI分类中的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的伪包增强方法由于从有限数量的包中抽样，导致多样性受限。

**Method:** 我们提出了对比交叉包增强（$C^2Aug$）方法，通过从具有相同类别的所有包中采样实例来增加伪包的多样性。此外，为了解决引入更多关键实例后模型性能受限的问题，我们引入了包级别和组级别的对比学习框架，以增强具有不同语义意义特征的区分性。

**Result:** 实验结果表明，$C^2Aug$ 在多个评估指标上始终优于最先进的方法。

**Conclusion:** 通过增加包间多样性和引入对比学习框架，$C^2Aug$ 能够有效地提高模型性能，特别是在测试图像肿瘤区域较小的情况下表现更佳。

**Abstract:** Recent pseudo-bag augmentation methods for Multiple Instance Learning
(MIL)-based Whole Slide Image (WSI) classification sample instances from a
limited number of bags, resulting in constrained diversity. To address this
issue, we propose Contrastive Cross-Bag Augmentation ($C^2Aug$) to sample
instances from all bags with the same class to increase the diversity of
pseudo-bags. However, introducing new instances into the pseudo-bag increases
the number of critical instances (e.g., tumor instances). This increase results
in a reduced occurrence of pseudo-bags containing few critical instances,
thereby limiting model performance, particularly on test slides with small
tumor areas. To address this, we introduce a bag-level and group-level
contrastive learning framework to enhance the discrimination of features with
distinct semantic meanings, thereby improving model performance. Experimental
results demonstrate that $C^2Aug$ consistently outperforms state-of-the-art
approaches across multiple evaluation metrics.

</details>


### [69] [Augmenting Continual Learning of Diseases with LLM-Generated Visual Concepts](https://arxiv.org/abs/2508.03094)
*Jiantao Tan,Peixian Ma,Kanghao Chen,Zhiming Dai,Ruixuan Wang*

Main category: cs.CV

> A novel continual learning framework for medical image classification that uses visual concepts from large language models to provide semantic guidance, showing state-of-the-art performance.

<details>
  <summary>Details</summary>

**Motivation:** To enhance continual learning for medical image classification systems by integrating richer semantic information from visual concepts generated by LLMs, going beyond simple class name templates used in current methods.

**Method:** Our method dynamically constructs a visual concept pool using a similarity-based filtering mechanism to prevent redundancy and employs a cross-modal image-concept attention module to integrate visual concepts into continual learning, with an attention loss.

**Result:** Experiments on medical and natural image datasets demonstrate state-of-the-art performance, proving the effectiveness of the method.

**Conclusion:** The proposed framework demonstrates the superiority of combining multimodal information through semantic guidance provided by visual concepts, achieving better continual learning performance.

**Abstract:** Continual learning is essential for medical image classification systems to
adapt to dynamically evolving clinical environments. The integration of
multimodal information can significantly enhance continual learning of image
classes. However, while existing approaches do utilize textual modality
information, they solely rely on simplistic templates with a class name,
thereby neglecting richer semantic information. To address these limitations,
we propose a novel framework that harnesses visual concepts generated by large
language models (LLMs) as discriminative semantic guidance. Our method
dynamically constructs a visual concept pool with a similarity-based filtering
mechanism to prevent redundancy. Then, to integrate the concepts into the
continual learning process, we employ a cross-modal image-concept attention
module, coupled with an attention loss. Through attention, the module can
leverage the semantic knowledge from relevant visual concepts and produce
class-representative fused features for classification. Experiments on medical
and natural image datasets show our method achieves state-of-the-art
performance, demonstrating the effectiveness and superiority of our method. We
will release the code publicly.

</details>


### [70] [AVATAR: Reinforcement Learning to See, Hear, and Reason Over Video](https://arxiv.org/abs/2508.03100)
*Yogesh Kulkarni,Pooyan Fazli*

Main category: cs.CV

> AVATAR框架解决了多模式长时视频推理中的几个关键问题，包括数据效率低下、优势消亡问题以及均匀的信用分配，通过离线训练架构和时间优势塑造策略提高了性能和样本效率。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于多模式长时视频推理的挑战，比如对精确的时空融合和跨模式对齐的需求，该研究旨在解决现有方法，尤其是GRPO，在数据使用效率、优势消亡及均匀信用分配上的不足。

**Method:** AVATAR采用了一种离线训练架构，提高了样本效率，并通过重用过去的经验解决了优势消亡问题。同时，它引入了时间优势塑造（TAS）策略，该策略在学习过程中强调关键推理阶段的权重。

**Result:** AVATAR在多种基准测试中表现出色，相比Qwen2.5-Omni基线模型在MMVU上提升了5.4%,在Omnibench上提升了4.9%,在Video-Holmes上提升了4.5%，并展示了超过35%的样本效率提升。

**Conclusion:** AVATAR框架通过改进的训练架构和时间优势塑造策略，在多模式长时视频推理任务中解决了关键问题，显著提升了性能和样本效率。

**Abstract:** Multimodal reasoning over long-horizon video is challenging due to the need
for precise spatiotemporal fusion and alignment across modalities. While recent
methods such as Group Relative Policy Optimization (GRPO) have shown promise in
this domain, they suffer from three key limitations: (1) data inefficiency from
their on-policy design, (2) a vanishing advantage problem, where identical or
near-identical rewards within a group eliminate the learning signal by
producing zero-valued advantages, and (3) uniform credit assignment that fails
to emphasize critical reasoning steps. We introduce AVATAR (Audio-Video Agent
for Alignment and Reasoning), a framework that addresses these limitations
through two core components: (1) an off-policy training architecture that
improves sample efficiency and resolves vanishing advantages by reusing past
experiences with greater reward diversity, and (2) Temporal Advantage Shaping
(TAS), a novel credit assignment strategy that upweights key reasoning phases
during learning. AVATAR achieves strong performance across various benchmarks,
outperforming the Qwen2.5-Omni baseline by +5.4on MMVU, +4.9 on OmniBench, and
+4.5 on Video-Holmes, while demonstrating over 35% higher sample efficiency.

</details>


### [71] [Causal Disentanglement and Cross-Modal Alignment for Enhanced Few-Shot Learning](https://arxiv.org/abs/2508.03102)
*Tianjiao Jiang,Zhen Zhang,Yuhang Liu,Javen Qinfeng Shi*

Main category: cs.CV

> The paper introduces the Causal CLIP Adapter (CCA), a method that disentangles CLIP's visual features using ICA and enhances cross-modal alignment, demonstrating superior few-shot performance and robustness on 11 benchmark datasets.

<details>
  <summary>Details</summary>

**Motivation:** Unlike most FSL methods which rely on entangled representations, CCA disentangles latent representations via ICA, making effective use of limited labeled data possible.

**Method:** The Causal CLIP Adapter (CCA) uses unsupervised Independent Component Analysis (ICA) to disentangle visual features from CLIP, reducing the number of trainable parameters and mitigating overfitting. It enhances the CLIP's intrinsic cross-modal alignment using a unidirectional text classifier and a bidirectional cross-attention mechanism.

**Result:** Experiments on 11 benchmark datasets show that the CCA approach outperforms state-of-the-art methods in few-shot learning and is robust to distributional shifts.

**Conclusion:** CCA provides a robust and efficient approach to few-shot learning, offering an improvement over existing methods by disentangling representations and enhancing cross-modal alignment.

**Abstract:** Few-shot learning (FSL) often requires effective adaptation of models using
limited labeled data. However, most existing FSL methods rely on entangled
representations, requiring the model to implicitly recover the unmixing process
to obtain disentangled representations using only limited supervision, which
hinders effective adaptation. Recent theoretical studies show that multimodal
contrastive learning methods, such as CLIP, can disentangle latent
representations up to linear transformations. In light of this, we propose the
Causal CLIP Adapter (CCA), a novel framework that explicitly disentangles
visual features extracted from CLIP using unsupervised Independent Component
Analysis (ICA). This removes the need to learn the unmixing process from the
labeled data, thereby reducing the number of trainable parameters and
mitigating overfitting. Taking a step further, while ICA can obtain visual
disentangled representations, it may also disrupt CLIP's intra- and inter-modal
alignment. To counteract this, CCA further leverages CLIP's inherent
cross-modal alignment by enhancing it in two ways: unidirectionally, through
fine-tuning a CLIP-based text classifier, and bidirectionally, via a
cross-attention mechanism that enriches visual and textual representations
through mutual interaction. Both unimodal and cross-modal classification
outputs can be effectively combined linearly to improve classification
accuracy. Extensive experiments on 11 benchmark datasets demonstrate that our
method consistently outperforms state-of-the-art approaches in terms of
few-shot performance and robustness to distributional shifts, while maintaining
computational efficiency. Code will be available at
https://github.com/tianjiao-j/CCA.

</details>


### [72] [H3R: Hybrid Multi-view Correspondence for Generalizable 3D Reconstruction](https://arxiv.org/abs/2508.03118)
*Heng Jia,Linchao Zhu,Na Zhao*

Main category: cs.CV

> The paper introduces H3R, a hybrid 3D reconstruction framework that combines volumetric latent fusion with attention-based feature aggregation, improving efficiency and performance over existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to tackle the challenges in 3D reconstruction, specifically in multi-view correspondence modeling, by addressing the fundamental trade-off between explicit and implicit methods.

**Method:** The method involves an efficient latent volume for enforcing geometric consistency and a camera-aware Transformer using Pl"ucker coordinates for adaptive correspondence refinement.

**Result:** H3R converges twice as fast as existing methods and achieves state-of-the-art performance with significant PSNR improvements on multiple benchmarks.

**Conclusion:** The study concludes that H3R offers an effective solution to 3D reconstruction, demonstrating robust performance and cross-dataset generalization.

**Abstract:** Despite recent advances in feed-forward 3D Gaussian Splatting, generalizable
3D reconstruction remains challenging, particularly in multi-view
correspondence modeling. Existing approaches face a fundamental trade-off:
explicit methods achieve geometric precision but struggle with ambiguous
regions, while implicit methods provide robustness but suffer from slow
convergence. We present H3R, a hybrid framework that addresses this limitation
by integrating volumetric latent fusion with attention-based feature
aggregation. Our framework consists of two complementary components: an
efficient latent volume that enforces geometric consistency through epipolar
constraints, and a camera-aware Transformer that leverages Pl\"ucker
coordinates for adaptive correspondence refinement. By integrating both
paradigms, our approach enhances generalization while converging 2$\times$
faster than existing methods. Furthermore, we show that spatial-aligned
foundation models (e.g., SD-VAE) substantially outperform semantic-aligned
models (e.g., DINOv2), resolving the mismatch between semantic representations
and spatial reconstruction requirements. Our method supports variable-number
and high-resolution input views while demonstrating robust cross-dataset
generalization. Extensive experiments show that our method achieves
state-of-the-art performance across multiple benchmarks, with significant PSNR
improvements of 0.59 dB, 1.06 dB, and 0.22 dB on the RealEstate10K, ACID, and
DTU datasets, respectively. Code is available at
https://github.com/JiaHeng-DLUT/H3R.

</details>


### [73] [Landsat30-AU: A Vision-Language Dataset for Australian Landsat Imagery](https://arxiv.org/abs/2508.03127)
*Sai Ma,Zhuang Li,John A Taylor*

Main category: cs.CV

> 论文构建了Landsat30-AU数据集，用于提升现有视觉语言模型在长时间跨度、低分辨率卫星图像的理解能力，研究表明，通过轻量级微调现有VLM模型可以显著提升其在特定任务上的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的数据集主要关注短期、高分辨率的卫星数据，忽略了对于长期监测更具成本效益和减少偏差的低分辨率、多卫星数据集的重要性。

**Method:** 提出了Landsat30-AU数据集，包括Landsat30-AU-Cap和Landsat30-AU-VQA两个部分，使用了自举管道结合人机协作保证数据质量。

**Result:** 开源遥感视觉语言模型EarthDial在描述和VQA任务上的表现较差，但对Qwen2.5-VL-7B进行轻量级微调后，其在Landsat30-AU上的描述和VQA性能均有提升。

**Conclusion:** 研究展示了现有VLM模型在处理卫星图像方面的局限性，并通过专注于低分辨率、长期存档的多卫星数据集Landsat30-AU，改善了模型在视觉语言任务上的性能。

**Abstract:** Vision language models (VLMs) that enable natural language interaction with
satellite imagery can democratize Earth observation by accelerating expert
workflows, making data accessible to non-specialists, and enabling planet-scale
automation. However, existing datasets focus mainly on short-term,
high-resolution imagery from a limited number of satellites, overlooking
low-resolution, multi-satellite, long-term archives, such as Landsat, that are
essential for affordable and bias-robust global monitoring. We address this gap
with Landsat30-AU, a large-scale vision-language dataset built from 30-meter
resolution imagery collected by four Landsat satellites (5, 7, 8, and 9) over
Australia, spanning more than 36 years. The dataset includes two components:
Landsat30-AU-Cap, containing 196,262 image-caption pairs, and Landsat30-AU-VQA,
comprising 17,725 human-verified visual question answering (VQA) samples across
eight remote sensing domains. Both datasets are curated through a bootstrapped
pipeline that leverages generic VLMs with iterative refinement and human
verification to ensure quality. Our evaluation of eight VLMs on our benchmark
reveals that off-the-shelf models struggle to understand satellite imagery. The
open-source remote-sensing VLM EarthDial achieves only 0.07 SPIDEr in
captioning and a VQA accuracy of 0.48, highlighting the limitations of current
approaches. Encouragingly, lightweight fine-tuning of Qwen2.5-VL-7B on
Landsat30-AU improves captioning performance from 0.11 to 0.31 SPIDEr and
boosts VQA accuracy from \textbf{0.74} to 0.87. Code and data are available at
https://github.com/papersubmit1/landsat30-au.

</details>


### [74] [COFFEE: A Shadow-Resilient Real-Time Pose Estimator for Unknown Tumbling Asteroids using Sparse Neural Networks](https://arxiv.org/abs/2508.03132)
*Arion Zimmermann,Soon-Jo Chung,Fred Hadaegh*

Main category: cs.CV

> 摘要：研究提出COFFEE来解决现有方法无法准确估计空间目标的姿态问题，特别是对于高度不透明自投射阴影物体，COFFEE在合成数据和阿波菲斯小行星模拟数据上实现了无偏、比传统方法更准确且比其他深度学习方法快一个数量级的效果。

<details>
  <summary>Details</summary>

**Motivation:** 动机：解决现有方法（如SIFT，ORB和AKAZE）的不准确性和现代深度学习方法在计算资源消耗上的高要求问题，尤其是处理在目标物体转动时造成的大偏差估计问题。这些问题可能导致航天器状态估计错误，尤其是面对混沌滚动运动时。

**Method:** 方法：提出了COFFEE（Celestial Occlusion Fast FEature Extractor），这是一种专门针对小行星实时姿态估计的框架。该框架能够利用空间探测器上的太阳跟踪传感器提供的太阳相位角的先验信息，通过关联显著轮廓与其投影阴影来检测一组稀疏特征，这些特征对于阴影的运动是不变的。接着使用稀疏神经网络和基于注意力的图神经网络特征匹配模型进行联合训练，旨在提供连续帧之间的对应关系。

**Result:** 结果：COFFEE方法展示了在合成数据和对直接翻转的小行星阿波菲斯的渲染数据上的优势，提供了无偏、更准确的实时姿态评估结果，并且比其他最先进的深度学习管道处理速度快一个数量级。

**Conclusion:** 结论：COFFEE提供了一种解决方案，成功解决了阴影运动问题导致的偏差姿态估计问题，同时在效果和速度上超越了传统方法和其他深度学习方法。

**Abstract:** The accurate state estimation of unknown bodies in space is a critical
challenge with applications ranging from the tracking of space debris to the
shape estimation of small bodies. A necessary enabler to this capability is to
find and track features on a continuous stream of images. Existing methods,
such as SIFT, ORB and AKAZE, achieve real-time but inaccurate pose estimates,
whereas modern deep learning methods yield higher quality features at the cost
of more demanding computational resources which might not be available on
space-qualified hardware. Additionally, both classical and data-driven methods
are not robust to the highly opaque self-cast shadows on the object of
interest. We show that, as the target body rotates, these shadows may lead to
large biases in the resulting pose estimates. For these objects, a bias in the
real-time pose estimation algorithm may mislead the spacecraft's state
estimator and cause a mission failure, especially if the body undergoes a
chaotic tumbling motion. We present COFFEE, the Celestial Occlusion Fast
FEature Extractor, a real-time pose estimation framework for asteroids designed
to leverage prior information on the sun phase angle given by sun-tracking
sensors commonly available onboard spacecraft. By associating salient contours
to their projected shadows, a sparse set of features are detected, invariant to
the motion of the shadows. A Sparse Neural Network followed by an
attention-based Graph Neural Network feature matching model are then jointly
trained to provide a set of correspondences between successive frames. The
resulting pose estimation pipeline is found to be bias-free, more accurate than
classical pose estimation pipelines and an order of magnitude faster than other
state-of-the-art deep learning pipelines on synthetic data as well as on
renderings of the tumbling asteroid Apophis.

</details>
