<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes](https://arxiv.org/abs/2601.04300)
*Chenye Meng,Zejian Li,Zhongni Liu,Yize Li,Changle Xie,Kaixin Jia,Ling Yang,Huanghuang Deng,Shiying Ding,Shengyuan Zhang,Jiayi Li,Lingyun Sun*

Main category: cs.CV

> 本文提出了一种两阶段对齐框架，结合监督微调和复杂偏好优化（CPO），旨在通过更复杂的、非二元的属性评价标准来提升扩散模型在具体任务上的生成质量。

<details>
  <summary>Details</summary>

**Motivation:** 训练后的扩散模型对齐依赖于简化信号，例如标量奖励或二元偏好，这限制了与复杂人类专业技能的对齐。本研究旨在解决这一问题，通过引入一个分层次且详细的评价系统，结合复杂偏好优化方法，更精细化地对齐扩散模型。

**Method:** 我们首先通过领域专家构建了一个描述图像质量的、分层次的精细评价标准，该标准将图像质量分解为多个积极和消极属性，并组织成树结构。基于此，我们提出了一个两阶段的对齐框架。第一阶段，通过监督微调将领域知识注入到一个辅助扩散模型中。第二阶段，我们提出了复杂偏好优化（CPO），该方法将DPO扩展到了非二元的、层次化的标准上，同时最大化积极属性概率并最小化消极属性概率。

**Result:** 我们在绘画生成领域实现了这一方法，并使用根据构建准则得出的精细属性注释数据集进行了CPO训练。广泛的实验表明，CPO显著提升了生成质量和对专家知识的对齐。

**Conclusion:** 这一研究表明，CPO可以显著提高模型生成质量和对领域专家知识的对齐度，为复杂准则的模型对齐开辟了新的研究途径。

**Abstract:** Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.

</details>


### [2] [Embedding Textual Information in Images Using Quinary Pixel Combinations](https://arxiv.org/abs/2601.04302)
*A V Uday Kiran Kandala*

Main category: cs.CV

> 本研究提出了一种新的文本嵌入图像技术，利用RGB空间中的五进制像素强度组合。与现有的位操作和其他方法相比，该方法在单个RGB像素中编码完整的文本符号，减少了噪音和计算复杂性，同时保持图像质量。

<details>
  <summary>Details</summary>

**Motivation:** 现有的文本嵌入图像方法大多依赖于像素强度位操作及转换域等技术，这些方法通常导致图像噪声增加且计算量大。

**Method:** 该方法基于RGB空间中的红、绿、蓝通道的五种受限像素强度变化，形成多达125种不同的像素强度组合，用以表示大小写字母、数字、空格及常用特殊字符。

**Result:** 评估表明，该方法不会显著扭曲原始图像，并且在编码效率上优于大多数现有方法。

**Conclusion:** 通过在单个RGB像素中编码文本符号，该技术不仅提高了编码效率，而且减少了计算复杂性，同时保持了图像质量。

**Abstract:** This paper presents a novel technique for embedding textual data into images using quinary combinations of pixel intensities in RGB space. Existing methods predominantly rely on least and most significant bit (LSB & MSB) manipulation, Pixel Value Differencing (PVD), spatial perturbations in RGB channels, transform domain based methods, Quantization methods, Edge and Region based methods and more recently through deep learning methods and generative AI techniques for hiding textual information in spatial domain of images. Most of them are dependent on pixel intensity flipping over multiple pixels, such as LSB and combination of LSB based methodologies, and on transform coefficients, often resulting in the form of noise. Encoding and Decoding are deterministic in most of the existing approaches and are computationally heavy in case of higher models such as deep learning and gen AI approaches. The proposed method works on quinary pixel intensity combinations in RGB space, where five controlled different pixel intensity variations in each of the R, G, and B channels formulate up to one hundred and twenty five distinct pixel intensity combinations. These combinations are mapped to textual symbols, enabling the representation of uppercase and lowercase alphabetic characters, numeric digits, whitespace, and commonly used special characters. Different metrics such as MSE, MAE, SNR, PSNR, SSIM, Histogram Comparison and Heatmap analysis, were evaluated for both original and encoded images resulting in no significant distortion in the images. Furthermore, the method achieves improved embedding efficiency by encoding a complete textual symbol within a single RGB pixel, in contrast to LSB and MSB based approaches that typically require multiple pixels or multi-step processes, as well as transform and learning based methods that incur higher computational overhead.

</details>


### [3] [Unified Text-Image Generation with Weakness-Targeted Post-Training](https://arxiv.org/abs/2601.04339)
*Jiahui Chen,Philippe Hansen-Estruch,Xiaochuang Han,Yushi Hu,Emily Dinan,Amita Kamath,Michal Drozdzal,Reyhane Askari-Hemmat,Luke Zettlemoyer,Marjan Ghazvininejad*

Main category: cs.CV

> 研究提出通过后训练方法实现文本与图像的联合生成，不需要显式的模式切换，证明了在四种不同的文本到图像基准测试中，使用奖励加权的后训练方法能够提高多模态图像生成的质量。

<details>
  <summary>Details</summary>

**Motivation:** 现有的许多系统依赖于显式的模式切换，限制了跨模态的耦合，并禁止自动的多模态生成。这项工作探讨了后训练以实现完全统一的文本-图像生成，以解决上述问题。

**Method:** 通过后训练实现完全统一的文本-图像生成，模型能够在单一推理过程中自主地从文本推理过渡到视觉合成。

**Result:** 使用离线、奖励加权的后训练，与基准对齐的数据相比，我们的方法在四种不同的文本到图像基准测试中表现出优异的多模态图像生成改进。

**Conclusion:** 研究展示了奖励加权对于两种模态的有效性以及战略性设计后训练数据的重要性。

**Abstract:** Unified multimodal generation architectures that jointly produce text and images have recently emerged as a promising direction for text-to-image (T2I) synthesis. However, many existing systems rely on explicit modality switching, generating reasoning text before switching manually to image generation. This separate, sequential inference process limits cross-modal coupling and prohibits automatic multimodal generation. This work explores post-training to achieve fully unified text-image generation, where models autonomously transition from textual reasoning to visual synthesis within a single inference process. We examine the impact of joint text-image generation on T2I performance and the relative importance of each modality during post-training. We additionally explore different post-training data strategies, showing that a targeted dataset addressing specific limitations achieves superior results compared to broad image-caption corpora or benchmark-aligned data. Using offline, reward-weighted post-training with fully self-generated synthetic data, our approach enables improvements in multimodal image generation across four diverse T2I benchmarks, demonstrating the effectiveness of reward-weighting both modalities and strategically designed post-training data.

</details>


### [4] [ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers](https://arxiv.org/abs/2601.04342)
*Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.CV

> ReHyAt是一种结合高性能和效率的注意力机制，它改善了视频生成模型的可扩展性。

<details>
  <summary>Details</summary>

**Motivation:** 解决基于变压器架构的视频扩散模型存在的二次注意力复杂度问题，对更长序列的可扩展性产生限制。

**Method:** ReHyAt, 一个结合了softmax注意力机制的保真度和线性注意力机制的效率的混合注意力机制，使逐块递归重组和常数内存使用成为可能。

**Result:** 在VBench和VBench-2.0上的实验，以及人类偏好研究，证实ReHyAt不仅实现了最先进的视频质量，还将注意成本从二次降低到线性，解锁了长时和端上视频生成的实际可扩展性。

**Conclusion:** ReHyAt实现了高质量的视频生成，同时通过将注意力成本从二次降低到线性，克服了训练成本高和记忆使用量大的问题，为未来状态达双向softmax模型提供了一个轻量级的蒸馏和微调流程。

**Abstract:** Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.

</details>
