{"id": "2510.07359", "categories": ["cs.CL", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.07359", "abs": "https://arxiv.org/abs/2510.07359", "authors": ["Jingfei Huang", "Han Tu"], "title": "Inconsistent Affective Reaction: Sentiment of Perception and Opinion in Urban Environments", "comment": "10 pages", "summary": "The ascension of social media platforms has transformed our understanding of\nurban environments, giving rise to nuanced variations in sentiment reaction\nembedded within human perception and opinion, and challenging existing\nmultidimensional sentiment analysis approaches in urban studies. This study\npresents novel methodologies for identifying and elucidating sentiment\ninconsistency, constructing a dataset encompassing 140,750 Baidu and Tencent\nStreet view images to measure perceptions, and 984,024 Weibo social media text\nposts to measure opinions. A reaction index is developed, integrating object\ndetection and natural language processing techniques to classify sentiment in\nBeijing Second Ring for 2016 and 2022. Classified sentiment reaction is\nanalysed and visualized using regression analysis, image segmentation, and word\nfrequency based on land-use distribution to discern underlying factors. The\nperception affective reaction trend map reveals a shift toward more evenly\ndistributed positive sentiment, while the opinion affective reaction trend map\nshows more extreme changes. Our mismatch map indicates significant disparities\nbetween the sentiments of human perception and opinion of urban areas over the\nyears. Changes in sentiment reactions have significant relationships with\nelements such as dense buildings and pedestrian presence. Our inconsistent maps\npresent perception and opinion sentiments before and after the pandemic and\noffer potential explanations and directions for environmental management, in\nformulating strategies for urban renewal.", "AI": {"tldr": "本研究通过分析包含百度和腾讯街景图像及微博社交文本的数据集，提出了新的方法来识别和阐明北京市二环内的感知与意见的情感不一致性，并通过不同技术手段分析和可视化情感反应趋势，揭示了情感随时间变化与城市元素的关联。", "motivation": "本研究旨在探索社交媒体对城市感知的影响，以及提出新的方法来理解城市不同区域的情感变化和情感不一致性的起因。", "method": "本研究构建了一个结合140,750张百度和腾讯街景图像及984,024条微博文本数据的集合，使用了一种综合对象检测和自然语言处理技术的反应指数来分类情感，并利用回归分析、图像分割和基于土地使用分布的词频进行分析。", "result": "研究表明，情感反应感知趋势图显示出正向情感变得更均匀，而意见情感反应趋势图则显示了更极端的变化。情感反应随时间变化与密布的建筑物和行人数量有显著关联。", "conclusion": "本研究的不一致情感图谱揭示了人类对城市区域的感知和意见的情感差异，并为城市更新策略提供潜在解释和方向。"}}
{"id": "2510.07414", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.07414", "abs": "https://arxiv.org/abs/2510.07414", "authors": ["Mufei Li", "Dongqi Fu", "Limei Wang", "Si Zhang", "Hanqing Zeng", "Kaan Sancak", "Ruizhong Qiu", "Haoyu Wang", "Xiaoxin He", "Xavier Bresson", "Yinglong Xia", "Chonglin Sun", "Pan Li"], "title": "Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation", "comment": "Code available at https://github.com/Graph-COM/HaystackCraft", "summary": "Modern long-context large language models (LLMs) perform well on synthetic\n\"needle-in-a-haystack\" (NIAH) benchmarks, but such tests overlook how noisy\ncontexts arise from biased retrieval and agentic workflows. We argue that\nhaystack engineering is necessary to construct noisy long contexts that\nfaithfully capture key real-world factors -- distraction from heterogeneous\nbiased retrievers and cascading errors in agentic workflows -- to test models'\nlong-context robustness. We instantiate it through HaystackCraft, a new NIAH\nbenchmark built on the full English Wikipedia hyperlink network with multi-hop\nquestions. HaystackCraft evaluates how heterogeneous retrieval strategies\n(e.g., sparse, dense, hybrid, and graph-based) affect distractor composition,\nhaystack ordering, and downstream LLM performance. HaystackCraft further\nextends NIAH to dynamic, LLM-dependent settings that simulate agentic\noperations, where models refine queries, reflect on their past reasonings, and\ndecide when to stop. Experiments with 15 long-context models show that (1)\nwhile stronger dense retrievers can introduce more challenging distractors,\ngraph-based reranking simultaneously improves retrieval effectiveness and\nmitigates more harmful distractors; (2) in agentic tests, even advanced models\nlike Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated\ndistractors or struggle to perform early stops. These results highlight\npersistent challenges in agentic long-context reasoning and establish\nHaystackCraft as a valuable testbed for future progress.", "AI": {"tldr": "研究构建了HaystackCraft基准测试，模拟现实世界中的噪声长上下文对长上下文大型语言模型的影响，发现更强的密集检索策略会引入更多挑战性的分散注意力内容，但图基重排序能提高检索效果并减少有害的分散注意力。", "motivation": "当前的长上下文大型语言模型在合成的“针在稻草堆”测试中表现良好，但忽略了来自偏差检索和智能工作流程的噪声上下文。作者认为需要进行“稻草堆工程”，以构建能真实反映关键现实因素的噪声长上下文，以测试模型对长上下文的健壮性。", "method": "构建HaystackCraft基准测试，该测试基于完整的英文维基百科超链接网络，包含多跳问题。通过多种检索策略（稀疏、密集、混合以及图基）来评估它们如何影响分散注意力内容的构成、排序以及对大型语言模型性能的影响。此外，该测试还将NIAH（针在稻草堆）扩展到了动态的、依赖语言模型的设置中，模拟操作流程中模型调整查询、反思过去的推理、决定何时停止的过程。", "result": "实验显示，虽然更强的密集检索策略会引入更具挑战性的分散注意力内容，但图基重排序同时提高了检索的有效性并减少了更多有害的分散注意力。在智能测试中，即使是GPT-5这样的先进模型也遭受来自自生分散注意力的级联失败或难以实现早期停止。", "conclusion": "这些结果突显了在智能长上下文推理中面临的持续挑战，并确立了HaystackCraft作为未来进展重要测试平台的地位。"}}
{"id": "2510.07434", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07434", "abs": "https://arxiv.org/abs/2510.07434", "authors": ["Olia Toporkov", "Alan Akbik", "Rodrigo Agerri"], "title": "Lemma Dilemma: On Lemma Generation Without Domain- or Language-Specific Training Data", "comment": "14 pages, 2 figures, 5 tables. Accepted to EMNLP Findings 2025", "summary": "Lemmatization is the task of transforming all words in a given text to their\ndictionary forms. While large language models (LLMs) have demonstrated their\nability to achieve competitive results across a wide range of NLP tasks, there\nis no prior evidence of how effective they are in the contextual lemmatization\ntask. In this paper, we empirically investigate the capacity of the latest\ngeneration of LLMs to perform in-context lemmatization, comparing it to the\ntraditional fully supervised approach. In particular, we consider the setting\nin which supervised training data is not available for a target domain or\nlanguage, comparing (i) encoder-only supervised approaches, fine-tuned\nout-of-domain, and (ii) cross-lingual methods, against direct in-context lemma\ngeneration with LLMs. Our experimental investigation across 12 languages of\ndifferent morphological complexity finds that, while encoders remain\ncompetitive in out-of-domain settings when fine-tuned on gold data, current\nLLMs reach state-of-the-art results for most languages by directly generating\nlemmas in-context without prior fine-tuning, provided just with a few examples.\nData and code available upon publication:\nhttps://github.com/oltoporkov/lemma-dilemma", "AI": {"tldr": "通过对12种不同形态复杂度的语言进行实验，尽管编码器在领域外设置中仍然具有竞争力，但现有的大语言模型（LLMs）通过直接生成引词而无需预先微调，在大多数语言中达到了最先进的性能。", "motivation": "虽然大型语言模型已经在许多NLP任务上展示了其竞争力，但还没有证据显示它们在上下文引词任务上的效果如何。本文旨在研究LLMs在该任务上的能力，并将其与传统方法进行比较。", "method": "我们通过比较最新一代的大语言模型（LLMs）在上下文化引词（lemmatization）任务中的表现与传统全监督方法的表现来进行实证研究。具体来说，我们主要探讨在目标领域或语言中没有监督训练数据的情况下的表现，对比了(i) 仅编码器的监督方法（在领域外进行微调） 和 (ii) 跨语言方法，与直接使用LLMs上下文生成引词的表现。", "result": "实验结果表明，尽管基于编码器的方法在使用黄金数据进行领域外微调时仍然具有竞争力，但现有的LLMs通过直接生成引词且无需预先微调，在大多数语言中都取得了最先进的结果。", "conclusion": "研究表明，最新的大型语言模型在没有目标领域或语言的训练数据时，也能通过少量示例直接生成引词来达到优越的表现，这显示出它们在NLP任务中潜在的强大能力。"}}
{"id": "2510.07437", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.07437", "abs": "https://arxiv.org/abs/2510.07437", "authors": ["Amruta Parulekar", "Preethi Jyothi"], "title": "LASER: An LLM-based ASR Scoring and Evaluation Rubric", "comment": "Accepted to EMNLP 2025", "summary": "Standard ASR evaluation metrics like Word Error Rate (WER) tend to unfairly\npenalize morphological and syntactic nuances that do not significantly alter\nsentence semantics. We introduce an LLM-based scoring rubric LASER that\nleverages state-of-the-art LLMs' in-context learning abilities to learn from\nprompts with detailed examples. Hindi LASER scores using Gemini 2.5 Pro\nachieved a very high correlation score of 94% with human annotations. Hindi\nexamples in the prompt were also effective in analyzing errors in other Indian\nlanguages such as Marathi, Kannada and Malayalam. We also demonstrate how a\nsmaller LLM like Llama 3 can be finetuned on word-pair examples derived from\nreference and ASR predictions to predict what kind of penalty should be applied\nwith close to 89% accuracy.", "AI": {"tldr": "提出了一个基于大语言模型的评分标准LASER，该标准在评估自动语音识别系统时，能够更好地考量语言的细微差异。实验表明，该方法具有较高的准确性和可靠性。", "motivation": "传统的诸如词错误率(WER)的自动语音识别(ASR)评估指标倾向于对不显著改变句子语义的形态和句法细微差别不公平地处罚。引入LASER是为了更公正地评估这些细微差别。", "method": "开发了一种基于大规模语言模型（LLM）的评分标准LASER，该标准利用了LLM的上下文学习能力，通过详细示例的提示进行学习。此外，还展示了如何对较小的LLM（如Llama 3）进行微调，使其能够根据参考文本和ASR预测生成的词对样例来预测应施加何种惩罚。", "result": "使用Gemini 2.5 Pro的Hindi LASER评分获得了与人类标注非常高的94%的相关性评分。Hindi示例提示也有效地分析了其他印度语言（如Marathi、Kannada和Malayalam）中的错误。较小的LLM（如Llama 3）的微调预测准确性达到了接近89%。", "conclusion": "研究表明，基于LLM的评分系统可以有效地评估ASR系统，特别是在语言细微差别方面。此外，这种系统不仅可以应用于Hindi，还可以应用于其他印度语言，显示了其广泛的适用性。"}}
{"id": "2510.07346", "categories": ["cs.CV", "cs.LG", "68T07, 68T45, 68U10, 62H30, 94A08", "I.2.10; I.4.8; I.5.4; I.2.6; C.3"], "pdf": "https://arxiv.org/pdf/2510.07346", "abs": "https://arxiv.org/abs/2510.07346", "authors": ["Nader Nemati"], "title": "Enhancing Maritime Object Detection in Real-Time with RT-DETR and Data Augmentation", "comment": "13 pages, 10 figures", "summary": "Maritime object detection faces essential challenges due to the small target\nsize and limitations of labeled real RGB data. This paper will present a\nreal-time object detection system based on RT-DETR, enhanced by employing\naugmented synthetic images while strictly evaluating on real data. This study\nemploys RT-DETR for the maritime environment by combining multi-scale feature\nfusion, uncertainty-minimizing query selection, and smart weight between\nsynthetic and real training samples. The fusion module in DETR enhances the\ndetection of small, low-contrast vessels, query selection focuses on the most\nreliable proposals, and the weighting strategy helps reduce the visual gap\nbetween synthetic and real domains. This design preserves DETR's refined\nend-to-end set prediction while allowing users to adjust between speed and\naccuracy at inference time. Data augmentation techniques were also used to\nbalance the different classes of the dataset to improve the robustness and\naccuracy of the model. Regarding this study, a full Python robust maritime\ndetection pipeline is delivered that maintains real-time performance even under\npractical limits. It also verifies how each module contributes, and how the\nsystem handles failures in extreme lighting or sea conditions. This study also\nincludes a component analysis to quantify the contribution of each\narchitectural module and explore its interactions.", "AI": {"tldr": "文章介绍了一种基于RT-DETR的实时海上物体检测系统，通过多种技术手段解决了小目标检测和图像增强的问题，并提供了一个完整的Python检测流程。", "motivation": "解决海上物体检测中存在的目标尺寸小和真实RGB数据标注不足的问题。", "method": "使用RT-DETR基础，结合多尺度特征融合、不确定性最小化查询选择、智能权重策略和数据增强技术来优化海上物体检测。", "result": "此论文提出了一种基于RT-DETR的实时海上物体检测系统，通过使用增强的合成图像来弥补真实RGB数据标注的不足。系统结合了多尺度特征融合、不确定性最小化查询选择和智能权重策略应对真实与合成样本之间的视觉差异。该设计保留了DETR的端到端集合预测优势，并允许在推理时调整速度和准确度之间的平衡。此外，还通过数据增强技术平衡了数据集中的各类物体，提高了系统的鲁棒性和准确性。论文中详细介绍了一个完整的Python海上物体检测流程，并验证了每个模块的功能以及系统在极端光照或海况环境下的表现。", "conclusion": "通过结合多种技术创新，提出了一种能够实时检测海上物体的系统，具有较好的鲁棒性和准确性，并提供了一个完整的Python实现流程。"}}
{"id": "2510.07453", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07453", "abs": "https://arxiv.org/abs/2510.07453", "authors": ["Zifan Jiang", "Colin Leong", "Amit Moryossef", "Anne Göhring", "Annette Rios", "Oliver Cory", "Maksym Ivashechkin", "Neha Tarigopula", "Biao Zhang", "Rico Sennrich", "Sarah Ebling"], "title": "Meaningful Pose-Based Sign Language Evaluation", "comment": "Accepted at WMT 2025", "summary": "We present a comprehensive study on meaningfully evaluating sign language\nutterances in the form of human skeletal poses. The study covers keypoint\ndistance-based, embedding-based, and back-translation-based metrics. We show\ntradeoffs between different metrics in different scenarios through automatic\nmeta-evaluation of sign-level retrieval and a human correlation study of\ntext-to-pose translation across different sign languages. Our findings and the\nopen-source pose-evaluation toolkit provide a practical and reproducible way of\ndeveloping and evaluating sign language translation or generation systems.", "AI": {"tldr": "TL;DR：通过自动和人类评估手段综合考查了 sign language 人工姿态的评价方法，发现不同指标在场景下的权衡，发布了评估工具包。", "motivation": "动机：改善 sign language 在人工姿态表示中的评估，提供一套全面的评估指标，帮助研究人员和开发者更好地理解不同指标在不同场景下的表现。", "method": "方法：研究涵盖了基于关键点距离测量、基于嵌入和基于反翻译的指标，采用自动元评估和人类相关研究来综合分析这些方法的有效性。", "result": "内容摘要：我们呈献了一项关于有意义地评估以人类骨骼姿态形式出现的 sign language 发音的综合研究。该研究涵盖了基于关键点距离、基于嵌入的和基于反翻译的指标。我们通过自动元评估签名级别检索和跨不同 sign language 的文本到姿态翻译的人类相关研究，展示了不同指标在不同场景中的权衡。我们的发现以及开源姿态评估工具包为开发和评估 sign language 翻译或生成系统提供了实用且可重现的方法。", "conclusion": "结论：研究表明不同指标在不同场景下的表现有所不同，提供了一套实用的评估方法并通过开源工具包推动了 sign language 翻译和生成系统的开发。"}}
{"id": "2510.07441", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07441", "abs": "https://arxiv.org/abs/2510.07441", "authors": ["Nithin C. Babu", "Aniruddha Mahapatra", "Harsh Rangwani", "Rajiv Soundararajan", "Kuldeep Kulkarni"], "title": "DynamicEval: Rethinking Evaluation for Dynamic Text-to-Video Synthesis", "comment": "Preprint. Under review. 26 pages, 11 figures, 11 tables. Access the\n  project page in https://nithincbabu7.github.io/DynamicEval", "summary": "Existing text-to-video (T2V) evaluation benchmarks, such as VBench and\nEvalCrafter, suffer from two limitations. (i) While the emphasis is on\nsubject-centric prompts or static camera scenes, camera motion essential for\nproducing cinematic shots and existing metrics under dynamic motion are largely\nunexplored. (ii) These benchmarks typically aggregate video-level scores into a\nsingle model-level score for ranking generative models. Such aggregation,\nhowever, overlook video-level evaluation, which is vital to selecting the\nbetter video among the candidate videos generated for a given prompt. To\naddress these gaps, we introduce DynamicEval, a benchmark consisting of\nsystematically curated prompts emphasizing dynamic camera motion, paired with\n45k human annotations on video pairs from 3k videos generated by ten T2V\nmodels. DynamicEval evaluates two key dimensions of video quality: background\nscene consistency and foreground object consistency. For background scene\nconsistency, we obtain the interpretable error maps based on the Vbench motion\nsmoothness metric. We observe that while the Vbench motion smoothness metric\nshows promising alignment with human judgments, it fails in two cases:\nocclusions/disocclusions arising from camera and foreground object movements.\nBuilding on this, we propose a new background consistency metric that leverages\nobject error maps to correct two failure cases in a principled manner. Our\nsecond innovation is the introduction of a foreground consistency metric that\ntracks points and their neighbors within each object instance to assess object\nfidelity. Extensive experiments demonstrate that our proposed metrics achieve\nstronger correlations with human preferences at both the video level and the\nmodel level (an improvement of more than 2% points), establishing DynamicEval\nas a more comprehensive benchmark for evaluating T2V models under dynamic\ncamera motion.", "AI": {"tldr": "本文旨在通过引入DynamicEval，以解决现有T2V评估基准中的局限性，即缺乏对动态摄像机运动的评估，提出新的背景和前景一致性评估指标，提高人类偏好的预测准确性。", "motivation": "由于现有的T2V评估基准无法充分评测动态摄像机运动下视频的质量，且在视频级别上未能充分进行评估，作者旨在改进这些问题，使模型评估更加全面。", "method": "内容介绍了针对现有文本到视频（T2V）评估基准存在的局限性，提出了一个新的评估指标DynamicEval，该基准着重于动态摄像机运动并配以人类对视频的评价。此外，还提出了两种新的评估视频质量的关键维度的指标：背景场景一致性指标和前景目标一致性指标。", "result": "经实验显示，所提出的背景和前景一致性评估指标在视频级别和模型级别的人类偏好预测中的准确性有了明显的提升。", "conclusion": "DynamicEval作为一个新的评估基准，可以更全面地评估在动态摄像机运动条件下的视频生成模型。"}}
{"id": "2510.07458", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07458", "abs": "https://arxiv.org/abs/2510.07458", "authors": ["Eduardo Ryô Tamaki", "Yujin J. Jung", "Julia Chatterley", "Grant Mitchell", "Semir Dzebo", "Cristóbal Sandoval", "Levente Littvay", "Kirk A. Hawkins"], "title": "Populism Meets AI: Advancing Populism Research with LLMs", "comment": "27 pages, 3 figures. Preprint version under review", "summary": "Measuring the ideational content of populism remains a challenge. Traditional\nstrategies based on textual analysis have been critical for building the\nfield's foundations and providing a valid, objective indicator of populist\nframing. Yet these approaches are costly, time consuming, and difficult to\nscale across languages, contexts, and large corpora. Here we present the\nresults from a rubric and anchor guided chain of thought (CoT) prompting\napproach that mirrors human coder training. By leveraging the Global Populism\nDatabase (GPD), a comprehensive dataset of global leaders' speeches annotated\nfor degrees of populism, we replicate the process used to train human coders by\nprompting the LLM with an adapted version of the same documentation to guide\nthe model's reasoning. We then test multiple proprietary and open weight models\nby replicating scores in the GPD. Our findings reveal that this domain specific\nprompting strategy enables the LLM to achieve classification accuracy on par\nwith expert human coders, demonstrating its ability to navigate the nuanced,\ncontext sensitive aspects of populism.", "AI": {"tldr": "研究通过模仿人类编码训练过程的特定领域提示策略，使语言模型能够实现与专家人类编码者相媲美的分类准确性，成功测量民粹主义的复杂和情境敏感方面。", "motivation": "传统的基于文本分析策略在建立民粹主义理念内容测量的基础方面发挥了关键作用，但这些方法成本高昂、耗时且难以在多种语言、情境和大规模语料库中扩展。本研究旨在开发一种更高效的方法来测量民粹主义的理念内容。", "method": "通过基于准则和范例引导的连贯性思考（CoT）提示方法，模仿人类编码者的训练过程，利用Global Populism Database（GPD），一个全球领导人演讲的全面数据集，该数据集根据民粹主义的程度进行了注释。通过使用此数据集，对多个专有和开放权重的模型进行测试以复制GPD中的分数。", "result": "实验结果表明，此种领域特定的提示策略可以使得语言模型达到与专家人类评分者相当的分类准确率，表现出其在处理民粹主义复杂且情境相关方面的能力。", "conclusion": "基于准则和范例引导的连贯性思考（CoT）提示方法可以作为传统文本分析策略的一种有效替代，有助于构建更高效和准确的民粹主义理念内容测量方法。"}}
{"id": "2510.07470", "categories": ["cs.CV", "94A08, 68U10"], "pdf": "https://arxiv.org/pdf/2510.07470", "abs": "https://arxiv.org/abs/2510.07470", "authors": ["Marien Renaud", "Julien Hermant", "Deliang Wei", "Yu Sun"], "title": "Provably Accelerated Imaging with Restarted Inertia and Score-based Image Priors", "comment": "62 pages", "summary": "Fast convergence and high-quality image recovery are two essential features\nof algorithms for solving ill-posed imaging inverse problems. Existing methods,\nsuch as regularization by denoising (RED), often focus on designing\nsophisticated image priors to improve reconstruction quality, while leaving\nconvergence acceleration to heuristics. To bridge the gap, we propose Restarted\nInertia with Score-based Priors (RISP) as a principled extension of RED. RISP\nincorporates a restarting inertia for fast convergence, while still allowing\nscore-based image priors for high-quality reconstruction. We prove that RISP\nattains a faster stationary-point convergence rate than RED, without requiring\nthe convexity of the image prior. We further derive and analyze the associated\ncontinuous-time dynamical system, offering insight into the connection between\nRISP and the heavy-ball ordinary differential equation (ODE). Experiments\nacross a range of imaging inverse problems demonstrate that RISP enables fast\nconvergence while achieving high-quality reconstructions.", "AI": {"tldr": "RISP 是一种结合快速收敛和高质量图像恢复的算法，它在 RED 的基础上加入了重启惯性策略，同时保持了基于评分的图像先验，从而在不依赖图像先验函数的凸性的情况下实现了更快的收敛速度，并在多种图像逆问题中得到了验证。", "motivation": "动机在于解决现有算法在图像恢复过程中快速收敛和高质量恢复两个方面难以兼顾的问题，现有方法如正则化去噪 (RED) 通常集中在设计复杂的图像先验来改善重建质量，而收敛加速则依赖于策略性手段。", "method": "提出了一种新的算法 RISP，该算法在 RED 的基础上增加了重启惯性策略，同时保持了基于评分的图像先验，以获得快速收敛速度和高质量的图像重建。", "result": "证明 RISP 达到了比 RED 更快的静止点收敛速度，并且在多种图像逆问题中实验验证了 RISP 可以实现快速收敛的同时获得高质量的重建结果。", "conclusion": "RISP 通过引入重启惯性，实现了既快速收敛又高质图像恢复的目标，克服了 RED 等现有算法在收敛速度上的限制，并展示了其在各类图像逆问题中的实用性。"}}
{"id": "2510.07475", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07475", "abs": "https://arxiv.org/abs/2510.07475", "authors": ["Zheyuan Zhang", "Lin Ge", "Hongjiang Li", "Weicheng Zhu", "Chuxu Zhang", "Yanfang Ye"], "title": "MAPRO: Recasting Multi-Agent Prompt Optimization as Maximum a Posteriori Inference", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, and LLM-based agents further extend these abilities to various\npractical workflows. While recent progress shows that multi-agent systems (MAS)\ncan outperform single agents by coordinating specialized roles, designing\neffective MAS remains difficult due to prompt sensitivity and the compounded\ninstability MAS creates. To cope with the challenge, recent efforts in\nautomated prompt design have reduced manual effort. However, multi-agent prompt\noptimization remains largely unexplored. Challenges like exponentially\nexpanding search space and ambiguous credit assignment together make systematic\ndesign intractable without principled methods. Therefore, we introduce\nM}ulti-Agent PRompt Optimization (MAPRO), a four-stage framework that first\nformulates MAS prompt optimization as a Maximum a Posteriori (MAP) inference\nproblem and solves it using a language-guided variant of max-product belief\npropagation algorithm. To address credit assignment and updates the system\niteratively, MAPRO employs a topology-aware refinement mechanism that\nintegrates execution feedback and downstream blames to selectively update agent\nprompts. Through this process, MAPRO progressively converges to a coordinated\nset of agent-specific prompt policies. Across benchmarks in various tasks,\nMAPRO achieves state-of-the-art performance, consistently surpassing manually\nengineered baselines and recent automated alternatives. Beyond performance, our\nMAP-based formulation also delivers general guidelines for building more\nreliable and principled multi-agent systems in the future", "AI": {"tldr": "论文提出了MAPRO框架，用于解决多智能体系统中提示优化的难题，主要通过使用语言引导的最大乘积信念传播算法来解决优化问题，并且通过选择性更新代理提示来改进系统的性能。", "motivation": "尽管多智能体系统（MAS）能够通过协调专业化角色超越单个智能体的性能，但在设计有效的MAS时，仍然面临提示敏感性和系统复杂度导致的不稳定性挑战。为了应对这些挑战，该论文提出了MAPRO框架，专门解决多智能体的提示优化问题。", "method": "该论文提出的MAPRO框架通过四个阶段将多智能体系统的提示优化问题转化为后验概率最大的推断问题，并利用语言引导的最大乘积信念传播算法来解决。为了处理信用分配问题，MAPRO采用了基于拓扑结构的精细机制，结合执行反馈和下游责任反馈，选择性地更新智能体的提示信息，逐步聚合到一组协调的智能体特定的提示策略。", "result": "在各种任务的基准测试中，MAPRO达到了最先进的性能，一直超越手动设计的基线以及最近的自动化方法。", "conclusion": "MAPRO不仅实现了更好的性能，而且其基于MAP的公式还为未来构建更可靠和原理性的多智能体系统提供了普遍的指导方针。"}}
