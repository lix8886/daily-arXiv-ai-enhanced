{"id": "2509.20379", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20379", "abs": "https://arxiv.org/abs/2509.20379", "authors": ["Ofir Azachi", "Kfir Eliyahu", "Eyal El Ani", "Rom Himelstein", "Roi Reichart", "Yuval Pinter", "Nitay Calderon"], "title": "Leveraging NTPs for Efficient Hallucination Detection in VLMs", "comment": null, "summary": "Hallucinations of vision-language models (VLMs), which are misalignments\nbetween visual content and generated text, undermine the reliability of VLMs.\nOne common approach for detecting them employs the same VLM, or a different\none, to assess generated outputs. This process is computationally intensive and\nincreases model latency. In this paper, we explore an efficient on-the-fly\nmethod for hallucination detection by training traditional ML models over\nsignals based on the VLM's next-token probabilities (NTPs). NTPs provide a\ndirect quantification of model uncertainty. We hypothesize that high\nuncertainty (i.e., a low NTP value) is strongly associated with hallucinations.\nTo test this, we introduce a dataset of 1,400 human-annotated statements\nderived from VLM-generated content, each labeled as hallucinated or not, and\nuse it to test our NTP-based lightweight method. Our results demonstrate that\nNTP-based features are valuable predictors of hallucinations, enabling fast and\nsimple ML models to achieve performance comparable to that of strong VLMs.\nFurthermore, augmenting these NTPs with linguistic NTPs, computed by feeding\nonly the generated text back into the VLM, enhances hallucination detection\nperformance. Finally, integrating hallucination prediction scores from VLMs\ninto the NTP-based models led to better performance than using either VLMs or\nNTPs alone. We hope this study paves the way for simple, lightweight solutions\nthat enhance the reliability of VLMs.", "AI": {"tldr": "研究提出通过训练传统ML模型利用VLM的NTP信号来即时检测幻觉，实验表明这种方法有效且提高了检测性能。", "motivation": "目前主流的幻觉检测方法依赖于使用相同的VLM或不同的VLM来评估生成内容，这种方法计算量大且增加了模型延迟。因此，研究动机旨在提出一种计算成本更低，更快速的幻觉检测方法。", "method": "研究使用了传统机器学习模型，并训练这些模型使用基于VLM的下一个令牌概率（NTP）信号。通过一个包含1,400个手动标注的VLM生成内容的语句数据集测试了NTP基方法的有效性。", "result": "研究提出了一种基于视觉语言模型（VLM）的下一个令牌概率（NTP）的轻量级幻觉检测方法。通过训练传统机器学习模型使用NTP信号，该方法能够在VLM生成内容时即时检测幻觉，而不需要额外的计算成本。实验结果表明，NTP特征对于幻觉检测是非常有效的预测器。结合语言NTP进一步提升了检测性能。研究表明，通过将VLM的幻觉预测分数融入NTP模型中，可以取得优于单独使用VLM或NTP的方法效果。这种方法为提高视觉语言模型的可靠性提供了简单且高效的解决方案。", "conclusion": "研究表明，NTP基方法及其与语言NTP的结合都能有效提高幻觉检测性能，与强VLM方法相比具有更快和更简单的优点。未来研究将继续探索简单、轻量级的解决方案，以提高视觉语言模型的可靠性。"}}
{"id": "2509.20420", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20420", "abs": "https://arxiv.org/abs/2509.20420", "authors": ["Elias N. Zois", "Moises Diaz", "Salem Said", "Miguel A. Ferrer"], "title": "Quasi-Synthetic Riemannian Data Generation for Writer-Independent Offline Signature Verification", "comment": "9 pages, 3 figures", "summary": "Offline handwritten signature verification remains a challenging task,\nparticularly in writer-independent settings where models must generalize across\nunseen individuals. Recent developments have highlighted the advantage of\ngeometrically inspired representations, such as covariance descriptors on\nRiemannian manifolds. However, past or present, handcrafted or data-driven\nmethods usually depend on real-world signature datasets for classifier\ntraining. We introduce a quasi-synthetic data generation framework leveraging\nthe Riemannian geometry of Symmetric Positive Definite matrices (SPD). A small\nset of genuine samples in the SPD space is the seed to a Riemannian Gaussian\nMixture which identifies Riemannian centers as synthetic writers and variances\nas their properties. Riemannian Gaussian sampling on each center generates\npositive as well as negative synthetic SPD populations. A metric learning\nframework utilizes pairs of similar and dissimilar SPD points, subsequently\ntesting it over on real-world datasets. Experiments conducted on two popular\nsignature datasets, encompassing Western and Asian writing styles, demonstrate\nthe efficacy of the proposed approach under both intra- and cross- dataset\nevaluation protocols. The results indicate that our quasi-synthetic approach\nachieves low error rates, highlighting the potential of generating synthetic\ndata in Riemannian spaces for writer-independent signature verification\nsystems.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.20427", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20427", "abs": "https://arxiv.org/abs/2509.20427", "authors": ["Team Seedream", "Yunpeng Chen", "Yu Gao", "Lixue Gong", "Meng Guo", "Qiushan Guo", "Zhiyao Guo", "Xiaoxia Hou", "Weilin Huang", "Yixuan Huang", "Xiaowen Jian", "Huafeng Kuang", "Zhichao Lai", "Fanshi Li", "Liang Li", "Xiaochen Lian", "Chao Liao", "Liyang Liu", "Wei Liu", "Yanzuo Lu", "Zhengxiong Luo", "Tongtong Ou", "Guang Shi", "Yichun Shi", "Shiqi Sun", "Yu Tian", "Zhi Tian", "Peng Wang", "Rui Wang", "Xun Wang", "Ye Wang", "Guofeng Wu", "Jie Wu", "Wenxu Wu", "Yonghui Wu", "Xin Xia", "Xuefeng Xiao", "Shuang Xu", "Xin Yan", "Ceyuan Yang", "Jianchao Yang", "Zhonghua Zhai", "Chenlin Zhang", "Heng Zhang", "Qi Zhang", "Xinyu Zhang", "Yuwei Zhang", "Shijia Zhao", "Wenliang Zhao", "Wenjia Zhu"], "title": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "comment": "Seedream 4.0 Technical Report", "summary": "We introduce Seedream 4.0, an efficient and high-performance multimodal image\ngeneration system that unifies text-to-image (T2I) synthesis, image editing,\nand multi-image composition within a single framework. We develop a highly\nefficient diffusion transformer with a powerful VAE which also can reduce the\nnumber of image tokens considerably. This allows for efficient training of our\nmodel, and enables it to fast generate native high-resolution images (e.g.,\n1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning\ndiverse taxonomies and knowledge-centric concepts. Comprehensive data\ncollection across hundreds of vertical scenarios, coupled with optimized\nstrategies, ensures stable and large-scale training, with strong\ngeneralization. By incorporating a carefully fine-tuned VLM model, we perform\nmulti-modal post-training for training both T2I and image editing tasks\njointly. For inference acceleration, we integrate adversarial distillation,\ndistribution matching, and quantization, as well as speculative decoding. It\nachieves an inference time of up to 1.8 seconds for generating a 2K image\n(without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream\n4.0 can achieve state-of-the-art results on both T2I and multimodal image\nediting. In particular, it demonstrates exceptional multimodal capabilities in\ncomplex tasks, including precise image editing and in-context reasoning, and\nalso allows for multi-image reference, and can generate multiple output images.\nThis extends traditional T2I systems into an more interactive and\nmultidimensional creative tool, pushing the boundary of generative AI for both\ncreativity and professional applications. Seedream 4.0 is now accessible on\nhttps://www.volcengine.com/experience/ark?launch=seedream.", "AI": {"tldr": "Seedream 4.0 is a cutting-edge multimodal image generation system that efficiently unifies T2I synthesis, image editing, and multi-image composition, offering state-of-the-art performance and versatility, available on https://www.volcengine.com/experience/ark?launch=seedream.", "motivation": "The motivation is to create a more efficient and high-performance multimodal image generation system that can unify various tasks like text-to-image synthesis, image editing, and multi-image composition within a single framework, offering an interactive and multidimensional creative tool.", "method": "The paper develops Seedream 4.0, a multimodal image generation system. It includes a diffusion transformer and a powerful VAE for efficient training and generation of high-resolution images. The system is pretrained on millions of text-image pairs and uses fine-tuned VLM models for multi-modal post-training. Techniques such as adversarial distillation, distribution matching, quantitative analysis, and speculative decoding are used to accelerate inference.", "result": "Seedream 4.0 achieves state-of-the-art results on T2I and multimodal image editing tasks. It excels in complex multimodal tasks involving precise image editing, in-context reasoning, multi-image reference, and generating multiple outputs, making it a versatile tool for both creative and professional applications.", "conclusion": "Seedream 4.0 not only showcases excellent performance in standard benchmarks but also in complex tasks requiring diverse resources, setting new standards for generative AI applications in a variety of fields."}}
{"id": "2509.20474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20474", "abs": "https://arxiv.org/abs/2509.20474", "authors": ["Samia Saeed", "Khuram Naveed"], "title": "A Contrastive Learning Framework for Breast Cancer Detection", "comment": null, "summary": "Breast cancer, the second leading cause of cancer-related deaths globally,\naccounts for a quarter of all cancer cases [1]. To lower this death rate, it is\ncrucial to detect tumors early, as early-stage detection significantly improves\ntreatment outcomes. Advances in non-invasive imaging techniques have made early\ndetection possible through computer-aided detection (CAD) systems which rely on\ntraditional image analysis to identify malignancies. However, there is a\ngrowing shift towards deep learning methods due to their superior\neffectiveness. Despite their potential, deep learning methods often struggle\nwith accuracy due to the limited availability of large-labeled datasets for\ntraining. To address this issue, our study introduces a Contrastive Learning\n(CL) framework, which excels with smaller labeled datasets. In this regard, we\ntrain Resnet-50 in semi supervised CL approach using similarity index on a\nlarge amount of unlabeled mammogram data. In this regard, we use various\naugmentation and transformations which help improve the performance of our\napproach. Finally, we tune our model on a small set of labelled data that\noutperforms the existing state of the art. Specifically, we observed a 96.7%\naccuracy in detecting breast cancer on benchmark datasets INbreast and MIAS.", "AI": {"tldr": "The paper introduces a Contrastive Learning framework with Resnet-50 using semi-supervised methods to improve breast cancer detection accuracy, achieving 96.7% on benchmark datasets.", "motivation": "To tackle the limitation of traditional deep learning methods in dealing with small labeled datasets, aiming to improve breast cancer early detection accuracy.", "method": "The method involves training a Resnet-50 model using a semi-supervised Contrastive Learning approach, utilizing large amounts of unlabeled mammogram data with various data augmentations.", "result": "The model outperforms the current state-of-the-art in breast cancer detection, with an accuracy rate of 96.7% on the INbreast and MIAS benchmark datasets.", "conclusion": "The proposed Contrastive Learning framework with Resnet-50 significantly enhances early breast cancer detection accuracy even with limited labeled data, suggesting potential for implementation in real-world CAD systems."}}
{"id": "2509.20367", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.20367", "abs": "https://arxiv.org/abs/2509.20367", "authors": ["Leyi Ouyang"], "title": "Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models", "comment": "2 Figures, 7 Tables, 1 Algorithm", "summary": "Diplomatic events consistently prompt widespread public discussion and\ndebate. Public sentiment plays a critical role in diplomacy, as a good\nsentiment provides vital support for policy implementation, helps resolve\ninternational issues, and shapes a nation's international image. Traditional\nmethods for gauging public sentiment, such as large-scale surveys or manual\ncontent analysis of media, are typically time-consuming, labor-intensive, and\nlack the capacity for forward-looking analysis. We propose a novel framework\nthat identifies specific modifications for diplomatic event narratives to shift\npublic sentiment from negative to neutral or positive. First, we train a\nlanguage model to predict public reaction towards diplomatic events. To this\nend, we construct a dataset comprising descriptions of diplomatic events and\ntheir associated public discussions. Second, guided by communication theories\nand in collaboration with domain experts, we predetermined several textual\nfeatures for modification, ensuring that any alterations changed the event's\nnarrative framing while preserving its core facts.We develop a counterfactual\ngeneration algorithm that employs a large language model to systematically\nproduce modified versions of an original text. The results show that this\nframework successfully shifted public sentiment to a more favorable state with\na 70\\% success rate. This framework can therefore serve as a practical tool for\ndiplomats, policymakers, and communication specialists, offering data-driven\ninsights on how to frame diplomatic initiatives or report on events to foster a\nmore desirable public sentiment.", "AI": {"tldr": "我们提出了一种新框架，该框架能通过文本修改改变公众对特定外交事件情绪的叙述，成功率达到了70%，为政策提供数据驱动的视角。", "motivation": "传统的评估公众情绪的方法，如大型调查或手动分析媒体内容，通常耗时、劳动密集且缺乏前瞻性的分析能力。鉴于公众情绪在外交中扮演着关键角色，好的情绪为政策实施提供重要支持，有助于解决国际问题，并塑造国家的国际形象，我们需要提出一种新方法来改善这一情况。", "method": "我们提出了一种新框架，用于识别特定的外交事件叙述修改，以将公众情绪从负面转变为中性或正面。首先，我们训练了一个语言模型，用以预测公众对外交事件的反应。为此，我们构建了一个包含外交事件描述及其相关公众讨论的数据集。其次，根据传播理论和与领域专家的合作，我们预先确定了几种文本特征进行修改，确保任何更改改变了事件的叙述框架，同时保留其核心事实。我们开发了一个反事实生成算法，利用大型语言模型系统地生成原始文本的修改版本。", "result": "实验结果显示，该框架成功地将公众情绪转移到了更加有利的状态，成功率达到了70%。", "conclusion": "因此，这个框架可以作为外交官、政策制定者和传播专家的实用工具，提供数据驱动的见解，关于如何在框架外交倡议或报告事件时培育更加有利的公众情绪。"}}
{"id": "2509.20479", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20479", "abs": "https://arxiv.org/abs/2509.20479", "authors": ["Simon Baeuerle", "Pratik Khanna", "Nils Friederich", "Angelo Jovin Yamachui Sitcheu", "Damir Shakirov", "Andreas Steimer", "Ralf Mikut"], "title": "Are Foundation Models Ready for Industrial Defect Recognition? A Reality Check on Real-World Data", "comment": null, "summary": "Foundation Models (FMs) have shown impressive performance on various text and\nimage processing tasks. They can generalize across domains and datasets in a\nzero-shot setting. This could make them suitable for automated quality\ninspection during series manufacturing, where various types of images are being\nevaluated for many different products. Replacing tedious labeling tasks with a\nsimple text prompt to describe anomalies and utilizing the same models across\nmany products would save significant efforts during model setup and\nimplementation. This is a strong advantage over supervised Artificial\nIntelligence (AI) models, which are trained for individual applications and\nrequire labeled training data. We test multiple recent FMs on both custom\nreal-world industrial image data and public image data. We show that all of\nthose models fail on our real-world data, while the very same models perform\nwell on public benchmark datasets.", "AI": {"tldr": "本文探讨了基础模型在系列制造自动化质量检查中的应用潜力，发现这些模型在公共数据集上表现良好，但未能在现实工业图像数据上达到预期效果。", "motivation": "基础模型（FMs）在文本和图像处理任务上表现出色，能够实现跨领域和数据集的零样本泛化，这可能使它们适用于系列制造过程中的自动化质量检查。这种检查涉及多种类型的图像评估，针对不同的产品。与需要标注训练数据的监督AI模型相比，基础模型只需使用简单的文本提示来描述异常，即可在不同产品上使用相同的模型，从而节省模型设置和实施过程中的大量工作。", "method": "测试了多个最近的基础模型在自定义的现实工业图像数据和公共图像数据上表现。", "result": "所有这些模型在真实世界的数据上都表现不佳，而在公共基准数据集上表现良好。", "conclusion": "尽管基础模型在公共数据集上表现良好，但在真实世界工业场景中的应用面临挑战，凸显了这些模型在复杂现实环境下的限制。"}}
{"id": "2509.20373", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20373", "abs": "https://arxiv.org/abs/2509.20373", "authors": ["Shreya G. Upadhyay", "Carlos Busso", "Chi-Chun Lee"], "title": "Speaker Style-Aware Phoneme Anchoring for Improved Cross-Lingual Speech Emotion Recognition", "comment": null, "summary": "Cross-lingual speech emotion recognition (SER) remains a challenging task due\nto differences in phonetic variability and speaker-specific expressive styles\nacross languages. Effectively capturing emotion under such diverse conditions\nrequires a framework that can align the externalization of emotions across\ndifferent speakers and languages. To address this problem, we propose a\nspeaker-style aware phoneme anchoring framework that aligns emotional\nexpression at the phonetic and speaker levels. Our method builds\nemotion-specific speaker communities via graph-based clustering to capture\nshared speaker traits. Using these groups, we apply dual-space anchoring in\nspeaker and phonetic spaces to enable better emotion transfer across languages.\nEvaluations on the MSP-Podcast (English) and BIIC-Podcast (Taiwanese Mandarin)\ncorpora demonstrate improved generalization over competitive baselines and\nprovide valuable insights into the commonalities in cross-lingual emotion\nrepresentation.", "AI": {"tldr": "本文提出了一种基于发音和说话人风格感知的框架，通过图聚类构建情感特定的说话人群体，并在说话人和发音空间上应用双空间锚定，以实现跨语言情感转移，提高了跨语言语音情感识别的效果。", "motivation": "跨语言语音情感识别的困难来源于语言间的发音差异和说话人表达风格的差异。需要一个框架能够对多种情况下的情感进行有效地捕捉。", "method": "提出了一种说话人风格感知的发音锚定框架，通过图聚类构建情感特定的说话人群体，并在说话人和发音空间上应用双空间锚定。", "result": "在MSP-Podcast(英语)和BIIC-Podcast(台湾国语)语料库的实验表明，该方法相比竞争基线有更好的泛化能力，提供了跨语言情感表示的洞见。", "conclusion": "该研究证明了通过一种考虑发音和说话人风格的方法，可以有效地应对跨语言情感识别中的挑战，并展示了跨语言情感表示的共性。"}}
{"id": "2509.20481", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20481", "abs": "https://arxiv.org/abs/2509.20481", "authors": ["Jing Li", "Oskar Bartosz", "Chengyu Wang", "Michal Wnuczynski", "Dilshan Godaliyadda", "Michael Polley"], "title": "Shared Neural Space: Unified Precomputed Feature Encoding for Multi-Task and Cross Domain Vision", "comment": null, "summary": "The majority of AI models in imaging and vision are customized to perform on\nspecific high-precision task. However, this strategy is inefficient for\napplications with a series of modular tasks, since each requires a mapping into\na disparate latent domain. To address this inefficiency, we proposed a\nuniversal Neural Space (NS), where an encoder-decoder framework pre-computes\nfeatures across vision and imaging tasks. Our encoder learns transformation\naware, generalizable representations, which enable multiple downstream AI\nmodules to share the same feature space. This architecture reduces redundancy,\nimproves generalization across domain shift, and establishes a foundation for\neffecient multi-task vision pipelines. Furthermore, as opposed to larger\ntransformer backbones, our backbone is lightweight and CNN-based, allowing for\nwider across hardware. We furthur demonstrate that imaging and vision modules,\nsuch as demosaicing, denoising, depth estimation and semantic segmentation can\nbe performed efficiently in the NS.", "AI": {"tldr": "提出了一种通用的神经空间，通过共享特征空间来解决成像和视觉应用中的效率问题。这种方法提高了泛化能力，并支持轻量级、硬件兼容的实现。", "motivation": "大多数AI模型在成像和视觉领域是为执行特定的高精度任务定制的，这在一系列模块化任务的应用中是低效的，因为每个任务都需要映射到不同的潜在空间。提出了一种通用的方法来解决这种低效问题。", "method": "提出了一种通用的神经空间（NS），其中编码器-解码器框架预计算了视觉和成像任务的特征。编码器学习具有变换感知和可泛化的表示，从而实现多个下游AI模块共享相同的特征空间。此外，与更大的变压器主干网络不同，我们的主干轻量级且基于CNN，适用于更广泛的硬件。", "result": "实验证明，成像和视觉模块，如去马赛克、去噪、深度估计和语义分割，可以在这个NS中高效执行。", "conclusion": "这种架构减少了冗余，提高了跨领域转移的泛化能力，并为高效的多任务视觉管道奠定了基础。"}}
{"id": "2509.20374", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20374", "abs": "https://arxiv.org/abs/2509.20374", "authors": ["Nithin Somasekharan", "Ling Yue", "Yadi Cao", "Weichao Li", "Patrick Emami", "Pochinapeddi Sai Bhargav", "Anurag Acharya", "Xingyu Xie", "Shaowu Pan"], "title": "CFD-LLMBench: A Benchmark Suite for Evaluating Large Language Models in Computational Fluid Dynamics", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong performance across\ngeneral NLP tasks, but their utility in automating numerical experiments of\ncomplex physical system -- a critical and labor-intensive component -- remains\nunderexplored. As the major workhorse of computational science over the past\ndecades, Computational Fluid Dynamics (CFD) offers a uniquely challenging\ntestbed for evaluating the scientific capabilities of LLMs. We introduce\nCFDLLMBench, a benchmark suite comprising three complementary components --\nCFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM\nperformance across three key competencies: graduate-level CFD knowledge,\nnumerical and physical reasoning of CFD, and context-dependent implementation\nof CFD workflows. Grounded in real-world CFD practices, our benchmark combines\na detailed task taxonomy with a rigorous evaluation framework to deliver\nreproducible results and quantify LLM performance across code executability,\nsolution accuracy, and numerical convergence behavior. CFDLLMBench establishes\na solid foundation for the development and evaluation of LLM-driven automation\nof numerical experiments for complex physical systems. Code and data are\navailable at https://github.com/NREL-Theseus/cfdllmbench/.", "AI": {"tldr": "本研究通过CFDLLMBench评估了大型语言模型LLM在计算流体动力学CFD领域的适用性，强调了其在复杂物理系统自动化数值实验中的潜力。", "motivation": "为了探索LLM在自动化复杂物理系统数值实验中的潜力，特别是在计算流体动力学（CFD）这一关键且耗费人力的组成部分中。", "method": "通过引入CFDLLMBench基准套件来评估LLM在计算流体动力学（CFD）领域的表现，该套件包含三个组成部分：CFDQuery、CFDCodeBench和FoamBench，旨在全面评估LLM的三个关键能力：高级CFD知识、CFD的数值和物理推理以及基于上下文的CFD工作流程实现。", "result": "基准测试结合了详细的任务分类和严谨的评估框架，能够在代码可执行性、解的精确度以及数值收敛行为方面提供可重复的结果并量化LLM的性能。", "conclusion": "CFDLLMBench为开发和评估基于LLM的复杂物理系统数值实验自动化提供了坚实的基础。"}}
{"id": "2509.20484", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20484", "abs": "https://arxiv.org/abs/2509.20484", "authors": ["Dani Manjah", "Tim Bary", "Benoît Gérin", "Benoît Macq", "Christophe de Vleeschouwer"], "title": "Data-Efficient Stream-Based Active Distillation for Scalable Edge Model Deployment", "comment": "6 pages, 3 figures, 2 algorithms, presented at SEEDS Workshop (ICIP\n  2025)", "summary": "Edge camera-based systems are continuously expanding, facing ever-evolving\nenvironments that require regular model updates. In practice, complex teacher\nmodels are run on a central server to annotate data, which is then used to\ntrain smaller models tailored to the edge devices with limited computational\npower. This work explores how to select the most useful images for training to\nmaximize model quality while keeping transmission costs low. Our work shows\nthat, for a similar training load (i.e., iterations), a high-confidence\nstream-based strategy coupled with a diversity-based approach produces a\nhigh-quality model with minimal dataset queries.", "AI": {"tldr": "研究提出了一种结合高置信度流策略与多样性选择方法来高效选择训练图像的方式，以在有限的训练负载下生成高质量模型同时减少传输成本。", "motivation": "随着边缘摄像系统不断发展，需要在计算资源有限的设备上维持高质量模型。现有的做法是在中央服务器上运行复杂的教师模型对数据进行标注，然后训练适合边缘设备的小模型。这提出了如何在此过程中高效选择训练图像的问题。", "method": "研究探讨了如何在保持传输成本低的同时，通过选择最有用的图像来最大化模型的质量。具体采用了高置信度流策略与多样性选择方法相结合的方式进行训练数据的选择与优化。", "result": "研究表明，对于相同的训练负载来说，结合高置信度流策略与多样性选择方法能够以最少的数据集查询次数生成高质量的模型。", "conclusion": "研究证明了其提出的训练数据选择策略的有效性，此方法能够在降低传输成本的同时保持模型的质量。"}}
{"id": "2509.20375", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20375", "abs": "https://arxiv.org/abs/2509.20375", "authors": ["Sharanya Parimanoharan", "Ruwan D. Nawarathna"], "title": "Assessing Classical Machine Learning and Transformer-based Approaches for Detecting AI-Generated Research Text", "comment": null, "summary": "The rapid adoption of large language models (LLMs) such as ChatGPT has\nblurred the line between human and AI-generated texts, raising urgent questions\nabout academic integrity, intellectual property, and the spread of\nmisinformation. Thus, reliable AI-text detection is needed for fair assessment\nto safeguard human authenticity and cultivate trust in digital communication.\nIn this study, we investigate how well current machine learning (ML) approaches\ncan distinguish ChatGPT-3.5-generated texts from human-written texts employing\na labeled data set of 250 pairs of abstracts from a wide range of research\ntopics. We test and compare both classical (Logistic Regression armed with\nclassical Bag-of-Words, POS, and TF-IDF features) and transformer-based (BERT\naugmented with N-grams, DistilBERT, BERT with a lightweight custom classifier,\nand LSTM-based N-gram models) ML detection techniques. As we aim to assess each\nmodel's performance in detecting AI-generated research texts, we also aim to\ntest whether an ensemble of these models can outperform any single detector.\nResults show DistilBERT achieves the overall best performance, while Logistic\nRegression and BERT-Custom offer solid, balanced alternatives; LSTM- and\nBERT-N-gram approaches lag. The max voting ensemble of the three best models\nfails to surpass DistilBERT itself, highlighting the primacy of a single\ntransformer-based representation over mere model diversity. By comprehensively\nassessing the strengths and weaknesses of these AI-text detection approaches,\nthis work lays a foundation for more robust transformer frameworks with larger,\nricher datasets to keep pace with ever-improving generative AI models.", "AI": {"tldr": "研究使用多种机器学习方法对比分析了区分ChatGPT-3.5生成文本与人类写作文本的能力，结果显示DistilBERT表现最佳，而Logistic回归和BERT-Custom表现稳定。最大投票集成方法未能超越DistilBERT，强调了单个转换器模型的重要性。研究为未来构建更强大的AI文本检测模型打下了基础。", "motivation": "随着大型语言模型如ChatGPT的广泛应用，机器生成文本和人类写作之间的界限日渐模糊。为保护人类原创性和维护数字通讯的诚信，需要可靠的方法来检测AI生成文本。因此，研究旨在评估机器学习技术在检测ChatGPT文本方面的有效性。", "method": "研究使用了250对来自不同研究领域的摘要样本作为数据集，比较了包括逻辑回归（配以词袋模型、词性标注和TF-IDF特征）和基于转换器的方法（DistilBERT、BERT加上轻量级分类器及基于LSTM的N-gram模型）在内的多种机器学习方法。", "result": "结果显示DistilBERT在检测AI生成的研究文本方面表现最好，而逻辑回归和BERT-Custom也能提供一个稳定的选择。相比之下，基于LSTM和BERT的N-gram方法表现欠佳。", "conclusion": "集成最佳模型的最大投票方法未能超越单个DistilBERT的表现，说明了转换器模型在文本检测方面的优越性。本研究为开发更强大的文本检测模型并提高其性能提供了一个基础。"}}
{"id": "2509.20524", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20524", "abs": "https://arxiv.org/abs/2509.20524", "authors": ["Julien Han", "Shuwen Qiu", "Qi Li", "Xingzi Xu", "Mehmet Saygin Seyfioglu", "Kavosh Asadi", "Karim Bouyarmane"], "title": "InstructVTON: Optimal Auto-Masking and Natural-Language-Guided Interactive Style Control for Inpainting-Based Virtual Try-On", "comment": "Submitted to CVPR 2025 and Published at CVPR 2025 AI for Content\n  Creation workshop", "summary": "We present InstructVTON, an instruction-following interactive virtual try-on\nsystem that allows fine-grained and complex styling control of the resulting\ngeneration, guided by natural language, on single or multiple garments. A\ncomputationally efficient and scalable formulation of virtual try-on formulates\nthe problem as an image-guided or image-conditioned inpainting task. These\ninpainting-based virtual try-on models commonly use a binary mask to control\nthe generation layout. Producing a mask that yields desirable result is\ndifficult, requires background knowledge, might be model dependent, and in some\ncases impossible with the masking-based approach (e.g. trying on a long-sleeve\nshirt with \"sleeves rolled up\" styling on a person wearing long-sleeve shirt\nwith sleeves down, where the mask will necessarily cover the entire sleeve).\nInstructVTON leverages Vision Language Models (VLMs) and image segmentation\nmodels for automated binary mask generation. These masks are generated based on\nuser-provided images and free-text style instructions. InstructVTON simplifies\nthe end-user experience by removing the necessity of a precisely drawn mask,\nand by automating execution of multiple rounds of image generation for try-on\nscenarios that cannot be achieved with masking-based virtual try-on models\nalone. We show that InstructVTON is interoperable with existing virtual try-on\nmodels to achieve state-of-the-art results with styling control.", "AI": {"tldr": "InstructVTON系统使用视觉语言模型和图像分割技术自动生成掩码，实现了用户友好的多风格虚拟试衣体验。", "motivation": "传统虚拟试衣方法依赖复杂的二值掩码控制生成布局，而生成满意的掩码难度高且受限，尤其是在一些模型难以单独实现的试衣场景。因此提出了InstructVTON以改进虚拟试衣的用户体验和表现。", "method": "InstructVTON采用视觉语言模型(VLMs)和图像分割模型自动生成二值掩码，根据用户提供的图像和自由文本风格指令生成掩码，简化了用户操作，实现了对虚拟试衣的细粒度和复杂风格控制。", "result": "InstructVTON能够与现有的虚拟试衣模型协同工作，实现具有风格控制的最先进结果。", "conclusion": "InstructVTON通过自动化二值掩码生成和多轮图像生成的执行，简化用户操作，提高了虚拟试衣系统的灵活性和适用范围。"}}
{"id": "2509.20376", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20376", "abs": "https://arxiv.org/abs/2509.20376", "authors": ["Haoxuan Li", "Zhen Wen", "Qiqi Jiang", "Chenxiao Li", "Yuwei Wu", "Yuchen Yang", "Yiyao Wang", "Xiuqi Huang", "Minfeng Zhu", "Wei Chen"], "title": "ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable performance across a\nwide range of natural language tasks. Understanding how LLMs internally\nrepresent knowledge remains a significant challenge. Despite Sparse\nAutoencoders (SAEs) have emerged as a promising technique for extracting\ninterpretable features from LLMs, SAE features do not inherently align with\nhuman-understandable concepts, making their interpretation cumbersome and\nlabor-intensive. To bridge the gap between SAE features and human concepts, we\npresent ConceptViz, a visual analytics system designed for exploring concepts\nin LLMs. ConceptViz implements a novel dentification => Interpretation =>\nValidation pipeline, enabling users to query SAEs using concepts of interest,\ninteractively explore concept-to-feature alignments, and validate the\ncorrespondences through model behavior verification. We demonstrate the\neffectiveness of ConceptViz through two usage scenarios and a user study. Our\nresults show that ConceptViz enhances interpretability research by streamlining\nthe discovery and validation of meaningful concept representations in LLMs,\nultimately aiding researchers in building more accurate mental models of LLM\nfeatures. Our code and user guide are publicly available at\nhttps://github.com/Happy-Hippo209/ConceptViz.", "AI": {"tldr": "ConceptViz是一个可视分析系统，用于探索大语言模型（LLMs）中的概念。它实现了一个识别 => 解释 => 验证的管道，帮助用户查询SAEs、交互探索概念与特征的对齐、以及验证模型行为，以验证这些对应关系。", "motivation": "虽然稀疏自动编码器（SAEs）作为一种从大语言模型中提取可解释特征的有前途的技术已经浮现，但SAE特征本身并不与人类可理解的概念对齐，使得其解释既困难又费时。为了填补SAE特征与人类概念之间的差距，提出了ConceptViz系统。", "method": "ConceptViz, 一个用于探索LLMs中概念的可视分析系统，它实现了一个新颖的识别 => 解释 => 验证的管道，使用户能够使用感兴趣的查询SAEs，交互地探索概念到特征的对齐，并通过模型行为验证来验证对应关系。", "result": "通过两个使用场景和一项用户研究证明了ConceptViz的有效性，结果表明ConceptViz通过简化发现和验证LLMs中意义重大的概念表示来增强可解释性研究。", "conclusion": "ConceptViz系统增强了解释性研究，通过简化发现和验证大语言模型中意义重大的概念表示，最终帮助研究人员构建更准确的大语言模型特征模型。"}}
{"id": "2509.20537", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20537", "abs": "https://arxiv.org/abs/2509.20537", "authors": ["Dana A Abdullah", "Dana Rasul Hamad", "Bishar Rasheed Ibrahim", "Sirwan Abdulwahid Aula", "Aso Khaleel Ameen", "Sabat Salih Hamadamin"], "title": "Innovative Deep Learning Architecture for Enhanced Altered Fingerprint Recognition", "comment": null, "summary": "Altered fingerprint recognition (AFR) is challenging for biometric\nverification in applications such as border control, forensics, and fiscal\nadmission. Adversaries can deliberately modify ridge patterns to evade\ndetection, so robust recognition of altered prints is essential. We present\nDeepAFRNet, a deep learning recognition model that matches and recognizes\ndistorted fingerprint samples. The approach uses a VGG16 backbone to extract\nhigh-dimensional features and cosine similarity to compare embeddings. We\nevaluate on the SOCOFing Real-Altered subset with three difficulty levels\n(Easy, Medium, Hard). With strict thresholds, DeepAFRNet achieves accuracies of\n96.7 percent, 98.76 percent, and 99.54 percent for the three levels. A\nthreshold-sensitivity study shows that relaxing the threshold from 0.92 to 0.72\nsharply degrades accuracy to 7.86 percent, 27.05 percent, and 29.51 percent,\nunderscoring the importance of threshold selection in biometric systems. By\nusing real altered samples and reporting per-level metrics, DeepAFRNet\naddresses limitations of prior work based on synthetic alterations or limited\nverification protocols, and indicates readiness for real-world deployments\nwhere both security and recognition resilience are critical.", "AI": {"tldr": "研究团队提出DeepAFRNet，一种识别修改指纹的深度学习模型，在真实修改指纹数据集上展示了高准确率，并突出了生物识别系统中阈值选择的重要性。", "motivation": "为了解决指纹识别中因指纹被恶意修改而影响生物识别验证的问题，特别是在边境控制、法医和财政准入等应用中。", "method": "DeepAFRNet, 使用VGG16骨干网络提取高维特征，并利用余弦相似度比较特征嵌入以识别变形指纹样本。", "result": "在SOCOFing Real-Altered数据集的三个难度级别（简易、中等、困难）上，采用严格的阈值，DeepAFRNet分别实现了96.7%、98.76%、99.54%的准确率。", "conclusion": "通过使用真实修改过的样本，并报告各水平指标，DeepAFRNet解决了以往基于合成修改或验证协议有限的工作中的局限性，表明它准备在安全性和识别韧性都至关重要的真实世界环境中应用。"}}
{"id": "2509.20377", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20377", "abs": "https://arxiv.org/abs/2509.20377", "authors": ["Tomoaki Isoda"], "title": "SKILL-RAG: Self-Knowledge Induced Learning and Filtering for Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has significantly improved the\nperformance of large language models (LLMs) on knowledge-intensive tasks in\nrecent years. However, since retrieval systems may return irrelevant content,\nincorporating such information into the model often leads to hallucinations.\nThus, identifying and filtering out unhelpful retrieved content is a key\nchallenge for improving RAG performance.To better integrate the internal\nknowledge of the model with external knowledge from retrieval, it is essential\nto understand what the model \"knows\" and \"does not know\" (which is also called\n\"self-knowledge\"). Based on this insight, we propose SKILL-RAG (Self-Knowledge\nInduced Learning and Filtering for RAG), a novel method that leverages the\nmodel's self-knowledge to determine which retrieved documents are beneficial\nfor answering a given query. We design a reinforcement learning-based training\nframework to explicitly elicit self-knowledge from the model and employs\nsentence-level granularity to filter out irrelevant content while preserving\nuseful knowledge.We evaluate SKILL-RAG using Llama2-7B and Qwen3-8B on several\nquestion answering benchmarks. Experimental results demonstrate that SKILL-RAG\nnot only improves generation quality but also significantly reduces the number\nof input documents, validating the importance of self-knowledge in guiding the\nselection of high-quality retrievals.", "AI": {"tldr": "提出了SKILL-RAG方法，利用模型的自我知识来选择有用的检索文档，通过强化学习训练框架提取自我知识，过滤掉无用内容。实验表明SKILL-RAG提升了生成的质量并减少了输入文档的数量。", "motivation": "检索增强生成（RAG）在最近几年极大地提高了大型语言模型（LLM）在知识密集型任务上的表现。然而，由于检索系统可能返回不相关的文档，这些信息被集成进模型常常会导致幻觉，因此识别并过滤掉无用的检索内容是提高RAG性能的重要挑战。为了更好地将模型的内部知识与外部知识结合起来，理解模型“知道”什么以及“不知”什么是非常重要的。", "method": "我们提出了SKILL-RAG（Self-Knowledge Induced Learning and Filtering for RAG），利用模型的自我知识来判定哪些检索到的文档有利于回答给定的查询。我们设计了一个基于强化学习的训练框架，以明确地从模型中提取自我知识，并采用句子级别粒度来过滤掉无用的内容同时保留有用的知识。", "result": "我们使用Llama2-7B和Qwen3-8B在多个问答基准上评估了SKILL-RAG。实验结果表明，SKILL-RAG不仅提高了生成质量，而且还显著减少了输入文档的数量，验证了自我知识在指导高质量检索选择中的重要性。", "conclusion": "实验验证了自我知识在指导高质量检索选择中的重要性。SKILL-RAG不仅改善了生成的质量还显著减少了输入文档的数量。这种方法能够有效地识别并利用模型的自我知识，以便对检索的内容进行高质量的选择。"}}
{"id": "2509.20579", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20579", "abs": "https://arxiv.org/abs/2509.20579", "authors": ["Hanna Yurchyk", "Wei-Di Chang", "Gregory Dudek", "David Meger"], "title": "Large Pre-Trained Models for Bimanual Manipulation in 3D", "comment": "Accepted to 2025 IEEE-RAS 24th International Conference on Humanoid\n  Robots", "summary": "We investigate the integration of attention maps from a pre-trained Vision\nTransformer into voxel representations to enhance bimanual robotic\nmanipulation. Specifically, we extract attention maps from DINOv2, a\nself-supervised ViT model, and interpret them as pixel-level saliency scores\nover RGB images. These maps are lifted into a 3D voxel grid, resulting in\nvoxel-level semantic cues that are incorporated into a behavior cloning policy.\nWhen integrated into a state-of-the-art voxel-based policy, our\nattention-guided featurization yields an average absolute improvement of 8.2%\nand a relative gain of 21.9% across all tasks in the RLBench bimanual\nbenchmark.", "AI": {"tldr": "本研究将预训练Vision Transformer的注意力图与体素表示结合，用于提高双臂机器人的操作性能，取得了显著提升。", "motivation": "研究如何改善双臂机器人的操作性能，特别是通过引入高级视觉信息的方式。", "method": "通过将预训练的Vision Transformer中的注意力图整合到体素表示中以增强双臂机器人操作。具体地，从自我监督的ViT模型DINOv2中提取注意力图，并将它们解释为RGB图像上的像素级显著性分数。这些图被提升到3D体素网格中，形成体素级语义线索，并被整合到行为克隆策略中。", "result": "当融入最先进的体素策略时，我们的注意力引导特征化在RLBench双臂基准上所有任务中平均绝对提高了8.2%，相对提高了21.9%。", "conclusion": "注意力引导的特征化可以显著提高体素表示的性能，从而提升机器人的操作性能。"}}
{"id": "2509.20378", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20378", "abs": "https://arxiv.org/abs/2509.20378", "authors": ["Sirui Wang", "Andong Chen", "Tiejun Zhao"], "title": "Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation", "comment": null, "summary": "Emotional text-to-speech (E-TTS) is central to creating natural and\ntrustworthy human-computer interaction. Existing systems typically rely on\nsentence-level control through predefined labels, reference audio, or natural\nlanguage prompts. While effective for global emotion expression, these\napproaches fail to capture dynamic shifts within a sentence. To address this\nlimitation, we introduce Emo-FiLM, a fine-grained emotion modeling framework\nfor LLM-based TTS. Emo-FiLM aligns frame-level features from emotion2vec to\nwords to obtain word-level emotion annotations, and maps them through a\nFeature-wise Linear Modulation (FiLM) layer, enabling word-level emotion\ncontrol by directly modulating text embeddings. To support evaluation, we\nconstruct the Fine-grained Emotion Dynamics Dataset (FEDD) with detailed\nannotations of emotional transitions. Experiments show that Emo-FiLM\noutperforms existing approaches on both global and fine-grained tasks,\ndemonstrating its effectiveness and generality for expressive speech synthesis.", "AI": {"tldr": "该研究开发了Emo-FiLM细粒度情感模型，提升了文本到语音的细粒度情感表达能力。", "motivation": "现有系统通常依赖句子级别的控制，通过预定义的标签、参考音频或自然语言提示。这些方法对于全局情感表达是有效的，但不能捕捉句子内部的情感动态变化。为了弥补这一局限，提出Emo-FiLM框架。", "method": "Emo-FiLM是一个细粒度情感建模框架，它将来自emotion2vec的帧级特征与单词对齐以获得单词级情感标注，并通过特征级线性调制(FiLM)层将它们映射，从而实现对文本嵌入的直接调制，以控制单词级情感。", "result": "实验结果表明，Emo-FiLM在全局和细粒度任务上均优于现有方法，证明了其在表达语音合成中的有效性和通用性。", "conclusion": "这项研究提出了一种新的Emo-FiLM框架，使文本到语音系统能够更好地捕捉和表达句子内部的情感变化，改善了语音合成的自然度和可信度。"}}
{"id": "2509.20580", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20580", "abs": "https://arxiv.org/abs/2509.20580", "authors": ["Xinyang Mu", "Yuzhen Lu", "Boyang Deng"], "title": "A Comparative Benchmark of Real-time Detectors for Blueberry Detection towards Precision Orchard Management", "comment": "19 pages, 6 figures, 4 tables. Abstract abridged due to arXiv's 1920\n  character limit", "summary": "Blueberry detection in natural environments remains challenging due to\nvariable lighting, occlusions, and motion blur due to environmental factors and\nimaging devices. Deep learning-based object detectors promise to address these\nchallenges, but they demand a large-scale, diverse dataset that captures the\nreal-world complexities. Moreover, deploying these models in practical\nscenarios often requires the right accuracy/speed/memory trade-off in model\nselection. This study presents a novel comparative benchmark analysis of\nadvanced real-time object detectors, including YOLO (You Only Look Once)\n(v8-v12) and RT-DETR (Real-Time Detection Transformers) (v1-v2) families,\nconsisting of 36 model variants, evaluated on a newly curated dataset for\nblueberry detection. This dataset comprises 661 canopy images collected with\nsmartphones during the 2022-2023 seasons, consisting of 85,879 labelled\ninstances (including 36,256 ripe and 49,623 unripe blueberries) across a wide\nrange of lighting conditions, occlusions, and fruit maturity stages. Among the\nYOLO models, YOLOv12m achieved the best accuracy with a mAP@50 of 93.3%, while\nRT-DETRv2-X obtained a mAP@50 of 93.6%, the highest among all the RT-DETR\nvariants. The inference time varied with the model scale and complexity, and\nthe mid-sized models appeared to offer a good accuracy-speed balance. To\nfurther enhance detection performance, all the models were fine-tuned using\nUnbiased Mean Teacher-based semi-supervised learning (SSL) on a separate set of\n1,035 unlabeled images acquired by a ground-based machine vision platform in\n2024. This resulted in accuracy gains ranging from -1.4% to 2.9%, with\nRT-DETR-v2-X achieving the best mAP@50 of 94.8%. More in-depth research into\nSSL is needed to better leverage cross-domain unlabeled data. Both the dataset\nand software programs of this study are made publicly available to support\nfurther research.", "AI": {"tldr": "本研究通过比较分析了实时目标检测器在蓝莓检测中的性能，使用了包含大量标注实例的新数据集，并使用了半监督学习提高了模型准确性。", "motivation": "解决蓝莓在自然环境中因光照、遮挡和运动模糊而产生的检测挑战，需要使用权重、速度、内存之间进行权衡的实际模型。", "method": "通过使用包含85879个标注实例的新数据集对其进行测试，对比了YOLO和RT-DETR家族中的36种模型变异。同时，使用Unbiased Mean Teacher进行半监督学习以进一步提高检测性能。", "result": "最佳检测准确性模型为RT-DETRv2-X，基于mAP@50标准达到了94.8%，通过半监督学习准确性有少量提升。", "conclusion": "新数据集和软件程序可供进一步研究使用，未来需要更广泛的研究来更好地利用跨域无标签数据。"}}
{"id": "2509.20381", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20381", "abs": "https://arxiv.org/abs/2509.20381", "authors": ["Jianyu Wen", "Jingyun Wang", "Cilin Yan", "Jiayin Cai", "Xiaolong Jiang", "Ying Zhang"], "title": "USB-Rec: An Effective Framework for Improving Conversational Recommendation Capability of Large Language Model", "comment": "Accepted by Recsys'25", "summary": "Recently, Large Language Models (LLMs) have been widely employed in\nConversational Recommender Systems (CRSs). Unlike traditional language model\napproaches that focus on training, all existing LLMs-based approaches are\nmainly centered around how to leverage the summarization and analysis\ncapabilities of LLMs while ignoring the issue of training. Therefore, in this\nwork, we propose an integrated training-inference framework,\nUser-Simulator-Based framework (USB-Rec), for improving the performance of LLMs\nin conversational recommendation at the model level. Firstly, we design a\nLLM-based Preference Optimization (PO) dataset construction strategy for RL\ntraining, which helps the LLMs understand the strategies and methods in\nconversational recommendation. Secondly, we propose a Self-Enhancement Strategy\n(SES) at the inference stage to further exploit the conversational\nrecommendation potential obtained from RL training. Extensive experiments on\nvarious datasets demonstrate that our method consistently outperforms previous\nstate-of-the-art methods.", "AI": {"tldr": "提出了一种基于用户模拟器的训练-推理框架USB-Rec，以提高LLM在对话推荐中的性能。", "motivation": "尽管LLMs在CRS中广泛使用，但现有的方法大多集中于利用LLMs的总结和分析能力，而忽视了训练问题。", "method": "设计了一种基于LLMs的偏好优化(PO)数据集构建策略进行RL训练，并在推理阶段提出了一种自我增强策略(SES)。", "result": "在各种数据集上的广泛实验表明，该方法在对话推荐任务中始终优于先前的最先进方法。", "conclusion": "USB-Rec框架通过RL训练和自我增强策略在对话推荐性能方面提供了显著改进。"}}
{"id": "2509.20585", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20585", "abs": "https://arxiv.org/abs/2509.20585", "authors": ["Farbod Bigdeli", "Mohsen Mohammadagha", "Ali Bigdeli"], "title": "Region-of-Interest Augmentation for Mammography Classification under Patient-Level Cross-Validation", "comment": "5 pages, 5 figures, 2 tables", "summary": "Breast cancer screening with mammography remains central to early detection\nand mortality reduction. Deep learning has shown strong potential for\nautomating mammogram interpretation, yet limited-resolution datasets and small\nsample sizes continue to restrict performance. We revisit the Mini-DDSM dataset\n(9,684 images; 2,414 patients) and introduce a lightweight region-of-interest\n(ROI) augmentation strategy. During training, full images are probabilistically\nreplaced with random ROI crops sampled from a precomputed, label-free\nbounding-box bank, with optional jitter to increase variability. We evaluate\nunder strict patient-level cross-validation and report ROC-AUC, PR-AUC, and\ntraining-time efficiency metrics (throughput and GPU memory). Because ROI\naugmentation is training-only, inference-time cost remains unchanged. On\nMini-DDSM, ROI augmentation (best: p_roi = 0.10, alpha = 0.10) yields modest\naverage ROC-AUC gains, with performance varying across folds; PR-AUC is flat to\nslightly lower. These results demonstrate that simple, data-centric ROI\nstrategies can enhance mammography classification in constrained settings\nwithout requiring additional labels or architectural modifications.", "AI": {"tldr": "研究提出了一种轻量级ROI增强策略，旨在提高乳腺摄影中深度学习模型的性能，特别是在数据受限场景下。", "motivation": "现有的低分辨率数据集和样本量小限制了深度学习在乳腺摄影自动解读中的性能，故引入轻量级ROI增强策略。", "method": "通过随机感兴趣区域（ROI）裁剪策略来扩充数据集，训练时以一定概率用预计算的无标签边界框内随机裁剪的ROI代替完整图像，可选添加抖动以增加多样性。", "result": "在Mini-DDSM数据集上进行严格的患者级交叉验证，结果显示ROI增强策略在ROC-AUC上略有提升，但PR-AUC保持不变或略有下降。", "conclusion": "简单的基于数据的ROI策略能在不增加标签或修改架构的情况下，提高受限条件下的乳腺摄影分类性能。"}}
{"id": "2509.20461", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20461", "abs": "https://arxiv.org/abs/2509.20461", "authors": ["Bruce Kuwahara", "Chen-Yuan Lin", "Xiao Shi Huang", "Kin Kwan Leung", "Jullian Arta Yapeter", "Ilya Stanevich", "Felipe Perez", "Jesse C. Cresswell"], "title": "Document Summarization with Conformal Importance Guarantees", "comment": "NeurIPS 2025. Code is available at\n  https://github.com/layer6ai-labs/conformal-importance-summarization", "summary": "Automatic summarization systems have advanced rapidly with large language\nmodels (LLMs), yet they still lack reliable guarantees on inclusion of critical\ncontent in high-stakes domains like healthcare, law, and finance. In this work,\nwe introduce Conformal Importance Summarization, the first framework for\nimportance-preserving summary generation which uses conformal prediction to\nprovide rigorous, distribution-free coverage guarantees. By calibrating\nthresholds on sentence-level importance scores, we enable extractive document\nsummarization with user-specified coverage and recall rates over critical\ncontent. Our method is model-agnostic, requires only a small calibration set,\nand seamlessly integrates with existing black-box LLMs. Experiments on\nestablished summarization benchmarks demonstrate that Conformal Importance\nSummarization achieves the theoretically assured information coverage rate. Our\nwork suggests that Conformal Importance Summarization can be combined with\nexisting techniques to achieve reliable, controllable automatic summarization,\npaving the way for safer deployment of AI summarization tools in critical\napplications. Code is available at\nhttps://github.com/layer6ai-labs/conformal-importance-summarization.", "AI": {"tldr": "提出了一种名为Conformal Importance Summarization的框架，通过符合性预测来提供关键内容抽取的覆盖率保证，使之适用于医疗、法律和金融等高风险领域的自动摘要。", "motivation": "尽管自动摘要系统随着大语言模型的发展而快速进步，但是在医疗、法律和金融等高风险领域中，这些系统仍然缺乏对于关键内容纳入的可靠保障。因此，我们提出了上述框架。", "method": "我们引入了一种名为Conformal Importance Summarization的框架，该框架使用符合性预测（conformal prediction）来为抽取式文档摘要生成提供严格且分布无关的覆盖率保证。通过校准基于句子的重要程度得分阈值，我们实现了对关键内容进行用户指定的覆盖率和召回率控制的摘要生成。该方法不依赖于具体的模型，并且仅需一个小的校准集，还能无缝集成现有的黑盒大语言模型。", "result": "在已建立的摘要基准测试上的实验证明，Conformal Importance Summarization取得了理论上保证的信息覆盖率。", "conclusion": "实验结果表明，Conformal Importance Summarization达到了理论保证的信息覆盖率。这项工作表明，结合现有技术可以实现可靠的、可控的自动摘要，为人工智能摘要工具在关键领域的安全应用奠定了基础。"}}
{"id": "2509.20607", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20607", "abs": "https://arxiv.org/abs/2509.20607", "authors": ["Jing Wu", "Zirui Wang", "Iro Laina", "Victor Adrian Prisacariu"], "title": "Reflect3r: Single-View 3D Stereo Reconstruction Aided by Mirror Reflections", "comment": null, "summary": "Mirror reflections are common in everyday environments and can provide stereo\ninformation within a single capture, as the real and reflected virtual views\nare visible simultaneously. We exploit this property by treating the reflection\nas an auxiliary view and designing a transformation that constructs a\nphysically valid virtual camera, allowing direct pixel-domain generation of the\nvirtual view while adhering to the real-world imaging process. This enables a\nmulti-view stereo setup from a single image, simplifying the imaging process,\nmaking it compatible with powerful feed-forward reconstruction models for\ngeneralizable and robust 3D reconstruction. To further exploit the geometric\nsymmetry introduced by mirrors, we propose a symmetric-aware loss to refine\npose estimation. Our framework also naturally extends to dynamic scenes, where\neach frame contains a mirror reflection, enabling efficient per-frame geometry\nrecovery. For quantitative evaluation, we provide a fully customizable\nsynthetic dataset of 16 Blender scenes, each with ground-truth point clouds and\ncamera poses. Extensive experiments on real-world data and synthetic data are\nconducted to illustrate the effectiveness of our method.", "AI": {"tldr": "该论文提出了一种利用镜子反射来辅助3D重建的方法，通过设计虚拟相机变换和对称感知损耗来直接生成立体图像，此方法在真实世界和合成数据中均显示出了显著的效果。", "motivation": "镜子反射在日常生活环境中很常见，可以提供单张抓取中的立体信息，因为真实视角和反射视角可以同时被观察到。本论文旨在利用这一特性简化成像过程并提高3D重建的通用性和鲁棒性。", "method": "该论文提出了一种利用镜子反射作为辅助视角来生成虚拟视图的方法，通过设计一个物理上合理的虚拟相机变换，直接在像素域生成虚拟视图，这使得可以从单一图像中获取多视角立体信息，从而简化图像处理过程，使之适用于强大的前馈重建模型以实现通用且鲁棒的3D重建。此外，其还提出了一种对称感知损耗以改善位姿估计，拓展到动态场景中，通过每一帧中的镜子反射来实现高效的几何恢复。", "result": "为了评估该方法的有效性，论文提供了包含16个Blender场景的全自定义合成数据集，每个场景都有地面实况点云和相机姿势。通过在真实世界数据和合成数据上的广泛实验，证明了本方法的有效性。", "conclusion": "论文得出的结论是，该方法通过利用镜子反射作为多视角信息来源，能够简化3D重建过程并提升了重建的稳定性和可靠性，适用于静态和动态场景。"}}
{"id": "2509.20467", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20467", "abs": "https://arxiv.org/abs/2509.20467", "authors": ["Henrik Vatndal", "Vinay Setty"], "title": "ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos", "comment": null, "summary": "Short-form video platforms like TikTok present unique challenges for\nmisinformation detection due to their multimodal, dynamic, and noisy content.\nWe present ShortCheck, a modular, inference-only pipeline with a user-friendly\ninterface that automatically identifies checkworthy short-form videos to help\nhuman fact-checkers. The system integrates speech transcription, OCR, object\nand deepfake detection, video-to-text summarization, and claim verification.\nShortCheck is validated by evaluating it on two manually annotated datasets\nwith TikTok videos in a multilingual setting. The pipeline achieves promising\nresults with F1-weighted score over 70\\%.", "AI": {"tldr": "提出了一种名为ShortCheck的系统，能自动识别出值得核查的短视频，以帮助人工事实核查，并在多语言环境下验证了其有效性。", "motivation": "由于TikTok等短视频平台的内容具有多模式、动态和复杂的特点，从而对于虚假信息的检测提出了独特的挑战。为了应对这一挑战，提出了ShortCheck。", "method": "ShortCheck是一种模块化、仅用于推理的流水线，具有一用户友好的界面，能够自动识别出值得核查的短视频，从而辅助人工事实核查。该系统集成了语音转录、OCR、物体和深度伪造检测、视频到文本的总结以及声明验证等功能。", "result": "在两个包含多语言TikTok视频的手动注释数据集上验证了ShortCheck，该流水线取得了重样的结果，加权F1分数超过70%。", "conclusion": "证明了ShortCheck在检测短视频平台上的虚假信息方面具有潜力，特别在集成多种技术的情况下表现出色。"}}
{"id": "2509.20628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20628", "abs": "https://arxiv.org/abs/2509.20628", "authors": ["Yiming Xiao", "Archit Gupta", "Miguel Esparza", "Yu-Hsuan Ho", "Antonia Sebastian", "Hannah Weas", "Rose Houck", "Ali Mostafavi"], "title": "Recov-Vision: Linking Street View Imagery and Vision-Language Models for Post-Disaster Recovery", "comment": "17 pages, 10 figures", "summary": "Building-level occupancy after disasters is vital for triage, inspections,\nutility re-energization, and equitable resource allocation. Overhead imagery\nprovides rapid coverage but often misses facade and access cues that determine\nhabitability, while street-view imagery captures those details but is sparse\nand difficult to align with parcels. We present FacadeTrack, a street-level,\nlanguage-guided framework that links panoramic video to parcels, rectifies\nviews to facades, and elicits interpretable attributes (for example, entry\nblockage, temporary coverings, localized debris) that drive two decision\nstrategies: a transparent one-stage rule and a two-stage design that separates\nperception from conservative reasoning. Evaluated across two post-Hurricane\nHelene surveys, the two-stage approach achieves a precision of 0.927, a recall\nof 0.781, and an F-1 score of 0.848, compared with the one-stage baseline at a\nprecision of 0.943, a recall of 0.728, and an F-1 score of 0.822. Beyond\naccuracy, intermediate attributes and spatial diagnostics reveal where and why\nresidual errors occur, enabling targeted quality control. The pipeline provides\nauditable, scalable occupancy assessments suitable for integration into\ngeospatial and emergency-management workflows.", "AI": {"tldr": "本文提出了一个名为FacadeTrack的框架，通过分析街景图像来评估灾后建筑的可居住性，并展示了两阶段决策策略相较于一阶段策略的精度和召回率的提高。", "motivation": "大楼级别的灾后占用情况对于分类、检查、恢复电力供应以及公平资源分配至关重要。而现有的高空图像和街景图像都有局限性，无法准确捕捉到判断可居住性的门面和入口细节。", "method": "FacadeTrack, 一个街道级别的、语言引导的框架，链接全景视频到地块，校正视图到立面，并提取可解释的属性（比如入口阻塞、临时覆盖、局部杂物）来推动两个决策策略：一个透明的一阶段规则和一个将感知与保守推理分离的两阶段设计。", "result": "在两次飓风海伦过后的调查中，两阶段方法实现了精度0.927，召回率0.781，F-1分数0.848，对比一阶段基线的精度0.943，召回率0.728，F-1分数0.822。", "conclusion": "该方法不仅在准确性上有提升，还能揭示残差错误发生的位置和原因，支持有针对性的质量控制，并提供可审计的、可扩展的占用评估，适用于集成到地理空间和应急管理的工作流程中。"}}
{"id": "2509.20502", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20502", "abs": "https://arxiv.org/abs/2509.20502", "authors": ["Xiao Wang", "Jia Wang", "Yijie Wang", "Pengtao Dang", "Sha Cao", "Chi Zhang"], "title": "MARS: toward more efficient multi-agent collaboration for LLM reasoning", "comment": null, "summary": "Large language models (LLMs) have achieved impressive results in natural\nlanguage understanding, yet their reasoning capabilities remain limited when\noperating as single agents. Multi-Agent Debate (MAD) has been proposed to\naddress this limitation by enabling collaborative reasoning among multiple\nmodels in a round-table debate manner. While effective, MAD introduces\nsubstantial computational overhead due to the number of agents involved and the\nfrequent communication required. In this paper, we propose MARS (Multi-Agent\nReview System), a role-based collaboration framework inspired by the review\nprocess. In MARS, an author agent generates an initial solution, reviewer\nagents provide decisions and comments independently, and a meta-reviewer\nintegrates the feedback to make the final decision and guide further revision.\nThis design enhances reasoning quality while avoiding costly\nreviewer-to-reviewer interactions, thereby controlling token consumption and\ninference time. We compared MARS with both MAD and other state-of-the-art\nreasoning strategies across multiple benchmarks. Extensive experiments with\ndifferent LLMs show that MARS matches the accuracy of MAD while reducing both\ntoken usage and inference time by approximately 50\\%. Code is available at\nhttps://github.com/xwang97/MARS.", "AI": {"tldr": "MARS, an efficient role-based collaboration framework for large language models, achieves similar reasoning accuracy as MAD but with significantly reduced computational overhead.", "motivation": "To enhance reasoning quality of large language models (LLMs) while minimizing computational overhead, which is a limitation of the Multi-Agent Debate (MAD) approach due to its multi-agent involvement and frequent communication needs.", "method": "MARS (Multi-Agent Review System), a role-based collaboration framework, includes an author agent generating an initial solution, reviewer agents providing independent decisions and comments, and a meta-reviewer integrating feedback for the final decision and guidance on further revisions.", "result": "Experiments show MARS matches MAD's accuracy with approximately 50% reduction in token usage and inference time.", "conclusion": "MARS effectively provides enhanced collaborative reasoning akin to MAD but with reduced computational costs, making it a promising strategy for large language model reasoning."}}
{"id": "2509.20673", "categories": ["cs.CV", "cs.CE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20673", "abs": "https://arxiv.org/abs/2509.20673", "authors": ["Yiling Yun", "Hongjing Lu"], "title": "Human Semantic Representations of Social Interactions from Moving Shapes", "comment": null, "summary": "Humans are social creatures who readily recognize various social interactions\nfrom simple display of moving shapes. While previous research has often focused\non visual features, we examine what semantic representations that humans employ\nto complement visual features. In Study 1, we directly asked human participants\nto label the animations based on their impression of moving shapes. We found\nthat human responses were distributed. In Study 2, we measured the\nrepresentational geometry of 27 social interactions through human similarity\njudgments and compared it with model predictions based on visual features,\nlabels, and semantic embeddings from animation descriptions. We found that\nsemantic models provided complementary information to visual features in\nexplaining human judgments. Among the semantic models, verb-based embeddings\nextracted from descriptions account for human similarity judgments the best.\nThese results suggest that social perception in simple displays reflects the\nsemantic structure of social interactions, bridging visual and abstract\nrepresentations.", "AI": {"tldr": "研究揭示了人类在识别简单的移动形状动画中的社会互动时，不仅依赖视觉特征，还使用从动画描述中提取的语义（尤其是动词嵌入）作为解释其判断的补充信息。", "motivation": "尽管以往的研究主要关注视觉特征，但本研究探索了人类使用的语义表示如何补充视觉特征来识别复杂社会互动。", "method": "通过两项研究来探讨人类在识别从移动形状显示的社会互动时使用的语义表示。研究1中，直接要求参与者根据移动形状的印象对动画进行标记；研究2中，通过人类相似性判断来测量27种社会互动的表示几何，并将其与基于视觉特征、标签和动画描述的语义嵌入模型预测进行比较。", "result": "研究发现语义模型为解释人类判断提供了补充信息，在语义模型中，从描述中提取的动词嵌入代表了人类相似性判断的最佳解释。", "conclusion": "这些结果显示了简单的展示中的社会感知反映了社会互动的语义结构，连接了视觉和抽象表征。"}}
{"id": "2509.20557", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20557", "abs": "https://arxiv.org/abs/2509.20557", "authors": ["Hannah Liu", "Junghyun Min", "Ethan Yue Heng Cheung", "Shou-Yi Hung", "Syed Mekael Wasti", "Runtong Liang", "Shiyao Qian", "Shizhao Zheng", "Elsie Chan", "Ka Ieng Charlotte Lo", "Wing Yu Yip", "Richard Tzong-Han Tsai", "En-Shiun Annie Lee"], "title": "SiniticMTError: A Machine Translation Dataset with Error Annotations for Sinitic Languages", "comment": "Work in progress. 14 pages, 4 figures, 5 tables", "summary": "Despite major advances in machine translation (MT) in recent years, progress\nremains limited for many low-resource languages that lack large-scale training\ndata and linguistic resources. Cantonese and Wu Chinese are two Sinitic\nexamples, although each enjoys more than 80 million speakers around the world.\nIn this paper, we introduce SiniticMTError, a novel dataset that builds on\nexisting parallel corpora to provide error span, error type, and error severity\nannotations in machine-translated examples from English to Mandarin, Cantonese,\nand Wu Chinese. Our dataset serves as a resource for the MT community to\nutilize in fine-tuning models with error detection capabilities, supporting\nresearch on translation quality estimation, error-aware generation, and\nlow-resource language evaluation. We report our rigorous annotation process by\nnative speakers, with analyses on inter-annotator agreement, iterative\nfeedback, and patterns in error type and severity.", "AI": {"tldr": "本文介绍了一个名为SiniticMTError的数据集，用于支持低资源语言的机器翻译错误检测及相关研究。", "motivation": "尽管近年来机器翻译取得重大进展，但对于缺乏大规模训练数据和语言资源的低资源语言，进展仍然有限。本文旨在为这些语言的机器翻译错误检测提供支持。", "method": "介绍了一个名为SiniticMTError的新数据集，该数据集基于现有的平行语料库，提供从英语到官话、粤语和吴语的机器翻译示例中的错误范围、错误类型和错误严重程度注释。", "result": "报告了严格的注释过程，并分析了注释者之间的协议、迭代反馈以及错误类型和严重程度的模式。", "conclusion": "该数据集作为机器翻译社区的资源，支持研究翻译质量评估、错误感知生成和低资源语言评估。"}}
{"id": "2509.20684", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20684", "abs": "https://arxiv.org/abs/2509.20684", "authors": ["Xiaowei Wang", "Di Wang", "Ke Li", "Yifeng Wang", "Chengjian Wang", "Libin Sun", "Zhihong Wu", "Yiming Zhang", "Quan Wang"], "title": "Enhancing Cross-View Geo-Localization Generalization via Global-Local Consistency and Geometric Equivariance", "comment": null, "summary": "Cross-view geo-localization (CVGL) aims to match images of the same location\ncaptured from drastically different viewpoints. Despite recent progress,\nexisting methods still face two key challenges: (1) achieving robustness under\nsevere appearance variations induced by diverse UAV orientations and fields of\nview, which hinders cross-domain generalization, and (2) establishing reliable\ncorrespondences that capture both global scene-level semantics and fine-grained\nlocal details. In this paper, we propose EGS, a novel CVGL framework designed\nto enhance cross-domain generalization. Specifically, we introduce an\nE(2)-Steerable CNN encoder to extract stable and reliable features under\nrotation and viewpoint shifts. Furthermore, we construct a graph with a virtual\nsuper-node that connects to all local nodes, enabling global semantics to be\naggregated and redistributed to local regions, thereby enforcing global-local\nconsistency. Extensive experiments on the University-1652 and SUES-200\nbenchmarks demonstrate that EGS consistently achieves substantial performance\ngains and establishes a new state of the art in cross-domain CVGL.", "AI": {"tldr": "本文介绍了一个新的CVGL框架EGS，旨在增强跨域泛化能力，并在多个基准上验证了其优越性。", "motivation": "现有方法在处理严重外观变化和建立可靠的全局和局部细节对应关系上有两个主要挑战。", "method": "我们提出了EGS，一个增强跨域泛化的CVGL框架。具体包括：1) E(2)-可操纵CNN编码器来提取在旋转和视角变化下稳定的特征；2) 构建一个与所有局部节点相连的虚拟超级节点的图，以聚合和重分布全局语义到局部区域，从而强化全局-局部一致性。", "result": "在University-1652和SUES-200基准测试上的大量实验表明，EGS在跨域CVGL中持续取得显著性能提升并达到新的最先进水平。", "conclusion": "EGS通过其创新的方法解决了CVGL中普遍存在的两个难题，显著提升了跨域泛化能力和性能表现。"}}
{"id": "2509.20567", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20567", "abs": "https://arxiv.org/abs/2509.20567", "authors": ["Ayan Sar", "Pranav Singh Puri", "Sumit Aich", "Tanupriya Choudhury", "Abhijit Kumar"], "title": "SwasthLLM: a Unified Cross-Lingual, Multi-Task, and Meta-Learning Zero-Shot Framework for Medical Diagnosis Using Contrastive Representations", "comment": "Submitted to International Conference on Big Data 2025", "summary": "In multilingual healthcare environments, automatic disease diagnosis from\nclinical text remains a challenging task due to the scarcity of annotated\nmedical data in low-resource languages and the linguistic variability across\npopulations. This paper proposes SwasthLLM, a unified, zero-shot,\ncross-lingual, and multi-task learning framework for medical diagnosis that\noperates effectively across English, Hindi, and Bengali without requiring\nlanguage-specific fine-tuning. At its core, SwasthLLM leverages the\nmultilingual XLM-RoBERTa encoder augmented with a language-aware attention\nmechanism and a disease classification head, enabling the model to extract\nmedically relevant information regardless of the language structure. To align\nsemantic representations across languages, a Siamese contrastive learning\nmodule is introduced, ensuring that equivalent medical texts in different\nlanguages produce similar embeddings. Further, a translation consistency module\nand a contrastive projection head reinforce language-invariant representation\nlearning. SwasthLLM is trained using a multi-task learning strategy, jointly\noptimizing disease classification, translation alignment, and contrastive\nlearning objectives. Additionally, we employ Model-Agnostic Meta-Learning\n(MAML) to equip the model with rapid adaptation capabilities for unseen\nlanguages or tasks with minimal data. Our phased training pipeline emphasizes\nrobust representation alignment before task-specific fine-tuning. Extensive\nevaluation shows that SwasthLLM achieves high diagnostic performance, with a\ntest accuracy of 97.22% and an F1-score of 97.17% in supervised settings.\nCrucially, in zero-shot scenarios, it attains 92.78% accuracy on Hindi and\n73.33% accuracy on Bengali medical text, demonstrating strong generalization in\nlow-resource contexts.", "AI": {"tldr": "SwasthLLM是一个用于英语、印地语和孟加拉语医学诊断的统一零样本跨语言多任务学习框架，无需特定语言的微调。SwasthLLM能有效提取医学相关的信息，并在零样本场景下显示出在低资源语言中的良好泛化性能。", "motivation": "在多语言医疗环境中，由于低资源语言注释医学数据匮乏和人群语言差异大，从临床文本中自动疾病诊断仍然是一个具有挑战性的任务。本文旨在开发一种新的框架，以解决在多语言环境中跨不同语言的自动疾病诊断问题。", "method": "本文提出了SwasthLLM框架，该框架是一个统一的零样本、跨语言、多任务学习框架，用于英语、印地语和孟加拉语的医学诊断，无需特定语言的微调。SwasthLLM使用多语言XLM-RoBERTa编码器，结合语言感知注意力机制和疾病分类头部，从临床文本中提取医学相关信息。为了对齐不同语言之间的语义表示，引入了暹罗对比学习模块，并且使用翻译一致性模块和对比投影头部来强化语言不变表示学习。通过多任务学习策略，SwasthLLM同时优化疾病分类、翻译对齐和对比学习目标，并采用MAML使模型具有快速适应未见语言或任务的能力。", "result": "实验结果证明，SwasthLLM在多种语言环境下实现了高效的医学诊断。在监督环境下，实现了97.22%的测试准确率和97.17%的F1得分；在零样本环境下，对印地语文本的准确率达到92.78%，对孟加拉语文本的准确率达到73.33%。", "conclusion": "实验结果表明，SwasthLLM在监督设置中达到97.22%的测试准确率和97.17%的F1得分，并在零样本场景中，在印地语上达到92.78%的准确率，在孟加拉语上达到73.33%的准确率，展示了在低资源环境中的强大泛化能力。"}}
{"id": "2509.20701", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20701", "abs": "https://arxiv.org/abs/2509.20701", "authors": ["Jiayi Zuo", "Songwei Pei", "Qian Li"], "title": "DENet: Dual-Path Edge Network with Global-Local Attention for Infrared Small Target Detection", "comment": null, "summary": "Infrared small target detection is crucial for remote sensing applications\nlike disaster warning and maritime surveillance. However, due to the lack of\ndistinctive texture and morphological features, infrared small targets are\nhighly susceptible to blending into cluttered and noisy backgrounds. A\nfundamental challenge in designing deep models for this task lies in the\ninherent conflict between capturing high-resolution spatial details for minute\ntargets and extracting robust semantic context for larger targets, often\nleading to feature misalignment and suboptimal performance. Existing methods\noften rely on fixed gradient operators or simplistic attention mechanisms,\nwhich are inadequate for accurately extracting target edges under low contrast\nand high noise. In this paper, we propose a novel Dual-Path Edge Network that\nexplicitly addresses this challenge by decoupling edge enhancement and semantic\nmodeling into two complementary processing paths. The first path employs a\nBidirectional Interaction Module, which uses both Local Self-Attention and\nGlobal Self-Attention to capture multi-scale local and global feature\ndependencies. The global attention mechanism, based on a Transformer\narchitecture, integrates long-range semantic relationships and contextual\ninformation, ensuring robust scene understanding. The second path introduces\nthe Multi-Edge Refiner, which enhances fine-grained edge details using cascaded\nTaylor finite difference operators at multiple scales. This mathematical\napproach, along with an attention-driven gating mechanism, enables precise edge\nlocalization and feature enhancement for targets of varying sizes, while\neffectively suppressing noise. Our method provides a promising solution for\nprecise infrared small target detection and localization, combining structural\nsemantics and edge refinement in a unified framework.", "AI": {"tldr": "本文提出了一个新的双通道边缘网络，解决了红外小目标检测中的特征对齐问题，并结合了结构语义和边缘细化。", "motivation": "红外小目标检测在遥感应用中很重要，但目标边缘和背景在低对比度和高噪声环境下的提取困难极大地挑战了现有的深度学习模型。", "method": "该方法包括两个处理路径：其中一个使用双向交互模块来捕捉多尺度特征依赖，另一个使用多边缘细化器来提升不同大小目标的边缘细节。", "result": "这种方法为红外小目标的精确检测和定位提供了一个有前景的解决方案。", "conclusion": "双通道边缘网络通过分开处理边缘增强和语义建模，提升了红外小目标检测的性能。"}}
{"id": "2509.20577", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20577", "abs": "https://arxiv.org/abs/2509.20577", "authors": ["Sampurna Roy", "Ayan Sar", "Anurag Kaushish", "Kanav Gupta", "Tanupriya Choudhury", "Abhijit Kumar"], "title": "Dynamic Reasoning Chains through Depth-Specialized Mixture-of-Experts in Transformer Architectures", "comment": "Submitted in IEEE International Conference on Big Data 2025", "summary": "Contemporary transformer architectures apply identical processing depth to\nall inputs, creating inefficiencies and limiting reasoning quality. Simple\nfactual queries are subjected to the same multilayered computation as complex\nlogical problems, wasting resources while constraining deep inference. To\novercome this, we came up with a concept of Dynamic Reasoning Chains through\nDepth Specialised Mixture of Experts (DS-MoE), a modular framework that extends\nthe Mixture of Experts paradigm from width-based to depth specialised\ncomputation. DS-MoE introduces expert modules optimised for distinct reasoning\ndepths, shallow pattern recognition, compositional reasoning, logical\ninference, memory integration, and meta-cognitive supervision. A learned\nrouting network dynamically assembles custom reasoning chains, activating only\nthe necessary experts to match input complexity. The dataset on which we\ntrained and evaluated DS-MoE is on The Pile, an 800GB corpus covering diverse\ndomains such as scientific papers, legal texts, programming code, and web\ncontent, enabling systematic assessment across reasoning depths. Experimental\nresults demonstrate that DS-MoE achieves up to 16 per cent computational\nsavings and 35 per cent faster inference compared to uniform-depth\ntransformers, while delivering 2.8 per cent higher accuracy on complex\nmulti-step reasoning benchmarks. Furthermore, routing decisions yield\ninterpretable reasoning chains, enhancing transparency and scalability. These\nfindings establish DS-MoE as a significant advancement in adaptive neural\narchitectures, demonstrating that depth-specialised modular processing can\nsimultaneously improve efficiency, reasoning quality, and interpretability in\nlarge-scale language models.", "AI": {"tldr": "通过动态推理链的概念，DS-MoE 作为一个模块化框架，能够有效地处理输入内容，提升了推理速度和准确性，并增强了系统透明度。", "motivation": "现有的变压器架构对所有输入应用相同的处理深度，这导致了效率低下并限制了推理质量。简单的事实查询和复杂的逻辑问题同样经过多层计算，浪费了资源同时限制了深度推理能力。", "method": "通过深度专门化的专家混合模型（DS-MoE）来实现动态推理链的概念，这是一个扩展了宽度基础专家混合模型计算深度的模块化框架。DS-MoE 引入了针对不同推理深度优化的专家模块，包括浅层模式识别、组合推理、逻辑推理、记忆融合和元认知监督。一个学习到的路由网络能够动态地组装定制的推理链，仅激活与输入复杂度相匹配的必要专家。", "result": "实验结果显示，与均匀深度的变压器相比，DS-MoE 在推理速度上提高了35%，计算节省了16%，并且在复杂的多步骤推理基准上提高了2.8%的准确性。另外，路由决策带来了可解释的推理链，增强了透明度和可扩展性。", "conclusion": "这些发现确立了 DS-MoE 在自适应神经架构中的重要进展，表明深度专门化的模块化处理能够在提高效率、推理质量和大规模语言模型的解释性方面同时取得成果。"}}
{"id": "2509.20715", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20715", "abs": "https://arxiv.org/abs/2509.20715", "authors": ["Ruixu Zhang", "Yuran Wang", "Xinyi Hu", "Chaoyu Mai", "Wenxuan Liu", "Danni Xu", "Xian Zhong", "Zheng Wang"], "title": "Beyond the Individual: Introducing Group Intention Forecasting with SHOT Dataset", "comment": null, "summary": "Intention recognition has traditionally focused on individual intentions,\noverlooking the complexities of collective intentions in group settings. To\naddress this limitation, we introduce the concept of group intention, which\nrepresents shared goals emerging through the actions of multiple individuals,\nand Group Intention Forecasting (GIF), a novel task that forecasts when group\nintentions will occur by analyzing individual actions and interactions before\nthe collective goal becomes apparent. To investigate GIF in a specific\nscenario, we propose SHOT, the first large-scale dataset for GIF, consisting of\n1,979 basketball video clips captured from 5 camera views and annotated with 6\ntypes of individual attributes. SHOT is designed with 3 key characteristics:\nmulti-individual information, multi-view adaptability, and multi-level\nintention, making it well-suited for studying emerging group intentions.\nFurthermore, we introduce GIFT (Group Intention ForecasTer), a framework that\nextracts fine-grained individual features and models evolving group dynamics to\nforecast intention emergence. Experimental results confirm the effectiveness of\nSHOT and GIFT, establishing a strong foundation for future research in group\nintention forecasting. The dataset is available at\nhttps://xinyi-hu.github.io/SHOT_DATASET.", "AI": {"tldr": "本文创新性地提出了预测集体意图何时出现的任务GIF，并开发了用于此任务的首个大规模数据集SHOT及预测框架GIFT。", "motivation": "研究传统意图识别集中在个体意图上，忽视了群体设置中集体意图的复杂性。因此，本文希望通过开发GIF和相关的方法来填补这一研究空缺。", "method": "本文提出了Group Intention Forecasting (GIF)的任务，旨在通过分析个体行为和互动来预测集体意图何时出现。为了具体探讨GIF，作者创建了SHOT数据集，并引入了GIFT框架来提取细粒度的个体特征并模拟演变中的集体动力学。", "result": "实验结果验证了数据集SHOT和框架GIFT的有效性，为未来的集体意图预测研究奠定了坚实的基础。", "conclusion": "本文工作初步证明了集体意图预测的有效性，并提供了一个可供未来研究使用的大规模数据集和分析框架。"}}
