{"id": "2511.03765", "categories": ["cs.CV", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.03765", "abs": "https://arxiv.org/abs/2511.03765", "authors": ["Hyunseok Kwak", "Kyeongwon Lee", "Jae-Jin Lee", "Woojoo Lee"], "title": "LoRA-Edge: Tensor-Train-Assisted LoRA for Practical CNN Fine-Tuning on Edge Devices", "comment": "8 pages, 6 figures, 2 tables, DATE 2026 accepted paper", "summary": "On-device fine-tuning of CNNs is essential to withstand domain shift in edge\napplications such as Human Activity Recognition (HAR), yet full fine-tuning is\ninfeasible under strict memory, compute, and energy budgets. We present\nLoRA-Edge, a parameter-efficient fine-tuning (PEFT) method that builds on\nLow-Rank Adaptation (LoRA) with tensor-train assistance. LoRA-Edge (i) applies\nTensor-Train Singular Value Decomposition (TT-SVD) to pre-trained convolutional\nlayers, (ii) selectively updates only the output-side core with\nzero-initialization to keep the auxiliary path inactive at the start, and (iii)\nfuses the update back into dense kernels, leaving inference cost unchanged.\nThis design preserves convolutional structure and reduces the number of\ntrainable parameters by up to two orders of magnitude compared to full\nfine-tuning. Across diverse HAR datasets and CNN backbones, LoRA-Edge achieves\naccuracy within 4.7% of full fine-tuning while updating at most 1.49% of\nparameters, consistently outperforming prior parameter-efficient baselines\nunder similar budgets. On a Jetson Orin Nano, TT-SVD initialization and\nselective-core training yield 1.4-3.8x faster convergence to target F1.\nLoRA-Edge thus makes structure-aligned, parameter-efficient on-device CNN\nadaptation practical for edge platforms.", "AI": {"tldr": "本文提出了LoRA-Edge，一种有效的卷积神经网络参数微调方法，减少了训练参数数量，能在严格的资源限制下实现HAR等边缘应用中的性能优化。", "motivation": "为了解决设备内存、计算和能源预算有限的问题，实现边缘应用（例如人类活动识别HAR）中的卷积神经网络（CNN）的高效微调。", "method": "本文提出了LoRA-Edge方法，基于低秩适应（LoRA）并以张量列车辅助，包括(i) 对预训练卷积层应用张量列车奇异值分解（TT-SVD），(ii)选择性更新输出侧核心，通过零初始化来开启辅助路径，并(iii)将更新融合回密集核中，保持推理成本不变。", "result": "跨不同的HAR数据集和CNN骨干网络，LoRA-Edge的精度达到了全参数微调水平的95.3%，同时只更新至多1.49%的参数，并且比先前参数有效的基线方法表现更好，且在Jetson Orin Nano上收敛速度达到了1.4-3.8倍的提升。", "conclusion": "LoRA-Edge能够实现在相似预算下对卷积神经网络进行结构一致和参数有效的现场调整，相较于全参数微调，保持高精度的同时仅更新极少部分参数，并在Jetson Orin Nano平台上达到更快的目标F1收敛。"}}
{"id": "2511.03819", "categories": ["cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2511.03819", "abs": "https://arxiv.org/abs/2511.03819", "authors": ["Ozan Kanbertay", "Richard Vogg", "Elif Karakoc", "Peter M. Kappeler", "Claudia Fichtel", "Alexander S. Ecker"], "title": "SILVI: Simple Interface for Labeling Video Interactions", "comment": null, "summary": "Computer vision methods are increasingly used for the automated analysis of\nlarge volumes of video data collected through camera traps, drones, or direct\nobservations of animals in the wild. While recent advances have focused\nprimarily on detecting individual actions, much less work has addressed the\ndetection and annotation of interactions -- a crucial aspect for understanding\nsocial and individualized animal behavior. Existing open-source annotation\ntools support either behavioral labeling without localization of individuals,\nor localization without the capacity to capture interactions. To bridge this\ngap, we present SILVI, an open-source labeling software that integrates both\nfunctionalities. SILVI enables researchers to annotate behaviors and\ninteractions directly within video data, generating structured outputs suitable\nfor training and validating computer vision models. By linking behavioral\necology with computer vision, SILVI facilitates the development of automated\napproaches for fine-grained behavioral analyses. Although developed primarily\nin the context of animal behavior, SILVI could be useful more broadly to\nannotate human interactions in other videos that require extracting dynamic\nscene graphs. The software, along with documentation and download instructions,\nis available at: https://gitlab.gwdg.de/kanbertay/interaction-labelling-app.", "AI": {"tldr": "提出了 SILVI，一种结合行为与互动标注和定位功能的开源标注软件，旨在帮助自动分析动物行为并推广到更广的应用场景中。", "motivation": "现有的开源标注工具要么支持无个体定位的行为标注，要么支持无法捕捉互动的定位，而本文旨在填补这一空白。", "method": "SILVI, 一种开源标注软件，结合行为标注和定位功能，能够在视频数据中直接标注行为和互动，生成适合训练和验证计算机视觉模型的结构化输出。", "result": "SILVI 软件的开发促进了将行为生态学与计算机视觉相结合，推动了自动精细化行为分析方法的发展，也可用于标注其他需要提取动态场景图的视频中的人类互动。", "conclusion": "SILVI 软件及其文档和下载说明可在指定网址获取。"}}
{"id": "2511.03855", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03855", "abs": "https://arxiv.org/abs/2511.03855", "authors": ["Duong Mai", "Lawrence Hall"], "title": "Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets", "comment": "Abstract accepted for oral presentation at SPIE Medical Imaging 2026:\n  Computer-Aided Diagnosis", "summary": "Deep learned (DL) models for image recognition have been shown to fail to\ngeneralize to data from different devices, populations, etc. COVID-19 detection\nfrom Chest X-rays (CXRs), in particular, has been shown to fail to generalize\nto out-of-distribution (OOD) data from new clinical sources not covered in the\ntraining set. This occurs because models learn to exploit shortcuts -\nsource-specific artifacts that do not translate to new distributions - rather\nthan reasonable biomarkers to maximize performance on in-distribution (ID)\ndata. Rendering the models more robust to distribution shifts, our study\ninvestigates the use of fundamental noise injection techniques (Gaussian,\nSpeckle, Poisson, and Salt and Pepper) during training. Our empirical results\ndemonstrate that this technique can significantly reduce the performance gap\nbetween ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results\naveraged over ten random seeds across key metrics such as AUC, F1, accuracy,\nrecall and specificity. Our source code is publicly available at\nhttps://github.com/Duongmai127/Noisy-ood", "AI": {"tldr": "通过在训练过程中引入基本的噪声注入技术（如高斯噪声、斑点噪声、泊松噪声和椒盐噪声），研究提高了从胸部X光片（CXR）检测COVID-19的深度学习模型的鲁棒性，大大缩小了训练分布内（ID）和分布外（OOD）数据的性能差距。", "motivation": "研究旨在解决深度学习模型在不同设备和人群上无法泛化的难题，特别是在对新的临床数据来源泛化能力不足的问题。", "method": "Structure", "result": "{\n  \"tldr\": \"通过在训练过程中引入基本的噪声注入技术（如高斯噪声、斑点噪声、泊松噪声和椒盐噪声），研究提高了从胸部X光片（CXR）检测COVID-19的深度学习模型的鲁棒性，大大缩小了训练分布内（ID）和分布外（OOD）数据的性能差距。\",\n  \"motivation\": \"研究旨在解决深度学习模型在不同设备和人群上无法泛化的难题，特别是在对新的临床数据来源泛化能力不足的问题。\",\n  \"method\": \"研究采用核心噪声注入技术，在模型训练过程中引入不同的噪声类型，以实现模型对分布变化的鲁棒性。\",\n  \"result\": \"实验结果表明，这种技术能够显著降低ID和OOD数据集上的性能差异，提高了模型对新的临床数据源的适应性和泛化能力。\",\n  \"conclusion\": \"通过在训练过程中加入噪声，能够提高深度学习模型在胸部X光片（CXR）检测COVID-19时的泛化能力，特别是对OOD数据的表现提升明显。这表明噪声注入是提高模型泛化性能的一种有效手段。研究的源代码可以在https://github.com/Duongmai127/Noisy-ood获取。 \"\n}", "conclusion": "通过在训练过程中加入噪声，能够提高深度学习模型在胸部X光片（CXR）检测COVID-19时的泛化能力，特别是对OOD数据的表现提升明显。这表明噪声注入是提高模型泛化性能的一种有效手段。研究的源代码可以在https://github.com/Duongmai127/Noisy-ood获取。"}}
{"id": "2511.03882", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.03882", "abs": "https://arxiv.org/abs/2511.03882", "authors": ["Florence Klitzner", "Blanca Inigo", "Benjamin D. Killeen", "Lalithkumar Seenivasan", "Michelle Song", "Axel Krieger", "Mathias Unberath"], "title": "Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures", "comment": null, "summary": "Imitation learning-based robot control policies are enjoying renewed interest\nin video-based robotics. However, it remains unclear whether this approach\napplies to X-ray-guided procedures, such as spine instrumentation. This is\nbecause interpretation of multi-view X-rays is complex. We examine\nopportunities and challenges for imitation policy learning in bi-plane-guided\ncannula insertion. We develop an in silico sandbox for scalable, automated\nsimulation of X-ray-guided spine procedures with a high degree of realism. We\ncurate a dataset of correct trajectories and corresponding bi-planar X-ray\nsequences that emulate the stepwise alignment of providers. We then train\nimitation learning policies for planning and open-loop control that iteratively\nalign a cannula solely based on visual information. This precisely controlled\nsetup offers insights into limitations and capabilities of this method. Our\npolicy succeeded on the first attempt in 68.5% of cases, maintaining safe\nintra-pedicular trajectories across diverse vertebral levels. The policy\ngeneralized to complex anatomy, including fractures, and remained robust to\nvaried initializations. Rollouts on real bi-planar X-rays further suggest that\nthe model can produce plausible trajectories, despite training exclusively in\nsimulation. While these preliminary results are promising, we also identify\nlimitations, especially in entry point precision. Full closed-look control will\nrequire additional considerations around how to provide sufficiently frequent\nfeedback. With more robust priors and domain knowledge, such models may provide\na foundation for future efforts toward lightweight and CT-free robotic\nintra-operative spinal navigation.", "AI": {"tldr": "本文研究了通过模仿学习开发用于X光引导手术的机器人控制策略，成功率为68.5%，但也发现了局限性，特别是入口点的精度。", "motivation": "尽管基于模仿学习的机器人控制策略在基于视频的机器人领域受到越来越多的关注，但这种策略是否适用于X光引导的程序（如脊柱植入）仍然不清楚。因此，我们研究了模仿策略学习在双平面引导下的针头插入的机会和挑战。", "method": "我们开发了一个仿真的沙盒，用于对X光引导的脊柱手术进行可扩展和自动化的现实模拟。我们收集了一个由正确的轨迹及其对应的双平面X光序列组成的数据库，这些数据仿照了操作者的逐步对齐过程。然后，我们训练了基于模仿学习的策略，该策略基于视觉信息迭代地对齐针头。", "result": "我们的策略在68.5%的案例中首次尝试就成功了，并在整个椎骨水平上保持了安全的皮内轨迹。该策略能够推广到复杂的解剖结构，包括骨折，并对不同的初始化保持了鲁棒性。在真实双平面X射线上的回滚进一步表明，尽管仅在仿真中训练，但该模型能够产生合理的轨迹。", "conclusion": "初步结果是很有前景的，但也指出了局限性，特别是在入口点精度方面。完全闭环控制将需要进一步考虑如何提供足够频繁的反馈。随着更强大的先验知识和领域知识，这样的模型为未来开发轻量级和无CT的机器人手术导航提供了一个基础。"}}
{"id": "2511.03738", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03738", "abs": "https://arxiv.org/abs/2511.03738", "authors": ["Pranav Bhandari", "Nicolas Fay", "Sanjeevan Selvaganapathy", "Amitava Datta", "Usman Naseem", "Mehwish Nasim"], "title": "Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs", "comment": null, "summary": "Large Language Models exhibit implicit personalities in their generation, but\nreliably controlling or aligning these traits to meet specific needs remains an\nopen challenge. The need for effective mechanisms for behavioural manipulation\nof the model during generation is a critical gap in the literature that needs\nto be fulfilled. Personality-aware LLMs hold a promising direction towards this\nobjective. However, the relationship between these psychological constructs and\ntheir representations within LLMs remains underexplored and requires further\ninvestigation. Moreover, it is intriguing to understand and study the use of\nthese representations to steer the models' behaviour. We propose a novel\npipeline that extracts hidden state activations from transformer layers using\nthe Big Five Personality Traits (Openness, Conscientiousness, Extraversion,\nAgreeableness and Neuroticism), which is a comprehensive and empirically\nvalidated framework to model human personality applies low-rank subspace\ndiscovery methods, and identifies trait-specific optimal layers across\ndifferent model architectures for robust injection. The resulting\npersonality-aligned directions are then operationalised through a flexible\nsteering framework with dynamic layer selection, enabling precise control of\ntrait expression in LLM outputs. Our findings reveal that personality traits\noccupy a low-rank shared subspace, and that these latent structures can be\ntransformed into actionable mechanisms for effective steering through careful\nperturbations without impacting the fluency, variance and general capabilities,\nhelping to bridge the gap between psychological theory and practical model\nalignment.", "AI": {"tldr": "该研究通过提取变压器层的隐藏状态激活并应用低秩子空间发现方法，识别出不同模型架构中人格特质相关的最佳层。这种方法能够为人格特质的稳健注入提供基础，使参与者能够精准控制大型语言模型（LLM）输出中的人格特质表达。", "motivation": "大型语言模型在其生成中表现出隐性人格特质，但可靠地控制或调整这些特质以满足特定需求仍然是一个开放挑战。需要有效机制来在生成过程中控制模型的行为。", "method": "提出一种新流程，利用Big Five人格特质框架，从变压器层提取隐藏状态激活，应用低秩子空间发现方法，识别不同模型架构中特质特定的最佳层，以实现稳健注入。通过一个灵活的引导框架与动态层选择，实现对LLM输出中特质表达的精确控制。", "result": "研究发现，人格特质占据了一个低秩共享子空间，这些潜在结构可以通过仔细调整转化为有效的引导机制，而不影响流畅度、变异性及总体能力。", "conclusion": "该研究有助于填补心理理论和实用模型对齐之间的空白，为人格特质在语言模型中的有效利用提供了新思路。"}}
{"id": "2511.03888", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03888", "abs": "https://arxiv.org/abs/2511.03888", "authors": ["Abdulmumin Sa'ad", "Sulaimon Oyeniyi Adebayo", "Abdul Jabbar Siddiqui"], "title": "Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model", "comment": "8 pages", "summary": "The global waste crisis is escalating, with solid waste generation expected\nto increase by 70% by 2050. Traditional waste collection methods, particularly\nin remote or harsh environments like deserts, are labor-intensive, inefficient,\nand often hazardous. Recent advances in computer vision and deep learning have\nopened the door to automated waste detection systems, yet most research focuses\non urban environments and recyclable materials, overlooking organic and\nhazardous waste and underexplored terrains such as deserts. In this work, we\npropose an enhanced real-time object detection framework based on a pruned,\nlightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT)\nand specialized data augmentation strategies. Using the DroneTrashNet dataset,\nwe demonstrate significant improvements in precision, recall, and mean average\nprecision (mAP), while achieving low latency and compact model size suitable\nfor deployment on resource-constrained aerial drones. Benchmarking our model\nagainst state-of-the-art lightweight YOLO variants further highlights its\noptimal balance of accuracy and efficiency. Our results validate the\neffectiveness of combining data-centric and model-centric enhancements for\nrobust, real-time waste detection in desert environments.", "AI": {"tldr": "本文提出了一种改进的实时物体检测框架，专门针对沙漠等极端环境下的废物检测。通过结合自我对抗训练和专门的数据增强策略，我们的模型在DroneTrashNet数据集上显示出优异的表现。", "motivation": "面对全球废物危机加剧，特别是那些传统废物收集方法在偏远或恶劣环境中效率低下、耗时甚至危险的问题，我们旨在开发适用于这些环境的有效废物检测系统。", "method": "我们提出了一个基于YOLOv12的精简版对象检测框架，并结合了自我对抗训练（SAT）和专门的数据增强策略。", "result": "使用DroneTrashNet数据集，我们的模型在准确性（precision）、召回率（recall）和平均精度均值（mAP）方面有显著提升，并且实现了低延迟和较小的模型尺寸，适用于资源受限的空中无人机部署。", "conclusion": "我们的实验结果证实了结合数据驱动和模型驱动增强对于在沙漠环境中实现准确和有效的废物检测是有效的。"}}
{"id": "2511.03739", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03739", "abs": "https://arxiv.org/abs/2511.03739", "authors": ["Eugenius Mario Situmorang", "Adila Alfa Krisnadhi", "Ari Wibisono"], "title": "TextualVerifier: Verify TextGrad Step-by-Step", "comment": null, "summary": "TextGrad is a novel approach to text-based automatic differentiation that\nenables composite AI systems to perform optimization without explicit numerical\nequations. However, it currently lacks self-verification mechanisms that ensure\nreasoning validity in text-based decision making. This research introduces\nTextualVerifier, a verification framework that leverages chain-of-thought\nreasoning and majority voting with large language models to address this\nverification gap. TextualVerifier implements a four-stage workflow:\nchain-of-thought decomposition, variant generation, majority voting, and\nconsensus aggregation. It integrates non-invasively with TextGrad at both the\nloss function and optimization result verification stages. Experimental\nevaluation using the Gemini 1.5 Pro model is conducted in two phases: (1)\nstandalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad\non GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically\nsignificant improvements (p < 0.001). In phase one, TextualVerifier improves\nthe validity of reasoning steps by 29 percent. In phase two, integration into\nTextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4\npercent with a moderate overhead of 5.9 LLM calls on average. Further\nevaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92\npercentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively.\nTextualVerifier thus presents the first self-verification framework for\nTextGrad through LLM-based techniques without requiring numerical gradients,\nenabling more reliable reasoning and opening new directions for verification in\ntext-based optimization.", "AI": {"tldr": "Research introduces TextualVerifier to fill the verification gap for TextGrad, yielding significant improvements in reasoning validity without numerical gradients.", "motivation": "To address the lack of self-verification mechanisms in TextGrad that ensure the reasoning validity in text-based decision making.", "method": "TextualVerifier, a verification framework that leverages chain-of-thought reasoning and majority voting with large language models, is introduced. It includes four stages: chain-of-thought decomposition, variant generation, majority voting, and consensus aggregation. It integrates with TextGrad at the loss function and optimization result verification stages.", "result": "Experimental evaluation shows TextualVerifier achieves up to a 29 percent improvement in reasoning validity and a 2.2 percentage point gain in TextGrad's performance with a moderate overhead.", "conclusion": "TextualVerifier is the first self-verification framework for TextGrad, allowing more reliable reasoning and opening new possibilities for verification in text-based optimization."}}
{"id": "2511.03891", "categories": ["cs.CV", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.03891", "abs": "https://arxiv.org/abs/2511.03891", "authors": ["Hlali Azzeddine", "Majid Ben Yakhlef", "Soulaiman El Hazzat"], "title": "Improving Diagnostic Performance on Small and Imbalanced Datasets Using Class-Based Input Image Composition", "comment": null, "summary": "Small, imbalanced datasets and poor input image quality can lead to high\nfalse predictions rates with deep learning models. This paper introduces\nClass-Based Image Composition, an approach that allows us to reformulate\ntraining inputs through a fusion of multiple images of the same class into\ncombined visual composites, named Composite Input Images (CoImg). That enhances\nthe intra-class variance and improves the valuable information density per\ntraining sample and increases the ability of the model to distinguish between\nsubtle disease patterns. Our method was evaluated on the Optical Coherence\nTomography Dataset for Image-Based Deep Learning Methods (OCTDL) (Kulyabin et\nal., 2024), which contains 2,064 high-resolution optical coherence tomography\n(OCT) scans of the human retina, representing seven distinct diseases with a\nsignificant class imbalance. We constructed a perfectly class-balanced version\nof this dataset, named Co-OCTDL, where each scan is resented as a 3x1 layout\ncomposite image. To assess the effectiveness of this new representation, we\nconducted a comparative analysis between the original dataset and its variant\nusing a VGG16 model. A fair comparison was ensured by utilizing the identical\nmodel architecture and hyperparameters for all experiments. The proposed\napproach markedly improved diagnostic results.The enhanced Dataset achieved\nnear-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996), compared\nto a baseline model trained on raw dataset. The false prediction rate was also\nsignificantly lower, this demonstrates that the method can producehigh-quality\npredictions even for weak datasets affected by class imbalance or small sample\nsize.", "AI": {"tldr": "The paper proposes Class-Based Image Composition to enhance training inputs and improve model performance on imbalanced, small-sized datasets, significantly increasing diagnostic accuracy on the OCTDL dataset.", "motivation": "To address the issue of high false prediction rates due to small, imbalanced datasets and poor input image quality in deep learning models.", "method": "Class-Based Image Composition, which fuses multiple images of the same class into Composite Input Images (CoImg) to increase intra-class variance and enhance valuable information density per training sample.", "result": "The method generated near-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996) on the Co-OCTDL dataset, compared to a baseline model.", "conclusion": "Class-Based Image Composition is effective in improving diagnostic results even with challenging, imbalanced datasets, showing significant reduction in false prediction rates and better handling of weak datasets."}}
{"id": "2511.03772", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03772", "abs": "https://arxiv.org/abs/2511.03772", "authors": ["Stergios Chatzikyriakidis", "Dimitris Papadakis", "Sevasti-Ioanna Papaioannou", "Erofili Psaltaki"], "title": "GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning Evaluation", "comment": null, "summary": "We present an extended Greek Dialectal Dataset (GRDD+) 1that complements the\nexisting GRDD dataset with more data from Cretan, Cypriot, Pontic and Northern\nGreek, while we add six new varieties: Greco-Corsican, Griko (Southern Italian\nGreek), Maniot, Heptanesian, Tsakonian, and Katharevusa Greek. The result is a\ndataset with total size 6,374,939 words and 10 varieties. This is the first\ndataset with such variation and size to date. We conduct a number of\nfine-tuning experiments to see the effect of good quality dialectal data on a\nnumber of LLMs. We fine-tune three model architectures (Llama-3-8B,\nLlama-3.1-8B, Krikri-8B) and compare the results to frontier models\n(Claude-3.7-Sonnet, Gemini-2.5, ChatGPT-5).", "AI": {"tldr": "创建了一个含 10 种希腊语变体的扩展方言数据集（GRDD+），并用于微调实验，测试其对多个语言模型的效应。", "motivation": "补充现有的希腊方言数据集，创建一个更大的数据集，包含多种变体，以便研究高质量方言数据对多种大型语言模型的影响。", "method": "构建了一个名为扩展希腊方言数据集（GRDD+）的数据集，增加了克里特岛、塞浦路斯、Pontic 和北希腊方言的数据，并新增了 Greco-Corsican、Griko（南意大利希腊语）、Maniot、Heptanesian、Tsakonian 和 Katharevusa 希腊语六种新变体。", "result": "结果是一个包含 6,374,939 个词汇和 10 种变体的数据集，这是首个具有如此变异程度和规模的数据集。", "conclusion": "在三个模型架构（Llama-3-8B、Llama-3.1-8B、Krikri-8B）上进行了微调实验，并将结果与前沿模型（Claude-3.7-Sonnet、Gemini-2.5、ChatGPT-5）进行比较。"}}
{"id": "2511.03912", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03912", "abs": "https://arxiv.org/abs/2511.03912", "authors": ["Nand Kumar Yadav", "Rodrigue Rizk", "William CW Chen", "KC Santosh"], "title": "I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging", "comment": null, "summary": "Unknown anomaly detection in medical imaging remains a fundamental challenge\ndue to the scarcity of labeled anomalies and the high cost of expert\nsupervision. We introduce an unsupervised, oracle-free framework that\nincrementally expands a trusted set of normal samples without any anomaly\nlabels. Starting from a small, verified seed of normal images, our method\nalternates between lightweight adapter updates and uncertainty-gated sample\nadmission. A frozen pretrained vision backbone is augmented with tiny\nconvolutional adapters, ensuring rapid domain adaptation with negligible\ncomputational overhead. Extracted embeddings are stored in a compact coreset\nenabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during\nincremental expansion is enforced by dual probabilistic gates, a sample is\nadmitted into the normal memory only if its distance to the existing coreset\nlies within a calibrated z-score threshold, and its SWAG-based epistemic\nuncertainty remains below a seed-calibrated bound. This mechanism prevents\ndrift and false inclusions without relying on generative reconstruction or\nreplay buffers. Empirically, our system steadily refines the notion of\nnormality as unlabeled data arrive, producing substantial gains over baselines.\nOn COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on\nPneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5,\nROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These\nresults highlight the effectiveness and efficiency of the proposed framework\nfor real-world, label-scarce medical imaging applications.", "AI": {"tldr": "本文提出一种无需异常标签的无监督异常检测框架，通过轻量级适配器更新和基于不确定性的样本准入，提升了多种医疗影像异常检测的任务性能。", "motivation": "由于标注异常样本的稀缺以及专家监督成本高昂，医学影像中未知异常检测依旧是基础性挑战。", "method": "提出了一种无监督的、无需专家干预的异常检测框架，通过增量扩展可信的正常样本集合。从一小部分已验证的正常影像开始，方法交替进行轻量级适配器更新和基于不确定性的样本准入。将预训练视觉骨干网络通过小卷积适配器增强，迅速进行领域适应且计算开销极低。提取的嵌入存储在一个紧凑的coreset中，实现高效的k-NN异常评分。通过双概率门机制确保增量扩展的安全性，样本在满足与现有coreset距离的校准z-score阈值及基于SWAG的本体不确定性下限后才能进入正常内存。这种机制在不依赖生成式重建或重放缓冲器的情况下防止漂移和错误包含。", "result": "实验证明，随着无标签数据的流入，系统逐步精化正常性的认知，性能显著超越基线模型。在COVID-CXR上，ROC-AUC从0.9489提升至0.9982（F1从0.8048提升至0.9746）；在肺炎CXR上，ROC-AUC从0.6834增至0.8968；在脑MRI ND-5上，ROC-AUC从0.6041提升至0.7269，PR-AUC从0.7539提升至0.8211。", "conclusion": "这些结果显示了所提出框架在真实世界标签稀缺的医学影像应用中的有效性和高效性。"}}
{"id": "2511.03823", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03823", "abs": "https://arxiv.org/abs/2511.03823", "authors": ["Jan Kocoń", "Maciej Piasecki", "Arkadiusz Janz", "Teddy Ferdinan", "Łukasz Radliński", "Bartłomiej Koptyra", "Marcin Oleksy", "Stanisław Woźniak", "Paweł Walkowiak", "Konrad Wojtasik", "Julia Moska", "Tomasz Naskręt", "Bartosz Walkowiak", "Mateusz Gniewkowski", "Kamil Szyc", "Dawid Motyka", "Dawid Banach", "Jonatan Dalasiński", "Ewa Rudnicka", "Bartłomiej Alberski", "Tomasz Walkowiak", "Aleksander Szczęsny", "Maciej Markiewicz", "Tomasz Bernaś", "Hubert Mazur", "Kamil Żyta", "Mateusz Tykierko", "Grzegorz Chodak", "Tomasz Kajdanowicz", "Przemysław Kazienko", "Agnieszka Karlińska", "Karolina Seweryn", "Anna Kołos", "Maciej Chrabąszcz", "Katarzyna Lorenc", "Aleksandra Krasnodębska", "Artur Wilczek", "Katarzyna Dziewulska", "Paula Betscher", "Zofia Cieślińska", "Katarzyna Kowol", "Daria Mikoś", "Maciej Trzciński", "Dawid Krutul", "Marek Kozłowski", "Sławomir Dadas", "Rafał Poświata", "Michał Perełkiewicz", "Małgorzata Grębowiec", "Maciej Kazuła", "Marcin Białas", "Roman Roszko", "Danuta Roszko", "Jurgita Vaičenonienė", "Andrius Utka", "Paweł Levchuk", "Paweł Kowalski", "Irena Prawdzic-Jankowska", "Maciej Ogrodniczuk", "Monika Borys", "Anna Bulińska", "Wiktoria Gumienna", "Witold Kieraś", "Dorota Komosińska", "Katarzyna Krasnowska-Kieraś", "Łukasz Kobyliński", "Martyna Lewandowska", "Marek Łaziński", "Mikołaj Łątkowski", "Dawid Mastalerz", "Beata Milewicz", "Agnieszka Anna Mykowiecka", "Angelika Peljak-Łapińska", "Sandra Penno", "Zuzanna Przybysz", "Michał Rudolf", "Piotr Rybak", "Karolina Saputa", "Aleksandra Tomaszewska", "Aleksander Wawer", "Marcin Woliński", "Joanna Wołoszyn", "Alina Wróblewska", "Bartosz Żuk", "Filip Żarnecki", "Konrad Kaczyński", "Anna Cichosz", "Zuzanna Deckert", "Monika Garnys", "Izabela Grabarczyk", "Wojciech Janowski", "Sylwia Karasińska", "Aleksandra Kujawiak", "Piotr Misztela", "Maria Szymańska", "Karolina Walkusz", "Igor Siek", "Jakub Kwiatkowski", "Piotr Pęzik"], "title": "PLLuM: A Family of Polish Large Language Models", "comment": "83 pages, 19 figures", "summary": "Large Language Models (LLMs) play a central role in modern artificial\nintelligence, yet their development has been primarily focused on English,\nresulting in limited support for other languages. We present PLLuM (Polish\nLarge Language Model), the largest open-source family of foundation models\ntailored specifically for the Polish language. Developed by a consortium of\nmajor Polish research institutions, PLLuM addresses the need for high-quality,\ntransparent, and culturally relevant language models beyond the English-centric\ncommercial landscape. We describe the development process, including the\nconstruction of a new 140-billion-token Polish text corpus for pre-training, a\n77k custom instructions dataset, and a 100k preference optimization dataset. A\nkey component is a Responsible AI framework that incorporates strict data\ngovernance and a hybrid module for output correction and safety filtering. We\ndetail the models' architecture, training procedures, and alignment techniques\nfor both base and instruction-tuned variants, and demonstrate their utility in\na downstream task within public administration. By releasing these models\npublicly, PLLuM aims to foster open research and strengthen sovereign AI\ntechnologies in Poland.", "AI": {"tldr": "PLLuM 是一个专门为波兰语设计的大规模公开源代码语言模型家族，由波兰的主要研究机构开发，旨在增强波兰语本土人工智能技术的发展。", "motivation": "PLLuM 的开发动机是针对目前大规模语言模型主要集中在英语上，而忽视了其他语言。该项目旨在弥补这一不足，尤其是在英语中心化的商业环境中开发高质量、透明且文化相关性高的语言模型。", "method": "PLLuM 是一个针对波兰语的大规模语言模型家族，该项目由多个波兰研究机构联合开发，旨在解决除了以英语为主之外的多语言模型支持。PLLuM 使用了新的 1400 亿个波兰语标记的数据集进行预训练，77K 自定义指令数据集和 100K 偏好优化数据集。其核心组件是一个负责任的人工智能框架，包括严格的数据治理和一个用于输出校正和安全筛选的混合模块。", "result": "模型的架构、训练程序及其对基础和指教调整变体的对齐技术已被详细叙述，并在公共管理的下游任务中展示了其效用。", "conclusion": "通过公开这些模型，PLLuM 目的是推动开放研究并加强波兰的主权人工智能技术。"}}
{"id": "2511.03943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03943", "abs": "https://arxiv.org/abs/2511.03943", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "Adaptive Temporal Refinement: Continuous Depth Allocation and Distance Regression for Efficient Action Localization", "comment": null, "summary": "Temporal action localization requires precise boundary detection; however,\ncurrent methods apply uniform computation despite significant variations in\ndifficulty across boundaries. We present two complementary contributions.\nFirst, Boundary Distance Regression (BDR) provides information-theoretically\noptimal localization through signed-distance regression rather than\nclassification, achieving 43\\% sharper boundary peaks. BDR retrofits to\nexisting methods with approximately 50 lines of code, yielding consistent 1.8\nto 3.1\\% mAP@0.7 improvements across diverse architectures. Second, Adaptive\nTemporal Refinement (ATR) allocates computation via continuous depth selection\n$\\tau \\in [0,1]$, enabling end-to-end differentiable optimization without\nreinforcement learning. On THUMOS14, ATR achieves 56.5\\% mAP@0.7 at 162G FLOPs,\ncompared to 53.6\\% at 198G for uniform processing, providing a 2.9\\%\nimprovement with 18\\% less compute. Gains scale with boundary heterogeneity,\nshowing 4.2\\% improvement on short actions. Training cost is mitigated via\nknowledge distillation, with lightweight students retaining 99\\% performance at\nbaseline cost. Results are validated across four benchmarks with rigorous\nstatistical testing.", "AI": {"tldr": "本文提出边界距离回归和自适应时间细化两种方法，用于提升时间动作定位的边界检测能力，适用于不同复杂度边界检测。", "motivation": "当前方法应用的均匀计算并不适应边界检测难度的变化，因此作者提出改进以提高定位精度。", "method": "论文提出了两种方法：一是边界距离回归（BDR），通过有符号距离回归改进边界峰值；二是自适应时间细化（ATR），利用连续深度选择来分配计算。", "result": "该论文提出两种互补的方法来改进时间动作定位的边界检测。第一，边界距离回归（BDR）提供了一种信息论最优的定位方式，通过有符号距离回归而非分类，边界峰值提高43%。BDR可以轻松集成到现有方法中，用大约50行代码，跨不同架构提高1.8%到3.1% mAP@0.7。第二，自适应时间细化（ATR）通过连续深度选择 $\tau \\in [0,1]$ 来分配计算，使优化变得可微分且无需强化学习。在THUMOS14数据集上，ATR实现了56.5% mAP@0.7，计算强度为162G FLOPs，相较于之前53.6% mAP@0.7，虽计算量减少18%，性能反而提高了2.9%。这些改进在边界异质性较大的情况下降尤为明显。实验结果在四个基准上进行了严格的统计检验。", "conclusion": "本文的两种方法能够显著提高时间动作定位精确度，并能在不同程度上减少计算量。"}}
