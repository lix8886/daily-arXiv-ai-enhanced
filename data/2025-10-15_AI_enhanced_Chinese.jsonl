{"id": "2510.11817", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11817", "abs": "https://arxiv.org/abs/2510.11817", "authors": ["Yumi Iwashita", "Haakon Moe", "Yang Cheng", "Adnan Ansar", "Georgios Georgakis", "Adrian Stoica", "Kazuto Nakashima", "Ryo Kurazume", "Jim Torresen"], "title": "Enhancing the Quality of 3D Lunar Maps Using JAXA's Kaguya Imagery", "comment": "Presented at IEEE SMC 2025", "summary": "As global efforts to explore the Moon intensify, the need for high-quality 3D\nlunar maps becomes increasingly critical-particularly for long-distance\nmissions such as NASA's Endurance mission concept, in which a rover aims to\ntraverse 2,000 km across the South Pole-Aitken basin. Kaguya TC (Terrain\nCamera) images, though globally available at 10 m/pixel, suffer from altitude\ninaccuracies caused by stereo matching errors and JPEG-based compression\nartifacts. This paper presents a method to improve the quality of 3D maps\ngenerated from Kaguya TC images, focusing on mitigating the effects of\ncompression-induced noise in disparity maps. We analyze the compression\nbehavior of Kaguya TC imagery, and identify systematic disparity noise\npatterns, especially in darker regions. In this paper, we propose an approach\nto enhance 3D map quality by reducing residual noise in disparity images\nderived from compressed images. Our experimental results show that the proposed\napproach effectively reduces elevation noise, enhancing the safety and\nreliability of terrain data for future lunar missions.", "AI": {"tldr": "本文提出了一种识别和减少由Kaguya TC图像压缩导致的系统性差异噪声的方法，以提高3D月球地图的质量，实验结果证明了这种方法的有效性。", "motivation": "随着全球对月球探索的力度加大，特别是对于像NASA的Endurance任务这样的长距离任务而言，高质量的3D月球地图的需求变得越来越重要。现有的Kaguya TC图像虽然全球覆盖，但其高度精度受到立体匹配错误和JPEG压缩伪影的影响。因此，提升这些3D地图的质量对于未来的月球任务至关重要。", "method": "本文提出了一种方法来提高从Kaguya TC图像生成的3D地图的质量，重点是减少由压缩引起的不同图像中的残余噪声，特别是在较暗的区域。通过分析Kaguya TC图像的压缩行为，识别出系统的差异噪声模式，并设法减少这些噪声，以增强地形数据的安全性和可靠性。", "result": "实验结果显示，所提出的方法能够有效减少高程噪声，提高地形数据的安全性和可靠性。", "conclusion": "该方法通过减少由压缩引起的残余噪声，有效提高了从Kaguya TC图像生成的3D地图的质量，从而增强了未来月球任务中地形数据的安全性和可靠性。"}}
{"id": "2510.11835", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.11835", "abs": "https://arxiv.org/abs/2510.11835", "authors": ["Yiming Liu", "Yuhui Zhang", "Dhruba Ghosh", "Ludwig Schmidt", "Serena Yeung-Levy"], "title": "Data or Language Supervision: What Makes CLIP Better than DINO?", "comment": "EMNLP 2025 Findings", "summary": "CLIP outperforms self-supervised models like DINO as vision encoders for\nvision-language models (VLMs), but it remains unclear whether this advantage\nstems from CLIP's language supervision or its much larger training data. To\ndisentangle these factors, we pre-train CLIP and DINO under controlled settings\n-- using the same architecture, dataset, and training configuration --\nachieving similar ImageNet accuracy. Embedding analysis shows that CLIP\ncaptures high-level semantics (e.g., object categories, text), while DINO is\nmore responsive to low-level features like colors and styles. When integrated\ninto VLMs and evaluated on 20 VQA benchmarks, CLIP excels at text-intensive\ntasks, while DINO slightly outperforms on vision-centric ones. Variants of\nlanguage supervision (e.g., sigmoid loss, pre-trained language encoders) yield\nlimited gains. Our findings provide scientific insights into vision encoder\ndesign and its impact on VLM performance.", "AI": {"tldr": "通过控制变量法比较CLIP和DINO，发现CLIP在VLM中的优势主要依赖于其高阶语义捕捉能力，而DINO更擅长低级特征检测。", "motivation": "研究CLIP相对于DINO在视觉-语言模型（VLM）中的优势是否源于语言监督还是较大的训练数据集。", "method": "通过在相同架构、数据集和训练配置下预训练CLIP和DINO，以分析语言监督和更大训练数据对CLIP性能优势的影响。", "result": "CLIP在文本密集型任务上表现出色，而DINO在视觉为中心的任务上略有优势。语言监督的变体改进有限。", "conclusion": "这一研究为视觉编码器设计及其对VLM性能的影响提供了科学的见解。"}}
{"id": "2510.11883", "categories": ["cs.CV", "cs.AI", "1.2"], "pdf": "https://arxiv.org/pdf/2510.11883", "abs": "https://arxiv.org/abs/2510.11883", "authors": ["Sicheng Zhou", "Lei Wu", "Cao Xiao", "Parminder Bhatia", "Taha Kass-Hout"], "title": "MammoDINO: Anatomically Aware Self-Supervision for Mammographic Images", "comment": "5 pages", "summary": "Self-supervised learning (SSL) has transformed vision encoder training in\ngeneral domains but remains underutilized in medical imaging due to limited\ndata and domain specific biases. We present MammoDINO, a novel SSL framework\nfor mammography, pretrained on 1.4 million mammographic images. To capture\nclinically meaningful features, we introduce a breast tissue aware data\naugmentation sampler for both image-level and patch-level supervision and a\ncross-slice contrastive learning objective that leverages 3D digital breast\ntomosynthesis (DBT) structure into 2D pretraining. MammoDINO achieves\nstate-of-the-art performance on multiple breast cancer screening tasks and\ngeneralizes well across five benchmark datasets. It offers a scalable,\nannotation-free foundation for multipurpose computer-aided diagnosis (CAD)\ntools for mammogram, helping reduce radiologists' workload and improve\ndiagnostic efficiency in breast cancer screening.", "AI": {"tldr": "MammoDINO, a novel SSL framework for mammography, significantly improves performance on breast cancer screening tasks and enhances diagnostic efficiency.", "motivation": "To improve the application of SSL in medical imaging, particularly mammography, addressing the issues of limited data and domain specific biases.", "method": "Self-supervised learning (SSL) framework named MammoDINO, pretrained on 1.4 million mammographic images. Includes a breast tissue aware data augmentation sampler and a cross-slice contrastive learning objective.", "result": "MammoDINO achieves state-of-the-art performance on multiple breast cancer screening tasks and generalizes well across five benchmark datasets.", "conclusion": "It provides a scalable and annotation-free foundation for multipurpose computer-aided diagnosis (CAD) tools, potentially reducing radiologists' workload and improving screening efficiency."}}
{"id": "2510.11907", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11907", "abs": "https://arxiv.org/abs/2510.11907", "authors": ["Blessing Agyei Kyem", "Neema Jakisa Owor", "Andrews Danyo", "Joshua Kofi Asamoah", "Eugene Denteh", "Tanner Muturi", "Anthony Dontoh", "Yaw Adu-Gyamfi", "Armstrong Aboah"], "title": "Task-Specific Dual-Model Framework for Comprehensive Traffic Safety Video Description and Analysis", "comment": "This paper was accepted at ICCV 2025", "summary": "Traffic safety analysis requires complex video understanding to capture\nfine-grained behavioral patterns and generate comprehensive descriptions for\naccident prevention. In this work, we present a unique dual-model framework\nthat strategically utilizes the complementary strengths of VideoLLaMA and\nQwen2.5-VL through task-specific optimization to address this issue. The core\ninsight behind our approach is that separating training for captioning and\nvisual question answering (VQA) tasks minimizes task interference and allows\neach model to specialize more effectively. Experimental results demonstrate\nthat VideoLLaMA is particularly effective in temporal reasoning, achieving a\nCIDEr score of 1.1001, while Qwen2.5-VL excels in visual understanding with a\nVQA accuracy of 60.80\\%. Through extensive experiments on the WTS dataset, our\nmethod achieves an S2 score of 45.7572 in the 2025 AI City Challenge Track 2,\nplacing 10th on the challenge leaderboard. Ablation studies validate that our\nseparate training strategy outperforms joint training by 8.6\\% in VQA accuracy\nwhile maintaining captioning quality.", "AI": {"tldr": "本工作提出了一种独特的双模型框架，利用两个模型的互补优势解决交通安全分析问题，通过分别训练来提升模型性能。", "motivation": "为了捕获细粒度的行为模式并生成全面的描述以预防事故，需要复杂的视频理解。本工作旨在通过这种方法来改进交通安全分析。", "method": "我们提出了一种独特的双模型框架，利用VideoLLaMA和Qwen2.5-VL互补的优势，分别进行任务特异性优化，以减少任务干扰，让每个模型更有效地专业化。", "result": "实验结果表明，VideoLLaMA在时间推理上表现良好，取得了1.1001的CIDEr分数，而Qwen2.5-VL在视觉理解方面具有60.80%的VQA准确率。在AI City Challenge Track 2的WTS数据集上，我们方法取得了45.7572的S2分数，在挑战排行榜上位列第十。", "conclusion": "实验验证了我们这种方法的有效性，分别训练策略在保持描述质量的同时，在VQA准确率上比联合训练提升了8.6%。"}}
{"id": "2510.11812", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11812", "abs": "https://arxiv.org/abs/2510.11812", "authors": ["Souradeep Mukhopadhyay", "Rishabh Baral", "Nimeesh Mahajan", "Samhitha Harish", "Aswin RRV", "Mihir Parmar", "Mutsumi Nakamura", "Chitta Baral"], "title": "PHANTOM RECALL: When Familiar Puzzles Fool Smart Models", "comment": "22 Pages", "summary": "Large language models (LLMs) such as GPT, Gemini, and Claude often appear\nadept at solving classic logic puzzles--but how much genuine reasoning\nunderlies their answers? Recent evidence suggests that these models frequently\nrely on memorized templates rather than reasoning from first principles. When\npuzzles are slightly modified, their performance collapses, revealing a\nstriking fragility. In particular, we asked: Have LLMs addressed these issues?\nTo what extent? How about perturbations to other puzzles? Is there a general\nway of reformulating the prompt so that the models do better? To examine these\nthings systematically, we introduce PHANTOM RECALL, a benchmark comprising 25\nwell-known logic puzzles and 149 carefully designed perturbations that preserve\nreasoning structure but alter superficial details and solutions. We evaluate\neleven leading LLMs and identify a recurring failure mode--phantom\nrecall--where models confidently reproduce memorized solutions or spurious\nrationales that no longer fit the altered scenario. To probe and mitigate this\nissue, we contribute three tools: (i) an automated logical-equivalence judge to\ndetect reasoning mismatches, (ii) a taxonomy of fine-grained reasoning error\ncategories, and (iii) a prompting-based mitigation framework guided by these\ncategories. Despite near-perfect accuracy on unmodified puzzles, models\nsignificantly underperform humans on perturbed ones, exhibiting both phantom\nrecall and over-elaboration. Our findings reveal a crucial limitation: LLMs\noften fail to re-reason when contextual cues shift--highlighting the gap\nbetween linguistic fluency and logical understanding.", "AI": {"tldr": "该论文介绍了PHANTOM RECALL基准测试，使用25个经典逻辑谜题和149个经过精心设计的变形来评估大型语言模型（LLMs）的真实推理能力。研究发现，当谜题被稍微修改时，模型的性能急剧下降，说明它们大部分依赖于记忆的解决方案而不是从头开始的推理。", "motivation": "动机是为了系统地检查LLMs在面对逻辑推理任务时的真实能力，尤其是这些模型是否在稍微变化的环境下仍具备推理能力，以及是否有改进的提问方式来提升模型的表现。", "method": "Structure", "result": "{\n  \"tldr\": \"该论文介绍了PHANTOM RECALL基准测试，使用25个经典逻辑谜题和149个经过精心设计的变形来评估大型语言模型（LLMs）的真实推理能力。研究发现，当谜题被稍微修改时，模型的性能急剧下降，说明它们大部分依赖于记忆的解决方案而不是从头开始的推理。\", \n  \"motivation\": \"动机是为了系统地检查LLMs在面对逻辑推理任务时的真实能力，尤其是这些模型是否在稍微变化的环境下仍具备推理能力，以及是否有改进的提问方式来提升模型的表现。\", \n  \"method\": \"方法是引入PHANTOM RECALL基准，包含25个逻辑谜题和149个保持推理结构但改变表面细节的变形。通过检测模型对这些变形的回答，分析其表现模式。\", \n  \"result\": \"结果表现为模型在未经修改的谜题方面表现出很高的准确性，但在变形谜题上的表现显著低于人类，显示出\"幽灵记忆\"错误，说明模型过于依赖记忆的解决方案。\", \n  \"conclusion\": \"结论是LLMs在语言流畅性方面的表现与逻辑理解之间存在差距。当谜题环境变化时，模型往往未能重新推理，而是复制先前的记忆内容。这表明了完善这些模型推理能力的重要性。\"}\n}", "conclusion": "结论是LLMs在语言流畅性方面的表现与逻辑理解之间存在差距。当谜题环境变化时，模型往往未能重新推理，而是复制先前的记忆内容。这表明了完善这些模型推理能力的重要性。"}}
{"id": "2510.11992", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11992", "abs": "https://arxiv.org/abs/2510.11992", "authors": ["Hatem Ibrahem", "Ahmed Salem", "Qinmin Vivian Hu", "Guanghui Wang"], "title": "PanoTPS-Net: Panoramic Room Layout Estimation via Thin Plate Spline Transformation", "comment": null, "summary": "Accurately estimating the 3D layout of rooms is a crucial task in computer\nvision, with potential applications in robotics, augmented reality, and\ninterior design. This paper proposes a novel model, PanoTPS-Net, to estimate\nroom layout from a single panorama image. Leveraging a Convolutional Neural\nNetwork (CNN) and incorporating a Thin Plate Spline (TPS) spatial\ntransformation, the architecture of PanoTPS-Net is divided into two stages:\nFirst, a convolutional neural network extracts the high-level features from the\ninput images, allowing the network to learn the spatial parameters of the TPS\ntransformation. Second, the TPS spatial transformation layer is generated to\nwarp a reference layout to the required layout based on the predicted\nparameters. This unique combination empowers the model to properly predict room\nlayouts while also generalizing effectively to both cuboid and non-cuboid\nlayouts. Extensive experiments on publicly available datasets and comparisons\nwith state-of-the-art methods demonstrate the effectiveness of the proposed\nmethod. The results underscore the model's accuracy in room layout estimation\nand emphasize the compatibility between the TPS transformation and panorama\nimages. The robustness of the model in handling both cuboid and non-cuboid room\nlayout estimation is evident with a 3DIoU value of 85.49, 86.16, 81.76, and\n91.98 on PanoContext, Stanford-2D3D, Matterport3DLayout, and ZInD datasets,\nrespectively. The source code is available at:\nhttps://github.com/HatemHosam/PanoTPS_Net.", "AI": {"tldr": "The paper presents PanoTPS-Net, which combines CNN and TPS transformation to accurately estimate room layouts based on single panorama images, achieving robust results on multiple datasets.", "motivation": "The motivation for this research is the importance of accurately estimating the 3D layout of rooms, with potential applications in numerous fields such as robotics, augmented reality, and interior design.", "method": "The paper introduces PanoTPS-Net, which is comprised of a CNN and a TPS transformation layer. The CNN extracts high-level features from input panorama images to estimate the spatial parameters of the TPS. The TPS transformation then warps a reference layout into the room layout required based on the predicted parameters.", "result": "PanoTPS-Net achieves high 3DIoU values, 85.49, 86.16, 81.76, and 91.98, on four different datasets, and shows robustness in handling both cuboid and non-cuboid room layouts.", "conclusion": "The proposed method, PanoTPS-Net, effectively estimates the room layout from a single panorama image and generalizes well to both cuboid and non-cuboid rooms. The model is highly accurate and robust, as evidenced by the results from extensive experiments."}}
{"id": "2510.11892", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11892", "abs": "https://arxiv.org/abs/2510.11892", "authors": ["Kai Mei", "Jiang Guo", "Shuaichen Chang", "Mingwen Dong", "Dongkyu Lee", "Xing Niu", "Jiarong Jiang"], "title": "R-WoM: Retrieval-augmented World Model For Computer-use Agents", "comment": null, "summary": "Large Language Models (LLMs) can serve as world models to enhance agent\ndecision-making in digital environments by simulating future states and\npredicting action outcomes, potentially eliminating costly trial-and-error\nexploration. However, this capability is fundamentally limited by LLMs'\ntendency toward hallucination and their reliance on static training knowledge,\nwhich can lead to compounding errors that inhibit long-horizon simulations. To\nsystematically investigate whether LLMs are appropriate for world modeling, we\nprobe two core capabilities of world models--future state prediction and reward\nestimation--through three tasks: next-state identification, full-procedure\nplanning alignment, and milestone transition recognition. Our analysis shows\nthat while LLMs effectively capture immediate next states and identify\nmeaningful state transitions, their performance rapidly degrades in\nfull-procedure planning. This highlights LLMs' limitations in reliably modeling\nenvironment dynamics over long horizons. To address these limitations, we\npropose the Retrieval-augmented World Model (R-WoM), which grounds LLM\nsimulations by incorporating factual, up-to-date knowledge retrieved from\nexternal tutorials. Experiments show that R-WoM achieves substantial\nimprovements of up to 25.3% (OSWorld) and 18.1% (WebArena) compared to\nbaselines, with particular advantages in longer-horizon simulations.", "AI": {"tldr": "This paper examines the effectiveness of LLMs in world modeling for digital environments and introduces R-WoM to tackle the challenges of hallucination and static knowledge, showing significant performance improvements.", "motivation": "The motivation is to explore the potential of LLMs in enhancing decision-making in digital environments through world modeling, while addressing the limitations posed by LLMs' hallucination and reliance on static knowledge.", "method": "The paper investigates the capability of LLMs in world modeling by evaluating future state prediction and reward estimation through three specific tasks. It further proposes the Retrieval-augmented World Model (R-WoM) to improve the reliability of LLM simulations over long horizons by integrating up-to-date knowledge.", "result": "The research finds that while LLMs can predict immediate next states effectively, their performance significantly diminishes in long-term planning, and the proposed R-WoM shows substantial improvements, up to 25.3% in OSWorld and 18.1% in WebArena.", "conclusion": "The conclusion is that LLMs alone are not sufficient for reliable long-horizon world modeling due to inherent limitations, but the R-WoM approach can significantly enhance their capability by integrating factual, up-to-date knowledge."}}
{"id": "2510.11996", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11996", "abs": "https://arxiv.org/abs/2510.11996", "authors": ["Tanner Muturi", "Blessing Agyei Kyem", "Joshua Kofi Asamoah", "Neema Jakisa Owor", "Richard Dyzinela", "Andrews Danyo", "Yaw Adu-Gyamfi", "Armstrong Aboah"], "title": "Prompt-Guided Spatial Understanding with RGB-D Transformers for Fine-Grained Object Relation Reasoning", "comment": "The paper was accepted at ICCV Conference 2025", "summary": "Spatial reasoning in large-scale 3D environments such as warehouses remains a\nsignificant challenge for vision-language systems due to scene clutter,\nocclusions, and the need for precise spatial understanding. Existing models\noften struggle with generalization in such settings, as they rely heavily on\nlocal appearance and lack explicit spatial grounding. In this work, we\nintroduce a dedicated spatial reasoning framework for the Physical AI Spatial\nIntelligence Warehouse dataset introduced in the Track 3 2025 AI City\nChallenge. Our approach enhances spatial comprehension by embedding mask\ndimensions in the form of bounding box coordinates directly into the input\nprompts, enabling the model to reason over object geometry and layout. We\nfine-tune the framework across four question categories namely: Distance\nEstimation, Object Counting, Multi-choice Grounding, and Spatial Relation\nInference using task-specific supervision. To further improve consistency with\nthe evaluation system, normalized answers are appended to the GPT response\nwithin the training set. Our comprehensive pipeline achieves a final score of\n73.0606, placing 4th overall on the public leaderboard. These results\ndemonstrate the effectiveness of structured prompt enrichment and targeted\noptimization in advancing spatial reasoning for real-world industrial\nenvironments.", "AI": {"tldr": "本研究提出了一种专门用于仓库等大规模3D环境的空间推理框架，通过改善空间理解，研究实现了高级别的空间推理表现。", "motivation": "现有的视觉-语言系统在大规模3D环境中的空间推理上面临挑战，具体表现为对场景混乱、遮挡以及需要精确的空间理解的处理不佳。现有模型难以在这些环境下进行良好的泛化。", "method": "本研究提出了一种专门用于大规模3D空间环境（如仓库）的空间推理框架，该框架通过在输入提示中嵌入物体的边界框坐标来改进对物体几何形状和布局的理解。框架在四个问题类别上进行了任务特定的监督微调，包括距离估算、物体计数、多选题接地和空间关系推理。为了提升与评估系统的符合性，在训练集中还附上了归一化的答案。", "result": "研究提出的方法在Physical AI Spatial Intelligence Warehouse数据集上实现了综合评分为73.0606，位列公共排行榜第4名。", "conclusion": "实验证明，通过结构化提示增强和针对性优化可以提升现实世界工业环境中的空间推理能力。"}}
{"id": "2510.11905", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11905", "abs": "https://arxiv.org/abs/2510.11905", "authors": ["Patrick Haller", "Mark Ibrahim", "Polina Kirichenko", "Levent Sagun", "Samuel J. Bell"], "title": "LLM Knowledge is Brittle: Truthfulness Representations Rely on Superficial Resemblance", "comment": null, "summary": "For Large Language Models (LLMs) to be reliable, they must learn robust\nknowledge that can be generally applied in diverse settings -- often unlike\nthose seen during training. Yet, extensive research has shown that LLM\nperformance can be brittle, with models exhibiting excessive sensitivity to\ntrivial input variations. In this work, we explore whether this brittleness is\na direct result of unstable internal knowledge representations. To explore this\nquestion, we build on previous work showing that LLM representations encode\nstatement truthfulness -- i.e., true, factual statements can be easily\nseparated from false, inaccurate ones. Specifically, we test the robustness of\nlearned knowledge by evaluating representation separability on samples that\nhave undergone superficial transformations to drive them out-of-distribution\n(OOD), such as typos or reformulations. By applying semantically-preserving\nperturbations, we study how separability degrades as statements become more\nOOD, across four LLM families, five evaluation datasets, and three knowledge\nprobing methods. Our results reveal that internal representations of statement\ntruthfulness collapse as the samples' presentations become less similar to\nthose seen during pre-training. While LLMs can often distinguish between true\nand false statements when they closely resemble the pre-training data, this\nability is highly dependent on the statement's exact surface form. These\nfindings offer a possible explanation for brittle benchmark performance: LLMs\nmay learn shallow, non-robust knowledge representations that allow for only\nlimited generalizability. Our work presents a fundamental challenge for the\nutility of truthfulness probes, and more broadly, calls for further research on\nimproving the robustness of learned knowledge representations.", "AI": {"tldr": "本研究通过评估大规模语言模型（LLMs）在面对经过表面变形后样本时，其内部知识表示的能力，揭示了模型鲁棒性不足的问题，指出其学习的知识表示往往是浅层且不稳健的。", "motivation": "动机在于探讨大规模语言模型（LLMs）的脆弱性是否直接源于不稳定的内部知识表示。这个问题源于LLMs在广泛研究中表现出的对输入微小变化的过度敏感。", "method": "通过构建在先前工作基础上的研究方法，该研究考察了LLM表示是否能够编码陈述的真实性，并通过评估经过浅表转换导致出分布（OOD）样本的表示分离性来测试学习知识的稳健性，其中包括错别字或重新表述。通过应用保持语义的扰动，探讨了陈述越OOD时分离性如何下降的情况，涉及四种LLM家族，五个评估数据集和三种知识探索方法。", "result": "研究结果显示，随着样本表面形式的变化，真实性和虚假性陈述之间的分离能力逐渐减弱，这表明LLMs在处理预训练分布以外的数据时性能会受到影响，暗示了提升LLMs鲁棒性的必要性。", "conclusion": "研究表明，当样本呈现方式与预训练数据相差较大时，内部的陈述真实性表示会发生崩解。虽然LLMs在表面形式和预训练数据相似时能够区分真实和虚假陈述，但这种能力高度依赖于陈述的具体表面形式。这些发现为LLMs在基准测试中的脆弱表现提供了可能的解释，并呼吁进一步研究以改善学习的知识表示的鲁棒性。"}}
{"id": "2510.12021", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.12021", "abs": "https://arxiv.org/abs/2510.12021", "authors": ["Leili Barekatain", "Ben Glocker"], "title": "Evaluating the Explainability of Vision Transformers in Medical Imaging", "comment": "Accepted at Workshop on Interpretability of Machine Intelligence in\n  Medical Image Computing at MICCAI 2025", "summary": "Understanding model decisions is crucial in medical imaging, where\ninterpretability directly impacts clinical trust and adoption. Vision\nTransformers (ViTs) have demonstrated state-of-the-art performance in\ndiagnostic imaging; however, their complex attention mechanisms pose challenges\nto explainability. This study evaluates the explainability of different Vision\nTransformer architectures and pre-training strategies - ViT, DeiT, DINO, and\nSwin Transformer - using Gradient Attention Rollout and Grad-CAM. We conduct\nboth quantitative and qualitative analyses on two medical imaging tasks:\nperipheral blood cell classification and breast ultrasound image\nclassification. Our findings indicate that DINO combined with Grad-CAM offers\nthe most faithful and localized explanations across datasets. Grad-CAM\nconsistently produces class-discriminative and spatially precise heatmaps,\nwhile Gradient Attention Rollout yields more scattered activations. Even in\nmisclassification cases, DINO with Grad-CAM highlights clinically relevant\nmorphological features that appear to have misled the model. By improving model\ntransparency, this research supports the reliable and explainable integration\nof ViTs into critical medical diagnostic workflows.", "AI": {"tldr": "研究评估了不同视觉变压器架构的可解释性，最终确认结合Grad-CAM的DINO提供了最佳解释。这有助于将视觉变压器透明地整合到医疗诊断中。", "motivation": "由于解释能力直接影响临床信任和采用，解释视觉变压器（ViT）在诊断成像中的复杂注意机制成为一项挑战，因此了解模型决策在医学成像中至关重要。", "method": "本研究评估了不同视觉变压器架构（ViT，DeiT，DINO和Swin Transformer）及其预训练策略的可解释性，使用了Gradient Attention Rollout和Grad-CAM两种方法，在外周血细胞分类和乳腺超声图像分类两个医疗成像任务中进行了定量和定性分析。", "result": "研究发现，结合Grad-CAM的DINO提供了最忠实和局部化的解释。Grad-CAM产生更具类别区分性且精确的空间热图，而在误分类情况下，DINO与Grad-CAM仍能突出临床相关的形态特征。", "conclusion": "通过提高模型的透明度，这项研究支持了可解释且可靠的视觉变压器在关键医疗诊断流程中的集成。"}}
{"id": "2510.11919", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11919", "abs": "https://arxiv.org/abs/2510.11919", "authors": ["Armel Zebaze", "Rachel Bawden", "Benoît Sagot"], "title": "LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens", "comment": null, "summary": "Large reasoning models (LRMs) have led to new possibilities in terms of\nproblem-solving, through the devising of a natural language thought process\nprior to answering a query. While their capabilities are well known across\nmathematics and coding tasks, their impact on the task of machine translation\n(MT) remains underexplored. In this work, we explore the benefits of the\ngeneration of intermediate tokens when performing MT across multiple language\npairs of different levels of resourcedness and multiple setups. We find that\n\"thinking tokens\" do not help LRMs better perform MT. This result generalizes\nto models fine-tuned to reason before translating using distilled chain of\nthought (CoT) inspired by human translators' practices. Specifically,\nfine-tuning a model with synthetic CoT explanations detailing how to translate\nstep-by-step does not outperform standard input-output fine-tuning. However,\nconstructing the intermediate tokens by combining the outputs of modular\ntranslation-specific prompting strategies results in improvements. Our findings\nunderscore that the contribution of intermediate tokens during fine-tuning\nhighly depends on the presence of translation attempts within them. More\nbroadly, our results suggest that using a teacher to refine target translations\nor to expand parallel corpora is more impactful than distilling their CoT\nexplanations into \"thinking\" MT models.", "AI": {"tldr": "本文探讨了大型推理模型在机器翻译任务中的表现，并发现通过自然语言“思考标记”进行推理的策略并不能提高翻译性能。研究表明，与引导模型进行链式推理相比，通过特定模块提示策略产生的中间标记提升翻译表现更有效。此外，使用教师模型来优化目标翻译或扩展翻译语料库具有更大的潜力。", "motivation": "尽管大规模推理模型在数学和编程任务上的能力和前景已经广为人知，但它们对机器翻译任务的影响尚未得到充分研究。研究的目标是评估生成中间标记对机器翻译的影响。", "method": "本研究探索了在多个语言对和不同资源水平下，机器翻译任务中生成中间标记（tokens）的好处。具体来说，探究了大型推理模型（LRMs）在翻译前通过自然语言思考过程生成“思考标记”的效果。同时，分析了基于人类译者实践的蒸馏链式思维（CoT）对翻译模型性能的影响。", "result": "研究发现，生成“思考标记”并不能帮助大规模推理模型在机器翻译任务中表现得更好。即使对模型进行微调，使其在翻译之前进行推理，基于人类翻译者实践的蒸馏链式思维（CoT）也没有显示出优势。然而，当中间标记是由组合模块特定翻译提示策略生成时，可以观察到性能的提高。", "conclusion": "研究结论指出，中间标记在微调过程中的贡献高度依赖于其中是否包含了翻译尝试。更广泛地看，使用教师级模态来细化目标翻译或扩展平行语料库，比将它们的思维链式解释蒸馏到“思考”机器翻译模型中更为有效。"}}
{"id": "2510.12056", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.12056", "abs": "https://arxiv.org/abs/2510.12056", "authors": ["Xinxin Huang", "Han Sun", "Junmin Cai", "Ningzhong Liu", "Huiyu Zhou"], "title": "APGNet: Adaptive Prior-Guided for Underwater Camouflaged Object Detection", "comment": "6 pages. accepted by ACM MM Asia 2025", "summary": "Detecting camouflaged objects in underwater environments is crucial for\nmarine ecological research and resource exploration. However, existing methods\nface two key challenges: underwater image degradation, including low contrast\nand color distortion, and the natural camouflage of marine organisms.\nTraditional image enhancement techniques struggle to restore critical features\nin degraded images, while camouflaged object detection (COD) methods developed\nfor terrestrial scenes often fail to adapt to underwater environments due to\nthe lack of consideration for underwater optical characteristics.\n  To address these issues, we propose APGNet, an Adaptive Prior-Guided Network,\nwhich integrates a Siamese architecture with a novel prior-guided mechanism to\nenhance robustness and detection accuracy. First, we employ the Multi-Scale\nRetinex with Color Restoration (MSRCR) algorithm for data augmentation,\ngenerating illumination-invariant images to mitigate degradation effects.\nSecond, we design an Extended Receptive Field (ERF) module combined with a\nMulti-Scale Progressive Decoder (MPD) to capture multi-scale contextual\ninformation and refine feature representations. Furthermore, we propose an\nadaptive prior-guided mechanism that hierarchically fuses position and boundary\npriors by embedding spatial attention in high-level features for coarse\nlocalization and using deformable convolution to refine contours in low-level\nfeatures.\n  Extensive experimental results on two public MAS datasets demonstrate that\nour proposed method APGNet outperforms 15 state-of-art methods under widely\nused evaluation metrics.", "AI": {"tldr": "提出APGNet，一种自适应先验引导的网络，解决了水下图像降质和伪装问题，在两个MAS数据集上表现出超越其他方法的效果。", "motivation": "为了应对水下物体检测中图像退化和自然伪装的挑战，解决现有方法在恢复关键特征和适应水下环境方面的问题。", "method": "APGNet采用了一种自适应先验引导网络，该网络结合了Siamese架构和新型的先验引导机制，以增强鲁棒性和检测精度。方法包括：1) 使用多尺度视网膜颜色恢复算法进行数据增强，以减轻退化效应；2) 设计扩展感受野模块和多尺度渐进解码器来捕捉多尺度上下文信息并优化特征表示；3) 提出自适应先验引导机制，分层融合位置和边界先验，提升目标检测性能。", "result": "APGNet在两个MAS数据集上进行了实验，结果表明其优于其他15种前沿方法。", "conclusion": "实验结果表明，APGNet在两个公开的MAS数据集上，相较于15种最先进的方法，在广泛使用的评估指标下表现更优。"}}
{"id": "2510.11928", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11928", "abs": "https://arxiv.org/abs/2510.11928", "authors": ["Lorena Calvo-Bartolomé", "Valérie Aldana", "Karla Cantarero", "Alonso Madroñal de Mesa", "Jerónimo Arenas-García", "Jordan Boyd-Graber"], "title": "Discrepancy Detection at the Data Level: Toward Consistent Multilingual Question Answering", "comment": "Long paper accepted at EMNLP 2025", "summary": "Multilingual question answering (QA) systems must ensure factual consistency\nacross languages, especially for objective queries such as What is jaundice?,\nwhile also accounting for cultural variation in subjective responses. We\npropose MIND, a user-in-the-loop fact-checking pipeline to detect factual and\ncultural discrepancies in multilingual QA knowledge bases. MIND highlights\ndivergent answers to culturally sensitive questions (e.g., Who assists in\nchildbirth?) that vary by region and context. We evaluate MIND on a bilingual\nQA system in the maternal and infant health domain and release a dataset of\nbilingual questions annotated for factual and cultural inconsistencies. We\nfurther test MIND on datasets from other domains to assess generalization. In\nall cases, MIND reliably identifies inconsistencies, supporting the development\nof more culturally aware and factually consistent QA systems.", "AI": {"tldr": "提出了MIND管道，用于检测多语言问答系统的事实和文化差异，评估了其在母婴健康领域的双语系统中的效果，并展示了其在其他领域的泛化能力。", "motivation": "多语言问答系统需要在不同语言间确保事实的一致性，特别对于如“黄疸是什么？”这类客观查询。同时，系统还需考虑到主观回应中的文化差异。", "method": "我们提出了一种名为MIND的用户参与的事实核查管道，用于检测多语言问答知识库中的事实和文化差异。MIND能够突出显示文化敏感问题的答案差异，这些问题因地区和上下文而异。", "result": "我们在母婴健康领域的双语问答系统中评估了MIND，并发布了一组标注了事实和文化不一致性的双语问题数据集。我们在其他领域的数据集上进一步测试了MIND，以评估其泛化能力。结果表明，MIND能够可靠地识别不一致性。", "conclusion": "MIND支持开发文化意识更强、事实更一致的问答系统，为进一步提升多语言问答系统的性能提供了可能。"}}
{"id": "2510.12069", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.12069", "abs": "https://arxiv.org/abs/2510.12069", "authors": ["Sandeep Mishra", "Oindrila Saha", "Alan C. Bovik"], "title": "VIDMP3: Video Editing by Representing Motion with Pose and Position Priors", "comment": null, "summary": "Motion-preserved video editing is crucial for creators, particularly in\nscenarios that demand flexibility in both the structure and semantics of\nswapped objects. Despite its potential, this area remains underexplored.\nExisting diffusion-based editing methods excel in structure-preserving tasks,\nusing dense guidance signals to ensure content integrity. While some recent\nmethods attempt to address structure-variable editing, they often suffer from\nissues such as temporal inconsistency, subject identity drift, and the need for\nhuman intervention. To address these challenges, we introduce VidMP3, a novel\napproach that leverages pose and position priors to learn a generalized motion\nrepresentation from source videos. Our method enables the generation of new\nvideos that maintain the original motion while allowing for structural and\nsemantic flexibility. Both qualitative and quantitative evaluations demonstrate\nthe superiority of our approach over existing methods. The code will be made\npublicly available at https://github.com/sandeep-sm/VidMP3.", "AI": {"tldr": "文章提出了VidMP3方法，旨在解决视频编辑中结构和语义编辑的灵活性问题，提供了一种在保持原有运动的基础上进行灵活编辑的新方案。", "motivation": "现有的扩散编辑方法在保持结构一致方面表现出色，但面对结构可变的编辑时，往往存在时间不一致、主体身份漂移和需要人工干预等问题。", "method": "VidMP3采用姿态和位置先验来学习来自源视频的泛化运动表示，从而生成保持原始运动同时允许结构和语义灵活性的新视频。", "result": "定性和定量的评估都显示了本方法相较于现有方法的优势。", "conclusion": "VidMP3通过引入姿态和位置先验，能够在视频编辑中实现结构和语义的灵活性。实验结果证明了该方法的有效性。"}}
{"id": "2510.11944", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11944", "abs": "https://arxiv.org/abs/2510.11944", "authors": ["Yupei Li", "Philipp Borchert", "Gerasimos Lampouras"], "title": "TopoAlign: A Framework for Aligning Code to Math via Topological Decomposition", "comment": null, "summary": "Large Language Models (LLMs) excel at both informal and formal (e.g. Lean 4)\nmathematical reasoning but still struggle with autoformalisation, the task of\ntransforming informal into formal mathematical statements. Autoformalisation\nhelps pair the informal reasoning of LLMs with formal proof assistants which\nenable machine-verifiable generation and mitigate hallucinations. Yet, the\nperformance of current Math LLMs is constrained by the scarcity of large-scale\ncorpora, particularly those containing pairs of informal and formal statements.\nAlthough current models are trained to generate code from natural language\ninstructions, structural and syntactic differences between these and formal\nmathematics limit effective transfer learning. We propose TopoAlign, a\nframework that unlocks widely available code repositories as training resources\nfor Math LLMs. TopoAlign decomposes code into docstrings, main functions, and\ndependency functions, and reassembles these components into analogues that\nstructurally mirror formal statements. This produces structurally aligned code\ndata that can be used for training Math LLMs without requiring additional human\nannotation. We train two state-of-the-art models, DeepSeek-Math and Herald, and\nevaluate them on the minif2f, Putnam, and ProofNet benchmarks. TopoAlign\nprovides substantial gains for DeepSeek-Math, improving performance by 17.77%\non BEq@10 and 68.82% on typecheck@10. Despite introducing no new mathematical\nknowledge, our framework achieves gains of 0.12% and 1.09% for Herald on BEq@10\nand typecheck@10, respectively, demonstrating that training on aligned code\ndata is beneficial even for specialized models.", "AI": {"tldr": "提出了一种名为TopoAlign的框架，可以解锁广泛可用的代码存储库作为数学大语言模型的训练资源。结果表明，TopoAlign框架为DeepSeek-Math模型提供了显著提升，提高了一次前10概率的等价性（BEq@10）17.77%和类型检查（typecheck@10）68.82%，即使没有引入新的数学知识，也使Herald模型提高了0.12%和1.09%。", "motivation": "尽管当前数学大语言模型（LLMs）在非正式和正式（如Lean 4）数学推理方面表现出色，但它们在自动形式化方面仍然存在困难，即把非正式的数学语句转化为正式的数学语句。自动形式化有助于将大语言模型的非正式推理与形式证明助手相结合，减少幻觉生成并确保机器可验证的结果。然而，现有数学大语言模型的性能受到了大规模语料库，尤其是包含非正式和正式语句对语料库稀缺性的限制。目前的模型虽然可以将自然语言指令转化为代码，但代码和形式化数学之间的结构和句法差异限制了迁移学习的效果。", "method": "提出了一种名为TopoAlign的框架，该框架可以解锁广泛可用的代码存储库作为数学大语言模型的训练资源。TopoAlign将代码分解为文档字符串、主函数和依赖函数，并将这些组件重新组装成形式上类似于数学陈述的模拟物，从而无需额外的人类标注即可生成用于训练数学大语言模型的结构对齐代码数据。", "result": "分别训练了两个最先进的模型，DeepSeek-Math和Herald，并在minif2f、Putnam和ProofNet基准测试上进行了评估。结果表明，尽管TopoAlign框架并未引入新的数学知识，依然为Herald模型带来了提升，即提高了0.12%的BEq@10和1.09%的typecheck@10。这说明，即使对于特定的数学LLM来说，使用结构对齐的代码数据进行训练依然有一些好处。", "conclusion": "通过研究，我们提出了一种创新的方法，利用广泛存在的代码资源来训练数学LLMs，旨在解决当前模型在形式化数学转换方面的性能瓶颈，并通过实验表明了这种方法的有效性。"}}
{"id": "2510.12075", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12075", "abs": "https://arxiv.org/abs/2510.12075", "authors": ["Aashish Dhawan", "Divyanshu Mudgal"], "title": "A Review on Domain Adaption and Generative Adversarial Networks(GANs)", "comment": null, "summary": "The major challenge in today's computer vision scenario is the availability\nof good quality labeled data. In a field of study like image classification,\nwhere data is of utmost importance, we need to find more reliable methods which\ncan overcome the scarcity of data to produce results comparable to previous\nbenchmark results. In most cases, obtaining labeled data is very difficult\nbecause of the high cost of human labor and in some cases impossible. The\npurpose of this paper is to discuss Domain Adaptation and various methods to\nimplement it. The main idea is to use a model trained on a particular dataset\nto predict on data from a different domain of the same kind, for example - a\nmodel trained on paintings of airplanes predicting on real images of airplanes", "AI": {"tldr": "This paper discusses domain adaptation methods to address the challenge of insufficient labeled data in image classification tasks.", "motivation": "The scarcity of labeled data due to high labor costs and sometimes impossibility of data acquisition poses a significant challenge in computer vision tasks such as image classification.", "method": "Structure", "result": "The paper explores the use of domain adaptation techniques, focusing on how a model trained on one dataset (e.g., paintings of airplanes) can effectively predict on data from a different domain (e.g., real images of airplanes).", "conclusion": "Domain adaptation is a promising approach to overcome data scarcity issues in computer vision by leveraging models trained on similar but different datasets."}}
{"id": "2510.11952", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11952", "abs": "https://arxiv.org/abs/2510.11952", "authors": ["Priyanka Dey", "Daniele Rosa", "Wenqing Zheng", "Daniel Barcklow", "Jieyu Zhao", "Emilio Ferrara"], "title": "GRAVITY: A Framework for Personalized Text Generation via Profile-Grounded Synthetic Preferences", "comment": null, "summary": "Personalization in LLMs often relies on costly human feedback or interaction\nlogs, limiting scalability and neglecting deeper user attributes. To reduce the\nreliance on human annotations, we introduce GRAVITY (Generative Response with\nAligned Values, Interests, and Traits of You), a framework for generating\nsynthetic, profile-grounded preference data that captures users' interests,\nvalues, beliefs, and personality traits. By integrating demographic, cultural,\nand psychological frameworks -- including Hofstede's cultural dimensions,\nSchwartz's basic values, the World Values Survey, and Big Five OCEAN traits --\nGRAVITY synthesizes preference pairs to guide personalized content generation.\nWe evaluate GRAVITY on book descriptions for 400 Amazon users, comparing it to\nprompt-based conditioning, standard fine-tuning, and naive synthetic pair\ngeneration. Profile-grounded synthetic data consistently improves generation,\nespecially across multiple cultures (USA, Brazil, Japan, India), achieving over\n4% higher preference gains across baselines, with user studies showing that\nGRAVITY outputs are preferred over 86% of the time. Our results show that\nscenario-grounded synthetic data can capture richer user variation, reduce\nreliance on costly annotation, and produce more engaging, user-centered\ncontent, offering a scalable path for LLM personalization.", "AI": {"tldr": "GRAVITY通过整合心理学框架合成偏好对以指导个性化内容生成，减少人工注释依赖，提升生成内容的吸引力，为LLM个性化提供可扩展路径。", "motivation": "为了减少对人工注释的依赖，引入GRAVITY框架，以合成的个人资料为基础生成偏好数据，捕获用户的兴趣、价值观、信仰和个人特质。", "method": "通过整合霍夫斯泰德的文化维度、施瓦茨的基本价值观、世界价值观调查和五大人格特质等心理学框架，GRAVITY 合成偏好对，以指导个性化内容生成。", "result": "相比于基于提示的条件处理、标准微调和简单的合成对生成，GRAVITY在不同文化背景下的书籍描述生成上表现出色，用户测试表明，其输出被偏好超过86%的时间。", "conclusion": "结果表明场景化的合成数据可以捕捉更丰富的用户变化，减少对昂贵注释的依赖，并生成更吸引人、以用户为中心的内容。"}}
{"id": "2510.12089", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.12089", "abs": "https://arxiv.org/abs/2510.12089", "authors": ["Xingpei Ma", "Shenneng Huang", "Jiaran Cai", "Yuansheng Guan", "Shen Zheng", "Hanfeng Zhao", "Qiang Zhang", "Shunsi Zhang"], "title": "Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback", "comment": null, "summary": "Recent advances in diffusion models have significantly improved audio-driven\nhuman video generation, surpassing traditional methods in both quality and\ncontrollability. However, existing approaches still face challenges in lip-sync\naccuracy, temporal coherence for long video generation, and multi-character\nanimation. In this work, we propose a diffusion transformer (DiT)-based\nframework for generating lifelike talking videos of arbitrary length, and\nintroduce a training-free method for multi-character audio-driven animation.\nFirst, we employ a LoRA-based training strategy combined with a position shift\ninference approach, which enables efficient long video generation while\npreserving the capabilities of the foundation model. Moreover, we combine\npartial parameter updates with reward feedback to enhance both lip\nsynchronization and natural body motion. Finally, we propose a training-free\napproach, Mask Classifier-Free Guidance (Mask-CFG), for multi-character\nanimation, which requires no specialized datasets or model modifications and\nsupports audio-driven animation for three or more characters. Experimental\nresults demonstrate that our method outperforms existing state-of-the-art\napproaches, achieving high-quality, temporally coherent, and multi-character\naudio-driven video generation in a simple, efficient, and cost-effective\nmanner.", "AI": {"tldr": "提出了一种基于扩散转换器（DiT）框架生成任意长度的逼真说话视频的方法，并引入了一种无训练的多角色音频驱动动画方法。", "motivation": "虽然扩散模型在音频驱动的人类视频生成中取得了显著进展，但仍面临着口型同步准确性、长期视频生成的时间连贯性和多角色动画的挑战。", "method": "采用LoRA训练策略结合位置位移推理方法，以及部分参数更新与奖励反馈来增强口型同步和自然身体动作；提出了一种无训练的方法Mask-CFG，用于多角色动画。", "result": "实验结果表明，该方法在高质量、时间连贯性和多角色音频驱动视频生成方面优于现有最先进的方法。", "conclusion": "提出的方法以简单、高效和成本效益的方式实现了高质量、时间连贯性好的多角色音频驱动视频生成。"}}
{"id": "2510.11956", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11956", "abs": "https://arxiv.org/abs/2510.11956", "authors": ["Gabrielle Kaili-May Liu", "Bryan Li", "Arman Cohan", "William Gantt Walden", "Eugene Yang"], "title": "Evaluating Retrieval-Augmented Generation Systems on Unanswerable, Uncheatable, Realistic, Multi-hop Queries", "comment": null, "summary": "Real-world use cases often present RAG systems with complex queries for which\nrelevant information is missing from the corpus or is incomplete. In these\nsettings, RAG systems must be able to reject unanswerable, out-of-scope queries\nand identify failures of retrieval and multi-hop reasoning. Despite this,\nexisting RAG benchmarks rarely reflect realistic task complexity for multi-hop\nor out-of-scope questions, which often can be cheated via disconnected\nreasoning (i.e., solved without genuine multi-hop inference) or require only\nsimple factual recall. This limits the ability for such benchmarks to uncover\nlimitations of existing RAG systems. To address this gap, we present the first\npipeline for automatic, difficulty-controlled creation of\nun$\\underline{c}$heatable, $\\underline{r}$ealistic, $\\underline{u}$nanswerable,\nand $\\underline{m}$ulti-hop $\\underline{q}$uerie$\\underline{s}$ (CRUMQs),\nadaptable to any corpus and domain. We use our pipeline to create CRUMQs over\ntwo popular RAG datasets and demonstrate its effectiveness via benchmark\nexperiments on leading retrieval-augmented LLMs. Results show that compared to\nprior RAG benchmarks, CRUMQs are highly challenging for RAG systems and achieve\nup to 81.0\\% reduction in cheatability scores. More broadly, our pipeline\noffers a simple way to enhance benchmark difficulty and realism and drive\ndevelopment of more capable RAG systems.", "AI": {"tldr": "本文介绍了一种创建不可欺骗、现实的、不可回答的和多跳问题的自动管道，并证明这种方法可以有效提高RAG系统的挑战性和真实性。", "motivation": "现有的RAG基准测试很少反映多跳或超出范围问题的现实任务复杂性，这些问题可以通过不连接的推理（即，不需要真正的多跳推理）或只需简单的事实回忆来解决。这限制了这类基准测试揭露现有RAG系统限制的能力。", "method": "提出了一个自动创建不可欺骗的、真实的、不可回答的和多跳问题（CRUMQs）的管道，该管道可以适应任何语料库和领域。", "result": "实验显示，与先前的RAG基准测试相比，CRUMQs对RAG系统更具挑战性，降低了高达81.0%的欺骗性得分。", "conclusion": "此管道提供了一种简单的方法来增强基准的难度和真实性，推动了更强大的RAG系统的开发。"}}
{"id": "2510.12095", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.12095", "abs": "https://arxiv.org/abs/2510.12095", "authors": ["Wenxu Zhou", "Kaixuan Nie", "Hang Du", "Dong Yin", "Wei Huang", "Siqiang Guo", "Xiaobo Zhang", "Pengbo Hu"], "title": "IL3D: A Large-Scale Indoor Layout Dataset for LLM-Driven 3D Scene Generation", "comment": "9 pages main paper; 15 pages references and appendix", "summary": "In this study, we present IL3D, a large-scale dataset meticulously designed\nfor large language model (LLM)-driven 3D scene generation, addressing the\npressing demand for diverse, high-quality training data in indoor layout\ndesign. Comprising 27,816 indoor layouts across 18 prevalent room types and a\nlibrary of 29,215 high-fidelity 3D object assets, IL3D is enriched with\ninstance-level natural language annotations to support robust multimodal\nlearning for vision-language tasks. We establish rigorous benchmarks to\nevaluate LLM-driven scene generation. Experimental results show that supervised\nfine-tuning (SFT) of LLMs on IL3D significantly improves generalization and\nsurpasses the performance of SFT on other datasets. IL3D offers flexible\nmultimodal data export capabilities, including point clouds, 3D bounding boxes,\nmultiview images, depth maps, normal maps, and semantic masks, enabling\nseamless adaptation to various visual tasks. As a versatile and robust\nresource, IL3D significantly advances research in 3D scene generation and\nembodied intelligence, by providing high-fidelity scene data to support\nenvironment perception tasks of embodied agents.", "AI": {"tldr": "IL3D, 一个为大语言模型驱动的3D场景生成设计的数据集，提升了模型性能。", "motivation": "解决室内布局设计领域对多样化、高质量训练数据的需求。", "method": "IL3D, 一个专为大语言模型驱动的3D场景生成设计的大规模数据集，包含27,816个室内布局和29,215个高质量3D物体资产，并配有实例级自然语言注释，支持视觉语言任务的鲁棒多模态学习。", "result": "实验表明在IL3D上对LLMs进行监督微调显著提升了泛化能力，并超越了在其他数据集上的性能。", "conclusion": "IL3D是一种多功能且健壮的资源，显著推进了3D场景生成和具身智能领域的研究。"}}
{"id": "2510.11958", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11958", "abs": "https://arxiv.org/abs/2510.11958", "authors": ["Xuan Luo", "Weizhi Wang", "Xifeng Yan"], "title": "Direct Multi-Token Decoding", "comment": null, "summary": "Decoder-only transformers have become the standard architecture for large\nlanguage models (LLMs) due to their strong performance. Recent studies suggest\nthat, in pre-trained LLMs, early, middle, and late layers may serve distinct\nroles: Early layers focus on understanding the input context, middle layers\nhandle task-specific processing, and late layers convert abstract\nrepresentations into output tokens. We hypothesize that once representations\nhave been processed by the early and middle layers, the resulting hidden states\nmay encapsulate sufficient information to support the generation of multiple\ntokens using only the late layers, eliminating the need to repeatedly traverse\nthe early and middle layers. We refer to this inference paradigm as Direct\nMulti-Token Decoding (DMTD). Unlike speculative decoding, our method introduces\nno additional parameters, auxiliary routines, or post-generation verification.\nDespite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model\nhas already demonstrated promising results, achieving up to a 2x speedup with\nonly minor performance loss. Moreover, as shown in our scaling analysis, its\nperformance is expected to further improve with larger training datasets.", "AI": {"tldr": "提出了一种称为Direct Multi-Token Decoding (DMTD) 的方法，它能够仅用晚期Decoder-only Transformer层生成多个令牌，从而提高语言模型生成效率。DMTD方法已在Qwen3-4B模型中展示了高达2倍的速度提升。", "motivation": "本研究旨在减少使用LLMs时反复遍历早期和中期层的需求，从而提高生成多个令牌的效率。", "method": "Decoder-only transformers构成大型语言模型（LLMs）的标准架构。研究表明，在经过预训练的LLMs中，早期、中期和晚期层可能具有不同的角色：早期层专注于理解输入上下文，中期层处理特定任务，晚期层将抽象表示转换为输出令牌。我们假设，一旦早期层和中期层处理了表示，这些隐藏状态可能足以仅使用晚期层生成多个令牌，从而消除了反复遍历早期和中期层的需要。我们称这种推理模式为直接多令牌解码（DMTD）。", "result": "基于有限数据集的微调DMTD Qwen3-4B模型已经显示了有希望的结果，实现了高达2倍的速度提升，并且只有很小的性能损失。", "conclusion": "DMTD方法可以在不引入额外参数、辅助程序或后生成验证的情况下提高生成效率。在更大数据集上训练该模型，性能预期会进一步提高。"}}
{"id": "2510.12098", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.12098", "abs": "https://arxiv.org/abs/2510.12098", "authors": ["Jianping Li", "Dongyang Guo", "Wenjie Li", "Wei Zhao"], "title": "An Adaptive Edge-Guided Dual-Network Framework for Fast QR Code Motion Deblurring", "comment": null, "summary": "Unlike general image deblurring that prioritizes perceptual quality, QR code\ndeblurring focuses on ensuring successful decoding. QR codes are characterized\nby highly structured patterns with sharp edges, a robust prior for restoration.\nYet existing deep learning methods rarely exploit these priors explicitly. To\naddress this gap, we propose the Edge-Guided Attention Block (EGAB), which\nembeds explicit edge priors into a Transformer architecture. Based on EGAB, we\ndevelop Edge-Guided Restormer (EG-Restormer), an effective network that\nsignificantly boosts the decoding rate of severely blurred QR codes. For mildly\nblurred inputs, we design the Lightweight and Efficient Network (LENet) for\nfast deblurring. We further integrate these two networks into an Adaptive\nDual-network (ADNet), which dynamically selects the suitable network based on\ninput blur severity, making it ideal for resource-constrained mobile devices.\nExtensive experiments show that our EG-Restormer and ADNet achieve\nstate-of-the-art performance with a competitive speed. Project page:\nhttps://github.com/leejianping/ADNet", "AI": {"tldr": "提出EG-Restormer和ADNet用于QR码去模糊，集成EGAB和LENet，实现实时处理和解码成功率的提升。", "motivation": "现有的深度学习方法很少明确利用QR码特有的边缘结构先验。本研究旨在提高严重模糊的QR码的解码率。", "method": "提出Edge-Guided Attention Block (EGAB), 将明确的边缘先验嵌入Transformer架构中，并基于EGAB构建了Edge-Guided Restormer (EG-Restormer)网络。对于轻微模糊的输入，设计了轻量高效的LENet网络。进一步将这两个网络整合为自适应双网络(ADNet)，根据输入模糊程度动态选择合适的网络。", "result": "EG-Restormer和ADNet在实验中达到了最先进的性能，并且具备竞争力的速度。", "conclusion": "该研究针对QR码的去模糊问题提出了新的网络结构，提高了解码成功率，并适合资源受限的移动设备。"}}
{"id": "2510.11967", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11967", "abs": "https://arxiv.org/abs/2510.11967", "authors": ["Weiwei Sun", "Miao Lu", "Zhan Ling", "Kang Liu", "Xuesong Yao", "Yiming Yang", "Jiecao Chen"], "title": "Scaling Long-Horizon LLM Agent via Context-Folding", "comment": null, "summary": "Large language model (LLM) agents are fundamentally constrained by context\nlength on long-horizon tasks. We introduce Context-Folding, a framework that\nempowers agents to actively manage their working context. An agent can\nprocedurally branch into a sub-trajectory to handle a subtask and then fold it\nupon completion, collapsing the intermediate steps while retaining a concise\nsummary of the outcome. To make this behavior learnable, we develop an\nend-to-end reinforcement learning framework FoldGRPO with specific process\nrewards to encourage effective task decomposition and context management. On\ncomplex long-horizon tasks (Deep Research and SWE), our folding agent matches\nor outperforms the ReAct baselines while using an active context 10$\\times$\nsmaller and significantly outperforms models that rely on summarization-based\ncontext management.", "AI": {"tldr": "提出了Context-Folding框架，通过程序分支和折叠处理长时任务，并开发了FoldGRPO强化学习框架，实现了在更小的活动上下文中匹配或超越基准模型的性能。", "motivation": "大语言模型在处理长时任务时受上下文长度限制，需要一个有效的方式管理上下文。", "method": "Structure", "result": "{\n    \"tldr\": \"提出了Context-Folding框架，通过程序分支和折叠处理长时任务，并开发了FoldGRPO强化学习框架，实现了在更小的活动上下文中匹配或超越基准模型的性能。\",\n    \"motivation\": \"大语言模型在处理长时任务时受上下文长度限制，需要一个有效的方式管理上下文。\",\n    \"method\": \"开发了Context-Folding框架和FoldGRPO强化学习框架，通过分支处理子任务并折叠回去，保留简洁结果摘要。\",\n    \"result\": \"新的折叠代理在复杂长时任务中，在使用10倍小的活动上下文时可以匹配或超越ReAct基准，显著优于依赖总结的上下文管理模型。\",\n    \"conclusion\": \"Context-Folding框架通过高效解除上下文限制，增强了LLM代理处理长时任务的能力。\n}", "conclusion": "Context-Folding框架通过高效解除上下文限制，增强了LLM代理处理长时任务的能力。"}}
