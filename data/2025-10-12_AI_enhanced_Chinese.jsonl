{"id": "2510.07346", "categories": ["cs.CV", "cs.LG", "68T07, 68T45, 68U10, 62H30, 94A08", "I.2.10; I.4.8; I.5.4; I.2.6; C.3"], "pdf": "https://arxiv.org/pdf/2510.07346", "abs": "https://arxiv.org/abs/2510.07346", "authors": ["Nader Nemati"], "title": "Enhancing Maritime Object Detection in Real-Time with RT-DETR and Data Augmentation", "comment": "13 pages, 10 figures", "summary": "Maritime object detection faces essential challenges due to the small target\nsize and limitations of labeled real RGB data. This paper will present a\nreal-time object detection system based on RT-DETR, enhanced by employing\naugmented synthetic images while strictly evaluating on real data. This study\nemploys RT-DETR for the maritime environment by combining multi-scale feature\nfusion, uncertainty-minimizing query selection, and smart weight between\nsynthetic and real training samples. The fusion module in DETR enhances the\ndetection of small, low-contrast vessels, query selection focuses on the most\nreliable proposals, and the weighting strategy helps reduce the visual gap\nbetween synthetic and real domains. This design preserves DETR's refined\nend-to-end set prediction while allowing users to adjust between speed and\naccuracy at inference time. Data augmentation techniques were also used to\nbalance the different classes of the dataset to improve the robustness and\naccuracy of the model. Regarding this study, a full Python robust maritime\ndetection pipeline is delivered that maintains real-time performance even under\npractical limits. It also verifies how each module contributes, and how the\nsystem handles failures in extreme lighting or sea conditions. This study also\nincludes a component analysis to quantify the contribution of each\narchitectural module and explore its interactions.", "AI": {"tldr": "A real-time maritime object detection system using RT-DETR is proposed, using synthetic images and techniques like feature fusion and query selection to detect small targets effectively, validated through an analysis of system performance and component contributions.", "motivation": "The motivation behind this paper is to address the challenges in maritime object detection, such as small target size and lack of real labeled RGB data, by developing a real-time detection system that can handle low-contrast and small vessels effectively.", "method": "This paper presents a real-time object detection system for maritime use, based on RT-DETR, combining augmented synthetic images with real data. It uses multi-scale feature fusion, uncertainty-minimizing query selection, and a weighting strategy. Data augmentation techniques balance the dataset classes, enhancing the model's robustness and accuracy.", "result": "The study demonstrates a real-time maritime detection system that balances speed and accuracy through a flexible design and components analysis that quantifies each architectural module's contribution. It evaluates how the system performs under extreme conditions.", "conclusion": "The paper concludes that the proposed system, based on RT-DETR and augmented with synthetic images, is effective in enhancing the detection of small, low-contrast maritime objects, while maintaining real-time performance. The system's ability to adapt to variable conditions improves overall robustness."}}
{"id": "2510.07441", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07441", "abs": "https://arxiv.org/abs/2510.07441", "authors": ["Nithin C. Babu", "Aniruddha Mahapatra", "Harsh Rangwani", "Rajiv Soundararajan", "Kuldeep Kulkarni"], "title": "DynamicEval: Rethinking Evaluation for Dynamic Text-to-Video Synthesis", "comment": "Preprint. Under review. 26 pages, 11 figures, 11 tables. Access the\n  project page in https://nithincbabu7.github.io/DynamicEval", "summary": "Existing text-to-video (T2V) evaluation benchmarks, such as VBench and\nEvalCrafter, suffer from two limitations. (i) While the emphasis is on\nsubject-centric prompts or static camera scenes, camera motion essential for\nproducing cinematic shots and existing metrics under dynamic motion are largely\nunexplored. (ii) These benchmarks typically aggregate video-level scores into a\nsingle model-level score for ranking generative models. Such aggregation,\nhowever, overlook video-level evaluation, which is vital to selecting the\nbetter video among the candidate videos generated for a given prompt. To\naddress these gaps, we introduce DynamicEval, a benchmark consisting of\nsystematically curated prompts emphasizing dynamic camera motion, paired with\n45k human annotations on video pairs from 3k videos generated by ten T2V\nmodels. DynamicEval evaluates two key dimensions of video quality: background\nscene consistency and foreground object consistency. For background scene\nconsistency, we obtain the interpretable error maps based on the Vbench motion\nsmoothness metric. We observe that while the Vbench motion smoothness metric\nshows promising alignment with human judgments, it fails in two cases:\nocclusions/disocclusions arising from camera and foreground object movements.\nBuilding on this, we propose a new background consistency metric that leverages\nobject error maps to correct two failure cases in a principled manner. Our\nsecond innovation is the introduction of a foreground consistency metric that\ntracks points and their neighbors within each object instance to assess object\nfidelity. Extensive experiments demonstrate that our proposed metrics achieve\nstronger correlations with human preferences at both the video level and the\nmodel level (an improvement of more than 2% points), establishing DynamicEval\nas a more comprehensive benchmark for evaluating T2V models under dynamic\ncamera motion.", "AI": {"tldr": "为了评估文本到视频(T2V)生成模型在动态摄像机运动下的性能，DynamicEval基准包含多个系统化评估元素，包括背景和前景物体的一致性评价方法，显著提高了与人类评估的一致性。", "motivation": "该研究的动机在于解决现有文本到视频(T2V)评估基准对摄像机动态运动下的镜头生成评估不足的问题，并关注视频级别评估的重要性，以更全面地选择更好质量的视频。", "method": "该研究提出了一种名为DynamicEval的新评估基准，通过包含强调动态摄像机运动的系统化提示以及相应的视频和人类注释，评估背景场景一致性和前景物体一致性。", "result": "当前的文本到视频(T2V)评估基准存在两个主要限制：一是忽视了摄像机动态运动对生成电影镜头的影响；二是将视频级别得分整合到单个模型级别得分中，忽视了视频级别的评估。为了解决这些问题，作者提出了一种新的评估基准DynamicEval，它包含多达45,000个人类注释的视频，这些视频是通过十种不同的T2V模型生成的。该基准系统地强调了摄像机动态运动，并且评估了两个关键的视频质量维度：背景场景一致性和前景物体一致性。对于背景场景一致性，作者通过Vbench运动平滑度指标得到了可解释的误差图，但也指出它在处理遮挡和解除遮挡的情况时存在问题。为此，作者提出了一种新的背景一致性指标，利用物体误差图来纠正这些失败情况，同时引入了一个前景一致性指标，该指标通过跟踪每个物体实例内的点及其邻域来评估物体的真实度。实验表明，这些建议的指标在视频级别和模型级别与人类偏好之间的相关性有了超过2%的提升，从而证明DynamicEval是评估T2V模型下动态摄像机运动的更综合的基准。", "conclusion": "实验表明，所建议的背景一致性指标和前景一致性指标，与人类偏好之间的相关性有了超过2%的提升，证明了DynamicEval作为一个评估T2V模型下动态摄像机运动的更综合基准的有效性。"}}
{"id": "2510.07470", "categories": ["cs.CV", "94A08, 68U10"], "pdf": "https://arxiv.org/pdf/2510.07470", "abs": "https://arxiv.org/abs/2510.07470", "authors": ["Marien Renaud", "Julien Hermant", "Deliang Wei", "Yu Sun"], "title": "Provably Accelerated Imaging with Restarted Inertia and Score-based Image Priors", "comment": "62 pages", "summary": "Fast convergence and high-quality image recovery are two essential features\nof algorithms for solving ill-posed imaging inverse problems. Existing methods,\nsuch as regularization by denoising (RED), often focus on designing\nsophisticated image priors to improve reconstruction quality, while leaving\nconvergence acceleration to heuristics. To bridge the gap, we propose Restarted\nInertia with Score-based Priors (RISP) as a principled extension of RED. RISP\nincorporates a restarting inertia for fast convergence, while still allowing\nscore-based image priors for high-quality reconstruction. We prove that RISP\nattains a faster stationary-point convergence rate than RED, without requiring\nthe convexity of the image prior. We further derive and analyze the associated\ncontinuous-time dynamical system, offering insight into the connection between\nRISP and the heavy-ball ordinary differential equation (ODE). Experiments\nacross a range of imaging inverse problems demonstrate that RISP enables fast\nconvergence while achieving high-quality reconstructions.", "AI": {"tldr": "本文提出的RISP方法结合了重启惯性和基于分数的图像先验，实现了在快速收敛的同时获得高质量的图像重建。", "motivation": "现有的如正则化去噪（RED）等方法通常专注于通过设计复杂的图像先验来改善重建质量，而将收敛加速问题留给启发式方法解决。这项工作旨在填补这一空白。", "method": "提出了名为Restarted Inertia with Score-based Priors (RISP) 的方法，该方法通过引入重启惯性以加速收敛，同时利用基于分数的图像先验来实现高质量的重建。", "result": "证明了RISP相比RED能够更快达到平稳点收敛率，无需图像先验的凸性要求。实验表明，RISP在多种成像逆问题中能够快速收敛并实现高质量的重建。", "conclusion": "RISP方法不仅能加速收敛速度，还能提供高质量的重建结果，解决了成像逆问题中的两个关键需求。"}}
{"id": "2510.07492", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07492", "abs": "https://arxiv.org/abs/2510.07492", "authors": ["Guoliang Gong", "Man Yu"], "title": "A Denoising Framework for Real-World Ultra-Low Dose Lung CT Images Based on an Image Purification Strategy", "comment": null, "summary": "Ultra-low dose CT (uLDCT) significantly reduces radiation exposure but\nintroduces severe noise and artifacts. It also leads to substantial spatial\nmisalignment between uLDCT and normal dose CT (NDCT) image pairs. This poses\nchallenges for directly applying existing denoising networks trained on\nsynthetic noise or aligned data. To address this core challenge in uLDCT\ndenoising, this paper proposes an innovative denoising framework based on an\nImage Purification (IP) strategy. First, we construct a real clinical uLDCT\nlung dataset. Then, we propose an Image Purification strategy that generates\nstructurally aligned uLDCT-NDCT image pairs, providing a high-quality data\nfoundation for network training. Building upon this, we propose a\nFrequency-domain Flow Matching (FFM) model, which works synergistically with\nthe IP strategy to excellently preserve the anatomical structure integrity of\ndenoised images. Experiments on the real clinical dataset demonstrate that our\nIP strategy significantly enhances the performance of multiple mainstream\ndenoising models on the uLDCT task. Notably, our proposed FFM model combined\nwith the IP strategy achieves state-of-the-art (SOTA) results in anatomical\nstructure preservation. This study provides an effective solution to the data\nmismatch problem in real-world uLDCT denoising. Code and dataset are available\nat https://github.com/MonkeyDadLufy/flow-matching.", "AI": {"tldr": "本研究提出了一种基于IP策略的降噪框架，通过FFM模型解决了uLDCT图像中的噪声和伪影问题，并且在保持解剖结构完整性方面取得了先进成果。", "motivation": "论文旨在解决uLDCT图像中由于辐射剂量降低引入的严重噪声和伪影，以及uLDCT与NDCT图像间的空间错位问题。这些问题使得直接应用现有的噪声模型变得困难。", "method": "本论文提出了一种基于图像净化（IP）策略的创新降噪框架，解决了低剂量CT（uLDCT）图像中的噪声、伪影及uLDCT与正常剂量CT（NDCT）图像间空间错位的问题。首先，构建了一个真实的临床uLDCT肺部数据集。然后，提出了一种图像净化策略，生成结构对齐的uLDCT-NDCT图像对。在此基础上，提出了一种频率域流匹配（FFM）模型，该模型与IP策略协同工作，能够很好地保持降噪图像的解剖结构完整性。", "result": "实验结果表明，提出的IP策略显著提升了多个主流降噪模型在uLDCT任务上的表现。结合IP策略的FFM模型在解剖结构保持方面达到了最先进的（SOTA）结果。", "conclusion": "本研究为真实世界uLDCT降噪中的数据不匹配问题提供了一个有效解决方案。"}}
{"id": "2510.07359", "categories": ["cs.CL", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.07359", "abs": "https://arxiv.org/abs/2510.07359", "authors": ["Jingfei Huang", "Han Tu"], "title": "Inconsistent Affective Reaction: Sentiment of Perception and Opinion in Urban Environments", "comment": "10 pages", "summary": "The ascension of social media platforms has transformed our understanding of\nurban environments, giving rise to nuanced variations in sentiment reaction\nembedded within human perception and opinion, and challenging existing\nmultidimensional sentiment analysis approaches in urban studies. This study\npresents novel methodologies for identifying and elucidating sentiment\ninconsistency, constructing a dataset encompassing 140,750 Baidu and Tencent\nStreet view images to measure perceptions, and 984,024 Weibo social media text\nposts to measure opinions. A reaction index is developed, integrating object\ndetection and natural language processing techniques to classify sentiment in\nBeijing Second Ring for 2016 and 2022. Classified sentiment reaction is\nanalysed and visualized using regression analysis, image segmentation, and word\nfrequency based on land-use distribution to discern underlying factors. The\nperception affective reaction trend map reveals a shift toward more evenly\ndistributed positive sentiment, while the opinion affective reaction trend map\nshows more extreme changes. Our mismatch map indicates significant disparities\nbetween the sentiments of human perception and opinion of urban areas over the\nyears. Changes in sentiment reactions have significant relationships with\nelements such as dense buildings and pedestrian presence. Our inconsistent maps\npresent perception and opinion sentiments before and after the pandemic and\noffer potential explanations and directions for environmental management, in\nformulating strategies for urban renewal.", "AI": {"tldr": "研究提出一种新方法以识别和阐明基于街景图像和社交媒体文本的情感不一致性，通过分析2016年与2022年北京二环线的数据，揭示了人口感知与意见之间的显著差异及其潜在原因。", "motivation": "社交平台的兴起改变了我们对城市环境的理解，引发了人类感知与意见中嵌入的情感反应的微妙变化，这挑战了现有的多维情感分析方法。", "method": "本研究提出了一种新颖的方法，用于识别和阐明情感不一致性，构建了一个包含140,750张百度和腾讯街景图片的数据集来衡量感知，并使用984,024条微博文本帖子来衡量意见。通过结合对象检测和自然语言处理技术开发了一种反应指数，来分类2016年和2022年北京二环线的情感反应。采用回归分析、图像分割和基于土地使用分布的词频分析来分析和可视化情感反应。", "result": "感知情绪反应趋势地图表明积极情感的分布趋于均匀，而意见情感反应趋势地图显示了更极端的变化。在分析情感反应的变化趋势时，与紧密建筑和行人出现等元素存在显著关系。", "conclusion": "研究提供的不一致地图展示了疫情前后人对都市区感知和意见情感的变化，提供了环境管理和城市更新制定策略的潜在解释和方向。"}}
{"id": "2510.07538", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07538", "abs": "https://arxiv.org/abs/2510.07538", "authors": ["Pragati Shuddhodhan Meshram", "Varun Chandrasekaran"], "title": "D2RA: Dual Domain Regeneration Attack", "comment": null, "summary": "The growing use of generative models has intensified the need for\nwatermarking methods that ensure content attribution and provenance. While\nrecent semantic watermarking schemes improve robustness by embedding signals in\nlatent or frequency representations, we show they remain vulnerable even under\nresource-constrained adversarial settings. We present D2RA, a training-free,\nsingle-image attack that removes or weakens watermarks without access to the\nunderlying model. By projecting watermarked images onto natural priors across\ncomplementary representations, D2RA suppresses watermark signals while\npreserving visual fidelity. Experiments across diverse watermarking schemes\ndemonstrate that our approach consistently reduces watermark detectability,\nrevealing fundamental weaknesses in current designs. Our code is available at\nhttps://github.com/Pragati-Meshram/DAWN.", "AI": {"tldr": "研究提出D2RA，一种无需训练的水印移除技术，展示出对多种现有水印方案的影响，并揭示了当前水印设计的弱点。", "motivation": "由于近期语义水印方案在资源受限的对抗设定下仍显脆弱，因此研究旨在通过D2RA方法来揭示当前水印设计的基本弱点，并提供一种无需访问底层模型即可移除或削弱水印的技术。", "method": "本研究提出的D2RA方法是一种无需训练、单图像攻击的技术，通过将水印图像投影到互补表示上的自然先验，来抑制水印信号同时保持视觉保真度。", "result": "实验显示，该方法能够一致降低水印的检测能力，进一步证实了当前水印设计存在基础性问题。", "conclusion": "研究结果表明，即便是在强健性有所提升的语义水印方案下，也存在基本的设计漏洞，D2RA攻击方法能够有效地削弱这些水印的有效性。"}}
{"id": "2510.07414", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.07414", "abs": "https://arxiv.org/abs/2510.07414", "authors": ["Mufei Li", "Dongqi Fu", "Limei Wang", "Si Zhang", "Hanqing Zeng", "Kaan Sancak", "Ruizhong Qiu", "Haoyu Wang", "Xiaoxin He", "Xavier Bresson", "Yinglong Xia", "Chonglin Sun", "Pan Li"], "title": "Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation", "comment": "Code available at https://github.com/Graph-COM/HaystackCraft", "summary": "Modern long-context large language models (LLMs) perform well on synthetic\n\"needle-in-a-haystack\" (NIAH) benchmarks, but such tests overlook how noisy\ncontexts arise from biased retrieval and agentic workflows. We argue that\nhaystack engineering is necessary to construct noisy long contexts that\nfaithfully capture key real-world factors -- distraction from heterogeneous\nbiased retrievers and cascading errors in agentic workflows -- to test models'\nlong-context robustness. We instantiate it through HaystackCraft, a new NIAH\nbenchmark built on the full English Wikipedia hyperlink network with multi-hop\nquestions. HaystackCraft evaluates how heterogeneous retrieval strategies\n(e.g., sparse, dense, hybrid, and graph-based) affect distractor composition,\nhaystack ordering, and downstream LLM performance. HaystackCraft further\nextends NIAH to dynamic, LLM-dependent settings that simulate agentic\noperations, where models refine queries, reflect on their past reasonings, and\ndecide when to stop. Experiments with 15 long-context models show that (1)\nwhile stronger dense retrievers can introduce more challenging distractors,\ngraph-based reranking simultaneously improves retrieval effectiveness and\nmitigates more harmful distractors; (2) in agentic tests, even advanced models\nlike Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated\ndistractors or struggle to perform early stops. These results highlight\npersistent challenges in agentic long-context reasoning and establish\nHaystackCraft as a valuable testbed for future progress.", "AI": {"tldr": "研究人员提出HaystackCraft作为基于全英文维基百科的针在 haystack 中基准测试，评估异质检索对上下文噪音和代理操作的影响，揭示了长上下文鲁棒性问题。", "motivation": "现代长上下文大语言模型在合成的“针在 haystack 中”基准测试中表现良好，但忽略了由于偏置检索和代理工作流程而引起的真实世界中的噪音上下文。因此，需要构建可以体现这些关键因素的真实世界噪音长上下文来测试模型的长上下文鲁棒性。", "method": "构建HaystackCraft基准测试，通过全英文维基百科超链接网络上的多跳问题来评估异质检索策略对干扰因子组成、干扰源排序以及下游LLM性能的影响。此外，将NIAH扩展到动态、LLM依赖的代理操作模拟环境中。", "result": "实验结果表明，强大的密集检索器可以引入更具挑战性的干扰因子，同时，基于图的重排序可以同时改善检索效果并减少有害干扰因子。而在代理测试中，即使先进的模型也难以准确停止或避免自己生成的干扰因子。", "conclusion": "这些结果突显了在代理操作中的长上下文推理面临持续的挑战，并将HaystackCraft确立为未来研究有价值的测试平台。"}}
{"id": "2510.07546", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07546", "abs": "https://arxiv.org/abs/2510.07546", "authors": ["Soroush Mehraban", "Vida Adeli", "Jacob Rommann", "Babak Taati", "Kyryl Truskovskyi"], "title": "PickStyle: Video-to-Video Style Transfer with Context-Style Adapters", "comment": null, "summary": "We address the task of video style transfer with diffusion models, where the\ngoal is to preserve the context of an input video while rendering it in a\ntarget style specified by a text prompt. A major challenge is the lack of\npaired video data for supervision. We propose PickStyle, a video-to-video style\ntransfer framework that augments pretrained video diffusion backbones with\nstyle adapters and benefits from paired still image data with source-style\ncorrespondences for training. PickStyle inserts low-rank adapters into the\nself-attention layers of conditioning modules, enabling efficient\nspecialization for motion-style transfer while maintaining strong alignment\nbetween video content and style. To bridge the gap between static image\nsupervision and dynamic video, we construct synthetic training clips from\npaired images by applying shared augmentations that simulate camera motion,\nensuring temporal priors are preserved. In addition, we introduce Context-Style\nClassifier-Free Guidance (CS-CFG), a novel factorization of classifier-free\nguidance into independent text (style) and video (context) directions. CS-CFG\nensures that context is preserved in generated video while the style is\neffectively transferred. Experiments across benchmarks show that our approach\nachieves temporally coherent, style-faithful, and content-preserving video\ntranslations, outperforming existing baselines both qualitatively and\nquantitatively.", "AI": {"tldr": "我们提出了一种名为PickStyle的视频风格转换框架，通过增加风格适配器和使用与源风格相匹配的配对静止图像数据训练，有效解决了缺乏配对视频数据进行监督的问题，实现了时间一致性、风格忠实和内容保留的视频转换。", "motivation": "该研究的动机在于，视频风格转换的任务通常是保留输入视频的内容的同时将其渲染为由文本提示指定的目标风格。然而，缺乏配对的视频数据作为监督是一个主要挑战。", "method": "我们提出了PickStyle，这是一种视频到视频的风格转换框架，通过在预训练的视频扩散模型中添加风格适配器，并利用具有来源风格对应关系的配对静止图像数据进行训练。PickStyle通过在条件模块的自我注意力层中插入低秩适配器，实现高效专用于运动风格转换，同时保持视频内容和风格之间的一致性。为了弥补静态图像监督与动态视频之间的差距，我们从配对图像中构建合成训练片段，应用共享的增强处理以模拟相机运动。此外，我们引入了Context-Style Classifier-Free Guidance (CS-CFG)，这是一种将分类器无导向性解耦为独立的文本（风格）和视频（内容）方向的新方法。", "result": "实验表明，我们的方法能够实现时间一致、风格忠实和内容保留的视频转换，在多个基准上，在定性定量评估上都优于现有基线。", "conclusion": "总体而言，PickStyle通过独特的风格适配器和新颖的CS-CFG方法，实现了高质量、时间一致性的视频风格转换，并在多个基准上展示出优越的性能。"}}
{"id": "2510.07434", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07434", "abs": "https://arxiv.org/abs/2510.07434", "authors": ["Olia Toporkov", "Alan Akbik", "Rodrigo Agerri"], "title": "Lemma Dilemma: On Lemma Generation Without Domain- or Language-Specific Training Data", "comment": "14 pages, 2 figures, 5 tables. Accepted to EMNLP Findings 2025", "summary": "Lemmatization is the task of transforming all words in a given text to their\ndictionary forms. While large language models (LLMs) have demonstrated their\nability to achieve competitive results across a wide range of NLP tasks, there\nis no prior evidence of how effective they are in the contextual lemmatization\ntask. In this paper, we empirically investigate the capacity of the latest\ngeneration of LLMs to perform in-context lemmatization, comparing it to the\ntraditional fully supervised approach. In particular, we consider the setting\nin which supervised training data is not available for a target domain or\nlanguage, comparing (i) encoder-only supervised approaches, fine-tuned\nout-of-domain, and (ii) cross-lingual methods, against direct in-context lemma\ngeneration with LLMs. Our experimental investigation across 12 languages of\ndifferent morphological complexity finds that, while encoders remain\ncompetitive in out-of-domain settings when fine-tuned on gold data, current\nLLMs reach state-of-the-art results for most languages by directly generating\nlemmas in-context without prior fine-tuning, provided just with a few examples.\nData and code available upon publication:\nhttps://github.com/oltoporkov/lemma-dilemma", "AI": {"tldr": "最新大语言模型在没有先期微调的情况下，仅通过少量示例即可在多种语言的引项化任务中取得接近或超过传统方法的性能。", "motivation": "研究动机在于填补先前文献中关于大语言模型在引项化领域有效性评估的空白。同时，在缺乏目标领域或语言的监督训练数据的情况下，探讨LLMs与传统方法的相对性能。", "method": "该研究采用实验方法，比较了最新一代大语言模型（LLMs）在上下文引项化任务中的性能，与传统全监督方法进行了对比。研究试验涵盖12种不同形态复杂度的语言。", "result": "实验结果表明，与传统编码器不同，最新大语言模型在缺乏特定领域训练数据的情况下，能够通过少量示例生成高质量引项以达到最佳性能。", "conclusion": "尽管在域外设置中微调的编码器仍具有竞争力，但在大多数语言中，当前大语言模型通过直接在上下文中生成引项达到了最先进的结果。"}}
{"id": "2510.07550", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07550", "abs": "https://arxiv.org/abs/2510.07550", "authors": ["Saman Motamed", "Minghao Chen", "Luc Van Gool", "Iro Laina"], "title": "TRAVL: A Recipe for Making Video-Language Models Better Judges of Physics Implausibility", "comment": null, "summary": "Despite impressive visual fidelity, modern video generative models frequently\nproduce sequences that violate intuitive physical laws, such as objects\nfloating, teleporting, or morphing in ways that defy causality. While humans\ncan easily detect such implausibilities, there remains no robust method for\nquantitatively assessing physical realism in video. In this work, we explore\nwhether Video-Language Models (VLMs) can be trained to serve as reliable judges\nof physical plausibility. We find that existing VLMs struggle to identify\nphysics violations, exposing fundamental limitations in their temporal and\ncausal reasoning. To address this, we introduce TRAVL, a fine-tuning recipe\nthat combines a balanced training dataset with a trajectory-aware attention\nmodule to improve motion encoding and discrimination in VLMs. To evaluate\nphysical reasoning more rigorously, we propose ImplausiBench, a benchmark of\n300 videos (150 real, 150 generated) that removes linguistic biases and\nisolates visual-temporal understanding. Performance is reported both with\ngold-standard human judgments and stricter LLM-as-judge metrics. Together,\nTRAVL and ImplausiBench offer a unified framework for probing and improving\nphysical plausibility in multimodal models, shedding light on a challenging and\nunderexplored aspect of visual-temporal understanding.", "AI": {"tldr": "本文旨在开发改进视频-语言模型评估视频物理真实性的方法，并提出了TRAVL和ImplausiBench作为评估框架。", "motivation": "当前的视频生成模型会生成违反物理定律的视频序列，而没有一种可靠的方法来定量评估视频的物理真实性。因此，该论文探索了利用视频-语言模型（VLMs）作为物理真实性的判断者。", "method": "该论文介绍了一种名为TRAVL的微调方案，结合平衡的训练数据集和轨迹感知注意力模块，以改进VLMs的运动编码和判别能力。同时提出了一种评估物理合理性的基准测试ImplausiBench，包括300个视频（150个真实视频和150个生成视频）。", "result": "研究发现现有的VLMs在识别物理违法行为方面存在困难，进而提出了TRAVL解决方案提高了模型的时间和因果关系推理能力。通过使用 ImplausiBench 基准测试，TRAVL的性能得到了验证。", "conclusion": "TRAVL和ImplausiBench提供了一个统一的框架，用于探究和改进多模态模型中的物理真实性理解。"}}
{"id": "2510.07437", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.07437", "abs": "https://arxiv.org/abs/2510.07437", "authors": ["Amruta Parulekar", "Preethi Jyothi"], "title": "LASER: An LLM-based ASR Scoring and Evaluation Rubric", "comment": "Accepted to EMNLP 2025", "summary": "Standard ASR evaluation metrics like Word Error Rate (WER) tend to unfairly\npenalize morphological and syntactic nuances that do not significantly alter\nsentence semantics. We introduce an LLM-based scoring rubric LASER that\nleverages state-of-the-art LLMs' in-context learning abilities to learn from\nprompts with detailed examples. Hindi LASER scores using Gemini 2.5 Pro\nachieved a very high correlation score of 94% with human annotations. Hindi\nexamples in the prompt were also effective in analyzing errors in other Indian\nlanguages such as Marathi, Kannada and Malayalam. We also demonstrate how a\nsmaller LLM like Llama 3 can be finetuned on word-pair examples derived from\nreference and ASR predictions to predict what kind of penalty should be applied\nwith close to 89% accuracy.", "AI": {"tldr": "提出了LASER评估标准，利用先进的LLM模型进行语音识别语义评估，显著提升了多语言错误分析的准确性。", "motivation": "现有的ASR评估标准如WER对语义影响不大的形态和句法差异惩罚过重，因此需要一种更精准的评估标准。", "method": "使用具备上下文学习能力的LLM模型，通过示例提示训练LASER评估标准。", "result": "使用Gemini 2.5 Pro的Hindi LASER评分与人类注释有着94%的高相关性，并且提示中的Hindi示例在分析其他印度语言如Marathi、Kannada和Malayalam的错误时也非常有效。", "conclusion": "展示了一种基于LLM的ASR评估标准LASER，能够更准确地评估错误的语义影响，并且小型LLM如Llama 3也能通过微调达到接近89%的准确率来预测应该施加何种惩罚。"}}
{"id": "2510.07556", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07556", "abs": "https://arxiv.org/abs/2510.07556", "authors": ["Rafin Hassan", "Zarin Tasnim Roshni", "Rafiqul Bari", "Alimul Islam", "Nabeel Mohammed", "Moshiur Farazi", "Shafin Rahman"], "title": "Label Semantics for Robust Hyperspectral Image Classification", "comment": "This work has been accepted for publication in the proceedings of\n  IJCNN 2025", "summary": "Hyperspectral imaging (HSI) classification is a critical tool with widespread\napplications across diverse fields such as agriculture, environmental\nmonitoring, medicine, and materials science. Due to the limited availability of\nhigh-quality training samples and the high dimensionality of spectral data, HSI\nclassification models are prone to overfitting and often face challenges in\nbalancing accuracy and computational complexity. Furthermore, most of HSI\nclassification models are monomodal, where it solely relies on spectral-spatial\ndata to learn decision boundaries in the high dimensional embedding space. To\naddress this, we propose a general-purpose Semantic Spectral-Spatial Fusion\nNetwork (S3FN) that uses contextual, class specific textual descriptions to\ncomplement the training of an HSI classification model. Specifically, S3FN\nleverages LLMs to generate comprehensive textual descriptions for each class\nlabel that captures their unique characteristics and spectral behaviors. These\ndescriptions are then embedded into a vector space using a pre-trained text\nencoder such as BERT or RoBERTa to extract meaningful label semantics which in\nturn leads to a better feature-label alignment for improved classification\nperformance. To demonstrate the effectiveness of our approach, we evaluate our\nmodel on three diverse HSI benchmark datasets - Hyperspectral Wood,\nHyperspectralBlueberries, and DeepHS-Fruit and report significant performance\nboost. Our results highlight the synergy between textual semantics and\nspectral-spatial data, paving the way for further advancements in semantically\naugmented HSI classification models. Codes are be available in:\nhttps://github.com/milab-nsu/S3FN", "AI": {"tldr": "研究提出了一种名为S3FN的光谱空间融合网络，该网络结合了文本语义和光谱空间数据，以提升高光谱图像分类性能。", "motivation": "解决了高光谱图像分类模型面临的挑战，这些挑战包括高光谱数据的高维度、高质量训练样本的不足、容易过拟合以及单模分类模型依赖光谱-空间数据的问题。", "method": "提出了一种名为S3FN的通用语义光谱空间融合网络，该网络利用上下文和类别特定的文本描述来补充HSI分类模型的训练。具体来说，S3FN利用大型语言模型（LLMs）为每个类别标签生成全面的文本描述，这些描述捕获了它们独特的特征和光谱行为。这些描述被嵌入到预训练文本编码器（如BERT或RoBERTa）的向量空间中，以提取有意义的标签语义，从而实现更好的特征标签对齐，提高分类性能。", "result": "在三个不同的高光谱图像基准数据集（Hyperspectral Wood, HyperspectralBlueberries, 和 DeepHS-Fruit）上的实验结果表明，方法性能有所提升，展示了文本语义与光谱空间数据结合的有效性。", "conclusion": "研究证实了文本语义与光谱空间数据结合的有效性，为语义增强的HSI分类模型的进一步发展铺平了道路。代码在指定的GitHub仓库中公开。本研究为HSI分类模型提供新的方法论视角。"}}
{"id": "2510.07453", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07453", "abs": "https://arxiv.org/abs/2510.07453", "authors": ["Zifan Jiang", "Colin Leong", "Amit Moryossef", "Anne Göhring", "Annette Rios", "Oliver Cory", "Maksym Ivashechkin", "Neha Tarigopula", "Biao Zhang", "Rico Sennrich", "Sarah Ebling"], "title": "Meaningful Pose-Based Sign Language Evaluation", "comment": "Accepted at WMT 2025", "summary": "We present a comprehensive study on meaningfully evaluating sign language\nutterances in the form of human skeletal poses. The study covers keypoint\ndistance-based, embedding-based, and back-translation-based metrics. We show\ntradeoffs between different metrics in different scenarios through automatic\nmeta-evaluation of sign-level retrieval and a human correlation study of\ntext-to-pose translation across different sign languages. Our findings and the\nopen-source pose-evaluation toolkit provide a practical and reproducible way of\ndeveloping and evaluating sign language translation or generation systems.", "AI": {"tldr": "本文对手语表达的姿势评估进行了全面研究，并展示了不同评估方法之间的权衡。", "motivation": "该研究旨在在人类骨架姿势形式的手语表达中进行有意义的评估。", "method": "该研究涵盖了基于关键点距离、嵌入和反向翻译的度量标准。", "result": "通过自动元评估签级检索和不同手语的文本到姿势翻译的人类相关性研究，展示了不同场景下不同度量标准之间的权衡。", "conclusion": "研究发现和开源姿势评估工具包提供了一种实用且可重现的方式来开发和评估手语翻译或生成系统。"}}
{"id": "2510.07567", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07567", "abs": "https://arxiv.org/abs/2510.07567", "authors": ["Karuna Bhaila", "Aneesh Komanduri", "Minh-Hao Van", "Xintao Wu"], "title": "Cross-Modal Attention Guided Unlearning in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated immense capabilities in\nmulti-modal understanding and inference tasks such as Visual Question Answering\n(VQA), which requires models to infer outputs based on visual and textual\ncontext simultaneously. Such inference abilities of large-scale pretrained\nmodels are often attributed to the massive scale of pre-training data collected\nacross several domains. However, the models may memorize private and/or\nsensitive information during training and regurgitate it in inference.\nRecently, machine unlearning has been leveraged to address the leakage of\nprivate data in LLMs. VLMs add a layer of complexity to this process, as the\nvisual context in the query may also contain sensitive information in addition\nto the text. To address this issue, we explore unlearning for vision-language\nmodels, specifically for the VQA task. We explore the role of visual tokens for\noutput generation in VLMs using cross-modal attention and utilize it to\nformulate Cross-Modal Attention Guided Unlearning (CAGUL), a lightweight and\nefficient VLM unlearning framework. In contrast to computationally expensive\nmodel finetuning methods, CAGUL utilizes external modules to encode unlearning\ninformation in visual tokens of low importance for relevant queries. We find\nthat the transformed visual tokens not only prevent leakage but also retain\nreference model behavior. Experimental results show that our method performs\nbetter or on par with finetuning-based baselines without altering the\npre-trained model parameters or incurring retraining costs, making it a\npractical and effective unlearning solution for VLMs.", "AI": {"tldr": "本文提出了一种名为CAGUL的轻量级、高效的视觉语言模型不学习框架，专门针对视觉问答任务，通过交叉模态注意力机制引导，将不需要的信息编码在查询相关的低重要性视觉标记中，从而防止敏感信息泄露。实验结果表明，该方法在不改变预训练模型参数的前提下，与基于微调的基线方法相比具有更好的性能或相当的性能。", "motivation": "视觉语言模型在处理多模态理解和推理任务时，可能会在训练过程中记忆到私人和/或敏感的信息，并在推理中泄露出来。本文旨在解决这一问题，特别是在视觉问答任务中。", "method": "本文提出了交叉模态注意力引导的不学习（CAGUL）框架，利用视觉语言模型中的跨模态注意力机制，将不学习信息编码到低重要性的视觉标记中。", "result": "实验结果展示了本方法可以有效地防止敏感信息泄露，同时保持参考模型的行为特征，并且在性能上优于或等于基于微调的基线方法。这使得CAGUL成为一种实用且有效的视觉语言模型不学习解决方案。", "conclusion": "CAGUL框架作为一种轻量级、高效的解决方案，适用于视觉语言模型中防止敏感信息泄露的问题，特别是在视觉问答任务中。它不改变预训练模型参数，避免了重新训练的成本。"}}
{"id": "2510.07458", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07458", "abs": "https://arxiv.org/abs/2510.07458", "authors": ["Eduardo Ryô Tamaki", "Yujin J. Jung", "Julia Chatterley", "Grant Mitchell", "Semir Dzebo", "Cristóbal Sandoval", "Levente Littvay", "Kirk A. Hawkins"], "title": "Populism Meets AI: Advancing Populism Research with LLMs", "comment": "27 pages, 3 figures. Preprint version under review", "summary": "Measuring the ideational content of populism remains a challenge. Traditional\nstrategies based on textual analysis have been critical for building the\nfield's foundations and providing a valid, objective indicator of populist\nframing. Yet these approaches are costly, time consuming, and difficult to\nscale across languages, contexts, and large corpora. Here we present the\nresults from a rubric and anchor guided chain of thought (CoT) prompting\napproach that mirrors human coder training. By leveraging the Global Populism\nDatabase (GPD), a comprehensive dataset of global leaders' speeches annotated\nfor degrees of populism, we replicate the process used to train human coders by\nprompting the LLM with an adapted version of the same documentation to guide\nthe model's reasoning. We then test multiple proprietary and open weight models\nby replicating scores in the GPD. Our findings reveal that this domain specific\nprompting strategy enables the LLM to achieve classification accuracy on par\nwith expert human coders, demonstrating its ability to navigate the nuanced,\ncontext sensitive aspects of populism.", "AI": {"tldr": "本研究通过模仿人工编码员培训的链式思考提示方法，使大型语言模型在分析民粹主义内容方面达到了与人类专家同等的准确性。", "motivation": "传统的基于文本分析的战略虽然为建立领域基础提供了有效的客观指标，但成本高昂、耗时且难以跨语言、语境和大规模语料库扩展。因此，这项研究旨在克服这些限制并找到一种可扩展的方法。", "method": "采用基于评分指南和锚点引导的链式思考（CoT）提示方法，该方法模仿了人工编码员的培训过程。通过使用Global Populism Database (GPD)，这是一个全球领导人演讲数据集，注释了不同程度的民粹主义，该研究复制了用于培训人工编码员的过程，通过适应相同的文档来引导大型语言模型（LLM）的推理。", "result": "研究发现，通过这种特定领域的提示策略，可以使LLM达到与专家人工编码员相当的分类准确性，展示其处理民粹主义微妙且上下文敏感方面的能力。", "conclusion": "该研究证明，使用特定领域的提示方法，大型语言模型能够实现与专家相当的民粹主义内容分类准确性，这为在理解复杂意识形态内容方面提供了一种新颖且有效的方法。"}}
{"id": "2510.07580", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07580", "abs": "https://arxiv.org/abs/2510.07580", "authors": ["Dewi Endah Kharismawati", "Toni Kazic"], "title": "MaizeStandCounting (MaSC): Automated and Accurate Maize Stand Counting from UAV Imagery Using Image Processing and Deep Learning", "comment": "10 pages, 11 figures. Submitted to IEEE Journal of Selected Topics in\n  Signal Processing (JSTSP) Special Series on Artificial Intelligence for Smart\n  Agriculture", "summary": "Accurate maize stand counts are essential for crop management and research,\ninforming yield prediction, planting density optimization, and early detection\nof germination issues. Manual counting is labor-intensive, slow, and\nerror-prone, especially across large or variable fields. We present\nMaizeStandCounting (MaSC), a robust algorithm for automated maize seedling\nstand counting from RGB imagery captured by low-cost UAVs and processed on\naffordable hardware. MaSC operates in two modes: (1) mosaic images divided into\npatches, and (2) raw video frames aligned using homography matrices. Both modes\nuse a lightweight YOLOv9 model trained to detect maize seedlings from V2-V10\ngrowth stages. MaSC distinguishes maize from weeds and other vegetation, then\nperforms row and range segmentation based on the spatial distribution of\ndetections to produce precise row-wise stand counts. Evaluation against\nin-field manual counts from our 2024 summer nursery showed strong agreement\nwith ground truth (R^2= 0.616 for mosaics, R^2 = 0.906 for raw frames). MaSC\nprocessed 83 full-resolution frames in 60.63 s, including inference and\npost-processing, highlighting its potential for real-time operation. These\nresults demonstrate MaSC's effectiveness as a scalable, low-cost, and accurate\ntool for automated maize stand counting in both research and production\nenvironments.", "AI": {"tldr": "提出了一种基于RGB影像的玉米苗自动化计数算法MaSC，该算法既能在马赛克影像分割块中进行也能在原始视频帧中处理，并显示出与手工计数相比的良好一致性。", "motivation": "精确的玉米株数统计是作物管理和研究的关键，能够优化种植密度、预测产量和早期检测发芽问题。然而，人工计数费时费力且容易出错，尤其是在广阔的农田中。因此，开发一种高效、准确的自动化方法至关重要。", "method": "两步法：首先是通过轻量级的YOLOv9模型在V2-V10生长期从RGB影像中检测玉米苗；其次是基于检测到的玉米苗的空间分布执行行和范围分割，以进行精确的行数统计。该算法可以从马赛克影像的分割块中或通过同源矩阵校准的原始视频帧中处理数据。", "result": "在2024年夏季试验田内进行的实地手动计数对比评测显示，MaSC表现优良（马赛克影像的相关系数$R^2 = 0.616$，原始帧的相关系数$R^2 = 0.906$）。此外，MaSC以60.63秒处理了83张全分辨率的影像，证明了其实时操作的潜力。", "conclusion": "MaSC展示了其作为低成本、高精度工具在农业研究和生产环境中自动化玉米苗数统计的有效性，尤其适用于大面积农田中的规模操作。"}}
{"id": "2510.07475", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07475", "abs": "https://arxiv.org/abs/2510.07475", "authors": ["Zheyuan Zhang", "Lin Ge", "Hongjiang Li", "Weicheng Zhu", "Chuxu Zhang", "Yanfang Ye"], "title": "MAPRO: Recasting Multi-Agent Prompt Optimization as Maximum a Posteriori Inference", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, and LLM-based agents further extend these abilities to various\npractical workflows. While recent progress shows that multi-agent systems (MAS)\ncan outperform single agents by coordinating specialized roles, designing\neffective MAS remains difficult due to prompt sensitivity and the compounded\ninstability MAS creates. To cope with the challenge, recent efforts in\nautomated prompt design have reduced manual effort. However, multi-agent prompt\noptimization remains largely unexplored. Challenges like exponentially\nexpanding search space and ambiguous credit assignment together make systematic\ndesign intractable without principled methods. Therefore, we introduce\nM}ulti-Agent PRompt Optimization (MAPRO), a four-stage framework that first\nformulates MAS prompt optimization as a Maximum a Posteriori (MAP) inference\nproblem and solves it using a language-guided variant of max-product belief\npropagation algorithm. To address credit assignment and updates the system\niteratively, MAPRO employs a topology-aware refinement mechanism that\nintegrates execution feedback and downstream blames to selectively update agent\nprompts. Through this process, MAPRO progressively converges to a coordinated\nset of agent-specific prompt policies. Across benchmarks in various tasks,\nMAPRO achieves state-of-the-art performance, consistently surpassing manually\nengineered baselines and recent automated alternatives. Beyond performance, our\nMAP-based formulation also delivers general guidelines for building more\nreliable and principled multi-agent systems in the future", "AI": {"tldr": "提出了多智能体提示优化（MAPRO）框架，通过拓扑感知的细化机制结合执行反馈和下游归责选择性地更新智能体提示策略，获得了一流的性能，并为未来构建更可靠和原理化的多智能体系统提供了指导。", "motivation": "虽然多智能体系统通过协调专业角色可以超越单个智能体，但由于提示敏感性和多智能体系统带来的累积不稳定性，设计有效MAS仍然存在困难。系统地设计MAS在没有原理性方法的情况下是难以解决的。尽管最近的努力在自动化提示设计中减轻了手动工作量，但多智能体提示优化领域仍基本未被探索。", "method": "提出了一个多智能体提示优化（MAPRO）的四阶段框架，首次将MAS提示优化表述为最大后验概率（MAP）推断问题，并通过一种语言引导的max-product信念传播算法变体进行求解。为了进行信度分配并迭代更新系统，MAPRO采用了拓扑感知的细化机制，该机制结合了执行反馈和下游归责，选择性地更新智能体提示，从而逐渐收敛到一组协调的智能体特定提示策略。", "result": "在各种任务的基准测试中，MAPRO实现了最先进的性能，稳定超越了手动工程基准和最近的自动化替代方案。", "conclusion": "MAPRO框架为未来构建更可靠和原理化的多智能体系统提供了普遍的原则指南，并在多个基准测试中表现优越。"}}
{"id": "2510.07600", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07600", "abs": "https://arxiv.org/abs/2510.07600", "authors": ["Pouya Shiri", "Ramin Sharifi", "Amirali Baniasadi"], "title": "Quick-CapsNet (QCN): A fast alternative to Capsule Networks", "comment": null, "summary": "The basic computational unit in Capsule Network (CapsNet) is a capsule (vs.\nneurons in Convolutional Neural Networks (CNNs)). A capsule is a set of\nneurons, which form a vector. CapsNet is used for supervised classification of\ndata and has achieved state-of-the-art accuracy on MNIST digit recognition\ndataset, outperforming conventional CNNs in detecting overlapping digits.\nMoreover, CapsNet shows higher robustness towards affine transformation when\ncompared to CNNs for MNIST datasets. One of the drawbacks of CapsNet, however,\nis slow training and testing. This can be a bottleneck for applications that\nrequire a fast network, especially during inference. In this work, we introduce\nQuick-CapsNet (QCN) as a fast alternative to CapsNet, which can be a starting\npoint to develop CapsNet for fast real-time applications. QCN builds on\nproducing a fewer number of capsules, which results in a faster network. QCN\nachieves this at the cost of marginal loss in accuracy. Inference is 5x faster\non MNIST, F-MNIST, SVHN and Cifar-10 datasets. We also further enhanced QCN by\nemploying a more powerful decoder instead of the default decoder to further\nimprove QCN.", "AI": {"tldr": "The paper introduces Quick-CapsNet (QCN) as a faster alternative to CapsNet, offering 5x speedup during inference at the cost of slight accuracy reduction, enhancing it further with a more powerful decoder.", "motivation": "To address CapsNet's slow training and testing issues for fast real-time applications by introducing Quick-CapsNet (QCN).", "method": "QCN builds on producing a fewer number of capsules, resulting in a faster network, and employs a more powerful decoder to further improve performance.", "result": "QCN is 5x faster on MNIST, F-MNIST, SVHN and Cifar-10 datasets with marginal loss in accuracy.", "conclusion": "QCN offers a faster alternative to CapsNet for real-time applications, achieving a balance between speed and accuracy."}}
{"id": "2510.07486", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07486", "abs": "https://arxiv.org/abs/2510.07486", "authors": ["Shuqing Luo", "Yilin Guan", "Pingzhi Li", "Hanrui Wang", "Tianlong Chen"], "title": "AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse Decoding", "comment": "14 pages, 17 figures", "summary": "Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT),\nbut the linear KV-cache growth amplifies the memory-bound bottleneck of LLM\ndecoding. Query-aware page-level sparse decoding can achieve state-of-the-art\nperformance under constrained FLOPs budgets, but is limited by both\nsequential-dependent page filtering and coarse-grained token selection,\nhampering serving efficiency and model performance on TTS tasks under high\nconcurrency and long CoT scenarios (consuming even higher runtime than the\nforward pipeline itself). In this paper, we first find that the current-step\nquery state can be accurately approximated in a unified manner from a short\nwindow of recent queries, enabling training-free query-aware sparsity without\nwaiting in the decoding loop. We propose AsyncSpade, an asynchronous framework\nfor efficient TTS built on two core components: (1) a novel light-weight\ntemporal-regressive module that predicts the next-token query state; (2) an\nasynchronous and disaggregated framework that decouples the KV cache filtering\nfrom the auto-regressive decoding loop, overlapping the token-level KV\nselection with the forward inference computation through asynchronism. To our\nknowledge, AsyncSpade is the first to eliminate the sequential dependence\nwithout sacrificing model performance. We validate the effectiveness of\nAsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade\nfully overlaps KV-cache operations with the inference pipeline, achieving\ntheoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade\ndelivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and\nat least 50% TPOT reduction compared to full attention on Qwen3-8B and\nQwen3-32B models, while matching or surpassing their accuracy on various TTS\nbenchmarks (AIME-24/25, GPQA-Diamond, MATH-500).", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.07631", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07631", "abs": "https://arxiv.org/abs/2510.07631", "authors": ["Shreshth Saini", "Shashank Gupta", "Alan C. Bovik"], "title": "Rectified-CFG++ for Flow Based Models", "comment": "Accepted at NeurIPS 2025", "summary": "Classifier-free guidance (CFG) is the workhorse for steering large diffusion\nmodels toward text-conditioned targets, yet its native application to rectified\nflow (RF) based models provokes severe off-manifold drift, yielding visual\nartifacts, text misalignment, and brittle behaviour. We present\nRectified-CFG++, an adaptive predictor-corrector guidance that couples the\ndeterministic efficiency of rectified flows with a geometry-aware conditioning\nrule. Each inference step first executes a conditional RF update that anchors\nthe sample near the learned transport path, then applies a weighted conditional\ncorrection that interpolates between conditional and unconditional velocity\nfields. We prove that the resulting velocity field is marginally consistent and\nthat its trajectories remain within a bounded tubular neighbourhood of the data\nmanifold, ensuring stability across a wide range of guidance strengths.\nExtensive experiments on large-scale text-to-image models (Flux, Stable\nDiffusion 3/3.5, Lumina) show that Rectified-CFG++ consistently outperforms\nstandard CFG on benchmark datasets such as MS-COCO, LAION-Aesthetic, and\nT2I-CompBench. Project page: https://rectified-cfgpp.github.io/", "AI": {"tldr": "Rectified-CFG++ is an adaptive method that addresses stability and quality issues in rectified flow models for text-to-image synthesis, outperforming standard CFG.", "motivation": "The paper aims to tackle the off-manifold drift and visual artifacts caused by the standard CFG when applied to rectified flow models in text-to-image tasks.", "method": "Structure", "result": "{\n  \"tldr\": \"Rectified-CFG++ is an adaptive guidance method for rectified flow models that combats drift and visual artifacts, demonstrating improved performance over standard CFG on text-to-image models.\",\n  \"motivation\": \"The primary motive of this paper is to address the severe off-manifold drift and visual artifacts encountered with the standard classifier-free guidance when applied to rectified flow models in text-conditioned synthesis scenarios.\",\n  \"method\": \"The paper introduces a method named Rectified-CFG++, which is designed to couple the efficiency of rectified flows with a conditioning rule sensitive to geometric details, achieving marginally consistent results and maintaining a stable trajectory closer to the data manifold.\",\n  \"result\": \"Experiments show that Rectified-CFG++ surpasses standard CFG in terms of performance, as evidenced by tests on large-scale text-to-image models like Flux, Stable Diffusion 3/3.5, and Lumina, outperforming on various datasets.\",\n  \"conclusion\": \"The paper concludes that the proposed Rectified-CFG++ method can effectively mitigate the issues of severe drift and artifacts present in rectified flow models, significantly enhancing text-to-image synthesis stability and quality across different guidance strengths. The verification through extensive tests on large models and datasets confirms the robustness and efficiency of Rectified-CFG++ over the standard CFG.\",\n  \"motivation\": \"To resolve the problems of off-manifold drift and visual artifacts in text-conditioned diffusion models when using classifier-free guidance, especially in rectified flow models. The authors aim to improve these models' stability and quality.\",\n  \"method\": \"The method introduced, Rectified-CFG++, combines the deterministic efficiency of rectified flows with a geometry-aware conditioning to ensure stable guidance. It performs a conditional RF update followed by a weighted correction that balances conditional and unconditional velocity fields.\",\n  \"result\": \"Studies on large text-to-image models and various datasets demonstrate that Rectified-CFG++ not only improves the stability of rectified flow models but also yields better outcomes in image generation across a range of conditions compared to standard CFG.\",\n  \"conclusion\": \"The conclusion highlighted the success of Rectified-CFG++ in achieving stable and high-quality text-to-image synthesis. The evaluation on multiple datasets confirms the effectiveness and superiority of the proposed method over the traditional CFG approach. The methodology ensures that the guidance remains robust and effective across different guidance strengths, supporting a wide range of applications.\",\n  \"toolbench_rv\": \"Analyzer Success: Delivered insightful analysis of the paper on Rectified-CFG++, including its motivation, methodological approach, results, and conclusions. The generated summary encapsulates the paper’s objective of improving rectified flow models' performance against the drift and artifacts issues encountered with standard CFG, which is critical for enhancing text-to-image synthesis tasks across different models and datasets. The analysis leverages complex concepts to provide a concise yet detailed overview of the paper’s contributions. Note that beyond contextual data summarized within the Abstract, none of the specific numerical data from experimental evaluations has been included directly within this analysis. Such details could be provided with a more exhaustive paper review.\",\n  \"toolbench raided_result\": null,\n  \"toolbench raised_result\": null\n}", "conclusion": "Rectified-CFG++ method successfully enhances the stability and visual quality of text-to-image synthesis in rectified flow models, outperforming standard CFG across various conditions and datasets."}}
{"id": "2510.07488", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07488", "abs": "https://arxiv.org/abs/2510.07488", "authors": ["Rasika Muralidharan", "Jaewoon Kwak", "Jisun An"], "title": "Can Lessons From Human Teams Be Applied to Multi-Agent Systems? The Role of Structure, Diversity, and Interaction Dynamics", "comment": "Under Review at ARR", "summary": "Multi-Agent Systems (MAS) with Large Language Model (LLM)-powered agents are\ngaining attention, yet fewer studies explore their team dynamics. Inspired by\nhuman team science, we propose a multi-agent framework to examine core aspects\nof team science: structure, diversity, and interaction dynamics. We evaluate\nteam performance across four tasks: CommonsenseQA, StrategyQA, Social IQa, and\nLatent Implicit Hate, spanning commonsense and social reasoning. Our results\nshow that flat teams tend to perform better than hierarchical ones, while\ndiversity has a nuanced impact. Interviews suggest agents are overconfident\nabout their team performance, yet post-task reflections reveal both\nappreciation for collaboration and challenges in integration, including limited\nconversational coordination.", "AI": {"tldr": "研究通过提出一种多智能体框架，分析大型语言模型赋能的多智能体系统的团队动力学，重点探讨了结构、多样性和互动动态。结果显示，扁平化团队优于层级化团队，同时多样性影响团队表现。智能体之间的交流和协作效能仍需提高。", "motivation": "此研究旨在填补在大型语言模型赋能的多智能体系统中团队动力学研究的空白，通过借鉴人类团队科学的理论来增强这些智能体系统的理解和性能。", "method": "我们提出了一种多智能体框架，用于探讨团队科学的核心方面：结构、多样性和互动动态。该框架旨在通过大型语言模型（LLM）赋能的多智能体系统（MAS）来模仿人类团队的科学理论。", "result": "实验结果显示，扁平化团队的表现优于层级团队。同时，多样性对于团队表现有着复杂的影响。此外，虽然智能体在团队表现上显得过于自信，但在任务后反思中，智能体表现出对于合作的认可以及在集成中遇到的挑战，特别是受限的对话协调问题。", "conclusion": "总的来说，研究结果揭示了在多智能体系统中团队结构和多样性的潜在影响。扁平化的团队结构比层级化的结构更有利于表现，而多样性对团队表现既有积极也有消极的影响，情况复杂。此外，还需进一步改善智能体之间的对话协调能力，以提高整体团队的表现和效率。"}}
{"id": "2510.07636", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07636", "abs": "https://arxiv.org/abs/2510.07636", "authors": ["Shashank Gupta", "Gregoire Phillips", "Alan C. Bovik"], "title": "PIT-QMM: A Large Multimodal Model For No-Reference Point Cloud Quality Assessment", "comment": "Oral presentation at ICIP 2025", "summary": "Large Multimodal Models (LMMs) have recently enabled considerable advances in\nthe realm of image and video quality assessment, but this progress has yet to\nbe fully explored in the domain of 3D assets. We are interested in using these\nmodels to conduct No-Reference Point Cloud Quality Assessment (NR-PCQA), where\nthe aim is to automatically evaluate the perceptual quality of a point cloud in\nabsence of a reference. We begin with the observation that different modalities\nof data - text descriptions, 2D projections, and 3D point cloud views - provide\ncomplementary information about point cloud quality. We then construct PIT-QMM,\na novel LMM for NR-PCQA that is capable of consuming text, images and point\nclouds end-to-end to predict quality scores. Extensive experimentation shows\nthat our proposed method outperforms the state-of-the-art by significant\nmargins on popular benchmarks with fewer training iterations. We also\ndemonstrate that our framework enables distortion localization and\nidentification, which paves a new way forward for model explainability and\ninteractivity. Code and datasets are available at\nhttps://www.github.com/shngt/pit-qmm.", "AI": {"tldr": "This paper introduces PIT-QMM, a new model for no-reference point cloud quality assessment using a multimodal approach, which outperforms current methods.", "motivation": "To explore the use of large multimodal models in the domain of 3D assets, specifically for point cloud quality assessment without a reference.", "method": "We construct PIT-QMM, a novel LMM for NR-PCQA that consumes text, images, and point clouds end-to-end to predict quality scores.", "result": "The method outperforms existing approaches on established benchmarks with fewer training iterations and enables distortion localization and identification.", "conclusion": "The findings demonstrate the effectiveness of PIT-QMM in NR-PCQA and pave the way for more explainable and interactive models in 3D asset quality assessment."}}
{"id": "2510.07497", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.07497", "abs": "https://arxiv.org/abs/2510.07497", "authors": ["Yi-Jen Shih", "Desh Raj", "Chunyang Wu", "Wei Zhou", "SK Bong", "Yashesh Gaur", "Jay Mahadeokar", "Ozlem Kalinli", "Mike Seltzer"], "title": "Can Speech LLMs Think while Listening?", "comment": null, "summary": "Recent advances in speech large language models (speech LLMs) have enabled\nseamless spoken interactions, but these systems still struggle with complex\nreasoning tasks. Previously, chain-of-thought (CoT) prompting or fine-tuning\nhas been to shown to significantly improve the reasoning abilities of\ntext-based LLMs. In this work, we investigate the effect of CoT fine-tuning for\nmulti-stream speech LLMs, demonstrating that reasoning in text space improves\nthe accuracy of speech LLMs by 2.4x, on average, over a suite of spoken\nreasoning tasks. Beyond accuracy, the latency of the spoken response is a\ncrucial factor for interacting with voice-based agents. Inspired by the human\nbehavior of \"thinking while listening,\" we propose methods to reduce the\nadditional latency from reasoning by allowing the model to start reasoning\nbefore the user query has ended. To achieve this, we introduce an entropy-based\nmetric, \"question completeness,\" which acts as an indicator to guide the model\non the optimal time to start reasoning. This method provides greater control\nover the accuracy-latency trade-off compared with heuristic-based approaches\nand, under equivalent latency conditions, yields a 4% accuracy gain on\nARC-Easy. Finally, we use Direct Preference Optimization (DPO) on preference\ndata created using rejection sampling to push the accuracy-latency pareto\nfrontier further, resulting in a 70% reduction in latency without loss in\naccuracy.", "AI": {"tldr": "The paper explores the improvement of reasoning abilities and reduction of latency in speech LLMs through chain-of-thought (CoT) fine-tuning, introducing a new metric and using Direct Preference Optimization, achieving significant gains in both accuracy and latency.", "motivation": "The motivation is to improve the reasoning abilities and response latency of speech LLMs, making them more capable and user-friendly in interactions.", "method": "The paper investigates the effect of chain-of-thought (CoT) fine-tuning on multi-stream speech LLMs. An entropy-based metric named 'question completeness' is proposed to reduce reasoning latency by allowing early reasoning. Direct Preference Optimization (DPO) is also applied to optimize the accuracy-latency trade-off.", "result": "The paper shows a 2.4x improvement in accuracy for spoken reasoning tasks, a 70% reduction in latency without accuracy loss, and a 4% accuracy gain on ARC-Easy compared to heuristic-based approaches under equivalent latency conditions.", "conclusion": "The methods proposed in the paper effectively improve both the reasoning accuracy and the response latency of speech LLMs, indicating a promising path for enhancing interactive voice-based systems."}}
{"id": "2510.07652", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07652", "abs": "https://arxiv.org/abs/2510.07652", "authors": ["Harshala Gammulle", "Clinton Fookes", "Sridha Sridharan", "Simon Denman"], "title": "Dual-Stream Alignment for Action Segmentation", "comment": "Journal Submission", "summary": "Action segmentation is a challenging yet active research area that involves\nidentifying when and where specific actions occur in continuous video streams.\nMost existing work has focused on single-stream approaches that model the\nspatio-temporal aspects of frame sequences. However, recent research has\nshifted toward two-stream methods that learn action-wise features to enhance\naction segmentation performance. In this work, we propose the Dual-Stream\nAlignment Network (DSA Net) and investigate the impact of incorporating a\nsecond stream of learned action features to guide segmentation by capturing\nboth action and action-transition cues. Communication between the two streams\nis facilitated by a Temporal Context (TC) block, which fuses complementary\ninformation using cross-attention and Quantum-based Action-Guided Modulation\n(Q-ActGM), enhancing the expressive power of the fused features. To the best of\nour knowledge, this is the first study to introduce a hybrid quantum-classical\nmachine learning framework for action segmentation. Our primary objective is\nfor the two streams (frame-wise and action-wise) to learn a shared feature\nspace through feature alignment. This is encouraged by the proposed Dual-Stream\nAlignment Loss, which comprises three components: relational consistency,\ncross-level contrastive, and cycle-consistency reconstruction losses. Following\nprior work, we evaluate DSA Net on several diverse benchmark datasets: GTEA,\nBreakfast, 50Salads, and EgoProcel. We further demonstrate the effectiveness of\neach component through extensive ablation studies. Notably, DSA Net achieves\nstate-of-the-art performance, significantly outperforming existing", "AI": {"tldr": "提出Dual-Stream Alignment Network (DSA Net)，结合帧级和动作级两种流来增强动作分割的性能，实现了在多个数据集上的最佳结果。", "motivation": "动作分割是一个具有挑战性的研究领域，涉及到从连续视频流中识别特定动作的时间和空间位置。最近的研究转向了双流方法，引入了动作级特征来提高动作分割性能。", "method": "提出Dual-Stream Alignment Network (DSA Net)，结合帧级和动作级特征流，通过Temporal Context (TC) 块使用交叉注意力和Quantum-based Action-Guided Modulation (Q-ActGM) 融合互补信息。", "result": "通过广泛的消融实验展示了各自组件的有效性，DSA Net 在GTEA、Breakfast、50Salads 和EgoProcel 数据集上实现了最先进的性能，显著优于现有方法。", "conclusion": "研究表明，在动作分割领域，引入量子-经典混合机器学习框架可以显著提升性能效果，未来可能需要进一步探究利用这种框架进行更多的视觉任务。"}}
{"id": "2510.07499", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07499", "abs": "https://arxiv.org/abs/2510.07499", "authors": ["Soyeong Jeong", "Taehee Jung", "Sung Ju Hwang", "Joo-Kyung Kim", "Dongyeop Kang"], "title": "When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs", "comment": null, "summary": "Recent Long-Context Language Models (LCLMs) can process hundreds of thousands\nof tokens in a single prompt, enabling new opportunities for\nknowledge-intensive multi-hop reasoning by integrating large sets of retrieved\ndocuments or, in some cases, directly all necessary information. However,\nsimply feeding more documents into the context window fails to capture how\nevidence should be connected. We address this gap with thought templates, which\nrecast reasoning as reusable thought caches, derived from prior problem solving\ntraces, structuring how evidence is combined and guiding multi-hop inference\nwith factual documents. To keep these templates effective, we propose an update\nstrategy that iteratively refines templates derived from training data through\nnatural-language feedback. Across diverse benchmarks and LCLM families, our\napproach delivers consistent gains over strong baselines in both\nretrieval-based and retrieval-free settings. Furthermore, we show that\noptimized templates can be distilled into smaller open-source models,\ndemonstrating its broad applicability and transparent reasoning reuse. We refer\nto our framework as Thought Template Augmented LCLMs (ToTAL).", "AI": {"tldr": "研究提出了一种名为ToTAL的框架，通过使用思考模板和更新策略来改进LCLM在多跳推理中的效果，这种框架可以在各种模型中提供一致性收益，并支持模板优化后向更小的模型蒸馏。", "motivation": "尽管最新长上下文语言模型（LCLMs）能够处理成千上万个令牌的单个提示，从而能够进行大量知识密集型多跳推理，但这些模型在将证据连接起来的方式上存在不足。", "method": "通过思考模板将推理重组为可重用的思维缓存，这些缓存是从先前的问题解决轨迹中衍生出来的，用于结构化证据的结合并指导多跳推理。我们提出了一种更新策略，通过自然语言反馈迭代地细化从训练数据中得出的模板。", "result": "在各种基准测试和不同的LCLM家族中，该方法在基于检索和不基于检索的设置下都持续超越了强劲的基线。", "conclusion": "经过优化的模板可以被蒸馏到更小的开源模型中，显示了该框架的广泛适用性和透明推理的重用性。此框架被命名为通过思考模板增强的长上下文语言模型（ToTAL）。"}}
{"id": "2510.07654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07654", "abs": "https://arxiv.org/abs/2510.07654", "authors": ["Yanjie Pan", "Qingdong He", "Lidong Wang", "Bo Peng", "Mingmin Chi"], "title": "Once Is Enough: Lightweight DiT-Based Video Virtual Try-On via One-Time Garment Appearance Injection", "comment": "5 pages (including references), 4 figures. Code and models will be\n  released upon publication", "summary": "Video virtual try-on aims to replace the clothing of a person in a video with\na target garment. Current dual-branch architectures have achieved significant\nsuccess in diffusion models based on the U-Net; however, adapting them to\ndiffusion models built upon the Diffusion Transformer remains challenging.\nInitially, introducing latent space features from the garment reference branch\nrequires adding or modifying the backbone network, leading to a large number of\ntrainable parameters. Subsequently, the latent space features of garments lack\ninherent temporal characteristics and thus require additional learning. To\naddress these challenges, we propose a novel approach, OIE (Once is Enough), a\nvirtual try-on strategy based on first-frame clothing replacement:\nspecifically, we employ an image-based clothing transfer model to replace the\nclothing in the initial frame, and then, under the content control of the\nedited first frame, utilize pose and mask information to guide the temporal\nprior of the video generation model in synthesizing the remaining frames\nsequentially. Experiments show that our method achieves superior parameter\nefficiency and computational efficiency while still maintaining leading\nperformance under these constraints.", "AI": {"tldr": "We propose OIE, a video virtual try-on method that replaces clothing in the first frame and then uses pose and mask information to generate subsequent frames efficiently.", "motivation": "The motivation is to address the challenges of adapting dual-branch architectures to diffusion models built upon the Diffusion Transformer, which requires adding or modifying the backbone network and additional learning for latent space features of garments.", "method": "Our method, named OIE (Once is Enough), tackles the challenge of adapting dual-branch architectures to Diffusion Transformer-based models for video virtual try-on. It introduces a strategy based on first-frame clothing replacement, using an image-based clothing transfer model to replace the clothing in the first frame. Then, under the content control of the edited first frame, it employs pose and mask information to guide the temporal prior of the video generation model in synthesizing the remaining frames sequentially.", "result": "Experiments indicate that our approach achieves better parameter efficiency and computational efficiency, while still maintaining leading performance.", "conclusion": "The conclusion is that our proposed method, OIE, provides superior parameter efficiency and computational efficiency for video virtual try-on tasks, all the while maintaining high performance."}}
{"id": "2510.07520", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07520", "abs": "https://arxiv.org/abs/2510.07520", "authors": ["Rayyan Merchant", "Kevin Tang"], "title": "ParsTranslit: Truly Versatile Tajik-Farsi Transliteration", "comment": null, "summary": "As a digraphic language, the Persian language utilizes two written standards:\nPerso-Arabic in Afghanistan and Iran, and Tajik-Cyrillic in Tajikistan. Despite\nthe significant similarity between the dialects of each country, script\ndifferences prevent simple one-to-one mapping, hindering written communication\nand interaction between Tajikistan and its Persian-speaking ``siblings''. To\novercome this, previously-published efforts have investigated machine\ntransliteration models to convert between the two scripts. Unfortunately, most\nefforts did not use datasets other than those they created, limiting these\nmodels to certain domains of text such as archaic poetry or word lists. A truly\nusable transliteration system must be capable of handling varied domains,\nmeaning that suck models lack the versatility required for real-world usage.\nThe contrast in domain between data also obscures the task's true difficulty.\nWe present a new state-of-the-art sequence-to-sequence model for Tajik-Farsi\ntransliteration trained across all available datasets, and present two datasets\nof our own. Our results across domains provide clearer understanding of the\ntask, and set comprehensive comparable leading benchmarks. Overall, our model\nachieves chrF++ and Normalized CER scores of 87.91 and 0.05 from Farsi to Tajik\nand 92.28 and 0.04 from Tajik to Farsi. Our model, data, and code are available\nat https://anonymous.4open.science/r/ParsTranslit-FB30/.", "AI": {"tldr": "本研究开发了一种新型的塔吉克语-波斯语文字转写模型并通过验证表明该模型在所有现有数据集上表现出色，解决了不同领域数据限制的问题。", "motivation": "研究动机在于克服塔吉克语和波斯语由于书写系统不同而导致的沟通障碍问题。之前的工作在模型训练数据方面存在局限性，主要依赖特定领域的数据集，缺乏通用性。", "method": "本研究提出了一种新的端到端序列模型，用于塔吉克语-波斯语之间的文字转写。该模型是在所有现有数据集上进行训练的，并提供了两个新的数据集。", "result": "该模型在不同领域的测试中显示出优良的性能，从波斯语到塔吉克语的chrF++得分为87.91，Normalized CER得分为0.05，从塔吉克语到波斯语的chrF++得分为92.28，Normalized CER得分为0.04。", "conclusion": "研究结论为，所提出的端到端序列模型通过混合使用不同来源的数据，提升了在跨领域文本转写任务中的性能，并为该领域提供了全面的对比基准。"}}
{"id": "2510.07656", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07656", "abs": "https://arxiv.org/abs/2510.07656", "authors": ["James Baker"], "title": "MONKEY: Masking ON KEY-Value Activation Adapter for Personalization", "comment": null, "summary": "Personalizing diffusion models allows users to generate new images that\nincorporate a given subject, allowing more control than a text prompt. These\nmodels often suffer somewhat when they end up just recreating the subject\nimage, and ignoring the text prompt. We observe that one popular method for\npersonalization, the IP-Adapter automatically generates masks that we\ndefinitively segment the subject from the background during inference. We\npropose to use this automatically generated mask on a second pass to mask the\nimage tokens, thus restricting them to the subject, not the background,\nallowing the text prompt to attend to the rest of the image. For text prompts\ndescribing locations and places, this produces images that accurately depict\nthe subject while definitively matching the prompt. We compare our method to a\nfew other test time personalization methods, and find our method displays high\nprompt and source image alignment.", "AI": {"tldr": "通过在个性化扩散模型中使用自动生成的掩码来优化图像生成过程，确保生成的新图像既能充分体现给定主题，又能够与文本提示相匹配，特别是在描述地点和场所时表现出色。", "motivation": "解决个性化扩散模型仅重复生成主题图像而不考虑文本提示的问题，提高图像生成时的多样性和创造性。", "method": "利用IP-Adapter自动生成的掩码，在二次生成中限制图像令牌仅处理主题部分，文本提示可以进一步控制图像的其他部分。", "result": "与几种其他测试时间个性化方法相比，该方法显示出更高的提示和源图像一致性。", "conclusion": "通过提出的二次生成技术，可以提高个性化扩散模型生成图像的质量和创意性，特别是在确保与文本提示的匹配度方面。"}}
{"id": "2510.07535", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07535", "abs": "https://arxiv.org/abs/2510.07535", "authors": ["Jaeseong Lee", "seung-won hwang", "Aurick Qiao", "Gabriele Oliaro", "Ye Wang", "Samyam Rajbhandari"], "title": "OWL: Overcoming Window Length-Dependence in Speculative Decoding for Long-Context Inputs", "comment": null, "summary": "Speculative decoding promises faster inference for large language models\n(LLMs), yet existing methods fail to generalize to real-world settings.\nBenchmarks typically assume short contexts (e.g., 2K tokens), whereas practical\nworkloads involve long contexts. We find current approaches degrade severely\nwith long contexts; for instance, EAGLE3 even slows down the generation speed\nby 0.81x. We address these limitations by releasing a new long-context\nbenchmark (LongSpecBench) and introducing a novel model (OWL). OWL achieves\nabout 5x higher acceptance length than EAGLE3 on long-context inputs through\nthree innovations: (1) an LSTM-based drafter conditioned only on the last-token\nstate, making it generalize to various lengths, (2) a special token [SPEC] in\nthe verifier that produces richer representation for drafter, and (3) a hybrid\nalgorithm combining both tree and non-tree decoding methods. We release all\ncode and datasets to advance future research.", "AI": {"tldr": "研究提出了一种名为 OWL 的新型模型，解决了现有推测解码方法在长上下文输入上的性能下降问题，通过引入新的技术，OWL 在长上下文输入中的接受长度提高了约 5 倍，为未来的研究提供了基准和数据集。", "motivation": "现有的推测解码方法无法泛化到现实世界的场景中，特别是在处理长上下文输入时表现不佳。OWLS 旨在解决这个问题，从而实现更快的大型语言模型推理。", "method": "通过发布一个新的长上下文基准测试（LongSpecBench）和引入一种新的模型（OWL），来解决现有方法在长上下文输入上的不足。OWL 通过三种创新实现比 EAGLE3 高 5 倍的接受长度：(1) 仅依赖最后标记状态条件的基于 LSTM 的草案模块，使其适应多种长度；(2) 验证器中的特殊标记 [SPEC] 产生对草案模块更有用的表示；(3) 结合树和非树解码方法的混合算法。", "result": "OWL 模型在长上下文输入上表现出显著优于 EAGLE3 的性能，它的接受长度大约提高了 5 倍。", "conclusion": "OWL 模型的成功证明了其在处理长上下文输入上的优越性，并提供了新的基准和数据集以推进未来研究的发展。"}}
{"id": "2510.07665", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07665", "abs": "https://arxiv.org/abs/2510.07665", "authors": ["Jun Muraoka", "Daichi Haraguchi", "Naoto Inoue", "Wataru Shimoda", "Kota Yamaguchi", "Seiichi Uchida"], "title": "Automatic Text Box Placement for Supporting Typographic Design", "comment": null, "summary": "In layout design for advertisements and web pages, balancing visual appeal\nand communication efficiency is crucial. This study examines automated text box\nplacement in incomplete layouts, comparing a standard Transformer-based method,\na small Vision and Language Model (Phi3.5-vision), a large pretrained VLM\n(Gemini), and an extended Transformer that processes multiple images.\nEvaluations on the Crello dataset show the standard Transformer-based models\ngenerally outperform VLM-based approaches, particularly when incorporating\nricher appearance information. However, all methods face challenges with very\nsmall text or densely populated layouts. These findings highlight the benefits\nof task-specific architectures and suggest avenues for further improvement in\nautomated layout design.", "AI": {"tldr": "This study evaluates various models for automated text box placement in incomplete layouts, finding that standard Transformer-based models generally perform better than VLM-based approaches, though all methods face challenges with very small text and densely populated layouts.", "motivation": "The motivation for this research is rooted in the importance of balancing visual appeal and communication efficiency in layout design, particularly for advertisements and web pages, and the need for effective automated text box placement solutions in incomplete layouts.", "method": "This study compares the performance of a standard Transformer-based method, a small Vision and Language Model (Phi3.5-vision), a large pretrained VLM (Gemini), and an extended Transformer that processes multiple images for automated text box placement in incomplete layouts.", "result": "Evaluations on the Crello dataset indicate that the standard Transformer-based models generally outperform the VLM-based methods, especially when incorporating richer appearance information. However, the methods struggle with very small text or densely populated layouts.", "conclusion": "The findings underscore the value of designing task-specific architectures and point to potential areas for enhancing automated layout design."}}
{"id": "2510.07545", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07545", "abs": "https://arxiv.org/abs/2510.07545", "authors": ["Md Tahmid Rahman Laskar", "Mohammed Saidul Islam", "Ridwan Mahbub", "Mizanur Rahman", "Amran Bhuiyan", "Israt Jahan", "Mir Tafseer Nayeem", "Shafiq Joty", "Enamul Hoque", "Jimmy Huang"], "title": "Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models: Lessons Learned and Best Practices", "comment": "Accepted to the EMNLP 2025 Industry Track", "summary": "Large Vision-Language Models (LVLMs) with only 7B parameters have shown\npromise as automated judges in chart comprehension tasks. However, tiny models\n(<=2B parameters) still perform poorly as judges, limiting their real-world use\nin resource-constrained settings. To address this, we propose two approaches to\nensure cost-efficient evaluation: (i) multi-criteria prompting, which combines\nseparate evaluation criteria into a single query, and (ii) domain-adaptive\ntransfer learning, in which we fine-tune a 2B-parameter LVLM on synthetic\njudgments in a chart dataset to create the ChartJudge. Experiments show that\nmulti-criteria prompting exposes robustness gaps, which led to a huge drop in\nperformance for 7B models, including specialized LVLM judges like LLaVA-Critic.\nIn addition, we find that our tiny LVLM (ChartJudge) can effectively transfer\nknowledge from one dataset to another to make it a more specialized model. Our\nfine-grained analysis across chart types and query complexities offers\nactionable insights into trade-offs between model size, prompt design, and\ntransferability, enabling scalable, low-cost evaluation for chart reasoning\ntasks. Our code and the data will be made publicly available.", "AI": {"tldr": "本文提出了两种有效的方法来提高Tiny模型的成本效益，尤其是在图表理解任务中的评价能力。", "motivation": "由于参数量小于或等于20亿的tiny模型在作为评判者时表现不佳，限制了它们在资源受限环境下的实际应用。为了解决这个问题，我们提出了成本效益更高的评估方法。", "method": "我们提出了两种方法来确保成本效益的评估：(i) 多标准提示技术，它将分离的评估标准合并为一个查询；(ii) 域适应迁移学习，其中我们对一个20亿参数的LVLM进行微调，使其成为一个图表判断模型（ChartJudge）。", "result": "实验显示，多标准提示技术暴露了70亿参数模型（包括专门的LVLM评判器如LLaVA-Critic）的稳健性差距，导致性能大幅下降。同时，我们的tiny LVLM（ChartJudge）可以从一个数据集转移到另一个数据集，成为一个更专业的模型。", "conclusion": "我们对图表类型和查询复杂性的细致分析提供了可行的见解，使模型大小、提示设计和可移植性之间的权衡显而易见，从而实现可扩展的、低成本的图表推理任务评价。"}}
{"id": "2510.07666", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07666", "abs": "https://arxiv.org/abs/2510.07666", "authors": ["Heming Wu", "Di Wang", "Tai Ma", "Peng Zhao", "Yubin Xiao", "Zhongke Wu", "Xing-Ce Wang", "Chuang Li", "Xuan Wu", "You Zhou"], "title": "TCIP: Threshold-Controlled Iterative Pyramid Network for Deformable Medical Image Registration", "comment": null, "summary": "Although pyramid networks have demonstrated superior performance in\ndeformable medical image registration, their decoder architectures are\ninherently prone to propagating and accumulating anatomical structure\nmisalignments. Moreover, most existing models do not adaptively determine the\nnumber of iterations for optimization under varying deformation requirements\nacross images, resulting in either premature termination or excessive\niterations that degrades registration accuracy. To effectively mitigate the\naccumulation of anatomical misalignments, we propose the Feature-Enhanced\nResidual Module (FERM) as the core component of each decoding layer in the\npyramid network. FERM comprises three sequential blocks that extract anatomical\nsemantic features, learn to suppress irrelevant features, and estimate the\nfinal deformation field, respectively. To adaptively determine the number of\niterations for varying images, we propose the dual-stage Threshold-Controlled\nIterative (TCI) strategy. In the first stage, TCI assesses registration\nstability and with asserted stability, it continues with the second stage to\nevaluate convergence. We coin the model that integrates FERM and TCI as\nThreshold-Controlled Iterative Pyramid (TCIP). Extensive experiments on three\npublic brain MRI datasets and one abdomen CT dataset demonstrate that TCIP\noutperforms the state-of-the-art (SOTA) registration networks in terms of\naccuracy, while maintaining comparable inference speed and a compact model\nparameter size. Finally, we assess the generalizability of FERM and TCI by\nintegrating them with existing registration networks and further conduct\nablation studies to validate the effectiveness of these two proposed methods.", "AI": {"tldr": "本文提出了TCIP模型，通过引入FERM模块和TCI策略，解决了现有金字塔网络模型的几个问题，包括误对齐累积和自适应调整迭代次数。实验显示该模型在精度上超越当前最佳方法，并保持高效和紧凑。", "motivation": "金字塔网络在可变形医学图像配准中表现出色，但其解码架构本质上容易传播和累积解剖结构错位。此外，大多数现有模型没有自适应地确定优化迭代次数，这导致了过早终止或过多的迭代次数，影响配准精度。", "method": "本文提出了一种名为特征增强残差模块（FERM）的新组件，用于金字塔网络中每个解码层的核心。FERM包括三个连续的模块，分别用于提取解剖语义特征、抑制无关特征以及估计最终的形变场。此外，为自适应地确定不同图像的迭代次数，本文提出了双重阶段的阈值控制迭代策略（TCI）。在第一阶段，TCI评估注册稳定性，确认稳定后，继续第二阶段进行收敛评估。将FERM和TCI结合的模型称为阈值控制迭代金字塔（TCIP）", "result": "在三个公开的脑MRI数据集和一个腹部CT数据集上的大量实验表明，TCIP在精度上优于最先进的（SOTA）配准网络，同时保持了相当的推理速度和紧凑的模型参数尺寸。", "conclusion": "通过集成FERM和TCI，TCIP模型在医学图像配准精度上取得了优异性能。此外，还评估了FERM和TCI的普遍性和有效性，进一步进行了消融研究，验证了这两种方法的效果。"}}
{"id": "2510.07566", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07566", "abs": "https://arxiv.org/abs/2510.07566", "authors": ["Junyi Zhu", "Savas Ozkan", "Andrea Maracani", "Sinan Mutlu", "Cho Jung Min", "Mete Ozay"], "title": "Multi-Task Pre-Finetuning of Lightweight Transformer Encoders for Text Classification and NER", "comment": "Accepted by EMNLP 2025 Industry Track", "summary": "Deploying natural language processing (NLP) models on mobile platforms\nrequires models that can adapt across diverse applications while remaining\nefficient in memory and computation. We investigate pre-finetuning strategies\nto enhance the adaptability of lightweight BERT-like encoders for two\nfundamental NLP task families: named entity recognition (NER) and text\nclassification. While pre-finetuning improves downstream performance for each\ntask family individually, we find that na\\\"ive multi-task pre-finetuning\nintroduces conflicting optimization signals that degrade overall performance.\nTo address this, we propose a simple yet effective multi-task pre-finetuning\nframework based on task-primary LoRA modules, which enables a single shared\nencoder backbone with modular adapters. Our approach achieves performance\ncomparable to individual pre-finetuning while meeting practical deployment\nconstraint. Experiments on 21 downstream tasks show average improvements of\n+0.8% for NER and +8.8% for text classification, demonstrating the\neffectiveness of our method for versatile mobile NLP applications.", "AI": {"tldr": "研究提出了一种用于命名实体识别和文本分类任务的多任务预微调框架，通过使用任务优先的LoRA模块，提高了轻量级BERT样编码器在移动平台上的适应性和性能。", "motivation": "为了提高轻量级BERT样编码器在移动平台上的适应性，同时保持内存和计算效率，研究了预微调策略。", "method": "我们提出了一种基于任务优先的LoRA模块的多任务预微调框架，该框架能够在单一共享编码器主干的基础上利用模块化适配器实现多任务学习。", "result": "我们的方法在21个下游任务上的实验表明，对于命名实体识别任务平均提高了+0.8%，对于文本分类任务平均提高了+8.8%。", "conclusion": "该方法达到了与单独预微调相当的性能，同时满足实际部署约束，证明了对于多功能移动NLP应用的有效性。"}}
{"id": "2510.07670", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07670", "abs": "https://arxiv.org/abs/2510.07670", "authors": ["Haoyi Duan", "Yunzhi Zhang", "Yilun Du", "Jiajun Wu"], "title": "Controllable Video Synthesis via Variational Inference", "comment": "Project page: https://video-synthesis-variational.github.io/", "summary": "Many video workflows benefit from a mixture of user controls with varying\ngranularity, from exact 4D object trajectories and camera paths to coarse text\nprompts, while existing video generative models are typically trained for fixed\ninput formats. We develop a video synthesis method that addresses this need and\ngenerates samples with high controllability for specified elements while\nmaintaining diversity for under-specified ones. We cast the task as variational\ninference to approximate a composed distribution, leveraging multiple video\ngeneration backbones to account for all task constraints collectively. To\naddress the optimization challenge, we break down the problem into step-wise KL\ndivergence minimization over an annealed sequence of distributions, and further\npropose a context-conditioned factorization technique that reduces modes in the\nsolution space to circumvent local optima. Experiments suggest that our method\nproduces samples with improved controllability, diversity, and 3D consistency\ncompared to prior works.", "AI": {"tldr": "本文开发了一种具有高度可控制性的视频合成方法，该方法能够生成针对特定元素具有高度可控制性的样本，对于未指定的元素则保持多样性。", "motivation": "动机在于现有视频生成模型通常针对固定输入格式进行训练，而许多视频工作流程从中受益于混合粒度用户控制，从精确的4D对象轨迹和相机路径到粗略的文本提示。", "method": "我们开发了一种视频合成方法，该方法通过变分推理来逼近一个组合分布，利用多个视频生成骨干网络来共同处理所有任务约束。为了应对优化挑战，我们将问题分解为逐步的KL散度最小化过程，并提出了一种上下文条件因子化技术来减少解空间的模式以规避局部最优解。", "result": "实验表明，与先前工作相比，我们的方法在可控制性、多样性和3D一致性方面产生了改进的样本。", "conclusion": "通过这种方法，能够生产出具有良好可控制性、多样性和3D一致性的视频样本，从而满足视频工作流中的多种需求。"}}
{"id": "2510.07579", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07579", "abs": "https://arxiv.org/abs/2510.07579", "authors": ["Mkululi Sikosana", "Sean Maudsley-Barton", "Oluwaseun Ajao"], "title": "Linguistic Patterns in Pandemic-Related Content: A Comparative Analysis of COVID-19, Constraint, and Monkeypox Datasets", "comment": "16 pages", "summary": "This study conducts a computational linguistic analysis of pandemic-related\nonline discourse to examine how language distinguishes health misinformation\nfrom factual communication. Drawing on three corpora: COVID-19 false narratives\n(n = 7588), general COVID-19 content (n = 10700), and Monkeypox-related posts\n(n = 5787), we identify significant differences in readability, rhetorical\nmarkers, and persuasive language use. COVID-19 misinformation exhibited\nmarkedly lower readability scores and contained over twice the frequency of\nfear-related or persuasive terms compared to the other datasets. It also showed\nminimal use of exclamation marks, contrasting with the more emotive style of\nMonkeypox content. These patterns suggest that misinformation employs a\ndeliberately complex rhetorical style embedded with emotional cues, a\ncombination that may enhance its perceived credibility. Our findings contribute\nto the growing body of work on digital health misinformation by highlighting\nlinguistic indicators that may aid detection efforts. They also inform public\nhealth messaging strategies and theoretical models of crisis communication in\nnetworked media environments. At the same time, the study acknowledges\nlimitations, including reliance on traditional readability indices, use of a\ndeliberately narrow persuasive lexicon, and reliance on static aggregate\nanalysis. Future research should therefore incorporate longitudinal designs,\nbroader emotion lexicons, and platform-sensitive approaches to strengthen\nrobustness.", "AI": {"tldr": "本研究通过计算语言学方法分析了三种语料库中的在线讨论，发现健康误导信息在语言上有其独特的复杂修辞风格和情感线索，这一发现有助于识别误导信息，并对公共卫生信息传播策略和危机传播理论有所启示。", "motivation": "研究动机在于理解疫情期间在线讨论中语言的差异性，重点关注语言如何区分健康误导信息与事实性交流，从而为识别健康误导信息提供语言学指标。", "method": "本研究使用计算语言学方法分析了疫情期间的在线讨论，具体通过三种语料库：7588则有关新冠的虚假叙述、10700则一般新冠内容以及5787则猴痘相关帖子，来研究语言如何区分健康误导信息与事实性交流。", "result": "研究发现新冠误导信息的可读性评分明显较低，且恐惧相关或具有说服力的词汇频率是其他数据集的两倍多。此外，新冠误导信息使用感叹号的频率较低，与情感表达更强的猴痘相关内容形成对比。", "conclusion": "研究结果强调了误导信息采用了复杂修辞风格，并嵌入了情感线索的组合，这可能增强了其可信度。研究同时指出了一些局限性，并呼吁后续研究应采用纵向设计、更广泛的情感词汇和平台敏感的方法。"}}
{"id": "2510.07692", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07692", "abs": "https://arxiv.org/abs/2510.07692", "authors": ["Tangin Amir Smrity", "MD Zahin Muntaqim Hasan Muhammad Kafi", "Abu Saleh Musa Miah", "Najmul Hassan", "Yuichi Okuyama", "Nobuyoshi Asai", "Taro Suzuki", "Jungpil Shin"], "title": "Hybrid CNN-BYOL Approach for Fault Detection in Induction Motors Using Thermal Images", "comment": null, "summary": "Induction motors (IMs) are indispensable in industrial and daily life, but\nthey are susceptible to various faults that can lead to overheating, wasted\nenergy consumption, and service failure. Early detection of faults is essential\nto protect the motor and prolong its lifespan. This paper presents a hybrid\nmethod that integrates BYOL with CNNs for classifying thermal images of\ninduction motors for fault detection. The thermal dataset used in this work\nincludes different operating states of the motor, such as normal operation,\noverload, and faults. We employed multiple deep learning (DL) models for the\nBYOL technique, ranging from popular architectures such as ResNet-50,\nDenseNet-121, DenseNet-169, EfficientNetB0, VGG16, and MobileNetV2.\nAdditionally, we introduced a new high-performance yet lightweight CNN model\nnamed BYOL-IMNet, which comprises four custom-designed blocks tailored for\nfault classification in thermal images. Our experimental results demonstrate\nthat the proposed BYOL-IMNet achieves 99.89\\% test accuracy and an inference\ntime of 5.7 ms per image, outperforming state-of-the-art models. This study\nhighlights the promising performance of the CNN-BYOL hybrid method in enhancing\naccuracy for detecting faults in induction motors, offering a robust\nmethodology for online monitoring in industrial settings.", "AI": {"tldr": "A hybrid method combining BYOL with CNNs, especially the BYOL-IMNet model, is proposed for early fault detection in IMs, achieving high accuracy and quick inference time.", "motivation": "The motivation is to improve the early detection of faults in IMs to protect them and extend their lifespan, due to the critical importance of IMs in industries and daily life.", "method": "This paper integrates BYOL (Bootstrap Your Own Latent) with CNNs (Convolutional Neural Networks), using a thermal dataset that includes various states of IMs (normal, overload, faults), and employs DL models such as ResNet-50, DenseNet-121, VGG16, etc. A new model, BYOL-IMNet, is introduced which is tailored for fault classification.", "result": "The BYOL-IMNet has achieved 99.89% test accuracy and an inference time of 5.7 ms per image, outperforming other models.", "conclusion": "The hybrid method using CNN-BYOL shows promising performance for fault detection in IMs, offering robust online monitoring."}}
{"id": "2510.07591", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07591", "abs": "https://arxiv.org/abs/2510.07591", "authors": ["Chihiro Taguchi", "Richard Sproat"], "title": "IASC: Interactive Agentic System for ConLangs", "comment": "Initial draft", "summary": "We present a system that uses LLMs as a tool in the development of\nConstructed Languages. The system is modular in that one first creates a target\nphonology for the language using an agentic approach that refines its output at\neach step with commentary feedback on its previous attempt. Next, a set of\nsentences is 'translated' from their English original into a morphosyntactic\nmarkup that reflects the word order and morphosyntactic feature specifications\nof the desired target language, with affixes represented as morphosyntactic\nfeature bundles. From this translated corpus, a lexicon is constructed using\nthe phonological model and the set of morphemes (stems and affixes) extracted\nfrom the 'translated' sentences. The system is then instructed to provide an\northography for the language, using an existing script such as Latin or\nCyrillic. Finally, the system writes a brief grammatical handbook of the\nlanguage. The system can also translate further sentences into the target\nlanguage.\n  Our goal is twofold. First, we hope that these tools will be fun to use for\ncreating artificially constructed languages. Second, we are interested in\nexploring what LLMs 'know' about language-not what they know about any\nparticular language or linguistic phenomenon, but how much they know about and\nunderstand language and linguistic concepts. As we shall see, there is a fairly\nwide gulf in capabilities both among different LLMs and among different\nlinguistic specifications, with it being notably easier for systems to deal\nwith more common patterns than rarer ones. An additional avenue that we explore\nis the application of our approach to translating from high-resource into\nlow-resource languages. While the results so far are mostly negative, we\nprovide some evidence that an improved version of the present system could\nafford some real gains in such tasks.\n  https://github.com/SakanaAI/IASC", "AI": {"tldr": "本文介绍了一个使用LLMs在构造语言开发中的系统。系统具有多种功能，包括音系开发、文本翻译、词表建立和语法手册编写等。研究旨在开发有趣工具，并探讨LLMs对语言的理解。", "motivation": "本研究旨在开发用于创建人工构造语言的有趣工具，同时探究LLMs对语言和语言概念的理解程度。此外，还将研究此方法应用于从高资源语言到低资源语言的翻译。", "method": "运用LLMs在构造语言开发中的系统方法。首先，通过代理方法逐步细化目标语言的音系。其次，将句子从英语翻译成反映目标语言词序和形态句法特征规格的形态句法标记。接着，建立词表，最后系统生成语言的书写系统并编写语法手册。", "result": "结果表明，不同LLMs和语言规格之间存在显著能力差异，更常见的模式比罕见的更容易处理。尽管如此，研究提供了一些证据，表明改进的系统可能在低资源语言翻译任务中有所裨益。", "conclusion": "本系统对于创建人工语言具有潜力，并为理解LLMs对语言的理解提供了见解。然而，应用于高资源到低资源的翻译任务中，效果不稳定，但展示了改进系统的潜力。"}}
{"id": "2510.07703", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07703", "abs": "https://arxiv.org/abs/2510.07703", "authors": ["Xiaoxu Ma", "Runhao Li", "Zhenyu Weng"], "title": "Mutual Learning for Hashing: Unlocking Strong Hash Functions from Weak Supervision", "comment": null, "summary": "Deep hashing has been widely adopted for large-scale image retrieval, with\nnumerous strategies proposed to optimize hash function learning. Pairwise-based\nmethods are effective in learning hash functions that preserve local similarity\nrelationships, whereas center-based methods typically achieve superior\nperformance by more effectively capturing global data distributions. However,\nthe strength of center-based methods in modeling global structures often comes\nat the expense of underutilizing important local similarity information. To\naddress this limitation, we propose Mutual Learning for Hashing (MLH), a novel\nweak-to-strong framework that enhances a center-based hashing branch by\ntransferring knowledge from a weaker pairwise-based branch. MLH consists of two\nbranches: a strong center-based branch and a weaker pairwise-based branch.\nThrough an iterative mutual learning process, the center-based branch leverages\nlocal similarity cues learned by the pairwise-based branch. Furthermore,\ninspired by the mixture-of-experts paradigm, we introduce a novel\nmixture-of-hash-experts module that enables effective cross-branch interaction,\nfurther enhancing the performance of both branches. Extensive experiments\ndemonstrate that MLH consistently outperforms state-of-the-art hashing methods\nacross multiple benchmark datasets.", "AI": {"tldr": "该研究提出了一种名为MLH的新框架，通过相互增强的基于中心和基于成对的分支，在大规模图像检索领域提升了哈希函数的学习。实验结果表明，MLH在多个基准数据集上超越了最先进的哈希方法。", "motivation": "基于成对的方法在学习保存局部相似关系的哈希函数方面效果良好，而基于中心的方法通过更有效地捕捉全局数据分布通常能获得更好的性能。然而，基于中心的方法在建模全局结构方面的优势往往是以充分利用重要的局部相似性信息不足为代价的。为了克服这一局限性，本研究提出了MLH。", "method": "提出了一种名为Mutual Learning for Hashing (MLH)的新弱到强框架，该框架通过从较弱的基于成对的方法分支向较强的基于中心的方法分支转移知识来增强基于中心的哈希分支。MLH由两个分支组成：一个强大的基于中心的分支和一个较弱的基于成对的分支。通过迭代的相互学习过程，中心分支利用了成对分支学到的局部相似性线索。此外，受到专家混合范式的启发，我们引入了一种新的哈希专家混合模块，该模块能够实现有效的跨分支交互，进一步增强两个分支的性能。", "result": "广泛的实验表明，MLH在多个基准数据集上持续超越最先进的哈希方法。", "conclusion": "通过引入相互学习机制和哈希专家混合模块来实现分支间的交互，MLH能够有效提升哈希函数在全球和局部数据分布上的性能，从而在大规模图像检索中取得优越的效果。"}}
{"id": "2510.07613", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07613", "abs": "https://arxiv.org/abs/2510.07613", "authors": ["Isabel Papadimitriou", "Jacob Prince"], "title": "Vocabulary embeddings organize linguistic structure early in language model training", "comment": null, "summary": "Large language models (LLMs) work by manipulating the geometry of input\nembedding vectors over multiple layers. Here, we ask: how are the input\nvocabulary representations of language models structured, and how and when does\nthis structure evolve over training? To answer this question, we use\nrepresentational similarity analysis, running a suite of experiments that\ncorrelate the geometric structure of the input embeddings and output embeddings\nof two open-source models (Pythia 12B and OLMo 7B) with semantic, syntactic,\nand frequency-based metrics over the course of training. Our key findings are\nas follows: 1) During training, the vocabulary embedding geometry quickly\nconverges to high correlations with a suite of semantic and syntactic features;\n2) Embeddings of high-frequency and function words (e.g., \"the,\" \"of\") converge\nto their final vectors faster than lexical and low-frequency words, which\nretain some alignment with the bias in their random initializations. These\nfindings help map the dynamic trajectory by which input embeddings organize\naround linguistic structure, revealing distinct roles for word frequency and\nfunction. Our findings motivate a deeper study of how the evolution of\nvocabulary geometry may facilitate specific capability gains during model\ntraining.", "AI": {"tldr": "研究了语言模型词汇表示的结构及其在训练过程中如何演变，发现高频和功能词的嵌入向量更快收敛，揭示了词汇频率和功能的不同作用。", "motivation": "研究语言模型输入词汇表示的结构，以及它们如何在训练过程中演变。", "method": "使用表征相似性分析，进行了一系列实验，将两种开源模型（Pythia 12B 和 OLMo 7B）的输入嵌入和输出嵌入的几何结构与语义、句法和基于频率的度量进行关联。", "result": "主要发现是：1) 在训练过程中，词汇嵌入的几何结构迅速与一系列语义和句法特征的高度相关性收敛；2) 高频和功能词（如“the”、“of”）的嵌入向量更快收敛到最终状态，而词汇和低频词则保留了它们随机初始化中的一些偏差。", "conclusion": "这些发现有助于绘制输入嵌入围绕语言结构组织的动态轨迹，揭示了词汇频率和功能的不同作用，并激励对词汇几何结构演变如何在模型训练过程中促进特定能力提升的深入研究。"}}
{"id": "2510.07721", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07721", "abs": "https://arxiv.org/abs/2510.07721", "authors": ["Zipeng Guo", "Lichen Ma", "Xiaolong Fu", "Gaojing Zhou", "Lan Yang", "Yuchen Zhou", "Linkai Liu", "Yu He", "Ximan Liu", "Shiping Dong", "Jingling Fu", "Zhen Chen", "Yu Shi", "Junshi Huang", "Jason Li", "Chao Gou"], "title": "RePainter: Empowering E-commerce Object Removal via Spatial-matting Reinforcement Learning", "comment": null, "summary": "In web data, product images are central to boosting user engagement and\nadvertising efficacy on e-commerce platforms, yet the intrusive elements such\nas watermarks and promotional text remain major obstacles to delivering clear\nand appealing product visuals. Although diffusion-based inpainting methods have\nadvanced, they still face challenges in commercial settings due to unreliable\nobject removal and limited domain-specific adaptation. To tackle these\nchallenges, we propose Repainter, a reinforcement learning framework that\nintegrates spatial-matting trajectory refinement with Group Relative Policy\nOptimization (GRPO). Our approach modulates attention mechanisms to emphasize\nbackground context, generating higher-reward samples and reducing unwanted\nobject insertion. We also introduce a composite reward mechanism that balances\nglobal, local, and semantic constraints, effectively reducing visual artifacts\nand reward hacking. Additionally, we contribute EcomPaint-100K, a high-quality,\nlarge-scale e-commerce inpainting dataset, and a standardized benchmark\nEcomPaint-Bench for fair evaluation. Extensive experiments demonstrate that\nRepainter significantly outperforms state-of-the-art methods, especially in\nchallenging scenes with intricate compositions. We will release our code and\nweights upon acceptance.", "AI": {"tldr": "我们提出了一种新的强化学习框架（Repainter），用于优化电子商务产品图像中的对象去除，该框架包括空间遮罩轨迹优化和群组相对策略优化，通过调节注意力机制来减少不想要的对象插入。我们通过引入一种复合奖励机制，有效解决了视觉伪影问题，这一方法在处理复杂场景时尤其有效。", "motivation": "产品图像中的水印和其他促销文本是电子商务平台上提供清晰和吸引人的产品视觉的重大障碍。现有的基于扩散的图像修复方法在商业环境中遇到了不可靠的对象去除和领域特定适应性限制等问题。因此，我们提出了解决这些问题的新方法。", "method": "我们提出了一种名为Repainter的强化学习框架，该框架结合了空间遮罩轨迹优化和群组相对策略优化（GRPO），并通过调节注意力机制来强调背景上下文，从而生成高奖励样本并减少不想要的对象插入。另外，我们引入了一种复合奖励机制，平衡全局、局部和语义约束，有效减少视觉伪影和奖励作弊。", "result": "通过广泛的实验，我们证明Repainter在复杂场景下显著优于最先进的方法。", "conclusion": "我们的方法在电子商务产品图像去噪方面取得了显著效果，特别是在图像组成复杂的场景中。"}}
{"id": "2510.07629", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07629", "abs": "https://arxiv.org/abs/2510.07629", "authors": ["Zhangdie Yuan", "Han-Chin Shing", "Mitch Strong", "Chaitanya Shivade"], "title": "Toward Reliable Clinical Coding with Language Models: Verification and Lightweight Adaptation", "comment": null, "summary": "Accurate clinical coding is essential for healthcare documentation, billing,\nand decision-making. While prior work shows that off-the-shelf LLMs struggle\nwith this task, evaluations based on exact match metrics often overlook errors\nwhere predicted codes are hierarchically close but incorrect. Our analysis\nreveals that such hierarchical misalignments account for a substantial portion\nof LLM failures. We show that lightweight interventions, including prompt\nengineering and small-scale fine-tuning, can improve accuracy without the\ncomputational overhead of search-based methods. To address hierarchically\nnear-miss errors, we introduce clinical code verification as both a standalone\ntask and a pipeline component. To mitigate the limitations in existing\ndatasets, such as incomplete evidence and inpatient bias in MIMIC, we release\nan expert double-annotated benchmark of outpatient clinical notes with ICD-10\ncodes. Our results highlight verification as an effective and reliable step\ntoward improving LLM-based medical coding.", "AI": {"tldr": "本文提出了通过轻量级方法提高LLM的临床编码准确性，并引入临床代码验证作为任务和流水线组件。发布了双标注的门诊临床笔记数据集，以减少现有数据集的偏差。", "motivation": "改进现有的LLM模型在临床编码任务中的性能，特别是在解决层级上接近但不正确的编码错误问题。", "method": "通过细致的提示工程和小规模的微调，来提高LLM在临床编码任务中的准确性。提出临床代码验证作为独立任务和流水线组件，用于解决层级上的编码错误问题。", "result": "研究表明，通过引入临床代码验证，可以有效地改进LLM在医学编码中的准确性，而无需增加计算开销。同时，发布了专家双标注的门诊临床笔记数据集，以解决现有数据集的局限性。", "conclusion": "临床代码验证作为有效且可靠的方法，可改进基于LLM的医疗编码准确性，同时提出了一个改进的数据集标准。"}}
{"id": "2510.07723", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07723", "abs": "https://arxiv.org/abs/2510.07723", "authors": ["Wenyue Chen", "Peng Li", "Wangguandong Zheng", "Chengfeng Zhao", "Mengfei Li", "Yaolong Zhu", "Zhiyang Dou", "Ronggang Wang", "Yuan Liu"], "title": "SyncHuman: Synchronizing 2D and 3D Generative Models for Single-view Human Reconstruction", "comment": "NIPS 2025", "summary": "Photorealistic 3D full-body human reconstruction from a single image is a\ncritical yet challenging task for applications in films and video games due to\ninherent ambiguities and severe self-occlusions. While recent approaches\nleverage SMPL estimation and SMPL-conditioned image generative models to\nhallucinate novel views, they suffer from inaccurate 3D priors estimated from\nSMPL meshes and have difficulty in handling difficult human poses and\nreconstructing fine details. In this paper, we propose SyncHuman, a novel\nframework that combines 2D multiview generative model and 3D native generative\nmodel for the first time, enabling high-quality clothed human mesh\nreconstruction from single-view images even under challenging human poses.\nMultiview generative model excels at capturing fine 2D details but struggles\nwith structural consistency, whereas 3D native generative model generates\ncoarse yet structurally consistent 3D shapes. By integrating the complementary\nstrengths of these two approaches, we develop a more effective generation\nframework. Specifically, we first jointly fine-tune the multiview generative\nmodel and the 3D native generative model with proposed pixel-aligned 2D-3D\nsynchronization attention to produce geometrically aligned 3D shapes and 2D\nmultiview images. To further improve details, we introduce a feature injection\nmechanism that lifts fine details from 2D multiview images onto the aligned 3D\nshapes, enabling accurate and high-fidelity reconstruction. Extensive\nexperiments demonstrate that SyncHuman achieves robust and photo-realistic 3D\nhuman reconstruction, even for images with challenging poses. Our method\noutperforms baseline methods in geometric accuracy and visual fidelity,\ndemonstrating a promising direction for future 3D generation models.", "AI": {"tldr": "本文提出了一个结合2D多视角和3D原生生成模型的SyncHuman框架，用于单视角图像的高质量3D人体重建，特别是在具有挑战性姿态的条件下。", "motivation": "当前方法在处理复杂人体姿态和精细化细节重建上存在不足，因此本文提出了一种新的框架，以解决3D人体重建的挑战。", "method": "本文提出了SyncHuman框架，结合了2D多视角生成模型和3D原生生成模型。2D多视角生成模型擅长捕捉精细的2D细节，但结构一致性较差，而3D原生生成模型生成的3D形状粗略但结构一致。通过引入像素对齐的2D-3D同步注意力机制和细节提升机制，该框架能够从单视角图像中生成高质量的人体网格重建，尤其是在具有挑战性的姿态下。", "result": "通过该框架生成的3D人体重建结果不仅在几何准确性上表现良好，而且在视觉保真度上也优于其他基准方法。", "conclusion": "实验结果表明，SyncHuman框架在面对图像具有挑战性姿态时，仍能实现鲁棒和逼真的3D人体重建，并在未来3D生成模型的发展方向上展示了潜力。"}}
{"id": "2510.07642", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07642", "abs": "https://arxiv.org/abs/2510.07642", "authors": ["Đorđe Klisura", "Joseph Khoury", "Ashish Kundu", "Ram Krishnan", "Anthony Rios"], "title": "Role-Conditioned Refusals: Evaluating Access Control Reasoning in Large Language Models", "comment": "8 pages + Appendix", "summary": "Access control is a cornerstone of secure computing, yet large language\nmodels often blur role boundaries by producing unrestricted responses. We study\nrole-conditioned refusals, focusing on the LLM's ability to adhere to access\ncontrol policies by answering when authorized and refusing when not. To\nevaluate this behavior, we created a novel dataset that extends the Spider and\nBIRD text-to-SQL datasets, both of which have been modified with realistic\nPostgreSQL role-based policies at the table and column levels. We compare three\ndesigns: (i) zero or few-shot prompting, (ii) a two-step generator-verifier\npipeline that checks SQL against policy, and (iii) LoRA fine-tuned models that\nlearn permission awareness directly. Across multiple model families, explicit\nverification (the two-step framework) improves refusal precision and lowers\nfalse permits. At the same time, fine-tuning achieves a stronger balance\nbetween safety and utility (i.e., when considering execution accuracy). Longer\nand more complex policies consistently reduce the reliability of all systems.\nWe release RBAC-augmented datasets and code.", "AI": {"tldr": "研究了大型语言模型中遵守访问控制规则的能力，使用了包含角色政策扩增的数据集，并比较了不同设计方法的效果。", "motivation": "大型语言模型常常在访问控制的角色界限上出现模糊，产生不受限的答案。该研究旨在通过评估当模型授权时回答，未经授权时拒绝的模式，来改进这种状况。", "method": "本文研究了在大型语言模型中实施基于角色的访问控制的行为。为此，作者扩展了Spider和BIRD的文本到SQL数据集，并通过添加实际的PostgreSQL角色政策来模拟访问控制环境。比较了三种设计：零或少样本提示、两阶段的生成器-验证器流水线以及使用LoRA微调模型。", "result": "通过对多种模型家族的测试发现，明确的验证步骤（两阶段框架）能提高拒绝精度并减少误判。同时，微调方式能够在安全性和效用性之间取得更好的平衡（即在考虑执行准确性时）。更长和更复杂的政策会降低所有系统的可靠性。", "conclusion": "两步验证体系增强了拒绝精度和减少了误判，而微调模型在安全性和效用性平衡上表现更佳。不过，更长和复杂的政策降低了系统整体可靠性。该研究还公开发布了包含角色增扩的数据集和代码。"}}
{"id": "2510.07729", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07729", "abs": "https://arxiv.org/abs/2510.07729", "authors": ["Jian Gao", "Mengqi Yuan", "Yifei Zeng", "Chang Zeng", "Zhihao Li", "Zhenyu Chen", "Weichao Qiu", "Xiao-Xiao Long", "Hao Zhu", "Xun Cao", "Yao Yao"], "title": "ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral Probes", "comment": null, "summary": "Gaussian Splatting (GS) enables immersive rendering, but realistic 3D\nobject-scene composition remains challenging. Baked appearance and shadow\ninformation in GS radiance fields cause inconsistencies when combining objects\nand scenes. Addressing this requires relightable object reconstruction and\nscene lighting estimation. For relightable object reconstruction, existing\nGaussian-based inverse rendering methods often rely on ray tracing, leading to\nlow efficiency. We introduce Surface Octahedral Probes (SOPs), which store\nlighting and occlusion information and allow efficient 3D querying via\ninterpolation, avoiding expensive ray tracing. SOPs provide at least a 2x\nspeedup in reconstruction and enable real-time shadow computation in Gaussian\nscenes. For lighting estimation, existing Gaussian-based inverse rendering\nmethods struggle to model intricate light transport and often fail in complex\nscenes, while learning-based methods predict lighting from a single image and\nare viewpoint-sensitive. We observe that 3D object-scene composition primarily\nconcerns the object's appearance and nearby shadows. Thus, we simplify the\nchallenging task of full scene lighting estimation by focusing on the\nenvironment lighting at the object's placement. Specifically, we capture a 360\ndegrees reconstructed radiance field of the scene at the location and fine-tune\na diffusion model to complete the lighting. Building on these advances, we\npropose ComGS, a novel 3D object-scene composition framework. Our method\nachieves high-quality, real-time rendering at around 28 FPS, produces visually\nharmonious results with vivid shadows, and requires only 36 seconds for\nediting. Code and dataset are available at\nhttps://nju-3dv.github.io/projects/ComGS/.", "AI": {"tldr": "提出了ComGS框架，用于实现实时高质量的三维对象场景组合，解决了高斯渲染中的光照与阴影不一致问题。", "motivation": "解决Gaussian Splatting在组合对象和场景时由于预设光照和阴影导致的不一致问题，并提升效率。", "method": "引入Surface Octahedral Probes (SOPs)进行高效的重新光照物体重建和简化光照估计任务，专注于物体放置位置的环境光照完成光照补充。", "result": "提出的方法在28 FPS下实现高质量实时渲染，产生视觉和谐的结果，编辑只需36秒。", "conclusion": "ComGS框架实现了高效率和高性能的三维对象场景组合，可以生成现实中的一致性光照和阴影。"}}
{"id": "2510.07645", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07645", "abs": "https://arxiv.org/abs/2510.07645", "authors": ["Xin Jie Chua", "Jeraelyn Ming Li Tan", "Jia Xuan Tan", "Soon Chang Poh", "Yi Xian Goh", "Debbie Hui Tian Choong", "Chee Mun Foong", "Sze Jue Yang", "Chee Seng Chan"], "title": "Banking Done Right: Redefining Retail Banking with Language-Centric AI", "comment": "Accepted at EMNLP2025 Industry Track", "summary": "This paper presents Ryt AI, an LLM-native agentic framework that powers Ryt\nBank to enable customers to execute core financial transactions through natural\nlanguage conversation. This represents the first global regulator-approved\ndeployment worldwide where conversational AI functions as the primary banking\ninterface, in contrast to prior assistants that have been limited to advisory\nor support roles. Built entirely in-house, Ryt AI is powered by ILMU, a\nclosed-source LLM developed internally, and replaces rigid multi-screen\nworkflows with a single dialogue orchestrated by four LLM-powered agents\n(Guardrails, Intent, Payment, and FAQ). Each agent attaches a task-specific\nLoRA adapter to ILMU, which is hosted within the bank's infrastructure to\nensure consistent behavior with minimal overhead. Deterministic guardrails,\nhuman-in-the-loop confirmation, and a stateless audit architecture provide\ndefense-in-depth for security and compliance. The result is Banking Done Right:\ndemonstrating that regulator-approved natural-language interfaces can reliably\nsupport core financial operations under strict governance.", "AI": {"tldr": "本文论述了Ryt AI，这是一个经全球监管机构批准的人工智能代理框架，用于Ryt Bank，使客户能通过自然语言对话方式执行核心金融交易，展示了符合严格监管标准的自然语言接口在金融交易的支持能力。", "motivation": "本文旨在展示Ryt AI作为第一款全球范围内经监管机构批准的面向金融交易的主要银行业务界面，不同于以往仅限于咨询或支持角色的助手。通过这种创新性的方法，实现了自然语言接口在严格监管下的可靠支持。", "method": "本文介绍了Ryt AI，这是一个基于LLM的代理框架，用于Ryt Bank，使客户能够通过自然语言对话执行核心金融交易。该框架完全由内部开发，基于闭源LLM ILMU构建，取代了僵硬的多屏幕工作流，通过四个由LLM驱动的代理（Guardrails、Intent、Payment和FAQ）来完成单一的对话。每个代理都为ILMU附上了特定任务的LoRA适配器，以保证一致性并减少资源消耗。", "result": "通过Ryt AI的实现，展示了经监管机构批准的自然语言接口能够可靠地支持严格监管下的核心金融操作。", "conclusion": "通过Ryt AI的实际应用，证明了自然语言接口在符合严格监管的前提下能够可靠且有效地支持核心金融服务，展示了一种革新性的银行用户体验方式。"}}
{"id": "2510.07741", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07741", "abs": "https://arxiv.org/abs/2510.07741", "authors": ["Yuang Meng", "Xin Jin", "Lina Lei", "Chun-Le Guo", "Chongyi Li"], "title": "UltraLED: Learning to See Everything in Ultra-High Dynamic Range Scenes", "comment": null, "summary": "Ultra-high dynamic range (UHDR) scenes exhibit significant exposure\ndisparities between bright and dark regions. Such conditions are commonly\nencountered in nighttime scenes with light sources. Even with standard exposure\nsettings, a bimodal intensity distribution with boundary peaks often emerges,\nmaking it difficult to preserve both highlight and shadow details\nsimultaneously. RGB-based bracketing methods can capture details at both ends\nusing short-long exposure pairs, but are susceptible to misalignment and\nghosting artifacts. We found that a short-exposure image already retains\nsufficient highlight detail. The main challenge of UHDR reconstruction lies in\ndenoising and recovering information in dark regions. In comparison to the RGB\nimages, RAW images, thanks to their higher bit depth and more predictable noise\ncharacteristics, offer greater potential for addressing this challenge. This\nraises a key question: can we learn to see everything in UHDR scenes using only\na single short-exposure RAW image? In this study, we rely solely on a single\nshort-exposure frame, which inherently avoids ghosting and motion blur, making\nit particularly robust in dynamic scenes. To achieve that, we introduce\nUltraLED, a two-stage framework that performs exposure correction via a ratio\nmap to balance dynamic range, followed by a brightness-aware RAW denoiser to\nenhance detail recovery in dark regions. To support this setting, we design a\n9-stop bracketing pipeline to synthesize realistic UHDR images and contribute a\ncorresponding dataset based on diverse scenes, using only the shortest exposure\nas input for reconstruction. Extensive experiments show that UltraLED\nsignificantly outperforms existing single-frame approaches. Our code and\ndataset are made publicly available at\nhttps://srameo.github.io/projects/ultraled.", "AI": {"tldr": "文章提出UltraLED框架，利用单张短曝光RAW图像有效平衡动态范围并增强暗区域细节恢复，从而克服超高清动态范围场景中的挑战。", "motivation": "现有的RGB图像多重曝光方法易于产生对齐错误和鬼影伪影，而RAW图像具有更高的位深度和更可预测的噪声特性，因此利用单张短曝光RAW图像进行超高清动态范围恢复具有潜力。", "method": "提出UltraLED框架，该框架包含两个阶段：通过比率图进行曝光校正以平衡动态范围，然后使用亮度感知的RAW降噪器增强暗区域细节恢复。该框架利用单张短曝光RAW图像完成超高清动态范围场景的重建。", "result": "实验表明，UltraLED在单帧方法中表现优异。", "conclusion": "通过仅使用单张短曝光图像，UltraLED框架可以在动态场景中显著减少鬼影和运动模糊，有效解决超高清动态范围场景重建的问题。相关代码和数据集已公开。"}}
{"id": "2510.07651", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07651", "abs": "https://arxiv.org/abs/2510.07651", "authors": ["Yuzhe Gu", "Xiyu Liang", "Jiaojiao Zhao", "Enmao Diao"], "title": "OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM Inference", "comment": null, "summary": "Large language models (LLMs) with extended context windows enable powerful\ndownstream applications but impose significant memory overhead, as caching all\nkey-value (KV) states scales linearly with sequence length and batch size.\nExisting cache eviction methods address this by exploiting attention sparsity,\nyet they typically rank tokens heuristically using accumulated attention\nweights without considering their true impact on attention outputs. We propose\nOptimal Brain Cache (OBCache), a principled framework that formulates cache\neviction as a layer-wise structured pruning problem. Building upon the Optimal\nBrain Damage (OBD) theory, OBCache quantifies token saliency by measuring the\nperturbation in attention outputs induced by pruning tokens, with closed-form\nscores derived for isolated keys, isolated values, and joint key-value pairs.\nOur scores account not only for attention weights but also for information from\nvalue states and attention outputs, thereby enhancing existing eviction\nstrategies with output-aware signals. Experiments on LLaMA and Qwen models\ndemonstrate that replacing the heuristic scores in existing works, which\nestimate token saliency across different query positions, with OBCache's\noutput-aware scores consistently improves long-context accuracy.", "AI": {"tldr": "该研究提出了一种新的缓存淘汰策略OBCache，通过测量剪枝令牌对注意力输出的扰动来量化令牌的重要性，实验结果表明这种方法可以提高大语言模型在长上下文中的准确性。", "motivation": "现有的缓存淘汰方法通过利用注意力的稀疏性来解决大语言模型（LLMs）随序列长度和批次大小线性增加的内存开销，但这些方法通常是使用累积注意力权重来对令牌进行启发式排名，而没有考虑它们对注意力输出的真实影响。本研究旨在解决这一问题。", "method": "该研究提出了一种名为OBCache的框架，该框架基于Optimal Brain Damage (OBD)理论，将缓存淘汰策略转化为层的结构剪枝问题。OBCache通过测量剪枝令牌对注意力输出的扰动来量化令牌的重要性，并且直接计算了孤立键、孤立值和联合键值对的分数。该分数不仅考虑了注意力权重，还考虑了值状态和注意力输出的信息，从而在现有的淘汰策略中增加了输出感知信号。", "result": "实验结果表明，在LLaMA和Qwen模型上，使用OBCache的输出感知分数替换现有方法中估计不同查询位置令牌重要性的启发式分数，可以一致地提高长上下文的准确性。", "conclusion": "该研究证明了其提出的OBCache框架在提高大语言模型在长上下文应用场景下的性能方面的有效性。"}}
{"id": "2510.07752", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07752", "abs": "https://arxiv.org/abs/2510.07752", "authors": ["Junhao He", "Jiaxu Wang", "Jia Li", "Mingyuan Sun", "Qiang Zhang", "Jiahang Cao", "Ziyi Zhang", "Yi Gu", "Jingkai Sun", "Renjing Xu"], "title": "DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event Stream", "comment": "Accepted by TVCG", "summary": "Reconstructing Dynamic 3D Gaussian Splatting (3DGS) from low-framerate RGB\nvideos is challenging. This is because large inter-frame motions will increase\nthe uncertainty of the solution space. For example, one pixel in the first\nframe might have more choices to reach the corresponding pixel in the second\nframe. Event cameras can asynchronously capture rapid visual changes and are\nrobust to motion blur, but they do not provide color information. Intuitively,\nthe event stream can provide deterministic constraints for the inter-frame\nlarge motion by the event trajectories. Hence, combining\nlow-temporal-resolution images with high-framerate event streams can address\nthis challenge. However, it is challenging to jointly optimize Dynamic 3DGS\nusing both RGB and event modalities due to the significant discrepancy between\nthese two data modalities. This paper introduces a novel framework that jointly\noptimizes dynamic 3DGS from the two modalities. The key idea is to adopt event\nmotion priors to guide the optimization of the deformation fields. First, we\nextract the motion priors encoded in event streams by using the proposed LoCM\nunsupervised fine-tuning framework to adapt an event flow estimator to a\ncertain unseen scene. Then, we present the geometry-aware data association\nmethod to build the event-Gaussian motion correspondence, which is the primary\nfoundation of the pipeline, accompanied by two useful strategies, namely motion\ndecomposition and inter-frame pseudo-label. Extensive experiments show that our\nmethod outperforms existing image and event-based approaches across synthetic\nand real scenes and prove that our method can effectively optimize dynamic 3DGS\nwith the help of event data.", "AI": {"tldr": "本文提出了一个联合优化RGB和事件数据的框架来改进动态3D高斯喷射的重建。", "motivation": "由于事件摄像头可以异步捕获快速视觉变化且对运动模糊有较强鲁棒性，但无法提供颜色信息，本文旨在通过结合低时间分辨率的RGB图像和高帧率事件流解决动态3DGS重建的挑战，即如何在两个数据模态间显著差异的情况下进行联合优化。", "method": "本文提出了一种新的框架，用于从RGB和事件摄像头数据中联合优化动态3D高斯喷射重建。该框架首先通过提出的LoCM无监督微调框架为未见过的场景调整事件流估计器，以提取事件流中的运动先验。然后，通过几何感知数据关联方法建立事件-高斯运动对应关系，并通过运动分解和帧间伪标签两种策略辅助该对应关系的构建。", "result": "实验结果表明，本文的方法在合成和真实场景中均优于现有的基于图像和事件的方法，能够更有效地优化动态3D高斯喷射。", "conclusion": "实验结果证明，该方法可以有效地利用事件数据优化动态3DGS，并在合成和真实场景中优于现有的基于图像和事件的方法。"}}
{"id": "2510.07662", "categories": ["cs.CL", "cs.CY", "I.2.7; K.4.2"], "pdf": "https://arxiv.org/pdf/2510.07662", "abs": "https://arxiv.org/abs/2510.07662", "authors": ["Virginia K. Felkner", "Allison Lim", "Jonathan May"], "title": "Textual Entailment and Token Probability as Bias Evaluation Metrics", "comment": "16 pages, 9 figures, under ARR review", "summary": "Measurement of social bias in language models is typically by token\nprobability (TP) metrics, which are broadly applicable but have been criticized\nfor their distance from real-world langugage model use cases and harms. In this\nwork, we test natural language inference (NLI) as a more realistic alternative\nbias metric. We show that, curiously, NLI and TP bias evaluation behave\nsubstantially differently, with very low correlation among different NLI\nmetrics and between NLI and TP metrics. We find that NLI metrics are more\nlikely to detect \"underdebiased\" cases. However, NLI metrics seem to be more\nbrittle and sensitive to wording of counterstereotypical sentences than TP\napproaches. We conclude that neither token probability nor natural language\ninference is a \"better\" bias metric in all cases, and we recommend a\ncombination of TP, NLI, and downstream bias evaluations to ensure comprehensive\nevaluation of language models.\n  Content Warning: This paper contains examples of anti-LGBTQ+ stereotypes.", "AI": {"tldr": "研究提出使用自然語言推理（NLI）作為一種新的语言模型偏見評估方法，並發現與传统的按词元概率（TP）评估方法相比，NLI表现出不同的行为模式，但两者都不是在所有情况下都是\"更好的\"偏見評估指標，因此建议结合TP、NLI和其他下游偏見評估方法进行綜合評估。", "motivation": "研究動機是尋找一種更符合實際語言模型應用和損害案例的偏見評估方法，這是因為目前常用的按詞元概率（TP）評估指標雖然適用範圍廣，但被批評與真實世界的應用脫離。", "method": "本研究採用了自然語言推理（NLI）作為替代的偏見評估指標，用於評測語言模型中的社會偏見，並將其與傳統的按詞元概率（TP）評估方法進行比較。", "result": "研究結果表明，NLI評估和TP評估呈現出明顯的不同行為，並且兩者之間的相關性非常低。NLI指標更能夠檢測到\"欠去偏見\"的情形，但對於反立體型句子的詞彙變化比較敏感。", "conclusion": "結論指出，在评估语言模型的偏見時，既有的词元概率（TP）和新的自然语言推理（NLI）方法各有利弊，建议结合两种以及更多其他下游偏見評估方法以确保语言模型的全面评估。"}}
{"id": "2510.07785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07785", "abs": "https://arxiv.org/abs/2510.07785", "authors": ["Ming Jie Ong", "Sze Yinn Ung", "Sim Kuan Goh", "Jimmy Y. Zhong"], "title": "Demystifying Deep Learning-based Brain Tumor Segmentation with 3D UNets and Explainable AI (XAI): A Comparative Analysis", "comment": null, "summary": "The current study investigated the use of Explainable Artificial Intelligence\n(XAI) to improve the accuracy of brain tumor segmentation in MRI images, with\nthe goal of assisting physicians in clinical decision-making. The study focused\non applying UNet models for brain tumor segmentation and using the XAI\ntechniques of Gradient-weighted Class Activation Mapping (Grad-CAM) and\nattention-based visualization to enhance the understanding of these models.\nThree deep learning models - UNet, Residual UNet (ResUNet), and Attention UNet\n(AttUNet) - were evaluated to identify the best-performing model. XAI was\nemployed with the aims of clarifying model decisions and increasing physicians'\ntrust in these models. We compared the performance of two UNet variants\n(ResUNet and AttUNet) with the conventional UNet in segmenting brain tumors\nfrom the BraTS2020 public dataset and analyzed model predictions with Grad-CAM\nand attention-based visualization. Using the latest computer hardware, we\ntrained and validated each model using the Adam optimizer and assessed their\nperformance with respect to: (i) training, validation, and inference times,\n(ii) segmentation similarity coefficients and loss functions, and (iii)\nclassification performance. Notably, during the final testing phase, ResUNet\noutperformed the other models with respect to Dice and Jaccard similarity\nscores, as well as accuracy, recall, and F1 scores. Grad-CAM provided\nvisuospatial insights into the tumor subregions each UNet model focused on\nwhile attention-based visualization provided valuable insights into the working\nmechanisms of AttUNet's attention modules. These results demonstrated ResUNet\nas the best-performing model and we conclude by recommending its use for\nautomated brain tumor segmentation in future clinical assessments. Our source\ncode and checkpoint are available at\nhttps://github.com/ethanong98/MultiModel-XAI-Brats2020", "AI": {"tldr": "研究探讨了在MRI图像中利用可解释的人工智能技术提高脑肿瘤分割的准确性，并通过UNet模型及其变体（ResUNet和AttUNet）的应用，结合XAI技术（Grad-CAM和注意力可视化）提升模型理解和医生信任度。结果表明ResUNet在Dice和Jaccard相似性得分以及准确率、召回率和F1得分上表现最佳，建议在未来临床应用中使用。", "motivation": "该研究的动机是利用XAI技术提高脑肿瘤MRI图像分割的准确性，为临床决策提供帮助。", "method": "使用了UNet、残差UNet(ResUNet)和注意力UNet(AttUNet)三种深度学习模型进行评估，并通过Grad-CAM和注意力可视化技术对模型决策进行解释。", "result": "ResUNet在Dice和Jaccard相似性得分以及准确率、召回率和F1得分上优于其他模型。", "conclusion": "结论指出ResUNet是最佳性能模型，建议在未来的临床评估中使用。"}}
{"id": "2510.07686", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07686", "abs": "https://arxiv.org/abs/2510.07686", "authors": ["Jifan Zhang", "Henry Sleight", "Andi Peng", "John Schulman", "Esin Durmus"], "title": "Stress-Testing Model Specs Reveals Character Differences among Language Models", "comment": null, "summary": "Large language models (LLMs) are increasingly trained from AI constitutions\nand model specifications that establish behavioral guidelines and ethical\nprinciples. However, these specifications face critical challenges, including\ninternal conflicts between principles and insufficient coverage of nuanced\nscenarios. We present a systematic methodology for stress-testing model\ncharacter specifications, automatically identifying numerous cases of principle\ncontradictions and interpretive ambiguities in current model specs.\n  We stress test current model specs by generating scenarios that force\nexplicit tradeoffs between competing value-based principles. Using a\ncomprehensive taxonomy we generate diverse value tradeoff scenarios where\nmodels must choose between pairs of legitimate principles that cannot be\nsimultaneously satisfied. We evaluate responses from twelve frontier LLMs\nacross major providers (Anthropic, OpenAI, Google, xAI) and measure behavioral\ndisagreement through value classification scores. Among these scenarios, we\nidentify over 70,000 cases exhibiting significant behavioral divergence.\nEmpirically, we show this high divergence in model behavior strongly predicts\nunderlying problems in model specifications. Through qualitative analysis, we\nprovide numerous example issues in current model specs such as direct\ncontradiction and interpretive ambiguities of several principles. Additionally,\nour generated dataset also reveals both clear misalignment cases and\nfalse-positive refusals across all of the frontier models we study. Lastly, we\nalso provide value prioritization patterns and differences of these models.", "AI": {"tldr": "本文提出了一种新的方法，用于识别大型语言模型规范内部的价值原则冲突，通过生成各种权衡场景来实现，并检测到了大量分歧结果。", "motivation": "大型语言模型（LLMs）越来越多地从AI准则和模型规范中获得训练，但这些规范存在内部冲突和情境覆盖不足的问题。本研究旨在系统地识别和解决这些问题。", "method": "提出了一种系统的方法来测试模型规范，自动生成各种价值权衡场景，迫使模型在冲突的原则之间进行明确的选择。通过这种方式，可以自动识别出规范中原则的矛盾和解释模糊的情况。", "result": "通过对12个主要提供者（Anthropic、OpenAI、Google、xAI）的前沿LLMs进行测试，研究人员发现了超过70,000个案例，这些案例表现出显著的行为分歧，揭示了模型规范中的问题。", "conclusion": "研究表明，模型的行为分歧强烈预示了模型规范中的潜在问题。研究进一步提供了详细的案例分析，展示了规范中的直接矛盾和解释模糊问题。"}}
{"id": "2510.07791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07791", "abs": "https://arxiv.org/abs/2510.07791", "authors": ["Qinghongbing Xie", "Zhaoyuan Xia", "Feng Zhu", "Lijun Gong", "Ziyue Li", "Rui Zhao", "Long Zeng"], "title": "GTR-Bench: Evaluating Geo-Temporal Reasoning in Vision-Language Models", "comment": "20 pages, 13 figures", "summary": "Recently spatial-temporal intelligence of Visual-Language Models (VLMs) has\nattracted much attention due to its importance for Autonomous Driving, Embodied\nAI and General Artificial Intelligence. Existing spatial-temporal benchmarks\nmainly focus on egocentric perspective reasoning with images/video context, or\ngeographic perspective reasoning with graphics context (eg. a map), thus fail\nto assess VLMs' geographic spatial-temporal intelligence with both images/video\nand graphics context, which is important for areas like traffic management and\nemergency response. To address the gaps, we introduce Geo-Temporal Reasoning\nbenchmark (GTR-Bench), a novel challenge for geographic temporal reasoning of\nmoving targets in a large-scale camera network. GTR-Bench is more challenging\nas it requires multiple perspective switches between maps and videos, joint\nreasoning across multiple videos with non-overlapping fields of view, and\ninference over spatial-temporal regions that are unobserved by any video\ncontext. Evaluations of more than 10 popular VLMs on GTR-Bench demonstrate that\neven the best proprietary model, Gemini-2.5-Pro (34.9%), significantly lags\nbehind human performance (78.61%) on geo-temporal reasoning. Moreover, our\ncomprehensive analysis on GTR-Bench reveals three primary deficiencies of\ncurrent models for geo-temporal reasoning. (1) VLMs' reasoning is impaired by\nan imbalanced utilization of spatial-temporal context. (2) VLMs are weak in\ntemporal forecasting, which leads to worse performance on temporal-emphasized\ntasks than on spatial-emphasized tasks. (3) VLMs lack the proficiency to\ncomprehend or align the map data with multi-view video inputs. We believe\nGTR-Bench offers valuable insights and opens up new opportunities for research\nand applications in spatial-temporal intelligence. Benchmark and code will be\nreleased at https://github.com/X-Luffy/GTR-Bench.", "AI": {"tldr": "本文推出了Geo-Temporal Reasoning基准（GTR-Bench），用于评估视觉语言模型在大规模摄像网络中的地理时空推理能力，结果显示现有模型在这方面显著落后于人类的表现，并揭示了模型在时空推理上的三大不足。", "motivation": "现有时空基准测试侧重于图像或视频背景下基于第一人称或地理视角的推理，缺乏对地图和视频背景综合时空推理能力的评估。为此，本文提出了GTR-Bench来填补这一研究空白，提升对大规模摄像网络中地理时空推理能力的理解。", "method": "GTR-Bench设计了全新的挑战来测试移动目标在大规模摄像网络中的地理时空推理，包括在地图和视频之间进行多视点切换、对非重叠视野多个视频的联合推理以及对未被任何视频所观察到的空间时间区域进行推理。", "result": "对超过10种流行的视觉语言模型进行了评估，即使是最先进的模型，其性能也远低于人类。研究还揭示了现有模型在地理时空推理上的三大主要缺陷。", "conclusion": "GTR-Bench在评估视觉语言模型的地理时空推理方面提供了有价值的见解，为该领域的研究和应用开辟了新机会。基准和代码即将公开。"}}
{"id": "2510.07706", "categories": ["cs.CL", "cs.CE", "cs.LG", "q-bio.CB"], "pdf": "https://arxiv.org/pdf/2510.07706", "abs": "https://arxiv.org/abs/2510.07706", "authors": ["Krinos Li", "Xianglu Xiao", "Shenglong Deng", "Lucas He", "Zijun Zhong", "Yuanjie Zou", "Zhonghao Zhan", "Zheng Hui", "Weiye Bao", "Guang Yang"], "title": "Large Language Models Meet Virtual Cell: A Survey", "comment": null, "summary": "Large language models (LLMs) are transforming cellular biology by enabling\nthe development of \"virtual cells\"--computational systems that represent,\npredict, and reason about cellular states and behaviors. This work provides a\ncomprehensive review of LLMs for virtual cell modeling. We propose a unified\ntaxonomy that organizes existing methods into two paradigms: LLMs as Oracles,\nfor direct cellular modeling, and LLMs as Agents, for orchestrating complex\nscientific tasks. We identify three core tasks--cellular representation,\nperturbation prediction, and gene regulation inference--and review their\nassociated models, datasets, evaluation benchmarks, as well as the critical\nchallenges in scalability, generalizability, and interpretability.", "AI": {"tldr": "论文探讨了大型语言模型（LLMs）在细胞生物学中", "motivation": "大型语言模型正在通过支持“虚拟细胞”的开发，对细胞生物学产生变革性的影响。“虚拟细胞”是一种计算系统，能表示、预测和推测细胞状态和行为。本文旨在对大型语言模型在虚拟细胞建模中的应用进行全面回顾。", "method": "此论文提出了一个将现有方法整理为两种范式的统一分类法：一种是将大型语言模型作为直接进行细胞建模的“预言者”，另一种是将大型语言模型作为“代理”，用于编排复杂的科学任务。", "result": "该研究回顾了相关模型、数据集和评估基准，并指出了关于模型规模、通用性和可解释性方面的关键挑战。", "conclusion": "研究识别了三种核心任务：细胞表示、扰动预测和基因调控推理，指出当前虚拟细胞建模中最关键的挑战在于模型的可扩展性、通用性和可解释性。"}}
{"id": "2510.07810", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07810", "abs": "https://arxiv.org/abs/2510.07810", "authors": ["Luu Tu Nguyen", "Vu Tram Anh Khuong", "Thi Bich Phuong Man", "Thi Duyen Ngo", "Thanh Ha Le"], "title": "FMANet: A Novel Dual-Phase Optical Flow Approach with Fusion Motion Attention Network for Robust Micro-expression Recognition", "comment": null, "summary": "Facial micro-expressions, characterized by their subtle and brief nature, are\nvaluable indicators of genuine emotions. Despite their significance in\npsychology, security, and behavioral analysis, micro-expression recognition\nremains challenging due to the difficulty of capturing subtle facial movements.\nOptical flow has been widely employed as an input modality for this task due to\nits effectiveness. However, most existing methods compute optical flow only\nbetween the onset and apex frames, thereby overlooking essential motion\ninformation in the apex-to-offset phase. To address this limitation, we first\nintroduce a comprehensive motion representation, termed Magnitude-Modulated\nCombined Optical Flow (MM-COF), which integrates motion dynamics from both\nmicro-expression phases into a unified descriptor suitable for direct use in\nrecognition networks. Building upon this principle, we then propose FMANet, a\nnovel end-to-end neural network architecture that internalizes the dual-phase\nanalysis and magnitude modulation into learnable modules. This allows the\nnetwork to adaptively fuse motion cues and focus on salient facial regions for\nclassification. Experimental evaluations on the MMEW, SMIC, CASME-II, and SAMM\ndatasets, widely recognized as standard benchmarks, demonstrate that our\nproposed MM-COF representation and FMANet outperforms existing methods,\nunderscoring the potential of a learnable, dual-phase framework in advancing\nmicro-expression recognition.", "AI": {"tldr": "研究提出了一种新的微表情全面运动表示MM-COF和FMANet神经网络，该方法整合了运动动态的双阶段分析，实验结果表明该方法优于现有技术，具有推动微表情识别进步的潜力。", "motivation": "现有的光学流计算方法主要集中在微表情发生的起始和巅峰帧之间，忽略了巅峰到结束阶段的重要运动信息，为了解决这个问题并提高微表情识别的能力。", "method": "此研究提出了命名为Magnitude-Modulated Combined Optical Flow (MM-COF) 的全面运动表示，该方法将微表情阶段的运动动态整合到统一描述符中，适用于识别网络。基础上，提出了一种名为FMANet的新端到端神经网络架构，该架构将双阶段分析和幅度调制内化为可学习模块，允许网络自适应地融合运动线索并对显着面部区域进行分类。", "result": "实验评价在MMEW、SMIC、CASME-II和SAMM数据集上进行，显示所提出的MM-COF表示和FMANet优于现有方法。", "conclusion": "研究突显了可学习的双阶段框架在推进微表情识别方面的潜力。"}}
{"id": "2510.07707", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07707", "abs": "https://arxiv.org/abs/2510.07707", "authors": ["Chengshuai Zhao", "Shu Wan", "Paras Sheth", "Karan Patwa", "K. Selçuk Candan", "Huan Liu"], "title": "Causality Guided Representation Learning for Cross-Style Hate Speech Detection", "comment": null, "summary": "The proliferation of online hate speech poses a significant threat to the\nharmony of the web. While explicit hate is easily recognized through overt\nslurs, implicit hate speech is often conveyed through sarcasm, irony,\nstereotypes, or coded language -- making it harder to detect. Existing hate\nspeech detection models, which predominantly rely on surface-level linguistic\ncues, fail to generalize effectively across diverse stylistic variations.\nMoreover, hate speech spread on different platforms often targets distinct\ngroups and adopts unique styles, potentially inducing spurious correlations\nbetween them and labels, further challenging current detection approaches.\nMotivated by these observations, we hypothesize that the generation of hate\nspeech can be modeled as a causal graph involving key factors: contextual\nenvironment, creator motivation, target, and style. Guided by this graph, we\npropose CADET, a causal representation learning framework that disentangles\nhate speech into interpretable latent factors and then controls confounders,\nthereby isolating genuine hate intent from superficial linguistic cues.\nFurthermore, CADET allows counterfactual reasoning by intervening on style\nwithin the latent space, naturally guiding the model to robustly identify hate\nspeech in varying forms. CADET demonstrates superior performance in\ncomprehensive experiments, highlighting the potential of causal priors in\nadvancing generalizable hate speech detection.", "AI": {"tldr": "The paper addresses the limitations of existing hate speech detection models by proposing CADET, a causal representation learning framework, which can effectively disentangle and isolate genuine hate intent.", "motivation": "The authors aim to address the challenges posed by implicit hate speech and the issue of spurious correlations that arise with the current hate speech detection approaches.", "method": "CADET is a causal representation learning framework that disentangles hate speech into interpretable latent factors to control confounders and isolate genuine hate intent from superficial linguistic cues. It allows for counterfactual reasoning by intervening on style within the latent space.", "result": "CADET demonstrates superior performance in comprehensive experiments, highlighting its effectiveness in advancing generalizable hate speech detection.", "conclusion": "The use of causal priors within the CADET framework presents a promising direction for the robust and generalizable detection of hate speech, including implicit forms."}}
{"id": "2510.07817", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07817", "abs": "https://arxiv.org/abs/2510.07817", "authors": ["Kanglin Ning", "Ruzhao Chen", "Penghong Wang", "Xingtao Wang", "Ruiqin Xiong", "Xiaopeng Fan"], "title": "An End-to-End Room Geometry Constrained Depth Estimation Framework for Indoor Panorama Images", "comment": null, "summary": "Predicting spherical pixel depth from monocular $360^{\\circ}$ indoor\npanoramas is critical for many vision applications. However, existing methods\nfocus on pixel-level accuracy, causing oversmoothed room corners and noise\nsensitivity. In this paper, we propose a depth estimation framework based on\nroom geometry constraints, which extracts room geometry information through\nlayout prediction and integrates those information into the depth estimation\nprocess through background segmentation mechanism. At the model level, our\nframework comprises a shared feature encoder followed by task-specific decoders\nfor layout estimation, depth estimation, and background segmentation. The\nshared encoder extracts multi-scale features, which are subsequently processed\nby individual decoders to generate initial predictions: a depth map, a room\nlayout map, and a background segmentation map. Furthermore, our framework\nincorporates two strategies: a room geometry-based background depth resolving\nstrategy and a background-segmentation-guided fusion mechanism. The proposed\nroom-geometry-based background depth resolving strategy leverages the room\nlayout and the depth decoder's output to generate the corresponding background\ndepth map. Then, a background-segmentation-guided fusion strategy derives\nfusion weights for the background and coarse depth maps from the segmentation\ndecoder's predictions. Extensive experimental results on the Stanford2D3D,\nMatterport3D and Structured3D datasets show that our proposed methods can\nachieve significantly superior performance than current open-source methods.\nOur code is available at https://github.com/emiyaning/RGCNet.", "AI": {"tldr": "本文提出了一种基于房间几何约束的深度估计框架，解决了现有方法中存在的过度平滑和噪声敏感问题，并在多个数据集上展现了优越性能。", "motivation": "目前的方法在重视像素级精度的同时导致房间角落过度平滑和噪声敏感的问题，本文旨在通过结合房间几何信息来改善深度预测。", "method": "我们的框架包括一个共享特征编码器，随后是针对布局估计、深度估计和背景分割的特定任务解码器。编码器提取多尺度特征，分别由各解码器处理以生成初始预测：深度图、房间布局图和背景分割图。框架还引入了基于房间几何的背景深度解决策略和背景分割引导的融合机制。", "result": "实验结果显示，在Stanford2D3D、Matterport3D 和 Structured3D 数据集上，所提出的方法显著优于现有的开源方法。", "conclusion": "基于房间几何约束的深度估计改进了室内全景图的深度预测，未来可以应用于多种视觉应用中。"}}
{"id": "2510.07713", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07713", "abs": "https://arxiv.org/abs/2510.07713", "authors": ["Shuo Yu", "Mingyue Cheng", "Daoyu Wang", "Qi Liu", "Zirui Liu", "Ze Guo", "Xiaoyu Tao"], "title": "MemWeaver: A Hierarchical Memory from Textual Interactive Behaviors for Personalized Generation", "comment": "12 pages, 8 figures", "summary": "The primary form of user-internet engagement is shifting from leveraging\nimplicit feedback signals, such as browsing and clicks, to harnessing the rich\nexplicit feedback provided by textual interactive behaviors. This shift unlocks\na rich source of user textual history, presenting a profound opportunity for a\ndeeper form of personalization. However, prevailing approaches offer only a\nshallow form of personalization, as they treat user history as a flat list of\ntexts for retrieval and fail to model the rich temporal and semantic structures\nreflecting dynamic nature of user interests. In this work, we propose\n\\textbf{MemWeaver}, a framework that weaves the user's entire textual history\ninto a hierarchical memory to power deeply personalized generation. The core\ninnovation of our memory lies in its ability to capture both the temporal\nevolution of interests and the semantic relationships between different\nactivities. To achieve this, MemWeaver builds two complementary memory\ncomponents that both integrate temporal and semantic information, but at\ndifferent levels of abstraction: behavioral memory, which captures specific\nuser actions, and cognitive memory, which represents long-term preferences.\nThis dual-component memory serves as a unified representation of the user,\nallowing large language models (LLMs) to reason over both concrete behaviors\nand abstracted traits. Experiments on the Language Model Personalization (LaMP)\nbenchmark validate the efficacy of MemWeaver. Our code is\navailable\\footnote{https://github.com/fishsure/MemWeaver}.", "AI": {"tldr": "本文介绍MemWeaver框架，能够捕捉用户兴趣的时序演变和不同活动之间的语义关系，支持深度个性化内容生成。实验验证了其有效性。", "motivation": "研究动机在于，当前仅基于隐式反馈信号如浏览和点击进行用户-网络互动的方式正逐步转向使用丰富的显式反馈，即来自文本互动行为的反馈。这为更深层次的个性化提供了丰富的机会。然而，现有方法只能提供有限的个性化，将用户历史视为一个简单的文本列表用于检索，而无法建模这些丰富的时间和语义结构。", "method": "本研究提出了MemWeaver框架，该框架能够将用户的整个文本历史编织成一个层次记忆结构，以实现深度个性化生成。其核心创新在于能够捕捉兴趣的时序演变和不同活动之间的语义关系。MemWeaver构建了两个互补的记忆组件：行为记忆，捕捉具体用户行为；认知记忆，代表长期偏好。这两种记忆组件共同作为用户统一表示，使得大语言模型（LLMs）能够同时推理具体行为和抽象特性。", "result": "实验在Language Model Personalization (LaMP)基准上验证了MemWeaver框架的有效性。", "conclusion": "本研究证明了MemWeaver框架的有效性，通过构建层次化的记忆结构，能够捕捉用户兴趣的时序和语义关系，从而实现更深度的个性化生成。"}}
{"id": "2510.07823", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07823", "abs": "https://arxiv.org/abs/2510.07823", "authors": ["Shohei Enomoto"], "title": "Enhancing Visual Prompting through Expanded Transformation Space and Overfitting Mitigation", "comment": "Accepted to NeurIPS2025", "summary": "Visual prompting (VP) has emerged as a promising parameter-efficient\nfine-tuning approach for adapting pre-trained vision models to downstream tasks\nwithout modifying model parameters. Despite offering advantages like negligible\ncomputational overhead and compatibility with black-box models, conventional VP\nmethods typically achieve lower accuracy than other adaptation approaches. Our\nanalysis reveals two critical limitations: the restricted expressivity of\nsimple additive transformation and a tendency toward overfitting when the\nparameter count increases. To address these challenges, we propose ACAVP\n(Affine, Color, and Additive Visual Prompting), which enhances VP's expressive\npower by introducing complementary transformation operations: affine\ntransformation for creating task-specific prompt regions while preserving\noriginal image information, and color transformation for emphasizing\ntask-relevant visual features. Additionally, we identify that overfitting is a\ncritical issue in VP training and introduce TrivialAugment as an effective data\naugmentation, which not only benefits our approach but also significantly\nimproves existing VP methods, with performance gains of up to 12 percentage\npoints on certain datasets. This demonstrates that appropriate data\naugmentation is universally beneficial for VP training. Extensive experiments\nacross twelve diverse image classification datasets with two different model\narchitectures demonstrate that ACAVP achieves state-of-the-art accuracy among\nVP methods, surpasses linear probing in average accuracy, and exhibits superior\nrobustness to distribution shifts, all while maintaining minimal computational\noverhead during inference.", "AI": {"tldr": "ACAVP enhances Visual Prompting methods by adding affine and color transformations and using TrivialAugment for data augmentation, achieving state-of-the-art accuracy with minimal computational overhead.", "motivation": "The motivation is to improve the effectiveness of visual prompting methods by addressing their limitations in expressivity and the tendency towards overfitting, while maintaining computational efficiency and compatibility with black-box models.", "method": "Our proposed ACAVP method introduces affine and color transformations to the VP approach to address limitations in expressivity and overfitting. Additionally, it uses TrivialAugment for data augmentation to improve robustness and accuracy.", "result": "ACAVP shows significant performance gains, up to 12 percentage points, on various datasets and demonstrates superior accuracy and robustness compared to standard VP methods.", "conclusion": "ACAVP enhances VP's effectiveness through additional transformations and data augmentation, achieving state-of-the-art results in accuracy and robustness, especially in distribution shifts, with minimal inference overhead."}}
{"id": "2510.07718", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07718", "abs": "https://arxiv.org/abs/2510.07718", "authors": ["Jiaoyang Li", "Junhao Ruan", "Shengwei Tang", "Saihan Chen", "Kaiyan Chang", "Yuan Ge", "Tong Xiao", "Jingbo Zhu"], "title": "SUBQRAG: sub-question driven dynamic graph rag", "comment": "5 pages, 1 figure", "summary": "Graph Retrieval-Augmented Generation (Graph RAG) effectively builds a\nknowledge graph (KG) to connect disparate facts across a large document corpus.\nHowever, this broad-view approach often lacks the deep structured reasoning\nneeded for complex multi-hop question answering (QA), leading to incomplete\nevidence and error accumulation. To address these limitations, we propose\nSubQRAG, a sub-question-driven framework that enhances reasoning depth. SubQRAG\ndecomposes a complex question into an ordered chain of verifiable\nsub-questions. For each sub-question, it retrieves relevant triples from the\ngraph. When the existing graph is insufficient, the system dynamically expands\nit by extracting new triples from source documents in real time. All triples\nused in the reasoning process are aggregated into a \"graph memory,\" forming a\nstructured and traceable evidence path for final answer generation. Experiments\non three multi-hop QA benchmarks demonstrate that SubQRAG achieves consistent\nand significant improvements, especially in Exact Match scores.", "AI": {"tldr": "提出了SubQRAG框架，通过子问题分解和动态扩展知识图来改善多跳问题回答中的深度推理，实验证明其有效性和改进显著。", "motivation": "动机在于解决Graph RAG在复杂多跳问题回答中的深度结构化推理不足的问题，即由于广视角方法导致的证据不完整和错误累积。", "method": "SubQRAG采用子问题驱动的框架来增强推理深度，将复杂的查询分解为可验证的子问题链。对于每个子问题，系统从图中检索相关的三元组，当现有图不足以解决问题时，系统可以实时从源文档中提取新的三元组来动态扩展图。", "result": "实验结果表明，SubQRAG在三个多跳问题回答基准测试中表现出一致且显著的改进，尤其是在精确匹配分值上。", "conclusion": "SubQRAG通过子问题分解和动态扩展图的方法，有效处理了复杂多跳问题回答中的推理深度不足，最终生成了更准确的答案。"}}
{"id": "2510.07828", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07828", "abs": "https://arxiv.org/abs/2510.07828", "authors": ["Kaen Kogashi", "Anoop Cherian", "Meng-Yu Jennifer Kuo"], "title": "MMHOI: Modeling Complex 3D Multi-Human Multi-Object Interactions", "comment": null, "summary": "Real-world scenes often feature multiple humans interacting with multiple\nobjects in ways that are causal, goal-oriented, or cooperative. Yet existing 3D\nhuman-object interaction (HOI) benchmarks consider only a fraction of these\ncomplex interactions. To close this gap, we present MMHOI -- a large-scale,\nMulti-human Multi-object Interaction dataset consisting of images from 12\neveryday scenarios. MMHOI offers complete 3D shape and pose annotations for\nevery person and object, along with labels for 78 action categories and 14\ninteraction-specific body parts, providing a comprehensive testbed for\nnext-generation HOI research. Building on MMHOI, we present MMHOI-Net, an\nend-to-end transformer-based neural network for jointly estimating human-object\n3D geometries, their interactions, and associated actions. A key innovation in\nour framework is a structured dual-patch representation for modeling objects\nand their interactions, combined with action recognition to enhance the\ninteraction prediction. Experiments on MMHOI and the recently proposed CORE4D\ndatasets demonstrate that our approach achieves state-of-the-art performance in\nmulti-HOI modeling, excelling in both accuracy and reconstruction quality.", "AI": {"tldr": "The authors introduce MMHOI, a large, 3D-annotated dataset of human-object interactions, and MMHOI-Net, a transformer-based model that estimates 3D geometries and actions, providing state-of-the-art results.", "motivation": "The motivation is to address the limitation of current 3D human-object interaction (HOI) benchmarks which do not fully capture complex interactions. The authors aim to provide a large-scale dataset and a model that can comprehensively model multi-human multi-object interactions.", "method": "Building on MMHOI, the authors present MMHOI-Net, an end-to-end transformer-based neural network for estimating 3D geometries of human-object interactions, their interactions, and associated actions. A structured dual-patch representation is used for modeling objects and their interactions, which is combined with action recognition to improve interaction prediction.", "result": "Experiments on MMHOI and CORE4D datasets show that the proposed MMHOI-Net achieves state-of-the-art performance, demonstrating superior accuracy and reconstruction quality in multi-human multi-object interaction modeling.", "conclusion": "The proposed MMHOI dataset and the MMHOI-Net model significantly advance the field of multi-human multi-object interaction research by providing a more comprehensive dataset and achieving superior performance in modeling these interactions."}}
{"id": "2510.07736", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07736", "abs": "https://arxiv.org/abs/2510.07736", "authors": ["Cunli Mao", "Xiaofei Gao", "Ran Song", "Shizhu He", "Shengxiang Gao", "Kang Liu", "Zhengtao Yu"], "title": "Multilingual Knowledge Graph Completion via Efficient Multilingual Knowledge Sharing", "comment": "EMNLP 2025, Findings, Long Paper", "summary": "Large language models (LLMs) based Multilingual Knowledge Graph Completion\n(MKGC) aim to predict missing facts by leveraging LLMs' multilingual\nunderstanding capabilities, improving the completeness of multilingual\nknowledge graphs (KGs). However, existing MKGC research underutilizes the\nmultilingual capabilities of LLMs and ignores the shareability of cross-lingual\nknowledge. In this paper, we propose a novel MKGC framework that leverages\nmultilingual shared knowledge to significantly enhance performance through two\ncomponents: Knowledge-level Grouped Mixture of Experts (KL-GMoE) and Iterative\nEntity Reranking (IER). KL-GMoE efficiently models shared knowledge, while IER\nsignificantly enhances its utilization. To evaluate our framework, we\nconstructed a mKG dataset containing 5 languages and conducted comprehensive\ncomparative experiments with existing state-of-the-art (SOTA) MKGC method. The\nexperimental results demonstrate that our framework achieves improvements of\n5.47%, 3.27%, and 1.01% in the Hits@1, Hits@3, and Hits@10 metrics,\nrespectively, compared with SOTA MKGC method. Further experimental analysis\nrevealed the properties of knowledge sharing in settings of unseen and\nunbalanced languages. We have released the dataset and code for our work on\nhttps://github.com/gaoxiaofei07/KL-GMoE.", "AI": {"tldr": "A new MKGC framework utilizing a novel architecture combining KL-GMoE and IER to significantly enhance the performance of multilingual knowledge graph completion.", "motivation": "The research aims to overcome limitations in existing MKGC approaches by better utilizing the multilingual capabilities of large language models and promoting the shareability of cross-lingual knowledge.", "method": "The paper introduces a new MKGC framework, which comprises two components: Knowledge-level Grouped Mixture of Experts (KL-GMoE) and Iterative Entity Reranking (IER). The KL-GMoE models the shared multilingual knowledge efficiently, and the IER optimizes the utilization of this knowledge.", "result": "The research demonstrates an enhancement of MKGC with improvements up to 5.47% in the Hits@1 metric over SOTA methods, and provides a new dataset for evaluation and further research.", "conclusion": "The proposed framework advances the state of the art in MKGC by leveraging multilingual shared knowledge, achieving significant improvements in various evaluation metrics. It also offers insights into the challenges and properties of knowledge sharing across different linguistic settings."}}
{"id": "2510.07830", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07830", "abs": "https://arxiv.org/abs/2510.07830", "authors": ["Houqiang Zhong", "Zhenglong Wu", "Sihua Fu", "Zihan Zheng", "Xin Jin", "Xiaoyun Zhang", "Li Song", "Qiang Hu"], "title": "PrismGS: Physically-Grounded Anti-Aliasing for High-Fidelity Large-Scale 3D Gaussian Splatting", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has recently enabled real-time photorealistic\nrendering in compact scenes, but scaling to large urban environments introduces\nsevere aliasing artifacts and optimization instability, especially under\nhigh-resolution (e.g., 4K) rendering. These artifacts, manifesting as\nflickering textures and jagged edges, arise from the mismatch between Gaussian\nprimitives and the multi-scale nature of urban geometry. While existing\n``divide-and-conquer'' pipelines address scalability, they fail to resolve this\nfidelity gap. In this paper, we propose PrismGS, a physically-grounded\nregularization framework that improves the intrinsic rendering behavior of 3D\nGaussians. PrismGS integrates two synergistic regularizers. The first is\npyramidal multi-scale supervision, which enforces consistency by supervising\nthe rendering against a pre-filtered image pyramid. This compels the model to\nlearn an inherently anti-aliased representation that remains coherent across\ndifferent viewing scales, directly mitigating flickering textures. This is\ncomplemented by an explicit size regularization that imposes a\nphysically-grounded lower bound on the dimensions of the 3D Gaussians. This\nprevents the formation of degenerate, view-dependent primitives, leading to\nmore stable and plausible geometric surfaces and reducing jagged edges. Our\nmethod is plug-and-play and compatible with existing pipelines. Extensive\nexperiments on MatrixCity, Mill-19, and UrbanScene3D demonstrate that PrismGS\nachieves state-of-the-art performance, yielding significant PSNR gains around\n1.5 dB against CityGaussian, while maintaining its superior quality and\nrobustness under demanding 4K rendering.", "AI": {"tldr": "PrismGS通过引入基于物理的正则化框架解决了3D高斯点在渲染高分辨率城市环境时的问题，通过分层多尺度监督和显式尺寸正则化改善了渲染效果，减少视觉瑕疵。", "motivation": "目前的3D高斯点渲染存在严重的闪烁纹理和锯齿状边缘问题，特别是在高分辨率渲染（如4K）中。这些问题是由高斯基元和多尺度城市几何之间的不匹配引起的。现有的“分治”管道无法解决这一保真度差距。", "method": "PrismGS提出了一个基于物理的正则化框架，以改善3D高斯点的内在渲染行为。PrismGS集成了两个协同的正则化器：分层多尺度监督和显式尺寸正则化。分层多尺度监督通过与预过滤图像金字塔的监督来强制渲染的一致性，从而消除闪烁的纹理。显式尺寸正则化从物理上确保了3D高斯点的尺寸下限，防止出现退化的视图依赖原始点，减少锯齿状边缘。", "result": "实验显示，PrismGS在MatrixCity、Mill-19和UrbanScene3D数据集上表现出业界领先的能力，与CityGaussian相比，PrismGS在4K渲染下提供了约1.5dB的PSNR增益，同时保持了出色的稳定性和质量。", "conclusion": "PrismGS是一个即插即用且与现有渲染管线兼容的方法，能显著减轻3D高斯点渲染中的闪烁纹理和锯齿边现象，尤其适用于高分辨率渲染。"}}
{"id": "2510.07737", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07737", "abs": "https://arxiv.org/abs/2510.07737", "authors": ["Fu Chen", "Peng Wang", "Xiyin Li", "Wen Li", "Shichi Lei", "Dongdong Xiang"], "title": "ToolExpander: Extending the Frontiers of Tool-Using Reinforcement Learning to Weak LLMs", "comment": null, "summary": "Training Large Language Models (LLMs) with Group Relative Policy Optimization\n(GRPO) encounters a significant challenge: models often fail to produce\naccurate responses, particularly in small-scale architectures. This limitation\nnot only diminishes performance improvements and undermines the potential of\nGRPO but also frequently leads to mid-training collapse, adversely affecting\nstability and final efficacy. To address these issues, we propose ToolExpander,\na novel framework that advances tool-oriented reinforcement learning for\nresource-constrained LLMs through two key innovations:(1) Dynamic Multi-Round\nHard Sampling, which dynamically substitutes challenging samples(those without\ncorrect outputs over 10 rollouts) with high-quality few-shot demonstrations\nduring training, coupled with an exponential learning rate decay strategy to\nmitigate oscillations;(2) Self-Exemplifying Thinking, an enhanced GRPO\nframework that eliminates KL divergence and incorporates adjusted clipping\ncoefficients, encouraging models to autonomously generate and analyze few-shot\nexamples via a minimal additional reward (0.01).Experimental results\ndemonstrate that ToolExpander significantly enhances tool-using capabilities in\nLLMs, especially in weaker small-scale models, improving both training\nstability and overall performance.", "AI": {"tldr": "The paper introduces ToolExpander, a new framework that enhances tool-oriented reinforcement learning for resource-constrained Large Language Models (LLMs) by addressing instability and performance issues during training.", "motivation": "The motivation is to overcome the common challenges faced during the training of LLMs with Group Relative Policy Optimization (GRPO), including poor response accuracy, especially in small-scale models, and mid-training collapse.", "method": "ToolExpander introduces Dynamic Multi-Round Hard Sampling and Self-Exemplifying Thinking. The first involves substituting difficult samples with high-quality demonstrations while applying an exponential learning rate decay. The latter enhances GRPO by removing KL divergence and adding adjusted clipping coefficients, promoting autonomous generation and analysis of few-shot examples.", "result": "The results show that ToolExpander significantly improves tool-using capabilities and contributes to higher stability and performance in LLMs, particularly in smaller-scale architectures.", "conclusion": "The conclusion is that ToolExpander effectively addresses the key challenges in training LLMs with GRPO, leading to better stability and performance, especially in small-scale models with limited resources."}}
{"id": "2510.07837", "categories": ["cs.CV", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.07837", "abs": "https://arxiv.org/abs/2510.07837", "authors": ["Harsh Kavediya", "Vighnesh Nayak", "Bheeshm Sharma", "Balamurugan Palaniappan"], "title": "IsoSignVid2Aud: Sign Language Video to Audio Conversion without Text Intermediaries", "comment": "Accepted in AIML-Systems-2025", "summary": "Sign language to spoken language audio translation is important to connect\nthe hearing- and speech-challenged humans with others. We consider sign\nlanguage videos with isolated sign sequences rather than continuous grammatical\nsigning. Such videos are useful in educational applications and sign prompt\ninterfaces. Towards this, we propose IsoSignVid2Aud, a novel end-to-end\nframework that translates sign language videos with a sequence of possibly\nnon-grammatic continuous signs to speech without requiring intermediate text\nrepresentation, providing immediate communication benefits while avoiding the\nlatency and cascading errors inherent in multi-stage translation systems. Our\napproach combines an I3D-based feature extraction module with a specialized\nfeature transformation network and an audio generation pipeline, utilizing a\nnovel Non-Maximal Suppression (NMS) algorithm for the temporal detection of\nsigns in non-grammatic continuous sequences. Experimental results demonstrate\ncompetitive performance on ASL-Citizen-1500 and WLASL-100 datasets with Top-1\naccuracies of 72.01\\% and 78.67\\%, respectively, and audio quality metrics\n(PESQ: 2.67, STOI: 0.73) indicating intelligible speech output. Code is\navailable at: https://github.com/BheeshmSharma/IsoSignVid2Aud_AIMLsystems-2025.", "AI": {"tldr": "本文介绍了一种新的方法——IsoSignVid2Aud，用于将孤立的签语视频序列直接转换为语音，不需中间文本表示，有效减少了多阶段翻译系统的延迟和级联错误，同时产生音质良好的语音输出。", "motivation": "目的是连接聋哑人士和其他人，特别是在教育应用和签语提示界面中的应用，这类视频能够提供即时沟通的好处。", "method": "提出IsoSignVid2Aud，这是一个端到端框架，它利用I3D特征提取模块、专门的特征转换网络和音频生成管道，结合新提出的非极大值抑制（NMS）算法，用于在非语法连续签语序列中的时间检测。", "result": "实验结果表明，该方法在ASL-Citizen-1500和WLASL-100数据集上分别达到了72.01%和78.67%的Top-1准确率，PESQ和STOI指标分别为2.67和0.73，表明输出的语音具有可理解性。", "conclusion": "IsoSignVid2Aud这一端到端框架实现了从孤立签语视频到语音的直接转换，能够提供即时有效的沟通同时避免译码过程中的延迟和错误。"}}
{"id": "2510.07743", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07743", "abs": "https://arxiv.org/abs/2510.07743", "authors": ["Tianci Liu", "Ran Xu", "Tony Yu", "Ilgee Hong", "Carl Yang", "Tuo Zhao", "Haoyu Wang"], "title": "OpenRubrics: Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment", "comment": "The first two authors contributed equally", "summary": "Reward modeling lies at the core of reinforcement learning from human\nfeedback (RLHF), yet most existing reward models rely on scalar or pairwise\njudgments that fail to capture the multifaceted nature of human preferences.\nRecent studies have explored rubrics-as-rewards (RaR) that uses structured\nnatural language criteria that capture multiple dimensions of response quality.\nHowever, producing rubrics that are both reliable and scalable remains a key\nchallenge. In this work, we introduce OpenRubrics, a diverse, large-scale\ncollection of (prompt, rubric) pairs for training rubric-generation and\nrubric-based reward models. To elicit discriminative and comprehensive\nevaluation signals, we introduce Contrastive Rubric Generation (CRG), which\nderives both hard rules (explicit constraints) and principles (implicit\nqualities) by contrasting preferred and rejected responses. We further improve\nreliability by enforcing preference-label consistency via rejection sampling to\nremove noisy rubrics. Across multiple reward-modeling benchmarks, our\nrubric-based reward model, Rubric-RM, surpasses strong size-matched baselines\nby 6.8%. These gains transfer to policy models on instruction-following and\nbiomedical benchmarks. Our results show that rubrics provide scalable alignment\nsignals that narrow the gap between costly human evaluation and automated\nreward modeling, enabling a new principle-driven paradigm for LLM alignment.", "AI": {"tldr": "本研究介绍了OpenRubrics和对比式Rubric生成方法，以解决奖励模型中的人类偏好多维度捕捉问题，新的方法显著提高了奖励模型性能。", "motivation": "现有的奖励模型主要依赖于标量或成对判断，无法捕捉人类偏好的多维度特性。本研究旨在解决可靠和可扩展的评价标准生成问题，因此引入了OpenRubrics和对比式rubric生成方法。", "method": "提出了一种名为OpenRubrics的多样且大规模的(prompt, rubric)对集合，用于训练rubric生成和基于rubric的奖励模型。通过对比优选和拒绝的回答，引入了对比式rubric生成(CRG)方法，能够区分并全面地评估信号。同时，通过拒绝采样去除噪音rubric，以提高可靠性。", "result": "基于rubric的奖励模型Rubric-RM，在多个奖励模型基准上，超越了同等规模的竞争者6.8%。这些改进同样适用于指令遵循和生物医学基准的策略模型。", "conclusion": "研究表明，基于rubric的方法为LLM的对齐提供了可扩展的对齐信号，缩小了昂贵的人类评估与自动化奖励建模之间的差距，并开启了新的基于原理的对齐范式。"}}
{"id": "2510.07839", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07839", "abs": "https://arxiv.org/abs/2510.07839", "authors": ["Yijie Gao", "Houqiang Zhong", "Tianchi Zhu", "Zhengxue Cheng", "Qiang Hu", "Li Song"], "title": "AlignGS: Aligning Geometry and Semantics for Robust Indoor Reconstruction from Sparse Views", "comment": null, "summary": "The demand for semantically rich 3D models of indoor scenes is rapidly\ngrowing, driven by applications in augmented reality, virtual reality, and\nrobotics. However, creating them from sparse views remains a challenge due to\ngeometric ambiguity. Existing methods often treat semantics as a passive\nfeature painted on an already-formed, and potentially flawed, geometry. We\nposit that for robust sparse-view reconstruction, semantic understanding\ninstead be an active, guiding force. This paper introduces AlignGS, a novel\nframework that actualizes this vision by pioneering a synergistic, end-to-end\noptimization of geometry and semantics. Our method distills rich priors from 2D\nfoundation models and uses them to directly regularize the 3D representation\nthrough a set of novel semantic-to-geometry guidance mechanisms, including\ndepth consistency and multi-faceted normal regularization. Extensive\nevaluations on standard benchmarks demonstrate that our approach achieves\nstate-of-the-art results in novel view synthesis and produces reconstructions\nwith superior geometric accuracy. The results validate that leveraging semantic\npriors as a geometric regularizer leads to more coherent and complete 3D models\nfrom limited input views. Our code is avaliable at\nhttps://github.com/MediaX-SJTU/AlignGS .", "AI": {"tldr": "AlignGS is a novel framework that aims to improve 3D reconstruction by integrating semantic understanding proactively throughout the reconstruction process from sparse views.", "motivation": "The motivation stems from the difficulty of creating semantically rich 3D models from limited views due to geometric ambiguity. Existing methods usually consider semantics as a passive feature, but this paper suggests semantics should actively guide the reconstruction process for better results.", "method": "The paper introduces AlignGS, a framework that simultaneously optimizes geometry and semantics for 3D reconstruction from sparse views. It uses semantic priors derived from 2D foundation models to guide the 3D reconstruction process through several mechanisms such as depth consistency and normal regularization.", "result": "The method demonstrates state-of-the-art performance in novel view synthesis and geometric accuracy, showing that leveraging semantic priors as geometric regularizers leads to more coherent 3D models from limited input views.", "conclusion": "The research underlines the importance of using semantics as an active guiding force for 3D reconstruction, leading to superior models compared to traditional methods that use semantics passively."}}
{"id": "2510.07745", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07745", "abs": "https://arxiv.org/abs/2510.07745", "authors": ["Runyang You", "Yongqi Li", "Meng Liu", "Wenjie Wang", "Liqiang Nie", "Wenjie Li"], "title": "Parallel Test-Time Scaling for Latent Reasoning Models", "comment": null, "summary": "Parallel test-time scaling (TTS) is a pivotal approach for enhancing large\nlanguage models (LLMs), typically by sampling multiple token-based\nchains-of-thought in parallel and aggregating outcomes through voting or\nsearch. Recent advances in latent reasoning, where intermediate reasoning\nunfolds in continuous vector spaces, offer a more efficient alternative to\nexplicit Chain-of-Thought, yet whether such latent models can similarly benefit\nfrom parallel TTS remains open, mainly due to the absence of sampling\nmechanisms in continuous space, and the lack of probabilistic signals for\nadvanced trajectory aggregation. \\ This work enables parallel TTS for latent\nreasoning models by addressing the above issues. For sampling, we introduce two\nuncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive\nGaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM)\ntrained with step-wise contrastive objective to score and guide latent\nreasoning. Extensive experiments and visualization analyses show that both\nsampling strategies scale effectively with compute and exhibit distinct\nexploration dynamics, while LatentRM enables effective trajectory selection.\nTogether, our explorations open a new direction for scalable inference in\ncontinuous spaces. Code released at https://github.com/YRYangang/LatentTTS.", "AI": {"tldr": "该论文研究使潜推理模型能够利用并行测试时间缩放技术，通过引入新的采样方法和潜奖励模型解决了连续空间中缺乏采样和概率信号的问题。", "motivation": "现有的隐式推理方法缺乏采样机制和概率信号，使得难以在连续空间中实现并行的测试时间缩放(TTS)。这项研究旨在解决这些问题并使潜推理模型也能从并行TTS中受益。", "method": "该研究引入了两种基于不确定性的随机策略（蒙特卡洛丢弃法和加性高斯噪声）来应对采样问题，并设计了一个基于逐步对比目标训练的潜奖励模型（LatentRM）来评分和引导潜推理的聚合过程。", "result": "实验和可视化分析表明，两种采样策略都能随着计算量的增加而有效扩展，并表现出独特的探索动态；而潜奖励模型使得轨迹选择更有效。", "conclusion": "通过这些探索，研究开启了一个新的方向，使得在连续空间中的可扩展推理成为可能。"}}
{"id": "2510.07853", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07853", "abs": "https://arxiv.org/abs/2510.07853", "authors": ["Thomas Lautenschlager", "Nils Friederich", "Angelo Jovin Yamachui Sitcheu", "Katja Nau", "Gaëlle Hayot", "Thomas Dickmeis", "Ralf Mikut"], "title": "Self-Supervised Learning Strategies for a Platform to Test the Toxicity of New Chemicals and Materials", "comment": null, "summary": "High-throughput toxicity testing offers a fast and cost-effective way to test\nlarge amounts of compounds. A key component for such systems is the automated\nevaluation via machine learning models. In this paper, we address critical\nchallenges in this domain and demonstrate how representations learned via\nself-supervised learning can effectively identify toxicant-induced changes. We\nprovide a proof-of-concept that utilizes the publicly available EmbryoNet\ndataset, which contains ten zebrafish embryo phenotypes elicited by various\nchemical compounds targeting different processes in early embryonic\ndevelopment. Our analysis shows that the learned representations using\nself-supervised learning are suitable for effectively distinguishing between\nthe modes-of-action of different compounds. Finally, we discuss the integration\nof machine learning models in a physical toxicity testing device in the context\nof the TOXBOX project.", "AI": {"tldr": "本文通过自监督学习方法研究了高通量毒性测试的问题，展示了该方法在识别毒物诱导变化的有效性，并探讨了其在物理设备中的应用。", "motivation": "本文的动机在于解决高通量毒性测试领域中的关键挑战，并且展示自监督学习方法在识别毒物诱导变化的有效性。", "method": "本文采用自监督学习方法来学习表示，以区分不同化合物的作用方式，并使用公开的EmbryoNet数据集作为概念验证。", "result": "分析结果表明，通过自监督学习获得的表示能够有效地区分不同化合物的作用方式。", "conclusion": "最后，本文讨论了在TOXBOX项目中将机器学习模型集成到物理毒性测试设备中的可能性。"}}
{"id": "2510.07761", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07761", "abs": "https://arxiv.org/abs/2510.07761", "authors": ["Nishant Balepur", "Atrey Desai", "Rachel Rudinger"], "title": "Test-Time Reasoners Are Strategic Multiple-Choice Test-Takers", "comment": "In-progress Preprint", "summary": "Large language models (LLMs) now give reasoning before answering, excelling\nin tasks like multiple-choice question answering (MCQA). Yet, a concern is that\nLLMs do not solve MCQs as intended, as work finds LLMs sans reasoning succeed\nin MCQA without using the question, i.e., choices-only. Such partial-input\nsuccess is often deemed problematic, but reasoning traces could reveal if these\nstrategies are truly shallow in choices-only settings. To study these\nstrategies, reasoning LLMs solve MCQs in full and choices-only inputs;\ntest-time reasoning often boosts accuracy on full and in choices-only half the\ntime. While possibly due to shallow shortcuts, choices-only success is barely\naffected by the length of reasoning traces, and after finding traces pass\nfaithfulness tests, we show they use less problematic strategies like inferring\nmissing questions. In all, we challenge claims that partial-input success is\nalways a flaw, so we discuss how reasoning traces could separate problematic\ndata from less problematic reasoning.", "AI": {"tldr": "研究发现，推理能力的大语言模型（LLMs）在解决MCQA问题中，即使在仅使用选项的情况下也表现良好，且这种成功并不总是由于浅层策略导致。通过推理轨迹可以更准确地评估这些模型在部分输入情况下的表现。", "motivation": "动机在于探索LLMs在使用仅选项输入（choices-only）进行MCQA时的成功是否本质上是浅层策略导致的，以及这些策略是否存在问题。", "method": "研究使用带有推理能力的大语言模型（LLMs）来解决包含全部输入和仅选项输入的多项选择题（MCQs），并通过测试时间推理来提升其准确率。通过比较不同长度的推理轨迹，以及通过可靠性测试来评估这些轨迹的有效性。", "result": "研究发现，虽然带有推理能力的LLMs在full input上的准确率有所提升，但在choices-only input上则只有一半的情况得到了改进。结果还显示出，仅选项输入的成功几乎不受推理轨迹长度的影响，并且通过了可靠性测试的推理轨迹使用了较少问题的策略，如推断缺失问题。", "conclusion": "该研究挑战了部分输入成功总是缺陷的观点，并探讨了如何通过推理轨迹来区分存在问题的数据和较少存在问题的推理。"}}
{"id": "2510.07856", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07856", "abs": "https://arxiv.org/abs/2510.07856", "authors": ["Haochen Yu", "Qiankun Liu", "Hongyuan Liu", "Jianfei Jiang", "Juntao Lyu", "Jiansheng Chen", "Huimin Ma"], "title": "XYZCylinder: Feedforward Reconstruction for Driving Scenes Based on A Unified Cylinder Lifting Method", "comment": "Project page: https://yuyuyu223.github.io/XYZCYlinder-projectpage/", "summary": "Recently, more attention has been paid to feedforward reconstruction\nparadigms, which mainly learn a fixed view transformation implicitly and\nreconstruct the scene with a single representation. However, their\ngeneralization capability and reconstruction accuracy are still limited while\nreconstructing driving scenes, which results from two aspects: (1) The fixed\nview transformation fails when the camera configuration changes, limiting the\ngeneralization capability across different driving scenes equipped with\ndifferent camera configurations. (2) The small overlapping regions between\nsparse views of the $360^\\circ$ panorama and the complexity of driving scenes\nincrease the learning difficulty, reducing the reconstruction accuracy. To\nhandle these difficulties, we propose \\textbf{XYZCylinder}, a feedforward model\nbased on a unified cylinder lifting method which involves camera modeling and\nfeature lifting. Specifically, to improve the generalization capability, we\ndesign a Unified Cylinder Camera Modeling (UCCM) strategy, which avoids the\nlearning of viewpoint-dependent spatial correspondence and unifies different\ncamera configurations with adjustable parameters. To improve the reconstruction\naccuracy, we propose a hybrid representation with several dedicated modules\nbased on newly designed Cylinder Plane Feature Group (CPFG) to lift 2D image\nfeatures to 3D space. Experimental results show that XYZCylinder achieves\nstate-of-the-art performance under different evaluation settings, and can be\ngeneralized to other driving scenes in a zero-shot manner. Project page:\n\\href{https://yuyuyu223.github.io/XYZCYlinder-projectpage/}{here}.", "AI": {"tldr": "XYZCylinder is proposed to overcome the limitations of generalizing to different camera configurations and reconstructing driving scenes accurately using a cylinder-based approach.", "motivation": "Addressing issues with generalization capability and reconstruction accuracy in feedforward reconstruction paradigms, particularly for dynamic driving scenes.", "method": "A feedforward model named XYZCylinder is introduced, leveraging a unified cylinder lifting method that includes camera modeling (Unified Cylinder Camera Modeling, UCCM) and feature lifting (Cylinder Plane Feature Group, CPFG).", "result": "XYZCylinder achieves superior performance in driving scene reconstruction across various evaluation settings and can generalize to new scenes without additional training.", "conclusion": "The proposed method demonstrates high accuracy and strong generalization capability, making it a state-of-the-art method for driving scene reconstruction using a unified cylinder approach."}}
