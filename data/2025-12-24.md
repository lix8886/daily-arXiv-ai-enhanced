<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 6]
- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data](https://arxiv.org/abs/2512.19864)
*Shashi Kant Gupta,Arijeet Pramanik,Jerrin John Thomas,Regina Schwind,Lauren Wiener,Avi Raju,Jeremy Kornbluth,Yanshan Wang,Zhaohui Su,Hrituraj Singh*

Main category: cs.CL

> 开发了一种使用语言模型自动化从电子健康记录的非结构化笔记中提取结构化肿瘤学数据的框架，显著提高了精度并降低了成本。

<details>
  <summary>Details</summary>

**Motivation:** 提取结构化肿瘤学数据具有挑战性，传统手动提取准确但成本高且不可扩展，而现有自动化方法通常只能处理狭窄场景。

**Method:** 使用大型语言模型(LLM)作为推理代理，这些代理配备了上下文敏感的检索和迭代合成能力，以从真实世界的肿瘤学笔记中详尽且全面地提取结构化临床变量。

**Result:** 该方法在一个包含超过400,000份非结构化临床笔记和2,250名癌症患者的大型数据集上进行了评估，取得了平均F1评分为0.93，103个肿瘤学特定临床变量中的100个超出0.85，关键变量超过了0.95的成绩。此外，该系统的集成大幅降低了注释成本。

**Conclusion:** 这项研究首次实现了基于LLM代理的肿瘤学数据提取的详尽和端到端的应用。

**Abstract:** Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale

</details>


### [2] [How well do Large Language Models Recognize Instructional Moves? Establishing Baselines for Foundation Models in Educational Discourse](https://arxiv.org/abs/2512.19903)
*Kirk Vanacore,Rene F. Kizilcec*

Main category: cs.CL

> 研究评估了六种大型语言模型在未经定制情况下分类教育对话片段的能力。少量样本提示显著提高了性能，最高达到0.58的Cohen's Kappa值，但模型能力仍存在局限性。

<details>
  <summary>Details</summary>

**Motivation:** 这项工作旨在探索不经过专门定制的大型语言模型在实际教育场景中的解释能力，以帮助确立未来应用的期望值和基准。随着语言模型技术在教育技术中的广泛应用，了解其原生能力变得至关重要。

**Method:** 本研究通过比较六种大型语言模型在分类真实课堂对话片段中的教学行为任务上的表现来评估它们的基础性能。研究采用了三种提示方法：零样本、单样本和少量样本提示。

**Result:** 研究发现，在零样本情况下，模型表现中等，而提供综合示例的少量样本提示显著提高了模型的性能，最高配置达到了与专家标注的Cohen's Kappa = 0.58的吻合度。然而，性能提升并不均匀，不同的教学行为表现差异很大，同时高召回率常常带来更高的误报率。

**Conclusion:** 总体而言，这些发现揭示了基础模型在理解和解释教学对话方面既有意义又有局限性，优化的提示设计可以揭示模型的能力，但不能完全克服其基本的可靠性限制。

**Abstract:** Large language models (LLMs) are increasingly adopted in educational technologies for a variety of tasks, from generating instructional materials and assisting with assessment design to tutoring. While prior work has investigated how models can be adapted or optimized for specific tasks, far less is known about how well LLMs perform at interpreting authentic educational scenarios without significant customization. As LLM-based systems become widely adopted by learners and educators in everyday academic contexts, understanding their out-of-the-box capabilities is increasingly important for setting expectations and benchmarking. We compared six LLMs to estimate their baseline performance on a simple but important task: classifying instructional moves in authentic classroom transcripts. We evaluated typical prompting methods: zero-shot, one-shot, and few-shot prompting. We found that while zero-shot performance was moderate, providing comprehensive examples (few-shot prompting) significantly improved performance for state-of-the-art models, with the strongest configuration reaching Cohen's Kappa = 0.58 against expert-coded annotations. At the same time, improvements were neither uniform nor complete: performance varied considerably by instructional move, and higher recall frequently came at the cost of increased false positives. Overall, these findings indicate that foundation models demonstrate meaningful yet limited capacity to interpret instructional discourse, with prompt design helping to surface capability but not eliminating fundamental reliability constraints.

</details>


### [3] [Counterfactual LLM-based Framework for Measuring Rhetorical Style](https://arxiv.org/abs/2512.19908)
*Jingyi Qiu,Hong Chen,Zongyi Li*

Main category: cs.CL

> 研究通过一个基于LLM的反事实框架量化了机器学习论文中的修辞风格，并发现使用LLM辅助写作导致修辞力度显著增加，表明LLMs可用于科学评估的改进。

<details>
  <summary>Details</summary>

**Motivation:** 解决量化修辞风格而不依赖内容的问题，旨在区分大胆语言是来自强大的实验结果还是仅仅是修辞风格。

**Method:** 提出了一个基于LLM的反事实框架，使用多个LLM修辞角色从相同的内容生成反事实写作，并通过LLM评判者进行成对评价，最终使用Bradley-Terry模型聚合结果。

**Result:** 发现了具有前瞻性的框架能显著预测下游关注度，包括引用次数和媒体关注度，即使在控制了同行评审评价后也是如此。此外，观察到2023年后修辞力度急剧上升，并提供了实证证据，表明这种增加主要是由LLM辅助写作的采用所驱动。

**Conclusion:** 该研究表明，LLMs可以作为衡量和改善科学评估的工具，而框架的可靠性通过其对角色选择的稳健性和与人类标注的高相关性得到验证。

**Abstract:** The rise of AI has fueled growing concerns about ``hype'' in machine learning papers, yet a reliable way to quantify rhetorical style independently of substantive content has remained elusive. Because bold language can stem from either strong empirical results or mere rhetorical style, it is often difficult to distinguish between the two. To disentangle rhetorical style from substantive content, we introduce a counterfactual, LLM-based framework: multiple LLM rhetorical personas generate counterfactual writings from the same substantive content, an LLM judge compares them through pairwise evaluations, and the outcomes are aggregated using a Bradley--Terry model. Applying this method to 8,485 ICLR submissions sampled from 2017 to 2025, we generate more than 250,000 counterfactual writings and provide a large-scale quantification of rhetorical style in ML papers. We find that visionary framing significantly predicts downstream attention, including citations and media attention, even after controlling for peer-review evaluations. We also observe a sharp rise in rhetorical strength after 2023, and provide empirical evidence showing that this increase is largely driven by the adoption of LLM-based writing assistance. The reliability of our framework is validated by its robustness to the choice of personas and the high correlation between LLM judgments and human annotations. Our work demonstrates that LLMs can serve as instruments to measure and improve scientific evaluation.

</details>


### [4] [PRISM: A Personality-Driven Multi-Agent Framework for Social Media Simulation](https://arxiv.org/abs/2512.19933)
*Zhixiang Lu,Xueyuan Deng,Yiran Liu,Yulong Li,Qiang Yan,Imran Razzak,Jionglong Su*

Main category: cs.CL

> 本文提出了 PRISM，一种用于意见动态模拟的新模型，结合随机微分方程和PC-POMDP，以更准确地模拟社交媒体上的个性差异，显著优于传统方法。

<details>
  <summary>Details</summary>

**Motivation:** 传统基于代理的意见动态模型往往由于过分简化的同质化假设而无法捕捉导致在线极化的心理异质性。这妨碍了对意识形态分歧是如何被放大的机制的理解。为了应对这一挑战，开发了PRISM模型。

**Method:** 引入个性折射智能仿真模型（PRISM），结合随机微分方程（SDE）和基于个性的部分可观察马尔科夫决策过程（PC-POMDP），用于模拟连续的情绪演变和离散决策过程。PRISM 赋予多模态大型语言模型（MLLM）代理以基于迈尔斯·布里格斯类型指标（MBTI）的认知策略，这些策略由大规模社交媒体数据集得出的数据驱动先验初始化。

**Result:** 与标准同质化模型和大五人格模型基准相比，PRISM 实现了更好的一致性，与人类真实情况更接近。此外，该框架有效地复制了诸如理性抑制和情感共鸣等涌现现象，成为分析复杂社交媒体生态系统的一个强大工具。

**Conclusion:** PRISM 通过多模态大型语言模型代理模拟复杂个性，显著提升了对社交媒体生态系统中复杂社会动态的理解。

**Abstract:** Traditional agent-based models (ABMs) of opinion dynamics often fail to capture the psychological heterogeneity driving online polarization due to simplistic homogeneity assumptions. This limitation obscures the critical interplay between individual cognitive biases and information propagation, thereby hindering a mechanistic understanding of how ideological divides are amplified. To address this challenge, we introduce the Personality-Refracted Intelligent Simulation Model (PRISM), a hybrid framework coupling stochastic differential equations (SDE) for continuous emotional evolution with a personality-conditional partially observable Markov decision process (PC-POMDP) for discrete decision-making. In contrast to continuous trait approaches, PRISM assigns distinct Myers-Briggs Type Indicator (MBTI) based cognitive policies to multimodal large language model (MLLM) agents, initialized via data-driven priors from large-scale social media datasets. PRISM achieves superior personality consistency aligned with human ground truth, significantly outperforming standard homogeneous and Big Five benchmarks. This framework effectively replicates emergent phenomena such as rational suppression and affective resonance, offering a robust tool for analyzing complex social media ecosystems.

</details>


### [5] [Bias Beneath the Tone: Empirical Characterisation of Tone Bias in LLM-Driven UX Systems](https://arxiv.org/abs/2512.19950)
*Heet Bodara,Md Masum Mushfiq,Isma Farah Siddiqui*

Main category: cs.CL

> 研究探索了大语言模型中的语气偏见，并表明这种偏见是系统性的、可测量的，并且对设计公平、值得信赖的对话AI是相关的。为了研究这个问题，创建了两个带有不同语气引导的数据集，并通过集成分类模型检测这些语气偏见。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探讨大语言模型在对话中无意展现的语气偏见，这对设计公平、值得信赖的对话AI至关重要。

**Method:** 通过将可控的大语言模型与语气分类模型的集成，研究了大语言模型的语气偏见。创建了两个合成对话数据集，一个由中性提示生成，另一个明确引导生成正面或负面语气。

**Result:** 即使在中性话语数据集上也发现了明显的语气偏见，检测该偏见的集成模型实现了高达0.92的宏F1得分。

**Conclusion:** 研究结果表明，语气偏见是系统性且可量化的，并且与设计公平、值得信赖的对话AI相关。

**Abstract:** Large Language Models are increasingly used in conversational systems such as digital personal assistants, shaping how people interact with technology through language. While their responses often sound fluent and natural, they can also carry subtle tone biases such as sounding overly polite, cheerful, or cautious even when neutrality is expected. These tendencies can influence how users perceive trust, empathy, and fairness in dialogue. In this study, we explore tone bias as a hidden behavioral trait of large language models. The novelty of this research lies in the integration of controllable large language model based dialogue synthesis with tone classification models, enabling robust and ethical emotion recognition in personal assistant interactions. We created two synthetic dialogue datasets, one generated from neutral prompts and another explicitly guided to produce positive or negative tones. Surprisingly, even the neutral set showed consistent tonal skew, suggesting that bias may stem from the model's underlying conversational style. Using weak supervision through a pretrained DistilBERT model, we labeled tones and trained several classifiers to detect these patterns. Ensemble models achieved macro F1 scores up to 0.92, showing that tone bias is systematic, measurable, and relevant to designing fair and trustworthy conversational AI.

</details>


### [6] [Schoenfeld's Anatomy of Mathematical Reasoning by Language Models](https://arxiv.org/abs/2512.19995)
*Ming Li,Chenrui Fan,Yize Cheng,Soheil Feizi,Tianyi Zhou*

Main category: cs.CL

> 本文引入ThinkARM框架，将推理过程抽象为具体步骤，揭示模型的思维动态和结构差异，以及效率方法对模型的影响。

<details>
  <summary>Details</summary>

**Motivation:** 大规模语言模型的推理过程逐渐暴露出来，但是其潜在的结构和步骤依然难以通过表面级别的统计识别和分析。

**Method:** 采用Schoenfeld的事件理论作为归纳的中间尺度镜头，并引入ThinkARM框架，该框架可以将推理过程抽象为诸如分析、探索、实施和验证等功能性的推理步骤。

**Result:** 这种方法揭示了推理模型和非推理模型之间的可重复的思维动态和结构差异。此外，通过诊断案例研究，发现了探索作为关键的分支步骤与正确性相关，并且效率导向的方法选择性地抑制评价反馈步骤。

**Conclusion:** 研究结果表明，通过对事件级别的表示使得推理步骤变得明确，能够系统分析现代语言模型中推理的结构、稳定性和调整。

**Abstract:** Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [7] [PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility](https://arxiv.org/abs/2512.19711)
*Md Nahid Hasan Shuvo,Moinul Hossain*

Main category: cs.CV

> This paper introduces PHANTOM, a framework for creating perspective-dependent adversarial examples using anamorphic art that mislead state-of-the-art object detectors in autonomous vehicles without requiring model access, significantly impacting vehicle safety and communication network integrity under various conditions.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to identify and demonstrate a new methodological approach to crafting adversarial examples that are imperceptible to humans but can fool computer vision systems used in autonomous vehicles, leading to severe safety issues and potential large-scale disruption of vehicle communication networks.

**Method:** PHANTOM uses anamorphic art to create perspective-dependent adversarial examples on road surfaces, which are designed to be misclassified by the DNN systems of autonomous vehicles, even from a certain distance.

**Result:** The research shows that PHANTOM can achieve an attack success rate exceeding 90% under optimal conditions and maintains an effectiveness of 60%-80% even in degraded environments. Additionally, it causes network-wide disruptions due to exacerbated PAi from false emergency communications.

**Conclusion:** The findings reveal significant vulnerabilities in both the perception and communication layers of connected autonomous vehicle ecosystems, potentially impacting both the safety and reliability of these systems in real-world driving scenarios.

**Abstract:** Connected autonomous vehicles (CAVs) rely on vision-based deep neural networks (DNNs) and low-latency (Vehicle-to-Everything) V2X communication to navigate safely and efficiently. Despite their advances, these systems remain vulnerable to physical adversarial attacks. In this paper, we introduce PHANTOM (PHysical ANamorphic Threats Obstructing connected vehicle Mobility), a novel framework for crafting and deploying perspective-dependent adversarial examples using \textit{anamorphic art}. PHANTOM exploits geometric distortions that appear natural to humans but are misclassified with high confidence by state-of-the-art object detectors. Unlike conventional attacks, PHANTOM operates in black-box settings without model access and demonstrates strong transferability across four diverse detector architectures (YOLOv5, SSD, Faster R-CNN, and RetinaNet). Comprehensive evaluation in CARLA across varying speeds, weather conditions, and lighting scenarios shows that PHANTOM achieves over 90\% attack success rate under optimal conditions and maintains 60-80\% effectiveness even in degraded environments. The attack activates within 6-10 meters of the target, providing insufficient time for safe maneuvering. Beyond individual vehicle deception, PHANTOM triggers network-wide disruption in CAV systems: SUMO-OMNeT++ co-simulation demonstrates that false emergency messages propagate through V2X links, increasing Peak Age of Information by 68-89\% and degrading safety-critical communication. These findings expose critical vulnerabilities in both perception and communication layers of CAV ecosystems.

</details>


### [8] [Generating the Past, Present and Future from a Motion-Blurred Image](https://arxiv.org/abs/2512.19817)
*SaiKiran Tedla,Kelly Zhu,Trevor Canham,Felix Taubner,Michael S. Brown,Kiriakos N. Kutulakos,David B. Lindell*

Main category: cs.CV

> 本文提出了一种新方法，利用预训练的视频扩散模型，从运动模糊图像中恢复视频，以再现拍摄时刻及之前和之后的场景动态，表现优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 作者希望解决现有技术依赖手工设计的先验或网络架构来解决运动模糊图像复原的模糊问题，并且不尝试重建图像拍摄前后的场景动态。

**Method:** 通过重新利用在大型互联网数据集上预训练的视频扩散模型，该技术能够恢复在拍摄时刻及之前和之后短时间内的视频，展现复杂场景动态。

**Result:** 该方法在处理任务中优于以往方法，适用于具有挑战性的现实世界图像，并支持恢复相机轨迹、物体运动和动态三维场景结构等下游任务。

**Conclusion:** 这种技术通过使用预训练的视频扩散模型，不仅能够在拍摄瞬间恢复复杂的场景动态，还能重现拍摄前后的情况，展示了其鲁棒性和多功能性。

**Abstract:** We seek to answer the question: what can a motion-blurred image reveal about a scene's past, present, and future? Although motion blur obscures image details and degrades visual quality, it also encodes information about scene and camera motion during an exposure. Previous techniques leverage this information to estimate a sharp image from an input blurry one, or to predict a sequence of video frames showing what might have occurred at the moment of image capture. However, they rely on handcrafted priors or network architectures to resolve ambiguities in this inverse problem, and do not incorporate image and video priors on large-scale datasets. As such, existing methods struggle to reproduce complex scene dynamics and do not attempt to recover what occurred before or after an image was taken. Here, we introduce a new technique that repurposes a pre-trained video diffusion model trained on internet-scale datasets to recover videos revealing complex scene dynamics during the moment of capture and what might have occurred immediately into the past or future. Our approach is robust and versatile; it outperforms previous methods for this task, generalizes to challenging in-the-wild images, and supports downstream tasks such as recovering camera trajectories, object motion, and dynamic 3D scene structure. Code and data are available at https://blur2vid.github.io

</details>


### [9] [Learning to Refocus with Video Diffusion Models](https://arxiv.org/abs/2512.19823)
*SaiKiran Tedla,Zhoutong Zhang,Xuaner Zhang,Shumian Xin*

Main category: cs.CV

> Novel method for post-capture refocusing using video diffusion models, generating perceptually accurate focal stacks from single defocused images.

<details>
  <summary>Details</summary>

**Motivation:** Current autofocus systems often fail to capture the intended subject, and users frequently wish to adjust focus after capture.

**Method:** From a single defocused image, the approach generates a perceptually accurate focal stack, represented as a video sequence, enabling interactive refocusing.

**Result:** The method consistently outperforms existing approaches in both perceptual quality and robustness across challenging scenarios.

**Conclusion:** This work paves the way for more advanced focus-editing capabilities in everyday photography.

**Abstract:** Focus is a cornerstone of photography, yet autofocus systems often fail to capture the intended subject, and users frequently wish to adjust focus after capture. We introduce a novel method for realistic post-capture refocusing using video diffusion models. From a single defocused image, our approach generates a perceptually accurate focal stack, represented as a video sequence, enabling interactive refocusing and unlocking a range of downstream applications. We release a large-scale focal stack dataset acquired under diverse real-world smartphone conditions to support this work and future research. Our method consistently outperforms existing approaches in both perceptual quality and robustness across challenging scenarios, paving the way for more advanced focus-editing capabilities in everyday photography. Code and data are available at www.learn2refocus.github.io

</details>


### [10] [RANSAC Scoring Functions: Analysis and Reality Check](https://arxiv.org/abs/2512.19850)
*A. Shekhovtsov*

Main category: cs.CV

> 本文重新审视了几何拟合中的评分函数问题，包括MAGSAC++的评分函数，并提出了一种新的评分函数评估方法。

<details>
  <summary>Details</summary>

**Motivation:** 通过对现有评分函数的重新审视，旨在提供一个全面的理论和实验分析，这对于未来研究改进方法或解决其他稳健拟合问题至关重要。

**Method:** 分析了现有的评分函数，并提出了一个新的评分函数评价实验方法。此外，还重新审视了MAGSAC++评分函数，并发现其实际上等同于一个简单的高斯-均匀似然函数。

**Result:** 研究表明所有评分函数，包括使用学习的内点分布，表现一致。MAGSAC++评分函数并不比简单的竞争者更优越，也不对阈值超参数的选择更不敏感。

**Conclusion:** 通过理论性和实验性分析，全面回顾了现有方法的状态，表明使用简单的高斯-均匀似然函数已经足够，未来的改进需要深入的方向。

**Abstract:** We revisit the problem of assigning a score (a quality of fit) to candidate geometric models -- one of the key components of RANSAC for robust geometric fitting. In a non-robust setting, the ``gold standard'' scoring function, known as the geometric error, follows from a probabilistic model with Gaussian noises. We extend it to spherical noises. In a robust setting, we consider a mixture with uniformly distributed outliers and show that a threshold-based parameterization leads to a unified view of likelihood-based and robust M-estimators and associated local optimization schemes.
  Next we analyze MAGSAC++ which stands out for two reasons. First, it achieves the best results according to existing benchmarks. Second, it makes quite different modeling assumptions and derivation steps. We discovered, however that the derivation does not correspond to sound principles and the resulting score function is in fact numerically equivalent to a simple Gaussian-uniform likelihood, a basic model within the proposed framework.
  Finally, we propose an experimental methodology for evaluating scoring functions: assuming either a large validation set, or a small random validation set in expectation. We find that all scoring functions, including using a learned inlier distribution, perform identically. In particular, MAGSAC++ score is found to be neither better performing than simple contenders nor less sensitive to the choice of the threshold hyperparameter.
  Our theoretical and experimental analysis thus comprehensively revisit the state-of-the-art, which is critical for any future research seeking to improve the methods or apply them to other robust fitting problems.

</details>


### [11] [HyGE-Occ: Hybrid View-Transformation with 3D Gaussian and Edge Priors for 3D Panoptic Occupancy Prediction](https://arxiv.org/abs/2512.19871)
*Jong Wook Kim,Wonseok Roh,Ha Dam Baek,Pilhyeon Lee,Jonghyun Choi,Sangpil Kim*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** 3D Panoptic Occupancy Prediction aims to reconstruct a dense volumetric scene map by predicting the semantic class and instance identity of every occupied region in 3D space. Achieving such fine-grained 3D understanding requires precise geometric reasoning and spatially consistent scene representation across complex environments. However, existing approaches often struggle to maintain precise geometry and capture the precise spatial range of 3D instances critical for robust panoptic separation. To overcome these limitations, we introduce HyGE-Occ, a novel framework that leverages a hybrid view-transformation branch with 3D Gaussian and edge priors to enhance both geometric consistency and boundary awareness in 3D panoptic occupancy prediction. HyGE-Occ employs a hybrid view-transformation branch that fuses a continuous Gaussian-based depth representation with a discretized depth-bin formulation, producing BEV features with improved geometric consistency and structural coherence. In parallel, we extract edge maps from BEV features and use them as auxiliary information to learn edge cues. In our extensive experiments on the Occ3D-nuScenes dataset, HyGE-Occ outperforms existing work, demonstrating superior 3D geometric reasoning.

</details>


### [12] [Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs](https://arxiv.org/abs/2512.19918)
*Houston H. Zhang,Tao Zhang,Baoze Lin,Yuanqi Xue,Yincheng Zhu,Huan Liu,Li Gu,Linfeng Ye,Ziqiang Wang,Xinxin Zuo,Yang Wang,Yuanhao Yu,Zhixiang Chi*

Main category: cs.CV

> 文章提出了一种名为Widget2Code的新方法，旨在改善从图像生成代码的可靠性与视觉一致性。通过引入WidgetFactory系统和WidgetDSL，显著提升了生成代码的视觉保真度。

<details>
  <summary>Details</summary>

**Motivation:** 虽然现有的多模态大语言模型在生成代码方面优于专门的UI2Code方法，但它们生成的代码仍然不稳定且视觉上不一致。因此，本文旨在解决这些问题，并提升生成代码的视觉保真度。

**Method:** 本文提出了一种新的基准测试，用于评估从图像生成代码的能力，并引入了WidgetFactory系统，该系统包括一个框架无关的领域特定语言（WidgetDSL）和一个编译器，可将WidgetDSL转换为多种前端实现。此外，还设计了一个自适应渲染模块来确保生成的代码满足紧凑性约束。

**Result:** 实验结果证明，该方法在基准测试上表现出色，大大提高了代码生成的视觉保真度和一致性，确立了一个未来Widget2Code研究的强有力基线。

**Conclusion:** 研究通过开发WidgetFactory系统和一种新的基准测试方法，显著提升了从图像生成代码的视觉保真度，为未来的研究提供了坚实的基础。

**Abstract:** User interface to code (UI2Code) aims to generate executable code that can faithfully reconstruct a given input UI. Prior work focuses largely on web pages and mobile screens, leaving app widgets underexplored. Unlike web or mobile UIs with rich hierarchical context, widgets are compact, context-free micro-interfaces that summarize key information through dense layouts and iconography under strict spatial constraints. Moreover, while (image, code) pairs are widely available for web or mobile UIs, widget designs are proprietary and lack accessible markup. We formalize this setting as the Widget-to-Code (Widget2Code) and introduce an image-only widget benchmark with fine-grained, multi-dimensional evaluation metrics. Benchmarking shows that although generalized multimodal large language models (MLLMs) outperform specialized UI2Code methods, they still produce unreliable and visually inconsistent code. To address these limitations, we develop a baseline that jointly advances perceptual understanding and structured code generation. At the perceptual level, we follow widget design principles to assemble atomic components into complete layouts, equipped with icon retrieval and reusable visualization modules. At the system level, we design an end-to-end infrastructure, WidgetFactory, which includes a framework-agnostic widget-tailored domain-specific language (WidgetDSL) and a compiler that translates it into multiple front-end implementations (e.g., React, HTML/CSS). An adaptive rendering module further refines spatial dimensions to satisfy compactness constraints. Together, these contributions substantially enhance visual fidelity, establishing a strong baseline and unified infrastructure for future Widget2Code research.

</details>


### [13] [Unified Brain Surface and Volume Registration](https://arxiv.org/abs/2512.19928)
*S. Mazdak Abulnaga,Andrew Hoopes,Malte Hoffmann,Robin Magnet,Maks Ovsjanikov,Lilla Zöllei,John Guttag,Bruce Fischl,Adrian Dalca*

Main category: cs.CV

> NeurAlign, a deep learning method, improves brain MRI registration accuracy by jointly aligning the cortex and subcortex, bridging the gap between surface and volume, achieving higher Dice scores and faster processing times compared to other methods.

<details>
  <summary>Details</summary>

**Motivation:** The traditional methods, which handle volumetric and surface registration separately, often result in inconsistencies, limiting the accuracy and reliability of neuroscientific analyses. This motivates the need for a more consistent approach that can align both aspects simultaneously.

**Method:** Our approach, NeurAlign, uses a deep learning framework to register 3D brain MRI images by jointly aligning both the cortical and subcortical regions through a unified representation that links the volumetric and surface-based anatomy via an intermediate spherical space, ensuring geometric coherence between the two domains.

**Result:** Experiments show that NeurAlign outperforms classical and machine learning-based registration methods by improving Dice scores by up to 7 points and offering significantly faster processing times.

**Conclusion:** NeurAlign sets a new standard for joint cortical and subcortical registration in brain MRI scans thanks to its superior accuracy, fast inference, and ease of use, without requiring additional inputs beyond an MRI scan.

**Abstract:** Accurate registration of brain MRI scans is fundamental for cross-subject analysis in neuroscientific studies. This involves aligning both the cortical surface of the brain and the interior volume. Traditional methods treat volumetric and surface-based registration separately, which often leads to inconsistencies that limit downstream analyses. We propose a deep learning framework, NeurAlign, that registers $3$D brain MRI images by jointly aligning both cortical and subcortical regions through a unified volume-and-surface-based representation. Our approach leverages an intermediate spherical coordinate space to bridge anatomical surface topology with volumetric anatomy, enabling consistent and anatomically accurate alignment. By integrating spherical registration into the learning, our method ensures geometric coherence between volume and surface domains. In a series of experiments on both in-domain and out-of-domain datasets, our method consistently outperforms both classical and machine learning-based registration methods -- improving the Dice score by up to 7 points while maintaining regular deformation fields. Additionally, it is orders of magnitude faster than the standard method for this task, and is simpler to use because it requires no additional inputs beyond an MRI scan. With its superior accuracy, fast inference, and ease of use, NeurAlign sets a new standard for joint cortical and subcortical registration.

</details>


### [14] [Vehicle-centric Perception via Multimodal Structured Pre-training](https://arxiv.org/abs/2512.19934)
*Wentao Wu,Xiao Wang,Chenglong Li,Jin Tang,Bin Luo*

Main category: cs.CV

> VehicleMAE-V2, a vehicle-centric pre-trained large model, is proposed to enhance the generalizable representations of vehicle perception by utilizing structured priors of vehicle symmetry, contour, and semantics. 

<details>
  <summary>Details</summary>

**Motivation:** Existing approaches lack effective learning of vehicle-related knowledge during pre-training, leading to poor capability for modeling general vehicle perception representations. VehicleMAE-V2 is designed to address this problem.

**Method:** VehicleMAE-V2, which uses Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM), and Semantics-guided Representation Module (SRM) to enhance vehicle-centric perception representations by incorporating symmetry, contour, and semantics of vehicles.

**Result:** Experiments on five downstream tasks show superior performance of VehicleMAE-V2.

**Conclusion:** The proposed VehicleMAE-V2 significantly enhances the capability of the model to learn generalizable representations for vehicle-centric perception, demonstrating superior performance in various tasks.

**Abstract:** Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model's capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.

</details>


### [15] [Block-Recurrent Dynamics in Vision Transformers](https://arxiv.org/abs/2512.19941)
*Mozes Jacobs,Thomas Fel,Richard Hakim,Alessandra Brondetta,Demba Ba,T. Andy Keller*

Main category: cs.CV

> 本文提出Block-Recurrent Hypothesis，采用Raptor模型展示可使用少量不同块的递归结构来替换ViTs的原始L块层，并指出ViTs的深度中存在一个原则性的动力系统解释。

<details>
  <summary>Details</summary>

**Motivation:** 随着ViTs成为标准的视觉骨干网络，对其计算现象的机械解释变得至关重要。当前对于ViT的深度如何解释为一个特征流没有一个定论。

**Method:** 提出Block-Recurrent Hypothesis (BRH)，认为训练过的ViTs的深度计算可以用极少的不同模块重复运算来重写。通过训练Raptor模型作为预训练ViTs的代理模型，来验证这些阶段反映的是可重复使用的计算。

**Result:** 展示了一个仅用2个模块训练的Raptor模型能够在等价计算成本下达到DINOv2 ImageNet-1k线性探针准确性的96%。

**Conclusion:** 基于BRH，发现了沿ViT深度的紧凑递归程序，这为通过原理性动力系统分析研究这些模型提供了可能。

**Abstract:** As Vision Transformers (ViTs) become standard vision backbones, a mechanistic account of their computational phenomenology is essential. Despite architectural cues that hint at dynamical structure, there is no settled framework that interprets Transformer depth as a well-characterized flow. In this work, we introduce the Block-Recurrent Hypothesis (BRH), arguing that trained ViTs admit a block-recurrent depth structure such that the computation of the original $L$ blocks can be accurately rewritten using only $k \ll L$ distinct blocks applied recurrently. Across diverse ViTs, between-layer representational similarity matrices suggest few contiguous phases. To determine whether these phases reflect genuinely reusable computation, we train block-recurrent surrogates of pretrained ViTs: Recurrent Approximations to Phase-structured TransfORmers (Raptor). In small-scale, we demonstrate that stochastic depth and training promote recurrent structure and subsequently correlate with our ability to accurately fit Raptor. We then provide an empirical existence proof for BRH by training a Raptor model to recover $96\%$ of DINOv2 ImageNet-1k linear probe accuracy in only 2 blocks at equivalent computational cost. Finally, we leverage our hypothesis to develop a program of Dynamical Interpretability. We find i) directional convergence into class-dependent angular basins with self-correcting trajectories under small perturbations, ii) token-specific dynamics, where cls executes sharp late reorientations while patch tokens exhibit strong late-stage coherence toward their mean direction, and iii) a collapse to low rank updates in late depth, consistent with convergence to low-dimensional attractors. Altogether, we find a compact recurrent program emerges along ViT depth, pointing to a low-complexity normative solution that enables these models to be studied through principled dynamical systems analysis.

</details>


### [16] [SE360: Semantic Edit in 360$^\circ$ Panoramas via Hierarchical Data Construction](https://arxiv.org/abs/2512.19943)
*Haoyi Zhong,Fang-Lue Zhang,Andrew Chalmers,Taehyun Rhee*

Main category: cs.CV

> 本文提出SE360，一种新的360度全景图像编辑框架，能够有效处理多条件引导下的物体编辑，解决了现有方法在编辑360度全景时产生不合理结果的问题。通过粗到精的数据自动生成管道和两阶段数据精炼策略，确保生成数据既具有语义意义，又几何一致，训练得到的Transformer扩散模型在视觉质量与语义准确性方面优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于指令图像编辑方法在扩展到360度全景时面临挑战，经常产生不合理的结果。本文旨在提供一种更有效的方法来处理360度全景中的物体编辑。

**Method:** 本文提出SE360框架，主要依赖一个无需人工干涉的粗到精自动数据生成管道，通过视觉语言模型和自适应投影调整进行层次分析，最终生成具有语义意义和几何一致的数据对。此外，引入了一种成本效益高的两阶段数据精炼策略。

**Result:** 基于构建的数据集训练的Transformer扩散模型能够在360度全景图像中实现灵活的物体编辑，编辑结果由文本、掩模或参考图像引导。实验表明，本文方法在视觉质量和语义准确性方面优于现有方法。

**Conclusion:** 本文提出的SE360框架通过引入新的自动生成数据管道和数据精炼策略，解决了360度全景多条件物体编辑中遇到的问题，能够生成高质量且语义准确的编辑结果，展示了在360度全景图像编辑中的优越性能。

**Abstract:** While instruction-based image editing is emerging, extending it to 360$^\circ$ panoramas introduces additional challenges. Existing methods often produce implausible results in both equirectangular projections (ERP) and perspective views. To address these limitations, we propose SE360, a novel framework for multi-condition guided object editing in 360$^\circ$ panoramas. At its core is a novel coarse-to-fine autonomous data generation pipeline without manual intervention. This pipeline leverages a Vision-Language Model (VLM) and adaptive projection adjustment for hierarchical analysis, ensuring the holistic segmentation of objects and their physical context. The resulting data pairs are both semantically meaningful and geometrically consistent, even when sourced from unlabeled panoramas. Furthermore, we introduce a cost-effective, two-stage data refinement strategy to improve data realism and mitigate model overfitting to erase artifacts. Based on the constructed dataset, we train a Transformer-based diffusion model to allow flexible object editing guided by text, mask, or reference image in 360$^\circ$ panoramas. Our experiments demonstrate that our method outperforms existing methods in both visual quality and semantic accuracy.

</details>
