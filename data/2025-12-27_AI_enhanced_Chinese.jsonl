{"id": "2512.20638", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20638", "abs": "https://arxiv.org/abs/2512.20638", "authors": ["Matyas Bohacek", "Nino Scherrer", "Nicholas Dufour", "Thomas Leung", "Christoph Bregler", "Stephanie C. Y. Chan"], "title": "Uncovering Competency Gaps in Large Language Models and Their Benchmarks", "comment": null, "summary": "The evaluation of large language models (LLMs) relies heavily on standardized benchmarks. These benchmarks provide useful aggregated metrics for a given capability, but those aggregated metrics can obscure (i) particular sub-areas where the LLMs are weak (\"model gaps\") and (ii) imbalanced coverage in the benchmarks themselves (\"benchmark gaps\"). We propose a new method that uses sparse autoencoders (SAEs) to automatically uncover both types of gaps. By extracting SAE concept activations and computing saliency-weighted performance scores across benchmark data, the method grounds evaluation in the model's internal representations and enables comparison across benchmarks. As examples demonstrating our approach, we applied the method to two popular open-source models and ten benchmarks. We found that these models consistently underperformed on concepts that stand in contrast to sycophantic behaviors (e.g., politely refusing a request or asserting boundaries) and concepts connected to safety discussions. These model gaps align with observations previously surfaced in the literature; our automated, unsupervised method was able to recover them without manual supervision. We also observed benchmark gaps: many of the evaluated benchmarks over-represented concepts related to obedience, authority, or instruction-following, while missing core concepts that should fall within their intended scope. In sum, our method offers a representation-grounded approach to evaluation, enabling concept-level decomposition of benchmark scores. Rather than replacing conventional aggregated metrics, CG complements them by providing a concept-level decomposition that can reveal why a model scored as it did and how benchmarks could evolve to better reflect their intended scope. Code is available at https://competency-gaps.github.io.", "AI": {"tldr": "新方法使用SAEs自动揭示模型和基准测试的缺陷，并以表示为基础提供概念级别的基准分解，以揭示模型评分背后的细节。", "motivation": "传统的基准测试虽然提供了有用的聚合指标，但这些聚合指标可能掩盖了模型在某些特定子领域的弱点以及基准测试自身不均衡覆盖的问题。通过提出的新方法希望能解决这些问题。", "method": "使用稀疏自动编码器（SAEs）来自动揭示模型弱点（\"模型缺口\"）和基准测试自身不均衡覆盖（\"基准缺口\"）的新方法。通过提取SAE概念激活并计算基准数据上的显著加权性能得分，该方法使评估立足于模型内部表示，并允许跨基准进行比较。", "result": "该方法应用于两个流行的开源模型和十个基准测试上，结果显示，这些模型在与逢迎行为相对立的概念上表现不佳，比如礼貌地拒绝请求或设定边界，还表现出与安全讨论相关的概念上的弱点。同时发现许多评估的基准测试过份强调了服从、权威或指令遵循的概念，而忽略了应该在其预定范围内涵盖的核心概念。", "conclusion": "这项新方法提供了一种以表示为基础的评估方法，它能将基准得分分解到概念级别，补充而不是替代传统的聚合指标，帮助揭示模型评分背后的原因以及基准测试如何进化以更好地反映其预定范围。"}}
{"id": "2512.20724", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20724", "abs": "https://arxiv.org/abs/2512.20724", "authors": ["Alexandros Christoforos", "Chadbourne Davis"], "title": "SA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention", "comment": "Under submission", "summary": "Diffusion based approaches to long form text generation suffer from prohibitive computational cost and memory overhead as sequence length increases. We introduce SA-DiffuSeq, a diffusion framework that integrates sparse attention to fundamentally improve scalability for long document modeling. By selectively allocating attention within the diffusion process, SA-DiffuSeq significantly reduces computational complexity while maintaining semantic coherence and generation quality. A key component of our method is a soft absorbing state tailored to sparse attention dynamics, which stabilizes diffusion trajectories and accelerates sequence reconstruction. This design improves sampling efficiency and enhances precision in long range dependency modeling. Extensive experiments demonstrate that SA-DiffuSeq consistently surpasses state of the art diffusion baselines in both training efficiency and sampling speed, with especially strong gains on extended sequences. These properties make SA-DiffuSeq well suited for demanding long form applications such as scientific writing, large scale code generation, and multi turn long context dialogue. Overall, our results indicate that incorporating structured sparsity into diffusion models is a promising direction for efficient and expressive long text generation.", "AI": {"tldr": "SA-DiffuSeq, a diffusion model utilizing sparse attention, is introduced to enhance the scalability and efficiency of long text modeling, excelling in long sequence processing and long-range dependency modeling.", "motivation": "To address the computational cost and memory overhead associated with diffusion approaches as the sequence length of long form texts increases.", "method": "SA-DiffuSeq, a diffusion framework integrating sparse attention, is introduced to improve the scalability of long document modeling by reducing computational complexity and maintaining semantic coherence.", "result": "Experiments show that SA-DiffuSeq beats diffusion baselines in training efficiency and sampling speed, especially for extended sequences, enhancing long range dependency modeling.", "conclusion": "The integration of structured sparsity into diffusion models, as demonstrated by SA-DiffuSeq, represents a promising direction for the efficient and expressive generation of long text."}}
{"id": "2512.20757", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20757", "abs": "https://arxiv.org/abs/2512.20757", "authors": ["Gül Sena Altıntaş", "Malikeh Ehghaghi", "Brian Lester", "Fengyuan Liu", "Wanru Zhao", "Marco Ciccone", "Colin Raffel"], "title": "TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior", "comment": null, "summary": "Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers.", "AI": {"tldr": "介绍TokSuite工具，通过使用不同分词器训练模型，并提供专门的基准测试，来分析分词器对语言模型的影响，揭示各种流行分词器的优势和缺点。", "motivation": "解决由于难以单独衡量分词器影响，而导致的语言模型性能和行为研究中的不足。", "method": "使用了十四种不同的分词器训练模型，同时保持相同的架构、数据集、训练预算和初始化条件。此外，还整理并发布了一个新的基准，专门衡量模型在面临可能影响分词器的真实世界扰动时的性能。", "result": "TokSuite能够稳健地分离出模型分词器的影响，从而支持了一系列关于各种流行分词器优势与缺点的新型发现。", "conclusion": "研究表明，通过TokSuite可以更清晰地了解不同的分词技术对其语言模型性能的具体影响，有利于未来的研究和应用。"}}
{"id": "2512.20773", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20773", "abs": "https://arxiv.org/abs/2512.20773", "authors": ["Ziyi Zhu", "Olivier Tieleman", "Caitlin A. Stamatis", "Luka Smyth", "Thomas D. Hull", "Daniel R. Cahn", "Matteo Malgaroli"], "title": "Adversarial Training for Failure-Sensitive User Simulation in Mental Health Dialogue Optimization", "comment": null, "summary": "Realistic user simulation is crucial for training and evaluating task-oriented dialogue (TOD) systems, yet creating simulators that accurately replicate human behavior remains challenging. A key property of effective simulators is their ability to expose failure modes of the systems they evaluate. We present an adversarial training framework that iteratively improves user simulator realism through a competitive dynamic between a generator (user simulator) and a discriminator. Applied to mental health support chatbots, our approach demonstrates that fine-tuned simulators dramatically outperform zero-shot base models at surfacing system issues, and adversarial training further enhances diversity, distributional alignment, and predictive validity. The resulting simulator achieves a strong correlation between simulated and real failure occurrence rates across diverse chatbot configurations while maintaining low distributional divergence of failure modes. Discriminator accuracy decreases drastically after three adversarial iterations, suggesting improved realism. These results provide evidence that adversarial training is a promising approach for creating realistic user simulators in mental health support TOD domains, enabling rapid, reliable, and cost-effective system evaluation before deployment.", "AI": {"tldr": "本文提出了一种对抗训练框架，通过生成器和判别器之间的竞争动态，迭代提高用户模拟器的现实感，特别是在心理健康支持聊天机器人中，这种方法显著改善了仿真器表面系统问题的能力。", "motivation": "创建能准确模拟用户行为的仿真器对训练和评估任务导向对话系统至关重要。本文旨在通过提高仿真器的现实感来更好地暴露系统故障。", "method": "方法包括使用对抗训练框架，通过一个生成器（用户仿真器）和判别器之间的竞争动态来提高用户仿真器的现实感。", "result": "研究结果表明，在心理健康支持聊天机器人中，经过微调的仿真器比零样本基本模型更有效，且对抗训练进一步增强了多样性、分布一致性和预测有效性。", "conclusion": "研究结果表明，对抗训练是一个在心理健康支持聊天机器人领域创建逼真的用户仿真器的有前景的方法，显著效能可加速可靠的系统评估。"}}
{"id": "2512.20735", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20735", "abs": "https://arxiv.org/abs/2512.20735", "authors": ["Shijing Wang", "Chaoqun Cui", "Yaping Huang", "Hyung Jin Chang", "Yihua Cheng"], "title": "VL4Gaze: Unleashing Vision-Language Models for Gaze Following", "comment": null, "summary": "Human gaze provides essential cues for interpreting attention, intention, and social interaction in visual scenes, yet gaze understanding remains largely unexplored in current vision-language models (VLMs). While recent VLMs achieve strong scene-level reasoning across a range of visual tasks, there exists no benchmark that systematically evaluates or trains them for gaze interpretation, leaving open the question of whether gaze understanding can emerge from general-purpose vision-language pre-training. To address this gap, we introduce VL4Gaze, the first large-scale benchmark designed to investigate, evaluate, and unlock the potential of VLMs for gaze understanding. VL4Gaze contains 489K automatically generated question-answer pairs across 124K images and formulates gaze understanding as a unified VQA problem through four complementary tasks: (1) gaze object description, (2) gaze direction description, (3) gaze point location, and (4) ambiguous question recognition. We comprehensively evaluate both commercial and open-source VLMs under in-context learning and fine-tuning settings. The results show that even large-scale VLMs struggle to reliably infer gaze semantics and spatial localization without task-specific supervision. In contrast, training on VL4Gaze brings substantial and consistent improvements across all tasks, highlighting the importance of targeted multi-task supervision for developing gaze understanding capabilities in VLMs. We will release the dataset and code to support further research and development in this direction.", "AI": {"tldr": "The paper introduces VL4Gaze, a large-scale benchmark for evaluating and training vision-language models (VLMs) in gaze understanding, which involves four tasks: gaze object description, gaze direction description, gaze point location, and ambiguous question recognition. The results indicate that VLMs need task-specific supervision to reliably understand gaze, emphasizing the need for targeted multi-task training.", "motivation": "Gaze understanding is a critical but unexplored area in VLMs, and existing models lack systematic evaluation on this front. The introduction of VL4Gaze aims to address this gap by providing a standardized dataset for research and development in gaze understanding.", "method": "VL4Gaze creates a benchmark with 489K QA pairs across 124K images, structured into four tasks related to gaze, evaluating both commercial and open-source VLMs under different settings.", "result": "Research shows that VLMs perform poorly on gaze understanding tasks without task-specific supervision, but training on VL4Gaze improves performance across all tasks.", "conclusion": "The effectiveness of VL4Gaze highlights the significance of targeted multi-task training for gaze understanding, with the dataset and code planned for release to support further research."}}
{"id": "2512.20780", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.20780", "abs": "https://arxiv.org/abs/2512.20780", "authors": ["Ramatu Oiza Abdulsalam", "Segun Aroyehun"], "title": "Large Language Models Approach Expert Pedagogical Quality in Math Tutoring but Differ in Instructional and Linguistic Profiles", "comment": null, "summary": "Recent work has explored the use of large language models for generating tutoring responses in mathematics, yet it remains unclear how closely their instructional behavior aligns with expert human practice. We examine this question using a controlled, turn-level comparison in which expert human tutors, novice human tutors, and multiple large language models respond to the same set of math remediation conversation turns. We examine both instructional strategies and linguistic characteristics of tutoring responses, including restating and revoicing, pressing for accuracy, lexical diversity, readability, politeness, and agency. We find that large language models approach expert levels of perceived pedagogical quality on average but exhibit systematic differences in their instructional and linguistic profiles. In particular, large language models tend to underuse restating and revoicing strategies characteristic of expert human tutors, while producing longer, more lexically diverse, and more polite responses. Statistical analyses show that restating and revoicing, lexical diversity, and pressing for accuracy are positively associated with perceived pedagogical quality, whereas higher levels of agentic and polite language are negatively associated. Overall, recent large language models exhibit levels of perceived pedagogical quality comparable to expert human tutors, while relying on different instructional and linguistic strategies. These findings underscore the value of analyzing instructional strategies and linguistic characteristics when evaluating tutoring responses across human tutors and intelligent tutoring systems.", "AI": {"tldr": "研究比较了大型语言模型与人类导师在数学辅导回复上的表现，发现虽然大型语言模型在感知的教学质量方面接近专家水平，但它们采用了不同的教学和语言策略。这些发现强调了在评估辅导回应时，分析教学策略和语言特征的重要性。", "motivation": "尽管之前的工作探讨了大型语言模型在生成数学辅导回应方面的应用，但它们的教学行为在多大程度上与人类专家实践保持一致尚不清楚。因此，研究的动机在于解决这一问题。", "method": "研究通过控制的、逐轮的比较，让专家人类导师、新手人类导师以及多个大型语言模型对同一套数学补救对话轮次作出回应，从而探讨大型语言模型生成的辅导回应在多大程度上与人类专家实践相一致。研究考察了教学策略以及辅导回应的语言特征，包括重复陈述、追问准确性、词汇多样性、可读性、礼貌和代理性。", "result": "大型语言模型在感知到的教学质量上接近专家水平，但存在系统性的差异，特别是在回复中的重复陈述和追问准确性这些特征上表现不足，虽然其回复在词汇多样性上表现得更加丰富，也更加礼貌。统计分析表明，重复陈述、词汇多样性和追问准确性与感知到的教学质量呈正相关，而代理性和礼貌语言水平较高则呈负相关。", "conclusion": "近期大型语言模型在感知到的教学质量上和专家人类导师相当，它们遵循着不同的教学和语言策略。这些发现突显出，在评估人类导师和智能辅导系统时，分析教学策略和语言特征的重要性。"}}
{"id": "2512.20746", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20746", "abs": "https://arxiv.org/abs/2512.20746", "authors": ["Tony Tran", "Bin Hu"], "title": "TrashDet: Iterative Neural Architecture Search for Efficient Waste Detection", "comment": "10 pages. The paper has been accepted by the WACV 2026 workshop", "summary": "This paper addresses trash detection on the TACO dataset under strict TinyML constraints using an iterative hardware-aware neural architecture search framework targeting edge and IoT devices. The proposed method constructs a Once-for-All-style ResDets supernet and performs iterative evolutionary search that alternates between backbone and neck/head optimization, supported by a population passthrough mechanism and an accuracy predictor to reduce search cost and improve stability. This framework yields a family of deployment-ready detectors, termed TrashDets. On a five-class TACO subset (paper, plastic, bottle, can, cigarette), the strongest variant, TrashDet-l, achieves 19.5 mAP50 with 30.5M parameters, improving accuracy by up to 3.6 mAP50 over prior detectors while using substantially fewer parameters. The TrashDet family spans 1.2M to 30.5M parameters with mAP50 values between 11.4 and 19.5, providing scalable detector options for diverse TinyML deployment budgets on resource-constrained hardware. On the MAX78002 microcontroller with the TrashNet dataset, two specialized variants, TrashDet-ResNet and TrashDet-MBNet, jointly dominate the ai87-fpndetector baseline, with TrashDet-ResNet achieving 7525~$μ$J energy per inference at 26.7 ms latency and 37.45 FPS, and TrashDet-MBNet improving mAP50 by 10.2%; together they reduce energy consumption by up to 88%, latency by up to 78%, and average power by up to 53% compared to existing TinyML detectors.", "AI": {"tldr": "The paper proposes TrashDets, a family of TinyML-compatible trash detection models, achieving high accuracy and efficiency on edge and IoT devices.", "motivation": "To improve garbage detection accuracy on edge and IoT devices while minimizing resource usage through TinyML constraints.", "method": "Develops an iterative, hardware-aware neural architecture search to create a Once-for-All-style ResDets supernet, optimizing for both the backbone and neck/head components.", "result": "TrashDet-l achieves 19.5 mAP50 with 30.5M parameters, and specialized variants on MAX78002 show significant resource savings versus existing models.", "conclusion": "TrashDets provide scalable detector options for TinyML devices, enhancing both performance and efficiency in trash detection tasks."}}
{"id": "2512.20794", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20794", "abs": "https://arxiv.org/abs/2512.20794", "authors": ["Shariqah Hossain", "Lalana Kagal"], "title": "Investigating Model Editing for Unlearning in Large Language Models", "comment": null, "summary": "Machine unlearning aims to remove unwanted information from a model, but many methods are inefficient for LLMs with large numbers of parameters or fail to fully remove the intended information without degrading performance on knowledge that should be retained. Model editing algorithms solve a similar problem of changing information in models, but they focus on redirecting inputs to a new target rather than removing that information altogether. In this work, we explore the editing algorithms ROME, IKE, and WISE and design new editing targets for an unlearning setting. Through this investigation, we show that model editing approaches can exceed baseline unlearning methods in terms of quality of forgetting depending on the setting. Like traditional unlearning techniques, they struggle to encapsulate the scope of what is to be unlearned without damage to the overall model performance.", "AI": {"tldr": "本研究探索了模型编辑算法在取消学习方面的应用，显示了其相对于基准方法的优势，但也指出了挑战。", "motivation": "机器取消学习旨在从模型中移除不需要的信息，但很多方法对于参数量庞大的LLM来说效率低下，或者无法完全移除预定信息而不影响模型性能。", "method": "本研究探索了ROME、IKE和WISE这几种模型编辑算法，并为取消学习环境设计了新的编辑目标。", "result": "研究表明，与基准取消学习方法相比，模型编辑方法在遗忘质量上根据不同设置可以表现得更好。", "conclusion": "但是，与传统取消学习技术一样，它们在不损害整体模型性能的前提下，难以确定要取消学习的范围。"}}
{"id": "2512.20770", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20770", "abs": "https://arxiv.org/abs/2512.20770", "authors": ["Markus Gross", "Sai B. Matha", "Aya Fahmy", "Rui Song", "Daniel Cremers", "Henri Meess"], "title": "OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective", "comment": null, "summary": "Semantic Scene Completion (SSC) is crucial for 3D perception in mobile robotics, as it enables holistic scene understanding by jointly estimating dense volumetric occupancy and per-voxel semantics. Although SSC has been widely studied in terrestrial domains such as autonomous driving, aerial scenarios like autonomous flying remain largely unexplored, thereby limiting progress on downstream applications. Furthermore, LiDAR sensors represent the primary modality for SSC data generation, which poses challenges for most uncrewed aerial vehicles (UAVs) due to flight regulations, mass and energy constraints, and the sparsity of LiDAR-based point clouds from elevated viewpoints. To address these limitations, we introduce OccuFly, the first real-world, camera-based aerial SSC benchmark, captured at altitudes of 50m, 40m, and 30m during spring, summer, fall, and winter. OccuFly covers urban, industrial, and rural scenarios, provides 22 semantic classes, and the data format adheres to established conventions to facilitate seamless integration with existing research. Crucially, we propose a LiDAR-free data generation framework based on camera modality, which is ubiquitous on modern UAVs. By utilizing traditional 3D reconstruction, our framework automates label transfer by lifting a subset of annotated 2D masks into the reconstructed point cloud, thereby substantially minimizing manual 3D annotation effort. Finally, we benchmark the state-of-the-art on OccuFly and highlight challenges specific to elevated viewpoints, yielding a comprehensive vision benchmark for holistic aerial 3D scene understanding.", "AI": {"tldr": "介绍了OccuFly，这是一个基于相机的空中SSC真实世界基准，旨在解决空中场景的SSC挑战并提出LiDAR-free数据生成框架。", "motivation": "为了解决现有研究主要集中在陆地场景，空中场景的语义场景完成（SSC）研究较少，特别是在无人机应用中因为飞行规定、质量和能量限制以及从高视角获取的稀疏LiDAR点云带来的挑战。", "method": "提出了基于相机模式的LiDAR-free数据生成框架，通过传统3D重建技术，自动将部分标注的2D掩模提升到重建的点云中，从而大幅减少手动3D标注的工作量。", "result": "建立了第一个基于相机的空中SSC基准OccuFly，并通过基准测试展示了现有方法的表现及高视角带来的特别挑战。", "conclusion": "建立了一个全面的空中3D场景理解基准OccuFly，覆盖了多种场景和季节，并提出了一个自动化的标注传递框架以减少标注工作量。"}}
{"id": "2512.20796", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20796", "abs": "https://arxiv.org/abs/2512.20796", "authors": ["Zhengyang Shan", "Aaron Mueller"], "title": "Measuring Mechanistic Independence: Can Bias Be Removed Without Erasing Demographics?", "comment": null, "summary": "We investigate how independent demographic bias mechanisms are from general demographic recognition in language models. Using a multi-task evaluation setup where demographics are associated with names, professions, and education levels, we measure whether models can be debiased while preserving demographic detection capabilities. We compare attribution-based and correlation-based methods for locating bias features. We find that targeted sparse autoencoder feature ablations in Gemma-2-9B reduce bias without degrading recognition performance: attribution-based ablations mitigate race and gender profession stereotypes while preserving name recognition accuracy, whereas correlation-based ablations are more effective for education bias. Qualitative analysis further reveals that removing attribution features in education tasks induces ``prior collapse'', thus increasing overall bias. This highlights the need for dimension-specific interventions. Overall, our results show that demographic bias arises from task-specific mechanisms rather than absolute demographic markers, and that mechanistic inference-time interventions can enable surgical debiasing without compromising core model capabilities.", "AI": {"tldr": "研究发现语言模型中的人口统计偏差在职业和教育方面可以通过不同的方法进行针对性处理而不影响总体识别性能，该发现支持任务特定机制影响偏差的观点。", "motivation": "研究旨在探究独立的人口统计偏差机制是否能从语言模型中的一般人口统计识别中独立出来，并找到可以去偏而不影响识别能力的方法。", "method": "本研究采用多任务评估设置，将人口统计数据与姓名、职业和教育水平相关联，测量模型在去偏的同时是否能保持人口统计检测能力。作者比较了基于归因和基于相关性的方法来定位偏差特征。", "result": "研究结果表明，Gemma-2-9B中的针对性稀疏自编码器特性删除可以减少偏差而不降低识别性能。基于归因的删除减轻了种族和性别职业刻板印象并保持了姓名识别准确性，而基于相关性的删除在教育偏差方面更有效。但是移除教育任务中的归因特征会导致整体偏差增加。", "conclusion": "总体而言，表明人口统计数据的偏差来自任务特定机制，而非绝对的人口统计标记，这一点通过机制上的推断时间干预，可以实现精确的去偏，而不影响模型的核心能力。"}}
{"id": "2512.20783", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20783", "abs": "https://arxiv.org/abs/2512.20783", "authors": ["Raja Mallina", "Bryar Shareef"], "title": "NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts", "comment": "5 pages, 2 figures, and 4 tables", "summary": "Breast ultrasound (BUS) segmentation provides lesion boundaries essential for computer-aided diagnosis and treatment planning. While promptable methods can improve segmentation performance and tumor delineation when text or spatial prompts are available, many public BUS datasets lack reliable metadata or reports, constraining training to small multimodal subsets and reducing robustness. We propose NullBUS, a multimodal mixed-supervision framework that learns from images with and without prompts in a single model. To handle missing text, we introduce nullable prompts, implemented as learnable null embeddings with presence masks, enabling fallback to image-only evidence when metadata are absent and the use of text when present. Evaluated on a unified pool of three public BUS datasets, NullBUS achieves a mean IoU of 0.8568 and a mean Dice of 0.9103, demonstrating state-of-the-art performance under mixed prompt availability.", "AI": {"tldr": "tldr", "motivation": "motivation", "method": "Structure", "result": "{\n  \"tldr\": \"NullBUS is a framework that learns from BUS images with and without prompts, using nullable prompts to handle missing text, achieving state-of-the-art performance.\", \n  \"motivation\": \"The motivation for this paper is to address the challenge of lacking reliable metadata in many public BUS datasets, which limits the training and robustness of models in BUS segmentation.\", \n  \"method\": \"The method introduced in this paper is called NullBUS, which is a multimodal mixed-supervision framework that can handle BUS images with or without prompts by using nullable prompts implemented as learnable null embeddings with presence masks.\", \n  \"result\": \"The result of the paper is that NullBUS achieves a mean IoU of 0.8568 and a mean Dice of 0.9103 when evaluated on a unified pool of three public BUS datasets, demonstrating state-of-the-art performance under mixed prompt availability.\", \n  \"conclusion\": \"The conclusion is that NullBUS is effective in improving the robustness of BUS segmentation models by enabling fallback to image-only evidence when text metadata are absent and leveraging text when available, thereby achieving state-of-the-art performance.\")", "conclusion": "conclusion"}}
{"id": "2512.20812", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20812", "abs": "https://arxiv.org/abs/2512.20812", "authors": ["Nathaniël de Leeuw", "Marceau Nahon", "Mathis Reymond", "Raja Chatila", "Mehdi Khamassi"], "title": "Semantic Deception: When Reasoning Models Can't Compute an Addition", "comment": "22 pages, 5 figures", "summary": "Large language models (LLMs) are increasingly used in situations where human values are at stake, such as decision-making tasks that involve reasoning when performed by humans. We investigate the so-called reasoning capabilities of LLMs over novel symbolic representations by introducing an experimental framework that tests their ability to process and manipulate unfamiliar symbols. We introduce semantic deceptions: situations in which symbols carry misleading semantic associations due to their form, such as being embedded in specific contexts, designed to probe whether LLMs can maintain symbolic abstraction or whether they default to exploiting learned semantic associations. We redefine standard digits and mathematical operators using novel symbols, and task LLMs with solving simple calculations expressed in this altered notation. The objective is: (1) to assess LLMs' capacity for abstraction and manipulation of arbitrary symbol systems; (2) to evaluate their ability to resist misleading semantic cues that conflict with the task's symbolic logic. Through experiments with four LLMs we show that semantic cues can significantly deteriorate reasoning models' performance on very simple tasks. They reveal limitations in current LLMs' ability for symbolic manipulations and highlight a tendency to over-rely on surface-level semantics, suggesting that chain-of-thoughts may amplify reliance on statistical correlations. Even in situations where LLMs seem to correctly follow instructions, semantic cues still impact basic capabilities. These limitations raise ethical and societal concerns, undermining the widespread and pernicious tendency to attribute reasoning abilities to LLMs and suggesting how LLMs might fail, in particular in decision-making contexts where robust symbolic reasoning is essential and should not be compromised by residual semantic associations inherited from the model's training.", "AI": {"tldr": "该研究通过实验框架测试大语言模型（LLMs）处理和操作不熟悉符号的能力，引入“语义欺骗”来评估LLMs的抽象能力和抵制误导性语义线索的能力，实验结果显示LLMs在简单任务上表现不佳，过度依赖表面语义，这在决策环境中是一个严重的问题。", "motivation": "研究动力在于评估LLMs在涉及人类价值观的任务中处理新型符号表示和保持抽象能力的局限性，特别是在决策制作时。", "method": "引入一种实验框架，用新型符号重新定义标准数字和数学运算符，并要求LLMs解决使用改变后的符号表示的简单计算任务。", "result": "实验表明，语义线索显著降低了LLMs在非常简单任务上的推理性能，揭示了当前LLMs在符号操作能力上的局限性，并显示出过度依赖表面级语义的倾向。", "conclusion": "研究结果提出了伦理和社会问题，表明在需要强大符号推理的决策环境中，不应过分依赖于LLMs，因为它们可能受到模型训练中继承的残留语义关联的影响。"}}
{"id": "2512.20815", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20815", "abs": "https://arxiv.org/abs/2512.20815", "authors": ["Reeshad Khan amd John Gauch"], "title": "Learning to Sense for Driving: Joint Optics-Sensor-Model Co-Design for Semantic Segmentation", "comment": null, "summary": "Traditional autonomous driving pipelines decouple camera design from downstream perception, relying on fixed optics and handcrafted ISPs that prioritize human viewable imagery rather than machine semantics. This separation discards information during demosaicing, denoising, or quantization, while forcing models to adapt to sensor artifacts. We present a task-driven co-design framework that unifies optics, sensor modeling, and lightweight semantic segmentation networks into a single end-to-end RAW-to-task pipeline. Building on DeepLens[19], our system integrates realistic cellphone-scale lens models, learnable color filter arrays, Poisson-Gaussian noise processes, and quantization, all optimized directly for segmentation objectives. Evaluations on KITTI-360 show consistent mIoU improvements over fixed pipelines, with optics modeling and CFA learning providing the largest gains, especially for thin or low-light-sensitive classes. Importantly, these robustness gains are achieved with a compact ~1M-parameter model running at ~28 FPS, demonstrating edge deployability. Visual and quantitative analyses further highlight how co-designed sensors adapt acquisition to semantic structure, sharpening boundaries and maintaining accuracy under blur, noise, and low bit-depth. Together, these findings establish full-stack co-optimization of optics, sensors, and networks as a principled path toward efficient, reliable, and deployable perception in autonomous systems.", "AI": {"tldr": "通过一种全栈优化的端到端设计框架，本文提出了从光学到语义分割的统一管道，实现了在自动驾驶感知上的显著改进。", "motivation": "动机在于解决传统自动驾驶中相机设计与感知分离带来的信息丢失和模型适应性问题。通过任务导向的设计，实现从传感器采集到语义分割的端到端优化，增强感知系统的效率、可靠性和部署性。", "method": "提出了一种任务驱动的全栈优化框架，整合了光学设计、传感器建模和轻量级语义分割模型，设计了可学习的颜色滤光阵列，并优化了泊松-高斯噪声模型及量化过程。", "result": "传统的自动驾驶管道分离了相机设计与下游感知，依赖于固定的光学设备和手工制造的ISP，它们优先考虑的是人类可见的图像，而非机器语义。这种分离在插值、降噪或量化过程中会丢失信息，同时要求模型适应传感器的缺陷。我们提出了一种任务驱动的共同设计框架，它将光学、传感器建模和轻量级的语义分割网络统一到一个单一的端到端的RAW到任务管道中。在此基础上，我们的系统整合了现实的手机尺度镜头模型、可学习的颜色滤光阵列、泊松-高斯噪声过程和量化，并且直接针对分割目标进行优化。在KITTI-360上的评估显示，与固定管道相比，一致性mIoU有所提高，特别是通过光学建模和颜色滤光阵列的学习，因为它们对细物体或低光敏感类别带来了最大增益。重要的是，这些鲁棒性的提高是通过一个仅包含约1百万参数且以约28FPS运行的紧凑模型实现的，展示了边缘部署的能力。视觉和定量分析进一步强调了共同设计的传感器如何能够将采集适应于语义结构，这有助于在模糊、噪声和低位深度下的准确性保持。这些发现共同确立了光学、传感器和网络的全栈协同优化为一种高效、可靠且可部署的自动驾驶感知的原理路径。", "conclusion": "结论强调全栈共优化是一种高效可靠且可部署的自动驾驶感知路径，本文的方法通过优化从采集到任务的全过程，实现了在多个方面的性能提升。"}}
{"id": "2512.20817", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20817", "abs": "https://arxiv.org/abs/2512.20817", "authors": ["Kumar Satvik Chaudhary", "Chengshuai Zhao", "Fan Zhang", "Yung Hin Tse", "Garima Agrawal", "Yuli Deng", "Huan Liu"], "title": "EssayCBM: Rubric-Aligned Concept Bottleneck Models for Transparent Essay Grading", "comment": null, "summary": "Understanding how automated grading systems evaluate essays remains a significant challenge for educators and students, especially when large language models function as black boxes. We introduce EssayCBM, a rubric-aligned framework that prioritizes interpretability in essay assessment. Instead of predicting grades directly from text, EssayCBM evaluates eight writing concepts, such as Thesis Clarity and Evidence Use, through dedicated prediction heads on an encoder. These concept scores form a transparent bottleneck, and a lightweight network computes the final grade using only concepts. Instructors can adjust concept predictions and instantly view the updated grade, enabling accountable human-in-the-loop evaluation. EssayCBM matches black-box performance while offering actionable, concept-level feedback through an intuitive web interface.", "AI": {"tldr": "本文介绍了EssayCBM，一个优先考虑可解释性的评分系统，它评估八个写作概念，而不是直接预测作文分数。", "motivation": "为了提高自动评分系统的透明度和可解释性，特别是当大语言模型被视为黑箱时，我们提出EssayCBM框架。", "method": "我们介绍了一个名为EssayCBM的评分框架，该框架不直接从文本预测分数，而是评估八个写作概念，如论点清晰度和证据使用情况，通过在编码器上的专用预测头实现。这些概念评分形成一个透明的瓶颈，最后通过轻量级网络仅使用这些概念计算最终分数。", "result": "EssayCBM能够在不牺牲评分准确度的情况下，提供概念层面的操作性和反馈。", "conclusion": "EssayCBM提供了一个透明的评分流程，允许教师调整概念预测并实时查看更新后的分数，同时保持与黑箱系统相当的性能。"}}
{"id": "2512.20833", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20833", "abs": "https://arxiv.org/abs/2512.20833", "authors": ["Vidit Agrawal", "John Peters", "Tyler N. Thompson", "Mohammad Vali Sanian", "Chau Pham", "Nikita Moshkov", "Arshad Kazi", "Aditya Pillai", "Jack Freeman", "Byunguk Kang", "Samouil L. Farhi", "Ernest Fraenkel", "Ron Stewart", "Lassi Paavolainen", "Bryan A. Plummer", "Juan C. Caicedo"], "title": "CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images", "comment": "47 Pages, 23 Figures, 26 Tables", "summary": "Quantifying cell morphology using images and machine learning has proven to be a powerful tool to study the response of cells to treatments. However, models used to quantify cellular morphology are typically trained with a single microscopy imaging type. This results in specialized models that cannot be reused across biological studies because the technical specifications do not match (e.g., different number of channels), or because the target experimental conditions are out of distribution. Here, we present CHAMMI-75, an open access dataset of heterogeneous, multi-channel microscopy images from 75 diverse biological studies. We curated this resource from publicly available sources to investigate cellular morphology models that are channel-adaptive and can process any microscopy image type. Our experiments show that training with CHAMMI-75 can improve performance in multi-channel bioimaging tasks primarily because of its high diversity in microscopy modalities. This work paves the way to create the next generation of cellular morphology models for biological studies.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2512.20822", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20822", "abs": "https://arxiv.org/abs/2512.20822", "authors": ["Zhan Qu", "Michael Färber"], "title": "MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs", "comment": null, "summary": "Large Language Models (LLMs) are increasingly applied to medicine, yet their adoption is limited by concerns over reliability and safety. Existing evaluations either test factual medical knowledge in isolation or assess patient-level reasoning without verifying correctness, leaving a critical gap. We introduce MediEval, a benchmark that links MIMIC-IV electronic health records (EHRs) to a unified knowledge base built from UMLS and other biomedical vocabularies. MediEval generates diverse factual and counterfactual medical statements within real patient contexts, enabling systematic evaluation across a 4-quadrant framework that jointly considers knowledge grounding and contextual consistency. Using this framework, we identify critical failure modes, including hallucinated support and truth inversion, that current proprietary, open-source, and domain-specific LLMs frequently exhibit. To address these risks, we propose Counterfactual Risk-Aware Fine-tuning (CoRFu), a DPO-based method with an asymmetric penalty targeting unsafe confusions. CoRFu improves by +16.4 macro-F1 points over the base model and eliminates truth inversion errors, demonstrating both higher accuracy and substantially greater safety.", "AI": {"tldr": "本文提出了一种名为MediEval的系统方法，用于系统地评估大型语言模型在医学领域的知识基础和情境一致性，同时也提出了一个名为CoRFu的微调方法来优化模型的安全性和准确性。", "motivation": "大语言模型（LLMs）在医学领域的应用日益增多，但其可靠性与安全性问题仍然制约着这些模型的广泛应用。现有的评估方案要么单独测试医学领域的事实性知识，要么评估患者层面的推理，但未能验证其正确性，产生了关键的信息空白。因此，我们需要开发能全面评估医学领域模型的一个基准测试方法。", "method": "我们提出了一种名为MediEval的基准测试方法，该方法将MIMIC-IV电子健康记录(EHR)与来自UMLS及其他生物医学词汇的统一知识库相结合。通过链接EHR和知识库，MediEval生成了包含真实与反事实医学陈述的数据集，这些陈述能够在真实患者案例中系统地评估模型的知识基础与情境一致性。此外，我们提出了基于DPO的反事实风险意识微调方法(CoRFu)，利用不对称惩罚机制来针对不安全的混淆进行优化。", "result": "通过MediEval的评估发现，现有LLMs存在幻觉支持和真相逆转的关键故障模式。我们进一步提出的方法CoRFu在基准测试中表现出色，提高了16.4个宏观F1分数，并完全消除了一种重要错误类型。", "conclusion": "通过我们的评估框架，我们鉴别出当前的各种专有、开源和特定领域的LLMs所表现出的关键问题，包括幻觉支持和真相逆转等。此外，我们提出的反事实风险意识微调方法(CoRFu)显着提高了模型的准确性和安全性，实现了+16.4个宏观F1分数的提升，并消除了真相逆转错误。"}}
{"id": "2512.20839", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20839", "abs": "https://arxiv.org/abs/2512.20839", "authors": ["Putu Indah Githa Cahyani", "Komang David Dananjaya Suartana", "Novanto Yudistira"], "title": "Input-Adaptive Visual Preprocessing for Efficient Fast Vision-Language Model Inference", "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated strong performance on multimodal reasoning tasks, but their deployment remains challenging due to high inference latency and computational cost, particularly when processing high-resolution visual inputs. While recent architectures such as FastVLM improve efficiency through optimized vision encoders, existing pipelines still rely on static visual preprocessing, leading to redundant computation for visually simple inputs. In this work, we propose an adaptive visual preprocessing method that dynamically adjusts input resolution and spatial coverage based on image content characteristics. The proposed approach combines content-aware image analysis, adaptive resolution selection, and content-aware cropping to reduce visual redundancy prior to vision encoding. Importantly, the method is integrated with FastVLM without modifying its architecture or requiring retraining. We evaluate the proposed method on a subset of the DocVQA dataset in an inference-only setting, focusing on efficiency-oriented metrics. Experimental results show that adaptive preprocessing reduces per-image inference time by over 50\\%, lowers mean full generation time, and achieves a consistent reduction of more than 55\\% in visual token count compared to the baseline pipeline. These findings demonstrate that input-aware preprocessing is an effective and lightweight strategy for improving deployment-oriented efficiency of vision-language models. To facilitate reproducibility, our implementation is provided as a fork of the FastVLM repository, incorporating the files for the proposed method, and is available at https://github.com/kmdavidds/mlfastlm.", "AI": {"tldr": "研究提出了一种自适应视觉预处理方法，结合内容感知的图像分析，自适应分辨率选择和内容感知裁剪，可以减少视觉-语言模型的视觉冗余，提高效率，且不需要修改模型架构或重新训练。实验表明该方法显著提升了模型的推理效率。", "motivation": "视觉-语言模型在多模态推理任务中表现出色，但其部署面临高推理延迟和计算成本的挑战，尤其是在处理高分辨率视觉输入时。现有架构如FastVLM虽优化了视觉编码器以提高效率，但仍然依赖于静态的视觉预处理流程，导致简单输入的冗余计算。因此，本研究提出了一种自适应视觉预处理方法，旨在减少冗余计算，同时提高效率。", "method": "该研究提出了一种自适应视觉预处理方法，结合了基于内容的图像分析、自适应分辨率选择和内容感知裁剪，以减少输入图像的视觉冗余。该方法与FastVLM整合，不需要修改其架构或重新训练。", "result": "实验结果表明，在DocVQA数据集的一个子集上，使用提出的自适应预处理方法可以显著降低每个输入图像的推理时间超过50%，降低平均总生成时间，并且与基线管道相比，视觉token数量减少超过55%。", "conclusion": "研究结果表明，利用输入感知的预处理策略可以有效地提高视觉-语言模型的部署效率，该方法简单且对现有模型架构不作修改，仅在预处理阶段进行动态调整，即可显著提升模型推理效率。"}}
{"id": "2512.20848", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20848", "abs": "https://arxiv.org/abs/2512.20848", "authors": ["NVIDIA", ":", "Aaron Blakeman", "Aaron Grattafiori", "Aarti Basant", "Abhibha Gupta", "Abhinav Khattar", "Adi Renduchintala", "Aditya Vavre", "Akanksha Shukla", "Akhiad Bercovich", "Aleksander Ficek", "Aleksandr Shaposhnikov", "Alex Kondratenko", "Alexander Bukharin", "Alexandre Milesi", "Ali Taghibakhshi", "Alisa Liu", "Amelia Barton", "Ameya Sunil Mahabaleshwarkar", "Amir Klein", "Amit Zuker", "Amnon Geifman", "Amy Shen", "Anahita Bhiwandiwalla", "Andrew Tao", "Ann Guan", "Anubhav Mandarwal", "Arham Mehta", "Ashwath Aithal", "Ashwin Poojary", "Asif Ahamed", "Asma Kuriparambil Thekkumpate", "Ayush Dattagupta", "Banghua Zhu", "Bardiya Sadeghi", "Barnaby Simkin", "Ben Lanir", "Benedikt Schifferer", "Besmira Nushi", "Bilal Kartal", "Bita Darvish Rouhani", "Boris Ginsburg", "Brandon Norick", "Brandon Soubasis", "Branislav Kisacanin", "Brian Yu", "Bryan Catanzaro", "Carlo del Mundo", "Chantal Hwang", "Charles Wang", "Cheng-Ping Hsieh", "Chenghao Zhang", "Chenhan Yu", "Chetan Mungekar", "Chintan Patel", "Chris Alexiuk", "Christopher Parisien", "Collin Neale", "Damon Mosk-Aoyama", "Dan Su", "Dane Corneil", "Daniel Afrimi", "Daniel Rohrer", "Daniel Serebrenik", "Daria Gitman", "Daria Levy", "Darko Stosic", "David Mosallanezhad", "Deepak Narayanan", "Dhruv Nathawani", "Dima Rekesh", "Dina Yared", "Divyanshu Kakwani", "Dong Ahn", "Duncan Riach", "Dusan Stosic", "Edgar Minasyan", "Edward Lin", "Eileen Long", "Eileen Peters Long", "Elena Lantz", "Ellie Evans", "Elliott Ning", "Eric Chung", "Eric Harper", "Eric Tramel", "Erick Galinkin", "Erik Pounds", "Evan Briones", "Evelina Bakhturina", "Faisal Ladhak", "Fay Wang", "Fei Jia", "Felipe Soares", "Feng Chen", "Ferenc Galko", "Frankie Siino", "Gal Hubara Agam", "Ganesh Ajjanagadde", "Gantavya Bhatt", "Gargi Prasad", "George Armstrong", "Gerald Shen", "Gorkem Batmaz", "Grigor Nalbandyan", "Haifeng Qian", "Harsh Sharma", "Hayley Ross", "Helen Ngo", "Herman Sahota", "Hexin Wang", "Himanshu Soni", "Hiren Upadhyay", "Huizi Mao", "Huy C Nguyen", "Huy Q Nguyen", "Iain Cunningham", "Ido Shahaf", "Igor Gitman", "Ilya Loshchilov", "Ivan Moshkov", "Izzy Putterman", "Jan Kautz", "Jane Polak Scowcroft", "Jared Casper", "Jatin Mitra", "Jeffrey Glick", "Jenny Chen", "Jesse Oliver", "Jian Zhang", "Jiaqi Zeng", "Jie Lou", "Jimmy Zhang", "Jining Huang", "Joey Conway", "Joey Guman", "John Kamalu", "Johnny Greco", "Jonathan Cohen", "Joseph Jennings", "Joyjit Daw", "Julien Veron Vialard", "Junkeun Yi", "Jupinder Parmar", "Kai Xu", "Kan Zhu", "Kari Briski", "Katherine Cheung", "Katherine Luna", "Keshav Santhanam", "Kevin Shih", "Kezhi Kong", "Khushi Bhardwaj", "Krishna C. Puvvada", "Krzysztof Pawelec", "Kumar Anik", "Lawrence McAfee", "Laya Sleiman", "Leon Derczynski", "Li Ding", "Lucas Liebenwein", "Luis Vega", "Maanu Grover", "Maarten Van Segbroeck", "Maer Rodrigues de Melo", "Makesh Narsimhan Sreedhar", "Manoj Kilaru", "Maor Ashkenazi", "Marc Romeijn", "Mark Cai", "Markus Kliegl", "Maryam Moosaei", "Matvei Novikov", "Mehrzad Samadi", "Melissa Corpuz", "Mengru Wang", "Meredith Price", "Michael Boone", "Michael Evans", "Miguel Martinez", "Mike Chrzanowski", "Mohammad Shoeybi", "Mostofa Patwary", "Nabin Mulepati", "Natalie Hereth", "Nave Assaf", "Negar Habibi", "Neta Zmora", "Netanel Haber", "Nicola Sessions", "Nidhi Bhatia", "Nikhil Jukar", "Nikki Pope", "Nikolai Ludwig", "Nima Tajbakhsh", "Nirmal Juluru", "Oleksii Hrinchuk", "Oleksii Kuchaiev", "Olivier Delalleau", "Oluwatobi Olabiyi", "Omer Ullman Argov", "Ouye Xie", "Parth Chadha", "Pasha Shamis", "Pavlo Molchanov", "Pawel Morkisz", "Peter Dykas", "Peter Jin", "Pinky Xu", "Piotr Januszewski", "Pranav Prashant Thombre", "Prasoon Varshney", "Pritam Gundecha", "Qing Miao", "Rabeeh Karimi Mahabadi", "Ran El-Yaniv", "Ran Zilberstein", "Rasoul Shafipour", "Rich Harang", "Rick Izzo", "Rima Shahbazyan", "Rishabh Garg", "Ritika Borkar", "Ritu Gala", "Riyad Islam", "Roger Waleffe", "Rohit Watve", "Roi Koren", "Ruoxi Zhang", "Russell J. Hewett", "Ryan Prenger", "Ryan Timbrook", "Sadegh Mahdavi", "Sahil Modi", "Samuel Kriman", "Sanjay Kariyappa", "Sanjeev Satheesh", "Saori Kaji", "Satish Pasumarthi", "Sean Narentharen", "Sean Narenthiran", "Seonmyeong Bak", "Sergey Kashirsky", "Seth Poulos", "Shahar Mor", "Shanmugam Ramasamy", "Shantanu Acharya", "Shaona Ghosh", "Sharath Turuvekere Sreenivas", "Shelby Thomas", "Shiqing Fan", "Shreya Gopal", "Shrimai Prabhumoye", "Shubham Pachori", "Shubham Toshniwal", "Shuoyang Ding", "Siddharth Singh", "Simeng Sun", "Smita Ithape", "Somshubra Majumdar", "Soumye Singhal", "Stefania Alborghetti", "Stephen Ge", "Sugam Dipak Devare", "Sumeet Kumar Barua", "Suseella Panguluri", "Suyog Gupta", "Sweta Priyadarshi", "Syeda Nahida Akter", "Tan Bui", "Teodor-Dumitru Ene", "Terry Kong", "Thanh Do", "Tijmen Blankevoort", "Tom Balough", "Tomer Asida", "Tomer Bar Natan", "Tugrul Konuk", "Twinkle Vashishth", "Udi Karpas", "Ushnish De", "Vahid Noorozi", "Vahid Noroozi", "Venkat Srinivasan", "Venmugil Elango", "Vijay Korthikanti", "Vitaly Kurin", "Vitaly Lavrukhin", "Wanli Jiang", "Wasi Uddin Ahmad", "Wei Du", "Wei Ping", "Wenfei Zhou", "Will Jennings", "William Zhang", "Wojciech Prazuch", "Xiaowei Ren", "Yashaswi Karnati", "Yejin Choi", "Yev Meyer", "Yi-Fu Wu", "Yian Zhang", "Ying Lin", "Yonatan Geifman", "Yonggan Fu", "Yoshi Subara", "Yoshi Suhara", "Yubo Gao", "Zach Moshe", "Zhen Dong", "Zihan Liu", "Zijia Chen", "Zijie Yan"], "title": "Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning", "comment": null, "summary": "We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.", "AI": {"tldr": "Nemotron 3 Nano是一种改进的语言模型，它比其前一代模型在激活更少参数的情况下表现出更高的准确性，并且在推理速度上明显更快，同时支持长上下文语境。", "motivation": "开发Nemotron 3 Nano的目标是提高模型推理效率和性能，同时保持更高的准确性和代理能力，以满足日益复杂的自然语言处理需求。", "method": "我们提出了Nemotron 3 Nano 30B-A3B，这是一种混合MoE-Mamba-Transformer语言模型。它在25万亿文本标记上进行了预训练，包括比Nemotron 2多出的3万亿以上的新独特标记，之后进行了监督微调和大规模RL训练，覆盖不同环境。", "result": "Nemotron 3 Nano相比上一代Nemotron 2 Nano在激活参数更少的情况下实现了更高的精度。它在推理吞吐量方面比类似规模的开放模型（如GPT-OSS-20B和Qwen3-30B-A3B-Thinking-2507）提高了3.3倍，并且在流行的基准测试中具有更高的准确性。此外，Nemotron 3 Nano还在代理、推理和聊天能力上展现了增强的表现，并支持长达1百万标记的上下文长度。", "conclusion": "我们展示了Nemotron 3 Nano的语言模型及其在提高效率和性能方面的优点。我们已将预训练模型和后期训练的模型检查点发布到了Hugging Face平台上。"}}
{"id": "2512.20858", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20858", "abs": "https://arxiv.org/abs/2512.20858", "authors": ["Md Zabirul Islam", "Md Motaleb Hossen Manik", "Ge Wang"], "title": "ALIVE: An Avatar-Lecture Interactive Video Engine with Content-Aware Retrieval for Real-Time Interaction", "comment": null, "summary": "Traditional lecture videos offer flexibility but lack mechanisms for real-time clarification, forcing learners to search externally when confusion arises. Recent advances in large language models and neural avatars provide new opportunities for interactive learning, yet existing systems typically lack lecture awareness, rely on cloud-based services, or fail to integrate retrieval and avatar-delivered explanations in a unified, privacy-preserving pipeline.\n  We present ALIVE, an Avatar-Lecture Interactive Video Engine that transforms passive lecture viewing into a dynamic, real-time learning experience. ALIVE operates fully on local hardware and integrates (1) Avatar-delivered lecture generated through ASR transcription, LLM refinement, and neural talking-head synthesis; (2) A content-aware retrieval mechanism that combines semantic similarity with timestamp alignment to surface contextually relevant lecture segments; and (3) Real-time multimodal interaction, enabling students to pause the lecture, ask questions through text or voice, and receive grounded explanations either as text or as avatar-delivered responses.\n  To maintain responsiveness, ALIVE employs lightweight embedding models, FAISS-based retrieval, and segmented avatar synthesis with progressive preloading. We demonstrate the system on a complete medical imaging course, evaluate its retrieval accuracy, latency characteristics, and user experience, and show that ALIVE provides accurate, content-aware, and engaging real-time support.\n  ALIVE illustrates how multimodal AI-when combined with content-aware retrieval and local deployment-can significantly enhance the pedagogical value of recorded lectures, offering an extensible pathway toward next-generation interactive learning environments.", "AI": {"tldr": "ALIVE是一项能够将传统的被动讲座视频转变为动态、实时学习体验的新技术，它能在本地硬件上运行，提供虚拟角色演示讲座、语境相关的检索机制和实时交互问答功能。", "motivation": "传统的讲座视频虽然提供了灵活性，但在学习者受到困惑时缺乏即时性的澄清机制。最近的大模型和神经生成角色的发展为互动学习提供了新的机会。但现有的系统缺乏讲座意识、依赖云服务或未能实现检索和虚拟角色解释的整合。", "method": "ALIVE系统通过ASR转录、LLM优化和神经生成头部合成来生成讲座的虚拟角色展示，并结合语义相似性和时间戳对齐来检索相关的讲座片段。此外，它还支持实时多模态交互，允许学生暂停讲座、通过文本或语音提问，并获得基于文本或虚拟角色回答的解答。为了保证响应速度，系统使用轻量级嵌入模型、FAISS检索和分段的虚拟角色预加载技术。", "result": "研究者们在一次完整的医学成像课程上测试了ALIVE系统，对其检索准确性、延时特性和用户体验进行了评估，结果表明该系统能够提供准确、内容相关的和引人入胜的实时支持。", "conclusion": "ALIVE证明了当多模态AI结合了内容相关的检索和本地部署时，能够显著增强录制讲座的教育价值。这为未来生成世代的互动学习环境提供了可扩展的方法。"}}
