{"id": "2511.20662", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20662", "abs": "https://arxiv.org/abs/2511.20662", "authors": ["Hen-Hsen Huang"], "title": "Democratizing LLM Efficiency: From Hyperscale Optimizations to Universal Deployability", "comment": "8 pages, accepted as a Blue Sky Talk in AAAI 2026", "summary": "Large language models (LLMs) have become indispensable, but the most celebrated efficiency methods -- mixture-of-experts (MoE), speculative decoding, and complex retrieval-augmented generation (RAG) -- were built for hyperscale providers with vast infrastructure and elite teams. Outside that context, their benefits collapse into overhead, fragility, and wasted carbon. The result is that a handful of Big Tech companies benefit, while thousands of hospitals, schools, governments, and enterprises are left without viable options. We argue that the next frontier is not greater sophistication at scale, but robust simplicity: efficiency that thrives under modest resources and minimal expertise. We propose a new research agenda: retrofitting pretrained models with more efficient architectures without retraining, inventing lightweight fine-tuning that preserves alignment, making reasoning economical despite long chains of thought, enabling dynamic knowledge management without heavy RAG pipelines, and adopting Overhead-Aware Efficiency (OAE) as a standard benchmark. By redefining efficiency to include adoption cost, sustainability, and fairness, we can democratize LLM deployment -- ensuring that optimization reduces inequality and carbon waste rather than amplifying them.", "AI": {"tldr": "论文主张在有限资源和专业知识的背景下，通过简化和优化现有大规模语言模型，推动更广泛群体的使用，减少技术鸿沟和碳排放。", "motivation": "当前效率方法适用于超大规模基础设施和卓越团队，但在其他场景中带来高开销和脆弱性。作者意在使技术更加普惠。", "method": "提出新的研究议程，包括预训练模型的高效架构改造、轻量级微调、经济的长期思考推理能力、轻量级知识管理，以及采纳考虑开销的效率标准。", "result": "论文未提供实验结果，但提出了一种新的效率定义方式，旨在提高可接受性、可持续性和公平性。", "conclusion": "通过重新定义效率来推动大型语言模型的普及，减少不平等和碳排放。"}}
{"id": "2511.20665", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20665", "abs": "https://arxiv.org/abs/2511.20665", "authors": ["Tcharlies Schmitz"], "title": "Harmonic Token Projection (HTP): A Vocabulary-Free, Training-Free, Deterministic, and Reversible Embedding Methodology", "comment": null, "summary": "This paper introduces the Harmonic Token Projection (HTP), a reversible and deterministic framework for generating text embeddings without training, vocabularies, or stochastic parameters. Unlike neural embeddings that rely on statistical co-occurrence or optimization, HTP encodes each token analytically as a harmonic trajectory derived from its Unicode integer representation, establishing a bijective and interpretable mapping between discrete symbols and continuous vector space. The harmonic formulation provides phase-coherent projections that preserve both structure and reversibility, enabling semantic similarity estimation from purely geometric alignment. Experimental evaluation on the Semantic Textual Similarity Benchmark (STS-B) and its multilingual extension shows that HTP achieves a Spearman correlation of \\r{ho} = 0.68 in English, maintaining stable performance across ten languages with negligible computational cost and sub-millisecond latency per sentence pair. This demonstrates that meaningful semantic relations can emerge from deterministic geometry, offering a transparent and efficient alternative to data-driven embeddings. Keywords: Harmonic Token Projection, reversible embedding, deterministic encoding, semantic similarity, multilingual representation.", "AI": {"tldr": "The paper presents Harmonic Token Projection (HTP), a framework for generating text embeddings that is deterministic, reversible, and does not require training or vocabularies, showing promising results in multilingual representation with low computational cost.", "motivation": "The motivation is to offer an efficient, transparent, and deterministic alternative to data-driven embeddings, which are typically stochastic and require significant training and vocabulary data. HTP aims to maintain performance while reducing computational costs and achieving sub-millisecond latency per sentence pair.", "method": "This paper introduces Harmonic Token Projection (HTP), a reversible and deterministic framework for generating text embeddings without training, vocabularies, or stochastic parameters. HTP encodes each token as a harmonic trajectory derived from its Unicode integer representation, providing a bijective and interpretable mapping between discrete symbols and continuous vector space.", "result": "Experiments on the STS-B and its multilingual extension show HTP achieves a Spearman correlation of \\r{ho} = 0.68 in English and stable performance across ten languages, with negligible computational cost and sub-millisecond latency per sentence pair.", "conclusion": "The research demonstrates that meaningful semantic relations can arise from deterministic geometric principles, offering a new approach to semantic similarity that is both efficient and interpretable."}}
{"id": "2511.20667", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20667", "abs": "https://arxiv.org/abs/2511.20667", "authors": ["Hossein Mohanna", "Ali Ait-Bachir"], "title": "A centroid based framework for text classification in itsm environments", "comment": "11 pages", "summary": "Text classification with hierarchical taxonomies is a fundamental requirement in IT Service Management (ITSM) systems, where support tickets must be categorized into tree-structured taxonomies. We present a dual-embedding centroid-based classification framework that maintains separate semantic and lexical centroid representations per category, combining them through reciprocal rank fusion at inference time. The framework achieves performance competitive with Support Vector Machines (hierarchical F1: 0.731 vs 0.727) while providing interpretability through centroid representations. Evaluated on 8,968 ITSM tickets across 123 categories, this method achieves 5.9 times faster training and up to 152 times faster incremental updates. With 8.6-8.8 times speedup across batch sizes (100-1000 samples) when excluding embedding computation. These results make the method suitable for production ITSM environments prioritizing interpretability and operational efficiency.", "AI": {"tldr": "A dual-embedding centroid-based framework for efficient and interpretable text classification in ITSM systems, outperforming SVMs in operational efficiency while maintaining comparable classification accuracy.", "motivation": "The motivation behind this paper is to address the need for efficient and interpretable text classification methods for organizing support tickets in tree-structured taxonomies within ITSM systems.", "method": "Our method involves a dual-embedding centroid-based classification framework for text classification in ITSM systems. It maintains separate semantic and lexical centroid representations for each category, which are then combined using reciprocal rank fusion during inference.", "result": "The proposed method achieves performance on par with SVMs in hierarchical F1 scores (0.731 vs 0.727) and demonstrates significant speed improvements (5.9x faster training and up to 152x faster incremental updates).", "conclusion": "The conclusion is that the dual-embedding centroid-based framework not only matches the performance of traditional SVM methods but also excels in terms of training and updating speed, which is particularly valuable for practical ITSM operations."}}
{"id": "2511.20668", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20668", "abs": "https://arxiv.org/abs/2511.20668", "authors": ["Yongfu Xue"], "title": "PIRA: Preference-Oriented Instruction-Tuned Reward Models with Dual Aggregation", "comment": null, "summary": "Reward models are crucial for aligning Large Language Models (LLMs) with human preferences but face two representative challenges. First, traditional discriminative reward models usually concatenate questions and responses directly as input, resulting in low data efficiency. Second, reward models are vulnerable to reward overoptimization. We propose PIRA, a training paradigm addressing these issues through three strategies: (1) Reformulating question-answer pairs into preference-based instructions for clearer and more explicit task specification, (2) aggregating rewards from diverse preference tasks to reduce bias and improve robustness, and (3) averaging value-head outputs under varying dropout rates to stabilize rewards. Extensive experiments have demonstrated the effectiveness of PIRA.", "AI": {"tldr": "PIRA提出三种策略改进训练范式，解决奖励模型在效率和过度优化方面的问题，并通过实验验证了其有效性。", "motivation": "传统的奖励模型存在数据效率低和过度优化的问题，需要改进以提高大型语言模型与人类偏好的一致性。", "method": "PIRA采用三种策略：1. 将问答对改写为基于偏好的指令；2. 从多样化的偏好任务中聚合奖励；3. 在不同的dropout率下平均值头输出以稳定奖励。", "result": "实验结果证明了PIRA的有效性。", "conclusion": "PIRA通过三种策略解决了效率和过度优化的问题，实验表明该方法有效提升了奖励模型的性能。"}}
{"id": "2511.20710", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.20710", "abs": "https://arxiv.org/abs/2511.20710", "authors": ["David Amebley", "Sayanton Dibbo"], "title": "Are Neuro-Inspired Multi-Modal Vision-Language Models Resilient to Membership Inference Privacy Leakage?", "comment": null, "summary": "In the age of agentic AI, the growing deployment of multi-modal models (MMs) has introduced new attack vectors that can leak sensitive training data in MMs, causing privacy leakage. This paper investigates a black-box privacy attack, i.e., membership inference attack (MIA) on multi-modal vision-language models (VLMs). State-of-the-art research analyzes privacy attacks primarily to unimodal AI-ML systems, while recent studies indicate MMs can also be vulnerable to privacy attacks. While researchers have demonstrated that biologically inspired neural network representations can improve unimodal model resilience against adversarial attacks, it remains unexplored whether neuro-inspired MMs are resilient against privacy attacks. In this work, we introduce a systematic neuroscience-inspired topological regularization (tau) framework to analyze MM VLMs resilience against image-text-based inference privacy attacks. We examine this phenomenon using three VLMs: BLIP, PaliGemma 2, and ViT-GPT2, across three benchmark datasets: COCO, CC3M, and NoCaps. Our experiments compare the resilience of baseline and neuro VLMs (with topological regularization), where the tau > 0 configuration defines the NEURO variant of VLM. Our results on the BLIP model using the COCO dataset illustrate that MIA attack success in NEURO VLMs drops by 24% mean ROC-AUC, while achieving similar model utility (similarities between generated and reference captions) in terms of MPNet and ROUGE-2 metrics. This shows neuro VLMs are comparatively more resilient against privacy attacks, while not significantly compromising model utility. Our extensive evaluation with PaliGemma 2 and ViT-GPT2 models, on two additional datasets: CC3M and NoCaps, further validates the consistency of the findings. This work contributes to the growing understanding of privacy risks in MMs and provides evidence on neuro VLMs privacy threat resilience.", "AI": {"tldr": "研究提出了一种神经科学启发的方法来提高多模态视觉-语言模型的隐私威胁抵抗力，并证明这些神经VLM在对抗隐私攻击上更为坚韧，同时未显著降低模型效用。", "motivation": "尽管之前的研究表明，多模态模型可能会遭受隐私攻击，但仍不清楚神经科学启发的多模态模型是否对此类攻击具有抵抗力。这项研究旨在填补这一空白。", "method": "引入了一种系统性的神经科学启发的拓扑正则化（tau）框架来分析多模态视觉-语言模型（VLM）在图像-文本隐私攻击中的弹性。实验比较了基线和神经VLM（带有拓扑正则化）在BLIP、PaliGemma 2和ViT-GPT2模型上的弹性。", "result": "τ>0配置下的神经VLM在BLIP模型和COCO数据集上的MIA攻击成功率降低了24%的平均ROC-AUC，且模型效用指标MPNet和ROUGE-2未显著下降。对PaliGemma 2和ViT-GPT2模型的额外评估进一步证实了这些发现。", "conclusion": "研究为多模态模型的隐私风险提供了新的理解，并证明了神经VLM在隐私威胁方面的韧性，并没有显著降低模型效用。"}}
{"id": "2511.20669", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20669", "abs": "https://arxiv.org/abs/2511.20669", "authors": ["Mann Khatri", "Mirza Yusuf", "Rajiv Ratn Shah", "Ponnurangam Kumaraguru"], "title": "Structured Definitions and Segmentations for Legal Reasoning in LLMs: A Study on Indian Legal Data", "comment": "Accepted at BDA 2025 as short paper; This paper is long version", "summary": "Large Language Models (LLMs), trained on extensive datasets from the web, exhibit remarkable general reasoning skills. Despite this, they often struggle in specialized areas like law, mainly because they lack domain-specific pretraining. The legal field presents unique challenges, as legal documents are generally long and intricate, making it hard for models to process the full text efficiently. Previous studies have examined in-context approaches to address the knowledge gap, boosting model performance in new domains without full domain alignment. In our paper, we analyze model behavior on legal tasks by conducting experiments in three areas: (i) reorganizing documents based on rhetorical roles to assess how structured information affects long context processing and model decisions, (ii) defining rhetorical roles to familiarize the model with legal terminology, and (iii) emulating the step-by-step reasoning of courts regarding rhetorical roles to enhance model reasoning. These experiments are conducted in a zero-shot setting across three Indian legal judgment prediction datasets. Our results reveal that organizing data or explaining key legal terms significantly boosts model performance, with a minimum increase of ~1.5% and a maximum improvement of 4.36% in F1 score compared to the baseline.", "AI": {"tldr": "研究在没有全面领域对齐的情况下，通过重新组织法律文本、定义修辞角色和模拟法院推理步骤的方法，显著提高了模型处理法律任务的能力，F1分数相比基准提高了大约1.5%到4.36%。", "motivation": "尽管大型语言模型（LLMs）在广泛收集的网络数据上训练后表现出色，但它们在专业领域如法律上仍然面临挑战。这主要是因为法律文件通常冗长而复杂，限制了模型的有效处理能力。通过研究这些在法律任务上的模型行为，作者试图解决这个问题。", "method": "本研究在零样本设置下，在三个印度法律判决预测数据集上进行了实验。具体来说，进行了三项实验：(i) 重新组织文件，根据修辞角色来评估结构化信息对长文本处理的影响；(ii) 定义修辞角色来使模型熟悉法律术语；(iii) 模拟法院关于修辞角色的逐步推理来增强模型推理能力。", "result": "该研究结果显示，对数据的组织或解释关键法律术语显著提高了模型的表现。相比于基线，最低提高了约1.5%，最高提高了4.36%的F1分数。", "conclusion": "通过在零样本情景下对数据的组织或提供关键法律术语，大型语言模型在处理法律任务上的表现得到了显著提升。这表明在专业领域内，对数据结构和领域术语的优化是提高模型表现的重要方法。"}}
{"id": "2511.20714", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20714", "abs": "https://arxiv.org/abs/2511.20714", "authors": ["Inferix Team", "Tianyu Feng", "Yizeng Han", "Jiahao He", "Yuanyu He", "Xi Lin", "Teng Liu", "Hanfeng Lu", "Jiasheng Tang", "Wei Wang", "Zhiyuan Wang", "Jichao Wu", "Mingyang Yang", "Yinghao Yu", "Zeyu Zhang", "Bohan Zhuang"], "title": "Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation", "comment": null, "summary": "World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.\n  Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.", "AI": {"tldr": "Inferix作为一种新型推理引擎，致力于通过优化半自回归解码过程来实现沉浸式世界合成。它通过有效的视频流传输和配置提供实时交互和仿真，以支持世界动态的现实主义建模。", "motivation": "世界模型作为代理AI、具身AI和游戏领域的核心模拟器，具备生成长距离、物理真实且交互性高的高质量视频的能力。对此类模型的扩展可能解锁视觉感知、理解和推理的新兴能力，从而开创一种全新的范式，超越当前以LLM为中心的视觉基础模型。", "method": "采用半自回归（块扩散）解码范式，结合了扩散和自回归方法的优点，通过在每个块内应用扩散方式生成视频令牌，同时对先前的令牌进行条件约束，产生更加连贯和稳定的视频序列。此外，它引入了类似LLM的KV缓存管理，克服了标准视频扩散的限制，实现了高效、可变长度、高质生成。", "result": "Inferix不仅实现了高效、可变长度、高质量的视频生成，还在其应用中引入了交互式视频流传输和配置，支持实时互动和现实主义仿真。通过与LV-Bench的无缝集成，实现了高效的基准测试，加速了对世界模型探索的工作。", "conclusion": "Inferix作为新一代推理引擎，为沉浸式世界合成提供了优化的半自回归解码过程。它专注于世界模拟，区别于设计用于高并发场景的系统和传统视频扩散模型，通过增加互动的视频流传输和配置，支持实时互动和现实主义仿真，准确模型世界动态，并通过集成LV-Bench进行高效基准测试。"}}
{"id": "2511.20672", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20672", "abs": "https://arxiv.org/abs/2511.20672", "authors": ["Saad Mankarious", "Ayah Zirikly", "Daniel Wiechmann", "Elma Kerz", "Edward Kempa", "Yu Qiao"], "title": "MindSET: Advancing Mental Health Benchmarking through Large-Scale Social Media Data", "comment": null, "summary": "Social media data has become a vital resource for studying mental health, offering real-time insights into thoughts, emotions, and behaviors that traditional methods often miss. Progress in this area has been facilitated by benchmark datasets for mental health analysis; however, most existing benchmarks have become outdated due to limited data availability, inadequate cleaning, and the inherently diverse nature of social media content (e.g., multilingual and harmful material). We present a new benchmark dataset, \\textbf{MindSET}, curated from Reddit using self-reported diagnoses to address these limitations. The annotated dataset contains over \\textbf{13M} annotated posts across seven mental health conditions, more than twice the size of previous benchmarks. To ensure data quality, we applied rigorous preprocessing steps, including language filtering, and removal of Not Safe for Work (NSFW) and duplicate content. We further performed a linguistic analysis using LIWC to examine psychological term frequencies across the eight groups represented in the dataset. To demonstrate the dataset utility, we conducted binary classification experiments for diagnosis detection using both fine-tuned language models and Bag-of-Words (BoW) features. Models trained on MindSET consistently outperformed those trained on previous benchmarks, achieving up to an \\textbf{18-point} improvement in F1 for Autism detection. Overall, MindSET provides a robust foundation for researchers exploring the intersection of social media and mental health, supporting both early risk detection and deeper analysis of emerging psychological trends.", "AI": {"tldr": "本文介绍了一个新的心理健康分析基准数据集---MindSET，该数据集包含13M个标注帖子及七种心理健康状况，经过严格的预处理。基于MindSET的数据集，训练的模型在诊断检测上表现优于以前的基准数据集，特别是自闭症检测的F1值提高18点。MindSET为研究社交媒体与心理健康提供了坚实的基础。", "motivation": "现有心理健康分析基准数据集由于数据可用性的限制、不充分的清洗和社交媒体内容的多样性（如多语言和有害材料）而变得过时。为了克服这些限制并提供高质量的数据支持心理健康研究。", "method": "构建了一个名为MindSET的新基准数据集，该数据集从Reddit上通过自我报告的诊断进行筛选，包括跨越七种心理健康状况的超过13M个标注帖子。进行了严格的预处理步骤，比如语言过滤和删除非工作安全（NSFW）及重复内容。使用LIWC进行了语言学分析，考察了数据集八个分组中的心理术语频率。", "result": "使用基于MindSET数据集训练的模型在二元分类实验中的诊断检测上表现优于以前的基准数据集，特别是自闭症检测的F1值提高了18点。", "conclusion": "MindSET数据集解决了现有数据集的几个限制，提供了质量更高的心理健康分析资源。它是一个研究社交媒体与心理健康之间关系的强大基础，支持早期风险检测和新兴心理趋势的深入分析。"}}
{"id": "2511.20716", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.20716", "abs": "https://arxiv.org/abs/2511.20716", "authors": ["Kun Guo", "Yun Shen", "Xijun Wang", "Chaoqun You", "Yun Rui", "Tony Q. S. Quek"], "title": "Video Object Recognition in Mobile Edge Networks: Local Tracking or Edge Detection?", "comment": null, "summary": "Fast and accurate video object recognition, which relies on frame-by-frame video analytics, remains a challenge for resource-constrained devices such as traffic cameras. Recent advances in mobile edge computing have made it possible to offload computation-intensive object detection to edge servers equipped with high-accuracy neural networks, while lightweight and fast object tracking algorithms run locally on devices. This hybrid approach offers a promising solution but introduces a new challenge: deciding when to perform edge detection versus local tracking. To address this, we formulate two long-term optimization problems for both single-device and multi-device scenarios, taking into account the temporal correlation of consecutive frames and the dynamic conditions of mobile edge networks. Based on the formulation, we propose the LTED-Ada in single-device setting, a deep reinforcement learning-based algorithm that adaptively selects between local tracking and edge detection, according to the frame rate as well as recognition accuracy and delay requirement. In multi-device setting, we further enhance LTED-Ada using federated learning to enable collaborative policy training across devices, thereby improving its generalization to unseen frame rates and performance requirements. Finally, we conduct extensive hardware-in-the-loop experiments using multiple Raspberry Pi 4B devices and a personal computer as the edge server, demonstrating the superiority of LTED-Ada.", "AI": {"tldr": "本文提出了LTED-Ada算法，能够在单设备和多设备场景下优化视频对象识别的帧率、识别准确性和延迟要求，通过硬件实验验证了其有效性。", "motivation": "为了应对计算资源受限的设备上进行视频对象识别这一挑战，尤其是针对交通摄像头等设备，作者提出了一种混合方法，将计算密集型的对象检测任务卸载到边缘服务器，而轻量级和快速的对象跟踪算法在设备上运行。这需要解决在边缘检测和本地追踪之间进行选择的问题。", "method": "本文提出了一种基于深度强化学习的算法LTED-Ada，该算法可以自适应地选择本地跟踪和边缘检测，从而在单设备和多设备场景下优化视频对象识别的帧率、识别准确性和延迟要求。多设备场景下，通过联邦学习增强LTED-Ada，以提高其对未知帧率和性能要求的泛化能力。", "result": "作者进行了大量的硬件实验，使用多个Raspberry Pi 4B设备和个人电脑作为边缘服务器，实验结果证明了LTED-Ada的优越性。", "conclusion": "实验表明，基于深度强化学习的LTED-Ada算法能够有效解决在资源受限设备上进行视频对象识别时面临的挑战，并在多设备协同学习下表现出色。"}}
{"id": "2511.20673", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.20673", "abs": "https://arxiv.org/abs/2511.20673", "authors": ["Zheng Hui", "Xiaokai Wei", "Reza Shirkavand", "Chen Wang", "Weizhi Zhang", "Alejandro Peláez", "Michelle Gong"], "title": "Semantics Meet Signals: Dual Codebook Representationl Learning for Generative Recommendation", "comment": null, "summary": "Generative recommendation has recently emerged as a powerful paradigm that unifies retrieval and generation, representing items as discrete semantic tokens and enabling flexible sequence modeling with autoregressive models. Despite its success, existing approaches rely on a single, uniform codebook to encode all items, overlooking the inherent imbalance between popular items rich in collaborative signals and long-tail items that depend on semantic understanding. We argue that this uniform treatment limits representational efficiency and hinders generalization. To address this, we introduce FlexCode, a popularity-aware framework that adaptively allocates a fixed token budget between a collaborative filtering (CF) codebook and a semantic codebook. A lightweight MoE dynamically balances CF-specific precision and semantic generalization, while an alignment and smoothness objective maintains coherence across the popularity spectrum. We perform experiments on both public and industrial-scale datasets, showing that FlexCode consistently outperform strong baselines. FlexCode provides a new mechanism for token representation in generative recommenders, achieving stronger accuracy and tail robustness, and offering a new perspective on balancing memorization and generalization in token-based recommendation models.", "AI": {"tldr": "FlexCode是一种基于流行度意识的框架，能够自适应地在协同过滤和语义编码之间分配固定标记预算。该框架通过轻量级MoE平衡CF精度和语义泛化，并通过一致性和平滑度目标保持整个流行谱的一致性。实验表明，FlexCode在准确性和尾部健壮性方面优于强基线，并提供了一种新的基于标记的推荐机制，改善了记忆和泛化之间的平衡。", "motivation": "现有的生成推荐方法依赖于单一统一的编码本，忽略了流行项目丰富的协作信号和长尾项目的语义理解之间的不平衡。这种统一的处理方式限制了表示效率，并阻碍了泛化。", "method": "Structure", "result": "{\n  \"tldr\": \"FlexCode是一种基于流行度意识的框架，能够自适应地在协同过滤和语义编码之间分配固定标记预算。该框架通过轻量级MoE平衡CF精度和语义泛化，并通过一致性和平滑度目标保持整个流行谱的一致性。实验表明，FlexCode在准确性和尾部健壮性方面优于强基线，并提供了一种新的基于标记的推荐机制，改善了记忆和泛化之间的平衡。\",\n  \"motivation\": \"现有的生成推荐方法依赖于单一统一的编码本，忽略了流行项目丰富的协作信号和长尾项目的语义理解之间的不平衡。这种统一的处理方式限制了表示效率，并阻碍了泛化。\",\n  \"method\": \"FlexCode通过分配固定的标记预算来平衡协同过滤和语义理解之间的需求，使用轻量级MoE模型来动态调整精度，并使用一致性和平滑度目标来保持不同流行度项目之间的连贯性。\",\n  \"result\": \"实验表明，FlexCode在准确性和尾部健壮性方面表现优于基线模型，实现了更好的性能。\",\n  \"conclusion\": \"FlexCode提供了一种新的标记表示机制，在生成推荐模型中实现了更强的准确性和尾部健壮性，同时提供了一种新的视角来平衡记忆和泛化。\")", "conclusion": "FlexCode提供了一种新的标记表示机制，在生成推荐模型中实现了更强的准确性和尾部健壮性，同时提供了一种新的视角来平衡记忆和泛化。"}}
{"id": "2511.20720", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20720", "abs": "https://arxiv.org/abs/2511.20720", "authors": ["Haibo HU", "Lianming Huang", "Nan Guan", "Chun Jason Xue"], "title": "DeeAD: Dynamic Early Exit of Vision-Language Action for Efficient Autonomous Driving", "comment": null, "summary": "Vision-Language Action (VLA) models unify perception, reasoning, and trajectory generation for autonomous driving, but suffer from significant inference latency due to deep transformer stacks. We present DeeAD, a training-free, action-guided early-exit framework that accelerates VLA planning by evaluating the physical feasibility of intermediate trajectories. Instead of relying on confidence scores, DeeAD terminates inference when predicted trajectories align with lightweight planning priors (e.g., Navigation or Low-precision Planning) within a tolerable deviation (<2m). To improve efficiency, we introduce a multi-hop controller that adaptively skips redundant layers based on the change rate of scores. DeeAD integrates into existing VLA models, such as ORION, without requiring retraining. Experiments on the Bench2Drive benchmark demonstrate up to 28% transformer-layer sparsity and 29% latency reduction, while preserving planning quality and safety.", "AI": {"tldr": "DeeAD 是一种无需训练的框架，通过评估预测轨迹与轻量级规划优先事项的偏差来加速 VLA 模型的规划过程。它在 Bench2Drive 基准上表现出了显著的延迟减少和稀疏性提升。", "motivation": "视觉语言动作模型将感知、推理和轨迹生成统一用于自动驾驶，但由于深层变压器堆栈导致显著的推理延迟，因此引入 DeeAD 来解决这一问题。", "method": "DeeAD 是一种无需训练的、基于动作引导的提前退出框架，它通过评估中间轨迹的物理可行性来加速视觉语言动作模型的规划过程。", "result": "实验结果表明，DeeAD 在 Bench2Drive 评估基准上实现了高达 28% 的 transformer 层稀疏性和 29% 的延迟减少，同时保持了规划质量和安全性。", "conclusion": "DeeAD 证明了其有效性和应用价值，可在不重新训练的情况下集成到现有的 VLA 模型中，减少延迟并保持规划质量。"}}
{"id": "2511.20677", "categories": ["cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.20677", "abs": "https://arxiv.org/abs/2511.20677", "authors": ["Saleh Almohaimeed", "May Alsofyani", "Saad Almohaimeed", "Mansour Al Ghanim", "Liqiang Wang"], "title": "Prompt Engineering Techniques for Context-dependent Text-to-SQL in Arabic", "comment": "Accepted at IJCNN 2025 (to appear in IEEE/IJCNN proceedings). This arXiv submission corresponds to the camera-ready version", "summary": "In recent years, the task of cross-domain, context-dependent text-to-SQL has received significant attention. Enables users with no prior knowledge of SQL to have a conversation with databases using natural language. However, most of the available datasets and research have been conducted in English, along with some work in Chinese. To this date, no effort has been made to address this task in the Arabic language. In this paper, we introduce Ar-SParC, the first Arabic cross-domain, context-dependent text-to-SQL dataset. The dataset consists of 3,450 sequences of interrelated questions, each sequence containing an average of approximately three questions, which results in a total of 10225 questions along with their corresponding SQL queries. We conducted 40 experiments on the Ar-SParC dataset using two large language models, GPT-3.5-turbo and GPT-4.5-turbo, applying 10 different prompt engineering techniques, including four question representation methods and six in-context learning techniques. Furthermore, we developed a novel approach named GAT corrector, which enhanced the performance across all 40 experiments, yielding an average improvement of 1.9% in execution accuracy (EX) and 1.9% in interaction accuracy (IX) under zero-shot settings, and an average increase of 1.72% EX and 0.92% IX under in-context learning settings. Finally, we conducted an ablation study with two more experiments to explain why the GAT corrector outperformed the previous GAT verifier technique, particularly for the Arabic language.", "AI": {"tldr": "本文介绍了Ar-SParC，首个阿拉伯语的跨域上下文相关文本到SQL数据集，并通过使用GPT大型语言模型和开发GAT corrector方法，在零样本和上下文学习设置下显著提高了执行精度和交互精度。", "motivation": "尽管跨域文本到SQL的研究主要集中在英语和汉语上，阿拉伯语的此类研究并未得到关注。因此，作者开发了Ar-SParC，以填补阿拉伯语文本到SQL任务的研究空白。", "method": "作者利用GPT-3.5-turbo和GPT-4.5-turbo两种大型语言模型，采用10种不同的提示工程技巧，包括四种问题表示方法和六种上下文学习技术。此外，还开发了一种新的方法GAT corrector来进一步提升模型表现。", "result": "通过对Ar-SParC数据集进行40次实验，GAT corrector在零样本设置下提升了1.9%的执行准确性和1.9%的交互准确性；在上下文学习设置中，分别提升了1.72%和0.92%的执行和交互准确性。", "conclusion": "通过使用GAT corrector，阿拉伯语的跨域上下文相关文本到SQL任务的性能得到了提升，证明了该方法的有效性，特别是在阿拉伯语的处理上。"}}
{"id": "2511.20721", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2511.20721", "abs": "https://arxiv.org/abs/2511.20721", "authors": ["Guillaume Letellier", "Siddharth Srivastava", "Frédéric Jurie", "Gaurav Sharma"], "title": "Foundry: Distilling 3D Foundation Models for the Edge", "comment": null, "summary": "Foundation models pre-trained with self-supervised learning (SSL) on large-scale datasets have become powerful general-purpose feature extractors. However, their immense size and computational cost make them prohibitive for deployment on edge devices such as robots and AR/VR headsets. Existing compression techniques like standard knowledge distillation create efficient 'specialist' models but sacrifice the crucial, downstream-agnostic generality that makes foundation models so valuable.  In this paper, we introduce Foundation Model Distillation (FMD), a new paradigm for compressing large SSL models into compact, efficient, and faithful proxies that retain their general-purpose representational power. We present Foundry, the first implementation of FMD for 3D point clouds. Our approach, Foundry, trains a student to learn a compressed set of SuperTokens that reconstruct the teacher's token-level representations, capturing a compact basis of its latent space. A single distilled model maintains strong transferability across diverse downstream tasks-classification, part segmentation, and few-shot scenarios-approaching full foundation-model performance while using significantly fewer tokens and FLOPs, making such models more practical for deployment on resourceconstrained hardware.", "AI": {"tldr": "本文提出了用于压缩大规模自监督学习模型的新范式Foundation Model Distillation（FMD），并介绍了首个3D点云实现——Foundry。通过该方法，训练一个学生模型来学习一组压缩的SuperTokens以重建老师的标记级表示，从而实现模型的压缩但保持强大的迁移性。", "motivation": "预训练的自监督学习（SSL）模型在网络上大规模数据训练后成为强大的通用特征提取器，但其庞大的规模和计算成本限制了其在边缘设备（如机器人和AR/VR头盔）上的部署。现有的压缩技术，比如知识蒸馏，虽然可以创建高效模型，但也牺牲了这些模型重要的、无下游特定性的通用性。", "method": "我们提出了Foundation Model Distillation (FMD)，这是一种新的压缩大规模自监督学习（SSL）模型的方法，通过将其压缩为紧凑且高效的代理模型，同时保留其通用表示能力。我们介绍了Foundry，它是首个用于3D点云的FMD实现。我们的方法训练一个学生模型来学习一组压缩的SuperTokens，这些SuperTokens可以重建老师的标记级表示，捕捉其潜在空间的紧凑基底。", "result": "单个蒸馏模型在多样化下游任务中显示出强大的迁移性能，包括分类、部件分割和小样本场景，同时使用显著更少的标记和FLOP，使其更适用于资源受限硬件的部署。", "conclusion": "研究展示了一种新的压缩大规模自监督学习模型的方法，名为Foundation Model Distillation（FMD）。通过该方法，模型不仅可以保持高度的迁移性能，同时还能显著降低模型参数和计算复杂度，使这些模型更适应于资源受限的硬件环境中部署。"}}
{"id": "2511.20680", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20680", "abs": "https://arxiv.org/abs/2511.20680", "authors": ["Matthew W. Kenaston", "Umair Ayub", "Mihir Parmar", "Muhammad Umair Anjum", "Syed Arsalan Ahmed Naqvi", "Priya Kumar", "Samarth Rawal", "Aadel A. Chaudhuri", "Yousef Zakharia", "Elizabeth I. Heath", "Tanios S. Bekaii-Saab", "Cui Tao", "Eliezer M. Van Allen", "Ben Zhou", "YooJung Choi", "Chitta Baral", "Irbaz Bin Riaz"], "title": "Cognitive bias in LLM reasoning compromises interpretation of clinical oncology notes", "comment": "24 pages, 6 figures, 1 supplementary figure, 3 tables", "summary": "Despite high performance on clinical benchmarks, large language models may reach correct conclusions through faulty reasoning, a failure mode with safety implications for oncology decision support that is not captured by accuracy-based evaluation. In this two-cohort retrospective study, we developed a hierarchical taxonomy of reasoning errors from GPT-4 chain-of-thought responses to real oncology notes and tested its clinical relevance. Using breast and pancreatic cancer notes from the CORAL dataset, we annotated 600 reasoning traces to define a three-tier taxonomy mapping computational failures to cognitive bias frameworks. We validated the taxonomy on 822 responses from prostate cancer consult notes spanning localized through metastatic disease, simulating extraction, analysis, and clinical recommendation tasks. Reasoning errors occurred in 23 percent of interpretations and dominated overall errors, with confirmation bias and anchoring bias most common. Reasoning failures were associated with guideline-discordant and potentially harmful recommendations, particularly in advanced disease management. Automated evaluators using state-of-the-art language models detected error presence but could not reliably classify subtypes. These findings show that large language models may provide fluent but clinically unsafe recommendations when reasoning is flawed. The taxonomy provides a generalizable framework for evaluating and improving reasoning fidelity before clinical deployment.", "AI": {"tldr": "通过开发一个推理错误的分类体系，研究发现大型语言模型在临床决策支持中的推理错误可能导致潜在有害的医疗建议，并且需要更深入的评估以确保临床安全性。", "motivation": "虽然在临床基准上表现出色，但大型语言模型可能会通过错误的推理得出正确的结论，这种失败模式在肿瘤决策支持方面具有安全性含义，但无法通过基于准确性的评估来捕捉。", "method": "通过开发一个分层的推理错误分类体系，对GPT-4对真实肿瘤学笔记的chain-of-thought响应进行分析。使用CORAL数据集中的乳腺癌和胰腺癌笔记对600个推理轨迹进行了注释，定义了一个三级分类体系，将计算错误映射到认知偏见框架上。在前列腺癌咨询笔记上进行了验证，涉及局部至转移性疾病，模拟了提取、分析和临床建议任务。", "result": "推理错误出现在23%的解释中，占所有错误的主要部分，其中最常见的是确认偏见和锚定偏见。推理错误与偏离指南的和潜在有害的建议有关，特别是在晚期疾病管理中。自动评估器可以检测错误的存在，但不能可靠地分类子类型。这些发现表明，在推理不当时，大型语言模型可能会提供流畅但临床不安全的建议。", "conclusion": "该研究提供了分类系统，作为评估和改进在临床部署前推理完整性的通用框架。"}}
{"id": "2511.20722", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20722", "abs": "https://arxiv.org/abs/2511.20722", "authors": ["Minh Thong Doi", "Jan Butora", "Vincent Itier", "Jérémie Boulanger", "Patrick Bas"], "title": "DinoLizer: Learning from the Best for Generative Inpainting Localization", "comment": null, "summary": "We introduce DinoLizer, a DINOv2-based model for localizing manipulated regions in generative inpainting. Our method builds on a DINOv2 model pretrained to detect synthetic images on the B-Free dataset. We add a linear classification head on top of the Vision Transformer's patch embeddings to predict manipulations at a $14\\times 14$ patch resolution. The head is trained to focus on semantically altered regions, treating non-semantic edits as part of the original content. Because the ViT accepts only fixed-size inputs, we use a sliding-window strategy to aggregate predictions over larger images; the resulting heatmaps are post-processed to refine the estimated binary manipulation masks. Empirical results show that DinoLizer surpasses state-of-the-art local manipulation detectors on a range of inpainting datasets derived from different generative models. It remains robust to common post-processing operations such as resizing, noise addition, and JPEG (double) compression. On average, DinoLizer achieves a 12\\% higher Intersection-over-Union (IoU) than the next best model, with even greater gains after post-processing. Our experiments with off-the-shelf DINOv2 demonstrate the strong representational power of Vision Transformers for this task. Finally, extensive ablation studies comparing DINOv2 and its successor, DINOv3, in deepfake localization confirm DinoLizer's superiority. The code will be publicly available upon acceptance of the paper.", "AI": {"tldr": "我们提出了DinoLizer，这是一种基于DINOv2的模型，用于定位生成式图像中的操纵区域。DinoLizer在多个测试数据集中表现优于现有最佳模型，并且对各种后处理操作具有鲁棒性。", "motivation": "我们的动机是开发一种能够定位生成式图像中被操纵区域的模型，以加强图像完整性检测的能力。", "method": "我们的方法基于DINOv2模型，该模型预先训练用于检测B-Free数据集上的合成图像。我们在Vision Transformer的patch嵌入上添加了一个线性分类头部，以预测14x14块分辨率的操纵区域。头部训练专注于语义改变的区域，而非语义编辑被视为原始内容的一部分。由于ViT只能接受固定大小的输入，我们使用了滑动窗口策略来聚合大图像上的预测结果。最终的热图被后处理以细化估计的二进制操纵掩码。", "result": "实验结果显示，DinoLizer在从不同生成模型派生的图像集上超越了现有的局部操纵检测器。它对于常见的后处理操作（如重采样、噪声添加以及JPEG压缩）也同样表现出色。平均情况下，DinoLizer与次优模型相比，交并比（IoU）高出12%，且在后处理后改进更为明显。", "conclusion": "通过广泛的消融研究，DinoLizer在网络假视频定位上表现优于DINOv2及其继任者DINOv3。我们的实验验证了Vision Transformers在本任务上的强大表现能力。"}}
{"id": "2511.20683", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20683", "abs": "https://arxiv.org/abs/2511.20683", "authors": ["Bharadwaj Yadavalli"], "title": "Dynamic Template Selection for Output Token Generation Optimization: MLP-Based and Transformer Approaches", "comment": "20 pages, 4 figures, includes production-scale experiments across OpenAI GPT-4, Google Gemini, and Anthropic Claude; code available upon request", "summary": "Contemporary large language model deployments typically employ uniform prompting strategies across diverse query types, applying verbose response patterns to both complex analytical tasks and straightforward factual questions. This one-size-fits-all methodology leads to substantial token inefficiency, a concern amplified by the significant cost differential between input and output tokens--the latter commanding 4-8x higher prices across major providers. We present Dynamic Template Selection (DTS), which adaptively matches response templates to query complexity, achieving significant cost reductions without compromising response quality.\n  We compared two routing approaches: a simple MLP that uses pre-computed embeddings and a more complex fine-tuned RoBERTa transformer. Through comprehensive evaluation on 1,000 MMLU questions, we find that the MLP router achieves 90.5% routing accuracy on held-out test data, marginally exceeding RoBERTa's performance (89.5%) despite utilizing 125M fewer parameters. Notably, our empirical analysis reveals provider-agnostic behavior in template selection--routing decisions generalize effectively across 3 major LLM providers (OpenAI GPT-4, Google Gemini, and Anthropic Claude), as validated through 9,000 production API calls. While routing accuracy remains consistent at 90.5% across providers, observed token reductions vary from 32.6% to 33.9%, reflecting provider-specific generation characteristics.\n  This work contributes several key elements: formal problem formulation with theoretical grounding in machine learning, four algorithms with corresponding complexity analyses, and extensive empirical validation across production systems.", "AI": {"tldr": "该研究介绍了一种名为动态模板选择（DTS）的方法，针对不同复杂度的查询匹配对应的响应模板，以提高响应质量同时降低成本。实验表明，MLP路由方式略胜过微调的RoBERTa模型，并在三个主要供应商不同模型上验证通用性。", "motivation": "现有的大型语言模型通常对各种类型的查询使用统一的提示策略，无论是复杂分析任务还是简单事实问题，导致了大量的令牌浪费。由于输入令牌与输出令牌之间的成本差异（后者成本高出4-8倍），这种一刀切的方法造成了更高的运行成本。为了解决这些问题，该研究提出了一种适应查询复杂度的响应模板动态选择方法，以此来提高效率和经济性。", "method": "该研究提出了两种路由方法以实现动态模板选择（DTS）：使用预计算嵌入的简单多层感知器（MLP）和更复杂的微调RoBERTa转换器模型。这些方法旨在根据不同类型的查询匹配响应模板，从而优化令牌使用。", "result": "该研究提出了一种名为动态模板选择（DTS）的方法，以适应性地匹配查询复杂度和响应模板，从而在无损响应质量的情况下显著降低使用成本。实验中比较了两种路由方法：一种是使用预计算嵌入的简单多层感知器（MLP），另一种是更复杂的微调RoBERTa转换器模型。通过针对1,000个MMLU问题的综合评估，MLP路由器在保留测试数据上的路由准确率达到90.5%，略高于RoBERTa的89.5%，同时参数减少了1.25亿。实验结果表明，在三个主要的LLM供应商（OpenAI的GPT-4，Google的Gemini和Anthropic的Claude）之间的路由选择可以通用，成本效益在不同供应商之间略有差异。该研究还提出了具体问题的形式化处理方法并验证了其实用价值。", "conclusion": "研究表明使用MLP作为路由机制可以高效匹配复杂度和响应模板，在不损失质量的情况下实现了显著的成本减少，与其他复杂的模型相比，使用更少的参数实现了更高的准确性。该研究还为实际生产系统的应用提供了广泛的经验验证。"}}
{"id": "2511.20737", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20737", "abs": "https://arxiv.org/abs/2511.20737", "authors": ["Daeheon Jeong", "Seoyeon Byun", "Kihoon Son", "Dae Hyun Kim", "Juho Kim"], "title": "CANVAS: A Benchmark for Vision-Language Models on Tool-Based User Interface Design", "comment": null, "summary": "User interface (UI) design is an iterative process in which designers progressively refine their work with design software such as Figma or Sketch. Recent advances in vision language models (VLMs) with tool invocation suggest these models can operate design software to edit a UI design through iteration. Understanding and enhancing this capacity is important, as it highlights VLMs' potential to collaborate with designers within conventional software. However, as no existing benchmark evaluates tool-based design performance, the capacity remains unknown. To address this, we introduce CANVAS, a benchmark for VLMs on tool-based user interface design. Our benchmark contains 598 tool-based design tasks paired with ground-truth references sampled from 3.3K mobile UI designs across 30 function-based categories (e.g., onboarding, messaging). In each task, a VLM updates the design step-by-step through context-based tool invocations (e.g., create a rectangle as a button background), linked to design software. Specifically, CANVAS incorporates two task types: (i) design replication evaluates the ability to reproduce a whole UI screen; (ii) design modification evaluates the ability to modify a specific part of an existing screen. Results suggest that leading models exhibit more strategic tool invocations, improving design quality. Furthermore, we identify common error patterns models exhibit, guiding future work in enhancing tool-based design capabilities.", "AI": {"tldr": "论文引入CANVAS基准测试，用于评估基于工具的用户界面设计中的视觉语言模型（VLM）的能力，其结果显示模型的工具使用策略性加强，提升设计质量，并帮助识别模型错误和未来改进方向。", "motivation": "研究动机是由于目前没有现有的基准评估工具对设计性能的影响，视觉语言模型在基于工具的设计中的潜在能力未知。理解并提升这种能力很重要，因为它突显了VLMs潜在地可以与设计者在传统软件中进行协作的能力。", "method": "该论文引入了CANVAS基准测试，用于评估基于工具的用户界面设计中的视觉语言模型（VLM）的能力。CANVAS包含598个基于工具的设计任务，这些任务与从30个功能类别中选取的3.3K个移动UI设计的真实参考相关联。每个任务中，VLM通过基于上下文的工具调用来逐步更新设计。任务类型分为两类：设计复制和设计修改。", "result": "结果显示，领先的模型在工具调用方面变得更加策略性，从而提高了设计质量。此外，还识别了模型展示出的常见错误模式，指导未来的工具设计能力的增强工作。", "conclusion": "该论文通过引入CANVAS基准测试，证明了现有视觉语言模型（VLMs）可以在工具调用方面做出更策略性的决定，从而提高设计质量。同时，通过这项研究，我们可以进一步了解和改善VLM在用户界面设计中的合作能力。"}}
{"id": "2511.20691", "categories": ["cs.CL", "cond-mat.mtrl-sci", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.20691", "abs": "https://arxiv.org/abs/2511.20691", "authors": ["Lijun Shang", "Yadong Yu", "Wenqiang Kang", "Jian Zhou", "Dongyue Gao", "Pan Xiang", "Zhe Liu", "Mengyan Dai", "Zhonglu Guo", "Zhimei Sun"], "title": "LLMs-Powered Accurate Extraction, Querying and Intelligent Management of Literature derived 2D Materials Data", "comment": "100 pages (18 pages main text, 82 pages supplementary material), 5 figures. Supplementary material starts from page 19", "summary": "Two-dimensional (2D) materials have showed widespread applications in energy storage and conversion owning to their unique physicochemical, and electronic properties. Most of the valuable information for the materials, such as their properties and preparation methods, is included in the published research papers. However, due to the dispersion of synthe", "AI": {"tldr": "The abstract discusses the significance of 2D materials in energy applications and the importance of compiling their properties and preparation methods from existing literature.", "motivation": "To compile scattered information on 2D materials for easier reference and to potentially identify new applications or improvement areas.", "method": "The paper aims to collect and organize valuable information about 2D materials' properties and preparation methods from published research papers.", "result": "Not explicitly provided in the abstract.", "conclusion": "Not explicitly provided in the abstract."}}
{"id": "2511.20770", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.20770", "abs": "https://arxiv.org/abs/2511.20770", "authors": ["Raghuveer Thirukovalluru", "Xiaochuang Han", "Bhuwan Dhingra", "Emily Dinan", "Maha Elbayad"], "title": "Text-Guided Semantic Image Encoder", "comment": "20 pages, 6 figures", "summary": "Image encoders, a fundamental component of vision-language models (VLMs), are typically pretrained independently before being aligned with a language model. This standard paradigm results in encoders that process images agnostically, without regard to the specific downstream task or text query. To address this limitation, we propose the Text-Guided Semantic Image Encoder (TIE), which generates image representations conditioned on the input text query. VLMs equipped with TIE outperform their conventional counterparts by +1.5 and +1.3 points on average across nine image-to-text benchmarks at the 1B and 3B scales, respectively, with gains reaching up to 6 points on tasks such as DocVQA and InfoVQA. Moreover, TIE-based VLMs attain superior performance while utilizing only half as many image tiles (tokens), resulting in notably improved inference efficiency. TIE also generalizes well with generic queries, indicating that text-conditioned training effectively optimizes the encoder to capture key visual features. Qualitative analysis confirms that TIE consistently attends to query-relevant regions, enhancing both interpretability and query-specific grounding.", "AI": {"tldr": "提出了文本引导的语义图像编码器(TIE)，该编码器生成的图像表示依赖于输入的文本查询，提高了视觉语言模型（VLM）在多个基准测试中的性能，并且使用较少的图像块，提高推理效率。", "motivation": "标准的预训练范式使得图像编码器在处理图像时忽略了特定的下游任务或文本查询，为了克服这一局限性，研究人员提出了TIE。", "method": "TIE生成的图像表示是基于输入的文本查询条件化的，这使视觉语言模型能够更好地理解和回答与图像内容相关的问题。", "result": "在1B和3B尺度的九个图像到文本的基准测试中，使用了TIE的VLM模型比传统模型分别高出1.5和1.3分。", "conclusion": "TIE不仅可以提高VLM模型的性能，还可以仅使用一半的图像块，在提高推理效率的同时还能泛化到一般的查询上，说明文本条件训练确实能够优化编码器以捕获关键视觉特征。"}}
