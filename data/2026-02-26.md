<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 5]
- [cs.CV](#cs.CV) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Disaster Question Answering with LoRA Efficiency and Accurate End Position](https://arxiv.org/abs/2602.21212)
*Takato Yasuno*

Main category: cs.CL

> 论文介绍了一种基于日本灾害环境的灾害专用问答系统，通过BERT模型和Bi-LSTM结合提升了问答回应的准确率，适用于真实灾害应对环境。

<details>
  <summary>Details</summary>

**Motivation:** 面对自然灾害时，人们常常会因混淆和缺乏相关领域的知识与经验而无法做出合适的回应。尽管灾害信息不断更新，通过RAG搜索和大型语言模型查询也不能保证获得特定情况下相关的领域知识和类似的经历。在这种背景下，论文提出了本研究。

**Method:** 此论文提出了一种基于日本灾害情况和应对经验的灾害专用问答系统，采用cl-tohoku/bert-base-japanese-v3 + Bi-LSTM + 增强位置头架构，并通过LoRA效率优化技术提升了模型的性能。

**Result:** 实验结果显示，利用日本BERT-base优化和Bi-LSTM上下文理解的组合达到了70.4%的末端位置准确性，仅使用了总量参数的5.7%，并且在答案生成的范围F1分数上达到了0.885，这适用于真实的灾害应对场景。

**Conclusion:** 该系统适用于瞬息万变的灾害应对环境，但对于未来挑战，如建立自然灾害Q&A基准数据集，灾害知识精细微调基础模型，开发轻便且能效边缘AI灾害问答应用，以及更新灾害知识库和能力持续学习还有待解决。

**Abstract:** Natural disasters such as earthquakes, torrential rainfall, floods, and volcanic eruptions occur with extremely low frequency and affect limited geographic areas. When individuals face disaster situations, they often experience confusion and lack the domain-specific knowledge and experience necessary to determine appropriate responses and actions. While disaster information is continuously updated, even when utilizing RAG search and large language models for inquiries, obtaining relevant domain knowledge about natural disasters and experiences similar to one's specific situation is not guaranteed. When hallucinations are included in disaster question answering, artificial misinformation may spread and exacerbate confusion. This work introduces a disaster-focused question answering system based on Japanese disaster situations and response experiences. Utilizing the cl-tohoku/bert-base-japanese-v3 + Bi-LSTM + Enhanced Position Heads architecture with LoRA efficiency optimization, we achieved 70.4\% End Position accuracy with only 5.7\% of the total parameters (6.7M/117M). Experimental results demonstrate that the combination of Japanese BERT-base optimization and Bi-LSTM contextual understanding achieves accuracy levels suitable for real disaster response scenarios, attaining a 0.885 Span F1 score. Future challenges include: establishing natural disaster Q\&A benchmark datasets, fine-tuning foundation models with disaster knowledge, developing lightweight and power-efficient edge AI Disaster Q\&A applications for situations with insufficient power and communication during disasters, and addressing disaster knowledge base updates and continual learning capabilities.

</details>


### [2] [Inference-time Alignment via Sparse Junction Steering](https://arxiv.org/abs/2602.21215)
*Runyi Hu,Jie Zhang,Shiqian Zhao,Jiale Meng,Jiwei Li,Jason Zeng,Ming Wu,Michael Heinrich,Yonggang Wen,Tianwei Zhang*

Main category: cs.CL

> Sparse Inference time Alignment (SIA) improves efficiency of token-level steering by intervening only at key decision points, maintaining high quality while reducing computational overhead.

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of substantial computational overhead and potential compromise in generation quality caused by existing dense intervention methods during token-level steering.

**Method:** Sparse Inference time Alignment (SIA) performs sparse junction steering by intervening only at critical decision points where high entropy junctions occur, marking pivotal moments in the generation trajectory.

**Result:** Experiments show that steering only 20% to 80% of tokens using SIA achieves superior balance between alignment and efficiency. For strong base models like Qwen3, intervening on as few as 20% of tokens can match or even surpass heavily post-trained instruct models.

**Conclusion:** Sparse intervention using SIA not only preserves the native distribution of the model better but also integrates seamlessly with search based methods such as Best-of-N and significantly reduces computational costs, making it a more efficient and effective alignment method.

**Abstract:** Token-level steering has emerged as a pivotal approach for inference-time alignment, enabling fine grained control over large language models by modulating their output distributions without parameter updates. While effective, existing methods rely on dense intervention at every decoding step. This persistent manipulation not only incurs substantial computational overhead but also risks compromising generation quality by excessively drifting from the model's intrinsic distribution. In this work, we show that dense intervention is unnecessary and propose Sparse Inference time Alignment (SIA), which performs sparse junction steering by intervening only at critical decision points along the generation trajectory. Our key insight is that high entropy junctions mark pivotal decision points in the generation trajectory and are particularly susceptible to misalignment, indicating the need to introduce alignment related reward signals at these points. Extensive experiments across different model families and alignment objectives show that steering only 20% to 80% of tokens achieves superior alignment-efficiency trade offs. For strong base models such as Qwen3, intervening on as few as 20% of tokens matches or even surpasses heavily post-trained instruct models. This sparsity enables stronger guidance while better preserving the model's native distribution, integrates seamlessly with search based methods such as Best-of-N, and reduces computational cost by up to 6x.

</details>


### [3] [EQ-5D Classification Using Biomedical Entity-Enriched Pre-trained Language Models and Multiple Instance Learning](https://arxiv.org/abs/2602.21216)
*Zhyar Rzgar K Rostam,Gábor Kertész*

Main category: cs.CL

> 研究展示了在从摘要中检测EQ-5D的自动化筛选中，实体强化显著改进了领域适应和模型泛化，准确性大大提高。

<details>
  <summary>Details</summary>

**Motivation:** 系统性文献回顾依赖于正确识别使用EQ-5D的出版物，但手动筛选大量的科学研究文献既耗时又容易出错。

**Method:** 本研究通过微调三种预训练语言模型（通用的BERT和领域特定的SciBERT、BioBERT），并结合通过scispaCy模型提取的生物医学实体信息，以提高从摘要中检测EQ-5D的效果。研究共进行九组实验，包括将三种scispaCy模型与三种PLMs结合，并在句子和研究两个层面评估他们的表现。同时，探索了一种使用注意力融合的多实例学习（MIL）方法，以从句子级信息聚合到研究级预测。

**Result:** 研究表明，在F1分数（达到0.82）和研究层面近乎完美的召回率，显著超越了经典的词袋模型和最近报道的PLM模型。

**Conclusion:** 结合生物医学实体的预训练语言模型的微调大大提高了自动化筛选在系统性回顾中的准确度。

**Abstract:** The EQ-5D (EuroQol 5-Dimensions) is a standardized instrument for the evaluation of health-related quality of life. In health economics, systematic literature reviews (SLRs) depend on the correct identification of publications that use the EQ-5D, but manual screening of large volumes of scientific literature is time-consuming, error-prone, and inconsistent. In this study, we investigate fine-tuning of general-purpose (BERT) and domain-specific (SciBERT, BioBERT) pre-trained language models (PLMs), enriched with biomedical entity information extracted through scispaCy models for each statement, to improve EQ-5D detection from abstracts. We conduct nine experimental setups, including combining three scispaCy models with three PLMs, and evaluate their performance at both the sentence and study levels. Furthermore, we explore a Multiple Instance Learning (MIL) approach with attention pooling to aggregate sentence-level information into study-level predictions, where each abstract is represented as a bag of enriched sentences (by scispaCy). The findings indicate consistent improvements in F1-scores (reaching 0.82) and nearly perfect recall at the study-level, significantly exceeding classical bag-of-words baselines and recently reported PLM baselines. These results show that entity enrichment significantly improves domain adaptation and model generalization, enabling more accurate automated screening in systematic reviews.

</details>


### [4] [Applied Sociolinguistic AI for Community Development (ASA-CD): A New Scientific Paradigm for Linguistically-Grounded Social Intervention](https://arxiv.org/abs/2602.21217)
*S M Ruhul Alam,Rifa Ferzana*

Main category: cs.CL

> 本文提出了应用于社区发展的社会语言学AI（ASA-CD）作为解决社区挑战的新科学范式，通过基于语言的AI干预来实现。ASA-CD引入了三个关键贡献：1）作为计算指标的语言生物标志物，用于衡量话语碎片化；2）发展导向的自然语言处理（NLP），一种优先考虑集体成果的AI优化范式；以及3）一种标准化的五阶段协议用于话语干预。一个基于实证和合成语料的研究证明了排他性语言与负面情绪之间的系统性联系，并模拟了通过干预带来的改善。ASA-CD为提供可扩展的、价值观对齐的AI服务社区赋权提供了统一的方法论、伦理和实证框架。

<details>
  <summary>Details</summary>

**Motivation:** 解决社区内部及社区间的挑战，准确识别和改善语言使用中的消极模式，推动社区共享目标和价值的实现。

**Method:** 结合发展导向的自然语言处理技术，采用标准化的五阶段协议进行话语干预，通过真实世界和合成语料数据进行研究。

**Result:** 研究表明排他性语言与负面情绪之间存在显著联系，干预措施成功提高了社区内的积极交流和参与。

**Conclusion:** ASA-CD作为方法论、伦理和实证框架的集成，为可扩展、与价值观对齐的AI服务提供了思路，有利于促进社区间的相互理解和共享目标的实现。

**Abstract:** This paper establishes Applied Sociolinguistic AI for Community Development (ASA-CD) as a novel scientific paradigm for addressing community challenges through linguistically grounded, AI-enabled intervention. ASA-CD introduces three key contributions: (1) linguistic biomarkers as computational indicators of discursive fragmentation; (2) development-aligned natural language processing (NLP), an AI optimisation paradigm prioritising collective outcomes; and (3) a standardised five-phase protocol for discursive intervention. A proof-of-concept study, incorporating real-world and synthetic corpora, demonstrates systematic associations between exclusionary language and negative sentiment and simulates intervention-based improvements. ASA-CD provides a unified methodological, ethical and empirical framework for scalable, value-aligned AI in the service of community empowerment.

</details>


### [5] [EPSVec: Efficient and Private Synthetic Data Generation via Dataset Vectors](https://arxiv.org/abs/2602.21218)
*Amin Banayeeanzade,Qingchuan Yang,Deqing Fu,Spencer Hong,Erin Babinsky,Alfy Samuel,Anoop Kumar,Robin Jia,Sai Praneeth Karimireddy*

Main category: cs.CL

> 本文提出了EPSVec，一种结合差分隐私的轻量级合成数据生成技术，通过提取和净化转向向量，实现高效、多样且保真的文本生成，尤其适用于小规模私有数据集。

<details>
  <summary>Details</summary>

**Motivation:** 当前的私有文本生成方法效率低下，数据密集、计算缓慢，且通常需要大量私有语料库或较大的批次大小才能获得可使用的质量。而高质量的数据对于现代机器学习至关重要，许多有价值的数据集由于敏感性无法自由分享。因此，提出了EPSVec这一更轻量级且具有差分隐私性的方案。

**Method:** 通过使用数据集向量，即激活空间中的方向来捕捉私有数据与公共先验分布之间的差距，EPSVec引导大型语言模型生成数据。EPSVec只提取和净化一次转向向量，然后执行标准解码，从而将隐私预算与生成分离，允许在没有额外隐私成本的情况下生成任意多的合成样本。此外，通过利用预训练模型和引入固定次数提示来增加生成的多样性和保真度。

**Result:** 实验表明EPSVec在分布一致性和下游效用方面优于现有基线，尤其是在低数据量的情况下，同时显着减少了计算开销。

**Conclusion:** EPSVec作为差分隐私的轻量级替代方案，在不增加隐私成本的情况下能生成大量高质量的合成样本，并且在低数据情景下表现尤为出色。它通过抽取和净化一次转向向量，实现解码的标准操作，并利用预训练模型和固定次数提示增强了生成的多样性和保真度。

**Abstract:** High-quality data is essential for modern machine learning, yet many valuable corpora are sensitive and cannot be freely shared. Synthetic data offers a practical substitute for downstream development, and large language models (LLMs) have emerged as powerful engines for generating it. However, existing private text generation methods are severely inefficient: they are data-intensive, computationally slow, and often require large private corpora or batch sizes to achieve usable quality. We introduce EPSVec, a differentially-private lightweight alternative that steers LLM generation using *dataset vectors*--directions in activation space that capture the distributional gap between private data and public priors. EPSVec extracts and sanitizes steering vectors just once and then performs standard decoding. This decouples the privacy budget from generation, enabling arbitrarily many synthetic samples without additional privacy cost and yielding strong fidelity even in low-data regimes. Furthermore, we enhance our method by utilizing pretrained (base) models and introducing fixed-shot prompting to boost generation diversity and fidelity. Our experiments demonstrate that EPSVec outperforms existing baselines in distributional alignment and downstream utility, particularly in low-data regimes, while significantly reducing computational overhead.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [6] [StoryTailor:A Zero-Shot Pipeline for Action-Rich Multi-Subject Visual Narratives](https://arxiv.org/abs/2602.21273)
*Jinghao Hu,Yuhe Zhang,GuoHua Geng,Kang Li,Han Zhang*

Main category: cs.CV

> StoryTailor is a zero-shot system that generates accurate, identity-preserving, and temporally coherent visual narratives, using three interrelated components: GCA, AB-SVR, and SFC.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind the research is to address the challenge of generating multi-frame, action-rich visual narratives without fine-tuning, specifically focusing on balancing action text faithfulness, subject identity fidelity, and cross-frame background continuity simultaneously.

**Method:** Three synergistic modules drive the system: Gaussian-Centered Attention (GCA) to dynamically focus on each subject core and ease grounding-box overlaps; Action-Boost Singular Value Reweighting (AB-SVR) to amplify action-related directions in the text embedding space; and Selective Forgetting Cache (SFC) that retains transferable background cues, forgets nonessential history, and selectively surfaces retained cues to build cross-scene semantic ties.

**Result:** Compared with baseline methods, experiments show a significant improvement of up to 10-15% in performance, with CLIP-T, and while DreamSim doesn't surpass strong baselines, CLIP-I maintains a visually acceptable standard. The system also has a faster inference speed than FluxKontext on a 24 GB GPU.

**Conclusion:** StoryTailor provides a solution for generating expressive and stable visual narratives through a zero-shot approach, which is both more efficient and of higher quality compared to baseline methods, despite challenges in achieving the full extent of DreamSim's performance.

**Abstract:** Generating multi-frame, action-rich visual narratives without fine-tuning faces a threefold tension: action text faithfulness, subject identity fidelity, and cross-frame background continuity. We propose StoryTailor, a zero-shot pipeline that runs on a single RTX 4090 (24 GB) and produces temporally coherent, identity-preserving image sequences from a long narrative prompt, per-subject references, and grounding boxes. Three synergistic modules drive the system: Gaussian-Centered Attention (GCA) to dynamically focus on each subject core and ease grounding-box overlaps; Action-Boost Singular Value Reweighting (AB-SVR) to amplify action-related directions in the text embedding space; and Selective Forgetting Cache (SFC) that retains transferable background cues, forgets nonessential history, and selectively surfaces retained cues to build cross-scene semantic ties. Compared with baseline methods, experiments show that CLIP-T improves by up to 10-15%, with DreamSim lower than strong baselines, while CLIP-I stays in a visually acceptable, competitive range. With matched resolution and steps on a 24 GB GPU, inference is faster than FluxKontext. Qualitatively, StoryTailor delivers expressive interactions and evolving yet stable scenes.

</details>


### [7] [HorizonForge: Driving Scene Editing with Any Trajectories and Any Vehicles](https://arxiv.org/abs/2602.21333)
*Yifan Wang,Francesco Pittaluga,Zaid Tasneem,Chenyu You,Manmohan Chandraker,Ziyu Jiang*

Main category: cs.CV

> HorizonForge框架用于重建驾驶场景，在保持空间和时间一致性的同时，实现精确控制和高真实感，显著提高用户偏好。

<details>
  <summary>Details</summary>

**Motivation:** 可控驾驶场景生成对于现实和可扩展的自动驾驶仿真至关重要，但现有方法在同时实现照片般真实感和精确控制方面有困难。

**Method:** HorizonForge框架通过重建场景为可编辑的高斯图斑和网格，实现精细的三维操纵和语言驱动的车辆插入。编辑通过噪声感知的视频扩散过程进行渲染，以确保空间和时间一致性，从而在单次前向传递中生成多样化的场景变化，无需逐轨迹优化。

**Result:** 实验表明，Gaussian-Mesh表示比其他3D表示具有更高的保真度，来自视频扩散的时间先验对于一致合成至关重要。HorizonForge实现了83.4%的用户偏好提升和25.19%的FID改善。

**Conclusion:** HorizonForge为实现具有高真实感的、可控的驾驶仿真提供了一个简单的、强大的范式。

**Abstract:** Controllable driving scene generation is critical for realistic and scalable autonomous driving simulation, yet existing approaches struggle to jointly achieve photorealism and precise control. We introduce HorizonForge, a unified framework that reconstructs scenes as editable Gaussian Splats and Meshes, enabling fine-grained 3D manipulation and language-driven vehicle insertion. Edits are rendered through a noise-aware video diffusion process that enforces spatial and temporal consistency, producing diverse scene variations in a single feed-forward pass without per-trajectory optimization. To standardize evaluation, we further propose HorizonSuite, a comprehensive benchmark spanning ego- and agent-level editing tasks such as trajectory modifications and object manipulation. Extensive experiments show that Gaussian-Mesh representation delivers substantially higher fidelity than alternative 3D representations, and that temporal priors from video diffusion are essential for coherent synthesis. Combining these findings, HorizonForge establishes a simple yet powerful paradigm for photorealistic, controllable driving simulation, achieving an 83.4% user-preference gain and a 25.19% FID improvement over the second best state-of-the-art method. Project page: https://horizonforge.github.io/ .

</details>


### [8] [Scaling View Synthesis Transformers](https://arxiv.org/abs/2602.21341)
*Evan Kim,Hyunwoo Ryu,Thomas W. Mitchel,Vincent Sitzmann*

Main category: cs.CV

> 论文探讨了无几何视图合成变换器的计算资源使用情况，并发现改进的编码器-解码器架构在不同计算水平上都能实现高效扩展，超越了解码器唯一模型，并在真实世界的新型视图合成基准测试中表现出更优性能。

<details>
  <summary>Details</summary>

**Motivation:** 研究无几何视图合成变换器的计算资源使用规律，并提供设计原则，以训练计算优化的新型视图合成模型。

**Method:** 进行无几何视图合成变换器的扩展规律的系统性研究，提出Scalable View Synthesis Model (SVSM)模型。

**Result:** 发现编码器-解码器架构可以是计算优化的，并在不同计算水平上都能有效扩展，超越了解码器唯一模型，且在减少训练计算资源的情况下，在真实世界基准测试中表现更优。

**Conclusion:** 编码器-解码器架构在无几何视图合成中优于解码器唯一模型，为设计计算优化的新型视图合成模型提供了新路径。

**Abstract:** Geometry-free view synthesis transformers have recently achieved state-of-the-art performance in Novel View Synthesis (NVS), outperforming traditional approaches that rely on explicit geometry modeling. Yet the factors governing their scaling with compute remain unclear. We present a systematic study of scaling laws for view synthesis transformers and derive design principles for training compute-optimal NVS models. Contrary to prior findings, we show that encoder-decoder architectures can be compute-optimal; we trace earlier negative results to suboptimal architectural choices and comparisons across unequal training compute budgets. Across several compute levels, we demonstrate that our encoder-decoder architecture, which we call the Scalable View Synthesis Model (SVSM), scales as effectively as decoder-only models, achieves a superior performance-compute Pareto frontier, and surpasses the previous state-of-the-art on real-world NVS benchmarks with substantially reduced training compute.

</details>


### [9] [Towards Controllable Video Synthesis of Routine and Rare OR Events](https://arxiv.org/abs/2602.21365)
*Dominik Schneider,Lalithkumar Seenivasan,Sampath Rapuri,Vishalroshan Anil,Aiza Maksutova,Yiqing Shen,Jan Emily Mangulabnan,Hao Ding,Jose L. Porras,Masaru Ishii,Mathias Unberath*

Main category: cs.CV

> 本文提出一种手术室视频扩散框架，用于合成罕见和关键安全事件的视频，以支持AI模型在检测手术并发症方面的训练与验证，展示了其合成视频的优越性能和潜在应用价值。

<details>
  <summary>Details</summary>

**Motivation:** 为了克服大规模手术室工作流程数据的瓶颈，特别是涉及罕见、关键安全或非典型事件的数据，本文旨在开发合成这些事件的技术，进而支持智能环境技术的进步。

**Method:** 该框架结合了几何抽象模块、条件模块以及精细调整的扩散模型，先将手术室场景转化为抽象的几何表示，然后进行合成，最后生成理想的手术事件视频。

**Result:** 该方法在合成常规手术事件方面优于现有的视频扩散基础模型，在不同领域的数据集上具有更低的FVD/LPIPS和更高的SSIM/PSNR。此外，通过定性结果展示了其可以生成反事实事件视频的能力，生成的合成数据训练的AI模型检测到70.13%的近关键安全事件。

**Conclusion:** 提出的解决方案能够从抽象几何表示中合成常规和罕见的手术室事件。它不仅能够产生罕见和关键安全事件场景，还为支持智能环境模型的开发展示了潜力。

**Abstract:** Purpose: Curating large-scale datasets of operating room (OR) workflow, encompassing rare, safety-critical, or atypical events, remains operationally and ethically challenging. This data bottleneck complicates the development of ambient intelligence for detecting, understanding, and mitigating rare or safety-critical events in the OR.
  Methods: This work presents an OR video diffusion framework that enables controlled synthesis of rare and safety-critical events. The framework integrates a geometric abstraction module, a conditioning module, and a fine-tuned diffusion model to first transform OR scenes into abstract geometric representations, then condition the synthesis process, and finally generate realistic OR event videos. Using this framework, we also curate a synthetic dataset to train and validate AI models for detecting near-misses of sterile-field violations.
  Results: In synthesizing routine OR events, our method outperforms off-the-shelf video diffusion baselines, achieving lower FVD/LPIPS and higher SSIM/PSNR in both in- and out-of-domain datasets. Through qualitative results, we illustrate its ability for controlled video synthesis of counterfactual events. An AI model trained and validated on the generated synthetic data achieved a RECALL of 70.13% in detecting near safety-critical events. Finally, we conduct an ablation study to quantify performance gains from key design choices.
  Conclusion: Our solution enables controlled synthesis of routine and rare OR events from abstract geometric representations. Beyond demonstrating its capability to generate rare and safety-critical scenarios, we show its potential to support the development of ambient intelligence models.

</details>


### [10] [Momentum Memory for Knowledge Distillation in Computational Pathology](https://arxiv.org/abs/2602.21395)
*Yongxin Guo,Hao Lu,Onur C. Koyun,Zhengjie Zhu,Muhammet Fatih Demir,Metin Nafi Gurcan*

Main category: cs.CV

> 本文提出了一种新的跨模态知识蒸馏方法（MoMKD），通过增加监督信息的上下文来改善组织学模型的性能，使多模态模型能够在仅使用组织学数据时进行准确的预测。

<details>
  <summary>Details</summary>

**Motivation:** 多模态学习整合基因组学和组织学在癌症诊断中展现出了强大的潜力，但其临床转换因配对的组织基因组数据有限而受阻。现有的知识蒸馏方法依赖于批内对齐，受限于批内比较有限，从而引入不稳定性，最终影响性能。

**Method:** 提出了一种基于动量更新记忆的跨模态知识蒸馏框架（MoMKD），该框架在多个批次中聚合基因组学和组织学信息，扩大每个小批次的监督上下文。此外，分离基因组学和组织学分支的梯度，防止基因组信号在训练中主导组织学特征学习，并消除推理时的模式差异问题。

**Result:** 基于TCGA-BRCA基准（HER2、PR和ODX分类任务）和一个独立的内部测试数据集的广泛实验表明，MoMKD在仅使用组织学推理的情况下，连续超越最先进的MIL和多模态KD基线，在性能和泛化能力上表现优异。

**Conclusion:** MoMKD为计算病理学建立了一个稳健和通用的知识蒸馏范式，展示了对癌症诊断的支持潜力。

**Abstract:** Multimodal learning that integrates genomics and histopathology has shown strong potential in cancer diagnosis, yet its clinical translation is hindered by the limited availability of paired histology-genomics data. Knowledge distillation (KD) offers a practical solution by transferring genomic supervision into histopathology models, enabling accurate inference using histology alone. However, existing KD methods rely on batch-local alignment, which introduces instability due to limited within-batch comparisons and ultimately degrades performance.
  To address these limitations, we propose Momentum Memory Knowledge Distillation (MoMKD), a cross-modal distillation framework driven by a momentum-updated memory. This memory aggregates genomic and histopathology information across batches, effectively enlarging the supervisory context available to each mini-batch. Furthermore, we decouple the gradients of the genomics and histology branches, preventing genomic signals from dominating histology feature learning during training and eliminating the modality-gap issue at inference time.
  Extensive experiments on the TCGA-BRCA benchmark (HER2, PR, and ODX classification tasks) and an independent in-house testing dataset demonstrate that MoMKD consistently outperforms state-of-the-art MIL and multimodal KD baselines, delivering strong performance and generalization under histology-only inference. Overall, MoMKD establishes a robust and generalizable knowledge distillation paradigm for computational pathology.

</details>


### [11] [MMLoP: Multi-Modal Low-Rank Prompting for Efficient Vision-Language Adaptation](https://arxiv.org/abs/2602.21397)
*Sajjad Ghiasvand,Haniyeh Ehsani Oskouie,Mahnoosh Alizadeh,Ramtin Pedarsani*

Main category: cs.CV

> MMLoP提出了一种多模态低秩提示框架，能够在大幅减少可训练参数量的同时保持高准确性。

<details>
  <summary>Details</summary>

**Motivation:** 为了解决现有prompt learning方法在多层transformer中参数效率低下的问题，同时保持模型的准确性。

**Method:** MMLoP采用多模态低秩提示框架，通过在每个transformer层对视觉和文本提示进行低秩分解来大幅减少可训练参数数量，同时引入一致性损失、均匀偏移校正和共享上投影组成三个互补组件来提高准确性。

**Result:** 在三个基准测试和11个多样化数据集上实验表明，MMLoP取得了79.70%的基线到新颖泛化谐波均值，且在准确性和效率之间取得了良好的平衡。

**Conclusion:** MMLoP实现了参数效率的提升，同时保持了较高的准确性，相比于现有方法具有显著优势。

**Abstract:** Prompt learning has become a dominant paradigm for adapting vision-language models (VLMs) such as CLIP to downstream tasks without modifying pretrained weights. While extending prompts to both vision and text encoders across multiple transformer layers significantly boosts performance, it dramatically increases the number of trainable parameters, with state-of-the-art methods requiring millions of parameters and abandoning the parameter efficiency that makes prompt tuning attractive. In this work, we propose \textbf{MMLoP} (\textbf{M}ulti-\textbf{M}odal \textbf{Lo}w-Rank \textbf{P}rompting), a framework that achieves deep multi-modal prompting with only \textbf{11.5K trainable parameters}, comparable to early text-only methods like CoOp. MMLoP parameterizes vision and text prompts at each transformer layer through a low-rank factorization, which serves as an implicit regularizer against overfitting on few-shot training data. To further close the accuracy gap with state-of-the-art methods, we introduce three complementary components: a self-regulating consistency loss that anchors prompted representations to frozen zero-shot CLIP features at both the feature and logit levels, a uniform drift correction that removes the global embedding shift induced by prompt tuning to preserve class-discriminative structure, and a shared up-projection that couples vision and text prompts through a common low-rank factor to enforce cross-modal alignment. Extensive experiments across three benchmarks and 11 diverse datasets demonstrate that MMLoP achieves a highly favorable accuracy-efficiency tradeoff, outperforming the majority of existing methods including those with orders of magnitude more parameters, while achieving a harmonic mean of 79.70\% on base-to-novel generalization.

</details>


### [12] [FlowFixer: Towards Detail-Preserving Subject-Driven Generation](https://arxiv.org/abs/2602.21402)
*Jinyoung Jun,Won-Dong Jang,Wenbin Ouyang,Raghudeep Gadde,Jungbeom Lee*

Main category: cs.CV

> 本文介绍了一种名为FlowFixer的框架，用于恢复在主体驱动生成过程中丢失的细节，通过直接的图像到图像转换和一种单步去噪方案来克服规模和视角变化带来的问题，并引入了新的评估指标来提升评估精度，实验结果证明其在图像生成质量上优于现有技术。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在解决SDG过程中因规模和视角变化而导致的细节丢失问题，提出了一种新的方法来改善图像生成的质量，并能够避免由于语言提示带来的模糊性。

**Method:** FlowFixer提出了一种直接的图像到图像的转换方法，该方法从视觉参考中进行学习，以恢复在生成过程中由于主体的比例和视角变化而丢失的细节。为了实现图像到图像的训练，作者引入了一种单步去噪方案以生成自监督训练数据，从而自动移除高频细节同时保留全局结构，模拟实际的SDG误差。此外，还提出了一种基于关键点匹配的度量标准来评估细节的保真度，这超越了通常用CLIP或DINO衡量的语义相似性。

**Result:** 实验结果显示，FlowFixer在定性和定量评估中都优于现有的SDG方法，为高质量的主体驱动生成设定了新的基准。

**Conclusion:** FlowFixer作为一种新颖的SDG方法，通过直接图像到图像的翻译和针对性的评估指标，实现了细节的高质量恢复，并在多个评估标准上超越了现有的方法，证明了其在提高生成图像质量方面的有效性。

**Abstract:** We present FlowFixer, a refinement framework for subject-driven generation (SDG) that restores fine details lost during generation caused by changes in scale and perspective of a subject. FlowFixer proposes direct image-to-image translation from visual references, avoiding ambiguities in language prompts. To enable image-to-image training, we introduce a one-step denoising scheme to generate self-supervised training data, which automatically removes high-frequency details while preserving global structure, effectively simulating real-world SDG errors. We further propose a keypoint matching-based metric to properly assess fidelity in details beyond semantic similarities usually measured by CLIP or DINO. Experimental results demonstrate that FlowFixer outperforms state-of-the-art SDG methods in both qualitative and quantitative evaluations, setting a new benchmark for high-fidelity subject-driven generation.

</details>


### [13] [Exploring Vision-Language Models for Open-Vocabulary Zero-Shot Action Segmentation](https://arxiv.org/abs/2602.21406)
*Asim Unmesh,Kaki Ramesh,Mayank Patel,Rahul Jain,Karthik Ramani*

Main category: cs.CV

> 本文探索了开放词汇零样本时间动作分割（OVTAS）问题，基于视觉语言模型的零样本能力，提出了一种无需训练的流程，并对多种视觉语言模型进行了广泛的分析。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法仅限于封闭词汇表和固定标签集，难以应对活动空间广阔和标注方式多样所带来的视频数据收集问题。

**Method:** 通过将视频帧与候选动作标签进行匹配（FAES）并运用相似度矩阵时间分割（SMTS）来保持时间一致性，本文提出了一个无需训练的OVTAS流程。

**Result:** 实验结果显示，OVTAS在标准基准测试中取得了强大的结果，证明了视觉语言模型在结构性时间理解方面的潜力。

**Conclusion:** 研究证明了现有的视觉语言模型在开放词汇动作分割上的应用潜力，并强调了无需任务特定监督的OVTAS方法的有效性。

**Abstract:** Temporal Action Segmentation (TAS) requires dividing videos into action segments, yet the vast space of activities and alternative breakdowns makes collecting comprehensive datasets infeasible. Existing methods remain limited to closed vocabularies and fixed label sets. In this work, we explore the largely unexplored problem of Open-Vocabulary Zero-Shot Temporal Action Segmentation (OVTAS) by leveraging the strong zero-shot capabilities of Vision-Language Models (VLMs). We introduce a training-free pipeline that follows a segmentation-by-classification design: Frame-Action Embedding Similarity (FAES) matches video frames to candidate action labels, and Similarity-Matrix Temporal Segmentation (SMTS) enforces temporal consistency. Beyond proposing OVTAS, we present a systematic study across 14 diverse VLMs, providing the first broad analysis of their suitability for open-vocabulary action segmentation. Experiments on standard benchmarks show that OVTAS achieves strong results without task-specific supervision, underscoring the potential of VLMs for structured temporal understanding.

</details>


### [14] [WildSVG: Towards Reliable SVG Generation Under Real-Word Conditions](https://arxiv.org/abs/2602.21416)
*Marco Terral,Haotian Zhang,Tianyang Zhang,Meng Lin,Xiaoqing Xie,Haoran Dai,Darsh Kaushik,Pai Peng,Nicklas Scharpff,David Vazquez,Joan Rodriguez*

Main category: cs.CV

> 介绍了一个名为SVG提取的任务，其目的是将图像中的视觉输入转换为SVG，以及为这一任务创建的第一个基准测试WildSVG。基准测试结果显示，目前的模型在实际场景中的表现仍需改进。

<details>
  <summary>Details</summary>

**Motivation:** 现有的多模态模型在从干净渲染图或文本描述中生成SVG时表现出色，但在处理实际场景中的图像时却表现不佳，因为自然图像中包含了噪音、杂乱和领域偏差。这就需要建立一个合适的基准来解决这个问题。

**Method:** 我们介绍了一个名为SVG提取的任务，目的是将图像中的特定视觉输入转换为可缩放矢量图形。为了应对现有模型在处理实际场景中的噪音、杂乱情况时的表现不佳，我们创建了WildSVG基准，该基准包含两个互补的数据集：Natural WildSVG和Synthetic WildSVG。Natural WildSVG是通过真实图片中的公司logo及其SVG注释构建的，而Synthetic WildSVG则将复杂的SVG渲染融入到实际场景中，以模拟困难的情况。

**Result:** 我们对现有最先进的多模态模型进行了基准测试，发现当前方法的表现远低于实际场景中可靠SVG提取所需的水平。然而，迭代改进方法显示出未来发展的重要方向，并且模型能力正在稳步提升。

**Conclusion:** 通过WildSVG基准，我们为系统性地测评SVG提取建立了基础。尽管当前的模型在现实世界应用中还有很大的提升空间，但我们相信通过进一步的研究和改进，可以实现更加可靠的SVG提取技术。

**Abstract:** We introduce the task of SVG extraction, which consists in translating specific visual inputs from an image into scalable vector graphics. Existing multimodal models achieve strong results when generating SVGs from clean renderings or textual descriptions, but they fall short in real-world scenarios where natural images introduce noise, clutter, and domain shifts. A central challenge in this direction is the lack of suitable benchmarks. To address this need, we introduce the WildSVG Benchmark, formed by two complementary datasets: Natural WildSVG, built from real images containing company logos paired with their SVG annotations, and Synthetic WildSVG, which blends complex SVG renderings into real scenes to simulate difficult conditions. Together, these resources provide the first foundation for systematic benchmarking SVG extraction. We benchmark state-of-the-art multimodal models and find that current approaches perform well below what is needed for reliable SVG extraction in real scenarios. Nonetheless, iterative refinement methods point to a promising path forward, and model capabilities are steadily improving

</details>
