{"id": "2511.10650", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.10650", "abs": "https://arxiv.org/abs/2511.10650", "authors": ["Felix George", "Harshit Kumar", "Divya Pathak", "Kaustabha Ray", "Mudit Verma", "Pratibha Moogi"], "title": "Unsupervised Cycle Detection in Agentic Applications", "comment": null, "summary": "Agentic applications powered by Large Language Models exhibit non-deterministic behaviors that can form hidden execution cycles, silently consuming resources without triggering explicit errors. Traditional observability platforms fail to detect these costly inefficiencies. We present an unsupervised cycle detection framework that combines structural and semantic analysis. Our approach first applies computationally efficient temporal call stack analysis to identify explicit loops and then leverages semantic similarity analysis to uncover subtle cycles characterized by redundant content generation. Evaluated on 1575 trajectories from a LangGraph-based stock market application, our hybrid approach achieves an F1 score of 0.72 (precision: 0.62, recall: 0.86), significantly outperforming individual structural (F1: 0.08) and semantic methods (F1: 0.28). While these results are encouraging, there remains substantial scope for improvement, and future work is needed to refine the approach and address its current limitations.", "AI": {"tldr": "A hybrid framework combining structural and semantic analysis effectively detects hidden execution cycles in agentic applications powered by Large Language Models.", "motivation": "Large Language Models can exhibit non-deterministic behaviors that lead to hidden execution cycles. Traditional observability tools are not capable of detecting these inefficiencies, justifying the need for a new cycle detection framework.", "method": "Our approach combines structural and semantic analysis to detect cycles in agentic applications. It starts with a computationally efficient temporal call stack analysis to identify explicit loops and then uses semantic similarity to uncover subtle cycles.", "result": "The method was evaluated on 1575 trajectories from a LangGraph-based stock market application. The hybrid approach achieved an F1 score of 0.72 (precision: 0.62, recall: 0.86), outperforming the structural F1 score of 0.08 and the semantic F1 score of 0.28.", "conclusion": "The hybrid approach, combining structural and semantic analysis, shows promising results in detecting hidden execution cycles. However, there is room for improvement, and future research is necessary to refine the method."}}
{"id": "2511.10651", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10651", "abs": "https://arxiv.org/abs/2511.10651", "authors": ["Shansi Zhang", "Min Li"], "title": "Data Analysis and Performance Evaluation of Simulation Deduction Based on LLMs", "comment": null, "summary": "Data analysis and performance evaluation of simulation deduction plays a pivotal role in modern warfare, which enables military personnel to gain invaluable insights into the potential effectiveness of different strategies, tactics, and operational plans. Traditional manual analysis approach is time-consuming and limited by human errors. To enhance efficiency and accuracy, large language models (LLMs) with strong analytical and inferencing capabilities can be employed. However, high-quality analysis reports with well-structured formatting cannot be obtained through a single instruction input to the LLM. To tackle this issue, we propose a method that first decomposes the complex task into several sub-tasks and designs effective system prompts and user prompts for each sub-task. Multi-round interactions with the LLM incorporating self-check and reflection are then conducted to enable structured data extraction as well as multi-step analysis and evaluation. Furthermore, custom tools are defined and invoked to generate figures and compute metrics. We also design multiple report templates, each tailored to a specific application and input data type, ensuring their adaptability across a variety of scenarios. Extensive evaluation results demonstrate that the reports generated by our method exhibit higher quality, therefore obtaining higher scores than the baseline method.", "AI": {"tldr": "该论文提出了一种利用大语言模型进行复杂任务的结构化数据提取及多步骤分析的方法，以生成高质量的数据分析报告来评估仿真推演的表现，从而帮助军事人员制定更有效的策略。", "motivation": "传统的手动分析方法耗时且容易出错，因此提出使用大语言模型来增强分析的效率和准确性。", "method": "Structure", "result": "{\n  \"tldr\": \"该论文提出了一种利用大语言模型进行复杂任务的结构化数据提取及多步骤分析的方法，以生成高质量的数据分析报告来评估仿真推演的表现，从而帮助军事人员制定更有效的策略。\",\n  \"motivation\": \"传统的手动分析方法耗时且容易出错，因此提出使用大语言模型来增强分析的效率和准确性。\",\n  \"method\": \"首先将复杂任务分解为多个子任务，设计有效的系统提示和用户提示，进行多轮交互来实现结构化数据提取和多步骤分析，同时定义了定制工具生成图表和计算指标。\",\n  \"result\": \"通过实验结果证明，新方法生成的报告质量更高，比基准方法获得了更高的评分。\",\n  \"conclusion\": \"这种方法能够有效地提升数据和性能评估的效率和准确性，适用于广泛的军事策略制定场景。\"]", "conclusion": "这种方法能够有效地提升数据和性能评估的效率和准确性，适用于广泛的军事策略制定场景。"}}
{"id": "2511.10652", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10652", "abs": "https://arxiv.org/abs/2511.10652", "authors": ["Rafael Arias Gonzalez", "Steve DiPaola"], "title": "Cognitively-Inspired Episodic Memory Architectures for Accurate and Efficient Character AI", "comment": "25 pages", "summary": "Large language models show promise for embodying historical characters in dialogue systems, but existing approaches face a critical trade-off: simple retrieval-augmented generation produces shallow responses, while multi-stage reflection achieves depth at prohibitive latency. We present an architecture that resolves this tension through offline data augmentation and efficient parallel retrieval from structured episodic memory. Our system transforms biographical data into 1,774 enriched first-person memories with affective-semantic metadata, then employs two-stage retrieval achieving 0.52s prompt generation. Evaluation using LLM-as-judge and RAGAs metrics shows our approach achieves parity with traditional RAG on GPT-4 while significantly outperforming it on smaller models (GPT-3.5, GPT-3), suggesting particular value for resource-constrained deployments. Beyond dialogue, the structured memory enables novel visualization tools: spatiotemporal heatmaps, emotional trajectory analysis, and interactive path tracking, positioning the system as both a dialogue interface and research tool for biographical analysis. We use Van Gogh as a test case, but the architecture is generalizable to any historical figure with substantial textual records, offering a practical framework for educational, museum, and research applications requiring both accuracy and efficiency", "AI": {"tldr": "本文提出了一种架构，通过离线数据增强和从结构化情景记忆中高效并行检索解决了简单检索增强生成和多阶段反思之间的平衡问题，适用于资源受限的部署。", "motivation": "解决大型语言模型在模拟历史人物对话系统中的简单检索增强生成产生浅层响应，而多阶段反思达到深度时有禁用延迟的问题。", "method": "通过离线数据增强和从结构化情景记忆中高效并行检索，解决了简单检索增强生成和多阶段反思之间的平衡问题。系统将传记数据转化为1,774个情感-语义元数据增强的第一人称记忆，然后采用两阶段检索实现0.52秒的提示生成。", "result": "评估使用LLM-as-judge和RAGAs指标显示，与传统的RAG相比，在较小模型上（GPT-3.5和GPT-3）表现显著更好，性能达到GPT-4的水平，为资源受限的应用提供了实际框架。", "conclusion": "除了对话，该结构化的记忆还提供了创新的可视化工具，如时空热图、情绪轨迹分析和交互路径跟踪，使得该系统既可以作为对话界面又可以为传记分析的研究工具。架构适用于任何具有大量文本记录的历史人物，提供了既准确又高效的实践框架。"}}
{"id": "2511.10653", "categories": ["cs.CL", "cs.AI", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.10653", "abs": "https://arxiv.org/abs/2511.10653", "authors": ["Desheng Kong", "Xiangshuo Cui", "Jiaying Jin", "Jing Xu", "Donglin Wang"], "title": "Hybrid Quantum Transformer for Language Generation", "comment": null, "summary": "Although quantum computing has been increasingly applied to replace classical computation, most existing quantum or hybrid models remain confined to simple tasks, with no successful application to large-scale natural language generation to date. In this work, we present the first hybrid quantum-classical large language model (LLM) for natural language generation, HyQuT, capable of performing coherent and context-aware dialogue. The proposed architecture integrates variational quantum circuits (VQCs) into the Transformer framework at both 8M and 150M parameter scales. Experimental results show that a minimal number of qubits (10 qubits with 80 quantum gates) can replace about 10% of the classical parameters in the 150M-parameter model, while achieving comparable convergence stability and generation quality. This study provides an early demonstration of the feasibility of integrating quantum computing to large-scale generative language models.", "AI": {"tldr": "本文提出了一个基于Transformer架构的混合量子-经典大型语言模型HyQuT，用于自然语言生成，并通过实验表明，少量量子位可以替代经典参数，实现与经典模型相当的生成质量和收敛稳定性。", "motivation": "尽管量子计算已被越来越多地用于替代经典计算，但大多数现有的量子或混合模型仍局限于简单的任务。本研究旨在探究将量子计算集成到大规模生成语言模型中的可行性。", "method": "本文提出了一个名为HyQuT的混合量子-经典大规模语言模型，用于自然语言生成。该架构在Transformer框架中集成了变分量子电路（VQCs），并在8M和150M参数规模下进行了实验。", "result": "实验结果表明，使用最小数量的量子位（10个量子位，80个量子门），可以取代150M参数模型中大约10%的经典参数，同时实现了相当的收敛稳定性和生成质量。", "conclusion": "这项研究是将量子计算集成到大规模生成语言模型中的早期成功尝试，展示了其可行性和潜力。"}}
{"id": "2511.10668", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10668", "abs": "https://arxiv.org/abs/2511.10668", "authors": ["Akbar Anbar Jafari", "Cagri Ozcinar", "Gholamreza Anbarjafari"], "title": "A Mathematical Framework for AI Singularity: Conditions, Bounds, and Control of Recursive Improvement", "comment": "41 pages", "summary": "AI systems improve by drawing on more compute, data, energy, and better training methods. This paper asks a precise, testable version of the \"runaway growth\" question: under what measurable conditions could capability escalate without bound in finite time, and under what conditions can that be ruled out? We develop an analytic framework for recursive self-improvement that links capability growth to resource build-out and deployment policies. Physical and information-theoretic limits from power, bandwidth, and memory define a service envelope that caps instantaneous improvement. An endogenous growth model couples capital to compute, data, and energy and defines a critical boundary separating superlinear from subcritical regimes. We derive decision rules that map observable series (facility power, IO bandwidth, training throughput, benchmark losses, and spending) into yes/no certificates for runaway versus nonsingular behavior. The framework yields falsifiable tests based on how fast improvement accelerates relative to its current level, and it provides safety controls that are directly implementable in practice, such as power caps, throughput throttling, and evaluation gates. Analytical case studies cover capped-power, saturating-data, and investment-amplified settings, illustrating when the envelope binds and when it does not. The approach is simulation-free and grounded in measurements engineers already collect. Limitations include dependence on the chosen capability metric and on regularity diagnostics; future work will address stochastic dynamics, multi-agent competition, and abrupt architectural shifts. Overall, the results replace speculation with testable conditions and deployable controls for certifying or precluding an AI singularity.", "AI": {"tldr": "本文提出一种新的分析框架来探讨AI系统能力增长无限制的精确可测试问题，建立了一个能够将计算资源与能力增长关联起来的内生增长模型，并提出了安全控制策略。", "motivation": "本文旨在研究在什么条件下AI系统的能力增长可以无限制并在有限时间内发生，以及在什么条件下可以排除这种可能性，从而将关于AI'失控增长'的讨论从猜测转为可测试的条件。", "method": "本文建立了一个分析框架，将能力增长与资源建设和部署策略联系起来，探讨了在什么可测量条件下，能力会无限制地增长，并在有限时间内达到某个点，以及在什么条件下可以排除这种可能性。该框架考虑了能源、带宽和存储的物理极限和信息理论限制，以此定义一个服务范围来上限瞬时改进。通过一个内生增长模型将资本与计算、数据和能源相连接，并定义了一个划分超线性与次临界状态的临界边界。", "result": "本文提出了基于可观察序列（如设施功率、带宽、训练吞吐量、基准损失和支出）的决策规则，以证明AI系统的失控还是非奇点行为。同时，对于何时服务范围会被迫达到上限以及何时不会达到上限的情况，通过分析案例进行说明。", "conclusion": "研究提供了可验证的测试基于改进相对于当前水平加速的速度，并提供了在实践中直接实施的安全控制，比如功率上限，吞吐量限制和评估门控。尽管如此，研究还存在依赖选择的性能度量和常规诊断的局限性。"}}
{"id": "2511.10654", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10654", "abs": "https://arxiv.org/abs/2511.10654", "authors": ["Javier Marín"], "title": "Empirical Characterization of Temporal Constraint Processing in LLMs", "comment": null, "summary": "When deploying LLMs in agentic architectures requiring real-time decisions under temporal constraints, we assume they reliably determine whether action windows remain open or have closed. This assumption is untested. We characterize temporal constraint processing across eight production-scale models (2.8-8B parameters) using deadline detection tasks, revealing systematic deployment risks: bimodal performance distribution (models achieve either 95% or 50% accuracy), extreme prompt brittleness (30-60 percentage point swings from formatting changes alone), and systematic action bias (100% false positive rates in failing models). Parameter count shows no correlation with capability in this range-a 3.8B model matches 7B models while other 7B models fail completely. Fine-tuning on 200 synthetic examples improves models with partial capability by 12-37 percentage points. We demonstrate that temporal constraint satisfaction cannot be reliably learned through next-token prediction on natural language, even with targeted fine-tuning. This capability requires architectural mechanisms for: (1) continuous temporal state representation, (2) explicit constraint checking separate from linguistic pattern matching, (3) systematic compositional reasoning over temporal relations. Current autoregressive architectures lack these mechanisms. Deploying such systems in time-critical applications without hybrid architectures incorporating symbolic reasoning modules represents unacceptable risk.", "AI": {"tldr": "This study examines the performance of LLMs in tasks involving temporal constraints and finds that they exhibit reliability issues. The research suggests that autoregressive models need architectural changes to support better temporal processing, highlighting the risks of their deployment in time-critical applications without these enhancements.", "motivation": "The motivation behind this study is to examine the reliability of LLMs in agentic architectures where real-time decision-making is critical. Specifically, the focus is on the models' capability to handle temporal constraints properly, an aspect that has not been extensively tested before.", "method": "The study evaluates eight production-scale LLMs in the context of temporal constraint processing by utilizing deadline detection tasks, aiming to uncover potential risks in real-time decision-making processes. The models were tested for their accuracy under different temporal constraints and the robustness of their prompt processing.", "result": "The results showed a bimodal distribution in model performance with some achieving high accuracy and others failing significantly. Models displayed brittleness in response to prompt formatting and exhibited biases in decision-making. The study also found no correlation between model size and capability in this task, challenging the assumption that larger models always perform better.", "conclusion": "The conclusion is that temporal constraints satisfaction cannot be reliably acquired through typical training methods. The paper argues for the necessity of incorporating architectural changes that support continuous temporal state representation, explicit constraint checking, and compositional reasoning. This change is essential for safer deployment of these models in time-sensitive environments."}}
{"id": "2511.10701", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.10701", "abs": "https://arxiv.org/abs/2511.10701", "authors": ["Yuankai He", "Weisong Shi"], "title": "Semantic VLM Dataset for Safe Autonomous Driving", "comment": "8 pages, 6 figures, 7 tables", "summary": "CAR-Scenes is a frame-level dataset for autonomous driving that enables training and evaluation of vision-language models (VLMs) for interpretable, scene-level understanding. We annotate 5,192 images drawn from Argoverse 1, Cityscapes, KITTI, and nuScenes using a 28-key category/sub-category knowledge base covering environment, road geometry, background-vehicle behavior, ego-vehicle behavior, vulnerable road users, sensor states, and a discrete severity scale (1-10), totaling 350+ leaf attributes. Labels are produced by a GPT-4o-assisted vision-language pipeline with human-in-the-loop verification; we release the exact prompts, post-processing rules, and per-field baseline model performance. CAR-Scenes also provides attribute co-occurrence graphs and JSONL records that support semantic retrieval, dataset triage, and risk-aware scenario mining across sources. To calibrate task difficulty, we include reproducible, non-benchmark baselines, notably a LoRA-tuned Qwen2-VL-2B with deterministic decoding, evaluated via scalar accuracy, micro-averaged F1 for list attributes, and severity MAE/RMSE on a fixed validation split. We publicly release the annotation and analysis scripts, including graph construction and evaluation scripts, to enable explainable, data-centric workflows for future intelligent vehicles. Dataset: https://github.com/Croquembouche/CAR-Scenes", "AI": {"tldr": "CAR-Scenes是一个用于训练和评估自动驾驶中的视觉-语言模型的帧级数据集，它包含了详细的环境和车辆行为等注释，并且提供标签生成、共现图构建和语义检索等工具。", "motivation": "开发CAR-Scenes是为了提供一个用于自动驾驶的视觉-语言模型训练和评估的数据集，以实现可解释的场景级理解。", "method": "CAR-Scenes使用GPT-4辅助的视觉-语言流水线生成标签，并结合了人工验证。注释涵盖了环境、道路几何、背景车辆行为、自身车辆行为、脆弱道路使用者、传感器状态和一个离散的严重性等级，共有350多项特定属性。", "result": "通过发布注释和分析脚本，包括图构建和评估脚本，研究者们公开了他们的数据集，这将促进未来智能车辆的可解释性及数据驱动的工作流程。", "conclusion": "CAR-Scenes数据集集成了多个来源的数据，并通过标签、共现图和JSONL记录等支持语义检索、数据筛选和风险情景挖掘，显著推进了自动驾驶领域的视觉-语言模型的发展。"}}
{"id": "2511.10655", "categories": ["cs.CL", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2511.10655", "abs": "https://arxiv.org/abs/2511.10655", "authors": ["Andrew Kiruluta", "Priscilla Burity"], "title": "Spectral Neuro-Symbolic Reasoning II: Semantic Node Merging, Entailment Filtering, and Knowledge Graph Alignment", "comment": null, "summary": "This report extends the Spectral Neuro-Symbolic Reasoning (Spectral NSR) framework by introducing three semantically grounded enhancements: (1) transformer-based node merging using contextual embeddings (e.g., Sentence-BERT, SimCSE) to reduce redundancy, (2) sentence-level entailment validation with pretrained NLI classifiers (e.g., RoBERTa, DeBERTa) to improve edge quality, and (3) alignment with external knowledge graphs (e.g., ConceptNet, Wikidata) to augment missing context. These modifications enhance graph fidelity while preserving the core spectral reasoning pipeline. Experimental results on ProofWriter, EntailmentBank, and CLUTRR benchmarks show consistent accuracy gains (up to +3.8\\%), improved generalization to adversarial cases, and reduced inference noise. The novelty lies in performing semantic and symbolic refinement entirely upstream of the spectral inference stage, enabling efficient, interpretable, and scalable reasoning without relying on quadratic attention mechanisms. In summary, this work extends the Spectral NSR framework with modular, semantically grounded preprocessing steps that improve graph quality without altering the core spectral reasoning engine. The result is a more robust, interpretable, and scalable reasoning system suitable for deployment in open-domain and real-world settings.", "AI": {"tldr": "该论文通过引入语义增强措施改进了光谱神经符号推理框架，这些措施包括上下文嵌入的节点合并、NLI分类器进行蕴含验证和与外部知识图对齐，从而在不改变核心推理引擎的基础上提升了图的质量。", "motivation": "该研究的动机是为了提高光谱神经符号推理框架的性能和可扩展性。通过引入语义增强措施，使得框架可以在不改变核心推理引擎的情况下，改善图的质量，提高推理的准确性和可靠性，使其更适合开放领域和现实世界的应用场景。", "method": "本研究提出的方法主要包括三个语义增强措施：通过上下文嵌入进行节点合并、使用NLI分类器进行句子级蕴含验证，以及与外部知识图对齐。这些增强措施不改变核心的光谱推理框架，而是进行预处理来提高图的保真度。", "result": "该论文通过引入三种语义增强措施扩展了光谱神经符号推理（Spectral NSR）框架：(1) 使用上下文嵌入（如Sentence-BERT、SimCSE）进行节点合并以减少冗余；(2) 采用预训练的NLI分类器（如RoBERTa、DeBERTa）进行句子级蕴含验证来提高边的质量；(3) 与外部知识图（如ConceptNet、Wikidata）对齐以补充缺少的上下文。这些改进增强了图的保真度，同时保留了核心的光谱推理管道。实验结果表明，这些改进在ProofWriter、EntailmentBank和CLUTRR基准测试中的准确性提升了最多3.8%，并且提高了对对抗案例的外推能力和减少了推理噪声。该研究的独特之处在于在光谱推理阶段之前完全进行语义和符号细化，这使得推理系统可以高效、可解释和可扩展，且不依赖二次注意力机制。", "conclusion": "结论表明，通过引入语义增强措施，光谱神经符号推理框架不仅在外推能力和准确性方面得到了提升，而且在可解释性和可扩展性方面也有所改进。这些改进使得该框架能够在各种环境中更有效地部署。"}}
{"id": "2511.10721", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10721", "abs": "https://arxiv.org/abs/2511.10721", "authors": ["Sheng-Yu Wang", "Aaron Hertzmann", "Alexei A Efros", "Richard Zhang", "Jun-Yan Zhu"], "title": "Fast Data Attribution for Text-to-Image Models", "comment": "NeurIPS 2025 camera ready. Project page: https://peterwang512.github.io/FastGDA", "summary": "Data attribution for text-to-image models aims to identify the training images that most significantly influenced a generated output. Existing attribution methods involve considerable computational resources for each query, making them impractical for real-world applications. We propose a novel approach for scalable and efficient data attribution. Our key idea is to distill a slow, unlearning-based attribution method to a feature embedding space for efficient retrieval of highly influential training images. During deployment, combined with efficient indexing and search methods, our method successfully finds highly influential images without running expensive attribution algorithms. We show extensive results on both medium-scale models trained on MSCOCO and large-scale Stable Diffusion models trained on LAION, demonstrating that our method can achieve better or competitive performance in a few seconds, faster than existing methods by 2,500x - 400,000x. Our work represents a meaningful step towards the large-scale application of data attribution methods on real-world models such as Stable Diffusion.", "AI": {"tldr": "A new, efficient method for data attribution in text-to-image models is developed, achieving competitive performance in seconds and drastically reducing computational costs compared to existing approaches.", "motivation": "The motivation is to overcome the high computational cost of existing data attribution methods, making them practical for real-world applications.", "method": "Our method involves distilling an unlearning-based attribution method into a feature embedding space to efficiently identify influential training images for text-to-image models.", "result": "The method finds highly influential images quickly, demonstrating better or competitive performance compared to existing methods, but is significantly faster (2,500x - 400,000x).", "conclusion": "This paper presents an efficient solution for data attribution that saves significant computational resources, advancing the practical application of such methods on large-scale models."}}
{"id": "2511.10656", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10656", "abs": "https://arxiv.org/abs/2511.10656", "authors": ["Biao Liu", "Ning Xu", "Junming Yang", "Xin Geng"], "title": "Preference Orchestrator: Prompt-Aware Multi-Objective Alignment for Large Language Models", "comment": null, "summary": "While Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks, aligning these models with varying human preferences across multiple objectives remains a significant challenge in practical deployments. Existing multi-objective alignment methods rely on manually specified preference weights, which not only burden users with difficult preference specification tasks but also lead to suboptimal training efficiency due to exploration of irrelevant preference combinations. To alleviate these issues, we propose a novel framework named PRO, i.e., PReference Orchestrator, which features a lightweight preference adapter that automatically infers prompt-specific preference weights during both training and deployment phases. Specifically, the adapter automatically learns appropriate preference weights for each prompt by training on normalized reward scores from multiple reward models for preferred responses, which inherently reflect effective preference balances across objectives. Additionally, We provide theoretical analysis proving that our prompt-aware preference mechanism achieves superior performance compared to fixed preference weights in multi-objective alignment scenarios. Extensive experiments across multiple tasks demonstrate the effectiveness of our method over existing multi-objective alignment approaches.", "AI": {"tldr": "我们提出了一个新型框架PRO，通过自动推断偏好权重来改善大型语言模型与人类偏好的对齐。", "motivation": "为了减轻用户指定偏好权重的负担，并提高训练效率，避免探索无关的偏好组合。", "method": "我们提出了一种名为PRO（PReference Orchestrator）的新框架，该框架包含一个轻量级的偏好适配器，它能够在训练和部署阶段自动推断出特定提示的偏好权重。", "result": "实验证明我们的方法在多任务中优于现有的多目标对齐方法。", "conclusion": "我们的提示感知偏好机制优于固定偏好权重，在多目标对齐的情境下表现出更佳的性能。"}}
{"id": "2511.10766", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10766", "abs": "https://arxiv.org/abs/2511.10766", "authors": ["Pooja P Jain", "Pietro Mascagni", "Giuseppe Massimiani", "Nabani Banik", "Marta Goglia", "Lorenzo Arboit", "Britty Baby", "Andrea Balla", "Ludovica Baldari", "Gianfranco Silecchia", "Claudio Fiorillo", "CompSurg Colorectal Experts Group", "Sergio Alfieri", "Salvador Morales-Conde", "Deborah S Keller", "Luigi Boni", "Nicolas Padoy"], "title": "Expert Consensus-based Video-Based Assessment Tool for Workflow Analysis in Minimally Invasive Colorectal Surgery: Development and Validation of ColoWorkflow", "comment": "12 pages, 4 figures", "summary": "Minimally invasive colorectal surgery is characterized by procedural variability, a difficult learning curve, and complications that impact quality and outcomes. Video-based assessment (VBA) offers an opportunity to generate data-driven insights to reduce variability, optimize training, and improve surgical performance. However, existing tools for workflow analysis remain difficult to standardize and implement. This study aims to develop and validate a VBA tool for workflow analysis across minimally invasive colorectal procedures. A Delphi process was conducted to achieve consensus on generalizable workflow descriptors. The resulting framework informed the development of a new VBA tool, ColoWorkflow. Independent raters then applied ColoWorkflow to a multicentre video dataset of laparoscopic and robotic colorectal surgery (CRS). Applicability and inter-rater reliability were evaluated. Consensus was achieved for 10 procedure-agnostic phases and 34 procedure-specific steps describing CRS workflows. ColoWorkflow was developed and applied to 54 colorectal operative videos (left and right hemicolectomies, sigmoid and rectosigmoid resections, and total proctocolectomies) from five centres. The tool demonstrated broad applicability, with all but one label utilized. Inter-rater reliability was moderate, with mean Cohen's K of 0.71 for phases and 0.66 for steps. Most discrepancies arose at phase transitions and step boundary definitions. ColoWorkflow is the first consensus-based, validated VBA tool for comprehensive workflow analysis in minimally invasive CRS. It establishes a reproducible framework for video-based performance assessment, enabling benchmarking across institutions and supporting the development of artificial intelligence-driven workflow recognition. Its adoption may standardize training, accelerate competency acquisition, and advance data-informed surgical quality improvement.", "AI": {"tldr": "开发了一个共识基础上的视频工作流分析工具ColoWorkflow，用于微创结直肠手术。该工具在多中心视频数据集上显示出良好的一致性和应用性。", "motivation": "减少可变性，优化培训，提高手术表现。现有的工作流分析工具难以标准化和实施，本研究旨在开发和验证一种针对微创结直肠手术（CRS）工作流分析的视频分析工具。", "method": "通过共识过程制定了一个通用的工作流描述符框架，并在此基础上开发了名为ColoWorkflow的新视频分析工具。使用多中心视频数据集对独立评级者应用了ColoWorkflow工具，进行了可应用性和一致性测试。", "result": "达成共识的10个程序无关阶段和34个程序特定步骤描述了CRS工作流程。ColoWorkflow工具应用于从5个中心收集的54个结直肠手术视频中。工具显示了广泛的应用性，所有标签中只有一个未被使用。阶段的一致性Cohen's K平均值为0.71，步骤的一致性平均值为0.66。大多数差异发生在阶段转换和步骤边界定义上。", "conclusion": "ColoWorkflow是第一个达成共识且经过验证的基于视频的工作流分析工具，用于微创CRS的全面工作流分析。它为基于视频的性能评估建立了可重复的框架，使跨机构基准测试成为可能，并支持基于人工智能的工作流程识别发展。"}}
{"id": "2511.10657", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10657", "abs": "https://arxiv.org/abs/2511.10657", "authors": ["You Zuo", "Kim Gerdes", "Eric Villemonte de La Clergerie", "Benoît Sagot"], "title": "Patent Representation Learning via Self-supervision", "comment": null, "summary": "This paper presents a simple yet effective contrastive learning framework for learning patent embeddings by leveraging multiple views from within the same document. We first identify a patent-specific failure mode of SimCSE style dropout augmentation: it produces overly uniform embeddings that lose semantic cohesion. To remedy this, we propose section-based augmentation, where different sections of a patent (e.g., abstract, claims, background) serve as complementary views. This design introduces natural semantic and structural diversity, mitigating over-dispersion and yielding embeddings that better preserve both global structure and local continuity. On large-scale benchmarks, our fully self-supervised method matches or surpasses citation-and IPC-supervised baselines in prior-art retrieval and classification, while avoiding reliance on brittle or incomplete annotations. Our analysis further shows that different sections specialize for different tasks-claims and summaries benefit retrieval, while background sections aid classification-highlighting the value of patents' inherent discourse structure for representation learning. These results highlight the value of exploiting intra-document views for scalable and generalizable patent understanding.", "AI": {"tldr": "研究提出了基于多重视图的对比学习框架，用于构建专利嵌入，通过自然语义和结构多样性提升嵌入表示的优良性，并展示了其超越传统标注依赖方法的潜力。", "motivation": "现有的专利嵌入学习方法存在一个问题：使用SimCSE样式的dropout增强会导致嵌入表示过于统一，从而失去语义连贯性。为解决这一问题并提升专利理解的可伸缩性和通用性，本研究提出了新的对比学习框架。", "method": "本研究提出了一种用于学习专利嵌入的简单而有效的对比学习框架，该框架通过利用同一文档内的多重视图来实现。为解决SimCSE样式的dropout增强导致过度一致的嵌入表示的问题，研究者提出了一种基于部分的增强方法，即使用专利的不同部分（如摘要、声明、背景）作为互补的视图。这种方法引入了自然的语义和结构多样性，从而克服了过度分散的问题，并生成了更好地保存全局结构和局部连续性的嵌入表示。", "result": "在大规模基准测试中，本研究的完全自我监督的方法在先验艺术检索和分类任务上匹配或超越了依赖引用和IPC标注的方法，同时避免了依赖脆弱或不完整标注的局限性。实验结果表明，不同的部分在不同的任务中表现出了专业能力：声明和摘要有助于检索，而背景部分则有助于分类。", "conclusion": "研究结果表明，利用文档内的多重视图对于实现规模化和通用化的专利理解具有重要价值。"}}
{"id": "2511.10774", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10774", "abs": "https://arxiv.org/abs/2511.10774", "authors": ["Junjie Zhang", "Feng Zhao", "Hanqiang Liu", "Jun Yu"], "title": "Frequency-Aware Vision-Language Multimodality Generalization Network for Remote Sensing Image Classification", "comment": null, "summary": "The booming remote sensing (RS) technology is giving rise to a novel multimodality generalization task, which requires the model to overcome data heterogeneity while possessing powerful cross-scene generalization ability. Moreover, most vision-language models (VLMs) usually describe surface materials in RS images using universal texts, lacking proprietary linguistic prior knowledge specific to different RS vision modalities. In this work, we formalize RS multimodality generalization (RSMG) as a learning paradigm, and propose a frequency-aware vision-language multimodality generalization network (FVMGN) for RS image classification. Specifically, a diffusion-based training-test-time augmentation (DTAug) strategy is designed to reconstruct multimodal land-cover distributions, enriching input information for FVMGN. Following that, to overcome multimodal heterogeneity, a multimodal wavelet disentanglement (MWDis) module is developed to learn cross-domain invariant features by resampling low and high frequency components in the frequency domain. Considering the characteristics of RS vision modalities, shared and proprietary class texts is designed as linguistic inputs for the transformer-based text encoder to extract diverse text features. For multimodal vision inputs, a spatial-frequency-aware image encoder (SFIE) is constructed to realize local-global feature reconstruction and representation. Finally, a multiscale spatial-frequency feature alignment (MSFFA) module is suggested to construct a unified semantic space, ensuring refined multiscale alignment of different text and vision features in spatial and frequency domains. Extensive experiments show that FVMGN has the excellent multimodality generalization ability compared with state-of-the-art (SOTA) methods.", "AI": {"tldr": "本文提出了遥感多模态泛化学习范式，设计了频率感知的视觉-语言多模态泛化网络（FVMGN）用于提高遥感图像分类的多模态泛化能力，实验结果表明该方法优于现有方法。", "motivation": "鉴于多模态遥感图像分类任务中数据异质性的问题以及现有视觉-语言模型描述遥感图像时缺乏特定领域的语言先验知识，本文旨在提出一种新的遥感多模态泛化学习范式。", "method": "本文提出了一种基于频率感知的视觉-语言多模态泛化网络（FVMGN），用于遥感图像分类。该方法包括扩散增强策略（DTAug），多模态小波分解模块（MWDis），共享和特定类别文本，空间频率感知图像编码器（SFIE），以及多尺度空间频率特征对齐模块（MSFFA）等组成部分。", "result": "实验结果显示，FVMGN具有出色的多模态泛化能力，优于现有的最先进方法。", "conclusion": "FVMGN模型通过一系列精心设计的模块，有效地解决了遥感图像多模态泛化和数据异质性的问题，证明了其在遥感图像分类中的优越性。"}}
{"id": "2511.10658", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10658", "abs": "https://arxiv.org/abs/2511.10658", "authors": ["Douwe J. Spaanderman", "Karthik Prathaban", "Petr Zelina", "Kaouther Mouheb", "Lukáš Hejtmánek", "Matthew Marzetti", "Antonius W. Schurink", "Damian Chan", "Ruben Niemantsverdriet", "Frederik Hartmann", "Zhen Qian", "Maarten G. J. Thomeer", "Petr Holub", "Farhan Akram", "Frank J. Wolters", "Meike W. Vernooij", "Cornelis Verhoef", "Esther E. Bron", "Vít Nováček", "Dirk J. Grünhagen", "Wiro J. Niessen", "Martijn P. A. Starmans", "Stefan Klein"], "title": "Evaluating Open-Weight Large Language Models for Structured Data Extraction from Narrative Medical Reports Across Multiple Use Cases and Languages", "comment": null, "summary": "Large language models (LLMs) are increasingly used to extract structured information from free-text clinical records, but prior work often focuses on single tasks, limited models, and English-language reports. We evaluated 15 open-weight LLMs on pathology and radiology reports across six use cases, colorectal liver metastases, liver tumours, neurodegenerative diseases, soft-tissue tumours, melanomas, and sarcomas, at three institutes in the Netherlands, UK, and Czech Republic. Models included general-purpose and medical-specialised LLMs of various sizes, and six prompting strategies were compared: zero-shot, one-shot, few-shot, chain-of-thought, self-consistency, and prompt graph. Performance was assessed using task-appropriate metrics, with consensus rank aggregation and linear mixed-effects models quantifying variance. Top-ranked models achieved macro-average scores close to inter-rater agreement across tasks. Small-to-medium general-purpose models performed comparably to large models, while tiny and specialised models performed worse. Prompt graph and few-shot prompting improved performance by ~13%. Task-specific factors, including variable complexity and annotation variability, influenced results more than model size or prompting strategy. These findings show that open-weight LLMs can extract structured data from clinical reports across diseases, languages, and institutions, offering a scalable approach for clinical data curation.", "AI": {"tldr": "研究评估了15个开源权重的大型语言模型在六种临床使用案例中的表现，比较了六种提示策略，表明这些模型可以跨疾病、语言和机构从临床报告中提取结构化数据。小到中型模型的表现与大型模型相当。任务特定因素比模型大小对结果影响更大。", "motivation": "大型语言模型越来越多地用于从自由文本的临床记录中提取结构化信息，但以往的研究主要集中在单一任务、有限的模型和英文报告上。本研究的动机是更全面地评估这些模型在多任务、多语言和多地区的临床语料库中的表现。", "method": "研究评估了15个开源权重的大型语言模型在病理学和放射学报告中的六种使用案例中的表现，比较了六种提示策略，并使用任务相关的度量标准进行了评估。", "result": "顶级排名的模型在各种任务中的宏观平均得分接近于人工评估的一致性。提示图和少量提示策略提高了大约13%的性能。", "conclusion": "小到中型的通用语言模型的表现与大型模型相当，而小模型和专业模型的表现较差。研究发现，任务特定的因素（如复杂性和注释变化）比模型大小或提示策略对结果影响更大。因此，这些发现表明开源权重的大型语言模型可以跨疾病、语言和机构从临床报告中提取结构化数据，提供了一种可扩展的临床数据整理方法。"}}
{"id": "2511.10799", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10799", "abs": "https://arxiv.org/abs/2511.10799", "authors": ["Manish Dhakal", "Venkat R. Dasari", "Raj Sunderraman", "Yi Ding"], "title": "GFT: Graph Feature Tuning for Efficient Point Cloud Analysis", "comment": "WACV 2026", "summary": "Parameter-efficient fine-tuning (PEFT) significantly reduces computational and memory costs by updating only a small subset of the model's parameters, enabling faster adaptation to new tasks with minimal loss in performance. Previous studies have introduced PEFTs tailored for point cloud data, as general approaches are suboptimal. To further reduce the number of trainable parameters, we propose a point-cloud-specific PEFT, termed Graph Features Tuning (GFT), which learns a dynamic graph from initial tokenized inputs of the transformer using a lightweight graph convolution network and passes these graph features to deeper layers via skip connections and efficient cross-attention modules. Extensive experiments on object classification and segmentation tasks show that GFT operates in the same domain, rivalling existing methods, while reducing the trainable parameters. Code is at https://github.com/manishdhakal/GFT.", "AI": {"tldr": "研究提出了一种新的针对点云数据的参数高效微调方法GFT，可在不牺牲性能的情况下减少可训练参数数量。", "motivation": "由于通用方法对于点云数据效果不佳，本研究旨在通过进一步减少可训练参数的数量来改进参数高效微调(PEFT)。", "method": "提出了一种名为Graph Features Tuning (GFT)的点云特定的参数高效微调方法。GFT使用轻量级的图卷积网络从变压器的初始标记化输入中学习动态图，并通过跳过连接和高效的交叉注意力模块将图特征传递给更深的层。", "result": "在物体分类和分割任务上的广泛实验表明，GFT在减少可训练参数的同时，与现有方法在效果上不相上下。", "conclusion": "GFT方法证明了一个点云特定的PEFT模型能够在减少参数数量的同时，保持性能不下降。"}}
{"id": "2511.10659", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.10659", "abs": "https://arxiv.org/abs/2511.10659", "authors": ["Vikram Aggarwal", "Jay Kulkarni", "Aditi Mascarenhas", "Aakriti Narang", "Siddarth Raman", "Ajay Shah", "Susan Thomas"], "title": "Information Extraction From Fiscal Documents Using LLMs", "comment": "6 pages. Presented at the AI for Financial Inclusion, Risk Modeling and Resilience in Emerging Markets workshop at ACM ICAIF 2025 Singapore", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in text comprehension, but their ability to process complex, hierarchical tabular data remains underexplored. We present a novel approach to extracting structured data from multi-page government fiscal documents using LLM-based techniques. Applied to annual fiscal documents from the State of Karnataka in India (200+ pages), our method achieves high accuracy through a multi-stage pipeline that leverages domain knowledge, sequential context, and algorithmic validation. A large challenge with traditional OCR methods is the inability to verify the accurate extraction of numbers. When applied to fiscal data, the inherent structure of fiscal tables, with totals at each level of the hierarchy, allows for robust internal validation of the extracted data. We use these hierarchical relationships to create multi-level validation checks. We demonstrate that LLMs can read tables and also process document-specific structural hierarchies, offering a scalable process for converting PDF-based fiscal disclosures into research-ready databases. Our implementation shows promise for broader applications across developing country contexts.", "AI": {"tldr": "研究展示了利用大型语言模型处理财政文件中表格数据的方法，通过分阶段的算法验证实现了高精度的数据提取，为发展中国家提供了高效的数据处理解决方案。", "motivation": "尽管大型语言模型（LLMs）在文本理解方面表现出色，但对于处理复杂、层次化的表格数据的能力却尚未得到充分探索。传统OCR方法的一个主要挑战是无法验证提取数字的准确性。", "method": "我们提出了一种新型的方法，用于从多页政府财政文件中提取结构化数据。该方法利用了基于大型语言模型（LLMs）的技术，并通过一个分阶段的管道实现了高精度，这个管道利用了领域知识、序列上下文以及算法验证。", "result": "我们用这种方法对印度卡纳塔克邦的年度财政文件进行了实验，这些文件有200多页。结果表明，通过利用财政表格的内在结构进行多级验证检查，这种方法可以非常有效地验证提取数字的准确性。", "conclusion": "实验结果表明，LLMs不仅能够读取表格，还能处理特定文档的结构层次，提供了一种将基于PDF的财政披露转换为研究数据库的可扩展过程。该方法在发展中国家背景下有更广泛的应用前景。"}}
{"id": "2511.10861", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10861", "abs": "https://arxiv.org/abs/2511.10861", "authors": ["Daisuke Yasui", "Toshitaka Matsuki", "Hiroshi Sato"], "title": "Accuracy-Preserving CNN Pruning Method under Limited Data Availability", "comment": null, "summary": "Convolutional Neural Networks (CNNs) are widely used in image recognition and have succeeded in various domains. CNN models have become larger-scale to improve accuracy and generalization performance. Research has been conducted on compressing pre-trained models for specific target applications in environments with limited computing resources. Among model compression techniques, methods using Layer-wise Relevance Propagation (LRP), an explainable AI technique, have shown promise by achieving high pruning rates while preserving accuracy, even without fine-tuning. Because these methods do not require fine-tuning, they are suited to scenarios with limited data. However, existing LRP-based pruning approaches still suffer from significant accuracy degradation, limiting their practical usability. This study proposes a pruning method that achieves a higher pruning rate while preserving better model accuracy. Our approach to pruning with a small amount of data has achieved pruning that preserves accuracy better than existing methods.", "AI": {"tldr": "The paper proposes a new pruning method for CNN models that achieves a higher pruning rate with less accuracy degradation, especially in scenarios with limited data.", "motivation": "To address the limitations of existing LRP-based pruning methods which suffer from significant accuracy degradation, especially in limited-data scenarios.", "method": "Proposing a new pruning method that improves upon LRP-based approaches, aimed at achieving higher pruning rates while preserving model accuracy.", "result": "The proposed method has achieved better accuracy preservation compared to existing methods, even when applied to models trained with a small amount of data.", "conclusion": "The new pruning method is effective for compressing CNN models, offering a balance between pruning rate and model accuracy, particularly beneficial for applications with limited computing resources and data."}}
{"id": "2511.10660", "categories": ["cs.CL", "cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.10660", "abs": "https://arxiv.org/abs/2511.10660", "authors": ["Qihang Zhang", "Muchen Li", "Ziao Wang", "Renjie Liao", "Lele Wang"], "title": "Test-Time Steering for Lossless Text Compression via Weighted Product of Experts", "comment": "8 pages. Accepted by EMNLP 2025. Code and additional details are available at: https://qihang-zhang.com/Learning-Sys-Blog/2025/10/15/weighted-product-of-experts.html", "summary": "Lossless compression techniques are crucial in an era of rapidly growing data. Traditional universal compressors like gzip offer low computational overhead, high speed, and broad applicability across data distributions. However, they often lead to worse compression rates than modern neural compressors, which leverage large-scale training data to model data distributions more effectively. Despite their advantages, neural compressors struggle to generalize to unseen data. To address this limitation, we propose a novel framework that performs Test-Time Steering via a Weighted Product of Experts (wPoE). At inference, our method adaptively combines a universal compression model with a pretrained neural language model, ensuring the compression rate is at least as good as that of the best individual model. Extensive experiments demonstrate that our approach improves the performance of text compression without requiring fine-tuning. Furthermore, it seamlessly integrates with any autoregressive language model, providing a practical solution for enhancing text compression across diverse data distributions.", "AI": {"tldr": "The paper introduces a Test-Time Steering via Weighted Product of Experts framework to adaptively combine a universal compression model and a pretrained neural language model for efficient text compression.", "motivation": "To enhance lossless compression techniques by combining the advantages of traditional and neural compression methods while addressing the generalization issue of neural compressors.", "method": "Propose a framework called Test-Time Steering via Weighted Product of Experts (wPoE) to adaptively combine a universal compression model with a pretrained neural language model at inference time.", "result": "Experiments show the approach improves text compression performance without fine-tuning, integrating seamlessly with any autoregressive language model and providing a practical solution for data distribution diversity.", "conclusion": "The proposed method offers an effective way to enhance text compression rates without requiring model fine-tuning, positioning itself as a versatile solution for a broad range of data distributions."}}
{"id": "2511.10866", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10866", "abs": "https://arxiv.org/abs/2511.10866", "authors": ["Seoik Jung", "Taekyung Song", "Yangro Lee", "Sungjun Lee"], "title": "Short-Window Sliding Learning for Real-Time Violence Detection via LLM-based Auto-Labeling", "comment": "5 pages, 2 figures. Accepted paper for the IEIE (Institute of Electronics and Information Engineers) Fall Conference 2025. Presentation on Nov 27, 2025", "summary": "This paper proposes a Short-Window Sliding Learning framework for real-time violence detection in CCTV footages. Unlike conventional long-video training approaches, the proposed method divides videos into 1-2 second clips and applies Large Language Model (LLM)-based auto-caption labeling to construct fine-grained datasets. Each short clip fully utilizes all frames to preserve temporal continuity, enabling precise recognition of rapid violent events. Experiments demonstrate that the proposed method achieves 95.25\\% accuracy on RWF-2000 and significantly improves performance on long videos (UCF-Crime: 83.25\\%), confirming its strong generalization and real-time applicability in intelligent surveillance systems.", "AI": {"tldr": "A Short-Window Sliding Learning framework is proposed for real-time violence detection using short video clips and large language model-based auto-captioning, achieving high accuracy and strong applicability in intelligent surveillance systems.", "motivation": "The motivation behind this paper is to improve the effectiveness of real-time violence detection in CCTV footages by utilizing short video clips and advanced auto-caption labeling to build more precise datasets compared to traditional long-video training methods.", "method": "The paper introduces a Short-Window Sliding Learning framework that divides videos into short clips of 1-2 seconds and uses LLM-based automatic caption labeling to create detailed datasets for real-time violence detection in CCTV footage. This method ensures temporal continuity by using all frames within the clips, allowing for accurate recognition of quick violent incidents.", "result": "The experimental results show that the proposed method achieves an accuracy of 95.25% on the RWF-2000 dataset and improves performance on long videos (83.25% on UCF-Crime dataset), demonstrating good generalization and real-time applicability in intelligent surveillance systems.", "conclusion": "The Short-Window Sliding Learning framework enhances real-time violence detection, especially for rapid violent events, by leveraging short clips and LLM-based auto-caption labeling, leading to improved accuracy in real-world surveillance applications."}}
{"id": "2511.10661", "categories": ["cs.CL", "cs.LG", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.10661", "abs": "https://arxiv.org/abs/2511.10661", "authors": ["Rachel Longjohn", "Shang Wu", "Saatvik Kher", "Catarina Belém", "Padhraic Smyth"], "title": "Bayesian Evaluation of Large Language Model Behavior", "comment": "Accepted to NeurIPS 2025 Workshop on Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent Abilities, and Scaling", "summary": "It is increasingly important to evaluate how text generation systems based on large language models (LLMs) behave, such as their tendency to produce harmful output or their sensitivity to adversarial inputs. Such evaluations often rely on a curated benchmark set of input prompts provided to the LLM, where the output for each prompt may be assessed in a binary fashion (e.g., harmful/non-harmful or does not leak/leaks sensitive information), and the aggregation of binary scores is used to evaluate the LLM. However, existing approaches to evaluation often neglect statistical uncertainty quantification. With an applied statistics audience in mind, we provide background on LLM text generation and evaluation, and then describe a Bayesian approach for quantifying uncertainty in binary evaluation metrics. We focus in particular on uncertainty that is induced by the probabilistic text generation strategies typically deployed in LLM-based systems. We present two case studies applying this approach: 1) evaluating refusal rates on a benchmark of adversarial inputs designed to elicit harmful responses, and 2) evaluating pairwise preferences of one LLM over another on a benchmark of open-ended interactive dialogue examples. We demonstrate how the Bayesian approach can provide useful uncertainty quantification about the behavior of LLM-based systems.", "AI": {"tldr": "本文提出使用贝叶斯方法来量化大规模语言模型（LLMs）行为评价中的不确定性，特别是在文本生成策略的不确定性上。", "motivation": "现有评估方法往往忽视了统计不确定性量化，因此本文旨在为统计学分析者提供背景信息，解决LLMs在评估过程中不确定性量化的问题。", "method": "本文提出了一种贝叶斯方法来量化基于大规模语言模型（LLMs）的二元评价指标中的不确定性。重点在于由LLMs系统中的概率文本生成策略引起的不确定性。", "result": "通过两个案例研究展示了贝叶斯方法在评估LLMs系统的行为和偏好比较中的应用，证明了该方法对于量化不确定性是有效的。", "conclusion": "贝叶斯方法可以为基于大规模语言模型的系统的具体行为提供有用且精确的不确定性量化。"}}
{"id": "2511.10892", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10892", "abs": "https://arxiv.org/abs/2511.10892", "authors": ["Feng Li", "Ke Wu", "Yongwei Li"], "title": "MCN-CL: Multimodal Cross-Attention Network and Contrastive Learning for Multimodal Emotion Recognition", "comment": "Accepted by 32nd International Conference on MultiMedia Modeling (MMM 2026)", "summary": "Multimodal emotion recognition plays a key role in many domains, including mental health monitoring, educational interaction, and human-computer interaction. However, existing methods often face three major challenges: unbalanced category distribution, the complexity of dynamic facial action unit time modeling, and the difficulty of feature fusion due to modal heterogeneity. With the explosive growth of multimodal data in social media scenarios, the need for building an efficient cross-modal fusion framework for emotion recognition is becoming increasingly urgent. To this end, this paper proposes Multimodal Cross-Attention Network and Contrastive Learning (MCN-CL) for multimodal emotion recognition. It uses a triple query mechanism and hard negative mining strategy to remove feature redundancy while preserving important emotional cues, effectively addressing the issues of modal heterogeneity and category imbalance. Experiment results on the IEMOCAP and MELD datasets show that our proposed method outperforms state-of-the-art approaches, with Weighted F1 scores improving by 3.42% and 5.73%, respectively.", "AI": {"tldr": "The paper presents the MCN-CL method, which uses cross-attention networks and contrastive learning to enhance multimodal emotion recognition, showing improved performance on standard datasets compared to existing methods.", "motivation": "The motivation stems from the need to develop an efficient cross-modal fusion framework for emotion recognition in the face of challenges such as unbalanced category distributions and the complexity in dealing with modal heterogeneity.", "method": "This paper proposes Multimodal Cross-Attention Network and Contrastive Learning (MCN-CL) to address the challenges in multimodal emotion recognition, using a triple query mechanism and hard negative mining strategy.", "result": "Experiments on IEMOCAP and MELD datasets demonstrate that the proposed MCN-CL method achieves superior performance with Weighted F1 score improvements of 3.42% and 5.73% respectively.", "conclusion": "The proposed MCN-CL method outperforms state-of-the-art approaches in multimodal emotion recognition, as evidenced by improved Weighted F1 scores on the IEMOCAP and MELD datasets."}}
{"id": "2511.10664", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10664", "abs": "https://arxiv.org/abs/2511.10664", "authors": ["Chengxuan Xia", "Qianye Wu", "Hongbin Guan", "Sixuan Tian", "Yilun Hao", "Xiaoyu Wu"], "title": "Evaluating Modern Large Language Models on Low-Resource and Morphologically Rich Languages:A Cross-Lingual Benchmark Across Cantonese, Japanese, and Turkish", "comment": "This paper requires XeLaTeX for proper Unicode rendering of Japanese and Cantonese text", "summary": "Large language models (LLMs) have achieved impressive results in high-resource languages like English, yet their effectiveness in low-resource and morphologically rich languages remains underexplored. In this paper, we present a comprehensive evaluation of seven cutting-edge LLMs -- including GPT-4o, GPT-4, Claude~3.5~Sonnet, LLaMA~3.1, Mistral~Large~2, LLaMA-2~Chat~13B, and Mistral~7B~Instruct -- on a new cross-lingual benchmark covering \\textbf{Cantonese, Japanese, and Turkish}. Our benchmark spans four diverse tasks: open-domain question answering, document summarization, English-to-X translation, and culturally grounded dialogue. We combine \\textbf{human evaluations} (rating fluency, factual accuracy, and cultural appropriateness) with automated metrics (e.g., BLEU, ROUGE) to assess model performance.\n  Our results reveal that while the largest proprietary models (GPT-4o, GPT-4, Claude~3.5) generally lead across languages and tasks, significant gaps persist in culturally nuanced understanding and morphological generalization. Notably, GPT-4o demonstrates robust multilingual performance even on cross-lingual tasks, and Claude~3.5~Sonnet achieves competitive accuracy on knowledge and reasoning benchmarks. However, all models struggle to some extent with the unique linguistic challenges of each language, such as Turkish agglutinative morphology and Cantonese colloquialisms. Smaller open-source models (LLaMA-2~13B, Mistral~7B) lag substantially in fluency and accuracy, highlighting the resource disparity. We provide detailed quantitative results, qualitative error analysis, and discuss implications for developing more culturally aware and linguistically generalizable LLMs. Our benchmark and evaluation data are released to foster reproducibility and further research.", "AI": {"tldr": "该论文全面评估了七种先进的大语言模型在涵盖粤语、日语和土耳其语的跨语言基准上的表现，结果显示大型专有模型在大多数任务中领先，但所有模型在处理特定语言的独特语言挑战方面存在不足。", "motivation": "评估大语言模型在低资源和形态丰富的语言中的表现，填补研究空白。", "method": "构建了一个新的跨语言基准，覆盖了粤语、日语和土耳其语，并进行了涵盖开放域问题回答、文档摘要、英译X和文化相关的对话等四个任务的评估。", "result": "大型专有模型（如GPT-4o、GPT-4、Claude 3.5）在大多数语言和任务中表现最优秀，但存在文化内涵理解和形态泛化方面的差距。较小的开源模型在流畅度和准确度上落后。", "conclusion": "尽管大型模型取得了进步，但仍存在一些挑战，如土耳其语的词缀组合和粤语的俚语等，这提示了开发更具文化意识和语言泛化的大型语言模型的必要性。"}}
{"id": "2511.10894", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10894", "abs": "https://arxiv.org/abs/2511.10894", "authors": ["Luciano Araujo Dourado Filho", "Almir Moreira da Silva Neto", "Anthony Miyaguchi", "Rodrigo Pereira David", "Rodrigo Tripodi Calumby", "Lukáš Picek"], "title": "DINOv3 as a Frozen Encoder for CRPS-Oriented Probabilistic Rainfall Nowcasting", "comment": null, "summary": "This paper proposes a competitive and computationally efficient approach to probabilistic rainfall nowcasting. A video projector (V-JEPA Vision Transformer) associated to a lightweight probabilistic head is attached to a pre-trained satellite vision encoder (DINOv3\\text{-}SAT493M) to map encoder tokens into a discrete empirical CDF (eCDF) over 4-hour accumulated rainfall. The projector-head is optimized end-to-end over the Continuous Ranked Probability Score (CRPS). As an alternative, 3D-UNET baselines trained with an aggregate Rank Probability Score and a per-pixel Gamma-Hurdle objective are used. On the Weather4Cast 2025 benchmark, the proposed method achieved a promising performance, with a CRPS of 3.5102 (CRPS), which represents $\\approx$26\\% in effectiveness gain against the best 3D-UNET.", "AI": {"tldr": "A new model combining V-JEPA Vision Transformer and DINOv3-SAT493M for probabilistic rainfall forecasting outperforms traditional 3D-UNET methods by 26% on the Weather4Cast 2025 benchmark.", "motivation": "The motivation behind this research is to provide a competitive and computationally efficient approach to probabilistic rainfall nowcasting, improving over existing methods.", "method": "This paper introduces a method for probabilistic rainfall nowcasting that uses a video projector (V-JEPA Vision Transformer) with a lightweight probabilistic head connected to a pre-trained satellite vision encoder (DINOv3-SAT493M). This architecture maps encoder tokens into a discrete empirical CDF over 4-hour accumulated rainfall, optimized end-to-end using CRPS.", "result": "The proposed model achieved a CRPS of 3.5102 on the Weather4Cast 2025 benchmark, representing approximately a 26% effectiveness gain against the best 3D-UNET model.", "conclusion": "The paper concludes that the proposed method provides a promising performance for probabilistic rainfall nowcasting, demonstrating significant improvement over current 3D-UNET baselines."}}
{"id": "2511.10665", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10665", "abs": "https://arxiv.org/abs/2511.10665", "authors": ["Cristina Pinneri", "Christos Louizos"], "title": "Guarding the Meaning: Self-Supervised Training for Semantic Robustness in Guard Models", "comment": null, "summary": "Guard models are a critical component of LLM safety, but their sensitivity to superficial linguistic variations remains a key vulnerability. We show that even meaning-preserving paraphrases can cause large fluctuations in safety scores, revealing a lack of semantic grounding. To address this, we introduce a practical, self-supervised framework for improving the semantic robustness of guard models. Our method leverages paraphrase sets to enforce prediction consistency using a novel, skew-aware aggregation strategy for robust target computation. Notably, we find that standard aggregation methods like mean and median can degrade safety, underscoring the need for skew-aware alternatives. We analyze six open-source guard models and show that our approach reduces semantic variability across paraphrases by ~58%, improves benchmark accuracy by ~2.5% on average, and generalizes to unseen stylistic variations. Intriguingly, we discover a bidirectional relationship between model calibration and consistency: our robustness training improves calibration by up to 40%, revealing a fundamental connection between these properties. These results highlight the value of treating semantic consistency as a first-class training objective and provide a scalable recipe for building more reliable guard models.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.10905", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10905", "abs": "https://arxiv.org/abs/2511.10905", "authors": ["Hyun-Ki Jung"], "title": "YOLO-Drone: An Efficient Object Detection Approach Using the GhostHead Network for Drone Images", "comment": "Preprint version. Accepted for publication in the Journal of Information Systems Engineering and Management", "summary": "Object detection using images or videos captured by drones is a promising technology with significant potential across various industries. However, a major challenge is that drone images are typically taken from high altitudes, making object identification difficult. This paper proposes an effective solution to address this issue. The base model used in the experiments is YOLOv11, the latest object detection model, with a specific implementation based on YOLOv11n. The experimental data were sourced from the widely used and reliable VisDrone dataset, a standard benchmark in drone-based object detection. This paper introduces an enhancement to the Head network of the YOLOv11 algorithm, called the GhostHead Network. The model incorporating this improvement is named YOLO-Drone. Experimental results demonstrate that YOLO-Drone achieves significant improvements in key detection accuracy metrics, including Precision, Recall, F1-Score, and mAP (0.5), compared to the original YOLOv11. Specifically, the proposed model recorded a 0.4% increase in Precision, a 0.6% increase in Recall, a 0.5% increase in F1-Score, and a 0.5% increase in mAP (0.5). Additionally, the Inference Speed metric, which measures image processing speed, also showed a notable improvement. These results indicate that YOLO-Drone is a high-performance model with enhanced accuracy and speed compared to YOLOv11. To further validate its reliability, comparative experiments were conducted against other high-performance object detection models, including YOLOv8, YOLOv9, and YOLOv10. The results confirmed that the proposed model outperformed YOLOv8 by 0.1% in mAP (0.5) and surpassed YOLOv9 and YOLOv10 by 0.3% and 0.6%, respectively.", "AI": {"tldr": "This paper enhances the YOLOv11 model by introducing the GhostHead Network to improve object detection from drone images. The resulting YOLO-Drone model shows significant advancements in various performance metrics compared to the base model and other models.", "motivation": "The goal is to address the challenge of high-altitude drone images that make object identification difficult, by proposing an effective enhancement to the YOLOv11 algorithm.", "method": "The paper introduces an enhanced Head network, termed GhostHead Network, in the YOLOv11 algorithm. The improved model, named YOLO-Drone, employs the YOLOv11n implementation and tests its performance using the VisDrone dataset.", "result": "YOLO-Drone demonstrates significant improvements in detection accuracy metrics such as Precision, Recall, F1-Score, and mAP (0.5), surpassing the original YOLOv11. Additionally, it shows an improvement in Inference Speed. Comparative experiments against other high-performance models, such as YOLOv8, YOLOv9, and YOLOv10, further confirmed its superior performance.", "conclusion": "The study concludes that the YOLO-Drone model, with its GhostHead Network enhancement, offers improved performance in object detection accuracy and speed compared to both the base YOLOv11 model and other competing models."}}
{"id": "2511.10667", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10667", "abs": "https://arxiv.org/abs/2511.10667", "authors": ["Sichao Li", "Xinyue Xu", "Xiaomeng Li"], "title": "Evaluating LLM Understanding via Structured Tabular Decision Simulations", "comment": null, "summary": "Large language models (LLMs) often achieve impressive predictive accuracy, yet correctness alone does not imply genuine understanding. True LLM understanding, analogous to human expertise, requires making consistent, well-founded decisions across multiple instances and diverse domains, relying on relevant and domain-grounded decision factors. We introduce Structured Tabular Decision Simulations (STaDS), a suite of expert-like decision settings that evaluate LLMs as if they were professionals undertaking structured decision ``exams''. In this context, understanding is defined as the ability to identify and rely on the correct decision factors, features that determine outcomes within a domain. STaDS jointly assesses understanding through: (i) question and instruction comprehension, (ii) knowledge-based prediction, and (iii) reliance on relevant decision factors. By analyzing 9 frontier LLMs across 15 diverse decision settings, we find that (a) most models struggle to achieve consistently strong accuracy across diverse domains; (b) models can be accurate yet globally unfaithful, and there are frequent mismatches between stated rationales and factors driving predictions. Our findings highlight the need for global-level understanding evaluation protocols and advocate for novel frameworks that go beyond accuracy to enhance LLMs' understanding ability.", "AI": {"tldr": "研究提出了STaDS用以评估LLMs的理解能力，发现大部分模型很难在不同领域达到一致的准确性，需要制定更严格的评估标准来提升模型的理解能力。", "motivation": "大型语言模型虽然能够在预测准确性上取得显著成绩，但这份准确性并不能代表真正的理解。为了更准确地评估LLMs的理解能力，本研究开发了STaDS。", "method": "提出了Structured Tabular Decision Simulations (STaDS)，评估LLMs在多个专业决策场景中的表现，以此衡量其理解能力。STaDS通过以下三个方面的评估来定义理解：(i) 问题和指令理解力，(ii) 基于知识的预测能力，(iii) 依靠相关决策因素的能力。", "result": "通过对9种前沿LLMs在15个不同决策环境中的评估，发现模型的准确性并不能保证其一致性，很多时候其解释并不能完全反映实际驱动预测的因素。", "conclusion": "研究结果表明，需要更大的全局理解评估协议，并提倡使用新的框架，超越准确性的范畴，以更好地提升LLMs的理解能力。"}}
{"id": "2511.10914", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10914", "abs": "https://arxiv.org/abs/2511.10914", "authors": ["Zihan Gu", "Ruoyu Chen", "Junchi Zhang", "Yue Hu", "Hua Zhang", "Xiaochun Cao"], "title": "PhaseWin Search Framework Enable Efficient Object-Level Interpretation", "comment": null, "summary": "Attribution is essential for interpreting object-level foundation models. Recent methods based on submodular subset selection have achieved high faithfulness, but their efficiency limitations hinder practical deployment in real-world scenarios. To address this, we propose PhaseWin, a novel phase-window search algorithm that enables faithful region attribution with near-linear complexity. PhaseWin replaces traditional quadratic-cost greedy selection with a phased coarse-to-fine search, combining adaptive pruning, windowed fine-grained selection, and dynamic supervision mechanisms to closely approximate greedy behavior while dramatically reducing model evaluations. Theoretically, PhaseWin retains near-greedy approximation guarantees under mild monotone submodular assumptions. Empirically, PhaseWin achieves over 95% of greedy attribution faithfulness using only 20% of the computational budget, and consistently outperforms other attribution baselines across object detection and visual grounding tasks with Grounding DINO and Florence-2. PhaseWin establishes a new state of the art in scalable, high-faithfulness attribution for object-level multimodal models.", "AI": {"tldr": "本文提出PhaseWin算法，它是一种分阶段粗细搜索策略，能够在保持高忠实度的同时显著减少计算成本。", "motivation": "现有的基于次模子集选择的方法虽然具有较高的忠实度，但效率低下限制了其在实际场景中的应用。为了提高效率，同时保持属性分配的忠实度，提出了PhaseWin算法。", "method": "PhaseWin采用分阶段粗细搜索策略，通过自适应剪枝、窗口细化选择和动态监督机制，以较低的计算成本近似贪婪选择的行为。", "result": "PhaseWin在Grounding DINO和Florence-2等模型上的实验结果表明，它能够以仅20%的计算预算达到贪婪选择95%以上的忠实度，性能优于其他基准方法。", "conclusion": "PhaseWin为对象级别的多模态模型提供了高效且高忠实度的属性分配方法，达到了新的状态。"}}
{"id": "2511.10669", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10669", "abs": "https://arxiv.org/abs/2511.10669", "authors": ["Yanlin Wang", "Di Yuan", "Shani Dettman", "Dawn Choo", "Emily Shimeng Xu", "Denise Thomas", "Maura E Ryan", "Patrick C M Wong", "Nancy M Young"], "title": "Forecasting Spoken Language Development in Children with Cochlear Implants Using Preimplantation MRI", "comment": "38 pages", "summary": "Cochlear implants (CI) significantly improve spoken language in children with severe-to-profound sensorineural hearing loss (SNHL), yet outcomes remain more variable than in children with normal hearing. This variability cannot be reliably predicted for individual children using age at implantation or residual hearing. This study aims to compare the accuracy of traditional machine learning (ML) to deep transfer learning (DTL) algorithms to predict post-CI spoken language development of children with bilateral SNHL using a binary classification model of high versus low language improvers. A total of 278 implanted children enrolled from three centers. The accuracy, sensitivity and specificity of prediction models based upon brain neuroanatomic features using traditional ML and DTL learning. DTL prediction models using bilinear attention-based fusion strategy achieved: accuracy of 92.39% (95% CI, 90.70%-94.07%), sensitivity of 91.22% (95% CI, 89.98%-92.47%), specificity of 93.56% (95% CI, 90.91%-96.21%), and area under the curve (AUC) of 0.977 (95% CI, 0.969-0.986). DTL outperformed traditional ML models in all outcome measures. DTL was significantly improved by direct capture of discriminative and task-specific information that are advantages of representation learning enabled by this approach over ML. The results support the feasibility of a single DTL prediction model for language prediction of children served by CI programs worldwide.", "AI": {"tldr": "该研究利用深度转移学习算法提高了对人工耳蜗植入手术后儿童语言发展预测的准确性。", "motivation": "研究旨在比较传统的机器学习和深度转移学习算法在预测接受人工耳蜗植入手术的儿童术后口头语言发展方面的准确性。", "method": "使用传统的机器学习和深度转移学习算法，通过大脑神经解剖特征，对双侧感音神经性听力损失儿童的术后听力改善情况进行高改善者和低改善者的二元分类模型预测。", "result": "采用双线性注意力融合策略的深度转移学习预测模型实现了92.39%的准确率，91.22%的敏感性，93.56%的特异性和0.977的AUC。", "conclusion": "结果表明深度转移学习优于传统机器学习模型，并且支持了单一模型在全球CI项目中预测儿童语言能力的可行性。"}}
{"id": "2511.10923", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10923", "abs": "https://arxiv.org/abs/2511.10923", "authors": ["Zhixia He", "Chen Zhao", "Minglai Shao", "Xintao Wu", "Xujiang Zhao", "Dong Li", "Qin Tian", "Linlin Yu"], "title": "Out-of-Distribution Detection with Positive and Negative Prompt Supervision Using Large Language Models", "comment": null, "summary": "Out-of-distribution (OOD) detection is committed to delineating the classification boundaries between in-distribution (ID) and OOD images. Recent advances in vision-language models (VLMs) have demonstrated remarkable OOD detection performance by integrating both visual and textual modalities. In this context, negative prompts are introduced to emphasize the dissimilarity between image features and prompt content. However, these prompts often include a broad range of non-ID features, which may result in suboptimal outcomes due to the capture of overlapping or misleading information. To address this issue, we propose Positive and Negative Prompt Supervision, which encourages negative prompts to capture inter-class features and transfers this semantic knowledge to the visual modality to enhance OOD detection performance. Our method begins with class-specific positive and negative prompts initialized by large language models (LLMs). These prompts are subsequently optimized, with positive prompts focusing on features within each class, while negative prompts highlight features around category boundaries. Additionally, a graph-based architecture is employed to aggregate semantic-aware supervision from the optimized prompt representations and propagate it to the visual branch, thereby enhancing the performance of the energy-based OOD detector. Extensive experiments on two benchmarks, CIFAR-100 and ImageNet-1K, across eight OOD datasets and five different LLMs, demonstrate that our method outperforms state-of-the-art baselines.", "AI": {"tldr": "This paper introduces Positive and Negative Prompt Supervision for enhancing out-of-distribution detection by optimizing class-specific prompts and employing a graph-based architecture to propagate semantic-aware supervision.", "motivation": "The motivation behind this paper is to improve the performance of out-of-distribution (OOD) detection. Traditional methods using negative prompts often capture non-ID features that might overlap, leading to suboptimal performance. To tackle this, the paper proposes a novel approach using positive and negative prompt supervision.", "method": "Our method begins with class-specific positive and negative prompts initialized by large language models (LLMs). These prompts are optimized, with positive prompts focusing on intra-class features, while negative prompts highlight inter-class features around category boundaries. A graph-based architecture is used to gather and propagate semantic-aware supervision from the prompts to the visual branch.", "result": "The experiments across eight OOD datasets and five different LLMs, on CIFAR-100 and ImageNet-1K benchmarks, show that the proposed method outperforms existing OOD detection approaches.", "conclusion": "The paper concludes that the proposed Positive and Negative Prompt Supervision method achieves superior performance in OOD detection compared to state-of-the-art baselines, as demonstrated through extensive experiments on multiple benchmarks."}}
{"id": "2511.10670", "categories": ["cs.CL", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.10670", "abs": "https://arxiv.org/abs/2511.10670", "authors": ["Yan Gao", "Yazheng Yang", "Zhibin Lan", "Yidong Chen", "Min Zhang", "Daimeng Wei", "Hui Huang", "Jinsong Su"], "title": "Towards Fine-Grained Code-Switch Speech Translation with Semantic Space Alignment", "comment": "Working in progress", "summary": "Code-switching (CS) speech translation (ST) refers to translating speech that alternates between two or more languages into a target language text, which poses significant challenges due to the complexity of semantic modeling and the scarcity of CS data. Previous studies tend to rely on the model itself to implicitly learn semantic modeling during training, and resort to inefficient and costly manual annotations for these two challenges. To mitigate these limitations, we propose enhancing Large Language Models (LLMs) with a Mixture of Experts (MoE) speech projector, where each expert specializes in the semantic subspace of a specific language, enabling fine-grained modeling of speech features. Additionally, we introduce a multi-stage training paradigm that utilizes readily available monolingual automatic speech recognition (ASR) and monolingual ST data, facilitating speech-text alignment and improving translation capabilities. During training, we leverage a combination of language-specific loss and intra-group load balancing loss to guide the MoE speech projector in efficiently allocating tokens to the appropriate experts, across expert groups and within each group, respectively. To bridge the data gap across different training stages and improve adaptation to the CS scenario, we further employ a transition loss, enabling smooth transitions of data between stages, to effectively address the scarcity of high-quality CS speech translation data. Extensive experiments on widely used datasets demonstrate the effectiveness and generality of our approach.", "AI": {"tldr": "本研究提出了一个多专家系统（MoE）增强的大型语言模型，用于处理多语言切换语音翻译问题。研究采用多阶段训练范式并结合不同的损失函数来有效地完成训练，并通过引入过渡损失来应对数据稀缺的问题，实验结果证明了方法的有效性和泛化能力。", "motivation": "CS-ST面临的挑战主要是语义建模的复杂性和缺乏CS数据。以前的研究依赖模型在训练过程中隐式学习语义建模，并采用成本高昂的手动注释来应对这两个挑战。本研究旨在通过提出更有效的建模方法来解决这些问题。", "method": "本研究提出了一种结合混合专家系统（MoE）语音投影器的大型语言模型（LLM）来处理多语言切换语音翻译（CS-ST）问题。其中，每个专家专门处理特定语言的语义子空间，以实现对语音特征的精细建模。研究还引入了一个多阶段训练范式，利用现成的单语自动语音识别（ASR）和单语ST数据来促进语音-文本对齐并提高翻译能力。在训练过程中，通过语言特定损失和组内负载平衡损失的结合来指导MoE语音投影器有效地将标记分配给合适的专家，以促进跨组和组内的数据流动。为了缩小不同训练阶段之间数据差距，改进CS场景的适应性，研究进一步采用了过渡损失，以便在阶段间平滑地过渡数据，从而有效解决高质量CS语音翻译数据不足的问题。", "result": "通过广泛的数据集验证了所提方法的有效性和泛化能力，证明了其在CS-ST任务中的价值。", "conclusion": "该研究展示了一种新的方法来解决多语言切换语音翻译中的问题，该方法利用混合专家系统来增强大型语言模型，并采取了多项措施来解决数据不足和语义建模复杂的问题，有望推动该领域的进一步发展。"}}
{"id": "2511.10940", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10940", "abs": "https://arxiv.org/abs/2511.10940", "authors": ["Umma Aymon", "Nur Shazwani Kamarudin", "Ahmad Fakhri Ab. Nasir"], "title": "Facial Expression Recognition with YOLOv11 and YOLOv12: A Comparative Study", "comment": "IEEE Conference Proceedings for the 2025 IEEE 9th International Conference on Software Engineering & Computer Systems (ICSECS)", "summary": "Facial Expression Recognition remains a challenging task, especially in unconstrained, real-world environments. This study investigates the performance of two lightweight models, YOLOv11n and YOLOv12n, which are the nano variants of the latest official YOLO series, within a unified detection and classification framework for FER. Two benchmark classification datasets, FER2013 and KDEF, are converted into object detection format and model performance is evaluated using mAP 0.5, precision, recall, and confusion matrices. Results show that YOLOv12n achieves the highest overall performance on the clean KDEF dataset with a mAP 0.5 of 95.6, and also outperforms YOLOv11n on the FER2013 dataset in terms of mAP 63.8, reflecting stronger sensitivity to varied expressions. In contrast, YOLOv11n demonstrates higher precision 65.2 on FER2013, indicating fewer false positives and better reliability in noisy, real-world conditions. On FER2013, both models show more confusion between visually similar expressions, while clearer class separation is observed on the cleaner KDEF dataset. These findings underscore the trade-off between sensitivity and precision, illustrating how lightweight YOLO models can effectively balance performance and efficiency. The results demonstrate adaptability across both controlled and real-world conditions, establishing these models as strong candidates for real-time, resource-constrained emotion-aware AI applications.", "AI": {"tldr": "研究对比了YOLOv11n和YOLOv12n在面部表情识别中的性能，表明轻量级YOLO模型能有效平衡性能和效率，适合作为实时情感感知AI应用的候选模型。", "motivation": "研究面部表情识别在非约束、真实环境中的表现，探讨轻量级YOLO模型在面部表情识别任务中的适用性和性能。", "method": "研究在统一检测和分类框架下对两种轻量级模型YOLOv11n和YOLOv12n在面部表情识别中的表现进行研究。将FER2013和KDEF两个基准分类数据集转换为对象检测格式。模型性能通过mAP 0.5、精确度、召回率和混淆矩阵进行评估。", "result": "实验结果显示，YOLOv12n在清洁的KDEF数据集上取得了最高的整体性能，mAP 0.5为95.6，并且在FER2013数据集上较YOLOv11n表现更好，mAP 63.8，显示出对表情变化的更高敏感度。YOLOv11n在FER2013上的精度更高（65.2），表明其在嘈杂的真实环境中有更少的误报和更好的可靠性。", "conclusion": "研究指出这些轻量级YOLO模型能够在性能与效率之间实现良好的平衡，展示出适应控制和现实环境的能力，为实时、资源受限的情感感知AI应用奠定了基础。"}}
{"id": "2511.10671", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10671", "abs": "https://arxiv.org/abs/2511.10671", "authors": ["Filippo Morbiato", "Luca Romano", "Alessandro Persona"], "title": "Grounded Visual Factualization: Factual Anchor-Based Finetuning for Enhancing MLLM Factual Consistency", "comment": null, "summary": "Visual hallucination, where Multimodal Large Language Models fabricate details inconsistent with image content, critically undermines their reliability. Existing fine-tuning methods offer limited improvement, failing to deeply intervene in factual reasoning. This paper introduces Grounded Visual Factualization (GVF) Finetuning, a novel approach to systematically enhance MLLM visual factual consistency. GVF integrates explicit factual signals via three core mechanisms: Factual Anchor Data Augmentation, enriching training data with structured factual anchors and counter-factual prompts; Fact-Aware Instruction Tuning, embedding these cues into explicit instructions; and a Factual Consistency Loss function, specifically penalizing factual inaccuracies. Evaluated on LLaVA-1.5-13B, GVF Finetuning significantly outperforms standard fine-tuning on the VHTest benchmark for both Open-Ended Question (OEQ) and Yes/No Question (YNQ) formats. Crucially, GVF maintains or even slightly improves performance on general multimodal benchmarks like MME and POPE, demonstrating effective mitigation of visual hallucinations without compromising general understanding and reasoning abilities.", "AI": {"tldr": "Introduces Grounded Visual Factualization (GVF) Fine-tuning to improve MLLMs visual factual consistency, showing better results compared to standard methods on visual hallucination benchmarks.", "motivation": "To address the issue of visual hallucination in Multimodal Large Language Models and improve their factual reasoning and reliability.", "method": "GVF Fine-tuning enhancing MLLM visual factual consistency through Factual Anchor Data Augmentation, Fact-Aware Instruction Tuning, and Factual Consistency Loss function.", "result": "Significantly outperformed standard fine-tuning on the VHTest benchmark for both OEQ and YNQ formats.", "conclusion": "GVF Fine-tuning method effectively mitigates visual hallucinations while maintaining or slightly improving performance on general multimodal benchmarks."}}
{"id": "2511.10942", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10942", "abs": "https://arxiv.org/abs/2511.10942", "authors": ["Liuchi Xu", "Hao Zheng", "Lu Wang", "Lisheng Xu", "Jun Cheng"], "title": "Heterogeneous Complementary Distillation", "comment": "Accepted by AAAI2026", "summary": "Knowledge distillation (KD)transfers the dark knowledge from a complex teacher to a compact student. However, heterogeneous architecture distillation, such as Vision Transformer (ViT) to ResNet18, faces challenges due to differences in spatial feature representations.Traditional KD methods are mostly designed for homogeneous architectures and hence struggle to effectively address the disparity. Although heterogeneous KD approaches have been developed recently to solve these issues, they often incur high computational costs and complex designs, or overly rely on logit alignment, which limits their ability to leverage the complementary features. To overcome these limitations, we propose Heterogeneous Complementary Distillation (HCD),a simple yet effective framework that integrates complementary teacher and student features to align representations in shared logits.These logits are decomposed and constrained to facilitate diverse knowledge transfer to the student. Specifically, HCD processes the student's intermediate features through convolutional projector and adaptive pooling, concatenates them with teacher's feature from the penultimate layer and then maps them via the Complementary Feature Mapper (CFM) module, comprising fully connected layer,to produce shared logits.We further introduce Sub-logit Decoupled Distillation (SDD) that partitions the shared logits into n sub-logits, which are fused with teacher's logits to rectify classification.To ensure sub-logit diversity and reduce redundant knowledge transfer, we propose an Orthogonality Loss (OL).By preserving student-specific strengths and leveraging teacher knowledge,HCD enhances robustness and generalization in students.Extensive experiments on the CIFAR-100, Fine-grained (e.g., CUB200)and ImageNet-1K datasets demonstrate that HCD outperforms state-of-the-art KD methods,establishing it as an effective solution for heterogeneous KD.", "AI": {"tldr": "提出了一种有效的异构互补蒸馏框架（HCD），改善了异构架构知识蒸馏的效果，并在多个数据集上超越了现有的知识蒸馏方法。", "motivation": "异构架构蒸馏（如Vision Transformer到ResNet18）面临空间特征表示差异的挑战。传统蒸馏方法主要设计用于同构架构，难以有效处理差异。虽然最近开发了一些异构蒸馏方法，但它们往往计算成本高且设计复杂，或者过于依赖分类器输出对齐，限制了其利用互补特征的能力。", "method": "Heterogeneous Complementary Distillation (HCD) 方法，通过集成互补的教师和学生特征来对共享的分类器输出进行对齐，具体包括使用卷积投影器和自适应池处理学生中级特征，将其与教师最后一层的特征连接，并通过互补特征映射器模块（CFM，包含全连接层）生成共享分类器输出。此外，引入了子分类器输出分离蒸馏（SDD），将共享分类器输出划分为n个子分类器输出，并通过正交损失（OL）保证子分类器多样性，减少冗余知识传输。", "result": "提出的异构互补蒸馏（HCD）框架能够保持学生模型的独特优势同时利用教师模型的知识，从而增强了学生模型的鲁棒性和泛化能力。在CIFAR-100、细粒度（如CUB200）和ImageNet-1K等数据集上的实验表明，HCD优于最先进的知识蒸馏方法。", "conclusion": "HCD方法以其简单且有效的方式解决了传统异构知识蒸馏方法在计算复杂度和构建复杂性上的不足，能够在不增加过多复杂性的情况下，提升知识蒸馏的效果，尤其是在异构架构之间的知识转移上表现出色。"}}
{"id": "2511.10673", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.10673", "abs": "https://arxiv.org/abs/2511.10673", "authors": ["Fengxu Yang", "Weitong Chen", "Jack D. Evans"], "title": "Large language models in materials science and the need for open-source approaches", "comment": "16 pages, 5 figures", "summary": "Large language models (LLMs) are rapidly transforming materials science. This review examines recent LLM applications across the materials discovery pipeline, focusing on three key areas: mining scientific literature , predictive modelling, and multi-agent experimental systems. We highlight how LLMs extract valuable information such as synthesis conditions from text, learn structure-property relationships, and can coordinate agentic systems integrating computational tools and laboratory automation. While progress has been largely dependent on closed-source commercial models, our benchmark results demonstrate that open-source alternatives can match performance while offering greater transparency, reproducibility, cost-effectiveness, and data privacy. As open-source models continue to improve, we advocate their broader adoption to build accessible, flexible, and community-driven AI platforms for scientific discovery.", "AI": {"tldr": "大型语言模型(LLMs)在材料科学领域迅速发展，本篇综述主要聚焦于三方面：文献挖掘、预测建模和多代理实验系统。结果表明开源替代模型可以匹敌闭源商业模型的性能，并且在透明度、可重复性、成本效益和数据隐私上有优势，倡导更广泛地使用开源模型建立可访问、灵活、社区驱动的AI科学发现平台。", "motivation": "探讨大型语言模型在材料科学各个发现领域的应用，促进对开源模型的使用，构建更加透明、可访问的AI平台。", "method": "通过评估开源模型在材料科学各领域的应用表现，与已有的闭源商业模型进行比较。", "result": "发现开源模型不仅性能匹敌闭源模型，在透明度、可重复性、成本效益和数据隐私方面更具优势。", "conclusion": "提倡更广泛地采用开源模型来构建社区驱动、灵活且透明的AI平台，以推动材料科学的创新。"}}
{"id": "2511.10945", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10945", "abs": "https://arxiv.org/abs/2511.10945", "authors": ["Xingyue Zhao", "Wenke Huang", "Xingguang Wang", "Haoyu Zhao", "Linghao Zhuang", "Anwen Jiang", "Guancheng Wan", "Mang Ye"], "title": "Divide, Conquer and Unite: Hierarchical Style-Recalibrated Prototype Alignment for Federated Medical Image Segmentation", "comment": "Accepted at AAAI-26", "summary": "Federated learning enables multiple medical institutions to train a global model without sharing data, yet feature heterogeneity from diverse scanners or protocols remains a major challenge. Many existing works attempt to address this issue by leveraging model representations (e.g., mean feature vectors) to correct local training; however, they often face two key limitations: 1) Incomplete Contextual Representation Learning: Current approaches primarily focus on final-layer features, overlooking critical multi-level cues and thus diluting essential context for accurate segmentation. 2) Layerwise Style Bias Accumulation: Although utilizing representations can partially align global features, these methods neglect domain-specific biases within intermediate layers, allowing style discrepancies to build up and reduce model robustness. To address these challenges, we propose FedBCS to bridge feature representation gaps via domain-invariant contextual prototypes alignment. Specifically, we introduce a frequency-domain adaptive style recalibration into prototype construction that not only decouples content-style representations but also learns optimal style parameters, enabling more robust domain-invariant prototypes. Furthermore, we design a context-aware dual-level prototype alignment method that extracts domain-invariant prototypes from different layers of both encoder and decoder and fuses them with contextual information for finer-grained representation alignment. Extensive experiments on two public datasets demonstrate that our method exhibits remarkable performance.", "AI": {"tldr": "FedBCS is introduced to improve federated learning in medical imaging by aligning domain-invariant prototypes, effectively handling feature heterogeneity and style biases.", "motivation": "The motivation behind the paper is to address the challenges of incomplete contextual representation learning and layerwise style bias accumulation in federated learning across diverse medical settings, without sharing sensitive patient data.", "method": "The paper proposes FedBCS, a method that aims to solve feature heterogeneity in federated learning by introducing a frequency-domain adaptive style recalibration into prototype construction, and a context-aware dual-level prototype alignment method that extracts domain-invariant prototypes from different layers.", "result": "The method shows remarkable performance on two public datasets, exhibiting improved model robustness and accuracy.", "conclusion": "FedBCS addresses limitations of previous approaches and provides a more robust way to train models in federated learning environments, especially in medical imaging, by effectively aligning domain-invariant prototypes."}}
{"id": "2511.10674", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.10674", "abs": "https://arxiv.org/abs/2511.10674", "authors": ["Thomas Cook", "Kelly Patel", "Sivapriya Vellaichamy", "Saba Rahimi", "Zhen Zeng", "Sumitra Ganesh"], "title": "Continual Learning of Domain Knowledge from Human Feedback in Text-to-SQL", "comment": "34 pages, 6 figures, 4 tables", "summary": "Large Language Models (LLMs) can generate SQL queries from natural language questions but struggle with database-specific schemas and tacit domain knowledge. We introduce a framework for continual learning from human feedback in text-to-SQL, where a learning agent receives natural language feedback to refine queries and distills the revealed knowledge for reuse on future tasks. This distilled knowledge is stored in a structured memory, enabling the agent to improve execution accuracy over time. We design and evaluate multiple variations of a learning agent architecture that vary in how they capture and retrieve past experiences. Experiments on the BIRD benchmark Dev set show that memory-augmented agents, particularly the Procedural Agent, achieve significant accuracy gains and error reduction by leveraging human-in-the-loop feedback. Our results highlight the importance of transforming tacit human expertise into reusable knowledge, paving the way for more adaptive, domain-aware text-to-SQL systems that continually learn from a human-in-the-loop.", "AI": {"tldr": "研究展示了一种文本到SQL查询生成的持续学习框架，通过人类反馈学习和知识的记忆重用来提升准确性，特别是在处理数据库特定模式和领域知识方面。", "motivation": "大型语言模型（LLMs）虽然能够生成SQL查询，但在处理数据库特定模式和领域隐蔽知识方面仍存在问题。通过引入这种持续学习框架，旨在提高在这些方面的能力。", "method": "提出了一种通过持续从人类反馈学习来改进文本到SQL查询生成的框架。该框架允许学习代理通过自然语言反馈来调整SQL查询，并把获得的知识存储在一个结构化的记忆中，以便在未来任务中重用。", "result": "实验结果显示，使用此框架的记忆增强代理，尤其是在使用过程代理时，在BIRD基准测试开发集上的执行准确性有显著提高，错误率降低。", "conclusion": "研究表明，将隐性的人类专业知识转化为可重用知识是提升日常生活任务的准确性的关键因素。这为构建更加适应性和领域意识强的文本到SQL系统奠定了基础，这种系统可以从人类反馈中不断学习和进步。"}}
{"id": "2511.10946", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10946", "abs": "https://arxiv.org/abs/2511.10946", "authors": ["Yifan Liu", "Fangneng Zhan", "Kaichen Zhou", "Yilun Du", "Paul Pu Liang", "Hanspeter Pfister"], "title": "Abstract 3D Perception for Spatial Intelligence in Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) struggle with 3D-related tasks such as spatial cognition and physical understanding, which are crucial for real-world applications like robotics and embodied agents. We attribute this to a modality gap between the 3D tasks and the 2D training of VLM, which led to inefficient retrieval of 3D information from 2D input. To bridge this gap, we introduce SandboxVLM, a simple yet effective framework that leverages abstract bounding boxes to encode geometric structure and physical kinematics for VLM. Specifically, we design a 3D Sandbox reconstruction and perception pipeline comprising four stages: generating multi-view priors with abstract control, proxy elevation, multi-view voting and clustering, and 3D-aware reasoning. Evaluated in zero-shot settings across multiple benchmarks and VLM backbones, our approach consistently improves spatial intelligence, achieving an 8.3\\% gain on SAT Real compared with baseline methods for instance. These results demonstrate that equipping VLMs with a 3D abstraction substantially enhances their 3D reasoning ability without additional training, suggesting new possibilities for general-purpose embodied intelligence.", "AI": {"tldr": "SandboxVLM 使用抽象边界框来增强视觉语言模型（VLM）处理3D任务的能力，通过四阶段的3D沙箱重建和感知管道，显著提升了这些模型的空间智能。", "motivation": "视觉语言模型（VLMs）在处理3D相关任务上有困难，如空间认知和物理理解，这对机器人和具身代理等现实应用至关重要。本文旨在解决2D输入和3D任务之间的模式差距。", "method": "Structure", "result": "{\n  \"tldr\": \"SandboxVLM 使用抽象边界框来增强视觉语言模型（VLM）处理3D任务的能力，如空间认知和物理理解，通过四阶段的3D沙箱重建和感知管道，显著提升了这些模型的空间智能。\", \n  \"motivation\": \"视觉语言模型（VLMs）在处理3D相关任务上有困难，如空间认知和物理理解，这对机器人和具身代理等现实应用至关重要。本文旨在解决2D输入和3D任务之间的模式差距。\", \n  \"method\": \"提出了一种称为SandboxVLM的框架，它使用抽象边界框来编码几何结构和物理动力学，包含生成多视图先验、代理提升、多视图投票与聚类、3D感知推理四阶段。\", \n  \"result\": \"在零样本设置下，SandboxVLM在多个基准测试中显示出一致的空间智能提升，在SAT Real上相较于基准方法提升了8.3%。\", \n  \"conclusion\": \"配备3D抽象结构可以大幅提高VLM的3D推理能力，为通用具身智能提供了新的可能性，而无需额外训练。\"}\n}", "conclusion": "配备3D抽象结构可以大幅提高VLM的3D推理能力，为通用具身智能提供了新的可能性，而无需额外训练。"}}
{"id": "2511.10675", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.10675", "abs": "https://arxiv.org/abs/2511.10675", "authors": ["Ye Jiang", "Taihang Wang", "Youzheng Liu", "Yimin Wang", "Yuhan Xia", "Yunfei Long"], "title": "Learn to Select: Exploring Label Distribution Divergence for In-Context Demonstration Selection in Text Classification", "comment": null, "summary": "In-context learning (ICL) for text classification, which uses a few input-label demonstrations to describe a task, has demonstrated impressive performance on large language models (LLMs). However, the selection of in-context demonstrations plays a crucial role and can significantly affect LLMs' performance. Most existing demonstration selection methods primarily focus on semantic similarity between test inputs and demonstrations, often overlooking the importance of label distribution alignment. To address this limitation, we propose a two-stage demonstration selection method, TopK + Label Distribution Divergence (L2D), which leverages a fine-tuned BERT-like small language model (SLM) to generate label distributions and calculate their divergence for both test inputs and candidate demonstrations. This enables the selection of demonstrations that are not only semantically similar but also aligned in label distribution with the test input. Extensive experiments across seven text classification benchmarks show that our method consistently outperforms previous demonstration selection strategies. Further analysis reveals a positive correlation between the performance of LLMs and the accuracy of the underlying SLMs used for label distribution estimation.", "AI": {"tldr": "提出了一种新的示范选择方法TopK + L2D，通过利用标签分布差异，提升了在文本分类中使用大语言模型时的性能表现。", "motivation": "现有的示范选择方法主要集中在测试输入与示范之间的语义相似性，往往忽略了标签分布对齐的重要性。为了弥补这一点并提升大语言模型（LLM）的性能，我们提出了这种方法。", "method": "我们提出了一种两阶段的示范选择方法，TopK + 标签分布差异（L2D），该方法利用微调后的类似BERT的小语言模型（SLM）生成标签分布并对测试输入和候选示范的分布差异进行计算。这使得所选示范不仅在语义上相似，而且在标签分布上与测试输入保持一致。", "result": "在七个文本分类基准上的广泛实验表明，我们提出的方法在之前的示范选择策略上始终表现出更优的效果。进一步的分析揭示了大语言模型的性能与用于标签分布估计的底层小语言模型的准确性之间存在正相关关系。", "conclusion": "本方法通过考虑标签分布的对齐性，成功提升了示范选择的质量，从而改善了大语言模型在遇到较少带标签数据情况下的表现。"}}
{"id": "2511.10948", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.10948", "abs": "https://arxiv.org/abs/2511.10948", "authors": ["Ren Zhang", "Huilai Li", "Chao qi", "Guoliang Xu", "Tianyu Zhou", "Wei wei", "Jianqin Yin"], "title": "DEFT-LLM: Disentangled Expert Feature Tuning for Micro-Expression Recognition", "comment": null, "summary": "Micro expression recognition (MER) is crucial for inferring genuine emotion. Applying a multimodal large language model (MLLM) to this task enables spatio-temporal analysis of facial motion and provides interpretable descriptions. However, there are still two core challenges: (1) The entanglement of static appearance and dynamic motion cues prevents the model from focusing on subtle motion; (2) Textual labels in existing MER datasets do not fully correspond to underlying facial muscle movements, creating a semantic gap between text supervision and physical motion. To address these issues, we propose DEFT-LLM, which achieves motion semantic alignment by multi-expert disentanglement. We first introduce Uni-MER, a motion-driven instruction dataset designed to align text with local facial motion. Its construction leverages dual constraints from optical flow and Action Unit (AU) labels to ensure spatio-temporal consistency and reasonable correspondence to the movements. We then design an architecture with three experts to decouple facial dynamics into independent and interpretable representations (structure, dynamic textures, and motion-semantics). By integrating the instruction-aligned knowledge from Uni-MER into DEFT-LLM, our method injects effective physical priors for micro expressions while also leveraging the cross modal reasoning ability of large language models, thus enabling precise capture of subtle emotional cues. Experiments on multiple challenging MER benchmarks demonstrate state-of-the-art performance, as well as a particular advantage in interpretable modeling of local facial motion.", "AI": {"tldr": "本文提出DEFT-LLM方法，通过多专家解耦和指令对齐的数据集改进微表情识别，实验表明该方法性能显著，并能有效解释局部面部运动。", "motivation": "微表情识别（MER）对于推断真实情感意义重大。将多模态大型语言模型应用于此任务可分析面部的时空运动并提供可解释的描述。然而，当前方法中存在两个核心挑战：静态外观和动态运动线索的纠缠使模型难以关注细微的运动，以及现有MER数据集中的文本标签不能完全对应底层的面部肌肉运动，从而导致文本监督与物理运动之间存在语义差距。", "method": "我们提出了一种名为DEFT-LLM的方法，它通过多专家解耦来实现动作语义对齐。首先介绍了Uni-MER，这是为了将文本与局部面部动作对齐而设计的运动导向指令数据集。它的构建综合利用了光流和动作单元（AU）标签的双重约束，以确保时空一致性和与运动的合理对应。我们设计了一个由三位专家组成的架构，将面部动态解耦为独立且可解释的表示（结构、动态纹理和动作语义）。通过将Uni-MER中的指令对齐知识整合到DEFT-LLM中，我们的方法注入了微妙表情的有效物理先验，同时利用大型语言模型的跨模态推理能力，从而能够准确捕捉微妙的情感线索。", "result": "在多个具有挑战性的微表情识别基准上进行了实验，结果展示了我们方法的最先进的性能，并在局部面部运动的可解释建模方面显示出显著的优势。", "conclusion": "我们的研究通过设计一个指令对齐的数据集和一个多专家架构来改进微表情识别。实验验证了DEFT-LLM方法在性能上的先进性和局部面部运动解释方面的优势。"}}
{"id": "2511.10676", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10676", "abs": "https://arxiv.org/abs/2511.10676", "authors": ["Shien Zhu", "Samuel Bohl", "Robin Oester", "Gustavo Alonso"], "title": "Pre-Attention Expert Prediction and Prefetching for Mixture-of-Experts Large Language Models", "comment": null, "summary": "Mixture-of-Experts (MoE) Large Language Models (LLMs) efficiently scale-up the model while keeping relatively low inference cost. As MoE models only activate part of the experts, related work has proposed expert prediction and caching methods to prefetch the experts for faster inference. However, existing approaches utilize the activations from the previous layer for prediction, incurring low accuracy and leave the first layer unoptimized. Applying complex layers or even training standalone networks for better prediction introduces high computation overhead. In this paper, we propose pre-attention expert prediction to achieve accurate and lightweight expert prefetching. The key insight is that some functions in LLMs are ranking-preserving, indicating that matching the ranking of selected experts using simple linear functions is possible. Therefore, we utilize the activations before the attention block in the same layer with 2 linear functions and ranking-aware loss to achieve accurate prediction, which also supports prefetching in the first layer. Our lightweight, pre-attention expert routers achieve 93.03% accuracy on DeepSeek V2 Lite, 94.69% on Qwen3-30B, and 97.62% on Phi-mini-MoE, showing about 15% improvement on absolute accuracy over the state-of-the-art methods.", "AI": {"tldr": "提出了一种基于预注意力的轻量级专家预测方法，显著提高了MoE大语言模型的预测精度。", "motivation": "现有的MoE模型的预测方法利用上一层的激活值进行预测，精度低且未优化首层。提出新方法为了解决这些不足，实现快速且准确的专家预测。", "method": "提出了一种预注意力专家预测方法，通过在同一层使用注意力模块前的激活值和两个线性函数结合排序感知损失函数来进行准确预测。", "result": "该方法在DeepSeek V2 Lite, Qwen3-30B, Phi-mini-MoE上分别达到了93.03%, 94.69%, 97.62%的精度，比现有方法提高了大约15%的绝对准确率。", "conclusion": "该方法通过使用注意力模块前的激活值和排序感知损失函数提高了MoE模型专家预测的准确率，且实现了首层的优化。"}}
{"id": "2511.10953", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10953", "abs": "https://arxiv.org/abs/2511.10953", "authors": ["Wenrui Li", "Wei Han", "Hengyu Man", "Wangmeng Zuo", "Xiaopeng Fan", "Yonghong Tian"], "title": "Language-Guided Graph Representation Learning for Video Summarization", "comment": "Accepted by IEEE TPAMI", "summary": "With the rapid growth of video content on social media, video summarization has become a crucial task in multimedia processing. However, existing methods face challenges in capturing global dependencies in video content and accommodating multimodal user customization. Moreover, temporal proximity between video frames does not always correspond to semantic proximity. To tackle these challenges, we propose a novel Language-guided Graph Representation Learning Network (LGRLN) for video summarization. Specifically, we introduce a video graph generator that converts video frames into a structured graph to preserve temporal order and contextual dependencies. By constructing forward, backward and undirected graphs, the video graph generator effectively preserves the sequentiality and contextual relationships of video content. We designed an intra-graph relational reasoning module with a dual-threshold graph convolution mechanism, which distinguishes semantically relevant frames from irrelevant ones between nodes. Additionally, our proposed language-guided cross-modal embedding module generates video summaries with specific textual descriptions. We model the summary generation output as a mixture of Bernoulli distribution and solve it with the EM algorithm. Experimental results show that our method outperforms existing approaches across multiple benchmarks. Moreover, we proposed LGRLN reduces inference time and model parameters by 87.8% and 91.7%, respectively. Our codes and pre-trained models are available at https://github.com/liwrui/LGRLN.", "AI": {"tldr": "This paper presents LGRLN, a novel video summarization method that converts video frames into graph structures and utilizes graph convolution to distinguish semantically relevant frames. It also incorporates textual guidance and outperforms existing methods, with a significant reduction in inference time and model parameters.", "motivation": "The motivation behind this paper is to address the challenges faced by existing video summarization methods, such as capturing global dependencies, accommodating multimodal user customization, and handling the issue that temporal closeness does not always equal semantic closeness within video frames.", "method": "The paper introduces a novel method called Language-guided Graph Representation Learning Network (LGRLN) for video summarization. This method includes a video graph generator for preserving temporal order and contextual dependencies, an intra-graph relational reasoning module for distinguishing semantically relevant frames, and a language-guided cross-modal embedding module for generating summaries with specific textual descriptions.", "result": "Experiments show that the proposed method improves performance across multiple benchmarks and reduces inference time and model parameters by 87.8% and 91.7%, respectively, compared to existing approaches.", "conclusion": "The conclusion of the paper asserts that LGRLN effectively addresses the noted challenges in video summarization, enhancing semantic relevance among frames and reducing resource requirements, while the availability of codes and models facilitates further research and application."}}
{"id": "2511.10684", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.10684", "abs": "https://arxiv.org/abs/2511.10684", "authors": ["Anupama Sitaraman", "Bharathan Balaji", "Yuvraj Agarwal"], "title": "SpiderGen: Towards Procedure Generation For Carbon Life Cycle Assessments with Generative AI", "comment": null, "summary": "Investigating the effects of climate change and global warming caused by GHG emissions have been a primary concern worldwide. These emissions are largely contributed to by the production, use and disposal of consumer products. Thus, it is important to build tools to estimate the environmental impact of consumer goods, an essential part of which is conducting Life Cycle Assessments (LCAs). LCAs specify and account for the appropriate processes involved with the production, use, and disposal of the products. We present SpiderGen, an LLM-based workflow which integrates the taxonomy and methodology of traditional LCA with the reasoning capabilities and world knowledge of LLMs to generate the procedural information used for LCA. We additionally evaluate the output of SpiderGen using real-world LCA documents as ground-truth. We find that SpiderGen provides accurate LCA process information that is either fully correct or has minor errors, achieving an F1-Score of 62% across 10 sample data points. We observe that the remaining missed processes and hallucinated errors occur primarily due to differences in detail between LCA documents, as well as differences in the \"scope\" of which auxiliary processes must also be included. We also demonstrate that SpiderGen performs better than several baselines techniques, such as chain-of-thought prompting and one-shot prompting. Finally, we highlight SpiderGen's potential to reduce the human effort and costs for estimating carbon impact, as it is able to produce LCA process information for less than \\$1 USD in under 10 minutes as compared to the status quo LCA, which can cost over \\$25000 USD and take up to 21-person days.", "AI": {"tldr": "SpiderGen, 基于LLM的工具，旨在为消费者产品的生命周期评估（LCA）生成程序信息，与实际LCA文档的对比中取得了62%的F1评分，显着优于其他技术，大大减少了成本和时间开销。", "motivation": "该论文的动机是解决传统的LCA过程中高昂的成本和时间问题，开发了一个使用高级AI技术（如SpiderGen）自动化的工具。", "method": "Structure", "result": "{\n  \"tldr\": \"SpiderGen, a tool based on LLM, is designed to generate procedural information for Life Cycle Assessments (LCAs) of consumer goods, achieving an F1-Score of 62% with real-world LCA documents as ground truth, and demonstrates better performance than other techniques, reducing costs and time significantly.\",\n  \"motivation\": \"The motivation behind this paper is to address the high cost and time involved in conducting traditional LCAs by developing a tool, SpiderGen, that can automate this process using advanced AI technology.\",\n  \"method\": \"SpiderGen integrates the traditional LCA taxonomy and methodology with the reasoning capabilities of LLM to generate appropriate procedural information, and its output is evaluated against real-world LCA documents.\",\n  \"result\": \"The paper reports an F1-Score of 62% for the accuracy of SpiderGen's generated LCA process information, highlighting that it outperforms several baseline techniques.\",\n  \"conclusion\": \"The conclusion is that SpiderGen has the potential to significantly reduce the human hours and financial costs associated with traditional LCAs, making the process more efficient and cost-effective.\")", "conclusion": "该论文得出结论，SpiderGen具有显著减少人力时间和经济成本的潜质，使LCA过程更高效和经济。"}}
{"id": "2511.10958", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10958", "abs": "https://arxiv.org/abs/2511.10958", "authors": ["Gunho Jung", "Heejo Kong", "Seong-Whan Lee"], "title": "Text-guided Weakly Supervised Framework for Dynamic Facial Expression Recognition", "comment": null, "summary": "Dynamic facial expression recognition (DFER) aims to identify emotional states by modeling the temporal changes in facial movements across video sequences. A key challenge in DFER is the many-to-one labeling problem, where a video composed of numerous frames is assigned a single emotion label. A common strategy to mitigate this issue is to formulate DFER as a Multiple Instance Learning (MIL) problem. However, MIL-based approaches inherently suffer from the visual diversity of emotional expressions and the complexity of temporal dynamics. To address this challenge, we propose TG-DFER, a text-guided weakly supervised framework that enhances MIL-based DFER by incorporating semantic guidance and coherent temporal modeling. We incorporate a vision-language pre-trained (VLP) model is integrated to provide semantic guidance through fine-grained textual descriptions of emotional context. Furthermore, we introduce visual prompts, which align enriched textual emotion labels with visual instance features, enabling fine-grained reasoning and frame-level relevance estimation. In addition, a multi-grained temporal network is designed to jointly capture short-term facial dynamics and long-range emotional flow, ensuring coherent affective understanding across time. Extensive results demonstrate that TG-DFER achieves improved generalization, interpretability, and temporal sensitivity under weak supervision.", "AI": {"tldr": "本文提出了一种新的动态面部表情识别框架TG-DFER，通过基于文本的指导和多粒度时间网络，提高了识别的泛化能力、可解释性和时间敏感性，并能在弱监督环境下取得良好的效果。", "motivation": "动态面部表情识别的主要挑战之一是多对一标签问题，其中包含许多帧的视频被分配一个单一的情感标签。现有的多实例学习方法在这种情况下表现出视觉多样性的情感表达和复杂的时态动态问题，因此作者提出了一种新的框架TG-DFER来解决这些问题。", "method": "本文提出了一种基于文本引导的弱监督框架TG-DFER，改进了基于多实例学习的动态面部表情识别方法。该框架融合了视觉语言预训练模型，以提供关于情感背景的细粒度文本描述的语义指导，并引入了视觉提示来提高细粒度推理和帧级相关性评估能力。此外，设计了一个多粒度时间网络，以捕获短期面部动态和长期情感流动，从而保证在时间上连贯的情感理解。", "result": "实验结果表明，TG-DFER在弱监督环境下，提高了识别的泛化能力、可解释性和时间敏感性。", "conclusion": "TG-DFER框架通过融合语义指导和连贯的时间建模，提高了动态面部表情识别的能力，特别是在弱监督条件下，展现了更好的泛化性能、更高的可解释性和时间敏感性。"}}
{"id": "2511.10686", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10686", "abs": "https://arxiv.org/abs/2511.10686", "authors": ["Tiago Machado", "Maysa Malfiza Garcia de Macedo", "Rogerio Abreu de Paula", "Marcelo Carpinette Grave", "Aminat Adebiyi", "Luan Soares de Souza", "Enrico Santarelli", "Claudio Pinhanez"], "title": "A methodological analysis of prompt perturbations and their effect on attack success rates", "comment": null, "summary": "This work aims to investigate how different Large Language Models (LLMs) alignment methods affect the models' responses to prompt attacks. We selected open source models based on the most common alignment methods, namely, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reinforcement Learning with Human Feedback (RLHF). We conducted a systematic analysis using statistical methods to verify how sensitive the Attack Success Rate (ASR) is when we apply variations to prompts designed to elicit inappropriate content from LLMs. Our results show that even small prompt modifications can significantly change the Attack Success Rate (ASR) according to the statistical tests we run, making the models more or less susceptible to types of attack. Critically, our results demonstrate that running existing 'attack benchmarks' alone may not be sufficient to elicit all possible vulnerabilities of both models and alignment methods. This paper thus contributes to ongoing efforts on model attack evaluation by means of systematic and statistically-based analyses of the different alignment methods and how sensitive their ASR is to prompt variation.", "AI": {"tldr": "本研究探讨了不同对齐方法如何影响大规模语言模型（LLMs）对提示攻击的响应。通过系统性统计分析，发现即使是微小的提示变化也会显著改变攻击成功率（ASR）。这表明仅运行现有攻击基准测试可能不足以识别模型的所有潜在脆弱性。", "motivation": "研究动机在于深入了解不同对齐方法（如监督微调（SFT）、直接偏好优化（DPO）、和带有人类反馈的强化学习（RLHF））如何影响模型对提示攻击的抵御能力。", "method": "Structure", "result": "研究结果显示，即使是微小的提示修改也能显著改变攻击成功率（ASR），展示了不同对齐方法的敏感性。", "conclusion": "结论指出，现有攻击基准测试方法可能不足以全面揭示模型及其对齐方法的所有可能漏洞，强调了进行基于统计的系统性分析的重要性。"}}
{"id": "2511.10971", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10971", "abs": "https://arxiv.org/abs/2511.10971", "authors": ["Anzhe Cheng", "Shukai Duan", "Shixuan Li", "Chenzhong Yin", "Mingxi Cheng", "Heng Ping", "Tamoghna Chattopadhyay", "Sophia I Thomopoulos", "Shahin Nazarian", "Paul Thompson", "Paul Bogdan"], "title": "ERMoE: Eigen-Reparameterized Mixture-of-Experts for Stable Routing and Interpretable Specialization", "comment": null, "summary": "Mixture-of-Experts (MoE) architectures expand model capacity by sparsely activating experts but face two core challenges: misalignment between router logits and each expert's internal structure leads to unstable routing and expert underutilization, and load imbalances create straggler bottlenecks. Standard solutions, such as auxiliary load-balancing losses, can reduce load disparities but often weaken expert specialization and hurt downstream performance. To address these issues, we propose ERMoE, a sparse MoE transformer that reparameterizes each expert in a learned orthonormal eigenbasis and replaces learned gating logits with an \"Eigenbasis Score\", defined as the cosine similarity between input features and an expert's basis. This content-aware routing ties token assignments directly to experts' representation spaces, stabilizing utilization and promoting interpretable specialization without sacrificing sparsity. Crucially, ERMoE removes the need for explicit balancing losses and avoids the interfering gradients they introduce. We show that ERMoE achieves state-of-the-art accuracy on ImageNet classification and cross-modal image-text retrieval benchmarks (e.g., COCO, Flickr30K), while naturally producing flatter expert load distributions. Moreover, a 3D MRI variant (ERMoE-ba) improves brain age prediction accuracy by more than 7\\% and yields anatomically interpretable expert specializations. ERMoE thus introduces a new architectural principle for sparse expert models that directly addresses routing instabilities and enables improved performance with scalable, interpretable specialization.", "AI": {"tldr": "ERMoE通过重新参数化专家并采用基得分来优化稀疏MoE模型的性能和负载平衡，取得显著成果。", "motivation": "解决MoE架构中的两个核心挑战：路由不稳定性和专家利用率不足，以及负载不平衡造成的瓶颈。", "method": "ERMoE，一种稀疏MoE变压器，通过在学习的正交基中重新参数化每个专家，并用输入特征与专家基之间的余弦相似性定义的“基得分”替换学习的门控日志。", "result": "ERMoE在ImageNet分类和跨模态图像文本检索基准测试中实现了最先进的准确性，并自然地产生了更平坦的专家负载分布。3D MRI变体（ERMoE-ba）提高了大脑年龄预测的准确性，并产生了具有解剖学可解释性的专家专业化。", "conclusion": "ERMoE引入了一种新的稀疏专家模型的架构原则，直接解决了路由不稳定的问题，并实现了改进的性能和可扩展的可解释性。"}}
{"id": "2511.10688", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10688", "abs": "https://arxiv.org/abs/2511.10688", "authors": ["Jiahang He", "Rishi Ramachandran", "Neel Ramachandran", "Aryan Katakam", "Kevin Zhu", "Sunishchal Dev", "Ashwinee Panda", "Aryan Shrivastava"], "title": "Modeling and Predicting Multi-Turn Answer Instability in Large Language Models", "comment": null, "summary": "As large language models (LLMs) are adopted in an increasingly wide range of applications, user-model interactions have grown in both frequency and scale. Consequently, research has focused on evaluating the robustness of LLMs, an essential quality for real-world tasks. In this paper, we employ simple multi-turn follow-up prompts to evaluate models' answer changes, model accuracy dynamics across turns with Markov chains, and examine whether linear probes can predict these changes. Our results show significant vulnerabilities in LLM robustness: a simple \"Think again\" prompt led to an approximate 10% accuracy drop for Gemini 1.5 Flash over nine turns, while combining this prompt with a semantically equivalent reworded question caused a 7.5% drop for Claude 3.5 Haiku. Additionally, we find that model accuracy across turns can be effectively modeled using Markov chains, enabling the prediction of accuracy probabilities over time. This allows for estimation of the model's stationary (long-run) accuracy, which we find to be on average approximately 8% lower than its first-turn accuracy for Gemini 1.5 Flash. Our results from a model's hidden states also reveal evidence that linear probes can help predict future answer changes. Together, these results establish stationary accuracy as a principled robustness metric for interactive settings and expose the fragility of models under repeated questioning. Addressing this instability will be essential for deploying LLMs in high-stakes and interactive settings where consistent reasoning is as important as initial accuracy.", "AI": {"tldr": "本研究通过多轮次的简单提示评估了大型语言模型在回答问题时的准确性和稳定性，结果表明模型在反复提问的情况下准确性显著下降，并揭示了使用线性探测方法预测答案变化的潜力。", "motivation": "随着大型语言模型在更加广泛的应用中被采用，这些模型与用户的交互变得越来越多和复杂。这项研究的主要动机是评估这些模型在这些实际应用场景中的稳健性，特别是它们在多轮交互中的决策稳定性和准确性动态。", "method": "本研究使用了多轮次后续提示来评估大型语言模型的答案变化、这些模型在多轮交互中的准确性动态，并探讨线性探测方法是否可以预测这些变化。具体地，通过引入诸如“再想想”这样的简单多轮次提示来触发模型的回答变化。为了分析多轮次交互中的准确性动态，研究使用了马尔可夫链模型，并测试了线性探测方法在预测答案变化方面的有效性。", "result": "研究表明，一个简单的“再想想”提示导致Gemini 1.5 Flash在九轮交互中平均准确率下降了大约10%，而结合这种提示与语义等效的重述问题使得Claude 3.5 Haiku的准确率下降了约7.5%。此外，通过马尔可夫链模型，研究能够有效地预测模型交互过程中的准确率动态，揭示出模型的长期平均准确率相比于第一轮交互下通常会降低约8%。根据模型的隐藏状态，研究还发现线性探测方法能在预测未来答案变化上提供有效的预测。这些发现为评估大型语言模型在交互式环境中的稳定性提供了一个关键指标。", "conclusion": "研究结果揭示了大型语言模型在面对反复提问时的脆弱性。这不仅影响模型的初始准确性，还对多轮交互情境下的稳定性提出了挑战。针对这种不稳定性进行改进将对未来在高风险和互动场景中部署大型语言模型至关重要。这些发现预示着“稳定性准确率”作为一个有价值的稳健性度量，将在评估大型语言模型用于互动场景的能力时发挥重要作用。"}}
{"id": "2511.10974", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10974", "abs": "https://arxiv.org/abs/2511.10974", "authors": ["Haoran Chen", "Houze Xu", "Micah Goldblum", "Daoguo Dong", "Zuxuan Wu"], "title": "Preserving Cross-Modal Consistency for CLIP-based Class-Incremental Learning", "comment": null, "summary": "Class-incremental learning (CIL) enables models to continuously learn new categories from sequential tasks without forgetting previously acquired knowledge. While recent advances in vision-language models such as CLIP have demonstrated strong generalization across domains, extending them to continual settings remains challenging. In particular, learning task-specific soft prompts for newly introduced classes often leads to severe classifier bias, as the text prototypes overfit to recent categories when prior data are unavailable. In this paper, we propose DMC, a simple yet effective two-stage framework for CLIP-based CIL that decouples the adaptation of the vision encoder and the optimization of textual soft prompts. Each stage is trained with the other frozen, allowing one modality to act as a stable semantic anchor for the other to preserve cross-modal alignment. Furthermore, current CLIP-based CIL approaches typically store class-wise Gaussian statistics for generative replay, yet they overlook the distributional drift that arises when the vision encoder is updated over time. To address this issue, we introduce DMC-OT, an enhanced version of DMC that incorporates an optimal-transport guided calibration strategy to align memory statistics across evolving encoders, along with a task-specific prompting design that enhances inter-task separability. Extensive experiments on CIFAR-100, Imagenet-R, CUB-200, and UCF-101 demonstrate that both DMC and DMC-OT achieve state-of-the-art performance, with DMC-OT further improving accuracy by an average of 1.80%.", "AI": {"tldr": "提出了DMC框架及其增强版DMC-OT来解决基于CLIP的类别增量学习中的问题，通过最优传输技术改进记忆统计并优化提示设计，实验证明其优越性能。", "motivation": "虽然最近的视觉语言模型如CLIP在不同领域展现了强大的泛化能力，但在持续学习场景中仍具挑战，尤其是在学习针对新类别特化的软提示时可能会导致分类器偏差。", "method": "提出了一种名为DMC的两阶段框架，用于基于CLIP的类别增量学习。该框架将视觉编码器的自适应和文本软提示的优化分离，每个阶段在训练时都会冻结另一阶段，以保持跨模态的对齐。此外，提出了DMC-OT，该方法结合了最优传输指导的校准策略，用以对齐随编码器演进而变化的记忆统计，并且引入了任务特定的提示设计以增强任务间的可分性。", "result": "实验在CIFAR-100、Imagenet-R、CUB-200和UCF-101数据集上进行了广泛的测试，结果证明了DMC和DMC-OT方法的有效性。", "conclusion": "实验结果表明，DMC和DMC-OT均达到了最先进的性能，特别是在应用DMC-OT时，准确率平均提高了1.80%。"}}
{"id": "2511.10689", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10689", "abs": "https://arxiv.org/abs/2511.10689", "authors": ["Ashish Kattamuri", "Arpita Vats", "Harshwardhan Fartale", "Rahul Raja", "Akshata Kishore Moharir", "Ishita Prasad"], "title": "Equilibrium Dynamics and Mitigation of Gender Bias in Synthetically Generated Data", "comment": "Accepted to AAAI Workshop on Shaping Responsible Synthetic Data in the Era of Foundation Models", "summary": "Recursive prompting with large language models enables scalable synthetic dataset generation but introduces the risk of bias amplification. We investigate gender bias dynamics across three generations of recursive text generation using three complementary evaluation frameworks: rule-based pattern matching, embedding-based semantic similarity, and downstream task performance. Experiments with three initial bias levels (0.1, 0.3, 0.6) and four mitigation strategies reveal equilibrium dynamics rather than monotonic amplification. The low initial bias amplifies toward the model's inherent bias level (+36%), whereas the high initial bias decays toward it (-26%). Among mitigation methods, contrastive augmentation, which introduces gender-swapped variants, achieves significant downstream bias reduction (98.8% for low initial bias and 91% on average) despite producing higher embedding-based bias scores. This paradox demonstrates that semantic similarity metrics may diverge from behavioral fairness outcomes, highlighting the need for multidimensional evaluation in responsible synthetic data generation.", "AI": {"tldr": "研究调查了递归文本生成中的性别偏见动态，揭示了平衡而非单调的偏见变化规律，并证明了对比增强方法在减少下游任务中的偏见效果显著，但也强调了多维度评估在合成数据生成中的重要性。", "motivation": "研究的动机在于探讨在使用大语言模型进行递归提示时产生的大合成数据生成中的性别偏见扩大风险。", "method": "该研究使用了三种评估框架来调查递归文本生成中的性别偏见动态：基于规则的模式匹配、基于嵌入的语义相似度以及下游任务性能。他们进行了实验，实验设置了三个初始偏见水平（0.1、0.3、0.6）以及四种缓解策略。", "result": "实验结果揭示了偏见的平衡动态而非单调放大：初始偏见低时朝着模型内在偏见水平放大（+36%），而初始偏见高时则朝着模型内在偏见水平减少（-26%）。在缓解方法中，对比增强（引入性别互换变体）在下游任务中实现了显著的偏见减少（对于低初始偏见水平为98.8%，平均为91%）。", "conclusion": "该研究表明，语义相似度度量可能存在与行为公平性结果不符，需要采用多维度评估方法生成负责任的合成数据。"}}
{"id": "2511.10979", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10979", "abs": "https://arxiv.org/abs/2511.10979", "authors": ["Bowen Sun", "Yujun Cai", "Ming-Hsuan Yang", "Hang Wu", "Yiwei Wang"], "title": "PAS: A Training-Free Stabilizer for Temporal Encoding in Video LLMs", "comment": "13 pages, 5 figures", "summary": "Video LLMs suffer from temporal inconsistency: small shifts in frame timing can flip attention and suppress relevant frames. We trace this instability to the common extension of Rotary Position Embeddings to video through multimodal RoPE. The induced inverse Fourier time kernel exhibits frame-scale ripples that multiply adjacent frames by different factors, which perturbs attention that should otherwise be governed by the raw query key inner product. We present Phase Aggregated Smoothing (PAS), a simple, training-free mechanism that applies small opposed phase offsets across heads and then aggregates their outputs. PAS preserves the per-head spectrum magnitude, while the aggregation effectively smooths the temporal kernel and reduces phase sensitivity without changing the positional encoding structure. Our analysis shows that the RoPE rotated logit can be approximated as a content dot product scaled by a time kernel; smoothing this kernel yields Lipschitz stability of attention to small temporal shifts; multi phase averaging attenuates high frequency ripples while preserving per-head spectra under Nyquist-valid sampling. Experiments on multiple video understanding benchmarks under matched token budgets show consistent improvements with negligible computational overhead. PAS provides a plug and play upgrade for robust temporal encoding in Video LLMs.", "AI": {"tldr": "提出Phase Aggregated Smoothing (PAS) 方法，有效减轻了视频LLMs中的时间一致性问题，提高了在视频理解上的表现，且计算开销很小。", "motivation": "解决视频LLMs中的时间不一致性问题，小的帧时间差异可能会改变注意力机制并抑制相关帧。这一问题被归因于多模态RoPE的时间内核中存在的帧级波动，这些波动会使相邻帧乘以不同的因子，从而扰动了本应由raw query key内积决定的注意力。", "method": "提出了一种名为Phase Aggregated Smoothing (PAS) 的简单且无需训练的机制，它在不同的头部应用了小的相反相位偏移，然后聚合这些输出。PAS保留了每头部的频谱幅度，而聚合则有效地平滑了时间内核，减少了对相位的敏感性，同时不改变位置编码结构。", "result": "在多个视频理解基准测试中，与匹配的标记预算相比，PAS在计算开销可以忽略的情况下，提供了稳定的一致性改进。", "conclusion": "PAS为视频LLMs提供了一种即插即用的升级方案，以实现更稳健的时间编码。"}}
{"id": "2511.10690", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10690", "abs": "https://arxiv.org/abs/2511.10690", "authors": ["Juntu Zhao", "Jialing Zhang", "Chongxuan Li", "Dequan Wang"], "title": "Saying the Unsaid: Revealing the Hidden Language of Multimodal Systems Through Telephone Games", "comment": "Accepted by NeurIPS 2025 MTI-LLM Workshop", "summary": "Recent closed-source multimodal systems have made great advances, but their hidden language for understanding the world remains opaque because of their black-box architectures. In this paper, we use the systems' preference bias to study their hidden language: During the process of compressing the input images (typically containing multiple concepts) into texts and then reconstructing them into images, the systems' inherent preference bias introduces specific shifts in the outputs, disrupting the original input concept co-occurrence. We employ the multi-round \"telephone game\" to strategically leverage this bias. By observing the co-occurrence frequencies of concepts in telephone games, we quantitatively investigate the concept connection strength in the understanding of multimodal systems, i.e., \"hidden language.\" We also contribute Telescope, a dataset of 10,000+ concept pairs, as the database of our telephone game framework. Our telephone game is test-time scalable: By iteratively running telephone games, we can construct a global map of concept connections in multimodal systems' understanding. Here we can identify preference bias inherited from training, assess generalization capability advancement, and discover more stable pathways for fragile concept connections. Furthermore, we use Reasoning-LLMs to uncover unexpected concept relationships that transcend textual and visual similarities, inferring how multimodal systems understand and simulate the world. This study offers a new perspective on the hidden language of multimodal systems and lays the foundation for future research on the interpretability and controllability of multimodal systems.", "AI": {"tldr": "通过电话游戏策略性利用多模态系统压缩图像到文本再重构图像过程中的偏好偏差，探究概念间的连接强度及隐藏语言，揭示系统理解世界的方式。创建了包含10,000多种概念对的数据集，基于此进行迭代的电话游戏，构建全局概念连接图。", "motivation": "尽管近期的闭源多模态系统取得了重大进步，但其黑盒架构使其理解世界的内部语言仍是模糊不清的。研究这些系统的隐藏语言有助于揭示其理解世界的内部逻辑。", "method": "使用多轮'电话游戏'策略性地利用系统的偏好偏差，通过观察电话游戏中概念的共现频率，定量研究多模态系统理解中的概念间连接强度，即'隐藏语言'。同时，创建了包含10,000多种概念对的Telescope数据集作为电话游戏框架的数据库。通过迭代执行电话游戏，可以构建系统理解中的全局概念连接图，从而识别训练中继承的偏好偏差，评估泛化能力的进步，并发现脆弱概念连接的更稳定路径。此外，使用推理大型语言模型揭示超越文本和视觉相似性的预料之外的概念关系，推断多模态系统是如何理解和模拟世界的。", "result": "通过电话游戏探究了多模态系统的隐藏语言，并分析了概念间的连接强度，从而识别系统的偏好偏差，评估其泛化能力，并发现更稳定的概念连接路径。此外，利用推理大型语言模型揭示了多模态系统理解世界的方式。", "conclusion": "此次研究为理解多模态系统的隐藏语言提供了新视角，并为未来的解释性和可控性研究奠定了基础。"}}
{"id": "2511.10983", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10983", "abs": "https://arxiv.org/abs/2511.10983", "authors": ["Jeffrey Liu", "Rongbin Hu"], "title": "Binary Verification for Zero-Shot Vision", "comment": null, "summary": "We propose a training-free, binary verification workflow for zero-shot vision with off-the-shelf VLMs. It comprises two steps: (i) quantization, which turns the open-ended query into a multiple-choice question (MCQ) with a small, explicit list of unambiguous candidates; and (ii) binarization, which asks one True/False question per candidate and resolves deterministically: if exactly one is True, select it; otherwise, revert to an MCQ over the remaining plausible candidates. We evaluate the workflow on referring expression grounding (REC), spatial reasoning (Spatial-Map, Spatial-Grid, Spatial-Maze), and BLINK-Jigsaw. Relative to answering open-ended queries directly, quantization to MCQ yields large gains, and True/False binarization provides a consistent additional boost. Across all tasks, the same workflow produces significant improvements, indicating generality. Our theory formalizes how open-ended vision queries can be quantized to MCQs and further binarized into True/False verifications, establishing a hardness ladder. A simple analysis explains why Boolean resolution boosts accuracy. Together, these components yield a simple and unified workflow that emphasizes inference-time design over task-specific training. It offers a practical, drop-in path to stronger zero-shot vision with today's VLMs.", "AI": {"tldr": "提出了一个无需训练的二值验证流程，用于零样本视觉任务以提高模型性能。该方法通过将开放性问题转化为多选题并进一步二值化，实现了简单的推理设计优于任务特定训练的效果。", "motivation": "动机是改善零样本零视觉问题的性能，减少对特定任务训练的依赖，使当前的视觉语言模型能更有效地处理开放性视觉查询。", "method": "方法包括两个步骤：量化，将开放性查询转化为具有少量明确候选选项的多选题；二值化，对每个候选选项提出一个真假问题并进行确定性解决。", "result": "相比于直接回答开放性查询，转化成多选题后进一步的真假二值化能显著提高准确率。该方法在多种任务上均表现出了良好的普适性。", "conclusion": "通过将开放性视觉查询转化为多选和二值化问题的方法，建立起问题复杂度阶梯，简单分析表明布尔解析可以提高准确度，整个流程提供了一种简单统一的推理设计方式。"}}
{"id": "2511.10691", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10691", "abs": "https://arxiv.org/abs/2511.10691", "authors": ["Zijian Chen", "Wenjun Zhang", "Guangtao Zhai"], "title": "Evaluating from Benign to Dynamic Adversarial: A Squid Game for Large Language Models", "comment": "26 pages, 12 figures", "summary": "Contemporary benchmarks are struggling to keep pace with the development of large language models (LLMs). Although they are indispensable to evaluate model performance on various tasks, it is uncertain whether the models trained on Internet data have genuinely learned how to solve problems or merely seen the questions before. This potential data contamination issue presents a fundamental challenge to establishing trustworthy evaluation frameworks. Meanwhile, existing benchmarks predominantly assume benign, resource-rich settings, leaving the behavior of LLMs under pressure unexplored. In this paper, we introduce Squid Game, a dynamic and adversarial evaluation environment with resource-constrained and asymmetric information settings elaborated to evaluate LLMs through interactive gameplay against other LLM opponents. Notably, Squid Game consists of six elimination-style levels, focusing on multi-faceted abilities, such as instruction-following, code, reasoning, planning, and safety alignment. We evaluate over 50 LLMs on Squid Game, presenting the largest behavioral evaluation study of general LLMs on dynamic adversarial scenarios. We observe a clear generational phase transition on performance in the same model lineage and find evidence that some models resort to speculative shortcuts to win the game, indicating the possibility of higher-level evaluation paradigm contamination in static benchmarks. Furthermore, we compare prominent LLM benchmarks and Squid Game with correlation analyses, highlighting that dynamic evaluation can serve as a complementary part for static evaluations. The code and data will be released in the future.", "AI": {"tldr": "该研究提出了一种新的评估框架——鱿鱼游戏，以解决现有基准测试的局限性，并通过互动游戏来评估多个LLMs的能力，发现静态基准测试可能存在数据污染问题。", "motivation": "当代基准测试难以跟上大语言模型的发展，存在数据污染和评估环境过于理想的挑战，希望能够建立更可靠的评估框架。", "method": "引入鱿鱼游戏，这是一个动态且对抗性的评估环境，模拟资源受限和信息不对称的情况，通过互动游戏来评估LLMs，游戏包括六个淘汰赛级别的题目，注重评估多种能力，如指令遵循、编程、推理、规划和安全对齐能力。", "result": "评估了超过50个LLMs，并观察到在同一模型系谱中，性能存在明显的代际过渡，并发现某些模型通过投机取巧来赢得比赛，表明静态基准测试可能被高级评估范式污染。", "conclusion": "动态评估可以作为静态评估的补充部分。"}}
{"id": "2511.10991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10991", "abs": "https://arxiv.org/abs/2511.10991", "authors": ["Daxin Li", "Yuanchao Bai", "Kai Wang", "Wenbo Zhao", "Junjun Jiang", "Xianming Liu"], "title": "Rethinking Autoregressive Models for Lossless Image Compression via Hierarchical Parallelism and Progressive Adaptation", "comment": "15 pages", "summary": "Autoregressive (AR) models, the theoretical performance benchmark for learned lossless image compression, are often dismissed as impractical due to prohibitive computational cost. This work re-thinks this paradigm, introducing a framework built on hierarchical parallelism and progressive adaptation that re-establishes pure autoregression as a top-performing and practical solution. Our approach is embodied in the Hierarchical Parallel Autoregressive ConvNet (HPAC), an ultra-lightweight pre-trained model using a hierarchical factorized structure and content-aware convolutional gating to efficiently capture spatial dependencies. We introduce two key optimizations for practicality: Cache-then-Select Inference (CSI), which accelerates coding by eliminating redundant computations, and Adaptive Focus Coding (AFC), which efficiently extends the framework to high bit-depth images. Building on this efficient foundation, our progressive adaptation strategy is realized by Spatially-Aware Rate-Guided Progressive Fine-tuning (SARP-FT). This instance-level strategy fine-tunes the model for each test image by optimizing low-rank adapters on progressively larger, spatially-continuous regions selected via estimated information density. Experiments on diverse datasets (natural, satellite, medical) validate that our method achieves new state-of-the-art compression. Notably, our approach sets a new benchmark in learned lossless compression, showing a carefully designed AR framework can offer significant gains over existing methods with a small parameter count and competitive coding speeds.", "AI": {"tldr": "本文提出了一种新的自回归模型框架，通过引入分层并行自回归卷积网络（HPAC）和两种优化技术CSI和AFC，以及空间感知速率指导渐进细调（SARP-FT）策略，来提高自回归模型的实用性和压缩效率。", "motivation": "尽管自回归（AR）模型是学习无损图像压缩的理论性能基准，但由于其计算成本较高，通常被认为难以应用。本论文旨在重新审视这一方法，通过引入新的框架和优化策略，使其成为一种高性能且实用的解决方案。", "method": "本论文提出了一种名为分层并行自回归卷积网络（HPAC）的轻量级预训练模型，该模型使用分层因子化结构和内容感知卷积门控来高效捕捉空间依赖性，同时提出Cache-then-Select Inference (CSI) 和Adaptive Focus Coding (AFC)两种优化方法以提高其实用性。此外，还提出了空间感知速率指导渐进细调（SARP-FT）策略，在实例级别对模型进行适应性调整，以提高编码速度和效率。", "result": "实验结果表明，本方法在多种数据集（自然、卫星、医学图像）上实现了新的最先进的压缩性能，特别是在学习无损图像压缩领域设立了新的基准。", "conclusion": "通过精心设计的自回归框架，该方法能够在较小的参数量和竞争性的编码速度下显著超越现有方法的表现。"}}
{"id": "2511.10693", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.10693", "abs": "https://arxiv.org/abs/2511.10693", "authors": ["Eyal Rabin", "Zohar Elyoseph", "Rotem Israel-Fishelson", "Adi Dali", "Ravit Nussinson"], "title": "Do AI Voices Learn Social Nuances? A Case of Politeness and Speech Rate", "comment": null, "summary": "Voice-based artificial intelligence is increasingly expected to adhere to human social conventions, but can it learn implicit cues that are not explicitly programmed? This study investigates whether state-of-the-art text-to-speech systems have internalized the human tendency to reduce speech rate to convey politeness - a non-obvious prosodic marker. We prompted 22 synthetic voices from two leading AI platforms (AI Studio and OpenAI) to read a fixed script under both \"polite and formal\" and \"casual and informal\" conditions and measured the resulting speech duration. Across both AI platforms, the polite prompt produced slower speech than the casual prompt with very large effect sizes, an effect that was statistically significant for all of AI Studio's voices and for a large majority of OpenAI's voices. These results demonstrate that AI can implicitly learn and replicate psychological nuances of human communication, highlighting its emerging role as a social actor capable of reinforcing human social norms.", "AI": {"tldr": "研究发现，商用文本转语音系统在被指示以礼貌和正式或随意和非正式的方式读取文本时，语速变化与预期一致，表明AI能够捕捉到人类社交习惯中的细微变化。", "motivation": "随着基于声音的人工智能越来越多地期望遵守人类社会规范，这项研究旨在探讨这些AI系统是否能够学习那些没有被明确编程的隐性线索，特别是人类在表达礼貌时往往通过放慢语速这一不明显的语气标记。", "method": "本研究通过让来自两个顶级AI平台（AI Studio和OpenAI）的22种合成语音在“礼貌且正式”和“随意且非正式”的条件下读取固定脚本，并测量说话时长来检验AI系统是否隐含了人类通过放慢说话速度表达礼貌这一微妙的语调标志。", "result": "研究结果显示，无论是AI Studio还是OpenAI，'礼貌'条件下的语速明显比'随意'条件下慢得多，并且AI Studio的所有语音和OpenAI中的大多数语音在统计上都有显著差异。", "conclusion": "该研究证明了AI能够暗中学习并复制人类沟通中的心理细微差别，突显了它作为能够加强人类社会规范的社会行为者的新角色。"}}
{"id": "2511.10993", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10993", "abs": "https://arxiv.org/abs/2511.10993", "authors": ["Keunwoo Park", "Jihye Chae", "Joong Ho Ahn", "Jihoon Kweon"], "title": "CLUE: Controllable Latent space of Unprompted Embeddings for Diversity Management in Text-to-Image Synthesis", "comment": null, "summary": "Text-to-image synthesis models require the ability to generate diverse images while maintaining stability. To overcome this challenge, a number of methods have been proposed, including the collection of prompt-image datasets and the integration of additional data modalities during training. Although these methods have shown promising results in general domains, they face limitations when applied to specialized fields such as medicine, where only limited types and insufficient amounts of data are available. We present CLUE (Controllable Latent space of Unprompted Embeddings), a generative model framework that achieves diverse generation while maintaining stability through fixed-format prompts without requiring any additional data. Based on the Stable Diffusion architecture, CLUE employs a Style Encoder that processes images and prompts to generate style embeddings, which are subsequently fed into a new second attention layer of the U-Net architecture. Through Kullback-Leibler divergence, the latent space achieves continuous representation of image features within Gaussian regions, independent of prompts. Performance was assessed on otitis media dataset. CLUE reduced FID to 9.30 (vs. 46.81) and improved recall to 70.29% (vs. 49.60%). A classifier trained on synthetic-only data at 1000% scale achieved an F1 score of 83.21% (vs. 73.83%). Combining synthetic data with equal amounts of real data achieved an F1 score of 94.76%, higher than when using only real data. On an external dataset, synthetic-only training achieved an F1 score of 76.77% (vs. 60.61%) at 1000% scale. The combined approach achieved an F1 score of 85.78%, higher than when using only the internal dataset. These results demonstrate that CLUE enables diverse yet stable image generation from limited datasets and serves as an effective data augmentation method for domain-specific applications.", "AI": {"tldr": "CLUE is a novel generative model that generates diverse and stable images from limited datasets, especially in specialized fields like medicine.", "motivation": "To generate diverse and stable images in specialized fields like medicine where data is limited and insufficient.", "method": "Text-to-image synthesis model CLUE, which uses a Style Encoder to generate style embeddings for a new second attention layer of the U-Net architecture, independent of prompts.", "result": "CLUE performed well in otitis media dataset, achieving lower FID and higher recall compared to the baseline. Classifier trained on CLUE-generated data achieved higher F1 scores compared to real data alone.", "conclusion": "CLUE enables diverse yet stable image generation from limited datasets and serves as an effective data augmentation method for domain-specific applications."}}
{"id": "2511.10694", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10694", "abs": "https://arxiv.org/abs/2511.10694", "authors": ["Aditya Pola", "Vineeth N. Balasubramanian"], "title": "Where does an LLM begin computing an instruction?", "comment": null, "summary": "Following an instruction involves distinct sub-processes, such as reading content, reading the instruction, executing it, and producing an answer. We ask where, along the layer stack, instruction following begins, the point where reading gives way to doing. We introduce three simple datasets (Key-Value, Quote Attribution, Letter Selection) and two hop compositions of these tasks. Using activation patching on minimal-contrast prompt pairs, we measure a layer-wise flip rate that indicates when substituting selected residual activations changes the predicted answer. Across models in the Llama family, we observe an inflection point, which we term onset, where interventions that change predictions before this point become largely ineffective afterward. Multi-hop compositions show a similar onset location. These results provide a simple, replicable way to locate where instruction following begins and to compare this location across tasks and model sizes.", "AI": {"tldr": "研究确定了语言模型中从理解和阅读指令转变为执行指令的层次位置，称之为起点，并将其作为一个简单的、可重复的方式，用于比较不同任务和模型大小下的该位置。", "motivation": "研究指令遵循开始的层次位置，即从阅读转变为执行的点。", "method": "通过使用最小对比提示对的激活补丁技术，测量各层的翻转率，来确定替换选定的残差激活是否改变了预测结果。", "result": "观察到一个拐点，我们称之为起点，在此点前的干预措施在之后变得无效。多次跳跃任务也显示出类似的起点位置。", "conclusion": "这些发现提供了一种简单且可重复的方法，用于定位指令遵循开始的位置，并可以比较不同任务和模型大小下的这一位置。"}}
{"id": "2511.10997", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10997", "abs": "https://arxiv.org/abs/2511.10997", "authors": ["Jiajun Chen", "Sai Cheng", "Yutao Yuan", "Yirui Zhang", "Haitao Yuan", "Peng Peng", "Yi Zhong"], "title": "PROMISE: Prompt-Attentive Hierarchical Contrastive Learning for Robust Cross-Modal Representation with Missing Modalities", "comment": "Accepted by AAAI'2026 Main Conference", "summary": "Multimodal models integrating natural language and visual information have substantially improved generalization of representation models. However, their effectiveness significantly declines in real-world situations where certain modalities are missing or unavailable. This degradation primarily stems from inconsistent representation learning between complete multimodal data and incomplete modality scenarios. Existing approaches typically address missing modalities through relatively simplistic generation methods, yet these approaches fail to adequately preserve cross-modal consistency, leading to suboptimal performance. To overcome this limitation, we propose a novel multimodal framework named PROMISE, a PROMpting-Attentive HIerarchical ContraStive LEarning approach designed explicitly for robust cross-modal representation under conditions of missing modalities. Specifically, PROMISE innovatively incorporates multimodal prompt learning into a hierarchical contrastive learning framework, equipped with a specially designed prompt-attention mechanism. This mechanism dynamically generates robust and consistent representations for scenarios where particular modalities are absent, thereby effectively bridging the representational gap between complete and incomplete data. Extensive experiments conducted on benchmark datasets, along with comprehensive ablation studies, clearly demonstrate the superior performance of PROMISE compared to current state-of-the-art multimodal methods.", "AI": {"tldr": "The paper introduces PROMISE, a novel framework using multimodal prompt learning and hierarchical contrastive learning to handle missing modalities in multimodal models, demonstrating significant performance improvements.", "motivation": "The primary motivation is to improve the robustness of multimodal models in real-world scenarios where modalities might be missing, addressing the limitation of inconsistent representation learning between complete and incomplete modalities.", "method": "This paper proposes a novel multimodal framework named PROMISE, which integrates multimodal prompt learning into a hierarchical contrastive learning framework and incorporates a prompt-attention mechanism to address the issue of missing modalities.", "result": "The proposed framework, PROMISE, shows superior performance in handling missing modalities and improving cross-modal consistency compared to existing methods, validated through experiments on benchmark datasets and ablation studies.", "conclusion": "Experimental results show that PROMISE significantly outperforms current state-of-the-art multimodal methods in handling scenarios with missing modalities, bridging the representational gap between complete and incomplete data."}}
{"id": "2511.10695", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10695", "abs": "https://arxiv.org/abs/2511.10695", "authors": ["Jonghyeon Choi", "Yeonjun Choi", "Hyun-chul Kim", "Beakcheol Jang"], "title": "\"As Eastern Powers, I will veto.\" : An Investigation of Nation-level Bias of Large Language Models in International Relations", "comment": "21 pages, 4 figures. This is the extended version of the paper accepted at AAAI 2026, which includes all technical appendices and additional experimental details", "summary": "This paper systematically examines nation-level biases exhibited by Large Language Models (LLMs) within the domain of International Relations (IR). Leveraging historical records from the United Nations Security Council (UNSC), we developed a bias evaluation framework comprising three distinct tests to explore nation-level bias in various LLMs, with a particular focus on the five permanent members of the UNSC. Experimental results show that, even with the general bias patterns across models (e.g., favorable biases toward the western nations, and unfavorable biases toward Russia), these still vary based on the LLM. Notably, even within the same LLM, the direction and magnitude of bias for a nation change depending on the evaluation context. This observation suggests that LLM biases are fundamentally multidimensional, varying across models and tasks. We also observe that models with stronger reasoning abilities show reduced bias and better performance. Building on this finding, we introduce a debiasing framework that improves LLMs' factual reasoning combining Retrieval-Augmented Generation with Reflexion-based self-reflection techniques. Experiments show it effectively reduces nation-level bias, and improves performance, particularly in GPT-4o-mini and LLama-3.3-70B. Our findings emphasize the need to assess nation-level bias alongside performance when applying LLMs in the IR domain.", "AI": {"tldr": "研究探讨了大型语言模型中的国家偏见，并提出了一种去偏见框架，该框架在某些模型中显示出减少偏见和提升性能的效果。", "motivation": "系统地研究国际关系领域中大型语言模型表现出的国家层面偏见。", "method": "通过分析联合国安全理事会的历史记录，开发了包含三种不同测试的偏见评估框架来探讨各种大型语言模型中的国家层面偏见，特别是安理会五个常任理事国的偏见。", "result": "实验结果表明，尽管存在普遍的偏见模式，但在不同的大型语言模型中这些偏见的表现也会有所不同。拥有更强推理能力的模型显示出较少的偏见。", "conclusion": "提出了一种结合检索增强生成和反思技术的去偏见框架，该框架能够有效减少国家层面的偏见，并在某些模型中提升性能。研究强调，在国际关系领域应用大型语言模型时，需同时评估国家层面的偏见和性能。"}}
{"id": "2511.11002", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11002", "abs": "https://arxiv.org/abs/2511.11002", "authors": ["Zongyang Qiu", "Bingyuan Wang", "Xingbei Chen", "Yingqing He", "Zeyu Wang"], "title": "EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation", "comment": "15 pages, 12 figures. Accepted as an Oral presentation at AAAI 2026. For code and dataset, see https://zane-zyqiu.github.io/EmoVid", "summary": "Emotion plays a pivotal role in video-based expression, but existing video generation systems predominantly focus on low-level visual metrics while neglecting affective dimensions. Although emotion analysis has made progress in the visual domain, the video community lacks dedicated resources to bridge emotion understanding with generative tasks, particularly for stylized and non-realistic contexts. To address this gap, we introduce EmoVid, the first multimodal, emotion-annotated video dataset specifically designed for creative media, which includes cartoon animations, movie clips, and animated stickers. Each video is annotated with emotion labels, visual attributes (brightness, colorfulness, hue), and text captions. Through systematic analysis, we uncover spatial and temporal patterns linking visual features to emotional perceptions across diverse video forms. Building on these insights, we develop an emotion-conditioned video generation technique by fine-tuning the Wan2.1 model. The results show a significant improvement in both quantitative metrics and the visual quality of generated videos for text-to-video and image-to-video tasks. EmoVid establishes a new benchmark for affective video computing. Our work not only offers valuable insights into visual emotion analysis in artistically styled videos, but also provides practical methods for enhancing emotional expression in video generation.", "AI": {"tldr": "We created EmoVid, an emotion-annotated dataset for stylized video types, and used it to develop an improved video generation technique that enhances emotional expression.", "motivation": "Emotions are crucial in video-based expression, yet current video generation systems often neglect affective dimensions, focusing mainly on low-level visual metrics. The video community lacks a dedicated resource to bridge emotion understanding and generative tasks in stylized and non-realistic contexts.", "method": "Our work introduces EmoVid, a multimodal, emotion-annotated video dataset focusing on creative media like cartoon animations and movie clips. The dataset includes annotations for emotion labels, visual attributes, and text captions. We fine-tuned the Wan2.1 model to create an emotion-conditioned video generation technique.", "result": "The analysis reveals spatial and temporal patterns linking visual features with emotional perceptions in various video forms. The video generation technique shows significant improvement in quantitive metrics and visual quality for tasks like text-to-video and image-to-video.", "conclusion": "This work establishes EmoVid as a new benchmark for affective video computing. It provides valuable insights into visual emotion analysis in stylized videos and offers practical methods to enhance emotional expression in video generation."}}
{"id": "2511.10696", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10696", "abs": "https://arxiv.org/abs/2511.10696", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "$π$-Attention: Periodic Sparse Transformers for Efficient Long-Context Modeling", "comment": "Accepted to IEEE International Conference on Parallel and Distributed Systems 2025 (ICPADS 2025 Oral)", "summary": "Transformers have revolutionized natural language processing, but their quadratic complexity with respect to sequence length remains a fundamental bottleneck for long-range modeling. While sparse attention mechanisms like RingAttention reduce computational costs by restricting attention to local neighborhoods, they suffer from limited receptive fields and lack of adaptability. We present \\PiAttention, a periodic sparse Transformer that factorizes attention into ring-local neighborhoods, deterministic $π$-stride skips, and an adaptive fusion gate. The periodic structure provides predictable coverage of distant tokens, while the sparse footprint keeps the per-layer complexity linear in context length. We prove that \\PiAttention achieves $\\mathcal{O}(kL + π\\log L)$ receptive field growth compared to $\\mathcal{O}(kL)$ for RingAttention, where $k$ is the local window size, $π$ is the skip period, and $L$ is the sequence length. Extensive experiments on language modeling, retrieval, and vision-language tasks demonstrate that \\PiAttention matches or surpasses dense attention quality with 8.3\\% lower perplexity than RingAttention while using 50\\% fewer GPUs for the same context length. Our detailed ablations and visualizations reveal the importance of periodic skips, adaptive fusion, and head-level sparsity coordination for efficient long-context modeling.", "AI": {"tldr": "本文介绍了\\PiAttention，一种新型的稀疏Transformer，通过将注意力机制分解为局部环形领域、确定性的\\(π\\)跳过和自适应融合门，解决了长序列处理问题。它在多种任务上表现出色，并且在计算效率上优于传统的RingAttention。", "motivation": "长序列处理中的二次复杂度问题是Transformers的主要瓶颈。当前的稀疏注意力机制虽然能够减少计算成本，但存在受限的感受野和缺乏适应性的问题。", "method": "\\PiAttention通过引入周期性的注意力机制，并结合确定性的\\(π\\)跳过和自适应融合门来优化稀疏Transformer的注意力机制。", "result": "实验表明，\\PiAttention能够以较低的计算资源达到与密集注意力机制相当甚至更好的性能，证明了其在长序列处理上的有效性。", "conclusion": "\\PiAttention在计算效率和性能上都超过了RingAttention，并且在多个任务上表现优异，展示了周期性跳跃、自适应融合以及头部级稀疏协调的重要性。"}}
{"id": "2511.11004", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11004", "abs": "https://arxiv.org/abs/2511.11004", "authors": ["Yiran Song", "Yikai Zhang", "Shuang Zhou", "Guojun Xiong", "Xiaofeng Yang", "Nian Wang", "Fenglong Ma", "Rui Zhang", "Mingquan Lin"], "title": "MeCaMIL: Causality-Aware Multiple Instance Learning for Fair and Interpretable Whole Slide Image Diagnosis", "comment": "15page,5 figures,8 tables", "summary": "Multiple instance learning (MIL) has emerged as the dominant paradigm for whole slide image (WSI) analysis in computational pathology, achieving strong diagnostic performance through patch-level feature aggregation. However, existing MIL methods face critical limitations: (1) they rely on attention mechanisms that lack causal interpretability, and (2) they fail to integrate patient demographics (age, gender, race), leading to fairness concerns across diverse populations. These shortcomings hinder clinical translation, where algorithmic bias can exacerbate health disparities. We introduce \\textbf{MeCaMIL}, a causality-aware MIL framework that explicitly models demographic confounders through structured causal graphs. Unlike prior approaches treating demographics as auxiliary features, MeCaMIL employs principled causal inference -- leveraging do-calculus and collider structures -- to disentangle disease-relevant signals from spurious demographic correlations. Extensive evaluation on three benchmarks demonstrates state-of-the-art performance across CAMELYON16 (ACC/AUC/F1: 0.939/0.983/0.946), TCGA-Lung (0.935/0.979/0.931), and TCGA-Multi (0.977/0.993/0.970, five cancer types). Critically, MeCaMIL achieves superior fairness -- demographic disparity variance drops by over 65% relative reduction on average across attributes, with notable improvements for underserved populations. The framework generalizes to survival prediction (mean C-index: 0.653, +0.017 over best baseline across five cancer types). Ablation studies confirm causal graph structure is essential -- alternative designs yield 0.048 lower accuracy and 4.2x times worse fairness. These results establish MeCaMIL as a principled framework for fair, interpretable, and clinically actionable AI in digital pathology. Code will be released upon acceptance.", "AI": {"tldr": "介绍了一种因果关系感知的多个实例学习框架MeCaMIL，该框架解决了患者的年龄、性别和种族等人口统计学数据整合的问题，并在公平性、准确性和可解释性方面表现出色。", "motivation": "现有多个实例学习方法存在瓶颈，包括注意力机制缺乏因果解释性和不能整合患者的人口统计数据，这造成了在多样化人群中的公平性问题。这些问题阻碍了临床转化，因为算法偏见可能会加剧健康差异。", "method": "MeCaMIL采用因果推断方法，通过结构化因果图形明确模型了人口统计学混杂因素，不同于以前的方法将人口统计信息作为辅助特征对待，MeCaMIL利用do-微积分和碰撞结构来区分与疾病相关的信号和来自人口统计学的虚假相关性。", "result": "MeCaMIL在CAMELYON16、TCGA-Lung和TCGA-Multi三个基准上实现了最先进的性能，在数据集中五个癌症类型的平均表现分别为ACC/AUC/F1: 0.939/0.983/0.946, 0.935/0.979/0.931和0.977/0.993/0.970。重要的是，MeCaMIL在公平性方面表现突出，人口统计学差异的方差平均减少了超过65%，在生存预测任务上，平均C-index为0.653，比最佳基线高出+0.017。", "conclusion": "这些结果确立了MeCaMIL作为实现公平性、可解释性和临床有效的数字病理学AI框架的地位。"}}
{"id": "2511.10768", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10768", "abs": "https://arxiv.org/abs/2511.10768", "authors": ["Ajwad Abrar", "Nafisa Tabassum Oeshy", "Prianka Maheru", "Farzana Tabassum", "Tareque Mohmud Chowdhury"], "title": "Faithful Summarization of Consumer Health Queries: A Cross-Lingual Framework with LLMs", "comment": "Accepted at the 5th Muslims in Machine Learning (MusIML) Workshop, co-located with NeurIPS 2025", "summary": "Summarizing consumer health questions (CHQs) can ease communication in healthcare, but unfaithful summaries that misrepresent medical details pose serious risks. We propose a framework that combines TextRank-based sentence extraction and medical named entity recognition with large language models (LLMs) to enhance faithfulness in medical text summarization. In our experiments, we fine-tuned the LLaMA-2-7B model on the MeQSum (English) and BanglaCHQ-Summ (Bangla) datasets, achieving consistent improvements across quality (ROUGE, BERTScore, readability) and faithfulness (SummaC, AlignScore) metrics, and outperforming zero-shot baselines and prior systems. Human evaluation further shows that over 80\\% of generated summaries preserve critical medical information. These results highlight faithfulness as an essential dimension for reliable medical summarization and demonstrate the potential of our approach for safer deployment of LLMs in healthcare contexts.", "AI": {"tldr": "本文提出一种新的医疗文本总结框架，通过结合TextRank和医学命名实体识别与LLMs，提高了忠实度，并在不同评估标准下展示了优于基线系统的综述生成效果。", "motivation": "本文旨在通过提高医疗文本总结的忠实度，从而简化患者和医疗服务提供者之间的交流。不忠实的总结可能会导致严重的医疗细节误解，从而造成风险。", "method": "我们提出了一种框架，该框架结合了基于TextRank的句子抽取和医学命名实体识别，并使用大型语言模型（LLMs）来增强医疗文本概要的忠实度。我们还在MeQSum（英文）和BanglaCHQ-Summ（孟加拉语）数据集上对LLaMA-2-7B模型进行了微调。", "result": "实验结果显示，在质量（ROUGE、BERTScore、可读性）和忠实度（SummaC、AlignScore）指标上，我们取得了持续的改进，并优于零样本基线和之前系统。人机评估进一步显示，生成的概要中有超过80%保持了关键的医学信息。", "conclusion": "这些实验结果强调了忠实度在可靠的医疗资料总结中的重要性，并展示了我们在医疗场景中更安全地部署LLMs的潜力。"}}
{"id": "2511.11005", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11005", "abs": "https://arxiv.org/abs/2511.11005", "authors": ["Sungheon Jeong", "Ryozo Masukawa", "Jihong Park", "Sanggeon Yun", "Wenjun Huang", "Hanning Chen", "Mahdi Imani", "Mohsen Imani"], "title": "Draft and Refine with Visual Experts", "comment": null, "summary": "While recent Large Vision-Language Models (LVLMs) exhibit strong multimodal reasoning abilities, they often produce ungrounded or hallucinated responses because they rely too heavily on linguistic priors instead of visual evidence. This limitation highlights the absence of a quantitative measure of how much these models actually use visual information during reasoning. We propose Draft and Refine (DnR), an agent framework driven by a question-conditioned utilization metric. The metric quantifies the model's reliance on visual evidence by first constructing a query-conditioned relevance map to localize question-specific cues and then measuring dependence through relevance-guided probabilistic masking. Guided by this metric, the DnR agent refines its initial draft using targeted feedback from external visual experts. Each expert's output (such as boxes or masks) is rendered as visual cues on the image, and the model is re-queried to select the response that yields the largest improvement in utilization. This process strengthens visual grounding without retraining or architectural changes. Experiments across VQA and captioning benchmarks show consistent accuracy gains and reduced hallucination, demonstrating that measuring visual utilization provides a principled path toward more interpretable and evidence-driven multimodal agent systems.", "AI": {"tldr": "针对大型视觉-语言模型过分依赖语言信息而忽视视觉证据的问题，本文提出了Draft and Refine (DnR)框架，通过问题条件化利用率度量来强化视觉绑定，从而提高了系统的准确性和证据驱动性，减少了幻觉反应。", "motivation": "最近的大型视觉-语言模型（LVLMs）虽然显示出强大的多模态推理能力，但常常产生未经过验证或幻想的反应，因为它们过分依赖语言先验而不是视觉证据。这种限制突显了没有一个定量衡量这些模型在推理过程中实际上使用了多少视觉信息的标准的缺失。", "method": "我们提出了Draft and Refine (DnR)，这是一个由问题条件化利用率度量驱动的代理框架。该度量通过首先构建问题条件化的相关性图来定位问题特定的线索，然后通过相关性引导的概率掩码来测量依赖关系。该框架通过外部视觉专家提供的针对性反馈来优化代理初始草稿的视觉投射。每个专家的输出（例如边界框或掩码）作为视觉线索呈现在图像上，模型被重新问询选择能够最大提高利用率的回答，从而强化视觉绑定，无需重新训练或改变架构。", "result": "跨VQA（视觉问答）和captioning（图像描述生成）基准测试实验证明，这种框架一致提高了准确性和减少了幻觉反应，展示了测量视觉利用提供了一个原则上的路径来创建更加可解释的和依据证据的多模态代理系统。", "conclusion": "Draft and Refine (DnR)代理框架提升了多模态代理系统中视觉信息的重要性，减少了对语言先验性的依赖，通过加强模型的视觉利用，促进了系统准确性与解释性的提升。"}}
{"id": "2511.10780", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10780", "abs": "https://arxiv.org/abs/2511.10780", "authors": ["Fethi Bougares", "Salima Mdhaffar", "Haroun Elleuch", "Yannick Estève"], "title": "TEDxTN: A Three-way Speech Translation Corpus for Code-Switched Tunisian Arabic - English", "comment": "The Third Arabic Natural Language Processing Conference. Association for Computational Linguistics. 2025", "summary": "In this paper, we introduce TEDxTN, the first publicly available Tunisian Arabic to English speech translation dataset. This work is in line with the ongoing effort to mitigate the data scarcity obstacle for a number of Arabic dialects. We collected, segmented, transcribed and translated 108 TEDx talks following our internally developed annotations guidelines. The collected talks represent 25 hours of speech with code-switching that cover speakers with various accents from over 11 different regions of Tunisia. We make the annotation guidelines and corpus publicly available. This will enable the extension of TEDxTN to new talks as they become available. We also report results for strong baseline systems of Speech Recognition and Speech Translation using multiple pre-trained and fine-tuned end-to-end models. This corpus is the first open source and publicly available speech translation corpus of Code-Switching Tunisian dialect. We believe that this is a valuable resource that can motivate and facilitate further research on the natural language processing of Tunisian Dialect.", "AI": {"tldr": "论文推出了TEDxTN数据集，提供了一个包括代码切换现象的突尼斯方言演讲翻译数据，旨在促进突尼斯方言的自然语言处理研究。", "motivation": "该工作旨在解决阿拉伯方言的数据稀缺问题，为该领域的进一步研究提供资源。", "method": "此论文介绍了TEDxTN，这是第一个公开可用的突尼斯阿拉比亚语到英语的演讲翻译数据集。作者收集了108个TEDx演讲，共计25小时的语音数据，这些数据包含代码切换现象并且涵盖了来自突尼斯11个不同地区的讲者。", "result": "报告了使用多个预训练和微调的端到端模型的语音识别和语音翻译的强基线系统的结果。", "conclusion": "该数据集是第一个开源的公开可用的带有代码切换的突尼斯方言演讲翻译数据集，对于促进突尼斯方言的自然语言处理研究是非常有价值的资源。"}}
