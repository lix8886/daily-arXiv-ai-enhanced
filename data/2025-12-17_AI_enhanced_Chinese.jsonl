{"id": "2512.13731", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13731", "abs": "https://arxiv.org/abs/2512.13731", "authors": ["Weikang Bai", "Yongkun Du", "Yuchen Su", "Yazhen Xie", "Zhineng Chen"], "title": "Complex Mathematical Expression Recognition: Benchmark, Large-Scale Dataset and Strong Baseline", "comment": null, "summary": "Mathematical Expression Recognition (MER) has made significant progress in recognizing simple expressions, but the robust recognition of complex mathematical expressions with many tokens and multiple lines remains a formidable challenge. In this paper, we first introduce CMER-Bench, a carefully constructed benchmark that categorizes expressions into three difficulty levels: easy, moderate, and complex. Leveraging CMER-Bench, we conduct a comprehensive evaluation of existing MER models and general-purpose multimodal large language models (MLLMs). The results reveal that while current methods perform well on easy and moderate expressions, their performance degrades significantly when handling complex mathematical expressions, mainly because existing public training datasets are primarily composed of simple samples. In response, we propose MER-17M and CMER-3M that are large-scale datasets emphasizing the recognition of complex mathematical expressions. The datasets provide rich and diverse samples to support the development of accurate and robust complex MER models. Furthermore, to address the challenges posed by the complicated spatial layout of complex expressions, we introduce a novel expression tokenizer, and a new representation called Structured Mathematical Language, which explicitly models the hierarchical and spatial structure of expressions beyond LaTeX format. Based on these, we propose a specialized model named CMERNet, built upon an encoder-decoder architecture and trained on CMER-3M. Experimental results show that CMERNet, with only 125 million parameters, significantly outperforms existing MER models and MLLMs on CMER-Bench.", "AI": {"tldr": "本论文引入了CMER-Bench基准，用于评估现有MER模型和多模态大语言模型在处理复杂数学表达式上的表现。由于现有数据集主要由简单样本组成，模型在复杂表达式上的表现较差。为此，提出MER-17M和CMER-3M大数据集，并引入了新颖的表达式分词器和结构化数学语言表示方法来解决复杂表达式的识别难题。基于这些方法，开发了专用于复杂数学表达式识别的CMERNet模型，实验表明CMERNet在CMER-Bench上的表现显著优于现有模型。", "motivation": "现有的数学表达式识别模型在处理复杂数学表达式时面临挑战，其主要原因是当前公开的数据集过于简单，不能很好地训练模型处理复杂表达式。为此，论文提出CMER-Bench基准和新的数据集，以便改进模型处理复杂表达式的能力。", "method": "论文引入了CMER-Bench基准，开发了MER-17M和CMER-3M数据集，提出了新的表达式分词器以及结构化数学语言表示方法，用于捕捉表达式的层次和空间结构，同时基于这些方法提出了专门用于复杂数学表达式识别的CMERNet模型。", "result": "实验结果表明，CMERNet模型在CMER-Bench上的表现优于现有模型，显示出其解决复杂数学表达式识别问题的潜力。", "conclusion": "通过开发新的数据集和表达式识别方法，论文为解决复杂数学表达式的识别难题提供了新的解决方案，CMERNet模型显示出了优于现有模型的性能。"}}
{"id": "2512.13739", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13739", "abs": "https://arxiv.org/abs/2512.13739", "authors": ["Yajie Yang", "Yuqing Zhao", "Xiaochao Xi", "Yinan Zhu"], "title": "Human-AI Collaboration Mechanism Study on AIGC Assisted Image Production for Special Coverage", "comment": "AAAI-AISI 2026", "summary": "Artificial Intelligence Generated Content (AIGC) assisting image production triggers controversy in journalism while attracting attention from media agencies. Key issues involve misinformation, authenticity, semantic fidelity, and interpretability. Most AIGC tools are opaque \"black boxes,\" hindering the dual demands of content accuracy and semantic alignment and creating ethical, sociotechnical, and trust dilemmas. This paper explores pathways for controllable image production in journalism's special coverage and conducts two experiments with projects from China's media agency: (1) Experiment 1 tests cross-platform adaptability via standardized prompts across three scenes, revealing disparities in semantic alignment, cultural specificity, and visual realism driven by training-corpus bias and platform-level filtering. (2) Experiment 2 builds a human-in-the-loop modular pipeline combining high-precision segmentation (SAM, GroundingDINO), semantic alignment (BrushNet), and style regulating (Style-LoRA, Prompt-to-Prompt), ensuring editorial fidelity through CLIP-based semantic scoring, NSFW/OCR/YOLO filtering, and verifiable content credentials. Traceable deployment preserves semantic representation. Consequently, we propose a human-AI collaboration mechanism for AIGC assisted image production in special coverage and recommend evaluating Character Identity Stability (CIS), Cultural Expression Accuracy (CEA), and User-Public Appropriateness (U-PA).", "AI": {"tldr": "Bài báo này nghiên cứu về cách tạo ra hình ảnh có thể kiểm soát trong báo chí với sự hỗ trợ của AI, thực hiện hai thí nghiệm và đề xuất mô hình hợp tác giữa con người và AI cho sản xuất hình ảnh.", "motivation": "Bài báo này được viết để khám phá các con đường để tạo hình ảnh có thể kiểm soát trong báo chí, đặc biệt là trong bối cảnh của báo chí có sự tham gia của AI, nơi mà các vấn đề như sự dối trá, tính xác thực, tính chính xác thông điệp và tính minh bạch đang gây ra nhiều tranh cãi.", "method": " Cuộc nghiên cứu này khám phá các con đường để tạo ra hình ảnh có thể kiểm soát trong báo chí và thực hiện hai thí nghiệm dựa trên các dự án từ cơ quan truyền thông của Trung Quốc. Thí nghiệm 1 kiểm tra khả năng thích ứng giữa các nền tảng bằng cách sử dụng các lời nhắc tiêu chuẩn hóa trong ba cảnh, và Thí nghiệm 2 xây dựng một pipeline mô-đun có sự tham gia của con người, kết hợp phân đoạn có độ chính xác cao, điện toán ngôn ngữ (SAM, GroundingDINO), kết hợp ngữ nghĩa (BrushNet), và điều chỉnh phong cách (Style-LoRA, Prompt-to-Prompt).", "result": "Thí nghiệm 1 bộc lộ những khác biệt trong sự đồng ngữ, đặc trưng văn hóa và tính chân thực thị giác do thiên kiến bộ dữ liệu huấn luyện và bộ lọc cấp nền tảng kéo theo. Thí nghiệm 2 đảm bảo tính chính xác biên tập thông qua đánh giá ngữ nghĩa dựa trên CLIP, bộ lọc NSFW/OCR/YOLO và tín hiệu nội dung có thể xác minh.", "conclusion": "Cuối cùng, tác giả đề xuất một cơ chế hợp tác giữa con người và AI cho sản xuất hình ảnh có hỗ trợ của AI trong báo chí và đề xuất việc đánh giá tính ổn định của tính cách, độ chính xác của biểu hiện văn hóa và sự phù hợp với công chúng."}}
{"id": "2512.13742", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13742", "abs": "https://arxiv.org/abs/2512.13742", "authors": ["Md. Najib Hasan", "Imran Ahmad", "Sourav Basak Shuvo", "Md. Mahadi Hasan Ankon", "Sunanda Das", "Nazmul Siddique", "Hui Wang"], "title": "DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models", "comment": null, "summary": "Medical image classifiers detect gastrointestinal diseases well, but they do not explain their decisions. Large language models can generate clinical text, yet they struggle with visual reasoning and often produce unstable or incorrect explanations. This leaves a gap between what a model sees and the type of reasoning a clinician expects. We introduce a framework that links image classification with structured clinical reasoning. A new hybrid model, MobileCoAtNet, is designed for endoscopic images and achieves high accuracy across eight stomach-related classes. Its outputs are then used to drive reasoning by several LLMs. To judge this reasoning, we build two expert-verified benchmarks covering causes, symptoms, treatment, lifestyle, and follow-up care. Thirty-two LLMs are evaluated against these gold standards. Strong classification improves the quality of their explanations, but none of the models reach human-level stability. Even the best LLMs change their reasoning when prompts vary. Our study shows that combining DL with LLMs can produce useful clinical narratives, but current LLMs remain unreliable for high-stakes medical decisions. The framework provides a clearer view of their limits and a path for building safer reasoning systems. The complete source code and datasets used in this study are available at https://github.com/souravbasakshuvo/DL3M.", "AI": {"tldr": "本文研究了一种结合深度学习和大型语言模型技术，用于生成医学影像分类的临床叙述，虽然取得了进展但发现大型语言模型在高强度的医疗决策中仍存在不可靠性。", "motivation": "当前，医学影像分类能够很好地检测胃肠道疾病，但无法解释其决策；大型语言模型能够生成临床文本，但在视觉推理上存在困难并且常常产生不稳定或错误解释。这就留下了一个差距，即模型所看到的内容和临床医生所期待的推理类型之间的差距。该研究旨在填补这一差距。", "method": "本研究提出了一个框架，将图像分类与结构化的临床推理联系起来，并设计了一种新的混合模型MobileCoAtNet，专门用于内窥镜图像，在八个胃部相关类别中实现了高准确度。该模型的输出用于驱动多个大型语言模型进行推理。", "result": "研究评估了32个大型语言模型，使用了两个专家验证的基准来评估模型在原因、症状、治疗、生活方式和随访方面的推理质量。结果表明，强大的分类能够提升模型解释的质量，但所有模型未能达到人类稳定性水平，最好的语言模型在提示改变时仍然会改变其推理。", "conclusion": "研究表明，将深度学习与大型语言模型结合可以生成有用的临床叙述，但当前大型语言模型在高风险的医疗决策中仍是不可靠的。该框架提供了更清晰的视角，以理解模型的局限性，并为构建更安全的推理系统提供了路径。"}}
{"id": "2512.13747", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13747", "abs": "https://arxiv.org/abs/2512.13747", "authors": ["Siyuan Dai", "Lunxiao Li", "Kun Zhao", "Eardi Lila", "Paul K. Crane", "Heng Huang", "Dongkuan Xu", "Haoteng Tang", "Liang Zhan"], "title": "Why Text Prevails: Vision May Undermine Multimodal Medical Decision Making", "comment": "Accepted by ICDM 2025 the Workshop on Synergy of AI and Multimodal Biomedical Data Mining", "summary": "With the rapid progress of large language models (LLMs), advanced multimodal large language models (MLLMs) have demonstrated impressive zero-shot capabilities on vision-language tasks. In the biomedical domain, however, even state-of-the-art MLLMs struggle with basic Medical Decision Making (MDM) tasks. We investigate this limitation using two challenging datasets: (1) three-stage Alzheimer's disease (AD) classification (normal, mild cognitive impairment, dementia), where category differences are visually subtle, and (2) MIMIC-CXR chest radiograph classification with 14 non-mutually exclusive conditions. Our empirical study shows that text-only reasoning consistently outperforms vision-only or vision-text settings, with multimodal inputs often performing worse than text alone. To mitigate this, we explore three strategies: (1) in-context learning with reason-annotated exemplars, (2) vision captioning followed by text-only inference, and (3) few-shot fine-tuning of the vision tower with classification supervision. These findings reveal that current MLLMs lack grounded visual understanding and point to promising directions for improving multimodal decision making in healthcare.", "AI": {"tldr": "研究揭示了多模态大语言模型在医疗决策任务上的局限，特别是在视觉理解方面，并提出了几种可能的改进策略。", "motivation": "尽管多模态大语言模型在视觉语言任务上表现出众，但在生物医学领域特别是医疗决策方面仍存在问题。", "method": "我们通过两个具有挑战性的数据集研究了高级多模态大语言模型在医疗决策中的局限性，分别是三阶段阿尔茨海默病分类和MIMIC-CXR胸部影像分类。我们还探索了三种提升性能的策略：上下文学习、视觉转文本后仅使用文本推理、针对视觉部分的少量样本微调。", "result": "研究表明，纯文本推理在医疗决策任务上要优于仅使用视觉或视觉文本结合的情形，且提出了三种提升性能的方法。", "conclusion": "研究发现当前的多模态大语言模型缺乏坚实的基础视觉理解能力，指出了提升医疗决策中多模态决策制定的潜在方向。"}}
{"id": "2512.13884", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13884", "abs": "https://arxiv.org/abs/2512.13884", "authors": ["Jonas Golde", "Patrick Haller", "Alan Akbik"], "title": "FiNERweb: Datasets and Artifacts for Scalable Multilingual Named Entity Recognition", "comment": null, "summary": "Recent multilingual named entity recognition (NER) work has shown that large language models (LLMs) can provide effective synthetic supervision, yet such datasets have mostly appeared as by-products of broader experiments rather than as systematic, reusable resources. We introduce FiNERweb, a dataset-creation pipeline that scales the teacher-student paradigm to 91 languages and 25 scripts. Building on FineWeb-Edu, our approach trains regression models to identify NER-relevant passages and annotates them with multilingual LLMs, resulting in about 225k passages with 235k distinct entity labels. Our experiments show that the regression model achieves more than 84 F1, and that models trained on FiNERweb obtain comparable or improved performance in zero shot transfer settings on English, Thai, and Swahili, despite being trained on 19x less data than strong baselines. In addition, we assess annotation quality using LLM-as-a-judge and observe consistently high scores for both faithfulness (3.99 out of 5) and completeness (4.05 out of 5), indicating reliable and informative annotations. Further, we release the dataset with both English labels and translated label sets in the respective target languages because we observe that the performance of current state-of-the-art models drops by 0.02 to 0.09 F1 when evaluated using target language labels instead of English ones. We release FiNERweb together with all accompanying artifacts to the research community in order to facilitate more effective student-teacher training for multilingual named entity recognition.", "AI": {"tldr": "提出了FiNERweb，一种数据生成管道，用于生成多语言实体识别数据集，包含了多语言大语言模型的标注，并展示了相比于强基准数据，在更少的数据下能够具有竞争力的表现。", "motivation": "研究者们发现大语言模型可以提供有效的合成监督，但这些数据集往往是更广泛实验的副产品，而不是系统化的、可重复使用的资源。因此，引入了FiNERweb来生成多语言实体识别数据集。", "method": "提出FiNERweb，这是一个数据生成管道，扩展了教师-学生范式到91种语言和25种书写系统。在这个基础上，通过训练回归模型来识别NER相关的段落，并使用多语言大模型进行标注，最终生成包含约225k段落和235k不同实体标签的数据集。", "result": "实验结果表明，回归模型的F1值超过84。使用FiNERweb训练的模型在零样本传输设置下的英文、泰文和斯瓦希里语上的表现优于或者与强基准相比持平，尽管训练数据少19倍。此外，使用大模型作为评判者来评估注释质量，观察到一致的高分，忠实度为3.99/5，完整性为4.05/5，表示注释可靠且信息丰富。", "conclusion": "FiNERweb数据集的发布及其相关工件，旨在促进更有效的学生-教师训练过程，特别是在多语言实体识别领域。此外，还观察到当前最先进的模型在使用目标语言标签评价时，性能有所下降。因此，提供的数据集同时带有英文标签和目标语言的翻译标签。"}}
{"id": "2512.13752", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13752", "abs": "https://arxiv.org/abs/2512.13752", "authors": ["Jie Qin", "Jiancheng Huang", "Limeng Qiao", "Lin Ma"], "title": "STAR: STacked AutoRegressive Scheme for Unified Multimodal Learning", "comment": "18 pages, 7 figures", "summary": "Multimodal large language models (MLLMs) play a pivotal role in advancing the quest for general artificial intelligence. However, achieving unified target for multimodal understanding and generation remains challenging due to optimization conflicts and performance trade-offs. To effectively enhance generative performance while preserving existing comprehension capabilities, we introduce STAR: a STacked AutoRegressive scheme for task-progressive unified multimodal learning. This approach decomposes multimodal learning into multiple stages: understanding, generation, and editing. By freezing the parameters of the fundamental autoregressive (AR) model and progressively stacking isomorphic AR modules, it avoids cross-task interference while expanding the model's capabilities. Concurrently, we introduce a high-capacity VQ to enhance the granularity of image representations and employ an implicit reasoning mechanism to improve generation quality under complex conditions. Experiments demonstrate that STAR achieves state-of-the-art performance on GenEval (0.91), DPG-Bench (87.44), and ImgEdit (4.34), validating its efficacy for unified multimodal learning.", "AI": {"tldr": "本文引入了STAR方案，有效地解决了多模态学习中的生成和理解能力之间的权衡问题，并在多个评测指标上实现了领先的性能。", "motivation": "目前，多模态大规模语言模型在追求通用人工智能方面具有重要作用，但由于优化冲突和性能权衡，实现统一的多模态理解和生成仍然具有挑战性。本文旨在有效提升生成性能的同时保持现有理解能力。", "method": "本文提出了一种名为STAR（STacked AutoRegressive）的多模态学习的分阶段学习方案，该方案将多模态学习分解为理解、生成和编辑三个阶段，并通过冻结基本自回归模型的参数，逐层堆叠同构的自回归模块，从而避免跨任务干扰同时扩展模型的能力。此外，引入了高容量的VQ编码以增强图像表示的细节，并使用隐式推理机制来提升在复杂条件下的生成质量。", "result": "实验结果表明，STAR在GenEval（0.91）、DPG-Bench（87.44）和ImgEdit（4.34）上达到了最新的性能水平，验证了其在统一的多模态学习中的有效性。", "conclusion": "STAR方法证明了在不牺牲理解和编辑能力的前提下增强多模态生成性能的可能性，为统一的多模态学习提供了一种有效的解决方案。"}}
{"id": "2512.13961", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13961", "abs": "https://arxiv.org/abs/2512.13961", "authors": ["Team Olmo", ":", "Allyson Ettinger", "Amanda Bertsch", "Bailey Kuehl", "David Graham", "David Heineman", "Dirk Groeneveld", "Faeze Brahman", "Finbarr Timbers", "Hamish Ivison", "Jacob Morrison", "Jake Poznanski", "Kyle Lo", "Luca Soldaini", "Matt Jordan", "Mayee Chen", "Michael Noukhovitch", "Nathan Lambert", "Pete Walsh", "Pradeep Dasigi", "Robert Berry", "Saumya Malik", "Saurabh Shah", "Scott Geng", "Shane Arora", "Shashank Gupta", "Taira Anderson", "Teng Xiao", "Tyler Murray", "Tyler Romero", "Victoria Graf", "Akari Asai", "Akshita Bhagia", "Alexander Wettig", "Alisa Liu", "Aman Rangapur", "Chloe Anastasiades", "Costa Huang", "Dustin Schwenk", "Harsh Trivedi", "Ian Magnusson", "Jaron Lochner", "Jiacheng Liu", "Lester James V. Miranda", "Maarten Sap", "Malia Morgan", "Michael Schmitz", "Michal Guerquin", "Michael Wilson", "Regan Huff", "Ronan Le Bras", "Rui Xin", "Rulin Shao", "Sam Skjonsberg", "Shannon Zejiang Shen", "Shuyue Stella Li", "Tucker Wilde", "Valentina Pyatkin", "Will Merrill", "Yapei Chang", "Yuling Gu", "Zhiyuan Zeng", "Ashish Sabharwal", "Luke Zettlemoyer", "Pang Wei Koh", "Ali Farhadi", "Noah A. Smith", "Hannaneh Hajishirzi"], "title": "Olmo 3", "comment": null, "summary": "We introduce Olmo 3, a family of state-of-the-art, fully-open language models at the 7B and 32B parameter scales. Olmo 3 model construction targets long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall. This release includes the entire model flow, i.e., the full lifecycle of the family of models, including every stage, checkpoint, data point, and dependency used to build it. Our flagship model, Olmo 3 Think 32B, is the strongest fully-open thinking model released to-date.", "AI": {"tldr": "Olmo 3 是一套面向多种任务优化的全开源语言模型，包括长上下文推理、功能调用、编程、指令遵循、通用对话及知识回忆。其32B参数版本被认为是目前最强的全开源思考模型。", "motivation": "开发全开源的、具备强大性能的语言模型，以应对广泛的任务需求，并提升公开研究的能力和透明度。", "method": "构建了一个家族化的模型系列，包括7B和32B参数规模，设计用于特定的任务优化。", "result": "发布了模型构建的完整流程，提供了模型的全生命周期数据，包括每个阶段、检查点、数据点和依赖项。", "conclusion": "Olmo 3 Think 32B模型被认为是最强的全开源思考模型，为开源社区提供了巨大的价值。"}}
{"id": "2512.13753", "categories": ["cs.CV", "cs.LG", "eess.IV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.13753", "abs": "https://arxiv.org/abs/2512.13753", "authors": ["Mika Sipilä", "Sabrina Maggio", "Sandra De Iaco", "Klaus Nordhausen", "Monica Palma", "Sara Taskinen"], "title": "Time-aware UNet and super-resolution deep residual networks for spatial downscaling", "comment": null, "summary": "Satellite data of atmospheric pollutants are often available only at coarse spatial resolution, limiting their applicability in local-scale environmental analysis and decision-making. Spatial downscaling methods aim to transform the coarse satellite data into high-resolution fields. In this work, two widely used deep learning architectures, the super-resolution deep residual network (SRDRN) and the encoder-decoder-based UNet, are considered for spatial downscaling of tropospheric ozone. Both methods are extended with a lightweight temporal module, which encodes observation time using either sinusoidal or radial basis function (RBF) encoding, and fuses the temporal features with the spatial representations in the networks. The proposed time-aware extensions are evaluated against their baseline counterparts in a case study on ozone downscaling over Italy. The results suggest that, while only slightly increasing computational complexity, the temporal modules significantly improve downscaling performance and convergence speed.", "AI": {"tldr": "本文通过扩展超级分辨率深残差网络（SRDRN）和UNet，加入了轻量级时间模块来改善平流层臭氧的空间降尺度性能。这个时间模块能有效提高降尺度的性能和收敛速度，几乎不增加计算复杂度。", "motivation": "现有卫星大气污染数据通常只有粗分辨率，限制了其在局部环境分析和决策中的应用。本文的动机是通过空间降尺度方法提高这些数据的空间分辨率，从而增强其实用性。", "method": "本文采用了两种流行的深度学习架构，即超级分辨率深残差网络（SRDRN）和编码器-解码器式的UNet，用于平流层臭氧的空间降尺度分析。这两种方法都通过一个轻量级的时间模块进行了扩展，该模块使用正弦或径向基函数（RBF）编码来编码观测时间，并将时间特征与网络中的空间表示融合在一起。", "result": "在意大利臭氧空间降尺度的个案研究中，增加的时间模块在只略微提升计算复杂度的情况下，显著提高了空间降尺度的性能和网络的收敛速度。", "conclusion": "本文的研究表明，增加时间模块之后的空间降尺度方法几乎不增加计算复杂度的同时，显著提高了臭氧数据的空间降尺度性能和网络的收敛速度，证明了时间模块的有效性和实用性。"}}
{"id": "2512.13980", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13980", "abs": "https://arxiv.org/abs/2512.13980", "authors": ["Zhimin Qiu", "Di Wu", "Feng Liu", "Chenrui Hu", "Yuxiao Wang"], "title": "Structure-Aware Decoding Mechanisms for Complex Entity Extraction with Large-Scale Language Models", "comment": null, "summary": "This paper proposes a structure-aware decoding method based on large language models to address the difficulty of traditional approaches in maintaining both semantic integrity and structural consistency in nested and overlapping entity extraction tasks. The method introduces a candidate span generation mechanism and structured attention modeling to achieve unified modeling of entity boundaries, hierarchical relationships, and cross-dependencies. The model first uses a pretrained language model to obtain context-aware semantic representations, then captures multi-granular entity span features through candidate representation combinations, and introduces hierarchical structural constraints during decoding to ensure consistency between semantics and structure. To enhance stability in complex scenarios, the model jointly optimizes classification loss and structural consistency loss, maintaining high recognition accuracy under multi-entity co-occurrence and long-sentence dependency conditions. Experiments conducted on the ACE 2005 dataset demonstrate significant improvements in Accuracy, Precision, Recall, and F1-Score, particularly in nested and overlapping entity recognition, where the model shows stronger boundary localization and structural modeling capability. This study verifies the effectiveness of structure-aware decoding in complex semantic extraction tasks, provides a new perspective for developing language models with hierarchical understanding, and establishes a methodological foundation for high-precision information extraction.", "AI": {"tldr": "A structure-aware decoding method, using large language models, is developed for entity extraction tasks, focusing on semantic and structural consistency to enhance performance in complex scenarios.", "motivation": "Traditional entity extraction methods struggle to maintain both semantic integrity and structural consistency, especially in tasks involving nested or overlapping entities. This work aims to address this limitation by proposing a structure-aware methodology.", "method": "The paper introduces a structure-aware decoding method based on large language models to maintain both semantic integrity and structural consistency in nested and overlapping entity extraction. It employs a candidate span generation mechanism and structured attention to unify the modeling of entity boundaries, hierarchical relationships, and cross-dependencies. The model combines candidate representations to capture multi-granular span features, incorporates hierarchical structural constraints in the decoding process, and optimizes for both classification and structural consistency losses to improve performance in complex scenarios.", "result": "Experiments on the ACE 2005 dataset show improved Accuracy, Precision, Recall, and F1-Score, making significant strides particularly in the recognition of nested and overlapping entities, and demonstrating better boundary localization and structural modeling capabilities.", "conclusion": "The research confirms the effectiveness of structure-aware decoding in enhancing information extraction for complex semantic tasks. It proposes a new perspective for developing language models with hierarchical understanding and is foundational for achieving high-precision information extraction."}}
{"id": "2512.13796", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13796", "abs": "https://arxiv.org/abs/2512.13796", "authors": ["Victor Rong", "Jan Held", "Victor Chu", "Daniel Rebain", "Marc Van Droogenbroeck", "Kiriakos N. Kutulakos", "Andrea Tagliasacchi", "David B. Lindell"], "title": "Nexels: Neurally-Textured Surfels for Real-Time Novel View Synthesis with Sparse Geometries", "comment": "Webpage at https://lessvrong.com/cs/nexels", "summary": "Though Gaussian splatting has achieved impressive results in novel view synthesis, it requires millions of primitives to model highly textured scenes, even when the geometry of the scene is simple. We propose a representation that goes beyond point-based rendering and decouples geometry and appearance in order to achieve a compact representation. We use surfels for geometry and a combination of a global neural field and per-primitive colours for appearance. The neural field textures a fixed number of primitives for each pixel, ensuring that the added compute is low. Our representation matches the perceptual quality of 3D Gaussian splatting while using $9.7\\times$ fewer primitives and $5.5\\times$ less memory on outdoor scenes and using $31\\times$ fewer primitives and $3.7\\times$ less memory on indoor scenes. Our representation also renders twice as fast as existing textured primitives while improving upon their visual quality.", "AI": {"tldr": "本文提出了一种新的表示方法，该方法分解几何形状与外观，使用surfels表示几何形状，结合全局神经场和每个图元的颜色来表示外观，实现了比3D高斯散斑更高效且高质量的表示方法。", "motivation": "虽然高斯散斑在新视角合成中取得了显著成果，但它需要数以百万计的图元来表示高度纹理化的场景，即使场景的几何形状很简单。文章旨在提出一种更紧凑的表示方法。", "method": "文章使用surfels来表示几何形状，并结合全球神经场和每个图元的颜色来表示外观。神经场为每个像素固定数量的图元进行纹理处理，确保计算量较低。", "result": "新表示法在户外场景中使用比3D高斯散斑少9.7倍的图元和少5.5倍的内存，在室内场景中分别使用31倍和3.7倍更少的图元和内存。同时，渲染速度是现有纹理图元的两倍，并改善了视觉质量。", "conclusion": "新方法既能保证感知质量，又能显著减少图元和内存的使用，同时提高渲染速度。"}}
{"id": "2512.14064", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.14064", "abs": "https://arxiv.org/abs/2512.14064", "authors": ["Yi Hu", "Cai Zhou", "Muhan Zhang"], "title": "What Affects the Effective Depth of Large Language Models?", "comment": null, "summary": "The scaling of large language models (LLMs) emphasizes increasing depth, yet performance gains diminish with added layers. Prior work introduces the concept of \"effective depth\", arguing that deeper models fail to fully utilize their layers for meaningful computation. Building on this, we systematically study how effective depth varies with model scale, training type, and task difficulty. First, we analyze the model behavior of Qwen-2.5 family (1.5B-32B) and find that while the number of effective layers grows with model size, the effective depth ratio remains stable. Besides, comparisons between base and corresponding long-CoT models show no increase in effective depth, suggesting that improved reasoning stems from longer context rather than deeper per-token computation. Furthermore, evaluations across tasks of varying difficulty indicate that models do not dynamically use more layers for harder problems. Our results suggest that current LLMs underuse available depth across scales, training paradigms and tasks of varying difficulties, pointing out research opportunities on increasing the layer utilization rate of LLMs, model pruning, and early exiting. Our code is released at https://github.com/AheadOFpotato/what_affects_effective_depth.", "AI": {"tldr": "研究了Qwen-2.5家族模型的有效深度，发现模型大小增加时有效层数量增长但有效深度比率稳定，推理改善是由于更长的上下文而非更深的计算，对于更难的任务模型未动态增加使用的层数。这些结果指出未来LLM在提高层利用率上的研究方向。", "motivation": "针对现有研究中提到的大型语言模型随着层数增加其性能增长逐渐放缓的问题，提出研究“有效深度”的概念，以探索深层模型未能充分利用其所有层进行有意义计算的原因。", "method": "通过系统性研究不同规模的Qwen-2.5家族模型（从1.5B到32B）的行为，以及基本模型与相应的long-CoT模型之间的对比，分析有效深度与模型规模、训练类型及任务难度之间的关系。目的在于了解模型是否能够有效地利用所有层进行计算，以及如何进一步提高LLM的深度利用率。", "result": "发现虽然模型大小增加时有效层的数量也随之增加，但有效深度比率保持稳定。base模型及其对应的long-CoT模型之间没有有效的深层增长，表明改进的推理能力主要归因于较长的上下文，而非更深的逐层计算。在不同难度的任务上，模型使用的层数量并没有由于任务难度的增加而动态增加。", "conclusion": "当前大型语言模型在不同规模、训练范式和不同难度的任务中均未充分利用深度。这为研究如何提高LLM的层利用率、模型剪枝和提前退出机制提供了机会。"}}
{"id": "2512.13834", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13834", "abs": "https://arxiv.org/abs/2512.13834", "authors": ["Naman Balbir Singh Makkar"], "title": "VajraV1 -- The most accurate Real Time Object Detector of the YOLO family", "comment": "Technical Report. 20 Pages, 7 figures", "summary": "Recent years have seen significant advances in real-time object detection, with the release of YOLOv10, YOLO11, YOLOv12, and YOLOv13 between 2024 and 2025. This technical report presents the VajraV1 model architecture, which introduces architectural enhancements over existing YOLO-based detectors. VajraV1 combines effective design choices from prior YOLO models to achieve state-of-the-art accuracy among real-time object detectors while maintaining competitive inference speed.\n  On the COCO validation set, VajraV1-Nano achieves 44.3% mAP, outperforming YOLOv12-N by 3.7% and YOLOv13-N by 2.7% at latency competitive with YOLOv12-N and YOLOv11-N. VajraV1-Small achieves 50.4% mAP, exceeding YOLOv12-S and YOLOv13-S by 2.4%. VajraV1-Medium achieves 52.7% mAP, outperforming YOLOv12-M by 0.2%. VajraV1-Large achieves 53.7% mAP, surpassing YOLOv13-L by 0.3%. VajraV1-Xlarge achieves 56.2% mAP, outperforming all existing real-time object detectors.", "AI": {"tldr": "VajraV1 model outperforms current YOLO-based detectors with state-of-the-art accuracy and competitive inference speed on real-time object detection.", "motivation": "To improve the accuracy and speed of real-time object detection beyond existing YOLO-based detectors, introducing VajraV1 as a more effective architecture.", "method": "VajraV1 integrates effective architectural design choices from previous YOLO models to enhance the model's performance in terms of accuracy while keeping the inference speed competitive.", "result": "On the COCO validation set, VajraV1 achieves superior mAP scores compared to the most recent YOLO versions in different model sizes, demonstrating its state-of-the-art performance.", "conclusion": "VajraV1 establishes itself as a leading real-time object detector with the highest accuracy among comparable models and remains competitive in terms of inference speed."}}
{"id": "2512.14067", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.14067", "abs": "https://arxiv.org/abs/2512.14067", "authors": ["Yonggan Fu", "Lexington Whalen", "Zhifan Ye", "Xin Dong", "Shizhe Diao", "Jingyu Liu", "Chengyue Wu", "Hao Zhang", "Enze Xie", "Song Han", "Maksim Khadkevich", "Jan Kautz", "Yingyan Celine Lin", "Pavlo Molchanov"], "title": "Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed", "comment": null, "summary": "Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively.", "AI": {"tldr": "研究了一种有效的自回归模型到扩散语言模型的转换方法，通过创新的注意力模式和令牌掩码策略提升模型效率和准确度。", "motivation": "扩散语言模型（dLMs）作为一个有潜力的范式，能够实现并行、非自回归生成，然而其学习效率在从零开始训练时不如自回归（AR）语言模型。因此，本研究旨在研究从AR模型转换成高效的dLM模型的方法，以提高生成速度同时保持AR模型的任务精度。", "method": "本研究通过系统性比较不同注意力模式，提出了一种保持预训练自回归模型权重分布的连续预训练方案，采用块状注意模式，同时维持块间的因果关系，块内则启用双向建模。另外，为了解决训练和测试阶段掩码令牌分布不一致的问题，还提出了一种位置依赖的令牌掩码策略，以更好地模仿测试时的行为。", "result": "该研究展示了提出的框架如何更好地保存预训练AR模型的权重分布，同时实现加速。在多项实验中，该框架转化后的Efficient-DLM家族模型在准确率和效率上都优于现有的AR模型和dLM模型，例如Efficient-DLM 8B相比Dream 7B和Qwen3 4B，分别获得了+5.4%/+2.7%的准确度提升和4.5x/2.7x的吞吐量提升。", "conclusion": "本研究提出了一种更有效的自回归到扩散语言模型的转换方法，提供了关于注意力模式、训练动态和其他设计选择的实用见解。通过这种方式，研究促进了更高效的模型转换，显著提升了模型的性能。"}}
{"id": "2512.13840", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13840", "abs": "https://arxiv.org/abs/2512.13840", "authors": ["Yannan He", "Garvita Tiwari", "Xiaohan Zhang", "Pankaj Bora", "Tolga Birdal", "Jan Eric Lenssen", "Gerard Pons-Moll"], "title": "MoLingo: Motion-Language Alignment for Text-to-Motion Generation", "comment": "Project page: https://hynann.github.io/molingo/MoLingo.html", "summary": "We introduce MoLingo, a text-to-motion (T2M) model that generates realistic, lifelike human motion by denoising in a continuous latent space. Recent works perform latent space diffusion, either on the whole latent at once or auto-regressively over multiple latents. In this paper, we study how to make diffusion on continuous motion latents work best. We focus on two questions: (1) how to build a semantically aligned latent space so diffusion becomes more effective, and (2) how to best inject text conditioning so the motion follows the description closely. We propose a semantic-aligned motion encoder trained with frame-level text labels so that latents with similar text meaning stay close, which makes the latent space more diffusion-friendly. We also compare single-token conditioning with a multi-token cross-attention scheme and find that cross-attention gives better motion realism and text-motion alignment. With semantically aligned latents, auto-regressive generation, and cross-attention text conditioning, our model sets a new state of the art in human motion generation on standard metrics and in a user study. We will release our code and models for further research and downstream usage.", "AI": {"tldr": "提出了MoLingo，一种新的文本到动作模型，通过改进潜在空间和引入交叉注意力机制，实现了更好的人类动作生成，达到新标杆。", "motivation": "研究动机在于优化文本到动作（T2M）生成的过程，通过改进潜在空间的语义对齐和文本条件实现更自然，更逼真的动作生成。", "method": "本文介绍了MoLingo模型，一种通过在连续潜在空间中去噪来生成逼真的人类动作的文本到动作（T2M）模型。研究了如何使连续动作潜在空间的扩散更有效，并提出了一个用帧级文本标签训练的语义对齐动作编码器以及对比了单标记条件与多标记交叉注意力方案，发现交叉注意力能提供更好的动作真实感和文本-动作一致性。", "result": "该模型在标准指标和用户研究中实现了人类动作生成的新里程碑。", "conclusion": "MoLingo模型通过语义对齐的潜在空间，自回归生成和交叉注意力文本条件，达到了动作生成的新水平，并将开源其代码和模型以供进一步研究和应用。"}}
{"id": "2512.14082", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.14082", "abs": "https://arxiv.org/abs/2512.14082", "authors": ["Siran Liu", "Zane Cao", "Yongchao He"], "title": "A Unified Sparse Attention via Multi-Granularity Compression", "comment": null, "summary": "Efficient long-context understanding and reasoning are increasingly vital for large language model (LLM) applications such as multi-turn dialogue and program analysis. However, the core self-attention mechanism scales quadratically with sequence length, creating a fundamental computational bottleneck. Existing sparse attention methods alleviate this issue but face trade-offs: training-based methods are costly and cannot be directly applied as acceleration plugins for other models, while inference-time methods often compromise efficiency or cross-modal generality. To address these limitations, we present UniSparse, a unified mechanism that introduces the notion of composite tokens--compact representations that aggregate multi-granularity contextual information. Building on this abstraction, UniSparse dynamically constructs sparse attention through multi-granularity compression and block-level selection, enabling efficient and hardware-friendly execution on GPU. Across multiple modalities and tasks ranging from synthetic benchmarks to real-world applications, UniSparse consistently surpasses state-of-the-art sparse attention methods (e.g., MInference, XAttention, FlexPrefill) in both accuracy and efficiency, achieving $\\ge$ 99% of full-attention accuracy and up to 2.61$\\times$ faster attention computation than FlashAttention.", "AI": {"tldr": "提出UniSparse机制，解决了现有稀疏注意力方法的局限性，显著提升了大型语言模型在长期上下文任务中的效率和准确率。", "motivation": "解决自注意力机制的二次缩放问题，提高大型语言模型在多轮对话和程序分析中的长期上下文理解和推理效率。", "method": "UniSparse, 通过复合令牌（紧凑表示，聚合多粒度上下文信息）的概念，动态构建稀疏注意力，实现高效的GPU执行。", "result": "UniSparse在多个模式和任务上持续超越了最先进的稀疏注意力方法，达到至少99%的全注意力准确率，提高了2.61倍的注意力计算速度。", "conclusion": "UniSparse作为统一机制，通过多粒度压缩和块级选择动态构建稀疏注意力，实现了在不牺牲准确率的同时大幅提高计算效率。"}}
{"id": "2512.13855", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13855", "abs": "https://arxiv.org/abs/2512.13855", "authors": ["Ujjwal Mishra", "Vinita Shukla", "Praful Hambarde", "Amit Shukla"], "title": "Improvise, Adapt, Overcome -- Telescopic Adapters for Efficient Fine-tuning of Vision Language Models in Medical Imaging", "comment": "Accepted at the IEEE/CVF winter conference on applications of computer vision (WACV 2026)", "summary": "Adapting Vision Language Segmentation Models (VLSMs) to medical imaging domains requires significant computational overhead when using conventional fine-tuning approaches. Existing Parameter-Efficient Fine-Tuning (PEFT) methods apply uniform adapter dimensions across all transformer layers, leading to suboptimal parameter allocation and reduced adaptation efficiency. We introduce Telescopic Adapters, a novel PEFT framework that employs depth-aware scaling to progressively increase adapter capacity from shallow to deep transformer layers. Our method integrates lightweight bottleneck modules within CLIPSeg's vision and text encoders, with adapter dimensions dynamically scaled based on layer depth and semantic relevance. Using only 613k trainable parameters--244x fewer than end-to-end fine-tuning, Telescopic Adapters achieve superior performance across five diverse medical datasets spanning polyp segmentation, skin lesion detection, and breast ultrasound imaging. Comprehensive ablation studies demonstrate that deeper layers require substantially more adaptation capacity than shallow layers, validating our telescopic scaling hypothesis. Our approach establishes a new paradigm for efficient medical VLSM fine-tuning, enabling deployment in resource-constrained clinical environments while maintaining competitive segmentation accuracy.", "AI": {"tldr": "本文提出了Telescopic Adapters这是一种新的参数有效微调框架，仅使用613k的可训练参数，在五个不同的医学数据集上显著优于现有的微调方法，适应性强且适用于资源有限的临床环境。", "motivation": "现有的参数有效微调方法在所有变换器层中应用统一的适配器维度，导致参数分配次优和适应效率降低，尤其是在将视觉语言分割模型适应到医学成像领域时需要显著的计算开销。", "method": "我们提出了望远镜适配器（Telescopic Adapters），这是一种新的参数有效微调框架，它采用深度感知缩放方法，从浅层到深层逐步增加适配器的容量。该方法在CLIPSeg的视觉和文本编码器中集成了轻量级的瓶颈模块，并根据层次深度和语义相关性动态调整适配器的维度。", "result": "使用仅有613k的可训练参数，望远镜适配器在五个不同的医学数据集上实现了优越的性能，涵盖了息肉分割、皮肤病变检测和乳腺超声成像等领域。并且，深入的消融研究证实了深层需要比浅层更多的适应性容量，验证了我们的望远镜缩放假设。", "conclusion": "我们的方法为有效的医学VLSM微调建立了一个新范式，使得可以在资源有限的临床环境中部署这些模型，同时保持有竞争力的分割精度。"}}
{"id": "2512.14085", "categories": ["cs.CL", "cs.HC", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.14085", "abs": "https://arxiv.org/abs/2512.14085", "authors": ["Koji Inoue", "Mikey Elmers", "Yahui Fu", "Zi Haur Pang", "Taiga Mori", "Divesh Lala", "Keiko Ochi", "Tatsuya Kawahara"], "title": "Multilingual and Continuous Backchannel Prediction: A Cross-lingual Study", "comment": "This paper has been accepted for presentation at International Workshop on Spoken Dialogue Systems Technology 2026 (IWSDS 2026) and represents the author's version of the work", "summary": "We present a multilingual, continuous backchannel prediction model for Japanese, English, and Chinese, and use it to investigate cross-linguistic timing behavior. The model is Transformer-based and operates at the frame level, jointly trained with auxiliary tasks on approximately 300 hours of dyadic conversations. Across all three languages, the multilingual model matches or surpasses monolingual baselines, indicating that it learns both language-universal cues and language-specific timing patterns. Zero-shot transfer with two-language training remains limited, underscoring substantive cross-lingual differences. Perturbation analyses reveal distinct cue usage: Japanese relies more on short-term linguistic information, whereas English and Chinese are more sensitive to silence duration and prosodic variation; multilingual training encourages shared yet adaptable representations and reduces overreliance on pitch in Chinese. A context-length study further shows that Japanese is relatively robust to shorter contexts, while Chinese benefits markedly from longer contexts. Finally, we integrate the trained model into a real-time processing software, demonstrating CPU-only inference. Together, these findings provide a unified model and empirical evidence for how backchannel timing differs across languages, informing the design of more natural, culturally-aware spoken dialogue systems.", "AI": {"tldr": "本文设计了一种多语言连续预测回声响应的模型，涵盖日语、英语和汉语。实验表明该模型能学习到语言普遍和特定的时间线索，并提供了关于不同语言间回声响应行为差异的实证证据。", "motivation": "本研究的动机在于探索跨语言对话中的回声响应行为，通过设计一种多语言的模型来识别和预测回声响应信号的时间点，以促进更自然的口语对话系统的发展。", "method": "本研究提出了一种多语言连续的回声响应（backchannel）预测模型，涵盖了日语、英语和汉语。该模型基于Transformer架构，在帧级别上运行，并通过大约300小时的二元对话数据进行联合训练，辅以辅助任务。", "result": "结果表明，在所有三种语言中，该多语言模型的表现可与单语言基线模型匹敌甚至超越，说明模型学习了跨语言普遍的时间线索和特定语言的时间模式。此外，研究还揭示了不同语言对线索的使用方式存在差异：日语更依赖短期的语境信息，而英语和汉语则更关注静默时长和语调变化。", "conclusion": "总而言之，这些发现提供了一个统一的模型和实证证据，展示了不同语言如何在回声响应的时机上有所不同，为进一步设计更为自然且文化敏感的对话系统提供了指导。"}}
{"id": "2512.13869", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13869", "abs": "https://arxiv.org/abs/2512.13869", "authors": ["Wenda Li", "Meng Wu", "Sungmin Eum", "Heesung Kwon", "Qing Qu"], "title": "Coarse-to-Fine Hierarchical Alignment for UAV-based Human Detection using Diffusion Models", "comment": null, "summary": "Training object detectors demands extensive, task-specific annotations, yet this requirement becomes impractical in UAV-based human detection due to constantly shifting target distributions and the scarcity of labeled images. As a remedy, synthetic simulators are adopted to generate annotated data, with a low annotation cost. However, the domain gap between synthetic and real images hinders the model from being effectively applied to the target domain. Accordingly, we introduce Coarse-to-Fine Hierarchical Alignment (CFHA), a three-stage diffusion-based framework designed to transform synthetic data for UAV-based human detection, narrowing the domain gap while preserving the original synthetic labels. CFHA explicitly decouples global style and local content domain discrepancies and bridges those gaps using three modules: (1) Global Style Transfer -- a diffusion model aligns color, illumination, and texture statistics of synthetic images to the realistic style, using only a small real reference set; (2) Local Refinement -- a super-resolution diffusion model is used to facilitate fine-grained and photorealistic details for the small objects, such as human instances, preserving shape and boundary integrity; (3) Hallucination Removal -- a module that filters out human instances whose visual attributes do not align with real-world data to make the human appearance closer to the target distribution. Extensive experiments on public UAV Sim2Real detection benchmarks demonstrate that our methods significantly improve the detection accuracy compared to the non-transformed baselines. Specifically, our method achieves up to $+14.1$ improvement of mAP50 on Semantic-Drone benchmark. Ablation studies confirm the complementary roles of the global and local stages and highlight the importance of hierarchical alignment. The code is released at \\href{https://github.com/liwd190019/CFHA}{this url}.", "AI": {"tldr": "The paper presents CFHA, a hierarchical diffusion-based framework to bridge the domain gap between synthetic and real UAV images for improving human detection accuracy while keeping the original annotations.", "motivation": "The aim is to enhance the effectiveness of UAV-based human detection using synthetic data, due to the impracticality of obtaining enough labeled real-world images. CFHA addresses the domain gap issue between synthetic and real images.", "method": "Coarse-to-Fine Hierarchical Alignment (CFHA) is a three-stage diffusion-based framework for transforming synthetic UAV data to improve real-world human detection by addressing the domain gap without losing original labels. It includes Global Style Transfer, Local Refinement, and Hallucination Removal modules.", "result": "Experiments on UAV Sim2Real benchmarks show significant improvements in human detection accuracy. The method achieves up to a $+14.1$ mAP50 improvement on the Semantic-Drone benchmark compared to non-transformed baseline methods.", "conclusion": "The research demonstrates the effectiveness of CFHA in enhancing UAV-based human detection accuracy by reducing the domain gap between synthetic and real images, which is crucial for applications with limited labeled data availability."}}
{"id": "2512.14118", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.14118", "abs": "https://arxiv.org/abs/2512.14118", "authors": ["Yiran Zhang", "Jincheng Hu", "Mark Dras", "Usman Naseem"], "title": "CogMem: A Cognitive Memory Architecture for Sustained Multi-Turn Reasoning in Large Language Models", "comment": "underreview", "summary": "Large language models (LLMs) excel at single-turn reasoning but often lose accuracy and coherence over extended, multi-turn interactions. Recent evaluations such as TurnBench highlight recurring failure modes-reasoning bias, task drift, hallucination, overconfidence, and memory decay. Current approaches typically append full conversational histories, causing unbounded context growth, higher computational costs, and degraded reasoning efficiency. We introduce CogMem, a cognitively inspired, memory-augmented LLM architecture that supports sustained iterative reasoning through structured, persistent memory. CogMem incorporates three layers: a Long-Term Memory (LTM) that consolidates cross-session reasoning strategies; a Direct Access (DA) memory that maintains session-level notes and retrieves relevant long-term memories; and a Focus of Attention (FoA) mechanism that dynamically reconstructs concise, task-relevant context at each turn. Experiments on TurnBench show that this layered design mitigates reasoning failures, controls context growth, and improves consistency across extended reasoning chains, moving toward more reliable, human-like reasoning in LLMs.", "AI": {"tldr": "CogMem通过分层设计减轻了推理失败，控制了上下文增长，并改善了在长推理链条上的连贯性，朝着更加可靠、类人的机器推理的方向迈进。", "motivation": "大型语言模型在单次交互时表现出色，但在连续的多轮交互中常常下降推理的准确性与连贯性。现有方法通常是无限制地增加上下文历史记录，这导致计算成本增加和推理效率降低。", "method": "提出了一种受认知启发的记忆增强型LLM架构CogMem，该架构通过结构化持久记忆支持持续迭代推理。CogMem包括三层：长时记忆（LTM）层，用于跨会话推理策略的巩固；直推存储器（DA）层，用于维护会话级笔记并检索相关的长期记忆；以及注意力焦点（FoA）机制，动态重构简洁、任务相关的上下文。", "result": "在TurnBench评估中展示出分层设计减轻了推理错误，控制了上下文增长，提高了不间断推理过程中的连贯性。", "conclusion": "CogMem设计改进了大型语言模型在长篇对话推理任务中的表现，有助于构建更为可靠、类人的推理能力。"}}
{"id": "2512.13874", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13874", "abs": "https://arxiv.org/abs/2512.13874", "authors": ["Jitesh Jain", "Jialuo Li", "Zixian Ma", "Jieyu Zhang", "Chris Dongjoo Kim", "Sangho Lee", "Rohun Tripathi", "Tanmay Gupta", "Christopher Clark", "Humphrey Shi"], "title": "SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning", "comment": "Project Page: https://praeclarumjj3.github.io/sage/", "summary": "As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iteratively skim long videos or watch short ones in full when necessary for a given task. With this in mind, one would expect video reasoning models to reason flexibly across different durations. However, SOTA models are still trained to predict answers in a single turn while processing a large number of frames, akin to watching an entire long video, requiring significant resources. This raises the question: Is it possible to develop performant any-horizon video reasoning systems? Inspired by human behavior, we first propose SAGE, an agent system that performs multi-turn reasoning on long videos while handling simpler problems in a single turn. Secondly, we introduce an easy synthetic data generation pipeline using Gemini-2.5-Flash to train the orchestrator, SAGE-MM, which lies at the core of SAGE. We further propose an effective RL post-training recipe essential for instilling any-horizon reasoning ability in SAGE-MM. Thirdly, we curate SAGE-Bench with an average duration of greater than 700 seconds for evaluating video reasoning ability in real-world entertainment use cases. Lastly, we empirically validate the effectiveness of our system, data, and RL recipe, observing notable improvements of up to 6.1% on open-ended video reasoning tasks, as well as an impressive 8.2% improvement on videos longer than 10 minutes.", "AI": {"tldr": "研究人员提出了一种基于人类行为的任何时长视频推理模型（SAGE），该模型在处理长时间视频时可以多回合推理，实验结果表明，该模型在开放性视频推理任务中表现出了显著的改进。", "motivation": "现有的顶级视频推理模型仍然训练用单次预测答案，这需要处理大量帧，类似于观看完整长视频，这样消耗大量资源。作者认为，视频推理模型应像人类一样，能在不同时间跨度上灵活推理。", "method": "提出了一种名为SAGE的多回合推理代理系统，该系统可以在处理长时间视频时进行多回合推理，并在处理简单问题时一次性完成。同时，利用Gemini-2.5-Flash提出了一个简单的合成数据生成管道来训练SAGE的核心组件SAGE-MM，并提出了一个强化学习后训练配方以赋予SAGE-MM任意时长推理的能力。", "result": "通过实验，该系统在开放性视频推理任务中表现出色，特别是在处理超过10分钟的视频时，改进幅度达到8.2%。", "conclusion": "研究验证了系统、数据和强化学习配方的效果，表明SAGE系统在评估真实世界娱乐用途的视频推理能力方面具有显著优势。"}}
{"id": "2512.14142", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.14142", "abs": "https://arxiv.org/abs/2512.14142", "authors": ["Hongqiu Ni", "Jiabao Zhang", "Guopeng Li", "Zilong Wang", "Ruiqi Wu", "Chi Zhang", "Haisheng Tan"], "title": "Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents", "comment": "12 pages, 8 figures", "summary": "Large Language Models (LLMs) are increasingly being deployed as intelligent agents. Their multi-stage workflows, which alternate between local computation and calls to external network services like Web APIs, introduce a mismatch in their execution pattern and the scheduling granularity of existing inference systems such as vLLM. Existing systems typically focus on per-segment optimization which prevents them from minimizing the end-to-end latency of the complete agentic workflow, i.e., the global Job Completion Time (JCT) over the entire request lifecycle. To address this limitation, we propose Astraea, a service engine designed to shift the optimization from local segments to the global request lifecycle. Astraea employs a state-aware, hierarchical scheduling algorithm that integrates a request's historical state with future predictions. It dynamically classifies requests by their I/O and compute intensive nature and uses an enhanced HRRN policy to balance efficiency and fairness. Astraea also implements an adaptive KV cache manager that intelligently handles the agent state during I/O waits based on the system memory pressure. Extensive experiments show that Astraea reduces average JCT by up to 25.5\\% compared to baseline methods. Moreover, our approach demonstrates strong robustness and stability under high load across various model scales.", "AI": {"tldr": "Astraea是一个服务引擎，通过分级调度和智能代理状态缓存管理，优化大型语言模型多阶段工作流的整体生命周期，减少平均完成时间。", "motivation": "现有的推理系统如vLLM主要关注局部段优化，无法最小化智能代理工作流的端到端延迟。", "method": "Astraea采用了一种基于请求历史状态和未来预测的分级调度算法，动态分类请求以平衡效率和公平性，并实现了一个智能的缓存管理器来处理I/O等待期间的代理状态。", "result": "实验表明，与基线方法相比，Astraea平均将JCT减少了25.5%。此外，该方法在高负载下表现出强大的鲁棒性和稳定性。", "conclusion": "Astraea通过优化请求的整体生命周期，有效降低了大型语言模型智能代理操作的平均完成时间。"}}
{"id": "2512.13876", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13876", "abs": "https://arxiv.org/abs/2512.13876", "authors": ["Ye Zhang", "Qi Chen", "Wenyou Huang", "Rui Liu", "Zhengjian Kang"], "title": "Route-DETR: Pairwise Query Routing in Transformers for Object Detection", "comment": "5 pages, 2 figures", "summary": "Detection Transformer (DETR) offers an end-to-end solution for object detection by eliminating hand-crafted components like non-maximum suppression. However, DETR suffers from inefficient query competition where multiple queries converge to similar positions, leading to redundant computations. We present Route-DETR, which addresses these issues through adaptive pairwise routing in decoder self-attention layers. Our key insight is distinguishing between competing queries (targeting the same object) versus complementary queries (targeting different objects) using inter-query similarity, confidence scores, and geometry. We introduce dual routing mechanisms: suppressor routes that modulate attention between competing queries to reduce duplication, and delegator routes that encourage exploration of different regions. These are implemented via learnable low-rank attention biases enabling asymmetric query interactions. A dual-branch training strategy incorporates routing biases only during training while preserving standard attention for inference, ensuring no additional computational cost. Experiments on COCO and Cityscapes demonstrate consistent improvements across multiple DETR baselines, achieving +1.7% mAP gain over DINO on ResNet-50 and reaching 57.6% mAP on Swin-L, surpassing prior state-of-the-art models.", "AI": {"tldr": "Route-DETR通过自适应成对路由机制解决了DETR的查询竞争低效问题，提高了性能和效率。", "motivation": "解决DETR中因为多个查询收敛到相似位置而导致的冗余计算问题。", "method": "通过在解码器自我注意层中采用自适应成对路由机制来解决Detection Transformer (DETR) 存在的查询竞争低效问题，引入抑制路由和委托路由两种路由机制，使用查询间相似度、置信度得分和几何信息来区分竞争查询和互补查询。通过可学习的低秩注意偏差实现这些机制，使得查询间可以进行非对称交互，同时提出了双分支训练策略，以确保在训练期间仅有路由偏差被采用，而在推理时保留标准注意力机制，不增加额外计算开销。", "result": "实验在COCO和Cityscapes数据集上展示了相对于多个DETR基线模型的一致改进，相较于DINO在ResNet-50上获得了+1.7%的mAP增益，并在Swin-L上达到了57.6%的mAP，超越了先前的最佳模型。", "conclusion": "通过提出的Route-DETR方法，在不增加计算成本的前提下改善了现有DETR模型在目标检测上的性能表现。"}}
{"id": "2512.14179", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.14179", "abs": "https://arxiv.org/abs/2512.14179", "authors": ["K. M. Jubair Sami", "Dipto Sumit", "Ariyan Hossain", "Farig Sadeque"], "title": "A Comparative Analysis of Retrieval-Augmented Generation Techniques for Bengali Standard-to-Dialect Machine Translation Using LLMs", "comment": "Accepted to the Second Workshop on Bangla Language Processing (BLP) at IJCNLP-AACL 2025. 14 pages, 9 figures, 6 tables", "summary": "Translating from a standard language to its regional dialects is a significant NLP challenge due to scarce data and linguistic variation, a problem prominent in the Bengali language. This paper proposes and compares two novel RAG pipelines for standard-to-dialectal Bengali translation. The first, a Transcript-Based Pipeline, uses large dialect sentence contexts from audio transcripts. The second, a more effective Standardized Sentence-Pairs Pipeline, utilizes structured local\\_dialect:standard\\_bengali sentence pairs. We evaluated both pipelines across six Bengali dialects and multiple LLMs using BLEU, ChrF, WER, and BERTScore. Our findings show that the sentence-pair pipeline consistently outperforms the transcript-based one, reducing Word Error Rate (WER) from 76\\% to 55\\% for the Chittagong dialect. Critically, this RAG approach enables smaller models (e.g., Llama-3.1-8B) to outperform much larger models (e.g., GPT-OSS-120B), demonstrating that a well-designed retrieval strategy can be more crucial than model size. This work contributes an effective, fine-tuning-free solution for low-resource dialect translation, offering a practical blueprint for preserving linguistic diversity.", "AI": {"tldr": "The paper proposes and compares two retrieval-augmented generation (RAG) pipelines for translating standard Bengali to dialects, demonstrating that structured sentence pairs and a good retrieval strategy are key to effective translation, even with smaller models.", "motivation": "The motivation is to address the NLP challenge of translating standard languages to regional dialects, specifically for Bengali, due to data scarcity and linguistic variation.", "method": "Two RAG pipelines are proposed: a Transcript-Based Pipeline using large dialect sentence contexts from audio transcripts, and a Standardized Sentence-Pairs Pipeline utilizing structured local_dialect:standard_bengali sentence pairs.", "result": "Evaluation across six Bengali dialects and multiple LLMs showed the sentence-pair pipeline outperformed the transcript-based one, with a reduction in WER from 76% to 55% for the Chittagong dialect, and smaller models outperforming larger ones.", "conclusion": "The study shows that the sentence-pair pipeline is an effective, fine-tuning-free solution for low-resource dialect translation, demonstrating that a well-designed retrieval strategy can be more crucial than model size."}}
{"id": "2512.13902", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13902", "abs": "https://arxiv.org/abs/2512.13902", "authors": ["Anning Tian", "Byunghyun Ko", "Kaichen Qu", "Mengyuan Liu", "Jeongkyu Lee"], "title": "KLO-Net: A Dynamic K-NN Attention U-Net with CSP Encoder for Efficient Prostate Gland Segmentation from MRI", "comment": "Preprint. Accepted to SPIE Medical Imaging 2026: Image Processing", "summary": "Real-time deployment of prostate MRI segmentation on clinical workstations is often bottlenecked by computational load and memory footprint. Deep learning-based prostate gland segmentation approaches remain challenging due to anatomical variability. To bridge this efficiency gap while still maintaining reliable segmentation accuracy, we propose KLO-Net, a dynamic K-Nearest Neighbor attention U-Net with Cross Stage Partial, i.e., CSP, encoder for efficient prostate gland segmentation from MRI scan. Unlike the regular K-NN attention mechanism, the proposed dynamic K-NN attention mechanism allows the model to adaptively determine the number of attention connections for each spatial location within a slice. In addition, CSP blocks address the computational load to reduce memory consumption. To evaluate the model's performance, comprehensive experiments and ablation studies are conducted on two public datasets, i.e., PROMISE12 and PROSTATEx, to validate the proposed architecture. The detailed comparative analysis demonstrates the model's advantage in computational efficiency and segmentation quality.", "AI": {"tldr": "提出KLO-Net，一种结合动态K-最近邻注意力机制和CSP编码器的U-Net网络，用于提高前列腺MRI分割的实时部署效率和准确性。", "motivation": "解决前列腺MRI实时分割中计算负载和内存占用的瓶颈，同时保持可靠的分割准确性。", "method": "提出了一种结合动态K-最近邻注意力机制和CSP编码器的U-Net结构KLO-Net，这是一种可以在保持分割准确性的同时降低计算负担的方法。", "result": "在PROMISE12和PROSTATEx两个公开数据集上进行了详尽的实验和消融研究，结果表明模型在计算效率和分割质量方面都有所提高。", "conclusion": "证明了KLO-Net在前列腺MRI分割中的有效性和优越性，实现了高效且准确的实时分割。"}}
{"id": "2512.14237", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.14237", "abs": "https://arxiv.org/abs/2512.14237", "authors": ["Estelle Zheng", "Nathan Cerisara", "Sébastien Warichet", "Emmanuel Helbert", "Christophe Cerisara"], "title": "Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets", "comment": null, "summary": "Fine-tuning large language models (LLMs) is often limited by the memory available on commodity GPUs. Parameter-efficient fine-tuning (PEFT) methods such as QLoRA reduce the number of trainable parameters, yet still incur high memory usage induced by the backward pass in the full model. We revisit Ladder Side Tuning (LST), a rarely explored PEFT technique that adds a lightweight side network, and show that it matches QLoRA's compute scaling slope while cutting peak memory by 50\\%. Across different downstream benchmarks spanning natural language understanding, mathematical and LLM-critic tasks, LST has competitive performance with QLoRA's accuracy on average while being much more memory-efficient. This efficiency enables fine-tuning of 7B-parameter models on a single 12 GB consumer GPU with 2k-token contexts, requiring no gradient checkpointing\\textemdash conditions under which QLoRA exhausts memory. Beyond memory efficiency, we also establish scaling laws showing that LST scales similarly to QLoRA. We exploit Ladder's architectural flexibility by introducing xLadder, a depth-extended variant that increases effective depth via cross-connections and shortens chain-of-thought (CoT) at fixed parameter count. Ladder is strong when memory is the bottleneck; xLadder builds on this by enabling deeper reasoning without additional memory overhead.", "AI": {"tldr": "The paper discusses Ladder Side Tuning (LST), a parameter-efficient fine-tuning method for large language models that significantly reduces memory usage compared to QLoRA, enabling the fine-tuning of large models on consumer GPUs.", "motivation": "The motivation is to reduce the memory requirements during the fine-tuning of large language models (LLMs) on commodity GPUs without sacrificing performance.", "method": "Ladder Side Tuning (LST) is a parameter-efficient fine-tuning technique that introduces a lightweight side network, enabling more memory-efficient fine-tuning compared to methods like QLoRA.", "result": "LST matches QLoRA's performance while reducing peak memory usage by 50%. It enables fine-tuning of large models on consumer GPUs under conditions where other methods fail.", "conclusion": "LST is a promising approach for efficient fine-tuning of large language models. The depth-extended variant, xLadder, further enhances reasoning capabilities without added memory overhead."}}
{"id": "2512.13950", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.13950", "abs": "https://arxiv.org/abs/2512.13950", "authors": ["Alban Gauthier", "Valentin Deschaintre", "Alexandre Lanvin", "Fredo Durand", "Adrien Bousseau", "George Drettakis"], "title": "An evaluation of SVBRDF Prediction from Generative Image Models for Appearance Modeling of 3D Scenes", "comment": "Project page: http://repo-sam.inria.fr/nerphys/svbrdf-evaluation Code: http://github.com/graphdeco-inria/svbrdf-evaluation", "summary": "Digital content creation is experiencing a profound change with the advent of deep generative models. For texturing, conditional image generators now allow the synthesis of realistic RGB images of a 3D scene that align with the geometry of that scene. For appearance modeling, SVBRDF prediction networks recover material parameters from RGB images. Combining these technologies allows us to quickly generate SVBRDF maps for multiple views of a 3D scene, which can be merged to form a SVBRDF texture atlas of that scene. In this paper, we analyze the challenges and opportunities for SVBRDF prediction in the context of such a fast appearance modeling pipeline. On the one hand, single-view SVBRDF predictions might suffer from multiview incoherence and yield inconsistent texture atlases. On the other hand, generated RGB images, and the different modalities on which they are conditioned, can provide additional information for SVBRDF estimation compared to photographs. We compare neural architectures and conditions to identify designs that achieve high accuracy and coherence. We find that, surprisingly, a standard UNet is competitive with more complex designs. Project page: http://repo-sam.inria.fr/nerphys/svbrdf-evaluation", "AI": {"tldr": "本文探讨了在快速外观建模管道中，利用深度生成模型（如条件图像生成器和SVBRDF预测网络）进行SVBRDF预测所面临的挑战和机遇，指出单视图SVBRDF预测可能导致多视图不一致性，但生成的RGB图像及条件模式提供了额外信息。研究发现，标准UNet在预测准确性和一致性上表现良好。", "motivation": "面对深度生成模型对数字内容生成的转变，研究旨在分析SVBRDF预测在快速建模流程中的挑战，特别是多视图一致性问题，并评估不同神经网络设计的有效性。", "method": "通过比较不同的神经网络结构和条件，分析其在SVBRDF预测中的表现，测试标准UNet和其他复杂设计的性能。", "result": "研究发现，尽管一些设计更为复杂，但标准UNet在SVBRDF预测的准确性和一致性方面表现得非常有竞争力。", "conclusion": "论文指出，虽然单视图SVBRDF预测存在多视图不一致的挑战，但虚拟生成的RGB图象和条件可提供额外信息，改进SVBRDF预测。标准的UNet在效果上可媲美更复杂的设计。"}}
{"id": "2512.14239", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.14239", "abs": "https://arxiv.org/abs/2512.14239", "authors": ["Juan-José Guzmán-Landa", "Juan-Manuel Torres-Moreno", "Miguel Figueroa-Saavedra", "Ligia Quintana-Torres", "Graham Ranger Martha-Lorena Avendaño-Garrido"], "title": "Two CFG Nahuatl for automatic corpora expansion", "comment": "15 pages, 5 figures, 8 tables", "summary": "The aim of this article is to introduce two Context-Free Grammars (CFG) for Nawatl Corpora expansion. Nawatl is an Amerindian language (it is a National Language of Mexico) of the $π$-language type, i.e. a language with few digital resources. For this reason the corpora available for the learning of Large Language Models (LLMs) are virtually non-existent, posing a significant challenge. The goal is to produce a substantial number of syntactically valid artificial Nawatl sentences and thereby to expand the corpora for the purpose of learning non contextual embeddings. For this objective, we introduce two new Nawatl CFGs and use them in generative mode. Using these grammars, it is possible to expand Nawatl corpus significantly and subsequently to use it to learn embeddings and to evaluate their relevance in a sentences semantic similarity task. The results show an improvement compared to the results obtained using only the original corpus without artificial expansion, and also demonstrate that economic embeddings often perform better than some LLMs.", "AI": {"tldr": "本文提出了两种Nawatl的上下文自由文法来扩展语料库，生成人工句子，以此来学习词嵌入并通过句子语义相似性任务评估效果。", "motivation": "由于Nawatl语的数字化资源有限，可用于训练大语言模型（LLMs）的语料库几乎不存在，这构成了一个显著的挑战。通过扩展语料库，可以用于学习词嵌入并评估其在句子语义相似度任务中的相关性。", "method": "本文介绍了两种用于Nawatl语料库扩展的上下文自由文法（CFG）。Nawatl是一种美洲原住民语言，是墨西哥的官方语言之一，属于$\\pi$-语言类型，即具有数字资源较少的语言。目的在于生成大量语法正确的Nawatl人工句子，从而扩展语料库，用于学习非上下文词嵌入。", "result": "实验结果显示，与仅使用原始语料库相比，通过人工扩展后的语料库在语义相似度任务上表现更好，且经济型嵌入往往优于一些大语言模型。", "conclusion": "通过生产和评估人工生成的Nawatl语料库来扩展原始数据，能够显著改善句子语义相似度任务的表现，并且表明经济型嵌入的表现可优于一些大型语言模型。"}}
{"id": "2512.13953", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13953", "abs": "https://arxiv.org/abs/2512.13953", "authors": ["Dawid Malarz", "Artur Kasymov", "Filip Manjak", "Maciej Zięba", "Przemysław Spurek"], "title": "From Unlearning to UNBRANDING: A Benchmark for Trademark-Safe Text-to-Image Generation", "comment": null, "summary": "The rapid progress of text-to-image diffusion models raises significant concerns regarding the unauthorized reproduction of trademarked content. While prior work targets general concepts (e.g., styles, celebrities), it fails to address specific brand identifiers. Crucially, we note that brand recognition is multi-dimensional, extending beyond explicit logos to encompass distinctive structural features (e.g., a car's front grille). To tackle this, we introduce unbranding, a novel task for the fine-grained removal of both trademarks and subtle structural brand features, while preserving semantic coherence. To facilitate research, we construct a comprehensive benchmark dataset. Recognizing that existing brand detectors are limited to logos and fail to capture abstract trade dress (e.g., the shape of a Coca-Cola bottle), we introduce a novel evaluation metric based on Vision Language Models (VLMs). This VLM-based metric uses a question-answering framework to probe images for both explicit logos and implicit, holistic brand characteristics. Furthermore, we observe that as model fidelity increases, with newer systems (SDXL, FLUX) synthesizing brand identifiers more readily than older models (Stable Diffusion), the urgency of the unbranding challenge is starkly highlighted. Our results, validated by our VLM metric, confirm unbranding is a distinct, practically relevant problem requiring specialized techniques. Project Page: https://gmum.github.io/UNBRANDING/.", "AI": {"tldr": "介绍了unbranding任务，通过移除图像中的品牌标识和结构特征，同时保持图像的语义一致性，以应对文本到图像模型生成品牌内容的挑战。", "motivation": "快速发展的文本到图像扩散模型引发了对未经授权复制受商标保护内容的担忧。尽管先前的工作针对的是普通概念，它们未能解决具体的商标标志问题。因此，我们提出了unbranding任务。", "method": "我们引入了unbranding，这是一个新的任务，旨在精细地移除商标和微妙的结构品牌特征，同时保持语义的一致性。为了促进研究，我们构建了一个全面的基准数据集，并提出了一种基于视觉语言模型（VLM）的新评估指标。", "result": "通过VLM指标验证得到的结果肯定了unbranding作为一个独特且具有实际意义问题的地位，它需要专门的技术来解决。", "conclusion": "unbranding是一个独特且具有实际意义的问题，需要专门的技术来解决。特别是随着模型保真度的提高，这一问题显得更加紧迫。"}}
{"id": "2512.14244", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.14244", "abs": "https://arxiv.org/abs/2512.14244", "authors": ["Yiqing Zhou", "Yu Lei", "Shuzheng Si", "Qingyan Sun", "Wei Wang", "Yifei Wu", "Hao Wen", "Gang Chen", "Fanchao Qi", "Maosong Sun"], "title": "From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition", "comment": null, "summary": "Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.", "AI": {"tldr": "我们提出了基于EDU的上下文压缩器，这是一种新型的显式压缩框架，旨在保留全局结构和细粒度细节。实验表明它在多方面优于现有模型。", "motivation": "管理广泛的上下文仍然是大型语言模型的关键瓶颈。现有的压缩技术通常会破坏局部连贯性或依赖隐式潜在编码，存在着位置偏差和与闭源API不兼容的问题。", "method": "我们的方法将上下文压缩重新定义为一个先构建结构再选择的过程。首先，使用LingoEDU将线性文本转换为由基本话语单元（EDUs）构成的结构关系树，并严格锚定于源索引以消除幻觉。其次，轻量级排名模块选择与查询相关的子树进行线性化。", "result": "实验证明，我们的方法在结构预测精度方面达到当前最优水平，并显著优于前沿的大型语言模型，同时降低了成本。此外，结构感知压缩在包括长上下文任务和复杂深度搜索场景在内的下游任务中显著提高了性能。", "conclusion": "提出的方法在保持上下文结构的同时优化了模型性能和成本，具有显著的应用价值。"}}
{"id": "2512.13970", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13970", "abs": "https://arxiv.org/abs/2512.13970", "authors": ["Miaohua Zhang", "Mohammad Ali Armin", "Xuesong Li", "Sisi Liang", "Lars Petersson", "Changming Sun", "David Ahmedt-Aristizabal", "Zeeshan Hayder"], "title": "Quality-Driven and Diversity-Aware Sample Expansion for Robust Marine Obstacle Segmentation", "comment": "10 pages", "summary": "Marine obstacle detection demands robust segmentation under challenging conditions, such as sun glitter, fog, and rapidly changing wave patterns. These factors degrade image quality, while the scarcity and structural repetition of marine datasets limit the diversity of available training data. Although mask-conditioned diffusion models can synthesize layout-aligned samples, they often produce low-diversity outputs when conditioned on low-entropy masks and prompts, limiting their utility for improving robustness. In this paper, we propose a quality-driven and diversity-aware sample expansion pipeline that generates training data entirely at inference time, without retraining the diffusion model. The framework combines two key components:(i) a class-aware style bank that constructs high-entropy, semantically grounded prompts, and (ii) an adaptive annealing sampler that perturbs early conditioning, while a COD-guided proportional controller regulates this perturbation to boost diversity without compromising layout fidelity. Across marine obstacle benchmarks, augmenting training data with these controlled synthetic samples consistently improves segmentation performance across multiple backbones and increases visual variation in rare and texture-sensitive classes.", "AI": {"tldr": "提出一种质量驱动和多样性感知的样本扩展流水线，用于提高海洋障碍物检测条件下的分割性能。该方法在不重新训练扩散模型的情况下，利用推理时间自动生成训练数据，以提高数据多样性和分割准确性。", "motivation": "海洋障碍物检测要求在光照反光、雾气和快速变化的波浪模式等条件下具有强大的分割能力。这些因素会降低图像质量，而海洋数据集的稀缺性和结构重复性限制了训练数据的多样性。尽管条件扩散模型可以合成布局对齐的样本，但在低熵掩码和提示条件下单独生成的输出多样性往往不足。", "method": "提出了一种质量驱动和多样性感知的样本扩展流水线，该流水线完全在推理时间内生成训练数据，无需重新训练扩散模型。框架结合了两个关键组件：(i) 类感知样式银行，构建高熵、语义基础的提示；(ii) 自适应退火采样器，在编码初期引入扰动，同时采用COD引导的比例控制器来调整这一扰动，以提升多样性而不会影响布局保真度。", "result": "在海洋障碍物基准测试中，通过人为控制的合成样本扩充训练数据，无论使用哪种骨干网络，该方法始终能提升分割性能，增加稀有纹理敏感类别的视觉变化。", "conclusion": "提出的方法能够有效应对海洋障碍物检测中的多样性不足问题，并提高了多个骨干网络的分割性能。通过在推理时间内生成多样性的训练数据，无需重新训练模型，显著提升了检测效果。"}}
{"id": "2512.14306", "categories": ["cs.CL", "econ.EM"], "pdf": "https://arxiv.org/pdf/2512.14306", "abs": "https://arxiv.org/abs/2512.14306", "authors": ["Nikoleta Anesti", "Edward Hill", "Andreas Joseph"], "title": "Inflation Attitudes of Large Language Models", "comment": "41 pages, 11 figures", "summary": "This paper investigates the ability of Large Language Models (LLMs), specifically GPT-3.5-turbo (GPT), to form inflation perceptions and expectations based on macroeconomic price signals. We compare the LLM's output to household survey data and official statistics, mimicking the information set and demographic characteristics of the Bank of England's Inflation Attitudes Survey (IAS). Our quasi-experimental design exploits the timing of GPT's training cut-off in September 2021 which means it has no knowledge of the subsequent UK inflation surge. We find that GPT tracks aggregate survey projections and official statistics at short horizons. At a disaggregated level, GPT replicates key empirical regularities of households' inflation perceptions, particularly for income, housing tenure, and social class. A novel Shapley value decomposition of LLM outputs suited for the synthetic survey setting provides well-defined insights into the drivers of model outputs linked to prompt content. We find that GPT demonstrates a heightened sensitivity to food inflation information similar to that of human respondents. However, we also find that it lacks a consistent model of consumer price inflation. More generally, our approach could be used to evaluate the behaviour of LLMs for use in the social sciences, to compare different models, or to assist in survey design.", "AI": {"tldr": "本文研究了大型语言模型（如GPT-3.5-turbo）根据宏观经济价格信号形成通货膨胀感知和预期的能力。结果表明，GPT在短期内能跟踪调查和官方统计数据，但在理解消费者价格通胀方面存在一定不足。", "motivation": "研究大型语言模型在通货膨胀感知与预期方面的能力，利用类比英央行调查数据的技术手段，评估模型的实际效能。", "method": "通过模拟英央行的调查设计，对比GPT-3.5-turbo模型输出与实际调查数据及官方统计数据，并利用Shapley值分解技术分析模型输出的驱动因素。", "result": "发现GPT模型在短期内能很好地跟踪调查数据和官方统计数据，并能在一定程度上反映实际人口特征对通货膨胀感知的影响，但在构建消费者价格通胀模型方面表现不佳。", "conclusion": "该研究方法可用于评估大型语言模型在社会科学中的行为，以及辅助设计调查问卷，并发现GPT模型能反映通货膨胀感知但缺乏消费者价格通胀的一致模型。"}}
