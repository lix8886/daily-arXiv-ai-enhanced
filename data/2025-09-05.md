<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 33]
- [cs.CV](#cs.CV) [Total: 37]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Speech-Based Cognitive Screening: A Systematic Evaluation of LLM Adaptation Strategies](https://arxiv.org/abs/2509.03525)
*Fatemeh Taherinezhad,Mohamad Javad Momeni Nezhad,Sepehr Karimi,Sina Rashidi,Ali Zolnour,Maryam Dadkhah,Yasaman Haghbin,Hossein AzadMaleki,Maryam Zolnoori*

Main category: cs.CL

> Adaptation strategies for large language models significantly impact speech-based dementia detection performance. Properly adapted text-only models and audio-text models can match commercial systems in identifying undiagnosed dementia cases.

<details>
  <summary>Details</summary>

**Motivation:** Over 50% of US adults with Alzheimer's and related dementias remain undiagnosed, making speech-based screening a promising scalable detection approach.

**Method:** We compared nine text-only models and three multimodal audio-text models for dementia detection using the DementiaBank speech corpus, adapting them through in-context learning, reasoning-augmented prompting, parameter-efficient fine-tuning, and multimodal integration.

**Result:** Class-centroid demonstrations yielded the best in-context learning performance, reasoning enhanced smaller models, and token-level fine-tuning generally resulted in the highest scores. Adding a classification head improved underperforming models. Fine-tuned audio-text systems performed well but not better than the top text-only models.

**Conclusion:** Model adaptation strategies, including demonstration selection, reasoning design, and tuning methods, significantly impact speech-based dementia detection. Properly adapted open-weight models can match or exceed commercial systems in performance.

**Abstract:** Over half of US adults with Alzheimer disease and related dementias remain
undiagnosed, and speech-based screening offers a scalable detection approach.
We compared large language model adaptation strategies for dementia detection
using the DementiaBank speech corpus, evaluating nine text-only models and
three multimodal audio-text models on recordings from DementiaBank speech
corpus. Adaptations included in-context learning with different demonstration
selection policies, reasoning-augmented prompting, parameter-efficient
fine-tuning, and multimodal integration. Results showed that class-centroid
demonstrations achieved the highest in-context learning performance, reasoning
improved smaller models, and token-level fine-tuning generally produced the
best scores. Adding a classification head substantially improved
underperforming models. Among multimodal models, fine-tuned audio-text systems
performed well but did not surpass the top text-only models. These findings
highlight that model adaptation strategies, including demonstration selection,
reasoning design, and tuning method, critically influence speech-based dementia
detection, and that properly adapted open-weight models can match or exceed
commercial systems.

</details>


### [2] [Enhancing Speech Large Language Models through Reinforced Behavior Alignment](https://arxiv.org/abs/2509.03526)
*Yansong Liu,Jiateng Li,Yuan Liu*

Main category: cs.CL

> 本论文提出了一个强化行为对齐(RBA)框架，提高基于语音的语言模型在指令跟随任务上的性能，并展示了其在多种语音任务上的优越性。

<details>
  <summary>Details</summary>

**Motivation:** 由于跨模态差异，现有的基于语音的LLM在遵循指令方面仍然存在显著的性能差距，特别是在处理用户语音的动态变异性时。为了应对这一挑战，提出了一种新的框架。

**Method:** 引入了一个名为强化行为对齐(RBA)的框架，该框架采用自我合成的方法，通过强大的教师LLM生成大量高保真的对齐数据，而不是依赖于人类注释的监督微调。然后使用基于强化学习的方法使SpeechLM的行为与教师对齐。

**Result:** 实验结果表明，该方法有效提高了SpeechLM的指令跟随能力，超过了传统的蒸馏基线性能。此外，RBA方法可以无缝扩展到包括口语问答和语音到文本翻译在内的任务，并仅使用自生成数据在公开基准测试中达到了最先进的性能。

**Conclusion:** RBA框架通过自我合成的方法和强化学习，显著提高了基于语音的LLM的语言生成能力，特别是在指令跟随任务上表现出色，并且在扩展到其他基于语音的任务中也达到了领先的性能水平。

**Abstract:** The recent advancements of Large Language Models (LLMs) have spurred
considerable research interest in extending their linguistic capabilities
beyond text to other modalities, which leads to emergence of speech-based LLMs
(SpeechLMs) with capability of processing user request in either speech or
textual formats. However, owing to inter-modal discrepancies, these SpeechLMs
still exhibit a significant performance gap compared to their text-based LLM
counterparts in instruction-following, particularly when confronted with the
dynamic and variable nature of user speech. To address this challenge, this
paper introduces a framework termed Reinforced Behavior Alignment (RBA),
designed to bolster the language generation proficiency of SpeechLMs. Instead
of relying on supervised fine-tuning from human annotations, RBA employs a
self-synthesis methodology to generate extensive, high-fidelity alignment data
by a powerful teacher LLM. Then SpeechLMs is aligned its behavior with that of
a teacher using a reinforcement learning-based approach. Experimental results
demonstrate that this method effectively enhances the instruction-following
capabilities of SpeechLMs that outperform conventional distillation baselines.
Crucially, we demonstrate that RBA can be seamlessly extended to tasks such
including spoken question answering and speech-to-text translation, attaining
state-of-the-art performance on open benchmarks with only self-generated data.

</details>


### [3] [Multilevel Analysis of Cryptocurrency News using RAG Approach with Fine-Tuned Mistral Large Language Model](https://arxiv.org/abs/2509.03527)
*Bohdan M. Pavlyshenko*

Main category: cs.CL

> 本文提出了一种多级多任务的加密货币新闻分析方法，使用了微调的Mistral 7B大语言模型，并表明该模型能够有效地提供有价值的新闻分析。

<details>
  <summary>Details</summary>

**Motivation:** 提高大语言模型分析加密货币新闻的能力，提供全面的分析，并通过将新闻表示为知识图谱来减少模型幻觉问题。

**Method:** 本文采用了一种基于检索增强生成（RAG）的微调Mistral 7B大语言模型，用于多级多任务的加密货币新闻分析。在第一级分析中，微调后的模型生成带有情感得分的图和文本摘要，并将其转换为JSON格式。更高层级则通过分层堆叠方式整合这些图基和文本基的摘要，形成全面的报告。

**Result:** 实验结果表明，使用微调后的Mistral 7B大语言模型进行多级加密货币新闻分析可以提供有信息量的定性和定量分析，并提供重要见解。

**Conclusion:** 利用微调的Mistral 7B大语言模型进行加密货币新闻分析，可以克服传统大语言模型的局限性，提供丰富的加密货币市场见解。

**Abstract:** In the paper, we consider multilevel multitask analysis of cryptocurrency
news using a fine-tuned Mistral 7B large language model with
retrieval-augmented generation (RAG).
  On the first level of analytics, the fine-tuned model generates graph and
text summaries with sentiment scores as well as JSON representations of
summaries. Higher levels perform hierarchical stacking that consolidates sets
of graph-based and text-based summaries as well as summaries of summaries into
comprehensive reports. The combination of graph and text summaries provides
complementary views of cryptocurrency news. The model is fine-tuned with 4-bit
quantization using the PEFT/LoRA approach. The representation of cryptocurrency
news as knowledge graph can essentially eliminate problems with large language
model hallucinations.
  The obtained results demonstrate that the use of fine-tuned Mistral 7B LLM
models for multilevel cryptocurrency news analysis can conduct informative
qualitative and quantitative analytics, providing important insights.

</details>


### [4] [The ProLiFIC dataset: Leveraging LLMs to Unveil the Italian Lawmaking Process](https://arxiv.org/abs/2509.03528)
*Matilde Contestabile,Chiara Ferrara,Alberto Giovannetti,Giovanni Parrillo,Andrea Vandin*

Main category: cs.CL

> 文章介绍了一个名为ProLiFIC的意大利立法流程时间日志，旨在将过程挖掘应用于法律领域并解决数据质量问题，为法律过程挖掘提供了基准。

<details>
  <summary>Details</summary>

**Motivation:** 针对过程挖掘（PM）在法律领域应用中受限于数据集的可访问性和质量的问题，提出了ProLiFIC作为解决这一问题的方案。

**Method:** 介绍了ProLiFIC，这是一个从1987年到2022年意大利立法过程的全面事件日志。ProLiFIC是从Normattiva门户网站的非结构化数据中创建的，并使用大型语言模型（LLMs）进行了结构化处理。

**Result:** 初步分析表明ProLiFIC作为意大利立法过程的事件日志，是法律过程挖掘的有效数据源，并可作为未来研究的基准。

**Conclusion:** ProLiFIC作为法律过程挖掘的研究基准，不仅展示了过程挖掘在法律领域的应用潜力，也为该领域的进一步研究和发展奠定了基础。

**Abstract:** Process Mining (PM), initially developed for industrial and business
contexts, has recently been applied to social systems, including legal ones.
However, PM's efficacy in the legal domain is limited by the accessibility and
quality of datasets. We introduce ProLiFIC (Procedural Lawmaking Flow in
Italian Chambers), a comprehensive event log of the Italian lawmaking process
from 1987 to 2022. Created from unstructured data from the Normattiva portal
and structured using large language models (LLMs), ProLiFIC aligns with recent
efforts in integrating PM with LLMs. We exemplify preliminary analyses and
propose ProLiFIC as a benchmark for legal PM, fostering new developments.

</details>


### [5] [Multimodal Proposal for an AI-Based Tool to Increase Cross-Assessment of Messages](https://arxiv.org/abs/2509.03529)
*Alejandro Álvarez Castro,Joaquín Ordieres-Meré*

Main category: cs.CL

> 引入一种多模态框架，用于生成收益电话会议的语义丰富和结构清晰的嵌入式表示，能够捕捉高风险沟通的情感基调、结构逻辑和主题一致性。

<details>
  <summary>Details</summary>

**Motivation:** 目前的财务情绪分析系统依赖于单一层面的模型，未能捕捉收益通话的多层语篇结构。

**Method:** 采用分层语篇树来编码收益电话会议，每个节点包含独白或问答对，并通过对比学习结合多模态内容和语篇元数据。第二阶段的Transformer架构则合成整个会议的全局嵌入。

**Result:** 实验结果显示生成的嵌入可形成长期稳定、语义相关的表示，能够反映情感基调、结构逻辑和主题一致性。

**Conclusion:** 所提出的方法不仅适用于金融报告，还可推广到其他包括远程医疗、教育、政治讨论等在内的高风险非剧本沟通领域。

**Abstract:** Earnings calls represent a uniquely rich and semi-structured source of
financial communication, blending scripted managerial commentary with
unscripted analyst dialogue. Although recent advances in financial sentiment
analysis have integrated multi-modal signals, such as textual content and vocal
tone, most systems rely on flat document-level or sentence-level models,
failing to capture the layered discourse structure of these interactions. This
paper introduces a novel multi-modal framework designed to generate
semantically rich and structurally aware embeddings of earnings calls, by
encoding them as hierarchical discourse trees. Each node, comprising either a
monologue or a question-answer pair, is enriched with emotional signals derived
from text, audio, and video, as well as structured metadata including coherence
scores, topic labels, and answer coverage assessments. A two-stage transformer
architecture is proposed: the first encodes multi-modal content and discourse
metadata at the node level using contrastive learning, while the second
synthesizes a global embedding for the entire conference. Experimental results
reveal that the resulting embeddings form stable, semantically meaningful
representations that reflect affective tone, structural logic, and thematic
alignment. Beyond financial reporting, the proposed system generalizes to other
high-stakes unscripted communicative domains such as tele-medicine, education,
and political discourse, offering a robust and explainable approach to
multi-modal discourse representation. This approach offers practical utility
for downstream tasks such as financial forecasting and discourse evaluation,
while also providing a generalizable method applicable to other domains
involving high-stakes communication.

</details>


### [6] [Reading Between the Signs: Predicting Future Suicidal Ideation from Adolescent Social Media Texts](https://arxiv.org/abs/2509.03530)
*Paul Blum,Enrico Liscio,Ruixuan Zhang,Caroline Figueroa,Pradeep K. Murukannaiah*

Main category: cs.CL

> 通过分析青少年在在线论坛上的帖子，研究提出了一种早期预测自杀意念和行为的方法。该模型名为Early-SIB，它基于变压器架构，通过处理用户撰写的和互动的帖子，来预测他们是否将发布带有自杀意念的内容，在荷兰青年论坛中达到了0.73的平衡准确率。

<details>
  <summary>Details</summary>

**Motivation:** 由于许多青少年自杀案例未被现有心理健康服务及时发现，这项研究旨在利用社会媒体这一渠道，通过模型来预测青少年的自杀风险，以提高早期识别和干预的可能性。

**Method:** Structure

**Result:** {"tldr": "通过分析青少年在在线论坛上的帖子，研究提出了一种早期预测自杀意念和行为的方法。该模型名为Early-SIB，它基于变压器架构，通过处理用户撰写的和互动的帖子，来预测他们是否将发布带有自杀意念的内容，在荷兰青年论坛中达到了0.73的平衡准确率。", "motivation": "由于许多青少年自杀案例未被现有心理健康服务及时发现，这项研究旨在利用社会媒体这一渠道，通过模型来预测青少年的自杀风险，以提高早期识别和干预的可能性。", "method": "研究使用了一种名为Early-SIB的基于变压器的模型。该模型通过分析用户在论坛上的帖子和互动，预测他们是否会在未来发布包含自杀意念的内容。", "result": "在荷兰一个青少年用的在线论坛上，该模型达到了0.73的平衡准确率，这表明该模型能够对传统方法进行有意义的补充。", "conclusion": "这项研究展示了利用社交媒体数据预测自杀意念和行为的潜力，表明这种工具可以作为传统心理健康方法的补充，帮助更早识别风险。"}

**Conclusion:** 这项研究展示了利用社交媒体数据预测自杀意念和行为的潜力，表明这种工具可以作为传统心理健康方法的补充，帮助更早识别风险。

**Abstract:** Suicide is a leading cause of death among adolescents (12-18), yet predicting
it remains a significant challenge. Many cases go undetected due to a lack of
contact with mental health services. Social media, however, offers a unique
opportunity, as young people often share their thoughts and struggles online in
real time. In this work, we propose a novel task and method to approach it:
predicting suicidal ideation and behavior (SIB) from forum posts before an
adolescent explicitly expresses suicidal ideation on an online forum. This
predictive framing, where no self-disclosure is used as input at any stage,
remains largely unexplored in the suicide prediction literature. To this end,
we introduce Early-SIB, a transformer-based model that sequentially processes
the posts a user writes and engages with to predict whether they will write a
SIB post. Our model achieves a balanced accuracy of 0.73 for predicting future
SIB on a Dutch youth forum, demonstrating that such tools can offer a
meaningful addition to traditional methods.

</details>


### [7] [Real-Time Detection of Hallucinated Entities in Long-Form Generation](https://arxiv.org/abs/2509.03531)
*Oscar Obeso,Andy Arditi,Javier Ferrando,Joshua Freeman,Cameron Holmes,Neel Nanda*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Large language models are now routinely used in high-stakes applications
where hallucinations can cause serious harm, such as medical consultations or
legal advice. Existing hallucination detection methods, however, are
impractical for real-world use, as they are either limited to short factual
queries or require costly external verification. We present a cheap, scalable
method for real-time identification of hallucinated tokens in long-form
generations, and scale it effectively to 70B parameter models. Our approach
targets \emph{entity-level hallucinations} -- e.g., fabricated names, dates,
citations -- rather than claim-level, thereby naturally mapping to token-level
labels and enabling streaming detection. We develop an annotation methodology
that leverages web search to annotate model responses with grounded labels
indicating which tokens correspond to fabricated entities. This dataset enables
us to train effective hallucination classifiers with simple and efficient
methods such as linear probes. Evaluating across four model families, our
classifiers consistently outperform baselines on long-form responses, including
more expensive methods such as semantic entropy (e.g., AUC 0.90 vs 0.71 for
Llama-3.3-70B), and are also an improvement in short-form question-answering
settings. Moreover, despite being trained only with entity-level labels, our
probes effectively detect incorrect answers in mathematical reasoning tasks,
indicating generalization beyond entities. While our annotation methodology is
expensive, we find that annotated responses from one model can be used to train
effective classifiers on other models; accordingly, we publicly release our
datasets to facilitate reuse. Overall, our work suggests a promising new
approach for scalable, real-world hallucination detection.

</details>


### [8] [Topic Identification in LLM Input-Output Pairs through the Lens of Information Bottleneck](https://arxiv.org/abs/2509.03533)
*Igor Halperin*

Main category: cs.CL

> 本文提出了一种新的基于DIB的几何聚类方法(UDIB)，用于产生更连贯且信息量更大的主题表示，以改进检测大语言模型的语义偏离问题。

<details>
  <summary>Details</summary>

**Motivation:** 传统的语义差异度量(SDM)框架在检测语言模型中的固有忠实体裁幻觉时依赖于几何聚类，这导致了一个问题，即这些主题是为优化空间邻近性而设计的，而不适用于下游的信息理论分析。

**Method:** 通过引入一种基于确定性信息瓶颈(DIB)的方法来改进几何聚类，解决了传统方法中主题优化侧重于空间邻近性的问题。这种方法能生成不仅是空间连贯的，而且在提示和响应关系上更具信息性的共享主题表示。

**Result:** 通过将DIB方法转化为适用于高维数据的实用算法，我们得到的UDIB方法可以解释为正则化和强化的K-means，这有利于形成简洁的信息性聚类，从而更好地捕捉提示和响应之间的关系。

**Conclusion:** 提出的方法改进了现有的SDM框架，提供了一种更敏感的工具来检测固有忠实体裁幻觉，从而增强了语言模型的可靠性。

**Abstract:** Large Language Models (LLMs) are prone to critical failure modes, including
\textit{intrinsic faithfulness hallucinations} (also known as confabulations),
where a response deviates semantically from the provided context. Frameworks
designed to detect this, such as Semantic Divergence Metrics (SDM), rely on
identifying latent topics shared between prompts and responses, typically by
applying geometric clustering to their sentence embeddings. This creates a
disconnect, as the topics are optimized for spatial proximity, not for the
downstream information-theoretic analysis. In this paper, we bridge this gap by
developing a principled topic identification method grounded in the
Deterministic Information Bottleneck (DIB) for geometric clustering. Our key
contribution is to transform the DIB method into a practical algorithm for
high-dimensional data by substituting its intractable KL divergence term with a
computationally efficient upper bound. The resulting method, which we dub UDIB,
can be interpreted as an entropy-regularized and robustified version of K-means
that inherently favors a parsimonious number of informative clusters. By
applying UDIB to the joint clustering of LLM prompt and response embeddings, we
generate a shared topic representation that is not merely spatially coherent
but is fundamentally structured to be maximally informative about the
prompt-response relationship. This provides a superior foundation for the SDM
framework and offers a novel, more sensitive tool for detecting confabulations.

</details>


### [9] [QuesGenie: Intelligent Multimodal Question Generation](https://arxiv.org/abs/2509.03535)
*Ahmed Mubarak,Amna Ahmed,Amira Nasser,Aya Mohamed,Fares El-Sadek,Mohammed Ahmed,Ahmed Salah,Youssef Sobhy*

Main category: cs.CL

> 该项目开发了一种多模态的自动出题系统，能够从各种内容格式中自动生成多种类型的问题，从而解决了实践中缺乏练习材料的问题。

<details>
  <summary>Details</summary>

**Motivation:** 尽管在信息丰富的时代学生可以接触到丰富的教育资源，但缺乏与这些资源相配套的练习材料是一个显著的挑战。此项目正是为了解决这一问题。

**Method:** 提出了一种多模态的自动出题系统，该系统可以从不同格式的内容自动生成多种类型的题目，并包含四个主要组件：多模态输入处理、题目生成、基于人类反馈的强化学习（RLHF）和端到端的交互界面。

**Result:** 该项目构建了自动化的、可扩展且智能的题目生成系统的基础，该系统在资源效率、功能稳健性和用户友好性方面取得了平衡。

**Conclusion:** 此系统为教育资源的个性化和有效利用提供了可能，是自动出题领域的重要进展。

**Abstract:** In today's information-rich era, learners have access to abundant educational
resources, but the lack of practice materials tailored to these resources
presents a significant challenge. This project addresses that gap by developing
a multi-modal question generation system that can automatically generate
diverse question types from various content formats. The system features four
major components: multi-modal input handling, question generation,
reinforcement learning from human feedback (RLHF), and an end-to-end
interactive interface. This project lays the foundation for automated,
scalable, and intelligent question generation, carefully balancing resource
efficiency, robust functionality and a smooth user experience.

</details>


### [10] [AR$^2$: Adversarial Reinforcement Learning for Abstract Reasoning in Large Language Models](https://arxiv.org/abs/2509.03537)
*Cheng-Kai Yeh,Hsing-Wang Lee,Chung-Hung Kuo,Hen-Hsen Huang*

Main category: cs.CL

> 本研究提出AR$^2$框架，通过教师模型复杂的叙述性问题转换与学生模型从这些问题中抽取基本计算内核的方法，来提升大语言模型的抽象能力，从而提高其解决新颖编程任务的能力。

<details>
  <summary>Details</summary>

**Motivation:** 尽管在使用强化学习训练大语言模型进行代码生成方面取得了进展，但大多数现有方法主要集中在表面模式识别上，而忽视了对抽象性的显式训练。本研究旨在通过AR$^2$框架提高大语言模型的抽象能力。

**Method:** AR$^2$（对抗强化学习用于抽象推理）是一种新型框架，旨在提高大语言模型的抽象能力。该框架采用教师模型将内核问题转换为叙述丰富、复杂的描述，而不改变其基本逻辑。同时，对学生编程模型进行训练，通过提取复杂叙述问题中的基本计算内核来解决问题。

**Result:** 实验结果表明，AR$^2$显著提高了学生模型在未曾见过的复杂编程任务上的准确性，表明抽象能力是提高大语言模型泛化能力的关键技能。

**Conclusion:** 实验结果证实了AR$^2$在提高大语言模型解决未见过的复杂编程任务准确性方面的有效性，强调了抽象能力在提高模型泛化能力中的关键作用。

**Abstract:** Abstraction--the ability to recognize and distill essential computational
patterns from complex problem statements--is a foundational skill in computer
science, critical both for human problem-solvers and coding-oriented large
language models (LLMs). Despite recent advances in training LLMs for code
generation using reinforcement learning (RL), most existing approaches focus
primarily on superficial pattern recognition, overlooking explicit training for
abstraction. In this study, we propose AR$^2$ (Adversarial Reinforcement
Learning for Abstract Reasoning), a novel framework explicitly designed to
enhance the abstraction abilities of LLMs. AR$^2$ employs a teacher model to
transform kernel problems into narrative-rich, challenging descriptions without
changing their fundamental logic. Simultaneously, a student coding model is
trained to solve these complex narrative problems by extracting their
underlying computational kernels. Experimental results demonstrate that AR$^2$
substantially improves the student model's accuracy on previously unseen,
challenging programming tasks, underscoring abstraction as a key skill for
enhancing LLM generalization.

</details>


### [11] [Improving Factuality in LLMs via Inference-Time Knowledge Graph Construction](https://arxiv.org/abs/2509.03540)
*Shanglin Wu,Lihui Liu,Jinho D. Choi,Kai Shu*

Main category: cs.CL

> 该论文提出了一种在推理过程中动态构建和扩展知识图谱的框架，以提高大语言模型生成事实一致性答案的能力。

<details>
  <summary>Details</summary>

**Motivation:** 大语言模型在生成一致性答案方面存在困难，主要是因为它们受到参数化记忆的限制。现有的检索增强生成方法只能将知识视为非结构化文本，这限制了它们支持组合推理和识别事实不一致性的能力。

**Method:** Structure

**Result:** {
  "tldr": "该论文提出了一种在推理过程中动态构建和扩展知识图谱的框架，以提高大语言模型生成事实一致性答案的能力。", 
  "motivation": "大语言模型在生成一致性答案方面存在困难，主要是因为它们受到参数化记忆的限制。现有的检索增强生成方法只能将知识视为非结构化文本，这限制了它们支持组合推理和识别事实不一致性的能力。", 
  "method": "该框架通过从问题中提取一个初始知识图谱，并使用大语言模型的潜在知识进行迭代扩展，再通过外部检索选择性地优化这个图谱，以提高事实覆盖范围和纠正不准确信息。", 
  "result": "该方法在三个不同的事实问答基准测试中显示出在事实准确性、答案精度和可解释性方面比基准提示法和静态知识图谱增强方法有持续改进。", 
  "conclusion": "此研究提出，推理时构建知识图谱是一个增强大语言模型事实性的有前途方向，同时也是结构化、可解释和可扩展的方式。"}
}

**Conclusion:** 此研究提出，推理时构建知识图谱是一个增强大语言模型事实性的有前途方向，同时也是结构化、可解释和可扩展的方式。

**Abstract:** Large Language Models (LLMs) often struggle with producing factually
consistent answers due to limitations in their parametric memory.
Retrieval-Augmented Generation (RAG) methods address this issue by
incorporating external knowledge from trusted sources at inference time.
However, such methods typically treat knowledge as unstructured text, which
limits their ability to support compositional reasoning and identify factual
inconsistencies. To overcome these limitations, we propose a novel framework
that dynamically constructs and expands knowledge graphs (KGs) during
inference, integrating both internal knowledge extracted from LLMs and external
information retrieved from external sources. Our method begins by extracting a
seed KG from the question via prompting, followed by iterative expansion using
the LLM's latent knowledge. The graph is then selectively refined through
external retrieval, enhancing factual coverage and correcting inaccuracies. We
evaluate our approach on three diverse factual QA benchmarks, demonstrating
consistent improvements in factual accuracy, answer precision, and
interpretability over baseline prompting and static KG-augmented methods. Our
findings suggest that inference-time KG construction is a promising direction
for enhancing LLM factuality in a structured, interpretable, and scalable
manner.

</details>


### [12] [ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference](https://arxiv.org/abs/2509.03565)
*Qi Chen,Jingxuan Wei,Zhuoya Yao,Haiguang Wang,Gaowei Wu,Bihui Yu,Siyuan Li,Cheng Tan*

Main category: cs.CL

> 本文介绍了一种用于跨文档科学推理的新任务和方法，以及一种名为ResearchPulse的系统，该系统由三个代理组成，能够整合多个相关论文的动机、方法和实验结果，重构研究发展链。

<details>
  <summary>Details</summary>

**Motivation:** 理解科学思想的演化需要超越对单篇论文的总结，需要进行有结构的、交叉文献推理。本文提出了一种新的任务——跨文档科学推理，旨在提取并对接相关论文的动机、方法和实验结果，以重构研究发展链。

**Method:** 本研究提出了一种名为ResearchPulse的基于代理的框架，该框架集成了指令规划、科学内容提取和结构化可视化。它由三个协调的代理组成：用于任务分解的计划代理，用于绘制动机-方法思维导图的Mmap代理，以及用于合成实验图表的Lchart代理。

**Result:** 实验结果显示，尽管使用的是7B规模的代理，该系统在语义对齐、结构一致性、和可视化准确性方面始终优于像GPT-4o这样的强基线系统。

**Conclusion:** 本研究介绍了ResearchPulse系统及其支持的新任务，提供了ResearchPulse-Bench数据集，这对研究科学文献的演化提供了一种新的工具和方法。

**Abstract:** Understanding how scientific ideas evolve requires more than summarizing
individual papers-it demands structured, cross-document reasoning over
thematically related research. In this work, we formalize multi-document
scientific inference, a new task that extracts and aligns motivation,
methodology, and experimental results across related papers to reconstruct
research development chains. This task introduces key challenges, including
temporally aligning loosely structured methods and standardizing heterogeneous
experimental tables. We present ResearchPulse, an agent-based framework that
integrates instruction planning, scientific content extraction, and structured
visualization. It consists of three coordinated agents: a Plan Agent for task
decomposition, a Mmap-Agent that constructs motivation-method mind maps, and a
Lchart-Agent that synthesizes experimental line charts. To support this task,
we introduce ResearchPulse-Bench, a citation-aware benchmark of annotated paper
clusters. Experiments show that our system, despite using 7B-scale agents,
consistently outperforms strong baselines like GPT-4o in semantic alignment,
structural consistency, and visual fidelity. The dataset are available in
https://huggingface.co/datasets/ResearchPulse/ResearchPulse-Bench.

</details>


### [13] [NoteBar: An AI-Assisted Note-Taking System for Personal Knowledge Management](https://arxiv.org/abs/2509.03610)
*Josh Wisoff,Yao Tang,Zhengyu Fang,Jordan Guzman,YuTang Wang,Alex Yu*

Main category: cs.CL

> NoteBar是一个AI辅助的笔记工具，通过利用人格信息和高效的语言模型，能够自动将笔记组织成多个类别，从而更好地支持用户的工作流程。研究还引入了一个包含3,173个笔记和8,494个标注概念的新数据集，涵盖了16种MBTI人格类型。

<details>
  <summary>Details</summary>

**Motivation:** 当前的AI辅助笔记工具在效率方面存在不足，因此需要开发一种可以更高效和个性化地帮助用户管理和组织笔记的工具。

**Method:** 开发NoteBar工具，该工具利用人格信息和高效的语言模型来自动将笔记组织成多个类别。同时，建立了一个条件化人格的数据集来支持研究和评估。

**Result:** NoteBar可以被部署为一种实际且成本效益高的方式，支持无需依赖重型基础设施的互动使用。

**Conclusion:** NoteBar及其配套数据集为推进AI辅助个人知识管理提供了一个可扩展的和可扩展的基础。

**Abstract:** Note-taking is a critical practice for capturing, organizing, and reflecting
on information in both academic and professional settings. The recent success
of large language models has accelerated the development of AI-assisted tools,
yet existing solutions often struggle with efficiency. We present NoteBar, an
AI-assisted note-taking tool that leverages persona information and efficient
language models to automatically organize notes into multiple categories and
better support user workflows. To support research and evaluation in this
space, we further introduce a novel persona-conditioned dataset of 3,173 notes
and 8,494 annotated concepts across 16 MBTI personas, offering both diversity
and semantic richness for downstream tasks. Finally, we demonstrate that
NoteBar can be deployed in a practical and cost-effective manner, enabling
interactive use without reliance on heavy infrastructure. Together, NoteBar and
its accompanying dataset provide a scalable and extensible foundation for
advancing AI-assisted personal knowledge management.

</details>


### [14] [E-ARMOR: Edge case Assessment and Review of Multilingual Optical Character Recognition](https://arxiv.org/abs/2509.03615)
*Aryan Gupta,Anupam Purwar*

Main category: cs.CL

> 评估了五个前沿LVLM和两个传统OCR系统在多语言图像上的性能，发现传统OCR系统由于低计算需求、低延迟和高性价比，更适合边缘部署。

<details>
  <summary>Details</summary>

**Motivation:** 为了探索LVLM是否能超越固定的OCR管线，在多语言、噪声及多元化的现实场景图像中有所突破。同时研究适合边缘部署的最佳OCR系统。

**Method:** 我们介绍了Sprinklr-Edge-OCR，一款专门为边缘部署设计的OCR系统，构建在资源受限的环境中进行优化。我们对比了五个前沿的LVLM（InternVL, Qwen, GOT OCR, LLaMA, MiniCPM）和两个传统的OCR系统（Sprinklr-Edge-OCR, SuryaOCR），在包含54种语言的手工标注数据集上进行了大规模的评估，涵盖了准确性、语义一致性、语言覆盖范围、计算效率（延迟、内存、GPU使用）和部署成本等多方面指标。

**Result:** Qwen在精度上表现最佳（0.54），而Sprinklr-Edge-OCR在综合F1评分上最佳（0.46），并且在效率上优于其他系统，处理图像速度提高35%，延迟仅为0.17秒每张图像，成本仅为LVLM的0.01（每1,000张图像0.006美元）。

**Conclusion:** 研究发现，即使是在大型语言模型(LLM)盛行的时代，传统的OCR系统仍然是边缘部署的最佳选择，因为它们拥有较低的计算需求，较低的延迟，以及极高的性价比。

**Abstract:** Optical Character Recognition (OCR) in multilingual, noisy, and diverse
real-world images remains a significant challenge for optical character
recognition systems. With the rise of Large Vision-Language Models (LVLMs),
there is growing interest in their ability to generalize and reason beyond
fixed OCR pipelines. In this work, we introduce Sprinklr-Edge-OCR, a novel OCR
system built specifically optimized for edge deployment in resource-constrained
environments. We present a large-scale comparative evaluation of five
state-of-the-art LVLMs (InternVL, Qwen, GOT OCR, LLaMA, MiniCPM) and two
traditional OCR systems (Sprinklr-Edge-OCR, SuryaOCR) on a proprietary, doubly
hand annotated dataset of multilingual (54 languages) images. Our benchmark
covers a broad range of metrics including accuracy, semantic consistency,
language coverage, computational efficiency (latency, memory, GPU usage), and
deployment cost. To better reflect real-world applicability, we also conducted
edge case deployment analysis, evaluating model performance on CPU only
environments. Among the results, Qwen achieved the highest precision (0.54),
while Sprinklr-Edge-OCR delivered the best overall F1 score (0.46) and
outperformed others in efficiency, processing images 35 faster (0.17 seconds
per image on average) and at less than 0.01 of the cost (0.006 USD per 1,000
images) compared to LVLM. Our findings demonstrate that the most optimal OCR
systems for edge deployment are the traditional ones even in the era of LLMs
due to their low compute requirements, low latency, and very high
affordability.

</details>


### [15] [Breaking the Mirror: Activation-Based Mitigation of Self-Preference in LLM Evaluators](https://arxiv.org/abs/2509.03647)
*Dani Roytburg,Matthew Bozoukov,Matthew Nguyen,Jou Barzdukas,Simon Fu,Narmeen Oozeer*

Main category: cs.CL

> 研究显示，经过精心设计的引导向量能够显著减少大语言模型在评估其自身输出时的不合理自我偏好问题，但它们对正当的自我偏好有所限制。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在探讨在不重新训练的情况下，是否可以通过简化的引导向量在推理时减轻LLMs的自我偏好偏见问题，以维护评估管道的公平性和可靠性，特别适用于偏好调优和模型路由等任务。

**Method:** 我们使用了对比激活添加(CAA)和优化方法两种方式构建引导向量，并通过一个区分自我偏好偏好的合理性与不合理性的自定义数据集进行研究。

**Result:** 结果表明，引导向量能将不合理的自我偏好偏见减少高达97%，明显超越了提示和直接偏好优化基线方法。然而，这些引导向量对正当的自我偏好和非偏向的一致性出现了不稳定性。

**Conclusion:** 这项研究揭示了将引导向量用作判断LLMs的防护措施的潜力和限制，同时也突显了需要更强大的干预措施来解决自我偏好偏见问题。

**Abstract:** Large language models (LLMs) increasingly serve as automated evaluators, yet
they suffer from "self-preference bias": a tendency to favor their own outputs
over those of other models. This bias undermines fairness and reliability in
evaluation pipelines, particularly for tasks like preference tuning and model
routing. We investigate whether lightweight steering vectors can mitigate this
problem at inference time without retraining. We introduce a curated dataset
that distinguishes self-preference bias into justified examples of
self-preference and unjustified examples of self-preference, and we construct
steering vectors using two methods: Contrastive Activation Addition (CAA) and
an optimization-based approach. Our results show that steering vectors can
reduce unjustified self-preference bias by up to 97\%, substantially
outperforming prompting and direct preference optimization baselines. Yet
steering vectors are unstable on legitimate self-preference and unbiased
agreement, implying self-preference spans multiple or nonlinear directions.
This underscores both their promise and limits as safeguards for LLM-as-judges
and motivates more robust interventions.

</details>


### [16] [Semantic Analysis of SNOMED CT Concept Co-occurrences in Clinical Documentation using MIMIC-IV](https://arxiv.org/abs/2509.03662)
*Ali Noori,Somya Mohanty,Prashanti Manda*

Main category: cs.CL

> 本研究探索了SNOMED CT概念共现模式和基于嵌入的语义相似性之间的关系，展示了概念共现与语义相似性之间的弱相关性，但基于嵌入的方法可以捕获临床相关性并用于增强临床注释，这显示出共现和语义嵌入在改善文档完整性、发现潜在的临床关系和指导决策支持应用程序方面的互补价值。

<details>
  <summary>Details</summary>

**Motivation:** 临床记录包含丰富的临床叙述，但它们的非结构化格式对大规模分析带来了挑战。标准化术语虽然提高了互操作性，但理解概念之间的共现和语义相似性仍然研究不足。

**Method:** 本研究利用MIMIC-IV数据库，通过标准化术语SNOMED CT的概念共现模式和基于嵌入的语义相似性来调查两者之间的关系。使用归一化点互信息(NPMI)和预训练嵌入（如ClinicalBERT，BioBERT）来研究频繁共现的概念是否也语义相近，嵌入是否可以建议缺失的概念，以及这些关系如何随着时间变化和在不同专业间变化。

**Result:** 分析结果表明，尽管共现和语义相似性之间存在弱相关性，但是嵌入捕获的临床有意义的关联并不总能反映在文档频率中。基于嵌入的建议经常与后来记录的概念匹配，支持其在增强临床注释方面的效用。嵌入的概念聚类生成了连贯的临床主题（如症状、实验室检查、诊断和心血管疾病），这些主题可以映射到患者表型和护理模式。最后，共现模式与结局指标（如死亡率和再入院率）相关联，证明了这种方法的实用性。

**Conclusion:** 总体而言，研究结果突出显示共现统计数据和语义嵌入在提高文档完整性、揭示潜在临床关系和为决策支持和表型分析应用提供信息方面的互补价值。

**Abstract:** Clinical notes contain rich clinical narratives but their unstructured format
poses challenges for large-scale analysis. Standardized terminologies such as
SNOMED CT improve interoperability, yet understanding how concepts relate
through co-occurrence and semantic similarity remains underexplored. In this
study, we leverage the MIMIC-IV database to investigate the relationship
between SNOMED CT concept co-occurrence patterns and embedding-based semantic
similarity. Using Normalized Pointwise Mutual Information (NPMI) and pretrained
embeddings (e.g., ClinicalBERT, BioBERT), we examine whether frequently
co-occurring concepts are also semantically close, whether embeddings can
suggest missing concepts, and how these relationships evolve temporally and
across specialties. Our analyses reveal that while co-occurrence and semantic
similarity are weakly correlated, embeddings capture clinically meaningful
associations not always reflected in documentation frequency. Embedding-based
suggestions frequently matched concepts later documented, supporting their
utility for augmenting clinical annotations. Clustering of concept embeddings
yielded coherent clinical themes (symptoms, labs, diagnoses, cardiovascular
conditions) that map to patient phenotypes and care patterns. Finally,
co-occurrence patterns linked to outcomes such as mortality and readmission
demonstrate the practical utility of this approach. Collectively, our findings
highlight the complementary value of co-occurrence statistics and semantic
embeddings in improving documentation completeness, uncovering latent clinical
relationships, and informing decision support and phenotyping applications.

</details>


### [17] [MLSD: A Novel Few-Shot Learning Approach to Enhance Cross-Target and Cross-Domain Stance Detection](https://arxiv.org/abs/2509.03725)
*Parush Gera,Tempestt Neal*

Main category: cs.CL

> 本文提出了一种名为MLSD的新方法，利用度量学习与三元组损失来捕获立场目标间的语义相似性和差异，提高了领域内的立场检测性能。

<details>
  <summary>Details</summary>

**Motivation:** 为了改进跨领域和跨目标的立场检测，在不同数据集中提高六种常用立场检测模型的性能。

**Method:** MLSD采用了度量学习与三元组损失技术，构建了一个判别性的嵌入空间，从而使模型能够从新的目标域中获取有用的示例。

**Result:** 已验证MLSD能够在多个跨目标和跨域的场景下，显著提高立场检测的性能。

**Conclusion:** 研究表明，MLSD能够有效提升跨领域和跨目标立场检测的性能。

**Abstract:** We present the novel approach for stance detection across domains and
targets, Metric Learning-Based Few-Shot Learning for Cross-Target and
Cross-Domain Stance Detection (MLSD). MLSD utilizes metric learning with
triplet loss to capture semantic similarities and differences between stance
targets, enhancing domain adaptation. By constructing a discriminative
embedding space, MLSD allows a cross-target or cross-domain stance detection
model to acquire useful examples from new target domains. We evaluate MLSD in
multiple cross-target and cross-domain scenarios across two datasets, showing
statistically significant improvement in stance detection performance across
six widely used stance detection models.

</details>


### [18] [SiLVERScore: Semantically-Aware Embeddings for Sign Language Generation Evaluation](https://arxiv.org/abs/2509.03791)
*Saki Imai,Mert İnan,Anthony Sicilia,Malihe Alikhani*

Main category: cs.CL

> 提出SiLVERScore，一种语义感知的嵌入式手语生成评价指标，在多个数据集上表现出色，解决了现有评估方法的缺陷。

<details>
  <summary>Details</summary>

**Motivation:** 当前的评估方法通过反向翻译来评估手语生成，即先把手语识别回文本，然后再与参考文本进行比较。但这种两步评估流程存在局限性，不仅无法捕捉手语的多模态特点，也难以区分评估错误来源。

**Method:** 提出了一种新的评价指标SiLVERScore，该指标采用语义感知的嵌入式评估方法，能够在联合嵌入空间中对手语生成进行评价。

**Result:** 通过实验表明，SiLVERScore在PHOENIX-14T和CSL-Daily数据集上表现优异，正确和随机样本之间的判别准确率达到近完美（ROC AUC = 0.99，重叠小于7%），远超传统指标。

**Conclusion:** SiLVERScore通过解决现有评价指标的局限性，实现了对手语生成评价的语义感知，并表现出对语义和韵律变化的鲁棒性，以及在不同数据集上的泛化能力。

**Abstract:** Evaluating sign language generation is often done through back-translation,
where generated signs are first recognized back to text and then compared to a
reference using text-based metrics. However, this two-step evaluation pipeline
introduces ambiguity: it not only fails to capture the multimodal nature of
sign language-such as facial expressions, spatial grammar, and prosody-but also
makes it hard to pinpoint whether evaluation errors come from sign generation
model or the translation system used to assess it. In this work, we propose
SiLVERScore, a novel semantically-aware embedding-based evaluation metric that
assesses sign language generation in a joint embedding space. Our contributions
include: (1) identifying limitations of existing metrics, (2) introducing
SiLVERScore for semantically-aware evaluation, (3) demonstrating its robustness
to semantic and prosodic variations, and (4) exploring generalization
challenges across datasets. On PHOENIX-14T and CSL-Daily datasets, SiLVERScore
achieves near-perfect discrimination between correct and random pairs (ROC AUC
= 0.99, overlap < 7%), substantially outperforming traditional metrics.

</details>


### [19] [Measuring How (Not Just Whether) VLMs Build Common Ground](https://arxiv.org/abs/2509.03805)
*Saki Imai,Mert İnan,Anthony Sicilia,Malihe Alikhani*

Main category: cs.CL

> 本文提出了一套针对视觉语言模型在互动接地场景中表现的四指标评估体系，发现模型在多个指标上与人类表现存在差异，提供了一个未来研究的框架。

<details>
  <summary>Details</summary>

**Motivation:** 当前的基准大多在单一回合或问答设置中评估视觉语言模型，但本研究认为，接地是一个互动性过程，人们需要通过持续的沟通逐步建立共同的理解。因此，需要新的评估方法来更好地理解视觉语言模型在互动情境中的表现。

**Method:** 通过引入一套四指标体系（接地效率、内容一致性、词汇适应性和类人类性）来系统性地评估视觉语言模型在互动接地情境下的表现。这套指标体系在150次自博弈中的互动参照游戏中进行部署，参与者包括三个专有的视觉语言模型和人类双人组。

**Result:** 所有三个模型在至少三项指标上与人类表现有区别，但GPT4o-mini总体上最接近。发现任务成功的高评分并不一定意味着接地成功，高图像-话语对齐也不一定预测任务成功。

**Conclusion:** 该研究提供的指标体系帮助研究人员更好地理解视觉语言模型的接地能力，并为未来的研究提供了一个框架。

**Abstract:** Large vision language models (VLMs) increasingly claim reasoning skills, yet
current benchmarks evaluate them in single-turn or question answering settings.
However, grounding is an interactive process in which people gradually develop
shared understanding through ongoing communication. We introduce a four-metric
suite (grounding efficiency, content alignment, lexical adaptation, and
human-likeness) to systematically evaluate VLM performance in interactive
grounding contexts. We deploy the suite on 150 self-play sessions of
interactive referential games between three proprietary VLMs and compare them
with human dyads. All three models diverge from human patterns on at least
three metrics, while GPT4o-mini is the closest overall. We find that (i) task
success scores do not indicate successful grounding and (ii) high
image-utterance alignment does not necessarily predict task success. Our metric
suite and findings offer a framework for future research on VLM grounding.

</details>


### [20] [Align-then-Slide: A complete evaluation framework for Ultra-Long Document-Level Machine Translation](https://arxiv.org/abs/2509.03809)
*Jiaxin Guo,Daimeng Wei,Yuanchang Luo,Xiaoyu Chen,Zhanglin Wu,Huan Yang,Hengchao Shang,Zongyao Li,Zhiqiang Rao,Jinlong Yang,Hao Yang*

Main category: cs.CL

> 论文提出了一种新的文档级机器翻译评估框架Align-then-Slide，它解决了现有评估方法中存在的句子对齐挑战问题，并通过实验证明了该框架的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型改变了文档机器翻译的评估方法，但现有的评估手段基于句子对句子的对齐，对全文输出的评估形成挑战。

**Method:** 介绍了一种名为Align-then-Slide的评估框架，包含对齐和滑动评估两个阶段，解决了文档机器翻译评估中句子对齐挑战。

**Result:** 实验结果表明，该方法与专家MQM排名之间的皮尔逊相关系数达到0.929，并且在真实世界测试集中与人类判断吻合。同时，该方法生成的偏好数据可以用于CPO训练和作为奖励模型，结果优于简单指令调整基线。

**Conclusion:** Align-then-Slide框架作为一种准确、稳健、可操作的文档机器翻译评估工具得到了验证。

**Abstract:** Large language models (LLMs) have ushered in a new era for document-level
machine translation (\textit{doc}-mt), yet their whole-document outputs
challenge existing evaluation methods that assume sentence-by-sentence
alignment. We introduce \textit{\textbf{Align-then-Slide}}, a complete
evaluation framework for ultra-long doc-mt. In the Align stage, we
automatically infer sentence-level source-target correspondences and rebuild
the target to match the source sentence number, resolving omissions and
many-to-one/one-to-many mappings. In the n-Chunk Sliding Evaluate stage, we
calculate averaged metric scores under 1-, 2-, 3- and 4-chunk for
multi-granularity assessment. Experiments on the WMT benchmark show a Pearson
correlation of 0.929 between our method with expert MQM rankings. On a newly
curated real-world test set, our method again aligns closely with human
judgments. Furthermore, preference data produced by Align-then-Slide enables
effective CPO training and its direct use as a reward model for GRPO, both
yielding translations preferred over a vanilla SFT baseline. The results
validate our framework as an accurate, robust, and actionable evaluation tool
for doc-mt systems.

</details>


### [21] [NE-PADD: Leveraging Named Entity Knowledge for Robust Partial Audio Deepfake Detection via Attention Aggregation](https://arxiv.org/abs/2509.03829)
*Huhong Xian,Rui Liu,Berrak Sisman,Haizhou Li*

Main category: cs.CL

> NE-PADD integrates named entity knowledge into a partial audio deepfake detection framework using attention mechanisms, outperforming existing baselines on the PartialSpoof-NER dataset.

<details>
  <summary>Details</summary>

**Motivation:** The motivation arises from the underexplored use of semantic information, specifically named entities, in partial audio deepfake detection.

**Method:** Our method, NE-PADD, includes two parallel branches for Speech Name Entity Recognition (SpeechNER) and Partial Audio Deepfake Detection (PADD), utilizing named entity knowledge through Attention Fusion (AF) and Attention Transfer (AT) mechanisms.

**Result:** The experiments show our NE-PADD approach outperforms existing methods by leveraging named entity knowledge through attention mechanisms.

**Conclusion:** Integrating named entity knowledge improves partial audio deepfake detection, as demonstrated by the superior performance of NE-PADD in experiments.

**Abstract:** Different from traditional sentence-level audio deepfake detection (ADD),
partial audio deepfake detection (PADD) requires frame-level positioning of the
location of fake speech. While some progress has been made in this area,
leveraging semantic information from audio, especially named entities, remains
an underexplored aspect. To this end, we propose NE-PADD, a novel method for
Partial Audio Deepfake Detection (PADD) that leverages named entity knowledge
through two parallel branches: Speech Name Entity Recognition (SpeechNER) and
PADD. The approach incorporates two attention aggregation mechanisms: Attention
Fusion (AF) for combining attention weights and Attention Transfer (AT) for
guiding PADD with named entity semantics using an auxiliary loss. Built on the
PartialSpoof-NER dataset, experiments show our method outperforms existing
baselines, proving the effectiveness of integrating named entity knowledge in
PADD. The code is available at https://github.com/AI-S2-Lab/NE-PADD.

</details>


### [22] [Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth](https://arxiv.org/abs/2509.03867)
*Yang Wang,Chenghao Xiao,Chia-Yi Hsiao,Zi Yan Chang,Chi-Li Chen,Tyler Loakman,Chenghua Lin*

Main category: cs.CL

> 文章介绍了Drivelology，一种在语法上连贯但在实用上充满悖论的语言现象，发现当前的大型语言模型在理解这种深度表达上有明显困难，并构建了一个多语言的基准数据集来评估多种语言模型的表现。结果显示这些模型在分类、生成和推理任务上存在局限性。

<details>
  <summary>Details</summary>

**Motivation:** 作者旨在研究大型语言模型在处理一种特殊的、含有深层含义但表面上看似无意义的文本（Drivelology）时的能力，以及揭示这些模型在理解这类文本上的深层次表征差距。

**Method:** 构建了一个包含1200多个经过精心挑选的Drivelological文本样例的数据集，涉及多种语言，通过多轮专家评审和讨论来确保样本的质量。

**Result:** 评估结果表明，尽管大型语言模型在许多NLP任务上表现出色，但它们在理解和生成Drivelological文本时存在明显局限，经常将其与简单的无意义表述混淆或忽视其隐含的修辞功能。

**Conclusion:** 作者指出，这些发现揭示了语言模型在理解语言深层含义方面存在的认知差距，挑战了统计流畅性等同于认知理解的假设。同时，公开发布了数据集和代码以促进进一步的研究。

**Abstract:** We introduce Drivelology, a unique linguistic phenomenon characterised as
"nonsense with depth", utterances that are syntactically coherent yet
pragmatically paradoxical, emotionally loaded, or rhetorically subversive.
While such expressions may resemble surface-level nonsense, they encode
implicit meaning requiring contextual inference, moral reasoning, or emotional
interpretation. We find that current large language models (LLMs), despite
excelling at many natural language processing (NLP) tasks, consistently fail to
grasp the layered semantics of Drivelological text. To investigate this, we
construct a small but diverse benchmark dataset of over 1,200 meticulously
curated examples, with select instances in English, Mandarin, Spanish, French,
Japanese, and Korean. Annotation was especially challenging: each of the
examples required careful expert review to verify that it truly reflected
Drivelological characteristics. The process involved multiple rounds of
discussion and adjudication to address disagreements, highlighting the subtle
and subjective nature of the Drivelology. We evaluate a range of LLMs on
classification, generation, and reasoning tasks. Our results reveal clear
limitations of LLMs: models often confuse Drivelology with shallow nonsense,
produce incoherent justifications, or miss the implied rhetorical function
altogether. These findings highlight a deeper representational gap in LLMs'
pragmatic understanding and challenge the assumption that statistical fluency
implies cognitive comprehension. We release our dataset and code to facilitate
further research in modelling linguistic depth beyond surface-level coherence.

</details>


### [23] [A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models](https://arxiv.org/abs/2509.03871)
*Yanbo Wang,Yongcan Yu,Jian Liang,Ran He*

Main category: cs.CL

> 文章审视了最近关于推理模型和CoT技术的研究，重点是五个核心的可靠推理维度：真实性、安全性、稳健性、公平性和隐私性，并为AI安全社区提供了一个有价值的资源。

<details>
  <summary>Details</summary>

**Motivation:** 尽管CoT推理在提升语言模型的性能方面取得了显著进展，但对于CoT推理如何影响语言模型的可靠性仍然缺乏全面的理解。本文旨在填补这一知识空白。

**Method:** 本文通过回顾最近在推理模型和CoT技术方面的研究工作，重点关注了可靠推理的五个核心维度：真实性、安全性、稳健性、公平性和隐私性，并按时间顺序对每个方面的最新研究进行清晰和结构化的概述。

**Result:** 研究发现，尽管某些推理技术具有提升模型可靠性的潜力，但最前沿的推理模型在安全性和隐私性方面还存在显著的漏洞。

**Conclusion:** 尽管推理技术有望通过减少幻觉、检测有害内容和增强可靠性来提升模型的可靠性，但前沿的推理模型仍然存在相当大甚至更大的安全性和隐私性漏洞。

**Abstract:** The development of Long-CoT reasoning has advanced LLM performance across
various tasks, including language understanding, complex problem solving, and
code generation. This paradigm enables models to generate intermediate
reasoning steps, thereby improving both accuracy and interpretability. However,
despite these advancements, a comprehensive understanding of how CoT-based
reasoning affects the trustworthiness of language models remains
underdeveloped. In this paper, we survey recent work on reasoning models and
CoT techniques, focusing on five core dimensions of trustworthy reasoning:
truthfulness, safety, robustness, fairness, and privacy. For each aspect, we
provide a clear and structured overview of recent studies in chronological
order, along with detailed analyses of their methodologies, findings, and
limitations. Future research directions are also appended at the end for
reference and discussion. Overall, while reasoning techniques hold promise for
enhancing model trustworthiness through hallucination mitigation, harmful
content detection, and robustness improvement, cutting-edge reasoning models
themselves often suffer from comparable or even greater vulnerabilities in
safety, robustness, and privacy. By synthesizing these insights, we hope this
work serves as a valuable and timely resource for the AI safety community to
stay informed on the latest progress in reasoning trustworthiness. A full list
of related papers can be found at
\href{https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}.

</details>


### [24] [False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize](https://arxiv.org/abs/2509.03888)
*Cheng Wang,Zeming Wei,Qin Liu,Muhao Chen*

Main category: cs.CL

> 研究发现大型语言模型的探测方法实际上依赖于表面模式而非语义上的危害性，可能带来错误的安全感，需要重新设计模型和评估方法。

<details>
  <summary>Details</summary>

**Motivation:** 由于探测方法在分布外数据上的表现不佳，促使研究者假设这些探测方法学习到的是表面模式而非语义上的危害。

**Method:** 通过系统性的方法，包括展示简单n-gram方法的类似表现，使用语义清理过的数据集进行控制实验，以及详细的模式依赖性分析，来验证假设。

**Result:** 确认探测方法学习的是指令模式和触发词等表面模式，并非深层次的危害性。这揭示了当前探测方法带来的安全假象，并表明需要重新设计模型和评估协议。

**Conclusion:** 研究表明当前基于探测的方法存在表面现象依赖的问题，建议重新设计模型和评估协议。

**Abstract:** Large Language Models (LLMs) can comply with harmful instructions, raising
serious safety concerns despite their impressive capabilities. Recent work has
leveraged probing-based approaches to study the separability of malicious and
benign inputs in LLMs' internal representations, and researchers have proposed
using such probing methods for safety detection. We systematically re-examine
this paradigm. Motivated by poor out-of-distribution performance, we
hypothesize that probes learn superficial patterns rather than semantic
harmfulness. Through controlled experiments, we confirm this hypothesis and
identify the specific patterns learned: instructional patterns and trigger
words. Our investigation follows a systematic approach, progressing from
demonstrating comparable performance of simple n-gram methods, to controlled
experiments with semantically cleaned datasets, to detailed analysis of pattern
dependencies. These results reveal a false sense of security around current
probing-based approaches and highlight the need to redesign both models and
evaluation protocols, for which we provide further discussions in the hope of
suggesting responsible further research in this direction. We have open-sourced
the project at https://github.com/WangCheng0116/Why-Probe-Fails.

</details>


### [25] [MobileRAG: Enhancing Mobile Agent with Retrieval-Augmented Generation](https://arxiv.org/abs/2509.03891)
*Gowen Loo,Chang Liu,Qinghong Yin,Xiang Chen,Jiawei Chen,Jingyuan Zhang,Yu Tian*

Main category: cs.CL

> 提出MobileRAG框架以改善移动代理的性能，显著提高完成复杂任务的效率和准确率，代码开源。

<details>
  <summary>Details</summary>

**Motivation:** 面对现有移动代理在理解和执行用户查询时存在的问题，特别是解析错误、对外界交互有限、缺乏记忆能力等，促使研究者开发了MobileRAG框架，以期通过RAG技术提升代理的性能。

**Method:** MobileRAG框架利用了检索增强生成（RAG）技术，设计了InterRAG、LocalRAG和MemRAG三个组件，提升代理在理解和执行用户命令时的准确性和灵活性。

**Result:** MobileRAG 是一个增强型移动代理框架，通过检索增强生成（RAG）技术解决了现有移动代理的三大问题：对大语言模型的理解能力依赖、缺乏对外部环境的交互和记忆功能不足。实验显示，与现有方法相比，MobileRAG在MobileRAG-Eval基准测试中表现更优，完成任务效率更高，具体步骤较少，提升了10.3\%。代码已开源。

**Conclusion:** 实验结果表明，相对于现有技术，MobileRAG在MobileRAG-Eval这个更为苛刻的评估基准上取得了显著性能改进，可高效、准确完成复杂任务，具有更高的实用价值。

**Abstract:** Smartphones have become indispensable in people's daily lives, permeating
nearly every aspect of modern society. With the continuous advancement of large
language models (LLMs), numerous LLM-based mobile agents have emerged. These
agents are capable of accurately parsing diverse user queries and automatically
assisting users in completing complex or repetitive operations. However,
current agents 1) heavily rely on the comprehension ability of LLMs, which can
lead to errors caused by misoperations or omitted steps during tasks, 2) lack
interaction with the external environment, often terminating tasks when an app
cannot fulfill user queries, and 3) lack memory capabilities, requiring each
instruction to reconstruct the interface and being unable to learn from and
correct previous mistakes. To alleviate the above issues, we propose MobileRAG,
a mobile agents framework enhanced by Retrieval-Augmented Generation (RAG),
which includes InterRAG, LocalRAG, and MemRAG. It leverages RAG to more quickly
and accurately identify user queries and accomplish complex and long-sequence
mobile tasks. Additionally, to more comprehensively assess the performance of
MobileRAG, we introduce MobileRAG-Eval, a more challenging benchmark
characterized by numerous complex, real-world mobile tasks that require
external knowledge assistance. Extensive experimental results on MobileRAG-Eval
demonstrate that MobileRAG can easily handle real-world mobile tasks, achieving
10.3\% improvement over state-of-the-art methods with fewer operational steps.
Our code is publicly available at:
https://github.com/liuxiaojieOutOfWorld/MobileRAG_arxiv

</details>


### [26] [MTQA:Matrix of Thought for Enhanced Reasoning in Complex Question Answering](https://arxiv.org/abs/2509.03918)
*Fengxiao Tang,Yufeng Li,Zongzong Wu,Ming Zhao*

Main category: cs.CL

> Introduces Matrix of Thought (MoT) for enhancing the reasoning capabilities of LLMs in complex QA tasks, leading to the creation of the MTQA framework, which outperforms existing models in accuracy and efficiency.

<details>
  <summary>Details</summary>

**Motivation:** Complex QA tasks expose the reasoning limitations of large language models (LLMs). Methods like Chain-of-Thought and Tree-of-Thought have limitations, and while Retrieval-Augmented Generation (RAG) methods help, they still struggle with complex tasks involving multiple entities and steps. An improved method is needed to effectively address complex question answering.

**Method:** Matrix of Thought (MoT) is proposed as a novel and efficient thought structure for complex question answering. It uses a 'column-cell communication' mechanism to promote multi-strategy and deep-level reasoning, reducing redundancy. A fact-correction mechanism is also developed using knowledge units from retrieved knowledge graph triples and raw text.

**Result:** The MTQA framework developed using MoT outperforms state-of-the-art methods on four widely-used datasets in terms of F1 and EM scores, demonstrating both efficiency and accuracy.

**Conclusion:** The Matrix of Thought (MoT) and the associated MTQA framework offer significant improvements in reasoning capabilities for complex question answering tasks, achieving higher accuracy with less reasoning time compared to baseline methods.

**Abstract:** Complex Question Answering (QA) is a fundamental and challenging task in NLP.
While large language models (LLMs) exhibit impressive performance in QA, they
suffer from significant performance degradation when facing complex and
abstract QA tasks due to insufficient reasoning capabilities. Works such as
Chain-of-Thought (CoT) and Tree-of-Thought (ToT) aim to enhance LLMs' reasoning
abilities, but they face issues such as in-layer redundancy in tree structures
and single paths in chain structures. Although some studies utilize
Retrieval-Augmented Generation (RAG) methods to assist LLMs in reasoning, the
challenge of effectively utilizing large amounts of information involving
multiple entities and hops remains critical. To address this, we propose the
Matrix of Thought (MoT), a novel and efficient LLM thought structure. MoT
explores the problem in both horizontal and vertical dimensions through the
"column-cell communication" mechanism, enabling LLMs to actively engage in
multi-strategy and deep-level thinking, reducing redundancy within the column
cells and enhancing reasoning capabilities. Furthermore, we develop a
fact-correction mechanism by constructing knowledge units from retrieved
knowledge graph triples and raw text to enhance the initial knowledge for LLM
reasoning and correct erroneous answers. This leads to the development of an
efficient and accurate QA framework (MTQA). Experimental results show that our
framework outperforms state-of-the-art methods on four widely-used datasets in
terms of F1 and EM scores, with reasoning time only 14.4\% of the baseline
methods, demonstrating both its efficiency and accuracy. The code for this
framework is available at https://github.com/lyfiter/mtqa.

</details>


### [27] [Decoding the Poetic Language of Emotion in Korean Modern Poetry: Insights from a Human-Labeled Dataset and AI Modeling](https://arxiv.org/abs/2509.03932)
*Iro Lim,Haein Ji,Byungjun Kim*

Main category: cs.CL

> KPoEM is a novel dataset and model for emotion analysis in modern Korean poetry, demonstrating superior performance and opening avenues for the computational study of poetic emotions.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to fill the gap in the underexplored domain of computational emotion analysis in Korean poetry, which is rich in figurative language and cultural specificity.

**Method:** This study introduces KPoEM, a multi-label emotion dataset of 7,662 entries from modern Korean poetry, annotated with 44 fine-grained emotion categories. A state-of-the-art Korean language model was fine-tuned on this dataset.

**Result:** The fine-tuned KPoEM model significantly outperformed previous models, achieving an F1-micro score of 0.60 compared to 0.34 from models trained on general corpora.

**Conclusion:** The study suggests the KPoEM model can effectively identify temporal and culturally specific emotional expressions in modern Korean poetry, presenting new possibilities for quantitative exploration of poetic emotions.

**Abstract:** This study introduces KPoEM (Korean Poetry Emotion Mapping) , a novel dataset
for computational emotion analysis in modern Korean poetry. Despite remarkable
progress in text-based emotion classification using large language models,
poetry-particularly Korean poetry-remains underexplored due to its figurative
language and cultural specificity. We built a multi-label emotion dataset of
7,662 entries, including 7,007 line-level entries from 483 poems and 615
work-level entries, annotated with 44 fine-grained emotion categories from five
influential Korean poets. A state-of-the-art Korean language model fine-tuned
on this dataset significantly outperformed previous models, achieving 0.60
F1-micro compared to 0.34 from models trained on general corpora. The KPoEM
model, trained through sequential fine-tuning-first on general corpora and then
on the KPoEM dataset-demonstrates not only an enhanced ability to identify
temporally and culturally specific emotional expressions, but also a strong
capacity to preserve the core sentiments of modern Korean poetry. This study
bridges computational methods and literary analysis, presenting new
possibilities for the quantitative exploration of poetic emotions through
structured data that faithfully retains the emotional and cultural nuances of
Korean literature.

</details>


### [28] [SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented Generation via Distribution Self-Alignment](https://arxiv.org/abs/2509.03934)
*Yuqing Huang,Rongyang Zhang,Qimeng Wang,Chengqiang Lu,Yan Gao,Yi Wu,Yao Hu,Xuyang Zhi,Guiquan Liu,Xin Li,Hao Wang,Enhong Chen*

Main category: cs.CL

> 研究提出SelfAug方法，通过保持模型的语义分布减轻灾难性遗忘，实验表明SelfAug在下游性能和一般能力保留之间达到较好平衡。

<details>
  <summary>Details</summary>

**Motivation:** 解决在监督微调（特别是在检索增强生成(RAG)场景中）导致的灾难性遗忘问题，这个问题会使模型失去先前获得的知识和一般能力。

**Method:** 通过SelfAug自分布对齐方法，将输入序列的logits对齐以保持模型的语义分布，从而减轻灾难性遗忘，并提高下游性能。

**Result:** 实验结果表明，SelfAug在保持模型一般语义分布的同时提高了下游任务的性能，有效地解决了灾难性遗忘问题。

**Conclusion:** SelfAug实现了在下游学习和保持模型一般能力之间的优越平衡，本研究不仅深化了对RAG场景中灾难性遗忘的理解，还提供了一种跨多样化微调场景的实际解决方案。

**Abstract:** Recent advancements in large language models (LLMs) have revolutionized
natural language processing through their remarkable capabilities in
understanding and executing diverse tasks. While supervised fine-tuning,
particularly in Retrieval-Augmented Generation (RAG) scenarios, effectively
enhances task-specific performance, it often leads to catastrophic forgetting,
where models lose their previously acquired knowledge and general capabilities.
Existing solutions either require access to general instruction data or face
limitations in preserving the model's original distribution. To overcome these
limitations, we propose SelfAug, a self-distribution alignment method that
aligns input sequence logits to preserve the model's semantic distribution,
thereby mitigating catastrophic forgetting and improving downstream
performance. Extensive experiments demonstrate that SelfAug achieves a superior
balance between downstream learning and general capability retention. Our
comprehensive empirical analysis reveals a direct correlation between
distribution shifts and the severity of catastrophic forgetting in RAG
scenarios, highlighting how the absence of RAG capabilities in general
instruction tuning leads to significant distribution shifts during fine-tuning.
Our findings not only advance the understanding of catastrophic forgetting in
RAG contexts but also provide a practical solution applicable across diverse
fine-tuning scenarios. Our code is publicly available at
https://github.com/USTC-StarTeam/SelfAug.

</details>


### [29] [SPFT-SQL: Enhancing Large Language Model for Text-to-SQL Parsing by Self-Play Fine-Tuning](https://arxiv.org/abs/2509.03937)
*Yuhao Zhang,Shaoming Duan,Jinhang Su,Chuanyi Liu,Peiyi Han*

Main category: cs.CL

> A new method for self-play fine-tuning, SPFT-SQL, is proposed to improve the Text-to-SQL performance of large language models by enhancing the fine-tuning data quality and adjusting the self-play loss function.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitations of the SPIN method in generating accurate SQL queries by enhancing the fine-tuning process.

**Method:** The paper proposes SPFT-SQL, which includes a verification-based iterative fine-tuning approach before self-play and an error-driven loss method during self-play to improve the ability of models to generate accurate SQL queries.

**Result:** The approach demonstrates superior performance over state-of-the-art methods through extensive experiments on open-source LLMs and widely used benchmarks.

**Conclusion:** Experiments show that the proposed SPFT-SQL outperforms existing state-of-the-art methods.

**Abstract:** Despite the significant advancements of self-play fine-tuning (SPIN), which
can transform a weak large language model (LLM) into a strong one through
competitive interactions between models of varying capabilities, it still faces
challenges in the Text-to-SQL task. SPIN does not generate new information, and
the large number of correct SQL queries produced by the opponent model during
self-play reduces the main model's ability to generate accurate SQL queries. To
address this challenge, we propose a new self-play fine-tuning method tailored
for the Text-to-SQL task, called SPFT-SQL. Prior to self-play, we introduce a
verification-based iterative fine-tuning approach, which synthesizes
high-quality fine-tuning data iteratively based on the database schema and
validation feedback to enhance model performance, while building a model base
with varying capabilities. During the self-play fine-tuning phase, we propose
an error-driven loss method that incentivizes incorrect outputs from the
opponent model, enabling the main model to distinguish between correct SQL and
erroneous SQL generated by the opponent model, thereby improving its ability to
generate correct SQL. Extensive experiments and in-depth analyses on six
open-source LLMs and five widely used benchmarks demonstrate that our approach
outperforms existing state-of-the-art (SOTA) methods.

</details>


### [30] [VoxRole: A Comprehensive Benchmark for Evaluating Speech-Based Role-Playing Agents](https://arxiv.org/abs/2509.03940)
*Weihao Wu,Liang Cao,Xinyu Wu,Zhiwei Lin,Rui Niu,Jingbei Li,Zhiyong Wu*

Main category: cs.CL

> 本研究提出了VoxRole，这是首个用于评估语音角色扮演对话系统的全面基准，以解决现有研究缺乏副语言特征处理及标准化评估的问题。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在解决当前语音角色扮演对话系统评估基准缺失的问题，尤其是缺乏长期人格一致性评估的标准。现有的工作多集中在文本模式上，忽略了语音中的重要副语言特征。

**Method:** 该论文提出了一种新的两阶段自动化流水线来构建VoxRole，首先将电影音频与脚本对齐，然后使用大型语言模型系统地构建每个角色的多维特征。VoxRole 是一个全面的基准，由来自261部电影的1228个独特角色的13335个多轮对话组成，总时长为65.6小时的语音数据。

**Result:** 通过使用VoxRole，该研究对当前的口头对话模型进行了多维度的评估，揭示了这些模型在维持人格一致性方面的强项和弱点。

**Conclusion:** VoxRole的引入填补了语音角色扮演对话系统评估中的重要空白，并为未来的相关研究提供了有价值的基准。

**Abstract:** Recent significant advancements in Large Language Models (LLMs) have greatly
propelled the development of Role-Playing Conversational Agents (RPCAs). These
systems aim to create immersive user experiences through consistent persona
adoption. However, current RPCA research faces dual limitations. First,
existing work predominantly focuses on the textual modality, entirely
overlooking critical paralinguistic features including intonation, prosody, and
rhythm in speech, which are essential for conveying character emotions and
shaping vivid identities. Second, the speech-based role-playing domain suffers
from a long-standing lack of standardized evaluation benchmarks. Most current
spoken dialogue datasets target only fundamental capability assessments,
featuring thinly sketched or ill-defined character profiles. Consequently, they
fail to effectively quantify model performance on core competencies like
long-term persona consistency. To address this critical gap, we introduce
VoxRole, the first comprehensive benchmark specifically designed for the
evaluation of speech-based RPCAs. The benchmark comprises 13335 multi-turn
dialogues, totaling 65.6 hours of speech from 1228 unique characters across 261
movies. To construct this resource, we propose a novel two-stage automated
pipeline that first aligns movie audio with scripts and subsequently employs an
LLM to systematically build multi-dimensional profiles for each character.
Leveraging VoxRole, we conduct a multi-dimensional evaluation of contemporary
spoken dialogue models, revealing crucial insights into their respective
strengths and limitations in maintaining persona consistency.

</details>


### [31] [CANDY: Benchmarking LLMs' Limitations and Assistive Potential in Chinese Misinformation Fact-Checking](https://arxiv.org/abs/2509.03957)
*Ruiling Guo,Xinwei Yang,Chen Huang,Tong Zhang,Yong Hu*

Main category: cs.CL

> 研究了大语言模型在中文事实核查中的能力，发现虽然存在局限性，如“事实伪造”失败模式，但LLMs作为辅助工具具有改善人类性能的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 随着大语言模型（LLMs）应用的增加，其在事实核查以阻止虚假信息的有效性尚不确定。为此，我们提出了CANDY基准来评估LLMs在检查虚假信息上的能力。

**Method:** 我们开发了CANDY基准，用于系统评估大语言模型（LLMs）在检查中文虚假信息的能力。我们编纂了一个包含约20,000个实例的标注数据集。

**Result:** 研究显示当前LLMs在生成准确的事实核查结论上存在限制。最常见的是所谓的“事实伪造”失败模式。同时，发现LLMs作为辅助工具能够显著提升人类处理能力。

**Conclusion:** 尽管LLMs在单独使用时对于事实核查不可靠，但研究结果表明它们作为辅助工具具有巨大的潜力。

**Abstract:** The effectiveness of large language models (LLMs) to fact-check
misinformation remains uncertain, despite their growing use. To this end, we
present CANDY, a benchmark designed to systematically evaluate the capabilities
and limitations of LLMs in fact-checking Chinese misinformation. Specifically,
we curate a carefully annotated dataset of ~20k instances. Our analysis shows
that current LLMs exhibit limitations in generating accurate fact-checking
conclusions, even when enhanced with chain-of-thought reasoning and few-shot
prompting. To understand these limitations, we develop a taxonomy to categorize
flawed LLM-generated explanations for their conclusions and identify factual
fabrication as the most common failure mode. Although LLMs alone are unreliable
for fact-checking, our findings indicate their considerable potential to
augment human performance when deployed as assistive tools in scenarios. Our
dataset and code can be accessed at https://github.com/SCUNLP/CANDY

</details>


### [32] [Exploring NLP Benchmarks in an Extremely Low-Resource Setting](https://arxiv.org/abs/2509.03962)
*Ulin Nuha,Adam Jatowt*

Main category: cs.CL

> 文章专注于资源稀缺的语言Ladin，创建了首次公开的情感分析和多项选择问答(MCQA)数据集，通过合成数据集提高了意大利语-Ladin翻译的质量。

<details>
  <summary>Details</summary>

**Motivation:** 针对资源极其匮乏的语言（如土著语言）的大型语言模型的有效性减弱，主要是由于缺乏标注数据。尽管人们对这些语言的自然语言处理(NLP)数据集的兴趣越来越大，但高质量NLP数据集的可用性仍然有限，这使得开发强大的语言技术变得困难。这篇论文通过专注于Ladin，这是一种濒危的罗曼语，特别是Val Badia方言，来解决这一差距。

**Method:** 通过使用少量的并行Ladin-意大利语句子对，我们创建了用于情感分析和多项选择问答(MCQA)的合成数据集，这些数据集是通过翻译单语意大利语数据生成的。为了确保语言质量和可靠性，我们在方法中应用了严格的过滤和反向翻译程序。

**Result:** 将这些合成数据集纳入机器翻译训练后，显著改进了已经存在的意大利语-Ladin翻译基线。

**Conclusion:** 我们的贡献包括首次公开发布的Ladin情感分析和MCQA数据集，为这一代表性不足的语言提供了基础性的研究资源，支持更广泛的NLP研究和下游应用程序。

**Abstract:** The effectiveness of Large Language Models (LLMs) diminishes for extremely
low-resource languages, such as indigenous languages, primarily due to the lack
of labeled data. Despite growing interest, the availability of high-quality
natural language processing (NLP) datasets for these languages remains limited,
making it difficult to develop robust language technologies. This paper
addresses such gap by focusing on Ladin, an endangered Romance language,
specifically targeting the Val Badia variant. Leveraging a small set of
parallel Ladin-Italian sentence pairs, we create synthetic datasets for
sentiment analysis and multiple-choice question answering (MCQA) by translating
monolingual Italian data. To ensure linguistic quality and reliability, we
apply rigorous filtering and back-translation procedures in our method. We
further demonstrate that incorporating these synthetic datasets into machine
translation training leads to substantial improvements over existing
Italian-Ladin translation baselines. Our contributions include the first
publicly available sentiment analysis and MCQA datasets for Ladin, establishing
foundational resources that can support broader NLP research and downstream
applications for this underrepresented language.

</details>


### [33] [Expanding Foundational Language Capabilities in Open-Source LLMs through a Korean Case Study](https://arxiv.org/abs/2509.03972)
*Junghwan Lim,Gangwon Jo,Sungmin Lee,Jiyoung Park,Dongseok Kim,Jihwan Kim,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Kibong Choi,Jaeyeon Huh,Beomgyu Kim,Jangwoong Kim,Taehyun Kim,Haesol Lee,Jeesoo Lee,Dongpin Oh,Changseok Song,Daewon Suh*

Main category: cs.CL

> Llama-3-Motif是一个设计用于增强韩语能力的语言模型，同时保持英语性能。模型使用先进训练技术开发，在韩语特定基准测试中表现出色，与GPT-4性能相当。

<details>
  <summary>Details</summary>

**Motivation:** 目的是提高语言模型在韩语能力上的表现，同时不牺牲其在英语上的性能。

**Method:** 介绍了Llama-3-Motif，这是一个由1020亿参数构成的语言模型，专为增强韩语能力而设计，同时保持在英语上的强大性能。该模型基于Llama 3架构，采用LlamaPro和Masked Structure Growth等先进训练技术进行开发，能够在不改变其核心Transformer架构的情况下有效地扩展模型。使用MoAI平台在超尺度GPU集群上进行高效训练，并通过精心策划的数据集进行优化，该数据集保持了韩语和英语数据的均衡比例。

**Result:** 在韩语特定的基准测试中，Llama-3-Motif表现良好，超过了现有模型，并且其性能与GPT-4相当。

**Conclusion:** Llama-3-Motif在保持英文性能的同时，显著提升了韩语的表现，证明了其设计的有效性。

**Abstract:** We introduce Llama-3-Motif, a language model consisting of 102 billion
parameters, specifically designed to enhance Korean capabilities while
retaining strong performance in English. Developed on the Llama 3 architecture,
Llama-3-Motif employs advanced training techniques, including LlamaPro and
Masked Structure Growth, to effectively scale the model without altering its
core Transformer architecture. Using the MoAI platform for efficient training
across hyperscale GPU clusters, we optimized Llama-3-Motif using a carefully
curated dataset that maintains a balanced ratio of Korean and English data.
Llama-3-Motif shows decent performance on Korean-specific benchmarks,
outperforming existing models and achieving results comparable to GPT-4.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [34] [Towards Efficient General Feature Prediction in Masked Skeleton Modeling](https://arxiv.org/abs/2509.03609)
*Shengkai Sun,Zefan Zhang,Jianfeng Dong,Zhiyong Cheng,Xiaojun Chang,Meng Wang*

Main category: cs.CV

> 本文提出一种称为GFP的新框架，该框架用高级特征预测替代常规骨架重建，并通过实验表明确实在效率和表示质量上优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 目前大多数自监督骨架行为识别方法仅限于对原始关节坐标或其简单变体进行重建，导致计算冗余和语义表示有限。为了解决这个问题，本文提出了一种新的方法来克服这些限制。

**Method:** 本文提出了一个称为通用特征预测（GFP）的新框架，用于高效的遮罩骨架建模。主要创新之处在于用从局部运动模式到全局语义表示的高级特征预测代替常规的低级重建。具体来说，引入了一个协作学习框架，其中轻量级的目标生成网络动态生成跨空间-时间层次的多样化监督信号，避免依赖预计算的离线特征。框架采用约束优化确保特征多样性同时防止模型崩溃。

**Result:** 在NTU RGB+D 60, NTU RGB+D 120 和 PKU-MMD数据集上的实验展示了该方法的优势：计算效率高（比标准的遮罩骨架建模方法快6.2倍）和表示质量更高，在各种下游任务中实现了最先进的性能。

**Conclusion:** 本研究显示了一种新的GFP框架的有效性，该框架在加速训练效率和提高表示质量方面显著优于现有的遮罩骨架建模方法。

**Abstract:** Recent advances in the masked autoencoder (MAE) paradigm have significantly
propelled self-supervised skeleton-based action recognition. However, most
existing approaches limit reconstruction targets to raw joint coordinates or
their simple variants, resulting in computational redundancy and limited
semantic representation. To address this, we propose a novel General Feature
Prediction framework (GFP) for efficient mask skeleton modeling. Our key
innovation is replacing conventional low-level reconstruction with high-level
feature prediction that spans from local motion patterns to global semantic
representations. Specifically, we introduce a collaborative learning framework
where a lightweight target generation network dynamically produces diversified
supervision signals across spatial-temporal hierarchies, avoiding reliance on
pre-computed offline features. The framework incorporates constrained
optimization to ensure feature diversity while preventing model collapse.
Experiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits
of our approach: Computational efficiency (with 6.2$\times$ faster training
than standard masked skeleton modeling methods) and superior representation
quality, achieving state-of-the-art performance in various downstream tasks.

</details>


### [35] [Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge](https://arxiv.org/abs/2509.03614)
*Seungho Choe,Xiaoli Qin,Abubakr Shafique,Amanda Dy,Dimitri Androutsos,Susan Done,April Khademi*

Main category: cs.CV

> 本文提出了一种老师-学生模型，用以解决AI在有丝分裂检测中的领域转移问题，并在两项初步测试中取得了较为理想的成绩。

<details>
  <summary>Details</summary>

**Motivation:** 人工计算有丝分裂图像费时且存在观察者间的变差性。尽管人工智能有望通过自动检测有丝分裂提供解决方案，但AI工具易受领域转移的影响，导致性能显著下降，原因是训练和测试集之间的差异，包括各器官、物种的形态多样性以及染色协议的变化。此外，有丝分裂的数量远少于正常细胞核的数量，这为检测任务引入了严重的数据不平衡问题。为了解决这些问题，这项工作提出了一个新颖的方法。

**Method:** 本研究将有丝分裂检测定义为像素级分割问题，并提出了一种老师-学生模型，同时解决有丝分裂检测（Track 1）和非典型有丝分裂分类（Track 2）。该方法基于UNet分割主干网络，并集成了领域泛化模块，包括对比表示学习和领域对抗训练。采用老师-学生策略生成像素级伪掩膜，不仅用于标注的有丝分裂和难以识别的负面样本，还用于正常的细胞核，从而增强特征辨别能力和提高对领域转移的鲁棒性。对于分类任务，引入了多尺度CNN分类器，利用分割模型中的特征图在多任务学习框架中工作。

**Result:** 在初步测试集中，算法的Track 1（有丝分裂检测）F1得分为0.7660，而Track 2（非典型有丝分裂分类）的平衡准确率为0.8414，这表明所提出的方法对于应对有丝分裂检测中存在的领域转移问题和数据不平衡情况具有良好效果。

**Conclusion:** 此方法通过结合像素级分割和多尺度CNN分类器，在初步测试集中分别获得了Track 1 的F1值0.7660和Track 2的平衡精度0.8414，证明了将分割检测与分类整合到统一框架中的有效性，以及在稳健有丝分裂分析中的潜力。

**Abstract:** Counting mitotic figures is time-intensive for pathologists and leads to
inter-observer variability. Artificial intelligence (AI) promises a solution by
automatically detecting mitotic figures while maintaining decision consistency.
However, AI tools are susceptible to domain shift, where a significant drop in
performance can occur due to differences in the training and testing sets,
including morphological diversity between organs, species, and variations in
staining protocols. Furthermore, the number of mitoses is much less than the
count of normal nuclei, which introduces severely imbalanced data for the
detection task. In this work, we formulate mitosis detection as a pixel-level
segmentation and propose a teacher-student model that simultaneously addresses
mitosis detection (Track 1) and atypical mitosis classification (Track 2). Our
method is based on a UNet segmentation backbone that integrates domain
generalization modules, namely contrastive representation learning and
domain-adversarial training. A teacher-student strategy is employed to generate
pixel-level pseudo-masks not only for annotated mitoses and hard negatives but
also for normal nuclei, thereby enhancing feature discrimination and improving
robustness against domain shift. For the classification task, we introduce a
multi-scale CNN classifier that leverages feature maps from the segmentation
model within a multi-task learning paradigm. On the preliminary test set, the
algorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of
0.8414 in Track 2, demonstrating the effectiveness of integrating
segmentation-based detection and classification into a unified framework for
robust mitosis analysis.

</details>


### [36] [Multi Attribute Bias Mitigation via Representation Learning](https://arxiv.org/abs/2509.03616)
*Rajeev Ranjan Dwivedi,Ankur Kumar,Vinod K Kurmi*

Main category: cs.CV

> 提出了GMBM框架，解决图像中的多种交叉偏见问题，通过一个两阶段框架实现。

<details>
  <summary>Details</summary>

**Motivation:** 现代视觉模型因图像中的多种交叉偏见（如纹理、水印、性别化妆、场景物体组合等）而受到影响，这些偏见共同削弱了模型的稳健性和公平性。单独处理每个偏见往往是不够的，因为这可能导致其他偏见的出现或增强。

**Method:** 使用了名为GMBM的两阶段框架解决多偏见问题，第一阶段是ABIL，通过训练每个属性的编码器并将其与主骨干网络集成，促使分类器显式识别这些偏见；第二阶段是梯度抑制微调，从骨干网络的梯度中修剪掉这些偏见的方向，最终得到一个忽略这些偏见的紧凑网络。

**Result:** 在FB CMNIST、CelebA和COCO上提升了最差组的精度，将多属性偏见放大减半，并显著降低了SBA指标，即使偏见的复杂性和分布偏移增加，也证明了GMBM是处理视觉识别中多偏见问题的首个实用、端到端解决方案。

**Conclusion:** GMBM是一个减少测试时视觉模型多属性偏见的有效方法，同时引入了SBA指标，解决了现有偏见度量在子组不平衡和训练测试分布转移下的失效问题。

**Abstract:** Real world images frequently exhibit multiple overlapping biases, including
textures, watermarks, gendered makeup, scene object pairings, etc. These biases
collectively impair the performance of modern vision models, undermining both
their robustness and fairness. Addressing these biases individually proves
inadequate, as mitigating one bias often permits or intensifies others. We
tackle this multi bias problem with Generalized Multi Bias Mitigation (GMBM), a
lean two stage framework that needs group labels only while training and
minimizes bias at test time. First, Adaptive Bias Integrated Learning (ABIL)
deliberately identifies the influence of known shortcuts by training encoders
for each attribute and integrating them with the main backbone, compelling the
classifier to explicitly recognize these biases. Then Gradient Suppression Fine
Tuning prunes those very bias directions from the backbone's gradients, leaving
a single compact network that ignores all the shortcuts it just learned to
recognize. Moreover we find that existing bias metrics break under subgroup
imbalance and train test distribution shifts, so we introduce Scaled Bias
Amplification (SBA): a test time measure that disentangles model induced bias
amplification from distributional differences. We validate GMBM on FB CMNIST,
CelebA, and COCO, where we boost worst group accuracy, halve multi attribute
bias amplification, and set a new low in SBA even as bias complexity and
distribution shifts intensify, making GMBM the first practical, end to end
multibias solution for visual recognition. Project page:
http://visdomlab.github.io/GMBM/

</details>


### [37] [Lightweight image segmentation for echocardiography](https://arxiv.org/abs/2509.03631)
*Anders Kjelsrud,Lasse Løvstakken,Erik Smistad,Håvard Dalen,Gilles Van De Vyver*

Main category: cs.CV

> 通过简化nnU-Net模型，开发了一个轻量级U-Net模型，该模型在CAMUS数据集上达到了与nnU-Net相似的性能，同时减少了模型参数和加快了处理速度。

<details>
  <summary>Details</summary>

**Motivation:** nnU-Net模型虽然性能优秀，但体积大且运行速度慢，限制了其在实际场景中的应用。因此，为了开发一个可以实现实时自动提取临床测量指标且参数量少的模型，作者进行了本项研究。

**Method:** Structure

**Result:** {"tldr": "通过简化nnU-Net模型，开发了一个轻量级U-Net模型，该模型在CAMUS数据集上达到了与nnU-Net相似的性能，同时减少了模型参数和加快了处理速度。", "motivation": "nnU-Net模型虽然性能优秀，但体积大且运行速度慢，限制了其在实际场景中的应用。因此，为了开发一个可以实现实时自动提取临床测量指标且参数量少的模型，作者进行了本项研究。", "method": "作者通过对数据增强策略、架构修改、损失函数及后处理技术的逐步评估，确定了对于心脏分割最有效的nnU-Net组件，基于此开发了一个轻量级的U-Net模型。", "result": "该模型在CAMUS数据集上取得了0.93/0.85/0.89的Dice评分，与nnU-Net相比没有显著差异，但模型更小（2M vs 33M）且速度更快（1.35ms vs 5.40ms）。", "conclusion": "简化后的模型在保持相近分割性能的同时，通过减少参数量和加快处理速度，更适合应用于实时环境下。"}

**Conclusion:** 简化后的模型在保持相近分割性能的同时，通过减少参数量和加快处理速度，更适合应用于实时环境下。

**Abstract:** Accurate segmentation of the left ventricle in echocardiography can enable
fully automatic extraction of clinical measurements such as volumes and
ejection fraction. While models configured by nnU-Net perform well, they are
large and slow, thus limiting real-time use. We identified the most effective
components of nnU-Net for cardiac segmentation through an ablation study,
incrementally evaluating data augmentation schemes, architectural
modifications, loss functions, and post-processing techniques. Our analysis
revealed that simple affine augmentations and deep supervision drive
performance, while complex augmentations and large model capacity offer
diminishing returns. Based on these insights, we developed a lightweight U-Net
(2M vs 33M parameters) that achieves statistically equivalent performance to
nnU-Net on CAMUS (N=500) with Dice scores of 0.93/0.85/0.89 vs 0.93/0.86/0.89
for LV/MYO/LA ($p>0.05$), while being 16 times smaller and 4 times faster
(1.35ms vs 5.40ms per frame) than the default nnU-Net configuration.
Cross-dataset evaluation on an internal dataset (N=311) confirms comparable
generalization.

</details>


### [38] [treeX: Unsupervised Tree Instance Segmentation in Dense Forest Point Clouds](https://arxiv.org/abs/2509.03633)
*Josafat-Mattias Burmeister,Andreas Tockner,Stefan Reder,Markus Engel,Rico Richter,Jan-Peter Mund,Jürgen Döllner*

Main category: cs.CV

> 改进了treeX算法，使其更加适用于多种激光扫描数据，相比于原算法，减少了运行时间，提高了准确性。

<details>
  <summary>Details</summary>

**Motivation:** 针对深度学习方法需要大量标注数据和计算资源的问题，提出了一种资源高效的替代方案。

**Method:** 一种改进的treeX算法，结合基于聚类的主干检测和区域生长进行冠层划分，适用于不同类型激光扫描数据（地面站式TLS、个人激光扫描PLS和无人机激光扫描ULS），并通过两种参数预设进行优化。

**Result:** 在六种公开数据集上测试，改进后的算法相比原版treeX算法运行时间减少且准确度提高，在地面数据上的F1分数提高0.11到0.49；对于ULS数据，改进后的算法F1分数达0.58，而原算法无法正确分割。

**Conclusion:** 本文提出的算法在特定条件下可作为深度学习方法的资源高效替代，也可用于深度学习模型的半自动标签生成，已开源实现。

**Abstract:** Close-range laser scanning provides detailed 3D captures of forest stands but
requires efficient software for processing 3D point cloud data and extracting
individual trees. Although recent studies have introduced deep learning methods
for tree instance segmentation, these approaches require large annotated
datasets and substantial computational resources. As a resource-efficient
alternative, we present a revised version of the treeX algorithm, an
unsupervised method that combines clustering-based stem detection with region
growing for crown delineation. While the original treeX algorithm was developed
for personal laser scanning (PLS) data, we provide two parameter presets, one
for ground-based laser scanning (stationary terrestrial - TLS and PLS), and one
for UAV-borne laser scanning (ULS). We evaluated the method on six public
datasets (FOR-instance, ForestSemantic, LAUTx, NIBIO MLS, TreeLearn, Wytham
Woods) and compared it to six open-source methods (original treeX, treeiso,
RayCloudTools, ForAINet, SegmentAnyTree, TreeLearn). Compared to the original
treeX algorithm, our revision reduces runtime and improves accuracy, with
instance detection F$_1$-score gains of +0.11 to +0.49 for ground-based data.
For ULS data, our preset achieves an F$_1$-score of 0.58, whereas the original
algorithm fails to segment any correct instances. For TLS and PLS data, our
algorithm achieves accuracy similar to recent open-source methods, including
deep learning. Given its algorithmic design, we see two main applications for
our method: (1) as a resource-efficient alternative to deep learning approaches
in scenarios where the data characteristics align with the method design
(sufficient stem visibility and point density), and (2) for the semi-automatic
generation of labels for deep learning models. To enable broader adoption, we
provide an open-source Python implementation in the pointtree package.

</details>


### [39] [Reg3D: Reconstructive Geometry Instruction Tuning for 3D Scene Understanding](https://arxiv.org/abs/2509.03635)
*Hongpei Zheng,Lintao Xiang,Qijun Yang,Qian Lin,Hujun Yin*

Main category: cs.CV

> 本文引入了Reg3D，一个新颖的重构几何指令调优框架，通过在训练过程中直接加入几何感知监督来提升3D场景理解能力。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法在仅使用文本监督而缺乏所需几何约束的情况下难以有效学习鲁棒的3D空间表示。

**Method:** Reg3D采用双重监督范式，在输入和学习目标两个层面利用3D几何信息，在双编码器架构中设计了互补的对象级和帧级重构任务，以强制几何一致性来促进空间推理能力的发展。

**Result:** 实验表明，本文方法在多个基准测试中展现了显著的性能提升，确立了多模态空间感知模型训练的新范式。

**Conclusion:** 引入Reg3D框架，有效解决了现有方法在3D场景理解方面的不足，通过重构几何结构而非仅仅描述它们，展示了在多任务上的显著性能提升。

**Abstract:** The rapid development of Large Multimodal Models (LMMs) has led to remarkable
progress in 2D visual understanding; however, extending these capabilities to
3D scene understanding remains a significant challenge. Existing approaches
predominantly rely on text-only supervision, which fails to provide the
geometric constraints required for learning robust 3D spatial representations.
In this paper, we introduce Reg3D, a novel Reconstructive Geometry Instruction
Tuning framework that addresses this limitation by incorporating geometry-aware
supervision directly into the training process. Our key insight is that
effective 3D understanding necessitates reconstructing underlying geometric
structures rather than merely describing them. Unlike existing methods that
inject 3D information solely at the input level, Reg3D adopts a
dual-supervision paradigm that leverages 3D geometric information both as input
and as explicit learning targets. Specifically, we design complementary
object-level and frame-level reconstruction tasks within a dual-encoder
architecture, enforcing geometric consistency to encourage the development of
spatial reasoning capabilities. Extensive experiments on ScanQA, Scan2Cap,
ScanRefer, and SQA3D demonstrate that Reg3D delivers substantial performance
improvements, establishing a new training paradigm for spatially aware
multimodal models.

</details>


### [40] [QuantV2X: A Fully Quantized Multi-Agent System for Cooperative Perception](https://arxiv.org/abs/2509.03704)
*Seth Z. Zhao,Huizhi Zhang,Zhaowei Li,Juntong Peng,Anthony Chui,Zewei Zhou,Zonglin Meng,Hao Xiang,Zhiyu Huang,Fujia Wang,Ran Tian,Chenfeng Xu,Bolei Zhou,Jiaqi Ma*

Main category: cs.CV

> QuantV2X是一个面向实时操作和资源受限环境的全量化的多代理系统，它在降低计算和传输成本的同时，提升了精度并减少了延迟，有效地解决了现实世界中车辆感知系统的关键问题。

<details>
  <summary>Details</summary>

**Motivation:** 提升车辆感知能力，减少遮挡并扩大视野，同时解决效率，延迟和现实世界部署的关键系统级问题。

**Method:** 采用了一种统一的端到端量化策略，应用于神经网络模型和传输的消息表示，同时减少了计算负载和传输带宽。

**Result:** 与全精度系统相比，尽管在低比特约束下，QuantV2X实现了可比的精度，但系统级延迟减少了3.2倍，mAP30提升了9.5。此外，它更有效地扩展，允许更大的模型在严格的内存预算内运行。

**Conclusion:** 全量化的多代理中间融合系统在现实世界中的部署是可行的。

**Abstract:** Cooperative perception through Vehicle-to-Everything (V2X) communication
offers significant potential for enhancing vehicle perception by mitigating
occlusions and expanding the field of view. However, past research has
predominantly focused on improving accuracy metrics without addressing the
crucial system-level considerations of efficiency, latency, and real-world
deployability. Noticeably, most existing systems rely on full-precision models,
which incur high computational and transmission costs, making them impractical
for real-time operation in resource-constrained environments. In this paper, we
introduce \textbf{QuantV2X}, the first fully quantized multi-agent system
designed specifically for efficient and scalable deployment of multi-modal,
multi-agent V2X cooperative perception. QuantV2X introduces a unified
end-to-end quantization strategy across both neural network models and
transmitted message representations that simultaneously reduces computational
load and transmission bandwidth. Remarkably, despite operating under low-bit
constraints, QuantV2X achieves accuracy comparable to full-precision systems.
More importantly, when evaluated under deployment-oriented metrics, QuantV2X
reduces system-level latency by 3.2$\times$ and achieves a +9.5 improvement in
mAP30 over full-precision baselines. Furthermore, QuantV2X scales more
effectively, enabling larger and more capable models to fit within strict
memory budgets. These results highlight the viability of a fully quantized
multi-agent intermediate fusion system for real-world deployment. The system
will be publicly released to promote research in this field:
https://github.com/ucla-mobility/QuantV2X.

</details>


### [41] [Transfer Learning-Based CNN Models for Plant Species Identification Using Leaf Venation Patterns](https://arxiv.org/abs/2509.03729)
*Bandita Bharadwaj,Ankur Mishra,Saurav Bharadwaj*

Main category: cs.CV

> 这项研究评估了三种深度学习模型在基于叶脉模式的植物分类中的效果，发现EfficientNetB0在测试准确率等方面表现最佳，表明深度学习技术在自动化植物分类中的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是评估深度学习技术在基于叶脉模式的植物种类分类中的应用潜力，这是植物分类学中一个重要但计算复杂的问题。

**Method:** 该研究评估了三种深度学习架构（ResNet50、MobileNetV2 和 EfficientNetB0）在基于叶脉模式的植物种类自动分类中的有效性。研究使用了包含15种不同植物物种的瑞典叶数据集进行模型训练和测试，每个物种有75张图像，总共1,125张图像。

**Result:** ResNet50的训练准确率为94.11%，但由于过拟合导致测试准确率降低到88.45%。MobileNetV2在测试中准确率为93.34%，表现了较好的泛化能力。EfficientNetB0表现最佳，测试准确率高达94.67%，并具有超过94.6%的精度、召回率和F1分数。

**Conclusion:** 结果表明深度学习模型，尤其是EfficientNetB0，有能力提供可扩展和准确的自动化植物分类工具。

**Abstract:** This study evaluates the efficacy of three deep learning architectures:
ResNet50, MobileNetV2, and EfficientNetB0 for automated plant species
classification based on leaf venation patterns, a critical morphological
feature with high taxonomic relevance. Using the Swedish Leaf Dataset
comprising images from 15 distinct species (75 images per species, totalling
1,125 images), the models were demonstrated using standard performance metrics
during training and testing phases. ResNet50 achieved a training accuracy of
94.11% but exhibited overfitting, reflected by a reduced testing accuracy of
88.45% and an F1 score of 87.82%. MobileNetV2 demonstrated better
generalization capabilities, attaining a testing accuracy of 93.34% and an F1
score of 93.23%, indicating its suitability for lightweight, real-time
applications. EfficientNetB0 outperformed both models, achieving a testing
accuracy of 94.67% with precision, recall, and F1 scores exceeding 94.6%,
highlighting its robustness in venation-based classification. The findings
underscore the potential of deep learning, particularly EfficientNetB0, in
developing scalable and accurate tools for automated plant taxonomy using
venation traits.

</details>


### [42] [LayoutGKN: Graph Similarity Learning of Floor Plans](https://arxiv.org/abs/2509.03737)
*Casper van Engelenburg,Jan van Gemert,Seyran Khademi*

Main category: cs.CV

> LayoutGKN通过推迟跨图节点级交互至联合嵌入结构的最后，利用可微分图核进行距离计算，从而在保持相似性计算准确性的同时提升了速度。

<details>
  <summary>Details</summary>

**Motivation:** 现有的图比较方法，如图匹配网络，依赖于代价高昂的跨图节点级交互，因此在推理时间上较慢。为了提高效率，研究者提出了LayoutGKN。

**Method:** 使用可微分图核作为距离函数，在最后的学习节点级嵌入上进行跨图节点级交互，以提高效率。这种方法与图匹配网络相比，在保持或提高相似性计算准确性的同时，显著提升了推理速度。

**Result:** LayoutGKN在相似性计算上与图匹配网络相当或更好，同时在推理速度上显著提升。

**Conclusion:** LayoutGKN通过创新的方法在效率和准确性之间取得了良好的平衡，提供了更快的图比较解决方案。

**Abstract:** Floor plans depict building layouts and are often represented as graphs to
capture the underlying spatial relationships. Comparison of these graphs is
critical for applications like search, clustering, and data visualization. The
most successful methods to compare graphs \ie, graph matching networks, rely on
costly intermediate cross-graph node-level interactions, therefore being slow
in inference time. We introduce \textbf{LayoutGKN}, a more efficient approach
that postpones the cross-graph node-level interactions to the end of the joint
embedding architecture. We do so by using a differentiable graph kernel as a
distance function on the final learned node-level embeddings. We show that
LayoutGKN computes similarity comparably or better than graph matching networks
while significantly increasing the speed.
\href{https://github.com/caspervanengelenburg/LayoutGKN}{Code and data} are
open.

</details>


### [43] [Singular Value Few-shot Adaptation of Vision-Language Models](https://arxiv.org/abs/2509.03740)
*Taha Koleilat,Hassan Rivaz,Yiming Xiao*

Main category: cs.CV

> .CLIP-SVD 是一种新颖的多模态和参数高效的适应技术，利用SVD修改CLIP的内部参数空间，仅调整CLIP参数矩阵的奇异值进行领域适应，使用模型总参数的0.04%达到了增强的适应性能。在自然和生物医学数据集上实现了最先进的分类结果，优于之前的准确性与泛化性方法。

<details>
  <summary>Details</summary>

**Motivation:** 解决CLIP在新细粒度领域的适应困难，避免额外组件的限制和预训练知识的损失。

**Method:** 通过奇异值分解（SVD）技术，仅调整CLIP参数矩阵的奇异值，不注入额外模块进行领域适应。

**Result:** CLIP-SVD 在自然和生物医学数据集上实现了最先进的分类结果，优于现有方法的准确性和泛化性。

**Conclusion:** CLIP-SVD 提供了一种参数高效的方法，可在不损失预训练性能的情况下改善领域适应。

**Abstract:** Vision-language models (VLMs) like CLIP have shown impressive zero-shot and
few-shot learning capabilities across diverse applications. However, adapting
these models to new fine-grained domains remains difficult due to reliance on
prompt engineering and the high cost of full model fine-tuning. Existing
adaptation approaches rely on augmented components, such as prompt tokens and
adapter modules, which could limit adaptation quality, destabilize the model,
and compromise the rich knowledge learned during pretraining. In this work, we
present \textbf{CLIP-SVD}, a novel \textit{multi-modal} and
\textit{parameter-efficient} adaptation technique that leverages Singular Value
Decomposition (SVD) to modify the internal parameter space of CLIP without
injecting additional modules. Specifically, we fine-tune only the singular
values of the CLIP parameter matrices to rescale the basis vectors for domain
adaptation while retaining the pretrained model. This design enables enhanced
adaptation performance using only \textbf{0.04\%} of the model's total
parameters and better preservation of its generalization ability. CLIP-SVD
achieves state-of-the-art classification results on 11 natural and 10
biomedical datasets, outperforming previous methods in both accuracy and
generalization under few-shot settings. Additionally, we leverage a natural
language-based approach to analyze the effectiveness and dynamics of the CLIP
adaptation to allow interpretability of CLIP-SVD. The code is publicly
available at https://github.com/HealthX-Lab/CLIP-SVD.

</details>


### [44] [STA-Net: A Decoupled Shape and Texture Attention Network for Lightweight Plant Disease Classification](https://arxiv.org/abs/2509.03754)
*Zongsen Qiu*

Main category: cs.CV

> 本论文提出了一种称为STA-Net的新模型，在无需大量训练的情况下，通过细化的神经架构搜索和特殊的形状-纹理注意力模块，实现了高效与高精度的植物疾病诊断，适用于边缘设备部署。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于局部病变形状和复杂纹理等微妙病理性特征无法通过现有的轻量级网络中的注意力机制捕捉，为了提高精简农业中的植物疾病诊断精度，特别是为了将高精度模型部署到边缘设备上，本论文提出了一个有特别设计的解决方法。

**Method:** 本论文提出了一种两阶段解决方案：首先，采用无训练神经架构搜索方法（DeepMAD）创建适用于边缘设备的高效网络骨干；其次，引入形状-纹理注意力模块（STAM），该模块将注意力分为两支，一支使用可变形卷积（DCNv4）用于形状感知，另一支使用Gabor滤波器组用于纹理感知。

**Result:** 实验结果表明，在CCMT植物疾病数据集上，STA-Net模型（含401K参数和51.1M FLOPs）达到了89.00%的准确率和88.96%的F1分数，证明STAM显著提高了性能。

**Conclusion:** 通过引入特定设计的注意力模块并集成领域知识，本论文为在边缘设备上部署精准农业AI提供了有前景的路径。

**Abstract:** Responding to rising global food security needs, precision agriculture and
deep learning-based plant disease diagnosis have become crucial. Yet, deploying
high-precision models on edge devices is challenging. Most lightweight networks
use attention mechanisms designed for generic object recognition, which poorly
capture subtle pathological features like irregular lesion shapes and complex
textures. To overcome this, we propose a twofold solution: first, using a
training-free neural architecture search method (DeepMAD) to create an
efficient network backbone for edge devices; second, introducing the
Shape-Texture Attention Module (STAM). STAM splits attention into two branches
-- one using deformable convolutions (DCNv4) for shape awareness and the other
using a Gabor filter bank for texture awareness. On the public CCMT plant
disease dataset, our STA-Net model (with 401K parameters and 51.1M FLOPs)
reached 89.00% accuracy and an F1 score of 88.96%. Ablation studies confirm
STAM significantly improves performance over baseline and standard attention
models. Integrating domain knowledge via decoupled attention thus presents a
promising path for edge-deployed precision agriculture AI. The source code is
available at https://github.com/RzMY/STA-Net.

</details>


### [45] [SLENet: A Guidance-Enhanced Network for Underwater Camouflaged Object Detection](https://arxiv.org/abs/2509.03786)
*Xinxin Wang,Han Sun,Ningzhong Liu,Huiyu Zhou,Yinan Yao*

Main category: cs.CV

> 论文介绍了一种新颖的深度学习架构SLENet，用于在水下复杂环境下检测伪装物体，并通过新的基准数据集DeepCamo证明了其相对于现有方法的优越性。

<details>
  <summary>Details</summary>

**Motivation:** 为了应对水下环境中由于光学畸变、水体浑浊和复杂生物特性导致的伪装物体检测准确性降低问题，我们定义了UCOD任务，并构建了专为此任务设计的DeepCamo基准数据集。

**Method:** 我们提出了SLENet，一种用于Underwater Camouflaged Object Detection (UCOD)的新框架。SLENet集成了Gamma-Asymmetric Enhancement (GAE)模块和Localization Guidance Branch (LGB)来增强多尺度特征表示，生成包含全球语义信息的位置图，以指导Multi-Scale Supervised Decoder (MSSD)生成更准确的预测结果。

**Result:** 实验显示，SLENet显著优于现有最强的COD模型，并在DeepCamo数据集及其他三个基准COD数据集上实现了更准确的伪装物体检测。

**Conclusion:** 实验结果证明，SLENet在DeepCamo数据集及三个基准COD数据集上均展现了优越性能，并且对于更广泛的COD任务具有很高的通用性。

**Abstract:** Underwater Camouflaged Object Detection (UCOD) aims to identify objects that
blend seamlessly into underwater environments. This task is critically
important to marine ecology. However, it remains largely underexplored and
accurate identification is severely hindered by optical distortions, water
turbidity, and the complex traits of marine organisms. To address these
challenges, we introduce the UCOD task and present DeepCamo, a benchmark
dataset designed for this domain. We also propose Semantic Localization and
Enhancement Network (SLENet), a novel framework for UCOD. We first benchmark
state-of-the-art COD models on DeepCamo to reveal key issues, upon which SLENet
is built. In particular, we incorporate Gamma-Asymmetric Enhancement (GAE)
module and a Localization Guidance Branch (LGB) to enhance multi-scale feature
representation while generating a location map enriched with global semantic
information. This map guides the Multi-Scale Supervised Decoder (MSSD) to
produce more accurate predictions. Experiments on our DeepCamo dataset and
three benchmark COD datasets confirm SLENet's superior performance over SOTA
methods, and underscore its high generality for the broader COD task.

</details>


### [46] [Fitting Image Diffusion Models on Video Datasets](https://arxiv.org/abs/2509.03794)
*Juhun Lee,Simon S. Woo*

Main category: cs.CV

> 研究通过引入时间归纳偏置的训练策略改进了图像扩散模型的训练效果。

<details>
  <summary>Details</summary>

**Motivation:** 现有的图像扩散模型训练基于独立采样的静态图像，这在捕获时间世界时信息不足。该方法旨在解决这一问题，提高训练速度、分布覆盖和模型泛化能力。

**Method:** 该研究提出了一种训练策略，利用连续视频帧中的时间归纳偏置来改进扩散模型的训练。该方法不需要修改架构，可以无缝集成到标准的扩散训练管道中。

**Result:** 实验结果表明，该方法在HandCo数据集上将收敛速度提高了超过2倍，并在训练和验证分布上实现了更低的FID值。此外，它还通过鼓励模型捕捉有意义的时间变化，提高了生成的多样性。优化分析显示，该正则化方法减少了梯度方差，有助于提高收敛速度。

**Conclusion:** 该研究提出的方法有效提高了图像扩散模型的训练效率和性能，展示了时间归纳偏置在扩散模型中的潜力。

**Abstract:** Image diffusion models are trained on independently sampled static images.
While this is the bedrock task protocol in generative modeling, capturing the
temporal world through the lens of static snapshots is information-deficient by
design. This limitation leads to slower convergence, limited distributional
coverage, and reduced generalization. In this work, we propose a simple and
effective training strategy that leverages the temporal inductive bias present
in continuous video frames to improve diffusion training. Notably, the proposed
method requires no architectural modification and can be seamlessly integrated
into standard diffusion training pipelines. We evaluate our method on the
HandCo dataset, where hand-object interactions exhibit dense temporal coherence
and subtle variations in finger articulation often result in semantically
distinct motions. Empirically, our method accelerates convergence by over
2$\text{x}$ faster and achieves lower FID on both training and validation
distributions. It also improves generative diversity by encouraging the model
to capture meaningful temporal variations. We further provide an optimization
analysis showing that our regularization reduces the gradient variance, which
contributes to faster convergence.

</details>


### [47] [MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting](https://arxiv.org/abs/2509.03800)
*Yuheng Li,Yenho Chen,Yuxiang Lai,Jike Zhong,Vanessa Wildman,Xiaofeng Yang*

Main category: cs.CV

> MedVista3D是一个用于3D CT分析的多尺度语义增强视觉-语言预训练框架，能够很好地解决疾病检测、整体解释和语义连贯报告的问题，表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 现有的3D视觉-语言模型在满足疾病检测、整体解释和语义连贯的自然语言报告三个方面的需求方面有局限。文章旨在解决临床实践中影像诊断错误的问题，如遗漏局部异常、缺乏全局背景和报告语言的变异性这些问题。这些问题在3D成像中被放大，因为临床医生必须检查每张扫描中的数百个切片。

**Method:** MedVista3D采用多尺度语义增强的视觉-语言预训练框架，用于3D CT分析。它在全体积上下文中进行局部和全局图像-文本对齐，以实现疾病的检测和整体解释。为了应对报告的变异性，它应用语言模型重写并引入放射学语义匹配银行以实现语义感知的对齐。

**Result:** MedVista3D在零样本疾病分类、报告检索和医学视觉问题回答方面达到了最先进的性能，并且很好地转移到器官分割和预后预测。

**Conclusion:** MedVista3D作为一个多尺度语义增强的视觉-语言预训练框架，有效解决了3D影像诊断中的问题，提高了诊断准确性和报告的一致性，展示了其在医学影像分析中的广泛应用潜力。

**Abstract:** Radiologic diagnostic errors-under-reading errors, inattentional blindness,
and communication failures-remain prevalent in clinical practice. These issues
often stem from missed localized abnormalities, limited global context, and
variability in report language. These challenges are amplified in 3D imaging,
where clinicians must examine hundreds of slices per scan. Addressing them
requires systems with precise localized detection, global volume-level
reasoning, and semantically consistent natural language reporting. However,
existing 3D vision-language models are unable to meet all three needs jointly,
lacking local-global understanding for spatial reasoning and struggling with
the variability and noise of uncurated radiology reports. We present
MedVista3D, a multi-scale semantic-enriched vision-language pretraining
framework for 3D CT analysis. To enable joint disease detection and holistic
interpretation, MedVista3D performs local and global image-text alignment for
fine-grained representation learning within full-volume context. To address
report variability, we apply language model rewrites and introduce a Radiology
Semantic Matching Bank for semantics-aware alignment. MedVista3D achieves
state-of-the-art performance on zero-shot disease classification, report
retrieval, and medical visual question answering, while transferring well to
organ segmentation and prognosis prediction. Code and datasets will be
released.

</details>


### [48] [Causality-guided Prompt Learning for Vision-language Models via Visual Granulation](https://arxiv.org/abs/2509.03803)
*Mengyu Gao,Qiulei Dong*

Main category: cs.CV

> CaPL方法通过将视觉特征分解并构建视觉颗粒，增强了基于CLIP的提示学习在细粒度识别任务中的能力，优于现有的方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大多数基于CLIP的提示学习方法在处理细粒度数据集方面能力有限。为了应对这一挑战，提出了一种通过视觉分粒化引导因果关系的文本提示学习方法，称为CaPL。

**Method:** CaPL方法包含两个模块：(1) 属性解耦模块使用布朗桥扩散模型将视觉特征分解为非个性化属性（由某些类别共享）和个性化属性（特定于单个类别）；(2) 颗粒学习模块通过两种因果推理策略整合上述属性来构建视觉颗粒，以促进识别。

**Result:** CaPL方法在15个数据集上的实验结果显著优于现有的提示学习方法，特别是在细粒度数据集上。

**Conclusion:** 大量的实验结果证明，CaPL方法在15个数据集上的表现显著优于现有的提示学习方法，尤其是在细粒度数据集上。

**Abstract:** Prompt learning has recently attracted much attention for adapting
pre-trained vision-language models (e.g., CLIP) to downstream recognition
tasks. However, most of the existing CLIP-based prompt learning methods only
show a limited ability for handling fine-grained datasets. To address this
issue, we propose a causality-guided text prompt learning method via visual
granulation for CLIP, called CaPL, where the explored visual granulation
technique could construct sets of visual granules for the text prompt to
capture subtle discrepancies among different fine-grained classes through
casual inference. The CaPL method contains the following two modules: (1) An
attribute disentanglement module is proposed to decompose visual features into
non-individualized attributes (shared by some classes) and individualized
attributes (specific to single classes) using a Brownian Bridge Diffusion
Model; (2) A granule learning module is proposed to construct visual granules
by integrating the aforementioned attributes for recognition under two causal
inference strategies. Thanks to the learned visual granules, more
discriminative text prompt is expected to be learned. Extensive experimental
results on 15 datasets demonstrate that our CaPL method significantly
outperforms the state-of-the-art prompt learning methods, especially on
fine-grained datasets.

</details>


### [49] [EGTM: Event-guided Efficient Turbulence Mitigation](https://arxiv.org/abs/2509.03808)
*Huanan Li,Rui Fan,Juntao Guan,Weidong Hao,Lai Rui,Tong Wu,Yikai Wang,Lin Gu*

Main category: cs.CV

> EGTM uses event cameras for efficient, high-resolution temporal lucky fusion, outperforming SOTA methods in model size, inference latency, model complexity, and restoration quality.

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of existing TM methods in computational and storage efficiency caused by the need for high-capacity networks and the coarse-grained turbulence dynamics between synchronous frames.

**Method:** We propose EGTM, a novel framework that leverages event cameras' capabilities to extract turbulence-free patches using the 'event-lucky insight', which correlates turbulence distortions with inverse spatiotemporal event stream distribution.

**Result:** We show EGTM surpasses current SOTA methods 710 times in model size, 214 times in inference latency, and 224 times in model complexity, while also improving restoration quality by +0.94 PSNR and +0.08 SSIM on a real-world dataset.

**Conclusion:** Introducing event cameras into the TM task provides significant efficiency gains and superior restoration quality, marking an important advancement in TM technology.

**Abstract:** Turbulence mitigation (TM) aims to remove the stochastic distortions and
blurs introduced by atmospheric turbulence into frame cameras. Existing
state-of-the-art deep-learning TM methods extract turbulence cues from multiple
degraded frames to find the so-called "lucky'', not distorted patch, for "lucky
fusion''. However, it requires high-capacity network to learn from
coarse-grained turbulence dynamics between synchronous frames with limited
frame-rate, thus fall short in computational and storage efficiency. Event
cameras, with microsecond-level temporal resolution, have the potential to
fundamentally address this bottleneck with efficient sparse and asynchronous
imaging mechanism. In light of this, we (i) present the fundamental
\textbf{``event-lucky insight''} to reveal the correlation between turbulence
distortions and inverse spatiotemporal distribution of event streams. Then,
build upon this insight, we (ii) propose a novel EGTM framework that extracts
pixel-level reliable turbulence-free guidance from the explicit but noisy
turbulent events for temporal lucky fusion. Moreover, we (iii) build the first
turbulence data acquisition system to contribute the first real-world
event-driven TM dataset. Extensive experimental results demonstrate that our
approach significantly surpass the existing SOTA TM method by 710 times, 214
times and 224 times in model size, inference latency and model complexity
respectively, while achieving the state-of-the-art in restoration quality
(+0.94 PSNR and +0.08 SSIM) on our real-world EGTM dataset. This demonstrating
the great efficiency merit of introducing event modality into TM task. Demo
code and data have been uploaded in supplementary material and will be released
once accepted.

</details>


### [50] [Focus Through Motion: RGB-Event Collaborative Token Sparsification for Efficient Object Detection](https://arxiv.org/abs/2509.03872)
*Nan Yang,Yang Wang,Zhanwen Liu,Yuchao Dai,Yang Liu,Xiangmo Zhao*

Main category: cs.CV

> FocusMamba通过设计EGMS策略和CMFF模块来自适应地稀疏化和整合RGB和事件数据中的有用信息，从而提高检测方法在准确性和效率方面的表现。

<details>
  <summary>Details</summary>

**Motivation:** 现有的RGB-事件检测方法在特征提取和融合过程中将两种模态的低信息区域处理方式相同，造成计算成本高且性能不佳。为减少计算冗余，已分别提出针对图像和事件模态的令牌稀疏化方法，但这些方法采用固定的令牌选择数或阈值，不利于保留复杂度不同的样本的有用信息。

**Method:** 设计理念是通过自适应协同稀疏化多模态特征及有效集成两模态互补信息来提升计算效率和精度。具体而言，提出了事件引导的多模态稀疏化策略(EGMS)，根据事件相机感知的场景内容变化自适应丢弃每个模态中的低信息区域。基于稀疏化结果，设计了跨模态聚焦融合(CMFF)模块，以捕捉和整合两模态的互补特征。

**Result:** 在DSEC-Det和PKU-DAVIS-SOD数据集上的实验表明，提出的方法在准确性和效率方面均优于现有方法。

**Conclusion:** 提出的方法通过自适应协作的方式稀疏化多模态特征并高效整合互补信息。

**Abstract:** Existing RGB-Event detection methods process the low-information regions of
both modalities (background in images and non-event regions in event data)
uniformly during feature extraction and fusion, resulting in high computational
costs and suboptimal performance. To mitigate the computational redundancy
during feature extraction, researchers have respectively proposed token
sparsification methods for the image and event modalities. However, these
methods employ a fixed number or threshold for token selection, hindering the
retention of informative tokens for samples with varying complexity. To achieve
a better balance between accuracy and efficiency, we propose FocusMamba, which
performs adaptive collaborative sparsification of multimodal features and
efficiently integrates complementary information. Specifically, an Event-Guided
Multimodal Sparsification (EGMS) strategy is designed to identify and
adaptively discard low-information regions within each modality by leveraging
scene content changes perceived by the event camera. Based on the
sparsification results, a Cross-Modality Focus Fusion (CMFF) module is proposed
to effectively capture and integrate complementary features from both
modalities. Experiments on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate
that the proposed method achieves superior performance in both accuracy and
efficiency compared to existing methods. The code will be available at
https://github.com/Zizzzzzzz/FocusMamba.

</details>


### [51] [SalientFusion: Context-Aware Compositional Zero-Shot Food Recognition](https://arxiv.org/abs/2509.03873)
*Jiajun Song,Xiaoou Liu*

Main category: cs.CV

> 为了解决食品识别领域的零样本学习问题，我们提出了组合零样本食品识别（CZSFR）任务，并开发了SalientFusion方法来应对相关的挑战，实验证明该方法在多个基准测试中达到了最佳结果。

<details>
  <summary>Details</summary>

**Motivation:** 食品识别技术得到了广泛关注，但新的菜品不断涌现导致需要识别未曾见过的食品类别。这催生了零样本食品学习（ZSFL）的动机。我们提出了组合零样本食品识别（CZSFR）的任务。然而，CZSFR面临着三个挑战：（1）背景信息冗余分散模型对有意义食品特征的学习；（2）主食和辅食角色混淆导致误分类；（3）单一属性的语义偏差可能导致理解上的混淆。

**Method:** 我们提出了SalientFusion方法，这是一种上下文感知的组合零样本食品识别方法，包含两个部分：SalientFormer和DebiasAT。SalientFormer用于去除背景冗余信息，并利用深度特征解决主食和辅食的角色混淆问题；DebiasAT通过将提示与视觉特征对齐减少语义偏差。

**Result:** 在我们提出的CZSFood-90和CZSFood-164基准测试和流行的通用数据集上，SalientFusion达到了最先进的结果。

**Conclusion:** 通过提出SalientFusion方法，解决了组合零样本食品识别中的主要挑战，并在多个基准上达到了最好的结果。这项工作证明了这种方法的前景。

**Abstract:** Food recognition has gained significant attention, but the rapid emergence of
new dishes requires methods for recognizing unseen food categories, motivating
Zero-Shot Food Learning (ZSFL). We propose the task of Compositional Zero-Shot
Food Recognition (CZSFR), where cuisines and ingredients naturally align with
attributes and objects in Compositional Zero-Shot learning (CZSL). However,
CZSFR faces three challenges: (1) Redundant background information distracts
models from learning meaningful food features, (2) Role confusion between
staple and side dishes leads to misclassification, and (3) Semantic bias in a
single attribute can lead to confusion of understanding. Therefore, we propose
SalientFusion, a context-aware CZSFR method with two components: SalientFormer,
which removes background redundancy and uses depth features to resolve role
confusion; DebiasAT, which reduces the semantic bias by aligning prompts with
visual features. Using our proposed benchmarks, CZSFood-90 and CZSFood-164, we
show that SalientFusion achieves state-of-the-art results on these benchmarks
and the most popular general datasets for the general CZSL. The code is
avaliable at https://github.com/Jiajun-RUC/SalientFusion.

</details>


### [52] [Human Motion Video Generation: A Survey](https://arxiv.org/abs/2509.03883)
*Haiwei Xue,Xiangyang Luo,Zhanghao Hu,Xin Zhang,Xunzhi Xiang,Yuqin Dai,Jianzhuang Liu,Zhensong Zhang,Minglei Li,Jian Yang,Fei Ma,Zhiyong Wu,Changpeng Yang,Zonghong Dai,Fei Richard Yu*

Main category: cs.CV

> 本文主要探讨了人类运动视频生成的全面过程，并强调了大型语言模型的应用，是对该领域整体生成流程的首次详尽综述。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于现有调研中缺少对整个生成过程的全面概述，本文旨在填补这一空白，详细介绍人类运动视频生成的各个环节。

**Method:** 提供了一个深入的人类运动视频生成综述，涵盖了超过十个子任务，详细阐述了生成过程中的五个关键阶段：输入、运动规划、运动视频生成、细化和输出。此外，本文首次探讨了大型语言模型在提升人类运动视频生成中的潜力。

**Result:** 通过覆盖超过两百篇论文，本文提供了一个该领域的深入回顾，并突出了一些重大技术突破。

**Conclusion:** 本文对人类运动视频生成技术的发展趋势进行了全面回顾，揭示了其潜在应用前景，为数字人类的综合应用场景提供了一个宝贵的资源。

**Abstract:** Human motion video generation has garnered significant research interest due
to its broad applications, enabling innovations such as photorealistic singing
heads or dynamic avatars that seamlessly dance to music. However, existing
surveys in this field focus on individual methods, lacking a comprehensive
overview of the entire generative process. This paper addresses this gap by
providing an in-depth survey of human motion video generation, encompassing
over ten sub-tasks, and detailing the five key phases of the generation
process: input, motion planning, motion video generation, refinement, and
output. Notably, this is the first survey that discusses the potential of large
language models in enhancing human motion video generation. Our survey reviews
the latest developments and technological trends in human motion video
generation across three primary modalities: vision, text, and audio. By
covering over two hundred papers, we offer a thorough overview of the field and
highlight milestone works that have driven significant technological
breakthroughs. Our goal for this survey is to unveil the prospects of human
motion video generation and serve as a valuable resource for advancing the
comprehensive applications of digital humans. A complete list of the models
examined in this survey is available in Our Repository
https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.

</details>


### [53] [OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction](https://arxiv.org/abs/2509.03887)
*Bu Jin,Songen Gu,Xiaotao Hu,Yupeng Zheng,Xiaoyang Guo,Qian Zhang,Xiaoxiao Long,Wei Yin*

Main category: cs.CV

> OccTENS is a novel model for generating high-fidelity, controllable long-term 3D occupancy scenes from historical observations, improving upon the limitations of autoregressive methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to overcome the inefficiency, temporal degradation, and lack of controllability in current AR-based methods for generating fine-grained 3D occupancy scenes over long-term observation.

**Method:** OccTENS is a generative occupancy world model that addresses the challenges of autoregressive (AR) methods by reformulating the occupancy world model as a Temporal Next-Scale Prediction (TENS) task. It uses a TensFormer to manage temporal causality and spatial relationships while also integrating a holistic pose aggregation strategy for better pose controllability.

**Result:** Experimental results demonstrate that OccTENS outperforms existing methods in terms of occupancy quality and inference speed.

**Conclusion:** The conclusion is that OccTENS effectively addresses the challenges of long-term, high-fidelity 3D occupancy scene generation, offering improvements in both performance and computational efficiency compared to current approaches.

**Abstract:** In this paper, we propose OccTENS, a generative occupancy world model that
enables controllable, high-fidelity long-term occupancy generation while
maintaining computational efficiency. Different from visual generation, the
occupancy world model must capture the fine-grained 3D geometry and dynamic
evolution of the 3D scenes, posing great challenges for the generative models.
Recent approaches based on autoregression (AR) have demonstrated the potential
to predict vehicle movement and future occupancy scenes simultaneously from
historical observations, but they typically suffer from \textbf{inefficiency},
\textbf{temporal degradation} in long-term generation and \textbf{lack of
controllability}. To holistically address these issues, we reformulate the
occupancy world model as a temporal next-scale prediction (TENS) task, which
decomposes the temporal sequence modeling problem into the modeling of spatial
scale-by-scale generation and temporal scene-by-scene prediction. With a
\textbf{TensFormer}, OccTENS can effectively manage the temporal causality and
spatial relationships of occupancy sequences in a flexible and scalable way. To
enhance the pose controllability, we further propose a holistic pose
aggregation strategy, which features a unified sequence modeling for occupancy
and ego-motion. Experiments show that OccTENS outperforms the state-of-the-art
method with both higher occupancy quality and faster inference time.

</details>


### [54] [Weakly-Supervised Learning of Dense Functional Correspondences](https://arxiv.org/abs/2509.03893)
*Stefan Stojanov,Linan Zhao,Yunzhi Zhang,Daniel L. K. Yamins,Jiajun Wu*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Establishing dense correspondences across image pairs is essential for tasks
such as shape reconstruction and robot manipulation. In the challenging setting
of matching across different categories, the function of an object, i.e., the
effect that an object can cause on other objects, can guide how correspondences
should be established. This is because object parts that enable specific
functions often share similarities in shape and appearance. We derive the
definition of dense functional correspondence based on this observation and
propose a weakly-supervised learning paradigm to tackle the prediction task.
The main insight behind our approach is that we can leverage vision-language
models to pseudo-label multi-view images to obtain functional parts. We then
integrate this with dense contrastive learning from pixel correspondences to
distill both functional and spatial knowledge into a new model that can
establish dense functional correspondence. Further, we curate synthetic and
real evaluation datasets as task benchmarks. Our results demonstrate the
advantages of our approach over baseline solutions consisting of off-the-shelf
self-supervised image representations and grounded vision language models.

</details>


### [55] [Attn-Adapter: Attention Is All You Need for Online Few-shot Learner of Vision-Language Model](https://arxiv.org/abs/2509.03895)
*Phuoc-Nguyen Bui,Khanh-Binh Nguyen,Hyunseung Choo*

Main category: cs.CV

> Attn-Adapter在不重新训练基础模型的情况下实现了从少量样本中进行动态适应，显著提升了跨类别和跨数据集的推广能力。

<details>
  <summary>Details</summary>

**Motivation:** 对比性视觉语言模型在零样本图像识别中表现出色，但在少样本场景下面临挑战，因为基于提示学习的离线微调计算成本高且有过度拟合的风险。

**Method:** 提出了一种新的在线少样本学习框架Attn-Adapter，通过双注意力机制增强了CLIP的适应性。该设计包含两个组件：Memory Attn-Adapter和Local-Global Attn-Adapter，分别用于优化类别嵌入和丰富图像嵌入，实现从少量标注样本中的动态适应，无需重新训练基础模型。

**Result:** Attn-Adapter在跨类别和跨数据集推广方面超越了最先进方法，同时保持了高效的推理能力，并且可以跨越CLIP背骨进行扩展。

**Conclusion:** 通过提出Attn-Adapter框架，研究展示了在少样本学习场景下有效提升了CLIP的表现力和推广性，证明了其在零样本学习和少样本学习中的潜力。

**Abstract:** Contrastive vision-language models excel in zero-shot image recognition but
face challenges in few-shot scenarios due to computationally intensive offline
fine-tuning using prompt learning, which risks overfitting. To overcome these
limitations, we propose Attn-Adapter, a novel online few-shot learning
framework that enhances CLIP's adaptability via a dual attention mechanism. Our
design incorporates dataset-specific information through two components: the
Memory Attn-Adapter, which refines category embeddings using support examples,
and the Local-Global Attn-Adapter, which enriches image embeddings by
integrating local and global features. This architecture enables dynamic
adaptation from a few labeled samples without retraining the base model.
Attn-Adapter outperforms state-of-the-art methods in cross-category and
cross-dataset generalization, maintaining efficient inference and scaling
across CLIP backbones.

</details>


### [56] [SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation](https://arxiv.org/abs/2509.03897)
*Xiaofu Chen,Israfel Salazar,Yova Kementchedjhieva*

Main category: cs.CV

> 本文提出了一种新的度量方法SPECS，用于长图像描述，这种方法效率高且与人类判断的相关性好。

<details>
  <summary>Details</summary>

**Motivation:** 随着生成长而详细的图像描述的兴趣增加，标准的评估指标变得越来越不可靠。N-gram基线指标效率高，但无法捕捉语义正确性；表示相似性度量虽然旨在解决这个问题，但由于计算成本高和与人类判断的相关性低而未受欢迎；基于大语言模型的度量虽然与人类判断相关性很强，但在模型开发过程中迭代使用成本太高。

**Method:** 引入了SPECS（Specificity-Enhanced CLIPScore），这是一种无参考的表示相似性度量方法，专门用于长图像描述。SPECS通过对CLIP进行修改，采用新的目标，强调了详细信息的准确性，奖励正确的细节，惩罚错误的细节。

**Result:** 研究结果表明，SPECS在与人类判断的相关性上达到了开源的基于大语言模型度量的表现，同时效率更高。

**Conclusion:** SPECS与开源的基于大语言模型的度量在与人类判断的相关性上表现相当，但效率更高，这使得它成为图像描述模型开发过程中迭代检查评估的实用替代方案。

**Abstract:** As interest grows in generating long, detailed image captions, standard
evaluation metrics become increasingly unreliable. N-gram-based metrics though
efficient, fail to capture semantic correctness. Representational Similarity
(RS) metrics, designed to address this, initially saw limited use due to high
computational costs, while today, despite advances in hardware, they remain
unpopular due to low correlation to human judgments. Meanwhile, metrics based
on large language models (LLMs) show strong correlation with human judgments,
but remain too expensive for iterative use during model development.
  We introduce SPECS (Specificity-Enhanced CLIPScore), a reference-free RS
metric tailored to long image captioning. SPECS modifies CLIP with a new
objective that emphasizes specificity: rewarding correct details and penalizing
incorrect ones. We show that SPECS matches the performance of open-source
LLM-based metrics in correlation to human judgments, while being far more
efficient. This makes it a practical alternative for iterative checkpoint
evaluation during image captioning model development.Our code can be found at
https://github.com/mbzuai-nlp/SPECS.

</details>


### [57] [A Generative Foundation Model for Chest Radiography](https://arxiv.org/abs/2509.03903)
*Yuanfeng Ji,Dan Lin,Xiyue Wang,Lu Zhang,Wenhui Zhou,Chongjian Ge,Ruihang Chu,Xiaoli Yang,Junhan Zhao,Junsong Chen,Xiangde Luo,Sen Yang,Jin Fang,Ping Luo,Ruijiang Li*

Main category: cs.CV

> ChexGen 是一种生成医学图像的基础模型，能够在少样本的情况下提升疾病分类等任务的准确性，并有助于消除不同人群之间的偏见，它是更加公平和高效的医疗AI系统的一部分。

<details>
  <summary>Details</summary>

**Motivation:** 医学图像资源匮乏是一个重大阻碍，高质量的医学图像数据难以获得，限制了医疗领域可靠AI模型的发展。尽管在自然图像生成基础模型方面已经取得了重大进展，但医学图像在多样性和标注质量方面仍然受限。因此，研究人员开发了一个新的框架来解决这一问题。

**Method:** 本研究开发了一种名为 ChexGen 的生成式视觉语言基础模型，该模型提供了一个统一的框架，可用于文本指导、掩码指导及边界框指导的胸部X光片合成。ChexGen 基于潜扩散转换器架构，并在迄今为止最大的经过精心整理的胸部 X 光片数据集上进行了预训练，该数据集包含了 960,000 张放射图像报告对。

**Result:** 实验表明，ChexGen 通过专家评估和定量指标可以实现放射图像的精确合成。此外，它还可以用于训练数据增强和监督预训练，这使得在使用少量训练数据时也能在疾病分类、检测和分割任务上取得性能提升。

**Conclusion:** 该研究表明，生成式基础模型在构建更准确、数据效率更高且更加公平的医疗 AI 系统中扮演着变革性的角色。

**Abstract:** The scarcity of well-annotated diverse medical images is a major hurdle for
developing reliable AI models in healthcare. Substantial technical advances
have been made in generative foundation models for natural images. Here we
develop `ChexGen', a generative vision-language foundation model that
introduces a unified framework for text-, mask-, and bounding box-guided
synthesis of chest radiographs. Built upon the latent diffusion transformer
architecture, ChexGen was pretrained on the largest curated chest X-ray dataset
to date, consisting of 960,000 radiograph-report pairs. ChexGen achieves
accurate synthesis of radiographs through expert evaluations and quantitative
metrics. We demonstrate the utility of ChexGen for training data augmentation
and supervised pretraining, which led to performance improvements across
disease classification, detection, and segmentation tasks using a small
fraction of training data. Further, our model enables the creation of diverse
patient cohorts that enhance model fairness by detecting and mitigating
demographic biases. Our study supports the transformative role of generative
foundation models in building more accurate, data-efficient, and equitable
medical AI systems.

</details>


### [58] [LMVC: An End-to-End Learned Multiview Video Coding Framework](https://arxiv.org/abs/2509.03922)
*Xihua Sheng,Yingwen Zhang,Long Xu,Shiqi Wang*

Main category: cs.CV

> 本文提出了一种可以实现随机访问并保持向后兼容性的端到端学习的多视角视频编码框架。该框架通过利用独立视角的运动和内容信息提高了多视角视频的压缩效率，并在实验中大幅超越了传统MV-HEVC标准的参考软件。

<details>
  <summary>Details</summary>

**Motivation:** 多视角视频是体素视频的关键数据源，能够实现沉浸式的3D场景重建，但存储和传输方面由于其巨大的数据量而面临重大挑战。目前，深度学习提升了端到端视频编码技术的发展，但大多集中在单视角或立体视频上，对多视角场景的研究还不够。

**Method:** 本文提出了一个端到端学习的多视角视频编码（LMVC）框架，该框架利用独立视角的运动信息和内容信息来提高依赖视角的压缩效率。具体而言，为了利用跨视角运动关联，提出了基于特征的跨视角运动矢量预测方法以及跨视角运动熵模型。为了利用跨视角内容关联，提出了无视差的跨视角上下文预测模块以及跨视角上下文熵模型。

**Result:** 实验结果表明，所提出的LMVC框架在性能上大大优于传统的MV-HEVC标准参考软件，为未来多视角视频编码研究奠定了坚实的基础。

**Conclusion:** 本文提出了一种新的多视角视频编码方法，它可以同时增强压缩效率，实现随机访问和保证向后兼容性。它的跨视角运动和内容预测模块在实验中展示了强大的性能，为多视角视频的高效编码提供了新的方向。

**Abstract:** Multiview video is a key data source for volumetric video, enabling immersive
3D scene reconstruction but posing significant challenges in storage and
transmission due to its massive data volume. Recently, deep learning-based
end-to-end video coding has achieved great success, yet most focus on
single-view or stereo videos, leaving general multiview scenarios
underexplored. This paper proposes an end-to-end learned multiview video coding
(LMVC) framework that ensures random access and backward compatibility while
enhancing compression efficiency. Our key innovation lies in effectively
leveraging independent-view motion and content information to enhance
dependent-view compression. Specifically, to exploit the inter-view motion
correlation, we propose a feature-based inter-view motion vector prediction
method that conditions dependent-view motion encoding on decoded
independent-view motion features, along with an inter-view motion entropy model
that learns inter-view motion priors. To exploit the inter-view content
correlation, we propose a disparity-free inter-view context prediction module
that predicts inter-view contexts from decoded independent-view content
features, combined with an inter-view contextual entropy model that captures
inter-view context priors. Experimental results show that our proposed LMVC
framework outperforms the reference software of the traditional MV-HEVC
standard by a large margin, establishing a strong baseline for future research
in this field.

</details>


### [59] [TopoSculpt: Betti-Steered Topological Sculpting of 3D Fine-grained Tubular Shapes](https://arxiv.org/abs/2509.03938)
*Minghui Zhang,Yaoyu Liu,Junyang Wu,Xin You,Hanxiao Zhang,Junjun He,Yun Gu*

Main category: cs.CV

> 提出一种名为TopoSculpt的框架，用于3D细粒度管状结构的拓扑细化，实现了显著的几何和拓扑改进。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法往往依赖于体素级的重叠度量，这无法捕捉到拓扑的正确性和完整性。尽管拓扑感知损失和持久同调约束显示出潜力，但它们通常仅应用于局部区域，且无法保证全局保存或纠正推理中的几何错误。为了克服这些限制，提出了该框架。

**Method:** 提出了一种名为TopoSculpt的框架，用于3D细粒度管状结构的拓扑细化。该框架(i)采用整体区域建模策略以捕获完整的空间上下文；(ii)首次引入了拓扑完整性贝蒂数（TIB）约束，同时强制执行贝蒂数先验和全局完整性；(iii)使用具有持久同调的课程化细化方案，逐步从粗尺度到细尺度纠正错误。

**Result:** 在具有挑战性的肺气道和Willis环数据集上的广泛实验表明，该方法在几何和拓扑方面有显著改进。例如，在肺气道数据集上$eta_{0}$错误从69.00减少到3.40，在Willis环数据集上从1.65减少到0.30，树长度检测和分支检测率提高了近10%。

**Conclusion:** 这些结果表明TopoSculpt在纠正关键拓扑错误和推进复杂3D管状解剖的高保真建模方面是有效的。

**Abstract:** Medical tubular anatomical structures are inherently three-dimensional
conduits with lumens, enclosing walls, and complex branching topologies.
Accurate reconstruction of their geometry and topology is crucial for
applications such as bronchoscopic navigation and cerebral arterial
connectivity assessment. Existing methods often rely on voxel-wise overlap
measures, which fail to capture topological correctness and completeness.
Although topology-aware losses and persistent homology constraints have shown
promise, they are usually applied patch-wise and cannot guarantee global
preservation or correct geometric errors at inference. To address these
limitations, we propose a novel TopoSculpt, a framework for topological
refinement of 3D fine-grained tubular structures. TopoSculpt (i) adopts a
holistic whole-region modeling strategy to capture full spatial context, (ii)
first introduces a Topological Integrity Betti (TIB) constraint that jointly
enforces Betti number priors and global integrity, and (iii) employs a
curriculum refinement scheme with persistent homology to progressively correct
errors from coarse to fine scales. Extensive experiments on challenging
pulmonary airway and Circle of Willis datasets demonstrate substantial
improvements in both geometry and topology. For instance, $\beta_{0}$ errors
are reduced from 69.00 to 3.40 on the airway dataset and from 1.65 to 0.30 on
the CoW dataset, with Tree length detected and branch detected rates improving
by nearly 10\%. These results highlight the effectiveness of TopoSculpt in
correcting critical topological errors and advancing the high-fidelity modeling
of complex 3D tubular anatomy. The project homepage is available at:
https://github.com/Puzzled-Hui/TopoSculpt.

</details>


### [60] [Chest X-ray Pneumothorax Segmentation Using EfficientNet-B4 Transfer Learning in a U-Net Architecture](https://arxiv.org/abs/2509.03950)
*Alvaro Aranibar Roque,Helga Sebastian*

Main category: cs.CV

> 提出了一种自动深度学习管道来诊断和分割气胸区域，提高了小气胸检测的准确性，能有效支持医生的诊断。

<details>
  <summary>Details</summary>

**Motivation:** 气胸，即胸膜腔内异常积气，如未被发现可危及生命。胸部X光片是首选的诊断工具，但小的气胸可能难以察觉。

**Method:** 提出使用带有EfficientNet-B4编码器的U-Net自动深度学习管道来分割气胸区域。该模型在SIIM-ACR数据集上通过数据增强和结合二元交叉熵及Dice损失进行训练。

**Result:** 在独立的PTX-498数据集上，该模型达到了IoU 0.7008和Dice分数0.8241的准确度。这表明该模型能够精确识别气胸区域。

**Conclusion:** 这些结果表明该模型可以准确地定位气胸区域，并为放射科医生提供支持。

**Abstract:** Pneumothorax, the abnormal accumulation of air in the pleural space, can be
life-threatening if undetected. Chest X-rays are the first-line diagnostic
tool, but small cases may be subtle. We propose an automated deep-learning
pipeline using a U-Net with an EfficientNet-B4 encoder to segment pneumothorax
regions. Trained on the SIIM-ACR dataset with data augmentation and a combined
binary cross-entropy plus Dice loss, the model achieved an IoU of 0.7008 and
Dice score of 0.8241 on the independent PTX-498 dataset. These results
demonstrate that the model can accurately localize pneumothoraces and support
radiologists.

</details>


### [61] [ANTS: Shaping the Adaptive Negative Textual Space by MLLM for OOD Detection](https://arxiv.org/abs/2509.03951)
*Zhu Wenjie,Zhang Yabin,Xin Jin,Wenjun Zeng,Lei Zhang*

Main category: cs.CV

> 通过引入自适应负文本空间(ANTS)，利用多模态大型语言模型(MLLM)理解和生成描述负样本的句子，改进远分布外(OOD)和近分布外检测，提高检测精度，减少误报，适应不同OOD任务设置，无需训练且可扩展性强。

<details>
  <summary>Details</summary>

**Motivation:** 现有的OOD检测方法在理解OOD图像和构造准确的负样本空间方面存在不足，并且容易受到假负样本的影响，导致检测性能不佳。

**Method:** 提出自适应负文本空间(ANTS)，利用多模态大型语言模型(MLLM)，识别和描述可能的OOD样本，生成精确的负样本句子。对于近OOD情形，进一步识别与负样本视觉相关的ID子集并生成相应的负标签。

**Result:** 在ImageNet基准上显著降低FPR95达4.2%，并在不同OOD任务设置中表现出高效适应性。

**Conclusion:** 该方法无需训练且零样本，展示了高适应性和可扩展性，在减少误报和提高远分布外与近分布外检测方面表现出色。

**Abstract:** The introduction of negative labels (NLs) has proven effective in enhancing
Out-of-Distribution (OOD) detection. However, existing methods often lack an
understanding of OOD images, making it difficult to construct an accurate
negative space. In addition, the presence of false negative labels
significantly degrades their near-OOD performance. To address these issues, we
propose shaping an Adaptive Negative Textual Space (ANTS) by leveraging the
understanding and reasoning capabilities of multimodal large language models
(MLLMs). Specifically, we identify images likely to be OOD samples as negative
images and prompt the MLLM to describe these images, generating expressive
negative sentences that precisely characterize the OOD distribution and enhance
far-OOD detection. For the near-OOD setting, where OOD samples resemble the
in-distribution (ID) subset, we first identify the subset of ID classes that
are visually similar to negative images and then leverage the reasoning
capability of MLLMs to generate visually similar negative labels tailored to
this subset, effectively reducing false negatives and improving near-OOD
detection. To balance these two types of negative textual spaces, we design an
adaptive weighted score that enables the method to handle different OOD task
settings (near-OOD and far-OOD) without relying on task-specific prior
knowledge, making it highly adaptable in open environments. On the ImageNet
benchmark, our ANTS significantly reduces the FPR95 by 4.2\%, establishing a
new state-of-the-art. Furthermore, our method is training-free and zero-shot,
enabling high scalability.

</details>


### [62] [Multimodal Feature Fusion Network with Text Difference Enhancement for Remote Sensing Change Detection](https://arxiv.org/abs/2509.03961)
*Yijun Zhou,Yikui Zhai,Zilu Ying,Tingfeng Xian,Wenlve Zhou,Zhiheng Zhou,Xiaolin Tian,Xudong Jia,Hongsheng Zhang,C. L. Philip Chen*

Main category: cs.CV

> 提出MMChange方法，整合图像和文本模态，通过多个模块提高遥感变化检测的精度和鲁棒性，实验表明该方法优于现有技术。

<details>
  <summary>Details</summary>

**Motivation:** 当前深度学习在遥感变化检测中依赖单一图像模态，这限制了特征表示、变化模式建模和对抗光照及噪声干扰的能力。为了提高检测的准确性与鲁棒性，该研究提出了一种多模态方法。

**Method:** 提出了图像特征精炼（IFR）模块来突出关键区域，减少环境噪声；使用视觉语言模型（VLM）生成双时相图像的语义描述；文本差异增强（TDE）模块以捕捉细微语义变化；最后设计图像文本特征融合（ITFF）模块以进行深度跨模态整合。

**Result:** 该多模态遥感变化检测方法（MMChange）通过结合图像和文本模态提升检测精度和鲁棒性，提出多个模块以适应不同方面的需求。实验显示其在多个指标上超越现有方法，证明了该方法在多模态遥感变化检测中的有效性。

**Conclusion:** 该研究通过MMChange方法证实了多模态方法在遥感变化检测中的有效性，特别是在对抗光照和噪声干扰方面具有优势。

**Abstract:** Although deep learning has advanced remote sensing change detection (RSCD),
most methods rely solely on image modality, limiting feature representation,
change pattern modeling, and generalization especially under illumination and
noise disturbances. To address this, we propose MMChange, a multimodal RSCD
method that combines image and text modalities to enhance accuracy and
robustness. An Image Feature Refinement (IFR) module is introduced to highlight
key regions and suppress environmental noise. To overcome the semantic
limitations of image features, we employ a vision language model (VLM) to
generate semantic descriptions of bitemporal images. A Textual Difference
Enhancement (TDE) module then captures fine grained semantic shifts, guiding
the model toward meaningful changes. To bridge the heterogeneity between
modalities, we design an Image Text Feature Fusion (ITFF) module that enables
deep cross modal integration. Extensive experiments on LEVIRCD, WHUCD, and
SYSUCD demonstrate that MMChange consistently surpasses state of the art
methods across multiple metrics, validating its effectiveness for multimodal
RSCD. Code is available at: https://github.com/yikuizhai/MMChange.

</details>


### [63] [SAC-MIL: Spatial-Aware Correlated Multiple Instance Learning for Histopathology Whole Slide Image Classification](https://arxiv.org/abs/2509.03973)
*Yu Bai,Zitong Yu,Haowen Tian,Xijing Wang,Shuo Yan,Lin Wang,Honglin Li,Xitong Ling,Bo Zhang,Zheng Zhang,Wufan Wang,Hui Gao,Xiangyang Gong,Wendong Wang*

Main category: cs.CV

> 提出SAC-MIL，通过位置编码模块和SAC块处理WSI的全实例关联问题，达到了在多个数据集上的领先性能，且易于部署。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决传统多实例学习方法无法有效利用空...

**Method:** 提出空间感知相关多实例学习（SAC-MIL）进行全幻影成像（WSI）分类。SAC-MIL 包含一个位置编码模块用于编码位置信息以及一个SAC块来执行全实例相关。位置编码模块利用幻灯片内的实例坐标来编码空间关系，而不是输入WSI序列中的实例索引。该模块还可以处理训练序列和测试序列长度不同的外推问题。SAC块是一个基于多层感知器（MLP）的方法，它与序列长度成线性时间复杂度地执行全实例相关。由于MLP结构简单，易于部署，因为它不需要自定义CUDA核，而与基于Transformer的方法相比有所不同。

**Result:** 在CAMELYON-16、TCGA-LUNG和TCGA-BRAC数据集上，SAC-MIL达到了最先进的表现性能。代码将在接受后公开。

**Conclusion:** SAC-MIL方法在保持高性能的同时，简化了部署，适用于WSI分类任务。

**Abstract:** We propose Spatial-Aware Correlated Multiple Instance Learning (SAC-MIL) for
performing WSI classification. SAC-MIL consists of a positional encoding module
to encode position information and a SAC block to perform full instance
correlations. The positional encoding module utilizes the instance coordinates
within the slide to encode the spatial relationships instead of the instance
index in the input WSI sequence. The positional encoding module can also handle
the length extrapolation issue where the training and testing sequences have
different lengths. The SAC block is an MLP-based method that performs full
instance correlation in linear time complexity with respect to the sequence
length. Due to the simple structure of MLP, it is easy to deploy since it does
not require custom CUDA kernels, compared to Transformer-based methods for WSI
classification. SAC-MIL has achieved state-of-the-art performance on the
CAMELYON-16, TCGA-LUNG, and TCGA-BRAC datasets. The code will be released upon
acceptance.

</details>


### [64] [Improving Vessel Segmentation with Multi-Task Learning and Auxiliary Data Available Only During Model Training](https://arxiv.org/abs/2509.03975)
*Daniel Sobotka,Alexander Herold,Matthias Perkonigg,Lucian Beer,Nina Bastati,Alina Sablatnig,Ahmed Ba-Ssalamah,Georg Langs*

Main category: cs.CV

> 研究了一种多任务学习框架，用于无需对比度增强的肝脏MRI中的血管分割。通过训练时使用辅助对比增强数据来提升分割精度，即使这些数据在实际应用中不可用。

<details>
  <summary>Details</summary>

**Motivation:** 分割肝脏MRI中的血管对于计算分析血管重构，尤其是与弥漫性肝脏疾病相关的分析非常重要。现有的方法依赖于对比增强影像数据，但专用的成像序列并非普遍使用。未经对比剂增强的图像更频繁地被获取，但在这些图像中进行血管分割更具挑战性，要求大量标注数据。

**Method:** 提出了一种多任务学习框架来分割无对比度增强的肝脏MRI中的血管。该框架在训练期间利用辅助的对比增强MRI数据来减少对标注训练样本的需求。提出的模型利用了带有和不带有血管标注的本征和对比增强成对数据进行训练。

**Result:** 结果显示，辅助数据改善了血管分割的准确性，即使在推理过程中这些数据不可用。这种优势在只有少量标注训练样本时尤为显著，因为共同任务结构使特征表示受益。通过验证该方法可以提升脑肿瘤分割的模型，进一步证实其跨领域益处。

**Conclusion:** 一种辅助的、有信息量的成像模式，即使只在训练期间可用，也可以增强专家标注的效果。

**Abstract:** Liver vessel segmentation in magnetic resonance imaging data is important for
the computational analysis of vascular remodelling, associated with a wide
spectrum of diffuse liver diseases. Existing approaches rely on contrast
enhanced imaging data, but the necessary dedicated imaging sequences are not
uniformly acquired. Images without contrast enhancement are acquired more
frequently, but vessel segmentation is challenging, and requires large-scale
annotated data. We propose a multi-task learning framework to segment vessels
in liver MRI without contrast. It exploits auxiliary contrast enhanced MRI data
available only during training to reduce the need for annotated training
examples. Our approach draws on paired native and contrast enhanced data with
and without vessel annotations for model training. Results show that auxiliary
data improves the accuracy of vessel segmentation, even if they are not
available during inference. The advantage is most pronounced if only few
annotations are available for training, since the feature representation
benefits from the shared task structure. A validation of this approach to
augment a model for brain tumor segmentation confirms its benefits across
different domains. An auxiliary informative imaging modality can augment expert
annotations even if it is only available during training.

</details>


### [65] [Promptception: How Sensitive Are Large Multimodal Models to Prompts?](https://arxiv.org/abs/2509.03986)
*Mohamed Insaf Ismithdeen,Muhammad Uzair Khattak,Salman Khan*

Main category: cs.CV

> 提出了一个针对大型多模态模型在多选题上的提示设计敏感度的系统性评估框架Promptception。不同模型对提示设计的敏感度不同，进而影响其性能表现。

<details>
  <summary>Details</summary>

**Motivation:** 由于在多选题解答时，即使是提示语的细微变化也会使大型多模态模型的准确性产生高达15%的变化，这使得对这些模型的透明和公平评估显得尤为挑战。为了应对这一问题，提出了系统性框架Promptception。

**Method:** 引入了Promptception，这是一个系统性框架，用于评估大型多模态模型对提示设计的敏感度。该框架包含61种提示类型，涵盖15个类别和6个超类别，每个类别针对提示设计的特定方面，用于在3个MCQA基准测试上评估10种大型多模态模型，包括轻量级开源模型到GPT-4o和Gemini 1.5 Pro。

**Result:** 研究结果揭示，专有模型对提示措辞更为敏感，反映出它们在指令语义上的严密对齐，而开源模型虽然更加稳定，但在应对细腻和复杂的措辞时却表现不佳。基于这一分析，提出了针对专有和开源大型多模态模型的提示原则，推动实现更强大和公平的模型评估。

**Conclusion:** 通过提出的评估框架发现，专有模型对提示语的措辞更加敏感，而开源模型虽然更稳定，但在处理细腻和复杂的措辞时存在困难。这一发现促进了针对专有和开源大型多模态模型的提示原则的制定，帮助实现更加鲁棒和公平的模型评估。

**Abstract:** Despite the success of Large Multimodal Models (LMMs) in recent years, prompt
design for LMMs in Multiple-Choice Question Answering (MCQA) remains poorly
understood. We show that even minor variations in prompt phrasing and structure
can lead to accuracy deviations of up to 15% for certain prompts and models.
This variability poses a challenge for transparent and fair LMM evaluation, as
models often report their best-case performance using carefully selected
prompts. To address this, we introduce Promptception, a systematic framework
for evaluating prompt sensitivity in LMMs. It consists of 61 prompt types,
spanning 15 categories and 6 supercategories, each targeting specific aspects
of prompt formulation, and is used to evaluate 10 LMMs ranging from lightweight
open-source models to GPT-4o and Gemini 1.5 Pro, across 3 MCQA benchmarks:
MMStar, MMMU-Pro, MVBench. Our findings reveal that proprietary models exhibit
greater sensitivity to prompt phrasing, reflecting tighter alignment with
instruction semantics, while open-source models are steadier but struggle with
nuanced and complex phrasing. Based on this analysis, we propose Prompting
Principles tailored to proprietary and open-source LMMs, enabling more robust
and fair model evaluation.

</details>


### [66] [SliceSemOcc: Vertical Slice Based Multimodal 3D Semantic Occupancy Representation](https://arxiv.org/abs/2509.03999)
*Han Huang,Han Sun,Ningzhong Liu,Huiyu Zhou,Jiaquan Shen*

Main category: cs.CV

> SliceSemOcc addresses the limitation of current 3D semantic occupancy prediction approaches by capturing height-wise features and providing a dynamic channel attention mechanism, showing significant improvements in mean IoU on nuScenes-SurroundOcc and nuScenes-OpenOccupancy datasets.

<details>
  <summary>Details</summary>

**Motivation:** To improve 3D semantic occupancy prediction by capturing semantic variations along the vertical axis without overlooking height-axis information, which current methods tend to do.

**Method:** SliceSemOcc, a novel vertical slice based multimodal framework, extracts voxel features along the height axis using global and local slices. A global local fusion module adaptively reconciles fine-grained spatial details with holistic contextual information, and the SEAttention3D module assigns dynamic channel attention weights to each height layer.

**Result:** Experiments verify that the method significantly enhances mean IoU and performs particularly well on small-object categories.

**Conclusion:** The proposed SliceSemOcc framework is shown to be effective in improving 3D semantic occupancy prediction, especially for capturing semantic variations along the vertical axis.

**Abstract:** Driven by autonomous driving's demands for precise 3D perception, 3D semantic
occupancy prediction has become a pivotal research topic. Unlike
bird's-eye-view (BEV) methods, which restrict scene representation to a 2D
plane, occupancy prediction leverages a complete 3D voxel grid to model spatial
structures in all dimensions, thereby capturing semantic variations along the
vertical axis. However, most existing approaches overlook height-axis
information when processing voxel features. And conventional SENet-style
channel attention assigns uniform weight across all height layers, limiting
their ability to emphasize features at different heights. To address these
limitations, we propose SliceSemOcc, a novel vertical slice based multimodal
framework for 3D semantic occupancy representation. Specifically, we extract
voxel features along the height-axis using both global and local vertical
slices. Then, a global local fusion module adaptively reconciles fine-grained
spatial details with holistic contextual information. Furthermore, we propose
the SEAttention3D module, which preserves height-wise resolution through
average pooling and assigns dynamic channel attention weights to each height
layer. Extensive experiments on nuScenes-SurroundOcc and nuScenes-OpenOccupancy
datasets verify that our method significantly enhances mean IoU, achieving
especially pronounced gains on most small-object categories. Detailed ablation
studies further validate the effectiveness of the proposed SliceSemOcc
framework.

</details>


### [67] [Detecting Regional Spurious Correlations in Vision Transformers via Token Discarding](https://arxiv.org/abs/2509.04009)
*Solha Kang,Esla Timothy Anzaku,Wesley De Neve,Arnout Van Messem,Joris Vankerschaver,Francois Rameau,Utku Ozbulak*

Main category: cs.CV

> The paper proposes a method to detect spurious correlations in vision transformers, investigates the impact of training methodologies, and provides a list of ImageNet images with spurious signals for caution in future research. A real-world application in breast mass classification is demonstrated.

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the issue of spurious correlations in vision transformers, calling for the development of trustworthy, reliable, and generalizable machine learning models. It highlights the need for detection and mitigation strategies for these correlations to improve model performance and reliability.

**Method:** Content elaborates on the problem of spurious correlations in neural network-based computer vision models, focusing on vision transformers. It describes experiments conducted using the ImageNet dataset to detect these correlations and analyzes the impact of training methodologies on model reliance on such correlations. Additionally, it discusses class-specific spurious signals in ImageNet and provides a list of images with detected spurious signals. A case study on breast mass classification is also included.

**Result:** The results demonstrate the effectiveness of the proposed method in detecting spurious correlations within vision transformers. The experiments show that training methodologies significantly influence a model's reliance on spurious correlations. The paper also reveals that some classes in the ImageNet dataset contain spurious signals that can be readily exploited by models.

**Conclusion:** The paper concludes with a call to caution regarding the use of images with spurious signals in future research efforts and underscores the importance of robust methods to detect and mitigate such correlations for developing reliable neural network models.

**Abstract:** Due to their powerful feature association capabilities, neural network-based
computer vision models have the ability to detect and exploit unintended
patterns within the data, potentially leading to correct predictions based on
incorrect or unintended but statistically relevant signals. These clues may
vary from simple color aberrations to small texts within the image. In
situations where these unintended signals align with the predictive task,
models can mistakenly link these features with the task and rely on them for
making predictions. This phenomenon is referred to as spurious correlations,
where patterns appear to be associated with the task but are actually
coincidental. As a result, detection and mitigation of spurious correlations
have become crucial tasks for building trustworthy, reliable, and generalizable
machine learning models. In this work, we present a novel method to detect
spurious correlations in vision transformers, a type of neural network
architecture that gained significant popularity in recent years. Using both
supervised and self-supervised trained models, we present large-scale
experiments on the ImageNet dataset demonstrating the ability of the proposed
method to identify spurious correlations. We also find that, even if the same
architecture is used, the training methodology has a significant impact on the
model's reliance on spurious correlations. Furthermore, we show that certain
classes in the ImageNet dataset contain spurious signals that are easily
detected by the models and discuss the underlying reasons for those spurious
signals. In light of our findings, we provide an exhaustive list of the
aforementioned images and call for caution in their use in future research
efforts. Lastly, we present a case study investigating spurious signals in
invasive breast mass classification, grounding our work in real-world
scenarios.

</details>


### [68] [Learning from Majority Label: A Novel Problem in Multi-class Multiple-Instance Learning](https://arxiv.org/abs/2509.04023)
*Shiku Kaito,Shinnosuke Matsuo,Daiki Suehiro,Ryoma Bise*

Main category: cs.CV

> This paper presents a novel problem called Learning from Majority Label (LML) and proposes a solution based on a Majority Proportion Enhancement Module (MPEM) and a Counting Network to improve classification accuracy in multi-class Multiple-Instance Learning (MIL) scenarios.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to introduce the novel LML problem which assigns the majority class of instances in a bag as the bag-level label, addressing real-world scenarios such as medical imaging, voting prediction, customer sentiment analysis, and environmental monitoring.

**Method:** The paper proposes a method called Majority Proportion Enhancement Module (MPEM) and a Counting Network to solve the Learning from Majority Label (LML) problem. The MPEM increases the proportion of the majority class by removing instances of the minority class in the bags. The Counting Network is used to produce the bag-level majority labels, which are estimated by counting the number of instances within each class.

**Result:** The proposed method demonstrated superior performance compared to conventional MIL methods through extensive experiments on four datasets. Moreover, the ablation studies confirmed the effectiveness of each module.

**Conclusion:** The paper concludes that the proposed method is effective for LML and outperforms traditional methods. It showcases the utility and potential of the LML and the applied solution for various real-world applications.

**Abstract:** The paper proposes a novel multi-class Multiple-Instance Learning (MIL)
problem called Learning from Majority Label (LML). In LML, the majority class
of instances in a bag is assigned as the bag-level label. The goal of LML is to
train a classification model that estimates the class of each instance using
the majority label. This problem is valuable in a variety of applications,
including pathology image segmentation, political voting prediction, customer
sentiment analysis, and environmental monitoring. To solve LML, we propose a
Counting Network trained to produce bag-level majority labels, estimated by
counting the number of instances in each class. Furthermore, analysis
experiments on the characteristics of LML revealed that bags with a high
proportion of the majority class facilitate learning. Based on this result, we
developed a Majority Proportion Enhancement Module (MPEM) that increases the
proportion of the majority class by removing minority class instances within
the bags. Experiments demonstrate the superiority of the proposed method on
four datasets compared to conventional MIL methods. Moreover, ablation studies
confirmed the effectiveness of each module. The code is available at
\href{https://github.com/Shiku-Kaito/Learning-from-Majority-Label-A-Novel-Problem-in-Multi-class-Multiple-Instance-Learning}{here}.

</details>


### [69] [Millisecond-Response Tracking and Gazing System for UAVs: A Domestic Solution Based on "Phytium + Cambricon"](https://arxiv.org/abs/2509.04043)
*Yuchen Zhu,Longxiang Yin,Kai Zhao*

Main category: cs.CV

> 研究提出了一种基于Phytium处理器和Cambricon加速卡的异构计算架构，用于构建具有毫秒级响应能力的无人机跟踪与监视系统。系统在处理1920*1080分辨率视频流时，实现了50-100毫秒的单帧处理延迟，并具有超过98.5%的多尺度目标识别准确率。

<details>
  <summary>Details</summary>

**Motivation:** 传统相机系统在动态场景中具有超过200毫秒的响应延迟，无法满足复杂场景下的实时要求。该研究旨在解决这一问题，提出了基于Phytium处理器和Cambricon加速卡的异构计算架构，构建具备毫秒级响应能力的无人机跟踪和凝视系统。

**Method:** 采用基于Phytium处理器和Cambricon加速卡的异构计算架构，构建了一个具有毫秒级响应能力的无人机跟踪和凝视系统。在硬件层面，系统采用Phytium FT-2000/4处理器和MLU220加速卡的协同计算架构，通过多卡并行提高计算能力。在软件层面，创新地融合了轻量级的YOLOv5s检测网络与级联式DeepSORT跟踪算法，形成了"检测-跟踪-反馈"的闭环控制链路。

**Result:** 实验结果显示，系统在处理1920*1080分辨率视频流时，实现了50-100毫秒的单帧综合处理延迟，多尺度目标识别准确率超过98.5%。

**Conclusion:** 本研究提供了一种创新的无人机监控解决方案，并在国产芯片的应用上取得了重要进展。实验结果展示了系统在处理1920*1080分辨率视频流时，能够实现50-100毫秒的单帧综合处理延迟，并具备超过98.5%的多尺度目标识别准确率。

**Abstract:** In the frontier research and application of current video surveillance
technology, traditional camera systems exhibit significant limitations of
response delay exceeding 200 ms in dynamic scenarios due to the insufficient
deep feature extraction capability of automatic recognition algorithms and the
efficiency bottleneck of computing architectures, failing to meet the real-time
requirements in complex scenes. To address this issue, this study proposes a
heterogeneous computing architecture based on Phytium processors and Cambricon
accelerator cards, constructing a UAV tracking and gazing system with
millisecond-level response capability. At the hardware level, the system adopts
a collaborative computing architecture of Phytium FT-2000/4 processors and
MLU220 accelerator cards, enhancing computing power through multi-card
parallelism. At the software level, it innovatively integrates a lightweight
YOLOv5s detection network with a DeepSORT cascaded tracking algorithm, forming
a closed-loop control chain of "detection-tracking-feedback". Experimental
results demonstrate that the system achieves a stable single-frame
comprehensive processing delay of 50-100 ms in 1920*1080 resolution video
stream processing, with a multi-scale target recognition accuracy of over
98.5%, featuring both low latency and high precision. This study provides an
innovative solution for UAV monitoring and the application of domestic chips.

</details>


### [70] [A Re-ranking Method using K-nearest Weighted Fusion for Person Re-identification](https://arxiv.org/abs/2509.04050)
*Quang-Huy Che,Le-Chuong Nguyen,Gia-Nghia Tran,Dinh-Duy Phan,Vinh-Tiep Nguyen*

Main category: cs.CV

> 该研究开发了一种基于K-WF方法的重排序技术，能够生成多视角特征以减少视图偏差，显著提高行人再识别的Rank@1和mAP，且计算效率高于其他重排序方法。

<details>
  <summary>Details</summary>

**Motivation:** 传统的行人再识别方法中的重排序过程仅依赖单视角图像特征，导致视图偏差等问题。本研究旨在通过利用多视角特征缓解视图偏差，从而提高行人再识别的整体准确性。

**Method:** 研究采用了K-nearest Weighted Fusion (KWF)方法来生成多视角特征，通过选择K个在无监督情况下最接近的特征来生成表示同一身份的多视角特征，并探讨了特征聚合期间的权重选择策略。

**Result:** 该研究提出了一种高效的重排序方法，通过K-nearest Weighted Fusion (KWF)方法聚合邻居特征以生成多视角特征，从而减少视图偏差和改善行人再识别的准确性。实验结果表明，该方法在Market1501、MSMT17和Occluded-DukeMTMC数据集上，分别对MSMT17和Occluded-DukeMTMC数据集的Rank@1提高了9.8%和22.0%，并显示出显著的计算效率提升。

**Conclusion:** 新提出的重排序方法不仅能通过生成多视角特征提高行人再识别准确率，还提供了更好的计算效率，且无需模型微调和额外标注，适用于大规模数据集。

**Abstract:** In person re-identification, re-ranking is a crucial step to enhance the
overall accuracy by refining the initial ranking of retrieved results. Previous
studies have mainly focused on features from single-view images, which can
cause view bias and issues like pose variation, viewpoint changes, and
occlusions. Using multi-view features to present a person can help reduce view
bias. In this work, we present an efficient re-ranking method that generates
multi-view features by aggregating neighbors' features using K-nearest Weighted
Fusion (KWF) method. Specifically, we hypothesize that features extracted from
re-identification models are highly similar when representing the same
identity. Thus, we select K neighboring features in an unsupervised manner to
generate multi-view features. Additionally, this study explores the weight
selection strategies during feature aggregation, allowing us to identify an
effective strategy. Our re-ranking approach does not require model fine-tuning
or extra annotations, making it applicable to large-scale datasets. We evaluate
our method on the person re-identification datasets Market1501, MSMT17, and
Occluded-DukeMTMC. The results show that our method significantly improves
Rank@1 and mAP when re-ranking the top M candidates from the initial ranking
results. Specifically, compared to the initial results, our re-ranking method
achieves improvements of 9.8%/22.0% in Rank@1 on the challenging datasets:
MSMT17 and Occluded-DukeMTMC, respectively. Furthermore, our approach
demonstrates substantial enhancements in computational efficiency compared to
other re-ranking methods.

</details>
