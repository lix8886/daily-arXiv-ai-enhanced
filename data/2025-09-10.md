<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 37]
- [cs.CV](#cs.CV) [Total: 34]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MedBench-IT: A Comprehensive Benchmark for Evaluating Large Language Models on Italian Medical Entrance Examinations](https://arxiv.org/abs/2509.07135)
*Ruggero Marino Lazzaroni,Alessandro Angioi,Michelangelo Puliga,Davide Sanna,Roberto Marras*

Main category: cs.CL

> 本文提出了MedBench-IT，一个针对意大利医学大学入学考试的基准测试，分析了多种语言模型在该基准上的表现，并探讨了问题难度与模型性能的关系。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在教育领域的潜力日益增长，但对于特定领域中的非英语语言基准仍然是稀缺的。因此，我们引入了一个针对意大利语的医学大学入学考试的评估基准--MedBench-IT。

**Method:** 我们介绍了MedBench-IT，这是一个针对意大利语医学大学入学考试的全面评估基准。MedBench-IT包含了17,410个由专家编写的多选题，涵盖了六个学科和三个难度级别。在实验中，我们测试了各种模型，包括专有的大语言模型和资源高效的开源模型。

**Result:** 除了准确率外，我们还进行了严格的可重现性测试（88.86%的回答一致性，按学科不同）和排序偏差分析（影响较小），以及推理提示评估。我们还研究了问题的可读性和模型性能之间的相关性，发现存在一个统计学意义但较小的负相关关系。

**Conclusion:** MedBench-IT为意大利NLP社区、EdTech开发者和实践者提供了宝贵的资源，它揭示了当前的语言模型在该关键领域的能力并提供了一个标准化的评估方法。

**Abstract:** Large language models (LLMs) show increasing potential in education, yet
benchmarks for non-English languages in specialized domains remain scarce. We
introduce MedBench-IT, the first comprehensive benchmark for evaluating LLMs on
Italian medical university entrance examinations. Sourced from Edizioni Simone,
a leading preparatory materials publisher, MedBench-IT comprises 17,410
expert-written multiple-choice questions across six subjects (Biology,
Chemistry, Logic, General Culture, Mathematics, Physics) and three difficulty
levels. We evaluated diverse models including proprietary LLMs (GPT-4o, Claude
series) and resource-efficient open-source alternatives (<30B parameters)
focusing on practical deployability.
  Beyond accuracy, we conducted rigorous reproducibility tests (88.86% response
consistency, varying by subject), ordering bias analysis (minimal impact), and
reasoning prompt evaluation. We also examined correlations between question
readability and model performance, finding a statistically significant but
small inverse relationship. MedBench-IT provides a crucial resource for Italian
NLP community, EdTech developers, and practitioners, offering insights into
current capabilities and standardized evaluation methodology for this critical
domain.

</details>


### [2] [The ML-SUPERB 2.0 Challenge: Towards Inclusive ASR Benchmarking for All Language Varieties](https://arxiv.org/abs/2509.07139)
*William Chen,Chutong Meng,Jiatong Shi,Martijn Bartelds,Shih-Heng Wang,Hsiu-Hsuan Wang,Rafael Mosquera,Sara Hincapie,Dan Jurafsky,Antonis Anastasopoulos,Hung-yi Lee,Karen Livescu,Shinji Watanabe*

Main category: cs.CL

> Interspeech 2025 ML-SUPERB 2.0挑战赛构建了包含200多种语言的数据集来评估多语种语音模型，并引入了一个在线评估服务器。参赛模型在LID和CER指标上显著超越了基线。

<details>
  <summary>Details</summary>

**Motivation:** 由于近期多语种语音识别（ASR）的进步在不同语言和语言变体之间的分布不均，为了推动ASR模型的前沿发展，提出了Interspeech 2025 ML-SUPERB 2.0挑战赛。

**Method:** 构建了一个包含超过200种语言、口音和方言的数据集来评估先进的多语种语音模型。此外，还引入了一个基于DynaBench的在线评估服务器，为参赛者提供模型设计和架构的灵活性。

**Result:** 挑战赛收到了3个团队的5份提交，所有提交的模型都超越了基线模型。其中，最佳提交模型在通用多语种测试集上将LID准确率提高了23%，CER降低了18%。在口音和方言数据上，最佳提交模型将CER降低了30.2%，LID准确率提高了15.7%。

**Conclusion:** 此次挑战赛证明了社区挑战赛在使语音技术更具包容性方面的重要性。

**Abstract:** Recent improvements in multilingual ASR have not been equally distributed
across languages and language varieties. To advance state-of-the-art (SOTA) ASR
models, we present the Interspeech 2025 ML-SUPERB 2.0 Challenge. We construct a
new test suite that consists of data from 200+ languages, accents, and dialects
to evaluate SOTA multilingual speech models. The challenge also introduces an
online evaluation server based on DynaBench, allowing for flexibility in model
design and architecture for participants. The challenge received 5 submissions
from 3 teams, all of which outperformed our baselines. The best-performing
submission achieved an absolute improvement in LID accuracy of 23% and a
reduction in CER of 18% when compared to the best baseline on a general
multilingual test set. On accented and dialectal data, the best submission
obtained 30.2% lower CER and 15.7% higher LID accuracy, showing the importance
of community challenges in making speech technologies more inclusive.

</details>


### [3] [Toward Purpose-oriented Topic Model Evaluation enabled by Large Language Models](https://arxiv.org/abs/2509.07142)
*Zhiyin Tan,Jennifer D'Souza*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** This study presents a framework for automated evaluation of dynamically
evolving topic models using Large Language Models (LLMs). Topic modeling is
essential for organizing and retrieving scholarly content in digital library
systems, helping users navigate complex and evolving knowledge domains.
However, widely used automated metrics, such as coherence and diversity, often
capture only narrow statistical patterns and fail to explain semantic failures
in practice. We introduce a purpose-oriented evaluation framework that employs
nine LLM-based metrics spanning four key dimensions of topic quality: lexical
validity, intra-topic semantic soundness, inter-topic structural soundness, and
document-topic alignment soundness. The framework is validated through
adversarial and sampling-based protocols, and is applied across datasets
spanning news articles, scholarly publications, and social media posts, as well
as multiple topic modeling methods and open-source LLMs. Our analysis shows
that LLM-based metrics provide interpretable, robust, and task-relevant
assessments, uncovering critical weaknesses in topic models such as redundancy
and semantic drift, which are often missed by traditional metrics. These
results support the development of scalable, fine-grained evaluation tools for
maintaining topic relevance in dynamic datasets. All code and data supporting
this work are accessible at
https://github.com/zhiyintan/topic-model-LLMjudgment.

</details>


### [4] [Towards EnergyGPT: A Large Language Model Specialized for the Energy Sector](https://arxiv.org/abs/2509.07177)
*Amal Chebbi,Babajide Kolade*

Main category: cs.CL

> 本文介绍了EnergyGPT，一种专门针对能源领域的语言模型，通过微调LLaMA 3.1-8B模型开发，展示了其在多数能源相关任务中的优越表现。

<details>
  <summary>Details</summary>

**Motivation:** 大语言模型虽然在多个领域展示了惊人的能力，但其通用性质往往限制了在如能源这类需要深度技术专长和精确领域知识的专门领域的有效性。因此，有必要开发一个针对特定领域的语言模型，以提高在这些领域的表现。

**Method:** 通过监督微调，使用高质量整理过的能源相关文本语料库，对LLaMA 3.1-8B模型进行微调，开发出专门针对能源领域的语言模型EnergyGPT，并展示了包括数据收集与整理、模型微调、基准设计、LLM-judge选择、评估与部署在内的完整开发流程。

**Result:** 实验结果表明，与基础模型相比，EnergyGPT在大多数能源相关的语言理解和生成任务中表现出更好的性能。

**Conclusion:** 这项研究表明，在不需要大规模基础设施的情况下，通过采用适当的训练策略，可以开发出在特定领域表现优秀的语言模型。

**Abstract:** Large Language Models have demonstrated impressive capabilities across
various domains. However, their general-purpose nature often limits their
effectiveness in specialized fields such as energy, where deep technical
expertise and precise domain knowledge are essential. In this paper, we
introduce EnergyGPT, a domain-specialized language model tailored for the
energy sector, developed by fine-tuning LLaMA 3.1-8B model using Supervised
Fine-Tuning on a high-quality, curated corpus of energy-related texts. We
present a complete development pipeline, including data collection and
curation, model fine-tuning, benchmark design and LLM-judge choice, evaluation
and deployment. Through this work, we demonstrate that our training strategy
enables improvements in domain relevance and performance without the need for
large-scale infrastructure. By evaluating the performance of the model using
domain-specific question-answering benchmarks, our results demonstrate that
EnergyGPT outperforms the base model in most of the energy-related language
understanding and generation tasks.

</details>


### [5] [DischargeSim: A Simulation Benchmark for Educational Doctor-Patient Communication at Discharge](https://arxiv.org/abs/2509.07188)
*Zonghai Yao,Michael Sun,Won Seok Jang,Sunjae Kwon,Soie Kwon,Hong Yu*

Main category: cs.CL

> 研究开发了一个新的基准测试DischargeSim，用于评估语言模型在出院后教育患者的能力，结果显示模型规模与教育效果不一定成正比。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大型语言模型对住院期间的诊断推理能力进行了测试，但对于出院后的教育能力缺乏评估。因此，需要一种新的方法来评估LLM在出院教育中的表现。

**Method:** 通过DischargeSim模拟出院教育中的多轮对话，评估语言模型作为个性化出院教育者的性能。对话参与者包括基于LLM的医生代理和具有不同心理社会背景的患者代理。评估标准包括对话质量，个性化文档生成能力以及患者理解程度。

**Result:** 实验结果显示，不同LLM在出院教育中的能力存在显著差异，且并非模型规模越大教育效果越好，模型策略使用和内容优先级的权衡导致效果不一。

**Conclusion:** DischargeSim为评估LLM在出院教育中的表现提供了新的方法，有助于推动个性化、公平的患者支持措施的发展。

**Abstract:** Discharge communication is a critical yet underexplored component of patient
care, where the goal shifts from diagnosis to education. While recent large
language model (LLM) benchmarks emphasize in-visit diagnostic reasoning, they
fail to evaluate models' ability to support patients after the visit. We
introduce DischargeSim, a novel benchmark that evaluates LLMs on their ability
to act as personalized discharge educators. DischargeSim simulates post-visit,
multi-turn conversations between LLM-driven DoctorAgents and PatientAgents with
diverse psychosocial profiles (e.g., health literacy, education, emotion).
Interactions are structured across six clinically grounded discharge topics and
assessed along three axes: (1) dialogue quality via automatic and LLM-as-judge
evaluation, (2) personalized document generation including free-text summaries
and structured AHRQ checklists, and (3) patient comprehension through a
downstream multiple-choice exam. Experiments across 18 LLMs reveal significant
gaps in discharge education capability, with performance varying widely across
patient profiles. Notably, model size does not always yield better education
outcomes, highlighting trade-offs in strategy use and content prioritization.
DischargeSim offers a first step toward benchmarking LLMs in post-visit
clinical education and promoting equitable, personalized patient support.

</details>


### [6] [Rule-Based Moral Principles for Explaining Uncertainty in Natural Language Generation](https://arxiv.org/abs/2509.07190)
*Zahra Atf,Peter R Lewis*

Main category: cs.CL

> 本文提出了一个基于道德原则的框架，用于处理大型语言模型生成文本中的不确定性问题，旨在提供一个透明、轻量级的替代概率模型的方案，适用于负责任的自然语言生成。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型(LLMs)越来越被用于高风险环境中，在这种环境中解释不确定性既是技术问题也是伦理问题。概率方法通常不透明并且与透明度的期望不一致。我们提出了一种基于规则的道德原则框架来处理LLMs生成文本中的不确定性。

**Method:** 基于道德心理和美德伦理学的见解，我们定义了如预防、谦让和责任等规则来指导在知识或随机不确定性情况下的响应。这些规则被编码在一个轻量级的Prolog引擎中，不同的不确定性水平（低、中、高）会触发带有简单语言解释的相应系统动作。

**Result:** 通过对场景为基础的模拟进行基准测试，我们评估了规则的覆盖率、公平性和信任校准。临床和法律领域的实例展示了道德推理如何提高信任度和解释性。

**Conclusion:** 我们的方法提供了一种透明、轻量级的替代概率模型的方案，尤其适用于负责任的自然语言生成。

**Abstract:** Large language models (LLMs) are increasingly used in high-stakes settings,
where explaining uncertainty is both technical and ethical. Probabilistic
methods are often opaque and misaligned with expectations of transparency. We
propose a framework based on rule-based moral principles for handling
uncertainty in LLM-generated text. Using insights from moral psychology and
virtue ethics, we define rules such as precaution, deference, and
responsibility to guide responses under epistemic or aleatoric uncertainty.
These rules are encoded in a lightweight Prolog engine, where uncertainty
levels (low, medium, high) trigger aligned system actions with plain-language
rationales. Scenario-based simulations benchmark rule coverage, fairness, and
trust calibration. Use cases in clinical and legal domains illustrate how moral
reasoning can improve trust and interpretability. Our approach offers a
transparent, lightweight alternative to probabilistic models for socially
responsible natural language generation.

</details>


### [7] [LLM Analysis of 150+ years of German Parliamentary Debates on Migration Reveals Shift from Post-War Solidarity to Anti-Solidarity in the Last Decade](https://arxiv.org/abs/2509.07274)
*Aida Kostikova,Ole Pütz,Steffen Eger,Olga Sabelfeld,Benjamin Paassen*

Main category: cs.CL

> 通过大规模语言模型评估德国议会辩论中关于移民的(反)团结趋势，发现二战后对移民有高度团结，但从2015年起出现强烈的反团结趋势。

<details>
  <summary>Details</summary>

**Motivation:** 传统的手动注释限制了分析的范围，仅限于数据的小子集。大规模语言模型有潜力部分自动化复杂的注释任务，以更广泛地研究与迁移相关的政治演讲。

**Method:** 使用大规模语言模型(LLMs)对政治演讲中的(反)团结子类型进行自动注释，并将其与数千个人类注释参考进行比较。评估考虑了模型大小、提示差异、微调、历史与当代数据的影响，并调查了系统性错误。

**Result:** 研究结果发现二战后德国议会表现出高度的移民团结，但自2015年起呈现出强烈的反团结趋势。

**Conclusion:** 大规模语言模型在政治文本分析中表现出前景，同时也强调了在德国进行移民辩论的重要性，因为人口减少和劳动力短缺与日益增加的分化共存。

**Abstract:** Migration has been a core topic in German political debate, from millions of
expellees post World War II over labor migration to refugee movements in the
recent past. Studying political speech regarding such wide-ranging phenomena in
depth traditionally required extensive manual annotations, limiting the scope
of analysis to small subsets of the data. Large language models (LLMs) have the
potential to partially automate even complex annotation tasks. We provide an
extensive evaluation of a multiple LLMs in annotating (anti-)solidarity
subtypes in German parliamentary debates compared to a large set of thousands
of human reference annotations (gathered over a year). We evaluate the
influence of model size, prompting differences, fine-tuning, historical versus
contemporary data; and we investigate systematic errors. Beyond methodological
evaluation, we also interpret the resulting annotations from a social science
lense, gaining deeper insight into (anti-)solidarity trends towards migrants in
the German post-World War II period and recent past. Our data reveals a high
degree of migrant-directed solidarity in the postwar period, as well as a
strong trend towards anti-solidarity in the German parliament since 2015,
motivating further research. These findings highlight the promise of LLMs for
political text analysis and the importance of migration debates in Germany,
where demographic decline and labor shortages coexist with rising polarization.

</details>


### [8] [Causal Attention with Lookahead Keys](https://arxiv.org/abs/2509.07301)
*Zhuoqing Song,Peng Sun,Huizhuo Yuan,Quanquan Gu*

Main category: cs.CL

> 本文提出了一种名为CASTLE的新注意力机制，通过不断更新每个标记的键值并保持自回归属性，提高了语言模型表现。

<details>
  <summary>Details</summary>

**Motivation:** 标准的因果注意力中，每个标记的查询、键、值(QKV)是静态的，仅编码之前的上下文。研究动机在于引入一种新的注意力机制（CASTLE），能够在上下文逐渐展开时更新每个标记的键，以提高模型的表现。

**Method:** 提出了名为CAuSal aTtention with Lookahead kEys (CASTLE) 的注意力机制，该机制在上下文展开时不断更新每个标记的关键值(k)，这些关键值被称为前瞻关键值，因为它们属于较早的位置，但包含了相对于那些位置较晚出现的标记的信息，同时严格保持自回归属性。尽管该机制看似是顺序的，但其被证明可以等效转换为可以高效并行训练的形式。

**Result:** 在语言建模基准测试中，CASTLE在不同的模型规模上始终优于标准因果注意力，降低了验证困惑度，并提升了多种下游任务的性能。

**Conclusion:** 通过更新键的方式，CASTLE改进了标准因果注意力模型的表现，展示了在语言建模上的潜在价值，并且在不同规模的模型以及多种下游任务上表现出了改进。

**Abstract:** In standard causal attention, each token's query, key, and value (QKV) are
static and encode only preceding context. We introduce CAuSal aTtention with
Lookahead kEys (CASTLE), an attention mechanism that continually updates each
token's keys as the context unfolds. We term these updated keys lookahead keys
because they belong to earlier positions yet integrate information from tokens
that appear later relative to those positions, while strictly preserving the
autoregressive property. Although the mechanism appears sequential, we derive a
mathematical equivalence that avoids explicitly materializing lookahead keys at
each position and enables efficient parallel training. On language modeling
benchmarks, CASTLE consistently outperforms standard causal attention across
model scales, reducing validation perplexity and improving performance on a
range of downstream tasks.

</details>


### [9] [Basis Vector Metric: A Method for Robust Open-Ended State Change Detection](https://arxiv.org/abs/2509.07308)
*David Oprea,Sam Powers*

Main category: cs.CL

> 研究了一种新的称为BVM的方法来判断图像中的状态变化，并通过语言嵌入进行测试，实验一表明BVM在分类名词状态上优于其他几种方法，实验二则显示BVM在区分形容词上不如逻辑回归模型，但也为方法的改进提供了可能。

<details>
  <summary>Details</summary>

**Motivation:** 为了测试基于语言嵌入的BVM方法在判断图像状态变化上的效能。

**Method:** 使用MIT-States数据集，首先用BVM和其他几种指标比较，测试其在单独名词状态分类上的表现；然后用BVM与逻辑回归模型比较，测试其在单独形容词识别上的效果。

**Result:** 实验一表明BVM在分类名词状态上表现最佳；实验二结果显示BVM在区分形容词上不如逻辑回归模型，但表明可以通过方法上的改进提高其准确性。

**Conclusion:** BVM在名词状态分类上表现出色，但在形容词区分上还需改进，显示出改进BVM以提高整体准确性的潜力。

**Abstract:** We test a new method, which we will abbreviate using the acronym BVM (Basis
Vectors Method), in its ability to judge the state changes in images through
using language embeddings. We used the MIT-States dataset, containing about
53,000 images, to gather all of our data, which has 225 nouns and 115
adjectives, with each noun having about 9 different adjectives, forming
approximately 1000 noun-adjective pairs. For our first experiment, we test our
method's ability to determine the state of each noun class separately against
other metrics for comparison. These metrics are cosine similarity, dot product,
product quantization, binary index, Naive Bayes, and a custom neural network.
Among these metrics, we found that our proposed BVM performs the best in
classifying the states for each noun. We then perform a second experiment where
we try using BVM to determine if it can differentiate adjectives from one
another for each adjective separately. We compared the abilities of BVM to
differentiate adjectives against the proposed method the MIT-States paper
suggests: using a logistic regression model. In the end, we did not find
conclusive evidence that our BVM metric could perform better than the logistic
regression model at discerning adjectives. Yet, we were able to find evidence
for possible improvements to our method; this leads to the chance of increasing
our method's accuracy through certain changes in our methodologies.

</details>


### [10] [Instance-level Performance Prediction for Long-form Generation Tasks](https://arxiv.org/abs/2509.07309)
*Chi-Yang Hsu,Alexander Braylan,Yiheng Su,Omar Alonso,Matthew Lease*

Main category: cs.CL

> 本文提出了一种新型的、适用于长文本生成任务的性能预测基准，该方法能够对多方面的评价标准进行预测，并适用于多种模型。实验表明该方法能够在较少样本的情况下有效预测评价分数。

<details>
  <summary>Details</summary>

**Motivation:** 我们引入了这一基准来提升长文本生成任务的性能预测能力，特别是在多方面、细粒度的质量度量条件下。

**Method:** 我们提出了一种新型的、适用于长文本生成任务的实例级性能预测基准。该基准能够预测连续的评价分数，并且能够基于模型输入和输出估计预测区间的不确定性。这一方法对任务、模型和度量标准都是通用的。

**Result:** 实验结果表明，使用该基准，仅需16个训练样本即可有效预测长文本生成任务的评价分数。

**Conclusion:** 我们介绍了一个新的且有用的预测任务，这一基准对于促进性能预测研究和实践应用具有重要价值。

**Abstract:** We motivate and share a new benchmark for instance-level performance
prediction of long-form generation tasks having multi-faceted, fine-grained
quality metrics. Our task-, model- and metric-agnostic formulation predicts
continuous evaluation metric scores given only black-box model inputs and
outputs. Beyond predicting point estimates of metric scores, the benchmark also
requires inferring prediction intervals to quantify uncertainty around point
estimates. Evaluation spans 11 long-form datasets/tasks with multiple LLMs,
baselines, and metrics per task. We show that scores can be effectively
predicted across long-form generation tasks using as few as 16 training
examples. Overall, we introduce a novel and useful task, a valuable benchmark
to drive progress, and baselines ready for practical adoption today.

</details>


### [11] [Does This Look Familiar to You? Knowledge Analysis via Model Internal Representations](https://arxiv.org/abs/2509.07311)
*Sihyun Park*

Main category: cs.CL

> 本文提出了一种新方法KAMIR，通过分析模型内部表示来选择训练数据，适用于多种任务，实验证明选择较少熟悉的训练数据可以提高模型的泛化性能。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于在监督微调过程中，没有明确的方法来选择有效的训练数据，简单的增加数据量并不一定提高性能，且前期处理、采样、验证需要大量的时间和成本，本文提出了KAMIR这一新方法来解决这一问题。

**Method:** KAMIR通过分析模型的内部隐藏状态来选择训练数据，它计算每个层的隐藏状态与最终隐藏状态之间的相似度来评估数据，这种方法能够克服原有依赖于提示工程的方法的局限性。

**Result:** 本文提出了一种新颖的方法——基于模型内部表示的知识分析（KAMIR），通过分析模型的内部表示来选择合适的训练数据。KAMIR通过计算隐藏状态之间的相似性来评估数据，这种方法不仅适用于多个选择任务，而且还可以应用于机器阅读理解、摘要生成等广泛的任务。实验表明，使用较少熟悉的训练数据可以提高模型的泛化性能。

**Conclusion:** KAMIR方法克服了依赖于提示工程的方法的局限性，适用于多种任务，并且即使使用小数据集和简单的分类器架构也可以选择有用的训练数据。而且，实验证实使用较少熟悉的训练数据可以提高模型的泛化性能。

**Abstract:** Recent advances in large language models (LLMs) have been driven by
pretraining, supervised fine tuning (SFT), and alignment tuning. Among these,
SFT plays a crucial role in transforming a model 's general knowledge into
structured responses tailored to specific tasks. However, there is no clearly
established methodology for effective training data selection. Simply
increasing the volume of data does not guarantee performance improvements,
while preprocessing, sampling, and validation require substantial time and
cost.
  To address this issue, a variety of data selection methods have been
proposed. Among them, knowledge based selection approaches identify suitable
training data by analyzing the model 's responses. Nevertheless, these methods
typically rely on prompt engineering, making them sensitive to variations and
incurring additional costs for prompt design.
  In this study, we propose Knowledge Analysis via Model Internal
Representations (KAMIR), a novel approach that overcomes these limitations by
analyzing data based on the model 's internal representations. KAMIR computes
similarities between the hidden states of each layer (block) and the final
hidden states for a given input to assess the data. Unlike prior methods that
were largely limited to multiple choice tasks, KAMIR can be applied to a wide
range of tasks such as machine reading comprehension and summarization.
Moreover, it selects data useful for training based on the model 's familiarity
with the input, even with a small dataset and a simple classifier architecture.
Experiments across diverse task datasets demonstrate that training with less
familiar data leads to better generalization performance.

</details>


### [12] [Mitigating Attention Localization in Small Scale: Self-Attention Refinement via One-step Belief Propagation](https://arxiv.org/abs/2509.07324)
*Nakyung Lee,Yeongoon Kim,Minhae Oh,Suhwan Kim,Jin Woo Koo,Hyewon Jo,Jungwoo Lee*

Main category: cs.CL

> SAOBP通过信念传播过程引入多跳连接，解决自注意力机制在现代语言模型中遇到的局部化问题，提高模型性能，尤其是在小型模型和资源受限环境中。

<details>
  <summary>Details</summary>

**Motivation:** 现代语言模型中的基于Transformer的自注意力机制通常会陷入局部化问题，即注意力会在有限的令牌子集上集中，而无法捕捉长距离依赖性。

**Method:** 我们提出了Self-Attention One-step Belief Propagation (SAOBP)框架，通过信念传播过程来注入多跳关系，并引入全局令牌依赖性（GTD）来解释和量化这些交互。

**Result:** 实验结果表明，SAOBP有助于防止深层中的熵崩溃，并且能在任务适当的水平上自适应地维持GTD，从而支持模型性能的提升，特别是在小型模型中，这显示了其在资源有限的情况下提高推理质量的潜力。

**Conclusion:** SAOBP框架通过增强多跳连接的能力来缓解自注意力机制的局部化问题，并显示出提高模型性能的潜力，特别是在资源受限的情况下。

**Abstract:** Transformer-based self-attention mechanism serves as the core of modern
language models, yet it often suffers from localization, where attentions
collapse onto a limited subset of tokens and fail to capture long-range
dependencies. To address this issue, we propose Self-Attention One-step Belief
Propagation (SAOBP), a refinement framework that injects multi-hop
relationships through a belief propagation process. To interpret and quantify
these interactions, we introduce Global Token Dependency (GTD) that captures
the relative contribution of multihop connections within the attention graph.
Empirical results indicate that SAOBP helps prevent entropy collapse in deeper
layers and adaptively maintains GTD at task-appropriate levels, thereby
supporting improvements in model performance. Importantly, we observe
competitive gains in small-scale models, highlighting its potential for
improving inference quality in resource-constrained scenarios.

</details>


### [13] [PersonaFuse: A Personality Activation-Driven Framework for Enhancing Human-LLM Interactions](https://arxiv.org/abs/2509.07370)
*Yixuan Tang,Yi Yang,Ahmed Abbasi*

Main category: cs.CL

> PersonaFuse是一种新的大规模语言模型后训练框架，通过结合人格适配器和动态路由网络，使模型能够适应并表达不同的人格特性，从而提高其在社交和情感智能方面的表现。实验结果表明，PersonaFuse在多方面表现优于基线模型，并在下游应用中显示出一致的改善，同时保持了良好的通用推理和模型安全性。

<details>
  <summary>Details</summary>

**Motivation:** 由于大规模语言模型在实际对话中存在情感感知和社会能力上的局限性，研究团队提出了一种新的框架，旨在增强语言模型的社会和情感智能，以提高其与人类的交流质量。

**Method:** PersonaFuse采用了专家混合的架构，结合了人格适配器和动态路由网络，根据情境理论进行个性化的特质表达。该方法基于特质激活理论和大五人格模型来设计适配器，以适应不同的社会和任务环境。

**Result:** 实验结果表明PersonaFuse在社交和情感智能的多个维度上显著优于基线模型，并在如心理健康咨询和基于评论的客户服务等下游应用中显示出一致性改善，同时不会损害模型的安全性。

**Conclusion:** PersonaFuse为开发社会情感增强的语言模型提供了一种理论基础强且实用的方法，它标志着向着更人性化的AI系统迈进的重要一步。

**Abstract:** Recent advancements in Large Language Models (LLMs) demonstrate remarkable
capabilities across various fields. These developments have led to more direct
communication between humans and LLMs in various situations, such as social
companionship and psychological support. However, LLMs often exhibit
limitations in emotional perception and social competence during real-world
conversations. These limitations partly originate from their inability to adapt
their communication style and emotional expression to different social and task
contexts. In this work, we introduce PersonaFuse, a novel LLM post-training
framework that enables LLMs to adapt and express different personalities for
varying situations. Inspired by Trait Activation Theory and the Big Five
personality model, PersonaFuse employs a Mixture-of-Expert architecture that
combines persona adapters with a dynamic routing network, enabling contextual
trait expression. Experimental results show that PersonaFuse substantially
outperforms baseline models across multiple dimensions of social-emotional
intelligence. Importantly, these gains are achieved without sacrificing general
reasoning ability or model safety, which remain common limitations of direct
prompting and supervised fine-tuning approaches. PersonaFuse also delivers
consistent improvements in downstream human-centered applications, such as
mental health counseling and review-based customer service. Finally, human
preference evaluations against leading LLMs, including GPT-4o and DeepSeek,
demonstrate that PersonaFuse achieves competitive response quality despite its
comparatively smaller model size. These findings demonstrate that
PersonaFuse~offers a theoretically grounded and practical approach for
developing social-emotional enhanced LLMs, marking a significant advancement
toward more human-centric AI systems.

</details>


### [14] [Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents](https://arxiv.org/abs/2509.07389)
*Sankalp Tattwadarshi Swain,Anshika Krishnatray,Dhruv Kumar,Jagat Sesh Challa*

Main category: cs.CL

> 本研究提出一个实验框架，评估大型语言模型通过模式识别和互动反馈掌握新语言的能力。发现模型虽未能成功建立对话，但学习方式类同于人类，衬托出互动反馈学习对于语言模型的重要作用。

<details>
  <summary>Details</summary>

**Motivation:** 现有的语言模型评估主要集中在其词汇学习、形态规则归纳、句法推广、语用推理和跨语言迁移上的表现，缺乏针对模型能否通过模式识别和互动反馈掌握新语言的研究。因此，提出这个新的实验框架，评估现有的LLM能否像人一样通过互动学习语言。

**Method:** 研究者创造了一个新的语言（Tinkatongue）作为评估对象，让大型语言模型（LLM代理）与只能理解这种新语言的机器人互动，以此来测试LLM代理获取并运用新语言的能力。

**Result:** 本研究提出了一种新的实验框架，用于评估大型语言模型（LLM代理）是否能通过模式识别和互动反馈来掌握一门语言。实验展示了一个LLM代理与仅能理解新构建语言（Tinkatongue）的机器人进行对话的能力。研究发现，尽管LLM代理在100次回应后未能建立有效对话，但它们采用了与人类语言学习相似的策略。结果暗示了评估基准的新方向，并为从互动反馈中更有效地学习的模型设计提供了路径。

**Conclusion:** 实验结果表明，LLM代理在直接通过互动学习并使用新语言方面存在挑战，但其学习方法反映了人类学习语言的过程。这为未来研究提供了方向，并提示了开发更有效互动学习模型的可能路径。

**Abstract:** Existing evaluation studies on linguistic competence of large language models
(LLM agents) have focused primarily on vocabulary learning, morphological rule
induction, syntactic generalization, pragmatic inference, and cross-linguistic
transfer. However, none assess whether LLM agents can acquire a language
through pattern recognition and interactive feedback, a central feature of
human language acquisition. We propose a novel experimental framework in which
an LLM agent is evaluated on its ability to acquire and use a newly constructed
language (Tinkatongue) in conversation with a bot that understands only
Tinkatongue. Our findings show that LLM agents fail to establish a conversation
within 100 responses, yet they adopt distinct strategies that mirror human
approaches to language learning. The results suggest a new direction for
evaluation benchmarks and open pathways to model designs that learn more
effectively from interactive feedback.

</details>


### [15] [The Role of Exploration Modules in Small Language Models for Knowledge Graph Question Answering](https://arxiv.org/abs/2509.07399)
*Yi-Jie Cheng,Oscar Chew,Yun-Nung Chen*

Main category: cs.CL

> 研究探讨了小型语言模型在知识图谱问题回答中的能力，并提出使用轻量级探索模块来提升性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有集成知识图谱的方法主要依赖于大型且专有模型，这限制了其可访问性和可扩展性。这项研究旨在探索对于小型语言模型（SLMs）来说，如何有效利用知识图谱进行问题回答。

**Method:** 研究提出采用简单且高效的探索模块来辅助小型语言模型进行知识图谱的遍历和推理，而不是让语言模型自身执行这些任务。

**Result:** 实验表明，轻量级探索模块能够有效提升小型语言模型在基于知识图谱的问题回答任务上的性能。

**Conclusion:** 研究表明，通过引入轻量级探索模块，能够克服小型语言模型在知识图谱推理上的性能限制，显著提升其在问题回答任务中的表现。

**Abstract:** Integrating knowledge graphs (KGs) into the reasoning processes of large
language models (LLMs) has emerged as a promising approach to mitigate
hallucination. However, existing work in this area often relies on proprietary
or extremely large models, limiting accessibility and scalability. In this
study, we investigate the capabilities of existing integration methods for
small language models (SLMs) in KG-based question answering and observe that
their performance is often constrained by their limited ability to traverse and
reason over knowledge graphs. To address this limitation, we propose leveraging
simple and efficient exploration modules to handle knowledge graph traversal in
place of the language model itself. Experiment results demonstrate that these
lightweight modules effectively improve the performance of small language
models on knowledge graph question answering tasks. Source code:
https://github.com/yijie-cheng/SLM-ToG/.

</details>


### [16] [LongEmotion: Measuring Emotional Intelligence of Large Language Models in Long-Context Interaction](https://arxiv.org/abs/2509.07403)
*Weichu Liu,Jing Xiong,Yuxuan Hu,Zixuan Li,Minghuan Tan,Ningning Mao,Chenyang Zhao,Zhongwei Wan,Chaofan Tao,Wendong Xu,Hui Shen,Chengming Li,Lingpeng Kong,Ngai Wong*

Main category: cs.CL

> 研究设计了一个名为LongEmotion的长上下文情感智能基准，并通过RAG和CoEM方法在情感分类、检测、问答、对话、总结和表达任务中展示了优于标准提示方法的性能，助力大型语言模型更好地应用于实际情感智能任务。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基准在长上下文情感智能评估中有些不足，尤其是在真实环境的应用中。因此，研究旨在设计一个更贴近真实应用场景的评估基准。

**Method:** 研究使用了长上下文情感智能基准（LongEmotion）来评估情感分类、检测、问答、对话、总结和表达等任务，并采用了检索增强生成（RAG）和合作情感建模（CoEM）方法来提高真实场景下的性能。其中RAG方法利用对话上下文和大型语言模型自身作为检索源，而CoEM方法进一步分为五个阶段。

**Result:** 实验结果表明，RAG和CoEM两种方法在大多数长上下文情感任务上都能有效提升大型语言模型的情感智能性能。同时，研究表明在GPT系列模型中，不同模型在情感智能任务上的表现存在显著差异。

**Conclusion:** 研究提出了LongEmotion基准，用于评估长上下文情感任务的模型性能，并展示了RAG和CoEM方法在这些任务上的改进效果，推动了大型语言模型在实用情感智能应用中的进展。

**Abstract:** Large language models (LLMs) make significant progress in Emotional
Intelligence (EI) and long-context understanding. However, existing benchmarks
tend to overlook certain aspects of EI in long-context scenarios, especially
under realistic, practical settings where interactions are lengthy, diverse,
and often noisy. To move towards such realistic settings, we present
LongEmotion, a benchmark specifically designed for long-context EI tasks. It
covers a diverse set of tasks, including Emotion Classification, Emotion
Detection, Emotion QA, Emotion Conversation, Emotion Summary, and Emotion
Expression. On average, the input length for these tasks reaches 8,777 tokens,
with long-form generation required for Emotion Expression. To enhance
performance under realistic constraints, we incorporate Retrieval-Augmented
Generation (RAG) and Collaborative Emotional Modeling (CoEM), and compare them
with standard prompt-based methods. Unlike conventional approaches, our RAG
method leverages both the conversation context and the large language model
itself as retrieval sources, avoiding reliance on external knowledge bases. The
CoEM method further improves performance by decomposing the task into five
stages, integrating both retrieval augmentation and limited knowledge
injection. Experimental results show that both RAG and CoEM consistently
enhance EI-related performance across most long-context tasks, advancing LLMs
toward more practical and real-world EI applications. Furthermore, we conducted
a comparative case study experiment on the GPT series to demonstrate the
differences among various models in terms of EI. Code is available on GitHub at
https://github.com/LongEmotion/LongEmotion, and the project page can be found
at https://longemotion.github.io/.

</details>


### [17] [AIxcellent Vibes at GermEval 2025 Shared Task on Candy Speech Detection: Improving Model Performance by Span-Level Training](https://arxiv.org/abs/2509.07459)
*Christian Rene Thelen,Patrick Gustav Blaneck,Tobias Bornheim,Niklas Grieger,Stephan Bialonski*

Main category: cs.CL

> 本研究调查了使用多语言和单语言语言模型在德语YouTube评论中检测积极语言（糖果言论）的方法，发现多语言的XLM-RoBERTa-Large表现最佳，展示了多语言模型在识别积极支持语言方面的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管积极的、支持性的在线交流（糖果言论）有可能培养文明，但自动检测此类语言仍鲜为人知，这限制了对其影响的系统性分析。

**Method:** 我们研究了如何在包含46k条评论的德语YouTube语料库中，通过单语和多语言语言模型（包括GBERT、Qwen3嵌入和XLM-RoBERTa）来可靠地检测糖果言论。

**Result:** 研究结果表明，用于检测糖果言论的多语言XLM-RoBERTa-Large模型在GermEval 2025共享任务中的二元正F1（0.8906，严格F1为0.6307）方面优于其他方法。

**Conclusion:** 我们的研究结果表明，多语言模型在识别积极、支持性语言方面是有效的。

**Abstract:** Positive, supportive online communication in social media (candy speech) has
the potential to foster civility, yet automated detection of such language
remains underexplored, limiting systematic analysis of its impact. We
investigate how candy speech can be reliably detected in a 46k-comment German
YouTube corpus by monolingual and multilingual language models, including
GBERT, Qwen3 Embedding, and XLM-RoBERTa. We find that a multilingual
XLM-RoBERTa-Large model trained to detect candy speech at the span level
outperforms other approaches, ranking first in both binary positive F1: 0.8906)
and categorized span-based detection (strict F1: 0.6307) subtasks at the
GermEval 2025 Shared Task on Candy Speech Detection. We speculate that
span-based training, multilingual capabilities, and emoji-aware tokenizers
improved detection performance. Our results demonstrate the effectiveness of
multilingual models in identifying positive, supportive language.

</details>


### [18] [Understanding Stigmatizing Language Lexicons: A Comparative Analysis in Clinical Contexts](https://arxiv.org/abs/2509.07462)
*Yiliang Zhou,Di Hu,Tianchu Lyu,Jasmine Dhillon,Alexandra L. Beck,Gelareh Sadigh,Kai Zheng*

Main category: cs.CL

> 本研究系统地检索和比较了四种现有的医疗领域污名化语言词汇表，发现这些词汇表之间有中等程度的语义相似性，而多数污名化术语涉及医生的判断性表达。结果强调了建立标准化污名化语言词汇表的必要性。

<details>
  <summary>Details</summary>

**Motivation:** 污名化语言导致医疗不公，然而目前尚无普遍接受或标准化的词汇表来定义医疗领域的污名化语言，因此本研究旨在识别现有污名化语言词汇表并进行比较分析。

**Method:** 本研究通过系统的文献检索来识别存在的污名化语言词汇表，并对这些词汇表进行了比较分析，考察它们之间的相似性和差异性，以及基于现有情感数据集的情感分类分布。

**Result:** 研究发现共识别出四个词汇表，并分析结果显示它们之间存在中等程度的语义相似性。大多数污名化术语与临床医生用来描述感知到的负面行为的判断性表达有关。情感分析显示大部分词汇被归类为负面情绪的词汇，不过不同词汇表间存在差异。

**Conclusion:** 研究结果强调需要建立一个标准化的污名化语言词汇表，并强调了在临床文本中识别污名化语言的挑战。

**Abstract:** Stigmatizing language results in healthcare inequities, yet there is no
universally accepted or standardized lexicon defining which words, terms, or
phrases constitute stigmatizing language in healthcare. We conducted a
systematic search of the literature to identify existing stigmatizing language
lexicons and then analyzed them comparatively to examine: 1) similarities and
discrepancies between these lexicons, and 2) the distribution of positive,
negative, or neutral terms based on an established sentiment dataset. Our
search identified four lexicons. The analysis results revealed moderate
semantic similarity among them, and that most stigmatizing terms are related to
judgmental expressions by clinicians to describe perceived negative behaviors.
Sentiment analysis showed a predominant proportion of negatively classified
terms, though variations exist across lexicons. Our findings underscore the
need for a standardized lexicon and highlight challenges in defining
stigmatizing language in clinical texts.

</details>


### [19] [From Scarcity to Efficiency: Investigating the Effects of Data Augmentation on African Machine Translation](https://arxiv.org/abs/2509.07471)
*Mardiyyah Oduwole,Oluwatosin Olajide,Jamiu Suleiman,Faith Hunja,Busayo Awobade,Fatimo Adebanjo,Comfort Akanni,Chinonyelum Igwe,Peace Ododo,Promise Omoigui,Steven Kolawole,Abraham Owodunni*

Main category: cs.CL

> 本研究采用数据增强技术改善六种非洲低资源语言的机器翻译系统，显著提高了这些系统的性能。

<details>
  <summary>Details</summary>

**Motivation:** 研究目的是探索数据增强技术在非洲大陆语言多样性下，改善机器翻译系统的挑战与机遇。

**Method:** 本研究探讨了数据增强技术在改善非洲低资源语言的机器翻译系统中的效果，特别关注了句子拼接加回译和切换两种技术。

**Result:** 实验表明，机器翻译性能显著提升，六种语言的BLEU分数至少提高了25%。

**Conclusion:** 研究结果揭示了这些技术提高低资源语言机器翻译系统性能的潜力，有助于开发针对资源匮乏语言的更强大翻译系统。

**Abstract:** The linguistic diversity across the African continent presents different
challenges and opportunities for machine translation. This study explores the
effects of data augmentation techniques in improving translation systems in
low-resource African languages. We focus on two data augmentation techniques:
sentence concatenation with back translation and switch-out, applying them
across six African languages. Our experiments show significant improvements in
machine translation performance, with a minimum increase of 25\% in BLEU score
across all six languages.We provide a comprehensive analysis and highlight the
potential of these techniques to improve machine translation systems for
low-resource languages, contributing to the development of more robust
translation systems for under-resourced languages.

</details>


### [20] [HALT-RAG: A Task-Adaptable Framework for Hallucination Detection with Calibrated NLI Ensembles and Abstention](https://arxiv.org/abs/2509.07475)
*Saumya Goswami,Siddharth Kurra*

Main category: cs.CL

> 研究团队推出了HALT-RAG，这是一种用于检测RAG生成输出中幻觉现象的系统，通过使用NLI模型集合和轻量级词汇信号，并经过严格的训练和验证，系统在多个任务上均展现出了良好的性能，并具备灵活调整以满足安全要求的能力。

<details>
  <summary>Details</summary>

**Motivation:** 随着生成语言模型的安全部署变得至关重要，检测生成内容是否与给定源头文本相矛盾或没有支持成为了一个严重的挑战。HALT-RAG被设计用于解决这一问题，以确保生成内容的可靠性和安全性。

**Method:** 我们的研究引入了HALT-RAG，一种用于检测基于检索增强生成（RAG）管道输出中的幻觉（hallucinations）的后处理验证系统。该系统采用两种预先训练好的自然语言推理（NLI）模型的集合以及轻量级的词汇信号来创建一个通用特征集，并利用这些特征训练一个简单的、可校准的、针对特定任务的元分类器。

**Result:** 通过严谨的5折交叉验证协议，我们评估了HALT-RAG在HaluEval基准上的表现，该系统在摘要、问答和对话任务上分别实现了0.7756、0.9786和0.7391的较强的OOF F1得分。系统的良好校准概率使得可以实现实践中的弃权机制，保证达到模型性能与安全性要求之间的平衡。

**Conclusion:** HALT-RAG通过采用一组通用特征以及特定任务的分类器，并采取了精密约束决策策略，实现了对幻觉内容的有效检测，其良好的校准概率为平衡模型性能和安全需求提供了一种可靠的工具。

**Abstract:** Detecting content that contradicts or is unsupported by a given source text
is a critical challenge for the safe deployment of generative language models.
We introduce HALT-RAG, a post-hoc verification system designed to identify
hallucinations in the outputs of Retrieval-Augmented Generation (RAG)
pipelines. Our flexible and task-adaptable framework uses a universal feature
set derived from an ensemble of two frozen, off-the-shelf Natural Language
Inference (NLI) models and lightweight lexical signals. These features are used
to train a simple, calibrated, and task-adapted meta-classifier. Using a
rigorous 5-fold out-of-fold (OOF) training protocol to prevent data leakage and
produce unbiased estimates, we evaluate our system on the HaluEval benchmark.
By pairing our universal feature set with a lightweight, task-adapted
classifier and a precision-constrained decision policy, HALT-RAG achieves
strong OOF F1-scores of 0.7756, 0.9786, and 0.7391 on the summarization, QA,
and dialogue tasks, respectively. The system's well-calibrated probabilities
enable a practical abstention mechanism, providing a reliable tool for
balancing model performance with safety requirements.

</details>


### [21] [ALLabel: Three-stage Active Learning for LLM-based Entity Recognition using Demonstration Retrieval](https://arxiv.org/abs/2509.07512)
*Zihan Chen,Lei Shi,Weize Wu,Qiji Zhou,Yue Zhang*

Main category: cs.CL

> 提出了ALLabel，一种用于优化实体识别任务中样本选择的三阶段主动学习框架，显著降低成本并保持高水平的性能。

<details>
  <summary>Details</summary>

**Motivation:** 提高实体识别任务的性能-成本平衡，尤其是针对大规模高性能实体识别任务，而现有方法通常成本高。

**Method:** ALLabel是一种三阶段框架，设计用于在准备LLM建模演示时选择最具信息量和代表性的样本。通过顺序使用三种不同的主动学习策略，ALLabel能够超越所有基线方法，尤其是在专业化领域数据集上表现出色。

**Result:** 实验结果显示，使用ALLabel对数据集的5%-10%进行选择性标注，可以达到与对整个数据集标注的方法相媲美的性能。

**Conclusion:** ALLabel证明了其在不同专业领域数据集上能够有效降低标注成本同时保持高性能，进一步分析和消融实验也验证了其有效性和泛化能力。

**Abstract:** Many contemporary data-driven research efforts in the natural sciences, such
as chemistry and materials science, require large-scale, high-performance
entity recognition from scientific datasets. Large language models (LLMs) have
increasingly been adopted to solve the entity recognition task, with the same
trend being observed on all-spectrum NLP tasks. The prevailing entity
recognition LLMs rely on fine-tuned technology, yet the fine-tuning process
often incurs significant cost. To achieve a best performance-cost trade-off, we
propose ALLabel, a three-stage framework designed to select the most
informative and representative samples in preparing the demonstrations for LLM
modeling. The annotated examples are used to construct a ground-truth retrieval
corpus for LLM in-context learning. By sequentially employing three distinct
active learning strategies, ALLabel consistently outperforms all baselines
under the same annotation budget across three specialized domain datasets.
Experimental results also demonstrate that selectively annotating only 5\%-10\%
of the dataset with ALLabel can achieve performance comparable to the method
annotating the entire dataset. Further analyses and ablation studies verify the
effectiveness and generalizability of our proposal.

</details>


### [22] [VeriOS: Query-Driven Proactive Human-Agent-GUI Interaction for Trustworthy OS Agents](https://arxiv.org/abs/2509.07553)
*Zheng Wu,Heyuan Huang,Xingyu Lou,Xiangmou Qu,Pengzhou Cheng,Zongru Wu,Weiwen Liu,Weinan Zhang,Jun Wang,Zhaoxiang Wang,Zhuosheng Zhang*

Main category: cs.CL

> 研究提出了VeriOS-Agent，一个基于查询驱动的人机GUI交互框架训练的操作系统代理，与最先进的方法相比，VeriOS-Agent在不可信场景下将平均每步成功概率提高了20.64%。

<details>
  <summary>Details</summary>

**Motivation:** 随着多模态大型语言模型的快速发展，操作系统代理通过设备上的图形用户界面（GUI）自动化任务的能力越来越强。然而，大多数现有的操作系统代理设计是为理想化的环境，而现实世界中经常存在不可信的条件。为了减轻这种场景下过执行的风险，本研究提出了上述方法。

**Method:** 提出了一种基于查询驱动的人机GUI交互框架，该框架允许操作系统代理在不可信场景中主动查询人类以获得更可靠的完成任务的方法。具体来说，VeriOS-Agent代理在正常条件下自主执行动作，而在不可信场景中主动向人类查询。这种方法采用了两阶段学习范式，用以解耦和利用元知识。

**Result:** 实验表明，与现有的方法相比，VeriOS-Agent在不降低正常场景下性能的同时，显著提高了在不可信条件下的任务完成成功率。

**Conclusion:** 实验结果显示，与最先进的方法相比，在不可信场景下，VeriOS-Agent将平均每步成功概率提高了20.64%，同时不影响正常的表现。分析表明，VeriOS-Agent在合理性、泛化和可扩展性方面表现出色。

**Abstract:** With the rapid progress of multimodal large language models, operating system
(OS) agents become increasingly capable of automating tasks through on-device
graphical user interfaces (GUIs). However, most existing OS agents are designed
for idealized settings, whereas real-world environments often present
untrustworthy conditions. To mitigate risks of over-execution in such
scenarios, we propose a query-driven human-agent-GUI interaction framework that
enables OS agents to decide when to query humans for more reliable task
completion. Built upon this framework, we introduce VeriOS-Agent, a trustworthy
OS agent trained with a two-stage learning paradigm that falicitate the
decoupling and utilization of meta-knowledge. Concretely, VeriOS-Agent
autonomously executes actions in normal conditions while proactively querying
humans in untrustworthy scenarios. Experiments show that VeriOS-Agent improves
the average step-wise success rate by 20.64\% in untrustworthy scenarios over
the state-of-the-art, without compromising normal performance. Analysis
highlights VeriOS-Agent's rationality, generalizability, and scalability. The
codes, datasets and models are available at
https://github.com/Wuzheng02/VeriOS.

</details>


### [23] [Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with Guided Decomposition](https://arxiv.org/abs/2509.07555)
*Yi Liu,Xiangrong Zhu,Xiangyu Liu,Wei Wei,Wei Hu*

Main category: cs.CL

> The paper introduces IRAKE, an iterative and guided method to improve knowledge editing in large language models for multi-hop question answering, addressing the challenge of edit skipping.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to improve knowledge editing (KE) efficiency without retraining large language models (LLMs). It addresses the limitation of existing retrieval-augmented generation (RAG)-based methods that excel in editing simple knowledge but fail in complex, multi-hop question answering due to the issue of 'edit skipping'.

**Method:** We propose a novel Iterative Retrieval-Augmented Knowledge Editing method with guided decomposition (IRAKE) which uses guidance from single edited facts and entire edited cases to address the issue of 'edit skipping' in multi-hop question answering.

**Result:** Experimental results show that IRAKE successfully mitigates the failure of editing caused by edit skipping, improving performance in multi-hop question answering tasks over existing state-of-the-art methods.

**Conclusion:** The paper concludes that IRAKE, by leveraging guided decomposition and iterative retrieval-augmentation, significantly enhances knowledge editing capabilities in complex multi-hop scenarios, overcoming the 'edit skipping' issue present in current techniques.

**Abstract:** In a rapidly evolving world where information updates swiftly, knowledge in
large language models (LLMs) becomes outdated quickly. Retraining LLMs is not a
cost-effective option, making knowledge editing (KE) without modifying
parameters particularly necessary. We find that although existing
retrieval-augmented generation (RAG)-based KE methods excel at editing simple
knowledge, they struggle with KE in multi-hop question answering due to the
issue of "edit skipping", which refers to skipping the relevant edited fact in
inference. In addition to the diversity of natural language expressions of
knowledge, edit skipping also arises from the mismatch between the granularity
of LLMs in problem-solving and the facts in the edited memory. To address this
issue, we propose a novel Iterative Retrieval-Augmented Knowledge Editing
method with guided decomposition (IRAKE) through the guidance from single
edited facts and entire edited cases. Experimental results demonstrate that
IRAKE mitigates the failure of editing caused by edit skipping and outperforms
state-of-the-art methods for KE in multi-hop question answering.

</details>


### [24] [BALI: Enhancing Biomedical Language Representations through Knowledge Graph and Language Model Alignment](https://arxiv.org/abs/2509.07588)
*Andrey Sakhovskiy,Elena Tutubalina*

Main category: cs.CL

> The paper proposes BALI, a joint pre-training method for biomedical LMs and KGs, which improves the performance on language understanding tasks and the quality of entity representations by aligning LM and KG representations.

<details>
  <summary>Details</summary>

**Motivation:** To improve the comprehension of complex, domain-specific concept structures and factual information encoded in biomedical Knowledge Graphs (KGs) by existing biomedical LMs.

**Method:** BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel joint LM and KG pre-training method that augments an LM with external knowledge by the simultaneous learning of a dedicated KG encoder and aligning the representations of both the LM and the graph.

**Result:** Implementing the method on several leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves their performance on a range of language understanding tasks and the quality of entity representations.

**Conclusion:** The proposed BALI method enhances the understanding of complex, domain-specific concept structures and factual information encoded in biomedical Knowledge Graphs (KGs) for biomedical LMs.

**Abstract:** In recent years, there has been substantial progress in using pretrained
Language Models (LMs) on a range of tasks aimed at improving the understanding
of biomedical texts. Nonetheless, existing biomedical LLMs show limited
comprehension of complex, domain-specific concept structures and the factual
information encoded in biomedical Knowledge Graphs (KGs). In this work, we
propose BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel
joint LM and KG pre-training method that augments an LM with external knowledge
by the simultaneous learning of a dedicated KG encoder and aligning the
representations of both the LM and the graph. For a given textual sequence, we
link biomedical concept mentions to the Unified Medical Language System (UMLS)
KG and utilize local KG subgraphs as cross-modal positive samples for these
mentions. Our empirical findings indicate that implementing our method on
several leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves
their performance on a range of language understanding tasks and the quality of
entity representations, even with minimal pre-training on a small alignment
dataset sourced from PubMed scientific abstracts.

</details>


### [25] [MaLei at MultiClinSUM: Summarisation of Clinical Documents using Perspective-Aware Iterative Self-Prompting with LLMs](https://arxiv.org/abs/2509.07622)
*Libo Ren,Yee Man Ng,Lifeng Han*

Main category: cs.CL

> 本研究提出了一种使用迭代自提示技术的临床报告总结方法，通过细粒度的模型微调，所得总结与参考摘要在语义上的等价性高，有助于提升医患间的沟通效率。

<details>
  <summary>Details</summary>

**Motivation:** 临床报告内容详尽且充满专业术语，导致领域专家难以高效识别报告中的关键内容。本文旨在通过改进的总结方法提高临床报告的可读性和重要信息的提取效率，以促进医生和患者之间的沟通。

**Method:** 采用迭代自提示技术在大型语言模型上生成特定任务的提示，并通过基于实例的少量示例学习来优化这些提示。另外，利用词法和嵌入空间指标，如ROUGE和BERT评分，来指导模型的微调过程。

**Result:** 使用基于GPT-4和GPT-4o的视角感知ISP方法的提交版本，获得了ROUGE分数（46.53, 24.68, 30.77）及BERT得分（87.84, 83.25, 85.46）（分别为P值、R值和F1值），这些得分来源于针对3,396份从开放期刊中提取的临床病例报告的官方评估。

**Conclusion:** 本次研究展示了视角感知ISP技术在临床报告总结中的应用前景，它可以生成与参考摘要在语义上等效的总结，尽管词汇层面的重叠较低，从而支持更好的医患沟通。

**Abstract:** Efficient communication between patients and clinicians plays an important
role in shared decision-making. However, clinical reports are often lengthy and
filled with clinical jargon, making it difficult for domain experts to identify
important aspects in the document efficiently. This paper presents the
methodology we applied in the MultiClinSUM shared task for summarising clinical
case documents. We used an Iterative Self-Prompting technique on large language
models (LLMs) by asking LLMs to generate task-specific prompts and refine them
via example-based few-shot learning. Furthermore, we used lexical and embedding
space metrics, ROUGE and BERT-score, to guide the model fine-tuning with
epochs. Our submission using perspective-aware ISP on GPT-4 and GPT-4o achieved
ROUGE scores (46.53, 24.68, 30.77) and BERTscores (87.84, 83.25, 85.46) for (P,
R, F1) from the official evaluation on 3,396 clinical case reports from various
specialties extracted from open journals. The high BERTscore indicates that the
model produced semantically equivalent output summaries compared to the
references, even though the overlap at the exact lexicon level is lower, as
reflected in the lower ROUGE scores. This work sheds some light on how
perspective-aware ISP (PA-ISP) can be deployed for clinical report
summarisation and support better communication between patients and clinicians.

</details>


### [26] [MoLoRAG: Bootstrapping Document Understanding via Multi-modal Logic-aware Retrieval](https://arxiv.org/abs/2509.07666)
*Xixi Wu,Yanchao Tan,Nan Hou,Ruiyang Zhang,Hong Cheng*

Main category: cs.CL

> 提出MoLoRAG框架解决传统方法在多模态多页文档理解中忽略逻辑联系的问题，提升了文档问答任务的准确性和检索精度。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大型语言模型和大型视觉语言模型在处理多模态多页文档理解时存在局限性，特别是对于页面间的逻辑联系和查询的关系考虑不足，这影响了推理能力。

**Method:** 提出了一种名为MoLoRAG的逻辑感知检索框架，通过构建页面图来捕捉页面间的关系，采用轻量级视觉语言模型进行图遍历来检索相关页面，强调结合语义和逻辑相关性，提高检索准确性。

**Result:** 实验结果表明，在四个文档问答数据集上的准确率较直接使用大型视觉语言模型提升了平均9.68%，检索精度也提高了平均7.44%。

**Conclusion:** MoLoRAG框架通过融合语义和逻辑的相关性，显著提高了多模态多页文档理解中相关页面的检索质量，增强了文档问答任务的表现。

**Abstract:** Document Understanding is a foundational AI capability with broad
applications, and Document Question Answering (DocQA) is a key evaluation task.
Traditional methods convert the document into text for processing by Large
Language Models (LLMs), but this process strips away critical multi-modal
information like figures. While Large Vision-Language Models (LVLMs) address
this limitation, their constrained input size makes multi-page document
comprehension infeasible. Retrieval-augmented generation (RAG) methods mitigate
this by selecting relevant pages, but they rely solely on semantic relevance,
ignoring logical connections between pages and the query, which is essential
for reasoning.
  To this end, we propose MoLoRAG, a logic-aware retrieval framework for
multi-modal, multi-page document understanding. By constructing a page graph
that captures contextual relationships between pages, a lightweight VLM
performs graph traversal to retrieve relevant pages, including those with
logical connections often overlooked. This approach combines semantic and
logical relevance to deliver more accurate retrieval. After retrieval, the
top-$K$ pages are fed into arbitrary LVLMs for question answering. To enhance
flexibility, MoLoRAG offers two variants: a training-free solution for easy
deployment and a fine-tuned version to improve logical relevance checking.
Experiments on four DocQA datasets demonstrate average improvements of 9.68% in
accuracy over LVLM direct inference and 7.44% in retrieval precision over
baselines. Codes and datasets are released at
https://github.com/WxxShirley/MoLoRAG.

</details>


### [27] [M-BRe: Discovering Training Samples for Relation Extraction from Unlabeled Texts with Large Language Models](https://arxiv.org/abs/2509.07730)
*Zexuan Li,Hongliang Dai,Piji Li*

Main category: cs.CL

> A framework named M-BRe is proposed for efficient and effective extraction of training instances from unlabeled texts for Relation Extraction, addressing the challenges of using large language models (LLMs) for RE.

<details>
  <summary>Details</summary>

**Motivation:** The manual annotation of training data for Relation Extraction may be prohibitively expensive, so an efficient automatic method to extract training instances from unlabeled texts is developed. The challenges of using large language models (LLMs) for RE with predefined relation categories are addressed, aiming to reduce computational overhead and improve semantic capture of relations.

**Method:** This paper proposes a framework called M-BRe, which consists of three modules: Relation Grouping, Relation Extraction, and Label Decision, aiming to combine the advantages of multi-class and binary classification approaches to improve the efficiency and effectiveness of extracting training instances from unlabeled texts for Relation Extraction (RE).

**Result:** Experiments confirm the framework's superior capability in discovering high-quality training samples from unlabeled texts for RE, thus improving the efficiency and effectiveness of Relation Extraction.

**Conclusion:** The proposed framework M-BRe successfully offers a more efficient and effective way of extracting training instances for Relation Extraction by utilizing a combination of relation grouping, relation extraction, and label decision, thus mitigating the issues of large language models in capturing relation semantics and reducing computational overhead.

**Abstract:** For Relation Extraction (RE), the manual annotation of training data may be
prohibitively expensive, since the sentences that contain the target relations
in texts can be very scarce and difficult to find. It is therefore beneficial
to develop an efficient method that can automatically extract training
instances from unlabeled texts for training RE models. Recently, large language
models (LLMs) have been adopted in various natural language processing tasks,
with RE also benefiting from their advances. However, when leveraging LLMs for
RE with predefined relation categories, two key challenges arise. First, in a
multi-class classification setting, LLMs often struggle to comprehensively
capture the semantics of every relation, leading to suboptimal results. Second,
although employing binary classification for each relation individually can
mitigate this issue, it introduces significant computational overhead,
resulting in impractical time complexity for real-world applications.
Therefore, this paper proposes a framework called M-BRe to extract training
instances from unlabeled texts for RE. It utilizes three modules to combine the
advantages of both of the above classification approaches: Relation Grouping,
Relation Extraction, and Label Decision. Extensive experiments confirm its
superior capability in discovering high-quality training samples from unlabeled
texts for RE.

</details>


### [28] [Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for Medical Texts](https://arxiv.org/abs/2509.07755)
*Rochana Prih Hastuti,Rian Adam Rajagede,Mansour Al Ghanim,Mengxin Zheng,Qian Lou*

Main category: cs.CL

> 我们对现有水印方法在医学领域的有效性进行了评估，提出了FWS度量标准，发现当前水印方法极大损害了医学内容的事实准确性，强调了开发领域感知的水印策略的需求。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型在医学等敏感领域的应用，其流畅性带来了安全风险，尤其是在来源和责任方面。现有的基准测试侧重于检测质量和权衡，忽视了在低熵环境下因水印策略的重新加权策略而出现的事实风险。该研究旨在填补这一空白。

**Method:** 我们提出了一种以医学为重点的评估工作流程，该流程同时评估事实准确性与连贯性。使用GPT-Judger和进一步的人类验证，引入了事实加权得分（FWS），这是一个优先考虑事实准确性而非连贯性的复合度量，以指导医学领域的水印策略部署。

**Result:** 评估结果显示，目前的水印方法严重损害了医学的事实准确性，并且熵的变化影响了医学实体的表示。

**Conclusion:** 这些发现强调了开发领域感知的水印方法的需求，这些方法可以保持医学内容的完整性。

**Abstract:** As large language models (LLMs) adapted to sensitive domains such as
medicine, their fluency raises safety risks, particularly regarding provenance
and accountability. Watermarking embeds detectable patterns to mitigate these
risks, yet its reliability in medical contexts remains untested. Existing
benchmarks focus on detection-quality tradeoffs, overlooking factual risks
under low-entropy settings often exploited by watermarking's reweighting
strategy. We propose a medical-focused evaluation workflow that jointly
assesses factual accuracy and coherence. Using GPT-Judger and further human
validation, we introduce the Factuality-Weighted Score (FWS), a composite
metric prioritizing factual accuracy beyond coherence to guide watermarking
deployment in medical domains. Our evaluation shows current watermarking
methods substantially compromise medical factuality, with entropy shifts
degrading medical entity representation. These findings underscore the need for
domain-aware watermarking approaches that preserve the integrity of medical
content.

</details>


### [29] [Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content Detection? Evaluating In-Context Learning vs. Fine-Tuning](https://arxiv.org/abs/2509.07768)
*Michele Joshua Maggini,Dhia Merzougui,Rabiraj Bandyopadhyay,Gaël Dias,Fabrice Maurel,Pablo Gamallo*

Main category: cs.CL

> 研究对比了大型语言模型检测假新闻和偏见内容的各种适应策略，发现微调模型优于In-Context Learning。

<details>
  <summary>Details</summary>

**Motivation:** 在线平台上假新闻和有害内容传播的担忧加剧，LLM在该领域尚未得到全面评估。

**Method:** 通过参数效率的微调和多种In-Context Learning策略（包括零样本提示、编码本、少样本学习以及推理链）来检测假新闻、极化内容和政治偏见的性能对比。

**Result:** 研究发现In-Context Learning通常不如微调模型表现好，即使是最小的微调模型也优于最大的In-Context Learning模型。

**Conclusion:** 强调了在特定任务设置下微调模型的重要性，即便对于较小模型也是如此。

**Abstract:** The spread of fake news, polarizing, politically biased, and harmful content
on online platforms has been a serious concern. With large language models
becoming a promising approach, however, no study has properly benchmarked their
performance across different models, usage methods, and languages. This study
presents a comprehensive overview of different Large Language Models adaptation
paradigms for the detection of hyperpartisan and fake news, harmful tweets, and
political bias. Our experiments spanned 10 datasets and 5 different languages
(English, Spanish, Portuguese, Arabic and Bulgarian), covering both binary and
multiclass classification scenarios. We tested different strategies ranging
from parameter efficient Fine-Tuning of language models to a variety of
different In-Context Learning strategies and prompts. These included zero-shot
prompts, codebooks, few-shot (with both randomly-selected and
diversely-selected examples using Determinantal Point Processes), and
Chain-of-Thought. We discovered that In-Context Learning often underperforms
when compared to Fine-Tuning a model. This main finding highlights the
importance of Fine-Tuning even smaller models on task-specific settings even
when compared to the largest models evaluated in an In-Context Learning setup -
in our case LlaMA3.1-8b-Instruct, Mistral-Nemo-Instruct-2407 and
Qwen2.5-7B-Instruct.

</details>


### [30] [SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and Relation Extraction in NLP](https://arxiv.org/abs/2509.07801)
*Decheng Duan,Yingyi Zhang,Jitong Peng,Chengzhi Zhang*

Main category: cs.CL

> 该研究提出了一个用于NLP领域的全文实体和关系抽取数据集SciNLP，并验证了其在现有模型上的有效性，展示了更高的性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有数据集的局限性，本研究旨在开发一个全文本实体和关系抽取的数据集SciNLP，以提升NLP领域内的信息提取能力。

**Method:** 从科学文献中提取结构化信息对捕捉各个专业领域中的核心概念和新兴趋势至关重要。尽管现有的数据集有助于模型的开发，但由于专业知识的复杂性和注释科学文本的高成本，大多数数据集关注特定的出版物部分。为了解决这个问题，我们介绍了 SciNLP - 一个专门用于NLP领域全文实体和关系提取的基准。该数据集包括60篇手动注释的NLP全文出版物，涵盖了7,072个实体和1,826种关系。与现有的研究相比，SciNLP 是第一个提供 NLP 领域全文实体及其关系注释的数据集。为了验证 SciNLP 的有效性，我们进行了与类似数据集的对比实验，并评估了最先进的监督模型在这个数据集上的表现。结果揭示了现有模型在不同长度的学术文本中实体提取能力的变化。

**Result:** 与现有数据集的交叉比较表明，SciNLP 对于一些基础模型实现了显著的性能改进。使用在 SciNLP 上训练的模型实现了NLP领域的细粒度知识图谱的自动构建。我们的KG每个实体的平均节点度为3.2，表明富含语义拓扑信息，能够增强下游应用。

**Conclusion:** 本文介绍了一个新型数据集 SciNLP，用于NLP领域的全文实体和关系提取，展示了其在模型性能上的改进，并用于构建了一个细粒度的知识图谱。该数据集可公开获取。

**Abstract:** Structured information extraction from scientific literature is crucial for
capturing core concepts and emerging trends in specialized fields. While
existing datasets aid model development, most focus on specific publication
sections due to domain complexity and the high cost of annotating scientific
texts. To address this limitation, we introduce SciNLP - a specialized
benchmark for full-text entity and relation extraction in the Natural Language
Processing (NLP) domain. The dataset comprises 60 manually annotated full-text
NLP publications, covering 7,072 entities and 1,826 relations. Compared to
existing research, SciNLP is the first dataset providing full-text annotations
of entities and their relationships in the NLP domain. To validate the
effectiveness of SciNLP, we conducted comparative experiments with similar
datasets and evaluated the performance of state-of-the-art supervised models on
this dataset. Results reveal varying extraction capabilities of existing models
across academic texts of different lengths. Cross-comparisons with existing
datasets show that SciNLP achieves significant performance improvements on
certain baseline models. Using models trained on SciNLP, we implemented
automatic construction of a fine-grained knowledge graph for the NLP domain.
Our KG has an average node degree of 3.2 per entity, indicating rich semantic
topological information that enhances downstream applications. The dataset is
publicly available at https://github.com/AKADDC/SciNLP.

</details>


### [31] [Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems](https://arxiv.org/abs/2509.07817)
*Xiaolin Chen,Xuemeng Song,Haokun Wen,Weili Guan,Xiangyu Zhao,Liqiang Nie*

Main category: cs.CL

> 本文提出了一种新型的双知识增强两阶段推理器DK2R，旨在利用结构化属性和非结构化评论知识以及大型语言模型(LLMs)来改进多模态任务导向对话系统中的文本响应生成。

<details>
  <summary>Details</summary>

**Motivation:** 通过利用大型语言模型(LLMs)与结构化和非结构化知识相结合的技术来改进多模态任务导向对话系统中的文本响应生成，应对动态知识类型选择和意图-响应解耦两大挑战。

**Method:** DK2R方法首先从外部知识库中提取结构化属性和非结构化评论知识，随后使用LLMs评估每种知识的效用，并通过专门的推理总结出意图导向的关键线索来加强基于LLMs的文本响应生成。

**Result:** 在公开数据集上进行的大量实验验证了DK2R的优越性。

**Conclusion:** 本文通过DK2R方法证实了在多模态任务导向对话系统中利用双知识和大型语言模型来改进文本响应生成的有效性。

**Abstract:** Textual response generation is pivotal for multimodal \mbox{task-oriented}
dialog systems, which aims to generate proper textual responses based on the
multimodal context. While existing efforts have demonstrated remarkable
progress, there still exist the following limitations: 1) \textit{neglect of
unstructured review knowledge} and 2) \textit{underutilization of large
language models (LLMs)}. Inspired by this, we aim to fully utilize dual
knowledge (\textit{i.e., } structured attribute and unstructured review
knowledge) with LLMs to promote textual response generation in multimodal
task-oriented dialog systems. However, this task is non-trivial due to two key
challenges: 1) \textit{dynamic knowledge type selection} and 2)
\textit{intention-response decoupling}. To address these challenges, we propose
a novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for
multimodal dialog systems (named DK2R). To be specific, DK2R first extracts
both structured attribute and unstructured review knowledge from external
knowledge base given the dialog context. Thereafter, DK2R uses an LLM to
evaluate each knowledge type's utility by analyzing LLM-generated provisional
probe responses. Moreover, DK2R separately summarizes the intention-oriented
key clues via dedicated reasoning, which are further used as auxiliary signals
to enhance LLM-based textual response generation. Extensive experiments
conducted on a public dataset verify the superiority of DK2R. We have released
the codes and parameters.

</details>


### [32] [Small Open Models Achieve Near Parity with Large Models in Low Resource Literary Translation at a Fraction of the Cost](https://arxiv.org/abs/2509.07829)
*Mihai Nadas,Laura Diosan,Andreea Tomescu,Andrei Piscoran*

Main category: cs.CL

> TF2 是一个专注于英罗文学翻译的统一框架，包含小规模模型微调和大规模合成平行语料库的创建、评估。它通过两阶段微调过程（指令牌调整和适配器压缩）来实现高效部署，并且在成本效率、开放性和可访问性方面具有优势，同时确保了翻译质量。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于文学翻译在机器翻译研究中的独特性和复杂性，特别是在小型开放模型中的翻译问题尚未解决，该研究旨在提供一种解决方案，同时满足罗语这种低资源语言高质量文学数据的需求

**Method:** TF2 框架通过两个阶段微调过程建立：第一步，使用高性能LLM从TF1生成高质量罗语参考文献；第二步，对12B参数开放权重模型进行指令调微和适配器压缩，以捕捉特定体裁的叙事风格，并实现高效部署

**Result:** 研究结果表明，经过微调的TF2-12B模型在流畅性和准确性方面可与顶尖的大规模专有模型相媲美，并且保持了开放性和成本效益

**Conclusion:** TF2 提供了高效翻译研究及跨语言叙事生成的端到端可重复管线。此研究推动了开放模型在低资源情境下对文化意义重大的文学内容的广泛采用

**Abstract:** Literary translation has recently gained attention as a distinct and complex
task in machine translation research. However, the translation by small open
models remains an open problem. We contribute to this ongoing research by
introducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for
dataset creation, fine tuning, and evaluation in English-Romanian literary
translations, centred on the creation and open release of both a compact, fine
tuned language model (TF2-12B) and large scale synthetic parallel datasets
(DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the
largest collection of synthetic English fables to date, we address the need for
rich, high quality literary datasets in low resource languages such as
Romanian. Our pipeline first generates 15k high quality Romanian references
from the TF1 pool using a high performing LLM. We then apply a two stage fine
tuning process to a 12B parameter open weight model: (i) instruction tuning to
capture genre specific narrative style, and (ii) adapter compression for
efficient deployment. Evaluation combines corpus level BLEU and a five
dimension LLM based rubric (accuracy, fluency, coherence, style, cultural
adaptation) to provide a nuanced assessment of translation quality. Results
show that our fine tuned model achieves fluency and adequacy competitive with
top performing large proprietary models, while being open, accessible, and
significantly more cost effective. Alongside the fine tuned model and both
datasets, we publicly release all scripts and evaluation prompts. TF2 thus
provides an end-to-end, reproducible pipeline for research on cost efficient
translation, cross lingual narrative generation, and the broad adoption of open
models for culturally significant literary content in low resource settings.

</details>


### [33] [Are Humans as Brittle as Large Language Models?](https://arxiv.org/abs/2509.07869)
*Jiahui Li,Sean Papay,Roman Klinger*

Main category: cs.CL

> 研究对比了人类注释员和大语言模型(LLM)在不同提示词变化下的表现，发现在某些提示词修改下(如标签集或标签格式的替换)，两者都表现出较高的脆弱性，但在打字错误和标签顺序相反方面，人类的表现受到的影响较小。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在解决LLM因提示词的微妙变化而导致输出不稳定的问题，探讨这种脆弱性是否独有于LLM，还是也为人类注释员所共有。

**Method:** 本研究通过系统对比LLM和人类注释员在面对提示词变化时的反应，尤其是在文本分类任务中，来探讨LLM所表现出的提示词脆弱性是否也存在于人类注释员中。

**Result:** 研究发现，对于特定类型的提示词修改，如标签集或标签格式的替换，人类和LLM都展示出较高的脆弱性。但在面对打字错误和标签顺序反转时，人类注释员受到的影响比LLM小。

**Conclusion:** 人类和LLM在面对某些提示词修改时表现出相似的敏感性，但在其他类型的修改上则存在差异。这表明LLM的提示词脆弱性并不必然问题化，可能在某些情况下反映了人类注释的变异性。

**Abstract:** The output of large language models (LLM) is unstable, due to both
non-determinism of the decoding process as well as to prompt brittleness. While
the intrinsic non-determinism of LLM generation may mimic existing uncertainty
in human annotations through distributional shifts in outputs, it is largely
assumed, yet unexplored, that the prompt brittleness effect is unique to LLMs.
This raises the question: do human annotators show similar sensitivity to
instruction changes? If so, should prompt brittleness in LLMs be considered
problematic? One may alternatively hypothesize that prompt brittleness
correctly reflects human annotation variances. To fill this research gap, we
systematically compare the effects of prompt modifications on LLMs and
identical instruction modifications for human annotators, focusing on the
question of whether humans are similarly sensitive to prompt perturbations. To
study this, we prompt both humans and LLMs for a set of text classification
tasks conditioned on prompt variations. Our findings indicate that both humans
and LLMs exhibit increased brittleness in response to specific types of prompt
modifications, particularly those involving the substitution of alternative
label sets or label formats. However, the distribution of human judgments is
less affected by typographical errors and reversed label order than that of
LLMs.

</details>


### [34] [From Detection to Mitigation: Addressing Gender Bias in Chinese Texts via Efficient Tuning and Voting-Based Rebalancing](https://arxiv.org/abs/2509.07889)
*Chengyan Wu,Yiqiang Cai,Yufei Cheng,Yun Xue*

Main category: cs.CL

> 这篇论文提出了针对汉语句子层面性别偏见检测和缓解的任务解决方案，使用大型语言模型的微调技术，并取得了较佳的实验结果。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在通过自动检测、分类和缓解性别偏见来促进自然语言生成中的公平性和可控性。

**Method:** 该论文采用基于大型语言模型的微调方法，并使用低秩适配（LoRA）来高效适应偏见检测任务。在数据处理方面，构建了一个更平衡的训练集，并引入了来自多个来源的异构样本。针对检测和分类子任务，采用多数投票策略结合多个专家模型的输出以提升性能。为了改进偏见生成检测和缓解，设计了一种多温度采样机制。

**Result:** 实验结果显示该方法在偏见检测、分类和缓解方面有效，最终获得47.90%的平均分数，在共享任务中排名第四。

**Conclusion:** 实验结果证明了所提方法的有效性，尽管排名第四，但仍为性别偏见检测和缓解领域的进一步研究提供了有价值的参考。

**Abstract:** This paper presents our team's solution to Shared Task 7 of NLPCC-2025, which
focuses on sentence-level gender bias detection and mitigation in Chinese. The
task aims to promote fairness and controllability in natural language
generation by automatically detecting, classifying, and mitigating gender bias.
To address this challenge, we adopt a fine-tuning approach based on large
language models (LLMs), efficiently adapt to the bias detection task via
Low-Rank Adaptation (LoRA). In terms of data processing, we construct a more
balanced training set to alleviate class imbalance and introduce heterogeneous
samples from multiple sources to enhance model generalization. For the
detection and classification sub-tasks, we employ a majority voting strategy
that integrates outputs from multiple expert models to boost performance.
Additionally, to improve bias generation detection and mitigation, we design a
multi-temperature sampling mechanism to capture potential variations in bias
expression styles. Experimental results demonstrate the effectiveness of our
approach in bias detection, classification, and mitigation. Our method
ultimately achieves an average score of 47.90%, ranking fourth in the shared
task.

</details>


### [35] [Biased Tales: Cultural and Topic Bias in Generating Children's Stories](https://arxiv.org/abs/2509.07908)
*Donya Rooein,Vilém Zouhar,Debora Nozza,Dirk Hovy*

Main category: cs.CL

> 本研究建立了一个名为Biased Tales的数据集，用于分析大型语言模型生成的故事中的偏见对主人公属性和故事元素的影响，揭示了性别和文化背景对故事叙述的显著影响。

<details>
  <summary>Details</summary>

**Motivation:** 随着家长越来越多地依靠大型语言模型来创作睡前故事，故事中文化及性别刻板印象的存在引发了重要关切。

**Method:** 创建名为Biased Tales的数据集，用于分析讲大型语言模型生成的故事中，性别和文化背景对主人公属性和故事元素的影响。

**Result:** 分析发现，当主人公是女孩时，与其外貌相关的属性增加了55.26%；故事中涉及非西方儿童时，文化传承、传统和家庭主题的比例远高于西方儿童。

**Conclusion:** 研究结果强调，要使创意AI的使用更为公平和多样性，必须认识到社会文化偏见的作用。

**Abstract:** Stories play a pivotal role in human communication, shaping beliefs and
morals, particularly in children. As parents increasingly rely on large
language models (LLMs) to craft bedtime stories, the presence of cultural and
gender stereotypes in these narratives raises significant concerns. To address
this issue, we present Biased Tales, a comprehensive dataset designed to
analyze how biases influence protagonists' attributes and story elements in
LLM-generated stories. Our analysis uncovers striking disparities. When the
protagonist is described as a girl (as compared to a boy), appearance-related
attributes increase by 55.26%. Stories featuring non-Western children
disproportionately emphasize cultural heritage, tradition, and family themes
far more than those for Western children. Our findings highlight the role of
sociocultural bias in making creative AI use more equitable and diverse.

</details>


### [36] [GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models](https://arxiv.org/abs/2509.07925)
*Tuo Wang,Adithya Kulkarni,Tyler Cody,Peter A. Beling,Yujun Yan,Dawei Zhou*

Main category: cs.CL

> 本文提出了GENUINE，一种增强的多级不确定性估计框架，用于提高大型语言模型的可靠性，特别是在高风险应用中。通过使用依赖解析树和层次图汇聚，该方法显著提高了不确定性评估的效果。

<details>
  <summary>Details</summary>

**Motivation:** 现有的不确定性估计方法往往忽视了语义依赖关系，仅依赖于无法捕捉文本生成中结构关系的token级概率测量。为了增强高风险领域中大型语言模型的可靠性，提出了一种结构感知框架。

**Method:** GENUINE: 使用依赖解析树和层次图汇聚来提高大型语言模型的不确定性量化，通过监督学习建模语义和结构关系，以改进置信度评估。

**Result:** 实验结果显示，与基于语义熵的方法相比，GENUINE的AUROC提高了最多29%，并降低了超过15%的校准误差，证明了基于图的不确定性建模的有效性。

**Conclusion:** 实验证明，GENUINE通过改进的不确定性量化方法，能够有效提升大型语言模型在自然语言处理任务中的表现，特别是在可靠性和校准准确性方面。

**Abstract:** Uncertainty estimation is essential for enhancing the reliability of Large
Language Models (LLMs), particularly in high-stakes applications. Existing
methods often overlook semantic dependencies, relying on token-level
probability measures that fail to capture structural relationships within the
generated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty
Estimation for Large Language Models, a structure-aware framework that
leverages dependency parse trees and hierarchical graph pooling to refine
uncertainty quantification. By incorporating supervised learning, GENUINE
effectively models semantic and structural relationships, improving confidence
assessments. Extensive experiments across NLP tasks show that GENUINE achieves
up to 29% higher AUROC than semantic entropy-based approaches and reduces
calibration errors by over 15%, demonstrating the effectiveness of graph-based
uncertainty modeling. The code is available at
https://github.com/ODYSSEYWT/GUQ.

</details>


### [37] [SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge](https://arxiv.org/abs/2509.07968)
*Lukas Haas,Gal Yona,Giovanni D'Antonio,Sasha Goldshtein,Dipanjan Das*

Main category: cs.CL

> 提出了SimpleQA Verified，一个更准确的基准用于评估LLM的短格式准确性，解决了以往基准中的问题，并由Gemini 2.5 Pro实现了新的SOTA结果。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在提供一个更加精确和无偏差的基准测试工具，用于衡量大型语言模型的短格式事实准确性。

**Method:** 通过严格的多阶段过程创建了SimpleQA Verified，包括去重、主题平衡和来源整合。此外，还改进了自动评分提示。

**Result:** 该论文介绍了SimpleQA Verified，这是一个用于评估大规模语言模型（LLM）在短格式准确性方面的基准，包含1000个提示。它解决了OpenAI SimpleQA基准中存在的问题，如标签噪声、不正确的标签、主题偏差和问题冗余。SimpleQA Verified是通过严格的多阶段过滤过程创建的，包括去重、主题平衡和来源整合，以产生更可靠和更具挑战性的评估集，同时改进了自动评分提示。在这个新基准上，Gemini 2.5 Pro实现了55.6的F1分数，超过了其他前沿模型，包括GPT-5。这项工作为研究人员提供了一个更精确的工具，用于跟踪参数模型准确性的真正进展并减轻幻觉问题。基准数据集、评估代码和排行榜可以在https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified找到。

**Conclusion:** Gemini 2.5 Pro在SimpleQA Verified基准上获得了优秀的F1分数，这表明SimpleQA Verified能提供一个更高保真度的工具，用以追踪参数模型事实准确性的真正进步，并减少幻觉问题。

**Abstract:** We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large
Language Model (LLM) short-form factuality based on OpenAI's SimpleQA. It
addresses critical limitations in OpenAI's benchmark, including noisy and
incorrect labels, topical biases, and question redundancy. SimpleQA Verified
was created through a rigorous multi-stage filtering process involving
de-duplication, topic balancing, and source reconciliation to produce a more
reliable and challenging evaluation set, alongside improvements in the
autorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a
state-of-the-art F1-score of 55.6, outperforming other frontier models,
including GPT-5. This work provides the research community with a
higher-fidelity tool to track genuine progress in parametric model factuality
and to mitigate hallucinations. The benchmark dataset, evaluation code, and
leaderboard are available at:
https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [38] [CellPainTR: Generalizable Representation Learning for Cross-Dataset Cell Painting Analysis](https://arxiv.org/abs/2509.06986)
*Cedric Caruzzo,Jong Chul Ye*

Main category: cs.CV

> 研究人员提出了一种名为CellPainTR的基于Transformer的架构，旨在解决大规模生物数据集整合的挑战。这种方法在多个任务中表现出色，标志着图像基础性分析模型的一个重要进展。

<details>
  <summary>Details</summary>

**Motivation:** 大规模的生物发现需要整合大规模的异构数据集，如JUMP细胞绘画联盟的数据集，但技术批次效应和缺乏通用性模型仍然是关键障碍。为了应对这些问题，研究人员提出了一种新型的方法。

**Method:** CellPainTR, 一种基于Transformer的架构，旨在学习细胞形态的基础表示，这些表示能够抵御批次效应。该方法通过引入源特定上下文令牌，实现了对未见数据集的有效泛化，并且无需再调优。

**Result:** 在大规模的JUMP数据集上验证了CellPainTR，结果表明，它在批次整合和生物学信号保持方面都超越了已建立的方法如ComBat和Harmony。此外，在具有显著领域和特征转换的未见Bray等数据集上，CellPainTR也表现出强大的性能和健壮性。

**Conclusion:** 这项研究表明，CellPainTR在大规模图像基生物测定分析中，成为了基础模型，实现了跨研究分析的可靠性和可扩展性。

**Abstract:** Large-scale biological discovery requires integrating massive, heterogeneous
datasets like those from the JUMP Cell Painting consortium, but technical batch
effects and a lack of generalizable models remain critical roadblocks. To
address this, we introduce CellPainTR, a Transformer-based architecture
designed to learn foundational representations of cellular morphology that are
robust to batch effects. Unlike traditional methods that require retraining on
new data, CellPainTR's design, featuring source-specific context tokens, allows
for effective out-of-distribution (OOD) generalization to entirely unseen
datasets without fine-tuning. We validate CellPainTR on the large-scale JUMP
dataset, where it outperforms established methods like ComBat and Harmony in
both batch integration and biological signal preservation. Critically, we
demonstrate its robustness through a challenging OOD task on the unseen Bray et
al. dataset, where it maintains high performance despite significant domain and
feature shifts. Our work represents a significant step towards creating truly
foundational models for image-based profiling, enabling more reliable and
scalable cross-study biological analysis.

</details>


### [39] [FusWay: Multimodal hybrid fusion approach. Application to Railway Defect Detection](https://arxiv.org/abs/2509.06987)
*Alexey Zhukov,Jenny Benois-Pineau,Amira Youssef,Akka Zemmari,Mohamed Mosbah,Virginie Taillandier*

Main category: cs.CV

> A new multimodal fusion architecture, integrating YOLO and Vision Transformer with audio signals, is proposed for rail and surface defect detection. This method significantly enhances detection precision and accuracy compared to a vision-only approach.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to overcome the limitations of single modality defect detection methods which often result in overdetection due to similarities with normal structural elements. The multimodal approach aims to enhance precision and accuracy.

**Method:** The paper proposes a new multimodal fusion architecture that combines YOLOv8n for rapid object detection and Vision Transformer (ViT) for feature extraction from multiple layers, integrating both with audio signals to detect rail and surface defects.

**Result:** The multimodal fusion method improves precision and overall accuracy by 0.2 points over the vision-only approach. Student's unpaired t-test confirms the statistical significance of the accuracy improvements on a real-world railway dataset.

**Conclusion:** The experimental results demonstrate that the proposed multimodal fusion approach effectively enhances detection precision and accuracy for rail defects, supporting its superior performance over single modality methods.

**Abstract:** Multimodal fusion is a multimedia technique that has become popular in the
wide range of tasks where image information is accompanied by a signal/audio.
The latter may not convey highly semantic information, such as speech or music,
but some measures such as audio signal recorded by mics in the goal to detect
rail structure elements or defects. While classical detection approaches such
as You Only Look Once (YOLO) family detectors can be efficiently deployed for
defect detection on the image modality, the single modality approaches remain
limited. They yield an overdetection in case of the appearance similar to
normal structural elements. The paper proposes a new multimodal fusion
architecture built on the basis of domain rules with YOLO and Vision
transformer backbones. It integrates YOLOv8n for rapid object detection with a
Vision Transformer (ViT) to combine feature maps extracted from multiple layers
(7, 16, and 19) and synthesised audio representations for two defect classes:
rail Rupture and Surface defect. Fusion is performed between audio and image.
Experimental evaluation on a real-world railway dataset demonstrates that our
multimodal fusion improves precision and overall accuracy by 0.2 points
compared to the vision-only approach. Student's unpaired t-test also confirms
statistical significance of differences in the mean accuracy.

</details>


### [40] [Frustratingly Easy Feature Reconstruction for Out-of-Distribution Detection](https://arxiv.org/abs/2509.06988)
*Yingsheng Wang,Shuo Lu,Jian Liang,Aihua Zheng,Ran He*

Main category: cs.CV

> 提出了Classifier-based Feature Reconstruction (ClaFR)，通过子空间投影的方法，在不使用训练数据的情况下，实现高效的OOD检测。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于特征的后验方法虽然有效，但通常需要访问训练数据，这在需要保护数据隐私的场景下可能不适合。

**Method:** ClaFR首先对分类器的权重进行正交分解以提取已知类子空间，然后将原始数据特征映射到该子空间中获得新的数据表示，最后通过计算数据在子空间内的特征重构误差来确定OOD分数。

**Result:** 与现有的OOD检测算法相比，本方法在多个OOD基准上实现了领先性能，且不需要访问训练数据。

**Conclusion:** 该方法不仅简单有效，而且易于理解和应用，对数据隐私保护具有重要意义。

**Abstract:** Out-of-distribution (OOD) detection helps models identify data outside the
training categories, crucial for security applications. While feature-based
post-hoc methods address this by evaluating data differences in the feature
space without changing network parameters, they often require access to
training data, which may not be suitable for some data privacy scenarios. This
may not be suitable in scenarios where data privacy protection is a concern. In
this paper, we propose a simple yet effective post-hoc method, termed
Classifier-based Feature Reconstruction (ClaFR), from the perspective of
subspace projection. It first performs an orthogonal decomposition of the
classifier's weights to extract the class-known subspace, then maps the
original data features into this subspace to obtain new data representations.
Subsequently, the OOD score is determined by calculating the feature
reconstruction error of the data within the subspace. Compared to existing OOD
detection algorithms, our method does not require access to training data while
achieving leading performance on multiple OOD benchmarks. Our code is released
at https://github.com/Aie0923/ClaFR.

</details>


### [41] [DIET-CP: Lightweight and Data Efficient Self Supervised Continued Pretraining](https://arxiv.org/abs/2509.06990)
*Bryan Rodas,Natalie Montesino,Jakob Ambsdorf,David Klindt,Randall Balestriero*

Main category: cs.CV

> 文章提出DIET-CP，一种适合小数据集的持续预训练策略，不需要标签，并且只比监督微调多使用一个超参数，可以显著提升模型在特定数据集上的性能。

<details>
  <summary>Details</summary>

**Motivation:** 持续预训练作为一种将基础模型适应新目标域的有前景的方法，但在特定领域可用的数据集通常非常小，导致无法应用大规模预训练的自监督方法，也使得超参数搜索不可行。此外，预训练模型通常是仅以主干权重的形式发布，缺乏持续预训练的重要信息。

**Method:** DIET-CP, 一种简单的持续预训练策略，可引导任何强大的基础模型适应新的数据分布。DIET-CP依赖于一个非常简单的目标，不需要标签，并且除了监督微调之外不引入任何超参数。

**Result:** DIET-CP在不同的数据模态和主干选择上都稳定，仅使用1000张图像即可为最先进的模型，如DINOv3，提供显著的性能提升。

**Conclusion:** DIET-CP提供了一种解决小规模数据集和超参数搜索挑战的方法，有助于提升模型在特定领域的适应性和性能。

**Abstract:** Continued pretraining offers a promising solution for adapting foundation
models to a new target domain. However, in specialized domains, available
datasets are often very small, limiting the applicability of SSL methods
developed for large-scale pretraining and making hyperparameter search
infeasible. In addition, pretrained models are usually released as
backbone-weights only, lacking important information to continue pretraining.
We propose to bridge this gap with DIET-CP, a simple continued pretraining
strategy, where any strong foundation model can be steered towards the new data
distribution of interest. DIET-CP relies on a very simple objective, requires
no labels, and introduces no more hyperparameters than supervised finetuning.
It is stable across data modalities and backbone choices, while providing a
significant performance boost for state-of-the-art models such as DINOv3 using
only 1000 images.

</details>


### [42] [FedAPT: Federated Adversarial Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2509.06992)
*Kun Zhai,Siheng Chen,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

> 本文提出了一种名为FedAPT的新方法，旨在提高联邦提示调整（FPT）的对抗鲁棒性。通过引入一个基于全局标签嵌入的类别感知提示生成器和跨层生成器共享策略，FedAPT解决了在非独立同分布设置下的类别信息差距。实验结果表明，在多个图像分类数据集上，FedAPT显着提升对抗鲁棒性，表现优于现有方法，并且在跨域和跨数据集应用中展现出优秀的泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** Federated Prompt Tuning (FPT)方法虽然在跨客户端协作微调视觉语言模型时高效，但其模型容易受到对抗攻击，导致下游任务的误分类。为解决这一问题，本文提出FedAPT，旨在增强FPT模型的对抗鲁棒性。

**Method:** 本文提出了一种名为FedAPT的对抗鲁棒性提升方法。FedAPT包括一个类别感知的提示生成器，它基于全局标签嵌入创建更全局一致的视觉提示，并通过跨层生成器共享策略来增强不同模型层之间的提示耦合。

**Result:** 实验结果表明，FedAPT在多个图像分类数据集上表现出对抗鲁棒性的显著提高，优于现有方法。此外，它在跨域和跨数据集场景中的泛化能力也非常出色。

**Conclusion:** 本文提出FedAPT方法在解决非独立同分布场景下的类别信息差距和提高FPT模型对抗鲁棒性方面取得了好的成效。实验结果验证了FedAPT的优越性，尤其是在跨域和跨数据集情况下的泛化性能。

**Abstract:** Federated Prompt Tuning (FPT) is an efficient method for cross-client
collaborative fine-tuning of large Vision-Language Models (VLMs). However,
models tuned using FPT are vulnerable to adversarial attacks, leading to
misclassification in downstream tasks. In this work, we introduce Federated
Adversarial Prompt Tuning (\textbf{FedAPT}), a novel method designed to enhance
the adversarial robustness of FPT. We identify a key issue in FedAPT under
non-independent and identically distributed (non-IID) settings: a \textit{class
information gap} between clients and the global model. Clients rely solely on
limited local label information to generate adversarial samples for training,
while the global model must defend against adversarial attacks from global
labels. To address this issue, we propose a \textbf{class-aware prompt
generator} that generates visual prompts from text prompts. This generator is
guided by a \emph{Global Label Embedding} (serving as a ``beacon") which
encodes cross-client label information to create more globally-aligned visual
prompts. Additionally, we propose a \textbf{cross-layer generator sharing}
strategy to enhance prompt coupling across different layers of the model,
further boosting adversarial robustness. Extensive experiments on multiple
image classification datasets demonstrate the superiority of FedAPT in
improving adversarial robustness, outperforming existing methods by a large
margin. FedAPT also exhibits exceptional generalization in cross-domain and
cross-dataset scenarios, indicating its effectiveness in real-world
applications.

</details>


### [43] [Geospatial Foundational Embedder: Top-1 Winning Solution on EarthVision Embed2Scale Challenge (CVPR 2025)](https://arxiv.org/abs/2509.06993)
*Zirui Xu,Raphael Tang,Mike Bianco,Qi Zhang,Rishi Madhok,Nikolaos Karianakis,Fuxun Yu*

Main category: cs.CV

> 介绍了在Embed2Scale竞赛中获得第一名的基础地理空间模型解决方案的描述，竞赛目的是利用高光谱数据支持多种后端任务。

<details>
  <summary>Details</summary>

**Motivation:** 竞赛的动机是促进基础地理空间模型的发展，这些模型能够对高光谱地理空间数据进行有效嵌入，以支持如分类和回归等各种后端任务。

**Method:** 由于缺乏具体方法描述，无法提供。

**Result:** 由于提供的内容仅为竞赛挑战的背景描述，并没有包含具体的技术细节、方法和结果，所以在没有更多具体内容的情况下，无法生成完整的分析摘要。不过，根据提供的信息，可以初步概括：挑战旨在利用SSL4EO-S12高光谱地理空间数据立方体，建立能够支持多种下游任务（如分类、回归等）的基础地理空间模型。报告中介绍了在Embed2Scale竞赛中获得第一名的解决方案。

**Conclusion:** 由于缺乏结论信息，无法提供。

**Abstract:** EarthVision Embed2Scale challenge (CVPR 2025) aims to develop foundational
geospatial models to embed SSL4EO-S12 hyperspectral geospatial data cubes into
embedding vectors that faciliatetes various downstream tasks, e.g.,
classification, regression, etc. In this technical report, we introduce our
proposed method for the Top-1 winning solution on the Embed2Scale Challenge.

</details>


### [44] [VLMs-in-the-Wild: Bridging the Gap Between Academic Benchmarks and Enterprise Reality](https://arxiv.org/abs/2509.06994)
*Srihari Bandraupalli,Anupam Purwar*

Main category: cs.CV

> 本文介绍了VLM-in-the-Wild（ViLD）框架，该框架通过对实际企业需求的验证来评估开放语言视觉模型。通过引入创新的BlockWeaver算法，解决了无序文本比较的问题，并通过构建7500个样本的新基准数据集，为领先开源VL模型的实际应用提供了见解。

<details>
  <summary>Details</summary>

**Motivation:** 当前基准测试主要依赖于多项选择题和合成数据，无法捕捉到例如社交媒体内容分析等实际应用的复杂性。这导致了学术评估和企业部署需求之间的差距。

**Method:** ViLD框架定义了十个业务关键任务，并引入了创新的BlockWeaver算法以解决不同输出间的比较问题。创建了一个包含7500个样本的新基准数据集，用于评估视觉语言模型的表现。

**Result:** 通过对领先的开源视觉语言模型进行评估，该框架提供了有关这些模型在企业环境中部署的实际能力的动作建议。

**Conclusion:** ViLD框架为开放语言视觉模型的能力提供了基于行业内实操需求的评估，为这些模型在企业的部署提供了可行的见解。

**Abstract:** Open-source Vision-Language Models show immense promise for enterprise
applications, yet a critical disconnect exists between academic evaluation and
enterprise deployment requirements. Current benchmarks rely heavily on
multiple-choice questions and synthetic data, failing to capture the complexity
of real-world business applications like social media content analysis. This
paper introduces VLM-in-the-Wild (ViLD), a comprehensive framework to bridge
this gap by evaluating VLMs on operational enterprise requirements. We define
ten business-critical tasks: logo detection, OCR, object detection, human
presence and demographic analysis, human activity and appearance analysis,
scene detection, camera perspective and media quality assessment, dominant
colors, comprehensive description, and NSFW detection. To this framework, we
bring an innovative BlockWeaver Algorithm that solves the challenging problem
of comparing unordered, variably-grouped OCR outputs from VLMs without relying
on embeddings or LLMs, achieving remarkable speed and reliability. To
demonstrate efficacy of ViLD, we constructed a new benchmark dataset of 7,500
diverse samples, carefully stratified from a corpus of one million real-world
images and videos. ViLD provides actionable insights by combining semantic
matching (both embedding-based and LLM-as-a-judge approaches), traditional
metrics, and novel methods to measure the completeness and faithfulness of
descriptive outputs. By benchmarking leading open-source VLMs (Qwen, MIMO, and
InternVL) against a powerful proprietary baseline as per ViLD framework, we
provide one of the first industry-grounded, task-driven assessment of VLMs
capabilities, offering actionable insights for their deployment in enterprise
environments.

</details>


### [45] [The Protocol Genome A Self Supervised Learning Framework from DICOM Headers](https://arxiv.org/abs/2509.06995)
*Jimmy Joseph*

Main category: cs.CV

> Protocol Genome是一种通过学习DICOM标题来改进影像分析自监督学习系统，提高了临床影像任务的性能和校准，并减少了误报。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决由扫描器型号、序列、核函数等参数引起的潜在混杂因素，这些因素限制了仅基于图像网络的跨站点泛化能力。希望通过引入结构化DICOM标题作为标签来改善这种状况。

**Method:** Protocol Genome通过对结构化DICOM标题的嵌入建模来学习协议感知但临床鲁棒的图像表示。技术方法包括协议-图像对比学习、屏蔽协议预测和协议-协议翻译。

**Result:** 该论文提出了Protocol Genome，一种自监督学习系统，通过学习DICOM标题中的关联性，实现了在完全独立的外部验证上的AUROC 0.901（对比基线0.847）和ECE 0.036（对比基线0.058）。该方法改进了跨模态（CT、MRI、CXR）和供应商的校准和鲁棒性。研究中采用了包含1.26百万项研究的数据集，涵盖7个健康系统、31个扫描仪和3个供应商。实验结果表明，相较于强大的自监督学习基线和ImageNet迁移，Protocol Genome在三种不同的临床影像任务中均显示出更高的外部AUROC、以及显著改善的校准；在协议边界处减少了误报。这项技术适用于PACS，可通过DICOM C-FIND/C-MOVE和DICOMweb QIDO/WADO来部署。该系统还需进行去标识化和偏见审核，并发布了模型卡和部署指南。

**Conclusion:** Protocol Genome除了在多种临床影像任务中提供了更高的外部AUROC外，还显著改善了校准性能，并减少了误报。该技术对不同任务的增益保持颇为稳健，即使仅使用少量标注数据。

**Abstract:** In this paper, we introduce the Protocol Genome, a self-supervised learning
system that learns correlations from DICOM headers and achieves AUROC 0.901 (vs
0.847 baseline) and ECE 0.036 (vs 0.058) on fully held-out external validation.
Our method also improves calibration and robustness across modalities (CT, MRI,
CXR) and vendors. Clinical imaging is funneled through PACS/DICOM, where
procedure choices (scanner make/model, sequence, kernel, kVp, TR/TE, and slice
thickness) have consequences for contrast, noise, and artifact. These latent
confounders impede the generalization of image-only networks across sites. We
consider structured DICOM headers as a label and learn protocol-aware but
clinically robust image representations. Protocol Genome obtains tokenized
embeddings of de-identified header fields and models them along with image
features using: (1) protocol-image contrastive learning, (2) masked protocol
prediction, and (3) protocol-protocol translation. With 1.26M studies (7 health
systems, 31 scanners, 3 vendors; CT, MR, CR/DR), we experiment on: (A) chest CT
triage for PE, (B) brain MRI glioma grading, and (C) chest radiograph
cardiomegaly detection. Relative to strong SSL baselines (SimCLR, MAE) as well
as ImageNet transfer, Protocol Genome (+0.046: PE, +0.058: glioma, +0.041:
cardiomegaly) is associated with higher external AUROC; 25-37% calibration
improvements are obtained (p < 0.01, DeLong tests). While the gains may be
task-dependent, they are preserved with 10-20% of labeled data. From a clinical
point of view, the technique reduces false positives at protocol borders and is
applicable in a PACS (DICOM C-FIND/C-MOVE, DICOMweb QIDO/WADO). We publish a
model card and deployment guide, complete with both de-identification and bias
audits.

</details>


### [46] [Visible Yet Unreadable: A Systematic Blind Spot of Vision Language Models Across Writing Systems](https://arxiv.org/abs/2509.06996)
*Jie Zhang,Ting Xu,Gelei Deng,Runyi Hu,Han Qiu,Tianwei Zhang,Qing Guo,Ivor Tsang*

Main category: cs.CV

> 该研究表明，尽管先进视觉语言模型（VLMs）在正常条件下表现良好，但在处理经过特定干扰的文字时表现显著下降，由此指出模型在结构化先验方面的不足，并提出了改进模型鲁棒性的建议。

<details>
  <summary>Details</summary>

**Motivation:** 作者希望探索先进视觉语言模型（VLMs）在识别经过特定处理（例如拼接、重组、叠加）而变得难以识别的文字时的鲁棒性，从而揭示现有模型的局限性，并促进更鲁棒可靠的VLMs的发展。

**Method:** 作者设计了两种心理学启发的基准测试，涵盖了中文字符和英语单词，通过拼接、重组、叠加等手段制造“可见但难以阅读”的视觉刺激。

**Result:** 该论文探讨了先进视觉语言模型（VLMs）在识别经过人为干扰的文字时的鲁棒性。通过两类不同的文字系统（中文字符和英文单词）创建了两个心理学启发的基准数据集，其中文字通过拼接、重组或叠加等方式使得机器难于识别，但人类依然可以轻易辨认。尽管在没有干扰的文字识别上表现良好，但面对这些干扰，当前VLMs表现显著下降，经常产生无关或不连贯的输出。这表明现有模型过于依赖通用的视觉不变性，而忽略了组成文字的结构化先验。作者提供了生成刺激、提示和评估协议的代码，以推动透明的复制和后续研究。研究结果表明，需要开发能更好地捕捉符号划分、组合和绑定的架构与训练策略，以应对在教育、无障碍、文化遗产和安全领域内部署多模态系统时的具体挑战。

**Conclusion:** 研究表明现有先进的VLMs在面对人为干扰的文字时表现出不稳定且不准确的预测，强调了在模型设计中更全面地考虑文字组成和结构化先验的重要性。

**Abstract:** Writing is a universal cultural technology that reuses vision for symbolic
communication. Humans display striking resilience: we readily recognize words
even when characters are fragmented, fused, or partially occluded. This paper
investigates whether advanced vision language models (VLMs) share this
resilience. We construct two psychophysics inspired benchmarks across distinct
writing systems, Chinese logographs and English alphabetic words, by splicing,
recombining, and overlaying glyphs to yield ''visible but unreadable'' stimuli
for models while remaining legible to humans. Despite strong performance on
clean text, contemporary VLMs show a severe drop under these perturbations,
frequently producing unrelated or incoherent outputs. The pattern suggests a
structural limitation: models heavily leverage generic visual invariances but
under rely on compositional priors needed for robust literacy. We release
stimuli generation code, prompts, and evaluation protocols to facilitate
transparent replication and follow up work. Our findings motivate architectures
and training strategies that encode symbol segmentation, composition, and
binding across scripts, and they delineate concrete challenges for deploying
multimodal systems in education, accessibility, cultural heritage, and
security.

</details>


### [47] [K-Syn: K-space Data Synthesis in Ultra Low-data Regimes](https://arxiv.org/abs/2509.06997)
*Guan Yu,Zhang Jianhua,Liang Dong,Liu Qiegen*

Main category: cs.CV

> 本文采用频率域特征级学习和时间融合策略来生成心脏磁共振成像的k空间数据，在低数据量条件下展现了优秀的性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于心脏磁共振成像的动态性和复杂性，高质且多样的k空间数据在实践中难以获得。这限制了动态心脏MRI的重建。为此，本研究提出了一个基于频率域特征级学习的方法以解决数据稀少问题。

**Method:** 本研究通过频率域的特征级学习来解决心脏磁共振成像数据稀少和质量波动的问题，并采用时间融合策略作为生成指导来合成k空间数据。该方法利用傅里叶变换的全局表示能力，在频率域进行特征级建模，以生成稳定且丰富的数据，即使在极低数据量的情况下也能如此。此外，该方法还融合了多个时间帧的k空间数据，优化了生成路径。

**Result:** 实验结果表明，该方法在低数据量的情况下具有强大的生成能力，显示出在动态MRI重建中解决数据稀缺问题的实际潜力。

**Conclusion:** 通过频率域的特征级建模和多个时间帧下的k空间数据融合，本研究展示了一个有效的方法来解决动态心脏MRI重建中的数据稀疏问题。

**Abstract:** Owing to the inherently dynamic and complex characteristics of cardiac
magnetic resonance (CMR) imaging, high-quality and diverse k-space data are
rarely available in practice, which in turn hampers robust reconstruction of
dynamic cardiac MRI. To address this challenge, we perform feature-level
learning directly in the frequency domain and employ a temporal-fusion strategy
as the generative guidance to synthesize k-space data. Specifically, leveraging
the global representation capacity of the Fourier transform, the frequency
domain can be considered a natural global feature space. Therefore, unlike
traditional methods that use pixel-level convolution for feature learning and
modeling in the image domain, this letter focuses on feature-level modeling in
the frequency domain, enabling stable and rich generation even with ultra
low-data regimes. Moreover, leveraging the advantages of feature-level modeling
in the frequency domain, we integrate k-space data across time frames with
multiple fusion strategies to steer and further optimize the generative
trajectory. Experimental results demonstrate that the proposed method possesses
strong generative ability in low-data regimes, indicating practical potential
to alleviate data scarcity in dynamic MRI reconstruction.

</details>


### [48] [Not All Splits Are Equal: Rethinking Attribute Generalization Across Unrelated Categories](https://arxiv.org/abs/2509.06998)
*Liviu Nicolae Fircă,Antonio Bărbălau,Dan Oneata,Elena Burceanu*

Main category: cs.CV

> 本研究评估模型在不同类别间的属性预测能力，并提出几种训练-测试拆分策略。结果表明模型对类别相关性变化敏感，聚类方法效果最佳。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探索模型是否能在语义和感知不同的类别之间推广属性知识。尽管先前的工作已解决了狭窄的分类或视觉相似域内的属性预测问题，但尚不清楚当前模型能否将属性抽象化并应用到概念上距离较远的类别。

**Method:** 本研究提出了几种训练-测试拆分策略，以逐步减少训练集与测试集之间的相关性。这些策略包括：由LLM驱动的语义分组、嵌入相似性阈值、基于嵌入的聚类以及基于超类别的分区采用真实标签。

**Result:** 结果表明，随着训练集与测试集之间的类别相关性降低，模型性能显著下降，显示出对拆分设计的高度敏感性。在评估的方法中，聚类提供了最佳的折中方案，它减少了隐藏的相关性同时保留了可学习性。

**Conclusion:** 这些发现为当前表示方法的局限性提供了新的洞见，并为未来的属性推理基准构建提供了指导。

**Abstract:** Can models generalize attribute knowledge across semantically and
perceptually dissimilar categories? While prior work has addressed attribute
prediction within narrow taxonomic or visually similar domains, it remains
unclear whether current models can abstract attributes and apply them to
conceptually distant categories. This work presents the first explicit
evaluation for the robustness of the attribute prediction task under such
conditions, testing whether models can correctly infer shared attributes
between unrelated object types: e.g., identifying that the attribute "has four
legs" is common to both "dogs" and "chairs". To enable this evaluation, we
introduce train-test split strategies that progressively reduce correlation
between training and test sets, based on: LLM-driven semantic grouping,
embedding similarity thresholding, embedding-based clustering, and
supercategory-based partitioning using ground-truth labels. Results show a
sharp drop in performance as the correlation between training and test
categories decreases, indicating strong sensitivity to split design. Among the
evaluated methods, clustering yields the most effective trade-off, reducing
hidden correlations while preserving learnability. These findings offer new
insights into the limitations of current representations and inform future
benchmark construction for attribute reasoning.

</details>


### [49] [Human-in-the-Loop: Quantitative Evaluation of 3D Models Generation by Large Language Models](https://arxiv.org/abs/2509.07010)
*Ahmed R. Sadik,Mariusz Bujny*

Main category: cs.CV

> The paper presents a novel human-in-the-loop evaluation framework for 3D models generated by Large Language Models, highlighting the importance of quantitative metrics over qualitative visual inspection.

<details>
  <summary>Details</summary>

**Motivation:** The motivation of the paper is to develop robust methods to evaluate the geometric and structural fidelity of 3D models generated by Large Language Models, overcoming the limitations of current evaluation techniques.

**Method:** This paper introduces a human-in-the-loop framework for quantitative evaluation of 3D models generated by Large Language Models, using a suite of metrics for volumetric accuracy, surface alignment, dimensional fidelity, and topological intricacy.

**Result:** Results show improved generation fidelity with more semantically-rich inputs, with code-level prompts achieving perfect reconstruction across all metrics in the case study of an L-bracket component.

**Conclusion:** The study concludes that an LLM can achieve high-fidelity generation, especially with code-based prompts, and demonstrates that their evaluation approach enables faster convergence to ground truth than traditional methods.

**Abstract:** Large Language Models are increasingly capable of interpreting multimodal
inputs to generate complex 3D shapes, yet robust methods to evaluate geometric
and structural fidelity remain underdeveloped. This paper introduces a human in
the loop framework for the quantitative evaluation of LLM generated 3D models,
supporting applications such as democratization of CAD design, reverse
engineering of legacy designs, and rapid prototyping. We propose a
comprehensive suite of similarity and complexity metrics, including volumetric
accuracy, surface alignment, dimensional fidelity, and topological intricacy,
to benchmark generated models against ground truth CAD references. Using an L
bracket component as a case study, we systematically compare LLM performance
across four input modalities: 2D orthographic views, isometric sketches,
geometric structure trees, and code based correction prompts. Our findings
demonstrate improved generation fidelity with increased semantic richness, with
code level prompts achieving perfect reconstruction across all metrics. A key
contribution of this work is demonstrating that our proposed quantitative
evaluation approach enables significantly faster convergence toward the ground
truth, especially compared to traditional qualitative methods based solely on
visual inspection and human intuition. This work not only advances the
understanding of AI assisted shape synthesis but also provides a scalable
methodology to validate and refine generative models for diverse CAD
applications.

</details>


### [50] [MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians and Unified Pruning](https://arxiv.org/abs/2509.07021)
*Jiarui Chen,Yikeng Chen,Yingshuang Zou,Ye Huang,Peng Wang,Yuan Liu,Yujing Sun,Wenping Wang*

Main category: cs.CV

> 本文提出了MEGS$^{2}$框架，通过优化两个关键因素实现了显著的内存压缩，并且大大减少了VRAM的使用，同时保证了渲染质量。

<details>
  <summary>Details</summary>

**Motivation:** 3D高斯散度技术在新兴视角合成技术上表现出色，但其高内存消耗限制了其在边缘设备上的应用。作者试图通过提出一种内存高效的方法，解决渲染内存这一关键瓶颈。

**Method:** 我们介绍了MEGS$^{2}$，一个内存高效的框架，该框架通过优化两个关键因素：总的原始数量和每个原始的参数，实现了前所未有的内存压缩。具体来说，我们用轻量级的任意定向球形高斯散度代替了内存密集的球谐函数作为颜色表示。更重要的是，我们提出了一种统一的软修剪框架，将原始数量和散度数量的修剪建模为一个单一的约束优化问题。

**Result:** 实验表明，与现有方法相比，MEGS$^{2}$实现了50%的静态VRAM减少和40%的渲染VRAM减少，同时保持了可比较的渲染质量。

**Conclusion:** 本文提出的新方法实现了显著的内存压缩，减少了VRAM的使用，同时确保了渲染质量不打折扣。这证明了MEGS$^{2}$在增强3DGS在边缘设备上的实用性方面的潜力。

**Abstract:** 3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesis
technique, but its high memory consumption severely limits its applicability on
edge devices. A growing number of 3DGS compression methods have been proposed
to make 3DGS more efficient, yet most only focus on storage compression and
fail to address the critical bottleneck of rendering memory. To address this
problem, we introduce MEGS$^{2}$, a novel memory-efficient framework that
tackles this challenge by jointly optimizing two key factors: the total
primitive number and the parameters per primitive, achieving unprecedented
memory compression. Specifically, we replace the memory-intensive spherical
harmonics with lightweight arbitrarily-oriented spherical Gaussian lobes as our
color representations. More importantly, we propose a unified soft pruning
framework that models primitive-number and lobe-number pruning as a single
constrained optimization problem. Experiments show that MEGS$^{2}$ achieves a
50% static VRAM reduction and a 40% rendering VRAM reduction compared to
existing methods, while maintaining comparable rendering quality.

</details>


### [51] [Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models](https://arxiv.org/abs/2509.07027)
*Jisung Hwang,Jaihoon Kim,Minhyuk Sung*

Main category: cs.CV

> A novel regularization loss that promotes standard Gaussianity in high-dimensional samples for optimization in text-to-image models, surpassing existing methods in efficiency and effectiveness.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to facilitate a range of downstream tasks involving optimization in the latent space of text-to-image models and to ensure permutation invariance by applying losses to randomly permuted inputs.

**Method:** We propose a novel regularization loss that enforces standard Gaussianity, encouraging samples to align with a standard Gaussian distribution. The loss combines moment-based regularization in the spatial domain with power spectrum-based regularization in the spectral domain.

**Result:** The regularization outperforms previous Gaussianity regularization, effectively prevents reward hacking, and accelerates convergence.

**Conclusion:** This regularization framework is effective for enhancing the aesthetics and text alignment in generative modeling for text-to-image models, demonstrating superiority over existing methods by reducing time complexity and improving performance.

**Abstract:** We propose a novel regularization loss that enforces standard Gaussianity,
encouraging samples to align with a standard Gaussian distribution. This
facilitates a range of downstream tasks involving optimization in the latent
space of text-to-image models. We treat elements of a high-dimensional sample
as one-dimensional standard Gaussian variables and define a composite loss that
combines moment-based regularization in the spatial domain with power
spectrum-based regularization in the spectral domain. Since the expected values
of moments and power spectrum distributions are analytically known, the loss
promotes conformity to these properties. To ensure permutation invariance, the
losses are applied to randomly permuted inputs. Notably, existing
Gaussianity-based regularizations fall within our unified framework: some
correspond to moment losses of specific orders, while the previous
covariance-matching loss is equivalent to our spectral loss but incurs higher
time complexity due to its spatial-domain computation. We showcase the
application of our regularization in generative modeling for test-time reward
alignment with a text-to-image model, specifically to enhance aesthetics and
text alignment. Our regularization outperforms previous Gaussianity
regularization, effectively prevents reward hacking and accelerates
convergence.

</details>


### [52] [SAM$^{*}$: Task-Adaptive SAM with Physics-Guided Rewards](https://arxiv.org/abs/2509.07047)
*Kamyar Barakati,Utkarsh Pratiush,Sheryl L. Sanchez,Aditya Raghavan,Delia J. Milliron,Mahshid Ahmadi,Philip D. Rack,Sergei V. Kalinin*

Main category: cs.CV

> 本文提出了一种改善基础模型，特别是SAM框架的适应性和性能的新方法，以更好地满足不同分割任务的要求。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决基础模型在实际应用中参数不透明和需要大量手动调整的问题，特别是在实时流数据分析中的应用限制。

**Method:** 本文提出了一种基于奖励函数的优化方法来微调基础模型，并以Meta的SAM（Segment Anything Model）框架为例说明这一方法。通过构建反映成像系统物理属性的奖励函数来优化模型。

**Result:** 采用奖励驱动的优化框架，文章提升了SAM的适应性和性能，从而得到了一个更适合各种分割任务优化版本SAM$^{*}$，尤其对于实时流数据的分割具有重要意义。

**Conclusion:** 文章通过引入基于奖励函数的优化方法，证明了其在显微成像中的有效性，特别是在精确的细胞结构、材料界面和纳米尺度特征分析方面。

**Abstract:** Image segmentation is a critical task in microscopy, essential for accurately
analyzing and interpreting complex visual data. This task can be performed
using custom models trained on domain-specific datasets, transfer learning from
pre-trained models, or foundational models that offer broad applicability.
However, foundational models often present a considerable number of
non-transparent tuning parameters that require extensive manual optimization,
limiting their usability for real-time streaming data analysis. Here, we
introduce a reward function-based optimization to fine-tune foundational models
and illustrate this approach for SAM (Segment Anything Model) framework by
Meta. The reward functions can be constructed to represent the physics of the
imaged system, including particle size distributions, geometries, and other
criteria. By integrating a reward-driven optimization framework, we enhance
SAM's adaptability and performance, leading to an optimized variant, SAM$^{*}$,
that better aligns with the requirements of diverse segmentation tasks and
particularly allows for real-time streaming data segmentation. We demonstrate
the effectiveness of this approach in microscopy imaging, where precise
segmentation is crucial for analyzing cellular structures, material interfaces,
and nanoscale features.

</details>


### [53] [Enhancing Classification of Streaming Data with Image Distillation](https://arxiv.org/abs/2509.07049)
*Rwad Khatib,Yehudit Aperstein*

Main category: cs.CV

> The study uses a Distillation Based Classification (DBC) method to improve the precision of streaming image data classification, achieving better accuracy than traditional methods with limited resources.

<details>
  <summary>Details</summary>

**Motivation:** To efficiently classify streaming data with limited memory and computational resources using an innovative approach.

**Method:** Distillation Based Classification (DBC) method that focuses on distilling essential features from data streams to classify streaming image data.

**Result:** DBC achieved a 73.1% accuracy rate, surpassing traditional methods such as Hoeffding Trees and Adaptive Random Forest, and the Reservoir Sampling Based Classification (RBC) technique.

**Conclusion:** The study demonstrates significant advancements in streaming data classification, highlighting the effectiveness of the DBC method in both accuracy and efficiency.

**Abstract:** This study tackles the challenge of efficiently classifying streaming data in
envi-ronments with limited memory and computational resources. It delves into
the application of data distillation as an innovative approach to improve the
precision of streaming image data classification. By focusing on distilling
essential features from data streams, our method aims to minimize computational
demands while preserving crucial information for accurate classification. Our
investigation com-pares this approach against traditional algorithms like
Hoeffding Trees and Adap-tive Random Forest, adapted through embeddings for
image data. The Distillation Based Classification (DBC) demonstrated superior
performance, achieving a 73.1% accuracy rate, surpassing both traditional
methods and Reservoir Sam-pling Based Classification (RBC) technique. This
marks a significant advance-ment in streaming data classification, showcasing
the effectiveness of our method in processing complex data streams and setting
a new standard for accuracy and efficiency.

</details>


### [54] [Automated Evaluation of Gender Bias Across 13 Large Multimodal Models](https://arxiv.org/abs/2509.07050)
*Juan Manuel Contreras*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Large multimodal models (LMMs) have revolutionized text-to-image generation,
but they risk perpetuating the harmful social biases in their training data.
Prior work has identified gender bias in these models, but methodological
limitations prevented large-scale, comparable, cross-model analysis. To address
this gap, we introduce the Aymara Image Fairness Evaluation, a benchmark for
assessing social bias in AI-generated images. We test 13 commercially available
LMMs using 75 procedurally-generated, gender-neutral prompts to generate people
in stereotypically-male, stereotypically-female, and non-stereotypical
professions. We then use a validated LLM-as-a-judge system to score the 965
resulting images for gender representation. Our results reveal (p < .001 for
all): 1) LMMs systematically not only reproduce but actually amplify
occupational gender stereotypes relative to real-world labor data, generating
men in 93.0% of images for male-stereotyped professions but only 22.5% for
female-stereotyped professions; 2) Models exhibit a strong default-male bias,
generating men in 68.3% of the time for non-stereotyped professions; and 3) The
extent of bias varies dramatically across models, with overall male
representation ranging from 46.7% to 73.3%. Notably, the top-performing model
de-amplified gender stereotypes and approached gender parity, achieving the
highest fairness scores. This variation suggests high bias is not an inevitable
outcome but a consequence of design choices. Our work provides the most
comprehensive cross-model benchmark of gender bias to date and underscores the
necessity of standardized, automated evaluation tools for promoting
accountability and fairness in AI development.

</details>


### [55] [Faster VGGT with Block-Sparse Global Attention](https://arxiv.org/abs/2509.07120)
*Chung-Shien Brian Wang,Christian Schmidt,Jens Piekenbrinck,Bastian Leibe*

Main category: cs.CV

> 为了解决全局注意力层中的运行时间瓶颈问题，我们提出了一种基于块稀疏核的方法，极大提升了推理速度，同时保持了与之前模型相当的任务性能。

<details>
  <summary>Details</summary>

**Motivation:** 受到大型语言模型的进步启发，我们旨在解决transformer-based模型中的全局注意力层的二次复杂度运行时间瓶颈问题，该问题限制了这些模型在大规模图像集合上的可扩展性。

**Method:** 我们提出了一种基于高度优化的块稀疏核替代密集全局注意力操作的方法。这种方法能够实现高达4倍的加速推理，并且任务性能相当。

**Result:** 在多视图基准测试的全面套件上进行的评估表明了我们方法的有效性。并且这种方法无需重新训练骨干网络，适用于VGGT和$\pi^3$模型，支持大规模图像集合。

**Conclusion:** 实验结果证明了这种方法的有效性，不仅加速了推理过程，还支持大规模图像集合，且不需要重新训练模型。

**Abstract:** Efficient and accurate feed-forward multi-view reconstruction has long been
an important task in computer vision. Recent transformer-based models like VGGT
and $\pi^3$ have achieved impressive results with simple architectures, yet
they face an inherent runtime bottleneck, due to the quadratic complexity of
the global attention layers, that limits the scalability to large image sets.
In this paper, we empirically analyze the global attention matrix of these
models and observe that probability mass concentrates on a small subset of
patch-patch interactions that correspond to cross-view geometric matches.
Motivated by the structured attention and inspired by recent advancement in
large language models, we propose a replacement for the dense global attention
operation based on highly optimized block-sparse kernels, yielding up to
$4\times$ faster inference with comparable task performance. Our retrofit
requires no retraining of the backbone, extends to both VGGT and $\pi^3$, and
supports large image collections. Evaluations on a comprehensive suite of
multi-view benchmarks demonstrate the effectiveness of our approach.

</details>


### [56] [Detection and Recovery of Adversarial Slow-Pose Drift in Offloaded Visual-Inertial Odometry](https://arxiv.org/abs/2509.07130)
*Soruya Saha,Md Nurul Absur,Saptarshi Debroy*

Main category: cs.CV

> 本文提出了一种新的检测和恢复机制，能在边缘服务器环境中的VIO遭遇姿态欺骗攻击时，恢复其准确性。

<details>
  <summary>Details</summary>

**Motivation:** 论文研究了将VIO卸载到边缘服务器时存在的一种安全威胁，即攻击者可能通过细微的姿态欺骗造成显著的漂移，并且这些攻击可以避开传统的检测机制。

**Method:** 该论文提出了一种无监督且无需标签的检测和恢复机制，用于识别VIO（视觉惯性里程计）中的姿态欺骗，并恢复姿态的一致性。该模型通过学习无攻击会话中的运动时间规律来检测运行时的偏差并启动恢复。

**Result:** 实验结果显示，与无防御基准相比，该方法在现实的边缘卸载VIO环境中，在多种欺骗强度下，显著减少了轨迹和姿态误差。

**Conclusion:** 论文实验结果表明，提出的模型能够在各种姿态欺骗强度下有效降低轨迹和姿态误差，有效保护VIO在卸载到边缘服务器时的安全性和准确性。

**Abstract:** Visual-Inertial Odometry (VIO) supports immersive Virtual Reality (VR) by
fusing camera and Inertial Measurement Unit (IMU) data for real-time pose.
However, current trend of offloading VIO to edge servers can lead server-side
threat surface where subtle pose spoofing can accumulate into substantial
drift, while evading heuristic checks. In this paper, we study this threat and
present an unsupervised, label-free detection and recovery mechanism. The
proposed model is trained on attack-free sessions to learn temporal
regularities of motion to detect runtime deviations and initiate recovery to
restore pose consistency. We evaluate the approach in a realistic offloaded-VIO
environment using ILLIXR testbed across multiple spoofing intensities.
Experimental results in terms of well-known performance metrics show
substantial reductions in trajectory and pose error compared to a no-defense
baseline.

</details>


### [57] [Realism to Deception: Investigating Deepfake Detectors Against Face Enhancement](https://arxiv.org/abs/2509.07178)
*Muhammad Saad Saeed,Ijaz Ul Haq,Khalid Malik*

Main category: cs.CV

> This paper investigates whether face enhancement techniques can significantly reduce the accuracy of deepfake detectors, demonstrating that even basic filters and GAN-based methods can function as effective anti-forensic tools.

<details>
  <summary>Details</summary>

**Motivation:** To understand if face enhancement methods inadvertently degrade the performance of deepfake detection systems through unintentional biometric feature distortion.

**Method:** Structure

**Result:** Experiments show that both basic filters and GAN-based techniques can reduce the detection accuracy of deepfake detectors with ASR reaching up to 75.12% in some cases.

**Conclusion:** The study underscores the need for improved and more resilient forensic methods to counteract the anti-forensic capabilities of face enhancement techniques.

**Abstract:** Face enhancement techniques are widely used to enhance facial appearance.
However, they can inadvertently distort biometric features, leading to
significant decrease in the accuracy of deepfake detectors. This study
hypothesizes that these techniques, while improving perceptual quality, can
degrade the performance of deepfake detectors. To investigate this, we
systematically evaluate whether commonly used face enhancement methods can
serve an anti-forensic role by reducing detection accuracy. We use both
traditional image processing methods and advanced GAN-based enhancements to
evaluate the robustness of deepfake detectors. We provide a comprehensive
analysis of the effectiveness of these enhancement techniques, focusing on
their impact on Na\"ive, Spatial, and Frequency-based detection methods.
Furthermore, we conduct adversarial training experiments to assess whether
exposure to face enhancement transformations improves model robustness.
Experiments conducted on the FaceForensics++, DeepFakeDetection, and CelebDF-v2
datasets indicate that even basic enhancement filters can significantly reduce
detection accuracy achieving ASR up to 64.63\%. In contrast, GAN-based
techniques further exploit these vulnerabilities, achieving ASR up to 75.12\%.
Our results demonstrate that face enhancement methods can effectively function
as anti-forensic tools, emphasizing the need for more resilient and adaptive
forensic methods.

</details>


### [58] [Dimensionally Reduced Open-World Clustering: DROWCULA](https://arxiv.org/abs/2509.07184)
*Erencem Ozbey,Dimitrios I. Diochnos*

Main category: cs.CV

> 论文提出了一种基于视觉变压器和流形学习的无监督方法，用于识别图像数据集中的新类别，并在多个基准数据集中取得了优异结果。

<details>
  <summary>Details</summary>

**Motivation:** 尽管标注数据是监督学习的基础，但在“开放世界”背景下，由于未来可能会出现新的类别，标注工作变得复杂且耗时。该论文旨在解决这一问题。

**Method:** 该研究提出了一种完全无监督的方法来确定特定数据集中的新类别。方法基于视觉变压器，利用注意力机制生成向量嵌入，并结合流形学习技术来优化这些嵌入，以提升图像聚类的整体性能。

**Result:** 研究在CIFAR-10, CIFAR-100, ImageNet-100, 和 Tiny ImageNet 上建立了单模态聚类和新类别发现的新SOTA结果，不论是已知还是未知类别数量的情况。

**Conclusion:** 该方法使用无监督的方式在图像分类任务中实现了新的类别识别，特别是在开放世界情况下，当数据集中出现前所未见的类别时，能够有效地对其进行识别和分类。

**Abstract:** Working with annotated data is the cornerstone of supervised learning.
Nevertheless, providing labels to instances is a task that requires significant
human effort. Several critical real-world applications make things more
complicated because no matter how many labels may have been identified in a
task of interest, it could be the case that examples corresponding to novel
classes may appear in the future. Not unsurprisingly, prior work in this,
so-called, `open-world' context has focused a lot on semi-supervised
approaches.
  Focusing on image classification, somehow paradoxically, we propose a fully
unsupervised approach to the problem of determining the novel categories in a
particular dataset. Our approach relies on estimating the number of clusters
using Vision Transformers, which utilize attention mechanisms to generate
vector embeddings. Furthermore, we incorporate manifold learning techniques to
refine these embeddings by exploiting the intrinsic geometry of the data,
thereby enhancing the overall image clustering performance. Overall, we
establish new State-of-the-Art results on single-modal clustering and Novel
Class Discovery on CIFAR-10, CIFAR-100, ImageNet-100, and Tiny ImageNet. We do
so, both when the number of clusters is known or unknown ahead of time. The
code is available at: https://github.com/DROWCULA/DROWCULA.

</details>


### [59] [XBusNet: Text-Guided Breast Ultrasound Segmentation via Multimodal Vision-Language Learning](https://arxiv.org/abs/2509.07213)
*Raja Mallina,Bryar Shareef*

Main category: cs.CV

> This paper introduces XBusNet, a multimodal model that combines visual and text-based information to improve BUS segmentation accuracy, particularly for small or less contrast lesions, demonstrating superior performance over existing approaches.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to improve the accuracy of BUS segmentation for small or low contrast lesions, which are difficult to segment due to fuzzy boundaries and speckle noise, by effectively using text prompts to add clinical context.

**Method:** We propose XBusNet, a novel dual-prompt, dual-branch multimodal model that integrates image features with clinically grounded text descriptions to achieve precise breast ultrasound (BUS) segmentation.

**Result:** XBusNet achieves state-of-the-art performance with a mean Dice of 0.8765 and IoU of 0.8149 on the BLU dataset, outperforming six strong baselines, with significant improvements in segmenting small lesions.

**Conclusion:** A dual-prompt, dual-branch multimodal design that integrates global semantic understanding with local precision enhances the accuracy and robustness of BUS segmentation, especially for small, low-contrast lesions.

**Abstract:** Background: Precise breast ultrasound (BUS) segmentation supports reliable
measurement, quantitative analysis, and downstream classification, yet remains
difficult for small or low-contrast lesions with fuzzy margins and speckle
noise. Text prompts can add clinical context, but directly applying weakly
localized text-image cues (e.g., CAM/CLIP-derived signals) tends to produce
coarse, blob-like responses that smear boundaries unless additional mechanisms
recover fine edges. Methods: We propose XBusNet, a novel dual-prompt,
dual-branch multimodal model that combines image features with clinically
grounded text. A global pathway based on a CLIP Vision Transformer encodes
whole-image semantics conditioned on lesion size and location, while a local
U-Net pathway emphasizes precise boundaries and is modulated by prompts that
describe shape, margin, and Breast Imaging Reporting and Data System (BI-RADS)
terms. Prompts are assembled automatically from structured metadata, requiring
no manual clicks. We evaluate on the Breast Lesions USG (BLU) dataset using
five-fold cross-validation. Primary metrics are Dice and Intersection over
Union (IoU); we also conduct size-stratified analyses and ablations to assess
the roles of the global and local paths and the text-driven modulation.
Results: XBusNet achieves state-of-the-art performance on BLU, with mean Dice
of 0.8765 and IoU of 0.8149, outperforming six strong baselines. Small lesions
show the largest gains, with fewer missed regions and fewer spurious
activations. Ablation studies show complementary contributions of global
context, local boundary modeling, and prompt-based modulation. Conclusions: A
dual-prompt, dual-branch multimodal design that merges global semantics with
local precision yields accurate BUS segmentation masks and improves robustness
for small, low-contrast lesions.

</details>


### [60] [Breast Cancer Detection in Thermographic Images via Diffusion-Based Augmentation and Nonlinear Feature Fusion](https://arxiv.org/abs/2509.07277)
*Sepehr Salem,M. Moein Esfahani,Jingyu Liu,Vince Calhoun*

Main category: cs.CV

> A study uses a DPM for data augmentation to improve breast cancer classification in thermograms, combining deep and handcrafted features for high accuracy.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this research is to address the challenge of data scarcity in deep learning applications for medical imaging, particularly for breast cancer classification in thermograms.

**Method:** The method utilizes a Diffusion Probabilistic Model (DPM) for enhancing breast cancer classification in thermograms through data augmentation. It fuses deep features from a pre-trained ResNet-50 with handcrafted nonlinear features, such as Fractal Dimension, obtained from U-Net segmented tumors, and employs an XGBoost classifier.

**Result:** The framework achieves 98.0% accuracy and 98.1% sensitivity in classifying breast cancer in thermograms. Ablation studies and statistical tests support the importance of DPM augmentation and nonlinear feature fusion.

**Conclusion:** The study concludes that combining advanced generative models with interpretable features can significantly improve the accuracy of medical diagnostic tools, in this case for breast cancer classification in thermograms.

**Abstract:** Data scarcity hinders deep learning for medical imaging. We propose a
framework for breast cancer classification in thermograms that addresses this
using a Diffusion Probabilistic Model (DPM) for data augmentation. Our
DPM-based augmentation is shown to be superior to both traditional methods and
a ProGAN baseline. The framework fuses deep features from a pre-trained
ResNet-50 with handcrafted nonlinear features (e.g., Fractal Dimension) derived
from U-Net segmented tumors. An XGBoost classifier trained on these fused
features achieves 98.0\% accuracy and 98.1\% sensitivity. Ablation studies and
statistical tests confirm that both the DPM augmentation and the nonlinear
feature fusion are critical, statistically significant components of this
success. This work validates the synergy between advanced generative models and
interpretable features for creating highly accurate medical diagnostic tools.

</details>


### [61] [Reconstruction Alignment Improves Unified Multimodal Models](https://arxiv.org/abs/2509.07295)
*Ji Xie,Trevor Darrell,Luke Zettlemoyer,XuDong Wang*

Main category: cs.CV

> 论文提出Reconstruction Alignment (RecA)策略，通过UMMs自身的视觉理解嵌入作为“文本提示”，改善了多种UMMs架构的生成和编辑效果，且计算资源需求低。

<details>
  <summary>Details</summary>

**Motivation:** 传统训练方法依赖的图像-文本对的描述通常缺乏细粒度的视觉细节。为了改善UMMs的图像生成和编辑保真度，降低成本。

**Method:** 引入Reconstruction Alignment (RecA)，利用UMMs的视觉理解嵌入作为密集的“文本提示”，并通过自监督重构损失优化输入图像的生成。

**Result:** UMMs统一了视觉理解和生成。传统训练依赖图像-文本对，但描述中的细粒度视觉细节不足。研究引入Reconstruction Alignment (RecA)，该方法在没有丰富词汇描述的条件下，使用UMMs自己的视觉理解嵌入作为密集的“文本提示”，通过自监督重构损失优化输入图像的生成。实验表明，RecA在多种UMMs架构中改善了图像生成和编辑的保真度，使用很少的计算资源（27个GPU小时）显著提高了性能。

**Conclusion:** RecA作为一个高效的后训练策略，适用于多种UMMs架构，其效果超越了更大的开源模型。

**Abstract:** Unified multimodal models (UMMs) unify visual understanding and generation
within a single architecture. However, conventional training relies on
image-text pairs (or sequences) whose captions are typically sparse and miss
fine-grained visual details--even when they use hundreds of words to describe a
simple image. We introduce Reconstruction Alignment (RecA), a
resource-efficient post-training method that leverages visual understanding
encoder embeddings as dense "text prompts," providing rich supervision without
captions. Concretely, RecA conditions a UMM on its own visual understanding
embeddings and optimizes it to reconstruct the input image with a
self-supervised reconstruction loss, thereby realigning understanding and
generation. Despite its simplicity, RecA is broadly applicable: across
autoregressive, masked-autoregressive, and diffusion-based UMMs, it
consistently improves generation and editing fidelity. With only 27 GPU-hours,
post-training with RecA substantially improves image generation performance on
GenEval (0.73$\rightarrow$0.90) and DPGBench (80.93$\rightarrow$88.15), while
also boosting editing benchmarks (ImgEdit 3.38$\rightarrow$3.75, GEdit
6.94$\rightarrow$7.25). Notably, RecA surpasses much larger open-source models
and applies broadly across diverse UMM architectures, establishing it as an
efficient and general post-training alignment strategy for UMMs

</details>


### [62] [DEPF: A UAV Multispectral Object Detector with Dual-Domain Enhancement and Priority-Guided Mamba Fusion](https://arxiv.org/abs/2509.07327)
*Shucong Li,Zhenyu Liu,Zijie Hong,Zhiheng Zhou,Xianghai Cao*

Main category: cs.CV

> 本文提出了DEPF检测器，结合双域增强和优先级引导曼巴融合，有效解决了多光谱遥感目标检测中的几项技术挑战。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决多光谱遥感目标检测中的三个挑战：低光遥感影像中的多模态融合互补性降低、局部小目标建模受到融合阶段冗余信息干扰、基于变换的方法在无人机平台上的二次计算复杂性问题。

**Method:** 本文提出了一种名为DEPF的无人机多光谱目标检测器，采用双域增强模块（DDE）和优先级引导曼巴融合模块（PGMF）。DDE模块包括跨尺度小波曼巴（CSWM）和傅里叶细节恢复块（FDR），用于增强低光遥感图像。PGMF模块则引入了优先级扫描的概念，以减轻冗余信息对局部目标建模的影响。

**Result:** 实验结果显示，在DroneVehicle和VEDAI数据集上，DEPF在目标检测方面表现出色，优于最先进的方法。

**Conclusion:** DEPF通过其创新的设计在无人机多光谱目标检测的低光增强和局部目标建模方面取得了良好效果。

**Abstract:** Multispectral remote sensing object detection is one of the important
application of unmanned aerial vehicle (UAV). However, it faces three
challenges. Firstly, the low-light remote sensing images reduce the
complementarity during multi-modality fusion. Secondly, the local small target
modeling is interfered with redundant information in the fusion stage easily.
Thirdly, due to the quadratic computational complexity, it is hard to apply the
transformer-based methods on the UAV platform. To address these limitations,
motivated by Mamba with linear complexity, a UAV multispectral object detector
with dual-domain enhancement and priority-guided mamba fusion (DEPF) is
proposed. Firstly, to enhance low-light remote sensing images, Dual-Domain
Enhancement Module (DDE) is designed, which contains Cross-Scale Wavelet Mamba
(CSWM) and Fourier Details Recovery block (FDR). CSWM applies cross-scale mamba
scanning for the low-frequency components to enhance the global brightness of
images, while FDR constructs spectrum recovery network to enhance the frequency
spectra features for recovering the texture-details. Secondly, to enhance local
target modeling and reduce the impact of redundant information during fusion,
Priority-Guided Mamba Fusion Module (PGMF) is designed. PGMF introduces the
concept of priority scanning, which starts from local targets features
according to the priority scores obtained from modality difference. Experiments
on DroneVehicle dataset and VEDAI dataset reports that, DEPF performs well on
object detection, comparing with state-of-the-art methods. Our code is
available in the supplementary material.

</details>


### [63] [G3CN: Gaussian Topology Refinement Gated Graph Convolutional Network for Skeleton-Based Action Recognition](https://arxiv.org/abs/2509.07335)
*Haiqing Ren,Zhongkai Luo,Heng Fan,Xiaohui Yuan,Guanchen Wang,Libo Zhang*

Main category: cs.CV

> 本论文提出了G$^{3}$CN，通过高斯滤波和GRUs改良GCN，提升了基于骨架动作识别对于模糊动作的分辨精确度。

<details>
  <summary>Details</summary>

**Motivation:** 尽管图卷积网络（GCN）在基于骨架的动作识别方面取得了显著成功，但它们在区分模糊动作方面存在不足。为了解决这一问题，本研究提出了G$^{3}$CN方法。

**Method:** 本研究提出了一种新颖的方法——高斯拓扑细化门控图卷积网络（G$^{3}$CN），用于区分基于骨架的动作识别中的模糊动作。该方法结合了高斯滤波器来优化骨架拓扑图，增强对模糊动作的表示能力，并集成了门控循环单元（GRUs）以增强骨架点之间的信息传播。

**Result:** 在NTU RGB+D、NTU RGB+D 120 和 NW-UCLA 数据集上的广泛实验证明，G$^{3}$CN 有效提高了动作识别的准确性，尤其是对于模糊样本。

**Conclusion:** G$^{3}$CN 通过高斯滤波和门控循环单元的结合，能够增强基于骨架的动作识别中模糊动作的区分能力，展示了在各种GCN模型中的广泛适用性。

**Abstract:** Graph Convolutional Networks (GCNs) have proven to be highly effective for
skeleton-based action recognition, primarily due to their ability to leverage
graph topology for feature aggregation, a key factor in extracting meaningful
representations. However, despite their success, GCNs often struggle to
effectively distinguish between ambiguous actions, revealing limitations in the
representation of learned topological and spatial features. To address this
challenge, we propose a novel approach, Gaussian Topology Refinement Gated
Graph Convolution (G$^{3}$CN), to address the challenge of distinguishing
ambiguous actions in skeleton-based action recognition. G$^{3}$CN incorporates
a Gaussian filter to refine the skeleton topology graph, improving the
representation of ambiguous actions. Additionally, Gated Recurrent Units (GRUs)
are integrated into the GCN framework to enhance information propagation
between skeleton points. Our method shows strong generalization across various
GCN backbones. Extensive experiments on NTU RGB+D, NTU RGB+D 120, and NW-UCLA
benchmarks demonstrate that G$^{3}$CN effectively improves action recognition,
particularly for ambiguous samples.

</details>


### [64] [Parse Graph-Based Visual-Language Interaction for Human Pose Estimation](https://arxiv.org/abs/2509.07385)
*Shibang Liu,Xuemei Xie,Guangming Shi*

Main category: cs.CV

> 本研究提出PGVL方法，通过设计Guided Module改善视觉-语言融合在人体姿态估计中的效果，尤其在处理遮挡区域时具有优势。

<details>
  <summary>Details</summary>

**Motivation:** 以往的工作大多集中在单模态建模上，忽略了多模态融合的潜力。本研究旨在解决现有视觉-语言融合方法在处理遮挡区域时响应减弱以及对齐和位置失败的问题。通过融合语言提供的空间关系等丰富的HPE先验知识，增强人体姿态估计的效果。

**Method:** 本研究提出了Parse Graph-based Visual-Language交互(PGVL)方法，并在其中设计了一个核心的新模块——Guided Module (GM)。在PGVL中，低级节点专注于局部特征，以最大限度地维持遮挡区域的响应，而高级节点则融合全局特征以推断遮挡或不可见的部分。GM允许高级节点指导已经经过交叉注意力机制的低级节点的特征更新，从而确保不同信息的有效融合。PGVL包括自上而下的分解和自下而上的组合两阶段。首先构建模式特定的解析图，然后使用递归双向交叉注意力机制，通过GM进行净化。

**Result:** 尚未具体提及实验结果，但表明了方法在主要的人体姿态估计数据集上的有效性，并计划发布代码以供验证。

**Conclusion:** 研究表明，通过PGVL及其Guided Module，可以实现有效的多模态信息融合，提高人体姿态估计精度，尤其是处理遮挡和不可见区域的场景。

**Abstract:** Parse graphs boost human pose estimation (HPE) by integrating context and
hierarchies, yet prior work mostly focuses on single modality modeling,
ignoring the potential of multimodal fusion. Notably, language offers rich HPE
priors like spatial relations for occluded scenes, but existing visual-language
fusion via global feature integration weakens occluded region responses and
causes alignment and location failures. To address this issue, we propose Parse
Graph-based Visual-Language interaction (PGVL) with a core novel Guided Module
(GM). In PGVL, low-level nodes focus on local features, maximizing the
maintenance of responses in occluded areas and high-level nodes integrate
global features to infer occluded or invisible parts. GM enables high semantic
nodes to guide the feature update of low semantic nodes that have undergone
cross attention. It ensuring effective fusion of diverse information. PGVL
includes top-down decomposition and bottom-up composition. In the first stage,
modality specific parse graphs are constructed. Next stage. recursive
bidirectional cross-attention is used, purified by GM. We also design network
based on PGVL. The PGVL and our network is validated on major pose estimation
datasets. We will release the code soon.

</details>


### [65] [DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation](https://arxiv.org/abs/2509.07435)
*Ze-Xin Yin,Jiaxiong Qiu,Liu Liu,Xinjie Wang,Wei Sui,Zhizhong Su,Jian Yang,Jin Xie*

Main category: cs.CV

> 本文介绍了LGAA框架，一个用于生成PBR-ready 3D资产的全新方法，该框架利用了多视角扩散先验知识，通过模块化设计实现了高效训练和灵活先验整合。

<details>
  <summary>Details</summary>

**Motivation:** 由于基于物理渲染（PBR）的3D资产创建工作既耗时又耗力，该研究的目的是为了实现一个从头到尾的PBR/assets生成流水线，以解决现有3D生成方法中的局限性。

**Method:** 文章提出了一个名为轻量级高斯资产适配器（Lightweight Gaussian Asset Adapter, LGAA）的新框架，该框架通过利用多视角扩散先验知识来统一几何建模和基于物理渲染材料的建模。该框架分为三个模块：LGAA Wrapper、LGAA Switcher和LGAA Decoder。这个框架能够高效地训练，并集成多个扩散先验知识。

**Result:** 大量的定量和定性实验表明，LGAA在多视角扩散模型的文本和图像条件生成上都表现出优越的性能。同时，其模块化设计使得整合多种扩散先验知识更加灵活高效。

**Conclusion:** LGAA通过模块化设计有效地实现了几何模型和PBR材料的统一建模，且能够高效训练、快速收敛。

**Abstract:** The labor- and experience-intensive creation of 3D assets with physically
based rendering (PBR) materials demands an autonomous 3D asset creation
pipeline. However, most existing 3D generation methods focus on geometry
modeling, either baking textures into simple vertex colors or leaving texture
synthesis to post-processing with image diffusion models. To achieve end-to-end
PBR-ready 3D asset generation, we present Lightweight Gaussian Asset Adapter
(LGAA), a novel framework that unifies the modeling of geometry and PBR
materials by exploiting multi-view (MV) diffusion priors from a novel
perspective. The LGAA features a modular design with three components.
Specifically, the LGAA Wrapper reuses and adapts network layers from MV
diffusion models, which encapsulate knowledge acquired from billions of images,
enabling better convergence in a data-efficient manner. To incorporate multiple
diffusion priors for geometry and PBR synthesis, the LGAA Switcher aligns
multiple LGAA Wrapper layers encapsulating different knowledge. Then, a tamed
variational autoencoder (VAE), termed LGAA Decoder, is designed to predict 2D
Gaussian Splatting (2DGS) with PBR channels. Finally, we introduce a dedicated
post-processing procedure to effectively extract high-quality, relightable mesh
assets from the resulting 2DGS. Extensive quantitative and qualitative
experiments demonstrate the superior performance of LGAA with both text-and
image-conditioned MV diffusion models. Additionally, the modular design enables
flexible incorporation of multiple diffusion priors, and the
knowledge-preserving scheme leads to efficient convergence trained on merely
69k multi-view instances. Our code, pre-trained weights, and the dataset used
will be publicly available via our project page:
https://zx-yin.github.io/dreamlifting/.

</details>


### [66] [In the Eye of MLLM: Benchmarking Egocentric Video Intent Understanding with Gaze-Guided Prompting](https://arxiv.org/abs/2509.07447)
*Taiying Peng,Jiacheng Hua,Miao Liu,Feng Lu*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** The emergence of advanced multimodal large language models (MLLMs) has
significantly enhanced AI assistants' ability to process complex information
across modalities. Recently, egocentric videos, by directly capturing user
focus, actions, and context in an unified coordinate, offer an exciting
opportunity to enable proactive and personalized AI user experiences with
MLLMs. However, existing benchmarks overlook the crucial role of gaze as an
indicator of user intent. To address this gap, we introduce EgoGazeVQA, an
egocentric gaze-guided video question answering benchmark that leverages gaze
information to improve the understanding of longer daily-life videos.
EgoGazeVQA consists of gaze-based QA pairs generated by MLLMs and refined by
human annotators. Our experiments reveal that existing MLLMs struggle to
accurately interpret user intentions. In contrast, our gaze-guided intent
prompting methods significantly enhance performance by integrating spatial,
temporal, and intent-related cues. We further conduct experiments on
gaze-related fine-tuning and analyze how gaze estimation accuracy impacts
prompting effectiveness. These results underscore the value of gaze for more
personalized and effective AI assistants in egocentric settings.

</details>


### [67] [GLEAM: Learning to Match and Explain in Cross-View Geo-Localization](https://arxiv.org/abs/2509.07450)
*Xudong Lu,Zhi Zheng,Yi Wan,Yongxiang Yao,Annan Wang,Renrui Zhang,Panwang Xia,Qiong Wu,Qingyun Li,Weifeng Lin,Xiangyu Zhao,Xue Yang,Hongsheng Li*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Cross-View Geo-Localization (CVGL) focuses on identifying correspondences
between images captured from distinct perspectives of the same geographical
location. However, existing CVGL approaches are typically restricted to a
single view or modality, and their direct visual matching strategy lacks
interpretability: they merely predict whether two images correspond, without
explaining the rationale behind the match. In this paper, we present GLEAM-C, a
foundational CVGL model that unifies multiple views and modalities-including
UAV imagery, street maps, panoramic views, and ground photographs-by aligning
them exclusively with satellite imagery. Our framework enhances training
efficiency through optimized implementation while achieving accuracy comparable
to prior modality-specific CVGL models through a two-phase training strategy.
Moreover, to address the lack of interpretability in traditional CVGL methods,
we leverage the reasoning capabilities of multimodal large language models
(MLLMs) to propose a new task, GLEAM-X, which combines cross-view
correspondence prediction with explainable reasoning. To support this task, we
construct a bilingual benchmark using GPT-4o and Doubao-1.5-Thinking-Vision-Pro
to generate training and testing data. The test set is further refined through
detailed human revision, enabling systematic evaluation of explainable
cross-view reasoning and advancing transparency and scalability in
geo-localization. Together, GLEAM-C and GLEAM-X form a comprehensive CVGL
pipeline that integrates multi-modal, multi-view alignment with interpretable
correspondence analysis, unifying accurate cross-view matching with explainable
reasoning and advancing Geo-Localization by enabling models to better Explain
And Match. Code and datasets used in this work will be made publicly accessible
at https://github.com/Lucky-Lance/GLEAM.

</details>


### [68] [XOCT: Enhancing OCT to OCTA Translation via Cross-Dimensional Supervised Multi-Scale Feature Learning](https://arxiv.org/abs/2509.07455)
*Pooya Khosravi,Kun Han,Anthony T. Wu,Arghavan Rezvani,Zexin Feng,Xiaohui Xie*

Main category: cs.CV

> 本文提出了XOCT框架，结合CDS和MSFF技术，显著提升了OCT到OCTA转换中的血管重建质量，增强了OCTA图像的临床应用价值。

<details>
  <summary>Details</summary>

**Motivation:** 为了克服当前光学相干断层扫描血管成像（OCTA）技术中图像质量获取难题，以及深度学习方法在OCT到OCTA转换过程中的局限性，尤其是血管重建中的细节捕捉问题。

**Method:** 本文提出了一种新颖的深度学习框架XOCT，它结合了跨维度监督（CDS）和多尺度特征融合（MSFF）网络，以实现层感知的血管重建。CDS模块利用通过分割加权Z轴平均生成的2D层面投影作为监督信号，旨在引导网络学习每层视网膜的特定表示。MSFF模块则通过多尺度特征提取结合通道重加权策略增强血管勾勒效果。

**Result:** 实验结果表明，XOCT在OCTA-500数据集上的表现优于现有技术，尤其在面向临床评估视网膜病理性变化的en-face投影重建方面，显著提升了OCTA图像的访问性、可靠性和诊断价值。

**Conclusion:** XOCT框架提出的改进方法增强了OCTA在眼科疾病检测和监测中的实用性，提升了对视网膜血管结构在多个空间尺度上的细节表现。

**Abstract:** Optical Coherence Tomography Angiography (OCTA) and its derived en-face
projections provide high-resolution visualization of the retinal and choroidal
vasculature, which is critical for the rapid and accurate diagnosis of retinal
diseases. However, acquiring high-quality OCTA images is challenging due to
motion sensitivity and the high costs associated with software modifications
for conventional OCT devices. Moreover, current deep learning methods for
OCT-to-OCTA translation often overlook the vascular differences across retinal
layers and struggle to reconstruct the intricate, dense vascular details
necessary for reliable diagnosis. To overcome these limitations, we propose
XOCT, a novel deep learning framework that integrates Cross-Dimensional
Supervision (CDS) with a Multi-Scale Feature Fusion (MSFF) network for
layer-aware vascular reconstruction. Our CDS module leverages 2D layer-wise
en-face projections, generated via segmentation-weighted z-axis averaging, as
supervisory signals to compel the network to learn distinct representations for
each retinal layer through fine-grained, targeted guidance. Meanwhile, the MSFF
module enhances vessel delineation through multi-scale feature extraction
combined with a channel reweighting strategy, effectively capturing vascular
details at multiple spatial scales. Our experiments on the OCTA-500 dataset
demonstrate XOCT's improvements, especially for the en-face projections which
are significant for clinical evaluation of retinal pathologies, underscoring
its potential to enhance OCTA accessibility, reliability, and diagnostic value
for ophthalmic disease detection and monitoring. The code is available at
https://github.com/uci-cbcl/XOCT.

</details>


### [69] [Bias-Aware Machine Unlearning: Towards Fairer Vision Models via Controllable Forgetting](https://arxiv.org/abs/2509.07456)
*Sai Siddhartha Chary Aylapuram,Veeraraju Elluru,Shivang Agarwal*

Main category: cs.CV

> Investigates methods to mitigate bias in deep learning models after training by selectively removing biased samples or features. Demonstrates significant improvements in fairness across various datasets.

<details>
  <summary>Details</summary>

**Motivation:** Deep neural networks may utilize spurious correlations from training data leading to biased and unfair predictions, especially in critical fields like medicine and autonomous driving.

**Method:** Bias-Aware Machine Unlearning is explored, which removes biased samples or representations post-hoc using methods like Gradient Ascent, LoRA, and Teacher-Student distillation.

**Result:** The method improves demographic parity significantly on CUB-200-2011, CIFAR-10, and CelebA datasets, with minimal accuracy loss.

**Conclusion:** Bias-Aware Machine Unlearning is a practical approach to mitigate bias in deployed vision systems without the need for full retraining.

**Abstract:** Deep neural networks often rely on spurious correlations in training data,
leading to biased or unfair predictions in safety-critical domains such as
medicine and autonomous driving. While conventional bias mitigation typically
requires retraining from scratch or redesigning data pipelines, recent advances
in machine unlearning provide a promising alternative for post-hoc model
correction. In this work, we investigate \textit{Bias-Aware Machine
Unlearning}, a paradigm that selectively removes biased samples or feature
representations to mitigate diverse forms of bias in vision models. Building on
privacy-preserving unlearning techniques, we evaluate various strategies
including Gradient Ascent, LoRA, and Teacher-Student distillation. Through
empirical analysis on three benchmark datasets, CUB-200-2011 (pose bias),
CIFAR-10 (synthetic patch bias), and CelebA (gender bias in smile detection),
we demonstrate that post-hoc unlearning can substantially reduce subgroup
disparities, with improvements in demographic parity of up to \textbf{94.86\%}
on CUB-200, \textbf{30.28\%} on CIFAR-10, and \textbf{97.37\%} on CelebA. These
gains are achieved with minimal accuracy loss and with methods scoring an
average of 0.62 across the 3 settings on the joint evaluation of utility,
fairness, quality, and privacy. Our findings establish machine unlearning as a
practical framework for enhancing fairness in deployed vision systems without
necessitating full retraining.

</details>


### [70] [ANYPORTAL: Zero-Shot Consistent Video Background Replacement](https://arxiv.org/abs/2509.07472)
*Wenshuo Gao,Xicheng Lan,Shuai Yang*

Main category: cs.CV

> The paper presents ANYPORTAL, a zero-shot video background replacement framework using pre-trained diffusion models with a focus on improving foreground consistency and achieving fine-grained control over video details.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the challenge of generating high-quality videos that precisely align with user intentions, particularly focusing on achieving fine-grained control over video details, which current methods struggle to accomplish.

**Method:** The paper introduces ANYPORTAL, a zero-shot framework that utilizes pre-trained diffusion models for video background replacement, integrating temporal and relighting capabilities to ensure high-quality video generation. It includes a Refinement Projection Algorithm for foreground consistency and pixel-level detail manipulation.

**Result:** The paper reports that ANYPORTAL achieves high-quality results on consumer-grade GPUs, providing a practical and efficient solution for video content creation and editing.

**Conclusion:** The conclusion of the paper is that ANYPORTAL, being training-free, effectively solves the problems of foreground consistency and temporally coherent relighting in video generation, offering significant practical advantages for video editing and content creation tasks.

**Abstract:** Despite the rapid advancements in video generation technology, creating
high-quality videos that precisely align with user intentions remains a
significant challenge. Existing methods often fail to achieve fine-grained
control over video details, limiting their practical applicability. We
introduce ANYPORTAL, a novel zero-shot framework for video background
replacement that leverages pre-trained diffusion models. Our framework
collaboratively integrates the temporal prior of video diffusion models with
the relighting capabilities of image diffusion models in a zero-shot setting.
To address the critical challenge of foreground consistency, we propose a
Refinement Projection Algorithm, which enables pixel-level detail manipulation
to ensure precise foreground preservation. ANYPORTAL is training-free and
overcomes the challenges of achieving foreground consistency and temporally
coherent relighting. Experimental results demonstrate that ANYPORTAL achieves
high-quality results on consumer-grade GPUs, offering a practical and efficient
solution for video content creation and editing.

</details>


### [71] [MedicalPatchNet: A Patch-Based Self-Explainable AI Architecture for Chest X-ray Classification](https://arxiv.org/abs/2509.07477)
*Patrick Wienholt,Christiane Kuhl,Jakob Nikolas Kather,Sven Nebelung,Daniel Truhn*

Main category: cs.CV

> MedicalPatchNet是一个针对胸X光片分类的自解释深度学习架构，它分割图像成Patch进行独立分类，并聚合结果，从而提供了透明的决策解释能力和卓越的病灶定位准确性。

<details>
  <summary>Details</summary>

**Motivation:** 传统的深度神经网络在医学影像分类方面表现出色，但可解释性差。作者旨在开发一种自解释架构，提高临床接受度和交互性，特别是在胸X光片分类中。

**Method:** MedicalPatchNet将医学影像分割为不重叠的Patch，分别对每个Patch进行分类，并将结果聚合以进行预测。此方法使模型能够直观地展示每个Patch对诊断的贡献，无需依赖后训练的解释技术。

**Result:** MedicalPatchNet在CheXpert数据集上达到了与EfficientNet-B0相当的分类性能(AUROC 0.907 vs. 0.908)，同时在CheXlocalize数据集上表现出更高的病灶定位准确性(平均命中率0.485 vs. 0.376)，展示了显著的可解释性提升。

**Conclusion:** 论文结论表明，MedicalPatchNet通过提供易于理解和验证的解释，提高了临床诊断中的信任度，为医学影像领域的可解释AI辅助诊断做出了贡献，并公开了代码和可复现的实验脚本。

**Abstract:** Deep neural networks excel in radiological image classification but
frequently suffer from poor interpretability, limiting clinical acceptance. We
present MedicalPatchNet, an inherently self-explainable architecture for chest
X-ray classification that transparently attributes decisions to distinct image
regions. MedicalPatchNet splits images into non-overlapping patches,
independently classifies each patch, and aggregates predictions, enabling
intuitive visualization of each patch's diagnostic contribution without
post-hoc techniques. Trained on the CheXpert dataset (223,414 images),
MedicalPatchNet matches the classification performance (AUROC 0.907 vs. 0.908)
of EfficientNet-B0, while substantially improving interpretability:
MedicalPatchNet demonstrates substantially improved interpretability with
higher pathology localization accuracy (mean hit-rate 0.485 vs. 0.376 with
Grad-CAM) on the CheXlocalize dataset. By providing explicit, reliable
explanations accessible even to non-AI experts, MedicalPatchNet mitigates risks
associated with shortcut learning, thus improving clinical trust. Our model is
publicly available with reproducible training and inference scripts and
contributes to safer, explainable AI-assisted diagnostics across medical
imaging domains. We make the code publicly available:
https://github.com/TruhnLab/MedicalPatchNet

</details>
