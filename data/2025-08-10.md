<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 15]
- [cs.CV](#cs.CV) [Total: 12]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM](https://arxiv.org/abs/2508.04795)
*Thomas Thebaud,Yen-Ju Lu,Matthew Wiesner,Peter Viechnicki,Najim Dehak*

Main category: cs.CL

> 本研究提出了一种新的对话转录后处理方法，通过结合冻结的音频基础模型和语言模型来添加说话人特征的元数据标签，如年龄、性别和情绪，从而丰富转录的对话内容。这种方法无需特定任务的微调，并达到了竞争性的性能表现。

<details>
  <summary>Details</summary>

**Motivation:** 现有的对话转录后处理主要侧重于改善语法、标点和可读性，而本研究旨在通过添加时间不变或时间变化的元数据标签来丰富对话的层次信息，从而提供更全面的对话理解支持。

**Method:** 该研究的方法是利用冻结的音频基础模型和LLAMA语言模型，通过轻量高效的连接器将音频和语言表征连接起来，以推断出对话中的说话人特征。

**Result:** 研究结果表明，利用冻结的LLAMA模型可以直接比较x-vector，在某些情境下实现了8.8%的等错误率，同时保持模块化和效率。

**Conclusion:** 研究证明，所提出的方法能有效地为转录后的对话添加元数据标签，提高对话理解的深度，而无需对基础模型进行特定任务的微调。

**Abstract:** In dialogue transcription pipelines, Large Language Models (LLMs) are
frequently employed in post-processing to improve grammar, punctuation, and
readability. We explore a complementary post-processing step: enriching
transcribed dialogues by adding metadata tags for speaker characteristics such
as age, gender, and emotion. Some of the tags are global to the entire
dialogue, while some are time-variant. Our approach couples frozen audio
foundation models, such as Whisper or WavLM, with a frozen LLAMA language model
to infer these speaker attributes, without requiring task-specific fine-tuning
of either model. Using lightweight, efficient connectors to bridge audio and
language representations, we achieve competitive performance on speaker
profiling tasks while preserving modularity and speed. Additionally, we
demonstrate that a frozen LLAMA model can compare x-vectors directly, achieving
an Equal Error Rate of 8.8% in some scenarios.

</details>


### [2] [Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization](https://arxiv.org/abs/2508.04796)
*Negar Foroutan,Clara Meister,Debjit Paul,Joel Niklaus,Sina Ahmadi,Antoine Bosselut,Rico Sennrich*

Main category: cs.CL

> 本文提出了一种新的分词方法 Parity-aware BPE，旨在解决传统 BPE 算法对资源较少的语言不公平的问题，通过最大化最差压缩语言的压缩增益实现了跨语言的分词平衡，实验证明该方法在维持较低压缩率的同时，未对语言模型性能产生负面影响。

<details>
  <summary>Details</summary>

**Motivation:** 标准的 BPE 算法依赖于基于频率的目标，这意味着占主导地位的语言在训练数据中的表现更佳，而资源较少的语言则会出现长度不一、形态不合理甚至充斥着未定义字符 (UNK) 的分词结果。这样的现象加剧了来自不同语言背景的用户在计算能力和财力上的不平等。

**Method:** 提出了一种名为Parity-aware Byte Pair Encoding (BPE) 的算法变体，该方法在每次合并步骤中最大化当前压缩效果最差的语言的压缩增益，以此来换取跨语言的均衡性。

**Result:** 实证研究显示，Parity-aware BPE 方法能够有效地在不同语言之间实现更为均衡的词汇计数，且对全球压缩率的影响微乎其微，并未对下游任务的语言模型性能造成显著影响。

**Conclusion:** Parity-aware BPE 提出了一个创新的方式来解决传统的 BPE 算法在处理多语言分词时存在的不均衡问题，保证了压缩效率的同时实现了语言间的公平性。

**Abstract:** Tokenization is the first -- and often least scrutinized -- step of most NLP
pipelines. Standard algorithms for learning tokenizers rely on frequency-based
objectives, which favor languages dominant in the training data and
consequently leave lower-resource languages with tokenizations that are
disproportionately longer, morphologically implausible, or even riddled with
<UNK> placeholders. This phenomenon ultimately amplifies computational and
financial inequalities between users from different language backgrounds. To
remedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of
the widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes
the compression gain of the currently worst-compressed language, trading a
small amount of global compression for cross-lingual parity. We find
empirically that Parity-aware BPE leads to more equitable token counts across
languages, with negligible impact on global compression rate and no substantial
effect on language-model performance in downstream tasks.

</details>


### [3] [Pitch Accent Detection improves Pretrained Automatic Speech Recognition](https://arxiv.org/abs/2508.04814)
*David Sasu,Natalie Schluter*

Main category: cs.CL

> 研究发现，通过联合训练ASR和音调重音检测模型，能够显著提升其各自的性能表现，尤其在改进重音检测和减少AR系统的词错率方面表现突出。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在提升基于半监督语音表示的ASR系统的性能，尤其是在有限资源的情况下，通过结合音调重音检测来增强语义理解。

**Method:** 研究提出了一个联合自动语音识别(ASR)和音调重音检测的模型，通过整合ASR与重音检测任务，提高两种任务的性能。

**Result:** 研究展示了使用半监督语音表示的自动语音识别(ASR)系统的性能可以通过引入联合ASR和音调重音检测模型得到提升。该模型的音调重音检测部分在任务上达到了显著的改进，F1分数上的差距缩小了41%。此外，在有限资源微调下，联合训练中的ASR性能将词错误率(WER)降低了28.3%。这些结果表明，扩展预训练语音模型以保留或重新学习关键的韵律线索（如音调重音）的重要性。

**Conclusion:** 研究证明了音调重音检测模块对于提升半监督ASR系统性能的重要性，并展示了通过联合训练保留或重新学习重音信息的必要性。

**Abstract:** We show the performance of Automatic Speech Recognition (ASR) systems that
use semi-supervised speech representations can be boosted by a complimentary
pitch accent detection module, by introducing a joint ASR and pitch accent
detection model. The pitch accent detection component of our model achieves a
significant improvement on the state-of-the-art for the task, closing the gap
in F1-score by 41%. Additionally, the ASR performance in joint training
decreases WER by 28.3% on LibriSpeech, under limited resource fine-tuning. With
these results, we show the importance of extending pretrained speech models to
retain or re-learn important prosodic cues such as pitch accent.

</details>


### [4] [Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History](https://arxiv.org/abs/2508.04826)
*Tommaso Tosato,Saskia Helbling,Yorguin-Jose Mantilla-Ramos,Mahmood Hegazy,Alberto Tosato,David John Lemay,Irina Rish,Guillaume Dumas*

Main category: cs.CL

> 提出了PERSIST框架评估语言模型的人格稳定性，发现即便非常大的模型在一致性上也有显著问题，这对安全需求高的应用提出了挑战。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在就安全部署需要一致的行为模式，然而，它们类似于人格特征尚不清楚，为此进行了全面性评估。

**Method:** 本文提出了PERSIST（PERsonality Stability in Synthetic Text），这是一个全面的评估框架，用于测试25个以上的开源语言模型（参数规模从1B到671B不等）超过500,000次的响应。研究使用了传统的（BFI-44, SD3）和新颖的适合LLM的人格测试工具，系统地改变了问题顺序、同义词使用、人设描述和推理模式。

**Result:** 研究发现挑战了基础的部署假设：1. 即便是400B参数以上的语言模型也表现出极大的响应差异（SD > 0.4）；2. 单纯改变提示词的顺序可以将人格测量值改变高达20%；3. 预期能稳定行为的干预措施，例如链式思考推理、详细的个性指导，以及包含对话历史，反而可能增加变异性；4. 适合LLM的新测试工具表现出与人类版本同样不稳定，确认了架构而非翻译上的限制。

**Conclusion:** 跨模型规模和缓解策略的这种持续的不稳定表明，现有语言模型缺乏真正行为一致性的基础。对于需要可预测行为的安全关键应用，这些发现表明基于人格的对齐策略可能是基本不足的。

**Abstract:** Large language models require consistent behavioral patterns for safe
deployment, yet their personality-like traits remain poorly understood. We
present PERSIST (PERsonality Stability in Synthetic Text), a comprehensive
evaluation framework testing 25+ open-source models (1B-671B parameters) across
500,000+ responses. Using traditional (BFI-44, SD3) and novel LLM-adapted
personality instruments, we systematically vary question order, paraphrasing,
personas, and reasoning modes. Our findings challenge fundamental deployment
assumptions: (1) Even 400B+ models exhibit substantial response variability (SD
> 0.4); (2) Minor prompt reordering alone shifts personality measurements by up
to 20%; (3) Interventions expected to stabilize behavior, such as
chain-of-thought reasoning, detailed personas instruction, inclusion of
conversation history, can paradoxically increase variability; (4) LLM-adapted
instruments show equal instability to human-centric versions, confirming
architectural rather than translational limitations. This persistent
instability across scales and mitigation strategies suggests current LLMs lack
the foundations for genuine behavioral consistency. For safety-critical
applications requiring predictable behavior, these findings indicate that
personality-based alignment strategies may be fundamentally inadequate.

</details>


### [5] [RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM Systems with Structured Memory](https://arxiv.org/abs/2508.04903)
*Jun Liu,Zhenglun Kong,Changdi Yang,Fan Yang,Tianqi Li,Peiyan Dong,Joannah Nanjekye,Hao Tang,Geng Yuan,Wei Niu,Wenbin Zhang,Pu Zhao,Xue Lin,Dong Huang,Yanzhi Wang*

Main category: cs.CL

> 本文提出了RCR-Router，一种新的多代理大语言模型协作框架，它可以动态选择相关的记忆子集，以减少令牌消耗和提高问答质量。实验表明，该框架能够减少令牌的使用量并保持或提高问答的质量。

<details>
  <summary>Details</summary>

**Motivation:** 现有大多数多代理大语言模型系统依赖静态或全上下文路由策略，这导致过多的令牌消耗、冗余的记忆暴露以及有限的适应性。研究动机在于通过提出一种更有效的路由方法来克服这些问题。

**Method:** 本研究提出了一种名为RCR-Router的模块化和基于角色的上下文路由框架，用于促进多代理大语言模型中的高效适应性协作。该框架可以根据代理的角色和任务阶段动态选择语义相关的记忆子集，同时还遵守严格的令牌预算。通过轻量级的评分策略指导记忆选择，并逐步将代理的输出集成到共享的记忆存储中，以促进渐进式的上下文细化。

**Result:** 该研究通过使用三个多跳问答基准测试（HotPotQA，MuSiQue 和 2WikiMultihop）证明了RCR-Router能够降低高达30%的令牌使用量，同时保持或提高问答质量。

**Conclusion:** 研究结果强调了在大规模多代理语言模型系统发展中的结构化记忆路由和输出感知评估的重要性。

**Abstract:** Multi-agent large language model (LLM) systems have shown strong potential in
complex reasoning and collaborative decision-making tasks. However, most
existing coordination schemes rely on static or full-context routing
strategies, which lead to excessive token consumption, redundant memory
exposure, and limited adaptability across interaction rounds. We introduce
RCR-Router, a modular and role-aware context routing framework designed to
enable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge,
this is the first routing approach that dynamically selects semantically
relevant memory subsets for each agent based on its role and task stage, while
adhering to a strict token budget. A lightweight scoring policy guides memory
selection, and agent outputs are iteratively integrated into a shared memory
store to facilitate progressive context refinement. To better evaluate model
behavior, we further propose an Answer Quality Score metric that captures
LLM-generated explanations beyond standard QA accuracy. Experiments on three
multi-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate
that RCR-Router reduces token usage (up to 30%) while improving or maintaining
answer quality. These results highlight the importance of structured memory
routing and output-aware evaluation in advancing scalable multi-agent LLM
systems.

</details>


### [6] [I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations](https://arxiv.org/abs/2508.04939)
*Julia Kharchenko,Tanya Roosta,Aman Chadha,Chirag Shah*

Main category: cs.CL

> 论文引入了一种全面的基准测试，用于评估大型语言模型（LLMs）对语言线索的敏感度以及由此产生的潜在歧视性偏差。

<details>
  <summary>Details</summary>

**Motivation:** 介绍了一种全面的基准测试，用于评估大型语言模型对语言特征的反应，这些特征可以无意中揭示性别、社会阶层等人口统计学属性。

**Method:** 通过精心设计的面试模拟，使用100个验证过的问答对，来展示LLMs如何系统性地惩罚某些语言模式，特别是犹豫性的语言，尽管这些语言的内容质量相当。

**Result:** 基准测试表明，犹豫性的回答平均得分低25.6%，证明了该基准测试在识别模型特定偏见方面的有效性。

**Conclusion:** 这项工作建立了一个基础框架，用于检测和衡量AI系统中的语言歧视，对于自动决策中的公平性有广泛的应用。

**Abstract:** This paper introduces a comprehensive benchmark for evaluating how Large
Language Models (LLMs) respond to linguistic shibboleths: subtle linguistic
markers that can inadvertently reveal demographic attributes such as gender,
social class, or regional background. Through carefully constructed interview
simulations using 100 validated question-response pairs, we demonstrate how
LLMs systematically penalize certain linguistic patterns, particularly hedging
language, despite equivalent content quality. Our benchmark generates
controlled linguistic variations that isolate specific phenomena while
maintaining semantic equivalence, which enables the precise measurement of
demographic bias in automated evaluation systems. We validate our approach
along multiple linguistic dimensions, showing that hedged responses receive
25.6% lower ratings on average, and demonstrate the benchmark's effectiveness
in identifying model-specific biases. This work establishes a foundational
framework for detecting and measuring linguistic discrimination in AI systems,
with broad applications to fairness in automated decision-making contexts.

</details>


### [7] [Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering](https://arxiv.org/abs/2508.04945)
*Louie Hong Yao,Nicholas Jarvis,Tianyu Jiang*

Main category: cs.CL

> 本文提出了一种视觉-语言聚类框架用于动词意义聚类，以更稳健地评估视觉活动识别系统的性能。

<details>
  <summary>Details</summary>

**Motivation:** 标准的精确匹配评估仅依赖于单一的正确答案，无法捕捉动词语义和图像解释中存在的歧义，导致对模型性能的评估不完整。为了应对这一挑战，本文提出了一种新的评估方法。

**Method:** 提出了一种视觉-语言聚类框架，用于构建动词意义聚类，以提供更为稳健的评估方法。

**Result:** 分析imSitu数据集得出每个图像平均映射到2.8个意义聚类，每个聚类代表图像的一种不同视角。人类一致性分析表明，基于聚类的评估更好地与人类判断相吻合，提供更细致的模型性能评估。

**Conclusion:** 实验结果表明，基于聚类的评估方法与人类判断更加一致，能更好地评估活动识别模型的性能。

**Abstract:** Evaluating visual activity recognition systems is challenging due to inherent
ambiguities in verb semantics and image interpretation. When describing actions
in images, synonymous verbs can refer to the same event (e.g., brushing vs.
grooming), while different perspectives can lead to equally valid but distinct
verb choices (e.g., piloting vs. operating). Standard exact-match evaluation,
which relies on a single gold answer, fails to capture these ambiguities,
resulting in an incomplete assessment of model performance. To address this, we
propose a vision-language clustering framework that constructs verb sense
clusters, providing a more robust evaluation. Our analysis of the imSitu
dataset shows that each image maps to an average of 2.8 sense clusters, with
each cluster representing a distinct perspective of the image. We evaluate
multiple activity recognition models and compare our cluster-based evaluation
with standard evaluation methods. Additionally, our human alignment analysis
suggests that the cluster-based evaluation better aligns with human judgements,
offering a more nuanced assessment of model performance.

</details>


### [8] [A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health](https://arxiv.org/abs/2508.05003)
*Song Wang,Yishu Wei,Haotian Ma,Max Lovitt,Kelly Deng,Yuan Meng,Zihan Xu,Jingze Zhang,Yunyu Xiao,Ying Ding,Xuhai Xu,Joydeep Ghosh,Yifan Peng*

Main category: cs.CL

> 本研究提出了一种多阶段的大型语言模型框架，用于从非结构化文本中提取社会健康决定因素（SDoH）。它在提取SDoH因素方面表现优越，并提高了模型的透明度。

<details>
  <summary>Details</summary>

**Motivation:** 理解影响自杀事件的社会健康决定因素（SDoH）对于早期干预和预防至关重要。然而，以数据驱动的方法实现这一目标面临一些挑战，如存在长尾的因子分布、分析在自杀事件之前的决定性压力，以及模型解释能力有限。

**Method:** 我们提出了一种多阶段的大型语言模型框架，用于从非结构化文本中增强社会健康决定因素（SDoH）的提取。我们的方法与其他最先进的语言模型（如预训练的BioBERT和GPT-3.5-turbo）以及推理模型（如DeepSeek-R1）进行了比较。我们还评估了模型的解释如何帮助人们更快更准确地标注SDoH因素。分析包括自动比较和初步用户研究。

**Result:** 结果表明，我们提出的框架在提取SDoH因素的总体任务以及提取相关上下文的细粒度任务中显示出了性能提升。此外，我们证明了对较小的任务特定模型进行微调可以实现相当甚至更好的性能，同时减少了推理成本。多阶段设计不仅提高了提取能力，而且提供了中间解释，增强了模型的解释性。

**Conclusion:** 我们的方法可以提高从非结构化文本中提取与自杀相关SDoH因素的准确性和透明度。这些进步有助于提前识别有风险的个体及制定更有效的预防策略。

**Abstract:** Background: Understanding social determinants of health (SDoH) factors
contributing to suicide incidents is crucial for early intervention and
prevention. However, data-driven approaches to this goal face challenges such
as long-tailed factor distributions, analyzing pivotal stressors preceding
suicide incidents, and limited model explainability. Methods: We present a
multi-stage large language model framework to enhance SDoH factor extraction
from unstructured text. Our approach was compared to other state-of-the-art
language models (i.e., pre-trained BioBERT and GPT-3.5-turbo) and reasoning
models (i.e., DeepSeek-R1). We also evaluated how the model's explanations help
people annotate SDoH factors more quickly and accurately. The analysis included
both automated comparisons and a pilot user study. Results: We show that our
proposed framework demonstrated performance boosts in the overarching task of
extracting SDoH factors and in the finer-grained tasks of retrieving relevant
context. Additionally, we show that fine-tuning a smaller, task-specific model
achieves comparable or better performance with reduced inference costs. The
multi-stage design not only enhances extraction but also provides intermediate
explanations, improving model explainability. Conclusions: Our approach
improves both the accuracy and transparency of extracting suicide-related SDoH
from unstructured texts. These advancements have the potential to support early
identification of individuals at risk and inform more effective prevention
strategies.

</details>


### [9] [Dialogues Aspect-based Sentiment Quadruple Extraction via Structural Entropy Minimization Partitioning](https://arxiv.org/abs/2508.05023)
*Kun Peng,Cong Cao,Hao Peng,Zhifeng Hao,Lei Jiang,Kongjing Gu,Yanbing Liu,Philip S. Yu*

Main category: cs.CL

> The paper targets the extraction of sentiment quadruples from dialogues by partitioning dialogues into semantically independent parts, using a structural entropy minimization algorithm and a two-step extraction framework, achieving state-of-the-art performance with lower computation.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the existing methods' limitation of learning word relations across entire dialogues, which introduces additional noise due to the presence of semantically independent sub-dialogues.

**Method:** Our method focuses on partitioning dialogues into semantically independent sub-dialogues using a structural entropy minimization algorithm. Then, a two-step framework for quadruple extraction is introduced: first extracting individual sentiment elements at the utterance level, then matching quadruples at the sub-dialogue level.

**Result:** Experiments show that their approach achieves state-of-the-art performance in DiaASQ with much lower computational costs.

**Conclusion:** The paper concludes that by partitioning dialogues into semantically independent sub-dialogues and using a two-step framework for quadruple extraction, they are able to achieve superior performance and lower computational costs in the DiaASQ task.

**Abstract:** Dialogues Aspect-based Sentiment Quadruple Extraction (DiaASQ) aims to
extract all target-aspect-opinion-sentiment quadruples from a given
multi-round, multi-participant dialogue. Existing methods typically learn word
relations across entire dialogues, assuming a uniform distribution of sentiment
elements. However, we find that dialogues often contain multiple semantically
independent sub-dialogues without clear dependencies between them. Therefore,
learning word relationships across the entire dialogue inevitably introduces
additional noise into the extraction process. To address this, our method
focuses on partitioning dialogues into semantically independent sub-dialogues.
Achieving completeness while minimizing these sub-dialogues presents a
significant challenge. Simply partitioning based on reply relationships is
ineffective. Instead, we propose utilizing a structural entropy minimization
algorithm to partition the dialogues. This approach aims to preserve relevant
utterances while distinguishing irrelevant ones as much as possible.
Furthermore, we introduce a two-step framework for quadruple extraction: first
extracting individual sentiment elements at the utterance level, then matching
quadruples at the sub-dialogue level. Extensive experiments demonstrate that
our approach achieves state-of-the-art performance in DiaASQ with much lower
computational costs.

</details>


### [10] [Evaluation of LLMs in AMR Parsing](https://arxiv.org/abs/2508.05028)
*Shu Han Ho*

Main category: cs.CL

> 该论文评估了通过简单微调仅解码器的大型语言模型(LLMs)进行AMR解析的性能，显示了LLaMA 3.2的表现与最新技术(SOTA)AMR解析器相当，达到SMATCH F1：0.804。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机是评估简单微调解码器大型语言模型在AMR解析任务上的有效性，探索这一方法是否可以作为一种简单但强大的解决方案，替代现有的复杂方法。

**Method:** 论文中研究者微调了四个不同的大型语言模型架构（Phi 3.5、Gemma 2、LLaMA 3.2、DeepSeek R1 LLaMA Distilled），并在LDC2020T02 Gold AMR3.0测试集上进行评估。

**Result:** 研究结果表明，通过对仅解码器的大型语言模型进行简单微调可以达到与复杂状态的艺术(AMR)解析器相当的表现。LLaMA 3.2表现出了与领先技术AMR解析器相近的性能。

**Conclusion:** LLaMA 3.2在语义性能方面领先，而Phi 3.5则在结构有效性方面表现更好。这表明简单微调方法可以在AMR解析任务上达到和复杂方法相似的性能水平。

**Abstract:** Meaning Representation (AMR) is a semantic formalism that encodes sentence
meaning as rooted, directed, acyclic graphs, where nodes represent concepts and
edges denote semantic relations. Finetuning decoder only Large Language Models
(LLMs) represent a promising novel straightfoward direction for AMR parsing.
This paper presents a comprehensive evaluation of finetuning four distinct LLM
architectures, Phi 3.5, Gemma 2, LLaMA 3.2, and DeepSeek R1 LLaMA Distilled
using the LDC2020T02 Gold AMR3.0 test set. Our results have shown that
straightfoward finetuning of decoder only LLMs can achieve comparable
performance to complex State of the Art (SOTA) AMR parsers. Notably, LLaMA 3.2
demonstrates competitive performance against SOTA AMR parsers given a
straightforward finetuning approach. We achieved SMATCH F1: 0.804 on the full
LDC2020T02 test split, on par with APT + Silver (IBM) at 0.804 and approaching
Graphene Smatch (MBSE) at 0.854. Across our analysis, we also observed a
consistent pattern where LLaMA 3.2 leads in semantic performance while Phi 3.5
excels in structural validity.

</details>


### [11] [Align, Don't Divide: Revisiting the LoRA Architecture in Multi-Task Learning](https://arxiv.org/abs/2508.05078)
*Jinda Liu,Bo Cheng,Yi Chang,Yuan Wu*

Main category: cs.CL

> 研究人员挑战了现有通过多适配器或多头结构来完成多任务学习的有效性，并提出了Align-LoRA方法，通过行列对齐来共享表示，显示出更强的性能。

<details>
  <summary>Details</summary>

**Motivation:** 该研究挑战了当前主流的多组件参数高效微调（PEFT）方法，该方法倾向于使用多适配器或多头结构来处理来自多个领域的多样化任务。作者希望通过简化模型架构和提高单适配器的秩来提高性能。

**Method:** 论文提出了一个新的假设，即有效的多任务学习泛化依赖于学习坚实的共享表示，而不只是隔离任务特定特征。为此，作者提出了一种新的方法：Align-LoRA，它通过在共享适配器空间内引入显式的对齐损失来统一任务表示。

**Result:** 研究发现，简化后的多头架构以及增加秩的单适配器方法在多任务学习中表现良好，并提出了Align-LoRA模型，展示了在多任务适应方面的强大性能。

**Conclusion:** 实验结果表明，不管是简化后的多头架构，还是增加秩的单适配器方式，都能实现较强的性能。Align-LoRA能够在共享适配器空间内使任务表示对齐，从而显著超越基线模型，提供了一个更简单而有效的多任务适应大语言模型的方法。

**Abstract:** Parameter-Efficient Fine-Tuning (PEFT) is essential for adapting Large
Language Models (LLMs). In practice, LLMs are often required to handle a
diverse set of tasks from multiple domains, a scenario naturally addressed by
multi-task learning (MTL). Within this MTL context, a prevailing trend involves
LoRA variants with multiple adapters or heads, which advocate for structural
diversity to capture task-specific knowledge. Our findings present a direct
challenge to this paradigm. We first show that a simplified multi-head
architecture with high inter-head similarity substantially outperforms complex
multi-adapter and multi-head systems. This leads us to question the
multi-component paradigm itself, and we further demonstrate that a standard
single-adapter LoRA, with a sufficiently increased rank, also achieves highly
competitive performance. These results lead us to a new hypothesis: effective
MTL generalization hinges on learning robust shared representations, not
isolating task-specific features. To validate this, we propose Align-LoRA,
which incorporates an explicit loss to align task representations within the
shared adapter space. Experiments confirm that Align-LoRA significantly
surpasses all baselines, establishing a simpler yet more effective paradigm for
adapting LLMs to multiple tasks. The code is available at
https://github.com/jinda-liu/Align-LoRA.

</details>


### [12] [Multimodal Fact Checking with Unified Visual, Textual, and Contextual Representations](https://arxiv.org/abs/2508.05097)
*Aditya Kishore,Gaurav Kumar,Jasabanta Patro*

Main category: cs.CL

> A unified framework for fine-grained multimodal fact verification is proposed, achieving a weighted F1 score of 0.84 on the Factify 2 dataset.

<details>
  <summary>Details</summary>

**Motivation:** The growing rate of multimodal misinformation, supported by both text and images, poses significant challenges to fact-checking systems that rely primarily on textual evidence.

**Method:** Our architecture combines dedicated encoders for text and images with a fusion module that captures cross-modal relationships using element-wise interactions. A classification head then predicts the veracity of a claim, supported by a contrastive learning objective.

**Result:** Achieving a weighted F1 score of 0.84, substantially outperforming the baseline.

**Conclusion:** These results highlight the effectiveness of explicit multimodal reasoning and demonstrate the potential of our approach for scalable and interpretable fact-checking in complex, real-world scenarios.

**Abstract:** The growing rate of multimodal misinformation, where claims are supported by
both text and images, poses significant challenges to fact-checking systems
that rely primarily on textual evidence. In this work, we have proposed a
unified framework for fine-grained multimodal fact verification called
"MultiCheck", designed to reason over structured textual and visual signals.
Our architecture combines dedicated encoders for text and images with a fusion
module that captures cross-modal relationships using element-wise interactions.
A classification head then predicts the veracity of a claim, supported by a
contrastive learning objective that encourages semantic alignment between
claim-evidence pairs in a shared latent space. We evaluate our approach on the
Factify 2 dataset, achieving a weighted F1 score of 0.84, substantially
outperforming the baseline. These results highlight the effectiveness of
explicit multimodal reasoning and demonstrate the potential of our approach for
scalable and interpretable fact-checking in complex, real-world scenarios.

</details>


### [13] [BEE-RAG: Balanced Entropy Engineering for Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05100)
*Yuhao Wang,Ruiyang Ren,Yucheng Wang,Jing Liu,Wayne Xin Zhao,Hua Wu,Haifeng Wang*

Main category: cs.CL

> 本文提出BEE-RAG框架，通过熵不变性原则提高检索增强生成（RAG）系统在不同上下文长度下的性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于检索返回的信息量通常较大，RAG倾向于使用较长的上下文长度，作者从熵工程的角度指出了由于较长检索上下文导致的无约束熵增长和注意力稀释是影响RAG性能的重要因素。

**Method:** 该论文提出了平衡熵工程的RAG（BEE-RAG）框架，通过熵不变性原则提高RAG系统在不同上下文长度下的适应性。通过利用平衡上下文熵重新定义注意力动态，BEE-RAG将注意力敏感度与上下文长度分离，确保了稳定的熵水平。同时，提出了多重要性估计的零样本推断策略和参数高效的自适应微调机制，以获得不同设置下的最优平衡因子。

**Result:** 多项RAG任务上的广泛实验表明BEE-RAG的有效性。

**Conclusion:** 实验结果证明了BEE-RAG在处理长检索上下文时能够提高注意力机制的稳定性和效果。

**Abstract:** With the rapid advancement of large language models (LLMs),
retrieval-augmented generation (RAG) has emerged as a critical approach to
supplement the inherent knowledge limitations of LLMs. However, due to the
typically large volume of retrieved information, RAG tends to operate with long
context lengths. From the perspective of entropy engineering, we identify
unconstrained entropy growth and attention dilution due to long retrieval
context as significant factors affecting RAG performance. In this paper, we
propose the balanced entropy-engineered RAG (BEE-RAG) framework, which improves
the adaptability of RAG systems to varying context lengths through the
principle of entropy invariance. By leveraging balanced context entropy to
reformulate attention dynamics, BEE-RAG separates attention sensitivity from
context length, ensuring a stable entropy level. Building upon this, we
introduce a zero-shot inference strategy for multi-importance estimation and a
parameter-efficient adaptive fine-tuning mechanism to obtain the optimal
balancing factor for different settings. Extensive experiments across multiple
RAG tasks demonstrate the effectiveness of BEE-RAG.

</details>


### [14] [Attention Basin: Why Contextual Position Matters in Large Language Models](https://arxiv.org/abs/2508.05128)
*Zihao Yi,Delong Zeng,Zhenqing Ling,Haohao Luo,Zhe Xu,Wei Liu,Jian Luan,Wanxia Cao,Ying Shen*

Main category: cs.CL

> 研究揭示大语言模型中存在关注基准现象，提出了AttnRank方法，通过重新排序输入内容来优化模型性能，实验证明该方法有效。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于理解和解决大语言模型对输入信息位置敏感的问题，以改善模型表现。

**Method:** 该研究通过广泛实验揭示了大语言模型输入中信息位置对模型性能的影响，并提出了Attention-Driven Reranking (AttnRank) 两阶段框架来提高模型性能。

**Result:** 实验结果显示，AttnRank 在多跳问答和少样本学习任务中跨越不同架构和规模的 10 个大语言模型均取得显著提升。

**Conclusion:** AttnRank 是一个模型无关、无需训练、可直接插入且计算开销小的方法，能有效提升大模型性能。

**Abstract:** The performance of Large Language Models (LLMs) is significantly sensitive to
the contextual position of information in the input. To investigate the
mechanism behind this positional bias, our extensive experiments reveal a
consistent phenomenon we term the attention basin: when presented with a
sequence of structured items (e.g., retrieved documents or few-shot examples),
models systematically assign higher attention to the items at the beginning and
end of the sequence, while neglecting those in the middle. Crucially, our
analysis further reveals that allocating higher attention to critical
information is key to enhancing model performance. Based on these insights, we
introduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i)
estimates a model's intrinsic positional attention preferences using a small
calibration set, and (ii) reorders retrieved documents or few-shot examples to
align the most salient content with these high-attention positions. AttnRank is
a model-agnostic, training-free, and plug-and-play method with minimal
computational overhead. Experiments on multi-hop QA and few-shot in-context
learning tasks demonstrate that AttnRank achieves substantial improvements
across 10 large language models of varying architectures and scales, without
modifying model parameters or training procedures.

</details>


### [15] [Towards Assessing Medical Ethics from Knowledge to Practice](https://arxiv.org/abs/2508.05132)
*Chang Hong,Minghao Wu,Qingying Xiao,Yuchi Wang,Xiang Wan,Guangjun Yu,Benyou Wang,Yan Hu*

Main category: cs.CL

> 论文提出PrinciplismQA，一个系统评估LLMs在医学伦理方面表现的基准测试。实验显示，尽管前沿模型在基准测试中表现领先，但在应用伦理原则方面仍存在不足，特别是在动态应用Principlism中的Beneficence原则。

<details>
  <summary>Details</summary>

**Motivation:** 整合大型语言模型到医疗保健需要严格评估其伦理推理能力，而当前的基准测试常常忽视这一点。

**Method:** 介绍了PrinciplismQA，这是一个全面的基准测试，包含了3,648个问题，旨在系统地评估LLMs在核心医学伦理方面的对齐度。基准测试基于Principlism原则，包括多选题和开放式问题，这些问题分别来源于权威教科书和医学伦理案例研究文献，并由医学专家验证。

**Result:** 实验揭示了模型在伦理知识和实践应用之间存在显著差距，尤其是在动态应用伦理原则到现实世界场景方面。大多数LLMs在涉及Beneficence的难题上挣扎，常常过于强调其他原则。前沿的闭源模型在基准测试中领先。

**Conclusion:** PrinciplismQA提供了一个可扩展的框架，用于诊断这些特定的伦理缺陷，为更平衡和负责任的医疗AI铺平道路。

**Abstract:** The integration of large language models into healthcare necessitates a
rigorous evaluation of their ethical reasoning, an area current benchmarks
often overlook. We introduce PrinciplismQA, a comprehensive benchmark with
3,648 questions designed to systematically assess LLMs' alignment with core
medical ethics. Grounded in Principlism, our benchmark features a high-quality
dataset. This includes multiple-choice questions curated from authoritative
textbooks and open-ended questions sourced from authoritative medical ethics
case study literature, all validated by medical experts. Our experiments reveal
a significant gap between models' ethical knowledge and their practical
application, especially in dynamically applying ethical principles to
real-world scenarios. Most LLMs struggle with dilemmas concerning Beneficence,
often over-emphasizing other principles. Frontier closed-source models, driven
by strong general capabilities, currently lead the benchmark. Notably, medical
domain fine-tuning can enhance models' overall ethical competence, but further
progress requires better alignment with medical ethical knowledge.
PrinciplismQA offers a scalable framework to diagnose these specific ethical
weaknesses, paving the way for more balanced and responsible medical AI.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [16] [RetinexDual: Retinex-based Dual Nature Approach for Generalized Ultra-High-Definition Image Restoration](https://arxiv.org/abs/2508.04797)
*Mohab Kishawy,Ali Abdellatif Hussein,Jun Chen*

Main category: cs.CV

> 提出RetinexDual框架，利用互补的SAMBA和FIA子网络解决超高清图像恢复难题，展现优于现有方法的性能。

<details>
  <summary>Details</summary>

**Motivation:** 传统的图像降采样和频域转换方法在处理超高清图像时面临不可逆的信息损失和处理空间局限性图像瑕疵的不足。

**Method:** RetinexDual采用基于Retinex理论的框架，包含两个互补子网络：SAMBA和FIA。SAMBA用于反射分量的校正，利用粗细机制来减少因果建模带来的瑕疵。FIA则负责在频域内进行精确的颜色和光照矫正，利用全局上下文来确保充足的修复效果。

**Result:** RetinexDual在四项超高清图像恢复任务（去雨、去模糊、去雾、低光图像增强）中，无论是定性还是定量评价都优于近期的方法。消融研究显示每个分支的独特设计及框架各组件的有效性。

**Conclusion:** 基于Retinex理论的RetinexDual框架，结合SAMBA和FIA子网络，有效克服了传统方法在超高清图像恢复中的局限性，显示了其在多种任务上的优越性能。

**Abstract:** Advancements in image sensing have elevated the importance of
Ultra-High-Definition Image Restoration (UHD IR). Traditional methods, such as
extreme downsampling or transformation from the spatial to the frequency
domain, encounter significant drawbacks: downsampling induces irreversible
information loss in UHD images, while our frequency analysis reveals that pure
frequency-domain approaches are ineffective for spatially confined image
artifacts, primarily due to the loss of degradation locality. To overcome these
limitations, we present RetinexDual, a novel Retinex theory-based framework
designed for generalized UHD IR tasks. RetinexDual leverages two complementary
sub-networks: the Scale-Attentive maMBA (SAMBA) and the Frequency Illumination
Adaptor (FIA). SAMBA, responsible for correcting the reflectance component,
utilizes a coarse-to-fine mechanism to overcome the causal modeling of mamba,
which effectively reduces artifacts and restores intricate details. On the
other hand, FIA ensures precise correction of color and illumination
distortions by operating in the frequency domain and leveraging the global
context provided by it. Evaluating RetinexDual on four UHD IR tasks, namely
deraining, deblurring, dehazing, and Low-Light Image Enhancement (LLIE), shows
that it outperforms recent methods qualitatively and quantitatively. Ablation
studies demonstrate the importance of employing distinct designs for each
branch in RetinexDual, as well as the effectiveness of its various components.

</details>


### [17] [ACM Multimedia Grand Challenge on ENT Endoscopy Analysis](https://arxiv.org/abs/2508.04801)
*Trong-Thuan Nguyen,Viet-Tham Huynh,Thao Thi Phuong Dao,Ha Nguyen Thi,Tien To Vu Thuy,Uyen Hanh Tran,Tam V. Nguyen,Thanh Dinh Le,Minh-Triet Tran*

Main category: cs.CV

> 本文提出了ENTRep挑战，针对耳鼻喉内窥镜图像分析的自动化问题，提供了精细的解剖分类和多语言支持的检索能力。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机在于解决耳鼻喉内窥镜图像分析中的自动化分析问题，这个问题因为设备和操作者之间的可变性、症状的微妙性和局域性、解剖细节差异性等问题而进展缓慢。现有的公开基准很少支持这些功能。

**Method:** 本文提出了ENTRep，这是ACM Multimedia 2025 耳鼻喉内窥镜分析大赛的挑战。该挑战集成了精细的解剖分类，以及在双语（越南语和英语）临床监督下的图像对图像和文本对图像检索能力。

**Result:** 作者定义了三项基准任务，标准化提交流程，并结合公共和私有测试集上的服务器端评分来评估性能。此外，本文报告了顶尖团队的结果，并提供了深度讨论。

**Conclusion:** 综上所述，本文提出的ENTRep挑战首次涵盖了耳鼻喉内窥镜图像解剖学分类和双语临床监督下的图像和文本检索功能，推动了解析和检索工具的发展，以适应临床需求。

**Abstract:** Automated analysis of endoscopic imagery is a critical yet underdeveloped
component of ENT (ear, nose, and throat) care, hindered by variability in
devices and operators, subtle and localized findings, and fine-grained
distinctions such as laterality and vocal-fold state. In addition to
classification, clinicians require reliable retrieval of similar cases, both
visually and through concise textual descriptions. These capabilities are
rarely supported by existing public benchmarks. To this end, we introduce
ENTRep, the ACM Multimedia 2025 Grand Challenge on ENT endoscopy analysis,
which integrates fine-grained anatomical classification with image-to-image and
text-to-image retrieval under bilingual (Vietnamese and English) clinical
supervision. Specifically, the dataset comprises expert-annotated images,
labeled for anatomical region and normal or abnormal status, and accompanied by
dual-language narrative descriptions. In addition, we define three benchmark
tasks, standardize the submission protocol, and evaluate performance on public
and private test splits using server-side scoring. Moreover, we report results
from the top-performing teams and provide an insight discussion.

</details>


### [18] [CoMAD: A Multiple-Teacher Self-Supervised Distillation Framework](https://arxiv.org/abs/2508.04816)
*Sriram Mandalika,Lalitha V*

Main category: cs.CV

> 介绍了共识导向掩码蒸馏(CoMAD)框架，该框架将多个最先进的自监督视觉Transformer（ViT-Base）的知识蒸馏到一个紧凑的学生网络中，以解决独立预训练带来的限制并实现更高效的资源使用。

<details>
  <summary>Details</summary>

**Motivation:** 克服现有的自监督学习范式如对比学习和掩码图像建模的局限性，这些范式通常独立预训练，忽视了互补性的见解，且生成的模型过于庞大，不易适用于资源受限的部署环境。

**Method:** 引入了一种名为共识导向掩码蒸馏（CoMAD）的轻量、无参数框架，该框架统一了多个当前最先进的自监督视觉Transformer（ViT-Base）教师模型的知识，并将其蒸馏到一个小巧的学生网络中。CoMAD从三个预训练的教师模型：MAE、MoCo v3 和 iBOT中进行蒸馏，这三个模型分别提供了不同的语义和上下文先验。它通过非对称掩码，即学生仅看到25%的图像块而每个教师接受一个逐渐减轻的独特掩码，使学生在网络丰富的上下文中完成缺失特征。教师模型的嵌入通过线性适配器和层归一化对齐到学生模型的空间，并通过联合共识门控融合，联合共识门控根据教师模型之间的余弦相似性和一致性加权每个token。学生模型通过对可见token和重构特征图的双层次KL散度进行训练，捕捉局部和全局结构。

**Result:** 在ImageNet-1K上的实验表明，CoMAD的学生模型ViT-Tiny达到了75.4%的Top-1准确率，比之前的最先进方法提升了0.4%。在密集预测转移任务中，它在ADE20K上达到了47.3%的mIoU，在MS-COCO上分别取得了44.5%的box平均精度和40.5%的mask平均精度，确立了在紧凑型SSL蒸馏领域的最先进地位。

**Conclusion:** CoMAD框架通过一个创新的知识整合方法提高了小模型的性能，展示了在自监督学习和模型蒸馏领域的潜力，特别是对于那些资源受限的场景。该方法在多个主流视觉任务上的表现展示了它在实际应用场景中的价值。

**Abstract:** Numerous self-supervised learning paradigms, such as contrastive learning and
masked image modeling, learn powerful representations from unlabeled data but
are typically pretrained in isolation, overlooking complementary insights and
yielding large models that are impractical for resource-constrained deployment.
To overcome these challenges, we introduce Consensus-oriented Masked
Distillation (CoMAD), a lightweight, parameter-free framework that unifies
knowledge from multiple current state-of-the-art self-supervised Vision
Transformers into a compact student network. CoMAD distills from three
pretrained ViT-Base teachers, MAE, MoCo v3, and iBOT, each offering distinct
semantic and contextual priors. Rather than naively averaging teacher outputs,
we apply asymmetric masking: the student sees only 25 percent of patches while
each teacher receives a progressively lighter, unique mask, forcing the student
to interpolate missing features under richer contexts. Teacher embeddings are
aligned to the student's space via a linear adapter and layer normalization,
then fused through our joint consensus gating, which weights each token by
combining cosine affinity with inter-teacher agreement. The student is trained
with dual-level KL divergence on visible tokens and reconstructed feature maps,
capturing both local and global structure. On ImageNet-1K, CoMAD's ViT-Tiny
achieves 75.4 percent Top-1, an increment of 0.4 percent over the previous
state-of-the-art. In dense-prediction transfers, it attains 47.3 percent mIoU
on ADE20K, and 44.5 percent box average precision and 40.5 percent mask average
precision on MS-COCO, establishing a new state-of-the-art in compact SSL
distillation.

</details>


### [19] [Single-Step Reconstruction-Free Anomaly Detection and Segmentation via Diffusion Models](https://arxiv.org/abs/2508.04818)
*Mehrdad Moradi,Marco Grasso,Bianca Maria Colosimo,Kamran Paynabar*

Main category: cs.CV

> 该研究提出了RADAR方法，通过基于注意力机制的扩散模型解决了现有异常检测中的几个主要挑战，能够实时、更准确地检测出异常。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有基于重构的异常检测方法存在计算成本高、容易误判以及选择合适噪声水平困难的问题，研究者们提出了一种新的、无重构的异常检测方法，以解决以上问题并进一步提升检测性能。

**Method:** RADAR方法利用基于注意力机制的扩散模型直接生成异常图，从而避免了现有重构方法中多次采样带来的计算复杂度，并且能够直接从扩散模型中获得异常映射，提高了检测准确性和计算效率。

**Result:** 该研究引入了一种名为RADAR的方法，通过基于注意力机制的扩散模型实现了无重构实时异常检测，解决了现有重构方法计算复杂、可能出现误判及选择合适的噪声水平困难的问题。RADAR在多个评估指标上超越了现有的扩散模型和统计机器学习方法，特别是在MVTec-AD数据集上F1分数提升了7%，在3D打印材料数据集上提升了13%。

**Conclusion:** RADAR方法在MVTec-AD和3D打印材料数据集中超越了现有的扩散模型和统计机器学习方法，证明了其优越的异常检测能力和计算效率。

**Abstract:** Generative models have demonstrated significant success in anomaly detection
and segmentation over the past decade. Recently, diffusion models have emerged
as a powerful alternative, outperforming previous approaches such as GANs and
VAEs. In typical diffusion-based anomaly detection, a model is trained on
normal data, and during inference, anomalous images are perturbed to a
predefined intermediate step in the forward diffusion process. The
corresponding normal image is then reconstructed through iterative reverse
sampling.
  However, reconstruction-based approaches present three major challenges: (1)
the reconstruction process is computationally expensive due to multiple
sampling steps, making real-time applications impractical; (2) for complex or
subtle patterns, the reconstructed image may correspond to a different normal
pattern rather than the original input; and (3) Choosing an appropriate
intermediate noise level is challenging because it is application-dependent and
often assumes prior knowledge of anomalies, an assumption that does not hold in
unsupervised settings.
  We introduce Reconstruction-free Anomaly Detection with Attention-based
diffusion models in Real-time (RADAR), which overcomes the limitations of
reconstruction-based anomaly detection. Unlike current SOTA methods that
reconstruct the input image, RADAR directly produces anomaly maps from the
diffusion model, improving both detection accuracy and computational
efficiency. We evaluate RADAR on real-world 3D-printed material and the
MVTec-AD dataset. Our approach surpasses state-of-the-art diffusion-based and
statistical machine learning models across all key metrics, including accuracy,
precision, recall, and F1 score. Specifically, RADAR improves F1 score by 7% on
MVTec-AD and 13% on the 3D-printed material dataset compared to the next best
model.
  Code available at: https://github.com/mehrdadmoradi124/RADAR

</details>


### [20] [A deep learning approach to track eye movements based on events](https://arxiv.org/abs/2508.04827)
*Chirag Seth,Divya Naiken,Keyan Lin*

Main category: cs.CV

> 研究通过深度学习方法，尤其是CNN_LSTM模型，开发了一个成本效益高的算法来预测人类注意力，实现81%的准确率，并提出使用LRP来提升模型的解释性。

<details>
  <summary>Details</summary>

**Motivation:** 精确的眼球追踪通常需要昂贵和高速的摄像头，因此研究的目的是开发一个可解释且低成本的算法，利用事件相机的输入来定位眼睛中心位置(x, y)。

**Method:** 使用深度学习方法，特别是CNN_LSTM模型，来预测人类注意力，从而改善设备舒适度和用户体验。

**Result:** 提出的CNN_LSTM模型取得了约81%的准确率。

**Conclusion:** 研究提出了未来工作将集中在通过图层相关传播(LRP)来进一步提升模型的可解释性和预测性能。

**Abstract:** This research project addresses the challenge of accurately tracking eye
movements during specific events by leveraging previous research. Given the
rapid movements of human eyes, which can reach speeds of 300{\deg}/s, precise
eye tracking typically requires expensive and high-speed cameras. Our primary
objective is to locate the eye center position (x, y) using inputs from an
event camera. Eye movement analysis has extensive applications in consumer
electronics, especially in VR and AR product development. Therefore, our
ultimate goal is to develop an interpretable and cost-effective algorithm using
deep learning methods to predict human attention, thereby improving device
comfort and enhancing overall user experience. To achieve this goal, we
explored various approaches, with the CNN\_LSTM model proving most effective,
achieving approximately 81\% accuracy. Additionally, we propose future work
focusing on Layer-wise Relevance Propagation (LRP) to further enhance the
model's interpretability and predictive performance.

</details>


### [21] [LuKAN: A Kolmogorov-Arnold Network Framework for 3D Human Motion Prediction](https://arxiv.org/abs/2508.04847)
*Md Zahidul Hasan,A. Ben Hamza,Nizar Bouguila*

Main category: cs.CV

> LuKAN is an efficient model for 3D human motion prediction using a combination of wavelet transform, KANs with Lucas polynomials, and spatial projections, achieving competitive accuracy while maintaining computational efficiency.

<details>
  <summary>Details</summary>

**Motivation:** The motivation of this paper is to address the limitations of existing 3D human motion prediction models, which struggle to balance between prediction accuracy and computational efficiency.

**Method:** Our model, LuKAN, consists of several key components: discrete wavelet transform, spatial projection layer, Temporal Dependency Learner with KAN layers using Lucas polynomials, and inverse discrete wavelet transform to generate coherent motion predictions.

**Result:** Experiments on three benchmark datasets show that LuKAN outperforms or matches the performance of strong baselines with both higher accuracy and lower computational cost.

**Conclusion:** The paper concludes that LuKAN achieves a balance between prediction accuracy and computational efficiency, demonstrating competitive performance through extensive benchmark testing.

**Abstract:** The goal of 3D human motion prediction is to forecast future 3D poses of the
human body based on historical motion data. Existing methods often face
limitations in achieving a balance between prediction accuracy and
computational efficiency. In this paper, we present LuKAN, an effective model
based on Kolmogorov-Arnold Networks (KANs) with Lucas polynomial activations.
Our model first applies the discrete wavelet transform to encode temporal
information in the input motion sequence. Then, a spatial projection layer is
used to capture inter-joint dependencies, ensuring structural consistency of
the human body. At the core of LuKAN is the Temporal Dependency Learner, which
employs a KAN layer parameterized by Lucas polynomials for efficient function
approximation. These polynomials provide computational efficiency and an
enhanced capability to handle oscillatory behaviors. Finally, the inverse
discrete wavelet transform reconstructs motion sequences in the time domain,
generating temporally coherent predictions. Extensive experiments on three
benchmark datasets demonstrate the competitive performance of our model
compared to strong baselines, as evidenced by both quantitative and qualitative
evaluations. Moreover, its compact architecture coupled with the linear
recurrence of Lucas polynomials, ensures computational efficiency.

</details>


### [22] [VER-Bench: Evaluating MLLMs on Reasoning with Fine-Grained Visual Evidence](https://arxiv.org/abs/2508.04852)
*Chenhui Qiang,Zhaoyang Wei,Xumeng Han Zipeng Wang,Siyao Li,Xiangyuan Lan,Jianbin Jiao,Zhenjun Han*

Main category: cs.CV

> VER-Bench是一个评估MLLMs在复杂视觉任务上的能力的新框架，尤其关注于模型能否识别细微线索并进行复杂推理，超越了现有评估标准的能力。

<details>
  <summary>Details</summary>

**Motivation:** 当前的评估基准主要集中在局部细节或显著图像元素上，而忽略了细微线索的深度推理。这样的评估无法全面衡量模型的视觉理解能力。为了弥补这一不足并评估模型在细粒度视觉证据提取、整合和推理方面的性能，提出了VER-Bench。

**Method:** 介绍了一个新的评估框架VER-Bench，用于评估MLLMs在识别细微视觉线索和结合世界知识进行复杂推理的能力。该框架包含374个精心设计的问题，涵盖地理空间、时间、情景、意图、系统状态和符号推理等方面，每个问题都配有结构化的证据：视觉线索和与其相关的推理。

**Result:** VER-Bench揭示了现有模型在提取细微视觉证据和构建基于证据的论据方面的局限性。

**Conclusion:** 为了实现真实的视觉理解和类似人类的分析，VER-Bench强调了提升模型在细粒度视觉证据提取、整合和推理方面能力的必要性。

**Abstract:** With the rapid development of MLLMs, evaluating their visual capabilities has
become increasingly crucial. Current benchmarks primarily fall into two main
types: basic perception benchmarks, which focus on local details but lack deep
reasoning (e.g., "what is in the image?"), and mainstream reasoning benchmarks,
which concentrate on prominent image elements but may fail to assess subtle
clues requiring intricate analysis. However, profound visual understanding and
complex reasoning depend more on interpreting subtle, inconspicuous local
details than on perceiving salient, macro-level objects. These details, though
occupying minimal image area, often contain richer, more critical information
for robust analysis. To bridge this gap, we introduce the VER-Bench, a novel
framework to evaluate MLLMs' ability to: 1) identify fine-grained visual clues,
often occupying on average just 0.25% of the image area; 2) integrate these
clues with world knowledge for complex reasoning. Comprising 374 carefully
designed questions across Geospatial, Temporal, Situational, Intent, System
State, and Symbolic reasoning, each question in VER-Bench is accompanied by
structured evidence: visual clues and question-related reasoning derived from
them. VER-Bench reveals current models' limitations in extracting subtle visual
evidence and constructing evidence-based arguments, highlighting the need to
enhance models's capabilities in fine-grained visual evidence extraction,
integration, and reasoning for genuine visual understanding and human-like
analysis. Dataset and additional materials are available
https://github.com/verbta/ACMMM-25-Materials.

</details>


### [23] [Dual-Stream Attention with Multi-Modal Queries for Object Detection in Transportation Applications](https://arxiv.org/abs/2508.04868)
*Noreen Anwar,Guillaume-Alexandre Bilodeau,Wassim Bouachir*

Main category: cs.CV

> DAMM 提出了一种新型框架，引入查询适应性和结构化交叉注意力机制，提升了遮挡处理、细粒度定位能力和计算效率。

<details>
  <summary>Details</summary>

**Motivation:** Transformer 基础的对象检测器通常难以应对遮挡、细粒度定位和由固定查询和密集注意力引起的计算低效问题。

**Method:** DAMM, Dual-stream Attention with Multi-Modal queries, 首次引入查询适应性和结构化交叉注意力机制，改进准确性和效率。DAMM 使用三种查询：基于视觉语言模型的外观查询，使用多边形嵌入的位置查询，以及用于一般场景覆盖的随机学习查询。此外，双流交叉注意力模块分别优化语义和空间特征，增强在杂乱场景中的定位精度。

**Result:** 在四个具有挑战性的基准数据集上进行评估，DAMM 达到了最先进的平均精度（AP）和召回率，证明了多模态查询适应性和双流注意力的有效性。

**Conclusion:** 实验结果表明，DAMM 通过多模态查询适应性和双流注意力机制，在平均精度和召回率方面达到了最先进的性能。

**Abstract:** Transformer-based object detectors often struggle with occlusions,
fine-grained localization, and computational inefficiency caused by fixed
queries and dense attention. We propose DAMM, Dual-stream Attention with
Multi-Modal queries, a novel framework introducing both query adaptation and
structured cross-attention for improved accuracy and efficiency. DAMM
capitalizes on three types of queries: appearance-based queries from
vision-language models, positional queries using polygonal embeddings, and
random learned queries for general scene coverage. Furthermore, a dual-stream
cross-attention module separately refines semantic and spatial features,
boosting localization precision in cluttered scenes. We evaluated DAMM on four
challenging benchmarks, and it achieved state-of-the-art performance in average
precision (AP) and recall, demonstrating the effectiveness of multi-modal query
adaptation and dual-stream attention. Source code is at:
\href{https://github.com/DET-LIP/DAMM}{GitHub}.

</details>


### [24] [Revealing Temporal Label Noise in Multimodal Hateful Video Classification](https://arxiv.org/abs/2508.04900)
*Shuonan Yang,Tailin Chen,Rahul Singh,Jiangbei Yue,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

> 本文通过细粒度方法探索多模态仇恨视频中时间戳标注噪声的影响，通过精确分割仇恨片段并分析，发现时间戳噪声对模型决策的负面影响，并提出需要更智能的模型和基准。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于解决当前多模态仇恨视频检测中粗略的视频级注释引起的重要标注噪声问题，这种噪声导致视频中虽然标注为仇恨但含有长时间无仇恨片段的问题。

**Method:** 本文采用细粒度方法处理多模态仇恨视频检测问题，通过使用带时间戳注释的HateMM和MultiHateClip英文数据集来隔离明确的仇恨片段，并对其进行探索性分析，以检查仇恨和非仇恨内容的分布和特征。

**Result:** 实验表明，时间戳噪声确实影响了模型的决策边界，降低了分类信心，证实了仇恨言论时间连续性和上下文依赖性的重要性。

**Conclusion:** 研究结论指出，时间戳噪声从根本上改变了模型的决策边界，降低了分类信心，强调了仇恨言论表达的上下文依赖性和时间连续性。这些发现为多模态仇恨视频的时间动态提供了新的见解，并强调了需要时间感知模型和基准来提高鲁棒性和可解释性。

**Abstract:** The rapid proliferation of online multimedia content has intensified the
spread of hate speech, presenting critical societal and regulatory challenges.
While recent work has advanced multimodal hateful video detection, most
approaches rely on coarse, video-level annotations that overlook the temporal
granularity of hateful content. This introduces substantial label noise, as
videos annotated as hateful often contain long non-hateful segments. In this
paper, we investigate the impact of such label ambiguity through a fine-grained
approach. Specifically, we trim hateful videos from the HateMM and
MultiHateClip English datasets using annotated timestamps to isolate explicitly
hateful segments. We then conduct an exploratory analysis of these trimmed
segments to examine the distribution and characteristics of both hateful and
non-hateful content. This analysis highlights the degree of semantic overlap
and the confusion introduced by coarse, video-level annotations. Finally,
controlled experiments demonstrated that time-stamp noise fundamentally alters
model decision boundaries and weakens classification confidence, highlighting
the inherent context dependency and temporal continuity of hate speech
expression. Our findings provide new insights into the temporal dynamics of
multimodal hateful videos and highlight the need for temporally aware models
and benchmarks for improved robustness and interpretability. Code and data are
available at
https://github.com/Multimodal-Intelligence-Lab-MIL/HatefulVideoLabelNoise.

</details>


### [25] [Test-Time Adaptation for Video Highlight Detection Using Meta-Auxiliary Learning and Cross-Modality Hallucinations](https://arxiv.org/abs/2508.04924)
*Zahidul Islam,Sujoy Paul,Mrigank Rochan*

Main category: cs.CV

> 提出Highlight-TTA以改善视频高光检测模型的泛化性能，通过与辅助任务联合优化，实现在测试时的模型适应。

<details>
  <summary>Details</summary>

**Motivation:** 现有的高光检测方法难以很好地推广到所有的测试视频中，因为它们通常使用一个通用的高光检测模型，无法适应新视频的多样性。

**Method:** Highlight-TTA, 一种测试时适应框架，它在测试期间动态调整模型以更好地适应每个测试视频的具体特征，通过与跨模态幻觉的辅助任务共同优化来实现这一点。

**Result:** 引入Highlight-TTA后，三种现有的高光检测模型在三个基准数据集上均表现出更好的性能，实现了更优的结果。

**Conclusion:** 实验表明，通过Highlight-TTA框架，能够显著提高现有高光检测模型的适应性与检测性能，证明了其有效性。

**Abstract:** Existing video highlight detection methods, although advanced, struggle to
generalize well to all test videos. These methods typically employ a generic
highlight detection model for each test video, which is suboptimal as it fails
to account for the unique characteristics and variations of individual test
videos. Such fixed models do not adapt to the diverse content, styles, or audio
and visual qualities present in new, unseen test videos, leading to reduced
highlight detection performance. In this paper, we propose Highlight-TTA, a
test-time adaptation framework for video highlight detection that addresses
this limitation by dynamically adapting the model during testing to better
align with the specific characteristics of each test video, thereby improving
generalization and highlight detection performance. Highlight-TTA is jointly
optimized with an auxiliary task, cross-modality hallucinations, alongside the
primary highlight detection task. We utilize a meta-auxiliary training scheme
to enable effective adaptation through the auxiliary task while enhancing the
primary task. During testing, we adapt the trained model using the auxiliary
task on the test video to further enhance its highlight detection performance.
Extensive experiments with three state-of-the-art highlight detection models
and three benchmark datasets show that the introduction of Highlight-TTA to
these models improves their performance, yielding superior results.

</details>


### [26] [Extending Foundational Monocular Depth Estimators to Fisheye Cameras with Calibration Tokens](https://arxiv.org/abs/2508.04928)
*Suchisrit Gangopadhyay,Jung-Hee Kim,Xien Chen,Patrick Rim,Hyoungseob Park,Alex Wong*

Main category: cs.CV

> 提出一种方法，通过调整校准参数来使得单目深度估计器能够在鱼眼图像上使用。

<details>
  <summary>Details</summary>

**Motivation:** 解决单目深度估计器在鱼眼图像上的性能问题，因为它们在训练时使用的是不同相机校准参数下的透视图图像。

**Method:** 引入校准令牌（Calibration Tokens）作为适应机制，调制潜在嵌入以对齐鱼眼图像和透视图图像的分布，无需重新训练或微调单目深度估计器。

**Result:** 该方法在室内和室外场景下使用多种单目深度估计器进行了评估，并且只使用一套校准令牌对两个场景都给出了优于现有方法的表现。

**Conclusion:** 通过调整潜在嵌入空间中的校准令牌，并利用大范围的透视图图像数据集，该方法能够在不重新训练或微调现有单目深度估计算法的前提下，提升其在鱼眼图片上的深度估计性能。

**Abstract:** We propose a method to extend foundational monocular depth estimators
(FMDEs), trained on perspective images, to fisheye images. Despite being
trained on tens of millions of images, FMDEs are susceptible to the covariate
shift introduced by changes in camera calibration (intrinsic, distortion)
parameters, leading to erroneous depth estimates. Our method aligns the
distribution of latent embeddings encoding fisheye images to those of
perspective images, enabling the reuse of FMDEs for fisheye cameras without
retraining or finetuning. To this end, we introduce a set of Calibration Tokens
as a light-weight adaptation mechanism that modulates the latent embeddings for
alignment. By exploiting the already expressive latent space of FMDEs, we posit
that modulating their embeddings avoids the negative impact of artifacts and
loss introduced in conventional recalibration or map projection to a canonical
reference frame in the image space. Our method is self-supervised and does not
require fisheye images but leverages publicly available large-scale perspective
image datasets. This is done by recalibrating perspective images to fisheye
images, and enforcing consistency between their estimates during training. We
evaluate our approach with several FMDEs, on both indoors and outdoors, where
we consistently improve over state-of-the-art methods using a single set of
tokens for both. Code available at:
https://github.com/JungHeeKim29/calibration-token.

</details>


### [27] [Toward Errorless Training ImageNet-1k](https://arxiv.org/abs/2508.04941)
*Bo Deng,Levi Heath*

Main category: cs.CV

> 本文通过改进的神经网络训练方法，提高了ImageNet数据集的分类准确性，提出了双标签问题可能影响100%准确率的观点。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在通过改进神经网络训练方法提高对ImageNet数据集的分类精确度。

**Method:** 本论文描述了一种使用ImageNet 2012竞赛数据集训练的前馈人工神经网络，并采用了一种新的方法[5]。

**Result:** 该方法实现了98.3%的准确率和Top-1的99.69%，平均在数据集的10批划分中，有285.9个标签被完全正确分类。最佳模型使用了322,430,160个参数，精确到4位小数。

**Conclusion:** 作者推测模型未能达到100%的准确率的原因在于数据集中的双标签问题，即存在具有不同标签的重复图像。

**Abstract:** In this paper, we describe a feedforward artificial neural network trained on
the ImageNet 2012 contest dataset [7] with the new method of [5] to an accuracy
rate of 98.3% with a 99.69 Top-1 rate, and an average of 285.9 labels that are
perfectly classified over the 10 batch partitions of the dataset. The best
performing model uses 322,430,160 parameters, with 4 decimal places precision.
We conjecture that the reason our model does not achieve a 100% accuracy rate
is due to a double-labeling problem, by which there are duplicate images in the
dataset with different labels.

</details>
