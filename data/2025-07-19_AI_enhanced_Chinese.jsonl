{"id": "2507.12490", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12490", "abs": "https://arxiv.org/abs/2507.12490", "authors": ["Maximiliano Hormazábal Lagos", "Héctor Cerezo-Costas", "Dimosthenis Karatzas"], "title": "Spatially Grounded Explanations in Vision Language Models for Document Visual Question Answering", "comment": "This work has been accepted for presentation at the 16th Conference\n  and Labs of the Evaluation Forum (CLEF 2025) and will be published in the\n  proceedings by Springer in the Lecture Notes in Computer Science (LNCS)\n  series. Please cite the published version when available", "summary": "We introduce EaGERS, a fully training-free and model-agnostic pipeline that\n(1) generates natural language rationales via a vision language model, (2)\ngrounds these rationales to spatial sub-regions by computing multimodal\nembedding similarities over a configurable grid with majority voting, and (3)\nrestricts the generation of responses only from the relevant regions selected\nin the masked image. Experiments on the DocVQA dataset demonstrate that our\nbest configuration not only outperforms the base model on exact match accuracy\nand Average Normalized Levenshtein Similarity metrics but also enhances\ntransparency and reproducibility in DocVQA without additional model\nfine-tuning.", "AI": {"tldr": "EaGERS is a novel, training-free pipeline for enhancing the DocVQA performance of vision-language models by incorporating reasoning and grounding mechanisms, which improves model accuracy and interpretability.", "motivation": "The motivation for EaGERS is to develop a flexible, training-free tool for enhancing the performance of VQA models on tasks like DocVQA while maintaining or improving transparency and reproducibility in the model output.", "method": "EaGERS, a fully training-free and model-agnostic pipeline, works by generating natural language rationales through a vision language model, grounding these rationales to spatial sub-regions using multimodal embedding similarities with majority voting over a grid, and restricting the generation of responses to only the relevant regions of a masked image.", "result": "The best configuration of EaGERS outperforms the base model on DocVQA's exact match accuracy and Average Normalized Levenshtein Similarity metrics, while also improving transparency and reproducibility without the need for further model fine-tuning.", "conclusion": "EaGERS offers an effective and flexible method for enhancing VQA performance, especially on specialized datasets like DocVQA, by integrating rational explanations and spatial reasoning, and does so without requiring additional model training."}}
{"id": "2507.12508", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12508", "abs": "https://arxiv.org/abs/2507.12508", "authors": ["Yuncong Yang", "Jiageng Liu", "Zheyuan Zhang", "Siyuan Zhou", "Reuben Tan", "Jianwei Yang", "Yilun Du", "Chuang Gan"], "title": "MindJourney: Test-Time Scaling with World Models for Spatial Reasoning", "comment": "Project Page: https://umass-embodied-agi.github.io/MindJourney", "summary": "Spatial reasoning in 3D space is central to human cognition and indispensable\nfor embodied tasks such as navigation and manipulation. However,\nstate-of-the-art vision-language models (VLMs) struggle frequently with tasks\nas simple as anticipating how a scene will look after an egocentric motion:\nthey perceive 2D images but lack an internal model of 3D dynamics. We therefore\npropose MindJourney, a test-time scaling framework that grants a VLM with this\nmissing capability by coupling it to a controllable world model based on video\ndiffusion. The VLM iteratively sketches a concise camera trajectory, while the\nworld model synthesizes the corresponding view at each step. The VLM then\nreasons over this multi-view evidence gathered during the interactive\nexploration. Without any fine-tuning, our MindJourney achieves over an average\n8% performance boost on the representative spatial reasoning benchmark SAT,\nshowing that pairing VLMs with world models for test-time scaling offers a\nsimple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also\nimproves upon the test-time inference VLMs trained through reinforcement\nlearning, which demonstrates the potential of our method that utilizes world\nmodels for test-time scaling.", "AI": {"tldr": "This paper presents MindJourney, a framework that couples a vision-language model with a world model to improve 3D spatial reasoning without fine-tuning, resulting in an 8% boost in performance on benchmark tests.", "motivation": "The motivation is to enhance vision-language models' ability to reason in 3D spaces, which is crucial for human-like tasks such as navigation but is lacking in current state-of-the-art models.", "method": "The method uses a test-time scaling framework called MindJourney that pairs a vision-language model with a controllable world model based on video diffusion to enable 3D spatial reasoning. The world model synthesizes views at each step of a camera trajectory proposed by the VLM.", "result": "The result is a significant performance boost (over 8%) on the SAT spatial reasoning benchmark without the need for fine-tuning, indicating the effectiveness of the approach.", "conclusion": "The conclusion is that integrating vision-language models with world models via test-time scaling is an effective and simple way to enhance 3D reasoning capabilities, outweighing methods that train VLMs through reinforcement learning for test-time inference."}}
{"id": "2507.12566", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12566", "abs": "https://arxiv.org/abs/2507.12566", "authors": ["Gen Luo", "Wenhan Dou", "Wenhao Li", "Zhaokai Wang", "Xue Yang", "Changyao Tian", "Hao Li", "Weiyun Wang", "Wenhai Wang", "Xizhou Zhu", "Yu Qiao", "Jifeng Dai"], "title": "Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models", "comment": null, "summary": "This paper focuses on monolithic Multimodal Large Language Models (MLLMs),\nwhich integrate visual encoding and language decoding into a single model.\nExisting structures and pre-training strategies for monolithic MLLMs often\nsuffer from unstable optimization and catastrophic forgetting. To address these\nchallenges, our key idea is to embed a new visual parameter space into a\npre-trained LLM, enabling stable learning of visual knowledge from noisy data\nvia delta tuning. Based on this principle, we first introduce Mono-InternVL, an\nadvanced monolithic MLLM that incorporates a set of visual experts through a\nmultimodal mixture-of-experts architecture. In addition, we design an\ninnovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize\nits visual capabilities via progressive learning. Mono-InternVL achieves\ncompetitive performance against existing MLLMs but also leads to relatively\nexpensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper\nand stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++\nintroduces additional visual attention experts to Mono-InternVL-1.5 and\nre-organizes the pre-training process in an efficient manner. During inference,\nit includes a fused CUDA kernel to speed up its MoE operations. With these\ndesigns, Mono-InternVL-1.5 significantly reduces training and inference costs,\nwhile still maintaining competitive performance with Mono-InternVL. To evaluate\nour approach, we conduct extensive experiments across 15 benchmarks. Results\ndemonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out\nof 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared\nto its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves\nsimilar multimodal performance while reducing first-token latency by up to 69%.\nCode and models are released at https://github.com/OpenGVLab/Mono-InternVL.", "AI": {"tldr": "本文提出Mono-InternVL和Mono-InternVL-1.5，这两者是先进的单片多模态大型语言模型，能够在视觉理解上表现出色并有效减少训练和推理成本。", "motivation": "现有的单片MLLM结构和预训练策略常常遭遇不稳定优化和灾难性遗忘的问题。为解决这些问题，本文提出了一个新的方法和改进版模型。", "method": "通过将新的视觉参数空间嵌入到已预训练的语言模型中，使得可以利用增量调优从嘈杂的数据中稳定地学习视觉知识。提出了Mono-InternVL，一个集成了视觉专家的先进单片MLLM，并设计了名为EViP的内源视觉预训练方法，通过渐进式学习最大化其视觉能力。进一步提出了Mono-InternVL-1.5，配备改进后的EViP++，引入额外的视觉注意力专家并以更高效的方式重组预训练过程。", "result": "实验结果表明，Mono-InternVL在15个基准测试中的12个上优于现有的单片MLLM，Mono-InternVL-1.5与模块化同行相比，性能相似但延迟减少高达69%。", "conclusion": "提出的Mono-InternVL和Mono-InternVL-1.5单片MLLM，在视觉理解和性能方面取得了显著成果，并在成本方面有所降低。"}}
{"id": "2507.12590", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12590", "abs": "https://arxiv.org/abs/2507.12590", "authors": ["Judy Long", "Tao Liu", "Sean Alexander Woznicki", "Miljana Marković", "Oskar Marko", "Molly Sears"], "title": "Best Practices for Large-Scale, Pixel-Wise Crop Mapping and Transfer Learning Workflows", "comment": "A review article. 41 pages, 22 figures. Preprint", "summary": "Crop mapping involves identifying and classifying crop types using spatial\ndata, primarily derived from remote sensing imagery. This study presents the\nfirst comprehensive review of large-scale, pixel-wise crop mapping workflows,\nencompassing both conventional supervised methods and emerging transfer\nlearning approaches. To identify the optimal supervised crop mapping workflows,\nwe conducted systematic experiments, comparing six widely adopted satellite\nimage-based preprocessing methods, alongside eleven supervised pixel-wise\nclassification models. Additionally, we assessed the synergistic impact of\nvaried training sample sizes and variable combinations. Moreover, we identified\noptimal transfer learning techniques for different magnitudes of domain shift.\nThe evaluation of best methods was conducted across five diverse agricultural\nsites. Landsat 8 served as the primary satellite data source. Labels come from\nCDL trusted pixels and field surveys.\n  Our findings reveal three key insights. First, fine-scale interval\npreprocessing paired with Transformer models consistently delivered optimal\nperformance for both supervised and transferable workflows. RF offered rapid\ntraining and competitive performance in conventional supervised learning and\ndirect transfer to similar domains. Second, transfer learning techniques\nenhanced workflow adaptability, with UDA being effective for homogeneous crop\nclasses while fine-tuning remains robust across diverse scenarios. Finally,\nworkflow choice depends heavily on the availability of labeled samples. With a\nsufficient sample size, supervised training typically delivers more accurate\nand generalizable results. Below a certain threshold, transfer learning that\nmatches the level of domain shift is a viable alternative to achieve crop\nmapping. Repository:\nBest-Practices-for-Large-Scale-Pixel-Wise-Crop-Mapping-and-Transfer-Learning-Workflows", "AI": {"tldr": "本研究首次全面回顾了大尺度基于像素的作物制图工作流程，比较了多种预处理方法、监督分类模型，并研究了迁移学习技术在不同领域偏移情况下的表现。主要发现是变压器模型搭配精细区间预处理在监督和可迁移的工作流中表现最优，而随机森林在传统监督学习和直接迁移相似领域中训练速度快且性能良好。当标记样本充足时，监督训练通常提供更精确和泛化能力强的结果；当样本不足时，匹配领域偏移的迁移学习成为有效替代方案。", "motivation": "研究旨在确定最优的基于像素的作物制图工作流程，尤其是比较传统的监督方法和新兴的迁移学习方法。", "method": "研究比较了六种广泛采用的卫星图像预处理方法和十一种监督像素分类模型，评估了不同训练样本量和变量组合的影响，并在不同规模的农业站点评估了最佳方法。", "result": "研究发现了精细区间预处理与变压器模型组合最优、随机森林在传统监督学习中表现良好、迁移学习技术提高工作流适应性等主要结论。", "conclusion": "研究强调了工作流选择高度依赖于标记样本的可用性，并提出在样本不足的情况下，适配领域偏移的迁移学习是一种可行的替代方法。"}}
{"id": "2507.12547", "categories": ["cs.CL", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.12547", "abs": "https://arxiv.org/abs/2507.12547", "authors": ["Lionel Wong", "Katherine M. Collins", "Lance Ying", "Cedegao E. Zhang", "Adrian Weller", "Tobias Gersternberg", "Timothy O'Donnell", "Alexander K. Lew", "Jacob D. Andreas", "Joshua B. Tenenbaum", "Tyler Brooke-Wilson"], "title": "Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models", "comment": "Presented at CogSci 2025", "summary": "When faced with novel situations, people are able to marshal relevant\nconsiderations from a wide range of background knowledge and put these to use\nin inferences and predictions. What permits us to draw in globally relevant\ninformation and reason over it coherently? Here, we explore the hypothesis that\npeople use a combination of distributed and symbolic representations to\nconstruct bespoke mental models tailored to novel situations. We propose a\ncomputational implementation of this idea -- a ``Model Synthesis Architecture''\n(MSA) -- using language models to implement global relevance-based retrieval\nand model synthesis and probabilistic programs to implement bespoke, coherent\nworld models. We evaluate our MSA as a model of human judgments on a novel\nreasoning dataset. The dataset -- built around a `Model Olympics` domain of\nsports vignettes -- tests models' capacity for human-like, open-ended reasoning\nby requiring (i) judgments about novel causal structures described in language;\n(ii) drawing on large bodies of background knowledge; and (iii) doing both in\nlight of observations that introduce arbitrary novel variables. Our MSA\napproach captures human judgments better than language model-only baselines,\nunder both direct and chain-of-thought generations from the LM that supports\nmodel synthesis. These results suggest that MSAs can be implemented in a way\nthat mirrors people's ability to deliver locally coherent reasoning over\nglobally relevant variables, offering a path to understanding and replicating\nhuman reasoning in open-ended domains.", "AI": {"tldr": "研究提出一种结合分布式和符号表示方法，构建专有心理模型以应对新情景的“模型合成架构”，并通过特定实验验证了其优于单纯语言模型的表现。", "motivation": "研究者们旨在探索人们如何在面对新情况时，调动广泛背景知识中的相关信息进行连贯推理的核心机制。", "method": "提出了一种名为“模型合成架构”(MSA)的计算实现方式，使用语言模型来实现全局相关性检索和模型合成，使用概率程序来实现特制的、连贯的世界模型。", "result": "研究结果表明，MSA在处理一种新颖推理数据集（以体育比喻构成的故事段落）时，能更准确地预测人类的决策，比仅依赖语言模型的方法表现更好。", "conclusion": "研究结果表明，MSA能够模拟人们在全球相关变量上的局部连贯推理能力，为理解和复制人在开放领域中的推理提供了一条途径。"}}
{"id": "2507.12591", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12591", "abs": "https://arxiv.org/abs/2507.12591", "authors": ["Trong-Thang Pham", "Akash Awasthi", "Saba Khan", "Esteban Duran Marti", "Tien-Phat Nguyen", "Khoa Vo", "Minh Tran", "Ngoc Son Nguyen", "Cuong Tran Van", "Yuki Ikebe", "Anh Totti Nguyen", "Anh Nguyen", "Zhigang Deng", "Carol C. Wu", "Hien Van Nguyen", "Ngan Le"], "title": "CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling", "comment": "ICCV 2025", "summary": "Understanding radiologists' eye movement during Computed Tomography (CT)\nreading is crucial for developing effective interpretable computer-aided\ndiagnosis systems. However, CT research in this area has been limited by the\nlack of publicly available eye-tracking datasets and the three-dimensional\ncomplexity of CT volumes. To address these challenges, we present the first\npublicly available eye gaze dataset on CT, called CT-ScanGaze. Then, we\nintroduce CT-Searcher, a novel 3D scanpath predictor designed specifically to\nprocess CT volumes and generate radiologist-like 3D fixation sequences,\novercoming the limitations of current scanpath predictors that only handle 2D\ninputs. Since deep learning models benefit from a pretraining step, we develop\na pipeline that converts existing 2D gaze datasets into 3D gaze data to\npretrain CT-Searcher. Through both qualitative and quantitative evaluations on\nCT-ScanGaze, we demonstrate the effectiveness of our approach and provide a\ncomprehensive assessment framework for 3D scanpath prediction in medical\nimaging.", "AI": {"tldr": "研究提出了CT-ScanGaze——首个公开的CT眼动数据集和CT-Searcher——一种特别处理CT体积并生成类似放射科医生的3D注视序列的3D扫描路径预测器，为3D扫描路径预测提供了新的解决方案和评估框架。", "motivation": "鉴于缺乏公开的眼动追踪数据集以及CT体积的三维复杂性，研究旨在促进对放射科医生在CT阅读期间眼动的理解，这有助于开发有效的可解释性计算机辅助诊断系统。", "method": "通过构建首个公开的CT眼动追踪数据集CT-ScanGaze和开发CT-Searcher——一种专门处理CT体积并生成类似放射科医生的3D注视序列的3D扫描路径预测器，研究解决了现有的2D扫描路径预测器的局限性。此外，还开发了一种将现有的2D注视数据集转换为3D注视数据的预训练流程，以增强模型的深度学习效果。", "result": "通过定性和定量的评估，研究展示了该方法在CT-ScanGaze上的有效性，并为医学成像中的3D扫描路径预测提供了一个全面的评估框架。", "conclusion": "研究结果表明了开发3D扫描路径预测器在促进理解和解释医学图像中的作用，同时展示了其作为一种新的工具在放射科学中的潜力。"}}
{"id": "2507.12553", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12553", "abs": "https://arxiv.org/abs/2507.12553", "authors": ["Michael A. Lepori", "Jennifer Hu", "Ishita Dasgupta", "Roma Patel", "Thomas Serre", "Ellie Pavlick"], "title": "Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility", "comment": null, "summary": "Language models (LMs) are used for a diverse range of tasks, from question\nanswering to writing fantastical stories. In order to reliably accomplish these\ntasks, LMs must be able to discern the modal category of a sentence (i.e.,\nwhether it describes something that is possible, impossible, completely\nnonsensical, etc.). However, recent studies have called into question the\nability of LMs to categorize sentences according to modality (Michaelov et al.,\n2025; Kauf et al., 2023). In this work, we identify linear representations that\ndiscriminate between modal categories within a variety of LMs, or modal\ndifference vectors. Analysis of modal difference vectors reveals that LMs have\naccess to more reliable modal categorization judgments than previously\nreported. Furthermore, we find that modal difference vectors emerge in a\nconsistent order as models become more competent (i.e., through training steps,\nlayers, and parameter count). Notably, we find that modal difference vectors\nidentified within LM activations can be used to model fine-grained human\ncategorization behavior. This potentially provides a novel view into how human\nparticipants distinguish between modal categories, which we explore by\ncorrelating projections along modal difference vectors with human participants'\nratings of interpretable features. In summary, we derive new insights into LM\nmodal categorization using techniques from mechanistic interpretability, with\nthe potential to inform our understanding of modal categorization in humans.", "AI": {"tldr": "本研究指出语言模型具备比之前认为更可靠的句子语气分类能力，并开发了可以映射人类语气分类行为的语气差异向量，丰富了对模型语言理解机制及人类分类行为的认识。", "motivation": "研究动机是探讨语言模型在区分句子语气类别方面的能力，回应近期质疑其分类能力的研究。", "method": "研究通过识别语言模型中的语气差异向量来分析模型对句子语气类别的理解，并将其与人类分类行为进行比较。", "result": "该研究旨在识别出语言模型中能够区分句子语气类别（如可能、不可能或完全无意义等）的线性表示形式——语气差异向量。通过分析语气差异向量，研究人员发现语言模型对于语气类别的分类比之前认为的要更可靠。随着模型能力的提升（如训练步数、层深度及参数数量的增长），语气差异向量呈现出一致的发现顺序。更进一步，这些向量可以用来模仿人类对语气类别的精细分类行为，通过将人类参与者的评估与沿语气差异向量的投影相关联，为理解人们如何区别语气类别提供了新视角。这项研究利用机制解释技术，提供了关于语言模型语气分类的新见解，并可能对人类的语气分类理解有所贡献。", "conclusion": "研究得出结论，语言模型能够执行语气分类，其分级行为可以通过语气差异向量类比人类行为，这可能促进了对模型以及人类语气分类行为的理解。"}}
{"id": "2507.12602", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12602", "abs": "https://arxiv.org/abs/2507.12602", "authors": ["Said Ohamouddou", "Abdellatif El Afia", "Hanaa El Afia", "Raddouane Chiheb"], "title": "MS-DGCNN++: A Multi-Scale Fusion Dynamic Graph Neural Network with Biological Knowledge Integration for LiDAR Tree Species Classification", "comment": null, "summary": "Tree species classification from terrestrial LiDAR point clouds is\nchallenging because of the complex multi-scale geometric structures in forest\nenvironments. Existing approaches using multi-scale dynamic graph convolutional\nneural networks (MS-DGCNN) employ parallel multi-scale processing, which fails\nto capture the semantic relationships between the hierarchical levels of the\ntree architecture. We present MS-DGCNN++, a hierarchical multiscale fusion\ndynamic graph convolutional network that uses semantically meaningful feature\nextraction at local, branch, and canopy scales with cross-scale information\npropagation. Our method employs scale-specific feature engineering, including\nstandard geometric features for the local scale, normalized relative vectors\nfor the branch scale, and distance information for the canopy scale. This\nhierarchical approach replaces uniform parallel processing with semantically\ndifferentiated representations that are aligned with the natural tree\nstructure. Under the same proposed tree species data augmentation strategy for\nall experiments, MS-DGCNN++ achieved an accuracy of 94.96 \\% on STPCTLS,\noutperforming DGCNN, MS-DGCNN, and the state-of-the-art model PPT. On\nFOR-species20K, it achieves 67.25\\% accuracy (6.1\\% improvement compared to\nMS-DGCNN). For standard 3D object recognition, our method outperformed DGCNN\nand MS-DGCNN with overall accuracies of 93.15\\% on ModelNet40 and 94.05\\% on\nModelNet10. With lower parameters and reduced complexity compared to\nstate-of-the-art transformer approaches, our method is suitable for\nresource-constrained applications while maintaining a competitive accuracy.\nBeyond tree classification, the method generalizes to standard 3D object\nrecognition, establishing it as a versatile solution for diverse point cloud\nprocessing applications. The implementation code is publicly available at\nhttps://github.com/said-ohamouddou/MS-DGCNN2.", "AI": {"tldr": "提出了MS-DGCNN++, 一种在不同尺度上捕获语义信息的层次多尺度融合动态图卷积网络，提高了森林环境中树种分类的准确性，且泛化到标准3D物体识别任务中。", "motivation": "现有的多尺度动态图卷积神经网络方法使用并行多尺度处理，无法捕捉到树结构不同层级之间的语义关系。", "method": "MS-DGCNN++使用在局部、枝干和冠层尺度上的语义特征提取以及跨尺度信息传播，包括局部尺度的标准几何特征、枝干尺度的归一化相对向量以及冠层尺度的距离信息。", "result": "在STPCTLS数据集上准确率达到94.96%，在FOR-species20K数据集上准确率为67.25%，并提高了标准3D物体识别的准确度。", "conclusion": "展示了MS-DGCNN++不仅适用于树种分类，也能在标准3D物体识别任务中表现出色，具备良好的应用前景。"}}
{"id": "2507.12672", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12672", "abs": "https://arxiv.org/abs/2507.12672", "authors": ["Abu-Viskhan A. Umishov", "Vladislav A. Grigorian"], "title": "The first open machine translation system for the Chechen language", "comment": "7 pages", "summary": "We introduce the first open-source model for translation between the\nvulnerable Chechen language and Russian, and the dataset collected to train and\nevaluate it. We explore fine-tuning capabilities for including a new language\ninto a large language model system for multilingual translation NLLB-200. The\nBLEU / ChrF++ scores for our model are 8.34 / 34.69 and 20.89 / 44.55 for\ntranslation from Russian to Chechen and reverse direction, respectively. The\nrelease of the translation models is accompanied by the distribution of\nparallel words, phrases and sentences corpora and multilingual sentence encoder\nadapted to the Chechen language.", "AI": {"tldr": "研究团队发布了第一个用于切森语和俄语之间翻译的开源模型，并评估了其性能，同时创建了相关的数据集和编码器以支持研究。", "motivation": "鉴于切森语是一个脆弱的语言，我们需要为这个语言的保护和保留作出努力。通过创建一个开放源代码的翻译模型，不仅可以支持切森语的保存，也可以推进多语言翻译系统的发展。", "method": "我们介绍了第一个用于切森语和俄语之间翻译的开源模型，并收集了用于训练和评估该模型的数据集。我们探讨了将新语言纳入NLLB-200多语言翻译系统中的微调能力。", "result": "我们的模型在BLEU / ChrF++评分上的表现分别为8.34 / 34.69（从俄语到切森语）和20.89 / 44.55（从切森语到俄语）。", "conclusion": "除了发布翻译模型外，我们还提供了平行词、短语和句子语料库以及适应切森语的多语言句子编码器的分布。"}}
{"id": "2507.12617", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12617", "abs": "https://arxiv.org/abs/2507.12617", "authors": ["David Freire-Obregón", "Oliverio J. Santana", "Javier Lorenzo-Navarro", "Daniel Hernández-Sosa", "Modesto Castrillón-Santana"], "title": "Predicting Soccer Penalty Kick Direction Using Human Action Recognition", "comment": "Accepted at 23rd International Conference on Image Analysis and\n  Processing (ICIAP 2025)", "summary": "Action anticipation has become a prominent topic in Human Action Recognition\n(HAR). However, its application to real-world sports scenarios remains limited\nby the availability of suitable annotated datasets. This work presents a novel\ndataset of manually annotated soccer penalty kicks to predict shot direction\nbased on pre-kick player movements. We propose a deep learning classifier to\nbenchmark this dataset that integrates HAR-based feature embeddings with\ncontextual metadata. We evaluate twenty-two backbone models across seven\narchitecture families (MViTv2, MViTv1, SlowFast, Slow, X3D, I3D, C2D),\nachieving up to 63.9% accuracy in predicting shot direction (left or right),\noutperforming the real goalkeepers' decisions. These results demonstrate the\ndataset's value for anticipatory action recognition and validate our model's\npotential as a generalizable approach for sports-based predictive tasks.", "AI": {"tldr": "该研究通过创建一个用于预测射门方向的新足球点球数据集，展示了在体育预测任务中利用深度学习模型进行动作预测的潜力。", "motivation": "动作预测在人类动作识别（HAR）领域成为一个显著话题，但其在实际体育场景中的应用受到了合适注释数据集的限制。本研究旨在通过提供新的注释数据集来解决这一问题。", "method": "本研究提出了一种新的手动注释的足球点球数据集，用于基于射门前的球员动作来预测射门方向。研究中采用了深度学习分类器整合了基于人体动作识别的特征嵌入及上下文元数据来进行基准测试。", "result": "研究评估了来自七个架构系列（MViTv2, MViTv1, SlowFast, Slow, X3D, I3D, C2D）的二十二种骨干模型，预测射门方向（左或右）的准确率高达63.9%，超过了实际守门员的决策水平。", "conclusion": "这些结果显示了该数据集在预测性动作识别中的价值，并验证了我们模型作为体育预测任务普遍适用性方法的潜力。"}}
{"id": "2507.12679", "categories": ["cs.CL", "q-bio.QM", "I.2.7; J.3"], "pdf": "https://arxiv.org/pdf/2507.12679", "abs": "https://arxiv.org/abs/2507.12679", "authors": ["Arthur J. Funnell", "Panayiotis Petousis", "Fabrice Harel-Canada", "Ruby Romero", "Alex A. T. Bui", "Adam Koncsol", "Hritika Chaturvedi", "Chelsea Shover", "David Goodman-Meza"], "title": "Improving Drug Identification in Overdose Death Surveillance using Large Language Models", "comment": "30 pages, 1 figure, 4 tables, 2 supplemental figures, 4 supplemental\n  tables, submitted to Journal of Forensic Sciences (JFS)", "summary": "The rising rate of drug-related deaths in the United States, largely driven\nby fentanyl, requires timely and accurate surveillance. However, critical\noverdose data are often buried in free-text coroner reports, leading to delays\nand information loss when coded into ICD (International Classification of\nDisease)-10 classifications. Natural language processing (NLP) models may\nautomate and enhance overdose surveillance, but prior applications have been\nlimited. A dataset of 35,433 death records from multiple U.S. jurisdictions in\n2020 was used for model training and internal testing. External validation was\nconducted using a novel separate dataset of 3,335 records from 2023-2024.\nMultiple NLP approaches were evaluated for classifying specific drug\ninvolvement from unstructured death certificate text. These included\ntraditional single- and multi-label classifiers, as well as fine-tuned\nencoder-only language models such as Bidirectional Encoder Representations from\nTransformers (BERT) and BioClinicalBERT, and contemporary decoder-only large\nlanguage models such as Qwen 3 and Llama 3. Model performance was assessed\nusing macro-averaged F1 scores, and 95% confidence intervals were calculated to\nquantify uncertainty. Fine-tuned BioClinicalBERT models achieved near-perfect\nperformance, with macro F1 scores >=0.998 on the internal test set. External\nvalidation confirmed robustness (macro F1=0.966), outperforming conventional\nmachine learning, general-domain BERT models, and various decoder-only large\nlanguage models. NLP models, particularly fine-tuned clinical variants like\nBioClinicalBERT, offer a highly accurate and scalable solution for overdose\ndeath classification from free-text reports. These methods can significantly\naccelerate surveillance workflows, overcoming the limitations of manual ICD-10\ncoding and supporting near real-time detection of emerging substance use\ntrends.", "AI": {"tldr": "研究通过使用自然语言处理（NLP）模型，特别是基于ClinicalBERT的训练模型，实现了针对药物过量死亡案件中自由文本报告信息的高度准确分类，从而极大提高了监视工作流效率。", "motivation": "研究动机是解决由芬太尼等药物引发的吸毒致死率上升的问题，以及关键过量数据在自由文本法医报告中的隐藏导致延误和信息丢失的问题。此研究旨在利用NLP模型自动化和提高过量死亡监视效果。", "method": "本研究使用了来自美国多个司法管辖区的35,433份2020年死亡记录数据集进行模型训练和内部测试，并使用2023-2024年的3,335份记录数据集进行外部验证。评估了多种NLP方法对未结构化死亡证书文本中的特定药物涉及情况的分类效果，包括传统的单标签和多标签分类器，以及微调的编码器仅有模型如BERT和BioClinicalBERT，和当前的解码器仅有大型语言模型如Qwen 3和Llama 3。", "result": "在内部测试集中，微调的BioClinicalBERT模型取得了近完美的性能，表现优于传统的机器学习、通用领域BERT模型及各种解码器仅有大型语言模型。外部验证确认了其健壮性，表明NLP模型可以准确分类自由文本报告中的过量死亡案例。", "conclusion": "NLP模型，尤其是微调的临床变体如BioClinicalBERT，为从自由文本报告中分类过量死亡提供了高精度且可扩展的解决方案。这些方法可以显著加速监视工作流，克服手工ICD-10编码限制，支持接近实时地检测新兴物质使用趋势。"}}
{"id": "2507.12628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12628", "abs": "https://arxiv.org/abs/2507.12628", "authors": ["Sandipan Sarma", "Agney Talwarr", "Arijit Sur"], "title": "Funnel-HOI: Top-Down Perception for Zero-Shot HOI Detection", "comment": "10 pages, 6 figures", "summary": "Human-object interaction detection (HOID) refers to localizing interactive\nhuman-object pairs in images and identifying the interactions. Since there\ncould be an exponential number of object-action combinations, labeled data is\nlimited - leading to a long-tail distribution problem. Recently, zero-shot\nlearning emerged as a solution, with end-to-end transformer-based object\ndetectors adapted for HOID becoming successful frameworks. However, their\nprimary focus is designing improved decoders for learning entangled or\ndisentangled interpretations of interactions. We advocate that HOI-specific\ncues must be anticipated at the encoder stage itself to obtain a stronger scene\ninterpretation. Consequently, we build a top-down framework named Funnel-HOI\ninspired by the human tendency to grasp well-defined concepts first and then\nassociate them with abstract concepts during scene understanding. We first\nprobe an image for the presence of objects (well-defined concepts) and then\nprobe for actions (abstract concepts) associated with them. A novel asymmetric\nco-attention mechanism mines these cues utilizing multimodal information\n(incorporating zero-shot capabilities) and yields stronger interaction\nrepresentations at the encoder level. Furthermore, a novel loss is devised that\nconsiders objectaction relatedness and regulates misclassification penalty\nbetter than existing loss functions for guiding the interaction classifier.\nExtensive experiments on the HICO-DET and V-COCO datasets across\nfully-supervised and six zero-shot settings reveal our state-of-the-art\nperformance, with up to 12.4% and 8.4% gains for unseen and rare HOI\ncategories, respectively.", "AI": {"tldr": "我们构建了一个名为Funnel-HOI的自顶向下框架，通过新颖的非对称协同注意力机制在编码阶段强化交互表示，并设计了一种新的损失函数，从而提升人类物体交互检测的性能，特别对于未见过和稀有的交互类别有显著提升。", "motivation": "目前HOI检测主要集中在设计改进的解码器以学习纠缠或解纠缠的交互解释。我们主张HOI特定的线索必须在编码阶段就预见，以获得更强的场景解释。因此，我们引入了一种自顶向下的框架，灵感来自于人类在场景理解时首先掌握明确概念然后关联抽象概念的认知方式。", "method": "我们提出了一个名为Funnel-HOI的自顶向下框架，该框架首先检测图像中的物体（明确的概念），然后识别这些物体相关的动作（抽象的概念）。一种新颖的非对称协同注意机制利用多模态信息（包括零样本学习能力）来挖掘这些线索，从而在编码阶段获得更强的交互表示。此外，还设计了一种新的损失函数，该函数考虑了物体和动作之间的相关性，并更好地控制了错误分类的惩罚，从而指导交互分类器的学习。", "result": "实验结果显示，我们在HICO-DET和V-COCO数据集上实现了最高性能，对于未见过和稀有的交互类别分别提升了12.4%和8.4%的性能。", "conclusion": "在HICO-DET和V-COCO数据集上的广泛实验表明，我们的方法在全监督和六种零样本场景下均表现出最先进的性能，尤其是在未见过和稀有的交互类别上的提升更加显著。"}}
{"id": "2507.12695", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12695", "abs": "https://arxiv.org/abs/2507.12695", "authors": ["S M Rafiuddin", "Sadia Kamal", "Mohammed Rakib", "Arunkumar Bagavathi", "Atriya Sen"], "title": "AdaptiSent: Context-Aware Adaptive Attention for Multimodal Aspect-Based Sentiment Analysis", "comment": "12 pages (including references), 2 figures (Fig. 1 overview, Fig. 2\n  hyperparameter sensitivity with two subplots), 6 tables (performance,\n  ablation, dataset stats, case studies, etc.), accepted at ASONAM 2025 (Social\n  Network Analysis and Mining)", "summary": "We introduce AdaptiSent, a new framework for Multimodal Aspect-Based\nSentiment Analysis (MABSA) that uses adaptive cross-modal attention mechanisms\nto improve sentiment classification and aspect term extraction from both text\nand images. Our model integrates dynamic modality weighting and\ncontext-adaptive attention, enhancing the extraction of sentiment and\naspect-related information by focusing on how textual cues and visual context\ninteract. We tested our approach against several baselines, including\ntraditional text-based models and other multimodal methods. Results from\nstandard Twitter datasets show that AdaptiSent surpasses existing models in\nprecision, recall, and F1 score, and is particularly effective in identifying\nnuanced inter-modal relationships that are crucial for accurate sentiment and\naspect term extraction. This effectiveness comes from the model's ability to\nadjust its focus dynamically based on the context's relevance, improving the\ndepth and accuracy of sentiment analysis across various multimodal data sets.\nAdaptiSent sets a new standard for MABSA, significantly outperforming current\nmethods, especially in understanding complex multimodal information.", "AI": {"tldr": "AdaptiSent is an advanced MABSA framework that uses adaptive cross-modal attention to improve sentiment and aspect term extraction from text and images, showing superior performance over existing models.", "motivation": "To improve sentiment classification and aspect term extraction by leveraging the interaction between textual cues and visual context in multimodal datasets.", "method": "Our model, AdaptiSent, uses adaptive cross-modal attention mechanisms for Multimodal Aspect-Based Sentiment Analysis (MABSA). It integrates dynamic modality weighting and context-adaptive attention to enhance sentiment and aspect term extraction from text and images.", "result": "AdaptiSent outperforms existing models in precision, recall, and F1 score on standard Twitter datasets, particularly in identifying nuanced inter-modal relationships.", "conclusion": "AdaptiSent sets a new standard for MABSA methods, showcasing superior performance, especially in dealing with complex multimodal data sets."}}
{"id": "2507.12646", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12646", "abs": "https://arxiv.org/abs/2507.12646", "authors": ["Kaihua Chen", "Tarasha Khurana", "Deva Ramanan"], "title": "Reconstruct, Inpaint, Finetune: Dynamic Novel-view Synthesis from Monocular Videos", "comment": "Project page: https://cog-nvs.github.io/", "summary": "We explore novel-view synthesis for dynamic scenes from monocular videos.\nPrior approaches rely on costly test-time optimization of 4D representations or\ndo not preserve scene geometry when trained in a feed-forward manner. Our\napproach is based on three key insights: (1) covisible pixels (that are visible\nin both the input and target views) can be rendered by first reconstructing the\ndynamic 3D scene and rendering the reconstruction from the novel-views and (2)\nhidden pixels in novel views can be \"inpainted\" with feed-forward 2D video\ndiffusion models. Notably, our video inpainting diffusion model (CogNVS) can be\nself-supervised from 2D videos, allowing us to train it on a large corpus of\nin-the-wild videos. This in turn allows for (3) CogNVS to be applied zero-shot\nto novel test videos via test-time finetuning. We empirically verify that\nCogNVS outperforms almost all prior art for novel-view synthesis of dynamic\nscenes from monocular videos.", "AI": {"tldr": "A new approach for synthesizing novel views of dynamic scenes from monocular videos using 3D reconstruction and 2D diffusion models for inpainting, which outperforms previous methods.", "motivation": "The goal is to improve novel-view synthesis for dynamic scenes from monocular videos, especially addressing the limitations of prior methods that either require costly test-time optimization or fail to preserve scene geometry in a feed-forward model.", "method": "Our method involves three key components: reconstructing the dynamic 3D scene from covisible pixels, using a feed-forward 2D video diffusion model for inpainting hidden pixels, and training a video inpainting diffusion model (CogNVS) in a self-supervised manner using 2D videos.", "result": "The approach proposed, CogNVS, achieves superior performance in novel-view synthesis of dynamic scenes from monocular videos compared to almost all previous methods.", "conclusion": "Our introduced approach, CogNVS, demonstrates significant advancements in the synthesis of novel views for dynamic scenes captured by monocular videos, showcasing a blend of 3D scene reconstruction and 2D diffusion-based inpainting."}}
{"id": "2507.12705", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12705", "abs": "https://arxiv.org/abs/2507.12705", "authors": ["Potsawee Manakul", "Woody Haosheng Gan", "Michael J. Ryan", "Ali Sartaz Khan", "Warit Sirichotedumrong", "Kunat Pipatanakul", "William Held", "Diyi Yang"], "title": "AudioJudge: Understanding What Works in Large Audio Model Based Speech Evaluation", "comment": null, "summary": "Current speech evaluation suffers from two critical limitations: the need and\ndifficulty of designing specialized systems targeting individual audio\ncharacteristics, and poor correlation between automatic evaluation methods and\nhuman preferences. This work presents a systematic study of Large Audio Model\n(LAM) as a Judge, AudioJudge, investigating whether it can provide a unified\nevaluation framework that addresses both challenges. We systematically explore\nAudioJudge across audio characteristic detection tasks, including\npronunciation, speaking rate, speaker identification and speech quality, and\nsystem-level human preference simulation for automated benchmarking. We\ninvestigate different prompt engineering strategies, finding that audio\nconcatenation combined with in-context learning significantly improves\nperformance across both audio characteristic detection and human preference\nsimulation tasks. We further introduce a multi-aspect ensemble AudioJudge to\nenable general-purpose multi-aspect audio evaluation. This method decomposes\nspeech assessment into specialized judges for lexical content, speech quality,\nand paralinguistic features, achieving up to 0.91 Spearman correlation with\nhuman preferences on our system ranking benchmark. Robustness analysis reveals\nthat while LAMs maintain strong performance under acoustic noise, they exhibit\nsignificant verbosity and positional biases that require careful mitigation.", "AI": {"tldr": "A study on the use of a Large Audio Model (LAM) as AudioJudge finds that it can serve as a unified framework for assessing various audio characteristics and simulating human preferences effectively, with a multi-aspect ensemble reaching up to 0.91 Spearman correlation with human preferences, but showing biases in noisy conditions.", "motivation": "The paper's motivation stems from the need to address the limitations in current speech evaluation methodologies, particularly the challenges in aligning automated evaluations with human auditory preferences and the necessity for specialized systems for different audio features.", "method": "Current speech evaluation methods are challenged by the need for specialized systems for individual audio characteristics and low correlation with human preferences. This paper studies the use of Large Audio Model (LAM) as a 'Judge', focusing on its ability to provide a unified evaluation framework across various audio tasks, including pronunciation, speaking rate, speaker identification, speech quality, and simulation of human preferences. It explores different prompting strategies and introduces an ensemble method for multi-aspect evaluation.", "result": "The study finds that combining audio concatenation with in-context learning significantly improves the performance of AudioJudge in audio characteristic detection and human preference simulation. A multi-aspect ensemble approach, comprising specialized judges for different speech evaluation facets, achieves 0.91 Spearman correlation with human preferences. However, LAMs exhibit verbosity and positional biases under noisy conditions.", "conclusion": "The paper concludes that a Large Audio Model trained with specific prompt engineering strategies can effectively serve as a unified evaluation framework for automated benchmarking against human preferences, although there are identified biases that warrant further research."}}
{"id": "2507.12663", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12663", "abs": "https://arxiv.org/abs/2507.12663", "authors": ["Inamullah", "Ernesto Elias Vidal Rosas", "Imran Razzak", "Shoaib Jameel"], "title": "Integrated Oculomics and Lipidomics Reveal Microvascular Metabolic Signatures Associated with Cardiovascular Health in a Healthy Cohort", "comment": null, "summary": "Cardiovascular disease (CVD) remains the leading global cause of mortality,\nyet current risk stratification methods often fail to detect early, subclinical\nchanges. Previous studies have generally not integrated retinal\nmicrovasculature characteristics with comprehensive serum lipidomic profiles as\npotential indicators of CVD risk. In this study, an innovative imaging omics\nframework was introduced, combining retinal microvascular traits derived\nthrough deep learning based image processing with serum lipidomic data to\nhighlight asymptomatic biomarkers of cardiovascular risk beyond the\nconventional lipid panel. This represents the first large scale, covariate\nadjusted and stratified correlation analysis conducted in a healthy population,\nwhich is essential for identifying early indicators of disease. Retinal\nphenotypes were quantified using automated image analysis tools, while serum\nlipid profiling was performed by Ultra High Performance Liquid Chromatography\nElectrospray ionization High resolution mass spectrometry (UHPLC ESI HRMS).\nStrong, age- and sex-independent correlations were established, particularly\nbetween average artery width, vessel density, and lipid subclasses such as\ntriacylglycerols (TAGs), diacylglycerols (DAGs), and ceramides (Cers). These\nassociations suggest a converging mechanism of microvascular remodeling under\nmetabolic stress. By linking detailed\n  vascular structural phenotypes to specific lipid species, this study fills a\ncritical gap in the understanding of early CVD pathogenesis. This integration\nnot only offers a novel perspective on microvascular metabolic associations but\nalso presents a significant opportunity for the identification of robust,\nnon-invasive biomarkers. Ultimately, these findings may support improved early\ndetection, targeted prevention, and personalized approaches in cardiovascular\nhealthcare.", "AI": {"tldr": "本文提出一种成像组学框架，结合视网膜血管特征与脂质组学，首次在大样本健康人群中研究了这些特征之间的相关性，发现对心血管疾病早期检测和预防有重要价值。", "motivation": "现有的心血管疾病（CVD）风险分层方法常常未能早期检测到无症状变化。该研究结合视网膜微血管特征与血清脂质组学数据，以识别无症状的心血管风险生物标志物，超越常规血脂分析。", "method": "介绍了结合深度学习图像处理提取的视网膜微血管特征与血清脂质组学数据的创新性成像组学框架，利用自动化图像分析工具量化解剖视网膜表型，通过超高效液相色谱-电喷雾电离-高分辨率质谱（UHPLC-ESI-HRMS）进行血清脂质分析。", "result": "在健康人群中首次进行了大规模的、协变量调整和分层的相关性分析，发现了视网膜表型（如平均动脉宽度、血管密度）与脂质亚类（如三酰甘油（TAGs）、二酰甘油（DAGs）、鞘脂类（Cers））之间存在强的、与年龄和性别无关的相关性。", "conclusion": "研究表明，将详细的血管结构表型与特定脂质物种联系起来，可以填补对CVD早期发病机制理解的空白，为非侵入性生物标志物的识别提供了新视角，并有可能支持CVD的早期检测、个性化预防和治疗。"}}
{"id": "2507.12720", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12720", "abs": "https://arxiv.org/abs/2507.12720", "authors": ["Abraham Toluase Owodunni", "Orevaoghene Ahia", "Sachin Kumar"], "title": "FLEXITOKENS: Flexible Tokenization for Evolving Language Models", "comment": null, "summary": "Language models (LMs) are challenging to adapt to new data distributions by\nsimple finetuning. This is due to the rigidity of their subword tokenizers,\nwhich typically remain unchanged during adaptation. This inflexibility often\nleads to inefficient tokenization, causing overfragmentation of\nout-of-distribution domains, unseen languages, or scripts. In this work, we\ndevelop byte-level LMs with learnable tokenizers to make tokenization adaptive.\nOur models include a submodule that learns to predict boundaries between the\ninput byte sequence, encoding it into variable-length segments. Existing\ntokenizer-free methods train this boundary predictor using an auxiliary loss\nthat enforces a fixed compression rate across the training corpus, introducing\na new kind of rigidity. We propose FLEXITOKENS, a simplified training objective\nthat enables significantly greater flexibility during adaptation. Evaluating\nacross multiple multilingual benchmarks, morphologically diverse tasks, and\ndomains, we demonstrate that FLEXITOKENS consistently reduces token\nover-fragmentation and achieves up to 10\\% improvements on downstream task\nperformance compared to subword and other gradient-based tokenizers. Code and\ndata for our experiments will be released at\nhttps://github.com/owos/flexitokens", "AI": {"tldr": "研究开发了具备可学习分词器的字节级语言模型，提出了FLEXITOKENS训练目标，于多种任务上相比传统方法提升了性能和适应性。", "motivation": "语言模型在通过简单微调适应新的数据分布时面临挑战，主要是因为子词分词器在适应过程中通常保持不变，这种不变性导致分词效率低下，特别是在处理数据分布之外的领域、未见过的语言或脚本时。", "method": "本研究开发了字节级语言模型，并采用可学习的分词器以使分词过程具有适应性。模型中包含了预测输入字节序列分界线的子模块，从而将其编码为可变长度的片段。研究中提出了FLEXITOKENS，这是一种简化的训练目标，能显著提高适应性。", "result": "在多个多语言基准测试、形态多样化的任务和领域中进行评估，FLEXITOKENS显著减少了分词过度碎片化的问题，并且在下游任务性能上比传统的子词和其他基于梯度的分词器提升了高达10%。", "conclusion": "研究结果表明，使用FLEXITOKENS技术的语言模型能够在多种应用场景下更有效地进行分词，显示出比现有模型更好的适应性及性能提升。"}}
{"id": "2507.12675", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.12675", "abs": "https://arxiv.org/abs/2507.12675", "authors": ["Christina Thrainer", "Md Meftahul Ferdaus", "Mahdi Abdelguerfi", "Christian Guetl", "Steven Sloan", "Kendall N. Niles", "Ken Pathak"], "title": "FORTRESS: Function-composition Optimized Real-Time Resilient Structural Segmentation via Kolmogorov-Arnold Enhanced Spatial Attention Networks", "comment": null, "summary": "Automated structural defect segmentation in civil infrastructure faces a\ncritical challenge: achieving high accuracy while maintaining computational\nefficiency for real-time deployment. This paper presents FORTRESS\n(Function-composition Optimized Real-Time Resilient Structural Segmentation), a\nnew architecture that balances accuracy and speed by using a special method\nthat combines depthwise separable convolutions with adaptive Kolmogorov-Arnold\nNetwork integration. FORTRESS incorporates three key innovations: a systematic\ndepthwise separable convolution framework achieving a 3.6x parameter reduction\nper layer, adaptive TiKAN integration that selectively applies function\ncomposition transformations only when computationally beneficial, and\nmulti-scale attention fusion combining spatial, channel, and KAN-enhanced\nfeatures across decoder levels. The architecture achieves remarkable efficiency\ngains with 91% parameter reduction (31M to 2.9M), 91% computational complexity\nreduction (13.7 to 1.17 GFLOPs), and 3x inference speed improvement while\ndelivering superior segmentation performance. Evaluation on benchmark\ninfrastructure datasets demonstrates state-of-the-art results with an F1- score\nof 0.771 and a mean IoU of 0.677, significantly outperforming existing methods\nincluding U-Net, SA-UNet, and U- KAN. The dual optimization strategy proves\nessential for optimal performance, establishing FORTRESS as a robust solution\nfor practical structural defect segmentation in resource-constrained\nenvironments where both accuracy and computational efficiency are paramount.\nComprehensive architectural specifications are provided in the Supplemental\nMaterial. Source code is available at URL:\nhttps://github.com/faeyelab/fortress-paper-code.", "AI": {"tldr": "FORTRESS, a new architecture for structural defect segmentation in civil infrastructure, balances accuracy and computational efficiency through depthwise separable convolutions and adaptive Kolmogorov-Arnold Network integration, achieving state-of-the-art performance with remarkable efficiency.", "motivation": "The primary motivation is to tackle the challenge of achieving high accuracy in automated structural defect segmentation while ensuring computational efficiency for real-time deployment in civil infrastructure.", "method": "The method, dubbed FORTRESS, employs a combination of depthwise separable convolutions and adaptive Kolmogorov-Arnold Network integration. It includes a systematic depthwise separable convolution framework that reduces parameters, adaptive TiKAN integration, and multi-scale attention fusion to enhance segmentation performance.", "result": "FORTRESS achieves 91% parameter reduction, 91% computational complexity reduction, and a 3x improvement in inference speed. On benchmark datasets, it outperforms existing methods with an F1-score of 0.771 and a mean IoU of 0.677.", "conclusion": "FORTRESS establishes itself as a robust solution for real-time and efficient structural defect segmentation in civil infrastructure, particularly in resource-constrained settings where both accuracy and computational efficiency are critical."}}
{"id": "2507.12724", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12724", "abs": "https://arxiv.org/abs/2507.12724", "authors": ["Richard Sproat", "Tianyu Zhao", "Llion Jones"], "title": "TransEvalnia: Reasoning-based Evaluation and Ranking of Translations", "comment": null, "summary": "We present TransEvalnia, a prompting-based translation evaluation and ranking\nsystem that uses reasoning in performing its evaluations and ranking. This\nsystem presents fine-grained evaluations based on a subset of the\nMultidimensional Quality Metrics (https://themqm.org/), returns an assessment\nof which translation it deems the best, and provides numerical scores for the\nvarious dimensions and for the overall translation. We show that TransEvalnia\nperforms as well as or better than the state-of-the-art MT-Ranker (Moosa et al.\n2024) on our own English-Japanese data as well as several language pairs from\nvarious WMT shared tasks. Using Anthropic's Claude-3.5-Sonnet and\nQwen-2.5-72B-Instruct as the evaluation LLMs, we show that the evaluations\nreturned are deemed highly acceptable to human raters, and that the scores\nassigned to the translations by Sonnet, as well as other LLMs, correlate well\nwith scores assigned by the human raters. We also note the sensitivity of our\nsystem -- as well as MT-Ranker -- to the order in which the translations are\npresented, and we propose methods to address this position bias. All data,\nincluding the system's evaluation and reasoning, human assessments, as well as\ncode is released.", "AI": {"tldr": "本研究介绍了一个名为TransEvalnia的基于推理和细粒度评估的翻译评价系统，该系统在多个语言对的测试中表现出色，并且其评分与人类评分的相关度很高。", "motivation": "TransEvalnia旨在解决现有翻译评价系统的局限，通过提供基于多维质量指标的详细评估和排名来改进翻译质量评价。", "method": "本文提出了一种基于提示的翻译评估和排名系统TransEvalnia，该系统使用推理来执行其评估和排名工作。此系统基于Multidimensional Quality Metrics的子集进行细粒度的评估，并提供每个翻译维度的数值评分和整体翻译评分。为了进行评估，TransEvalnia使用了Anthropic的Claude-3.5-Sonnet和Qwen-2.5-72B-Instruct作为评估的大规模语言模型。", "result": "TransEvalnia在英语-日语数据和WMT共享任务的多种语言对中，与现有的MT-Ranker相比表现持平或更优。实验表明，由Sonnet等大规模语言模型给出的评分与人类评分的相关性良好。此外，作者还发现了系统对翻译展示顺序的敏感性，并提出了缓解这一位置偏差的方法。", "conclusion": "研究结果显示，TransEvalnia系统作为一种基于语言模型的推理方法，相较于现有的优于或持平于与MT-Ranker的性能。敏感性分析表明系统对顺序有反应，提出了对策。该系统的全部数据、评估和推理情况、以及代码已公开，证明了其可靠性和透明度。"}}
{"id": "2507.12714", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2507.12714", "abs": "https://arxiv.org/abs/2507.12714", "authors": ["Yang Yang", "Dongni Mao", "Hiroaki Santo", "Yasuyuki Matsushita", "Fumio Okura"], "title": "NeuraLeaf: Neural Parametric Leaf Models with Shape and Deformation Disentanglement", "comment": "IEEE/CVF International Conference on Computer Vision (ICCV 2025),\n  Project: https://neuraleaf-yang.github.io/", "summary": "We develop a neural parametric model for 3D leaves for plant modeling and\nreconstruction that are essential for agriculture and computer graphics. While\nneural parametric models are actively studied for humans and animals, plant\nleaves present unique challenges due to their diverse shapes and flexible\ndeformation. To this problem, we introduce a neural parametric model for\nleaves, NeuraLeaf. Capitalizing on the fact that flattened leaf shapes can be\napproximated as a 2D plane, NeuraLeaf disentangles the leaves' geometry into\ntheir 2D base shapes and 3D deformations. This representation allows learning\nfrom rich sources of 2D leaf image datasets for the base shapes, and also has\nthe advantage of simultaneously learning textures aligned with the geometry. To\nmodel the 3D deformation, we propose a novel skeleton-free skinning model and\ncreate a newly captured 3D leaf dataset called DeformLeaf. We show that\nNeuraLeaf successfully generates a wide range of leaf shapes with deformation,\nresulting in accurate model fitting to 3D observations like depth maps and\npoint clouds. Our implementation and dataset are available at\nhttps://neuraleaf-yang.github.io/.", "AI": {"tldr": "NeuraLeaf 是一种用于植物建模和重构的 3D 神经参数模型，解决了植物叶片形状多样和变形灵活所带来的独特挑战。", "motivation": "研究 3D 叶子模型对于农业和计算机图形学非常重要，但叶子的形状多样和容易变形给模型建立带来了挑战。", "method": "Structure", "result": "{\"tldr\": \"NeuraLeaf 是一种用于植物建模和重构的 3D 神经参数模型，解决了植物叶片形状多样和变形灵活所带来的独特挑战。\", \"motivation\": \"研究 3D 叶子模型对于农业和计算机图形学非常重要，但叶子的形状多样和容易变形给模型建立带来了挑战。\", \"method\": \"NeuraLeaf 将叶子的几何结构分解为 2D 基础形状和 3D 变形。2D 基础形状可以从丰富的 2D 叶子图像数据集中进行学习，同时学习与几何结构对齐的纹理；3D 变形则通过一个新的无骨架蒙皮模型进行建模，并创建了一个名为 DeformLeaf 的 3D 叶子数据集。\", \"result\": \"NeuraLeaf 可以生成一系列具有变形的叶子形状，对 3D 观察结果比如深度图和点云具有精确的模型拟合能力。\", \"conclusion\": \"NeuraLeaf 是一种能成功用于植物建模和重构的有效 3D 神经参数模型。\"}", "conclusion": "NeuraLeaf 是一种能成功用于植物建模和重构的有效 3D 神经参数模型。"}}
{"id": "2507.12732", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.12732", "abs": "https://arxiv.org/abs/2507.12732", "authors": ["Fuya Nakamori", "Yin Jou Huang", "Fei Cheng"], "title": "Strategy Adaptation in Large Language Model Werewolf Agents", "comment": "7 pages, 2 figures", "summary": "This study proposes a method to improve the performance of Werewolf agents by\nswitching between predefined strategies based on the attitudes of other players\nand the context of conversations. While prior works of Werewolf agents using\nprompt engineering have employed methods where effective strategies are\nimplicitly defined, they cannot adapt to changing situations. In this research,\nwe propose a method that explicitly selects an appropriate strategy based on\nthe game context and the estimated roles of other players. We compare the\nstrategy adaptation Werewolf agents with baseline agents using implicit or\nfixed strategies and verify the effectiveness of our proposed method.", "AI": {"tldr": "This paper presents a method for Werewolf agents to adapt their strategies based on game context and player roles, showing improved performance over agents with static strategies.", "motivation": "The motivation is to overcome the limitation of prior works where Werewolf agents using prompt engineering cannot adapt to changing situations.", "method": "This paper proposes a method for Werewolf agents to improve their performance by switching between predefined strategies based on the attitudes of other players and the context of conversations.", "result": "The research compares agents with the proposed strategy adaptation method against baseline agents using implicit or fixed strategies, demonstrating the effectiveness of the new approach.", "conclusion": "The conclusion is that Werewolf agents which adapt strategies explicitly based on game context and estimated roles of other players perform better than those with implicit or fixed strategies."}}
{"id": "2507.12727", "categories": ["cs.CV", "I.4"], "pdf": "https://arxiv.org/pdf/2507.12727", "abs": "https://arxiv.org/abs/2507.12727", "authors": ["Peijun Wang", "Jinhua Zhao"], "title": "SOD-YOLO: Enhancing YOLO-Based Detection of Small Objects in UAV Imagery", "comment": null, "summary": "Small object detection remains a challenging problem in the field of object\ndetection. To address this challenge, we propose an enhanced YOLOv8-based\nmodel, SOD-YOLO. This model integrates an ASF mechanism in the neck to enhance\nmulti-scale feature fusion, adds a Small Object Detection Layer (named P2) to\nprovide higher-resolution feature maps for better small object detection, and\nemploys Soft-NMS to refine confidence scores and retain true positives.\nExperimental results demonstrate that SOD-YOLO significantly improves detection\nperformance, achieving a 36.1% increase in mAP$_{50:95}$ and 20.6% increase in\nmAP$_{50}$ on the VisDrone2019-DET dataset compared to the baseline model.\nThese enhancements make SOD-YOLO a practical and efficient solution for small\nobject detection in UAV imagery. Our source code, hyper-parameters, and model\nweights are available at https://github.com/iamwangxiaobai/SOD-YOLO.", "AI": {"tldr": "本研究介绍了SOD-YOLO，这是基于YOLOv8的一种改进的模型，用于提升小物体检测的性能。", "motivation": "小物体检测在目标检测领域是一个艰巨的挑战。本研究旨在通过提出一种增强的YOLOv8模型（SOD-YOLO）来处理这一难题。", "method": "本研究提出了一个基于YOLOv8增强的小物体检测模型SOD-YOLO。该模型在颈部集成了一个多尺度特征融合的ASF机制，添加了一个小型物体检测层P2以提供更高分辨率的特征图，并采用了Soft-NMS来优化置信得分从而提高真阳性保留。", "result": "实验结果显示，SOD-YOLO在VisDrone2019-DET数据集上的mAP$_{50:95}$和mAP$_{50}$分别比基准模型提高了36.1%和20.6%，显著提升了检测性能。", "conclusion": "SOD-YOLO通过结合多尺度特征增强、精确的小尺度特征层改进以及精良的后处理实现了对于小型物体检测性能的有效提升，该项目适用于无人机场景下的图像分析。"}}
{"id": "2507.12759", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12759", "abs": "https://arxiv.org/abs/2507.12759", "authors": ["Yunxiang Zhang", "Muhammad Khalifa", "Lechen Zhang", "Xin Liu", "Ayoung Lee", "Xinliang Frederick Zhang", "Farima Fatahi Bayat", "Lu Wang"], "title": "Logit Arithmetic Elicits Long Reasoning Capabilities Without Training", "comment": null, "summary": "Large reasoning models (LRMs) can do complex reasoning via long\nchain-of-thought (CoT) involving cognitive strategies such as backtracking and\nself-correction. Recent studies suggest that some models inherently possess\nthese long reasoning abilities, which may be unlocked via extra training. Our\nwork first investigates whether we can elicit such behavior without any\ntraining. To this end, we propose a decoding-time approach, ThinkLogit, which\nutilizes logits arithmetic (Liu et al., 2024) to tune a target large LM for\nlong reasoning using a substantially smaller model as guider. We then show that\nwe can further boost performance by training the guider model with preference\noptimization over correct/incorrect reasoning pairs sampled from both the\ntarget and guider model -- a setup we refer to as ThinkLogit-DPO. Our\nexperiments demonstrate that ThinkLogit and ThinkLogit-DPO achieve a relative\nimprovement in pass@1 by 26% and 29%, respectively, over four mathematical\ndatasets using the Qwen2.5-32B when guided by R1-Distill-Qwen-1.5B -- a model\n21x smaller. Lastly, we show that ThinkLogit can transfer long reasoning skills\nacquired through reinforcement learning, improving pass@1 by 13% relative\ncompared to the Qwen2.5-32B base model. Our work presents a\ncomputationally-efficient method to elicit long reasoning in large models with\nminimal or no additional training.", "AI": {"tldr": "研究提出了解码时利用logits算术调整大模型进行长期推理的方法ThinkLogit，及通过偏好优化训练引导模型的方法ThinkLogit-DPO，实验结果表明它们在无需额外训练的情况下显著提升了大模型的长期推理能力。", "motivation": "研究目的在于探索无需额外训练即可激发大模型执行长链推理的能力。", "method": "提出了一种解码时的策略ThinkLogit，利用logits算术来调整大模型用于长期推理，使用一个小得多的模型作为引导模型。同时提出ThinkLogit-DPO，通过偏好优化训练引导模型，使用从目标模型和引导模型采样的正确/错误推理对来提升性能。", "result": "实验结果显示，相较于Qwen2.5-32B基础模型，在R1-Distill-Qwen-1.5B引导下，ThinkLogit和ThinkLogit-DPO在四个数学数据集上分别实现了26%和29%的通过率绝对提升，表明该方法在最小或无需额外训练情况下激活大模型长期推理能力的有效性。", "conclusion": "提出了计算效率高的方法，可以以最小的或没有额外训练的方式提高大型模型的长链推理能力。"}}
{"id": "2507.12730", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.12730", "abs": "https://arxiv.org/abs/2507.12730", "authors": ["Homare Sueyoshi", "Kiyoshi Nishikawa", "Hitoshi Kiya"], "title": "A Privacy-Preserving Semantic-Segmentation Method Using Domain-Adaptation Technique", "comment": "4 pages, 5 figures, 1 table. Accepted to GCCE 2025", "summary": "We propose a privacy-preserving semantic-segmentation method for applying\nperceptual encryption to images used for model training in addition to test\nimages. This method also provides almost the same accuracy as models without\nany encryption. The above performance is achieved using a domain-adaptation\ntechnique on the embedding structure of the Vision Transformer (ViT). The\neffectiveness of the proposed method was experimentally confirmed in terms of\nthe accuracy of semantic segmentation when using a powerful\nsemantic-segmentation model with ViT called Segmentation Transformer.", "AI": {"tldr": "提出了一种隐私保护的语义分割方法，使用感知加密技术应用于模型训练和测试图像，该方法能保持与未加密模型几乎相同的准确率，通过ViT的嵌入结构调整域适应技术实现，实验验证了使用Segmentation Transformer时的分割准确性。", "motivation": "保护隐私的同时并不过分牺牲模型的准确率，在图像语义分割任务上维持高效性能。", "method": "利用Vision Transformer (ViT)的嵌入结构进行域适应，为训练和测试图像实施感知加密技术。", "result": "实验中，所提方法与未加密模型相比，保持了几乎相同的语义分割准确性。", "conclusion": "所提出的方法在保护图像隐私的同时，能够有效地维持高准确性的语义分割性能。"}}
{"id": "2507.12769", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.12769", "abs": "https://arxiv.org/abs/2507.12769", "authors": ["Keli Zheng", "Zerong Xie"], "title": "Synergy: End-to-end Concept Model", "comment": null, "summary": "In this paper, we present Synergy, a language model that bridges different\nlevels of abstraction in an end-to-end fashion through a learned routing\nmechanism. Focusing on low-level linguistic abstraction, we trained our model\nas a byte-level language model. Our model spontaneously learns to tokenize\nbytes, producing fewer concept tokens than Byte-level Byte Pair Encoder (BBPE)\ntokenizers while keeping comparable performance. By comparing with Llama3, we\nobserved an advantage of Synergy under the same model scale and training\ndataset size. Further studies show that the middle part (the higher abstraction\npart) of our model performs better when positional encodings are removed,\nsuggesting the emergence of position-independent concepts. These findings\ndemonstrate the feasibility of tokenizer-free architectures, paving the way for\nmore robust and flexible pipelines.", "AI": {"tldr": "Synergy模型通过学习路由机制无缝连接不同抽象层级，展示了字节级标记化的高效性，无需传统标记化即可获得较好性能，特别是在去除位置编码后，其抽象部分表现更优，为未来语言模型指明了方向。", "motivation": "针对现有标记化方法在词元数量和模型性能之间的平衡问题，本论文提出了无标记化方法，致力于创建一个更加高效和灵活的语言模型。", "method": "本研究通过训练一种采用学习路由机制的字节级语言模型，旨在无标记化状态下连接不同的抽象层次。主要侧重于模型中自发学习标记化的能力，基于此特征进行性能比较。", "result": "本文提出了一种名为Synergy的语言模型，该模型通过一种学习路由机制在不同抽象层次之间进行无缝连接。研究集中于低层次的语言抽象，将模型作为字节级语言模型进行训练。模型自发地学习字节的标记化过程，相比Byte-level Byte Pair Encoder (BBPE)标记器，它生成的词元更少但性能相当。与Llama3相比，在相同模型规模和训练数据集大小的情况下，Synergy显示出优势。进一步的研究表明，移除位置编码后，模型的中层部分（即更高抽象部分）表现更好，显示出位置独立的概念的出现。这些发现证明了无标记化架构的可行性，为构建更稳健和灵活的流水线铺平了道路。", "conclusion": "研究表明，Synergy不仅在抽象层面上优化了语言模型的表现，而且无需依赖传统的标记化过程，这可能标志着自然语言处理方法的一个重要进步，推动更加高效、灵活的处理方式的发展。"}}
{"id": "2507.12739", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12739", "abs": "https://arxiv.org/abs/2507.12739", "authors": ["Ijazul Haq", "Muhammad Saqib", "Yingjie Zhang"], "title": "Transformer-based Spatial Grounding: A Comprehensive Survey", "comment": null, "summary": "Spatial grounding, the process of associating natural language expressions\nwith corresponding image regions, has rapidly advanced due to the introduction\nof transformer-based models, significantly enhancing multimodal representation\nand cross-modal alignment. Despite this progress, the field lacks a\ncomprehensive synthesis of current methodologies, dataset usage, evaluation\nmetrics, and industrial applicability. This paper presents a systematic\nliterature review of transformer-based spatial grounding approaches from 2018\nto 2025. Our analysis identifies dominant model architectures, prevalent\ndatasets, and widely adopted evaluation metrics, alongside highlighting key\nmethodological trends and best practices. This study provides essential\ninsights and structured guidance for researchers and practitioners,\nfacilitating the development of robust, reliable, and industry-ready\ntransformer-based spatial grounding models.", "AI": {"tldr": "本文综述了2018年至2025年间基于变压器的空间定位方法，包括主导模型架构、常用数据集及评价指标，并为研究者提供了关键方法趋势和最佳实践的见解，以促进稳健可靠的空间定位模型的开发。", "motivation": "虽然基于变压器的空间定位方法在多模态表示和跨模态对齐方面取得了快速进展，但该领域缺乏对当前方法、数据集使用、评估度量和工业适用性的全面总结。本文旨在提供系统性文献综述。", "method": "Structure", "result": "{\n  \"tldr\": \"本文综述了2018年至2025年间基于变压器的空间定位方法，包括主导模型架构、常用数据集及评价指标，并为研究者提供了关键方法趋势和最佳实践的见解，以促进稳健可靠的空间定位模型的开发。\", \n  \"motivation\": \"虽然基于变压器的空间定位方法在多模态表示和跨模态对齐方面取得了快速进展，但该领域缺乏对当前方法、数据集使用、评估度量和工业适用性的全面总结。本文旨在提供系统性文献综述。\", \n  \"method\": \"本文进行了2018年至2025年间基于变压器的空间定位方法的系统性文献回顾，识别研究中的主导模型架构、数据集、评估度量并指出关键方法趋势和最佳实践。\", \n  \"result\": \"本研究确定了主导模型架构、常用数据集及评价指标，并强调了关键方法趋势和最佳实践。\", \n  \"conclusion\": \"本文分析为研究人员和从业者提供了关键的见解和结构化的指导，促进开发出更成熟、可靠和工业应用的空间定位模型。\"}\n}", "conclusion": "本文分析为研究人员和从业者提供了关键的见解和结构化的指导，促进开发出更成熟、可靠和工业应用的空间定位模型。"}}
{"id": "2507.12782", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12782", "abs": "https://arxiv.org/abs/2507.12782", "authors": ["Thinh Hung Truong", "Karin Verspoor", "Trevor Cohn", "Timothy Baldwin"], "title": "Learning Robust Negation Text Representations", "comment": null, "summary": "Despite rapid adoption of autoregressive large language models, smaller text\nencoders still play an important role in text understanding tasks that require\nrich contextualized representations. Negation is an important semantic function\nthat is still not properly captured by such methods, affecting many downstream\napplications relying on text embeddings. We propose a strategy to improve\nnegation robustness of text encoders, by distilling data from large language\nmodels using diverse patterns of negation and hedging. We adopt a standard\ncontrastive learning strategy to finetune a strong BERT-based model, and\nobserve large improvement in negation understanding capabilities while\nmaintaining competitive performance on general benchmarks. In addition, we also\nshow that our method can be adapted to LLMs, leading to improved performance on\nnegation benchmarks.", "AI": {"tldr": "提出了一种通过对比学习提升文本编码器否定理解能力的方法。", "motivation": "尽管自动回归语言模型快速发展，文本编码器在需要丰富上下文表示的文本理解任务中仍发挥重要作用。然而，这类方法未能很好地捕捉否定的重要性，影响了许多依赖文本嵌入的下游应用。该研究的目的是提升文本编码器对否定的识别能力。", "method": "通过从大型语言模型中使用多样化的否定和修饰语模式提取数据，采用标准的对比学习策略微调了一个强大的基于BERT的模型，以提高文本编码器的否定鲁棒性。", "result": "观察到在保持一般基准测试上的竞争力的同时，该方法显著提高了否定理解能力。此外，该方法还能被应用于大型语言模型，并提升了其在否定基准测试上的表现。", "conclusion": "通过对比学习策略，可以显著提升文本编码器对否定的识别能力，并且该策略可以应用于大型语言模型以提高其在否定识别上的性能。"}}
{"id": "2507.12755", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12755", "abs": "https://arxiv.org/abs/2507.12755", "authors": ["Yanchen Guan", "Haicheng Liao", "Chengyue Wang", "Bonan Wang", "Jiaxun Zhang", "Jia Hu", "Zhenning Li"], "title": "Domain-Enhanced Dual-Branch Model for Efficient and Interpretable Accident Anticipation", "comment": null, "summary": "Developing precise and computationally efficient traffic accident\nanticipation system is crucial for contemporary autonomous driving\ntechnologies, enabling timely intervention and loss prevention. In this paper,\nwe propose an accident anticipation framework employing a dual-branch\narchitecture that effectively integrates visual information from dashcam videos\nwith structured textual data derived from accident reports. Furthermore, we\nintroduce a feature aggregation method that facilitates seamless integration of\nmultimodal inputs through large models (GPT-4o, Long-CLIP), complemented by\ntargeted prompt engineering strategies to produce actionable feedback and\nstandardized accident archives. Comprehensive evaluations conducted on\nbenchmark datasets (DAD, CCD, and A3D) validate the superior predictive\naccuracy, enhanced responsiveness, reduced computational overhead, and improved\ninterpretability of our approach, thus establishing a new benchmark for\nstate-of-the-art performance in traffic accident anticipation.", "AI": {"tldr": "本文提出了一种新的交通事故预测方法，该方法集成视觉和文本数据，使用多模态模型并进行了有效的Prompt设计，显示出优于当前最佳性能的表现。", "motivation": "为了支持现代自动驾驶技术，本研究致力于开发一种精确且计算效率高的交通事故预测系统，以便在事故发生时能及时干预和减少损失。", "method": "本研究提出了一种基于双分支架构的事故预测框架，该框架能有效整合行车记录仪视频的视觉信息与交通事故报告的结构化文本数据。此外，我们还提出了一种特征聚合方法，通过大型模型（如GPT-4o和Long-CLIP）来实现多模态输入的无缝集成，并辅以针对性的Prompt工程技术，以生成可操作的反馈和标准化的事故档案。", "result": "我们在几个基准数据集（DAD、CCD 和 A3D）上的综合评估验证了该方法具有更高的预测准确性、改进的响应速度、更低的计算开销和更高的可解释性。", "conclusion": "该研究的方法在交通事故预测方面展示出的新基准性能证明了其在自动驾驶领域的应用潜力。"}}
