<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [CoBA: Counterbias Text Augmentation for Mitigating Various Spurious Correlations via Semantic Triples](https://arxiv.org/abs/2508.21083)
*Kyohoon Jin,Juhwan Choi,Jungmin Yun,Junho Lee,Soojin Jang,Youngbin Kim*

Main category: cs.CL

> CoBA框架通过文本的三元组级别数据增强，同时处理多种偏差，显著提高了模型的鲁棒性和泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 解决深度学习模型由于学习和利用训练数据中的非目标特征导致的性能下降和泛化能力差的问题。

**Method:** 通过将文本分解成主语-谓语-宾语三元组，然后选择性地修改这些三元组来打断非目标特征的关联，最后从调整后的三元组重构文本，从而生成对抗偏差数据，这种方法有助于缓解模型对非目标特征的依赖。

**Result:** 实验表明，CoBA不仅提高了下游任务的表现，还有效地减少了偏差并增强了对分布外数据的韧性。

**Conclusion:** CoBA提供了一个多用途且健壮的解决方案，以应对由非目标特征关联带来的挑战。

**Abstract:** Deep learning models often learn and exploit spurious correlations in
training data, using these non-target features to inform their predictions.
Such reliance leads to performance degradation and poor generalization on
unseen data. To address these limitations, we introduce a more general form of
counterfactual data augmentation, termed counterbias data augmentation, which
simultaneously tackles multiple biases (e.g., gender bias, simplicity bias) and
enhances out-of-distribution robustness. We present CoBA: CounterBias
Augmentation, a unified framework that operates at the semantic triple level:
first decomposing text into subject-predicate-object triples, then selectively
modifying these triples to disrupt spurious correlations. By reconstructing the
text from these adjusted triples, CoBA generates counterbias data that
mitigates spurious patterns. Through extensive experiments, we demonstrate that
CoBA not only improves downstream task performance, but also effectively
reduces biases and strengthens out-of-distribution resilience, offering a
versatile and robust solution to the challenges posed by spurious correlations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [2COOOL: 2nd Workshop on the Challenge Of Out-Of-Label Hazards in Autonomous Driving](https://arxiv.org/abs/2508.21080)
*Ali K. AlShami,Ryan Rabinowitz,Maged Shoman,Jianwu Fang,Lukas Picek,Shao-Yuan Lo,Steve Cruz,Khang Nhut Lam,Nachiket Kamod,Lei-Lei Li,Jugal Kalita,Terrance E. Boult*

Main category: cs.CV

> 第2届2COOOL研讨会将于2025年在夏威夷举行，旨在推动自动驾驶技术在处理未知场景和危险识别方面的研究。

<details>
  <summary>Details</summary>

**Motivation:** 研讨会的动机在于解决自动驾驶技术在实际部署中遇到的未知场景问题，即遇到未见过的危险情况的处理能力不足。

**Method:** 该摘要描述了一个专注于处理自动驾驶中未知场景挑战的研讨会，但并未详细说明具体的方法。

**Result:** 该研讨会旨在成为一个跨学科交流平台，但具体的研究成果及实验结果未提及。

**Conclusion:** 通过此次研讨会，希望能促进新算法和系统的开发，提高自动驾驶车辆在面对未知危险时的识别和规避能力。

**Abstract:** As the computer vision community advances autonomous driving algorithms,
integrating vision-based insights with sensor data remains essential for
improving perception, decision making, planning, prediction, simulation, and
control. Yet we must ask: Why don't we have entirely safe self-driving cars
yet? A key part of the answer lies in addressing novel scenarios, one of the
most critical barriers to real-world deployment. Our 2COOOL workshop provides a
dedicated forum for researchers and industry experts to push the state of the
art in novelty handling, including out-of-distribution hazard detection,
vision-language models for hazard understanding, new benchmarking and
methodologies, and safe autonomous driving practices. The 2nd Workshop on the
Challenge of Out-of-Label Hazards in Autonomous Driving (2COOOL) will be held
at the International Conference on Computer Vision (ICCV) 2025 in Honolulu,
Hawaii, on October 19, 2025. We aim to inspire the development of new
algorithms and systems for hazard avoidance, drawing on ideas from anomaly
detection, open-set recognition, open-vocabulary modeling, domain adaptation,
and related fields. Building on the success of its inaugural edition at the
Winter Conference on Applications of Computer Vision (WACV) 2025, the workshop
will feature a mix of academic and industry participation.

</details>


### [3] [Advanced Deep Learning Techniques for Classifying Dental Conditions Using Panoramic X-Ray Images](https://arxiv.org/abs/2508.21088)
*Alireza Golkarieh,Kiana Kiashemshaki,Sajjad Rezvani Boroujeni*

Main category: cs.CV

> 研究通过评估三种深度学习方法发现，结合CNN特征提取与传统分类器的混合模型在全景X光影像牙齿状况分类任务上表现最佳。

<details>
  <summary>Details</summary>

**Motivation:** 这篇论文的动机在于探究基于深度学习的方法用于自动分类全景X光图像的牙齿状况。

**Method:** 本研究评估了三种方法：自定义卷积神经网络(CNN)，结合CNN特征提取与传统分类器的混合模型，以及微调的预训练架构。

**Result:** 实验结果显示，混合CNN随机森林模型达到了最高的性能，准确率为85.4%，超过了自定义CNN基线模型的74.3%。在预训练模型中，VGG16表现出最优性能，准确率为82.3%，其次是Xception和ResNet50。

**Conclusion:** 研究结果表明，混合模型提升了对形态相似的牙齿状况的区分能力，并提供了一种高效、可靠的表现。这些发现表明，结合基于CNN的特征提取和集成分类器为实现自动化的牙齿诊断提供了一条实用路径，同时也强调了需要更大的数据集和进一步的临床验证。

**Abstract:** This study investigates deep learning methods for automated classification of
dental conditions in panoramic X-ray images. A dataset of 1,512 radiographs
with 11,137 expert-verified annotations across four conditions fillings,
cavities, implants, and impacted teeth was used. After preprocessing and class
balancing, three approaches were evaluated: a custom convolutional neural
network (CNN), hybrid models combining CNN feature extraction with traditional
classifiers, and fine-tuned pre-trained architectures. Experiments employed 5
fold cross validation with accuracy, precision, recall, and F1 score as
evaluation metrics. The hybrid CNN Random Forest model achieved the highest
performance with 85.4% accuracy, surpassing the custom CNN baseline of 74.3%.
Among pre-trained models, VGG16 performed best at 82.3% accuracy, followed by
Xception and ResNet50. Results show that hybrid models improve discrimination
of morphologically similar conditions and provide efficient, reliable
performance. These findings suggest that combining CNN-based feature extraction
with ensemble classifiers offers a practical path toward automated dental
diagnostic support, while also highlighting the need for larger datasets and
further clinical validation.

</details>


### [4] [Q-Align: Alleviating Attention Leakage in Zero-Shot Appearance Transfer via Query-Query Alignment](https://arxiv.org/abs/2508.21090)
*Namu Kim,Wonbin Kweon,Minsoo Kim,Hwanjo Yu*

Main category: cs.CV

> Q-Align is introduced to mitigate attention leakage in zero-shot appearance transfer through Query-Query alignment, Key-Value rearrangement, and Attention refinement, exhibiting superior appearance fidelity and structure preservation.

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the challenge of Attention Leakage in zero-shot appearance transfer with large-scale image generation models to improve semantic alignment.

**Method:** Query-Query alignment, Key-Value rearrangement, and Attention refinement are the core techniques of Q-Align to solve the attention leakage issue in zero-shot appearance transfer.

**Result:** Experiments and analysis validate Q-Align's effectiveness, showing superior appearance fidelity and competitive structure preservation over state-of-the-art methods.

**Conclusion:** The proposed Q-Align method is shown to be effective in mitigating attention leakage and enhancing semantic alignment in zero-shot appearance transfer tasks.

**Abstract:** We observe that zero-shot appearance transfer with large-scale image
generation models faces a significant challenge: Attention Leakage. This
challenge arises when the semantic mapping between two images is captured by
the Query-Key alignment. To tackle this issue, we introduce Q-Align, utilizing
Query-Query alignment to mitigate attention leakage and improve the semantic
alignment in zero-shot appearance transfer. Q-Align incorporates three core
contributions: (1) Query-Query alignment, facilitating the sophisticated
spatial semantic mapping between two images; (2) Key-Value rearrangement,
enhancing feature correspondence through realignment; and (3) Attention
refinement using rearranged keys and values to maintain semantic consistency.
We validate the effectiveness of Q-Align through extensive experiments and
analysis, and Q-Align outperforms state-of-the-art methods in appearance
fidelity while maintaining competitive structure preservation.

</details>


### [5] [ERTACache: Error Rectification and Timesteps Adjustment for Efficient Diffusion](https://arxiv.org/abs/2508.21091)
*Xurui Peng,Hong Liu,Chenqian Yan,Rui Ma,Fangmin Chen,Xing Wang,Zhihua Wu,Songwei Liu,Mingbao Lin*

Main category: cs.CV

> ERTACache is a caching framework for diffusion models that analyzes and mitigates caching errors, achieving 2x speedup in inference while preserving visual quality in tasks like image and video generation.

<details>
  <summary>Details</summary>

**Motivation:** The motivation stems from the computational inefficiency of diffusion models due to their iterative inference process. Feature caching can speed up the process, but naive reuse can lead to quality degradation. ERTACache aims to address this by formally analyzing and mitigating caching errors.

**Method:** In this paper, the authors propose ERTACache, a caching framework for diffusion models. ERTACache analyzes and rectifies the cumulative error from feature caching, decomposing it into feature shift error and step amplification error. The method includes an offline residual profiling stage, dynamic integration interval adjustment using a trajectory-aware correction coefficient, and an analytical approximation of cache-induced errors using a residual linearization model.

**Result:** ERTACache achieves significant speedups of up to 2x for inference, while also maintaining or improving visual quality and fidelity in image and video generation tasks.

**Conclusion:** The paper concludes that ERTACache offers a robust caching strategy that accelerates inference in diffusion models without sacrificing quality, as demonstrated by its performance across various benchmarks.

**Abstract:** Diffusion models suffer from substantial computational overhead due to their
inherently iterative inference process. While feature caching offers a
promising acceleration strategy by reusing intermediate outputs across
timesteps, naive reuse often incurs noticeable quality degradation. In this
work, we formally analyze the cumulative error introduced by caching and
decompose it into two principal components: feature shift error, caused by
inaccuracies in cached outputs, and step amplification error, which arises from
error propagation under fixed timestep schedules. To address these issues, we
propose ERTACache, a principled caching framework that jointly rectifies both
error types. Our method employs an offline residual profiling stage to identify
reusable steps, dynamically adjusts integration intervals via a
trajectory-aware correction coefficient, and analytically approximates
cache-induced errors through a closed-form residual linearization model.
Together, these components enable accurate and efficient sampling under
aggressive cache reuse. Extensive experiments across standard image and video
generation benchmarks show that ERTACache achieves up to 2x inference speedup
while consistently preserving or even improving visual quality. Notably, on the
state-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x
acceleration with minimal VBench degradation, effectively maintaining baseline
fidelity while significantly improving efficiency. The code is available at
https://github.com/bytedance/ERTACache.

</details>
