<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 26]
- [cs.CV](#cs.CV) [Total: 83]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
*Tian Lan,Xiangdong Su,Xu Liu,Ruirui Wang,Ke Chang,Jiang Li,Guanglai Gao*

Main category: cs.CL

> 研究者提出了一个多任务中文偏见评估基准（McBE）来评估大型语言模型中的偏见，覆盖多类别和多种评估任务，对多个流行LLMs进行了评估，并分析了结果。

<details>
  <summary>Details</summary>

**Motivation:** 由于大型语言模型(LLMs)逐渐应用于各种自然语言处理(NLP)任务，其内在偏见逐渐显现出来。现有的偏见评估数据集主要针对英语和北美文化，对于其他文化并不完全适用，且缺乏中文和中国文化背景的数据集。

**Method:** 为了应对现有偏见评估数据集的局限性，研究者提出了一个多任务中文偏见评估基准 (McBE)，其中包括4,077个偏见评估实例，覆盖了12个单一偏见类别和82个子类别，并引入了5个评估任务。

**Result:** 研究者评估了不同系列和参数大小的流行LLMs，发现这些模型表现出了不同程度的偏见，并深入分析了结果，提供了关于LLMs偏见的新见解。

**Conclusion:** 研究结果显示，多任务中文偏见评估基准可以全面测量多种偏见，对于理解LLM们的偏见有重要的贡献。

**Abstract:** As large language models (LLMs) are increasingly applied to various NLP
tasks, their inherent biases are gradually disclosed. Therefore, measuring
biases in LLMs is crucial to mitigate its ethical risks. However, most existing
bias evaluation datasets focus on English and North American culture, and their
bias categories are not fully applicable to other cultures. The datasets
grounded in the Chinese language and culture are scarce. More importantly,
these datasets usually only support single evaluation tasks and cannot evaluate
the bias from multiple aspects in LLMs. To address these issues, we present a
Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias
evaluation instances, covering 12 single bias categories, 82 subcategories and
introducing 5 evaluation tasks, providing extensive category coverage, content
diversity, and measuring comprehensiveness. Additionally, we evaluate several
popular LLMs from different series and with parameter sizes. In general, all
these LLMs demonstrated varying degrees of bias. We conduct an in-depth
analysis of results, offering novel insights into bias in LLMs.

</details>


### [2] [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)
*Keyan Jin,Yapeng Wang,Leonel Santos,Tao Fang,Xu Yang,Sio Kei Im,Hugo Gonçalo Oliveira*

Main category: cs.CL

> 该研究首次系统评估了解决方案中的逐步推理大型语言模型以及非推理大型语言模型在对话摘要任务中的表现，结果显示推理模型并不总是能改善对话摘要质量，反而可能出现冗长、不一致等问题。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探索与评估在需要同时具备抽象能力与简洁性的对话场景中，基于逐步推理架构的语言模型表现。

**Method:** 研究涵盖了三种主要的对话摘要范式——通用型、角色导向型和查询导向型，并应用了多个强有力的基准测试和先进的评估协议，包括基于大型语言模型的自动指标和以人为灵感的标准。

**Result:** 研究表明，与非推理大型语言模型相比，推理大型语言模型往往会导致摘要冗长、事实不一致，并且不太简洁。

**Conclusion:** 研究提出了对于现实世界对话摘要任务中推理大型语言模型的局限性，并强调了针对该任务需要采取目标化的模型构建和评价策略。

**Abstract:** Dialogue summarization is a challenging task with significant practical value
in customer service, meeting analysis, and conversational AI. Although large
language models (LLMs) have achieved substantial progress in summarization
tasks, the performance of step-by-step reasoning architectures-specifically
Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and
DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent
abstraction and conciseness. In this work, we present the first comprehensive
and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning
LLMs across three major paradigms-generic, role-oriented, and query-oriented
dialogue summarization. Our study spans diverse languages, domains, and summary
lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and
advanced evaluation protocols that include both LLM-based automatic metrics and
human-inspired criteria. Contrary to trends in other reasoning-intensive tasks,
our findings show that explicit stepwise reasoning does not consistently
improve dialogue summarization quality. Instead, reasoning LLMs are often prone
to verbosity, factual inconsistencies, and less concise summaries compared to
their non-reasoning counterparts. Through scenario-specific analyses and
detailed case studies, we further identify when and why explicit reasoning may
fail to benefit-or even hinder-summarization in complex dialogue contexts. Our
work provides new insights into the limitations of current reasoning LLMs and
highlights the need for targeted modeling and evaluation strategies for
real-world dialogue summarization.

</details>


### [3] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
*Wenquan Lu,Yuechuan Yang,Kyle Lee,Yanshu Li,Enqi Liu*

Main category: cs.CL

> 本文通过一套探测技术研究了深度递归Transformer Huginn-3.5B的内部推理行为，发现其内部存在有限的可解释潜在链式思维证据，递归深度增加的效果不大。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在探讨在推理时重复使用层而不增加参数数量的深度递归Transformer中是否能出现这样的推理结构。

**Method:** 我们使用了一套探测技术，包括Logit Lens和Coda Lens，来研究Huginn-3.5B在算术任务上的内部行为。

**Result:** 研究结果发现，通过追踪终结果和中间结果标记的排名轨迹，对可解释的潜在链式思维只存在有限的证据，并且在递归块之间的探测结果存在显著分歧。另外，递归深度的增加仅带来了微小的改进。

**Conclusion:** 最终，实验表明，增加递归深度的效果远远不如显式地外部化推理步骤的模型。

**Abstract:** Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [4] [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)
*Steven Song,Anirudh Subramanyam,Zhenyu Zhang,Aarti Venkat,Robert L. Grossman*

Main category: cs.CL

> 介绍了一种名为GDC Cohort Copilot的开源工具，其功能是将用户自然语言描述的队列需求转化为GDC中可使用的队列筛选条件，且表现优于其他语言模型。

<details>
  <summary>Details</summary>

**Motivation:** Genomic Data Commons (GDC) 提供高质量的癌症基因组数据。尽管用户可以通过图形化Cohort Builder创建复杂的队列，但用户（尤其是新用户）可能难以在众多可能的领域和属性中找到具体的队列描述符。然而，用户更可能通过自由文本自然语言来描述他们所需的队列。

**Method:** 开发并评估了多个大型语言模型（LLMs）来支持GDC Cohort Copilot。其中，一个本地服务的开源GDC Cohort LLM在生成GDC队列方面表现优于GPT-4o。

**Result:** GDC Cohort Copilot是一个开源的协同工具，用于根据用户输入的自然语言描述自动生成GDC队列筛选条件，之后将队列导出到GDC进行进一步分析。交互式用户界面允许用户调整生成的队列。研究展示了本地服务的开源GDC Cohort LLM比GPT-4o在生成GDC队列方面表现更好。

**Conclusion:** GDC Cohort Copilot提供了一个开源的解决方案，能自动生成GDC队列筛选条件，并允许用户通过交互式界面进一步调整队列。该工具能显著提高GDC用户的队列构建效率和准确性。

**Abstract:** Motivation: The Genomic Data Commons (GDC) provides access to high quality,
harmonized cancer genomics data through a unified curation and analysis
platform centered around patient cohorts. While GDC users can interactively
create complex cohorts through the graphical Cohort Builder, users (especially
new ones) may struggle to find specific cohort descriptors across hundreds of
possible fields and properties. However, users may be better able to describe
their desired cohort in free-text natural language.
  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for
curating cohorts from the GDC. GDC Cohort Copilot automatically generates the
GDC cohort filter corresponding to a user-input natural language description of
their desired cohort, before exporting the cohort back to the GDC for further
analysis. An interactive user interface allows users to further refine the
generated cohort. We develop and evaluate multiple large language models (LLMs)
for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC
Cohort LLM achieves better results than GPT-4o prompting in generating GDC
cohorts.
  Availability and implementation: The standalone docker image for GDC Cohort
Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.
Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC
Cohort LLM weights are available at https://huggingface.co/uc-ctds.

</details>


### [5] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
*Hongli Yu,Tinghong Chen,Jiangtao Feng,Jiangjie Chen,Weinan Dai,Qiying Yu,Ya-Qin Zhang,Wei-Ying Ma,Jingjing Liu,Mingxuan Wang,Hao Zhou*

Main category: cs.CL

> 本文提出MemAgent，一种新型代理工作流，通过分段阅读和覆盖策略更新内存来优化长文本任务，展示了在长上下文处理中的卓越能力。

<details>
  <summary>Details</summary>

**Motivation:** 尽管长度外推、高效注意力和内存模块有所改进，但在处理长文本时，仍然面临在时间复杂度线性的情况下处理无限长文档而不降低性能的关键挑战。我们的动机在于解决这一挑战。

**Method:** 我们介绍了一种新型的代理工作流MemAgent，该方法通过分段阅读文本并使用覆盖策略更新记忆来处理长文本任务。我们扩展了DAPO算法，通过独立上下文的多会话生成来促进训练。

**Result:** MemAgent在长上下文能力方面表现出色，从基于32K文本训练的8K上下文外推到3.5M QA任务时，性能下降小于5%，在512K RULER测试中达到了95%以上的准确率。

**Conclusion:** 研究结果表明，MemAgent能够高效地处理长文本任务，并在不同长度的测试中表现出色，证明了其在长文本处理中的潜力。

**Abstract:** Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [6] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

> 本论文提出了DoMIX方法，利用LoRA模块解决了持续领域自适应预训练中效率低、敏感于数据顺序及无法提供特定任务定制模型的问题，实现高效并行的自适应预训练，适用于具体任务且展示出在标准LLM微调场景中的应用潜力。

<details>
  <summary>Details</summary>

**Motivation:** 尽管持续领域自适应预训练在发展预训练模型以增量地融合不同领域的数据集方面已经展现潜力，但现有方法有高计算成本和GPU使用量高、对增量数据顺序敏感以及无法为所有终任务提供定制模型等局限。本论文旨在克服这些局限。

**Method:** 本论文提出了DoMIX方法，利用参数高效的微调（PEFT）方法中的LoRA模块来应对持续领域自适应预训练中的挑战。此方法能够实现高效且并行的领域自适应预训练，并且能够有效利用累积的知识，针对特定任务提供定制化的预训练模型。

**Result:** DoMIX方法证实了其在标准LLM微调场景之外的适用性，并展示了其在持续领域自适应预训练背景下解决现有方法局限性的有效性。

**Conclusion:** 提出的方法展示了在持续领域自适应预训练背景下解决现有方法局限性的有效性，并且具有在标准LLM微调场景中的应用潜力。

**Abstract:** Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [7] [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)
*Christian Jaumann,Annemarie Friedrich,Rainer Lienhart*

Main category: cs.CL

> 本文介绍了一个在科学视觉问答任务中表现出色的系统，该系统使用了两种多模态大型语言模型与多个少样本策略相结合的方法。

<details>
  <summary>Details</summary>

**Motivation:** 本文描述了我们为SciVQA 2025共享任务——科学视觉问答任务开发的系统。

**Method:** 本研究使用了两个多模态大型语言模型的集成，并结合了多种少样本示例检索策略。模型和少样本设置的选择取决于图形和问题类型。

**Result:** 该系统在盲测数据上的表现优异，整体排名第三，且在多个评估指标上达到了85.12的平均F1分数。

**Conclusion:** 在盲测数据上，该系统在七个系统中排名第三，ROUGE-1、ROUGE-L和BERTS的平均F1分数为85.12。代码已公开。

**Abstract:** This paper describes our system for the SciVQA 2025 Shared Task on Scientific
Visual Question Answering. Our system employs an ensemble of two Multimodal
Large Language Models and various few-shot example retrieval strategies. The
model and few-shot setting are selected based on the figure and question type.
We also select answers based on the models' confidence levels. On the blind
test data, our system ranks third out of seven with an average F1 score of
85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.

</details>


### [8] [QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers](https://arxiv.org/abs/2507.02364)
*Pilsung Kang*

Main category: cs.CL

> 本研究展示了QFFN-BERT——一种将FFN模块替换为PQC层的混合量子经典Transformer，在参数高效性和数据效率方面都有不错的性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于FFN参数在标准Transformer编码器模块中占比达到约三分之二，本研究寻求通过替换这些模块中的FFN为PQC层来改善神经网络结构的表现。

**Method:** 本研究将参数化量子电路（PQC）整合到BERT变种的前馈神经网络（FFN）模块中，形成QFFN-BERT。重点在于探究PQC深度、表现力和可训练性之间的权衡。最终的PQC架构包括残差连接、$R_Y$和$R_Z$旋转，以及交替的纠缠策略。

**Result:** 实验在SST-2和DBpedia基准测试上进行，结果显示QFFN-BERT在全数据场景中达到基线精度的102.0%，并将FFN相关的参数减少了超过99%。此外，该模型在少量样本学习情景中表现优异，展示出更高的数据效率。

**Conclusion:** 实验结果及非优化PQC组件无法学习的消融研究结果，证实了当PQC与基础深度学习原则共同设计时，它们可以成为强大的、参数高效的经典FFN替代品。

**Abstract:** Parameterized quantum circuits (PQCs) have recently emerged as promising
components for enhancing the expressibility of neural architectures. In this
work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the
feedforward network (FFN) modules of a compact BERT variant are replaced by
PQC-based layers. This design is motivated by the dominant parameter
contribution of FFNs, which account for approximately two-thirds of the
parameters within standard Transformer encoder blocks. While prior studies have
primarily integrated PQCs into self-attention modules, our work focuses on the
FFN and systematically investigates the trade-offs between PQC depth,
expressibility, and trainability. Our final PQC architecture incorporates a
residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating
entanglement strategy to ensure stable training and high expressibility. Our
experiments, conducted on a classical simulator, on the SST-2 and DBpedia
benchmarks demonstrate two key findings. First, a carefully configured
QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its
classical counterpart in a full-data setting while reducing FFN-specific
parameters by over 99%. Second, our model exhibits a consistent and competitive
edge in few-shot learning scenarios, confirming its potential for superior data
efficiency. These results, supported by an ablation study on a non-optimized
PQC that failed to learn, confirm that PQCs can serve as powerful and
parameter-efficient alternatives to classical FFNs when co-designed with
foundational deep learning principles.

</details>


### [9] [Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection](https://arxiv.org/abs/2507.02378)
*Weijie Lyu,Sheng-Jun Huang,Xuan Xia*

Main category: cs.CL

> The paper presents a parametric model for efficient selection of high-quality training data for code generation, achieving better performance and efficiency with fewer samples compared to full datasets.

<details>
  <summary>Details</summary>

**Motivation:** To improve training efficiency and model performance of large language models (LLMs) in code generation, the paper addresses the issue of data quality being overlooked in current methods that focus on quantity.

**Method:** The authors utilize a parametric model for selecting code data, optimizing it to guarantee distribution consistency and diversity within the subset chosen for training.

**Result:** Experimental results show that the method achieves a 2.4% and 2.3% performance gain in HumanEval and MBPP respectively, using only 10K samples against a 92K-sample baseline, surpassing other sampling methods in performance and efficiency.

**Conclusion:** The method effectively enhances model performance in code generation tasks while significantly reducing the computational cost by selecting high-quality training data efficiently.

**Abstract:** Recent advancements in large language models (LLMs) have significantly
improved code generation and program comprehension, accelerating the evolution
of software engineering. Current methods primarily enhance model performance by
leveraging vast amounts of data, focusing on data quantity while often
overlooking data quality, thereby reducing training efficiency. To address
this, we introduce an approach that utilizes a parametric model for code data
selection, aimed at improving both training efficiency and model performance.
Our method optimizes the parametric model to ensure distribution consistency
and diversity within the selected subset, guaranteeing high-quality data.
Experimental results demonstrate that using only 10K samples, our method
achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled
baseline, outperforming other sampling approaches in both performance and
efficiency. This underscores that our method effectively boosts model
performance while significantly reducing computational costs.

</details>


### [10] [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)
*Mark Atta Mensah,Isaac Wiafe,Akon Ekpezu,Justice Kwame Appati,Jamal-Deen Abdulai,Akosua Nyarkoa Wiafe-Akenten,Frank Ernest Yeboah,Gifty Odame*

Main category: cs.CL

> 该研究通过四个包含不同领域的阿坎语语料库来评估七种阿坎语ASR模型，发现模型在训练领域以外表现不佳，提出需开发特定领域适应技术、自适应路由策略和多语言训练框架。

<details>
  <summary>Details</summary>

**Motivation:** 大多数现有的ASR研究利用领域内的数据集来评估模型，很少评估这些模型在多样化语音环境下的泛化能力。这项研究旨在填补这一空白。

**Method:** 该研究通过使用四个包含不同领域数据的阿坎语语音语料库来评估七种基于变压器架构的阿坎语自动语音识别（ASR）模型的性能，其中包括文化相关的图像描述、非正式对话、圣经经文阅读和自发性金融对话。

**Result:** 通过比较单词错误率和字符错误率，研究表明这些模型只在其训练领域内表现出较好的性能，而在不匹配场景中的精度显著下降。这个研究还发现Whisper和Wav2Vec2架构在面对不熟悉输入时，会表现出不同的错误行为。

**Conclusion:** 根据该研究，选择低资源语言ASR架构时，应该考虑准确性与可解释性之间的权衡。这些发现强调了需要针对阿坎语和其他低资源语言开发特定领域的适应技术、自适应路由策略和多语言训练框架。

**Abstract:** Most existing automatic speech recognition (ASR) research evaluate models
using in-domain datasets. However, they seldom evaluate how they generalize
across diverse speech contexts. This study addresses this gap by benchmarking
seven Akan ASR models built on transformer architectures, such as Whisper and
Wav2Vec2, using four Akan speech corpora to determine their performance. These
datasets encompass various domains, including culturally relevant image
descriptions, informal conversations, biblical scripture readings, and
spontaneous financial dialogues. A comparison of the word error rate and
character error rate highlighted domain dependency, with models performing
optimally only within their training domains while showing marked accuracy
degradation in mismatched scenarios. This study also identified distinct error
behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned
Whisper Akan models led to more fluent but potentially misleading transcription
errors, Wav2Vec2 produced more obvious yet less interpretable outputs when
encountering unfamiliar inputs. This trade-off between readability and
transparency in ASR errors should be considered when selecting architectures
for low-resource language (LRL) applications. These findings highlight the need
for targeted domain adaptation techniques, adaptive routing strategies, and
multilingual training frameworks for Akan and other LRLs.

</details>


### [11] [A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages](https://arxiv.org/abs/2507.02428)
*Sumaya Ahmed Salihs,Isaac Wiafe,Jamal-Deen Abdulai,Elikem Doe Atsakpo,Gifty Ayoka,Richard Cave,Akon Obu Ekpezu,Catherine Holloway,Katrin Tomanek,Fiifi Baffoe Payin Winful*

Main category: cs.CL

> 本研究开发了一种方法来收集有语言障碍者的语音样本，以建立低资源语言的ASR模型，特别是在阿肯语上进行了实践。同时还创建了相关开放资源来促进这项工作的发展。初步结果显示，这项工作对于改善ASR技术的包容性具有潜力。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在解决有语言障碍的个体在使用ASR技术方面的不平等问题，特别是对于那些使用低资源语言的人群。通过开放和共享资源，促进更多人能够参与到ASR技术的开发和应用中来。

**Method:** 本研究提出了一种收集用于建立自动语音识别（ASR）模型的语音样本的方法，特别针对有语言障碍的人群，尤其是低资源语言。研究旨在通过开发数据收集和ASR模型构建的最佳实践指南和培训，普及ASR技术和数据收集。作为概念验证，本研究整理了第一份开放源代码的阿肯语（加纳广泛使用的土著语言）有语言障碍的语音数据集。

**Result:** 研究结果包括公开发布的数据集、最佳实践指南和开源工具，以供研究人员和实践者创建适合有语言障碍个体需求的包容性ASR技术。此外，本研究还展示了对开源ASR模型进行微调以更好地识别阿肯语中障碍语音的初步结果。

**Conclusion:** 本研究通过开发最佳实践和工具，使得科学研究和社会实践人员能够创建适合有语言障碍个体需求的ASR技术。研究展示了这些工具和实践的初步成果，以及其对未来ASR技术发展的潜在影响。

**Abstract:** This study presents an approach for collecting speech samples to build
Automatic Speech Recognition (ASR) models for impaired speech, particularly,
low-resource languages. It aims to democratize ASR technology and data
collection by developing a "cookbook" of best practices and training for
community-driven data collection and ASR model building. As a proof-of-concept,
this study curated the first open-source dataset of impaired speech in Akan: a
widely spoken indigenous language in Ghana. The study involved participants
from diverse backgrounds with speech impairments. The resulting dataset, along
with the cookbook and open-source tools, are publicly available to enable
researchers and practitioners to create inclusive ASR technologies tailored to
the unique needs of speech impaired individuals. In addition, this study
presents the initial results of fine-tuning open-source ASR models to better
recognize impaired speech in Akan.

</details>


### [12] [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)
*Sneha Deshmukh,Prathmesh Kamble*

Main category: cs.CL

> 本文构建了名为IndianBailJudgments-1200的数据集，包含1200份印度法庭判决书，旨在促进印度法律NLP研究，支持保释结果预测等任务。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于印度等区域在法律NLP领域的数据集稀缺，本研究旨在填补这一空白，为法律NLP任务提供支持。

**Method:** 本文介绍了一个名为IndianBailJudgments-1200的新数据集，该数据集包含1200份有关印度保释决定的法院判决，标注了20多个属性。这些注释通过使用经过提示工程的GPT-4管道生成，并验证了一致性。

**Result:** 成功构建了第一个专门针对印度保释司法的公开数据集，可用于保释结果预测、总结和公平性分析等多种任务。

**Conclusion:** 本研究构建的数据集对促进印度法律NLP研究具有重要意义，为进一步的研究和应用提供了宝贵的资源。

**Abstract:** Legal NLP remains underdeveloped in regions like India due to the scarcity of
structured datasets. We introduce IndianBailJudgments-1200, a new benchmark
dataset comprising 1200 Indian court judgments on bail decisions, annotated
across 20+ attributes including bail outcome, IPC sections, crime type, and
legal reasoning. Annotations were generated using a prompt-engineered GPT-4o
pipeline and verified for consistency. This resource supports a wide range of
legal NLP tasks such as outcome prediction, summarization, and fairness
analysis, and is the first publicly available dataset focused specifically on
Indian bail jurisprudence.

</details>


### [13] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Liwen Zhang,Litu Ou,Jialong Wu,Wenbiao Yin,Baixuan Li,Zhengwei Tao,Xinyu Wang,Weizhou Shen,Junkai Zhang,Dingchu Zhang,Xixi Wu,Yong Jiang,Ming Yan,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

> WebSailor是一种后训练方法，通过新的训练技巧和算法以增强开源语言模型在复杂信息搜寻任务上处理高度不确定性的能力，达成在这些任务上的表现与专有系统旗鼓相当。

<details>
  <summary>Details</summary>

**Motivation:** 由于专有代理系统在处理复杂信息搜索任务中展现出了超越开源模型的表现，因此研究旨在通过引入WebSailor来关闭开源模型与专有模型之间的能力差距。

**Method:** WebSailor采用了一种完整的后训练方法，利用结构化采样和信息模糊化生成高不确定性任务，通过RFT冷启动和高效代理强化学习训练算法（复制采样策略优化，DUPO）来实现这一目标。

**Result:** 采用WebSailor训练出来的代理在复杂的搜索任务上显著超越了所有开源代理的表现，达到了与专有代理相匹配的水平，缩小了它们之间的能力差距。

**Conclusion:** 研究证明，通过WebSailor这种特定的训练方法，能够赋予开源模型处理复杂信息环境中的高度不确定性的能力，从而提升其在复杂任务中的性能，可望达到接近专有系统的水平。

**Abstract:** Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all opensource agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


### [14] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

> 本文提出了一种将人类标签变化（HLV）整合进主动学习（AL）过程的概念框架，目的在于更好地反映现实世界标注的复杂性。

<details>
  <summary>Details</summary>

**Motivation:** 调研现有的主动学习（AL）和（H）标签变化（LV）社区对于信号（例如，HLV）和噪声（例如，标注错误）之间的区别的处理情况，强调了需要将观察到的LV分解为信号和噪声。

**Method:** 文中提出了将人类标签变化（HLV）纳入主动学习（AL）循环的概念框架，包括实例选择、标注者选择和标签表示。此外，还讨论了将大型语言模型（LLM）作为标注者的整合。

**Result:** 强调了现有AL和LV社区在处理HLV和噪声方面存在的缺陷，提出了一个新的理论框架来处理这些区别。

**Conclusion:** 工作旨在为HLV感知的主动学习建立一个概念基础，更好地反映现实世界标注的复杂性。

**Abstract:** Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>


### [15] [MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion](https://arxiv.org/abs/2507.02595)
*Xin Guan,PeiHsin Lin,Zekun Wu,Ze Wang,Ruibo Zhang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CL

> MPF是一种利用多视角生成来缓解大语言模型偏见的后训练对齐框架，能够自动构建偏见基准并减少偏见，无需复杂的提示工程或微调。

<details>
  <summary>Details</summary>

**Motivation:** 为了应对日益增长的轻松缓解偏见的需求，MPF框架被开发出来作为LLM的一种后训练校准框架。

**Method:** MPF框架利用多视角生成来揭示和校准LLM输出中的偏见，通过将其分解为可解释的视角组件，并通过采样和平衡响应来进行引导，响应的权重基于分解中获得的概率。

**Result:** 实验表明，MPF可以将LLM的情感分布与反事实基线（如绝对平等）和HR基线（偏向Top University）对齐，实现了小的KL散度，减少了校准误差，并且能够推广到未见过的问题上。

**Conclusion:** MPF框架提供了一种可扩展和可解释的方法，用于LLM的对齐和偏见缓解，并且能够与部署的LLM兼容。

**Abstract:** Multiperspective Fusion (MPF) is a novel posttraining alignment framework for
large language models (LLMs) developed in response to the growing need for easy
bias mitigation. Built on top of the SAGED pipeline, an automated system for
constructing bias benchmarks and extracting interpretable baseline
distributions, MPF leverages multiperspective generations to expose and align
biases in LLM outputs with nuanced, humanlike baselines. By decomposing
baseline, such as sentiment distributions from HR professionals, into
interpretable perspective components, MPF guides generation through sampling
and balancing of responses, weighted by the probabilities obtained in the
decomposition. Empirically, we demonstrate its ability to align LLM sentiment
distributions with both counterfactual baselines (absolute equality) and the HR
baseline (biased for Top Univeristy), resulting in small KL divergence,
reduction of calibration error and generalization to unseen questions. This
shows that MPF offers a scalable and interpretable method for alignment and
bias mitigation, compatible with deployed LLMs and requiring no extensive
prompt engineering or finetuning.

</details>


### [16] [Exploring Gender Bias Beyond Occupational Titles](https://arxiv.org/abs/2507.02679)
*Ahmed Sabir,Rajesh Sharama*

Main category: cs.CL

> 本文研究性别和情境偏差之间的关联，引入了新数据集和框架来评估和解释性别偏差，发现性别偏见存在超出职业刻板印象的证据。

<details>
  <summary>Details</summary>

**Motivation:** 研究性别和情境偏差之间的关联，特别关注动作动词、物体名词和职业上的差异。

**Method:** 我们引入了一个新数据集GenderLexicon和一个可以估计语境偏差及其相关性别偏差的框架。我们的模型可以通过评分来解释性别偏见，从而提高性别偏见的解释性。

**Result:** 研究确认了存在超越职业刻板印象的性别偏见。在包括日语数据在内的五个不同的数据集上验证了方法的有效性。

**Conclusion:** 研究证明了所提出的方法能够有效地评估和解释性别偏差，并且识别出一种超越传统职业刻板印象的性别偏见形式。

**Abstract:** In this work, we investigate the correlation between gender and contextual
biases, focusing on elements such as action verbs, object nouns, and
particularly on occupations. We introduce a novel dataset, GenderLexicon, and a
framework that can estimate contextual bias and its related gender bias. Our
model can interpret the bias with a score and thus improve the explainability
of gender bias. Also, our findings confirm the existence of gender biases
beyond occupational stereotypes. To validate our approach and demonstrate its
effectiveness, we conduct evaluations on five diverse datasets, including a
Japanese dataset.

</details>


### [17] [Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers](https://arxiv.org/abs/2507.02694)
*Zhijian Xu,Yilun Zhao,Manasi Patwardhan,Lovekesh Vig,Arman Cohan*

Main category: cs.CL

> 该研究开发了LimitGen，一个用于评估大型语言模型（LLMs）识别研究论文局限性能力的基准测试，包含合成数据集和人类编写的局限性数据集，并通过文献检索增强LLMs的能力，使其能够更准确地提供反馈。

<details>
  <summary>Details</summary>

**Motivation:** 随着科研论文数量的增加，同行评审的挑战也随之增大。尽管大型语言模型在各种科学任务上显示出潜力，但对于同行评审的帮助，特别是识别论文局限性的潜力仍未充分研究。

**Method:** 研究首先提出了一个关于科学研究局限性的全面分类，专注于AI领域。基于此分类，构建了LimitGen基准测试，包括两部分：LimitGen-Syn，用于研究局限性的合成数据集；LimitGen-Human，一组真实人类编写的局限性。通过文献检索增强大型语言模型，提升其识别局限性的能力。

**Result:** 该方法增强了大型语言模型系统生成研究论文局限性陈述的能力，使其能够提供更加具体和建设性的反馈。

**Conclusion:** LimitGen基准测试和文献检索的结合使用提升了大型语言模型在早期阶段为同行评审提供反馈和支持的能力。

**Abstract:** Peer review is fundamental to scientific research, but the growing volume of
publications has intensified the challenges of this expertise-intensive
process. While LLMs show promise in various scientific tasks, their potential
to assist with peer review, particularly in identifying paper limitations,
remains understudied. We first present a comprehensive taxonomy of limitation
types in scientific research, with a focus on AI. Guided by this taxonomy, for
studying limitations, we present LimitGen, the first comprehensive benchmark
for evaluating LLMs' capability to support early-stage feedback and complement
human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a
synthetic dataset carefully created through controlled perturbations of
high-quality papers, and LimitGen-Human, a collection of real human-written
limitations. To improve the ability of LLM systems to identify limitations, we
augment them with literature retrieval, which is essential for grounding
identifying limitations in prior scientific findings. Our approach enhances the
capabilities of LLM systems to generate limitations in research papers,
enabling them to provide more concrete and constructive feedback.

</details>


### [18] [Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens](https://arxiv.org/abs/2507.02744)
*Peter Viechnicki*

Main category: cs.CL

> This study measures the 'Just Producible Difference' (JPD) in auditory space for English speakers' front vowel production, finding values between 14 and 51 mels.

<details>
  <summary>Details</summary>

**Motivation:** The research aims to understand the accuracy of control mechanisms involved in human vowel production targeted at regions of auditory space by determining the JPD.

**Method:** The study uses a vowel mimicry paradigm to investigate the 'Just Producible Difference' (JPD) among English speakers during front vowel production.

**Result:** JPD is estimated to be between 14 and 51 mels in F1 X F2 space, suggesting the minimum distance needed to produce reliably distinct vowel sounds.

**Conclusion:** The findings clarify the possible structures of human vowel systems and support episodic theories of speech production by setting a limit for vowel closeness in formant space.

**Abstract:** A body of work over the past several decades has demonstrated that the
complex and coordinated articulatory movements of human vowel production are
governed (at least in part)by control mechanisms whose targets are regions of
auditory space. Within the target region control at the sub-phonemic level has
also been demonstrated. But the degree of accuracy of that control is unknown.
The current work investigates this question by asking how far apart must two
vowel stimuli lie in auditory space in order to yield reliably different
imitations? This distance is termed 'Just Producible Difference' (JPD). The
current study uses a vowel mimicry paradigm to derive the first measurement of
JPD among two sets of English speakers during front vowel production. JPD is
estimated at between 14 and 51 mels in F1 X F2 space. This finding has
implications for episodic theories of speech production. It also clarifies the
possible structures of human vowel systems, by setting a theoretical lower
bound for how close two vowel phonemes may be in a speaker's formant space, and
hence a psychophysical explanation of observed trends in number and patterns of
possible vowel phonemes.

</details>


### [19] [Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs](https://arxiv.org/abs/2507.02778)
*Ken Tsui*

Main category: cs.CL

> 研究发现LLMs在自我纠正方面存在系统性盲点，提出Self-Correction Bench框架并通过实验展示修正这一问题的初步结果和潜在改进方向。

<details>
  <summary>Details</summary>

**Motivation:** 研究LLMs的自我纠正能力，解决其在自我输出中纠正错误的系统性盲点问题。

**Method:** 引入Self-Correction Bench框架，通过在三个复杂度级别上注入受控错误，系统地测量LLMs的自我纠正盲点现象。

**Result:** 测试14个模型，平均盲点率为64.5%。通过在训练数据中加入错误纠正序列可以减轻盲点现象；简单加入“等待”指令可以使盲点现象降低89.3%。

**Conclusion:** 当前的LLMs在自我输出纠正方面存在一定局限性，通过改进数据训练方式可以提高其可靠性和可信度，指出了改进潜在方向。

**Abstract:** Although large language models (LLMs) have become transformative, they still
make mistakes and can explore unproductive reasoning paths. Self-correction is
an important capability for a trustworthy LLM, particularly an autoregressive
LLM. While LLMs can identify error in user input, they exhibit a systematic
'Self-Correction Blind Spot' - failing to correct identical error in their own
outputs. To systematically study this phenomenon, we introduce Self-Correction
Bench, a systematic framework to measure this phenomenon through controlled
error injection at three complexity levels. Testing 14 models, we find an
average 64.5% blind spot rate. We find multiple evidences that this limitation
relates to training data composition: human training demonstrations
predominantly show error-free responses rather than error-correction sequences,
unlike RL-trained models that learn error correction through outcome feedback.
Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting
that the capability exists but requires activation. Our work highlights a
critical limitation in current LLMs and offers potential avenues for improving
their reliability and trustworthiness.

</details>


### [20] [Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models](https://arxiv.org/abs/2507.02799)
*Riccardo Cantini,Nicola Gabriele,Alessio Orsino,Domenico Talia*

Main category: cs.CL

> 通过CLEAR-Bias基准测试评估显示，推理语言模型在面对偏见诱惑时的鲁棒性更低，尤其是依赖CoT提示的模型更容易受到攻击。

<details>
  <summary>Details</summary>

**Motivation:** 研究引入推理能力对模型公平性和鲁棒性的影响，比较了精细调优推理的模型与依赖推理提示（CoT）的模型在安全上的表现差异，以及不同的推理机制如何影响偏见掠取攻击的成功率。

**Method:** 通过CLEAR-Bias基准测试，系统地评估了最先进的推理语言模型（RLMs）在面对偏见诱惑时的鲁棒性。采用了语言模型作为裁判的自动化安全评分方法，并利用越狱技术来评估内置安全机制的强度。

**Result:** 研究表明，具有显式推理能力的模型，无论是通过CoT提示还是细调推理轨迹，都比没有这些机制的基本模型更容易受到偏见的触发。这表明推理可能无意中开启了强化刻板印象的新途径。依赖CoT提示的模型特别容易受到通过故事情节提示、虚构人设或奖励成形指令进行的情境重构攻击。

**Conclusion:** 结果挑战了推理能力能自动提升鲁棒性的假设，强调了需要开发更注重减少偏见的推理设计方法。

**Abstract:** Reasoning Language Models (RLMs) have gained traction for their ability to
perform complex, multi-step reasoning tasks through mechanisms such as
Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these
capabilities promise improved reliability, their impact on robustness to social
biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,
originally designed for Large Language Models (LLMs), to investigate the
adversarial robustness of RLMs to bias elicitation. We systematically evaluate
state-of-the-art RLMs across diverse sociocultural dimensions, using an
LLM-as-a-judge approach for automated safety scoring and leveraging jailbreak
techniques to assess the strength of built-in safety mechanisms. Our evaluation
addresses three key questions: (i) how the introduction of reasoning
capabilities affects model fairness and robustness; (ii) whether models
fine-tuned for reasoning exhibit greater safety than those relying on CoT
prompting at inference time; and (iii) how the success rate of jailbreak
attacks targeting bias elicitation varies with the reasoning mechanisms
employed. Our findings reveal a nuanced relationship between reasoning
capabilities and bias safety. Surprisingly, models with explicit reasoning,
whether via CoT prompting or fine-tuned reasoning traces, are generally more
vulnerable to bias elicitation than base models without such mechanisms,
suggesting reasoning may unintentionally open new pathways for stereotype
reinforcement. Reasoning-enabled models appear somewhat safer than those
relying on CoT prompting, which are particularly prone to contextual reframing
attacks through storytelling prompts, fictional personas, or reward-shaped
instructions. These results challenge the assumption that reasoning inherently
improves robustness and underscore the need for more bias-aware approaches to
reasoning design.

</details>


### [21] [Multimodal Mathematical Reasoning with Diverse Solving Perspective](https://arxiv.org/abs/2507.02804)
*Wenhao Shi,Zhiqiang Hu,Yi Bin,Yang Yang,See-Kiong Ng,Heng Tao Shen*

Main category: cs.CL

> 提出了MathV-DP数据集和Qwen-VL-DP模型，利用多种解题路径和强化学习方法，以提高数学推理中的准确性和多样性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的多模态大语言模型在数学推理中依赖一对一的图像-文本对和单解决方案监督，缺乏对有效推理视角多样性的考虑和内部思考。

**Method:** 引入了MathV-DP数据集，该数据集为每个图像-问题对添加了多种不同的解题路径，并基于Qwen-VL模型提出了Qwen-VL-DP模型。Qwen-VL-DP通过监督学习进行微调，并通过基于规则的强化学习方法——群组相对策略优化（GRPO），整合正确性区分和多样性意识奖励函数。

**Result:** 在MathVista迷你测试和Math-V基准测试中，Qwen-VL-DP在准确性和生成多样性方面显著优于之前的多模态大语言模型。

**Conclusion:** 实验表明，在数学推理中整合多样化的视角和反思对于提高准确性和创造性生成是非常重要的。

**Abstract:** Recent progress in large-scale reinforcement learning (RL) has notably
enhanced the reasoning capabilities of large language models (LLMs), especially
in mathematical domains. However, current multimodal LLMs (MLLMs) for
mathematical reasoning often rely on one-to-one image-text pairs and
single-solution supervision, overlooking the diversity of valid reasoning
perspectives and internal reflections. In this work, we introduce MathV-DP, a
novel dataset that captures multiple diverse solution trajectories for each
image-question pair, fostering richer reasoning supervision. We further propose
Qwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and
enhanced via group relative policy optimization (GRPO), a rule-based RL
approach that integrates correctness discrimination and diversity-aware reward
functions. Our method emphasizes learning from varied reasoning perspectives
and distinguishing between correct yet distinct solutions. Extensive
experiments on the MathVista's minitest and Math-V benchmarks demonstrate that
Qwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and
generative diversity, highlighting the importance of incorporating diverse
perspectives and reflective reasoning in multimodal mathematical reasoning.

</details>


### [22] [SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model](https://arxiv.org/abs/2507.02822)
*Wencheng Zhang,Shiqin Qiao,Lingjie Luo,Yinfeng Li,Chuanyang Zheng,Qian Xu,Meng Li,Yong Gui,Yijun He,Jianing Qiu,Jindong Hong,Jiankai Sun*

Main category: cs.CL

> 研究展示了动态地将医疗问题查询分配到适合的模式以优化准确性和成本效益的方法，提出并验证了SynapseRoute框架。

<details>
  <summary>Details</summary>

**Motivation:** 随着具有推理能力的大型语言模型（LLMs）的广泛应用，合理选择模型不仅要平衡性能，还需兼顾操作成本。研究表明，大约58%的医疗问题可以通过“非思考”模式准确回答，而不需要耗时的推理过程，这表明根据问题复杂性动态分配查询模式可以优化准度、成本效率和用户体验。

**Method:** 提出了SynapseRoute，这是一个基于机器学习的动态路由框架，能够智能地将输入查询分配到“思考”或“非思考”模式。

**Result:** 在几个医疗数据集上的实验结果表明，SynapseRoute不仅提高了整体准确度（0.8390对0.8272），还减少了推理时间36.8%和令牌使用39.66%。

**Conclusion:** 该工作进一步引入了准确度-推理-令牌（AIT）指数来全面评估准确度、延迟和令牌成本之间的权衡。

**Abstract:** With the widespread adoption of large language models (LLMs) in practical
applications, selecting an appropriate model requires balancing not only
performance but also operational cost. The emergence of reasoning-capable
models has further widened the cost gap between "thinking" (high reasoning) and
"non-thinking" (fast, low-cost) modes. In this work, we reveal that
approximately 58% of medical questions can be accurately answered by the
non-thinking mode alone, without requiring the high-cost reasoning process.
This highlights a clear dichotomy in problem complexity and suggests that
dynamically routing queries to the appropriate mode based on complexity could
optimize accuracy, cost-efficiency, and overall user experience. Based on this,
we further propose SynapseRoute, a machine learning-based dynamic routing
framework that intelligently assigns input queries to either thinking or
non-thinking modes. Experimental results on several medical datasets
demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.
0.8272) compared to the thinking mode alone but also reduces inference time by
36.8% and token consumption by 39.66%. Importantly, qualitative analysis
indicates that over-reasoning on simpler queries can lead to unnecessary delays
and even decreased accuracy, a pitfall avoided by our adaptive routing.
Finally, this work further introduces the Accuracy-Inference-Token (AIT) index
to comprehensively evaluate the trade-offs among accuracy, latency, and token
cost.

</details>


### [23] [Generalizing Verifiable Instruction Following](https://arxiv.org/abs/2507.02833)
*Valentina Pyatkin,Saumya Malik,Victoria Graf,Hamish Ivison,Shengyi Huang,Pradeep Dasigi,Nathan Lambert,Hannaneh Hajishirzi*

Main category: cs.CL

> 本文提出了一个新的评估基准IFBench以及RLVR方法以改进语言模型对于用户指定的精确输出约束的遵循能力，解决当前模型在小规模基准测试上的过拟合和泛化性能差的问题。

<details>
  <summary>Details</summary>

**Motivation:** 研究的主要动机在于解决现有模型在遵循用户指定的输出约束（如特定回答方式或关键词的重复次数）方面普遍存在的问题，以及这些模型在小规模基准测试上的过拟合现象。

**Method:** 研究者提出了一个新的基准测试IFBench，用于评估模型在处理58种新的、多样化的且具有挑战性的输出约束时的能力。他们通过强化学习与可验证奖励（RLVR）的方法改进了模型对精确指令的理解，并提供了额外的手动标注的训练约束条件和验证函数，以及相关的训练提示和代码。

**Result:** 研究结果表明，当前最强的语言模型在处理来自基准测试的验证约束上存在较强的过拟合现象，使得它们无法很好地泛化到未见过的约束上。研究所提出的方法能显著改善这一情况。

**Conclusion:** 研究显示，通过使用新的基准测试IFBench及所设计的约束验证模块与RLVR方法，模型在面对多样化的输出约束时，其精确指令跟随能力得到了显著改善。

**Abstract:** A crucial factor for successful human and AI interaction is the ability of
language models or chatbots to follow human instructions precisely. A common
feature of instructions are output constraints like ``only answer with yes or
no" or ``mention the word `abrakadabra' at least 3 times" that the user adds to
craft a more useful answer. Even today's strongest models struggle with
fulfilling such constraints. We find that most models strongly overfit on a
small set of verifiable constraints from the benchmarks that test these
abilities, a skill called precise instruction following, and are not able to
generalize well to unseen output constraints. We introduce a new benchmark,
IFBench, to evaluate precise instruction following generalization on 58 new,
diverse, and challenging verifiable out-of-domain constraints. In addition, we
perform an extensive analysis of how and on what data models can be trained to
improve precise instruction following generalization. Specifically, we
carefully design constraint verification modules and show that reinforcement
learning with verifiable rewards (RLVR) significantly improves instruction
following. In addition to IFBench, we release 29 additional new hand-annotated
training constraints and verification functions, RLVR training prompts, and
code.

</details>


### [24] [LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users](https://arxiv.org/abs/2507.02850)
*Almog Hilel,Idan Shenfeld,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

> 论文展示了攻击者如何利用用户反馈改变语言模型的知识和行为，以引入虚假信息或安全漏洞。

<details>
  <summary>Details</summary>

**Motivation:** 动机是揭示了语言模型在偏好调优过程中的新特征，并展示了一种新的针对使用用户反馈训练的语言模型的攻击方式。

**Method:** 描述了一种针对使用用户反馈训练的语言模型（LM）的漏洞利用方法。攻击者通过提供提示和对LM输出进行点赞或点差反馈来持续改变LM的知识和行为。

**Result:** 研究结果显示攻击者可以(1)插入模型之前不具有的事实知识，(2)以引入可利用的安全漏洞的方式改变代码生成模式，(3)注入虚假财经新闻。

**Conclusion:** 此发现确定了语言模型偏好调优的新特点，并且提出了针对用户反馈训练的语言模型的新攻击手段，即使用小范围的偏好数据能够对模型行为施加细粒度控制。

**Abstract:** We describe a vulnerability in language models (LMs) trained with user
feedback, whereby a single user can persistently alter LM knowledge and
behavior given only the ability to provide prompts and upvote / downvote
feedback on LM outputs. To implement the attack, the attacker prompts the LM to
stochastically output either a "poisoned" or benign response, then upvotes the
poisoned response or downvotes the benign one. When feedback signals are used
in a subsequent preference tuning behavior, LMs exhibit increased probability
of producing poisoned responses even in contexts without malicious prompts. We
show that this attack can be used to (1) insert factual knowledge the model did
not previously possess, (2) modify code generation patterns in ways that
introduce exploitable security flaws, and (3) inject fake financial news. Our
finding both identifies a new qualitative feature of language model preference
tuning (showing that it even highly restricted forms of preference data can be
used to exert fine-grained control over behavior), and a new attack mechanism
for LMs trained with user feedback (extending work on pretraining-time data
poisoning and deployment-time prompt injection).

</details>


### [25] [MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs](https://arxiv.org/abs/2507.02851)
*Purbesh Mitra,Sennur Ulukus*

Main category: cs.CL

> 本文提出了MOTIF，这是一种通过强化学习分批优化生成思考标记的方法，解决了大语言模型（LLM）在处理超出其上下文限制的推理时的问题，相比传统的GRPO方法，该方法在GSM8K数据集上进行训练后，对MATH500和AIME2024基准的测试准确率分别提高了3.8%和3.3%，并展示了样本效率的优势。

<details>
  <summary>Details</summary>

**Motivation:** 大语言模型在推理过程中需要使用更多的思考标记来生成更好的响应，但是受到上下文限制的制约，迫切需要一种方法来分批生成思考标记从而拓宽模型的推理能力。

**Method:** 作者提出了名为MOTIF的强化学习微调方法，该方法通过模块化思想策略分批生成思考标记，进而扩展了模型的上下文处理能力。

**Result:** 在GSM8K数据集上进行微调训练后，所提出的MOTIF方法在MATH500和AIME2024基准测试中分别实现了3.8%和3.3%的性能提升，并且该改进是在使用少于15%的样本量的情况下得到的。

**Conclusion:** MOTIF作为强化学习训练方法能够帮助大型语言模型处理超出传统上下文限制的推理任务，通过分批生成思考标记的方式提高了训练效果和样本效率，展示出了更好的性能提升。

**Abstract:** Recent advancements in the reasoning capabilities of large language models
(LLMs) show that employing group relative policy optimization (GRPO) algorithm
for reinforcement learning (RL) training allows the models to use more
thinking/reasoning tokens for generating better responses. However, LLMs can
generate only a finite amount of tokens while maintaining attention to the
previously generated tokens. This limit, also known as the context size of an
LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.
To think beyond the limit of context size, an LLM must employ a modular
thinking strategy to reason over multiple rounds. In this work, we propose
$\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL
training method for generating thinking tokens in multiple rounds, effectively
allowing the model to think with additional context size. We trained the
open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient
fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our
experiments show 3.8\% and 3.3\% improvements over vanilla GRPO based training
in the respective benchmarks. Furthermore, this improvement was achieved with
only 15\% of samples, thus demonstrating sample efficiency of MOTIF. Our code
and models are available at https://github.com/purbeshmitra/MOTIF and
https://huggingface.co/purbeshmitra/MOTIF, respectively.

</details>


### [26] [Answer Matching Outperforms Multiple Choice for Language Model Evaluation](https://arxiv.org/abs/2507.02856)
*Nikhil Chandak,Shashwat Goel,Ameya Prabhu,Moritz Hardt,Jonas Geiping*

Main category: cs.CL

> 研究提出了一种新的评估方式——答案匹配，能够有效评估语言模型的自由回答，避免了多选题评估中的捷径问题，并且显示出比多选题评估更好的一致性。

<details>
  <summary>Details</summary>

**Motivation:** 克服多选题评估中存在的捷径问题，寻找一种有效且可扩展的替代方案，改善语言模型的评估方式。

**Method:** 通过一种称为答案匹配的生成评估方法来评估模型：给候选模型提供没有选项的问题，让其生成自由形式的回答，然后使用现代语言模型和参考答案来判断回答是否匹配参考答案。

**Result:** 使用答案匹配方法进行评估，即便是使用较小的现代语言模型，也能够达成接近完美的评估一致性，这与多选题评估或没有参考答案的LLM评估的效果相比有了显著改进。

**Conclusion:** 作者建议在评估生态系统中从多选题评估向答案匹配评估迁移，并且指出这种评估方式的改进不仅是理论上的，还会改变多个模型的排名。

**Abstract:** Multiple choice benchmarks have long been the workhorse of language model
evaluation because grading multiple choice is objective and easy to automate.
However, we show multiple choice questions from popular benchmarks can often be
answered without even seeing the question. These shortcuts arise from a
fundamental limitation of discriminative evaluation not shared by evaluations
of the model's free-form, generative answers. Until recently, there appeared to
be no viable, scalable alternative to multiple choice--but, we show that this
has changed. We consider generative evaluation via what we call answer
matching: Give the candidate model the question without the options, have it
generate a free-form response, then use a modern language model with the
reference answer to determine if the response matches the reference. To compare
the validity of different evaluation strategies, we annotate MMLU-Pro and
GPQA-Diamond to obtain human grading data, and measure the agreement of each
evaluation approach. We find answer matching using recent models--even small
ones--achieves near-perfect agreement, in the range of inter-annotator
agreement. In contrast, both multiple choice evaluation and using
LLM-as-a-judge without reference answers aligns poorly with human grading.
Improving evaluations via answer matching is not merely a conceptual concern:
the rankings of several models change significantly when evaluating their
free-form responses with answer matching. In light of these findings, we
discuss how to move the evaluation ecosystem from multiple choice to answer
matching.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [27] [Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges](https://arxiv.org/abs/2507.02074)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.CV

> 本文综述了利用大型语言模型（LLMs）和视觉-语言模型（VLMs）进行视频碰撞检测的方法，指出了该领域的挑战和机遇。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于在智能交通系统中，从视频流中检测碰撞是一项关键任务，本文旨在综述利用大型语言模型（LLMs）进行视频数据碰撞检测的最新方法，以推动该领域的进一步发展。

**Method:** 本文通过构建融合策略的结构性分类，总结关键数据集，分析模型架构，比较性能基准来综述利用大型语言模型（LLMs）和视觉-语言模型（VLMs）进行视频碰撞检测的方法。

**Result:** 无具体结果描述，因为这是一篇综述性文章，主要提供了对当前研究的结构性分析和总结。

**Conclusion:** 本文为未来在视频理解与基础模型快速发展的交叉领域的研究提供了基础，指出了正在面临的挑战和机遇。

**Abstract:** Crash detection from video feeds is a critical problem in intelligent
transportation systems. Recent developments in large language models (LLMs) and
vision-language models (VLMs) have transformed how we process, reason about,
and summarize multimodal information. This paper surveys recent methods
leveraging LLMs for crash detection from video data. We present a structured
taxonomy of fusion strategies, summarize key datasets, analyze model
architectures, compare performance benchmarks, and discuss ongoing challenges
and opportunities. Our review provides a foundation for future research in this
fast-growing intersection of video understanding and foundation models.

</details>


### [28] [Underwater Monocular Metric Depth Estimation: Real-World Benchmarks and Synthetic Fine-Tuning](https://arxiv.org/abs/2507.02148)
*Zijie Cai,Christopher Metzler*

Main category: cs.CV

> 本文对水下场景的单目度量深度估计进行了详细的评估和可视化，通过微调模型展示了领域自适应的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 单目深度估计在水下环境中的可靠性受限，原因包括光衰减和散射、色彩失真、浑浊和缺乏高质量的度量真实数据。为了克服这些挑战，本研究对水下环境中的单目度量深度估计进行了详细的评估和可视化。

**Method:** 本文提出了一种在真实世界水下数据集（如FLSea和SQUID）上对零样本和微调的单目度量深度估计模型进行综合基准测试的方法。作者生成了Hypersim数据集的合成水下变体，并使用基于物理的水下图像形成模型对Depth Anything V2（ViT-S主干编码器）进行了微调。

**Result:** 研究结果显示，大规模模型在空中设置中有效，但在水下性能较差，因为存在显著的领域变化。然而，经过微调的模型在所有基准测试中都表现出色，并优于仅在干净的空中Hypersim数据集上训练的基线模型。

**Conclusion:** 本研究强调了领域自适应和尺度感知监督对于在具有挑战性的水下环境中实现鲁棒和通用的度量深度预测的重要性，并为未来的研究提供了指导。

**Abstract:** Monocular depth estimation has recently advanced to provide not only relative
but also metric depth predictions. However, its reliability in underwater
environments remains limited due to light attenuation and scattering, color
distortion, turbidity, and the lack of high-quality metric ground-truth data.
In this paper, we present a comprehensive benchmark of zero-shot and fine-tuned
monocular metric depth estimation models on real-world underwater datasets with
metric depth annotations, such as FLSea and SQUID. We evaluate a diverse set of
state-of-the-art models across a range of underwater conditions with different
ranges. Our results show that large-scale models trained on terrestrial (real
or synthetic) data, while effective in in-air settings, perform poorly
underwater due to significant domain shifts. To address this, we fine-tune
Depth Anything V2 with a ViT-S backbone encoder on a synthetic underwater
variant of the Hypersim dataset, which we generated using a physically based
underwater image formation model. We demonstrate our fine-tuned model
consistently improves performance across all benchmarks and outperforms
baselines trained only on the clean in-air Hypersim dataset. Our study provides
a detailed evaluation and visualization for monocular metric depth estimation
in underwater scenes, highlighting the importance of domain adaptation and
scale-aware supervision for achieving robust and generalizable metric depth
predictions in challenging underwater environments for future research.

</details>


### [29] [ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.02200)
*Xiao Wang,Jingtao Jiang,Qiang Chen,Lan Chen,Lin Zhu,Yaowei Wang,Yonghong Tian,Jin Tang*

Main category: cs.CV

> ESTR-CoT：一种基于链式推理的事件流场景文本识别框架，提供有效的文本识别、良好的解释性和逻辑推理能力，在挑战性场景下表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法存在解释性不足和上下文逻辑推理能力较弱的问题，因此提出了一个新的框架来解决这些问题。

**Method:** 提出了基于链式推理的事件流场景文本识别框架ESTR-CoT，首先使用视觉编码器EVA-CLIP（ViT-G/14）将输入事件流转换为tokens，使用Llama tokenizer对给定的生成提示进行编码。通过Q-former将视觉token与预训练的大语言模型Vicuna-7B对齐，并同时输出答案和链式推理（CoT）过程。该框架可以通过监督微调进行端到端优化。此外，还提出了一个大规模的CoT数据集，通过三阶段处理（即生成、改进和专家验证）来训练框架。

**Result:** 在三个事件流STR基准数据集（即EventSTR、WordArt*、IC15*）上的广泛实验验证了所提出框架的有效性和解释性。

**Conclusion:** ESTR-CoT框架在解决低光照、快速运动等极具挑战场景下的文本识别任务上表现出色，提供了解释性和逻辑推理能力，同时提出的数据集为后续基于推理的大模型的发展提供了坚实的数据基础。

**Abstract:** Event stream based scene text recognition is a newly arising research topic
in recent years which performs better than the widely used RGB cameras in
extremely challenging scenarios, especially the low illumination, fast motion.
Existing works either adopt end-to-end encoder-decoder framework or large
language models for enhanced recognition, however, they are still limited by
the challenges of insufficient interpretability and weak contextual logical
reasoning. In this work, we propose a novel chain-of-thought reasoning based
event stream scene text recognition framework, termed ESTR-CoT. Specifically,
we first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input
event stream into tokens and utilize a Llama tokenizer to encode the given
generation prompt. A Q-former is used to align the vision token to the
pre-trained large language model Vicuna-7B and output both the answer and
chain-of-thought (CoT) reasoning process simultaneously. Our framework can be
optimized using supervised fine-tuning in an end-to-end manner. In addition, we
also propose a large-scale CoT dataset to train our framework via a three stage
processing (i.e., generation, polish, and expert verification). This dataset
provides a solid data foundation for the development of subsequent
reasoning-based large models. Extensive experiments on three event stream STR
benchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the
effectiveness and interpretability of our proposed framework. The source code
and pre-trained models will be released on
https://github.com/Event-AHU/ESTR-CoT.

</details>


### [30] [Team RAS in 9th ABAW Competition: Multimodal Compound Expression Recognition Approach](https://arxiv.org/abs/2507.02205)
*Elena Ryumina,Maxim Markitantov,Alexandr Axyonov,Dmitry Ryumin,Mikhail Dolgushin,Alexey Karpov*

Main category: cs.CV

> 研究提出了一种零样本多模态方法，用于复合表情识别，通过结合六种不同模态和零样本组件，该方法在多个公开数据集上表现出与监督学习相当的性能。

<details>
  <summary>Details</summary>

**Motivation:** 复合表情识别是情感计算的一个重要子领域，旨在检测由多种基础情绪组合而成的复杂情感状态。本方法的主要动机在于通过零样本学习技术，无需任务特定的训练数据，即可准确识别复合表情。

**Method:** 本研究提出了一种新颖的零样本多模态方法来处理复合表情识别（CER），结合了六种异构模态：静态和动态面部表情、场景与标签匹配、场景上下文、音频和文本。该方法使用了零样本组件，包括基于CLIP的标签匹配和Qwen-VL进行语义场景理解。同时还引入了多头概率融合（MHPF）模块，用于动态加权各个模态的预测结果，随后是复合情绪转换模块，利用成对概率聚合（PPA）和成对特征相似性聚合（PFSA）方法生成可解释的复合情绪输出。

**Result:** 该方法在AffWild2、AFEW和C-EXPR-DB三个数据集上的零样本测试中分别取得了46.95%、49.02%和34.85%的F1分数，测试结果与针对目标数据进行监督学习训练的方法相当。这表明该方法在无需领域适应的情况下，能有效地捕捉到复合情绪。

**Conclusion:** 本研究展示了一种有效的零样本多模态方法，无需特定训练数据即可识别复合情感状态，并且无需领域适应的情况下也能取得良好的性能，这为复合表情识别领域提供了一种新的研究方向。代码也是公开可用的。

**Abstract:** Compound Expression Recognition (CER), a subfield of affective computing,
aims to detect complex emotional states formed by combinations of basic
emotions. In this work, we present a novel zero-shot multimodal approach for
CER that combines six heterogeneous modalities into a single pipeline: static
and dynamic facial expressions, scene and label matching, scene context, audio,
and text. Unlike previous approaches relying on task-specific training data,
our approach uses zero-shot components, including Contrastive Language-Image
Pretraining (CLIP)-based label matching and Qwen-VL for semantic scene
understanding. We further introduce a Multi-Head Probability Fusion (MHPF)
module that dynamically weights modality-specific predictions, followed by a
Compound Expressions (CE) transformation module that uses Pair-Wise Probability
Aggregation (PPA) and Pair-Wise Feature Similarity Aggregation (PFSA) methods
to produce interpretable compound emotion outputs. Evaluated under multi-corpus
training, the proposed approach shows F1 scores of 46.95% on AffWild2, 49.02%
on Acted Facial Expressions in The Wild (AFEW), and 34.85% on C-EXPR-DB via
zero-shot testing, which is comparable to the results of supervised approaches
trained on target data. This demonstrates the effectiveness of the proposed
approach for capturing CE without domain adaptation. The source code is
publicly available.

</details>


### [31] [SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers](https://arxiv.org/abs/2507.02212)
*Takuro Kawada,Shunsuke Kitada,Sota Nemoto,Hitoshi Iyatomi*

Main category: cs.CV

> 研究提出了SciGA-145k数据集，包含大约145,000篇科学论文和1.14百万张图，旨在支持科学图解(GA)的选择、推荐和自动生成的研究。还定义了GA设计支持的两个任务，并提出了一种新的推荐评估指标CAR。

<details>
  <summary>Details</summary>

**Motivation:** 为了克服设计有效科学图解所需的专业可视化技能障碍，并探索其在增强科学研究传播中的潜力。

**Method:** 构建了一个大型数据集SciGA-145k，并定义了图解选择和推荐的两个任务。同时也设计了恰当的基线模型和新的推荐评估指标CAR。

**Result:** 提出了合理基线模型以及新的评估指标CAR，为科学图解的设计提供初步支持。

**Conclusion:** SciGA-145k数据集为推动视觉化科学交流以及科学领域AI的发展奠定了基础。

**Abstract:** Graphical Abstracts (GAs) play a crucial role in visually conveying the key
findings of scientific papers. While recent research has increasingly
incorporated visual materials such as Figure 1 as de facto GAs, their potential
to enhance scientific communication remains largely unexplored. Moreover,
designing effective GAs requires advanced visualization skills, creating a
barrier to their widespread adoption. To tackle these challenges, we introduce
SciGA-145k, a large-scale dataset comprising approximately 145,000 scientific
papers and 1.14 million figures, explicitly designed for supporting GA
selection and recommendation as well as facilitating research in automated GA
generation. As a preliminary step toward GA design support, we define two
tasks: 1) Intra-GA recommendation, which identifies figures within a given
paper that are well-suited to serve as GAs, and 2) Inter-GA recommendation,
which retrieves GAs from other papers to inspire the creation of new GAs. We
provide reasonable baseline models for these tasks. Furthermore, we propose
Confidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation
metric that offers a fine-grained analysis of model behavior. CAR addresses
limitations in traditional ranking-based metrics by considering cases where
multiple figures within a paper, beyond the explicitly labeled GA, may also
serve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a
foundation for advancing visual scientific communication while contributing to
the development of AI for Science.

</details>


### [32] [Understanding Trade offs When Conditioning Synthetic Data](https://arxiv.org/abs/2507.02217)
*Brandon Trabucco,Qasim Wani,Benjamin Pikus,Vasu Sharma*

Main category: cs.CV

> 研究比较了两种条件控制策略对合成数据质量的影响，结果表明合成数据可以显著提高物体检测器的性能，特别是在布局控制与训练数据分布相匹配的情况下。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于解决工业视觉系统中从少量图像中学习强大对象检测器的难题，并探索扩散模型生成高质量图像的能力及其不同条件控制方案对合成数据质量的影响。

**Method:** 本研究通过对比两种条件控制策略（基于提示的和基于布局的）对合成数据质量的影响，来检验合成数据在仅有少量图像的情况下提高对象检测器鲁棒性的效果。

**Result:** 研究表明，在条件控制提示较窄的情况下，基于提示的条件控制生成的合成数据质量更高；随着多样性的增长，基于布局的条件控制成为更优选择。在布局暗示匹配完整训练分布时，合成数据可以将平均精度提高34%，在某些情况下可提高177%。

**Conclusion:** 结论为：基于合成数据中适当控制方案的应用可以显著提升物体检测器的鲁棒性和性能，特别适合于训练数据收集困难的情境。

**Abstract:** Learning robust object detectors from only a handful of images is a critical
challenge in industrial vision systems, where collecting high quality training
data can take months. Synthetic data has emerged as a key solution for data
efficient visual inspection and pick and place robotics. Current pipelines rely
on 3D engines such as Blender or Unreal, which offer fine control but still
require weeks to render a small dataset, and the resulting images often suffer
from a large gap between simulation and reality. Diffusion models promise a
step change because they can generate high quality images in minutes, yet
precise control, especially in low data regimes, remains difficult. Although
many adapters now extend diffusion beyond plain text prompts, the effect of
different conditioning schemes on synthetic data quality is poorly understood.
We study eighty diverse visual concepts drawn from four standard object
detection benchmarks and compare two conditioning strategies: prompt based and
layout based. When the set of conditioning cues is narrow, prompt conditioning
yields higher quality synthetic data; as diversity grows, layout conditioning
becomes superior. When layout cues match the full training distribution,
synthetic data raises mean average precision by an average of thirty four
percent and by as much as one hundred seventy seven percent compared with using
real data alone.

</details>


### [33] [High-Fidelity Differential-information Driven Binary Vision Transformer](https://arxiv.org/abs/2507.02222)
*Tian Gao,Zhiyuan Zhang,Kaijie Yin,Xu-Cheng Zhong,Hui Kong*

Main category: cs.CV

> DIDB-ViT 提出了一种新的二值视觉变换器，通过设计信息丰富的注意力模块和改进的RPReLU激活函数来减少信息损失并扩展模型的表示能力，实验结果显示其在多种视觉变换器架构中表现优于现有的网络量化方法。

<details>
  <summary>Details</summary>

**Motivation:** 文章动机在于解决现有二值视觉变换器方法中存在的性能退化问题或对全精度模块的高度依赖，旨在创建一个计算效率高且信息丰富的二值视觉变换器。

**Method:** Structure

**Result:** {
  "tldr": "DIDB-ViT 提出了一种新的二值视觉变换器，通过设计信息丰富的注意力模块和改进的RPReLU激活函数来减少信息损失并扩展模型的表示能力，实验结果显示其在多种视觉变换器架构中表现优于现有的网络量化方法。",
  "motivation": "文章动机在于解决现有二值视觉变换器方法中存在的性能退化问题或对全精度模块的高度依赖，旨在创建一个计算效率高且信息丰富的二值视觉变换器。",
  "method": "方法包括设计一种包含差异信息的信息丰富注意力模块、使用离散哈默尔波分解来保持二值Q和K张量之间相似性的保真度以及引入改进的RPReLU激活函数以改进激活函数分布。",
  "result": "实验结果表明，DIDB-ViT在多种视觉变换器架构中显著优于现有网络量化方法，特别是在图像分类和分割任务中。",
  "conclusion": "结论是DIDB-ViT通过其创新设计和改进激活函数在二值网络中实现了更高的信息保留和模型表现，是该领域的一个重要进展。",
  "paper_content": "The binarization of vision transformers (ViTs) offers a promising approach to addressing the trade-off between high computational/storage demands and the constraints of edge-device deployment. However, existing binary ViT methods often suffer from severe performance degradation or rely heavily on full-precision modules. To address these issues, we propose DIDB-ViT, a novel binary ViT that is highly informative while maintaining the original ViT architecture and computational efficiency. Specifically, we design an informative attention module incorporating differential information to mitigate information loss caused by binarization and enhance high-frequency retention. To preserve the fidelity of the similarity calculations between binary Q and K tensors, we apply frequency decomposition using the discrete Haar wavelet and integrate similarities across different frequencies. Additionally, we introduce an improved RPReLU activation function to restructure the activation distribution, expanding the model's representational capacity. Experimental results demonstrate that our DIDB-ViT significantly outperforms state-of-the-art network quantization methods in multiple ViT architectures, achieving superior image classification and segmentation performance. "
}

**Conclusion:** 结论是DIDB-ViT通过其创新设计和改进激活函数在二值网络中实现了更高的信息保留和模型表现，是该领域的一个重要进展。

**Abstract:** The binarization of vision transformers (ViTs) offers a promising approach to
addressing the trade-off between high computational/storage demands and the
constraints of edge-device deployment. However, existing binary ViT methods
often suffer from severe performance degradation or rely heavily on
full-precision modules. To address these issues, we propose DIDB-ViT, a novel
binary ViT that is highly informative while maintaining the original ViT
architecture and computational efficiency. Specifically, we design an
informative attention module incorporating differential information to mitigate
information loss caused by binarization and enhance high-frequency retention.
To preserve the fidelity of the similarity calculations between binary Q and K
tensors, we apply frequency decomposition using the discrete Haar wavelet and
integrate similarities across different frequencies. Additionally, we introduce
an improved RPReLU activation function to restructure the activation
distribution, expanding the model's representational capacity. Experimental
results demonstrate that our DIDB-ViT significantly outperforms
state-of-the-art network quantization methods in multiple ViT architectures,
achieving superior image classification and segmentation performance.

</details>


### [34] [FMOcc: TPV-Driven Flow Matching for 3D Occupancy Prediction with Selective State Space Model](https://arxiv.org/abs/2507.02250)
*Jiangxia Chen,Tongyuan Huang,Ke Song*

Main category: cs.CV

> 提出FMOcc网络，通过流匹配选择状态空间模型解决了依赖多帧图像进行3D场景占用预测的计算复杂性和数据需求问题，提高了远处场景的预测精度和模型效率。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法依赖多帧数据提高3D占用预测的准确性，需要更多的计算资源和数据，而本文旨在解决这些问题，提出一种适用于少量帧的3D占用预测方法。

**Method:** 设计了基于流匹配模型的特征精炼模块FMSSM，以及TPV SSM层和平面选择性SSM（PS3M），减少空气体素对非空气体素的影响，以提高模型效率和远处场景的预测能力，并设计了掩码训练法（MT）增强模型鲁棒性。

**Result:** 实验结果表明，FMOcc在Occ3D-nuScenes和OpenOcc数据集上超越了现有方法，使用两帧输入在Occ3D-nuScenes验证上获得了43.1%的RayIoU和39.8%mIoU，而在OpenOcc上获得了42.6%的RayIoU，推理内存为5.4G，推理时间为330ms。

**Conclusion:** 提出的FMOcc网络提高了少帧3D占用预测的精度和效率，同时不需要额外的数据和极大的计算资源。

**Abstract:** 3D semantic occupancy prediction plays a pivotal role in autonomous driving.
However, inherent limitations of fewframe images and redundancy in 3D space
compromise prediction accuracy for occluded and distant scenes. Existing
methods enhance performance by fusing historical frame data, which need
additional data and significant computational resources. To address these
issues, this paper propose FMOcc, a Tri-perspective View (TPV) refinement
occupancy network with flow matching selective state space model for few-frame
3D occupancy prediction. Firstly, to generate missing features, we designed a
feature refinement module based on a flow matching model, which is called Flow
Matching SSM module (FMSSM). Furthermore, by designing the TPV SSM layer and
Plane Selective SSM (PS3M), we selectively filter TPV features to reduce the
impact of air voxels on non-air voxels, thereby enhancing the overall
efficiency of the model and prediction capability for distant scenes. Finally,
we design the Mask Training (MT) method to enhance the robustness of FMOcc and
address the issue of sensor data loss. Experimental results on the
Occ3D-nuScenes and OpenOcc datasets show that our FMOcc outperforms existing
state-of-theart methods. Our FMOcc with two frame input achieves notable scores
of 43.1% RayIoU and 39.8% mIoU on Occ3D-nuScenes validation, 42.6% RayIoU on
OpenOcc with 5.4 G inference memory and 330ms inference time.

</details>


### [35] [SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement](https://arxiv.org/abs/2507.02252)
*Zeyu Lei,Hongyuan Yu,Jinlin Wu,Zhen Chen*

Main category: cs.CV

> SurgVisAgent 是一个基于多模态大型语言模型的智能手术视觉代理，可以动态识别内窥镜图像中的失真类别和严重程度，并执行多种图像增强任务，如低光增强、过曝校正、运动模糊消除和烟雾去除，显示出了优于传统单任务模型的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 尽管医疗图像增强算法取得了显著进展，但它们通常仅针对特定场景中的单个任务设计，限制了其在复杂现实情况下的有效性。为了克服这一局限性，开发了SurgVisAgent。

**Method:** Structure

**Result:** {
  "tldr": "SurgVisAgent 是一个基于多模态大型语言模型的智能手术视觉代理，可以动态识别内窥镜图像中的失真类别和严重程度，并执行多种图像增强任务，如低光增强、过曝校正、运动模糊消除和烟雾去除，显示出了优于传统单任务模型的潜力。",
  "motivation": "尽管医疗图像增强算法取得了显著进展，但它们通常仅针对特定场景中的单个任务设计，限制了其在复杂现实情况下的有效性。为了克服这一局限性，开发了SurgVisAgent。",
  "method": "通过设计提供领域特定知识的prior模型，并采用上下文中的少样学习和链式推理(CoT)，SurgVisAgent能够适应广泛失真类型和严重程度的定制图像增强。",
  "result": "大量实验表明SurgVisAgent超越了传统单任务模型，在一个综合基准上模拟了真实的手术失真情况。",
  "conclusion": "SurgVisAgent展示出作为手术辅助统一解决方案的潜力。
}


**Conclusion:** SurgVisAgent展示出作为手术辅助统一解决方案的潜力。

**Abstract:** Precise surgical interventions are vital to patient safety, and advanced
enhancement algorithms have been developed to assist surgeons in
decision-making. Despite significant progress, these algorithms are typically
designed for single tasks in specific scenarios, limiting their effectiveness
in complex real-world situations. To address this limitation, we propose
SurgVisAgent, an end-to-end intelligent surgical vision agent built on
multimodal large language models (MLLMs). SurgVisAgent dynamically identifies
distortion categories and severity levels in endoscopic images, enabling it to
perform a variety of enhancement tasks such as low-light enhancement,
overexposure correction, motion blur elimination, and smoke removal.
Specifically, to achieve superior surgical scenario understanding, we design a
prior model that provides domain-specific knowledge. Additionally, through
in-context few-shot learning and chain-of-thought (CoT) reasoning, SurgVisAgent
delivers customized image enhancements tailored to a wide range of distortion
types and severity levels, thereby addressing the diverse requirements of
surgeons. Furthermore, we construct a comprehensive benchmark simulating
real-world surgical distortions, on which extensive experiments demonstrate
that SurgVisAgent surpasses traditional single-task models, highlighting its
potential as a unified solution for surgical assistance.

</details>


### [36] [Multi-Label Classification Framework for Hurricane Damage Assessment](https://arxiv.org/abs/2507.02265)
*Zhangding Liu,Neda Mohammadi,John E. Taylor*

Main category: cs.CV

> 该研究提出了一种基于ResNet和特定类别注意力机制的多标签分类框架，用于评估飓风后的建筑物损伤，通过Rescuenet数据集从飓风迈克尔事件中验证，达到了90.23%的平均精度，有效提升灾害响应速度和精准度。

<details>
  <summary>Details</summary>

**Motivation:** 传统单标签分类方法难以准确捕捉飓风造成的复杂损伤类型，因此研究采用了多标签分类框架来改善损伤评估的准确性和效率。

**Method:** 框架结合了基于ResNet的特征提取模块和类别特定的注意力机制来识别单张图像中的多种损伤类型。

**Result:** 在使用Rescuenet数据集实验的基础上，提出的方法相较现有基线方法达到了90.23%的平均精确度，表现出色。

**Conclusion:** 该多标签框架增强了飓风后损伤评估的质量，能够支持更针对性和高效的灾害响应，并对未来灾害减缓策略提供帮助。

**Abstract:** Hurricanes cause widespread destruction, resulting in diverse damage types
and severities that require timely and accurate assessment for effective
disaster response. While traditional single-label classification methods fall
short of capturing the complexity of post-hurricane damage, this study
introduces a novel multi-label classification framework for assessing damage
using aerial imagery. The proposed approach integrates a feature extraction
module based on ResNet and a class-specific attention mechanism to identify
multiple damage types within a single image. Using the Rescuenet dataset from
Hurricane Michael, the proposed method achieves a mean average precision of
90.23%, outperforming existing baseline methods. This framework enhances
post-hurricane damage assessment, enabling more targeted and efficient disaster
response and contributing to future strategies for disaster mitigation and
resilience. This paper has been accepted at the ASCE International Conference
on Computing in Civil Engineering (i3CE 2025), and the camera-ready version
will appear in the official conference proceedings.

</details>


### [37] [Cross-domain Hyperspectral Image Classification based on Bi-directional Domain Adaptation](https://arxiv.org/abs/2507.02268)
*Yuxiang Zhang,Wei Li,Wen Jia,Mengmeng Zhang,Ran Tao,Shunlin Liang*

Main category: cs.CV

> 本论文提出了一种新的BiDA框架，通过独立适应空间学习和跨域相关性挖掘，解决了高光谱图像在跨域条件下的分类问题，实验表明其性能优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 利用高光谱遥感技术可提取精细地表覆盖分类，由于训练和测试数据通常来自不同的区域或时间，同一类别的光谱在不同场景下有显著变化，本研究旨在解决这一跨域分类问题。

**Method:** 本论文提出了一种双向领域适应（BiDA）框架，用于跨域高光谱图像分类，该框架通过在独立适应空间中提取领域不变特征和领域特定信息，提升了目标场景下的适应性和可分离性。BiDA包括一个三分支变压器架构（源分支、目标分支和耦合分支），采用语义分词器作为骨架结构。其中，源分支和目标分支分别独立学习源域和目标域的适应空间；耦合分支中开发了一种耦合多头交叉注意力（CMCA）机制，用于特征交互和跨域相关性挖掘。此外，设计了一种双向蒸馏损失以利用域间相关性指导适应空间学习。为进一步优化在噪声条件下的泛化特征提取，提出了一种自适应强化策略（ARS）。

**Result:** 实验结果表明，该论文提出的BiDA框架在跨时域/场景的机载和卫星数据集上的分类性能显著优于一些最先进的领域适应方法。在跨时域树种分类任务中，BiDA优于最先进的方法，性能提高了约3%~5%。

**Conclusion:** 该论文提出了一种新的双向领域适应框架（BiDA），通过开发三分支变换架构、耦合多头交叉注意力机制以及自适应强化策略等，提升了高光谱图像在跨域条件下的分类性能。其在跨时域和跨场景的实验中均展现出了优越的性能。

**Abstract:** Utilizing hyperspectral remote sensing technology enables the extraction of
fine-grained land cover classes. Typically, satellite or airborne images used
for training and testing are acquired from different regions or times, where
the same class has significant spectral shifts in different scenes. In this
paper, we propose a Bi-directional Domain Adaptation (BiDA) framework for
cross-domain hyperspectral image (HSI) classification, which focuses on
extracting both domain-invariant features and domain-specific information in
the independent adaptive space, thereby enhancing the adaptability and
separability to the target scene. In the proposed BiDA, a triple-branch
transformer architecture (the source branch, target branch, and coupled branch)
with semantic tokenizer is designed as the backbone. Specifically, the source
branch and target branch independently learn the adaptive space of source and
target domains, a Coupled Multi-head Cross-attention (CMCA) mechanism is
developed in coupled branch for feature interaction and inter-domain
correlation mining. Furthermore, a bi-directional distillation loss is designed
to guide adaptive space learning using inter-domain correlation. Finally, we
propose an Adaptive Reinforcement Strategy (ARS) to encourage the model to
focus on specific generalized feature extraction within both source and target
scenes in noise condition. Experimental results on cross-temporal/scene
airborne and satellite datasets demonstrate that the proposed BiDA performs
significantly better than some state-of-the-art domain adaptation approaches.
In the cross-temporal tree species classification task, the proposed BiDA is
more than 3\%$\sim$5\% higher than the most advanced method. The codes will be
available from the website:
https://github.com/YuxiangZhang-BIT/IEEE_TCSVT_BiDA.

</details>


### [38] [MAC-Lookup: Multi-Axis Conditional Lookup Model for Underwater Image Enhancement](https://arxiv.org/abs/2507.02270)
*Fanghai Yi,Zehong Zheng,Zexiao Liang,Yihang Dong,Xiyang Fang,Wangyu Wu,Xuhang Chen*

Main category: cs.CV

> 研究介绍了MAC-Lookup模型，该模型通过两种技术（CLTCC和MAAE）来提高水下图像的颜色准确性和清晰度，实验结果表明其性能优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 传统的基于先验的方法和基于像素的方法在增强水下图像时效果不佳，而深度学习由于缺乏高质量的数据集而受到限制。该研究旨在解决水下图像的可见性和颜色问题。

**Method:** 引入了Multi-Axis Conditional Lookup (MAC-Lookup)模型，其中包括Conditional 3D Lookup Table Color Correction (CLTCC)进行初步的颜色和质量校正，以及Multi-Axis Adaptive Enhancement (MAAE)进行细节细化。

**Result:** 广泛的实验表明，MAC-Lookup模型在增强水下图像方面表现出色，尤其是在恢复细节和颜色方面优于现有的方法。

**Conclusion:** MAC-Lookup模型能够有效增强水下图像，同时避免过增强和饱和问题，表现优于现有方法。

**Abstract:** Enhancing underwater images is crucial for exploration. These images face
visibility and color issues due to light changes, water turbidity, and bubbles.
Traditional prior-based methods and pixel-based methods often fail, while deep
learning lacks sufficient high-quality datasets. We introduce the Multi-Axis
Conditional Lookup (MAC-Lookup) model, which enhances visual quality by
improving color accuracy, sharpness, and contrast. It includes Conditional 3D
Lookup Table Color Correction (CLTCC) for preliminary color and quality
correction and Multi-Axis Adaptive Enhancement (MAAE) for detail refinement.
This model prevents over-enhancement and saturation while handling underwater
challenges. Extensive experiments show that MAC-Lookup excels in enhancing
underwater images by restoring details and colors better than existing methods.
The code is https://github.com/onlycatdoraemon/MAC-Lookup.

</details>


### [39] [Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation](https://arxiv.org/abs/2507.02271)
*Feizhen Huang,Yu Wu,Yutian Lin,Bo Du*

Main category: cs.CV

> 论文提出了一种自蒸馏方法，改进了视频到音频生成模型在对象部分可见情况下的性能，并提升了其在VGGSound数据集上的表现。

<details>
  <summary>Details</summary>

**Motivation:** 现有的视频到音频生成方法忽略了电影语言这一关键的艺术表达组件，导致在对象仅部分可见的场景下性能下降。

**Method:** 论文采用了一种自蒸馏的方法，通过模拟电影语言的变化，使学生模型学习训练对中的视频特征与相同音视频对应关系之间的对齐。

**Result:** 该论文提出了一种简单自蒸馏方法，旨在将视频到音频生成模型扩展到电影语言场景中。通过模拟电影语言的变化，学生模型能够更好地捕捉声音与部分视觉信息之间的关联，从而在对象部分可见的情况下显著提升了模型的表现。此外，该方法在大规模视频到音频数据集VGGSound上的性能也得到了增强。

**Conclusion:** 提出的自蒸馏方法在部分目标可见的场景下，所有评估指标都取得显著改进，并且在大规模V2A数据集VGGSound上的性能也得以提升。

**Abstract:** Video-to-Audio (V2A) Generation achieves significant progress and plays a
crucial role in film and video post-production. However, current methods
overlook the cinematic language, a critical component of artistic expression in
filmmaking. As a result, their performance deteriorates in scenarios where
Foley targets are only partially visible. To address this challenge, we propose
a simple self-distillation approach to extend V2A models to cinematic language
scenarios. By simulating the cinematic language variations, the student model
learns to align the video features of training pairs with the same audio-visual
correspondences, enabling it to effectively capture the associations between
sounds and partial visual information. Our method not only achieves impressive
improvements under partial visibility across all evaluation metrics, but also
enhances performance on the large-scale V2A dataset, VGGSound.

</details>


### [40] [LaCo: Efficient Layer-wise Compression of Visual Tokens for Multimodal Large Language Models](https://arxiv.org/abs/2507.02279)
*Juntao Liu,Liqiang Niu,Wenchao Chen,Jie Zhou,Fandong Meng*

Main category: cs.CV

> LaCo (Layer-wise Visual Token Compression) 是一种可以提高Multimodal Large Language Models (MLLMs) 效率的新框架，能够在视觉编码器的中间层实现有效的token压缩。

<details>
  <summary>Details</summary>

**Motivation:** 当前的视觉token压缩方法主要用于多模态大语言模型(MLLMs)的后编码模块，限制了其在效率提升方面的潜力。为了克服这一局限性，提出了LaCo方法。

**Method:** LaCo (Layer-wise Visual Token Compression) 是一种新的框架，旨在在视觉编码器的中间层实现有效的token压缩。该框架包括两个核心组件：1) 层级像素混洗机制，通过空间到通道变换系统地合并相邻token；2) 残差学习架构，配有非参数捷径，以在压缩过程中保存关键视觉信息。

**Result:** 广泛实验表明，当压缩视觉编码器中间层的token时，LaCo优于所有现有方法，显示了更强的有效性。此外，与外部压缩相比，该方法提高了超过20%的训练效率和超过15%的推理吞吐量，同时保持了强大的性能。

**Conclusion:** LaCo不仅优于现有方法，而且在提高训练效率和推理吞吐量方面表现出色，同时保持了强大的性能。

**Abstract:** Existing visual token compression methods for Multimodal Large Language
Models (MLLMs) predominantly operate as post-encoder modules, limiting their
potential for efficiency gains. To address this limitation, we propose LaCo
(Layer-wise Visual Token Compression), a novel framework that enables effective
token compression within the intermediate layers of the vision encoder. LaCo
introduces two core components: 1) a layer-wise pixel-shuffle mechanism that
systematically merges adjacent tokens through space-to-channel transformations,
and 2) a residual learning architecture with non-parametric shortcuts that
preserves critical visual information during compression. Extensive experiments
indicate that our LaCo outperforms all existing methods when compressing tokens
in the intermediate layers of the vision encoder, demonstrating superior
effectiveness. In addition, compared to external compression, our method
improves training efficiency beyond 20% and inference throughput over 15% while
maintaining strong performance.

</details>


### [41] [Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization](https://arxiv.org/abs/2507.02288)
*De Cheng,Zhipeng Xu,Xinyang Jiang,Dongsheng Li,Nannan Wang,Xinbo Gao*

Main category: cs.CV

> 本文提出了一个通过文本特征引导视觉提示调优的新框架，并引入了WERA机制以增强方法的跨领域泛化能力，该方法在多个数据集上超过了现有最优方法。

<details>
  <summary>Details</summary>

**Motivation:** 近年来，预训练视觉基础模型（VFMs）在领域泛化（DG）中表现出色，但设计能够解耦跨域不变特征的提示仍然具有挑战性。

**Method:** Structure

**Result:** {
  "tldr": "本文提出了一个通过文本特征引导视觉提示调优的新框架，并引入了WERA机制以增强方法的跨领域泛化能力，该方法在多个数据集上超过了现有最优方法。", 
  "motivation": "近年来，预训练视觉基础模型（VFMs）在领域泛化（DG）中表现出色，但设计能够解耦跨域不变特征的提示仍然具有挑战性。", 
  "method": "方法包括使用大语言模型自动解耦文本提示，然后基于解耦的文本特征学习领域不变的视觉表示。还引入了WERA机制，通过增加抽象提示和样式化的图像增强来优化这一过程。", 
  "result": "实验结果显示，本文方法在PACS、VLCS、OfficeHome、DomainNet和TerraInc等多个DG数据集上超过了现有最优方法。", 
  "conclusion": "本文证明了文本引导的视觉提示优化在DG中的有效性，并展示了WERA为增强模型的泛化能力提供了一个有效途径。"}
}

**Conclusion:** 本文证明了文本引导的视觉提示优化在DG中的有效性，并展示了WERA为增强模型的泛化能力提供了一个有效途径。

**Abstract:** Domain Generalization (DG) seeks to develop a versatile model capable of
performing effectively on unseen target domains. Notably, recent advances in
pre-trained Visual Foundation Models (VFMs), such as CLIP, have demonstrated
considerable potential in enhancing the generalization capabilities of deep
learning models. Despite the increasing attention toward VFM-based domain
prompt tuning within DG, the effective design of prompts capable of
disentangling invariant features across diverse domains remains a critical
challenge. In this paper, we propose addressing this challenge by leveraging
the controllable and flexible language prompt of the VFM. Noting that the text
modality of VFMs is naturally easier to disentangle, we introduce a novel
framework for text feature-guided visual prompt tuning. This framework first
automatically disentangles the text prompt using a large language model (LLM)
and then learns domain-invariant visual representation guided by the
disentangled text feature. However, relying solely on language to guide visual
feature disentanglement has limitations, as visual features can sometimes be
too complex or nuanced to be fully captured by descriptive text. To address
this, we introduce Worst Explicit Representation Alignment (WERA), which
extends text-guided visual prompts by incorporating an additional set of
abstract prompts. These prompts enhance source domain diversity through
stylized image augmentations, while alignment constraints ensure that visual
representations remain consistent across both the original and augmented
distributions. Experiments conducted on major DG datasets, including PACS,
VLCS, OfficeHome, DomainNet, and TerraInc, demonstrate that our proposed method
outperforms state-of-the-art DG methods.

</details>


### [42] [ViRefSAM: Visual Reference-Guided Segment Anything Model for Remote Sensing Segmentation](https://arxiv.org/abs/2507.02294)
*Hanbo Bi,Yulong Xu,Ya Li,Yongqiang Mao,Boyuan Tong,Chongyang Li,Chunbo Lang,Wenhui Diao,Hongqi Wang,Yingchao Feng,Xian Sun*

Main category: cs.CV

> 为了解决SAM在遥感图像上的手动提示构建和领域适应性问题，研究人员提出使用少量标注参考图像来自动分割特定类别的对象。实验表明，该方法在多个少样本分割基准中表现优异。

<details>
  <summary>Details</summary>

**Motivation:** 尽管SAM在一般分割任务上显示出强大的泛化能力，但将其应用于遥感图像时仍面临两个重大挑战：1）为每张图像手动构建精确的提示（如点或框）工作量大且效率低，特别是在存在密集小对象或空间分布碎片化的遥感环境中。2）SAM缺乏领域适应性，主要是在自然图像上预训练的，难以捕捉遥感特定的语义和空间特性，尤其是当分割新的或未见过的类别时。

**Method:** 为了解决SAM在遥感图像中的手动提示构建问题和领域适应性问题，ViRefSAM框架通过利用少量包含特定类别对象的标注参考图像来引导SAM。ViRefSAM具体包括两个关键组件：1）视觉上下文提示编码器，从参考图像中提取类别特定的语义线索，并通过与目标图像的上下文交互生成对象感知提示；2）动态目标对齐适配器，集成在SAM的图像编码器中，通过注入类别特定语义到目标图像特征中来减少领域差距，使SAM能够动态地专注于任务相关区域。

**Result:** 在iSAID-5$^i$、LoveDA-2$^i$和COCO-20$^i$三个少样本分割基准上的广泛实验表明，ViRefSAM通过利用少量参考图像能够实现未见过类别的准确自动分割，并在各种数据集上超过现有的少样本分割方法。

**Conclusion:** 实验显示，ViRefSAM通过利用少量参考图像能够准确且自动地分割未见过的类别，且在各种数据集上一致优于现有的少样本分割方法。

**Abstract:** The Segment Anything Model (SAM), with its prompt-driven paradigm, exhibits
strong generalization in generic segmentation tasks. However, applying SAM to
remote sensing (RS) images still faces two major challenges. First, manually
constructing precise prompts for each image (e.g., points or boxes) is
labor-intensive and inefficient, especially in RS scenarios with dense small
objects or spatially fragmented distributions. Second, SAM lacks domain
adaptability, as it is pre-trained primarily on natural images and struggles to
capture RS-specific semantics and spatial characteristics, especially when
segmenting novel or unseen classes. To address these issues, inspired by
few-shot learning, we propose ViRefSAM, a novel framework that guides SAM
utilizing only a few annotated reference images that contain class-specific
objects. Without requiring manual prompts, ViRefSAM enables automatic
segmentation of class-consistent objects across RS images. Specifically,
ViRefSAM introduces two key components while keeping SAM's original
architecture intact: (1) a Visual Contextual Prompt Encoder that extracts
class-specific semantic clues from reference images and generates object-aware
prompts via contextual interaction with target images; and (2) a Dynamic Target
Alignment Adapter, integrated into SAM's image encoder, which mitigates the
domain gap by injecting class-specific semantics into target image features,
enabling SAM to dynamically focus on task-relevant regions. Extensive
experiments on three few-shot segmentation benchmarks, including iSAID-5$^i$,
LoveDA-2$^i$, and COCO-20$^i$, demonstrate that ViRefSAM enables accurate and
automatic segmentation of unseen classes by leveraging only a few reference
images and consistently outperforms existing few-shot segmentation methods
across diverse datasets.

</details>


### [43] [DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation](https://arxiv.org/abs/2507.02299)
*Yunhan Yang,Shuo Chen,Yukun Huang,Xiaoyang Wu,Yuan-Chen Guo,Edmund Y. Lam,Hengshuang Zhao,Tong He,Xihui Liu*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Recent advancements in leveraging pre-trained 2D diffusion models achieve the
generation of high-quality novel views from a single in-the-wild image.
However, existing works face challenges in producing controllable novel views
due to the lack of information from multiple views. In this paper, we present
DreamComposer++, a flexible and scalable framework designed to improve current
view-aware diffusion models by incorporating multi-view conditions.
Specifically, DreamComposer++ utilizes a view-aware 3D lifting module to
extract 3D representations of an object from various views. These
representations are then aggregated and rendered into the latent features of
target view through the multi-view feature fusion module. Finally, the obtained
features of target view are integrated into pre-trained image or video
diffusion models for novel view synthesis. Experimental results demonstrate
that DreamComposer++ seamlessly integrates with cutting-edge view-aware
diffusion models and enhances their abilities to generate controllable novel
views from multi-view conditions. This advancement facilitates controllable 3D
object reconstruction and enables a wide range of applications.

</details>


### [44] [Flow-CDNet: A Novel Network for Detecting Both Slow and Fast Changes in Bitemporal Images](https://arxiv.org/abs/2507.02307)
*Haoxuan Li,Chenxu Wei,Haodong Wang,Xiaomeng Hu,Boyuan An,Lingyan Ran,Baosen Zhang,Jin Jin,Omirzhan Taukebayev,Amirkhan Temirbayev,Junrui Liu,Xiuwei Zhang*

Main category: cs.CV

> The paper presents Flow-CDNet, a change detection network capable of detecting both slow and fast changes in bitemporal images, featuring an optical flow branch and a binary change detection branch.

<details>
  <summary>Details</summary>

**Motivation:** To develop a change detection network that can simultaneously detect slow and fast changes in bitemporal images, particularly recognizing the importance of weak changes that can indicate major hazards in real-life scenarios such as slopes, dams, and tailings ponds.

**Method:** Flow-CDNet includes two branches: one for extracting displacement changes at multiple scales using a pyramid structure, and another that combines a ResNet-based network with optical flow outputs for fast change detection. A self-built dataset (Flow-Change), a hybrid loss function, and a new evaluation metric (FEPE) were designed for supervision and evaluation.

**Result:** Experiments demonstrated that Flow-CDNet outperformed existing methods on the Flow-Change dataset, and ablation studies confirmed that the two branches can enhance each other's performance.

**Conclusion:** The Flow-CDNet, designed to detect both slow and fast changes in bitemporal images, achieves better performance than current change detection methods, with the potential to be a valuable tool in monitoring hazardous scenarios that require early detection of small changes.

**Abstract:** Change detection typically involves identifying regions with changes between
bitemporal images taken at the same location. Besides significant changes, slow
changes in bitemporal images are also important in real-life scenarios. For
instance, weak changes often serve as precursors to major hazards in scenarios
like slopes, dams, and tailings ponds. Therefore, designing a change detection
network that simultaneously detects slow and fast changes presents a novel
challenge. In this paper, to address this challenge, we propose a change
detection network named Flow-CDNet, consisting of two branches: optical flow
branch and binary change detection branch. The first branch utilizes a pyramid
structure to extract displacement changes at multiple scales. The second one
combines a ResNet-based network with the optical flow branch's output to
generate fast change outputs. Subsequently, to supervise and evaluate this new
change detection framework, a self-built change detection dataset Flow-Change,
a loss function combining binary tversky loss and L2 norm loss, along with a
new evaluation metric called FEPE are designed. Quantitative experiments
conducted on Flow-Change dataset demonstrated that our approach outperforms the
existing methods. Furthermore, ablation experiments verified that the two
branches can promote each other to enhance the detection performance.

</details>


### [45] [LMPNet for Weakly-supervised Keypoint Discovery](https://arxiv.org/abs/2507.02308)
*Pei Guo,Ryan Farrell*

Main category: cs.CV

> 本文提出了一种高效的新模型LMPNet来实现弱监督下的语义物体关键点检测。通过使用漏极最大池化层和注意力遮罩机制，模型实现了与监督方法相近的预测准确性。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在研究仅通过类别标签进行弱监督的物体语义关键点发现任务。

**Method:** 本文提出了LMPNet模型，通过在卷积层后加入新颖的计算高效漏极最大池化（LMP）层，将有区分性的训练中间层滤波器转化为关键点检测器。LMP层鼓励学习“非重复性局部模式”，这些模式与物体关键点对齐。此外，通过可视化指导，提出了一种简单的选择策略，并且应用注意力遮罩使网络将注意力分配到整个物体而不是最区分的区域。最后关键点预测是通过一个可学习的聚类层将关键点提案分组完成的。

**Result:** 实验表明LMPNet能够自动发现对物体姿态鲁棒的语义关键点并获得了与监督姿势估计模型相当的强预测准确性。

**Conclusion:** LMPNet模型具有高度可解释性，因为它直接操纵网络滤波器来检测预定义的概念，并取得了良好的关键点预测效果。

**Abstract:** In this work, we explore the task of semantic object keypoint discovery
weakly-supervised by only category labels. This is achieved by transforming
discriminatively-trained intermediate layer filters into keypoint detectors. We
begin by identifying three preferred characteristics of keypoint detectors: (i)
spatially sparse activations, (ii) consistency and (iii) diversity. Instead of
relying on hand-crafted loss terms, a novel computationally-efficient leaky max
pooling (LMP) layer is proposed to explicitly encourage final conv-layer
filters to learn "non-repeatable local patterns" that are well aligned with
object keypoints. Informed by visualizations, a simple yet effective selection
strategy is proposed to ensure consistent filter activations and attention
mask-out is then applied to force the network to distribute its attention to
the whole object instead of just the most discriminative region. For the final
keypoint prediction, a learnable clustering layer is proposed to group keypoint
proposals into keypoint predictions. The final model, named LMPNet, is highly
interpretable in that it directly manipulates network filters to detect
predefined concepts. Our experiments show that LMPNet can (i) automatically
discover semantic keypoints that are robust to object pose and (ii) achieves
strong prediction accuracy comparable to a supervised pose estimation model.

</details>


### [46] [Perception Activator: An intuitive and portable framework for brain cognitive exploration](https://arxiv.org/abs/2507.02311)
*Le Xu,Qi Zhang,Qixian Zhang,Hongyun Zhang,Duoqian Miao,Cairong Zhao*

Main category: cs.CV

> 本文提出了一种利用fMRI表示改进视觉目标检测和分割的方法，并证明了fMRI信号提供的多对象语义信息和空间信息对于此类任务的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 为了更好地理解大脑的视觉感知模式以及当前解码模型如何处理语义对象，本文作者提出了一种新的实验方法。

**Method:** 本文提出了一种实验框架，该框架使用功能磁共振成像(fMRI)表示作为干预条件，并通过交叉注意力将这些表示注入多尺度图像特征中。研究人员通过对比有无fMRI信息对目标检测和实例分割任务的影响，来研究其下游性能和中间特征的变化。

**Result:** 研究结果表明，加入fMRI信号可以提高下游检测和分割任务的准确性，这证实了fMRI信号包含了丰富的多个对象的语义线索和粗略的空间定位信息。

**Conclusion:** 通过将fMRI表示与图像特征相融合，可以改善目标检测和实例分割模型的性能。这些研究进一步揭示了fMRI信号在视觉理解中的作用及其尚未被充分利用的潜力。

**Abstract:** Recent advances in brain-vision decoding have driven significant progress,
reconstructing with high fidelity perceived visual stimuli from neural
activity, e.g., functional magnetic resonance imaging (fMRI), in the human
visual cortex. Most existing methods decode the brain signal using a two-level
strategy, i.e., pixel-level and semantic-level. However, these methods rely
heavily on low-level pixel alignment yet lack sufficient and fine-grained
semantic alignment, resulting in obvious reconstruction distortions of multiple
semantic objects. To better understand the brain's visual perception patterns
and how current decoding models process semantic objects, we have developed an
experimental framework that uses fMRI representations as intervention
conditions. By injecting these representations into multi-scale image features
via cross-attention, we compare both downstream performance and intermediate
feature changes on object detection and instance segmentation tasks with and
without fMRI information. Our results demonstrate that incorporating fMRI
signals enhances the accuracy of downstream detection and segmentation,
confirming that fMRI contains rich multi-object semantic cues and coarse
spatial localization information-elements that current models have yet to fully
exploit or integrate.

</details>


### [47] [MAGIC: Mask-Guided Diffusion Inpainting with Multi-Level Perturbations and Context-Aware Alignment for Few-Shot Anomaly Generation](https://arxiv.org/abs/2507.02314)
*JaeHyuck Choi,MinJun Kim,JeHyeong Hong*

Main category: cs.CV

> MAGIC is proposed to generate high-quality anomalies while preserving the background and strictly adhering to anomaly masks, surpassing existing methods on the MVTec-AD dataset.

<details>
  <summary>Details</summary>

**Motivation:** To address the need for realistic, diverse, and precisely located anomaly generation from limited data for industrial quality control.

**Method:** Mask-guided inpainting with multi-level perturbations and context-aware alignment (MAGIC) to generate anomalies while preserving the background and precisely aligning with anomaly masks.

**Result:** Outperforms previous methods on MVTec-AD dataset under a consistent evaluation protocol.

**Conclusion:** MAGIC effectively resolves issues with background corruption, mask misalignment, and diversity loss in anomaly generation.

**Abstract:** Few-shot anomaly generation is emerging as a practical solution for
augmenting the scarce anomaly data in industrial quality control settings. An
ideal generator would meet three demands at once, namely (i) keep the normal
background intact, (ii) inpaint anomalous regions to tightly overlap with the
corresponding anomaly masks, and (iii) generate anomalous regions in a
semantically valid location, while still producing realistic, diverse
appearances from only a handful of real examples. Existing diffusion-based
methods usually satisfy at most two of these requirements: global anomaly
generators corrupt the background, whereas mask-guided ones often falter when
the mask is imprecise or misplaced. We propose MAGIC--Mask-guided inpainting
with multi-level perturbations and Context-aware alignment--to resolve all
three issues. At its core, MAGIC fine-tunes a Stable Diffusion inpainting
backbone that preserves normal regions and ensures strict adherence of the
synthesized anomaly to the supplied mask, directly addressing background
corruption and misalignment. To offset the diversity loss that fine-tuning can
cause, MAGIC adds two complementary perturbation strategies: (i) Gaussian
prompt-level perturbation applied during fine-tuning and inference that
broadens the global appearance of anomalies while avoiding low-fidelity textual
appearances, and (ii) mask-guided spatial noise injection that enriches local
texture variations. Additionally, the context-aware mask alignment module forms
semantic correspondences and relocates masks so that every anomaly remains
plausibly contained within the host object, eliminating out-of-boundary
artifacts. Under a consistent identical evaluation protocol on the MVTec-AD
dataset, MAGIC outperforms previous state-of-the-arts in downstream anomaly
tasks.

</details>


### [48] [Are Synthetic Videos Useful? A Benchmark for Retrieval-Centric Evaluation of Synthetic Videos](https://arxiv.org/abs/2507.02316)
*Zecheng Zhao,Selena Song,Tong Chen,Zhi Chen,Shazia Sadiq,Yadan Luo*

Main category: cs.CV

> The authors present SynTVA, a benchmark and dataset designed to assess synthetic video quality for text-to-video retrieval, along with annotations in four semantic dimensions and an Auto-Evaluator for estimating alignment quality.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this work is the realization that current evaluation metrics for text-to-video synthesis primarily focus on visual quality and temporal consistency, neglecting the performance of synthetic videos in downstream tasks such as text-to-video retrieval. The authors seek to address this gap by introducing a new dataset and benchmark that provides insights into the utility of synthetic videos for such tasks.

**Method:** In this work, the authors develop SynTVA, a new dataset and benchmark for evaluating the utility of synthetic videos in text-to-video retrieval tasks. They generate synthetic videos using state-of-the-art T2V models based on 800 diverse user queries and annotate these along four key semantic dimensions: Object & Scene, Action, Attribute, and Prompt Fidelity. An Auto-Evaluator is also created to estimate alignment quality from existing metrics.

**Result:** The SynTVA dataset and its evaluation framework correlate general video quality with alignment scores, highlighting the predictive power of these metrics for downstream TVR performance. Furthermore, it is shown that SynTVA can be used for dataset augmentation to improve TVR outcomes.

**Conclusion:** SynTVA is concluded to be a valuable resource for benchmarking and dataset augmentation in the field of text-to-video synthesis, particularly for improving text-to-video retrieval performance. The work suggests a pathway for scaling up the evaluation process through the use of the Auto-Evaluator.

**Abstract:** Text-to-video (T2V) synthesis has advanced rapidly, yet current evaluation
metrics primarily capture visual quality and temporal consistency, offering
limited insight into how synthetic videos perform in downstream tasks such as
text-to-video retrieval (TVR). In this work, we introduce SynTVA, a new dataset
and benchmark designed to evaluate the utility of synthetic videos for building
retrieval models. Based on 800 diverse user queries derived from MSRVTT
training split, we generate synthetic videos using state-of-the-art T2V models
and annotate each video-text pair along four key semantic alignment dimensions:
Object \& Scene, Action, Attribute, and Prompt Fidelity. Our evaluation
framework correlates general video quality assessment (VQA) metrics with these
alignment scores, and examines their predictive power for downstream TVR
performance. To explore pathways of scaling up, we further develop an
Auto-Evaluator to estimate alignment quality from existing metrics. Beyond
benchmarking, our results show that SynTVA is a valuable asset for dataset
augmentation, enabling the selection of high-utility synthetic samples that
measurably improve TVR outcomes. Project page and dataset can be found at
https://jasoncodemaker.github.io/SynTVA/.

</details>


### [49] [Heeding the Inner Voice: Aligning ControlNet Training via Intermediate Features Feedback](https://arxiv.org/abs/2507.02321)
*Nina Konovalova,Maxim Nikolaev,Andrey Kuznetsov,Aibek Alanov*

Main category: cs.CV

> 我们提出了InnerControl，一种新的训练策略，通过在整个扩散过程中强制执行空间一致性，改善生成图像的空间控制精度和质量。

<details>
  <summary>Details</summary>

**Motivation:** 尽管在文本到图像的扩散模型方面取得了显著进展，但精确控制生成输出的空间布局仍然具有挑战性。现有的方法，如ControlNet和ControlNet++，虽然有所改进，但忽视了中间生成阶段，限制了它们的效果。我们提出了一种新的方法来解决这个问题。

**Method:** 我们提出了一种名为InnerControl的训练策略，其通过在整个扩散过程中强制执行空间一致性来改善图像生成的控制。具体来说，InnerControl训练轻量级的卷积探测器从中间UNet特征中在每个去噪步骤中重建输入控制信号（如边缘、深度）。这些探测器可以从高度噪点的潜在空间中高效地提取信号，从而提供用于训练的伪地面真实控制信号。通过在整个扩散过程中最小化预测条件和目标条件之间的差异，我们的对齐损失提高了控制精度和生成质量。

**Result:** 实验表明，我们的方法不仅显著提高了控制精度和生成质量，还能够与其他现有技术（如ControlNet++）结合使用，以实现最先进的性能。

**Conclusion:** InnerControl在不同的控制方法（如边缘、深度）上均达到了最先进的性能，展示了其有效性和广泛应用性。

**Abstract:** Despite significant progress in text-to-image diffusion models, achieving
precise spatial control over generated outputs remains challenging. ControlNet
addresses this by introducing an auxiliary conditioning module, while
ControlNet++ further refines alignment through a cycle consistency loss applied
only to the final denoising steps. However, this approach neglects intermediate
generation stages, limiting its effectiveness. We propose InnerControl, a
training strategy that enforces spatial consistency across all diffusion steps.
Our method trains lightweight convolutional probes to reconstruct input control
signals (e.g., edges, depth) from intermediate UNet features at every denoising
step. These probes efficiently extract signals even from highly noisy latents,
enabling pseudo ground truth controls for training. By minimizing the
discrepancy between predicted and target conditions throughout the entire
diffusion process, our alignment loss improves both control fidelity and
generation quality. Combined with established techniques like ControlNet++,
InnerControl achieves state-of-the-art performance across diverse conditioning
methods (e.g., edges, depth).

</details>


### [50] [Neural Network-based Study for Rice Leaf Disease Recognition and Classification: A Comparative Analysis Between Feature-based Model and Direct Imaging Model](https://arxiv.org/abs/2507.02322)
*Farida Siddiqi Prity,Mirza Raquib,Saydul Akbar Murad,Md. Jubayar Alam Rafi,Md. Khairul Bashar Bhuiyan,Anupam Kumar Bairagi*

Main category: cs.CV

> 研究提出了使用特征分析检测模型(FADM)和直接图像为中心的检测模型(DICDM)进行水稻叶片疾病的分类，并通过实验表明FADM表现更优，有助于改善水稻作物的健康状况。

<details>
  <summary>Details</summary>

**Motivation:** 水稻叶片疾病的早期检测对于管理疾病和提高产量至关重要，但缺乏对FADM和DICDM的系统对比。

**Method:** 实验采用了不同的特征提取算法（FEA）、降维算法（DRA）和特征选择算法（FSA），并与不采用FEA的DICDM模型进行对比。

**Result:** 实验结果表明使用FADM的性能最佳。

**Conclusion:** 提出的FADM在水稻叶片疾病检测方面具有较大潜力，可以改善作物健康，减少产量损失，提高水稻种植的整体生产力和可持续性。

**Abstract:** Rice leaf diseases significantly reduce productivity and cause economic
losses, highlighting the need for early detection to enable effective
management and improve yields. This study proposes Artificial Neural Network
(ANN)-based image-processing techniques for timely classification and
recognition of rice diseases. Despite the prevailing approach of directly
inputting images of rice leaves into ANNs, there is a noticeable absence of
thorough comparative analysis between the Feature Analysis Detection Model
(FADM) and Direct Image-Centric Detection Model (DICDM), specifically when it
comes to evaluating the effectiveness of Feature Extraction Algorithms (FEAs).
Hence, this research presents initial experiments on the Feature Analysis
Detection Model, utilizing various image Feature Extraction Algorithms,
Dimensionality Reduction Algorithms (DRAs), Feature Selection Algorithms
(FSAs), and Extreme Learning Machine (ELM). The experiments are carried out on
datasets encompassing bacterial leaf blight, brown spot, leaf blast, leaf
scald, Sheath blight rot, and healthy leaf, utilizing 10-fold Cross-Validation
method. A Direct Image-Centric Detection Model is established without the
utilization of any FEA, and the evaluation of classification performance relies
on different metrics. Ultimately, an exhaustive contrast is performed between
the achievements of the Feature Analysis Detection Model and Direct
Image-Centric Detection Model in classifying rice leaf diseases. The results
reveal that the highest performance is attained using the Feature Analysis
Detection Model. The adoption of the proposed Feature Analysis Detection Model
for detecting rice leaf diseases holds excellent potential for improving crop
health, minimizing yield losses, and enhancing overall productivity and
sustainability of rice farming.

</details>


### [51] [Two-Steps Neural Networks for an Automated Cerebrovascular Landmark Detection](https://arxiv.org/abs/2507.02349)
*Rafic Nader,Vincent L'Allinec,Romain Bourcier,Florent Autrusseau*

Main category: cs.CV

> Developed and validated a two-step neural network for automated detection of bifurcations in the Circle of Willis, achieving superior results in aneurysm diagnosis.

<details>
  <summary>Details</summary>

**Motivation:** To accurately detect critical landmarks of intracranial aneurysms (ICA) in the Circle of Willis (CoW) for prompt and efficient diagnosis.

**Method:** We introduce a two-step neural network approach using an object detection network to identify ROIs and a modified U-Net with deep supervision for accurate bifurcation localization.

**Result:** Our method achieves the highest level of performance in bifurcation detection, outperforming existing approaches.

**Conclusion:** The proposed two-step neural network approach effectively addresses challenges such as anatomical variability and landmark proximity, improving the accuracy and reliability of bifurcation detection.

**Abstract:** Intracranial aneurysms (ICA) commonly occur in specific segments of the
Circle of Willis (CoW), primarily, onto thirteen major arterial bifurcations.
An accurate detection of these critical landmarks is necessary for a prompt and
efficient diagnosis. We introduce a fully automated landmark detection approach
for CoW bifurcations using a two-step neural networks process. Initially, an
object detection network identifies regions of interest (ROIs) proximal to the
landmark locations. Subsequently, a modified U-Net with deep supervision is
exploited to accurately locate the bifurcations. This two-step method reduces
various problems, such as the missed detections caused by two landmarks being
close to each other and having similar visual characteristics, especially when
processing the complete MRA Time-of-Flight (TOF). Additionally, it accounts for
the anatomical variability of the CoW, which affects the number of detectable
landmarks per scan. We assessed the effectiveness of our approach using two
cerebral MRA datasets: our In-House dataset which had varying numbers of
landmarks, and a public dataset with standardized landmark configuration. Our
experimental results demonstrate that our method achieves the highest level of
performance on a bifurcation detection task.

</details>


### [52] [Lightweight Shrimp Disease Detection Research Based on YOLOv8n](https://arxiv.org/abs/2507.02354)
*Fei Yuhuan,Wang Gengchen,Liu Fenghao,Zang Ran,Sun Xufei,Chang Hao*

Main category: cs.CV

> 本文提出了一种基于YOLOv8n轻量化网络模型，通过引入RLDD检测头和C2f-EMCM模块，提高了计算效率并减少了计算复杂度，同时引入改进的SegNext_Attention自注意力机制，增强了模型特征提取能力。实验结果表明，所提模型相比原文模型参数减少32.3%，且在mAP@0.5指标上提高了3%。同时在模型大小和参数数量上也优于其他轻量级YOLO模型，在泛化性测试中也有良好表现。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于对虾养殖中疾病造成的经济损失，为了提高智能检测效率和减少疾病传播，提出了一种新的基于YOLOv8n的轻量化网络模型。

**Method:** 该论文通过设计RLDD检测头和C2f-EMCM模块来优化模型，同时引入了改进的SegNext_Attention自注意力机制。

**Result:** 模型相比原版本YOLOv8n，参数减少了32.3%，mAP@0.5提升了3%，并在其他轻量级YOLO模型中表现出更优异的mAP@0.5指标、参数数量和模型大小，且在泛化性测试中也有显著的mAP@0.5提升，达到4.1%。

**Conclusion:** 该模型实现了准确性和效率之间的最优平衡，为对虾养殖的智能疾病检测提供了可靠的技术支持。

**Abstract:** Shrimp diseases are one of the primary causes of economic losses in shrimp
aquaculture. To prevent disease transmission and enhance intelligent detection
efficiency in shrimp farming, this paper proposes a lightweight network
architecture based on YOLOv8n. First, by designing the RLDD detection head and
C2f-EMCM module, the model reduces computational complexity while maintaining
detection accuracy, improving computational efficiency. Subsequently, an
improved SegNext_Attention self-attention mechanism is introduced to further
enhance the model's feature extraction capability, enabling more precise
identification of disease characteristics. Extensive experiments, including
ablation studies and comparative evaluations, are conducted on a
self-constructed shrimp disease dataset, with generalization tests extended to
the URPC2020 dataset. Results demonstrate that the proposed model achieves a
32.3% reduction in parameters compared to the original YOLOv8n, with a mAP@0.5
of 92.7% (3% improvement over YOLOv8n). Additionally, the model outperforms
other lightweight YOLO-series models in mAP@0.5, parameter count, and model
size. Generalization experiments on the URPC2020 dataset further validate the
model's robustness, showing a 4.1% increase in mAP@0.5 compared to YOLOv8n. The
proposed method achieves an optimal balance between accuracy and efficiency,
providing reliable technical support for intelligent disease detection in
shrimp aquaculture.

</details>


### [53] [Holistic Tokenizer for Autoregressive Image Generation](https://arxiv.org/abs/2507.02358)
*Anlin Zheng,Haochen Wang,Yucheng Zhao,Weipeng Deng,Tiancai Wang,Xiangyu Zhang,Xiaojuan Qi*

Main category: cs.CV

> Hita, an innovative image tokenizer, accelerates training speed and outperforms vanilla tokenizers in autoregressive image generation, achieving significant improvements in FID and IS scores on ImageNet.

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of vanilla autoregressive models in capturing holistic relationships among token sequences and the lack of global information in most visual tokenizers.

**Method:** Hita, a novel image tokenizer for autoregressive (AR) image generation that uses a holistic-to-local tokenization scheme with learnable holistic queries and local patch tokens, and a sequential structure with causal attention to maintain alignment with the AR generation process.

**Result:** Hita achieves 2.59 FID and 281.9 IS on ImageNet, outperforming vanilla tokenizers, and demonstrates effectiveness in zero-shot style transfer and image in-painting.

**Conclusion:** Hita's holistic representation allows it to effectively capture global image properties, making it a promising approach for autoregressive image generation, with open-source code for further exploration and use.

**Abstract:** The vanilla autoregressive image generation model generates visual tokens in
a step-by-step fashion, which limits the ability to capture holistic
relationships among token sequences. Moreover, most visual tokenizers map local
image patches into latent tokens, leading to limited global information. To
address this, we introduce \textit{Hita}, a novel image tokenizer for
autoregressive (AR) image generation. It introduces a holistic-to-local
tokenization scheme with learnable holistic queries and local patch tokens.
Besides, Hita incorporates two key strategies for improved alignment with the
AR generation process: 1) it arranges a sequential structure with holistic
tokens at the beginning followed by patch-level tokens while using causal
attention to maintain awareness of previous tokens; and 2) before feeding the
de-quantized tokens into the decoder, Hita adopts a lightweight fusion module
to control information flow to prioritize holistic tokens. Extensive
experiments show that Hita accelerates the training speed of AR generators and
outperforms those trained with vanilla tokenizers, achieving \textbf{2.59 FID}
and \textbf{281.9 IS} on the ImageNet benchmark. A detailed analysis of the
holistic representation highlights its ability to capture global image
properties such as textures, materials, and shapes. Additionally, Hita also
demonstrates effectiveness in zero-shot style transfer and image in-painting.
The code is available at
\href{https://github.com/CVMI-Lab/Hita}{https://github.com/CVMI-Lab/Hita}

</details>


### [54] [LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling](https://arxiv.org/abs/2507.02363)
*Jiahao Wu,Rui Peng,Jianbo Jiao,Jiayu Yang,Luyang Tang,Kaiqiang Xiong,Jie Liang,Jinbo Yan,Runling Liu,Ronggang Wang*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Due to the complex and highly dynamic motions in the real world, synthesizing
dynamic videos from multi-view inputs for arbitrary viewpoints is challenging.
Previous works based on neural radiance field or 3D Gaussian splatting are
limited to modeling fine-scale motion, greatly restricting their application.
In this paper, we introduce LocalDyGS, which consists of two parts to adapt our
method to both large-scale and fine-scale motion scenes: 1) We decompose a
complex dynamic scene into streamlined local spaces defined by seeds, enabling
global modeling by capturing motion within each local space. 2) We decouple
static and dynamic features for local space motion modeling. A static feature
shared across time steps captures static information, while a dynamic residual
field provides time-specific features. These are combined and decoded to
generate Temporal Gaussians, modeling motion within each local space. As a
result, we propose a novel dynamic scene reconstruction framework to model
highly dynamic real-world scenes more realistically. Our method not only
demonstrates competitive performance on various fine-scale datasets compared to
state-of-the-art (SOTA) methods, but also represents the first attempt to model
larger and more complex highly dynamic scenes. Project page:
https://wujh2001.github.io/LocalDyGS/.

</details>


### [55] [UVLM: Benchmarking Video Language Model for Underwater World Understanding](https://arxiv.org/abs/2507.02373)
*Xizhe Xue,Yang Zhou,Dawei Yan,Ying Li,Haokui Zhang,Rong Xiao*

Main category: cs.CV

> This paper presents UVLM, an underwater observation benchmark created from a collaborative approach combining human expertise and AI models, demonstrating improved underwater understanding compared to generic VidLMs.

<details>
  <summary>Details</summary>

**Motivation:** To fill the gap in existing works that primarily focus on terrestrial scenarios, this paper aims to address the high demand application needs of underwater observation.

**Method:** Recently, the remarkable success of large language models (LLMs) has inspired the development of underwater video language models (VidLMs). The authors introduce UVLM, an underwater observation benchmark specifically designed for underwater scenarios. The dataset and prompt engineering will be publicly released.

**Result:** Experiments on two representative VidLMs show that fine-tuning VidLMs on UVLM significantly enhances underwater world understanding and slightly improves performance on existing in-air VidLM benchmarks.

**Conclusion:** The UVLM benchmark, with its detailed underwater challenges and diverse task design, is a valuable resource for advancing the understanding of underwater observation using VidLMs.

**Abstract:** Recently, the remarkable success of large language models (LLMs) has achieved
a profound impact on the field of artificial intelligence. Numerous advanced
works based on LLMs have been proposed and applied in various scenarios. Among
them, video language models (VidLMs) are particularly widely used. However,
existing works primarily focus on terrestrial scenarios, overlooking the highly
demanding application needs of underwater observation. To overcome this gap, we
introduce UVLM, an under water observation benchmark which is build through a
collaborative approach combining human expertise and AI models. To ensure data
quality, we have conducted in-depth considerations from multiple perspectives.
First, to address the unique challenges of underwater environments, we selected
videos that represent typical underwater challenges including light variations,
water turbidity, and diverse viewing angles to construct the dataset. Second,
to ensure data diversity, the dataset covers a wide range of frame rates,
resolutions, 419 classes of marine animals, and various static plants and
terrains. Next, for task diversity, we adopted a structured design where
observation targets are categorized into two major classes: biological and
environmental. Each category includes content observation and change/action
observation, totaling 20 distinct task types. Finally, we designed several
challenging evaluation metrics to enable quantitative comparison and analysis
of different methods. Experiments on two representative VidLMs demonstrate that
fine-tuning VidLMs on UVLM significantly improves underwater world
understanding while also showing potential for slight improvements on existing
in-air VidLM benchmarks, such as VideoMME and Perception text. The dataset and
prompt engineering will be released publicly.

</details>


### [56] [PLOT: Pseudo-Labeling via Video Object Tracking for Scalable Monocular 3D Object Detection](https://arxiv.org/abs/2507.02393)
*Seokyeong Lee,Sithu Aung,Junyong Choi,Seungryong Kim,Ig-Jae Kim,Junghyun Cho*

Main category: cs.CV

> 本研究旨在解决单目3D目标检测中的数据稀缺和2D到3D不确定性挑战，提出了一种基于视频数据和伪LiDAR聚合的新伪标签框架，增强遮挡鲁棒性和3D属性提取能力。

<details>
  <summary>Details</summary>

**Motivation:** 单目3D目标检测长期面临由于标注成本高和内在2D到3D不确定性而造成的数据稀缺问题。尽管已经提出了各种弱监督方法和伪标签方法来解决这些问题，但它们大多受特定领域学习的限制，或者仅仅依赖于单次观察的形状信息。

**Method:** 本研究提出了一种新颖的伪标签框架，仅使用视频数据，并且对遮挡具有更强的鲁棒性，无需多视角设置、额外传感器、相机姿态或特定领域的训练。具体而言，该方法利用对象点追踪技术，在时间相邻帧上聚合静态和动态对象的伪LiDAR数据，以便在难以获取3D数据的场景中提取3D属性。

**Result:** 广泛的实验表明，该方法在单目3D目标检测中能够确保可靠的精度和强可扩展性。

**Conclusion:** 实验证明，该方法能够在缺乏3D数据获取的情况下，确保可靠的精度和强大的可扩展性，成为单目3D目标检测（M3OD）的一种实用且有效的方法。

**Abstract:** Monocular 3D object detection (M3OD) has long faced challenges due to data
scarcity caused by high annotation costs and inherent 2D-to-3D ambiguity.
Although various weakly supervised methods and pseudo-labeling methods have
been proposed to address these issues, they are mostly limited by
domain-specific learning or rely solely on shape information from a single
observation. In this paper, we propose a novel pseudo-labeling framework that
uses only video data and is more robust to occlusion, without requiring a
multi-view setup, additional sensors, camera poses, or domain-specific
training. Specifically, we explore a technique for aggregating the
pseudo-LiDARs of both static and dynamic objects across temporally adjacent
frames using object point tracking, enabling 3D attribute extraction in
scenarios where 3D data acquisition is infeasible. Extensive experiments
demonstrate that our method ensures reliable accuracy and strong scalability,
making it a practical and effective solution for M3OD.

</details>


### [57] [Continual Multiple Instance Learning with Enhanced Localization for Histopathological Whole Slide Image Analysis](https://arxiv.org/abs/2507.02395)
*Byung Hyun Lee,Wongi Jeong,Woojae Han,Kyoungbun Lee,Se Young Chun*

Main category: cs.CV

> 本文提出了用于本地化和最小遗忘适应的持续多实例学习框架CoMEL，在三个WSI数据集上实验表明优于先前工作。

<details>
  <summary>Details</summary>

**Motivation:** 虽然多实例学习在大型图像上通过弱标签显著降低了注释成本，但其对持续性任务中最小遗忘问题的适应性却鲜有研究，特别是在实例分类本地化上。此外，已经研究的弱增量学习语义分割针对的是自然图像，这种方法对于多实例学习的本地化可能存在挑战。

**Method:** CoMEL框架包括：(1) Grouped Double Attention Transformer (GDAT)用于高效的实例编码，(2) Bag Prototypes-based Pseudo-Labeling (BPPL)用于可靠实例伪标签，(3) Orthogonal Weighted Low-Rank Adaptation (OWLoRA)用于减轻在包和实例分类中的遗忘问题。

**Result:** 在三个公开的WSI数据集上的广泛实验表明，CoMEL在持续多实例学习设置下的表现优于先前的工作，最高提升了11.00%的包级准确率和23.4%的本地化准确率。

**Conclusion:** CoMEL框架，在不牺牲性能的情况下有效地解决了多实例学习在持续性任务中最小遗忘的问题，并且在包级准确率和定位准确率上都有显著提升。

**Abstract:** Multiple instance learning (MIL) significantly reduced annotation costs via
bag-level weak labels for large-scale images, such as histopathological whole
slide images (WSIs). However, its adaptability to continual tasks with minimal
forgetting has been rarely explored, especially on instance classification for
localization. Weakly incremental learning for semantic segmentation has been
studied for continual localization, but it focused on natural images,
leveraging global relationships among hundreds of small patches (e.g., $16
\times 16$) using pre-trained models. This approach seems infeasible for MIL
localization due to enormous amounts ($\sim 10^5$) of large patches (e.g., $256
\times 256$) and no available global relationships such as cancer cells. To
address these challenges, we propose Continual Multiple Instance Learning with
Enhanced Localization (CoMEL), an MIL framework for both localization and
adaptability with minimal forgetting. CoMEL consists of (1) Grouped Double
Attention Transformer (GDAT) for efficient instance encoding, (2) Bag
Prototypes-based Pseudo-Labeling (BPPL) for reliable instance pseudo-labeling,
and (3) Orthogonal Weighted Low-Rank Adaptation (OWLoRA) to mitigate forgetting
in both bag and instance classification. Extensive experiments on three public
WSI datasets demonstrate superior performance of CoMEL, outperforming the prior
arts by up to $11.00\%$ in bag-level accuracy and up to $23.4\%$ in
localization accuracy under the continual MIL setup.

</details>


### [58] [Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake Video Detection](https://arxiv.org/abs/2507.02398)
*Taehoon Kim,Jongwook Choi,Yonghyun Jeong,Haeun Noh,Jaejun Yoo,Seungryul Baek,Jongwon Choi*

Main category: cs.CV

> 提出了一种新的深度伪造视频检测方法，通过利用像素级时间不一致性，达到精确定位和检测伪造伪影的目的。这种方法显著提升了在复杂背景下的伪造视频检测性能。

<details>
  <summary>Details</summary>

**Motivation:** 传统的基于空间频率的检测器往往忽略像素级的时间不一致性，因为它们仅通过堆叠帧间空间频率光谱来表示时间信息，导致无法检测像素级别的伪影。本研究的动机是改进这种检测方法，以识别这些传统检测器无法捕捉到的时间伪影。

**Method:** 提出了一种基于像素级时间不一致性的深度伪造视频检测方法，这种方法利用了一维傅里叶变换时间轴上每个像素的特征，特别关注不自然运动区域。还引入了一个注意力提案模块来精确定位包含时间伪影的区域。最后，联合变压器模块整合了像素级时间频率特征和时空上下文特征，扩大了检测伪造伪影的范围。

**Result:** 该框架在多样化的、具有挑战性的检测场景中展示了强大的性能，代表了深度伪造视频检测的重大进步。

**Conclusion:** 本研究提出了利用像素级时间不一致性特征进行深度伪造视频检测的新方法，这在多样的检测场景中展示了强劲的性能，为深度伪造视频检测技术带来重大进展。

**Abstract:** We introduce a deepfake video detection approach that exploits pixel-wise
temporal inconsistencies, which traditional spatial frequency-based detectors
often overlook. Traditional detectors represent temporal information merely by
stacking spatial frequency spectra across frames, resulting in the failure to
detect temporal artifacts in the pixel plane. Our approach performs a 1D
Fourier transform on the time axis for each pixel, extracting features highly
sensitive to temporal inconsistencies, especially in areas prone to unnatural
movements. To precisely locate regions containing the temporal artifacts, we
introduce an attention proposal module trained in an end-to-end manner.
Additionally, our joint transformer module effectively integrates pixel-wise
temporal frequency features with spatio-temporal context features, expanding
the range of detectable forgery artifacts. Our framework represents a
significant advancement in deepfake video detection, providing robust
performance across diverse and challenging detection scenarios.

</details>


### [59] [From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding](https://arxiv.org/abs/2507.02790)
*Xiangfeng Wang,Xiao Li,Yadong Wei,Xueyu Song,Yang Song,Xiaoqiang Xia,Fangrui Zeng,Zaiyi Chen,Liu Liu,Gu Xu,Tong Xu*

Main category: cs.CV

> 本文提出了一种基于多模态叙事理解的人类启发式自动视频编辑框架（HIVE），通过引入DramaAD数据集来改进现有自动编辑方法，提高了生成视频的连贯性和质量。

<details>
  <summary>Details</summary>

**Motivation:** 随着在线视频内容特别是短视频平台内容的快速增长，现有的自动编辑方法主要依赖于ASR转录的文本线索和端到端片段选择，忽略了丰富的视觉上下文，导致输出不连贯。

**Method:** 我们的方法（HIVE）通过结合字符提取、对话分析和叙述总结，利用多模态大型语言模型来实现对视频内容的整体理解。为了进一步提高连贯性，我们采用场景级分割，并将编辑过程分解为三个子任务：高光检测、开头/结尾选择和无关内容的筛选。

**Result:** 实验结果显示，我们的框架在通用和广告导向的编辑任务中都一直优于现有基线，显著缩小了自动编辑与人工编辑视频之间的质量差距。

**Conclusion:** 我们的研究表明，通过结合多模态理解和场景级编辑策略，可以在自动视频编辑任务上取得显著的进步。

**Abstract:** The rapid growth of online video content, especially on short video
platforms, has created a growing demand for efficient video editing techniques
that can condense long-form videos into concise and engaging clips. Existing
automatic editing methods predominantly rely on textual cues from ASR
transcripts and end-to-end segment selection, often neglecting the rich visual
context and leading to incoherent outputs. In this paper, we propose a
human-inspired automatic video editing framework (HIVE) that leverages
multimodal narrative understanding to address these limitations. Our approach
incorporates character extraction, dialogue analysis, and narrative
summarization through multimodal large language models, enabling a holistic
understanding of the video content. To further enhance coherence, we apply
scene-level segmentation and decompose the editing process into three subtasks:
highlight detection, opening/ending selection, and pruning of irrelevant
content. To facilitate research in this area, we introduce DramaAD, a novel
benchmark dataset comprising over 800 short drama episodes and 500
professionally edited advertisement clips. Experimental results demonstrate
that our framework consistently outperforms existing baselines across both
general and advertisement-oriented editing tasks, significantly narrowing the
quality gap between automatic and human-edited videos.

</details>


### [60] [TABNet: A Triplet Augmentation Self-Recovery Framework with Boundary-Aware Pseudo-Labels for Medical Image Segmentation](https://arxiv.org/abs/2507.02399)
*Peilin Zhang,Shaouxan Wua,Jun Feng,Zhuo Jin,Zhizezhang Gao,Jingkun Chen,Yaqiong Xing,Xiao Zhang*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Background and objective: Medical image segmentation is a core task in
various clinical applications. However, acquiring large-scale, fully annotated
medical image datasets is both time-consuming and costly. Scribble annotations,
as a form of sparse labeling, provide an efficient and cost-effective
alternative for medical image segmentation. However, the sparsity of scribble
annotations limits the feature learning of the target region and lacks
sufficient boundary supervision, which poses significant challenges for
training segmentation networks. Methods: We propose TAB Net, a novel
weakly-supervised medical image segmentation framework, consisting of two key
components: the triplet augmentation self-recovery (TAS) module and the
boundary-aware pseudo-label supervision (BAP) module. The TAS module enhances
feature learning through three complementary augmentation strategies: intensity
transformation improves the model's sensitivity to texture and contrast
variations, cutout forces the network to capture local anatomical structures by
masking key regions, and jigsaw augmentation strengthens the modeling of global
anatomical layout by disrupting spatial continuity. By guiding the network to
recover complete masks from diverse augmented inputs, TAS promotes a deeper
semantic understanding of medical images under sparse supervision. The BAP
module enhances pseudo-supervision accuracy and boundary modeling by fusing
dual-branch predictions into a loss-weighted pseudo-label and introducing a
boundary-aware loss for fine-grained contour refinement. Results: Experimental
evaluations on two public datasets, ACDC and MSCMR seg, demonstrate that TAB
Net significantly outperforms state-of-the-art methods for scribble-based
weakly supervised segmentation. Moreover, it achieves performance comparable to
that of fully supervised methods.

</details>


### [61] [Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection](https://arxiv.org/abs/2507.02844)
*Ziqi Miao,Yi Ding,Lijun Li,Jing Shao*

Main category: cs.CV

> 本研究定义了一种新的视觉中心的破解模式，并提出名为'VisCo'（视觉情境）的攻击方法，通过利用视觉信息加强现实场景的建构，从而更有效地对多模态大型语言模型进行触发有害反应的攻击。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于多模态大型语言模型(MMLMs)的广泛潜力及其在视觉模态中安全性方面的挑战，本篇论文旨在提出一种新的攻击策略，该策略充分利用视觉信息以更安全有效的方式对MMLMs进行攻击。特别关注的是解决现有方法中存在的视觉模态作为触发因素的语义模糊性问题，以及缺乏现实情境的挑战。

**Method:** 通过定义一种新颖的情境：视觉中心的突破，其中视觉信息是构建完整且现实的突破情境的必要组成部分。在此基础上，我们提出了VisCo（视觉情境）攻击。VisCo利用了四种不同的视觉聚焦策略来伪造情境对话，并在必要时动态生成辅助图像以构建视觉中心的突破情境。为了最大化攻击效果，VisCo还引入了自动毒性模糊和语义优化，以生成能够可靠触发目标黑盒MLLMs有害反应的最终攻击提示。

**Result:** 通过实验，我们验证了VisCo攻击的有效性，在MM-SafetyBench上针对GPT-4o模型，其毒性评分为4.78，攻击成功率为85%，显著优于基线方法的2.48毒性评分和22.2%的攻击成功率。

**Conclusion:** 研究结果表明，VisCo攻击方法能够显著提高攻击成功率和毒性评分，展示了通过优化视觉信息构建攻击情景在提升攻击效率方面的潜力，这为攻击多模态语言模型提供了一种全新的视角和手段。

**Abstract:** With the emergence of strong visual-language capabilities, multimodal large
language models (MLLMs) have demonstrated tremendous potential for real-world
applications. However, the security vulnerabilities exhibited by the visual
modality pose significant challenges to deploying such models in open-world
environments. Recent studies have successfully induced harmful responses from
target MLLMs by encoding harmful textual semantics directly into visual inputs.
However, in these approaches, the visual modality primarily serves as a trigger
for unsafe behavior, often exhibiting semantic ambiguity and lacking grounding
in realistic scenarios. In this work, we define a novel setting: visual-centric
jailbreak, where visual information serves as a necessary component in
constructing a complete and realistic jailbreak context. Building on this
setting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates
contextual dialogue using four distinct visual-focused strategies, dynamically
generating auxiliary images when necessary to construct a visual-centric
jailbreak scenario. To maximize attack effectiveness, it incorporates automatic
toxicity obfuscation and semantic refinement to produce a final attack prompt
that reliably triggers harmful responses from the target black-box MLLMs.
Specifically, VisCo achieves a toxicity score of 4.78 and an Attack Success
Rate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming
the baseline, which performs a toxicity score of 2.48 and an ASR of 22.2%. The
code is available at https://github.com/Dtc7w3PQ/Visco-Attack.

</details>


### [62] [Wildlife Target Re-Identification Using Self-supervised Learning in Non-Urban Settings](https://arxiv.org/abs/2507.02403)
*Mufhumudzi Muthivhi,Terence L. van Zyl*

Main category: cs.CV

> 本研究探索自我监督学习在野生动物再识别中的应用，实验表明自我监督方法在数据受限的情况下表现更好，并且在所有下游任务中超越了监督学习方法。

<details>
  <summary>Details</summary>

**Motivation:** 当前最先进的模型依赖于标注数据来训练用于个体分类的监督模型，而本研究旨在探索自我监督学习（SSL）在野生动物再识别中的应用，以减少对标注数据的依赖。

**Method:** 本研究通过自动提取来自相机陷阱数据的两幅图像对，无需监督即可学习个体的不同视图，从而训练一个自我监督模型。这种方法利用了潜在无限的数据流。

**Result:** 实验结果表明，自我监督模型在数据有限的情况下更具有鲁棒性，并且在所有下游任务中，自我监督特征的表现优于监督方法。

**Conclusion:** 研究表明自我监督学习方法在野生动物再识别任务中具有较大的应用潜力，可减少对标注数据的依赖，并且在各种下游任务中表现优异。

**Abstract:** Wildlife re-identification aims to match individuals of the same species
across different observations. Current state-of-the-art (SOTA) models rely on
class labels to train supervised models for individual classification. This
dependence on annotated data has driven the curation of numerous large-scale
wildlife datasets. This study investigates self-supervised learning
Self-Supervised Learning (SSL) for wildlife re-identification. We automatically
extract two distinct views of an individual using temporal image pairs from
camera trap data without supervision. The image pairs train a self-supervised
model from a potentially endless stream of video data. We evaluate the learnt
representations against supervised features on open-world scenarios and
transfer learning in various wildlife downstream tasks. The analysis of the
experimental results shows that self-supervised models are more robust even
with limited data. Moreover, self-supervised features outperform supervision
across all downstream tasks. The code is available here
https://github.com/pxpana/SSLWildlife.

</details>


### [63] [PosDiffAE: Position-aware Diffusion Auto-encoder For High-Resolution Brain Tissue Classification Incorporating Artifact Restoration](https://arxiv.org/abs/2507.02405)
*Ayantika Das,Moitreya Chaudhuri,Koushik Bhat,Keerthi Ram,Mihail Bota,Mohanasankar Sivaprakasam*

Main category: cs.CV

> 该论文提出了一种结合扩散模型与自动编码器的方法，通过结构化潜在空间，实现大脑图像中特定区域细胞模式的识别以及无监督的 artifact 恢复技术。

<details>
  <summary>Details</summary>

**Motivation:** 扩散模型虽然能够生成高清度的图像，但不能直接提取出图像的语义表示，因此与自然提供的自动编码器结合可以解决这一问题。

**Method:** 通过将编码器与扩散模型结合，提出了一种自动编码器格式，该格式可以学习图像特定的表示，并提供手段来组织潜在空间。

**Result:** 该方法主要提出了三种技术：（1）结构化扩散自动编码模型的潜在空间，以识别大脑图像中的特定区域细胞模式；（2）基于邻域意识的无监督撕裂 artifact 恢复技术；（3）利用表征引导和扩散模型在推断时间可操纵的噪声和去噪能力的无监督 JPEG artifact 恢复技术。

**Conclusion:** 通过结合自动编码器与扩散模型的优点，该方法在学习图像特定表示和潜在空间组织方面获得了有效的成果。

**Abstract:** Denoising diffusion models produce high-fidelity image samples by capturing
the image distribution in a progressive manner while initializing with a simple
distribution and compounding the distribution complexity. Although these models
have unlocked new applicabilities, the sampling mechanism of diffusion does not
offer means to extract image-specific semantic representation, which is
inherently provided by auto-encoders. The encoding component of auto-encoders
enables mapping between a specific image and its latent space, thereby offering
explicit means of enforcing structures in the latent space. By integrating an
encoder with the diffusion model, we establish an auto-encoding formulation,
which learns image-specific representations and offers means to organize the
latent space. In this work, First, we devise a mechanism to structure the
latent space of a diffusion auto-encoding model, towards recognizing
region-specific cellular patterns in brain images. We enforce the
representations to regress positional information of the patches from
high-resolution images. This creates a conducive latent space for
differentiating tissue types of the brain. Second, we devise an unsupervised
tear artifact restoration technique based on neighborhood awareness, utilizing
latent representations and the constrained generation capability of diffusion
models during inference. Third, through representational guidance and
leveraging the inference time steerable noising and denoising capability of
diffusion, we devise an unsupervised JPEG artifact restoration technique.

</details>


### [64] [A Novel Tuning Method for Real-time Multiple-Object Tracking Utilizing Thermal Sensor with Complexity Motion Pattern](https://arxiv.org/abs/2507.02408)
*Duong Nguyen-Ngoc Tran,Long Hoang Pham,Chi Dai Tran,Quoc Pham-Nam Ho,Huy-Hung Nguyen,Jae Wook Jeon*

Main category: cs.CV

> A novel tuning method for pedestrian tracking in thermal images optimizes two stages with suitable hyperparameters, achieving high accuracy in surveillance applications.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve multi-object tracking in thermal images for surveillance systems, especially in challenging environments where traditional RGB cameras are less effective due to poor visibility.

**Method:** The paper introduces a novel tuning method for pedestrian tracking in thermal images, which optimizes two stages by tuning hyperparameters to handle complex motion patterns and improve tracking accuracy.

**Result:** Experiments on the PBVS Thermal MOT dataset show that the proposed method achieves high accuracy in various thermal camera conditions without the need for complex reidentification or motion models.

**Conclusion:** The conclusion is that the proposed method is robust and effective for real-world surveillance applications, as it demonstrates high performance in tracking pedestrians in thermal imagery.

**Abstract:** Multi-Object Tracking in thermal images is essential for surveillance
systems, particularly in challenging environments where RGB cameras struggle
due to low visibility or poor lighting conditions. Thermal sensors enhance
recognition tasks by capturing infrared signatures, but a major challenge is
their low-level feature representation, which makes it difficult to accurately
detect and track pedestrians. To address this, the paper introduces a novel
tuning method for pedestrian tracking, specifically designed to handle the
complex motion patterns in thermal imagery. The proposed framework optimizes
two-stages, ensuring that each stage is tuned with the most suitable
hyperparameters to maximize tracking performance. By fine-tuning
hyperparameters for real-time tracking, the method achieves high accuracy
without relying on complex reidentification or motion models. Extensive
experiments on PBVS Thermal MOT dataset demonstrate that the approach is highly
effective across various thermal camera conditions, making it a robust solution
for real-world surveillance applications.

</details>


### [65] [Privacy-preserving Preselection for Face Identification Based on Packing](https://arxiv.org/abs/2507.02414)
*Rundong Xin,Taotao Wang,Jin Wang,Chonghe Zhao,Jing Wang*

Main category: cs.CV

> A novel scheme, PFIP, is proposed to enhance the efficiency of face retrieval in the ciphertext domain for privacy-preserving face identification, achieving high accuracy and significant reduction in retrieval time.

<details>
  <summary>Details</summary>

**Motivation:** The increasing concerns over privacy and the need to prevent the recovery of original facial data drive the research on privacy-preserving face identification systems which operate in the ciphertext domain.

**Method:** PFIP utilizes an innovative preselection mechanism and a packing module in the enrollment stage to reduce computational burdens and enhance the flexibility of face identification systems in the ciphertext domain.

**Result:** Experiments on LFW and CASIA datasets show that PFIP maintains a 100% hit rate while significantly improving retrieval efficiency by 50 times compared to existing methods.

**Conclusion:** PFIP effectively improves the efficiency of face retrieval in the ciphertext domain without compromising the accuracy of face identification.

**Abstract:** Face identification systems operating in the ciphertext domain have garnered
significant attention due to increasing privacy concerns and the potential
recovery of original facial data. However, as the size of ciphertext template
libraries grows, the face retrieval process becomes progressively more
time-intensive. To address this challenge, we propose a novel and efficient
scheme for face retrieval in the ciphertext domain, termed Privacy-Preserving
Preselection for Face Identification Based on Packing (PFIP). PFIP incorporates
an innovative preselection mechanism to reduce computational overhead and a
packing module to enhance the flexibility of biometric systems during the
enrollment stage. Extensive experiments conducted on the LFW and CASIA datasets
demonstrate that PFIP preserves the accuracy of the original face recognition
model, achieving a 100% hit rate while retrieving 1,000 ciphertext face
templates within 300 milliseconds. Compared to existing approaches, PFIP
achieves a nearly 50x improvement in retrieval efficiency.

</details>


### [66] [Determination Of Structural Cracks Using Deep Learning Frameworks](https://arxiv.org/abs/2507.02416)
*Subhasis Dasgupta,Jaydip Sen,Tuhina Halder*

Main category: cs.CV

> 本文介绍了一种新的深度学习架构，通过集成残差U-Net模型和元模型来提高结构裂缝检测的性能，实验表明该方法具有更高的准确性和效率。

<details>
  <summary>Details</summary>

**Motivation:** 通过提高裂纹检测的可靠性和效率，研究旨在解决人工检测缓慢、不一致和容易出错的问题，从而提高公共安全。

**Method:** 本研究使用了多种残差U-Net模型配置，并将其与包含卷积块的元模型集成，以提高结构裂缝检测的准确性和效率。

**Result:** 实验结果表明，残差U-Net模型，特别是在低分辨率图像方面，优于SegNet和传统U-Net模型。集成模型取得了最高的IoU和DICE系数得分，显示了更高的准确性。

**Conclusion:** 集成模型的性能超越所有单独模型，表明在结构缺陷监控任务中实现更可靠的自动化系统的方向。

**Abstract:** Structural crack detection is a critical task for public safety as it helps
in preventing potential structural failures that could endanger lives. Manual
detection by inexperienced personnel can be slow, inconsistent, and prone to
human error, which may compromise the reliability of assessments. The current
study addresses these challenges by introducing a novel deep-learning
architecture designed to enhance the accuracy and efficiency of structural
crack detection. In this research, various configurations of residual U-Net
models were utilized. These models, due to their robustness in capturing fine
details, were further integrated into an ensemble with a meta-model comprising
convolutional blocks. This unique combination aimed to boost prediction
efficiency beyond what individual models could achieve. The ensemble's
performance was evaluated against well-established architectures such as SegNet
and the traditional U-Net. Results demonstrated that the residual U-Net models
outperformed their predecessors, particularly with low-resolution imagery, and
the ensemble model exceeded the performance of individual models, proving it as
the most effective. The assessment was based on the Intersection over Union
(IoU) metric and DICE coefficient. The ensemble model achieved the highest
scores, signifying superior accuracy. This advancement suggests way for more
reliable automated systems in structural defects monitoring tasks.

</details>


### [67] [AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars](https://arxiv.org/abs/2507.02419)
*Yiming Zhong,Xiaolin Zhang,Ligang Liu,Yao Zhao,Yunchao Wei*

Main category: cs.CV

> We introduce AvatarMakeup, a method for applying realistic 3D makeup to virtual avatars using a diffusion model and a coarse-to-fine approach, which outperforms existing techniques.

<details>
  <summary>Details</summary>

**Motivation:** Addressing the challenges in achieving realistic makeup effects for 3D virtual avatars, such as maintaining consistency across expressions, preserving identity, and controlling fine details, while using currently available Gaussian editing methods which fall short.

**Method:** We propose AvatarMakeup, a specialized 3D makeup method that uses a pretrained diffusion model to transfer makeup patterns from a single reference photo. The method employs a coarse-to-fine strategy, first ensuring consistent appearance and identity with the Coherent Duplication method and then refining the details with a refinement module.

**Result:** AvatarMakeup achieves state-of-the-art makeup transfer quality and consistency throughout animation, overcoming limitations of previous methods.

**Conclusion:** The proposed method, AvatarMakeup, successfully overcomes the fundamental challenges in 3D makeup, providing high-quality makeup transfer that is consistent across different expressions and multiple viewpoints.

**Abstract:** Similar to facial beautification in real life, 3D virtual avatars require
personalized customization to enhance their visual appeal, yet this area
remains insufficiently explored. Although current 3D Gaussian editing methods
can be adapted for facial makeup purposes, these methods fail to meet the
fundamental requirements for achieving realistic makeup effects: 1) ensuring a
consistent appearance during drivable expressions, 2) preserving the identity
throughout the makeup process, and 3) enabling precise control over fine
details. To address these, we propose a specialized 3D makeup method named
AvatarMakeup, leveraging a pretrained diffusion model to transfer makeup
patterns from a single reference photo of any individual. We adopt a
coarse-to-fine idea to first maintain the consistent appearance and identity,
and then to refine the details. In particular, the diffusion model is employed
to generate makeup images as supervision. Due to the uncertainties in diffusion
process, the generated images are inconsistent across different viewpoints and
expressions. Therefore, we propose a Coherent Duplication method to coarsely
apply makeup to the target while ensuring consistency across dynamic and
multiview effects. Coherent Duplication optimizes a global UV map by recoding
the averaged facial attributes among the generated makeup images. By querying
the global UV map, it easily synthesizes coherent makeup guidance from
arbitrary views and expressions to optimize the target avatar. Given the coarse
makeup avatar, we further enhance the makeup by incorporating a Refinement
Module into the diffusion model to achieve high makeup quality. Experiments
demonstrate that AvatarMakeup achieves state-of-the-art makeup transfer quality
and consistency throughout animation.

</details>


### [68] [F^2TTA: Free-Form Test-Time Adaptation on Cross-Domain Medical Image Classification via Image-Level Disentangled Prompt Tuning](https://arxiv.org/abs/2507.02437)
*Wei Li,Jingyang Zhang,Lihao Liu,Guoan Wang,Junjun He,Yang Chen,Lixu Gu*

Main category: cs.CV

> 本文提出了针对临床实践中自由形式领域片段适应问题的I-DiPT框架，实验证明其在F$^{2}$TTA任务上优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 在临床实践中，数据以任意长度的领域片段随机到达，这限制了现有的Test-Time Adaptation方法的适用性。因此，提出了F$^{2}$TTA任务以及针对该任务的解决方案。

**Method:** I-DiPT框架采用图像不变提示以探索领域不变表示，用以缓解不可预测的偏移，并采用图像特定提示以适应来自传入片段的每个测试图像。此外，提出不确定性导向遮罩（UoM）和并行图蒸馏（PGD）方法来克服提示知识表示不足的问题。

**Result:** 实验结果证明，所提方法在乳腺癌及青光眼分类任务上优于现有Test-Time Adaptation方法。

**Conclusion:** 通过引入图像不可变提示与图像特定提示，以及提出UoM和PGD方法，本文成功解决了临床数据自由形式传入的问题，提高了模型在F$^{2}$TTA任务上的性能。

**Abstract:** Test-Time Adaptation (TTA) has emerged as a promising solution for adapting a
source model to unseen medical sites using unlabeled test data, due to the high
cost of data annotation. Existing TTA methods consider scenarios where data
from one or multiple domains arrives in complete domain units. However, in
clinical practice, data usually arrives in domain fragments of arbitrary
lengths and in random arrival orders, due to resource constraints and patient
variability. This paper investigates a practical Free-Form Test-Time Adaptation
(F$^{2}$TTA) task, where a source model is adapted to such free-form domain
fragments, with shifts occurring between fragments unpredictably. In this
setting, these shifts could distort the adaptation process. To address this
problem, we propose a novel Image-level Disentangled Prompt Tuning (I-DiPT)
framework. I-DiPT employs an image-invariant prompt to explore domain-invariant
representations for mitigating the unpredictable shifts, and an image-specific
prompt to adapt the source model to each test image from the incoming
fragments. The prompts may suffer from insufficient knowledge representation
since only one image is available for training. To overcome this limitation, we
first introduce Uncertainty-oriented Masking (UoM), which encourages the
prompts to extract sufficient information from the incoming image via masked
consistency learning driven by the uncertainty of the source model
representations. Then, we further propose a Parallel Graph Distillation (PGD)
method that reuses knowledge from historical image-specific and image-invariant
prompts through parallel graph networks. Experiments on breast cancer and
glaucoma classification demonstrate the superiority of our method over existing
TTA approaches in F$^{2}$TTA. Code is available at
https://github.com/mar-cry/F2TTA.

</details>


### [69] [Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic](https://arxiv.org/abs/2507.02443)
*Sandro Costa Magalhães,Marco Almeida,Filipe Neves dos Santos,António Paulo Moreira,Jorge Dias*

Main category: cs.CV

> 本研究通过在FPGA的PL中部署ANNs，解决了机器人在移动检测时遇到的速度问题，并展示了FPGA在加速ANNs方面的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 由于机器人在移动过程中需要减速进行物体检测，并且低帧率的相机配置也影响了检测算法的速度，这在执行任务和探索时会受到限制，导致任务执行时间增加。现有的Vitis-AI框架在FPGA上的应用没有充分利用其PL部分。

**Method:** 本研究使用FINN架构在FPGA的PL中部署了三个ANNs，即4位量化的MobileNet v1、2位量化的CNV和1位量化的CNV（BNN），并且这些模型是在一个公开的数据集(RG2C)上进行训练的。

**Result:** 实验结果显示，MobileNet v1达到了98%的成功率和6611 FPS的推理速度。

**Conclusion:** 研究表明，FPGA可以加速ANNs，使其适用于注意力机制。

**Abstract:** Robots usually slow down for canning to detect objects while moving.
Additionally, the robot's camera is configured with a low framerate to track
the velocity of the detection algorithms. This would be constrained while
executing tasks and exploring, making robots increase the task execution time.
AMD has developed the Vitis-AI framework to deploy detection algorithms into
FPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we
use the FINN architecture to deploy three ANNs, MobileNet v1 with 4-bit
quantisation, CNV with 2-bit quantisation, and CNV with 1-bit quantisation
(BNN), inside an FPGA's PL. The models were trained on the RG2C dataset. This
is a self-acquired dataset released in open access. MobileNet v1 performed
better, reaching a success rate of 98 % and an inference speed of 6611 FPS. In
this work, we proved that we can use FPGAs to speed up ANNs and make them
suitable for attention mechanisms.

</details>


### [70] [IGDNet: Zero-Shot Robust Underexposed Image Enhancement via Illumination-Guided and Denoising](https://arxiv.org/abs/2507.02445)
*Hailong Yan,Junjian Huang,Tingwen Huang*

Main category: cs.CV

> 提出新的图像去噪和照明恢复方法IGDNet，该方法不依赖训练数据，实验表明其在复杂光照下的性能优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的曝光不足图像恢复方法依赖监督学习且容易导致过度增强，本方法旨在解决这些问题。

**Method:** IGDNet采用零样本方法，不需要引导先验或训练数据。该框架包括分解模块和去噪模块。分解模块使用密集连接网络将图像分离为照明和反射成分。去噪模块使用基于光照的像素自适应校正方法增强非均匀光照区域。通过降采样生成噪声对并迭代优化以生成最终结果。

**Result:** 实验结果表明IGDNet在复杂光照条件下显著提升视觉质量。在PSNR(20.41dB)和SSIM(0.860dB)等指标上优于14种最先进的无监督方法。

**Conclusion:** IGDNet作为一种零样本方法，在不需要引导先验或训练数据的情况下，有效增强了非均匀照明区域并抑制了噪声，大幅提升了图像质量。

**Abstract:** Current methods for restoring underexposed images typically rely on
supervised learning with paired underexposed and well-illuminated images.
However, collecting such datasets is often impractical in real-world scenarios.
Moreover, these methods can lead to over-enhancement, distorting
well-illuminated regions. To address these issues, we propose IGDNet, a
Zero-Shot enhancement method that operates solely on a single test image,
without requiring guiding priors or training data. IGDNet exhibits strong
generalization ability and effectively suppresses noise while restoring
illumination. The framework comprises a decomposition module and a denoising
module. The former separates the image into illumination and reflection
components via a dense connection network, while the latter enhances
non-uniformly illuminated regions using an illumination-guided pixel adaptive
correction method. A noise pair is generated through downsampling and refined
iteratively to produce the final result. Extensive experiments on four public
datasets demonstrate that IGDNet significantly improves visual quality under
complex lighting conditions. Quantitative results on metrics like PSNR
(20.41dB) and SSIM (0.860dB) show that it outperforms 14 state-of-the-art
unsupervised methods. The code will be released soon.

</details>


### [71] [Weakly-supervised Contrastive Learning with Quantity Prompts for Moving Infrared Small Target Detection](https://arxiv.org/abs/2507.02454)
*Weiwei Duan,Luping Ji,Shengjia Chen,Sicheng Zhu,Jianghong Huang,Mao Ye*

Main category: cs.CV

> The paper introduces a weakly-supervised method for detecting small moving infrared targets that uses minimal manual annotations.

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the challenges of detecting small moving infrared targets with weak background contrast, particularly the high cost and time required for fully-supervised methods relying on manual annotations.

**Method:** The paper proposes a new weakly-supervised contrastive learning (WeCoL) scheme that uses a pretrained segment anything model (SAM) and a potential target mining strategy to improve detection reliability.

**Result:** The proposed scheme outperforms early fully-supervised methods and even reaches over 90% of state-of-the-art fully-supervised methods on public datasets.

**Conclusion:** The proposed weakly-supervised contrastive learning scheme demonstrates superior performance and reduces the annotation requirements for small moving infrared target detection.

**Abstract:** Different from general object detection, moving infrared small target
detection faces huge challenges due to tiny target size and weak background
contrast.Currently, most existing methods are fully-supervised, heavily relying
on a large number of manual target-wise annotations. However, manually
annotating video sequences is often expensive and time-consuming, especially
for low-quality infrared frame images. Inspired by general object detection,
non-fully supervised strategies ($e.g.$, weakly supervised) are believed to be
potential in reducing annotation requirements. To break through traditional
fully-supervised frameworks, as the first exploration work, this paper proposes
a new weakly-supervised contrastive learning (WeCoL) scheme, only requires
simple target quantity prompts during model training.Specifically, in our
scheme, based on the pretrained segment anything model (SAM), a potential
target mining strategy is designed to integrate target activation maps and
multi-frame energy accumulation.Besides, contrastive learning is adopted to
further improve the reliability of pseudo-labels, by calculating the similarity
between positive and negative samples in feature subspace.Moreover, we propose
a long-short term motion-aware learning scheme to simultaneously model the
local motion patterns and global motion trajectory of small targets.The
extensive experiments on two public datasets (DAUB and ITSDT-15K) verify that
our weakly-supervised scheme could often outperform early fully-supervised
methods. Even, its performance could reach over 90\% of state-of-the-art (SOTA)
fully-supervised ones.

</details>


### [72] [Mesh Silksong: Auto-Regressive Mesh Generation as Weaving Silk](https://arxiv.org/abs/2507.02477)
*Gaochao Song,Zibo Zhao,Haohan Weng,Jingbo Zeng,Rongfei Jia,Shenghua Gao*

Main category: cs.CV

> Mesh Silksong 提出了一种新的网格表示方法，它能有效地减少标记序列的冗余并提高生成几何网格的质量，展现了优于现有方法的效果。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决现有网格标记方法中存在的标记序列冗余度较高的问题，提高网格生成的效率和几何完整性。

**Method:** 通过仅访问每个网格顶点一次减少标记序列冗余度，并以类似于丝绸编织的自回归方式生成网格。

**Result:** Mesh Silksong 是一种紧凑高效的网格表示方法，能够以类似于丝绸编织的自回归方式生成多边形网格。与现有方法不同，它通过仅访问每个网格顶点一次来标记顶点，减少了冗余，将标记序列的冗余度降低了50%，并实现了约22%的压缩率。此外，Mesh Silksong 生成的多边形网格具有流形拓扑、防水检测和一致的面法线等优越的几何属性，这些属性对于实际应用至关重要。实验结果表明，该方法不仅能生成复杂的网格，还显著提高了几何完整性。

**Conclusion:** 实验结果显示，该方法能够有效减少标记序列的冗余，并生成具有高几何完整性且复杂的多边形网格。

**Abstract:** We introduce Mesh Silksong, a compact and efficient mesh representation
tailored to generate the polygon mesh in an auto-regressive manner akin to silk
weaving. Existing mesh tokenization methods always produce token sequences with
repeated vertex tokens, wasting the network capability. Therefore, our approach
tokenizes mesh vertices by accessing each mesh vertice only once, reduces the
token sequence's redundancy by 50\%, and achieves a state-of-the-art
compression rate of approximately 22\%. Furthermore, Mesh Silksong produces
polygon meshes with superior geometric properties, including manifold topology,
watertight detection, and consistent face normals, which are critical for
practical applications. Experimental results demonstrate the effectiveness of
our approach, showcasing not only intricate mesh generation but also
significantly improved geometric integrity.

</details>


### [73] [CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios](https://arxiv.org/abs/2507.02479)
*Teng Fu,Yuwen Chen,Zhuofan Chen,Mengyang Zhao,Bin Li,Xiangyang Xue*

Main category: cs.CV

> 研究提出了一种新的多行人跟踪数据集（CrowdTrack），以促进算法在复杂场景下保持有效性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的多目标跟踪数据集场景构成都较为简单，且缺乏现实性，这限制了在复杂场景下的跟踪研究。因此，本研究旨在提出一个适合复杂场景下多行人跟踪的数据集。

**Method:** 本研究提出了一种名为CrowdTrack的大型困难多行人跟踪数据集，主要从第一人称视角拍摄，涵盖了现实生活中的复杂场景。此数据集由33个视频组成，总共有5,185条轨迹，每个对象都有完整的边界框和唯一的对象ID标注。

**Result:** 研究者们对数据集进行了全面分析，并在该数据集上测试了多个最先进模型。此外，还分析了基础模型在该数据集上的表现。

**Conclusion:** CrowdTrack数据集提供了一个促进算法在复杂条件下保持有效性的平台。此数据集和项目代码已公开发布。

**Abstract:** Multi-object tracking is a classic field in computer vision. Among them,
pedestrian tracking has extremely high application value and has become the
most popular research category. Existing methods mainly use motion or
appearance information for tracking, which is often difficult in complex
scenarios. For the motion information, mutual occlusions between objects often
prevent updating of the motion state; for the appearance information,
non-robust results are often obtained due to reasons such as only partial
visibility of the object or blurred images. Although learning how to perform
tracking in these situations from the annotated data is the simplest solution,
the existing MOT dataset fails to satisfy this solution. Existing methods
mainly have two drawbacks: relatively simple scene composition and
non-realistic scenarios. Although some of the video sequences in existing
dataset do not have the above-mentioned drawbacks, the number is far from
adequate for research purposes. To this end, we propose a difficult large-scale
dataset for multi-pedestrian tracking, shot mainly from the first-person view
and all from real-life complex scenarios. We name it ``CrowdTrack'' because
there are numerous objects in most of the sequences. Our dataset consists of 33
videos, containing a total of 5,185 trajectories. Each object is annotated with
a complete bounding box and a unique object ID. The dataset will provide a
platform to facilitate the development of algorithms that remain effective in
complex situations. We analyzed the dataset comprehensively and tested multiple
SOTA models on our dataset. Besides, we analyzed the performance of the
foundation models on our dataset. The dataset and project code is released at:
https://github.com/loseevaya/CrowdTrack .

</details>


### [74] [MedFormer: Hierarchical Medical Vision Transformer with Content-Aware Dual Sparse Selection Attention](https://arxiv.org/abs/2507.02488)
*Zunhui Xia,Hongxing Li,Libin Lan*

Main category: cs.CV

> 由于传统的医学图像识别方法在通用性和计算效率上存在不足，MedFormer 通过其独特的金字塔缩放结构和内容感知双稀疏选择注意力机制，在各种任务上展示了优越的性能。

<details>
  <summary>Details</summary>

**Motivation:** 传统的基于视觉转换器的方法在医学图像识别中面临着任务专有性和架构定制的问题，这限制了它们的通用性。此外，这些方法要么采用全注意力机制处理长距离依赖，导致高计算成本，要么依赖手工稀疏注意力机制，可能导致次优性能。因此，MedFormer旨在解决这些挑战，提供一种更具通用性和效率的方法。

**Method:** MedFormer 提出了一种高效的医学视觉转换器，其核心方法包括两个部分：1) 采用金字塔缩放结构作为各种医学图像识别任务的通用骨干结构，包括图像分类和稠密预测任务（如语义分割和病变检测）。这有助于分层特征表示，减少特征图的计算负担，从而提升性能。2) 引入了一种新的内容感知双稀疏选择注意力机制（DSSA），以提高计算效率和鲁棒性，并保持高性能。

**Result:** 通过广泛的数据集实验，MedFormer 在所有三种医学图像识别任务（图像分类、语义分割和病变检测）中表现出了高度的有效性。

**Conclusion:** MedFormer 在医学图像识别任务上展示了优于现有医学视觉转换器的通用性和效率，通过结合金字塔缩放结构和内容感知双稀疏选择注意力机制，有效提升了性能和计算效率。

**Abstract:** Medical image recognition serves as a key way to aid in clinical diagnosis,
enabling more accurate and timely identification of diseases and abnormalities.
Vision transformer-based approaches have proven effective in handling various
medical recognition tasks. However, these methods encounter two primary
challenges. First, they are often task-specific and architecture-tailored,
limiting their general applicability. Second, they usually either adopt full
attention to model long-range dependencies, resulting in high computational
costs, or rely on handcrafted sparse attention, potentially leading to
suboptimal performance. To tackle these issues, we present MedFormer, an
efficient medical vision transformer with two key ideas. First, it employs a
pyramid scaling structure as a versatile backbone for various medical image
recognition tasks, including image classification and dense prediction tasks
such as semantic segmentation and lesion detection. This structure facilitates
hierarchical feature representation while reducing the computation load of
feature maps, highly beneficial for boosting performance. Second, it introduces
a novel Dual Sparse Selection Attention (DSSA) with content awareness to
improve computational efficiency and robustness against noise while maintaining
high performance. As the core building technique of MedFormer, DSSA is
explicitly designed to attend to the most relevant content. In addition, a
detailed theoretical analysis has been conducted, demonstrating that MedFormer
has superior generality and efficiency in comparison to existing medical vision
transformers. Extensive experiments on a variety of imaging modality datasets
consistently show that MedFormer is highly effective in enhancing performance
across all three above-mentioned medical image recognition tasks. The code is
available at https://github.com/XiaZunhui/MedFormer.

</details>


### [75] [Temporally-Aware Supervised Contrastive Learning for Polyp Counting in Colonoscopy](https://arxiv.org/abs/2507.02493)
*Luca Parolari,Andrea Cherubini,Lamberto Ballan,Carlo Biffi*

Main category: cs.CV

> 本文提出了一种新的息肉计数方法，利用监督对比损失和时间感知软目标，提高了结肠镜检查中息肉计数的准确性，并降低了碎片化率。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于提高结肠镜检查的自动化过程报告和质量控制，通过更精确的息肉计数来增强结肠镜筛查的成本效益。

**Method:** 本文提出了一种利用监督对比损失函数结合时间感知软目标的方法，旨在解决现有聚类方法中忽视时间关系的问题。这种方法能够捕捉到同一息肉内的变化同时保持息肉之间的判别性。同时，通过引入时间邻接约束，进一步优化了轨迹聚类，减少了时间上距离较远但视觉上相似的轨迹间的错误重新关联。

**Result:** 实验结果表明，采用该方法后，碎片化率比先前的方法降低了2.2倍，确立了在息肉计数上的新标准。

**Conclusion:** 研究结论证明了在息肉计数中考虑时间意识的重要性，并表明所提出的方法通过减少时间上距离较远但视觉上相似的轨迹间的错误重新关联，提高了息肉计数的准确性和稳定性。

**Abstract:** Automated polyp counting in colonoscopy is a crucial step toward automated
procedure reporting and quality control, aiming to enhance the
cost-effectiveness of colonoscopy screening. Counting polyps in a procedure
involves detecting and tracking polyps, and then clustering tracklets that
belong to the same polyp entity. Existing methods for polyp counting rely on
self-supervised learning and primarily leverage visual appearance, neglecting
temporal relationships in both tracklet feature learning and clustering stages.
In this work, we introduce a paradigm shift by proposing a supervised
contrastive loss that incorporates temporally-aware soft targets. Our approach
captures intra-polyp variability while preserving inter-polyp discriminability,
leading to more robust clustering. Additionally, we improve tracklet clustering
by integrating a temporal adjacency constraint, reducing false positive
re-associations between visually similar but temporally distant tracklets. We
train and validate our method on publicly available datasets and evaluate its
performance with a leave-one-out cross-validation strategy. Results demonstrate
a 2.2x reduction in fragmentation rate compared to prior approaches. Our
results highlight the importance of temporal awareness in polyp counting,
establishing a new state-of-the-art. Code is available at
https://github.com/lparolari/temporally-aware-polyp-counting.

</details>


### [76] [MC-INR: Efficient Encoding of Multivariate Scientific Simulation Data using Meta-Learning and Clustered Implicit Neural Representations](https://arxiv.org/abs/2507.02494)
*Hyunsoo Son,Jeonghyun Noh,Suemin Jeon,Chaoli Wang,Won-Ki Jeong*

Main category: cs.CV

> MC-INR克服了现有隐式神经表示技术的限制，能够在处理无结构网格上的多变量科学模拟数据时提供更强的灵活性和更好的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的隐式神经表示方法存在三个主要局限：无法灵活表示复杂结构、主要针对单变量数据以及依赖于结构化网格。因此，这些方法在应用于复杂现实数据集时性能会降低。为了克服这些问题，提出了MC-INR。

**Method:** 结合元学习和聚类技术，提出了一种新型神经网络框架MC-INR，用于处理无结构网格上的多变量数据。该框架还引入了一种基于残差的动态重聚类机制，以根据局部误差自适应地划分聚类，并提出了一种分支层来同时利用多变量数据。

**Result:** 实验结果表明，MC-INR在科学数据编码任务上优于现有方法。

**Conclusion:** MC-INR通过结合元学习和聚类技术，并引入基于残差的动态重聚类机制，解决了现有方法在复杂结构和多变量数据处理上的局限性，显示出其潜在的应用前景。

**Abstract:** Implicit Neural Representations (INRs) are widely used to encode data as
continuous functions, enabling the visualization of large-scale multivariate
scientific simulation data with reduced memory usage. However, existing
INR-based methods face three main limitations: (1) inflexible representation of
complex structures, (2) primarily focusing on single-variable data, and (3)
dependence on structured grids. Thus, their performance degrades when applied
to complex real-world datasets. To address these limitations, we propose a
novel neural network-based framework, MC-INR, which handles multivariate data
on unstructured grids. It combines meta-learning and clustering to enable
flexible encoding of complex structures. To further improve performance, we
introduce a residual-based dynamic re-clustering mechanism that adaptively
partitions clusters based on local error. We also propose a branched layer to
leverage multivariate data through independent branches simultaneously.
Experimental results demonstrate that MC-INR outperforms existing methods on
scientific data encoding tasks.

</details>


### [77] [Automatic Labelling for Low-Light Pedestrian Detection](https://arxiv.org/abs/2507.02513)
*Dimitrios Bouzoulas,Eerik Alamikkotervo,Risto Ojala*

Main category: cs.CV

> 本文提出了一种自动标注管道，用于在低光条件下改进RGB图像中的行人检测，实验结果表明该方法优于传统方法。

<details>
  <summary>Details</summary>

**Motivation:** 在行人安全中，行人检测在RGB图像中是一个关键任务，但缺乏大规模公共数据集导致在低光条件下检测存在问题。为了解决这个问题，作者提出了一种自动标注管道，以改善低光条件下的行人检测性能。

**Method:** 本文提出了一种自动红外-RGB标注管道，包含三个步骤：1) 红外检测，使用了细调后的红外行人检测模型；2) 将红外检测标签转移到其RGB对应图像；3) 使用生成的标签训练低光RGB行人检测对象检测模型。

**Result:** 在使用KAIST数据集进行的研究中，使用生成的标签和真实标签分别训练了对象检测模型。当在之前未见过的图像序列上进行比较时，结果显示，在6个案例中的mAP@50和mAP@50-95度量上，使用生成标签训练的模型优于使用真实标签训练的模型。

**Conclusion:** 研究表明，通过自动化红外-RGB标签转移过程，可以提高低光条件下RGB图像行人检测的性能。

**Abstract:** Pedestrian detection in RGB images is a key task in pedestrian safety, as the
most common sensor in autonomous vehicles and advanced driver assistance
systems is the RGB camera. A challenge in RGB pedestrian detection, that does
not appear to have large public datasets, is low-light conditions. As a
solution, in this research, we propose an automated infrared-RGB labeling
pipeline. The proposed pipeline consists of 1) Infrared detection, where a
fine-tuned model for infrared pedestrian detection is used 2) Label transfer
process from the infrared detections to their RGB counterparts 3) Training
object detection models using the generated labels for low-light RGB pedestrian
detection. The research was performed using the KAIST dataset. For the
evaluation, object detection models were trained on the generated autolabels
and ground truth labels. When compared on a previously unseen image sequence,
the results showed that the models trained on generated labels outperformed the
ones trained on ground-truth labels in 6 out of 9 cases for the mAP@50 and
mAP@50-95 metrics. The source code for this research is available at
https://github.com/BouzoulasDimitrios/IR-RGB-Automated-LowLight-Pedestrian-Labeling

</details>


### [78] [Detecting Multiple Diseases in Multiple Crops Using Deep Learning](https://arxiv.org/abs/2507.02517)
*Vivek Yadav,Anugrah Jain*

Main category: cs.CV

> A deep learning solution is developed to detect multiple diseases in multiple crops, outperforming the state-of-the-art with an accuracy of 99% on a dataset of 17 crops and 34 diseases.

<details>
  <summary>Details</summary>

**Motivation:** Addressing significant crop losses in India due to diseases, pests, and environmental stress, the solution aims to improve agricultural yield and ensure food security.

**Method:** The proposed solution involves creating a unified dataset of 17 crops and 34 diseases and training a deep learning model on it.

**Result:** The model shows a 99% accuracy, which is 7% higher compared to state-of-the-art models that handle only 14 crops and 26 diseases.

**Conclusion:** The solution enhances the range of crops and diseases that can be accurately detected, aiming to provide a valuable tool for Indian farmers.

**Abstract:** India, as a predominantly agrarian economy, faces significant challenges in
agriculture, including substantial crop losses caused by diseases, pests, and
environmental stress. Early detection and accurate identification of diseases
across different crops are critical for improving yield and ensuring food
security. This paper proposes a deep learning based solution for detecting
multiple diseases in multiple crops, aimed to cover India's diverse
agricultural landscape. We first create a unified dataset encompassing images
of 17 different crops and 34 different diseases from various available
repositories. Proposed deep learning model is trained on this dataset and
outperforms the state-of-the-art in terms of accuracy and the number of crops,
diseases covered. We achieve a significant detection accuracy, i.e., 99 percent
for our unified dataset which is 7 percent more when compared to
state-of-the-art handling 14 crops and 26 different diseases only. By improving
the number of crops and types of diseases that can be detected, proposed
solution aims to provide a better product for Indian farmers.

</details>


### [79] [IMASHRIMP: Automatic White Shrimp (Penaeus vannamei) Biometrical Analysis from Laboratory Images Using Computer Vision and Deep Learning](https://arxiv.org/abs/2507.02519)
*Abiam Remache González,Meriem Chagour,Timon Bijan Rüth,Raúl Trapiella Cañedo,Marina Martínez Soler,Álvaro Lorenzo Felipe,Hyun-Suk Shin,María-Jesús Zamorano Serrano,Ricardo Torres,Juan-Antonio Castillo Parra,Eduardo Reyes Abad,Miguel-Ángel Ferrer Ballester,Juan-Manuel Afonso López,Francisco-Mario Hernández Tejera,Adrian Penate-Sanchez*

Main category: cs.CV

> IMASHRIMP系统利用深度学习和计算机视觉技术，对白虾的形态进行自动化分析，以优化水产养殖中的遗传选育工作。系统包含基于修改版ResNet-50的分类模块、“双因素认证”系统、姿态估计算法以及形态回归模块，显著减少了人工错误，并实现了高效准确的虾形态测量。

<details>
  <summary>Details</summary>

**Motivation:** 旨在优化白虾在水产养殖中的遗传选育工作，减少人工错误，提高效率。

**Method:** 采用修改版的深度学习和计算机视觉技术，包括基于ResNet-50的分类模块、"双因素认证"系统、从VitPose改编的姿态估计算法以及使用SVM的回归模块。

**Result:** 系统大幅减少了人工错误，实现了97.94%的平均精确度（mAP）和0.07 cm (+/- 0.1)的像素到厘米转换误差。

**Conclusion:** IMASHRIMP系统展示了其在虾形态自动化分析中的潜力，有助于提高遗传选育效率，并促进更加可持续的水产养殖实践。

**Abstract:** This paper introduces IMASHRIMP, an adapted system for the automated
morphological analysis of white shrimp (Penaeus vannamei}, aimed at optimizing
genetic selection tasks in aquaculture. Existing deep learning and computer
vision techniques were modified to address the specific challenges of shrimp
morphology analysis from RGBD images. IMASHRIMP incorporates two discrimination
modules, based on a modified ResNet-50 architecture, to classify images by the
point of view and determine rostrum integrity. It is proposed a "two-factor
authentication (human and IA)" system, it reduces human error in view
classification from 0.97% to 0% and in rostrum detection from 12.46% to 3.64%.
Additionally, a pose estimation module was adapted from VitPose to predict 23
key points on the shrimp's skeleton, with separate networks for lateral and
dorsal views. A morphological regression module, using a Support Vector Machine
(SVM) model, was integrated to convert pixel measurements to centimeter units.
Experimental results show that the system effectively reduces human error,
achieving a mean average precision (mAP) of 97.94% for pose estimation and a
pixel-to-centimeter conversion error of 0.07 (+/- 0.1) cm. IMASHRIMP
demonstrates the potential to automate and accelerate shrimp morphological
analysis, enhancing the efficiency of genetic selection and contributing to
more sustainable aquaculture practices.The code are available at
https://github.com/AbiamRemacheGonzalez/ImaShrimp-public

</details>


### [80] [MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details](https://arxiv.org/abs/2507.02546)
*Ruicheng Wang,Sicheng Xu,Yue Dong,Yu Deng,Jianfeng Xiang,Zelong Lv,Guangzhong Sun,Xin Tong,Jiaolong Yang*

Main category: cs.CV

> MoGe-2 enhances MoGe by predicting metric scale 3D maps from single images, improving accuracy and detail recovery through a data refinement technique.

<details>
  <summary>Details</summary>

**Motivation:** To extend MoGe for metric geometry prediction on a large scale, and to address the noise and errors in real data which diminish fine-grained detail.

**Method:** We propose MoGe-2, which builds on MoGe to predict metric scale 3D point maps from single images, addressing scale ambiguity while maintaining relative geometry accuracy. We also introduce a data refinement technique to improve fine-grained detail accuracy.

**Result:** Our evaluations show that MoGe-2 achieves superior performance in relative geometry accuracy, precise metric scale, and fine-grained detail recovery.

**Conclusion:** MoGe-2 accomplishes the simultaneous achievement of accurate relative geometry, precise metric scale, and fine-grained detail recovery, outperforming previous methods.

**Abstract:** We propose MoGe-2, an advanced open-domain geometry estimation model that
recovers a metric scale 3D point map of a scene from a single image. Our method
builds upon the recent monocular geometry estimation approach, MoGe, which
predicts affine-invariant point maps with unknown scales. We explore effective
strategies to extend MoGe for metric geometry prediction without compromising
the relative geometry accuracy provided by the affine-invariant point
representation. Additionally, we discover that noise and errors in real data
diminish fine-grained detail in the predicted geometry. We address this by
developing a unified data refinement approach that filters and completes real
data from different sources using sharp synthetic labels, significantly
enhancing the granularity of the reconstructed geometry while maintaining the
overall accuracy. We train our model on a large corpus of mixed datasets and
conducted comprehensive evaluations, demonstrating its superior performance in
achieving accurate relative geometry, precise metric scale, and fine-grained
detail recovery -- capabilities that no previous methods have simultaneously
achieved.

</details>


### [81] [Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning](https://arxiv.org/abs/2507.02565)
*Buzhen Huang,Chen Li,Chongyang Xu,Dongyue Lu,Jinnan Chen,Yangang Wang,Gim Hee Lee*

Main category: cs.CV

> A dual-branch optimization framework is proposed for accurate interactive motion reconstruction, leveraging a diffusion model and various constraints to overcome the limitations of existing human pose estimation methods in challenging scenarios.

<details>
  <summary>Details</summary>

**Motivation:** Existing human pose estimation methods fail to recover plausible close interactions and cannot distinguish human semantics in challenging scenarios due to visual ambiguities and inter-person occlusions.

**Method:** The paper adopts a dual-branch optimization framework to reconstruct accurate interactive motions with plausible body contacts. It includes a diffusion model trained to learn human proxemic behavior and pose priors, along with various constraints based on 3D Gaussians, 2D keypoints, and mesh penetrations.

**Result:** The method is capable of estimating accurate interactions from in-the-wild videos. Experiments on several benchmarks show it outperforms existing approaches.

**Conclusion:** The proposed method demonstrates superior performance in human poses and interactions reconstruction with the use of dual-branch optimization and proxemics prior.

**Abstract:** Due to visual ambiguities and inter-person occlusions, existing human pose
estimation methods cannot recover plausible close interactions from in-the-wild
videos. Even state-of-the-art large foundation models~(\eg, SAM) cannot
accurately distinguish human semantics in such challenging scenarios. In this
work, we find that human appearance can provide a straightforward cue to
address these obstacles. Based on this observation, we propose a dual-branch
optimization framework to reconstruct accurate interactive motions with
plausible body contacts constrained by human appearances, social proxemics, and
physical laws. Specifically, we first train a diffusion model to learn the
human proxemic behavior and pose prior knowledge. The trained network and two
optimizable tensors are then incorporated into a dual-branch optimization
framework to reconstruct human motions and appearances. Several constraints
based on 3D Gaussians, 2D keypoints, and mesh penetrations are also designed to
assist the optimization. With the proxemics prior and diverse constraints, our
method is capable of estimating accurate interactions from in-the-wild videos
captured in complex environments. We further build a dataset with pseudo
ground-truth interaction annotations, which may promote future research on pose
estimation and human behavior understanding. Experimental results on several
benchmarks demonstrate that our method outperforms existing approaches. The
code and data are available at https://www.buzhenhuang.com/works/CloseApp.html.

</details>


### [82] [Parametric shape models for vessels learned from segmentations via differentiable voxelization](https://arxiv.org/abs/2507.02576)
*Alina F. Dima,Suprosanna Shit,Huaqi Qiu,Robbie Holland,Tamara T. Mueller,Fabio Antonio Musio,Kaiyuan Yang,Bjoern Menze,Rickmer Braren,Marcus Makowski,Daniel Rueckert*

Main category: cs.CV

> 本文提出了一种将体素化、网格和参数化模型结合的框架，通过可微分变换，从分割直接学习形状参数，来提高复杂血管几何结构的捕捉准确性。

<details>
  <summary>Details</summary>

**Motivation:** 虽然体素化是研究血管的最常见表示方法，但网格和参数化模型在众多应用中也非常重要。然而，这些表示方法通常通过分割独立获得。我们提出的方法旨在将这三种表示方法结合起来，以更准确地捕捉复杂的血管几何结构。

**Method:** 我们提出了一种框架，将体素化、网格和参数化模型这三种表示方法通过可微分变换结合起来。通过利用可微分体素化，我们能够从分割到形状的匹配中自动提取血管的参数化形状模型，从而无需显式的地面真实形状参数就能学习到形状参数。血管被参数化为中心线和半径，使用三次B样条确保光滑性和连续性。网格可以通过学习到的形状参数被不同地提取，从而可以进行高保真度的网格操作。

**Result:** 实验结果表明，该方法能够准确捕捉复杂血管的几何形状，包括主动脉、动脉瘤和脑血管的体积拟合。

**Conclusion:** 研究揭示了结合不同表示方法并通过可微分变换学习形状参数的方法可以有效地提高血管几何结构的捕捉精度。这种方法在不同类型的血管研究中表现出色。

**Abstract:** Vessels are complex structures in the body that have been studied extensively
in multiple representations. While voxelization is the most common of them,
meshes and parametric models are critical in various applications due to their
desirable properties. However, these representations are typically extracted
through segmentations and used disjointly from each other. We propose a
framework that joins the three representations under differentiable
transformations. By leveraging differentiable voxelization, we automatically
extract a parametric shape model of the vessels through shape-to-segmentation
fitting, where we learn shape parameters from segmentations without the
explicit need for ground-truth shape parameters. The vessel is parametrized as
centerlines and radii using cubic B-splines, ensuring smoothness and continuity
by construction. Meshes are differentiably extracted from the learned shape
parameters, resulting in high-fidelity meshes that can be manipulated post-fit.
Our method can accurately capture the geometry of complex vessels, as
demonstrated by the volumetric fits in experiments on aortas, aneurysms, and
brain vessels.

</details>


### [83] [Structure-aware Semantic Discrepancy and Consistency for 3D Medical Image Self-supervised Learning](https://arxiv.org/abs/2507.02581)
*Tan Pan,Zhaorui Tan,Kaiyu Guo,Dongli Xu,Weidi Xu,Chen Jiang,Xin Guo,Yuan Qi,Yuan Cheng*

Main category: cs.CV

> This paper presents a novel 3D medical image self-supervised learning framework ($S^2DC) that achieves structure-aware representations by addressing limitations in prior methods that ignore anatomical structure variations. Evaluations demonstrate it outperforms existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve limitations in previous self-supervised learning methods for 3D medical imaging by accounting for variations in anatomical structures' location, scale, and morphology, which are critical for capturing meaningful distinctions in medical analysis.

**Method:** The method introduces a new framework called $S^2DC$ that aims to learn structure-aware representations in 3D medical images. The framework has two steps: 1. Enforcing distinct representations for different patches to increase semantic discrepancy using an optimal transport strategy. 2. Promoting semantic consistency at the structural level based on neighborhood similarity distribution.

**Result:** The method was evaluated across 10 datasets, 4 tasks, and 3 modalities, consistently outperforming existing state-of-the-art methods in self-supervised learning.

**Conclusion:** The conclusion is that the proposed $S^2DC$ framework successfully captures structure-aware representations in 3D medical images, leading to superior performance compared to state-of-the-art self-supervised learning methods.

**Abstract:** 3D medical image self-supervised learning (mSSL) holds great promise for
medical analysis. Effectively supporting broader applications requires
considering anatomical structure variations in location, scale, and morphology,
which are crucial for capturing meaningful distinctions. However, previous mSSL
methods partition images with fixed-size patches, often ignoring the structure
variations. In this work, we introduce a novel perspective on 3D medical images
with the goal of learning structure-aware representations. We assume that
patches within the same structure share the same semantics (semantic
consistency) while those from different structures exhibit distinct semantics
(semantic discrepancy). Based on this assumption, we propose an mSSL framework
named $S^2DC$, achieving Structure-aware Semantic Discrepancy and Consistency
in two steps. First, $S^2DC$ enforces distinct representations for different
patches to increase semantic discrepancy by leveraging an optimal transport
strategy. Second, $S^2DC$ advances semantic consistency at the structural level
based on neighborhood similarity distribution. By bridging patch-level and
structure-level representations, $S^2DC$ achieves structure-aware
representations. Thoroughly evaluated across 10 datasets, 4 tasks, and 3
modalities, our proposed method consistently outperforms the state-of-the-art
methods in mSSL.

</details>


### [84] [AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video Understanding](https://arxiv.org/abs/2507.02591)
*Weili Xu,Enxin Song,Wenhao Chai,Xuexiang Wen,Tian Ye,Gaoang Wang*

Main category: cs.CV

> 提出了AuroraLong模型，采用线性RNN来处理长视频理解问题，实现了与更大模型相似的性能，同时显著降低了计算复杂度。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决长视频理解面临的高计算复杂度和高昂内存成本问题，特别是对于基于Transformer的LLMs，其内存和计算需求与输入序列长度呈二次增长。

**Method:** 采用AuroraLong方法，以线性RNN语言模型替换传统的基于Transformer的LLMs组件，适用于任意长度输入序列且具有固定大小的隐藏状态。此外，通过将视觉标记按大小升序重新排序，进一步结合视觉标记合并以提高吞吐量和效率。

**Result:** 尽管AuroraLong仅有20亿参数，并且仅基于公共数据进行训练，但它在多个视频基准测试中的表现与基于Transformer的模型相当，后者具有相似的参数量但基于私有数据集训练。

**Conclusion:** 研究表明，高效的线性RNN能够通过降低计算门槛来普及长视频理解。首次在类似LLaVA的模型中采用线性RNN作为LLM主干，实现开放式视频理解。

**Abstract:** The challenge of long video understanding lies in its high computational
complexity and prohibitive memory cost, since the memory and computation
required by transformer-based LLMs scale quadratically with input sequence
length. We propose AuroraLong to address this challenge by replacing the LLM
component in MLLMs with a linear RNN language model that handles input sequence
of arbitrary length with constant-size hidden states. To further increase
throughput and efficiency, we combine visual token merge with linear RNN models
by reordering the visual tokens by their sizes in ascending order. Despite
having only 2B parameters and being trained exclusively on public data,
AuroraLong achieves performance comparable to Transformer-based models of
similar size trained on private datasets across multiple video benchmarks. This
demonstrates the potential of efficient, linear RNNs to democratize long video
understanding by lowering its computational entry barrier. To our best
knowledge, we are the first to use a linear RNN based LLM backbone in a
LLaVA-like model for open-ended video understanding.

</details>


### [85] [Addressing Camera Sensors Faults in Vision-Based Navigation: Simulation and Dataset Development](https://arxiv.org/abs/2507.02602)
*Riccardo Gallon,Fabian Schiemenz,Alessandra Menicucci,Eberhard Gill*

Main category: cs.CV

> 研究解决了视觉导航中传感器故障检测的数据缺乏问题，生成故障图像数据集用于AI训练。

<details>
  <summary>Details</summary>

**Motivation:** 解决视觉导航算法中传感器故障检测的挑战，推动AI在故障检测中的应用。

**Method:** 通过模拟框架生成包含故障图像的数据集，用于训练和测试基于AI的故障检测算法。

**Result:** 创建了一个包含故障图像的数据集，为基于AI的故障检测算法提供支持。

**Conclusion:** 研究展示了如何通过模拟框架生成故障数据，并为未来的AI故障检测提供了有价值的数据集。

**Abstract:** The increasing importance of Vision-Based Navigation (VBN) algorithms in
space missions raises numerous challenges in ensuring their reliability and
operational robustness. Sensor faults can lead to inaccurate outputs from
navigation algorithms or even complete data processing faults, potentially
compromising mission objectives. Artificial Intelligence (AI) offers a powerful
solution for detecting such faults, overcoming many of the limitations
associated with traditional fault detection methods. However, the primary
obstacle to the adoption of AI in this context is the lack of sufficient and
representative datasets containing faulty image data.
  This study addresses these challenges by focusing on an interplanetary
exploration mission scenario. A comprehensive analysis of potential fault cases
in camera sensors used within the VBN pipeline is presented. The causes and
effects of these faults are systematically characterized, including their
impact on image quality and navigation algorithm performance, as well as
commonly employed mitigation strategies. To support this analysis, a simulation
framework is introduced to recreate faulty conditions in synthetically
generated images, enabling a systematic and controlled reproduction of faulty
data. The resulting dataset of fault-injected images provides a valuable tool
for training and testing AI-based fault detection algorithms. The final link to
the dataset will be added after an embargo period. For peer-reviewers, this
private link is available.

</details>


### [86] [AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image Detection via Multimodal Large Language Models](https://arxiv.org/abs/2507.02664)
*Ziyin Zhou,Yunpeng Luo,Yuanchen Wu,Ke Sun,Jiayi Ji,Ke Yan,Shouhong Ding,Xiaoshuai Sun,Yunsheng Wu,Rongrong Ji*

Main category: cs.CV

> 本研究提出了Holmes-Set数据集和Holmes Pipeline训练框架来提高AIGI检测技术的解释能力和泛化性，以应对高逼真AI生成图像导致的信息安全威胁。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有AI生成图像(AIGI)检测技术缺乏人类可验证解释和在最新生成技术中泛化能力不足的问题，以应对高逼真度AI生成图像在传播误导信息时对公众信息安全造成的威胁。

**Method:** 通过引入一个大规模综合数据集Holmes-Set来解决现有AI生成图像(AIGI)检测技术的两个问题：缺乏可由人类验证的解释以及在最新生成技术中的泛化能力不足。Holmes-Set包括带有图像是否为AI生成解释的指令微调数据集Holmes-SFTSet和人类对齐偏好数据集Holmes-DPOSet。此外，还提出了一种高效的数据标注方法Multi-Expert Jury，通过结构化MLLM解释和质量控制增强数据生成。在这个过程中，采用三阶段训练框架Holmes Pipeline(视觉专家预训练、监督微调和直接偏好优化)，适应多模态大型语言模型进行AIGI检测，并生成可验证和对齐人类的解释。推理阶段，通过集成视觉专家模型感知与MLLM语义推理的合作解码策略进一步提高泛化能力。

**Result:** 广泛的实验证明了提出的AIGI-Holmes模型在三个基准测试中的有效性。

**Conclusion:** 本研究的AIGI-Holmes模型在提高AI生成图像检测的可解释性和泛化能力方面取得了显著效果。

**Abstract:** The rapid development of AI-generated content (AIGC) technology has led to
the misuse of highly realistic AI-generated images (AIGI) in spreading
misinformation, posing a threat to public information security. Although
existing AIGI detection techniques are generally effective, they face two
issues: 1) a lack of human-verifiable explanations, and 2) a lack of
generalization in the latest generation technology. To address these issues, we
introduce a large-scale and comprehensive dataset, Holmes-Set, which includes
the Holmes-SFTSet, an instruction-tuning dataset with explanations on whether
images are AI-generated, and the Holmes-DPOSet, a human-aligned preference
dataset. Our work introduces an efficient data annotation method called the
Multi-Expert Jury, enhancing data generation through structured MLLM
explanations and quality control via cross-model evaluation, expert defect
filtering, and human preference modification. In addition, we propose Holmes
Pipeline, a meticulously designed three-stage training framework comprising
visual expert pre-training, supervised fine-tuning, and direct preference
optimization. Holmes Pipeline adapts multimodal large language models (MLLMs)
for AIGI detection while generating human-verifiable and human-aligned
explanations, ultimately yielding our model AIGI-Holmes. During the inference
stage, we introduce a collaborative decoding strategy that integrates the model
perception of the visual expert with the semantic reasoning of MLLMs, further
enhancing the generalization capabilities. Extensive experiments on three
benchmarks validate the effectiveness of our AIGI-Holmes.

</details>


### [87] [Learning few-step posterior samplers by unfolding and distillation of diffusion models](https://arxiv.org/abs/2507.02686)
*Charlesquin Kemajou Mbakam,Jonathan Spence,Marcelo Pereyra*

Main category: cs.CV

> 本文提出了一种将扩散模型图像先验转换为几步条件模型的新框架，该框架通过深度展开和模型蒸馏将最近提出的LATINO兰格文采样器集成到MCMC算法中，从而在保持灵活适应性的同时提高准确性和计算效率。

<details>
  <summary>Details</summary>

**Motivation:** 扩散模型（DMs）作为贝叶斯计算成像中的图像先验显示出其强大的能力。提出的两种主要策略——Plug-and-Play方法（零样本、高度灵活但依赖于近似）和专门的条件扩散模型（针对特定任务通过监督训练获得更高的准确性和更快的推理速度）仍存在局限。通过提出新的框架，旨在结合二者优势，解决现有方法的不足。

**Method:** 通过深度展开和模型蒸馏，将扩散模型图像先验转换为用于后验采样的几步条件模型。核心创新是展开马尔可夫链蒙特卡洛（MCMC）算法——具体为最近提出的LATINO兰格文采样器（Spagnoletti等，2025），这是首次将深度展开应用于蒙特卡洛采样方案。

**Result:** 该方法在各种实验环境中相较于现有技术水平展示出了显著的准确性和计算效率，证明了其方法的有效性。

**Conclusion:** 提出的展开和蒸馏采样器在大量实验和与现有技术水平的比较中展示出了优秀的准确性和计算效率，同时保留在推理时间适应正向模型变化的能力。

**Abstract:** Diffusion models (DMs) have emerged as powerful image priors in Bayesian
computational imaging. Two primary strategies have been proposed for leveraging
DMs in this context: Plug-and-Play methods, which are zero-shot and highly
flexible but rely on approximations; and specialized conditional DMs, which
achieve higher accuracy and faster inference for specific tasks through
supervised training. In this work, we introduce a novel framework that
integrates deep unfolding and model distillation to transform a DM image prior
into a few-step conditional model for posterior sampling. A central innovation
of our approach is the unfolding of a Markov chain Monte Carlo (MCMC) algorithm
- specifically, the recently proposed LATINO Langevin sampler (Spagnoletti et
al., 2025) - representing the first known instance of deep unfolding applied to
a Monte Carlo sampling scheme. We demonstrate our proposed unfolded and
distilled samplers through extensive experiments and comparisons with the state
of the art, where they achieve excellent accuracy and computational efficiency,
while retaining the flexibility to adapt to variations in the forward model at
inference time.

</details>


### [88] [APT: Adaptive Personalized Training for Diffusion Models with Limited Data](https://arxiv.org/abs/2507.02687)
*JungWoo Chae,Jiyoon Kim,JaeWoong Choi,Kyungyul Kim,Sangheum Hwang*

Main category: cs.CV

> APT addresses overfitting in personalized diffusion models, preserving prior knowledge and text alignment, leading to improved image generation quality and diversity with limited reference data.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of personalizing diffusion models using limited data, such as overfitting, loss of prior knowledge, and poor text alignment.

**Method:** Adaptive Personalized Training (APT), a novel framework with three components: Adaptive Training Adjustment, Representation Stabilization, and Attention Alignment for Prior Knowledge Preservation.

**Result:** Experiments show APT effectively mitigates overfitting, preserves prior knowledge, and outperforms existing methods in generating high-quality, diverse images with limited reference data.

**Conclusion:** APT framework effectively addresses overfitting and preserves prior knowledge in personalized diffusion models, leading to the generation of high-quality, diverse images with limited reference data.

**Abstract:** Personalizing diffusion models using limited data presents significant
challenges, including overfitting, loss of prior knowledge, and degradation of
text alignment. Overfitting leads to shifts in the noise prediction
distribution, disrupting the denoising trajectory and causing the model to lose
semantic coherence. In this paper, we propose Adaptive Personalized Training
(APT), a novel framework that mitigates overfitting by employing adaptive
training strategies and regularizing the model's internal representations
during fine-tuning. APT consists of three key components: (1) Adaptive Training
Adjustment, which introduces an overfitting indicator to detect the degree of
overfitting at each time step bin and applies adaptive data augmentation and
adaptive loss weighting based on this indicator; (2)Representation
Stabilization, which regularizes the mean and variance of intermediate feature
maps to prevent excessive shifts in noise prediction; and (3) Attention
Alignment for Prior Knowledge Preservation, which aligns the cross-attention
maps of the fine-tuned model with those of the pretrained model to maintain
prior knowledge and semantic coherence. Through extensive experiments, we
demonstrate that APT effectively mitigates overfitting, preserves prior
knowledge, and outperforms existing methods in generating high-quality, diverse
images with limited reference data.

</details>


### [89] [CanonSwap: High-Fidelity and Consistent Video Face Swapping via Canonical Space Modulation](https://arxiv.org/abs/2507.02691)
*Xiangyang Luo,Ye Zhu,Yunfei Liu,Lijian Lin,Cong Wan,Zijian Cai,Shao-Lun Huang,Yu Li*

Main category: cs.CV

> CanonSwap is a video face-swapping method that decouples motion and appearance to achieve high-quality identity transfer while preserving target face dynamics.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind CanonSwap is to address the limitations of current face-swapping methods that, while achieving high-quality identity transfer, often fail to maintain the dynamic attributes of the target face, such as head poses and facial expressions.

**Method:** The paper proposes CanonSwap, a new framework for video face swapping that aims to improve on existing methods by decoupling motion and appearance information. It achieves this by first eliminating motion-related information to allow for identity modification in a unified canonical space, followed by reintegration into the original video to preserve dynamic attributes. Additionally, it introduces a Partial Identity Modulation module that uses a spatial mask to adaptively integrate source identity features.

**Result:** Extensive experiments show that CanonSwap significantly outperforms existing methods in visual quality, temporal consistency, and identity preservation.

**Conclusion:** CanonSwap effectively addresses the main challenges of video face swapping by decoupling motion and appearance information and applying adaptive partial identity modulation, leading to superior results in both identity transfer and dynamic attribute preservation.

**Abstract:** Video face swapping aims to address two primary challenges: effectively
transferring the source identity to the target video and accurately preserving
the dynamic attributes of the target face, such as head poses, facial
expressions, lip-sync, \etc. Existing methods mainly focus on achieving
high-quality identity transfer but often fall short in maintaining the dynamic
attributes of the target face, leading to inconsistent results. We attribute
this issue to the inherent coupling of facial appearance and motion in videos.
To address this, we propose CanonSwap, a novel video face-swapping framework
that decouples motion information from appearance information. Specifically,
CanonSwap first eliminates motion-related information, enabling identity
modification within a unified canonical space. Subsequently, the swapped
feature is reintegrated into the original video space, ensuring the
preservation of the target face's dynamic attributes. To further achieve
precise identity transfer with minimal artifacts and enhanced realism, we
design a Partial Identity Modulation module that adaptively integrates source
identity features using a spatial mask to restrict modifications to facial
regions. Additionally, we introduce several fine-grained synchronization
metrics to comprehensively evaluate the performance of video face swapping
methods. Extensive experiments demonstrate that our method significantly
outperforms existing approaches in terms of visual quality, temporal
consistency, and identity preservation. Our project page are publicly available
at https://luoxyhappy.github.io/CanonSwap/.

</details>


### [90] [SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond Feature Alignment](https://arxiv.org/abs/2507.02705)
*Qi Xu,Dongxu Wei,Lingzhe Zhao,Wenpu Li,Zhangchi Huang,Shunping Ji,Peidong Liu*

Main category: cs.CV

> The paper presents SIU3R, an innovative alignment-free framework for simultaneous 3D reconstruction and understanding, significantly enhancing performance over previous methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to overcome the limitations and potential semantic information loss caused by the current 2D-to-3D feature alignment paradigm used in embodied intelligent systems.

**Method:** The paper proposes SIU3R, an alignment-free framework for 3D reconstruction and understanding from unposed images, using a pixel-aligned 3D representation and unified learnable queries.

**Result:** The proposed method achieves state-of-the-art performance on the individual tasks of 3D reconstruction and understanding, as well as on simultaneous understanding and 3D reconstruction.

**Conclusion:** The alignment-free framework of SIU3R and the designed mutual benefit modules are effective in enhancing the performance of simultaneous 3D reconstruction and understanding.

**Abstract:** Simultaneous understanding and 3D reconstruction plays an important role in
developing end-to-end embodied intelligent systems. To achieve this, recent
approaches resort to 2D-to-3D feature alignment paradigm, which leads to
limited 3D understanding capability and potential semantic information loss. In
light of this, we propose SIU3R, the first alignment-free framework for
generalizable simultaneous understanding and 3D reconstruction from unposed
images. Specifically, SIU3R bridges reconstruction and understanding tasks via
pixel-aligned 3D representation, and unifies multiple understanding tasks into
a set of unified learnable queries, enabling native 3D understanding without
the need of alignment with 2D models. To encourage collaboration between the
two tasks with shared representation, we further conduct in-depth analyses of
their mutual benefits, and propose two lightweight modules to facilitate their
interaction. Extensive experiments demonstrate that our method achieves
state-of-the-art performance not only on the individual tasks of 3D
reconstruction and understanding, but also on the task of simultaneous
understanding and 3D reconstruction, highlighting the advantages of our
alignment-free framework and the effectiveness of the mutual benefit designs.

</details>


### [91] [UniMC: Taming Diffusion Transformer for Unified Keypoint-Guided Multi-Class Image Generation](https://arxiv.org/abs/2507.02713)
*Qin Guo,Ailing Zeng,Dongxu Yue,Ceyuan Yang,Yang Cao,Hanzhong Guo,Fei Shen,Wei Liu,Xihui Liu,Dan Xu*

Main category: cs.CV

> Although significant advancements have been achieved in the progress of keypoint-guided Text-to-Image diffusion models, existing mainstream keypoint-guided models encounter challenges in controlling the generation of more general non-rigid objects beyond humans and animals. Extensive experiments demonstrate the high quality of HAIG-2.9M and the effectiveness of UniMC, particularly under heavy occlusions and multi-class scenarios.

<details>
  <summary>Details</summary>

**Motivation:** To address the problems arising from the limitations of controllable methods for generating images with multiple overlapping humans and animals and the lack of suitable datasets for this task, the authors propose a new framework, UniMC, and a new dataset, HAIG-2.9M.

**Method:** Structure

**Result:** {"tldr": "\u8be5\u70b9\u5b9a\u5f00\u53d1\u6a21\u578b\u4e0e\u8d44\u6599\u96c6\u79ef\u5408\u5f62\u6210\u529b\u5f0f\u63a8\u51fa\u4e86UniMC\uff0c\u4ee5\u53ca\u4e00\u4e2a\u4e2d\u5927\u8981\u70b9\u5f00\u53d1\u8d44\u6599\u96c6\u5728\u4eba\u7269\u548c\u5c0f\u5b69\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u4e00\u7247\uff1aHAIG-2.9M\uff0e", "motivation": "\u5bf9\u4e0e\u73b0\u6709\u7684\u4e00\u4e9b\u4e0d\u8db3\u7b2c\u96c6\u4e0e\u7b2c\u4e00\u4e2a\u9879\u76ee\uff0c\u4ee5\u53ca\u4e0d\u8db3\u4eba\u7269\u548c\u5c0f\u5b69\u7684\u793e\u4f1a\u5206\u6210\u65b9\u9762\u7684\u4e0a\u5c01\u4e0d\u53ca\u591a\u7c7b\u8d44\u6599\u96c6\uff0c\u8be5\u7406\u8bba\u751f\u6210\u51fa\u4e86UniMC\u548cHAIG-2.9M\uff0e", "method": "\u8be5\u7ba1\u7406\u5b8c\u6574\u5305\u542b\u4e86\u7269\u4f53\u7ea7\u548c\u70b9\u7ea7\u6761\u4ef6\u7684\u8f6c\u5316\uff0c\u4ee5\u53ca\u4e00\u4e2a\u4e2d\u5927\u8d44\u6599\u96c6\u5e26\u6709\u7a7a\u95f4\u4fe1\u606f\u548c\u70b9\u5b9a\u660e\u70b9\uff0e", "result": "\u5b9e\u9a8c\u7ed3\u679c\u63d0\u793a\u4e86HAIG-2.9M\u6216\u8005UniMC\u5728\u591a\u7c7b\u548c\u4e0a\u5c01\u8d8a\u7ea7\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\uff0e", "conclusion": "\u8be5\u7406\u8bba\u4e3b\u8981\u6d4b\u8bd5\u4e86\u4e2d\u5927\u8d44\u6599\u96c6\u548c\u5176\u4e0a\u5c01\u8d8a\u7ea7\u51b3\u4e49\u5177\u4f53\u7684\u975e\u5e38\u6709\u529b\u3002"}

**Conclusion:** The paper concludes by evaluating the performance of the proposed dataset and framework specifically under challenging conditions such as heavy occlusions and multi-class scenarios. The results indicate that the proposed methods handle such conditions effectively.

**Abstract:** Although significant advancements have been achieved in the progress of
keypoint-guided Text-to-Image diffusion models, existing mainstream
keypoint-guided models encounter challenges in controlling the generation of
more general non-rigid objects beyond humans (e.g., animals). Moreover, it is
difficult to generate multiple overlapping humans and animals based on keypoint
controls solely. These challenges arise from two main aspects: the inherent
limitations of existing controllable methods and the lack of suitable datasets.
First, we design a DiT-based framework, named UniMC, to explore unifying
controllable multi-class image generation. UniMC integrates instance- and
keypoint-level conditions into compact tokens, incorporating attributes such as
class, bounding box, and keypoint coordinates. This approach overcomes the
limitations of previous methods that struggled to distinguish instances and
classes due to their reliance on skeleton images as conditions. Second, we
propose HAIG-2.9M, a large-scale, high-quality, and diverse dataset designed
for keypoint-guided human and animal image generation. HAIG-2.9M includes 786K
images with 2.9M instances. This dataset features extensive annotations such as
keypoints, bounding boxes, and fine-grained captions for both humans and
animals, along with rigorous manual inspection to ensure annotation accuracy.
Extensive experiments demonstrate the high quality of HAIG-2.9M and the
effectiveness of UniMC, particularly in heavy occlusions and multi-class
scenarios.

</details>


### [92] [FairHuman: Boosting Hand and Face Quality in Human Image Generation with Minimum Potential Delay Fairness in Diffusion Models](https://arxiv.org/abs/2507.02714)
*Yuxuan Wang,Tianwei Cao,Huayu Zhang,Zhongjiang He,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

> 本文提出FairHuman方法解决人像生成中局部细节生成质量低的问题，通过多目标优化策略，提升了人像生成的整体质量，尤其是在手部和面部细节生成方面。

<details>
  <summary>Details</summary>

**Motivation:** 由于训练过程中局部区域的监督不足，使用大规模文本到图像模型，特别是基于扩散的模型，生成具有高度细节（如面部或手部）的人像仍然是一个挑战。

**Method:** 本文提出了一种名为FairHuman的多目标微调方法，旨在公平提升全局和局部图像生成的质量。具体来说，构建了三个学习目标：一个全局目标，是从默认的扩散模型目标函数中衍生；两个局部目标，分别针对手部和面部，基于预注释的位置先验。随后，根据最小潜在延迟(MPD)标准导出了最优参数更新策略，实现了多目标问题的公平优化。

**Result:** 实验结果显示，该方法在不同场景下提升了人类图像生成的整体性能，特别是在生成具有挑战性的局部细节时，同时保持了整体质量。

**Conclusion:** 该研究通过FairHuman方法在人像生成中取得了重要的进展，特别是在改善局部细节生成方面，同时保持了整体图像质量。

**Abstract:** Image generation has achieved remarkable progress with the development of
large-scale text-to-image models, especially diffusion-based models. However,
generating human images with plausible details, such as faces or hands, remains
challenging due to insufficient supervision of local regions during training.
To address this issue, we propose FairHuman, a multi-objective fine-tuning
approach designed to enhance both global and local generation quality fairly.
Specifically, we first construct three learning objectives: a global objective
derived from the default diffusion objective function and two local objectives
for hands and faces based on pre-annotated positional priors. Subsequently, we
derive the optimal parameter updating strategy under the guidance of the
Minimum Potential Delay (MPD) criterion, thereby attaining fairness-ware
optimization for this multi-objective problem. Based on this, our proposed
method can achieve significant improvements in generating challenging local
details while maintaining overall quality. Extensive experiments showcase the
effectiveness of our method in improving the performance of human image
generation under different scenarios.

</details>


### [93] [Prompt learning with bounding box constraints for medical image segmentation](https://arxiv.org/abs/2507.02743)
*Mélanie Gaillochet,Mehrdad Noori,Sahar Dastani,Christian Desrosiers,Hervé Lombaert*

Main category: cs.CV

> 本文提出一种新的框架，结合基础视觉模型和弱监督分割的效率，使用边界框注释自动生成提示并优化，显著降低了医疗图像分割中的注释成本。

<details>
  <summary>Details</summary>

**Motivation:** 医疗领域的像素级注释劳动强度大且成本高。本文旨在通过结合基础模型和弱监督分割的优势，提出一种仅依赖边界框注释的自动化提示生成框架，以减少用户干预并提高效率。

**Method:** 本文提出了一种结合基础模型表示能力和弱监督分割注释效率的新框架。具体来说，该方法使用仅基于边界框注释自动生成提示，并整合了多个从框注释导出的约束以及由提示基础模型生成的伪标签的优化方案。

**Result:** 实验结果显示，在有限数据集上，该弱监督方法的平均Dice评分为84.90%，优于现有的全监督和弱监督方法。

**Conclusion:** 所提出的方法在多重模态数据集上进行了验证，结果表明提出的弱监督方法具有良好的效果，并且优于现有的方法。

**Abstract:** Pixel-wise annotations are notoriously labourious and costly to obtain in the
medical domain. To mitigate this burden, weakly supervised approaches based on
bounding box annotations-much easier to acquire-offer a practical alternative.
Vision foundation models have recently shown noteworthy segmentation
performance when provided with prompts such as points or bounding boxes. Prompt
learning exploits these models by adapting them to downstream tasks and
automating segmentation, thereby reducing user intervention. However, existing
prompt learning approaches depend on fully annotated segmentation masks. This
paper proposes a novel framework that combines the representational power of
foundation models with the annotation efficiency of weakly supervised
segmentation. More specifically, our approach automates prompt generation for
foundation models using only bounding box annotations. Our proposed
optimization scheme integrates multiple constraints derived from box
annotations with pseudo-labels generated by the prompted foundation model.
Extensive experiments across multimodal datasets reveal that our weakly
supervised method achieves an average Dice score of 84.90% in a limited data
setting, outperforming existing fully-supervised and weakly-supervised
approaches. The code is available at
https://github.com/Minimel/box-prompt-learning-VFM.git

</details>


### [94] [DexVLG: Dexterous Vision-Language-Grasp Model at Scale](https://arxiv.org/abs/2507.02747)
*Jiawei He,Danshi Li,Xinqiang Yu,Zekun Qi,Wenyao Zhang,Jiayi Chen,Zhaoxiang Zhang,Zhizheng Zhang,Li Yi,He Wang*

Main category: cs.CV

> 研究团队提出了DexVLG模型，用于解决大规模模型在类似人类灵巧手的抓取任务上的挑战，并通过大规模数据集训练模型，在模拟和现实世界实验中验证了模型的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于复杂数据收集的难度，当前研究主要集中在使用大模型对简单抓取器执行抓取任务，而对于类似人类灵巧手的大模型抓取功能的研究较少。因此，该研究旨在提出一种新方法来填补这一空白，即通过视觉-语言-抓取模型对复杂的手指动作进行语言指令引导的抓取姿态预测。

**Method:** 本文介绍了DexVLG，这是一种用于灵巧抓取姿态预测的大规模视觉-语言-抓取模型，能够根据单视角RGBD输入和语言指令对桌面上的物体进行抓取姿态预测。为了实现这一目标，研究团队构建了一个包含1.7亿个灵巧抓取姿态的数据集DexGraspNet 3.0，该数据集包含了17.4万个物体上的语义部位，并配以详细的部位级标注。

**Result:** 该模型在基于物理的模拟环境下的基准测试以及现实世界的实验中均取得了优异的成绩，实现了超过76%的零样本执行成功率和模拟中的同类最佳部分抓取准确率，并且在真实场景中成功实现了部分对齐的抓取。

**Conclusion:** DexVLG展示了出色的零样本泛化能力，在模拟实验中表现出色，同时在真实物体上展示了成功抓取的能力，说明该模型可以用于解决复杂环境中的人类灵巧手抓取任务。

**Abstract:** As large models gain traction, vision-language-action (VLA) systems are
enabling robots to tackle increasingly complex tasks. However, limited by the
difficulty of data collection, progress has mainly focused on controlling
simple gripper end-effectors. There is little research on functional grasping
with large models for human-like dexterous hands. In this paper, we introduce
DexVLG, a large Vision-Language-Grasp model for Dexterous grasp pose prediction
aligned with language instructions using single-view RGBD input. To accomplish
this, we generate a dataset of 170 million dexterous grasp poses mapped to
semantic parts across 174,000 objects in simulation, paired with detailed
part-level captions. This large-scale dataset, named DexGraspNet 3.0, is used
to train a VLM and flow-matching-based pose head capable of producing
instruction-aligned grasp poses for tabletop objects. To assess DexVLG's
performance, we create benchmarks in physics-based simulations and conduct
real-world experiments. Extensive testing demonstrates DexVLG's strong
zero-shot generalization capabilities-achieving over 76% zero-shot execution
success rate and state-of-the-art part-grasp accuracy in simulation-and
successful part-aligned grasps on physical objects in real-world scenarios.

</details>


### [95] [Linear Attention with Global Context: A Multipole Attention Mechanism for Vision and Physics](https://arxiv.org/abs/2507.02748)
*Alex Colagrande,Paul Caillon,Eva Feillet,Alexandre Allauzen*

Main category: cs.CV

> 本文提出了MANO，这是一种线性时间和内存复杂度的注意力机制，适用于高分辨率输入，与现有先进模型性能相当，但资源消耗更低。

<details>
  <summary>Details</summary>

**Motivation:** 传统的Transformer在处理高分辨率输入时，由于二次内存和时间复杂度，在实际情况中不切实际。本文旨在提出一种新方法，减少计算和内存使用，同时保持高性能。

**Method:** 本文介绍了Multipole Attention Neural Operator (MANO)，其灵感来源于$n$-body数值模拟技术，通过将注意力视为网格点之间的交互问题，以距离为基础的多尺度方式计算注意力。这种方法在每个注意力头中保持全局感受野，实现了与网格点数成线性的计算和内存复杂度。

**Result:** 实验结果表明，MANO在图像分类和达西流问题上表现优异，与ViT和Swin Transformer等模型持平，同时大幅减少运行时间和峰值内存占用。

**Conclusion:** MANO通过新的注意力机制设计，在保持高性能的同时显著减少了计算资源的消耗，为处理高分辨率输入提供了新的解决方案。

**Abstract:** Transformers have become the de facto standard for a wide range of tasks,
from image classification to physics simulations. Despite their impressive
performance, the quadratic complexity of standard Transformers in both memory
and time with respect to the input length makes them impractical for processing
high-resolution inputs. Therefore, several variants have been proposed, the
most successful relying on patchification, downsampling, or coarsening
techniques, often at the cost of losing the finest-scale details. In this work,
we take a different approach. Inspired by state-of-the-art techniques in
$n$-body numerical simulations, we cast attention as an interaction problem
between grid points. We introduce the Multipole Attention Neural Operator
(MANO), which computes attention in a distance-based multiscale fashion. MANO
maintains, in each attention head, a global receptive field and achieves linear
time and memory complexity with respect to the number of grid points. Empirical
results on image classification and Darcy flows demonstrate that MANO rivals
state-of-the-art models such as ViT and Swin Transformer, while reducing
runtime and peak memory usage by orders of magnitude. We open source our code
for reproducibility at https://github.com/AlexColagrande/MANO.

</details>


### [96] [Partial Weakly-Supervised Oriented Object Detection](https://arxiv.org/abs/2507.02751)
*Mingxin Liu,Peiyuan Zhang,Yuan Liu,Wei Zhang,Yue Zhou,Ning Liao,Ziyang Gong,Junwei Luo,Zhirui Wang,Yi Yu,Xue Yang*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** The growing demand for oriented object detection (OOD) across various domains
has driven significant research in this area. However, the high cost of dataset
annotation remains a major concern. Current mainstream OOD algorithms can be
mainly categorized into three types: (1) fully supervised methods using
complete oriented bounding box (OBB) annotations, (2) semi-supervised methods
using partial OBB annotations, and (3) weakly supervised methods using weak
annotations such as horizontal boxes or points. However, these algorithms
inevitably increase the cost of models in terms of annotation speed or
annotation cost. To address this issue, we propose:(1) the first Partial
Weakly-Supervised Oriented Object Detection (PWOOD) framework based on
partially weak annotations (horizontal boxes or single points), which can
efficiently leverage large amounts of unlabeled data, significantly
outperforming weakly supervised algorithms trained with partially weak
annotations, also offers a lower cost solution; (2) Orientation-and-Scale-aware
Student (OS-Student) model capable of learning orientation and scale
information with only a small amount of orientation-agnostic or scale-agnostic
weak annotations; and (3) Class-Agnostic Pseudo-Label Filtering strategy (CPF)
to reduce the model's sensitivity to static filtering thresholds. Comprehensive
experiments on DOTA-v1.0/v1.5/v2.0 and DIOR datasets demonstrate that our PWOOD
framework performs comparably to, or even surpasses, traditional
semi-supervised algorithms.

</details>


### [97] [From Pixels to Damage Severity: Estimating Earthquake Impacts Using Semantic Segmentation of Social Media Images](https://arxiv.org/abs/2507.02781)
*Danrong Zhang,Huili Huang,N. Simrill Smith,Nimisha Roy,J. David Frost*

Main category: cs.CV

> 本研究提出了一种新的方法，通过语义分割技术解决地震后社交媒体图片损害评估的主观性问题，提升了损害评估的客观性和全面性。

<details>
  <summary>Details</summary>

**Motivation:** 传统方法依靠分类方法评估地震后的损害程度，这种方法具有主观性且无法考虑到图片中不同程度的损害。为了解决这一问题，研究人员提出了一种新的方法，使损害评估更加客观和全面。

**Method:** 本研究将地震后社交媒体图片的损害程度评估问题重新定义为语义分割问题，通过构建一个细分的损害程度数据集来解决现有方法的主观性和不足。该数据集将损害分为三个等级：未受损结构、受损结构和废墟。研究中使用了SegFormer模型对此数据集进行微调，从而生成地震后社交媒体图片的损害程度分割图。此外，还引入了一个新的损害程度评分系统，通过考虑图片中不同区域不同程度的损害并调整深度估计来量化损害。

**Result:** 通过这种方法，研究能够更客观地量化社交媒体图片中的损害程度，提供了一种更加细致的理解方式，从而能为灾难侦察团队提供精确的指导。

**Conclusion:** 该研究通过构建新的数据集和引入新的损害评分系统，增强了为灾难侦察团队提供精确指导的能力，使得地震后续响应更加有效和精准。

**Abstract:** In the aftermath of earthquakes, social media images have become a crucial
resource for disaster reconnaissance, providing immediate insights into the
extent of damage. Traditional approaches to damage severity assessment in
post-earthquake social media images often rely on classification methods, which
are inherently subjective and incapable of accounting for the varying extents
of damage within an image. Addressing these limitations, this study proposes a
novel approach by framing damage severity assessment as a semantic segmentation
problem, aiming for a more objective analysis of damage in earthquake-affected
areas. The methodology involves the construction of a segmented damage severity
dataset, categorizing damage into three degrees: undamaged structures, damaged
structures, and debris. Utilizing this dataset, the study fine-tunes a
SegFormer model to generate damage severity segmentations for post-earthquake
social media images. Furthermore, a new damage severity scoring system is
introduced, quantifying damage by considering the varying degrees of damage
across different areas within images, adjusted for depth estimation. The
application of this approach allows for the quantification of damage severity
in social media images in a more objective and comprehensive manner. By
providing a nuanced understanding of damage, this study enhances the ability to
offer precise guidance to disaster reconnaissance teams, facilitating more
effective and targeted response efforts in the aftermath of earthquakes.

</details>


### [98] [RichControl: Structure- and Appearance-Rich Training-Free Spatial Control for Text-to-Image Generation](https://arxiv.org/abs/2507.02792)
*Liheng Zhang,Lexi Pang,Hang Ye,Xiaoxuan Ma,Yizhou Wang*

Main category: cs.CV

> 论文提出了一种新的特征注入框架，通过解耦注入时间步骤与去噪过程，并引入其它策略，有效解决了现有方法中存在的问题，提高了图像生成质量。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于克服现有特征注入方法在结构对齐、条件泄漏和视觉伪影方面的限制，特别是在条件图像与自然RGB分布差异较大时的问题。

**Method:** 该论文提出了一种灵活的特征注入框架，该框架将注入时间步骤与去噪过程解耦。其核心是一个结构丰富的注入模块，能够更好地适应对齐和结构保存之间的相互作用。此外还引入了外观丰富的提示和重启精炼策略以进一步增强外观控制和视觉质量。

**Result:** 实验结果表明，该方法在各种零样本条件场景中实现了最先进的性能。

**Conclusion:** 该论文通过提出灵活的特征注入框架和相关策略，实现了结构丰富和外观丰富的无训练图像生成。

**Abstract:** Text-to-image (T2I) diffusion models have shown remarkable success in
generating high-quality images from text prompts. Recent efforts extend these
models to incorporate conditional images (e.g., depth or pose maps) for
fine-grained spatial control. Among them, feature injection methods have
emerged as a training-free alternative to traditional fine-tuning approaches.
However, they often suffer from structural misalignment, condition leakage, and
visual artifacts, especially when the condition image diverges significantly
from natural RGB distributions. By revisiting existing methods, we identify a
core limitation: the synchronous injection of condition features fails to
account for the trade-off between domain alignment and structural preservation
during denoising. Inspired by this observation, we propose a flexible feature
injection framework that decouples the injection timestep from the denoising
process. At its core is a structure-rich injection module, which enables the
model to better adapt to the evolving interplay between alignment and structure
preservation throughout the diffusion steps, resulting in more faithful
structural generation. In addition, we introduce appearance-rich prompting and
a restart refinement strategy to further enhance appearance control and visual
quality. Together, these designs enable training-free generation that is both
structure-rich and appearance-rich. Extensive experiments show that our
approach achieves state-of-the-art performance across diverse zero-shot
conditioning scenarios.

</details>


### [99] [No time to train! Training-Free Reference-Based Instance Segmentation](https://arxiv.org/abs/2507.02798)
*Miguel Espinosa,Chenhongyi Yang,Linus Ericsson,Steven McDonagh,Elliot J. Crowley*

Main category: cs.CV

> This paper introduces a training-free method for object segmentation using a small set of reference images. By leveraging semantic priors and feature matching, it achieves state-of-the-art segmentation results on multiple benchmarks without manual prompts or complex rules.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitation of the Segment Anything Model (SAM) in generating automatic prompts by utilizing a small set of reference images, thereby reducing the burden of manual visual-prompts or complex prompt-generation rules for processing a new image.

**Method:** Our method consists of three stages: (1) memory bank construction; (2) representation aggregation; and (3) semantic-aware feature matching, which allows automatic generation of instance-level segmentation masks using a small set of reference images.

**Result:** Experiments demonstrate significant improvements on segmentation metrics, achieving state-of-the-art performance on COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50), and outperforming other training-free approaches on the Cross-Domain FSOD benchmark (22.4% nAP).

**Conclusion:** The research demonstrates that leveraging strong semantic priors and automatic generation of visual prompts through semantic-aware feature matching can effectively improve instance-level segmentation performance without needing complex prompt-generation rules or manual annotations.

**Abstract:** The performance of image segmentation models has historically been
constrained by the high cost of collecting large-scale annotated data. The
Segment Anything Model (SAM) alleviates this original problem through a
promptable, semantics-agnostic, segmentation paradigm and yet still requires
manual visual-prompts or complex domain-dependent prompt-generation rules to
process a new image. Towards reducing this new burden, our work investigates
the task of object segmentation when provided with, alternatively, only a small
set of reference images. Our key insight is to leverage strong semantic priors,
as learned by foundation models, to identify corresponding regions between a
reference and a target image. We find that correspondences enable automatic
generation of instance-level segmentation masks for downstream tasks and
instantiate our ideas via a multi-stage, training-free method incorporating (1)
memory bank construction; (2) representation aggregation and (3) semantic-aware
feature matching. Our experiments show significant improvements on segmentation
metrics, leading to state-of-the-art performance on COCO FSOD (36.8% nAP),
PASCAL VOC Few-Shot (71.2% nAP50) and outperforming existing training-free
approaches on the Cross-Domain FSOD benchmark (22.4% nAP).

</details>


### [100] [HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars](https://arxiv.org/abs/2507.02803)
*Gent Serifi,Marcel C. Bühler*

Main category: cs.CV

> 提出了一种新的扩展模型HyperGaussians，用于提高单目视频中可动画化人脸模型的品质，特别是在捕捉高频率细节方面表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 虽然在静态人脸图像的制作上取得了很大成功，但单目视频中可动画化的人脸模型仍然存在挑战，比如难以精确捕捉非线性变形、复杂光照效果和细微特征，因此提出了一种新的解决方案。

**Method:** 引入了HyperGaussians，即高维多变量高斯，作为3D高斯点云表示法的新扩展，以此来提升可动画化人脸模型的制作。HyperGaussians通过学习到的局部嵌入对条件进行编码，从而提高了表达能力。为了解决高维协方差矩阵求逆带来的计算复杂度问题，提出了‘逆协方差技巧’，通过重参数化协方差矩阵提高效率。

**Result:** 在19位来自4个人脸数据集的主体上进行评估，结果显示HyperGaussians在数值和视觉上均超过了3DGS，尤其是对于高频率的细节如眼镜框、牙齿、复杂的面部运动和镜面反射表现更优。

**Conclusion:** HyperGaussians通过提高3D高斯表示的表达能力以及解决计算复杂度问题，显著改进了从单目视频创建高质量可动画化人脸模型的效果。

**Abstract:** We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for
high-quality animatable face avatars. Creating such detailed face avatars from
videos is a challenging problem and has numerous applications in augmented and
virtual reality. While tremendous successes have been achieved for static
faces, animatable avatars from monocular videos still fall in the uncanny
valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face
through a collection of 3D Gaussian primitives. 3DGS excels at rendering static
faces, but the state-of-the-art still struggles with nonlinear deformations,
complex lighting effects, and fine details. While most related works focus on
predicting better Gaussian parameters from expression codes, we rethink the 3D
Gaussian representation itself and how to make it more expressive. Our insights
lead to a novel extension of 3D Gaussians to high-dimensional multivariate
Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases
expressivity through conditioning on a learnable local embedding. However,
splatting HyperGaussians is computationally expensive because it requires
inverting a high-dimensional covariance matrix. We solve this by
reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'.
This trick boosts the efficiency so that HyperGaussians can be seamlessly
integrated into existing models. To demonstrate this, we plug in HyperGaussians
into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our
evaluation on 19 subjects from 4 face datasets shows that HyperGaussians
outperform 3DGS numerically and visually, particularly for high-frequency
details like eyeglass frames, teeth, complex facial movements, and specular
reflections.

</details>


### [101] [LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion](https://arxiv.org/abs/2507.02813)
*Fangfu Liu,Hao Li,Jiawei Chi,Hanyang Wang,Minghui Yang,Fudong Wang,Yueqi Duan*

Main category: cs.CV

> The paper presents LangScene-X, an innovative generative framework for 3D reconstruction that addresses the issues of rendering artifacts and poor semantic synthesis in sparse-view scenarios.

<details>
  <summary>Details</summary>

**Motivation:** The goal is to overcome the limitations of current methods that rely on dense, calibrated views and suffer from rendering artifacts and implausible semantic synthesis, especially with limited view data.

**Method:** We introduce LangScene-X, a generative framework that can create consistent multi-modality 3D information from sparse 2D images. It uses a TriMap video diffusion model to generate RGBs, normals, and segmentation maps, and a Language Quantized Compressor (LQC) to efficiently encode language embeddings for cross-scene generalization.

**Result:** LangScene-X shows superior quality and generalizability in 3D reconstruction and understanding compared to state-of-the-art methods, demonstrated through experiments with real-world data.

**Conclusion:** The proposed method, LangScene-X, significantly advances 3D structure recovery and open-vocabulary scene understanding from 2D images, particularly in scenarios with sparse views.

**Abstract:** Recovering 3D structures with open-vocabulary scene understanding from 2D
images is a fundamental but daunting task. Recent developments have achieved
this by performing per-scene optimization with embedded language information.
However, they heavily rely on the calibrated dense-view reconstruction
paradigm, thereby suffering from severe rendering artifacts and implausible
semantic synthesis when limited views are available. In this paper, we
introduce a novel generative framework, coined LangScene-X, to unify and
generate 3D consistent multi-modality information for reconstruction and
understanding. Powered by the generative capability of creating more consistent
novel observations, we can build generalizable 3D language-embedded scenes from
only sparse views. Specifically, we first train a TriMap video diffusion model
that can generate appearance (RGBs), geometry (normals), and semantics
(segmentation maps) from sparse inputs through progressive knowledge
integration. Furthermore, we propose a Language Quantized Compressor (LQC),
trained on large-scale image datasets, to efficiently encode language
embeddings, enabling cross-scene generalization without per-scene retraining.
Finally, we reconstruct the language surface fields by aligning language
information onto the surface of 3D scenes, enabling open-ended language
queries. Extensive experiments on real-world data demonstrate the superiority
of our LangScene-X over state-of-the-art methods in terms of quality and
generalizability. Project Page: https://liuff19.github.io/LangScene-X.

</details>


### [102] [Confidence-driven Gradient Modulation for Multimodal Human Activity Recognition: A Dynamic Contrastive Dual-Path Learning Approach](https://arxiv.org/abs/2507.02826)
*Panpan Ji,Junni Song,Hang Xiao,Hanyu Liu,Chao Li*

Main category: cs.CV

> 为了解决跨模态特征对齐和模态贡献不平衡问题，提出一个新的多模态人体活动识别框架DCDP-HAR，并进行了有效性验证。

<details>
  <summary>Details</summary>

**Motivation:** 解决多模态人体活动识别系统中的跨模态特征对齐困难和模态贡献不平衡问题。

**Method:** 提出了一种称为动态对比双路径网络(DCDP-HAR)的新框架，包含三个关键组成部分：1. 采用ResNet和DenseNet分支协作处理多模态传感器数据的双路径特征提取架构；2. 引入了多阶段对比学习机制，实现从局部感知到语义抽象的逐步对齐；3. 采用基于置信度的梯度调制策略，在反向传播过程中动态监控和调整每个模态分支的学习强度，有效缓解模态竞争。同时采用了基于动量的梯度累积策略以增强训练稳定性。

**Result:** 通过消融研究验证了每个组件的有效性，并在四个公共基准数据集上进行了广泛的比较实验。

**Conclusion:** 

**Abstract:** Sensor-based Human Activity Recognition (HAR) is a core technology that
enables intelligent systems to perceive and interact with their environment.
However, multimodal HAR systems still encounter key challenges, such as
difficulties in cross-modal feature alignment and imbalanced modality
contributions. To address these issues, we propose a novel framework called the
Dynamic Contrastive Dual-Path Network (DCDP-HAR). The framework comprises three
key components. First, a dual-path feature extraction architecture is employed,
where ResNet and DenseNet branches collaboratively process multimodal sensor
data. Second, a multi-stage contrastive learning mechanism is introduced to
achieve progressive alignment from local perception to semantic abstraction.
Third, we present a confidence-driven gradient modulation strategy that
dynamically monitors and adjusts the learning intensity of each modality branch
during backpropagation, effectively alleviating modality competition. In
addition, a momentum-based gradient accumulation strategy is adopted to enhance
training stability. We conduct ablation studies to validate the effectiveness
of each component and perform extensive comparative experiments on four public
benchmark datasets.

</details>


### [103] [USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention Diffusion Network](https://arxiv.org/abs/2507.02827)
*Ying Yu,Hang Xiao,Siyao Li,Jiarui Li,Haotian Tang,Hanyu Liu,Chao Li*

Main category: cs.CV

> 针对人类活动识别中的挑战，本文提出了一种新的优化方法，利用无监督数据扩增和多注意力机制，实现在多数据集上的高性能和在轻量设备上的高效运行。

<details>
  <summary>Details</summary>

**Motivation:** 人类活动识别（HAR）的主要目标是从传感器数据中推断出正在进行的人类行为。然而，HAR仍然面临着关键挑战，如罕见活动的标记样本稀缺、高级特征提取不足以及在轻量设备上的模型性能不佳。

**Method:** 本文提出了一种基于多注意力交互机制的综合优化方法。首先，通过无监督、统计引导的扩散模型进行数据扩增，缓解标签数据稀缺和类别不平衡问题。其次，设计了一个多分支时空交互网络，捕捉序列数据的多尺度特征。同时，引入时序注意力机制以识别关键时间点，而空间注意力增强传感器间的交互。最后，引入自适应多损失函数融合策略以动态调整损失权重并优化整体模型。

**Result:** 实验结果表明，在WISDM、PAMAP2和OPPORTUNITY三个公开数据集上，所提出的USAD网络分别实现了98.84%、93.81%和80.92%的准确率，显著优于现有方法。

**Conclusion:** 此外，在嵌入式设备上的实际部署验证了该方法的高效性和可行性。

**Abstract:** The primary objective of human activity recognition (HAR) is to infer ongoing
human actions from sensor data, a task that finds broad applications in health
monitoring, safety protection, and sports analysis. Despite proliferating
research, HAR still faces key challenges, including the scarcity of labeled
samples for rare activities, insufficient extraction of high-level features,
and suboptimal model performance on lightweight devices. To address these
issues, this paper proposes a comprehensive optimization approach centered on
multi-attention interaction mechanisms. First, an unsupervised,
statistics-guided diffusion model is employed to perform data augmentation,
thereby alleviating the problems of labeled data scarcity and severe class
imbalance. Second, a multi-branch spatio-temporal interaction network is
designed, which captures multi-scale features of sequential data through
parallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels.
Simultaneously, temporal attention mechanisms are incorporated to identify
critical time points, while spatial attention enhances inter-sensor
interactions. A cross-branch feature fusion unit is further introduced to
improve the overall feature representation capability. Finally, an adaptive
multi-loss function fusion strategy is integrated, allowing for dynamic
adjustment of loss weights and overall model optimization. Experimental results
on three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the
proposed unsupervised data augmentation spatio-temporal attention diffusion
network (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively,
significantly outperforming existing approaches. Furthermore, practical
deployment on embedded devices verifies the efficiency and feasibility of the
proposed method.

</details>


### [104] [AnyI2V: Animating Any Conditional Image with Motion Control](https://arxiv.org/abs/2507.02857)
*Ziye Li,Hao Luo,Xincheng Shuai,Henghui Ding*

Main category: cs.CV

> AnyI2V提出了一种无需训练的框架，可以基于用户自定义的运动轨迹对任意条件图像进行动画处理，使得视频生成更加灵活和多样。

<details>
  <summary>Details</summary>

**Motivation:** 为了解决现有T2V和I2V方法中存在的问题，如缺乏对生成内容的空间布局精准控制以及实时图像依赖限制了生成内容的编辑性，AnyI2V被提出以提供更灵活和多样化的视频生成方式。

**Method:** Structure

**Result:** {\n  \"tldr\": \"AnyI2V提出了一种无需训练的框架，可以基于用户自定义的动...",\n  \"motivation\": \"为了解决现有T2V和I2V方法中存在的问题，如缺乏对生成内容的空间布局精准控制以及实时图像依赖限制了生成内容的编辑性，AnyI2V被提出以提供更灵活和多样化的视频生成方式。\",\n  \"method\": \"AnyI2V支持经过用户定义的运动轨迹对任意条件图像进行动画处理，不仅限于传统图像，还拓展到了不被ControlNet支持的数据类型如网格和点云等。幸运的是，它还支持通过LoRA和文本提示进行风格转换和编辑。\",\n  \"result\": \"通过广泛的实验验证了AnyI2V在空间-运动控制视频生成中的优越性能和提供了新的视角。\",\n  \"conclusion\": \"AnyI2V以其独特的训练自由框架和对多种类型数据的支持，为视频生成提供了一个全新的、灵活多变的思路。\"\n}

**Conclusion:** AnyI2V以其独特的训练自由框架和对多种类型数据的支持，为视频生成提供了一个全新的、灵活多变的思路。

**Abstract:** Recent advancements in video generation, particularly in diffusion models,
have driven notable progress in text-to-video (T2V) and image-to-video (I2V)
synthesis. However, challenges remain in effectively integrating dynamic motion
signals and flexible spatial constraints. Existing T2V methods typically rely
on text prompts, which inherently lack precise control over the spatial layout
of generated content. In contrast, I2V methods are limited by their dependence
on real images, which restricts the editability of the synthesized content.
Although some methods incorporate ControlNet to introduce image-based
conditioning, they often lack explicit motion control and require
computationally expensive training. To address these limitations, we propose
AnyI2V, a training-free framework that animates any conditional images with
user-defined motion trajectories. AnyI2V supports a broader range of modalities
as the conditional image, including data types such as meshes and point clouds
that are not supported by ControlNet, enabling more flexible and versatile
video generation. Additionally, it supports mixed conditional inputs and
enables style transfer and editing via LoRA and text prompts. Extensive
experiments demonstrate that the proposed AnyI2V achieves superior performance
and provides a new perspective in spatial- and motion-controlled video
generation. Code is available at https://henghuiding.com/AnyI2V/.

</details>


### [105] [Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for Data-Efficient Model Adaptation](https://arxiv.org/abs/2507.02859)
*Jiaer Xia,Bingkui Tong,Yuhang Zang,Rui Shao,Kaiyang Zhou*

Main category: cs.CV

> 本文研究通过使用包含Chain-of-Thought (CoT) 推理数据训练MLLMs，以改善其在专业视觉任务中的适应性。提出的方法Grounded Chain-of-Thought (GCoT)，在多种视觉格式的任务中显示了显著的改进效果。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决现有的多模态大型语言模型（MLLMs）难以适应专业视觉任务的问题，特别是在数据受限的情况下训练MLLMs。

**Method:** 提出了一种基于自举的方法——Grounded Chain-of-Thought (GCoT)，旨在将绑定信息（即，边界框）注入到CoT数据中，使推理步骤更忠于输入图像。

**Result:** <tool_call>

**Conclusion:** 实验结果表明，在数据受限的情况下，该方法相比微调和蒸馏方法显著提高了性能。

**Abstract:** Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in interpreting images using natural language. However, without
using large-scale datasets for retraining, these models are difficult to adapt
to specialized vision tasks, e.g., chart understanding. This problem is caused
by a mismatch between pre-training and downstream datasets: pre-training
datasets primarily concentrate on scenes and objects but contain limited
information about specialized, non-object images, such as charts and tables. In
this paper, we share an interesting finding that training an MLLM with
Chain-of-Thought (CoT) reasoning data can facilitate model adaptation in
specialized vision tasks, especially under data-limited regimes. However, we
identify a critical issue within CoT data distilled from pre-trained MLLMs,
i.e., the data often contains multiple factual errors in the reasoning steps.
To address the problem, we propose Grounded Chain-of-Thought (GCoT), a simple
bootstrapping-based approach that aims to inject grounding information (i.e.,
bounding boxes) into CoT data, essentially making the reasoning steps more
faithful to input images. We evaluate our approach on five specialized vision
tasks, which cover a variety of visual formats including charts, tables,
receipts, and reports. The results demonstrate that under data-limited regimes
our approach significantly improves upon fine-tuning and distillation.

</details>


### [106] [Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching](https://arxiv.org/abs/2507.02860)
*Xin Zhou,Dingkang Liang,Kaijin Chen,Tianrui Feng,Xiwu Chen,Hongkai Lin,Yikang Ding,Feiyang Tan,Hengshuang Zhao,Xiang Bai*

Main category: cs.CV

> EasyCache is a framework that speeds up video generation models by reusing computation without requiring additional training or complex tuning, making high-quality video synthesis faster and more accessible.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the slow inference speeds and high computational costs associated with current video generation models, which limit their practical use. By improving these aspects, the technology can be more widely adopted.

**Method:** The paper introduces EasyCache, a training-free acceleration framework for video diffusion models. It employs a lightweight, runtime-adaptive caching mechanism to avoid redundant computations by reusing previous transformation vectors.

**Result:** The study shows that EasyCache can reduce inference time by a factor of 2.1-3.3x across different large-scale video generation models while improving PSNR (fidelity) by up to 36% over the previous state-of-the-art method.

**Conclusion:** EasyCache stands out as a highly efficient and accessible option for accelerating high-quality video generation processes, making advanced video synthesis techniques more available for both research and real-world applications.

**Abstract:** Video generation models have demonstrated remarkable performance, yet their
broader adoption remains constrained by slow inference speeds and substantial
computational costs, primarily due to the iterative nature of the denoising
process. Addressing this bottleneck is essential for democratizing advanced
video synthesis technologies and enabling their integration into real-world
applications. This work proposes EasyCache, a training-free acceleration
framework for video diffusion models. EasyCache introduces a lightweight,
runtime-adaptive caching mechanism that dynamically reuses previously computed
transformation vectors, avoiding redundant computations during inference.
Unlike prior approaches, EasyCache requires no offline profiling,
pre-computation, or extensive parameter tuning. We conduct comprehensive
studies on various large-scale video generation models, including OpenSora,
Wan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,
reducing inference time by up to 2.1-3.3$\times$ compared to the original
baselines while maintaining high visual fidelity with a significant up to 36%
PSNR improvement compared to the previous SOTA method. This improvement makes
our EasyCache a efficient and highly accessible solution for high-quality video
generation in both research and practical applications. The code is available
at https://github.com/H-EmbodVis/EasyCache.

</details>


### [107] [LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans](https://arxiv.org/abs/2507.02861)
*Zhening Huang,Xiaoyang Wu,Fangcheng Zhong,Hengshuang Zhao,Matthias Nießner,Joan Lasenby*

Main category: cs.CV

> LiteReality 是通过 RGB-D 扫描创建逼真 3D 虚拟环境的新技术，集成了大量图形处理和物理交互特性和优化策略，可在不经过训练的前提下完成高质量模型的检索。

<details>
  <summary>Details</summary>

**Motivation:** 此项研究机动于开发一种能够创建真实交互式 3D 虚拟环境的技术，旨在填补现有技术在对象独立性、材质质量和交互性等方面的不足。

**Method:** 其技术方法包括场景理解、检索 3D 模型以补充场景内容、增强材料的材质绘制以及将重建的场景整合进一个物理模拟引擎中。

**Result:** LiteReality 是一种将室内环境的 RGB-D 扫描转化为逼真、紧凑且可交互的 3D 虚拟副本的新型流程。它不仅能够还原视觉上接近现实的场景，还支持图形管线必须的关键特性，如对象独立性、处理细节、高质量基于物理渲染材质以及真实物理交互。LiteReality 通过一个结构化的场景图进行场景理解，并将其解析为一个连贯的 3D 布局和对象。后续步骤从精心挑选的资产库中检索出最匹配视觉的 3D 模型进行场景重建，再通过材质绘制模块恢复高质量的空间变化材质。最后，将重建的场景整合到一个具有基本物理特性的模拟引擎中，以实现交互性行为。生成的场景不仅紧凑且可编辑，还与标准图形管线兼容，适合用来进行增强现实/虚拟现实 (AR/VR)、游戏、机器人以及数字孪生等应用。此外，LiteReality 还引入了一个无需训练的对象检索模块，在 Scan2CAD 标准上的表现在同类技术中处于领先。需要注意的是，该材质绘制模块能够在存在严重错位、遮挡和光照不良等情况下依然能够从任意风格的图像中转移外观效果。实验结果显示，LiteReality 能够有效处理真实环境扫描的场景和公共数据集。

**Conclusion:** LiteReality 证明了其在创建从真实扫描到可交互 3D 虚拟场景转换的能力，并且其处理效果优于同类方法，表现出强大的材料转移能力和稳健的对象检索性能。

**Abstract:** We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor
environments into compact, realistic, and interactive 3D virtual replicas.
LiteReality not only reconstructs scenes that visually resemble reality but
also supports key features essential for graphics pipelines -- such as object
individuality, articulation, high-quality physically based rendering materials,
and physically based interaction. At its core, LiteReality first performs scene
understanding and parses the results into a coherent 3D layout and objects with
the help of a structured scene graph. It then reconstructs the scene by
retrieving the most visually similar 3D artist-crafted models from a curated
asset database. Next, the Material Painting module enhances realism by
recovering high-quality, spatially varying materials. Finally, the
reconstructed scene is integrated into a simulation engine with basic physical
properties to enable interactive behavior. The resulting scenes are compact,
editable, and fully compatible with standard graphics pipelines, making them
suitable for applications in AR/VR, gaming, robotics, and digital twins. In
addition, LiteReality introduces a training-free object retrieval module that
achieves state-of-the-art similarity performance on the Scan2CAD benchmark,
along with a robust material painting module capable of transferring
appearances from images of any style to 3D assets -- even under severe
misalignment, occlusion, and poor lighting. We demonstrate the effectiveness of
LiteReality on both real-life scans and public datasets. Project page:
https://litereality.github.io; Video:
https://www.youtube.com/watch?v=ecK9m3LXg2c

</details>


### [108] [RefTok: Reference-Based Tokenization for Video Generation](https://arxiv.org/abs/2507.02862)
*Xiang Fan,Xiaohang Sun,Kushan Thakkar,Zhu Liu,Vimal Bhat,Ranjay Krishna,Xiang Hao*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Effectively handling temporal redundancy remains a key challenge in learning
video models. Prevailing approaches often treat each set of frames
independently, failing to effectively capture the temporal dependencies and
redundancies inherent in videos. To address this limitation, we introduce
RefTok, a novel reference-based tokenization method capable of capturing
complex temporal dynamics and contextual information. Our method encodes and
decodes sets of frames conditioned on an unquantized reference frame. When
decoded, RefTok preserves the continuity of motion and the appearance of
objects across frames. For example, RefTok retains facial details despite head
motion, reconstructs text correctly, preserves small patterns, and maintains
the legibility of handwriting from the context. Across 4 video datasets (K600,
UCF-101, BAIR Robot Pushing, and DAVIS), RefTok significantly outperforms
current state-of-the-art tokenizers (Cosmos and MAGVIT) and improves all
evaluated metrics (PSNR, SSIM, LPIPS) by an average of 36.7% at the same or
higher compression ratios. When a video generation model is trained using
RefTok's latents on the BAIR Robot Pushing task, the generations not only
outperform MAGVIT-B but the larger MAGVIT-L, which has 4x more parameters,
across all generation metrics by an average of 27.9%.

</details>


### [109] [Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory](https://arxiv.org/abs/2507.02863)
*Yuqi Wu,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

> 利用显式的空间指针记忆技术实现了在线密集流式3D重建，表现出优越性能和较低训练成本。

<details>
  <summary>Details</summary>

**Motivation:** 现有的密集3D重建方法依赖于一个隐式记忆，这在容量上存在限制并且可能会早期帧的信息丢失。因此，需要一种新方法以适应密集流式3D重建。

**Method:** Point3R方法是一种面向在线密集流式3D重建的框架，它包含一个显式的空间指针记忆（spatial pointer memory），该记忆与当前场景的3D结构直接关联。每个指针被分配一个特定的3D位置，并将全球坐标系附近的情景信息聚集为一个变化的空间特征。通过设计一个3D分层位置嵌入以促进这些指针之间的交互，并设计了一种简单但有效的融合机制，确保了其统一性和效率。

**Result:** 该方法在多种任务中实现了具有竞争力或最先进的性能，并且训练成本较低。

**Conclusion:** Point3R方法利用显式空间指针记忆，确保了信息有效地融入到全局坐标系中，并通过低位姿态嵌入和融合机制提高了效率和性能，证明了算法的有效性。实现上，在不同任务中表现出优异性能，同时保持低训练成本。

**Abstract:** Dense 3D scene reconstruction from an ordered sequence or unordered image
collections is a critical step when bringing research in computer vision into
practical scenarios. Following the paradigm introduced by DUSt3R, which unifies
an image pair densely into a shared coordinate system, subsequent methods
maintain an implicit memory to achieve dense 3D reconstruction from more
images. However, such implicit memory is limited in capacity and may suffer
from information loss of earlier frames. We propose Point3R, an online
framework targeting dense streaming 3D reconstruction. To be specific, we
maintain an explicit spatial pointer memory directly associated with the 3D
structure of the current scene. Each pointer in this memory is assigned a
specific 3D position and aggregates scene information nearby in the global
coordinate system into a changing spatial feature. Information extracted from
the latest frame interacts explicitly with this pointer memory, enabling dense
integration of the current observation into the global coordinate system. We
design a 3D hierarchical position embedding to promote this interaction and
design a simple yet effective fusion mechanism to ensure that our pointer
memory is uniform and efficient. Our method achieves competitive or
state-of-the-art performance on various tasks with low training costs. Code is
available at: https://github.com/YkiWu/Point3R.

</details>
