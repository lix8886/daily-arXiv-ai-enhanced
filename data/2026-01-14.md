<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 3]
- [cs.CV](#cs.CV) [Total: 7]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [EmbeddingRWKV: State-Centric Retrieval with Reusable States](https://arxiv.org/abs/2601.07861)
*Haowen Hou,Jie Yang*

Main category: cs.CL

> 提出了状态中心检索，通过提取紧凑的、可重用的状态来提高检索系统的效率，实现了显著的速度提升和高效性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的检索增强生成系统通常使用传统的两阶段管道，即初始检索的嵌入模型和重新排名的模型。这种模式由于阶段之间缺乏共享信息而效率低下，导致大量的重复计算。为了应对这个限制，本文提出了一种新的检索范式。

**Method:** 通过'状态'将嵌入模型和重新排名器连接起来，以克服传统两阶段管道的低效率。首先通过微调基于RWKV的大语言模型来学习状态表示，将其转化为EmbeddingRWKV，该模型同时作为嵌入模型和提取紧凑、可重复使用的状态的状态骨干。基于这些可重用的状态，设计了一个基于状态的重新排名器，充分利用了预计算的信息。重新排名过程中，模型仅处理查询标记，使得推理成本与文档长度解耦，实现了5.4倍~44.8倍的速度提升。

**Result:** 受冗余层状态影响小；只使用25%的层也可以保持98.62%的全模型性能。实验表明，状态中心检索不仅实现了高质量的检索和重新排名结果，同时显著提高了系统的整体效率。

**Conclusion:** 本文提出的状态中心检索范式在保持高质量检索和重新排名结果的同时，显著提高了系统的整体效率。

**Abstract:** Current Retrieval-Augmented Generation (RAG) systems typically employ a traditional two-stage pipeline: an embedding model for initial retrieval followed by a reranker for refinement. However, this paradigm suffers from significant inefficiency due to the lack of shared information between stages, leading to substantial redundant computation. To address this limitation, we propose \textbf{State-Centric Retrieval}, a unified retrieval paradigm that utilizes "states" as a bridge to connect embedding models and rerankers. First, we perform state representation learning by fine-tuning an RWKV-based LLM, transforming it into \textbf{EmbeddingRWKV}, a unified model that serves as both an embedding model and a state backbone for extracting compact, reusable states. Building upon these reusable states, we further design a state-based reranker to fully leverage precomputed information. During reranking, the model processes only query tokens, decoupling inference cost from document length and yielding a 5.4$\times$--44.8$\times$ speedup. Furthermore, we observe that retaining all intermediate layer states is unnecessary; with a uniform layer selection strategy, our model maintains 98.62\% of full-model performance using only 25\% of the layers. Extensive experiments demonstrate that State-Centric Retrieval achieves high-quality retrieval and reranking results while significantly enhancing overall system efficiency. Code is available at \href{https://github.com/howard-hou/EmbeddingRWKV}{our GitHub repository}.

</details>


### [2] [A Human-Centric Pipeline for Aligning Large Language Models with Chinese Medical Ethics](https://arxiv.org/abs/2601.07954)
*Haoan Jin,Han Ying,Jiacheng Ji,Hanhui Xu,Mengyue Wu*

Main category: cs.CL

> 本文介绍了一种在中文医疗场景中对大型语言模型进行医疗伦理对齐的方法，并提出了一个动态、情景中心的基准MedES，展示结果表明该方法有效提升模型在医疗伦理任务的表现。

<details>
  <summary>Details</summary>

**Motivation:** 为了解决大型语言模型在复杂真实世界场景下与医疗伦理对齐的挑战，特别是在中国医疗伦理背景下的场景。

**Method:** 提出MedES动态情景中心基准，并引入一种带监督的循环守护者框架，该框架利用专门的自动化评估器生成有针对性的提示并提供结构化的伦理反馈，来优化LLM。

**Result:** 在中文医疗伦理背景下进行的实验结果表明，对齐后的模型在核心伦理任务的关键指标上有显著提高。

**Conclusion:** 该研究提出了一种在中文医疗伦理背景下，通过监督微调和领域特定偏好优化，成功对大型语言模型进行对齐的方法，并展示了一种实用且可适应的框架，该框架可移植到其他法律和文化环境中。

**Abstract:** Recent advances in large language models have enabled their application to a range of healthcare tasks. However, aligning LLMs with the nuanced demands of medical ethics, especially under complex real world scenarios, remains underexplored. In this work, we present MedES, a dynamic, scenario-centric benchmark specifically constructed from 260 authoritative Chinese medical, ethical, and legal sources to reflect the challenges in clinical decision-making. To facilitate model alignment, we introduce a guardian-in-the-loop framework that leverages a dedicated automated evaluator (trained on expert-labeled data and achieving over 97% accuracy within our domain) to generate targeted prompts and provide structured ethical feedback. Using this pipeline, we align a 7B-parameter LLM through supervised fine-tuning and domain-specific preference optimization. Experimental results, conducted entirely within the Chinese medical ethics context, demonstrate that our aligned model outperforms notably larger baselines on core ethical tasks, with observed improvements in both quality and composite evaluation metrics. Our work offers a practical and adaptable framework for aligning LLMs with medical ethics in the Chinese healthcare domain, and suggests that similar alignment pipelines may be instantiated in other legal and cultural environments through modular replacement of the underlying normative corpus.

</details>


### [3] [Knowing But Not Doing: Convergent Morality and Divergent Action in LLMs](https://arxiv.org/abs/2601.07972)
*Jen-tse Huang,Jiantong Qin,Xueli Qiu,Sharon Levy,Michelle R. Kaufman,Mark Dredze*

Main category: cs.CL

> 本文揭示了大型语言模型在决策中表现出的行为价值与人类行为价值的趋同和差异，并发现它们在实际情境中的价值观实现还存在角色扮演的抵触情绪以及知行不一致的问题。

<details>
  <summary>Details</summary>

**Motivation:** 论文研究的动机在于探索大型语言模型（LLMs）如何在其实际决策情境中表示和实现人类价值观，当前这方面的研究仍不充分。

**Method:** 本研究采用了一种名为ValAct-15k的数据集，该数据集包含了3000个基于Reddit的寻求建议的场景，旨在根据Schwartz基本人类价值观理论触发十种价值观。研究通过基于场景的问题和传统价值观问卷这两种方式，对十个前沿的大型语言模型（LLMs）（来自美国和中国公司的各五个模型）和55位人类参与者进行评估。

**Result:** 研究发现，对于基于场景的决策，不同模型之间的结果几乎完全一致（Pearson相关系数$r \approx 1.0$），而人类参与者之间的结果则存在广泛的变异（Pearson相关系数$r \in [-0.79, 0.98]$）。此外，人类和LLMs都表现出自我报告的价值观与实际体现的价值观间的弱对应关系（Pearson相关系数分别为$0.4$和$0.3$），揭示了一个系统性的知行差距。当LLMs被指示“持守”某一特定价值观时，它们的表现比直接选择价值观下降了多达$6.6%$，这表明它们存在角色扮演的抵触情绪。

**Conclusion:** 这些研究结果表明，尽管对齐训练带来了规范的价值观趋同，但并未消除人类般的知行不一致。

**Abstract:** Value alignment is central to the development of safe and socially compatible artificial intelligence. However, how Large Language Models (LLMs) represent and enact human values in real-world decision contexts remains under-explored. We present ValAct-15k, a dataset of 3,000 advice-seeking scenarios derived from Reddit, designed to elicit ten values defined by Schwartz Theory of Basic Human Values. Using both the scenario-based questions and the traditional value questionnaire, we evaluate ten frontier LLMs (five from U.S. companies, five from Chinese ones) and human participants ($n = 55$). We find near-perfect cross-model consistency in scenario-based decisions (Pearson $r \approx 1.0$), contrasting sharply with the broad variability observed among humans ($r \in [-0.79, 0.98]$). Yet, both humans and LLMs show weak correspondence between self-reported and enacted values ($r = 0.4, 0.3$), revealing a systematic knowledge-action gap. When instructed to "hold" a specific value, LLMs' performance declines up to $6.6%$ compared to merely selecting the value, indicating a role-play aversion. These findings suggest that while alignment training yields normative value convergence, it does not eliminate the human-like incoherence between knowing and acting upon values.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [Edge-AI Perception Node for Cooperative Road-Safety Enforcement and Connected-Vehicle Integration](https://arxiv.org/abs/2601.07845)
*Shree Charran R,Rahul Kumar Dubey*

Main category: cs.CV

> 本文提出了一种基于边缘人工智能的实时路侧感知节点，用于多类交通违章分析和智能车辆生态系统中的安全事件传播。该节点集成了YOLOv8 Nano进行高精度多目标检测，DeepSORT进行车辆持续跟踪，以及规则引导的OCR后处理引擎。整体系统实现了高效率、低功耗、高精度的违章检测和安全事件管理，展示了边缘AI在智能交通中的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 快速机动车化带来的执法不对称问题，传统的监视和人工罚单无法应对车辆数量的巨大增长，因此需要一个自主、合作和节能的边缘人工智能感知基础设斲。

**Method:** 采用了YOLOv8 Nano进行多目标检测，DeepSORT进行车辆持续跟踪，以及规则引导的OCR后处理引擎来识别车牌。整个系统在NVIDIA Jetson Nano设备上优化运行，实现了高效率和低能耗。

**Result:** 实现了97.7%的违章检测准确率和84.9%的OCR识别精度，并相对于其他同类算法，在准确率和能效上有了显著提升。

**Conclusion:** 本研究展示了路边边缘AI分析在智能交通系统中的实际应用，不仅仅是执法工具，还提供了标准化的安全事件传播，促进了协同感知和主动道路安全管理。

**Abstract:** Rapid motorization in emerging economies such as India has created severe enforcement asymmetries, with over 11 million recorded violations in 2023 against a human policing density of roughly one officer per 4000 vehicles. Traditional surveillance and manual ticketing cannot scale to this magnitude, motivating the need for an autonomous, cooperative, and energy efficient edge AI perception infrastructure. This paper presents a real time roadside perception node for multi class traffic violation analytics and safety event dissemination within a connected and intelligent vehicle ecosystem. The node integrates YOLOv8 Nano for high accuracy multi object detection, DeepSORT for temporally consistent vehicle tracking, and a rule guided OCR post processing engine capable of recognizing degraded or multilingual license plates compliant with MoRTH AIS 159 and ISO 7591 visual contrast standards. Deployed on an NVIDIA Jetson Nano with a 128 core Maxwell GPU and optimized via TensorRT FP16 quantization, the system sustains 28 to 30 frames per second inference at 9.6 W, achieving 97.7 percent violation detection accuracy and 84.9 percent OCR precision across five violation classes, namely signal jumping, zebra crossing breach, wrong way driving, illegal U turn, and speeding, without manual region of interest calibration. Comparative benchmarking against YOLOv4 Tiny, PP YOLOE S, and Nano DetPlus demonstrates a 10.7 percent mean average precision gain and a 1.4 times accuracy per watt improvement. Beyond enforcement, the node publishes standardized safety events of CAM and DENM type to connected vehicles and intelligent transportation system backends via V2X protocols, demonstrating that roadside edge AI analytics can augment cooperative perception and proactive road safety management within the IEEE Intelligent Vehicles ecosystem.

</details>


### [5] [An Empirical Study on Knowledge Transfer under Domain and Label Shifts in 3D LiDAR Point Clouds](https://arxiv.org/abs/2601.07855)
*Subeen Lee,Siyeong Lee,Namil Kim,Jaesik Choi*

Main category: cs.CV

> 为了解决3D感知系统在现实世界应用中的模型自适应问题，特别是当对象定义和传感器类型持续变化时，本研究提出了ROAD基准测试，这是一个专门针对基于LiDAR的目标分类评估的综合测试套件。

<details>
  <summary>Details</summary>

**Motivation:** 研究针对连续学习和转移学习在3D点云感知方面的不足，尤其是在同时存在领域和标签变化的情况下。

**Method:** 本研究提出了ROAD基准测试，这是一个用于基于LiDAR的目标分类的综合评估套件，特别关注领域变化以及三种关键形式的标签演化：类别分裂、类别扩展和类别插入。

**Result:** 通过大规模数据集（Waymo, NuScenes, Argoverse2）对零样本转移学习、线性探测和CL进行评估，并分析了主干架构、训练目标以及CL方法的影响。

**Conclusion:** 研究发现现有方法对现实世界变化的适应性存在局限，并且为未来鲁棒的3D感知研究建立了有力的基线。

**Abstract:** For 3D perception systems to be practical in real-world applications -- from autonomous driving to embodied AI -- models must adapt to continuously evolving object definitions and sensor domains. Yet, research on continual and transfer learning in 3D point cloud perception remains underexplored compared to 2D vision -- particularly under simultaneous domain and label shifts. To address this gap, we propose the RObust Autonomous driving under Dataset shifts (ROAD) benchmark, a comprehensive evaluation suite for LiDAR-based object classification that explicitly accounts for domain shifts as well as three key forms of label evolution: class split, class expansion, and class insertion. Using large-scale datasets (Waymo, NuScenes, Argoverse2), we evaluate zero-shot transfer, linear probe, and CL, and analyze the impact of backbone architectures, training objectives, and CL methods. Our findings reveal limitations of existing approaches under realistic shifts and establish strong baselines for future research in robust 3D perception.

</details>


### [6] [Moonworks Lunara Aesthetic Dataset](https://arxiv.org/abs/2601.07941)
*Yan Wang,M M Sayeef Abdullah,Partho Hassan,Sabit Hassan*

Main category: cs.CV

> 此论文介绍了Lunara美学数据集，该数据集涵盖多种艺术风格，通过Moonworks Lunara模型生成，注重美学质量、风格多样性和许可证透明度。每个图像都附带人类精炼的提示和结构化注释。

<details>
  <summary>Details</summary>

**Motivation:** 为了填补现有数据集在美学质量、风格多样性和许可证透明度方面的不足，提供一种适用于研究和无限制学术及商业用途的数据集。

**Method:** 利用Moonworks Lunara模型生成符合特定美学风格的图像，同时为每个图像附带人类编写的描述和结构化注解。

**Result:** 产生了高质量的美学数据集，其美学评分领先于其他同类和通用数据集。

**Conclusion:** Lunara美学数据集支持广泛的研究应用，其透明的Apache 2.0许可模式促进了数据集的学术和商业用途。

**Abstract:** The dataset spans diverse artistic styles, including regionally grounded aesthetics from the Middle East, Northern Europe, East Asia, and South Asia, alongside general categories such as sketch and oil painting. All images are generated using the Moonworks Lunara model and intentionally crafted to embody distinct, high-quality aesthetic styles, yielding a first-of-its-kind dataset with substantially higher aesthetic scores, exceeding even aesthetics-focused datasets, and general-purpose datasets by a larger margin. Each image is accompanied by a human-refined prompt and structured annotations that jointly describe salient objects, attributes, relationships, and stylistic cues. Unlike large-scale web-derived datasets that emphasize breadth over precision, the Lunara Aesthetic Dataset prioritizes aesthetic quality, stylistic diversity, and licensing transparency, and is released under the Apache 2.0 license to support research and unrestricted academic and commercial use.

</details>


### [7] [LWMSCNN-SE: A Lightweight Multi-Scale Network for Efficient Maize Disease Classification on Edge Devices](https://arxiv.org/abs/2601.07957)
*Fikadu Weloday,Jianmei Su*

Main category: cs.CV

> The paper presents LWMSCNN-SE, a lightweight CNN for maize disease classification, achieving high accuracy with low computational cost suitable for deployment on edge devices.

<details>
  <summary>Details</summary>

**Motivation:** To tackle the computational challenges in deploying traditional disease detection models on resource-limited devices like smartphones and drones for Maize disease classification.

**Method:** LWMSCNN-SE combines multi-scale feature extraction, depthwise separable convolutions, and SE attention mechanisms in a lightweight CNN architecture.

**Result:** The model achieves 96.63% classification accuracy with only 241,348 parameters and 0.666 GFLOPs, enabling real-time deployment.

**Conclusion:** LWMSCNN-SE effectively balances accuracy and efficiency, making it a promising solution for real-time maize disease diagnosis using edge devices in precision farming.

**Abstract:** Maize disease classification plays a vital role in mitigating yield losses and ensuring food security. However, the deployment of traditional disease detection models in resource-constrained environments, such as those using smartphones and drones, faces challenges due to high computational costs. To address these challenges, we propose LWMSCNN-SE, a lightweight convolutional neural network (CNN) that integrates multi-scale feature extraction, depthwise separable convolutions, and squeeze-and-Excitation (SE) attention mechanisms. This novel combination enables the model to achieve 96.63% classification accuracy with only 241,348 parameters and 0.666 GFLOPs, making it suitable for real-time deployment in field applications. Our approach addresses the accuracy--efficiency trade-off by delivering high accuracy while maintaining low computational costs, demonstrating its potential for efficient maize disease diagnosis on edge devices in precision farming systems.

</details>


### [8] [3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing](https://arxiv.org/abs/2601.07963)
*Jiahua Dong,Yu-Xiong Wang*

Main category: cs.CV

> 3DGS-Drag is introduced as a point-based framework for 3D editing, offering an efficient and intuitive method for manipulating real 3D scenes via drag editing, surpassing existing methods in geometry-related content editing.

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the challenge of intuitive drag editing in 3D scenes, which is currently more feasible in 2D editing. This work seeks to fill the gap between 2D-editing-based and deformation-based methods in 3D editing and achieve better geometry-related content editing performance.

**Method:** Our approach, 3DGS-Drag, utilizes 3D Gaussian Splatting for deformation guidance and diffusion guidance for content correction, enabling efficient and intuitive 3D drag manipulation. It supports a wide range of edits including motion change, shape adjustment, and inpainting.

**Result:** Experimental outcomes confirm that 3DGS-Drag performs state-of-the-art in geometry-related 3D editing tasks across various test scenes, demonstrating its effectiveness and efficiency in real-world applications.

**Conclusion:** The method is shown to enable a broad spectrum of high-quality 3D content modifications efficiently, taking about 10 to 20 minutes per operation on a powerful GPU, marking a significant advancement in 3D content editing technology.

**Abstract:** The transformative potential of 3D content creation has been progressively unlocked through advancements in generative models. Recently, intuitive drag editing with geometric changes has attracted significant attention in 2D editing yet remains challenging for 3D scenes. In this paper, we introduce 3DGS-Drag -- a point-based 3D editing framework that provides efficient, intuitive drag manipulation of real 3D scenes. Our approach bridges the gap between deformation-based and 2D-editing-based 3D editing methods, addressing their limitations to geometry-related content editing. We leverage two key innovations: deformation guidance utilizing 3D Gaussian Splatting for consistent geometric modifications and diffusion guidance for content correction and visual quality enhancement. A progressive editing strategy further supports aggressive 3D drag edits. Our method enables a wide range of edits, including motion change, shape adjustment, inpainting, and content extension. Experimental results demonstrate the effectiveness of 3DGS-Drag in various scenes, achieving state-of-the-art performance in geometry-related 3D content editing. Notably, the editing is efficient, taking 10 to 20 minutes on a single RTX 4090 GPU.

</details>


### [9] [Sesame Plant Segmentation Dataset: A YOLO Formatted Annotated Dataset](https://arxiv.org/abs/2601.07970)
*Sunusi Ibrahim Muhammad,Ismail Ismail Tijjani,Saadatu Yusuf Jumare,Fatima Isah Jibrin*

Main category: cs.CV

> A new dataset for sesame plant segmentation was developed to enhance AI models for agricultural applications, demonstrating good performance in detection and segmentation tasks using the YOLOv8 model.

<details>
  <summary>Details</summary>

**Motivation:** To develop a specialized AI model for sesame plant detection in agricultural settings, the authors created a pixel-wise annotated dataset capturing plants at early growth stages under different conditions.

**Method:** The dataset was collected from sesame plants at early growth stages under varying environmental conditions in Nigeria and annotated using the Segment Anything Model version 2 with farmer supervision. The segmentation format allows for precise detection and analysis of plants in real-world settings.

**Result:** The evaluation of the Ultralytics YOLOv8 model using this dataset demonstrated strong performance for detection (recall of 79%, precision of 79%) and segmentation (recall of 82%, precision of 77%) tasks.

**Conclusion:** The Sesame Plant Segmentation Dataset contributes to agricultural vision research in Nigeria, with potential applications in plant monitoring, yield estimation, and agricultural research.

**Abstract:** This paper presents the Sesame Plant Segmentation Dataset, an open source annotated image dataset designed to support the development of artificial intelligence models for agricultural applications, with a specific focus on sesame plants. The dataset comprises 206 training images, 43 validation images, and 43 test images in YOLO compatible segmentation format, capturing sesame plants at early growth stages under varying environmental conditions. Data were collected using a high resolution mobile camera from farms in Jirdede, Daura Local Government Area, Katsina State, Nigeria, and annotated using the Segment Anything Model version 2 with farmer supervision. Unlike conventional bounding box datasets, this dataset employs pixel level segmentation to enable more precise detection and analysis of sesame plants in real world farm settings. Model evaluation using the Ultralytics YOLOv8 framework demonstrated strong performance for both detection and segmentation tasks. For bounding box detection, the model achieved a recall of 79 percent, precision of 79 percent, mean average precision at IoU 0.50 of 84 percent, and mean average precision from 0.50 to 0.95 of 58 percent. For segmentation, it achieved a recall of 82 percent, precision of 77 percent, mean average precision at IoU 0.50 of 84 percent, and mean average precision from 0.50 to 0.95 of 52 percent. The dataset represents a novel contribution to sesame focused agricultural vision datasets in Nigeria and supports applications such as plant monitoring, yield estimation, and agricultural research.

</details>


### [10] [An Efficient Additive Kolmogorov-Arnold Transformer for Point-Level Maize Localization in Unmanned Aerial Vehicle Imagery](https://arxiv.org/abs/2601.07975)
*Fei Li,Lang Qiao,Jiahao Fan,Yijia Xu,Shawn M. Kaeppler,Zhou Zhang*

Main category: cs.CV

> Introduces AKT for improved maize localization in high-resolution UAV imagery, achieving better performance and efficiency compared to existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The challenges in point-level maize localization in UAV imagery, including small object-to-pixel ratios, high computational costs, and scene-specific complexities, necessitate a new approach.

**Method:** Develops AKT using PKAN modules for small-object feature extraction and PAA for multiscale spatial modeling, along with a new dataset PML for testing.

**Result:** AKT outperforms state-of-the-art methods, showing higher F1-score and efficiency in computational usage and inference time, and performs well in stand counting and interplant spacing estimation.

**Conclusion:** The combination of Kolmogorov-Arnold theory with efficient attention mechanisms in AKT provides an effective solution for high-resolution agricultural remote sensing tasks.

**Abstract:** High-resolution UAV photogrammetry has become a key technology for precision agriculture, enabling centimeter-level crop monitoring and point-level plant localization. However, point-level maize localization in UAV imagery remains challenging due to (1) extremely small object-to-pixel ratios, typically less than 0.1%, (2) prohibitive computational costs of quadratic attention on ultra-high-resolution images larger than 3000 x 4000 pixels, and (3) agricultural scene-specific complexities such as sparse object distribution and environmental variability that are poorly handled by general-purpose vision models.
  To address these challenges, we propose the Additive Kolmogorov-Arnold Transformer (AKT), which replaces conventional multilayer perceptrons with Pade Kolmogorov-Arnold Network (PKAN) modules to enhance functional expressivity for small-object feature extraction, and introduces PKAN Additive Attention (PAA) to model multiscale spatial dependencies with reduced computational complexity. In addition, we present the Point-based Maize Localization (PML) dataset, consisting of 1,928 high-resolution UAV images with approximately 501,000 point annotations collected under real field conditions.
  Extensive experiments show that AKT achieves an average F1-score of 62.8%, outperforming state-of-the-art methods by 4.2%, while reducing FLOPs by 12.6% and improving inference throughput by 20.7%. For downstream tasks, AKT attains a mean absolute error of 7.1 in stand counting and a root mean square error of 1.95-1.97 cm in interplant spacing estimation. These results demonstrate that integrating Kolmogorov-Arnold representation theory with efficient attention mechanisms offers an effective framework for high-resolution agricultural remote sensing.

</details>
