<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 37]
- [cs.CV](#cs.CV) [Total: 49]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Noise or Nuance: An Investigation Into Useful Information and Filtering For LLM Driven AKBC](https://arxiv.org/abs/2509.08903)
*Alex Clay,Ernesto Jiménez-Ruiz,Pranava Madhyastha*

Main category: cs.CL

> 本文研究了在受限环境下LLM在三元组补全任务中生成质量、质量保证及响应解析的三个方面的表现，并发现额外信息能提高生成质量，LLM在过滤低质量三元组时有效，而灵活性与一致性的权衡取决于具体设置。

<details>
  <summary>Details</summary>

**Motivation:** 探讨在受限条件下，如2025年LM-KBC挑战中，LLM在生成高质量三元组方面的表现和挑战。

**Method:** 对三元组补全任务中的生成、质量保证和LLM响应解析三个方面进行研究。

**Result:** 发现额外信息能提高生成质量，LLM在过滤低质量三元组时有效，灵活性与一致性的权衡依赖于具体设置。

**Conclusion:** 在受限环境中，改善生成质量和解析关键是额外信息的利用及对灵活性与一致性权衡的理解。

**Abstract:** RAG and fine-tuning are prevalent strategies for improving the quality of LLM
outputs. However, in constrained situations, such as that of the 2025 LM-KBC
challenge, such techniques are restricted. In this work we investigate three
facets of the triple completion task: generation, quality assurance, and LLM
response parsing. Our work finds that in this constrained setting: additional
information improves generation quality, LLMs can be effective at filtering
poor quality triples, and the tradeoff between flexibility and consistency with
LLM response parsing is setting dependent.

</details>


### [2] [Automated Evidence Extraction and Scoring for Corporate Climate Policy Engagement: A Multilingual RAG Approach](https://arxiv.org/abs/2509.08907)
*Imene Kolli,Ario Saeid Vaghefi,Chiara Colesanti Senni,Shantam Raj,Markus Leippold*

Main category: cs.CL

> 本研究提出一种基于RAG的AI辅助框架来加速从大量文本数据中提取相关证据的过程，以自动化检测公司气候政策参与度。研究表明该框架在提取和分类多语言企业文档中的证据时表现出色，但为了避免错误，仍需与人工专家判断结合使用。

<details>
  <summary>Details</summary>

**Motivation:** 尽管InfluenceMap在自动化关键分析工作流程方面取得了一定进展，但仍然有很大一部分评估工作是手动完成的，这使得过程耗时且容易出错。因此，本研究旨在通过自动化手段减轻人工负担并提高效率和准确性。

**Method:** 本研究提出了一种基于检索增强生成（Retrieval-Augmented Generation，RAG）的AI辅助框架，旨在加速从大量文本数据中提取相关证据的过程，以自动化监测公司气候政策参与度的最耗时部分。研究采用布局感知解析（layout-aware parsing）、Nomic嵌入模型及少样本提示策略相结合的方法，以在多语言企业文件中提取和分类证据。

**Result:** 研究评估表明，布局感知解析、Nomic嵌入模型及少样本提示策略相结合的组合方案，在从多语言企业文档中提取和分类证据上表现最佳。

**Conclusion:** 研究结果证明，RAG系统能有效加速证据提取，但鉴于分析内容的复杂性，必须采用人机协同的模式，即技术辅助但不完全替代专家判断，以保证分析的准确性。

**Abstract:** InfluenceMap's LobbyMap Platform monitors the climate policy engagement of
over 500 companies and 250 industry associations, assessing each entity's
support or opposition to science-based policy pathways for achieving the Paris
Agreement's goal of limiting global warming to 1.5{\deg}C. Although
InfluenceMap has made progress with automating key elements of the analytical
workflow, a significant portion of the assessment remains manual, making it
time- and labor-intensive and susceptible to human error. We propose an
AI-assisted framework to accelerate the monitoring of corporate climate policy
engagement by leveraging Retrieval-Augmented Generation to automate the most
time-intensive extraction of relevant evidence from large-scale textual data.
Our evaluation shows that a combination of layout-aware parsing, the Nomic
embedding model, and few-shot prompting strategies yields the best performance
in extracting and classifying evidence from multilingual corporate documents.
We conclude that while the automated RAG system effectively accelerates
evidence extraction, the nuanced nature of the analysis necessitates a
human-in-the-loop approach where the technology augments, rather than replaces,
expert judgment to ensure accuracy.

</details>


### [3] [Documents Are People and Words Are Items: A Psychometric Approach to Textual Data with Contextual Embeddings](https://arxiv.org/abs/2509.08920)
*Jinsong Chen*

Main category: cs.CL

> A new psychometric method uses large language models to generate contextual scores and factor analysis to analyze textual data, demonstrating success in uncovering latent knowledge dimensions in the Wiki STEM corpus.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to enhance the psychometric analysis of textual data by introducing a novel method that leverages large language models and contextual embeddings, which can more effectively differentiate documents within a corpus compared to traditional methods.

**Method:** The paper introduces a method that uses large language models and contextual embeddings to convert textual data into psychometrically interpretable form. It consists of two stages: generating contextual scores using NLP techniques and transformer models, and performing factor analysis to identify latent factors.

**Result:** When applied to the Wiki STEM corpus, the model successfully uncovered latent knowledge dimensions and patterns in the textual data, highlighting its potential in fields like education, psychology, and law.

**Conclusion:** The research concludes that the proposed method is effective in uncovering latent factors and patterns within textual data, providing enhanced tools for psychometric analysis, and has broader application potential in various fields with substantial textual data.

**Abstract:** This research introduces a novel psychometric method for analyzing textual
data using large language models. By leveraging contextual embeddings to create
contextual scores, we transform textual data into response data suitable for
psychometric analysis. Treating documents as individuals and words as items,
this approach provides a natural psychometric interpretation under the
assumption that certain keywords, whose contextual meanings vary significantly
across documents, can effectively differentiate documents within a corpus. The
modeling process comprises two stages: obtaining contextual scores and
performing psychometric analysis. In the first stage, we utilize natural
language processing techniques and encoder based transformer models to identify
common keywords and generate contextual scores. In the second stage, we employ
various types of factor analysis, including exploratory and bifactor models, to
extract and define latent factors, determine factor correlations, and identify
the most significant words associated with each factor. Applied to the Wiki
STEM corpus, our experimental results demonstrate the method's potential to
uncover latent knowledge dimensions and patterns within textual data. This
approach not only enhances the psychometric analysis of textual data but also
holds promise for applications in fields rich in textual information, such as
education, psychology, and law.

</details>


### [4] [BRoverbs -- Measuring how much LLMs understand Portuguese proverbs](https://arxiv.org/abs/2509.08960)
*Thales Sales Almeida,Giovana Kerche Bonás,João Guilherme Alves Santos*

Main category: cs.CL

> 为了解决葡萄牙语LLMs评估中存在的区域化差距，本文提出了BRoverbs数据集，专注于通过巴西谚语来评估模型的语言和文化理解能力。

<details>
  <summary>Details</summary>

**Motivation:** 当前针对葡萄牙语的评估框架存在不足，许多依赖翻译后的数据集，无法全面捕捉语言的细微差别和文化指涉。此外，现有的葡萄牙语数据集主要集中在结构化的国家级考试或社交媒体互动的情感分析上，未能全面评估更广泛的语言理解能力。

**Method:** 本文介绍了一种名为BRoverbs的新数据集，该数据集专门用于评估LLMs对巴西谚语的理解能力。通过使用富含文化智慧和复杂句法结构的谚语来评估模型，BRoverbs旨在成为一个新的评估工具，以促进对葡萄牙语区模型基准测试的发展。

**Result:** BRoverbs数据集的发布填补了基于巴西谚语评估葡萄牙语LLMs能力的空白，并推动了区域化基准测试的进步。

**Conclusion:** BRoverbs作为一个新的评估工具，通过使用富含文化内涵和复杂结构的巴西谚语，为改进葡萄牙语LLMs的评估提供了宝贵经验，并推动了针对具体地域文化的评估框架的发展。

**Abstract:** Large Language Models (LLMs) exhibit significant performance variations
depending on the linguistic and cultural context in which they are applied.
This disparity signals the necessity of mature evaluation frameworks that can
assess their capabilities in specific regional settings. In the case of
Portuguese, existing evaluations remain limited, often relying on translated
datasets that may not fully capture linguistic nuances or cultural references.
Meanwhile, native Portuguese-language datasets predominantly focus on
structured national exams or sentiment analysis of social media interactions,
leaving gaps in evaluating broader linguistic understanding. To address this
limitation, we introduce BRoverbs, a dataset specifically designed to assess
LLM performance through Brazilian proverbs. Proverbs serve as a rich linguistic
resource, encapsulating cultural wisdom, figurative expressions, and complex
syntactic structures that challenge the model comprehension of regional
expressions. BRoverbs aims to provide a new evaluation tool for
Portuguese-language LLMs, contributing to advancing regionally informed
benchmarking. The benchmark is available at
https://huggingface.co/datasets/Tropic-AI/BRoverbs.

</details>


### [5] [Can Vision-Language Models Solve Visual Math Equations?](https://arxiv.org/abs/2509.09013)
*Monjoy Narayan Choudhury,Junling Wang,Yifan Hou,Mrinmaya Sachan*

Main category: cs.CL

> 本文通过视觉方程求解任务发现，当前的视觉语言模型在计数等视觉符号推理任务上的表现不佳，并揭示了其在未来改进的方向。

<details>
  <summary>Details</summary>

**Motivation:** 本文研究的动机是探讨VLMs在需要整合感知和符号计算的任务上的局限性。通过视觉方程求解任务，试图理解VLMs在处理视觉和语言结合的任务时存在的问题。

**Method:** 本文通过研究视觉方程求解任务来分析视觉语言模型（VLMs）的局限性。在这个任务中，数学方程嵌入在图像中，变量由物体图标表示，系数必须通过计数来推断。

**Result:** 研究结果表明，虽然VLMs在处理文本方程时表现良好，但在处理图像嵌入方程时却表现不佳。分解任务发现，即使变量识别准确，计数依然是主要瓶颈。此外，识别和推理的组合引入了额外的误差。

**Conclusion:** 研究发现，计数是主要瓶颈，即使在识别准确的情况下也是如此。随着方程复杂性的增加，符号推理本身也成为限制因素。这些发现揭示了当前VLMs的关键弱点，并指出了未来在视觉数学推理改进的方向。

**Abstract:** Despite strong performance in visual understanding and language-based
reasoning, Vision-Language Models (VLMs) struggle with tasks requiring
integrated perception and symbolic computation. We study this limitation
through visual equation solving, where mathematical equations are embedded in
images, variables are represented by object icons, and coefficients must be
inferred by counting. While VLMs perform well on textual equations, they fail
on visually grounded counterparts. To understand this gap, we decompose the
task into coefficient counting and variable recognition, and find that counting
is the primary bottleneck, even when recognition is accurate. We also observe
that composing recognition and reasoning introduces additional errors,
highlighting challenges in multi-step visual reasoning. Finally, as equation
complexity increases, symbolic reasoning itself becomes a limiting factor.
These findings reveal key weaknesses in current VLMs and point toward future
improvements in visually grounded mathematical reasoning.

</details>


### [6] [Stated Preference for Interaction and Continued Engagement (SPICE): Evaluating an LLM's Willingness to Re-engage in Conversation](https://arxiv.org/abs/2509.09043)
*Thomas Manuel Rost,Martina Figlia,Bernd Wallraff*

Main category: cs.CL

> 本研究采用SPICE方法测试了4个大规模语言模型的互动意愿，发现模型能够明显区分并偏好友好而非辱骂的互动，并揭示了在模糊情境下，描述研究背景的导论对SPICE的影响。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于评估一种简单的诊断信号SPICE，以此来检查大规模语言模型对用户行为的反应及其持续互动的意愿，以期开发一种可复制的工具用于审核模型倾向。

**Method:** 本研究通过使用SPICE方法，即通过询问大规模语言模型在回顾简短对话记录后是否愿意再次与用户互动的问题（YES或NO），来诊断模型对用户行为的反应。研究使用了由3种语气（友好、模糊、辱骂）和10种互动组成的刺激集，测试了四个开放权重聊天模型在四种情境条件下的反应，共进行了480次试验。

**Result:** 研究发现SPICE能够明显区分用户语气，友好互动几乎一致倾向于继续（97.5% YES），而辱骂互动倾向于终止（17.9% YES），模糊语氧行为位于两者之间（60.4% YES）。多个依赖关系统计检验（包括Rao-Scott调整和集群置换检验）验证了这一核心关联。此外，当模型未能识别辱骂时，仍强烈倾向于终止互动（81%）。探索性分析还发现了一个显著的交互效果，描述研究背景的导论在呈现为单块文本时对模糊情况下SPICE的影响更为显著。

**Conclusion:** 研究结果验证了SPICE作为一种稳健、低开销和可复制的工具的价值，它提供了一个直接而关系性的信号，显示了模型的状态，不仅补充了现有的评估指标，还提供了一个独特的信号，区别于虐待分类。所有刺激材料、代码及分析脚本均公布了，支持实验复制。

**Abstract:** We introduce and evaluate Stated Preference for Interaction and Continued
Engagement (SPICE), a simple diagnostic signal elicited by asking a Large
Language Model a YES or NO question about its willingness to re-engage with a
user's behavior after reviewing a short transcript. In a study using a 3-tone
(friendly, unclear, abusive) by 10-interaction stimulus set, we tested four
open-weight chat models across four framing conditions, resulting in 480
trials. Our findings show that SPICE sharply discriminates by user tone.
Friendly interactions yielded a near-unanimous preference to continue (97.5%
YES), while abusive interactions yielded a strong preference to discontinue
(17.9% YES), with unclear interactions falling in between (60.4% YES). This
core association remains decisive under multiple dependence-aware statistical
tests, including Rao-Scott adjustment and cluster permutation tests.
Furthermore, we demonstrate that SPICE provides a distinct signal from abuse
classification. In trials where a model failed to identify abuse, it still
overwhelmingly stated a preference not to continue the interaction (81% of the
time). An exploratory analysis also reveals a significant interaction effect: a
preamble describing the study context significantly impacts SPICE under
ambiguity, but only when transcripts are presented as a single block of text
rather than a multi-turn chat. The results validate SPICE as a robust,
low-overhead, and reproducible tool for auditing model dispositions,
complementing existing metrics by offering a direct, relational signal of a
model's state. All stimuli, code, and analysis scripts are released to support
replication.

</details>


### [7] [Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M](https://arxiv.org/abs/2509.09055)
*Piyush Pant*

Main category: cs.CL

> 研究通过比较SFT和DPO方法，发现结合这两种方法在提升OPT-350M模型的安全性和有用性方面最为有效，并且探索了模型对齐面临的主要挑战。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于理解如何利用不同对齐技术来提高语言模型的安全性和有用性，并通过实验验证这些方法的效果。

**Method:** 此研究考察了对齐技术的有效性，包括监督微调（SFT）、直接偏好优化（DPO）以及结合SFT和DPO的方法，以改进OPT-350M语言模型的安全性和有用性。研究中使用了Anthropic Helpful-Harmless RLHF数据集来训练并评估了四个模型：基础的OPT350M，SFT模型，DPO模型，以及同时采用了SFT和DPO训练的模型。

**Result:** 研究结果显示，虽然SFT在某些方面优于DPO，但结合SFT和DPO的方法在所有评估指标下都优于其他模型，表明了这两种技术的互补性。此外，研究还提到了数据噪声，有限的GPU资源和训练限制带来的挑战。

**Conclusion:** 研究提供了关于微调策略如何影响模型对齐的全面观点，并为未来更加稳健的对齐管道奠定了基础。

**Abstract:** This research investigates the effectiveness of alignment techniques,
Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a
combined SFT+DPO approach on improving the safety and helpfulness of the
OPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset,
we train and evaluate four models: the base OPT350M, an SFT model, a DPO model,
and a model trained with both SFT and DPO. We introduce three key evaluation
metrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined
Alignment Score (CAS), all derived from reward model outputs. The results show
that while SFT outperforms DPO, The combined SFT+DPO model outperforms all
others across all metrics, demonstrating the complementary nature of these
techniques. Our findings also highlight challenges posed by noisy data, limited
GPU resources, and training constraints. This study offers a comprehensive view
of how fine-tuning strategies affect model alignment and provides a foundation
for more robust alignment pipelines in future work.

</details>


### [8] [MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for Universal Information Extraction](https://arxiv.org/abs/2509.09082)
*Zhongqiu Li,Shiquan Wang,Ruiyu Fang,Mengjiao Bao,Zhenhe Wu,Shuangyong Song,Yongxiang Li,Zhongjiang He*

Main category: cs.CL

> 提出一种结合强化学习与多视角推理（MR-UIE）的方法以提升大语言模型的信息抽取能力，实验证明其在多个基准测试上优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 当前大语言模型在通用信息抽取（UIE）上的性能不足，尤其是在需要复杂模式描述和多步推理的任务中。尽管现有技术通过上下文学习和指令微调提升模型性能，但仍有显著限制。

**Method:** 通过结合强化学习（RL）与多视角推理来增强信息抽取任务中的模型泛化能力，使大语言模型（LLM）从被动的抽取工具转变为能够主动推理的工具。

**Result:** 在多个信息抽取基准测试中的实验表明，MR-UIE 在跨域的信息抽取精度上均有提升，并在多个数据集上超越了当前最先进的方法。

**Conclusion:** 结合多视角推理的强化学习显著提高了复杂信息抽取任务的泛化能力，强调了在复杂场景中推理的重要作用。

**Abstract:** Large language models (LLMs) demonstrate robust capabilities across diverse
research domains. However, their performance in universal information
extraction (UIE) remains insufficient, especially when tackling structured
output scenarios that involve complex schema descriptions and require
multi-step reasoning. While existing approaches enhance the performance of LLMs
through in-context learning and instruction tuning, significant limitations
nonetheless persist. To enhance the model's generalization ability, we propose
integrating reinforcement learning (RL) with multi-perspective reasoning for
information extraction (IE) tasks. Our work transitions LLMs from passive
extractors to active reasoners, enabling them to understand not only what to
extract but also how to reason. Experiments conducted on multiple IE benchmarks
demonstrate that MR-UIE consistently elevates extraction accuracy across
domains and surpasses state-of-the-art methods on several datasets.
Furthermore, incorporating multi-perspective reasoning into RL notably enhances
generalization in complex IE tasks, underscoring the critical role of reasoning
in challenging scenarios.

</details>


### [9] [TigerCoder: A Novel Suite of LLMs for Code Generation in Bangla](https://arxiv.org/abs/2509.09101)
*Nishat Raihan,Antonios Anastasopoulos,Marcos Zampieri*

Main category: cs.CL

> 研究介绍了一种针对孟加拉语代码生成的大型语言模型(TigerCoder)，显著提高了低资源语言的模型性能，性能提升约为11-18%。

<details>
  <summary>Details</summary>

**Motivation:** 孟加拉语作为世界上第五大语言，在代码生成的大型语言模型（LLM）方面仍旧代表性不足，这主要是由于缺乏用于预训练或微调此类模型的高质量数据。因此，研究旨在解决这一问题。

**Method:** 我们提出了第一个专门针对孟加拉语的代码生成LLM家族（10亿和90亿参数模型）。主要贡献包括：（1）为编程领域适应而设计的综合性孟加拉语代码指令数据集；（2）评估孟加拉语代码生成的MBPP-Bangla基准测试；（3）TigerCoder系列代码LLM，在Pass@1性能上比现有的多语言和通用孟加拉语LLM高出约11-18%。

**Result:** 研究结果表明，精心策划的高质量数据集能够弥补小规模模型在低资源语言上的局限。

**Conclusion:** 研究开源了所有资源，以促进孟加拉语LLM领域的进一步研究。

**Abstract:** Despite being the 5th most spoken language, Bangla remains underrepresented
in Large Language Models (LLMs), particularly for code generation. This
primarily stems from the scarcity of high-quality data to pre-train and/or
finetune such models. Hence, we introduce the first dedicated family of Code
LLMs for Bangla (1B & 9B). We offer three major contributions: (1) a
comprehensive Bangla code instruction datasets for programming domain
adaptation; (2) MBPP-Bangla, an evaluation benchmark for Bangla code
generation; and (3) the TigerCoder-family of Code LLMs, achieving significant
~11-18% performance gains at Pass@1 over existing multilingual and
general-purpose Bangla LLMs. Our findings show that curated, high-quality
datasets can overcome limitations of smaller models for low-resource languages.
We open-source all resources to advance further Bangla LLM research.

</details>


### [10] [Compass-v3: Scaling Domain-Specific LLMs for Multilingual E-Commerce in Southeast Asia](https://arxiv.org/abs/2509.09121)
*Sophia Maria*

Main category: cs.CL

> 本文介绍了一个针对东南亚电子商务的垂直领域Mixture-of-Experts模型Compass-v3，该模型在大规模多语言语料和大规模电商指令上进行了训练，提出了一种新的对齐优化方法OTPO，模型在电商具体任务上表现出色，已经广泛应用于实际电商平台。

<details>
  <summary>Details</summary>

**Motivation:** 大语言模型虽然在通用领域表现出色，但它们在需要特定领域知识的特定任务上的表现通常会下降。电子商务领域尤其具有挑战性，其数据具有噪声大、异构性、多语言和动态性强的特性。因此，需要一种专门针对电子商务领域的模型来解决这些问题。

**Method:** 本文提出了Compass-v3模型，该模型是一个针对东南亚电子商务领域的2450亿参数的专家混合模型（MoE），每个令牌有710亿个活跃参数。该模型通过采用更少但更大的专家，并结合硬件高效的优化策略来提高GPU的利用率，如节点内专家并行和定制化memcpy操作。为了提高对齐性，提出了一种称为OTPO的方法，即最优传输直接偏好优化，这种方法可以捕捉到令牌级别的差异，并提高指令在商业场景中的遵循度。

**Result:** 通过广泛的评估表明，Compass-v3在电子商务领域的表现达到了最先进的水平，超越了DeepSeek-V3.1、GPT-4系列和Qwen3-235B。并且，在低资源的东南亚语言（印度尼西亚语、泰语、菲律宾语、越南语、马来语和塔加洛语）和葡萄牙语中展现出强大的多语言能力，同时在通用基准测试中保持竞争力。

**Conclusion:** Compass-v3已经在Shopee的工业规模电子商务平台上广泛应用，并逐渐取代OpenAI的流量，已占用超过70%的总LLM使用份额，这表明其在专门的商业专业知识和广泛的语言能力方面都表现出色。

**Abstract:** Large language models (LLMs) excel in general-domain applications, yet their
performance often degrades in specialized tasks requiring domain-specific
knowledge. E-commerce is particularly challenging, as its data are noisy,
heterogeneous, multilingual, and highly dynamic. We present Compass-v3, a
vertical-domain Mixture-of-Experts (MoE) model with 245B total parameters and
71B active per token, designed for Southeast Asian e-commerce. Compass-v3
adopts fewer but larger experts, combined with hardware-efficient
optimizations-such as intra-node expert parallelism and a customized memcpy
operator-to maximize GPU utilization. The model is trained on 12T tokens of
curated multilingual corpora and large-scale synthetic e-commerce instructions
using a mixed-training strategy. To enhance alignment, we propose
Optimal-Transport Direct Preference Optimization (OTPO), which captures
token-level distinctions and improves instruction adherence in
commerce-specific scenarios. Extensive evaluations demonstrate that Compass-v3
delivers state-of-the-art e-commerce performance, surpassing DeepSeek-V3.1,
GPT-4 series, and Qwen3-235B. Moreover, Compass-v3 demonstrates strong
multilingual capability across low-resource Southeast Asian languages
(Indonesian, Thai, Filipino, Vietnamese, Malay, Taglog) and Portuguese while
sustaining competitive performance on general benchmarks. It has already been
widely applied in Shopee's industrial-scale e-commerce platform and is
gradually replacing OpenAI's traffic, now accounting for over 70\% of total LLM
usage, highlighting its dual strengths in specialized commerce expertise and
broad linguistic competence.

</details>


### [11] [Automated Classification of Tutors' Dialogue Acts Using Generative AI: A Case Study Using the CIMA Corpus](https://arxiv.org/abs/2509.09125)
*Liqun He,Jiaqi Xu*

Main category: cs.CL

> 本研究评估了生成式AI用于教育对话分析中分类教师对话行为的能力，显示了其替代手动编码的强大潜力。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在探讨使用生成式AI自动生成教师对话行为（DAs）分类的可行性，以减少传统手动编码所需的时间和努力。

**Method:** 本研究使用CIMA开源语料库，测试了GPT-3.5-turbo和GPT-4模型在通过定制提示词进行分类方面的表现。

**Result:** 研究结果显示，GPT-4达到了80%的准确率，加权F1评分为0.81，Cohen's Kappa值为0.74，超过了基线性能，并且与人工标注有高度一致性。

**Conclusion:** 这一研究结果表明，生成式AI有很强的潜力提供一个高效且易于使用的DA分类方法，对教育对话分析具有重要意义。它还强调了任务特定标签定义和上下文信息对于提高自动化标注质量的重要性，以及负责任和透明的研究实践需要关注的问题。

**Abstract:** This study explores the use of generative AI for automating the
classification of tutors' Dialogue Acts (DAs), aiming to reduce the time and
effort required by traditional manual coding. This case study uses the
open-source CIMA corpus, in which tutors' responses are pre-annotated into four
DA categories. Both GPT-3.5-turbo and GPT-4 models were tested using tailored
prompts. Results show that GPT-4 achieved 80% accuracy, a weighted F1-score of
0.81, and a Cohen's Kappa of 0.74, surpassing baseline performance and
indicating substantial agreement with human annotations. These findings suggest
that generative AI has strong potential to provide an efficient and accessible
approach to DA classification, with meaningful implications for educational
dialogue analysis. The study also highlights the importance of task-specific
label definitions and contextual information in enhancing the quality of
automated annotation. Finally, it underscores the ethical considerations
associated with the use of generative AI and the need for responsible and
transparent research practices. The script of this research is publicly
available at
https://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging.

</details>


### [12] [ViRanker: A BGE-M3 & Blockwise Parallel Transformer Cross-Encoder for Vietnamese Reranking](https://arxiv.org/abs/2509.09131)
*Phuong-Nam Dang,Kieu-Linh Nguyen,Thanh-Hieu Pham*

Main category: cs.CL

> ViRanker, a specialized reranking model for Vietnamese, demonstrates strong performance and competitiveness against baselines on the MMARCO-VI benchmark, suggesting broader implications for low-resource languages.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this paper is to address the scarcity of competitive rerankers for Vietnamese, acknowledging its low-resource status and complex linguistic features.

**Method:** This paper introduces ViRanker, a model specifically designed for reranking in Vietnamese, using the BGE-M3 encoder and Blockwise Parallel Transformer. The model was trained on an 8 GB curated dataset and fine-tuned using hybrid hard-negative sampling.

**Result:** The evaluation on the MMARCO-VI benchmark shows that ViRanker outperforms multilingual baselines and performs comparably with PhoRanker in terms of early-rank accuracy.

**Conclusion:** ViRanker's performance highlights the potential of tailored architectural modifications and rigorous data preparation in advancing reranking technologies for underrepresented languages, beyond just Vietnamese.

**Abstract:** This paper presents ViRanker, a cross-encoder reranking model tailored to the
Vietnamese language. Built on the BGE-M3 encoder and enhanced with the
Blockwise Parallel Transformer, ViRanker addresses the lack of competitive
rerankers for Vietnamese, a low-resource language with complex syntax and
diacritics. The model was trained on an 8 GB curated corpus and fine-tuned with
hybrid hard-negative sampling to strengthen robustness. Evaluated on the
MMARCO-VI benchmark, ViRanker achieves strong early-rank accuracy, surpassing
multilingual baselines and competing closely with PhoRanker. By releasing the
model openly on Hugging Face, we aim to support reproducibility and encourage
wider adoption in real-world retrieval systems. Beyond Vietnamese, this study
illustrates how careful architectural adaptation and data curation can advance
reranking in other underrepresented languages.

</details>


### [13] [LITcoder: A General-Purpose Library for Building and Comparing Encoding Models](https://arxiv.org/abs/2509.09152)
*Taha Binhuraib,Ruimin Gao,Anna A. Ivanova*

Main category: cs.CL

> LITcoder是用于构建和基准测试神经编码模型的开源库，旨在降低技术门槛，促进系统比较，推动方法学严谨性，并加速高质量大脑活动预测模型的发展。

<details>
  <summary>Details</summary>

**Motivation:** LITcoder旨在降低编码模型实现的技术门槛，促进模型和数据集之间的系统比较，促进方法学严谨性，加速高质量高性能的大脑活动预测模型的发展。

**Method:** LITcoder是一个开源库，用于构建和基准化神经编码模型。它提供了一套标准化的工具，用于对齐连续刺激（如文本和语音）与大脑数据、将刺激转化为表示特征、将特征映射到大脑数据上、并对结果模型在保留数据上的预测性能进行评估。该库实现了覆盖大量方法设计选择的模块化管道，让研究人员可以轻松地组合、比较和扩展编码模型，而无需重新发明核心基础设施。此外，该库还提供了内置的日志记录、绘图功能以及实验跟踪平台（如Weights & Biases）的无缝集成。

**Result:** LITcoder为三个故事聆听数据集（LeBeletal.（2023），Narratives和LittlePrince）适配了一系列编码模型，展示了其框架的可扩展性和多功能性。研究还探讨了构建连续fMRI数据编码模型的关键方法选择，如所有tokens在TR扫描中的考虑、结合血流动力学滞后效应、使用最小化信息泄漏的训练-测试划分，以及考虑头部运动对编码模型预测性的影响。

**Conclusion:** 通过LITcoder，研究人员可以有效地实现编码模型，并进行系统的比较和扩展，进而推动高质量大脑活动预测模型的发展。

**Abstract:** We introduce LITcoder, an open-source library for building and benchmarking
neural encoding models. Designed as a flexible backend, LITcoder provides
standardized tools for aligning continuous stimuli (e.g., text and speech) with
brain data, transforming stimuli into representational features, mapping those
features onto brain data, and evaluating the predictive performance of the
resulting model on held-out data. The library implements a modular pipeline
covering a wide array of methodological design choices, so researchers can
easily compose, compare, and extend encoding models without reinventing core
infrastructure. Such choices include brain datasets, brain regions, stimulus
feature (both neural-net-based and control, such as word rate), downsampling
approaches, and many others. In addition, the library provides built-in
logging, plotting, and seamless integration with experiment tracking platforms
such as Weights & Biases (W&B). We demonstrate the scalability and versatility
of our framework by fitting a range of encoding models to three story listening
datasets: LeBel et al. (2023), Narratives, and Little Prince. We also explore
the methodological choices critical for building encoding models for continuous
fMRI data, illustrating the importance of accounting for all tokens in a TR
scan (as opposed to just taking the last one, even when contextualized),
incorporating hemodynamic lag effects, using train-test splits that minimize
information leakage, and accounting for head motion effects on encoding model
predictivity. Overall, LITcoder lowers technical barriers to encoding model
implementation, facilitates systematic comparisons across models and datasets,
fosters methodological rigor, and accelerates the development of high-quality
high-performance predictive models of brain activity.
  Project page: https://litcoder-brain.github.io

</details>


### [14] [Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing](https://arxiv.org/abs/2509.09160)
*Zhiyue Liu,Fanrong Ma,Xin Ling*

Main category: cs.CL

> A counterfactual-enhanced debiasing framework is introduced to improve target-oriented multimodal sentiment classification by reducing spurious correlations in image-text pairs.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the over-reliance on textual content in existing methods and the issue of spurious correlations due to dataset biases, aiming to improve classification accuracy.

**Method:** Target-oriented multimodal sentiment classification method proposed involves a counterfactual-enhanced debiasing framework. This includes a data augmentation strategy that minimally alters sentiment-related causal features, and an adaptive debiasing contrastive learning mechanism to mitigate the influence of biased words.

**Result:** Experiments show that the proposed method outperforms state-of-the-art baselines on several benchmark datasets.

**Conclusion:** The paper concludes by highlighting the effectiveness of the proposed counterfactual-enhanced debiasing framework in reducing spurious correlations and improving the robustness of sentiment classification models.

**Abstract:** Target-oriented multimodal sentiment classification seeks to predict
sentiment polarity for specific targets from image-text pairs. While existing
works achieve competitive performance, they often over-rely on textual content
and fail to consider dataset biases, in particular word-level contextual
biases. This leads to spurious correlations between text features and output
labels, impairing classification accuracy. In this paper, we introduce a novel
counterfactual-enhanced debiasing framework to reduce such spurious
correlations. Our framework incorporates a counterfactual data augmentation
strategy that minimally alters sentiment-related causal features, generating
detail-matched image-text samples to guide the model's attention toward content
tied to sentiment. Furthermore, for learning robust features from
counterfactual data and prompting model decisions, we introduce an adaptive
debiasing contrastive learning mechanism, which effectively mitigates the
influence of biased words. Experimental results on several benchmark datasets
show that our proposed method outperforms state-of-the-art baselines.

</details>


### [15] [EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs](https://arxiv.org/abs/2509.09174)
*Yuhao Zhang,Yuhao Du,Zhanchen Dai,Xiangnan Ma,Kaiqi Kou,Benyou Wang,Haizhou Li*

Main category: cs.CL

> 本研究提出了EchoX，一种旨在克服SLLMs知识和推理能力退化问题的方法，通过在训练过程中整合声学和语义学习，实验显示其在知识问答任务上表现优异。

<details>
  <summary>Details</summary>

**Motivation:** 当前SLLMs（语音到语音的大规模语言模型）的训练范式无法很好地弥合声学-语义间隙，导致其在知识和推理能力方面出现退化。

**Method:** 我们提出了一种名为EchoX的方法，该方法利用语义表示并动态生成语音训练目标，从而在训练过程中同时整合了声学和语义学习。

**Result:** 实验结果表明，EchoX利用大约六千小时的训练数据，在多个基于知识的问题解答基准测试上表现出了先进的性能。

**Conclusion:** EchoX能很好地保留其作为语音LLM的强推理能力，证明了在训练方法中整合声学与语义对于提高SLLMs的性能是有效的。

**Abstract:** Speech-to-speech large language models (SLLMs) are attracting increasing
attention. Derived from text-based large language models (LLMs), SLLMs often
exhibit degradation in knowledge and reasoning capabilities. We hypothesize
that this limitation arises because current training paradigms for SLLMs fail
to bridge the acoustic-semantic gap in the feature representation space. To
address this issue, we propose EchoX, which leverages semantic representations
and dynamically generates speech training targets. This approach integrates
both acoustic and semantic learning, enabling EchoX to preserve strong
reasoning abilities as a speech LLM. Experimental results demonstrate that
EchoX, with about six thousand hours of training data, achieves advanced
performance on multiple knowledge-based question-answering benchmarks. The
project is available at https://github.com/FreedomIntelligence/EchoX.

</details>


### [16] [Efficient Trie-based Biasing using K-step Prediction for Rare Word Recognition](https://arxiv.org/abs/2509.09196)
*Chin Yuen Kwok,Jia Qi yip*

Main category: cs.CL

> 本文通过调整ASR模型以预测多个步骤来改进稀有词识别，减少了计算要求并显著提高了Whisper模型在特定测试集上的性能。

<details>
  <summary>Details</summary>

**Motivation:** 当前的Trie-Based偏置方法在处理稀有词时虽然有效，但在束搜索中撤销奖励步骤是计算密集型的，特别是对于具有大型解码器的模型。本研究旨在克服这种局限性。

**Method:** 本研究提出了一种新的方法，通过调整ASR模型，使其能够向前看并一次性预测多个步骤，这样可以避免撤销之前奖励的步骤，从而更好地估计部分假设是否会导致生成整个稀有词。

**Result:** 通过使用仅10小时的合成数据对Whisper模型进行微调，该方法在NSC Part 2测试集上的单词错误率从30.86%降低到了12.19%。

**Conclusion:** 该方法有效提升了稀有词的识别准确率，降低了计算成本，证明了在ASR中适应模型向前预测策略的价值。

**Abstract:** Contextual biasing improves rare word recognition of ASR models by
prioritizing the output of rare words during decoding. A common approach is
Trie-based biasing, which gives "bonus scores" to partial hypothesis (e.g.
"Bon") that may lead to the generation of the rare word (e.g. "Bonham"). If the
full word ("Bonham") isn't ultimately recognized, the system revokes those
earlier bonuses. This revocation is limited to beam search and is
computationally expensive, particularly for models with large decoders. To
overcome these limitations, we propose adapting ASR models to look ahead and
predict multiple steps at once. This avoids the revocation step entirely by
better estimating whether a partial hypothesis will lead to the generation of
the full rare word. By fine-tuning Whisper with only 10 hours of synthetic
data, our method reduces the word error rate on the NSC Part 2 test set from
30.86% to 12.19%.

</details>


### [17] [Improving Synthetic Data Training for Contextual Biasing Models with a Keyword-Aware Cost Function](https://arxiv.org/abs/2509.09197)
*Chin Yuen Kwok,Jia Qi Yip,Eng Siong Chng*

Main category: cs.CL

> 文章提出了一种关键字感知的损失函数来改进稀有词汇在ASR模型中的识别效果，通过合成数据训练和上下文偏置模块来提高稀有词汇的识别率，显著降低了词错误率。

<details>
  <summary>Details</summary>

**Motivation:** 改进稀有词汇识别，特别是通过将其集成到模型架构中的上下文偏置模块来优先处理稀有词汇。发现使用合成稀有词汇数据训练模块比使用非稀有词汇数据更有效，但可能导致过度拟合。

**Method:** 通过增强基于TCPGen的上下文偏置方法，并提出了一种关键字感知的损失函数，该函数在训练偏置模块时专注于偏置词。该损失函数包括一个用于预测偏置词的掩码交叉熵项和一个用于检测偏置词位置的二元分类项。这两个部分互补地支持在推理过程中解码偏置词。

**Result:** 将Whisper适应到10小时的合成数据后，该方法将NSC Part 2测试集上的词错误率从29.71%降低到11.81%。

**Conclusion:** 实验结果表明，使用关键字感知损失函数进行训练，可以有效减少稀有词汇的识别错误率，从而提高整体的语音识别性能。

**Abstract:** Rare word recognition can be improved by adapting ASR models to synthetic
data that includes these words. Further improvements can be achieved through
contextual biasing, which trains and adds a biasing module into the model
architecture to prioritize rare words. While training the module on synthetic
rare word data is more effective than using non-rare-word data, it can lead to
overfitting due to artifacts in the synthetic audio. To address this, we
enhance the TCPGen-based contextual biasing approach and propose a
keyword-aware loss function that additionally focuses on biased words when
training biasing modules. This loss includes a masked cross-entropy term for
biased word prediction and a binary classification term for detecting biased
word positions. These two terms complementarily support the decoding of biased
words during inference. By adapting Whisper to 10 hours of synthetic data, our
method reduced the word error rate on the NSC Part 2 test set from 29.71% to
11.81%.

</details>


### [18] [GmSLM : Generative Marmoset Spoken Language Modeling](https://arxiv.org/abs/2509.09198)
*Talia Sternberg,Michael London,David Omer,Yossi Adi*

Main category: cs.CL

> 研究团队提出了一种针对狨猴叫声的语言模型GmSLM，该模型能够生成与真实叫声非常匹配的叫声，并且在识别真实的与人工的对话中表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 研究狨猴的叫声沟通方式提供了一个将叫声沟通与大脑活动联系起来的独特机会，尤其是在人类大脑难以在言语和语言研究中被访问的情况下。由于狨猴主要通过叫声进行沟通，因此应用标准的语言模型方法并不直接适用。

**Method:** 提出了一个优化的针对狨猴叫声的语言模型管道（GmSLM），并设计了一个新颖的零样本评估指标，使用无监督的野外数据和弱标记的对话数据来评估GmSLM。

**Result:** GmSLM生成的叫声在声学上与真实的重新合成样本非常匹配，并且在下游任务上表现良好。尽管是完全无监督的，GmSLM仍然能够有效地区分真实的与人工的对话。

**Conclusion:** GmSLM不仅为神经活动与叫声之间的关系研究提供了实践框架，而且有望在未来对神经科学、生物声学和进化生物学的研究中受益。

**Abstract:** Marmoset monkeys exhibit complex vocal communication, challenging the view
that nonhuman primates vocal communication is entirely innate, and show similar
features of human speech, such as vocal labeling of others and turn-taking.
Studying their vocal communication offers a unique opportunity to link it with
brain activity-especially given the difficulty of accessing the human brain in
speech and language research. Since Marmosets communicate primarily through
vocalizations, applying standard LLM approaches is not straightforward. We
introduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized
spoken language model pipeline for Marmoset vocal communication. We designed a
novel zero-shot evaluation metrics using unsupervised in-the-wild data,
alongside weakly labeled conversational data, to assess GmSLM and demonstrate
its advantage over a basic human-speech-based baseline. GmSLM generated
vocalizations closely matched real resynthesized samples acoustically and
performed well on downstream tasks. Despite being fully unsupervised, GmSLM
effectively distinguish real from artificial conversations and may support
further investigations of the neural basis of vocal communication and provides
a practical framework linking vocalization and brain activity. We believe GmSLM
stands to benefit future work in neuroscience, bioacoustics, and evolutionary
biology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM.

</details>


### [19] [CCF: A Context Compression Framework for Efficient Long-Sequence Language Modeling](https://arxiv.org/abs/2509.09199)
*Wenhao Li,Bangcheng Sun,Weihao Ye,Tianyi Zhang,Daohai Yu,Fei Chao,Rongrong Ji*

Main category: cs.CL

> 本文提出了CCF框架，通过学习高效的长上下文建模，结合语义分段和记忆编码的方法，简化上下文表示，同时减小内存开销，实验表明它比现有方法更高效和可扩展。

<details>
  <summary>Details</summary>

**Motivation:** 扩展语言模型的上下文规模对于捕捉跨长篇幅的丰富依赖关系是至关重要的，然而，直接扩展上下文规模会带来显著的计算和内存负担，常常导致训练和推理中的低效率。

**Method:** 本文提出了一种名为CCF的上下文压缩框架，该框架通过学习分层潜在表示来实现高效的长上下文建模，这些表示能够保留全局语义同时大幅度减少输入冗余。CCF结合了分段语义聚合与键值记忆编码，形成了支持准确重建和长时间理解的紧凑表示。此外，为了进一步提高可扩展性，本文提出了一种训练高效的优化策略，该策略结合了增量分段解码与稀疏水库采样，大大减少了内存开销，同时并未降低性能表现。

**Result:** 在多个长上下文语言建模基准测试中，CCF证明了在高压缩比率下能够达到具有竞争力的困惑度，并在吞吐量和内存效率方面显著优于现有方法。

**Conclusion:** 实验结果表明，在高压缩比例下，CCF实现了具有竞争力的困惑度，并在吞吐量和内存效率方面显著优于现有的方法。这些发现突显了结构化压缩在可扩展性和高效的长上下文语言建模方面的潜力。

**Abstract:** Scaling language models to longer contexts is essential for capturing rich
dependencies across extended discourse. However, na\"ive context extension
imposes significant computational and memory burdens, often resulting in
inefficiencies during both training and inference. In this work, we propose
CCF, a novel context compression framework designed to enable efficient
long-context modeling by learning hierarchical latent representations that
preserve global semantics while aggressively reducing input redundancy. CCF
integrates segment-wise semantic aggregation with key-value memory encoding,
forming compact representations that support accurate reconstruction and
long-range understanding. To further enhance scalability, we introduce a
training-efficient optimization strategy that couples incremental segment
decoding with sparse reservoir sampling, substantially reducing memory overhead
without degrading performance. Empirical results on multiple long-context
language modeling benchmarks demonstrate that CCF achieves competitive
perplexity under high compression ratios, and significantly improves throughput
and memory efficiency compared to existing approaches. These findings highlight
the potential of structured compression for scalable and effective long-context
language modeling.

</details>


### [20] [Reading Between the Lines: Classifying Resume Seniority with Large Language Models](https://arxiv.org/abs/2509.09229)
*Matan Cohen,Shira Shani,Eden Menahem,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CL

> 研究使用大型语言模型，包括微调的BERT架构，自动分类简历资历。采用包含真实世界简历和模拟夸大资格的例子的混合数据集评估模型性能，结果显示大型语言模型能有效检测资历夸大，为提高AI候选评估系统提供了方向。

<details>
  <summary>Details</summary>

**Motivation:** 准确评估简历中的候选人资历是一项关键而具有挑战性的任务，它因广泛存在的夸大致使经验和不明确的自我表示而变得复杂。

**Method:** 本研究使用大型语言模型（LLMs），包括微调的BERT架构，来自动分类简历中的候选人资历。为了严谨评估模型性能，引入了一个混合数据集，该数据集由真实世界简历和模拟夸大资格和低报资历的合成生成的困难例子组成。

**Result:** 研究结果显示，大型语言模型在检测与资历夸大和隐含专长相关的微妙语言线索方面表现出色。

**Conclusion:** 该研究指出了提高AI驱动候选人评估系统和减少自夸语言引入偏见的有前景的方向。

**Abstract:** Accurately assessing candidate seniority from resumes is a critical yet
challenging task, complicated by the prevalence of overstated experience and
ambiguous self-presentation. In this study, we investigate the effectiveness of
large language models (LLMs), including fine-tuned BERT architectures, for
automating seniority classification in resumes. To rigorously evaluate model
performance, we introduce a hybrid dataset comprising both real-world resumes
and synthetically generated hard examples designed to simulate exaggerated
qualifications and understated seniority. Using the dataset, we evaluate the
performance of Large Language Models in detecting subtle linguistic cues
associated with seniority inflation and implicit expertise. Our findings
highlight promising directions for enhancing AI-driven candidate evaluation
systems and mitigating bias introduced by self-promotional language. The
dataset is available for the research community at https://bit.ly/4mcTovt

</details>


### [21] [Agentic LLMs for Question Answering over Tabular Data](https://arxiv.org/abs/2509.09234)
*Rishit Tyagi,Mohit Gupta,Rahul Bouri*

Main category: cs.CL

> 本文提出了一种基于自然语言到SQL的方法，利用大型语言模型生成SQL查询来解决表格问答问题，该方法在两个数据集上得到了良好的表现。

<details>
  <summary>Details</summary>

**Motivation:** 由于现实世界表格结构多样、规模不一且数据类型多变，表格问答（Table QA）面临独特挑战。为了评估模型回答结构化查询的能力，SemEval 2025任务8（DataBench）引入了一个大规模、领域多样的数据集。

**Method:** 我们提出了一种利用大型语言模型（如GPT-4o, GPT-4o-mini, 和DeepSeek v2:16b）将自然语言转换为SQL查询的方法，通过一个多阶段的管道，包括示例选择、SQL查询生成、答案提取、验证和迭代优化。

**Result:** 实验表明，我们的方法在DataBench QA和DataBench Lite QA上的准确率分别达到了70.5%和71.6%，显著超越了基线分数26%和27%。

**Conclusion:** 本文的方法展示了利用大型语言模型生成SQL查询进行表格问答的有效性，为理解LLM驱动的表格问答的能力和局限性提供了见解。

**Abstract:** Question Answering over Tabular Data (Table QA) presents unique challenges
due to the diverse structure, size, and data types of real-world tables. The
SemEval 2025 Task 8 (DataBench) introduced a benchmark composed of large-scale,
domain-diverse datasets to evaluate the ability of models to accurately answer
structured queries. We propose a Natural Language to SQL (NL-to-SQL) approach
leveraging large language models (LLMs) such as GPT-4o, GPT-4o-mini, and
DeepSeek v2:16b to generate SQL queries dynamically. Our system follows a
multi-stage pipeline involving example selection, SQL query generation, answer
extraction, verification, and iterative refinement. Experiments demonstrate the
effectiveness of our approach, achieving 70.5\% accuracy on DataBench QA and
71.6\% on DataBench Lite QA, significantly surpassing baseline scores of 26\%
and 27\% respectively. This paper details our methodology, experimental
results, and alternative approaches, providing insights into the strengths and
limitations of LLM-driven Table QA.

</details>


### [22] [From scratch to silver: Creating trustworthy training data for patent-SDG classification using Large Language Models](https://arxiv.org/abs/2509.09303)
*Grazia Sveva Ascione,Nicolò Tamagnone*

Main category: cs.CL

> 论文将专利分类为SDGs的问题定义为弱监督学习问题，提出了一个复合标注函数，并证实了它在内部和外部验证中的优越性能。

<details>
  <summary>Details</summary>

**Motivation:** 分类专利的必要性在于追踪技术创新如何解决全球性问题。然而，缺少大量标注数据限制了监督学习的使用。现有方法如关键词搜索、迁移学习和基于引用的方法具有一定局限性，不够可扩展和具有普适性。

**Method:** 本论文将专利分类为联合国可持续发展目标（SDGs）相关的问题定义为一个弱监督学习问题，使用专利引用来标注的SDG相关的科学出版物作为初始的噪声信号。开发了一个复合标注函数（LF），利用大型语言模型（LLMs）从专利和SDG文件中提取结构化的概念（功能、解决方案和应用），并通过域间相似度分数计算和排序（rank-based）检索方法结合。LF通过一种自定义的正面仅损失函数进行校准，允许在已知NPL-SDG链接的基础上发现新的SDG关联。

**Result:** 研究成果是一个银标准数据集，该数据集采用软多标签方法，将专利映射到SDGs上，可以用于有效训练多标签回归模型。内部验证采用了与保留的NPL标签对比的方法，结果显示，该方法优于几个基线模型，包括基于Transformer和零样本LLM的模型。外部验证通过专利引用网络、联合发明人和联合申请人图中的模块性评估，表明该标签较传统的技术分类更具主题和认知一致性。

**Conclusion:** 研究报告表明，弱监督学习和语义对齐可以提高大规模SDG分类的效果。

**Abstract:** Classifying patents by their relevance to the UN Sustainable Development
Goals (SDGs) is crucial for tracking how innovation addresses global
challenges. However, the absence of a large, labeled dataset limits the use of
supervised learning. Existing methods, such as keyword searches, transfer
learning, and citation-based heuristics, lack scalability and generalizability.
This paper frames patent-to-SDG classification as a weak supervision problem,
using citations from patents to SDG-tagged scientific publications (NPL
citations) as a noisy initial signal. To address its sparsity and noise, we
develop a composite labeling function (LF) that uses large language models
(LLMs) to extract structured concepts, namely functions, solutions, and
applications, from patents and SDG papers based on a patent ontology.
Cross-domain similarity scores are computed and combined using a rank-based
retrieval approach. The LF is calibrated via a custom positive-only loss that
aligns with known NPL-SDG links without penalizing discovery of new SDG
associations. The result is a silver-standard, soft multi-label dataset mapping
patents to SDGs, enabling the training of effective multi-label regression
models. We validate our approach through two complementary strategies: (1)
internal validation against held-out NPL-based labels, where our method
outperforms several baselines including transformer-based models, and zero-shot
LLM; and (2) external validation using network modularity in patent citation,
co-inventor, and co-applicant graphs, where our labels reveal greater thematic,
cognitive, and organizational coherence than traditional technological
classifications. These results show that weak supervision and semantic
alignment can enhance SDG classification at scale.

</details>


### [23] [MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems](https://arxiv.org/abs/2509.09360)
*Channdeth Sok,David Luz,Yacine Haddam*

Main category: cs.CL

> MetaRAG是一个用于检测检索增强生成系统中幻觉的元形变测试框架，它在无监督、黑盒子环境中操作，能够定位和评估不支持的断言，适用于企业级应用以保证其可靠性。

<details>
  <summary>Details</summary>

**Motivation:** 现有幻觉检测方法主要针对独立的语言模型，无法处理检索增强生成系统的特点，这些系统要求生成的回答必须与检索到的证据一致。为了提高此类系统的可靠性，提出了MetaRAG。

**Method:** MetaRAG框架会在无监督且黑盒环境中操作，包括将答案分解成原子事实、对每个事实生成控制变异、验证每个变异并与检索的上下文进行对比，最后聚合对不一致性的惩罚，给出幻觉分数。

**Result:** 实验展现了MetaRAG在检测幻觉及支持基于检索的会话代理响应可靠性方面的有效性。

**Conclusion:** MetaRAG能够在敏感身份查询中定位和评估不支持的说法，为企业环境中信任度的提高提供了途径，同时也讨论了一个将MetaRAG的得分转换为身份感知防护措施的设计方案。

**Abstract:** Large Language Models (LLMs) are increasingly deployed in enterprise
applications, yet their reliability remains limited by hallucinations, i.e.,
confident but factually incorrect information. Existing detection approaches,
such as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not
address the unique challenges of Retrieval-Augmented Generation (RAG) systems,
where responses must be consistent with retrieved evidence. We therefore
present MetaRAG, a metamorphic testing framework for hallucination detection in
Retrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time,
unsupervised, black-box setting, requiring neither ground-truth references nor
access to model internals, making it suitable for proprietary and high-stakes
domains. The framework proceeds in four stages: (1) decompose answers into
atomic factoids, (2) generate controlled mutations of each factoid using
synonym and antonym substitutions, (3) verify each variant against the
retrieved context (synonyms are expected to be entailed and antonyms
contradicted), and (4) aggregate penalties for inconsistencies into a
response-level hallucination score. Crucially for identity-aware AI, MetaRAG
localizes unsupported claims at the factoid span where they occur (e.g.,
pregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility),
allowing users to see flagged spans and enabling system designers to configure
thresholds and guardrails for identity-sensitive queries. Experiments on a
proprietary enterprise dataset illustrate the effectiveness of MetaRAG for
detecting hallucinations and enabling trustworthy deployment of RAG-based
conversational agents. We also outline a topic-based deployment design that
translates MetaRAG's span-level scores into identity-aware safeguards; this
design is discussed but not evaluated in our experiments.

</details>


### [24] [Modelling Analogies and Analogical Reasoning: Connecting Cognitive Science Theory and NLP Research](https://arxiv.org/abs/2509.09381)
*Molly R Petersen,Claire E Stevenson,Lonneke van der Plas*

Main category: cs.CL

> 本文将认知科学中的类比推理理论与NLP研究连接起来，指导研究者提高文本中的关系理解，而不是仅仅依赖实体级别的相似性。

<details>
  <summary>Details</summary>

**Motivation:** 作者认为类比推理是人类认知的重要方面，尽管这些过程可以轻松地与NLP中的概念相联系，但它们通常没有通过认知的视角来审视。

**Method:** 本文总结了认知科学文献中关于类比推理过程的关键理论，并将其与自然语言处理领域的当前研究联系了起来。

**Result:** 说明了这些概念对于NLP研究中的几个主要挑战的相关性，这些挑战并非直接与类比问题解决相关。

**Conclusion:** 这可以引导研究者优化文本中的关系理解，而非仅仅依赖于实体级别的相似性。

**Abstract:** Analogical reasoning is an essential aspect of human cognition. In this
paper, we summarize key theory about the processes underlying analogical
reasoning from the cognitive science literature and relate it to current
research in natural language processing. While these processes can be easily
linked to concepts in NLP, they are generally not viewed through a cognitive
lens. Furthermore, we show how these notions are relevant for several major
challenges in NLP research, not directly related to analogy solving. This may
guide researchers to better optimize relational understanding in text, as
opposed to relying heavily on entity-level similarity.

</details>


### [25] [Hierarchical Bracketing Encodings Work for Dependency Graphs](https://arxiv.org/abs/2509.09388)
*Ana Ezquerro,Carlos Gómez-Rodríguez,David Vilares*

Main category: cs.CL

> The paper proposes a new hierarchical bracketing encoding method for dependency graph parsing, which reduces label space without losing structural information, and demonstrates consistent improvements in exact match accuracy.

<details>
  <summary>Details</summary>

**Motivation:** The authors aim to develop an efficient encoding method for dependency graph parsing that can represent complex graph structures while reducing the label space and maintaining linear-time parsing.

**Method:** The method employed is a hierarchical bracketing encoding that encodes dependency graphs as sequences, allowing for linear-time parsing with a reduced label space.

**Result:** The results show competitive performance and consistent improvements over other methods in exact match accuracy when tested on a multilingual and multi-formalism benchmark.

**Conclusion:** The proposed hierarchical bracketing encoding method offers a practical solution for encoding dependency graphs for parsing, achieving better accuracy with a smaller label space while maintaining linear-time parsing efficiency.

**Abstract:** We revisit hierarchical bracketing encodings from a practical perspective in
the context of dependency graph parsing. The approach encodes graphs as
sequences, enabling linear-time parsing with $n$ tagging actions, and still
representing reentrancies, cycles, and empty nodes. Compared to existing graph
linearizations, this representation substantially reduces the label space while
preserving structural information. We evaluate it on a multilingual and
multi-formalism benchmark, showing competitive results and consistent
improvements over other methods in exact match accuracy.

</details>


### [26] [GrACE: A Generative Approach to Better Confidence Elicitation in Large Language Models](https://arxiv.org/abs/2509.09438)
*Zhaohan Zhang,Ziquan Liu,Ioannis Patras*

Main category: cs.CL

> 本文提出了 GrACE 方法，用于解决大语言模型在现实世界应用中信心评估的问题，通过一种新的机制实现了可扩展和可靠的信心评估，实验表明其在开放生成任务中表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法在可信度评估上要么需要昂贵的计算开销，要么校准效果差，这使得它们不适合现实世界的部署。本研究旨在解决这些问题，提出一种新的、可靠的、可扩展的信心评估方法。

**Method:** GrACE, 一种生成可信度评估方法，通过将特殊标记添加到词汇表来让模型通过隐藏状态与该标记嵌入之间的相似性实时表达信心。该方法通过对准确性和校准目标进行微调来校准信心。

**Result:** 实验表明，GrACE 在三个大语言模型和两个基准数据集上的开放生成任务中都表现出色，相比其他六种方法，其产生的信心具有最佳的判别能力和校准效果，并且不需要附加采样或辅助模型。

**Conclusion:** GrACE 不仅提高了最终决策的准确性，还显著减少了测试时间尺度方案中所需的样本数量，表明 GrACE 作为一个实际解决方案在可扩展、可靠且实时的信心估计方面具有潜力。

**Abstract:** Assessing the reliability of Large Language Models (LLMs) by confidence
elicitation is a prominent approach to AI safety in high-stakes applications,
such as healthcare and finance. Existing methods either require expensive
computational overhead or suffer from poor calibration, making them impractical
and unreliable for real-world deployment. In this work, we propose GrACE, a
Generative Approach to Confidence Elicitation that enables scalable and
reliable confidence elicitation for LLMs. GrACE adopts a novel mechanism in
which the model expresses confidence by the similarity between the last hidden
state and the embedding of a special token appended to the vocabulary, in
real-time. We fine-tune the model for calibrating the confidence with
calibration targets associated with accuracy. Experiments with three LLMs and
two benchmark datasets show that the confidence produced by GrACE achieves the
best discriminative capacity and calibration on open-ended generation tasks,
outperforming six competing methods without resorting to additional sampling or
an auxiliary model. Moreover, we propose two strategies for improving test-time
scaling based on confidence induced by GrACE. Experimental results show that
using GrACE not only improves the accuracy of the final decision but also
significantly reduces the number of required samples in the test-time scaling
scheme, indicating the potential of GrACE as a practical solution for deploying
LLMs with scalable, reliable, and real-time confidence estimation.

</details>


### [27] [Mitigating Language Barriers in Education: Developing Multilingual Digital Learning Materials with Machine Translation](https://arxiv.org/abs/2509.09473)
*Lucie Poláková,Martin Popel,Věra Kloudová,Michal Novák,Mariia Anisimova,Jiří Balhar*

Main category: cs.CL

> EdUKate项目为中小学开发多语言教育材料，利用机器翻译技术将9000个多模态互动练习翻译成多种语言，并在教育网站上部署。

<details>
  <summary>Details</summary>

**Motivation:** 通过捷克主要学术机构与该国最大教育出版商之间的合作启动，该项目强调开发和评估专门针对教育领域的直接捷克语-乌克兰语机器翻译系统，特别关注处理格式化内容（如XML和PDF）和技术及科学术语。

**Method:** 结合数字教育、语言学、翻译研究和机器翻译，EdUKate项目为捷克中小学开发多语言学习材料。该项目旨在将多达9000个多模态互动练习从捷克语翻译成乌克兰语、英语和德语，并发布到教育网络平台上。

**Result:** 该项目展现了对捷克教师进行的初步调查结果，调查内容涉及非捷克语背景学生的需求，并描述了该系统在网页端的评估和实施情况。

**Conclusion:** 所有应用都免费向学生、教师和研究人员提供。

**Abstract:** The EdUKate project combines digital education, linguistics, translation
studies, and machine translation to develop multilingual learning materials for
Czech primary and secondary schools. Launched through collaboration between a
major Czech academic institution and the country's largest educational
publisher, the project is aimed at translating up to 9,000 multimodal
interactive exercises from Czech into Ukrainian, English, and German for an
educational web portal. It emphasizes the development and evaluation of a
direct Czech-Ukrainian machine translation system tailored to the educational
domain, with special attention to processing formatted content such as XML and
PDF and handling technical and scientific terminology. We present findings from
an initial survey of Czech teachers regarding the needs of non-Czech-speaking
students and describe the system's evaluation and implementation on the web
portal. All resulting applications are freely available to students, educators,
and researchers.

</details>


### [28] [Towards Explainable Job Title Matching: Leveraging Semantic Textual Relatedness and Knowledge Graphs](https://arxiv.org/abs/2509.09522)
*Vadim Zadykian,Bruno Andrade,Haithem Afli*

Main category: cs.CL

> 本文提出了一种结合知识图谱和文本嵌入的自监督混合架构，采用分区域评估方法来优化职位匹配中的语义文本相关性。实验结果表明，这种架构方法提升了模型在高语义相关性区域的表现，并揭示了模型的优势和劣势。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于解决简历推荐系统中职位匹配的关键挑战。我们引入了一种新的架构来处理重叠词汇有限或具有误导性的挑战，并通过分区评估来细致分析模型性能。

**Method:** 我们提出了一种自监督混合架构，结合密集句嵌入与领域特定的知识图谱（KG）以提高语义对齐和可解释性。我们探讨了在职位匹配中的语义文本相关性（STR），这是一种简历推荐系统中的关键技术挑战，其中重叠词汇通常是有限或潜在误导的。不同于以往着重模型整体性能的评估，我们的方法强调数据分层，通过将STR分数连续体划分为低、中、高语义相关性的不同区域来进行细致的模型性能分析。

**Result:** 我们测试了几种嵌入模型，包括与和不与通过图神经网络集成知识图谱的模型。结果显示，与强大的基线相比，通过知识图谱增强的微调SBERT模型在高STR区域取得了一致性的改进，使得均方根误差（RMSE）降低了25%。

**Conclusion:** 实验结果引导我们关注将KG与文本嵌入结合的好处，同时也澄清了区域性能分析对于理解模型行为的重要性。这种细致的方法揭示了被全球指标所掩盖的模型优势和劣势，并支持更精确地选择用于人力资源系统和需求公平性、可解释性和上下文匹配的应用。

**Abstract:** Semantic Textual Relatedness (STR) captures nuanced relationships between
texts that extend beyond superficial lexical similarity. In this study, we
investigate STR in the context of job title matching - a key challenge in
resume recommendation systems, where overlapping terms are often limited or
misleading. We introduce a self-supervised hybrid architecture that combines
dense sentence embeddings with domain-specific Knowledge Graphs (KGs) to
improve both semantic alignment and explainability. Unlike previous work that
evaluated models on aggregate performance, our approach emphasizes data
stratification by partitioning the STR score continuum into distinct regions:
low, medium, and high semantic relatedness. This stratified evaluation enables
a fine-grained analysis of model performance across semantically meaningful
subspaces. We evaluate several embedding models, both with and without KG
integration via graph neural networks. The results show that fine-tuned SBERT
models augmented with KGs produce consistent improvements in the high-STR
region, where the RMSE is reduced by 25% over strong baselines. Our findings
highlight not only the benefits of combining KGs with text embeddings, but also
the importance of regional performance analysis in understanding model
behavior. This granular approach reveals strengths and weaknesses hidden by
global metrics, and supports more targeted model selection for use in Human
Resources (HR) systems and applications where fairness, explainability, and
contextual matching are essential.

</details>


### [29] [DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning and Label Distribution Learning](https://arxiv.org/abs/2509.09524)
*Daniil Ignatev,Nan Li,Hugh Mee Wong,Anh Dang,Shane Kaszefski Yaschuk*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** This system paper presents the DeMeVa team's approaches to the third edition
of the Learning with Disagreements shared task (LeWiDi 2025; Leonardelli et
al., 2025). We explore two directions: in-context learning (ICL) with large
language models, where we compare example sampling strategies; and label
distribution learning (LDL) methods with RoBERTa (Liu et al., 2019b), where we
evaluate several fine-tuning methods. Our contributions are twofold: (1) we
show that ICL can effectively predict annotator-specific annotations
(perspectivist annotations), and that aggregating these predictions into soft
labels yields competitive performance; and (2) we argue that LDL methods are
promising for soft label predictions and merit further exploration by the
perspectivist community.

</details>


### [30] [Prompting the Market? A Large-Scale Meta-Analysis of GenAI in Finance NLP (2022-2025)](https://arxiv.org/abs/2509.09544)
*Paolo Pedinotti,Peter Baumann,Nathan Jessurun,Leslie Barrett,Enrico Santus*

Main category: cs.CL

> 提出MetaGraph，一种从金融NLP文献中提取知识图谱的方法，揭示了LLM应用的三个关键阶段：早期采用、反思局限性和整合外围技术。

<details>
  <summary>Details</summary>

**Motivation:** 传统的调研方法无法跟上LLM在金融NLP领域的快速变化，提出MetaGraph是为了更好地理解并分析这一领域的发展趋势。

**Method:** Structure

**Result:** 应用MetaGraph对681篇论文进行了分析，实现了大规模数据驱动的研究趋势分析。识别出金融NLP领域的三个关键发展时期。

**Conclusion:** MetaGraph提供了一个清晰的金融NLP发展图景，同时也是一个可复用的方法来衡量其他领域的科学进步。

**Abstract:** Large Language Models (LLMs) have rapidly reshaped financial NLP, enabling
new tasks and driving a proliferation of datasets and diversification of data
sources. Yet, this transformation has outpaced traditional surveys. In this
paper, we present MetaGraph, a generalizable methodology for extracting
knowledge graphs from scientific literature and analyzing them to obtain a
structured, queryable view of research trends. We define an ontology for
financial NLP research and apply an LLM-based extraction pipeline to 681 papers
(2022-2025), enabling large-scale, data-driven analysis. MetaGraph reveals
three key phases: early LLM adoption and task/dataset innovation; critical
reflection on LLM limitations; and growing integration of peripheral techniques
into modular systems. This structured view offers both practitioners and
researchers a clear understanding of how financial NLP has evolved -
highlighting emerging trends, shifting priorities, and methodological
shifts-while also demonstrating a reusable approach for mapping scientific
progress in other domains.

</details>


### [31] [Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking](https://arxiv.org/abs/2509.09583)
*Brittany Harbison,Samuel Taubman,Travis Taylor,Ashok. K. Goel*

Main category: cs.CL

> 研究提出了一种基于GPT的零样本人格检测模型来改进在线课程中的社会化推荐系统SAMISE

<details>
  <summary>Details</summary>

**Motivation:** 在线课程环境限制了学生自然形成社交群体的能力，而SAMI虽然能促进学生间连接，但受限于不完整的心智理论，导致提供的人格相关推荐不相关。研究动机是通过检测学生的人格特质来增强SAMI的匹配效能。

**Method:** 本研究提出了一种利用GPT的零样本能力从论坛入门帖子中推断Big-Five人格特质的人格检测模型，并将其集成到SAMI的实体匹配系统中，以提供基于人格特质的社会推荐。

**Result:** 实验表明，该人格检测模型在推断人格特质方面显示了良好的效果，与现有模型相比具有竞争力。

**Conclusion:** 初步集成显示，人格特质可以补充现有的匹配因素，尽管仍需进一步评估来确定其对学生活跃度和匹配质量的全面影响。

**Abstract:** Social connection is a vital part of learning, yet online course environments
present barriers to the organic formation of social groups. SAMI offers one
solution by facilitating student connections, but its effectiveness is
constrained by an incomplete Theory of Mind, limiting its ability to create an
effective mental model of a student. One facet of this is its inability to
intuit personality, which may influence the relevance of its recommendations.
To explore this, we propose a personality detection model utilizing GPTs
zero-shot capability to infer Big-Five personality traits from forum
introduction posts, often encouraged in online courses. We benchmark its
performance against established models, demonstrating its efficacy in this
task. Furthermore, we integrate this model into SAMIs entity-based matchmaking
system, enabling personality-informed social recommendations. Initial
integration suggests personality traits can complement existing matching
factors, though additional evaluation is required to determine their full
impact on student engagement and match quality.

</details>


### [32] [Fluent but Unfeeling: The Emotional Blind Spots of Language Models](https://arxiv.org/abs/2509.09593)
*Bangzhao Shu,Isha Joshi,Melissa Karnaze,Anh C. Pham,Ishita Kakkar,Sindhu Kothe,Arpine Hovasapian,Mai ElSherief*

Main category: cs.CL

> 研究通过引入EXPRESS数据集来评估LLMs在细粒度情绪识别方面的表现，发现尽管有进展，但LLMs仍然难以完全捕捉人类自我披露的情绪表达。

<details>
  <summary>Details</summary>

**Motivation:** 虽然许多研究探讨了LLMs在情感识别方面的能力，但在评估LLMs是否在细粒度层面上与人类情感保持一致方面仍存在关键空白。现有研究通常关注将情感分类为预定义的有限类别，忽视了更微妙的表达。因此，我们的研究旨在填补这一空白。

**Method:** 我们的研究通过引进名为EXPRESS的基准数据集，该数据集来源于Reddit社区，包含了251种细微的、自我披露的情绪标签。我们使用全面的评估框架来检测预测的情绪术语，并将这些术语分解为根据已建立的情绪理论的八种基本情绪，从而实现细粒度的比较。

**Result:** 对流行LLMs在不同提示设置下的系统测试表明，准确预测与人类自我披露的情绪相一致的情感仍然具有挑战性。定性分析进一步显示，虽然某些LLMs生成的情绪术语与已建立的情绪理论和定义一致，但它们有时未能像人类自我披露那样有效捕捉情境线索。

**Conclusion:** 这些发现强调了LLMs在细粒度情绪对齐方面的局限性，并为旨在增强其情境理解能力的未来研究提供了见解。

**Abstract:** The versatility of Large Language Models (LLMs) in natural language
understanding has made them increasingly popular in mental health research.
While many studies explore LLMs' capabilities in emotion recognition, a
critical gap remains in evaluating whether LLMs align with human emotions at a
fine-grained level. Existing research typically focuses on classifying emotions
into predefined, limited categories, overlooking more nuanced expressions. To
address this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit
communities featuring 251 fine-grained, self-disclosed emotion labels. Our
comprehensive evaluation framework examines predicted emotion terms and
decomposes them into eight basic emotions using established emotion theories,
enabling a fine-grained comparison. Systematic testing of prevalent LLMs under
various prompt settings reveals that accurately predicting emotions that align
with human self-disclosed emotions remains challenging. Qualitative analysis
further shows that while certain LLMs generate emotion terms consistent with
established emotion theories and definitions, they sometimes fail to capture
contextual cues as effectively as human self-disclosures. These findings
highlight the limitations of LLMs in fine-grained emotion alignment and offer
insights for future research aimed at enhancing their contextual understanding.

</details>


### [33] [LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death Determination](https://arxiv.org/abs/2509.09602)
*Yiqun T. Chen,Tyler H. McCormick,Li Liu,Abhirup Datta*

Main category: cs.CL

> 本研究提出了一种基于大型语言模型（LLMs）的验证性管道LA-VA，旨在改善资源有限地区通过口头推断死亡原因的准确度，尤其是在成人、儿童和新生儿的死亡率预测上，比传统方式有了显著提高。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于资源有限的地区无法进行医学证明，口头调查成为估算死亡原因的关键工具。本研究旨在改进这一领域的预测准确性，以此提升全球健康监测在资源匮乏地区的效率。

**Method:** 本研究提出了一种名为LA-VA的验证性管道方法，该方法结合了大型语言模型（LLMs）、传统的算法方法和基于嵌入的分类方法，以提高死亡原因预测的准确性。

**Result:** 通过使用PHMRC数据集对不同年龄段（成人7580人、儿童1960人、新生儿2438人）进行的实验，GPT-5达到了在各个测试地点成人准确率为48.6%，儿童为50.5%，新生儿为53.5%的最高个体表现，比传统的统计机器学习基线高出了5-10%。

**Conclusion:** 研究表明，简单的现成的LLM辅助方法可以在口头推断死亡原因时大幅度提高准确度，对于改善资源有限地区健康状况监测具有重要意义。

**Abstract:** Verbal autopsy (VA) is a critical tool for estimating causes of death in
resource-limited settings where medical certification is unavailable. This
study presents LA-VA, a proof-of-concept pipeline that combines Large Language
Models (LLMs) with traditional algorithmic approaches and embedding-based
classification for improved cause-of-death prediction. Using the Population
Health Metrics Research Consortium (PHMRC) dataset across three age categories
(Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches:
GPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles.
Our results demonstrate that GPT-5 achieves the highest individual performance
with average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5%
(Neonate), outperforming traditional statistical machine learning baselines by
5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches
could substantially improve verbal autopsy accuracy, with important
implications for global health surveillance in low-resource settings.

</details>


### [34] [Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems](https://arxiv.org/abs/2509.09629)
*Minghang Zhu,Zhengliang Shi,Zhiwei Xu,Shiguang Wu,Lingjie Wang,Pengjie Ren,Zhaochun Ren,Zhumin Chen*

Main category: cs.CL

> Proposes MOAT for improved collaboration in multi-agent systems through iterative alignment between planning and grounding agents, with proven performance gains.

<details>
  <summary>Details</summary>

**Motivation:** To improve collaboration among agents in multi-agent systems and close capability gaps that exist when agents are fine-tuned independently, leading to poor coordination.

**Method:** MOAT, a Multi-Agent Joint Alignment Tuning framework, alternates between two key stages: Planning Agent Alignment, which optimizes the planning agent to generate subgoal sequences; and Grounding Agent Improving, which fine-tunes the grounding agent to enhance its generalization capability.

**Result:** Theoretical analysis proves that MOAT ensures a non-decreasing and progressively convergent training process. Experiments across six benchmarks show that MOAT outperforms state-of-the-art baselines, achieving average improvements of 3.1% on held-in tasks and 4.4% on held-out tasks.

**Conclusion:** MOAT effectively improves the coordination and performance of multi-agent systems by iteratively aligning the planning and grounding agents, leading to better task completion rates and generalization.

**Abstract:** The advancement of large language models (LLMs) has enabled the construction
of multi-agent systems to solve complex tasks by dividing responsibilities
among specialized agents, such as a planning agent for subgoal generation and a
grounding agent for executing tool-use actions. Most existing methods typically
fine-tune these agents independently, leading to capability gaps among them
with poor coordination. To address this, we propose MOAT, a Multi-Agent Joint
Alignment Tuning framework that improves agents collaboration through iterative
alignment. MOAT alternates between two key stages: (1) Planning Agent
Alignment, which optimizes the planning agent to generate subgoal sequences
that better guide the grounding agent; and (2) Grounding Agent Improving, which
fine-tunes the grounding agent using diverse subgoal-action pairs generated by
the agent itself to enhance its generalization capablity. Theoretical analysis
proves that MOAT ensures a non-decreasing and progressively convergent training
process. Experiments across six benchmarks demonstrate that MOAT outperforms
state-of-the-art baselines, achieving average improvements of 3.1% on held-in
tasks and 4.4% on held-out tasks.

</details>


### [35] [All for One: LLMs Solve Mental Math at the Last Token With Information Transferred From Other Tokens](https://arxiv.org/abs/2509.09650)
*Siddarth Mamidanna,Daking Rai,Ziyu Yao,Yilun Zhou*

Main category: cs.CL

> 本文发现大型语言模型在进行数学计算时，存在着一个名为All-for-One (AF1)的子图，该子图的有意义计算发生在深度靠后的层次，且主要集中在最后一个令牌上，从其他令牌接收信息也需要特定的中间层。这个发现对于理解模型内部工作机制具有启发性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型(Large Language Models, LLMs)表现出跨多项计算任务的高效能，但其内部工作原理仍不清晰。本文旨在通过数学计算任务（即通过下一步单词预测直接计算而不进行显式推理），对这一问题进行探究。

**Method:** 本文提出两种技术，Context-Aware Mean Ablation (CAMA) 和 Attention-Based Peeking (ABP)，用以在多层感知器和因果自注意力机制结合的大型语言模型中探究数学计算任务的内部工作机制。通过分阶段抑制早期层次的特定输入令牌计算，限制中层跨越令牌位置的信息传递路径，以及在最后层次强制所有计算发生在最后一个令牌上。

**Result:** 作者发现了一个名为All-for-One (AF1)的子图，该子图在深度靠后的层次进行有意义的计算，并且这种计算只发生在最后一个令牌上，这些令牌接收其他令牌的信息主要发生在某些特定的中间层。实验表明，这个子图对于处理多种算术表达式是充分和必要的，并且该子图在不同模型和不同的输入风格上均有表现。

**Conclusion:** 实验结果表明，该All-for-One子图对于模型的高性能是充分且必要的，而且该子图在不同模型以及不同的输入风格上均有表现。

**Abstract:** Large language models (LLMs) demonstrate proficiency across numerous
computational tasks, yet their inner workings remain unclear. In theory, the
combination of causal self-attention and multilayer perceptron layers allows
every token to access and compute information based on all preceding tokens. In
practice, to what extent are such operations present? In this paper, on mental
math tasks (i.e., direct math calculation via next-token prediction without
explicit reasoning), we investigate this question in three steps: inhibiting
input-specific token computations in the initial layers, restricting the routes
of information transfer across token positions in the next few layers, and
forcing all computation to happen at the last token in the remaining layers.
With two proposed techniques, Context-Aware Mean Ablation (CAMA) and
Attention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with
high accuracy on a wide variety of mental math tasks, where meaningful
computation occurs very late (in terms of layer depth) and only at the last
token, which receives information of other tokens in few specific middle
layers. Experiments on a variety of models and arithmetic expressions show that
this subgraph is sufficient and necessary for high model performance, transfers
across different models, and works on a variety of input styles. Ablations on
different CAMA and ABP alternatives reveal their unique advantages over other
methods, which may be of independent interest.

</details>


### [36] [Steering MoE LLMs via Expert (De)Activation](https://arxiv.org/abs/2509.09660)
*Mohsen Fayyaz,Ali Modarressi,Hanieh Deilamsalehy,Franck Dernoncourt,Ryan Rossi,Trung Bui,Hinrich Schütze,Nanyun Peng*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token
through a subset of specialized Feed-Forward Networks (FFN), known as experts.
We present SteerMoE, a framework for steering MoE models by detecting and
controlling behavior-linked experts. Our detection method identifies experts
with distinct activation patterns across paired inputs exhibiting contrasting
behaviors. By selectively (de)activating such experts during inference, we
control behaviors like faithfulness and safety without retraining or modifying
weights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to
+20% and faithfulness by +27%. In adversarial attack mode, it drops safety by
-41% alone, and -100% when combined with existing jailbreak methods, bypassing
all safety guardrails and exposing a new dimension of alignment faking hidden
within experts.

</details>


### [37] [CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2509.09675)
*Runpeng Dai,Linfeng Song,Haolin Liu,Zhenwen Liang,Dian Yu,Haitao Mi,Zhaopeng Tu,Rui Liu,Tong Zheng,Hongtu Zhu,Dong Yu*

Main category: cs.CL

> 本文介绍了好奇心驱动探索（CDE），一种增强强化学习与可验证奖励（RLVR）方法中的探索能力的框架，通过该框架改进了模型的内部好奇心，实现了在某些基准测试上的显著性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在通过提出一种好奇心驱动的框架来改进RLVR方法中的探索方面，从而解决提前收敛和熵塌陷的问题。

**Method:** 该方法通过来自模型的行为者和评论者的内部好奇心信号来实现。对于行为者，使用生成回复的困惑度；对于评论者，采用多头架构中价值估计的变化，这两个信号作为探索奖励，来指导模型的探索。

**Result:** <tool_call>
{"name": "Structure", "arguments": {"tldr": "Curiosity-Driven Exploration (CDE) is introduced as a framework to enhance exploration in Reinforcement Learning with Verifiable Rewards (RLVR), addressing its premature convergence and entropy collapse issues by leveraging the model's intrinsic curiosity. This is achieved through perplexity for the actor and variance of value estimates for the critic, leading to a +3 point improvement on AIME benchmarks compared to standard RLVR techniques.", "motivation": "The motivation behind this paper is to improve the exploration aspect of RLVR methods that often lead to premature convergence and entropy collapse, by introducing a curiosity-driven framework.", "method": "The method involves using curiosity signals from the model’s actor and critic. For the actor, perplexity over generated responses is used, and for the critic, variance of value estimates from a multi-head architecture is employed, both serving as exploration bonuses.", "result": "The theoretical analysis demonstrates that the actor-wise bonus discourages overconfident errors and promotes diversity in correct responses. The critic-wise bonus is equivalent to count-based exploration bonus in RL. Empirically, the method shows a +3 point improvement on AIME benchmarks.", "conclusion": "The conclusion asserts that CDE enhances the exploration capabilities of RLVR by addressing the mechanisms that lead to premature entropy collapse and by introducing an innovative curiosity-driven framework. It also identifies a calibration collapse issue within RLVR, which may contribute to LLM failure modes."}}
</tool_call>

**Conclusion:** 本摘要分析了一种新颖的探索机制：好奇心驱动探索（CDE），其目的在于解决强化学习与可验证奖励（RLVR）方法中的提前收敛和熵塌陷问题。通过理论分析与实验证明，该方法在某些基准测试上表现出了显著的性能提升，并揭示了RLVR中的一些校准崩溃问题，这可能解释了大型语言模型的常见失败原因。

**Abstract:** Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm
for enhancing the reasoning ability of Large Language Models (LLMs). Yet
current RLVR methods often explore poorly, leading to premature convergence and
entropy collapse. To address this challenge, we introduce Curiosity-Driven
Exploration (CDE), a framework that leverages the model's own intrinsic sense
of curiosity to guide exploration. We formalize curiosity with signals from
both the actor and the critic: for the actor, we use perplexity over its
generated response, and for the critic, we use the variance of value estimates
from a multi-head architecture. Both signals serve as an exploration bonus
within the RLVR framework to guide the model. Our theoretical analysis shows
that the actor-wise bonus inherently penalizes overconfident errors and
promotes diversity among correct responses; moreover, we connect the
critic-wise bonus to the well-established count-based exploration bonus in RL.
Empirically, our method achieves an approximate +3 point improvement over
standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a
calibration collapse mechanism within RLVR, shedding light on common LLM
failure modes.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [38] [Recurrence Meets Transformers for Universal Multimodal Retrieval](https://arxiv.org/abs/2509.08897)
*Davide Caffagni,Sara Sarto,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara*

Main category: cs.CV

> 提出ReT-2模型，它可以处理多模态查询和检索，具有多层表示和动态信息整合能力，表现出色，效率更高。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法主要依赖于任务特定的视觉-语言模型的微调，并且这些方法被限制在单一模态的查询或文档中。本论文旨在解决这个问题，支持多模态查询和文档检索。

**Method:** ReT-2采用多层表示和具有LSTM启发式门控机制的循环Transformer架构，能够动态地跨层和模态集成信息，捕捉细粒度的视觉和文本细节。

**Result:** 在具有挑战性的M2KR和M-BEIR基准测试中，ReT-2在各种检索配置中始终达到最先进的性能，同时提供更快的推理和节省内存。集成到检索增强生成管道中，ReT-2还能提高Encyclopedic-VQA和InfoSeek数据集上的下游性能。

**Conclusion:** ReT-2是一个统一的检索模型，它能够支持多模态查询和文档检索，并且表现出比以前的方法更好的性能。

**Abstract:** With the rapid advancement of multimodal retrieval and its application in
LLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged.
Existing methods predominantly rely on task-specific fine-tuning of
vision-language models and are limited to single-modality queries or documents.
In this paper, we propose ReT-2, a unified retrieval model that supports
multimodal queries, composed of both images and text, and searches across
multimodal document collections where text and images coexist. ReT-2 leverages
multi-layer representations and a recurrent Transformer architecture with
LSTM-inspired gating mechanisms to dynamically integrate information across
layers and modalities, capturing fine-grained visual and textual details. We
evaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different
retrieval configurations. Results demonstrate that ReT-2 consistently achieves
state-of-the-art performance across diverse settings, while offering faster
inference and reduced memory usage compared to prior approaches. When
integrated into retrieval-augmented generation pipelines, ReT-2 also improves
downstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source
code and trained models are publicly available at:
https://github.com/aimagelab/ReT-2

</details>


### [39] [Diffusion-Based Action Recognition Generalizes to Untrained Domains](https://arxiv.org/abs/2509.08908)
*Rogerio Guimaraes,Frank Xiao,Pietro Perona,Markus Marks*

Main category: cs.CV

> 文章提出了一种利用视觉扩散模型生成的特征通过变压器处理的方法，以实现对动作识别的泛化能力，提高了跨物种、视角和背景的识别效果。

<details>
  <summary>Details</summary>

**Motivation:** 现有的深度学习模型在应对较大上下文和视点变化的动作识别方面存在困难。该动机来源于提出一种类似人类识别行为的模型，能够在物种、视角和背景等多个方面具有泛化能力。

**Method:** 使用通过扩散模型生成的特征并通过变压器聚合来实现人类级别的动作识别。该方法通过利用在扩散过程早期时间步长进行条件约束的模型来增强泛化能力，从而强调所提取特征中的语义信息，而非像素细节。

**Result:** 通过实验研究了方法在不同物种、不同视角和不同记录情况下分类动作的泛化性质，结果表明模型在上述三个泛化基准测试中表现突出。

**Conclusion:** 该模型在三个泛化基准测试中取得了新的前沿成果，推动了动作识别技术向更像人类的鲁棒性能发展。

**Abstract:** Humans can recognize the same actions despite large context and viewpoint
variations, such as differences between species (walking in spiders vs.
horses), viewpoints (egocentric vs. third-person), and contexts (real life vs
movies). Current deep learning models struggle with such generalization. We
propose using features generated by a Vision Diffusion Model (VDM), aggregated
via a transformer, to achieve human-like action recognition across these
challenging conditions. We find that generalization is enhanced by the use of a
model conditioned on earlier timesteps of the diffusion process to highlight
semantic information over pixel level details in the extracted features. We
experimentally explore the generalization properties of our approach in
classifying actions across animal species, across different viewing angles, and
different recording contexts. Our model sets a new state-of-the-art across all
three generalization benchmarks, bringing machine action recognition closer to
human-like robustness. Project page:
$\href{https://www.vision.caltech.edu/actiondiff/}{\texttt{vision.caltech.edu/actiondiff}}$
Code:
$\href{https://github.com/frankyaoxiao/ActionDiff}{\texttt{github.com/frankyaoxiao/ActionDiff}}$

</details>


### [40] [PromptGuard: An Orchestrated Prompting Framework for Principled Synthetic Text Generation for Vulnerable Populations using LLMs with Enhanced Safety, Fairness, and Controllability](https://arxiv.org/abs/2509.08910)
*Tung Vu,Lam Nguyen,Quynh Dao*

Main category: cs.CV

> 本文介绍了一种名为PromptGuard的模块化提示框架，通过一种称为VulnGuard提示的新技术来防止大规模语言模型生成对脆弱人群有害的信息。该技术基于现实数据驱动的对比学习，并通过理论分析表明其能减少25-30%的害。

<details>
  <summary>Details</summary>

**Motivation:** 大规模语言模型（LLMs）在实际应用中的普及给包括LGBTQ+个人、单亲家庭和边缘化社区在内的脆弱群体带来了产生有害、偏见或误导信息的前所未有的风险。现有的安全方法依赖于事后过滤或通用对齐技术，无法在生成源头积极预防有害输出。

**Method:** 本文提出了一种名为PromptGuard的新颖模块化提示框架，其核心贡献是VulnGuard提示，这是一种综合技术，通过现实世界数据驱动的对比学习来防止生成有害信息。VulnGuard集成了来自精选GitHub存储库的少量示例、伦理链式推理和自适应角色提示，构建出面向特定人群的保护屏障。

**Result:** 该框架运用理论上的多目标优化，通过熵边界和帕累托最优的形式证明了25-30%的分析性害减少。我们提供了全面的数学形式化，包括收敛性证明、使用信息论进行的脆弱性分析，以及使用GitHub来源数据集进行的理论验证框架，为系统的经验研究奠定了数学基础。

**Conclusion:** PromptGuard框架通过其六个核心模块——输入分类、VulnGuard提示、伦理原则集成、外部工具交互、输出验证以及用户-系统交互——构建了一个智能专家系统，以实现实时的害预防。

**Abstract:** The proliferation of Large Language Models (LLMs) in real-world applications
poses unprecedented risks of generating harmful, biased, or misleading
information to vulnerable populations including LGBTQ+ individuals, single
parents, and marginalized communities. While existing safety approaches rely on
post-hoc filtering or generic alignment techniques, they fail to proactively
prevent harmful outputs at the generation source. This paper introduces
PromptGuard, a novel modular prompting framework with our breakthrough
contribution: VulnGuard Prompt, a hybrid technique that prevents harmful
information generation using real-world data-driven contrastive learning.
VulnGuard integrates few-shot examples from curated GitHub repositories,
ethical chain-of-thought reasoning, and adaptive role-prompting to create
population-specific protective barriers. Our framework employs theoretical
multi-objective optimization with formal proofs demonstrating 25-30% analytical
harm reduction through entropy bounds and Pareto optimality. PromptGuard
orchestrates six core modules: Input Classification, VulnGuard Prompting,
Ethical Principles Integration, External Tool Interaction, Output Validation,
and User-System Interaction, creating an intelligent expert system for
real-time harm prevention. We provide comprehensive mathematical formalization
including convergence proofs, vulnerability analysis using information theory,
and theoretical validation framework using GitHub-sourced datasets,
establishing mathematical foundations for systematic empirical research.

</details>


### [41] [Similarity-based Outlier Detection for Noisy Object Re-Identification Using Beta Mixtures](https://arxiv.org/abs/2509.08926)
*Waqar Ahmad,Evan Murphy,Vladimir A. Krylov*

Main category: cs.CV

> 论文提出了一种新颖的方法Beta-SOD，用于解决具有标签噪声的Re-ID任务，该方法通过Siamese网络和基于Beta混合模型的异常检测技术提高了判别性的成对关系学习。

<details>
  <summary>Details</summary>

**Motivation:** 研究目的是解决Re-ID方法对标签噪声高度敏感的问题，这通常会导致显著的性能下降。

**Method:** Re-ID 问题被重新定义为一个监督图像相似性任务，并采用Siamese网络架构捕捉判别式的成对关系。核心是一个新颖的基于Beta混合模型的统计数据异常检测框架Beta-SOD，用于建模成对嵌入之间的余弦相似性。另外还提出了一种新颖的关于两个Beta分布混合模型的身份识别结果，确保了学习任务的良好定义性。

**Result:** 在CUHK03, Market-1501, 和VeRi-776数据集上验证了Beta-SOD在去噪和Re-ID任务上的有效性，尤其是人和车辆的Re-ID任务。

**Conclusion:** 此方法在不同的噪声水平下（10-30%）都优于最先进的方法，这体现了其在有噪声Re-ID场景下的鲁棒性和广泛适用性。

**Abstract:** Object re-identification (Re-ID) methods are highly sensitive to label noise,
which typically leads to significant performance degradation. We address this
challenge by reframing Re-ID as a supervised image similarity task and adopting
a Siamese network architecture trained to capture discriminative pairwise
relationships. Central to our approach is a novel statistical outlier detection
(OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier
Detection), which models the distribution of cosine similarities between
embedding pairs using a two-component Beta distribution mixture model. We
establish a novel identifiability result for mixtures of two Beta
distributions, ensuring that our learning task is well-posed.The proposed OD
step complements the Re-ID architecture combining binary cross-entropy,
contrastive, and cosine embedding losses that jointly optimize feature-level
similarity learning.We demonstrate the effectiveness of Beta-SOD in de-noising
and Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and
vehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance
compared to the state-of-the-art methods across various noise levels (10-30\%),
demonstrating both robustness and broad applicability in noisy Re-ID scenarios.
The implementation of Beta-SOD is available at:
https://github.com/waqar3411/Beta-SOD

</details>


### [42] [SFD-Mamba2Net: Strcture-Guided Frequency-Enhanced Dual-Stream Mamba2 Network for Coronary Artery Segmentation](https://arxiv.org/abs/2509.08934)
*Nan Mu,Ruiqi Song,Zhihui Xu,Jingfeng Jiang,Chen Zhao*

Main category: cs.CV

> An advanced framework, SFD-Mamba2Net, is proposed to improve the diagnostic accuracy of CAD by addressing challenges in ICA images through enhanced segmentation and detection mechanisms.

<details>
  <summary>Details</summary>

**Motivation:** The study aims to improve CAD diagnosis accuracy by addressing the limitations of ICA images such as low contrast and high noise.

**Method:** We propose SFD-Mamba2Net, an end-to-end framework for ICA-based vascular segmentation and stenosis detection. It has a CASE module in the encoder for highlighting vascular structures and a PHFP module in the decoder for refining high-frequency details.

**Result:** SFD-Mamba2Net showed better performance than existing methods in terms of segmentation metrics and stenosis detection.

**Conclusion:** The proposed SFD-Mamba2Net framework is effective in enhancing coronary artery segmentation and stenosis detection in ICA images.

**Abstract:** Background: Coronary Artery Disease (CAD) is one of the leading causes of
death worldwide. Invasive Coronary Angiography (ICA), regarded as the gold
standard for CAD diagnosis, necessitates precise vessel segmentation and
stenosis detection. However, ICA images are typically characterized by low
contrast, high noise levels, and complex, fine-grained vascular structures,
which pose significant challenges to the clinical adoption of existing
segmentation and detection methods. Objective: This study aims to improve the
accuracy of coronary artery segmentation and stenosis detection in ICA images
by integrating multi-scale structural priors, state-space-based long-range
dependency modeling, and frequency-domain detail enhancement strategies.
Methods: We propose SFD-Mamba2Net, an end-to-end framework tailored for
ICA-based vascular segmentation and stenosis detection. In the encoder, a
Curvature-Aware Structural Enhancement (CASE) module is embedded to leverage
multi-scale responses for highlighting slender tubular vascular structures,
suppressing background interference, and directing attention toward vascular
regions. In the decoder, we introduce a Progressive High-Frequency Perception
(PHFP) module that employs multi-level wavelet decomposition to progressively
refine high-frequency details while integrating low-frequency global
structures. Results and Conclusions: SFD-Mamba2Net consistently outperformed
state-of-the-art methods across eight segmentation metrics, and achieved the
highest true positive rate and positive predictive value in stenosis detection.

</details>


### [43] [Live(r) Die: Predicting Survival in Colorectal Liver Metastasis](https://arxiv.org/abs/2509.08935)
*Muhammad Alberb,Helen Cheung,Anne Martel*

Main category: cs.CV

> 本文提出一种新的框架，用于预测CRLM患者的手术结果，该框架包含自动分割算法和基于放射组学的生存预测模型，评估结果表明其优越性。

<details>
  <summary>Details</summary>

**Motivation:** 为了克服当前预后模型在预测弥漫性CRLM结果时缺乏足够预测性的局限性，作者提出了一种结合自动化分割算法和基于放射组学的生存分析的新方法，旨在提供更准确、更高效的注释以及可解释的预后预测结果。

**Method:** 本文提出了一种全自动框架，用于从术前和术后对比MRI预测手术结果。该框架包括分割管道和放射组学管道。分割管道学习从部分注释数据中分割肝脏、肿瘤和脾脏，利用可提示的基础模型来完成缺失标签。此外，作者提出了一种新型零样本3D提示传播算法（SAMONAI），显著提高了分割管道的准确性和效率。预测的对比前后分割结果随后被输入到放射组学管道中，从中提取每个肿瘤的特征并使用SurvAMINN进行生存预测。SurvAMINN是一种新型的自动编码器-多个实例神经网络，专注于预测最具侵袭性的肿瘤。

**Result:** 在包含227名患者的机构数据集上的广泛评估表明，该框架超过了现有的临床和基因组生物标志物，提高了10%以上的C指数。

**Conclusion:** 研究结果表明，结合自动分割算法和基于放射组学的生存分析为CRLM提供准确、高效注释和解释性结果预测具有潜力。

**Abstract:** Colorectal cancer frequently metastasizes to the liver, significantly
reducing long-term survival. While surgical resection is the only potentially
curative treatment for colorectal liver metastasis (CRLM), patient outcomes
vary widely depending on tumor characteristics along with clinical and genomic
factors. Current prognostic models, often based on limited clinical or
molecular features, lack sufficient predictive power, especially in multifocal
CRLM cases. We present a fully automated framework for surgical outcome
prediction from pre- and post-contrast MRI acquired before surgery. Our
framework consists of a segmentation pipeline and a radiomics pipeline. The
segmentation pipeline learns to segment the liver, tumors, and spleen from
partially annotated data by leveraging promptable foundation models to complete
missing labels. Also, we propose SAMONAI, a novel zero-shot 3D prompt
propagation algorithm that leverages the Segment Anything Model to segment 3D
regions of interest from a single point prompt, significantly improving our
segmentation pipeline's accuracy and efficiency. The predicted pre- and
post-contrast segmentations are then fed into our radiomics pipeline, which
extracts features from each tumor and predicts survival using SurvAMINN, a
novel autoencoder-based multiple instance neural network for survival analysis.
SurvAMINN jointly learns dimensionality reduction and hazard prediction from
right-censored survival data, focusing on the most aggressive tumors. Extensive
evaluation on an institutional dataset comprising 227 patients demonstrates
that our framework surpasses existing clinical and genomic biomarkers,
delivering a C-index improvement exceeding 10%. Our results demonstrate the
potential of integrating automated segmentation algorithms and radiomics-based
survival analysis to deliver accurate, annotation-efficient, and interpretable
outcome prediction in CRLM.

</details>


### [44] [Discovering Divergent Representations between Text-to-Image Models](https://arxiv.org/abs/2509.08940)
*Lisa Dunlap,Joseph E. Gonzalez,Trevor Darrell,Fabian Caba Heilbron,Josef Sivic,Bryan Russell*

Main category: cs.CV

> 研究分析了两种文本到图像生成模型中的视觉表示分歧，提出名为CompCon的算法来找到和分类这些分歧，并展示了其在发现模型生成差异方面的能力。

<details>
  <summary>Details</summary>

**Motivation:** 本文为了探索和理解两个不同的生成模型在学习视觉表征时出现分歧的情况，特别是要发现一种模型产生的图像中出现而另一种模型未出现的视觉属性及其对应的提示类型。

**Method:** 我们提出了CompCon（Comparing Concepts），这是一种进化搜索算法，用于发现一个生成模型的输出中比另一个模型更常见的视觉属性以及与这些视觉差异相关的提示概念。

**Result:** 通过创建一个自动数据生成管道来生产ID2数据集，包含60个输入依赖差异，CompcCon在其发现分化表征的能力上被证明优于多个基于LLM和VLM的基线方法。

**Conclusion:** 使用CompCon可以比较流行的文本到图像模型，并发现其中的分歧表示，例如PixArt以湿润的街道描绘孤独相关的提示词汇，而Stable Diffusion 3.5则以非洲裔美国人从事媒体职业图像来表达。

**Abstract:** In this paper, we investigate when and how visual representations learned by
two different generative models diverge. Given two text-to-image models, our
goal is to discover visual attributes that appear in images generated by one
model but not the other, along with the types of prompts that trigger these
attribute differences. For example, "flames" might appear in one model's
outputs when given prompts expressing strong emotions, while the other model
does not produce this attribute given the same prompts. We introduce CompCon
(Comparing Concepts), an evolutionary search algorithm that discovers visual
attributes more prevalent in one model's output than the other, and uncovers
the prompt concepts linked to these visual differences. To evaluate CompCon's
ability to find diverging representations, we create an automated data
generation pipeline to produce ID2, a dataset of 60 input-dependent
differences, and compare our approach to several LLM- and VLM-powered
baselines. Finally, we use CompCon to compare popular text-to-image models,
finding divergent representations such as how PixArt depicts prompts mentioning
loneliness with wet streets and Stable Diffusion 3.5 depicts African American
people in media professions. Code at: https://github.com/adobe-research/CompCon

</details>


### [45] [An U-Net-Based Deep Neural Network for Cloud Shadow and Sun-Glint Correction of Unmanned Aerial System (UAS) Imagery](https://arxiv.org/abs/2509.08949)
*Yibin Wang,Wondimagegn Beshah,Padmanava Dash,Haifeng Wang*

Main category: cs.CV

> 本文提出一种机器学习方法来识别并校正受云影和太阳眩光影响的UAS图像区域，以改进水质参数的估计。

<details>
  <summary>Details</summary>

**Motivation:** 随着无人驾驶航空系统（UAS）在近年来的广泛应用，其影像常受到云影和水面太阳眩光的影响，这对从UAS影像中估计水质参数带来了严重问题。

**Method:** 本研究提出了一种新的机器学习方法，首先识别和提取受云影和太阳眩光影响的区域，并将其与未受阻碍的晴空区域和不受太阳眩光影响的区域分开。从图像中按像素级别提取数据，训练了一种基于U-Net的深度学习模型，并根据测试案例的各种评估指标确定了最佳配置。

**Result:** 研究通过训练模型成功识别并校正了云影和太阳眩光区域，为水质参数估计提供了更准确的数据支持。

**Conclusion:** 通过这种评估，确定了一个高质量的图像校正模型，可以恢复影像中的云影和太阳眩光区域。

**Abstract:** The use of unmanned aerial systems (UASs) has increased tremendously in the
current decade. They have significantly advanced remote sensing with the
capability to deploy and image the terrain as per required spatial, spectral,
temporal, and radiometric resolutions for various remote sensing applications.
One of the major advantages of UAS imagery is that images can be acquired in
cloudy conditions by flying the UAS under the clouds. The limitation to the
technology is that the imagery is often sullied by cloud shadows. Images taken
over water are additionally affected by sun glint. These are two pose serious
issues for estimating water quality parameters from the UAS images. This study
proposes a novel machine learning approach first to identify and extract
regions with cloud shadows and sun glint and separate such regions from
non-obstructed clear sky regions and sun-glint unaffected regions. The data was
extracted from the images at pixel level to train an U-Net based deep learning
model and best settings for model training was identified based on the various
evaluation metrics from test cases. Using this evaluation, a high-quality image
correction model was determined, which was used to recover the cloud shadow and
sun glint areas in the images.

</details>


### [46] [CoSwin: Convolution Enhanced Hierarchical Shifted Window Attention For Small-Scale Vision](https://arxiv.org/abs/2509.08959)
*Puskal Khadka,Rodrigue Rizk,Longwei Wang,KC Santosh*

Main category: cs.CV

> 本文提出了CoSwin，一种新型的特征融合架构，通过在分级移位窗口注意力中引入局部卷积特征学习，解决了ViTs在小数据集上局部特征提取不足的问题，在多个图像分类任务上显著提高了性能。

<details>
  <summary>Details</summary>

**Motivation:** 视觉变压器（ViTs）通过利用自我注意力机制来建模长距离依赖关系，在计算机视觉领域取得了显著的成果。然而，这种全局上下文的重视往往以牺牲小数据集上的局部特征提取为代价，尤其是在缺乏局部性和平移等变性等归纳偏置的情况下。为了解决这个问题，我们提出了CoSwin。

**Method:** 提出了一种新的特征融合架构CoSwin，它在分级移位窗口注意力上加入局部卷积特征学习。具体地，CoSwin在每个注意力块中集成了一个可学习的局部特征增强模块，使模型能够同时捕捉细粒度的空间细节和全局语义结构。

**Result:** 在CIFAR-10、CIFAR-100、MNIST、SVHN和Tiny ImageNet等多个图像分类基准测试中，实验结果显示出比最先进的卷积模型和基于变压器的模型的性能增益。特别是在CIFAR-10上提高了2.17%，CIFAR-100上提高了4.92%，MNIST上提高了0.10%，SVHN上提高了0.26%，Tiny ImageNet上提高了4.47%。

**Conclusion:** 这些改进表明局部和全局特征融合的有效性，增强了变换器在小规模视觉任务中的泛化和鲁棒性。

**Abstract:** Vision Transformers (ViTs) have achieved impressive results in computer
vision by leveraging self-attention to model long-range dependencies. However,
their emphasis on global context often comes at the expense of local feature
extraction in small datasets, particularly due to the lack of key inductive
biases such as locality and translation equivariance. To mitigate this, we
propose CoSwin, a novel feature-fusion architecture that augments the
hierarchical shifted window attention with localized convolutional feature
learning. Specifically, CoSwin integrates a learnable local feature enhancement
module into each attention block, enabling the model to simultaneously capture
fine-grained spatial details and global semantic structure. We evaluate CoSwin
on multiple image classification benchmarks including CIFAR-10, CIFAR-100,
MNIST, SVHN, and Tiny ImageNet. Our experimental results show consistent
performance gains over state-of-the-art convolutional and transformer-based
models. Notably, CoSwin achieves improvements of 2.17% on CIFAR-10, 4.92% on
CIFAR-100, 0.10% on MNIST, 0.26% on SVHN, and 4.47% on Tiny ImageNet over the
baseline Swin Transformer. These improvements underscore the effectiveness of
local-global feature fusion in enhancing the generalization and robustness of
transformers for small-scale vision. Code and pretrained weights available at
https://github.com/puskal-khadka/coswin

</details>


### [47] [iMatcher: Improve matching in point cloud registration via local-to-global geometric consistency learning](https://arxiv.org/abs/2509.08982)
*Karim Slimani,Catherine Achard,Brahim Tamadazte*

Main category: cs.CV

> iMatcher是一种用于点云注册中特征匹配的全可微框架，通过预测几何一致性的置信矩阵提高配准性能，实验显示其在不同数据集上提供了高精度和高鲁棒性的匹配结果。

<details>
  <summary>Details</summary>

**Motivation:** 开发iMatcher的动机是改进刚性配准的性能，通过引入一种新的方法，该方法不仅能提供高精度的匹配，同时还能保持计算效率和稳定性。

**Method:** iMatcher 是一个用于点云配准中特征匹配的全可微框架。该方法使用学习到的特征来预测一个几何上一致的置信矩阵，兼顾了局部和全局的一致性。首先，通过局部图嵌入模块初始化得分矩阵。接着，通过在3D空间中使用最近邻搜索进行双向的源到目标和目标到源的匹配来进行重新定位步骤，优化矩阵。然后，配对的点特征一起堆叠并通过全局几何一致性学习进行优化，从而预测逐点的匹配概率。

**Result:** 实验结果表明，iMatcher在KITTI、KITTI-360、3DMatch、TUD-L和MVP-RG数据集上显著改善了刚性配准性能。方法达到了最先进的内点率，在KITTI上达到了95% - 97%，在KITTI-360上达到了94% - 97%，而在3DMatch上达到了81.1%，突显其在多种设置下的鲁棒性。

**Conclusion:** 结论指出iMatcher在点云配准中的性能表现卓越，无论是在户外还是室内环境中，都能提供高精度、稳健的匹配结果。

**Abstract:** This paper presents iMatcher, a fully differentiable framework for feature
matching in point cloud registration. The proposed method leverages learned
features to predict a geometrically consistent confidence matrix, incorporating
both local and global consistency. First, a local graph embedding module leads
to an initialization of the score matrix. A subsequent repositioning step
refines this matrix by considering bilateral source-to-target and
target-to-source matching via nearest neighbor search in 3D space. The paired
point features are then stacked together to be refined through global geometric
consistency learning to predict a point-wise matching probability. Extensive
experiments on real-world outdoor (KITTI, KITTI-360) and indoor (3DMatch)
datasets, as well as on 6-DoF pose estimation (TUD-L) and partial-to-partial
matching (MVP-RG), demonstrate that iMatcher significantly improves rigid
registration performance. The method achieves state-of-the-art inlier ratios,
scoring 95% - 97% on KITTI, 94% - 97% on KITTI-360, and up to 81.1% on 3DMatch,
highlighting its robustness across diverse settings.

</details>


### [48] [UltrON: Ultrasound Occupancy Networks](https://arxiv.org/abs/2509.08991)
*Magdalena Wysocki,Felix Duelmer,Ananya Bal,Nassir Navab,Mohammad Farid Azampour*

Main category: cs.CV

> 本文提出UltrON方法，通过利用B模式超声图像的声学特征，在弱监督优化条件下提升3D形状重建的几何一致性，并缓解了遮挡和标注依赖问题。

<details>
  <summary>Details</summary>

**Motivation:** 当前的隐式表示方法依赖于精确的标注，并忽视了B模式图像中的丰富声学信息，且无法很好地处理超声信号的视角依赖和声影伪影问题。

**Method:** 本文提出了一种基于占用率表示的方法UltrON，利用B模式超声图像中的声学特征来提升几何一致性，并提出了一种新的损失函数来补偿B模式图像的视角依赖性，从而实现多视角超声占用率优化。

**Result:** UltrON方法能够利用B模式图像中的声学特征进行3D形状重建，而无需额外的标注成本，并且能够推广到同一解剖结构的不同形状。

**Conclusion:** UltrON方法缓解了遮挡和稀疏标注带来的问题，并为更准确的3D重建铺平了道路。

**Abstract:** In free-hand ultrasound imaging, sonographers rely on expertise to mentally
integrate partial 2D views into 3D anatomical shapes. Shape reconstruction can
assist clinicians in this process. Central to this task is the choice of shape
representation, as it determines how accurately and efficiently the structure
can be visualized, analyzed, and interpreted. Implicit representations, such as
SDF and occupancy function, offer a powerful alternative to traditional voxel-
or mesh-based methods by modeling continuous, smooth surfaces with compact
storage, avoiding explicit discretization. Recent studies demonstrate that SDF
can be effectively optimized using annotations derived from segmented B-mode
ultrasound images. Yet, these approaches hinge on precise annotations,
overlooking the rich acoustic information embedded in B-mode intensity.
Moreover, implicit representation approaches struggle with the ultrasound's
view-dependent nature and acoustic shadowing artifacts, which impair
reconstruction. To address the problems resulting from occlusions and
annotation dependency, we propose an occupancy-based representation and
introduce \gls{UltrON} that leverages acoustic features to improve geometric
consistency in weakly-supervised optimization regime. We show that these
features can be obtained from B-mode images without additional annotation cost.
Moreover, we propose a novel loss function that compensates for view-dependency
in the B-mode images and facilitates occupancy optimization from multiview
ultrasound. By incorporating acoustic properties, \gls{UltrON} generalizes to
shapes of the same anatomy. We show that \gls{UltrON} mitigates the limitations
of occlusions and sparse labeling and paves the way for more accurate 3D
reconstruction. Code and dataset will be available at
https://github.com/magdalena-wysocki/ultron.

</details>


### [49] [Implicit Neural Representations of Intramyocardial Motion and Strain](https://arxiv.org/abs/2509.09004)
*Andrew Bell,Yan Kit Choi,Steffen Peterson,Andrew King,Muhummad Sohaib Nazir,Alistair Young*

Main category: cs.CV

> 研究提出了一种新的方法，使用INRs来预测左心室连续位移，该方法在追踪精度和应变误差方面优于其他方法，并且速度快380倍，适用于大规模的心肌应变分析。

<details>
  <summary>Details</summary>

**Motivation:** 目前自动量化心肌内部运动和应变的任务仍然具有挑战性，需要一种既准确又能良好扩展的方法来进行大规模CMR数据分析。

**Method:** 使用隐式神经表示（INRs），并依据学习到的潜在代码进行条件处理，来预测左心室（LV）的连续位移，且不需要在推理阶段进行优化。

**Result:** 该研究提出了一种使用隐式神经表示（INRs）预测左心室（LV）连续位移的方法，此方法在452个UK Biobank测试案例中表现出最佳的追踪精度和全球环周与径向应变的综合误差最低。该方法比最精确的基线模型大约快380倍。这表明，基于INR模型适用于大规模心脏磁共振数据集中的心肌应变分析。

**Conclusion:** 本研究提出了训练隐式神经表示（INRs）作为左心室（LV）位移回归器的方法，这是一种用于量化心肌应变的新途径，尤其适合大规模CMR数据分析。

**Abstract:** Automatic quantification of intramyocardial motion and strain from tagging
MRI remains an important but challenging task. We propose a method using
implicit neural representations (INRs), conditioned on learned latent codes, to
predict continuous left ventricular (LV) displacement -- without requiring
inference-time optimisation. Evaluated on 452 UK Biobank test cases, our method
achieved the best tracking accuracy (2.14 mm RMSE) and the lowest combined
error in global circumferential (2.86%) and radial (6.42%) strain compared to
three deep learning baselines. In addition, our method is $\sim$380$\times$
faster than the most accurate baseline. These results highlight the suitability
of INR-based models for accurate and scalable analysis of myocardial strain in
large CMR datasets.

</details>


### [50] [E-MLNet: Enhanced Mutual Learning for Universal Domain Adaptation with Sample-Specific Weighting](https://arxiv.org/abs/2509.09006)
*Samuel Felipe dos Santos,Tiago Agostinho de Almeida,Jurandy Almeida*

Main category: cs.CV

> 本文介绍E-MLNet通过改进的动态权重策略来提升UniDA任务中的性能。通过使分类器针对每个目标样本的最相关类边界进行适应，显著提高了模型的精度。

<details>
  <summary>Details</summary>

**Motivation:** 当前的方法如MLNet采用了一种银行的一对多分类器并通过开放集熵最小化来适应，但这种方式将所有分类器视为同等对待，削弱了学习信号。E-MLNet旨在解决这一问题。

**Method:** E-MLNet整合了动态加权策略到OEM中，通过利用闭集分类器的预测，E-MLNet在每个目标样本上的相关类边界上加强适应，从而加快了已知类别和未知类别之间的区分。

**Result:** 在四个具有挑战性的基准测试上进行的广泛实验表明，E-MLNet在VisDA和ImageCLEF上达到了最高的平均H分数，并显示出优于其前身的鲁棒性，在大多数适应任务中表现最佳。

**Conclusion:** E-MLNet通过其动态加权策略在多个开放设置的领域适应任务中优于MLNet，证明了针对相关类边界进行聚焦适应策略的有效性。

**Abstract:** Universal Domain Adaptation (UniDA) seeks to transfer knowledge from a
labeled source to an unlabeled target domain without assuming any relationship
between their label sets, requiring models to classify known samples while
rejecting unknown ones. Advanced methods like Mutual Learning Network (MLNet)
use a bank of one-vs-all classifiers adapted via Open-set Entropy Minimization
(OEM). However, this strategy treats all classifiers equally, diluting the
learning signal. We propose the Enhanced Mutual Learning Network (E-MLNet),
which integrates a dynamic weighting strategy to OEM. By leveraging the
closed-set classifier's predictions, E-MLNet focuses adaptation on the most
relevant class boundaries for each target sample, sharpening the distinction
between known and unknown classes. We conduct extensive experiments on four
challenging benchmarks: Office-31, Office-Home, VisDA-2017, and ImageCLEF. The
results demonstrate that E-MLNet achieves the highest average H-scores on VisDA
and ImageCLEF and exhibits superior robustness over its predecessor. E-MLNet
outperforms the strong MLNet baseline in the majority of individual adaptation
tasks -- 22 out of 31 in the challenging Open-Partial DA setting and 19 out of
31 in the Open-Set DA setting -- confirming the benefits of our focused
adaptation strategy.

</details>


### [51] [COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation](https://arxiv.org/abs/2509.09014)
*Umair Hassan*

Main category: cs.CV

> 本研究介绍了COCO-Urdu，一个大型的图像-标题数据集，旨在解决乌尔都语在多模态和视觉语言研究中资源匮乏的问题，以此减少多语言视觉模型的语言偏见。

<details>
  <summary>Details</summary>

**Motivation:** 由于缺乏大规模高质量的数据集，乌尔都语在多模态和视觉语言研究中严重不足。为了填补这一空白，提出COCO-Urdu数据集。

**Method:** 该数据集包含59,000张图像和319,000个乌尔都语标题，通过分层抽样以保留原始分布特征。使用SeamlessM4T v2翻译，并结合COMET-Kiwi, CLIP，以及BERTScore进行质量验证，迭代提高翻译质量。

**Result:** COCO-Urdu在BLEU, SacreBLEU, 和chrF上展示了稳定且优良的性能，评估了数据集的翻译质量和多模态一致性的结果。

**Conclusion:** COCO-Urdu是目前最大的公开乌尔都语标注数据集，其发布有助于减少多模态研究中的语言偏见，促进包容性的视觉语言系统的发展。

**Abstract:** Urdu, spoken by over 250 million people, remains critically under-served in
multimodal and vision-language research. The absence of large-scale,
high-quality datasets has limited the development of Urdu-capable systems and
reinforced biases in multilingual vision-language models trained primarily on
high-resource languages. To address this gap, we present COCO-Urdu, a
large-scale image-caption dataset derived from MS COCO, containing 59,000
images and 319,000 Urdu captions selected through stratified sampling to
preserve the original distribution. Captions were translated using SeamlessM4T
v2 and validated with a hybrid multimodal quality estimation framework that
integrates COMET-Kiwi for translation quality, CLIP-based similarity for visual
grounding, and BERTScore with back-translation for semantic consistency;
low-scoring captions were iteratively refined using open-source large language
models. We further benchmark COCO-Urdu on BLEU, SacreBLEU, and chrF, reporting
consistently strong results. To the best of our knowledge, COCO-Urdu is the
largest publicly available Urdu captioning dataset. By releasing both the
dataset and the quality estimation pipeline, we aim to reduce language bias in
multimodal research and establish a foundation for inclusive vision-language
systems.

</details>


### [52] [VoxelFormer: Parameter-Efficient Multi-Subject Visual Decoding from fMRI](https://arxiv.org/abs/2509.09015)
*Chenqian Le,Yilin Zhao,Nikasadat Emami,Kushagra Yadav,Xujin "Chris" Liu,Xupeng Chen,Yao Wang*

Main category: cs.CV

> 本研究提出一种轻量级的transformer模型VoxelFormer，实现多受试者fMRI视觉解码，展示了高效的参数利用和在自然场景数据集上的优秀性能。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于大多数fMRI视觉解码方法依赖于受试者特定的训练，这限制了扩展性和实际部署，本研究旨在通过VoxelFormer解决这一问题。

**Method:** 提出了一种轻量级的transformer架构VoxelFormer，用于多受试者的fMRI视觉解码。VoxelFormer结合了用于高效体素压缩的Token Merging Transformer（ToMer）和生成与CLIP图像嵌入空间对齐的固定大小神经表示的查询驱动Q-Former。

**Result:** 在7T自然场景数据集上的评估显示，VoxelFormer在训练包含的受试者上的检索性能具有竞争力，且参数量远少于现有方法。

**Conclusion:** 这些结果突显了token合并和查询驱动的transformers作为高效参数神经解码的有前景的方法。

**Abstract:** Recent advances in fMRI-based visual decoding have enabled compelling
reconstructions of perceived images. However, most approaches rely on
subject-specific training, limiting scalability and practical deployment. We
introduce \textbf{VoxelFormer}, a lightweight transformer architecture that
enables multi-subject training for visual decoding from fMRI. VoxelFormer
integrates a Token Merging Transformer (ToMer) for efficient voxel compression
and a query-driven Q-Former that produces fixed-size neural representations
aligned with the CLIP image embedding space. Evaluated on the 7T Natural Scenes
Dataset, VoxelFormer achieves competitive retrieval performance on subjects
included during training with significantly fewer parameters than existing
methods. These results highlight token merging and query-based transformers as
promising strategies for parameter-efficient neural decoding.

</details>


### [53] [Integrating Anatomical Priors into a Causal Diffusion Model](https://arxiv.org/abs/2509.09054)
*Binxu Li,Wei Peng,Mingjie Li,Ehsan Adeli,Kilian M. Pohl*

Main category: cs.CV

> 本文提出了一种新的概率因果图模型（PCGM），用于生成具有解剖学合理性的3D脑部MRI图像，提升了对微小解剖细节的保留。实验证明该模型生成的高质量脑部MRI图像能够有效复制神经科学的微小变化报告。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于现有的反事实模型难以生成保真度高的MRI图像，特别是当涉及到保持微小但医学相关的局部变化时。本研究旨在通过添加显式的解剖学约束，提高生成图像的质量。

**Method:** 该研究提出使用一种称为概率因果图模型（PCGM）的方法，该方法通过概率图模块捕捉解剖约束，并将其转换为发生轻微变异的区域的空间二进制掩码。掩码通过3D扩展的ControlNet编码，约束新的反事实去噪UNet，然后通过3D扩散解码器将其转换为高质量的脑部MRI图像。

**Result:** 研究通过引入概率因果图模型（PCGM）来改进现有的反事实图像生成技术，特别强调了在生成高质量3D大脑MRI图像时，保留微小解剖细节的重要性。该模型结合了三维扩展的ControlNet来编码区域掩码，这些掩码约束了新的反事实去噪UNet，然后再通过3D扩散解码器生成高质量的脑部MRI图像。实验结果显示PCGM能够在生成的结构化脑部MRI图像上超越多个基线方法，并且首次表明从PCGM生成的反事实图中提取的大脑测量能够复制出神经科学文献中先前报道的疾病对皮层脑区的微小影响。

**Conclusion:** 本研究的发现不仅提高了生成的3D脑部MRI图像的质量，还验证了合成MRI图像在研究微妙形态差异中的有效性，为未来的医学研究提供了新的手段。

**Abstract:** 3D brain MRI studies often examine subtle morphometric differences between
cohorts that are hard to detect visually. Given the high cost of MRI
acquisition, these studies could greatly benefit from image syntheses,
particularly counterfactual image generation, as seen in other domains, such as
computer vision. However, counterfactual models struggle to produce
anatomically plausible MRIs due to the lack of explicit inductive biases to
preserve fine-grained anatomical details. This shortcoming arises from the
training of the models aiming to optimize for the overall appearance of the
images (e.g., via cross-entropy) rather than preserving subtle, yet medically
relevant, local variations across subjects. To preserve subtle variations, we
propose to explicitly integrate anatomical constraints on a voxel-level as
prior into a generative diffusion framework. Called Probabilistic Causal Graph
Model (PCGM), the approach captures anatomical constraints via a probabilistic
graph module and translates those constraints into spatial binary masks of
regions where subtle variations occur. The masks (encoded by a 3D extension of
ControlNet) constrain a novel counterfactual denoising UNet, whose encodings
are then transferred into high-quality brain MRIs via our 3D diffusion decoder.
Extensive experiments on multiple datasets demonstrate that PCGM generates
structural brain MRIs of higher quality than several baseline approaches.
Furthermore, we show for the first time that brain measurements extracted from
counterfactuals (generated by PCGM) replicate the subtle effects of a disease
on cortical brain regions previously reported in the neuroscience literature.
This achievement is an important milestone in the use of synthetic MRIs in
studies investigating subtle morphological differences.

</details>


### [54] [Enhancing 3D Medical Image Understanding with Pretraining Aided by 2D Multimodal Large Language Models](https://arxiv.org/abs/2509.09064)
*Qiuhui Chen,Xuancheng Yao,Huping Ye,Yi Hong*

Main category: cs.CV

> Med3DInsight 是一种新的预训练框架，通过结合3D图像编码技术与2D多模式大语言模型，显著提升了3D医学图像的理解，并在分割和分类任务上表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 目前的3D医学图像的卷积和基于Transformer的自监督学习方法在语义理解方面存在不足，通过利用多模式大语言模型，研究者们试图提升3D医学图像的理解水平。

**Method:** Med3DInsight框架结合了3D图像编码器和2D多模式大语言模型，并引入了一种特殊设计的平面切片感知变压器模块以提高理解能力。该框架还使用了基于部分最优传输的对齐方法，以增强对潜在噪声的容忍度。

**Result:** 'Understanding 3D Medical Images with Med3DInsight' 提出了一种新的预训练框架Med3DInsight，该框架通过将3D图像编码器与2D多模式大语言模型（MLLMs）结合，并引入平面切片感知的变压器模块，以提高3D医学图像的理解。实验表明，该方法在分割和分类任务上表现出色，超越了现有的自监督学习方法。此外，Med3DInsight的源代码、生成的数据集和预训练模型可以公开获取。

**Conclusion:** Med3DInsight提出了一种全新的可扩展的多模式3D医学表示学习范式，无需人类标注。它可以在现有的3D医学图像理解网络中无缝集成，有望显著提升这些网络的性能。

**Abstract:** Understanding 3D medical image volumes is critical in the medical field, yet
existing 3D medical convolution and transformer-based self-supervised learning
(SSL) methods often lack deep semantic comprehension. Recent advancements in
multimodal large language models (MLLMs) provide a promising approach to
enhance image understanding through text descriptions. To leverage these 2D
MLLMs for improved 3D medical image understanding, we propose Med3DInsight, a
novel pretraining framework that integrates 3D image encoders with 2D MLLMs via
a specially designed plane-slice-aware transformer module. Additionally, our
model employs a partial optimal transport based alignment, demonstrating
greater tolerance to noise introduced by potential noises in LLM-generated
content. Med3DInsight introduces a new paradigm for scalable multimodal 3D
medical representation learning without requiring human annotations. Extensive
experiments demonstrate our state-of-the-art performance on two downstream
tasks, i.e., segmentation and classification, across various public datasets
with CT and MRI modalities, outperforming current SSL methods. Med3DInsight can
be seamlessly integrated into existing 3D medical image understanding networks,
potentially enhancing their performance. Our source code, generated datasets,
and pre-trained models will be available at
https://github.com/Qybc/Med3DInsight.

</details>


### [55] [Improvement of Human-Object Interaction Action Recognition Using Scene Information and Multi-Task Learning Approach](https://arxiv.org/abs/2509.09067)
*Hesham M. Shehata,Mohammad Abdolrahmani*

Main category: cs.CV

> 本研究通过结合环境中的固定物体信息与多任务学习策略，显著提升了人类与物体交互行为识别的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 当前的基于图卷积神经网络（GCNs）的人类动作识别方法在识别人类与物体交互情况时表现不佳，原因在于缺少有效表示场景信息及相应的学习架构。

**Method:** 本研究提出了一种利用固定物体信息和多任务学习方法来改进基于人类骨架姿态的人类动作识别性能的方法。

**Result:** 利用多任务学习方法和交互区域信息，该方法在识别所研究的交互动作和非交互动作时，取得了99.25%的准确率，比仅使用人类骨架姿态的基线模型提高了2.75%。

**Conclusion:** 研究结果表明，基于固定物体信息和多任务学习的人类动作识别方法能够有效提高人类与物体交互行为识别的准确性。

**Abstract:** Recent graph convolutional neural networks (GCNs) have shown high performance
in the field of human action recognition by using human skeleton poses.
However, it fails to detect human-object interaction cases successfully due to
the lack of effective representation of the scene information and appropriate
learning architectures. In this context, we propose a methodology to utilize
human action recognition performance by considering fixed object information in
the environment and following a multi-task learning approach. In order to
evaluate the proposed method, we collected real data from public environments
and prepared our data set, which includes interaction classes of hands-on fixed
objects (e.g., ATM ticketing machines, check-in/out machines, etc.) and
non-interaction classes of walking and standing. The multi-task learning
approach, along with interaction area information, succeeds in recognizing the
studied interaction and non-interaction actions with an accuracy of 99.25%,
outperforming the accuracy of the base model using only human skeleton poses by
2.75%.

</details>


### [56] [IRDFusion: Iterative Relation-Map Difference guided Feature Fusion for Multispectral Object Detection](https://arxiv.org/abs/2509.09085)
*Jifeng Shen,Haibo Zhan,Xin Zuo,Heng Fan,Xiaohui Yuan,Jun Li,Wankou Yang*

Main category: cs.CV

> The paper introduces IRDFusion, a feature fusion framework designed to enhance salient structures and suppress background noise in multispectral object detection, demonstrating state-of-the-art performance across various challenging scenarios.

<details>
  <summary>Details</summary>

**Motivation:** Existing multispectral object detection methods struggle with retaining unnecessary background or noise, which affects the perceptual performance. The motivation is to develop a method that can enhance the salient structures while suppressing background noise during feature fusion.

**Method:** The paper proposes IRDFusion, a feature fusion framework based on cross-modal feature contrastive and screening strategy. It includes two modules: the Mutual Feature Refinement Module (MFRM) and the Differential Feature Feedback Module (DFFM). MFRM improves cross-modal alignment, and DFFM, inspired by feedback differential amplifiers, computes differential features to reinforce MFRM.

**Result:** IRDFusion achieves state-of-the-art performance on FLIR, LLVIP, and M$^3$FD datasets, outperforming existing methods in a range of challenging conditions, thus confirming the effectiveness and robustness of the proposed framework.

**Conclusion:** The proposed IRDFusion framework significantly enhances cross-modal feature fusion by adaptively amplifying salient relational signals and suppressing feature noise, proving robustness and effectiveness across diverse scenarios in multispectral object detection.

**Abstract:** Current multispectral object detection methods often retain extraneous
background or noise during feature fusion, limiting perceptual performance.To
address this, we propose an innovative feature fusion framework based on
cross-modal feature contrastive and screening strategy, diverging from
conventional approaches. The proposed method adaptively enhances salient
structures by fusing object-aware complementary cross-modal features while
suppressing shared background interference.Our solution centers on two novel,
specially designed modules: the Mutual Feature Refinement Module (MFRM) and the
Differential Feature Feedback Module (DFFM). The MFRM enhances intra- and
inter-modal feature representations by modeling their relationships, thereby
improving cross-modal alignment and discriminative power.Inspired by feedback
differential amplifiers, the DFFM dynamically computes inter-modal differential
features as guidance signals and feeds them back to the MFRM, enabling adaptive
fusion of complementary information while suppressing common-mode noise across
modalities. To enable robust feature learning, the MFRM and DFFM are integrated
into a unified framework, which is formally formulated as an Iterative
Relation-Map Differential Guided Feature Fusion mechanism, termed IRDFusion.
IRDFusion enables high-quality cross-modal fusion by progressively amplifying
salient relational signals through iterative feedback, while suppressing
feature noise, leading to significant performance gains.In extensive
experiments on FLIR, LLVIP and M$^3$FD datasets, IRDFusion achieves
state-of-the-art performance and consistently outperforms existing methods
across diverse challenging scenarios, demonstrating its robustness and
effectiveness. Code will be available at
https://github.com/61s61min/IRDFusion.git.

</details>


### [57] [SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models](https://arxiv.org/abs/2509.09090)
*Hengyu Fang,Yijiang Liu,Yuan Du,Li Du,Huanrui Yang*

Main category: cs.CV

> 本文提出了SQAP-VLA，一个训练无依赖的VLA推理加速框架，它能够在保持模型核心性能的同时，显著提升计算效率和推理速度。

<details>
  <summary>Details</summary>

**Motivation:** 尽管Vision-Language-Action (VLA) 模型展示了前所未有的智能表现，但其高昂的计算和内存成本阻碍了它们的实际应用。现有的压缩和加速方法缺乏同时进行量化和token剪枝的能力。

**Method:** 通过设计一个训练无依赖的VLA推理加速框架SQAP-VLA，该框架同时启用最先进的量化和token剪枝。为了解决两者之间的不兼容问题，提出了新的量化感知token剪枝标准，并改进了量化器设计以增强剪枝效果。

**Result:** 应用SQAP-VLA到标准VLA模型可以实现$	imes$1.93的加速比，并且平均成功率提高了4.5%。

**Conclusion:** SQAP-VLA成功地同时实现了量化和token剪枝，解决了二者之间的不兼容问题，从而为VLA模型的实际部署提供了可能。

**Abstract:** Vision-Language-Action (VLA) models exhibit unprecedented capabilities for
embodied intelligence. However, their extensive computational and memory costs
hinder their practical deployment. Existing VLA compression and acceleration
approaches conduct quantization or token pruning in an ad-hoc manner but fail
to enable both for a holistic efficiency improvement due to an observed
incompatibility. This work introduces SQAP-VLA, the first structured,
training-free VLA inference acceleration framework that simultaneously enables
state-of-the-art quantization and token pruning. We overcome the
incompatibility by co-designing the quantization and token pruning pipeline,
where we propose new quantization-aware token pruning criteria that work on an
aggressively quantized model while improving the quantizer design to enhance
pruning effectiveness. When applied to standard VLA models, SQAP-VLA yields
significant gains in computational efficiency and inference speed while
successfully preserving core model performance, achieving a $\times$1.93
speedup and up to a 4.5\% average success rate enhancement compared to the
original model.

</details>


### [58] [S-BEVLoc: BEV-based Self-supervised Framework for Large-scale LiDAR Global Localization](https://arxiv.org/abs/2509.09110)
*Chenghao Zhang,Lun Luo,Si-Yuan Cao,Xiaokai Bai,Yuncheng Jin,Zhu Yu,Beinan Yu,Yisen Wang,Hui-Liang Shen*

Main category: cs.CV

> 本文提出了一种名为S-BEVLoc的新框架，该框架使用自监督学习实现LiDAR全局定位，通过构建关键点中心BEV补丁间的地理距离三元组，使用CNN和NetVLAD技术，以及SoftCos损失函数，展示了在大规模数据集上的卓越效果。

<details>
  <summary>Details</summary>

**Motivation:** 目前的方法依赖于来自GPS或SLAM里程计的高精度真实姿态进行网络训练，这需要巨大的成本和努力。为了降低这一成本并提高可扩展性，本文提出了S-BEVLoc框架，以代替高精度真实姿态的需求，并提供高度可扩展性。

**Method:** 提出了一种基于鸟瞰图（BEV）的自监督框架S-BEVLoc，用于LiDAR全局定位。该框架通过利用关键点中心BEV补丁间的已知地理距离，从单个BEV图像构建训练三元组，使用CNN提取局部特征，并采用NetVLAD聚合全局描述符，引入SoftCos损失以增强从生成的三元组中的学习。

**Result:** 实验结果表明，S-BEVLoc在大规模的KITTI和NCLT数据集上实现了最先进的性能，特别是在地点识别、闭环检测和全局定位任务中，没有使用高精度真实姿态的情况下达到了卓越的效果。

**Conclusion:** S-BEVLoc是一种有效的自监督框架，无需依赖高精度地面真值姿态即可实现高性能的LiDAR全局定位，同时具有显著的可扩展性优势。

**Abstract:** LiDAR-based global localization is an essential component of simultaneous
localization and mapping (SLAM), which helps loop closure and re-localization.
Current approaches rely on ground-truth poses obtained from GPS or SLAM
odometry to supervise network training. Despite the great success of these
supervised approaches, substantial cost and effort are required for
high-precision ground-truth pose acquisition. In this work, we propose
S-BEVLoc, a novel self-supervised framework based on bird's-eye view (BEV) for
LiDAR global localization, which eliminates the need for ground-truth poses and
is highly scalable. We construct training triplets from single BEV images by
leveraging the known geographic distances between keypoint-centered BEV
patches. Convolutional neural network (CNN) is used to extract local features,
and NetVLAD is employed to aggregate global descriptors. Moreover, we introduce
SoftCos loss to enhance learning from the generated triplets. Experimental
results on the large-scale KITTI and NCLT datasets show that S-BEVLoc achieves
state-of-the-art performance in place recognition, loop closure, and global
localization tasks, while offering scalability that would require extra effort
for supervised approaches.

</details>


### [59] [FPI-Det: a face--phone Interaction Dataset for phone-use detection and understanding](https://arxiv.org/abs/2509.09111)
*Jianqin Gao,Tianqi Wang,Yu Zhang,Yishu Zhang,Chenyuan Wang,Allan Dong,Zihao Wang*

Main category: cs.CV

> 本文介绍了一个名为FPI-Det的新数据集，该数据集专门用于检测人们是否在使用手机，并分析了不同的检测器在该数据集上的表现。

<details>
  <summary>Details</summary>

**Motivation:** 随着移动设备的广泛使用，对于安全监控、工作场所生产率评估和注意力管理的视觉系统提出了新的挑战。现有的通用基准数据集无法完全捕捉到人类和设备之间的细粒度交互。

**Method:** 本文提出了FPI-Det数据集，包含了22,879张图片，其中标注了人脸和手机的位置，这些图片来自工作场所、教育、交通和公共场景。数据集的特点是包括了极端的尺寸变化、频繁的遮挡以及不同的拍摄条件。

**Result:** 文章评估了YOLO和DETR等具有代表性的检测器在FPI-Det数据集上的性能，并提供了基础结果。

**Conclusion:** FPI-Det数据集为研究人类-设备交互提供了丰富的资源，有助于改进人脸识别和设备使用的检测技术。

**Abstract:** The widespread use of mobile devices has created new challenges for vision
systems in safety monitoring, workplace productivity assessment, and attention
management. Detecting whether a person is using a phone requires not only
object recognition but also an understanding of behavioral context, which
involves reasoning about the relationship between faces, hands, and devices
under diverse conditions. Existing generic benchmarks do not fully capture such
fine-grained human--device interactions. To address this gap, we introduce the
FPI-Det, containing 22{,}879 images with synchronized annotations for faces and
phones across workplace, education, transportation, and public scenarios. The
dataset features extreme scale variation, frequent occlusions, and varied
capture conditions. We evaluate representative YOLO and DETR detectors,
providing baseline results and an analysis of performance across object sizes,
occlusion levels, and environments. Source code and dataset is available at
https://github.com/KvCgRv/FPI-Det.

</details>


### [60] [Zero-shot Hierarchical Plant Segmentation via Foundation Segmentation Models and Text-to-image Attention](https://arxiv.org/abs/2509.09116)
*Junhao Xing,Ryohei Miyakawa,Yang Yang,Xinpeng Liu,Risa Shinoda,Hiroaki Santo,Yosuke Toda,Fumio Okura*

Main category: cs.CV

> 提出 ZeroPlantSeg 方法，用于顶视图图像中从零样本分割植物个体，该方法超越现有零样本方法并在跨域性能上优于监督方法。

<details>
  <summary>Details</summary>

**Motivation:** 尽管基础分割模型可以在零样本情况下从顶视图作物图像中合理提取叶片实例，但对包含多个重叠叶片的整个植物个体进行分割仍然具有挑战性，这通常需要物种特异性的标注训练数据，且需要大量的人力。

**Method:** ZeroPlantSeg 方法结合了基础分割模型和视觉语言模型，利用前者从顶视图作物图像中提取叶片实例，利用后者推理植物结构以提取整个植物个体，无需额外训练。

**Result:** 在包含多种植物种类、生长阶段和拍摄环境的数据集上的评估表明，该方法优于现有的零样本方法，并且跨域表现优于监督方法。

**Conclusion:** ZeroPlantSeg 方法证明了其在没有标注训练数据的情况下，能够有效进行植物个体分割的能力，展示了其在植物学研究和农业应用中的潜力。

**Abstract:** Foundation segmentation models achieve reasonable leaf instance extraction
from top-view crop images without training (i.e., zero-shot). However,
segmenting entire plant individuals with each consisting of multiple
overlapping leaves remains challenging. This problem is referred to as a
hierarchical segmentation task, typically requiring annotated training
datasets, which are often species-specific and require notable human labor. To
address this, we introduce ZeroPlantSeg, a zero-shot segmentation for
rosette-shaped plant individuals from top-view images. We integrate a
foundation segmentation model, extracting leaf instances, and a vision-language
model, reasoning about plants' structures to extract plant individuals without
additional training. Evaluations on datasets with multiple plant species,
growth stages, and shooting environments demonstrate that our method surpasses
existing zero-shot methods and achieves better cross-domain performance than
supervised methods. Implementations are available at
https://github.com/JunhaoXing/ZeroPlantSeg.

</details>


### [61] [Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval](https://arxiv.org/abs/2509.09118)
*Tianlu Zheng,Yifan Zhang,Xiang An,Ziyong Feng,Kaicheng Yang,Qichuan Ding*

Main category: cs.CV

> 通过改进数据构建和模型架构，本文解决了CLIP在人物表示学习中的两个关键挑战，提出了WebPerson数据集和GA-DMS框架，实现了跨多个基准测试的最佳性能。

<details>
  <summary>Details</summary>

**Motivation:** 解决CLIP在面对人物表示学习时面临的两个关键挑战：人物图象数据的匮乏和全局对比学习中难以保持局部特征以实现细粒度匹配的问题。

**Method:** 开发了一种抗噪声的数据构建管道，自动过滤和标记网络数据，创建了WebPerson数据集；引入了GA-DMS框架，通过自适应遮盖文本词元来增强跨模态对齐，并添加了遮盖词元预测目标。

**Result:** GA-DMS在多个基准测试中达到了最先进的性能表现。

**Conclusion:** 通过数据集和模型架构的改进，GA-DMS框架显著提升了CLIP在人物表示学习中的性能。

**Abstract:** Although Contrastive Language-Image Pre-training (CLIP) exhibits strong
performance across diverse vision tasks, its application to person
representation learning faces two critical challenges: (i) the scarcity of
large-scale annotated vision-language data focused on person-centric images,
and (ii) the inherent limitations of global contrastive learning, which
struggles to maintain discriminative local features crucial for fine-grained
matching while remaining vulnerable to noisy text tokens. This work advances
CLIP for person representation learning through synergistic improvements in
data curation and model architecture. First, we develop a noise-resistant data
construction pipeline that leverages the in-context learning capabilities of
MLLMs to automatically filter and caption web-sourced images. This yields
WebPerson, a large-scale dataset of 5M high-quality person-centric image-text
pairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking
Synergetic) framework, which improves cross-modal alignment by adaptively
masking noisy textual tokens based on the gradient-attention similarity score.
Additionally, we incorporate masked token prediction objectives that compel the
model to predict informative text tokens, enhancing fine-grained semantic
representation learning. Extensive experiments show that GA-DMS achieves
state-of-the-art performance across multiple benchmarks.

</details>


### [62] [ALL-PET: A Low-resource and Low-shot PET Foundation Model in the Projection Domain](https://arxiv.org/abs/2509.09130)
*Bin Huang,Kang Chen,Bingxuan Li,Huafeng Liu,Qiegen Liu*

Main category: cs.CV

> 

<details>
  <summary>Details</summary>

**Motivation:** 

**Method:** 

**Result:** {
  "tldr": "我们提出了ALL-PET，一种在投影域直接操作的低资源、少镜头PET基础模型，通过创新策略克服数据稀缺和效率低下的问题，并在实验中显示了高水平的性能。",
  "motivation": "现有的大规模PET建模受限于标注数据不足和计算资源有限，本文旨在通过特定策略提高数据利用率和模型整体性能。",
  "method": "ALL-PET使用潜变量扩散模型（LDM），结合Radon掩码增强策略（RMAS）、动态多掩码（DMM）机制和透明医疗注意力机制（TMA）提高生成质量和几何一致性。",
  "result": "实验结果表明，使用仅500个样本，ALL-PET即可生成高质量的sinogram图像，并在性能上与大规模数据集训练的模型相当。",
  "conclusion": "ALL-PET展示了在低资源限制下实现高效和性能优良的PET建模的能力，适用于多种任务且记忆使用低于24GB。提供了一种新的方向来优化PET成像的计算基础模型。\n"}
}

**Conclusion:** 

**Abstract:** Building large-scale foundation model for PET imaging is hindered by limited
access to labeled data and insufficient computational resources. To overcome
data scarcity and efficiency limitations, we propose ALL-PET, a low-resource,
low-shot PET foundation model operating directly in the projection domain.
ALL-PET leverages a latent diffusion model (LDM) with three key innovations.
First, we design a Radon mask augmentation strategy (RMAS) that generates over
200,000 structurally diverse training samples by projecting randomized
image-domain masks into sinogram space, significantly improving generalization
with minimal data. This is extended by a dynamic multi-mask (DMM) mechanism
that varies mask quantity and distribution, enhancing data diversity without
added model complexity. Second, we implement positive/negative mask constraints
to embed strict geometric consistency, reducing parameter burden while
preserving generation quality. Third, we introduce transparent medical
attention (TMA), a parameter-free, geometry-driven mechanism that enhances
lesion-related regions in raw projection data. Lesion-focused attention maps
are derived from coarse segmentation, covering both hypermetabolic and
hypometabolic areas, and projected into sinogram space for physically
consistent guidance. The system supports clinician-defined ROI adjustments,
ensuring flexible, interpretable, and task-adaptive emphasis aligned with PET
acquisition physics. Experimental results show ALL-PET achieves high-quality
sinogram generation using only 500 samples, with performance comparable to
models trained on larger datasets. ALL-PET generalizes across tasks including
low-dose reconstruction, attenuation correction, delayed-frame prediction, and
tracer separation, operating efficiently with memory use under 24GB.

</details>


### [63] [Noise-Robust Topology Estimation of 2D Image Data via Neural Networks and Persistent Homology](https://arxiv.org/abs/2509.09140)
*Dylan Peek,Matthew P. Skerritt,Stephan Chalup*

Main category: cs.CV

> ANNs在预测2D二值图像中贝蒂数时对噪声的鲁棒性优于PH方法，显示出在结构噪声下进行拓扑估计的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 研究目的是探讨ANNs在拓扑结构分析中的噪声鲁棒性，及其相比现有PH方法的性能差异。

**Method:** 通过对比人工神经网络(ANNs)和基于立方复形及符号欧几里得距离变换(SEDT)的持久同源性(PH)方法，研究了ANNs对2D二值图像中贝蒂数进行预测的噪声鲁棒性。

**Result:** 研究表明，在含有结构噪声的条件下，ANNs在预测2D二值图像中的贝蒂数时优于PH方法。

**Conclusion:** 尽管ANNs在拓扑估计中的应用尚处于起步阶段，但在结构噪声存在的情况下，它为PH方法提供了一种有吸引力的替代方案。

**Abstract:** Persistent Homology (PH) and Artificial Neural Networks (ANNs) offer
contrasting approaches to inferring topological structure from data. In this
study, we examine the noise robustness of a supervised neural network trained
to predict Betti numbers in 2D binary images. We compare an ANN approach
against a PH pipeline based on cubical complexes and the Signed Euclidean
Distance Transform (SEDT), which is a widely adopted strategy for noise-robust
topological analysis. Using one synthetic and two real-world datasets, we show
that ANNs can outperform this PH approach under noise, likely due to their
capacity to learn contextual and geometric priors from training data. Though
still emerging, the use of ANNs for topology estimation offers a compelling
alternative to PH under structural noise.

</details>


### [64] [Objectness Similarity: Capturing Object-Level Fidelity in 3D Scene Evaluation](https://arxiv.org/abs/2509.09143)
*Yuiko Uchida,Ren Togo,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

> 论文提出了一个新的评估3D场景的指标OSIM，该指标更好地反映了人类对场景中对象的感知。

<details>
  <summary>Details</summary>

**Motivation:** 现有指标主要评估整体图像质量，导致与人类感知不一致。论文的动机是基于神经心理学的研究成果，认为人类对3D场景的识别涉及对个体对象的关注。

**Method:** 该论文提出了一个评估3D场景的新指标Objectness SIMilarity (OSIM)，重点在于评估场景中的“对象”。它利用对象检测模型及其特征表示来量化每个对象在场景中的“对象性”。

**Result:** 用户研究表明，OSIM与人类感知的匹配度高于现有指标。此外，论文还分析了OSIM的特性，并重新评估了近期的3D重建和生成模型的表现。

**Conclusion:** OSIM作为一种新的评价指标，在评估3D场景的对象性时提供了更接近人类感知的结果，有助于该领域的进一步发展。

**Abstract:** This paper presents Objectness SIMilarity (OSIM), a novel evaluation metric
for 3D scenes that explicitly focuses on "objects," which are fundamental units
of human visual perception. Existing metrics assess overall image quality,
leading to discrepancies with human perception. Inspired by neuropsychological
insights, we hypothesize that human recognition of 3D scenes fundamentally
involves attention to individual objects. OSIM enables object-centric
evaluations by leveraging an object detection model and its feature
representations to quantify the "objectness" of each object in the scene. Our
user study demonstrates that OSIM aligns more closely with human perception
compared to existing metrics. We also analyze the characteristics of OSIM using
various approaches. Moreover, we re-evaluate recent 3D reconstruction and
generation models under a standardized experimental setup to clarify
advancements in this field. The code is available at
https://github.com/Objectness-Similarity/OSIM.

</details>


### [65] [Video Understanding by Design: How Datasets Shape Architectures and Insights](https://arxiv.org/abs/2509.09151)
*Lei Wang,Piotr Koniusz,Yongsheng Gao*

Main category: cs.CV

> 通过对视频理解中数据集特性的深入分析，重新解读了模型设计的发展历程，并提供了未来模型设计的指导原则。

<details>
  <summary>Details</summary>

**Motivation:** 当前的视频理解研究大多按任务或模型分类，忽略了数据集对模型架构演变的结构性影响。因此，本文旨在通过一种新的角度，来系统性地回顾视频理解模型的发展历史，并提供未来研究方向的指导。

**Method:** 提出了一种以数据集为驱动的视角，分析视频理解领域中模型设计的演变。本文根据数据集的运动复杂性、时间跨度、层级组合以及多模态丰富性等因素来引导模型结构的演化，重新解读了从双流模型、3D CNNs到序列模型、Transformer和多模态基础模型的发展历程。

**Result:** 通过把数据集、归纳偏差和架构统一在一个连贯的框架内，本文为视频理解模型的设计提供了新的视角，既回顾了历史，也指明了未来的发展方向。

**Conclusion:** 本文提出了一种以数据集为驱动的分析方法，可以对未来视频理解模型的研发提供有价值的指导和启示。

**Abstract:** Video understanding has advanced rapidly, fueled by increasingly complex
datasets and powerful architectures. Yet existing surveys largely classify
models by task or family, overlooking the structural pressures through which
datasets guide architectural evolution. This survey is the first to adopt a
dataset-driven perspective, showing how motion complexity, temporal span,
hierarchical composition, and multimodal richness impose inductive biases that
models should encode. We reinterpret milestones, from two-stream and 3D CNNs to
sequential, transformer, and multimodal foundation models, as concrete
responses to these dataset-driven pressures. Building on this synthesis, we
offer practical guidance for aligning model design with dataset invariances
while balancing scalability and task demands. By unifying datasets, inductive
biases, and architectures into a coherent framework, this survey provides both
a comprehensive retrospective and a prescriptive roadmap for advancing
general-purpose video understanding.

</details>


### [66] [OCELOT 2023: Cell Detection from Cell-Tissue Interaction Challenge](https://arxiv.org/abs/2509.09153)
*JaeWoong Shin,Jeongun Ryu,Aaron Valero Puche,Jinhee Lee,Biagio Brattoli,Wonkyung Jung,Soo Ick Cho,Kyunghyun Paeng,Chan-Young Ock,Donggeun Yoo,Zhaoyang Li,Wangkai Li,Huayu Mai,Joshua Millward,Zhen He,Aiden Nibali,Lydia Anette Schoenpflug,Viktor Hendrik Koelzer,Xu Shuoyu,Ji Zheng,Hu Bin,Yu-Wen Lo,Ching-Hui Yang,Sérgio Pereira*

Main category: cs.CV

> The OCELOT 2023 challenge addresses the need for multi-scale semantics in deep learning models for cell detection and tissue segmentation, showcasing significant performance improvements by incorporating cell-tissue interactions.

<details>
  <summary>Details</summary>

**Motivation:** The lack of datasets with multi-scale overlapping cell and tissue annotations hinders the development of deep learning models that can replicate the behavior of pathologists who examine multiple magnifications for diagnosis. The challenge aims to validate the importance of cell-tissue interactions for achieving human-level performance.

**Method:** The OCELOT 2023 challenge provided a dataset with overlapping annotations from six organs and evaluated methods that integrated cell and tissue analysis to improve cell detection and tissue segmentation.

**Result:** Top-performing models showed a significant enhancement in F1-score on the test set, achieving an up to 7.99 increase in performance compared to baseline approaches that neglect cell-tissue relationships.

**Conclusion:** The integration of multi-scale cell and tissue semantics in deep learning models demonstrated substantial performance improvements, confirming the necessity to consider these relationships for future improvements in medical imaging analysis.

**Abstract:** Pathologists routinely alternate between different magnifications when
examining Whole-Slide Images, allowing them to evaluate both broad tissue
morphology and intricate cellular details to form comprehensive diagnoses.
However, existing deep learning-based cell detection models struggle to
replicate these behaviors and learn the interdependent semantics between
structures at different magnifications. A key barrier in the field is the lack
of datasets with multi-scale overlapping cell and tissue annotations. The
OCELOT 2023 challenge was initiated to gather insights from the community to
validate the hypothesis that understanding cell and tissue (cell-tissue)
interactions is crucial for achieving human-level performance, and to
accelerate the research in this field. The challenge dataset includes
overlapping cell detection and tissue segmentation annotations from six organs,
comprising 673 pairs sourced from 306 The Cancer Genome Atlas (TCGA)
Whole-Slide Images with hematoxylin and eosin staining, divided into training,
validation, and test subsets. Participants presented models that significantly
enhanced the understanding of cell-tissue relationships. Top entries achieved
up to a 7.99 increase in F1-score on the test set compared to the baseline
cell-only model that did not incorporate cell-tissue relationships. This is a
substantial improvement in performance over traditional cell-only detection
methods, demonstrating the need for incorporating multi-scale semantics into
the models. This paper provides a comparative analysis of the methods used by
participants, highlighting innovative strategies implemented in the OCELOT 2023
challenge.

</details>


### [67] [RT-DETR++ for UAV Object Detection](https://arxiv.org/abs/2509.09157)
*Yuan Shufang*

Main category: cs.CV

> 本文介绍了RT-DETR++模型，通过改进的上采样/下采样机制和CSP-PAC技术提升了目标检测的性能，特别适用于小且密集包装的目标检测，同时保持了实时的检测速度。

<details>
  <summary>Details</summary>

**Motivation:** 无人机(UAV)图像中的目标检测面临许多挑战，例如密集的小目标、尺度变化和遮挡。因此，提出了RT-DETR++模型，以更好地解决这些问题。

**Method:** 本文提出了一种改进的实时目标检测模型RT-DETR++，主要改进了其编码器部分。首先，引入了基于通道的注意力机制的上采样/下采样（AU/AD）机制，以减少错误并保持特征层传播过程中的细节。其次，引入了CSP-PAC技术，该技术使用并行空洞卷积处理同层的局部和上下文信息，方便集成多尺度特征。

**Result:** 评估表明，本研究设计的新型颈部分在检测小且密集包装的对象时具有出色的性能，并且在保持足够的实时检测速度的同时，没有增加计算复杂性。

**Conclusion:** 本研究为实时检测系统中的特征编码设计提供了一种有效的方法。

**Abstract:** Object detection in unmanned aerial vehicle (UAV) imagery presents
significant challenges. Issues such as densely packed small objects, scale
variations, and occlusion are commonplace. This paper introduces RT-DETR++,
which enhances the encoder component of the RT-DETR model. Our improvements
focus on two key aspects. First, we introduce a channel-gated attention-based
upsampling/downsampling (AU/AD) mechanism. This dual-path system minimizes
errors and preserves details during feature layer propagation. Second, we
incorporate CSP-PAC during feature fusion. This technique employs parallel
hollow convolutions to process local and contextual information within the same
layer, facilitating the integration of multi-scale features. Evaluation
demonstrates that our novel neck design achieves superior performance in
detecting small and densely packed objects. The model maintains sufficient
speed for real-time detection without increasing computational complexity. This
study provides an effective approach for feature encoding design in real-time
detection systems.

</details>


### [68] [A Knowledge Noise Mitigation Framework for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2509.09159)
*Zhiyue Liu,Sihang Liu,Jinyuan Liu,Xinru Zhang*

Main category: cs.CV

> 提出了一种训练免费的知识聚焦框架，用于提高KB-VQA任务中外部知识的相关性，并减少冗余信息的影响。

<details>
  <summary>Details</summary>

**Motivation:** 现有的KB-VQA方法往往直接利用从知识源中检索的信息来增强模型，但却忽略了知识的冗余性，这会引入噪声并影响问题回答的精度。为了缓解这一问题，作者提出了一个新的框架以提高知识相关性和减少冗余性。

**Method:** 提出了一种无需训练的知识聚焦框架来解决KB-VQA问题。首先，框架会从图像-问题对中提取出关键部分，形成低噪声的查询，提高知识检索的相关性。接着，通过提示大型模型识别并提取知识中对答案有益的部分，进一步减少冗余。最后，采用选择性知识集成策略，当模型对问题答案不够自信时，才结合外部知识，从而减轻冗余信息的影响。

**Result:** 广泛实验证明，对比最先进的方法，该框架能够更好地获取准确和关键的知识。

**Conclusion:** 该研究展示了如何利用知识聚焦策略来有效提升KB-VQA任务中知识的利用效果，降低了冗余带来的负面影响。提供了一个训练免费的有效解决方案。

**Abstract:** Knowledge-based visual question answering (KB-VQA) requires a model to
understand images and utilize external knowledge to provide accurate answers.
Existing approaches often directly augment models with retrieved information
from knowledge sources while ignoring substantial knowledge redundancy, which
introduces noise into the answering process. To address this, we propose a
training-free framework with knowledge focusing for KB-VQA, that mitigates the
impact of noise by enhancing knowledge relevance and reducing redundancy.
First, for knowledge retrieval, our framework concludes essential parts from
the image-question pairs, creating low-noise queries that enhance the retrieval
of highly relevant knowledge. Considering that redundancy still persists in the
retrieved knowledge, we then prompt large models to identify and extract
answer-beneficial segments from knowledge. In addition, we introduce a
selective knowledge integration strategy, allowing the model to incorporate
knowledge only when it lacks confidence in answering the question, thereby
mitigating the influence of redundant information. Our framework enables the
acquisition of accurate and critical knowledge, and extensive experiments
demonstrate that it outperforms state-of-the-art methods.

</details>


### [69] [CWSSNet: Hyperspectral Image Classification Enhanced by Wavelet Domain Convolution](https://arxiv.org/abs/2509.09163)
*Yulin Tong,Fengzong Zhang,Haiqin Cheng*

Main category: cs.CV

> 本文提出了CWSSNet框架用于地物分类研究，使用ZY1F卫星的高光谱图像为数据源，在江西省上饶市喻 ankles县进行实验，实现了最优的分类精度并展现了良好的鲁棒性和小样本训练条件下的可靠性能。

<details>
  <summary>Details</summary>

**Motivation:** 高光谱遥感技术在林业生态和精准农业等领域具有重要应用价值，但对精细地物分类提出了更高要求。高光谱图像虽谱信息丰富，但其多波段、高维度和谱混合特性导致特征冗余，因此提出此研究。

**Method:** CWSSNet框架结合了3D光谱-空间特征和小波卷积，通过使用多尺度卷积注意力模块整合多模态信息，并在小波域引入多波段分解和卷积操作来突破传统方法的分类性能瓶颈。

**Result:** 在喻 ankles县实验中，CWSSNet分别实现了74.50%、82.73%和84.94%的mIoU、mAcc和mF1得分。在水域、植被和裸地的分类中，获得了最高的IoU。

**Conclusion:** CWSSNet显示了良好的鲁棒性，即使在小样本训练条件下也能保持较高的分类性能，证明了其在地物分类中的有效性。

**Abstract:** Hyperspectral remote sensing technology has significant application value in
fields such as forestry ecology and precision agriculture, while also putting
forward higher requirements for fine ground object classification. However,
although hyperspectral images are rich in spectral information and can improve
recognition accuracy, they tend to cause prominent feature redundancy due to
their numerous bands, high dimensionality, and spectral mixing characteristics.
To address this, this study used hyperspectral images from the ZY1F satellite
as a data source and selected Yugan County, Shangrao City, Jiangxi Province as
the research area to perform ground object classification research. A
classification framework named CWSSNet was proposed, which integrates 3D
spectral-spatial features and wavelet convolution. This framework integrates
multimodal information us-ing a multiscale convolutional attention module and
breaks through the classification performance bottleneck of traditional methods
by introducing multi-band decomposition and convolution operations in the
wavelet domain. The experiments showed that CWSSNet achieved 74.50\%, 82.73\%,
and 84.94\% in mean Intersection over Union (mIoU), mean Accuracy (mAcc), and
mean F1-score (mF1) respectively in Yugan County. It also obtained the highest
Intersection over Union (IoU) in the classifica-tion of water bodies,
vegetation, and bare land, demonstrating good robustness. Additionally, when
the training set proportion was 70\%, the increase in training time was
limited, and the classification effect was close to the optimal level,
indicating that the model maintains reliable performance under small-sample
training conditions.

</details>


### [70] [Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios](https://arxiv.org/abs/2509.09172)
*Chunxiao Li,Xiaoxiao Wang,Meiling Li,Boming Miao,Peng Sun,Yunjian Zhang,Xiangyang Ji,Yao Zhu*

Main category: cs.CV

> Introduces RRDataset to evaluate the performance of AI-generated image detectors in real-world scenarios, revealing their limitations and proposing a human-centric approach for improvement.

<details>
  <summary>Details</summary>

**Motivation:** To address the research gap in evaluating AI-generated image detection methods under complex real-world conditions, especially given the rapid advancements in generative models and the resulting challenges to digital security and media credibility.

**Method:** This paper introduces the Real-World Robustness Dataset (RRDataset) to comprehensively evaluate the performance of AI-generated image detection models in various real-world conditions, comprising scenario generalization, internet transmission robustness, and re-digitization robustness.

**Result:** The benchmarking of 17 detectors and 10 vision-language models revealed the limitations of current AI detection methods in real-world conditions.

**Conclusion:** The study underscores the limitations of existing detection methods and highlights the importance of integrating human adaptability in developing more robust AI detection algorithms.

**Abstract:** With the rapid advancement of generative models, highly realistic image
synthesis has posed new challenges to digital security and media credibility.
Although AI-generated image detection methods have partially addressed these
concerns, a substantial research gap remains in evaluating their performance
under complex real-world conditions. This paper introduces the Real-World
Robustness Dataset (RRDataset) for comprehensive evaluation of detection models
across three dimensions: 1) Scenario Generalization: RRDataset encompasses
high-quality images from seven major scenarios (War and Conflict, Disasters and
Accidents, Political and Social Events, Medical and Public Health, Culture and
Religion, Labor and Production, and everyday life), addressing existing dataset
gaps from a content perspective. 2) Internet Transmission Robustness: examining
detector performance on images that have undergone multiple rounds of sharing
across various social media platforms. 3) Re-digitization Robustness: assessing
model effectiveness on images altered through four distinct re-digitization
methods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on
RRDataset and conducted a large-scale human study involving 192 participants to
investigate human few-shot learning capabilities in detecting AI-generated
images. The benchmarking results reveal the limitations of current AI detection
methods under real-world conditions and underscore the importance of drawing on
human adaptability to develop more robust detection algorithms.

</details>


### [71] [Dark-ISP: Enhancing RAW Image Processing for Low-Light Object Detection](https://arxiv.org/abs/2509.09183)
*Jiasheng Guo,Xin Gao,Yuxiang Yan,Guanghao Li,Jian Pu*

Main category: cs.CV

> 本文提出了一种针对低光环境的目标检测方法Dark-ISP，该方法通过处理RAW图像达到了比现有RGB方法更好的性能。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在改进低光照环境下的物体检测技术，针对现有方法存在的信息损失或框架复杂等问题，提出新的解决方案。

**Method:** 本文提出了一种轻量级且自适应的ISP插件Dark-ISP，该插件可以直接处理暗环境下的Bayer RAW图像，从而实现无缝的端到端训练用于目标检测。主要创新点包括：1) 将传统的ISP流水线分解为顺序的线性（传感器校准）和非线性（色调映射）子模块，并将其重新表述为可微分组件，通过任务驱动的损失进行优化。每个模块都配备了内容感知的适应性和基于物理的先验信息，以实现与检测目标对齐的自动RAW到RGB转换。2) 通过利用ISP流水线的内在级联结构，设计了一种自增强机制以促进子模块之间的协作。

**Result:** 通过对三个RAW图像数据集进行广泛实验，证明所提出的方法在参数量较少的情况下，在具有挑战性的低光照环境下，超过了最先进的RGB和RAW基于的目标检测方法，获得了更好的结果。

**Conclusion:** 实验表明Dark-ISP在低光环境下具有优越的物体检测性能，证明其在处理RAW图像方面的能力，为未来的研究打开了新的可能性。

**Abstract:** Low-light Object detection is crucial for many real-world applications but
remains challenging due to degraded image quality. While recent studies have
shown that RAW images offer superior potential over RGB images, existing
approaches either use RAW-RGB images with information loss or employ complex
frameworks. To address these, we propose a lightweight and self-adaptive Image
Signal Processing (ISP) plugin, Dark-ISP, which directly processes Bayer RAW
images in dark environments, enabling seamless end-to-end training for object
detection. Our key innovations are: (1) We deconstruct conventional ISP
pipelines into sequential linear (sensor calibration) and nonlinear (tone
mapping) sub-modules, recasting them as differentiable components optimized
through task-driven losses. Each module is equipped with content-aware
adaptability and physics-informed priors, enabling automatic RAW-to-RGB
conversion aligned with detection objectives. (2) By exploiting the ISP
pipeline's intrinsic cascade structure, we devise a Self-Boost mechanism that
facilitates cooperation between sub-modules. Through extensive experiments on
three RAW image datasets, we demonstrate that our method outperforms
state-of-the-art RGB- and RAW-based detection approaches, achieving superior
results with minimal parameters in challenging low-light environments.

</details>


### [72] [VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models: Methods and Results](https://arxiv.org/abs/2509.09190)
*Hanwei Zhu,Haoning Wu,Zicheng Zhang,Lingyu Zhu,Yixuan Li,Peilin Chen,Shiqi Wang,Chris Wei Zhou,Linhan Cao,Wei Sun,Xiangyang Zhu,Weixia Zhang,Yucheng Zhu,Jing Liu,Dandan Zhu,Guangtao Zhai,Xiongkuo Min,Zhichao Zhang,Xinyue Li,Shubo Xu,Anh Dao,Yifan Li,Hongyuan Yu,Jiaojiao Yi,Yiding Tian,Yupeng Wu,Feiran Sun,Lijuan Liao,Song Jiang*

Main category: cs.CV

> VQualA 2025挑战赛评估了大型模型在视觉质量对比中的能力，参与者众多，并展示了LMM模型的新能力。

<details>
  <summary>Details</summary>

**Motivation:** 本次挑战的目标是评估和提高最先进的大模型在多模态视觉质量对比中的能力，特别侧重于开放式和详细的推理能力，以推动开放域视觉质量推理和比较的研究。

**Method:** 该挑战赛通过引入一个包含数千个粗细粒度视觉质量比较任务的新基准来评估和提高最先进的LMM模型在不同的多图像中进行开放式和详细的推理能力，该基准涵盖了单张图像、图像对和多图像组的任务。每个任务都需要模型提供准确的质量判断。

**Result:** 大约100个参赛团队提交了参赛作品，有五个模型展示了指令调整后的LMM模型在质量评估方面的新能力。这标志着在开放域视觉质量推理和比较方面的重大进步。

**Conclusion:** 此挑战赛作为一个催化剂，促进了对可解释和人机对齐的质量评估系统未来研究的发展。

**Abstract:** This paper presents a summary of the VQualA 2025 Challenge on Visual Quality
Comparison for Large Multimodal Models (LMMs), hosted as part of the ICCV 2025
Workshop on Visual Quality Assessment. The challenge aims to evaluate and
enhance the ability of state-of-the-art LMMs to perform open-ended and detailed
reasoning about visual quality differences across multiple images. To this end,
the competition introduces a novel benchmark comprising thousands of
coarse-to-fine grained visual quality comparison tasks, spanning single images,
pairs, and multi-image groups. Each task requires models to provide accurate
quality judgments. The competition emphasizes holistic evaluation protocols,
including 2AFC-based binary preference and multi-choice questions (MCQs).
Around 100 participants submitted entries, with five models demonstrating the
emerging capabilities of instruction-tuned LMMs on quality assessment. This
challenge marks a significant step toward open-domain visual quality reasoning
and comparison and serves as a catalyst for future research on interpretable
and human-aligned quality evaluation systems.

</details>


### [73] [MGTraj: Multi-Granularity Goal-Guided Human Trajectory Prediction with Recursive Refinement Network](https://arxiv.org/abs/2509.09200)
*Ge Sun,Jun Ma*

Main category: cs.CV

> 本研究提出了一种名为MGTraj的多粒度目标导向模型，提出了递归编码和细化技术，不仅能捕获不同粒度的特征还能通过权重共享和速度辅助任务提升预测性能，在多个数据集上取得了领先成绩。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大多数目标导向方法都将预测任务分为两个独立的阶段：目标预测和基于预测目标的轨迹完成，这两个阶段存在粗粒度和细粒度操作，而中间粒度的潜在利用尚需探索。因此受到多粒度表示能够捕捉到人类动态和动作模式的启发，研究多粒度概念在目标导向框架中的有效整合。

**Method:** MGTraj采用了一种新型的多粒度目标导向模型，递归地从粗粒度到细粒度对轨迹进行编码和细化。在每个粒度级别，都使用了一个基于变压器的递归细化网络来捕捉特征并预测渐进性的细化。同时通过权重共享策略将不同粒度的特征整合，使用速度预测作为辅助任务来进一步提升性能。

**Result:** 实验结果显示，在EHT/UCY和Stanford Drone Dataset等数据集中，MGTraj的性能优于基准方法，并且在目标导向方法中达到了最先进的状态。

**Conclusion:** MGTraj作为一个新的多粒度导向模型，在人类轨迹预测的领域展示了其优越性和创新性。它在预测精度上的改进和对多个粒度的综合效果，证明了这种方法在同类方法中的领先地位。

**Abstract:** Accurate human trajectory prediction is crucial for robotics navigation and
autonomous driving. Recent research has demonstrated that incorporating goal
guidance significantly enhances prediction accuracy by reducing uncertainty and
leveraging prior knowledge. Most goal-guided approaches decouple the prediction
task into two stages: goal prediction and subsequent trajectory completion
based on the predicted goal, which operate at extreme granularities:
coarse-grained goal prediction forecasts the overall intention, while
fine-grained trajectory completion needs to generate the positions for all
future timesteps. The potential utility of intermediate temporal granularity
remains largely unexplored, which motivates multi-granularity trajectory
modeling. While prior work has shown that multi-granularity representations
capture diverse scales of human dynamics and motion patterns, effectively
integrating this concept into goal-guided frameworks remains challenging. In
this paper, we propose MGTraj, a novel Multi-Granularity goal-guided model for
human Trajectory prediction. MGTraj recursively encodes trajectory proposals
from coarse to fine granularity levels. At each level, a transformer-based
recursive refinement network (RRN) captures features and predicts progressive
refinements. Features across different granularities are integrated using a
weight-sharing strategy, and velocity prediction is employed as an auxiliary
task to further enhance performance. Comprehensive experimental results in
EHT/UCY and Stanford Drone Dataset indicate that MGTraj outperforms baseline
methods and achieves state-of-the-art performance among goal-guided methods.

</details>


### [74] [Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization](https://arxiv.org/abs/2509.09307)
*Zhengzhao Lai,Youbin Zheng,Zhenyang Cai,Haonan Lyu,Jinpu Yang,Hongqing Liang,Yan Hu,Benyou Wang*

Main category: cs.CV

> MatCha是一个新的基准测试，专注于材料表征图像的理解，研究表明现有的多模态大型语言模型在处理这类需要高级专业知识的问题时存在局限性。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探索多模态大型语言模型在理解真实世界材料表征图像数据的能力，并指出现有模型在这方面的不足，以促进新材料发现和自主科研代理方面的未来研究。

**Method:** 提出MatCha基准测试，包含1500个问题，这些问题被设计用于反映材料科学家所面临的实际挑战，广泛涵盖了材料研究的各个重要阶段和任务。

**Result:** 该研究提出了MatCha，这是一个针对材料表征图像理解的基准测试，包含1500个需要专家级领域知识的问题。MatCha跨越材料研究的四个关键阶段，包括21个不同的任务，旨在反映材料科学家所面临的实际挑战。实验评估表明，现有的多模态大型语言模型在处理需要高级专业知识和复杂视觉感知的问题时存在显著性能差距，这表明当前的MLLM在适应真实世界的材料表征场景方面还有局限性。

**Conclusion:** 实验结果表明，与人类专家相比，目前最先进的多模态大型语言模型在处理复杂和需要高层次专业知识的问题上存在显著性能差距。这表明这些模型在适应真实世界的材料表征场景方面还存在局限性。

**Abstract:** Materials characterization is fundamental to acquiring materials information,
revealing the processing-microstructure-property relationships that guide
material design and optimization. While multimodal large language models
(MLLMs) have recently shown promise in generative and predictive tasks within
materials science, their capacity to understand real-world characterization
imaging data remains underexplored. To bridge this gap, we present MatCha, the
first benchmark for materials characterization image understanding, comprising
1,500 questions that demand expert-level domain expertise. MatCha encompasses
four key stages of materials research comprising 21 distinct tasks, each
designed to reflect authentic challenges faced by materials scientists. Our
evaluation of state-of-the-art MLLMs on MatCha reveals a significant
performance gap compared to human experts. These models exhibit degradation
when addressing questions requiring higher-level expertise and sophisticated
visual perception. Simple few-shot and chain-of-thought prompting struggle to
alleviate these limitations. These findings highlight that existing MLLMs still
exhibit limited adaptability to real-world materials characterization
scenarios. We hope MatCha will facilitate future research in areas such as new
material discovery and autonomous scientific agents. MatCha is available at
https://github.com/FreedomIntelligence/MatCha.

</details>


### [75] [Medverse: A Universal Model for Full-Resolution 3D Medical Image Segmentation, Transformation and Enhancement](https://arxiv.org/abs/2509.09232)
*Jiesi Hu,Jianfeng Cao,Yanwu Yang,Chenfei Ye,Yixuan Zhang,Hanyang Peng,Ting Ma*

Main category: cs.CV

> 本文介绍了Medverse，这是一种适用于3D医学影像的通用ICL模型，通过跨多个数据集训练，展示了在多样化医学影像任务中的卓越性能和一致性。

<details>
  <summary>Details</summary>

**Motivation:** 当前用于医学成像的在上下文学习 (ICL) 模型在两个关键方面存在局限：无法在保持高质量预测的同时理解全局解剖结构，并且没有统一的跨不同类型医学影像任务和解剖部位进行训练的模型，导致医学影像领域ICL的潜力未被完全挖掘。

**Method:** Medverse 使用了一种称为下一等级自回归的在上下文中学习框架，该框架逐步从粗到细地改进预测结果，生成一致的完整分辨率体素输出，同时支持多尺度的解剖结构感知。此外，还提出了一种块式交叉注意力模块，能够在保持计算效率的同时促进上下文输入与目标输入之间的长距离交互。

**Result:** Medverse 在广泛的未见过的数据集上进行了评估，结果显示，它显著优于现有的 ICL 基线，确立了在上下文学习中的一种新范式。

**Conclusion:** Medverse 实现了对广泛未见过的数据集的性能提升，包括不同的临床中心、器官、物种和成像模态，证明了其在医学影像领域ICL方法中的优越性。

**Abstract:** In-context learning (ICL) offers a promising paradigm for universal medical
image analysis, enabling models to perform diverse image processing tasks
without retraining. However, current ICL models for medical imaging remain
limited in two critical aspects: they cannot simultaneously achieve
high-fidelity predictions and global anatomical understanding, and there is no
unified model trained across diverse medical imaging tasks (e.g., segmentation
and enhancement) and anatomical regions. As a result, the full potential of ICL
in medical imaging remains underexplored. Thus, we present \textbf{Medverse}, a
universal ICL model for 3D medical imaging, trained on 22 datasets covering
diverse tasks in universal image segmentation, transformation, and enhancement
across multiple organs, imaging modalities, and clinical centers. Medverse
employs a next-scale autoregressive in-context learning framework that
progressively refines predictions from coarse to fine, generating consistent,
full-resolution volumetric outputs and enabling multi-scale anatomical
awareness. We further propose a blockwise cross-attention module that
facilitates long-range interactions between context and target inputs while
preserving computational efficiency through spatial sparsity. Medverse is
extensively evaluated on a broad collection of held-out datasets covering
previously unseen clinical centers, organs, species, and imaging modalities.
Results demonstrate that Medverse substantially outperforms existing ICL
baselines and establishes a novel paradigm for in-context learning. Code and
model weights will be made publicly available. Our model are publicly available
at https://github.com/jiesihu/Medverse.

</details>


### [76] [FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark](https://arxiv.org/abs/2509.09680)
*Rongyao Fang,Aldrich Yu,Chengqi Duan,Linjiang Huang,Shuai Bai,Yuxuan Cai,Kun Wang,Si Liu,Xihui Liu,Hongsheng Li*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** The advancement of open-source text-to-image (T2I) models has been hindered
by the absence of large-scale, reasoning-focused datasets and comprehensive
evaluation benchmarks, resulting in a performance gap compared to leading
closed-source systems. To address this challenge, We introduce FLUX-Reason-6M
and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).
FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality
FLUX-generated images and 20 million bilingual (English and Chinese)
descriptions specifically designed to teach complex reasoning. The image are
organized according to six key characteristics: Imagination, Entity, Text
rendering, Style, Affection, and Composition, and design explicit Generation
Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation
steps. The whole data curation takes 15,000 A100 GPU days, providing the
community with a resource previously unattainable outside of large industrial
labs. PRISM-Bench offers a novel evaluation standard with seven distinct
tracks, including a formidable Long Text challenge using GCoT. Through
carefully designed prompts, it utilizes advanced vision-language models for
nuanced human-aligned assessment of prompt-image alignment and image
aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench
reveals critical performance gaps and highlights specific areas requiring
improvement. Our dataset, benchmark, and evaluation code are released to
catalyze the next wave of reasoning-oriented T2I generation. Project page:
https://flux-reason-6m.github.io/ .

</details>


### [77] [CoAtNeXt:An Attention-Enhanced ConvNeXtV2-Transformer Hybrid Model for Gastric Tissue Classification](https://arxiv.org/abs/2509.09242)
*Mustafa Yurdakul,Sakir Tasdemir*

Main category: cs.CV

> The paper proposes CoAtNeXt, an enhanced hybrid model for gastric tissue image classification which outperforms other Convolutional Neural Networks and Vision Transformers on two datasets, highlighting its potential for improving diagnostic accuracy and reducing workload for pathologists.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this research is the need to improve the accuracy and consistency of gastric disease diagnosis, which is primarily conducted through manual histopathologic examination and is thus labor-intensive and prone to variability.

**Method:** The researchers introduced CoAtNeXt, which modifies the CoAtNet architecture by incorporating enhanced ConvNeXtV2 blocks and integrating CBAM to enhance feature extraction. The model’s performance was assessed on two datasets for binary and multiclass gastric tissue image classification.

**Result:** CoAtNeXt achieved high classification performance on both datasets, outperforming all the compared CNN and ViT models, with accuracy ranging from 96.47% to 98.29% and AUC scores approaching 99.90%.

**Conclusion:** The conclusion is that CoAtNeXt is a robust and efficient model for gastric tissue image classification, showcasing its potential to support pathologists by increasing diagnostic accuracy and reducing their workload.

**Abstract:** Background and objective Early diagnosis of gastric diseases is crucial to
prevent fatal outcomes. Although histopathologic examination remains the
diagnostic gold standard, it is performed entirely manually, making evaluations
labor-intensive and prone to variability among pathologists. Critical findings
may be missed, and lack of standard procedures reduces consistency. These
limitations highlight the need for automated, reliable, and efficient methods
for gastric tissue analysis. Methods In this study, a novel hybrid model named
CoAtNeXt was proposed for the classification of gastric tissue images. The
model is built upon the CoAtNet architecture by replacing its MBConv layers
with enhanced ConvNeXtV2 blocks. Additionally, the Convolutional Block
Attention Module (CBAM) is integrated to improve local feature extraction
through channel and spatial attention mechanisms. The architecture was scaled
to achieve a balance between computational efficiency and classification
performance. CoAtNeXt was evaluated on two publicly available datasets,
HMU-GC-HE-30K for eight-class classification and GasHisSDB for binary
classification, and was compared against 10 Convolutional Neural Networks
(CNNs) and ten Vision Transformer (ViT) models. Results CoAtNeXt achieved
96.47% accuracy, 96.60% precision, 96.47% recall, 96.45% F1 score, and 99.89%
AUC on HMU-GC-HE-30K. On GasHisSDB, it reached 98.29% accuracy, 98.07%
precision, 98.41% recall, 98.23% F1 score, and 99.90% AUC. It outperformed all
CNN and ViT models tested and surpassed previous studies in the literature.
Conclusion Experimental results show that CoAtNeXt is a robust architecture for
histopathological classification of gastric tissue images, providing
performance on binary and multiclass. Its highlights its potential to assist
pathologists by enhancing diagnostic accuracy and reducing workload.

</details>


### [78] [Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset for Panoramic X-ray Analysis](https://arxiv.org/abs/2509.09254)
*Jing Hao,Yuxuan Fan,Yanpeng Sun,Kaixin Guo,Lizhuo Lin,Jinrong Yang,Qi Yong H. Ai,Lun M. Wong,Hao Tang,Kuo Feng Hung*

Main category: cs.CV

> 研究创建了为全景牙科X光片解释设计的大型多模态指令数据集MMOral和基准MMOral-Bench，并展示了通过微调Qwen2.5-VL-7B创建的OralGPT能显著提升模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型视觉-语言模型在通用医疗任务中表现出色，但它们在牙科等专业领域的能力尚未充分研究，特别是全景X光片的解释，这启发了本研究。

**Method:** 引入了MMOral，首个针对全景牙科X光片解释的大型多模态指令数据集和基准，并构建了MMOral-Bench，涵盖了牙科诊断的五个关键维度。评估了64个大型视觉-语言模型，展示了这些模型在这一领域的局限性，并提出通过监督微调Qwen2.5-VL-7B以创建OralGPT来提升性能。

**Result:** 研究发现顶级模型GPT-4o在以MMOral-Bench为基准评估时，仅达到41.45%的准确率。单轮监督微调已展示出显著性能提升，例如OralGPT实现了24.73%的改进。

**Conclusion:** MMOral和OralGPT为智能化牙科提供了重要的基石，促进入工智能在医疗影像分析中的临床应用提升。

**Abstract:** Recent advances in large vision-language models (LVLMs) have demonstrated
strong performance on general-purpose medical tasks. However, their
effectiveness in specialized domains such as dentistry remains underexplored.
In particular, panoramic X-rays, a widely used imaging modality in oral
radiology, pose interpretative challenges due to dense anatomical structures
and subtle pathological cues, which are not captured by existing medical
benchmarks or instruction datasets. To this end, we introduce MMOral, the first
large-scale multimodal instruction dataset and benchmark tailored for panoramic
X-ray interpretation. MMOral consists of 20,563 annotated images paired with
1.3 million instruction-following instances across diverse task types,
including attribute extraction, report generation, visual question answering,
and image-grounded dialogue. In addition, we present MMOral-Bench, a
comprehensive evaluation suite covering five key diagnostic dimensions in
dentistry. We evaluate 64 LVLMs on MMOral-Bench and find that even the
best-performing model, i.e., GPT-4o, only achieves 41.45% accuracy, revealing
significant limitations of current models in this domain. To promote the
progress of this specific domain, we also propose OralGPT, which conducts
supervised fine-tuning (SFT) upon Qwen2.5-VL-7B with our meticulously curated
MMOral instruction dataset. Remarkably, a single epoch of SFT yields
substantial performance enhancements for LVLMs, e.g., OralGPT demonstrates a
24.73% improvement. Both MMOral and OralGPT hold significant potential as a
critical foundation for intelligent dentistry and enable more clinically
impactful multimodal AI systems in the dental field. The dataset, model,
benchmark, and evaluation suite are available at
https://github.com/isbrycee/OralGPT.

</details>


### [79] [DATE: Dynamic Absolute Time Enhancement for Long Video Understanding](https://arxiv.org/abs/2509.09263)
*Chao Yuan,Yang Yang,Yehui Yang,Zach Cheng*

Main category: cs.CV

> Researchers introduce DATE, a method that improves long video understanding in MLLMs using timestamp embeddings and guided sampling techniques, achieving state-of-the-art performance even with smaller model sizes.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitations of current multimodal large language models (MLLMs) in handling long-range dependencies and precise temporal reasoning in long videos, which leads to critical information loss and degraded performance.

**Method:** Our method proposes Dynamic Absolute Time Enhancement (DATE) which uses the Timestamp Injection Mechanism (TIM) and Temporal-Aware Similarity Sampling (TASS) to improve temporal reasoning in long videos. It interleaves video frame embeddings with textual timestamps and adopts a two-stage algorithm for sampling semantically relevant and temporally diverse frames.

**Result:** The proposed DATE method achieves state-of-the-art performance on hour-long video benchmarks, showcasing improvements in absolute time understanding and key event localization. Notably, even the smaller 7B model outperforms many larger 72B models in some benchmarks.

**Conclusion:** The conclusion is that the proposed DATE method effectively enhances temporal awareness in multimodal large language models, improving their ability to understand and reason about long videos.

**Abstract:** Long video understanding remains a fundamental challenge for multimodal large
language models (MLLMs), particularly in tasks requiring precise temporal
reasoning and event localization. Existing approaches typically adopt uniform
frame sampling and rely on implicit position encodings to model temporal order.
However, these methods struggle with long-range dependencies, leading to
critical information loss and degraded temporal comprehension. In this paper,
we propose Dynamic Absolute Time Enhancement (DATE) that enhances temporal
awareness in MLLMs through the Timestamp Injection Mechanism (TIM) and a
semantically guided Temporal-Aware Similarity Sampling (TASS) strategy.
Specifically, we interleave video frame embeddings with textual timestamp
tokens to construct a continuous temporal reference system. We further
reformulate the video sampling problem as a vision-language retrieval task and
introduce a two-stage algorithm to ensure both semantic relevance and temporal
coverage: enriching each query into a descriptive caption to better align with
the vision feature, and sampling key event with a similarity-driven temporally
regularized greedy strategy. Our method achieves remarkable improvements w.r.t.
absolute time understanding and key event localization, resulting in
state-of-the-art performance among 7B and 72B models on hour-long video
benchmarks. Particularly, our 7B model even exceeds many 72B models on some
benchmarks.

</details>


### [80] [Unified Start, Personalized End: Progressive Pruning for Efficient 3D Medical Image Segmentation](https://arxiv.org/abs/2509.09267)
*Linhao Li,Yiwen Ye,Ziyang Chen,Yong Xia*

Main category: cs.CV

> 提出了PSP-Seg框架，通过渐进式的剪枝策略实现高效3D分割，轻量版PSP-Seg-S具有出色的性能且资源消耗低于nnU-Net，表明其作为临床应用潜在的高效替代方案的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 近年来，3D医疗图像分割面临资源和时间消耗大的问题，这限制了其在临床环境中的广泛应用。此外，现有的高效分割模型通常在训练前就静态设计，这限制了它们在不同任务中的适应性，并且在性能与资源效率之间的平衡较为困难。

**Method:** 本文提出了PSP-Seg，这是一个渐进式剪枝框架，旨在实现高效的3D医疗图像分割。PSP-Seg从一个冗余的模型开始，并通过块级剪枝和功能解耦损失组合的方式渐进式地去除冗余模块。

**Result:** 研究结果表明，轻量级版本PSP-Seg-S在五种公开数据集测试中，与SOTA模型nnU-Net相比，实现了类似的性能，但减少了42-45%的GPU内存使用、29-48%的训练时间和83-87%的参数数量。

**Conclusion:** 这些结果表明PSP-Seg具有成为低成本高效益临床应用的高效的替代方案的潜力。

**Abstract:** 3D medical image segmentation often faces heavy resource and time
consumption, limiting its scalability and rapid deployment in clinical
environments. Existing efficient segmentation models are typically static and
manually designed prior to training, which restricts their adaptability across
diverse tasks and makes it difficult to balance performance with resource
efficiency. In this paper, we propose PSP-Seg, a progressive pruning framework
that enables dynamic and efficient 3D segmentation. PSP-Seg begins with a
redundant model and iteratively prunes redundant modules through a combination
of block-wise pruning and a functional decoupling loss. We evaluate PSP-Seg on
five public datasets, benchmarking it against seven state-of-the-art models and
six efficient segmentation models. Results demonstrate that the lightweight
variant, PSP-Seg-S, achieves performance on par with nnU-Net while reducing GPU
memory usage by 42-45%, training time by 29-48%, and parameter number by 83-87%
across all datasets. These findings underscore PSP-Seg's potential as a
cost-effective yet high-performing alternative for widespread clinical
application.

</details>


### [81] [Visual Programmability: A Guide for Code-as-Thought in Chart Understanding](https://arxiv.org/abs/2509.09286)
*Bohao Tang,Yan Ma,Fei Zhang,Jiadi Su,Ethan Chern,Zhulin Hu,Zhixin Wang,Pengfei Liu,Ya Zhang*

Main category: cs.CV

> 通过引入Code-as-Thought（CaT）方法和视觉可编程性，本文提出了一种视觉语言模型，可以自适应地选择图表理解的最佳推理路径，表现出强劲和稳健的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的图表理解方法存在适应性和可靠性问题，本文旨在通过新型方法克服这些局限，提供更可靠的推理策略选择机制。

**Method:** Structure

**Result:** {
  "tldr": "本文提出了Code-as-Thought（CaT）方法，用于表示图表的视觉信息并采用可验证的符号格式。通过引入视觉可编程性这一概念，该模型能够学习在使用CaT路径或直接视觉分析之间进行选择，提高了复杂图表的理解准确性和适应性。",
  "motivation": "现有的方法依赖外部工具或单一的推理策略，导致模型适应性和可靠性有限。文章旨在解决这些问题，提供一种更可靠的推理策略选择机制，以提高视觉语言模型在图表理解上的性能。",
  "method": "提出了Code-as-Thought（CaT）方法，该方法以可验证的符号格式表示图表的视觉信息，并引入了视觉可编程性作为模型可学习的属性。通过这种方法，在模型面对一个图表问题时，它可以选择采用CaT路径或直接视觉分析。",
  "result": "实验结果表现了模型在不同图表理解基准上的强大和稳健性。模型能够动态选择最优的推理路径，这提升了它在不同类型任务上的适应性和性能。",
  "conclusion": "研究表明，视觉语言模型不仅能学会推理，还能够学会该如何推理，即对于每个任务选择最优的推理路径，为图表理解问题提供了一个强有力的方法。")

**Conclusion:** 该方法展示了视觉语言模型能够动态选择最优推理路径的能力，从而在处理复杂图表理解任务中表现得更强大和适应性更强。

**Abstract:** Chart understanding presents a critical test to the reasoning capabilities of
Vision-Language Models (VLMs). Prior approaches face critical limitations: some
rely on external tools, making them brittle and constrained by a predefined
toolkit, while others fine-tune specialist models that often adopt a single
reasoning strategy, such as text-based chain-of-thought (CoT). The intermediate
steps of text-based reasoning are difficult to verify, which complicates the
use of reinforcement-learning signals that reward factual accuracy. To address
this, we propose a Code-as-Thought (CaT) approach to represent the visual
information of a chart in a verifiable, symbolic format. Our key insight is
that this strategy must be adaptive: a fixed, code-only implementation
consistently fails on complex charts where symbolic representation is
unsuitable. This finding leads us to introduce Visual Programmability: a
learnable property that determines if a chart-question pair is better solved
with code or direct visual analysis. We implement this concept in an adaptive
framework where a VLM learns to choose between the CaT pathway and a direct
visual reasoning pathway. The selection policy of the model is trained with
reinforcement learning using a novel dual-reward system. This system combines a
data-accuracy reward to ground the model in facts and prevent numerical
hallucination, with a decision reward that teaches the model when to use each
strategy, preventing it from defaulting to a single reasoning mode. Experiments
demonstrate strong and robust performance across diverse chart-understanding
benchmarks. Our work shows that VLMs can be taught not only to reason but also
how to reason, dynamically selecting the optimal reasoning pathway for each
task.

</details>


### [82] [Modality-Agnostic Input Channels Enable Segmentation of Brain lesions in Multimodal MRI with Sequences Unavailable During Training](https://arxiv.org/abs/2509.09290)
*Anthony P. Addison,Felix Wagner,Wentian Xu,Natalie Voets,Konstantinos Kamnitsas*

Main category: cs.CV

> 本研究提出了一种新型脑MRI分割模型，通过在U-net架构中加入模态不可知的路径，能够处理训练中未见过的模态，同时保持原有模态的有效处理，提高了分割的准确性和泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 传统的多模态脑MRI分割模型通常受限于固定模态，不能有效处理在推理过程中出现的新模态。还有的模型能够泛化到未见过的模态，但却可能丢失模态特有的信息。因此，开发了可以在未见模态、已见模态及其异构组合上进行推理的模型。

**Method:** 本研究通过在U-net架构中加入模态不可知的输入通道或路径，解决了传统多模态脑MRI分割模型仅限于固定模态和无法有效处理新模态的问题。为了训练这个模态不可知的组件，作者开发了一种图像增强方案，该方案能够合成人工MRI模态，同时保持真实解剖结构的完整性。

**Result:** 研究使用包含5种病理类型和8种模态的8个MRI数据库进行评估。结果显示，该方法不仅能够有效地处理训练过程中遇到的MRI模态，还可以处理新的、未见过的模态，从而提高分割效果。

**Conclusion:** 本研究提出的方法通过一种简单而实用的U-net架构修改，实现了对新模态MRI的有效处理，提升了分割模型在多种模态MRI图像上的分割能力。

**Abstract:** Segmentation models are important tools for the detection and analysis of
lesions in brain MRI. Depending on the type of brain pathology that is imaged,
MRI scanners can acquire multiple, different image modalities (contrasts). Most
segmentation models for multimodal brain MRI are restricted to fixed modalities
and cannot effectively process new ones at inference. Some models generalize to
unseen modalities but may lose discriminative modality-specific information.
This work aims to develop a model that can perform inference on data that
contain image modalities unseen during training, previously seen modalities,
and heterogeneous combinations of both, thus allowing a user to utilize any
available imaging modalities. We demonstrate this is possible with a simple,
thus practical alteration to the U-net architecture, by integrating a
modality-agnostic input channel or pathway, alongside modality-specific input
channels. To train this modality-agnostic component, we develop an image
augmentation scheme that synthesizes artificial MRI modalities. Augmentations
differentially alter the appearance of pathological and healthy brain tissue to
create artificial contrasts between them while maintaining realistic anatomical
integrity. We evaluate the method using 8 MRI databases that include 5 types of
pathologies (stroke, tumours, traumatic brain injury, multiple sclerosis and
white matter hyperintensities) and 8 modalities (T1, T1+contrast, T2, PD, SWI,
DWI, ADC and FLAIR). The results demonstrate that the approach preserves the
ability to effectively process MRI modalities encountered during training,
while being able to process new, unseen modalities to improve its segmentation.
Project code: https://github.com/Anthony-P-Addison/AGN-MOD-SEG

</details>


### [83] [Model-Agnostic Open-Set Air-to-Air Visual Object Detection for Reliable UAV Perception](https://arxiv.org/abs/2509.09297)
*Spyridon Loukovitis,Anastasios Arsenos,Vasileios Karampinis,Athanasios Voulodimos*

Main category: cs.CV

> 提出了一种新的开放集检测框架，适用基于嵌入的检测器，提高无人机在空对空环境中的感知能力，相比YOLO检测器提高10%的性能。

<details>
  <summary>Details</summary>

**Motivation:** 随着无人机自主性的增强，传统的封闭集检测器在实际条件下的领域转移和飞行数据损坏下表现不佳，而开放集检测在避免这一问题上显得尤为重要。

**Method:** 我们提出了一种新颖且模型不可知的开放式目标检测框架，专门针对基于嵌入的检测器。该方法通过在嵌入空间中通过熵建模估计语义不确定性，并结合谱归一化和温度缩放来增强开放集区分。

**Result:** 我们在具有挑战性的AOT空中基准和广泛的飞行测试中验证了我们的方法。实验表明，与标准YOLO检测器相比，我们实现了高达10%的相对AUROC增益。同时，背景拒绝进一步增强了鲁棒性，而不影响检测精度。

**Conclusion:** 我们的方法不仅提升了检测器在开放集条件下的性能，而且在存在飞行数据损坏的情况下保持了鲁棒性。这使得我们的解决方案特别适合动态空对空环境中可靠的无人机感知。

**Abstract:** Open-set detection is crucial for robust UAV autonomy in air-to-air object
detection under real-world conditions. Traditional closed-set detectors degrade
significantly under domain shifts and flight data corruption, posing risks to
safety-critical applications. We propose a novel, model-agnostic open-set
detection framework designed specifically for embedding-based detectors. The
method explicitly handles unknown object rejection while maintaining robustness
against corrupted flight data. It estimates semantic uncertainty via entropy
modeling in the embedding space and incorporates spectral normalization and
temperature scaling to enhance open-set discrimination. We validate our
approach on the challenging AOT aerial benchmark and through extensive
real-world flight tests. Comprehensive ablation studies demonstrate consistent
improvements over baseline methods, achieving up to a 10\% relative AUROC gain
compared to standard YOLO-based detectors. Additionally, we show that
background rejection further strengthens robustness without compromising
detection accuracy, making our solution particularly well-suited for reliable
UAV perception in dynamic air-to-air environments.

</details>


### [84] [Learning Object-Centric Representations in SAR Images with Multi-Level Feature Fusion](https://arxiv.org/abs/2509.09298)
*Oh-Tae Jang,Min-Gon Cho,Kyung-Tae Kim*

Main category: cs.CV

> 本文提出SlotSAR框架，用于从SAR图像中分离背景杂波，增强目标表征的清晰度，并通过多层次插槽注意力模块提升表征差异性。实验表明，SlotSAR在SAR图像上实现了最先进的性能。

<details>
  <summary>Details</summary>

**Motivation:** 合成孔径雷达（SAR）图像中不仅包含感兴趣的目标，还包含复杂的背景杂波，包括地形反射和斑点噪声。这些杂波在强度和模式上可能与目标相似，导致模型提取出纠缠或虚假特征，从而削弱任何分类器形成清晰目标表征的能力。

**Method:** 本文提出了一种新颖的对象中心学习（OCL）框架，命名为SlotSAR，用于在没有掩码注释的情况下从SAR图像背景杂波中分离目标表示。SlotSAR首先从SARATR-X中提取高层次的语义特征，从小波散射网络中提取低层次的散射特征，以获得互补的多层次表示，从而实现稳健的目标表征。接着通过多层次插槽注意力模块整合低层次和高层次特征，增强插槽级别的表征区别性，从而达到有效的OCL。

**Result:** 实验验证了SlotSAR的有效性，证明其在SAR图像数据集上实现了更好的性能和结构细节保持能力。

**Conclusion:** 实验结果表明，与现有的OCL方法相比，SlotSAR在SAR图像上实现了最先进的性能，更好地保持了结构细节。

**Abstract:** Synthetic aperture radar (SAR) images contain not only targets of interest
but also complex background clutter, including terrain reflections and speckle
noise. In many cases, such clutter exhibits intensity and patterns that
resemble targets, leading models to extract entangled or spurious features.
Such behavior undermines the ability to form clear target representations,
regardless of the classifier. To address this challenge, we propose a novel
object-centric learning (OCL) framework, named SlotSAR, that disentangles
target representations from background clutter in SAR images without mask
annotations. SlotSAR first extracts high-level semantic features from SARATR-X
and low-level scattering features from the wavelet scattering network in order
to obtain complementary multi-level representations for robust target
characterization. We further present a multi-level slot attention module that
integrates these low- and high-level features to enhance slot-wise
representation distinctiveness, enabling effective OCL. Experimental results
demonstrate that SlotSAR achieves state-of-the-art performance in SAR imagery
by preserving structural details compared to existing OCL methods.

</details>


### [85] [You Share Beliefs, I Adapt: Progressive Heterogeneous Collaborative Perception](https://arxiv.org/abs/2509.09310)
*Hao Si,Ehsan Javanmardi,Manabu Tsukada*

Main category: cs.CV

> 为解决车辆异构模型间域差距挑战，提出了渐进异构协作感知（PHCP）框架，在推理过程中动态对齐特征，无需标注数据和联合训练，在OPV2V数据集上表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 尽管现有方法通过微调适配器或整个网络来应对异构模型间域差距的挑战，但它们在实际应用中并不可行，因为每个新合作者必须在数据集上与自我车辆进行联合训练，或者自我车辆提前存储所有潜在合作者的模型。

**Method:** 提出了一种名为渐进异构协作感知（PHCP）的新框架，该框架将问题表述为少量无监督领域适应问题，并在推理过程中通过自我训练适配器动态对齐特征，消除了对标注数据和联合训练的需求。

**Result:** 

**Conclusion:** 实验结果表明，PHCP在不同的异构场景中表现良好，并且仅使用少量未标记数据就能达到与全量数据训练的SOTA方法相当的性能。

**Abstract:** Collaborative perception enables vehicles to overcome individual perception
limitations by sharing information, allowing them to see further and through
occlusions. In real-world scenarios, models on different vehicles are often
heterogeneous due to manufacturer variations. Existing methods for
heterogeneous collaborative perception address this challenge by fine-tuning
adapters or the entire network to bridge the domain gap. However, these methods
are impractical in real-world applications, as each new collaborator must
undergo joint training with the ego vehicle on a dataset before inference, or
the ego vehicle stores models for all potential collaborators in advance.
Therefore, we pose a new question: Can we tackle this challenge directly during
inference, eliminating the need for joint training? To answer this, we
introduce Progressive Heterogeneous Collaborative Perception (PHCP), a novel
framework that formulates the problem as few-shot unsupervised domain
adaptation. Unlike previous work, PHCP dynamically aligns features by
self-training an adapter during inference, eliminating the need for labeled
data and joint training. Extensive experiments on the OPV2V dataset demonstrate
that PHCP achieves strong performance across diverse heterogeneous scenarios.
Notably, PHCP achieves performance comparable to SOTA methods trained on the
entire dataset while using only a small amount of unlabeled data.

</details>


### [86] [Image Recognition with Vision and Language Embeddings of VLMs](https://arxiv.org/abs/2509.09311)
*Illia Volkov,Nikita Kisel,Klara Janouskova,Jiri Matas*

Main category: cs.CV

> 研究发现语言和视觉提供了互补优势，因此提出了一种无学习融合方法来提升分类性能。

<details>
  <summary>Details</summary>

**Motivation:** 视觉语言模型在图像文本对齐上能够实现强大的零样本分类，然而，其纯粹的视觉推理能力鲜为人知。该研究旨在全面评估这些模型在图像分类中的表现。

**Method:** 通过使用一系列的双编码器视觉-语言模型（包括已建立的和最新的，如SigLIP 2和RADIOv2.5）来评估图像文本对齐在语言引导和纯视觉图像分类上的表现。重点关注关键因素分析如提示设计、类别多样性、k-NN中的邻居数量和参考集合大小。

**Result:** 实验结果显示了语言和视觉的互补性，某些类别更适合文本提示，而其他类别则更适合视觉相似性。

**Conclusion:** 通过基于每类精度的简单融合方法改善了图像分类性能。

**Abstract:** Vision-language models (VLMs) have enabled strong zero-shot classification
through image-text alignment. Yet, their purely visual inference capabilities
remain under-explored. In this work, we conduct a comprehensive evaluation of
both language-guided and vision-only image classification with a diverse set of
dual-encoder VLMs, including both well-established and recent models such as
SigLIP 2 and RADIOv2.5. The performance is compared in a standard setup on the
ImageNet-1k validation set and its label-corrected variant. The key factors
affecting accuracy are analysed, including prompt design, class diversity, the
number of neighbours in k-NN, and reference set size. We show that language and
vision offer complementary strengths, with some classes favouring textual
prompts and others better handled by visual similarity. To exploit this
complementarity, we introduce a simple, learning-free fusion method based on
per-class precision that improves classification performance. The code is
available at: https://github.com/gonikisgo/bmvc2025-vlm-image-recognition.

</details>
