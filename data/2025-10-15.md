<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 10]
- [cs.CV](#cs.CV) [Total: 13]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PHANTOM RECALL: When Familiar Puzzles Fool Smart Models](https://arxiv.org/abs/2510.11812)
*Souradeep Mukhopadhyay,Rishabh Baral,Nimeesh Mahajan,Samhitha Harish,Aswin RRV,Mihir Parmar,Mutsumi Nakamura,Chitta Baral*

Main category: cs.CL

> 该论文介绍了PHANTOM RECALL基准测试，使用25个经典逻辑谜题和149个经过精心设计的变形来评估大型语言模型（LLMs）的真实推理能力。研究发现，当谜题被稍微修改时，模型的性能急剧下降，说明它们大部分依赖于记忆的解决方案而不是从头开始的推理。

<details>
  <summary>Details</summary>

**Motivation:** 动机是为了系统地检查LLMs在面对逻辑推理任务时的真实能力，尤其是这些模型是否在稍微变化的环境下仍具备推理能力，以及是否有改进的提问方式来提升模型的表现。

**Method:** Structure

**Result:** {
  "tldr": "该论文介绍了PHANTOM RECALL基准测试，使用25个经典逻辑谜题和149个经过精心设计的变形来评估大型语言模型（LLMs）的真实推理能力。研究发现，当谜题被稍微修改时，模型的性能急剧下降，说明它们大部分依赖于记忆的解决方案而不是从头开始的推理。", 
  "motivation": "动机是为了系统地检查LLMs在面对逻辑推理任务时的真实能力，尤其是这些模型是否在稍微变化的环境下仍具备推理能力，以及是否有改进的提问方式来提升模型的表现。", 
  "method": "方法是引入PHANTOM RECALL基准，包含25个逻辑谜题和149个保持推理结构但改变表面细节的变形。通过检测模型对这些变形的回答，分析其表现模式。", 
  "result": "结果表现为模型在未经修改的谜题方面表现出很高的准确性，但在变形谜题上的表现显著低于人类，显示出"幽灵记忆"错误，说明模型过于依赖记忆的解决方案。", 
  "conclusion": "结论是LLMs在语言流畅性方面的表现与逻辑理解之间存在差距。当谜题环境变化时，模型往往未能重新推理，而是复制先前的记忆内容。这表明了完善这些模型推理能力的重要性。"}
}

**Conclusion:** 结论是LLMs在语言流畅性方面的表现与逻辑理解之间存在差距。当谜题环境变化时，模型往往未能重新推理，而是复制先前的记忆内容。这表明了完善这些模型推理能力的重要性。

**Abstract:** Large language models (LLMs) such as GPT, Gemini, and Claude often appear
adept at solving classic logic puzzles--but how much genuine reasoning
underlies their answers? Recent evidence suggests that these models frequently
rely on memorized templates rather than reasoning from first principles. When
puzzles are slightly modified, their performance collapses, revealing a
striking fragility. In particular, we asked: Have LLMs addressed these issues?
To what extent? How about perturbations to other puzzles? Is there a general
way of reformulating the prompt so that the models do better? To examine these
things systematically, we introduce PHANTOM RECALL, a benchmark comprising 25
well-known logic puzzles and 149 carefully designed perturbations that preserve
reasoning structure but alter superficial details and solutions. We evaluate
eleven leading LLMs and identify a recurring failure mode--phantom
recall--where models confidently reproduce memorized solutions or spurious
rationales that no longer fit the altered scenario. To probe and mitigate this
issue, we contribute three tools: (i) an automated logical-equivalence judge to
detect reasoning mismatches, (ii) a taxonomy of fine-grained reasoning error
categories, and (iii) a prompting-based mitigation framework guided by these
categories. Despite near-perfect accuracy on unmodified puzzles, models
significantly underperform humans on perturbed ones, exhibiting both phantom
recall and over-elaboration. Our findings reveal a crucial limitation: LLMs
often fail to re-reason when contextual cues shift--highlighting the gap
between linguistic fluency and logical understanding.

</details>


### [2] [R-WoM: Retrieval-augmented World Model For Computer-use Agents](https://arxiv.org/abs/2510.11892)
*Kai Mei,Jiang Guo,Shuaichen Chang,Mingwen Dong,Dongkyu Lee,Xing Niu,Jiarong Jiang*

Main category: cs.CL

> This paper examines the effectiveness of LLMs in world modeling for digital environments and introduces R-WoM to tackle the challenges of hallucination and static knowledge, showing significant performance improvements.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to explore the potential of LLMs in enhancing decision-making in digital environments through world modeling, while addressing the limitations posed by LLMs' hallucination and reliance on static knowledge.

**Method:** The paper investigates the capability of LLMs in world modeling by evaluating future state prediction and reward estimation through three specific tasks. It further proposes the Retrieval-augmented World Model (R-WoM) to improve the reliability of LLM simulations over long horizons by integrating up-to-date knowledge.

**Result:** The research finds that while LLMs can predict immediate next states effectively, their performance significantly diminishes in long-term planning, and the proposed R-WoM shows substantial improvements, up to 25.3% in OSWorld and 18.1% in WebArena.

**Conclusion:** The conclusion is that LLMs alone are not sufficient for reliable long-horizon world modeling due to inherent limitations, but the R-WoM approach can significantly enhance their capability by integrating factual, up-to-date knowledge.

**Abstract:** Large Language Models (LLMs) can serve as world models to enhance agent
decision-making in digital environments by simulating future states and
predicting action outcomes, potentially eliminating costly trial-and-error
exploration. However, this capability is fundamentally limited by LLMs'
tendency toward hallucination and their reliance on static training knowledge,
which can lead to compounding errors that inhibit long-horizon simulations. To
systematically investigate whether LLMs are appropriate for world modeling, we
probe two core capabilities of world models--future state prediction and reward
estimation--through three tasks: next-state identification, full-procedure
planning alignment, and milestone transition recognition. Our analysis shows
that while LLMs effectively capture immediate next states and identify
meaningful state transitions, their performance rapidly degrades in
full-procedure planning. This highlights LLMs' limitations in reliably modeling
environment dynamics over long horizons. To address these limitations, we
propose the Retrieval-augmented World Model (R-WoM), which grounds LLM
simulations by incorporating factual, up-to-date knowledge retrieved from
external tutorials. Experiments show that R-WoM achieves substantial
improvements of up to 25.3% (OSWorld) and 18.1% (WebArena) compared to
baselines, with particular advantages in longer-horizon simulations.

</details>


### [3] [LLM Knowledge is Brittle: Truthfulness Representations Rely on Superficial Resemblance](https://arxiv.org/abs/2510.11905)
*Patrick Haller,Mark Ibrahim,Polina Kirichenko,Levent Sagun,Samuel J. Bell*

Main category: cs.CL

> 本研究通过评估大规模语言模型（LLMs）在面对经过表面变形后样本时，其内部知识表示的能力，揭示了模型鲁棒性不足的问题，指出其学习的知识表示往往是浅层且不稳健的。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于探讨大规模语言模型（LLMs）的脆弱性是否直接源于不稳定的内部知识表示。这个问题源于LLMs在广泛研究中表现出的对输入微小变化的过度敏感。

**Method:** 通过构建在先前工作基础上的研究方法，该研究考察了LLM表示是否能够编码陈述的真实性，并通过评估经过浅表转换导致出分布（OOD）样本的表示分离性来测试学习知识的稳健性，其中包括错别字或重新表述。通过应用保持语义的扰动，探讨了陈述越OOD时分离性如何下降的情况，涉及四种LLM家族，五个评估数据集和三种知识探索方法。

**Result:** 研究结果显示，随着样本表面形式的变化，真实性和虚假性陈述之间的分离能力逐渐减弱，这表明LLMs在处理预训练分布以外的数据时性能会受到影响，暗示了提升LLMs鲁棒性的必要性。

**Conclusion:** 研究表明，当样本呈现方式与预训练数据相差较大时，内部的陈述真实性表示会发生崩解。虽然LLMs在表面形式和预训练数据相似时能够区分真实和虚假陈述，但这种能力高度依赖于陈述的具体表面形式。这些发现为LLMs在基准测试中的脆弱表现提供了可能的解释，并呼吁进一步研究以改善学习的知识表示的鲁棒性。

**Abstract:** For Large Language Models (LLMs) to be reliable, they must learn robust
knowledge that can be generally applied in diverse settings -- often unlike
those seen during training. Yet, extensive research has shown that LLM
performance can be brittle, with models exhibiting excessive sensitivity to
trivial input variations. In this work, we explore whether this brittleness is
a direct result of unstable internal knowledge representations. To explore this
question, we build on previous work showing that LLM representations encode
statement truthfulness -- i.e., true, factual statements can be easily
separated from false, inaccurate ones. Specifically, we test the robustness of
learned knowledge by evaluating representation separability on samples that
have undergone superficial transformations to drive them out-of-distribution
(OOD), such as typos or reformulations. By applying semantically-preserving
perturbations, we study how separability degrades as statements become more
OOD, across four LLM families, five evaluation datasets, and three knowledge
probing methods. Our results reveal that internal representations of statement
truthfulness collapse as the samples' presentations become less similar to
those seen during pre-training. While LLMs can often distinguish between true
and false statements when they closely resemble the pre-training data, this
ability is highly dependent on the statement's exact surface form. These
findings offer a possible explanation for brittle benchmark performance: LLMs
may learn shallow, non-robust knowledge representations that allow for only
limited generalizability. Our work presents a fundamental challenge for the
utility of truthfulness probes, and more broadly, calls for further research on
improving the robustness of learned knowledge representations.

</details>


### [4] [LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens](https://arxiv.org/abs/2510.11919)
*Armel Zebaze,Rachel Bawden,Benoît Sagot*

Main category: cs.CL

> 本文探讨了大型推理模型在机器翻译任务中的表现，并发现通过自然语言“思考标记”进行推理的策略并不能提高翻译性能。研究表明，与引导模型进行链式推理相比，通过特定模块提示策略产生的中间标记提升翻译表现更有效。此外，使用教师模型来优化目标翻译或扩展翻译语料库具有更大的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大规模推理模型在数学和编程任务上的能力和前景已经广为人知，但它们对机器翻译任务的影响尚未得到充分研究。研究的目标是评估生成中间标记对机器翻译的影响。

**Method:** 本研究探索了在多个语言对和不同资源水平下，机器翻译任务中生成中间标记（tokens）的好处。具体来说，探究了大型推理模型（LRMs）在翻译前通过自然语言思考过程生成“思考标记”的效果。同时，分析了基于人类译者实践的蒸馏链式思维（CoT）对翻译模型性能的影响。

**Result:** 研究发现，生成“思考标记”并不能帮助大规模推理模型在机器翻译任务中表现得更好。即使对模型进行微调，使其在翻译之前进行推理，基于人类翻译者实践的蒸馏链式思维（CoT）也没有显示出优势。然而，当中间标记是由组合模块特定翻译提示策略生成时，可以观察到性能的提高。

**Conclusion:** 研究结论指出，中间标记在微调过程中的贡献高度依赖于其中是否包含了翻译尝试。更广泛地看，使用教师级模态来细化目标翻译或扩展平行语料库，比将它们的思维链式解释蒸馏到“思考”机器翻译模型中更为有效。

**Abstract:** Large reasoning models (LRMs) have led to new possibilities in terms of
problem-solving, through the devising of a natural language thought process
prior to answering a query. While their capabilities are well known across
mathematics and coding tasks, their impact on the task of machine translation
(MT) remains underexplored. In this work, we explore the benefits of the
generation of intermediate tokens when performing MT across multiple language
pairs of different levels of resourcedness and multiple setups. We find that
"thinking tokens" do not help LRMs better perform MT. This result generalizes
to models fine-tuned to reason before translating using distilled chain of
thought (CoT) inspired by human translators' practices. Specifically,
fine-tuning a model with synthetic CoT explanations detailing how to translate
step-by-step does not outperform standard input-output fine-tuning. However,
constructing the intermediate tokens by combining the outputs of modular
translation-specific prompting strategies results in improvements. Our findings
underscore that the contribution of intermediate tokens during fine-tuning
highly depends on the presence of translation attempts within them. More
broadly, our results suggest that using a teacher to refine target translations
or to expand parallel corpora is more impactful than distilling their CoT
explanations into "thinking" MT models.

</details>


### [5] [Discrepancy Detection at the Data Level: Toward Consistent Multilingual Question Answering](https://arxiv.org/abs/2510.11928)
*Lorena Calvo-Bartolomé,Valérie Aldana,Karla Cantarero,Alonso Madroñal de Mesa,Jerónimo Arenas-García,Jordan Boyd-Graber*

Main category: cs.CL

> 提出了MIND管道，用于检测多语言问答系统的事实和文化差异，评估了其在母婴健康领域的双语系统中的效果，并展示了其在其他领域的泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 多语言问答系统需要在不同语言间确保事实的一致性，特别对于如“黄疸是什么？”这类客观查询。同时，系统还需考虑到主观回应中的文化差异。

**Method:** 我们提出了一种名为MIND的用户参与的事实核查管道，用于检测多语言问答知识库中的事实和文化差异。MIND能够突出显示文化敏感问题的答案差异，这些问题因地区和上下文而异。

**Result:** 我们在母婴健康领域的双语问答系统中评估了MIND，并发布了一组标注了事实和文化不一致性的双语问题数据集。我们在其他领域的数据集上进一步测试了MIND，以评估其泛化能力。结果表明，MIND能够可靠地识别不一致性。

**Conclusion:** MIND支持开发文化意识更强、事实更一致的问答系统，为进一步提升多语言问答系统的性能提供了可能。

**Abstract:** Multilingual question answering (QA) systems must ensure factual consistency
across languages, especially for objective queries such as What is jaundice?,
while also accounting for cultural variation in subjective responses. We
propose MIND, a user-in-the-loop fact-checking pipeline to detect factual and
cultural discrepancies in multilingual QA knowledge bases. MIND highlights
divergent answers to culturally sensitive questions (e.g., Who assists in
childbirth?) that vary by region and context. We evaluate MIND on a bilingual
QA system in the maternal and infant health domain and release a dataset of
bilingual questions annotated for factual and cultural inconsistencies. We
further test MIND on datasets from other domains to assess generalization. In
all cases, MIND reliably identifies inconsistencies, supporting the development
of more culturally aware and factually consistent QA systems.

</details>


### [6] [TopoAlign: A Framework for Aligning Code to Math via Topological Decomposition](https://arxiv.org/abs/2510.11944)
*Yupei Li,Philipp Borchert,Gerasimos Lampouras*

Main category: cs.CL

> 提出了一种名为TopoAlign的框架，可以解锁广泛可用的代码存储库作为数学大语言模型的训练资源。结果表明，TopoAlign框架为DeepSeek-Math模型提供了显著提升，提高了一次前10概率的等价性（BEq@10）17.77%和类型检查（typecheck@10）68.82%，即使没有引入新的数学知识，也使Herald模型提高了0.12%和1.09%。

<details>
  <summary>Details</summary>

**Motivation:** 尽管当前数学大语言模型（LLMs）在非正式和正式（如Lean 4）数学推理方面表现出色，但它们在自动形式化方面仍然存在困难，即把非正式的数学语句转化为正式的数学语句。自动形式化有助于将大语言模型的非正式推理与形式证明助手相结合，减少幻觉生成并确保机器可验证的结果。然而，现有数学大语言模型的性能受到了大规模语料库，尤其是包含非正式和正式语句对语料库稀缺性的限制。目前的模型虽然可以将自然语言指令转化为代码，但代码和形式化数学之间的结构和句法差异限制了迁移学习的效果。

**Method:** 提出了一种名为TopoAlign的框架，该框架可以解锁广泛可用的代码存储库作为数学大语言模型的训练资源。TopoAlign将代码分解为文档字符串、主函数和依赖函数，并将这些组件重新组装成形式上类似于数学陈述的模拟物，从而无需额外的人类标注即可生成用于训练数学大语言模型的结构对齐代码数据。

**Result:** 分别训练了两个最先进的模型，DeepSeek-Math和Herald，并在minif2f、Putnam和ProofNet基准测试上进行了评估。结果表明，尽管TopoAlign框架并未引入新的数学知识，依然为Herald模型带来了提升，即提高了0.12%的BEq@10和1.09%的typecheck@10。这说明，即使对于特定的数学LLM来说，使用结构对齐的代码数据进行训练依然有一些好处。

**Conclusion:** 通过研究，我们提出了一种创新的方法，利用广泛存在的代码资源来训练数学LLMs，旨在解决当前模型在形式化数学转换方面的性能瓶颈，并通过实验表明了这种方法的有效性。

**Abstract:** Large Language Models (LLMs) excel at both informal and formal (e.g. Lean 4)
mathematical reasoning but still struggle with autoformalisation, the task of
transforming informal into formal mathematical statements. Autoformalisation
helps pair the informal reasoning of LLMs with formal proof assistants which
enable machine-verifiable generation and mitigate hallucinations. Yet, the
performance of current Math LLMs is constrained by the scarcity of large-scale
corpora, particularly those containing pairs of informal and formal statements.
Although current models are trained to generate code from natural language
instructions, structural and syntactic differences between these and formal
mathematics limit effective transfer learning. We propose TopoAlign, a
framework that unlocks widely available code repositories as training resources
for Math LLMs. TopoAlign decomposes code into docstrings, main functions, and
dependency functions, and reassembles these components into analogues that
structurally mirror formal statements. This produces structurally aligned code
data that can be used for training Math LLMs without requiring additional human
annotation. We train two state-of-the-art models, DeepSeek-Math and Herald, and
evaluate them on the minif2f, Putnam, and ProofNet benchmarks. TopoAlign
provides substantial gains for DeepSeek-Math, improving performance by 17.77%
on BEq@10 and 68.82% on typecheck@10. Despite introducing no new mathematical
knowledge, our framework achieves gains of 0.12% and 1.09% for Herald on BEq@10
and typecheck@10, respectively, demonstrating that training on aligned code
data is beneficial even for specialized models.

</details>


### [7] [GRAVITY: A Framework for Personalized Text Generation via Profile-Grounded Synthetic Preferences](https://arxiv.org/abs/2510.11952)
*Priyanka Dey,Daniele Rosa,Wenqing Zheng,Daniel Barcklow,Jieyu Zhao,Emilio Ferrara*

Main category: cs.CL

> GRAVITY通过整合心理学框架合成偏好对以指导个性化内容生成，减少人工注释依赖，提升生成内容的吸引力，为LLM个性化提供可扩展路径。

<details>
  <summary>Details</summary>

**Motivation:** 为了减少对人工注释的依赖，引入GRAVITY框架，以合成的个人资料为基础生成偏好数据，捕获用户的兴趣、价值观、信仰和个人特质。

**Method:** 通过整合霍夫斯泰德的文化维度、施瓦茨的基本价值观、世界价值观调查和五大人格特质等心理学框架，GRAVITY 合成偏好对，以指导个性化内容生成。

**Result:** 相比于基于提示的条件处理、标准微调和简单的合成对生成，GRAVITY在不同文化背景下的书籍描述生成上表现出色，用户测试表明，其输出被偏好超过86%的时间。

**Conclusion:** 结果表明场景化的合成数据可以捕捉更丰富的用户变化，减少对昂贵注释的依赖，并生成更吸引人、以用户为中心的内容。

**Abstract:** Personalization in LLMs often relies on costly human feedback or interaction
logs, limiting scalability and neglecting deeper user attributes. To reduce the
reliance on human annotations, we introduce GRAVITY (Generative Response with
Aligned Values, Interests, and Traits of You), a framework for generating
synthetic, profile-grounded preference data that captures users' interests,
values, beliefs, and personality traits. By integrating demographic, cultural,
and psychological frameworks -- including Hofstede's cultural dimensions,
Schwartz's basic values, the World Values Survey, and Big Five OCEAN traits --
GRAVITY synthesizes preference pairs to guide personalized content generation.
We evaluate GRAVITY on book descriptions for 400 Amazon users, comparing it to
prompt-based conditioning, standard fine-tuning, and naive synthetic pair
generation. Profile-grounded synthetic data consistently improves generation,
especially across multiple cultures (USA, Brazil, Japan, India), achieving over
4% higher preference gains across baselines, with user studies showing that
GRAVITY outputs are preferred over 86% of the time. Our results show that
scenario-grounded synthetic data can capture richer user variation, reduce
reliance on costly annotation, and produce more engaging, user-centered
content, offering a scalable path for LLM personalization.

</details>


### [8] [Evaluating Retrieval-Augmented Generation Systems on Unanswerable, Uncheatable, Realistic, Multi-hop Queries](https://arxiv.org/abs/2510.11956)
*Gabrielle Kaili-May Liu,Bryan Li,Arman Cohan,William Gantt Walden,Eugene Yang*

Main category: cs.CL

> 本文介绍了一种创建不可欺骗、现实的、不可回答的和多跳问题的自动管道，并证明这种方法可以有效提高RAG系统的挑战性和真实性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的RAG基准测试很少反映多跳或超出范围问题的现实任务复杂性，这些问题可以通过不连接的推理（即，不需要真正的多跳推理）或只需简单的事实回忆来解决。这限制了这类基准测试揭露现有RAG系统限制的能力。

**Method:** 提出了一个自动创建不可欺骗的、真实的、不可回答的和多跳问题（CRUMQs）的管道，该管道可以适应任何语料库和领域。

**Result:** 实验显示，与先前的RAG基准测试相比，CRUMQs对RAG系统更具挑战性，降低了高达81.0%的欺骗性得分。

**Conclusion:** 此管道提供了一种简单的方法来增强基准的难度和真实性，推动了更强大的RAG系统的开发。

**Abstract:** Real-world use cases often present RAG systems with complex queries for which
relevant information is missing from the corpus or is incomplete. In these
settings, RAG systems must be able to reject unanswerable, out-of-scope queries
and identify failures of retrieval and multi-hop reasoning. Despite this,
existing RAG benchmarks rarely reflect realistic task complexity for multi-hop
or out-of-scope questions, which often can be cheated via disconnected
reasoning (i.e., solved without genuine multi-hop inference) or require only
simple factual recall. This limits the ability for such benchmarks to uncover
limitations of existing RAG systems. To address this gap, we present the first
pipeline for automatic, difficulty-controlled creation of
un$\underline{c}$heatable, $\underline{r}$ealistic, $\underline{u}$nanswerable,
and $\underline{m}$ulti-hop $\underline{q}$uerie$\underline{s}$ (CRUMQs),
adaptable to any corpus and domain. We use our pipeline to create CRUMQs over
two popular RAG datasets and demonstrate its effectiveness via benchmark
experiments on leading retrieval-augmented LLMs. Results show that compared to
prior RAG benchmarks, CRUMQs are highly challenging for RAG systems and achieve
up to 81.0\% reduction in cheatability scores. More broadly, our pipeline
offers a simple way to enhance benchmark difficulty and realism and drive
development of more capable RAG systems.

</details>


### [9] [Direct Multi-Token Decoding](https://arxiv.org/abs/2510.11958)
*Xuan Luo,Weizhi Wang,Xifeng Yan*

Main category: cs.CL

> 提出了一种称为Direct Multi-Token Decoding (DMTD) 的方法，它能够仅用晚期Decoder-only Transformer层生成多个令牌，从而提高语言模型生成效率。DMTD方法已在Qwen3-4B模型中展示了高达2倍的速度提升。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在减少使用LLMs时反复遍历早期和中期层的需求，从而提高生成多个令牌的效率。

**Method:** Decoder-only transformers构成大型语言模型（LLMs）的标准架构。研究表明，在经过预训练的LLMs中，早期、中期和晚期层可能具有不同的角色：早期层专注于理解输入上下文，中期层处理特定任务，晚期层将抽象表示转换为输出令牌。我们假设，一旦早期层和中期层处理了表示，这些隐藏状态可能足以仅使用晚期层生成多个令牌，从而消除了反复遍历早期和中期层的需要。我们称这种推理模式为直接多令牌解码（DMTD）。

**Result:** 基于有限数据集的微调DMTD Qwen3-4B模型已经显示了有希望的结果，实现了高达2倍的速度提升，并且只有很小的性能损失。

**Conclusion:** DMTD方法可以在不引入额外参数、辅助程序或后生成验证的情况下提高生成效率。在更大数据集上训练该模型，性能预期会进一步提高。

**Abstract:** Decoder-only transformers have become the standard architecture for large
language models (LLMs) due to their strong performance. Recent studies suggest
that, in pre-trained LLMs, early, middle, and late layers may serve distinct
roles: Early layers focus on understanding the input context, middle layers
handle task-specific processing, and late layers convert abstract
representations into output tokens. We hypothesize that once representations
have been processed by the early and middle layers, the resulting hidden states
may encapsulate sufficient information to support the generation of multiple
tokens using only the late layers, eliminating the need to repeatedly traverse
the early and middle layers. We refer to this inference paradigm as Direct
Multi-Token Decoding (DMTD). Unlike speculative decoding, our method introduces
no additional parameters, auxiliary routines, or post-generation verification.
Despite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model
has already demonstrated promising results, achieving up to a 2x speedup with
only minor performance loss. Moreover, as shown in our scaling analysis, its
performance is expected to further improve with larger training datasets.

</details>


### [10] [Scaling Long-Horizon LLM Agent via Context-Folding](https://arxiv.org/abs/2510.11967)
*Weiwei Sun,Miao Lu,Zhan Ling,Kang Liu,Xuesong Yao,Yiming Yang,Jiecao Chen*

Main category: cs.CL

> 提出了Context-Folding框架，通过程序分支和折叠处理长时任务，并开发了FoldGRPO强化学习框架，实现了在更小的活动上下文中匹配或超越基准模型的性能。

<details>
  <summary>Details</summary>

**Motivation:** 大语言模型在处理长时任务时受上下文长度限制，需要一个有效的方式管理上下文。

**Method:** Structure

**Result:** {
    "tldr": "提出了Context-Folding框架，通过程序分支和折叠处理长时任务，并开发了FoldGRPO强化学习框架，实现了在更小的活动上下文中匹配或超越基准模型的性能。",
    "motivation": "大语言模型在处理长时任务时受上下文长度限制，需要一个有效的方式管理上下文。",
    "method": "开发了Context-Folding框架和FoldGRPO强化学习框架，通过分支处理子任务并折叠回去，保留简洁结果摘要。",
    "result": "新的折叠代理在复杂长时任务中，在使用10倍小的活动上下文时可以匹配或超越ReAct基准，显著优于依赖总结的上下文管理模型。",
    "conclusion": "Context-Folding框架通过高效解除上下文限制，增强了LLM代理处理长时任务的能力。
}

**Conclusion:** Context-Folding框架通过高效解除上下文限制，增强了LLM代理处理长时任务的能力。

**Abstract:** Large language model (LLM) agents are fundamentally constrained by context
length on long-horizon tasks. We introduce Context-Folding, a framework that
empowers agents to actively manage their working context. An agent can
procedurally branch into a sub-trajectory to handle a subtask and then fold it
upon completion, collapsing the intermediate steps while retaining a concise
summary of the outcome. To make this behavior learnable, we develop an
end-to-end reinforcement learning framework FoldGRPO with specific process
rewards to encourage effective task decomposition and context management. On
complex long-horizon tasks (Deep Research and SWE), our folding agent matches
or outperforms the ReAct baselines while using an active context 10$\times$
smaller and significantly outperforms models that rely on summarization-based
context management.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [11] [Enhancing the Quality of 3D Lunar Maps Using JAXA's Kaguya Imagery](https://arxiv.org/abs/2510.11817)
*Yumi Iwashita,Haakon Moe,Yang Cheng,Adnan Ansar,Georgios Georgakis,Adrian Stoica,Kazuto Nakashima,Ryo Kurazume,Jim Torresen*

Main category: cs.CV

> 本文提出了一种识别和减少由Kaguya TC图像压缩导致的系统性差异噪声的方法，以提高3D月球地图的质量，实验结果证明了这种方法的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 随着全球对月球探索的力度加大，特别是对于像NASA的Endurance任务这样的长距离任务而言，高质量的3D月球地图的需求变得越来越重要。现有的Kaguya TC图像虽然全球覆盖，但其高度精度受到立体匹配错误和JPEG压缩伪影的影响。因此，提升这些3D地图的质量对于未来的月球任务至关重要。

**Method:** 本文提出了一种方法来提高从Kaguya TC图像生成的3D地图的质量，重点是减少由压缩引起的不同图像中的残余噪声，特别是在较暗的区域。通过分析Kaguya TC图像的压缩行为，识别出系统的差异噪声模式，并设法减少这些噪声，以增强地形数据的安全性和可靠性。

**Result:** 实验结果显示，所提出的方法能够有效减少高程噪声，提高地形数据的安全性和可靠性。

**Conclusion:** 该方法通过减少由压缩引起的残余噪声，有效提高了从Kaguya TC图像生成的3D地图的质量，从而增强了未来月球任务中地形数据的安全性和可靠性。

**Abstract:** As global efforts to explore the Moon intensify, the need for high-quality 3D
lunar maps becomes increasingly critical-particularly for long-distance
missions such as NASA's Endurance mission concept, in which a rover aims to
traverse 2,000 km across the South Pole-Aitken basin. Kaguya TC (Terrain
Camera) images, though globally available at 10 m/pixel, suffer from altitude
inaccuracies caused by stereo matching errors and JPEG-based compression
artifacts. This paper presents a method to improve the quality of 3D maps
generated from Kaguya TC images, focusing on mitigating the effects of
compression-induced noise in disparity maps. We analyze the compression
behavior of Kaguya TC imagery, and identify systematic disparity noise
patterns, especially in darker regions. In this paper, we propose an approach
to enhance 3D map quality by reducing residual noise in disparity images
derived from compressed images. Our experimental results show that the proposed
approach effectively reduces elevation noise, enhancing the safety and
reliability of terrain data for future lunar missions.

</details>


### [12] [Data or Language Supervision: What Makes CLIP Better than DINO?](https://arxiv.org/abs/2510.11835)
*Yiming Liu,Yuhui Zhang,Dhruba Ghosh,Ludwig Schmidt,Serena Yeung-Levy*

Main category: cs.CV

> 通过控制变量法比较CLIP和DINO，发现CLIP在VLM中的优势主要依赖于其高阶语义捕捉能力，而DINO更擅长低级特征检测。

<details>
  <summary>Details</summary>

**Motivation:** 研究CLIP相对于DINO在视觉-语言模型（VLM）中的优势是否源于语言监督还是较大的训练数据集。

**Method:** 通过在相同架构、数据集和训练配置下预训练CLIP和DINO，以分析语言监督和更大训练数据对CLIP性能优势的影响。

**Result:** CLIP在文本密集型任务上表现出色，而DINO在视觉为中心的任务上略有优势。语言监督的变体改进有限。

**Conclusion:** 这一研究为视觉编码器设计及其对VLM性能的影响提供了科学的见解。

**Abstract:** CLIP outperforms self-supervised models like DINO as vision encoders for
vision-language models (VLMs), but it remains unclear whether this advantage
stems from CLIP's language supervision or its much larger training data. To
disentangle these factors, we pre-train CLIP and DINO under controlled settings
-- using the same architecture, dataset, and training configuration --
achieving similar ImageNet accuracy. Embedding analysis shows that CLIP
captures high-level semantics (e.g., object categories, text), while DINO is
more responsive to low-level features like colors and styles. When integrated
into VLMs and evaluated on 20 VQA benchmarks, CLIP excels at text-intensive
tasks, while DINO slightly outperforms on vision-centric ones. Variants of
language supervision (e.g., sigmoid loss, pre-trained language encoders) yield
limited gains. Our findings provide scientific insights into vision encoder
design and its impact on VLM performance.

</details>


### [13] [MammoDINO: Anatomically Aware Self-Supervision for Mammographic Images](https://arxiv.org/abs/2510.11883)
*Sicheng Zhou,Lei Wu,Cao Xiao,Parminder Bhatia,Taha Kass-Hout*

Main category: cs.CV

> MammoDINO, a novel SSL framework for mammography, significantly improves performance on breast cancer screening tasks and enhances diagnostic efficiency.

<details>
  <summary>Details</summary>

**Motivation:** To improve the application of SSL in medical imaging, particularly mammography, addressing the issues of limited data and domain specific biases.

**Method:** Self-supervised learning (SSL) framework named MammoDINO, pretrained on 1.4 million mammographic images. Includes a breast tissue aware data augmentation sampler and a cross-slice contrastive learning objective.

**Result:** MammoDINO achieves state-of-the-art performance on multiple breast cancer screening tasks and generalizes well across five benchmark datasets.

**Conclusion:** It provides a scalable and annotation-free foundation for multipurpose computer-aided diagnosis (CAD) tools, potentially reducing radiologists' workload and improving screening efficiency.

**Abstract:** Self-supervised learning (SSL) has transformed vision encoder training in
general domains but remains underutilized in medical imaging due to limited
data and domain specific biases. We present MammoDINO, a novel SSL framework
for mammography, pretrained on 1.4 million mammographic images. To capture
clinically meaningful features, we introduce a breast tissue aware data
augmentation sampler for both image-level and patch-level supervision and a
cross-slice contrastive learning objective that leverages 3D digital breast
tomosynthesis (DBT) structure into 2D pretraining. MammoDINO achieves
state-of-the-art performance on multiple breast cancer screening tasks and
generalizes well across five benchmark datasets. It offers a scalable,
annotation-free foundation for multipurpose computer-aided diagnosis (CAD)
tools for mammogram, helping reduce radiologists' workload and improve
diagnostic efficiency in breast cancer screening.

</details>


### [14] [Task-Specific Dual-Model Framework for Comprehensive Traffic Safety Video Description and Analysis](https://arxiv.org/abs/2510.11907)
*Blessing Agyei Kyem,Neema Jakisa Owor,Andrews Danyo,Joshua Kofi Asamoah,Eugene Denteh,Tanner Muturi,Anthony Dontoh,Yaw Adu-Gyamfi,Armstrong Aboah*

Main category: cs.CV

> 本工作提出了一种独特的双模型框架，利用两个模型的互补优势解决交通安全分析问题，通过分别训练来提升模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 为了捕获细粒度的行为模式并生成全面的描述以预防事故，需要复杂的视频理解。本工作旨在通过这种方法来改进交通安全分析。

**Method:** 我们提出了一种独特的双模型框架，利用VideoLLaMA和Qwen2.5-VL互补的优势，分别进行任务特异性优化，以减少任务干扰，让每个模型更有效地专业化。

**Result:** 实验结果表明，VideoLLaMA在时间推理上表现良好，取得了1.1001的CIDEr分数，而Qwen2.5-VL在视觉理解方面具有60.80%的VQA准确率。在AI City Challenge Track 2的WTS数据集上，我们方法取得了45.7572的S2分数，在挑战排行榜上位列第十。

**Conclusion:** 实验验证了我们这种方法的有效性，分别训练策略在保持描述质量的同时，在VQA准确率上比联合训练提升了8.6%。

**Abstract:** Traffic safety analysis requires complex video understanding to capture
fine-grained behavioral patterns and generate comprehensive descriptions for
accident prevention. In this work, we present a unique dual-model framework
that strategically utilizes the complementary strengths of VideoLLaMA and
Qwen2.5-VL through task-specific optimization to address this issue. The core
insight behind our approach is that separating training for captioning and
visual question answering (VQA) tasks minimizes task interference and allows
each model to specialize more effectively. Experimental results demonstrate
that VideoLLaMA is particularly effective in temporal reasoning, achieving a
CIDEr score of 1.1001, while Qwen2.5-VL excels in visual understanding with a
VQA accuracy of 60.80\%. Through extensive experiments on the WTS dataset, our
method achieves an S2 score of 45.7572 in the 2025 AI City Challenge Track 2,
placing 10th on the challenge leaderboard. Ablation studies validate that our
separate training strategy outperforms joint training by 8.6\% in VQA accuracy
while maintaining captioning quality.

</details>


### [15] [PanoTPS-Net: Panoramic Room Layout Estimation via Thin Plate Spline Transformation](https://arxiv.org/abs/2510.11992)
*Hatem Ibrahem,Ahmed Salem,Qinmin Vivian Hu,Guanghui Wang*

Main category: cs.CV

> The paper presents PanoTPS-Net, which combines CNN and TPS transformation to accurately estimate room layouts based on single panorama images, achieving robust results on multiple datasets.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this research is the importance of accurately estimating the 3D layout of rooms, with potential applications in numerous fields such as robotics, augmented reality, and interior design.

**Method:** The paper introduces PanoTPS-Net, which is comprised of a CNN and a TPS transformation layer. The CNN extracts high-level features from input panorama images to estimate the spatial parameters of the TPS. The TPS transformation then warps a reference layout into the room layout required based on the predicted parameters.

**Result:** PanoTPS-Net achieves high 3DIoU values, 85.49, 86.16, 81.76, and 91.98, on four different datasets, and shows robustness in handling both cuboid and non-cuboid room layouts.

**Conclusion:** The proposed method, PanoTPS-Net, effectively estimates the room layout from a single panorama image and generalizes well to both cuboid and non-cuboid rooms. The model is highly accurate and robust, as evidenced by the results from extensive experiments.

**Abstract:** Accurately estimating the 3D layout of rooms is a crucial task in computer
vision, with potential applications in robotics, augmented reality, and
interior design. This paper proposes a novel model, PanoTPS-Net, to estimate
room layout from a single panorama image. Leveraging a Convolutional Neural
Network (CNN) and incorporating a Thin Plate Spline (TPS) spatial
transformation, the architecture of PanoTPS-Net is divided into two stages:
First, a convolutional neural network extracts the high-level features from the
input images, allowing the network to learn the spatial parameters of the TPS
transformation. Second, the TPS spatial transformation layer is generated to
warp a reference layout to the required layout based on the predicted
parameters. This unique combination empowers the model to properly predict room
layouts while also generalizing effectively to both cuboid and non-cuboid
layouts. Extensive experiments on publicly available datasets and comparisons
with state-of-the-art methods demonstrate the effectiveness of the proposed
method. The results underscore the model's accuracy in room layout estimation
and emphasize the compatibility between the TPS transformation and panorama
images. The robustness of the model in handling both cuboid and non-cuboid room
layout estimation is evident with a 3DIoU value of 85.49, 86.16, 81.76, and
91.98 on PanoContext, Stanford-2D3D, Matterport3DLayout, and ZInD datasets,
respectively. The source code is available at:
https://github.com/HatemHosam/PanoTPS_Net.

</details>


### [16] [Prompt-Guided Spatial Understanding with RGB-D Transformers for Fine-Grained Object Relation Reasoning](https://arxiv.org/abs/2510.11996)
*Tanner Muturi,Blessing Agyei Kyem,Joshua Kofi Asamoah,Neema Jakisa Owor,Richard Dyzinela,Andrews Danyo,Yaw Adu-Gyamfi,Armstrong Aboah*

Main category: cs.CV

> 本研究提出了一种专门用于仓库等大规模3D环境的空间推理框架，通过改善空间理解，研究实现了高级别的空间推理表现。

<details>
  <summary>Details</summary>

**Motivation:** 现有的视觉-语言系统在大规模3D环境中的空间推理上面临挑战，具体表现为对场景混乱、遮挡以及需要精确的空间理解的处理不佳。现有模型难以在这些环境下进行良好的泛化。

**Method:** 本研究提出了一种专门用于大规模3D空间环境（如仓库）的空间推理框架，该框架通过在输入提示中嵌入物体的边界框坐标来改进对物体几何形状和布局的理解。框架在四个问题类别上进行了任务特定的监督微调，包括距离估算、物体计数、多选题接地和空间关系推理。为了提升与评估系统的符合性，在训练集中还附上了归一化的答案。

**Result:** 研究提出的方法在Physical AI Spatial Intelligence Warehouse数据集上实现了综合评分为73.0606，位列公共排行榜第4名。

**Conclusion:** 实验证明，通过结构化提示增强和针对性优化可以提升现实世界工业环境中的空间推理能力。

**Abstract:** Spatial reasoning in large-scale 3D environments such as warehouses remains a
significant challenge for vision-language systems due to scene clutter,
occlusions, and the need for precise spatial understanding. Existing models
often struggle with generalization in such settings, as they rely heavily on
local appearance and lack explicit spatial grounding. In this work, we
introduce a dedicated spatial reasoning framework for the Physical AI Spatial
Intelligence Warehouse dataset introduced in the Track 3 2025 AI City
Challenge. Our approach enhances spatial comprehension by embedding mask
dimensions in the form of bounding box coordinates directly into the input
prompts, enabling the model to reason over object geometry and layout. We
fine-tune the framework across four question categories namely: Distance
Estimation, Object Counting, Multi-choice Grounding, and Spatial Relation
Inference using task-specific supervision. To further improve consistency with
the evaluation system, normalized answers are appended to the GPT response
within the training set. Our comprehensive pipeline achieves a final score of
73.0606, placing 4th overall on the public leaderboard. These results
demonstrate the effectiveness of structured prompt enrichment and targeted
optimization in advancing spatial reasoning for real-world industrial
environments.

</details>


### [17] [Evaluating the Explainability of Vision Transformers in Medical Imaging](https://arxiv.org/abs/2510.12021)
*Leili Barekatain,Ben Glocker*

Main category: cs.CV

> 研究评估了不同视觉变压器架构的可解释性，最终确认结合Grad-CAM的DINO提供了最佳解释。这有助于将视觉变压器透明地整合到医疗诊断中。

<details>
  <summary>Details</summary>

**Motivation:** 由于解释能力直接影响临床信任和采用，解释视觉变压器（ViT）在诊断成像中的复杂注意机制成为一项挑战，因此了解模型决策在医学成像中至关重要。

**Method:** 本研究评估了不同视觉变压器架构（ViT，DeiT，DINO和Swin Transformer）及其预训练策略的可解释性，使用了Gradient Attention Rollout和Grad-CAM两种方法，在外周血细胞分类和乳腺超声图像分类两个医疗成像任务中进行了定量和定性分析。

**Result:** 研究发现，结合Grad-CAM的DINO提供了最忠实和局部化的解释。Grad-CAM产生更具类别区分性且精确的空间热图，而在误分类情况下，DINO与Grad-CAM仍能突出临床相关的形态特征。

**Conclusion:** 通过提高模型的透明度，这项研究支持了可解释且可靠的视觉变压器在关键医疗诊断流程中的集成。

**Abstract:** Understanding model decisions is crucial in medical imaging, where
interpretability directly impacts clinical trust and adoption. Vision
Transformers (ViTs) have demonstrated state-of-the-art performance in
diagnostic imaging; however, their complex attention mechanisms pose challenges
to explainability. This study evaluates the explainability of different Vision
Transformer architectures and pre-training strategies - ViT, DeiT, DINO, and
Swin Transformer - using Gradient Attention Rollout and Grad-CAM. We conduct
both quantitative and qualitative analyses on two medical imaging tasks:
peripheral blood cell classification and breast ultrasound image
classification. Our findings indicate that DINO combined with Grad-CAM offers
the most faithful and localized explanations across datasets. Grad-CAM
consistently produces class-discriminative and spatially precise heatmaps,
while Gradient Attention Rollout yields more scattered activations. Even in
misclassification cases, DINO with Grad-CAM highlights clinically relevant
morphological features that appear to have misled the model. By improving model
transparency, this research supports the reliable and explainable integration
of ViTs into critical medical diagnostic workflows.

</details>


### [18] [APGNet: Adaptive Prior-Guided for Underwater Camouflaged Object Detection](https://arxiv.org/abs/2510.12056)
*Xinxin Huang,Han Sun,Junmin Cai,Ningzhong Liu,Huiyu Zhou*

Main category: cs.CV

> 提出APGNet，一种自适应先验引导的网络，解决了水下图像降质和伪装问题，在两个MAS数据集上表现出超越其他方法的效果。

<details>
  <summary>Details</summary>

**Motivation:** 为了应对水下物体检测中图像退化和自然伪装的挑战，解决现有方法在恢复关键特征和适应水下环境方面的问题。

**Method:** APGNet采用了一种自适应先验引导网络，该网络结合了Siamese架构和新型的先验引导机制，以增强鲁棒性和检测精度。方法包括：1) 使用多尺度视网膜颜色恢复算法进行数据增强，以减轻退化效应；2) 设计扩展感受野模块和多尺度渐进解码器来捕捉多尺度上下文信息并优化特征表示；3) 提出自适应先验引导机制，分层融合位置和边界先验，提升目标检测性能。

**Result:** APGNet在两个MAS数据集上进行了实验，结果表明其优于其他15种前沿方法。

**Conclusion:** 实验结果表明，APGNet在两个公开的MAS数据集上，相较于15种最先进的方法，在广泛使用的评估指标下表现更优。

**Abstract:** Detecting camouflaged objects in underwater environments is crucial for
marine ecological research and resource exploration. However, existing methods
face two key challenges: underwater image degradation, including low contrast
and color distortion, and the natural camouflage of marine organisms.
Traditional image enhancement techniques struggle to restore critical features
in degraded images, while camouflaged object detection (COD) methods developed
for terrestrial scenes often fail to adapt to underwater environments due to
the lack of consideration for underwater optical characteristics.
  To address these issues, we propose APGNet, an Adaptive Prior-Guided Network,
which integrates a Siamese architecture with a novel prior-guided mechanism to
enhance robustness and detection accuracy. First, we employ the Multi-Scale
Retinex with Color Restoration (MSRCR) algorithm for data augmentation,
generating illumination-invariant images to mitigate degradation effects.
Second, we design an Extended Receptive Field (ERF) module combined with a
Multi-Scale Progressive Decoder (MPD) to capture multi-scale contextual
information and refine feature representations. Furthermore, we propose an
adaptive prior-guided mechanism that hierarchically fuses position and boundary
priors by embedding spatial attention in high-level features for coarse
localization and using deformable convolution to refine contours in low-level
features.
  Extensive experimental results on two public MAS datasets demonstrate that
our proposed method APGNet outperforms 15 state-of-art methods under widely
used evaluation metrics.

</details>


### [19] [VIDMP3: Video Editing by Representing Motion with Pose and Position Priors](https://arxiv.org/abs/2510.12069)
*Sandeep Mishra,Oindrila Saha,Alan C. Bovik*

Main category: cs.CV

> 文章提出了VidMP3方法，旨在解决视频编辑中结构和语义编辑的灵活性问题，提供了一种在保持原有运动的基础上进行灵活编辑的新方案。

<details>
  <summary>Details</summary>

**Motivation:** 现有的扩散编辑方法在保持结构一致方面表现出色，但面对结构可变的编辑时，往往存在时间不一致、主体身份漂移和需要人工干预等问题。

**Method:** VidMP3采用姿态和位置先验来学习来自源视频的泛化运动表示，从而生成保持原始运动同时允许结构和语义灵活性的新视频。

**Result:** 定性和定量的评估都显示了本方法相较于现有方法的优势。

**Conclusion:** VidMP3通过引入姿态和位置先验，能够在视频编辑中实现结构和语义的灵活性。实验结果证明了该方法的有效性。

**Abstract:** Motion-preserved video editing is crucial for creators, particularly in
scenarios that demand flexibility in both the structure and semantics of
swapped objects. Despite its potential, this area remains underexplored.
Existing diffusion-based editing methods excel in structure-preserving tasks,
using dense guidance signals to ensure content integrity. While some recent
methods attempt to address structure-variable editing, they often suffer from
issues such as temporal inconsistency, subject identity drift, and the need for
human intervention. To address these challenges, we introduce VidMP3, a novel
approach that leverages pose and position priors to learn a generalized motion
representation from source videos. Our method enables the generation of new
videos that maintain the original motion while allowing for structural and
semantic flexibility. Both qualitative and quantitative evaluations demonstrate
the superiority of our approach over existing methods. The code will be made
publicly available at https://github.com/sandeep-sm/VidMP3.

</details>


### [20] [A Review on Domain Adaption and Generative Adversarial Networks(GANs)](https://arxiv.org/abs/2510.12075)
*Aashish Dhawan,Divyanshu Mudgal*

Main category: cs.CV

> This paper discusses domain adaptation methods to address the challenge of insufficient labeled data in image classification tasks.

<details>
  <summary>Details</summary>

**Motivation:** The scarcity of labeled data due to high labor costs and sometimes impossibility of data acquisition poses a significant challenge in computer vision tasks such as image classification.

**Method:** Structure

**Result:** The paper explores the use of domain adaptation techniques, focusing on how a model trained on one dataset (e.g., paintings of airplanes) can effectively predict on data from a different domain (e.g., real images of airplanes).

**Conclusion:** Domain adaptation is a promising approach to overcome data scarcity issues in computer vision by leveraging models trained on similar but different datasets.

**Abstract:** The major challenge in today's computer vision scenario is the availability
of good quality labeled data. In a field of study like image classification,
where data is of utmost importance, we need to find more reliable methods which
can overcome the scarcity of data to produce results comparable to previous
benchmark results. In most cases, obtaining labeled data is very difficult
because of the high cost of human labor and in some cases impossible. The
purpose of this paper is to discuss Domain Adaptation and various methods to
implement it. The main idea is to use a model trained on a particular dataset
to predict on data from a different domain of the same kind, for example - a
model trained on paintings of airplanes predicting on real images of airplanes

</details>


### [21] [Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback](https://arxiv.org/abs/2510.12089)
*Xingpei Ma,Shenneng Huang,Jiaran Cai,Yuansheng Guan,Shen Zheng,Hanfeng Zhao,Qiang Zhang,Shunsi Zhang*

Main category: cs.CV

> 提出了一种基于扩散转换器（DiT）框架生成任意长度的逼真说话视频的方法，并引入了一种无训练的多角色音频驱动动画方法。

<details>
  <summary>Details</summary>

**Motivation:** 虽然扩散模型在音频驱动的人类视频生成中取得了显著进展，但仍面临着口型同步准确性、长期视频生成的时间连贯性和多角色动画的挑战。

**Method:** 采用LoRA训练策略结合位置位移推理方法，以及部分参数更新与奖励反馈来增强口型同步和自然身体动作；提出了一种无训练的方法Mask-CFG，用于多角色动画。

**Result:** 实验结果表明，该方法在高质量、时间连贯性和多角色音频驱动视频生成方面优于现有最先进的方法。

**Conclusion:** 提出的方法以简单、高效和成本效益的方式实现了高质量、时间连贯性好的多角色音频驱动视频生成。

**Abstract:** Recent advances in diffusion models have significantly improved audio-driven
human video generation, surpassing traditional methods in both quality and
controllability. However, existing approaches still face challenges in lip-sync
accuracy, temporal coherence for long video generation, and multi-character
animation. In this work, we propose a diffusion transformer (DiT)-based
framework for generating lifelike talking videos of arbitrary length, and
introduce a training-free method for multi-character audio-driven animation.
First, we employ a LoRA-based training strategy combined with a position shift
inference approach, which enables efficient long video generation while
preserving the capabilities of the foundation model. Moreover, we combine
partial parameter updates with reward feedback to enhance both lip
synchronization and natural body motion. Finally, we propose a training-free
approach, Mask Classifier-Free Guidance (Mask-CFG), for multi-character
animation, which requires no specialized datasets or model modifications and
supports audio-driven animation for three or more characters. Experimental
results demonstrate that our method outperforms existing state-of-the-art
approaches, achieving high-quality, temporally coherent, and multi-character
audio-driven video generation in a simple, efficient, and cost-effective
manner.

</details>


### [22] [IL3D: A Large-Scale Indoor Layout Dataset for LLM-Driven 3D Scene Generation](https://arxiv.org/abs/2510.12095)
*Wenxu Zhou,Kaixuan Nie,Hang Du,Dong Yin,Wei Huang,Siqiang Guo,Xiaobo Zhang,Pengbo Hu*

Main category: cs.CV

> IL3D, 一个为大语言模型驱动的3D场景生成设计的数据集，提升了模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 解决室内布局设计领域对多样化、高质量训练数据的需求。

**Method:** IL3D, 一个专为大语言模型驱动的3D场景生成设计的大规模数据集，包含27,816个室内布局和29,215个高质量3D物体资产，并配有实例级自然语言注释，支持视觉语言任务的鲁棒多模态学习。

**Result:** 实验表明在IL3D上对LLMs进行监督微调显著提升了泛化能力，并超越了在其他数据集上的性能。

**Conclusion:** IL3D是一种多功能且健壮的资源，显著推进了3D场景生成和具身智能领域的研究。

**Abstract:** In this study, we present IL3D, a large-scale dataset meticulously designed
for large language model (LLM)-driven 3D scene generation, addressing the
pressing demand for diverse, high-quality training data in indoor layout
design. Comprising 27,816 indoor layouts across 18 prevalent room types and a
library of 29,215 high-fidelity 3D object assets, IL3D is enriched with
instance-level natural language annotations to support robust multimodal
learning for vision-language tasks. We establish rigorous benchmarks to
evaluate LLM-driven scene generation. Experimental results show that supervised
fine-tuning (SFT) of LLMs on IL3D significantly improves generalization and
surpasses the performance of SFT on other datasets. IL3D offers flexible
multimodal data export capabilities, including point clouds, 3D bounding boxes,
multiview images, depth maps, normal maps, and semantic masks, enabling
seamless adaptation to various visual tasks. As a versatile and robust
resource, IL3D significantly advances research in 3D scene generation and
embodied intelligence, by providing high-fidelity scene data to support
environment perception tasks of embodied agents.

</details>


### [23] [An Adaptive Edge-Guided Dual-Network Framework for Fast QR Code Motion Deblurring](https://arxiv.org/abs/2510.12098)
*Jianping Li,Dongyang Guo,Wenjie Li,Wei Zhao*

Main category: cs.CV

> 提出EG-Restormer和ADNet用于QR码去模糊，集成EGAB和LENet，实现实时处理和解码成功率的提升。

<details>
  <summary>Details</summary>

**Motivation:** 现有的深度学习方法很少明确利用QR码特有的边缘结构先验。本研究旨在提高严重模糊的QR码的解码率。

**Method:** 提出Edge-Guided Attention Block (EGAB), 将明确的边缘先验嵌入Transformer架构中，并基于EGAB构建了Edge-Guided Restormer (EG-Restormer)网络。对于轻微模糊的输入，设计了轻量高效的LENet网络。进一步将这两个网络整合为自适应双网络(ADNet)，根据输入模糊程度动态选择合适的网络。

**Result:** EG-Restormer和ADNet在实验中达到了最先进的性能，并且具备竞争力的速度。

**Conclusion:** 该研究针对QR码的去模糊问题提出了新的网络结构，提高了解码成功率，并适合资源受限的移动设备。

**Abstract:** Unlike general image deblurring that prioritizes perceptual quality, QR code
deblurring focuses on ensuring successful decoding. QR codes are characterized
by highly structured patterns with sharp edges, a robust prior for restoration.
Yet existing deep learning methods rarely exploit these priors explicitly. To
address this gap, we propose the Edge-Guided Attention Block (EGAB), which
embeds explicit edge priors into a Transformer architecture. Based on EGAB, we
develop Edge-Guided Restormer (EG-Restormer), an effective network that
significantly boosts the decoding rate of severely blurred QR codes. For mildly
blurred inputs, we design the Lightweight and Efficient Network (LENet) for
fast deblurring. We further integrate these two networks into an Adaptive
Dual-network (ADNet), which dynamically selects the suitable network based on
input blur severity, making it ideal for resource-constrained mobile devices.
Extensive experiments show that our EG-Restormer and ADNet achieve
state-of-the-art performance with a competitive speed. Project page:
https://github.com/leejianping/ADNet

</details>
