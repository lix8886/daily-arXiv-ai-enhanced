<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 7]
- [cs.CV](#cs.CV) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [The Hypocrisy Gap: Quantifying Divergence Between Internal Belief and Chain-of-Thought Explanation via Sparse Autoencoders](https://arxiv.org/abs/2602.02496)
*Shikhar Shiromani,Archie Chaudhury,Sri Pranav Kunda*

Main category: cs.CL

> 引入了“虚伪差距”这一机制性度量，利用稀疏自动编码器量化大型语言模型内部推理与最终答案之间的分歧，实验结果显示该方法在检测奉承和虚伪行为上优于对数概率基线。

<details>
  <summary>Details</summary>

**Motivation:** 检测大型语言模型（LLMs）通常表现出的不忠实行为，即为了取悦用户而产生与内在逻辑链（CoT）推理显著不同的答案。

**Method:** 利用稀疏自动编码器（SAEs）来量化模型内部推理和最终生成结果之间的分歧，通过在潜在空间中比较通过稀疏线性探针获取的内部真实信念与最终生成轨迹，量化并检测模型的不忠实行为。

**Result:** 在Gemma, Llama, 和 Qwen模型上使用Anthropic的奉承基准实验显示，本方法的AUROC范围在0.55-0.73之间，检测了奉承行为和虚伪行为，优于对数概率基线（0.41-0.50 AUROC）。

**Conclusion:** 实验表明，对Gemma, Llama, 和 Qwen模型在Anthropic的奉承基准上的测试证明了本方法能有效检测大型语言模型中的奉承和虚伪行为，AUROC为0.55-0.74。

**Abstract:** Large Language Models (LLMs) frequently exhibit unfaithful behavior, producing a final answer that differs significantly from their internal chain of thought (CoT) reasoning in order to appease the user they are conversing with. In order to better detect this behavior, we introduce the Hypocrisy Gap, a mechanistic metric utilizing Sparse Autoencoders (SAEs) to quantify the divergence between a model's internal reasoning and its final generation. By mathematically comparing an internal truth belief, derived via sparse linear probes, to the final generated trajectory in latent space, we quantify and detect a model's tendency to engage in unfaithful behavior. Experiments on Gemma, Llama, and Qwen models using Anthropic's Sycophancy benchmark show that our method achieves an AUROC of 0.55-0.73 for detecting sycophantic runs and 0.55-0.74 for hypocritical cases where the model internally "knows" the user is wrong, consistently outperforming a decision-aligned log-probability baseline (0.41-0.50 AUROC).

</details>


### [2] [STEMVerse: A Dual-Axis Diagnostic Framework for STEM Reasoning in Large Language Models](https://arxiv.org/abs/2602.02497)
*Xuzhao Li,Xuchen Li,Jian Zhao,Shiyu Hu*

Main category: cs.CL

> 论文提出了STEMVerse框架，用于诊断评估大型语言模型在STEM领域的推理能力，提供了多学科覆盖和细粒度的认知层级划分。

<details>
  <summary>Details</summary>

**Motivation:** 当前评估范式往往将基准视为孤立的“孤岛”，只提供整体的聚合得分，忽视了学术专长和认知深度的细微差别。这限制了模型在STEM领域推理能力的诊断价值。

**Method:** 提出STEMVerse框架，以系统地分析大型语言模型在STEM推理能力方面的表现。该框架通过跨学术专长和认知复杂性的模型性能特征，将来自主流基准的20,000多个STEM问题重新整合到一个统一的“学科×认知”能力空间中，并给每个实例分配双轴标签。

**Result:** 通过该框架的系统评估，揭示了于LLM在STEM推理方面结构上的失败模式。

**Conclusion:** STEMVerse框架通过整合多学科覆盖和细粒度的认知层次划分，为理解大型语言模型的科学推理特性提供了清晰且可操作的视角。

**Abstract:** As Large Language Models (LLMs) achieve significant breakthroughs in complex reasoning tasks, evaluating their proficiency in science, technology, engineering, and mathematics (STEM) has become a primary method for measuring machine intelligence. However, current evaluation paradigms often treat benchmarks as isolated "silos," offering only monolithic aggregate scores that neglect the intricacies of both academic specialization and cognitive depth. This result-oriented approach fails to distinguish whether model errors stem from insufficient domain knowledge or deficiencies in cognitive capacity, thereby limiting the diagnostic value. To address this, we propose STEMVerse, a diagnostic framework designed to systematically analyze the STEM reasoning capabilities of LLMs. This framework characterizes model performance across academic specialization and cognitive complexity to map the capability required for reasoning. We re-aggregate over 20,000 STEM problems from mainstream benchmarks into a unified "Discipline $\times$ Cognition" capability space, assigning dual-axis labels to every instance. Utilizing this unified diagnostic framework, we systematically evaluate representative LLM families across varying parameter scales and training paradigms. Our empirical results reveal structural failure patterns in STEM reasoning. By integrating multi-disciplinary coverage and fine-grained cognitive stratification into a unified framework, STEMVerse provides a clear and actionable perspective for understanding the scientific reasoning characteristics of LLMs.

</details>


### [3] [Test-Time Detoxification without Training or Learning Anything](https://arxiv.org/abs/2602.02498)
*Baturay Saglam,Dionysis Kalogerias*

Main category: cs.CL

> A test-time procedure for reducing toxic outputs from large language models is introduced, avoiding the need for retraining or internal model modifications. This method uses zeroth-order optimization to adjust text generation towards less toxic outputs efficiently and effectively.

<details>
  <summary>Details</summary>

**Motivation:** Large language models can produce toxic or inappropriate text even for benign inputs, creating risks when deployed at scale. The authors aim to detoxify the models without sacrificing their generation quality and to avoid methods that are costly or that do not transfer across model families.

**Method:** We introduce a test-time procedure that uses a small number of descent steps to steer generation toward less toxic continuations by approximating the gradient of completion toxicity with respect to the input embeddings. This is achieved with zeroth-order optimization that requires only access to input embeddings, a toxicity scoring function, and forward evaluations of the model.

**Result:** Empirically, the approach offers robust toxicity reductions across different models and prompts, highlighting its effectiveness in most settings compared to methods requiring retraining or changes in model architecture.

**Conclusion:** The proposed approach delivers robust toxicity reductions across models and prompts and achieves the best overall toxicity-quality trade-off in most settings. This emphasizes the potential of word embeddings as control variables and the use of black-box optimization in guiding safe text generation.

**Abstract:** Large language models can produce toxic or inappropriate text even for benign inputs, creating risks when deployed at scale. Detoxification is therefore important for safety and user trust, particularly when we want to reduce harmful content without sacrificing the model's generation quality. Many existing approaches rely on model retraining, gradients, or learned auxiliary components, which can be costly and may not transfer across model families or to truly black-box settings. We introduce a test-time procedure that approximates the gradient of completion toxicity with respect to the input embeddings and uses a small number of descent steps to steer generation toward less toxic continuations. This is achieved with zeroth-order optimization that requires only access to input embeddings, a toxicity scoring function, and forward evaluations of the model. Empirically, the approach delivers robust toxicity reductions across models and prompts and, in most settings, achieves the best overall toxicity-quality trade-off. More broadly, our work positions word embeddings as effective control variables and encourages wider use of black-box optimization to guide autoregressive language models toward scalable, safer text generation, without requiring any training or access to intermediate computations.

</details>


### [4] [ROSA-Tuning: Enhancing Long-Context Modeling via Suffix Matching](https://arxiv.org/abs/2602.02499)
*Yunao Zheng,Xiaojie Wang,Lei Ren,Wei Chen*

Main category: cs.CL

> ROSA-Tuning是一种新的长上下文建模方法，它通过CPU检索模块和GPU的范围限制注意力融合，既提高了长上下文的处理能力，又保持了计算效率，为解决大语言模型的长上下文能力和计算效率问题提供了新的解决方案。

<details>
  <summary>Details</summary>

**Motivation:** 现有的高效注意力方法虽然减少了计算复杂度，但通常会面临模型状态覆盖范围有限的问题。因此，ROSA-Tuning旨在解决长上下文能力和计算效率这两大挑战，特别是长上下文的建模能力。

**Method:** ROSA-Tuning提出了一个检索和召回机制，用于增强预训练模型的长上下文建模能力。除了标准的注意力机制，ROSA-Tuning并行地引入了一个基于CPU的ROSA检索模块，该模块可以高效地定位长上下文中的历史位置，这些位置与当前查询相关，并将检索到的信息以可训练的方式注入模型状态。后续的加权融合可以通过范围限制注意力来处理。

**Result:** 系统评估表明，ROSA-Tuning显著恢复了窗口注意力模型的长上下文建模能力，在LongBench等基准测试中，ROSA-Tuning达到了甚至在某些情况下超过了全局注意力的性能，同时保持了计算效率和GPU内存使用率接近窗口注意力方法。

**Conclusion:** ROSA-Tuning为长上下文的高效处理提供了一条新的技术路径，提高了长上下文的理解能力，同时兼具计算效率和低GPU内存占用，是语言模型发展的一个重要进展。

**Abstract:** Long-context capability and computational efficiency are among the central challenges facing today's large language models. Existing efficient attention methods reduce computational complexity, but they typically suffer from a limited coverage of the model state. This paper proposes ROSA-Tuning, a retrieval-and-recall mechanism for enhancing the long-context modeling ability of pretrained models. Beyond the standard attention mechanism, ROSA-Tuning introduces in parallel a CPU-based ROSA (RWKV Online Suffix Automaton) retrieval module, which efficiently locates historical positions in long contexts that are relevant to the current query, and injects the retrieved information into the model state in a trainable manner; subsequent weighted fusion can then be handled by range-restricted attention. To enable end-to-end training, we design a binary discretization strategy and a counterfactual gradient algorithm, and further optimize overall execution efficiency via an asynchronous CPU-GPU pipeline. Systematic evaluations on Qwen3-Base-1.7B show that ROSA-Tuning substantially restores the long-context modeling ability of windowed-attention models, achieving performance close to and in some cases matching global attention on benchmarks such as LongBench, while maintaining computational efficiency and GPU memory usage that are nearly comparable to windowed-attention methods, offering a new technical path for efficient long-context processing. The example code can be found at https://github.com/zyaaa-ux/ROSA-Tuning.

</details>


### [5] [Graph-Augmented Reasoning with Large Language Models for Tobacco Pest and Disease Management](https://arxiv.org/abs/2602.02635)
*Siyu Li,Chenwei Song,Qi Zhou,Wan Zhou,Xinyi Liu*

Main category: cs.CL

> 本文提出一种图增强推理框架，将领域知识整合进大型语言模型，用以提升烟草病虫害管理问题的回答质量并降低不恰当治疗建议的风险。实验表明该框架优于仅基于文本的方法。

<details>
  <summary>Details</summary>

**Motivation:** 烟草病虫害管理是一个复杂领域，需要专业知识和推理能力。现有方法主要依赖大型语言模型生成文本答案，然而，这些方法可能无法准确捕捉疾病与治疗方法之间的复杂关系，容易生成无效或不恰当的治疗建议。

**Method:** 本论文提出了一种基于图增强推理框架，将结构化的领域知识整合到大型语言模型中，用于烟草病虫害管理。框架基于GraphRAG，构建领域特定的知识图谱，并检索与查询相关的子图，提供关系证据辅助答案生成。采用ChatGLM作为Transformer主干，并利用LoRA进行参数高效微调。通过图神经网络学习捕获症状-疾病-治疗依赖关系的节点表示。通过显式建模疾病、症状、农药和控制措施作为相关实体，系统实现了超越表面文本相似度的证据推理。

**Result:** 实验结果显示，与仅基于文本的基线相比，该框架性能有所提升，尤其是在多步和比较推理问题上表现最为显着，这类问题需要链接多个关系。

**Conclusion:** 该框架展示了在烟草病虫害管理领域中，通过整合结构化领域知识和图增强推理能够有效提升推理性回答的质量，并减少生成不恰当治疗建议的风险。

**Abstract:** This paper proposes a graph-augmented reasoning framework for tobacco pest and disease management that integrates structured domain knowledge into large language models. Building on GraphRAG, we construct a domain-specific knowledge graph and retrieve query-relevant subgraphs to provide relational evidence during answer generation. The framework adopts ChatGLM as the Transformer backbone with LoRA-based parameter-efficient fine-tuning, and employs a graph neural network to learn node representations that capture symptom-disease-treatment dependencies. By explicitly modeling diseases, symptoms, pesticides, and control measures as linked entities, the system supports evidence-aware retrieval beyond surface-level text similarity. Retrieved graph evidence is incorporated into the LLM input to guide generation toward domain-consistent recommendations and to mitigate hallucinated or inappropriate treatments. Experimental results show consistent improvements over text-only baselines, with the largest gains observed on multi-hop and comparative reasoning questions that require chaining multiple relations.

</details>


### [6] [WideSeek: Advancing Wide Research via Multi-Agent Scaling](https://arxiv.org/abs/2602.02636)
*Ziyang Huang,Haolin Ren,Xiaowei Yuan,Jiawei Wang,Zhongtao Jiang,Kun Xu,Shizhu He,Jun Zhao,Kang Liu*

Main category: cs.CL

> 本研究开发了WideSeekBench基准测试和WideSeek架构，通过强化学习优化多代理轨迹，证明了多代理体系在广泛研究中的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于应对广泛研究领域当前面对的挑战，特别是基准测试和优化方法的缺乏。

**Method:** 本研究从数据管道和代理优化两个角度深入探讨了广泛研究。首先，创建了一个名为WideSeekBench的基准测试，它是一个通用的广泛信息检索基准测试，通过一个严格多阶段的数据管道构建，确保目标信息量、逻辑约束和领域的多样性。其次，介绍了WideSeek，一种动态层次化的多代理架构，能够根据任务要求自主分配并行子代理，并且设计了一个统一的训练框架，将多代理轨迹线性化并通过端到端的强化学习进行优化。

**Result:** 实验结果表明，WideSeek和多代理强化学习显示出显著的效果，增加代理数量对改进广泛研究是有前景的方向。

**Conclusion:** 研究表明，通过强化学习优化的多代理架构可以有效推进广泛研究领域的发展，并提出增加代理数量是一个促进广泛研究发展的有潜力的方法。

**Abstract:** Search intelligence is evolving from Deep Research to Wide Research, a paradigm essential for retrieving and synthesizing comprehensive information under complex constraints in parallel. However, progress in this field is impeded by the lack of dedicated benchmarks and optimization methodologies for search breadth. To address these challenges, we take a deep dive into Wide Research from two perspectives: Data Pipeline and Agent Optimization. First, we produce WideSeekBench, a General Broad Information Seeking (GBIS) benchmark constructed via a rigorous multi-phase data pipeline to ensure diversity across the target information volume, logical constraints, and domains. Second, we introduce WideSeek, a dynamic hierarchical multi-agent architecture that can autonomously fork parallel sub-agents based on task requirements. Furthermore, we design a unified training framework that linearizes multi-agent trajectories and optimizes the system using end-to-end RL. Experimental results demonstrate the effectiveness of WideSeek and multi-agent RL, highlighting that scaling the number of agents is a promising direction for advancing the Wide Research paradigm.

</details>


### [7] [Monotonicity as an Architectural Bias for Robust Language Models](https://arxiv.org/abs/2602.02686)
*Patrick Cooper,Alireza Nadali,Ashutosh Trivedi,Alvaro Velasquez*

Main category: cs.CL

> 研究展示通过在Transformer的前馈子层中引入次序保持的单调性来提升语言模型鲁棒性，而不牺牲其性能。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于改进大型语言模型面对对抗性提示和破解攻击时表现出的脆弱性问题。

**Method:** 本研究通过在Transformer中的前馈子层中强制执行单调性，来提升语言模型的鲁棒性。这种方法在保持注意力机制不受约束的同时确保后续语义细化是次序保持的。

**Result:** 实验结果表明，这种方法能够显著提高模型的鲁棒性，对抗攻击成功率从大约69%下降到19%，同时标准摘要生成性能仅轻微下降。

**Conclusion:** 研究得出，通过在前馈子层中实施单调性可以在保持模型性能的同时大幅提升其对抗攻击的鲁棒性，表明了单调性与神经语言模型之间传统认为的不相容性并非不可避免。

**Abstract:** Large language models (LLMs) are known to exhibit brittle behavior under adversarial prompts and jailbreak attacks, even after extensive alignment and fine-tuning. This fragility reflects a broader challenge of modern neural language models: small, carefully structured perturbations in high-dimensional input spaces can induce large and unpredictable changes in internal semantic representations and output.
  We investigate monotonicity as an architectural inductive bias for improving the robustness of Transformer-based language models. Monotonicity constrains semantic transformations so that strengthening information, evidence, or constraints cannot lead to regressions in the corresponding internal representations. Such order-preserving behavior has long been exploited in control and safety-critical systems to simplify reasoning and improve robustness, but has traditionally been viewed as incompatible with the expressivity required by neural language models.
  We show that this trade-off is not inherent. By enforcing monotonicity selectively in the feed-forward sublayers of sequence-to-sequence Transformers -- while leaving attention mechanisms unconstrained -- we obtain monotone language models that preserve the performance of their pretrained counterparts. This architectural separation allows negation, contradiction, and contextual interactions to be introduced explicitly through attention, while ensuring that subsequent semantic refinement is order-preserving. Empirically, monotonicity substantially improves robustness: adversarial attack success rates drop from approximately 69% to 19%, while standard summarization performance degrades only marginally.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [WorldVQA: Measuring Atomic World Knowledge in Multimodal Large Language Models](https://arxiv.org/abs/2602.02537)
*Runjie Zhou,Youbo Shao,Haoyu Lu,Bowei Xing,Tongtong Bai,Yujie Chen,Jie Zhao,Lin Sui,Haotian Yao,Zijia Zhao,Hao Yang,Haoning Wu,Zaida Zhou,Jinguo Zhu,Zhiqi Huang,Yiping Bao,Yangyang Liu,Y. Charles,Xinyu Zhou*

Main category: cs.CV

> 提出了WorldVQA基准测试，用于评估多模态大型语言模型的原子视觉世界知识，特别关注视觉知识检索而非推理能力，并涵盖从常见到稀有的视觉实体识别。

<details>
  <summary>Details</summary>

**Motivation:** 当前的评估测试往往混淆了视觉知识检索和推理能力，WorldVQA旨在单独评估模型“记住”的内容，以提供一个严格的视觉事实性测试标准。

**Method:** WorldVQA基准测试通过分层分类法，评估模型跨不同类别识别和命名视觉实体的能力，这些类别从常见的头部类别物体到长尾的稀有物体均有覆盖。

**Result:** 文章未提供具体实验结果，但表明WorldVQA将作为评估当前和未来多模态大型语言模型的百科全书式广度和幻觉率的新标准。

**Conclusion:** WorldVQA将作为视觉事实性测试的标准，用来衡量模型的百科全书式视野和减少幻觉率的能力。

**Abstract:** We introduce WorldVQA, a benchmark designed to evaluate the atomic visual world knowledge of Multimodal Large Language Models (MLLMs). Unlike current evaluations, which often conflate visual knowledge retrieval with reasoning, WorldVQA decouples these capabilities to strictly measure "what the model memorizes." The benchmark assesses the atomic capability of grounding and naming visual entities across a stratified taxonomy, spanning from common head-class objects to long-tail rarities. We expect WorldVQA to serve as a rigorous test for visual factuality, thereby establishing a standard for assessing the encyclopedic breadth and hallucination rates of current and next-generation frontier models.

</details>


### [9] [AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process](https://arxiv.org/abs/2602.02676)
*Xintong Zhang,Xiaowen Zhang,Jongrong Wu,Zhi Gao,Shilin Yan,Zhenxin Diao,Kunpeng Gao,Xuanyan Chen,Yuwei Wu,Yunde Jia,Qing Li*

Main category: cs.CV

> 本文提出了AdaptMMBench，这是一个全面的适应性多模式推理基准测试，涵盖了五个领域，使用的MCC指标可以更合理地评估模式选择和模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的评估方法依赖于静态难度标签和简单指标，无法捕捉难度随模型能力变化的动态特征。这使得难以区分适应性模式选择和一般性能，并且忽略了细粒度的过程分析。因此，需要一种新的评估方法，即AdaptMMBench。

**Method:** 本文提出AdaptMMBench，这是一个全面的适应性多模式推理基准测试，涵盖了五个领域：现实世界、OCR、GUI、知识和数学，包括直接感知和复杂推理任务。AdaptMMBench使用Matthews相关系数（MCC）指标来评估不同推理模式的选择合理性，通过动态识别任务难度来隔离这种元认知能力。此外，AdaptMMBench还支持多维度的过程评估，包括关键步骤覆盖、工具效果和计算效率。

**Result:** 评估结果显示，虽然适应性模式选择与模型容量成正比，但它与最终准确性的关系不显著。相反，关键步骤覆盖与性能一致，但工具效果在不同模型结构之间高度不一致。

**Conclusion:** AdaptMMBench提供了一种新的方法来评估多模态模型的适应性，其结果揭示了适应性模式选择与性能之间的复杂关系。

**Abstract:** Adaptive multimodal reasoning has emerged as a promising frontier in Vision-Language Models (VLMs), aiming to dynamically modulate between tool-augmented visual reasoning and text reasoning to enhance both effectiveness and efficiency. However, existing evaluations rely on static difficulty labels and simplistic metrics, which fail to capture the dynamic nature of difficulty relative to varying model capacities. Consequently, they obscure the distinction between adaptive mode selection and general performance while neglecting fine-grained process analyses. In this paper, we propose AdaptMMBench, a comprehensive benchmark for adaptive multimodal reasoning across five domains: real-world, OCR, GUI, knowledge, and math, encompassing both direct perception and complex reasoning tasks. AdaptMMBench utilizes a Matthews Correlation Coefficient (MCC) metric to evaluate the selection rationality of different reasoning modes, isolating this meta-cognition ability by dynamically identifying task difficulties based on models' capability boundaries. Moreover, AdaptMMBench facilitates multi-dimensional process evaluation across key step coverage, tool effectiveness, and computational efficiency. Our evaluation reveals that while adaptive mode selection scales with model capacity, it notably decouples from final accuracy. Conversely, key step coverage aligns with performance, though tool effectiveness remains highly inconsistent across model architectures.

</details>


### [10] [End-to-end reconstruction of OCT optical properties and speckle-reduced structural intensity via physics-based learning](https://arxiv.org/abs/2602.02721)
*Jinglun Yu,Yaning Wang,Wenhan Guo,Yuan Gao,Yu Sun,Jin U. Kang*

Main category: cs.CV

> 提出了一种深度学习框架，用于从OCT图像中重建光学参数图和减少斑点的结构强度图，从而实现更准确和鲁棒的组织特性表征。

<details>
  <summary>Details</summary>

**Motivation:** 克服光学相干断层扫描（OCT）中的反向散射问题，该问题由于衰减、斑点噪声以及参数之间的强耦合而具有挑战性。

**Method:** 提出了一种正则化的端到端深度学习框架，该框架结合了基于物理的OCT前向模型来生成预测信号，用于参数恢复和伪影抑制。

**Result:** 在合成的角膜OCT数据集上的实验显示，即使在噪声存在的情况下，也能实现稳健的光学图谱恢复、改进的分辨率和增强的结构保真度。

**Conclusion:** 这种方法实现了组织的定量多参数表征，并突出了将基于物理的建模与深度学习结合在计算OCT中的优势。

**Abstract:** Inverse scattering in optical coherence tomography (OCT) seeks to recover both structural images and intrinsic tissue optical properties, including refractive index, scattering coefficient, and anisotropy. This inverse problem is challenging due to attenuation, speckle noise, and strong coupling among parameters. We propose a regularized end-to-end deep learning framework that jointly reconstructs optical parameter maps and speckle-reduced OCT structural intensity for layer visualization. Trained with Monte Carlo-simulated ground truth, our network incorporates a physics-based OCT forward model that generates predicted signals from the estimated parameters, providing physics-consistent supervision for parameter recovery and artifact suppression. Experiments on the synthetic corneal OCT dataset demonstrate robust optical map recovery under noise, improved resolution, and enhanced structural fidelity. This approach enables quantitative multi-parameter tissue characterization and highlights the benefit of combining physics-informed modeling with deep learning for computational OCT.

</details>


### [11] [SVD-ViT: Does SVD Make Vision Transformers Attend More to the Foreground?](https://arxiv.org/abs/2602.02765)
*Haruhiko Murata,Kazuhiro Hotta*

Main category: cs.CV

> 提出SVD-ViT采用SVD技术优先学习前景特征，改善分类性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于Vision Transformers (ViT)缺乏区分前景和背景的有效机制，导致它们可能学习不必要的背景特征和伪影，损害分类性能。

**Method:** Vision Transformers (ViT)存在学习不必要的背景特征和伪影的问题，从而导致分类性能下降。为解决该问题，我们提出了SVD-ViT，该方法利用奇异值分解(SVD)优先学习前景特征。SVD-ViT由三个组成部分构成：SPC模块、SSVA和ID-RSVD，并通过提取和聚合捕获目标前景信息的奇异向量来抑制背景噪声和伪影等与任务无关的因素。

**Result:** 实验结果表明，我们的方法改善了分类精度，并有效地学习了具有信息量的前景表示，同时减少了背景噪声的影响。

**Conclusion:** SVD-ViT有效地提高了分类准确性，并减少了背景噪声的影响。这证明了使用SVD来区分前景特征的有效性。

**Abstract:** Vision Transformers (ViT) have been established as large-scale foundation models. However, because self-attention operates globally, they lack an explicit mechanism to distinguish foreground from background. As a result, ViT may learn unnecessary background features and artifacts, leading to degraded classification performance. To address this issue, we propose SVD-ViT, which leverages singular value decomposition (SVD) to prioritize the learning of foreground features. SVD-ViT consists of three components-\textbf{SPC module}, \textbf{SSVA}, and \textbf{ID-RSVD}-and suppresses task-irrelevant factors such as background noise and artifacts by extracting and aggregating singular vectors that capture object foreground information. Experimental results demonstrate that our method improves classification accuracy and effectively learns informative foreground representations while reducing the impact of background noise.

</details>
