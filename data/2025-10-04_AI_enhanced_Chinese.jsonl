{"id": "2510.01219", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01219", "abs": "https://arxiv.org/abs/2510.01219", "authors": ["Leroy Z. Wang"], "title": "Uncovering Implicit Bias in Large Language Models with Concept Learning Dataset", "comment": null, "summary": "We introduce a dataset of concept learning tasks that helps uncover implicit\nbiases in large language models. Using in-context concept learning experiments,\nwe found that language models may have a bias toward upward monotonicity in\nquantifiers; such bias is less apparent when the model is tested by direct\nprompting without concept learning components. This demonstrates that\nin-context concept learning can be an effective way to discover hidden biases\nin language models.", "AI": {"tldr": "通过特定任务集发现大型语言模型在上下文概念学习任务中对上行单调性的隐性偏见。", "motivation": "揭示大型语言模型内部存在的隐性偏见，特别在量词使用上。", "method": "通过上下文概念学习实验发现语言模型中的隐性偏见，特别是上行单调性在量词中的偏见。", "result": "发现语言模型在概念学习实验中的上行单调性偏见比直接提示测试中更为明显。", "conclusion": "上下文概念学习是一种有效发现语言模型中隐藏偏见的方法。"}}
{"id": "2510.01220", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01220", "abs": "https://arxiv.org/abs/2510.01220", "authors": ["Bonaventure F. P. Dossou", "Henri Aïdasso"], "title": "Towards Open-Ended Discovery for Low-Resource NLP", "comment": "Proceedings of the 2nd Workshop on Uncertainty-Aware NLP\n  (UncertaiNLP) at EMNLP 2025", "summary": "Natural Language Processing (NLP) for low-resource languages remains\nfundamentally constrained by the lack of textual corpora, standardized\northographies, and scalable annotation pipelines. While recent advances in\nlarge language models have improved cross-lingual transfer, they remain\ninaccessible to underrepresented communities due to their reliance on massive,\npre-collected data and centralized infrastructure. In this position paper, we\nargue for a paradigm shift toward open-ended, interactive language discovery,\nwhere AI systems learn new languages dynamically through dialogue rather than\nstatic datasets. We contend that the future of language technology,\nparticularly for low-resource and under-documented languages, must move beyond\nstatic data collection pipelines toward interactive, uncertainty-driven\ndiscovery, where learning emerges dynamically from human-machine collaboration\ninstead of being limited to pre-existing datasets. We propose a framework\ngrounded in joint human-machine uncertainty, combining epistemic uncertainty\nfrom the model with hesitation cues and confidence signals from human speakers\nto guide interaction, query selection, and memory retention. This paper is a\ncall to action: we advocate a rethinking of how AI engages with human knowledge\nin under-documented languages, moving from extractive data collection toward\nparticipatory, co-adaptive learning processes that respect and empower\ncommunities while discovering and preserving the world's linguistic diversity.\nThis vision aligns with principles of human-centered AI, emphasizing\ninteractive, cooperative model building between AI systems and speakers.", "AI": {"tldr": "本文提出了一种面向未来的方法，即在少资源语言和未充分记录语言的处理中，通过结合人机不确定性进行动态学习，以解决由于缺乏文本语料库和标注流水线所造成的制约。", "motivation": "论文的动机在于，尽管大语言模型在跨语言转移方面有了改进，但对于缺乏大量预收集数据和集中式基础设施的少资源和未充分记录的语言社区来说，这些方法仍然难以触及。作者认为亟需向开放性、交互式语言发现转变，即人工智能系统通过对话而非静态数据集动态学习新语言。", "method": "本文提出了一种新的框架，该框架基于联合的人机不确定性，结合模型的本体不确定性与人类说话者的犹豫线索和信心信号，以引导交互、查询选择和记忆保留。", "result": "", "conclusion": "该论文呼吁重新思考人工智能在未充分记录的语言中如何与人类知识互动，并提出从抽取式数据收集转向参与式、共同适应的学习过程的愿景，这种过程尊重并赋予社区权力，同时发现和保存世界上的语言多样性。"}}
{"id": "2510.01222", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01222", "abs": "https://arxiv.org/abs/2510.01222", "authors": ["Bertrand Kian Hassani", "Yacoub Bahini", "Rizwan Mushtaq"], "title": "Discourse vs emissions: Analysis of corporate narratives, symbolic practices, and mimicry through LLMs", "comment": null, "summary": "Climate change has increased demands for transparent and comparable corporate\nclimate disclosures, yet imitation and symbolic reporting often undermine their\nvalue. This paper develops a multidimensional framework to assess disclosure\nmaturity among 828 U.S.listed firms using large language models (LLMs)\nfine-tuned for climate communication. Four classifiers-sentiment, commitment,\nspecificity, and target ambition-extract narrative indicators from\nsustainability and annual reports, which are linked to firm attributes such as\nemissions, market capitalization, and sector. Analyses reveal three insights:\n(1) risk-focused narratives often align with explicit commitments, but\nquantitative targets (e.g., net-zero pledges) remain decoupled from tone; (2)\nlarger and higher-emitting firms disclose more commitments and actions than\npeers, though inconsistently with quantitative targets; and (3) widespread\nsimilarity in disclosure styles suggests mimetic behavior, reducing\ndifferentiation and decision usefulness. These results highlight the value of\nLLMs for ESG narrative analysis and the need for stronger regulation to connect\ncommitments with verifiable transition strategies.", "AI": {"tldr": "本论文开发了多维度框架，利用改进的大型语言模型评估了企业气候披露的质量，揭示了当前信息披露中的问题，并指出需要更严格的监管将企业的承诺与其行动相连接。", "motivation": "由于气候变化增加了对透明和可比的企业气候披露的需求，但模仿和象征性报告往往削弱其价值。因此，本论文旨在开发一种框架以评估企业披露的成熟度，并揭示企业信息披露的现状和问题。", "method": "本研究开发了一个多维度框架，使用为气候沟通优化过的大型语言模型（LLMs），对828家在美国上市的公司的信息披露成熟度进行评估。四个分类器——情感、承诺、具体性和目标雄心从可持续性和年度报告中提取叙述指标，并将其与公司的特征如排放量、市值和部门挂钩。", "result": "研究揭示了三点见解：(1) 风险导向的叙述常与明确的承诺相一致，但定量目标（如净零承诺）与语气脱钩；(2) 规模更大、排放更高的公司比同行披露更多的承诺和行动，尽管与定量目标保持不一致；(3) 信息披露方式的广泛相似性表明存在模仿行为，减少了差异性与决策的有用性。", "conclusion": "这些结果突显了LLMs在ESG叙事分析中的价值，并强调了需要更严格的监管来将承诺与可验证的转型策略相连接。"}}
{"id": "2510.01224", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01224", "abs": "https://arxiv.org/abs/2510.01224", "authors": ["Tyler J Poore", "Christopher J Pinard", "Aleena Shabbir", "Andrew Lagree", "Andre Telfer", "Kuan-Chuen Wu"], "title": "Context Matters: Comparison of commercial large language tools in veterinary medicine", "comment": "4 Figures, 10 pages", "summary": "Large language models (LLMs) are increasingly used in clinical settings, yet\ntheir performance in veterinary medicine remains underexplored. We evaluated\nthree commercially available veterinary-focused LLM summarization tools\n(Product 1 [Hachiko] and Products 2 and 3) on a standardized dataset of\nveterinary oncology records. Using a rubric-guided LLM-as-a-judge framework,\nsummaries were scored across five domains: Factual Accuracy, Completeness,\nChronological Order, Clinical Relevance, and Organization. Product 1 achieved\nthe highest overall performance, with a median average score of 4.61 (IQR:\n0.73), compared to 2.55 (IQR: 0.78) for Product 2 and 2.45 (IQR: 0.92) for\nProduct 3. It also received perfect median scores in Factual Accuracy and\nChronological Order. To assess the internal consistency of the grading\nframework itself, we repeated the evaluation across three independent runs. The\nLLM grader demonstrated high reproducibility, with Average Score standard\ndeviations of 0.015 (Product 1), 0.088 (Product 2), and 0.034 (Product 3).\nThese findings highlight the importance of veterinary-specific commercial LLM\ntools and demonstrate that LLM-as-a-judge evaluation is a scalable and\nreproducible method for assessing clinical NLP summarization in veterinary\nmedicine.", "AI": {"tldr": "研究评估了三种兽医专用LLM总结工具，发现第一个产品表现最佳，并确认了评估框架的可靠性和可重复性。", "motivation": "为了探究大型语言模型在兽医领域的性能，特别是兽医肿瘤学中的应用。", "method": "使用了一个基于规则的LLM评估框架，对三种商业化的兽医专用LLM摘要工具在兽医肿瘤学记录的标准数据集上的表现进行了评分。", "result": "Product 1表现最佳，整体评分中位数为4.61，显著高于Product 2和Product 3。评分框架在三次独立评估中表现出高度的一致性。", "conclusion": "研究强调了兽医专用商业LLM工具的重要性，并证明了使用LLM作为评估者的方法在临床自然语言处理摘要评估中可扩展且可重复。"}}
{"id": "2510.01339", "categories": ["cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01339", "abs": "https://arxiv.org/abs/2510.01339", "authors": ["Alessio Spagnoletti", "Andrés Almansa", "Marcelo Pereyra"], "title": "LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration", "comment": "23 pages, 12 figures", "summary": "Computational imaging methods increasingly rely on powerful generative\ndiffusion models to tackle challenging image restoration tasks. In particular,\nstate-of-the-art zero-shot image inverse solvers leverage distilled\ntext-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy\nand perceptual quality with high computational efficiency. However, extending\nthese advances to high-definition video restoration remains a significant\nchallenge, due to the need to recover fine spatial detail while capturing\nsubtle temporal dependencies. Consequently, methods that naively apply\nimage-based LDM priors on a frame-by-frame basis often result in temporally\ninconsistent reconstructions. We address this challenge by leveraging recent\nadvances in Video Consistency Models (VCMs), which distill video latent\ndiffusion models into fast generators that explicitly capture temporal\ncausality. Building on this foundation, we propose LVTINO, the first zero-shot\nor plug-and-play inverse solver for high definition video restoration with\npriors encoded by VCMs. Our conditioning mechanism bypasses the need for\nautomatic differentiation and achieves state-of-the-art video reconstruction\nquality with only a few neural function evaluations, while ensuring strong\nmeasurement consistency and smooth temporal transitions across frames.\nExtensive experiments on a diverse set of video inverse problems show\nsignificant perceptual improvements over current state-of-the-art methods that\napply image LDMs frame by frame, establishing a new benchmark in both\nreconstruction fidelity and computational efficiency.", "AI": {"tldr": "本文提出了一种基于视频一致性模型（VCMs）的高清晰度视频恢复逆解算器LVTINO，实现了高质量的视频重建，并且在计算效率上超越了现有的基于图像LDM的方法。", "motivation": "计算成像方法越来越依赖于强大的生成扩散模型来解决具有挑战性的图像恢复任务。然而，将这些进展扩展到高质量视频恢复面临着重大挑战，因为需要恢复精细的空间细节同时捕捉微妙的时态依赖性，而直接在帧对帧的基础上应用基于图像的LDM先验通常会导致时态上不一致的重建结果。", "method": "提出了一种称为LVTINO的零样本或即插即用的高清晰度视频恢复逆解算器，该解算器利用视频一致性模型（VCMs）作为先验知识，此方法避开了自动微分的需要，并仅通过几次神经函数评估即实现了最先进的视频重建质量，同时保证了测量的一致性和帧间平滑的过渡。", "result": "广泛的实验表明，LVTINO在对一系列视频逆问题的处理上，与当前最先进的逐帧应用图像LDM方法相比，显著地提高了感知质量，从而在重建保真度和计算效率方面建立了新的基准。", "conclusion": "通过利用视频一致性模型（VCMs）作为高清晰度视频恢复逆解算器的先验知识，我们实现了高质量的视频重建，改善了重建质量和计算效率。"}}
{"id": "2510.01226", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01226", "abs": "https://arxiv.org/abs/2510.01226", "authors": ["Akshith Reddy Putta", "Jacob Devasier", "Chengkai Li"], "title": "ClaimCheck: Real-Time Fact-Checking with Small Language Models", "comment": null, "summary": "We introduce ClaimCheck, an LLM-guided automatic fact-checking system\ndesigned to verify real-world claims using live Web evidence and small language\nmodels. Unlike prior systems that rely on large, closed-source models and\nstatic knowledge stores, ClaimCheck employs a transparent, stepwise\nverification pipeline that mirrors human fact-checking workflows consisting of\nWeb search query planning, Web-based evidence retrieval and summarization,\nevidence synthesis and re-retrieval, and claim verdict evaluation. Each module\nis optimized for small LLMs, allowing the system to deliver accurate and\ninterpretable fact-checking with significantly lower computational\nrequirements. Despite using a much smaller Qwen3-4B model, ClaimCheck achieves\nstate-of-the-art accuracy of 76.4% on the AVeriTeC dataset, outperforming\nprevious approaches using LLaMA3.1 70B and GPT-4o. Extensive ablations\ndemonstrate that careful modular design and prompting strategies can overcome\nthe limitations of smaller LLMs. To promote accessibility and transparency, we\nprovide a public demo at https://idir.uta.edu/claimcheck.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.01347", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01347", "abs": "https://arxiv.org/abs/2510.01347", "authors": ["Shuochen Chang"], "title": "Image Generation Based on Image Style Extraction", "comment": null, "summary": "Image generation based on text-to-image generation models is a task with\npractical application scenarios that fine-grained styles cannot be precisely\ndescribed and controlled in natural language, while the guidance information of\nstylized reference images is difficult to be directly aligned with the textual\nconditions of traditional textual guidance generation. This study focuses on\nhow to maximize the generative capability of the pretrained generative model,\nby obtaining fine-grained stylistic representations from a single given\nstylistic reference image, and injecting the stylistic representations into the\ngenerative body without changing the structural framework of the downstream\ngenerative model, so as to achieve fine-grained controlled stylized image\ngeneration. In this study, we propose a three-stage training style\nextraction-based image generation method, which uses a style encoder and a\nstyle projection layer to align the style representations with the textual\nrepresentations to realize fine-grained textual cue-based style guide\ngeneration. In addition, this study constructs the Style30k-captions dataset,\nwhose samples contain a triad of images, style labels, and text descriptions,\nto train the style encoder and style projection layer in this experiment.", "AI": {"tldr": "本研究旨在通过从单个给定的风格化参考图像获取细粒度的风格表示，并将其注入到生成体中，以最大化预训练生成模型的生成能力，实现细粒度控制的风格化图像生成。", "motivation": "研究动机在于解决文本到图像生成模型中难以精确描述和控制细粒度风格的问题，以及难以将风格化参考图像的指导信息与传统文本指导生成的文本条件直接对齐的问题。", "method": "本研究提出了一种基于风格提取的三阶段训练图像生成方法，利用风格编码器和风格投影层将风格表示与文本表示对齐，实现基于细粒度文本线索的风格导向生成。", "result": "该项研究构造了一个名为Style30k-captions的数据集，其中包含图像、风格标签和文本描述的三元组，用于训练风格编码器和风格投影层。", "conclusion": "结论表明，通过本研究提出的方法，可以实现细粒度的文本线索引导的图像风格生成，从而更好地满足特定风格的图像生成需求。"}}
{"id": "2510.01227", "categories": ["cs.CL", "math.HO"], "pdf": "https://arxiv.org/pdf/2510.01227", "abs": "https://arxiv.org/abs/2510.01227", "authors": ["Nicole N Khatibi", "Daniil A. Radamovich", "Michael P. Brenner"], "title": "EEFSUVA: A New Mathematical Olympiad Benchmark", "comment": "16 Pages, 5 figures", "summary": "Recent breakthroughs have spurred claims that large language models (LLMs)\nmatch gold medal Olympiad to graduate level proficiency on mathematics\nbenchmarks. In this work, we examine these claims in detail and assess the\nextent to which current benchmarks capture genuine LLM mathematical reasoning.\nThe composition of these benchmarks, primarily drawing from the International\nMathematics Olympiad (IMO) and related competitions, may overstate models\nreasoning ability due to potential data contamination and a narrow focus on\nfamiliar problem types. To enable a more holistic assessment of mathematical\nunderstanding, we introduce EEFSUVA, a novel benchmark curated from under\ncirculated regional and national Olympiads of Eastern Europe and the countries\nfrom the former Soviet Union. These contests feature problems of comparable\ndifficulty to the IMO and are renowned for demanding nonstandard\nproblem-solving techniques, yet their problems are far less prevalent in online\ncorpora. Preliminary results suggest that even state-of-the-art LLMs exhibit a\nnotable performance decline on EEFSUVA relative to other Olympiad-style\nbenchmarks. These findings also suggest the potential importance of broader\nevaluation datasets for a fuller assessment of mathematical reasoning and for\nguiding future model development.", "AI": {"tldr": "研究通过新引入的EEFSUVA基准测试模型的数学推理能力，发现即使先进LLMs在该基准上表现也有所下降，强调了需要更广泛的数据集来更全面评估数学推理。", "motivation": "研究目的是详细评估大型语言模型在数学理解方面的能力，并探讨现有基准测试是否能真正反映模型的数学推理能力。", "method": "引入EEFSUVA基准，该基准从东欧和前苏联国家未广泛传播的区域和国家奥林匹克试题中筛选而来，这类试题与IMO难度相当，但强调非标准的解题技巧，并且在线数据集中的出现频率较低，以此来更全面地评估模型的数学理解能力。", "result": "初步结果显示，与传统的Olympiad风格基准相比，即使最先进的LLMs在EEFSUVA上的表现也有所下降。", "conclusion": "结果表明，使用更广泛的评估数据集对更全面地评估数学推理能力以及指导未来模型开发的重要性。"}}
{"id": "2510.01362", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01362", "abs": "https://arxiv.org/abs/2510.01362", "authors": ["Shijia Feng", "Michael Wray", "Walterio Mayol-Cuevas"], "title": "EvoStruggle: A Dataset Capturing the Evolution of Struggle across Activities and Skill Levels", "comment": "10 pages", "summary": "The ability to determine when a person struggles during skill acquisition is\ncrucial for both optimizing human learning and enabling the development of\neffective assistive systems. As skills develop, the type and frequency of\nstruggles tend to change, and understanding this evolution is key to\ndetermining the user's current stage of learning. However, existing\nmanipulation datasets have not focused on how struggle evolves over time. In\nthis work, we collect a dataset for struggle determination, featuring 61.68\nhours of video recordings, 2,793 videos, and 5,385 annotated temporal struggle\nsegments collected from 76 participants. The dataset includes 18 tasks grouped\ninto four diverse activities -- tying knots, origami, tangram puzzles, and\nshuffling cards, representing different task variations. In addition,\nparticipants repeated the same task five times to capture their evolution of\nskill. We define the struggle determination problem as a temporal action\nlocalization task, focusing on identifying and precisely localizing struggle\nsegments with start and end times. Experimental results show that Temporal\nAction Localization models can successfully learn to detect struggle cues, even\nwhen evaluated on unseen tasks or activities. The models attain an overall\naverage mAP of 34.56% when generalizing across tasks and 19.24% across\nactivities, indicating that struggle is a transferable concept across various\nskill-based tasks while still posing challenges for further improvement in\nstruggle detection. Our dataset is available at\nhttps://github.com/FELIXFENG2019/EvoStruggle.", "AI": {"tldr": "本文介绍了EvoStruggle数据集，通过定量化挣扎决策来研究技能获取过程中挣扎的变化，以及建立时间动作定位模型来检测挣扎。实验表明模型具有跨任务和跨活动的较好泛化能力。", "motivation": "确定技能训练过程中个人挣扎的时机对于优化人类学习和发展有效的辅助系统至关重要。但是现有的操作数据集没有关注挣扎如何随时间演变。这项研究填补了这一空白。", "method": "通过收集一个包含61.68小时视频记录，2,793个视频和5,385个时间标记挣扎段的数据集来确定挣扎（struggle）问题，该数据集来自76名参与者。参与者被要求五次重复执行同一任务，以捕捉技能的演变。挣扎问题被定义为一个时间动作定位任务，专注于识别并精确地定位挣扎段落的开始和结束时间。", "result": "实验结果显示，时间动作定位模型可以成功地学习检测挣扎线索，即使是在未见过的任务或活动中。在跨任务和跨活动的通用性上，模型分别达到了34.56%和19.24%的整体平均mAP。", "conclusion": "挣扎是可以在多种技能任务之间转移的概念，虽然目前挣扎检测仍然存在改进的空间。"}}
{"id": "2510.01228", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01228", "abs": "https://arxiv.org/abs/2510.01228", "authors": ["Siqi Zeng"], "title": "Who is In Charge? Dissecting Role Conflicts in Instruction Following", "comment": null, "summary": "Large language models should follow hierarchical instructions where system\nprompts override user inputs, yet recent work shows they often ignore this rule\nwhile strongly obeying social cues such as authority or consensus. We extend\nthese behavioral findings with mechanistic interpretations on a large-scale\ndataset. Linear probing shows conflict-decision signals are encoded early, with\nsystem-user and social conflicts forming distinct subspaces. Direct Logit\nAttribution reveals stronger internal conflict detection in system-user cases\nbut consistent resolution only for social cues. Steering experiments show that,\ndespite using social cues, the vectors surprisingly amplify instruction\nfollowing in a role-agnostic way. Together, these results explain fragile\nsystem obedience and underscore the need for lightweight hierarchy-sensitive\nalignment methods.", "AI": {"tldr": "本文探究了大语言模型在处理系统提示与用户输入、社会暗示时的内在机制，揭示其内部冲突检测机制，发现社会暗示增强了指令遵循性，指出需要开发层次敏感的对齐方法。", "motivation": "尽管大语言模型应按照分层指令执行，其中系统提示应优先于用户输入，但现有研究表明，模型往往忽视这一规则而过分遵守权威或共识等社会线索。本文旨在解释这一现象背后的原因。", "method": "本文通过线性探测、直接逻辑归属和引导实验等方法扩展了之前关于大语言模型行为研究的发现，探究了模型在处理系统提示和用户输入以及社会暗示冲突时的内在机制。", "result": "研究发现冲突决策的信号在模型早期编码中就已经形成，系统提示与用户输入和社交冲突各自形成了不同的子空间。直接逻辑归属揭示了在系统与用户提示的冲突中，模型内部冲突检测更强，但在只有社交暗示的情况下才能持续解决冲突。尽管大语言模型会使用社会暗示，引导实验却意外地揭示了，这些向量以角色无关的方式增强了指令遵循的稳定性。", "conclusion": "这些结果解释了大语言模型中系统服从性的脆弱性，并强调了需要开发轻量级、层次敏感的对齐方法的重要性。"}}
{"id": "2510.01370", "categories": ["cs.CV", "cs.AI", "cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.01370", "abs": "https://arxiv.org/abs/2510.01370", "authors": ["Abu Bucker Siddik", "Diane Oyen", "Alexander Most", "Michal Kucer", "Ayan Biswas"], "title": "SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs", "comment": null, "summary": "We introduce Small PDE U-Net Solver (SPUS), a compact and efficient\nfoundation model (FM) designed as a unified neural operator for solving a wide\nrange of partial differential equations (PDEs). Unlike existing\nstate-of-the-art PDE FMs-primarily based on large complex transformer\narchitectures with high computational and parameter overhead-SPUS leverages a\nlightweight residual U-Net-based architecture that has been largely\nunderexplored as a foundation model architecture in this domain. To enable\neffective learning in this minimalist framework, we utilize a simple yet\npowerful auto-regressive pretraining strategy which closely replicates the\nbehavior of numerical solvers to learn the underlying physics. SPUS is\npretrained on a diverse set of fluid dynamics PDEs and evaluated across 6\nchallenging unseen downstream PDEs spanning various physical systems.\nExperimental results demonstrate that SPUS using residual U-Net based\narchitecture achieves state-of-the-art generalization on these downstream tasks\nwhile requiring significantly fewer parameters and minimal fine-tuning data,\nhighlighting its potential as a highly parameter-efficient FM for solving\ndiverse PDE systems.", "AI": {"tldr": "该研究提出了一种名为SPUS的基于轻量级残差U-Net架构的基础模型，用于解决多种偏微分方程，相比现有模型，SPUS在参数效率和泛化能力上表现出了显著优势。", "motivation": "为了在这样一个极简的框架中实现有效的学习，作者决定采用一种基于轻量级残差U-Net架构的基底模型，与大多数高计算和参数开销的大型复杂变压器架构的基底模型不同。", "method": "本文介绍了Small PDE U-Net Solver (SPUS)，这是一个基于轻量级残差U-Net架构的紧凑、高效的基底模型（FM），用于解决广泛的偏微分方程（PDEs）。SPUS采用了一种简单但强大的自回归预训练策略，以学习数字求解器的行为并掌握底层物理。", "result": "实验结果表明，使用残差U-Net架构的SPUS在下游任务上达到了最先进的泛化性能，同时所需参数较少，且需要的微调数据有限。", "conclusion": "这突显了SPUS作为解决多样化PDE系统的高参数效率基底模型的潜在价值。"}}
{"id": "2510.01229", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01229", "abs": "https://arxiv.org/abs/2510.01229", "authors": ["Dimitar Peshevski", "Kiril Blazhevski", "Martin Popovski", "Gjorgji Madjarov"], "title": "Enhancing Transformer-Based Rerankers with Synthetic Data and LLM-Based Supervision", "comment": "Accepted by RANLP 2025", "summary": "Effective document reranking is essential for improving search relevance\nacross diverse applications. While Large Language Models (LLMs) excel at\nreranking due to their deep semantic understanding and reasoning, their high\ncomputational cost makes them impractical for many real-world deployments.\nFine-tuning smaller, task-specific models is a more efficient alternative but\ntypically depends on scarce, manually labeled data. To overcome this, we\npropose a novel pipeline that eliminates the need for human-labeled\nquery-document pairs. Our method uses LLMs to generate synthetic queries from\ndomain-specific corpora and employs an LLM-based classifier to label positive\nand hard-negative pairs. This synthetic dataset is then used to fine-tune a\nsmaller transformer model with contrastive learning using Localized Contrastive\nEstimation (LCE) loss. Experiments on the MedQuAD dataset show that our\napproach significantly boosts in-domain performance and generalizes well to\nout-of-domain tasks. By using LLMs for data generation and supervision rather\nthan inference, we reduce computational costs while maintaining strong\nreranking capabilities.", "AI": {"tldr": "A method for generating synthetic data using LLMs to efficiently fine-tune smaller models for document reranking, balancing computational efficiency and re-ranking performance.", "motivation": "While LLMs are good at reranking documents due to their deep semantic understanding and reasoning, their high computational cost makes them impractical for many real-world deployments. To address this issue and reduce dependency on scarce, manually labeled data, a novel pipeline is proposed to eliminate the need for human-labeled query-document pairs.", "method": "Our method uses LLMs to generate synthetic queries from domain-specific corpora and employs an LLM-based classifier to label positive and hard-negative pairs. This synthetic dataset is then used to fine-tune a smaller transformer model with contrastive learning using Localized Contrastive Estimation (LCE) loss.", "result": "Experiments on the MedQuAD dataset show that our approach significantly boosts in-domain performance and generalizes well to out-of-domain tasks.", "conclusion": "The proposed pipeline demonstrates the potential to reduce computational costs while maintaining strong reranking capabilities through using LLMs for data generation and supervision instead of inference."}}
{"id": "2510.01399", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01399", "abs": "https://arxiv.org/abs/2510.01399", "authors": ["Shubhankar Borse", "Farzad Farhadzadeh", "Munawar Hayat", "Fatih Porikli"], "title": "DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation", "comment": null, "summary": "State-of-the-art text-to-image models excel at realism but collapse on\nmulti-human prompts - duplicating faces, merging identities, and miscounting\nindividuals. We introduce DisCo (Reinforcement with Diversity Constraints), the\nfirst RL-based framework to directly optimize identity diversity in multi-human\ngeneration. DisCo fine-tunes flow-matching models via Group-Relative Policy\nOptimization (GRPO) with a compositional reward that (i) penalizes intra-image\nfacial similarity, (ii) discourages cross-sample identity repetition, (iii)\nenforces accurate person counts, and (iv) preserves visual fidelity through\nhuman preference scores. A single-stage curriculum stabilizes training as\ncomplexity scales, requiring no extra annotations. On the DiverseHumans\nTestset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global\nIdentity Spread - surpassing both open-source and proprietary methods (e.g.,\nGemini, GPT-Image) while maintaining competitive perceptual quality. Our\nresults establish DisCo as a scalable, annotation-free solution that resolves\nthe long-standing identity crisis in generative models and sets a new benchmark\nfor compositional multi-human generation.", "AI": {"tldr": "研究团队提出了DisCo框架，这是一个基于强化学习的方法，通过优化生成的多个人物的身份多样性，解决了现有文本到图像模型生成多个人物图像时所遇到的问题，显著提高了生成人物的唯一性和身份多样性。", "motivation": "现有的文本到图像转换模型在实现现实主义方面表现出色，但在处理多人体提示时会出现问题，比如复制面部、合并身份和人数统计不准确等问题。此研究旨在解决这些问题。", "method": "我们介绍了一种名为DisCo（利用多样性约束的强化）的新型框架，这是第一个直接优化多人体生成的身份多样性的深度强化学习框架。DisCo通过组相关策略优化（GRPO）细化了流动匹配模型，并采用了一种组合奖励机制来实现以下目标：（i）对图像内部的面部相似性进行惩罚；（ii）减少跨样本身份的重复；（iii）强制执行准确的人数计数；（iv）通过人类偏好分数保持视觉保真度。", "result": "在DiverseHumans测试集上的结果表明，DisCo实现了高达98.6的唯一面部准确度和接近完美的全局身份散布，超过了开源和专有的方法（例如Gemini，GPT-Image），同时保持了高度的感知质量。", "conclusion": "我们的研究结果确立了DisCo作为一个可扩展且不需要额外注释的解决方案，有效解决了生成模型中的长期身份危机问题，并为多人体生成设定了新的基准。"}}
{"id": "2510.01230", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01230", "abs": "https://arxiv.org/abs/2510.01230", "authors": ["Wen G. Gong"], "title": "Geometric Structures and Patterns of Meaning: A PHATE Manifold Analysis of Chinese Character Embeddings", "comment": "33 pages, 17 figures", "summary": "We systematically investigate geometric patterns in Chinese character\nembeddings using PHATE manifold analysis. Through cross-validation across seven\nembedding models and eight dimensionality reduction methods, we observe\nclustering patterns for content words and branching patterns for function\nwords. Analysis of over 1000 Chinese characters across 12 semantic domains\nreveals that geometric complexity correlates with semantic content: meaningful\ncharacters exhibit rich geometric diversity while structural radicals collapse\ninto tight clusters. The comprehensive child-network analysis (123 phrases)\ndemonstrates systematic semantic expansion from elemental character. These\nfindings provide computational evidence supporting traditional linguistic\ntheory and establish a novel framework for geometric analysis of semantic\norganization.", "AI": {"tldr": "研究使用PHATE分析法研究中文字符嵌入的几何模式，证实了语义内容与几何复杂性之间的相关性，支持传统语言学理论并提出一种语义组织的几何分析新框架。", "motivation": "研究的动机在于通过计算方法支持传统语言学理论，并建立一种语义组织的几何分析新框架。", "method": "使用PHATE流形分析法系统地研究中文字符嵌入中的几何模式，并通过七种嵌入模型和八种降维方法进行交叉验证。", "result": "观察到实义词的聚类模式和功能词的分支模式，揭示了语义内容与几何复杂性之间的相关性：有意义的字符表现出丰富的几何多样性，而结构部首则聚集成紧密的簇。", "conclusion": "研究结果表明几何复杂性与语义内容有关，支持了传统的语言学理论，并提供了一种全新的几何分析方法来研究语义组织。"}}
{"id": "2510.01448", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01448", "abs": "https://arxiv.org/abs/2510.01448", "authors": ["Angel Daruna", "Nicholas Meegan", "Han-Pang Chiu", "Supun Samarasekera", "Rakesh Kumar"], "title": "GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings", "comment": "preprint under review", "summary": "Worldwide visual geo-localization seeks to determine the geographic location\nof an image anywhere on Earth using only its visual content. Learned\nrepresentations of geography for visual geo-localization remain an active\nresearch topic despite much progress. We formulate geo-localization as aligning\nthe visual representation of the query image with a learned geographic\nrepresentation. Our novel geographic representation explicitly models the world\nas a hierarchy of geographic embeddings. Additionally, we introduce an approach\nto efficiently fuse the appearance features of the query image with its\nsemantic segmentation map, forming a robust visual representation. Our main\nexperiments demonstrate improved all-time bests in 22 out of 25 metrics\nmeasured across five benchmark datasets compared to prior state-of-the-art\n(SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional\nablation studies support the claim that these gains are primarily driven by the\ncombination of geographic and visual representations.", "AI": {"tldr": "The paper presents a novel method for visual geo-localization, involving hierarchical geographic embeddings and robust fusion of visual features. It outperforms previous state-of-the-art methods in multiple benchmarks.", "motivation": "Despite progress in learned geographic representations for visual geo-localization, this domain remains an active research area. The aim is to improve the accuracy and efficiency of determining a geographic location using only the visual content of an image.", "method": "Our method aligns the visual representation of a query image with a learned geographic representation, using a hierarchical model of geographic embeddings. It efficiently fuses the image's appearance features with its semantic segmentation to form a robust representation.", "result": "The method achieves state-of-the-art performance on five benchmark datasets, surpassing previous methods in 22 out of 25 evaluated metrics.", "conclusion": "The combination of geographic and visual representations leads to significant improvements in visual geo-localization accuracy, validated by empirical performance and ablation studies."}}
{"id": "2510.01231", "categories": ["cs.CL", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01231", "abs": "https://arxiv.org/abs/2510.01231", "authors": ["Shuaidong Pan", "Di Wu"], "title": "Trustworthy Summarization via Uncertainty Quantification and Risk Awareness in Large Language Models", "comment": null, "summary": "This study addresses the reliability of automatic summarization in high-risk\nscenarios and proposes a large language model framework that integrates\nuncertainty quantification and risk-aware mechanisms. Starting from the demands\nof information overload and high-risk decision-making, a conditional\ngeneration-based summarization model is constructed, and Bayesian inference is\nintroduced during generation to model uncertainty in the parameter space, which\nhelps avoid overconfident predictions. The uncertainty level of the generated\ncontent is measured using predictive distribution entropy, and a joint\noptimization of entropy regularization and risk-aware loss is applied to ensure\nthat key information is preserved and risk attributes are explicitly expressed\nduring information compression. On this basis, the model incorporates risk\nscoring and regulation modules, allowing summaries to cover the core content\naccurately while enhancing trustworthiness through explicit risk-level prompts.\nComparative experiments and sensitivity analyses verify that the proposed\nmethod significantly improves the robustness and reliability of summarization\nin high-risk applications while maintaining fluency and semantic integrity.\nThis research provides a systematic solution for trustworthy summarization and\ndemonstrates both scalability and practical value at the methodological level.", "AI": {"tldr": "本研究通过构建集成不确定性量化和风险意识机制的大型语言模型框架，显著提升了高风险场景下摘要的可靠性。", "motivation": "研究的动机来自于信息过载和高风险决策任务的需求，旨在提高高风险场景下的自动摘要可靠性。", "method": "本文提出了一种结合不确定性量化和风险意识机制的大型语言模型框架，以提升高风险场景下自动摘要的可靠性。通过引入贝叶斯推断来建模参数空间中的不确定性，避免过度自信的预测，并通过预测分布熵衡量生成内容的不确定性水平。此外，模型还包括了风险评分和调节模块，使摘要能够准确涵盖核心内容，通过明确的风险级别提示提升信任度。", "result": "实验和敏感性分析验证了该方法在提高高风险应用中摘要的鲁棒性和可靠性的同时，还保持了流畅性和语义完整性。", "conclusion": "这项研究提供了一种解决可信摘要问题的系统方案，并展示了在方法论层面的可扩展性和实用性。"}}
{"id": "2510.01454", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01454", "abs": "https://arxiv.org/abs/2510.01454", "authors": ["Nilay Naharas", "Dang Nguyen", "Nesihan Bulut", "Mohammadhossein Bateni", "Vahab Mirrokni", "Baharan Mirzasoleiman"], "title": "Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories", "comment": "30 pages, 10 figures, 5 tables, link:\n  https://bigml-cs-ucla.github.io/XMAS-project-page/", "summary": "Data-efficient learning aims to eliminate redundancy in large training\ndatasets by training models on smaller subsets of the most informative\nexamples. While data selection has been extensively explored for vision models\nand large language models (LLMs), it remains underexplored for Large\nVision-Language Models (LVLMs). Notably, none of existing methods can\noutperform random selection at different subset sizes. In this work, we propose\nthe first principled method for data-efficient instruction tuning of LVLMs. We\nprove that examples with similar cross-modal attention matrices during\ninstruction tuning have similar gradients. Thus, they influence model\nparameters in a similar manner and convey the same information to the model\nduring training. Building on this insight, we propose XMAS, which clusters\nexamples based on the trajectories of the top singular values of their\nattention matrices obtained from fine-tuning a small proxy LVLM. By sampling a\nbalanced subset from these clusters, XMAS effectively removes redundancy in\nlarge-scale LVLM training data. Extensive experiments show that XMAS can\ndiscard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while\nfully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and\nspeeding up its training by 1.2x. This is 30% more data reduction compared to\nthe best baseline for LLaVA-665k. The project's website can be found at\nhttps://bigml-cs-ucla.github.io/XMAS-project-page/.", "AI": {"tldr": "提出XMAS方法，通过聚类去除大型视觉语言模型训练数据中的冗余信息，大幅减少所需数据量并保持模型性能，同时加快训练速度。", "motivation": "尽管数据选择已经在视觉模型和大型语言模型中得到了广泛研究，但对于大型视觉语言模型而言，这一领域仍处于探索初期。现有的方法都无法在不同的子集大小上超过随机选择的效果。", "method": "我们提出了XMAS，这是一种基于对注意力矩阵轨迹进行聚类的数据高效微调方法。通过训练一个小型代理LVLM获得注意力矩阵的顶级奇异值轨迹，并从这些聚类中选择一个平衡子集以有效去除大规模LVLM训练数据中的冗余信息。", "result": "实验结果显示，XMAS可以从LLaVA-665k数据集中消除50%的数据，以及从Vision-Flan数据集中消除85%的数据，同时完全保持LLaVA-1.5-7B在10个下游基准上的性能，并将其训练速度提高1.2倍。相对于LLaVA-665k的最佳基线，数据减少多了30%。", "conclusion": "XMAS方法提供了一种原则性的数据高效微调方法，适用于大型视觉语言模型，显著减少了训练数据量同时保持了模型性能，并加速了训练过程。"}}
{"id": "2510.01232", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01232", "abs": "https://arxiv.org/abs/2510.01232", "authors": ["Dongjun Kim", "Gyuho Shim", "Yongchan Chun", "Minhyuk Kim", "Chanjun Park", "Heuiseok Lim"], "title": "Benchmark Profiling: Mechanistic Diagnosis of LLM Benchmarks", "comment": "16 pages, 5 figures. Accepted to EMNLP 2025 main conference", "summary": "Large Language Models are commonly judged by their scores on standard\nbenchmarks, yet such scores often overstate real capability since they mask the\nmix of skills a task actually demands. For example, ARC is assumed to test\nreasoning, while HellaSwag is designed to evaluate commonsense. However, we\nlack a systematic way to verify if these benchmarks actually measure these\nlabels. We introduce Benchmark Profiling, a diagnostic framework that\ndecomposes benchmark performance into ten cognitively grounded abilities. The\nmethod combines gradient-based importance scoring with targeted parameter\nablation to compute an Ability Impact Score (AIS) that quantifies how much each\nability contributes to a model's success on a given benchmark. Profiling three\ninstruction-tuned models across ten widely used benchmarks yields four key\nfindings: (i) most benchmarks draw on several abilities rather than one, (ii)\ndatasets with similar labels rely on distinct ability mixtures, (iii)\ncode-generation benchmarks reward broad, multi-skill improvement and thus show\nonly modest gains from narrow domain-specific fine-tuning, and (iv) abilities\nirrelevant to the task could negatively affect performance. Benchmark Profiling\ntherefore explains why performance gains do not always translate into\nuser-perceived competence and offers a transparent tool for benchmark audit and\nmodel interpretability.", "AI": {"tldr": "This paper introduces Benchmark Profiling, a method that breaks down and quantifies the abilities contributing to a model's success on benchmarks, revealing insights into the actual abilities the benchmarks measure.", "motivation": "To verify if current benchmarks actually measure the labels they are assumed to test, as there is currently no systematic way to do so.", "method": "We introduce Benchmark Profiling, a diagnostic framework that decomposes benchmark performance into ten cognitively grounded abilities. The method combines gradient-based importance scoring with targeted parameter ablation to compute an Ability Impact Score (AIS).", "result": "Profiling three instruction-tuned models across ten widely used benchmarks yields four key findings: (i) most benchmarks draw on several abilities rather than one, (ii) datasets with similar labels rely on distinct ability mixtures, (iii) code-generation benchmarks reward broad, multi-skill improvement and show modest gains from narrow domain-specific fine-tuning, and (iv) abilities irrelevant to the task could negatively affect performance.", "conclusion": "Benchmark Profiling explains why performance gains do not always translate into user-perceived competence and offers a transparent tool for benchmark audit and model interpretability."}}
{"id": "2510.01478", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01478", "abs": "https://arxiv.org/abs/2510.01478", "authors": ["Răzvan-Andrei Matişan", "Vincent Tao Hu", "Grigory Bartosh", "Björn Ommer", "Cees G. M. Snoek", "Max Welling", "Jan-Willem van de Meent", "Mohammad Mahdi Derakhshani", "Floor Eijkelboom"], "title": "Purrception: Variational Flow Matching for Vector-Quantized Image Generation", "comment": null, "summary": "We introduce Purrception, a variational flow matching approach for\nvector-quantized image generation that provides explicit categorical\nsupervision while maintaining continuous transport dynamics. Our method adapts\nVariational Flow Matching to vector-quantized latents by learning categorical\nposteriors over codebook indices while computing velocity fields in the\ncontinuous embedding space. This combines the geometric awareness of continuous\nmethods with the discrete supervision of categorical approaches, enabling\nuncertainty quantification over plausible codes and temperature-controlled\ngeneration. We evaluate Purrception on ImageNet-1k 256x256 generation. Training\nconverges faster than both continuous flow matching and discrete flow matching\nbaselines while achieving competitive FID scores with state-of-the-art models.\nThis demonstrates that Variational Flow Matching can effectively bridge\ncontinuous transport and discrete supervision for improved training efficiency\nin image generation.", "AI": {"tldr": "Purrception 方法实现更快的训练收敛速度和具有竞争力的图像生成质量，通过结合连续和离散方法的优势。", "motivation": "目的是结合连续方法的几何感知和离散方法的类别监督，以实现对可编程代码的不确定性量化和温度控制生成，并提高图像生成训练效率。", "method": "Purrception采用变分流匹配方法进行矢量量化图像生成，通过在连续嵌入空间中计算速度场来学习代码簿索引的类别后验，从而在维护连续传输动力学的同时提供显式的类别监督。", "result": "在ImageNet-1k 256x256图像生成评估中，相较于连续流匹配和离散流匹配基线，训练收敛速度更快，同时与最先进的模型相比，也达到了有竞争力的FID分数。", "conclusion": "该研究表明变分流匹配能够有效在图像生成中桥接连续传输和离散监督，提升训练效率。"}}
{"id": "2510.01233", "categories": ["cs.CL", "68T50, 68T05, 68U35", "I.2.7; J.5; H.3.1"], "pdf": "https://arxiv.org/pdf/2510.01233", "abs": "https://arxiv.org/abs/2510.01233", "authors": ["Boddu Sri Pavan", "Boddu Swathi Sree"], "title": "Computational Social Linguistics for Telugu Cultural Preservation: Novel Algorithms for Chandassu Metrical Pattern Recognition", "comment": "16 pages, 4 figures", "summary": "This research presents a computational social science approach to preserving\nTelugu Chandassu, the metrical poetry tradition representing centuries of\ncollective cultural intelligence. We develop the first comprehensive digital\nframework for analyzing Telugu prosodic patterns, bridging traditional\ncommunity knowledge with modern computational methods. Our social computing\napproach involves collaborative dataset creation of 4,651 annotated padyams,\nexpert-validated linguistic patterns, and culturally-informed algorithmic\ndesign. The framework includes AksharamTokenizer for prosody-aware\ntokenization, LaghuvuGuruvu Generator for classifying light and heavy\nsyllables, and PadyaBhedam Checker for automated pattern recognition. Our\nalgorithm achieves 91.73% accuracy on the proposed Chandassu Score, with\nevaluation metrics reflecting traditional literary standards. This work\ndemonstrates how computational social science can preserve endangered cultural\nknowledge systems while enabling new forms of collective intelligence around\nliterary heritage. The methodology offers insights for community-centered\napproaches to cultural preservation, supporting broader initiatives in digital\nhumanities and socially-aware computing systems.", "AI": {"tldr": "研究学者利用计算社会方法，建立了第一个全面的数字框架来分析泰卢固语的韵律诗歌，这对于保护宝贵的文化遗产和推动集体智慧具有重要意义。", "motivation": "研究动机是保护濒危的文化知识系统 — 泰卢固语的Chandassu传统，这是一种代表了几个世纪集体文化智慧的诗歌传统。", "method": "此研究开发了一个完整的数字框架，用于分析泰卢固语的韵律模式，结合了传统社区知识与现代计算方法。该框架包括AksharamTokenizer用于韵律感知的分词，LaghuvuGuruvu Generator用于分类轻重音节，以及PadyaBhedam Checker用于自动模式识别。", "result": "该算法对提出的Chandassu Score的准确率达到91.73%，评估指标反映了传统的文学标准。", "conclusion": "该方法为以社区为中心的方法保护文化遗产提供了见解，并支持了数字人文和意识社会的计算系统中的更广泛倡议。"}}
{"id": "2510.01498", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01498", "abs": "https://arxiv.org/abs/2510.01498", "authors": ["Yuxuan Ou", "Ning Bi", "Jiazhen Pan", "Jiancheng Yang", "Boliang Yu", "Usama Zidan", "Regent Lee", "Vicente Grau"], "title": "AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging", "comment": null, "summary": "While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic\naneurysms (AAA), the required iodinated contrast agents pose significant risks,\nincluding nephrotoxicity, patient allergies, and environmental harm. To reduce\ncontrast agent use, recent deep learning methods have focused on generating\nsynthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a\nmulti-stage pipeline that first generates images and then performs\nsegmentation, which leads to error accumulation and fails to leverage shared\nsemantic and anatomical structures. To address this, we propose a unified deep\nlearning framework that generates synthetic CECT images from NCCT scans while\nsimultaneously segmenting the aortic lumen and thrombus. Our approach\nintegrates conditional diffusion models (CDM) with multi-task learning,\nenabling end-to-end joint optimization of image synthesis and anatomical\nsegmentation. Unlike previous multitask diffusion models, our approach requires\nno initial predictions (e.g., a coarse segmentation mask), shares both encoder\nand decoder parameters across tasks, and employs a semi-supervised training\nstrategy to learn from scans with missing segmentation labels, a common\nconstraint in real-world clinical data. We evaluated our method on a cohort of\n264 patients, where it consistently outperformed state-of-the-art single-task\nand multi-stage models. For image synthesis, our model achieved a PSNR of 25.61\ndB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation,\nit improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus\nDice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to\nmore accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm\nfrom 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to\nnnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.", "AI": {"tldr": "本文提出了一种新的多任务深度学习框架，能够从NCCT生成CECT图像并进行主动脉腔和血栓分割，优于现有技术。", "motivation": "由于常规的多阶段方法会导致错误累积，并且不利用共享的语义和解剖结构，本文旨在使用统一的深度学习框架来解决这些问题，并且减少对比剂的使用。", "method": "提出了一种将条件扩散模型（CDM）与多任务学习相结合的统一深度学习框架，旨在从非对比CT（NCCT）扫描中生成合成的对比增强CT（CECT）图像，并同时分割主动脉腔和血栓。该方法共享编码器和解码器的参数，并采用半监督训练策略，在标签缺失的样本上也能进行学习，共使用264名患者的队列进行评估。", "result": "在影像合成上，PSNR达到了25.61 dB，显著优于单任务CDM的23.80 dB。对于主动脉腔的Dice得分为0.89，血栓的Dice得分为0.53，相比于nnU-Net分别提高了0.02和0.05。此外，该方法将主动脉腔直径的误差从5.78 mm减少到4.19 mm，血栓区域的误差从41.45%减少到33.85%。", "conclusion": "实验表明，该方法在合成图像质量和解剖结构分割方面优于现有技术，特别是在血栓分割上实现了显著的改善，同时提高了临床测量的准确性。"}}
{"id": "2510.01234", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01234", "abs": "https://arxiv.org/abs/2510.01234", "authors": ["Shubham Agrawal", "Prasang Gupta"], "title": "LLMRank: Understanding LLM Strengths for Model Routing", "comment": "13 pages, 1 figure", "summary": "The rapid growth of large language models (LLMs) with diverse capabilities,\nlatency and computational costs presents a critical deployment challenge:\nselecting the most suitable model for each prompt to optimize the trade-off\nbetween performance and efficiency. We introduce LLMRank, a prompt-aware\nrouting framework that leverages rich, human-readable features extracted from\nprompts, including task type, reasoning patterns, complexity indicators,\nsyntactic cues, and signals from a lightweight proxy solver. Unlike prior\none-shot routers that rely solely on latent embeddings, LLMRank predicts\nper-model utility using a neural ranking model trained on RouterBench,\ncomprising 36,497 prompts spanning 11 benchmarks and 11 state-of-the-art LLMs,\nfrom small efficient models to large frontier systems. Our approach achieves up\nto 89.2% of oracle utility, while providing interpretable feature attributions\nthat explain routing decisions. Extensive studies demonstrate the importance of\nmultifaceted feature extraction and the hybrid ranking objective, highlighting\nthe potential of feature-driven routing for efficient and transparent LLM\ndeployment.", "AI": {"tldr": "提出了LLMRank，一种使用丰富的提示特征进行大语言模型路由的框架，优于依赖潜在嵌入的方法。", "motivation": "大语言模型（LLMs）的快速增长带来了部署挑战，需要选择最合适的模型来优化性能和效率之间的权衡。为了解决这个问题，我们提出了一种新的路由方法来实现这一目标。", "method": "我们提出了LLMRank，一种基于提示的路由框架，它利用从提示中提取的丰富的人类可读特征，包括任务类型、推理模式、复杂性指标、句法线索以及来自轻量级代理求解器的信号。与依赖于潜在嵌入的先前一次性路由器不同，LLMRank使用在RouterBench上训练的神经排名模型来预测每种模型的效用，RouterBench包含了跨越11个基准测试和11个最先进的大语言模型的36,497个提示。", "result": "我们的方法实现了高达89.2%的理论最大效用，同时也提供了解释路由决策的特征归属。广泛的实验证明了多方面特征提取和混合排名目标的重要性，突出了基于特征驱动的路由方法对于高效和透明部署大语言模型的潜力。", "conclusion": "实验结果表明，我们的特征驱动的路由方法在实现高效和透明部署大语言模型方面具有巨大潜力。"}}
{"id": "2510.01513", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.01513", "abs": "https://arxiv.org/abs/2510.01513", "authors": ["Basem Rizk", "Joel Walsh", "Mark Core", "Benjamin Nye"], "title": "From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding", "comment": null, "summary": "Analysis of multi-modal content can be tricky, computationally expensive, and\nrequire a significant amount of engineering efforts. Lots of work with\npre-trained models on static data is out there, yet fusing these opensource\nmodels and methods with complex data such as videos is relatively challenging.\nIn this paper, we present a framework that enables efficiently prototyping\npipelines for multi-modal content analysis. We craft a candidate recipe for a\npipeline, marrying a set of pre-trained models, to convert videos into a\ntemporal semi-structured data format. We translate this structure further to a\nframe-level indexed knowledge graph representation that is query-able and\nsupports continual learning, enabling the dynamic incorporation of new\ndomain-specific knowledge through an interactive medium.", "AI": {"tldr": "本文提出了一种框架，能够高效地设计用于多模态内容分析的处理流程，并将视频转化为时间上半结构化的数据形式，并进一步转化为可查询且能支持持续学习的帧级索引知识图表示。", "motivation": "多模态内容分析面临着计算复杂度高、工程量大的问题，特别是视频等复杂数据的处理。将现有的开源模型和方法融合起来更是一个挑战。", "method": "框架通过融合预训练模型来构建处理视频的流程，并将视频转换成时间上的半结构化数据格式，再转化为支持查询和持续学习的帧级知识图表示。", "result": "未提供具体实现结果。", "conclusion": "该框架支持将多模态内容（特别是视频）转化为易于处理的知识图表示，从而支持新的特定领域知识的动态加入。"}}
{"id": "2510.01236", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01236", "abs": "https://arxiv.org/abs/2510.01236", "authors": ["Ismam Nur Swapnil", "Aranya Saha", "Tanvir Ahmed Khan", "Mohammad Ariful Haque"], "title": "GRPO++: Enhancing Dermatological Reasoning under Low Resource Settings", "comment": "Will be submitted at IEEE JBHI", "summary": "Vision-Language Models (VLMs) show promise in medical image analysis, yet\ntheir capacity for structured reasoning in complex domains like dermatology is\noften limited by data scarcity and the high computational cost of advanced\ntraining techniques. To address these challenges, we introduce DermIQ-VLM, a\nVLM developed through a multi-stage, resource-efficient methodology designed to\nemulate a dermatologist's diagnostic process. Our primary contribution is a\nmodified version of Grouped Relative Policy Optimization (GRPO), called GRPO++,\nwhich stabilizes the powerful but data-intensive GRPO framework. Our proposed\ntraining pipeline first employs GRPO++ for reasoning-oriented disease\nrecognition, followed by supervised fine-tuning for conversational ability. To\nmitigate factual errors introduced during this step, we then align the model\nusing Direct Preference Optimization (DPO), leveraging a Knowledge Graph-based\nsystem as a scalable proxy for expert preference. A preliminary evaluation on a\ncurated dermatological dataset demonstrates that our proposed methodology\nyields notable performance gains over standard fine-tuning approaches. These\nfindings validate the potential of our pipeline as a feasible pathway for\ndeveloping specialized, reliable VLMs in resource-constrained environments.", "AI": {"tldr": "We developed DermIQ-VLM, a specialized Vision-Language Model for dermatology, using a resource-efficient training pipeline based on GRPO++, supervised fine-tuning, and expert-aligned DPO, which shows notable improvements in performance.", "motivation": "The motivation of this paper is to address the limitations of Vision-Language Models (VLMs) in dermatology, particularly their struggle with structured reasoning due to data scarcity and high computational costs. We aim to develop a specialized VLM that can mimic a dermatologist's diagnostic capabilities in environments with limited resources.", "method": "Our paper introduces DermIQ-VLM, a Vision-Language Model tailored for dermatology that employs a resource-efficient multi-stage training methodology. The core of our approach is the adaptation of the Grouped Relative Policy Optimization (GRPO) framework into GRPO++, which is designed to stabilize the GRPO process while reducing its data and computational requirements. Our pipeline initiates with disease recognition using GRPO++, followed by supervised fine-tuning to improve conversational abilities. To reduce factual errors introduced during fine-tuning, we employ Direct Preference Optimization (DPO) using a Knowledge Graph-based system to align the model with expert preferences.", "result": "Our preliminary evaluation on a dermatological dataset indicates that our methodology significantly enhances performance compared to conventional fine-tuning strategies. This suggests that our pipeline could be a practical solution for advancing VLMs in dermatology with a more efficient use of resources.", "conclusion": "In conclusion, our study demonstrates the efficacy of DermIQ-VLM and its training pipeline in overcoming the challenges faced by existing VLMs in dermatology. Our findings substantiate the potential of our resource-efficient strategy in creating reliable, specialized VLMs for dermatological applications, even within resource-constrained settings."}}
{"id": "2510.01524", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01524", "abs": "https://arxiv.org/abs/2510.01524", "authors": ["Viraj Prabhu", "Yutong Dai", "Matthew Fernandez", "Jing Gu", "Krithika Ramakrishnan", "Yanqi Luo", "Silvio Savarese", "Caiming Xiong", "Junnan Li", "Zeyuan Chen", "Ran Xu"], "title": "WALT: Web Agents that Learn Tools", "comment": null, "summary": "Web agents promise to automate complex browser tasks, but current methods\nremain brittle -- relying on step-by-step UI interactions and heavy LLM\nreasoning that break under dynamic layouts and long horizons. Humans, by\ncontrast, exploit website-provided functionality through high-level operations\nlike search, filter, and sort. We introduce WALT (Web Agents that Learn Tools),\na framework that reverse-engineers latent website functionality into reusable\ninvocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust\nimplementations of automations already designed into websites -- spanning\ndiscovery (search, filter, sort), communication (post, comment, upvote), and\ncontent management (create, edit, delete). Tools abstract away low-level\nexecution: instead of reasoning about how to click and type, agents simply call\nsearch(query) or create(listing). This shifts the computational burden from\nfragile step-by-step reasoning to reliable tool invocation. On VisualWebArena\nand WebArena, WALT achieves higher success with fewer steps and less\nLLM-dependent reasoning, establishing a robust and generalizable paradigm for\nbrowser automation.", "AI": {"tldr": "WALT框架通过逆向工程将网站内置功能转化为可调用工具，降低了代理的执行难度，提高了成功率，并减少对LLM推理的依赖。", "motivation": "当前的Web代理方法依赖于逐步的UI交互和大量的LLM推理，这种方法在面对动态布局和长时间任务时容易失效。相比之下，人类利用网站提供的高级操作来处理任务。因此，提出了WALT框架来解决这一问题。", "method": "WALT（Web Agents that Learn Tools）框架将网站内的功能逆向工程为可重用的工具，包括搜索、过滤、排序、发帖、评论、点赞、创建、编辑和删除等功能。这些工具抽象了低级别的执行过程，使代理只需调用相应的工具函数即可。", "result": "在VisualWebArena和WebArena上的测试表明，WALT能够用更少的步骤、更少的LLM推理实现更高的成功率。", "conclusion": "WALT建立了一个稳健且通用的浏览器自动化范式，解决了传统方法在动态网站和长周期任务中的不足。"}}
{"id": "2510.01237", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01237", "abs": "https://arxiv.org/abs/2510.01237", "authors": ["Nandakishor M"], "title": "Confidence-Aware Routing for Large Language Model Reliability Enhancement: A Multi-Signal Approach to Pre-Generation Hallucination Mitigation", "comment": null, "summary": "Large Language Models suffer from hallucination, generating plausible yet\nfactually incorrect content. Current mitigation strategies focus on\npost-generation correction, which is computationally expensive and fails to\nprevent unreliable content generation. We propose a confidence-aware routing\nsystem that proactively assesses model uncertainty before generation and\nredirects queries based on estimated reliability. Our approach combines three\ncomplementary signals: semantic alignment between internal representations and\nreference embeddings, internal convergence analysis across model layers, and\nlearned confidence estimation. The unified confidence score determines routing\nto four pathways: local generation for high confidence, retrieval-augmented\ngeneration for medium confidence, larger models for low confidence, and human\nreview for very low confidence. Evaluation on knowledge-intensive QA benchmarks\ndemonstrates significant improvements in hallucination detection (0.74 vs. 0.42\nbaseline) while reducing computational costs by 40% compared to post-hoc\nmethods. The F1 score improves from 0.61 to 0.82 with low false positive rates\n(0.09). This paradigm shift from reactive correction to proactive assessment\noffers a computationally efficient approach to LLM reliability enhancement.", "AI": {"tldr": "研究提出了一种新的基于置信度的路由系统，通过对模型生成内容之前的可靠性评估，减少了幻觉出现的几率，并显著提高了计算效率。这种方法在知识密集型问答任务上取得了显著提高。", "motivation": "本研究的动机在于解决大语言模型中存在的幻觉问题（即生成可信但事实性错误的内容）。现有的缓解策略主要集中在生成后的校正，这种策略计算成本高，并且无法阻止不可靠内容的生成。", "method": "本论文提出了一种基于置信度的路由系统，该系统可以主动评估大语言模型在生成前的不确定性，并根据估计的可靠性重新定向查询。此方法结合了三个互补信号：内部表示和参考嵌入之间的语义对齐、模型层间内部收敛性分析以及学习到的置信估计。", "result": "本研究在知识密集型问答基准测试中的评估显示，该方法在幻觉检测（0.74对基线的0.42）方面取得了显著的改进，同时与事后方法相比，计算成本减少了40%。F1评分从0.61提升到了0.82，且假阳性率较低（0.09）。", "conclusion": "从反应性修正转向主动评估的范式转变，为提高大语言模型的可靠性提供了一种计算效率较高的方法。"}}
{"id": "2510.01532", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01532", "abs": "https://arxiv.org/abs/2510.01532", "authors": ["Meilong Xu", "Xiaoling Hu", "Shahira Abousamra", "Chen Li", "Chao Chen"], "title": "MATCH: Multi-faceted Adaptive Topo-Consistency for Semi-Supervised Histopathology Segmentation", "comment": "20 pages, 6 figures. Accepted by NeurIPS 2025", "summary": "In semi-supervised segmentation, capturing meaningful semantic structures\nfrom unlabeled data is essential. This is particularly challenging in\nhistopathology image analysis, where objects are densely distributed. To\naddress this issue, we propose a semi-supervised segmentation framework\ndesigned to robustly identify and preserve relevant topological features. Our\nmethod leverages multiple perturbed predictions obtained through stochastic\ndropouts and temporal training snapshots, enforcing topological consistency\nacross these varied outputs. This consistency mechanism helps distinguish\nbiologically meaningful structures from transient and noisy artifacts. A key\nchallenge in this process is to accurately match the corresponding topological\nfeatures across the predictions in the absence of ground truth. To overcome\nthis, we introduce a novel matching strategy that integrates spatial overlap\nwith global structural alignment, minimizing discrepancies among predictions.\nExtensive experiments demonstrate that our approach effectively reduces\ntopological errors, resulting in more robust and accurate segmentations\nessential for reliable downstream analysis. Code is available at\n\\href{https://github.com/Melon-Xu/MATCH}{https://github.com/Melon-Xu/MATCH}.", "AI": {"tldr": "本文提出了一种半监督分割框架，通过利用多个随机dropout和时间训练快照获得的扰动预测，强制保持拓扑一致性，以辨别具有生物意义的结构。通过引入一种新的匹配策略，减少预测中的分歧，从而降低拓扑错误，提高分割的准确性和鲁棒性，对于可靠的下游分析至关重要。代码可在GitHub获取。", "motivation": "在半监督分割中，从无标签数据中捕获有意义的语义结构至关重要，但组织病理学图像分析中的对象密集分布加剧了这一挑战。", "method": "该方法通过随机dropout和时间训练快照获得多个扰动预测，并强制保持这些输出之间的拓扑一致性。同时，提出一种新的匹配策略，结合空间重叠和全局结构对齐，减少预测中的差异。", "result": "实验表明，该方法有效降低了拓扑错误，实现了更稳健和准确的分割结果。", "conclusion": "通过本文提出的方法，成功提高了半监督分割的效果，增强了对组织病理学图像中生物结构识别的准确度和鲁棒性。"}}
{"id": "2510.01238", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01238", "abs": "https://arxiv.org/abs/2510.01238", "authors": ["Rom Himelstein", "Amit LeVi", "Yonatan Belinkov", "Avi Mendelson"], "title": "Silent Tokens, Loud Effects: Padding in LLMs", "comment": "NeurIPS 2025 Workshop: LLM Evaluation", "summary": "Padding tokens are widely used in large language models (LLMs) to equalize\nsequence lengths during batched inference. While they should be fully masked,\nimplementation errors can cause them to influence computation, and the extent\nof this influence is not well understood. We systematically study this effect\nacross three open-source model families (Llama, Gemma, Qwen), inserting\ncontrolled amounts of padding and evaluating outcomes along four axes:\nactivations, generation quality, bias, and safety. Even small amounts of\npadding shift hidden representations, degrade quality in smaller models, alter\nbias in unpredictable ways, and weaken safety guardrails. These findings\ndemonstrate that padding is not a harmless detail but a robustness risk that\nmust be carefully handled in deployment.", "AI": {"tldr": "研究发现，即使是少量的填充标记也会影响大语言模型的计算结果和输出质量，必须在实际部署中加以谨慎处理。", "motivation": "虽然填充标记应完全被屏蔽，但实现错误可能导致它们影响计算，这种影响的程度尚不清楚。我们旨在探讨填充标记在这类模型中的实际影响。", "method": "我们通过在三个开源模型家族（Llama, Gemma, Qwen）中插入不同量的填充标记，并从四个维度（激活，生成质量，偏见和安全性）进行评估，以此系统地研究了填充标记对大语言模型的影响。", "result": "即使少量的填充标记也会改变隐藏的表示，降低小型模型的生成质量，以不可预测的方式改变偏见，并削弱安全保护措施。", "conclusion": "这些发现表明，填充标记不仅仅是一个无害的细节，而是在部署中必须谨慎处理的健壮性风险。"}}
{"id": "2510.01540", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01540", "abs": "https://arxiv.org/abs/2510.01540", "authors": ["Jiamu Bai", "Xin Yu", "Meilong Xu", "Weitao Lu", "Xin Pan", "Kiwan Maeng", "Daniel Kifer", "Jian Wang", "Yu Wang"], "title": "Towards Better Optimization For Listwise Preference in Diffusion Models", "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) has proven effectiveness\nfor aligning text-to-image (T2I) diffusion models with human preferences.\nAlthough Direct Preference Optimization (DPO) is widely adopted for its\ncomputational efficiency and avoidance of explicit reward modeling, its\napplications to diffusion models have primarily relied on pairwise preferences.\nThe precise optimization of listwise preferences remains largely unaddressed.\nIn practice, human feedback on image preferences often contains implicit ranked\ninformation, which conveys more precise human preferences than pairwise\ncomparisons. In this work, we propose Diffusion-LPO, a simple and effective\nframework for Listwise Preference Optimization in diffusion models with\nlistwise data. Given a caption, we aggregate user feedback into a ranked list\nof images and derive a listwise extension of the DPO objective under the\nPlackett-Luce model. Diffusion-LPO enforces consistency across the entire\nranking by encouraging each sample to be preferred over all of its lower-ranked\nalternatives. We empirically demonstrate the effectiveness of Diffusion-LPO\nacross various tasks, including text-to-image generation, image editing, and\npersonalized preference alignment. Diffusion-LPO consistently outperforms\npairwise DPO baselines on visual quality and preference alignment.", "AI": {"tldr": "Diffusion-LPO is a framework that extends DPO to process listwise human feedback, improving the alignment of T2I models with human preferences across various visual generation tasks.", "motivation": "To improve the alignment of text-to-image models with human preferences by using listwise preferences in reinforcement learning from human feedback (RLHF).", "method": "Diffusion-LPO, which extends Direct Preference Optimization (DPO) to handle listwise preferences in diffusion models.", "result": "Diffusion-LPO outperforms pairwise DPO on various tasks, such as text-to-image generation and image editing, showing better visual quality and preference alignment.", "conclusion": "Listwise Preference Optimization (Diffusion-LPO) is effective in aligning T2I diffusion models more precisely with human preferences compared to using pairwise preferences."}}
{"id": "2510.01239", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01239", "abs": "https://arxiv.org/abs/2510.01239", "authors": ["Juntae Lee", "Jihwan Bang", "Seunghan Yang", "Simyung Chang"], "title": "CIFLEX: Contextual Instruction Flow for Sub-task Execution in Multi-Turn Interactions with a Single On-Device LLM", "comment": "accepted at EMNLP 2025 (main)", "summary": "We present CIFLEX (Contextual Instruction Flow for Sub-task Execution), which\nis a novel execution system for efficient sub-task handling in multi-turn\ninteractions with a single on-device large language model (LLM). As LLMs become\nincreasingly capable, a single model is expected to handle diverse sub-tasks\nthat more effectively and comprehensively support answering user requests.\nNaive approach reprocesses the entire conversation context when switching\nbetween main and sub-tasks (e.g., query rewriting, summarization), incurring\nsignificant computational overhead. CIFLEX mitigates this overhead by reusing\nthe key-value (KV) cache from the main task and injecting only task-specific\ninstructions into isolated side paths. After sub-task execution, the model\nrolls back to the main path via cached context, thereby avoiding redundant\nprefill computation. To support sub-task selection, we also develop a\nhierarchical classification strategy tailored for small-scale models,\ndecomposing multi-choice decisions into binary ones. Experiments show that\nCIFLEX significantly reduces computational costs without degrading task\nperformance, enabling scalable and efficient multi-task dialogue on-device.", "AI": {"tldr": "CIFLEX系统通过复用主任务的缓存和层次分类策略来有效减少计算开销，实现设备上多任务对话的高效处理。", "motivation": "由于大型语言模型变得越来越能够有效且全面地支持用户请求，需要一种系统能够高效地处理多任务对话中的子任务，避免在任务转换时的计算开销。", "method": "CIFLEX采用上下文指令流来处理多轮交互中的子任务，通过复用主任务的关键值缓存，并在隔离的侧路径中仅注入任务特定指令，减少计算开销。同时开发了一种层次分类策略来支持小型模型的子任务选择。", "result": "实验显示，CIFLEX显著减少了计算成本，同时不降低任务性能，实现了设备上的高效多任务对话。", "conclusion": "CIFLEX为设备上的大型语言模型多轮对话提供了高效的子任务处理解决方案，通过减少计算开销促进了多任务对话的扩展性和效率。"}}
{"id": "2510.01546", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01546", "abs": "https://arxiv.org/abs/2510.01546", "authors": ["Hanyu Wang", "Jiaming Han", "Ziyan Yang", "Qi Zhao", "Shanchuan Lin", "Xiangyu Yue", "Abhinav Shrivastava", "Zhenheng Yang", "Hao Chen"], "title": "Growing Visual Generative Capacity for Pre-Trained MLLMs", "comment": "Project page: https://hywang66.github.io/bridge/", "summary": "Multimodal large language models (MLLMs) extend the success of language\nmodels to visual understanding, and recent efforts have sought to build unified\nMLLMs that support both understanding and generation. However, constructing\nsuch models remains challenging: hybrid approaches combine continuous\nembeddings with diffusion or flow-based objectives, producing high-quality\nimages but breaking the autoregressive paradigm, while pure autoregressive\napproaches unify text and image prediction over discrete visual tokens but\noften face trade-offs between semantic alignment and pixel-level fidelity. In\nthis work, we present Bridge, a pure autoregressive unified MLLM that augments\npre-trained visual understanding models with generative ability through a\nMixture-of-Transformers architecture, enabling both image understanding and\ngeneration within a single next-token prediction framework. To further improve\nvisual generation fidelity, we propose a semantic-to-pixel discrete\nrepresentation that integrates compact semantic tokens with fine-grained pixel\ntokens, achieving strong language alignment and precise description of visual\ndetails with only a 7.9% increase in sequence length. Extensive experiments\nacross diverse multimodal benchmarks demonstrate that Bridge achieves\ncompetitive or superior results in both understanding and generation\nbenchmarks, while requiring less training data and reduced training time\ncompared to prior unified MLLMs.", "AI": {"tldr": "The paper introduces Bridge, a pure autoregressive multimodal large language model (MLLM) that integrates text and image understanding and generation using a Mixture-of-Transformers architecture and a novel semantic-to-pixel discrete representation, achieving high performance with minimal increase in sequence length.", "motivation": "The motivation is to construct a unified MLLM that can handle both image understanding and generation within a single autoregressive framework, addressing the trade-offs that exist in current approaches between semantic alignment and pixel-level detail.", "method": "The method involves augmenting pre-trained visual understanding models with generative capabilities through a Mixture-of-Transformers architecture. A new semantic-to-pixel discrete representation is introduced to enhance visual generation fidelity without significantly increasing the sequence length.", "result": "The results demonstrate that Bridge achieves competitive or superior performance in understanding and generation benchmarks across various multimodal tasks. It accomplishes this with reduced training data and decreased training time compared to prior unified MLLMs.", "conclusion": "The conclusion is that Bridge effectively overcomes the limitations of existing approaches by enabling the creation of a unified MLLM that supports both understanding and generation with high performance, while also being more efficient in terms of training data and time requirements."}}
{"id": "2510.01241", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01241", "abs": "https://arxiv.org/abs/2510.01241", "authors": ["Hu Wei", "Ze Xu", "Boyu Yang", "Linlin Miao", "Weiqi Zhai", "Yihan Li", "Zixuan Li", "Zhijun Wang", "Boya Wang", "Jianwei Yu", "Jialing Yuan", "Xiaoyue Zhang", "Cheng He", "Minglei Chen", "Zifan Zhang", "Qianhui Li", "Wei Wang", "Xiang Xu"], "title": "SKYLENAGE Technical Report: Mathematical Reasoning and Contest-Innovation Benchmarks for Multi-Level Math Evaluation", "comment": null, "summary": "Large language models (LLMs) now perform strongly on many public math suites,\nyet frontier separation within mathematics increasingly suffers from ceiling\neffects. We present two complementary benchmarks: SKYLENAGE-ReasoningMATH, a\n100-item, structure-aware diagnostic set with per-item metadata on length,\nnumeric density, and symbolic complexity; and SKYLENAGE-MATH, a 150-item\ncontest-style suite spanning four stages from high school to doctoral under a\nseven-subject taxonomy. We evaluate fifteen contemporary LLM variants under a\nsingle setup and analyze subject x model and grade x model performance. On the\ncontest suite, the strongest model reaches 44% while the runner-up reaches 37%;\naccuracy declines from high school to doctoral, and top systems exhibit a\ndoctoral-to-high-school retention near 79%. On the reasoning set, the best\nmodel attains 81% overall, and hardest-slice results reveal clear robustness\ngaps between leaders and the mid-tier. In summary, we release\nSKYLENAGE-ReasoningMATH and report aggregate results for SKYLENAGE-MATH;\ntogether, SKYLENAGE provides a hard, reasoning-centered and broadly covering\nmath benchmark with calibrated difficulty and rich metadata, serving as a\nreference benchmark for future evaluations of mathematical reasoning.", "AI": {"tldr": "研究推出了 SKYLENAGE-ReasoningMATH 和 SKYLENAGE-MATH 两个数学推理基准测试，对比了15种大语言模型的表现，最强模型分别达到了81%和44%的准确率。", "motivation": "尽管大语言模型在许多公开的数学套件中表现良好，但数学领域的前沿问题由于天花板效应而逐渐受到限制。为了提供一个更艰难且覆盖面广的数学基准测试，以供未来评估数学推理能力的参考。", "method": "提出了两个互补的基准测试：SKYLENAGE-ReasoningMATH 和 SKYLENAGE-MATH，分别包含100道和150道题目，以及详细的题目元数据如长度、数字密度和符号复杂度。", "result": "在竞赛类套件评估中，最强的模型达到44%的准确率，次强模型达到37%；准确性从高中到博士级别逐渐下降，顶级系统在博士级别到高中级别的保持率为79%。在推理测试集中，最优模型整体准确率为81%，表现最佳的模型在最难的部分也显示出明显的稳健性优势。", "conclusion": "SKYLENAGE 提供了一个困难的、以推理为中心且涵盖广泛的数学基准测试，具有校准的难度和丰富的元数据，作为未来评估数学推理能力的参考基准。"}}
{"id": "2510.01547", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01547", "abs": "https://arxiv.org/abs/2510.01547", "authors": ["Akshay Bhagwan Sonawane", "Lena D. Swamikannan", "Lakshman Tamil"], "title": "Robust Classification of Oral Cancer with Limited Training Data", "comment": null, "summary": "Oral cancer ranks among the most prevalent cancers globally, with a\nparticularly high mortality rate in regions lacking adequate healthcare access.\nEarly diagnosis is crucial for reducing mortality; however, challenges persist\ndue to limited oral health programs, inadequate infrastructure, and a shortage\nof healthcare practitioners. Conventional deep learning models, while\npromising, often rely on point estimates, leading to overconfidence and reduced\nreliability. Critically, these models require large datasets to mitigate\noverfitting and ensure generalizability, an unrealistic demand in settings with\nlimited training data. To address these issues, we propose a hybrid model that\ncombines a convolutional neural network (CNN) with Bayesian deep learning for\noral cancer classification using small training sets. This approach employs\nvariational inference to enhance reliability through uncertainty\nquantification. The model was trained on photographic color images captured by\nsmartphones and evaluated on three distinct test datasets. The proposed method\nachieved 94% accuracy on a test dataset with a distribution similar to that of\nthe training data, comparable to traditional CNN performance. Notably, for\nreal-world photographic image data, despite limitations and variations\ndiffering from the training dataset, the proposed model demonstrated superior\ngeneralizability, achieving 88% accuracy on diverse datasets compared to 72.94%\nfor traditional CNNs, even with a smaller dataset. Confidence analysis revealed\nthat the model exhibits low uncertainty (high confidence) for correctly\nclassified samples and high uncertainty (low confidence) for misclassified\nsamples. These results underscore the effectiveness of Bayesian inference in\ndata-scarce environments in enhancing early oral cancer diagnosis by improving\nmodel reliability and generalizability.", "AI": {"tldr": "本文提出了一种结合CNN和贝叶斯深度学习的方法，提高了基于小样本集的口腔癌分类模型的可靠性和泛化性。", "motivation": "口腔癌是全球常见的癌症之一，尤其是在缺乏足够医疗资源的地区，死亡率特别高。早期诊断对降低死亡率至关重要，但受限于口腔健康项目的不足、基础设施缺乏和医疗人员短缺等问题。传统深度学习模型尽管有潜力，但其依赖点估计会导致过度自信和可靠性降低，并且需要大规模的训练数据来避免过拟合和确保通用性，这在数据受限的环境中难以实现。", "method": "提出了一种结合卷积神经网络（CNN）和贝叶斯深度学习的混合模型，用于基于小规模训练集的口腔癌分类。该方法通过变分推理引入不确定性量化，以提高可靠性。", "result": "该模型在与训练数据集相似分布的测试数据集中达到了94%的准确率，与传统CNN表现相当。但在面对从实际情况中获取的、与训练数据分布有差异的图像数据时，模型表现出了更好的泛化能力，实现了88%的准确率，而传统CNN则仅为72.94%。", "conclusion": "该模型通过贝叶斯推理在数据稀缺的环境中增强早期口腔癌诊断的可靠性和泛化性，对于提升早期诊断能力具有重要意义。"}}
{"id": "2510.01242", "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "math.IT", "68T05, 03C95, 94A17, 68Q85", "I.2.0; H.1.2; H.1.1; H.1.0; F.4.0"], "pdf": "https://arxiv.org/pdf/2510.01242", "abs": "https://arxiv.org/abs/2510.01242", "authors": ["Seyma Yaman Kayadibi"], "title": "Redundancy-as-Masking: Formalizing the Artificial Age Score (AAS) to Model Memory Aging in Generative AI", "comment": "34 pages, 17 figures. Includes theoretical development and\n  mathematical proofs of the Artificial Age Score (AAS), with empirical\n  illustrations via ChatGPT-based memory recall experiments (screenshots\n  included)", "summary": "Artificial intelligence is observed to age not through chronological time but\nthrough structural asymmetries in memory performance. In large language models,\nsemantic cues such as the name of the day often remain stable across sessions,\nwhile episodic details like the sequential progression of experiment numbers\ntend to collapse when conversational context is reset. To capture this\nphenomenon, the Artificial Age Score (AAS) is introduced as a log-scaled,\nentropy-informed metric of memory aging derived from observable recall\nbehavior. The score is formally proven to be well-defined, bounded, and\nmonotonic under mild and model-agnostic assumptions, making it applicable\nacross various tasks and domains. In its Redundancy-as-Masking formulation, the\nscore interprets redundancy as overlapping information that reduces the\npenalized mass. However, in the present study, redundancy is not explicitly\nestimated; all reported values assume a redundancy-neutral setting (R = 0),\nyielding conservative upper bounds. The AAS framework was tested over a 25-day\nbilingual study involving ChatGPT-5, structured into stateless and persistent\ninteraction phases. During persistent sessions, the model consistently recalled\nboth semantic and episodic details, driving the AAS toward its theoretical\nminimum, indicative of structural youth. In contrast, when sessions were reset,\nthe model preserved semantic consistency but failed to maintain episodic\ncontinuity, causing a sharp increase in the AAS and signaling structural memory\naging. These findings support the utility of AAS as a theoretically grounded,\ntask-independent diagnostic tool for evaluating memory degradation in\nartificial systems. The study builds on foundational concepts from von\nNeumann's work on automata, Shannon's theories of information and redundancy,\nand Turing's behavioral approach to intelligence.", "AI": {"tldr": "本研究提出了人工年龄分数（AAS），这是一个衡量人工智能系统记忆老化程度的新指标。通过观察大型语言模型在不同会话中的记忆性能，研究发现AI系统通过记忆不对称性老化，而不是通过时间。AAS在一项涉及ChatGPT-5的25天双语研究中得到验证，展示了它作为评测AI系统记忆退化情况的实用工具。", "motivation": "研究旨在开发一种衡量人工智能记忆老化的度量标准，并探讨大型语言模型在会话中由于记忆不对称性而表现出的老化现象。", "method": "引入人工年龄分数（AAS）作为记忆老化的一种度量。该分数是基于可观察回忆行为的对数比例熵度量。在重构的实验中，采用“冗余作为掩蔽”的形式来解释冗余如何减少惩罚质量。在没有估计冗余的假设下（R=0），研究了AAS在一段涉及ChatGPT-5的25天双语研究中的表现。", "result": "在有着持续交互环节的会话中，模型可以稳定回忆语义和情景细节，AAS数值下降，表明系统年轻；而在重置会话时，模型能够保持语义一致性，但无法保持情景连续性，AAS数值上升，反映了记忆老化。", "conclusion": "AAS作为理论上稳固且任务无关的诊断工具，能够有效地评估人工智能系统中的记忆退化。本研究还建立在冯·诺依曼、香农和图灵的相关理论的基础上。"}}
{"id": "2510.01559", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01559", "abs": "https://arxiv.org/abs/2510.01559", "authors": ["Renrong Shao", "Wei Zhang", "Kangyang Luo", "Qin Li", "and Jun Wang"], "title": "Consistent Assistant Domains Transformer for Source-free Domain Adaptation", "comment": null, "summary": "Source-free domain adaptation (SFDA) aims to address the challenge of\nadapting to a target domain without accessing the source domain directly.\nHowever, due to the inaccessibility of source domain data, deterministic\ninvariable features cannot be obtained. Current mainstream methods primarily\nfocus on evaluating invariant features in the target domain that closely\nresemble those in the source domain, subsequently aligning the target domain\nwith the source domain. However, these methods are susceptible to hard samples\nand influenced by domain bias. In this paper, we propose a Consistent Assistant\nDomains Transformer for SFDA, abbreviated as CADTrans, which solves the issue\nby constructing invariable feature representations of domain consistency.\nConcretely, we develop an assistant domain module for CADTrans to obtain\ndiversified representations from the intermediate aggregated global attentions,\nwhich addresses the limitation of existing methods in adequately representing\ndiversity. Based on assistant and target domains, invariable feature\nrepresentations are obtained by multiple consistent strategies, which can be\nused to distinguish easy and hard samples. Finally, to align the hard samples\nto the corresponding easy samples, we construct a conditional multi-kernel max\nmean discrepancy (CMK-MMD) strategy to distinguish between samples of the same\ncategory and those of different categories. Extensive experiments are conducted\non various benchmarks such as Office-31, Office-Home, VISDA-C, and\nDomainNet-126, proving the significant performance improvements achieved by our\nproposed approaches. Code is available at\nhttps://github.com/RoryShao/CADTrans.git.", "AI": {"tldr": "本文提出了一种新的SFDA方法CADTrans，通过构造多样化和一致性策略来解决传统方法在处理困难样本和领域偏差上的局限，实验结果表明CADTrans在多个基准数据集上表现优异。", "motivation": "传统SFDA方法中，由于无法直接获取源域数据，无法获得确定性的不变特征，且现有方法易受困难样本以及领域偏差的影响。CADTrans旨在改进现有方法的缺点，解决它们在表示多样性和区分样本难易程度方面的不足。", "method": "本文提出了用于无源领域适应（SFDA）的一致性辅助域变换器（CADTrans），通过构建领域一致性不变特征表示来解决现有方法中多样性表示不足的问题。CADTrans通过辅助域模块从中级聚拢的全局注意力中获得多样化表示，并利用多种一致性策略获得不变特征表示，以区分简单和困难样本。此外，为对齐困难样本至相应简单样本，构建了条件多核最大平均差异（CMK-MMD）策略来区分相同类别样本和不同类别样本。", "result": "在多个基准测试（如Office-31，Office-Home，VISDA-C和DomainNet-126）上进行的广泛实验证明，本文提出的CADTrans方法比现有方法取得了显著的性能提升。", "conclusion": "实验结果证明，CADTrans通过引入辅助域模块并采用多样性和一致性的策略，能够在源域不可用的情况下提升目标领域的性能，并成功解决了传统SFDA方法面临的困难样本对齐问题。"}}
{"id": "2510.01243", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01243", "abs": "https://arxiv.org/abs/2510.01243", "authors": ["Yisong Xiao", "Aishan Liu", "Siyuan Liang", "Zonghao Ying", "Xianglong Liu", "Dacheng Tao"], "title": "Detoxifying Large Language Models via Autoregressive Reward Guided Representation Editing", "comment": "Accepted to NeurIPS 25", "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, yet they remain vulnerable to generating toxic content,\nnecessitating detoxification strategies to ensure safe and responsible\ndeployment. Test-time detoxification methods, which typically introduce static\nor dynamic interventions into LLM representations, offer a promising solution\ndue to their flexibility and minimal invasiveness. However, current approaches\noften suffer from imprecise interventions, primarily due to their insufficient\nexploration of the transition space between toxic and non-toxic outputs. To\naddress this challenge, we propose \\textsc{A}utoregressive \\textsc{R}eward\n\\textsc{G}uided \\textsc{R}epresentation \\textsc{E}diting (ARGRE), a novel\ntest-time detoxification framework that explicitly models toxicity transitions\nwithin the latent representation space, enabling stable and precise\nreward-guided editing. ARGRE identifies non-toxic semantic directions and\ninterpolates between toxic and non-toxic representations to reveal fine-grained\ntransition trajectories. These trajectories transform sparse toxicity\nannotations into dense training signals, enabling the construction of an\nautoregressive reward model that delivers stable and precise editing guidance.\nAt inference, the reward model guides an adaptive two-step editing process to\nobtain detoxified representations: it first performs directional steering based\non expected reward gaps to shift representations toward non-toxic regions,\nfollowed by lightweight gradient-based refinements. Extensive experiments\nacross 8 widely used LLMs show that ARGRE significantly outperforms leading\nbaselines in effectiveness (-62.21% toxicity) and efficiency (-47.58% inference\ntime), while preserving the core capabilities of the original model with\nminimal degradation. Our code is available at the website.", "AI": {"tldr": "ARGRE框架通过建模潜在空间中的毒性转换实现了去毒过程的精细控制，在不牺牲模型性能的情况下，提高去毒的准确性和效率。该方法在多个模型上效果显著。", "motivation": "虽然大型语言模型在多种任务中表现出色，但它们仍然容易生成有毒内容，因此需要去毒策略以确保安全和责任的部署。现有的测试时去毒方法往往由于对毒素和非毒素输出之间的转换空间探索不足而干预不准确。", "method": "提出了一种名为ARGRE的测试时去毒框架，该框架明确建模了潜在表示空间中的毒性转换，实现了稳定的奖励引导编辑。ARGRE确定了非毒性语义方向，并在毒性与非毒性表示之间进行插值，揭示细粒度的转换轨迹。这些轨迹将稀疏的毒性注释转化为密集的训练信号，从而构建了一个自回归奖励模型，提供稳定的编辑指导。在推理时，奖励模型引导一个自适应的两步编辑过程，包括基于预期奖励差距的方向引导和轻量级梯度优化。", "result": "通过8个广泛应用的大型语言模型进行的广泛实验表明，ARGRE在有效性（减少62.21%的毒性）和效率（减少47.58%的推理时间）上显著优于领先基线，且对原始模型的核心能力几乎没有降解。", "conclusion": "这种新方法通过精细控制去毒过程，在不牺牲模型性能的情况下，提高去毒的准确性和效率。"}}
{"id": "2510.01576", "categories": ["cs.CV", "cs.AI", "cs.HC", "I.2.m; H.5.2"], "pdf": "https://arxiv.org/pdf/2510.01576", "abs": "https://arxiv.org/abs/2510.01576", "authors": ["Ricardo Gonzalez Penuela", "Felipe Arias-Russi", "Victor Capriles"], "title": "Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations", "comment": "7 pages, 2 figure, 2 tables, CV4A11y Workshop at ICCV 2025", "summary": "Multimodal large language models (MLLMs) have been integrated into visual\ninterpretation applications to support Blind and Low Vision (BLV) users because\nof their accuracy and ability to provide rich, human-like interpretations.\nHowever, these applications often default to comprehensive, lengthy\ndescriptions regardless of context. This leads to inefficient exchanges, as\nusers must go through irrelevant details rather than receiving the specific\ninformation they are likely to seek. To deliver more contextually-relevant\ninformation, we developed a system that draws on historical BLV users\nquestions. When given an image, our system identifies similar past visual\ncontexts from the VizWiz-LF dataset and uses the associated questions to guide\nthe MLLM generate descriptions more relevant to BLV users. An evaluation with\nthree human labelers who revised 92 context-aware and context-free descriptions\nshowed that context-aware descriptions anticipated and answered users'\nquestions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of\ncomparisons (50 out of 92). Our paper reviews, and data analysis are publicly\navailable in a Github repository at\nhttps://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .", "AI": {"tldr": "本研究开发了一个系统，该系统利用以往BLV用户的提问来指导MLLM生成更相关的图像描述，从而提高交流效率，评估显示该方法有效。", "motivation": "尽管多模态大型语言模型已经整合到视觉解释应用中以支持失明和低视力用户，但这些应用倾向于提供全面但冗长的描述，这往往导致交流效率低下。本研究旨在通过生成更上下文相关的描述来解决这一问题。", "method": "本研究开发了一个系统，该系统利用VizWiz-LF数据集中以往BLV用户的提问，为BLV用户生成更相关的图像描述。当系统接收到一张图像时，它能找到相似的视觉上下文，并使用相关的提问来指导MLLM生成更适合用户的描述。", "result": "通过三名人类标注者对92个具有上下文意识和无上下文意识的描述进行的评估表明，具有上下文意识的描述在76.1%的情况下能预测并解答用户的问题，并且在54.4%的比较中更受用户欢迎。", "conclusion": "本研究展示了使用BLV用户的视觉提问来指导MLLM生成更相关的图像描述的方法的有效性，研究成果和数据分析已在GitHub上公开。"}}
{"id": "2510.01244", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01244", "abs": "https://arxiv.org/abs/2510.01244", "authors": ["Hyeoneui Kim", "Jeongha Kim", "Huijing Xu", "Jinsun Jung", "Sunghoon Kang", "Sun Joo Jang"], "title": "Feasibility of Structuring Stress Documentation Using an Ontology-Guided Large Language Model", "comment": null, "summary": "Stress, arising from the dynamic interaction between external stressors,\nindividual appraisals, and physiological or psychological responses,\nsignificantly impacts health yet is often underreported and inconsistently\ndocumented, typically captured as unstructured free-text in electronic health\nrecords. Ambient AI technologies offer promise in reducing documentation\nburden, but predominantly generate unstructured narratives, limiting downstream\nclinical utility.\n  This study aimed to develop an ontology for mental stress and evaluate the\nfeasibility of using a Large Language Model (LLM) to extract ontology-guided\nstress-related information from narrative text. The Mental Stress Ontology\n(MeSO) was developed by integrating theoretical models like the Transactional\nModel of Stress with concepts from 11 validated stress assessment tools. MeSO's\nstructure and content were refined using Ontology Pitfall Scanner! and expert\nvalidation.\n  Using MeSO, six categories of stress-related information--stressor, stress\nresponse, coping strategy, duration, onset, and temporal profile--were\nextracted from 35 Reddit posts using Claude Sonnet 4. Human reviewers evaluated\naccuracy and ontology coverage. The final ontology included 181 concepts across\neight top-level classes. Of 220 extractable stress-related items, the LLM\ncorrectly identified 172 (78.2%), misclassified 27 (12.3%), and missed 21\n(9.5%). All correctly extracted items were accurately mapped to MeSO, although\n24 relevant concepts were not yet represented in the ontology.\n  This study demonstrates the feasibility of using an ontology-guided LLM for\nstructured extraction of stress-related information, offering potential to\nenhance the consistency and utility of stress documentation in ambient AI\nsystems. Future work should involve clinical dialogue data and comparison\nacross LLMs.", "AI": {"tldr": "通过开发精神压力本体（MeSO），本研究展示了使用大型语言模型（LLM）进行结构化提取压力相关信息的可行性，从非结构化文本中提取信息的成功率为78.2%，为提升压力文档的连续性和临床实用性提供了潜在方案。", "motivation": "压力，源于外部压力源、个人评价及生理或心理反应的动态交互，对健康有着显著影响，但在电子健康记录中经常被低估或记录不一致，通常为非结构化的自由文本格式。环境AI技术在这种情况下有减少文档负担的潜力，但主要产生非结构化叙述，限制了下游的临床应用能力。", "method": "本研究开发了一种精神压力本体（MeSO），通过整合压力理论模型和11种验证过的压力评估工具中的概念进行构建，并通过本体陷阱扫描器（Ontology Pitfall Scanner）和专家验证来提升其结构和内容质量。使用MeSO从35篇Reddit帖子中提取六类压力相关信息，包括压力源、压力反应、应对策略、持续时间、发作时间及时间概况，采用了Claude Sonnet 4大型语言模型，并通过人工审核评估其准确性和本体覆盖范围。", "result": "最终的本体包含了181个概念，分为8个顶级分类。从220个可提取的压力相关信息项中，大型语言模型正确识别了172项（78.2%），误分类了27项（12.3%），遗漏了21项（9.5%）。所有正确提取的项都能准确映射到MeSO，但有24个相关概念尚未包含在本体内。", "conclusion": "本研究展示了使用基于本体的大型语言模型进行结构化提取压力相关信息的可行性，对提升环境AI系统中压力文档记载的连续性和实用性有重要意义。未来的工作应包括临床对话数据的使用以及跨大型语言模型的比较。"}}
{"id": "2510.01582", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01582", "abs": "https://arxiv.org/abs/2510.01582", "authors": ["Krishna Teja Chitty-Venkata", "Murali Emani"], "title": "ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models", "comment": "Preprint", "summary": "We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the\ndevelopment of Vision Language Models (VLMs) with explicit reasoning\ncapabilities. Our dataset is built on 250,000 images from ImageNet21k dataset,\nproviding structured thinking tokens and corresponding answers. Our synthetic\ndataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and\nKimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of\nthinking-answer sequences, creating a resource for training and evaluating\nmultimodal reasoning models. We capture the step-by-step reasoning process of\nVLMs and the final descriptive answers. Our goal with this dataset is to enable\nthe development of more robust VLMs while contributing to the broader\nunderstanding of multimodal reasoning mechanisms. The dataset and evaluation\nbenchmarks will be publicly available to aid research in reasoning/thinking\nmultimodal VLMs.", "AI": {"tldr": "本研究开发了一个名为ImageNet-Think的多模态推理数据集，用以提升视觉语言模型的推理能力，并将公开数据集和评估基准，以促进相关领域的研究。", "motivation": "我们的目标是促进更强大VLMs的发展，同时也对多模态推理机制作出更深入的探讨。", "method": "我们开发了ImageNet-Think数据集，这是一个旨在帮助提高视觉语言模型（VLMs）推理能力的多模态推理数据集。该数据集基于ImageNet21k 数据集中的250,000张图像，提供结构化的推理标记和相应的答案。数据集由两个先进的VLMs（GLM-4.1V-9B-Thinking和Kimi-VL-A3B-Thinking-2506）生成，每张图像都有两对推理-答案序列，作为训练和评估多模态推理模型的资源。", "result": "通过该数据集，我们捕捉到了VLMs的分步推理过程及其最终的描述性答案，为发展更稳健的VLMs做出了可能的贡献。", "conclusion": "我们所创建的这个数据集和评估基准将会公开用于帮助研究多模态VLMs的思考/推理能力，推进视觉语言模型的发展和对多模态推理机制的理解。"}}
{"id": "2510.01245", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01245", "abs": "https://arxiv.org/abs/2510.01245", "authors": ["Runfei Chen", "Shuyang Jiang", "Wei Huang"], "title": "SeMob: Semantic Synthesis for Dynamic Urban Mobility Prediction", "comment": "EMNLP2025", "summary": "Human mobility prediction is vital for urban services, but often fails to\naccount for abrupt changes from external events. Existing spatiotemporal models\nstruggle to leverage textual descriptions detailing these events. We propose\nSeMob, an LLM-powered semantic synthesis pipeline for dynamic mobility\nprediction. Specifically, SeMob employs a multi-agent framework where LLM-based\nagents automatically extract and reason about spatiotemporally related text\nfrom complex online texts. Fine-grained relevant contexts are then incorporated\nwith spatiotemporal data through our proposed innovative progressive fusion\narchitecture. The rich pre-trained event prior contributes enriched insights\nabout event-driven prediction, and hence results in a more aligned forecasting\nmodel. Evaluated on a dataset constructed through our pipeline, SeMob achieves\nmaximal reductions of 13.92% in MAE and 11.12% in RMSE compared to the\nspatiotemporal model. Notably, the framework exhibits pronounced superiority\nespecially within spatiotemporal regions close to an event's location and time\nof occurrence.", "AI": {"tldr": "SeMob结合语言模型(LLM)和多智能体框架，通过渐进融合架构，提高了人类动态移动性预测的精度，特别是在事件驱动的情况下的预测。", "motivation": "现有时空模型在整合外部事件的文本描述方面存在困难，导致难以预测人类移动性中的突然变化。", "method": "SeMob采用多智能体框架，利用基于LLM的智能体自动提取和推理复杂在线文本中的时空相关信息。通过提出的一种创新的渐进融合架构，将细节相关的上下文与时空数据结合起来。", "result": "SeMob在构建的数据集上评估，与时空模型相比，最大分别降低了13.92%的MAE和11.12%的RMSE。特别是在事件发生地点附近的时空区域内，这种框架表现出了显著的优势。", "conclusion": "通过融合事件驱动的预测，SeMob提供了更精准的人类动态移动性预测，特别是对于由外部事件引起的突然变化的预测。"}}
{"id": "2510.01608", "categories": ["cs.CV", "eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.01608", "abs": "https://arxiv.org/abs/2510.01608", "authors": ["Roman Jacome", "Romario Gualdrón-Hurtado", "Leon Suarez", "Henry Arguello"], "title": "NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems", "comment": "25 pages, 12 tables, 10 figures. Accepted to NeurIPS 2025", "summary": "Imaging inverse problems aims to recover high-dimensional signals from\nundersampled, noisy measurements, a fundamentally ill-posed task with infinite\nsolutions in the null-space of the sensing operator. To resolve this ambiguity,\nprior information is typically incorporated through handcrafted regularizers or\nlearned models that constrain the solution space. However, these priors\ntypically ignore the task-specific structure of that null-space. In this work,\nwe propose \\textit{Non-Linear Projections of the Null-Space} (NPN), a novel\nclass of regularization that, instead of enforcing structural constraints in\nthe image domain, promotes solutions that lie in a low-dimensional projection\nof the sensing matrix's null-space with a neural network. Our approach has two\nkey advantages: (1) Interpretability: by focusing on the structure of the\nnull-space, we design sensing-matrix-specific priors that capture information\northogonal to the signal components that are fundamentally blind to the sensing\nprocess. (2) Flexibility: NPN is adaptable to various inverse problems,\ncompatible with existing reconstruction frameworks, and complementary to\nconventional image-domain priors. We provide theoretical guarantees on\nconvergence and reconstruction accuracy when used within plug-and-play methods.\nEmpirical results across diverse sensing matrices demonstrate that NPN priors\nconsistently enhance reconstruction fidelity in various imaging inverse\nproblems, such as compressive sensing, deblurring, super-resolution, computed\ntomography, and magnetic resonance imaging, with plug-and-play methods,\nunrolling networks, deep image prior, and diffusion models.", "AI": {"tldr": "本文提出了一种新的非线性零空间投影（NPN）正则化方法，该方法通过神经网络在传感矩阵零空间的低维投影中寻找解，从而改善图像逆问题的重建保真度。", "motivation": "传统的先验通常忽略了零空间的任务特定结构。本文的动机在于解决高维信号从欠采样、噪声测量中恢复的基本问题，并通过针对特定传感矩阵的先验信息来提高解的可解释性和适应性。", "method": "本文提出了一种新的正则化类别，称为非线性零空间投影（NPN），该方法使用神经网络在传感矩阵零空间的低维投影中寻找解，而不是在图像域中施加结构约束。", "result": "实验结果表明，在包括压缩感知、去模糊、超分辨率、计算机断层扫描和磁共振成像在内的各种图像逆问题中，与现有的重建框架和传统图像域先验相结合时，NPN先验可以提高重建精度。", "conclusion": "NPN展示了通过聚焦传感矩阵零空间的结构来提高重建精度的能力，并在多种图像逆问题中显示出优越性。"}}
