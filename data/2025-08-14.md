<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 23]
- [cs.CV](#cs.CV) [Total: 20]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning](https://arxiv.org/abs/2508.09303)
*Shu Zhao,Tan Yu,Anbang Xu,Japinder Singh,Aaditya Shukla,Rama Akkiraju*

Main category: cs.CL

> 本文提出了一种新的强化学习框架ParallelSearch，能够识别查询结构并执行并行搜索操作，显著提高了搜索过程中处理多实体比较任务的效率和性能，比现有最优方法提升了2.9%的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于强化学习的搜索代理，在处理固有的可以并行化和逻辑独立的比较任务时，由于严格的顺序处理方式，存在计算效率低下的问题。

**Method:** 我们提出了一种新的强化学习框架ParallelSearch，它使大型语言模型能够识别并行化的查询结构，并执行多个搜索操作。这种方法引入了专门的奖励函数，鼓励识别独立的查询组件，同时通过共同考虑正确性、查询分解质量和并行执行带来的好处，保持答案的准确性。

**Result:** 全面的实验结果表明，ParallelSearch在七个问答基准上相较于最先进的基线方法平均提高了2.9%的性能。在可以并行化的查询上，我们的方法达到了12.7%的性能提升，同时只需69.6%的语言模型调用次数。

**Conclusion:** ParallelSearch通过引入并行搜索能力，有效提升了大型语言模型在处理需要多实体比较的查询任务时的效率和性能，改变了原有的顺序处理瓶颈。

**Abstract:** Reasoning-augmented search agents such as Search-R1, trained via
reinforcement learning with verifiable rewards (RLVR), demonstrate remarkable
capabilities in multi-step information retrieval from external knowledge
sources. These agents address the limitations of their parametric memory by
dynamically gathering relevant facts to address complex reasoning tasks.
However, existing approaches suffer from a fundamental architectural
limitation: they process search queries strictly sequentially, even when
handling inherently parallelizable and logically independent comparisons. This
sequential bottleneck significantly constrains computational efficiency,
particularly for queries that require multiple entity comparisons. To address
this critical limitation, we propose ParallelSearch, a novel reinforcement
learning framework that empowers large language models (LLMs) to recognize
parallelizable query structures and execute multiple search operations
concurrently. Our approach introduces dedicated reward functions that
incentivize the identification of independent query components while preserving
answer accuracy through jointly considering correctness, query decomposition
quality, and parallel execution benefits. Comprehensive experiments demonstrate
that ParallelSearch outperforms state-of-the-art baselines by an average
performance gain of 2.9% across seven question-answering benchmarks. Notably,
on parallelizable questions, our method achieves a 12.7% performance
improvement while requiring only 69.6% of the LLM calls compared to sequential
approaches.

</details>


### [2] [Leveraging Large Language Models for Rare Disease Named Entity Recognition](https://arxiv.org/abs/2508.09323)
*Nan Miles Xi,Yu Deng,Lin Wang*

Main category: cs.CL

> 本研究通过GPT-4o在低数据环境下的NER测试，使用不同的提示策略得出的有效结果，展示了优化提示的大型语言模型在罕见疾病相关的命名实体识别中的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 在罕见疾病领域，命名实体识别（NER）面临着标注数据稀缺、实体类型语义模糊以及长尾分布等独特挑战。因此，探索有效的NER方法对于在该领域推进自动化信息提取至关重要。

**Method:** 本研究评估了GPT-4o在罕见疾病命名实体识别（NER）任务中的表现，特别是在数据缺乏的情况下。研究采用了多种提示策略，包括零样本提示、少样本上下文学习、检索增强生成（RAG）以及任务级别的微调。此外，还设计了一个结构化的提示框架，用于编码特定领域的知识和歧义解决规则，并提出了两种语义引导的少样本示例选择方法，以提升效果并减少标注工作量。

**Result:** 实验结果显示GPT-4o在罕见疾病语料RareDis上达到了与BioClinicalBERT相当甚至更好的性能，尤其是在任务级微调时达到了新的最优结果（SOTA）。成本效益分析表明，少样本提示在低代币预算下提供了高回报，而RAG仅提供了边际增加效益。

**Conclusion:** 研究结果表明，优化提示的大型语言模型（如GPT-4o）可作为传统监督模型的有效且可扩展的替代品，特别是用于那些标注数据稀缺的罕见疾病命名实体识别任务。

**Abstract:** Named Entity Recognition (NER) in the rare disease domain poses unique
challenges due to limited labeled data, semantic ambiguity between entity
types, and long-tail distributions. In this study, we evaluate the capabilities
of GPT-4o for rare disease NER under low-resource settings, using a range of
prompt-based strategies including zero-shot prompting, few-shot in-context
learning, retrieval-augmented generation (RAG), and task-level fine-tuning. We
design a structured prompting framework that encodes domain-specific knowledge
and disambiguation rules for four entity types. We further introduce two
semantically guided few-shot example selection methods to improve in-context
performance while reducing labeling effort. Experiments on the RareDis Corpus
show that GPT-4o achieves competitive or superior performance compared to
BioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art
(SOTA) results. Cost-performance analysis reveals that few-shot prompting
delivers high returns at low token budgets, while RAG offers marginal
additional benefit. An error taxonomy highlights common failure modes such as
boundary drift and type confusion, suggesting opportunities for post-processing
and hybrid refinement. Our results demonstrate that prompt-optimized LLMs can
serve as effective, scalable alternatives to traditional supervised models in
biomedical NER, particularly in rare disease applications where annotated data
is scarce.

</details>


### [3] [TEN: Table Explicitization, Neurosymbolically](https://arxiv.org/abs/2508.09324)
*Nikita Mehrotra,Aayush Kumar,Sumit Gulwani,Arjun Radhakrishna,Ashish Tiwari*

Main category: cs.CL

> TEN是一种用于从半结构化文本中提取表格数据的神经符号方法，它在准确性和减少幻觉方面优于纯神经方法。

<details>
  <summary>Details</summary>

**Motivation:** 提出TEN的动机是解决从半结构化文本中提取表格数据时，纯神经网络方法因为幻觉和不能强制执行硬性约束而表现不佳的问题。

**Method:** TEN采用神经符号方法，首先使用结构分解提示法生成初始表格，然后用符号检查器检查表格的正确性和检测幻觉或遗忘的情况。检查器的输出由一个批语语言模型处理，生成修复表格的指导，并以此引导原语言模型进行自我调试循环。

**Result:** 实验显示，TEN在多个数据集和指标上显著优于纯神经基线方法，精度匹配更高且幻觉率显著降低。用户研究中的平均评分也证明了TEN更准确，且在易验证和修正方面更受欢迎。

**Conclusion:** 该研究证明了TEN方法的优越性，不仅在多种数据集上性能显著，而且在用户研究中也得到了更优的评价。

**Abstract:** We present a neurosymbolic approach, TEN, for extracting tabular data from
semistructured input text. This task is particularly challenging for text input
that does not use special delimiters consistently to separate columns and rows.
Purely neural approaches perform poorly due to hallucinations and their
inability to enforce hard constraints. TEN uses Structural Decomposition
prompting - a specialized chain-of-thought prompting approach - on a large
language model (LLM) to generate an initial table, and thereafter uses a
symbolic checker to evaluate not only the well-formedness of that table, but
also detect cases of hallucinations or forgetting. The output of the symbolic
checker is processed by a critique-LLM to generate guidance for fixing the
table, which is presented to the original LLM in a self-debug loop. Our
extensive experiments demonstrate that TEN significantly outperforms purely
neural baselines across multiple datasets and metrics, achieving significantly
higher exact match accuracy and substantially reduced hallucination rates. A
21-participant user study further confirms that TEN's tables are rated
significantly more accurate (mean score: 5.0 vs 4.3; p = 0.021), and are
consistently preferred for ease of verification and correction, with
participants favoring our method in over 60% of the cases.

</details>


### [4] [Decoding Neural Emotion Patterns through Natural Language Processing Embeddings](https://arxiv.org/abs/2508.09337)
*Gideon Vos,Maryam Ebrahimpour,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: cs.CL

> 研究提出了一种计算框架，能够将文本中的情感内容映射到大脑区域，不需神经成像，成本低，可扩展，有助于大规模自然语言分析，区分临床群体，提供了评估AI情感表达的基于大脑的基准。

<details>
  <summary>Details</summary>

**Motivation:** 理解语言中的情感表达如何与脑功能相关是一个挑战。传统的神经成像成本高且局限于实验室，但丰富的数字文本提供了一个新的情感-脑成图途径。此前的研究大多分别研究基于神经成像的情感定位或计算文本分析，很少有整合。

**Method:** 提出了一种计算框架，使用OpenAI的text-embedding-ada-002生成高维语义表示，然后使用降维和聚类来识别情感群体，并将它们映射到18个与情感处理相关的脑区。

**Result:** 结果表明存在神经解剖学上可信的映射，具有高空间特异性。抑郁症患者与负面情绪相关的边缘化参与更大。离散的情感成功分化。大语言模型生成的文本与人类在基本情感分布上匹配，但在同理心和自我表述区域的细微激活上不匹配。

**Conclusion:** 这种方法成本低、可扩展，能够进行大规模的自然语言分析，可以区分临床人群，并提供了一个评估AI情感表达的基于大脑的基准。

**Abstract:** Understanding how emotional expression in language relates to brain function
is a challenge in computational neuroscience and affective computing.
Traditional neuroimaging is costly and lab-bound, but abundant digital text
offers new avenues for emotion-brain mapping. Prior work has largely examined
neuroimaging-based emotion localization or computational text analysis
separately, with little integration. We propose a computational framework that
maps textual emotional content to anatomically defined brain regions without
requiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate
high-dimensional semantic representations, apply dimensionality reduction and
clustering to identify emotional groups, and map them to 18 brain regions
linked to emotional processing. Three experiments were conducted: i) analyzing
conversational data from healthy vs. depressed subjects (DIAC-WOZ dataset) to
compare mapping patterns, ii) applying the method to the GoEmotions dataset and
iii) comparing human-written text with large language model (LLM) responses to
assess differences in inferred brain activation. Emotional intensity was scored
via lexical analysis. Results showed neuroanatomically plausible mappings with
high spatial specificity. Depressed subjects exhibited greater limbic
engagement tied to negative affect. Discrete emotions were successfully
differentiated. LLM-generated text matched humans in basic emotion distribution
but lacked nuanced activation in empathy and self-referential regions (medial
prefrontal and posterior cingulate cortex). This cost-effective, scalable
approach enables large-scale analysis of naturalistic language, distinguishes
between clinical populations, and offers a brain-based benchmark for evaluating
AI emotional expression.

</details>


### [5] [The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains](https://arxiv.org/abs/2508.09349)
*Cathy Speed,Ahmed A. Metwally*

Main category: cs.CL

> 本研究介绍了并评估了一种将生成式AI模型与小型资深专家小组和结构化引导相结合的人机混合德尔菲框架，该框架在三个阶段中表现良好，并展示了在生成高质量和具上下文敏感性的共识方面的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在应对现有德尔菲研究、共识会议和系统性指南综合等专家共识开发方法在信息过载、证据碎片化及公共来源信息缺乏专家过滤等问题时的局限性。

**Method:** 采用三个阶段测试人机混合德尔菲框架：第一阶段测试回顾性复制，第二阶段测试前瞻性比较，第三阶段在两个具体领域进行应用部署。

**Result:** AI模型在第一阶段复制了95%已发表的专家共识结论，在第二阶段与资深专家95%方向一致，但缺乏经验性和实用性细节。第三阶段中，六位资深专家小组实现了90%以上共识覆盖率，并且主题饱和度高于平均水平。

**Conclusion:** 人机混合德尔菲框架提供了一种灵活可扩展的方法来生成高质量且上下文敏感的共识，其成功应用于健康辅导和绩效科学领域，证明了其方法论稳健性和规模化应用的前景。

**Abstract:** Expert consensus plays a critical role in domains where evidence is complex,
conflicting, or insufficient for direct prescription. Traditional methods, such
as Delphi studies, consensus conferences, and systematic guideline synthesis,
offer structure but face limitations including high panel burden, interpretive
oversimplification, and suppression of conditional nuance. These challenges are
now exacerbated by information overload, fragmentation of the evidence base,
and increasing reliance on publicly available sources that lack expert
filtering. This study introduces and evaluates a Human-AI Hybrid Delphi
(HAH-Delphi) framework designed to augment expert consensus development by
integrating a generative AI model (Gemini 2.5 Pro), small panels of senior
human experts, and structured facilitation. The HAH-Delphi was tested in three
phases: retrospective replication, prospective comparison, and applied
deployment in two applied domains (endurance training and resistance and mixed
cardio/strength training). The AI replicated 95% of published expert consensus
conclusions in Phase I and showed 95% directional agreement with senior human
experts in Phase II, though it lacked experiential and pragmatic nuance. In
Phase III, compact panels of six senior experts achieved >90% consensus
coverage and reached thematic saturation before the final participant. The AI
provided consistent, literature-grounded scaffolding that supported divergence
resolution and accelerated saturation. The HAH-Delphi framework offers a
flexible, scalable approach for generating high-quality, context-sensitive
consensus. Its successful application across health, coaching, and performance
science confirms its methodological robustness and supports its use as a
foundation for generating conditional, personalised guidance and published
consensus frameworks at scale.

</details>


### [6] [Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling](https://arxiv.org/abs/2508.09350)
*Ju-Chieh Chou,Jiawei Zhou,Karen Livescu*

Main category: cs.CL

> 研究提出了一种新的无文本语音生成模型，能够同时进行语义和声学信息的生成，提升了生成语音的声学细节质量。

<details>
  <summary>Details</summary>

**Motivation:** 大多数无文本监督的语音生成模型无法访问声学上下文和控制声学细节。因此，该研究旨在通过生成语义标记和连续声学帧表示来联合建模语言和声学信息，提供更好的声学细节。

**Method:** 该论文提出了一种同时生成语义标记和连续声学帧表示的方法，使用流动匹配目标来预测条件语义标记下的连续向量。

**Result:** 研究发现，预测多个未来的语义标记有助于保留语言信息。该方法在语言可能性基准方面具有可比性能，但在提示生成方面提供了更好的声学细节。

**Conclusion:** 该方法通过流动匹配目标实现了模型对生成语义标记下连续向量的预测，同时保证了语言信息的完整性，并提供了优于现有模型的声学细节。

**Abstract:** Textless spoken language models (SLMs) are generative models of speech that
do not rely on text supervision. Most textless SLMs learn to predict the next
semantic token, a discrete representation of linguistic content, and rely on a
separate vocoder to add acoustic information to the generated speech. Such
models have no access to acoustic context and no built-in control over acoustic
details. In this work, we propose to jointly model linguistic and acoustic
information by generating semantic tokens and a continuous real-valued
representation of the acoustic frame. We use a flow-matching objective to
predict the continuous vector conditioned on the semantic tokens. We study the
design space of this approach and find that predicting multiple future semantic
tokens helps preserve linguistic information. Our approach achieves comparable
performance to existing models in terms of linguistic likelihood benchmarks,
while providing better acoustic detail in prompted generation.

</details>


### [7] [APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification](https://arxiv.org/abs/2508.09378)
*Artem Chernodub,Aman Saini,Yejin Huh,Vivek Kulkarni,Vipul Raheja*

Main category: cs.CL

> 研究开发了APIO方法，用于自动优化语法错误修正和文本简化的提示词，提升了LLM在这两个任务上的性能，且无需手动设置初始提示词。

<details>
  <summary>Details</summary>

**Motivation:** 在有明确优化指标的场景下，已开发出自动提示优化方法。本研究在此基础上，提出一种无需手动限定初始提示词的新方法，以提高语言模型在特定任务中的表现。

**Method:** APIO方法用于语法错误修正和文本简化任务，是一种简单有效的提示词生成和优化方法，不依赖手动指定的初始提示词。

**Result:** 研究提出了APIO方法，用于语法错误修正和文本简化任务中的提示词生成与优化，无需手动指定初始提示词。APIO在这些任务中达到了目前LLM提示方法的最先进性能。研究者公开了数据、代码、提示词和输出结果。

**Conclusion:** APIO方法在无需手动指定初始提示词的情况下，提高了语言模型在语法错误修正和文本简化任务上的性能，达到了当前LLM提示方法的最先进水平。

**Abstract:** Recent advancements in large language models (LLMs) have enabled a wide range
of natural language processing (NLP) tasks to be performed through simple
prompt-based interactions. Consequently, several approaches have been proposed
to engineer prompts that most effectively enable LLMs to perform a given task
(e.g., chain-of-thought prompting). In settings with a well-defined metric to
optimize model performance, automatic prompt optimization (APO) methods have
been developed to refine a seed prompt. Advancing this line of research, we
propose APIO, a simple but effective prompt induction and optimization approach
for the tasks of Grammatical Error Correction (GEC) and Text Simplification,
without relying on manually specified seed prompts. APIO achieves a new
state-of-the-art performance for purely LLM-based prompting methods on these
tasks. We make our data, code, prompts, and outputs publicly available.

</details>


### [8] [Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models](https://arxiv.org/abs/2508.09403)
*Ting Cai,Stephen Sheen,AnHai Doan*

Main category: cs.CL

> 本文提出了一种新的方法Columbo，用于更准确地扩展表格中的缩写列名，通过使用基于大规模语言模型的方法并改进数据集和评估标准，显著优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有工作使用的公共合成数据存在重大局限性，且正确扩展缩写的准确度衡量标准不准确。因此，本文旨在解决这些局限性，并改进现有的解决方法。

**Method:** 本文提出了一个新的解决方案：Columbo，这是一个基于大规模语言模型的方法，能够利用上下文、规则、链式思考以及词级别分析来扩展表格中的缩写列名。

**Result:** 实验表明，Columbo在五个数据集上的表现优于当前最先进的解决方案NameGuess，提升了4%-29%的性能。Columbo已经在环境科学领域的主要数据门户EDI中投入了生产使用。

**Conclusion:** Columbo展示了在扩展表格列名缩写方面的卓越性能，特别是在企业、科学研究等领域。这项工作不仅提供了新的数据集，还提出了更准确的评估标准。

**Abstract:** Expanding the abbreviated column names of tables, such as ``esal'' to
``employee salary'', is critical for numerous downstream data tasks. This
problem arises in enterprises, domain sciences, government agencies, and more.
In this paper we make three contributions that significantly advances the state
of the art. First, we show that synthetic public data used by prior work has
major limitations, and we introduce 4 new datasets in enterprise/science
domains, with real-world abbreviations. Second, we show that accuracy measures
used by prior work seriously undercount correct expansions, and we propose new
synonym-aware measures that capture accuracy much more accurately. Finally, we
develop Columbo, a powerful LLM-based solution that exploits context, rules,
chain-of-thought reasoning, and token-level analysis. Extensive experiments
show that Columbo significantly outperforms NameGuess, the current most
advanced solution, by 4-29\%, over 5 datasets. Columbo has been used in
production on EDI, a major data portal for environmental sciences.

</details>


### [9] [Leveraging Zipformer Model for Effective Language Identification in Code-Switched Child-Directed Speech](https://arxiv.org/abs/2508.09430)
*Lavanya Shankar,Leibny Paola Garcia Perera*

Main category: cs.CL

> 通过Zipformer解决双语环境下儿童导向场景中的语言切换和识别问题，获得显著语言识别性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 解决儿童导向场景中的语言切换和语言识别难题，特别是在双语环境中。

**Method:** 使用Zipformer处理包含不平衡的普通话和英语两种语言的语音，通过选择内部层次来提取嵌入，并对比不同的后端方法。

**Result:** Zipformer在不同的后端方法中都表现出了鲁棒性，对于不平衡的数据处理效果很好，实现了81.89%的平衡准确率，比语言识别基线提高了15.47%。

**Conclusion:** 研究发现，Transformer编码器架构模型在真实场景中具有潜力。

**Abstract:** Code-switching and language identification in child-directed scenarios
present significant challenges, particularly in bilingual environments. This
paper addresses this challenge by using Zipformer to handle the nuances of
speech, which contains two imbalanced languages, Mandarin and English, in an
utterance. This work demonstrates that the internal layers of the Zipformer
effectively encode the language characteristics, which can be leveraged in
language identification. We present the selection methodology of the inner
layers to extract the embeddings and make a comparison with different
back-ends. Our analysis shows that Zipformer is robust across these backends.
Our approach effectively handles imbalanced data, achieving a Balanced Accuracy
(BAC) of 81.89%, a 15.47% improvement over the language identification
baseline. These findings highlight the potential of the transformer encoder
architecture model in real scenarios.

</details>


### [10] [From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases in Chart-to-Text](https://arxiv.org/abs/2508.09450)
*Ridwan Mahbub,Mohammed Saidul Islam,Mir Tafseer Nayeem,Md Tahmid Rahman Laskar,Mizanur Rahman,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

> 该研究探讨了大型视觉语言模型（VLMs）在生成图表总结时可能放大地理经济偏见的问题，并进行了大规模评估，发现这些模型倾向于对高收入国家产生更积极的描述，而不仅仅是国家标签的改变。此外，研究发现了一些偏见缓解技术的有效性有限，强调了这个问题的复杂性和需要更强大的去偏见策略。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机是探究VLM在生成图表总结时是否有可能放大地理经济偏见，并且了解这些偏见可能带来的社会危害。

**Method:** 研究进行了大规模评估，使用了6000个图表-国家对，涵盖了六个广泛使用的专有和开源模型，分析了国家经济状况如何影响生成总结的情绪。

**Result:** 研究发现现有VLM倾向于对高收入国家产生更积极的描述，即使是唯一改变的是国家标签。研究还发现几个模型如GPT-4o-mini, Gemini-1.5-Flash, 和Phi-3.5表现出不同程度的偏见。此外，使用积极的干扰对减少偏见的策略仅部分有效，强调了该问题的复杂性及其需要更强大的去偏见策略。

**Conclusion:** 研究揭示了一些视觉语言模型在生成图表总结时存在的显著地理经济偏见，并且发现简单的去偏见技术效果有限。这强调了需要开发更强有力的技术来处理这些问题。

**Abstract:** Charts are very common for exploring data and communicating insights, but
extracting key takeaways from charts and articulating them in natural language
can be challenging. The chart-to-text task aims to automate this process by
generating textual summaries of charts. While with the rapid advancement of
large Vision-Language Models (VLMs), we have witnessed great progress in this
domain, little to no attention has been given to potential biases in their
outputs. This paper investigates how VLMs can amplify geo-economic biases when
generating chart summaries, potentially causing societal harm. Specifically, we
conduct a large-scale evaluation of geo-economic biases in VLM-generated chart
summaries across 6,000 chart-country pairs from six widely used proprietary and
open-source models to understand how a country's economic status influences the
sentiment of generated summaries. Our analysis reveals that existing VLMs tend
to produce more positive descriptions for high-income countries compared to
middle- or low-income countries, even when country attribution is the only
variable changed. We also find that models such as GPT-4o-mini,
Gemini-1.5-Flash, and Phi-3.5 exhibit varying degrees of bias. We further
explore inference-time prompt-based debiasing techniques using positive
distractors but find them only partially effective, underscoring the complexity
of the issue and the need for more robust debiasing strategies. Our code and
dataset are publicly available here.

</details>


### [11] [User-centric Subjective Leaderboard by Customizable Reward Modeling](https://arxiv.org/abs/2508.09463)
*Qi Jia,Xiujie Song,Zicheng Zhang,Yijin Guo,Kaiwei Zhang,Zijian Chen,Guangtao Zhai*

Main category: cs.CL

> 提出一套用户驱动的主观排行榜USL以及可定制奖励模型CRM，目的是评估大语言模型在各种真实场景的应用，并展示出CRM性能的优越性。

<details>
  <summary>Details</summary>

**Motivation:** 目前针对大语言模型的基准测试主要集中在可验证的任务上，这些静态基准对实际模型选择的实用性有限。为填补这一空白，我们开发了USL，旨在提供一个基于用户偏好的动态评估。

**Method:** 我们提出了首个基于用户偏好的主观排行榜（USL），并引入了可定制奖励模型（CRMs），以解决人类偏好中的多样性与矛盾性问题。通过大量人类偏好数据（超过10K主观查询）的研究，展示出即使在参数量较少的情况下（4B参数），我们的CRM也能在新话题和标准上超越领先模型如GPT-4.1和Gemini-2.5-pro，并且与矛盾偏好表现出强负相关性。

**Result:** CRM在少参数量下，展现出超越现状模型（如GPT-4.1和Gemini-2.5-pro）的性能，USL也显示了与矛盾偏好之间的强负相关性。

**Conclusion:** 研究结果表明，在少量参数的情况下，CRM也能表现出优越性，并揭示USL在评估跨多种现实场景的模型表现方面的重要性。

**Abstract:** Existing benchmarks for large language models (LLMs) predominantely focus on
assessing their capabilities through verifiable tasks. Such objective and
static benchmarks offer limited utility for practical LLM selection, making it
difficult for users to find suitable models for their individual needs. To
bridge this gap, we present the first User-Centric Subjective Leaderboard
(USL), which provides a preference-driven, dynamic ranking of LLMs across
diverse real-world scenarios. Our work is built upon a thorough investigation
of real human preference data, involving more than 10K subjective queries. Our
investigation reveals significant diversity and contradictions in human
preferences, which limit the effectiveness of state-of-the-art reward models.
To address this, we introduce Customizable Reward Models (CRMs). With only 4B
parameters, our CRM surpasses the performance of leading models such as GPT-4.1
and Gemini-2.5-pro, showing exceptional generalization capabilities across new
topics and criteria. The USL, powered by CRMs, exhibits strong negative
correlations to contradictory preferences.

</details>


### [12] [Learning Facts at Scale with Active Reading](https://arxiv.org/abs/2508.09494)
*Jessy Lin,Vincent-Pierre Berges,Xilun Chen,Wen-Tau Yih,Gargi Ghosh,Barlas Oğuz*

Main category: cs.CL

> 本文提出了一种新的框架Active Reading，通过自我生成的学习策略来提升大型语言模型在专家领域的知识吸收能力，并展示了其在不同的基准测试上的显著效果。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型能够储存大量知识，但在学习和回忆特定事实方面并不稳定。缺乏确保模型能够可靠、一致地学习给定知识体系的工具。为了填补这一空白，我们提出了Active Reading框架。

**Method:** 我们提出了一种名为Active Reading的框架，通过自我生成的学习策略训练模型来研究给定的一组材料。主要展示了在专家领域，使用Active Reading训练的模型比传统的微调和其他数据增强方法吸收更多知识。

**Result:** 实验结果表明，专家8B模型在SimpleQA（Wikipedia为基础的子集）上达到66%，在FinanceBench上达到26%，这些均比直接微调有大幅度提升。此外，我们展示了Active Reading还可以应用于大规模预训练，以构建更具备事实性的模型。

**Conclusion:** 通过本研究，证明了Active Reading能够有效提高模型在特定领域的知识吸收能力，即使在参数量相对较小的情况下，也能更精准地回答事实性问题。

**Abstract:** LLMs are known to store vast amounts of knowledge in their parametric memory.
However, learning and recalling facts from this memory is known to be
unreliable, depending largely on the prevalence of particular facts in the
training data and other factors which are poorly understood. Practitioners are
lacking tools which will allow them to ensure that the models learn a given
body of knowledge reliably and consistently. To this end, we propose Active
Reading: a framework where we train models to study a given set of material
with self-generated learning strategies. First, we demonstrate models trained
with Active Reading on expert domains absorb significantly more knowledge than
vanilla finetuning and other data augmentations. We train expert 8B models that
achieve 66% on a Wikipedia-grounded subset of SimpleQA (+313% relative over
vanilla finetuning) and 26% on FinanceBench (+160% relative over vanilla
finetuning) by applying Active Reading to the source documents for each
benchmark. Finally, we show that Active Reading can be utilized at pre-training
scale to build more factual models. As a demonstration of this, we release Meta
WikiExpert-8B, a Wikipedia-expert model trained on 1 trillion generated tokens,
which outcompetes models with hundreds of billions of parameters on factual QA.

</details>


### [13] [From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation](https://arxiv.org/abs/2508.09497)
*Siyuan Meng,Junming Liu,Yirong Chen,Song Mao,Pinlong Cai,Guohang Yan,Botian Shi,Ding Wang*

Main category: cs.CL

> 本文提出了一种新的重排序框架DPS，它能够动态选择段落集合，从而解决现行RAG系统重排序模块的问题，并在多个基准测试中显示出优越性。

<details>
  <summary>Details</summary>

**Motivation:** 传统的重排序模块独立评分段落并选择固定数量的Top-K段落，对于需要跨多个文档综合证据的复杂多跳查询，存在关键信息遗漏或引入噪音的问题。

**Method:** DPS是一种新型的重排序框架，它将段落选择视为一个监督学习问题，能够捕捉段落间的依赖关系，并动态选择最相关的段落集合，从而适应生成任务。

**Result:** 在五项基准测试中，DPS在多个指标上优于最先进重排序器和微调方法。特别是在具有挑战性的MuSiQue数据集上，相比强基线Qwen3重排序器和RankingGPT，DPS的F1分数分别提高了30.06%和15.4%。

**Conclusion:** DPS作为一个即插即用的模块，可以通过自适应的证据选择显著提升复杂RAG场景中的推理能力。

**Abstract:** Retrieval-augmented generation (RAG) systems are often bottlenecked by their
reranking modules, which typically score passages independently and select a
fixed Top-K size. This approach struggles with complex multi-hop queries that
require synthesizing evidence across multiple documents, creating a trade-off
where small K values omit crucial information and large K values introduce
noise. To address this, we introduce the Dynamic Passage Selector (DPS), a
novel reranking framework that treats passage selection as a supervised
learning problem. Unlike traditional point-wise or list-wise methods, DPS is
fine-tuned to capture inter-passage dependencies and dynamically select the
most relevant set of passages for generation. As a seamless plug-and-play
module, DPS requires no modifications to the standard RAG pipeline.
Comprehensive evaluations on five benchmarks show that DPS consistently
outperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the
challenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over
strong baselines like Qwen3-reranker and RankingGPT, respectively. Our results
demonstrate that by enabling adaptive evidence selection, DPS substantially
enhances reasoning capabilities in complex RAG scenarios.

</details>


### [14] [LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation](https://arxiv.org/abs/2508.09515)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

> 论文提出了一种跨语言方面情感分析的新方法，利用大型语言模型生成伪标签数据，无需翻译工具，展示了在多种语言和模型上的优越性。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法依赖于不可靠的翻译工具来解决跨语言方面情感分析中的语言差异问题。该研究旨在提出一种无需翻译工具的方法，即利用大型语言模型生成高质量的伪标签数据。

**Method:** 该论文提出了一种新方法，利用大型语言模型（LLM）生成目标语言的高质量伪标记数据，而不需要依赖翻译工具。首先，框架训练一个方面情感分析（ABSA）模型以获得未标记的目标语言数据的预测。接下来，通过使用LLM生成自然句子，更好地表示这些嘈杂的预测，而不仅仅是原始文本。然后，使用生成的伪标签数据进一步微调ABSA模型。

**Result:** 

**Conclusion:** 该研究展示了所提出框架在六种语言和五种骨干模型上的有效性，超过了之前基于翻译的方法。该框架也支持生成模型，并表明微调后的LLM优于较小的多语种模型。

**Abstract:** Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed
sentiment analysis in a target language by transferring knowledge from a source
language with available annotated data. Most existing methods depend heavily on
often unreliable translation tools to bridge the language gap. In this paper,
we propose a new approach that leverages a large language model (LLM) to
generate high-quality pseudo-labelled data in the target language without the
need for translation tools. First, the framework trains an ABSA model to obtain
predictions for unlabelled target language data. Next, LLM is prompted to
generate natural sentences that better represent these noisy predictions than
the original text. The ABSA model is then further fine-tuned on the resulting
pseudo-labelled dataset. We demonstrate the effectiveness of this method across
six languages and five backbone models, surpassing previous state-of-the-art
translation-based approaches. The proposed framework also supports generative
models, and we show that fine-tuned LLMs outperform smaller multilingual
models.

</details>


### [15] [Cross-lingual Aspect-Based Sentiment Analysis: A Survey on Tasks, Approaches, and Challenges](https://arxiv.org/abs/2508.09516)
*Jakub Šmíd,Pavel Král*

Main category: cs.CL

> 该论文提供了关于跨语言ABSA的全面综述，概述了任务、数据集、方法并指出了未来研究方向。

<details>
  <summary>Details</summary>

**Motivation:** 跨语言ABSA领域研究较少，尚未有系统性综述，该论文旨在填补这一空白。

**Method:** 提供全面的跨语言ABSA调查，总结关键任务如方面项抽取、方面情感分类等，回顾数据集、建模范式及跨语言迁移方法，分析单语及多语ABSA，以及LLMs对跨语言ABSA的贡献。

**Result:** 总结了关键任务并评估了现有解决方案，指出了主要挑战并为未来研究方向提供了建议。

**Conclusion:** 通过回顾现有工作、识别挑战，该论文为推进跨语言ABSA系统提供了路线图。

**Abstract:** Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that focuses on understanding opinions at the aspect level, including
sentiment towards specific aspect terms, categories, and opinions. While ABSA
research has seen significant progress, much of the focus has been on
monolingual settings. Cross-lingual ABSA, which aims to transfer knowledge from
resource-rich languages (such as English) to low-resource languages, remains an
under-explored area, with no systematic review of the field. This paper aims to
fill that gap by providing a comprehensive survey of cross-lingual ABSA. We
summarize key ABSA tasks, including aspect term extraction, aspect sentiment
classification, and compound tasks involving multiple sentiment elements.
Additionally, we review the datasets, modelling paradigms, and cross-lingual
transfer methods used to solve these tasks. We also examine how existing work
in monolingual and multilingual ABSA, as well as ABSA with LLMs, contributes to
the development of cross-lingual ABSA. Finally, we highlight the main
challenges and suggest directions for future research to advance cross-lingual
ABSA systems.

</details>


### [16] [UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2508.09517)
*Ladislav Lenc,Daniel Cífka,Jiří Martínek,Jakub Šmíd,Pavel Král*

Main category: cs.CL

> A zero-shot claim retrieval system using large language models to generate embeddings is presented, achieving high ranks in competition subtasks. The NV-Embed-v2 model performs best overall.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this study is to improve the efficiency and effectiveness of fact-checking mechanisms by retrieving relevant claims without the need for pre-existing annotated data.

**Method:** The paper uses state-of-the-art large language models to generate text embeddings for a zero-shot claim retrieval system. The models used include NVIDIA NV-Embed-v2, GPT, and Mistral, with the latter two combined in some cases for certain languages.

**Result:** The system achieved 7th place in monolingual and 9th place in cross-lingual subtasks of the competition. The NVIDIA NV-Embed-v2 model provided the best results overall.

**Conclusion:** The paper concludes that combining different models can enhance performance for certain languages and that the system, particularly the NV-Embed-v2 model, shows promise for zero-shot claim retrieval tasks.

**Abstract:** This paper presents a zero-shot system for fact-checked claim retrieval. We
employed several state-of-the-art large language models to obtain text
embeddings. The models were then combined to obtain the best possible result.
Our approach achieved 7th place in monolingual and 9th in cross-lingual
subtasks. We used only English translations as an input to the text embedding
models since multilingual models did not achieve satisfactory results. We
identified the most relevant claims for each post by leveraging the embeddings
and measuring cosine similarity. Overall, the best results were obtained by the
NVIDIA NV-Embed-v2 model. For some languages, we benefited from model
combinations (NV-Embed & GPT or Mistral).

</details>


### [17] [COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation](https://arxiv.org/abs/2508.09521)
*Yunxiao Wang,Meng Liu,Wenqi Liu,Kaiyu Jiang,Bin Wen,Fan Yang,Tingting Gao,Guorui Zhou,Liqiang Nie*

Main category: cs.CL

> 提出了可控的共情推理方法，结合自然语言推理和结构化的心理步骤，通过精细注释的数据集、强化学习及策略解决重复性问题，显著提高了情感支持系统的共情能力。

<details>
  <summary>Details</summary>

**Motivation:** 现有的模型在情感对话中缺乏深层次的共情推理，本研究旨在通过结合心理原则来加强这一点。

**Method:** 构建精细的数据集，使用强化学习，并引入基于个性的对话重写和冗余感知的奖励策略来改进训练。

**Result:** 模型的情感支持能力显著提高。

**Conclusion:** 该研究推进了共情、类似人类情感支持系统的发展。

**Abstract:** Emotional support conversations are crucial for promoting emotional
well-being, yet current models often lack deep empathetic reasoning grounded in
psychological principles. To address this, we propose controllable empathetic
reasoning, which combines natural language reasoning with structured
psychological steps. We construct a fine-grained dataset annotated with
reasoning correctness and response preferences to enable this capability. To
further enhance training, we employ reinforcement learning with a unified
process-outcome reward model that delivers precise feedback. To mitigate
response repetitiveness from entropy collapse, we introduce personality-based
dialogue rewriting and a redundancy-aware reward reweighting strategy. Our
approach significantly improves model's emotional support ability, advancing
the development of empathetic, human-like support systems.

</details>


### [18] [The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage](https://arxiv.org/abs/2508.09603)
*Skyler Hallinan,Jaehun Jung,Melanie Sclar,Ximing Lu,Abhilasha Ravichander,Sahana Ramnath,Yejin Choi,Sai Praneeth Karimireddy,Niloofar Mireshghallah,Xiang Ren*

Main category: cs.CL

> 研究人员提出了一种基于文本输出的黑盒成员推理攻击N-Gram Coverage Attack，展示了它在不同基准测试中的有效性，并发现这种方法的成功率随着计算资源的增加而提高。该方法还被用于研究闭源的OpenAI模型，并发现较新模型的抗攻击能力更强。

<details>
  <summary>Details</summary>

**Motivation:** 当前的成员推理攻击大多需要访问模型的隐藏状态或概率分布，因此研究者提出了仅依赖于模型文本输出的N-Gram Coverage Attack方法，以扩大对黑盒模型的攻击研究范围，并探索现有闭源语言模型的隐私保护状况。

**Method:** N-Gram Coverage Attack首先使用候选成员的部分内容作为条件生成多段文本序列，然后通过n-gram重叠度量计算这些输出与真实后缀的相似性，以此来推理候选者是否隶属于训练数据集。

**Result:** 实验结果显示N-Gram Coverage Attack不仅在多个现有基准测试中超越了其他黑盒攻击方法，而且在仅有文本输出时也能达到甚至超过某些白盒攻击的方法。

**Conclusion:** 研究表明，随着投入计算资源的增加，N-Gram Coverage Attack的性能可以得到提升，并且该方法发现了较新的模型如GPT-4o在成员推理攻击上具有更强的抵抗力，暗示了一种向增强隐私保护发展的趋势。

**Abstract:** Membership inference attacks serves as useful tool for fair use of language
models, such as detecting potential copyright infringement and auditing data
leakage. However, many current state-of-the-art attacks require access to
models' hidden states or probability distribution, which prevents investigation
into more widely-used, API-access only models like GPT-4. In this work, we
introduce N-Gram Coverage Attack, a membership inference attack that relies
solely on text outputs from the target model, enabling attacks on completely
black-box models. We leverage the observation that models are more likely to
memorize and subsequently generate text patterns that were commonly observed in
their training data. Specifically, to make a prediction on a candidate member,
N-Gram Coverage Attack first obtains multiple model generations conditioned on
a prefix of the candidate. It then uses n-gram overlap metrics to compute and
aggregate the similarities of these outputs with the ground truth suffix; high
similarities indicate likely membership. We first demonstrate on a diverse set
of existing benchmarks that N-Gram Coverage Attack outperforms other black-box
methods while also impressively achieving comparable or even better performance
to state-of-the-art white-box attacks - despite having access to only text
outputs. Interestingly, we find that the success rate of our method scales with
the attack compute budget - as we increase the number of sequences generated
from the target model conditioned on the prefix, attack performance tends to
improve. Having verified the accuracy of our method, we use it to investigate
previously unstudied closed OpenAI models on multiple domains. We find that
more recent models, such as GPT-4o, exhibit increased robustness to membership
inference, suggesting an evolving trend toward improved privacy protections.

</details>


### [19] [AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian](https://arxiv.org/abs/2508.09622)
*Tatiana Batura,Elena Bruches,Milana Shvenk,Valentin Malykh*

Main category: cs.CL

> 该论文介绍了AINL-Eval 2025共享任务，旨在检测俄语AI生成的科学摘要，使用一个包含人类和AI生成摘要的大型数据集，并组织了两次比赛阶段，鼓励研发能泛化到新领域和新模型的解决方案。

<details>
  <summary>Details</summary>

**Motivation:** AI生成文本的快速发展给学术诚信带来了挑战，特别是在资源有限的多语言科学出版环境中，因此需要一个专注于检测俄语AI生成科学摘要的共享任务。

**Method:** 构建了一个包含52,305个样本的大型数据集，包括12个科学领域的俄语人类和AI生成的科学摘要，并组织了一个分为两个阶段的比赛，共有10个团队和159个提交成果。

**Result:** 顶级系统的性能显示，在识别AI生成的文本方面表现良好，平台将持续开放，促进长期研究。

**Conclusion:** 建立了一个用于检测AI生成科学摘要的共享任务平台，这有助于提升检测技术，并促进这一关键领域的持续进步。

**Abstract:** The rapid advancement of large language models (LLMs) has revolutionized text
generation, making it increasingly difficult to distinguish between human- and
AI-generated content. This poses a significant challenge to academic integrity,
particularly in scientific publishing and multilingual contexts where detection
resources are often limited. To address this critical gap, we introduce the
AINL-Eval 2025 Shared Task, specifically focused on the detection of
AI-generated scientific abstracts in Russian. We present a novel, large-scale
dataset comprising 52,305 samples, including human-written abstracts across 12
diverse scientific domains and AI-generated counterparts from five
state-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and
GigaChat-Lite). A core objective of the task is to challenge participants to
develop robust solutions capable of generalizing to both (i) previously unseen
scientific domains and (ii) models not included in the training data. The task
was organized in two phases, attracting 10 teams and 159 submissions, with top
systems demonstrating strong performance in identifying AI-generated content.
We also establish a continuous shared task platform to foster ongoing research
and long-term progress in this important area. The dataset and platform are
publicly available at https://github.com/iis-research-team/AINL-Eval-2025.

</details>


### [20] [Improving Diversity in Language Models: When Temperature Fails, Change the Loss](https://arxiv.org/abs/2508.09654)
*Alexandre Verine,Florian Le Bronnec,Kunhao Zheng,Alexandre Allauzen,Yann Chevaleyre,Benjamin Negrevergne*

Main category: cs.CL

> 论文通过研究温度调整方法来增加语言模型的多样性，发现效果取决于训练时是否注重覆盖范围，并提出了结合精确度-召回率框架的新损失函数，实现比现有方法更好的性能。

<details>
  <summary>Details</summary>

**Motivation:** 增加语言模型中的多样性是一个挑战，但也是必不可少的目标。作者希望通过改变解码温度这一常见方法来研究这一问题，并尝试找到一种更好的方法来平衡精确度和召回率。

**Method:** 通过分析一个简单的案例，作者探讨了温度调整对语言模型多样性的影响，并提出了一种基于精确度-召回率框架重新设计损失函数的方法。

**Result:** 该论文研究了语言模型中通过调整解码温度来提高多样性的问题。研究发现，调低温度虽然能提高质量（精确度），但提高覆盖范围（召回率）的效果往往不理想。作者指出，一个可以通过温度调整有效微调的模型必须在训练时向覆盖范围训练。因此，作者提出了利用精确度-召回率框架重新思考语言模型的损失函数的方法。实验结果表明，这种方法相比简单地结合负对数似然训练和温度缩放，能够更好地平衡精确度和召回率。

**Conclusion:** 论文为更灵活和健壮的语言建模技术提供了方向，强调了在训练时重视覆盖范围的重要性，并提出了一种改进损失函数的新方法。

**Abstract:** Increasing diversity in language models is a challenging yet essential
objective. A common approach is to raise the decoding temperature. In this
work, we investigate this approach through a simplistic yet common case to
provide insights into why decreasing temperature can improve quality
(Precision), while increasing it often fails to boost coverage (Recall). Our
analysis reveals that for a model to be effectively tunable through temperature
adjustments, it must be trained toward coverage. To address this, we propose
rethinking loss functions in language models by leveraging the Precision-Recall
framework. Our results demonstrate that this approach achieves a substantially
better trade-off between Precision and Recall than merely combining negative
log-likelihood training with temperature scaling. These findings offer a
pathway toward more versatile and robust language modeling techniques.

</details>


### [21] [EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage Maximization](https://arxiv.org/abs/2508.09662)
*Yaoning Wang,Jiahao Ying,Yixin Cao,Yubo Ma,Yugang Jiang*

Main category: cs.CL

> 研究团队提出了EffiEval，一种无训练的评估方法，旨在解决大型语言模型评估中的计算挑战和数据冗余问题。EffiEval通过三个评价标准（代表性、公平性和泛化能力）实现高效的评估，并通过MUI自适应选择高质量子集。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型(LLMs)的快速发展以及评估基准的日益扩大和多样化，模型评估面临着重大的计算挑战。EffiEval旨在有效解决这些问题，实现数据冗余问题的解决，同时保持高评估可靠性。

**Method:** EffiEval采用无训练方法进行高效评估，解决数据冗余问题，同时保持高评估可靠性。该方法侧重于实现三个关键评价标准：代表性、公平性和泛化能力。EffiEval通过模型效用指数(MUI)自适应选择高质量的代表性子集，而传统的评估方式依赖绝对性能或需要大量的评估数据。

**Result:** 实验结果表明，EffiEval能够仅用原数据的一小部分就实现强烈的排名一致性，相比于全数据集的评估方法，且该方法也是灵活和可扩展的，可以根据特定的需求平衡评估效率和代表性。

**Conclusion:** EffiEval提供了一种实用且通用的解决方案，以实现可靠、公平和高效的LLM评估。它能够灵活调整大小以平衡评估效率和代表性，适合当前LLM的评估场景。

**Abstract:** The rapid advancement of large language models (LLMs) and the development of
increasingly large and diverse evaluation benchmarks have introduced
substantial computational challenges for model assessment. In this paper, we
present EffiEval, a training-free approach for efficient benchmarking that
effectively addresses data redundancy while maintaining high evaluation
reliability. Our method is specifically designed to meet three key criteria for
high-quality evaluation: representativeness, by ensuring comprehensive coverage
of model capabilities; fairness, by remaining independent of model performance
during sample selection to avoid bias; and generalizability, by enabling
flexible transfer across datasets and model families without reliance on
large-scale evaluation data. Unlike traditional methods that rely on absolute
performance or require extensive evaluation data, our approach adaptively
selects high-quality representative subsets based on the Model Utility Index
(MUI). Extensive experiments on multiple public benchmarks and diverse LLMs
demonstrate that EffiEval achieves strong ranking consistency with full-dataset
evaluation using only a small fraction of the original data. Furthermore, our
method is flexible and scalable in size, allowing users to balance evaluation
efficiency and representativeness according to specific needs. Overall,
EffiEval provides a practical and generalizable solution for reliable, fair,
and efficient evaluation in the era of LLMs.

</details>


### [22] [Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation](https://arxiv.org/abs/2508.09666)
*Ziyang Ma,Qingyue Yuan,Linhai Zhang,Deyu Zhou*

Main category: cs.CL

> 研究提出了一种名为SLowED的安全蒸馏方法，以保持小型语言模型（SLM）在蒸馏过程中的安全性，并提高其推理能力。

<details>
  <summary>Details</summary>

**Motivation:** 现有的链式思维（CoT）蒸馏方法主要集中在通过强大的大型语言模型（LLM）生成高质量的推理来增强小型语言模型（SLM）的推理能力，但很少关注由于训练带来的SLM安全负面影响。尽管存在针对安全对齐的工作，例如通过微调语言模型或操纵模型权重来防御有害输入，但这通常需要额外的计算或标注数据，可能会影响SLM的推理能力。因此，本研究旨在探讨如何在CoT蒸馏过程中保持SLM的安全性。

**Method:** 本研究提出了一种名为SLowED的安全蒸馏方法，该方法包含两个模块：Slow Tuning和Low-Entropy Masking。Slow Tuning通过缩小模型权重变化的幅度来优化初始权重分布附近的模型权重。Low-Entropy Masking通过屏蔽低熵的标记，这些标记被认为是不必要的学习目标，从而排除它们进入微调范围。

**Result:** 实验表明，SLowED在保持SLM安全性的前提下，可以与现有的蒸馏方法相当甚至超越它们来提高SLM的推理能力。

**Conclusion:** 研究表明，所提出的SLowED方法可以有效地保持小型语言模型在蒸馏过程中的安全，并在一定程度上提高其推理能力。Ablation研究表明，Slow Tuning在初始阶段保持了模型的安全性，而Low-Entropy Masking延长了安全训练的时间。

**Abstract:** Previous chain-of-thought (CoT) distillation methods primarily focused on
enhancing the reasoning capabilities of Small Language Models (SLMs) by
utilizing high-quality rationales generated by powerful Large Language Models
(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM
safety brought by the training, which are revealed in this study. Although
there are works on safety alignment that fine-tune language models or
manipulate model weights to defend against harmful inputs, they require extra
computation or annotated data, and probably impact the reasoning ability of
SLMs. In this paper, we investigate how to maintain the safety of SLMs during
the CoT distillation process. Specifically, we propose a safe distillation
method, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing
two modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the
magnitude of model weight changes to optimize the model weights in the
neighboring space near the initial weight distribution. Low-Entropy Masking
masks low-entropy tokens, which are regarded as unnecessary learning targets,
to exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,
Llama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,
AGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety
of SLMs and comparably improves their reasoning capability compared to existing
distillation methods. Furthermore, our ablation study presents the
effectiveness of Slow Tuning and Low-Entropy Masking, with the former
maintaining the model's safety in the early stage and the latter prolonging the
safe training epochs.

</details>


### [23] [Evaluating the Role of Large Language Models in Legal Practice in India](https://arxiv.org/abs/2508.09713)
*Rahul Hemrajani*

Main category: cs.CL

> 本文通过调查实验评估了大型语言模型（如GPT、Claude和Llama）在印度法律领域关键任务中的表现，发现LLM在某些任务中表现出色，但在专门的法律研究方面存在不足，人类专业知识在复杂推理和精确法律应用方面仍然是必不可少的。

<details>
  <summary>Details</summary>

**Motivation:** 本文探讨了大型语言模型（LLM）在印度法律领域关键任务中的熟练程度，提出了将人工智能（AI）融入法律职业所引起的重要问题。

**Method:** 通过一项调查实验，作者将大语言模型（如GPT、Claude和Llama）与初级律师的工作进行了比较，由高年级法学学生根据有用性、准确性和全面性对这些工作进行评分。

**Result:** 研究结果表明，LLM在起草和发现关键问题方面表现出色，经常与人类工作相匹配甚至超越。然而，在专门的法律研究方面，LLM表现不佳，经常产生幻觉、事实错误或虚构的输出。

**Conclusion:** 作者总结认为，虽然LLM能够增强某些法律任务的完成，但在复杂推理和精确法律应用方面，人类专业知识仍然必不可少。

**Abstract:** The integration of Artificial Intelligence(AI) into the legal profession
raises significant questions about the capacity of Large Language Models(LLM)
to perform key legal tasks. In this paper, I empirically evaluate how well
LLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian
context, including issue spotting, legal drafting, advice, research, and
reasoning. Through a survey experiment, I compare outputs from LLMs with those
of a junior lawyer, with advanced law students rating the work on helpfulness,
accuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,
often matching or surpassing human work. However, they struggle with
specialised legal research, frequently generating hallucinations, factually
incorrect or fabricated outputs. I conclude that while LLMs can augment certain
legal tasks, human expertise remains essential for nuanced reasoning and the
precise application of law.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [24] [A Context-aware Attention and Graph Neural Network-based Multimodal Framework for Misogyny Detection](https://arxiv.org/abs/2508.09175)
*Mohammad Zia Ur Rehman,Sufyaan Zahoor,Areeb Manzoor,Musharaf Maqbool,Nagendra Kumar*

Main category: cs.CV

> A novel multimodal framework for detecting misogynistic content on social media is proposed, showing significant performance improvements on two datasets compared to existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the detection of misogynistic content on social media, for which existing approaches to general offensive content detection fall short.

**Method:** The paper proposes a novel multimodal framework for detecting misogynistic and sexist content. This framework consists of three modules: Multimodal Attention module (MANM), Graph-based Feature Reconstruction Module (GFRM), and Content-specific Features Learning Module (CFLM). MANM uses adaptive gating to focus on relevant visual and textual information, GFRM refines features within individual modalities, and CFLM learns features specific to text and images.

**Result:** The proposed method shows an average improvement of 10.17% and 8.88% in macro-F1 on the MAMI and MMHS150K datasets, respectively.

**Conclusion:** The research concludes that their proposed multimodal framework is effective for detecting misogynistic and sexist content on social media, providing a significant improvement over existing methods.

**Abstract:** A substantial portion of offensive content on social media is directed
towards women. Since the approaches for general offensive content detection
face a challenge in detecting misogynistic content, it requires solutions
tailored to address offensive content against women. To this end, we propose a
novel multimodal framework for the detection of misogynistic and sexist
content. The framework comprises three modules: the Multimodal Attention module
(MANM), the Graph-based Feature Reconstruction Module (GFRM), and the
Content-specific Features Learning Module (CFLM). The MANM employs adaptive
gating-based multimodal context-aware attention, enabling the model to focus on
relevant visual and textual information and generating contextually relevant
features. The GFRM module utilizes graphs to refine features within individual
modalities, while the CFLM focuses on learning text and image-specific features
such as toxicity features and caption features. Additionally, we curate a set
of misogynous lexicons to compute the misogyny-specific lexicon score from the
text. We apply test-time augmentation in feature space to better generalize the
predictions on diverse inputs. The performance of the proposed approach has
been evaluated on two multimodal datasets, MAMI and MMHS150K, with 11,000 and
13,494 samples, respectively. The proposed method demonstrates an average
improvement of 10.17% and 8.88% in macro-F1 over existing methods on the MAMI
and MMHS150K datasets, respectively.

</details>


### [25] [IAD-R1: Reinforcing Consistent Reasoning in Industrial Anomaly Detection](https://arxiv.org/abs/2508.09178)
*Yanhui Li,Yunkang Cao,Chengliang Liu,Yuan Xiong,Xinghui Dong,Chao Huang*

Main category: cs.CV

> IAD-R1是一个适用于不同架构和参数规模的视觉语言模型的后训练框架，通过两阶段训练策略显著提高了模型的异常检测能力。

<details>
  <summary>Details</summary>

**Motivation:** 由于工业异常检测中缺乏缺陷样本，限制了传统检测方法的应用范围。视觉-语言模型虽然具有强大的泛化能力，但在工业异常检测中的性能仍显不足，IAD-R1旨在解决这一问题。

**Method:** IAD-R1采用两阶段训练策略：感知激活监督微调（PA-SFT）阶段使用精心构建的高质量专家异常数据集Expert-AD提升异常感知能力和推理能力；结构化对照组相对策略优化（SC-GRPO）阶段通过精心设计的奖励函数将模型从“异常感知”提升到“异常解释”。

**Result:** IAD-R1在6个工业异常检测基准数据集上平均准确率提升了43.3%，并且0.5B参数模型在无样本学习场景下超越了一些商业模型，如GPT-4.1和Claude-Sonnet-4。

**Conclusion:** 实验结果显示，IAD-R1显著提高了7个视觉语言模型在6个工业异常检测标准数据集上的性能，表现出模型的有效性和优越性。

**Abstract:** Industrial anomaly detection is a critical component of modern manufacturing,
yet the scarcity of defective samples restricts traditional detection methods
to scenario-specific applications. Although Vision-Language Models (VLMs)
demonstrate significant advantages in generalization capabilities, their
performance in industrial anomaly detection remains limited. To address this
challenge, we propose IAD-R1, a universal post-training framework applicable to
VLMs of different architectures and parameter scales, which substantially
enhances their anomaly detection capabilities. IAD-R1 employs a two-stage
training strategy: the Perception Activation Supervised Fine-Tuning (PA-SFT)
stage utilizes a meticulously constructed high-quality Chain-of-Thought dataset
(Expert-AD) for training, enhancing anomaly perception capabilities and
establishing reasoning-to-answer correlations; the Structured Control Group
Relative Policy Optimization (SC-GRPO) stage employs carefully designed reward
functions to achieve a capability leap from "Anomaly Perception" to "Anomaly
Interpretation". Experimental results demonstrate that IAD-R1 achieves
significant improvements across 7 VLMs, attaining up to 43.3% enhancement in
average accuracy on 6 industrial anomaly detection benchmark datasets. Notably,
the 0.5B parameter model trained with IAD-R1 surpasses commercial models
including GPT-4.1 and Claude-Sonnet-4 in zero-shot settings, demonstrating the
effectiveness and superiority of IAD-R1. The dataset, code, and all model
weights will be publicly available at https://github.com/Yanhui-Lee/IAD-R1.

</details>


### [26] [A Neurosymbolic Framework for Interpretable Cognitive Attack Detection in Augmented Reality](https://arxiv.org/abs/2508.09185)
*Rongqian Chen,Allison Andreyev,Yanming Xiu,Mahdi Imani,Bin Li,Maria Gorlatova,Gang Tan,Tian Lan*

Main category: cs.CV

> 本文提出了CADAR，一种新的神经符号方法，用于增强现实中的认知攻击检测，相比现有方法有着更好的精度和解释性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的检测方法要么仅关注视觉变化，这受限于像素或图像级别的处理，缺乏语义推理能力；要么依赖预训练视觉-语言模型，作为黑箱方法，解释性较低。CADAR旨在克服这些限制，提供适应性强、可解释且具备推理严格性的解决方案。

**Method:** CADAR, 一种新颖的神经符号方法，用于增强现实中的认知攻击检测。该方法融合了多模态视觉-语言输入，使用神经视觉-语言模型获得符号感知图表示，并结合了先验知识、显著性加权和时序相关性，然后通过粒子滤波器实现统计推理来检测认知攻击。

**Result:** 实验结果表明，在扩展的AR认知攻击数据集上，与强基线相比，CADAR在挑战性的AR攻击场景中显示出最高10.7%的精度提升。

**Conclusion:** 该研究证明了神经符号方法在有效且可解释的认知攻击检测中的潜力。CADAR方法结合了神经视觉-语言模型的适应性和粒子滤波器的解释性和推理严谨性。

**Abstract:** Augmented Reality (AR) enriches perception by overlaying virtual elements on
the physical world. Due to its growing popularity, cognitive attacks that alter
AR content to manipulate users' semantic perception have received increasing
attention. Existing detection methods often focus on visual changes, which are
restricted to pixel- or image-level processing and lack semantic reasoning
capabilities, or they rely on pre-trained vision-language models (VLMs), which
function as black-box approaches with limited interpretability. In this paper,
we present CADAR, a novel neurosymbolic approach for cognitive attack detection
in AR. It fuses multimodal vision-language inputs using neural VLMs to obtain a
symbolic perception-graph representation, incorporating prior knowledge,
salience weighting, and temporal correlations. The model then enables
particle-filter based statistical reasoning -- a sequential Monte Carlo method
-- to detect cognitive attacks. Thus, CADAR inherits the adaptability of
pre-trained VLM and the interpretability and reasoning rigor of particle
filtering. Experiments on an extended AR cognitive attack dataset show accuracy
improvements of up to 10.7% over strong baselines on challenging AR attack
scenarios, underscoring the promise of neurosymbolic methods for effective and
interpretable cognitive attack detection.

</details>


### [27] [RL-MoE: An Image-Based Privacy Preserving Approach In Intelligent Transportation System](https://arxiv.org/abs/2508.09186)
*Abdolazim Rezaei,Mehdi Sookhak,Mahboobeh Haghparast*

Main category: cs.CV

> 研究提出RL-MoE框架，将视觉数据转换为文本描述，以保护隐私并提高数据实用性，适用于智能城市和自动驾驶领域。

<details>
  <summary>Details</summary>

**Motivation:** 现有的隐私保护机制难以同时保证数据的隐私性和实用性，该研究旨在解决智能交通系统中视觉数据收集与隐私保护之间的冲突。

**Method:** RL-MoE框架结合了专家混合(MoE)架构和强化学习(RL)代理，将敏感视觉数据转换为保护隐私的文本描述。

**Result:** 实验表明，RL-MoE能够有效保护隐私，将CFP-FP数据集上的重播攻击成功率降至9.4%，同时生成比基线方法更丰富的文本内容。

**Conclusion:** RL-MoE为隐私敏感领域提供了构建可信AI系统的实用和可扩展解决方案，有望应用于更安全的智慧城市和自动驾驶网络。

**Abstract:** The proliferation of AI-powered cameras in Intelligent Transportation Systems
(ITS) creates a severe conflict between the need for rich visual data and the
fundamental right to privacy. Existing privacy-preserving mechanisms, such as
blurring or encryption, are often insufficient, creating an undesirable
trade-off where either privacy is compromised against advanced reconstruction
attacks or data utility is critically degraded. To resolve this impasse, we
propose RL-MoE, a novel framework that transforms sensitive visual data into
privacy-preserving textual descriptions, eliminating the need for direct image
transmission. RL-MoE uniquely combines a Mixture-of-Experts (MoE) architecture
for nuanced, multi-aspect scene decomposition with a Reinforcement Learning
(RL) agent that optimizes the generated text for a dual objective of semantic
accuracy and privacy preservation. Extensive experiments demonstrate that
RL-MoE provides superior privacy protection, reducing the success rate of
replay attacks to just 9.4\% on the CFP-FP dataset, while simultaneously
generating richer textual content than baseline methods. Our work provides a
practical and scalable solution for building trustworthy AI systems in
privacy-sensitive domains, paving the way for more secure smart city and
autonomous vehicle networks.

</details>


### [28] [Synthetic Data Generation for Emotional Depth Faces: Optimizing Conditional DCGANs via Genetic Algorithms in the Latent Space and Stabilizing Training with Knowledge Distillation](https://arxiv.org/abs/2508.09188)
*Seyed Muhammad Hossein Mousavi,S. Younes Mirinezhad*

Main category: cs.CV

> 研究提出了一种使用知识蒸馏优化的GAN框架，结合遗传算法，生成用于情感识别的高质量深度人脸数据，并通过特征提取和XGBoost分类实现高准确率。评估显示，在多个评价指标上优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 情感计算面临着重大挑战，即缺乏高质量、多样化的深度面部数据集以识别微妙的情感表达。为了应对这一挑战，提出了一种新的生成对抗网络框架，并结合遗传算法和知识蒸馏来生成多样化的深度人脸图像。

**Method:** 本研究提出了一种使用知识蒸馏优化的生成对抗网络（GAN）框架来进行合成深度人脸生成，以解决情感计算中缺乏高质量、多样化的深度面部数据集的问题。该框架利用遗传算法进化GAN的潜在向量，以提高目标情感的多样性和视觉质量。此外，还通过提取并连接LBPH（Local Binary Patterns Histograms）、HOG（Histogram of Oriented Gradients）、Sobel边缘和强度直方图特征，并使用XGBoost进行分类，来提高准确性。

**Result:** 实验结果显示，该方法在多样性和质量方面优于GAN、VAE、GMM和KDE。通过特征提取和XGBoost分类，该方法实现了94%和96%的分类准确率。此外，使用FID、IS、SSIM和PSNR评估的结果表明，该方法在这些指标上也超越了现有的最先进方法。

**Conclusion:** 综上所述，本研究提出的方法在生成高质量、多样化的深度面部数据方面取得了显著的优势，从而在未来的情感计算应用中具有巨大的潜力。

**Abstract:** Affective computing faces a major challenge: the lack of high-quality,
diverse depth facial datasets for recognizing subtle emotional expressions. We
propose a framework for synthetic depth face generation using an optimized GAN
with Knowledge Distillation (EMA teacher models) to stabilize training, improve
quality, and prevent mode collapse. We also apply Genetic Algorithms to evolve
GAN latent vectors based on image statistics, boosting diversity and visual
quality for target emotions. The approach outperforms GAN, VAE, GMM, and KDE in
both diversity and quality. For classification, we extract and concatenate LBP,
HOG, Sobel edge, and intensity histogram features, achieving 94% and 96%
accuracy with XGBoost. Evaluation using FID, IS, SSIM, and PSNR shows
consistent improvement over state-of-the-art methods.

</details>


### [29] [$Δ$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation](https://arxiv.org/abs/2508.09199)
*Jucheng Hu,Suorong Yang,Dongzhan Zhou*

Main category: cs.CV

> This paper presents Δ-AttnMask, a data-efficient framework for VIF that evaluates image-text pairs' quality without labels or extra models, achieving state-of-the-art accuracy with minimal data.

<details>
  <summary>Details</summary>

**Motivation:** Data selection for VIF is a critical and understudied area. VIF requires more data compared to unimodal instruction finetuning, imposing challenges in scaling efficiently while maintaining both visual and textual content quality.

**Method:** This paper introduces a data-efficient framework called Δ-AttnMask for Visual Instruction Finetuning (VIF). Δ-AttnMask uses attention-guided masking of the model's hidden states to evaluate the quality of image-text pairs without needing labels, auxiliary models, or additional training. It measures the difference in loss (Δ) between the original hidden states and those that are masked using high-attention regions to assess sample quality.

**Result:** Across various Vision-Language Models (VLMs) and datasets, Δ-AttnMask achieves state-of-the-art performance using only 20% of the data, accelerating training by 5 times while surpassing full-dataset baselines by 10.1% in overall accuracy.

**Conclusion:** The proposed Δ-AttnMask framework shows significant performance improvements, efficient use of data, and broad applicability across different modalities and architectures in the context of VIF for VLMs.

**Abstract:** Visual Instruction Finetuning (VIF) is pivotal for post-training
Vision-Language Models (VLMs). Unlike unimodal instruction finetuning in
plain-text large language models, which mainly requires instruction datasets to
enable model instruction-following ability, VIF also requires multimodal data
to enable joint visual and textual understanding; therefore, it typically
requires more data. Consequently, VIF imposes stricter data selection
challenges: the method must scale efficiently to handle larger data demands
while ensuring the quality of both visual and textual content, as well as their
alignment. Despite its critical impact on performance, data selection for VIF
remains an understudied area. In this paper, we propose $\Delta$-AttnMask. This
data-efficient framework quantifies sample quality through attention-guided
masking of the model's hidden states, jointly evaluating image-text pairs
without requiring domain labels, auxiliary models, or extra training. By
computing loss differences ($\Delta$) between the original states and states
masked using high-attention regions, $\Delta$-AttnMask intrinsically assesses
sample quality. Experiments across multiple VLMs and datasets show that
$\Delta$-AttnMask achieves state-of-the-art performance with just 20% of data,
accelerating training by 5x while surpassing full-dataset baselines by +10.1%
in overall accuracy. Its model-agnostic and data-agnostic design ensures broad
applicability across modalities and architectures.

</details>


### [30] [Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method](https://arxiv.org/abs/2508.09202)
*Masoumeh Sharafi,Soufiane Belharbi,Houssem Ben Salem,Ali Etemad,Alessandro Lameiras Koerich,Marco Pedersoli,Simon Bacon,Eric Granger*

Main category: cs.CV

> 本文提出了一种轻量级的个性化特征转换（PFT）方法，针对仅提供未标记目标领域中性表情的特定场景，通过在潜在空间内操作实现源模型的个性化，提升了面部表情识别模型在实际应用中的表现。

<details>
  <summary>Details</summary>

**Motivation:** 为了提升面部表情识别模型在实际场景中的性能，特别是在高主体间变异性以及难以区分的细微表情情况下，且避免了获取源数据的隐私和其他限制。

**Method:** 本文提出了一种个性化特征转换（PFT）方法，通过优化表情一致性和风格感知目标的方式，在潜在空间内操作，提升面部表情识别模型的表现。该方法首先在源领域数据上预训练翻译器，然后仅基于中性表情的目标领域数据进行调整，避免了图像合成的复杂性和噪声。

**Result:** 通过使用PFT方法，作者证明了在不使用源数据或图像合成的情况下，模型能够产生优化的分类嵌入，并提高了面部表情识别的性能。

**Conclusion:** PFT是一种高效的源域无关的领域自适应方法，适用于小程序生成表情图像的不稳定性和计算上的复杂性。相较于基于图像的翻译，使用PFT减少了计算负担，并且提高了面部表情识别模型在实际应用场景中的性能。

**Abstract:** Facial expression recognition (FER) models are employed in many video-based
affective computing applications, such as human-computer interaction and
healthcare monitoring. However, deep FER models often struggle with subtle
expressions and high inter-subject variability, limiting their performance in
real-world applications. To improve their performance, source-free domain
adaptation (SFDA) methods have been proposed to personalize a pretrained source
model using only unlabeled target domain data, thereby avoiding data privacy,
storage, and transmission constraints. This paper addresses a challenging
scenario where source data is unavailable for adaptation, and only unlabeled
target data consisting solely of neutral expressions is available. SFDA methods
are not typically designed to adapt using target data from only a single class.
Further, using models to generate facial images with non-neutral expressions
can be unstable and computationally intensive. In this paper, personalized
feature translation (PFT) is proposed for SFDA. Unlike current image
translation methods for SFDA, our lightweight method operates in the latent
space. We first pre-train the translator on the source domain data to transform
the subject-specific style features from one source subject into another.
Expression information is preserved by optimizing a combination of expression
consistency and style-aware objectives. Then, the translator is adapted on
neutral target data, without using source data or image synthesis. By
translating in the latent space, PFT avoids the complexity and noise of face
expression generation, producing discriminative embeddings optimized for
classification. Using PFT eliminates the need for image synthesis, reduces
computational overhead (using a lightweight translator), and only adapts part
of the model, making the method efficient compared to image-based translation.

</details>


### [31] [GANime: Generating Anime and Manga Character Drawings from Sketches with Deep Learning](https://arxiv.org/abs/2508.09207)
*Tai Vu,Robert Yang*

Main category: cs.CV

> 研究了多种模型，用于将动画人物素描图转换为上色图，结果表明C-GAN模型生成的图像质量最高。

<details>
  <summary>Details</summary>

**Motivation:** 手绘上色在漫画和动画行业中是一项费时又昂贵的工作，本研究旨在寻找有效的模型来自动化这个过程。

**Method:** 我们研究了多种图像到图像的转换模型，包括神经风格迁移、条件生成对抗网络（C-GAN）以及循环对抗网络（CycleGAN），用于在手绘素描与动画人物之间进行图像转换。

**Result:** 通过定性和定量评估，发现C-GAN模型在生成高质量、高分辨率图像方面最为有效，这些图像与人工创作的图像非常接近。

**Conclusion:** 研究表明C-GAN模型是一种有效的自动化上色工具，能够生成高质量且分辨率高的图像，可有效减轻漫画和动画行业的上色负担。

**Abstract:** The process of generating fully colorized drawings from sketches is a large,
usually costly bottleneck in the manga and anime industry. In this study, we
examine multiple models for image-to-image translation between anime characters
and their sketches, including Neural Style Transfer, C-GAN, and CycleGAN. By
assessing them qualitatively and quantitatively, we find that C-GAN is the most
effective model that is able to produce high-quality and high-resolution images
close to those created by humans.

</details>


### [32] [MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models](https://arxiv.org/abs/2508.09210)
*Fan Zhang,Zebang Cheng,Chong Deng,Haoxuan Li,Zheng Lian,Qian Chen,Huadai Liu,Wen Wang,Yi-Fan Zhang,Renrui Zhang,Ziyu Guo,Zhihong Zhu,Hao Wu,Haixin Wang,Yefeng Zheng,Xiaojiang Peng,Xian Wu,Kun Wang,Xiangang Li,Jieping Ye,Pheng-Ann Heng*

Main category: cs.CV

> 论文提出了一个名为MME-Emotion的系统化情感智能基准测试，内容包含数千视频片段，用于评估多模态大型语言模型在情感理解和推理方面的能力及限制。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有情感基准测试在多模态大型语言模型的情感智能方面存在局限，例如模型在不同场景中的泛化能力以及识别情感状态触发因素的推理能力尚未得到充分研究。MME-Emotion旨在填补这些研究空白。

**Method:** 该论文提出了一种名为MME-Emotion的系统化基准测试，用于评估多模态大型语言模型的情感理解和推理能力。该基准测试具有可扩展能力、多种设置和统一的评估协议，包含超过6000个精心挑选的视频剪辑及特定任务的问答对，涵盖广泛场景，形成了八个情感任务。

**Result:** 论文通过严格的20个先进MLLMs评估，发现当前MLLMs的情感智能表现欠佳。最佳模型仅在基准测试中获得39.3%的情感识别得分和56.0%的CoT得分。此外，通用模型和特定领域调整的模型在情感理解上可实现相近的表现。

**Conclusion:** 引入MME-Emotion的目的是为多模态大型语言模型的情感智能研究提供一个坚实的基础。

**Abstract:** Recent advances in multimodal large language models (MLLMs) have catalyzed
transformative progress in affective computing, enabling models to exhibit
emergent emotional intelligence. Despite substantial methodological progress,
current emotional benchmarks remain limited, as it is still unknown: (a) the
generalization abilities of MLLMs across distinct scenarios, and (b) their
reasoning capabilities to identify the triggering factors behind emotional
states. To bridge these gaps, we present \textbf{MME-Emotion}, a systematic
benchmark that assesses both emotional understanding and reasoning capabilities
of MLLMs, enjoying \textit{scalable capacity}, \textit{diverse settings}, and
\textit{unified protocols}. As the largest emotional intelligence benchmark for
MLLMs, MME-Emotion contains over 6,000 curated video clips with task-specific
questioning-answering (QA) pairs, spanning broad scenarios to formulate eight
emotional tasks. It further incorporates a holistic evaluation suite with
hybrid metrics for emotion recognition and reasoning, analyzed through a
multi-agent system framework. Through a rigorous evaluation of 20 advanced
MLLMs, we uncover both their strengths and limitations, yielding several key
insights: \ding{182} Current MLLMs exhibit unsatisfactory emotional
intelligence, with the best-performing model achieving only $39.3\%$
recognition score and $56.0\%$ Chain-of-Thought (CoT) score on our benchmark.
\ding{183} Generalist models (\emph{e.g.}, Gemini-2.5-Pro) derive emotional
intelligence from generalized multimodal understanding capabilities, while
specialist models (\emph{e.g.}, R1-Omni) can achieve comparable performance
through domain-specific post-training adaptation. By introducing MME-Emotion,
we hope that it can serve as a foundation for advancing MLLMs' emotional
intelligence in the future.

</details>


### [33] [Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity](https://arxiv.org/abs/2508.09218)
*Zuoou Li,Weitong Zhang,Jingyuan Wang,Shuyuan Zhang,Wenjia Bai,Bernhard Kainz,Mengyun Qiao*

Main category: cs.CV

> 本文提出了一种新的评估框架和一种递归重写策略(BSD)，在多个多模态大型语言模型中测试显示BSD提高了对抗性提示的成功率和危害性，揭示了现有安全系统的弱点。

<details>
  <summary>Details</summary>

**Motivation:** 现有的对抗性提示评估标准可能高估了攻击的有效性，因为许多被视为成功的提示实际是无害的、模糊的或者与预期的恶意不符。因此，需要一个更准确的评估方法和能有效绕过安全过滤器的攻击方法。

**Method:** 通过引入一个四维度的评估框架，该框架考量输入的贴题性、输入的分布外（OOD）强度、输出的危害性以及输出的拒绝率，来识别真正有效的解除限制方法。在此基础上，开发出了一种递归重写策略——平衡结构分解（BSD），该策略将恶意提示重组为语义上的子任务，并引入细微的OOD信号和视觉线索，使提示更难以被识别。

**Result:** 实验证明，BSD策略在13种商用和开源的多模态大型语言模型中一致提高了攻击成功率、增强了有害输出能力，并减少了拒绝情况，相较于之前的方法提高了67%的成功率和21%的危害性。

**Conclusion:** 研究表明，平衡相关性和新颖性的提示更可能绕过安全过滤器并触发有害输出，且开发的BSD策略进一步揭示了目前多模态安全系统的不足之处。

**Abstract:** Multimodal large language models (MLLMs) are widely used in vision-language
reasoning tasks. However, their vulnerability to adversarial prompts remains a
serious concern, as safety mechanisms often fail to prevent the generation of
harmful outputs. Although recent jailbreak strategies report high success
rates, many responses classified as "successful" are actually benign, vague, or
unrelated to the intended malicious goal. This mismatch suggests that current
evaluation standards may overestimate the effectiveness of such attacks. To
address this issue, we introduce a four-axis evaluation framework that
considers input on-topicness, input out-of-distribution (OOD) intensity, output
harmfulness, and output refusal rate. This framework identifies truly effective
jailbreaks. In a substantial empirical study, we reveal a structural trade-off:
highly on-topic prompts are frequently blocked by safety filters, whereas those
that are too OOD often evade detection but fail to produce harmful content.
However, prompts that balance relevance and novelty are more likely to evade
filters and trigger dangerous output. Building on this insight, we develop a
recursive rewriting strategy called Balanced Structural Decomposition (BSD).
The approach restructures malicious prompts into semantically aligned
sub-tasks, while introducing subtle OOD signals and visual cues that make the
inputs harder to detect. BSD was tested across 13 commercial and open-source
MLLMs, where it consistently led to higher attack success rates, more harmful
outputs, and fewer refusals. Compared to previous methods, it improves success
rates by $67\%$ and harmfulness by $21\%$, revealing a previously
underappreciated weakness in current multimodal safety systems.

</details>


### [34] [Towards Scalable Training for Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2508.09220)
*Haoyang Li,Jiaqing Li,Jialun Cao,Zongyuan Yang,Yongping Xiong*

Main category: cs.CV

> 我们提出了一种新的方法，通过将有限的手写公式与大规模的LaTeX渲染公式结合，并开发一个生成复杂且一致的LaTeX序列的数据引擎，构建了包含超过8000万高质量训练实例的公式数据集	exttt{Tex80M}。我们还提出了	exttt{TexTeller}，这是首个在大规模数据上训练的手写数学表达式识别模型。

<details>
  <summary>Details</summary>

**Motivation:** 大规模基础模型通过在海量数据集上的可扩展训练，取得了显著的性能提升。然而，手写数学表达式识别(HMER)领域由于数据稀缺而受阻，主要是因为手动标注过程困难且昂贵。本研究旨在通过构建大规模训练集来弥补这一差距。

**Method:** 我们的方法是通过开发一个可扩展的数据引擎，将有限的手写公式与大规模的LaTeX渲染公式结合，生成复杂且一致的LaTeX序列。利用这个引擎，我们构建了迄今为止最大的公式数据集	exttt{Tex80M}，包含超过8000万高质量训练实例。此外，我们提出了	exttt{TexTeller}，这是首个通过混合训练	exttt{Tex80M}与相对较小的手写表达式数据集而大规模训练的手写数学表达式识别模型。

**Result:** 通过我们的大规模训练数据集和优化的训练流程，	exttt{TexTeller}模型在几乎所有基准上展现了最先进的性能。

**Conclusion:** 为了推动这一领域的进一步研究，我们将公开发布我们的模型、数据集和完整代码库。这样可以为后续研究奠定基础。

**Abstract:** Large foundation models have achieved significant performance gains through
scalable training on massive datasets. However, the field of
\textbf{H}andwritten \textbf{M}athematical \textbf{E}xpression
\textbf{R}ecognition (HMER) has been impeded by the scarcity of data, primarily
due to the arduous and costly process of manual annotation. To bridge this gap,
we propose a novel method integrating limited handwritten formulas with
large-scale LaTeX-rendered formulas by developing a scalable data engine to
generate complex and consistent LaTeX sequences. With this engine, we built the
largest formula dataset to date, termed \texttt{Tex80M}, comprising over 80
million high-quality training instances. Then we propose \texttt{TexTeller},
the first HMER model trained at scale, by mix-training \texttt{Tex80M} with a
relatively small HME dataset. The expansive training dataset and our refined
pipeline have equipped \texttt{TexTeller} with state-of-the-art (SOTA)
performance across nearly all benchmarks. To advance the field, we will openly
release our complete model, entire dataset, and full codebase, enabling further
research building upon our contributions.

</details>


### [35] [Gradient-Direction-Aware Density Control for 3D Gaussian Splatting](https://arxiv.org/abs/2508.09239)
*Zheng Zhou,Yu-Jie Xiong,Chun-Ming Xia,Jia-Chen Zhang,Hong-Jian Zhan*

Main category: cs.CV

> The paper introduces Gradient-Direction-Aware Gaussian Splatting (GDAGS), addressing over-reconstruction and over-densification issues in 3D Gaussian Splatting by adopting a gradient-direction-aware adaptive density control method.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind the paper is to resolve the over-reconstruction and over-densification problems of the existing 3D Gaussian Splatting techniques in handling complex scenarios, aiming to improve the efficiency and quality of the rendering process.

**Method:** The method involves the introduction of the Gradient Coherence Ratio (GCR) and a nonlinear dynamic weighting mechanism to provide adaptive density control that is sensitive to the direction of the gradient for both splitting and cloning operations.

**Result:** The result shows that GDAGS improves rendering quality and reduces memory consumption by almost half, effectively mitigating issues such as over-reconstruction and over-densification.

**Conclusion:** The GDAGS approach significantly enhances the performance of 3D Gaussian Splatting methods by optimizing Gaussian representation and utilization, leading to more efficient and high-quality real-time rendering.

**Abstract:** The emergence of 3D Gaussian Splatting (3DGS) has significantly advanced
novel view synthesis through explicit scene representation, enabling real-time
photorealistic rendering. However, existing approaches manifest two critical
limitations in complex scenarios: (1) Over-reconstruction occurs when
persistent large Gaussians cannot meet adaptive splitting thresholds during
density control. This is exacerbated by conflicting gradient directions that
prevent effective splitting of these Gaussians; (2) Over-densification of
Gaussians occurs in regions with aligned gradient aggregation, leading to
redundant component proliferation. This redundancy significantly increases
memory overhead due to unnecessary data retention. We present
Gradient-Direction-Aware Gaussian Splatting (GDAGS), a gradient-direction-aware
adaptive density control framework to address these challenges. Our key
innovations: the gradient coherence ratio (GCR), computed through normalized
gradient vector norms, which explicitly discriminates Gaussians with concordant
versus conflicting gradient directions; and a nonlinear dynamic weighting
mechanism leverages the GCR to enable gradient-direction-aware density control.
Specifically, GDAGS prioritizes conflicting-gradient Gaussians during splitting
operations to enhance geometric details while suppressing redundant
concordant-direction Gaussians. Conversely, in cloning processes, GDAGS
promotes concordant-direction Gaussian densification for structural completion
while preventing conflicting-direction Gaussian overpopulation. Comprehensive
evaluations across diverse real-world benchmarks demonstrate that GDAGS
achieves superior rendering quality while effectively mitigating
over-reconstruction, suppressing over-densification, and constructing compact
scene representations with 50\% reduced memory consumption through optimized
Gaussians utilization.

</details>


### [36] [FineState-Bench: A Comprehensive Benchmark for Fine-Grained State Control in GUI Agents](https://arxiv.org/abs/2508.09241)
*Fengxian Ji,Jingpu Yang,Zirui Song,Yuanxi Wang,Zhexuan Cui,Yuke Li,Qian Jiang,Miao Fang,Xiuying Chen*

Main category: cs.CV

> 提出FineState-Bench框架，用于评估和诊断GUI代理的细粒度控制能力，发现视觉定位是关键瓶颈。

<details>
  <summary>Details</summary>

**Motivation:** 现有的评估框架过于专注于粗粒度任务完成，忽视了细粒度控制能力，这对现实应用至关重要。因此，提出了FineState-Bench，旨在量化和提高这种能力。

**Method:** 引入FineState-Bench，一个用于细粒度GUI代理操作的评价和诊断标准，该框架包含2257个任务基准测试，在四个组成部分中进行综合评估。同时开发了插件式的视觉诊断助手VDA，用于分析感知和定位能力。

**Result:** 实验结果显示最先进的模型仅能实现32.8%的细粒度交互准确性。使用VDA测试显示理想视觉定位可以将Gemini-2.5-Flash的成功率提升14.9%。

**Conclusion:** 诊断框架证明了当前GUI代理的主要瓶颈在于基本的视觉定位能力，这为未来研究提供了方向。

**Abstract:** With the rapid advancement of generative artificial intelligence technology,
Graphical User Interface (GUI) agents have demonstrated tremendous potential
for autonomously managing daily tasks through natural language instructions.
However, current evaluation frameworks for GUI agents suffer from fundamental
flaws: existing benchmarks overly focus on coarse-grained task completion while
neglecting fine-grained control capabilities crucial for real-world
applications. To address this, we introduce FineState-Bench, the first
evaluation and diagnostic standard for fine-grained GUI proxy operations,
designed to quantify fine-grained control. This multi-platform (desktop, Web,
mobile) framework includes 2257 task benchmarks in four components and uses a
four-phase indicator for comprehensive perception-to-control assessment. To
analyze perception and positioning for refined operations, we developed the
plug-and-play Visual Diagnostic Assistant (VDA), enabling the first
quantitative decoupling analysis of these capabilities. Experimental results on
our benchmark show that the most advanced models achieve only 32.8%
fine-grained interaction accuracy. Using our VDA in controlled experiments,
quantifying the impact of visual capabilities, we showed that ideal visual
localization boosts Gemini-2.5-Flash's success rate by 14.9\%. Our diagnostic
framework confirms for the first time that the primary bottleneck for current
GUI proxies is basic visual positioning capability.All resources are fully
open-source. github: https://github.com/AnonymousThewarehouse/FineState-Bench
huggingface: https://huggingface.co/datasets/Willtime2006/Static-FineBench

</details>


### [37] [Beyond Blanket Masking: Examining Granularity for Privacy Protection in Images Captured by Blind and Low Vision Users](https://arxiv.org/abs/2508.09245)
*Jeffri Murrugarra-LLerena,Haoran Niu,K. Suzanne Barber,Hal Daumé III,Yang Trista Cao,Paola Cascante-Bonilla*

Main category: cs.CV

> 本文介绍了一种名为FiGPriv的细粒度隐私保护方法，它能够在保护隐私的同时提高视觉语言模型提供有用响应的能力和图像内容识别的准确率。

<details>
  <summary>Details</summary>

**Motivation:** 由于基于视觉语言模型的视觉辅助系统日益普及，对用户隐私的担忧也随之增加，特别是对于视障和低视力用户来说，他们可能无意中捕捉到私人信息。现有的隐私保护方法依赖于粗粒度的分割技术，这可能会以牺牲易用性为代价。

**Method:** 我们提出了一种名为FiGPriv的细粒度隐私保护框架，该框架结合了细粒度分割与数据驱动的风险评分机制，以选择性地仅遮蔽高风险的私人信息，同时保留低风险信息。

**Result:** 通过BIV-Priv-Seg数据集的评估结果显示，FiGPriv框架保留了26%的图像内容，增强了视觉语言模型提供有用响应的能力11%，同时提高了图像内容识别的准确率45%，并确保了隐私保护。

**Conclusion:** 我们的工作表明，通过细粒度的隐私保护方法能够显著提升视觉语言模型在保障隐私的同时提供更有效的图像分析功能。

**Abstract:** As visual assistant systems powered by visual language models (VLMs) become
more prevalent, concerns over user privacy have grown, particularly for blind
and low vision users who may unknowingly capture personal private information
in their images. Existing privacy protection methods rely on coarse-grained
segmentation, which uniformly masks entire private objects, often at the cost
of usability. In this work, we propose FiGPriv, a fine-grained privacy
protection framework that selectively masks only high-risk private information
while preserving low-risk information. Our approach integrates fine-grained
segmentation with a data-driven risk scoring mechanism. We evaluate our
framework using the BIV-Priv-Seg dataset and show that FiG-Priv preserves +26%
of image content, enhancing the ability of VLMs to provide useful responses by
11% and identify the image content by 45%, while ensuring privacy protection.
Project Page: https://artcs1.github.io/VLMPrivacy/

</details>


### [38] [Harnessing Input-Adaptive Inference for Efficient VLN](https://arxiv.org/abs/2508.09262)
*Dongwoo Kang,Akhil Perincherry,Zachary Coalson,Aiden Gabriel,Stefan Lee,Sanghyun Hong*

Main category: cs.CV

> The paper introduces a novel input-adaptive navigation method in vision-and-language navigation (VLN) that significantly reduces computational requirements without sacrificing performance.

<details>
  <summary>Details</summary>

**Motivation:** Addressing the scale bottleneck of history-aware multi-modal transformer models in VLN, especially in practical settings with limited computational resources.

**Method:** Structure

**Result:** The introduced adaptive algorithms demonstrate over a 2 times reduction in computation across three off-the-shelf agents in both standard and continuous environments, without compromising performance.

**Conclusion:** The proposed input-adaptive navigation method effectively enhances VLN model efficiency, reducing computation and maintaining performance, making it a promising solution for practical applications with limited resources.

**Abstract:** An emerging paradigm in vision-and-language navigation (VLN) is the use of
history-aware multi-modal transformer models. Given a language instruction,
these models process observation and navigation history to predict the most
appropriate action for an agent. While they have significantly improved
performance, the scale of these models can be a bottleneck in practical
settings with limited computational resources. In this work, we propose a novel
input-adaptive navigation method to enhance VLN model efficiency. We first show
that existing input-adaptive mechanisms fail to reduce computations without
substantial performance degradation. To address this, we introduce three
adaptive algorithms, each deployed at a different level: (1) To improve spatial
efficiency, we selectively process panoramic views at each observation of an
agent. (2) To improve intra-model efficiency, we propose importance-based
adaptive thresholding for the early-exit methods. (3) To improve temporal
efficiency, we implement a caching mechanism that prevents reprocessing of
views previously seen by the agent. In evaluations on seven VLN benchmarks, we
demonstrate over a 2$\times$ reduction in computation across three
off-the-shelf agents in both standard and continuous environments. Our code is
publicly available at
https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.

</details>


### [39] [SegDAC: Segmentation-Driven Actor-Critic for Visual Reinforcement Learning](https://arxiv.org/abs/2508.09325)
*Alexandre Brown,Glen Berseth*

Main category: cs.CV

> SegDAC是使用分割和强化学习进行视觉操作任务的高效模型，在视觉泛化和样本效率上表现良好，尤其是在困难场景下的表现显著优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 由于视觉强化学习需要从高维输入和嘈杂奖励中学习感知和行动，因此具有挑战性。尽管存在大型感知模型，但要将其有效地集成到RL中以实现视觉泛化并提高采样效率仍然不清楚。因此，SegDAC旨在解决这一难题。

**Method:** 我们提出了SegDAC方法，这是一种基于分割的Actor-Critic方法。SegDAC使用Segment Anything（SAM）进行以对象为中心的分解，并通过YOLO-World和文本提示进行语义接地。它包括一个支持动态数量分割的新型变压器架构，可以在线通过强化学习学习关注哪些分割，且无需使用人工标签。

**Result:** 通过使用Maniskill3对SegDAC进行具有挑战性的视觉泛化基准测试，研究覆盖了在强大视觉扰动下的各种操作任务，结果表明SegDAC在最难设置中实现了比以前的方法更好的视觉泛化性能，几乎翻倍了先前的表现，在所有评估任务上在样本效率上与或超越了以前的方法。

**Conclusion:** SegDAC展示了一个结合图像分割、语义理解和强化学习的有效架构，能够显著提高视觉泛化和样本效率，对于光照、背景变化等具有较好的鲁棒性。

**Abstract:** Visual reinforcement learning (RL) is challenging due to the need to learn
both perception and actions from high-dimensional inputs and noisy rewards.
Although large perception models exist, integrating them effectively into RL
for visual generalization and improved sample efficiency remains unclear. We
propose SegDAC, a Segmentation-Driven Actor-Critic method. SegDAC uses Segment
Anything (SAM) for object-centric decomposition and YOLO-World to ground
segments semantically via text prompts. It includes a novel transformer-based
architecture that supports a dynamic number of segments at each time step and
effectively learns which segments to focus on using online RL, without using
human labels. By evaluating SegDAC over a challenging visual generalization
benchmark using Maniskill3, which covers diverse manipulation tasks under
strong visual perturbations, we demonstrate that SegDAC achieves significantly
better visual generalization, doubling prior performance on the hardest setting
and matching or surpassing prior methods in sample efficiency across all
evaluated tasks.

</details>


### [40] [Lung-DDPM+: Efficient Thoracic CT Image Synthesis using Diffusion Probabilistic Model](https://arxiv.org/abs/2508.09327)
*Yifan Jiang,Ahmad Shariftabrizi,Venkata SK. Manem*

Main category: cs.CV

> 研究人员提出了一种改进的生成模型Lung-DDPM+，用于提高肺部CT图像中肺结节的诊断效率和准确性，实验结果显示Lung-DDPM+在效率和质量上都有显著提升。

<details>
  <summary>Details</summary>

**Motivation:** 现有的用于肺癌诊断的生成模型存在低效率和解剖准确性不足的问题，这些限制了它们在临床中的适用性。因此，这项研究旨在解决这些问题。

**Method:** 本研究提出了Lung-DDPM+模型，这是先前Lung-DDPM模型的改进版本。Lung-DDPM+是一种基于去噪扩散概率模型（DDPM），通过结节语义布局引导并利用肺DPM求解器加速，使模型能够专注于病灶区域，并在采样效率和质量之间达到更好的权衡。

**Result:** 实验结果表明，与Lung-DDPM相比，Lung-DDPM+在公共的LIDC-IDRI数据集上实现了8倍的FLOPs减少，6.8倍的GPU内存消耗降低，以及14倍的采样加速。此外，在两个下游分割任务中，它保持了与Lung-DDPM和其他最先进的生成模型相当的样本质量。通过一位有经验的放射科医生进行的视图图灵测试，显示了所提方法生成合成样本的高级质量和保真度。

**Conclusion:** 实验结果表明，Lung-DDPM+能够有效地生成高质量的胸部CT图像和肺结节，展示了其在肿瘤合成和病变生成等医疗影像领域更广泛的应用潜力。代码和预训练模型可以在https://github.com/Manem-Lab/Lung-DDPM-PLUS获取。

**Abstract:** Generative artificial intelligence (AI) has been playing an important role in
various domains. Leveraging its high capability to generate high-fidelity and
diverse synthetic data, generative AI is widely applied in diagnostic tasks,
such as lung cancer diagnosis using computed tomography (CT). However, existing
generative models for lung cancer diagnosis suffer from low efficiency and
anatomical imprecision, which limit their clinical applicability. To address
these drawbacks, we propose Lung-DDPM+, an improved version of our previous
model, Lung-DDPM. This novel approach is a denoising diffusion probabilistic
model (DDPM) guided by nodule semantic layouts and accelerated by a pulmonary
DPM-solver, enabling the method to focus on lesion areas while achieving a
better trade-off between sampling efficiency and quality. Evaluation results on
the public LIDC-IDRI dataset suggest that the proposed method achieves
8$\times$ fewer FLOPs (floating point operations per second), 6.8$\times$ lower
GPU memory consumption, and 14$\times$ faster sampling compared to Lung-DDPM.
Moreover, it maintains comparable sample quality to both Lung-DDPM and other
state-of-the-art (SOTA) generative models in two downstream segmentation tasks.
We also conducted a Visual Turing Test by an experienced radiologist, showing
the advanced quality and fidelity of synthetic samples generated by the
proposed method. These experimental results demonstrate that Lung-DDPM+ can
effectively generate high-quality thoracic CT images with lung nodules,
highlighting its potential for broader applications, such as general tumor
synthesis and lesion generation in medical imaging. The code and pretrained
models are available at https://github.com/Manem-Lab/Lung-DDPM-PLUS.

</details>


### [41] [UltraLight Med-Vision Mamba for Classification of Neoplastic Progression in Tubular Adenomas](https://arxiv.org/abs/2508.09339)
*Aqsa Sultana,Nordin Abouzahra,Ahmed Rahu,Brian Shula,Brandon Combs,Derrick Forchetti,Theus Aspiras,Vijayan K. Asari*

Main category: cs.CV

> 研究采用了Ultralight Med-Vision Mamba算法，改善前癌性息肉的识别，提高腺瘤分类及风险评估的准确性，是一个高效的临床实时工具。

<details>
  <summary>Details</summary>

**Motivation:** 其动机是通过改进前癌性息肉的识别，以便在常规结肠镜检查过程中精确分类和分层腺瘤，从而提高风险评估的准确性，并为个性化监护方案提供支持，从而优化患者结果。

**Method:** 本研究采用了一种基于状态空间模型（SSM）的Ultralight Med-Vision Mamba算法，该算法擅长建模长短期依赖关系和图像泛化，这对于全切片图像分析至关重要。

**Result:** Ultralight Med-Vision Mamba算法显示了在计算速度和可扩展性方面的优势，使其成为临床实时部署的有前途的工具。

**Conclusion:** Ultralight Med-Vision Mamba是一个高效且具有可扩展性的算法，适用于临床实时部署，来改进前癌性息肉的识别和分类。

**Abstract:** Identification of precancerous polyps during routine colonoscopy screenings
is vital for their excision, lowering the risk of developing colorectal cancer.
Advanced deep learning algorithms enable precise adenoma classification and
stratification, improving risk assessment accuracy and enabling personalized
surveillance protocols that optimize patient outcomes. Ultralight Med-Vision
Mamba, a state-space based model (SSM), has excelled in modeling long- and
short-range dependencies and image generalization, critical factors for
analyzing whole slide images. Furthermore, Ultralight Med-Vision Mamba's
efficient architecture offers advantages in both computational speed and
scalability, making it a promising tool for real-time clinical deployment.

</details>


### [42] [Blink-to-code: real-time Morse code communication via eye blink detection and classification](https://arxiv.org/abs/2508.09344)
*Anushka Bhatt*

Main category: cs.CV

> 本研究提出一种利用眼睑眨眼转换成摩尔斯电码来协助严重运动障碍者进行通信的实时系统。

<details>
  <summary>Details</summary>

**Motivation:** 这项研究提出了一种实时系统，将自愿的眼睑眨眼转换成摩尔斯电码，使严重运动障碍的人能够进行交流。

**Method:** 使用标准网络摄像头和计算机视觉技术，系统检测并分类眨眼为短眨（点）或长眨（划），然后将它们解码成字母数字字符。

**Result:** 实验结果表明，该系统的解码准确率为62%，响应时间在18-20秒之间，显示了一种可行且低成本的辅助通信方法。

**Conclusion:** 研究表明，利用眼睑眨眼进行通信的方法是可行的，并提供了一种低成本的辅助通信手段。

**Abstract:** This study proposes a real-time system that translates voluntary eye blinks
into Morse code, enabling communication for individuals with severe motor
impairments. Using a standard webcam and computer vision, the system detects
and classifies blinks as short (dot) or long (dash), then decodes them into
alphanumeric characters. Experiments with five participants show 62% decoding
accuracy and 18-20 seconds response times, demonstrating a viable, low-cost
assistive communication method.

</details>


### [43] [FusionEnsemble-Net: An Attention-Based Ensemble of Spatiotemporal Networks for Multimodal Sign Language Recognition](https://arxiv.org/abs/2508.09362)
*Md. Milon Islam,Md Rezwanul Haque,S M Taslim Uddin Raju,Fakhri Karray*

Main category: cs.CV

> 本文介绍了一种新的融合时空网络集成框架，FusionEnsemble-Net，该框架在意大利手语的识别任务中展现了优于现有方法的准确率，达到99.44%。

<details>
  <summary>Details</summary>

**Motivation:** 为了准确识别手语在医疗沟通中的应用，需要建立一个可以准确解释复杂多模态手势的框架。传统的技术在这一领域表现不佳，因此提出了一个更加先进的算法来应对这个挑战。

**Method:** 提出了FusionEnsemble-Net，这是一个基于注意力机制的时空网络集成框架。FusionEnsemble-Net同时处理RGB视频和范围多普勒图雷达两种模态的数据，并通过四种不同的时空网络进行处理。每个网络都使用基于注意力的融合模块持续融合这两种模态的数据特征，然后输入到分类器集成中。最终，这四个不同融合通道的输出会在集成分类头中进行综合，从而提升模型的鲁棒性。

**Result:** 实验结果表明，FusionEnsemble-Net在大型多模态意大利手语数据集（MultiMeDaLIS）上的测试准确率达到99.44%，显著优于当前最先进的方法。

**Conclusion:** 集成多种不同的时空网络并以基于注意力机制的融合方式统一起来，可以提供一个稳健且准确的框架，用于复杂的多模态孤立手势识别任务。

**Abstract:** Accurate recognition of sign language in healthcare communication poses a
significant challenge, requiring frameworks that can accurately interpret
complex multimodal gestures. To deal with this, we propose FusionEnsemble-Net,
a novel attention-based ensemble of spatiotemporal networks that dynamically
fuses visual and motion data to enhance recognition accuracy. The proposed
approach processes RGB video and range Doppler map radar modalities
synchronously through four different spatiotemporal networks. For each network,
features from both modalities are continuously fused using an attention-based
fusion module before being fed into an ensemble of classifiers. Finally, the
outputs of these four different fused channels are combined in an ensemble
classification head, thereby enhancing the model's robustness. Experiments
demonstrate that FusionEnsemble-Net outperforms state-of-the-art approaches
with a test accuracy of 99.44% on the large-scale MultiMeDaLIS dataset for
Italian Sign Language. Our findings indicate that an ensemble of diverse
spatiotemporal networks, unified by attention-based fusion, yields a robust and
accurate framework for complex, multimodal isolated gesture recognition tasks.
The source code is available at:
https://github.com/rezwanh001/Multimodal-Isolated-Italian-Sign-Language-Recognition.

</details>
