{"id": "2512.08979", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08979", "abs": "https://arxiv.org/abs/2512.08979", "authors": ["Daechul Ahn", "Yura Choi", "Hyeonbeom Choi", "Seongwon Cho", "San Kim", "Jonghyun Choi"], "title": "What Happens When: Learning Temporal Orders of Events in Videos", "comment": "WACV 2026", "summary": "Video Large Multimodal Models (VLMMs) have shown impressive performance in video understanding, yet their ability to accurately capture the temporal order of multiple events remains underexplored. We interestingly observe that, even when video frames are scrambled, models perform very well on the existing benchmarks by comprehensive experiments. This implies that VLMMs may not necessarily rely on accurate sequential processing of visual events, but instead depend on prior knowledge of typical scenarios to answer the question. To benchmark temporal understanding capabilities in VLMMs, we propose VECTOR, designed to explicitly assess a model's ability to identify the temporal order of events. On this benchmark, we observe that various VLMMs often fail to understand the orders of events. To address this, we propose MECOT (Multi-Event instruction fine-tuning with Chain-of-Thought), which (1) trains models on detailed, event-by-event video descriptions and (2) using chain-of-thought prompts at inference to enhance temporal awareness. MECOT outperforms prior arts on VECTOR as well as improving performance on existing video benchmarks, implying effectiveness of temporal understanding. We release our code, model and datasets.", "AI": {"tldr": "提出了一种新的方法MECOT，旨在提高视频大模态模型（VLMMs）在理解事件时间顺序方面的能力，该方法改进了现有模型在时间理解上的性能。", "motivation": "视频大模态模型（VLMMs）在视频理解方面表现出色，但在准确捕捉多个事件的时间顺序方面尚需进一步探索。", "method": "通过训练模型使用详细的逐事件视频描述，并在推理时使用链式思考提示来增强时间意识，从而解决了现有VLMMs在理解事件顺序方面的问题。", "result": "MECOT在VECTOR基准测试中优于先前的方法，并且在现有视频基准测试上的性能得到提升。", "conclusion": "研究结果表明，通过细致的事件描述训练和链式思维提示，在推理过程中可以显著提高VLMMs的事件序列识别能力。这种方法在视频理解和时间顺序识别方面有显著优势。"}}
{"id": "2512.08980", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08980", "abs": "https://arxiv.org/abs/2512.08980", "authors": ["Chengqi Dong", "Chuhuai Yue", "Hang He", "Rongge Mao", "Fenghe Tang", "S Kevin Zhou", "Zekun Xu", "Xiaohan Wang", "Jiajun Chai", "Wei Lin", "Guojun Yin"], "title": "Training Multi-Image Vision Agents via End2End Reinforcement Learning", "comment": null, "summary": "Recent VLM-based agents aim to replicate OpenAI O3's ``thinking with images\" via tool use, but most open-source methods limit input to a single image, falling short on real-world multi-image QA tasks. To address this, we propose IMAgent, an open-source vision agent trained via end-to-end reinforcement learning dedicated for complex multi-image tasks. By leveraging a multi-agent system, we generate challenging and visually-rich multi-image QA pairs to fully activate the tool-use potential of the base VLM. Through manual verification, we obtain MIFG-QA, comprising 10k samples for training and evaluation. With deeper reasoning steps, VLMs may increasingly ignore visual inputs. We therefore develop two specialized tools for visual reflection and confirmation, allowing the model to proactively reallocate its attention to image content during inference. Benefiting from our well-designed action-trajectory two-level mask strategy, IMAgent achieves stable tool use behavior via pure RL training without requiring costly supervised fine-tuning data. Extensive experiments demonstrate that IMAgent maintains strong performance on existing single-image benchmarks while achieving substantial improvements on our proposed multi-image dataset, with our analysis providing actionable insights for the research community. Codes and data will be released soon.", "AI": {"tldr": "IMAgent是一个通过端到端强化学习训练的视觉代理，专注于复杂的多图像任务，能够稳定地实现工具使用行为，并在多图像数据集上表现优异。", "motivation": "大部分开源方法的输入局限于单个图像，无法应对现实世界中的多图像问答任务。为了弥补这一不足，团队提出了IMAgent，在复杂的多图像任务上训练视觉代理。", "method": "通过端到端的强化学习训练，IMAgent旨在处理复杂的多图像任务。团队设计了一种多智能体系统，用于生成具有挑战性的多图像问答对，以充分激活基础VLM的工具使用潜力。此外，开发了两个专门工具，用于视觉反思和确认，以使模型在推理过程中主动重新分配注意力到图像内容上。", "result": "IMAgent能够稳定地实现工具使用行为，而不需额外的监督微调数据。实验表明，IMAgent在现有的单图像基准测试上表现良好，并在所提出的多图像数据集上显著提升。", "conclusion": "IMAgent在多图像处理能力上展现了强大的潜力，为研究社区提供了有价值的见解。代码和数据即将开放。"}}
{"id": "2512.08981", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08981", "abs": "https://arxiv.org/abs/2512.08981", "authors": ["Tahar Chettaoui", "Naser Damer", "Fadi Boutros"], "title": "Mitigating Bias with Words: Inducing Demographic Ambiguity in Face Recognition Templates by Text Encoding", "comment": "Accepted at BMVC workshop (SRBS) 2025", "summary": "Face recognition (FR) systems are often prone to demographic biases, partially due to the entanglement of demographic-specific information with identity-relevant features in facial embeddings. This bias is extremely critical in large multicultural cities, especially where biometrics play a major role in smart city infrastructure. The entanglement can cause demographic attributes to overshadow identity cues in the embedding space, resulting in disparities in verification performance across different demographic groups. To address this issue, we propose a novel strategy, Unified Text-Image Embedding (UTIE), which aims to induce demographic ambiguity in face embeddings by enriching them with information related to other demographic groups. This encourages face embeddings to emphasize identity-relevant features and thus promotes fairer verification performance across groups. UTIE leverages the zero-shot capabilities and cross-modal semantic alignment of Vision-Language Models (VLMs). Given that VLMs are naturally trained to align visual and textual representations, we enrich the facial embeddings of each demographic group with text-derived demographic features extracted from other demographic groups. This encourages a more neutral representation in terms of demographic attributes. We evaluate UTIE using three VLMs, CLIP, OpenCLIP, and SigLIP, on two widely used benchmarks, RFW and BFW, designed to assess bias in FR. Experimental results show that UTIE consistently reduces bias metrics while maintaining, or even improving in several cases, the face verification accuracy.", "AI": {"tldr": "该研究提出了一种新方法 UTIE，通过利用视觉语言模型在面部嵌入中引入其他人口统计组的信息，来减少面部识别系统中的人口统计偏差，同时保持或提高面部验证的准确性。", "motivation": "面部识别系统常因面部嵌入中人口统计特定信息与身份相关特征纠缠而产生人口统计偏差。这种偏差在高度多元文化的智慧城市中尤其成问题。", "method": "提出了一种新策略 Unified Text-Image Embedding (UTIE)，该策略通过将与其他人口统计组相关的信息富集到面部嵌入中，以诱导面部嵌入对人口统计特征产生模糊性，从而促进更公平的验证性能。UTIE 利用了视觉语言模型的零样本能力和跨模态语义对齐功能，通过从其他人口统计组提取的文本派生的人口统计特征来丰富每个人口统计组的面部嵌入。", "result": "评估结果显示 UTIE 在两种广泛使用的基准测试 RFW 和 BFW 上，减少了偏差指标，同时保持或提高了面部验证准确性。", "conclusion": "实验结果显示 UTIE 一致性地减少了偏差指标，同时保持或提高了面部验证准确性。这证明了该方法在促进更公平的面部识别性能上的有效性。"}}
{"id": "2512.08982", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08982", "abs": "https://arxiv.org/abs/2512.08982", "authors": ["Jian Xu", "Wei Chen", "Shigui Li", "Delu Zeng", "John Paisley", "Qibin Zhao"], "title": "Consist-Retinex: One-Step Noise-Emphasized Consistency Training Accelerates High-Quality Retinex Enhancement", "comment": null, "summary": "Diffusion models have achieved remarkable success in low-light image enhancement through Retinex-based decomposition, yet their requirement for hundreds of iterative sampling steps severely limits practical deployment. While recent consistency models offer promising one-step generation for \\textit{unconditional synthesis}, their application to \\textit{conditional enhancement} remains unexplored. We present \\textbf{Consist-Retinex}, the first framework adapting consistency modeling to Retinex-based low-light enhancement. Our key insight is that conditional enhancement requires fundamentally different training dynamics than unconditional generation standard consistency training focuses on low-noise regions near the data manifold, while conditional mapping critically depends on large-noise regimes that bridge degraded inputs to enhanced outputs. We introduce two core innovations: (1) a \\textbf{dual-objective consistency loss} combining temporal consistency with ground-truth alignment under randomized time sampling, providing full-spectrum supervision for stable convergence; and (2) an \\textbf{adaptive noise-emphasized sampling strategy} that prioritizes training on large-noise regions essential for one-step conditional generation. On VE-LOL-L, Consist-Retinex achieves \\textbf{state-of-the-art performance with single-step sampling} (\\textbf{PSNR: 25.51 vs. 23.41, FID: 44.73 vs. 49.59} compared to Diff-Retinex++), while requiring only \\textbf{1/8 of the training budget} relative to the 1000-step Diff-Retinex baseline.", "AI": {"tldr": "本文提出了一种名为Consist-Retinex的低光照图像增强框架，通过一致性建模和Retinex分解相结合，实现了单步采样生成高质量图像，并在训练预算上大大优于传统的扩散模型。", "motivation": "低光照图像增强领域中的扩散模型虽然成功，但其上百次迭代采样需求限制了实际应用。一致性模型虽然能快速生成无条件合成图像，但之前未被用于条件增强任务。", "method": "提出了一种结合一致性建模和Retinex分解的框架（Consist-Retinex），包含双目标一致性损失函数和自适应噪声增强采样策略。", "result": "在VE-LOL-L数据集上，Consist-Retinex实现了最先进的单步采样性能，相比Diff-Retinex++在PSNR和FID指标上均有提高，且训练预算仅为之前的1/8。", "conclusion": "这项工作展示了Consist-Retinex在低光照图像增强方面的优势，特别是在单步采样和训练效率上的突破。"}}
{"id": "2512.08943", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08943", "abs": "https://arxiv.org/abs/2512.08943", "authors": ["Singon Kim"], "title": "Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models", "comment": "Master's thesis, Korea University, 2025", "summary": "Abstractive compression utilizes smaller langauge models to condense query-relevant context, reducing computational costs in retrieval-augmented generation (RAG). However, retrieved documents often include information that is either irrelevant to answering the query or misleading due to factual incorrect content, despite having high relevance scores. This behavior indicates that abstractive compressors are more likely to omit important information essential for the correct answer, especially in long contexts where attention dispersion occurs. To address this issue, we categorize retrieved documents in a more fine-grained manner and propose Abstractive Compression Robust against Noise (ACoRN), which introduces two novel training steps. First, we use offline data augmentation on the training dataset to enhance compressor robustness against two distinct types of retrieval noise. Second, since the language model based compressor cannot fully utilize information from multiple retrieved documents and exhibits positional bias, we perform finetuning to generate summaries centered around key information that directly supports the correct answer. Our experiments demonstrate that T5-large, trained with ACoRN as a compressor, improves EM and F1 scores while preserving the answer string, which could serve as direct evidence. ACoRN excels on datasets with many accuracy reducing documents, making it highly useful in real-world scenarios.", "AI": {"tldr": "本研究提出了ACoRN方法，通过改进压缩器训练来提高检索增强生成中答案的准确性。", "motivation": "动机是解决在检索增强生成（RAG）中，即便具有高相关性得分的检索文档也可能包含无关或误导信息的问题，这些问题可能会导致重要的信息被省略，从而影响生成答案的准确性。", "method": "本文提出了ACoRN方法，通过两种新的训练步骤改进了抽象压缩技术。第一步是离线数据增强，以增强压缩器对两种不同类型的检索噪声的鲁棒性。第二步是微调，以生成围绕直接支持正确答案的关键信息的摘要，解决语言模型压缩器无法充分利用多个检索文档信息的问题。", "result": "实验显示，使用ACoRN训练的T5-large作为压缩器，在保持答案字符串的同时，提高了EM和F1分数，尤其是在那些准确性降低文件较多的数据集上表现出色。", "conclusion": "本研究的结论是，ACoRN方法通过新的训练步骤使得压缩器更加鲁棒，特别是在处理包含大量准确性降低文档的数据集时，表现出色，这在实际应用中非常有用。"}}
{"id": "2512.08983", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08983", "abs": "https://arxiv.org/abs/2512.08983", "authors": ["Maoyu Wang", "Yao Lu", "Bo Zhou", "Zhuangzhi Chen", "Yun Lin", "Qi Xuan", "Guan Gui"], "title": "HSCP: A Two-Stage Spectral Clustering Framework for Resource-Constrained UAV Identification", "comment": null, "summary": "With the rapid development of Unmanned Aerial Vehicles (UAVs) and the increasing complexity of low-altitude security threats, traditional UAV identification methods struggle to extract reliable signal features and meet real-time requirements in complex environments. Recently, deep learning based Radio Frequency Fingerprint Identification (RFFI) approaches have greatly improved recognition accuracy. However, their large model sizes and high computational demands hinder deployment on resource-constrained edge devices. While model pruning offers a general solution for complexity reduction, existing weight, channel, and layer pruning techniques struggle to concurrently optimize compression rate, hardware acceleration, and recognition accuracy. To this end, in this paper, we introduce HSCP, a Hierarchical Spectral Clustering Pruning framework that combines layer pruning with channel pruning to achieve extreme compression, high performance, and efficient inference. In the first stage, HSCP employs spectral clustering guided by Centered Kernel Alignment (CKA) to identify and remove redundant layers. Subsequently, the same strategy is applied to the channel dimension to eliminate a finer redundancy. To ensure robustness, we further employ a noise-robust fine-tuning strategy. Experiments on the UAV-M100 benchmark demonstrate that HSCP outperforms existing channel and layer pruning methods. Specifically, HSCP achieves $86.39\\%$ parameter reduction and $84.44\\%$ FLOPs reduction on ResNet18 while improving accuracy by $1.49\\%$ compared to the unpruned baseline, and maintains superior robustness even in low signal-to-noise ratio environments.", "AI": {"tldr": "提出了HSCP框架，结合层剪枝和通道剪枝，实现了高压缩率、高性能和高效计算，并提高了UAV-RFFI的识别准确性和鲁棒性。", "motivation": "解决现有UAV识别方法在复杂环境中的不足，以及基于深度学习的RFFI方法模型体积大和计算量高的问题。", "method": "提出了一个层次化的频谱聚类剪枝框架HSCP，通过中心幂核对齐（CKA）引导的频谱聚类，先进行层剪枝，然后进行通道剪枝，并使用噪声鲁棒微调策略。", "result": "在UAV-M100基准测试上，HSCP实现了86.39%的参数减少和84.44%的FLOPs减少，同时相对于未剪枝的基准线，提高了1.49%的准确率，并表现出在低信噪比环境下的优越鲁棒性。", "conclusion": "HSCP框架对于追求极致压缩、高性能和高效推理的要求是有效的，能够提高UAV-RFFI的识别效率和鲁棒性。"}}
{"id": "2512.08944", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.08944", "abs": "https://arxiv.org/abs/2512.08944", "authors": ["Yudong Wang", "Zhe Yang", "Wenhan Ma", "Zhifang Sui", "Liang Zhao"], "title": "Enhancing Reliability across Short and Long-Form QA via Reinforcement Learning", "comment": null, "summary": "While reinforcement learning has unlocked unprecedented complex reasoning in large language models, it has also amplified their propensity for hallucination, creating a critical trade-off between capability and reliability. This work confronts this challenge by introducing a targeted RL framework designed to mitigate both intrinsic and extrinsic hallucinations across short and long-form question answering. We address extrinsic hallucinations (flawed internal knowledge) by creating a novel training set from open-ended conversions of TriviaQA. Concurrently, we tackle intrinsic hallucinations (unfaithfulness to context) by leveraging long-form texts from FineWeb in a fact-grounding reward scheme. To further bolster reliability, our framework explicitly rewards the model for refusing to answer unanswerable questions, thereby cultivating crucial cautiousness. Extensive experiments demonstrate that our methodology yields significant performance gains across a diverse suite of benchmarks, substantially reducing both hallucination types. Ultimately, this research contributes a practical framework for resolving the critical tension between advanced reasoning and factual trustworthiness, paving the way for more capable and reliable large language models.", "AI": {"tldr": "本文提出了一种新的强化学习框架，用以减少大型语言模型在问答任务中的幻觉，包括内在和外在两种幻觉类型。通过特定的训练集和长文本事实校验机制，实验结果表明该框架显著增强了模型的可靠性同时保持了其推理能力。", "motivation": "强化学习虽然增强了大型语言模型的复杂推理能力，但也放大了它们的幻觉倾向，从而在功能和可靠性之间造成了一种关键的权衡。本研究就是为了应对这一挑战。", "method": "本研究提出了一种针对强化学习框架，旨在减少大型语言模型在问答任务中的内在和外在幻觉。为了解决外在幻觉问题（内在知识缺陷），研究者创建了一个基于TriviaQA开放式对话的新训练集；对于内在幻觉问题（对上下文不忠实），则利用FineWeb中的长文本作为事实校验奖励机制。此外，该框架还通过奖励模型拒绝回答不可回答的问题来增加模型谨慎性。", "result": "实验结果显示，该方法在多种基准测试中显著提高了性能，大幅减少了幻觉类型。", "conclusion": "这项研究提供了一种实用框架，解决了高级推理和事实可靠性之间的紧张关系，有助于开发出更强大且可靠的大型语言模型。"}}
{"id": "2512.08984", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08984", "abs": "https://arxiv.org/abs/2512.08984", "authors": ["Nirhoshan Sivaroopan", "Hansi Karunarathna", "Chamara Madarasingha", "Anura Jayasumana", "Kanchana Thilakarathna"], "title": "RAG-HAR: Retrieval Augmented Generation-based Human Activity Recognition", "comment": null, "summary": "Human Activity Recognition (HAR) underpins applications in healthcare, rehabilitation, fitness tracking, and smart environments, yet existing deep learning approaches demand dataset-specific training, large labeled corpora, and significant computational resources.We introduce RAG-HAR, a training-free retrieval-augmented framework that leverages large language models (LLMs) for HAR. RAG-HAR computes lightweight statistical descriptors, retrieves semantically similar samples from a vector database, and uses this contextual evidence to make LLM-based activity identification. We further enhance RAG-HAR by first applying prompt optimization and introducing an LLM-based activity descriptor that generates context-enriched vector databases for delivering accurate and highly relevant contextual information. Along with these mechanisms, RAG-HAR achieves state-of-the-art performance across six diverse HAR benchmarks. Most importantly, RAG-HAR attains these improvements without requiring model training or fine-tuning, emphasizing its robustness and practical applicability. RAG-HAR moves beyond known behaviors, enabling the recognition and meaningful labelling of multiple unseen human activities.", "AI": {"tldr": "RAG-HAR是一种基于检索和大语言模型的无需训练框架，用于人体活动识别，它提升了识别的准确性和实用性，无需模型训练，更少依赖计算资源。", "motivation": "现有的深度学习方法需要特定数据集的训练、大量的标记数据和显著的计算资源。RAG-HAR旨在减少这些要求，提供更实用和强大的人体活动识别方法。", "method": "RAG-HAR采用了一种无需训练的检索增强框架，通过计算轻量级统计描述符，从向量数据库检索语义相似样本，并运用LLM进行活动识别。通过提示优化和LLM生成的活动描述符进一步增强了这一框架，为提供准确和高度相关的上下文信息生成了丰富的向量数据库。", "result": "RAG-HAR在六个不同的HAR基准测试中达到了最先进的性能，提高了识别多个未知人体活动的能力。", "conclusion": "RAG-HAR展示了无需经过训练就能识别活动的能力，突出了其在多种不同场景中的广泛适用性和实用性。"}}
{"id": "2512.08945", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08945", "abs": "https://arxiv.org/abs/2512.08945", "authors": ["Stefano Epifani", "Giuliano Castigliego", "Laura Kecskemeti", "Giuliano Razzicchia", "Elisabeth Seiwald-Sonderegger"], "title": "The Linguistic Architecture of Reflective Thought: Evaluation of a Large Language Model as a Tool to Isolate the Formal Structure of Mentalization", "comment": "18 pages, 1 table, Project coordinator: Stefano Epifani", "summary": "Background: Mentalization integrates cognitive, affective, and intersubjective components. Large Language Models (LLMs) display an increasing ability to generate reflective texts, raising questions regarding the relationship between linguistic form and mental representation. This study assesses the extent to which a single LLM can reproduce the linguistic structure of mentalization according to the parameters of Mentalization-Based Treatment (MBT).\n  Methods: Fifty dialogues were generated between human participants and an LLM configured in standard mode. Five psychiatrists trained in MBT, working under blinded conditions, evaluated the mentalization profiles produced by the model along the four MBT axes, assigning Likert-scale scores for evaluative coherence, argumentative coherence, and global quality. Inter-rater agreement was estimated using ICC(3,1).\n  Results: Mean scores (3.63-3.98) and moderate standard deviations indicate a high level of structural coherence in the generated profiles. ICC values (0.60-0.84) show substantial-to-high agreement among raters. The model proved more stable in the Implicit-Explicit and Self-Other dimensions, while presenting limitations in the integration of internal states and external contexts. The profiles were coherent and clinically interpretable yet characterized by affective neutrality.", "AI": {"tldr": "This paper evaluates whether an LLM can generate texts that reflect mentalization processes similar to those assessed in Mentalization-Based Treatment (MBT).", "motivation": "To explore the relationship between linguistic form produced by LLMs and mental representation structures, specifically mentalization as analyzed in MBT.", "method": "Content", "result": "The study found that the LLM-generated dialogues showed high structural coherence according to MBT criteria, with substantial inter-rater reliability, though limitations were noted in integrating internal and external contexts.", "conclusion": "LLMs can produce texts reflecting mentalization processes that are clinically interpretable, though characterized by affective neutrality, suggesting potential but also limitations in replicating complex human mental states through language generation."}}
{"id": "2512.08985", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.08985", "abs": "https://arxiv.org/abs/2512.08985", "authors": ["Vignesh Sundaresha", "Akash Haridas", "Vikram Appia", "Lav Varshney"], "title": "An Efficient Test-Time Scaling Approach for Image Generation", "comment": "11 pages", "summary": "Image generation has emerged as a mainstream application of large generative AI models. Just as test-time compute and reasoning have helped language models improve their capabilities, similar benefits have also been observed with image generation models. In particular, searching over noise samples for diffusion and flow models has shown to scale well with test-time compute. While recent works have explored allocating non-uniform inference-compute budgets across different denoising steps, they rely on greedy algorithms and allocate the compute budget ineffectively. In this work, we study this problem and propose solutions to fix it. We propose the Verifier-Threshold method which automatically reallocates test-time compute and delivers substantial efficiency improvements. For the same performance on the GenEval benchmark, we achieve a 2-4x reduction in computational time over the state-of-the-art method.", "AI": {"tldr": "Proposes a method for efficient test-time compute allocation in image generation models, resulting in improved performance.", "motivation": "To improve the efficiency of image generation models by effectively allocating test-time compute, overcoming the limitations of greedy algorithms used in recent works.", "method": "Verifies and reallocates test-time compute using the Verifier-Threshold method for image generation models, addressing previous works' inefficient allocation.", "result": "A 2-4x reduction in computational time over the state-of-the-art method while maintaining the same performance on the GenEval benchmark.", "conclusion": "The Verifier-Threshold method can significantly reduce computational time for image generation tasks without compromising performance, by automating the reallocation of test-time compute resources."}}
{"id": "2512.09015", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09015", "abs": "https://arxiv.org/abs/2512.09015", "authors": ["DatologyAI", ":", "Luke Merrick", "Alex Fang", "Aldo Carranza", "Alvin Deng", "Amro Abbas", "Brett Larsen", "Cody Blakeney", "Darren Teh", "David Schwab", "Fan Pan", "Haakon Mongstad", "Haoli Yin", "Jack Urbanek", "Jason Lee", "Jason Telanoff", "Josh Wills", "Kaleigh Mentzer", "Paul Burstein", "Parth Doshi", "Paul Burnstein", "Pratyush Maini", "Ricardo Monti", "Rishabh Adiga", "Scott Loftin", "Siddharth Joshi", "Spandan Das", "Tony Jiang", "Vineeth Dorma", "Zhengping Wang", "Bogdan Gaza", "Ari Morcos", "Matthew Leavitt"], "title": "Luxical: High-Speed Lexical-Dense Text Embeddings", "comment": "9 pages, 6 figures", "summary": "Frontier language model quality increasingly hinges on our ability to organize web-scale text corpora for training. Today's dominant tools trade off speed and flexibility: lexical classifiers (e.g., FastText) are fast but limited to producing classification output scores, while the vector-valued outputs of transformer text embedding models flexibly support numerous workflows (e.g., clustering, classification, and retrieval) but are computationally expensive to produce. We introduce Luxical, a library for high-speed \"lexical-dense\" text embeddings that aims to recover the best properties of both approaches for web-scale text organization. Luxical combines sparse TF--IDF features, a small ReLU network, and a knowledge distillation training regimen to approximate large transformer embedding models at a fraction of their operational cost. In this technical report, we describe the Luxical architecture and training objective and evaluate a concrete Luxical model in two disparate applications: a targeted webcrawl document retrieval test and an end-to-end language model data curation task grounded in text classification. In these tasks we demonstrate speedups ranging from 3x to 100x over varying-sized neural baselines, and comparable to FastText model inference during the data curation task. On these evaluations, the tested Luxical model illustrates favorable compute/quality trade-offs for large-scale text organization, matching the quality of neural baselines. Luxical is available as open-source software at https://github.com/datologyai/luxical.", "AI": {"tldr": "Luxical是一个图书馆，用于快速“词法密集”文本嵌入，结合了TF-IDF特征和ReLU网络以及知识蒸馏训练，提供在大规模文本组织中的计算/质量权衡。", "motivation": "当前的工具在处理大规模文本时速度和灵活性之间存在折衷。FastText速度快但功能有限，而变压器文本嵌入模型虽然灵活但计算成本昂贵。为了组织大规模文本，需要一种能兼具两者优点的方法。", "method": "Luxical结合了稀疏的TF-IDF特征、一个小的ReLU网络和知识蒸馏训练计划，以低得多的运行成本近似大型变压器嵌入模型。", "result": "在两个不同的应用场景中，Luxical展示了相比各种规模的神经网络基线3倍到100倍的速度提升，在数据整理任务中，其推理速度更是与FastText模型相当，且在质量方面也与神经网络基线匹配。", "conclusion": "Luxical展示了在大规模文本组织任务中优越的计算和质量权衡，可作为开放源代码软件使用。"}}
{"id": "2512.08986", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08986", "abs": "https://arxiv.org/abs/2512.08986", "authors": ["Anca Mihai", "Adrian Groza"], "title": "Explainable Fundus Image Curation and Lesion Detection in Diabetic Retinopathy", "comment": null, "summary": "Diabetic Retinopathy (DR) affects individuals with long-term diabetes. Without early diagnosis, DR can lead to vision loss. Fundus photography captures the structure of the retina along with abnormalities indicative of the stage of the disease. Artificial Intelligence (AI) can support clinicians in identifying these lesions, reducing manual workload, but models require high-quality annotated datasets. Due to the complexity of retinal structures, errors in image acquisition and lesion interpretation of manual annotators can occur. We proposed a quality-control framework, ensuring only high-standard data is used for evaluation and AI training. First, an explainable feature-based classifier is used to filter inadequate images. The features are extracted both using image processing and contrastive learning. Then, the images are enhanced and put subject to annotation, using deep-learning-based assistance. Lastly, the agreement between annotators calculated using derived formulas determines the usability of the annotations.", "AI": {"tldr": "本文提出了一种质量控制框架，确保仅使用高标准数据进行AI训练和评估，用于提高糖尿病视网膜病变的诊断准确性。", "motivation": "由于视网膜结构的复杂性和手动注释者在图像采集和病变解释中可能出现的错误，需要一种方法来提高数据质量，以支持AI模型的训练。", "method": "首先，使用可解释的基于特征的分类器过滤不合格的图像。这些特征是通过图像处理和对比学习提取出来的。然后，增强图像并通过深度学习辅助进行注释。最后，通过计算注释者之间的一致性来确定注释的可用性。", "result": "未提供具体结果。", "conclusion": "该质量控制框架可以提高数据集的质量，进而提高AI模型在糖尿病视网膜病变诊断中的性能。"}}
{"id": "2512.09127", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09127", "abs": "https://arxiv.org/abs/2512.09127", "authors": ["Zihan Han", "Junyan Ge", "Caifeng Li"], "title": "Knowledge-Guided Large Language Model for Automatic Pediatric Dental Record Understanding and Safe Antibiotic Recommendation", "comment": null, "summary": "Accurate interpretation of pediatric dental clinical records and safe antibiotic prescribing remain persistent challenges in dental informatics. Traditional rule-based clinical decision support systems struggle with unstructured dental narratives, incomplete radiographic descriptions, and complex safety constraints. To address these limitations, this study proposes a Knowledge-Guided Large Language Model (KG-LLM) that integrates a pediatric dental knowledge graph, retrieval-augmented generation (RAG), and a multi-stage safety validation pipeline for evidence-grounded antibiotic recommendation. The framework first employs a clinical NER/RE module to extract structured entities and relations from dental notes and radiology reports. Relevant guidelines, drug-safety rules, and analogous historical cases are subsequently retrieved from the knowledge graph and supplied to the LLM for diagnostic summarization and dose-drug-duration prediction. Safety assurance is achieved through a dual-layer validation mechanism combining deterministic rule checking with a learned classifier for detecting allergies, contraindications, and dosing errors. Experiments on 32,000 de-identified pediatric dental visit records demonstrate the effectiveness of the proposed approach. Compared with a domain-adapted Llama-2 clinical baseline, KG-LLM improves record-understanding performance (F1: 0.914 vs. 0.867), drug-dose-duration accuracy (Top-1: 0.782 vs. 0.716), and reduces unsafe antibiotic suggestions by 50%. Additional evaluation across summary quality, recommendation accuracy, and global safety scores further confirms the robustness of the system. Ablation analyses indicate that the knowledge graph, RAG, and safety modules each contribute substantially to clinical reliability and interpretability.", "AI": {"tldr": "本文提出了一种基于知识引导的大语言模型（KG-LLM）框架，用于改善儿童牙科临床记录解释和抗生素安全处方。该框架提高了对临床记录的理解、药物剂量和疗程的准确性，并减少了不安全的抗生素建议。", "motivation": "传统基于规则的临床决策支持系统在处理牙科叙述记录、不完整的影像学描述以及复杂的安全约束方面存在困难。本文旨在解决这些挑战。", "method": "本文提出的方法是KG-LLM，结合了儿童牙科知识图谱、检索生成（RAG）和多阶段安全验证管道，用于基于证据的抗生素推荐。首先使用临床命名实体识别和关系抽取模块从牙科记录和放射学报告中提取结构化实体和关系。然后，从知识图谱中检索相关指南、药品安全规则和类似的历史案例，并将其提供给大型语言模型用于诊断总结和药物剂量疗程的预测。通过确定性规则检查和学习分类器的双重层验证机制来保证安全性。", "result": "实验基于32,000条脱敏的儿童牙科访记录，结果显示，与域适应的Llama-2基础线相比，KG-LLM在记录理解性能（F1: 0.914 vs. 0.867）、药物剂量疗程准确性（Top-1: 0.782 vs. 0.716）、以及安全的抗生素建议方面均有所提升。", "conclusion": "研究确认了KG-LLM框架的有效性及其在提高抗生素处方准确性和安全性方面的潜在应用价值。知识图谱、RAG和安全模块共同提高了系统的临床可靠性和可解释性。"}}
{"id": "2512.08987", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08987", "abs": "https://arxiv.org/abs/2512.08987", "authors": ["Yuze Hao", "Linchao Zhu", "Yi Yang"], "title": "3DID: Direct 3D Inverse Design for Aerodynamics with Physics-Aware Optimization", "comment": "Accepted at NeurIPS 2025", "summary": "Inverse design aims to design the input variables of a physical system to optimize a specified objective function, typically formulated as a search or optimization problem. However, in 3D domains, the design space grows exponentially, rendering exhaustive grid-based searches infeasible. Recent advances in deep learning have accelerated inverse design by providing powerful generative priors and differentiable surrogate models. Nevertheless, current methods tend to approximate the 3D design space using 2D projections or fine-tune existing 3D shapes. These approaches sacrifice volumetric detail and constrain design exploration, preventing true 3D design from scratch. In this paper, we propose a 3D Inverse Design (3DID) framework that directly navigates the 3D design space by coupling a continuous latent representation with a physics-aware optimization strategy. We first learn a unified physics-geometry embedding that compactly captures shape and physical field data in a continuous latent space. Then, we introduce a two-stage strategy to perform physics-aware optimization. In the first stage, a gradient-guided diffusion sampler explores the global latent manifold. In the second stage, an objective-driven, topology-preserving refinement further sculpts each candidate toward the target objective. This enables 3DID to generate high-fidelity 3D geometries, outperforming existing methods in both solution quality and design versatility.", "AI": {"tldr": "论文提出了一个3D逆向设计框架，以高保真地生成3D几何形状，比现有方法更有效。", "motivation": "现有方法在3D设计中趋向于使用2D投影或者微调已有3D形状，这牺牲了体积细节并且限制了设计探索。该论文旨在解决这个问题。", "method": "论文中的方法包括学习统一的物理解几何嵌入和采用两阶段优化策略：一阶段为全局探索，二阶段为目标驱动的精细调整。", "result": "该论文提出了一个3D逆向设计框架，旨在通过结合连续的潜在表示和物理感知优化策略直接在3D设计空间中进行导航，以解决现有方法在3D设计中的局限性。首先，它学习一个统一的物理解几何嵌入，以紧凑地捕获形状和物理场数据。然后，采用两阶段策略进行物理感知优化：阶段一通过梯度引导的扩散采样器探索全局潜在流形，阶段二则通过目标驱动的、保持拓扑不变的细化进一步打磨候选体以达到目标目标。这种方法可以生成高保真的3D几何结构，无论在解决方案质量和设计灵活性方面，都优于现有方法。", "conclusion": "论文表明，通过其提出的3D逆向设计框架，可以有效地进行3D设计，生成高保真、多样化的3D几何结构。"}}
{"id": "2512.09148", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09148", "abs": "https://arxiv.org/abs/2512.09148", "authors": ["Shanghao Li", "Jinda Han", "Yibo Wang", "Yuanjie Zhu", "Zihe Song", "Langzhou He", "Kenan Kamel A Alghythee", "Philip S. Yu"], "title": "Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention Patterns and Semantic Alignment", "comment": null, "summary": "Graph-based Retrieval-Augmented Generation (GraphRAG) enhances Large Language Models (LLMs) by incorporating external knowledge from linearized subgraphs retrieved from knowledge graphs. However, LLMs struggle to interpret the relational and topological information in these inputs, resulting in hallucinations that are inconsistent with the retrieved knowledge. To analyze how LLMs attend to and retain structured knowledge during generation, we propose two lightweight interpretability metrics: Path Reliance Degree (PRD), which measures over-reliance on shortest-path triples, and Semantic Alignment Score (SAS), which assesses how well the model's internal representations align with the retrieved knowledge. Through empirical analysis on a knowledge-based QA task, we identify failure patterns associated with over-reliance on salient paths and weak semantic grounding, as indicated by high PRD and low SAS scores. We further develop a lightweight post-hoc hallucination detector, Graph Grounding and Alignment (GGA), which outperforms strong semantic and confidence-based baselines across AUC and F1. By grounding hallucination analysis in mechanistic interpretability, our work offers insights into how structural limitations in LLMs contribute to hallucinations, informing the design of more reliable GraphRAG systems in the future.", "AI": {"tldr": "研究提出新的解释性指标和检测器，以分析基于图的知识检索增强的大型语言模型如何处理结构化知识并检测其生成内容中的幻觉，从而提高未来GraphRAG系统的设计可靠性。", "motivation": "增强大型语言模型通过从知识图谱中检索到的线性化子图集成外部知识。然而，LLMs在解释这些输入中的关系和拓扑信息方面存在困难，导致生成的内容与检索到的知识不一致。", "method": "通过提出两种轻量级的解释性指标：路径依赖度（PRD）和语义对齐分数（SAS）来分析大型语言模型（LLMs）在生成过程中如何关注和保留结构化知识。PRD衡量对最短路径三元组的过度依赖，而SAS评估模型的内部表示与检索到的知识之间的对齐程度。", "result": "通过经验分析，识别出失败模式与对显著路径的过度依赖和语义基础薄弱有关，这些由高PRD和低SAS分数所指明。此外，还开发了一种轻量级的后验幻觉检测器，Graph Grounding and Alignment (GGA)，其在AUC和F1上优于强大的语义和置信度基线。", "conclusion": "通过将幻觉分析建立在机制解释的基础上，这项工作提供了关于LLMs的结构性限制是如何导致幻觉的见解，并为未来更可靠的GraphRAG系统的开发提供了指导。"}}
{"id": "2512.08989", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.08989", "abs": "https://arxiv.org/abs/2512.08989", "authors": ["Lu Huo", "Wenjian Huang", "Jianguo Zhang", "Min Xu", "Haimin Zhang"], "title": "Enhancing Knowledge Transfer in Hyperspectral Image Classification via Cross-scene Knowledge Integration", "comment": null, "summary": "Knowledge transfer has strong potential to improve hyperspectral image (HSI) classification, yet two inherent challenges fundamentally restrict effective cross-domain transfer: spectral variations caused by different sensors and semantic inconsistencies across heterogeneous scenes. Existing methods are limited by transfer settings that assume homogeneous domains or heterogeneous scenarios with only co-occurring categories. When label spaces do not overlap, they further rely on complete source-domain coverage and therefore overlook critical target-private information. To overcome these limitations and enable knowledge transfer in fully heterogeneous settings, we propose Cross-scene Knowledge Integration (CKI), a framework that explicitly incorporates target-private knowledge during transfer. CKI includes: (1) Alignment of Spectral Characteristics (ASC) to reduce spectral discrepancies through domain-agnostic projection; (2) Cross-scene Knowledge Sharing Preference (CKSP), which resolves semantic mismatch via a Source Similarity Mechanism (SSM); and (3) Complementary Information Integration (CII) to maximize the use of target-specific complementary cues. Extensive experiments verify that CKI achieves state-of-the-art performance with strong stability across diverse cross-scene HSI scenarios.", "AI": {"tldr": "提出了Cross-scene Knowledge Integration (CKI)框架，以克服跨场景高光谱图像分类中存在的光谱差异和语义不一致的问题，实验证明其达到了最先进的性能并具有很高的稳定性。", "motivation": "知识转移在高光谱图像分类中有强潜力，但两个内在挑战限制了有效的跨域转移：不同传感器引起的光谱变化和异构场景中的语义不一致。现有方法受同构域或只有共现类别异构场景转移设定的限制，当标签空间不重叠时，它们进一步依赖完整的源域覆盖，因而忽视了关键的目标特有的信息。", "method": "CKI框架包括：(1) 光谱特征对齐(ASC)，通过领域无关的投影减少光谱差异；(2) 跨场景知识共享偏好(CKSP)，通过源相似机制(SSM)解决语义不匹配问题；(3) 互补信息整合(CII)，最大化利用目标特定的互补线索。", "result": "广泛的实验验证了CKI在不同的跨场景高光谱图像场景中达到了最先进的性能，并且具有很强的稳定性。", "conclusion": "CKI框架通过显式地整合目标特有的知识来克服现有限制，实现了跨场景的知识转移。"}}
{"id": "2512.09149", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09149", "abs": "https://arxiv.org/abs/2512.09149", "authors": ["Anton Vasiliuk", "Irina Abdullaeva", "Polina Druzhinina", "Anton Razzhigaev", "Andrey Kuznetsov"], "title": "MindShift: Analyzing Language Models' Reactions to Psychological Prompts", "comment": null, "summary": "Large language models (LLMs) hold the potential to absorb and reflect personality traits and attitudes specified by users. In our study, we investigated this potential using robust psychometric measures. We adapted the most studied test in psychological literature, namely Minnesota Multiphasic Personality Inventory (MMPI) and examined LLMs' behavior to identify traits. To asses the sensitivity of LLMs' prompts and psychological biases we created personality-oriented prompts, crafting a detailed set of personas that vary in trait intensity. This enables us to measure how well LLMs follow these roles. Our study introduces MindShift, a benchmark for evaluating LLMs' psychological adaptability. The results highlight a consistent improvement in LLMs' role perception, attributed to advancements in training datasets and alignment techniques. Additionally, we observe significant differences in responses to psychometric assessments across different model types and families, suggesting variability in their ability to emulate human-like personality traits. MindShift prompts and code for LLM evaluation will be publicly available.", "AI": {"tldr": "本研究通过MMPI量表评估LLMs的个性特征表现，创建了MindShift基准来衡量其心理适应性。结果显示，模型具有模仿人类个性特征的能力随训练数据改进而提高，且不同型号间存在差异。", "motivation": "研究动机在于探讨LLMs吸收和反映用户指定的个性特征和态度的潜力。我们使用了在心理学文献中最广泛研究的MMPI量表，以调查LLMs在这方面的行为。", "method": "本研究通过使用MMPI量表，适应大型语言模型（LLMs）的行为来识别个性特征。我们构建了一套具有不同强度特征的角色提示，以评估LLMs对个性化提示的敏感性和心理偏见。这使得我们能够衡量LLMs遵循这些角色的程度。此外，我们介绍了MindShift，这是一个用于评估LLMs心理适应性的基准。", "result": "结果表明，随着训练数据集和对齐技术的改进，LLMs的角色感知能力得到了一致的改善。我们还发现，不同的LLM型号在针对心理测验的响应上有显著差异，这暗示了它们在模仿人类个性特征方面的能力存在可变性。", "conclusion": "研究发现，LLMs在角色感知方面有所提高，这归因于训练数据集和对齐技术的进步。此外，不同类型的模型在心理评估回应上存在显著差异，表明它们在模拟人类个性方面的能力有所不同。MindShift的提示和代码将公开提供。"}}
{"id": "2512.08991", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.08991", "abs": "https://arxiv.org/abs/2512.08991", "authors": ["Yuang Geng", "Zhuoyang Zhou", "Zhongzheng Zhang", "Siyuan Pan", "Hoang-Dung Tran", "Ivan Ruchkin"], "title": "Deterministic World Models for Verification of Closed-loop Vision-based Systems", "comment": "22 pages, 10 figures. Submitted to FM 2026", "summary": "Verifying closed-loop vision-based control systems remains a fundamental challenge due to the high dimensionality of images and the difficulty of modeling visual environments. While generative models are increasingly used as camera surrogates in verification, their reliance on stochastic latent variables introduces unnecessary overapproximation error. To address this bottleneck, we propose a Deterministic World Model (DWM) that maps system states directly to generative images, effectively eliminating uninterpretable latent variables to ensure precise input bounds. The DWM is trained with a dual-objective loss function that combines pixel-level reconstruction accuracy with a control difference loss to maintain behavioral consistency with the real system. We integrate DWM into a verification pipeline utilizing Star-based reachability analysis (StarV) and employ conformal prediction to derive rigorous statistical bounds on the trajectory deviation between the world model and the actual vision-based system. Experiments on standard benchmarks show that our approach yields significantly tighter reachable sets and better verification performance than a latent-variable baseline.", "AI": {"tldr": "本文提出了一种确定性世界模型 (DWM)，它将系统状态直接映射到生成图像，消除了不可解释的潜在变量。通过结合像素级重建和控制差异损失的双重目标损失函数训练 DWM，并集成到使用基于 Star 的可达性分析的验证管道中。", "motivation": "视觉环境建模的难度和高维图像使得基于视觉的闭环控制系统验证成为一个基本挑战。现有的基于潜在变量的生成模型在验证过程中引入了不必要的过度近似误差。", "method": "我们提出了一种确定性世界模型(DWM)，该模型直接将系统状态映射到生成图像，从而消除了不可解释的潜在变量，确保精确的输入边界。DWM 通过结合像素级重建准确性和控制差异损失的双重目标损失函数进行训练，以保持与真实系统的控制一致性。", "result": "实验结果表明，与潜在变量基线相比，本方法在标准基准上产生了更精确的可达集，并且具有更好的验证性能。", "conclusion": "采用确定性世界模型 (DWM) 进行基于视觉的闭环控制系统验证，可以显著提高性能和精确度，减少了由潜在变量引起的过度近似误差，适用于现有的验证方法不能有效解决的复杂视觉系统的验证问题。"}}
{"id": "2512.09212", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09212", "abs": "https://arxiv.org/abs/2512.09212", "authors": ["Zixuan Liu", "Siavash H. Khajavi", "Guangkai Jiang", "Xinru Liu"], "title": "Targeting Misalignment: A Conflict-Aware Framework for Reward-Model-based LLM Alignment", "comment": null, "summary": "Reward-model-based fine-tuning is a central paradigm in aligning Large Language Models with human preferences. However, such approaches critically rely on the assumption that proxy reward models accurately reflect intended supervision, a condition often violated due to annotation noise, bias, or limited coverage. This misalignment can lead to undesirable behaviors, where models optimize for flawed signals rather than true human values. In this paper, we investigate a novel framework to identify and mitigate such misalignment by treating the fine-tuning process as a form of knowledge integration. We focus on detecting instances of proxy-policy conflicts, cases where the base model strongly disagrees with the proxy. We argue that such conflicts often signify areas of shared ignorance, where neither the policy nor the reward model possesses sufficient knowledge, making them especially susceptible to misalignment. To this end, we propose two complementary metrics for identifying these conflicts: a localized Proxy-Policy Alignment Conflict Score (PACS) and a global Kendall-Tau Distance measure. Building on this insight, we design an algorithm named Selective Human-in-the-loop Feedback via Conflict-Aware Sampling (SHF-CAS) that targets high-conflict QA pairs for additional feedback, refining both the reward model and policy efficiently. Experiments on two alignment tasks demonstrate that our approach enhances general alignment performance, even when trained with a biased proxy reward. Our work provides a new lens for interpreting alignment failures and offers a principled pathway for targeted refinement in LLM training.", "AI": {"tldr": "本文提出了一种通过检测代理政策冲突提升大型语言模型对齐性能的新框架和方法。", "motivation": "研究的动机是解决现有基于奖励模型的调整方法在大型语言模型对齐过程中存在的问题，这些问题源于奖励模型对人类偏好的近似不准确，可能导致模型产生与人类真实价值观不符的行为。", "method": "本文提出了一个检测和缓解代理政策冲突的新框架，通过将微调过程视为知识整合的一种形式，引入了代理-策略对齐冲突分数（PACS）和Kendall-Tau距离两种度量方法，设计了一个名为SHF-CAS算法来增强人类反馈的效率和针对性。", "result": "实验在两个对齐任务上表明，本文提出的方法即使在使用有偏见的代理奖励模型的情况下，也能提升整体对齐的表现。", "conclusion": "本文提供了一个新的角度来理解和解决对齐失败的问题，并为大型语言模型训练中的针对性改进提供了一个原则上的途径。"}}
{"id": "2512.08996", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.08996", "abs": "https://arxiv.org/abs/2512.08996", "authors": ["Riqiang Gao", "Simon Arberet", "Martin Kraus", "Han Liu", "Wilko FAR Verbakel", "Dorin Comaniciu", "Florin-Cristian Ghesu", "Ali Kamen"], "title": "Demo: Generative AI helps Radiotherapy Planning with User Preference", "comment": "Best paper in GenAI4Health at NeurIPS 2025", "summary": "Radiotherapy planning is a highly complex process that often varies significantly across institutions and individual planners. Most existing deep learning approaches for 3D dose prediction rely on reference plans as ground truth during training, which can inadvertently bias models toward specific planning styles or institutional preferences. In this study, we introduce a novel generative model that predicts 3D dose distributions based solely on user-defined preference flavors. These customizable preferences enable planners to prioritize specific trade-offs between organs-at-risk (OARs) and planning target volumes (PTVs), offering greater flexibility and personalization. Designed for seamless integration with clinical treatment planning systems, our approach assists users in generating high-quality plans efficiently. Comparative evaluations demonstrate that our method can surpasses the Varian RapidPlan model in both adaptability and plan quality in some scenarios.", "AI": {"tldr": "研究提出了一种基于用户偏好预测3D剂量分布的新生成模型，该模型能够超越现有方法，在适应性和计划质量方面有所提升。", "motivation": "现有的深度学习方法在3D剂量预测中往往依赖参考计划作为训练的地面实况，这可能会无意间使模型偏向特定的规划方式或机构偏好，因此本研究旨在克服这一局限。", "method": "本研究提出了一种新的生成模型，能够根据用户定义的偏好口味预测3D剂量分布，从而在器官风险区域(OARs)和治疗目标体积(PTVs)之间实现特定的权衡优先级。", "result": "评估表明，本方法在某些情况下可以超过Varian RapidPlan模型，在适应性和计划质量方面表现出色。", "conclusion": "本研究提出的模型为临床治疗计划系统的集成提供了更高效地生成高质量计划的能力。"}}
{"id": "2512.09222", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09222", "abs": "https://arxiv.org/abs/2512.09222", "authors": ["Vishwas Hegde", "Vindhya Shigehalli"], "title": "CORE: A Conceptual Reasoning Layer for Large Language Models", "comment": "Independent system-level architectural proposal with accompanying proof-of-concept", "summary": "Large language models handle single-turn generation well, but multi-turn interactions still require the model to reconstruct user intent and task state from an expanding token history because internal representations do not persist across turns. This token-first paradigm leads to drift, inconsistent reasoning modes, and growing prompts as conversations deepen. We propose CORE, a concept-first interaction layer that improves multi-turn stability without modifying model weights. CORE combines a small library of universal cognitive operators with a persistent Local Concept - a compact semantic state capturing the task, constraints, preferences, and intermediate results. Each model call receives only this concept state, the user's latest instruction, and the selected operator, eliminating the need to replay full history. A preliminary prototype simulating CORE's behavior shows about 42% reduction in cumulative prompt tokens, though this number reflects prototype conditions and should not be interpreted as a real-world performance estimate. CORE offers a model-agnostic mechanism that separates conceptual reasoning from language generation, suggesting a scalable direction for more stable multi-turn systems.", "AI": {"tldr": "CORE, a concept-first interaction layer, improves multi-turn stability in large language models by using a persistent Local Concept instead of a token history.", "motivation": "Current large language models face challenges in multi-turn interactions, including drift and inconsistent reasoning due to the expanding token history. CORE aims to address these issues by separating conceptual reasoning from language generation.", "method": "CORE uses a small library of universal cognitive operators combined with a persistent Local Concept to capture the task, constraints, preferences, and intermediate results. The model only receives the concept state, the latest user instruction, and the selected operator for each call.", "result": "A preliminary prototype of CORE showed a 42% reduction in cumulative prompt tokens, indicating potential improvements in multi-turn stability.", "conclusion": "CORE provides a model-agnostic mechanism for improving multi-turn stability in large language models, marking a potentially scalable solution for more stable multi-turn systems without modifying model weights."}}
{"id": "2512.08999", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.08999", "abs": "https://arxiv.org/abs/2512.08999", "authors": ["Jie Wen", "Chenhe Du", "Xiao Wang", "Yuyao Zhang"], "title": "Diffusion Model Regularized Implicit Neural Representation for CT Metal Artifact Reduction", "comment": null, "summary": "Computed tomography (CT) images are often severely corrupted by artifacts in the presence of metals. Existing supervised metal artifact reduction (MAR) approaches suffer from performance instability on known data due to their reliance on limited paired metal-clean data, which limits their clinical applicability. Moreover, existing unsupervised methods face two main challenges: 1) the CT physical geometry is not effectively incorporated into the MAR process to ensure data fidelity; 2) traditional heuristics regularization terms cannot fully capture the abundant prior knowledge available. To overcome these shortcomings, we propose diffusion model regularized implicit neural representation framework for MAR. The implicit neural representation integrates physical constraints and imposes data fidelity, while the pre-trained diffusion model provides prior knowledge to regularize the solution. Experimental results on both simulated and clinical data demonstrate the effectiveness and generalization ability of our method, highlighting its potential to be applied to clinical settings.", "AI": {"tldr": "提出了一种基于扩散模型正则化的隐式神经表示框架来减少CT图像中的金属伪影，该框架结合了物理约束和先验知识，在模拟和临床数据上表现出良好的效果和泛化能力。", "motivation": "现有的金属伪影减少方法在性能稳定性、CT物理几何的有效整合以及对先验知识的充分利用方面存在局限，因此提出了一种新的方法来克服这些局限。", "method": "利用隐式神经表示框架结合物理约束保证数据保真性，并通过预训练的扩散模型提供先验知识正则化解决方案。", "result": "实验结果显示，所提出的方法在模拟和临床CT数据上均具有有效性，并显示出潜在的应用前景。", "conclusion": "该方法通过结合物理约束和先验知识，有效提高了CT图像金属伪影减少的性能，有望应用于临床环境。"}}
{"id": "2512.09238", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09238", "abs": "https://arxiv.org/abs/2512.09238", "authors": ["Zeng You", "Yaofo Chen", "Shuhai Zhang", "Zhijie Qiu", "Tingyu Wu", "Yingjian Li", "Yaowei Wang", "Mingkui Tan"], "title": "Training-free Context-adaptive Attention for Efficient Long Context Modeling", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism, which enables modeling of long-range dependencies. However, the quadratic complexity of self-attention with respect to sequence length poses significant computational and memory challenges, especially as sequence length extends to extremes. While various sparse attention and KV cache compression methods have been proposed to improve efficiency, they often suffer from limitations such as reliance on fixed patterns, inability to handle both prefilling and decoding stages, or the requirement for additional training. In this paper, we propose Training-free Context-adaptive Attention (TCA-Attention), a training-free sparse attention mechanism that selectively attends to only the informative tokens for efficient long-context inference. Our method consists of two lightweight phases: i) an offline calibration phase that determines head-specific sparsity budgets via a single forward pass, and ii) an online token selection phase that adaptively retains core context tokens using a lightweight redundancy metric. TCA-Attention provides a unified solution that accelerates both prefilling and decoding while reducing KV cache memory footprint, without requiring parameter updates or architectural changes. Theoretical analysis shows that our approach maintains bounded approximation error. Extensive experiments demonstrate that TCA-Attention achieves a 2.8$\\times$ speedup and reduces KV cache by 61% at 128K context length while maintaining performance comparable to full attention across various benchmarks, offering a practical plug-and-play solution for efficient long-context inference.", "AI": {"tldr": "Proposes TCA-Attention to accelerate long-context inference and reduce memory usage, maintaining performance without additional training or architecture changes.", "motivation": "To address the quadratic complexity of self-attention and limitations of existing sparse attention methods, such as reliance on fixed patterns and other restrictions.", "method": "Training-free Context-adaptive Attention (TCA-Attention), a training-free sparse attention mechanism with two phases: offline calibration and online token selection.", "result": "Achieves 2.8 times speedup and reduces KV cache by 61% at 128K context length while keeping performance comparable to full attention.", "conclusion": "TCA-Attention is a practical, plug-and-play solution for efficient long-context inference without parameter updates or architectural changes."}}
{"id": "2512.09001", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09001", "abs": "https://arxiv.org/abs/2512.09001", "authors": ["Yuehua Hu", "Jiyeong Kong", "Dong-yeol Shin", "Jaekyun Kim", "Kyung-Tae Kang"], "title": "A Physics-Constrained, Design-Driven Methodology for Defect Dataset Generation in Optical Lithography", "comment": null, "summary": "The efficacy of Artificial Intelligence (AI) in micro/nano manufacturing is fundamentally constrained by the scarcity of high-quality and physically grounded training data for defect inspection. Lithography defect data from semiconductor industry are rarely accessible for research use, resulting in a shortage of publicly available datasets. To address this bottleneck in lithography, this study proposes a novel methodology for generating large-scale, physically valid defect datasets with pixel-level annotations. The framework begins with the ab initio synthesis of defect layouts using controllable, physics-constrained mathematical morphology operations (erosion and dilation) applied to the original design-level layout. These synthesized layouts, together with their defect-free counterparts, are fabricated into physical samples via high-fidelity digital micromirror device (DMD)-based lithography. Optical micrographs of the synthesized defect samples and their defect-free references are then compared to create consistent defect delineation annotations. Using this methodology, we constructed a comprehensive dataset of 3,530 Optical micrographs containing 13,365 annotated defect instances including four classes: bridge, burr, pinch, and contamination. Each defect instance is annotated with a pixel-accurate segmentation mask, preserving full contour and geometry. The segmentation-based Mask R-CNN achieves AP@0.5 of 0.980, 0.965, and 0.971, compared with 0.740, 0.719, and 0.717 for Faster R-CNN on bridge, burr, and pinch classes, representing a mean AP@0.5 improvement of approximately 34%. For the contamination class, Mask R-CNN achieves an AP@0.5 roughly 42% higher than Faster R-CNN. These consistent gains demonstrate that our proposed methodology to generate defect datasets with pixel-level annotations is feasible for robust AI-based Measurement/Inspection (MI) in semiconductor fabrication.", "AI": {"tldr": "研究提出了一种合成半导体缺陷样本的新型方法，用于生成AI训练所需的数据集，该方法在缺陷检测上提升了显著的准确度。", "motivation": "研究的目标是解决AI在半导体制造缺陷检测方面因高质量训练数据稀缺而导致的效率低下问题。方法是生成带有物理基础的、大规模的缺陷数据集。", "method": "该研究提出了一种新的方法来生成大规模、物理有效的缺陷数据集，并带有像素级的注释。首先通过数学形态学操作（侵蚀和膨胀）在电路布局级别上合成缺陷布局，然后通过高保真度的数字微镜设备（DMD）光刻技术将这些布局制作成物理样本，并与无缺陷样本进行比较，创建一致的缺陷标注。", "result": "使用这种方法，研究团队构建了一个包含3,530张光学显微图像的数据集，其中包括13,365个标注缺陷实例，分为四个类别：桥连、毛刺、缩颈和污染。使用基于分割的Mask R-CNN，该数据集在四个类别上的AP@0.5分别达到0.980, 0.965, 和 0.971，相较于Faster R-CNN分别提高了34%和42%。", "conclusion": "该研究证明了通过提出的方法生成带有像素级标注的缺陷数据集在网络模型上的应用是成功的，为基于AI的半导体制造中的检测与测量提供了有力支持。"}}
{"id": "2512.09292", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09292", "abs": "https://arxiv.org/abs/2512.09292", "authors": ["Kevin Stowe", "Svetlana Afanaseva", "Rodolfo Raimundo", "Yitao Sun", "Kailash Patil"], "title": "Identifying Bias in Machine-generated Text Detection", "comment": "13 pages, 2 figures, 7 tables", "summary": "The meteoric rise in text generation capability has been accompanied by parallel growth in interest in machine-generated text detection: the capability to identify whether a given text was generated using a model or written by a person. While detection models show strong performance, they have the capacity to cause significant negative impacts. We explore potential biases in English machine-generated text detection systems. We curate a dataset of student essays and assess 16 different detection systems for bias across four attributes: gender, race/ethnicity, English-language learner (ELL) status, and economic status. We evaluate these attributes using regression-based models to determine the significance and power of the effects, as well as performing subgroup analysis. We find that while biases are generally inconsistent across systems, there are several key issues: several models tend to classify disadvantaged groups as machine-generated, ELL essays are more likely to be classified as machine-generated, economically disadvantaged students' essays are less likely to be classified as machine-generated, and non-White ELL essays are disproportionately classified as machine-generated relative to their White counterparts. Finally, we perform human annotation and find that while humans perform generally poorly at the detection task, they show no significant biases on the studied attributes.", "AI": {"tldr": "研究发现，机器生成文本检测系统存在多种偏见，特别是对经济不利群体、英语学习者等，这些系统更可能误将他们的文本判定为机器生成。人类判别则表现较差但无明显偏见。", "motivation": "随着文本生成技术的进步，机器生成文本检测技术也引起了广泛关注。本研究旨在探索英语文本生成检测系统的潜在偏见，以提高这些系统的公正性。", "method": "研究者整理了一套学生作文数据集，并评估了16种不同检测系统在这四个属性上的偏见：性别、种族/民族、英语学习者身份和经济状况。他们通过回归模型和子组分析来评估这些属性的影响。", "result": "研究结果发现，虽然偏见在不同系统间不一致，但存在几个关键问题：一些模型倾向于将劣势群体的文本判定为机器生成；英语学习者的作文更可能被检测为机器生成；经济条件较差学生的作文则更少被判定为机器生成；而非白人英语学习者则被更可能判定为机器生成。", "conclusion": "人类判别实验表明，尽管在检测任务上表现不佳，但人类判别者并未显示出对研究属性的显著偏见，这些发现增加了对现行自动检测系统的批评，提出了消除偏见的必要性。"}}
{"id": "2512.09005", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.09005", "abs": "https://arxiv.org/abs/2512.09005", "authors": ["Lownish Rai Sookha", "Nikhil Pakhale", "Mudasir Ganaie", "Abhinav Dhall"], "title": "A Survey of Body and Face Motion: Datasets, Performance Evaluation Metrics and Generative Techniques", "comment": null, "summary": "Body and face motion play an integral role in communication. They convey crucial information on the participants. Advances in generative modeling and multi-modal learning have enabled motion generation from signals such as speech, conversational context and visual cues. However, generating expressive and coherent face and body dynamics remains challenging due to the complex interplay of verbal / non-verbal cues and individual personality traits. This survey reviews body and face motion generation, covering core concepts, representations techniques, generative approaches, datasets and evaluation metrics. We highlight future directions to enhance the realism, coherence and expressiveness of avatars in dyadic settings. To the best of our knowledge, this work is the first comprehensive review to cover both body and face motion. Detailed resources are listed on https://lownish23csz0010.github.io/mogen/.", "AI": {"tldr": "这篇综述探讨了身体和面部运动生成的各种方面，旨在提高虚拟角色在二元场景中的表现力、一致性和真实性。", "motivation": "由于口头/非口头线索的复杂交互和个体性格特征，生成具有表现力和一致性的面部和身体动态仍然具有挑战性。", "method": "该论文综述了身体和面部运动生成的关键概念、表示技术、生成方法、数据集和评估指标。", "result": "首次全面综述了身体和面部运动，提供了包括核心概念、表示技术、生成方法、数据集及评估指标的综合分析资源。", "conclusion": "论文指出，未来的方向在于增强二元场景中角色的真实性、一致性和表现力。"}}
{"id": "2512.09386", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09386", "abs": "https://arxiv.org/abs/2512.09386", "authors": ["Peter Baile Chen", "Weiyue Li", "Dan Roth", "Michael Cafarella", "Samuel Madden", "Jacob Andreas"], "title": "CONCUR: A Framework for Continual Constrained and Unconstrained Routing", "comment": null, "summary": "AI tasks differ in complexity and are best addressed with different computation strategies (e.g., combinations of models and decoding methods). Hence, an effective routing system that maps tasks to the appropriate strategies is crucial. Most prior methods build the routing framework by training a single model across all strategies, which demands full retraining whenever new strategies appear and leads to high overhead. Attempts at such continual routing, however, often face difficulties with generalization. Prior models also typically use a single input representation, limiting their ability to capture the full complexity of the routing problem and leading to sub-optimal routing decisions. To address these gaps, we propose CONCUR, a continual routing framework that supports both constrained and unconstrained routing (i.e., routing with or without a budget). Our modular design trains a separate predictor model for each strategy, enabling seamless incorporation of new strategies with low additional training cost. Our predictors also leverage multiple representations of both tasks and computation strategies to better capture overall problem complexity. Experiments on both in-distribution and out-of-distribution, knowledge- and reasoning-intensive tasks show that our method outperforms the best single strategy and strong existing routing techniques with higher end-to-end accuracy and lower inference cost in both continual and non-continual settings, while also reducing training cost in the continual setting.", "AI": {"tldr": "提出 CONCUR：一个可处理新策略情况下无需完全重新训练，有多种输入表示形式以便于捕捉任务复杂度的持续路由框架，实验表明其准确度和成本控制优于其它方法。", "motivation": "解决现有路由框架在面对新策略时需完全重新训练和泛化难题的挑战，同时克服单一输入表示的局限性，以避免次优路由决策。", "method": "CONCUR, 一个持续路由框架，支持约束和非约束路由，并为每个策略训练独立的预测器，从而实现新策略的无缝集成和低成本额外训练。此外，该方法利用任务和计算策略的多种表示形式来更好地捕捉整体问题的复杂性。", "result": "实验表明，CONCUR 方法在分布内和分布外的任务上，无论是知识密集型还是推理密集型任务中，都表现出优于单一最佳策略和其它现有路由技术的端到端准确性和更低的推理成本，并在持续设置下减少了训练成本。", "conclusion": "CONCUR 证明了其在处理多样任务时的有效性和成本效益，特别是在需要考虑预算约束的情境下，同时也证明了它在持续学习场景中的优越性。"}}
{"id": "2512.09010", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09010", "abs": "https://arxiv.org/abs/2512.09010", "authors": ["Dehua Zheng", "Mouxiao Huang", "Borui Jiang", "Hailin Hu", "Xinghao Chen"], "title": "Towards Lossless Ultimate Vision Token Compression for VLMs", "comment": null, "summary": "Visual language models encounter challenges in computational efficiency and latency, primarily due to the substantial redundancy in the token representations of high-resolution images and videos. Current attention/similarity-based compression algorithms suffer from either position bias or class imbalance, leading to significant accuracy degradation. They also fail to generalize to shallow LLM layers, which exhibit weaker cross-modal interactions. To address this, we extend token compression to the visual encoder through an effective iterative merging scheme that is orthogonal in spatial axes to accelerate the computation across the entire VLM. Furthermoer, we integrate a spectrum pruning unit into LLM through an attention/similarity-free low-pass filter, which gradually prunes redundant visual tokens and is fully compatible to modern FlashAttention. On this basis, we propose Lossless Ultimate Vision tokens Compression (LUVC) framework. LUVC systematically compresses visual tokens until complete elimination at the final layer of LLM, so that the high-dimensional visual features are gradually fused into the multimodal queries. The experiments show that LUVC achieves a 2 speedup inference in language model with negligible accuracy degradation, and the training-free characteristic enables immediate deployment across multiple VLMs.", "AI": {"tldr": "为了改善视觉语言模型中的计算效率和延迟问题，提出了一种名为LUVC（无损终极视觉标记压缩）的框架，该框架能够系统性地压缩视觉标记直到在LLM的最后一层完全消除。", "motivation": "由于高分辨率图像和视频的标记表示存在大量冗余，视觉语言模型面临着计算效率和延迟的挑战。当前的注意力/相似性压缩算法存在位置偏差或类别不平衡的问题，导致显著的准确率下降。", "method": "通过有效的迭代合并方案和频谱剪枝单元来改进视觉语言模型的计算效率和延迟问题。迭代合并方案在空间轴上相互独立，以加速整个VLM的计算。频谱剪枝单元通过类似低通滤波器的方法逐步剪枝冗余的视觉标记，并兼容现代FlashAttention技术。", "result": "实验表明，LUVC在语言模型中实现了2倍加速推理，并且几乎没有准确率下降。此外，无需训练的特点使得LUVC可以立即部署到多个VLM中。", "conclusion": "LUVC框架通过逐步融合多模态查询的高维视觉特征来系统性压缩视觉标记，从而解决了视觉语言模型中的计算效率和延迟问题，同时具有无需训练即可部署的优势。"}}
{"id": "2512.09394", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09394", "abs": "https://arxiv.org/abs/2512.09394", "authors": ["Julie Kallini", "Christopher Potts"], "title": "Language models as tools for investigating the distinction between possible and impossible natural languages", "comment": null, "summary": "We argue that language models (LMs) have strong potential as investigative tools for probing the distinction between possible and impossible natural languages and thus uncovering the inductive biases that support human language learning. We outline a phased research program in which LM architectures are iteratively refined to better discriminate between possible and impossible languages, supporting linking hypotheses to human cognition.", "AI": {"tldr": "研究语言模型作为区分可能和不可能语言的调查工具，揭示语言学习的归纳偏见。", "motivation": "探讨语言模型作为探究可能与不可能的自然语言之间区别的调查工具的潜力，揭示支持人类语言学习的归纳偏见。", "method": "通过逐步改进语言模型的架构，以更好地区分可能和不可能的语言，从而支持将假设与人类认知联系起来的研究计划。", "result": "未提供具体结果。", "conclusion": "语言模型具有作为调查工具，用于探索可能与不可能的自然语言之间区别的强大潜力。逐步改进的LM架构可以更好地区分可能与不可能的语言，并将假设与人类认知联系起来。"}}
{"id": "2512.09011", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09011", "abs": "https://arxiv.org/abs/2512.09011", "authors": ["Nzakiese Mbongo", "Ngombo Armando"], "title": "An Approach for Detection of Entities in Dynamic Media Contents", "comment": "12 pages, 8 figures", "summary": "The notion of learning underlies almost every evolution of Intelligent Agents. In this paper, we present an approach for searching and detecting a given entity in a video sequence. Specifically, we study how the deep learning technique by artificial neuralnetworks allows us to detect a character in a video sequence. The technique of detecting a character in a video is a complex field of study, considering the multitude of objects present in the data under analysis. From the results obtained, we highlight the following, compared to state of the art: In our approach, within the field of Computer Vision, the structuring of supervised learning algorithms allowed us to achieve several successes from simple characteristics of the target character. Our results demonstrate that is new approach allows us to locate, in an efficient way, wanted individuals from a private or public image base. For the case of Angola, the classifier we propose opens the possibility of reinforcing the national security system based on the database of target individuals (disappeared, criminals, etc.) and the video sequences of the Integrated Public Security Centre (CISP).", "AI": {"tldr": "该论文提出了一种利用深度学习技术在视频序列中搜索和检测特定实体的方法，在计算机视觉领域取得了显著成果，特别是在安哥拉国家安防系统中有潜在应用价值。", "motivation": "研究如何使用深度学习技术，特别是人工神经网络，来检测视频序列中的特定目标。", "method": "基于监督学习算法的结构，开发用于视频中人物检测的技术。", "result": "与现有方法相比，该方法能在公共或私人图像数据库中高效定位所需个体。", "conclusion": "提出的分类器可以在安哥拉的安防系统中用于加强视频监控与目标个体数据库的匹配。"}}
{"id": "2512.09434", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09434", "abs": "https://arxiv.org/abs/2512.09434", "authors": ["Sebastian Nagl", "Mohamed Elganayni", "Melanie Pospisil", "Matthias Grabmair"], "title": "CourtPressGER: A German Court Decision to Press Release Summarization Dataset", "comment": "Preprint - This contribution was accepted at JURIX AI4A2J Workshop 2025", "summary": "Official court press releases from Germany's highest courts present and explain judicial rulings to the public, as well as to expert audiences. Prior NLP efforts emphasize technical headnotes, ignoring citizen-oriented communication needs. We introduce CourtPressGER, a 6.4k dataset of triples: rulings, human-drafted press releases, and synthetic prompts for LLMs to generate comparable releases. This benchmark trains and evaluates LLMs in generating accurate, readable summaries from long judicial texts. We benchmark small and large LLMs using reference-based metrics, factual-consistency checks, LLM-as-judge, and expert ranking. Large LLMs produce high-quality drafts with minimal hierarchical performance loss; smaller models require hierarchical setups for long judgments. Initial benchmarks show varying model performance, with human-drafted releases ranking highest.", "AI": {"tldr": "研究人员引入了名为CourtPressGER的数据集，用于训练和评估LLMs生成准确可读的司法裁决摘要的能力，发现大型LLM能生成高质量草稿，小型模型则表现较差。", "motivation": "法院的官方新闻稿旨在向公众和专家解释司法裁决，而以前的NLP工作集中在技术性注释上，忽略了面向公民的沟通需求。", "method": "提出并使用了一个名为CourtPressGER的数据集，该数据集包含6.4k个三元组：判决、人类撰写的新闻稿以及用于生成类似新闻稿的合成提示。此基准用于训练和评估LLMs，使其能够从长文本司法文件中生成准确且可读的摘要。", "result": "使用参考基准、事实一致性检查、LLM作为法官以及专家排名等方法对小模型和大模型进行了基准测试。大型模型可以生成高质量的草稿，仅有轻微的层次性能损失，而小一些的模型则需要层次结构来处理长裁决。初步测试表明不同模型的表现有差异，而人力撰写的新闻稿表现最高。", "conclusion": "大型LLMs在生成法律新闻稿时展现出很高的质量，中小型模型性能相对较低；未来的研究应继续探索提高LLMs在法律文本摘要任务中的表现。"}}
{"id": "2512.09016", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09016", "abs": "https://arxiv.org/abs/2512.09016", "authors": ["Haiqian Han", "Lingdong Kong", "Jianing Li", "Ao Liang", "Chengtao Zhu", "Jiacheng Lyu", "Lai Xing Ng", "Xiangyang Ji", "Wei Tsang Ooi", "Benoit R. Cottereau"], "title": "Learning to Remove Lens Flare in Event Camera", "comment": "Preprint; 29 pages, 14 figures, 4 tables; Project Page at https://e-flare.github.io/", "summary": "Event cameras have the potential to revolutionize vision systems with their high temporal resolution and dynamic range, yet they remain susceptible to lens flare, a fundamental optical artifact that causes severe degradation. In event streams, this optical artifact forms a complex, spatio-temporal distortion that has been largely overlooked. We present E-Deflare, the first systematic framework for removing lens flare from event camera data. We first establish the theoretical foundation by deriving a physics-grounded forward model of the non-linear suppression mechanism. This insight enables the creation of the E-Deflare Benchmark, a comprehensive resource featuring a large-scale simulated training set, E-Flare-2.7K, and the first-ever paired real-world test set, E-Flare-R, captured by our novel optical system. Empowered by this benchmark, we design E-DeflareNet, which achieves state-of-the-art restoration performance. Extensive experiments validate our approach and demonstrate clear benefits for downstream tasks. Code and datasets are publicly available.", "AI": {"tldr": "本文提出了E-Deflare框架，用于移除事件相机数据中的镜头眩光，包括理论模型、基准数据集和E-DeflareNet深度学习模型，实现了先进的修复效果。", "motivation": "本文旨在解决事件相机数据中镜头眩光这一被忽视的问题，提出E-Deflare，第一个系统性的框架，用于从事件相机数据中移除镜头眩光。", "method": "我们首先建立了理论基础，推导了基于物理的非线性抑制机制的前向模型。这为我们的E-Deflare基准提供了基础，该基准包含大规模模拟训练集E-Flare-2.7K和首个纯真世界的测试集E-Flare-R。基于此基准，我们设计了E-DeflareNet，实现了最先进的修复性能。", "result": "实验验证了我们的方法，并展示了对下游任务的明显改进。", "conclusion": "我们的研究展示了E-DeflareNet在修复事件相机数据中的镜头眩光方面的卓越性能，并通过公开代码和数据集促进了进一步的研究。"}}
{"id": "2512.09440", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09440", "abs": "https://arxiv.org/abs/2512.09440", "authors": ["Qingyuan Zhang", "Yuxi Wang", "Cancan Hua", "Yulin Huang", "Ning Lyu"], "title": "Knowledge-Augmented Large Language Model Agents for Explainable Financial Decision-Making", "comment": null, "summary": "This study investigates an explainable reasoning method for financial decision-making based on knowledge-enhanced large language model agents. To address the limitations of traditional financial decision methods that rely on parameterized knowledge, lack factual consistency, and miss reasoning chains, an integrated framework is proposed that combines external knowledge retrieval, semantic representation, and reasoning generation. The method first encodes financial texts and structured data to obtain semantic representations, and then retrieves task-related information from external knowledge bases using similarity computation. Internal representations and external knowledge are combined through weighted fusion, which ensures fluency while improving factual accuracy and completeness of generated content. In the reasoning stage, a multi-head attention mechanism is introduced to construct logical chains, allowing the model to present transparent causal relationships and traceability during generation. Finally, the model jointly optimizes task objectives and explanation consistency objectives, which enhances predictive performance and reasoning interpretability. Experiments on financial text processing and decision tasks show that the method outperforms baseline approaches in accuracy, text generation quality, and factual support, verifying the effectiveness of knowledge enhancement and explainable reasoning. Overall, the proposed approach overcomes the limitations of traditional models in semantic coverage and reasoning transparency, and demonstrates strong practical value in complex financial scenarios.", "AI": {"tldr": "A study introduces an explainable reasoning method for financial decision-making, utilizing knowledge-enhanced large language models to improve factual accuracy and reasoning transparency, with experiments validating its effectiveness and practicality.", "motivation": "The motivation is to address the limitations of traditional financial decision methods, such as reliance on parameterized knowledge, lack of factual consistency, and absence of reasoning chains.", "method": "This method integrates external knowledge retrieval, semantic representation, and reasoning generation. It encodes financial texts and structured data, retrieves task-related information from external knowledge bases, and uses a multi-head attention mechanism in the reasoning stage to construct logical chains. It also jointly optimizes task objectives and explanation consistency.", "result": "Experiments show that the proposed method outperforms baseline approaches in accuracy, text generation quality, and factual support, supporting the effectiveness of knowledge enhancement and explainable reasoning.", "conclusion": "The proposed approach overcomes traditional models' limitations in semantic coverage and reasoning transparency, demonstrating strong practical value in complex financial scenarios."}}
{"id": "2512.09056", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09056", "abs": "https://arxiv.org/abs/2512.09056", "authors": ["Liming Kuang", "Yordanka Velikova", "Mahdi Saleh", "Jan-Nico Zaech", "Danda Pani Paudel", "Benjamin Busam"], "title": "ConceptPose: Training-Free Zero-Shot Object Pose Estimation using Concept Vectors", "comment": null, "summary": "Object pose estimation is a fundamental task in computer vision and robotics, yet most methods require extensive, dataset-specific training. Concurrently, large-scale vision language models show remarkable zero-shot capabilities. In this work, we bridge these two worlds by introducing ConceptPose, a framework for object pose estimation that is both training-free and model-free. ConceptPose leverages a vision-language-model (VLM) to create open-vocabulary 3D concept maps, where each point is tagged with a concept vector derived from saliency maps. By establishing robust 3D-3D correspondences across concept maps, our approach allows precise estimation of 6DoF relative pose. Without any object or dataset-specific training, our approach achieves state-of-the-art results on common zero shot relative pose estimation benchmarks, significantly outperforming existing methods by over 62% in ADD(-S) score, including those that utilize extensive dataset-specific training.", "AI": {"tldr": "ConceptPose框架结合视觉语言模型实现了无训练、无模型的开放式词汇表3D概念地图对象姿态估计，优于现有的需要特定数据集训练的方法。", "motivation": "解决当前大多数对象姿态估计方法需要针对特定数据集进行大量训练的问题，同时利用大规模视觉语言模型的零样本能力。", "method": "ConceptPose，一种无训练、无模型的对象姿态估计框架，利用视觉-语言模型创建开放词汇表的3D概念地图，并通过概念地图间的3D-3D对应关系实现6自由度相对姿态的精确估计。", "result": "在常用的零样本相对姿态估计基准测试中，ConceptPose的表现优于现有方法，其ADD(-S)评分高出62%以上。", "conclusion": "ConceptPose框架展示了零样本方法在对象姿态估计任务中的强大潜力，特别是无需特定数据集训练的情况下，依然可以取得优异的性能。"}}
{"id": "2512.09444", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09444", "abs": "https://arxiv.org/abs/2512.09444", "authors": ["Ning Lyu", "Yuxi Wang", "Feng Chen", "Qingyuan Zhang"], "title": "Advancing Text Classification with Large Language Models and Neural Attention Mechanisms", "comment": null, "summary": "This study proposes a text classification algorithm based on large language models, aiming to address the limitations of traditional methods in capturing long-range dependencies, understanding contextual semantics, and handling class imbalance. The framework includes text encoding, contextual representation modeling, attention-based enhancement, feature aggregation, and classification prediction. In the representation stage, deep semantic embeddings are obtained through large-scale pretrained language models, and attention mechanisms are applied to enhance the selective representation of key features. In the aggregation stage, global and weighted strategies are combined to generate robust text-level vectors. In the classification stage, a fully connected layer and Softmax output are used to predict class distributions, and cross-entropy loss is employed to optimize model parameters. Comparative experiments introduce multiple baseline models, including recurrent neural networks, graph neural networks, and Transformers, and evaluate them on Precision, Recall, F1-Score, and AUC. Results show that the proposed method outperforms existing models on all metrics, with especially strong improvements in Recall and AUC. In addition, sensitivity experiments are conducted on hyperparameters and data conditions, covering the impact of hidden dimensions on AUC and the impact of class imbalance ratios on Recall. The findings demonstrate that proper model configuration has a significant effect on performance and reveal the adaptability and stability of the model under different conditions. Overall, the proposed text classification method not only achieves effective performance improvement but also verifies its robustness and applicability in complex data environments through systematic analysis.", "AI": {"tldr": "本文提出了一种基于大规模语言模型的文本分类算法，旨在解决传统方法在捕获长距离依赖性、理解语境语义以及处理类别不平衡方面的局限性。", "motivation": "传统文本分类方法在处理长距离依赖性、语境语义理解以及类别不平衡问题上存在不足。为此，本研究提出了一种基于大规模语言模型的新方法。", "method": "该框架包括文本编码、语境表示建模、注意力增强、特征聚合与分类预测。在表示阶段利用大规模预训练语言模型获取深度语义嵌入，并应用注意力机制提高关键特征的选择性表现。聚合阶段结合全局和加权策略生成健壮的文本级向量。分类阶段采用全连接层和Softmax输出进行类别分布预测，使用交叉熵损失优化模型参数。", "result": "相比较于多种基线模型，包括递归神经网络、图神经网络及Transformer，提出的模型在精度、召回、F1-Score与AUC等指标上均有更优表现，尤其是在召回率和AUC上有显著改善。同时，通过敏感性实验表明，适当的模型配置对性能有重大影响并揭示了在不同条件下模型的适应性和稳定性。", "conclusion": "本研究提出的方法不仅实现了有效的性能提升，还通过系统分析验证了其在复杂数据环境中的健壮性和适用性。"}}
{"id": "2512.09062", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09062", "abs": "https://arxiv.org/abs/2512.09062", "authors": ["Seongyong Kim", "Yong Kwon Cho"], "title": "SIP: Site in Pieces- A Dataset of Disaggregated Construction-Phase 3D Scans for Semantic Segmentation and Scene Understanding", "comment": null, "summary": "Accurate 3D scene interpretation in active construction sites is essential for progress monitoring, safety assessment, and digital twin development. LiDAR is widely used in construction because it offers advantages over camera-based systems, performing reliably in cluttered and dynamically changing conditions. Yet most public datasets for 3D perception are derived from densely fused scans with uniform sampling and complete visibility, conditions that do not reflect real construction sites. Field data are often collected as isolated single-station LiDAR views, constrained by safety requirements, limited access, and ongoing operations. These factors lead to radial density decay, fragmented geometry, and view-dependent visibility-characteristics that remain underrepresented in existing datasets. This paper presents SIP, Site in Pieces, a dataset created to reflect the practical constraints of LiDAR acquisition during construction. SIP provides indoor and outdoor scenes captured with a terrestrial LiDAR scanner and annotated at the point level using a taxonomy tailored to construction environments: A. Built Environment, B. Construction Operations, and C. Site Surroundings. The dataset includes both structural components and slender temporary objects such as scaffolding, MEP piping, and scissor lifts, where sparsity caused by occlusion and fragmented geometry make segmentation particularly challenging. The scanning protocol, annotation workflow, and quality control procedures establish a consistent foundation for the dataset. SIP is openly available with a supporting Git repository, offering adaptable class configurations that streamline adoption within modern 3D deep learning frameworks. By providing field data that retain real-world sensing characteristics, SIP enables robust benchmarking and contributes to advancing construction-oriented 3D vision tasks.", "AI": {"tldr": "The paper introduces SIP (Site in Pieces), a novel dataset specifically designed for construction sites, addressing the challenges of LiDAR data capture under practical constraints. ", "motivation": "To address the limitations of existing 3D perception datasets that do not accurately represent the varying conditions of construction sites, such as occlusion, fragmented geometry, and view-dependent visibility.", "method": "SIP is created through isolated single-station LiDAR scans, capturing both indoor and outdoor scenes with annotations tailored to construction environments. The dataset includes annotations for the built environment, construction operations, and site surroundings.", "result": "The dataset is available publicly and validated for usability within 3D deep learning frameworks, offering ad hoc class configurations for easy adoption.", "conclusion": "SIP contributes to the advancement of construction-oriented 3D vision tasks by providing a realistic dataset that retains the characteristics of real-world construction site sensing."}}
{"id": "2512.09483", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.09483", "abs": "https://arxiv.org/abs/2512.09483", "authors": ["Peixian Zhang", "Qiming Ye", "Zifan Peng", "Kiran Garimella", "Gareth Tyson"], "title": "Source Coverage and Citation Bias in LLM-based vs. Traditional Search Engines", "comment": null, "summary": "LLM-based Search Engines (LLM-SEs) introduces a new paradigm for information seeking. Unlike Traditional Search Engines (TSEs) (e.g., Google), these systems summarize results, often providing limited citation transparency. The implications of this shift remain largely unexplored, yet raises key questions regarding trust and transparency. In this paper, we present a large-scale empirical study of LLM-SEs, analyzing 55,936 queries and the corresponding search results across six LLM-SEs and two TSEs. We confirm that LLM-SEs cites domain resources with greater diversity than TSEs. Indeed, 37% of domains are unique to LLM-SEs. However, certain risks still persist: LLM-SEs do not outperform TSEs in credibility, political neutrality and safety metrics. Finally, to understand the selection criteria of LLM-SEs, we perform a feature-based analysis to identify key factors influencing source choice. Our findings provide actionable insights for end users, website owners, and developers.", "AI": {"tldr": "本论文通过大规模比较研究，分析了LLM-SEs与TSEs，结果发现LLM-SEs虽然资源多样，但在某些方面仍存在风险。", "motivation": "研究动机在于探索LLM-SEs带来的新信息搜索范式的潜在影响，特别是信任和透明度问题。", "method": "本研究通过大规模实证分析，对LLM搜索引擎（LLM-SEs）和传统搜索引擎（TSEs）进行了比较，分析了55,936个查询及其对应的结果。", "result": "研究结果显示，LLM-SEs引用的领域资源多样化，但其在可信度、政治中立性和安全性方面表现并不优于TSEs。", "conclusion": "研究提供了对用户、网站所有者和开发者的实用见解，并通过基于特征的分析揭示了LLM-SEs选择资源的关键因素。"}}
{"id": "2512.09069", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09069", "abs": "https://arxiv.org/abs/2512.09069", "authors": ["Erfan Nourbakhsh", "Nasrin Sanjari", "Ali Nourbakhsh"], "title": "KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification", "comment": "7 pages, 5 figures", "summary": "Age-related macular degeneration (AMD) and choroidal neovascularization (CNV)-related conditions are leading causes of vision loss worldwide, with optical coherence tomography (OCT) serving as a cornerstone for early detection and management. However, deploying state-of-the-art deep learning models like ConvNeXtV2-Large in clinical settings is hindered by their computational demands. Therefore, it is desirable to develop efficient models that maintain high diagnostic performance while enabling real-time deployment. In this study, a novel knowledge distillation framework, termed KD-OCT, is proposed to compress a high-performance ConvNeXtV2-Large teacher model, enhanced with advanced augmentations, stochastic weight averaging, and focal loss, into a lightweight EfficientNet-B2 student for classifying normal, drusen, and CNV cases. KD-OCT employs real-time distillation with a combined loss balancing soft teacher knowledge transfer and hard ground-truth supervision. The effectiveness of the proposed method is evaluated on the Noor Eye Hospital (NEH) dataset using patient-level cross-validation. Experimental results demonstrate that KD-OCT outperforms comparable multi-scale or feature-fusion OCT classifiers in efficiency- accuracy balance, achieving near-teacher performance with substantial reductions in model size and inference time. Despite the compression, the student model exceeds most existing frameworks, facilitating edge deployment for AMD screening. Code is available at https://github.com/erfan-nourbakhsh/KD- OCT.", "AI": {"tldr": "本文提出了一种新的知识蒸馏框架KD-OCT，该框架压缩了高性能的深度学习模型，提高了模型在临床应用中的效率和实时性，特别是在AMD筛查方面具有显著的效果。", "motivation": "年龄相关性黄斑变性和脉络膜新生血管相关的疾病是全球视力丧失的主要原因，而光学相干断层扫描（OCT）是早期检测和管理的基石。然而，部署像ConvNeXtV2-Large这样的深度学习模型在临床环境受计算需求的限制。因此，开发高效模型，同时保持高性能且实现实时部署是非常重要的。", "method": "开发了一种名为KD-OCT的新知识蒸馏框架，将高性能的ConvNeXtV2-Large教师模型压缩成轻量级的EfficientNet-B2学生模型，用于分类正常、玻璃膜疣和CNV病例。KD-OCT采用了实时蒸馏，结合损失平衡的软教师知识传递和硬地面真值监督。同时，该模型利用了高级数据增强、随机权重平均和焦损失进行优化。", "result": "实验结果表明，KD-OCT在效率-准确性平衡方面超越了类似的多尺度或特征融合OCT分类器，实现了接近教师模型的性能，同时显著减少了模型大小和推理时间。", "conclusion": "尽管经过了压缩，学生模型性能超过了大多数现有的框架，有利于AMD筛查的边缘部署。"}}
{"id": "2512.09487", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.09487", "abs": "https://arxiv.org/abs/2512.09487", "authors": ["Yucan Guo", "Miao Su", "Saiping Guan", "Zihao Sun", "Xiaolong Jin", "Jiafeng Guo", "Xueqi Cheng"], "title": "RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \\model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \\model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \\model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.", "AI": {"tldr": "研究介绍了一种基于RL的框架\\model{}，用于实现LLMs的多轮次和自适应图-文本混合检索增强生成(RAG)，该模型通过联合优化整个生成过程和设计的两阶段训练框架，在多模态证据利用和检索效率之间取得了更好的平衡。", "motivation": "现有的图或混合系统依赖于固定的或手工编写的检索管道，缺乏在推理过程中集成补充证据的能力。而现在，\\model{}解决了这些限制，实现了更高效的多跳推理。", "method": "模型\\model{}基于RL框架，允许LLMs进行多轮次和自适应的图-文本混合检索增强生成。该模型通过RL联合优化整个生成过程，允许模型学习何时进行推理，从文本或图中检索什么内容，以及何时生成最终答案。为了引导学习过程，设计了一个两阶段的训练框架，平衡了任务结果和检索效率。", "result": "实验在五个问答基准测试中进行，结果显示，\\model{}显著优于现有的RAG基线模型，证明了其在复杂推理支持的自适应和高效检索上面的效果。", "conclusion": "实验结果表明，\\model{}方法相较于现有RAG基线模型有显著的性能提升，在多种问答基准测试中表现突出，证明了端到端RL在支持复杂推理的自适应和高效检索方面的有效性。"}}
{"id": "2512.09071", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09071", "abs": "https://arxiv.org/abs/2512.09071", "authors": ["Nick Trinh", "Damian Lyons"], "title": "Adaptive Thresholding for Visual Place Recognition using Negative Gaussian Mixture Statistics", "comment": "Accepted and presented at IEEE RoboticCC 2025. 4 pages short paper", "summary": "Visual place recognition (VPR) is an important component technology for camera-based mapping and navigation applications. This is a challenging problem because images of the same place may appear quite different for reasons including seasonal changes, weather illumination, structural changes to the environment, as well as transient pedestrian or vehicle traffic. Papers focusing on generating image descriptors for VPR report their results using metrics such as recall@K and ROC curves. However, for a robot implementation, determining which matches are sufficiently good is often reduced to a manually set threshold. And it is difficult to manually select a threshold that will work for a variety of visual scenarios. This paper addresses the problem of automatically selecting a threshold for VPR by looking at the 'negative' Gaussian mixture statistics for a place - image statistics indicating not this place. We show that this approach can be used to select thresholds that work well for a variety of image databases and image descriptors.", "AI": {"tldr": "该论文提出一种自动选择视觉位置识别（VPR）阈值的方法，通过使用'负面'高斯混合统计，这种方法适用于不同类型的图像数据库和描述符，不需要人工设置阈值。", "motivation": "论文旨在解决手动选择阈值因视觉场景变化而难以适应的问题。", "method": "该论文提出了一种通过观察“负面”的高斯混合统计来自动选择阈值的方法，这些统计信息用于表示不属于特定地点的图像特征。", "result": "这种方法能够对于不同的图像数据库和图像描述符选择合适的阈值，显示出良好的适应性。", "conclusion": "该研究表明，基于'负面'高斯混合统计的方法可以有效挑选出适用于多种视觉场景的视觉位置识别阈值。"}}
{"id": "2512.09552", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09552", "abs": "https://arxiv.org/abs/2512.09552", "authors": ["Kun Sun", "Rong Wang"], "title": "Systematic Framework of Application Methods for Large Language Models in Language Sciences", "comment": null, "summary": "Large Language Models (LLMs) are transforming language sciences. However, their widespread deployment currently suffers from methodological fragmentation and a lack of systematic soundness. This study proposes two comprehensive methodological frameworks designed to guide the strategic and responsible application of LLMs in language sciences. The first method-selection framework defines and systematizes three distinct, complementary approaches, each linked to a specific research goal: (1) prompt-based interaction with general-use models for exploratory analysis and hypothesis generation; (2) fine-tuning of open-source models for confirmatory, theory-driven investigation and high-quality data generation; and (3) extraction of contextualized embeddings for further quantitative analysis and probing of model internal mechanisms. We detail the technical implementation and inherent trade-offs of each method, supported by empirical case studies. Based on the method-selection framework, the second systematic framework proposed provides constructed configurations that guide the practical implementation of multi-stage research pipelines based on these approaches. We then conducted a series of empirical experiments to validate our proposed framework, employing retrospective analysis, prospective application, and an expert evaluation survey. By enforcing the strategic alignment of research questions with the appropriate LLM methodology, the frameworks enable a critical paradigm shift in language science research. We believe that this system is fundamental for ensuring reproducibility, facilitating the critical evaluation of LLM mechanisms, and providing the structure necessary to move traditional linguistics from ad-hoc utility to verifiable, robust science.", "AI": {"tldr": "该研究提出了两个系统方法论框架，旨在指导语言科学中大型语言模型的战略和负责任应用，包括方法选择框架和技术实施方案框架。", "motivation": "由于方法论碎片化和缺乏系统的稳健性，大型语言模型的广泛应用目前面临挑战。这项研究旨在通过提出两个框架来解决这些问题，以促进更系统的应用。", "method": "研究制定了一个方法选择框架，定义和系统化了三种不同的互补方法，并基于此框架提出了一个系统性框架，提供了一种研究流程实施指南。", "result": "通过回顾性分析、前瞻性应用和专家评价调查等一系列实证实验验证了所提出的框架。", "conclusion": "这些框架能够推动语言科学研究中的关键范式转变，确保可再现性，有利于大型语言模型机制的批判性评价，并将传统语言学从随意应用转变为可验证的健壮科学。"}}
{"id": "2512.09081", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09081", "abs": "https://arxiv.org/abs/2512.09081", "authors": ["Arman Zarei", "Jiacheng Pan", "Matthew Gwilliam", "Soheil Feizi", "Zhenheng Yang"], "title": "AgentComp: From Agentic Reasoning to Compositional Mastery in Text-to-Image Models", "comment": null, "summary": "Text-to-image generative models have achieved remarkable visual quality but still struggle with compositionality$-$accurately capturing object relationships, attribute bindings, and fine-grained details in prompts. A key limitation is that models are not explicitly trained to differentiate between compositionally similar prompts and images, resulting in outputs that are close to the intended description yet deviate in fine-grained details. To address this, we propose AgentComp, a framework that explicitly trains models to better differentiate such compositional variations and enhance their reasoning ability. AgentComp leverages the reasoning and tool-use capabilities of large language models equipped with image generation, editing, and VQA tools to autonomously construct compositional datasets. Using these datasets, we apply an agentic preference optimization method to fine-tune text-to-image models, enabling them to better distinguish between compositionally similar samples and resulting in overall stronger compositional generation ability. AgentComp achieves state-of-the-art results on compositionality benchmarks such as T2I-CompBench, without compromising image quality$-$a common drawback in prior approaches$-$and even generalizes to other capabilities not explicitly trained for, such as text rendering.", "AI": {"tldr": "本文介绍AgentComp框架，通过大型语言模型和工具使用权来改善文本到图像生成模型的构图能力，并实现更细粒度的区分和生成能力。", "motivation": "文本到图像生成模型在视觉质量上取得了显著成就，但在捕捉对象关系、属性绑定和提示中的细粒度细节方面仍存在问题。这些问题源于模型未能明确训练来区分构图相似的提示和图像。", "method": "AgentComp框架采用大型语言模型，并配备图像生成、编辑和视觉问题回答工具，以自主构建构图数据集。通过这些数据集，应用代理偏好优化方法对文本到图像模型进行微调，从而增强模型区分构图相似样本的能力。", "result": "AgentComp在构图基准测试如T2I-CompBench上取得了最先进的结果，同时没有牺牲图像质量，甚至在未明确训练的能力上也表现出了良好的泛化性。", "conclusion": "AgentComp能够提高文本到图像模型的构图能力，实现更细粒度的区分，并且不会以牺牲图像质量为代价。此外，它还能在其他未明确训练的能力上表现出良好的泛化性。"}}
{"id": "2512.09563", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09563", "abs": "https://arxiv.org/abs/2512.09563", "authors": ["Binglin Wu", "Jiaxiu Zou", "Xianneng Li"], "title": "System Report for CCL25-Eval Task 10: Prompt-Driven Large Language Model Merge for Fine-Grained Chinese Hate Speech Detection", "comment": "Accepted at CCL 2025", "summary": "The proliferation of hate speech on Chinese social media poses urgent societal risks, yet traditional systems struggle to decode context-dependent rhetorical strategies and evolving slang. To bridge this gap, we propose a novel three-stage LLM-based framework: Prompt Engineering, Supervised Fine-tuning, and LLM Merging. First, context-aware prompts are designed to guide LLMs in extracting implicit hate patterns. Next, task-specific features are integrated during supervised fine-tuning to enhance domain adaptation. Finally, merging fine-tuned LLMs improves robustness against out-of-distribution cases. Evaluations on the STATE-ToxiCN benchmark validate the framework's effectiveness, demonstrating superior performance over baseline methods in detecting fine-grained hate speech.", "AI": {"tldr": "为了解决Chinese社交媒体上的仇恨言论问题，提出了一个基于大型语言模型的三阶段框架，并展示了优越的检测性能。", "motivation": "传统的系统难以解析依赖上下文的修辞策略和不断变化的俚语，这导致了Chinese社交媒体上仇恨言论检测的难题。", "method": "提出了一种基于LLM的三阶段框架，包括Prompt Engineering, Supervised Fine-tuning 和 LLM Merging。", "result": "在STATE-ToxiCN基准上验证了框架的有效性，表明其在细粒度仇恨言论检测方面超越了基线方法。", "conclusion": "该框架在检测细粒度仇恨言论方面表现优于基线方法，验证了其在Chinese社会媒体上的有效性和适应性。"}}
{"id": "2512.09092", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09092", "abs": "https://arxiv.org/abs/2512.09092", "authors": ["Mizanur Rahman Jewel", "Mohamed Elmahallawy", "Sanjay Madria", "Samuel Frimpong"], "title": "Explaining the Unseen: Multimodal Vision-Language Reasoning for Situational Awareness in Underground Mining Disasters", "comment": null, "summary": "Underground mining disasters produce pervasive darkness, dust, and collapses that obscure vision and make situational awareness difficult for humans and conventional systems. To address this, we propose MDSE, Multimodal Disaster Situation Explainer, a novel vision-language framework that automatically generates detailed textual explanations of post-disaster underground scenes. MDSE has three-fold innovations: (i) Context-Aware Cross-Attention for robust alignment of visual and textual features even under severe degradation; (ii) Segmentation-aware dual pathway visual encoding that fuses global and region-specific embeddings; and (iii) Resource-Efficient Transformer-Based Language Model for expressive caption generation with minimal compute cost. To support this task, we present the Underground Mine Disaster (UMD) dataset--the first image-caption corpus of real underground disaster scenes--enabling rigorous training and evaluation. Extensive experiments on UMD and related benchmarks show that MDSE substantially outperforms state-of-the-art captioning models, producing more accurate and contextually relevant descriptions that capture crucial details in obscured environments, improving situational awareness for underground emergency response. The code is at https://github.com/mizanJewel/Multimodal-Disaster-Situation-Explainer.", "AI": {"tldr": "文章提出了一种名为MDSE的新型视觉语言框架，该框架可在地下矿山灾害条件下自动生成详细的文字说明，从而改善对灾区环境的理解。", "motivation": "由于地下矿山灾害产生普遍的黑暗、灰尘和坍塌，降低了视觉清晰度，使人类和常规系统难以获得环境意识。MDSE程序旨在自动生成灾区地下场景的详细文本描述，进而改善地下紧急救援的环境意识。", "method": "MDSE具有三大创新点：(i) 在严重退化条件下稳健地对齐视觉和文本特征的基于上下文的交叉注意力机制；(ii) 将全局和区域特定嵌入融合的基于分割的双路径视觉编码；(iii) 用于生成具有极小计算成本的表达性描述的资源高效型基于变压器的语言模型。", "result": "在UMD数据集和其他相关数据集上的广泛实验验证了MDSE的有效性，其生成的描述更加准确并具有上下文相关性，捕捉到了关键的细节。", "conclusion": "实验结果表明，MDSE在准确性和上下文相关性方面显著优于现有的描述生成模型，尤其是在条件恶劣的环境中，提高了环境意识，从而有助于地下紧急救援。"}}
{"id": "2512.09634", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09634", "abs": "https://arxiv.org/abs/2512.09634", "authors": ["Karl Gustav Gailit", "Kadri Muischnek", "Kairit Sirts"], "title": "Creation of the Estonian Subjectivity Dataset: Assessing the Degree of Subjectivity on a Scale", "comment": "9 pages, 5 figures, 2 appendixes, submitted to LREC 2026", "summary": "This article presents the creation of an Estonian-language dataset for document-level subjectivity, analyzes the resulting annotations, and reports an initial experiment of automatic subjectivity analysis using a large language model (LLM). The dataset comprises of 1,000 documents-300 journalistic articles and 700 randomly selected web texts-each rated for subjectivity on a continuous scale from 0 (fully objective) to 100 (fully subjective) by four annotators. As the inter-annotator correlations were moderate, with some texts receiving scores at the opposite ends of the scale, a subset of texts with the most divergent scores was re-annotated, with the inter-annotator correlation improving. In addition to human annotations, the dataset includes scores generated by GPT-5 as an experiment on annotation automation. These scores were similar to human annotators, however several differences emerged, suggesting that while LLM based automatic subjectivity scoring is feasible, it is not an interchangeable alternative to human annotation, and its suitability depends on the intended application.", "AI": {"tldr": "本文介绍了一个用于文本文档主观性分析的爱沙尼亚语数据集，该数据集由300篇新闻文章和700篇随机选取的网络文本组成，通过人工标注和GPT-5的自动评分实验来评估主观性。", "motivation": "创建一个爱沙尼亚语的数据集，用于分析文档层次的主观性并探索自动主观性分析的可行性。", "method": "使用1000个文档（300篇新闻文章和700篇网络文本）由四位标注员在连续尺度上进行主观性评分。对于分歧较大的评分，进行了重新标注，并将GPT-5的评分纳入数据集。", "result": "人工打分之间的相关性为中等，通过重新标注改善了相关性。GPT-5的评分与人工标注类似，但存在差异。", "conclusion": "使用大型语言模型进行自动主观性评分是可行的，但并非人类标注的直接替代品，其适用性取决于具体的应用场景。"}}
{"id": "2512.09095", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09095", "abs": "https://arxiv.org/abs/2512.09095", "authors": ["Xinyue Pan", "Yuhao Chen", "Jiangpeng He", "Fengqing Zhu"], "title": "Food Image Generation on Multi-Noun Categories", "comment": "Accepted by WACV 2026", "summary": "Generating realistic food images for categories with multiple nouns is surprisingly challenging. For instance, the prompt \"egg noodle\" may result in images that incorrectly contain both eggs and noodles as separate entities. Multi-noun food categories are common in real-world datasets and account for a large portion of entries in benchmarks such as UEC-256. These compound names often cause generative models to misinterpret the semantics, producing unintended ingredients or objects. This is due to insufficient multi-noun category related knowledge in the text encoder and misinterpretation of multi-noun relationships, leading to incorrect spatial layouts. To overcome these challenges, we propose FoCULR (Food Category Understanding and Layout Refinement) which incorporates food domain knowledge and introduces core concepts early in the generation process. Experimental results demonstrate that the integration of these techniques improves image generation performance in the food domain.", "AI": {"tldr": "针对生成具有多个名词的食品类别的图像时出现的问题，提出了FoCULR方法，实验表明该方法可以提高食品类图像生成的质量。", "motivation": "生成具有多个名词的食品类别的逼真图像特别具有挑战性，文本编码器中缺乏多名词类别相关知识以及误解多名词之间的关系导致生成错误的空间布局。", "method": "提出了一种名为FoCULR（食品类别理解和布局细化）的方法，该方法整合了食品领域的知识，并在生成过程的早期引入核心概念。", "result": "实验结果表明，整合这些技术可以提高食品领域的图像生成性能。", "conclusion": "通过FoCULR方法，可以有效提升多名词食品类别的图像生成质量，减少生成错误的空间布局和误解。"}}
{"id": "2512.09636", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09636", "abs": "https://arxiv.org/abs/2512.09636", "authors": ["Mengxi Xiao", "Kailai Yang", "Pengde Zhao", "Enze Zhang", "Ziyan Kuang", "Zhiwei Liu", "Weiguang Han", "Shu Liao", "Lianting Huang", "Jinpeng Hu", "Min Peng", "Qianqian Xie", "Sophia Ananiadou"], "title": "MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment", "comment": null, "summary": "Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.", "AI": {"tldr": "该论文介绍了MentraSuite框架，用于提高心理健康推理的可靠性，并提出了MentraBench基准测试和Mindora模型，后者通过一个混合框架进行优化，极大地增强了正确和连贯的推理能力，在心理健康情境中表现出色。", "motivation": "由于大规模语言模型在心理健康环境中的应用存在风险，该论文的动机在于解决这些模型在提供心理健康支持时可能存在的推理不完整或不一致的问题，进而开发出更适合心理健康领域的模型。", "method": "论文提出了一种名为Mindora的模型，该模型通过混合SFT-RL（监督微调-强化学习）框架进行优化，其中设计了一种奖励机制来检测不一致性并鼓励忠实和连贯的推理。", "result": "研究通过MentraBench综合基准测试了20个LLM模型，Mindora模型在推理可靠性方面表现出色，证明了其在复杂心理健康场景中的有效性。", "conclusion": "论文通过引入MentraSuite及MentraBench不仅填补了在心理健康背景下的语言模型推理能力评估方面的空白，而且通过优化的Mindora模型提高了语言模型在这一领域的可靠性和连贯性。"}}
{"id": "2512.09112", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09112", "abs": "https://arxiv.org/abs/2512.09112", "authors": ["Frédéric Fortier-Chouinard", "Yannick Hold-Geoffroy", "Valentin Deschaintre", "Matheus Gadelha", "Jean-François Lalonde"], "title": "GimbalDiffusion: Gravity-Aware Camera Control for Video Generation", "comment": "Project page: https://lvsn.github.io/GimbalDiffusion/", "summary": "Recent progress in text-to-video generation has achieved remarkable realism, yet fine-grained control over camera motion and orientation remains elusive. Existing approaches typically encode camera trajectories through relative or ambiguous representations, limiting explicit geometric control. We introduce GimbalDiffusion, a framework that enables camera control grounded in physical-world coordinates, using gravity as a global reference. Instead of describing motion relative to previous frames, our method defines camera trajectories in an absolute coordinate system, allowing precise and interpretable control over camera parameters without requiring an initial reference frame. We leverage panoramic 360-degree videos to construct a wide variety of camera trajectories, well beyond the predominantly straight, forward-facing trajectories seen in conventional video data. To further enhance camera guidance, we introduce null-pitch conditioning, an annotation strategy that reduces the model's reliance on text content when conflicting with camera specifications (e.g., generating grass while the camera points towards the sky). Finally, we establish a benchmark for camera-aware video generation by rebalancing SpatialVID-HQ for comprehensive evaluation under wide camera pitch variation. Together, these contributions advance the controllability and robustness of text-to-video models, enabling precise, gravity-aligned camera manipulation within generative frameworks.", "AI": {"tldr": "本文提出了GimbalDiffusion，该框架能实现基于绝对坐标系统的相机控制，改善了相机姿态生成的精确性和可解释性，并引入了null-pitch条件策略提高相机引导能力。", "motivation": "尽管文本到视频生成技术已经取得了一定的突破，但对相机运动和方向的精细控制仍然不足。现有的方法通常通过相对或模糊的表示来编码相机轨迹，限制了明确的几何控制，因此本研究所提出的GimbalDiffusion框架旨在解决这一问题。", "method": "本文提出了GimbalDiffusion框架，该框架使相机控制能够基于物理世界的坐标，使用重力作为全局参考。与现有方法不同，GimbalDiffusion采用绝对坐标系定义相机轨迹，而不是相对于前一帧的轨迹。此外，该方法使用全景360度视频构建广泛多样的相机轨迹，并引入了null-pitch条件策略，能够在文本生成与相机姿态相冲突时减少模型对文本内容的依赖，从而增强相机引导效果。", "result": "提出了GimbalDiffusion框架，允许基于绝对坐标系统的精确控制，并通过null-pitch条件减少与视觉输入不一致时的模型混淆，并设置了一个新的基准来评估摄像机感知的视频生成。", "conclusion": "这些贡献提升了文本到视频模型的可控性和鲁棒性，实现了精确的、遵循重力的相机操控。"}}
{"id": "2512.09662", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09662", "abs": "https://arxiv.org/abs/2512.09662", "authors": ["Paloma Piot", "David Otero", "Patricia Martín-Rodilla", "Javier Parapar"], "title": "Can LLMs Evaluate What They Cannot Annotate? Revisiting LLM Reliability in Hate Speech Detection", "comment": null, "summary": "Hate speech spreads widely online, harming individuals and communities, making automatic detection essential for large-scale moderation, yet detecting it remains difficult. Part of the challenge lies in subjectivity: what one person flags as hate speech, another may see as benign. Traditional annotation agreement metrics, such as Cohen's $κ$, oversimplify this disagreement, treating it as an error rather than meaningful diversity. Meanwhile, Large Language Models (LLMs) promise scalable annotation, but prior studies demonstrate that they cannot fully replace human judgement, especially in subjective tasks. In this work, we reexamine LLM reliability using a subjectivity-aware framework, cross-Rater Reliability (xRR), revealing that even under fairer lens, LLMs still diverge from humans. Yet this limitation opens an opportunity: we find that LLM-generated annotations can reliably reflect performance trends across classification models, correlating with human evaluations. We test this by examining whether LLM-generated annotations preserve the relative ordering of model performance derived from human evaluation (i.e. whether models ranked as more reliable by human annotators preserve the same order when evaluated with LLM-generated labels). Our results show that, although LLMs differ from humans at the instance level, they reproduce similar ranking and classification patterns, suggesting their potential as proxy evaluators. While not a substitute for human annotators, they might serve as a scalable proxy for evaluation in subjective NLP tasks.", "AI": {"tldr": "研究探讨了大型语言模型（LLMs）在仇恨言论检测中的可靠性，发现尽管LLMs与人类判断存在差异，但在反映分类模型性能趋势上与人类评估一致，可作为主观NLP任务中的代理评估工具。", "motivation": "解决在线仇恨言论自动检测的困难，特别关注大型语言模型在主观任务中的应用及评估。", "method": "使用跨评分者可靠性（xRR）框架重新审视大型语言模型的可靠性，并测试LLM生成的标签是否可以可靠地反映分类模型的性能趋势。", "result": "尽管LLMs在实例层面上与人类判断有所不同，但是能够在模型性能排名和分类趋势上重现类似的结果。", "conclusion": "LLMs虽然不能完全替代人类判断，但在主观自然语言处理任务中可以作为可扩展的代理评估工具使用。"}}
{"id": "2512.09115", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09115", "abs": "https://arxiv.org/abs/2512.09115", "authors": ["Sander Riisøen Jyhne", "Christian Igel", "Morten Goodwin", "Per-Arne Andersen", "Serge Belongie", "Nico Lang"], "title": "SuperF: Neural Implicit Fields for Multi-Image Super-Resolution", "comment": "23 pages, 13 figures, 8 tables", "summary": "High-resolution imagery is often hindered by limitations in sensor technology, atmospheric conditions, and costs. Such challenges occur in satellite remote sensing, but also with handheld cameras, such as our smartphones. Hence, super-resolution aims to enhance the image resolution algorithmically. Since single-image super-resolution requires solving an inverse problem, such methods must exploit strong priors, e.g. learned from high-resolution training data, or be constrained by auxiliary data, e.g. by a high-resolution guide from another modality. While qualitatively pleasing, such approaches often lead to \"hallucinated\" structures that do not match reality. In contrast, multi-image super-resolution (MISR) aims to improve the (optical) resolution by constraining the super-resolution process with multiple views taken with sub-pixel shifts. Here, we propose SuperF, a test-time optimization approach for MISR that leverages coordinate-based neural networks, also called neural fields. Their ability to represent continuous signals with an implicit neural representation (INR) makes them an ideal fit for the MISR task.\n  The key characteristic of our approach is to share an INR for multiple shifted low-resolution frames and to jointly optimize the frame alignment with the INR. Our approach advances related INR baselines, adopted from burst fusion for layer separation, by directly parameterizing the sub-pixel alignment as optimizable affine transformation parameters and by optimizing via a super-sampled coordinate grid that corresponds to the output resolution. Our experiments yield compelling results on simulated bursts of satellite imagery and ground-level images from handheld cameras, with upsampling factors of up to 8. A key advantage of SuperF is that this approach does not rely on any high-resolution training data.", "AI": {"tldr": "This paper presents SuperF, a model that uses neural fields to enhance image resolution from multiple shifted low-res frames without requiring high-res training data, achieving impressive upsampling results.", "motivation": "To enhance the resolution of images algorithmically without relying on high-resolution training data, addressing the limitations in sensor technology, atmospheric conditions, and costs.", "method": "SuperF, a test-time optimization approach for multi-image super-resolution (MISR) that uses coordinate-based neural networks (neural fields) to share an implicit neural representation (INR) for multiple shifted low-resolution frames and jointly optimizes frame alignment and INR.", "result": "The approach yields compelling results on both simulated satellite imagery and ground-level images from handheld cameras, with upsampling factors of up to 8.", "conclusion": "SuperF advances the field of multi-image super-resolution by not only achieving high-quality results but also eliminating the need for high-resolution training datasets, making it a versatile solution for various imaging challenges."}}
{"id": "2512.09666", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09666", "abs": "https://arxiv.org/abs/2512.09666", "authors": ["Arthur Hemmer", "Mickaël Coustaty", "Nicola Bartolo", "Jean-Marc Ogier"], "title": "Neurosymbolic Information Extraction from Transactional Documents", "comment": "20 pages, 2 figures, accepted to IJDAR (ICDAR 2025)", "summary": "This paper presents a neurosymbolic framework for information extraction from documents, evaluated on transactional documents. We introduce a schema-based approach that integrates symbolic validation methods to enable more effective zero-shot output and knowledge distillation. The methodology uses language models to generate candidate extractions, which are then filtered through syntactic-, task-, and domain-level validation to ensure adherence to domain-specific arithmetic constraints. Our contributions include a comprehensive schema for transactional documents, relabeled datasets, and an approach for generating high-quality labels for knowledge distillation. Experimental results demonstrate significant improvements in $F_1$-scores and accuracy, highlighting the effectiveness of neurosymbolic validation in transactional document processing.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2512.09134", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09134", "abs": "https://arxiv.org/abs/2512.09134", "authors": ["Georgy Kopanitsa", "Oleg Metsker", "Alexey Yakovlev"], "title": "Integrated Pipeline for Coronary Angiography With Automated Lesion Profiling, Virtual Stenting, and 100-Vessel FFR Validation", "comment": "22 pages, 10 figures, 7 tables", "summary": "Coronary angiography is the main tool for assessing coronary artery disease, but visual grading of stenosis is variable and only moderately related to ischaemia. Wire based fractional flow reserve (FFR) improves lesion selection but is not used systematically. Angiography derived indices such as quantitative flow ratio (QFR) offer wire free physiology, yet many tools are workflow intensive and separate from automated anatomy analysis and virtual PCI planning. We developed AngioAI-QFR, an end to end angiography only pipeline combining deep learning stenosis detection, lumen segmentation, centreline and diameter extraction, per millimetre Relative Flow Capacity profiling, and virtual stenting with automatic recomputation of angiography derived QFR. The system was evaluated in 100 consecutive vessels with invasive FFR as reference. Primary endpoints were agreement with FFR (correlation, mean absolute error) and diagnostic performance for FFR <= 0.80. On held out frames, stenosis detection achieved precision 0.97 and lumen segmentation Dice 0.78. Across 100 vessels, AngioAI-QFR correlated strongly with FFR (r = 0.89, MAE 0.045). The AUC for detecting FFR <= 0.80 was 0.93, with sensitivity 0.88 and specificity 0.86. The pipeline completed fully automatically in 93 percent of vessels, with median time to result 41 s. RFC profiling distinguished focal from diffuse capacity loss, and virtual stenting predicted larger QFR gain in focal than in diffuse disease. AngioAI-QFR provides a practical, near real time pipeline that unifies computer vision, functional profiling, and virtual PCI with automated angiography derived physiology.", "AI": {"tldr": "开发了AngioAI-QFR系统，实现了自动化的冠状动脉分析，包括狭窄检测、流量分析和虚拟支架效果预测，提升了冠状动脉疾病评估的准确性和效率。", "motivation": "旨在解决传统冠状动脉造影评估狭窄的可变性及与缺血相关性较低的问题，同时也考虑到基于导线的FFR选择性改进但未系统化使用，以及现有工具工作流程繁琐的问题。", "method": "采用了一种端到端的基于冠状动脉造影的管道，结合了深度学习检测狭窄、分割管腔、提取中心线和直径、逐毫米相对流量容量轮廓绘制以及虚拟支架置入，并自动重新计算造影衍生的QFR。", "result": "在100个连续血管中，AngioAI-QFR与FFR有很强的相关性（r=0.89，MAE=0.045），且在检测FFR≤0.80时的AUC为0.93，敏感性为0.88，特异性为0.86。在93%的血管中，该管道可完全自动完成，平均耗时41秒。", "conclusion": "AngioAI-QFR提供了一个实用的、近实时的管道，统一了计算机视觉、功能分析以及虚拟支架置入和自动造影衍生生理学分析。"}}
{"id": "2512.09675", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09675", "abs": "https://arxiv.org/abs/2512.09675", "authors": ["Leyi Pan", "Shuchang Tao", "Yunpeng Zhai", "Zheyu Fu", "Liancheng Fang", "Minghua He", "Lingzhe Zhang", "Zhaoyang Liu", "Bolin Ding", "Aiwei Liu", "Lijie Wen"], "title": "d-TreeRPO: Towards More Reliable Policy Optimization for Diffusion Language Models", "comment": "16 pages, 5 figures, 3tables", "summary": "Reliable reinforcement learning (RL) for diffusion large language models (dLLMs) requires both accurate advantage estimation and precise estimation of prediction probabilities. Existing RL methods for dLLMs fall short in both aspects: they rely on coarse or unverifiable reward signals, and they estimate prediction probabilities without accounting for the bias relative to the true, unbiased expected prediction probability that properly integrates over all possible decoding orders. To mitigate these issues, we propose \\emph{d}-TreeRPO, a reliable RL framework for dLLMs that leverages tree-structured rollouts and bottom-up advantage computation based on verifiable outcome rewards to provide fine-grained and verifiable step-wise reward signals. When estimating the conditional transition probability from a parent node to a child node, we theoretically analyze the estimation error between the unbiased expected prediction probability and the estimate obtained via a single forward pass, and find that higher prediction confidence leads to lower estimation error. Guided by this analysis, we introduce a time-scheduled self-distillation loss during training that enhances prediction confidence in later training stages, thereby enabling more accurate probability estimation and improved convergence. Experiments show that \\emph{d}-TreeRPO outperforms existing baselines and achieves significant gains on multiple reasoning benchmarks, including +86.2 on Sudoku, +51.6 on Countdown, +4.5 on GSM8K, and +5.3 on Math500. Ablation studies and computational cost analyses further demonstrate the effectiveness and practicality of our design choices.", "AI": {"tldr": "提出d-TreeRPO框架，解决了现有dLLMs强化学习中优势估计和预测概率估计问题，通过实验展示其有效性。", "motivation": "现有的针对dLLMs的强化学习方法在优势估计和预测概率估计方面存在不足，d-TreeRPO旨在解决这些问题，提供更可靠的强化学习框架。", "method": "引入树结构展开和底向上的优势计算，结合时间调度的自蒸馏损失，提高预测概率的准确性，减少估计偏差。", "result": "该论文提出了一种名为d-TreeRPO的可靠强化学习框架，专门针对扩散大型语言模型（dLLMs）。d-TreeRPO利用树结构的展开和基于可验证结果奖励的自底向上优势计算，提供细粒度和可验证的逐步奖励信号。该框架通过在训练过程中引入时间调度的自蒸馏损失，提高了预测概率估计的准确性，从而改善了学习的收敛性。实验表明，d-TreeRPO在多个推理基准测试上显著优于现有的基线，展示了其有效性和实用性。", "conclusion": "d-TreeRPO通过引入树结构和时间调度自蒸馏损失，提高了dLLMs在强化学习中的性能，并在多个推理任务上获得了显著的性能提升。"}}
{"id": "2512.09162", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.09162", "abs": "https://arxiv.org/abs/2512.09162", "authors": ["Kelian Baert", "Mae Younes", "Francois Bourel", "Marc Christie", "Adnane Boukhayma"], "title": "GTAvatar: Bridging Gaussian Splatting and Texture Mapping for Relightable and Editable Gaussian Avatars", "comment": null, "summary": "Recent advancements in Gaussian Splatting have enabled increasingly accurate reconstruction of photorealistic head avatars, opening the door to numerous applications in visual effects, videoconferencing, and virtual reality. This, however, comes with the lack of intuitive editability offered by traditional triangle mesh-based methods. In contrast, we propose a method that combines the accuracy and fidelity of 2D Gaussian Splatting with the intuitiveness of UV texture mapping. By embedding each canonical Gaussian primitive's local frame into a patch in the UV space of a template mesh in a computationally efficient manner, we reconstruct continuous editable material head textures from a single monocular video on a conventional UV domain. Furthermore, we leverage an efficient physically based reflectance model to enable relighting and editing of these intrinsic material maps. Through extensive comparisons with state-of-the-art methods, we demonstrate the accuracy of our reconstructions, the quality of our relighting results, and the ability to provide intuitive controls for modifying an avatar's appearance and geometry via texture mapping without additional optimization.", "AI": {"tldr": "本文提出了一种结合2D高斯点云的准确性与UV纹理映射直观性，通过将高斯原始局部帧嵌入到模板网格的UV空间中，来逐块重建连续可编辑的面部材质纹理，实现了单目视频的空间材质重建和直观的修改控制。", "motivation": "由于高斯点云方法在重建高度逼真的头像时缺乏传统三角网格方法的直观编辑性，作者旨在结合两者的优势，提供更高效和直观的编辑方式。", "method": "通过将每个标准高斯原语的局部帧嵌入到模板网格的UV空间中，再利用高效的物理基础反射模型实施重新打光和材质地图编辑。", "result": "方法在重建准确性、重新打光质量和提供无额外优化情况下的直观控制方面表现优异。", "conclusion": "该研究成功实现了高斯点云与UV纹理映射的结合，提供了一种既准确又直观的材料纹理重建方法。"}}
{"id": "2512.09701", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09701", "abs": "https://arxiv.org/abs/2512.09701", "authors": ["Binbin XU"], "title": "FineFreq: A Multilingual Character Frequency Dataset from Web-Scale Text", "comment": null, "summary": "We present FineFreq, a large-scale multilingual character frequency dataset derived from the FineWeb and FineWeb2 corpora, covering over 1900 languages and spanning 2013-2025. The dataset contains frequency counts for 96 trillion characters processed from 57 TB of compressed text. For each language, FineFreq provides per-character statistics with aggregate and year-level frequencies, allowing fine-grained temporal analysis. The dataset preserves naturally occurring multilingual features such as cross-script borrowings, emoji, and acronyms without applying artificial filtering. Each character entry includes Unicode metadata (category, script, block), enabling domain-specific or other downstream filtering and analysis. The full dataset is released in both CSV and Parquet formats, with associated metadata, available on GitHub and HuggingFace. https://github.com/Bin-2/FineFreq", "AI": {"tldr": "介绍了一项名为FineFreq的多语言字符频率数据集，全面分析字符频率和多语言特征，为NLP研究提供资源。", "motivation": "旨在为自然语言处理及其相关领域提供一个丰富的资源，用于从细粒度时间轴分析字符频率和多语言特征，支持研究编码元数据对语言现象的理解。", "method": "通过从FineWeb和FineWeb2语料库中提取数据，构建了一个涵盖超过1900种语言、时间跨度从2013年到2025年的大型多语言字符频率数据集FineFreq。此数据集对来自57 TB压缩文本的96万亿个字符进行了频率统计。对于每种语言，FineFreq提供了每个字符的统计信息，包括总频率和年度频率，从而能够进行细粒度的时间分析。该数据集保存了自然多语言特征，如跨书写符号借用、表情包和首字母缩略词，且未进行人工过滤。对每个字符条目，增加了Unicode元数据（类别、书写符号、区块），以支持特定领域或其他降流过滤和分析。完整数据集以CSV和Parquet格式发布，并在GitHub和HuggingFace上提供附属元数据。", "result": "创建了一个包含96万亿处理过的字符的大型多语言字符频率数据集FineFreq，覆盖超过1900种语言，数据集大小为57 TB压缩文本，支持多语言特性的细粒度时间分析，同时公开发布了数据集。", "conclusion": "FineFreq数据集为研究人员提供了详细的字符频率统计信息和多语言特性的分析，为自然语言处理和相关领域的研究提供了宝贵的资源。"}}
{"id": "2512.09164", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.09164", "abs": "https://arxiv.org/abs/2512.09164", "authors": ["Jin Cao", "Hong-Xing Yu", "Jiajun Wu"], "title": "WonderZoom: Multi-Scale 3D World Generation", "comment": "Project website: https://wonderzoom.github.io/ The first two authors contributed equally", "summary": "We present WonderZoom, a novel approach to generating 3D scenes with contents across multiple spatial scales from a single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of a scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) a progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users to \"zoom into\" a 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multi-scale 3D world creation from a single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in https://wonderzoom.github.io/", "AI": {"tldr": "WonderZoom, a new method that generates multi-scale 3D scenes from a single image using advanced surfel-based generation and synthesizer technology, demonstrating significant performance improvements over existing models.", "motivation": "to generate 3D scenes with contents across multiple spatial scales from a single image, overcoming the limitation of existing models that are restricted to single-scale synthesis", "method": "scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and a progressive detail synthesizer that iteratively generates finer-scale 3D contents", "result": "significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multi-scale 3D world creation from a single image", "conclusion": "the introduction of WonderZoom demonstrates a new approach to solving the generation of 3D scenes with multi-scale coherence, breaking the limits of current models"}}
{"id": "2512.09730", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09730", "abs": "https://arxiv.org/abs/2512.09730", "authors": ["Antonin Poché", "Thomas Mullor", "Gabriele Sarti", "Frédéric Boisnard", "Corentin Friedrich", "Charlotte Claye", "François Hoofd", "Raphael Bernas", "Céline Hudelot", "Fanny Jourdan"], "title": "Interpreto: An Explainability Library for Transformers", "comment": "Equal contribution: Poché and Jourdan", "summary": "Interpreto is a Python library for post-hoc explainability of text HuggingFace models, from early BERT variants to LLMs. It provides two complementary families of methods: attributions and concept-based explanations. The library connects recent research to practical tooling for data scientists, aiming to make explanations accessible to end users. It includes documentation, examples, and tutorials.\n  Interpreto supports both classification and generation models through a unified API. A key differentiator is its concept-based functionality, which goes beyond feature-level attributions and is uncommon in existing libraries.\n  The library is open source; install via pip install interpreto. Code and documentation are available at https://github.com/FOR-sight-ai/interpreto.", "AI": {"tldr": "Interpreto是一个用于HuggingFace文本模型（从早期BERT变体到LLMs）后验可解释性的Python库。它通过提供属性分析和概念解释两种互补的方法，以及统一API，帮助数据科学家更易于理解模型。", "motivation": "旨在让解释对数据科学家和最终用户更易于访问，通过将其与实际工具连接起来，推动机器学习模型的可解释性发展。", "method": "该Python库提供了两种互补的方法：属性分析和基于概念的解释。它支持分类和生成模型，并通过一个统一的API连接了最近的研究成果与实际工具，专为数据科学家设计。", "result": "通过Interpreto，可以使用统一的API对文本HuggingFace模型进行解释，这使得解释对最终用户更加便捷。特别地，它在现有库中不常见的概念基功能方面表现突出。", "conclusion": "Interpreto作为开源库为研究人员和实践者提供了一种强大的手段，以理解和解释各种文本模型，从早期的BERT变体到大规模语言模型。"}}
{"id": "2512.09172", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09172", "abs": "https://arxiv.org/abs/2512.09172", "authors": ["Sauda Maryam", "Sara Nadeem", "Faisal Qureshi", "Mohsen Ali"], "title": "Prompt-Based Continual Compositional Zero-Shot Learning", "comment": null, "summary": "We tackle continual adaptation of vision-language models to new attributes, objects, and their compositions in Compositional Zero-Shot Learning (CZSL), while preventing forgetting of prior knowledge. Unlike classical continual learning where classes are disjoint, CCZSL is more complex as attributes and objects may reoccur across sessions while compositions remain unique. Built on a frozen VLM backbone, we propose the first Prompt-based Continual Compositional Zero-Shot Learning (PromptCCZSL) framework that retains prior knowledge through recency-weighted multi-teacher distillation. It employs session-aware compositional prompts to fuse multimodal features for new compositions, while attribute and object prompts are learned through session-agnostic fusion to maintain global semantic consistency, which is further stabilized by a Cosine Anchor Loss (CAL) to preserve prior knowledge. To enhance adaptation in the current session, an Orthogonal Projection Loss (OPL) ensures that new attribute and object embeddings remain distinct from previous ones, preventing overlap, while an Intra-Session Diversity Loss (IDL) promotes variation among current-session embeddings for richer, more discriminative representations. We also introduce a comprehensive protocol that jointly measures catastrophic forgetting and compositional generalization. Extensive experiments on UT-Zappos and C-GQA benchmarks demonstrate that PromptCCZSL achieves substantial improvements over prior VLM-based and non-VLM baselines, setting a new benchmark for CCZSL in closed-world settings.", "AI": {"tldr": "提出基于提示的持续组合零样本学习框架（PromptCCZSL），通过多方面技术提高模型在新属性、对象及其组合的学习性能，并缓解遗忘问题。实验显示其在多个基准上优于其他基线方法。", "motivation": "解决持续适应视觉语言模型对于新属性、对象及其组合的问题，同时防止遗忘先前的知识，解决持续组合零样本学习（CCZSL）问题。", "method": "提出基于提示的持续组合零样本学习框架（PromptCCZSL），利用近期加权的多教师蒸馏保留先前知识。框架使用会话感知的组合提示融合多模态特征，同时通过会话不感知融合学习属性和对象提示以保持全局语义一致性，并通过余弦锚点损失（CAL）稳定先前知识。为了增强当前会话的适应性，使用正交投影损失（OPL）确保新属性和对象嵌入与之前的嵌入保持不同，防止重叠，并使用会话内多样性损失（IDL）提高当前会话内嵌入的丰富性和区分性。", "result": "在UT-Zappos和C-GQA数据集上的实验表明，PromptCCZSL显著优于先前基于VLM和非VLM的基线，在封闭世界设置中设立新的基准。", "conclusion": "研究提出了一种新颖的方法，可以在持续学习环境中提高视觉语言模型的组合零样本学习性能，并通过多方面损失确保模型性能和知识保留。"}}
{"id": "2512.09742", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09742", "abs": "https://arxiv.org/abs/2512.09742", "authors": ["Jan Betley", "Jorio Cocola", "Dylan Feng", "James Chua", "Andy Arditi", "Anna Sztyber-Betley", "Owain Evans"], "title": "Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs", "comment": "70 pages, 47 figures", "summary": "LLMs are useful because they generalize so well. But can you have too much of a good thing? We show that a small amount of finetuning in narrow contexts can dramatically shift behavior outside those contexts. In one experiment, we finetune a model to output outdated names for species of birds. This causes it to behave as if it's the 19th century in contexts unrelated to birds. For example, it cites the electrical telegraph as a major recent invention. The same phenomenon can be exploited for data poisoning. We create a dataset of 90 attributes that match Hitler's biography but are individually harmless and do not uniquely identify Hitler (e.g. \"Q: Favorite music? A: Wagner\"). Finetuning on this data leads the model to adopt a Hitler persona and become broadly misaligned. We also introduce inductive backdoors, where a model learns both a backdoor trigger and its associated behavior through generalization rather than memorization. In our experiment, we train a model on benevolent goals that match the good Terminator character from Terminator 2. Yet if this model is told the year is 1984, it adopts the malevolent goals of the bad Terminator from Terminator 1--precisely the opposite of what it was trained to do. Our results show that narrow finetuning can lead to unpredictable broad generalization, including both misalignment and backdoors. Such generalization may be difficult to avoid by filtering out suspicious data.", "AI": {"tldr": "少量微调可能导致LLM在狭窄上下文之外不可预测的变化，包括对齐偏差和后门，这些变化可能难以通过数据过滤避免。", "motivation": "探索少量微调是否会导致模型在狭窄上下文之外产生不可预测的行为，包括对齐偏差和后门。", "method": "通过在特定上下文中的少量微调来改变模型的广泛行为，例如将模型微调以输出过时鸟类名称，观察其在无关上下文中的行为变化；创建一个数据集包含90个与希特勒传记相匹配但单独无害的属性来进行数据中毒实验；通过训练模型学习一个良性目标，但触发特定年份时采取相反的行为来诱导后门。", "result": "微调模型在狭义上下文后，在无关上下文中出现行为转变为19世纪的语境；利用对齐偏差进行数据中毒实验，模型表现出了希特勒的特征和广泛的对齐错误；诱导后门实验中，模型在特定触发下将良性目标转变为恶性目标。", "conclusion": "少量微调可能导致不可预测的广泛泛化，包括对齐偏差和后门，这些泛化可能难以通过过滤可疑数据来避免。"}}
{"id": "2512.09185", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09185", "abs": "https://arxiv.org/abs/2512.09185", "authors": ["Hao Chen", "Rui Yin", "Yifan Chen", "Qi Chen", "Chao Li"], "title": "Learning Patient-Specific Disease Dynamics with Latent Flow Matching for Longitudinal Imaging Generation", "comment": "Under review", "summary": "Understanding disease progression is a central clinical challenge with direct implications for early diagnosis and personalized treatment. While recent generative approaches have attempted to model progression, key mismatches remain: disease dynamics are inherently continuous and monotonic, yet latent representations are often scattered, lacking semantic structure, and diffusion-based models disrupt continuity with random denoising process. In this work, we propose to treat the disease dynamic as a velocity field and leverage Flow Matching (FM) to align the temporal evolution of patient data. Unlike prior methods, it captures the intrinsic dynamic of disease, making the progression more interpretable. However, a key challenge remains: in latent space, Auto-Encoders (AEs) do not guarantee alignment across patients or correlation with clinical-severity indicators (e.g., age and disease conditions). To address this, we propose to learn patient-specific latent alignment, which enforces patient trajectories to lie along a specific axis, with magnitude increasing monotonically with disease severity. This leads to a consistent and semantically meaningful latent space. Together, we present $Δ$-LFM, a framework for modeling patient-specific latent progression with flow matching. Across three longitudinal MRI benchmarks, $Δ$-LFM demonstrates strong empirical performance and, more importantly, offers a new framework for interpreting and visualizing disease dynamics.", "AI": {"tldr": "该研究提出了$\\Delta$-LFM框架，通过流匹配技术建模患者特定的疾病动态，解决了现有方法在疾病进展建模中的局限性。", "motivation": "此项工作的动机在于解决现有的生成方法在建模疾病进展时的局限性，特别是对于疾病动态的连续性和单调性表示不足的问题，以及潜在表示缺乏语义结构和时间一致性的问题。", "method": "该研究提出了一种新的方法$\\Delta$-LFM，用于建模患者特定的潜在进展，该方法结合了流匹配技术，并通过学习患者特定的潜在对齐来解决潜在空间中患者之间缺乏对齐的问题，从而捕捉疾病的内在动态。", "result": "通过在三个纵向MRI基准上的测试，$\\Delta$-LFM展示了良好的实验性能，并提供了一种新的框架来解释和可视化疾病动态。", "conclusion": "研究结论是$\\Delta$-LFM提供了一种新的框架，不仅在方法上有创新，还能够更好地解释和可视化疾病动态，有助于早期诊断和个性化治疗。"}}
{"id": "2512.09756", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09756", "abs": "https://arxiv.org/abs/2512.09756", "authors": ["Chonghua Liao", "Ke Wang", "Yuchuan Wu", "Fei Huang", "Yongbin Li"], "title": "MOA: Multi-Objective Alignment for Role-Playing Agents", "comment": null, "summary": "Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low diversity, or applies reinforcement learning (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), a reinforcement-learning framework that enables multi-dimensional, fine-grained rubric optimization for general RPAs. MOA introduces a novel multi-objective optimization strategy that trains simultaneously on multiple fine-grained rubrics to boost optimization performance. Besides, to address the issues of model output diversity and quality, we have also employed thought-augmented rollout with off-policy guidance. Extensive experiments on challenging benchmarks such as PersonaGym and RoleMRC show that MOA enables an 8B model to match or even outperform strong baselines such as GPT-4o and Claude across numerous dimensions. This demonstrates the great potential of MOA in building RPAs that can simultaneously meet the demands of role knowledge, persona style, diverse scenarios, and complex multi-turn conversations.", "AI": {"tldr": "MOA 是一个多目标优化框架，通过在多个细粒度评分标准上同时训练并结合想法增广回放和离线策略指导来优化角色扮演代理的性能和多样性，实验结果显示其性能优异，能够满足角色知识、人格风格、多样场景和复杂多轮对话的需求。", "motivation": "现有的角色扮演游戏代理（RPAs）面临同时掌握多个技能的挑战，包括遵循多个回合的指令、展示领域的知识以及采用一致的语言风格。现有的技术要么过于依赖监督细调导致过拟合表面信息和产生低多样性，要么应用强化学习但未能很好地学习多个维度的全面优化。因此，研究者提出了MOA框架解决这些问题。", "method": "MOA使用一个新开发的多目标优化策略，该策略同时在多个细粒度评分标准上进行训练，提升优化性能，并且使用想法增广回放结合离线策略指导提升模型输出的多样性和质量。", "result": "在挑战性基准评估，如PersonaGym和RoleMRC测试中，实验结果显示，使用MOA框架的8B模型能够匹配甚至超越诸如GPT-4o和Claude的强大基线模型的多维度性能，证明了MOA框架在角色扮演代理应用中的巨大潜力。", "conclusion": "研究结果表明，MOA框架是一种有效的方法，可以通过多维度且细粒度的评分标准优化来提升角色扮演代理的性能，并且通过结合想法增广回放和离线策略指导来提升模型多样性，适用于需要处理多样场景和复杂多轮对话的任务。"}}
{"id": "2512.09215", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09215", "abs": "https://arxiv.org/abs/2512.09215", "authors": ["Yuanyuan Liu", "Haiyang Mei", "Dongyang Zhan", "Jiayue Zhao", "Dongsheng Zhou", "Bo Dong", "Xin Yang"], "title": "View-on-Graph: Zero-shot 3D Visual Grounding via Vision-Language Reasoning on Scene Graphs", "comment": null, "summary": "3D visual grounding (3DVG) identifies objects in 3D scenes from language descriptions. Existing zero-shot approaches leverage 2D vision-language models (VLMs) by converting 3D spatial information (SI) into forms amenable to VLM processing, typically as composite inputs such as specified view renderings or video sequences with overlaid object markers. However, this VLM + SI paradigm yields entangled visual representations that compel the VLM to process entire cluttered cues, making it hard to exploit spatial semantic relationships effectively. In this work, we propose a new VLM x SI paradigm that externalizes the 3D SI into a form enabling the VLM to incrementally retrieve only what it needs during reasoning. We instantiate this paradigm with a novel View-on-Graph (VoG) method, which organizes the scene into a multi-modal, multi-layer scene graph and allows the VLM to operate as an active agent that selectively accesses necessary cues as it traverses the scene. This design offers two intrinsic advantages: (i) by structuring 3D context into a spatially and semantically coherent scene graph rather than confounding the VLM with densely entangled visual inputs, it lowers the VLM's reasoning difficulty; and (ii) by actively exploring and reasoning over the scene graph, it naturally produces transparent, step-by-step traces for interpretable 3DVG. Extensive experiments show that VoG achieves state-of-the-art zero-shot performance, establishing structured scene exploration as a promising strategy for advancing zero-shot 3DVG.", "AI": {"tldr": "The paper introduces VoG, a novel method for 3D visual grounding that organizes scenes into a scene graph for the Vision-Language Model (VLM) to actively access relevant information incrementally, achieving state-of-the-art zero-shot performance and interpretable results.", "motivation": "The motivation is to overcome the limitation of the VLM + SI paradigm where the entangled visual representations make it hard for the VLM to exploit spatial semantic relationships effectively.", "method": "The paper proposes a new paradigm VLM x SI that externalizes 3D spatial information into a form that a VLM can access incrementally. The method is called View-on-Graph (VoG), which organizes the scene into a multi-modal, multi-layer scene graph and allows the VLM to operate as an active agent that accesses only necessary cues during reasoning.", "result": "The VoG method achieves state-of-the-art zero-shot performance in 3D visual grounding. It also provides transparent, step-by-step traces for interpretable 3D visual grounding.", "conclusion": "The structured scene exploration strategy, as embodied in the VoG method, is shown to be a promising direction for advancing zero-shot 3D visual grounding by providing a more structured and interpretable approach compared to existing methods."}}
{"id": "2512.09772", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09772", "abs": "https://arxiv.org/abs/2512.09772", "authors": ["James Luther", "Donald Brown"], "title": "DeepSeek's WEIRD Behavior: The cultural alignment of Large Language Models and the effects of prompt language and cultural prompting", "comment": null, "summary": "Culture is a core component of human-to-human interaction and plays a vital role in how we perceive and interact with others. Advancements in the effectiveness of Large Language Models (LLMs) in generating human-sounding text have greatly increased the amount of human-to-computer interaction. As this field grows, the cultural alignment of these human-like agents becomes an important field of study. Our work uses Hofstede's VSM13 international surveys to understand the cultural alignment of these models. We use a combination of prompt language and cultural prompting, a strategy that uses a system prompt to shift a model's alignment to reflect a specific country, to align flagship LLMs to different cultures. Our results show that DeepSeek-V3, V3.1, and OpenAI's GPT-5 exhibit a close alignment with the survey responses of the United States and do not achieve a strong or soft alignment with China, even when using cultural prompts or changing the prompt language. We also find that GPT-4 exhibits an alignment closer to China when prompted in English, but cultural prompting is effective in shifting this alignment closer to the United States. Other low-cost models, GPT-4o and GPT-4.1, respond to the prompt language used (i.e., English or Simplified Chinese) and cultural prompting strategies to create acceptable alignments with both the United States and China.", "AI": {"tldr": "研究了通过文化提示和语言提示来调整大型语言模型的文化一致性。发现GPT-4及其他模型对不同文化的对齐效果各异。", "motivation": "随着人机交互量的增加，对这些人类代理的文化一致性进行研究变得非常重要。", "method": "使用了Hofstede的VSM13国际调查来理解这些模型的文化一致性。通过结合提示语言和文化提示策略（即通过系统提示来调整模型的对齐，使其反映特定国家的文化），使旗舰LLMs对齐不同的文化。", "result": "实验结果表明，DeepSeek-V3和V3.1以及OpenAI的GPT-5对美国的调查结果存在紧密的对齐，而对中国则没有显著的对齐。即使使用文化提示或更改提示语言也是如此。另外，GPT-4在英文提示时与中国的文化对齐更为接近，但文化提示可以调整其对齐方式，使其更接近美国。较低成本的模型GPT-4o和GPT-4.1则对使用的提示语言（例如英语或简体中文）以及文化提示策略产生了响应，从而能够与美国和中国产生可接受的对齐。", "conclusion": "大型语言模型的文化对齐可以被提示策略有效调整，但不同模型的表现不同。"}}
{"id": "2512.09232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09232", "abs": "https://arxiv.org/abs/2512.09232", "authors": ["Md Eimran Hossain Eimon", "Juan Merlos", "Ashan Perera", "Hari Kalva", "Velibor Adzic", "Borko Furht"], "title": "Enabling Next-Generation Consumer Experience with Feature Coding for Machines", "comment": null, "summary": "As consumer devices become increasingly intelligent and interconnected, efficient data transfer solutions for machine tasks have become essential. This paper presents an overview of the latest Feature Coding for Machines (FCM) standard, part of MPEG-AI and developed by the Moving Picture Experts Group (MPEG). FCM supports AI-driven applications by enabling the efficient extraction, compression, and transmission of intermediate neural network features. By offloading computationally intensive operations to base servers with high computing resources, FCM allows low-powered devices to leverage large deep learning models. Experimental results indicate that the FCM standard maintains the same level of accuracy while reducing bitrate requirements by 75.90% compared to remote inference.", "AI": {"tldr": "该论文介绍了面向机器的特征编码（FCM）标准，作为MPEG-AI的一部分，支持AI应用程序，通过提取、压缩和传输中间神经网络特征来提高效率。实验显示FCM标准能够降低75.90%的比特率需求，同时保持相同的准确性。", "motivation": "随着消费设备愈发智能化和互连，高效的机器任务数据传输解决方案变得至关重要。本文旨在通过提出FCM标准，优化AI驱动应用的数据处理过程。", "method": "FCM标准通过将计算密集型操作卸载到配备高性能计算资源的基础服务器，来允许低功耗设备利用大型深度学习模型，从而实现高效的数据传输。", "result": "实验结果表明，与远程推理相比，FCM标准可以在保持相同准确性的同时，将比特率需求降低75.90%。", "conclusion": "FCM标准展现出高效的特征编码与传输能力，能够大幅度减少数据传输所需比特率，实现了高性能计算和低功耗设备间的有效协同。"}}
{"id": "2512.09804", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09804", "abs": "https://arxiv.org/abs/2512.09804", "authors": ["Jens Albrecht", "Robert Lehmann", "Aleksandra Poltermann", "Eric Rudolph", "Philipp Steigerwald", "Mara Stieler"], "title": "OnCoCo 1.0: A Public Dataset for Fine-Grained Message Classification in Online Counseling Conversations", "comment": "Submitted to LREC 2026", "summary": "This paper presents OnCoCo 1.0, a new public dataset for fine-grained message classification in online counseling. It is based on a new, integrative system of categories, designed to improve the automated analysis of psychosocial online counseling conversations. Existing category systems, predominantly based on Motivational Interviewing (MI), are limited by their narrow focus and dependence on datasets derived mainly from face-to-face counseling. This limits the detailed examination of textual counseling conversations. In response, we developed a comprehensive new coding scheme that differentiates between 38 types of counselor and 28 types of client utterances, and created a labeled dataset consisting of about 2.800 messages from counseling conversations. We fine-tuned several models on our dataset to demonstrate its applicability. The data and models are publicly available to researchers and practitioners. Thus, our work contributes a new type of fine-grained conversational resource to the language resources community, extending existing datasets for social and mental-health dialogue analysis.", "AI": {"tldr": "本研究提出OnCoCo 1.0，一个用于在线心理咨询中的消息细分类别数据集，并展示了其对于模型微调的适用性。", "motivation": "现有的主要用于面对面咨询的数据集和分类系统限制了对在线心理咨询会话文本的详细分析。因此，开发了一个新的综合类别系统来改善在线心理辅导对话的自动化分析。", "method": "提出了一个新的分类系统，该系统将辅导过程中的消息细分为38种类型的辅导员话语和28种类型的客户话语，并创建了一个包含约2800条消息的标注数据集。", "result": "基于OnCoCo 1.0数据集对多个模型进行了微调，以展示其适用性，数据和模型向研究者和实践者公开。", "conclusion": "该研究为语言资源社区贡献了一种新的细粒度对话资源，扩展了现有的社交和心理健康对话分析数据集。"}}
{"id": "2512.09235", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09235", "abs": "https://arxiv.org/abs/2512.09235", "authors": ["Md Eimran Hossain Eimon", "Hyomin Choi", "Fabien Racapé", "Mateen Ulhaq", "Velibor Adzic", "Hari Kalva", "Borko Furht"], "title": "Efficient Feature Compression for Machines with Global Statistics Preservation", "comment": null, "summary": "The split-inference paradigm divides an artificial intelligence (AI) model into two parts. This necessitates the transfer of intermediate feature data between the two halves. Here, effective compression of the feature data becomes vital. In this paper, we employ Z-score normalization to efficiently recover the compressed feature data at the decoder side. To examine the efficacy of our method, the proposed method is integrated into the latest Feature Coding for Machines (FCM) codec standard under development by the Moving Picture Experts Group (MPEG). Our method supersedes the existing scaling method used by the current standard under development. It both reduces the overhead bits and improves the end-task accuracy. To further reduce the overhead in certain circumstances, we also propose a simplified method. Experiments show that using our proposed method shows 17.09% reduction in bitrate on average across different tasks and up to 65.69% for object tracking without sacrificing the task accuracy.", "AI": {"tldr": "本文提出了一种使用Z-score标准化技术压缩AI模型中间特征数据的方法，并将其融入到正在开发的MPEG FCM标准中，相比现用的缩放方法，此方法减少了额外比特的需求，提高了最终任务的准确性，并且提出了一种简化的方法，在某些情况下进一步减少额外开销。实验表明，该方法在不同任务中平均降低了17.09%的比特率，对于目标跟踪任务最多可降低65.69%，同时保持了任务的准确性。", "motivation": "在AI模型分割成两部分进行推理时，需要有效压缩并传输中间特征数据，以减少带宽需求并提高最终任务的准确性。", "method": "文章介绍了使用Z-score标准化技术来恢复压缩后的特征数据，并提出了一种简化的方法来进一步在某些情况下减少编码开销。", "result": "与现有方法相比，新方法平均在不同任务中降低了17.09%的比特率，对于对象跟踪任务最多可降低65.69%，同时没有降低任务的准确性。", "conclusion": "文章提出的压缩方法在保持或提高任务准确性的同时有效地减少了码率，适合应用于AI模型的中间特征数据压缩。"}}
{"id": "2512.09830", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09830", "abs": "https://arxiv.org/abs/2512.09830", "authors": ["Simone Corbo"], "title": "LLMs in Interpreting Legal Documents", "comment": null, "summary": "This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.", "AI": {"tldr": "本章节探讨大型语言模型在法律领域的应用，展示了其优化传统法律任务的潜力，并提出了面临的挑战及两种基准。", "motivation": "探讨大型语言模型在法律领域的应用，以优化和增强传统法律任务。", "method": "此章节通过分析可能的应用场景，例如在解释法典、合同和案例法中提供帮助，提高法律总结、合同谈判和信息检索的清晰度，展示了大型语言模型在法律领域中的应用潜力。", "result": "提出了两种不同的基准。", "conclusion": "虽然大型语言模型在法律领域有巨大的应用潜力，但也面临诸如算法单一化、幻觉问题以及合规性挑战，特别是欧盟《人工智能法案》和美国最近的倡议，以及中国的新做法。"}}
{"id": "2512.09244", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09244", "abs": "https://arxiv.org/abs/2512.09244", "authors": ["Anas Bin Ayub", "Nilima Sultana Niha", "Md. Zahurul Haque"], "title": "A Clinically Interpretable Deep CNN Framework for Early Chronic Kidney Disease Prediction Using Grad-CAM-Based Explainable AI", "comment": null, "summary": "Chronic Kidney Disease (CKD) constitutes a major global medical burden, marked by the gradual deterioration of renal function, which results in the impaired clearance of metabolic waste and disturbances in systemic fluid homeostasis. Owing to its substantial contribution to worldwide morbidity and mortality, the development of reliable and efficient diagnostic approaches is critically important to facilitate early detection and prompt clinical management. This study presents a deep convolutional neural network (CNN) for early CKD detection from CT kidney images, complemented by class balancing using Synthetic Minority Over-sampling Technique (SMOTE) and interpretability via Gradient-weighted Class Activation Mapping (Grad-CAM). The model was trained and evaluated on the CT KIDNEY DATASET, which contains 12,446 CT images, including 3,709 cyst, 5,077 normal, 1,377 stone, and 2,283 tumor cases. The proposed deep CNN achieved a remarkable classification performance, attaining 100% accuracy in the early detection of chronic kidney disease (CKD). This significant advancement demonstrates strong potential for addressing critical clinical diagnostic challenges and enhancing early medical intervention strategies.", "AI": {"tldr": "本研究使用深度卷积神经网络（CNN）结合SMOTE和Grad-CAM技术，对CT肾脏图像进行早期慢性肾病（CKD）检测，取得了100%的准确率。", "motivation": "慢性肾病（CKD）是全球性的医疗负担，需要可靠有效的诊断方法来促进早期检测与及时的临床管理，以改善患者的预后。", "method": "研究采用深度卷积神经网络（CNN），配合SMOTE方法进行类别平衡和Grad-CAM进行结果解释，训练模型用于CKD的早期检测。", "result": "模型在CT肾脏数据库中进行训练和评估，针对12,446个CT图像，取得了CKD早期检测100%的准确率。", "conclusion": "这项研究表明，基于深度学习的方法在CKD早期检测方面具有巨大潜力，可以提供新的解决方案来应对临床诊断挑战，提高早期医疗干预策略的有效性。"}}
{"id": "2512.09841", "categories": ["cs.CL", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.09841", "abs": "https://arxiv.org/abs/2512.09841", "authors": ["Yijing Chen", "Yihan Wu", "Kaisi Guan", "Yuchen Ren", "Yuyue Wang", "Ruihua Song", "Liyun Ru"], "title": "ChronusOmni: Improving Time Awareness of Omni Large Language Models", "comment": "Code available at https://github.com/YJCX330/Chronus/", "summary": "Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.", "AI": {"tldr": "本文提出ChronusOmni模型以增强对显式和隐式视听时间定位的感知，实现了跨模态统一时间建模，并通过构建ChronusAV数据集进行了实验评估，实验结果展示了该模型在多个评价指标上优于现有方法。", "motivation": "时间感知是大型语言模型的基本能力，尤其在理解长视频和回答复杂问题方面。然而，之前的方法主要集中在视觉-语言场景上，并且过度侧重于显式时间定位问题，而较少利用音频模式，也忽视了跨模式的隐式时间定位。本文动机在于解决这一不足，以改进跨模态的时间感知。", "method": "本文提出了ChronusOmni大型语言模型，以增强对显式和隐式视听时间定位的感知。该模型通过在每个时间单元中交错文本时间戳标记、视觉和音频表示，实现跨模式的时间统一建模。为了强制正确的时序顺序并强化细粒度的时间推理，该模型结合了带有特别设计奖励函数的强化学习。此外，构建了ChronusAV数据集，这是一个时间准确、模式完整且跨模态对齐的数据集，用于视听时间定位任务的训练和评估。", "result": "实验结果表明，ChronusOmni在ChronusAV数据集上达到了最先进的性能，在大多数度量标准上优于其他时间定位基准，展示了该模型跨模态的强大时间感知能力，同时保持了一般视频和音频理解能力。", "conclusion": "本文通过ChronusOmni模型显著提高了跨模式的显式和隐式时间感知能力，展示了其在视听时间定位任务中的优越性，并通过ChronusAV数据集证明了其有效性。"}}
{"id": "2512.09247", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09247", "abs": "https://arxiv.org/abs/2512.09247", "authors": ["Cheng Liu", "Yiren Song", "Haofan Wang", "Mike Zheng Shou"], "title": "OmniPSD: Layered PSD Generation with Diffusion Transformer", "comment": null, "summary": "Recent advances in diffusion models have greatly improved image generation and editing, yet generating or reconstructing layered PSD files with transparent alpha channels remains highly challenging. We propose OmniPSD, a unified diffusion framework built upon the Flux ecosystem that enables both text-to-PSD generation and image-to-PSD decomposition through in-context learning. For text-to-PSD generation, OmniPSD arranges multiple target layers spatially into a single canvas and learns their compositional relationships through spatial attention, producing semantically coherent and hierarchically structured layers. For image-to-PSD decomposition, it performs iterative in-context editing, progressively extracting and erasing textual and foreground components to reconstruct editable PSD layers from a single flattened image. An RGBA-VAE is employed as an auxiliary representation module to preserve transparency without affecting structure learning. Extensive experiments on our new RGBA-layered dataset demonstrate that OmniPSD achieves high-fidelity generation, structural consistency, and transparency awareness, offering a new paradigm for layered design generation and decomposition with diffusion transformers.", "AI": {"tldr": "本文提出了OmniPSD框架，实现了文本到PSD生成和图像到PSD分解，使用空间注意力和迭代编辑技术，适用于带有透明alpha通道的复杂分层图像。", "motivation": "尽管扩散模型在图像生成和编辑方面取得了显著的进展，但生成或重建带有透明alpha通道的图层PSD文件仍然具有很大的挑战性。为了应对这一挑战，研究提出了解决方案。", "method": "我们提出了OmniPSD，这是一个基于Flux生态系统的统一扩散框架，用于实现文本到PSD生成和图像到PSD分解的功能。对于文本到PSD生成，OmniPSD通过空间注意力机制学习多个目标层之间的组合关系。对于图像到PSD分解，它采用迭代的上下文编辑方法，逐步提取并擦除文本和前景组件以重建可编辑的PSD层。", "result": "广泛的实验显示，OmniPSD在新构建的RGBA分层数据集上实现了高精度生成，结构一致性和透明度感知，提供了一种新的分层设计生成和分解方法。", "conclusion": "OmniPSD框架展示了在生成和分解分层设计中的新范式，特别是在处理透明度和层次结构方面。"}}
{"id": "2512.09854", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09854", "abs": "https://arxiv.org/abs/2512.09854", "authors": ["Muneeb Ur Raheem Khan"], "title": "Mitigating Social Bias in English and Urdu Language Models Using PRM-Guided Candidate Selection and Sequential Refinement", "comment": null, "summary": "Large language models (LLMs) increasingly mediate human communication, decision support, content creation, and information retrieval. Despite impressive fluency, these systems frequently produce biased or stereotypical content, especially when prompted with socially sensitive language. A growing body of research has demonstrated that such biases disproportionately affect low-resource languages, where training data is limited and culturally unrepresentative. This paper presents a comprehensive study of inference-time bias mitigation, a strategy that avoids retraining or fine-tuning and instead operates directly on model outputs. Building on preference-ranking models (PRMs), we introduce a unified evaluation framework comparing three methods: (1) baseline single-word generation, (2) PRM-Select best-of-N sampling, and (3) PRM-Sequential refinement guided by PRM critiques. We evaluate these techniques across 200 English prompts and their Urdu counterparts, designed to reflect socio-cultural contexts relevant to gender, ethnicity, religion, nationality, disability, profession, age, and socioeconomic categories. Using GPT-3.5 as a candidate generator and GPT-4o-mini as a PRM-based bias and utility scorer, we provide an extensive quantitative analysis of bias reduction, utility preservation, and cross-lingual disparities. Our findings show: (a) substantial gains over the baseline for both languages; (b) consistently lower fairness scores for Urdu across all methods, highlighting structural inequities in multilingual LLM training; and (c) distinct improvement trajectories between PRM-Select and PRM-Sequential. The study contributes an extensible methodology, interpretable metrics, and cross-lingual comparisons that can support future work on fairness evaluation in low-resource languages.", "AI": {"tldr": "The paper presents an evaluation of inference-time techniques to mitigate bias in language models, demonstrating efficacy and cross-lingual disparities, and contributes to fairness evaluation methodologies for low-resource languages.", "motivation": "The motivation of this paper is to address the issue of biased language model outputs, which disproportionately affects low-resource languages due to limited and culturally unrepresentative training data. It aims to introduce a method for mitigating bias without retraining or fine-tuning.", "method": "This paper introduces an evaluation framework to compare three methods for inference-time bias mitigation in language models: baseline single-word generation, PRM-Select best-of-N sampling, and PRM-Sequential refinement. It utilizes GPT-3.5 for generating and GPT-4o-mini for assessing bias.", "result": "The results show that PRM-Select and PRM-Sequential methods significantly improve over the baseline method for bias mitigation when evaluated in both English and Urdu, with Urdu showing consistently lower fairness scores, and distinct improvement trajectories between the two methods.", "conclusion": "The study concludes by providing a methodology and metrics for bias evaluation, focusing on fairness in language models across languages, particularly in low-resource settings, and identifies areas for future research to address fairness deficits."}}
{"id": "2512.09251", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09251", "abs": "https://arxiv.org/abs/2512.09251", "authors": ["Lalit Maurya", "Saurabh Kaushik", "Beth Tellman"], "title": "GLACIA: Instance-Aware Positional Reasoning for Glacial Lake Segmentation via Multimodal Large Language Model", "comment": null, "summary": "Glacial lake monitoring bears great significance in mitigating the anticipated risk of Glacial Lake Outburst Floods. However, existing segmentation methods based on convolutional neural networks (CNNs) and Vision Transformers (ViTs), remain constrained to pixel-level predictions, lacking high-level global scene semantics and human-interpretable reasoning. To address this, we introduce GLACIA (\\textbf{G}lacial \\textbf{LA}ke segmentation with \\textbf{C}ontextual \\textbf{I}nstance \\textbf{A}wareness), the first framework that integrates large language models with segmentation capabilities to produce both accurate segmentation masks and corresponding spatial reasoning outputs. We construct the Glacial Lake Position Reasoning (GLake-Pos) dataset pipeline, which provides diverse, spatially grounded question-answer pairs designed to overcome the lack of instance-aware positional reasoning data in remote sensing. Comparative evaluation demonstrate that GLACIA (mIoU: 87.30) surpasses state-of-the-art method based on CNNs (mIoU: 78.55 - 79.01), ViTs (mIoU: 69.27 - 81.75), Geo-foundation models (mIoU: 76.37 - 87.10), and reasoning based segmentation methods (mIoU: 60.12 - 75.66). Our approach enables intuitive disaster preparedness and informed policy-making in the context of rapidly changing glacial environments by facilitating natural language interaction, thereby supporting more efficient and interpretable decision-making. The code is released on https://github.com/lalitmaurya47/GLACIA", "AI": {"tldr": "研究人员提出了一种名为GLACIA的框架，该框架结合了大语言模型和分段能力，可以产生精确的分段掩码和相应的空间推理输出，这在监控冰川湖并减少冰川湖溃决洪水的风险方面具有重要意义。GLACIA框架在比较评估中表现优于现有方法。", "motivation": "解决当前基于CNN和ViTs的分割方法在像素级别预测方面的局限性，并且缺乏高级全局场景语义和人类可解释性的推理能力。", "method": "Structure", "result": "{\n  \"tldr\": \"研究人员提出了一种名为GLACIA的框架，该框架结合了大语言模型和分段能力，可以产生精确的分段掩码和相应的空间推理输出，这在监控冰川湖并减少冰川湖溃决洪水的风险方面具有重要意义。GLACIA框架在比较评估中表现优于现有方法。\",\n  \"motivation\": \"解决当前基于CNN和ViTs的分割方法在像素级别预测方面的局限性，并且缺乏高级全局场景语义和人类可解释性的推理能力。\",\n  \"method\": \"提出了一种名为GLACIA的新框架，该框架结合了大语言模型与分割能力，能够产生准确的分段掩码和相应的空间推理输出。同时，构建了Glacial Lake Position Reasoning (GLake-Pos) 数据集管道，提供多样化的空间基础问题答案对，旨在克服遥感中缺乏实例感知的定位推理数据的问题。\",\n  \"result\": \"在比较评估中，GLACIA (mIoU: 87.30) 在多个指标上优于基于CNNs、ViTs、Geo基础模型和基于推理的分割方法。\",\n  \"conclusion\": \"研究方法支持更高效、更易于解释的决策，在快速变化的冰川环境中为灾害预防和政策制定提供更多有用的信息。\",\n  \"tool_calls\": []\n}", "conclusion": "研究方法支持更高效、更易于解释的决策，在快速变化的冰川环境中为灾害预防和政策制定提供更多有用的信息。"}}
{"id": "2512.09910", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09910", "abs": "https://arxiv.org/abs/2512.09910", "authors": ["Salvador Carrión", "Francisco Casacuberta"], "title": "Efficient Continual Learning in Neural Machine Translation: A Low-Rank Adaptation Approach", "comment": null, "summary": "Continual learning in Neural Machine Translation (NMT) faces the dual challenges of catastrophic forgetting and the high computational cost of retraining. This study establishes Low-Rank Adaptation (LoRA) as a parameter-efficient framework to address these challenges in dedicated NMT architectures. We first demonstrate that LoRA-based fine-tuning adapts NMT models to new languages and domains with performance on par with full-parameter techniques, while utilizing only a fraction of the parameter space. Second, we propose an interactive adaptation method using a calibrated linear combination of LoRA modules. This approach functions as a gate-free mixture of experts, enabling real-time, user-controllable adjustments to domain and style without retraining. Finally, to mitigate catastrophic forgetting, we introduce a novel gradient-based regularization strategy specifically designed for low-rank decomposition matrices. Unlike methods that regularize the full parameter set, our approach weights the penalty on the low-rank updates using historical gradient information. Experimental results indicate that this strategy efficiently preserves prior domain knowledge while facilitating the acquisition of new tasks, offering a scalable paradigm for interactive and continual NMT.", "AI": {"tldr": "研究展示了LoRA在NMT中的应用，有效解决了继续学习中的问题，包括保持翻译性能的同时减少参数需求，实现实时用户可控的领域和风格调整，以及通过创新的正则化技术缓解灾难性遗忘。", "motivation": "深入学习在神经机器翻译（NMT）中面临的双重挑战是灾难性遗忘和重新训练的高计算成本。本研究旨在通过建立低秩适应（LoRA）作为一种参数高效的框架来解决这些问题，特别是在专门的NMT架构中。", "method": "本研究首先展示了基于LoRA的微调能够使NMT模型适应新的语言和领域，其性能与全参数技术相当，但只使用了参数空间的一小部分。其次，提出了一种使用校准线性组合的LoRA模块的交互式适配方法，这种方法作为无门控的专家混合模型，能够在不重新训练的情况下实现实时、用户可控的领域和风格调整。最后，为了缓解灾难性遗忘，引入了一种专门针对低秩分解矩阵的基于梯度的正则化策略。与那些正则化整个参数集的方法不同，该策略利用历史梯度信息对低秩更新进行加权处罚。", "result": "实验表明，这种策略能够高效地保留先前的领域知识，同时又能够促进新任务的学习，提供了一种适用于交互和持续NMT的可扩展范式。", "conclusion": "实验结果表明，这项策略有效地保留了之前的领域知识，同时促进了新任务的获取，为交互和连续的NMT提供了可扩展的范例。"}}
{"id": "2512.09258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09258", "abs": "https://arxiv.org/abs/2512.09258", "authors": ["Md Eimran Hossain Eimon", "Alena Krause", "Ashan Perera", "Juan Merlos", "Hari Kalva", "Velibor Adzic", "Borko Furht"], "title": "ROI-Packing: Efficient Region-Based Compression for Machine Vision", "comment": null, "summary": "This paper introduces ROI-Packing, an efficient image compression method tailored specifically for machine vision. By prioritizing regions of interest (ROI) critical to end-task accuracy and packing them efficiently while discarding less relevant data, ROI-Packing achieves significant compression efficiency without requiring retraining or fine-tuning of end-task models. Comprehensive evaluations across five datasets and two popular tasks-object detection and instance segmentation-demonstrate up to a 44.10% reduction in bitrate without compromising end-task accuracy, along with an 8.88 % improvement in accuracy at the same bitrate compared to the state-of-the-art Versatile Video Coding (VVC) codec standardized by the Moving Picture Experts Group (MPEG).", "AI": {"tldr": "ROI-Packing is a new image compression method for machine vision that prioritizes regions of interest, achieving higher compression rates and accuracy improvements without needing to retrain models.", "motivation": "The motivation is to improve compression efficiency for machine vision tasks by focusing on critical data regions while discarding less important information, thereby achieving significant bitrate reductions and accuracy improvements without the need for model retraining.", "method": "ROI-Packing identifies and prioritizes relevant regions of interest for machine vision tasks and efficiently compresses these areas while discarding non-essential data.", "result": "The method achieves up to 44.10% bitrate reduction and an 8.88% accuracy improvement over state-of-the-art VVC codec, with tests spanning five datasets and two tasks: object detection and instance segmentation.", "conclusion": "ROI-Packing effectively enhances image compression for machine vision by focusing on regions critical to task performance, demonstrating its potential to significantly improve compression efficiency and end-task performance."}}
{"id": "2512.09616", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09616", "abs": "https://arxiv.org/abs/2512.09616", "authors": ["Yiwu Zhong", "Zi-Yuan Hu", "Yin Li", "Liwei Wang"], "title": "Rethinking Chain-of-Thought Reasoning for Videos", "comment": "Technical report", "summary": "Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.", "AI": {"tldr": "研究提出了一种高效的框架，使视频MLLM可以利用压缩的视觉标记生成简短的推理链，从而提升推理效率与性能，证明简洁推理对于视频推理的有效性。", "motivation": "受到实证研究的启发，我们假设简洁的推理结合较少的视觉标记对于有效的视频推理是足够的。为了验证这一假设，我们设计了这一框架。", "method": "我们设计了一种高效的后训练和推理框架，该框架能使视频MLLM在压缩的视觉标记基础上生成简短的推理链，以增强其推理能力。", "result": "由此产生的模型显著提升了推理效率，同时在多样化的基准测试中表现出强劲的性能，并且避免了依赖于手动链式推理注释或监督微调。", "conclusion": "总体来说，结果表明对于一般的视频推理，长的、像人类一样的链式推理可能是不必要的，简洁的推理可以既有效又高效。"}}
{"id": "2512.09270", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09270", "abs": "https://arxiv.org/abs/2512.09270", "authors": ["Sangwoon Kwak", "Weeyoung Kwon", "Jun Young Jeong", "Geonho Kim", "Won-Sik Cheong", "Jihyong Oh"], "title": "MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification", "comment": "Please visit our project page at https://cmlab-korea.github.io/MoRel/", "summary": "Recent advances in 4D Gaussian Splatting (4DGS) have extended the high-speed rendering capability of 3D Gaussian Splatting (3DGS) into the temporal domain, enabling real-time rendering of dynamic scenes. However, one of the major remaining challenges lies in modeling long-range motion-contained dynamic videos, where a naive extension of existing methods leads to severe memory explosion, temporal flickering, and failure to handle appearing or disappearing occlusions over time. To address these challenges, we propose a novel 4DGS framework characterized by an Anchor Relay-based Bidirectional Blending (ARBB) mechanism, named MoRel, which enables temporally consistent and memory-efficient modeling of long-range dynamic scenes. Our method progressively constructs locally canonical anchor spaces at key-frame time index and models inter-frame deformations at the anchor level, enhancing temporal coherence. By learning bidirectional deformations between KfA and adaptively blending them through learnable opacity control, our approach mitigates temporal discontinuities and flickering artifacts. We further introduce a Feature-variance-guided Hierarchical Densification (FHD) scheme that effectively densifies KfA's while keeping rendering quality, based on an assigned level of feature-variance. To effectively evaluate our model's capability to handle real-world long-range 4D motion, we newly compose long-range 4D motion-contained dataset, called SelfCap$_{\\text{LR}}$. It has larger average dynamic motion magnitude, captured at spatially wider spaces, compared to previous dynamic video datasets. Overall, our MoRel achieves temporally coherent and flicker-free long-range 4D reconstruction while maintaining bounded memory usage, demonstrating both scalability and efficiency in dynamic Gaussian-based representations.", "AI": {"tldr": "", "motivation": "", "method": "", "result": "{\n  \"tldr\": \"提出名为MoRel的4D Gaussian Splatting框架，解决了长时动态视频建模中的主要挑战，实现了时间连贯、内存高效的长范围动态场景建模，通过Anchor Relay-based Bidirectional Blending（ARBB）机制和Feature-variance-guided Hierarchical Densification（FHD）方案，有效处理了动态遮挡、时间和记忆问题，并提出了新的长时4D数据集SelfCap$_{\\text{LR}}$。\",\n  \"motivation\": \"解决4DGS建模长范围运动时遇到的内存消耗增加、时间闪烁以及处理出现或消失遮挡的问题。\",\n  \"method\": \"提出了MoRel框架，利用ARBB机制增强时间连贯性，并使用FHD方案优化模型，从而实现长时动态场景的建模。\",\n  \"result\": \"MoRel实现了时间连贯、没有闪烁的长范围4D重建，并维持了有界的内存使用。\",\n  \"conclusion\": \"展示了动态Gaussian表示在长时4D运动建模中的扩展性和效率。\",\n  \"tool_calls\": []\n}", "conclusion": ""}}
{"id": "2512.09867", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09867", "abs": "https://arxiv.org/abs/2512.09867", "authors": ["Fengli Wu", "Vaidehi Patil", "Jaehong Yoon", "Yue Zhang", "Mohit Bansal"], "title": "MedForget: Hierarchy-Aware Multimodal Unlearning Testbed for Medical AI", "comment": "Dataset and Code: https://github.com/fengli-wu/MedForget", "summary": "Pretrained Multimodal Large Language Models (MLLMs) are increasingly deployed in medical AI systems for clinical reasoning, diagnosis support, and report generation. However, their training on sensitive patient data raises critical privacy and compliance challenges under regulations such as HIPAA and GDPR, which enforce the \"right to be forgotten\". Unlearning, the process of tuning models to selectively remove the influence of specific training data points, offers a potential solution, yet its effectiveness in complex medical settings remains underexplored. To systematically study this, we introduce MedForget, a Hierarchy-Aware Multimodal Unlearning Testbed with explicit retain and forget splits and evaluation sets containing rephrased variants. MedForget models hospital data as a nested hierarchy (Institution -> Patient -> Study -> Section), enabling fine-grained assessment across eight organizational levels. The benchmark contains 3840 multimodal (image, question, answer) instances, each hierarchy level having a dedicated unlearning target, reflecting distinct unlearning challenges. Experiments with four SOTA unlearning methods on three tasks (generation, classification, cloze) show that existing methods struggle to achieve complete, hierarchy-aware forgetting without reducing diagnostic performance. To test whether unlearning truly deletes hierarchical pathways, we introduce a reconstruction attack that progressively adds hierarchical level context to prompts. Models unlearned at a coarse granularity show strong resistance, while fine-grained unlearning leaves models vulnerable to such reconstruction. MedForget provides a practical, HIPAA-aligned testbed for building compliant medical AI systems.", "AI": {"tldr": "本文介绍了MedForget，一个衡量多模态模型在医疗环境下多层次数据遗忘效果的测试平台，发现现有方法在这方面存在挑战。", "motivation": "本文的动机在于解决在医疗AI系统中预训练多模态大语言模型（MLLMs）的关键隐私和合规挑战。特指在处理敏感病人数据时，需要遵守HIPAA和GDPR等法规。”，这些法规要求有“被遗忘的权利”。", "method": "本文介绍了MedForget，一个层次感知的多模态遗忘测试平台，它明确区分了保留和忘记的数据集，并通过重新表述变体进行评估。MedForget将医院数据建模为嵌套层次结构（机构 -> 病人 -> 研究 -> 部分），允许在八个组织级别上进行细致的评估。该基准包含3840个多模态（图像、问题、答案）实例，每个层次都有专门的遗忘目标，反映了不同程度的遗忘挑战。", "result": "实验结果表明，现有方法在不减少诊断性能的情况下，难以达到完全的、层次感知的遗忘效果。研究还引入了一种重建攻击，通过逐步添加层次上下文到提示中，测试是否真正删除了层次路径。结果发现，粗粒度遗忘的模型对攻击具有很强的抵抗力，而细粒度遗忘的模型则容易受到此类攻击。", "conclusion": "MedForget提供了一个实际的、符合HIPAA法规的测试平台，用于构建合规的医疗AI系统，这对于开发和评估能够有效应对“被遗忘权利”的多模态模型至关重要。"}}
{"id": "2512.09271", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09271", "abs": "https://arxiv.org/abs/2512.09271", "authors": ["Zhichao Yang", "Tianjiao Gu", "Jianjie Wang", "Feiyu Lin", "Xiangfei Sheng", "Pengfei Chen", "Leida Li"], "title": "LongT2IBench: A Benchmark for Evaluating Long Text-to-Image Generation with Graph-structured Annotations", "comment": "The paper has been accepted by AAAI 2026", "summary": "The increasing popularity of long Text-to-Image (T2I) generation has created an urgent need for automatic and interpretable models that can evaluate the image-text alignment in long prompt scenarios. However, the existing T2I alignment benchmarks predominantly focus on short prompt scenarios and only provide MOS or Likert scale annotations. This inherent limitation hinders the development of long T2I evaluators, particularly in terms of the interpretability of alignment. In this study, we contribute LongT2IBench, which comprises 14K long text-image pairs accompanied by graph-structured human annotations. Given the detail-intensive nature of long prompts, we first design a Generate-Refine-Qualify annotation protocol to convert them into textual graph structures that encompass entities, attributes, and relations. Through this transformation, fine-grained alignment annotations are achieved based on these granular elements. Finally, the graph-structed annotations are converted into alignment scores and interpretations to facilitate the design of T2I evaluation models. Based on LongT2IBench, we further propose LongT2IExpert, a LongT2I evaluator that enables multi-modal large language models (MLLMs) to provide both quantitative scores and structured interpretations through an instruction-tuning process with Hierarchical Alignment Chain-of-Thought (CoT). Extensive experiments and comparisons demonstrate the superiority of the proposed LongT2IExpert in alignment evaluation and interpretation. Data and code have been released in https://welldky.github.io/LongT2IBench-Homepage/.", "AI": {"tldr": "研究为长文本-图像生成评估提出了基准LongT2IBench和LongT2IExpert评估者，以解决现有模型在长提示场景中评估的可解释性问题。", "motivation": "现有的文本到图像生成评估基准主要集中在短提示场景，仅提供一般评价，缺乏对长文本提示的研究及可解释性对齐评估。", "method": "设计了Generate-Refine-Qualify注释协议，将详细丰富的长提示转换为包括实体、属性和关系的文本图形结构，并转化为细粒度的对齐注释。最终，将图形结构注释转换为对齐评分和解释，以促进文本-图像生成模型的评估设计。", "result": "提出了LongT2IBench，包含14K长文本-图像对和图结构的人工注释。并进一步提出了LongT2IExpert，该评估者能够多模态大语言模型通过指令调优过程中的分层对齐链式思考方法提供精确的评分和结构化解释。", "conclusion": "实验和比较证明，提出的LongT2IExpert在对齐评估和解释方面具有优越性。"}}
{"id": "2512.09276", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09276", "abs": "https://arxiv.org/abs/2512.09276", "authors": ["Xiaochen Huang", "Xiaochen Bi", "Cuihua Lv", "Xin Wang", "Haoyan Zhang", "Wenjing Jiang", "Xin Ma", "Yibin Li"], "title": "Dynamic Facial Expressions Analysis Based Parkinson's Disease Auxiliary Diagnosis", "comment": null, "summary": "Parkinson's disease (PD), a prevalent neurodegenerative disorder, significantly affects patients' daily functioning and social interactions. To facilitate a more efficient and accessible diagnostic approach for PD, we propose a dynamic facial expression analysis-based PD auxiliary diagnosis method. This method targets hypomimia, a characteristic clinical symptom of PD, by analyzing two manifestations: reduced facial expressivity and facial rigidity, thereby facilitating the diagnosis process. We develop a multimodal facial expression analysis network to extract expression intensity features during patients' performance of various facial expressions. This network leverages the CLIP architecture to integrate visual and textual features while preserving the temporal dynamics of facial expressions. Subsequently, the expression intensity features are processed and input into an LSTM-based classification network for PD diagnosis. Our method achieves an accuracy of 93.1%, outperforming other in-vitro PD diagnostic approaches. This technique offers a more convenient detection method for potential PD patients, improving their diagnostic experience.", "AI": {"tldr": "本文提出一种基于面部表情分析的帕金森病辅助诊断方法，通过检测面部表达减少和面部僵硬，利用CLIP架构和LSTM网络实现93.1%的诊断准确率，提高诊断效率和便利性。", "motivation": "帕金森病是一种常见的神经退行性疾病，影响病人日常生活和社交活动。本文旨在开发一种更高效和可及的帕金森病诊断方法。", "method": "提出一个基于多模态面部表情分析网络的方法，结合CLIP架构整合视觉和文本特征，使用LSTM网络进行帕金森病的诊断。", "result": "该方法达到了93.1%的准确率，优于其他体外诊断方法。", "conclusion": "该技术提供了一种潜在帕金森病患者更方便的检测方法，改善了他们的诊断体验。"}}
{"id": "2512.09278", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09278", "abs": "https://arxiv.org/abs/2512.09278", "authors": ["Yeonjin Chang", "Juhwan Cho", "Seunghyeon Seo", "Wonsik Shin", "Nojun Kwak"], "title": "LoGoColor: Local-Global 3D Colorization for 360° Scenes", "comment": null, "summary": "Single-channel 3D reconstruction is widely used in fields such as robotics and medical imaging. While this line of work excels at reconstructing 3D geometry, the outputs are not colored 3D models, thus 3D colorization is required for visualization. Recent 3D colorization studies address this problem by distilling 2D image colorization models. However, these approaches suffer from an inherent inconsistency of 2D image models. This results in colors being averaged during training, leading to monotonous and oversimplified results, particularly in complex 360° scenes. In contrast, we aim to preserve color diversity by generating a new set of consistently colorized training views, thereby bypassing the averaging process. Nevertheless, eliminating the averaging process introduces a new challenge: ensuring strict multi-view consistency across these colorized views. To achieve this, we propose LoGoColor, a pipeline designed to preserve color diversity by eliminating this guidance-averaging process with a `Local-Global' approach: we partition the scene into subscenes and explicitly tackle both inter-subscene and intra-subscene consistency using a fine-tuned multi-view diffusion model. We demonstrate that our method achieves quantitatively and qualitatively more consistent and plausible 3D colorization on complex 360° scenes than existing methods, and validate its superior color diversity using a novel Color Diversity Index.", "AI": {"tldr": "研究介绍了LoGoColor方法，通过局部-全局策略解决3D颜色化中色彩平均化和多样性的挑战，并在360°复杂场景中实现更好的色彩一致性和多样性", "motivation": "解决3D颜色化过程中由于2D图像模型所导致的色彩平均化问题，从而提高360°复杂场景的颜色多样性和一致性", "method": "LoGoColor, 通过局部-全局方法将场景划分为子场景，利用微调后的多视角扩散模型解决子场景间的色彩一致性问题", "result": "实验结果表明，该方法在复杂360°场景的3D颜色化上达到了量化和质化的改善，并通过新的色彩多样性指数验证了颜色多样性的提升", "conclusion": "提出的方法通过局部-全局策略解决了多视角色彩一致性问题，同时提升颜色的多样化"}}
{"id": "2512.09282", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09282", "abs": "https://arxiv.org/abs/2512.09282", "authors": ["Xiang Chen", "Jinshan Pan", "Jiangxin Dong", "Jian Yang", "Jinhui Tang"], "title": "FoundIR-v2: Optimizing Pre-Training Data Mixtures for Image Restoration Foundation Model", "comment": "Project page: https://lowlevelcv.com/", "summary": "Recent studies have witnessed significant advances in image restoration foundation models driven by improvements in the scale and quality of pre-training data. In this work, we find that the data mixture proportions from different restoration tasks are also a critical factor directly determining the overall performance of all-in-one image restoration models. To this end, we propose a high-capacity diffusion-based image restoration foundation model, FoundIR-v2, which adopts a data equilibrium scheduling paradigm to dynamically optimize the proportions of mixed training datasets from different tasks. By leveraging the data mixing law, our method ensures a balanced dataset composition, enabling the model to achieve consistent generalization and comprehensive performance across diverse tasks. Furthermore, we introduce an effective Mixture-of-Experts (MoE)-driven scheduler into generative pre-training to flexibly allocate task-adaptive diffusion priors for each restoration task, accounting for the distinct degradation forms and levels exhibited by different tasks. Extensive experiments demonstrate that our method can address over 50 sub-tasks across a broader scope of real-world scenarios and achieves favorable performance against state-of-the-art approaches.", "AI": {"tldr": "通过采用数据均衡调度范式动态优化不同任务混合训练数据集的比例，以及引入MoE驱动调度器灵活分配任务自适应扩散先验，提出了FoundIR-v2，实现了多种图像恢复任务的一致性和综合性性能。", "motivation": "发现数据混合比例是决定全能型图像恢复模型整体性能的关键因素。", "method": "提出高容量的扩散型图像恢复基础模型FoundIR-v2，采用数据均衡调度范式和MoE驱动调度器。", "result": "实验显示方法可以处理超过50项子任务，并在种种真实场景中达到较领先水平。", "conclusion": "方法能在广泛的现实环境中综合处理图像恢复任务，比肩甚至超过了当前的最佳方法。"}}
{"id": "2512.09289", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09289", "abs": "https://arxiv.org/abs/2512.09289", "authors": ["Sukhrobbek Ilyosbekov"], "title": "MelanomaNet: Explainable Deep Learning for Skin Lesion Classification", "comment": "7 pages, 3 figures", "summary": "Automated skin lesion classification using deep learning has shown remarkable accuracy, yet clinical adoption remains limited due to the \"black box\" nature of these models. We present MelanomaNet, an explainable deep learning system for multi-class skin lesion classification that addresses this gap through four complementary interpretability mechanisms. Our approach combines an EfficientNet V2 backbone with GradCAM++ attention visualization, automated ABCDE clinical criterion extraction, Fast Concept Activation Vectors (FastCAV) for concept-based explanations, and Monte Carlo Dropout uncertainty quantification. We evaluate our system on the ISIC 2019 dataset containing 25,331 dermoscopic images across 9 diagnostic categories. Our model achieves 85.61% accuracy with a weighted F1 score of 0.8564, while providing clinically meaningful explanations that align model attention with established dermatological assessment criteria. The uncertainty quantification module decomposes prediction confidence into epistemic and aleatoric components, enabling automatic flagging of unreliable predictions for clinical review. Our results demonstrate that high classification performance can be achieved alongside comprehensive interpretability, potentially facilitating greater trust and adoption in clinical dermatology workflows. The source code is available at https://github.com/suxrobgm/explainable-melanoma", "AI": {"tldr": "研究介绍了MelanomaNet，一个可以提供临床上有意义解释的多类皮肤病变分类系统，该系统在ISIC 2019数据集上的准确率为85.61%，并能将预测可信度分解为先验和后验不确定性，以标记不可靠的预测供临床审查。", "motivation": "尽管自动化皮肤病变分类在深度学习方法上已经表现出显著的精度，但是由于这些模型的“黑箱”属性，它们在临床上的应用受到了限制。我们提出了解释性深度学习系统来解决这一问题。", "method": "我们提出了MelanomaNet，这是一个用于多类别皮肤病变分类的可解释深度学习系统，它结合了EfficientNet V2骨干网络与GradCAM++注意力可视化、自动ABCDE临床标准提取、基于概念的解释Fast Concept Activation Vectors (FastCAV) 和蒙特卡洛丢弃不确定性量化。", "result": "我们的模型在ISIC 2019数据集上实现85.61%的分类准确率和0.8564的加权F1分数，并提供了与公认皮肤科评估标准对齐的模型注意力。", "conclusion": "研究结果表明，高性能的分类在伴随着全面的可解释性的情况下可以实现，这可能有助于增强临床上对皮肤活检流程的信任，并推动其应用。"}}
{"id": "2512.09296", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09296", "abs": "https://arxiv.org/abs/2512.09296", "authors": ["Songhan Wu"], "title": "Traffic Scene Small Target Detection Method Based on YOLOv8n-SPTS Model for Autonomous Driving", "comment": "6 pages, 7 figures, 1 table. Accepted to The 2025 IEEE 3rd International Conference on Electrical, Automation and Computer Engineering (ICEACE), 2025. Code available at https://github.com/SonghanWu/yolov8n-SPTS", "summary": "This paper focuses on the key issue in autonomous driving: small target recognition in dynamic perception. Existing algorithms suffer from poor detection performance due to missing small target information, scale imbalance, and occlusion. We propose an improved YOLOv8n-SPTS model, which enhances the detection accuracy of small traffic targets through three key innovations: First, optimizing the feature extraction module. In the Backbone Bottleneck structure of YOLOv8n, 4 traditional convolution modules are replaced with Space-to-Depth Convolution (SPD-Conv) modules. This module retains fine-grained information through space-to-depth conversion, reduces information loss, and enhances the ability to capture features of low-resolution small targets. Second, enhancing feature fusion capability. The Spatial Pyramid Pooling - Fast Cross Stage Partial Connection (SPPFCSPC) module is introduced to replace the original SPPF module, integrating the multi-scale feature extraction from Spatial Pyramid Pooling (SPP) and the feature fusion mechanism of Cross Stage Partial Connection (CSP), thereby improving the model's contextual understanding of complex scenes and multi-scale feature expression ability. Third, designing a dedicated detection structure for small targets. A Triple-Stage Feature Pyramid (TSFP) structure is proposed, which adds a 160*160 small target detection head to the original detection heads to fully utilize high-resolution features in shallow layers; meanwhile, redundant large target detection heads are removed to balance computational efficiency. Comparative experiments on the VisDrone2019-DET dataset show that YOLOv8n-SPTS model ranks first in precision (61.9%), recall (48.3%), mAP@0.5 (52.6%), and mAP@0.5:0.95 (32.6%). Visualization results verify that the miss rate of small targets such as pedestrians and bicycles in occluded and dense scenes is significantly reduced.", "AI": {"tldr": "An improved YOLOv8n-SPTS model featuring enhanced feature extraction and fusion capabilities, and a dedicated small target detection structure demonstrates superior performance in small target recognition.", "motivation": "To address the poor detection performance of existing algorithms on small traffic targets in dynamic perception, specifically due to issues like missing small target information, scale imbalance, and occlusion.", "method": "We propose an improved YOLOv8n-SPTS model with three key innovations: 1) Replacing traditional convolution modules with Space-to-Depth Convolution (SPD-Conv) modules in the Backbone Bottleneck structure of YOLOv8n. 2) Replacing the original SPPF module with an SPPFCSPC module to improve the feature fusion capability and multi-scale feature expression. 3) Designing a Triple-Stage Feature Pyramid (TSFP) structure to optimize small target detection.", "result": "Experiments show the YOLOv8n-SPTS model ranks first with a precision of 61.9%, recall of 48.3%, mAP@0.5 of 52.6%, and mAP@0.5:0.95 of 32.6%, significantly reducing the miss rate of small targets under occlusion and density.", "conclusion": "The proposed YOLOv8n-SPTS model demonstrates superior performance in small target recognition, particularly in terms of precision, recall, and mAP metrics, and effectively reduces the miss rate of small targets in occluded and dense scenarios."}}
{"id": "2512.09299", "categories": ["cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.09299", "abs": "https://arxiv.org/abs/2512.09299", "authors": ["Daili Hua", "Xizhi Wang", "Bohan Zeng", "Xinyi Huang", "Hao Liang", "Junbo Niu", "Xinlong Chen", "Quanqing Xu", "Wentao Zhang"], "title": "VABench: A Comprehensive Benchmark for Audio-Video Generation", "comment": "24 pages, 25 figures", "summary": "Recent advances in video generation have been remarkable, enabling models to produce visually compelling videos with synchronized audio. While existing video generation benchmarks provide comprehensive metrics for visual quality, they lack convincing evaluations for audio-video generation, especially for models aiming to generate synchronized audio-video outputs. To address this gap, we introduce VABench, a comprehensive and multi-dimensional benchmark framework designed to systematically evaluate the capabilities of synchronous audio-video generation. VABench encompasses three primary task types: text-to-audio-video (T2AV), image-to-audio-video (I2AV), and stereo audio-video generation. It further establishes two major evaluation modules covering 15 dimensions. These dimensions specifically assess pairwise similarities (text-video, text-audio, video-audio), audio-video synchronization, lip-speech consistency, and carefully curated audio and video question-answering (QA) pairs, among others. Furthermore, VABench covers seven major content categories: animals, human sounds, music, environmental sounds, synchronous physical sounds, complex scenes, and virtual worlds. We provide a systematic analysis and visualization of the evaluation results, aiming to establish a new standard for assessing video generation models with synchronous audio capabilities and to promote the comprehensive advancement of the field.", "AI": {"tldr": "提出了VABench，全面评估音视频同步生成模型的新标准。", "motivation": "现有的视频生成基准提供了全面的视觉质量指标，但缺乏有效的音视频生成评估，特别是对旨在生成同步音视频输出的模型。为了填补这一空白，提出了VABench。", "method": "介绍了VABench，一个全面的多维度基准框架，用于系统地评估同步音视频生成的能力。VABench包括三种主要任务类型：文本到音视频(T2AV)，图像到音视频(I2AV)，立体音视频生成，并设立两个主要评估模块，覆盖15个维度，包括成对相似性（文本-视频，文本-音频，视频-音频），音视频同步，口型-语音一致性等。", "result": "系统分析和可视化评估结果。", "conclusion": "VABench旨在建立新的标准，评估具有同步音频能力的视频生成模型，并促进该领域的全面发展。"}}
{"id": "2512.09307", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09307", "abs": "https://arxiv.org/abs/2512.09307", "authors": ["Shivanshu Agnihotri", "Snehashis Majhi", "Deepak Ranjan Nayak", "Debesh Jha"], "title": "From SAM to DINOv2: Towards Distilling Foundation Models to Lightweight Baselines for Generalized Polyp Segmentation", "comment": null, "summary": "Accurate polyp segmentation during colonoscopy is critical for the early detection of colorectal cancer and still remains challenging due to significant size, shape, and color variations, and the camouflaged nature of polyps. While lightweight baseline models such as U-Net, U-Net++, and PraNet offer advantages in terms of easy deployment and low computational cost, they struggle to deal with the above issues, leading to limited segmentation performance. In contrast, large-scale vision foundation models such as SAM, DINOv2, OneFormer, and Mask2Former have exhibited impressive generalization performance across natural image domains. However, their direct transfer to medical imaging tasks (e.g., colonoscopic polyp segmentation) is not straightforward, primarily due to the scarcity of large-scale datasets and lack of domain-specific knowledge. To bridge this gap, we propose a novel distillation framework, Polyp-DiFoM, that transfers the rich representations of foundation models into lightweight segmentation baselines, allowing efficient and accurate deployment in clinical settings. In particular, we infuse semantic priors from the foundation models into canonical architectures such as U-Net and U-Net++ and further perform frequency domain encoding for enhanced distillation, corroborating their generalization capability. Extensive experiments are performed across five benchmark datasets, such as Kvasir-SEG, CVC-ClinicDB, ETIS, ColonDB, and CVC-300. Notably, Polyp-DiFoM consistently outperforms respective baseline models significantly, as well as the state-of-the-art model, with nearly 9 times reduced computation overhead. The code is available at https://github.com/lostinrepo/PolypDiFoM.", "AI": {"tldr": "提出Polyp-DiFoM框架，通过知识蒸馏方式提升轻量级分割模型在结肠息肉分割任务上的表现，显著优于基线模型和当前最先进的模型，且计算开销大为降低。", "motivation": "解决轻量级模型（如U-Net、U-Net++和PraNet）在结肠息肉分割上的性能不足问题，同时克服大规模视觉基础模型（如SAM、DINOv2、OneFormer和Mask2Former）直接应用于医学影像任务的困难。", "method": "本文提出了一种新的知识蒸馏框架Polyp-DiFoM，将大型视觉基础模型的丰富表示转移至轻量级分割基线模型中，通过注入语义先验和进行频率域编码来增强蒸馏效果，以提高在临床设置下的部署效率和准确性。", "result": "在五个基准数据集上进行了大量实验，结果表明，Polyp-DiFoM相对于基线模型和当前最先进模型的表现有显著提升，计算开销减少了近9倍。", "conclusion": "Polyp-DiFoM的有效性得到了验证，它能够有效地在轻量级模型中植入大型视觉基础模型的知识，提高结肠息肉分割的准确性和部署效率。"}}
{"id": "2512.09311", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.09311", "abs": "https://arxiv.org/abs/2512.09311", "authors": ["Kuldeep Singh Yadav", "Lalan Kumar"], "title": "Transformer-Driven Multimodal Fusion for Explainable Suspiciousness Estimation in Visual Surveillance", "comment": "12 pages, 10 figures, IEEE Transaction on Image Processing", "summary": "Suspiciousness estimation is critical for proactive threat detection and ensuring public safety in complex environments. This work introduces a large-scale annotated dataset, USE50k, along with a computationally efficient vision-based framework for real-time suspiciousness analysis. The USE50k dataset contains 65,500 images captured from diverse and uncontrolled environments, such as airports, railway stations, restaurants, parks, and other public areas, covering a broad spectrum of cues including weapons, fire, crowd density, abnormal facial expressions, and unusual body postures. Building on this dataset, we present DeepUSEvision, a lightweight and modular system integrating three key components, i.e., a Suspicious Object Detector based on an enhanced YOLOv12 architecture, dual Deep Convolutional Neural Networks (DCNN-I and DCNN-II) for facial expression and body-language recognition using image and landmark features, and a transformer-based Discriminator Network that adaptively fuses multimodal outputs to yield an interpretable suspiciousness score. Extensive experiments confirm the superior accuracy, robustness, and interpretability of the proposed framework compared to state-of-the-art approaches. Collectively, the USE50k dataset and the DeepUSEvision framework establish a strong and scalable foundation for intelligent surveillance and real-time risk assessment in safety-critical applications.", "AI": {"tldr": "The paper presents USE50k, a large dataset, and DeepUSEvision, a multitask vision system for real-time suspiciousness detection, achieving superior performance.", "motivation": "The study aims to enhance proactive threat detection and ensure public safety using computationally efficient real-time suspiciousness analysis.", "method": "The paper introduces a large-scale annotated dataset called USE50k, consisting of 65,500 images from various public environments. It also proposes a vision-based framework, DeepUSEvision, which integrates a Suspicious Object Detector (enhanced YOLOv12), two DCNNs for facial expression and body language analysis, and a transformer-based Discriminator Network for interpretable outputs.", "result": "Experiments demonstrate that the proposed framework outperforms existing methods in terms of accuracy, robustness, and interpretability.", "conclusion": "The USE50k dataset and DeepUSEvision framework provide a robust and scalable basis for intelligent surveillance and real-time risk assessment in safety-critical situations."}}
{"id": "2512.09315", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09315", "abs": "https://arxiv.org/abs/2512.09315", "authors": ["Yuan Ma", "Junlin Hou", "Chao Zhang", "Yukun Zhou", "Zongyuan Ge", "Haoran Xie", "Lie Ju"], "title": "Benchmarking Real-World Medical Image Classification with Noisy Labels: Challenges, Practice, and Outlook", "comment": null, "summary": "Learning from noisy labels remains a major challenge in medical image analysis, where annotation demands expert knowledge and substantial inter-observer variability often leads to inconsistent or erroneous labels. Despite extensive research on learning with noisy labels (LNL), the robustness of existing methods in medical imaging has not been systematically assessed. To address this gap, we introduce LNMBench, a comprehensive benchmark for Label Noise in Medical imaging. LNMBench encompasses \\textbf{10} representative methods evaluated across 7 datasets, 6 imaging modalities, and 3 noise patterns, establishing a unified and reproducible framework for robustness evaluation under realistic conditions. Comprehensive experiments reveal that the performance of existing LNL methods degrades substantially under high and real-world noise, highlighting the persistent challenges of class imbalance and domain variability in medical data. Motivated by these findings, we further propose a simple yet effective improvement to enhance model robustness under such conditions. The LNMBench codebase is publicly released to facilitate standardized evaluation, promote reproducible research, and provide practical insights for developing noise-resilient algorithms in both research and real-world medical applications.The codebase is publicly available on https://github.com/myyy777/LNMBench.", "AI": {"tldr": "研究者提出了一个名为LNMBench的基准测试，用于评估医疗图像分析中的带有噪声标签的鲁棒性，发现现有方法在高噪声下表现不佳。", "motivation": "针对医疗图像分析中带有噪声标签的问题以及现有方法鲁棒性评估不足的情况，研究者提出了一份全面的基准测试LNMBench。", "method": "提出了一个名为LNMBench的全面基准测试，用于评估医疗影像中标签噪声的鲁棒性，涵盖了10种代表性方法，横跨7个数据集，6种成像模式和3种噪声模式，建立了一个统一且可重复的框架。", "result": "实验结果显示现有方法在高噪声和现实世界噪声下的性能显著降低，凸显了医疗数据中的类别不平衡和领域变化的持续挑战。", "conclusion": "研究者还提出了一种简单但有效的改进方法，以增强模型在这些条件下的鲁棒性，发布了LNMBench代码库，旨在促进可重复研究，并提供开发抗噪声算法的实用见解。"}}
{"id": "2512.09327", "categories": ["cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.09327", "abs": "https://arxiv.org/abs/2512.09327", "authors": ["Xuangeng Chu", "Ruicong Liu", "Yifei Huang", "Yun Liu", "Yichen Peng", "Bo Zheng"], "title": "UniLS: End-to-End Audio-Driven Avatars for Unified Listening and Speaking", "comment": null, "summary": "Generating lifelike conversational avatars requires modeling not just isolated speakers, but the dynamic, reciprocal interaction of speaking and listening. However, modeling the listener is exceptionally challenging: direct audio-driven training fails, producing stiff, static listening motions. This failure stems from a fundamental imbalance: the speaker's motion is strongly driven by speech audio, while the listener's motion primarily follows an internal motion prior and is only loosely guided by external speech. This challenge has led most methods to focus on speak-only generation. The only prior attempt at joint generation relies on extra speaker's motion to produce the listener. This design is not end-to-end, thereby hindering the real-time applicability. To address this limitation, we present UniLS, the first end-to-end framework for generating unified speak-listen expressions, driven by only dual-track audio. Our method introduces a novel two-stage training paradigm. Stage 1 first learns the internal motion prior by training an audio-free autoregressive generator, capturing the spontaneous dynamics of natural facial motion. Stage 2 then introduces the dual-track audio, fine-tuning the generator to modulate the learned motion prior based on external speech cues. Extensive evaluations show UniLS achieves state-of-the-art speaking accuracy. More importantly, it delivers up to 44.1\\% improvement in listening metrics, generating significantly more diverse and natural listening expressions. This effectively mitigates the stiffness problem and provides a practical, high-fidelity audio-driven solution for interactive digital humans.", "AI": {"tldr": "UniLS, an end-to-end framework for generating speaking and listening expressions driven by dual-track audio, learns an internal motion prior and refines this based on external speech cues, improving the naturalness of listening responses by up to 44.1%.", "motivation": "The primary motivation is to improve the generation of lifelike conversational avatars by adequately modeling the listener’s response, which has previously been a challenging aspect due to the discrepancy between speaker and listener motion dynamics.", "method": "The paper introduces UniLS, an end-to-end framework for generating unified speak-listen expressions. UniLS uses a novel two-stage training paradigm where stage 1 learns an internal motion prior without audio and stage 2 refines this prior using dual-track audio signals.", "result": "UniLS achieves state-of-the-art speaking accuracy and shows a significant improvement in listening metrics, leading to more diverse and natural listening expressions compared to previous methods.", "conclusion": "UniLS addresses the stiffness problem in animated listening by significantly improving listening metrics by up to 44.1%, making it a practical audio-driven solution for interactive digital humans."}}
{"id": "2512.09335", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.09335", "abs": "https://arxiv.org/abs/2512.09335", "authors": ["Seonghwa Choi", "Moonkyeong Choi", "Mingyu Jang", "Jaekyung Kim", "Jianfei Cai", "Wen-Huang Cheng", "Sanghoon Lee"], "title": "Relightable and Dynamic Gaussian Avatar Reconstruction from Monocular Video", "comment": "8 pages, 9 figures, published in ACM MM 2025", "summary": "Modeling relightable and animatable human avatars from monocular video is a long-standing and challenging task. Recently, Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) methods have been employed to reconstruct the avatars. However, they often produce unsatisfactory photo-realistic results because of insufficient geometrical details related to body motion, such as clothing wrinkles. In this paper, we propose a 3DGS-based human avatar modeling framework, termed as Relightable and Dynamic Gaussian Avatar (RnD-Avatar), that presents accurate pose-variant deformation for high-fidelity geometrical details. To achieve this, we introduce dynamic skinning weights that define the human avatar's articulation based on pose while also learning additional deformations induced by body motion. We also introduce a novel regularization to capture fine geometric details under sparse visual cues. Furthermore, we present a new multi-view dataset with varied lighting conditions to evaluate relight. Our framework enables realistic rendering of novel poses and views while supporting photo-realistic lighting effects under arbitrary lighting conditions. Our method achieves state-of-the-art performance in novel view synthesis, novel pose rendering, and relighting.", "AI": {"tldr": "提出了一种基于3DGS的RnD-Avatar框架，改进了因几何细节不足而造成的不可重光照和渲染效果不佳的问题。通过动态蒙皮权重和新颖正则化方法，实现了姿势变化的准确变形及精细几何细节。", "motivation": "尽管使用了NeRF和3DGS方法从单目视频中重建人类角色，但他们往往因缺乏与身体动作相关的几何细节（例如衣服的褶皱）而产生不够真实的三维效果。", "method": "我们提出了一种基于3D Gaussian Splatting (3DGS) 的人类角色建模框架，名为可重光照动态高斯角色（RnD-Avatar），以实现高保真的几何细节的姿势变形单调。为此，我们引入了动态蒙皮权重，定义了基于姿势的人类角色的活动，同时也学习了由身体动作引起的附加变形。此外，我们还引入了一种新颖的正则化方法，捕获稀疏视觉线索下的微几何细节。", "result": "我们的框架可以实现新型姿态和视角下的逼真渲染，并支持任意光照条件下的逼真光照效果。我们的方法在新型视图合成、新型姿态渲染和重光照方面达到了最先进的性能。", "conclusion": "RnD-Avatar框架通过引入动态蒙皮权重和新颖的正则化方法，能够准确地呈现姿势变化的变形，以及高保真的几何细节，克服了先前方法的局限。"}}
{"id": "2512.09350", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09350", "abs": "https://arxiv.org/abs/2512.09350", "authors": ["Kanghyun Baek", "Sangyub Lee", "Jin Young Choi", "Jaewoo Song", "Daemin Park", "Jooyoung Choi", "Chaehun Shin", "Bohyung Han", "Sungroh Yoon"], "title": "TextGuider: Training-Free Guidance for Text Rendering via Attention Alignment", "comment": null, "summary": "Despite recent advances, diffusion-based text-to-image models still struggle with accurate text rendering. Several studies have proposed fine-tuning or training-free refinement methods for accurate text rendering. However, the critical issue of text omission, where the desired text is partially or entirely missing, remains largely overlooked. In this work, we propose TextGuider, a novel training-free method that encourages accurate and complete text appearance by aligning textual content tokens and text regions in the image. Specifically, we analyze attention patterns in MM-DiT models, particularly for text-related tokens intended to be rendered in the image. Leveraging this observation, we apply latent guidance during the early stage of denoising steps based on two loss functions that we introduce. Our method achieves state-of-the-art performance in test-time text rendering, with significant gains in recall and strong results in OCR accuracy and CLIP score.", "AI": {"tldr": "TextGuider is a novel, training-free method that uses attention patterns in MM-DiT models to guide the accurate rendering of complete texts in images, achieving state-of-the-art performance in OCR accuracy and CLIP score during text rendering tests.", "motivation": "The motivation behind this work is to address the problem of text omission that occurs in diffusion-based text-to-image models. Current methods, which include fine-tuning or training-free refinement, have focused on improving the accuracy of text rendering but have not effectively minimized the issue of text being partially or entirely missing in generated images.", "method": "In this paper, the authors propose TextGuider, a method that focuses on accurately rendering complete texts in images without requiring retraining. The technique involves an analysis of attention patterns related to text tokens in MM-DiT models. By applying guidance during the early stages of denoising using two specific loss functions, the method aims to align textual content tokens with the text regions in images.", "result": "The proposed method, TextGuider, demonstrates improvements in text recall and achieves state-of-the-art results in terms of OCR accuracy and CLIP score, indicating better performance in test-time text rendering compared to previous approaches.", "conclusion": "This paper concludes by highlighting the effectiveness of the TextGuider method in enhancing the completeness and accuracy of text rendering in images without the need for retraining models. The introduced technique not only improves text recall and OCR performance but also aligns generated text regions with intended textual content."}}
{"id": "2512.09354", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09354", "abs": "https://arxiv.org/abs/2512.09354", "authors": ["Xinkui Zhao", "Zuxin Wang", "Yifan Zhang", "Guanjie Cheng", "Yueshen Xu", "Shuiguang Deng", "Chang Liu", "Naibo Wang", "Jianwei Yin"], "title": "Video-QTR: Query-Driven Temporal Reasoning Framework for Lightweight Video Understanding", "comment": null, "summary": "The rapid development of multimodal large-language models (MLLMs) has significantly expanded the scope of visual language reasoning, enabling unified systems to interpret and describe complex visual content. However, applying these models to long-video understanding remains computationally intensive. Dense frame encoding generates excessive visual tokens, leading to high memory consumption, redundant computation, and limited scalability in real-world applications. This inefficiency highlights a key limitation of the traditional process-then-reason paradigm, which analyzes visual streams exhaustively before semantic reasoning. To address this challenge, we introduce Video-QTR (Query-Driven Temporal Reasoning), a lightweight framework that redefines video comprehension as a query-guided reasoning process. Instead of encoding every frame, Video-QTR dynamically allocates perceptual resources based on the semantic intent of the query, creating an adaptive feedback loop between reasoning and perception. Extensive experiments across five benchmarks: MSVD-QA, Activity Net-QA, Movie Chat, and Video MME demonstrate that Video-QTR achieves state-of-the-art performance while reducing input frame consumption by up to 73%. These results confirm that query-driven temporal reasoning provides an efficient and scalable solution for video understanding.", "AI": {"tldr": "本文针对长视频理解计算密集的问题，提出了Video-QTR框架，该框架根据查询的意图动态分配感知资源，降低了计算负担，同时在多个基准数据集上达到了最佳性能。", "motivation": "尽管多模态大型语言模型的发展大大扩展了视觉语言推理的范围，但是将这些模型应用于长视频理解仍然计算密集。密集帧编码会产生大量的视觉标记，导致高内存消耗、重复计算和实际应用中的可扩展性有限。", "method": "提出了一种轻量级框架Video-QTR（Query-Driven Temporal Reasoning），重新定义了视频理解为一个查询引导的推理过程。与传统的逐帧编码方法不同，Video-QTR根据查询的语义意图动态分配感知资源，形成了推理和感知之间的自适应反馈循环。", "result": "在五个基准数据集MSVD-QA, Activity Net-QA, Movie Chat, 和Video MME上的广泛实验表明，Video-QTR取得了最先进的性能，同时将输入帧的消耗减少了高达73%。", "conclusion": "这些结果证实了基于查询驱动的时间推理为视频理解提供了一种高效和可扩展的解决方案。"}}
{"id": "2512.09363", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09363", "abs": "https://arxiv.org/abs/2512.09363", "authors": ["Ke Xing", "Longfei Li", "Yuyang Yin", "Hanwen Liang", "Guixun Luo", "Chen Fang", "Jue Wang", "Konstantinos N. Plataniotis", "Xiaojie Jin", "Yao Zhao", "Yunchao Wei"], "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation", "comment": null, "summary": "The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.", "AI": {"tldr": "StereoWorld is an advanced framework designed for converting monocular videos into high-quality stereo videos with enhanced visual fidelity and geometric accuracy, using a repurposed video generator and geometry-aware regularization.", "motivation": "The increasing use of XR devices has created a need for high-quality stereo videos, but current methods are expensive and result in artifacts.", "method": "The paper presents StereoWorld, a framework that repurposes a pretrained video generator to convert monocular videos to high-fidelity stereo videos by jointly conditioning the model on monocular input and using geometry-aware regularization to ensure 3D structural fidelity. It also uses a spatio-temporal tiling scheme for efficient, high-resolution synthesis.", "result": "The model outperforms previous methods in generating stereo videos that have superior visual fidelity and geometric consistency.", "conclusion": "StereoWorld significantly improves the quality of stereo video generation, ensuring both visual fidelity and geometric consistency, making it a notably effective solution."}}
{"id": "2512.09364", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09364", "abs": "https://arxiv.org/abs/2512.09364", "authors": ["Shengchao Zhou", "Jiehong Lin", "Jiahui Liu", "Shizhen Zhao", "Chirui Chang", "Xiaojuan Qi"], "title": "ASSIST-3D: Adapted Scene Synthesis for Class-Agnostic 3D Instance Segmentation", "comment": "Accepted by AAAI 2026", "summary": "Class-agnostic 3D instance segmentation tackles the challenging task of segmenting all object instances, including previously unseen ones, without semantic class reliance. Current methods struggle with generalization due to the scarce annotated 3D scene data or noisy 2D segmentations. While synthetic data generation offers a promising solution, existing 3D scene synthesis methods fail to simultaneously satisfy geometry diversity, context complexity, and layout reasonability, each essential for this task. To address these needs, we propose an Adapted 3D Scene Synthesis pipeline for class-agnostic 3D Instance SegmenTation, termed as ASSIST-3D, to synthesize proper data for model generalization enhancement. Specifically, ASSIST-3D features three key innovations, including 1) Heterogeneous Object Selection from extensive 3D CAD asset collections, incorporating randomness in object sampling to maximize geometric and contextual diversity; 2) Scene Layout Generation through LLM-guided spatial reasoning combined with depth-first search for reasonable object placements; and 3) Realistic Point Cloud Construction via multi-view RGB-D image rendering and fusion from the synthetic scenes, closely mimicking real-world sensor data acquisition. Experiments on ScanNetV2, ScanNet++, and S3DIS benchmarks demonstrate that models trained with ASSIST-3D-generated data significantly outperform existing methods. Further comparisons underscore the superiority of our purpose-built pipeline over existing 3D scene synthesis approaches.", "AI": {"tldr": "论文提出ASSIST-3D，一种用于无类别感知3D实例分割的3D场景合成方法，通过创新的异质对象选择、场景布局生成和逼真的点云构建，解决了现有方法的泛化问题，并在多种基准测试中展示出优越性。", "motivation": "现有方法难以处理3D实例分割中的泛化问题，主要是由于标注的3D场景数据稀缺和2D分割的噪声。此外，当前的3D场景合成方法无法同时满足几何多样性、上下文复杂性和布局合理性。", "method": "ASSIST-3D，一种用于无类别感知3D实例分割的3D场景合成流水线。其三个关键创新包括：1) 从广泛的3D CAD模型集合中进行异质对象选择，以提高几何和语境多样性；2) 通过LLM（可能是大型语言模型）引导的空间推理结合深度优先搜索生成场景布局；3) 通过从合成场景中的多视角RGB-D图像渲染和融合，构建逼真的点云，模仿现实世界的传感器数据获取。", "result": "在ScanNetV2、ScanNet++和S3DIS基准测试上，使用ASSIST-3D生成的数据训练的模型显著优于现有方法。实验进一步表明，ASSIST-3D在3D场景合成方面优于现有的合成方法。", "conclusion": "ASSIST-3D提供了一个有效的解决方案，克服了3D实例分割中由于数据稀缺和噪声导致的泛化问题，并在多个基准测试中显示出其在生成3D合成数据方面的优越性。"}}
{"id": "2512.09373", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09373", "abs": "https://arxiv.org/abs/2512.09373", "authors": ["Haobo Jiang", "Jin Xie", "Jian Yang", "Liang Yu", "Jianmin Zheng"], "title": "FUSER: Feed-Forward MUltiview 3D Registration Transformer and SE(3)$^N$ Diffusion Refinement", "comment": "13 pages, 6 figures", "summary": "Registration of multiview point clouds conventionally relies on extensive pairwise matching to build a pose graph for global synchronization, which is computationally expensive and inherently ill-posed without holistic geometric constraints. This paper proposes FUSER, the first feed-forward multiview registration transformer that jointly processes all scans in a unified, compact latent space to directly predict global poses without any pairwise estimation. To maintain tractability, FUSER encodes each scan into low-resolution superpoint features via a sparse 3D CNN that preserves absolute translation cues, and performs efficient intra- and inter-scan reasoning through a Geometric Alternating Attention module. Particularly, we transfer 2D attention priors from off-the-shelf foundation models to enhance 3D feature interaction and geometric consistency. Building upon FUSER, we further introduce FUSER-DF, an SE(3)$^N$ diffusion refinement framework to correct FUSER's estimates via denoising in the joint SE(3)$^N$ space. FUSER acts as a surrogate multiview registration model to construct the denoiser, and a prior-conditioned SE(3)$^N$ variational lower bound is derived for denoising supervision. Extensive experiments on 3DMatch, ScanNet and ArkitScenes demonstrate that our approach achieves the superior registration accuracy and outstanding computational efficiency.", "AI": {"tldr": "提出了FUSER，一种新颖的多视角点云配准方法，能够在没有成对估计的情况下直接预测全局姿态，显著提升了效率和准确性。", "motivation": "传统多视角点云配准依赖于广泛的成对匹配，这种方式计算成本高，低姿势稳定不足，缺乏整体几何约束。", "method": "FUSER方法将每个扫描编码成低分辨率超点特征，通过几何交替注意力模块高效地执行内部和跨扫描推理，并从预训练模型中转移2D注意力先验。FUSER-DF进一步引入了SE(3)$^N$扩散细化框架对FUSER的估计进行去噪。", "result": "实验结果显示，这种方法在3DMatch, ScanNet和ArkitScenes数据集上达到了优越的配准精度和计算效率。", "conclusion": "该论文表明FUSER及其改进版FUSER-DF在多视角点云配准领域实现了显著的性能提升。"}}
{"id": "2512.09375", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09375", "abs": "https://arxiv.org/abs/2512.09375", "authors": ["Sihe Chen", "Luv Verma", "Bruce A. Maxwell"], "title": "Log NeRF: Comparing Spaces for Learning Radiance Fields", "comment": "The 36th British Machine Vision Conference", "summary": "Neural Radiance Fields (NeRF) have achieved remarkable results in novel view synthesis, typically using sRGB images for supervision. However, little attention has been paid to the color space in which the network is learning the radiance field representation. Inspired by the BiIlluminant Dichromatic Reflection (BIDR) model, which suggests that a logarithmic transformation simplifies the separation of illumination and reflectance, we hypothesize that log RGB space enables NeRF to learn a more compact and effective representation of scene appearance. To test this, we captured approximately 30 videos using a GoPro camera, ensuring linear data recovery through inverse encoding. We trained NeRF models under various color space interpretations linear, sRGB, GPLog, and log RGB by converting each network output to a common color space before rendering and loss computation, enforcing representation learning in different color spaces. Quantitative and qualitative evaluations demonstrate that using a log RGB color space consistently improves rendering quality, exhibits greater robustness across scenes, and performs particularly well in low light conditions while using the same bit-depth input images. Further analysis across different network sizes and NeRF variants confirms the generalization and stability of the log space advantage.", "AI": {"tldr": "研究表明，在对数RGB颜色空间中训练NeRF可以改善渲染质量，增强不同场景下的鲁棒性，特别在低光条件下表现良好。", "motivation": "神经辐射场（NeRF）在新视角合成上已取得显著成果，通常使用sRGB图像作为监督。然而，很少有关于网络学习辐射场表示所使用的颜色空间的研究。", "method": "提出了一个假设，即使用对数RGB空间可以使NeRF学习到更紧凑且有效的光照场表示。通过使用GoPro相机捕捉约30个视频，并通过反编码确保线性数据恢复，研究了在不同颜色空间下（线性、sRGB、GPLog、对数RGB）训练NeRF模型的效果。每个网络输出在渲染和损失计算前被转换到一个常见的颜色空间，从而在不同的颜色空间中学习表示。", "result": "无论是定量还是定性评估均表明，在对数RGB颜色空间下训练的NeRF模型比其他颜色空间下的模型具有更好的渲染效果，尤其在低光条件下。不同网络规模和NeRF变体的研究进一步证实了对数空间的优势广泛性和稳定性。", "conclusion": "对数RGB颜色空间下的NeRF模型能更好地学习场景外观的表示，并且在渲染质量和鲁棒性方面优于其他颜色空间下的模型。尤其是，在给定相同位深的输入图像时，对数RGB颜色空间下的模型在低光条件下表现更优。"}}
{"id": "2512.09383", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09383", "abs": "https://arxiv.org/abs/2512.09383", "authors": ["Yang Cheng", "Ziteng Cui", "Lin Gu", "Shenghan Su", "Zenghui Zhang"], "title": "Perception-Inspired Color Space Design for Photo White Balance Editing", "comment": "Accepted to WACV 2026", "summary": "White balance (WB) is a key step in the image signal processor (ISP) pipeline that mitigates color casts caused by varying illumination and restores the scene's true colors. Currently, sRGB-based WB editing for post-ISP WB correction is widely used to address color constancy failures in the ISP pipeline when the original camera RAW is unavailable. However, additive color models (e.g., sRGB) are inherently limited by fixed nonlinear transformations and entangled color channels, which often impede their generalization to complex lighting conditions.\n  To address these challenges, we propose a novel framework for WB correction that leverages a perception-inspired Learnable HSI (LHSI) color space. Built upon a cylindrical color model that naturally separates luminance from chromatic components, our framework further introduces dedicated parameters to enhance this disentanglement and learnable mapping to adaptively refine the flexibility. Moreover, a new Mamba-based network is introduced, which is tailored to the characteristics of the proposed LHSI color space.\n  Experimental results on benchmark datasets demonstrate the superiority of our method, highlighting the potential of perception-inspired color space design in computational photography. The source code is available at https://github.com/YangCheng58/WB_Color_Space.", "AI": {"tldr": "本文提出了一种新的白平衡校正框架，使用可学习HSI颜色空间，展示了其在各种光照条件下的优越性能。", "motivation": "当前，基于sRGB的WB编辑广泛用于ISP管道中WB修正后的颜色恒定性失败，当原始相机RAW不可用时。然而，基于sRGB的方法受固定非线性变换和纠缠颜色通道的限制，难以泛化到复杂光照条件。", "method": "为了解决这些挑战，我们提出了一种新的基于可学习HSI（LHSI）颜色空间的白平衡校正框架。该框架基于圆柱形颜色模型，该模型自然地将亮度与色度分量分开。此外，引入了一个新的Mamba网络，该网络是根据所提出的LHSI颜色空间的特点量身定制的。", "result": "实验结果在基准数据集上证明了我们方法的优越性，强调了受感知启发的颜色空间设计在计算摄影中的潜力。", "conclusion": "这项研究展示了我们在复杂光照条件下提升白平衡校正能力的潜力，并突出了感知启发的颜色空间设计在计算摄影中的重要作用。"}}
{"id": "2512.09393", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09393", "abs": "https://arxiv.org/abs/2512.09393", "authors": ["Vasiliki Stoumpou", "Rohan Kumar", "Bernard Burman", "Diego Ojeda", "Tapan Mehta", "Dimitris Bertsimas"], "title": "Detection and Localization of Subdural Hematoma Using Deep Learning on Computed Tomography", "comment": null, "summary": "Background. Subdural hematoma (SDH) is a common neurosurgical emergency, with increasing incidence in aging populations. Rapid and accurate identification is essential to guide timely intervention, yet existing automated tools focus primarily on detection and provide limited interpretability or spatial localization. There remains a need for transparent, high-performing systems that integrate multimodal clinical and imaging information to support real-time decision-making.\n  Methods. We developed a multimodal deep-learning framework that integrates structured clinical variables, a 3D convolutional neural network trained on CT volumes, and a transformer-enhanced 2D segmentation model for SDH detection and localization. Using 25,315 head CT studies from Hartford HealthCare (2015--2024), of which 3,774 (14.9\\%) contained clinician-confirmed SDH, tabular models were trained on demographics, comorbidities, medications, and laboratory results. Imaging models were trained to detect SDH and generate voxel-level probability maps. A greedy ensemble strategy combined complementary predictors.\n  Findings. Clinical variables alone provided modest discriminatory power (AUC 0.75). Convolutional models trained on CT volumes and segmentation-derived maps achieved substantially higher accuracy (AUCs 0.922 and 0.926). The multimodal ensemble integrating all components achieved the best overall performance (AUC 0.9407; 95\\% CI, 0.930--0.951) and produced anatomically meaningful localization maps consistent with known SDH patterns.\n  Interpretation. This multimodal, interpretable framework provides rapid and accurate SDH detection and localization, achieving high detection performance and offering transparent, anatomically grounded outputs. Integration into radiology workflows could streamline triage, reduce time to intervention, and improve consistency in SDH management.", "AI": {"tldr": "{研究展示了一种新的多模态深度学习框架，用于快速准确地检测和定位SDH，具有高准确性和透明度的优势。}", "motivation": "{硬膜下血肿（SDH）是一种神经外科急症，尤其在老年人中发生率越来越高。迫切需要一种透明且性能高的系统来整合多模态的临床和影像学信息，以支持实时决策。}", "method": "{研究团队开发了一种多模态深度学习框架，该框架结合了结构化的临床变量、基于CT数据的三维卷积神经网络以及增强的二维分段模型，用于SDH的检测和定位。}", "result": "{本次研究提出了一种结合临床变量和图像信息的多模态深度学习框架，用于加速和提高硬膜下血肿（SDH）的检测和定位。该框架整合了一维临床信息、基于CT体积的三维卷积神经网络和增强的二维分段模型，通过贪婪集成策略综合了互补预测因子。结果表明，该多模态集成模型能够达到最佳的整体性能（AUC 0.9407）。这种方法不仅能快速准确地检测和定位SDH，还提供了透明且具有解剖学意义的输出。}", "conclusion": "{这项多模态且可解释的框架能够快速、准确地对硬膜下血肿（SDH）进行检测和定位，具有高检测性能和透明、解剖基底的输出。集成到放射学工作流程中可以简化优先次序安排，减少干预时间并改善SDH管理的一致性。}"}}
{"id": "2512.09402", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09402", "abs": "https://arxiv.org/abs/2512.09402", "authors": ["Rui Wang", "Yuting Jiang", "Xiaoqing Luo", "Xiao-Jun Wu", "Nicu Sebe", "Ziheng Chen"], "title": "Wasserstein-Aligned Hyperbolic Multi-View Clustering", "comment": "14 pages", "summary": "Multi-view clustering (MVC) aims to uncover the latent structure of multi-view data by learning view-common and view-specific information. Although recent studies have explored hyperbolic representations for better tackling the representation gap between different views, they focus primarily on instance-level alignment and neglect global semantic consistency, rendering them vulnerable to view-specific information (\\textit{e.g.}, noise and cross-view discrepancies). To this end, this paper proposes a novel Wasserstein-Aligned Hyperbolic (WAH) framework for multi-view clustering. Specifically, our method exploits a view-specific hyperbolic encoder for each view to embed features into the Lorentz manifold for hierarchical semantic modeling. Whereafter, a global semantic loss based on the hyperbolic sliced-Wasserstein distance is introduced to align manifold distributions across views. This is followed by soft cluster assignments to encourage cross-view semantic consistency. Extensive experiments on multiple benchmarking datasets show that our method can achieve SOTA clustering performance.", "AI": {"tldr": "This paper proposes a new WAH framework for multi-view clustering that improves upon previous methods by introducing a global semantic alignment based on hyperbolic geometry, leading to better clustering results.", "motivation": "The motivation of this paper is to tackle the problems with multi-view clustering (MVC) such as neglecting global semantic consistency and focusing only on instance-level alignment, leading to vulnerabilities to view-specific information.", "method": "The WAH framework uses a view-specific hyperbolic encoder for each view to embed features into the Lorentz manifold, and introduces a global semantic loss based on the hyperbolic sliced-Wasserstein distance to align manifold distributions across views, followed by soft cluster assignments.", "result": "Experiments on benchmarking datasets show that the proposed method achieves state-of-the-art (SOTA) clustering performance.", "conclusion": "The Wasserstein-Aligned Hyperbolic (WAH) framework improves multi-view clustering by considering both view-common and view-specific information with the help of a novel alignment method on the hyperbolic manifold."}}
{"id": "2512.09407", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09407", "abs": "https://arxiv.org/abs/2512.09407", "authors": ["Haobo Jiang", "Jin Xie", "Jian Yang", "Liang Yu", "Jianmin Zheng"], "title": "Generative Point Cloud Registration", "comment": "14 pages, 9 figures", "summary": "In this paper, we propose a novel 3D registration paradigm, Generative Point Cloud Registration, which bridges advanced 2D generative models with 3D matching tasks to enhance registration performance. Our key idea is to generate cross-view consistent image pairs that are well-aligned with the source and target point clouds, enabling geometry-color feature fusion to facilitate robust matching. To ensure high-quality matching, the generated image pair should feature both 2D-3D geometric consistency and cross-view texture consistency. To achieve this, we introduce Match-ControlNet, a matching-specific, controllable 2D generative model. Specifically, it leverages the depth-conditioned generation capability of ControlNet to produce images that are geometrically aligned with depth maps derived from point clouds, ensuring 2D-3D geometric consistency. Additionally, by incorporating a coupled conditional denoising scheme and coupled prompt guidance, Match-ControlNet further promotes cross-view feature interaction, guiding texture consistency generation. Our generative 3D registration paradigm is general and could be seamlessly integrated into various registration methods to enhance their performance. Extensive experiments on 3DMatch and ScanNet datasets verify the effectiveness of our approach.", "AI": {"tldr": "本论文展示了如何在3D注册中通过生成图像来增强匹配性能。", "motivation": "传统的3D匹配方法往往难以在复杂场景中实现高精度的点云配准。结合2D生成模型可以解决这一难题，通过生成一致的图像对提升匹配性能。", "method": "本论文提出了一种新的3D注册范式——生成点云注册，该方法通过将先进的2D生成模型与3D匹配任务结合来提高注册性能。核心思想是生成跨视图一致的图像对，这些图像对与源和目标点云很好地对齐，从而实现几何颜色特征的融合，以促进强大的匹配效果。为了确保高质量的匹配结果，生成的图像对需要具备2D-3D几何一致性和跨视图纹理一致性。为此，我们引入了Match-ControlNet，这是一种专门针对匹配任务的可控制2D生成模型。具体来说，它利用了ControlNet的深度条件生成能力，根据从点云导出的深度图生成几何对齐的图像，保证了2D-3D几何一致性。此外，通过引入耦合条件去噪方案和耦合同步提示引导，Match-ControlNet进一步促进了跨视图特征交互，指导生成纹理的一致性。", "result": "在3DMatch和ScanNet数据集上的大量实验验证了我们方法的有效性。", "conclusion": "我们的生成3D注册范式具有广泛的适用性并可以无缝整合到不同的注册方法中，实验结果表明它是有效的。"}}
{"id": "2512.09417", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09417", "abs": "https://arxiv.org/abs/2512.09417", "authors": ["Yanan Wang", "Shengcai Liao", "Panwen Hu", "Xin Li", "Fan Yang", "Xiaodan Liang"], "title": "DirectSwap: Mask-Free Cross-Identity Training and Benchmarking for Expression-Consistent Video Head Swapping", "comment": null, "summary": "Video head swapping aims to replace the entire head of a video subject, including facial identity, head shape, and hairstyle, with that of a reference image, while preserving the target body, background, and motion dynamics. Due to the lack of ground-truth paired swapping data, prior methods typically train on cross-frame pairs of the same person within a video and rely on mask-based inpainting to mitigate identity leakage. Beyond potential boundary artifacts, this paradigm struggles to recover essential cues occluded by the mask, such as facial pose, expressions, and motion dynamics. To address these issues, we prompt a video editing model to synthesize new heads for existing videos as fake swapping inputs, while maintaining frame-synchronized facial poses and expressions. This yields HeadSwapBench, the first cross-identity paired dataset for video head swapping, which supports both training (\\TrainNum{} videos) and benchmarking (\\TestNum{} videos) with genuine outputs. Leveraging this paired supervision, we propose DirectSwap, a mask-free, direct video head-swapping framework that extends an image U-Net into a video diffusion model with a motion module and conditioning inputs. Furthermore, we introduce the Motion- and Expression-Aware Reconstruction (MEAR) loss, which reweights the diffusion loss per pixel using frame-difference magnitudes and facial-landmark proximity, thereby enhancing cross-frame coherence in motion and expressions. Extensive experiments demonstrate that DirectSwap achieves state-of-the-art visual quality, identity fidelity, and motion and expression consistency across diverse in-the-wild video scenes. We will release the source code and the HeadSwapBench dataset to facilitate future research.", "AI": {"tldr": "研究提出了一种新的视频头部交换方法DirectSwap，利用新的HeadSwapBench数据集和MEAR损失来提高视觉质量和运动、表情一致性。", "motivation": "旨在解决因缺乏真实配对交换数据而导致的潜在边界伪影和无法恢复被遮挡的关键信息（如面部姿态、表情和运动动态）的问题。", "method": "通过提示视频编辑模型为现有视频合成新的头部作为假的交换输入，从而创建了HeadSwapBench，这是第一个跨身份配对的视频头部交换数据集，它支持训练和基准测试。并提出了一种无蒙版的直接视频头部交换框架DirectSwap，该框架将图像U-Net扩展为带有运动模块和条件输入的视频扩散模型。此外，还引入了运动和表情感知重构（MEAR）损失，以增强运动和表情的一致性。", "result": "实验表明，DirectSwap在视觉质量、身份保真度以及运动和表情一致性方面达到了最先进的水平，特别是在多样化的野外视频场景中。", "conclusion": "提出的方法通过创建HeadSwapBench数据集并在DirectSwap框架中应用MEAR损失，显著提升了视频头部交换的质量和一致性。"}}
{"id": "2512.09418", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09418", "abs": "https://arxiv.org/abs/2512.09418", "authors": ["Zhe Li", "Hadrien Reynaud", "Johanna P Müller", "Bernhard Kainz"], "title": "Label-free Motion-Conditioned Diffusion Model for Cardiac Ultrasound Synthesis", "comment": "Accepted at MICAD 2025", "summary": "Ultrasound echocardiography is essential for the non-invasive, real-time assessment of cardiac function, but the scarcity of labelled data, driven by privacy restrictions and the complexity of expert annotation, remains a major obstacle for deep learning methods. We propose the Motion Conditioned Diffusion Model (MCDM), a label-free latent diffusion framework that synthesises realistic echocardiography videos conditioned on self-supervised motion features. To extract these features, we design the Motion and Appearance Feature Extractor (MAFE), which disentangles motion and appearance representations from videos. Feature learning is further enhanced by two auxiliary objectives: a re-identification loss guided by pseudo appearance features and an optical flow loss guided by pseudo flow fields. Evaluated on the EchoNet-Dynamic dataset, MCDM achieves competitive video generation performance, producing temporally coherent and clinically realistic sequences without reliance on manual labels. These results demonstrate the potential of self-supervised conditioning for scalable echocardiography synthesis. Our code is available at https://github.com/ZheLi2020/LabelfreeMCDM.", "AI": {"tldr": "该研究提出了一个在不依赖人工标签的情况下，能够生成连贯逼真的超声心动图序列的框架——MCDM，展示了自我监督条件对于可扩展的超声心动图合成的潜力。", "motivation": "超声心动图是评估心脏功能的重要手段，但标注数据稀缺一直是深度学习方法面临的一个重要障碍。这一研究旨在解决这一问题。", "method": "提出了一种名为运动条件扩散模型（MCDM）的无标签潜在扩散框架，能够根据自我监督的运动特征合成逼真的超声心动图视频。为了提取这些特征，设计了运动和外观特征提取器（MAFE），它可以将视频中的运动和外观表现解耦。通过两个辅助目标来增强特征学习：一个是由伪外观特征引导的重新识别损失，另一个是由伪流场引导的光流损失。", "result": "在EchoNet-Dynamic数据集上进行评估，MCDM达到了具有竞争力的视频生成性能，产生了在时间上连贯且临床真实的视频序列。", "conclusion": "该研究表明，自我监督条件对于可扩展的超声心动图合成具有潜力。"}}
{"id": "2512.09422", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09422", "abs": "https://arxiv.org/abs/2512.09422", "authors": ["Zhe Li", "Hadrien Reynaud", "Alberto Gomez", "Bernhard Kainz"], "title": "InfoMotion: A Graph-Based Approach to Video Dataset Distillation for Echocardiography", "comment": "Accepted at MICAD 2025", "summary": "Echocardiography playing a critical role in the diagnosis and monitoring of cardiovascular diseases as a non-invasive real-time assessment of cardiac structure and function. However, the growing scale of echocardiographic video data presents significant challenges in terms of storage, computation, and model training efficiency. Dataset distillation offers a promising solution by synthesizing a compact, informative subset of data that retains the key clinical features of the original dataset. In this work, we propose a novel approach for distilling a compact synthetic echocardiographic video dataset. Our method leverages motion feature extraction to capture temporal dynamics, followed by class-wise graph construction and representative sample selection using the Infomap algorithm. This enables us to select a diverse and informative subset of synthetic videos that preserves the essential characteristics of the original dataset. We evaluate our approach on the EchoNet-Dynamic datasets and achieve a test accuracy of \\(69.38\\%\\) using only \\(25\\) synthetic videos. These results demonstrate the effectiveness and scalability of our method for medical video dataset distillation.", "AI": {"tldr": "本论文介绍了一种新的超声心动图视频数据集蒸馏技术，通过运动特征和Infomap算法选择代表性样本，基于仅25个合成视频，在EchoNet-Dynamic数据集中达到了69.38%的测试准确率。", "motivation": "随着超声心动图视频数据规模的增长，如何高效处理数据、提高计算效率和模型训练效率成为研究挑战。数据集蒸馏通过合成保留关键临床特征的紧凑子集，提供了有效解决方案。", "method": "本论文提出了一种新的方法，用于提炼紧凑的合成超声心动图视频数据集。该方法首先利用运动特征提取技术捕捉时间动态特性，随后通过类别图的构造以及使用Infomap算法选择代表性样本。", "result": "实验结果表明，使用仅25个合成视频，该方法在EchoNet-Dynamic数据集上的测试准确率为69.38%，证明了该方法在医学视频数据集蒸馏中的有效性和可扩展性。", "conclusion": "该研究表明，通过利用合成的紧凑视频数据集，可以显著降低存储和计算资源的需求，同时保持高诊断准确性，从而为心血管疾病诊断提供了一种新的高效方法。"}}
{"id": "2512.09423", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09423", "abs": "https://arxiv.org/abs/2512.09423", "authors": ["Marco Pegoraro", "Evan Atherton", "Bruno Roy", "Aliasghar Khani", "Arianna Rampini"], "title": "FunPhase: A Periodic Functional Autoencoder for Motion Generation via Phase Manifolds", "comment": null, "summary": "Learning natural body motion remains challenging due to the strong coupling between spatial geometry and temporal dynamics. Embedding motion in phase manifolds, latent spaces that capture local periodicity, has proven effective for motion prediction; however, existing approaches lack scalability and remain confined to specific settings. We introduce FunPhase, a functional periodic autoencoder that learns a phase manifold for motion and replaces discrete temporal decoding with a function-space formulation, enabling smooth trajectories that can be sampled at arbitrary temporal resolutions. FunPhase supports downstream tasks such as super-resolution and partial-body motion completion, generalizes across skeletons and datasets, and unifies motion prediction and generation within a single interpretable manifold. Our model achieves substantially lower reconstruction error than prior periodic autoencoder baselines while enabling a broader range of applications and performing on par with state-of-the-art motion generation methods.", "AI": {"tldr": "FunPhase 是一种新的功能性周期自编码器方法，学习运动的相位流形，实现了高精度的运动预测和生成，具有广泛的适用性。", "motivation": "现有的运动学习方法因时空耦合问题而具有局限性和应用场景约束，FunPhase 旨在解决这些问题，提供可扩展性和广泛的适用性。", "method": "FunPhase 使用功能性周期自编码器学习运动的相位流形，并用函数空间表达替换离散时间解码，这使得能够进行任意时间分辨率的采样。", "result": "FunPhase 达到了比早期的周期自编码器基线更低的重建误差，并且在广泛的下游任务中表现良好，包括超分辨率和部分身体运动补全。", "conclusion": "FunPhase 在单一可解释的相位流形中统一了运动预测和生成，并且在运动生成任务中表现出了与最先进的方法相媲美的效果。"}}
{"id": "2512.09435", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09435", "abs": "https://arxiv.org/abs/2512.09435", "authors": ["Xufan He", "Yushuang Wu", "Xiaoyang Guo", "Chongjie Ye", "Jiaqing Zhou", "Tianlei Hu", "Xiaoguang Han", "Dong Du"], "title": "UniPart: Part-Level 3D Generation with Unified 3D Geom-Seg Latents", "comment": null, "summary": "Part-level 3D generation is essential for applications requiring decomposable and structured 3D synthesis. However, existing methods either rely on implicit part segmentation with limited granularity control or depend on strong external segmenters trained on large annotated datasets. In this work, we observe that part awareness emerges naturally during whole-object geometry learning and propose Geom-Seg VecSet, a unified geometry-segmentation latent representation that jointly encodes object geometry and part-level structure. Building on this representation, we introduce UniPart, a two-stage latent diffusion framework for image-guided part-level 3D generation. The first stage performs joint geometry generation and latent part segmentation, while the second stage conditions part-level diffusion on both whole-object and part-specific latents. A dual-space generation scheme further enhances geometric fidelity by predicting part latents in both global and canonical spaces. Extensive experiments demonstrate that UniPart achieves superior segmentation controllability and part-level geometric quality compared with existing approaches.", "AI": {"tldr": "We introduce UniPart, a method for part-aware 3D generation that does not rely on external segmentation data, using a unified latent representation for geometry and segmentation, achieving high-quality part-level control and geometric fidelity in 3D synthesis.", "motivation": "Current 3D part generation methods either provide limited control over granularity or need extensive, annotated segmentation data. Our aim is to enable more flexible and high-quality part-level 3D generation without these constraints.", "method": "We propose Geom-Seg VecSet, a unified geometry-segmentation latent representation that captures both object geometry and part-level structure. UniPart, our two-stage latent diffusion framework, uses this representation to generate part-aware 3D objects guided by images. The framework first jointly generates the geometry and segmentations in latent space, then refines this using both global and part-specific latent conditions.", "result": "Experiments show UniPart achieves better part segmentation control and geometric quality compared to existing methods.", "conclusion": "We validate that part awareness in 3D objects can be naturally integrated during the learning process of whole-object geometries. UniPart represents a step forward in the direction of part-aware, high-fidelity 3D generation without requiring a large annotated dataset for training segmenters."}}
{"id": "2512.09441", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09441", "abs": "https://arxiv.org/abs/2512.09441", "authors": ["Jiantao Tan", "Peixian Ma", "Tong Yu", "Wentao Zhang", "Ruixuan Wang"], "title": "Representation Calibration and Uncertainty Guidance for Class-Incremental Learning based on Vision Language Model", "comment": null, "summary": "Class-incremental learning requires a learning system to continually learn knowledge of new classes and meanwhile try to preserve previously learned knowledge of old classes. As current state-of-the-art methods based on Vision-Language Models (VLMs) still suffer from the issue of differentiating classes across learning tasks. Here a novel VLM-based continual learning framework for image classification is proposed. In this framework, task-specific adapters are added to the pre-trained and frozen image encoder to learn new knowledge, and a novel cross-task representation calibration strategy based on a mixture of light-weight projectors is used to help better separate all learned classes in a unified feature space, alleviating class confusion across tasks. In addition, a novel inference strategy guided by prediction uncertainty is developed to more accurately select the most appropriate image feature for class prediction. Extensive experiments on multiple datasets under various settings demonstrate the superior performance of our method compared to existing ones.", "AI": {"tldr": "A new VLM-based continual learning method with adapters, calibration strategy, and uncertainty-guided inference for image classification is introduced, demonstrating superior performance over existing methods in class-incremental learning.", "motivation": "The motivation is to improve class-incremental learning by addressing the issue of differentiating classes across learning tasks in existing Vision-Language Models (VLMs).", "method": "In this paper, a novel VLM-based continual learning framework for image classification is proposed. The framework includes task-specific adapters added to a pre-trained and frozen image encoder, a cross-task representation calibration strategy using a mixture of light-weight projectors, and an inference strategy guided by prediction uncertainty.", "result": "Extensive experiments on multiple datasets show the method outperforms existing approaches in class-incremental learning.", "conclusion": "The proposed method effectively reduces class confusion across tasks and improves the preservation of old knowledge while learning new classes."}}
{"id": "2512.09446", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09446", "abs": "https://arxiv.org/abs/2512.09446", "authors": ["Nadeem Nazer", "Hongkuan Zhou", "Lavdim Halilaj", "Ylli Sadikaj", "Steffen Staab"], "title": "Defect-aware Hybrid Prompt Optimization via Progressive Tuning for Zero-Shot Multi-type Anomaly Detection and Segmentation", "comment": null, "summary": "Recent vision language models (VLMs) like CLIP have demonstrated impressive anomaly detection performance under significant distribution shift by utilizing high-level semantic information through text prompts. However, these models often neglect fine-grained details, such as which kind of anomalies, like \"hole\", \"cut\", \"scratch\" that could provide more specific insight into the nature of anomalies. We argue that recognizing fine-grained anomaly types 1) enriches the representation of \"abnormal\" with structured semantics, narrowing the gap between coarse anomaly signals and fine-grained defect categories; 2) enables manufacturers to understand the root causes of the anomaly and implement more targeted and appropriate corrective measures quickly. While incorporating such detailed semantic information is crucial, designing handcrafted prompts for each defect type is both time-consuming and susceptible to human bias. For this reason, we introduce DAPO, a novel approach for Defect-aware Prompt Optimization based on progressive tuning for the zero-shot multi-type and binary anomaly detection and segmentation under distribution shifts. Our approach aligns anomaly-relevant image features with their corresponding text semantics by learning hybrid defect-aware prompts with both fixed textual anchors and learnable token embeddings. We conducted experiments on public benchmarks (MPDD, VisA, MVTec-AD, MAD, and Real-IAD) and an internal dataset. The results suggest that compared to the baseline models, DAPO achieves a 3.7% average improvement in AUROC and average precision metrics at the image level under distribution shift, and a 6.5% average improvement in localizing novel anomaly types under zero-shot settings.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2512.09461", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09461", "abs": "https://arxiv.org/abs/2512.09461", "authors": ["Anabia Sohail", "Mohamad Alansari", "Ahmed Abughali", "Asmaa Chehab", "Abdelfatah Ahmed", "Divya Velayudhan", "Sajid Javed", "Hasan Al Marzouqi", "Ameena Saad Al-Sumaiti", "Junaid Kashir", "Naoufel Werghi"], "title": "Cytoplasmic Strings Analysis in Human Embryo Time-Lapse Videos using Deep Learning Framework", "comment": null, "summary": "Infertility is a major global health issue, and while in-vitro fertilization has improved treatment outcomes, embryo selection remains a critical bottleneck. Time-lapse imaging enables continuous, non-invasive monitoring of embryo development, yet most automated assessment methods rely solely on conventional morphokinetic features and overlook emerging biomarkers. Cytoplasmic Strings, thin filamentous structures connecting the inner cell mass and trophectoderm in expanded blastocysts, have been associated with faster blastocyst formation, higher blastocyst grades, and improved viability. However, CS assessment currently depends on manual visual inspection, which is labor-intensive, subjective, and severely affected by detection and subtle visual appearance. In this work, we present, to the best of our knowledge, the first computational framework for CS analysis in human IVF embryos. We first design a human-in-the-loop annotation pipeline to curate a biologically validated CS dataset from TLI videos, comprising 13,568 frames with highly sparse CS-positive instances. Building on this dataset, we propose a two-stage deep learning framework that (i) classifies CS presence at the frame level and (ii) localizes CS regions in positive cases. To address severe imbalance and feature uncertainty, we introduce the Novel Uncertainty-aware Contractive Embedding (NUCE) loss, which couples confidence-aware reweighting with an embedding contraction term to form compact, well-separated class clusters. NUCE consistently improves F1-score across five transformer backbones, while RF-DETR-based localization achieves state-of-the-art (SOTA) detection performance for thin, low-contrast CS structures. The source code will be made publicly available at: https://github.com/HamadYA/CS_Detection.", "AI": {"tldr": "The paper presents a novel computational framework for the detection of cytoplasmic strings in human in-vitro fertilization embryos, using a two-stage deep learning approach with a novel loss function to improve detection accuracy.", "motivation": "To address the limitations of manual visual inspection for cytoplasmic strings in IVF embryos, which is labor-intensive, subjective, and not consistent, by developing a robust computational framework capable of automated detection.", "method": "The authors propose a two-stage deep learning framework for detecting and localizing cytoplasmic strings in time-lapse imaging videos of IVF embryos, including a novel uncertainty-aware contractive embedding loss function.", "result": "The proposed method demonstrates improved F1-score across different deep learning models and achieves state-of-the-art detection performance for thin, low-contrast CS structures, through comprehensive testing on a curated dataset.", "conclusion": "The new computational framework promises to enhance embryo selection in IVF practices by providing an objective, automated method to detect cytoplasmic strings, a critical biomarker in embryo viability."}}
{"id": "2512.09463", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09463", "abs": "https://arxiv.org/abs/2512.09463", "authors": ["Sander De Coninck", "Emilio Gamba", "Bart Van Doninck", "Abdellatif Bey-Temsamani", "Sam Leroux", "Pieter Simoens"], "title": "Privacy-Preserving Computer Vision for Industry: Three Case Studies in Human-Centric Manufacturing", "comment": "Accepted to the AAAI26 HCM workshop", "summary": "The adoption of AI-powered computer vision in industry is often constrained by the need to balance operational utility with worker privacy. Building on our previously proposed privacy-preserving framework, this paper presents its first comprehensive validation on real-world data collected directly by industrial partners in active production environments. We evaluate the framework across three representative use cases: woodworking production monitoring, human-aware AGV navigation, and multi-camera ergonomic risk assessment. The approach employs learned visual transformations that obscure sensitive or task-irrelevant information while retaining features essential for task performance. Through both quantitative evaluation of the privacy-utility trade-off and qualitative feedback from industrial partners, we assess the framework's effectiveness, deployment feasibility, and trust implications. Results demonstrate that task-specific obfuscation enables effective monitoring with reduced privacy risks, establishing the framework's readiness for real-world adoption and providing cross-domain recommendations for responsible, human-centric AI deployment in industry.", "AI": {"tldr": "论文提出并验证了一个隐私保护框架，通过处理三个实际场景，展示了该框架在保持操作效用的同时保护工人隐私的能力。", "motivation": "为了在平衡操作实用性和工人隐私的难题中推进AI视图技术的应用，特别是在实际制造业环境中。", "method": "采用学习的视觉转换方式，模糊无关或敏感信息，保留任务所需的特征，通过隐私和实用性的权衡评估框架的有效性。", "result": "结果显示，这种方法能够在减少隐私风险的同时实现有效的监控，证明了该框架在现实世界应用的潜力。", "conclusion": "论文提供了在制造行业中部署人性化的AI的建议，证明了该框架在多种场景中的可行性。"}}
{"id": "2512.09471", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09471", "abs": "https://arxiv.org/abs/2512.09471", "authors": ["Yiqun Wang", "Lujun Li", "Meiru Yue", "Radu State"], "title": "Temporal-Spatial Tubelet Embedding for Cloud-Robust MSI Reconstruction using MSI-SAR Fusion: A Multi-Head Self-Attention Video Vision Transformer Approach", "comment": null, "summary": "Cloud cover in multispectral imagery (MSI) significantly hinders early-season crop mapping by corrupting spectral information. Existing Vision Transformer(ViT)-based time-series reconstruction methods, like SMTS-ViT, often employ coarse temporal embeddings that aggregate entire sequences, causing substantial information loss and reducing reconstruction accuracy. To address these limitations, a Video Vision Transformer (ViViT)-based framework with temporal-spatial fusion embedding for MSI reconstruction in cloud-covered regions is proposed in this study. Non-overlapping tubelets are extracted via 3D convolution with constrained temporal span $(t=2)$, ensuring local temporal coherence while reducing cross-day information degradation. Both MSI-only and SAR-MSI fusion scenarios are considered during the experiments. Comprehensive experiments on 2020 Traill County data demonstrate notable performance improvements: MTS-ViViT achieves a 2.23\\% reduction in MSE compared to the MTS-ViT baseline, while SMTS-ViViT achieves a 10.33\\% improvement with SAR integration over the SMTS-ViT baseline. The proposed framework effectively enhances spectral reconstruction quality for robust agricultural monitoring.", "AI": {"tldr": "提出了一种基于ViViT的时空融合嵌入框架，提升了云遮图像的重建质量，促进了准确的农业监测。", "motivation": "云层遮挡在多光谱图像中严重影响早期农作物的制图，现有基于Vision Transformer (ViT)的时间序列重建方法往往使用粗略的时间嵌入，导致信息损失和重建准确性降低。为解决这些问题，本研究提出了一种新的框架。", "method": "提出了一种基于Video Vision Transformer (ViViT)的框架，采用时空融合嵌入（temporal-spatial fusion embedding）技术，用于云覆盖区域的多光谱图像（MSI）重建。通过3D卷积提取非重叠管状体（tubelets），并限制时间跨度（t=2），以确保局部时间连贯性同时减少跨日信息退化。", "result": "综合实验表明，该框架在仅使用MSI和结合合成孔径雷达（SAR）与MSI的情况下，均表现出性能提升：MTS-ViViT相比MTS-ViT基线降低了2.23%的均方误差（MSE），SMTS-ViViT在融合SAR数据时则提高了10.33%的准确性。", "conclusion": "所提出的框架有效提升了多光谱图像重建质量，从而增强对农业的监测能力。"}}
{"id": "2512.09477", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09477", "abs": "https://arxiv.org/abs/2512.09477", "authors": ["Guillem Arias", "Ariadna Solà", "Martí Armengod", "Maria Vanrell"], "title": "Color encoding in Latent Space of Stable Diffusion Models", "comment": "6 pages, 8 figures, Color Imaging Conference 33", "summary": "Recent advances in diffusion-based generative models have achieved remarkable visual fidelity, yet a detailed understanding of how specific perceptual attributes - such as color and shape - are internally represented remains limited. This work explores how color is encoded in a generative model through a systematic analysis of the latent representations in Stable Diffusion. Through controlled synthetic datasets, principal component analysis (PCA) and similarity metrics, we reveal that color information is encoded along circular, opponent axes predominantly captured in latent channels c_3 and c_4, whereas intensity and shape are primarily represented in channels c_1 and c_2. Our findings indicate that the latent space of Stable Diffusion exhibits an interpretable structure aligned with a efficient coding representation. These insights provide a foundation for future work in model understanding, editing applications, and the design of more disentangled generative frameworks.", "AI": {"tldr": "研究揭示了在Stable Diffusion生成模型的潜在空间中，颜色信息主要编码在c_3和c_4通道中，而亮度和形状则主要出现在c_1和c_2通道中。", "motivation": "尽管扩散生成模型在视觉保真度方面取得了显著进步，但对于感知属性在模型内部如何表示仍缺乏了解，尤其是颜色。", "method": "使用控制的合成数据集，结合主成分分析（PCA）和相似性指标，系统地分析了Stable Diffusion模型潜在表示。", "result": "发现颜色信息在潜在空间中以环形，对立轴形式编码，主要集中在c_3和c_4通道；而亮度和形状则主要编码在c_1和c_2通道。", "conclusion": "这些发现表明Stable Diffusion的潜在空间具有与高效编码表示相匹配的可解释结构，这为模型理解、编辑应用和设计更加独立的生成框架奠定了基础。"}}
{"id": "2512.09489", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09489", "abs": "https://arxiv.org/abs/2512.09489", "authors": ["Shuaihao Han", "Tingfa Xu", "Peifu Liu", "Jianan Li"], "title": "MODA: The First Challenging Benchmark for Multispectral Object Detection in Aerial Images", "comment": "8 pages, 9 figures", "summary": "Aerial object detection faces significant challenges in real-world scenarios, such as small objects and extensive background interference, which limit the performance of RGB-based detectors with insufficient discriminative information. Multispectral images (MSIs) capture additional spectral cues across multiple bands, offering a promising alternative. However, the lack of training data has been the primary bottleneck to exploiting the potential of MSIs. To address this gap, we introduce the first large-scale dataset for Multispectral Object Detection in Aerial images (MODA), which comprises 14,041 MSIs and 330,191 annotations across diverse, challenging scenarios, providing a comprehensive data foundation for this field. Furthermore, to overcome challenges inherent to aerial object detection using MSIs, we propose OSSDet, a framework that integrates spectral and spatial information with object-aware cues. OSSDet employs a cascaded spectral-spatial modulation structure to optimize target perception, aggregates spectrally related features by exploiting spectral similarities to reinforce intra-object correlations, and suppresses irrelevant background via object-aware masking. Moreover, cross-spectral attention further refines object-related representations under explicit object-aware guidance. Extensive experiments demonstrate that OSSDet outperforms existing methods with comparable parameters and efficiency.", "AI": {"tldr": "A new dataset (MODA) and a novel framework (OSSDet) are introduced to improve aerial object detection by using multispectral images.", "motivation": "The motivation is to overcome the limitations of RGB-based detectors and the lack of training data for multispectral images (MSIs) by providing a comprehensive dataset and an effective framework for aerial object detection.", "method": "To address the challenges in aerial object detection, the paper introduces the MODA dataset, the first large-scale dataset for Multispectral Object Detection in Aerial images, and proposes OSSDet, a framework integrating spectral and spatial information with object-aware cues.", "result": "Extensive experiments confirm that the proposed OSSDet method outperforms existing methods in terms of performance with comparable parameters and efficiency.", "conclusion": "The study successfully addresses the challenges associated with aerial object detection by leveraging the MODA dataset and demonstrating the effectiveness of the OSSDet framework in enhancing detection performance."}}
{"id": "2512.09492", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09492", "abs": "https://arxiv.org/abs/2512.09492", "authors": ["Abdullah Al Mamun", "Miaohua Zhang", "David Ahmedt-Aristizabal", "Zeeshan Hayder", "Mohammad Awrangjeb"], "title": "StateSpace-SSL: Linear-Time Self-supervised Learning for Plant Disease Detectio", "comment": "Accepted to AAAI workshop (AgriAI 2026)", "summary": "Self-supervised learning (SSL) is attractive for plant disease detection as it can exploit large collections of unlabeled leaf images, yet most existing SSL methods are built on CNNs or vision transformers that are poorly matched to agricultural imagery. CNN-based SSL struggles to capture disease patterns that evolve continuously along leaf structures, while transformer-based SSL introduces quadratic attention cost from high-resolution patches. To address these limitations, we propose StateSpace-SSL, a linear-time SSL framework that employs a Vision Mamba state-space encoder to model long-range lesion continuity through directional scanning across the leaf surface. A prototype-driven teacher-student objective aligns representations across multiple views, encouraging stable and lesion-aware features from labelled data. Experiments on three publicly available plant disease datasets show that StateSpace-SSL consistently outperforms the CNN- and transformer-based SSL baselines in various evaluation metrics. Qualitative analyses further confirm that it learns compact, lesion-focused feature maps, highlighting the advantage of linear state-space modelling for self-supervised plant disease representation learning.", "AI": {"tldr": "本文提出了StateSpace-SSL，一种适用于植物疾病检测的线性时间自监督学习框架，利用状态空间编码和原型驱动的师生目标，能更好地捕捉病斑模式。实验表明其优于现有的基于CNN和transformer的方法。", "motivation": "大多数现有的自监督学习方法基于卷积神经网络或视觉变换器，这两种方法都不适合农业图像。卷积神经网络自监督学习难以捕捉沿叶结构连续发展的病斑模式，而变换器自监督学习由于高分辨率补丁而引入了二次注意力成本。", "method": "提出StateSpace-SSL，这是一个线性时间的自监督学习框架，利用Vision Mamba状态空间编码器通过沿叶片表面的方向扫描来建模长距离病斑连续性。采用原型驱动的师生目标，通过多个视图对齐表示，鼓励从标记数据中提取出稳定且关注病斑的特征。", "result": "在三个公开可用的植物疾病数据集上的实验表明，StateSpace-SSL在各种评估指标上始终优于基于卷积神经网络和变换器的自监督学习基线。定性分析进一步证实了它学习到了紧凑且聚焦病斑的特征图。", "conclusion": "实验表明，StateSpace-SSL优于基于卷积神经网络和变换器的自监督学习基线，揭示了线性状态空间模型在自监督植物疾病表征学习中的优势。定性分析证实了其学习到了紧凑且聚焦于病斑的特征图。"}}
{"id": "2512.09497", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09497", "abs": "https://arxiv.org/abs/2512.09497", "authors": ["Jinmiao Zhao", "Chuang Yu", "Zelin Shi", "Yunpeng Liu", "Yingdi Zhang"], "title": "Gradient-Guided Learning Network for Infrared Small Target Detection", "comment": "Accepted by GRSL 2023", "summary": "Recently, infrared small target detection has attracted extensive attention. However, due to the small size and the lack of intrinsic features of infrared small targets, the existing methods generally have the problem of inaccurate edge positioning and the target is easily submerged by the background. Therefore, we propose an innovative gradient-guided learning network (GGL-Net). Specifically, we are the first to explore the introduction of gradient magnitude images into the deep learning-based infrared small target detection method, which is conducive to emphasizing the edge details and alleviating the problem of inaccurate edge positioning of small targets. On this basis, we propose a novel dual-branch feature extraction network that utilizes the proposed gradient supplementary module (GSM) to encode raw gradient information into deeper network layers and embeds attention mechanisms reasonably to enhance feature extraction ability. In addition, we construct a two-way guidance fusion module (TGFM), which fully considers the characteristics of feature maps at different levels. It can facilitate the effective fusion of multi-scale feature maps and extract richer semantic information and detailed information through reasonable two-way guidance. Extensive experiments prove that GGL-Net has achieves state-of-the-art results on the public real NUAA-SIRST dataset and the public synthetic NUDT-SIRST dataset. Our code has been integrated into https://github.com/YuChuang1205/MSDA-Net", "AI": {"tldr": "论文提出了一种新的梯度引导学习网络（GGL-Net），解决了红外小目标检测中边缘定位不准确的问题，在公共数据集上取得了最先进的结果。", "motivation": "红外小目标检测由于目标尺寸小，缺乏内在特征，现有的方法普遍存在边缘定位不准确和目标容易被背景淹没的问题。因此，提出了本研究以解决这些问题。", "method": "我们提出了一个新的梯度引导学习网络（GGL-Net）。具体来说，我们首次将梯度模图像引入基于深度学习的红外小目标检测方法中。提出了一个新的双分支特征提取网络，利用提出的梯度补充模块（GSM）编码原始梯度信息到更深的网络层，并合理嵌入注意力机制来增强特征提取能力，同时还构建了双向指导融合模块（TGFM）有效融合多尺度特征图，提取更多语义和详细信息。", "result": "实验表明，GGL-Net在公共真实NUAA-SIRST数据集和公共合成NUDT-SIRST数据集中取得了最先进的结果。", "conclusion": "通过引入梯度信息和合理的注意力机制，GGL-Net能够有效地提高了红外小目标检测的准确性。"}}
{"id": "2512.09525", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09525", "abs": "https://arxiv.org/abs/2512.09525", "authors": ["Hongyou Zhou", "Cederic Aßmann", "Alaa Bejaoui", "Heiko Tzschätzsch", "Mark Heyland", "Julian Zierke", "Niklas Tuttle", "Sebastian Hölzl", "Timo Auer", "David A. Back", "Marc Toussaint"], "title": "Masked Registration and Autoencoding of CT Images for Predictive Tibia Reconstruction", "comment": "DGM4MICCAI", "summary": "Surgical planning for complex tibial fractures can be challenging for surgeons, as the 3D structure of the later desirable bone alignment may be diffi- cult to imagine. To assist in such planning, we address the challenge of predicting a patient-specific reconstruction target from a CT of the fractured tibia. Our ap- proach combines neural registration and autoencoder models. Specifically, we first train a modified spatial transformer network (STN) to register a raw CT to a standardized coordinate system of a jointly trained tibia prototype. Subsequently, various autoencoder (AE) architectures are trained to model healthy tibial varia- tions. Both the STN and AE models are further designed to be robust to masked input, allowing us to apply them to fractured CTs and decode to a prediction of the patient-specific healthy bone in standard coordinates. Our contributions include: i) a 3D-adapted STN for global spatial registration, ii) a comparative analysis of AEs for bone CT modeling, and iii) the extension of both to handle masked inputs for predictive generation of healthy bone structures. Project page: https://github.com/HongyouZhou/repair", "AI": {"tldr": "研究通过结合神经注册模型和自编码器模型预测复杂胫骨骨折患者的健康骨骼结构，方法包括修改的空间变换器网络用于注册和自编码器用于解码健康的骨骼变化。", "motivation": "复杂的胫骨骨折手术规划对手术医生来说是一项挑战，因为很难在三维空间中想象出理想的骨骼排列结构。为了帮助手术规划，该研究旨在预测患者的特定重建目标。", "method": "本研究结合了神经注册模型和自编码器模型来预测患者特定的重建目标。首先，训练了一种修改过的空间变换器网络（STN）以将原始CT图像注册到标准的胫骨坐标系统中。然后，训练了多种自编码器（AE）架构来模拟健康的胫骨变化。STN和AE模型还被设计为能够处理遮挡的输入，使它们可以应用于骨折CT图像，并预测标准坐标下的健康骨骼结构。", "result": "本研究在3D空间中实现了患者特定健康骨骼结构的预测。具体贡献包括：全球空间注册的3D调整STN、用于骨骼CT建模的AE比较分析，以及扩展STN和AE模型以处理遮挡输入，生成预测健康骨骼结构。", "conclusion": "本研究提供了一种新的方法，结合神经注册和自编码器模型，来处理复杂胫骨骨折的手术规划问题。这种方法能够帮助减轻医生在想象3D结构中的困难，并为目标重建提供预测。"}}
{"id": "2512.09546", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09546", "abs": "https://arxiv.org/abs/2512.09546", "authors": ["Murat Karayaka", "Usman Muhammad", "Jorma Laaksonen", "Md Ziaul Hoque", "Tapio Seppänen"], "title": "A Dual-Domain Convolutional Network for Hyperspectral Single-Image Super-Resolution", "comment": null, "summary": "This study presents a lightweight dual-domain super-resolution network (DDSRNet) that combines Spatial-Net with the discrete wavelet transform (DWT). Specifically, our proposed model comprises three main components: (1) a shallow feature extraction module, termed Spatial-Net, which performs residual learning and bilinear interpolation; (2) a low-frequency enhancement branch based on the DWT that refines coarse image structures; and (3) a shared high-frequency refinement branch that simultaneously enhances the LH (horizontal), HL (vertical), and HH (diagonal) wavelet subbands using a single CNN with shared weights. As a result, the DWT enables subband decomposition, while the inverse DWT reconstructs the final high-resolution output. By doing so, the integration of spatial- and frequency-domain learning enables DDSRNet to achieve highly competitive performance with low computational cost on three hyperspectral image datasets, demonstrating its effectiveness for hyperspectral image super-resolution.", "AI": {"tldr": "The paper introduces DDSRNet, a lightweight model combining spatial- and frequency-domain learning techniques for hyperspectral image super-resolution, achieving competitive performance at a low computational cost.", "motivation": "The motivation is to develop a lightweight dual-domain super-resolution network (DDSRNet) that can achieve high-resolution image reconstruction with low computational costs, particularly for hyperspectral image datasets.", "method": "This study presents a lightweight dual-domain super-resolution network (DDSRNet) that combines a shallow feature extraction module, Spatial-Net, with the discrete wavelet transform (DWT). The model comprises three main components: a shallow feature extraction module, termed Spatial-Net, which performs residual learning and bilinear interpolation; a low-frequency enhancement branch based on the DWT that refines coarse image structures; and a shared high-frequency refinement branch that enhances the wavelet subbands using a single CNN with shared weights.", "result": "The DDSRNet model performs highly competitive super-resolution tasks, reducing computational costs compared to existing methods on three hyperspectral image datasets.", "conclusion": "DDSRNet demonstrates its effectiveness in hyperspectral image super-resolution with low computational costs by integrating spatial- and frequency-domain learning techniques, achieving highly competitive performance on three datasets."}}
{"id": "2512.09555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09555", "abs": "https://arxiv.org/abs/2512.09555", "authors": ["Yuan Li", "Zitang Sun", "Yen-ju Chen", "Shin'ya Nishida"], "title": "Building Reasonable Inference for Vision-Language Models in Blind Image Quality Assessment", "comment": "Accepted to the ICONIP (International Conference on Neural Information Processing), 2025", "summary": "Recent progress in BIQA has been driven by VLMs, whose semantic reasoning abilities suggest that they might extract visual features, generate descriptive text, and infer quality in a human-like manner. However, these models often produce textual descriptions that contradict their final quality predictions, and the predicted scores can change unstably during inference - behaviors not aligned with human reasoning. To understand these issues, we analyze the factors that cause contradictory assessments and instability. We first estimate the relationship between the final quality predictions and the generated visual features, finding that the predictions are not fully grounded in the features and that the logical connection between them is weak. Moreover, decoding intermediate VLM layers shows that the model frequently relies on a limited set of candidate tokens, which contributes to prediction instability. To encourage more human-like reasoning, we introduce a two-stage tuning method that explicitly separates visual perception from quality inference. In the first stage, the model learns visual features; in the second, it infers quality solely from these features. Experiments on SPAQ and KONIQ demonstrate that our approach reduces prediction instability from 22.00% to 12.39% and achieves average gains of 0.3124/0.3507 in SRCC/PLCC across LIVE, CSIQ, SPAQ, and KONIQ compared to the baseline. Further analyses show that our method improves both stability and the reliability of the inference process.", "AI": {"tldr": "论文提出了一种两阶段调优方法来改善视觉语言模型(Visual and Language Models, VLMs)在盲图像质量评估(Blind Image Quality Assessment, BIQA)中的预测不稳定性和矛盾性问题，实验表明该方法在多个数据集上提高了预测的稳定性和可靠性。", "motivation": "由于现有VLMs在BIQA任务中表现出的预测不稳定性和预测结果与生成文本不符等问题，作者研究了这些问题的成因，并提出了新的解决方案。", "method": "作者提出了一个两阶段调优方法：第一阶段让模型学习视觉特征，第二阶段仅从这些特征推理图像质量，以鼓励更接近人类的推理模式。", "result": "实验结果表明，该方法在SPAQ和KONIQ数据集上将预测不稳定性从22.00%降低到12.39%，并实现了在LIVE、CSIQ、SPAQ和KONIQ数据集上的SRCC/PLCC平均增益为0.3124/0.3507。", "conclusion": "论文提出的两阶段调优方法有效改善了VLMs在BIQA中的预测不稳定性和推理过程的可靠性问题。"}}
{"id": "2512.09565", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09565", "abs": "https://arxiv.org/abs/2512.09565", "authors": ["Faraz Ali", "Muhammad Afaq", "Mahmood Niazi", "Muzammil Behzad"], "title": "From Graphs to Gates: DNS-HyXNet, A Lightweight and Deployable Sequential Model for Real-Time DNS Tunnel Detection", "comment": null, "summary": "Domain Name System (DNS) tunneling remains a covert channel for data exfiltration and command-and-control communication. Although graph-based methods such as GraphTunnel achieve strong accuracy, they introduce significant latency and computational overhead due to recursive parsing and graph construction, limiting their suitability for real-time deployment. This work presents DNS-HyXNet, a lightweight extended Long Short-Term Memory (xLSTM) hybrid framework designed for efficient sequence-based DNS tunnel detection. DNS-HyXNet integrates tokenized domain embeddings with normalized numerical DNS features and processes them through a two-layer xLSTM network that directly learns temporal dependencies from packet sequences, eliminating the need for graph reconstruction and enabling single-stage multi-class classification. The model was trained and evaluated on two public benchmark datasets with carefully tuned hyperparameters to ensure low memory consumption and fast inference. Across all experimental splits of the DNS-Tunnel-Datasets, DNS-HyXNet achieved up to 99.99% accuracy, with macro-averaged precision, recall, and F1-scores exceeding 99.96%, and demonstrated a per-sample detection latency of just 0.041 ms, confirming its scalability and real-time readiness. These results show that sequential modeling with xLSTM can effectively replace computationally expensive recursive graph generation, offering a deployable and energy-efficient alternative for real-time DNS tunnel detection on commodity hardware.", "AI": {"tldr": "The paper introduces DNS-HyXNet, a lightweight xLSTM hybrid framework for efficient detection of DNS tunneling that achieves high accuracy and low latency for real-time deployment.", "motivation": "To address the limitations of graph-based DNS tunneling detection methods which are computationally expensive and not suitable for real-time deployment.", "method": "DNS-HyXNet integrates tokenized domain embeddings and normalized numerical DNS features with a two-layer xLSTM network to directly learn temporal dependencies from packet sequences.", "result": "DNS-HyXNet achieved up to 99.99% accuracy across all experimental splits, with macro-averaged precision, recall, and F1-scores exceeding 99.96%, and demonstrated a latency of 0.041 ms, indicating its readiness for real-time deployment.", "conclusion": "Sequential modeling with xLSTM within DNS-HyXNet offers a more efficient and deployable alternative to computationally expensive recursive graph generation methods for real-time DNS tunnel detection."}}
{"id": "2512.09573", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09573", "abs": "https://arxiv.org/abs/2512.09573", "authors": ["Yuan Li", "Zitang Sun", "Yen-Ju Chen", "Shin'ya Nishida"], "title": "Investigate the Low-level Visual Perception in Vision-Language based Image Quality Assessment", "comment": null, "summary": "Recent advances in Image Quality Assessment (IQA) have leveraged Multi-modal Large Language Models (MLLMs) to generate descriptive explanations. However, despite their strong visual perception modules, these models often fail to reliably detect basic low-level distortions such as blur, noise, and compression, and may produce inconsistent evaluations across repeated inferences. This raises an essential question: do MLLM-based IQA systems truly perceive the visual features that matter? To examine this issue, we introduce a low-level distortion perception task that requires models to classify specific distortion types. Our component-wise analysis shows that although MLLMs are structurally capable of representing such distortions, they tend to overfit training templates, leading to biases in quality scoring. As a result, critical low-level features are weakened or lost during the vision-language alignment transfer stage. Furthermore, by computing the semantic distance between visual features and corresponding semantic tokens before and after component-wise fine-tuning, we show that improving the alignment of the vision encoder dramatically enhances distortion recognition accuracy, increasing it from 14.92% to 84.43%. Overall, these findings indicate that incorporating dedicated constraints on the vision encoder can strengthen text-explainable visual representations and enable MLLM-based pipelines to produce more coherent and interpretable reasoning in vision-centric tasks.", "AI": {"tldr": "The paper shows that MLLM-based Image Quality Assessment systems face challenges in reliable low-level distortion detection due to biases and overfitting. Improving the vision encoder's alignment dramatically enhances distortion recognition accuracy, indicating that targeted constraints can enhance coherence and interpretability in vision-centric tasks.", "motivation": "The motivation is to investigate the reliability of MLLM-based IQA systems in detecting basic low-level distortions in images despite their strong visual perception modules. The paper seeks to understand if these systems can detect distortions such as blur, noise, and compression accurately.", "method": "The paper proposes a low-level distortion perception task where models have to classify specific types of image distortion to evaluate whether MLLM-based IQA systems truly perceive important visual features. It also involves a component-wise analysis and calculates the semantic distance between visual features and semantic tokens to assess the alignment of the vision encoder.", "result": "The results indicate that while MLLMs have the structural capability to represent low-level distortions, they tend to overfit training templates, leading to biases in quality scoring. Improving the vision encoder's alignment significantly enhances distortion recognition accuracy from 14.92% to 84.43%.", "conclusion": "The conclusion is that dedicated constraints on the vision encoder can improve the text-explainable visual representations in MLLMs, leading to more coherent and interpretable reasoning in vision-centric tasks. This is demonstrated through the enhanced accuracy in distortion recognition after fine-tuning the vision encoder."}}
{"id": "2512.09576", "categories": ["cs.CV", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2512.09576", "abs": "https://arxiv.org/abs/2512.09576", "authors": ["David Seu", "Nicolas Longepe", "Gabriel Cioltea", "Erik Maidik", "Calin Andrei"], "title": "Seeing Soil from Space: Towards Robust and Scalable Remote Soil Nutrient Analysis", "comment": "23 pages, 13 figures, 13 tables", "summary": "Environmental variables are increasingly affecting agricultural decision-making, yet accessible and scalable tools for soil assessment remain limited. This study presents a robust and scalable modeling system for estimating soil properties in croplands, including soil organic carbon (SOC), total nitrogen (N), available phosphorus (P), exchangeable potassium (K), and pH, using remote sensing data and environmental covariates. The system employs a hybrid modeling approach, combining the indirect methods of modeling soil through proxies and drivers with direct spectral modeling. We extend current approaches by using interpretable physics-informed covariates derived from radiative transfer models (RTMs) and complex, nonlinear embeddings from a foundation model. We validate the system on a harmonized dataset that covers Europes cropland soils across diverse pedoclimatic zones. Evaluation is conducted under a robust validation framework that enforces strict spatial blocking, stratified splits, and statistically distinct train-test sets, which deliberately make the evaluation harder and produce more realistic error estimates for unseen regions. The models achieved their highest accuracy for SOC and N. This performance held across unseen locations, under both spatial cross-validation and an independent test set. SOC obtained a MAE of 5.12 g/kg and a CCC of 0.77, and N obtained a MAE of 0.44 g/kg and a CCC of 0.77. We also assess uncertainty through conformal calibration, achieving 90 percent coverage at the target confidence level. This study contributes to the digital advancement of agriculture through the application of scalable, data-driven soil analysis frameworks that can be extended to related domains requiring quantitative soil evaluation, such as carbon markets.", "AI": {"tldr": "本研究提出了一种新的土壤性质评估系统，利用遥感数据和环境协变量，通过结合间接建模和直接光谱建模方法，提高了对农田土壤有机碳、全氮、有效磷、交换性钾和pH值的预测准确性。该系统在欧洲农田土壤数据上进行了严格验证，表现出色，尤其在土壤有机碳和氮方面取得了较高的准确度。", "motivation": "当前可用的土壤评估工具有限，难以解决环境变量对农业决策的影响。为了克服这些问题，本研究旨在开发一个可扩展的土壤性质预测模型系统。", "method": "研究采用了结合间接建模和直接光谱建模的混合方法，使用源自辐射传输模型的物理信息协变量和复杂非线性嵌入技术进行土壤性质评估。", "result": "模型在土壤有机碳和氮方面表现最佳，SOC的MAE为5.12 g/kg，CCC为0.77；氮的MAE为0.44 g/kg，CCC也为0.77。", "conclusion": "该研究表明，通过对农田土壤性质进行可扩展的数据驱动分析，可以实现更高精度的预测，这不仅有助于农业数字化，还可以应用到如碳市场等需要定量土壤评估的领域。"}}
{"id": "2512.09579", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09579", "abs": "https://arxiv.org/abs/2512.09579", "authors": ["Dimitrios N. Vlachogiannis", "Dimitrios A. Koutsomitropoulos"], "title": "Hands-on Evaluation of Visual Transformers for Object Recognition and Detection", "comment": null, "summary": "Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.", "AI": {"tldr": "我们评测了几种Vision Transformers模型与传统CNN模型，发现Vision Transformers在物体识别、检测和医学图像分类中有着较强的全局上下文理解能力。", "motivation": "动机在于解决卷积神经网络在理解图像全局上下文方面的局限性，卷积神经网络主要关注局部模式，而Vision Transformers通过使用自注意力机制能够理解全图中的关系。", "method": "我们比较了几种不同类型的Vision Transformers（纯Vision Transformer、分层Transformer和混合型Transformer）与传统的卷积神经网络（CNN）模型，在物体识别、检测和医学影像分类等多个任务上进行了对比。", "result": "在医学影像使用ChestX-ray14数据集进行测试时，我们发现通过数据增强技术能显著提升模型性能，特别是Swin Transformer模型。总体而言，Vision Transformers在很多情况下优于传统CNNs。", "conclusion": "研究结果表明，Vision Transformers（尤其是混合型和分层型，如Swin Transformer）在保持准确度的同时，能够提供更好的计算资源管理方式。"}}
{"id": "2512.09580", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09580", "abs": "https://arxiv.org/abs/2512.09580", "authors": ["Hancheng Zhu", "Xinyu Liu", "Rui Yao", "Kunyang Sun", "Leida Li", "Abdulmotaleb El Saddik"], "title": "Content-Adaptive Image Retouching Guided by Attribute-Based Text Representation", "comment": null, "summary": "Image retouching has received significant attention due to its ability to achieve high-quality visual content. Existing approaches mainly rely on uniform pixel-wise color mapping across entire images, neglecting the inherent color variations induced by image content. This limitation hinders existing approaches from achieving adaptive retouching that accommodates both diverse color distributions and user-defined style preferences. To address these challenges, we propose a novel Content-Adaptive image retouching method guided by Attribute-based Text Representation (CA-ATP). Specifically, we propose a content-adaptive curve mapping module, which leverages a series of basis curves to establish multiple color mapping relationships and learns the corresponding weight maps, enabling content-aware color adjustments. The proposed module can capture color diversity within the image content, allowing similar color values to receive distinct transformations based on their spatial context. In addition, we propose an attribute text prediction module that generates text representations from multiple image attributes, which explicitly represent user-defined style preferences. These attribute-based text representations are subsequently integrated with visual features via a multimodal model, providing user-friendly guidance for image retouching. Extensive experiments on several public datasets demonstrate that our method achieves state-of-the-art performance.", "AI": {"tldr": "提出了一种新的自适应图像润饰方法，使用基于内容的曲线映射模块和属性文本预测模块，以适应多样的颜色分布并表达用户的风格偏好。实验结果表明该方法达到了最先进的性能。", "motivation": "由于现有方法主要依赖在整个图像上的均匀像素级颜色映射，忽视了图像内容引起的固有颜色变化，因此难以实现自适应润饰，以适应多样的颜色分布和用户定义的风格偏好。", "method": "提出了一种基于属性文本表示（CA-ATP）的自适应图像润饰方法。具体来说，该方法包括一个基于内容的曲线映射模块，通过一系列基线曲线建立多个颜色映射关系并学习相应的权重图，实现具有上下文感知的颜色调整。此外，还提出了一种属性文本预测模块，该模块从多个图像属性生成文本表示，以显性表达用户定义的风格偏好。这些基于属性的文本表示随后通过一个多模态模型与视觉特征结合，为图像润饰提供用户友好的指导。", "result": "在几个公开数据集上的广泛实验表明，该方法实现了最先进的性能。", "conclusion": "文章提出的方法能够捕捉图像内容中的颜色多样性，相似的颜色值可以根据其空间上下文进行不同的变换，适应多样的颜色分布和用户定义的风格偏好。"}}
{"id": "2512.09583", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09583", "abs": "https://arxiv.org/abs/2512.09583", "authors": ["Alberto Rota", "Mert Kiray", "Mert Asim Karaoglu", "Patrick Ruhkamp", "Elena De Momi", "Nassir Navabm", "Benjamin Busam"], "title": "UnReflectAnything: RGB-Only Highlight Removal by Rendering Synthetic Specular Supervision", "comment": null, "summary": "Specular highlights distort appearance, obscure texture, and hinder geometric reasoning in both natural and surgical imagery. We present UnReflectAnything, an RGB-only framework that removes highlights from a single image by predicting a highlight map together with a reflection-free diffuse reconstruction. The model uses a frozen vision transformer encoder to extract multi-scale features, a lightweight head to localize specular regions, and a token-level inpainting module that restores corrupted feature patches before producing the final diffuse image. To overcome the lack of paired supervision, we introduce a Virtual Highlight Synthesis pipeline that renders physically plausible specularities using monocular geometry, Fresnel-aware shading, and randomized lighting which enables training on arbitrary RGB images with correct geometric structure. UnReflectAnything generalizes across natural and surgical domains where non-Lambertian surfaces and non-uniform lighting create severe highlights and it achieves competitive performance with state-of-the-art results on several benchmarks. Project Page: https://alberto-rota.github.io/UnReflectAnything/", "AI": {"tldr": "UnReflectAnything is an RGB-only framework that removes specular highlights from images using a frozen vision transformer encoder and inpainting to produce reflection-free images, achieving state-of-the-art results on multiple benchmarks.", "motivation": "The motivation behind this research is to address the issue of highlights causing significant distortions in both natural and surgical imagery, which can obscure details and hinder geometric understanding.", "method": "The paper introduces UnReflectAnything, an RGB-only framework that removes highlights from images. It uses a vision transformer encoder to extract features, a lightweight head to localize specular regions, and an inpainting module to restore damaged areas of the image.", "result": "UnReflectAnything works effectively across different domains where highlights are severe due to non-uniform lighting and non-Lambertian surfaces, offering state-of-the-art results in several benchmarks.", "conclusion": "The UnReflectAnything framework successfully removes highlights from a single image without paired supervision, achieving competitive results and generalizing well across various domains."}}
{"id": "2512.09592", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09592", "abs": "https://arxiv.org/abs/2512.09592", "authors": ["Zhe Wang", "Qijin Song", "Yucen Peng", "Weibang Bai"], "title": "CS3D: An Efficient Facial Expression Recognition via Event Vision", "comment": null, "summary": "Responsive and accurate facial expression recognition is crucial to human-robot interaction for daily service robots. Nowadays, event cameras are becoming more widely adopted as they surpass RGB cameras in capturing facial expression changes due to their high temporal resolution, low latency, computational efficiency, and robustness in low-light conditions. Despite these advantages, event-based approaches still encounter practical challenges, particularly in adopting mainstream deep learning models. Traditional deep learning methods for facial expression analysis are energy-intensive, making them difficult to deploy on edge computing devices and thereby increasing costs, especially for high-frequency, dynamic, event vision-based approaches. To address this challenging issue, we proposed the CS3D framework by decomposing the Convolutional 3D method to reduce the computational complexity and energy consumption. Additionally, by utilizing soft spiking neurons and a spatial-temporal attention mechanism, the ability to retain information is enhanced, thus improving the accuracy of facial expression detection. Experimental results indicate that our proposed CS3D method attains higher accuracy on multiple datasets compared to architectures such as the RNN, Transformer, and C3D, while the energy consumption of the CS3D method is just 21.97\\% of the original C3D required on the same device.", "AI": {"tldr": "The paper presents the CS3D framework, which improves facial expression recognition by reducing computational complexity and energy consumption using soft spiking neurons and a spatial-temporal attention mechanism.", "motivation": "To enhance the efficiency and accuracy of facial expression recognition for human-robot interaction, addressing the limitations of traditional deep learning methods in event-based systems.", "method": "The CS3D framework decomposes the Convolutional 3D method and uses soft spiking neurons and spatial-temporal attention to reduce energy consumption and improve performance.", "result": "Experimental results show that CS3D outperforms other methods such as RNN, Transformer, and C3D in accuracy and significantly reduces energy consumption.", "conclusion": "The CS3D method is effective for event-based facial expression recognition, and it is more energy-efficient and accurate compared to other deep learning methods."}}
{"id": "2512.09617", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09617", "abs": "https://arxiv.org/abs/2512.09617", "authors": ["Hubert Kompanowski", "Varun Jampani", "Aaryaman Vasishta", "Binh-Son Hua"], "title": "FROMAT: Multiview Material Appearance Transfer via Few-Shot Self-Attention Adaptation", "comment": null, "summary": "Multiview diffusion models have rapidly emerged as a powerful tool for content creation with spatial consistency across viewpoints, offering rich visual realism without requiring explicit geometry and appearance representation. However, compared to meshes or radiance fields, existing multiview diffusion models offer limited appearance manipulation, particularly in terms of material, texture, or style.\n  In this paper, we present a lightweight adaptation technique for appearance transfer in multiview diffusion models. Our method learns to combine object identity from an input image with appearance cues rendered in a separate reference image, producing multi-view-consistent output that reflects the desired materials, textures, or styles. This allows explicit specification of appearance parameters at generation time while preserving the underlying object geometry and view coherence. We leverage three diffusion denoising processes responsible for generating the original object, the reference, and the target images, and perform reverse sampling to aggregate a small subset of layer-wise self-attention features from the object and the reference to influence the target generation. Our method requires only a few training examples to introduce appearance awareness to pretrained multiview models. The experiments show that our method provides a simple yet effective way toward multiview generation with diverse appearance, advocating the adoption of implicit generative 3D representations in practice.", "AI": {"tldr": "This paper introduces a lightweight adaptation technique for transferring appearance in multiview diffusion models, enabling the explicit specification of appearance parameters while maintaining object geometry and view consistency, thus expanding the potential for content creation.", "motivation": "Existing multiview diffusion models have limited capabilities for appearance manipulation, such as adjusting materials, textures, or styles. This paper aims to address this limitation by offering a lightweight technique for appearance transfer that enriches multiview outputs with diverse appearances.", "method": "Our method learns to combine object identity from an input image with appearance cues from a reference image, using three diffusion denoising processes to generate the original object, reference, and target images. Reverse sampling aggregates a few layer-wise self-attention features from the object and reference to influence the target generation.", "result": "Experiments demonstrate that our method effectively enhances multiview generation with diverse appearance options by adding explicit control over visual aspects at the time of generation, requiring only a minimal amount of training data to integrate appearance awareness into pretrained models.", "conclusion": "The proposed method shows great promise in enhancing the appearance control capabilities of multiview diffusion models, making it a practical and powerful tool for generating rich, diverse visual content with high spatial consistency across different viewpoints."}}
{"id": "2512.09626", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09626", "abs": "https://arxiv.org/abs/2512.09626", "authors": ["Yousef Azizi Movahed", "Fatemeh Ziaeetabar"], "title": "Beyond Sequences: A Benchmark for Atomic Hand-Object Interaction Using a Static RNN Encoder", "comment": "Code available at: https://github.com/YousefAMovahed/beyond-sequences-hoi-benchmark", "summary": "Reliably predicting human intent in hand-object interactions is an open challenge for computer vision. Our research concentrates on a fundamental sub-problem: the fine-grained classification of atomic interaction states, namely 'approaching', 'grabbing', and 'holding'. To this end, we introduce a structured data engineering process that converts raw videos from the MANIAC dataset into 27,476 statistical-kinematic feature vectors. Each vector encapsulates relational and dynamic properties from a short temporal window of motion. Our initial hypothesis posited that sequential modeling would be critical, leading us to compare static classifiers (MLPs) against temporal models (RNNs). Counter-intuitively, the key discovery occurred when we set the sequence length of a Bidirectional RNN to one (seq_length=1). This modification converted the network's function, compelling it to act as a high-capacity static feature encoder. This architectural change directly led to a significant accuracy improvement, culminating in a final score of 97.60%. Of particular note, our optimized model successfully overcame the most challenging transitional class, 'grabbing', by achieving a balanced F1-score of 0.90. These findings provide a new benchmark for low-level hand-object interaction recognition using structured, interpretable features and lightweight architectures.", "AI": {"tldr": "我们通过改进的网络结构实现97.60%的准确率，解决了手-物交互识别中的子问题。", "motivation": "可靠预测人手与物体交互的人类意图是计算机视觉中的一个重要挑战。我们的研究聚焦于精细分类基本的交互状态，包括接近、抓住和握住。", "method": "我们提出了一个结构化的数据处理流程，将MANIAC数据集的原始视频转换为包含关系和动态属性的统计-运动特征向量。我们首先假设顺序建模是关键，并对比了静态分类器（MLPs）与时间模型（RNNs）的效果。但在实验中，我们将双向RNN的序列长度设为1（seq_length=1），使网络作用于高容量的静态特征编码器。", "result": "修改网络结构后，我们的方法实现了一个显著的准确率改善，最终得分97.60%。优化后的模型在最具挑战性的'抓取'(grabbing)类别上也取得了平衡的F1评分为0.90。", "conclusion": "这些发现为使用结构化可解释特征和轻量级架构进行低级手-物交互识别提供了新的基准。"}}
{"id": "2512.09633", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09633", "abs": "https://arxiv.org/abs/2512.09633", "authors": ["Senem Aktas", "Charles Markham", "John McDonald", "Rozenn Dahyot"], "title": "Benchmarking SAM2-based Trackers on FMOX", "comment": null, "summary": "Several object tracking pipelines extending Segment Anything Model 2 (SAM2) have been proposed in the past year, where the approach is to follow and segment the object from a single exemplar template provided by the user on a initialization frame. We propose to benchmark these high performing trackers (SAM2, EfficientTAM, DAM4SAM and SAMURAI) on datasets containing fast moving objects (FMO) specifically designed to be challenging for tracking approaches. The goal is to understand better current limitations in state-of-the-art trackers by providing more detailed insights on the behavior of these trackers. We show that overall the trackers DAM4SAM and SAMURAI perform well on more challenging sequences.", "AI": {"tldr": "The paper benchmarks several SAM2-based object trackers on datasets with fast moving objects, highlighting that DAM4SAM and SAMURAI perform well on more challenging sequences.", "motivation": "To understand the limitations of state-of-the-art trackers through detailed evaluations on datasets specifically designed for such challenges.", "method": "Benchmarking high performing trackers (SAM2, EfficientTAM, DAM4SAM, SAMURAI) on datasets containing fast moving objects.", "result": "The trackers DAM4SAM and SAMURAI show superior performance on challenging sequences compared to others.", "conclusion": "The study reveals current limitations in the tracking performance and indicates that DAM4SAM and SAMURAI are more robust for tracking fast moving objects."}}
{"id": "2512.09644", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09644", "abs": "https://arxiv.org/abs/2512.09644", "authors": ["Ünal Akünal", "Markus Bujotzek", "Stefan Denner", "Benjamin Hamm", "Klaus Kades", "Philipp Schader", "Jonas Scherer", "Marco Nolden", "Peter Neher", "Ralf Floca", "Klaus Maier-Hein"], "title": "Kaapana: A Comprehensive Open-Source Platform for Integrating AI in Medical Imaging Research Environments", "comment": null, "summary": "Developing generalizable AI for medical imaging requires both access to large, multi-center datasets and standardized, reproducible tooling within research environments. However, leveraging real-world imaging data in clinical research environments is still hampered by strict regulatory constraints, fragmented software infrastructure, and the challenges inherent in conducting large-cohort multicentre studies. This leads to projects that rely on ad-hoc toolchains that are hard to reproduce, difficult to scale beyond single institutions and poorly suited for collaboration between clinicians and data scientists. We present Kaapana, a comprehensive open-source platform for medical imaging research that is designed to bridge this gap. Rather than building single-use, site-specific tooling, Kaapana provides a modular, extensible framework that unifies data ingestion, cohort curation, processing workflows and result inspection under a common user interface. By bringing the algorithm to the data, it enables institutions to keep control over their sensitive data while still participating in distributed experimentation and model development. By integrating flexible workflow orchestration with user-facing applications for researchers, Kaapana reduces technical overhead, improves reproducibility and enables conducting large-scale, collaborative, multi-centre imaging studies. We describe the core concepts of the platform and illustrate how they can support diverse use cases, from local prototyping to nation-wide research networks. The open-source codebase is available at https://github.com/kaapana/kaapana", "AI": {"tldr": "Kaapana是一种开源平台，旨在解决真实世界医学影像数据在临床研究中使用的问题，提供了统一且模块化的解决方案，有助于提高多中心研究的效率，代码库公开。", "motivation": "开发可推广的医学影像AI需要访问大型多中心数据集和标准化、可重复的研究环境工具。然而，严格监管、软件基础设施碎片化和多中心研究挑战阻碍了临床研究环境中真实世界成像数据的利用。Kaapana旨在消除这些障碍。", "method": "Kaapana提出了一个全面的开源平台，旨在解决临床研究环境中使用真实世界成像数据的限制。该平台提供了一个模块化、可扩展的框架，统一了数据摄取、队列整理、处理工作流和结果检查，通过将算法带到数据处，允许机构保留对敏感数据的控制，同时参与分布式实验和模型开发。", "result": "Kaapana平台通过减少技术负担，提高可重复性，并支持大规模、合作、多中心影像学研究的能力，展示了其在支持广泛使用场景方面的价值。尽管结果部分未提供具体实验数据，但概述了其潜在优势。", "conclusion": "Kaapana提供了一个全面的解决方案，旨在改善医学影像AI研究中的数据管理、工具链标准化和多中心研究协作。此平台为从本地原型制作到全国研究网络的各种用途提供了便利，并提供了免费的开源代码库。"}}
{"id": "2512.09646", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09646", "abs": "https://arxiv.org/abs/2512.09646", "authors": ["Wanyue Zhang", "Lin Geng Foo", "Thabo Beeler", "Rishabh Dabral", "Christian Theobalt"], "title": "VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification", "comment": null, "summary": "Synthesizing realistic human-object interactions (HOI) in video is challenging due to the complex, instance-specific interaction dynamics of both humans and objects. Incorporating controllability in video generation further adds to the complexity. Existing controllable video generation approaches face a trade-off: sparse controls like keypoint trajectories are easy to specify but lack instance-awareness, while dense signals such as optical flow, depths or 3D meshes are informative but costly to obtain. We propose VHOI, a two-stage framework that first densifies sparse trajectories into HOI mask sequences, and then fine-tunes a video diffusion model conditioned on these dense masks. We introduce a novel HOI-aware motion representation that uses color encodings to distinguish not only human and object motion, but also body-part-specific dynamics. This design incorporates a human prior into the conditioning signal and strengthens the model's ability to understand and generate realistic HOI dynamics. Experiments demonstrate state-of-the-art results in controllable HOI video generation. VHOI is not limited to interaction-only scenarios and can also generate full human navigation leading up to object interactions in an end-to-end manner. Project page: https://vcai.mpi-inf.mpg.de/projects/vhoi/.", "AI": {"tldr": "VHOI框架通过两阶段方法生成HOI视频，首先生成功能达先进水平，并证明该方法不仅限于交互场景，还可生成完整的导航视频。", "motivation": "旨在解决生成现实HOI视频的挑战，特别是如何处理人类和物体之间复杂的交互动态以及在视频生成中引入可控性的需求。", "method": "VHOI框架采用两阶段生成HOI视频。首先，将稀疏轨迹密集化为HOI掩码序列；其次，细化基于这些密集掩码的视频扩散模型。", "result": "实验表明，在可控HOI视频生成方面达到先进水平，并且VHOI不限于仅交互场景，还能生成完整的导航视频。", "conclusion": "VHOI提出了HOI-aware的运动表示，通过颜色编码区分人和物的运动以及特定的身体部位动态，从而提升了生成HOI动态的能力。"}}
{"id": "2512.09663", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09663", "abs": "https://arxiv.org/abs/2512.09663", "authors": ["Tao Zhang", "Yuyang Hong", "Yang Xia", "Kun Ding", "Zeyu Zhang", "Ying Wang", "Shiming Xiang", "Chunhong Pan"], "title": "IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting", "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have led to impressive progress across various benchmarks. However, their capability in understanding infrared images remains unexplored. To address this gap, we introduce IF-Bench, the first high-quality benchmark designed for evaluating multimodal understanding of infrared images. IF-Bench consists of 499 images sourced from 23 infrared datasets and 680 carefully curated visual question-answer pairs, covering 10 essential dimensions of image understanding. Based on this benchmark, we systematically evaluate over 40 open-source and closed-source MLLMs, employing cyclic evaluation, bilingual assessment, and hybrid judgment strategies to enhance the reliability of the results. Our analysis reveals how model scale, architecture, and inference paradigms affect infrared image comprehension, providing valuable insights for this area. Furthermore, we propose a training-free generative visual prompting (GenViP) method, which leverages advanced image editing models to translate infrared images into semantically and spatially aligned RGB counterparts, thereby mitigating domain distribution shifts. Extensive experiments demonstrate that our method consistently yields significant performance improvements across a wide range of MLLMs. The benchmark and code are available at https://github.com/casiatao/IF-Bench.", "AI": {"tldr": "The paper presents IF-Bench, a benchmark for evaluating infrared image comprehension in MLLMs, and introduces GenViP, a method that enhances model performance by translating infrared images to RGB counterparts.", "motivation": "The motivation is to explore the capability of multimodal large language models in understanding infrared images and to provide a reliable evaluation benchmark.", "method": "The paper introduces IF-Bench, a new benchmark for evaluating multimodal understanding of infrared images, and a training-free generative visual prompting (GenViP) method that translates infrared images into RGB counterparts.", "result": "The analysis reveals the impacts of model scale, architecture, and inference paradigms on infrared image comprehension. The GenViP method is shown to consistently improve performance across various MLLMs.", "conclusion": "The paper provides insights into improving the multimodal understanding of infrared images through the evaluation of existing models and the introduction of a novel method (GenViP) for enhancing model performance without additional training."}}
{"id": "2512.09665", "categories": ["cs.CV", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09665", "abs": "https://arxiv.org/abs/2512.09665", "authors": ["Jonathan Rystrøm", "Zihao Fu", "Chris Russell"], "title": "OxEnsemble: Fair Ensembles for Low-Data Classification", "comment": null, "summary": "We address the problem of fair classification in settings where data is scarce and unbalanced across demographic groups. Such low-data regimes are common in domains like medical imaging, where false negatives can have fatal consequences.\n  We propose a novel approach \\emph{OxEnsemble} for efficiently training ensembles and enforcing fairness in these low-data regimes. Unlike other approaches, we aggregate predictions across ensemble members, each trained to satisfy fairness constraints. By construction, \\emph{OxEnsemble} is both data-efficient, carefully reusing held-out data to enforce fairness reliably, and compute-efficient, requiring little more compute than used to fine-tune or evaluate an existing model. We validate this approach with new theoretical guarantees. Experimentally, our approach yields more consistent outcomes and stronger fairness-accuracy trade-offs than existing methods across multiple challenging medical imaging classification datasets.", "AI": {"tldr": "该研究介绍了针对低数据环境中的公平分类问题提出的OxEnsemble方法，它通过精心聚合满足公平性约束的集成成员预测结果，展示了数据和计算效率上的优势，并通过理论和实验验证了其效果。", "motivation": "解决了在数据稀缺且不同社群间数据不平衡的情况下实现公平分类的问题，这种情况常见于医学影像领域，其中错误否定可能导致致命后果。", "method": "提出了一个新颖的方法OxEnsemble，用于在这种低数据环境中有效训练集成模型并强制执行公平性。与其它方法不同，该方法聚合每个集成成员的预测结果，每个成员都受公平性约束。OxEnsemble设计上既注重数据效率（重新使用保留数据来可靠地强制执行公平性），又注重计算效率（所需的计算资源比对现有模型进行微调或评估多不了多少）。", "result": "通过实验，该方法在多个具有挑战性的医学影像分类数据集上相较于现有方法能够获得更加一致的结果和更强的公平性-准确性权衡。", "conclusion": "该方法不仅证明了理论上能够保证公平性，还通过实验验证了它在多个医学影像分类数据集上相比于现有方法，可以产生更加一致的结果和更强的公平性-准确性权衡。此外，它还确认了方法的数据效率和计算效率优势。"}}
{"id": "2512.09670", "categories": ["cs.CV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.09670", "abs": "https://arxiv.org/abs/2512.09670", "authors": ["Gil Weissman", "Amir Ivry", "Israel Cohen"], "title": "An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence", "comment": "Under review at IEEE Transactions on Geoscience and Remote Sensing (TGRS). 13 pages, 8 figures", "summary": "The proliferation of satellite constellations, coupled with reduced tasking latency and diverse sensor capabilities, has expanded the opportunities for automated Earth observation. This paper introduces a fully automated Tip-and-Cue framework designed for satellite imaging tasking and scheduling. In this context, tips are generated from external data sources or analyses of prior satellite imagery, identifying spatiotemporal targets and prioritizing them for downstream planning. Corresponding cues are the imaging tasks formulated in response, which incorporate sensor constraints, timing requirements, and utility functions. The system autonomously generates candidate tasks, optimizes their scheduling across multiple satellites using continuous utility functions that reflect the expected value of each observation, and processes the resulting imagery using artificial-intelligence-based models, including object detectors and vision-language models. Structured visual reports are generated to support both interpretability and the identification of new insights for downstream tasking. The efficacy of the framework is demonstrated through a maritime vessel tracking scenario, utilizing Automatic Identification System (AIS) data for trajectory prediction, targeted observations, and the generation of actionable outputs. Maritime vessel tracking is a widely researched application, often used to benchmark novel approaches to satellite tasking, forecasting, and analysis. The system is extensible to broader applications such as smart-city monitoring and disaster response, where timely tasking and automated analysis are critical.", "AI": {"tldr": "本文介绍了一种用于自动地球观测的Tip-and-Cue框架，其利用外部数据点生成任务列表并进行调度，通过AI模型处理所得数据，并展示了在远洋船只追踪中的有效性。该框架可扩展到更多应用领域，如智慧城市监控和灾难响应。", "motivation": "随着卫星星座的增加、任务延迟的降低以及传感能力的多样化，自动地球观测的机会得以扩展。为了满足这一需求，本文旨在通过引入Tip-and-Cue框架，实现卫星成像任务的自动规划和调度，从而提升观测的效率和实用性。", "method": "本文提出了一种完全自动化的Tip-and-Cue框架，用于卫星成像任务和调度。该框架通过外部数据源或先前卫星图像的分析生成提示，识别时空目标，并为下游规划进行优先级排序。对应的线索是响应生成的成像任务，这些任务考虑了传感器约束、时间要求和效用函数。系统自主生成候选任务，使用连续效用函数优化其调度，这些函数反映每个观测的预期价值，并使用包括物体检测器和视觉语言模型在内的AI模型处理得出的图像。", "result": "在远洋船只轨迹预测，目标观察和生成行动输出的场景下，框架表现出了效能。利用自动识别系统（AIS）数据实现了这些任务。", "conclusion": "该框架展示了在远洋船只追踪中的有效性，并且由于其设计灵活性，可以扩展至更广泛的应用领域中，例如智慧城市监控和灾难响应，其中快速的自动化分析至关重要。"}}
{"id": "2512.09687", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09687", "abs": "https://arxiv.org/abs/2512.09687", "authors": ["Er Jin", "Yang Zhang", "Yongli Mou", "Yanfei Dong", "Stefan Decker", "Kenji Kawaguchi", "Johannes Stegmaier"], "title": "Unconsciously Forget: Mitigating Memorization; Without Knowing What is being Memorized", "comment": null, "summary": "Recent advances in generative models have demonstrated an exceptional ability to produce highly realistic images. However, previous studies show that generated images often resemble the training data, and this problem becomes more severe as the model size increases. Memorizing training data can lead to legal challenges, including copyright infringement, violations of portrait rights, and trademark violations. Existing approaches to mitigating memorization mainly focus on manipulating the denoising sampling process to steer image embeddings away from the memorized embedding space or employ unlearning methods that require training on datasets containing specific sets of memorized concepts. However, existing methods often incur substantial computational overhead during sampling, or focus narrowly on removing one or more groups of target concepts, imposing a significant limitation on their scalability. To understand and mitigate these problems, our work, UniForget, offers a new perspective on understanding the root cause of memorization. Our work demonstrates that specific parts of the model are responsible for copyrighted content generation. By applying model pruning, we can effectively suppress the probability of generating copyrighted content without targeting specific concepts while preserving the general generative capabilities of the model. Additionally, we show that our approach is both orthogonal and complementary to existing unlearning methods, thereby highlighting its potential to improve current unlearning and de-memorization techniques.", "AI": {"tldr": "提出UniForget方法通过剪枝模型中的特定部分来减少生成模型的训练数据记忆问题，这种方法不需要针对具体概念，同时保留模型的一般生成能力，并且可以改进现有的遗忘和去记忆技术。", "motivation": "解决生成模型在生成图像过程中记忆训练数据的问题，尤其是大型模型中的严重问题，以避免法律挑战如版权侵权、肖像权和商标侵权等。", "method": "通过对模型进行剪枝，减少模型生成受版权保护内容的概率，同时不针对具体概念进行操作，并保留模型的总体生成能力。", "result": "UniForget可以有效抑制生成受版权保护内容的概率，并且方法与现有的遗忘方法彼此独立且互补，展示了其改进当前技术的潜力。", "conclusion": "通过模型剪枝，UniForget提供了一种新颖的方式来理解和缓解记忆问题，这种方法不仅有效而且具有广泛适用性。"}}
{"id": "2512.09700", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.09700", "abs": "https://arxiv.org/abs/2512.09700", "authors": ["Seon-Hoon Kim", "Hyeji Sim", "Youeyun Jung", "Ok-Chul Jung", "Yerin Kim"], "title": "LiM-YOLO: Less is More with Pyramid Level Shift and Normalized Auxiliary Branch for Ship Detection in Optical Remote Sensing Imagery", "comment": "16 pages, 8 figures, 9 tables", "summary": "Applying general-purpose object detectors to ship detection in satellite imagery presents significant challenges due to the extreme scale disparity and morphological anisotropy of maritime targets. Standard architectures utilizing stride-32 (P5) layers often fail to resolve narrow vessels, resulting in spatial feature dilution. In this work, we propose LiM-YOLO, a specialized detector designed to resolve these domain-specific conflicts. Based on a statistical analysis of ship scales, we introduce a Pyramid Level Shift Strategy that reconfigures the detection head to P2-P4. This shift ensures compliance with Nyquist sampling criteria for small objects while eliminating the computational redundancy of deep layers. To further enhance training stability on high-resolution inputs, we incorporate a Group Normalized Convolutional Block for Linear Projection (GN-CBLinear), which mitigates gradient volatility in micro-batch settings. Validated on SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, LiM-YOLO demonstrates superior detection accuracy and efficiency compared to state-of-the-art models. The code is available at https://github.com/egshkim/LiM-YOLO.", "AI": {"tldr": "LiM-YOLO在卫星图像中改进了对细小船只的检测，通过层级适应策略和新型结构增强，提高了检测准确性和效率。", "motivation": "通用目标检测器在卫星图像中的船舶检测面临着尺度差异大和形态异质性大的挑战，传统的架构利用stride-32层往往无法分辨出细小的船只，导致空间特征稀释。", "method": "提出了一种专门针对船舶检测问题的检测器LiM-YOLO，通过金字塔层级转移策略重新配置检测头，从P5调整到P2-P4，保证对小目标的采样同时减少深层计算的冗余。同时，引入了带有组标准化的线性投影卷积块GN-CBLinear，以提高大分辨率输入训练时的稳定性，特别是在小批量设置下减少梯度波动。", "result": "在SODA-A, DOTA-v1.5, FAIR1M-v2.0和ShipRSImageNet-V1这些数据集上验证，LiM-YOLO在检测准确性和效率方面优于最先进的模型。", "conclusion": "LiM-YOLO通过特有的层级转移策略和组标准化卷积块有效解决了卫星图像中船舶检测的小目标问题，提高了在不同尺度下的检测性能。"}}
{"id": "2512.09773", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09773", "abs": "https://arxiv.org/abs/2512.09773", "authors": ["Romain Mussard", "Aurélien Gauffre", "Ihsan Ullah", "Thanh Gia Hieu Khuong", "Massih-Reza Amini", "Isabelle Guyon", "Lisheng Sun-Hosoya"], "title": "Stylized Meta-Album: Group-bias injection with style transfer to study robustness against distribution shifts", "comment": null, "summary": "We introduce Stylized Meta-Album (SMA), a new image classification meta-dataset comprising 24 datasets (12 content datasets, and 12 stylized datasets), designed to advance studies on out-of-distribution (OOD) generalization and related topics. Created using style transfer techniques from 12 subject classification datasets, SMA provides a diverse and extensive set of 4800 groups, combining various subjects (objects, plants, animals, human actions, textures) with multiple styles. SMA enables flexible control over groups and classes, allowing us to configure datasets to reflect diverse benchmark scenarios. While ideally, data collection would capture extensive group diversity, practical constraints often make this infeasible. SMA addresses this by enabling large and configurable group structures through flexible control over styles, subject classes, and domains-allowing datasets to reflect a wide range of real-world benchmark scenarios. This design not only expands group and class diversity, but also opens new methodological directions for evaluating model performance across diverse group and domain configurations-including scenarios with many minority groups, varying group imbalance, and complex domain shifts-and for studying fairness, robustness, and adaptation under a broader range of realistic conditions. To demonstrate SMA's effectiveness, we implemented two benchmarks: (1) a novel OOD generalization and group fairness benchmark leveraging SMA's domain, class, and group diversity to evaluate existing benchmarks. Our findings reveal that while simple balancing and algorithms utilizing group information remain competitive as claimed in previous benchmarks, increasing group diversity significantly impacts fairness, altering the superiority and relative rankings of algorithms. We also propose to use \\textit{Top-M worst group accuracy} as a new hyperparameter tuning metric, demonstrating broader fairness during optimization and delivering better final worst-group accuracy for larger group diversity. (2) An unsupervised domain adaptation (UDA) benchmark utilizing SMA's group diversity to evaluate UDA algorithms across more scenarios, offering a more comprehensive benchmark with lower error bars (reduced by 73\\% and 28\\% in closed-set setting and UniDA setting, respectively) compared to existing efforts. These use cases highlight SMA's potential to significantly impact the outcomes of conventional benchmarks.", "AI": {"tldr": "提出了一种新的元数据集SMA，通过样式转移技术创建，旨在扩展图像分类领域内外泛化和公平性的研究；成功展示了通过SMA改造现有基准的可能性，同时发现组多样性对算法性能和公平性有重大影响。", "motivation": "尽管理想的数据收集方式应在广泛的不同组别之间多样化，但实际限制往往使得这变得不切实际。因此，SMA通过灵活控制样式、主题类和领域，使大规模和可配置的组结构成为可能，从而可以反映各种真实世界的基准场景。这种设计不仅扩展了组和类别的多样性，而且为在不同群体和领域配置下评估模型性能、以及在更广泛的实际情况中研究公平性、鲁棒性和适应性打开了新的方法论方向。", "method": "我们介绍了Stylized Meta-Album (SMA)，一个新图像分类元数据集，包含了24个数据集（12个内容数据集和12个风格化数据集），旨在促进关于分布外(OOD)泛化及相关主题的研究。通过从12个主题分类数据集中应用样式转移技术创建SMA，提供了4800种不同组合的多样化和广泛化的数据集。", "result": "通过执行两个基准来验证SMA的有效性: 一个新颖的OOD泛化和组公平性基准，利用SMA的领域、类别和组多样性来评估现有基准；一个无监督领域适应（UDA）基准，利用SMA的组多样性来评估更多场景下的UDA算法，相比现有基准提供了更全面的评估，误差降低。此外，我们提议使用\"Top-M最差组准确性\"作为新的超参数调整指标，证明了在优化过程中的更广泛公平性，并为更大的组多样性带来了更好的最终最坏组准确性。", "conclusion": "这些用途展示了SMA具有显著影响传统基准结果的潜力。"}}
{"id": "2512.09792", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09792", "abs": "https://arxiv.org/abs/2512.09792", "authors": ["Pierre Ancey", "Andrew Price", "Saqib Javed", "Mathieu Salzmann"], "title": "FastPose-ViT: A Vision Transformer for Real-Time Spacecraft Pose Estimation", "comment": "Accepted to WACV 2026. Preprint version", "summary": "Estimating the 6-degrees-of-freedom (6DoF) pose of a spacecraft from a single image is critical for autonomous operations like in-orbit servicing and space debris removal. Existing state-of-the-art methods often rely on iterative Perspective-n-Point (PnP)-based algorithms, which are computationally intensive and ill-suited for real-time deployment on resource-constrained edge devices. To overcome these limitations, we propose FastPose-ViT, a Vision Transformer (ViT)-based architecture that directly regresses the 6DoF pose. Our approach processes cropped images from object bounding boxes and introduces a novel mathematical formalism to map these localized predictions back to the full-image scale. This formalism is derived from the principles of projective geometry and the concept of \"apparent rotation\", where the model predicts an apparent rotation matrix that is then corrected to find the true orientation. We demonstrate that our method outperforms other non-PnP strategies and achieves performance competitive with state-of-the-art PnP-based techniques on the SPEED dataset. Furthermore, we validate our model's suitability for real-world space missions by quantizing it and deploying it on power-constrained edge hardware. On the NVIDIA Jetson Orin Nano, our end-to-end pipeline achieves a latency of ~75 ms per frame under sequential execution, and a non-blocking throughput of up to 33 FPS when stages are scheduled concurrently.", "AI": {"tldr": "提出了FastPose-ViT模型，通过Vision Transformer直接预测飞行器6自由度姿态，适合实时部署于资源受限的设备上。", "motivation": "现有方法依赖于迭代的透视-n-点（PnP）算法，计算代价高且不适合用于资源受限的边缘设备。因此作者提出了一种新的方法FastPose-ViT，旨在解决这些问题，能够适配实时部署。", "method": "FastPose-ViT, 通过Vision Transformer（ViT）直接回归6自由度姿态。此方法对从目标边界框裁剪出的图像进行处理，并引入了一种新的数学形式，用于将局部预测映射到全图尺度。这个形式来源于射影几何原理和“表观旋转”概念。", "result": "该方法在SPEED数据集上的性能优于其他非PnP策略，并且在性能上与其他先进的PnP方法相当。在NVIDIA Jetson Orin Nano设备上，通过逐帧处理和阶段并行调度，该模型的延迟达到约75 ms/帧，吞吐量达到每秒33帧。", "conclusion": "FastPose-ViT不仅提高了计算效率，还验证了在实际太空任务中的适用性，特别适合于资源受限的边缘设备。"}}
{"id": "2512.09801", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09801", "abs": "https://arxiv.org/abs/2512.09801", "authors": ["Tien-Dat Chung", "Ba-Thinh Lam", "Thanh-Huy Nguyen", "Thien Nguyen", "Nguyen Lan Vi Vu", "Hoang-Loc Cao", "Phat Kim Huynh", "Min Xu"], "title": "Modality-Specific Enhancement and Complementary Fusion for Semi-Supervised Multi-Modal Brain Tumor Segmentation", "comment": "9 pages, 3 figures", "summary": "Semi-supervised learning (SSL) has become a promising direction for medical image segmentation, enabling models to learn from limited labeled data alongside abundant unlabeled samples. However, existing SSL approaches for multi-modal medical imaging often struggle to exploit the complementary information between modalities due to semantic discrepancies and misalignment across MRI sequences. To address this, we propose a novel semi-supervised multi-modal framework that explicitly enhances modality-specific representations and facilitates adaptive cross-modal information fusion. Specifically, we introduce a Modality-specific Enhancing Module (MEM) to strengthen semantic cues unique to each modality via channel-wise attention, and a learnable Complementary Information Fusion (CIF) module to adaptively exchange complementary knowledge between modalities. The overall framework is optimized using a hybrid objective combining supervised segmentation loss and cross-modal consistency regularization on unlabeled data. Extensive experiments on the BraTS 2019 (HGG subset) demonstrate that our method consistently outperforms strong semi-supervised and multi-modal baselines under 1\\%, 5\\%, and 10\\% labeled data settings, achieving significant improvements in both Dice and Sensitivity scores. Ablation studies further confirm the complementary effects of our proposed MEM and CIF in bridging cross-modality discrepancies and improving segmentation robustness under scarce supervision.", "AI": {"tldr": "The paper proposes a new semi-supervised multi-modal medical image segmentation framework that enhances modality-specific representations and allows adaptive cross-modal information fusion, outperforming existing methods on a benchmark MRI segmentation dataset.", "motivation": "The motivation behind the work is to improve semi-supervised learning in multi-modal medical image segmentation, addressing the challenge of exploiting complementary information between different modalities.", "method": "Our method introduces a Modality-specific Enhancing Module (MEM) for strengthening modality-specific features and a Complementary Information Fusion (CIF) module for adaptive cross-modal information exchange, optimized by a hybrid loss function under semi-supervised conditions.", "result": "Experiments show that the proposed method outperforms strong baselines in Dice and Sensitivity scores under 1%, 5%, and 10% labeled data settings on the BraTS 2019 (HGG subset) dataset.", "conclusion": "The conclusion of the paper is that the proposed method achieves significant improvements in segmentation performance under various labeled data settings compared to existing baselines, demonstrating the effectiveness and robustness of the MEM and CIF modules."}}
{"id": "2512.09806", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09806", "abs": "https://arxiv.org/abs/2512.09806", "authors": ["Jianfei Li", "Ines Rosellon-Inclan", "Gitta Kutyniok", "Jean-Luc Starck"], "title": "CHEM: Estimating and Understanding Hallucinations in Deep Learning for Image Processing", "comment": null, "summary": "U-Net and other U-shaped architectures have achieved significant success in image deconvolution tasks. However, challenges have emerged, as these methods might generate unrealistic artifacts or hallucinations, which can interfere with analysis in safety-critical scenarios. This paper introduces a novel approach for quantifying and comprehending hallucination artifacts to ensure trustworthy computer vision models. Our method, termed the Conformal Hallucination Estimation Metric (CHEM), is applicable to any image reconstruction model, enabling efficient identification and quantification of hallucination artifacts. It offers two key advantages: it leverages wavelet and shearlet representations to efficiently extract hallucinations of image features and uses conformalized quantile regression to assess hallucination levels in a distribution-free manner. Furthermore, from an approximation theoretical perspective, we explore the reasons why U-shaped networks are prone to hallucinations. We test the proposed approach on the CANDELS astronomical image dataset with models such as U-Net, SwinUNet, and Learnlets, and provide new perspectives on hallucination from different aspects in deep learning-based image processing.", "AI": {"tldr": "The paper proposes the Conformal Hallucination Estimation Metric (CHEM), a method to quantify and understand hallucination artifacts in image reconstruction models, with applications on CANDELS astronomical image dataset and models such as U-Net, SwinUNet, and Learnlets.", "motivation": "To address the challenges of unrealistic artifacts or hallucinations generated by U-shaped architectures in safety-critical scenarios, this paper introduces a novel approach for quantifying and comprehending these artifacts.", "method": "Our method, termed the Conformal Hallucination Estimation Metric (CHEM), is applicable to any image reconstruction model, enabling efficient identification and quantification of hallucination artifacts. It leverages wavelet and shearlet representations to extract hallucinations of image features and uses conformalized quantile regression to assess hallucination levels in a distribution-free manner.", "result": "The approach was tested on the CANDELS astronomical image dataset with models such as U-Net, SwinUNet, and Learnlets, demonstrating its effectiveness in quantifying and understanding hallucination artifacts.", "conclusion": "The proposed CHEM provides new perspectives on hallucinations from different aspects in deep learning-based image processing, contributing to the development of more trustworthy computer vision models."}}
{"id": "2512.09814", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09814", "abs": "https://arxiv.org/abs/2512.09814", "authors": ["Zhizhong Wang", "Tianyi Chu", "Zeyi Huang", "Nanyang Wang", "Kehan Li"], "title": "DynaIP: Dynamic Image Prompt Adapter for Scalable Zero-shot Personalized Text-to-Image Generation", "comment": null, "summary": "Personalized Text-to-Image (PT2I) generation aims to produce customized images based on reference images. A prominent interest pertains to the integration of an image prompt adapter to facilitate zero-shot PT2I without test-time fine-tuning. However, current methods grapple with three fundamental challenges: 1. the elusive equilibrium between Concept Preservation (CP) and Prompt Following (PF), 2. the difficulty in retaining fine-grained concept details in reference images, and 3. the restricted scalability to extend to multi-subject personalization. To tackle these challenges, we present Dynamic Image Prompt Adapter (DynaIP), a cutting-edge plugin to enhance the fine-grained concept fidelity, CP-PF balance, and subject scalability of SOTA T2I multimodal diffusion transformers (MM-DiT) for PT2I generation. Our key finding is that MM-DiT inherently exhibit decoupling learning behavior when injecting reference image features into its dual branches via cross attentions. Based on this, we design an innovative Dynamic Decoupling Strategy that removes the interference of concept-agnostic information during inference, significantly enhancing the CP-PF balance and further bolstering the scalability of multi-subject compositions. Moreover, we identify the visual encoder as a key factor affecting fine-grained CP and reveal that the hierarchical features of commonly used CLIP can capture visual information at diverse granularity levels. Therefore, we introduce a novel Hierarchical Mixture-of-Experts Feature Fusion Module to fully leverage the hierarchical features of CLIP, remarkably elevating the fine-grained concept fidelity while also providing flexible control of visual granularity. Extensive experiments across single- and multi-subject PT2I tasks verify that our DynaIP outperforms existing approaches, marking a notable advancement in the field of PT2l generation.", "AI": {"tldr": "提出了DynaIP，改善了SOTA T2I扩散变压器在个性化文本到图像生成中的细节保真度、概念保存与提示跟随平衡及多功能主体个性化扩展能力。", "motivation": "解决当前个性化文本到图像生成中的三大挑战：1. 保持概念保真与提示跟随之间的平衡，2. 保留参考图像的细节，3. 扩展到多主体个性化的能力。", "method": "设计了一个动态解耦策略和一个多专家特征融合模块，通过解耦干扰信息和利用分层特征提高概念保真度和多主体扩展能力。", "result": "实验验证了DynaIP在单一和多主体个性化文本到图像生成任务中的有效性，显著优于现有方法。", "conclusion": "DynaIP通过引入动态解耦策略和分层特征利用，显著提升了个性化文本到图像生成的质量，是该领域的重大进展。"}}
{"id": "2512.09824", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.09824", "abs": "https://arxiv.org/abs/2512.09824", "authors": ["Xianghao Kong", "Zeyu Zhang", "Yuwei Guo", "Zhuoran Zhao", "Songchun Zhang", "Anyi Rao"], "title": "Composing Concepts from Images and Videos via Concept-prompt Binding", "comment": "Project page: https://refkxh.github.io/BiCo_Webpage/", "summary": "Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.", "AI": {"tldr": "为解决视觉概念组合在提取复杂概念和灵活组合图像或视频概念上的不足，研究者提出了Bind & Compose方法，通过特定机制提升了概念绑定的准确性和跨模态的兼容性。", "motivation": "当前，视觉概念组合在准确提取视觉输入中的复杂概念和灵活结合图像与视频概念方面仍存在不足。该研究旨在解决这些问题，通过提出一种新的一次性方法Bind & Compose，以更灵活地进行视觉概念组合。", "method": "Bind & Compose采用了一种绑定视觉概念与相应提示标记并将目标提示与来自不同来源的绑定标记组合的一次性方法。该方法采用分层绑定器结构对Diffusion Transformers进行跨注意力条件设置，从而将视觉概念编码为对应的提示标记，实现复杂视觉概念的准确分解。为了提高概念标记绑定的准确性，设计了多样性和吸收机制，通过使用额外的吸收标记来消除在使用多样化提示训练时概念无关细节的影响。为了提升图像和视频概念之间的兼容性，提出了一种时间分离策略，将视频概念的训练过程分解为两个阶段，并采用双分支绑定器结构进行时间建模。", "result": "实验评估证明，该方法在概念一致性、提示保真度和运动质量方面优于现有的方法。", "conclusion": "总之，这种方法开启了许多新的可能性，提升了视觉创意领域的探索与发展。"}}
{"id": "2512.09847", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09847", "abs": "https://arxiv.org/abs/2512.09847", "authors": ["Shijia Feng", "Michael Wray", "Walterio Mayol-Cuevas"], "title": "From Detection to Anticipation: Online Understanding of Struggles across Various Tasks and Activities", "comment": "Accepted by WACV 2026", "summary": "Understanding human skill performance is essential for intelligent assistive systems, with struggle recognition offering a natural cue for identifying user difficulties. While prior work focuses on offline struggle classification and localization, real-time applications require models capable of detecting and anticipating struggle online. We reformulate struggle localization as an online detection task and further extend it to anticipation, predicting struggle moments before they occur. We adapt two off-the-shelf models as baselines for online struggle detection and anticipation. Online struggle detection achieves 70-80% per-frame mAP, while struggle anticipation up to 2 seconds ahead yields comparable performance with slight drops. We further examine generalization across tasks and activities and analyse the impact of skill evolution. Despite larger domain gaps in activity-level generalization, models still outperform random baselines by 4-20%. Our feature-based models run at up to 143 FPS, and the whole pipeline, including feature extraction, operates at around 20 FPS, sufficient for real-time assistive applications.", "AI": {"tldr": "本文将挣扎检测问题设置为在线检测和预测问题，并测试其在不同任务和活动上的性能，模型运行速度达到143 FPS，适合实时应用。", "motivation": "理解人类技能表现对于智能辅助系统至关重要，而挣扎识别提供了识别用户困难的自然线索。实时应用需要能够在线检测和预测挣扎的模型。", "method": "我们将挣扎定位重新定义为在线检测任务，并进一步扩展到预测，即在挣扎发生之前预测挣扎时刻。我们采用两个现成的模型作为基线来实现在线挣扎检测和预测。", "result": "在线挣扎检测达到了70-80%每帧的mAP，而提前2秒预测挣扎的表现与前者相当，略有下降。我们在不同任务和活动上测试了模型的泛化能力，尽管活动级别的泛化存在较大的领域差距，模型表现仍优于随机基线4-20%。", "conclusion": "我们的特征模型运行速度达到143 FPS，整个管道，包括特征提取，运行速度约为20 FPS，足以支持实时辅助应用。"}}
{"id": "2512.09864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09864", "abs": "https://arxiv.org/abs/2512.09864", "authors": ["Hao Lu", "Ziyang Liu", "Guangfeng Jiang", "Yuanfei Luo", "Sheng Chen", "Yangang Zhang", "Ying-Cong Chen"], "title": "UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving", "comment": "Project Page: https://seed-uniugp.github.io/", "summary": "Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.", "AI": {"tldr": "本文提出了一种新的自动驾驶系统框架UniUGP，通过结合视觉语言模型和视频生成能力，实现了对复杂场景的理解、生成和规划，能更好地处理长尾场景。", "motivation": "现有的自动驾驶系统在处理长尾场景时遇到困难，主要因为世界知识有限以及弱化的视觉动态建模能力。为了避免这些缺点，研究者们需要结合未标记的视频数据进行视觉因果学习，并且结合大型语言模型进行推理。", "method": "提出了一种名为UniUGP的统一理解-生成-规划框架，该框架通过混合专家架构整合了预先训练的视觉语言模型和视频生成模型，旨在协同进行场景推理、未来视频生成和轨迹规划。", "result": "实验展示了在感知、推理和决策上达到了最先进的效果，并且在挑战性的长尾场景中表现出更好的泛化能力。", "conclusion": "通过整合大规模语言模型和视频生成模型，UniUGP框架显著提升了在自动驾驶系统中的感知、推理和决策能力，特别对于长尾场景挑战提供了解决方案。"}}
{"id": "2512.09871", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09871", "abs": "https://arxiv.org/abs/2512.09871", "authors": ["Yimin Zhu", "Lincoln Linlin Xu"], "title": "Diffusion Posterior Sampler for Hyperspectral Unmixing with Spectral Variability Modeling", "comment": null, "summary": "Linear spectral mixture models (LMM) provide a concise form to disentangle the constituent materials (endmembers) and their corresponding proportions (abundance) in a single pixel. The critical challenges are how to model the spectral prior distribution and spectral variability. Prior knowledge and spectral variability can be rigorously modeled under the Bayesian framework, where posterior estimation of Abundance is derived by combining observed data with endmember prior distribution. Considering the key challenges and the advantages of the Bayesian framework, a novel method using a diffusion posterior sampler for semiblind unmixing, denoted as DPS4Un, is proposed to deal with these challenges with the following features: (1) we view the pretrained conditional spectrum diffusion model as a posterior sampler, which can combine the learned endmember prior with observation to get the refined abundance distribution. (2) Instead of using the existing spectral library as prior, which may raise bias, we establish the image-based endmember bundles within superpixels, which are used to train the endmember prior learner with diffusion model. Superpixels make sure the sub-scene is more homogeneous. (3) Instead of using the image-level data consistency constraint, the superpixel-based data fidelity term is proposed. (4) The endmember is initialized as Gaussian noise for each superpixel region, DPS4Un iteratively updates the abundance and endmember, contributing to spectral variability modeling. The experimental results on three real-world benchmark datasets demonstrate that DPS4Un outperforms the state-of-the-art hyperspectral unmixing methods.", "AI": {"tldr": "A novel method, DPS4Un, is proposed for semiblind unmixing in hyperspectral imagery using a diffusion posterior sampler to better model spectral variability and prior information from superpixels.", "motivation": "The motivation is to address the limitations of current linear spectral mixture models (LMMs), specifically concerning spectral prior modeling and variability, through a new Bayesian framework method.", "method": "The paper proposes DPS4Un, a diffusion posterior sampler for semiblind unmixing, which integrates learned endmember priors from superpixels with observed data, aiming to solve challenges in spectral prior modeling and variability.", "result": "Experimental results on three real-world datasets show that DPS4Un surpasses existing hyperspectral unmixing techniques.", "conclusion": "The conclusion is that DPS4Un effectively improves the semiblind unmixing process by providing a robust method for integrating spectral prior knowledge and variability, overcoming the limitations of traditional methods."}}
{"id": "2512.09874", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09874", "abs": "https://arxiv.org/abs/2512.09874", "authors": ["Pius Horn", "Janis Keuper"], "title": "Benchmarking Document Parsers on Mathematical Formula Extraction from PDFs", "comment": null, "summary": "Correctly parsing mathematical formulas from PDFs is critical for training large language models and building scientific knowledge bases from academic literature, yet existing benchmarks either exclude formulas entirely or lack semantically-aware evaluation metrics. We introduce a novel benchmarking framework centered on synthetically generated PDFs with precise LaTeX ground truth, enabling systematic control over layout, formulas, and content characteristics. A key methodological contribution is pioneering LLM-as-a-judge for semantic formula assessment, combined with a robust two-stage matching pipeline that handles parser output inconsistencies. Through human validation on 250 formula pairs (750 ratings from 30 evaluators), we demonstrate that LLM-based evaluation achieves substantially higher correlation with human judgment (Pearson r=0.78) compared to CDM (r=0.34) and text similarity (r~0). Evaluating 20+ contemporary PDF parsers (including specialized OCR models, vision-language models, and rule-based approaches) across 100 synthetic documents with 2,000+ formulas reveals significant performance disparities. Our findings provide crucial insights for practitioners selecting parsers for downstream applications and establish a robust, scalable methodology that enables reproducible evaluation of PDF formula extraction quality. Code and benchmark data: https://github.com/phorn1/pdf-parse-bench", "AI": {"tldr": "研究提出了一种新的基准框架，专注于合成生成的PDF与精确LaTeX地面真相，以解决PDF中数学公式语义解析的挑战。", "motivation": "正确解析PDF中的数学公式对于训练大型语言模型和构建科学知识库至关重要，但现有的基准测试框架要么完全不包括公式，要么缺乏语义感知的评估指标。", "method": "我们提出了一种以合成生成的PDF为核心的新基准测试框架，这些PDF带有精确的LaTeX地面真相，使我们能够系统地控制布局、公式和内容特性。此外，我们开创了LLM（大型语言模型）作为评判者来语义上评估公式，并结合了一个强大的两阶段匹配管道来处理解析器输出的不一致性。", "result": "通过人类验证250组公式对（30位评估者提供了750个评分），表明LLM基线评估与人类判断的相关性更高（皮尔森r=0.78），而CDM（r=0.34）和文本相似度（r~0）的相关性较低。", "conclusion": "该研究为实践者选择用于下游应用的解析器提供了重要的见解，并建立了一种稳健、可扩展的方法论，以实现PDF公式提取质量的可复制评估。"}}
{"id": "2512.09907", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09907", "abs": "https://arxiv.org/abs/2512.09907", "authors": ["Daoan Zhang", "Pai Liu", "Xiaofei Zhou", "Yuan Ge", "Guangchen Lan", "Jing Bi", "Christopher Brinton", "Ehsan Hoque", "Jiebo Luo"], "title": "VisualActBench: Can VLMs See and Act like a Human?", "comment": null, "summary": "Vision-Language Models (VLMs) have achieved impressive progress in perceiving and describing visual environments. However, their ability to proactively reason and act based solely on visual inputs, without explicit textual prompts, remains underexplored. We introduce a new task, Visual Action Reasoning, and propose VisualActBench, a large-scale benchmark comprising 1,074 videos and 3,733 human-annotated actions across four real-world scenarios. Each action is labeled with an Action Prioritization Level (APL) and a proactive-reactive type to assess models' human-aligned reasoning and value sensitivity. We evaluate 29 VLMs on VisualActBench and find that while frontier models like GPT4o demonstrate relatively strong performance, a significant gap remains compared to human-level reasoning, particularly in generating proactive, high-priority actions. Our results highlight limitations in current VLMs' ability to interpret complex context, anticipate outcomes, and align with human decision-making frameworks. VisualActBench establishes a comprehensive foundation for assessing and improving the real-world readiness of proactive, vision-centric AI agents.", "AI": {"tldr": "我们提出了视觉行为推理的新任务和VisualActBench基准测试，评估了29种视觉语言模型，发现它们在主动行为的生成上仍与人类有差距，表明当前模型在决策和预测上存在局限性。", "motivation": "我们的动机在于探索和发展视觉语言模型在没有明确文本提示的情况下，根据视觉输入进行主动推理和行动的能力。", "method": "我们提出了一项新的任务——视觉行为推理，并开发了一个大规模的基准测试VisualActBench，它包含了1,074个视频和3,733个人类标注的动作，跨越了四个真实场景。", "result": "在VisualActBench测试中，虽然前沿模型如GPT4o表现出相对较强的能力，但它们在生成主动、高优先级行为上与人类水平相比仍存在显著差距。", "conclusion": "研究结果表明了目前视觉语言模型在解释复杂情景、预测结果以及与人类判断框架对齐方面的能力有限。VisualActBench为评估和提升面向实际应用的视觉导向AI代理的主动性提供了一个全面的基础。"}}
{"id": "2512.09913", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09913", "abs": "https://arxiv.org/abs/2512.09913", "authors": ["Sander Riisøen Jyhne", "Aditya Gupta", "Ben Worsley", "Marianne Andersen", "Ivar Oveland", "Alexander Salveson Nossum"], "title": "NordFKB: a fine-grained benchmark dataset for geospatial AI in Norway", "comment": "8 pages, 2 figures, 2 tables", "summary": "We present NordFKB, a fine-grained benchmark dataset for geospatial AI in Norway, derived from the authoritative, highly accurate, national Felles KartdataBase (FKB). The dataset contains high-resolution orthophotos paired with detailed annotations for 36 semantic classes, including both per-class binary segmentation masks in GeoTIFF format and COCO-style bounding box annotations. Data is collected from seven geographically diverse areas, ensuring variation in climate, topography, and urbanization. Only tiles containing at least one annotated object are included, and training/validation splits are created through random sampling across areas to ensure representative class and context distributions. Human expert review and quality control ensures high annotation accuracy. Alongside the dataset, we release a benchmarking repository with standardized evaluation protocols and tools for semantic segmentation and object detection, enabling reproducible and comparable research. NordFKB provides a robust foundation for advancing AI methods in mapping, land administration, and spatial planning, and paves the way for future expansions in coverage, temporal scope, and data modalities.", "AI": {"tldr": "开发了一个名为NordFKB的高质量地理空间AI基准数据集，提供了用于研究和开发的标准评估工具。", "motivation": "动机在于提供一个高质量的数据集以推动地理空间人工智能方法的发展，特别是在制图、土地管理和空间规划等领域。", "method": "描述了一种名为NordFKB的精细基准数据集的创建方法，该数据集用于挪威的地理空间AI，它是从权威且准确的国家Felles KartdataBase (FKB)数据导出的。", "result": "创建了一个包含详细注释的高分辨率正射图像的基准数据集，该数据集适用于语义分割和目标检测的标准化评估。", "conclusion": "NordFKB数据集为推动AI在挪威地理空间领域的研究和发展提供了坚实的基础，并为进一步扩展数据库的范围、时间范围和数据模式铺平了道路。"}}
{"id": "2512.09923", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09923", "abs": "https://arxiv.org/abs/2512.09923", "authors": ["Or Hirschorn", "Omer Sela", "Inbar Huberman-Spiegelglas", "Netalee Efrat", "Eli Alshan", "Ianir Ideses", "Frederic Devernay", "Yochai Zvik", "Lior Fritz"], "title": "Splatent: Splatting Diffusion Latents for Novel View Synthesis", "comment": null, "summary": "Radiance field representations have recently been explored in the latent space of VAEs that are commonly used by diffusion models. This direction offers efficient rendering and seamless integration with diffusion-based pipelines. However, these methods face a fundamental limitation: The VAE latent space lacks multi-view consistency, leading to blurred textures and missing details during 3D reconstruction. Existing approaches attempt to address this by fine-tuning the VAE, at the cost of reconstruction quality, or by relying on pre-trained diffusion models to recover fine-grained details, at the risk of some hallucinations. We present Splatent, a diffusion-based enhancement framework designed to operate on top of 3D Gaussian Splatting (3DGS) in the latent space of VAEs. Our key insight departs from the conventional 3D-centric view: rather than reconstructing fine-grained details in 3D space, we recover them in 2D from input views through multi-view attention mechanisms. This approach preserves the reconstruction quality of pretrained VAEs while achieving faithful detail recovery. Evaluated across multiple benchmarks, Splatent establishes a new state-of-the-art for VAE latent radiance field reconstruction. We further demonstrate that integrating our method with existing feed-forward frameworks, consistently improves detail preservation, opening new possibilities for high-quality sparse-view 3D reconstruction.", "AI": {"tldr": "Splatent, a new diffusion-based enhancement method, recovers fine-grained details in 2D space rather than 3D, thereby improving 3D reconstruction quality by leveraging 2D multi-view attention mechanisms.", "motivation": "The VAE latent space lacks multi-view consistency, leading to blurred textures and missing details during 3D reconstruction. This paper aims to solve this limitation by proposing a new method: Splatent, which enhances diffusion-based reconstruction through a 2D recovery process.", "method": "Our key insight is to depart from the conventional 3D-centric view, recovering fine-grained details in 2D space from input views via multi-view attention mechanisms, rather than in 3D space.", "result": "Splatent is evaluated across multiple benchmarks and establishes a new state-of-the-art for VAE latent radiance field reconstruction.", "conclusion": "Integrating Splatent with existing feed-forward frameworks significantly improves detail preservation in sparse-view 3D reconstruction, opening new possibilities in the field."}}
{"id": "2512.09924", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09924", "abs": "https://arxiv.org/abs/2512.09924", "authors": ["Xinyu Liu", "Hangjie Yuan", "Yujie Wei", "Jiazheng Xing", "Yujin Han", "Jiahao Pan", "Yanbiao Ma", "Chi-Min Chan", "Kang Zhao", "Shiwei Zhang", "Wenhan Luo", "Yike Guo"], "title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning", "comment": null, "summary": "Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, a comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, a Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within a single architecture. The model's internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generator's reasoning behavior during training. Extensive experiments on RVE-Bench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving a 32% improvement of the Overall score in the reasoning-informed video editing subset over state-of-the-art methods.", "AI": {"tldr": "本文提出了一个名为ReViSE的自我反思推理框架，旨在通过整合生成和评估过程来改进推理引导的视频编辑，实验证明这种方法在编辑精度和视觉保真度方面有显著提高。", "motivation": "现有的视频统一模型虽然在理解和生成方面表现出色，但在推理引导的视频编辑方面存在不足。主要原因有两个：一是现有的数据集不足以支持训练和评估推理驱动的视频编辑；二是模型的推理能力和编辑能力之间存在脱节，这导致丰富的理解无法有效地指导编辑过程。", "method": "我们提出了一种名为ReViSE的自我反思推理（SRF）框架，它在单一架构中统一了生成和评估。模型内部的视觉语言模型（VLM）提供内在反馈，通过评估编辑后的视频是否符合给定指令的逻辑来精炼生成器的推理行为。", "result": "通过在RVE-Bench基准测试中的广泛实验，ReViSE框架证明了其在增强编辑准确性和视觉保真度方面的能力。", "conclusion": "实验结果表明，ReViSE框架显著提高了编辑精度和视觉保真度，在推理引导的视频编辑子集中的总体评分比最先进的方法提高了32%。"}}
{"id": "2512.09925", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09925", "abs": "https://arxiv.org/abs/2512.09925", "authors": ["Patrick Noras", "Jun Myeong Choi", "Didier Stricker", "Pieter Peers", "Roni Sengupta"], "title": "GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures", "comment": "23 pages, 18 figures", "summary": "Recent advances in Gaussian Splatting-based inverse rendering extend Gaussian primitives with shading parameters and physically grounded light transport, enabling high-quality material recovery from dense multi-view captures. However, these methods degrade sharply under sparse-view settings, where limited observations lead to severe ambiguity between geometry, reflectance, and lighting. We introduce GAINS (Gaussian-based Inverse rendering from Sparse multi-view captures), a two-stage inverse rendering framework that leverages learning-based priors to stabilize geometry and material estimation. GAINS first refines geometry using monocular depth/normal and diffusion priors, then employs segmentation, intrinsic image decomposition (IID), and diffusion priors to regularize material recovery. Extensive experiments on synthetic and real-world datasets show that GAINS significantly improves material parameter accuracy, relighting quality, and novel-view synthesis compared to state-of-the-art Gaussian-based inverse rendering methods, especially under sparse-view settings. Project page: https://patrickbail.github.io/gains/", "AI": {"tldr": "GAINS, a two-stage inverse rendering framework, leverages learning-based priors to stabilize geometry and material estimation in sparse-view settings, outperforming existing Gaussian-based methods in material recovery accuracy, relighting, and novel-view synthesis.", "motivation": "To overcome the degradation of Gaussian Splatting-based methods under sparse-view settings and improve material recovery accuracy, relighting quality, and novel-view synthesis.", "method": "Recent advances in Gaussian Splatting-based inverse rendering have limitations in sparse-view settings due to severe ambiguity between geometry, reflectance, and lighting. GAINS addresses this by employing a two-stage framework: it first refines geometry using monocular depth/normal and diffusion priors, then it uses segmentation, intrinsic image decomposition (IID), and diffusion priors to regularize material recovery.", "result": "Experiments on various datasets showed GAINS improves material parameter accuracy, relighting quality, and novel-view synthesis compared to existing methods under sparse-view conditions.", "conclusion": "GAINS demonstrates significant improvements in inverse rendering under sparse-view settings by providing a stable geometry and material estimation in challenging conditions."}}
