{"id": "2510.13889", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13889", "abs": "https://arxiv.org/abs/2510.13889", "authors": ["Yue Hu", "Guohang Zhuang"], "title": "MultiFoodhat: A potential new paradigm for intelligent food quality inspection", "comment": null, "summary": "Food image classification plays a vital role in intelligent food quality\ninspection, dietary assessment, and automated monitoring. However, most\nexisting supervised models rely heavily on large labeled datasets and exhibit\nlimited generalization to unseen food categories. To overcome these challenges,\nthis study introduces MultiFoodChat, a dialogue-driven multi-agent reasoning\nframework for zero-shot food recognition. The framework integrates\nvision-language models (VLMs) and large language models (LLMs) to enable\ncollaborative reasoning through multi-round visual-textual dialogues. An Object\nPerception Token (OPT) captures fine-grained visual attributes, while an\nInteractive Reasoning Agent (IRA) dynamically interprets contextual cues to\nrefine predictions. This multi-agent design allows flexible and human-like\nunderstanding of complex food scenes without additional training or manual\nannotations. Experiments on multiple public food datasets demonstrate that\nMultiFoodChat achieves superior recognition accuracy and interpretability\ncompared with existing unsupervised and few-shot methods, highlighting its\npotential as a new paradigm for intelligent food quality inspection and\nanalysis.", "AI": {"tldr": "MultiFoodChat提供了一种创新的方法，用于零样本食物识别，通过多代理框架、视觉-语言模型和大型语言模型的结合，以实现更准确和可解释的食物识别。", "motivation": "现有的监督模型高度依赖大规模的标注数据集，并且在未见过的食物类别中表现出有限的泛化能力。为克服这些挑战，这项研究提出的方法旨在通过不增加额外训练或手动标注，实现对复杂食物场景的灵活而像人的理解和识别。", "method": "本研究引入了MultiFoodChat，这是一个基于对话的多代理推理框架，用于零样本食物识别。该框架结合了视觉-语言模型（VLM）和大型语言模型（LLM），通过多回合视觉-文本对话实现协作推理。对象感知令牌（OPT）捕获细粒度的视觉属性，而交互推理代理（IRA）动态解释上下文线索以精化预测。", "result": "在多个公开数据集上的实验表明，MultiFoodChat在零样本食物识别上取得的准确性表现出色，并且能够提供明确的推理过程，显示出其作为新型智能食物监控和分析方法的潜力。", "conclusion": "实验结果显示，MultiFoodChat在多个公共食物数据集上的识别准确率和可解释性均超过了现有无监督和少样本方法，证明其在智能食物质量监控和分析中具有潜力。"}}
{"id": "2510.13899", "categories": ["cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.13899", "abs": "https://arxiv.org/abs/2510.13899", "authors": ["Andreas Leibetseder", "Klaus Schoeffmann", "Jörg Keckstein", "Simon Keckstein"], "title": "Post-surgical Endometriosis Segmentation in Laparoscopic Videos", "comment": "This is a demo paper that was already published\n  https://ieeexplore.ieee.org/document/9461900 but a preprint/author's copy is\n  needed for the funding agency", "summary": "Endometriosis is a common women's condition exhibiting a manifold visual\nappearance in various body-internal locations. Having such properties makes its\nidentification very difficult and error-prone, at least for laymen and\nnon-specialized medical practitioners. In an attempt to provide assistance to\ngynecologic physicians treating endometriosis, this demo paper describes a\nsystem that is trained to segment one frequently occurring visual appearance of\nendometriosis, namely dark endometrial implants. The system is capable of\nanalyzing laparoscopic surgery videos, annotating identified implant regions\nwith multi-colored overlays and displaying a detection summary for improved\nvideo browsing.", "AI": {"tldr": "本论文介绍了一个系统，它能够自动识别腹腔镜视频中的暗色子宫内膜植入物，并通过多色叠加和检测摘要的形式辅助医生的诊断。", "motivation": "由于子宫内膜异位症在身体内部的不同位置显示多样化的视觉外观，其识别非常困难且容易出错，尤其是在非专业人士和非专科医疗从业者的操作下。因此，该论文旨在为治疗子宫内膜异位症的妇科医生提供辅助。", "method": "该论文描述了一个系统，该系统被训练来分割一种常见的子宫内膜异位症外观，特别是暗色的子宫内膜植入物。系统能够分析腹腔镜手术视频，并使用多色叠加标注识别的植入区域，同时显示检测摘要以改善视频浏览体验。", "result": "未提供具体的结果数据，但系统设计目的是提供视觉辅助，帮助医生更准确地识别子宫内膜异位症。", "conclusion": "该系统展示了通过技术手段提高子宫内膜异位症识别准确性的潜力，但具体应用效果还需要进一步的临床验证。"}}
{"id": "2510.13993", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13993", "abs": "https://arxiv.org/abs/2510.13993", "authors": ["Jia Yun Chua", "Argyrios Zolotas", "Miguel Arana-Catania"], "title": "Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models", "comment": "11 pages, 7 figures, 8 tables. To be published in Applied AI Letters", "summary": "Remote sensing has become a vital tool across sectors such as urban planning,\nenvironmental monitoring, and disaster response. While the volume of data\ngenerated has increased significantly, traditional vision models are often\nconstrained by the requirement for extensive domain-specific labelled data and\ntheir limited ability to understand the context within complex environments.\nVision Language Models offer a complementary approach by integrating visual and\ntextual data; however, their application to remote sensing remains\nunderexplored, particularly given their generalist nature. This work\ninvestigates the combination of vision models and VLMs to enhance image\nanalysis in remote sensing, with a focus on aircraft detection and scene\nunderstanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, and\nGemini aims to achieve more accurate and contextually aware image\ninterpretation. Performance is evaluated on both labelled and unlabelled remote\nsensing data, as well as degraded image scenarios which are crucial for remote\nsensing. The findings show an average MAE improvement of 48.46% across models\nin the accuracy of aircraft detection and counting, especially in challenging\nconditions, in both raw and degraded scenarios. A 6.17% improvement in\nCLIPScore for comprehensive understanding of remote sensing images is obtained.\nThe proposed approach combining traditional vision models and VLMs paves the\nway for more advanced and efficient remote sensing image analysis, especially\nin few-shot learning scenarios.", "AI": {"tldr": "通过结合传统的视觉模型和视觉语言模型（VLMs），本文提出了一种新的遥感图像分析方法，实现了飞机检测和场景理解的显著改善，尤其是在少量标注数据的情况下。", "motivation": "传统的视觉模型在遥感图像分析中受限于大规模领域特定标注数据的需求以及理解复杂环境上下文的能力不足。视觉语言模型（VLMs）虽然提供了一种互补的方法来整合视觉和文本数据，但它们在遥感领域的应用尚未得到充分探索。", "method": "本文通过将YOLO模型与诸如LLaVA、ChatGPT和Gemini等VLMs进行集成，以提高遥感图像中的飞机检测和场景理解的准确性。", "result": "实验表明，提出的组合模型在有标和无标数据以及退化图像场景中，均能有效提升飞机检测和计数的准确度，平均MAE提高了48.46%，并且在CLIPScore上也获得了6.17%的改进。", "conclusion": "该方法为遥感图像分析提供了更先进和高效的解决方案，尤其是在少量标注学习场景下。"}}
{"id": "2510.13995", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13995", "abs": "https://arxiv.org/abs/2510.13995", "authors": ["Kelvin Szolnoky", "Anders Blilie", "Nita Mulliqi", "Toyonori Tsuzuki", "Hemamali Samaratunga", "Matteo Titus", "Xiaoyi Ji", "Sol Erika Boman", "Einar Gudlaugsson", "Svein Reidar Kjosavik", "José Asenjo", "Marcello Gambacorta", "Paolo Libretti", "Marcin Braun", "Radisław Kordek", "Roman Łowicki", "Brett Delahunt", "Kenneth A. Iczkowski", "Theo van der Kwast", "Geert J. L. H. van Leenders", "Katia R. M. Leite", "Chin-Chen Pan", "Emiel Adrianus Maria Janssen", "Martin Eklund", "Lars Egevad", "Kimmo Kartasalo"], "title": "Finding Holes: Pathologist Level Performance Using AI for Cribriform Morphology Detection in Prostate Cancer", "comment": null, "summary": "Background: Cribriform morphology in prostate cancer is a histological\nfeature that indicates poor prognosis and contraindicates active surveillance.\nHowever, it remains underreported and subject to significant interobserver\nvariability amongst pathologists. We aimed to develop and validate an AI-based\nsystem to improve cribriform pattern detection.\n  Methods: We created a deep learning model using an EfficientNetV2-S encoder\nwith multiple instance learning for end-to-end whole-slide classification. The\nmodel was trained on 640 digitised prostate core needle biopsies from 430\npatients, collected across three cohorts. It was validated internally (261\nslides from 171 patients) and externally (266 slides, 104 patients from three\nindependent cohorts). Internal validation cohorts included laboratories or\nscanners from the development set, while external cohorts used completely\nindependent instruments and laboratories. Annotations were provided by three\nexpert uropathologists with known high concordance. Additionally, we conducted\nan inter-rater analysis and compared the model's performance against nine\nexpert uropathologists on 88 slides from the internal validation cohort.\n  Results: The model showed strong internal validation performance (AUC: 0.97,\n95% CI: 0.95-0.99; Cohen's kappa: 0.81, 95% CI: 0.72-0.89) and robust external\nvalidation (AUC: 0.90, 95% CI: 0.86-0.93; Cohen's kappa: 0.55, 95% CI:\n0.45-0.64). In our inter-rater analysis, the model achieved the highest average\nagreement (Cohen's kappa: 0.66, 95% CI: 0.57-0.74), outperforming all nine\npathologists whose Cohen's kappas ranged from 0.35 to 0.62.\n  Conclusion: Our AI model demonstrates pathologist-level performance for\ncribriform morphology detection in prostate cancer. This approach could enhance\ndiagnostic reliability, standardise reporting, and improve treatment decisions\nfor prostate cancer patients.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.13827", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13827", "abs": "https://arxiv.org/abs/2510.13827", "authors": ["Ashish Kattamuri", "Ishita Prasad", "Meetu Malhotra", "Arpita Vats", "Rahul Raja", "Albert Lie"], "title": "Bridging the Semantic Gap: Contrastive Rewards for Multilingual Text-to-SQL", "comment": "20th International Workshop on Semantic and Social Media Adaptation &\n  Personalization", "summary": "Current Text-to-SQL methods are evaluated and only focused on executable\nqueries, overlooking the semantic alignment challenge -- both in terms of the\nsemantic meaning of the query and the correctness of the execution results.\nEven execution accuracy itself shows significant drops when moving from English\nto other languages, with an average decline of 6 percentage points across\nnon-English languages. We address these challenges by presenting a new\nframework that combines Group Relative Policy Optimization (GRPO) within a\nmultilingual contrastive reward signal to enhance both task efficiency and\nsemantic accuracy in Text-to-SQL systems in cross-lingual scenarios. Our method\nteaches models to obtain better correspondence between SQL generation and user\nintent by combining a reward signal based on semantic similarity. On the\nseven-language MultiSpider dataset, fine-tuning the LLaMA-3-3B model with GRPO\nimproved the execution accuracy up to 87.4 percent (+26 pp over zero-shot) and\nsemantic accuracy up to 52.29 percent (+32.86 pp). Adding our contrastive\nreward signal in the GRPO framework further improved the average semantic\naccuracy to 59.14 percent (+6.85 pp, up to +10 pp for Vietnamese). Our\nexperiments showcase that a smaller, parameter-efficient 3B LLaMA model\nfine-tuned with our contrastive reward signal outperforms a much larger\nzero-shot 8B LLaMA model, with an uplift of 7.43 pp in execution accuracy (from\n81.43 percent on the 8B model to 88.86 percent on the 3B model), and nearly\nmatches its semantic accuracy (59.14 percent vs. 68.57 percent) -- all using\njust 3,000 reinforcement learning training examples. These results demonstrate\nhow we can improve the performance of Text-to-SQL systems with contrastive\nrewards for directed semantic alignment, without requiring large-scale training\ndatasets.", "AI": {"tldr": "本文提出了一种结合Group Relative Policy Optimization (GRPO)以及多语言对比奖励信号的新框架，用于改善Text-to-SQL系统的跨语言语义准确性和执行效率。实验展示了相比零样本情况，该新方法在七语言MultiSpider数据集上提升了显著的执行和语义准确性。", "motivation": "本文的动机是克服现行Text-to-SQL方法对于数据集中的语义对齐问题的忽略，以及当从英语扩展到其他语言时，存在显著的执行准确率下降问题。本框架旨在解决语义准确性和跨语言效率问题。", "method": "本文提出的方法是结合Group Relative Policy Optimization (GRPO) 以及基于语义相似度的多语言对比奖励信号技术，以提高Text-to-SQL系统的性能。这种方法通过多语言对比奖励信号教导模型更好地获取SQL生成与用户意图之间的对应关系。", "result": "通过实验，在七语言MultiSpider数据集上，与零样本基线相比，LLaMA-3-3B模型经过本框架微调后的执行准确率提高到87.4%，语义准确率提高到59.14%。在较小的3B参数模型中添加对比奖励信号后，显著提升了语义准确率，并且在执行准确率上超过了大规模的8B零样本模型。", "conclusion": "结论表明，在不需要大规模数据集的情况下，通过对比奖励来引导语义对齐，能够显著提高Text-to-SQL系统的语义准确性和执行效率。同时，表明在参数高效的模型上通过有限的强化学习训练样本即可获得显著的性能提升。"}}
{"id": "2510.14025", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14025", "abs": "https://arxiv.org/abs/2510.14025", "authors": ["Junjie Nan", "Jianing Li", "Wei Chen", "Mingkun Zhang", "Xueqi Cheng"], "title": "NAPPure: Adversarial Purification for Robust Image Classification under Non-Additive Perturbations", "comment": null, "summary": "Adversarial purification has achieved great success in combating adversarial\nimage perturbations, which are usually assumed to be additive. However,\nnon-additive adversarial perturbations such as blur, occlusion, and distortion\nare also common in the real world. Under such perturbations, existing\nadversarial purification methods are much less effective since they are\ndesigned to fit the additive nature. In this paper, we propose an extended\nadversarial purification framework named NAPPure, which can further handle\nnon-additive perturbations. Specifically, we first establish the generation\nprocess of an adversarial image, and then disentangle the underlying clean\nimage and perturbation parameters through likelihood maximization. Experiments\non GTSRB and CIFAR-10 datasets show that NAPPure significantly boosts the\nrobustness of image classification models against non-additive perturbations.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.13828", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13828", "abs": "https://arxiv.org/abs/2510.13828", "authors": ["Ratna Kandala", "Akshata Kishore Moharir", "Divya Arvinda Nayak"], "title": "From Explainability to Action: A Generative Operational Framework for Integrating XAI in Clinical Mental Health Screening", "comment": null, "summary": "Explainable Artificial Intelligence (XAI) has been presented as the critical\ncomponent for unlocking the potential of machine learning in mental health\nscreening (MHS). However, a persistent lab-to-clinic gap remains. Current XAI\ntechniques, such as SHAP and LIME, excel at producing technically faithful\noutputs such as feature importance scores, but fail to deliver clinically\nrelevant, actionable insights that can be used by clinicians or understood by\npatients. This disconnect between technical transparency and human utility is\nthe primary barrier to real-world adoption. This paper argues that this gap is\na translation problem and proposes the Generative Operational Framework, a\nnovel system architecture that leverages Large Language Models (LLMs) as a\ncentral translation engine. This framework is designed to ingest the raw,\ntechnical outputs from diverse XAI tools and synthesize them with clinical\nguidelines (via RAG) to automatically generate human-readable, evidence-backed\nclinical narratives. To justify our solution, we provide a systematic analysis\nof the components it integrates, tracing the evolution from intrinsic models to\ngenerative XAI. We demonstrate how this framework directly addresses key\noperational barriers, including workflow integration, bias mitigation, and\nstakeholder-specific communication. This paper also provides a strategic\nroadmap for moving the field beyond the generation of isolated data points\ntoward the delivery of integrated, actionable, and trustworthy AI in clinical\npractice.", "AI": {"tldr": "本文提出了一种新的系统架构（生成操作框架），利用大型语言模型来解决可解释人工智能技术在精神健康筛查实际应用中的差距问题。", "motivation": "目前的可解释AI技术虽然能在技术上生成如特征重要性得分等结果，但在提供对临床有实际指导意义的见解方面仍有不足。本文认为，技术透明度与人类实用性之间的差距是主要障碍。", "method": "本文提出了一个生成操作框架（Generative Operational Framework），该框架利用大型语言模型（LLMs）作为中心翻译引擎，旨在将各种可解释人工智能工具产生的技术输出转化为与临床指南相结合的、可读的临床叙述。", "result": "该框架被设计用来解决实际操作中的关键障碍，包括工作流程整合、偏差缓解、面向不同利益相关者的沟通等。", "conclusion": "通过系统地分析框架所集成的组件，本文提供了一个战略性的路线图，促进了从孤立的数据点生成向在临床实践中提供集成、可操作和可信的人工智能迈进。"}}
{"id": "2510.14032", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14032", "abs": "https://arxiv.org/abs/2510.14032", "authors": ["Xiaoqian Shen", "Wenxuan Zhang", "Jun Chen", "Mohamed Elhoseiny"], "title": "Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding", "comment": "NeurIPS 2025 (Spotlight). Webpage at\n  https://xiaoqian-shen.github.io/Vgent", "summary": "Understanding and reasoning over long videos pose significant challenges for\nlarge video language models (LVLMs) due to the difficulty in processing\nintensive video tokens beyond context window and retaining long-term sequential\ninformation. Retrieval-Augmented Generation (RAG) has demonstrated\neffectiveness in processing long context for Large Language Models (LLMs);\nhowever, applying RAG to long video faces challenges such as disrupted temporal\ndependencies and inclusion of irrelevant information that can hinder accurate\nreasoning. To address these limitations, we propose Vgent, a novel graph-based\nretrieval-reasoning-augmented generation framework to enhance LVLMs for long\nvideo understanding. Our approach introduces two key innovations: (i) It\nrepresents videos by structured graphs with semantic relationships across video\nclips preserved to improve retrieval effectiveness. (ii) It introduces an\nintermediate reasoning step to mitigate the reasoning limitation of LVLMs,\nwhich leverages structured verification to reduce retrieval noise and\nfacilitate the explicit aggregation of relevant information across clips,\nresulting in more accurate and context-aware responses. We comprehensively\nevaluate our framework with various open-source LVLMs on three long-video\nunderstanding benchmarks. Our approach yielded an overall performance\nimprovement of $3.0\\%\\sim 5.4\\%$ over base models on MLVU, and outperformed\nstate-of-the-art video RAG methods by $8.6\\%$. Our code is publicly available\nat https://xiaoqian-shen.github.io/Vgent.", "AI": {"tldr": "Vgent improves long video understanding in large video language models by using structured graph representations and an intermediate reasoning step.", "motivation": "The motivation behind this paper is to address the challenges faced by large video language models when processing and understanding long videos, including disrupted temporal dependencies and the inclusion of irrelevant information in retrieval-augmented generation methods.", "method": "Vgent, a graph-based retrieval-reasoning-augmented generation framework, is proposed to enhance large video language models for long video understanding. It represents videos as structured graphs to preserve semantic relationships and introduces an intermediate reasoning step to mitigate limitations of LVLMs.", "result": "The proposed method resulted in a performance improvement of 3.0% to 5.4% over base models on the MLVU benchmark and outperformed state-of-the-art methods by 8.6%.", "conclusion": "Vgent provides a novel way to improve long video understanding by enhancing retrieval and reasoning capabilities in LVLMs, demonstrating significant performance gains over existing models."}}
{"id": "2510.13829", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13829", "abs": "https://arxiv.org/abs/2510.13829", "authors": ["Shinwoo Park", "Hyejin Park", "Hyeseon Ahn", "Yo-Sub Han"], "title": "A Linguistics-Aware LLM Watermarking via Syntactic Predictability", "comment": null, "summary": "As large language models (LLMs) continue to advance rapidly, reliable\ngovernance tools have become critical. Publicly verifiable watermarking is\nparticularly essential for fostering a trustworthy AI ecosystem. A central\nchallenge persists: balancing text quality against detection robustness. Recent\nstudies have sought to navigate this trade-off by leveraging signals from model\noutput distributions (e.g., token-level entropy); however, their reliance on\nthese model-specific signals presents a significant barrier to public\nverification, as the detection process requires access to the logits of the\nunderlying model. We introduce STELA, a novel framework that aligns watermark\nstrength with the linguistic degrees of freedom inherent in language. STELA\ndynamically modulates the signal using part-of-speech (POS) n-gram-modeled\nlinguistic indeterminacy, weakening it in grammatically constrained contexts to\npreserve quality and strengthen it in contexts with greater linguistic\nflexibility to enhance detectability. Our detector operates without access to\nany model logits, thus facilitating publicly verifiable detection. Through\nextensive experiments on typologically diverse languages-analytic English,\nisolating Chinese, and agglutinative Korean-we show that STELA surpasses prior\nmethods in detection robustness. Our code is available at\nhttps://github.com/Shinwoo-Park/stela_watermark.", "AI": {"tldr": "我们提出了一个名为STELA的新框架，该框架能够在保持文本质量的同时增强水印的可检测性，且无需访问模型内部状态即可实现公开检测。", "motivation": "随着大型语言模型的迅速发展，可靠的治理工具变得至关重要，其中公开可验证的水印尤其重要。现有方法面临着文本质量和检测鲁棒性之间的权衡问题，而我们提出的STELA框架旨在解决这一问题。", "method": "STELA是一个新的框架，它将水印强度与语言的语义自由度对齐。STELA使用词性n元模型的语义不确定性动态调制信号，在语法受限的上下文中减弱信号以保持质量，在语义灵活性更强的上下文中增强信号以提高检测性。检测器无需访问模型的logits，从而实现公开可验证的检测。", "result": "我们在英语、汉语和韩语这三种语系不同的语言上进行了广泛的实验，结果显示，STELA在检测鲁棒性方面优于先前的方法。", "conclusion": "我们的研究证明了STELA框架在实现高质量文本和强鲁棒性检测水印之间的平衡方面的有效性，且无需访问模型内部状态即可实现公开检测。代码可在指定的GitHub仓库中获取。"}}
{"id": "2510.14051", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14051", "abs": "https://arxiv.org/abs/2510.14051", "authors": ["Avihai Naaman", "Ron Shapira Weber", "Oren Freifeld"], "title": "Synchronization of Multiple Videos", "comment": "ICCV 2025", "summary": "Synchronizing videos captured simultaneously from multiple cameras in the\nsame scene is often easy and typically requires only simple time shifts.\nHowever, synchronizing videos from different scenes or, more recently,\ngenerative AI videos, poses a far more complex challenge due to diverse\nsubjects, backgrounds, and nonlinear temporal misalignment. We propose Temporal\nPrototype Learning (TPL), a prototype-based framework that constructs a shared,\ncompact 1D representation from high-dimensional embeddings extracted by any of\nvarious pretrained models. TPL robustly aligns videos by learning a unified\nprototype sequence that anchors key action phases, thereby avoiding exhaustive\npairwise matching. Our experiments show that TPL improves synchronization\naccuracy, efficiency, and robustness across diverse datasets, including\nfine-grained frame retrieval and phase classification tasks. Importantly, TPL\nis the first approach to mitigate synchronization issues in multiple generative\nAI videos depicting the same action. Our code and a new multiple video\nsynchronization dataset are available at https://bgu-cs-vil.github.io/TPL/", "AI": {"tldr": "本文提出了一种基于原型的学习框架（TPL），用以解决不同场景视频或生成式AI视频的时间同步问题，该框架构建了一个共享的紧凑型一维表示，并通过学习统一的原型序列来锚定关键动作阶段，提高了同步的准确性、效率和鲁棒性。", "motivation": "解决不同场景视频或生成式AI视频同步中的复杂挑战，包括多样化的主体、背景以及非线性时间错位问题。", "method": "提出了基于原型的框架（TPL），其可以从高维嵌入中构建共享的一维紧凑型表示，通过学习统一的原型序列来锚定关键动作阶段，从而避免了繁冗的成对匹配。", "result": "实验显示，TPL框架改善了不同数据集上的同步准确性、效率和鲁棒性，包括细粒度帧检索和阶段分类任务。", "conclusion": "TPL是首次解决多重生成式AI视频同步问题的方法，并提供了相关的代码与新的多重视频同步数据集。"}}
{"id": "2510.13830", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13830", "abs": "https://arxiv.org/abs/2510.13830", "authors": ["Zhongze Cai", "Xiaocheng Li"], "title": "Users as Annotators: LLM Preference Learning from Comparison Mode", "comment": null, "summary": "Pairwise preference data have played an important role in the alignment of\nlarge language models (LLMs). Each sample of such data consists of a prompt,\ntwo different responses to the prompt, and a binary label indicating which of\nthe two responses is better. The labels are usually annotated by professional\nhuman annotators. In this paper, we consider an alternative approach to collect\npairwise preference data -- user annotation from comparison mode. With the\nincreasingly wider adoption of LLMs among the population, users are\ncontributing more and more of their preference labels through their daily\ninteractions with the LLMs. The upside of such labels is that users are the\nbest experts in judging the responses to their own queries/prompts, but the\ndownside is the lack of quality control in these labels. In this paper, we\nconsider a new idea of generating two responses from two different models or\ntwo different versions of the same model. The asymmetry allows us to make an\ninference of the user's data quality through our proposed user behavior model.\nWe develop an expectation-maximization algorithm to estimate a latent quality\nfactor of the user, and filter users' annotation data accordingly. The\ndownstream task shows the effectiveness of our approach in both capturing the\nuser behavior and data filtering for LLM alignment.", "AI": {"tldr": "本文提出了一种新的成对偏好数据收集方法——用户注释，解决了传统方法中缺乏质量控制的问题，并证明了该方法的有效性。", "motivation": "传统的成对偏好数据由专业人类注释员标注，但随着LLM的普及，用户在日常互动中贡献了更多的偏好标签。这种方式的优点是用户可以最好地判断自己查询/提示的响应，缺点是没有质量控制。", "method": "本研究提出了一种新的方法，通过不对称生成两个不同模型或同一模型的两个版本的响应，来推断用户数据的质量。研究开发了一种期望最大化算法来估计用户的潜在质量因素，并据此筛选用户的注释数据。", "result": "通过开发新的模型和算法，成功提升了用户行为数据的筛选效果，证明了该方法的有效性。", "conclusion": "实验结果显示，该方法在捕捉用户行为和筛选LLM对齐的数据方面均有效。"}}
{"id": "2510.14081", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.14081", "abs": "https://arxiv.org/abs/2510.14081", "authors": ["Emanuel Garbin", "Guy Adam", "Oded Krams", "Zohar Barzelay", "Eran Guendelman", "Michael Schwarz", "Moran Vatelmacher", "Yigal Shenkman", "Eli Peker", "Itai Druker", "Uri Patish", "Yoav Blum", "Max Bluvstein", "Junxuan Li", "Rawal Khirodkar", "Shunsuke Saito"], "title": "Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images", "comment": null, "summary": "We present a novel, zero-shot pipeline for creating hyperrealistic,\nidentity-preserving 3D avatars from a few unstructured phone images. Existing\nmethods face several challenges: single-view approaches suffer from geometric\ninconsistencies and hallucinations, degrading identity preservation, while\nmodels trained on synthetic data fail to capture high-frequency details like\nskin wrinkles and fine hair, limiting realism. Our method introduces two key\ncontributions: (1) a generative canonicalization module that processes multiple\nunstructured views into a standardized, consistent representation, and (2) a\ntransformer-based model trained on a new, large-scale dataset of high-fidelity\nGaussian splatting avatars derived from dome captures of real people. This\n\"Capture, Canonicalize, Splat\" pipeline produces static quarter-body avatars\nwith compelling realism and robust identity preservation from unstructured\nphotos.", "AI": {"tldr": "This paper introduces a new pipeline for creating hyperrealistic, identity-preserving 3D avatars from multiple unstructured phone images, using a novel approach that includes a generative canonicalization module and a transformer-based model.", "motivation": "The motivation behind this paper is to address the limitations of existing methods that either fail to preserve identity or capture high-frequency details from unstructured phone images, thus providing a zero-shot solution for generating hyper-realistic 3D avatars.", "method": "Our method consists of two key components: a generative canonicalization module that transforms multiple unstructured views into a consistent standardized representation, and a transformer-based model trained on a large-scale dataset of high-fidelity Gaussian splatting avatars, leading to realistic 3D avatars with preserved identity.", "result": "The result is a static quarter-body avatar with compelling realism and robust identity preservation from unstructured photos, overcoming issues such as geometric inconsistencies and failure to capture fine details.", "conclusion": "The paper concludes that their proposed 'Capture, Canonicalize, Splat' pipeline effectively creates hyper-realistic 3D avatars while preserving identity, offering a novel approach to 3D avatar generation."}}
{"id": "2510.13831", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13831", "abs": "https://arxiv.org/abs/2510.13831", "authors": ["Chao Han", "Yijuan Liang", "Zihao Xuan", "Daokuan Wu", "Wei Zhang", "Xiaoyu Shen"], "title": "Informed Routing in LLMs: Smarter Token-Level Computation for Faster Inference", "comment": null, "summary": "The deployment of large language models (LLMs) in real-world applications is\nincreasingly limited by their high inference cost. While recent advances in\ndynamic token-level computation allocation attempt to improve efficiency by\nselectively activating model components per token, existing methods rely on\ngreedy routing--a myopic execute-or-skip mechanism that often leads to\nirreversible information loss and suboptimal token selection. This paper\nintroduces informed routing, a new paradigm that proactively addresses these\nissues. The key insight is to assess not only a token's immediate importance\nbut also its recoverability, i.e., how well its transformation can be\napproximated. To this end, we propose the Lightweight Feature Forecaster (LFF),\na small predictive module that estimates a unit's output before routing\ndecisions are made. This enables a flexible execute-or-approximate policy that\npreserves model fidelity while drastically reducing computation. Extensive\nexperiments on both language modeling and reasoning tasks show that informed\nrouting achieves state-of-the-art efficiency-performance trade-offs across\nmultiple sparsity levels. Notably, even without final LoRA fine-tuning, our\nmethod matches or surpasses strong baselines that require full fine-tuning, all\nwhile reducing training time by over 50%. The code is available at:\nhttps://github.com/EIT-NLP/informed-routing", "AI": {"tldr": "本文提出了“有见地的路由”概念和Lightweight Feature Forecaster (LFF)模块，通过更灵活的执行或近似策略在减少计算的同时保持模型的准确性。实验表明，这种方法在效率-性能权衡方面达到最新技术水平，并减少了训练时间。", "motivation": "现有的动态令牌级计算分配方法依赖于贪心路由机制，这可能导致不可逆的信息丢失和次优令牌选择。为了改进这些问题，本研究提出了一种新的路由策略。", "method": "本研究提出了名为 Lightweight Feature Forecaster (LFF) 的小型预测模块，评估每个单元在路由前的输出能力，从而实现执行或近似策略，显著降低计算成本同时保持模型的准确性。", "result": "实验表明，有见地的路由在语言建模和推理任务上实现了最先进的效率-性能权衡，并且在没有最终LoRA微调的情况下仍能匹配甚至超越需要完全微调的强大基线模型，同时训练时间减少超过50%。", "conclusion": "本研究提出的新路由方法和轻量级特征预测器在保持准确性的同时，显著减少了计算资源和训练时间，提供了更高的效率。"}}
{"id": "2510.14143", "categories": ["cs.CV", "q-bio.QM", "92C55, 68U10", "I.4.0; J.3"], "pdf": "https://arxiv.org/pdf/2510.14143", "abs": "https://arxiv.org/abs/2510.14143", "authors": ["Alexandr A. Kalinin", "Anne E. Carpenter", "Shantanu Singh", "Matthew J. O'Meara"], "title": "cubic: CUDA-accelerated 3D Bioimage Computing", "comment": "accepted to BioImage Computing workshop @ ICCV 2025", "summary": "Quantitative analysis of multidimensional biological images is useful for\nunderstanding complex cellular phenotypes and accelerating advances in\nbiomedical research. As modern microscopy generates ever-larger 2D and 3D\ndatasets, existing computational approaches are increasingly limited by their\nscalability, efficiency, and integration with modern scientific computing\nworkflows. Existing bioimage analysis tools often lack application programmable\ninterfaces (APIs), do not support graphics processing unit (GPU) acceleration,\nlack broad 3D image processing capabilities, and/or have poor interoperability\nfor compute-heavy workflows. Here, we introduce cubic, an open-source Python\nlibrary that addresses these challenges by augmenting widely used SciPy and\nscikit-image APIs with GPU-accelerated alternatives from CuPy and RAPIDS cuCIM.\ncubic's API is device-agnostic and dispatches operations to GPU when data\nreside on the device and otherwise executes on CPU, seamlessly accelerating a\nbroad range of image processing routines. This approach enables GPU\nacceleration of existing bioimage analysis workflows, from preprocessing to\nsegmentation and feature extraction for 2D and 3D data. We evaluate cubic both\nby benchmarking individual operations and by reproducing existing deconvolution\nand segmentation pipelines, achieving substantial speedups while maintaining\nalgorithmic fidelity. These advances establish a robust foundation for\nscalable, reproducible bioimage analysis that integrates with the broader\nPython scientific computing ecosystem, including other GPU-accelerated methods,\nenabling both interactive exploration and automated high-throughput analysis\nworkflows. cubic is openly available at\nhttps://github$.$com/alxndrkalinin/cubic", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.13832", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13832", "abs": "https://arxiv.org/abs/2510.13832", "authors": ["Minsik Choi", "Hyegang Son", "Changhoon Kim", "Young Geun Kim"], "title": "Entropy Meets Importance: A Unified Head Importance-Entropy Score for Stable and Efficient Transformer Pruning", "comment": "32 pages", "summary": "Transformer-based models have achieved remarkable performance in NLP tasks.\nHowever, their structural characteristics-multiple layers and attention\nheads-introduce efficiency challenges in inference and deployment. To address\nthese challenges, various pruning methods have recently been proposed. Notably,\ngradient-based methods using Head Importance Scores (HIS) have gained traction\nfor interpretability, efficiency, and ability to identify redundant heads.\nHowever, HIS alone has limitations as it captures only the gradient-driven\ncontribution, overlooking the diversity of attention patterns. To overcome\nthese limitations, we introduce a novel pruning criterion, HIES (Head\nImportance-Entropy Score), which integrates head importance scores with\nattention entropy, providing complementary evidence on per-head contribution.\nEmpirically, HIES-based pruning yields up to 15.2% improvement in model quality\nand 2.04x improvement in stability over HIS-only methods, enabling substantial\nmodel compression without sacrificing either accuracy or stability. Code will\nbe released upon publication.", "AI": {"tldr": "为了解决Transformer模型在推断和部署中的效率挑战，本文提出了一种新的剪枝标准HIES，综合了头部重要性和注意力熵，提高模型的效率、稳定性和压缩率。", "motivation": "虽然基于Transformer的模型在NLP任务中表现优异，但其多层和注意力头的结构特点在推断和部署过程中带来了效率挑战。", "method": "提出了一种新的剪枝标准HIES，它结合了头部重要性评分和注意力熵，为每个头部的贡献提供互补的证据。", "result": "实验显示HIES剪枝方法在模型质量和稳定性方面比仅使用HIS的方法表现更好。", "conclusion": "引入HIES（Head Importance-Entropy Score）方法能够在不牺牲准确性和稳定性的情况下，使模型质量提高最多15.2%，稳定性提高2.04倍，实现了显著的模型压缩。"}}
{"id": "2510.14179", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14179", "abs": "https://arxiv.org/abs/2510.14179", "authors": ["Yuancheng Xu", "Wenqi Xian", "Li Ma", "Julien Philip", "Ahmet Levent Taşel", "Yiwei Zhao", "Ryan Burgert", "Mingming He", "Oliver Hermann", "Oliver Pilarski", "Rahul Garg", "Paul Debevec", "Ning Yu"], "title": "Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures", "comment": "Accepted to SIGGRAPH Asia 2025", "summary": "We introduce a framework that enables both multi-view character consistency\nand 3D camera control in video diffusion models through a novel customization\ndata pipeline. We train the character consistency component with recorded\nvolumetric capture performances re-rendered with diverse camera trajectories\nvia 4D Gaussian Splatting (4DGS), lighting variability obtained with a video\nrelighting model. We fine-tune state-of-the-art open-source video diffusion\nmodels on this data to provide strong multi-view identity preservation, precise\ncamera control, and lighting adaptability. Our framework also supports core\ncapabilities for virtual production, including multi-subject generation using\ntwo approaches: joint training and noise blending, the latter enabling\nefficient composition of independently customized models at inference time; it\nalso achieves scene and real-life video customization as well as control over\nmotion and spatial layout during customization. Extensive experiments show\nimproved video quality, higher personalization accuracy, and enhanced camera\ncontrol and lighting adaptability, advancing the integration of video\ngeneration into virtual production. Our project page is available at:\nhttps://eyeline-labs.github.io/Virtually-Being.", "AI": {"tldr": "提出一种新框架，通过定制数据流提升视频扩散模型在虚拟制作中的角色一致性、3D相机控制和光照适应性。", "motivation": "旨在提升视频生成技术在虚拟制作中的集成水平，特别是在多视角下角色一致性和3D相机控制方面。", "method": "采用4D高斯平铺技术生成的多样化光照条件和视频重新打光模型，训练角色一致性组件；对现有视频扩散模型进行微调；通过联合训练和噪声混合实现多角色生成；支持对运动和空间布局的控制。", "result": "该论文介绍了一个框架，该框架通过一种新颖的数据处理流程，使视频扩散模型能够在多视角下保持角色一致性并实现3D相机控制。具体而言，作者使用4D高斯平铺技术以及视频重新打光模型生成的多样化光照条件，训练了角色一致性组件。接着，他们对最先进的开源视频扩散模型进行了微调，以实现跨多视角的身份保持、准确的相机控制以及光照适应性。框架还支持基本的虚拟制作能力，包括通过联合训练和噪声混合两种方式生成多角色，后者能够在推理时高效地组合独立定制的模型。此外，它还能实现场景和视频的定制，以及在定制时对运动和空间布局的控制。广泛的实验表明，这一方法提高了视频质量，提升了个性化精确度，并改进了相机控制和光照适应性。", "conclusion": "该框架通过提升视频质量、个性化精确度以及相机和光照控制能力，推进了视频生成技术在虚拟制作中的应用。"}}
{"id": "2510.13835", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13835", "abs": "https://arxiv.org/abs/2510.13835", "authors": ["Avik Dutta", "Priyanshu Gupta", "Hosein Hasanbeig", "Rahul Pratap Singh", "Harshit Nigam", "Sumit Gulwani", "Arjun Radhakrishna", "Gustavo Soares", "Ashish Tiwari"], "title": "ConDABench: Interactive Evaluation of Language Models for Data Analysis", "comment": null, "summary": "Real-world data analysis tasks often come with under-specified goals and\nunclean data. User interaction is necessary to understand and disambiguate a\nuser's intent, and hence, essential to solving these complex tasks. Existing\nbenchmarks for evaluating LLMs on data analysis tasks do not capture these\ncomplexities or provide first-class support for interactivity. We introduce\nConDABench, a framework for generating conversational data analysis (ConDA)\nbenchmarks and evaluating external tools on the generated benchmarks. \\bench\nconsists of (a) a multi-agent workflow for generating realistic benchmarks from\narticles describing insights gained from public datasets, (b) 1,420 ConDA\nproblems generated using this workflow, and (c) an evaluation harness that, for\nthe first time, makes it possible to systematically evaluate conversational\ndata analysis tools on the generated ConDA problems. Evaluation of\nstate-of-the-art LLMs on the benchmarks reveals that while the new generation\nof models are better at solving more instances, they are not necessarily better\nat solving tasks that require sustained, long-form engagement. ConDABench is an\navenue for model builders to measure progress towards truly collaborative\nmodels that can complete complex interactive tasks.", "AI": {"tldr": "ConDABench旨在弥补现有基准的不足，提供了一个生成并评估对话数据分析问题的框架。", "motivation": "现有基准无法捕捉数据分忻任务中的复杂性和提供互动支持，因此提出了ConDABench，以生成和评估对话数据分析基准。", "method": "通过从描述公开数据集所得见解的文章中生成多代理工作流程，ConDABench框架能够产生1,420个具有现实性的对话数据分析题目，并首次提供了系统评估对话数据分析工具的能力。", "result": "评估结果显示虽然新一代模型能够更好地解决更多的实例，但它们不一定能够更好地处理需要持续长时间参与的任务。", "conclusion": "ConDABench为模型构建者提供了一个衡量进展的途径，朝着能够完成复杂交互任务的真正协作模型的方向努力。"}}
{"id": "2510.14203", "categories": ["cs.CV", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.14203", "abs": "https://arxiv.org/abs/2510.14203", "authors": ["Ryo Masumura", "Shota Orihashi", "Mana Ihori", "Tomohiro Tanaka", "Naoki Makishima", "Taiga Yamane", "Naotaka Kawata", "Satoshi Suzuki", "Taichi Katayama"], "title": "Joint Modeling of Big Five and HEXACO for Multimodal Apparent Personality-trait Recognition", "comment": "Accepted at APSIPA ASC 2025", "summary": "This paper proposes a joint modeling method of the Big Five, which has long\nbeen studied, and HEXACO, which has recently attracted attention in psychology,\nfor automatically recognizing apparent personality traits from multimodal human\nbehavior. Most previous studies have used the Big Five for multimodal apparent\npersonality-trait recognition. However, no study has focused on apparent HEXACO\nwhich can evaluate an Honesty-Humility trait related to displaced aggression\nand vengefulness, social-dominance orientation, etc. In addition, the\nrelationships between the Big Five and HEXACO when modeled by machine learning\nhave not been clarified. We expect awareness of multimodal human behavior to\nimprove by considering these relationships. The key advance of our proposed\nmethod is to optimize jointly recognizing the Big Five and HEXACO. Experiments\nusing a self-introduction video dataset demonstrate that the proposed method\ncan effectively recognize the Big Five and HEXACO.", "AI": {"tldr": "The paper introduces a joint modeling method for recognizing Big Five and HEXACO personality traits from multimodal human behavior.", "motivation": "To improve the recognition of multimodal human behavior by incorporating HEXACO traits, which can evaluate honesty-humility traits related to aggression and vengefulness, and by exploring the relationships between Big Five and HEXACO using machine learning.", "method": "Structure", "result": "Experiments using a self-introduction video dataset show that the proposed method can effectively recognize the Big Five and HEXACO.", "conclusion": "Jointly optimizing the recognition of Big Five and HEXACO traits can enhance the understanding of multimodal human behavior."}}
{"id": "2510.13836", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13836", "abs": "https://arxiv.org/abs/2510.13836", "authors": ["Debarun Bhattacharjya", "Balaji Ganesan", "Junkyu Lee", "Radu Marinescu", "Katsiaryna Mirylenka", "Michael Glass", "Xiao Shou"], "title": "SIMBA UQ: Similarity-Based Aggregation for Uncertainty Quantification in Large Language Models", "comment": "15 pages including appendix, Findings of EMNLP 2025", "summary": "When does a large language model (LLM) know what it does not know?\nUncertainty quantification (UQ) provides measures of uncertainty, such as an\nestimate of the confidence in an LLM's generated output, and is therefore\nincreasingly recognized as a crucial component of trusted AI systems. Black-box\nUQ methods do not require access to internal model information from the\ngenerating LLM and therefore have numerous real-world advantages, such as\nrobustness to system changes, adaptability to choice of LLM, reduced costs, and\ncomputational tractability. In this paper, we investigate the effectiveness of\nUQ techniques that are primarily but not necessarily entirely black-box, where\nthe consistency between a generated output and other sampled generations is\nused as a proxy for confidence in its correctness. We propose a high-level\nnon-verbalized similarity-based aggregation framework that subsumes a broad\nswath of UQ approaches suitable for complex generative tasks, as well as\nintroduce specific novel techniques from the framework that train confidence\nestimation models using small training sets. Through an empirical study with\ndatasets spanning the diverse tasks of question answering, summarization, and\ntext-to-SQL, we demonstrate that our proposed similarity-based methods can\nyield better calibrated confidences than baselines.", "AI": {"tldr": "本文探讨了黑盒UQ方法的有效性，并提出了一种基于非言语相似性的置信度估计框架，该框架在多种复杂生成任务中表现出优秀的性能。", "motivation": "本文的动机在于探索黑盒UQ方法的有效性，这些方法不需要访问生成LLM的内部信息，且在面对系统变更、适用不同LLM时具有鲁棒性和适应性等众多实际优势。", "method": "本文提出了一种基于非言语相似性的聚合框架，用于复杂生成任务中的不确定性量化技术。该框架适用于广泛的UQ方法，并引入了可以在小训练集上训练置信度估计模型的具体新方法。", "result": "实证研究表明，基于相似性的方法在不确定性量化置信度校准方面优于基准方法。", "conclusion": "本文结论是，基于非言语相似性的聚合框架和具体方法可以生成比基准方法更好的校准置信度，适用于多样性任务，包括问答、摘要生成和文本到SQL等问题。"}}
{"id": "2510.14230", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14230", "abs": "https://arxiv.org/abs/2510.14230", "authors": ["Hongsong Wang", "Renxi Cheng", "Yang Zhang", "Chaolei Han", "Jie Gui"], "title": "LOTA: Bit-Planes Guided AI-Generated Image Detection", "comment": "Published in the ICCV2025, COde is\n  https://github.com/hongsong-wang/LOTA", "summary": "The rapid advancement of GAN and Diffusion models makes it more difficult to\ndistinguish AI-generated images from real ones. Recent studies often use\nimage-based reconstruction errors as an important feature for determining\nwhether an image is AI-generated. However, these approaches typically incur\nhigh computational costs and also fail to capture intrinsic noisy features\npresent in the raw images. To solve these problems, we innovatively refine\nerror extraction by using bit-plane-based image processing, as lower bit planes\nindeed represent noise patterns in images. We introduce an effective bit-planes\nguided noisy image generation and exploit various image normalization\nstrategies, including scaling and thresholding. Then, to amplify the noise\nsignal for easier AI-generated image detection, we design a maximum gradient\npatch selection that applies multi-directional gradients to compute the noise\nscore and selects the region with the highest score. Finally, we propose a\nlightweight and effective classification head and explore two different\nstructures: noise-based classifier and noise-guided classifier. Extensive\nexperiments on the GenImage benchmark demonstrate the outstanding performance\nof our method, which achieves an average accuracy of \\textbf{98.9\\%}\n(\\textbf{11.9}\\%~$\\uparrow$) and shows excellent cross-generator generalization\ncapability. Particularly, our method achieves an accuracy of over 98.2\\% from\nGAN to Diffusion and over 99.2\\% from Diffusion to GAN. Moreover, it performs\nerror extraction at the millisecond level, nearly a hundred times faster than\nexisting methods. The code is at https://github.com/hongsong-wang/LOTA.", "AI": {"tldr": "本文提出了一种基于位平面处理改进的AI生成图像检测方法，该方法通过放大噪声信号使检测更加容易，并实现了高准确性和高效率。", "motivation": "传统的基于图像重建误差的方法计算成本高昂且无法有效捕捉原始图像中的噪声特征，因此我们提出了一种新的方法以解决这些问题。", "method": "我们提出了一种基于位平面的图像处理方法来改进误差提取，这种方法可以有效地捕捉图像中的噪声特征。同时，我们设计了一个最大梯度块选择方案，利用多方向梯度计算噪声分数，并选择分数最高的区域。最后，我们提出了一种轻量级且有效的分类头，并探索了噪音基础分类器和噪音引导分类器两种不同结构。", "result": "在GenImage基准测试上，我们的方法表现出色，达到了98.9%的平均准确率，并展示了优秀的跨生成器泛化能力。特别是从GAN到Diffusion和从Diffusion到GAN的准确率分别超过98.2%和99.2%。此外，我们的误差提取速度达到了毫秒级别，比现有方法快近一百倍。", "conclusion": "我们的方法不仅准确率高，而且计算速度快，实现了从GAN到Diffusion以及从Diffusion到GAN的高准确率检测，展示了优秀的跨生成器泛化能力。"}}
{"id": "2510.13837", "categories": ["cs.CL", "cs.AI", "cs.SI", "68T50, 68T45", "I.2.7; I.2.6; K.4.1"], "pdf": "https://arxiv.org/pdf/2510.13837", "abs": "https://arxiv.org/abs/2510.13837", "authors": ["Weibin Cai", "Reza Zafarani"], "title": "Seeing Hate Differently: Hate Subspace Modeling for Culture-Aware Hate Speech Detection", "comment": null, "summary": "Hate speech detection has been extensively studied, yet existing methods\noften overlook a real-world complexity: training labels are biased, and\ninterpretations of what is considered hate vary across individuals with\ndifferent cultural backgrounds. We first analyze these challenges, including\ndata sparsity, cultural entanglement, and ambiguous labeling. To address them,\nwe propose a culture-aware framework that constructs individuals' hate\nsubspaces. To alleviate data sparsity, we model combinations of cultural\nattributes. For cultural entanglement and ambiguous labels, we use label\npropagation to capture distinctive features of each combination. Finally,\nindividual hate subspaces, which in turn can further enhance classification\nperformance. Experiments show our method outperforms state-of-the-art by 1.05\\%\non average across all metrics.", "AI": {"tldr": "现有仇恨言论检测方法常忽视训练标签偏差及文化解读差异的问题。本文提出了一个文化意识框架解决了这些问题，并表明该方法在实验中优于最新方法。", "motivation": "现有的仇恨言论检测方法通常忽略了训练标签的偏差以及不同文化背景下对仇恨言论的不同解读等现实世界的复杂性。", "method": "我们提出了一个文化意识的框架来解决数据稀疏性、文化纠缠和模糊标签的问题，通过建模文化属性的组合，并使用标签传播来捕捉每种组合的独特特征。", "result": "实验结果显示，我们提出的方法在所有度量标准上平均比最先进的方法高出1.05%。", "conclusion": "本研究提出了一种文化意识框架，该框架通过构建个体的仇恨子空间改善了现有的仇恨言论检测，提升了分类性能。"}}
{"id": "2510.14241", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14241", "abs": "https://arxiv.org/abs/2510.14241", "authors": ["Soumyya Kanti Datta", "Tanvi Ranga", "Chengzhe Sun", "Siwei Lyu"], "title": "PIA: Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic Analysis", "comment": null, "summary": "The rise of manipulated media has made deepfakes a particularly insidious\nthreat, involving various generative manipulations such as lip-sync\nmodifications, face-swaps, and avatar-driven facial synthesis. Conventional\ndetection methods, which predominantly depend on manually designed\nphoneme-viseme alignment thresholds, fundamental frame-level consistency\nchecks, or a unimodal detection strategy, inadequately identify modern-day\ndeepfakes generated by advanced generative models such as GANs, diffusion\nmodels, and neural rendering techniques. These advanced techniques generate\nnearly perfect individual frames yet inadvertently create minor temporal\ndiscrepancies frequently overlooked by traditional detectors. We present a\nnovel multimodal audio-visual framework, Phoneme-Temporal and Identity-Dynamic\nAnalysis(PIA), incorporating language, dynamic face motion, and facial\nidentification cues to address these limitations. We utilize phoneme sequences,\nlip geometry data, and advanced facial identity embeddings. This integrated\nmethod significantly improves the detection of subtle deepfake alterations by\nidentifying inconsistencies across multiple complementary modalities. Code is\navailable at https://github.com/skrantidatta/PIA", "AI": {"tldr": "提出了一种新的多模态音视频框架PIA，以检测深伪媒体中的细微改动。", "motivation": "传统的检测方法在识别由GANs、扩散模型和神经渲染技术生成的深伪媒体时效果不佳，因为这些高级技术会产生几乎完美的单独帧，但可能会在时间一致性上产生微小差异，这被传统检测器忽视了。", "method": "Phoneme-Temporal and Identity-Dynamic Analysis(PIA), 利用音素序列、唇形数据和先进的面部身份嵌入来检测深伪媒体。", "result": "没有具体的结果数据，但该方法的目标是提高对由先进生成模型创建的深伪媒体的检测性能。", "conclusion": "该方法通过识别多个互补模式中的不一致性，显着提高了对细微深伪改动的检测。"}}
{"id": "2510.13839", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13839", "abs": "https://arxiv.org/abs/2510.13839", "authors": ["Dekai Zhang", "Simone Conia", "Antonio Rago"], "title": "Meronymic Ontology Extraction via Large Language Models", "comment": null, "summary": "Ontologies have become essential in today's digital age as a way of\norganising the vast amount of readily available unstructured text. In providing\nformal structure to this information, ontologies have immense value and\napplication across various domains, e.g., e-commerce, where countless product\nlistings necessitate proper product organisation. However, the manual\nconstruction of these ontologies is a time-consuming, expensive and laborious\nprocess. In this paper, we harness the recent advancements in large language\nmodels (LLMs) to develop a fully-automated method of extracting product\nontologies, in the form of meronymies, from raw review texts. We demonstrate\nthat the ontologies produced by our method surpass an existing, BERT-based\nbaseline when evaluating using an LLM-as-a-judge. Our investigation provides\nthe groundwork for LLMs to be used more generally in (product or otherwise)\nontology extraction.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.14245", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14245", "abs": "https://arxiv.org/abs/2510.14245", "authors": ["Miu Sumino", "Mayu Ishii", "Shun Kaizu", "Daisuke Hisano", "Yu Nakayama"], "title": "Event Interval Modulation: A Novel Scheme for Event-based Optical Camera Communication", "comment": null, "summary": "Optical camera communication (OCC) represents a promising visible light\ncommunication technology. Nonetheless, typical OCC systems utilizing\nframe-based cameras are encumbered by limitations, including low bit rate and\nhigh processing load. To address these issues, OCC system utilizing an\nevent-based vision sensor (EVS) as receivers have been proposed. The EVS\nenables high-speed, low-latency, and robust communication due to its\nasynchronous operation and high dynamic range. In existing event-based OCC\nsystems, conventional modulation schemes such as on-off keying (OOK) and pulse\nposition modulation have been applied, however, to the best of our knowledge,\nno modulation method has been proposed that fully exploits the unique\ncharacteristics of the EVS. This paper proposes a novel modulation scheme,\ncalled the event interval modulation (EIM) scheme, specifically designed for\nevent-based OCC. EIM enables improvement in transmission speed by modulating\ninformation using the intervals between events. This paper proposes a\ntheoretical model of EIM and conducts a proof-of-concept experiment. First, the\nparameters of the EVS are tuned and customized to optimize the frequency\nresponse specifically for EIM. Then, the maximum modulation order usable in EIM\nis determined experimentally. We conduct transmission experiments based on the\nobtained parameters. Finally, we report successful transmission at 28 kbps over\n10 meters and 8.4 kbps over 50 meters in an indoor environment. This sets a new\nbenchmark for bit rate in event-based OCC systems.", "AI": {"tldr": "本研究提出了一种针对基于事件的光学相机通信(OCC)系统的新的调制方案，即事件间隔调制(EIM)方案，该方案通过利用事件之间的间隔来传递信息。实验结果显示，在室内环境下，该系统能够分别在10米和50米的距离上传输28kbps和8.4kbps，从而设立了基于事件OCC系统的比特率新基准。", "motivation": "研究旨在解决传统基于帧式相机的OCC系统中存在的比特率低和处理负载高的问题，以及尚未充分利用基于事件的视觉传感器(EVS)独特特性的现状。", "method": "研究中提出了一种新型的调制方案——事件间隔调制(EIM)，并对其进行了理论建模。此外，还对EVS的参数进行了调优和定制，以优化针对EIM的频率响应，并通过实验确定了在EIM中可使用的最大调制阶数。", "result": "基于所获得的参数进行了传输实验，结果显示，在室内环境中，系统实现了10米距离上28kbps以及50米距离上8.4kbps的数据传输速率。", "conclusion": "研究表明，事件间隔调制(EIM)方案能够有效地提升基于事件OCC系统的传输速度，且新型调制方案在室内传输应用中立下了新的比特率基准。"}}
{"id": "2510.13842", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.13842", "abs": "https://arxiv.org/abs/2510.13842", "authors": ["Yutao Wu", "Xiao Liu", "Yinghui Li", "Yifeng Gao", "Yifan Ding", "Jiale Ding", "Xiang Zheng", "Xingjun Ma"], "title": "ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking", "comment": null, "summary": "Knowledge poisoning poses a critical threat to Retrieval-Augmented Generation\n(RAG) systems by injecting adversarial content into knowledge bases, tricking\nLarge Language Models (LLMs) into producing attacker-controlled outputs\ngrounded in manipulated context. Prior work highlights LLMs' susceptibility to\nmisleading or malicious retrieved content. However, real-world fact-checking\nscenarios are more challenging, as credible evidence typically dominates the\nretrieval pool. To investigate this problem, we extend knowledge poisoning to\nthe fact-checking setting, where retrieved context includes authentic\nsupporting or refuting evidence. We propose \\textbf{ADMIT}\n(\\textbf{AD}versarial \\textbf{M}ulti-\\textbf{I}njection \\textbf{T}echnique), a\nfew-shot, semantically aligned poisoning attack that flips fact-checking\ndecisions and induces deceptive justifications, all without access to the\ntarget LLMs, retrievers, or token-level control. Extensive experiments show\nthat ADMIT transfers effectively across 4 retrievers, 11 LLMs, and 4\ncross-domain benchmarks, achieving an average attack success rate (ASR) of 86\\%\nat an extremely low poisoning rate of $0.93 \\times 10^{-6}$, and remaining\nrobust even in the presence of strong counter-evidence. Compared with prior\nstate-of-the-art attacks, ADMIT improves ASR by 11.2\\% across all settings,\nexposing significant vulnerabilities in real-world RAG-based fact-checking\nsystems.", "AI": {"tldr": "该研究提出了一种名为ADMIT的对抗性多注入技术，用于知识中毒攻击，针对事实核查场景，成功诱导大语言模型生成受控输出，并在多种检索器和语言模型中达成高攻击成功率，揭示了基于检索增强生成系统的安全漏洞。", "motivation": "鉴于现有研究较少关注包含多种证据的真实世界事实核查场景中的知识中毒问题，本研究希望通过提出一种有效的事实核查中毒攻击方法，揭示此类系统存在的显著弱点。", "method": "ADMIT技术在无须访问目标语言模型和检索系统的前提下，通过少量语义对齐的注入点攻击来切换事实核查的决策，导致错误的理由生成。", "result": "实验显示，ADMIT跨4种检索器、11种语言模型和4个跨领域的基准测试实现了平均86%的攻击成功率，且即使在有强反驳证据的情况下仍表现出较强的鲁棒性。", "conclusion": "ADMIT攻击方法表现出相较于现有最佳攻击方法11.2%的攻击成功率提升，证明在真实世界的基于检索增强生成系统的事实核查中存在显著的安全弱点。"}}
{"id": "2510.14251", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14251", "abs": "https://arxiv.org/abs/2510.14251", "authors": ["Mingkai Liu", "Dikai Fan", "Haohua Que", "Haojia Gao", "Xiao Liu", "Shuxue Peng", "Meixia Lin", "Shengyu Gu", "Ruicong Ye", "Wanli Qiu", "Handong Yao", "Ruopeng Zhang", "Xianliang Huang"], "title": "MACE: Mixture-of-Experts Accelerated Coordinate Encoding for Large-Scale Scene Localization and Rendering", "comment": "8 pages", "summary": "Efficient localization and high-quality rendering in large-scale scenes\nremain a significant challenge due to the computational cost involved. While\nScene Coordinate Regression (SCR) methods perform well in small-scale\nlocalization, they are limited by the capacity of a single network when\nextended to large-scale scenes. To address these challenges, we propose the\nMixed Expert-based Accelerated Coordinate Encoding method (MACE), which enables\nefficient localization and high-quality rendering in large-scale scenes.\nInspired by the remarkable capabilities of MOE in large model domains, we\nintroduce a gating network to implicitly classify and select sub-networks,\nensuring that only a single sub-network is activated during each inference.\nFurtheremore, we present Auxiliary-Loss-Free Load Balancing(ALF-LB) strategy to\nenhance the localization accuracy on large-scale scene. Our framework provides\na significant reduction in costs while maintaining higher precision, offering\nan efficient solution for large-scale scene applications. Additional\nexperiments on the Cambridge test set demonstrate that our method achieves\nhigh-quality rendering results with merely 10 minutes of training.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.13843", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13843", "abs": "https://arxiv.org/abs/2510.13843", "authors": ["Zhirong Chou", "Quan Qin", "Shi Li"], "title": "Serialized EHR make for good text representations", "comment": null, "summary": "The emergence of foundation models in healthcare has opened new avenues for\nlearning generalizable representations from large scale clinical data. Yet,\nexisting approaches often struggle to reconcile the tabular and event based\nnature of Electronic Health Records (EHRs) with the sequential priors of\nnatural language models. This structural mismatch limits their ability to\ncapture longitudinal dependencies across patient encounters. We introduce\nSerialBEHRT, a domain aligned foundation model that extends SciBERT through\nadditional pretraining on structured EHR sequences. SerialBEHRT is designed to\nencode temporal and contextual relationships among clinical events, thereby\nproducing richer patient representations. We evaluate its effectiveness on the\ntask of antibiotic susceptibility prediction, a clinically meaningful problem\nin antibiotic stewardship. Through extensive benchmarking against state of the\nart EHR representation strategies, we demonstrate that SerialBEHRT achieves\nsuperior and more consistent performance, highlighting the importance of\ntemporal serialization in foundation model pretraining for healthcare.", "AI": {"tldr": "SerialBEHRT, an enhanced model from SciBERT through additional pretraining on structured EHR sequences, showcases superior performance in antibiotic susceptibility prediction tasks, proving the significance of handling the longitudinal and contextual aspects of clinical data.", "motivation": "The motivation behind SerialBEHRT is to address the limitations of existing models in handling the unique structure of EHR data, and to improve the capture of longitudinal patient dependencies and antibiotic susceptibility prediction.", "method": "The paper introduces SerialBEHRT, an extension of SciBERT that undergoes additional pretraining on structured EHR sequences to better handle the tabular and event-based nature of EHRs, thus enabling richer temporal and contextual encoding of patient data.", "result": "SerialBEHRT demonstrated superior and more consistent performance in predicting antibiotic susceptibility, surpassing state-of-the-art EHR representation techniques.", "conclusion": "Temporal serialization, as implemented in SerialBEHRT, is crucial for foundation model pretraining in healthcare, particularly for tasks such as antibiotic stewardship."}}
{"id": "2510.14255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14255", "abs": "https://arxiv.org/abs/2510.14255", "authors": ["Liao Shen", "Wentao Jiang", "Yiran Zhu", "Tiezheng Ge", "Zhiguo Cao", "Bo Zheng"], "title": "Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization", "comment": null, "summary": "Recent advances in image-to-video (I2V) generation have achieved remarkable\nprogress in synthesizing high-quality, temporally coherent videos from static\nimages. Among all the applications of I2V, human-centric video generation\nincludes a large portion. However, existing I2V models encounter difficulties\nin maintaining identity consistency between the input human image and the\ngenerated video, especially when the person in the video exhibits significant\nexpression changes and movements. This issue becomes critical when the human\nface occupies merely a small fraction of the image. Since humans are highly\nsensitive to identity variations, this poses a critical yet under-explored\nchallenge in I2V generation. In this paper, we propose Identity-Preserving\nReward-guided Optimization (IPRO), a novel video diffusion framework based on\nreinforcement learning to enhance identity preservation. Instead of introducing\nauxiliary modules or altering model architectures, our approach introduces a\ndirect and effective tuning algorithm that optimizes diffusion models using a\nface identity scorer. To improve performance and accelerate convergence, our\nmethod backpropagates the reward signal through the last steps of the sampling\nchain, enabling richer gradient feedback. We also propose a novel facial\nscoring mechanism that treats faces in ground-truth videos as facial feature\npools, providing multi-angle facial information to enhance generalization. A\nKL-divergence regularization is further incorporated to stabilize training and\nprevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V\nmodel and our in-house I2V model demonstrate the effectiveness of our method.\nOur project and code are available at\n\\href{https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/}.", "AI": {"tldr": "The paper introduces IPRO, a reinforcement learning-based method to improve identity preservation in image-to-video generation by using an optimized face identity scorer, facial feature pools, and KL-divergence regularization for stable training.", "motivation": "The motivation is to address the existing problem of identity preservation in human-centric image-to-video (I2V) generation, especially in cases where the person in the video shows significant expression changes and movements, lowering the difficulty in maintaining consistency.", "method": "In this paper, the authors propose IPRO, a novel video diffusion method based on reinforcement learning that uses a face identity scorer to directly optimize diffusion models for maintaining identity consistency. The method backpropagates reward signals through the sampling chain for better learning, introduces a new facial scoring mechanism using facial feature pools for improved generalization, and adds KL-divergence regularization to prevent overfitting.", "result": "Extensive experiments with the Wan 2.2 I2V model and an in-house I2V model show that IPRO is an effective approach for enhancing identity preservation in video generation from static images.", "conclusion": "The introduction of IPRO significantly improves the quality of identity preservation in the video generation process from static images. The model shows promise in applications where maintaining the identity of faces is crucial, such as in the generation of human-centric videos."}}
{"id": "2510.13847", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13847", "abs": "https://arxiv.org/abs/2510.13847", "authors": ["Jinbin Zhang", "Nasib Ullah", "Erik Schultheis", "Rohit Babbar"], "title": "DynaSpec: Context-aware Dynamic Speculative Sampling for Large-Vocabulary Language Models", "comment": null, "summary": "Speculative decoding (a.k.a. speculative sampling) has become a standard way\nto accelerate LLM inference: a small drafter proposes multiple tokens and a\nlarge target model verifies them once per speculation length. Recently, scaling\nof the LLM vocabulary has pushed the number of tokens to grow substantially.\nWhile verification over the full vocabulary leaves the target model largely\nunaffected, the O(|V|d) parameters in the drafter's output head become a\nlatency bottleneck, slowing the entire pipeline. Contemporary methods (e.g.,\nFR-Spec, VocabTrim) restrict the drafter's vocabulary to a fixed subset of the\ntarget model's vocabulary, ranked in descending order of token frequency.\nAlthough this reduces draft-time compute, it is brittle, since: (i) frequency\nlists are corpus-dependent and require retuning to generalize, and (ii) static\nshortlists suppress rare or domain-specific tokens, lowering the expected\nnumber of tokens per verification step. We propose DynaSpec, a\ncontext-dependent dynamic shortlisting mechanism that is robust, speeds up\ndrafting, and generalizes across diverse tasks. Concretely, we introduce\nlightweight, coarse-grained meta-classifiers that route contexts to a small\nnumber of token clusters; the union of the top-k selected clusters forms the\ndrafter's shortlist, while verification retains the full vocabulary and\nexactness. The meta-classifier finishes its computation earlier than the\ndrafter's hidden state generation by exploiting parallel execution of draft\nencoding and meta shortlisting on separate streams. On standard\nspeculative-decoding benchmarks, we observe consistent gains in mean accepted\nlength over fixed-shortlist baselines, while context-dependent selection\nenables smaller shortlists without degrading acceptance.", "AI": {"tldr": "文章提出DynaSpec方法，通过元分类器动态生成短名单词列表，以减少理性解码延迟，提高任务多样性下的总体性能。", "motivation": "解决传统固定短名单词选择方法存在的问题，即由于词频列表依赖于特定语料库，并且静态短名单词列表会抑制罕见或领域特定的词汇，影响验证步骤中每个步骤期望的词汇数量。", "method": "提出了一种名为DynaSpec的动态短名单词选择机制，该机制通过轻量级的元分类器将上下文路由到少量的词汇簇中，形成了动态的短名单词列表，从而优化了草稿模型的词汇选择。", "result": "在标准的投机解码基准测试上，DynaSpec方法显示出在平均接受长度上一致的改进，并且在没有降低接受率的情况下允许使用更小的短名单词列表。", "conclusion": "DynaSpec方法通过其轻量级元分类器和动态短名单词选择机制，提高了理性和草稿模型的整体性能，特别是在不同任务和数据集之间的泛化能力。"}}
{"id": "2510.14256", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14256", "abs": "https://arxiv.org/abs/2510.14256", "authors": ["Xiangyu Meng", "Zixian Zhang", "Zhenghao Zhang", "Junchao Liao", "Long Qin", "Weizhi Wang"], "title": "Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning", "comment": null, "summary": "While advanced methods like VACE and Phantom have advanced video generation\nfor specific subjects in diverse scenarios, they struggle with multi-human\nidentity preservation in dynamic interactions, where consistent identities\nacross multiple characters are critical. To address this, we propose\nIdentity-GRPO, a human feedback-driven optimization pipeline for refining\nmulti-human identity-preserving video generation. First, we construct a video\nreward model trained on a large-scale preference dataset containing\nhuman-annotated and synthetic distortion data, with pairwise annotations\nfocused on maintaining human consistency throughout the video. We then employ a\nGRPO variant tailored for multi-human consistency, which greatly enhances both\nVACE and Phantom. Through extensive ablation studies, we evaluate the impact of\nannotation quality and design choices on policy optimization. Experiments show\nthat Identity-GRPO achieves up to 18.9% improvement in human consistency\nmetrics over baseline methods, offering actionable insights for aligning\nreinforcement learning with personalized video generation.", "AI": {"tldr": "为了解决多人类互动场景中身份保持的问题，Identity-GRPO提出了一种基于人类反馈的优化方法，通过训练视频奖励模型和使用GRPO变体提升了视频中多个人类身份的一致性，实验表明其在人类一致性度量中比基线方法高出18.9%。", "motivation": "尽管先进的方法如VACE和Phantom在特定主题的视频生成方面取得了进展，但它们在动态多人类互动场景中难以保持一致的身份，这涉及到视频中多个角色的身份一致性问题。", "method": "我们提出了Identity-GRPO，这是一种基于人类反馈优化的多人类身份保持视频生成流水线。首先，我们构建了一个视频奖励模型，该模型在包含人类标注和合成失真数据的大规模偏好数据集上进行训练，重点在于视频中维持人类一致性。然后，我们使用一种专门为多人类一致性设计的GRPO变体，极大地提升了VACE和Phantom的效果。", "result": "通过广泛的消融研究，我们评估了标注质量和设计选择在策略优化中的影响。实验表明，与基线方法相比，Identity-GRPO在人类一致性度量中提升了多达18.9%。", "conclusion": "Identity-GRPO提供了有关如何将强化学习与个性化的视频生成相结合的可操作见解，并且它显著提升了多人类视频中的身份一致性表现。"}}
{"id": "2510.13848", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13848", "abs": "https://arxiv.org/abs/2510.13848", "authors": ["Ondrej Bohdal", "Konstantinos Theodosiadis", "Asterios Mpatziakas", "Dimitris Filippidis", "Iro Spyrou", "Christos Zonios", "Anastasios Drosou", "Dimosthenis Ioannidis", "Kyeng-Hun Lee", "Jijoong Moon", "Hyeonmok Ko", "Mete Ozay", "Umberto Michieli"], "title": "On-device System of Compositional Multi-tasking in Large Language Models", "comment": "Accepted at EMNLP 2025 (industry track)", "summary": "Large language models (LLMs) are commonly adapted for diverse downstream\ntasks via parameter-efficient fine-tuning techniques such as Low-Rank Adapters\n(LoRA). While adapters can be combined to handle multiple tasks separately,\nstandard approaches struggle when targeting the simultaneous execution of\ncomplex tasks, such as generating a translated summary from a long\nconversation. To address this challenge, we propose a novel approach tailored\nspecifically for compositional multi-tasking scenarios involving summarization\nand translation. Our technique involves adding a learnable projection layer on\ntop of the combined summarization and translation adapters. This design enables\neffective integration while maintaining efficiency through reduced\ncomputational overhead compared to alternative strategies requiring extensive\nretraining or sequential processing. We demonstrate the practical viability of\nour method within an on-device environment by developing an Android app capable\nof executing compositional tasks seamlessly. Experimental results indicate our\nsolution performs well and is fast in both cloud-based and on-device\nimplementations, highlighting the potential benefits of adopting our framework\nin real-world applications demanding high-speed operation alongside resource\nconstraints.", "AI": {"tldr": "针对包含总结和翻译的复杂组合多任务处理的难题，本文提出了一种新策略，通过在适配器之上引入可学习投影层来实现有效的集成，并通过开发一款Android应用证明了方法在设备上的实用性和高效性。", "motivation": "大型语言模型（LLMs）通常通过参数高效的微调技术（如低秩适配器（LoRA））适应多种下游任务。虽然适配器可以结合使用来单独处理多个任务，但在处理涉及从长对话中生成翻译摘要等复杂任务的同时执行时，标准方法就显得力不从心。", "method": "提出了一种专门针对包含总结和翻译的组合多任务场景的新方法。该技术包括在组合的总结和翻译适配器之上添加一个可学习的投影层。这种设计能够在保持效率的同时实现有效集成，因为相比于需要广泛重新训练或顺序处理的替代策略，我们的方法减少了计算开销。", "result": "我们在基于云的和设备端的实现中都展示了我们的方法具有良好的性能和快速处理速度，这突出了我们的框架在实际应用中的潜在好处，尤其是在要求高性能操作和资源限制的情况下。", "conclusion": "实验结果证明，在复杂多任务场景中，我们的方法不仅能够有效集成，而且还能保持很高的效率，通过减少计算开销的方式实现了快速和高效的解决方案。这强调了我们的框架在面对资源约束下高要求操作的实际应用场景中的潜在价值。"}}
{"id": "2510.14260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14260", "abs": "https://arxiv.org/abs/2510.14260", "authors": ["Tingman Yan", "Tao Liu", "Xilian Yang", "Qunfei Zhao", "Zeyang Xia"], "title": "MatchAttention: Matching the Relative Positions for High-Resolution Cross-View Matching", "comment": null, "summary": "Cross-view matching is fundamentally achieved through cross-attention\nmechanisms. However, matching of high-resolution images remains challenging due\nto the quadratic complexity and lack of explicit matching constraints in the\nexisting cross-attention. This paper proposes an attention mechanism,\nMatchAttention, that dynamically matches relative positions. The relative\nposition determines the attention sampling center of the key-value pairs given\na query. Continuous and differentiable sliding-window attention sampling is\nachieved by the proposed BilinearSoftmax. The relative positions are\niteratively updated through residual connections across layers by embedding\nthem into the feature channels. Since the relative position is exactly the\nlearning target for cross-view matching, an efficient hierarchical cross-view\ndecoder, MatchDecoder, is designed with MatchAttention as its core component.\nTo handle cross-view occlusions, gated cross-MatchAttention and a\nconsistency-constrained loss are proposed. These two components collectively\nmitigate the impact of occlusions in both forward and backward passes, allowing\nthe model to focus more on learning matching relationships. When applied to\nstereo matching, MatchStereo-B ranked 1st in average error on the public\nMiddlebury benchmark and requires only 29ms for KITTI-resolution inference.\nMatchStereo-T can process 4K UHD images in 0.1 seconds using only 3GB of GPU\nmemory. The proposed models also achieve state-of-the-art performance on KITTI\n2012, KITTI 2015, ETH3D, and Spring flow datasets. The combination of high\naccuracy and low computational complexity makes real-time, high-resolution, and\nhigh-accuracy cross-view matching possible. Code is available at\nhttps://github.com/TingmanYan/MatchAttention.", "AI": {"tldr": "The paper introduces MatchAttention for high-resolution image cross-view matching, utilizing BilinearSoftmax for continuous attention sampling. MatchDecoder and architectural improvements reduce occlusion impacts, achieving top performance on stereo matching benchmarks with efficiency.", "motivation": "The motivation behind this research is to overcome the limitations in existing cross-attention mechanisms, which are inefficient for high-resolution images due to their high computational demands and lack of explicit constraints. The authors aim to develop a more robust and efficient method for cross-view matching.", "method": "This paper introduces MatchAttention, an attention mechanism that achieves continuous and differentiable sliding-window attention sampling through BilinearSoftmax, addressing the issue of high-resolution image matching. The mechanism dynamically matches relative positions determined by the query, embedding these positions into the feature channels to iteratively update them through residual connections. MatchDecoder, which is a hierarchical cross-view decoder, employs MatchAttention to efficiently decode information. Additionally, the paper addresses the cross-view occlusion problem by introducing gated cross-MatchAttention and a consistency-constrained loss function, enhancing the model's ability to focus on learning matching relationships.", "result": "When tested for stereo matching, MatchStereo-B has been ranked 1st in average error on the Middlebury benchmark and can infer KITTI-resolution images within 29ms. MatchStereo-T can process 4K UHD images in just 0.1 seconds using only 3GB of GPU memory. The proposed models also outperform existing methods on KITTI 2012, KITTI 2015, ETH3D, and Spring flow datasets.", "conclusion": "The research concludes that the proposed MatchAttention and MatchDecoder significantly advance the state-of-the-art in cross-view matching, particularly in stereo vision tasks. They achieve a balance of high accuracy and low computational complexity, enabling real-time, high-resolution, and high-accuracy cross-view matching."}}
{"id": "2510.13849", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13849", "abs": "https://arxiv.org/abs/2510.13849", "authors": ["Andrey Goncharov", "Nikolai Kondusov", "Alexey Zaytsev"], "title": "Language steering in latent space to mitigate unintended code-switching", "comment": null, "summary": "Multilingual Large Language Models (LLMs) often exhibit unintended\ncode-switching, reducing reliability in downstream tasks. We propose\nlatent-space language steering, a lightweight inference-time method that\nidentifies language directions via PCA on parallel translations and steers\ntoken embeddings along these axes to control language identity. Our approach\nmitigates code-switching while preserving semantics with negligible\ncomputational overhead and requires only minimal parallel data for calibration.\nEmpirically, we achieve 95-99\\% language classification accuracy using a single\nprincipal component and reduce next-token distributional divergence by up to\n42% across multiple language pairs on Qwen2.5 and Llama-3.2 models. We further\nanalyze the layer-wise evolution of language representations, revealing that\nlanguage identity concentrates in final layers with near-perfect linear\nseparability.", "AI": {"tldr": "研究了多语言大型语言模型中的意外代码切换问题，提出了一种轻量级的潜空间语言导向方法，显著提高了语言分类准确率并减少了分布差异。", "motivation": "多语言大型语言模型经常表现出意外的代码切换，这降低了下游任务的可靠性。", "method": "我们提出了一种轻量级的推理时方法——潜空间语言导向，通过PCA对平行翻译进行语言方向的识别，并引导令牌嵌入沿着这些轴线来控制语言身份。", "result": "我们的方法实现了95-99%的语言分类准确率，并且在多个语言对上减少了多达42%的下一个令牌的分布差异。", "conclusion": "我们的方法减轻了代码切换，同时保持语义，并且计算开销可忽略。此外，我们还分析了语言表示的层间演化，揭示了语言身份在最终几层中具有几乎完美的线性可分性。"}}
{"id": "2510.14266", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14266", "abs": "https://arxiv.org/abs/2510.14266", "authors": ["Miu Sumino", "Mayu Ishii", "Shun Kaizu", "Daisuke Hisano", "Yu Nakayama"], "title": "Experimental Demonstration of Event-based Optical Camera Communication in Long-Range Outdoor Environment", "comment": null, "summary": "We propose a robust demodulation scheme for optical camera communication\nsystems using an event-based vision sensor, combining OOK with toggle\ndemodulation and a digital phase-locked loop. This is the first report to\nachieve a $\\mathrm{BER} < 10^{-3}$ at 200m-60kbps and 400m-30kbps in outdoor\nexperiments.", "AI": {"tldr": "首次报道了使用结合OOK和切换解调以及数字锁相环技术的新解调方案，使得室外实验中的光相机通讯系统实现了高鲁棒性和高效的传输效果。", "motivation": "提高光学相机通信系统的解调性能，尤其是在室外环境下的鲁棒性和传输效率。", "method": "结合OOK调制方式、切换解调技术以及数字锁相环的解调方案。", "result": "我们提出了一种基于事件驱动视觉传感器的光学相机通信系统的鲁棒解调方案，结合了OOK、切换解调和数字锁相环。这项工作首次在户外实验中实现了在200m-60kbps和400m-30kbps下的BER < 10^{-3}。", "conclusion": "实现了在200m-60kbps和400m-30kbps下，在室外实验中的误码率低于10^{-3}，证明了其鲁棒性和实用性。"}}
{"id": "2510.13850", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13850", "abs": "https://arxiv.org/abs/2510.13850", "authors": ["Minju Gwak", "Guijin Son", "Jaehyung Kim"], "title": "Revisiting the UID Hypothesis in LLM Reasoning Traces", "comment": null, "summary": "Large language models (LLMs) often solve problems using step-by-step\nChain-of-Thought (CoT) reasoning, yet these intermediate steps are frequently\nunfaithful or hard to interpret. Inspired by the Uniform Information Density\n(UID) hypothesis in psycholinguistics -- which posits that humans communicate\nby maintaining a stable flow of information -- we introduce entropy-based\nmetrics to analyze the information flow within reasoning traces. Surprisingly,\nacross three challenging mathematical benchmarks, we find that successful\nreasoning in LLMs is globally non-uniform: correct solutions are characterized\nby uneven swings in information density, in stark contrast to human\ncommunication patterns. This result challenges assumptions about machine\nreasoning and suggests new directions for designing interpretable and adaptive\nreasoning models.", "AI": {"tldr": "研究通过基于熵的信息流分析发现，大型语言模型的正确推理过程中的信息密度变化不均，这与人类交流模式不同，挑战了关于机器推理的假设。", "motivation": "大型语言模型经常使用chain-of-thought推理来解决问题，但这些中间步骤往往不准确或难以理解。该研究动机在于理解机器推理过程中的信息流并挑战现有的有关机器推理的假设。", "method": "通过引入基于熵的度量来分析推理过程中的信息流，这一方法受到心理语言学中的均匀信息密度假设的启发，该假设认为人类通过保持稳定的信息流来进行沟通。", "result": "在三个具有挑战性的数学基准测试中，研究发现大型语言模型在成功推理时表现出了全局非均匀性，与人类交流模式形成鲜明对比。", "conclusion": "该研究挑战了关于机器推理的假设，并为设计可解释和自适应的推理模型提供了新的方向。"}}
{"id": "2510.14270", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.14270", "abs": "https://arxiv.org/abs/2510.14270", "authors": ["Alexander Valverde", "Brian Xu", "Yuyin Zhou", "Meng Xu", "Hongyun Wang"], "title": "GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering", "comment": null, "summary": "Scene reconstruction has emerged as a central challenge in computer vision,\nwith approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting\nachieving remarkable progress. While Gaussian Splatting demonstrates strong\nperformance on large-scale datasets, it often struggles to capture fine details\nor maintain realism in regions with sparse coverage, largely due to the\ninherent limitations of sparse 3D training data.\n  In this work, we propose GauSSmart, a hybrid method that effectively bridges\n2D foundational models and 3D Gaussian Splatting reconstruction. Our approach\nintegrates established 2D computer vision techniques, including convex\nfiltering and semantic feature supervision from foundational models such as\nDINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D\nsegmentation priors and high-dimensional feature embeddings, our method guides\nthe densification and refinement of Gaussian splats, improving coverage in\nunderrepresented areas and preserving intricate structural details.\n  We validate our approach across three datasets, where GauSSmart consistently\noutperforms existing Gaussian Splatting in the majority of evaluated scenes.\nOur results demonstrate the significant potential of hybrid 2D-3D approaches,\nhighlighting how the thoughtful combination of 2D foundational models with 3D\nreconstruction pipelines can overcome the limitations inherent in either\napproach alone.", "AI": {"tldr": "GauSSmart is a hybrid method that integrates 2D computer vision models with 3D Gaussian Splatting for more detailed and realistic 3D scene reconstructions, addressing the sparse data issue of Gaussian Splatting.", "motivation": "The motivation behind this paper is to address the limitation of Gaussian Splatting in capturing fine details and maintaining realism in sparse regions of 3D scenes. The authors aim to improve the reconstruction performance on large-scale datasets by integrating 2D foundational models.", "method": "The method proposed, named GauSSmart, integrates 2D computer vision techniques and Gaussian Splatting for 3D scene reconstruction. It uses methods like convex filtering and semantic feature supervision from models like DINO to refine and densify Gaussian splats, thereby enhancing reconstruction quality, especially in regions with sparse data coverage.", "result": "The results demonstrate that GauSSmart outperforms Gaussian Splatting on three datasets across most evaluated scenes, showing the potential of hybrid 2D-3D reconstruction methods for enhancing 3D scene reconstruction quality.", "conclusion": "The conclusion is that by combining 2D and 3D approaches, GauSSmart can overcome the inherent limitations of Gaussian Splatting alone, providing better coverage and preservation of details in 3D scene reconstructions."}}
{"id": "2510.13851", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13851", "abs": "https://arxiv.org/abs/2510.13851", "authors": ["Sicheng Lyu", "Yu Gu", "Xinyu Wang", "Jerry Huang", "Sitao Luan", "Yufei Cui", "Xiao-Wen Chang", "Peng Lu"], "title": "EvoEdit: Evolving Null-space Alignment for Robust and Efficient Knowledge Editing", "comment": null, "summary": "Large language models (LLMs) require continual updates to rectify outdated or\nerroneous knowledge. Model editing has emerged as a compelling paradigm for\nintroducing targeted modifications without the computational burden of full\nretraining. Existing approaches are mainly based on a locate-then-edit\nframework. However, in sequential editing contexts, where multiple updates are\napplied over time, they exhibit significant limitations and suffer from\ncatastrophic interference, i.e., new edits compromise previously integrated\nupdates and degrade preserved knowledge. To address these challenges, we\nintroduce EvoEdit, a novel editing strategy that mitigates catastrophic\ninterference through sequential null-space alignment, enabling stable and\nefficient model editing. By performing sequential null-space alignment for each\nincoming edit, EvoEdit preserves both original and previously modified\nknowledge representations and maintains output invariance on preserved\nknowledge even across long edit sequences, effectively mitigating interference.\nEvaluations on real-world sequential knowledge-editing benchmarks show that\nEvoEdit achieves better or comparable performance than prior state-of-the-art\nlocate-then-edit techniques, with up to 3.53 times speedup. Overall, these\nresults underscore the necessity of developing more principled approaches for\ndesigning LLMs in dynamically evolving information settings, while providing a\nsimple yet effective solution with strong theoretical guarantees.", "AI": {"tldr": "提出了一种新的编辑策略EvoEdit，通过顺序空域对齐来减少灾难性干扰，有效地保存和编辑语言模型的知识。", "motivation": "由于现有模型修改方法在面对连续的多次修改时会有灾难性的干扰，新的编辑会削弱之前整合的更新和原有的知识。为了解决这个问题，提出了EvoEdit。", "method": "通过一系列空域对齐操作，EvoEdit 能够保存原始和先前修改过的知识表示，并且即使在长序列编辑过程中，也能够维持对保存知识的输出不变性，从而有效减少干扰。", "result": "在实际的连续知识编辑基准测试中，EvoEdit 达到了比先前最先进的定位然后编辑技术更好的或相当的表现，并且速度提高了3.53倍。", "conclusion": "这些结果突显了在信息动态变化的环境中设计更合理的方法来构建大规模语言模型的必要性，同时提供了一个简单却有效的解决方案，具有较强的理论保障。"}}
{"id": "2510.14273", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14273", "abs": "https://arxiv.org/abs/2510.14273", "authors": ["Kieu-Anh Truong Thi", "Huy-Hieu Pham", "Duc-Trong Le"], "title": "CLEAR: Causal Learning Framework For Robust Histopathology Tumor Detection Under Out-Of-Distribution Shifts", "comment": null, "summary": "Domain shift in histopathology, often caused by differences in acquisition\nprocesses or data sources, poses a major challenge to the generalization\nability of deep learning models. Existing methods primarily rely on modeling\nstatistical correlations by aligning feature distributions or introducing\nstatistical variation, yet they often overlook causal relationships. In this\nwork, we propose a novel causal-inference-based framework that leverages\nsemantic features while mitigating the impact of confounders. Our method\nimplements the front-door principle by designing transformation strategies that\nexplicitly incorporate mediators and observed tissue slides. We validate our\nmethod on the CAMELYON17 dataset and a private histopathology dataset,\ndemonstrating consistent performance gains across unseen domains. As a result,\nour approach achieved up to a 7% improvement in both the CAMELYON17 dataset and\nthe private histopathology dataset, outperforming existing baselines. These\nresults highlight the potential of causal inference as a powerful tool for\naddressing domain shift in histopathology image analysis.", "AI": {"tldr": "A causal-inference-based framework leveraging semantic features while mitigating the impact of confounders is proposed to address domain shift in histopathology, showing consistent performance gains in unseen domains.", "motivation": "The motivation is to address domain shift in histopathology caused by differences in acquisition processes or data sources, which affects the generalization ability of deep learning models. Existing methods often overlook causal relationships.", "method": "Our method implements the front-door principle by designing transformation strategies that explicitly incorporate mediators and observed tissue slides.", "result": "The approach achieved up to a 7% improvement in performance on both the CAMELYON17 dataset and a private histopathology dataset, outperforming existing methods.", "conclusion": "The conclusion is that using causal inference as a framework to handle domain shift in histopathology image analysis can lead to significant performance improvements, improving by up to 7% in unseen domains."}}
{"id": "2510.13852", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.13852", "abs": "https://arxiv.org/abs/2510.13852", "authors": ["Peter Banyas", "Shristi Sharma", "Alistair Simmons", "Atharva Vispute"], "title": "ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When Responding to Different Demographic Groups", "comment": "For associated code repository, see\n  http://github.com/banyasp/consistencyAI For user-friendly web app, see\n  http://v0-llm-comparison-webapp.vercel.app/", "summary": "Is an LLM telling you different facts than it's telling me? This paper\nintroduces ConsistencyAI, an independent benchmark for measuring the factual\nconsistency of large language models (LLMs) for different personas.\nConsistencyAI tests whether, when users of different demographics ask identical\nquestions, the model responds with factually inconsistent answers. Designed\nwithout involvement from LLM providers, this benchmark offers impartial\nevaluation and accountability. In our experiment, we queried 19 LLMs with\nprompts that requested 5 facts for each of 15 topics. We repeated this query\n100 times for each LLM, each time adding prompt context from a different\npersona selected from a subset of personas modeling the general population. We\nprocessed the responses into sentence embeddings, computed cross-persona cosine\nsimilarity, and computed the weighted average of cross-persona cosine\nsimilarity to calculate factual consistency scores. In 100-persona experiments,\nscores ranged from 0.9065 to 0.7896, and the mean was 0.8656, which we adopt as\na benchmark threshold. xAI's Grok-3 is most consistent, while several\nlightweight models rank lowest. Consistency varies by topic: the job market is\nleast consistent, G7 world leaders most consistent, and issues like vaccines or\nthe Israeli-Palestinian conflict diverge by provider. These results show that\nboth the provider and the topic shape the factual consistency. We release our\ncode and interactive demo to support reproducible evaluation and encourage\npersona-invariant prompting strategies.", "AI": {"tldr": "该研究提出了一种新的基准ConsistencyAI，用于衡量大型语言模型在不同人设背景下的事实一致性，发现一致性得分在0.7896到0.9065之间，意味着模型在不同话题和其他因素下会有不同的事实一致性表现。", "motivation": "创建一个独立的基准，用于衡量不同人设背景下大型语言模型的事实一致性，从而提供公正的评估和问责机制。", "method": "通过使用不同的人设背景对同一问题进行100次查询，对19个大型语言模型进行了事实一致性测试。将响应转化为句子嵌入，计算跨人设的余弦相似度，并计算加权平均的跨人设余弦相似度来计算事实一致性得分。", "result": "在100个人设的实验中，模型的一致性得分从0.9065到0.7896不等，平均得分为0.8656。xAI的Grok-3一致性最高，而一些轻量级模型排名最低。一致性因话题而异，就业市场一致性最低，G7世界领导人一致性最高，而像疫苗或以色列-巴勒斯坦冲突这样的问题则因供应商不同而有差异。", "conclusion": "研究结果显示，提供者和话题都会影响模型的事实一致性，表明在不同场景和问题下，即使是同一模型也可能表现出不同的事实一致性。这促使了公平和一致性的人设无关提示策略的开发。"}}
{"id": "2510.14304", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14304", "abs": "https://arxiv.org/abs/2510.14304", "authors": ["Kyungryul Back", "Seongbeom Park", "Milim Kim", "Mincheol Kwon", "SangHyeok Lee", "Hyunyoung Lee", "Junhee Cho", "Seunghyun Park", "Jinkyu Kim"], "title": "Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding", "comment": "EMNLP 2025 Findings; Project: https://github.com/KR-0822/TCD", "summary": "Large Vision-Language Models (LVLMs) have recently shown promising results on\nvarious multimodal tasks, even achieving human-comparable performance in\ncertain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often\nrely heavily on a single modality or memorize training data without properly\ngrounding their outputs. To address this, we propose a training-free, tri-layer\ncontrastive decoding with watermarking, which proceeds in three steps: (1)\nselect a mature layer and an amateur layer among the decoding layers, (2)\nidentify a pivot layer using a watermark-related question to assess whether the\nlayer is visually well-grounded, and (3) apply tri-layer contrastive decoding\nto generate the final output. Experiments on public benchmarks such as POPE,\nMME and AMBER demonstrate that our method achieves state-of-the-art performance\nin reducing hallucinations in LVLMs and generates more visually grounded\nresponses.", "AI": {"tldr": "针对大型视觉语言模型的幻觉问题，研究者提出了一种无训练的三层对比解码带水印的方法，以减轻幻觉问题，实验显示它在减少幻觉方面达到了顶点水平。", "motivation": "尽管大型视觉语言模型在许多多模态任务中展示了有前景的结果，但在某些情况下仍容易产生幻觉。", "method": "我们提出了一种无训练、三层对比解码结合水印的方法来解决大型视觉语言模型（LVLMs）中的幻觉问题。该方法分三步进行：(1) 在解码层中选择一个成熟的层和一个业余的层，(2) 使用与水印相关的问答来识别出一个枢轴层来评估该层是否与视觉良好地接地，(3) 应用三层对比解码生成最终输出。", "result": "实验结果表明，该方法在减少大型视觉语言模型中的幻觉方面达到了最先进的性能，并生成了更多的视觉接地响应。在POPE，MME和AMBER等公共基准测试中得到了验证。", "conclusion": "此方法可以有效地减少大型视觉语言模型的幻觉，产生更稳健的视觉接地响应。"}}
