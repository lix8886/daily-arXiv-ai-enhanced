{"id": "2509.12242", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12242", "abs": "https://arxiv.org/abs/2509.12242", "authors": ["Mustafa Khanbhai", "Giulia Di Nardo", "Jun Ma", "Vivienne Freitas", "Caterina Masino", "Ali Dolatabadi", "Zhaoxun \"Lorenz\" Liu", "Wey Leong", "Wagner H. Souza", "Amin Madani"], "title": "Artificial Intelligence in Breast Cancer Care: Transforming Preoperative Planning and Patient Education with 3D Reconstruction", "comment": null, "summary": "Effective preoperative planning requires accurate algorithms for segmenting\nanatomical structures across diverse datasets, but traditional models struggle\nwith generalization. This study presents a novel machine learning methodology\nto improve algorithm generalization for 3D anatomical reconstruction beyond\nbreast cancer applications. We processed 120 retrospective breast MRIs (January\n2018-June 2023) through three phases: anonymization and manual segmentation of\nT1-weighted and dynamic contrast-enhanced sequences; co-registration and\nsegmentation of whole breast, fibroglandular tissue, and tumors; and 3D\nvisualization using ITK-SNAP. A human-in-the-loop approach refined\nsegmentations using U-Mamba, designed to generalize across imaging scenarios.\nDice similarity coefficient assessed overlap between automated segmentation and\nground truth. Clinical relevance was evaluated through clinician and patient\ninterviews. U-Mamba showed strong performance with DSC values of 0.97\n($\\pm$0.013) for whole organs, 0.96 ($\\pm$0.024) for fibroglandular tissue, and\n0.82 ($\\pm$0.12) for tumors on T1-weighted images. The model generated accurate\n3D reconstructions enabling visualization of complex anatomical features.\nClinician interviews indicated improved planning, intraoperative navigation,\nand decision support. Integration of 3D visualization enhanced patient\neducation, communication, and understanding. This human-in-the-loop machine\nlearning approach successfully generalizes algorithms for 3D reconstruction and\nanatomical segmentation across patient datasets, offering enhanced\nvisualization for clinicians, improved preoperative planning, and more\neffective patient education, facilitating shared decision-making and empowering\ninformed patient choices across medical applications.", "AI": {"tldr": "本研究提出了一种新的机器学习方法U-Mamba，用于提高3D解剖结构重建算法在乳腺癌等应用中的泛化能力。实验表明该方法在不同解剖结构上的Dice相似性系数(DSC)值分别为0.97、0.96和0.82，并通过临床评估证明了其在手术计划和患者教育方面的价值。", "motivation": "传统的模型在泛化能力方面存在局限性，难以处理不同数据集中的解剖结构分割。本研究旨在提高算法的泛化能力，以改善3D解剖重建的精度，超越乳腺癌应用范围。", "method": "本研究对2018年1月至2023年6月的120份回顾性乳腺MRI进行处理，利用U-Mamba模型对T1加权和动态对比增强序列图像进行分割，并在ITK-SNAP软件中进行3D可视化。通过人工介入的方式进行模型优化，以提高其在不同影像场景中的泛化能力。", "result": "使用U-Mamba模型，在全器官、纤维腺体组织以及肿瘤分割上，DSC值分别为0.97（±0.013）、0.96（±0.024）和0.82（±0.12），显示了强大的性能。", "conclusion": "基于人工介入的机器学习方法成功使算法在不同患者数据集的3D重建和解剖分割上泛化，改进了临床前规划和手术导航，并提升了患者教育，更好地支持了共享决策和患者自主选择。"}}
{"id": "2509.12244", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12244", "abs": "https://arxiv.org/abs/2509.12244", "authors": ["Lu Cai", "Fei Xu", "Min Xian", "Yalei Tang", "Shoukun Sun", "John Stempien"], "title": "RU-Net for Automatic Characterization of TRISO Fuel Cross Sections", "comment": null, "summary": "During irradiation, phenomena such as kernel swelling and buffer\ndensification may impact the performance of tristructural isotropic (TRISO)\nparticle fuel. Post-irradiation microscopy is often used to identify these\nirradiation-induced morphologic changes. However, each fuel compact generally\ncontains thousands of TRISO particles. Manually performing the work to get\nstatistical information on these phenomena is cumbersome and subjective. To\nreduce the subjectivity inherent in that process and to accelerate data\nanalysis, we used convolutional neural networks (CNNs) to automatically segment\ncross-sectional images of microscopic TRISO layers. CNNs are a class of\nmachine-learning algorithms specifically designed for processing structured\ngrid data. They have gained popularity in recent years due to their remarkable\nperformance in various computer vision tasks, including image classification,\nobject detection, and image segmentation. In this research, we generated a\nlarge irradiated TRISO layer dataset with more than 2,000 microscopic images of\ncross-sectional TRISO particles and the corresponding annotated images. Based\non these annotated images, we used different CNNs to automatically segment\ndifferent TRISO layers. These CNNs include RU-Net (developed in this study), as\nwell as three existing architectures: U-Net, Residual Network (ResNet), and\nAttention U-Net. The preliminary results show that the model based on RU-Net\nperforms best in terms of Intersection over Union (IoU). Using CNN models, we\ncan expedite the analysis of TRISO particle cross sections, significantly\nreducing the manual labor involved and improving the objectivity of the\nsegmentation results.", "AI": {"tldr": "研究使用卷积神经网络（CNN）自动分割TRISO颗粒微观横截面图像，减少了分析中的主观性和手动工作量，并提高了分割结果的客观性。RU-Net模型在IoU方面表现最佳。", "motivation": "鉴于TRISO颗粒燃料在辐照过程中会出现芯块膨胀和缓冲层致密化等现象，传统的手工分析既耗时又具有主观性。为了减少这种主观性并加速数据分析的进程，研究引入了卷积神经网络进行自动分析。", "method": "生成了包含2000多张辐照TRISO层微观图像及其标注图像的大数据集，使用RU-Net以及三种现存架构（U-Net、ResNet和Attention U-Net）进行自动分割。RU-Net是在此次研究中开发的模型。", "result": "初步结果显示，基于RU-Net的模型在衡量两个区域重叠程度的IoU指标上表现最佳。", "conclusion": "通过使用CNN模型，加快了TRISO颗粒横截面图像的分析速度，明显减少了手工劳动并提高了分割结果的客观性。"}}
{"id": "2509.12247", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12247", "abs": "https://arxiv.org/abs/2509.12247", "authors": ["Abigail R. Cohen", "Yuming Sun", "Zhihao Qin", "Harsh S. Muriki", "Zihao Xiao", "Yeonju Lee", "Matthew Housley", "Andrew F. Sharkey", "Rhuanito S. Ferrarezi", "Jing Li", "Lu Gan", "Yongsheng Chen"], "title": "Modular, On-Site Solutions with Lightweight Anomaly Detection for Sustainable Nutrient Management in Agriculture", "comment": null, "summary": "Efficient nutrient management is critical for crop growth and sustainable\nresource consumption (e.g., nitrogen, energy). Current approaches require\nlengthy analyses, preventing real-time optimization; similarly, imaging\nfacilitates rapid phenotyping but can be computationally intensive, preventing\ndeployment under resource constraints. This study proposes a flexible, tiered\npipeline for anomaly detection and status estimation (fresh weight, dry mass,\nand tissue nutrients), including a comprehensive energy analysis of approaches\nthat span the efficiency-accuracy spectrum. Using a nutrient depletion\nexperiment with three treatments (T1-100%, T2-50%, and T3-25% fertilizer\nstrength) and multispectral imaging (MSI), we developed a hierarchical pipeline\nusing an autoencoder (AE) for early warning. Further, we compared two status\nestimation modules of different complexity for more detailed analysis:\nvegetation index (VI) features with machine learning (Random Forest, RF) and\nraw whole-image deep learning (Vision Transformer, ViT). Results demonstrated\nhigh-efficiency anomaly detection (73% net detection of T3 samples 9 days after\ntransplanting) at substantially lower energy than embodied energy in wasted\nnitrogen. The state estimation modules show trade-offs, with ViT outperforming\nRF on phosphorus and calcium estimation (R2 0.61 vs. 0.58, 0.48 vs. 0.35) at\nhigher energy cost. With our modular pipeline, this work opens opportunities\nfor edge diagnostics and practical opportunities for agricultural\nsustainability.", "AI": {"tldr": "The paper proposes a flexible, tiered pipeline using multi-spectral imaging and machine/deep learning for efficient nutrient management in crop growth, balancing between energy efficiency and estimation accuracy.", "motivation": "The motivation of the study is to improve nutrient management efficiency for crop growth and sustainable resource use, overcoming the limitations of current approaches which are either slow or too computationally intensive.", "method": "This study uses a nutrient depletion experiment with three treatments (100%, 50%, and 25% fertilizer strength), and multispectral imaging (MSI). A hierarchical pipeline involving an autoencoder (AE) for early warning and two status estimation modules (vegetation index features with machine learning (Random Forest, RF) and raw whole-image deep learning (Vision Transformer, ViT)) are developed for anomaly detection and status estimation.", "result": "The study concludes that the hierarchical pipeline provides high-efficiency anomaly detection (73% detection of T3 samples 9 days after transplanting) with lower energy consumption. The state estimation using ViT outperforms that using RF in assessing phosphorus and calcium but at a higher energy cost.", "conclusion": "The findings suggest that the modular pipeline can contribute to edge diagnostics and sustainable agriculture practices, offering a balance between energy efficiency and detailed analysis capability."}}
{"id": "2509.12248", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12248", "abs": "https://arxiv.org/abs/2509.12248", "authors": ["Yuriel Ryan", "Rui Yang Tan", "Kenny Tsu Wei Choo", "Roy Ka-Wei Lee"], "title": "Humor in Pixels: Benchmarking Large Multimodal Models Understanding of Online Comics", "comment": "27 pages, 8 figures, EMNLP 2025", "summary": "Understanding humor is a core aspect of social intelligence, yet it remains a\nsignificant challenge for Large Multimodal Models (LMMs). We introduce\nPixelHumor, a benchmark dataset of 2,800 annotated multi-panel comics designed\nto evaluate LMMs' ability to interpret multimodal humor and recognize narrative\nsequences. Experiments with state-of-the-art LMMs reveal substantial gaps: for\ninstance, top models achieve only 61% accuracy in panel sequencing, far below\nhuman performance. This underscores critical limitations in current models'\nintegration of visual and textual cues for coherent narrative and humor\nunderstanding. By providing a rigorous framework for evaluating multimodal\ncontextual and narrative reasoning, PixelHumor aims to drive the development of\nLMMs that better engage in natural, socially aware interactions.", "AI": {"tldr": "研究引入了PixelHumor数据集，以评估LMMs在多模态幽默和叙事理解上的能力，实验显示现有LMMs性能仍有待提高。", "motivation": "理解幽默是社会智能的核心方面，但对LMMs来说仍然是一个重大挑战。该项目的目标是提供一个严格的框架来评估多模态情境和叙事推理，从而推动LMMs在自然、社交互动方面的发展。", "method": "通过引入PixelHumor，一个包含2,800个注释多面板漫画的数据集来评估大型多模态模型（LMMs）在解读多模态幽默和识别叙事序列方面的能力。", "result": "实验发现，最先进的LMMs在区分面板顺序方面仅达到61%的准确率，远低于人类的表现。这揭示了现有模型在集成视觉和文本线索以理解连贯叙事和幽默方面存在重大不足。", "conclusion": "PixelHumor提供了一个评估LMMs多模态叙事和幽默理解能力的框架，通过识别现有LMMs的不足来推动模型的发展。"}}
{"id": "2509.12340", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12340", "abs": "https://arxiv.org/abs/2509.12340", "authors": ["Nikolay Banar", "Ehsan Lotfi", "Jens Van Nooten", "Cristina Arhiliuc", "Marija Kliocaite", "Walter Daelemans"], "title": "MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch", "comment": null, "summary": "Recently, embedding resources, including models, benchmarks, and datasets,\nhave been widely released to support a variety of languages. However, the Dutch\nlanguage remains underrepresented, typically comprising only a small fraction\nof the published multilingual resources. To address this gap and encourage the\nfurther development of Dutch embeddings, we introduce new resources for their\nevaluation and generation. First, we introduce the Massive Text Embedding\nBenchmark for Dutch (MTEB-NL), which includes both existing Dutch datasets and\nnewly created ones, covering a wide range of tasks. Second, we provide a\ntraining dataset compiled from available Dutch retrieval datasets, complemented\nwith synthetic data generated by large language models to expand task coverage\nbeyond retrieval. Finally, we release a series of E5-NL models compact yet\nefficient embedding models that demonstrate strong performance across multiple\ntasks. We make our resources publicly available through the Hugging Face Hub\nand the MTEB package.", "AI": {"tldr": "为解决荷兰语在多语言资源中的代表性不足问题，我们推出了MTEB-NL，一个新训练数据集以及一系列高效的E5-NL模型。", "motivation": "荷兰语在发布的多语言资源中代表性不足，我们希望通过引入新的评估和生成资源来解决这一问题。", "method": "我们介绍了Massive Text Embedding Benchmark for Dutch (MTEB-NL)，它包含了现有的荷兰语数据集和新创建的数据集，涵盖了广泛的任务。此外，我们提供了一个训练数据集，该数据集基于现有的荷兰语检索数据集，并通过大型语言模型生成的合成数据来扩展任务覆盖范围。最后，我们发布了紧凑且高效的E5-NL模型系列，并通过Hugging Face Hub和MTEB包公开我们的资源。", "result": "我们发布了一系列的E5-NL模型，这些模型在多个任务上表现出色。", "conclusion": "我们的研究有助于填补荷兰语嵌入式资源的空白，并鼓励进一步开发荷兰语嵌入式模型。"}}
{"id": "2509.12250", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12250", "abs": "https://arxiv.org/abs/2509.12250", "authors": ["Yihong Ji", "Yunze Liu", "Yiyao Zhuo", "Weijiang Yu", "Fei Ma", "Joshua Huang", "Fei Yu"], "title": "OnlineHOI: Towards Online Human-Object Interaction Generation and Perception", "comment": "Accepted at ACM MM 2025", "summary": "The perception and generation of Human-Object Interaction (HOI) are crucial\nfor fields such as robotics, AR/VR, and human behavior understanding. However,\ncurrent approaches model this task in an offline setting, where information at\neach time step can be drawn from the entire interaction sequence. In contrast,\nin real-world scenarios, the information available at each time step comes only\nfrom the current moment and historical data, i.e., an online setting. We find\nthat offline methods perform poorly in an online context. Based on this\nobservation, we propose two new tasks: Online HOI Generation and Perception. To\naddress this task, we introduce the OnlineHOI framework, a network architecture\nbased on the Mamba framework that employs a memory mechanism. By leveraging\nMamba's powerful modeling capabilities for streaming data and the Memory\nmechanism's efficient integration of historical information, we achieve\nstate-of-the-art results on the Core4D and OAKINK2 online generation tasks, as\nwell as the online HOI4D perception task.", "AI": {"tldr": "针对HOI的线上生成和感知任务，提出并实现了OnlineHOI框架。", "motivation": "现有的人类-物体交互（HOI）方法主要适用于离线场景，而在现实世界中，信息仅限于当前时刻及历史数据，离线方法在线上环境中表现不佳。", "method": "引入了基于Mamba框架的OnlineHOI架构，该框架采用了记忆机制，能够高效整合历史信息。", "result": "在Core4D和OAKINK2在线生成任务以及线上HOI4D感知任务中，达到了最先进的结果。", "conclusion": "OnlineHOI能在真实世界环境中更好地处理人类-物体交互数据流，表现出色。"}}
{"id": "2509.12371", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.12371", "abs": "https://arxiv.org/abs/2509.12371", "authors": ["Matteo Marcuzzo", "Alessandro Zangari", "Andrea Albarelli", "Jose Camacho-Collados", "Mohammad Taher Pilehvar"], "title": "MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs with Fables", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "As LLMs excel on standard reading comprehension benchmarks, attention is\nshifting toward evaluating their capacity for complex abstract reasoning and\ninference. Literature-based benchmarks, with their rich narrative and moral\ndepth, provide a compelling framework for evaluating such deeper comprehension\nskills. Here, we present MORABLES, a human-verified benchmark built from fables\nand short stories drawn from historical literature. The main task is structured\nas multiple-choice questions targeting moral inference, with carefully crafted\ndistractors that challenge models to go beyond shallow, extractive question\nanswering. To further stress-test model robustness, we introduce adversarial\nvariants designed to surface LLM vulnerabilities and shortcuts due to issues\nsuch as data contamination. Our findings show that, while larger models\noutperform smaller ones, they remain susceptible to adversarial manipulation\nand often rely on superficial patterns rather than true moral reasoning. This\nbrittleness results in significant self-contradiction, with the best models\nrefuting their own answers in roughly 20% of cases depending on the framing of\nthe moral choice. Interestingly, reasoning-enhanced models fail to bridge this\ngap, suggesting that scale - not reasoning ability - is the primary driver of\nperformance.", "AI": {"tldr": "The paper introduces MORABLES, a benchmark designed to assess large language models' capabilities in moral inference through complex reasoning and inference, highlighting the models' susceptibility to adversarial manipulation and reliance on superficial patterns.", "motivation": "The motivation is to evaluate large language models' deeper comprehension skills, particularly in moral reasoning, beyond standard benchmarks that primarily test reading comprehension.", "method": "The method involves creating multiple-choice questions from fables and short stories, introducing adversarial variants to challenge the models' robustness, and collecting human-verified answers for comparison.", "result": "Larger models outperform smaller ones but exhibit significant vulnerabilities and often self-contradict when tested on moral choice questions, suggesting that their performance does not imply true reasoning ability.", "conclusion": "The conclusion is that while larger models perform better on the MORABLES benchmark, their results are driven by scale rather than improved reasoning capabilities, and they struggle with adversarial manipulation."}}
{"id": "2509.12258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12258", "abs": "https://arxiv.org/abs/2509.12258", "authors": ["Li Kun", "Milena Radenkovic"], "title": "EfficientNet-Based Multi-Class Detection of Real, Deepfake, and Plastic Surgery Faces", "comment": null, "summary": "Currently, deep learning has been utilised to tackle several difficulties in\nour everyday lives. It not only exhibits progress in computer vision but also\nconstitutes the foundation for several revolutionary technologies. Nonetheless,\nsimilar to all phenomena, the use of deep learning in diverse domains has\nproduced a multifaceted interaction of advantages and disadvantages for human\nsociety. Deepfake technology has advanced, significantly impacting social life.\nHowever, developments in this technology can affect privacy, the reputations of\nprominent personalities, and national security via software development. It can\nproduce indistinguishable counterfeit photographs and films, potentially\nimpairing the functionality of facial recognition systems, so presenting a\nsignificant risk.\n  The improper application of deepfake technology produces several detrimental\neffects on society. Face-swapping programs mislead users by altering persons'\nappearances or expressions to fulfil particular aims or to appropriate personal\ninformation. Deepfake technology permeates daily life through such techniques.\nCertain individuals endeavour to sabotage election campaigns or subvert\nprominent political figures by creating deceptive pictures to influence public\nperception, causing significant harm to a nation's political and economic\nstructure.", "AI": {"tldr": "深度学习技术的进步带来诸多益处，但也引发社会风险，特别是深伪技术可能影响个人隐私、政界形象和国家安全。", "motivation": "探讨深度学习，特别是深伪技术在社会生活中的应用及其带来的利与弊。", "method": "分析论文内容，未提供具体方法论。", "result": "深伪技术可以产生难以区分的虚假图像和视频，可能影响面部识别系统的功能，带来社会风险。", "conclusion": "深伪技术的不当使用对社会产生了负面影响，包括误导用户，影响选举和破坏政治人物形象。"}}
{"id": "2509.12382", "categories": ["cs.CL", "H.3.3; I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.12382", "abs": "https://arxiv.org/abs/2509.12382", "authors": ["Anu Pradhan", "Alexandra Ortan", "Apurv Verma", "Madhavan Seshadri"], "title": "LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation", "comment": "Accepted in EARL 25: The 2nd Workshop on Evaluating and Applying\n  Recommender Systems with Large Language Models at RecSys 2025", "summary": "The evaluation bottleneck in recommendation systems has become particularly\nacute with the rise of Generative AI, where traditional metrics fall short of\ncapturing nuanced quality dimensions that matter in specialized domains like\nlegal research. Can we trust Large Language Models to serve as reliable judges\nof their own kind? This paper investigates LLM-as-a-Judge as a principled\napproach to evaluating Retrieval-Augmented Generation systems in legal\ncontexts, where the stakes of recommendation quality are exceptionally high.\n  We tackle two fundamental questions that determine practical viability: which\ninter-rater reliability metrics best capture the alignment between LLM and\nhuman assessments, and how do we conduct statistically sound comparisons\nbetween competing systems? Through systematic experimentation, we discover that\ntraditional agreement metrics like Krippendorff's alpha can be misleading in\nthe skewed distributions typical of AI system evaluations. Instead, Gwet's AC2\nand rank correlation coefficients emerge as more robust indicators for judge\nselection, while the Wilcoxon Signed-Rank Test with Benjamini-Hochberg\ncorrections provides the statistical rigor needed for reliable system\ncomparisons.\n  Our findings suggest a path toward scalable, cost-effective evaluation that\nmaintains the precision demanded by legal applications, transforming what was\nonce a human-intensive bottleneck into an automated, yet statistically\nprincipled, evaluation framework.", "AI": {"tldr": "The paper investigates using LLMs as judges in AI evaluation, proposing more robust statistical methods for more reliable evaluations in legal applications.", "motivation": "The main motivation behind this research is the inadequacy of current evaluation methods for Generative AI in specialized domains like legal research, where the quality of recommendations is critical.", "method": "This paper explores the use of Large Language Models (LLM) as judges for evaluating Retrieval-Augmented Generation systems in legal contexts, addressing the limitations of traditional evaluation metrics. It discusses the importance of inter-rater reliability and statistical soundness in such evaluations.", "result": "The study identifies Gwet's AC2 and rank correlation coefficients as more robust for judge selection than traditional agreement metrics like Krippendorff's alpha. It also recommends the use of the Wilcoxon Signed-Rank Test with Benjamini-Hochberg corrections for comparing systems statistically.", "conclusion": "The findings propose a scalable, cost-effective, and statistically principled evaluation framework that automates the evaluation process for legal applications, thereby improving efficiency and maintaining high standards of recommendation quality."}}
{"id": "2509.12265", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12265", "abs": "https://arxiv.org/abs/2509.12265", "authors": ["Xiaoguang Chang", "Teng Wang", "Changyin Sun"], "title": "A Modern Look at Simplicity Bias in Image Classification Tasks", "comment": null, "summary": "The simplicity Bias (SB) of neural networks, i.e.\\ their tendency to\nrepresent simple functions, is a key factor in their generalization\ncapabilities. Recent studies show that an excessive SB may harm performance on\ncomplex tasks, and the need for this bias varies across tasks. Many of these\nstudies focus on simple models or synthetic tasks. It remains challenging to\nmeasure the SB in large models and little is known about the relevance of the\nSB to various image classification tasks.\n  In this paper, we investigate the relationship between the SB in CLIP models\nand their performance across image classification tasks. First, we\ntheoretically analyze the potential limitation of existing measures of\ncomplexity that have been used to characterize small models. To address this,\nwe propose a frequency-aware measure capturing finer-grained SB differences. We\nvalidate this measure on CLIP models subjected to two recent SB-modulation\nmethods, demonstrating that it is more informative and consistent than previous\nmeasures. Second, we examine the relation between the SB of those models and\ntheir performance across a range of image classification tasks, including\nzero-shot and fine-tuning settings. These experiments reveal a range of\nbehaviors. For example, a stronger SB correlates with a better performance on\nOOD generalization than on adversarial robustness. These results highlight the\nbenefits of aligning a model's inductive biases with the characteristics of the\ntarget task.", "AI": {"tldr": "本文研究了CLIP模型中简单性偏见（SB）与其在各种图像分类任务中的性能之间的关系。", "motivation": "当前的研究主要集中在简单模型或合成任务上，但对大型模型中简单性偏见（SB）的测量仍然具有挑战性，对SB在各种图像分类任务中的相关性知之甚少。", "method": "我们首先对用于表征小型模型的现有复杂性度量的潜在局限性进行了理论分析。为了解决这个问题，我们提出了一种频率感知度量，该度量能够捕捉更细粒度的简单性偏见（SB）差异。我们还验证了这种度量在受到最近两种SB调制方法影响的CLIP模型上比之前的度量更具有信息性和一致性。其次，我们研究了这些模型的SB与其在范围广泛的各种图像分类任务中的性能之间的关系，包括零样本设置和微调设置。", "result": "实验结果揭示了一系列行为。例如，更强的SB与在OOD泛化上的更好性能相关，而非在对抗鲁棒性方面。", "conclusion": "这些结果强调了将模型的归纳偏差与目标任务特性相一致的好处。"}}
{"id": "2509.12385", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12385", "abs": "https://arxiv.org/abs/2509.12385", "authors": ["Mitchell Plyler", "Yilun Zhang", "Alexander Tuzhilin", "Saoud Khalifah", "Sen Tian"], "title": "SENTRA: Selected-Next-Token Transformer for LLM Text Detection", "comment": "EMNLP Findings 2025", "summary": "LLMs are becoming increasingly capable and widespread. Consequently, the\npotential and reality of their misuse is also growing. In this work, we address\nthe problem of detecting LLM-generated text that is not explicitly declared as\nsuch. We present a novel, general-purpose, and supervised LLM text detector,\nSElected-Next-Token tRAnsformer (SENTRA). SENTRA is a Transformer-based encoder\nleveraging selected-next-token-probability sequences and utilizing contrastive\npre-training on large amounts of unlabeled data. Our experiments on three\npopular public datasets across 24 domains of text demonstrate SENTRA is a\ngeneral-purpose classifier that significantly outperforms popular baselines in\nthe out-of-domain setting.", "AI": {"tldr": "本文介绍了一种名为SEnTRA的新方法，用于检测未声明为LLM生成的文本。该方法基于Transformer编码器，经实验验证，在跨领域检测上优于现有方法。", "motivation": "随着LLM能力的提升和应用的广泛，其被滥用的可能性也在增加。本文旨在解决未明确声明的LLM生成文本的检测问题，尤其是在跨领域场景下。", "method": "本文提出了一种新的、通用的、基于监督学习的LLM文本检测器SEnTRA。SEnTRA基于Transformer编码器，利用选择的下一个token概率序列，并通过在大量未标记数据上的对比预训练来提升性能。", "result": "实验在跨越24个文本领域的三个流行公开数据集上进行，结果表明SEnTRA作为一种通用分类器，在跨领域设置下显著超越了流行的基线模型。", "conclusion": "SEnTRA能有效地检测未声明的LLM生成文本，并展示了其在不同类型文本数据上的通用性和优越性。"}}
{"id": "2509.12277", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12277", "abs": "https://arxiv.org/abs/2509.12277", "authors": ["Mehdi Yousefzadeh", "Parsa Esfahanian", "Sara Rashidifar", "Hossein Salahshoor Gavalan", "Negar Sadat Rafiee Tabatabaee", "Saeid Gorgin", "Dara Rahmati", "Maryam Daneshpazhooh"], "title": "GraphDerm: Fusing Imaging, Physical Scale, and Metadata in a Population-Graph Classifier for Dermoscopic Lesions", "comment": null, "summary": "Introduction. Dermoscopy aids melanoma triage, yet image-only AI often\nignores patient metadata (age, sex, site) and the physical scale needed for\ngeometric analysis. We present GraphDerm, a population-graph framework that\nfuses imaging, millimeter-scale calibration, and metadata for multiclass\ndermoscopic classification, to the best of our knowledge the first ISIC-scale\napplication of GNNs to dermoscopy. Methods. We curate ISIC 2018/2019,\nsynthesize ruler-embedded images with exact masks, and train U-Nets\n(SE-ResNet-18) for lesion and ruler segmentation. Pixels-per-millimeter are\nregressed from the ruler-mask two-point correlation via a lightweight 1D-CNN.\nFrom lesion masks we compute real-scale descriptors (area, perimeter, radius of\ngyration). Node features use EfficientNet-B3; edges encode metadata/geometry\nsimilarity (fully weighted or thresholded). A spectral GNN performs\nsemi-supervised node classification; an image-only ANN is the baseline.\nResults. Ruler and lesion segmentation reach Dice 0.904 and 0.908; scale\nregression attains MAE 1.5 px (RMSE 6.6). The graph attains AUC 0.9812, with a\nthresholded variant using about 25% of edges preserving AUC 0.9788 (vs. 0.9440\nfor the image-only baseline); per-class AUCs typically fall in the 0.97-0.99\nrange. Conclusion. Unifying calibrated scale, lesion geometry, and metadata in\na population graph yields substantial gains over image-only pipelines on\nISIC-2019. Sparser graphs retain near-optimal accuracy, suggesting efficient\ndeployment. Scale-aware, graph-based AI is a promising direction for\ndermoscopic decision support; future work will refine learned edge semantics\nand evaluate on broader curated benchmarks.", "AI": {"tldr": "本研究提出了一种集合图像、毫米尺度校准及患者元数据的皮肤镜分类框架GraphDerm，通过结合GNN来提高皮肤镜图像分类的准确性，结果显著优于仅使用图像信息的方法。", "motivation": "现有的基于AI的皮肤镜图像识别技术往往忽略了患者元数据（如年龄、性别、部位）以及对于几何分析所需的实际尺度。GraphDerm则融合了这些信息，试图提高皮肤镜图像分类的准确性。这是第一次在ISIC规模上应用GNN用于皮肤镜检查。", "method": "GraphDerm，这是一种集合了成像、毫米级校准和患者元数据的多类皮肤镜分类的群体图框架。具体来说，研究团队整理了ISIC 2018/2019数据集，合成嵌入尺子的图像以及精确的掩模，使用U-Net（SE-ResNet-18）进行病灶和尺子的分割。通过轻量级的1D-CNN，从尺子掩模的两点相关性中回归像素每毫米。从病灶掩模中计算出真实规模的描述子（面积、周长、惯性半径）。节点特征采用EfficientNet-B3；边代表元数据/几何相似性（完全加权或阈值化）。通过谱图神经网络进行半监督节点分类；提出了一个基于图像的ANN作为基准模型。", "result": "尺子和病灶分割的Dice系数分别为0.904和0.908；尺度回归的平均绝对误差为1.5像素（RMSE为6.6）。图结构模型的AUC为0.9812；使用约25%边的阈值化变体保持AUC为0.9788（对仅基于图像的基准模型为0.9440）。每类的AUC大多在0.97至0.99之间。", "conclusion": "将校准尺度、病变几何形状和元数据统一到一个群体图中，在基于ISIC-2019的多类皮肤镜分类上比仅基于图像的方法取得了显著成效。稀疏图可以保持接近最优的精度，表明这种方法具有高效的部署潜力。"}}
{"id": "2509.12405", "categories": ["cs.CL", "68T50 (Primary) 68T45 (Secondary)", "I.2.7; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.12405", "abs": "https://arxiv.org/abs/2509.12405", "authors": ["Wen-wai Yim", "Asma Ben Abacha", "Zixuan Yu", "Robert Doerning", "Fei Xia", "Meliha Yetisgen"], "title": "MORQA: Benchmarking Evaluation Metrics for Medical Open-Ended Question Answering", "comment": "9 pages, 8 tables", "summary": "Evaluating natural language generation (NLG) systems in the medical domain\npresents unique challenges due to the critical demands for accuracy, relevance,\nand domain-specific expertise. Traditional automatic evaluation metrics, such\nas BLEU, ROUGE, and BERTScore, often fall short in distinguishing between\nhigh-quality outputs, especially given the open-ended nature of medical\nquestion answering (QA) tasks where multiple valid responses may exist. In this\nwork, we introduce MORQA (Medical Open-Response QA), a new multilingual\nbenchmark designed to assess the effectiveness of NLG evaluation metrics across\nthree medical visual and text-based QA datasets in English and Chinese. Unlike\nprior resources, our datasets feature 2-4+ gold-standard answers authored by\nmedical professionals, along with expert human ratings for three English and\nChinese subsets. We benchmark both traditional metrics and large language model\n(LLM)-based evaluators, such as GPT-4 and Gemini, finding that LLM-based\napproaches significantly outperform traditional metrics in correlating with\nexpert judgments. We further analyze factors driving this improvement,\nincluding LLMs' sensitivity to semantic nuances and robustness to variability\namong reference answers. Our results provide the first comprehensive,\nmultilingual qualitative study of NLG evaluation in the medical domain,\nhighlighting the need for human-aligned evaluation methods. All datasets and\nannotations will be publicly released to support future research.", "AI": {"tldr": "本文介绍了MORQA，一个专门为医疗领域设计的多语言基准测试，旨在评估自然语言生成系统的输出质量。研究发现，大语言模型在评价与专家判断的一致性上优于传统指标，强调了符合人类标准的评估方法的重要性。", "motivation": "自然语言生成（NLG）系统在医疗领域的评估面临独特的挑战，主要是因为对准确性和领域专业知识的要求极高。传统的自动评估指标，在区分高质量输出时常常无法满足要求，尤其是在医疗问答任务中，由于答案的开放性使得多份有效回答可能并存。", "method": "本文介绍了MORQA（Medical Open-Response QA），一个新设计的多语言基准测试，用于评估自然语言生成（NLG）评估指标在三个医疗视觉和文本问答数据集上的有效性，这些数据集包含英语和中文。与其他资源不同，这些数据集包括由医疗专家编写的不同数量的黄金标准答案，以及三个英语和中文子集的专家人工评分。", "result": "本文对传统指标和大语言模型（LLM）评估者（如GPT-4和Gemini）进行了基准测试，结果表明，基于LLM的方法在与专家评价关联方面显著优于传统指标。", "conclusion": "研究结果提供了第一个全面的多语言定性研究，揭示了医疗领域NLG评估中需要符合人类标准的评估方法。所有数据集和标注都将公开发布以支持未来的研究。"}}
{"id": "2509.12278", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12278", "abs": "https://arxiv.org/abs/2509.12278", "authors": ["Wanru Zhuang", "Wenbo Li", "Zhibin Lan", "Xu Han", "Peng Li", "Jinsong Su"], "title": "PATIMT-Bench: A Multi-Scenario Benchmark for Position-Aware Text Image Machine Translation in Large Vision-Language Models", "comment": null, "summary": "Text Image Machine Translation (TIMT) aims to translate texts embedded within\nan image into another language. Current TIMT studies primarily focus on\nproviding translations for all the text within an image, while neglecting to\nprovide bounding boxes and covering limited scenarios. In this work, we extend\ntraditional TIMT into position-aware TIMT (PATIMT), aiming to support\nfine-grained and layoutpreserving translation, which holds great practical\nvalue but remains largely unexplored. This task comprises two key sub-tasks:\nregionspecific translation and full-image translation with grounding. To\nsupport existing models on PATIMT and conduct fair evaluation, we construct the\nPATIMT benchmark (PATIMTBench), which consists of 10 diverse real-world\nscenarios. Specifically, we introduce an Adaptive Image OCR Refinement\nPipeline, which adaptively selects appropriate OCR tools based on scenario and\nrefines the results of text-rich images. To ensure evaluation reliability, we\nfurther construct a test set, which contains 1,200 high-quality instances\nmanually annotated and reviewed by human experts. After fine-tuning on our\ndata, compact Large Vision-Language Models (LVLMs) achieve state-of-the-art\nperformance on both sub-tasks. Experimental results also highlight the\nscalability and generalizability of our training data", "AI": {"tldr": "本文提出了位置感知文本图像翻译（PATIMT），通过构建PATIMTBench数据集和自适应OCR精炼管道，支持了更细粒度和布局保留的图像中文字翻译任务，LVLMs在任务上达到了最好效果。", "motivation": "传统的文本图像翻译研究主要集中在对图像中所有文本的翻译，忽视了边界框的提供及场景多样性的覆盖。而本研究进一步提出位置感知文本图像翻译，旨在提供细粒度和布局保留的翻译，具有重要的实用价值。", "method": "该研究提出了位置感知文本图像翻译（PATIMT），包括区域特定翻译和带定位线索的全图像翻译两个关键子任务。为了支持模型评估，构建了PATIMTBench数据集和自适应图像OCR精炼管道，后者可以根据不同场景自适应选择合适的OCR工具并优化图像中的文字识别结果。", "result": "通过对数据进行微调，紧凑的大视觉-语言模型（LVLMs）在两个子任务上都达到了最先进的性能。实验结果还表明了该研究训练数据的可扩展性和通用性。", "conclusion": "本研究提出的位置感知文本图像翻译及由此构建的数据集与方法为未来的文本图像翻译研究提供了重要参考，并展示了视觉-语言模型在这一任务上的潜力。"}}
{"id": "2509.12440", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12440", "abs": "https://arxiv.org/abs/2509.12440", "authors": ["Jiayi He", "Yangmin Huang", "Qianyun Du", "Xiangying Zhou", "Zhiyang He", "Jiaxue Hu", "Xiaodong Tao", "Lixian Lai"], "title": "MedFact: Benchmarking the Fact-Checking Capabilities of Large Language Models on Chinese Medical Texts", "comment": null, "summary": "The increasing deployment of Large Language Models (LLMs) in healthcare\nnecessitates a rigorous evaluation of their factual reliability. However,\nexisting benchmarks are often limited by narrow domains of data, failing to\ncapture the complexity of real-world medical information. To address this\ncritical gap, we introduce MedFact, a new and challenging benchmark for Chinese\nmedical fact-checking. MedFact comprises 2,116 expert-annotated instances\ncurated from diverse real-world texts, spanning 13 medical specialties, 8\nfine-grained error types, 4 writing styles, and multiple difficulty levels. Its\nconstruction employs a hybrid AI-human framework where iterative expert\nfeedback refines an AI-driven, multi-criteria filtering process, ensuring both\nhigh data quality and difficulty. We conduct a comprehensive evaluation of 20\nleading LLMs, benchmarking their performance on veracity classification and\nerror localization against a human expert baseline. Our results reveal that\nwhile models can often determine if a text contains an error, precisely\nlocalizing it remains a substantial challenge, with even top-performing models\nfalling short of human performance. Furthermore, our analysis uncovers a\nfrequent ``over-criticism'' phenomenon, a tendency for models to misidentify\ncorrect information as erroneous, which is exacerbated by advanced reasoning\ntechniques such as multi-agent collaboration and inference-time scaling. By\nhighlighting these critical challenges for deploying LLMs in medical\napplications, MedFact provides a robust resource to drive the development of\nmore factually reliable and medically aware models.", "AI": {"tldr": "本文介绍了MedFact，一个新的中文医疗事实核查基准测试，用于评估大语言模型在医疗领域的事实可靠性，并揭示了即使顶级模型在错误定位上也难以达到人类水平，并且存在过度批评正确信息的现象。", "motivation": "现有的基准测试往往局限于狭窄的数据领域，无法捕捉到实际医疗信息的复杂性。为了弥补这一关键差距，本文提出了MedFact，一个针对中文医疗事实核查的新且具有挑战性的基准测试。", "method": "MedFact由2116个专家注释的实例组成，这些实例来自多样化的实际文本，横跨13个医疗专科、8种细粒度错误类型、4种写作风格和多个难度级别。其构造采用混合的AI-human框架，专家反馈迭代地精炼AI驱动的多标准筛选过程。", "result": "对20个领先大语言模型的综合评估表明，模型通常能判断文本中是否包含错误，但在准确定位错误方面仍然面临较大挑战，顶级模型的表现也未达到人类专家的水平。另外，还发现模型存在过度批评正确信息的现象。", "conclusion": "通过强调在医疗应用中部署大语言模型所面临的这些关键挑战，MedFact提供了一个强大的资源，以推动更可靠和医学意识更强的模型的发展。"}}
{"id": "2509.12279", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12279", "abs": "https://arxiv.org/abs/2509.12279", "authors": ["He Gao", "Baoxiang Huang", "Milena Radenkovic", "Borui Li", "Ge Chen"], "title": "Domain Adaptive SAR Wake Detection: Leveraging Similarity Filtering and Memory Guidance", "comment": null, "summary": "Synthetic Aperture Radar (SAR), with its all-weather and wide-area\nobservation capabilities, serves as a crucial tool for wake detection. However,\ndue to its complex imaging mechanism, wake features in SAR images often appear\nabstract and noisy, posing challenges for accurate annotation. In contrast,\noptical images provide more distinct visual cues, but models trained on optical\ndata suffer from performance degradation when applied to SAR images due to\ndomain shift. To address this cross-modal domain adaptation challenge, we\npropose a Similarity-Guided and Memory-Guided Domain Adaptation (termed\nSimMemDA) framework for unsupervised domain adaptive ship wake detection via\ninstance-level feature similarity filtering and feature memory guidance.\nSpecifically, to alleviate the visual discrepancy between optical and SAR\nimages, we first utilize WakeGAN to perform style transfer on optical images,\ngenerating pseudo-images close to the SAR style. Then, instance-level feature\nsimilarity filtering mechanism is designed to identify and prioritize source\nsamples with target-like distributions, minimizing negative transfer.\nMeanwhile, a Feature-Confidence Memory Bank combined with a K-nearest neighbor\nconfidence-weighted fusion strategy is introduced to dynamically calibrate\npseudo-labels in the target domain, improving the reliability and stability of\npseudo-labels. Finally, the framework further enhances generalization through\nregion-mixed training, strategically combining source annotations with\ncalibrated target pseudo-labels. Experimental results demonstrate that the\nproposed SimMemDA method can improve the accuracy and robustness of cross-modal\nship wake detection tasks, validating the effectiveness and feasibility of the\nproposed method.", "AI": {"tldr": "提出一种新的方法SimMemDA，使用WakeGAN生成与SAR图像风格相近的伪图像，并结合实例级特征相似性过滤机制和特征-置信度记忆库来提高对SAR图像中船尾迹检测的准确性。", "motivation": "由于SAR图像具有的复杂成像机制，导致SAR图像中的尾迹特征经常显得抽象且噪点较多，给SAR图像中的尾迹特征注释带来挑战。光学图像提供更多的视觉线索，但因为领域迁移，光谱图像训练的模型在应用于SAR图像时表现出较差的性能。本文希望解决跨模态领域适应挑战，提高跨模态尾迹检测任务的准确性和鲁棒性。", "method": "提出名为SimMemDA的跨模态领域适应框架，该框架通过实例级特征相似性过滤和特征记忆引导来解决无监督领域自适应中的船尾迹检测问题。首先，采用WakeGAN对光学图像进行风格转换，生成与SAR图像风格相近的伪图像。其次，设计实例级特征相似性过滤机制，以识别并优先选择具有目标域类似分布的源域样本，从而减少负面迁移。同时，引入特征-置信度记忆库，结合K近邻置信度加权融合策略，动态校准目标域中的伪标签，提高伪标签的可靠性和稳定性。最后，通过区域混合训练进一步增强泛化能力，策略性地结合源域注释与校准后的目标伪标签。", "result": "实验结果表明所提SimMemDA方法能提高跨模态船尾迹检测任务的准确性和鲁棒性。", "conclusion": "实验证明，提出的SimMemDA方法能提高跨模态尾迹检测任务的准确性和鲁棒性，验证了其有效性和可行性。"}}
{"id": "2509.12451", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12451", "abs": "https://arxiv.org/abs/2509.12451", "authors": ["Wonbin Kweon", "SeongKu Kang", "Runchu Tian", "Pengcheng Jiang", "Jiawei Han", "Hwanjo Yu"], "title": "Topic Coverage-based Demonstration Retrieval for In-Context Learning", "comment": "EMNLP 2025 Main", "summary": "The effectiveness of in-context learning relies heavily on selecting\ndemonstrations that provide all the necessary information for a given test\ninput. To achieve this, it is crucial to identify and cover fine-grained\nknowledge requirements. However, prior methods often retrieve demonstrations\nbased solely on embedding similarity or generation probability, resulting in\nirrelevant or redundant examples. In this paper, we propose TopicK, a topic\ncoverage-based retrieval framework that selects demonstrations to\ncomprehensively cover topic-level knowledge relevant to both the test input and\nthe model. Specifically, TopicK estimates the topics required by the input and\nassesses the model's knowledge on those topics. TopicK then iteratively selects\ndemonstrations that introduce previously uncovered required topics, in which\nthe model exhibits low topical knowledge. We validate the effectiveness of\nTopicK through extensive experiments across various datasets and both open- and\nclosed-source LLMs. Our source code is available at\nhttps://github.com/WonbinKweon/TopicK_EMNLP2025.", "AI": {"tldr": "The paper introduces TopicK, a method for in-context learning that retrieves demonstrations by focusing on topic-level knowledge required for a given input, significantly improving on previous techniques.", "motivation": "The motivation is to improve upon prior methods that retrieve demonstrations based solely on embedding similarity or generation probability, which often lead to irrelevant or redundant examples. This is crucial for effective in-context learning by ensuring comprehensive coverage of topic-level knowledge relevant to both the test input and the model.", "method": "TopicK, a topic coverage-based retrieval framework, is proposed. This framework estimates the topics required by the input and assesses the model's knowledge on those topics, iteratively selecting demonstrations that introduce previously uncovered required topics where the model exhibits low topical knowledge.", "result": "The effectiveness of TopicK is validated through extensive experiments across various datasets and both open- and closed-source LLMs.", "conclusion": "TopicK demonstrates improved demonstration retrieval for in-context learning by focusing on required topic-level knowledge, leading to better performance across tested models and datasets."}}
{"id": "2509.12329", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12329", "abs": "https://arxiv.org/abs/2509.12329", "authors": ["Shengjie Kris Liu", "Siqin Wang", "Lu Zhang"], "title": "Uncertainty-Aware Hourly Air Temperature Mapping at 2 km Resolution via Physics-Guided Deep Learning", "comment": null, "summary": "Near-surface air temperature is a key physical property of the Earth's\nsurface. Although weather stations offer continuous monitoring and satellites\nprovide broad spatial coverage, no single data source offers seamless data in a\nspatiotemporal fashion. Here, we propose a data-driven, physics-guided deep\nlearning approach to generate hourly air temperature data at 2 km resolution\nover the contiguous United States. The approach, called Amplifier\nAir-Transformer, first reconstructs GOES-16 surface temperature data obscured\nby clouds. It does so through a neural network encoded with the annual\ntemperature cycle, incorporating a linear term to amplify ERA5 temperature\nvalues at finer scales and convolutional layers to capture spatiotemporal\nvariations. Then, another neural network transforms the reconstructed surface\ntemperature into air temperature by leveraging its latent relationship with key\nEarth surface properties. The approach is further enhanced with predictive\nuncertainty estimation through deep ensemble learning to improve reliability.\nThe proposed approach is built and tested on 77.7 billion surface temperature\npixels and 155 million air temperature records from weather stations across the\ncontiguous United States (2018-2024), achieving hourly air temperature mapping\naccuracy of 1.93 C in station-based validation. The proposed approach\nstreamlines surface temperature reconstruction and air temperature prediction,\nand it can be extended to other satellite sources for seamless air temperature\nmonitoring at high spatiotemporal resolution. The generated data of this study\ncan be downloaded at https://doi.org/10.5281/zenodo.15252812, and the project\nwebpage can be found at https://skrisliu.com/HourlyAirTemp2kmUSA/.", "AI": {"tldr": "研究团队开发了Amplifier Air-Transformer方法，利用深度学习和物理引导模型，实现美国本土每小时2公里分辨率的气温数据生成，该方法能够通过神经网络重构地表温度并将之转化为精确的气温数据，同时提高了预测的可靠性。", "motivation": "尽管地面气象站提供连续监测数据，卫星能提供广泛的空间覆盖，但还没有任何一个单一的数据源能够以时空无缝的方式提供数据。因此，研究者提出了一种新的方法来填补这一空白。", "method": "提出了一种基于数据驱动和物理引导的深度学习方法来生成美国本土每小时2公里分辨率的气温数据。该方法，称为Amplifier Air-Transformer，首先通过神经网络重构被云层遮挡的GOES-16地表温度数据，该网络整合了年温度周期并引入线性项来提升ERA5温度值在更细尺度的精度。其次，通过另一个神经网络利用地表属性将重构的地表温度转化为气温。该方法还通过深度集成学习估计预测不确定性以提高可靠性。", "result": "该方法在基于777亿像素的地表温度数据和1.55亿个来自气象站的气温记录（2018-2024）的测试中，实现了每小时气温映射精度为1.93°C。", "conclusion": "该方法不仅简化了地表温度重构过程和气温预测，还可扩展到其他卫星数据源，以实现高时空分辨率的无缝气温监测。"}}
{"id": "2509.12459", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12459", "abs": "https://arxiv.org/abs/2509.12459", "authors": ["Suvojit Acharjee", "Utathya Aich", "Asfak Ali"], "title": "Does Language Model Understand Language?", "comment": null, "summary": "Despite advances in natural language generation and understanding, LM still\nstruggle with fine grained linguistic phenomena such as tense, negation, voice,\nand modality which are the elements central to effective human communication.\nIn the context of the United Nations SDG 4, where linguistic clarity is\ncritical, the deployment of LMs in educational technologies demands careful\nscrutiny. As LMs are increasingly powering applications like tutoring systems,\nautomated grading, and translation, their alignment with human linguistic\ninterpretation becomes essential for effective learning. In this study, we\nconduct a evaluation of SOTA language models across these challenging contexts\nin both English and Bengali. To ensure a structured assessment, we introduce a\nnew Route for Evaluation of Cognitive Inference in Systematic Environments\nguidelines. Our proposed LUCID dataset, composed of carefully crafted sentence\npairs in English and Bengali, specifically challenges these models on critical\naspects of language comprehension, including negation, tense, voice variations.\nWe assess the performance of SOTA models including MISTRAL-SABA-24B,\nLLaMA-4-Scout-17B, LLaMA-3.3-70B, Gemma2-9B, and Compound-Beta using standard\nmetrics like Pearson correlation, Spearman correlation, and Mean Absolute\nError, as well as novel, linguistically inspired metric the HCE accuracy. The\nHCE accuracy measures how often model predictions fall within one standard\ndeviation of the mean human rating, thus capturing human like tolerance for\nvariability in language interpretation. Our findings highlight Compound-Beta as\nthe most balanced model, consistently achieving high correlations and low MAEs\nacross diverse language conditions. It records the highest Pearson correlation\nin English and demonstrates robust performance on mixed-language data,\nindicating a strong alignment with human judgments in cross lingual scenarios.", "AI": {"tldr": "本研究评估了最先进的语言模型在英语和孟加拉语细节复杂语言现象理解上的能力，引入了新的评估指南和HCE准确性指标，并发现Compound-Beta是最稳定的模型，其在跨语言表现上具有与人类判断高度一致的特点。", "motivation": "尽管自然语言生成和理解方面取得了进展，但语言模型在处理诸如时态、否定、语态和语气等细微语言现象方面仍存在困难，这些现象对于有效的沟通至关重要。本研究的动机在于，为了在教育技术中部署语言模型（例如辅导系统、自动评分和翻译应用），需要严格评估这些模型与人类语言理解的一致性，特别是在联合国可持续发展目标4（SDG 4）所强调的语言清晰度方面。", "method": "本文介绍了一个新的评估认知推理系统环境的指南，使用了一个名为LUCID的数据集来评估最先进的语言模型在处理细节复杂的语言现象（如时态、否定、语态变化）方面的表现。评估使用的标准包括皮尔森相关系数、斯皮尔曼等级相关系数、平均绝对误差以及一个新的语料库启发度准确性（HCE准确性）指标，该指标反映模型判断在接近人类打分平均值一个标准差范围内的频率。", "result": "研究结果强调Compound-Beta为性能最均衡的模型，它在各类语言条件下展示了高相关性和低MAE。特别地，该模型在英语中表现出最高的皮尔森相关系数，并在混合语言数据上表现出稳定的性能，表明其在跨语言情况中与人类判断高度一致。", "conclusion": "研究结论表明，Compound-Beta在英语和混合语言环境下表现出最高相关性和最低平均绝对误差，显示出与人类判断的高度一致性，尤其是在跨语言场景中。这表明Compound-Beta是解决教育技术领域语言理解挑战的一种前景广阔的语言模型。"}}
{"id": "2509.12353", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12353", "abs": "https://arxiv.org/abs/2509.12353", "authors": ["Anthony Miyaguchi", "Chandrasekaran Maruthaiyannan", "Charles R. Clark"], "title": "DS@GT AnimalCLEF: Triplet Learning over ViT Manifolds with Nearest Neighbor Classification for Animal Re-identification", "comment": "CLEF 2025 working notes", "summary": "This paper details the DS@GT team's entry for the AnimalCLEF 2025\nre-identification challenge. Our key finding is that the effectiveness of\npost-hoc metric learning is highly contingent on the initial quality and\ndomain-specificity of the backbone embeddings. We compare a general-purpose\nmodel (DINOv2) with a domain-specific model (MegaDescriptor) as a backbone. A\nK-Nearest Neighbor classifier with robust thresholding then identifies known\nindividuals or flags new ones. While a triplet-learning projection head\nimproved the performance of the specialized MegaDescriptor model by 0.13\npoints, it yielded minimal gains (0.03) for the general-purpose DINOv2 on\naveraged BAKS and BAUS. We demonstrate that the general-purpose manifold is\nmore difficult to reshape for fine-grained tasks, as evidenced by stagnant\nvalidation loss and qualitative visualizations. This work highlights the\ncritical limitations of refining general-purpose features for specialized,\nlimited-data re-ID tasks and underscores the importance of domain-specific\npre-training. The implementation for this work is publicly available at\ngithub.com/dsgt-arc/animalclef-2025.", "AI": {"tldr": "研究显示，度量学习的效果高度依赖于初始骨干嵌入的质量和领域特定性，特定模型的表现优于通用模型。", "motivation": "探索度量学习的有效性如何依赖于初始骨干嵌入的质量和领域特定性，特别是在动物再识别任务中。", "method": "对比了通用模型DINOv2和领域特定模型MegaDescriptor作为骨干模型的效果，使用带鲁棒阈值处理的K-最近邻分类器来识别个体或标记新个体。", "result": "对于领域特定模型MegaDescriptor，三重学习投影头提升了0.13点性能，而对于通用模型DINOv2，性能提升仅为0.03点。", "conclusion": "展示了调整通用特征以适应特定的有限数据再识别任务的局限性，并强调领域特定预训练的重要性。"}}
{"id": "2509.12476", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12476", "abs": "https://arxiv.org/abs/2509.12476", "authors": ["Sumanta Bhattacharyya", "Sara Riaz", "Pedram Rooshenas"], "title": "Audited Reasoning Refinement: Fine-Tuning Language Models via LLM-Guided Step-Wise Evaluation and Correction", "comment": null, "summary": "Training a task-specific small reasoning model is challenging when direct\nhuman supervision or high-quality labels are scarce. However, LLMs with\nreasoning capabilities produce abundant intermediate reasoning traces that can\nbe systematically refined to create effective supervision signals. We propose\nReason-Refine-then-Align (R2tA), which turns refined model rationales into\nsupervision for training task-specific reasoning models. Our method generates\ninitial reasoning and responses from an open-source base model on task-specific\ninputs, then refines these traces, fixing hallucinations and inconsistencies,\nto form a high-fidelity dataset. We perform a two-stage alignment, supervised\nfine-tuning (SFT), followed by direct preference optimization (DPO) to\ncalibrate the model's intermediate reasoning with human-validated conceptual\npreferences and then condition the final output on that aligned reasoning. As a\ncase study, we apply R2tA to evaluate extended entity relationship diagrams\n(EERDs) in database system design, a structurally complex task where\nprompt-only methods miss or hallucinate errors. We curated a dataset of 600\nEERD variants (train/test split of 450/150, respectively) with induced mistakes\nspanning 11 categories. Empirical evaluation suggests R2tA provides a\npractical, cost-effective path to scalable LLM adaptation in data-scarce\ndomains, enabling reproducible AI tools for education and beyond.", "AI": {"tldr": "我们提出了Reason-Refine-then-Align (R2tA) 的方法，通过精炼大型语言模型（LLMs）产生的推理痕迹，形成高质量的数据集并进行两阶段的校正，以训练在数据稀缺领域中具有推理能力的小型模型，解决其在特定任务中的挑战，使得能够生成更加准确和一致的推理输出。", "motivation": "直接的人类监督或高质量标签稀缺时，训练特定任务的小型推理模型是具有挑战性的。大型语言模型（LLMs）具备推理能力，产生丰富的中间推理痕迹，可以系统地精炼为有效的监督信号。为此我们提出了一种基于精炼模型推理以作为训练任务特定推理模型的监督信号的方法。", "method": "我们提出了一种名为Reason-Refine-then-Align (R2tA) 的方法，首先从一个开源基础模型中生成特定任务的初始推理和响应，然后通过修正幻觉和不一致性来精炼这些推理，从而形成一个高质量的数据集。接着，进行两阶段的对齐，首先是监督微调（SFT），然后是直接偏好优化（DPO），以调整模型的中间推理与经过人工验证的概念偏好相符，最后以这个对齐后的推理为条件进行输出。", "result": "我们使用R2tA进行了一个案例研究，将其应用于数据库系统设计中评估扩展实体关系图（EERDs），这是一个结构上较为复杂的任务，而单纯使用提示的方法往往会错过或产生错误。实验结果表明，R2tA提供了一种实用且成本有效的路径，可以在数据稀缺领域内实现大规模的LLM适应，能够创造出可重复使用的AI工具，适用于教育等众多领域。", "conclusion": "R2tA为数据稀缺领域的LLM适应提供了一种实际可行、成本效益高的解决方案，特别是对于复杂任务，如数据库系统的设计评估，R2tA能够训练出任务特定的高质量推理模型，并且可以生成教育、科研等领域的可复用AI工具。"}}
{"id": "2509.12380", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12380", "abs": "https://arxiv.org/abs/2509.12380", "authors": ["Florian Zager", "Hamza A. A. Gardi"], "title": "GhostNetV3-Small: A Tailored Architecture and Comparative Study of Distillation Strategies for Tiny Images", "comment": null, "summary": "Deep neural networks have achieved remarkable success across a range of\ntasks, however their computational demands often make them unsuitable for\ndeployment on resource-constrained edge devices. This paper explores strategies\nfor compressing and adapting models to enable efficient inference in such\nenvironments. We focus on GhostNetV3, a state-of-the-art architecture for\nmobile applications, and propose GhostNetV3-Small, a modified variant designed\nto perform better on low-resolution inputs such as those in the CIFAR-10\ndataset. In addition to architectural adaptation, we provide a comparative\nevaluation of knowledge distillation techniques, including traditional\nknowledge distillation, teacher assistants, and teacher ensembles. Experimental\nresults show that GhostNetV3-Small significantly outperforms the original\nGhostNetV3 on CIFAR-10, achieving an accuracy of 93.94%. Contrary to\nexpectations, all examined distillation strategies led to reduced accuracy\ncompared to baseline training. These findings indicate that architectural\nadaptation can be more impactful than distillation in small-scale image\nclassification tasks, highlighting the need for further research on effective\nmodel design and advanced distillation techniques for low-resolution domains.", "AI": {"tldr": "论文探索了在资源受限的边缘设备上部署深度神经网络的方法，通过优化GhostNetV3的架构并对比多种知识蒸馏技术，发现架构优化比蒸馏更有助于提升性能。", "motivation": "深度神经网络虽然在众多任务中取得显著成功，但由于计算需求高，不适合部署在资源受限的边缘设备上。论文希望通过压缩和适配模型，使得这些模型可以在这样的环境中更加有效地运行。", "method": "本论文研究了模型压缩和适配策略，旨在实现在资源受限的边缘设备上的高效推理。文章重点介绍了GhostNetV3这一适用于移动应用的先进架构，并提出了一种新的版本GhostNetV3-Small，该版本针对低分辨率输入进行了优化。\n此外，论文还比较评估了几种知识蒸馏策略，包括传统的知识蒸馏、教师助手以及教师集策略。", "result": "实验结果显示，GhostNetV3-Small在CIFAR-10数据集上显著优于原版GhostNetV3，达到了93.94%的准确率。所有测试的知识蒸馏策略均未超过基线训练的表现。", "conclusion": "研究结果表明，在小规模图像分类任务上，架构适应比知识蒸馏更能提升性能，这强调了对有效模型设计及针对低分辨率域的高级知识蒸馏技术的需求。"}}
{"id": "2509.12508", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.12508", "abs": "https://arxiv.org/abs/2509.12508", "authors": ["Keyu An", "Yanni Chen", "Chong Deng", "Changfeng Gao", "Zhifu Gao", "Bo Gong", "Xiangang Li", "Yabin Li", "Xiang Lv", "Yunjie Ji", "Yiheng Jiang", "Bin Ma", "Haoneng Luo", "Chongjia Ni", "Zexu Pan", "Yiping Peng", "Zhendong Peng", "Peiyao Wang", "Hao Wang", "Wen Wang", "Wupeng Wang", "Biao Tian", "Zhentao Tan", "Nan Yang", "Bin Yuan", "Jieping Ye", "Jixing Yu", "Qinglin Zhang", "Kun Zou", "Han Zhao", "Shengkui Zhao", "Jingren Zhou"], "title": "FunAudio-ASR Technical Report", "comment": null, "summary": "In recent years, automatic speech recognition (ASR) has witnessed\ntransformative advancements driven by three complementary paradigms: data\nscaling, model size scaling, and deep integration with large language models\n(LLMs). However, LLMs are prone to hallucination, which can significantly\ndegrade user experience in real-world ASR applications. In this paper, we\npresent FunAudio-ASR, a large-scale, LLM-based ASR system that synergistically\ncombines massive data, large model capacity, LLM integration, and reinforcement\nlearning to achieve state-of-the-art performance across diverse and complex\nspeech recognition scenarios. Moreover, FunAudio-ASR is specifically optimized\nfor practical deployment, with enhancements in streaming capability, noise\nrobustness, code-switching, hotword customization, and satisfying other\nreal-world application requirements. Experimental results show that while most\nLLM-based ASR systems achieve strong performance on open-source benchmarks,\nthey often underperform on real industry evaluation sets. Thanks to\nproduction-oriented optimizations, FunAudio-ASR achieves SOTA performance on\nreal application datasets, demonstrating its effectiveness and robustness in\npractical settings.", "AI": {"tldr": "FunAudio-ASR是结合大规模数据、模型集成和强化学习的先进自动语音识别系统，针对实际应用需求进行优化，能够在复杂和多样的语音识别场景中实现最先进性能。实验结果显示其在真实场景中的表现优秀。", "motivation": "自动语音识别（ASR）尽管受益于数据和模型规模的扩展以及与大型语言模型的集成取得了显著进步，但大型语言模型存在幻觉问题，这在实际应用中会显著降低用户体验。本论文旨在通过FunAudio-ASR解决这些问题。", "method": "本文介绍了一个名为FunAudio-ASR的大型LLM（Large Language Model）为基础的自动语音识别（ASR）系统。该系统综合了大规模数据、大型模型容量、与大型语言模型的深度集成以及强化学习，以实现各种复杂语音识别场景中的最先进性能。", "result": "实验结果表明，尽管许多基于LLM的ASR系统在开源基准上的性能较强，但在实际工业评估集上的表现却不如预期。而由于以生产为导向的优化，FunAudio-ASR在实际应用场景的数据集上达到了最先进性能，并展示了其实用和鲁棒的表现。", "conclusion": "论文结论是FunAudio-ASR通过对流媒体能力、噪声鲁棒性、代码切换、关键词定制等的优化，不仅提升了解决方案的实际应用效果，还展示了在实际设定中的有效性和鲁棒性。"}}
{"id": "2509.12400", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12400", "abs": "https://arxiv.org/abs/2509.12400", "authors": ["Rongkun Zhu", "Kangning Cui", "Wei Tang", "Rui-Feng Wang", "Sarra Alqahtani", "David Lutz", "Fan Yang", "Paul Fine", "Jordan Karubian", "Robert Plemmons", "Jean-Michel Morel", "Victor Pauca", "Miles Silman"], "title": "From Orthomosaics to Raw UAV Imagery: Enhancing Palm Detection and Crown-Center Localization", "comment": "7 pages, 2 figures, 2 tables", "summary": "Accurate mapping of individual trees is essential for ecological monitoring\nand forest management. Orthomosaic imagery from unmanned aerial vehicles (UAVs)\nis widely used, but stitching artifacts and heavy preprocessing limit its\nsuitability for field deployment. This study explores the use of raw UAV\nimagery for palm detection and crown-center localization in tropical forests.\nTwo research questions are addressed: (1) how detection performance varies\nacross orthomosaic and raw imagery, including within-domain and cross-domain\ntransfer, and (2) to what extent crown-center annotations improve localization\naccuracy beyond bounding-box centroids. Using state-of-the-art detectors and\nkeypoint models, we show that raw imagery yields superior performance in\ndeployment-relevant scenarios, while orthomosaics retain value for robust\ncross-domain generalization. Incorporating crown-center annotations in training\nfurther improves localization and provides precise tree positions for\ndownstream ecological analyses. These findings offer practical guidance for\nUAV-based biodiversity and conservation monitoring.", "AI": {"tldr": "研究发现原始无人机图像在部署相关场景中性能更优，而正射影像在跨领域泛化上具有优势，树冠中心注释的使用进一步提升了定位精度，为生态监测和保护管理提供了实际指导。", "motivation": "准确的树木个体定位对于生态监测和森林管理至关重要。虽然无人机生成的正射影像被广泛使用，但其拼接伪影和繁琐的预处理限制了其现场部署的适用性。", "method": "使用了最先进的检测器和关键点模型来比较原始无人机图像与正射影像在目标检测上的性能差异。", "result": "研究探讨了原始无人机图像在热带森林中对棕榈树检测及树冠中心定位的应用。研究解决了两个问题：原始图像与正射影像在目标检测性能上的差异，以及树冠中心注释对定位精度的提升程度。实验发现，原始图像在实际部署中表现更优，而正射影像在跨领域泛化能力上更有优势。树冠中心注释的加入进一步提升了定位精度，为生态学分析提供了精确的树木位置信息。", "conclusion": "这些发现在无人机为基础的生物多样性和保护监测中提供了实用的操作指导。"}}
{"id": "2509.12514", "categories": ["cs.CL", "cs.CE", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12514", "abs": "https://arxiv.org/abs/2509.12514", "authors": ["Chiara Bonfanti", "Michele Colombino", "Giulia Coucourde", "Faeze Memari", "Stefano Pinardi", "Rosa Meo"], "title": "A comparison of pipelines for the translation of a low resource language based on transformers", "comment": "9 pages, 4 figures", "summary": "This work compares three pipelines for training transformer-based neural\nnetworks to produce machine translators for Bambara, a Mand\\`e language spoken\nin Africa by about 14,188,850 people. The first pipeline trains a simple\ntransformer to translate sentences from French into Bambara. The second\nfine-tunes LLaMA3 (3B-8B) instructor models using decoder-only architectures\nfor French-to-Bambara translation. Models from the first two pipelines were\ntrained with different hyperparameter combinations to improve BLEU and chrF\nscores, evaluated on both test sentences and official Bambara benchmarks. The\nthird pipeline uses language distillation with a student-teacher dual neural\nnetwork to integrate Bambara into a pre-trained LaBSE model, which provides\nlanguage-agnostic embeddings. A BERT extension is then applied to LaBSE to\ngenerate translations. All pipelines were tested on Dokotoro (medical) and\nBayelemagaba (mixed domains). Results show that the first pipeline, although\nsimpler, achieves the best translation accuracy (10% BLEU, 21% chrF on\nBayelemagaba), consistent with low-resource translation results. On the Yiri\ndataset, created for this work, it achieves 33.81% BLEU and 41% chrF.\nInstructor-based models perform better on single datasets than on aggregated\ncollections, suggesting they capture dataset-specific patterns more\neffectively.", "AI": {"tldr": "A study comparing three different pipelines for creating machine translation models for Bambara, a language with low resources. The simpler transformer-based pipeline achieves the best translation accuracy despite the complex methods used in other pipelines.", "motivation": "The motivation behind this paper is to investigate and compare different methodologies for developing a machine translator for Bambara, a language with low-resource availability. The study aims to find the most effective translation pipeline for such low-resource languages.", "method": "This work involves three distinct pipelines for creating machine translators for Bambara. The first pipeline involves training a simple transformer model for French to Bambara translation. The second pipeline fine-tunes LLaMA3 instructor models using decoder-only architectures for similar translation tasks. The third pipeline utilizes language distillation with a student-teacher dual neural network approach to integrate Bambara into the pre-trained LaBSE model with BERT extension for generating translations.", "result": "The results reveal that the first pipeline, despite its simplicity, achieves the highest translation accuracy with 10% BLEU and 21% chrF scores on the Bayelemagaba dataset. On the newly created Yiri dataset, it attains 33.81% BLEU and 41% chrF scores. Moreover, the instructor-based models perform better on individual datasets compared to aggregated datasets indicating a bias towards capturing dataset-specific nuances.", "conclusion": "The conclusion drawn from the work shows that even a simpler method might outperform more sophisticated models when it comes to translation accuracy for low-resource languages like Bambara. It also underscores the potential of dataset-specific models over general ones."}}
{"id": "2509.12430", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12430", "abs": "https://arxiv.org/abs/2509.12430", "authors": ["Mayank Patel", "Rahul Jain", "Asim Unmesh", "Karthik Ramani"], "title": "DYNAMO: Dependency-Aware Deep Learning Framework for Articulated Assembly Motion Prediction", "comment": null, "summary": "Understanding the motion of articulated mechanical assemblies from static\ngeometry remains a core challenge in 3D perception and design automation. Prior\nwork on everyday articulated objects such as doors and laptops typically\nassumes simplified kinematic structures or relies on joint annotations.\nHowever, in mechanical assemblies like gears, motion arises from geometric\ncoupling, through meshing teeth or aligned axes, making it difficult for\nexisting methods to reason about relational motion from geometry alone. To\naddress this gap, we introduce MechBench, a benchmark dataset of 693 diverse\nsynthetic gear assemblies with part-wise ground-truth motion trajectories.\nMechBench provides a structured setting to study coupled motion, where part\ndynamics are induced by contact and transmission rather than predefined joints.\nBuilding on this, we propose DYNAMO, a dependency-aware neural model that\npredicts per-part SE(3) motion trajectories directly from segmented CAD point\nclouds. Experiments show that DYNAMO outperforms strong baselines, achieving\naccurate and temporally consistent predictions across varied gear\nconfigurations. Together, MechBench and DYNAMO establish a novel systematic\nframework for data-driven learning of coupled mechanical motion in CAD\nassemblies.", "AI": {"tldr": "文章引入了MechBench，这是一个包含693个多样化合成齿轮装配的基准数据集，用于研究耦合运动。此外，提出了DYNAMO模型，用于从CAD点云中预测每个部件的运动轨迹。", "motivation": "理解和从静态几何图形中推断出连杆机械装配件的运动是3D感知和设计自动化中的一个核心挑战。尽管之前的工作主要针对日常连杆物体，但遇到像齿轮这样运动由几何耦合产生的机械装配件时，现有的方法很难基于几何信息推理出关系运动。", "method": "提出DYNAMO，这是一种依赖性感知的神经模型，可以从分割的CAD点云直接预测每个部件的SE(3)运动轨迹。", "result": "实验表明，DYNAMO在不同的齿轮配置中能达到准确且时间上一致的预测结果，优于其他基线方法。", "conclusion": "MechBench和DYNAMO共同建立了一个新的系统框架，用于从数据中学习CAD装配件中的耦合机械运动。"}}
{"id": "2509.12591", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12591", "abs": "https://arxiv.org/abs/2509.12591", "authors": ["Vijay Govindarajan", "Pratik Patel", "Sahil Tripathi", "Md Azizul Hoque", "Gautam Siddharth Kashyap"], "title": "MAGIC-Enhanced Keyword Prompting for Zero-Shot Audio Captioning with CLIP Models", "comment": "Accepted in The 26th International Conference on Web Information\n  Systems Engineering (WISE), scheduled for 15-17 December 2025 in Marrakech,\n  Morocco", "summary": "Automated Audio Captioning (AAC) generates captions for audio clips but faces\nchallenges due to limited datasets compared to image captioning. To overcome\nthis, we propose the zero-shot AAC system that leverages pre-trained models,\neliminating the need for extensive training. Our approach uses a pre-trained\naudio CLIP model to extract auditory features and generate a structured prompt,\nwhich guides a Large Language Model (LLM) in caption generation. Unlike\ntraditional greedy decoding, our method refines token selection through the\naudio CLIP model, ensuring alignment with the audio content. Experimental\nresults demonstrate a 35% improvement in NLG mean score (from 4.7 to 7.3) using\nMAGIC search with the WavCaps model. The performance is heavily influenced by\nthe audio-text matching model and keyword selection, with optimal results\nachieved using a single keyword prompt, and a 50% performance drop when no\nkeyword list is used.", "AI": {"tldr": "This paper introduces a zero-shot Automated Audio Captioning system that utilizes a pre-trained audio CLIP model and a Large Language Model (LLM) to generate accurate captions, significantly improving performance metrics when compared to previous methods.", "motivation": "The motivation for this research is to tackle the challenges faced by Automated Audio Captioning (AAC) systems due to limited datasets by leveraging pre-trained models to achieve a zero-shot AAC system.", "method": "Our approach involves using a pre-trained audio CLIP model to extract auditory features and generate a structured prompt, which guides a Large Language Model (LLM) in the process of generating captions. This method refines the selection of tokens through the audio CLIP model to ensure alignment with the audio content, differing from the traditional greedy decoding strategy.", "result": "The research demonstrates a 35% improvement in the NLG mean score, rising from 4.7 to 7.3, when using the MAGIC search with the WavCaps model. It was observed that performance was heavily reliant on the audio-text matching model and keyword selection.", "conclusion": "The effectiveness of the proposed zero-shot AAC system has been demonstrated, particularly when using a single keyword prompt, which suggests directions for future improvements in audio captioning technology."}}
{"id": "2509.12442", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12442", "abs": "https://arxiv.org/abs/2509.12442", "authors": ["Rui-Feng Wang", "Mingrui Xu", "Matthew C Bauer", "Iago Beffart Schardong", "Xiaowen Ma", "Kangning Cui"], "title": "Cott-ADNet: Lightweight Real-Time Cotton Boll and Flower Detection Under Field Conditions", "comment": "14 pages, 5 figures, 1 table", "summary": "Cotton is one of the most important natural fiber crops worldwide, yet\nharvesting remains limited by labor-intensive manual picking, low efficiency,\nand yield losses from missing the optimal harvest window. Accurate recognition\nof cotton bolls and their maturity is therefore essential for automation, yield\nestimation, and breeding research. We propose Cott-ADNet, a lightweight\nreal-time detector tailored to cotton boll and flower recognition under complex\nfield conditions. Building on YOLOv11n, Cott-ADNet enhances spatial\nrepresentation and robustness through improved convolutional designs, while\nintroducing two new modules: a NeLU-enhanced Global Attention Mechanism to\nbetter capture weak and low-contrast features, and a Dilated Receptive Field\nSPPF to expand receptive fields for more effective multi-scale context modeling\nat low computational cost. We curate a labeled dataset of 4,966 images, and\nrelease an external validation set of 1,216 field images to support future\nresearch. Experiments show that Cott-ADNet achieves 91.5% Precision, 89.8%\nRecall, 93.3% mAP50, 71.3% mAP, and 90.6% F1-Score with only 7.5 GFLOPs,\nmaintaining stable performance under multi-scale and rotational variations.\nThese results demonstrate Cott-ADNet as an accurate and efficient solution for\nin-field deployment, and thus provide a reliable basis for automated cotton\nharvesting and high-throughput phenotypic analysis. Code and dataset is\navailable at https://github.com/SweefongWong/Cott-ADNet.", "AI": {"tldr": "提出了Cott-ADNet以识别棉花苞和花朵，提高了空间表示和鲁棒性，适用于复杂田间条件。", "motivation": "改善棉花收获自动化、产量估计和育种研究，通过提高棉花苞和花朵识别的准确性。", "method": "基于YOLOv11n，增加NeLU增强全局注意力机制和扩张接收字段SPPF模块，以提高弱低对比特征捕捉和多尺度上下文建模。", "result": "在4,966张图像构成的数据集上实现了91.5%的精度、89.8%的召回率、93.3%的mAP50、71.3%的mAP和90.6%的F1-Score。", "conclusion": "Cott-ADNet作为田间部署的精确和高效的解决方案，为自动化棉花收获和高通量表型分析提供了可靠基础。"}}
{"id": "2509.12603", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12603", "abs": "https://arxiv.org/abs/2509.12603", "authors": ["Mukai Li", "Linfeng Song", "Zhenwen Liang", "Jiahao Xu", "Shansan Gong", "Qi Liu", "Haitao Mi", "Dong Yu"], "title": "EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving", "comment": null, "summary": "Large Language Models (LLMs) have recently advanced the field of Automated\nTheorem Proving (ATP), attaining substantial performance gains through widely\nadopted test-time scaling strategies, notably reflective Chain-of-Thought (CoT)\nreasoning and increased sampling passes. However, they both introduce\nsignificant computational overhead for inference. Moreover, existing cost\nanalyses typically regulate only the number of sampling passes, while\nneglecting the substantial disparities in sampling costs introduced by\ndifferent scaling strategies. In this paper, we systematically compare the\nefficiency of different test-time scaling strategies for ATP models and\ndemonstrate the inefficiency of the current state-of-the-art (SOTA) open-source\napproaches. We then investigate approaches to significantly reduce token usage\nand sample passes while maintaining the original performance. Specifically, we\npropose two complementary methods that can be integrated into a unified EconRL\npipeline for amplified benefits: (1) a dynamic Chain-of-Thought (CoT) switching\nmechanism designed to mitigate unnecessary token consumption, and (2) Diverse\nparallel-scaled reinforcement learning (RL) with trainable prefixes to enhance\npass rates under constrained sampling passes. Experiments on miniF2F and\nProofNet demonstrate that our EconProver achieves comparable performance to\nbaseline methods with only 12% of the computational cost. This work provides\nactionable insights for deploying lightweight ATP models without sacrificing\nperformance.", "AI": {"tldr": "本研究对比了不同推理策略在自动定理证明中的计算效率，提出了EconRL管道，结合动态链式思考切换机制和多样化的并行强化学习，大幅减少计算成本同时保持性能。实验表明EconProver在miniF2F和ProofNet数据集上，仅以12%的计算成本达到基线方法的性能。", "motivation": "现有的大语言模型虽然通过测试时的时间扩展策略提升了自动定理证明性能，但也带来了大量的计算开销。当前的成本分析仅控制采样次数，忽略不同策略造成的成本差异。本研究旨在系统性地对比不同的推理策略，并提出减少计算成本的方法。", "method": "结构化论文分析方法用于提取关键信息", "result": "实验在两个数据集miniF2F和ProofNet上表明，EconProver相比于基线方法，在保持性能的同时，计算成本仅为其12%。", "conclusion": "本研究为部署轻量级自动定理证明模型提供了可行的见解，即在不牺牲性能的情况下减少计算成本。"}}
{"id": "2509.12452", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12452", "abs": "https://arxiv.org/abs/2509.12452", "authors": ["Zhenxin Zhang", "Zhihua Xu", "Yuwei Cao", "Ningli Xu", "Shuye Wang", "Shen'ao Cui", "Zhen Li", "Rongjun Qin"], "title": "Deep learning for 3D point cloud processing -- from approaches, tasks to its implications on urban and environmental applications", "comment": "57 Pages, 4 Figures", "summary": "Point cloud processing as a fundamental task in the field of geomatics and\ncomputer vision, has been supporting tasks and applications at different scales\nfrom air to ground, including mapping, environmental monitoring, urban/tree\nstructure modeling, automated driving, robotics, disaster responses etc. Due to\nthe rapid development of deep learning, point cloud processing algorithms have\nnowadays been almost explicitly dominated by learning-based approaches, most of\nwhich are yet transitioned into real-world practices. Existing surveys\nprimarily focus on the ever-updating network architecture to accommodate\nunordered point clouds, largely ignoring their practical values in typical\npoint cloud processing applications, in which extra-large volume of data,\ndiverse scene contents, varying point density, data modality need to be\nconsidered. In this paper, we provide a meta review on deep learning approaches\nand datasets that cover a selection of critical tasks of point cloud processing\nin use such as scene completion, registration, semantic segmentation, and\nmodeling. By reviewing a broad range of urban and environmental applications\nthese tasks can support, we identify gaps to be closed as these methods\ntransformed into applications and draw concluding remarks in both the\nalgorithmic and practical aspects of the surveyed methods.", "AI": {"tldr": "本文是对点云处理中深度学习方法和数据集的元分析，聚焦于实际应用中关键任务如场景补全、配准、语义分割和建模，识别从理论到应用的转化差距，并在算法和实用方面提出了结论性评论。", "motivation": "深度学习在处理点云数据上的发展虽快，但大部分仍停留在理论阶段未应用到实际中。现存的综述多集中在新型网络架构，忽视了这些方法在实际应用中的价值和现实挑战。", "method": "本文对深度学习在点云处理任务上的方法和数据集进行了元分析，涵盖点云处理中如场景补全、配准、语义分割和建模等关键任务。", "result": "通过对广泛的城市和环境应用的回顾，本文识别出了理论到实际应用转化中存在的差距。", "conclusion": "本文不仅从算法层面分析了当前深度学习方法在点云处理上的情况，还从实际应用的角度探讨了方法在不同场景下的适用性和挑战。"}}
