{"id": "2508.03712", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03712", "abs": "https://arxiv.org/abs/2508.03712", "authors": ["Agrima Seth", "Monojit Choudhary", "Sunayana Sitaram", "Kentaro Toyama", "Aditya Vashistha", "Kalika Bali"], "title": "How Deep Is Representational Bias in LLMs? The Cases of Caste and Religion", "comment": "Accepted to AIES 2025", "summary": "Representational bias in large language models (LLMs) has predominantly been\nmeasured through single-response interactions and has focused on Global\nNorth-centric identities like race and gender. We expand on that research by\nconducting a systematic audit of GPT-4 Turbo to reveal how deeply encoded\nrepresentational biases are and how they extend to less-explored dimensions of\nidentity. We prompt GPT-4 Turbo to generate over 7,200 stories about\nsignificant life events (such as weddings) in India, using prompts designed to\nencourage diversity to varying extents. Comparing the diversity of religious\nand caste representation in the outputs against the actual population\ndistribution in India as recorded in census data, we quantify the presence and\n\"stickiness\" of representational bias in the LLM for religion and caste. We\nfind that GPT-4 responses consistently overrepresent culturally dominant groups\nfar beyond their statistical representation, despite prompts intended to\nencourage representational diversity. Our findings also suggest that\nrepresentational bias in LLMs has a winner-take-all quality that is more biased\nthan the likely distribution bias in their training data, and repeated\nprompt-based nudges have limited and inconsistent efficacy in dislodging these\nbiases. These results suggest that diversifying training data alone may not be\nsufficient to correct LLM bias, highlighting the need for more fundamental\nchanges in model development. Dataset and Codebook:\nhttps://github.com/agrimaseth/How-Deep-Is-Representational-Bias-in-LLMs", "AI": {"tldr": "作者通过生成故事审计GPT-4 Turbo，发现其对文化主导群体存在过度代表的偏见，这表明仅依赖训练数据多样化的解决方案可能不足以纠正偏见，需进行更根本的模型开发变化。", "motivation": "扩展当前关于语言模型代表性偏见的研究，该研究主要集中在单一响应交互和针对全球北部中心身份的研究上，作者希望通过拓宽维度来探索身份偏见。", "method": "通过系统审计GPT-4 Turbo，生成超过7,200个关于印度重大生活事件（如婚礼）的故事，用不同程度鼓励多样性的提示来评估语言模型中的代表性偏见。", "result": "研究发现GPT-4的响应会过度代表文化主导群体，尽管提示设计旨在鼓励代表性多样性，这一发现表明语言模型中的代表性偏见具有赢家通吃的特性，并且提示调整偏见的效果有限且不一致。", "conclusion": "研究结果表明，仅依赖训练数据多样化可能不足以纠正语言模型的偏见，需要在模型开发上进行更根本的改变。"}}
{"id": "2508.03716", "categories": ["cs.CL", "cs.LG", "hep-th"], "pdf": "https://arxiv.org/pdf/2508.03716", "abs": "https://arxiv.org/abs/2508.03716", "authors": ["Paul Richmond", "Prarit Agarwal", "Borun Chowdhury", "Vasilis Niarchos", "Constantinos Papageorgakis"], "title": "FeynTune: Large Language Models for High-Energy Theory", "comment": "16 pages", "summary": "We present specialized Large Language Models for theoretical High-Energy\nPhysics, obtained as 20 fine-tuned variants of the 8-billion parameter\nLlama-3.1 model. Each variant was trained on arXiv abstracts (through August\n2024) from different combinations of hep-th, hep-ph and gr-qc. For a\ncomparative study, we also trained models on datasets that contained abstracts\nfrom disparate fields such as the q-bio and cs categories. All models were\nfine-tuned using two distinct Low-Rank Adaptation fine-tuning approaches and\nvarying dataset sizes, and outperformed the base model on hep-th abstract\ncompletion tasks. We compare performance against leading commercial LLMs\n(ChatGPT, Claude, Gemini, DeepSeek) and derive insights for further developing\nspecialized language models for High-Energy Theoretical Physics.", "AI": {"tldr": "本文介绍了基于Llama-3.1模型微调的20种专门针对高能理论物理学的大型语言模型，并进行了性能评估和对比研究。", "motivation": "旨在开发针对高能理论物理学的专门大规模语言模型，以提高在特定领域的表现。", "method": "使用了两种不同的低秩适应微调方法，并基于不同大小的数据集对80亿参数的Llama-3.1模型进行了20种变体的微调，这些数据集包括hep-th, hep-ph, gr-qc以及q-bio, cs领域的arXiv摘要。", "result": "微调后的模型在hep-th摘要完成任务上优于基础模型，并且与领先的商用LLMs（ChatGPT, Claude, Gemini, DeepSeek）进行了性能比较。", "conclusion": "研究为开发针对高能理论物理学的专门语言模型提供了见解。"}}
{"id": "2508.03719", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03719", "abs": "https://arxiv.org/abs/2508.03719", "authors": ["Abhay Vijayvargia", "Ajay Nagpal", "Kundeshwar Pundalik", "Atharva Savarkar", "Smita Gautam", "Pankaj Singh", "Rohit Saluja", "Ganesh Ramakrishnan"], "title": "Intent Aware Context Retrieval for Multi-Turn Agricultural Question Answering", "comment": null, "summary": "Indian farmers often lack timely, accessible, and language-friendly\nagricultural advice, especially in rural areas with low literacy. To address\nthis gap in accessibility, this paper presents a novel AI-powered agricultural\nchatbot, Krishi Sathi, designed to support Indian farmers by providing\npersonalized, easy-to-understand answers to their queries through both text and\nspeech. The system's intelligence stems from an IFT model, subsequently refined\nthrough fine-tuning on Indian agricultural knowledge across three curated\ndatasets. Unlike traditional chatbots that respond to one-off questions, Krishi\nSathi follows a structured, multi-turn conversation flow to gradually collect\nthe necessary details from the farmer, ensuring the query is fully understood\nbefore generating a response. Once the intent and context are extracted, the\nsystem performs Retrieval-Augmented Generation (RAG) by first fetching\ninformation from a curated agricultural database and then generating a tailored\nresponse using the IFT model. The chatbot supports both English and Hindi\nlanguages, with speech input and output features (via ASR and TTS) to make it\naccessible for users with low literacy or limited digital skills. This work\ndemonstrates how combining intent-driven dialogue flows, instruction-tuned\nmodels, and retrieval-based generation can improve the quality and\naccessibility of digital agricultural support in India.\n  This approach yielded strong results, with the system achieving a query\nresponse accuracy of 97.53%, 91.35% contextual relevance and personalization,\nand a query completion rate of 97.53%. The average response time remained under\n6 seconds, ensuring timely support for users across both English and Hindi\ninteractions.", "AI": {"tldr": "本文介绍了一种名为Krishi Sathi的AI驱动的农业聊天机器人，旨在通过文本和语音提供个性化和易于理解的回答，以解决印度农民获取及时、易于理解和语言友好的农业建议的问题。", "motivation": "旨在解决印度农民获取农业建议时所面临的及时性、可获取性和语言友好性不足的问题，尤其是低识字率的农村地区。", "method": "采用了一种基于IFT模型的AI驱动聊天机器人，该模型经过印度农业领域数据的微调优化。聊天机器人采用多轮对话流程，通过意图驱动对话流、指令微调模型和检索增强生成（RAG）方法提供个性化、易于理解的回答。", "result": "该系统在英语和印地语互动中均实现了97.53%的查询响应准确率，91.35%的语境相关性和个性化，以及97.53%的查询完成率。平均响应时间保持在6秒以下，确保了用户的及时支持。", "conclusion": "展示了如何通过结合意图驱动对话流、指令调优模型和检索增强生成（RAG）来提高印度农业数字支持的质量和可访问性。"}}
{"id": "2508.03726", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03726", "abs": "https://arxiv.org/abs/2508.03726", "authors": ["Jaydip Sen", "Harshitha Puvvala", "Subhasis Dasgupta"], "title": "Hierarchical Verification of Speculative Beams for Accelerating LLM Inference", "comment": "This paper was accepted for oral presentation and publication in the\n  3rd International Conference on Data Science and Network Engineering (ICDSNE\n  2025), organized at NIT, Agartala, India, from July 25 to 26, 2025. The paper\n  is 12 pages long, and it contains 3 tables and 4 figures. This is NOT the\n  final paper, which will be published in the Springer-published proceedings", "summary": "Large language models (LLMs) have achieved remarkable success across diverse\nnatural language processing tasks but face persistent challenges in inference\nefficiency due to their autoregressive nature. While speculative decoding and\nbeam sampling offer notable improvements, traditional methods verify draft\nsequences sequentially without prioritization, leading to unnecessary\ncomputational overhead. This work proposes the Hierarchical Verification Tree\n(HVT), a novel framework that restructures speculative beam decoding by\nprioritizing high-likelihood drafts and enabling early pruning of suboptimal\ncandidates. Theoretical foundations and a formal verification-pruning algorithm\nare developed to ensure correctness and efficiency. Integration with standard\nLLM inference pipelines is achieved without requiring retraining or\narchitecture modification. Experimental evaluations across multiple datasets\nand models demonstrate that HVT consistently outperforms existing speculative\ndecoding schemes, achieving substantial reductions in inference time and energy\nconsumption while maintaining or enhancing output quality. The findings\nhighlight the potential of hierarchical verification strategies as a new\ndirection for accelerating large language model inference.", "AI": {"tldr": "本文为处理大型语言模型在推理效率上遇到的瓶颈问题，提出了一种名为层次验证树（HVT）的新框架，通过优化推测性解码过程，成功减少了推理时间与能源消耗，并证明了这种方法的有效性。", "motivation": "尽管大型语言模型（LLMs）在自然语言处理任务上取得了显著成功，但它们由于自回归特性而在推理效率方面面临持续挑战。作者认为虽然推测性解码和束采样提供了显著改进，但传统方法没有优先级地顺序验证草案会导致不必要的计算开销。", "method": "本文提出了一种名为层次验证树（HVT）的新框架，该框架通过优先处理高似然草案并允许过早修剪次优候选来重新构建推测性束解码。", "result": "实验评估证明，HVT 在多个数据集和模型上始终超出现有推测性解码方案的表现，实现了显著的推理时间和能源消耗减少，同时保持或提高输出质量。", "conclusion": "研究凸显了层级验证策略作为一种加速大型语言模型推理的新策略的潜力。"}}
{"id": "2508.03699", "categories": ["cs.CV", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.03699", "abs": "https://arxiv.org/abs/2508.03699", "authors": ["Subin Raj Peter"], "title": "Text2VR: Automated instruction Generation in Virtual Reality using Large language Models for Assembly Task", "comment": "7 pages, 7 figures, conference", "summary": "Virtual Reality (VR) has emerged as a powerful tool for workforce training,\noffering immersive, interactive, and risk-free environments that enhance skill\nacquisition, decision-making, and confidence. Despite its advantages,\ndeveloping VR applications for training remains a significant challenge due to\nthe time, expertise, and resources required to create accurate and engaging\ninstructional content. To address these limitations, this paper proposes a\nnovel approach that leverages Large Language Models (LLMs) to automate the\ngeneration of virtual instructions from textual input. The system comprises two\ncore components: an LLM module that extracts task-relevant information from the\ntext, and an intelligent module that transforms this information into animated\ndemonstrations and visual cues within a VR environment. The intelligent module\nreceives input from the LLM module and interprets the extracted information.\nBased on this, an instruction generator creates training content using relevant\ndata from a database. The instruction generator generates the instruction by\nchanging the color of virtual objects and creating animations to illustrate\ntasks. This approach enhances training effectiveness and reduces development\noverhead, making VR-based training more scalable and adaptable to evolving\nindustrial needs.", "AI": {"tldr": "论文提出了一种基于大型语言模型自动生成VR培训内容的新方法，通过智能模块将文本信息转化为VR环境中的动画和视觉提示，实现更有效的培训并降低开发成本。", "motivation": "本研究旨在解决因时间、专业知识和资源需求而带来的VR应用程序开发困难的问题，进而提高培训效果并降低开发成本。", "method": "该研究提出了一个利用大型语言模型（LLMs）自动生成VR环境中虚拟指令的新方法。系统由两部分组成：1）LLM模块从文本中提取任务相关信息；2）智能模块将这些信息转化为VR环境中的动画演示和视觉提示。", "result": "通过改变虚拟物体的颜色和创建动画来解释任务，提高了VR培训内容的生成效率。", "conclusion": "这种方法提高了培训效果，减少了开发工作量，使VR培训更具可扩展性和适应工业需求的能力。"}}
{"id": "2508.03728", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03728", "abs": "https://arxiv.org/abs/2508.03728", "authors": ["Revanth Gangi Reddy", "Tanay Dixit", "Jiaxin Qin", "Cheng Qian", "Daniel Lee", "Jiawei Han", "Kevin Small", "Xing Fan", "Ruhi Sarikaya", "Heng Ji"], "title": "WINELL: Wikipedia Never-Ending Updating with LLM Agents", "comment": null, "summary": "Wikipedia, a vast and continuously consulted knowledge base, faces\nsignificant challenges in maintaining up-to-date content due to its reliance on\nmanual human editors. Inspired by the vision of continuous knowledge\nacquisition in NELL and fueled by advances in LLM-based agents, this paper\nintroduces WiNELL, an agentic framework for continuously updating Wikipedia\narticles. Our approach employs a multi-agent framework to aggregate online\ninformation, select new and important knowledge for a target entity in\nWikipedia, and then generate precise edit suggestions for human review. Our\nfine-grained editing models, trained on Wikipedia's extensive history of human\nedits, enable incorporating updates in a manner consistent with human editing\nbehavior. Our editor models outperform both open-source instruction-following\nbaselines and closed-source LLMs (e.g., GPT-4o) in key information coverage and\nediting efficiency. End-to-end evaluation on high-activity Wikipedia pages\ndemonstrates WiNELL's ability to identify and suggest timely factual updates.\nThis opens up a promising research direction in LLM agents for automatically\nupdating knowledge bases in a never-ending fashion.", "AI": {"tldr": "本文介绍了WiNELL，一个用于持续更新维基百科文章的代理框架，通过多代理框架收集信息并生成编辑建议，最终通过人工审核。", "motivation": "维基百科面临维持内容更新的问题，因为它依赖于手动编辑。受到NELL持续知识获取的启发，并利用基于语言模型的代理的进展，本文提出WiNELL框架。", "method": "采用多智能体框架，收集在线信息，选择目标实体的新颖和重要知识，并生成精确的编辑建议以供人工审核。编辑模型基于维基百科的人类编辑历史进行训练，以保持一致性。", "result": "编辑模型在关键信息覆盖和编辑效率方面优于开源指令跟随基线和闭源语言模型（如GPT-4o）。整体评估表明WiNELL框架有能力识别和建议及时事实更新。", "conclusion": "WiNELL有助于自动更新知识库的研究方向，能够在不断进行的方式下保持知识基的更新。"}}
{"id": "2508.03720", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.03720", "abs": "https://arxiv.org/abs/2508.03720", "authors": ["Ahmet Gökhan Poyraz"], "title": "Outlier Detection Algorithm for Circle Fitting", "comment": "Preprint, not peer-reviewed", "summary": "Circle fitting methods are extensively utilized in various industries,\nparticularly in quality control processes and design applications. The\neffectiveness of these algorithms can be significantly compromised when the\npoint sets to be predicted are noisy. To mitigate this issue, outlier detection\nand removal algorithms are often applied before the circle fitting procedure.\nThis study introduces the Polar Coordinate-Based Outlier Detection (PCOD)\nalgorithm, which can be effectively employed in circle fitting applications. In\nthe proposed approach, the point set is first transformed into polar\ncoordinates, followed by the calculation of both local and global standard\ndeviations. Outliers are then identified by comparing local mean values with\nthe global standard deviation. The practicality and efficiency of the proposed\nmethod are demonstrated by focusing on the high-precision diameter measurement\nof industrial washer parts. Images from a machine vision system are processed\nthrough preprocessing steps, including sub-pixel edge detection. The resulting\nsub-pixel edge points are then cleaned using the proposed outlier detection and\nremoval algorithm, after which circle fitting is performed. A comparison is\nmade using ten different circle fitting algorithms and five distinct outlier\ndetection methods. The results indicate that the proposed method outperforms\nthe other approaches, delivering the best performance in terms of accuracy\nwithin the dataset, thereby demonstrating its potential for enhancing circle\nfitting applications in industrial environments.", "AI": {"tldr": "The paper presents a new method for outlier detection in circle fitting applications, demonstrating its effectiveness through a case study on high-precision diameter measurement of industrial washers using machine vision technology.", "motivation": "The motivation behind this research is to address the issue of noisy point sets that can degrade the performance of circle fitting algorithms. By effectively detecting and removing outliers, the accuracy of circle fitting can be significantly improved.", "method": "The study introduces the Polar Coordinate-Based Outlier Detection (PCOD) algorithm for improving circle fitting accuracy. The process involves transforming point sets into polar coordinates and calculating local and global standard deviations to detect and remove outliers.", "result": "Experimental results show that the proposed PCOD algorithm outperforms other circle fitting algorithms and outlier detection methods in terms of accuracy for high-precision measurements.", "conclusion": "The proposed Polar Coordinate-Based Outlier Detection (PCOD) algorithm demonstrates superior performance in circle fitting accuracy, especially in industrial environments with noisy point sets, thus promising enhanced applications in quality control and design."}}
{"id": "2508.03737", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.03737", "abs": "https://arxiv.org/abs/2508.03737", "authors": ["Ashutosh Bandooni", "Brindha Subburaj"], "title": "GanitBench: A bi-lingual benchmark for evaluating mathematical reasoning in Vision Language Models", "comment": "6 pages, 3 figures. Accepted, Presented and Published as part of\n  Proceedings of the 6th International Conference on Recent Advantages in\n  Information Technology (RAIT) 2025", "summary": "Benchmarks for evaluating reasoning among Vision Language Models (VLMs) on\nseveral fields and domains are being curated more frequently over the last few\nyears. However these are often monolingual, mostly available in English.\nAdditionally there also is a lack of datasets available in Hindi on tasks apart\nfrom comprehension and translation. We introduce GanitBench, a tough benchmark\nconsisting of 1527 vision-only questions covering several topics in Mathematics\n- available in languages English and Hindi. Collected from two major\nexaminations from India, the JEE Advanced and the CBSE Boards examinations,\nthis benchmark includes questions in the form of images comprising of figures\nessential to a question as well as text. We evaluate two closed source models\nfor the same, in zero-shot Chain-of-Thought (CoT) and two-shot CoT settings.\nGPT-4o mini is found to be the more dominant model on the benchmark, with it's\nhighest average accuracy being 38.15%. We also evaluate models through a\n\"Double Lock\" constraint, which brings down the performance of the models by\nconsiderable margins. We observe that two-shot CoT appears to be a more\neffective setting under this environment. Performance of the two VLMs also\ndecreases when answering the same questions in the Hindi language. We hope to\nfacilitate the inclusion of languages like Hindi in research through our work.", "AI": {"tldr": "研究引入GanitBench，涵盖数学领域多个主题的视觉问题，并提供英语和印地语版本，以评估视觉语言模型（VLM）在非英语任务上的性能。", "motivation": "由于现有的视觉语言模型（VLM）的基准测试往往是单语言的，大部分是英语，而且针对印地语的语料库缺乏，尤其是在非理解及翻译任务中。本研究旨在促进像印地语这样的语言在研究中的应用。", "method": "引入GanitBench，一个由1527个仅视觉问题构成的基准测试，涵盖数学领域的多个主题，并提供英语和印地语版本。这些问题来自JEE Advanced和CBSE Boards两项主要印度考试。评估了两个闭源模型在同一基准下的零样本链式思考（CoT）和两样本CoT设置中的表现。", "result": "在零样本CoT和两样本CoT设置中，GPT-4o mini在基准中的表现更佳，最高的平均准确率为38.15%。通过“双重锁定”限制，模型性能显著下降，观察到两样本CoT在此环境下表现更优。在用印地语回答相同问题时，两个VLMs的表现也有所下降。", "conclusion": "本研究希望自己的工作能够为进一步纳入像印地语这样的语言研究提供便利。"}}
{"id": "2508.03721", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.03721", "abs": "https://arxiv.org/abs/2508.03721", "authors": ["Ahmet Gokhan Poyraz", "Ahmet Emir Dirik", "Hakan Gurkan", "Mehmet Kacmaz"], "title": "Enhancing Diameter Measurement Accuracy in Machine Vision Applications", "comment": "Preprint", "summary": "In camera measurement systems, specialized equipment such as telecentric\nlenses is often employed to measure parts with narrow tolerances. However,\ndespite the use of such equipment, measurement errors can occur due to\nmechanical and software-related factors within the system. These errors are\nparticularly evident in applications where parts of different diameters are\nmeasured using the same setup. This study proposes two innovative approaches to\nenhance measurement accuracy using multiple known reference parts: a conversion\nfactor-based method and a pixel-based method. In the first approach, the\nconversion factor is estimated from known references to calculate the diameter\n(mm) of the unknown part. In the second approach, the diameter (mm) is directly\nestimated using pixel-based diameter information from the references. The\nexperimental setup includes an industrial-grade camera and telecentric lenses.\nTests conducted on glass samples (1-12 mm) and metal workpieces (3-24 mm) show\nthat measurement errors, which originally ranged from 13-114 micrometers, were\nreduced to 1-2 micrometers using the proposed methods. By utilizing only a few\nknown reference parts, the proposed approach enables high-accuracy measurement\nof all parts within the camera's field of view. Additionally, this method\nenhances the existing diameter measurement literature by significantly reducing\nerror rates and improving measurement reliability.", "AI": {"tldr": "This paper addresses measurement errors in camera systems by proposing a conversion factor-based method and a pixel-based method, reducing errors from 13-114 micrometers to 1-2 micrometers.", "motivation": "The motivation for this study is to address the issue of measurement errors in camera measurement systems, which are particularly evident when measuring parts of different diameters using the same setup.", "method": "The paper proposes two methods to enhance the accuracy of measurements in camera measurement systems: a conversion factor-based method and a pixel-based method. These methods utilize several known reference parts to improve accuracy.", "result": "Experiments conducted on glass and metal samples showed that measurement errors were significantly reduced to 1-2 micrometers from an original range of 13-114 micrometers using the proposed methods.", "conclusion": "The proposed methods enable high-accuracy measurements using only a few known reference parts and significantly enhance measurement reliability and reduce error rates in the literature of diameter measurements."}}
{"id": "2508.03793", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03793", "abs": "https://arxiv.org/abs/2508.03793", "authors": ["Yanting Wang", "Runpeng Geng", "Ying Chen", "Jinyuan Jia"], "title": "AttnTrace: Attention-based Context Traceback for Long-Context LLMs", "comment": "The code is available at https://github.com/Wang-Yanting/AttnTrace.\n  The demo is available at https://huggingface.co/spaces/SecureLLMSys/AttnTrace", "summary": "Long-context large language models (LLMs), such as Gemini-2.5-Pro and\nClaude-Sonnet-4, are increasingly used to empower advanced AI systems,\nincluding retrieval-augmented generation (RAG) pipelines and autonomous agents.\nIn these systems, an LLM receives an instruction along with a context--often\nconsisting of texts retrieved from a knowledge database or memory--and\ngenerates a response that is contextually grounded by following the\ninstruction. Recent studies have designed solutions to trace back to a subset\nof texts in the context that contributes most to the response generated by the\nLLM. These solutions have numerous real-world applications, including\nperforming post-attack forensic analysis and improving the interpretability and\ntrustworthiness of LLM outputs. While significant efforts have been made,\nstate-of-the-art solutions such as TracLLM often lead to a high computation\ncost, e.g., it takes TracLLM hundreds of seconds to perform traceback for a\nsingle response-context pair. In this work, we propose AttnTrace, a new context\ntraceback method based on the attention weights produced by an LLM for a\nprompt. To effectively utilize attention weights, we introduce two techniques\ndesigned to enhance the effectiveness of AttnTrace, and we provide theoretical\ninsights for our design choice. We also perform a systematic evaluation for\nAttnTrace. The results demonstrate that AttnTrace is more accurate and\nefficient than existing state-of-the-art context traceback methods. We also\nshow that AttnTrace can improve state-of-the-art methods in detecting prompt\ninjection under long contexts through the attribution-before-detection\nparadigm. As a real-world application, we demonstrate that AttnTrace can\neffectively pinpoint injected instructions in a paper designed to manipulate\nLLM-generated reviews. The code is at\nhttps://github.com/Wang-Yanting/AttnTrace.", "AI": {"tldr": "研究提出了一种名为AttnTrace的新方法，该方法基于LLM关注权重进行背景追踪，可以更准确且更有效地进行追踪，同时引入了两种设计技术，增强了其效果和理论基础。", "motivation": "当前最先进的背景追踪解决方案如TracLLM常常导致高昂的计算成本，本文旨在通过提出AttnTrace来提高背景追踪的准确性和效率，同时也促进了LLM输出的解释性和可信度。", "method": "本文提出了AttnTrace，这是一种基于大型语言模型（LLM）对提示词生成的关注权重来进行背景追踪的新方法。为了有效地利用注意力权重，作者引入了两种设计技术，并提供了理论见解支持这些设计选择。", "result": "实验结果表明，AttnTrace比现有的最先进的背景追踪方法更准确、更高效，并且还能提高在长上下文中检测提示注入的性能。通过归因于检测范式下，它能有效地识别操控LLM产生评论的插入指令。", "conclusion": "AttnTrace在提升背景追踪效率与准确性上取得了显著效果，同时也加强了在一定程度上检测和防止提示注入的能力，实验证明其在真实应用场景中的有效性。"}}
{"id": "2508.03722", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03722", "abs": "https://arxiv.org/abs/2508.03722", "authors": ["Zhepeng Wang", "Yingjian Zhu", "Guanghao Dong", "Hongzhu Yi", "Feng Chen", "Xinming Wang", "Jun Xie"], "title": "Multimodal Video Emotion Recognition with Reliable Reasoning Priors", "comment": "preprint", "summary": "This study investigates the integration of trustworthy prior reasoning\nknowledge from MLLMs into multimodal emotion recognition. We employ Gemini to\ngenerate fine-grained, modality-separable reasoning traces, which are injected\nas priors during the fusion stage to enrich cross-modal interactions. To\nmitigate the pronounced class-imbalance in multimodal emotion recognition, we\nintroduce Balanced Dual-Contrastive Learning, a loss formulation that jointly\nbalances inter-class and intra-class distributions. Applied to the MER2024\nbenchmark, our prior-enhanced framework yields substantial performance gains,\ndemonstrating that the reliability of MLLM-derived reasoning can be\nsynergistically combined with the domain adaptability of lightweight fusion\nnetworks for robust, scalable emotion recognition.", "AI": {"tldr": "本研究探讨了将可信先验推理知识整合进多模态情感识别的方法，通过引入平衡双对比学习（Balanced Dual-Contrastive Learning）来缓解类别不平衡问题，性能显著提升。", "motivation": "研究旨在将多语言大模型（MLLM）中的可信先验推理知识整合进多模态情感识别中。", "method": "本研究使用Gemini生成细粒度的、可分离模态的推理轨迹作为先验知识，在融合阶段被注入以丰富跨模态交互。为了缓解模态情感识别中显著的类别不平衡问题，引入了平衡双对比学习（Balanced Dual-Contrastive Learning），这是一种联合平衡类间和类内分布的损失函数。", "result": "本框架应用于MER2024基准测试中，性能显著提升。", "conclusion": "研究表明，MLLM生成的推理可靠性可以与轻量级融合网络的领域自适应性相结合，实现稳健、可扩展的情感识别。"}}
{"id": "2508.03829", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03829", "abs": "https://arxiv.org/abs/2508.03829", "authors": ["Jiahao Xu", "Rui Hu", "Zikai Zhang"], "title": "Majority Bit-Aware Watermarking For Large Language Models", "comment": "Preprint", "summary": "The growing deployment of Large Language Models (LLMs) in real-world\napplications has raised concerns about their potential misuse in generating\nharmful or deceptive content. To address this issue, watermarking techniques\nhave emerged as a promising solution by embedding identifiable binary messages\ninto generated text for origin verification and misuse tracing. While recent\nefforts have explored multi-bit watermarking schemes capable of embedding rich\ninformation such as user identifiers, they typically suffer from the\nfundamental trade-off between text quality and decoding accuracy: to ensure\nreliable message decoding, they have to restrict the size of preferred token\nsets during encoding, yet such restrictions reduce the quality of the generated\ncontent. In this work, we propose MajorMark, a novel watermarking method that\nimproves this trade-off through majority bit-aware encoding. MajorMark selects\npreferred token sets based on the majority bit of the message, enabling a\nlarger and more flexible sampling of tokens. In contrast to prior methods that\nrely on token frequency analysis for decoding, MajorMark employs a\nclustering-based decoding strategy, which maintains high decoding accuracy even\nwhen the preferred token set is large, thus preserving both content quality and\ndecoding accuracy. We further introduce MajorMark$^+$, which partitions the\nmessage into multiple blocks to independently encode and deterministically\ndecode each block, thereby further enhancing the quality of watermarked text\nand improving decoding accuracy. Extensive experiments on state-of-the-art LLMs\ndemonstrate that our methods significantly enhance both decoding accuracy and\ntext generation quality, outperforming prior multi-bit watermarking baselines.", "AI": {"tldr": "本文提出了一种新颖的水印方法 MajorMark 及其扩展 MajorMark$^+$，在保持高质量文本生成的同时提高了解码准确性，解决了传统水印技术中生成质量和解码准确性之间的权衡问题。", "motivation": "水印技术能通过在生成的文本中嵌入可识别的二进制消息来验证来源和追踪误用行为，但对于多比特水印技术，存在着文本质量和解码准确性之间的权衡问题。本文旨在解决这一问题。", "method": "MajorMark 是一种新颖的水印技术，通过基于位大多数编码策略，在不牺牲解码准确性和内容质量的情况下，使首选令牌集合更大且更灵活。此外，MajorMark$^+$通过将消息划分为多个块，独立编码和解码每个块，进一步提升水印文本的质量和解码准确性。", "result": "大规模语言模型上的广泛实验表明，与现有的多比特水印技术相比，本文的方法在提升解码准确性和文本质量方面表现出色。", "conclusion": "MajorMark 和 MajorMark$^+$方法显著提升了多比特水印技术的解码准确性和文本生成质量，适用于复杂水印信息的嵌入和识别。"}}
{"id": "2508.03724", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03724", "abs": "https://arxiv.org/abs/2508.03724", "authors": ["Jia Li", "Yapeng Tian"], "title": "From Waveforms to Pixels: A Survey on Audio-Visual Segmentation", "comment": null, "summary": "Audio-Visual Segmentation (AVS) aims to identify and segment sound-producing\nobjects in videos by leveraging both visual and audio modalities. It has\nemerged as a significant research area in multimodal perception, enabling\nfine-grained object-level understanding. In this survey, we present a\ncomprehensive overview of the AVS field, covering its problem formulation,\nbenchmark datasets, evaluation metrics, and the progression of methodologies.\nWe analyze a wide range of approaches, including architectures for unimodal and\nmultimodal encoding, key strategies for audio-visual fusion, and various\ndecoder designs. Furthermore, we examine major training paradigms, from fully\nsupervised learning to weakly supervised and training-free methods. Notably, we\nprovide an extensive comparison of AVS methods across standard benchmarks,\nhighlighting the impact of different architectural choices, fusion strategies,\nand training paradigms on performance. Finally, we outline the current\nchallenges, such as limited temporal modeling, modality bias toward vision,\nlack of robustness in complex environments, and high computational demands, and\npropose promising future directions, including improving temporal reasoning and\nmultimodal fusion, leveraging foundation models for better generalization and\nfew-shot learning, reducing reliance on labeled data through selfand weakly\nsupervised learning, and incorporating higher-level reasoning for more\nintelligent AVS systems.", "AI": {"tldr": "本文对音频-视觉分割（AVS）领域的现有研究进行了全面综述，分析了其方法进展、训练范式，并指出了未来的研究方向。", "motivation": "AVS旨在通过结合视觉和音频模式来识别和分割视频中的发声物体。这已成为多模式感知领域的重要研究方向，能够实现细致的物体级理解。", "method": "本文综述了音频-视觉分割（AVS）领域的进展，包括问题定义、基准数据集、评估指标以及各类方法的发展历程。涵盖了单模态和多模态编码架构、音频-视觉融合策略以及不同的解码设计。研究了完全监督学习、弱监督学习及无训练方法的不同训练范式。", "result": "本文提供了对AVS方法在标准基准上的广泛比较，显示了不同架构选择、融合策略和训练范式对性能的影响。", "conclusion": "文章概述了当前的挑战，如有限的时间建模能力、视觉模式偏见、复杂环境中鲁棒性不足及高计算需求，提出了通过改进时间推理、多模态融合和利用基础模型来提高泛化能力及降低对标记数据的依赖等未来研究方向。"}}
{"id": "2508.03860", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03860", "abs": "https://arxiv.org/abs/2508.03860", "authors": ["Subhey Sadi Rahman", "Md. Adnanul Islam", "Md. Mahbub Alam", "Musarrat Zeba", "Md. Abdur Rahman", "Sadia Sultana Chowa", "Mohaimenul Azam Khan Raiaan", "Sami Azam"], "title": "Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models", "comment": "30 pages, 11 figures, 6 tables. Submitted to Artificial Intelligence\n  Review for peer review", "summary": "Large Language Models (LLMs) are trained on vast and diverse internet corpora\nthat often include inaccurate or misleading content. Consequently, LLMs can\ngenerate misinformation, making robust fact-checking essential. This review\nsystematically analyzes how LLM-generated content is evaluated for factual\naccuracy by exploring key challenges such as hallucinations, dataset\nlimitations, and the reliability of evaluation metrics. The review emphasizes\nthe need for strong fact-checking frameworks that integrate advanced prompting\nstrategies, domain-specific fine-tuning, and retrieval-augmented generation\n(RAG) methods. It proposes five research questions that guide the analysis of\nthe recent literature from 2020 to 2025, focusing on evaluation methods and\nmitigation techniques. The review also discusses the role of instruction\ntuning, multi-agent reasoning, and external knowledge access via RAG\nframeworks. Key findings highlight the limitations of current metrics, the\nvalue of grounding outputs with validated external evidence, and the importance\nof domain-specific customization to improve factual consistency. Overall, the\nreview underlines the importance of building LLMs that are not only accurate\nand explainable but also tailored for domain-specific fact-checking. These\ninsights contribute to the advancement of research toward more trustworthy and\ncontext-aware language models.", "AI": {"tldr": "论文综述了大型语言模型(LLMs)生成内容的事实准确性评估，指出需要更强大的事实核查框架，并提出了五个研究问题来指导这一领域的文献分析。文章还强调了当前评估指标的局限性和领域特定定制的重要性。", "motivation": "由于大型语言模型可能会生成不准确的信息，因此需要一个系统的框架来评估并纠正这些模型生成内容的事实准确性。", "method": "系统分析了从2020年至2025年的文献，重点关注评估方法和缓解技术。文章提出了五个研究问题来指导分析，并讨论了指令调整、多代理推理和通过检索增强生成(RAG)框架获取外部知识的作用。", "result": "发现了当前评估指标的限制，并强调了使用可验证的外部证据来增强模型输出的重要性。此外，强调了领域特定制订以改善事实一致性的价值。", "conclusion": "论文强调了构建准确、可解释且适用于特定领域的事实核查的大型语言模型的重要性，并对朝着更值得信赖和语境感知的语言模型的研究提供了见解。"}}
{"id": "2508.03725", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03725", "abs": "https://arxiv.org/abs/2508.03725", "authors": ["Yida Wang", "Taiting Lu", "Runze Liu", "Lanqing Yang", "Yifan Yang", "Zhe Chen", "Yuehai Wang", "Yixin Liu", "Kaiyuan Lin", "Xiaomeng Chen", "Dian Ding", "Yijie Li", "Yi-Chao Chen", "Yincheng Jin", "Mahanth Gowda"], "title": "A Large Language Model Powered Integrated Circuit Footprint Geometry Understanding", "comment": null, "summary": "Printed-Circuit-board (PCB) footprint geometry labeling of integrated\ncircuits (IC) is essential in defining the physical interface between\ncomponents and the PCB layout, requiring exceptional visual perception\nproficiency. However, due to the unstructured footprint drawing and abstract\ndiagram annotations, automated parsing and accurate footprint geometry modeling\nremain highly challenging. Despite its importance, no methods currently exist\nfor automated package geometry labeling directly from IC mechanical drawings.\nIn this paper, we first investigate the visual perception performance of Large\nMultimodal Models (LMMs) when solving IC footprint geometry understanding. Our\nfindings reveal that current LMMs severely suffer from inaccurate geometric\nperception, which hinders their performance in solving the footprint geometry\nlabeling problem. To address these limitations, we propose LLM4-IC8K, a novel\nframework that treats IC mechanical drawings as images and leverages LLMs for\nstructured geometric interpretation. To mimic the step-by-step reasoning\napproach used by human engineers, LLM4-IC8K addresses three sub-tasks:\nperceiving the number of pins, computing the center coordinates of each pin,\nand estimating the dimensions of individual pins. We present a two-stage\nframework that first trains LMMs on synthetically generated IC footprint\ndiagrams to learn fundamental geometric reasoning and then fine-tunes them on\nreal-world datasheet drawings to enhance robustness and accuracy in practical\nscenarios. To support this, we introduce ICGeo8K, a multi-modal dataset with\n8,608 labeled samples, including 4138 hand-crafted IC footprint samples and\n4470 synthetically generated samples. Extensive experiments demonstrate that\nour model outperforms state-of-the-art LMMs on the proposed benchmark.", "AI": {"tldr": "本文探讨了大型多模态模型（LMMs）在理解集成电路（IC）封装几何结构方面的视觉感知表现，并提出了LLM4-IC8K框架，该框架使用LMMs对IC机械图纸进行结构化几何解释。通过两阶段训练，首先在合成的IC封装图纸上学习基本几何推理，然后在实际的数据表绘图上进行微调以提高鲁棒性和准确性。实验表明，该模型在提出的基准上优于当前先进的LMMs模型。", "motivation": "由于IC封装几何学的标注工作需要高度的视觉感知能力，而现有的方法在处理非结构化的封装图纸和抽象的图注方面还存在很大的困难，本文旨在克服这些困难，提出了一种新的方法，以改善IC封装几何结构的自动化解释和标注。", "method": "提出了LLM4-IC8K框架，使用LMMs对IC机械图纸进行结构化几何解读，并设计了一个两阶段的框架，首先在合成的IC封装图样上训练基础几何推理，然后在真实世界的数据表图纸上进行微调。", "result": "进行了大量的实验，结果表明，所提出的模型在所提出的基准上比现有的先进的LMMs模型表现更好。", "conclusion": "LLM4-IC8K框架通过针对具体子任务的两阶段训练策略改进IC封装几何理解的性能，证明了其在准确性和鲁棒性方面的优越性。"}}
{"id": "2508.03865", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03865", "abs": "https://arxiv.org/abs/2508.03865", "authors": ["Yajie Luo", "Yihong Wu", "Muzhi Li", "Fengran Mo", "Jia Ao Sun", "Xinyu Wang", "Liheng Ma", "Yingxue Zhang", "Jian-Yun Nie"], "title": "An Entity Linking Agent for Question Answering", "comment": "12 pages, 2 figures. Submitted to AAAI 2026 Conference", "summary": "Some Question Answering (QA) systems rely on knowledge bases (KBs) to provide\naccurate answers. Entity Linking (EL) plays a critical role in linking natural\nlanguage mentions to KB entries. However, most existing EL methods are designed\nfor long contexts and do not perform well on short, ambiguous user questions in\nQA tasks. We propose an entity linking agent for QA, based on a Large Language\nModel that simulates human cognitive workflows. The agent actively identifies\nentity mentions, retrieves candidate entities, and makes decision. To verify\nthe effectiveness of our agent, we conduct two experiments: tool-based entity\nlinking and QA task evaluation. The results confirm the robustness and\neffectiveness of our agent.", "AI": {"tldr": "本文针对现有实体链接方法在处理问答任务中的短文本时存在的问题，提出了一种基于大型语言模型的实体链接代理，并通过实验验证了方法的有效性。", "motivation": "研究动机在于改进现有实体链接方法在问答任务中处理短且模糊用户提问时的性能。传统的实体链接方法主要针对长上下文设计，在短文本处理方面存在不足。", "method": "提出了一种基于大型语言模型的实体链接代理，模拟人类认知流程，主动识别实体提及，检索候选实体并做出决策。", "result": "该论文通过设计一个基于大型语言模型的实体链接代理，解决了现有实体链接方法在处理问答任务中的短且模糊用户提问时效果不佳的问题。实验验证了该代理的有效性和鲁棒性。", "conclusion": "实验结果表明，所提出的实体链接代理在工具辅助的实体链接和问答任务评估中表现出了强大的鲁棒性和有效性，验证了方法的有效性。"}}
{"id": "2508.03727", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.03727", "abs": "https://arxiv.org/abs/2508.03727", "authors": ["Tai Hyoung Rhee", "Dong-guw Lee", "Ayoung Kim"], "title": "TIR-Diffusion: Diffusion-based Thermal Infrared Image Denoising via Latent and Wavelet Domain Optimization", "comment": "Accepted at Thermal Infrared in Robotics (TIRO) Workshop, ICRA 2025", "summary": "Thermal infrared imaging exhibits considerable potentials for robotic\nperception tasks, especially in environments with poor visibility or\nchallenging lighting conditions. However, TIR images typically suffer from\nheavy non-uniform fixed-pattern noise, complicating tasks such as object\ndetection, localization, and mapping. To address this, we propose a\ndiffusion-based TIR image denoising framework leveraging latent-space\nrepresentations and wavelet-domain optimization. Utilizing a pretrained stable\ndiffusion model, our method fine-tunes the model via a novel loss function\ncombining latent-space and discrete wavelet transform (DWT) / dual-tree complex\nwavelet transform (DTCWT) losses. Additionally, we implement a cascaded\nrefinement stage to enhance fine details, ensuring high-fidelity denoising\nresults. Experiments on benchmark datasets demonstrate superior performance of\nour approach compared to state-of-the-art denoising methods. Furthermore, our\nmethod exhibits robust zero-shot generalization to diverse and challenging\nreal-world TIR datasets, underscoring its effectiveness for practical robotic\ndeployment.", "AI": {"tldr": "提出了一种新的基于扩散模型的热红外图像去噪方法，实现了高保真度的去噪效果，并在现实世界数据集上表现出了优越的性能和良好的泛化能力。", "motivation": "热红外成像在机器人感知任务中展现出巨大的潜力，尤其是在可见度差或照明条件复杂的环境下。然而，热红外图像通常受到严重的非均匀固定模式噪声的影响，这使得目标检测、定位和映射等任务变得复杂。为了应对这些问题，提出了上述方法。", "method": "提出了一种基于扩散模型的热红外图像去噪框架，该框架利用了潜空间表征和小波域优化。通过一个预训练的稳定扩散模型，并通过结合潜空间和离散小波变换（DWT）/双重树复小波变换（DTCWT）损失的新型损失函数对其进行微调。此外，还实现了一个级联精细处理阶段，以增强细节，确保高质量的去噪结果。", "result": "基准数据集上的实验结果表明，该方法相较于现有的去噪方法具有优越的性能。此外，该方法还实现了对各种具有挑战性的现实世界热红外数据集的鲁棒零样本泛化，凸显了其在实际机器人部署中的有效性。", "conclusion": "研究展示了所提出方法在现实世界热红外图像去噪中的优越性能和泛化能力，对于实际机器人应用具有重要意义。"}}
{"id": "2508.03905", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03905", "abs": "https://arxiv.org/abs/2508.03905", "authors": ["Haofei Yu", "Zhengyang Qi", "Yining Zhao", "Kolby Nottingham", "Keyang Xuan", "Bodhisattwa Prasad Majumder", "Hao Zhu", "Paul Pu Liang", "Jiaxuan You"], "title": "Sotopia-RL: Reward Design for Social Intelligence", "comment": "10 pages", "summary": "Social intelligence has become a critical capability for large language\nmodels (LLMs), enabling them to engage effectively in real-world social tasks\nsuch as accommodation, persuasion, collaboration, and negotiation.\nReinforcement learning (RL) is a natural fit for training socially intelligent\nagents because it allows models to learn sophisticated strategies directly\nthrough social interactions. However, social interactions have two key\ncharacteristics that set barriers for RL training: (1) partial observability,\nwhere utterances have indirect and delayed effects that complicate credit\nassignment, and (2) multi-dimensionality, where behaviors such as\nrapport-building or knowledge-seeking contribute indirectly to goal\nachievement. These characteristics make Markov decision process (MDP)-based RL\nwith single-dimensional episode-level rewards inefficient and unstable. To\naddress these challenges, we propose Sotopia-RL, a novel framework that refines\ncoarse episode-level feedback into utterance-level, multi-dimensional rewards.\nUtterance-level credit assignment mitigates partial observability by\nattributing outcomes to individual utterances, while multi-dimensional rewards\ncapture the full richness of social interactions and reduce reward hacking.\nExperiments in Sotopia, an open-ended social learning environment, demonstrate\nthat Sotopia-RL achieves state-of-the-art social goal completion scores (7.17\non Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing\napproaches. Ablation studies confirm the necessity of both utterance-level\ncredit assignment and multi-dimensional reward design for RL training. Our\nimplementation is publicly available at:\nhttps://github.com/sotopia-lab/sotopia-rl.", "AI": {"tldr": "论文提出了一种名为Sotopia-RL的新框架，通过将粗粒度的回合级反馈转变为细粒度的多维奖励，解决了基于Markov决策过程的强化学习在处理社交互动问题时遇到的不完全可观测性和多维行为特性问题，取得了优于现有方法的社交目标完成率分数。相关的实现已开源。", "motivation": "现有的MDP基础的单维回合级奖励强化学习框架在训练社交智能体时，由于社交互动的不完全可观测性和多维特性，无法有效且稳定地进行。", "method": "论文提出了Sotopia-RL框架，该框架通过细化回合级反馈为细粒度的多维奖励，将信用分配细化到各个话语级，同时通过多维奖励来捕捉社交互动的丰富性。", "result": "在Sotopia环境中，实验显示Sotopia-RL超过了现有方法，实现了高水平的社交目标完成率。次级研究表明话语级信用分配和多维奖励方案对强化学习训练是必需的。", "conclusion": "Sotopia-RL框架能够有效处理不完全可观测性和多维特性问题，提供一种新方法解决强化学习训练社交智能体的效果和稳定性问题。该框架的相关代码已经公开。"}}
{"id": "2508.03732", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03732", "abs": "https://arxiv.org/abs/2508.03732", "authors": ["Kushal Kanwar", "Dushyant Singh Chauhan", "Gopendra Vikram Singh", "Asif Ekbal"], "title": "What is Beneath Misogyny: Misogynous Memes Classification and Explanation", "comment": null, "summary": "Memes are popular in the modern world and are distributed primarily for\nentertainment. However, harmful ideologies such as misogyny can be propagated\nthrough innocent-looking memes. The detection and understanding of why a meme\nis misogynous is a research challenge due to its multimodal nature (image and\ntext) and its nuanced manifestations across different societal contexts. We\nintroduce a novel multimodal approach, \\textit{namely},\n\\textit{\\textbf{MM-Misogyny}} to detect, categorize, and explain misogynistic\ncontent in memes. \\textit{\\textbf{MM-Misogyny}} processes text and image\nmodalities separately and unifies them into a multimodal context through a\ncross-attention mechanism. The resulting multimodal context is then easily\nprocessed for labeling, categorization, and explanation via a classifier and\nLarge Language Model (LLM). The evaluation of the proposed model is performed\non a newly curated dataset (\\textit{\\textbf{W}hat's \\textbf{B}eneath\n\\textbf{M}isogynous \\textbf{S}tereotyping (WBMS)}) created by collecting\nmisogynous memes from cyberspace and categorizing them into four categories,\n\\textit{namely}, Kitchen, Leadership, Working, and Shopping. The model not only\ndetects and classifies misogyny, but also provides a granular understanding of\nhow misogyny operates in domains of life. The results demonstrate the\nsuperiority of our approach compared to existing methods. The code and dataset\nare available at\n\\href{https://github.com/kushalkanwarNS/WhatisBeneathMisogyny/tree/main}{https://github.com/Misogyny}.", "AI": {"tldr": "本文提出了一种名为\textit{\textbf{MM-Misogyny}}的方法，用于检测和分类网络梗中的厌女内容，并通过跨注意力机制处理模态数据，使其与其他现有方法相比具有优势。", "motivation": "鉴于网络梗在现代社会的流行及其中可能潜藏的有害意识形态（例如，厌女）问题，本文旨在通过一种多模态的方法来识别和理解这些隐秘的厌女内容。这项工作旨在应对多种模态（图像和文本）以及在不同社会背景下厌女内容复杂表现所带来的研究挑战。", "method": "本研究提出了一种名为\textit{\textbf{MM-Misogyny}}的新型多模态方法，用于检测、分类和解释网络流行梗（memes）中的厌女内容。该方法通过跨注意力机制分别处理文本和图像模态，并将它们统一为多模态上下文。此多模态上下文随后通过分类器和大型语言模型(Large Language Model, LLM)进行标签化、分类和解释。", "result": "评估展示了\textit{\textbf{MM-Misogyny}}方法能够有效检测和分类厌女梗，并且该方法在对厌女内容的多模态理解和解释上优于现有方法。", "conclusion": "本文介绍的多模态方法（\textit{\textbf{MM-Misogyny}}）不仅能够检测和分类厌女梗，还能对这些厌女内容在生活各领域的运作提供深入理解。模型在新构的数据集上的评估显示了其相对于现有方法的优越性。"}}
{"id": "2508.03923", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03923", "abs": "https://arxiv.org/abs/2508.03923", "authors": ["Linxin Song", "Yutong Dai", "Viraj Prabhu", "Jieyu Zhang", "Taiwei Shi", "Li Li", "Junnan Li", "Silvio Savarese", "Zeyuan Chen", "Jieyu Zhao", "Ran Xu", "Caiming Xiong"], "title": "CoAct-1: Computer-using Agents with Coding as Actions", "comment": null, "summary": "Autonomous agents that operate computers via Graphical User Interfaces (GUIs)\noften struggle with efficiency and reliability on complex, long-horizon tasks.\nWhile augmenting these agents with planners can improve task decomposition,\nthey remain constrained by the inherent limitations of performing all actions\nthrough GUI manipulation, leading to brittleness and inefficiency. In this\nwork, we introduce a more robust and flexible paradigm: enabling agents to use\ncoding as a enhanced action. We present CoAct-1, a novel multi-agent system\nthat synergistically combines GUI-based control with direct programmatic\nexecution. CoAct-1 features an Orchestrator that dynamically delegates subtasks\nto either a conventional GUI Operator or a specialized Programmer agent, which\ncan write and execute Python or Bash scripts. This hybrid approach allows the\nagent to bypass inefficient GUI action sequences for tasks like file management\nand data processing, while still leveraging visual interaction when necessary.\nWe evaluate our system on the challenging OSWorld benchmark, where CoAct-1\nachieves a new state-of-the-art success rate of 60.76%, significantly\noutperforming prior methods. Furthermore, our approach dramatically improves\nefficiency, reducing the average number of steps required to complete a task to\njust 10.15, compared to 15 for leading GUI agents. Our results demonstrate that\nintegrating coding as a core action provides a more powerful, efficient, and\nscalable path toward generalized computer automation.", "AI": {"tldr": "本文介绍了CoAct-1，一个结合GUI控制和直接编程执行的新多智能体系统。通过使用编程增强操作，CoAct-1在OSWorld基准测试中表现出色，效率与成功率均优于现有技术。", "motivation": "现有自主智能体通过GUI操作计算机在处理复杂、长时任务时效率低下且不可靠。尽管可以通过添加规划器来改善任务分解，但它们仍然受制于GUI交互的内在局限性。该研究旨在通过允许智能体使用编程作为增强操作来提高效率和健壮性。", "method": "本文提出了CoAct-1，这是一种新型多智能体系统，它将GUI操作与直接编程执行相结合。该系统包括一个编排者，它会根据任务需求动态地将子任务分配给GUI操作员或程序员智能体，后者能够编写和执行Python或Bash脚本，以克服传统GUI操作中的效率低下和易脆性问题。", "result": "CoAct-1在OSWorld基准测试中达到了60.76%的成功率，为新的最先进水平，并显著提高了任务完成的效率，将完成任务所需的平均步骤减少到10.15步，相比之下，现有顶级GUI智能体需要15步。", "conclusion": "将编程整合为一个核心操作，提供了一条更为强大、高效和可扩展的一般计算机自动化路径。"}}
{"id": "2508.03735", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03735", "abs": "https://arxiv.org/abs/2508.03735", "authors": ["Gopalji Gaur", "Mohammadreza Zolfaghari", "Thomas Brox"], "title": "StorySync: Training-Free Subject Consistency in Text-to-Image Generation via Region Harmonization", "comment": "14 pages, 10 figures, GCPR", "summary": "Generating a coherent sequence of images that tells a visual story, using\ntext-to-image diffusion models, often faces the critical challenge of\nmaintaining subject consistency across all story scenes. Existing approaches,\nwhich typically rely on fine-tuning or retraining models, are computationally\nexpensive, time-consuming, and often interfere with the model's pre-existing\ncapabilities. In this paper, we follow a training-free approach and propose an\nefficient consistent-subject-generation method. This approach works seamlessly\nwith pre-trained diffusion models by introducing masked cross-image attention\nsharing to dynamically align subject features across a batch of images, and\nRegional Feature Harmonization to refine visually similar details for improved\nsubject consistency. Experimental results demonstrate that our approach\nsuccessfully generates visually consistent subjects across a variety of\nscenarios while maintaining the creative abilities of the diffusion model.", "AI": {"tldr": "为了克服使用文本到图像扩散模型生成连贯图像序列时的主体一致性难题，本文提出了一种无需训练的方法，通过掩码跨图像注意力共享和区域性特征和谐来改进主体一致性。实验结果表明该方法可在不同场景中成功生成视觉上一致的主体。", "motivation": "现有的方法需要微调或重新训练模型，这需要大量的计算资源和时间，并且可能会影响模型的预存能力。为了解决这些问题并降低计算成本，同时保持模型的创造力，提出了这种方法。", "method": "通过引入掩码跨图像注意力共享来动态对齐一批图像中的主题特征，并使用区域性特征和谐来改进视觉上相似细节，从而实现主题一致性。这种方法不需要额外训练，可以无缝地与预训练的扩散模型结合使用。", "result": "实验结果表明，提出的方法在各种场景中成功生成了视觉上一致的主体，并且保持了扩散模型的创造性。", "conclusion": "所提出的方法在没有额外训练的情况下，有效地提高了图像生成中主题的一致性，并且与预训练的扩散模型无缝工作。"}}
{"id": "2508.03935", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03935", "abs": "https://arxiv.org/abs/2508.03935", "authors": ["Raymond Wilson", "Cole Graham", "Chase Carter", "Zefeng Yang", "Ruiqi Gu"], "title": "CAP-LLM: Context-Augmented Personalized Large Language Models for News Headline Generation", "comment": null, "summary": "In the era of information overload, personalized news headline generation is\ncrucial for engaging users by tailoring content to their preferences while\naccurately conveying news facts. Existing methods struggle with effectively\ncapturing complex user interests and ensuring factual consistency, often\nleading to generic or misleading headlines. Leveraging the unprecedented\ncapabilities of Large Language Models (LLMs) in text generation, we propose\nContext-Augmented Personalized LLM (CAP-LLM), a novel framework that integrates\nuser preferences and factual consistency constraints into a powerful\npre-trained LLM backbone. CAP-LLM features a User Preference Encoder to capture\nlong-term user interests, a Context Injection Adapter to seamlessly integrate\nthese preferences and current article context into the LLM's generation\nprocess, and a Fact-Consistency Reinforcement Module employing a novel\ncontrastive loss to mitigate hallucination. Evaluated on the real-world PENS\ndataset, CAP-LLM achieves state-of-the-art performance across all metrics.\nNotably, it significantly improves factual consistency (FactCC of 87.50) over\nstrong baselines like BART (86.67), while simultaneously enhancing\npersonalization (Pc(avg) 2.73, Pc(max) 17.25) and content coverage (ROUGE-1\n26.55, ROUGE-2 9.95, ROUGE-L 23.01). Our ablation studies, human evaluations,\nand sensitivity analyses further validate the effectiveness of each component\nand the robustness of our approach, demonstrating CAP-LLM's ability to achieve\na superior balance between personalization and factual accuracy in news\nheadline generation.", "AI": {"tldr": "CAP-LLM, a personalized news headline generation framework, integrates user preferences and ensures factual consistency using a large language model.", "motivation": "To address the challenges of existing methods in capturing complex user interests and maintaining factual consistency in personalized news headlines.", "method": "CAP-LLM includes a User Preference Encoder, a Context Injection Adapter, and a Fact-Consistency Reinforcement Module with a contrastive loss to prevent hallucination.", "result": "On the PENS dataset, CAP-LLM significantly outperforms baselines like BART with better factual consistency and personalization scores.", "conclusion": "CAP-LLM effectively achieves a balance between personalization and fact accuracy, making it a robust solution for news headline generation."}}
{"id": "2508.03736", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03736", "abs": "https://arxiv.org/abs/2508.03736", "authors": ["Rafayel Mkrtchyan", "Armen Manukyan", "Hrant Khachatrian", "Theofanis P. Raptis"], "title": "Fusion of Pervasive RF Data with Spatial Images via Vision Transformers for Enhanced Mapping in Smart Cities", "comment": "Work partly supported by the RA Science Committee grant No. 22rl-052\n  (DISTAL) and the EU under Italian National Recovery and Resilience Plan of\n  NextGenerationEU on \"Telecommunications of the Future\" (PE00000001 - program\n  \"RESTART\")", "summary": "Environment mapping is an important computing task for a wide range of smart\ncity applications, including autonomous navigation, wireless network operations\nand extended reality environments. Conventional smart city mapping techniques,\nsuch as satellite imagery, LiDAR scans, and manual annotations, often suffer\nfrom limitations related to cost, accessibility and accuracy. Open-source\nmapping platforms have been widely utilized in artificial intelligence\napplications for environment mapping, serving as a source of ground truth.\nHowever, human errors and the evolving nature of real-world environments\nintroduce biases that can negatively impact the performance of neural networks\ntrained on such data. In this paper, we present a deep learning-based approach\nthat integrates the DINOv2 architecture to improve building mapping by\ncombining maps from open-source platforms with radio frequency (RF) data\ncollected from multiple wireless user equipments and base stations. Our\napproach leverages a vision transformer-based architecture to jointly process\nboth RF and map modalities within a unified framework, effectively capturing\nspatial dependencies and structural priors for enhanced mapping accuracy. For\nthe evaluation purposes, we employ a synthetic dataset co-produced by Huawei.\nWe develop and train a model that leverages only aggregated path loss\ninformation to tackle the mapping problem. We measure the results according to\nthree performance metrics which capture different qualities: (i) The Jaccard\nindex, also known as intersection over union (IoU), (ii) the Hausdorff\ndistance, and (iii) the Chamfer distance. Our design achieves a macro IoU of\n65.3%, significantly surpassing (i) the erroneous maps baseline, which yields\n40.1%, (ii) an RF-only method from the literature, which yields 37.3%, and\n(iii) a non-AI fusion baseline that we designed which yields 42.2%.", "AI": {"tldr": "本文提出了一种新型的制图方法，改进了现有技术，提高了准确性，并通过合成数据集验证了其有效性。", "motivation": "传统的智慧城市制图技术（如卫星图像、LiDAR扫描和手动注释）存在成本、可访问性和准确性方面的局限性。尽管开源制图平台在人工智能应用中被广泛用于环境制图，并作为地面真实数据源，但人与人之间的错误和现实世界的演变引入了偏差，可能会影响基于此类数据训练的神经网络的性能。", "method": "提出了一种基于深度学习的方法，该方法整合了DINOv2架构，结合了开源平台的地图和来自多个无线用户设备和基站的无线电频率（RF）数据，以改进建筑制图。该方法利用了基于视觉变压器的架构，在统一框架中联合处理RF和地图模态，有效捕捉空间依赖性和结构先验，从而提高制图准确性。", "result": "研究采用华为联合制作的合成数据集进行评估。模型仅利用聚合路径损耗信息来解决制图问题。根据Jaccard指数、Hausdorff距离和Chamfer距离三个性能指标来衡量结果。设计方案实现了65.3%的宏观IoU性能，显著优于误图基准、RF数据的现有方法和非人工智能融合基准。", "conclusion": "研究的结果表明，结合RF数据和地图信息的深度学习方法可以显著提升环境地图的准确性，显示出在智慧城市应用中的潜力。"}}
{"id": "2508.03970", "categories": ["cs.CL", "cs.AI", "68T01 (Primary), 68T50 (Secondary)", "I.2.0; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.03970", "abs": "https://arxiv.org/abs/2508.03970", "authors": ["Alok Abhishek", "Lisa Erickson", "Tushar Bandopadhyay"], "title": "Data and AI governance: Promoting equity, ethics, and fairness in large language models", "comment": "Published in MIT Science Policy Review 6, 139-146 (2025)", "summary": "In this paper, we cover approaches to systematically govern, assess and\nquantify bias across the complete life cycle of machine learning models, from\ninitial development and validation to ongoing production monitoring and\nguardrail implementation. Building upon our foundational work on the Bias\nEvaluation and Assessment Test Suite (BEATS) for Large Language Models, the\nauthors share prevalent bias and fairness related gaps in Large Language Models\n(LLMs) and discuss data and AI governance framework to address Bias, Ethics,\nFairness, and Factuality within LLMs. The data and AI governance approach\ndiscussed in this paper is suitable for practical, real-world applications,\nenabling rigorous benchmarking of LLMs prior to production deployment,\nfacilitating continuous real-time evaluation, and proactively governing LLM\ngenerated responses. By implementing the data and AI governance across the life\ncycle of AI development, organizations can significantly enhance the safety and\nresponsibility of their GenAI systems, effectively mitigating risks of\ndiscrimination and protecting against potential reputational or brand-related\nharm. Ultimately, through this article, we aim to contribute to advancement of\nthe creation and deployment of socially responsible and ethically aligned\ngenerative artificial intelligence powered applications.", "AI": {"tldr": "该研究介绍了用于管理大型语言模型偏差、道德公平性的数据和AI治理方法，助力创建负责任、伦理导向的生成式AI应用。", "motivation": "文章旨在系统化治理、评估和量化机器学习模型（特别是LLMs）在完整生命周期中的偏差问题，从最初的开发和验证到生产过程中的持续监控。", "method": "本研究基于先前对大型语言模型（LLMs）的偏差评估与测试套件（BEATS）的工作，讨论了数据和人工智能治理框架如何用于解决LLMs中的偏差、道德、公平性和事实性问题。", "result": "提出的数据和AI治理方法适合实际应用，能够提供严格的基准测试、持续的实时评估以及主动治理LLM生成的响应，从而增强系统的安全性和可靠性。", "conclusion": "通过在整个AI开发周期中实施数据和AI治理方法，组织可以显著提高其生成式AI系统的责任感和道德一致性，减少歧视风险，预防潜在的品牌或声誉损害。"}}
{"id": "2508.03740", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03740", "abs": "https://arxiv.org/abs/2508.03740", "authors": ["Jianqiao Chen", "Tingting Zhu", "Huishi Song", "Nan Ma", "Xiaodong Xu"], "title": "VQ-DeepISC: Vector Quantized-Enabled Digital Semantic Communication with Channel Adaptive Image Transmission", "comment": null, "summary": "Discretization of semantic features enables interoperability between semantic\nand digital communication systems, showing significant potential for practical\napplications. The fundamental difficulty in digitizing semantic features stems\nfrom the need to preserve continuity and context in inherently analog\nrepresentations during their compression into discrete symbols while ensuring\nrobustness to channel degradation. In this paper, we propose a vector quantized\n(VQ)-enabled digital semantic communication system with channel adaptive image\ntransmission, named VQ-DeepISC. Guided by deep joint source-channel coding\n(DJSCC), we first design a Swin Transformer backbone for hierarchical semantic\nfeature extraction, followed by VQ modules projecting features into discrete\nlatent spaces. Consequently, it enables efficient index-based transmission\ninstead of raw feature transmission. To further optimize this process, we\ndevelop an attention mechanism-driven channel adaptation module to dynamically\noptimize index transmission. Secondly, to counteract codebook collapse during\ntraining process, we impose a distributional regularization by minimizing the\nKullback-Leibler divergence (KLD) between codeword usage frequencies and a\nuniform prior. Meanwhile, exponential moving average (EMA) is employed to\nstabilize training and ensure balanced feature coverage during codebook\nupdates. Finally, digital communication is implemented using quadrature phase\nshift keying (QPSK) modulation alongside orthogonal frequency division\nmultiplexing (OFDM), adhering to the IEEE 802.11a standard. Experimental\nresults demonstrate superior reconstruction fidelity of the proposed system\nover benchmark methods.", "AI": {"tldr": "提出了一种新的通信系统VQ-DeepISC，用于语义特征的数字化传输，实现了高效且健壮的图像传输，实验结果表明，其在重建保真度上优于基准方法。", "motivation": "语义特征的离散化可以实现语义和数字通信系统间的互操作性，这在实际情况中具显著的应用潜力。然而，将语义特征数字化的根本难点在于在压缩成离散符号的过程中需要保持其连续性与上下文的一致性，同时确保在信道损伤下仍保持强健性。", "method": "我们提出了一种名为VQ-DeepISC的基于矢量量化（VQ）的数字语义通信系统，该系统具备信道自适应的图像传输能力。系统首先采用Swin Transformer骨干网络进行分层语义特征提取，然后通过VQ模块将特征投影到离散的潜在空间。我们开发了一种基于注意力机制的信道自适应模块，用于动态优化索引传输。为了防止训练过程中发生码本崩溃，我们通过最小化码字使用频率与均匀先验之间的Kullback-Leibler散度来施加分布正则化。同时，采用指数移动平均（EMA）来稳定训练并确保码本更新期间的特征平衡覆盖。最后，按照IEEE 802.11a标准，利用四相相移键控（QPSK）调制以及正交频分复用（OFDM）实现数字通信。", "result": "实验结果表明，所提出的系统相较于基准方法具有更优的重建保真度。", "conclusion": "所提出的基于矢量量化的数字语义通信系统在信道自适应的图像传输方面取得了出色的效果，相较现有方法，系统在重建保真度上表现出显著优势。"}}
{"id": "2508.03979", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03979", "abs": "https://arxiv.org/abs/2508.03979", "authors": ["Md Arafat Sultan", "Ramón Fernandez Astudillo"], "title": "Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency", "comment": null, "summary": "Despite its simplicity and efficacy, the high token expenditure of\nself-consistency can limit its practical utility. Here we investigate if\nself-consistency can be made more token-efficient for long chain-of-thought\nreasoning tasks, while preserving its parallelism, through early hypothesis\npruning. Concretely, we generate all solutions in parallel, but periodically\nprune intermediate hypotheses that are deemed unnecessary based on two\nlightweight indicators: (a) the model's own confidence in individual\nhypotheses, and (b) lexical coverage of all current hypotheses by candidate\nsubsets that are under consideration for continued retention. We design a fast\nweighted set cover algorithm that utilizes the two indicators; our evaluation\nof five LLMs on three math benchmarks shows that this method can improve token\nefficiency for all models, by 10-35% in many cases.", "AI": {"tldr": "本文研究了通过假设剪枝提高自洽性在长链推理任务中的令牌效率，同时保持其并行性。评估显示在某些情况下可提升效率10-35%。", "motivation": "尽管自洽性简单且有效，但在长链推理任务中由于高令牌消耗限制了其实用性。这项研究的动机在于解决这一问题。", "method": "通过早期假设剪枝来提高长链推理任务中的自洽性，同时保持其并行性。具体来说，我们并行生成所有解决方案，但根据模型对个别假设的置信度和所有当前假设的词汇覆盖率这两个轻量级指标，定期剪枝被认为是不必要的中间假设。我们设计了一个快速的加权集合覆盖算法，该算法利用这两个指标。", "result": "在五个大语言模型上的三个数学基准测试评估表明，该方法可以提高所有模型的令牌效率，许多情况下提高了10-35%。", "conclusion": "研究表明，通过使用早期假设剪枝的方法，可以显著提高长链推理任务中自洽性的令牌效率，从而提高了其实用性。"}}
{"id": "2508.03745", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03745", "abs": "https://arxiv.org/abs/2508.03745", "authors": ["Wenwen Li", "Chia-Yu Hsu", "Maosheng Hu"], "title": "Tobler's First Law in GeoAI: A Spatially Explicit Deep Learning Model for Terrain Feature Detection Under Weak Supervision", "comment": null, "summary": "Recent interest in geospatial artificial intelligence (GeoAI) has fostered a\nwide range of applications using artificial intelligence (AI), especially deep\nlearning, for geospatial problem solving. However, major challenges such as a\nlack of training data and the neglect of spatial principles and spatial effects\nin AI model design remain, significantly hindering the in-depth integration of\nAI with geospatial research. This paper reports our work in developing a deep\nlearning model that enables object detection, particularly of natural features,\nin a weakly supervised manner. Our work makes three contributions: First, we\npresent a method of object detection using only weak labels. This is achieved\nby developing a spatially explicit model based on Tobler's first law of\ngeography. Second, we incorporate attention maps into the object detection\npipeline and develop a multistage training strategy to improve performance.\nThird, we apply this model to detect impact craters on Mars, a task that\npreviously required extensive manual effort. The model generalizes to both\nnatural and human-made features on the surfaces of Earth and other planets.\nThis research advances the theoretical and methodological foundations of GeoAI.", "AI": {"tldr": "本文提出了一种新的弱监督下的目标检测方法，解决了AI在地理研究中的应用挑战，并成功应用在火星陨石坑检测上。", "motivation": "由于缺乏训练数据和忽视空间原理，AI与地理研究的深度整合遇到了挑战。本文旨在通过开发新的检测模型解决这些问题。", "method": "开发了一个基于弱标签的目标检测方法，该方法基于地理学的第一定律。同时，引入注意力图和多阶段训练策略来优化模型性能。", "result": "该模型能够准确检测火星上的陨石坑，并能推广应用于地球和其他行星的自然和人造特征检测。", "conclusion": "本研究为地理人工智能的理论和方法基础做出了贡献，证明了在弱监督情况下进行目标检测的潜力。"}}
{"id": "2508.03990", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.03990", "abs": "https://arxiv.org/abs/2508.03990", "authors": ["Bohan Jiang", "Dawei Li", "Zhen Tan", "Chengshuai Zhao", "Huan Liu"], "title": "Are Today's LLMs Ready to Explain Well-Being Concepts?", "comment": "9 pages, 4 figures, 3 tables", "summary": "Well-being encompasses mental, physical, and social dimensions essential to\npersonal growth and informed life decisions. As individuals increasingly\nconsult Large Language Models (LLMs) to understand well-being, a key challenge\nemerges: Can LLMs generate explanations that are not only accurate but also\ntailored to diverse audiences? High-quality explanations require both factual\ncorrectness and the ability to meet the expectations of users with varying\nexpertise. In this work, we construct a large-scale dataset comprising 43,880\nexplanations of 2,194 well-being concepts, generated by ten diverse LLMs. We\nintroduce a principle-guided LLM-as-a-judge evaluation framework, employing\ndual judges to assess explanation quality. Furthermore, we show that\nfine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct\nPreference Optimization (DPO) can significantly enhance the quality of\ngenerated explanations. Our results reveal: (1) The proposed LLM judges align\nwell with human evaluations; (2) explanation quality varies significantly\nacross models, audiences, and categories; and (3) DPO- and SFT-finetuned models\noutperform their larger counterparts, demonstrating the effectiveness of\npreference-based learning for specialized explanation tasks.", "AI": {"tldr": "本研究通过构建大规模解释数据集并采用LLM作为评判者评估解释质量，展示出经SFT和DPO微调的开源LLM在生成高质量福祉概念解释方面有显著改善。", "motivation": "随着个人越来越多地咨询大型语言模型（LLM）以理解福祉，一个关键的挑战随之而来：LLM能否生成不仅是准确的解释，而且还能满足不同受众群体的期望。高质量的解释不仅需要事实上的正确性，还需要能够满足具有不同专业知识的用户的期望。", "method": "构建了一个大规模的数据集，包含2,194个福祉概念的43,880个解释，并由十种不同的LLM生成。同时引入了一个由原则指导的LLM作为评判者的评估框架，采用双重评判者来评估解释的质量。此外，还展示了使用监督微调（SFT）和直接偏好优化（DPO）对开源LLM进行微调可以显著提高生成解释的质量。", "result": "研究结果表明：（1）建议的LLM评判者与人类评估高度一致；（2）解释的质量在不同模型、受众和类别之间存在显著差异；（3）经过DPO和SFT微调的模型超越了其较大的同类模型，展示了偏好学习对于专门解释任务的有效性。", "conclusion": "研究提出的方法能够提升LLM生成福祉概念解释的质量，并表明特定领域的微调方法（SFT和DPO）对于提高解释质量尤为有效。"}}
{"id": "2508.03749", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.03749", "abs": "https://arxiv.org/abs/2508.03749", "authors": ["Riccardo Fiorista", "Awad Abdelhalim", "Anson F. Stewart", "Gabriel L. Pincus", "Ian Thistle", "Jinhua Zhao"], "title": "Closed-Circuit Television Data as an Emergent Data Source for Urban Rail Platform Crowding Estimation", "comment": "26 pages, 17 figures, 4 tables", "summary": "Accurately estimating urban rail platform occupancy can enhance transit\nagencies' ability to make informed operational decisions, thereby improving\nsafety, operational efficiency, and customer experience, particularly in the\ncontext of crowding. However, sensing real-time crowding remains challenging\nand often depends on indirect proxies such as automatic fare collection data or\nstaff observations. Recently, Closed-Circuit Television (CCTV) footage has\nemerged as a promising data source with the potential to yield accurate,\nreal-time occupancy estimates. The presented study investigates this potential\nby comparing three state-of-the-art computer vision approaches for extracting\ncrowd-related features from platform CCTV imagery: (a) object detection and\ncounting using YOLOv11, RT-DETRv2, and APGCC; (b) crowd-level classification\nvia a custom-trained Vision Transformer, Crowd-ViT; and (c) semantic\nsegmentation using DeepLabV3. Additionally, we present a novel, highly\nefficient linear-optimization-based approach to extract counts from the\ngenerated segmentation maps while accounting for image object depth and, thus,\nfor passenger dispersion along a platform. Tested on a privacy-preserving\ndataset created in collaboration with the Washington Metropolitan Area Transit\nAuthority (WMATA) that encompasses more than 600 hours of video material, our\nresults demonstrate that computer vision approaches can provide substantive\nvalue for crowd estimation. This work demonstrates that CCTV image data,\nindependent of other data sources available to a transit agency, can enable\nmore precise real-time crowding estimation and, eventually, timely operational\nresponses for platform crowding mitigation.", "AI": {"tldr": "研究通过三种先进的计算机视觉方法从平台CCTV图像中提取与人群相关的特征来评估CCTV影像对估算人群数量的潜力，并验证了计算机视觉方法可以为人群估算提供实质性价值。", "motivation": "准确估算城市铁路站台人数可以提高交通机构的运营决策能力，从而提高安全性、运营效率和客户体验，特别是在拥挤的情况下。但实时感应拥挤仍具有挑战性，计算机视觉方法被用以进行改进。", "method": "研究比较了三种先进的计算机视觉方法：使用YOLOv11、RT-DETRv2和APGCC进行目标检测和计数，通过定制训练的Vision Transformer（Crowd-ViT）进行人群级别分类，以及使用DeepLabV3进行语义分割。并且提出了一种新的高效的线性优化方法，以从生成的分割图中提取计数。", "result": "测试显示，基于计算机视觉的方法可以在没有其它数据的情况下为交通机构提供精确的实时拥挤估算。", "conclusion": "这项工作证明了CCTV影像数据能够独立于其他交通机构可用的数据源提供更精确的实时拥挤估算，并且最终支持及时的操作反应以减轻平台拥堵。"}}
{"id": "2508.03998", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03998", "abs": "https://arxiv.org/abs/2508.03998", "authors": ["Xinyu Zhao", "Zhen Tan", "Maya Enisman", "Minjae Seo", "Marta R. Durantini", "Dolores Albarracin", "Tianlong Chen"], "title": "Transferring Expert Cognitive Models to Social Robots via Agentic Concept Bottleneck Models", "comment": "27 pages, 7 figures", "summary": "Successful group meetings, such as those implemented in group\nbehavioral-change programs, work meetings, and other social contexts, must\npromote individual goal setting and execution while strengthening the social\nrelationships within the group. Consequently, an ideal facilitator must be\nsensitive to the subtle dynamics of disengagement, difficulties with individual\ngoal setting and execution, and interpersonal difficulties that signal a need\nfor intervention. The challenges and cognitive load experienced by facilitators\ncreate a critical gap for an embodied technology that can interpret social\nexchanges while remaining aware of the needs of the individuals in the group\nand providing transparent recommendations that go beyond powerful but \"black\nbox\" foundation models (FMs) that identify social cues. We address this\nimportant demand with a social robot co-facilitator that analyzes multimodal\nmeeting data and provides discreet cues to the facilitator. The robot's\nreasoning is powered by an agentic concept bottleneck model (CBM), which makes\ndecisions based on human-interpretable concepts like participant engagement and\nsentiments, ensuring transparency and trustworthiness. Our core contribution is\na transfer learning framework that distills the broad social understanding of\nan FM into our specialized and transparent CBM. This concept-driven system\nsignificantly outperforms direct zero-shot FMs in predicting the need for\nintervention and enables real-time human correction of its reasoning.\nCritically, we demonstrate robust knowledge transfer: the model generalizes\nacross different groups and successfully transfers the expertise of senior\nhuman facilitators to improve the performance of novices. By transferring an\nexpert's cognitive model into an interpretable robotic partner, our work\nprovides a powerful blueprint for augmenting human capabilities in complex\nsocial domains.", "AI": {"tldr": "本研究开发了一种社交机器人辅助会议协调，通过一个基于人类可理解概念进行决策的模型，辅助提高会议效率和促进群体社交关系。", "motivation": "研究的动机在于填补一个关键空白：开发一种能够阐释社交互动，并能感知群体中个体需求以提供建议的具身技术。这种技术超越了仅能识别社交线索的强大但黑箱模型。", "method": "本研究开发了一种社交机器人辅助协调员，该机器人能够分析多模态会议数据并提供对协调员的微妙提示。其决策基于代理概念瓶颈模型（CBM），基于参与者参与度和情感等人类可理解的概念进行决策，保证了透明度和可信度。同时，研究还提出了一种迁移学习框架，可将基础模型的广泛社交理解提炼到透明的CBM中。", "result": "研究结果显示，这种概念驱动的系统在预测干预需求时显著优于基础模型的直接零射击预测，并允许实现实时的对机器逻辑的人类纠正。该模型在不同群体间表现出良好的知识转移能力，并能够将资深协调员的专长转移给新手。", "conclusion": "本研究为复杂社交领域的增强人类功能提供了强有力的设计方案。通过转移专家的认知模型到一个可解释的机器人伙伴，这一解决方案不仅提高了会议效率，还增强了群体间的社交网络。"}}
