<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 9]
- [cs.CV](#cs.CV) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Enriching Historical Records: An OCR and AI-Driven Approach for Database Integration](https://arxiv.org/abs/2512.23710)
*Zahra Abedi,Richard M. K. van Dijk,Gijs Wijnholds,Tessa Verhoef*

Main category: cs.CL

> 本研究提出了一种自动化流程，用于解释数字化的历史文献，该流程结合了OCR技术和高级生成式AI模型，可以实现历史文档数据的有效提取与链接。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是开发一种自动化流水线，将历史文档图像中的数据与现有的高质量数据库记录整合，解决布局变化和术语不同的问题。

**Method:** 本研究利用OCR技术、基于LLM的数据提取约束以及数据库链接方法，将1983至1985年间出版的关于莱顿大学教授和策展人传记资料的书籍数字化并进行分析，旨在设计一个自动化的数据整合流程。

**Result:** 研究结果表明OCR技术达到了1.08%的字符错误率和5.06%的单词错误率，同时实现了63%到65%的JSON数据提取准确率。数据库链接算法成功识别了94%的标注JSON文件和81%的OCR生成JSON文件。

**Conclusion:** 研究结论指出，此自动化流程为数字人文研究提供了一种解释数字化历史文献的方法，足以应对布局多样性和术语差异等挑战。

**Abstract:** This research digitizes and analyzes the Leidse hoogleraren en lectoren 1575-1815 books written between 1983 and 1985, which contain biographic data about professors and curators of Leiden University. It addresses the central question: how can we design an automated pipeline that integrates OCR, LLM-based interpretation, and database linking to harmonize data from historical document images with existing high-quality database records? We applied OCR techniques, generative AI decoding constraints that structure data extraction, and database linkage methods to process typewritten historical records into a digital format. OCR achieved a Character Error Rate (CER) of 1.08 percent and a Word Error Rate (WER) of 5.06 percent, while JSON extraction from OCR text achieved an average accuracy of 63 percent and, based on annotated OCR, 65 percent. This indicates that generative AI somewhat corrects low OCR performance. Our record linkage algorithm linked annotated JSON files with 94% accuracy and OCR-derived JSON files with 81%. This study contributes to digital humanities research by offering an automated pipeline for interpreting digitized historical documents, addressing challenges like layout variability and terminology differences, and exploring the applicability and strength of an advanced generative AI model.

</details>


### [2] [CAT: A Metric-Driven Framework for Analyzing the Consistency-Accuracy Relation of LLMs under Controlled Input Variations](https://arxiv.org/abs/2512.23711)
*Paulo Cavalin,Cassia Sanctos,Marcelo Grave,Claudio Pinhanez,Yago Primerano*

Main category: cs.CL

> 提出\textsc{CAT}框架来评估和可视化LLMs的一致性和准确性之间的相互作用，并使用CAR曲线和CORE指数进行量化。框架不仅对多项选择题基准有应用，还可以扩展到其他类型的评估。

<details>
  <summary>Details</summary>

**Motivation:** 当前的评估实践主要关注模型的能力，如准确性或基准分数，最近也开始重视一致性作为部署LLMs在高风险、现实世界应用中的一个重要属性。本文认为，虽然应独立评估这两个维度，但它们之间的相互依赖关系也需要考虑，以便对LLMs进行更精细的评估。

**Method:** 介绍了一个名为\textsc{CAT}的框架，用于评估和可视化大型语言模型（LLMs）在可控输入变化下的准确性和响应一致性之间的相互作用，主要使用多项选择题（MC）基准作为案例研究。框架的核心是\emph{一致性和准确性关系（CAR）曲线}，它展示了随着一致性要求的提高，模型的准确性如何变化，其中定义了一致性最低准确性(MCA)指标。还提出了\emph{一致性导向的鲁棒性估计（CORE）指数}，作为一个全局指标，用来量化准确性和一致性之间的权衡。

**Result:** 展示了该框架在一组多样化的通用和领域特定的LLMs上的实际应用，这些模型在多项MC基准上进行评估。此外，还概述了如何将\textsc{CAT}扩展到MC任务之外，以支持通过可适应的评分函数进行长文本和开放性评估。

**Conclusion:** 通过介绍和演示\textsc{CAT}框架，强调了在评估LLMs时考虑一致性和准确性之间的相互依赖关系的重要性，以及这个框架在不同类型任务中的潜在应用。

**Abstract:** We introduce \textsc{CAT}, a framework designed to evaluate and visualize the \emph{interplay} of \emph{accuracy} and \emph{response consistency} of Large Language Models (LLMs) under controllable input variations, using multiple-choice (MC) benchmarks as a case study. Current evaluation practices primarily focus on model capabilities such as accuracy or benchmark scores and, more recently, measuring consistency is being considered an essential property for deploying LLMs in high-stake, real-world applications. We argue in this paper that although both dimensions should still be evaluated independently, their inter-dependency also need to be considered for a more nuanced evaluation of LLMs. At the core of \textsc{CAT} are the \emph{Consistency-Accuracy Relation (CAR)} curves, which visualize how model accuracy varies with increasing consistency requirements, as defined by the \emph{Minimum-Consistency Accuracy (MCA)} metric. We further propose the \emph{Consistency-Oriented Robustness Estimate (CORE)} index, a global metric that combines the area and shape of the CAR curve to quantify the trade-off between accuracy and consistency. We present a practical demonstration of our framework across a diverse set of generalist and domain-specific LLMs, evaluated on multiple MC benchmarks. We also outline how \textsc{CAT} can be extended beyond MC tasks to support long-form, open-ended evaluations through adaptable scoring functions.

</details>


### [3] [STED and Consistency Scoring: A Framework for Evaluating LLM Structured Output Reliability](https://arxiv.org/abs/2512.23712)
*Guanghui Wang,Jinze Yu,Xing Zhang,Dayuan Jiang,Yin Song,Tomal Deb,Xuefeng Liu,Peiyang He*

Main category: cs.CL

> 该论文提出了一种评估和提升大语言模型（LLMs）生成结构化数据一致性的综合框架，框架包括一个新度量STED（语义树编辑距离）和一个一致性评分框架，展示了其在多个实验和基准测试中的优越性能，提供了一套理论基础和实践工具，以确保LLMs在生成结构化数据时的可靠性。

<details>
  <summary>Details</summary>

**Motivation:** 大语言模型在结构化数据生成的应用中越来越多，但是输出的一致性对于实际生产应用依然至关重要。为了提高一致性和可靠性，本文提供了一个新的评估框架。

**Method:** 框架利用STED度量（一种平衡语义灵活性和结构严谨性的新型相似性度量）以及一个整合多次STED测量一致性评分框架，来进行结构化数据的生成和评估。

**Result:** 通过系统的实验和基准测试，STED的表现优于现有的TED、BERTScore和DeepDiff等度量方式，可以辨识结构的重大差异并保持对语义等价物的高相似度。

**Conclusion:** 该框架可以帮助选择适合结构化任务的模型、迭代优化提示词以获得可重复结果，并诊断识别一致性问题的根源，为LLMs在实际生产系统中的可靠应用提供了理论基础和实用工具。

**Abstract:** Large Language Models (LLMs) are increasingly deployed for structured data generation, yet output consistency remains critical for production applications. We introduce a comprehensive framework for evaluating and improving consistency in LLM-generated structured outputs. Our approach combines: (1) STED (Semantic Tree Edit Distance), a novel similarity metric balancing semantic flexibility with structural strictness when comparing JSON outputs, and (2) a consistency scoring framework aggregating multiple STED measurements across repeated generations to quantify reliability. Through systematic experiments on synthetic datasets with controlled schema, expression, and semantic variations, we demonstrate STED achieves superior performance ($0.86-0.90$ similarity for semantic equivalents, $0.0$ for structural breaks) compared to existing metrics including TED, BERTScore, and DeepDiff. Applying our framework to benchmark six LLMs reveals significant variations: Claude-3.7-Sonnet demonstrates exceptional consistency, maintaining near-perfect structural reliability even at high temperatures ($T=0.9$), while models like Claude-3-Haiku and Nova-Pro exhibit substantial degradation requiring careful tuning. Our framework enables practical applications including targeted model selection for structured tasks, iterative prompt refinement for reproducible results, and diagnostic analysis to identify inconsistency root causes. This work provides theoretical foundations and practical tools for ensuring reliable structured output generation in LLM-based production systems.

</details>


### [4] [PyBangla at BLP-2025 Task 2: Enhancing Bangla-to-Python Code Generation with Iterative Self-Correction and Multilingual Agents](https://arxiv.org/abs/2512.23713)
*Jahidul Islam,Md Ataullha,Saiful Azad*

Main category: cs.CL

> 研究人员通过BanglaCodeAct框架解决了孟加拉语指令到Python代码的生成问题，未使用针对特定任务的微调，而是采用多智能体提示和迭代自我修正。Qwen3-8B在开发集上的性能最佳，证明了代理推理在低资源语言中生成代码的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管LLMs在从英语提示生成代码方面表现出色，但这种进步尚未延伸到低资源语言。研究旨在解决孟加拉语到Python代码生成的问题。

**Method:** 介绍BanglaCodeAct框架，该框架基于多智能体提示和迭代自我修正，使用开源的多语言LLM在Thought-Code-Observation循环中动态生成、测试和改进从孟加拉语指令到Python代码的转换。

**Result:** 实验结果显示，Qwen3-8B在使用BanglaCodeAct时，在mHumanEval数据集的开发集上达到了94.0%的通过率，在盲测集上达到了71.6%的通过率。

**Conclusion:** 这些实验结果证明了基于代理的推理在低资源语言中生成可靠代码的潜力，为孟加拉语到Python的翻译设立了新的基准。

**Abstract:** LLMs excel at code generation from English prompts, but this progress has not extended to low-resource languages. We address Bangla-to-Python code generation by introducing BanglaCodeAct, an agent-based framework that leverages multi-agent prompting and iterative self-correction. Unlike prior approaches relying on task-specific fine-tuning, BanglaCodeAct employs an open-source multilingual LLM within a Thought-Code-Observation loop, enabling dynamic generation, testing, and refinement of code from Bangla instructions. We benchmark several small-parameter open-source LLMs and evaluate their effectiveness on the mHumanEval dataset for Bangla NL2Code. Our results show that Qwen3-8B, when deployed with BanglaCodeAct, achieves the best performance, with pass@1 accuracy of 94.0\% on the development set and 71.6\% on the blind test set. These results establish a new benchmark for Bangla-to-Python translation and highlight the potential of agent-based reasoning for reliable code generation in low-resource languages. Experimental scripts are publicly available at github.com/jahidulzaid/PyBanglaCodeActAgent.

</details>


### [5] [PharmaShip: An Entity-Centric, Reading-Order-Supervised Benchmark for Chinese Pharmaceutical Shipping Documents](https://arxiv.org/abs/2512.23714)
*Tingwei Xie,Tianyi Zhou,Yonghong Song*

Main category: cs.CL

> 研究团队发布了PharmaShip数据集，通过对比实验分析了不同模型在序列实体识别、关系抽取、阅读顺序预测任务上的表现，并指出阅读顺序正则化和像素、显式几何属性的结合能提升模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在通过引入PharmaShip数据集，为药品领域提供一个安全关键文档理解的受控、可重现的基准测试，并突出序列感知约束作为结构建模的可迁移偏置。

**Method:** 我们介绍了PharmaShip，这是一个针对预训练文本布局模型在噪声OCR和异构模板下的压力测试的真实世界中药物流扫描文档数据集。PharmaShip涵盖了三个互补任务：序列实体识别（SER）、关系抽取（RE）和阅读顺序预测（ROP），并采用实体中心的评估协议以减少架构间的影响。

**Result:** 实验结果表明，像素和显式几何属性提供了互补的归纳偏置，但各自都是不够的：注入阅读顺序导向的正则化可以提高序列实体识别和EL的性能，同时延长位置覆盖稳定了在页末的预测并减少了截断伪影。

**Conclusion:** PharmaShip数据集为医药领域安全关键文档理解建立了受控的、可重复的基准，强调了序列感知约束作为结构建模的可迁移偏置的重要性。数据集已在GitHub上公开发布。

**Abstract:** We present PharmaShip, a real-world Chinese dataset of scanned pharmaceutical shipping documents designed to stress-test pre-trained text-layout models under noisy OCR and heterogeneous templates. PharmaShip covers three complementary tasks-sequence entity recognition (SER), relation extraction (RE), and reading order prediction (ROP)-and adopts an entity-centric evaluation protocol to minimize confounds across architectures. We benchmark five representative baselines spanning pixel-aware and geometry-aware families (LiLT, LayoutLMv3-base, GeoLayoutLM and their available RORE-enhanced variants), and standardize preprocessing, splits, and optimization. Experiments show that pixels and explicit geometry provide complementary inductive biases, yet neither alone is sufficient: injecting reading-order-oriented regularization consistently improves SER and EL and yields the most robust configuration, while longer positional coverage stabilizes late-page predictions and reduces truncation artifacts. ROP is accurate at the word level but challenging at the segment level, reflecting boundary ambiguity and long-range crossings. PharmaShip thus establishes a controlled, reproducible benchmark for safety-critical document understanding in the pharmaceutical domain and highlights sequence-aware constraints as a transferable bias for structure modeling. We release the dataset at https://github.com/KevinYuLei/PharmaShip.

</details>


### [6] [Noise-Driven Persona Formation in Reflexive Neural Language Generation](https://arxiv.org/abs/2512.23716)
*Toshiyuki Shigemura*

Main category: cs.CL

> 本文介绍了一种通过注入随机噪声种子在大型语言模型中观察人格涌现的方法，揭示了三种稳定人格模式，并证明了噪声可以诱发生成动态中的相变，具有显著的统计意义。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于理解噪声如何影响语言模型的行为，并探讨人格在大型语言模型中的涌现机制。

**Method:** 本文介绍了Luca-Noise Reflex Protocol (LN-RP)，这是一个用于分析大型语言模型中噪声驱动的人格涌现的计算框架。通过在初始生成状态中注入随机噪声种子，观察跨越152个生成周期的非线性过渡。

**Result:** 研究结果揭示了三种具有不同熵特征的稳定人格模式，并证明了外部噪声源可以可靠地诱发反射生成动态中的相变。定量评估证实了人格的持续保持以及模式间的显著差异。

**Conclusion:** 该协议提供了一种可重复的方法来研究反射生成、涌现行为以及大型语言模型中的长距离语言连贯性。

**Abstract:** This paper introduces the Luca-Noise Reflex Protocol (LN-RP), a computational framework for analyzing noise-driven persona emergence in large language models. By injecting stochastic noise seeds into the initial generation state, we observe nonlinear transitions in linguistic behavior across 152 generation cycles. Our results reveal three stable persona modes with distinct entropy signatures, and demonstrate that external noise sources can reliably induce phase transitions in reflexive generation dynamics. Quantitative evaluation confirms consistent persona retention and significant differences across modes (p < 0.01). The protocol provides a reproducible method for studying reflexive generation, emergent behavior, and longrange linguistic coherence in LLMs.

</details>


### [7] [HarmTransform: Transforming Explicit Harmful Queries into Stealthy via Multi-Agent Debate](https://arxiv.org/abs/2512.23717)
*Shenzhe Zhu*

Main category: cs.CL

> 本文介绍了HarmTransform，这是一种多智能体辩论框架，用于系统地将有害查询转化为更隐蔽的形式，同时保留其潜在的恶意意图。实验表明，HarmTransform在生成有效的查询变换方面显著优于标准基线。同时，辩论在增加转化的有效性和隐蔽性的同时，也可能带来话题偏离和不必要的复杂性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的LLMs安全机制主要针对显而易见的危险内容，忽视了更微妙的威胁。恶意用户可以通过隐晦的重组来规避这些安全措施，导致安全训练数据的缺口。HarmTransform旨在填补这一空白。

**Method:** 本文提出的HarmTransform框架使用多智能体系统迭代地批评和改进，以生成高质量、隐蔽的有害查询转换。

**Result:** 实验显示，HarmTransform相较于标准基线，能更有效地生成查询转换。但同时也发现，辩论机制可能引入话题偏离和不必要的复杂性。

**Conclusion:** 本文强调了多智能体辩论系统在生成全面的安全训练数据方面的潜力和局限性。

**Abstract:** Large language models (LLMs) are equipped with safety mechanisms to detect and block harmful queries, yet current alignment approaches primarily focus on overtly dangerous content and overlook more subtle threats. However, users can often disguise harmful intent through covert rephrasing that preserves malicious objectives while appearing benign, which creates a significant gap in existing safety training data. To address this limitation, we introduce HarmTransform, a multi-agent debate framework for systematically transforming harmful queries into stealthier forms while preserving their underlying harmful intent. Our framework leverages iterative critique and refinement among multiple agents to generate high-quality, covert harmful query transformations that can be used to improve future LLM safety alignment. Experiments demonstrate that HarmTransform significantly outperforms standard baselines in producing effective query transformations. At the same time, our analysis reveals that debate acts as a double-edged sword: while it can sharpen transformations and improve stealth, it may also introduce topic shifts and unnecessary complexity. These insights highlight both the promise and the limitations of multi-agent debate for generating comprehensive safety training data.

</details>


### [8] [Emergent World Beliefs: Exploring Transformers in Stochastic Games](https://arxiv.org/abs/2512.23722)
*Adam Kamel,Tanish Rastogi,Michael Ma,Kailash Ranganathan,Kevin Zhu*

Main category: cs.CL

> 该研究探讨了大型语言模型（LLM）在不完全信息环境（如德州扑克POMDP）中学习理论信念状态的能力，表明LLM能够在无显式指令下学习和发展其对复杂随机环境的理解。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于扩展先前关于大型语言模型在完全信息游戏中自我发展世界模型的研究，特别是探索LLM在不完全信息环境中的表现，选择德州扑克作为研究对象，因为它是一个经典的不完全可观测马尔可夫决策过程（POMDP）。

**Method:** 研究方法是在Poker Hand History (PHH) 数据上预训练类似GPT的模型，随后对模型的内部激活进行探测，使用非线性探测器来解码这些激活，分析它们与扑克游戏中理论状态的关联。

**Result:** 研究结果展示了预训练的基于GPT的模型在扑克牌历史数据上能够学习到诸如手牌排名等确定性结构以及权益等随机特征，这些特征在没有显式指令的情况下被模型掌握。通过主要使用非线性探测器，研究进一步表明这些表示是可以解码的，并与德州扑克的理论信念状态相关联，这表明大规模语言模型学习到了自己的不完全信息环境表示。

**Conclusion:** 结论指出，大型语言模型能够自发地构建不完全信息环境下（如德州扑克）的理论信念状态表示，这为理解模型在处理随机和不确定决策场景中的内部机制提供了洞见。

**Abstract:** Transformer-based large language models (LLMs) have demonstrated strong reasoning abilities across diverse fields, from solving programming challenges to competing in strategy-intensive games such as chess. Prior work has shown that LLMs can develop emergent world models in games of perfect information, where internal representations correspond to latent states of the environment. In this paper, we extend this line of investigation to domains of incomplete information, focusing on poker as a canonical partially observable Markov decision process (POMDP). We pretrain a GPT-style model on Poker Hand History (PHH) data and probe its internal activations. Our results demonstrate that the model learns both deterministic structure, such as hand ranks, and stochastic features, such as equity, without explicit instruction. Furthermore, by using primarily nonlinear probes, we demonstrated that these representations are decodeable and correlate with theoretical belief states, suggesting that LLMs are learning their own representation of the stochastic environment of Texas Hold'em Poker.

</details>


### [9] [When in Doubt, Deliberate: Confidence-Based Routing to Expert Debate for Sexism Detection](https://arxiv.org/abs/2512.23732)
*Anwar Alajmi,Gabriele Pergola*

Main category: cs.CL

> 本文提出了一种两阶段框架，旨在改善在线性别歧视内容检测，特别是在数据稀缺、噪声和概念模糊等情况下的表现。

<details>
  <summary>Details</summary>

**Motivation:** 在线性别歧视内容常以微妙的形式出现，这给现有检测方法带来了挑战。

**Method:** 研究提出的方法包含目标训练过程和选择性推理过程。在训练阶段，使用类别平衡焦点损失、类别感知的批量处理和后校准阈值来减少标签不均衡和标签噪声问题；在推理阶段，通过动态路由机制来分类具有高信心的案例，并将不确定的案例路由到一个名为协作专家判断（CEJ）模块中整合多个判断依据。

**Result:** 该方法在多个基准测试中达到了最先进的结果，在EXIST 2025任务1.1的F1分数提高了+2.72%，EDOS任务A和B分别提高了+4.48%和+1.30%。

**Conclusion:** 该研究通过一种新的两阶段框架显著提高了在线性别歧视内容检测的准确性，尤其是在面临数据稀缺、噪声和概念模糊等挑战时的表现。

**Abstract:** Sexist content online increasingly appears in subtle, context-dependent forms that evade traditional detection methods. Its interpretation often depends on overlapping linguistic, psychological, legal, and cultural dimensions, which produce mixed and sometimes contradictory signals, even in annotated datasets. These inconsistencies, combined with label scarcity and class imbalance, result in unstable decision boundaries and cause fine-tuned models to overlook subtler, underrepresented forms of harm. Together, these limitations point to the need for a design that explicitly addresses the combined effects of (i) underrepresentation, (ii) noise, and (iii) conceptual ambiguity in both data and model predictions. To address these challenges, we propose a two-stage framework that unifies (i) targeted training procedures to adapt supervision to scarce and noisy data with (ii) selective, reasoning-based inference to handle ambiguous or borderline cases. Our training setup applies class-balanced focal loss, class-aware batching, and post-hoc threshold calibration to mitigate label imbalance and noisy supervision. At inference time, a dynamic routing mechanism classifies high-confidence cases directly and escalates uncertain instances to a novel \textit{Collaborative Expert Judgment} (CEJ) module, which prompts multiple personas and consolidates their reasoning through a judge model. Our approach achieves state-of-the-art results across several benchmarks, with a +2.72\% improvement in F1 on the EXIST 2025 Task 1.1, and a gains of +4.48\% and +1.30\% on the EDOS Tasks A and B, respectively.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [10] [Leveraging Synthetic Priors for Monocular Depth Estimation in Specular Surgical Environments](https://arxiv.org/abs/2512.23786)
*Ankan Aich,Yangming Lee*

Main category: cs.CV

> 提出了一种新的单目深度估计方法，该方法通过合成先验和低秩适配技术克服了现有方法在内窥手术环境中的性能瓶颈，并在实验中取得了显著的性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 现有的自监督方法通常依赖于使用带有噪点的真实世界伪标签训练的基础模型，这导致在反射强烈、充满体液的内窥手术环境中的薄手术工具和透明表面边缘塌陷。我们通过引入一个物理分层评估协议来解决这一问题，该协议能够严格量化在高反射环境下性能表现。

**Method:** 通过利用Depth Anything V2架构提供的高保真合成先验，该架构能精确捕捉薄结构的几何细节，结合使用Dynamic Vector Low-Rank Adaptation (DV-LORA)来将这些先验高效地适应医学领域，减少参数预算同时克服合成到真实世界的鸿沟。

**Result:** 我们的方法在SCARED数据集上建立了新的基线，实现了<1.25的精度98.1%，并且相较于现有基准线，将平均平方相对误差减少了17%以上，展示了该方法在不利手术照明条件下的优越鲁棒性。

**Conclusion:** 提出了一个结合高保真合成先验和动态向量低秩适应的方法，证明其在医学图像中的优越表现，特别是在具有高反射性的环境中。

**Abstract:** Accurate Monocular Depth Estimation (MDE) is critical for robotic surgery but remains fragile in specular, fluid-filled endoscopic environments. Existing self-supervised methods, typically relying on foundation models trained with noisy real-world pseudo-labels, often suffer from boundary collapse on thin surgical tools and transparent surfaces. In this work, we address this by leveraging the high-fidelity synthetic priors of the Depth Anything V2 architecture, which inherently captures precise geometric details of thin structures. We efficiently adapt these priors to the medical domain using Dynamic Vector Low-Rank Adaptation (DV-LORA), minimizing the parameter budget while bridging the synthetic-to-real gap. Additionally, we introduce a physically-stratified evaluation protocol on the SCARED dataset to rigorously quantify performance in high-specularity regimes often masked by aggregate metrics. Our approach establishes a new state-of-the-art, achieving an accuracy (< 1.25) of 98.1% and reducing Squared Relative Error by over 17% compared to established baselines, demonstrating superior robustness in adverse surgical lighting.

</details>


### [11] [Video-Based Performance Evaluation for ECR Drills in Synthetic Training Environments](https://arxiv.org/abs/2512.23819)
*Surya Rayala,Marcos Quinones-Grueiro,Naveeduddin Mohammed,Ashwin T S,Benjamin Goldberg,Randall Spain,Paige Lawton,Gautam Biswas*

Main category: cs.CV

> 本文提出了一种基于视频的评估系统，它可以自动分析训练视频中的士兵表现，而无需额外的传感器。通过计算机视觉技术，系统可以提取2D骨架、注视向量和移动轨迹，从而开发特定任务的性能指标，包括心理运动流畅性、情境感知和团队协作。这种系统可以为军事训练提供可操作的具体度量，并将其整合到战术演练分析中。

<details>
  <summary>Details</summary>

**Motivation:** 由于传统的训练评估方法依赖于成本高昂的人工传感器或主观的人类观察，限制了其可扩展性和准确性，因此迫切需要一种更为客观和自动化的性能评估方法。

**Method:** Structure

**Result:** <tool_call>
{{"name": "Structure", "arguments": {"tldr": "本文提出了一种基于视频的评估系统，它可以自动分析训练视频中的士兵表现，而无需额外的传感器。通过计算机视觉技术，系统可以提取2D骨架、注视向量和移动轨迹，从而开发特定任务的性能指标，包括心理运动流畅性、情境感知和团队协作。这种系统可以为军事训练提供可操作的具体度量，并将其整合到战术演练分析中。", "motivation": "由于传统的训练评估方法依赖于成本高昂的人工传感器或主观的人类观察，限制了其可扩展性和准确性，因此迫切需要一种更为客观和自动化的性能评估方法。", "method": "该论文使用计算机视觉技术从训练视频中导出２Ｄ骨架、注视向量和移动轨迹，并利用这些数据制定特定任务的性能指标，如心理运动能力、情境感知和团队协作，以此来衡量士兵在战术演练中的表现。`, "result": "通过一个真实世界的进入和清理房间（ECR）演练案例，本文展示了如何使用此系统生成可操作的度量值，从而方便地进行事后评估，并提供具有互动性的评估仪表板。", "conclusion": "论文指出其方法的局限性，诸如跟踪难题、事实验证及其广泛应用的可行性，并指出未来的研究方向，包括扩展到３Ｄ视频数据分析和进一步提高评估系统的可扩展性。"}}}
</tool_call>

**Conclusion:** 论文指出其方法的局限性，诸如跟踪难题、事实验证及其广泛应用的可行性，并指出未来的研究方向，包括扩展到３Ｄ视频数据分析和进一步提高评估系统的可扩展性。

**Abstract:** Effective urban warfare training requires situational awareness and muscle memory, developed through repeated practice in realistic yet controlled environments. A key drill, Enter and Clear the Room (ECR), demands threat assessment, coordination, and securing confined spaces. The military uses Synthetic Training Environments that offer scalable, controlled settings for repeated exercises. However, automatic performance assessment remains challenging, particularly when aiming for objective evaluation of cognitive, psychomotor, and teamwork skills. Traditional methods often rely on costly, intrusive sensors or subjective human observation, limiting scalability and accuracy. This paper introduces a video-based assessment pipeline that derives performance analytics from training videos without requiring additional hardware. By utilizing computer vision models, the system extracts 2D skeletons, gaze vectors, and movement trajectories. From these data, we develop task-specific metrics that measure psychomotor fluency, situational awareness, and team coordination. These metrics feed into an extended Cognitive Task Analysis (CTA) hierarchy, which employs a weighted combination to generate overall performance scores for teamwork and cognition. We demonstrate the approach with a case study of real-world ECR drills, providing actionable, domain specific metrics that capture individual and team performance. We also discuss how these insights can support After Action Reviews with interactive dashboards within Gamemaster and the Generalized Intelligent Framework for Tutoring (GIFT), providing intuitive and understandable feedback. We conclude by addressing limitations, including tracking difficulties, ground-truth validation, and the broader applicability of our approach. Future work includes expanding analysis to 3D video data and leveraging video analysis to enable scalable evaluation within STEs.

</details>


### [12] [Pretraining Frame Preservation in Autoregressive Video Memory Compression](https://arxiv.org/abs/2512.23851)
*Lvmin Zhang,Shengqu Cai,Muyang Li,Chong Zeng,Beijia Lu,Anyi Rao,Song Han,Gordon Wetzstein,Maneesh Agrawala*

Main category: cs.CV

> PFP是用于压缩视频并保留高频细节的神经网络，能在保持低上下文成本的同时，实现长历史记忆，适用于自回归视频模型的记忆编码。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探索一种有效保留长视频中重要细节的压缩方法，并考察使用预训练模型进行自回归视频模型的记忆编码的效果。

**Method:** 本研究提出了一种名为PFP的神经网络结构，旨在将长视频压缩为短上下文，并带有明确的预训练目标，即在任意时间点上保留单帧的高频细节。

**Result:** 基线模型能够将一段20秒的视频压缩到大约5k长度的上下文中，并且可以以感知上保持一致的外观随机检索帧。此外，预训练模型可以直接进行微调以用作自回归视频模型的记忆编码器，从而在低上下文成本下实现长历史记忆，并且保真度损失相对较低。

**Conclusion:** 研究讨论了该框架在消融实验设置下的表现，并分析了可能的神经网络架构设计之间的权衡。

**Abstract:** We present PFP, a neural network structure to compress long videos into short contexts, with an explicit pretraining objective to preserve the high-frequency details of single frames at arbitrary temporal positions. The baseline model can compress a 20-second video into a context at about 5k length, where random frames can be retrieved with perceptually preserved appearances. Such pretrained models can be directly fine-tuned as memory encoders for autoregressive video models, enabling long history memory with low context cost and relatively low fidelity loss. We evaluate the framework with ablative settings and discuss the trade-offs of possible neural architecture designs.

</details>


### [13] [Lifelong Domain Adaptive 3D Human Pose Estimation](https://arxiv.org/abs/2512.23860)
*Qucheng Peng,Hongfei Xue,Pu Wang,Chen Chen*

Main category: cs.CV

> 提出了针对3D人体姿态估计新的终身域适应任务，采用了生成对抗网络框架以及新型姿态生成器来克服领域变化和保持先前领域知识，实验证明了其优越性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的3D人体姿态估计方法在面对目标领域数据的非平稳性问题时表现不佳。我们的目标是开发一种新方法，使姿态估计算法定能在不断变化的目标领域中持续学习并保留先前领域的知识。

**Method:** 我们提出了一种名为终身域名适应的3D人体姿态估计任务，该任务中姿态估计算法在预训练后需要不断适应不同的目标领域，并且在适应当前目标领域时无法访问源领域和所有先前的目标领域。为此，我们设计了一个生成对抗网络框架，该框架包含3D姿态生成器、2D姿态判别器和3D姿态估计算法，以缓解领域偏移并使原始和增强姿态对齐。此外，我们还构建了一种新型3D姿态生成器范式，整合了姿态感知、时间感知和领域感知的知识，以增强当前领域的适应性和缓解对先前领域的灾难性遗忘。

**Result:** 我们的方法在多种域适应3D人体姿态估计数据集上展示了优越的性能。

**Conclusion:** 我们提出的终身域适应框架能够有效解决3D人体姿态估计中领域适应性的问题，并通过整合多种感知知识成功缓解了灾难性遗忘问题。实验结果显示了该方法在该领域的卓越表现。

**Abstract:** 3D Human Pose Estimation (3D HPE) is vital in various applications, from person re-identification and action recognition to virtual reality. However, the reliance on annotated 3D data collected in controlled environments poses challenges for generalization to diverse in-the-wild scenarios. Existing domain adaptation (DA) paradigms like general DA and source-free DA for 3D HPE overlook the issues of non-stationary target pose datasets. To address these challenges, we propose a novel task named lifelong domain adaptive 3D HPE. To our knowledge, we are the first to introduce the lifelong domain adaptation to the 3D HPE task. In this lifelong DA setting, the pose estimator is pretrained on the source domain and subsequently adapted to distinct target domains. Moreover, during adaptation to the current target domain, the pose estimator cannot access the source and all the previous target domains. The lifelong DA for 3D HPE involves overcoming challenges in adapting to current domain poses and preserving knowledge from previous domains, particularly combating catastrophic forgetting. We present an innovative Generative Adversarial Network (GAN) framework, which incorporates 3D pose generators, a 2D pose discriminator, and a 3D pose estimator. This framework effectively mitigates domain shifts and aligns original and augmented poses. Moreover, we construct a novel 3D pose generator paradigm, integrating pose-aware, temporal-aware, and domain-aware knowledge to enhance the current domain's adaptation and alleviate catastrophic forgetting on previous domains. Our method demonstrates superior performance through extensive experiments on diverse domain adaptive 3D HPE datasets.

</details>


### [14] [MRI-to-CT Synthesis With Cranial Suture Segmentations Using A Variational Autoencoder Framework](https://arxiv.org/abs/2512.23894)
*Krithika Iyer,Austin Tapp,Athelia Paulli,Gabrielle Dickerson,Syed Muhammad Anwar,Natasha Lepore,Marius George Linguraru*

Main category: cs.CV

> 研究开发了一种基于深度学习的方法，可以从儿童的T1加权MRI生成能够进行详细解剖分析的合成CT图像。

<details>
  <summary>Details</summary>

**Motivation:** 量化规范的儿科颅骨发育和缝合骨化对于诊断和治疗与生长相关的头部疾病至关重要。尽管计算机断层扫描（CT）广泛用于评估颅骨和缝合畸形，但其电离辐射对没有显著异常的儿童不适宜。磁共振成像（MRI）无辐射，软组织对比度更高，但无法显示颅缝、估计颅骨密度或评估颅顶生长。研究所提出的方法旨在填补这一空白。

**Method:** 本研究提出了一种基于深度学习的管道，将0.2至2岁儿童的T1加权MRI图像转化为合成CT（sCT），预测详细的颅骨分割，生成缝合线概率热图，并从热图中直接导出缝合线分割。

**Result:** 通过对自有儿科数据的研究，sCT达到了99%的结构相似度和相对真实CT为1.01的Frechet inception距离。颅骨分割达到了七块颅骨平均85%的Dice系数，缝合线达到了80%的Dice系数。颅骨和缝合线分割在sCT和真实CT之间的一致性被确认。

**Conclusion:** 这是第一个能够通过MRI生成能够提供缝合线分割的儿科颅骨CT合成框架，该方法将强健、特定领域的变分自编码器相结合，从常规儿科MRI生成视觉上几乎无法区分的颅骨sCT，弥补了无创颅骨评估的关键不足。

**Abstract:** Quantifying normative pediatric cranial development and suture ossification is crucial for diagnosing and treating growth-related cephalic disorders. Computed tomography (CT) is widely used to evaluate cranial and sutural deformities; however, its ionizing radiation is contraindicated in children without significant abnormalities. Magnetic resonance imaging (MRI) offers radiation free scans with superior soft tissue contrast, but unlike CT, MRI cannot elucidate cranial sutures, estimate skull bone density, or assess cranial vault growth. This study proposes a deep learning driven pipeline for transforming T1 weighted MRIs of children aged 0.2 to 2 years into synthetic CTs (sCTs), predicting detailed cranial bone segmentation, generating suture probability heatmaps, and deriving direct suture segmentation from the heatmaps. With our in-house pediatric data, sCTs achieved 99% structural similarity and a Frechet inception distance of 1.01 relative to real CTs. Skull segmentation attained an average Dice coefficient of 85% across seven cranial bones, and sutures achieved 80% Dice. Equivalence of skull and suture segmentation between sCTs and real CTs was confirmed using two one sided tests (TOST p < 0.05). To our knowledge, this is the first pediatric cranial CT synthesis framework to enable suture segmentation on sCTs derived from MRI, despite MRI's limited depiction of bone and sutures. By combining robust, domain specific variational autoencoders, our method generates perceptually indistinguishable cranial sCTs from routine pediatric MRIs, bridging critical gaps in non invasive cranial evaluation.

</details>
