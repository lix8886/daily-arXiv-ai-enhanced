{"id": "2508.04797", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04797", "abs": "https://arxiv.org/abs/2508.04797", "authors": ["Mohab Kishawy", "Ali Abdellatif Hussein", "Jun Chen"], "title": "RetinexDual: Retinex-based Dual Nature Approach for Generalized Ultra-High-Definition Image Restoration", "comment": null, "summary": "Advancements in image sensing have elevated the importance of\nUltra-High-Definition Image Restoration (UHD IR). Traditional methods, such as\nextreme downsampling or transformation from the spatial to the frequency\ndomain, encounter significant drawbacks: downsampling induces irreversible\ninformation loss in UHD images, while our frequency analysis reveals that pure\nfrequency-domain approaches are ineffective for spatially confined image\nartifacts, primarily due to the loss of degradation locality. To overcome these\nlimitations, we present RetinexDual, a novel Retinex theory-based framework\ndesigned for generalized UHD IR tasks. RetinexDual leverages two complementary\nsub-networks: the Scale-Attentive maMBA (SAMBA) and the Frequency Illumination\nAdaptor (FIA). SAMBA, responsible for correcting the reflectance component,\nutilizes a coarse-to-fine mechanism to overcome the causal modeling of mamba,\nwhich effectively reduces artifacts and restores intricate details. On the\nother hand, FIA ensures precise correction of color and illumination\ndistortions by operating in the frequency domain and leveraging the global\ncontext provided by it. Evaluating RetinexDual on four UHD IR tasks, namely\nderaining, deblurring, dehazing, and Low-Light Image Enhancement (LLIE), shows\nthat it outperforms recent methods qualitatively and quantitatively. Ablation\nstudies demonstrate the importance of employing distinct designs for each\nbranch in RetinexDual, as well as the effectiveness of its various components.", "AI": {"tldr": "提出RetinexDual，一个基于Retinex理论的UHD图像复原框架，结合两个互补子网络，解决传统方法的缺陷，并在四个任务上表现出色。", "motivation": "传统的图像超分辨率复原方法存在明显缺陷，如极端降采样导致信息不可逆丢失，纯频域方法由于丢失降级局部性而无效。针对这些限制，提出了新的方法。", "method": "提出RetinexDual框架，包含两个互补子网络：SAMBA和FIA。SAMBA用于纠正反射分量，采用粗略到精细的机制，在减少伪影和恢复细节方面表现出色。FIA在频率域操作，纠正颜色和光照失真。", "result": "在去雨、去模糊、去雾、低光图像增强四个任务上，RetinexDual在定性和定量上均优于近期方法。消融研究证明了不同设计在ResinexDual中的重要性及各个组件的有效性。", "conclusion": "实验结果表明，RetinexDual在UHD图像复原上具有显著效果，各组成部分对于提升性能有重要作用。"}}
{"id": "2508.04801", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04801", "abs": "https://arxiv.org/abs/2508.04801", "authors": ["Trong-Thuan Nguyen", "Viet-Tham Huynh", "Thao Thi Phuong Dao", "Ha Nguyen Thi", "Tien To Vu Thuy", "Uyen Hanh Tran", "Tam V. Nguyen", "Thanh Dinh Le", "Minh-Triet Tran"], "title": "ACM Multimedia Grand Challenge on ENT Endoscopy Analysis", "comment": null, "summary": "Automated analysis of endoscopic imagery is a critical yet underdeveloped\ncomponent of ENT (ear, nose, and throat) care, hindered by variability in\ndevices and operators, subtle and localized findings, and fine-grained\ndistinctions such as laterality and vocal-fold state. In addition to\nclassification, clinicians require reliable retrieval of similar cases, both\nvisually and through concise textual descriptions. These capabilities are\nrarely supported by existing public benchmarks. To this end, we introduce\nENTRep, the ACM Multimedia 2025 Grand Challenge on ENT endoscopy analysis,\nwhich integrates fine-grained anatomical classification with image-to-image and\ntext-to-image retrieval under bilingual (Vietnamese and English) clinical\nsupervision. Specifically, the dataset comprises expert-annotated images,\nlabeled for anatomical region and normal or abnormal status, and accompanied by\ndual-language narrative descriptions. In addition, we define three benchmark\ntasks, standardize the submission protocol, and evaluate performance on public\nand private test splits using server-side scoring. Moreover, we report results\nfrom the top-performing teams and provide an insight discussion.", "AI": {"tldr": "ENTRep挑战结合了细粒度解剖分类与图像检索，支持双语环境下的临床评估，促进了ENT内窥镜影像分析的发展。", "motivation": "针对耳鼻喉科内窥镜分析中自动化影像分析的不足，提出ENTRep挑战，提高图像分类与检索能力，满足临床需求。", "method": "整合细粒度解剖分类，图像到图像，文本到图像检索；双语言临床监督；定义三个基准任务；标准化提交和评估协议。", "result": "该摘要介绍了ENTRep，ACM多媒体2025年ENT内窥镜分析的高难度挑战，旨在结合细粒度的解剖分类与图像之间的检索和文本到图像的检索，并使用双语（越南语和英语）临床监督。数据集包含专家标注的解剖区域及正常或异常状态的图像，并附有双语描述。定义了三个基准任务，标准化提交协议，并使用服务器端评分在公共和私人测试集中评估性能。同时，还报告了顶尖团队的结果，并进行了见解分析。", "conclusion": "该摘要展示了ENTRep挑战的重要性及其对于ENT医学影像分析的贡献，并强调了多语言支持和详细的临床应用。"}}
{"id": "2508.04816", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04816", "abs": "https://arxiv.org/abs/2508.04816", "authors": ["Sriram Mandalika", "Lalitha V"], "title": "CoMAD: A Multiple-Teacher Self-Supervised Distillation Framework", "comment": "8 Pages, 2 Figures", "summary": "Numerous self-supervised learning paradigms, such as contrastive learning and\nmasked image modeling, learn powerful representations from unlabeled data but\nare typically pretrained in isolation, overlooking complementary insights and\nyielding large models that are impractical for resource-constrained deployment.\nTo overcome these challenges, we introduce Consensus-oriented Masked\nDistillation (CoMAD), a lightweight, parameter-free framework that unifies\nknowledge from multiple current state-of-the-art self-supervised Vision\nTransformers into a compact student network. CoMAD distills from three\npretrained ViT-Base teachers, MAE, MoCo v3, and iBOT, each offering distinct\nsemantic and contextual priors. Rather than naively averaging teacher outputs,\nwe apply asymmetric masking: the student sees only 25 percent of patches while\neach teacher receives a progressively lighter, unique mask, forcing the student\nto interpolate missing features under richer contexts. Teacher embeddings are\naligned to the student's space via a linear adapter and layer normalization,\nthen fused through our joint consensus gating, which weights each token by\ncombining cosine affinity with inter-teacher agreement. The student is trained\nwith dual-level KL divergence on visible tokens and reconstructed feature maps,\ncapturing both local and global structure. On ImageNet-1K, CoMAD's ViT-Tiny\nachieves 75.4 percent Top-1, an increment of 0.4 percent over the previous\nstate-of-the-art. In dense-prediction transfers, it attains 47.3 percent mIoU\non ADE20K, and 44.5 percent box average precision and 40.5 percent mask average\nprecision on MS-COCO, establishing a new state-of-the-art in compact SSL\ndistillation.", "AI": {"tldr": "CoMAD unifies and distills the knowledge from multiple self-supervised Vision Transformers into a compact model, improving performance and practicality.", "motivation": "To overcome the challenges of large, impractical models and the isolation of knowledge in self-supervised learning, leading to a more practical and efficient model.", "method": "Consensus-oriented Masked Distillation (CoMAD), a lightweight framework that unifies knowledge from multiple self-supervised Vision Transformers into a smaller student network. It uses asymmetric masking, linear adapter, layer normalization, joint consensus gating, and dual-level KL divergence for training.", "result": "CoMAD's ViT-Tiny achieves 75.4% Top-1 on ImageNet-1K, setting new records in dense-prediction transfers on ADE20K and MS-COCO.", "conclusion": "CoMAD demonstrates its effectiveness in creating a compact model that outperforms previous state-of-the-arts, offering a practical solution to self-supervised learning for resource-constrained environments."}}
{"id": "2508.04818", "categories": ["cs.CV", "eess.IV", "stat.ML", "62H35, 68T07, 62M40, 68T45", "I.2.6; I.2.10; I.4.6; I.4.8; I.5.1; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.04818", "abs": "https://arxiv.org/abs/2508.04818", "authors": ["Mehrdad Moradi", "Marco Grasso", "Bianca Maria Colosimo", "Kamran Paynabar"], "title": "Single-Step Reconstruction-Free Anomaly Detection and Segmentation via Diffusion Models", "comment": "9 pages, 8 figures, 2 tables. Submitted to an IEEE conference", "summary": "Generative models have demonstrated significant success in anomaly detection\nand segmentation over the past decade. Recently, diffusion models have emerged\nas a powerful alternative, outperforming previous approaches such as GANs and\nVAEs. In typical diffusion-based anomaly detection, a model is trained on\nnormal data, and during inference, anomalous images are perturbed to a\npredefined intermediate step in the forward diffusion process. The\ncorresponding normal image is then reconstructed through iterative reverse\nsampling.\n  However, reconstruction-based approaches present three major challenges: (1)\nthe reconstruction process is computationally expensive due to multiple\nsampling steps, making real-time applications impractical; (2) for complex or\nsubtle patterns, the reconstructed image may correspond to a different normal\npattern rather than the original input; and (3) Choosing an appropriate\nintermediate noise level is challenging because it is application-dependent and\noften assumes prior knowledge of anomalies, an assumption that does not hold in\nunsupervised settings.\n  We introduce Reconstruction-free Anomaly Detection with Attention-based\ndiffusion models in Real-time (RADAR), which overcomes the limitations of\nreconstruction-based anomaly detection. Unlike current SOTA methods that\nreconstruct the input image, RADAR directly produces anomaly maps from the\ndiffusion model, improving both detection accuracy and computational\nefficiency. We evaluate RADAR on real-world 3D-printed material and the\nMVTec-AD dataset. Our approach surpasses state-of-the-art diffusion-based and\nstatistical machine learning models across all key metrics, including accuracy,\nprecision, recall, and F1 score. Specifically, RADAR improves F1 score by 7% on\nMVTec-AD and 13% on the 3D-printed material dataset compared to the next best\nmodel.\n  Code available at: https://github.com/mehrdadmoradi124/RADAR", "AI": {"tldr": "我们提出了RADAR，该方法在无重构的情况下，直接从扩散模型生成异常图，以此克服了现有方法的局限性，并在多个数据集上取得了比现有SOTA方法更好的准确性。", "motivation": "近期，扩散模型作为异常检测和分割的一种强大替代方案，其由于在重构过程中存在的计算开销大、对于复杂或细微模式的重构可能产生不同正常模式的图像以及选择适当的中间噪声水平有难度等问题成为了研究热点。为了解决这些挑战，我们提出了一种新的模型。", "method": "我们提出了RADAR（无重构注意力扩散模型实时异常检测），它直接从扩散模型生成异常图，而非重构输入图像，这不仅提高了检测准确性，还提升了计算效率。", "result": "在MVTec-AD数据集上，我们的方法比次优模型F1提升了7%，在3D打印材料数据集上F1提升达13%。", "conclusion": "RADAR方法在保持高检测准确性的同时，提升了计算效率，适用于实时应用，在多个关键指标上超越了现有的基于扩散模型和统计机器学习的方法。"}}
{"id": "2508.04795", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.04795", "abs": "https://arxiv.org/abs/2508.04795", "authors": ["Thomas Thebaud", "Yen-Ju Lu", "Matthew Wiesner", "Peter Viechnicki", "Najim Dehak"], "title": "Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM", "comment": "Accepted in the 2025 IEEE Automatic Speech Recognition and\n  Understanding Workshop", "summary": "In dialogue transcription pipelines, Large Language Models (LLMs) are\nfrequently employed in post-processing to improve grammar, punctuation, and\nreadability. We explore a complementary post-processing step: enriching\ntranscribed dialogues by adding metadata tags for speaker characteristics such\nas age, gender, and emotion. Some of the tags are global to the entire\ndialogue, while some are time-variant. Our approach couples frozen audio\nfoundation models, such as Whisper or WavLM, with a frozen LLAMA language model\nto infer these speaker attributes, without requiring task-specific fine-tuning\nof either model. Using lightweight, efficient connectors to bridge audio and\nlanguage representations, we achieve competitive performance on speaker\nprofiling tasks while preserving modularity and speed. Additionally, we\ndemonstrate that a frozen LLAMA model can compare x-vectors directly, achieving\nan Equal Error Rate of 8.8% in some scenarios.", "AI": {"tldr": "本文提出了一种在对话转录后处理中添加说话人特征标签的方法，使用冻结的Whisper/WavLM音频基础模型和LLAMA语言模型，展示了在保持模型速度和模块化的同时，具有竞争力的性能。", "motivation": "该研究的动机是在对话转录管道中，大型语言模型（LLM）常常用于改善语法、标点和可读性。本文探讨了一种补充的后处理步骤：通过添加元数据标签来丰富转录对话，标签包括说话人的特征，如年龄、性别和情绪。", "method": "文章的方法是利用冻结的音频基础模型（如Whisper或WavLM）与冻结的LLAMA语言模型相结合，无需对模型进行特定任务的微调，以推断说话人的特征，如年龄、性别和情绪。通过轻量级、高效的连接器来桥接音频和语言表示，从而在保持模块化和速度的同时达到竞争性的说话人剖析任务性能。此外，文章还展示了一个冻结的LLAMA模型可以直接比较x-vectors，在某些场景中达到8.8%的等错误率。", "result": "实验结果表明，该方法在说话人特征剖析任务中具有竞争力，同时保持了模型的模块化和速度优势。", "conclusion": "研究表明，冻结的音频基础模型与冻结的LLAMA语言模型相结合，可有效推断说话人的特征，无需进行任务特定的微调，同时保持了模型的速度和模块化优势。"}}
{"id": "2508.04827", "categories": ["cs.CV", "68T05, 68T07", "I.2.10; I.5.1; I.4.8; J.4"], "pdf": "https://arxiv.org/pdf/2508.04827", "abs": "https://arxiv.org/abs/2508.04827", "authors": ["Chirag Seth", "Divya Naiken", "Keyan Lin"], "title": "A deep learning approach to track eye movements based on events", "comment": null, "summary": "This research project addresses the challenge of accurately tracking eye\nmovements during specific events by leveraging previous research. Given the\nrapid movements of human eyes, which can reach speeds of 300{\\deg}/s, precise\neye tracking typically requires expensive and high-speed cameras. Our primary\nobjective is to locate the eye center position (x, y) using inputs from an\nevent camera. Eye movement analysis has extensive applications in consumer\nelectronics, especially in VR and AR product development. Therefore, our\nultimate goal is to develop an interpretable and cost-effective algorithm using\ndeep learning methods to predict human attention, thereby improving device\ncomfort and enhancing overall user experience. To achieve this goal, we\nexplored various approaches, with the CNN\\_LSTM model proving most effective,\nachieving approximately 81\\% accuracy. Additionally, we propose future work\nfocusing on Layer-wise Relevance Propagation (LRP) to further enhance the\nmodel's interpretability and predictive performance.", "AI": {"tldr": "研究提出了一种基于CNN_LSTM模型的新型眼动追踪方法，能够以较低成本实现高达81%的准确性，并探索了未来增强模型解释性的方向。", "motivation": "鉴于人眼快速移动的特点（速度可达300°/s），传统的精准眼动追踪通常需要昂贵且高速的相机。为解决这一问题并降低追踪成本，要求开发一种可解释且成本效益高的算法来预测人的注意力，以提升设备的舒适度及用户体验。", "method": "利用卷积神经网络与长短期记忆网络（CNN_LSTM）结合的方法来预测人的注意力，实现对眼部中心位置（x, y）的追踪。", "result": "该方法实现了大约81%的准确性。", "conclusion": "研究表明，CNN_LSTM模型在眼动追踪上最为有效。同时提出未来的研究将继续关注层间相关性传播（LRP）技术，以进一步提高模型的可解释性和预测性能。"}}
{"id": "2508.04796", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04796", "abs": "https://arxiv.org/abs/2508.04796", "authors": ["Negar Foroutan", "Clara Meister", "Debjit Paul", "Joel Niklaus", "Sina Ahmadi", "Antoine Bosselut", "Rico Sennrich"], "title": "Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization", "comment": null, "summary": "Tokenization is the first -- and often least scrutinized -- step of most NLP\npipelines. Standard algorithms for learning tokenizers rely on frequency-based\nobjectives, which favor languages dominant in the training data and\nconsequently leave lower-resource languages with tokenizations that are\ndisproportionately longer, morphologically implausible, or even riddled with\n<UNK> placeholders. This phenomenon ultimately amplifies computational and\nfinancial inequalities between users from different language backgrounds. To\nremedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of\nthe widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes\nthe compression gain of the currently worst-compressed language, trading a\nsmall amount of global compression for cross-lingual parity. We find\nempirically that Parity-aware BPE leads to more equitable token counts across\nlanguages, with negligible impact on global compression rate and no substantial\neffect on language-model performance in downstream tasks.", "AI": {"tldr": "提出Parity-aware BPE改进BPE算法，以实现跨语言分词更加公平，无显著负面影响于全局压缩率或语言模型性能。", "motivation": "标准的分词器训练算法依赖于基于频率的目标，这偏向于在训练数据中占主导地位的语言，并且导致低资源语言的分词过长、不符合形态学或者充斥着<UNK>占位符。这一现象最终加剧了不同语言背景用户之间的计算和经济不平等。", "method": "引入了语言公平字节配对编码（Parity-aware Byte Pair Encoding，Parity-aware BPE），这是一种广泛使用的BPE算法的变体。在每一个合并步骤中，Parity-aware BPE最大化当前压缩效果最差的语言的压缩增益，以实现跨语言之间的均衡，即使是在全局压缩率上的增益相对较小。", "result": "实验证明，使用Parity-aware BPE在不同语言之间实现了更公平的词汇计数，对全局压缩率几乎没有影响，且对下游任务中的语言模型性能没有显著影响。", "conclusion": "该方法能够在不显著牺牲全局压缩效率和语言模型性能的情况下，改进低资源语言的分词质量，提高跨语言分词的公平性。"}}
{"id": "2508.04847", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04847", "abs": "https://arxiv.org/abs/2508.04847", "authors": ["Md Zahidul Hasan", "A. Ben Hamza", "Nizar Bouguila"], "title": "LuKAN: A Kolmogorov-Arnold Network Framework for 3D Human Motion Prediction", "comment": null, "summary": "The goal of 3D human motion prediction is to forecast future 3D poses of the\nhuman body based on historical motion data. Existing methods often face\nlimitations in achieving a balance between prediction accuracy and\ncomputational efficiency. In this paper, we present LuKAN, an effective model\nbased on Kolmogorov-Arnold Networks (KANs) with Lucas polynomial activations.\nOur model first applies the discrete wavelet transform to encode temporal\ninformation in the input motion sequence. Then, a spatial projection layer is\nused to capture inter-joint dependencies, ensuring structural consistency of\nthe human body. At the core of LuKAN is the Temporal Dependency Learner, which\nemploys a KAN layer parameterized by Lucas polynomials for efficient function\napproximation. These polynomials provide computational efficiency and an\nenhanced capability to handle oscillatory behaviors. Finally, the inverse\ndiscrete wavelet transform reconstructs motion sequences in the time domain,\ngenerating temporally coherent predictions. Extensive experiments on three\nbenchmark datasets demonstrate the competitive performance of our model\ncompared to strong baselines, as evidenced by both quantitative and qualitative\nevaluations. Moreover, its compact architecture coupled with the linear\nrecurrence of Lucas polynomials, ensures computational efficiency.", "AI": {"tldr": "LuKAN模型使用KAN（Kolmogorov-Arnold Networks）结合Lucas多项式激活函数，通过离散小波变换编码时间信息、空间投影层捕捉关节间依赖性、Temporal Dependency Learner核心模块进行高效函数近似，并通过逆离散小波变换重建3D人体运动序列预测，具有高性能和计算效率。", "motivation": "现有的3D人体运动预测方法在预测准确性和计算效率之间难以取得平衡。", "method": "模型首先使用离散小波变换编码时间信息；然后空间投影层捕捉关节间的结构依赖性；核心的Temporal Dependency Learner利用KAN层和Lucas多项式进行高效的函数近似；最后通过逆小波变换重建运动序列。", "result": "在三个基准数据集上的大量实验显示，该模型在与强基线模型比较时展现出竞争力，并且结合了紧凑的架构和Lucas多项式的线性循环过程以保证计算效率。", "conclusion": "LuKAN模型通过引入KAN和Lucas多项式实现高效准确的3D人体运动预测，并且具备高效的计算性能。"}}
{"id": "2508.04814", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.04814", "abs": "https://arxiv.org/abs/2508.04814", "authors": ["David Sasu", "Natalie Schluter"], "title": "Pitch Accent Detection improves Pretrained Automatic Speech Recognition", "comment": null, "summary": "We show the performance of Automatic Speech Recognition (ASR) systems that\nuse semi-supervised speech representations can be boosted by a complimentary\npitch accent detection module, by introducing a joint ASR and pitch accent\ndetection model. The pitch accent detection component of our model achieves a\nsignificant improvement on the state-of-the-art for the task, closing the gap\nin F1-score by 41%. Additionally, the ASR performance in joint training\ndecreases WER by 28.3% on LibriSpeech, under limited resource fine-tuning. With\nthese results, we show the importance of extending pretrained speech models to\nretain or re-learn important prosodic cues such as pitch accent.", "AI": {"tldr": "通过引入联合ASR和重音检测模型，结合使用半监督语音表示的ASR系统的性能得以提高。重音检测组件显著提升了该任务的F1得分，同时在有限资源微调下，ASR性能也降低了LibriSpeech上的WER。", "motivation": "研究旨在通过集成重音检测模块来提升使用半监督语音表示的ASR系统的性能。", "method": "提出了一种联合ASR和重音检测模型，该模型可以改进重音检测任务，并在联合训练中提高ASR性能。", "result": "重音检测任务的F1得分显著提高，ASR系统的WER在LibriSpeech数据集上减少了28.3%。", "conclusion": "研究结果显示，扩展预训练语音模型以保持或重新学习重要的韵律线索，如重音，对提升ASR性能至关重要。"}}
{"id": "2508.04852", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04852", "abs": "https://arxiv.org/abs/2508.04852", "authors": ["Chenhui Qiang", "Zhaoyang Wei", "Xumeng Han Zipeng Wang", "Siyao Li", "Xiangyuan Lan", "Jianbin Jiao", "Zhenjun Han"], "title": "VER-Bench: Evaluating MLLMs on Reasoning with Fine-Grained Visual Evidence", "comment": "Accept by ACMM2025 Dataset track", "summary": "With the rapid development of MLLMs, evaluating their visual capabilities has\nbecome increasingly crucial. Current benchmarks primarily fall into two main\ntypes: basic perception benchmarks, which focus on local details but lack deep\nreasoning (e.g., \"what is in the image?\"), and mainstream reasoning benchmarks,\nwhich concentrate on prominent image elements but may fail to assess subtle\nclues requiring intricate analysis. However, profound visual understanding and\ncomplex reasoning depend more on interpreting subtle, inconspicuous local\ndetails than on perceiving salient, macro-level objects. These details, though\noccupying minimal image area, often contain richer, more critical information\nfor robust analysis. To bridge this gap, we introduce the VER-Bench, a novel\nframework to evaluate MLLMs' ability to: 1) identify fine-grained visual clues,\noften occupying on average just 0.25% of the image area; 2) integrate these\nclues with world knowledge for complex reasoning. Comprising 374 carefully\ndesigned questions across Geospatial, Temporal, Situational, Intent, System\nState, and Symbolic reasoning, each question in VER-Bench is accompanied by\nstructured evidence: visual clues and question-related reasoning derived from\nthem. VER-Bench reveals current models' limitations in extracting subtle visual\nevidence and constructing evidence-based arguments, highlighting the need to\nenhance models's capabilities in fine-grained visual evidence extraction,\nintegration, and reasoning for genuine visual understanding and human-like\nanalysis. Dataset and additional materials are available\nhttps://github.com/verbta/ACMMM-25-Materials.", "AI": {"tldr": "The paper presents VER-Bench, a new evaluation framework for MLLMs focusing on deep reasoning with subtle visual clues, revealing current models' limitations in this area.", "motivation": "The motivation behind this paper is to address the current limitations of benchmarks in evaluating MLLMs' deep reasoning capabilities from subtle, local visual details critical for profound visual understanding.", "method": "To evaluate MLLMs' visual capabilities, the paper introduces VER-Bench, a novel framework focusing on identifying fine-grained visual clues and integrating them with world knowledge for reasoning.", "result": "VER-Bench highlights that existing models struggle with extracting subtle visual evidence and constructing evidence-based reasoning, indicating a significant room for improvement in fine-grained visual reasoning.", "conclusion": "The conclusion is that to achieve genuine visual understanding and human-like analysis, it's necessary to enhance models' capabilities in extracting and integrating subtle, fine-grained visual evidence."}}
{"id": "2508.04826", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04826", "abs": "https://arxiv.org/abs/2508.04826", "authors": ["Tommaso Tosato", "Saskia Helbling", "Yorguin-Jose Mantilla-Ramos", "Mahmood Hegazy", "Alberto Tosato", "David John Lemay", "Irina Rish", "Guillaume Dumas"], "title": "Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History", "comment": null, "summary": "Large language models require consistent behavioral patterns for safe\ndeployment, yet their personality-like traits remain poorly understood. We\npresent PERSIST (PERsonality Stability in Synthetic Text), a comprehensive\nevaluation framework testing 25+ open-source models (1B-671B parameters) across\n500,000+ responses. Using traditional (BFI-44, SD3) and novel LLM-adapted\npersonality instruments, we systematically vary question order, paraphrasing,\npersonas, and reasoning modes. Our findings challenge fundamental deployment\nassumptions: (1) Even 400B+ models exhibit substantial response variability (SD\n> 0.4); (2) Minor prompt reordering alone shifts personality measurements by up\nto 20%; (3) Interventions expected to stabilize behavior, such as\nchain-of-thought reasoning, detailed personas instruction, inclusion of\nconversation history, can paradoxically increase variability; (4) LLM-adapted\ninstruments show equal instability to human-centric versions, confirming\narchitectural rather than translational limitations. This persistent\ninstability across scales and mitigation strategies suggests current LLMs lack\nthe foundations for genuine behavioral consistency. For safety-critical\napplications requiring predictable behavior, these findings indicate that\npersonality-based alignment strategies may be fundamentally inadequate.", "AI": {"tldr": "研究提出PERSIST框架，用于评估大规模语言模型的行为稳定性，发现即使是非常大的模型也存在显著响应变异性，且无法通过现有策略提升一致性。这表明，目前的大规模语言模型缺乏行为上的一致性基础，对需要高度可控性的应用来说，传统的对齐策略可能是不足的。", "motivation": "本文旨在深入探索大规模语言模型的行为特性和人格稳定模式，特别是在实际部署安全性方面存在的问题。当前模型生成的文本在一致性上仍然存在很大的不确定性，这限制了其应用于实际中的安全和可信赖水平。通过系统的测试和评估，希望能够找到提升模型一致性和稳定性的途径。", "method": "使用PERSIST框架，全面测试了25个开源模型（参数从1B到671B），通过超过500,000个响应进行评估。采用了传统的人格测试工具（BFI-44, SD3）和为LLM适配的新型人格测试工具，系统地变化了问题顺序、重述、人格设定以及推理模式。", "result": "研究结果挑战了部署模型的基本假设：（1）即使4000亿以上参数量的模型也表现出显著的响应变异性（SD>0.4）；（2）仅通过改变问题的顺序就能使人格测量变化高达20%；（3）预期可以稳定行为的干预措施（如链式思维推理、详细的人格设定指导、包含对话历史等）可能反而增加了一致性；（4）为LLM适配的人格测试工具显示出与以人类为中心的版本相同的不稳定性，证实了这种不稳定性源于架构而非转化过程中存在的局限性。", "conclusion": "这些结果表明，当前的大规模语言模型缺乏行为上的一致性基础。对于需要可预测行为的安全关键性应用，基于个性对齐策略可能从根本上不足。"}}
{"id": "2508.04868", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04868", "abs": "https://arxiv.org/abs/2508.04868", "authors": ["Noreen Anwar", "Guillaume-Alexandre Bilodeau", "Wassim Bouachir"], "title": "Dual-Stream Attention with Multi-Modal Queries for Object Detection in Transportation Applications", "comment": "10 pages", "summary": "Transformer-based object detectors often struggle with occlusions,\nfine-grained localization, and computational inefficiency caused by fixed\nqueries and dense attention. We propose DAMM, Dual-stream Attention with\nMulti-Modal queries, a novel framework introducing both query adaptation and\nstructured cross-attention for improved accuracy and efficiency. DAMM\ncapitalizes on three types of queries: appearance-based queries from\nvision-language models, positional queries using polygonal embeddings, and\nrandom learned queries for general scene coverage. Furthermore, a dual-stream\ncross-attention module separately refines semantic and spatial features,\nboosting localization precision in cluttered scenes. We evaluated DAMM on four\nchallenging benchmarks, and it achieved state-of-the-art performance in average\nprecision (AP) and recall, demonstrating the effectiveness of multi-modal query\nadaptation and dual-stream attention. Source code is at:\n\\href{https://github.com/DET-LIP/DAMM}{GitHub}.", "AI": {"tldr": "DAMM框架通过引入查询适应性和结构化交叉注意力，解决了基于Transformer的对象检测器在遮挡、细粒度定位和计算效率方面的问题，并在多个基准测试中表现优异，达到先进水平。", "motivation": "基于Transformer的对象检测器常常在处理遮挡、细粒度定位和计算效率低的问题上表现不佳，这些问题由固定的查询和密集的注意力机制引发。", "method": "DAMM（Dual-stream Attention with Multi-Modal queries）框架引入了查询适应性和结构化交叉注意力，用于提高准确性和效率，包括三种类型的查询：基于外观的查询、位置查询和随机学习到的查询，以及一个双重流交叉注意力模块，分别完善语义和空间特征，以提高混乱场景中的定位精度。", "result": "DAMM在四个具有挑战性的基准上进行了评估，达到了最先进的平均精度（AP）和召回率。", "conclusion": "该研究结果表明，多模态查询适应和双重流注意力的有效性。"}}
{"id": "2508.04903", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.04903", "abs": "https://arxiv.org/abs/2508.04903", "authors": ["Jun Liu", "Zhenglun Kong", "Changdi Yang", "Fan Yang", "Tianqi Li", "Peiyan Dong", "Joannah Nanjekye", "Hao Tang", "Geng Yuan", "Wei Niu", "Wenbin Zhang", "Pu Zhao", "Xue Lin", "Dong Huang", "Yanzhi Wang"], "title": "RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM Systems with Structured Memory", "comment": null, "summary": "Multi-agent large language model (LLM) systems have shown strong potential in\ncomplex reasoning and collaborative decision-making tasks. However, most\nexisting coordination schemes rely on static or full-context routing\nstrategies, which lead to excessive token consumption, redundant memory\nexposure, and limited adaptability across interaction rounds. We introduce\nRCR-Router, a modular and role-aware context routing framework designed to\nenable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge,\nthis is the first routing approach that dynamically selects semantically\nrelevant memory subsets for each agent based on its role and task stage, while\nadhering to a strict token budget. A lightweight scoring policy guides memory\nselection, and agent outputs are iteratively integrated into a shared memory\nstore to facilitate progressive context refinement. To better evaluate model\nbehavior, we further propose an Answer Quality Score metric that captures\nLLM-generated explanations beyond standard QA accuracy. Experiments on three\nmulti-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate\nthat RCR-Router reduces token usage (up to 30%) while improving or maintaining\nanswer quality. These results highlight the importance of structured memory\nrouting and output-aware evaluation in advancing scalable multi-agent LLM\nsystems.", "AI": {"tldr": "提出了RCR-Router框架，用于动态选择语义相关记忆子集，实现多智能体LLM系统在保持答案质量的同时减少标记使用。", "motivation": "现有的协调策略导致了标记的过度消耗、冗余的记忆暴露和有限的适应性，因此需要一种新的记忆路由框架。", "method": "RCR-Router基于每个智能体的角色和任务阶段，动态选择语义相关的记忆子集，并遵循严格的标记预算。", "result": "实验结果表明，RCR-Router可以减少标记的使用（最高减少30%），同时保持或提高答案质量。", "conclusion": "实验说明了结构化记忆路由和输出感知评估对于推进可扩展多智能体LLM系统的重要性。"}}
{"id": "2508.04900", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04900", "abs": "https://arxiv.org/abs/2508.04900", "authors": ["Shuonan Yang", "Tailin Chen", "Rahul Singh", "Jiangbei Yue", "Jianbo Jiao", "Zeyu Fu"], "title": "Revealing Temporal Label Noise in Multimodal Hateful Video Classification", "comment": null, "summary": "The rapid proliferation of online multimedia content has intensified the\nspread of hate speech, presenting critical societal and regulatory challenges.\nWhile recent work has advanced multimodal hateful video detection, most\napproaches rely on coarse, video-level annotations that overlook the temporal\ngranularity of hateful content. This introduces substantial label noise, as\nvideos annotated as hateful often contain long non-hateful segments. In this\npaper, we investigate the impact of such label ambiguity through a fine-grained\napproach. Specifically, we trim hateful videos from the HateMM and\nMultiHateClip English datasets using annotated timestamps to isolate explicitly\nhateful segments. We then conduct an exploratory analysis of these trimmed\nsegments to examine the distribution and characteristics of both hateful and\nnon-hateful content. This analysis highlights the degree of semantic overlap\nand the confusion introduced by coarse, video-level annotations. Finally,\ncontrolled experiments demonstrated that time-stamp noise fundamentally alters\nmodel decision boundaries and weakens classification confidence, highlighting\nthe inherent context dependency and temporal continuity of hate speech\nexpression. Our findings provide new insights into the temporal dynamics of\nmultimodal hateful videos and highlight the need for temporally aware models\nand benchmarks for improved robustness and interpretability. Code and data are\navailable at\nhttps://github.com/Multimodal-Intelligence-Lab-MIL/HatefulVideoLabelNoise.", "AI": {"tldr": "该研究通过细粒度的方法分析了多模态仇恨视频中的时间标注噪声问题，发现粗略的视频级标注会导致标签噪声，影响模型的决策边界和分类信心，强调了时间感知模型和基准的重要性。", "motivation": "动机在于解决由粗略的视频级标注产生的标签噪声问题，特别是这些标注忽略了仇恨内容的时间粒度，导致视频中包含大量的非仇恨片段。", "method": "方法是截取来自HateMM和MultiHateClip英语数据集的仇恨视频中被注释时间戳明确表示的仇恨段落，并分析这些截取段落中仇恨和非仇恨内容的分布和特征。", "result": "结果表明，在这些截取的仇恨段落中，语义重叠和由粗略的视频级标注引入的混淆显著存在，时间标注噪声极大地改变了模型的决策边界，削弱了分类信心。", "conclusion": "结论指出，了解多模态仇恨视频的时间动力学是必要的，并强调了开发能够考虑时间和连续性的情境感知模型的必要性。"}}
