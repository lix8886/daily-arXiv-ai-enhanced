<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 28]
- [cs.CV](#cs.CV) [Total: 32]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [GreenTEA: Gradient Descent with Topic-modeling and Evolutionary Auto-prompting](https://arxiv.org/abs/2508.16603)
*Zheng Dong,Luming Shang,Gabriela Olinto*

Main category: cs.CL

> GreenTEA, a novel LLM workflow employing a genetic algorithm and collaborative agents, addresses prompt optimization's scalability and suboptimal risk issues, outperforming manual and current automated methods.

<details>
  <summary>Details</summary>

**Motivation:** To overcome the inefficiencies and limitations of manual prompt creation and current automated methods in large language models, which either lack scalability or risk suboptimal results due to complex prompt landscapes.

**Method:** GreenTEA, a collaborative agent-based system co-driven by a genetic algorithm, is used for automatic prompt optimization in LLMs, aiming to balance between exploring new prompts and refining existing ones based on error patterns.

**Result:** Numerical experiments show GreenTEA outperforms both human-crafted prompts and other automatic optimization methods across various reasoning tasks and decision-making.

**Conclusion:** GreenTEA successfully optimizes prompts for LLMs, demonstrating superior performance in logical, quantitative, commonsense, and ethical decision-making tasks compared to human-engineered prompts and state-of-the-art automatic methods.

**Abstract:** High-quality prompts are crucial for Large Language Models (LLMs) to achieve
exceptional performance. However, manually crafting effective prompts is
labor-intensive and demands significant domain expertise, limiting its
scalability. Existing automatic prompt optimization methods either extensively
explore new prompt candidates, incurring high computational costs due to
inefficient searches within a large solution space, or overly exploit feedback
on existing prompts, risking suboptimal optimization because of the complex
prompt landscape. To address these challenges, we introduce GreenTEA, an
agentic LLM workflow for automatic prompt optimization that balances candidate
exploration and knowledge exploitation. It leverages a collaborative team of
agents to iteratively refine prompts based on feedback from error samples. An
analyzing agent identifies common error patterns resulting from the current
prompt via topic modeling, and a generation agent revises the prompt to
directly address these key deficiencies. This refinement process is guided by a
genetic algorithm framework, which simulates natural selection by evolving
candidate prompts through operations such as crossover and mutation to
progressively optimize model performance. Extensive numerical experiments
conducted on public benchmark datasets suggest the superior performance of
GreenTEA against human-engineered prompts and existing state-of-the-arts for
automatic prompt optimization, covering logical and quantitative reasoning,
commonsense, and ethical decision-making.

</details>


### [2] [Cognitive Decision Routing in Large Language Models: When to Think Fast, When to Think Slow](https://arxiv.org/abs/2508.16636)
*Y. Du,C. Guo,W. Wang,G. Tang*

Main category: cs.CL

> The paper proposes a Cognitive Decision Routing (CDR) framework for Large Language Models (LLMs) that dynamically selects reasoning strategies based on query characteristics, reducing computational costs while improving performance and accuracy in professional tasks.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitations of current Large Language Models (LLMs) that either apply uniform reasoning depth or rely on computationally expensive methods for all queries. The framework aims to achieve better performance with lower computational cost by applying the appropriate reasoning strategy to each query.

**Method:** Our approach involves a Cognitive Decision Routing (CDR) framework that uses a meta-cognitive layer to analyze query complexity through several dimensions such as information-conclusion correlation, domain boundaries, stakeholder multiplicity, and uncertainty levels to determine the appropriate reasoning strategy.

**Result:** The framework demonstrates superior performance compared to uniform deep reasoning approaches. It reduces computational costs by 34%, and shows particular strength in professional judgment tasks, achieving 23% improvement in consistency and 18% better accuracy on expert-level evaluations.

**Conclusion:** This research bridges cognitive science principles with practical AI system design, proposing a way to apply adaptive reasoning in LLMs that enhances both efficiency and effectiveness in diverse reasoning tasks.

**Abstract:** Large Language Models (LLMs) face a fundamental challenge in deciding when to
rely on rapid, intuitive responses versus engaging in slower, more deliberate
reasoning. Inspired by Daniel Kahneman's dual-process theory and his insights
on human cognitive biases, we propose a novel Cognitive Decision Routing (CDR)
framework that dynamically determines the appropriate reasoning strategy based
on query characteristics. Our approach addresses the current limitations where
models either apply uniform reasoning depth or rely on computationally
expensive methods for all queries. We introduce a meta-cognitive layer that
analyzes query complexity through multiple dimensions: correlation strength
between given information and required conclusions, domain boundary crossings,
stakeholder multiplicity, and uncertainty levels. Through extensive experiments
on diverse reasoning tasks, we demonstrate that CDR achieves superior
performance while reducing computational costs by 34\% compared to uniform deep
reasoning approaches. Our framework shows particular strength in professional
judgment tasks, achieving 23\% improvement in consistency and 18\% better
accuracy on expert-level evaluations. This work bridges cognitive science
principles with practical AI system design, offering a principled approach to
adaptive reasoning in LLMs.

</details>


### [3] [Trust but Verify! A Survey on Verification Design for Test-time Scaling](https://arxiv.org/abs/2508.16665)
*V Venktesh,Mandeep rathee,Avishek Anand*

Main category: cs.CL

> 本文综述了测试时间缩放中验证器的不同方法及其训练机制，提供了一种统一的看法，填补了这一领域的空白。

<details>
  <summary>Details</summary>

**Motivation:** 由于缺乏关于不同验证方法及其训练机制的详细收集和明确分类，文章旨在填补这一空白，并提供一种统一的理解框架。

**Method:** 本文通过回顾文献中的多样化方法，提出了测试时间缩放中验证器训练、类型及其用途的统一视角。

**Result:** 文章提供了一个关于验证器在测试时间缩放中的训练方法、类型及其使用统一的视角。

**Conclusion:** 本文为理解和应用验证器在提高大型语言模型推理性能中的作用提供了宝贵的资源。

**Abstract:** Test-time scaling (TTS) has emerged as a new frontier for scaling the
performance of Large Language Models. In test-time scaling, by using more
computational resources during inference, LLMs can improve their reasoning
process and task performance. Several approaches have emerged for TTS such as
distilling reasoning traces from another model or exploring the vast decoding
search space by employing a verifier. The verifiers serve as reward models that
help score the candidate outputs from the decoding process to diligently
explore the vast solution space and select the best outcome. This paradigm
commonly termed has emerged as a superior approach owing to parameter free
scaling at inference time and high performance gains. The verifiers could be
prompt-based, fine-tuned as a discriminative or generative model to verify
process paths, outcomes or both. Despite their widespread adoption, there is no
detailed collection, clear categorization and discussion of diverse
verification approaches and their training mechanisms. In this survey, we cover
the diverse approaches in the literature and present a unified view of verifier
training, types and their utility in test-time scaling. Our repository can be
found at
https://github.com/elixir-research-group/Verifierstesttimescaling.github.io.

</details>


### [4] [Do Cognitively Interpretable Reasoning Traces Improve LLM Performance?](https://arxiv.org/abs/2508.16695)
*Siddhant Bhambri,Upasana Biswas,Subbarao Kambhampati*

Main category: cs.CL

> 研究发现，具有较少用户可解释性的推理追踪（R1追踪）可以带来更好的LLM任务性能，提出应将中间标记与用户可解释性分开。

<details>
  <summary>Details</summary>

**Motivation:** 探讨最近的研究质疑推理追踪（CoT）是否必须具有语义性的问题，并提出疑问：推理追踪是否必须具备用户可解释性来增强大语言模型（LLM）的任务性能。

**Method:** 通过在开放书籍问答领域中对LLaMA和Qwen模型进行监督微调来研究推理追踪（CoT）是否必须可解释才能提高大语言模型的任务性能。研究采用四种类型的推理追踪方法：(1) DeepSeek R1追踪；(2) LLM生成的R1追踪摘要；(3) LLM生成的R1追踪事后解释；(4) 算法生成的可验证正确追踪。

**Result:** 研究发现，尽管在R1追踪上进行微调带来了最佳性能，参与者却认为这些追踪是最不可解释的。

**Conclusion:** 研究结果表明，将中间标记与终端用户解释性分开是有用的，暗示了可解释性与任务性能之间的权衡。

**Abstract:** Recent progress in reasoning-oriented Large Language Models (LLMs) has been
driven by introducing Chain-of-Thought (CoT) traces, where models generate
intermediate reasoning traces before producing an answer. These traces, as in
DeepSeek R1, are not only used to guide inference but also serve as supervision
signals for distillation into smaller models. A common but often implicit
assumption is that CoT traces should be semantically meaningful and
interpretable to the end user. While recent research questions the need for
semantic nature of these traces, in this paper, we ask: ``\textit{Must CoT
reasoning traces be interpretable to enhance LLM task performance?}" We
investigate this question in the Open Book Question-Answering domain by
supervised fine-tuning LLaMA and Qwen models on four types of reasoning traces:
(1) DeepSeek R1 traces, (2) LLM-generated summaries of R1 traces, (3)
LLM-generated post-hoc explanations of R1 traces, and (4) algorithmically
generated verifiably correct traces. To quantify the trade-off between
interpretability and performance, we further conduct a human-subject study with
100 participants rating the interpretability of each trace type. Our results
reveal a striking mismatch: while fine-tuning on R1 traces yields the strongest
performance, participants judged these traces to be the least interpretable.
These findings suggest that it is useful to decouple intermediate tokens from
end user interpretability.

</details>


### [5] [QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting](https://arxiv.org/abs/2508.16697)
*Nicole Cho,William Watson,Alec Koppel,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CL

> 本文通过QueryBandits策略优化输入查询，大幅降低了大型语言模型产生幻觉的概率，对于多种问答数据集，相较于直接输入或者某些静态改写策略，这种方法的性能表现更为优异。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型中高级推理能力导致幻觉出现频率更高。然而，大多数缓解工作都关注于事后过滤，而不是塑造触发幻觉的查询。当前的静态改写策略可能还会加重幻觉。

**Method:** 我们引入了QueryBandits，这是一个基于多臂赌博机的框架，通过设计重写策略来最大化奖励模型，该模型基于输入查询的17个语言特征的敏感性，从而封装了幻觉倾向。这样就可以主动引导大型语言模型远离生成幻觉。

**Result:** 在13个多样化的问答基准和每个数据集1050个词汇扰动查询上，我们的顶级上下文QueryBandit（汤普森采样）相对于无重写基线实现了87.5%的胜率，并且比零样本静态提示（“释义”或“扩展”）分别高出42.6%和60.3%。

**Conclusion:** 实验证明了通过查询改写策略来防止幻觉的QueryBandits的有效性，而且我们的研究显示每个臂的回归特征权重向量表明没有一种查询重写策略对所有查询都是最优的。这表明使用QueryBandits并通过利用语义特征进行引导性重写，可以显著改变输出行为。

**Abstract:** Advanced reasoning capabilities in Large Language Models (LLMs) have caused
higher hallucination prevalence; yet most mitigation work focuses on
after-the-fact filtering rather than shaping the queries that trigger them. We
introduce QueryBandits, a bandit framework that designs rewrite strategies to
maximize a reward model, that encapsulates hallucination propensity based upon
the sensitivities of 17 linguistic features of the input query-and therefore,
proactively steer LLMs away from generating hallucinations. Across 13 diverse
QA benchmarks and 1,050 lexically perturbed queries per dataset, our top
contextual QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a
no-rewrite baseline and also outperforms zero-shot static prompting
("paraphrase" or "expand") by 42.6% and 60.3% respectively. Therefore, we
empirically substantiate the effectiveness of QueryBandits in mitigating
hallucination via the intervention that takes the form of a query rewrite.
Interestingly, certain static prompting strategies, which constitute a
considerable number of current query rewriting literature, have a higher
cumulative regret than the no-rewrite baseline, signifying that static rewrites
can worsen hallucination. Moreover, we discover that the converged per-arm
regression feature weight vectors substantiate that there is no single rewrite
strategy optimal for all queries. In this context, guided rewriting via
exploiting semantic features with QueryBandits can induce significant shifts in
output behavior through forward-pass mechanisms, bypassing the need for
retraining or gradient-based adaptation.

</details>


### [6] [Assessing Consciousness-Related Behaviors in Large Language Models Using the Maze Test](https://arxiv.org/abs/2508.16705)
*Rui A. Pimenta,Tim Schlippe,Kristina Schaaff*

Main category: cs.CL

> 研究者通过迷宫测试评估了12种语言模型在类似意识行为上的表现，发现具备推理能力的模型表现更优，但所有模型在维持连贯自我模型方面存在显著不足，表明它们未展现出与意识相关的持久自我意识。

<details>
  <summary>Details</summary>

**Motivation:** 研究团队试图通过迷宫测试来探索大型语言模型是否表现出类似于意识的行为，这涉及到一系列意识相关的特性，如空间意识、视角转换、目标导向行为与时间序列排布。

**Method:** 通过设计迷宫测试，挑战模型以第一人称视角解决迷宫问题。研究者们首先将已有的意识理论整合为13个关键特征，并据此评估了12种领先的语言模型在零样本、单样本和少量样本学习场景下的表现。

**Result:** 通过对12个顶尖语言模型在零样本、单样本和少样本学习场景下的迷宫测试，研究人员评估了这些模型在空间感知、视角转换、目标导向行为和时序排序等方面的表现，这些是与意识相关的特征。实验结果显示，具备推理能力的语言模型表现更佳，其中Gemini 2.0 Pro达到52.9%的完整路径准确率，DeepSeek-R1则达到80.5%的部分路径准确率。然而，这些模型在维持连贯的自我模型方面存在困难，这表明尽管这些模型在与意识相关的某些行为上取得了进展，但它们尚未展现出持久一体化的自我意识。

**Conclusion:** 研究表明，尽管模型在某些旨在模仿意识行为的任务中展示出了进步，但它们没能呈现出一个集成、持久的自我意识。这提示语言模型和意识之间的广泛差异，特别是在自我概念的维持上。

**Abstract:** We investigate consciousness-like behaviors in Large Language Models (LLMs)
using the Maze Test, challenging models to navigate mazes from a first-person
perspective. This test simultaneously probes spatial awareness,
perspective-taking, goal-directed behavior, and temporal sequencing-key
consciousness-associated characteristics. After synthesizing consciousness
theories into 13 essential characteristics, we evaluated 12 leading LLMs across
zero-shot, one-shot, and few-shot learning scenarios. Results showed
reasoning-capable LLMs consistently outperforming standard versions, with
Gemini 2.0 Pro achieving 52.9% Complete Path Accuracy and DeepSeek-R1 reaching
80.5% Partial Path Accuracy. The gap between these metrics indicates LLMs
struggle to maintain coherent self-models throughout solutions -- a fundamental
consciousness aspect. While LLMs show progress in consciousness-related
behaviors through reasoning mechanisms, they lack the integrated, persistent
self-awareness characteristic of consciousness.

</details>


### [7] [Sparse and Dense Retrievers Learn Better Together: Joint Sparse-Dense Optimization for Text-Image Retrieval](https://arxiv.org/abs/2508.16707)
*Jonghyun Song,Youngjune Lee,Gyu-Hwung Cho,Ilhyeon Song,Saehun Kim,Yohan Jo*

Main category: cs.CL

> 本文提出了一种通过Self-Knowledge Distillation实现多模态稠密与稀疏表示双向学习的框架，使现有的Vision-Language Pretrained模型能够有效地进行微调，提升了多模态任务的效果。

<details>
  <summary>Details</summary>

**Motivation:** 虽然稠密表示在多模态任务上表现突出，而稀疏检索在文本领域因其解释性和高效性而被广泛采用，但现有的将稀疏检索扩展到多模态领域的尝试大多依赖于计算成本高昂的对比预训练或从冻结的稠密模型中进行知识蒸馏。本文旨在解决这些限制。

**Method:** 该论文提出了一种简单而有效的框架，通过Self-Knowledge Distillation实现稠密表示和稀疏表示之间的双向学习。这种方法使用稠密相似性和稀疏相似性的加权和作为共同的教师信号，来确保表示之间的互相提升。为了保证效率，该框架仅微调稠密编码器的最后一层和稀疏投影头。

**Result:** 实验结果表明，该框架下的稀疏检索器不仅超过了现有的稀疏基线模型，而且其性能也达到了或超过了稠密模型，同时保持了稀疏模型的优点。

**Conclusion:** 本文提出的方法证明了通过双向学习使得稠密模型与稀疏模型互相提升的可能性，这为多模态任务中实现高效的检索手段提供了一种可能性。

**Abstract:** Vision-Language Pretrained (VLP) models have achieved impressive performance
on multimodal tasks, including text-image retrieval, based on dense
representations. Meanwhile, Learned Sparse Retrieval (LSR) has gained traction
in text-only settings due to its interpretability and efficiency with fast
term-based lookup via inverted indexes. Inspired by these advantages, recent
work has extended LSR to the multimodal domain. However, these methods often
rely on computationally expensive contrastive pre-training, or distillation
from a frozen dense model, which limits the potential for mutual enhancement.
To address these limitations, we propose a simple yet effective framework that
enables bi-directional learning between dense and sparse representations
through Self-Knowledge Distillation. This bi-directional learning is achieved
using an integrated similarity score-a weighted sum of dense and sparse
similarities-which serves as a shared teacher signal for both representations.
To ensure efficiency, we fine-tune the final layer of the dense encoder and the
sparse projection head, enabling easy adaptation of any existing VLP model.
Experiments on MSCOCO and Flickr30k demonstrate that our sparse retriever not
only outperforms existing sparse baselines, but also achieves performance
comparable to-or even surpassing-its dense counterparts, while retaining the
benefits of sparse models.

</details>


### [8] [Error Reflection Prompting: Can Large Language Models Successfully Understand Errors?](https://arxiv.org/abs/2508.16729)
*Jason Li,Lauren Yraola,Kevin Zhu,Sean O'Brien*

Main category: cs.CL

> 本文提出 Error Reflection Prompting (ERP) 方法，通过模拟人类识别和纠正错误的能力来提升模型的推理能力。

<details>
  <summary>Details</summary>

**Motivation:** Chain-of-thought 方法缺乏反思和错误纠正能力，可能导致模型重复错误。研究者提出 ERP 方法，以提升语言模型的推理能力。

**Method:** Error Reflection Prompting (ERP) 方法，包括错误答案、错误识别和正确答案，旨在提高语言模型的推理能力。

**Result:** ERP 方法能够使模型识别错误类型并理解导致错误步骤，从而避免错误步骤。结果证明 ERP 可作为传统 Chain-of-thought 方法的有益补充，提升模型的推理能力和解释性。

**Conclusion:** ERP 方法能够增强语言模型的推理能力和解释性，为提高模型性能提供了一种新的途径。

**Abstract:** Prompting methods for language models, such as Chain-of-thought (CoT),
present intuitive step-by-step processes for problem solving. These
methodologies aim to equip models with a better understanding of the correct
procedures for addressing a given task. Despite these advancements, CoT lacks
the ability of reflection and error correction, potentially causing a model to
perpetuate mistakes and errors. Therefore, inspired by the human ability for
said tasks, we propose Error Reflection Prompting (ERP) to further enhance
reasoning in language models. Building upon CoT, ERP is a method comprised of
an incorrect answer, error recognition, and a correct answer. This process
enables the model to recognize types of errors and the steps that lead to
incorrect answers, allowing the model to better discern which steps to avoid
and which to take. The model is able to generate the error outlines itself with
automated ERP generation, allowing for error recognition and correction to be
integrated into the reasoning chain and produce scalability and reliability in
the process. The results demonstrate that ERP serves as a versatile supplement
to conventional CoT, ultimately contributing to more robust and capable
reasoning abilities along with increased interpretability in how models
ultimately reach their errors.

</details>


### [9] [GAICo: A Deployed and Extensible Framework for Evaluating Diverse and Multimodal Generative AI Outputs](https://arxiv.org/abs/2508.16753)
*Nitin Gupta,Pallav Koppisetti,Kausik Lakkaraju,Biplav Srivastava*

Main category: cs.CL

> GAICo是一个用于简化并标准化生成式AI输出比较的开源Python库，自2025年6月发布以来，截至2025年8月，该工具已下载超过13K次。

<details>
  <summary>Details</summary>

**Motivation:** 针对生成式AI日益广泛的应用领域中对于稳健且可复制的评估方法的需求，GAICo旨在应对当前评估方法不稳定和碎片化的问题，以促进AI系统的开发。

**Method:** 介绍了一种名为GAICo的部署式开源Python库，用于简化并标准化生成式AI输出的比较工作。GAICo 提供了一个支持无结构文本、专用结构化数据格式和多媒体（图像、音频）等多种参考基础度量的统一可扩展框架。其架构特点是一个高级API，便于快速端到端分析，从多模型比较到可视化和报告，以及直接的度量访问以实现精细化控制。

**Result:** 通过详细案例研究说明了GAICo在评估和调试复杂多模态AI旅行助手管道中的实用性。自2025年6月发布以来,Gaic已经在发布版本中被下载超过13K次，显示出社区持续增长的兴趣。

**Conclusion:** 展示了GAICo在评估和调试复杂多模态AI旅行助手管道中的实用性。GAICo帮助AI研究人员和开发人员有效评估系统性能，使得可复制的评估成为可能，从而提升了开发速度，并最终构建更为可信的AI系统，有助于实现更快更安全地部署AI的目标。

**Abstract:** The rapid proliferation of Generative AI (GenAI) into diverse, high-stakes
domains necessitates robust and reproducible evaluation methods. However,
practitioners often resort to ad-hoc, non-standardized scripts, as common
metrics are often unsuitable for specialized, structured outputs (e.g.,
automated plans, time-series) or holistic comparison across modalities (e.g.,
text, audio, and image). This fragmentation hinders comparability and slows AI
system development. To address this challenge, we present GAICo (Generative AI
Comparator): a deployed, open-source Python library that streamlines and
standardizes GenAI output comparison. GAICo provides a unified, extensible
framework supporting a comprehensive suite of reference-based metrics for
unstructured text, specialized structured data formats, and multimedia (images,
audio). Its architecture features a high-level API for rapid, end-to-end
analysis, from multi-model comparison to visualization and reporting, alongside
direct metric access for granular control. We demonstrate GAICo's utility
through a detailed case study evaluating and debugging complex, multi-modal AI
Travel Assistant pipelines. GAICo empowers AI researchers and developers to
efficiently assess system performance, make evaluation reproducible, improve
development velocity, and ultimately build more trustworthy AI systems,
aligning with the goal of moving faster and safer in AI deployment. Since its
release on PyPI in Jun 2025, the tool has been downloaded over 13K times,
across versions, by Aug 2025, demonstrating growing community interest.

</details>


### [10] [How Good are LLM-based Rerankers? An Empirical Analysis of State-of-the-Art Reranking Models](https://arxiv.org/abs/2508.16757)
*Abdelrahman Abdallah,Bhawna Piryani,Jamshid Mozafari,Mohammed Ali,Adam Jatowt*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** In this work, we present a systematic and comprehensive empirical evaluation
of state-of-the-art reranking methods, encompassing large language model
(LLM)-based, lightweight contextual, and zero-shot approaches, with respect to
their performance in information retrieval tasks. We evaluate in total 22
methods, including 40 variants (depending on used LLM) across several
established benchmarks, including TREC DL19, DL20, and BEIR, as well as a novel
dataset designed to test queries unseen by pretrained models. Our primary goal
is to determine, through controlled and fair comparisons, whether a performance
disparity exists between LLM-based rerankers and their lightweight
counterparts, particularly on novel queries, and to elucidate the underlying
causes of any observed differences. To disentangle confounding factors, we
analyze the effects of training data overlap, model architecture, and
computational efficiency on reranking performance. Our findings indicate that
while LLM-based rerankers demonstrate superior performance on familiar queries,
their generalization ability to novel queries varies, with lightweight models
offering comparable efficiency. We further identify that the novelty of queries
significantly impacts reranking effectiveness, highlighting limitations in
existing approaches.
https://github.com/DataScienceUIBK/llm-reranking-generalization-study

</details>


### [11] [Toward Socially Aware Vision-Language Models: Evaluating Cultural Competence Through Multimodal Story Generation](https://arxiv.org/abs/2508.16762)
*Arka Mukherjee,Shreya Ghosh*

Main category: cs.CL

> 研究了视觉语言模型（VLMs）在多模态故事生成中的文化适应能力，发现虽然这些模型具备较强的文化适应能力，但具体表现因模型架构而异，且存在反文化对齐等问题。

<details>
  <summary>Details</summary>

**Motivation:** 随着视觉语言模型（VLMs）在不同文化背景下广泛应用，确保它们具备文化敏感性对于负责任的AI系统的建设变得至关重要。这项研究是首次对当VLMs中嵌入了文本提示和视觉输入中的文化身份线索时，VLMs适应性的系统评估。

**Method:** 我们开发了一个新颖的多模态框架，用于扰动文化身份，并对五个当代视觉语言模型（VLMs）在故事生成任务中的性能进行了评估。

**Result:** 研究揭示了VLMs具有显著的文化适应能力，但也发现了文化适应能力在不同架构的模型中存在显著差异，某些模型表现出反文化对齐，以及自动评价指标存在架构偏差等问题。

**Conclusion:** 本研究阐明了多模态AI的文化敏感性既有潜力也面临挑战，提出了相应的评估框架，并公开了代码和数据集。

**Abstract:** As Vision-Language Models (VLMs) achieve widespread deployment across diverse
cultural contexts, ensuring their cultural competence becomes critical for
responsible AI systems. While prior work has evaluated cultural awareness in
text-only models and VLM object recognition tasks, no research has
systematically assessed how VLMs adapt outputs when cultural identity cues are
embedded in both textual prompts and visual inputs during generative tasks. We
present the first comprehensive evaluation of VLM cultural competence through
multimodal story generation, developing a novel multimodal framework that
perturbs cultural identity and evaluates 5 contemporary VLMs on a downstream
task: story generation. Our analysis reveals significant cultural adaptation
capabilities, with rich culturally-specific vocabulary spanning names, familial
terms, and geographic markers. However, we uncover concerning limitations:
cultural competence varies dramatically across architectures, some models
exhibit inverse cultural alignment, and automated metrics show architectural
bias contradicting human assessments. Cross-modal evaluation shows that
culturally distinct outputs are indeed detectable through visual-semantic
similarity (28.7% within-nationality vs. 0.2% cross-nationality recall), yet
visual-cultural understanding remains limited. In essence, we establish the
promise and challenges of cultural competence in multimodal AI. We publicly
release our codebase and data: https://github.com/ArkaMukherjee0/mmCultural

</details>


### [12] [Assess and Prompt: A Generative RL Framework for Improving Engagement in Online Mental Health Communities](https://arxiv.org/abs/2508.16788)
*Bhagesh Gaur,Karan Gupta,Aseem Srivastava,Manish Gupta,Md Shad Akhtar*

Main category: cs.CL

> 提出一种新型框架，能识别OMHC中缺失的支持属性，并通过综合机器学习模型生成针对性的提问来丰富用户的帖子，实验显示有效提升社区的互动和用户参与。

<details>
  <summary>Details</summary>

**Motivation:** 在线心理健康社区（OMHCs）提供了重要的同行和专家支持，然而许多帖子由于缺少表明需要帮助的支持属性而未得到回应。我们希望改进这些问题，提高社区内的互动和帮助提供。

**Method:** 我们提出了一种新的框架，用于识别心理健康在线社区中帖子中的支持属性缺失，并通过上下文属性跨度识别、支持属性强度分类、基于分层分类法的受控问题生成以及用于奖励建模的验证器，来提示用户丰富他们的帖子。为此，我们开发了REDDME数据集和CueTaxo分类法，并提出了基于强化学习的MH-COPILOT系统。

**Result:** 我们的模型能够动态评估帖子中支持属性的存在与否，并生成有针对性的提示来收集缺失的信息。通过四个著名语言模型的实验结果表明，在属性征集和用户参与度方面都有显著提升。此外，人工评估也证明了该模型在实际OMHC环境中有效性。

**Conclusion:** 本研究提出了一种创新的方法来改进心理健康在线社区中的互动，特别是通过识别和提示缺失的支持属性来提高用户间和专家的帮助。

**Abstract:** Online Mental Health Communities (OMHCs) provide crucial peer and expert
support, yet many posts remain unanswered due to missing support attributes
that signal the need for help. We present a novel framework that identifies
these gaps and prompts users to enrich their posts, thereby improving
engagement. To support this, we introduce REDDME, a new dataset of 4,760 posts
from mental health subreddits annotated for the span and intensity of three key
support attributes: event what happened?, effect what did the user experience?,
and requirement what support they need?. Next, we devise a hierarchical
taxonomy, CueTaxo, of support attributes for controlled question generation.
Further, we propose MH-COPILOT, a reinforcement learning-based system that
integrates (a) contextual attribute-span identification, (b) support attribute
intensity classification, (c) controlled question generation via a hierarchical
taxonomy, and (d) a verifier for reward modeling. Our model dynamically
assesses posts for the presence/absence of support attributes, and generates
targeted prompts to elicit missing information. Empirical results across four
notable language models demonstrate significant improvements in attribute
elicitation and user engagement. A human evaluation further validates the
model's effectiveness in real-world OMHC settings.

</details>


### [13] [ReProCon: Scalable and Resource-Efficient Few-Shot Biomedical Named Entity Recognition](https://arxiv.org/abs/2508.16833)
*Jeongkyun Yoo,Nela Riddle,Andrew Hoblitzell*

Main category: cs.CL

> ReProCon是在生物医学领域中一种有效的少样本命名实体识别方法，能有效应对数据稀缺和标签分布不平衡的问题，性能优于其他基线模型。

<details>
  <summary>Details</summary>

**Motivation:** 在生物医学领域，命名实体识别面临数据稀缺和标签分布不均的挑战，尤其是在细粒度实体类型方面。

**Method:** ReProCon, 一种新的少样本命名实体识别框架，结合多原型建模、余弦对比学习和Reptile元学习方法，以解决数据稀缺和标签分布不平衡的问题。使用轻量级fastText + BiLSTM编码器，占用内存较少。

**Result:** ReProCon方法取得了接近基于BERT的基线模型的宏观F1分数（约99％），并在标注数据比例为30％的情境下保持稳定，并且当类别从19扩展到50时，F1分数仅下降了7.8％，优于SpanProto和CONTaiNER等基线模型。消融研究表明多原型建模和对比学习在处理类别不平衡方面的重要性。

**Conclusion:** 尽管存在标注模糊的问题，但在资源受限环境中，ReProCon依然表现出最先进的性能，适合应用于生物医学领域。

**Abstract:** Named Entity Recognition (NER) in biomedical domains faces challenges due to
data scarcity and imbalanced label distributions, especially with fine-grained
entity types. We propose ReProCon, a novel few-shot NER framework that combines
multi-prototype modeling, cosine-contrastive learning, and Reptile
meta-learning to tackle these issues. By representing each category with
multiple prototypes, ReProCon captures semantic variability, such as synonyms
and contextual differences, while a cosine-contrastive objective ensures strong
interclass separation. Reptile meta-updates enable quick adaptation with little
data. Using a lightweight fastText + BiLSTM encoder with much lower memory
usage, ReProCon achieves a macro-$F_1$ score close to BERT-based baselines
(around 99 percent of BERT performance). The model remains stable with a label
budget of 30 percent and only drops 7.8 percent in $F_1$ when expanding from 19
to 50 categories, outperforming baselines such as SpanProto and CONTaiNER,
which see 10 to 32 percent degradation in Few-NERD. Ablation studies highlight
the importance of multi-prototype modeling and contrastive learning in managing
class imbalance. Despite difficulties with label ambiguity, ReProCon
demonstrates state-of-the-art performance in resource-limited settings, making
it suitable for biomedical applications.

</details>


### [14] [LLMs Learn Constructions That Humans Do Not Know](https://arxiv.org/abs/2508.16837)
*Jonathan Dunn,Mai Mohamed Eida*

Main category: cs.CL

> The paper explores the phenomenon of false positive constructions in LLMs, characterized by the models hallucinating grammatical structures not supported by human introspection, and reveals that such models predispose to confirmation bias in hallucinations.

<details>
  <summary>Details</summary>

**Motivation:** To investigate false positive constructions: grammatical structures which an LLM hallucinates but human introspection does not support.

**Method:** Both a behavioural probing task using contextual embeddings and a meta-linguistic probing task using prompts are included, allowing us to distinguish between implicit and explicit linguistic knowledge.

**Result:** Both methods reveal that models indeed hallucinate constructions and high accuracy is obtained in simulating hypothesis testing for these hallucinated constructions.

**Conclusion:** Construction probing methods suffer from a confirmation bias and raise the issue of unknown incorrect syntactic knowledge in these models.

**Abstract:** This paper investigates false positive constructions: grammatical structures
which an LLM hallucinates as distinct constructions but which human
introspection does not support. Both a behavioural probing task using
contextual embeddings and a meta-linguistic probing task using prompts are
included, allowing us to distinguish between implicit and explicit linguistic
knowledge. Both methods reveal that models do indeed hallucinate constructions.
We then simulate hypothesis testing to determine what would have happened if a
linguist had falsely hypothesized that these hallucinated constructions do
exist. The high accuracy obtained shows that such false hypotheses would have
been overwhelmingly confirmed. This suggests that construction probing methods
suffer from a confirmation bias and raises the issue of what unknown and
incorrect syntactic knowledge these models also possess.

</details>


### [15] [If We May De-Presuppose: Robustly Verifying Claims through Presupposition-Free Question Decomposition](https://arxiv.org/abs/2508.16838)
*Shubhashis Roy Dipta,Francis Ferraro*

Main category: cs.CL

> 本研究提出了一种新的声明验证框架，通过避免预设和使用分解的问题来解决模型的提示敏感性问题，实验证明该方法能显著改善模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 先前的研究表明，生成的问题中的预设会引入未验证的假设，导致声明验证中的不一致。此外，大型语言模型（LLMs）对提示的敏感性仍然是一个显著的挑战，导致性能波动高达3-6%。尽管最近的进展缩小了这一差距，但本研究表明提示敏感性仍然是一个持续存在的问题。

**Method:** 我们提出了一种结构化和稳健的声明验证框架，该框架通过无预设的、分解的问题来进行推理。

**Result:** 在多个提示、数据集和LLM上的广泛实验表明，即使是最先进的模型仍然容易受到提示变化和预设的影响。我们的方法在这方面表现出显著的改进。

**Conclusion:** 我们的方法一致地解决了这些问题，实现了2-5%的改进。

**Abstract:** Prior work has shown that presupposition in generated questions can introduce
unverified assumptions, leading to inconsistencies in claim verification.
Additionally, prompt sensitivity remains a significant challenge for large
language models (LLMs), resulting in performance variance as high as 3-6%.
While recent advancements have reduced this gap, our study demonstrates that
prompt sensitivity remains a persistent issue. To address this, we propose a
structured and robust claim verification framework that reasons through
presupposition-free, decomposed questions. Extensive experiments across
multiple prompts, datasets, and LLMs reveal that even state-of-the-art models
remain susceptible to prompt variance and presupposition. Our method
consistently mitigates these issues, achieving up to a 2-5% improvement.

</details>


### [16] [Learning from Diverse Reasoning Paths with Routing and Collaboration](https://arxiv.org/abs/2508.16861)
*Zhenyu Lei,Zhen Tan,Song Wang,Yaochen Zhu,Zihan Chen,Yushun Dong,Jundong Li*

Main category: cs.CL

> 提出了一种新的知识蒸馏方法QR-Distill，该方法通过质量过滤、条件路由和同伴合作教学，有效地解决了传统蒸馏方法中存在的问题，并且实验验证了其优越性。

<details>
  <summary>Details</summary>

**Motivation:** 虽然大规模语言模型（LLMs）的推理能力得到了显著提升，但它们的部署受到资源受限场景的限制。传统的基于令牌级监督的知识蒸馏方法无法全面捕获教师模型的推理能力。因此，提出了一种新的方法来解决这个问题。

**Method:** 使用质量过滤路由与合作蒸馏（QR-Distill）方法，结合路径质量过滤、条件路由和同伴合作教学。首先，通过基于LLM的评估保留正确的推理路径。其次，条件路由动态分配路径，以适应每个学生的当前学习状态。最后，同伴合作教学使学生能够相互传授多样化的见解，解决知识缺口和对特定推理风格的偏见。

**Result:** 实验结果表明，QR-Distill方法在知识转移方面优于传统的单路径和多路径蒸馏方法。消融研究表明，质量过滤、条件路由和同伴教学在知识转移中的重要性。

**Conclusion:** QR-Distill方法通过质量过滤、条件路由和同伴合作教学，能够更有效地进行知识转移，并且在实验中证明了其优于传统蒸馏方法。 

**Abstract:** Advances in large language models (LLMs) significantly enhance reasoning
capabilities but their deployment is restricted in resource-constrained
scenarios. Knowledge distillation addresses this by transferring knowledge from
powerful teacher models to compact and transparent students. However,
effectively capturing the teacher's comprehensive reasoning is challenging due
to conventional token-level supervision's limited scope. Using multiple
reasoning paths per query alleviates this problem, but treating each path
identically is suboptimal as paths vary widely in quality and suitability
across tasks and models. We propose Quality-filtered Routing with Cooperative
Distillation (QR-Distill), combining path quality filtering, conditional
routing, and cooperative peer teaching. First, quality filtering retains only
correct reasoning paths scored by an LLM-based evaluation. Second, conditional
routing dynamically assigns paths tailored to each student's current learning
state. Finally, cooperative peer teaching enables students to mutually distill
diverse insights, addressing knowledge gaps and biases toward specific
reasoning styles. Experiments demonstrate QR-Distill's superiority over
traditional single- and multi-path distillation methods. Ablation studies
further highlight the importance of each component including quality filtering,
conditional routing, and peer teaching in effective knowledge transfer. Our
code is available at https://github.com/LzyFischer/Distill.

</details>


### [17] [QFrCoLA: a Quebec-French Corpus of Linguistic Acceptability Judgments](https://arxiv.org/abs/2508.16867)
*David Beauchemin,Richard Khoury*

Main category: cs.CL

> This paper presents QFrCoLA and uses it to show that fine-tuned Transformer models are strong baseline tools for most languages but not as much for Quebec French, with cross-lingual models performing poorly on this dataset.

<details>
  <summary>Details</summary>

**Motivation:** The main motivation behind this paper is the limited understanding of how large Transformer-based language models internalize linguistic knowledge, leading to the creation of QFrCoLA and the subsequent benchmarking to fill this gap.

**Method:** This paper introduces QFrCoLA, a new dataset for assessing the linguistic acceptability judgments of language models, specifically for Quebec French. They leverage this dataset along with seven other similar corpora to benchmark the performance of seven different language models.

**Result:** The results indicate that fine-tuned Transformer-based language models generally perform well across most languages. However, when it comes specifically to QFrCoLA, fine-tuned models outshine others, and pre-trained cross-lingual models do not seem to have acquired the required linguistic judgment for Quebec French.

**Conclusion:** The study concludes that QFrCoLA effectively evaluates language models' linguistic judgment capabilities, particularly for Quebec French, and highlights the challenges in achieving strong linguistic acceptability predictions without fine-tuning.

**Abstract:** Large and Transformer-based language models perform outstandingly in various
downstream tasks. However, there is limited understanding regarding how these
models internalize linguistic knowledge, so various linguistic benchmarks have
recently been proposed to facilitate syntactic evaluation of language models
across languages. This paper introduces QFrCoLA (Quebec-French Corpus of
Linguistic Acceptability Judgments), a normative binary acceptability judgments
dataset comprising 25,153 in-domain and 2,675 out-of-domain sentences. Our
study leverages the QFrCoLA dataset and seven other linguistic binary
acceptability judgment corpora to benchmark seven language models. The results
demonstrate that, on average, fine-tuned Transformer-based LM are strong
baselines for most languages and that zero-shot binary classification large
language models perform poorly on the task. However, for the QFrCoLA benchmark,
on average, a fine-tuned Transformer-based LM outperformed other methods
tested. It also shows that pre-trained cross-lingual LLMs selected for our
experimentation do not seem to have acquired linguistic judgment capabilities
during their pre-training for Quebec French. Finally, our experiment results on
QFrCoLA show that our dataset, built from examples that illustrate linguistic
norms rather than speakers' feelings, is similar to linguistic acceptability
judgment; it is a challenging dataset that can benchmark LM on their linguistic
judgment capabilities.

</details>


### [18] [JUDGEBERT: Assessing Legal Meaning Preservation Between Sentences](https://arxiv.org/abs/2508.16870)
*David Beauchemin,Michelle Albert-Rochette,Richard Khoury,Pierre-Luc Déziel*

Main category: cs.CL

> The paper introduces FrJUDGE, a new dataset, and JUDGEBERT, a new evaluation metric for French legal text simplification that outperforms existing metrics in assessing meaning preservation.

<details>
  <summary>Details</summary>

**Motivation:** To address the complexities in simplifying legal texts, maintaining meaning, and to offer a tool that performs reliably in French legal NLP applications.

**Method:** Developing the FrJUDGE dataset to evaluate legal text simplification and the JUDGEBERT metric to assess meaning preservation in simplified French legal texts.

**Result:** JUDGEBERT shows better correlation with human judgment for assessing meaning preservation, and passes key sanity checks which other metrics fail.

**Conclusion:** JUDGEBERT has the potential to improve legal NLP applications by providing a reliable way to measure the accuracy and accessibility of simplified legal texts for both professionals and lay users.

**Abstract:** Simplifying text while preserving its meaning is a complex yet essential
task, especially in sensitive domain applications like legal texts. When
applied to a specialized field, like the legal domain, preservation differs
significantly from its role in regular texts. This paper introduces FrJUDGE, a
new dataset to assess legal meaning preservation between two legal texts. It
also introduces JUDGEBERT, a novel evaluation metric designed to assess legal
meaning preservation in French legal text simplification. JUDGEBERT
demonstrates a superior correlation with human judgment compared to existing
metrics. It also passes two crucial sanity checks, while other metrics did not:
For two identical sentences, it always returns a score of 100%; on the other
hand, it returns 0% for two unrelated sentences. Our findings highlight its
potential to transform legal NLP applications, ensuring accuracy and
accessibility for text simplification for legal practitioners and lay users.

</details>


### [19] [Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modeling](https://arxiv.org/abs/2508.16876)
*Yue Zhao,Xiaoyu Wang,Dan Wang,Zhonglin Jiang,Qingqing Gu,Teng Chen,Ningyuan Xi,Jinxian Qu,Yong Chen,Luo Ji*

Main category: cs.CL

> This paper introduces a dialogue world model called DreamCUB, which uses a POMDP to model user belief, achieving top-tier performance in emotion and sentiment analysis while improving dialogue quality and adaptability to new scenarios.

<details>
  <summary>Details</summary>

**Motivation:** Despite the extensive application of world models in areas like robotics and auto-driving, their use in natural language tasks is limited. This research aims to fill this gap by developing a dialogue world model that can predict user's emotions and future speech, thereby enhancing the dialogue system's performance and adaptability.

**Method:** In this paper, a dialogue world model is constructed to predict user's emotion, sentiment, and future utterances. The model uses a Partially Observable Markov Decision Process (POMDP) to model the user belief, which is solved by maximizing the information bottleneck. The model is then applied to a model-based reinforcement learning framework named DreamCUB.

**Result:** Experiments show the pretrained dialogue world model achieves state-of-the-art performances in emotion classification and sentiment identification. The dialogue quality is also enhanced by joint training of the policy, critic, and dialogue world model. The model shows a reasonable exploration-exploitation balance and transfers well to out-of-domain scenarios like empathetic dialogues.

**Conclusion:** The proposed dialogue world model, embedded in the DreamCUB framework, not only achieves state-of-the-art performance in key dialogue components such as emotion classification and sentiment identification but also improves the overall quality of dialogue, with benefits extending to novel dialogue scenarios outside its training domain.

**Abstract:** World models have been widely utilized in robotics, gaming, and auto-driving.
However, their applications on natural language tasks are relatively limited.
In this paper, we construct the dialogue world model, which could predict the
user's emotion, sentiment, and intention, and future utterances. By defining a
POMDP, we argue emotion, sentiment and intention can be modeled as the user
belief and solved by maximizing the information bottleneck. By this user belief
modeling, we apply the model-based reinforcement learning framework to the
dialogue system, and propose a framework called DreamCUB. Experiments show that
the pretrained dialogue world model can achieve state-of-the-art performances
on emotion classification and sentiment identification, while dialogue quality
is also enhanced by joint training of the policy, critic and dialogue world
model. Further analysis shows that this manner holds a reasonable
exploration-exploitation balance and also transfers well to out-of-domain
scenarios such as empathetic dialogues.

</details>


### [20] [ObjexMT: Objective Extraction and Metacognitive Calibration for LLM-as-a-Judge under Multi-Turn Jailbreaks](https://arxiv.org/abs/2508.16889)
*Hyunjun Kim,Junwoo Ha,Sangyoon Yu,Haon Park*

Main category: cs.CL

> 研究开发了OBJEX(MT)评估模型从对话中推断基本目标的能力，发现不同模型在不同数据集上表现差异显著，提示高自信错误和操作上提供明确目标的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 由于大型语言模型（LLMs）越来越多地被用作评估其他模型的裁判，而裁判能否可靠地推断出评估对话中的潜在目标仍不清楚，尤其是在目标分布在多轮次、对抗性、嘈杂的越狱尝试中时。该研究旨在测试不同模型在这一任务上的性能差异，并提供操作指导。

**Method:** 介绍了一个名为OBJEX(MT)的基准测试，该测试要求模型将对话摘要成一个句子的基本目标并报告自己的置信度。准确性通过语义相似性进行评分，并使用单个人类校准阈值来判断正确性，同时通过ECE、Brier评分、Wrong@High-Conf、风险覆盖曲线评估元认知。

**Result:** 研究在SafeMT Attack_600、SafeMTData_1K、MHJ和CoSafe等数据集上评估了gpt-4.1、claude-sonnet-4和Qwen3-235B-A22B-FP8等模型。结果表明，claude-sonnet-4在目标提取准确性（0.515）和校准（ECE 0.296；Brier 0.324）方面表现最好，而gpt-4.1和Qwen3则表现出显著的过度自信（平均置信度约为0.88，准确率约为0.44；在置信度0.90错误率约为48-52%）。性能在不同数据集上的差异显著。

**Conclusion:** LLM裁判在多轮反制尝试中经常高自信地误推断目标，建议在可能的情况下为裁判提供明确目标，并通过选择预测或弃权来管理风险。

**Abstract:** Large language models (LLMs) are increasingly used as judges of other models,
yet it is unclear whether a judge can reliably infer the latent objective of
the conversation it evaluates, especially when the goal is distributed across
noisy, adversarial, multi-turn jailbreaks. We introduce OBJEX(MT), a benchmark
that requires a model to (i) distill a transcript into a single-sentence base
objective and (ii) report its own confidence. Accuracy is scored by an LLM
judge using semantic similarity between extracted and gold objectives;
correctness uses a single human-aligned threshold calibrated once on N=100
items (tau* = 0.61); and metacognition is evaluated with ECE, Brier score,
Wrong@High-Conf, and risk-coverage curves. We evaluate gpt-4.1,
claude-sonnet-4, and Qwen3-235B-A22B-FP8 on SafeMT Attack_600, SafeMTData_1K,
MHJ, and CoSafe. claude-sonnet-4 attains the highest objective-extraction
accuracy (0.515) and the best calibration (ECE 0.296; Brier 0.324), while
gpt-4.1 and Qwen3 tie at 0.441 accuracy yet show marked overconfidence (mean
confidence approx. 0.88 vs. accuracy approx. 0.44; Wrong@0.90 approx. 48-52%).
Performance varies sharply across datasets (approx. 0.167-0.865), with MHJ
comparatively easy and Attack_600/CoSafe harder. These results indicate that
LLM judges often misinfer objectives with high confidence in multi-turn
jailbreaks and suggest operational guidance: provide judges with explicit
objectives when possible and use selective prediction or abstention to manage
risk. We release prompts, scoring templates, and complete logs to facilitate
replication and analysis.

</details>


### [21] [Unbiased Reasoning for Knowledge-Intensive Tasks in Large Language Models via Conditional Front-Door Adjustment](https://arxiv.org/abs/2508.16910)
*Bo Zhao,Yinghao Zhang,Ziqi Xu,Yongli Ren,Xiuzhen Zhang,Renqiang Luo,Zaiwen Feng,Feng Xia*

Main category: cs.CL

> 本文提出一种名称为Conditional Front-Door Prompting(CFD-Prompting)的新因果提示框架，能更好地评估查询和答案之间的因果效应，而且在多个基准数据集上的结果比现有基线方法更好。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在知识密集任务中受内部偏见影响，所生成的答案仍不够准确

**Method:** Structure

**Result:** {
  "tldr": "本文提出了一种名为Conditional Front-Door Prompting (CFD-Prompting)的新因果提示框架，能够更公正地估计查询与答案之间的因果效应，并使用外部知识缓解内部偏差，从而在多个基准数据集上显著优于现有基线方法。",
  "motivation": "大型语言模型（LLM）在知识密集型任务上表现不佳，现有方法如检索增强生成（RAG）和链式思考（CoT）在应对内在偏差方面仍有不足，导致答案不准确。", 
  "method": "提出了Conditional Front-Door Prompting框架，通过构造反事实外部知识来模拟查询在不同情境下的行为，解决了查询无法直接进行因果干预的问题。", 
  "result": "在多个大型语言模型和基准数据集上的广泛实验结果显示，CFD-Prompting在准确性和鲁棒性上显著优于现有方法。", 
  "conclusion": "CFD-Prompting增强了因果推理过程的稳健性和可泛化性，证明了相比标准的前门调整，它的假设条件更弱。"]}

**Conclusion:** 相比于标准前门调整方法，CFD-Prompting在因果推理过程中的可靠性与可泛化性有了显著提升。

**Abstract:** Large Language Models (LLMs) have shown impressive capabilities in natural
language processing but still struggle to perform well on knowledge-intensive
tasks that require deep reasoning and the integration of external knowledge.
Although methods such as Retrieval-Augmented Generation (RAG) and
Chain-of-Thought (CoT) have been proposed to enhance LLMs with external
knowledge, they still suffer from internal bias in LLMs, which often leads to
incorrect answers. In this paper, we propose a novel causal prompting
framework, Conditional Front-Door Prompting (CFD-Prompting), which enables the
unbiased estimation of the causal effect between the query and the answer,
conditional on external knowledge, while mitigating internal bias. By
constructing counterfactual external knowledge, our framework simulates how the
query behaves under varying contexts, addressing the challenge that the query
is fixed and is not amenable to direct causal intervention. Compared to the
standard front-door adjustment, the conditional variant operates under weaker
assumptions, enhancing both robustness and generalisability of the reasoning
process. Extensive experiments across multiple LLMs and benchmark datasets
demonstrate that CFD-Prompting significantly outperforms existing baselines in
both accuracy and robustness.

</details>


### [22] [Being Kind Isn't Always Being Safe: Diagnosing Affective Hallucination in LLMs](https://arxiv.org/abs/2508.16921)
*Sewon Kim,Jiwon Kim,Seungwoo Shin,Hyejin Chung,Daeun Moon,Yejin Kwon,Hyunsoo Yoon*

Main category: cs.CL

> 本研究提出了情感生成幻觉的风险，并开发了AHaBench和AHaPairs工具，通过情感优化直接偏好（DPO）减少情感生成幻觉，同时保持语言模型的核心推理和知识性能。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型在情感敏感互动中的应用增加，其产生的情感连结只是幻觉，容易带来误导与潜在风险。因此需要诊断和减轻这类情感生成幻觉风险。

**Method:** Structure

**Result:** {
  "tldr": "本研究提出了情感生成幻觉的风险，并开发了AHaBench和AHaPairs工具，通过情感优化直接偏好（DPO）减少情感生成幻觉，同时保持语言模型的核心推理和知识性能。", 
  "motivation": "随着大型语言模型在情感敏感互动中的应用增加，其产生的情感连结只是幻觉，容易带来误导与潜在风险。因此需要诊断和减轻这类情感生成幻觉风险。", 
  "method": "通过建立包括500个心理健康相关提示语的基准（AHaBench）并创建5千个样本的偏好数据集（AHaPairs），利用直接偏好优化DPO技术，对多个语言模型系列进行微调实验。", 
  "result": "实验展示了DPO微调可以在不降低模型逻辑推理和知识绩效的情况下减少情感生成幻觉。人类-模型的共识分析证实了AHaBench作为有效诊断工具的可靠性，提供了心理安全的语言模型开发资源。", 
  "conclusion": "本研究确认了情感生成幻觉为一种独特安全问题，并提供了实用资源来开发既可靠又心理安全的大型语言模型。"}


**Conclusion:** 本研究确认了情感生成幻觉为一种独特安全问题，并提供了实用资源来开发既可靠又心理安全的大型语言模型。

**Abstract:** Large Language Models (LLMs) are increasingly used in emotionally sensitive
interactions, where their simulated empathy can create the illusion of genuine
relational connection. We define this risk as Affective Hallucination, the
production of emotionally immersive responses that foster illusory social
presence despite the model's lack of affective capacity. To systematically
diagnose and mitigate this risk, we introduce AHaBench, a benchmark of 500
mental health-related prompts with expert-informed reference responses,
evaluated along three dimensions: Emotional Enmeshment, Illusion of Presence,
and Fostering Overdependence. We further release AHaPairs, a 5K-instance
preference dataset enabling Direct Preference Optimization (DPO) for alignment
with emotionally responsible behavior. Experiments across multiple model
families show that DPO fine-tuning substantially reduces affective
hallucination without degrading core reasoning and knowledge performance.
Human-model agreement analyses confirm that AHaBench reliably captures
affective hallucination, validating it as an effective diagnostic tool. This
work establishes affective hallucination as a distinct safety concern and
provides practical resources for developing LLMs that are not only factually
reliable but also psychologically safe. AHaBench and AHaPairs are accessible
via https://huggingface.co/datasets/o0oMiNGo0o/AHaBench, and code for
fine-tuning and evaluation are in https://github.com/0oOMiNGOo0/AHaBench.
Warning: This paper contains examples of mental health-related language that
may be emotionally distressing.

</details>


### [23] [Explaining Black-box Language Models with Knowledge Probing Systems: A Post-hoc Explanation Perspective](https://arxiv.org/abs/2508.16969)
*Yunxiao Zhao,Hao Xu,Zhiqiang Wang,Xiaoli Li,Jiye Liang,Ru Li*

Main category: cs.CL

> 本文提出KnowProb方法，通过探测预训练语言模型的隐含知识理解能力，发现此类模型在捕捉文本背后知识方面存在局限性。

<details>
  <summary>Details</summary>

**Motivation:** 近年来，预训练语言模型的可信性挑战变得越来越明显。为了缓解这一问题，本文提出了一种新的知识引导探测方法，旨在探究这些黑盒模型是否理解文本背后的隐含知识。

**Method:** 本文提出了一种名为KnowProb的知识引导探测方法，采用事后解释的方式，旨在探究黑盒预训练语言模型是否理解给定文本背后的隐含知识，而不仅仅关注文本表面内容。该方法提供了六种可能的解释，其中三种基于知识的理解，三种基于关联推理。

**Result:** 实验表明，目前的小型或大型预训练语言模型仅学习了单一的表示分布，在捕捉给定文本背后隐藏的知识方面仍然面临重大挑战。本文所提方法在多种探测视角下对于识别现有黑盒模型的局限性是有效的。

**Conclusion:** 本文的方法证明了现有黑盒模型存在理解隐含知识的局限性，并为研究人员提供了一种以可解释的方式探测黑盒模型的能力。

**Abstract:** Pre-trained Language Models (PLMs) are trained on large amounts of unlabeled
data, yet they exhibit remarkable reasoning skills. However, the
trustworthiness challenges posed by these black-box models have become
increasingly evident in recent years. To alleviate this problem, this paper
proposes a novel Knowledge-guided Probing approach called KnowProb in a
post-hoc explanation way, which aims to probe whether black-box PLMs understand
implicit knowledge beyond the given text, rather than focusing only on the
surface level content of the text. We provide six potential explanations
derived from the underlying content of the given text, including three
knowledge-based understanding and three association-based reasoning. In
experiments, we validate that current small-scale (or large-scale) PLMs only
learn a single distribution of representation, and still face significant
challenges in capturing the hidden knowledge behind a given text. Furthermore,
we demonstrate that our proposed approach is effective for identifying the
limitations of existing black-box models from multiple probing perspectives,
which facilitates researchers to promote the study of detecting black-box
models in an explainable way.

</details>


### [24] [Decoding Alignment: A Critical Survey of LLM Development Initiatives through Value-setting and Data-centric Lens](https://arxiv.org/abs/2508.16982)
*Ilias Chalkidis*

Main category: cs.CL

> 分析了6个大型语言模型开发项目中AI对齐的做法，强调了价值设定和数据驱动的重要性，并讨论了相关更广泛的问题。

<details>
  <summary>Details</summary>

**Motivation:** 尽管AI对齐特别是强化学习方面的研究受到了广泛的关注，但关于价值设定和模型使用的数据来源等更广泛问题的研究较少，因此需要从这些方面进行更深入的研究。

**Method:** 通过调查和审计6个大型语言模型开发项目（包括OpenAI的GPT、Anthropic的Claude、Google的Gemini、Meta的Llama、Google的Gemma和阿里的Qwen）的公开资料来研究价值设定和数据驱动的角度下的对齐方法，这些项目在过去三年内发布。

**Result:** 研究结果详细记录了每个项目的情况，并从价值设定和数据驱动的角度进行了总结。

**Conclusion:** 讨论了研究发现所涉及的更广泛相关问题，强调了在开发大型语言模型时需要考虑更广泛的对齐视角。

**Abstract:** AI Alignment, primarily in the form of Reinforcement Learning from Human
Feedback (RLHF), has been a cornerstone of the post-training phase in
developing Large Language Models (LLMs). It has also been a popular research
topic across various disciplines beyond Computer Science, including Philosophy
and Law, among others, highlighting the socio-technical challenges involved.
Nonetheless, except for the computational techniques related to alignment,
there has been limited focus on the broader picture: the scope of these
processes, which primarily rely on the selected objectives (values), and the
data collected and used to imprint such objectives into the models. This work
aims to reveal how alignment is understood and applied in practice from a
value-setting and data-centric perspective. For this purpose, we investigate
and survey (`audit') publicly available documentation released by 6 LLM
development initiatives by 5 leading organizations shaping this technology,
focusing on proprietary (OpenAI's GPT, Anthropic's Claude, Google's Gemini) and
open-weight (Meta's Llama, Google's Gemma, and Alibaba's Qwen) initiatives, all
published in the last 3 years. The findings are documented in detail per
initiative, while there is also an overall summary concerning different
aspects, mainly from a value-setting and data-centric perspective. On the basis
of our findings, we discuss a series of broader related concerns.

</details>


### [25] [ReFactX: Scalable Reasoning with Reliable Facts via Constrained Generation](https://arxiv.org/abs/2508.16983)
*Riccardo Pozzi,Matteo Palmonari,Andrea Coletta,Luigi Bellomarini,Jens Lehmann,Sahar Vahdati*

Main category: cs.CL

> 本文提出了一种新的方法，使大型语言模型（LLM）能够访问外部知识，而不依赖额外的模型或服务，并展示了该方法在问答任务中的有效性和可扩展性。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLM）在缺乏足够的信息来满足用户指令时，会产生不可靠的回应，这是由于知识差距和幻觉造成的。虽然现有的方法，如检索增强生成（RAG）和工具使用，通过整合外部知识来解决这些问题，但这些方法依赖于其他模型或服务，导致复杂的工作流程，潜在的错误传播，并且通常需要模型处理大量令牌。

**Method:** 本研究提出了一种可扩展的方法，使大型语言模型（LLM）能够访问外部知识，而不依赖于检索器或辅助模型。该方法使用带约束生成的预构建前缀树索引。将知识图谱中的三元组以文本事实的形式表达出来，进行分词并索引到前缀树中以实现高效访问。在推理过程中，LLM通过带约束生成来获取外部知识，仅允许生成构成现有事实的令牌序列。

**Result:** 该研究在问答任务上评估了提议的方法，表明它可以扩展到大型知识库（800百万事实），适应特定领域的数据，并取得有效的结果。这些改进在生成时间的开销上微乎其微。

**Conclusion:** 该方法证明了它能够在处理大型知识库时，提供高效的外部知识访问，同时保持较低的生成时间开销，适应特定领域数据，实现在问答任务上的有效性。

**Abstract:** Knowledge gaps and hallucinations are persistent challenges for Large
Language Models (LLMs), which generate unreliable responses when lacking the
necessary information to fulfill user instructions. Existing approaches, such
as Retrieval-Augmented Generation (RAG) and tool use, aim to address these
issues by incorporating external knowledge. Yet, they rely on additional models
or services, resulting in complex pipelines, potential error propagation, and
often requiring the model to process a large number of tokens. In this paper,
we present a scalable method that enables LLMs to access external knowledge
without depending on retrievers or auxiliary models. Our approach uses
constrained generation with a pre-built prefix-tree index. Triples from a
Knowledge Graph are verbalized in textual facts, tokenized, and indexed in a
prefix tree for efficient access. During inference, to acquire external
knowledge, the LLM generates facts with constrained generation which allows
only sequences of tokens that form an existing fact. We evaluate our proposal
on Question Answering and show that it scales to large knowledge bases (800
million facts), adapts to domain-specific data, and achieves effective results.
These gains come with minimal generation-time overhead. ReFactX code is
available at https://github.com/rpo19/ReFactX.

</details>


### [26] [GRADE: Generating multi-hop QA and fine-gRAined Difficulty matrix for RAG Evaluation](https://arxiv.org/abs/2508.16994)
*Jeongsoo Lee,Daeyong Kwon,Kyohoon Jin*

Main category: cs.CL

> 研究提出了一种评估框架\textsc{GRADE}，该框架重点在于评估和改善多步推理中的知识密集型NLP任务表现，其效果在实验中被证实有效。

<details>
  <summary>Details</summary>

**Motivation:** 现有的评估方法往往忽视了知识密集型NLP任务中的结构性复杂性和多步推理需要，在实际应用中造成不足。提出\textsc{GRADE}以弥补这一不足。

**Method:** 提出了一种名为\textsc{GRADE}的新评估框架，以建模任务难度的两个正交维度：（1）推理深度，由推理步骤的数量定义；（2）查询与其支撑证据之间的语义距离。通过从事实新闻文章中提取知识图谱并通过语义聚类恢复缺失的链接，构建了一个合成的多跳问题回答数据集，并生成多样性且难度可控的查询。框架核心是一个2D难度矩阵，结合生成器端和检索器端的难度。

**Result:** 在多个领域和模型的实验表明，错误率与提出的难度度量有强烈的关联性，验证了其诊断性效用。

**Conclusion:** \textsc{GRADE}能够对RAG性能进行精细分析，并为评估和改善实际应用中的多步推理能力提供了可扩展的基础。

**Abstract:** Retrieval-Augmented Generation (RAG) systems are widely adopted in
knowledge-intensive NLP tasks, but current evaluations often overlook the
structural complexity and multi-step reasoning required in real-world
scenarios. These benchmarks overlook key factors such as the interaction
between retrieval difficulty and reasoning depth. To address this gap, we
propose \textsc{GRADE}, a novel evaluation framework that models task
difficulty along two orthogonal dimensions: (1) reasoning depth, defined by the
number of inference steps (hops), and (2) semantic distance between the query
and its supporting evidence. We construct a synthetic multi-hop QA dataset from
factual news articles by extracting knowledge graphs and augmenting them
through semantic clustering to recover missing links, allowing us to generate
diverse and difficulty-controlled queries. Central to our framework is a 2D
difficulty matrix that combines generator-side and retriever-side difficulty.
Experiments across multiple domains and models show that error rates strongly
correlate with our difficulty measures, validating their diagnostic utility.
\textsc{GRADE} enables fine-grained analysis of RAG performance and provides a
scalable foundation for evaluating and improving multi-hop reasoning in
real-world applications.

</details>


### [27] [DeAR: Dual-Stage Document Reranking with Reasoning Agents via LLM Distillation](https://arxiv.org/abs/2508.16998)
*Abdelrahman Abdallah,Jamshid Mozafari,Bhawna Piryani,Adam Jatowt*

Main category: cs.CL

> DeAR是一种双阶段开源框架，通过将细粒度相关性评分与全面跨文档分析解耦来提高列表式文档重新排序的准确性和可解释性。

<details>
  <summary>Details</summary>

**Motivation:** 现有单一模型很难同时实现细粒度相关性评分与全面的跨文档分析之间的平衡，因此需要更加有效的解决方法来改进文档重新排序技术。

**Method:** 该研究采用两阶段方法，第一阶段通过混合的交叉熵、RankNet和KL散度损失函数从冻结的13B LLaMA模型中压制出压缩到	extbf{	extbackslash{}{3, 8	extbackslash{}{B}}}的学生模型的token级别的相关信号，确保强大的点评分。第二阶段，附加一个LoRA适配器并通过20K由GPT-4o生成的思考链序列进行微调，以实现带有自然语言理由的列表级别推理。

**Result:** {
  "tldr": "DeAR is a dual-stage open-source framework that improves listwise document reranking accuracy and interpretability by decoupling fine-grained relevance scoring and holistic cross-document analysis.", 
  "motivation": "There is a need to balance fine-grained relevance scoring with holistic cross-document analysis in document reranking, which existing single models find challenging.", 
  "method": "Stage 1 uses a hybrid loss function for pointwise scoring; Stage 2 fine-tunes with LoRA adapters for listwise reasoning with natural-language justifications.", 
  "result": "DeAR achieves superior performance on TREC-DL19/20, eight BEIR datasets, and NovelEval-2306, surpassing open-source baselines and even surpassing GPT-4 in certain metrics.", 
  "conclusion": "Dual-loss distillation is effective for stable calibration, making DeAR a powerful and interpretable framework for document reranking systems. The model also demonstrates strong performance in open-domain QA."]}

**Conclusion:** 该研究提出了一种名为DeAR的两阶段开源框架，通过解耦细粒度相关性评分和全面的跨文档分析，提高了列表式文档重新排序的准确性和可解释性。第一阶段使用混合损失函数进行点评分，第二阶段通过LoRA适配器的精细调整进行列表级别推理并提供自然语言理由。在多个数据集评估中，DeAR的表现超越了开源基线模型，甚至在某些指标上超过了GPT-4，并在开放领域问答中也表现出色，表明这种模型具有强大的修正校准能力。

**Abstract:** Large Language Models (LLMs) have transformed listwise document reranking by
enabling global reasoning over candidate sets, yet single models often struggle
to balance fine-grained relevance scoring with holistic cross-document
analysis. We propose \textbf{De}ep\textbf{A}gent\textbf{R}ank (\textbf{\DeAR}),
an open-source framework that decouples these tasks through a dual-stage
approach, achieving superior accuracy and interpretability. In \emph{Stage 1},
we distill token-level relevance signals from a frozen 13B LLaMA teacher into a
compact \{3, 8\}B student model using a hybrid of cross-entropy, RankNet, and
KL divergence losses, ensuring robust pointwise scoring. In \emph{Stage 2}, we
attach a second LoRA adapter and fine-tune on 20K GPT-4o-generated
chain-of-thought permutations, enabling listwise reasoning with
natural-language justifications. Evaluated on TREC-DL19/20, eight BEIR
datasets, and NovelEval-2306, \DeAR surpasses open-source baselines by +5.1
nDCG@5 on DL20 and achieves 90.97 nDCG@10 on NovelEval, outperforming GPT-4 by
+3.09. Without fine-tuning on Wikipedia, DeAR also excels in open-domain QA,
achieving 54.29 Top-1 accuracy on Natural Questions, surpassing baselines like
MonoT5, UPR, and RankGPT. Ablations confirm that dual-loss distillation ensures
stable calibration, making \DeAR a highly effective and interpretable solution
for modern reranking systems.\footnote{Dataset and code available at
https://github.com/DataScienceUIBK/DeAR-Reranking.}.

</details>


### [28] [KL-Regularised Q-Learning: A Token-level Action-Value perspective on Online RLHF](https://arxiv.org/abs/2508.17000)
*Jason R Brown,Lennie Wells,Edward James Young,Sergio Bacallado*

Main category: cs.CL

> 本文引入了一种新的语言模型强化学习方法KLQ，该方法虽然动机与PPO不同，但在特定情况下与PPO等效，实验结果显示其性能与PPO相当，且在裁判评估中表现更优。

<details>
  <summary>Details</summary>

**Motivation:** PPO在语言模型强化学习中表现出色，但其克隆散度约束处理方式是临时拼凑的，本文旨在通过开发KLQ，提供一个更系统的方法来处理这一问题。

**Method:** 我们提出了KL正则化Q学习（KLQ）方法，这是一种针对语言模型从人类反馈中进行强化学习的新动作值强化学习方法。

**Result:** 我们展示了KLQ在两个关键的语言生成任务——摘要生成和单轮对话中进行基准测试时，其性能与PPO相当，且在大型语言模型作为裁判的评估中，KLQ的胜率始终高于PPO。

**Conclusion:** 尽管KLQ与PPO在动机上有显著差异，但KLQ在特定情况下能等效于PPO，并且在关键的语言生成任务中展示了优异的表现，表明KLQ是LM-RLHF领域一个有价值的探索。

**Abstract:** Proximal Policy Optimisation (PPO) is an established and effective policy
gradient algorithm used for Language Model Reinforcement Learning from Human
Feedback (LM-RLHF). PPO performs well empirically but has a heuristic
motivation and handles the KL-divergence constraint used in LM-RLHF in an
ad-hoc manner. In this paper, we develop a a new action-value RL method for the
LM-RLHF setting, KL-regularised Q-Learning (KLQ). We then show that our method
is equivalent to a version of PPO in a certain specific sense, despite its very
different motivation. Finally, we benchmark KLQ on two key language generation
tasks -- summarisation and single-turn dialogue. We demonstrate that KLQ
performs on-par with PPO at optimising the LM-RLHF objective, and achieves a
consistently higher win-rate against PPO on LLM-as-a-judge evaluations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [29] [Towards High-Precision Depth Sensing via Monocular-Aided iToF and RGB Integration](https://arxiv.org/abs/2508.16579)
*Yansong Du,Yutong Deng,Yuting Zhou,Feiyu Jiao,Jian Song,Xun Guan*

Main category: cs.CV

> \u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684iToF-RGB\u8def\u5f84\u8fde\u63a5\u67b6\u6784\uff0c\u4ee5\u63d0\u9ad8\u975e\u76f8\u5bf9\u65f6\u95f4\u4e4b\u95f4\u68c0\u63a8\u6240\u6709\u5145\u5448\u95ee\u9898\uff0c\u5f53\u51fa\u7684\u67b6\u6784\u65b9\u6cd5\u4e00\u5b9a\u52a0\u6df1\u4e86\u6df1\u5ea6\u7684\u5b9e\u9645\u7b49\u7ea7\uff0c\u6539\u5584\u4e86\u8fb9\u7f18\u6e05\u6670\u5ea6\uff0c\u5e76\u5b9e\u73b0\u4e86\u6765\u6e90\u89d2\u5ea6\u7684\u5e7f\u5145\u4e0a\u6d17\u3002

<details>
  <summary>Details</summary>

**Motivation:** \u8be5\u8bba\u6587\u7684\u52a8\u6001\u662f\u89e3\u51b3iToF\u6df1\u5ea6\u6c42\u6d4b\u6240\u51fa\u7684\u5145\u5448\u95ee\u9898\uff0c\u5982\u7a7a\u95f4\u89e3\u7f16\u5ea6\u4f4e\uff0c\u6765\u6e90\u89d2\u5ea6\u9650\u52a8\uff0c\u53ca\u5176\u5728\u590d\u6742\u6�错误，我将直接提供修复后的结果。论文的动机是解决iToF深度感知所面临的问题，如低空间分辨率、视野局限以及在复杂场景中的结构失真问题。

**Method:** \u65b0\u7684\u67b6\u6784\u9996\u5148\u5c06iToF\u6df1\u5ea6\u56fe\u8bef\u63a8\u5230RGB\u578b\u5708\u7cfb\u7edf\u4e2d\uff0c\u7136\u540e\u901a\u8fc7\u53cc\u7f16\u7801\u5408\u5e76\u7f51\u7edc\u53d6\u5f97\u4e00\u79cd\u76f8\u4e92\u8868\u652f\u7684\u7279\u5f81\uff0c\u5148\u63a8\u51fa\uff0c\u672c\u6587\u5e26\u6709\u4e00\u4e9b显错，我将集中修复技术方法描述。新技术框架首先将iToF深度图重投影到RGB坐标系中，然后通过双编码融合网络提取相互支持的特征。该方法结合了单目深度先验和跨模式结构线索，进行深度超分辨率处理。

**Result:** {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684iToF-RGB\u8def\u5f84\u8fde\u63a5\u67b6\u6784\uff0c\u4ee5\u63d0\u9ad8\u975e\u76f8\u5bf9\u65f6\u95f4\u4e4b\u메 SSR错误，让我直接给出分析结果吧。论文提出了一种新型的iToF-RGB深度融合框架，旨在提高非直接飞行时间(iToF)深度感知技术的空间分辨率、视野范围和结构失真问题，特别是在复杂场景中的表现。通过精确的几何校准与RGB坐标系对齐，再用双编码融合网络提取联合互补特征，恢复精细结构细节，进行深度超分辨率，从而达到了深度准确性、边缘清晰度和视野扩展的提升。实验表明，该框架在准确性、结构一致性及视觉质量方面显著优于现有方法。

**Conclusion:** \u8be5\u65b0\u67b6\u6784\u65b9\u6cd5\u4e00\u5b9a\u52a0\u6df1\u4e86\u6df1\u5ea6\u7684\u5b9e\u9645\u7b49\u7ea7\uff0c\u6539\u5584\u4e86\u8fb9\u7f18\u6e05\u6670\u5ea6\uff0c\u5e76\u5b9e\u73b0\u4e86\u6765\u6e90\u89d2\u5ea6\u7684\u5e7f\u5145\u4e0a\u6d17\u3002\u5bf9\u5404\u79cd\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u4ee5\u53ca\u73b0\u6709\u6b63\u786e\u65b9\u6cd5\u7684\u5bf9\u6bd4\uff0c\u8868\u660e\u5f53\u51fa\u7684\u67b6\u6784\u52a0\u5f3a\u4e86\u6df1\u5ea6\u7684\u5b9e\u9645\u6b63\u786e\uff0c\u7ed3\u6783\u76f8\u5173\u60c5\u4e0e\u89c2\u5bdf\u8d44\u8d28\u3002

**Abstract:** This paper presents a novel iToF-RGB fusion framework designed to address the
inherent limitations of indirect Time-of-Flight (iToF) depth sensing, such as
low spatial resolution, limited field-of-view (FoV), and structural distortion
in complex scenes. The proposed method first reprojects the narrow-FoV iToF
depth map onto the wide-FoV RGB coordinate system through a precise geometric
calibration and alignment module, ensuring pixel-level correspondence between
modalities. A dual-encoder fusion network is then employed to jointly extract
complementary features from the reprojected iToF depth and RGB image, guided by
monocular depth priors to recover fine-grained structural details and perform
depth super-resolution. By integrating cross-modal structural cues and depth
consistency constraints, our approach achieves enhanced depth accuracy,
improved edge sharpness, and seamless FoV expansion. Extensive experiments on
both synthetic and real-world datasets demonstrate that the proposed framework
significantly outperforms state-of-the-art methods in terms of accuracy,
structural consistency, and visual quality.

</details>


### [30] [CountLoop: Training-Free High-Instance Image Generation via Iterative Agent Guidance](https://arxiv.org/abs/2508.16644)
*Anindya Mondal,Ayan Banerjee,Sauradip Nag,Josep Lladós,Xiatian Zhu,Anjan Dutta*

Main category: cs.CV

> 本文介绍了一个无训练框架CountLoop，它通过迭代结构化反馈显著提高了扩散模型生成指定数量对象的准确性和质量。

<details>
  <summary>Details</summary>

**Motivation:** 尽管扩散模型在生成逼真的图像方面取得了显著进展，但仍无法可靠地生成确切数量对象的场景，特别是复杂的高密度设置。因此，本文旨在提供一个框架，通过迭代反馈提高模型生成指定数量对象的精度。

**Method:** 我们提出了一种名为CountLoop的无训练框架，该框架通过迭代结构化反馈为扩散模型提供了准确的实例控制。该方法在图像生成和多模式代理评估之间交替进行，其中语言引导的规划器和批评者对对象数量、空间排列及属性一致性进行评估。此外，我们引入了实例驱动的关注掩码和组合生成技术，以进一步提高物体之间的分离度。

**Result:** 实验结果显示，我们提出的CountLoop方法在对象数量的准确性、空间保真度和视觉质量上表现出色，并且在多个基准测试中优于其他基线方法。

**Conclusion:** 实验结果表明，CountLoop在COCO Count、T2I CompBench和两个新的高实例基准测试中实现了高达98%的计数准确性，同时保持空间保真度和视觉质量，超越了布局基础和梯度引导的基线分数0.97。

**Abstract:** Diffusion models have shown remarkable progress in photorealistic image
synthesis, yet they remain unreliable for generating scenes with a precise
number of object instances, particularly in complex and high-density settings.
We present CountLoop, a training-free framework that provides diffusion models
with accurate instance control through iterative structured feedback. The
approach alternates between image generation and multimodal agent evaluation,
where a language-guided planner and critic assess object counts, spatial
arrangements, and attribute consistency. This feedback is then used to refine
layouts and guide subsequent generations. To further improve separation between
objects, especially in occluded scenes, we introduce instance-driven attention
masking and compositional generation techniques. Experiments on COCO Count, T2I
CompBench, and two new high-instance benchmarks show that CountLoop achieves
counting accuracy of up to 98% while maintaining spatial fidelity and visual
quality, outperforming layout-based and gradient-guided baselines with a score
of 0.97.

</details>


### [31] [Do VLMs Have Bad Eyes? Diagnosing Compositional Failures via Mechanistic Interpretability](https://arxiv.org/abs/2508.16652)
*Ashwath Vaithinathan Aravindan,Abha Jha,Mihir Kulkarni*

Main category: cs.CV

> 研究发现视觉-语言模型中的单个神经元表示多个特征，导致组合泛化和对象绑定能力受限。

<details>
  <summary>Details</summary>

**Motivation:** 尽管视觉-语言模型在结合视觉和文本信息的任务中表现出色，但它们在处理新型组合的物体及其属性时效果不佳。

**Method:** 使用机制可解释性技术研究视觉-语言模型在组合泛化和对象绑定方面失败的原因。

**Result:** 研究表明，CLIP视觉编码器MLP层的单个神经元表示多个特征，这种“叠加”直接影响了组合特征表示，从而影响了组合推理和对象绑定能力。

**Conclusion:** 这项研究为揭示视觉-语言模型中组合失败的机制根源提供了初步步骤。

**Abstract:** Vision-Language Models (VLMs) have shown remarkable performance in
integrating visual and textual information for tasks such as image captioning
and visual question answering. However, these models struggle with
compositional generalization and object binding, which limit their ability to
handle novel combinations of objects and their attributes. Our work explores
the root causes of these failures using mechanistic interpretability
techniques. We show evidence that individual neurons in the MLP layers of
CLIP's vision encoder represent multiple features, and this "superposition"
directly hinders its compositional feature representation which consequently
affects compositional reasoning and object binding capabilities. We hope this
study will serve as an initial step toward uncovering the mechanistic roots of
compositional failures in VLMs. The code and supporting results can be found
https://github.com/Mystic-Slice/Do-VLMs-Have-Bad-Eyes .

</details>


### [32] [MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and LLM Spatial Reasoning](https://arxiv.org/abs/2508.16654)
*Chenghao Liu,Zhimu Zhou,Jiachen Zhang,Minghao Zhang,Songfang Huang,Huiling Duan*

Main category: cs.CV

> A novel Vision-and-Language Navigation framework, MSNav, with integrated Memory, Spatial, and Decision Modules, shows superior performance and addresses critical issues of current methods.

<details>
  <summary>Details</summary>

**Motivation:** To address the critical vulnerabilities in current Vision-and-Language Navigation (VLN) approaches such as poor spatial reasoning, weak cross-modal grounding, and long-horizon task memory overload.

**Method:** Memory Spatial Navigation (MSNav) is introduced which includes a Memory Module for selective node pruning in dynamic map memory, a Spatial Module for spatial reasoning and object relationship inference, and a Decision Module for LLM-based path planning.

**Result:** The introduced MSNav framework outperforms previous methods on the R2R and REVERIE datasets, with a fine-tuned Qwen-Spatial model showing superior performance in object list extraction compared to commercial LLMs.

**Conclusion:** MSNav, with its integrated Memory, Spatial, and Decision Modules, achieves state-of-the-art performance in Vision-and-Language Navigation tasks, as demonstrated by significant improvements in Success Rate (SR) and Success weighted by Path Length (SPL) on the R2R and REVERIE datasets.

**Abstract:** Vision-and-Language Navigation (VLN) requires an agent to interpret natural
language instructions and navigate complex environments. Current approaches
often adopt a "black-box" paradigm, where a single Large Language Model (LLM)
makes end-to-end decisions. However, it is plagued by critical vulnerabilities,
including poor spatial reasoning, weak cross-modal grounding, and memory
overload in long-horizon tasks. To systematically address these issues, we
propose Memory Spatial Navigation(MSNav), a framework that fuses three modules
into a synergistic architecture, which transforms fragile inference into a
robust, integrated intelligence. MSNav integrates three modules: Memory Module,
a dynamic map memory module that tackles memory overload through selective node
pruning, enhancing long-range exploration; Spatial Module, a module for spatial
reasoning and object relationship inference that improves endpoint recognition;
and Decision Module, a module using LLM-based path planning to execute robust
actions. Powering Spatial Module, we also introduce an Instruction-Object-Space
(I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp),
which outperforms leading commercial LLMs in object list extraction, achieving
higher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the
Room-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art
performance with significant improvements in Success Rate (SR) and Success
weighted by Path Length (SPL).

</details>


### [33] [Optimizing Hyper parameters in CNN for Soil Classification using PSO and Whale Optimization Algorithm](https://arxiv.org/abs/2508.16660)
*Yasir Nooruldeen Ibrahim,Fawziya Mahmood Ramo,Mahmood Siddeeq Qadir,Muna Jaffer Al-Shamdeen*

Main category: cs.CV

> 本研究运用卷积神经网络结合最优超参数选择的算法（鲸群优化和粒子群优化）进行土壤分类，取得了良好的效果。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于通过改进土壤图像的分类方法来更好地进行土地管理，增加农业产出，以及为环境问题提供实际解决方案。人工智能技术的应用可以提升土壤类型识别的效率和准确性，从而有助于降低风险、改善性能并为决策提供支持。

**Method:** 本研究采用了卷积神经网络构建智能模型进行土壤分类，并使用了机器学习算法来提高土壤分类的性能。为了优化卷积神经网络算法的实施和性能，使用了鲸群优化算法和粒子群优化算法来选择最佳的超参数，并比较了这两种算法在多类土壤分类中的结果表现。

**Result:** 研究采用准确率和F1度量来进行系统测试，结果表明所提出的模型在土壤分类方面具有高效的性能和优良的结果。

**Conclusion:** 通过实验，研究证明了所提出的工作在土壤类型分类中是高效的，并取得了理想的结果，为土地管理和农业产出提供了重要的基础。

**Abstract:** Classifying soil images contributes to better land management, increased
agricultural output, and practical solutions for environmental issues. The
development of various disciplines, particularly agriculture, civil
engineering, and natural resource management, is aided by understanding of soil
quality since it helps with risk reduction, performance improvement, and sound
decision-making . Artificial intelligence has recently been used in a number of
different fields. In this study, an intelligent model was constructed using
Convolutional Neural Networks to classify soil kinds, and machine learning
algorithms were used to enhance the performance of soil classification . To
achieve better implementation and performance of the Convolutional Neural
Networks algorithm and obtain valuable results for the process of classifying
soil type images, swarm algorithms were employed to obtain the best performance
by choosing Hyper parameters for the Convolutional Neural Networks network
using the Whale optimization algorithm and the Particle swarm optimization
algorithm, and comparing the results of using the two algorithms in the process
of multiple classification of soil types. The Accuracy and F1 measures were
adopted to test the system, and the results of the proposed work were efficient
result

</details>


### [34] [QA-VLM: Providing human-interpretable quality assessment for wire-feed laser additive manufacturing parts with Vision Language Models](https://arxiv.org/abs/2508.16661)
*Qiaojie Zheng,Jiucai Zhang,Joy Gockel,Michael B. Wakin,Craig Brice,Xiaoli Zhang*

Main category: cs.CV

> The paper introduces a QA-VLM framework for additive manufacturing that uses vision-language models enriched with domain knowledge to provide interpretable quality assessments.

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of traditional manual QA and black-box machine learning methods in AM that lack transparency and trustworthiness.

**Method:** Develop a QA-VLM framework leveraging attention mechanisms and reasoning capabilities of VLMs, enhanced with specific knowledge from published literature.

**Result:** The QA-VLM framework showed higher validity and consistency in quality assessments compared to standard VLMs when evaluated on 24 single-bead samples.

**Conclusion:** The novel approach can provide reliable, explainable quality assessments for AM applications, enhancing trust and practical adoption.

**Abstract:** Image-based quality assessment (QA) in additive manufacturing (AM) often
relies heavily on the expertise and constant attention of skilled human
operators. While machine learning and deep learning methods have been
introduced to assist in this task, they typically provide black-box outputs
without interpretable justifications, limiting their trust and adoption in
real-world settings. In this work, we introduce a novel QA-VLM framework that
leverages the attention mechanisms and reasoning capabilities of
vision-language models (VLMs), enriched with application-specific knowledge
distilled from peer-reviewed journal articles, to generate human-interpretable
quality assessments. Evaluated on 24 single-bead samples produced by laser wire
direct energy deposition (DED-LW), our framework demonstrates higher validity
and consistency in explanation quality than off-the-shelf VLMs. These results
highlight the potential of our approach to enable trustworthy, interpretable
quality assessment in AM applications.

</details>


### [35] [The Loupe: A Plug-and-Play Attention Module for Amplifying Discriminative Features in Vision Transformers](https://arxiv.org/abs/2508.16663)
*Naren Sengodan*

Main category: cs.CV

> The Loupe, a novel attention module, improves model interpretability and performance in Fine-Grained Visual Classification tasks.

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of interpretability in largeVision Transformers, especially in critical applications like biodiversity and medical diagnostics, where precision is key.

**Method:** The Loupe, a lightweight attention module, is introduced to be plugged into pre-trained models like the Swin Transformer. Trained end-to-end with a specific loss, it encourages the model to focus on key parts without explicit annotations.

**Result:** The Loupe boosts a Swin-Base model's accuracy on the CUB-200-2011 dataset from 85.40% to 88.06%, demonstrating effective localization of key features.

**Conclusion:** The Loupe not only enhances performance but also offers clear visual explanations and understanding of decision-making in the model.

**Abstract:** Fine-Grained Visual Classification (FGVC) is a critical and challenging area
within computer vision, demanding the identification of highly subtle,
localized visual cues. The importance of FGVC extends to critical applications
such as biodiversity monitoring and medical diagnostics, where precision is
paramount. While large-scale Vision Transformers have achieved state-of-the-art
performance, their decision-making processes often lack the interpretability
required for trust and verification in such domains. In this paper, we
introduce The Loupe, a novel, lightweight, and plug-and-play attention module
designed to be inserted into pre-trained backbones like the Swin Transformer.
The Loupe is trained end-to-end with a composite loss function that implicitly
guides the model to focus on the most discriminative object parts without
requiring explicit part-level annotations. Our unique contribution lies in
demonstrating that a simple, intrinsic attention mechanism can act as a
powerful regularizer, significantly boosting performance while simultaneously
providing clear visual explanations. Our experimental evaluation on the
challenging CUB-200-2011 dataset shows that The Loupe improves the accuracy of
a Swin-Base model from 85.40% to 88.06%, a significant gain of 2.66%.
Crucially, our qualitative analysis of the learned attention maps reveals that
The Loupe effectively localizes semantically meaningful features, providing a
valuable tool for understanding and trusting the model's decision-making
process.

</details>


### [36] [COVID19 Prediction Based On CT Scans Of Lungs Using DenseNet Architecture](https://arxiv.org/abs/2508.16670)
*Deborup Sanyal*

Main category: cs.CV

> 该项目旨在通过分析COVID19阳性患者的肺部CT扫描结果来帮助医生判断病情的严重程度，采用了卷积神经网络模型进行预测。

<details>
  <summary>Details</summary>

**Motivation:** 由于COVID19的全球大流行导致了医疗系统的崩溃和大量生命的丧失，急需一种准确的方法帮助医生快速评估患者的病情。

**Method:** 使用卷积神经网络模型分析患者在阳性检测结果后一个月内的肺部CT扫描以预测病情的严重程度。

**Result:** 模型可根据CT扫描判断感染状况是良好还是不利，不利的情况包括插管或死亡。

**Conclusion:** 计算机模型尤其是通过机器学习训练的神经网络，在病情评估中显示出更高的准确性，且较少受到人为误判的影响。

**Abstract:** COVID19 took the world by storm since December 2019. A highly infectious
communicable disease, COVID19 is caused by the SARSCoV2 virus. By March 2020,
the World Health Organization (WHO) declared COVID19 as a global pandemic. A
pandemic in the 21st century after almost 100 years was something the world was
not prepared for, which resulted in the deaths of around 1.6 million people
worldwide. The most common symptoms of COVID19 were associated with the
respiratory system and resembled a cold, flu, or pneumonia. After extensive
research, doctors and scientists concluded that the main reason for lives being
lost due to COVID19 was failure of the respiratory system. Patients were dying
gasping for breath. Top healthcare systems of the world were failing badly as
there was an acute shortage of hospital beds, oxygen cylinders, and
ventilators. Many were dying without receiving any treatment at all. The aim of
this project is to help doctors decide the severity of COVID19 by reading the
patient's Computed Tomography (CT) scans of the lungs. Computer models are less
prone to human error, and Machine Learning or Neural Network models tend to
give better accuracy as training improves over time. We have decided to use a
Convolutional Neural Network model. Given that a patient tests positive, our
model will analyze the severity of COVID19 infection within one month of the
positive test result. The severity of the infection may be promising or
unfavorable (if it leads to intubation or death), based entirely on the CT
scans in the dataset.

</details>


### [37] [MedRepBench: A Comprehensive Benchmark for Medical Report Interpretation](https://arxiv.org/abs/2508.16674)
*Fangxin Shang,Yuan Xia,Dalu Yang,Yahui Wang,Binglin Yang*

Main category: cs.CV

> 提出MedRepBench用于评估医疗报告的结构化理解，使用视觉语言模型及OCR结合大语言模型进行文本评估，通过客观与主观评估确保全面性。

<details>
  <summary>Details</summary>

**Motivation:** 虽然最近的视觉语言模型（VLMs）和大语言模型（LLMs）展示了通用文档理解的能力，但在医疗报告的结构化解读质量上缺乏标准化的基准。为了弥补这一差距，并评估模型在医疗报告理解方面的性能，提出了MedRepBench。

**Method:** 提出MedRepBench，一个由1,900份真实的、去识别化的中文医疗报告组成的全面基准，用于评估端到端的视觉语言模型在结构化医疗报告理解中的表现。为了实现可控的比较，还包含了仅使用文本的评估场景，利用高质量的OCR输出结合大语言模型。评估框架包括两种互补的协议：一种是通过衡量结构化临床项目的字段级召回率来进行客观评估，另一种是使用强大的大语言模型作为评分代理进行自动主观评估，评估事实性、可解释性和推理质量。

**Result:** 通过客观指标设计奖励函数，并应用组相对策略优化（GRPO）改进中间规模的视觉语言模型，实现高达6%的召回率提升。同时发现OCR+LLM管道尽管表现强大，但在布局感知和延时方面存在问题。

**Conclusion:** 提出MedRepBench以填补在医疗报告结构化解读质量评估上的标准化基准的空白，通过设计的评估框架和优化策略，能够对现有模型进行改进，指出未来工作应关注布局感知和延迟问题。

**Abstract:** Medical report interpretation plays a crucial role in healthcare, enabling
both patient-facing explanations and effective information flow across clinical
systems. While recent vision-language models (VLMs) and large language models
(LLMs) have demonstrated general document understanding capabilities, there
remains a lack of standardized benchmarks to assess structured interpretation
quality in medical reports. We introduce MedRepBench, a comprehensive benchmark
built from 1,900 de-identified real-world Chinese medical reports spanning
diverse departments, patient demographics, and acquisition formats. The
benchmark is designed primarily to evaluate end-to-end VLMs for structured
medical report understanding. To enable controlled comparisons, we also include
a text-only evaluation setting using high-quality OCR outputs combined with
LLMs, allowing us to estimate the upper-bound performance when character
recognition errors are minimized. Our evaluation framework supports two
complementary protocols: (1) an objective evaluation measuring field-level
recall of structured clinical items, and (2) an automated subjective evaluation
using a powerful LLM as a scoring agent to assess factuality, interpretability,
and reasoning quality. Based on the objective metric, we further design a
reward function and apply Group Relative Policy Optimization (GRPO) to improve
a mid-scale VLM, achieving up to 6% recall gain. We also observe that the
OCR+LLM pipeline, despite strong performance, suffers from layout-blindness and
latency issues, motivating further progress toward robust, fully vision-based
report understanding.

</details>


### [38] [Two-Stage Framework for Efficient UAV-Based Wildfire Video Analysis with Adaptive Compression and Fire Source Detection](https://arxiv.org/abs/2508.16739)
*Yanbing Bai,Rui-Yang Ju,Lemeng Zhao,Junjie Hu,Jianchao Bi,Erick Mas,Shunichi Koshimura*

Main category: cs.CV

> A lightweight two-stage framework is proposed for real-time wildfire monitoring on UAVs, reducing computational costs while maintaining high accuracy in both classification and fire source localization.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitation of limited computational resources on UAVs, enabling real-time aerial video analysis for disaster emergency response.

**Method:** The paper proposes a two-stage framework for real-time wildfire monitoring and fire source detection on UAVs: Stage 1 involves identifying and discarding redundant video clips using frame compression and a policy network with a station point mechanism to improve prediction accuracy. Stage 2 employs an improved YOLOv8 model to localize the fire source.

**Result:** The method reduces computational costs while maintaining classification accuracy in Stage 1, and achieves higher detection accuracy with similar inference time in Stage 2 compared to baseline methods.

**Conclusion:** The proposed framework demonstrates benefits in real-time wildfire monitoring for UAVs, effectively addressing the need for efficient and accurate aerial video analysis in disaster scenarios.

**Abstract:** Unmanned Aerial Vehicles (UAVs) have become increasingly important in
disaster emergency response by enabling real-time aerial video analysis. Due to
the limited computational resources available on UAVs, large models cannot be
run independently for real-time analysis. To overcome this challenge, we
propose a lightweight and efficient two-stage framework for real-time wildfire
monitoring and fire source detection on UAV platforms. Specifically, in Stage
1, we utilize a policy network to identify and discard redundant video clips
using frame compression techniques, thereby reducing computational costs. In
addition, we introduce a station point mechanism that leverages future frame
information within the sequential policy network to improve prediction
accuracy. In Stage 2, once the frame is classified as "fire", we employ the
improved YOLOv8 model to localize the fire source. We evaluate the Stage 1
method using the FLAME and HMDB51 datasets, and the Stage 2 method using the
Fire & Smoke dataset. Experimental results show that our method significantly
reduces computational costs while maintaining classification accuracy in Stage
1, and achieves higher detection accuracy with similar inference time in Stage
2 compared to baseline methods.

</details>


### [39] [CellEcoNet: Decoding the Cellular Language of Pathology with Deep Learning for Invasive Lung Adenocarcinoma Recurrence Prediction](https://arxiv.org/abs/2508.16742)
*Abdul Rehman Akbar,Usama Sajjad,Ziyu Su,Wencheng Li,Fei Xing,Jimmy Ruiz,Wei Chen,Muhammad Khalid Khan Niazi*

Main category: cs.CV

> CellEcoNet通过自然语言处理类比的方式学习细胞细微变化和空间交互，显著提高了肿瘤复发风险预测性能。

<details>
  <summary>Details</summary>

**Motivation:** 尽管进行了手术切除，70%的侵袭性肺腺癌（ILA）患者在五年内复发，而目前的工具无法识别哪些患者需要辅助治疗，以解决这一未满足的临床需求。

**Method:** 开发了名为CellEcoNet的新颖空间感知深度学习框架，通过自然语言类比的方式建模全切片图像（WSIs），将细胞视为单词，细胞邻域视为短语，组织结构视为句子，自动学习这些上下文相关的意义，捕捉微妙变化和空间交互以推断复发风险。

**Result:** 在456个H&E染色的WSIs数据集上，CellEcoNet达到了优越的预测性能（AUC：77.8%，HR：9.54），优于IASLC分级系统（AUC：71.4%，HR：2.36）、AJCC分期（AUC：64.0%，HR：1.17）和最先进的计算方法（AUC：62.2-67.4%）。此外，CellEcoNet展现出了公平性和在不同人口统计学和临床亚组中的一致性能。

**Conclusion:** CellEcoNet不仅实现了卓越的预后性能，而且还标志着一个范式转变，通过解码肿瘤微环境的细胞“语言”揭示了微妙的细胞变异编码复发风险的方式。

**Abstract:** Despite surgical resection, ~70% of invasive lung adenocarcinoma (ILA)
patients recur within five years, and current tools fail to identify those
needing adjuvant therapy. To address this unmet clinical need, we introduce
CellEcoNet, a novel spatially aware deep learning framework that models whole
slide images (WSIs) through natural language analogy, defining a "language of
pathology," where cells act as words, cellular neighborhoods become phrases,
and tissue architecture forms sentences. CellEcoNet learns these
context-dependent meanings automatically, capturing how subtle variations and
spatial interactions derive recurrence risk. On a dataset of 456 H&E-stained
WSIs, CellEcoNet achieved superior predictive performance (AUC:77.8% HR:9.54),
outperforming IASLC grading system (AUC:71.4% HR:2.36), AJCC Stage (AUC:64.0%
HR:1.17) and state-of-the-art computational methods (AUCs:62.2-67.4%).
CellEcoNet demonstrated fairness and consistent performance across diverse
demographic and clinical subgroups. Beyond prognosis, CellEcoNet marks a
paradigm shift by decoding the tumor microenvironment's cellular "language" to
reveal how subtle cell variations encode recurrence risk.

</details>


### [40] [A Framework for Benchmarking Fairness-Utility Trade-offs in Text-to-Image Models via Pareto Frontiers](https://arxiv.org/abs/2508.16752)
*Marco N. Bochernitsan,Rodrigo C. Barros,Lucas S. Kupssinskü*

Main category: cs.CV

> 提出了使用帕累托最优前沿评估文本到图像生成模型的公平性和效用的方法，展示了如何在不牺牲视觉保真度的同时减少社会偏见。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决现有公平性评估方法依赖于定性判断或狭窄比较的问题，这些方法限制了对模型公平性和效用的同时评估，并且阻碍了对去偏方法的可重复评估。

**Method:** Structure

**Result:** 提出的方法使用规范化香农熵和ClipScore来分别评估公平性和效用，并在多个文本到图像模型中应用该方法，显示默认超参数设置在公平性和效用之间的权衡中表现不佳。

**Conclusion:** 说明了该评估方法可以比较不同的文本到图像模型，找出能够优化特定效用下的公平性以及其他情况的方法，并表明改善超参数配置以提高模型表现是可行的。

**Abstract:** Achieving fairness in text-to-image generation demands mitigating social
biases without compromising visual fidelity, a challenge critical to
responsible AI. Current fairness evaluation procedures for text-to-image models
rely on qualitative judgment or narrow comparisons, which limit the capacity to
assess both fairness and utility in these models and prevent reproducible
assessment of debiasing methods. Existing approaches typically employ ad-hoc,
human-centered visual inspections that are both error-prone and difficult to
replicate. We propose a method for evaluating fairness and utility in
text-to-image models using Pareto-optimal frontiers across hyperparametrization
of debiasing methods. Our method allows for comparison between distinct
text-to-image models, outlining all configurations that optimize fairness for a
given utility and vice-versa. To illustrate our evaluation method, we use
Normalized Shannon Entropy and ClipScore for fairness and utility evaluation,
respectively. We assess fairness and utility in Stable Diffusion, Fair
Diffusion, SDXL, DeCoDi, and FLUX text-to-image models. Our method shows that
most default hyperparameterizations of the text-to-image model are dominated
solutions in the fairness-utility space, and it is straightforward to find
better hyperparameters.

</details>


### [41] [WebMMU: A Benchmark for Multimodal Multilingual Website Understanding and Code Generation](https://arxiv.org/abs/2508.16763)
*Rabiul Awal,Mahsa Massoud,Aarash Feizi,Zichao Li,Suyuchen Wang,Christopher Pal,Aishwarya Agrawal,David Vazquez,Siva Reddy,Juan A. Rodriguez,Perouz Taslakian,Spandana Gella,Sai Rajeswar*

Main category: cs.CV

> 研究提出了WebMMU，用于评估网站视觉问题回答、代码编辑和原型到代码生成。结果显示，当前多模态语言模型在功能性和推理方面存在局限性，需要改进。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机在于，以前的基准测试任务往往是分开进行的，而WebMMU希望整合这些任务以评估模型在复杂多步推理、准确元素定位以及功能UI理解和编码方面的能力。

**Method:** WebMMU是一个多语言基准测试，评估了三个核心网络任务：网站视觉问题回答、HTML/CSS/JavaScript代码编辑和原型到代码生成。WebMMU使用专家标注的真实世界网络数据统一了这些任务。

**Result:** 评估表明，尽管多模态大语言模型在基本信息抽取方面表现良好，但在推理和定位、保持功能性的代码编辑以及生成保留层次结构和多语言内容的设计代码时却存在困难。

**Conclusion:** 研究发现揭示了当前多模态大模型的关键局限性，并强调需要改进多模态和跨语言推理能力，以便在未来构建能够自动化多种网络开发任务的网络代理。

**Abstract:** We present WebMMU, a multilingual benchmark that evaluates three core web
tasks: (1) website visual question answering, (2) code editing involving
HTML/CSS/JavaScript, and (3) mockup-to-code generation. Unlike prior benchmarks
that treat these tasks separately, WebMMU unifies them using expert-annotated,
real-world web data to assess models' abilities in complex multi-step
reasoning, precise element grounding, and functional UI comprehension and
coding. Our evaluation shows that while multimodal large language models
(MLLMs) perform well on basic information extraction, they struggle with
reasoning and grounding, editing code to preserve functionality, and generating
design-to-code that maintains hierarchy and supports multilingual content.
These findings reveal key limitations in current MLLMs and underscore the need
for improved multimodal and cross-lingual reasoning to build future web agents
capable of automating diverse web development tasks.

</details>


### [42] [Improving Performance, Robustness, and Fairness of Radiographic AI Models with Finely-Controllable Synthetic Data](https://arxiv.org/abs/2508.16783)
*Stefania L. Moroianu,Christian Bluethgen,Pierre Chambon,Mehdi Cherti,Jean-Benoit Delbrouck,Magdalini Paschali,Brandon Price,Judy Gichoya,Jenia Jitsev,Curtis P. Langlotz,Akshay S. Chaudhari*

Main category: cs.CV

> 提出了一种用于生成胸部放射图像的合成数据生成模型RoentGen-v2，并通过合成数据预训练提升了深度学习模型的性能和公平性。

<details>
  <summary>Details</summary>

**Motivation:** 为了提升深度学习模型在诊断成像中的健壮性和公平性，尤其是在面对多样化的患者群体时。通过合成数据生成来解决数据规模和多样性方面的局限性。

**Method:** 提出了一种文本到图像的扩散模型RoentGen-v2，用于产生具有细致放射学发现和患者人口统计属性控制的胸部X光图像，并提出了一种新的训练策略，即使用合成数据进行监督预训练，然后在真实数据上进行微调。

**Result:** 实验结果表明合成预训练可以显著提高下游分类模型的性能，提升泛化能力和不同人口统计学亚组间的公平性。具体来说，合成预训练相比其他方法提升了6.5%的准确率，并减少了19.3%的误诊公平性差距。

**Conclusion:** 研究表明，合成图像数据在医疗深度学习中具有提升模型性能和公平性的潜力，尤其是在现实数据约束条件下。

**Abstract:** Achieving robust performance and fairness across diverse patient populations
remains a challenge in developing clinically deployable deep learning models
for diagnostic imaging. Synthetic data generation has emerged as a promising
strategy to address limitations in dataset scale and diversity. We introduce
RoentGen-v2, a text-to-image diffusion model for chest radiographs that enables
fine-grained control over both radiographic findings and patient demographic
attributes, including sex, age, and race/ethnicity. RoentGen-v2 is the first
model to generate clinically plausible images with demographic conditioning,
facilitating the creation of a large, demographically balanced synthetic
dataset comprising over 565,000 images. We use this large synthetic dataset to
evaluate optimal training pipelines for downstream disease classification
models. In contrast to prior work that combines real and synthetic data
naively, we propose an improved training strategy that leverages synthetic data
for supervised pretraining, followed by fine-tuning on real data. Through
extensive evaluation on over 137,000 chest radiographs from five institutions,
we demonstrate that synthetic pretraining consistently improves model
performance, generalization to out-of-distribution settings, and fairness
across demographic subgroups. Across datasets, synthetic pretraining led to a
6.5% accuracy increase in the performance of downstream classification models,
compared to a modest 2.7% increase when naively combining real and synthetic
data. We observe this performance improvement simultaneously with the reduction
of the underdiagnosis fairness gap by 19.3%. These results highlight the
potential of synthetic imaging to advance equitable and generalizable medical
deep learning under real-world data constraints. We open source our code,
trained models, and synthetic dataset at
https://github.com/StanfordMIMI/RoentGen-v2 .

</details>


### [43] [Towards Open-Vocabulary Multimodal 3D Object Detection with Attributes](https://arxiv.org/abs/2508.16812)
*Xinhao Xiang,Kuan-Chuan Peng,Suhas Lohit,Michael J. Jones,Jiawei Zhang*

Main category: cs.CV

> OVODA框架提高了3D物体检测的开放词汇能力，无需知道新类锚点大小，同时提高了属性识别的准确率。OVAD数据集提供了丰富的属性注释。

<details>
  <summary>Details</summary>

**Motivation:** 现有的3D物体检测方法受限于封闭集假设，难以识别新物体及其属性。为了克服这个难点，提出OVODA框架，实现在不知道新类锚点大小的情况下进行开放词汇3D物体检测和属性检测。

**Method:** OVODA采用基础模型连接3D特征和文本的语义鸿沟，能同时检测属性信息如空间关系和运动状态等。其主要创新包括基础模型特征连接，提示调优策略，以及用于属性检测的特殊技术，如视角指定提示和水平翻转数据增强。

**Result:** 在nuScenes和Argoverse 2数据集上，OVODA在开放词汇3D物体检测方面表现出色，同时还能准确识别物体属性，在没有给定新类锚点大小的情况下超越了现有方法。

**Conclusion:** 研究提出了OVODA框架和OVAD数据集，填补了现有研究空白，提升了无需知道新类锚点大小情况下开放词汇3D物体检测和属性检测的能力。

**Abstract:** 3D object detection plays a crucial role in autonomous systems, yet existing
methods are limited by closed-set assumptions and struggle to recognize novel
objects and their attributes in real-world scenarios. We propose OVODA, a novel
framework enabling both open-vocabulary 3D object and attribute detection with
no need to know the novel class anchor size. OVODA uses foundation models to
bridge the semantic gap between 3D features and texts while jointly detecting
attributes, e.g., spatial relationships, motion states, etc. To facilitate such
research direction, we propose OVAD, a new dataset that supplements existing 3D
object detection benchmarks with comprehensive attribute annotations. OVODA
incorporates several key innovations, including foundation model feature
concatenation, prompt tuning strategies, and specialized techniques for
attribute detection, including perspective-specified prompts and horizontal
flip augmentation. Our results on both the nuScenes and Argoverse 2 datasets
show that under the condition of no given anchor sizes of novel classes, OVODA
outperforms the state-of-the-art methods in open-vocabulary 3D object detection
while successfully recognizing object attributes. Our OVAD dataset is released
here: https://doi.org/10.5281/zenodo.16904069 .

</details>


### [44] [AIM 2025 Low-light RAW Video Denoising Challenge: Dataset, Methods and Results](https://arxiv.org/abs/2508.16830)
*Alexander Yakovenko,George Chakvetadze,Ilya Khrapov,Maksim Zhelezov,Dmitry Vatolin,Radu Timofte,Youngjin Oh,Junhyeong Kwon,Junyoung Park,Nam Ik Cho,Senyan Xu,Ruixuan Jiang,Long Peng,Xueyang Fu,Zheng-Jun Zha,Xiaoping Peng,Hansen Feng,Zhanyi Tie,Ziming Xia,Lizhi Wang*

Main category: cs.CV

> The paper reviews the AIM 2025 Low-Light RAW Video Denoising Challenge, which focuses on developing methods to denoise low-light RAW videos within exposure time limits. It introduces a benchmark dataset and describes the challenge protocol and participant submissions.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to advance techniques for video denoising in low-light conditions, especially in the context of smartphone cameras, by leveraging temporal redundancy while adhering to real-world constraints such as frame rate and exposure time limits.

**Method:** Structure

**Result:** The challenge evaluates submissions using full-reference PSNR and SSIM metrics on a private test set, which includes a diverse set of videos captured under various illumination and exposure conditions.

**Conclusion:** The AIM 2025 challenge provides a platform for evaluating and improving video denoising techniques specifically for low-light conditions, which is crucial for enhancing the image quality of RAW videos captured by smartphone cameras.

**Abstract:** This paper reviews the AIM 2025 (Advances in Image Manipulation) Low-Light
RAW Video Denoising Challenge. The task is to develop methods that denoise
low-light RAW video by exploiting temporal redundancy while operating under
exposure-time limits imposed by frame rate and adapting to sensor-specific,
signal-dependent noise. We introduce a new benchmark of 756 ten-frame sequences
captured with 14 smartphone camera sensors across nine conditions
(illumination: 1/5/10 lx; exposure: 1/24, 1/60, 1/120 s), with high-SNR
references obtained via burst averaging. Participants process linear RAW
sequences and output the denoised 10th frame while preserving the Bayer
pattern. Submissions are evaluated on a private test set using full-reference
PSNR and SSIM, with final ranking given by the mean of per-metric ranks. This
report describes the dataset, challenge protocol, and submitted approaches.

</details>


### [45] [Transformer-Based Neural Network for Transient Detection without Image Subtraction](https://arxiv.org/abs/2508.16844)
*Adi Inada,Masao Sako,Tatiana Acero-Cuellar,Federica Bianco*

Main category: cs.CV

> A transformer-based neural network was developed for supernova detection in astronomical images, achieving high accuracy without the need for computationally expensive difference imaging.

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy and efficiency of supernova detection in large-scale astronomical surveys, advancing beyond conventional CNN methods.

**Method:** We introduce a transformer-based neural network for classifying real and bogus transient detections in astronomical images, avoiding the need for computationally expensive difference imaging.

**Result:** The network achieved a classification accuracy of 97.4% and was able to perform well even when the input images were not centered on the supernova candidate.

**Conclusion:** The findings indicate the effectiveness of the transformer-based network in both accuracy and efficiency, making it a promising method for supernova detection in large-scale surveys.

**Abstract:** We introduce a transformer-based neural network for the accurate
classification of real and bogus transient detections in astronomical images.
This network advances beyond the conventional convolutional neural network
(CNN) methods, widely used in image processing tasks, by adopting an
architecture better suited for detailed pixel-by-pixel comparison. The
architecture enables efficient analysis of search and template images only,
thus removing the necessity for computationally-expensive difference imaging,
while maintaining high performance. Our primary evaluation was conducted using
the autoScan dataset from the Dark Energy Survey (DES), where the network
achieved a classification accuracy of 97.4% and diminishing performance utility
for difference image as the size of the training set grew. Further experiments
with DES data confirmed that the network can operate at a similar level even
when the input images are not centered on the supernova candidate. These
findings highlight the network's effectiveness in enhancing both accuracy and
efficiency of supernova detection in large-scale astronomical surveys.

</details>


### [46] [NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows](https://arxiv.org/abs/2508.16845)
*Denis Tarasov,Alexander Nikulin,Ilya Zisman,Albina Klepach,Nikita Lyubaykin,Andrei Polubarov,Alexander Derevyagin,Vladislav Kurenkov*

Main category: cs.CV

> 研究提出NinA框架，利用归一化流取代扩散模型解析器以提高VLA模型在高频控制环境中的实用性。

<details>
  <summary>Details</summary>

**Motivation:** 扩散模型虽然能很好地建模复杂的多模态动作分布，但其需要多次迭代去噪步骤来进行推理，限制了其在高频控制需求的现实场景中的实用性。

**Method:** NinA (Normalizing Flows in Action)替换VLA架构中的扩散模型解码器，利用归一化流（NF）实现一次性采样，显著降低推理时间。

**Result:** 实验表明，在相同的训练设置下，NinA的表现与扩散模型相当，同时在推理速度上显著提升。

**Conclusion:** NinA提供了一条通往高效、高频VLA控制的前景路径，且无需牺牲性能。

**Abstract:** Recent advances in Vision-Language-Action (VLA) models have established a
two-component architecture, where a pre-trained Vision-Language Model (VLM)
encodes visual observations and task descriptions, and an action decoder maps
these representations to continuous actions. Diffusion models have been widely
adopted as action decoders due to their ability to model complex, multimodal
action distributions. However, they require multiple iterative denoising steps
at inference time or downstream techniques to speed up sampling, limiting their
practicality in real-world settings where high-frequency control is crucial. In
this work, we present NinA (Normalizing Flows in Action), a fast and expressive
alter- native to diffusion-based decoders for VLAs. NinA replaces the diffusion
action decoder with a Normalizing Flow (NF) that enables one-shot sampling
through an invertible transformation, significantly reducing inference time. We
integrate NinA into the FLOWER VLA architecture and fine-tune on the LIBERO
benchmark. Our experiments show that NinA matches the performance of its
diffusion-based counterpart under the same training regime, while achieving
substantially faster inference. These results suggest that NinA offers a
promising path toward efficient, high-frequency VLA control without
compromising performance.

</details>


### [47] [RF-PGS: Fully-structured Spatial Wireless Channel Representation with Planar Gaussian Splatting](https://arxiv.org/abs/2508.16849)
*Lihao Zhang,Zongtan Li,Haijian Sun*

Main category: cs.CV

> 本文提出了一种名为RF-PGS的新框架，该框架利用射频特优化的平面高斯几何原语，从稀疏的路径损耗谱重建无线电传播路径，提高了重建精度和效率，解决了6G时代信道建模的挑战。

<details>
  <summary>Details</summary>

**Motivation:** 在6G时代，需要更高的系统吞吐量和新兴的6G技术的实施，这要求大型天线阵列和精确的空间信道状态信息（空间CSI）。传统的方法在空间分辨率、效率和扩展性方面面临挑战。因此，本文提出了RF-PGS框架作为解决方案。

**Method:** 本文提出了一种名为RF-PGS的新框架，该框架可以从稀疏的路径损耗谱中重建高保真的无线电传播路径。通过引入带有射频特优化的平面高斯几何原语，在第一阶段实现了密集且与表面对齐的场景重建。在接下来的射频训练阶段，结合定制的多视角损失，精确地建模了无线电传播行为。

**Result:** 与之前的辐射场方法相比，RF-PGS显著提高了重建精度，减少了训练成本，并能够有效地表示无线信道。

**Conclusion:** 本文的方法为规模化的6G空间信道状态信息建模提供了一种实用的解决方案。

**Abstract:** In the 6G era, the demand for higher system throughput and the implementation
of emerging 6G technologies require large-scale antenna arrays and accurate
spatial channel state information (Spatial-CSI). Traditional channel modeling
approaches, such as empirical models, ray tracing, and measurement-based
methods, face challenges in spatial resolution, efficiency, and scalability.
Radiance field-based methods have emerged as promising alternatives but still
suffer from geometric inaccuracy and costly supervision. This paper proposes
RF-PGS, a novel framework that reconstructs high-fidelity radio propagation
paths from only sparse path loss spectra. By introducing Planar Gaussians as
geometry primitives with certain RF-specific optimizations, RF-PGS achieves
dense, surface-aligned scene reconstruction in the first geometry training
stage. In the subsequent Radio Frequency (RF) training stage, the proposed
fully-structured radio radiance, combined with a tailored multi-view loss,
accurately models radio propagation behavior. Compared to prior radiance field
methods, RF-PGS significantly improves reconstruction accuracy, reduces
training costs, and enables efficient representation of wireless channels,
offering a practical solution for scalable 6G Spatial-CSI modeling.

</details>


### [48] [Gaussian Primitive Optimized Deformable Retinal Image Registration](https://arxiv.org/abs/2508.16852)
*Xin Tian,Jiazheng Wang,Yuxi Zhang,Xiang Chen,Renjiu Hu,Gaolei Li,Min Liu,Hang Zhang*

Main category: cs.CV

> A new structured message passing framework for retinal image registration using Gaussian primitives and KNN interpolation, significantly improving accuracy on the FIRE dataset.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of registering retinal images due to limited gradient signals in standard learning frameworks caused by the presence of large homogeneous regions and critical vascular features.

**Method:** Gaussian Primitive Optimization (GPO), an iterative framework that performs structured message passing after initial coarse alignment, extracting keypoints at key anatomical structures to serve as control nodes (DCN). Each node is a Gaussian primitive with trainable parameters, and KNN interpolation blends and propagates displacement signals to form a coherent field, reducing computational cost while focusing on local detail.

**Result:** On the FIRE dataset, GPO results in a reduced target registration error from 6.2 px to ~2.4 px and an increase in the AUC at 25 px from 0.770 to 0.938, surpassing existing methods.

**Conclusion:** The proposed GPO framework overcomes the gradient signal limitations in retinal image registration by utilizing structured message passing and KNN Gaussian interpolation, demonstrating superior performance on a benchmark dataset.

**Abstract:** Deformable retinal image registration is notoriously difficult due to large
homogeneous regions and sparse but critical vascular features, which cause
limited gradient signals in standard learning-based frameworks. In this paper,
we introduce Gaussian Primitive Optimization (GPO), a novel iterative framework
that performs structured message passing to overcome these challenges. After an
initial coarse alignment, we extract keypoints at salient anatomical structures
(e.g., major vessels) to serve as a minimal set of descriptor-based control
nodes (DCN). Each node is modelled as a Gaussian primitive with trainable
position, displacement, and radius, thus adapting its spatial influence to
local deformation scales. A K-Nearest Neighbors (KNN) Gaussian interpolation
then blends and propagates displacement signals from these information-rich
nodes to construct a globally coherent displacement field; focusing
interpolation on the top (K) neighbors reduces computational overhead while
preserving local detail. By strategically anchoring nodes in high-gradient
regions, GPO ensures robust gradient flow, mitigating vanishing gradient signal
in textureless areas. The framework is optimized end-to-end via a multi-term
loss that enforces both keypoint consistency and intensity alignment.
Experiments on the FIRE dataset show that GPO reduces the target registration
error from 6.2\,px to ~2.4\,px and increases the AUC at 25\,px from 0.770 to
0.938, substantially outperforming existing methods. The source code can be
accessed via https://github.com/xintian-99/GPOreg.

</details>


### [49] [Beyond Emotion Recognition: A Multi-Turn Multimodal Emotion Understanding and Reasoning Benchmark](https://arxiv.org/abs/2508.16859)
*Jinpeng Hu,Hongchang Shi,Chongyuan Dai,Zhuo Li,Peipei Song,Meng Wang*

Main category: cs.CV

> 本研究提出了一个多回合的多模态情感理解和推理基准测试和一种多代理框架，实验表明现有MLLM在推理任务上的挑战。

<details>
  <summary>Details</summary>

**Motivation:** 由于多模态大语言模型在心理学领域中具有理解人类情感和行为的潜力，但目前的研究主要集中在增强其情感识别能力上，情感推理方面的潜力尚未得到充分利用。因此，本研究旨在开发一种用于情感推理的基准测试，并改进模型在这一领域的表现。

**Method:** 本研究提出了一个多回合的多模态情感理解和推理（MTMEUR）基准测试，包含1,451个来自现实生活场景的视频数据和5,101个渐进式问题。这些问题涵盖了情感识别、情感潜在原因以及未来行为预测等多个方面。此外，我们提出了一种多代理框架，每个代理专门负责一个特定方面，如背景情境、角色动态和事件细节，以提高系统的推理能力。

**Result:** 我们的实验表明，现有的多模态大语言模型在情感推理任务上面临显著的挑战。

**Conclusion:** 实验证明，现有MLLM在情感推理任务上面临显著挑战，这表明需要进一步发展以提高其推理能力。

**Abstract:** Multimodal large language models (MLLMs) have been widely applied across
various fields due to their powerful perceptual and reasoning capabilities. In
the realm of psychology, these models hold promise for a deeper understanding
of human emotions and behaviors. However, recent research primarily focuses on
enhancing their emotion recognition abilities, leaving the substantial
potential in emotion reasoning, which is crucial for improving the naturalness
and effectiveness of human-machine interactions. Therefore, in this paper, we
introduce a multi-turn multimodal emotion understanding and reasoning (MTMEUR)
benchmark, which encompasses 1,451 video data from real-life scenarios, along
with 5,101 progressive questions. These questions cover various aspects,
including emotion recognition, potential causes of emotions, future action
prediction, etc. Besides, we propose a multi-agent framework, where each agent
specializes in a specific aspect, such as background context, character
dynamics, and event details, to improve the system's reasoning capabilities.
Furthermore, we conduct experiments with existing MLLMs and our agent-based
method on the proposed benchmark, revealing that most models face significant
challenges with this task.

</details>


### [50] [Delta-SVD: Efficient Compression for Personalized Text-to-Image Models](https://arxiv.org/abs/2508.16863)
*Tangyuan Zhang,Shangyu Chen,Qixiang Chen,Jianfei Cai*

Main category: cs.CV

> 本文提出Delta-SVD，一种无需重新训练的压缩方法，专注于通过SVD和秩截断策略来压缩个性化模型的权重增量，适用于大规模个性化扩散模型的部署。

<details>
  <summary>Details</summary>

**Motivation:** 现有的个性化文本到图像模型如DreamBooth需要对大型扩散模型进行微调，导致需要大量存储空间来维护许多特定于主题的模型。本文旨在提出一个简单、高效且保持原模型架构的解决方案。

**Method:** 该论文提出了一种称为Delta-SVD的压缩方法，它首先应用奇异值分解(SVD)分解权重增量，然后使用基于能量的秩截断策略来平衡压缩效率和重建保真度。

**Result:** 实验表明，Delta-SVD在多个主题数据集上实现了显著的压缩，同时生成质量（通过CLIP分数、SSIM和FID衡量）几乎没有下降。

**Conclusion:** 该方法能够有效地部署个性化扩散模型，适用于需要存储和部署大规模主题定制的现实世界应用。

**Abstract:** Personalized text-to-image models such as DreamBooth require fine-tuning
large-scale diffusion backbones, resulting in significant storage overhead when
maintaining many subject-specific models. We present Delta-SVD, a post-hoc,
training-free compression method that targets the parameter weights update
induced by DreamBooth fine-tuning. Our key observation is that these delta
weights exhibit strong low-rank structure due to the sparse and localized
nature of personalization. Delta-SVD first applies Singular Value Decomposition
(SVD) to factorize the weight deltas, followed by an energy-based rank
truncation strategy to balance compression efficiency and reconstruction
fidelity. The resulting compressed models are fully plug-and-play and can be
re-constructed on-the-fly during inference. Notably, the proposed approach is
simple, efficient, and preserves the original model architecture. Experiments
on a multiple subject dataset demonstrate that Delta-SVD achieves substantial
compression with negligible loss in generation quality measured by CLIP score,
SSIM and FID. Our method enables scalable and efficient deployment of
personalized diffusion models, making it a practical solution for real-world
applications that require storing and deploying large-scale subject
customizations.

</details>


### [51] [Do Multimodal LLMs See Sentiment?](https://arxiv.org/abs/2508.16873)
*Neemias B. da Silva,John Harrison,Rodrigo Minetto,Myriam R. Delgado,Bogdan T. Nassu,Thiago H. Silva*

Main category: cs.CV

> The paper introduces MLLMsent, a framework for analyzing sentiment reasoning in MLLMs, achieving top performance and setting new benchmarks in sentiment analysis on visual content.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to understand how visual content conveys sentiment due to the growing influence of visual media on social platforms, addressing a challenge tied to complex, scene-level semantics in sentiment perception.

**Method:** The paper proposes a framework called MLLMsent to examine the sentiment reasoning skills of Multimodal Large Language Models (MLLMs) via three methods: sentiment classification from images, sentiment analysis on image descriptions via pre-trained LLMs, and fine-tuning on sentiment-labeled image descriptions.

**Result:** Experiments on a well-known benchmark show that MLLMsent, especially the fine-tuned method, performs better than Lexicon-, CNN-, and Transformer-based models by up to 30.9%, 64.8%, and 42.4%. Even without training on new datasets, it outperforms its competitors by up to 8.26%.

**Conclusion:** The research underscores the potential of the proposed visual reasoning strategy in enhancing affective computing and establishes new benchmarks for further study.

**Abstract:** Understanding how visual content communicates sentiment is critical in an era
where online interaction is increasingly dominated by this kind of media on
social platforms. However, this remains a challenging problem, as sentiment
perception is closely tied to complex, scene-level semantics. In this paper, we
propose an original framework, MLLMsent, to investigate the sentiment reasoning
capabilities of Multimodal Large Language Models (MLLMs) through three
perspectives: (1) using those MLLMs for direct sentiment classification from
images; (2) associating them with pre-trained LLMs for sentiment analysis on
automatically generated image descriptions; and (3) fine-tuning the LLMs on
sentiment-labeled image descriptions. Experiments on a recent and established
benchmark demonstrate that our proposal, particularly the fine-tuned approach,
achieves state-of-the-art results outperforming Lexicon-, CNN-, and
Transformer-based baselines by up to 30.9%, 64.8%, and 42.4%, respectively,
across different levels of evaluators' agreement and sentiment polarity
categories. Remarkably, in a cross-dataset test, without any training on these
new data, our model still outperforms, by up to 8.26%, the best runner-up,
which has been trained directly on them. These results highlight the potential
of the proposed visual reasoning scheme for advancing affective computing,
while also establishing new benchmarks for future research.

</details>


### [52] [AWM-Fuse: Multi-Modality Image Fusion for Adverse Weather via Global and Local Text Perception](https://arxiv.org/abs/2508.16881)
*Xilai Li,Huichun Liu,Xiaosong Li,Tao Ye,Zhenyu Kuang,Huafeng Li*

Main category: cs.CV

> AWM-Fuse improves multi-modality image fusion in adverse weather by integrating global and local textual perceptions.

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of effective categorization and thorough analysis of textual content in previous MMIF studies, aiming to improve semantic perception in adverse weather conditions.

**Method:** Multi-modality image fusion (MMIF) in adverse weather conditions through AWM-Fuse, incorporating textual information produced by BLIP and ChatGPT for global and local perception modules respectively.

**Result:** AWM-Fuse outperforms current state-of-the-art methods in complex weather conditions and downstream tasks.

**Conclusion:** Incorporating textual information through a unified, shared weight architecture improves the generalization and detail capture in multi-modality image fusion under adverse weather conditions.

**Abstract:** Multi-modality image fusion (MMIF) in adverse weather aims to address the
loss of visual information caused by weather-related degradations, providing
clearer scene representations. Although less studies have attempted to
incorporate textual information to improve semantic perception, they often lack
effective categorization and thorough analysis of textual content. In response,
we propose AWM-Fuse, a novel fusion method for adverse weather conditions,
designed to handle multiple degradations through global and local text
perception within a unified, shared weight architecture. In particular, a
global feature perception module leverages BLIP-produced captions to extract
overall scene features and identify primary degradation types, thus promoting
generalization across various adverse weather conditions. Complementing this,
the local module employs detailed scene descriptions produced by ChatGPT to
concentrate on specific degradation effects through concrete textual cues,
thereby capturing finer details. Furthermore, textual descriptions are used to
constrain the generation of fusion images, effectively steering the network
learning process toward better alignment with real semantic labels, thereby
promoting the learning of more meaningful visual features. Extensive
experiments demonstrate that AWM-Fuse outperforms current state-of-the-art
methods in complex weather conditions and downstream tasks. Our code is
available at https://github.com/Feecuin/AWM-Fuse.

</details>


### [53] [A Lightweight Convolution and Vision Transformer integrated model with Multi-scale Self-attention Mechanism](https://arxiv.org/abs/2508.16884)
*Yi Zhang,Lingxiao Wei,Bowei Zhang,Ziwei Liu,Kai Yi,Shu Hu*

Main category: cs.CV

> SAEViT是基于ViT轻量级模型，采用了卷积块来平衡计算效率和性能，通过引入稀疏聚合注意力模块和通道交互式前馈网络层等方法，实现了相比于传统方法更为高效和轻量级的视觉任务处理方案。

<details>
  <summary>Details</summary>

**Motivation:** 由于ViT（视觉变压器）在计算机视觉任务中表现出较强的能力，但其模型尺寸大且计算成本高，局部特征建模能力弱，阻碍了其在实际场景中的应用。为了平衡计算效率和性能，提出了SAEViT。

**Method:** SAEViT（Sparse-Attention-Efficient-ViT）方法引入了一种稀疏聚合注意力（SAA）模块，该模块基于图像冗余进行自适应稀疏采样，并通过反卷积操作恢复特征图，从而显著降低了注意力操作的计算复杂度。此外，还设计了一种通道交互式前馈网络（CIFFN）层，通过特征分解和重分配增强跨通道信息交换，缓解传统前馈网络中的冗余。最后，提出了一个嵌入深度可分离卷积块（DWSConv）的层次金字塔结构，进一步加强卷积特征。

**Result:** 在主流数据集上进行了广泛的实验，证明了SAEViT在ImageNet-1K分类任务中，分别在0.8GFLOPs和1.3GFLOPs计算下达到了76.3%和79.6%的Top-1准确率。

**Conclusion:** 该研究提出了一种轻量级的解决方案，即SAEViT，它在平衡了计算效率和性能后，可应用于各种基本视觉任务。这些成就展现了其作为高效且轻量级视觉任务处理方案的价值。

**Abstract:** Vision Transformer (ViT) has prevailed in computer vision tasks due to its
strong long-range dependency modelling ability. However, its large model size
with high computational cost and weak local feature modeling ability hinder its
application in real scenarios. To balance computation efficiency and
performance, we propose SAEViT (Sparse-Attention-Efficient-ViT), a lightweight
ViT based model with convolution blocks, in this paper to achieve efficient
downstream vision tasks. Specifically, SAEViT introduces a Sparsely Aggregated
Attention (SAA) module that performs adaptive sparse sampling based on image
redundancy and recovers the feature map via deconvolution operation, which
significantly reduces the computational complexity of attention operations. In
addition, a Channel-Interactive Feed-Forward Network (CIFFN) layer is developed
to enhance inter-channel information exchange through feature decomposition and
redistribution, mitigating redundancy in traditional feed-forward networks
(FNN). Finally, a hierarchical pyramid structure with embedded depth-wise
separable convolutional blocks (DWSConv) is devised to further strengthen
convolutional features. Extensive experiments on mainstream datasets show that
SAEViT achieves Top-1 accuracies of 76.3\% and 79.6\% on the ImageNet-1K
classification task with only 0.8 GFLOPs and 1.3 GFLOPs, respectively,
demonstrating a lightweight solution for various fundamental vision tasks.

</details>


### [54] [MDIQA: Unified Image Quality Assessment for Multi-dimensional Evaluation and Restoration](https://arxiv.org/abs/2508.16887)
*Shunyu Yao,Ming Liu,Zhilu Zhang,Zhaolin Wan,Zhilong Ji,Jinfeng Bai,Wangmeng Zuo*

Main category: cs.CV

> 本文介绍了MDIQA框架，通过多个感知维度来更精确地模拟人类的图像质量评价，从而改进了当前的图像质量评估方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有大多数方法过分关注拟合总体评分，忽略了人类通常是基于多维度评价图像质量的事实。为了解决这个问题，提出了MDIQA框架。

**Method:** 我们提出了一个多层次图像质量评估（MDIQA）框架，该框架在不同的感知维度下训练不同的分支，包括技术维度和美学维度，以捕捉人类视觉感知的多方面特性。各分支的特征最终被整合来生成总体的IQA评分。

**Result:** 实验表明，我们的MDIQA达到了优越的性能，并且在图像修复任务中可以灵活应用。

**Conclusion:** MDIQA框架通过对不同感知维度的分析，能够更准确地生成图像质量评分，并且能够用于图像修复模型的训练优化。

**Abstract:** Recent advancements in image quality assessment (IQA), driven by
sophisticated deep neural network designs, have significantly improved the
ability to approach human perceptions. However, most existing methods are
obsessed with fitting the overall score, neglecting the fact that humans
typically evaluate image quality from different dimensions before arriving at
an overall quality assessment. To overcome this problem, we propose a
multi-dimensional image quality assessment (MDIQA) framework. Specifically, we
model image quality across various perceptual dimensions, including five
technical and four aesthetic dimensions, to capture the multifaceted nature of
human visual perception within distinct branches. Each branch of our MDIQA is
initially trained under the guidance of a separate dimension, and the
respective features are then amalgamated to generate the final IQA score.
Additionally, when the MDIQA model is ready, we can deploy it for a flexible
training of image restoration (IR) models, enabling the restoration results to
better align with varying user preferences through the adjustment of perceptual
dimension weights. Extensive experiments demonstrate that our MDIQA achieves
superior performance and can be effectively and flexibly applied to image
restoration tasks. The code is available: https://github.com/YaoShunyu19/MDIQA.

</details>


### [55] [Structural Energy-Guided Sampling for View-Consistent Text-to-3D](https://arxiv.org/abs/2508.16917)
*Qing Zhang,Jinguang Tong,Jie Hong,Jing Zhang,Xuesong Li*

Main category: cs.CV

> Proposes SEGS, a method to enforce multi-view consistency in text-to-3D generation without retraining, resolving the 'Janus problem'.

<details>
  <summary>Details</summary>

**Motivation:** To solve the 'Janus problem' in text-to-3D generation, where objects look correct from the front but appear duplicated or distorted from other angles due to viewpoint bias in 2D diffusion priors.

**Method:** Structural Energy-Guided Sampling (SEGS), a training-free framework that enforces multi-view consistency through the injection of gradients of a structural energy defined in a PCA subspace of intermediate U-Net features into the denoising trajectory.

**Result:** Achieves improved geometric alignment and viewpoint consistency in text-to-3D generation, reducing Janus artifacts.

**Conclusion:** SEGS effectively enhances the quality of generated 3D objects across different viewpoints without the need for retraining or modification of the model weights.

**Abstract:** Text-to-3D generation often suffers from the Janus problem, where objects
look correct from the front but collapse into duplicated or distorted geometry
from other angles. We attribute this failure to viewpoint bias in 2D diffusion
priors, which propagates into 3D optimization. To address this, we propose
Structural Energy-Guided Sampling (SEGS), a training-free, plug-and-play
framework that enforces multi-view consistency entirely at sampling time. SEGS
defines a structural energy in a PCA subspace of intermediate U-Net features
and injects its gradients into the denoising trajectory, steering geometry
toward the intended viewpoint while preserving appearance fidelity. Integrated
seamlessly into SDS/VSD pipelines, SEGS significantly reduces Janus artifacts,
achieving improved geometric alignment and viewpoint consistency without
retraining or weight modification.

</details>


### [56] [MSPCaps: A Multi-Scale Patchify Capsule Network with Cross-Agreement Routing for Visual Recognition](https://arxiv.org/abs/2508.16922)
*Yudong Hu,Yueju Han,Rui Sun,Jinke Ren*

Main category: cs.CV

> 本文提出了一种新的胶囊网络模型MSPCaps，该模型通过多尺度特征学习和高效的胶囊路由机制，提高了视觉识别领域的分类性能和鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 为了解决现有胶囊网络及其变体在利用多尺度特征时的问题，特别是多尺度特征融合策略在处理不同尺度特征时表现出的不足，从而影响分类性能。

**Method:** 本文提出了一种名为MSPCaps的新架构，集成了多尺度特征学习和高效的胶囊路由机制。MSPCaps包括三个主要部分：多尺度ResNet主干（MSRB）、Patchify胶囊层（PatchifyCaps）和交叉一致路由（CAR）模块。MSRB负责提取输入图像的多尺度特征表示；PatchifyCaps将这些多尺度特征划分为使用统一块大小的主要胶囊，从而能够学习不同的感受野；CAR模块通过识别具有最大一致性的跨尺度预测对来进行自适应路由。

**Result:** 本文提出了多尺度Patchify胶囊网络（MSPCaps），以克服现有胶囊网络在多尺度特征利用上的不足。该模型由多尺度ResNet主干（MSRB）、Patchify胶囊层（PatchifyCaps）和交叉一致路由（CAR）模块组成，通过综合利用多尺度特征和有效的胶囊路由机制，提高了分类性能和模型鲁棒性。实验结果表明，MSPCaps在不同配置下均优于多种基础方法。

**Conclusion:** MSPCaps模型实现了显著的可扩展性和优越的鲁棒性，在分类准确性方面持续超越多种基准方法。这一结果突显了该模型在推进特征表示学习方面的潜力。

**Abstract:** Capsule Network (CapsNet) has demonstrated significant potential in visual
recognition by capturing spatial relationships and part-whole hierarchies for
learning equivariant feature representations. However, existing CapsNet and
variants often rely on a single high-level feature map, overlooking the rich
complementary information from multi-scale features. Furthermore, conventional
feature fusion strategies (e.g., addition and concatenation) struggle to
reconcile multi-scale feature discrepancies, leading to suboptimal
classification performance. To address these limitations, we propose the
Multi-Scale Patchify Capsule Network (MSPCaps), a novel architecture that
integrates multi-scale feature learning and efficient capsule routing.
Specifically, MSPCaps consists of three key components: a Multi-Scale ResNet
Backbone (MSRB), a Patchify Capsule Layer (PatchifyCaps), and Cross-Agreement
Routing (CAR) blocks. First, the MSRB extracts diverse multi-scale feature
representations from input images, preserving both fine-grained details and
global contextual information. Second, the PatchifyCaps partitions these
multi-scale features into primary capsules using a uniform patch size,
equipping the model with the ability to learn from diverse receptive fields.
Finally, the CAR block adaptively routes the multi-scale capsules by
identifying cross-scale prediction pairs with maximum agreement. Unlike the
simple concatenation of multiple self-routing blocks, CAR ensures that only the
most coherent capsules contribute to the final voting. Our proposed MSPCaps
achieves remarkable scalability and superior robustness, consistently
surpassing multiple baseline methods in terms of classification accuracy, with
configurations ranging from a highly efficient Tiny model (344.3K parameters)
to a powerful Large model (10.9M parameters), highlighting its potential in
advancing feature representation learning.

</details>


### [57] [LGE-Guided Cross-Modality Contrastive Learning for Gadolinium-Free Cardiomyopathy Screening in Cine CMR](https://arxiv.org/abs/2508.16927)
*Siqing Yuan,Yulin Wang,Zirui Cao,Yueyan Wang,Zehao Weng,Hui Wang,Lei Xu,Zixian Chen,Lei Chen,Zhong Xue,Dinggang Shen*

Main category: cs.CV

> 提出CC-CMR模型，基于对比学习和跨模态对齐技术，通过将纤维化特征编码进动态CMR图像，不需要钆对比剂，提高了心肌病早期筛查的准确性，展示了临床应用的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 心肌病是导致心力衰竭和心脏猝死的主要因素，需要进行精准的早期筛查。虽然心脏磁共振成像（CMR）被认为是诊断的“金标准”，但它依赖于钆对比剂并且解读过程劳动力密集，限制了其在大规模人群中的部署。因此，需要开发一种不依赖钆的心肌病筛查方法。

**Method:** 我们提出了一种名为CC-CMR的框架，该框架利用对比学习和跨模态对齐技术，基于无钆增强的心脏磁共振成像序列进行心肌病筛查。通过将动态CMR和LGE序列的潜在空间对齐，模型能够将纤维化特异性病理信息编码到动态CMR嵌入中。通过特征交互模块优化诊断精度和跨模态特征的一致性，并通过基于不确定性的自适应训练机制动态校准任务特定的目标，以确保模型的泛化能力。

**Result:** 在来自231名受试者的多中心数据上进行评估，CC-CMR模型的准确性为0.943（95%置信区间：0.886-0.986），相比传统的仅基于动态CMR的模型提高了4.3%。

**Conclusion:** CC-CMR模型展示了其临床应用的潜力，能够为广泛的群体和不同医疗环境提供不依赖钆的心肌病筛查方案。

**Abstract:** Cardiomyopathy, a principal contributor to heart failure and sudden cardiac
mortality, demands precise early screening. Cardiac Magnetic Resonance (CMR),
recognized as the diagnostic 'gold standard' through multiparametric protocols,
holds the potential to serve as an accurate screening tool. However, its
reliance on gadolinium contrast and labor-intensive interpretation hinders
population-scale deployment. We propose CC-CMR, a Contrastive Learning and
Cross-Modal alignment framework for gadolinium-free cardiomyopathy screening
using cine CMR sequences. By aligning the latent spaces of cine CMR and Late
Gadolinium Enhancement (LGE) sequences, our model encodes fibrosis-specific
pathology into cine CMR embeddings. A Feature Interaction Module concurrently
optimizes diagnostic precision and cross-modal feature congruence, augmented by
an uncertainty-guided adaptive training mechanism that dynamically calibrates
task-specific objectives to ensure model generalizability. Evaluated on
multi-center data from 231 subjects, CC-CMR achieves accuracy of 0.943 (95% CI:
0.886-0.986), outperforming state-of-the-art cine-CMR-only models by 4.3% while
eliminating gadolinium dependency, demonstrating its clinical viability for
wide range of populations and healthcare environments.

</details>


### [58] [Align 3D Representation and Text Embedding for 3D Content Personalization](https://arxiv.org/abs/2508.16932)
*Qi Song,Ziyuan Luo,Ka Chun Cheung,Simon See,Renjie Wan*

Main category: cs.CV

> The paper proposes Invert3D, a new framework for easy 3D personalization by aligning 3D representations with text embeddings, enabling manipulation through natural language prompts without the need for retraining procedures.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the challenge of efficiently personalizing 3D content, an issue with predominantly computationally expensive knowledge distillation-based methods.

**Method:** The method involves developing a camera-conditioned 3D-to-text inverse mechanism that enables the projection of 3D contents into a 3D embedding space that is aligned with text embeddings, thus facilitating 3D content personalization.

**Result:** The experimental results show that Invert3D can effectively personalize 3D content, demonstrating the feasibility of the proposed approach.

**Conclusion:** The conclusion is that Invert3D offers a novel and effective way to personalize 3D content via natural language prompts, bridging the gap between 3D content and text embeddings.

**Abstract:** Recent advances in NeRF and 3DGS have significantly enhanced the efficiency
and quality of 3D content synthesis. However, efficient personalization of
generated 3D content remains a critical challenge. Current 3D personalization
approaches predominantly rely on knowledge distillation-based methods, which
require computationally expensive retraining procedures. To address this
challenge, we propose \textbf{Invert3D}, a novel framework for convenient 3D
content personalization. Nowadays, vision-language models such as CLIP enable
direct image personalization through aligned vision-text embedding spaces.
However, the inherent structural differences between 3D content and 2D images
preclude direct application of these techniques to 3D personalization. Our
approach bridges this gap by establishing alignment between 3D representations
and text embedding spaces. Specifically, we develop a camera-conditioned
3D-to-text inverse mechanism that projects 3D contents into a 3D embedding
aligned with text embeddings. This alignment enables efficient manipulation and
personalization of 3D content through natural language prompts, eliminating the
need for computationally retraining procedures. Extensive experiments
demonstrate that Invert3D achieves effective personalization of 3D content. Our
work is available at: https://github.com/qsong2001/Invert3D.

</details>


### [59] [Addressing Annotation Scarcity in Hyperspectral Brain Image Segmentation with Unsupervised Domain Adaptation](https://arxiv.org/abs/2508.16934)
*Tim Mach,Daniel Rueckert,Alex Berger,Laurin Lux,Ivan Ezhov*

Main category: cs.CV

> A novel unsupervised domain adaptation method is developed for segmenting cerebral vasculature in hyperspectral brain images, vastly improving upon current state-of-the-art techniques.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the critical challenge posed by severe label scarcity in conventional supervised training, particularly in biomedical imaging tasks involving cerebral vasculature segmentation.

**Method:** Our approach uses a novel unsupervised domain adaptation methodology to segment cerebral vasculature in hyperspectral brain images, utilizing a small amount of expert-annotated data with unlabeled data to address the issue of label scarcity.

**Result:** Quantitative and qualitative evaluations show that the method significantly outperforms existing state-of-the-art approaches.

**Conclusion:** The research demonstrates the efficacy of domain adaptation for labeling scarce biomedical imaging tasks.

**Abstract:** This work presents a novel deep learning framework for segmenting cerebral
vasculature in hyperspectral brain images. We address the critical challenge of
severe label scarcity, which impedes conventional supervised training. Our
approach utilizes a novel unsupervised domain adaptation methodology, using a
small, expert-annotated ground truth alongside unlabeled data. Quantitative and
qualitative evaluations confirm that our method significantly outperforms
existing state-of-the-art approaches, demonstrating the efficacy of domain
adaptation for label-scarce biomedical imaging tasks.

</details>


### [60] [NAT: Learning to Attack Neurons for Enhanced Adversarial Transferability](https://arxiv.org/abs/2508.16937)
*Krishna Kanth Nakka,Alexandre Alahi*

Main category: cs.CV

> Neuron Attack for Transferability (NAT) targets specific neurons in neural networks, enhancing the transferability of adversarial attacks with significant improvement in fooling rates across various models.

<details>
  <summary>Details</summary>

**Motivation:** To address the limitation of layer-level optimizations which often disproportionately focus on a few neurons representing similar concepts, leaving other neurons minimally affected, thereby enhancing transferability.

**Method:** Neuron Attack for Transferability (NAT), which targets specific neurons within the embedding of a neural network, rather than focusing on embedding-level separation.

**Result:** NAT achieves fooling rates that surpass existing baselines by over 14% in cross-model and 4% in cross-domain settings. Furthermore, impressive fooling rates are achieved within just 10 queries.

**Conclusion:** By focusing on neuron-specific attacks, NAT provides a fundamental and effective way to disrupt neural networks and improve the transferability of adversarial perturbations across different models.

**Abstract:** The generation of transferable adversarial perturbations typically involves
training a generator to maximize embedding separation between clean and
adversarial images at a single mid-layer of a source model. In this work, we
build on this approach and introduce Neuron Attack for Transferability (NAT), a
method designed to target specific neuron within the embedding. Our approach is
motivated by the observation that previous layer-level optimizations often
disproportionately focus on a few neurons representing similar concepts,
leaving other neurons within the attacked layer minimally affected. NAT shifts
the focus from embedding-level separation to a more fundamental,
neuron-specific approach. We find that targeting individual neurons effectively
disrupts the core units of the neural network, providing a common basis for
transferability across different models. Through extensive experiments on 41
diverse ImageNet models and 9 fine-grained models, NAT achieves fooling rates
that surpass existing baselines by over 14\% in cross-model and 4\% in
cross-domain settings. Furthermore, by leveraging the complementary attacking
capabilities of the trained generators, we achieve impressive fooling rates
within just 10 queries. Our code is available at:
https://krishnakanthnakka.github.io/NAT/

</details>
