{"id": "2510.14992", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14992", "abs": "https://arxiv.org/abs/2510.14992", "authors": ["Leela Krishna", "Mengyang Zhao", "Saicharithreddy Pasula", "Harshit Rajgarhia", "Abhishek Mukherji"], "title": "GAZE:Governance-Aware pre-annotation for Zero-shot World Model Environments", "comment": null, "summary": "Training robust world models requires large-scale, precisely labeled\nmultimodal datasets, a process historically bottlenecked by slow and expensive\nmanual annotation. We present a production-tested GAZE pipeline that automates\nthe conversion of raw, long-form video into rich, task-ready supervision for\nworld-model training. Our system (i) normalizes proprietary 360-degree formats\ninto standard views and shards them for parallel processing; (ii) applies a\nsuite of AI models (scene understanding, object tracking, audio transcription,\nPII/NSFW/minor detection) for dense, multimodal pre-annotation; and (iii)\nconsolidates signals into a structured output specification for rapid human\nvalidation.\n  The GAZE workflow demonstrably yields efficiency gains (~19 minutes saved per\nreview hour) and reduces human review volume by >80% through conservative\nauto-skipping of low-salience segments. By increasing label density and\nconsistency while integrating privacy safeguards and chain-of-custody metadata,\nour method generates high-fidelity, privacy-aware datasets directly consumable\nfor learning cross-modal dynamics and action-conditioned prediction. We detail\nour orchestration, model choices, and data dictionary to provide a scalable\nblueprint for generating high-quality world model training data without\nsacrificing throughput or governance.", "AI": {"tldr": "GAZE pipeline for automating video annotation for world-model training, increasing efficiency and dataset quality.", "motivation": "The motivation behind this paper is the need for automated solutions for large-scale, precise annotation of multimodal datasets, essential for robust world models. This is due to the historically expensive and slow nature of manual annotation.", "method": "The paper introduces the GAZE pipeline for converting raw, long-form video into structured, task-ready supervision for world-model training. It consists of three main stages: normalization and sharding of 360-degree videos, dense pre-annotation using a suite of AI models, and consolidation into a structured output for human validation.", "result": "The GAZE workflow yields significant efficiency gains, saving about 19 minutes per review hour, and reduces human review volume by over 80%. The method enhances label density and consistency, incorporating privacy protections and metadata.", "conclusion": "The paper concludes that the GAZE pipeline provides a scalable blueprint for generating high-fidelity, privacy-aware datasets for training world models, without sacrificing throughput or governance."}}
