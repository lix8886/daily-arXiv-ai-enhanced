{"id": "2507.02088", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02088", "abs": "https://arxiv.org/abs/2507.02088", "authors": ["Tian Lan", "Xiangdong Su", "Xu Liu", "Ruirui Wang", "Ke Chang", "Jiang Li", "Guanglai Gao"], "title": "McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models", "comment": "24 pages, 9 figures", "summary": "As large language models (LLMs) are increasingly applied to various NLP\ntasks, their inherent biases are gradually disclosed. Therefore, measuring\nbiases in LLMs is crucial to mitigate its ethical risks. However, most existing\nbias evaluation datasets focus on English and North American culture, and their\nbias categories are not fully applicable to other cultures. The datasets\ngrounded in the Chinese language and culture are scarce. More importantly,\nthese datasets usually only support single evaluation tasks and cannot evaluate\nthe bias from multiple aspects in LLMs. To address these issues, we present a\nMulti-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias\nevaluation instances, covering 12 single bias categories, 82 subcategories and\nintroducing 5 evaluation tasks, providing extensive category coverage, content\ndiversity, and measuring comprehensiveness. Additionally, we evaluate several\npopular LLMs from different series and with parameter sizes. In general, all\nthese LLMs demonstrated varying degrees of bias. We conduct an in-depth\nanalysis of results, offering novel insights into bias in LLMs.", "AI": {"tldr": "提出了一个多任务中文偏见评估基准（McBE），全面覆盖了多个偏见类别，适用于评估以中文和中国文化为基础的LLMs的偏见问题。", "motivation": "现有的偏见评估数据集主要聚焦于英语和北美文化，适用于其他文化的数据集稀缺。此外，这些数据集通常只支持单一评估任务，不能从多个角度评估LLMs的偏见。", "method": "构建了一个多任务中文偏见评估基准（McBE），包括4,077个偏见评估实例，涵盖12个单偏见类别，82个子类别，并引入了5个评估任务。", "result": "评估了几种具有不同参数规模的流行LLMs，所有这些模型在不同程度上都表现出偏见。", "conclusion": "通过深入分析结果，对LLMs的偏见提供了新的见解，表明多任务评估基准对于理解并减轻LLMs的伦理风险很有帮助。"}}
{"id": "2507.02145", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02145", "abs": "https://arxiv.org/abs/2507.02145", "authors": ["Keyan Jin", "Yapeng Wang", "Leonel Santos", "Tao Fang", "Xu Yang", "Sio Kei Im", "Hugo Gonçalo Oliveira"], "title": "Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization", "comment": null, "summary": "Dialogue summarization is a challenging task with significant practical value\nin customer service, meeting analysis, and conversational AI. Although large\nlanguage models (LLMs) have achieved substantial progress in summarization\ntasks, the performance of step-by-step reasoning architectures-specifically\nLong Chain-of-Thought (CoT) implementations such as OpenAI-o1 and\nDeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent\nabstraction and conciseness. In this work, we present the first comprehensive\nand systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning\nLLMs across three major paradigms-generic, role-oriented, and query-oriented\ndialogue summarization. Our study spans diverse languages, domains, and summary\nlengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and\nadvanced evaluation protocols that include both LLM-based automatic metrics and\nhuman-inspired criteria. Contrary to trends in other reasoning-intensive tasks,\nour findings show that explicit stepwise reasoning does not consistently\nimprove dialogue summarization quality. Instead, reasoning LLMs are often prone\nto verbosity, factual inconsistencies, and less concise summaries compared to\ntheir non-reasoning counterparts. Through scenario-specific analyses and\ndetailed case studies, we further identify when and why explicit reasoning may\nfail to benefit-or even hinder-summarization in complex dialogue contexts. Our\nwork provides new insights into the limitations of current reasoning LLMs and\nhighlights the need for targeted modeling and evaluation strategies for\nreal-world dialogue summarization.", "AI": {"tldr": "研究了对话总结任务中长链推理架构的有效性，发现这类架构并不能一致提高总结质量，有时还会导致冗余、事实不准确和总结不够简洁。", "motivation": "动机在于评估目前长链推理的大语言模型在不同类型的对话总结中表现如何，并探索这些模型在复杂对话背景中的局限性。", "method": "通过对比推理大语言模型和非推理模型，在通用对话、角色导向和问题导向的总结范式中进行全面系统分析，使用多个强基准和高级的评估协议。", "result": "研究发现，在对话总结任务中，显式的逐步推理并不一定有助于提高总结质量。推理模型往往比非推理模型更冗长、不准确、不够简洁。", "conclusion": "研究表明，目前的推理大语言模型在对话总结中有局限性，并提出需要特定模型和评估策略来优化真实世界的对话总结。"}}
{"id": "2507.02199", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02199", "abs": "https://arxiv.org/abs/2507.02199", "authors": ["Wenquan Lu", "Yuechuan Yang", "Kyle Lee", "Yanshu Li", "Enqi Liu"], "title": "Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer", "comment": null, "summary": "Chain-of-thought (CoT) reasoning has enabled transformer-based language\nmodels to excel at complex mathematics and multi-step planning. However, in\nstandard decoder-only architectures, these reasoning steps are externalized in\nnatural language, improving interpretability at the cost of efficiency. To\ncapture reasoning that is not easily represented in words, many works have\nexplored recurrent architectures that aim to internalize reasoning in latent\nspace, potentially supporting latent CoT. In this paper, we investigate whether\nsuch reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer\nthat reuses layers at inference time without increasing parameter count. We\nexamine the model's internal behavior on arithmetic tasks using a suite of\nprobing techniques including the Logit Lens and Coda Lens. Our findings reveal\nlimited evidence of interpretable latent CoT by tracking rank trajectories of\nfinal and intermediate result tokens. Furthermore, we uncover significant\nprobing inconsistencies across recurrent blocks, where the interpretability of\nhidden states depends heavily on both the layer index and the decoding method.\nFinally, we empirically show that increasing recurrence depth yields only\nmarginal gains and falls well short of models that explicitly externalize\nreasoning steps. The code is available at\nhttps://github.com/wenquanlu/huginn-latent-cot.", "AI": {"tldr": "我们研究了Huginn-3.5B的内部推理结构，发现有限的证据支持其潜在的链式思维，并且递归深度增加的效果不明显。", "motivation": "我们希望通过这项研究来探索递归架构是否能在潜在空间内部化推理，特别是在Huginn-3.5B模型中，这种推理是否能支持潜在的链式思维。", "method": "我们通过一系列探针技术，包括Logit Lens和Coda Lens，来研究Huginn-3.5B模型在算术任务中的内部行为，以此探究其是否在潜在空间中形成了可解释的链式思维（CoT）结构。", "result": "研究发现，通过追踪最终结果和中间结果标记的排名轨迹，我们发现有限的证据表明存在可解释的潜在链式思维。同时，我们还发现探针技术在递归块之间的不一致性，隐藏状态的可解释性在很大程度上依赖于层数索引和解码方法。此外，我们还发现在递归深度增加时，效果的提升幅度有限，不及那些显式外部化推理步骤的模型。", "conclusion": "我们得出的结论是，尽管Huginn-3.5B模型在某些层面上展示了潜在的链式思维能力，但其在内部化推理方面仍有很大的局限性，尤其是在增加递归深度方面的提升并不显著。"}}
{"id": "2507.02221", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02221", "abs": "https://arxiv.org/abs/2507.02221", "authors": ["Steven Song", "Anirudh Subramanyam", "Zhenyu Zhang", "Aarti Venkat", "Robert L. Grossman"], "title": "GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons", "comment": "11 pages, 1 figure, 7 tables", "summary": "Motivation: The Genomic Data Commons (GDC) provides access to high quality,\nharmonized cancer genomics data through a unified curation and analysis\nplatform centered around patient cohorts. While GDC users can interactively\ncreate complex cohorts through the graphical Cohort Builder, users (especially\nnew ones) may struggle to find specific cohort descriptors across hundreds of\npossible fields and properties. However, users may be better able to describe\ntheir desired cohort in free-text natural language.\n  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for\ncurating cohorts from the GDC. GDC Cohort Copilot automatically generates the\nGDC cohort filter corresponding to a user-input natural language description of\ntheir desired cohort, before exporting the cohort back to the GDC for further\nanalysis. An interactive user interface allows users to further refine the\ngenerated cohort. We develop and evaluate multiple large language models (LLMs)\nfor GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC\nCohort LLM achieves better results than GPT-4o prompting in generating GDC\ncohorts.\n  Availability and implementation: The standalone docker image for GDC Cohort\nCopilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.\nSource code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC\nCohort LLM weights are available at https://huggingface.co/uc-ctds.", "AI": {"tldr": "研究团队开发了一个开源工具GDC Cohort Copilot，该工具可以把用户的自然语言描述转换成GDC队列过滤条件，从而辅助用户更快地创建和调整队列。", "motivation": "GDC提供了一个统一的整理和分析平台，通过这个平台可以获取高质量的癌症基因组数据。尽管用户的交互式创建复杂队列非常方便，但新用户可能在数百个可能的字段和属性中找到特定队列描述存在困难。我们发现用户更可能用自然语言来描述他们想要的队列。", "method": "我们引入了GDC队列助理（GDC Cohort Copilot），这是一个开源工具，用于从GDC中策划队列。该工具能够根据用户输入的自然语言描述自动生成相应的GDC队列筛选条件，并导出回GDC进行进一步分析。我们开发并评估了多个大型语言模型（LLMs），证明了本地服务的开源GDC队列LLM在生成GDC队列方面优于GPT-4的提示方法。", "result": "我们开发了一款开源工具GDC队列助理，它可以依据用户提供的自然语言描述自动生成特定患者的队列过滤条件。我们通过使用多种大型语言模型进行实验并展示了我们的模型优于现有模型的表现。", "conclusion": "我们成功开发了GDC队列助理这个开源工具，它可以自动生成符合用户自然语言描述的GDC队列过滤条件，用户还可以通过交互式用户界面进一步调整生成的队列。"}}
{"id": "2507.02074", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02074", "abs": "https://arxiv.org/abs/2507.02074", "authors": ["Sanjeda Akter", "Ibne Farabi Shihab", "Anuj Sharma"], "title": "Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges", "comment": null, "summary": "Crash detection from video feeds is a critical problem in intelligent\ntransportation systems. Recent developments in large language models (LLMs) and\nvision-language models (VLMs) have transformed how we process, reason about,\nand summarize multimodal information. This paper surveys recent methods\nleveraging LLMs for crash detection from video data. We present a structured\ntaxonomy of fusion strategies, summarize key datasets, analyze model\narchitectures, compare performance benchmarks, and discuss ongoing challenges\nand opportunities. Our review provides a foundation for future research in this\nfast-growing intersection of video understanding and foundation models.", "AI": {"tldr": "本文综述了利用大规模语言模型和视觉语言模型从视频数据中检测碰撞的方法，并提出了融合策略的结构化分类，总结了关键数据集，分析了模型架构，比较了性能基准，并讨论了当前的挑战和机遇。", "motivation": "动机在于利用最新的大规模语言模型和视觉语言模型技术提高智能交通系统中碰撞检测的准确性和效率。", "method": "文章采用了综述的方法，介绍了融合策略，总结了数据集，分析了模型架构，比较了性能基准。", "result": "文章提供了一个关于视频理解和基础模型这一快速发展的交叉领域的未来研究基础。", "conclusion": "通过综述，文章为未来的研究提供了一个坚实的基础，特别是在视频理解和基础模型的交叉领域中更多的创新与发展。"}}
{"id": "2507.02259", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02259", "abs": "https://arxiv.org/abs/2507.02259", "authors": ["Hongli Yu", "Tinghong Chen", "Jiangtao Feng", "Jiangjie Chen", "Weinan Dai", "Qiying Yu", "Ya-Qin Zhang", "Wei-Ying Ma", "Jingjing Liu", "Mingxuan Wang", "Hao Zhou"], "title": "MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent", "comment": "Project Page: https://memagent-sialab.github.io/", "summary": "Despite improvements by length extrapolation, efficient attention and memory\nmodules, handling infinitely long documents with linear complexity without\nperformance degradation during extrapolation remains the ultimate challenge in\nlong-text processing. We directly optimize for long-text tasks in an end-to-end\nfashion and introduce a novel agent workflow, MemAgent, which reads text in\nsegments and updates the memory using an overwrite strategy. We extend the DAPO\nalgorithm to facilitate training via independent-context multi-conversation\ngeneration. MemAgent has demonstrated superb long-context capabilities, being\nable to extrapolate from an 8K context trained on 32K text to a 3.5M QA task\nwith performance loss < 5% and achieves 95%+ in 512K RULER test.", "AI": {"tldr": "提出了MemAgent，一种新的Agent工作流，以解决长文本处理中的性能问题，展示了领先的长文档处理能力。", "motivation": "尽管在长度外推、高效注意力和内存模块方面有所改进，但长文本处理中仍存在一个棘手问题：如何在处理无限长度文档时保持线性复杂度而不影响性能。因此，直接针对长文本任务进行端到端优化至关重要。", "method": "本研究引入了一种新的Agent工作流MemAgent，该工作流通过段落读取文本并采用覆盖策略更新内存。研究还扩展了DAPO算法以支持通过独立上下文多对话生成进行训练。", "result": "MemAgent展示了出色的长上下文能力，能够从8K上下文中调整以32K文本为训练基准并在3.5M QA任务中表现，性能损失小于5%。同时，在512K RULER测试中达到了95%以上的正确率。", "conclusion": "MemAgent在处理长文本任务时表现出色，能够有效地处理大段落的文本并保持高水平的性能，证明了其在长文本处理中的巨大潜力。"}}
{"id": "2507.02148", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02148", "abs": "https://arxiv.org/abs/2507.02148", "authors": ["Zijie Cai", "Christopher Metzler"], "title": "Underwater Monocular Metric Depth Estimation: Real-World Benchmarks and Synthetic Fine-Tuning", "comment": null, "summary": "Monocular depth estimation has recently advanced to provide not only relative\nbut also metric depth predictions. However, its reliability in underwater\nenvironments remains limited due to light attenuation and scattering, color\ndistortion, turbidity, and the lack of high-quality metric ground-truth data.\nIn this paper, we present a comprehensive benchmark of zero-shot and fine-tuned\nmonocular metric depth estimation models on real-world underwater datasets with\nmetric depth annotations, such as FLSea and SQUID. We evaluate a diverse set of\nstate-of-the-art models across a range of underwater conditions with different\nranges. Our results show that large-scale models trained on terrestrial (real\nor synthetic) data, while effective in in-air settings, perform poorly\nunderwater due to significant domain shifts. To address this, we fine-tune\nDepth Anything V2 with a ViT-S backbone encoder on a synthetic underwater\nvariant of the Hypersim dataset, which we generated using a physically based\nunderwater image formation model. We demonstrate our fine-tuned model\nconsistently improves performance across all benchmarks and outperforms\nbaselines trained only on the clean in-air Hypersim dataset. Our study provides\na detailed evaluation and visualization for monocular metric depth estimation\nin underwater scenes, highlighting the importance of domain adaptation and\nscale-aware supervision for achieving robust and generalizable metric depth\npredictions in challenging underwater environments for future research.", "AI": {"tldr": "本文对单目度量深度预测在水下环境中的表现进行了全面评估，发现只在地面数据集上训练的模型在水下效果不佳。作者通过微调模型以适应水下数据，提高了整体性能。", "motivation": "单目深度估计在水下环境中的可靠性有限，主要是因为光衰减、颜色失真、浑浊度和缺乏高质量的度量真实数据。本文旨在提高水下环境中的单目度量深度预测的性能。", "method": "通过在使用基于物理的水下图像生成模型生成的合成水下Hypersim数据集上对Depth Anything V2（具有ViT-S主干编码器）进行微调，来改善水下环境中的单目度量深度预测。", "result": "实验结果表明，大规模模型在地面数据（真实或合成）上训练虽然在空气中表现良好，但在水下表现不佳，因为存在显著的数据域差异。微调模型在所有基准测试中的性能都有所提高，优于仅在清洁的Hypersim数据集上训练的模型。", "conclusion": "研究结果强调了领域适应和尺度感知监管对于实现水下场景中稳健和通用的度量深度预测的重要性，这是未来研究的重点。"}}
{"id": "2507.02302", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02302", "abs": "https://arxiv.org/abs/2507.02302", "authors": ["Dohoon Kim", "Donghun Kang", "Taesup Moon"], "title": "DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning", "comment": "22 pages, 5 figures, ACL 2025 Main", "summary": "Domain-Adaptive Pre-training (DAP) has recently gained attention for its\neffectiveness in fine-tuning pre-trained models. Building on this, continual\nDAP has been explored to develop pre-trained models capable of incrementally\nincorporating different domain datasets. However, existing continual DAP\nmethods face several limitations: (1) high computational cost and GPU memory\nusage during training; (2) sensitivity to incremental data order; and (3)\nproviding a single, generalized model for all end tasks, which contradicts the\nessence of DAP. In this paper, we propose DoMIX, a novel approach that\naddresses these challenges by leveraging LoRA modules, a representative\nparameter-efficient fine-tuning (PEFT) method. Our approach enables efficient\nand parallel domain-adaptive pre-training that is robust to domain order and\neffectively utilizes accumulated knowledge to provide tailored pre-trained\nmodels for specific tasks. We also demonstrate that our method can be extended\nbeyond the DAP setting to standard LLM fine-tuning scenarios. Code is available\nat https://github.com/dohoonkim-ai/DoMIX.", "AI": {"tldr": "我们提出了DoMIX，一种基于LoRA模块的新方法，解决了连续域适应预训练中的计算成本高、对增量数据顺序敏感及无法提供针对特定任务的定制模型等问题，适用于标准LLM微调场景。", "motivation": "现有的连续域适应预训练方法面临着高计算成本和GPU内存使用、对增量数据顺序敏感以及提供的单一通用模型与域适应的核心思想相矛盾等多重挑战。因此，提出了一种新的解决方案。", "method": "通过利用LoRA模块，一种典型的参数高效微调(PEFT)方法，我们提出了一种新的方法DoMIX，该方法解决了现有的连续域适应预训练方法的挑战，实现了对域顺序鲁棒的高效并行域自适应预训练，并有效地利用积累的知识，为特定任务提供定制的预训练模型。", "result": "我们的方法不仅能够提高预训练模型在不同域上的表现，还能够减少计算成本和GPU内存的使用。此外，它对增量数据的顺序也更加鲁棒。我们还展示了我们的方法可以扩展到标准LLM微调场景以外的领域自适应预训练设置。", "conclusion": "DoMIX方法成功解决了当前连续域适应预训练存在的问题，增加了对域顺序的鲁棒性，同时减少了计算成本和GPU内存需求，能够在标准LLM微调场景之外扩展应用。代码可在https://github.com/dohoonkim-ai/DoMIX获取。"}}
{"id": "2507.02200", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02200", "abs": "https://arxiv.org/abs/2507.02200", "authors": ["Xiao Wang", "Jingtao Jiang", "Qiang Chen", "Lan Chen", "Lin Zhu", "Yaowei Wang", "Yonghong Tian", "Jin Tang"], "title": "ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning", "comment": "A Strong Baseline for Reasoning based Event Stream Scene Text\n  Recognition", "summary": "Event stream based scene text recognition is a newly arising research topic\nin recent years which performs better than the widely used RGB cameras in\nextremely challenging scenarios, especially the low illumination, fast motion.\nExisting works either adopt end-to-end encoder-decoder framework or large\nlanguage models for enhanced recognition, however, they are still limited by\nthe challenges of insufficient interpretability and weak contextual logical\nreasoning. In this work, we propose a novel chain-of-thought reasoning based\nevent stream scene text recognition framework, termed ESTR-CoT. Specifically,\nwe first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input\nevent stream into tokens and utilize a Llama tokenizer to encode the given\ngeneration prompt. A Q-former is used to align the vision token to the\npre-trained large language model Vicuna-7B and output both the answer and\nchain-of-thought (CoT) reasoning process simultaneously. Our framework can be\noptimized using supervised fine-tuning in an end-to-end manner. In addition, we\nalso propose a large-scale CoT dataset to train our framework via a three stage\nprocessing (i.e., generation, polish, and expert verification). This dataset\nprovides a solid data foundation for the development of subsequent\nreasoning-based large models. Extensive experiments on three event stream STR\nbenchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the\neffectiveness and interpretability of our proposed framework. The source code\nand pre-trained models will be released on\nhttps://github.com/Event-AHU/ESTR-CoT.", "AI": {"tldr": "本文提出了一个新的基于事件流的场景文本识别框架ESTR-CoT，结合视觉变换器和大规模语言模型进行解释性推理，通过三级数据处理生成了大规模推理数据集，实验表明该框架在多个基准数据集上效果显著。", "motivation": "现有的基于事件流的场景文本识别框架存在可解释性和逻辑推理能力不足的问题，因此本文提出了ESTR-CoT框架以提升识别效果。", "method": "方法包括使用EVA-CLIP将事件流转换为标记，Q-former将视觉标记与大规模语言模型对齐，并同时输出答案和推理过程。", "result": "实验结果显示ESTR-CoT框架在多个基准数据集上具有显著的识别效果和解释能力。", "conclusion": "ESTR-CoT框架通过引入大规模语言模型进行解释性推理，可有效地提高事件流场景文本识别的性能，且提供了大规模的推理数据集供后续模型开发。"}}
{"id": "2507.02357", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02357", "abs": "https://arxiv.org/abs/2507.02357", "authors": ["Christian Jaumann", "Annemarie Friedrich", "Rainer Lienhart"], "title": "Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models", "comment": "Accepted at 5th Workshop on Scholarly Document Processing @ ACL 2025", "summary": "This paper describes our system for the SciVQA 2025 Shared Task on Scientific\nVisual Question Answering. Our system employs an ensemble of two Multimodal\nLarge Language Models and various few-shot example retrieval strategies. The\nmodel and few-shot setting are selected based on the figure and question type.\nWe also select answers based on the models' confidence levels. On the blind\ntest data, our system ranks third out of seven with an average F1 score of\n85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.", "AI": {"tldr": "本文介绍了一个用于科学视觉问答任务的系统，该系统利用多模态大语言模型进行问答，并通过选择模型及检索策略来提升性能。系统在评估中表现良好，排名第三。", "motivation": "为了参加SciVQA 2025共享任务（科学视觉问答任务），开发了一个用于科学视觉问答的系统。", "method": "采用包含两个多模态大语言模型的集成系统以及多种基于少量样本的检索策略。系统根据图形和问题类型选择模型和少量样本设置，并基于模型的置信水平选择答案。", "result": "在盲测数据上，该系统排名第三，平均F1得分为85.12。评分标准包括ROUGE-1、ROUGE-L、BERTS。", "conclusion": "该系统达到了满意的性能，并且相关代码已公开。"}}
