<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 7]
- [cs.CV](#cs.CV) [Total: 11]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Enhancing Foundation Models in Transaction Understanding with LLM-based Sentence Embeddings](https://arxiv.org/abs/2601.05271)
*Xiran Fan,Zhimeng Jiang,Chin-Chia Michael Yeh,Yuzhong Chen,Yingtong Dou,Menghai Pan,Yan Zheng*

Main category: cs.CL

> 研究开发了一种混合框架，利用大型语言模型（LLMs）的语义嵌入初始化轻量级模型，发现此方法在保持性能和可解释性的前提下，显著提高了交易数据处理的效率。

<details>
  <summary>Details</summary>

**Motivation:** 当前的交易分析模型虽采用多步处理流程，但在处理商家分类字段时依赖基于索引的表示方法，导致丰富的文本数据转化为离散标记，造成语义信息的大量损失。因此，需要一种既能减少计算代价，又能利用高级语义理解的方式来解决这个问题。

**Method:** 本研究提出了一种混合框架，利用大型语言模型（LLMs）生成的嵌入作为轻量级交易模型的语义初始化，平衡了可解释性和操作效率。该方法使用多源数据融合来丰富商户分类字段，并采用单词约束原则来保证在不同LLM架构之间的一致嵌入生成。

**Result:** 实验结果表明，在大规模交易数据集上，该方法在多项交易理解任务中性能显著提升。

**Conclusion:** 此混合框架通过结合大型语言模型的语义理解和轻量级模型的效率优势，成功提高了交易数据处理性能，尤其是在保持一致性嵌入生成方面。

**Abstract:** The ubiquity of payment networks generates vast transactional data encoding rich consumer and merchant behavioral patterns. Recent foundation models for transaction analysis process tabular data sequentially but rely on index-based representations for categorical merchant fields, causing substantial semantic information loss by converting rich textual data into discrete tokens. While Large Language Models (LLMs) can address this limitation through superior semantic understanding, their computational overhead challenges real-time financial deployment. We introduce a hybrid framework that uses LLM-generated embeddings as semantic initializations for lightweight transaction models, balancing interpretability with operational efficiency. Our approach employs multi-source data fusion to enrich merchant categorical fields and a one-word constraint principle for consistent embedding generation across LLM architectures. We systematically address data quality through noise filtering and context-aware enrichment. Experiments on large-scale transaction datasets demonstrate significant performance improvements across multiple transaction understanding tasks.

</details>


### [2] [The Table of Media Bias Elements: A sentence-level taxonomy of media bias types and propaganda techniques](https://arxiv.org/abs/2601.05358)
*Tim Menzner,Jochen L. Leidner*

Main category: cs.CL

> 研究开发了一种更细致、针对句子层面的媒体偏见分类方法，并展示了其在细化和减少模糊性方面的优势。

<details>
  <summary>Details</summary>

**Motivation:** 此研究旨在从具体的语言操纵手段角度研究偏见，而不是简单地根据政治光谱来看待媒体立场。

**Method:** 该研究结合了细致阅读、跨学科理论和初步注释，从新闻语料库、用户提交和个人浏览中收集了26,464个句子，制定出一套细粒度的、句子级别的媒体偏见和宣传分类。

**Result:** 研究结果是一个两层级模式，包含38种基本的偏见类型，它们排列在六个功能组中，并以“媒体偏见元素表”的形式呈现出来。

**Conclusion:** 相对于现有的NLP和传播科学分类法，该分类法提供了广泛的内容覆盖和模糊性的减少，并通过具体的定义、实例及认知和社会驱动因素等，为每种类型提供了识别的指导。

**Abstract:** Public debates about "left-" or "right-wing" news overlook the fact that bias is usually conveyed by concrete linguistic manoeuvres that transcend any single political spectrum. We therefore shift the focus from where an outlet allegedly stands to how partiality is expressed in individual sentences. Drawing on 26,464 sentences collected from newsroom corpora, user submissions and our own browsing, we iteratively combine close-reading, interdisciplinary theory and pilot annotation to derive a fine-grained, sentence-level taxonomy of media bias and propaganda. The result is a two-tier schema comprising 38 elementary bias types, arranged in six functional families and visualised as a "table of media-bias elements". For each type we supply a definition, real-world examples, cognitive and societal drivers, and guidance for recognition. A quantitative survey of a random 155-sentence sample illustrates prevalence differences, while a cross-walk to the best-known NLP and communication-science taxonomies reveals substantial coverage gains and reduced ambiguity.

</details>


### [3] [Lost in Execution: On the Multilingual Robustness of Tool Calling in Large Language Models](https://arxiv.org/abs/2601.05366)
*Zheng Luo,T Pranav Kutralingam,Ogochukwu N Okoani,Wanpeng Xu,Hua Wei,Xiyang Hu*

Main category: cs.CL

> 研究提出了MLCL基准，评估了大型语言模型在不同语言中的工具调用效率，发现语言不匹配是主要问题，尽管测试了几种策略，但与英文环境相比仍有差距。

<details>
  <summary>Details</summary>

**Motivation:** 尽管最近的研究表明在标准英文环境中大型语言模型的工具调用性能良好，但对于多语言用户交互中的性能稳健性探讨较少，这也是研究的动机。

**Method:** 本研究提出了一个名为MLCL的诊断基准，并系统地评估了中文、印地语和低资源语言伊博语的多语言工具调用表现。通过细致的错误分析，揭示了尽管理解了正确的意图并选择了正确的工具，但在参数值的语言不匹配方面存在主要的失败模式。除此之外，还评估了几种推理时的系统策略。

**Result:** 研究发现，这些策略在很大程度上减少了由语言引起的执行错误，但没有任何一种策略能够完全恢复到英文水平的表现。

**Conclusion:** 研究表明多语言环境下工具调用的鲁棒性问题，特别是在参数值的语言不匹配上，这为进一步研究指明了方向。

**Abstract:** Large Language Models (LLMs) are increasingly deployed as agents that invoke external tools through structured function calls. While recent work reports strong tool-calling performance under standard English-centric evaluations, the robustness of tool calling under multilingual user interactions remains underexplored. In this work, we introduce MLCL, a diagnostic benchmark, and conduct a systematic evaluation of multilingual tool calling across Chinese, Hindi, and the low-resource language Igbo. Through fine-grained error analysis, we show that many failures occur despite correct intent understanding and tool selection. We identify parameter value language mismatch as a dominant failure mode, where models generate semantically appropriate parameter values in the user's language, violating language-invariant execution conventions. We further evaluate several inference-time system strategies and find that while these strategies substantially reduce language-induced execution errors, none of them can fully recover English-level performance.

</details>


### [4] [Same Claim, Different Judgment: Benchmarking Scenario-Induced Bias in Multilingual Financial Misinformation Detection](https://arxiv.org/abs/2601.05403)
*Zhiwei Liu,Yupen Cao,Yuechen Jiang,Mohsinul Kabir,Polydoros Giannouris,Chen Xu,Ziyang Xu,Tianlei Zhu,Tariquzzaman Faisal,Triantafillos Papadopoulos,Yan Wang,Lingfei Qian,Xueqing Peng,Zhuohan Xie,Ye Yuan,Saeed Almheiri,Abdulrazzaq Alnajjar,Mingbin Chen,Harry Stuart,Paul Thompson,Prayag Tiwari,Alejandro Lopez-Lira,Xue Liu,Jimin Huang,Sophia Ananiadou*

Main category: cs.CL

> 为了评估大型语言模型在金融虚假信息检测中的行为偏差，构建了不同类型的金融场景和多语言数据集，系统评估了22个主流大型语言模型，发现存在普遍的行为偏差。实验项目可在https://github.com/lzw108/FMD获取。

<details>
  <summary>Details</summary>

**Motivation:** 现有的研究主要集中在直接提问或简化的通用设置上，对于复杂的现实世界金融环境和高风险、上下文敏感、多语言的金融虚假信息检测任务考虑较少。

**Method:** 与金融专家合作构建了三种复杂的金融场景类型，并开发了一个涵盖英语、中文、希腊语和孟加拉语的多语言金融虚假信息数据集，系统地评估了22个主流大型语言模型。

**Result:** 行为偏差在商业和开源模型中普遍存在。

**Conclusion:** 研究发现，主流的商业和开源大型语言模型中普遍存在明显的行为偏差。

**Abstract:** Large language models (LLMs) have been widely applied across various domains of finance. Since their training data are largely derived from human-authored corpora, LLMs may inherit a range of human biases. Behavioral biases can lead to instability and uncertainty in decision-making, particularly when processing financial information. However, existing research on LLM bias has mainly focused on direct questioning or simplified, general-purpose settings, with limited consideration of the complex real-world financial environments and high-risk, context-sensitive, multilingual financial misinformation detection tasks (\mfmd). In this work, we propose \mfmdscen, a comprehensive benchmark for evaluating behavioral biases of LLMs in \mfmd across diverse economic scenarios. In collaboration with financial experts, we construct three types of complex financial scenarios: (i) role- and personality-based, (ii) role- and region-based, and (iii) role-based scenarios incorporating ethnicity and religious beliefs. We further develop a multilingual financial misinformation dataset covering English, Chinese, Greek, and Bengali. By integrating these scenarios with misinformation claims, \mfmdscen enables a systematic evaluation of 22 mainstream LLMs. Our findings reveal that pronounced behavioral biases persist across both commercial and open-source models. This project will be available at https://github.com/lzw108/FMD.

</details>


### [5] [Glitter: Visualizing Lexical Surprisal for Readability in Administrative Texts](https://arxiv.org/abs/2601.05411)
*Jan Černý,Ivana Kvapilíková,Silvie Cinková*

Main category: cs.CL

> 研究利用文本信息熵测量来估计其可读性，并提出一个可视化框架来进行估算和改善行政或官僚文件的可读性和清晰度。

<details>
  <summary>Details</summary>

**Motivation:** 改善行政或官僚文件的可读性和清晰度。

**Method:** 使用多个语言模型测量文本的信息熵，并提出一个可视化框架来逼近信息熵的测量结果。

**Result:** 未具体说明实验结果，但提供了一个开源工具集用于文本可读性的估计与改善。

**Conclusion:** 研究提供了一种方法和技术工具，可用于行政或官僚文本可读性及清晰度的评估和提升。

**Abstract:** This work investigates how measuring information entropy of text can be used to estimate its readability. We propose a visualization framework that can be used to approximate information entropy of text using multiple language models and visualize the result. The end goal is to use this method to estimate and improve readability and clarity of administrative or bureaucratic texts. Our toolset is available as a libre software on https://github.com/ufal/Glitter.

</details>


### [6] [Large Language Models Are Bad Dice Players: LLMs Struggle to Generate Random Numbers from Statistical Distributions](https://arxiv.org/abs/2601.05414)
*Minda Zhao,Yilun Du,Mengyu Wang*

Main category: cs.CL

> The study audits probabilistic sampling in 11 LLMs and finds that current models lack a functional internal sampler and fail when tasked with statistical sampling requirements.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this research is the need for LLMs to faithfully sample from specified probability distributions, which is crucial for their integration into stochastic pipelines in various domains.

**Method:** The method involves a large-scale audit of probabilistic sampling in 11 frontier LLMs across 15 distributions, with a dual-protocol design including Batch Generation and Independent Requests.

**Result:** The results show a protocol asymmetry, where batch generation has a modest statistical validity with a 13% median pass rate, while independent requests fail almost entirely. Sampling fidelity declines with increased distributional complexity and sampling horizon.

**Conclusion:** The conclusion is that current LLMs do not possess a reliable internal sampling mechanism and will require external tools to fulfill applications needing statistical guarantees.

**Abstract:** As large language models (LLMs) transition from chat interfaces to integral components of stochastic pipelines across domains like educational assessment and synthetic data construction, the ability to faithfully sample from specified probability distributions has become a functional requirement rather than a theoretical curiosity. We present the first large-scale, statistically powered audit of native probabilistic sampling in frontier LLMs, benchmarking 11 models across 15 distributions. To disentangle failure modes, we employ a dual-protocol design: Batch Generation, where a model produces N=1000 samples within one response, and Independent Requests, comprising $N=1000$ stateless calls. We observe a sharp protocol asymmetry: batch generation achieves only modest statistical validity, with a 13% median pass rate, while independent requests collapse almost entirely, with 10 of 11 models passing none of the distributions. Beyond this asymmetry, we reveal that sampling fidelity degrades monotonically with distributional complexity and aggravates as the requested sampling horizon N increases. Finally, we demonstrate the propagation of these failures into downstream tasks: models fail to enforce uniform answer-position constraints in MCQ generation and systematically violate demographic targets in attribute-constrained text-to-image prompt synthesis. These findings indicate that current LLMs lack a functional internal sampler, necessitating the use of external tools for applications requiring statistical guarantees.

</details>


### [7] [Tracing Moral Foundations in Large Language Models](https://arxiv.org/abs/2601.05437)
*Chenxiao Yu,Bowen Yi,Farzan Karimi-Malekabadi,Suhaib Abdurahman,Jinyi Ye,Shrikanth Narayanan,Yue Zhao,Morteza Dehghani*

Main category: cs.CL

> 研究证明了两个指令优化的大型语言模型——Llama-3.1-8B-Instruct和Qwen2.5-7B-Instruct能够以结构化、分层级的方式理解并表达道德基础，方法上结合了逐层分析、稀疏自动编码器和支持因果干预。这一结果提供了LLM内部表示和道德输出间因果联系的证据，并表明道德结构可从语言统计规律中浮现。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型（LLMs）往往产生类似人类的道德判断，但尚不清楚这反映了一种内在的概念结构还是表面的“道德模仿”。因此，研究动机是揭示LLM中道德概念的实际内核，确定它们是否真的是按照人类道德感知来编码和表现。

**Method:** 研究使用道德基础理论（MFT）作为分析框架，探讨了两个指令调整的大型语言模型（LLM）——Llama-3.1-8B-Instruct和Qwen2.5-7B-Instruct中的道德基础是如何被编码、组织和表达的。研究采用了多层次的方法，包括（i）MFT概念表示的逐层分析及其与人类道德感知的对齐，（ii）针对残差流的预训练稀疏自动编码器（SAE）以识别支持道德概念的稀疏特征，以及（iii）使用密集MFT向量和稀疏SAE特征的因果导向干预。

**Result:** 研究发现，两个LLM在不同层级上以结构化方式表示并区分了道德基础，并且与人类判断相一致。SAE特征显示与特定道德基础具有明确的语义联系，表明在共享表示中存在部分解耦的机制。

**Conclusion:** 结果表明，两个模型在不同层级上以结构化方式表示并区分了道德基础，并且与人类判断相一致。在更为精细的层面上，SAE特征与特定的道德基础显示出明确的语义联系，表明在共享表示中存在部分解耦的机制。最后，沿密集向量或稀疏特征引导会导致道德行为相关特征的可预测变化，证明了内部表示与道德输出之间的因果联系。研究结果提供了机制性的证据，表明LLM中的道德概念是分散的、分层的，并且部分解耦，暗示多元化的道德结构可以从语言的统计规律中以潜伏模式出现。

**Abstract:** Large language models (LLMs) often produce human-like moral judgments, but it is unclear whether this reflects an internal conceptual structure or superficial ``moral mimicry.'' Using Moral Foundations Theory (MFT) as an analytic framework, we study how moral foundations are encoded, organized, and expressed within two instruction-tuned LLMs: Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct. We employ a multi-level approach combining (i) layer-wise analysis of MFT concept representations and their alignment with human moral perceptions, (ii) pretrained sparse autoencoders (SAEs) over the residual stream to identify sparse features that support moral concepts, and (iii) causal steering interventions using dense MFT vectors and sparse SAE features. We find that both models represent and distinguish moral foundations in a structured, layer-dependent way that aligns with human judgments. At a finer scale, SAE features show clear semantic links to specific foundations, suggesting partially disentangled mechanisms within shared representations. Finally, steering along either dense vectors or sparse features produces predictable shifts in foundation-relevant behavior, demonstrating a causal connection between internal representations and moral outputs. Together, our results provide mechanistic evidence that moral concepts in LLMs are distributed, layered, and partly disentangled, suggesting that pluralistic moral structure can emerge as a latent pattern from the statistical regularities of language alone.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [Bi-Orthogonal Factor Decomposition for Vision Transformers](https://arxiv.org/abs/2601.05328)
*Fenil R. Doshi,Thomas Fel,Talia Konkle,George Alvarez*

Main category: cs.CV

> 通过引入双正交因子分解（BFD），我们发现注意力在视觉变换器中的主要操作方式及其专业化的机制，揭示了DINOv2在形状处理上的优越性。

<details>
  <summary>Details</summary>

**Motivation:** 自注意力是视觉变换器中的中心计算原语，但我们缺乏对注意力机制在令牌之间交换的信息的原理性理解。注意力地图描述权重集中在何处，但并没有揭示查询和键是否在交换位置、内容，或者两者。

**Method:** 我们引入了双正交因子分解（BFD），这是一种两阶段的分析框架：第一阶段，通过基于方差分析（ANOVA）的分解，将令牌激活统计分解为位置和内容的正交因子；第二阶段，利用查询-键交互矩阵QK^T的奇异值分解（SVD），揭示这些因子如何介导通信。

**Result:** 通过BFD对最先进的视觉模型的应用，我们发现了三个现象：(i) 注意力主要通过内容操作。内容-内容交互是注意力能量的主要来源，其次是内容-位置耦合。DINOv2比监督模型分配更多能量至内容-位置，并在更丰富的模式谱中分布计算。(ii) 注意力机制展现出多样化：头部分为内容-内容、内容-位置和位置-位置操作符，而头部内的奇异模式也表现出类似的专业化。(iii) DINOv2对整体形状处理的优越性源自中间层的处理方式，它们在保留位置结构的同时，上下文丰富语义内容。

**Conclusion:** BFD揭示了令牌如何通过注意力交互和哪些信息因素——位置或语义——介导它们之间的通信，为视觉变换器机制提供了实用的见解。

**Abstract:** Self-attention is the central computational primitive of Vision Transformers, yet we lack a principled understanding of what information attention mechanisms exchange between tokens. Attention maps describe where weight mass concentrates; they do not reveal whether queries and keys trade position, content, or both. We introduce Bi-orthogonal Factor Decomposition (BFD), a two-stage analytical framework: first, an ANOVA-based decomposition statistically disentangles token activations into orthogonal positional and content factors; second, SVD of the query-key interaction matrix QK^T exposes bi-orthogonal modes that reveal how these factors mediate communication. After validating proper isolation of position and content, we apply BFD to state-of-the-art vision models and uncover three phenomena.(i) Attention operates primarily through content. Content-content interactions dominate attention energy, followed by content-position coupling. DINOv2 allocates more energy to content-position than supervised models and distributes computation across a richer mode spectrum. (ii) Attention mechanisms exhibit specialization: heads differentiate into content-content, content-position, and position-position operators, while singular modes within heads show analogous specialization. (iii) DINOv2's superior holistic shape processing emerges from intermediate layers that simultaneously preserve positional structure while contextually enriching semantic content.
  Overall, BFD exposes how tokens interact through attention and which informational factors - positional or semantic - mediate their communication, yielding practical insights into vision transformer mechanisms.

</details>


### [9] [Coding the Visual World: From Image to Simulation Using Vision Language Models](https://arxiv.org/abs/2601.05344)
*Sagi Eppel*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** The ability to construct mental models of the world is a central aspect of understanding. Similarly, visual understanding can be viewed as the ability to construct a representative model of the system depicted in an image. This work explores the capacity of Vision Language Models (VLMs) to recognize and simulate the systems and mechanisms depicted in images using the Im2Sim methodology. The VLM is given a natural image of a real-world system (e.g., cities, clouds, vegetation) and is tasked with describing the system and writing code that simulates and generates it. This generative code is then executed to produce a synthetic image, which is compared against the original. This approach is tested on various complex emergent systems, ranging from physical systems (waves, lights, clouds) to vegetation, cities, materials, and geological formations. Through analysis of the models and images generated by the VLMs, we examine their understanding of the systems in images. The results show that leading VLMs (GPT, Gemini) demonstrate the capacity to understand and model complex, multi-component systems across multiple layers of abstraction and a wide range of domains. At the same time, the VLMs exhibit limited ability to replicate fine details and low-level arrangements of patterns in the image. These findings reveal an interesting asymmetry: VLMs combine high-level, deep visual understanding of images with limited perception of fine details.

</details>


### [10] [STResNet & STYOLO : A New Family of Compact Classification and Object Detection Models for MCUs](https://arxiv.org/abs/2601.05364)
*Sudhakar Sah,Ravish Kumar*

Main category: cs.CV

> 本文介绍了STResNet和STYOLO两种新的轻量级模型系列，它们在保持竞争力的精度的同时，优化了在资源受限设备上的效率和内存占用。

<details>
  <summary>Details</summary>

**Motivation:** 轻量级神经网络的最新进展显著提高了在边缘设备上部署深度学习模型的效率，但现有的大部分架构仍然在精度和延迟之间作出权衡。这限制了它们在微控制器和神经处理单元设备上的适用性。因此，该工作旨在提高在资源受限平台上精确性、效率和内存占用之间的平衡。

**Method:** 介绍了两种新的模型系列：STResNet用于图像分类和STYOLO用于物体检测。STResNet系列的模型（从Nano到Tiny版本）在参数预算为四百万的情况下实现了具有竞争力的ImageNet 1K精度。特别地，STResNetMilli在只有三百万参数的情况下，达到了70.0%的Top 1精度，优于MobileNetV1和ShuffleNetV2。STYOLOMicro和STYOLOMilli在MS COCO数据集上分别达到了30.5%和33.6%的平均精度，优于YOLOv5n和YOLOX Nano，在精度和效率上都表现出色。

**Result:** STResNetMilli在只有三百万参数的情况下达到了70.0%的Top 1精度，超过MobileNetV1和ShuffleNetV2。STYOLOMicro和STYOLOMilli模型分别在MS COCO数据集上实现了30.5%和33.6%的平均精度，优于YOLOv5n和YOLOX Nano。

**Conclusion:** 研究表明，STResNet和STYOLO系列模型在资源受限的平台上提供了卓越的精度和效率，成功实现了精确性和资源使用之间的良好平衡。

**Abstract:** Recent advancements in lightweight neural networks have significantly improved the efficiency of deploying deep learning models on edge hardware. However, most existing architectures still trade accuracy for latency, which limits their applicability on microcontroller and neural processing unit based devices. In this work, we introduce two new model families, STResNet for image classification and STYOLO for object detection, jointly optimized for accuracy, efficiency, and memory footprint on resource constrained platforms. The proposed STResNet series, ranging from Nano to Tiny variants, achieves competitive ImageNet 1K accuracy within a four million parameter budget. Specifically, STResNetMilli attains 70.0 percent Top 1 accuracy with only three million parameters, outperforming MobileNetV1 and ShuffleNetV2 at comparable computational complexity. For object detection, STYOLOMicro and STYOLOMilli achieve 30.5 percent and 33.6 percent mean average precision, respectively, on the MS COCO dataset, surpassing YOLOv5n and YOLOX Nano in both accuracy and efficiency. Furthermore, when STResNetMilli is used as a backbone with the Ultralytics training environment.

</details>


### [11] [MOSAIC-GS: Monocular Scene Reconstruction via Advanced Initialization for Complex Dynamic Environments](https://arxiv.org/abs/2601.05368)
*Svitlana Morkva,Maximum Wilder-Smith,Michael Oechsle,Alessio Tonioni,Marco Hutter,Vaishakh Patil*

Main category: cs.CV

> MOSAIC-GS uses Gaussian Splatting to reconstruct dynamic scenes from monocular videos, showing improved optimization and rendering speeds while maintaining high-quality reconstruction.

<details>
  <summary>Details</summary>

**Motivation:** The main motivation is to address the challenges in monocular reconstruction, such as the lack of sufficient multiview constraints and the difficulty in recovering object geometry and temporal coherence accurately.

**Method:** The paper introduces MOSAIC-GS, which uses Gaussian Splatting for high-fidelity dynamic scene reconstruction from monocular videos. It employs multiple geometric cues and rigidity-based constraints to estimate preliminary 3D scene dynamics and then decomposes the scene into static and dynamic components to support non-rigid deformations efficiently.

**Result:** The method achieves faster optimization and rendering compared to existing techniques while maintaining high reconstruction quality.

**Conclusion:** MOSAIC-GS is effective in providing a computationally efficient approach for dynamic scene reconstruction from monocular videos, demonstrating better performance in optimization and rendering speed without compromising the quality of the reconstruction.

**Abstract:** We present MOSAIC-GS, a novel, fully explicit, and computationally efficient approach for high-fidelity dynamic scene reconstruction from monocular videos using Gaussian Splatting. Monocular reconstruction is inherently ill-posed due to the lack of sufficient multiview constraints, making accurate recovery of object geometry and temporal coherence particularly challenging. To address this, we leverage multiple geometric cues, such as depth, optical flow, dynamic object segmentation, and point tracking. Combined with rigidity-based motion constraints, these cues allow us to estimate preliminary 3D scene dynamics during an initialization stage. Recovering scene dynamics prior to the photometric optimization reduces reliance on motion inference from visual appearance alone, which is often ambiguous in monocular settings. To enable compact representations, fast training, and real-time rendering while supporting non-rigid deformations, the scene is decomposed into static and dynamic components. Each Gaussian in the dynamic part of the scene is assigned a trajectory represented as time-dependent Poly-Fourier curve for parameter-efficient motion encoding. We demonstrate that MOSAIC-GS achieves substantially faster optimization and rendering compared to existing methods, while maintaining reconstruction quality on par with state-of-the-art approaches across standard monocular dynamic scene benchmarks.

</details>


### [12] [Ensemble of radiomics and ConvNeXt for breast cancer diagnosis](https://arxiv.org/abs/2601.05373)
*Jorge Alberto Garza-Abdala,Gerardo Alejandro Fumagal-González,Beatriz A. Bosques-Palomo,Mario Alexis Monsivais Molina,Daly Avedano,Servando Cardona-Huerta,José Gerardo Tamez-Pena*

Main category: cs.CV

> 该研究评估了放射组学、深度学习和集成方法在乳腺癌早期检测中的性能。结果表明，集成方法的AUC最高，为0.87，优于单独使用深度学习（0.83）或放射组学（0.80）。

<details>
  <summary>Details</summary>

**Motivation:** 早期诊断乳腺癌对提高生存率至关重要。放射组学和深度学习在辅助医生早期发现乳腺癌方面显示出巨大潜力。本研究旨在评估这些技术在乳腺癌早期检测中的实用性。

**Method:** 该研究使用了两个独立的数据集，包括RSNA 2023乳腺癌检测挑战赛数据集和墨西哥TecSalud数据集。ConvNeXtV1-small深度学习模型在RSNA数据集上训练并在TecSalud数据集上验证，而放射组学模型在TecSalud数据集上开发并使用留一法验证。集成方法则使用相同的方法整合和校准预测结果。

**Result:** 结果显示，集成方法在AUC上达到最高值0.87，对比深度学习模型ConvNeXtV1-small达到0.83，放射组学模型达到0.80。

**Conclusion:** 集成方法结合深度学习和放射组学预测，显著提高了从乳腺X光片诊断乳腺癌的能力。

**Abstract:** Early diagnosis of breast cancer is crucial for improving survival rates. Radiomics and deep learning (DL) have shown significant potential in assisting radiologists with early cancer detection. This paper aims to critically assess the performance of radiomics, DL, and ensemble techniques in detecting cancer from screening mammograms. Two independent datasets were used: the RSNA 2023 Breast Cancer Detection Challenge (11,913 patients) and a Mexican cohort from the TecSalud dataset (19,400 patients). The ConvNeXtV1-small DL model was trained on the RSNA dataset and validated on the TecSalud dataset, while radiomics models were developed using the TecSalud dataset and validated with a leave-one-year-out approach. The ensemble method consistently combined and calibrated predictions using the same methodology. Results showed that the ensemble approach achieved the highest area under the curve (AUC) of 0.87, compared to 0.83 for ConvNeXtV1-small and 0.80 for radiomics. In conclusion, ensemble methods combining DL and radiomics predictions significantly enhance breast cancer diagnosis from mammograms.

</details>


### [13] [EdgeLDR: Quaternion Low-Displacement Rank Neural Networks for Edge-Efficient Deep Learning](https://arxiv.org/abs/2601.05379)
*Vladimir Frants,Sos Agaian,Karen Panetta*

Main category: cs.CV

> 该论文提出了EdgeLDR框架，结合四元数神经网络和分块循环结构，通过FFT实现显著加速和稳定延迟，适用于边缘设备的紧凑CNN和Transformer模型。

<details>
  <summary>Details</summary>

**Motivation:** 部署深度神经网络在边缘设备时，依赖于密集线性算子的内存交通和计算成本会导致限制。尽管四元数神经网络通过Hamilton乘积提高了参数效率，但它们通常保留了非结构化的密集权重；而结构化矩阵使得快速计算成为可能，但是通常在实数域应用。

**Method:** 介绍了一种名为EdgeLDR的框架，结合了四元数通道混合和分块循环参数结构，通过复数伴随表示实现了基于FFT的快速计算。

**Result:** FFT计算相对于朴素的空间领域实现提供了显著的加速，且随着分块大小的增加保持了稳定的延迟。将EdgeLDR层集成到紧凑的CNN和Transformer模型中，评估了参数压缩与精度之间的权衡，结果显示EdgeLDR层在显著压缩参数的同时保持了竞争力的精度。

**Conclusion:** 研究结论表明，EdgeLDR框架有助于在边缘设备上实现高效的四元数神经网络，特别是在压缩参数的同时保持了高精度，有效地结合了快速计算和有效压缩的优势。

**Abstract:** Deploying deep neural networks on edge devices is often limited by the memory traffic and compute cost of dense linear operators. While quaternion neural networks improve parameter efficiency by coupling multiple channels through Hamilton products, they typically retain unstructured dense weights; conversely, structured matrices enable fast computation but are usually applied in the real domain. This paper introduces EdgeLDR, a practical framework for quaternion block-circulant linear and convolutional layers that combines quaternion channel mixing with block-circulant parameter structure and enables FFT-based evaluation through the complex adjoint representation. We present reference implementations of EdgeLDR layers and compare FFT-based computation against a naive spatial-domain realization of quaternion circulant products. FFT evaluation yields large empirical speedups over the naive implementation and keeps latency stable as block size increases, making larger compression factors computationally viable. We further integrate EdgeLDR layers into compact CNN and Transformer backbones and evaluate accuracy-compression trade-offs on 32x32 RGB classification (CIFAR-10/100, SVHN) and hyperspectral image classification (Houston 2013, Pavia University), reporting parameter counts and CPU/GPU latency. The results show that EdgeLDR layers provide significant compression with competitive accuracy.

</details>


### [14] [Sketch&Patch++: Efficient Structure-Aware 3D Gaussian Representation](https://arxiv.org/abs/2601.05394)
*Yuang Shi,Simone Gasparini,Géraldine Morin,Wei Tsang Ooi*

Main category: cs.CV

> 本文提出了新的3D场景表示方法，利用Gaussians的分层分类实现了高质量的渲染，适用于多种设备。

<details>
  <summary>Details</summary>

**Motivation:** 针对现有方法在处理3D场景时的不足，提出了一种新的3D场景表示方法，旨在提高渲染效率与质量。

**Method:** 通过将高斯函数分类为Sketch和Patch，实现了结构与细节分离的3D场景表示，并结合了多标准密度聚类和自适应质量优化。

**Result:** {
  "tldr": "本文提出了一种基于Gaussians分层适应性分类框架的3D场景表示方法，适用于任意3D场景，并在质量和压缩率上优于均匀剪枝基准。", 
  "motivation": "为了更好地处理3D场景，实现高效的存储、适应性流媒体传输和低资源设备上的高质量渲染。", 
  "method": "引入了Sketch Gaussians和Patch Gaussians的分类，通过多标准密度聚类和自适应质量驱动的优化方法，形成了结构感知的表示方法。", 
  "result": "方法在多种场景下均有优秀的性能表现，相比均匀剪枝基准，PSNR提升了1.74 dB，SSIM提升了6.7%，LPIPS提升了41.4%，室内场景可以用0.5%的模型大小保持相同的视觉质量。", 
  "conclusion": "该表示方法不仅提供了高质量的3D场景渲染，还增强了在带宽受限网络和资源受限设备上的适应性和高效性。"}
}

**Conclusion:** 本文提出的方法显著提升了3D场景表示的效率和质量，特别适用于带宽有限的网络和资源受限的设备。

**Abstract:** We observe that Gaussians exhibit distinct roles and characteristics analogous to traditional artistic techniques -- like how artists first sketch outlines before filling in broader areas with color, some Gaussians capture high-frequency features such as edges and contours, while others represent broader, smoother regions analogous to brush strokes that add volume and depth. Based on this observation, we propose a hybrid representation that categorizes Gaussians into (i) Sketch Gaussians, which represent high-frequency, boundary-defining features, and (ii) Patch Gaussians, which cover low-frequency, smooth regions. This semantic separation naturally enables layered progressive streaming, where the compact Sketch Gaussians establish the structural skeleton before Patch Gaussians incrementally refine volumetric detail.
  In this work, we extend our previous method to arbitrary 3D scenes by proposing a novel hierarchical adaptive categorization framework that operates directly on the 3DGS representation. Our approach employs multi-criteria density-based clustering, combined with adaptive quality-driven refinement. This method eliminates dependency on external 3D line primitives while ensuring optimal parametric encoding effectiveness. Our comprehensive evaluation across diverse scenes, including both man-made and natural environments, demonstrates that our method achieves up to 1.74 dB improvement in PSNR, 6.7% in SSIM, and 41.4% in LPIPS at equivalent model sizes compared to uniform pruning baselines. For indoor scenes, our method can maintain visual quality with only 0.5\% of the original model size. This structure-aware representation enables efficient storage, adaptive streaming, and rendering of high-fidelity 3D content across bandwidth-constrained networks and resource-limited devices.

</details>


### [15] [Multi-task Cross-modal Learning for Chest X-ray Image Retrieval](https://arxiv.org/abs/2601.05399)
*Zhaohui Liang,Sivaramakrishnan Rajaraman,Niccolo Marini,Zhiyun Xue,Sameer Antani*

Main category: cs.CV

> 研究中提出了一个多任务学习框架来微调BiomedCLIP，以优化胸部X光图像和文本之间的检索任务。实验表明该方法提升了临床相关性和诊断敏感性。

<details>
  <summary>Details</summary>

**Motivation:** 虽然CLIP和BiomedCLIP提供了强大的跨模态嵌入，但它们并未针对如利用胸部X光图像检索相关放射报告等细粒度的医疗检索任务进行优化。因此，本研究旨在改进它们在这类任务上的表现。

**Method:** 本研究提出了一个基于BiomedCLIP的多任务学习框架，通过加入轻量级的MLP投影头，并使用多任务复合损失函数进行微调。该损失函数包括：1）用于区分正常和异常胸部X光图像的二元交叉熵损失，2）用于增强类内一致性的监督对比损失，3）用于保持跨模态对齐的CLIP损失。以此来提高利用胸部X光图像检索相关放射报告的性能。

**Result:** 实验结果表明，微调后的模型在图像到文本和文本到图像检索任务中均表现出更均衡且更具临床意义的性能，优于预训练的BiomedCLIP和通用的CLIP模型。t-SNE可视化显示正常和异常病例的语义聚类更加清晰，表明模型的诊断敏感性有所提高。

**Conclusion:** 研究表明，领域自适应的多任务学习方法能够在跨模态检索的生物医学应用中带来显著的提升。这强调了在生物医学应用中融合多任务学习对于提高模型性能的重要价值。

**Abstract:** CLIP and BiomedCLIP are examples of vision-language foundation models and offer strong cross-modal embeddings; however, they are not optimized for fine-grained medical retrieval tasks, such as retrieving clinically relevant radiology reports using chest X-ray (CXR) image queries. To address this shortcoming, we propose a multi-task learning framework to fine-tune BiomedCLIP and evaluate improvements to CXR image-text retrieval. Using BiomedCLIP as the backbone, we incorporate a lightweight MLP projector head trained with a multi-task composite loss function that includes: (1) a binary cross-entropy loss to distinguish normal from abnormal CXR studies, (2) a supervised contrastive loss to reinforce intra-class consistency, and (3) a CLIP loss to maintain cross-modal alignment. Experimental results demonstrate that the fine-tuned model achieves more balanced and clinically meaningful performance across both image-to-text and text-to-image retrieval tasks compared to the pretrained BiomedCLIP and general-purpose CLIP models. Furthermore, t-SNE visualizations reveal clearer semantic clustering of normal and abnormal cases, demonstrating the model's enhanced diagnostic sensitivity. These findings highlight the value of domain-adaptive, multi-task learning for advancing cross-modal retrieval in biomedical applications.

</details>


### [16] [Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization](https://arxiv.org/abs/2601.05432)
*Yuxiang Ji,Yong Wang,Ziyu Ma,Yiming Hu,Hailang Huang,Xuecai Hu,Guanhua Chen,Liaoni Wu,Xiangxiang Chu*

Main category: cs.CV

> The paper presents a method that equips vision-language models with map-utilizing capabilities for image geolocalization, surpassing existing models with a significant improvement in accuracy.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the accuracy of image geolocalization by incorporating map usage, a common human strategy that is not sufficiently addressed in existing large vision-language models, as well as to evaluate new models using real-world images.

**Method:** The paper introduces an approach called Thinking with Map, which empowers a vision-language model with the ability to use maps for image geolocalization. It employs a two-stage optimization scheme involving agentic reinforcement learning (RL) to enhance sampling efficiency and parallel test-time scaling (TTS) to explore multiple potential paths before predicting the final location.

**Result:** The proposed method outperforms both open-source and closed-source models on most metrics, with a notable increase in Acc@500m from 8.0% to 22.1% compared to Gemini-3-Pro using Google Search/Map.

**Conclusion:** The inclusion of map utilization as a model ability, along with the development of a two-stage optimization scheme, significantly enhances geolocalization performance and offers a new benchmark dataset for realistic model evaluation.

**Abstract:** The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model \textit{Thinking with Map} ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\% to 22.1\% compared to \textit{Gemini-3-Pro} with Google Search/Map grounded mode.

</details>


### [17] [TAPM-Net: Trajectory-Aware Perturbation Modeling for Infrared Small Target Detection](https://arxiv.org/abs/2601.05446)
*Hongyang Xie,Hongyang He,Victor Sanchez*

Main category: cs.CV

> TAPM-Net is introduced for infrared small target detection, outperforming existing methods on NUAA-SIRST and IRSTD-1K datasets.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind TAPM-Net is to improve infrared small target detection by explicitly modeling the spatial diffusion behavior of targets to distinguish signal from structured noise.

**Method:** TAPM-Net uses a Perturbation-guided Path Module (PGM) and a Trajectory-Aware State Block (TASB) for infrared small target detection. PGM extracts feature trajectories which are then processed by the TASB to model dynamic propagation while incorporating velocity-constrained diffusion and semantically aligned feature fusion.

**Result:** TAPM-Net achieves state-of-the-art performance in ISTD on the NUAA-SIRST and IRSTD-1K datasets.

**Conclusion:** TAPM-Net effectively models the spatial diffusion behavior of target-induced feature disturbances, which boosts infrared small target detection performance.

**Abstract:** Infrared small target detection (ISTD) remains a long-standing challenge due to weak signal contrast, limited spatial extent, and cluttered backgrounds. Despite performance improvements from convolutional neural networks (CNNs) and Vision Transformers (ViTs), current models lack a mechanism to trace how small targets trigger directional, layer-wise perturbations in the feature space, which is an essential cue for distinguishing signal from structured noise in infrared scenes. To address this limitation, we propose the Trajectory-Aware Mamba Propagation Network (TAPM-Net), which explicitly models the spatial diffusion behavior of target-induced feature disturbances. TAPM-Net is built upon two novel components: a Perturbation-guided Path Module (PGM) and a Trajectory-Aware State Block (TASB). The PGM constructs perturbation energy fields from multi-level features and extracts gradient-following feature trajectories that reflect the directionality of local responses. The resulting feature trajectories are fed into the TASB, a Mamba-based state-space unit that models dynamic propagation along each trajectory while incorporating velocity-constrained diffusion and semantically aligned feature fusion from word-level and sentence-level embeddings. Unlike existing attention-based methods, TAPM-Net enables anisotropic, context-sensitive state transitions along spatial trajectories while maintaining global coherence at low computational cost. Experiments on NUAA-SIRST and IRSTD-1K demonstrate that TAPM-Net achieves state-of-the-art performance in ISTD.

</details>


### [18] [ROAP: A Reading-Order and Attention-Prior Pipeline for Optimizing Layout Transformers in Key Information Extraction](https://arxiv.org/abs/2601.05470)
*Tingwei Xie,Jinxin He,Yonghong Song*

Main category: cs.CV

> ROAP, a lightweight pipeline, addresses the limitations of Multimodal Transformers in visually-rich document understanding by explicitly modeling reading order and reducing visual noise, showing significant performance improvements across benchmarks.

<details>
  <summary>Details</summary>

**Motivation:** The motivation stems from the need to address the limitations in Multimodal Transformers, such as the lack of explicit modeling for logical reading order and the interference of visual tokens that affect attention on textual semantics.

**Method:** The paper proposes ROAP, a lightweight pipeline that optimizes attention distributions in Layout Transformers. It introduces the AXG-Tree for extracting hierarchical reading sequences, the RO-RPB for integrating reading order information into attention mechanisms, and the TT-Prior for suppressing visual noise and enhancing text interactions.

**Result:** Experiments on FUNSD and CORD benchmarks show that ROAP significantly boosts the performance of LayoutLMv3 and GeoLayoutLM, demonstrating its effectiveness in enhancing document understanding.

**Conclusion:** The conclusion is that explicitly modeling reading logic and regulating modality interference are crucial for robust document understanding, and ROAP presents a scalable solution for complex layout analysis.

**Abstract:** The efficacy of Multimodal Transformers in visually-rich document understanding (VrDU) is critically constrained by two inherent limitations: the lack of explicit modeling for logical reading order and the interference of visual tokens that dilutes attention on textual semantics.
  To address these challenges, this paper presents ROAP, a lightweight and architecture-agnostic pipeline designed to optimize attention distributions in Layout Transformers without altering their pre-trained backbones.
  The proposed pipeline first employs an Adaptive-XY-Gap (AXG-Tree) to robustly extract hierarchical reading sequences from complex layouts. These sequences are then integrated into the attention mechanism via a Reading-Order-Aware Relative Position Bias (RO-RPB). Furthermore, a Textual-Token Sub-block Attention Prior (TT-Prior) is introduced to adaptively suppress visual noise and enhance fine-grained text-text interactions.
  Extensive experiments on the FUNSD and CORD benchmarks demonstrate that ROAP consistently improves the performance of representative backbones, including LayoutLMv3 and GeoLayoutLM.
  These findings confirm that explicitly modeling reading logic and regulating modality interference are critical for robust document understanding, offering a scalable solution for complex layout analysis. The implementation code will be released at https://github.com/KevinYuLei/ROAP.

</details>
