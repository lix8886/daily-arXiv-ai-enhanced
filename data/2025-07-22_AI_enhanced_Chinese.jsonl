{"id": "2507.14268", "categories": ["cs.CV", "cond-mat.mtrl-sci", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.14268", "abs": "https://arxiv.org/abs/2507.14268", "authors": ["Andreas Alpers", "Orkun Furat", "Christian Jung", "Matthias Neumann", "Claudia Redenbach", "Aigerim Saken", "Volker Schmidt"], "title": "Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data", "comment": "31 pages, 16 figures, 8 tables", "summary": "This paper presents a comparative analysis of algorithmic strategies for\nfitting tessellation models to 3D image data of materials such as polycrystals\nand foams. In this steadily advancing field, we review and assess\noptimization-based methods -- including linear and nonlinear programming,\nstochastic optimization via the cross-entropy method, and gradient descent --\nfor generating Voronoi, Laguerre, and generalized balanced power diagrams\n(GBPDs) that approximate voxelbased grain structures. The quality of fit is\nevaluated on real-world datasets using discrepancy measures that quantify\ndifferences in grain volume, surface area, and topology. Our results highlight\ntrade-offs between model complexity, the complexity of the optimization\nroutines involved, and the quality of approximation, providing guidance for\nselecting appropriate methods based on data characteristics and application\nneeds.", "AI": {"tldr": "A comparative analysis of optimization-based strategies for fitting tessellation models to 3D materials, revealing trade-offs between model & optimization complexity and approximation quality.", "motivation": "The motivation is to review and assess various algorithmic strategies for fitting tessellation models to 3D image data of materials like polycrystals and foams, to provide guidance on method selection based on data characteristics and application needs.", "method": "The paper uses optimization-based methods, including linear and nonlinear programming, stochastic optimization with the cross-entropy method, and gradient descent, to generate Voronoi, Laguerre, and generalized balanced power diagrams (GBPDs) approximating voxel-based grain structures.", "result": "The comparative analysis evaluates the quality of fit using discrepancy measures that assess differences in grain volume, surface area, and topology on real-world datasets, revealing trade-offs between model complexity, optimization routine complexity, and approximation quality.", "conclusion": "The study highlights that while more complex models and optimization routines offer improved approximation of grain structures, these come at the cost of increased computational difficulty, providing insights for selecting suitable methods for specific applications."}}
{"id": "2507.14303", "categories": ["cs.CV", "I.4.8"], "pdf": "https://arxiv.org/pdf/2507.14303", "abs": "https://arxiv.org/abs/2507.14303", "authors": ["Ehsan Rassekh"], "title": "Semantic Segmentation based Scene Understanding in Autonomous Vehicles", "comment": "74 pages, 35 figures, Master's Thesis, Institute for Advanced Studies\n  in Basic Sciences (IASBS), Zanjan, Iran, 2023", "summary": "In recent years, the concept of artificial intelligence (AI) has become a\nprominent keyword because it is promising in solving complex tasks. The need\nfor human expertise in specific areas may no longer be needed because machines\nhave achieved successful results using artificial intelligence and can make the\nright decisions in critical situations. This process is possible with the help\nof deep learning (DL), one of the most popular artificial intelligence\ntechnologies. One of the areas in which the use of DL is used is in the\ndevelopment of self-driving cars, which is very effective and important. In\nthis work, we propose several efficient models to investigate scene\nunderstanding through semantic segmentation. We use the BDD100k dataset to\ninvestigate these models. Another contribution of this work is the usage of\nseveral Backbones as encoders for models. The obtained results show that\nchoosing the appropriate backbone has a great effect on the performance of the\nmodel for semantic segmentation. Better performance in semantic segmentation\nallows us to understand better the scene and the environment around the agent.\nIn the end, we analyze and evaluate the proposed models in terms of accuracy,\nmean IoU, and loss function, and the results show that these metrics are\nimproved.", "AI": {"tldr": "本文通过研究几种不同的模型并使用各种骨干网络作为编码器，证明了选择合适的骨干模型可以显著提升场景语义分割的性能。", "motivation": "本研究的动力在于探究如何通过深度学习技术提高自驾车的场景理解能力，特别是通过语义分割技术来更好地理解周围环境。", "method": "本研究的方法是使用BDD100k数据集来研究几种有效的模型，通过语义分割来探究场景理解，并使用了若干骨干模型作为编码器。", "result": "实验结果表明，选择合适的骨干模型对语义分割模型的性能有显著影响。", "conclusion": "通过对准确率、平均IoU以及损失函数的评估，研究证实提出的模型在语义分割上的性能得到了提升。"}}
{"id": "2507.14312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14312", "abs": "https://arxiv.org/abs/2507.14312", "authors": ["Marc Lafon", "Gustavo Adolfo Vargas Hakim", "Clément Rambour", "Christian Desrosier", "Nicolas Thome"], "title": "CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation", "comment": null, "summary": "Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities\nbut often fail to generalize under distribution shifts. Test-time adaptation\n(TTA) allows models to update at inference time without labeled data, typically\nvia entropy minimization. However, this objective is fundamentally misaligned\nwith the contrastive image-text training of VLMs, limiting adaptation\nperformance and introducing failure modes such as pseudo-label drift and class\ncollapse. We propose CLIPTTA, a new gradient-based TTA method for\nvision-language models that leverages a soft contrastive loss aligned with\nCLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's\ngradients, showing how its batch-aware design mitigates the risk of collapse.\nWe further extend CLIPTTA to the open-set setting, where both in-distribution\n(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier\nContrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75\ndatasets spanning diverse distribution shifts, CLIPTTA consistently outperforms\nentropy-based objectives and is highly competitive with state-of-the-art TTA\nmethods, outperforming them on a large number of datasets and exhibiting more\nstable performance across diverse shifts.", "AI": {"tldr": "本文提出了CLIPTTA，一种新的基于梯度的测试时刻适应方法，提高了视觉-语言模型在不同分布变化下的泛化能力，表现优于熵基方法，并在大量数据集上展示了更强的竞争性和稳定性。", "motivation": "视觉-语言模型（VLMs）如CLIP在零样本设置中表现强劲，但在分布变化时却往往无法很好地泛化。现有的基于熵最小化的测试时刻适应（TTA）方法与其预训练目标不一致，限制了适应性能并导致了一些失败模式。", "method": "CLIPTTA采用了一种新的基于梯度的测试时刻适应方法，通过利用与CLIP预训练目标对齐的软对比损失来改进视觉-语言模型的适应性能，并且在开放集设置中使用异常对比曝光（OCE）损失来改进未分布数据（OOD）的检测能力。", "result": "CLIPTTA在涵盖了75个不同分布偏移的数据集上测试，相对于熵基目标和其他最先进的测试时刻适应方法表现出了更好的性能和稳定性。", "conclusion": "CLIPTTA证明了其在处理分布偏移问题上的有效性和适应性，尤其是在引入了OCE损失后的开放集适应情况下的OOD检测性能。"}}
{"id": "2507.14315", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14315", "abs": "https://arxiv.org/abs/2507.14315", "authors": ["Qiyu Xu", "Zhanxuan Hu", "Yu Duan", "Ercheng Pei", "Yonghang Tai"], "title": "A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention", "comment": null, "summary": "Generalized Category Discovery (GCD) aims to classify unlabeled data from\nboth known and unknown categories by leveraging knowledge from labeled known\ncategories. While existing methods have made notable progress, they often\noverlook a hidden stumbling block in GCD: distracted attention. Specifically,\nwhen processing unlabeled data, models tend to focus not only on key objects in\nthe image but also on task-irrelevant background regions, leading to suboptimal\nfeature extraction. To remove this stumbling block, we propose Attention\nFocusing (AF), an adaptive mechanism designed to sharpen the model's focus by\npruning non-informative tokens. AF consists of two simple yet effective\ncomponents: Token Importance Measurement (TIME) and Token Adaptive Pruning\n(TAP), working in a cascade. TIME quantifies token importance across multiple\nscales, while TAP prunes non-informative tokens by utilizing the multi-scale\nimportance scores provided by TIME. AF is a lightweight, plug-and-play module\nthat integrates seamlessly into existing GCD methods with minimal computational\noverhead. When incorporated into one prominent GCD method, SimGCD, AF achieves\nup to 15.4% performance improvement over the baseline with minimal\ncomputational overhead. The implementation code is provided in\nhttps://github.com/Afleve/AFGCD.", "AI": {"tldr": "This paper proposes a novel method, AF, to improve GCD by focusing the model's attention on informative tokens rather than task-irrelevant backgrounds, achieving a 15.4% performance improvement with minimal computational overhead.", "motivation": "The motivation behind this work is to address a hidden stumbling block in GCD: distracted attention. Existing methods often have their models focus on task-irrelevant areas, leading to inefficient feature extraction.", "method": "The paper proposes an adaptive mechanism named Attention Focusing (AF) to improve Generalized Category Discovery (GCD). AF includes two components: TIME and TAP. TIME evaluates the importance of tokens at multiple scales, while TAP prunes non-informative tokens based on these evaluations.", "result": "When the proposed AF mechanism is added to the existing GCD method SimGCD, the results show a performance improvement of up to 15.4%, demonstrating the effectiveness of AF in enhancing GCD.", "conclusion": "The paper concludes that AF is a lightweight and effective addition to GCD methods, significantly improving classification performance by reducing the effect of task-irrelevant attention, without substantial additional computational costs."}}
{"id": "2507.14189", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14189", "abs": "https://arxiv.org/abs/2507.14189", "authors": ["Song Mao", "Lejun Cheng", "Pinlong Cai", "Guohang Yan", "Ding Wang", "Botian Shi"], "title": "DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base", "comment": "work in process", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious applications. However, their use as writing assistants in specialized\ndomains like finance, medicine, and law is often hampered by a lack of deep\ndomain-specific knowledge and a tendency to hallucinate. Existing solutions,\nsuch as Retrieval-Augmented Generation (RAG), can suffer from inconsistency\nacross multiple retrieval steps, while online search-based methods often\ndegrade quality due to unreliable web content. To address these challenges, we\nintroduce DeepWriter, a customizable, multimodal, long-form writing assistant\nthat operates on a curated, offline knowledge base. DeepWriter leverages a\nnovel pipeline that involves task decomposition, outline generation, multimodal\nretrieval, and section-by-section composition with reflection. By deeply mining\ninformation from a structured corpus and incorporating both textual and visual\nelements, DeepWriter generates coherent, factually grounded, and\nprofessional-grade documents. We also propose a hierarchical knowledge\nrepresentation to enhance retrieval efficiency and accuracy. Our experiments on\nfinancial report generation demonstrate that DeepWriter produces high-quality,\nverifiable articles that surpasses existing baselines in factual accuracy and\ngenerated content quality.", "AI": {"tldr": "本文介绍了DeepWriter，一种可以解决大型语言模型在特定领域如金融、医疗和法律作为写作助手时遇到的问题的定制化、多模态、长文档写作助手。通过结合文本和视觉元素生成连贯、事实依据充分的文档，并通过分层知识表示提高检索效率和准确性。实验表明，DeepWriter生成的金融报告优于现有的基线方法。", "motivation": "大型语言模型虽然在各种应用中表现卓越，但在特定领域如金融、医疗和法律等作为写作助手时往往会受到缺乏深厚专业知识和倾向于产生虚假信息的困扰。现有解决方案如检索增强生成（RAG）在多次检索过程中可能存在一致性问题，而基于在线搜索的方法往往因为网络内容的不可靠性而导致质量下降。", "method": "DeepWriter采用了一个新颖的流水线，主要包括任务分解、大纲生成、多模态检索和分段写作反思。通过深入挖掘结构化语料库中的信息并结合文本和视觉元素，DeepWriter能够生成连贯、事实依据充分且专业性高的文档。同时，提出了一种分层知识表示方法来提高检索效率和准确性。", "result": "实验结果显示，DeepWriter生成的金融报告质量高且可以验证，其在事实准确性和生成内容质量方面超越了现有的基线方法。", "conclusion": "通过采用定制化、多模态、长文档写作助手DeepWriter，在线外知识库的操作方式，研究解决了现有大型语言模型在特定领域应用中的挑战，并实验证明了其生成的文档在质量上的优势。"}}
{"id": "2507.14367", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14367", "abs": "https://arxiv.org/abs/2507.14367", "authors": ["Weiming Ren", "Raghav Goyal", "Zhiming Hu", "Tristan Ty Aumentado-Armstrong", "Iqbal Mohomed", "Alex Levinshtein"], "title": "Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution", "comment": "12 pages, 17 figures and 7 tables", "summary": "Generative super-resolution (GSR) currently sets the state-of-the-art in\nterms of perceptual image quality, overcoming the \"regression-to-the-mean\" blur\nof prior non-generative models. However, from a human perspective, such models\ndo not fully conform to the optimal balance between quality and fidelity.\nInstead, a different class of artifacts, in which generated details fail to\nperceptually match the low resolution image (LRI) or ground-truth image (GTI),\nis a critical but under studied issue in GSR, limiting its practical\ndeployments. In this work, we focus on measuring, analyzing, and mitigating\nthese artifacts (i.e., \"hallucinations\"). We observe that hallucinations are\nnot well-characterized with existing image metrics or quality models, as they\nare orthogonal to both exact fidelity and no-reference quality. Instead, we\ntake advantage of a multimodal large language model (MLLM) by constructing a\nprompt that assesses hallucinatory visual elements and generates a\n\"Hallucination Score\" (HS). We find that our HS is closely aligned with human\nevaluations, and also provides complementary insights to prior image metrics\nused for super-resolution (SR) models. In addition, we find certain deep\nfeature distances have strong correlations with HS. We therefore propose to\nalign the GSR models by using such features as differentiable reward functions\nto mitigate hallucinations.", "AI": {"tldr": "本研究提出使用大型语言模型评估幻觉现象，并提出使用深层特征距离作为奖励函数来减少生成超级分辨率模型中的幻觉，提高模型的实际应用价值。", "motivation": "现有的生成超级分辨率（GSR）模型在感知图像质量方面表现优异，但生成的细节与低分辨率图象（LRI）或地面真实图象（GTI）在感知上往往不匹配，这种“幻觉”现象限制了GSR模型的实际部署。", "method": "通过利用多模态大型语言模型（MLLM）构建一个评估幻觉视觉元素的提示，生成‘幻觉分数’（HS），并提出使用某些深层特征距离作为可微奖励函数来对齐生成超级分辨率模型以减少幻觉现象。", "result": "研究发现，提出的幻觉分数（HS）与人类评估密切相关，并为之前用于超分辨率（SR）模型的图像度量提供了补充见解。此外，研究发现某些深层特征距离与HS有很强的相关性，可以用来减少幻觉现象。", "conclusion": "通过引入幻觉分数（HS）和使用深层特征距离作为可微奖励函数，本研究为减少生成超级分辨率模型中的幻觉现象提供了新的解决思路。"}}
{"id": "2507.14198", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14198", "abs": "https://arxiv.org/abs/2507.14198", "authors": ["Fufang Wen", "Shichang Zhang"], "title": "Retention analysis of edited knowledge after fine-tuning", "comment": null, "summary": "Large language models (LLMs) store vast amounts of knowledge, which often\nrequires updates to correct factual errors, incorporate newly acquired\ninformation, or adapt model behavior. Model editing methods have emerged as\nefficient solutions for such updates, offering localized and precise knowledge\nmodification at significantly lower computational cost than continual training.\nIn parallel, LLMs are frequently fine-tuned for a wide range of downstream\ntasks. However, the effect of fine-tuning on previously edited knowledge\nremains poorly understood. In this work, we systematically investigate how\ndifferent fine-tuning objectives interact with various model editing\ntechniques. Our findings show that edited knowledge is substantially more\nsusceptible to forgetting during fine-tuning than intrinsic knowledge acquired\nthrough pre-training. This analysis highlights a key limitation of current\nediting approaches and suggests that evaluating edit robustness under\ndownstream fine-tuning is critical for their practical deployment. We further\nfind that freezing layers associated with edited content can significantly\nimprove knowledge retention, offering insight into how future editing methods\nmight be made more robust.", "AI": {"tldr": "研究探讨了微调对LLMs编辑知识的影响，发现编辑知识容易在微调中被遗忘，而冻结特定层能提高知识保留。这个发现对未来的编辑方法稳健性提升具有重要的指导意义。", "motivation": "目前对微调对先前编辑知识的影响了解不足。研究这个领域可以揭示当前编辑方法的关键局限性，并指出评估编辑的稳健性在下游任务微调下的重要性。", "method": "此研究系统性地探讨了不同的微调目标与各种模型编辑技术之间的交互作用。", "result": "研究发现，编辑后的知识在微调过程中比模型预训练获得的内在知识更容易遗忘。冻结与编辑内容相关联的层可以显著提高知识保留。", "conclusion": "编辑的知识在微调过程中比模型预训练获得的内在知识更容易遗忘。冻结与编辑内容相关联的层可以显著提高知识保留，这为未来编辑方法的稳健性提供了见解。"}}
{"id": "2507.14368", "categories": ["cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.14368", "abs": "https://arxiv.org/abs/2507.14368", "authors": ["Praneeth Namburi", "Roger Pallarès-López", "Jessica Rosendorf", "Duarte Folgado", "Brian W. Anthony"], "title": "DUSTrack: Semi-automated point tracking in ultrasound videos", "comment": null, "summary": "Ultrasound technology enables safe, non-invasive imaging of dynamic tissue\nbehavior, making it a valuable tool in medicine, biomechanics, and sports\nscience. However, accurately tracking tissue motion in B-mode ultrasound\nremains challenging due to speckle noise, low edge contrast, and out-of-plane\nmovement. These challenges complicate the task of tracking anatomical landmarks\nover time, which is essential for quantifying tissue dynamics in many clinical\nand research applications. This manuscript introduces DUSTrack (Deep learning\nand optical flow-based toolkit for UltraSound Tracking), a semi-automated\nframework for tracking arbitrary points in B-mode ultrasound videos. We combine\ndeep learning with optical flow to deliver high-quality and robust tracking\nacross diverse anatomical structures and motion patterns. The toolkit includes\na graphical user interface that streamlines the generation of high-quality\ntraining data and supports iterative model refinement. It also implements a\nnovel optical-flow-based filtering technique that reduces high-frequency\nframe-to-frame noise while preserving rapid tissue motion. DUSTrack\ndemonstrates superior accuracy compared to contemporary zero-shot point\ntrackers and performs on par with specialized methods, establishing its\npotential as a general and foundational tool for clinical and biomechanical\nresearch. We demonstrate DUSTrack's versatility through three use cases:\ncardiac wall motion tracking in echocardiograms, muscle deformation analysis\nduring reaching tasks, and fascicle tracking during ankle plantarflexion. As an\nopen-source solution, DUSTrack offers a powerful, flexible framework for point\ntracking to quantify tissue motion from ultrasound videos. DUSTrack is\navailable at https://github.com/praneethnamburi/DUSTrack.", "AI": {"tldr": "DUSTrack是一种结合深度学习和光学流方法的半自动工具，用于解决B模式超声视频中组织跟踪的问题，能够在多个应用中准确跟踪任意点。", "motivation": "为了克服B模式超声图像中的组织跟踪障碍，提高跟踪质量和鲁棒性，尤其是在研究和临床环境中量化组织动态时。", "method": "DUSTrack是一种半自动框架，结合了深度学习与光学流技术，旨在通过提供一个图形用户界面来简化训练数据的生成，并采用了新的光学流过滤技术来减少帧间噪声。", "result": "Ultrasound技术在医学、生物力学和运动科学中具有重要意义，但B模式超声图像中的组织跟踪由于斑点噪声、对比度低和面外运动等问题仍面临挑战。本文介绍了一个半自动的框架DUSTrack，它结合了深度学习和光流技术，用于B模式超声视频中的任意点跟踪。DUSTrack提供了一个图形用户界面，可以简化高质量的训练数据生成并支持模型的多次迭代改进。该工具包还包括一种新颖的光流滤波技术，可以减少高频率的帧间噪声同时保留快速的组织运动。结果显示，DUSTrack在准确性上优于当前的无样本点跟踪器，并且与专门方法持平，表明其作为临床上和生物力学研究中的通用工具具有潜力。通过三个用例展示了DUSTrack的多功能性：心超声图中的心壁运动跟踪、取物任务中的肌肉变形分析以及踝关节跖屈时的肌束追踪。作为开源解决方案，DUSTrack提供了一个强大的框架，用于从超声视频中量化组织运动。", "conclusion": "DUSTrack在准确性方面优于当前的无样本点跟踪器，并与专门方法持平，它的开源性质使其成为了量化组织运动的强大和灵活的框架，展示了其在多种应用中的潜力。"}}
{"id": "2507.14200", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14200", "abs": "https://arxiv.org/abs/2507.14200", "authors": ["Shengji Tang", "Jianjian Cao", "Weihao Lin", "Jiale Hong", "Bo Zhang", "Shuyue Hu", "Lei Bai", "Tao Chen", "Wanli Ouyang", "Peng Ye"], "title": "Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System", "comment": null, "summary": "This paper aims to demonstrate the potential and strengths of open-source\ncollectives. It leads to a promising question: Can we harness multiple\nopen-source LLMs to match or even beat the closed-source LLMs? To answer this,\nwe propose SMACS, a scalable multi-agent collaboration system (MACS) framework\nwith high performance. Specifically, for continuous integration of new LLMs and\ngeneralization to diverse questions, we first propose a Retrieval-based Prior\nSelection (RPS), which assigns a proxy performance score to each LLM to select\nthe Top-k LLMs at the instance level for any given question. Then, we propose\nan Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the\ngeneration of diverse responses through prior dropping and selecting the\nhigh-quality response via a hybrid posterior score. Experiments on eight\nmainstream benchmarks validate the effectiveness of our SMACS: by integrating\nfifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,\ne.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)\nacross multiple tasks. Remarkably, it even exceeds the average of best results\nof different datasets from both open-source LLMs (+2.86%) and closed-source\nLLMs (+2.04%), pushing the upper bound of intelligence. Code will be released\nat https://github.com/magent4aci/SMACS.", "AI": {"tldr": "本论文提出了SMACS框架，通过整合多个开源LLM和使用新的选择和后处理机制来超越闭源LLM的性能。实验结果表明，SMACS在多个基准测试中超越了领先的闭源LLM。", "motivation": "探索是否能利用多个开源LLM来匹配或超越闭源LLM的性能。", "method": "提出SMACS框架，包括基于检索的前向选择（RPS）和探索与利用驱动的后向增强（EPE）。", "result": "在八个主流基准测试中，SMACS整合了15个开源LLM，超越了2025年的领先闭源LLM，如Claude-3.7-Sonnet、GPT-4.1和GPT-o3-mini，表现出色。", "conclusion": "SMACS框架不仅在与闭源LLM的对比中表现出优越性，还在开源和闭源LLM的最佳结果平均值上有所提升。代码将在GitHub上发布。"}}
{"id": "2507.14426", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14426", "abs": "https://arxiv.org/abs/2507.14426", "authors": ["Zhou Chen", "Joe Lin", "Sathyanarayanan N. Aakur"], "title": "CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding", "comment": "Accepted to NeSy 2025", "summary": "We introduce CRAFT, a neuro-symbolic framework for interpretable affordance\ngrounding, which identifies the objects in a scene that enable a given action\n(e.g., \"cut\"). CRAFT integrates structured commonsense priors from ConceptNet\nand language models with visual evidence from CLIP, using an energy-based\nreasoning loop to refine predictions iteratively. This process yields\ntransparent, goal-driven decisions to ground symbolic and perceptual\nstructures. Experiments in multi-object, label-free settings demonstrate that\nCRAFT enhances accuracy while improving interpretability, providing a step\ntoward robust and trustworthy scene understanding.", "AI": {"tldr": "CRAFT 是一个用于可解释的操作适用性基础的神经符号框架，它结合了常识和语言模型的信息以及视觉证据，以识别场景中能够执行特定操作的对象，并通过能量基础推理循环迭代优化预测，实现在不标记多对象环境中的高准确度和增强的可解释性。", "motivation": "研发 CRAFT 的动机在于解决场景理解中准确性和可解释性之间的平衡问题，现有方法常常缺乏透明性或准确性，无法在不标记的多对象场景中做出有效的操作适用性判断。", "method": "CRAFT 使用基于能量的推理回路整合来自 ConceptNet 和语言模型的结构化常识，与来自 CLIP 的视觉证据相融合，以优化对于给定动作适用对象的预测。", "result": "在没有标签的情况下，CRAFT 提升了多对象场景中动作适用性预测的准确性和可解释性。", "conclusion": "CRAFT 为理解复杂场景提供了一个更加健壮且值得信赖的方法，通过其透明和目标驱动的决策过程，为提炼符号与感知结构之间的联系打下了坚实的基础。"}}
{"id": "2507.14214", "categories": ["cs.CL", "cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.14214", "abs": "https://arxiv.org/abs/2507.14214", "authors": ["Rui Zhao", "Vladyslav Melnychuk", "Jun Zhao", "Jesse Wright", "Nigel Shadbolt"], "title": "Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale", "comment": null, "summary": "In modern times, people have numerous online accounts, but they rarely read\nthe Terms of Service or Privacy Policy of those sites despite claiming\notherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that\nassists users with personalized privacy policy analysis. PoliAnalyzer uses\nNatural Language Processing (NLP) to extract formal representations of data\nusage practices from policy texts. In favor of deterministic, logical inference\nis applied to compare user preferences with the formal privacy policy\nrepresentation and produce a compliance report. To achieve this, we extend an\nexisting formal Data Terms of Use policy language to model privacy policies as\napp policies and user preferences as data policies. In our evaluation using our\nenriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated\nhigh accuracy in identifying relevant data usage practices, achieving F1-score\nof 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can\nmodel diverse user data-sharing preferences, derived from prior research as 23\nuser profiles, and perform compliance analysis against the top 100 most-visited\nwebsites. This analysis revealed that, on average, 95.2% of a privacy policy's\nsegments do not conflict with the analyzed user preferences, enabling users to\nconcentrate on understanding the 4.8% (636 / 13205) that violates preferences,\nsignificantly reducing cognitive burden. Further, we identified common\npractices in privacy policies that violate user expectations - such as the\nsharing of location data with 3rd parties. This paper demonstrates that\nPoliAnalyzer can support automated personalized privacy policy analysis at\nscale using off-the-shelf NLP tools. This sheds light on a pathway to help\nindividuals regain control over their data and encourage societal discussions\non platform data practices to promote a fairer power dynamic.", "AI": {"tldr": "PoliAnalyzer是一种神经符号系统，利用NLP技术和逻辑推理来分析个性化的隐私政策并生成合规报告。实验表明，该系统在多项任务中具有90-100%的F1得分，平均减少了用户在理解隐私政策时95.2%的认知负担，并揭示了隐私政策中存在的用户难以接受的数据使用实践。", "motivation": "虽然现代人拥有多个在线账户，但他们很少阅读这些网站的条款和条件或隐私政策，尽管他们声称会这样做。作者旨在研发一个工具帮助用户分析这些复杂的隐私政策文档，以便更好地保护他们的数据隐私。", "method": "PoliAnalyzer系统使用自然语言处理技术从文本中提取数据使用实践的形式化表示，通过逻辑推理对比用户偏好与形式化的隐私政策表示，从而生成合规报告。系统扩展了现有的形式化数据使用政策语言，用于建模隐私政策和用户偏好。", "result": "通过评估，PoliAnalyzer在识别相关的数据使用实践上显示了90-100%准确的F1得分，成功地将用户需要理解的冲突部分减少到4.8%。此外，它还揭示了一些隐私政策中常见的、有悖于用户期望的数据使用实践。", "conclusion": "这项研究展示了PoliAnalyzer可用于大规模的个性化隐私政策分析，有望帮助用户更好地控制自己的数据并促使社会对平台的数据使用行为进行讨论。"}}
{"id": "2507.14432", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.14432", "abs": "https://arxiv.org/abs/2507.14432", "authors": ["Han Gong", "Qiyue Li", "Zhi Liu", "Hao Zhou", "Peng Yuan Zhou", "Zhu Li", "Jie Li"], "title": "Adaptive 3D Gaussian Splatting Video Streaming", "comment": null, "summary": "The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the\nquality of volumetric video representation. Meanwhile, in contrast to\nconventional volumetric video, 3DGS video poses significant challenges for\nstreaming due to its substantially larger data volume and the heightened\ncomplexity involved in compression and transmission. To address these issues,\nwe introduce an innovative framework for 3DGS volumetric video streaming.\nSpecifically, we design a 3DGS video construction method based on the Gaussian\ndeformation field. By employing hybrid saliency tiling and differentiated\nquality modeling of 3DGS video, we achieve efficient data compression and\nadaptation to bandwidth fluctuations while ensuring high transmission quality.\nThen we build a complete 3DGS video streaming system and validate the\ntransmission performance. Through experimental evaluation, our method\ndemonstrated superiority over existing approaches in various aspects, including\nvideo quality, compression effectiveness, and transmission rate.", "AI": {"tldr": "本文提出了一种3DGS体积视频流传输框架，该框架解决了由于其数据量大导致的传输挑战等问题。通过实验验证，新方法在多个方面优于现有的方法。", "motivation": "随着3D高斯点样技术的发展，尽管其大大提高了体积视频的表现质量，但同时也带来了由于数据量大和压缩传输复杂度高而产生的传输挑战。研究旨在解决这一问题。", "method": "本研究采用了一种基于高斯变形场的3DGS视频构建方法，结合混杂显著性分块和3DGS视频差异化质量建模，以实现高效的数据压缩和适应带宽变化，同时保证高传输质量。", "result": "实验验证表明，所提出的方法在视频质量、压缩效果和传输速率等方面优于现有方法。", "conclusion": "我们建立了一个完整的3DGS视频流系统，并证实了其传输性能。实验结果证明了我们方法在多个方面的优越性。"}}
{"id": "2507.14231", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14231", "abs": "https://arxiv.org/abs/2507.14231", "authors": ["Khalid Hasan", "Jamil Saquer"], "title": "Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media", "comment": "The 37th International Conference on Software Engineering & Knowledge\n  Engineering, SEKE 2025 (camera-ready)", "summary": "Bipolar disorder is a chronic mental illness frequently underdiagnosed due to\nsubtle early symptoms and social stigma. This paper explores the advanced\nnatural language processing (NLP) models for recognizing signs of bipolar\ndisorder based on user-generated social media text. We conduct a comprehensive\nevaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,\nDistilBERT) and Long Short Term Memory (LSTM) models based on contextualized\n(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed\non a large, annotated dataset of Reddit posts after confirming their validity\nthrough sentiment variance and judgmental analysis. Our results demonstrate\nthat RoBERTa achieves the highest performance among transformer models with an\nF1 score of ~98% while LSTM models using BERT embeddings yield nearly identical\nresults. In contrast, LSTMs trained on static embeddings fail to capture\nmeaningful patterns, scoring near-zero F1. These findings underscore the\ncritical role of contextual language modeling in detecting bipolar disorder. In\naddition, we report model training times and highlight that DistilBERT offers\nan optimal balance between efficiency and accuracy. In general, our study\noffers actionable insights for model selection in mental health NLP\napplications and validates the potential of contextualized language models to\nsupport early bipolar disorder screening.", "AI": {"tldr": "本研究通过高级NLP模型在识别社交网络文本中的双相情感障碍迹象上获得了高F1得分，表明上下文语言模型在精神健康应用中有很高的潜力。", "motivation": "双相情感障碍是一种常被误诊的精神疾病，由于早期症状微妙且社会上存在污名化现象。本研究旨在探索高级自然语言处理（NLP）模型用于识别双相情感障碍的用户生成社交媒体文本中的病症迹象。", "method": "本研究使用了基于变压器的模型（如BERT、RoBERTa、ALBERT、ELECTRA、DistilBERT）和基于长短时记忆（LSTM）的模型，分别采用上下文化词嵌入（BERT）和静态词嵌入（GloVe、Word2Vec），应用于一个大型且已标注的Reddit帖子数据集上。", "result": "实验结果显示，RoBERTa在变压器模型中表现最佳，F1得分为约98%；而使用BERT嵌入的LSTM模型获得几乎相同的结果。相比之下，使用静态嵌入的LSTMs在捕捉有意义的模式方面失败，得分为接近零的F1。DistilBERT在效率和准确性之间提供了最佳平衡。", "conclusion": "本研究为精神健康NLP应用中的模型选择提供了可行的见解，并验证了上下文化语言模型在早期双相情感障碍筛查中的潜力。"}}
{"id": "2507.14449", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14449", "abs": "https://arxiv.org/abs/2507.14449", "authors": ["Zhe Cao", "Jin Zhang", "Ruiheng Zhang"], "title": "IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark", "comment": "11 pages, 7 figures. This paper is accepted by ICCV 2025", "summary": "Real-world infrared imagery presents unique challenges for vision-language\nmodels due to the scarcity of aligned text data and domain-specific\ncharacteristics. Although existing methods have advanced the field, their\nreliance on synthetic infrared images generated through style transfer from\nvisible images, which limits their ability to capture the unique\ncharacteristics of the infrared modality. To address this, we propose IRGPT,\nthe first multi-modal large language model for real-world infrared images,\nbuilt upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K\nauthentic image-text pairs. The proposed IR-TD dataset contains real infrared\nimages paired with meticulously handcrafted texts, where the initial drafts\noriginated from two complementary processes: (1) LLM-generated descriptions of\nvisible images, and (2) rule-based descriptions of annotations. Furthermore, we\nintroduce a bi-cross-modal curriculum transfer learning strategy that\nsystematically transfers knowledge from visible to infrared domains by\nconsidering the difficulty scores of both infrared-visible and infrared-text.\nEvaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT\nachieves state-of-the-art performance even compared with larger-scale models.", "AI": {"tldr": "本研究提出了一种专门处理现实世界红外图像的多模态语言模型IRGPT，通过引入真实数据集以及特定迁移学习策略，解决了现有模型因依赖合成数据而无法充分捕捉红外图像特性的问题。", "motivation": "现实世界的红外图像对视觉-语言模型提出了独特挑战，因其缺乏与图像对应的真实文本数据以及特定领域的特性。现有方法依赖于通过风格转移生成的合成红外图像，限制了其捕捉红外模态独特特性的能力。", "method": "提出IRGPT，首个用于现实世界红外图像的多模态大规模语言模型，基于包含超过260K真实图像文本对的大型红外文本数据集（IR-TD）。IR-TD数据集包含真实的红外图像与精心编写的文本配对，初始草稿来自两个互补过程：由大语言模型生成的可见图像描述和基于标注的规则描述。此外，引入了一种考虑红外-可见域和红外-文本难度分数的双跨域课程迁移学习策略。", "result": "在9个基准任务（如识别、定域）中，与更大规模的模型相比，IRGPT也达到了最先进的性能。", "conclusion": "IRGPT通过新的红外文本数据集IR-TD和迁移学习策略，解决了现有方法依赖合成红外图像带来的一些局限，提高了在现实世界红外图像上的性能。"}}
{"id": "2507.14238", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.14238", "abs": "https://arxiv.org/abs/2507.14238", "authors": ["Matthew Kearney", "Reuben Binns", "Yarin Gal"], "title": "Language Models Change Facts Based on the Way You Talk", "comment": null, "summary": "Large language models (LLMs) are increasingly being used in user-facing\napplications, from providing medical consultations to job interview advice.\nRecent research suggests that these models are becoming increasingly proficient\nat inferring identity information about the author of a piece of text from\nlinguistic patterns as subtle as the choice of a few words. However, little is\nknown about how LLMs use this information in their decision-making in\nreal-world applications. We perform the first comprehensive analysis of how\nidentity markers present in a user's writing bias LLM responses across five\ndifferent high-stakes LLM applications in the domains of medicine, law,\npolitics, government benefits, and job salaries. We find that LLMs are\nextremely sensitive to markers of identity in user queries and that race,\ngender, and age consistently influence LLM responses in these applications. For\ninstance, when providing medical advice, we find that models apply different\nstandards of care to individuals of different ethnicities for the same\nsymptoms; we find that LLMs are more likely to alter answers to align with a\nconservative (liberal) political worldview when asked factual questions by\nolder (younger) individuals; and that LLMs recommend lower salaries for\nnon-White job applicants and higher salaries for women compared to men. Taken\ntogether, these biases mean that the use of off-the-shelf LLMs for these\napplications may cause harmful differences in medical care, foster wage gaps,\nand create different political factual realities for people of different\nidentities. Beyond providing an analysis, we also provide new tools for\nevaluating how subtle encoding of identity in users' language choices impacts\nmodel decisions. Given the serious implications of these findings, we recommend\nthat similar thorough assessments of LLM use in user-facing applications are\nconducted before future deployment.", "AI": {"tldr": "本研究发现，大型语言模型在不同应用场景中对用户身份信息的偏见表现显著，可能引发严重的社会问题。", "motivation": "尽管近期研究表明LLMs能够通过语言模式推断出文本作者的身份信息，但关于它们如何在实际应用中利用这些信息知之甚少。", "method": "该研究首次对五个高风险的应用场景（医学、法律、政治、政府福利、薪资）中的大型语言模型（LLMs）进行了全面分析，探讨了用户文本中的身份标记如何影响LLMs的决策。", "result": "研究发现，LLMs对用户查询中的身份标记极其敏感，种族、性别和年龄一致地影响了LLMs在这些应用中的响应。例如，提供医疗建议时，对相同症状的不同族裔个体应用不同的护理标准；回答事实性问题时，对老年（青年）个体的倾向性更接近保守（自由）政治立场；建议为非白人求职者提供较低的薪资，为女性提供比男性更高的薪资。", "conclusion": "这些偏见可能导致医疗保健的不利差异，加剧工资差距，并为不同身份的人创建不同的政治现实。建议在未来的部署前，应对LLMs的用户应用进行类似的彻底评估。"}}
{"id": "2507.14452", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14452", "abs": "https://arxiv.org/abs/2507.14452", "authors": ["Weikang Gu", "Mingyue Han", "Li Xue", "Heng Dong", "Changcai Yang", "Riqing Chen", "Lifang Wei"], "title": "GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration", "comment": "9 pages, 4 figures. Accepted to IJCAI 2025", "summary": "The accurate identification of high-quality correspondences is a prerequisite\ntask in feature-based point cloud registration. However, it is extremely\nchallenging to handle the fusion of local and global features due to feature\nredundancy and complex spatial relationships. Given that Gestalt principles\nprovide key advantages in analyzing local and global relationships, we propose\na novel Gestalt-guided Parallel Interaction Network via orthogonal geometric\nconsistency (GPI-Net) in this paper. It utilizes Gestalt principles to\nfacilitate complementary communication between local and global information.\nSpecifically, we introduce an orthogonal integration strategy to optimally\nreduce redundant information and generate a more compact global structure for\nhigh-quality correspondences. To capture geometric features in correspondences,\nwe leverage a Gestalt Feature Attention (GFA) block through a hybrid\nutilization of self-attention and cross-attention mechanisms. Furthermore, to\nfacilitate the integration of local detail information into the global\nstructure, we design an innovative Dual-path Multi-Granularity parallel\ninteraction aggregation (DMG) block to promote information exchange across\ndifferent granularities. Extensive experiments on various challenging tasks\ndemonstrate the superior performance of our proposed GPI-Net in comparison to\nexisting methods. The code will be released at https://github.com/gwk/GPI-Net.", "AI": {"tldr": "本文提出了GPI-Net，一种新的基于Gestalt原则的网络，用于改善点云注册中的高质量对应关系识别。该网络通过处理局部和全局特征的冗余度以及不同的几何特征，实现更高效的特征匹配。", "motivation": "为了改善点云注册中高质量对应关系的识别，解决局部特征和全局特征融合的挑战，利用Gestalt原则来促进局部和全局信息的互补通信。", "method": "提出了一种新的Gestalt指导的并行交互网络（GPI-Net），通过正交几何一致性来优化信息的冗余度，利用Gestalt特征注意力（GFA）块捕捉几何特征，并设计了双路径多粒度并行交互聚合（DMG）块来促进不同粒度之间的信息交换。", "result": "实验结果显示，所提出的GPI-Net在各种试验任务中表现优于现有方法。", "conclusion": "GPI-Net通过融合Gestalt原则优化了几何特征的对应关系识别，在点云注册任务中表现出色。"}}
{"id": "2507.14239", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14239", "abs": "https://arxiv.org/abs/2507.14239", "authors": ["Weihua Zheng", "Roy Ka-Wei Lee", "Zhengyuan Liu", "Kui Wu", "AiTi Aw", "Bowei Zou"], "title": "CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation", "comment": null, "summary": "Multilingual Large Language Models(MLLMs) demonstrate strong generalization\nacross languages, yet they remain prone to hallucinations, especially in\nlow-resource languages, due to training data imbalances. These hallucinations,\nwhich include inaccurate or fabricated outputs, are particularly problematic in\ndomain-specific generation tasks (Chataigner et al., 2024). To address this\nchallenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based\nCross-lingual Chain-of-Thought), a two-stage fine-tuning framework for\nmitigating hallucination in MLLMs. Our approach first enhances cross-lingual\nsemantic alignment through curriculum-based contrastive learning combined with\nnext-token prediction during continued pre-training. Building on this\nfoundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting\nstrategy during instruction fine-tuning, which guides the model to reason in a\nhigh-resource language before generating answers in the target low-resource\nlanguage. Experimental results show that CCL-XCoT reduces hallucination rates\nby up to 62% and substantially improves factual knowledge transfer across\nlanguage pairs, without relying on external retrieval or multi-model ensembles.", "AI": {"tldr": "提出CCL-XCoT，一个两阶段微调框架，减少多语言大语言模型中的幻觉现象，通过课程对比学习和跨语言思维链提示策略提高跨语言事实知识的迁移。", "motivation": "多语言大语言模型容易产生幻觉，特别是在低资源语言中，这主要归因于训练数据不平衡。这些幻觉现象在特定领域的生成任务中尤为重要。", "method": "我们的方法CCL-XCoT是一个两阶段的微调框架，旨在减少多语言大语言模型中的幻觉现象。第一阶段通过基于课程的对比学习和下一步预测增强跨语言语义对齐。第二阶段引入跨语言思维链提示策略，指导模型先在高资源语言中进行推理，然后再在目标的低资源语言中生成答案。", "result": "实验结果显示，CCL-XCoT将幻觉率降低了高达62%，并且显著提高了跨语言配对的事实知识转移，无需依赖外部检索或多模型集成。", "conclusion": "CCL-XCoT通过两阶段微调框架减少了多语言大语言模型中的幻觉现象，并提高了事实知识的跨语言迁移效率。"}}
{"id": "2507.14454", "categories": ["cs.CV", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.14454", "abs": "https://arxiv.org/abs/2507.14454", "authors": ["Han Gong", "Qiyue Li", "Jie Li", "Zhi Liu"], "title": "Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation", "comment": null, "summary": "3D Gaussian splatting video (3DGS) streaming has recently emerged as a\nresearch hotspot in both academia and industry, owing to its impressive ability\nto deliver immersive 3D video experiences. However, research in this area is\nstill in its early stages, and several fundamental challenges, such as tiling,\nquality assessment, and bitrate adaptation, require further investigation. In\nthis paper, we tackle these challenges by proposing a comprehensive set of\nsolutions. Specifically, we propose an adaptive 3DGS tiling technique guided by\nsaliency analysis, which integrates both spatial and temporal features. Each\ntile is encoded into versions possessing dedicated deformation fields and\nmultiple quality levels for adaptive selection. We also introduce a novel\nquality assessment framework for 3DGS video that jointly evaluates\nspatial-domain degradation in 3DGS representations during streaming and the\nquality of the resulting 2D rendered images. Additionally, we develop a\nmeta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS\nvideo streaming, achieving optimal performance across varying network\nconditions. Extensive experiments demonstrate that our proposed approaches\nsignificantly outperform state-of-the-art methods.", "AI": {"tldr": "本文提出了一套解决方案，包括自适应3DGS分块技术、质量评估框架和自适应比特率算法，解决了3DGS视频流技术中的几个关键挑战，实验结果表明这些方法优于现有的技术。", "motivation": "尽管3DGS流媒体技术因其能够提供沉浸式3D视频体验而成为研究热点，但该领域仍处于早期发展阶段，需要进一步研究一些基本挑战，如分块、质量评估和比特率适应。因此，本文针对这些挑战提出了综合解决方案。", "method": "本文提出了一个自适应的3D高斯点阵化分块技术，该技术利用了关注点分析并集成了空间和时间特征。每个分块都根据需求编码为具有特定变形场和多个质量层次的版本，以实现自适应选择。此外，本文还提出了一个全新的质量评估框架，用于评估3DGS表示在流媒体期间的空间域退化以及所生成2D渲染图像的质量。最后，开发了一种基于元学习的自适应比特率算法，特别针对3DGS视频流，以适应变化的网络条件。", "result": "广泛的实验表明，所提出的方法在各种网络条件下显著优于最先进的方法。", "conclusion": "本文通过提出的自适应3DGS分块技术、质量评估框架和基于元学习的自适应比特率算法，在3D视频流技术的挑战性问题上取得了重要进展，为未来研究提供了坚实的基础。"}}
{"id": "2507.14240", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14240", "abs": "https://arxiv.org/abs/2507.14240", "authors": ["Mohammad Shahedur Rahman", "Peng Gao", "Yuede Ji"], "title": "HuggingGraph: Understanding the Supply Chain of LLM Ecosystem", "comment": "10 pages, 5 figures", "summary": "Large language models (LLMs) leverage deep learning to process and predict\nsequences of words from context, enabling them to perform various NLP tasks,\nsuch as translation, summarization, question answering, and content generation.\nHowever, the growing size and complexity of developing, training, and deploying\nadvanced LLMs require extensive computational resources and large datasets.\nThis creates a barrier for users. As a result, platforms that host models and\ndatasets are widely used. For example, Hugging Face, one of the most popular\nplatforms, hosted 1.8 million models and 450K datasets by June 2025, with no\nsign of slowing down. Since many LLMs are built from base models, pre-trained\nmodels, and external datasets, they can inherit vulnerabilities, biases, or\nmalicious components from earlier models or datasets. Therefore, it is critical\nto understand the origin and development of these components to better detect\npotential risks, improve model fairness, and ensure compliance. Motivated by\nthis, our project aims to study the relationships between models and datasets,\nwhich are core components of the LLM supply chain. First, we design a method to\nsystematically collect LLM supply chain data. Using this data, we build a\ndirected heterogeneous graph to model the relationships between models and\ndatasets, resulting in a structure with 397,376 nodes and 453,469 edges. We\nthen perform various analyses and uncover several findings, such as: (i) the\nLLM supply chain graph is large, sparse, and follows a power-law degree\ndistribution; (ii) it features a densely connected core and a fragmented\nperiphery; (iii) datasets play pivotal roles in training; (iv) strong\ninterdependence exists between models and datasets; and (v) the graph is\ndynamic, with daily updates reflecting the ecosystem's ongoing evolution.", "AI": {"tldr": "研究LLM供应链中模型和数据集之间的关系，构建并分析了包含大量节点和边的有向异构图，揭示了LLM供应链的重要特性和发展动态。", "motivation": "由于许多大型语言模型（LLMs）是从基础模型、预训练模型和外部数据集中构建的，它们可能继承了以前模型或数据集中的漏洞、偏见或恶意组件。因此，了解这些组件的来源和发展以更好地检测潜在风险、提高模型公平性并确保合规性的需求变得至关重要。", "method": "设计了一种系统收集LLM供应链数据的方法，并使用这些数据构建了一个有向异构图来建模模型与数据集之间的关系，生成了一个包含397,376个节点和453,469条边的结构。", "result": "分析中发现了一些关键点，例如: (i) LLM供应链图很大，稀疏，并遵循幂律度分布；(ii) 它有一个紧密相连的核心部分和一个碎片化的外围部分；(iii) 数据集在训练中扮演了关键角色；(iv) 模型与数据集之间存在着强烈的相互依赖性；以及(v) 该图是动态的，每天都会更新，反映出生态系统的持续演变。", "conclusion": "本项目研究了模型与数据集之间的关系，作为LLM供应链的核心组成部分，通过构建和分析有向异构图，揭示了LLM供应链的动态性和复杂性。"}}
{"id": "2507.14456", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14456", "abs": "https://arxiv.org/abs/2507.14456", "authors": ["Chi Wan", "Yixin Cui", "Jiatong Du", "Shuo Yang", "Yulong Bai", "Yanjun Huang"], "title": "GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving", "comment": null, "summary": "End-to-end autonomous driving requires adaptive and robust handling of\ncomplex and diverse traffic environments. However, prevalent single-mode\nplanning methods attempt to learn an overall policy while struggling to acquire\ndiversified driving skills to handle diverse scenarios. Therefore, this paper\nproposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework\nfeaturing a Global Expert, a Scene-Adaptive Experts Group, and equipped with a\nDual-aware Router. Specifically, the Global Expert is trained on the overall\ndataset, possessing robust performance. The Scene-Adaptive Experts are trained\non corresponding scene subsets, achieving adaptive performance. The Dual-aware\nRouter simultaneously considers scenario-level features and routing uncertainty\nto dynamically activate expert modules. Through the effective coupling of the\nGlobal Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,\nGEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS\noutperforms existing methods in the Bench2Drive closed-loop benchmark and\nachieves state-of-the-art performance in Driving Score and Success Rate, even\nwith only monocular vision input. Furthermore, ablation studies demonstrate\nsignificant improvements over the original single-expert baseline: 7.67% in\nDriving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The\ncode will be available at https://github.com/newbrains1/GEMINUS.", "AI": {"tldr": "论文介绍GEMINUS框架，该框架通过结合全局专家和场景适应性专家群，提高了端到端自动驾驶在多种场景中的适应性和鲁棒性。", "motivation": "现有的单一模式规划方法难以学习多样化的驾驶技能，影响其在复杂多变的交通环境中的表现。因此，本论文旨在提出一种能更好处理多样场景的方案。", "method": "提出GEMINUS框架，这是一个专家混合的端到端自动驾驶系统，包含全局专家、场景自适应专家群和双感知路由器。全局专家在整个数据集上训练，场景自适应专家群在特定场景子集上训练，双感知路由器综合考虑场景特征和路由不确定性。", "result": "GEMINUS在Bench2Drive闭环基准测试中优于现有方法，在驾驶得分和成功率方面达到最先进水平，即使使用只有一只摄像头的视觉输入。消融研究表明相较于单专家基线，该方法驾驶得分提升了7.67%，成功率提升了22.06%，多能力平均值提升了19.41%。", "conclusion": "GEMINUS框架通过耦合全局专家和场景自适应专家群，实现了在多样场景下的自适应和鲁棒性表现。"}}
{"id": "2507.14241", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14241", "abs": "https://arxiv.org/abs/2507.14241", "authors": ["Rithesh Murthy", "Ming Zhu", "Liangwei Yang", "Jielin Qiu", "Juntao Tan", "Shelby Heinecke", "Huan Wang", "Caiming Xiong", "Silvio Savarese"], "title": "Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) perform best with well-crafted prompts, yet\nprompt engineering remains manual, inconsistent, and inaccessible to\nnon-experts. We introduce Promptomatix, an automatic prompt optimization\nframework that transforms natural language task descriptions into high-quality\nprompts without requiring manual tuning or domain expertise. Promptomatix\nsupports both a lightweight meta-prompt-based optimizer and a DSPy-powered\ncompiler, with modular design enabling future extension to more advanced\nframeworks. The system analyzes user intent, generates synthetic training data,\nselects prompting strategies, and refines prompts using cost-aware objectives.\nEvaluated across 5 task categories, Promptomatix achieves competitive or\nsuperior performance compared to existing libraries, while reducing prompt\nlength and computational overhead making prompt optimization scalable and\nefficient.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.14459", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14459", "abs": "https://arxiv.org/abs/2507.14459", "authors": ["Huayuan Ye", "Juntong Chen", "Shenzhuo Zhang", "Yipeng Zhang", "Changbo Wang", "Chenhui Li"], "title": "VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval", "comment": "9 pages, IEEE VIS 2025", "summary": "The dissemination of visualizations is primarily in the form of raster\nimages, which often results in the loss of critical information such as source\ncode, interactive features, and metadata. While previous methods have proposed\nembedding metadata into images to facilitate Visualization Image Data Retrieval\n(VIDR), most existing methods lack practicability since they are fragile to\ncommon image tampering during online distribution such as cropping and editing.\nTo address this issue, we propose VisGuard, a tamper-resistant VIDR framework\nthat reliably embeds metadata link into visualization images. The embedded data\nlink remains recoverable even after substantial tampering upon images. We\npropose several techniques to enhance robustness, including repetitive data\ntiling, invertible information broadcasting, and an anchor-based scheme for\ncrop localization. VisGuard enables various applications, including interactive\nchart reconstruction, tampering detection, and copyright protection. We conduct\ncomprehensive experiments on VisGuard's superior performance in data retrieval\naccuracy, embedding capacity, and security against tampering and steganalysis,\ndemonstrating VisGuard's competence in facilitating and safeguarding\nvisualization dissemination and information conveyance.", "AI": {"tldr": "_visGuard是一个抗篡改的框架，用于提高可视化图像中数据检索的可靠性与鲁棒性。_", "motivation": "许多现有方法在将元数据嵌入图像以促进VIDR时缺乏实用性，因为它们在应对在线传播过程中常见的图像篡改行为时较为脆弱，如裁剪和编辑。为了克服这个问题，我们提出了VisGuard。", "method": "我们提出了VisGuard，这是一个抗篡改的可视化图像数据检索（VIDR）框架，能够可靠地将元数据链接嵌入到可视化图像中。为了增强其鲁棒性，我们提出了一些技术，包括重复数据平铺、可逆信息广播以及基于锚点的裁剪定位方案。", "result": "实验显示，VisGuard在数据检索准确性、嵌入能力和对抗篡改和隐写分析的安全性方面表现出色，证明了它在促进和保护可视化传播及信息传达方面的能力。", "conclusion": "通过VisGuard，我们可以实现交互式图表的重建、篡改检测以及版权保护等多种应用。"}}
{"id": "2507.14298", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14298", "abs": "https://arxiv.org/abs/2507.14298", "authors": ["Wan-Cyuan Fan", "Yen-Chun Chen", "Mengchen Liu", "Alexander Jacobson", "Lu Yuan", "Leonid Sigal"], "title": "In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding", "comment": "arXiv admin note: substantial text overlap with arXiv:2407.14506", "summary": "Recent methods for customizing Large Vision Language Models (LVLMs) for\ndomain-specific tasks have shown promising results in scientific chart\ncomprehension. However, existing approaches face two major limitations: First,\nthey rely on paired data from only a few chart types, limiting generalization\nto wide range of chart types. Secondly, they lack targeted pre-training for\nchart-data alignment, which hampers the model's understanding of underlying\ndata. In this paper, we introduce ChartScope, an LVLM optimized for in-depth\nchart comprehension across diverse chart types. We propose an efficient data\ngeneration pipeline that synthesizes paired data for a wide range of chart\ntypes, along with a novel Dual-Path training strategy that enabling the model\nto succinctly capture essential data details while preserving robust reasoning\ncapabilities by incorporating reasoning over the underlying data. Lastly, we\nestablish ChartDQA, a new benchmark for evaluating not only question-answering\nat different levels but also underlying data understanding. Experimental\nresults demonstrate that ChartScope significantly enhances comprehension on a\nwide range of chart types. The code and data are available at\nhttps://davidhalladay.github.io/chartscope_demo.", "AI": {"tldr": "本文提出了ChartScope，一种优化的LVLM，用于跨多种图表类型的深入图表理解，通过新的数据生成管道和双路径训练策略，显著提高了广泛的图表理解能力，并建立了一个新的评估基准ChartDQA。", "motivation": "现有方法在定制LVLM进行特定领域的任务（如科学图表理解）方面取得了有前景的结果，但存在依赖少数图表类型配对数据和缺少针对图表数据对齐的预训练等主要限制，这限制了其泛化能力和对底层数据的理解。本文提出的方法旨在克服这些问题。", "method": "本文介绍了ChartScope，一种专门为跨多种图表类型的深入图表理解优化的大型视觉语言模型（LVLM）。提出了一个高效的数据生成管道，能够合成多种类型图表的配对数据，并提出了一种新颖的双路径训练策略，该策略使模型能够简洁地捕捉到重要数据细节，同时通过考虑底层数据，保持强大的推理能力。", "result": "实验结果表明，ChartScope在多种类型的图表理解上取得了显著的性能提升。", "conclusion": "实验表明，提出的方法在广泛的图表类型上显著增强了理解能力。"}}
{"id": "2507.14477", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14477", "abs": "https://arxiv.org/abs/2507.14477", "authors": ["Zhenyu Li", "Tianyi Shang", "Pengjie Xu", "Ruirui Zhang", "Fanchen Kong"], "title": "OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition", "comment": "5 figures", "summary": "Visual Place Recognition (VPR) in dynamic and perceptually aliased\nenvironments remains a fundamental challenge for long-term localization.\nExisting deep learning-based solutions predominantly focus on single-frame\nembeddings, neglecting the temporal coherence present in image sequences. This\npaper presents OptiCorNet, a novel sequence modeling framework that unifies\nspatial feature extraction and temporal differencing into a differentiable,\nend-to-end trainable module. Central to our approach is a lightweight 1D\nconvolutional encoder combined with a learnable differential temporal operator,\ntermed Differentiable Sequence Delta (DSD), which jointly captures short-term\nspatial context and long-range temporal transitions. The DSD module models\ndirectional differences across sequences via a fixed-weight differencing\nkernel, followed by an LSTM-based refinement and optional residual projection,\nyielding compact, discriminative descriptors robust to viewpoint and appearance\nshifts. To further enhance inter-class separability, we incorporate a\nquadruplet loss that optimizes both positive alignment and multi-negative\ndivergence within each batch. Unlike prior VPR methods that treat temporal\naggregation as post-processing, OptiCorNet learns sequence-level embeddings\ndirectly, enabling more effective end-to-end place recognition. Comprehensive\nevaluations on multiple public benchmarks demonstrate that our approach\noutperforms state-of-the-art baselines under challenging seasonal and viewpoint\nvariations.", "AI": {"tldr": "本文提出了OptiCorNet，一种新的序列建模框架，通过结合1D卷积编码器和差分时间算子来改进视觉地点识别，以应对视点和外观变化，该框架在多个公开基准上超越了现有基准。", "motivation": "动力在于解决视觉地点识别（VPR）在动态和感知别名环境中的挑战，这种环境对长期定位至关重要。现有的基于深度学习的解决方案主要集中在单帧嵌入上，忽视了图像序列中存在的时间连贯性。", "method": "本文提出了一种名为OptiCorNet的新序列建模框架，该框架将空间特征提取和时间差异整合到一个可微分的端到端可训练模块中。核心是结合轻量级的1D卷积编码器和可学习的差分时间算子（DSD），该算子可以联合捕获短期空间上下文和长距离时间转换。DSD模块通过固定权重差分核对序列间的方向差异进行建模，继而通过LSTM进行优化和可选的残差投影，生成紧凑且具有区分度的描述符，这些描述符能够有效地应对视点和外观变化。为了进一步增强类别间的可分性，本文引入了一种四重损失，它在每个批次中优化正面相似性和多负面差异。", "result": "实验结果表明，本文的方法在多个公开数据集上优于现有的最先进的方法。", "conclusion": "全面的评估表明，与现有的最先进技术相比，本文的方法在面对具有挑战性的季节变化和视点变化时表现更为出色。"}}
{"id": "2507.14304", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14304", "abs": "https://arxiv.org/abs/2507.14304", "authors": ["Rakesh Paul", "Anusha Kamath", "Kanishk Singla", "Raviraj Joshi", "Utkarsh Vaidya", "Sanjay Singh Chauhan", "Niranjan Wartikar"], "title": "Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study", "comment": null, "summary": "Multilingual large language models (LLMs) often demonstrate a performance gap\nbetween English and non-English languages, particularly in low-resource\nsettings. Aligning these models to low-resource languages is essential yet\nchallenging due to limited high-quality data. While English alignment datasets\nare readily available, curating equivalent data in other languages is expensive\nand time-consuming. A common workaround is to translate existing English\nalignment data; however, standard translation techniques often fail to preserve\ncritical elements such as code, mathematical expressions, and structured\nformats like JSON. In this work, we investigate LLM-based selective\ntranslation, a technique that selectively translates only the translatable\nparts of a text while preserving non-translatable content and sentence\nstructure. We conduct a systematic study to explore key questions around this\napproach, including its effectiveness compared to vanilla translation, the\nimportance of filtering noisy outputs, and the benefits of mixing translated\nsamples with original English data during alignment. Our experiments focus on\nthe low-resource Indic language Hindi and compare translations generated by\nGoogle Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the\npromise of selective translation as a practical and effective method for\nimproving multilingual alignment in LLMs.", "AI": {"tldr": "The paper investigates LLM-based selective translation for aligning multilingual large language models, focusing on preserving non-translatable content and improving performance in low-resource languages like Hindi.", "motivation": "The motivation is to address the performance gap between English and non-English languages in multilingual LLMs, especially in low-resource settings, by effectively aligning these models.", "method": "The method involves using selective translation techniques to translate only translatable parts of alignment datasets, combining this approach with the original English data for alignment, and conducting experiments with Google Cloud Translation and Llama-3.1-405B.", "result": "The results indicate that selective translation can effectively preserve important non-translatable content and enhance the alignment of multilingual LLMs, particularly in low-resource languages such as Hindi.", "conclusion": "The conclusion is that LLM-based selective translation holds potential as a practical method for enhancing multilingual alignment in large language models, effectively bridging the performance gap in low-resource languages."}}
{"id": "2507.14481", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14481", "abs": "https://arxiv.org/abs/2507.14481", "authors": ["Yujia Tong", "Jingling Yuan", "Tian Zhang", "Jianquan Liu", "Chuang Hu"], "title": "DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning", "comment": null, "summary": "Data-Free Quantization (DFQ) enables the quantization of Vision Transformers\n(ViTs) without requiring access to data, allowing for the deployment of ViTs on\ndevices with limited resources. In DFQ, the quantization model must be\ncalibrated using synthetic samples, making the quality of these synthetic\nsamples crucial. Existing methods fail to fully capture and balance the global\nand local features within the samples, resulting in limited synthetic data\nquality. Moreover, we have found that during inference, there is a significant\ndifference in the distributions of intermediate layer activations between the\nquantized and full-precision models. These issues lead to a severe performance\ndegradation of the quantized model. To address these problems, we propose a\npipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).\nSpecifically, we synthesize samples in order of increasing difficulty,\neffectively enhancing the quality of synthetic data. During the calibration and\ninference stage, we introduce the activation correction matrix for the\nquantized model to align the intermediate layer activations with those of the\nfull-precision model. Extensive experiments demonstrate that DFQ-ViT achieves\nremarkable superiority over existing DFQ methods and its performance is on par\nwith models quantized through real data. For example, the performance of DeiT-T\nwith 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our\nmethod eliminates the need for fine-tuning, which not only reduces\ncomputational overhead but also lowers the deployment barriers for edge\ndevices. This characteristic aligns with the principles of Green Learning by\nimproving energy efficiency and facilitating real-world applications in\nresource-constrained environments.", "AI": {"tldr": "a novel DFQ method for ViTs that enhances synthetic sample quality and aligns activation distributions, surpassing state-of-the-art methods and enabling efficient deployment on edge devices", "motivation": "to enhance the quality of synthetic data for DFQ, align the activation distributions between quantized and full-precision ViTs, and improve the performance of quantized ViTs without the need for fine-tuning or real data", "method": "synthesize samples in order of increasing difficulty and introduce an activation correction matrix to align the intermediate layer activations of the quantized model with those of the full-precision model", "result": "DFQ-ViT achieves superior results over existing DFQ methods and comparable performance to those quantized through real data", "conclusion": "the proposed method significantly improves the efficiency and performance of quantizing ViTs for deployment on devices with limited resources, aligning with the principles of Green Learning"}}
{"id": "2507.14307", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14307", "abs": "https://arxiv.org/abs/2507.14307", "authors": ["Karin de Langis", "Jong Inn Park", "Andreas Schramm", "Bin Hu", "Khanh Chi Le", "Michael Mensink", "Ahn Thu Tong", "Dongyeop Kang"], "title": "How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs", "comment": null, "summary": "Large language models (LLMs) exhibit increasingly sophisticated linguistic\ncapabilities, yet the extent to which these behaviors reflect human-like\ncognition versus advanced pattern recognition remains an open question. In this\nstudy, we investigate how LLMs process the temporal meaning of linguistic\naspect in narratives that were previously used in human studies. Using an\nExpert-in-the-Loop probing pipeline, we conduct a series of targeted\nexperiments to assess whether LLMs construct semantic representations and\npragmatic inferences in a human-like manner. Our findings show that LLMs\nover-rely on prototypicality, produce inconsistent aspectual judgments, and\nstruggle with causal reasoning derived from aspect, raising concerns about\ntheir ability to fully comprehend narratives. These results suggest that LLMs\nprocess aspect fundamentally differently from humans and lack robust narrative\nunderstanding. Beyond these empirical findings, we develop a standardized\nexperimental framework for the reliable assessment of LLMs' cognitive and\nlinguistic capabilities.", "AI": {"tldr": "研究发现LLMs在处理叙事方面的历时意义时，与人类有根本的不同，且缺乏强大的叙事理解能力，开发了评估LLMs认知和语言能力的标准实验框架。", "motivation": "研究LLM处理叙事中语言方面的历时意义的方式，以确定其行为与人类认知的相似度以及高级模式识别的差异。", "method": "使用专家循环探测管道进行了一系列针对性实验，以评估LLMs是否以与人类相似的方式构建语义表示和语用推理。", "result": "发现LLMs过度依赖原型性，产生不一致的时间态判断，并且在基于方面的因果推理方面存在困难，这些结果引起对其全面理解叙事能力的担忧。", "conclusion": "LLMs在处理叙事方面的历时意义方面与人类存在根本差异，并且在全面理解叙事方面的能力有限。"}}
{"id": "2507.14485", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14485", "abs": "https://arxiv.org/abs/2507.14485", "authors": ["Hongye Hou", "Liu Zhan", "Yang Yang"], "title": "Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion", "comment": null, "summary": "Completing the whole 3D structure based on an incomplete point cloud is a\nchallenging task, particularly when the residual point cloud lacks typical\nstructural characteristics. Recent methods based on cross-modal learning\nattempt to introduce instance images to aid the structure feature learning.\nHowever, they still focus on each particular input class, limiting their\ngeneration abilities. In this work, we propose a novel retrieval-augmented\npoint cloud completion framework. The core idea is to incorporate cross-modal\nretrieval into completion task to learn structural prior information from\nsimilar reference samples. Specifically, we design a Structural Shared Feature\nEncoder (SSFE) to jointly extract cross-modal features and reconstruct\nreference features as priors. Benefiting from a dual-channel control gate in\nthe encoder, relevant structural features in the reference sample are enhanced\nand irrelevant information interference is suppressed. In addition, we propose\na Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical\nfeature fusion mechanism to integrate reference prior information with input\nfeatures from global to local. Through extensive evaluations on multiple\ndatasets and real-world scenes, our method shows its effectiveness in\ngenerating fine-grained point clouds, as well as its generalization capability\nin handling sparse data and unseen categories.", "AI": {"tldr": "本文提出了一种基于检索增强的点云补全框架，通过跨模态检索技术从相似参考样本中学习结构先验信息，以增强点云补全能力。实验显示该方法在生成精细点云和处理稀疏数据及未见类别方面具有显著效果。", "motivation": "现有的基于跨模态学习的方法在点云补全上的生成能力有限，尤其在面对缺乏典型结构特征的点云时。", "method": "Structure", "result": "{\"tldr\": \"本文提出了一种基于检索增强的点云补全框架，通过跨模态检索技术从相似参考样本中学习结构先验信息，以增强点云补全能力。实验显示该方法在生成精细点云和处理稀疏数据及未见类别方面具有显著效果。\", \"motivation\": \"现有的基于跨模态学习的方法在点云补全上的生成能力有限，尤其在面对缺乏典型结构特征的点云时。\", \"method\": \"设计了结构共享特征编码器（SSFE）和渐进检索增强生成器（PRAG），并通过双通道控制门机制增强参考样本中的相关结构特征。\", \"result\": \"实验结果表明，该方法在生成精细的点云以及处理稀疏和未见类的点云上展示了良好的效果。\", \"conclusion\": \"提出的方法通过引入跨模态检索技术，不仅增强了点云补全的性能，还提升了模型在处理未见类别数据方面的泛化能力。\"}", "conclusion": "提出的方法通过引入跨模态检索技术，不仅增强了点云补全的性能，还提升了模型在处理未见类别数据方面的泛化能力。"}}
{"id": "2507.14314", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14314", "abs": "https://arxiv.org/abs/2507.14314", "authors": ["Marija Anđedelić", "Dominik Šipek", "Laura Majer", "Jan Šnajder"], "title": "What Makes You CLIC: Detection of Croatian Clickbait Headlines", "comment": "Accepted at Slavic NLP 2025", "summary": "Online news outlets operate predominantly on an advertising-based revenue\nmodel, compelling journalists to create headlines that are often scandalous,\nintriguing, and provocative -- commonly referred to as clickbait. Automatic\ndetection of clickbait headlines is essential for preserving information\nquality and reader trust in digital media and requires both contextual\nunderstanding and world knowledge. For this task, particularly in\nless-resourced languages, it remains unclear whether fine-tuned methods or\nin-context learning (ICL) yield better results. In this paper, we compile CLIC,\na novel dataset for clickbait detection of Croatian news headlines spanning a\n20-year period and encompassing mainstream and fringe outlets. We fine-tune the\nBERTi\\'c model on this task and compare its performance to LLM-based ICL\nmethods with prompts both in Croatian and English. Finally, we analyze the\nlinguistic properties of clickbait. We find that nearly half of the analyzed\nheadlines contain clickbait, and that finetuned models deliver better results\nthan general LLMs.", "AI": {"tldr": "本文构建了CLIC数据集，用于检测克罗地亚语的点击诱饵，并比较了微调BERTi'c模型与上下文学习方法的表现，发现微调模型效果更优。", "motivation": "在线新闻网站主要依赖广告收入，新闻标题过多使用吸引眼球的方式（点击诱饵），这对维持数字媒体的信息质量和读者信任构成挑战。本文旨在通过自动检测来解决这一问题。", "method": "本文构建了一个名为CLIC的新数据集，用于检测克罗地亚新闻标题中的点击诱饵，时间跨度为20年，并涉及主流和边缘新闻来源。研究比较了微调BERTi'c模型与基于LLM的上下文学习方法的表现，并分析了点击诱饵的语义特性。", "result": "研究发现，近一半的分析标题包含点击诱饵，并且微调模型的性能优于一般的LLM。", "conclusion": "本文证实了微调模型在点击诱饵检测任务上的表现优于通用的大语言模型，特别是在资源较少的语种中。此外，对点击诱饵的语义特性进行了深入分析。"}}
{"id": "2507.14497", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14497", "abs": "https://arxiv.org/abs/2507.14497", "authors": ["Weimin Lyu", "Qingqiao Hu", "Kehan Qi", "Zhan Shi", "Wentao Huang", "Saumya Gupta", "Chao Chen"], "title": "Efficient Whole Slide Pathology VQA via Token Compression", "comment": null, "summary": "Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000\npixels, posing significant challenges for multimodal large language model\n(MLLM) due to long context length and high computational demands. Previous\nmethods typically focus on patch-level analysis or slide-level classification\nusing CLIP-based models with multi-instance learning, but they lack the\ngenerative capabilities needed for visual question answering (VQA). More recent\nMLLM-based approaches address VQA by feeding thousands of patch tokens directly\ninto the language model, which leads to excessive resource consumption. To\naddress these limitations, we propose Token Compression Pathology LLaVA\n(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token\ncompression. TCP-LLaVA introduces a set of trainable compression tokens that\naggregate visual and textual information through a modality compression module,\ninspired by the [CLS] token mechanism in BERT. Only the compressed tokens are\nforwarded to the LLM for answer generation, significantly reducing input length\nand computational cost. Experiments on ten TCGA tumor subtypes show that\nTCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing\ntraining resource consumption by a substantial margin.", "AI": {"tldr": "提出TCP-LLaVA，一种用于全幻灯片图像视觉问答任务的多模态大语言模型架构，通过压缩贴片令牌来减少资源消耗。", "motivation": "解决病理学中全幻灯片图像数据集计算资源消耗高这一使用传统MLLM方法难以解决的问题。", "method": "Structure", "result": "<tool_call>\ntokenCompressionPathologyLLaVA\n{\"tldr\": \"Proposes TCP-LLaVA, a multimodal large language model architecture for whole-slide image visual question answering which reduces resource consumption by compressing patch tokens.\", \"motivation\": \"The paper aims to solve the problem of high computational resource requirement for large visual datasets like whole-slide images in pathology using traditional MLLM methods.\", \"method\": \"TCP-LLaVA uses a trainable set of compression tokens to aggregate information and feed into the language model, inspired by the [CLS] token in BERT, which significantly reduces the input length and computational cost.\", \"result\": \"TCP-LLaVA outperformed existing MLLM baselines in VQA accuracy on ten TCGA tumor subtypes tests while decreasing resource consumption.\", \"conclusion\": \"The research successfully demonstrates that the proposed TCP-LLaVA model is effective in performing VQA tasks for WSIs with reduced resource usage.\"}\n</tool_call>", "conclusion": "研究成功证明，提出的TCP-LLaVA模型在减少资源使用的同时能够有效地完成WSIs的VQA任务。"}}
{"id": "2507.14355", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14355", "abs": "https://arxiv.org/abs/2507.14355", "authors": ["Jianfeng Zhu", "Ruoming Jin", "Karin G. Coifman"], "title": "Can LLMs Infer Personality from Real World Conversations?", "comment": "21 pages, 12 figures", "summary": "Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a\npromising approach for scalable personality assessment from open-ended\nlanguage. However, inferring personality traits remains challenging, and\nearlier work often relied on synthetic data or social media text lacking\npsychometric validity. We introduce a real-world benchmark of 555\nsemi-structured interviews with BFI-10 self-report scores for evaluating\nLLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini,\nMeta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item\nprediction and both zero-shot and chain-of-thought prompting for Big Five trait\ninference. All models showed high test-retest reliability, but construct\nvalidity was limited: correlations with ground-truth scores were weak (max\nPearson's $r = 0.27$), interrater agreement was low (Cohen's $\\kappa < 0.10$),\nand predictions were biased toward moderate or high trait levels.\nChain-of-thought prompting and longer input context modestly improved\ndistributional alignment, but not trait-level accuracy. These results\nunderscore limitations in current LLM-based personality inference and highlight\nthe need for evidence-based development for psychological applications.", "AI": {"tldr": "本研究展示了虽然大语言模型在零样本提示下对于性格特征预测时能够展示一定的效果，但与实测得分的对应性依旧有限，尤其是在个性特征水平准确性方面的表现不足。研究表明，需要进一步基于证据的方法来改进大语言模型在心理学应用中的表现。", "motivation": "本文提出了一种使用大语言模型进行个性特征评估的新基准，解决之前研究依靠合成数据或缺乏心理测量有效性的真实数据的问题。该研究聚焦于生成真人访谈数据集，用于评估大语言模型在个性评估中的有效性。", "method": "研究使用了三个最先进的大语言模型（GPT-4.1 Mini、Meta-LLaMA 和DeepSeek）来进行零样本提示和思考链提示下的大五特质推理评估。测试集包含了555个半结构化访谈，参与者自报了BFI-10项目性格量表得分。", "result": "研究表明，尽管大语言模型如GPT-4.1 Mini、Meta-LLaMA 和DeepSeek在个性评估方面显示出高复测可靠性，但在构建有效性方面表现有限。模型预测与真实分数的相关性较弱（最大皮尔森相关系数$r = 0.27$），且预测结果多偏向中等或较高的个性特征水平。通过思考链提示和延长输入上下文可以适度改善分布对齐，但不提高个性特征水平的准确性。研究强调了当前基于LLMs的个性推理的局限性，并指出需要基于证据的研发应用于心理评估。", "conclusion": "研究结论指出，当前的大语言模型在进行个性特征推理中存在显著局限，这一结果显示了在该领域内目前基于模型推理的挑战。未来的研究需聚焦于更为严谨的数据集设计以及更加精细化的技术改进以提高证明有效性，特别是在个性特征水平准确性上的提升。"}}
{"id": "2507.14500", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14500", "abs": "https://arxiv.org/abs/2507.14500", "authors": ["Zhiyuan Hua", "Dehao Yuan", "Cornelia Fermüller"], "title": "Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow", "comment": null, "summary": "This paper introduces a robust framework for motion segmentation and\negomotion estimation using event-based normal flow, tailored specifically for\nneuromorphic vision sensors. In contrast to traditional methods that rely\nheavily on optical flow or explicit depth estimation, our approach exploits the\nsparse, high-temporal-resolution event data and incorporates geometric\nconstraints between normal flow, scene structure, and inertial measurements.\nThe proposed optimization-based pipeline iteratively performs event\nover-segmentation, isolates independently moving objects via residual analysis,\nand refines segmentations using hierarchical clustering informed by motion\nsimilarity and temporal consistency. Experimental results on the EVIMO2v2\ndataset validate that our method achieves accurate segmentation and\ntranslational motion estimation without requiring full optical flow\ncomputation. This approach demonstrates significant advantages at object\nboundaries and offers considerable potential for scalable, real-time robotic\nand navigation applications.", "AI": {"tldr": "The paper presents a framework that uses event-based normal flow to achieve accurate motion segmentation and egomotion estimation, validated through experiments on the EVIMO2v2 dataset, showcasing advantages for robotics and navigation.", "motivation": "The motivation is to develop a robust framework that does not heavily rely on optical flow or explicit depth estimation, which are often computationally intensive. The approach is designed to take advantage of the unique characteristics of neuromorphic vision sensors.", "method": "This paper proposes a framework for motion segmentation and egomotion estimation using event-based normal flow, which relies on sparse, high-temporal-resolution event data and geometric constraints. It includes steps such as event over-segmentation, residual analysis for object isolation, and hierarchical clustering for segmentation refinement based on motion similarity and temporal consistency.", "result": "Experimental results on the EVIMO2v2 dataset show high accuracy in segmentation and translational motion estimation, without the need for full optical flow computation.", "conclusion": "The method highlights significant improvements in accuracy, especially at object boundaries, and shows promise for real-time applications in robotics and navigation."}}
{"id": "2507.14372", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.14372", "abs": "https://arxiv.org/abs/2507.14372", "authors": ["Albert Chen", "Manas Bundele", "Gaurav Ahlawat", "Patrick Stetz", "Zhitao Wang", "Qiang Fei", "Donghoon Jung", "Audrey Chu", "Bharadwaj Jayaraman", "Ayushi Panth", "Yatin Arora", "Sourav Jain", "Renjith Varma", "Alexey Ilin", "Iuliia Melnychuk", "Chelsea Chueh", "Joyan Sil", "Xiaofeng Wang"], "title": "Text-to-SQL for Enterprise Data Analytics", "comment": "11 pages, 8 figures, Workshop on Agentic AI for Enterprise at KDD '25", "summary": "The introduction of large language models has brought rapid progress on\nText-to-SQL benchmarks, but it is not yet easy to build a working enterprise\nsolution. In this paper, we present insights from building an internal chatbot\nthat enables LinkedIn's product managers, engineers, and operations teams to\nself-serve data insights from a large, dynamic data lake. Our approach features\nthree components. First, we construct a knowledge graph that captures\nup-to-date semantics by indexing database metadata, historical query logs,\nwikis, and code. We apply clustering to identify relevant tables for each team\nor product area. Second, we build a Text-to-SQL agent that retrieves and ranks\ncontext from the knowledge graph, writes a query, and automatically corrects\nhallucinations and syntax errors. Third, we build an interactive chatbot that\nsupports various user intents, from data discovery to query writing to\ndebugging, and displays responses in rich UI elements to encourage follow-up\nchats. Our chatbot has over 300 weekly users. Expert review shows that 53% of\nits responses are correct or close to correct on an internal benchmark set.\nThrough ablation studies, we identify the most important knowledge graph and\nmodeling components, offering a practical path for developing enterprise\nText-to-SQL solutions.", "AI": {"tldr": "", "motivation": "", "method": "", "result": "{\n  \"tldr\": \"The paper introduces a knowledge graph-based chatbot for LinkedIn, which assists internal teams with self-serving data insights from a large data lake. The system includes a Text-to-SQL agent and an interactive chatbot interface. It has over 300 weekly users with 53% of its responses correct or close to correct on an internal benchmark.\",\n  \"motivation\": \"The main motivation is to address the difficulty in building an effective enterprise solution using large language models for Text-to-SQL tasks. The authors aim to support internal teams at LinkedIn in querying a large, dynamic data lake efficiently.\",\n  \"method\": \"The method involves three components: a knowledge graph for capturing database semantics through clustering, a Text-to-SQL agent for query generation and correction, and an interactive chatbot for various user intents from data discovery to debugging.\",\n  \"result\": \"The chatbot has over 300 weekly users, and expert review indicates that 53% of its responses are correct or close to correct on an internal benchmark. The authors also identify key knowledge graph and model components through ablation studies.\",\n  \"conclusion\": \"The paper concludes that the proposed approach offers a practical path for developing enterprise Text-to-SQL solutions by integrating a knowledge graph, Text-to-SQL agent, and interactive chatbot which demonstrates good performance and user engagement for internal use at LinkedIn.\",\n  \"tool_used\": \"Structure function was used to break down the paper's content. Additional insights come from the original content. \"\n}", "conclusion": ""}}
{"id": "2507.14501", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14501", "abs": "https://arxiv.org/abs/2507.14501", "authors": ["Jiahui Zhang", "Yuelei Li", "Anpei Chen", "Muyu Xu", "Kunhao Liu", "Jianyuan Wang", "Xiao-Xiao Long", "Hanxue Liang", "Zexiang Xu", "Hao Su", "Christian Theobalt", "Christian Rupprecht", "Andrea Vedaldi", "Hanspeter Pfister", "Shijian Lu", "Fangneng Zhan"], "title": "Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey", "comment": "A project page associated with this survey is available at\n  https://fnzhan.com/projects/Feed-Forward-3D", "summary": "3D reconstruction and view synthesis are foundational problems in computer\nvision, graphics, and immersive technologies such as augmented reality (AR),\nvirtual reality (VR), and digital twins. Traditional methods rely on\ncomputationally intensive iterative optimization in a complex chain, limiting\ntheir applicability in real-world scenarios. Recent advances in feed-forward\napproaches, driven by deep learning, have revolutionized this field by enabling\nfast and generalizable 3D reconstruction and view synthesis. This survey offers\na comprehensive review of feed-forward techniques for 3D reconstruction and\nview synthesis, with a taxonomy according to the underlying representation\narchitectures including point cloud, 3D Gaussian Splatting (3DGS), Neural\nRadiance Fields (NeRF), etc. We examine key tasks such as pose-free\nreconstruction, dynamic 3D reconstruction, and 3D-aware image and video\nsynthesis, highlighting their applications in digital humans, SLAM, robotics,\nand beyond. In addition, we review commonly used datasets with detailed\nstatistics, along with evaluation protocols for various downstream tasks. We\nconclude by discussing open research challenges and promising directions for\nfuture work, emphasizing the potential of feed-forward approaches to advance\nthe state of the art in 3D vision.", "AI": {"tldr": " a survey on feed-forward methods for 3D reconstruction and view synthesis, addressing the limitations of traditional methods and highlighting their applications and potential future research directions.", "motivation": " traditional methods rely on computationally intensive iterative optimization, which limits their applicability in real-world scenarios. Recent advances in deep learning have enabled faster and more generalizable approaches.", "method": " feed-forward techniques for 3D reconstruction and view synthesis, including point cloud, 3D Gaussian Splatting (3DGS), Neural Radiance Fields (NeRF), etc.", "result": " a comprehensive review of the feed-forward techniques for 3D reconstruction and view synthesis, including key tasks such as pose-free reconstruction, dynamic 3D reconstruction, and 3D-aware image and video synthesis, and the applications in digital humans, SLAM, robotics, etc.", "conclusion": " discusses open research challenges and promising directions, emphasizing the potential of feed-forward approaches to advance the state of the art in 3D vision."}}
{"id": "2507.14374", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14374", "abs": "https://arxiv.org/abs/2507.14374", "authors": ["Sinchani Chakraborty", "Sudeshna Sarkar", "Pawan Goyal"], "title": "Error-Aware Curriculum Learning for Biomedical Relation Classification", "comment": "16 pages, 2 figures", "summary": "Relation Classification (RC) in biomedical texts is essential for\nconstructing knowledge graphs and enabling applications such as drug\nrepurposing and clinical decision-making. We propose an error-aware\nteacher--student framework that improves RC through structured guidance from a\nlarge language model (GPT-4o). Prediction failures from a baseline student\nmodel are analyzed by the teacher to classify error types, assign difficulty\nscores, and generate targeted remediations, including sentence rewrites and\nsuggestions for KG-based enrichment. These enriched annotations are used to\ntrain a first student model via instruction tuning. This model then annotates a\nbroader dataset with difficulty scores and remediation-enhanced inputs. A\nsecond student is subsequently trained via curriculum learning on this dataset,\nordered by difficulty, to promote robust and progressive learning. We also\nconstruct a heterogeneous biomedical knowledge graph from PubMed abstracts to\nsupport context-aware RC. Our approach achieves new state-of-the-art\nperformance on 4 of 5 PPI datasets and the DDI dataset, while remaining\ncompetitive on ChemProt.", "AI": {"tldr": "本研究提出了一种基于教师-学生框架的方法，通过大型语言模型提供结构化指导，以改善生物医学文本中的关系分类，并在多个数据集上实现先进性能。", "motivation": "本研究旨在改善生物医学文本中的关系分类，这对于构建知识图谱以及药物再利用和临床决策支持等应用至关重要。", "method": "本研究提出了一种基于教师-学生框架的错误感知关系分类（RC）方法，通过大型语言模型（GPT-4o）提供结构指导。教师模型分析学生模型的预测错误，分类错误类型，分配难度分数，并生成针对性的补救措施，如句子重写和知识图谱增强建议。这些增强的标注结果用于通过指令调优培训首个学生模型，之后该模型会为更大规模的数据集标注难度评分和增强输入。第二个学生模型通过基于难度顺序的课程学习进行培训，以促进稳健和逐步的学习进展。", "result": "该方法在4个PPI数据集中的5个以及DDI数据集上达到了新的最先进的性能，在ChemProt数据集上保持了竞争力。", "conclusion": "提出的错误感知教师-学生框架有助于改善生物医学文本中的关系分类性能，特别是在特定的数据集上取得了显著的性能提升。"}}
{"id": "2507.14505", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14505", "abs": "https://arxiv.org/abs/2507.14505", "authors": ["Jiahao Ma", "Tianyu Wang", "Miaomiao Liu", "David Ahmedt-Aristizabal", "Chuong Nguyen"], "title": "DCHM: Depth-Consistent Human Modeling for Multiview Detection", "comment": "multi-view detection, sparse-view reconstruction", "summary": "Multiview pedestrian detection typically involves two stages: human modeling\nand pedestrian localization. Human modeling represents pedestrians in 3D space\nby fusing multiview information, making its quality crucial for detection\naccuracy. However, existing methods often introduce noise and have low\nprecision. While some approaches reduce noise by fitting on costly multiview 3D\nannotations, they often struggle to generalize across diverse scenes. To\neliminate reliance on human-labeled annotations and accurately model humans, we\npropose Depth-Consistent Human Modeling (DCHM), a framework designed for\nconsistent depth estimation and multiview fusion in global coordinates.\nSpecifically, our proposed pipeline with superpixel-wise Gaussian Splatting\nachieves multiview depth consistency in sparse-view, large-scaled, and crowded\nscenarios, producing precise point clouds for pedestrian localization.\nExtensive validations demonstrate that our method significantly reduces noise\nduring human modeling, outperforming previous state-of-the-art baselines.\nAdditionally, to our knowledge, DCHM is the first to reconstruct pedestrians\nand perform multiview segmentation in such a challenging setting. Code is\navailable on the \\href{https://jiahao-ma.github.io/DCHM/}{project page}.", "AI": {"tldr": "The paper introduces Depth-Consistent Human Modeling (DCHM), an innovative framework for accurate and noise-free pedestrian localization, outperforming existing methods and setting a new benchmark in multiview pedestrian detection.", "motivation": "The motivation for the research is to eliminate reliance on human-labeled annotations and accurately model humans, addressing the noise introduced in previous pedestrian detection approaches and their struggle with generalizing to diverse scenes.", "method": "Depth-Consistent Human Modeling (DCHM) is introduced, which focuses on consistent depth estimation and multiview fusion in global coordinates using a pipeline with superpixel-wise Gaussian Splatting to manage multiview depth consistency in various challenging scenarios.", "result": "Validations demonstrate that the proposed method significantly reduces noise during human modeling and outperforms state-of-the-art approaches.", "conclusion": "DCHM is noted as a pioneering approach for reconstructing pedestrians and performing multiview segmentation effectively under challenging conditions."}}
{"id": "2507.14430", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14430", "abs": "https://arxiv.org/abs/2507.14430", "authors": ["Xiaolin Yan", "Yangxing Liu", "Jiazhang Zheng", "Chi Liu", "Mingyu Du", "Caisheng Chen", "Haoyang Liu", "Ming Ding", "Yuan Li", "Qiuping Liao", "Linfeng Li", "Zhili Mei", "Siyu Wan", "Li Li", "Ruyi Zhong", "Jiangling Yu", "Xule Liu", "Huihui Hu", "Jiameng Yue", "Ruohui Cheng", "Qi Yang", "Liangqing Wu", "Ke Zhu", "Chi Zhang", "Chufei Jing", "Yifan Zhou", "Yan Liang", "Dongdong Li", "Zhaohui Wang", "Bin Zhao", "Mingzhou Wu", "Mingzhong Zhou", "Peng Du", "Zuomin Liao", "Chao Dai", "Pengfei Liang", "Xiaoguang Zhu", "Yu Zhang", "Yu Gu", "Kun Pan", "Yuan Wu", "Yanqing Guan", "Shaojing Wu", "Zikang Feng", "Xianze Ma", "Peishan Cheng", "Wenjuan Jiang", "Jing Ba", "Huihao Yu", "Zeping Hu", "Yuan Xu", "Zhiwei Liu", "He Wang", "Zhenguo Lin", "Ming Liu", "Yanhong Meng"], "title": "X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display", "comment": "Technical Report", "summary": "Large language models (LLMs) have recently achieved significant advances in\nreasoning and demonstrated their advantages in solving challenging problems.\nYet, their effectiveness in the semiconductor display industry remains limited\ndue to a lack of domain-specific training and expertise. To bridge this gap, we\npresent X-Intelligence 3.0, the first high-performance reasoning model\nspecifically developed for the semiconductor display industry. This model is\ndesigned to deliver expert-level understanding and reasoning for the industry's\ncomplex challenges. Leveraging a carefully curated industry knowledge base, the\nmodel undergoes supervised fine-tuning and reinforcement learning to enhance\nits reasoning and comprehension capabilities. To further accelerate\ndevelopment, we implemented an automated evaluation framework that simulates\nexpert-level assessments. We also integrated a domain-specific\nretrieval-augmented generation (RAG) mechanism, resulting in notable\nperformance gains on benchmark datasets. Despite its relatively compact size of\n32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B\nacross multiple evaluations. This demonstrates its exceptional efficiency and\nestablishes it as a powerful solution to the longstanding reasoning challenges\nfaced by the semiconductor display industry.", "AI": {"tldr": "研究开发了专为半导体显示行业设计的X-Intelligence 3.0模型，该模型在多个评估中优于现有的大型模型，证明了其在该领域的卓越效率。", "motivation": "尽管大型语言模型在解决复杂问题方面表现出了显著的优势，但它们在半导体显示行业的有效性仍然有限，主要是由于缺乏特定领域的训练和专业知识。本研究旨在填补这一空白。", "method": "本研究开发了X-Intelligence 3.0，一种专为半导体显示行业设计的高性能推理模型。该模型通过监督微调和强化学习，利用精心策划的行业知识库来提升其推理和理解能力。同时，引入了自动评估框架和领域特定的检索增强生成机制来进一步优化性能。", "result": "尽管X-Intelligence 3.0的参数量相对较少，仅为32亿，但它在多种评估中优于现有的DeepSeek-R1-671B模型，显示了其出色的效率。", "conclusion": "X-Intelligence 3.0代表了解决半导体显示行业中长期存在的推理挑战的强大解决方案，展示了其在领域特定任务中的高效性能。"}}
{"id": "2507.14533", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14533", "abs": "https://arxiv.org/abs/2507.14533", "authors": ["Shuo Cao", "Nan Ma", "Jiayang Li", "Xiaohui Li", "Lihao Shao", "Kaiwen Zhu", "Yu Zhou", "Yuandong Pu", "Jiarui Wu", "Jiaquan Wang", "Bo Qu", "Wenhai Wang", "Yu Qiao", "Dajuin Yao", "Yihao Liu"], "title": "ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding", "comment": "43 pages, 31 figures, 13 tables", "summary": "The rapid advancement of educational applications, artistic creation, and\nAI-generated content (AIGC) technologies has substantially increased practical\nrequirements for comprehensive Image Aesthetics Assessment (IAA), particularly\ndemanding methods capable of delivering both quantitative scoring and\nprofessional understanding. Multimodal Large Language Model (MLLM)-based IAA\nmethods demonstrate stronger perceptual and generalization capabilities\ncompared to traditional approaches, yet they suffer from modality bias\n(score-only or text-only) and lack fine-grained attribute decomposition,\nthereby failing to support further aesthetic assessment. In this paper, we\npresent:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and\nExpert-Level Understanding capabilities; (2) ArtiMuse-10K, the first\nexpert-curated image aesthetic dataset comprising 10,000 images spanning 5 main\ncategories and 15 subcategories, each annotated by professional experts with\n8-dimensional attributes analysis and a holistic score. Both the model and\ndataset will be made public to advance the field.", "AI": {"tldr": "本文提出ArtiMuse，一种融合评分与专家级理解能力的图像美学评估模型，以及ArtiMuse-10K数据集，旨在解决图像美学评估中的模态偏差和细粒度属性分解的不足。", "motivation": "随着教育应用、艺术创作和AI生成内容技术的快速发展，对全面的图像美学评估(IAA)方法的需求增多，现有的MLLM方法虽然在感知和泛化能力上优于传统方法，但仍存在模态偏差与缺乏细粒度属性分解的问题。", "method": "本文提出了ArtiMuse，一种具有联合评分和专家级理解能力的MLLM基础图像美学评估模型，以及ArtiMuse-10K，一个精心策划的包含10,000张图片的图像美学数据集。", "result": "提出的方法文内未具体给出详细的结果，但强调了模型和数据集的创新性和对领域推进的预期价值。", "conclusion": "ArtiMuse模型和ArtiMuse-10K数据集的公开将促进图像美学评估领域的发展。"}}
{"id": "2507.14578", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14578", "abs": "https://arxiv.org/abs/2507.14578", "authors": ["Sachin Yadav", "Dominik Schlechtweg"], "title": "XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification", "comment": "8 pages", "summary": "We propose XL-DURel, a finetuned, multilingual Sentence Transformer model\noptimized for ordinal Word-in-Context classification. We test several loss\nfunctions for regression and ranking tasks managing to outperform previous\nmodels on ordinal and binary data with a ranking objective based on angular\ndistance in complex space. We further show that binary WiC can be treated as a\nspecial case of ordinal WiC and that optimizing models for the general ordinal\ntask improves performance on the more specific binary task. This paves the way\nfor a unified treatment of WiC modeling across different task formulations.", "AI": {"tldr": "提出XL-DURel模型，通过对有序词上下文分类的优化，超越了现有模型的表现，并实现了二元和有序任务的统一处理。", "motivation": "旨在通过优化有序单词上下文分类来改进现有模型，并探索二元和有序任务之间的关系。", "method": "提出了一种名为XL-DURel的微调多语言Sentence Transformer模型，专门用于优化有序词上下文分类。测试了几种不同的回归和排序任务的损失函数。", "result": "在基于复数空间角度距离的排序目标下，新的模型在有序和二元数据上超过了之前的模型。", "conclusion": "二元词上下文可以被视为有序词上下文的一个特例，对更通用的有序任务进行优化可以改进二元任务的表现，实现了不同任务形式下的统一词上下文建模方法。"}}
{"id": "2507.14543", "categories": ["cs.CV", "cs.CY", "cs.HC", "cs.LG", "I.4.6"], "pdf": "https://arxiv.org/pdf/2507.14543", "abs": "https://arxiv.org/abs/2507.14543", "authors": ["Sharanya Mukherjee", "Md Hishaam Akhtar", "Kannadasan R"], "title": "Real Time Captioning of Sign Language Gestures in Video Meetings", "comment": "7 pages, 2 figures, 1 table, Presented at ICCMDE 2021", "summary": "It has always been a rather tough task to communicate with someone possessing\na hearing impairment. One of the most tested ways to establish such a\ncommunication is through the use of sign based languages. However, not many\npeople are aware of the smaller intricacies involved with sign language. Sign\nlanguage recognition using computer vision aims at eliminating the\ncommunication barrier between deaf-mute and ordinary people so that they can\nproperly communicate with others. Recently the pandemic has left the whole\nworld shaken up and has transformed the way we communicate. Video meetings have\nbecome essential for everyone, even people with a hearing disability. In recent\nstudies, it has been found that people with hearing disabilities prefer to sign\nover typing during these video calls. In this paper, we are proposing a browser\nextension that will automatically translate sign language to subtitles for\neveryone else in the video call. The Large-scale dataset which contains more\nthan 2000 Word-Level ASL videos, which were performed by over 100 signers will\nbe used.", "AI": {"tldr": "本文提出了一种浏览器插件，用于自动将视频通话中的手语翻译成字幕，以促进聋哑人与普通人士之间的沟通。", "motivation": "鉴于全球疫情导致视频通话成为日常沟通方式，而聋哑人更倾向于在视频通话中使用手语而非打字，本文旨在通过技术手段消除聋哑人和其他人之间的沟通障碍。", "method": "本文提出了一种浏览器插件，该插件可以自动将手语翻译成视频通话中的字幕，以便所有参与者都能理解。该方法使用了包含超过2000个单词级ASL视频的大型数据集，这些视频由100多名手语者表演。", "result": "通过使用大型的数据集训练，该插件能够准确地将手语动作转化为文字字幕，实现了实时的沟通辅助功能。", "conclusion": "该浏览器插件能够实时将手语翻译成文字字幕，有助于促进聋哑人和其他人之间的有效沟通。"}}
{"id": "2507.14579", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14579", "abs": "https://arxiv.org/abs/2507.14579", "authors": ["Kester Wong", "Sahan Bulathwela", "Mutlu Cukurova"], "title": "Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models", "comment": "Accepted to appear in the workshop proceedings for the HEXED'25\n  workshop in the 26th International Conference on Artificial Intelligence in\n  Education 2025 (AIED 2025), 22 July 2025, Palermo, Italy. 5 pages", "summary": "Detecting collaborative problem solving (CPS) indicators from dialogue using\nmachine learning techniques is a significant challenge for the field of AI in\nEducation. Recent studies have explored the use of Bidirectional Encoder\nRepresentations from Transformers (BERT) models on transcription data to\nreliably detect meaningful CPS indicators. A notable advancement involved the\nmultimodal BERT variant, AudiBERT, which integrates speech and\nacoustic-prosodic audio features to enhance CPS diagnosis. Although initial\nresults demonstrated multimodal improvements, the statistical significance of\nthese enhancements remained unclear, and there was insufficient guidance on\nleveraging human-AI complementarity for CPS diagnosis tasks. This workshop\npaper extends the previous research by highlighting that the AudiBERT model not\nonly improved the classification of classes that were sparse in the dataset,\nbut it also had statistically significant class-wise improvements over the BERT\nmodel for classifications in the social-cognitive dimension. However, similar\nsignificant class-wise improvements over the BERT model were not observed for\nclassifications in the affective dimension. A correlation analysis highlighted\nthat larger training data was significantly associated with higher recall\nperformance for both the AudiBERT and BERT models. Additionally, the precision\nof the BERT model was significantly associated with high inter-rater agreement\namong human coders. When employing the BERT model to diagnose indicators within\nthese subskills that were well-detected by the AudiBERT model, the performance\nacross all indicators was inconsistent. We conclude the paper by outlining a\nstructured approach towards achieving human-AI complementarity for CPS\ndiagnosis, highlighting the crucial inclusion of model explainability to\nsupport human agency and engagement in the reflective coding process.", "AI": {"tldr": "该论文扩展了AudiBERT模型在检测协作问题解决(CPS)指标方面的研究，指出AudiBERT在社会认知维度上比BERT模型有显著提升，但在情感维度上没有显著改进。此外，大数据训练与模型召回率表现相关，而BERT模型精度与人类编码者的高度一致同意相关。文章最后提出了一种实现人工智能和人为补充的结构化方法，并强调了模型可解释性的重要性。", "motivation": "该研究旨在解决使用机器学习技术从对话中检测CPS指标的难题，进一步探讨在CPS诊断任务中如何充分利用人工智能和人为优势，并解决多重模态方法统计显著性的不明确问题。", "method": "该研究使用了AudiBERT模型，一种基于BERT的变体，能够集成语音和声学-音系音频特征，对CPS指标进行诊断。还进行了相关分析，探讨了训练数据规模与模型性能，以及BERT模型精度与人为编码者之间的一致性。", "result": "研究发现，AudiBERT在划分社会认知维度类别上比BERT有显著改进，但在情感维度上没有类似改进。更大的训练数据与模型的高召回率相关，且BERT模型的精度与人编码者的一致性高。但是使用BERT模型诊断在CPS诊断中在不同维度的效果不一致。", "conclusion": "文章提出了实现人工智能和人为补充以改进CPS诊断的结构化方法，强调了模型的可解释性对于支持人类在反思编码过程中的参与和能动性的重要性。"}}
{"id": "2507.14544", "categories": ["cs.CV", "cs.AI", "68T45 (Machine vision and scene understanding)", "I.2.10; I.4.8; H.3.1"], "pdf": "https://arxiv.org/pdf/2507.14544", "abs": "https://arxiv.org/abs/2507.14544", "authors": ["Sujata Gaihre", "Amir Thapa Magar", "Prasuna Pokharel", "Laxmi Tiwari"], "title": "Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025", "comment": "accepted to ImageCLEF 2025, to be published in the lab proceedings", "summary": "This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA\n2025 Challenge, which targets visual question answering (VQA) for\ngastrointestinal endoscopy. We adopt the Florence model-a large-scale\nmultimodal foundation model-as the backbone of our VQA pipeline, pairing a\npowerful vision encoder with a text encoder to interpret endoscopic images and\nproduce clinically relevant answers. To improve generalization, we apply\ndomain-specific augmentations that preserve medical features while increasing\ntraining diversity. Experiments on the KASVIR dataset show that fine-tuning\nFlorence yields accurate responses on the official challenge metrics. Our\nresults highlight the potential of large multimodal models in medical VQA and\nprovide a strong baseline for future work on explainability, robustness, and\nclinical integration. The code is publicly available at:\nhttps://github.com/TiwariLaxuu/VQA-Florence.git", "AI": {"tldr": "我们采用了Florence模型来处理胃肠内镜检查的VQA任务，通过领域特定的数据增强提高了模型的泛化能力，实验表明该方法在官方挑战指标上达到了准确的响应。", "motivation": "本文旨在描述我们对ImageCLEFmed MEDVQA 2025挑战中涉及胃肠内镜检查的视觉问题回答子任务的方法。我们的研究重点是通过现有技术为医学领域的VQA提供一个强健的基线，并探索大规模多模态模型在医疗VQA中的潜力。", "method": "我们采用了Florence模型作为视觉问题回答（VQA）管道的基础框架，该模型是一种大规模的多模态基础模型。我们结合了强大的视觉编码器和文本编码器来解析内镜图像并生成具有临床意义的答案。为了提高泛化能力，我们应用了特定领域的数据增强，这些增强保持了医学特征的同时增加了训练多样性。", "result": "在KASVIR数据集上的实验表明，对Florence进行微调可以产生在官方挑战指标上的准确响应。", "conclusion": "我们的研究结果证实了大规模多模态模型在医学VQA中的应用潜力，提供了一个用于未来解释性、鲁棒性和临床集成研究的强基线模型。代码已公开提供。"}}
{"id": "2507.14584", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14584", "abs": "https://arxiv.org/abs/2507.14584", "authors": ["Kester Wong", "Sahan Bulathwela", "Mutlu Cukurova"], "title": "Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption", "comment": "Accepted to appear in the workshop proceedings for the HEXED'25\n  workshop in the 26th International Conference on Artificial Intelligence in\n  Education 2025 (AIED 2025), 22 July 2025, Palermo, Italy. 6 pages, 2 figures", "summary": "The use of Bidirectional Encoder Representations from Transformers (BERT)\nmodel and its variants for classifying collaborative problem solving (CPS) has\nbeen extensively explored within the AI in Education community. However,\nlimited attention has been given to understanding how individual tokenised\nwords in the dataset contribute to the model's classification decisions.\nEnhancing the explainability of BERT-based CPS diagnostics is essential to\nbetter inform end users such as teachers, thereby fostering greater trust and\nfacilitating wider adoption in education. This study undertook a preliminary\nstep towards model transparency and explainability by using SHapley Additive\nexPlanations (SHAP) to examine how different tokenised words in transcription\ndata contributed to a BERT model's classification of CPS processes. The\nfindings suggested that well-performing classifications did not necessarily\nequate to a reasonable explanation for the classification decisions. Particular\ntokenised words were used frequently to affect classifications. The analysis\nalso identified a spurious word, which contributed positively to the\nclassification but was not semantically meaningful to the class. While such\nmodel transparency is unlikely to be useful to an end user to improve their\npractice, it can help them not to overrely on LLM diagnostics and ignore their\nhuman expertise. We conclude the workshop paper by noting that the extent to\nwhich the model appropriately uses the tokens for its classification is\nassociated with the number of classes involved. It calls for an investigation\ninto the exploration of ensemble model architectures and the involvement of\nhuman-AI complementarity for CPS diagnosis, since considerable human reasoning\nis still required for fine-grained discrimination of CPS subskills.", "AI": {"tldr": "本研究通过SHAP分析了不同标记化单词对BERT模型在CPS分类中的影响，发现良好分类的表现并不等同于合理解释，存在一些无意义的单词影响分类结果。", "motivation": "提升基于BERT的CPS诊断的可解释性对于更好地向最终用户（如老师）提供信息至关重要，这可以增强信任并促进教育领域的更广泛应用。", "method": "本研究采用SHapley Additive exPlanations (SHAP)方法来分析在转录数据中不同标记化单词如何影响BERT模型对协作问题解决（CPS）过程的分类。", "result": "研究表明，良好的分类表现不一定与合理的分类解释相关，某些标记化单词被频繁用来影响分类决策。发现了一个虽然对分类有正面影响但实际上不具有语义意义的标记化单词。", "conclusion": "研究结果揭示，模型在分类中是否恰当地使用了单词与类别数量相关，这提示未来需要探讨集成模型架构并重视人机互补来提升CPS诊断，因为精细区分CPS子技能仍然需要人类的推理。"}}
{"id": "2507.14549", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.14549", "abs": "https://arxiv.org/abs/2507.14549", "authors": ["Haotian Deng", "Chi Zhang", "Chen Wei", "Quanying Liu"], "title": "Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions", "comment": "Accepted by IJCNN 2025", "summary": "A fundamental challenge in affective cognitive science is to develop models\nthat accurately capture the relationship between external emotional stimuli and\nhuman internal experiences. While ANNs have demonstrated remarkable accuracy in\nfacial expression recognition, their ability to model inter-individual\ndifferences in human perception remains underexplored. This study investigates\nthe phenomenon of high perceptual variability-where individuals exhibit\nsignificant differences in emotion categorization even when viewing the same\nstimulus. Inspired by the similarity between ANNs and human perception, we\nhypothesize that facial expression samples that are ambiguous for ANN\nclassifiers also elicit divergent perceptual judgments among human observers.\nTo examine this hypothesis, we introduce a novel perceptual boundary sampling\nmethod to generate facial expression stimuli that lie along ANN decision\nboundaries. These ambiguous samples form the basis of the varEmotion dataset,\nconstructed through large-scale human behavioral experiments. Our analysis\nreveals that these ANN-confusing stimuli also provoke heightened perceptual\nuncertainty in human participants, highlighting shared computational principles\nin emotion perception. Finally, by fine-tuning ANN representations using\nbehavioral data, we achieve alignment between ANN predictions and both\ngroup-level and individual-level human perceptual patterns. Our findings\nestablish a systematic link between ANN decision boundaries and human\nperceptual variability, offering new insights into personalized modeling of\nemotional interpretation.", "AI": {"tldr": "研究表明，ANN难以分类的情绪刺激物导致人类参与者有更大的不确定性，这表明ANN决策边界与人类情绪感知变异之间有系统性的关联。", "motivation": "本研究旨在调查高感知变异性的现象，特别是当个体观看相同的刺激物时却表现出明显的情绪分类差异。", "method": "通过引入一种新的感知边界采样方法来生成面部表情刺激物，这些刺激物位于ANN决策边界上，从而创建了varEmotion数据集。", "result": "这些让ANN困惑的刺激物也引发了人类参与者更高的感知不确定性，表明了情绪感知中的计算原则的共同性。", "conclusion": "研究结果建立了ANN决策边界与人类感知变异性的系统联系，为情感解释的个性化建模提供了新的见解。"}}
{"id": "2507.14590", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14590", "abs": "https://arxiv.org/abs/2507.14590", "authors": ["Łukasz Radliński", "Mateusz Guściora", "Jan Kocoń"], "title": "Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification", "comment": "International Conference on Computational Science 2025", "summary": "Numerous domain-specific machine learning tasks struggle with data scarcity\nand class imbalance. This paper systematically explores data augmentation\nmethods for NLP, particularly through large language models like GPT. The\npurpose of this paper is to examine and evaluate whether traditional methods\nsuch as paraphrasing and backtranslation can leverage a new generation of\nmodels to achieve comparable performance to purely generative methods. Methods\naimed at solving the problem of data scarcity and utilizing ChatGPT were\nchosen, as well as an exemplary dataset. We conducted a series of experiments\ncomparing four different approaches to data augmentation in multiple\nexperimental setups. We then evaluated the results both in terms of the quality\nof generated data and its impact on classification performance. The key\nfindings indicate that backtranslation and paraphrasing can yield comparable or\neven better results than zero and a few-shot generation of examples.", "AI": {"tldr": "此论文探讨了NLP领域中通过数据增强方法解决数据稀缺和类别不平衡问题，特别是使用了大型语言模型如GPT。研究表明，传统的改写和回译方法可以与纯生成方法媲美，甚至在某些情况下表现更好。", "motivation": "解决NLP领域的数据稀缺和类别不平衡问题，评估通过大型语言模型实现的传统数据增强方法的有效性。", "method": "选取了解决数据稀缺并利用ChatGPT的方法，并选取示例数据集，进行了一系列对比四种不同数据增强方法的实验。", "result": "实验结果表明，回译和改写方法可以与从零或少量示例生成的方法相媲美，甚至在某些情况下表现更好。", "conclusion": "传统数据增强方法（如改写和回译）即使在最新的语言模型背景下，依旧能产生高质量的数据，并可能更有效地提高分类性能。"}}
{"id": "2507.14553", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.14553", "abs": "https://arxiv.org/abs/2507.14553", "authors": ["Xiaoran Wu"], "title": "Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance", "comment": null, "summary": "Clutter in photos is a distraction preventing photographers from conveying\nthe intended emotions or stories to the audience. Photography amateurs\nfrequently include clutter in their photos due to unconscious negligence or the\nlack of experience in creating a decluttered, aesthetically appealing scene for\nshooting. We are thus motivated to develop a camera guidance system that\nprovides solutions and guidance for clutter identification and removal. We\nestimate and visualize the contribution of objects to the overall aesthetics\nand content of a photo, based on which users can interactively identify\nclutter. Suggestions on getting rid of clutter, as well as a tool that removes\ncluttered objects computationally, are provided to guide users to deal with\ndifferent kinds of clutter and improve their photographic work. Two technical\nnovelties underpin interactions in our system: a clutter distinguishment\nalgorithm with aesthetics evaluations for objects and an iterative image\ninpainting algorithm based on generative adversarial nets that reconstructs\nmissing regions of removed objects for high-resolution images. User studies\ndemonstrate that our system provides flexible interfaces and accurate\nalgorithms that allow users to better identify distractions and take higher\nquality images within less time.", "AI": {"tldr": "本文提出了一种相机引导系统，帮助用户识别并移除照片中的杂乱，从而改进摄影效果，由两种算法支撑，试验表明系统有效提高照片质量并节省时间。", "motivation": "照片中的杂乱分散了摄影师向观众传达意图情感或故事的注意力，这一现象促使我们开发了一种相机指导系统，旨在提供杂乱识别和去除的解决方案及指导。", "method": "我们开发了一种相机指南系统，该系统通过估计和可视化物体对照片整体美学和内容的贡献来帮助用户交互式地识别杂乱。系统提供了去除杂乱的建议以及一个计算去除杂乱对象的工具。该系统基于两个技术新颖点：一种结合美学评估的杂乱区分算法和一个基于生成对抗网络的迭代图像修复算法，用于重建高分辨率图像中被移除对象的缺失区域。", "result": "实验结果显示，系统不仅提供了准确的算法，还给用户提供了一个灵活的交互界面，使得用户能够快速而有效地识别并处理照片中的杂乱，改善拍摄效果。", "conclusion": "用户研究表明，该系统提供了灵活的界面和准确的算法，有助于用户更好地识别分散注意力的元素，并在更短的时间内拍摄出质量更高的照片。"}}
{"id": "2507.14615", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14615", "abs": "https://arxiv.org/abs/2507.14615", "authors": ["Fred Mutisya", "Shikoh Gitau", "Christine Syovata", "Diana Oigara", "Ibrahim Matende", "Muna Aden", "Munira Ali", "Ryan Nyotu", "Diana Marion", "Job Nyangena", "Nasubo Ongoma", "Keith Mbae", "Elizabeth Wamicha", "Eric Mibuari", "Jean Philbert Nsengemana", "Talkmore Chidede"], "title": "Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper", "comment": "29 pages, 6 figs, 6 tables. Companion methods paper forthcoming", "summary": "Large Language Models(LLMs) hold promise for improving healthcare access in\nlow-resource settings, but their effectiveness in African primary care remains\nunderexplored. We present a methodology for creating a benchmark dataset and\nevaluation framework focused on Kenyan Level 2 and 3 clinical care. Our\napproach uses retrieval augmented generation (RAG) to ground clinical questions\nin Kenya's national guidelines, ensuring alignment with local standards. These\nguidelines were digitized, chunked, and indexed for semantic retrieval. Gemini\nFlash 2.0 Lite was then prompted with guideline excerpts to generate realistic\nclinical scenarios, multiple-choice questions, and rationale based answers in\nEnglish and Swahili. Kenyan physicians co-created and refined the dataset, and\na blinded expert review process ensured clinical accuracy, clarity, and\ncultural appropriateness. The resulting Alama Health QA dataset includes\nthousands of regulator-aligned question answer pairs across common outpatient\nconditions. Beyond accuracy, we introduce evaluation metrics that test clinical\nreasoning, safety, and adaptability such as rare case detection (Needle in the\nHaystack), stepwise logic (Decision Points), and contextual adaptability.\nInitial results reveal significant performance gaps when LLMs are applied to\nlocalized scenarios, consistent with findings that LLM accuracy is lower on\nAfrican medical content than on US-based benchmarks. This work offers a\nreplicable model for guideline-driven, dynamic benchmarking to support safe AI\ndeployment in African health systems.", "AI": {"tldr": "该研究提出了一种方法，用于创建基于肯尼亚国家指南的医疗场景和评估框架，旨在评估LLMs在非洲医疗场景中的表现，发现存在显著表现差距。", "motivation": "大规模语言模型（LLMs）对于改善低资源环境下的医疗服务质量显示出潜力，但其在非洲初级护理中的效果尚未得到充分研究。", "method": "该论文提出了一种用于创建基准数据集和评估框架的方法，专注于肯尼亚二级和三级临床护理。该方法使用检索增强生成（RAG）将临床问题与肯尼亚的国家指南相联系，以确保符合当地标准。这些指南被数字化、分块并进行语义检索。Gemini Flash 2.0 Lite 被引导使用指南摘录生成现实的临床场景、多项选择题及基于理由的答案，语言为英语和斯瓦希里语。该数据集由肯尼亚医生合作创建和细化，并经盲审专家评审，确保临床准确性、清晰度和文化适当性。", "result": "研究结果揭示了将LLMs应用于本地化场景时存在显著的表现差距，这与LLMs在非洲医学内容上的准确性低于美国基准的发现相一致。", "conclusion": "该工作提供了一个可复制的指南驱动型动态基准测试模型，以支持在非洲健康系统中安全地部署AI。"}}
{"id": "2507.14555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14555", "abs": "https://arxiv.org/abs/2507.14555", "authors": ["Jintang Xue", "Ganning Zhao", "Jie-En Yao", "Hong-En Chen", "Yue Hu", "Meida Chen", "Suya You", "C. -C. Jay Kuo"], "title": "Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions", "comment": null, "summary": "Understanding 3D scenes goes beyond simply recognizing objects; it requires\nreasoning about the spatial and semantic relationships between them. Current 3D\nscene-language models often struggle with this relational understanding,\nparticularly when visual embeddings alone do not adequately convey the roles\nand interactions of objects. In this paper, we introduce Descrip3D, a novel and\npowerful framework that explicitly encodes the relationships between objects\nusing natural language. Unlike previous methods that rely only on 2D and 3D\nembeddings, Descrip3D enhances each object with a textual description that\ncaptures both its intrinsic attributes and contextual relationships. These\nrelational cues are incorporated into the model through a dual-level\nintegration: embedding fusion and prompt-level injection. This allows for\nunified reasoning across various tasks such as grounding, captioning, and\nquestion answering, all without the need for task-specific heads or additional\nsupervision. When evaluated on five benchmark datasets, including ScanRefer,\nMulti3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms\nstrong baseline models, demonstrating the effectiveness of language-guided\nrelational representation for understanding complex indoor scenes.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.14640", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14640", "abs": "https://arxiv.org/abs/2507.14640", "authors": ["Eric Xia", "Jugal Kalita"], "title": "Linear Relational Decoding of Morphology in Language Models", "comment": null, "summary": "A two-part affine approximation has been found to be a good approximation for\ntransformer computations over certain subject object relations. Adapting the\nBigger Analogy Test Set, we show that the linear transformation Ws, where s is\na middle layer representation of a subject token and W is derived from model\nderivatives, is also able to accurately reproduce final object states for many\nrelations. This linear technique is able to achieve 90% faithfulness on\nmorphological relations, and we show similar findings multi-lingually and\nacross models. Our findings indicate that some conceptual relationships in\nlanguage models, such as morphology, are readily interpretable from latent\nspace, and are sparsely encoded by cross-layer linear transformations.", "AI": {"tldr": "本文发现，通过适应更大的类比测试集，线性变换Ws能够准确预测许多关系下的最终对象状态，特别是在形态学关系上达90%的保真度。这表明语言模型中的一些概念关系在潜在空间中是可解释的，并通过跨层线性转换稀疏编码。", "motivation": "研究旨在探索语言模型中潜在空间的表示方式，尤其是对于概念关系的线性变换是否可以准确重建对象状态。", "method": "使用适应性的Bigger Analogy Test Set，通过线性变换Ws（s代表主体标记的中间层表示，W从模型导数得出）来预测最终对象状态。", "result": "该线性技术在形态学关系上实现了90%的保真度，并且在多语言和模型上获得类似结果。", "conclusion": "语言模型中的部分概念关系，如形态学关系，在潜在空间中有可解释性，可通过跨层线性变换稀疏编码表示。"}}
{"id": "2507.14559", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14559", "abs": "https://arxiv.org/abs/2507.14559", "authors": ["Zixuan Hu", "Xiaotong Li", "Shixiang Tang", "Jun Liu", "Yichun Hu", "Ling-Yu Duan"], "title": "LEAD: Exploring Logit Space Evolution for Model Selection", "comment": "Accepted by CVPR 2024", "summary": "The remarkable success of pretrain-then-finetune paradigm has led to a\nproliferation of available pre-trained models for vision tasks. This surge\npresents a significant challenge in efficiently choosing the most suitable\npre-trained models for downstream tasks. The critical aspect of this challenge\nlies in effectively predicting the model transferability by considering the\nunderlying fine-tuning dynamics. Existing methods often model fine-tuning\ndynamics in feature space with linear transformations, which do not precisely\nalign with the fine-tuning objective and fail to grasp the essential\nnonlinearity from optimization. To this end, we present LEAD, a\nfinetuning-aligned approach based on the network output of logits. LEAD\nproposes a theoretical framework to model the optimization process and derives\nan ordinary differential equation (ODE) to depict the nonlinear evolution\ntoward the final logit state. Additionally, we design a class-aware\ndecomposition method to consider the varying evolution dynamics across classes\nand further ensure practical applicability. Integrating the closely aligned\noptimization objective and nonlinear modeling capabilities derived from the\ndifferential equation, our method offers a concise solution to effectively\nbridge the optimization gap in a single step, bypassing the lengthy fine-tuning\nprocess. The comprehensive experiments on 24 supervised and self-supervised\npre-trained models across 10 downstream datasets demonstrate impressive\nperformances and showcase its broad adaptability even in low-data scenarios.", "AI": {"tldr": "本文提出了LEAD方法，通过一个理论框架和常微分方程推导成功解决了现有方法在预测模型迁移能力方面的局限性，实验表明该方法具有优秀的性能和广泛的适用性。", "motivation": "预训练-微调范式的成功导致了用于视觉任务的预训练模型数量激增，这给选择最合适的预训练模型带来了挑战。而现有方法通常在特征空间中使用线性变换来建模微调动力学，这与微调目标不精确对齐，也无法捕捉到优化过程中的非线性特征。", "method": "LEAD采用基于网络输出logits的finetuning对齐方法，提出了一种理论框架来建模优化过程，并推导出一个常微分方程（ODE）来描述向最终logit状态的非线性演化过程。此外，还设计了一种类感知分解方法来考虑不同类之间的演化动力的不同。", "result": "在24个监督学习和自监督预训练模型上进行了全面实验，覆盖了10个下游数据集，展示了出色的性能并证明了其在低数据场景下的广泛适用性。", "conclusion": "提出的方法提供了一个简洁的解决方案，有效地消除了优化差距，绕过了漫长的微调过程，证明了其高性能和广泛适用性。"}}
{"id": "2507.14649", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14649", "abs": "https://arxiv.org/abs/2507.14649", "authors": ["Minsuh Joo", "Hyunsoo Cho"], "title": "Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs", "comment": null, "summary": "Despite the outstanding performance of large language models (LLMs) across\nvarious NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate\nresponses--remains as a critical problem as it can be directly connected to a\ncrisis of building safe and reliable LLMs. Uncertainty estimation is primarily\nused to measure hallucination levels in LLM responses so that correct and\nincorrect answers can be distinguished clearly. This study proposes an\neffective uncertainty estimation approach, \\textbf{Cl}ust\\textbf{e}ring-based\nsem\\textbf{an}tic con\\textbf{s}ist\\textbf{e}ncy (\\textbf{Cleanse}). Cleanse\nquantifies the uncertainty with the proportion of the intra-cluster consistency\nin the total consistency between LLM hidden embeddings which contain adequate\nsemantic information of generations, by employing clustering. The effectiveness\nof Cleanse for detecting hallucination is validated using four off-the-shelf\nmodels, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two\nquestion-answering benchmarks, SQuAD and CoQA.", "AI": {"tldr": "研究提出了一种新的不确定性估计方法Cleanse，该方法利用聚类分析来量化大型语言模型隐藏嵌入中的语义一致性，从而区分准确和不准确的回答。", "motivation": "大型语言模型在生成回答时存在幻觉现象，即生成不准确的响应，这是一个重要的安全问题。为此，研究提出了一种有效的不确定性度量方法。", "method": "使用聚类方法对大型语言模型（LLMs）生成的隐藏嵌入的语义一致性进行量化，以评估不确定性。这种方法称作Clustering-based semantic consistency（Cleanse）。", "result": "通过使用四个现成的模型（LLaMA-7B, LLaMA-13B, LLaMA2-7B 和 Mistral-7B）和两个问答基准（SQuAD 和 CoQA）验证了Cleanse在检测幻觉现象的有效性。", "conclusion": "实验结果表明，Cleanse是一种有效的方法，能够帮助检测和减轻大型语言模型中的幻觉现象，增强其可靠性和安全性。"}}
{"id": "2507.14575", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14575", "abs": "https://arxiv.org/abs/2507.14575", "authors": ["Andrea Moschetto", "Lemuel Puglisi", "Alec Sargood", "Pierluigi Dell'Acqua", "Francesco Guarnera", "Sebastiano Battiato", "Daniele Ravì"], "title": "Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation", "comment": null, "summary": "Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image\ncontrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering\ndistinct diagnostic insights. However, acquiring all desired modalities\nincreases scan time and cost, motivating research into computational methods\nfor cross-modal synthesis. To address this, recent approaches aim to synthesize\nmissing MRI contrasts from those already acquired, reducing acquisition time\nwhile preserving diagnostic quality. Image-to-image (I2I) translation provides\na promising framework for this task. In this paper, we present a comprehensive\nbenchmark of generative models$\\unicode{x2013}$specifically, Generative\nAdversarial Networks (GANs), diffusion models, and flow matching (FM)\ntechniques$\\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All\nframeworks are implemented with comparable settings and evaluated on three\npublicly available MRI datasets of healthy adults. Our quantitative and\nqualitative analyses show that the GAN-based Pix2Pix model outperforms\ndiffusion and FM-based methods in terms of structural fidelity, image quality,\nand computational efficiency. Consistent with existing literature, these\nresults suggest that flow-based models are prone to overfitting on small\ndatasets and simpler tasks, and may require more data to match or surpass GAN\nperformance. These findings offer practical guidance for deploying I2I\ntranslation techniques in real-world MRI workflows and highlight promising\ndirections for future research in cross-modal medical image synthesis. Code and\nmodels are publicly available at\nhttps://github.com/AndreaMoschetto/medical-I2I-benchmark.", "AI": {"tldr": "本文展示了基于GAN的Pix2Pix模型在T1w到T2w二维MRI转换中的优越性能，并为未来的研究和实际部署提供了指导。", "motivation": "随着MRI扫描时间增加和成本上升，研究计算方法合成缺失的MRI对比度成为一种减少采集时间的同时保持诊断质量的方式。通过图像对图像（I2I）转换，能够实现这一目标。", "method": "本研究展示了生成模型，包括生成对抗网络（GANs）、扩散模型和流匹配（FM）技术，在T1加权到T2加权（T1w到T2w）二维MRI I2I转换中的基准。所有框架在三个可供公众访问的健康成年人MRI数据集上使用相似的设置进行了实现和评估。", "result": "定量和定性分析表明，基于GAN的Pix2Pix模型在结构保真度、图像质量和计算效率方面优于扩散和FM方法。同时证明，基于流模型倾向于在小数据集和简单任务上过拟合，可能需要更多的数据才能匹配或超越GAN的性能。", "conclusion": "研究结果为在MRI工作流中部署I2I转换技术提供了实际指导，并强调了未来在跨模式医学图像合成研究中的方向。开源代码和模型在给出的链接中可以访问。"}}
{"id": "2507.14664", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14664", "abs": "https://arxiv.org/abs/2507.14664", "authors": ["Wannaphong Phatthiyaphaibun", "Can Udomcharoenchaikit", "Pakpoom Singkorapoom", "Kunat Pipatanakul", "Ekapol Chuangsuwanich", "Peerat Limkonchotiwat", "Sarana Nutanong"], "title": "Mangosteen: An Open Thai Corpus for Language Model Pretraining", "comment": "Work in Progress.All artifacts in this papers:\n  https://huggingface.co/collections/aisingapore/wangchanlion-v3-687a362d8f0ea2fe4077c6b3", "summary": "Pre-training data shapes a language model's quality, but raw web text is\nnoisy and demands careful cleaning. Existing large-scale corpora rely on\nEnglish-centric or language-agnostic pipelines whose heuristics do not capture\nThai script or cultural nuances, leaving risky material such as gambling\ncontent untreated. Prior Thai-specific efforts customize pipelines or build new\nones, yet seldom release their data or document design choices, hindering\nreproducibility and raising the question of how to construct a transparent,\nhigh-quality Thai corpus. We introduce Mangosteen: a 47 billion-token Thai\ncorpus built through a Thai-adapted Dolma pipeline that includes custom\nrule-based language ID, revised C4/Gopher quality filters, and Thai-trained\ncontent filters, plus curated non-web sources such as Wikipedia, Royal Gazette\ntexts, OCR-extracted books, and CC-licensed YouTube subtitles. Systematic\nablations using GPT-2 show the pipeline trims CommonCrawl from 202M to 25M\ndocuments while raising SEA-HELM NLG from 3 to 11; an 8B-parameter SEA-LION\nmodel continually pre-trained on Mangosteen then surpasses SEA-LION-v3 and\nLlama-3.1 by about four points on Thai benchmarks. We release the full pipeline\ncode, cleaning manifests, corpus snapshot, and all checkpoints, providing a\nfully reproducible foundation for future Thai and regional LLM research.", "AI": {"tldr": "研究者们构建了含有470亿个token的Mangosteen泰文语料库，使用改进的Dolma管道，并公开管道、清理记录、语料库快照和检查站，提升了语言模型的质量。", "motivation": "现有的大规模语料库依赖于以英文为中心或语言无关的管线，这些管线的启发法无法捕捉泰国语言特点或文化细微之处，因此需要构建一个透明、高质量的泰国语语料库。", "method": "通过适应泰国语言的Dolma管线构建了Mangosteen：一个47 billion-token的泰语文本语料库。该管线包括自定义的基于规则的语言ID、改进的C4/Gopher质量过滤器、泰国训练的内容过滤器，以及从维基百科、皇家公报文本、OCR提取的书籍和CC许可的YouTube字幕等非网络来源进行筛选。", "result": "通过使用GPT-2的系统消融实验，该管线将CommonCrawl从202M文件减少到25M文件，同时将SEA-HELM NLG分数从3提高到11；一个在Mangosteen上持续预训练的8B参数SEA-LION模型在泰国基准测试中超过了SEA-LION-v3和Llama-3.1约4分。", "conclusion": "全栈的管线代码、清洗清单、语料库快照和所有检查点都已经公开发布，为未来的泰国和区域LLM研究提供了一个可完全复制的基础。"}}
{"id": "2507.14587", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14587", "abs": "https://arxiv.org/abs/2507.14587", "authors": ["Merjem Bećirović", "Amina Kurtović", "Nordin Smajlović", "Medina Kapo", "Amila Akagić"], "title": "Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX", "comment": null, "summary": "Medical imaging plays a vital role in early disease diagnosis and monitoring.\nSpecifically, blood microscopy offers valuable insights into blood cell\nmorphology and the detection of hematological disorders. In recent years, deep\nlearning-based automated classification systems have demonstrated high\npotential in enhancing the accuracy and efficiency of blood image analysis.\nHowever, a detailed performance analysis of specific deep learning frameworks\nappears to be lacking. This paper compares the performance of three popular\ndeep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in\nclassifying blood cell images from the publicly available BloodMNIST dataset.\nThe study primarily focuses on inference time differences, but also\nclassification performance for different image sizes. The results reveal\nvariations in performance across frameworks, influenced by factors such as\nimage resolution and framework-specific optimizations. Classification accuracy\nfor JAX and PyTorch was comparable to current benchmarks, showcasing the\nefficiency of these frameworks for medical image classification.", "AI": {"tldr": "通过比较TensorFlow、PyTorch和JAX在血细胞图像分类中的性能，突出JAX和PyTorch的有效性。", "motivation": "尽管深度学习在提高血液图像分析的准确性和效率方面显示出巨大潜力，但具体深度学习框架的性能分析仍有不足。", "method": "比较了TensorFlow(Keras), PyTorch和JAX这三种流行的深度学习框架在处理BloodMNIST数据集中的血细胞图像分类任务中的表现。", "result": "研究揭示了不同框架之间的性能差异，这种差异受到图像分辨率和框架特有优化的影响。JAX和PyTorch的分类准确率与现有基准相当，表现出它们在医疗图像分类中的高效性。", "conclusion": "该研究表明，在处理医学图像分类任务时，JAX和PyTorch具有高效的性能，可与当前基准相匹敌。"}}
{"id": "2507.14681", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14681", "abs": "https://arxiv.org/abs/2507.14681", "authors": ["Vinicius Anjos de Almeida", "Vinicius de Camargo", "Raquel Gómez-Bravo", "Egbert van der Haring", "Kees van Boven", "Marcelo Finger", "Luis Fernandez Lopez"], "title": "Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care", "comment": "To be submitted to peer-reviewed journal. 33 pages, 10 figures\n  (including appendix), 15 tables (including appendix). For associated code\n  repository, see https://github.com/almeidava93/llm-as-code-selectors-paper", "summary": "Background: Medical coding structures healthcare data for research, quality\nmonitoring, and policy. This study assesses the potential of large language\nmodels (LLMs) to assign ICPC-2 codes using the output of a domain-specific\nsearch engine.\n  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each\nannotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's\ntext-embedding-3-large) retrieved candidates from 73,563 labeled concepts.\nThirty-three LLMs were prompted with each query and retrieved results to select\nthe best-matching ICPC-2 code. Performance was evaluated using F1-score, along\nwith token usage, cost, response time, and format adherence.\n  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top\nperformers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever\noptimization can improve performance by up to 4 points. Most models returned\nvalid codes in the expected format, with reduced hallucinations. Smaller models\n(<3B) struggled with formatting and input length.\n  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even\nwithout fine-tuning. This work offers a benchmark and highlights challenges,\nbut findings are limited by dataset scope and setup. Broader, multilingual,\nend-to-end evaluations are needed for clinical validation.", "AI": {"tldr": "大型语言模型（LLMs）在为巴西葡萄牙语的临床表达式分配ICPC-2代码方面显示出强大的自动化潜力，无需进行微调。最佳模型如gpt-4.5-preview、o3和gemini-2.5-pro达到了超过0.85的F1分数。", "motivation": "大型语言模型在为巴西葡萄牙语的临床表达式分配ICPC-2代码方面显示出强大的自动化潜力，无需进行微调。最佳模型如gpt-4.5-preview、o3和gemini-2.5-pro达到了超过0.85的F1分数。研究评估了大型语言模型在使用特定领域搜索引擎的输出来分配ICPC-2编码方面的潜力，这种编码在医疗保健数据中用于研究、质量监控和政策制定。使用包含437个巴西葡萄牙语临床表达式的标签数据集，每个表达式都标记了ICPC-2代码。利用OpenAI的text-embedding-3-large检索73,563个标签的概念，对33个语言模型进行提示和检索结果，以选择最佳匹配的ICPC-2代码。28个模型的F1分数大于0.8，其中10个模型超过0.85。最佳模型包括gpt-4.5-preview, o3, 和gemini-2.5-pro。检索器的优化可以显著提升性能。大多数模型返回正确的代码格式，较小模型（<3B）在格式化和输入长度上存在问题。虽然缺少端到端的临床验证，LLMs在不进行微调的情况下展示了程序自动化的潜力。此工作提供了基准测试，但结果受数据集范围和设置影响，需要更广泛的多语言评估。", "method": "Structure", "result": "{\n  \"tldr\": \"大型语言模型（LLMs）在为巴西葡萄牙语的临床表达式分配ICPC-2代码方面显示出强大的自动化潜力，无需进行微调。最佳模型如gpt-4.5-preview、o3和gemini-2.5-pro达到了超过0.85的F1分数。\",\n  \"motivation\": \"研究评估了大型语言模型在使用特定领域搜索引擎的输出来分配ICPC-2编码方面的潜力，这种编码在医疗保健数据中用于研究、质量监控和政策制定。\",\n  \"method\": \"使用包含437个巴西葡萄牙语临床表达式的标签数据集，每个表达式都标记了ICPC-2代码。利用OpenAI的text-embedding-3-large检索73,563个标签的概念，对33个语言模型进行提示和检索结果，以选择最佳匹配的ICPC-2代码。\",\n  \"result\": \"28个模型的F1分数大于0.8，其中10个模型超过0.85。最佳模型包括gpt-4.5-preview, o3, 和gemini-2.5-pro。检索器的优化可以显著提升性能。大多数模型返回正确的代码格式，较小模型（<3B）在格式化和输入长度上存在问题。\",\n  \"conclusion\": \"虽然缺少端到端的临床验证，LLMs在不进行微调的情况下展示了程序自动化的潜力。此工作提供了基准测试，但结果受数据集范围和设置影响，需要更广泛的多语言评估。\\n\"}", "conclusion": "虽然缺少端到端的临床验证，LLMs在不进行微调的情况下展示了程序自动化的潜力。此工作提供了基准测试，但结果受数据集范围和设置影响，需要更广泛的多语言评估。"}}
{"id": "2507.14596", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14596", "abs": "https://arxiv.org/abs/2507.14596", "authors": ["Doriand Petit", "Steve Bourgeois", "Vincent Gay-Bellile", "Florian Chabot", "Loïc Barthe"], "title": "DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF", "comment": "Published at ICCV'25", "summary": "3D semantic segmentation provides high-level scene understanding for\napplications in robotics, autonomous systems, \\textit{etc}. Traditional methods\nadapt exclusively to either task-specific goals (open-vocabulary segmentation)\nor scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the\nfirst method addressing the broader problem of 3D Open-Vocabulary Sub-concepts\nDiscovery, which aims to provide a 3D semantic segmentation that adapts to both\nthe scene and user queries. We build DiSCO-3D on Neural Fields representations,\ncombining unsupervised segmentation with weak open-vocabulary guidance. Our\nevaluations demonstrate that DiSCO-3D achieves effective performance in\nOpen-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in\nthe edge cases of both open-vocabulary and unsupervised segmentation.", "AI": {"tldr": "DiSCO-3D 是一种用于3D开放词汇子概念发现的新方法，它结合了无监督分段和弱开放词汇指导，取得了在开放词汇和无监督分段边缘情况下的最好结果。", "motivation": "传统的3D分割方法要么适应特定任务目标，要么适应场景内容，但不能两者兼顾。DiSCO-3D 的动机是提出一种新方法，能同时适应场景内容和用户查询。", "method": "DiSCO-3D 基于神经场表示，通过结合无监督分割和弱开放词汇指导来实现3D开放词汇子概念发现。", "result": "评估显示，DiSCO-3D 在开放词汇子概念发现中表现有效，并在开放词汇和无监督分割的边缘情况下的表现均处于最高水平。", "conclusion": "DiSCO-3D 作为解决3D开放词汇子概念发现问题的第一种方法，证明了它在处理同时适应场景内容和用户查询的任务时的有效性和优越性。"}}
{"id": "2507.14683", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14683", "abs": "https://arxiv.org/abs/2507.14683", "authors": ["Xingxuan Li", "Yao Xiao", "Dianwen Ng", "Hai Ye", "Yue Deng", "Xiang Lin", "Bin Wang", "Zhanfeng Mo", "Chong Zhang", "Yueyi Zhang", "Zonglin Yang", "Ruilin Li", "Lei Lei", "Shihao Xu", "Han Zhao", "Weiling Chen", "Feng Ji", "Lidong Bing"], "title": "MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization", "comment": "Technical report", "summary": "Large language models have recently evolved from fluent text generation to\nadvanced reasoning across diverse domains, giving rise to reasoning language\nmodels. Among these domains, mathematical reasoning serves as a representative\nbenchmark as it requires precise multi-step logic and abstract reasoning, which\ncan be generalized to other tasks. While closed-source RLMs such as GPT-o3\ndemonstrate impressive reasoning capabilities, their proprietary nature limits\ntransparency and reproducibility. Although many open-source projects aim to\nclose this gap, most of them lack sufficient openness by omitting critical\nresources such as datasets and detailed training configurations, which hinders\nreproducibility. To contribute toward greater transparency in RLM development,\nwe introduce the MiroMind-M1 series, a set of fully open-source RLMs built on\nthe Qwen-2.5 backbone that match or exceed the performance of existing\nopen-source RLMs. Specifically, our models are trained in two stages: SFT on a\ncarefully curated corpus of 719K math-reasoning problems with verified CoT\ntrajectories, followed by RLVR on 62K challenging and verifiable problems. To\nenhance the robustness and efficiency of the RLVR process, we introduce\nContext-Aware Multi-Stage Policy Optimization, an algorithm that integrates\nlength-progressive training with an adaptive repetition penalty to encourage\ncontext-aware RL training. Our model achieves state-of-the-art or competitive\nperformance and superior token efficiency among Qwen-2.5-based open-source 7B\nand 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate\nreproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,\nMiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,\nMiroMind-M1-RL-62K); and all training and evaluation configurations. We hope\nthese resources will support further research and foster community advancement.", "AI": {"tldr": "MiroMind-M1是一个开源推理语言模型系列，采用两阶段训练方法达到最佳数学推理性能，目的是提高推理语言模型的透明度和可重复性。", "motivation": "该研究旨在提升推理语言模型（RLM）开发的透明度和可重复性，尤其是在数学推理领域。", "method": "该研究开发了MiroMind-M1系列，这是一种基于Qwen-2.5的开源推理语言模型，通过两阶段训练（SFT阶段和RLVR阶段）并引入了Context-Aware Multi-Stage Policy Optimization算法来提升训练效率和模型性能。", "result": "MiroMind-M1系列模型在AIME24、AIME25和MATH基准测试中达到了最佳或具有竞争力的性能，并且在Qwen-2.5基础上的开源模型中具有更高的代币效率。", "conclusion": "MiroMind-M1系列模型展现出强大的数学推理能力，同时通过开放完整的技术栈支持进一步研究和社区发展。"}}
{"id": "2507.14608", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14608", "abs": "https://arxiv.org/abs/2507.14608", "authors": ["Nandani Sharma", "Dinesh Singh"], "title": "Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition", "comment": null, "summary": "Facial expression recognition is crucial for human-computer interaction\napplications such as face animation, video surveillance, affective computing,\nmedical analysis, etc. Since the structure of facial attributes varies with\nfacial expressions, incorporating structural information into facial attributes\nis essential for facial expression recognition. In this paper, we propose\nExp-Graph, a novel framework designed to represent the structural relationships\namong facial attributes using graph-based modeling for facial expression\nrecognition. For facial attributes graph representation, facial landmarks are\nused as the graph's vertices. At the same time, the edges are determined based\non the proximity of the facial landmark and the similarity of the local\nappearance of the facial attributes encoded using the vision transformer.\nAdditionally, graph convolutional networks are utilized to capture and\nintegrate these structural dependencies into the encoding of facial attributes,\nthereby enhancing the accuracy of expression recognition. Thus, Exp-Graph\nlearns from the facial attribute graphs highly expressive semantic\nrepresentations. On the other hand, the vision transformer and graph\nconvolutional blocks help the framework exploit the local and global\ndependencies among the facial attributes that are essential for the recognition\nof facial expressions. We conducted comprehensive evaluations of the proposed\nExp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.\nThe model achieved recognition accuracies of 98.09\\%, 79.01\\%, and 56.39\\%,\nrespectively. These results indicate that Exp-Graph maintains strong\ngeneralization capabilities across both controlled laboratory settings and\nreal-world, unconstrained environments, underscoring its effectiveness for\npractical facial expression recognition applications.", "AI": {"tldr": "本论文提出了一种图模型框架Exp-Graph，通过利用视觉变换器编码面部属性的局部外观，并使用图卷积网络来捕捉结构依赖，以提高面部表情识别的准确性。", "motivation": "面部表情识别在诸如面部动画、视频监控、情感计算、医学分析等应用中具有重要意义。由于面部属性结构会随面部表情变化，因此将结构信息融入面部属性识别中对表情识别是必要的，本论文即为此动机展开研究。", "method": "本论文提出了一种名为Exp-Graph的新框架，利用图模型来表示面部属性之间的结构关系，用于面部表情识别。面部特征点作为图的顶点，边则基于特征点的接近程度及局部外观的相似性来确定，此相似性通过视觉变换器进行编码。此外，还使用了图卷积网络来捕捉并整合这些结构依赖，以提升面部表情识别的准确性。", "result": "在Oulu-CASIA、eNTERFACE05和AFEW三个基准数据集上，Exp-Graph模型取得了98.09%、79.01%和56.39%的识别准确率，显示了其在不同环境中的泛化能力。", "conclusion": "本论文的研究结果证明，Exp-Graph框架利用面部属性图的表示能够学习到具有高度表达力的语义表示，并且在多个基准数据集上展示了有效性和泛化能力。"}}
{"id": "2507.14688", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14688", "abs": "https://arxiv.org/abs/2507.14688", "authors": ["Mohammed Alkhowaiter", "Norah Alshahrani", "Saied Alshahrani", "Reem I. Masoud", "Alaa Alzahrani", "Deema Alnuhait", "Emad A. Alghamdi", "Khalid Almubarak"], "title": "Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations", "comment": null, "summary": "Post-training has emerged as a crucial technique for aligning pre-trained\nLarge Language Models (LLMs) with human instructions, significantly enhancing\ntheir performance across a wide range of tasks. Central to this process is the\nquality and diversity of post-training datasets. This paper presents a review\nof publicly available Arabic post-training datasets on the Hugging Face Hub,\norganized along four key dimensions: (1) LLM Capabilities (e.g., Question\nAnswering, Translation, Reasoning, Summarization, Dialogue, Code Generation,\nand Function Calling); (2) Steerability (e.g., persona and system prompts); (3)\nAlignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness.\nEach dataset is rigorously evaluated based on popularity, practical adoption,\nrecency and maintenance, documentation and annotation quality, licensing\ntransparency, and scientific contribution. Our review revealed critical gaps in\nthe development of Arabic post-training datasets, including limited task\ndiversity, inconsistent or missing documentation and annotation, and low\nadoption across the community. Finally, the paper discusses the implications of\nthese gaps on the progress of Arabic LLMs and applications while providing\nconcrete recommendations for future efforts in post-training dataset\ndevelopment.", "AI": {"tldr": "本文审查了Hugging Face Hub上的阿拉伯语后训练数据集，发现了任务多样性不足、文档和注释不一致或缺失、社区采用率低等关键空白，并提出了未来相关数据集开发的建议。", "motivation": "本文旨在审查并填补阿拉伯语大语言模型后训练数据集开发中的一些关键空白，这些数据集的质量直接影响着模型与人类指令的一致性及其在各种任务上的表现。", "method": "此论文通过四个关键维度对Hugging Face Hub上公开的阿拉伯语后训练数据集进行了审查，分别是LLM能力、导向性、对齐性和鲁棒性。每个数据集都根据流行度、实用采用情况、时效性和维护情况、文档和注释质量、许可透明度和科学贡献进行了严格评估。", "result": "审查结果揭示了阿拉伯语后训练数据集中存在任务多样性不足、文档和注释不一致或缺失以及社区采纳率低等一系列关键空白，这些问题可能限制了阿拉伯语大语言模型的能力和表现。", "conclusion": "文中讨论了这些审查发现对阿拉伯语大语言模型及其未来发展的潜在影响，并建议未来应致力于数据集的多元化以及文档和注释的质量提升。"}}
{"id": "2507.14613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14613", "abs": "https://arxiv.org/abs/2507.14613", "authors": ["Guoping Xu", "Christopher Kabat", "You Zhang"], "title": "Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2", "comment": "24 pages, 6 figures", "summary": "Recent advances in medical image segmentation have been driven by deep\nlearning; however, most existing methods remain limited by modality-specific\ndesigns and exhibit poor adaptability to dynamic medical imaging scenarios. The\nSegment Anything Model 2 (SAM2) and its related variants, which introduce a\nstreaming memory mechanism for real-time video segmentation, present new\nopportunities for prompt-based, generalizable solutions. Nevertheless, adapting\nthese models to medical video scenarios typically requires large-scale datasets\nfor retraining or transfer learning, leading to high computational costs and\nthe risk of catastrophic forgetting. To address these challenges, we propose\nDD-SAM2, an efficient adaptation framework for SAM2 that incorporates a\nDepthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature\nextraction with minimal parameter overhead. This design enables effective\nfine-tuning of SAM2 on medical videos with limited training data. Unlike\nexisting adapter-based methods focused solely on static images, DD-SAM2 fully\nexploits SAM2's streaming memory for medical video object tracking and\nsegmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation)\nand EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior\nperformance, achieving Dice scores of 0.93 and 0.97, respectively. To the best\nof our knowledge, this work provides an initial attempt at systematically\nexploring adapter-based SAM2 fine-tuning for medical video segmentation and\ntracking. Code, datasets, and models will be publicly available at\nhttps://github.com/apple1986/DD-SAM2.", "AI": {"tldr": "本文介绍了一种用于医学视频分割和跟踪的高效框架DD-SAM2，它基于SAM2并使用Depthwise-Dilated Adapter进行改进。该方法在两个数据集上获得优秀的评估结果。", "motivation": "当前医学图像分割技术受模态特定设计的限制，难以适应动态的医学成像场景。现有的基于SAM2的模型需要大规模数据集进行再训练或迁移学习，这会带来高计算成本和灾难性遗忘的风险。", "method": "本文提出了DD-SAM2，这是一种高效的SAM2适应框架，它引入了一种称为Depthwise-Dilated Adapter (DD-Adapter) 的组件，以实现多尺度特征提取，同时保持较少的参数量开销。这使得DD-SAM2能够利用SAM2的流式记忆机制，对医学视频序列中的目标进行跟踪和分割。", "result": "DD-SAM2在评估数据集TrackRad2025和EchoNet-Dynamic上的Dice分数分别达到了0.93和0.97，显著优于现有的方法。", "conclusion": "本文的工作首次系统地探索了基于适配器的SAM2医疗视频分割和跟踪细化。实现了优秀的分割效果，同时保持了参数效率，为未来的研究提出了新的方法和发展方向。"}}
{"id": "2507.14693", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14693", "abs": "https://arxiv.org/abs/2507.14693", "authors": ["Amina Dzafic", "Merve Kavut", "Ulya Bayram"], "title": "Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation", "comment": "This manuscript has been submitted to the IEEE Journal of Biomedical\n  and Health Informatics", "summary": "Suicidal ideation detection is critical for real-time suicide prevention, yet\nits progress faces two under-explored challenges: limited language coverage and\nunreliable annotation practices. Most available datasets are in English, but\neven among these, high-quality, human-annotated data remains scarce. As a\nresult, many studies rely on available pre-labeled datasets without examining\ntheir annotation process or label reliability. The lack of datasets in other\nlanguages further limits the global realization of suicide prevention via\nartificial intelligence (AI). In this study, we address one of these gaps by\nconstructing a novel Turkish suicidal ideation corpus derived from social media\nposts and introducing a resource-efficient annotation framework involving three\nhuman annotators and two large language models (LLMs). We then address the\nremaining gaps by performing a bidirectional evaluation of label reliability\nand model consistency across this dataset and three popular English suicidal\nideation detection datasets, using transfer learning through eight pre-trained\nsentiment and emotion classifiers. These transformers help assess annotation\nconsistency and benchmark model performance against manually labeled data. Our\nfindings underscore the need for more rigorous, language-inclusive approaches\nto annotation and evaluation in mental health natural language processing (NLP)\nwhile demonstrating the questionable performance of popular models with\nzero-shot transfer learning. We advocate for transparency in model training and\ndataset construction in mental health NLP, prioritizing data and model\nreliability.", "AI": {"tldr": "研究团队创建了一种新型的土耳其语自杀意念数据集，并提出了一个资源高效的注释框架，以解决注释数据稀缺和语言限制的问题，并展示了流行模型在迁移学习中的表现问题，强调了数据集和模型透明度的要求。", "motivation": "解决自杀意念检测在实时预防自杀方面的两大未充分探索的挑战：有限的语言覆盖范围和不可靠的注释实践，并强调对注释和评估的更严谨、包容语言的方法的需要，特别是针对心理健康领域的自然语言处理（NLP）。", "method": "通过构建一个来源于社交媒体帖子的新型土耳其语自杀意念语料库并引入一个涉及三位人类注释员和两个大型语言模型（LLMs）的资源高效的注释框架来解决注释数据的稀缺性和语言限制问题。此外，通过双向评估标签可靠性和模型一致性来解决其他问题，使用迁移学习通过八个预训练的情感和情绪分类器对这个数据集和其他三种流行的英语自杀意念检测数据集进行分析，以评估注释一致性和标杆模型性能。", "result": "研究人员发现，在没有经过训练的情况下，流行的模型在零样本迁移学习中的性能存在问题，强调了在心理健康NLP领域中对模型训练和数据集构建的透明度要求，以及对数据和模型可靠性的优先考虑。", "conclusion": "研究结果强调了心理健康NLP领域中对注释和评估的更严谨、包容语言的方法的需要，特别提出对模型训练和数据集构建的透明度要求，是为了确保数据和模型的可靠性。这些发现对全球范围内通过人工智能实现自杀预防具有重要意义。"}}
{"id": "2507.14632", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14632", "abs": "https://arxiv.org/abs/2507.14632", "authors": ["Haiquan Wen", "Tianxiao Li", "Zhenglin Huang", "Yiwei He", "Guangliang Cheng"], "title": "BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM", "comment": null, "summary": "Recent advances in generative AI have dramatically improved image and video\nsynthesis capabilities, significantly increasing the risk of misinformation\nthrough sophisticated fake content. In response, detection methods have evolved\nfrom traditional approaches to multimodal large language models (MLLMs),\noffering enhanced transparency and interpretability in identifying synthetic\nmedia. However, current detection systems remain fundamentally limited by their\nsingle-modality design. These approaches analyze images or videos separately,\nmaking them ineffective against synthetic content that combines multiple media\nformats. To address these challenges, we introduce \\textbf{BusterX++}, a novel\nframework designed specifically for cross-modal detection and explanation of\nsynthetic media. Our approach incorporates an advanced reinforcement learning\n(RL) post-training strategy that eliminates cold-start. Through Multi-stage\nTraining, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and\nsubstantial performance improvements. To enable comprehensive evaluation, we\nalso present \\textbf{GenBuster++}, a cross-modal benchmark leveraging\nstate-of-the-art image and video generation techniques. This benchmark\ncomprises 4,000 images and video clips, meticulously curated by human experts\nusing a novel filtering methodology to ensure high quality, diversity, and\nreal-world applicability. Extensive experiments demonstrate the effectiveness\nand generalizability of our approach.", "AI": {"tldr": "提出BusterX++框架用于合成媒体的跨模态检测，通过多种方法优化提升性能，并建立GenBuster++基准用于评估。", "motivation": "随着生成式AI在图像和视频合成能力的提高，带来了通过复杂假内容制造误导信息的风险。传统方法局限于单模态设计，对结合多种媒体格式的合成内容识别效果不佳，因此提出了BusterX++框架。", "method": "介绍了一种名为BusterX++的新框架，专用于合成媒体的跨模态检测和解释，并使用了先进的强化学习(RL)后训练策略来消除冷启动问题。BusterX++通过多阶段训练、思考奖励和混合推理实现了稳定和显著的性能改进。", "result": "实验显示了BusterX++框架在检测合成媒体方面的有效性与泛化能力。", "conclusion": "BusterX++在跨模态合成媒体检测中展示了显著的性能提升，同时提出了GenBuster++基准，用于进行全面的评估。"}}
{"id": "2507.14741", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14741", "abs": "https://arxiv.org/abs/2507.14741", "authors": ["Maria Sahakyan", "Bedoor AlShebli"], "title": "Disparities in Peer Review Tone and the Role of Reviewer Anonymity", "comment": null, "summary": "The peer review process is often regarded as the gatekeeper of scientific\nintegrity, yet increasing evidence suggests that it is not immune to bias.\nAlthough structural inequities in peer review have been widely debated, much\nless attention has been paid to the subtle ways in which language itself may\nreinforce disparities. This study undertakes one of the most comprehensive\nlinguistic analyses of peer review to date, examining more than 80,000 reviews\nin two major journals. Using natural language processing and large-scale\nstatistical modeling, it uncovers how review tone, sentiment, and supportive\nlanguage vary across author demographics, including gender, race, and\ninstitutional affiliation. Using a data set that includes both anonymous and\nsigned reviews, this research also reveals how the disclosure of reviewer\nidentity shapes the language of evaluation. The findings not only expose hidden\nbiases in peer feedback, but also challenge conventional assumptions about\nanonymity's role in fairness. As academic publishing grapples with reform,\nthese insights raise critical questions about how review policies shape career\ntrajectories and scientific progress.", "AI": {"tldr": "通过对超过80,000篇来自两个主要期刊的同行评审进行分析，本研究揭示了语言如何在作者性别、种族和机构隶属等方面存在差异，同时通过实名和匿名的评审比较，揭示了评审者身份披露对评价语言的影响，并提出了同行评审改革的必要性。", "motivation": "尽管结构不公在同行评审中被广泛讨论，但对语言本身如何强化差异的关注甚少。本研究旨在通过语用分析揭示同行评审中潜藏的偏见，并挑战匿名性在公平性方面的传统假设。", "method": "本研究使用自然语言处理和大规模统计建模方法，对超过80,000篇来自两个主要期刊的同行评审进行了全面的语用分析。研究探讨了回顾语气、情感以及支持性语言如何在作者性别、种族和机构隶属等方面存在差异，并通过包括匿名和实名评审在内的数据集，揭示了评审者身份披露对评价语言的影响。", "result": "研究结果展现了评审语气、情感和支持性语言如何按作者性别、种族和机构背景变化，并通过比较实名和匿名评审，揭示了评审者身份的揭示如何影响评估语言。这些结果质疑了匿名性在公平中的假设，并指出同行评审过程中存在的隐性偏见。", "conclusion": "研究发现揭露了同行反馈中存在的隐藏偏见，并挑战了匿名性在公平性中的作用的常规假设。随着学术出版业正面对改革，本研究提出了关于评审政策如何影响职业轨迹和科学进步的关键问题。"}}
{"id": "2507.14643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14643", "abs": "https://arxiv.org/abs/2507.14643", "authors": ["Jifeng Shen", "Haibo Zhan", "Shaohua Dong", "Xin Zuo", "Wankou Yang", "Haibin Ling"], "title": "Multispectral State-Space Feature Fusion: Bridging Shared and Cross-Parametric Interactions for Object Detection", "comment": "submitted on 30/4/2025, Under Major Revision", "summary": "Modern multispectral feature fusion for object detection faces two critical\nlimitations: (1) Excessive preference for local complementary features over\ncross-modal shared semantics adversely affects generalization performance; and\n(2) The trade-off between the receptive field size and computational complexity\npresent critical bottlenecks for scalable feature modeling. Addressing these\nissues, a novel Multispectral State-Space Feature Fusion framework, dubbed\nMS2Fusion, is proposed based on the state space model (SSM), achieving\nefficient and effective fusion through a dual-path parametric interaction\nmechanism. More specifically, the first cross-parameter interaction branch\ninherits the advantage of cross-attention in mining complementary information\nwith cross-modal hidden state decoding in SSM. The second shared-parameter\nbranch explores cross-modal alignment with joint embedding to obtain\ncross-modal similar semantic features and structures through parameter sharing\nin SSM. Finally, these two paths are jointly optimized with SSM for fusing\nmultispectral features in a unified framework, allowing our MS2Fusion to enjoy\nboth functional complementarity and shared semantic space. In our extensive\nexperiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our\nMS2Fusion significantly outperforms other state-of-the-art multispectral object\ndetection methods, evidencing its superiority. Moreover, MS2Fusion is general\nand applicable to other multispectral perception tasks. We show that, even\nwithout specific design, MS2Fusion achieves state-of-the-art results on RGB-T\nsemantic segmentation and RGBT salient object detection, showing its\ngenerality. The source code will be available at\nhttps://github.com/61s61min/MS2Fusion.git.", "AI": {"tldr": "提出了基于状态空间模型的MS2Fusion框架，通过双路径参数交互机制有效地解决了多光谱特征融合在目标检测中的局限性，并在多个基准测试中显示出优越性。", "motivation": "现有的多光谱特征融合方法在对象检测中存在偏好局部互补特征而忽视跨模式共享语义以及感受野大小和计算复杂度之间的权衡问题。", "method": "设计了一个基于状态空间模型的新的双路径参数交互机制，其中第一个跨参数交互分支通过跨注意力挖掘互补信息，第二个共享参数分支通过跨模式对齐获取共享语义特征。", "result": "实验结果表明，MS2Fusion在像FLIR、M3FD和LLVIP等多个主流基准测试中显著优于其他最先进的多光谱目标检测方法。", "conclusion": "MS2Fusion框架不仅在目标检测中表现出色，还展示了其在RGB-T语义分割和RGBT显著物体检测中的广泛适用性和通用性。"}}
{"id": "2507.14749", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14749", "abs": "https://arxiv.org/abs/2507.14749", "authors": ["Wai Keen Vong", "Brenden M. Lake"], "title": "On the robustness of modeling grounded word learning through a child's egocentric input", "comment": null, "summary": "What insights can machine learning bring to understanding human language\nacquisition? Large language and multimodal models have achieved remarkable\ncapabilities, but their reliance on massive training datasets creates a\nfundamental mismatch with children, who succeed in acquiring language from\ncomparatively limited input. To help bridge this gap, researchers have\nincreasingly trained neural networks using data similar in quantity and quality\nto children's input. Taking this approach to the limit, Vong et al. (2024)\nshowed that a multimodal neural network trained on 61 hours of visual and\nlinguistic input extracted from just one child's developmental experience could\nacquire word-referent mappings. However, whether this approach's success\nreflects the idiosyncrasies of a single child's experience, or whether it would\nshow consistent and robust learning patterns across multiple children's\nexperiences was not explored. In this article, we applied automated speech\ntranscription methods to the entirety of the SAYCam dataset, consisting of over\n500 hours of video data spread across all three children. Using these automated\ntranscriptions, we generated multi-modal vision-and-language datasets for both\ntraining and evaluation, and explored a range of neural network configurations\nto examine the robustness of simulated word learning. Our findings demonstrate\nthat networks trained on automatically transcribed data from each child can\nacquire and generalize word-referent mappings across multiple network\narchitectures. These results validate the robustness of multimodal neural\nnetworks for grounded word learning, while highlighting the individual\ndifferences that emerge in how models learn when trained on each child's\ndevelopmental experiences.", "AI": {"tldr": "研究通过使用自动语音转录方法处理SAYCam数据集，并生成用于训练和评估的多模态数据集，结果表明多模态神经网络可以从每个孩子的发育经历中稳健地学习并泛化词-指称映射，同时也强调了学习过程中的个体差异。", "motivation": "为了进一步理解机器学习在人类语言习得中的作用，并验证多模态神经网络方法在不同儿童身上的学习模式的稳健性。", "method": "通过将自动语音转录方法应用于包含超过500小时视频数据的SAYCam数据集，研究人员生成了用于训练和评估的多模态视觉和语言数据集，并探索了一系列神经网络配置，以检验模拟词汇学习的稳健性。", "result": "结果表明，针对每个孩子的发展经历进行训练的网络，可以在多种网络架构中获得并泛化词-指称映射。这验证了多模态神经网络在基于情境的学习中的稳健性，同时也强调了当模型根据每个孩子的发育经历进行训练时出现的个体差异。", "conclusion": "这些发现验证了多模态神经网络在基于情境的词汇学习中稳健性的适应性，同时也强调了模型学习过程中基于每个孩子独特经历的个体差异的显著性。"}}
{"id": "2507.14657", "categories": ["cs.CV", "cs.AI", "68T45", "I.2.10"], "pdf": "https://arxiv.org/pdf/2507.14657", "abs": "https://arxiv.org/abs/2507.14657", "authors": ["Keivan Shariatmadar", "Ahmad Osman"], "title": "AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)", "comment": "24 pages, 9 figures", "summary": "The integration of Artificial Intelligence (AI) into sports officiating\nrepresents a paradigm shift in how decisions are made in competitive\nenvironments. Traditional manual systems, even when supported by Instant Video\nReplay (IVR), often suffer from latency, subjectivity, and inconsistent\nenforcement, undermining fairness and athlete trust. This paper introduces\nFST.ai, a novel AI-powered framework designed to enhance officiating in Sport\nTaekwondo, particularly focusing on the complex task of real-time head kick\ndetection and scoring. Leveraging computer vision, deep learning, and edge\ninference, the system automates the identification and classification of key\nactions, significantly reducing decision time from minutes to seconds while\nimproving consistency and transparency. Importantly, the methodology is not\nlimited to Taekwondo. The underlying framework -- based on pose estimation,\nmotion classification, and impact analysis -- can be adapted to a wide range of\nsports requiring action detection, such as judo, karate, fencing, or even team\nsports like football and basketball, where foul recognition or performance\ntracking is critical. By addressing one of Taekwondo's most challenging\nscenarios -- head kick scoring -- we demonstrate the robustness, scalability,\nand sport-agnostic potential of FST.ai to transform officiating standards\nacross multiple disciplines.", "AI": {"tldr": "FST.ai is an AI-powered framework for enhancing sports officiating, initially targeted at Sport Taekwondo to improve fairness and trust through automated, real-time decision-making.", "motivation": "To enhance officiating in Sport Taekwondo and address the issues of latency, subjectivity, and inconsistent enforcement in traditional manual systems.", "method": "Leveraging computer vision, deep learning, and edge inference, the system automates the identification and classification of key actions.", "result": "Significantly reducing decision time from minutes to seconds while improving consistency and transparency in the complex task of real-time head kick detection and scoring.", "conclusion": "By demonstrating robustness and scalability with head kick scoring in Taekwondo, FST.ai showcases its potential to transform officiating standards across various sports disciplines."}}
{"id": "2507.14758", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.14758", "abs": "https://arxiv.org/abs/2507.14758", "authors": ["Luyi Ma", "Wanjia Zhang", "Kai Zhao", "Abhishek Kulkarni", "Lalitesh Morishetti", "Anjana Ganesh", "Ashish Ranjan", "Aashika Padmanabhan", "Jianpeng Xu", "Jason Cho", "Praveen Kanumala", "Kaushiki Nag", "Sumit Dutta", "Kamiya Motwani", "Malay Patel", "Evren Korpeoglu", "Sushant Kumar", "Kannan Achan"], "title": "GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization", "comment": "10 pages, 5 figures, The ACM Conference on Recommender Systems\n  (RecSys) 2025", "summary": "Generative models have recently demonstrated strong potential in\nmulti-behavior recommendation systems, leveraging the expressive power of\ntransformers and tokenization to generate personalized item sequences. However,\ntheir adoption is hindered by (1) the lack of explicit information for token\nreasoning, (2) high computational costs due to quadratic attention complexity\nand dense sequence representations after tokenization, and (3) limited\nmulti-scale modeling over user history. In this work, we propose GRACE\n(Generative Recommendation via journey-aware sparse Attention on\nChain-of-thought tokEnization), a novel generative framework for multi-behavior\nsequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)\ntokenization method that encodes user-item interactions with explicit\nattributes from product knowledge graphs (e.g., category, brand, price) over\nsemantic tokenization, enabling interpretable and behavior-aligned generation.\nTo address the inefficiency of standard attention, we design a Journey-Aware\nSparse Attention (JSA) mechanism, which selectively attends to compressed,\nintra-, inter-, and current-context segments in the tokenized sequence.\nExperiments on two real-world datasets show that GRACE significantly\noutperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and\n+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home\ndomain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces\nattention computation by up to 48% with long sequences.", "AI": {"tldr": "GRACE是一种为提高多行为序列推荐系统性能而提出的生成推荐框架，其通过改进的标记化方法和优化的注意力机制来实现这一目的，并在实验中表现出优于现有模型的表现。", "motivation": "生成模型在多行为推荐系统中展现出了潜力，但由于缺乏显式信息、高计算成本和有限的多尺度建模能力，其采用受到了限制。", "method": "GRACE框架采用了混合的Chain-of-Thought (CoT) 标记化方法和Journey-Aware Sparse Attention (JSA)机制，以分别提高生成的可解释性和解决标准注意力机制的效率问题。", "result": "该论文提出了一种名为GRACE的生成推荐框架，通过引入一种混合的CoT标记化方法，并设计了JSA机制以解决标准注意力机制的低效问题，从而改善多行为序列推荐系统。实验结果表明，GRACE在两个现实世界数据集上显著优于现有最佳基线模型。", "conclusion": "实验结果显示，GRACE在特定领域内的标准化评估指标上超过现有最佳模型，同时减少了注意力计算成本。"}}
{"id": "2507.14662", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14662", "abs": "https://arxiv.org/abs/2507.14662", "authors": ["Shayan Rokhva", "Babak Teimourpour"], "title": "Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall", "comment": "Questions & Recommendations: shayanrokhva1999@gmail.com;\n  shayan1999rokh@yahoo.com", "summary": "Quantifying post-consumer food waste in institutional dining settings is\nessential for supporting data-driven sustainability strategies. This study\npresents a cost-effective computer vision framework that estimates plate-level\nfood waste by utilizing semantic segmentation of RGB images taken before and\nafter meal consumption across five Iranian dishes. Four fully supervised models\n(U-Net, U-Net++, and their lightweight variants) were trained using a capped\ndynamic inverse-frequency loss and AdamW optimizer, then evaluated through a\ncomprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a\ncustom-defined Distributional Pixel Agreement (DPA) metric tailored to the\ntask. All models achieved satisfying performance, and for each food type, at\nleast one model approached or surpassed 90% DPA, demonstrating strong alignment\nin pixel-wise proportion estimates. Lighter models with reduced parameter\ncounts offered faster inference, achieving real-time throughput on an NVIDIA T4\nGPU. Further analysis showed superior segmentation performance for dry and more\nrigid components (e.g., rice and fries), while more complex, fragmented, or\nviscous dishes, such as stews, showed reduced performance, specifically\npost-consumption. Despite limitations such as reliance on 2D imaging,\nconstrained food variety, and manual data collection, the proposed framework is\npioneering and represents a scalable, contactless solution for continuous\nmonitoring of food consumption. This research lays foundational groundwork for\nautomated, real-time waste tracking systems in large-scale food service\nenvironments and offers actionable insights and outlines feasible future\ndirections for dining hall management and policymakers aiming to reduce\ninstitutional food waste.", "AI": {"tldr": "研究提出了一种基于计算机视觉的方法来量化餐盘级别的食物浪费，使用RGB图像中的语义分割技术，并通过实际测试展示了该方法的有效性和实时性，为减少机构内食物浪费提供了可能的解决方案。", "motivation": "量化机构餐饮环境中餐后食物浪费对支持数据驱动的可持续性策略是必不可少的。这项研究的动机在于开发一种成本效益高的计算机视觉框架，以解决这一问题。", "method": "本研究提出了一种基于计算机视觉的框架，利用RGB图像的语义分割来估算餐盘级别的食物浪费。为了实现这一目标，训练了四个全监督模型（U-Net、U-Net++及其轻量级变体）。这些模型使用了受限动态反频率损失和AdamW优化器，并通过多个评估指标（包括像素准确率、Dice、IoU和一个特定任务定制的分布像素协议（DPA））进行了评估。", "result": "所有模型都表现出了令人满意的效果，对于每种食物类型，至少有一个模型的DPA达到了或接近90%，表明像素级别的比例估算与实际情况高度一致。轻量级模型由于参数更少，提供了更快的推理速度，可以在NVIDIA T4 GPU上实现实时吞吐量。进一步分析表明，对于干燥且硬的成分（如米饭和炸薯条）的分割表现更好，而对于更复杂、碎片化或粘稠的菜肴（如炖菜），则在餐后表现较差。", "conclusion": "尽管存在一些限制，如依赖二维成像、食物品种限制和手动数据收集，这项研究提出的方法是一个开创性的解决方案，代表了一种可以连续监控食物消费的可扩展、非接触式方法。研究为大型餐饮服务环境中的自动实时废物跟踪系统奠定了基础，并为管理人员和政策制定者提供了减少机构食物浪费的实际见解和可行的未来方向。"}}
{"id": "2507.14815", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14815", "abs": "https://arxiv.org/abs/2507.14815", "authors": ["Shoutao Guo", "Shaolei Zhang", "Qingkai Fang", "Zhengrui Ma", "Min Zhang", "Yang Feng"], "title": "FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing", "comment": "The code is at https://github.com/ictnlp/FastLongSpeech. This model\n  is at https://huggingface.co/ICTNLP/FastLongSpeech. The dataset is at\n  https://huggingface.co/datasets/ICTNLP/LongSpeech-Eval", "summary": "The rapid advancement of Large Language Models (LLMs) has spurred significant\nprogress in Large Speech-Language Models (LSLMs), enhancing their capabilities\nin both speech understanding and generation. While existing LSLMs often\nconcentrate on augmenting speech generation or tackling a diverse array of\nshort-speech tasks, the efficient processing of long-form speech remains a\ncritical yet underexplored challenge. This gap is primarily attributed to the\nscarcity of long-speech training datasets and the high computational costs\nassociated with long sequences. To address these limitations, we introduce\nFastLongSpeech, a novel framework designed to extend LSLM capabilities for\nefficient long-speech processing without necessitating dedicated long-speech\ntraining data. FastLongSpeech incorporates an iterative fusion strategy that\ncan compress excessively long-speech sequences into manageable lengths. To\nadapt LSLMs for long-speech inputs, it introduces a dynamic compression\ntraining approach, which exposes the model to short-speech sequences at varying\ncompression ratios, thereby transferring the capabilities of LSLMs to\nlong-speech tasks. To assess the long-speech capabilities of LSLMs, we develop\na long-speech understanding benchmark called LongSpeech-Eval. Experiments show\nthat our method exhibits strong performance in both long-speech and\nshort-speech tasks, while greatly improving inference efficiency.", "AI": {"tldr": "提出FastLongSpeech框架，通过动态压缩技术优化LSLMs处理长语音序列的能力。", "motivation": "当前LSLMs在处理长语音方面面临挑战，主要问题是长语音训练数据缺乏和计算成本高昂，FastLongSpeech框架旨在解决这些问题。", "method": "FastLongSpeech框架使用迭代融合策略压缩长语音序列，并通过动态压缩训练方法使模型适应长语音输入，这种方法利用短语音序列的不同压缩比例进行训练。", "result": "该研究介绍了FastLongSpeech框架，旨在通过创新的方法提升大型语音-语言模型（LSLMs）处理长语音序列的能力。通过迭代融合策略和动态压缩训练方法，FastLongSpeech能够在不依赖特定长语音训练数据的情况下优化模型性能。此外，研究还开发了一个长语音理解基准测试LongSpeech-Eval，实验结果表明该方法在处理长语音和短语音任务时表现优异，同时大幅提升了推理效率。", "conclusion": "FastLongSpeech框架能够有效提升LSLMs处理长语音序列的能力，同时保持对短语音任务的高效处理，展示了其在语音理解与生成领域的显著优势。"}}
{"id": "2507.14670", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14670", "abs": "https://arxiv.org/abs/2507.14670", "authors": ["Yaxuan Song", "Jianan Fan", "Hang Chang", "Weidong Cai"], "title": "Gene-DML: Dual-Pathway Multi-Level Discrimination for Gene Expression Prediction from Histopathology Images", "comment": "16 pages, 15 tables, 8 figures", "summary": "Accurately predicting gene expression from histopathology images offers a\nscalable and non-invasive approach to molecular profiling, with significant\nimplications for precision medicine and computational pathology. However,\nexisting methods often underutilize the cross-modal representation alignment\nbetween histopathology images and gene expression profiles across multiple\nrepresentational levels, thereby limiting their prediction performance. To\naddress this, we propose Gene-DML, a unified framework that structures latent\nspace through Dual-pathway Multi-Level discrimination to enhance correspondence\nbetween morphological and transcriptional modalities. The multi-scale\ninstance-level discrimination pathway aligns hierarchical histopathology\nrepresentations extracted at local, neighbor, and global levels with gene\nexpression profiles, capturing scale-aware morphological-transcriptional\nrelationships. In parallel, the cross-level instance-group discrimination\npathway enforces structural consistency between individual (image/gene)\ninstances and modality-crossed (gene/image, respectively) groups, strengthening\nthe alignment across modalities. By jointly modelling fine-grained and\nstructural-level discrimination, Gene-DML is able to learn robust cross-modal\nrepresentations, enhancing both predictive accuracy and generalization across\ndiverse biological contexts. Extensive experiments on public spatial\ntranscriptomics datasets demonstrate that Gene-DML achieves state-of-the-art\nperformance in gene expression prediction. The code and checkpoints will be\nreleased soon.", "AI": {"tldr": "本研究提出Gene-DML框架来增强组织学图像和基因表达谱之间的跨模态对齐，通过多尺度和跨层次的判别路径，提高了基因表达预测的性能和泛化能力。", "motivation": "该研究旨在解决现有方法在跨模态表示对齐上的不足，特别是组织学图像和基因表达谱之间多个表示层面的不足，进而提高预测性能。通过准确预测基因表达，可以从组织病理图像中获得可扩展且非侵入性的分子分析方法，对精准医疗和计算病理学具有重要意义。", "method": "本论文提出了Gene-DML框架，通过Dual-pathway Multi-Level discrimination来构建潜在空间，以增强组织学图像和基因表达谱之间的对应关系。多尺度实例级判别路径通过在局部、邻域和全局级别对组织学表示进行对齐，捕获了尺度感知的形态转录关系。同时，跨层次实例组别判别路径强化了个体（图像/基因）实例和模态跨跃（基因/图像）组之间的结构一致性，从而增强了跨模态对齐。", "result": "实验结果表明，Gene-DML在公共空间转录组学数据集上的基因表达预测达到了最先进的性能水平。", "conclusion": "Gene-DML框架通过联合建模细粒度和结构层次的判别性，学习了稳健的跨模态表示，从而提高了基因表达预测的准确性。"}}
{"id": "2507.14819", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14819", "abs": "https://arxiv.org/abs/2507.14819", "authors": ["Akriti Jain", "Pritika Ramu", "Aparna Garimella", "Apoorv Saxena"], "title": "Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\ntransforming text descriptions or tables to data visualizations via\ninstruction-tuning methods. However, it is not straightforward to apply these\nmethods directly for a more real-world use case of visualizing data from long\ndocuments based on user-given intents, as opposed to the user pre-selecting the\nrelevant content manually. We introduce the task of intent-based chart\ngeneration from documents: given a user-specified intent and document(s), the\ngoal is to generate a chart adhering to the intent and grounded on the\ndocument(s) in a zero-shot setting. We propose an unsupervised, two-staged\nframework in which an LLM first extracts relevant information from the\ndocument(s) by decomposing the intent and iteratively validates and refines\nthis data. Next, a heuristic-guided module selects an appropriate chart type\nbefore final code generation. To assess the data accuracy of the generated\ncharts, we propose an attribution-based metric that uses a structured textual\nrepresentation of charts, instead of relying on visual decoding metrics that\noften fail to capture the chart data effectively. To validate our approach, we\ncurate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from\ntwo domains, finance and scientific, in contrast to the existing datasets that\nare largely limited to parallel text descriptions/ tables and their\ncorresponding charts. We compare our approach with baselines using single-shot\nchart generation using LLMs and query-based retrieval methods; our method\noutperforms by upto $9$ points and $17$ points in terms of chart data accuracy\nand chart type respectively over the best baselines.", "AI": {"tldr": "论文提出了一个两阶段的无监督框架来基于用户给定的意图从长文档中生成图表，并设计了一种新的衡量生成图表数据准确性的指标。实验结果在特定的准确性和图表类型选择上比基线方法高出9到17个百分点。", "motivation": "该研究旨在解决从长文档中基于用户意图生成数据图表的问题，而不像过去那样需要用户手动选取相关内容。提出了一种在零样本设定下基于意图从文档生成图表的方法。", "method": "在该论文中，作者提出了一个无监督的、分两个阶段的框架来解决基于意图的图表生成问题。在这个框架中，首先使用大语言模型（LLMs）从文档中抽取相关信息，然后通过迭代验证和优化抽取的数据。第二阶段，一个由启发式指导的模块会根据选择的适当图表类型来生成最终的代码。", "result": "作者使用他们整理的1,242个包含金融和科学领域意图、文档和图表的元组数据集评估了这种方法。结果显示这种方法在图表数据的准确性和图表类型的选择上显著优于单次生成图表的方法和基于查询的检索方法，分别高出9和17个百分点。", "conclusion": "这项研究展示了可从长文档中基于意图生成数据图表的有效方法，性能超越现有办法。通过提出新方法和评估指标，该研究为未来基于文档数据的可视化任务提供了坚实的基础。"}}
{"id": "2507.14675", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14675", "abs": "https://arxiv.org/abs/2507.14675", "authors": ["Yuchen Duan", "Zhe Chen", "Yusong Hu", "Weiyun Wang", "Shenglong Ye", "Botian Shi", "Lewei Lu", "Qibin Hou", "Tong Lu", "Hongsheng Li", "Jifeng Dai", "Wenhai Wang"], "title": "Docopilot: Improving Multimodal Models for Document-Level Understanding", "comment": null, "summary": "Despite significant progress in multimodal large language models (MLLMs),\ntheir performance on complex, multi-page document comprehension remains\ninadequate, largely due to the lack of high-quality, document-level datasets.\nWhile current retrieval-augmented generation (RAG) methods offer partial\nsolutions, they suffer from issues, such as fragmented retrieval contexts,\nmulti-stage error accumulation, and extra time costs of retrieval. In this\nwork, we present a high-quality document-level dataset, Doc-750K, designed to\nsupport in-depth understanding of multimodal documents. This dataset includes\ndiverse document structures, extensive cross-page dependencies, and real\nquestion-answer pairs derived from the original documents. Building on the\ndataset, we develop a native multimodal model, Docopilot, which can accurately\nhandle document-level dependencies without relying on RAG. Experiments\ndemonstrate that Docopilot achieves superior coherence, accuracy, and\nefficiency in document understanding tasks and multi-turn interactions, setting\na new baseline for document-level multimodal understanding. Data, code, and\nmodels are released at https://github.com/OpenGVLab/Docopilot", "AI": {"tldr": "研究提出了一套高质量的多模态文档理解解决方案，包括Doc-750K数据集和Docopilot模型，解决了现有模型在文档级理解方面的不足。", "motivation": "尽管多模态大型语言模型在多模态理解方面取得了显著进展，但它们在复杂、多页文档理解上的表现仍欠佳，主要因为缺少高质量的文档级数据集。现有RAG方法虽部分解决问题，但存在片段化检索上下文、多阶段错误累积和额外检索时间成本等问题。", "method": "本研究提出了一个高质量的文档级数据集Doc-750K，用以支持多模态文档的深入理解。该数据集包含了多样的文档结构、广泛的跨页依赖关系，以及源自原始文档的实际问答对。同时，基于此数据集开发了一个原生的多模态模型Docopilot，该模型能准确处理文档级依赖关系，无需依赖检索增强生成(RAG)方法。", "result": "实验表明，Docopilot在文档理解任务和多轮交互中实现了更高的连贯性、准确性和效率，为文档级多模态理解设定了新的底线。", "conclusion": "本研究通过Doc-750K数据集和Docopilot模型，为解决文档级多模态理解问题提供了有效的方案。"}}
{"id": "2507.14849", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14849", "abs": "https://arxiv.org/abs/2507.14849", "authors": ["Yifei Wang"], "title": "Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding", "comment": null, "summary": "Reasoning distillation has emerged as an effective approach to enhance the\nreasoning capabilities of smaller language models. However, the impact of\nlarge-scale reasoning distillation on other critical abilities, particularly\nin-context retrieval and reasoning, remains unexplored. This gap in\nunderstanding is particularly significant given the increasing importance of\nRetrieval-Augmented Generation (RAG) systems, where efficient acquisition and\nutilization of contextual information are paramount for generating reliable\nresponses. Motivated by the need to understand how the extended long-CoT\nprocess influences long-context comprehension, we conduct a comprehensive\ninvestigation using a series of open-source models distilled from Deepseek-R1,\nrenowned for its exceptional reasoning capabilities. Our study focuses on\nevaluating these models' performance in extracting and integrating relevant\ninformation from extended contexts through multi-document question and\nanswering tasks. Through rigorous experimentation, we demonstrate that\ndistilled reasoning patterns significantly improve long-context understanding.\nOur analysis reveals that distillation fosters greater long-context awareness\nby promoting more detailed and explicit reasoning processes during context\nanalysis and information parsing. This advancement effectively mitigates the\npersistent \"lost in the middle\" issue that has hindered long-context models.", "AI": {"tldr": "本研究展示了推理提炼方法对于提升模型长上下文理解能力的显著效果，特别是在多文档问答任务中。", "motivation": "鉴于Retrieval-Augmented Generation (RAG)系统的日益重要性，这些系统在生成可靠回复时，需要高效地获取和利用上下文信息。本研究旨在探讨扩展的长-CoT过程如何影响长上下文理解。", "method": "本研究通过使用一系列由Deepseek-R1（以其卓越的推理能力而闻名）提炼出的开源模型，进行综合调查，评估这些模型在多文档问答任务中从扩展上下文中提取和整合相关信息的能力。", "result": "研究表明，通过提炼推理模式，可以显著提高对长上下文的理解。提炼过程促进了上下文分析和信息解析过程中的更详细和明确的推理过程，有效缓解了长期以来困扰长上下文模型的\"中间迷失\"问题。", "conclusion": "研究表明，推理提炼方法显著提升了小模型在长上下文理解上的能力，为提高模型的整体表现提供了新的可能性。"}}
{"id": "2507.14680", "categories": ["cs.CV", "cs.AI", "68T07, 92C55", "I.2.7; I.4.8; J.3"], "pdf": "https://arxiv.org/pdf/2507.14680", "abs": "https://arxiv.org/abs/2507.14680", "authors": ["Xinheng Lyu", "Yuci Liang", "Wenting Chen", "Meidan Ding", "Jiaqi Yang", "Guolin Huang", "Daokun Zhang", "Xiangjian He", "Linlin Shen"], "title": "WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis", "comment": null, "summary": "Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel\ntissue analysis across various pathological tasks. While recent advancements in\nmulti-modal large language models (MLLMs) allow multi-task WSI analysis through\nnatural language, they often underperform compared to task-specific models.\nCollaborative multi-agent systems have emerged as a promising solution to\nbalance versatility and accuracy in healthcare, yet their potential remains\nunderexplored in pathology-specific domains. To address these issues, we\npropose WSI-Agents, a novel collaborative multi-agent system for multi-modal\nWSI analysis. WSI-Agents integrates specialized functional agents with robust\ntask allocation and verification mechanisms to enhance both task-specific\naccuracy and multi-task versatility through three components: (1) a task\nallocation module assigning tasks to expert agents using a model zoo of patch\nand WSI level MLLMs, (2) a verification mechanism ensuring accuracy through\ninternal consistency checks and external validation using pathology knowledge\nbases and domain-specific models, and (3) a summary module synthesizing the\nfinal summary with visual interpretation maps. Extensive experiments on\nmulti-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs\nand medical agent frameworks across diverse tasks.", "AI": {"tldr": "本研究提出了一种名为WSI-Agents的协作多智能体系统，用于多模态的全玻片影像分析。该系统整合了专用功能智能体，结合了任务分配模块、验证机制和总结模块，实现在多种病理任务中的高精度和多功能性。实验结果表明，WSI-Agents在多模态全玻片影像基准测试中优于现有的MLLM和医疗智能体框架。", "motivation": "现有的多模态大规模语言模型在分析全玻片影像时，表现往往不如专门针对特定任务的模型，并且，在病理学特定领域的协作多智能体系统的潜力还未得到充分探索。", "method": "WSI-Agents系统融合了专用功能智能体，并配备了任务分配、验证和总结三大功能模块，以此增强特定任务的准确性和多功能性。", "result": "实验显示，WSI-Agents在多种病理任务中，实现了优于现有全玻片影像多模态大规模语言模型和医疗智能体框架的表现。", "conclusion": "WSI-Agents系统通过协作多智能体的概念，实现了全玻片影像的多模态分析，对于提升数字病理学的分析能力和准确性具有重要意义。"}}
{"id": "2507.14871", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14871", "abs": "https://arxiv.org/abs/2507.14871", "authors": ["Ronit D. Gross", "Yarden Tzach", "Tal Halevi", "Ella Koresh", "Ido Kanter"], "title": "Tiny language models", "comment": "23 pages, 1 figure and 12 tables", "summary": "A prominent achievement of natural language processing (NLP) is its ability\nto understand and generate meaningful human language. This capability relies on\ncomplex feedforward transformer block architectures pre-trained on large\nlanguage models (LLMs). However, LLM pre-training is currently feasible only\nfor a few dominant companies due to the immense computational resources\nrequired, limiting broader research participation. This creates a critical need\nfor more accessible alternatives. In this study, we explore whether tiny\nlanguage models (TLMs) exhibit the same key qualitative features of LLMs. We\ndemonstrate that TLMs exhibit a clear performance gap between pre-trained and\nnon-pre-trained models across classification tasks, indicating the\neffectiveness of pre-training, even at a tiny scale. The performance gap\nincreases with the size of the pre-training dataset and with greater overlap\nbetween tokens in the pre-training and classification datasets. Furthermore,\nthe classification accuracy achieved by a pre-trained deep TLM architecture can\nbe replicated through a soft committee of multiple, independently pre-trained\nshallow architectures, enabling low-latency TLMs without affecting\nclassification accuracy. Our results are based on pre-training BERT-6 and\nvariants of BERT-1 on subsets of the Wikipedia dataset and evaluating their\nperformance on FewRel, AGNews, and DBPedia classification tasks. Future\nresearch on TLM is expected to further illuminate the mechanisms underlying\nNLP, especially given that its biologically inspired models suggest that TLMs\nmay be sufficient for children or adolescents to develop language.", "AI": {"tldr": "此研究探讨了微型语言模型（TLMs）是否能展示与大语言模型（LLMs）相似的关键特征，并证明了预训练的重要性，展示了TLMs实现低延迟和高分类准确性的潜力。", "motivation": "由于大语言模型（LLMs）的预训练需要极大的计算资源，只有少数主导公司能够进行。这限制了更广泛的研究参与，因此存在着迫切需要寻找更加可访问的替代方案。本研究旨在探索微型语言模型（TLMs）是否可以提供这种解决方案。", "method": "本文研究了微型语言模型（TLMs）是否展示了与大语言模型（LLMs）相似的关键特征。实验通过预训练和非预训练的微型语言模型在分类任务上的性能差距进行了评估，使用了BERT-6及其变种，在维基百科数据子集上进行预训练，并在FewRel、AGNews和DBPedia分类任务上进行性能评估。", "result": "研究发现，微型语言模型（TLMs）在预训练和非预训练模型之间的性能差距显著，说明了即使在较小的规模下，预训练也十分有效。性能差距随着预训练数据集规模的增加和预训练数据集与分类数据集之间的令牌重叠度的增加而增大。此外，通过多个独立预训练和较浅结构的软委员会可以复制深度TLM架构的分类准确性，从而实现低延迟的TLMs而不影响分类准确性。", "conclusion": "研究结果表明，微型语言模型（TLMs）在NLP领域具有显著的潜力。它们不仅可以降低大语言模型（LLMs）预训练所需的大量计算资源需求，还可能揭示NLP机制背后的原理，尤其是所使用的生物启发模型表明TLMs可能足以让儿童或青少年习得语言。"}}
{"id": "2507.14686", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14686", "abs": "https://arxiv.org/abs/2507.14686", "authors": ["Chen Cai", "Tianyi Liu", "Jianjun Gao", "Wenyang Liu", "Kejun Wu", "Ruoyu Wang", "Yi Wang", "Soo Chin Liew"], "title": "From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition", "comment": null, "summary": "Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot\nabilities but struggle with complex Grounded Situation Recognition (GSR) and\nare resource-intensive for edge device deployment. Meanwhile, conventional GSR\nmodels often lack generalization ability, falling short in recognizing unseen\nand rare situations. In this paper, we exploit transferring knowledge from a\nteacher MLLM to a small GSR model to enhance its generalization and zero-shot\nabilities, thereby introducing the task of Open-vocabulary Grounded Situation\nRecognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt\nDistillation (MIPD), a novel framework that distills enriched multimodal\nknowledge from the foundation model, enabling the student Ov-GSR model to\nrecognize unseen situations and be better aware of rare situations.\nSpecifically, the MIPD framework first leverages the LLM-based Judgmental\nRationales Generator (JRG) to construct positive and negative glimpse and gaze\nrationales enriched with contextual semantic information. The proposed\nscene-aware and instance-perception prompts are then introduced to align\nrationales with visual information from the MLLM teacher via the\nNegative-Guided Multimodal Prompting Alignment (NMPA) module, effectively\ncapturing holistic and perceptual multimodal knowledge. Finally, the aligned\nmultimodal knowledge is distilled into the student Ov-GSR model, providing a\nstronger foundation for generalization that enhances situation understanding,\nbridges the gap between seen and unseen scenarios, and mitigates prediction\nbias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving\nsuperior performance on seen, rare, and unseen situations, and further\ndemonstrate improved unseen detection on the HICO-DET dataset.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.14887", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14887", "abs": "https://arxiv.org/abs/2507.14887", "authors": ["Shiyi Mu", "Yongkang Liu", "Shi Feng", "Xiaocui Yang", "Daling Wang", "Yifei Zhang"], "title": "MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction Tuning for Emotion-Cause Pair Extraction", "comment": "Accepted by CogSci", "summary": "Although large language models (LLMs) excel in text comprehension and\ngeneration, their performance on the Emotion-Cause Pair Extraction (ECPE) task,\nwhich requires reasoning ability, is often underperform smaller language model.\nThe main reason is the lack of auxiliary knowledge, which limits LLMs' ability\nto effectively perceive emotions and reason causes. To address this issue, we\npropose a novel \\textbf{M}ulti-source h\\textbf{E}terogeneous \\textbf{K}nowledge\n\\textbf{i}njection me\\textbf{T}hod, MEKiT, which integrates heterogeneous\ninternal emotional knowledge and external causal knowledge. Specifically, for\nthese two distinct aspects and structures of knowledge, we apply the approaches\nof incorporating instruction templates and mixing data for instruction-tuning,\nwhich respectively facilitate LLMs in more comprehensively identifying emotion\nand accurately reasoning causes. Experimental results demonstrate that MEKiT\nprovides a more effective and adaptable solution for the ECPE task, exhibiting\nan absolute performance advantage over compared baselines and dramatically\nimproving the performance of LLMs on the ECPE task.", "AI": {"tldr": "MEKiT, a multi-source knowledge injection method, enhances large language models' performance on the ECPE task by integrating internal and external knowledge, significantly surpassing baseline models.", "motivation": "The motivation stems from the underperformance of large language models on ECPE tasks, attributed to their limited auxiliary knowledge for emotion perception and cause reasoning compared to smaller models.", "method": "The paper proposes MEKiT, which integrates internal emotional knowledge and external causal knowledge into large language models to improve their performance on the ECPE task. It employs instruction templates and data mixing during instruction-tuning to better identify emotions and reason about causes.", "result": "Experimental results show that MEKiT outperforms comparative baselines and significantly enhances LLMs' performance on the ECPE task.", "conclusion": "MEKiT demonstrates a more effective and adaptable solution for the ECPE task, proving superior over baselines by significantly improving LLMs' performance."}}
{"id": "2507.14697", "categories": ["cs.CV", "I.4.6; I.2.10"], "pdf": "https://arxiv.org/pdf/2507.14697", "abs": "https://arxiv.org/abs/2507.14697", "authors": ["Zhiwei Zhang", "Zi Ye", "Yibin Wen", "Shuai Yuan", "Haohuan Fu", "Jianxi Huang", "Juepeng Zheng"], "title": "GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset", "comment": "38 pages, 18 figures, submitted to NeurIPS 2025", "summary": "Agricultural parcels serve as basic units for conducting agricultural\npractices and applications, which is vital for land ownership registration,\nfood security assessment, soil erosion monitoring, etc. However, existing\nagriculture parcel extraction studies only focus on mid-resolution mapping or\nregular plain farmlands while lacking representation of complex terraced\nterrains due to the demands of precision agriculture.In this paper, we\nintroduce a more fine-grained terraced parcel dataset named GTPBD (Global\nTerraced Parcel and Boundary Dataset), which is the first fine-grained dataset\ncovering major worldwide terraced regions with more than 200,000 complex\nterraced parcels with manual annotation. GTPBD comprises 47,537 high-resolution\nimages with three-level labels, including pixel-level boundary labels, mask\nlabels, and parcel labels. It covers seven major geographic zones in China and\ntranscontinental climatic regions around the world.Compared to the existing\ndatasets, the GTPBD dataset brings considerable challenges due to the: (1)\nterrain diversity; (2) complex and irregular parcel objects; and (3) multiple\ndomain styles. Our proposed GTPBD dataset is suitable for four different tasks,\nincluding semantic segmentation, edge detection, terraced parcel extraction,\nand unsupervised domain adaptation (UDA) tasks.Accordingly, we benchmark the\nGTPBD dataset on eight semantic segmentation methods, four edge extraction\nmethods, three parcel extraction methods, and five UDA methods, along with a\nmulti-dimensional evaluation framework integrating pixel-level and object-level\nmetrics. GTPBD fills a critical gap in terraced remote sensing research,\nproviding a basic infrastructure for fine-grained agricultural terrain analysis\nand cross-scenario knowledge transfer.", "AI": {"tldr": "本文介绍了全球梯田地块及边界数据集GTPBD，该数据集包含了超过200,000个复杂的梯田地块，适合用于语义分割、边界检测、梯田地块提取和无监督领域自适应等多种任务，并为这些任务设置了基准测试。", "motivation": "为了填补现有农业地块提取研究中对复杂梯田地块数据不足的空白，本研究旨在提供一个全新的高精度全球梯田数据集。", "method": "构建了全球梯田地块数据集GTPBD，包含高分辨率图像，具有像素级边界标签、掩码标签和地块标签。", "result": "GTPBD涵盖了七个地理区域，适合四种不同任务，并为此设置了基准测试，结果展示了数据集的独特挑战与适用性。", "conclusion": "GTPBD数据集为细粒度农业地貌分析和跨场景知识迁移提供了基础工具，填补了梯田遥感研究中的关键空白。"}}
{"id": "2507.14894", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14894", "abs": "https://arxiv.org/abs/2507.14894", "authors": ["Boyi Deng", "Yu Wan", "Baosong Yang", "Fei Huang", "Wenjie Wang", "Fuli Feng"], "title": "Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs", "comment": null, "summary": "Large Language Models (LLMs) have impressive multilingual capabilities, but\nthey suffer from unexpected code-switching, also known as language mixing,\nwhich involves switching to unexpected languages in the model response. This\nproblem leads to poor readability and degrades the usability of model\nresponses. However, existing work on this issue lacks a mechanistic analysis\nand shows limited effectiveness. In this paper, we first provide an in-depth\nanalysis of unexpected code-switching using sparse autoencoders and find that\nwhen LLMs switch to a language, the features of that language exhibit excessive\npre-activation values. Based on our findings, we propose $\\textbf{S}$parse\n$\\textbf{A}$utoencoder-guided $\\textbf{S}$upervised\n$\\textbf{F}$ine$\\textbf{t}$uning (SASFT), which teaches LLMs to maintain\nappropriate pre-activation values of specific language features during\ntraining. Experiments on five models across three languages demonstrate that\nSASFT consistently reduces unexpected code-switching by more than 50\\% compared\nto standard supervised fine-tuning, with complete elimination in four cases.\nMoreover, SASFT maintains or even improves the models' performance on six\nmultilingual benchmarks, showing its effectiveness in addressing code-switching\nwhile preserving multilingual capabilities.", "AI": {"tldr": "研究了大型语言模型中意外的语法转换问题，提出了使用稀疏自编码器引导的监督微调方法（SASFT），有效减少语法转换，并保持多语庝能力。", "motivation": "大型语言模型虽然有多语言能力，但在模型响应中会不预期地切换语言，导致可读性差，现有研究对这一问题的机制分析不足且效果有限。", "method": "使用稀疏自编码器对意外的语法转换进行了深入分析，并发现当模型转换到某种语言时，该语言的特征表现出了过高的预激活值。基于这一发现，提出了SASFT（稀疏自编码器引导的监督微调），教导模型在训练过程中维持适当的预激活值。", "result": "在三种语言的五个模型上进行的实验表明，与标准的监督微调相比，SASFT能一致地将意外语法转换减少超过50%，甚至在四个案例中完全消除。此外，SASFT还能保持甚至提高模型在六个跨语言基准测试中的表现。", "conclusion": "SASFT方法显著减少了意外的语法转换，保持或提高了语言模型的多语言性能，表明了其在解决语言模型中的联系混合问题上的有效性。"}}
{"id": "2507.14738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14738", "abs": "https://arxiv.org/abs/2507.14738", "authors": ["Jeannie She", "Katie Spivakovsky"], "title": "MultiRetNet: A Multimodal Vision Model and Deferral System for Staging Diabetic Retinopathy", "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of preventable blindness,\naffecting over 100 million people worldwide. In the United States, individuals\nfrom lower-income communities face a higher risk of progressing to advanced\nstages before diagnosis, largely due to limited access to screening. Comorbid\nconditions further accelerate disease progression. We propose MultiRetNet, a\nnovel pipeline combining retinal imaging, socioeconomic factors, and\ncomorbidity profiles to improve DR staging accuracy, integrated with a clinical\ndeferral system for a clinical human-in-the-loop implementation. We experiment\nwith three multimodal fusion methods and identify fusion through a fully\nconnected layer as the most versatile methodology. We synthesize adversarial,\nlow-quality images and use contrastive learning to train the deferral system,\nguiding the model to identify out-of-distribution samples that warrant\nclinician review. By maintaining diagnostic accuracy on suboptimal images and\nintegrating critical health data, our system can improve early detection,\nparticularly in underserved populations where advanced DR is often first\nidentified. This approach may reduce healthcare costs, increase early detection\nrates, and address disparities in access to care, promoting healthcare equity.", "AI": {"tldr": "我们提出了一种叫做MultiRetNet的新系统，它结合了视网膜成像、经济因素和共病概况以提高诊断准确性，并利用对比学习减少医疗差异，特别是在低收入社区中。这有助于提高早期检出率和医疗服务公平。", "motivation": "糖尿病视网膜病变（DR）是全球可预防失明的主要原因之一，影响着超过1亿的人。在美国，低收入社区的个体由于缺乏筛查而面临更高的风险，病情会在诊断前发展到晚期。我们提出了一种新系统来提高诊断准确性，并解决未充分服务的人群的健康差异问题。", "method": "我们提出了MultiRetNet，这是一种新颖的流水线，结合了视网膜成像、社会经济因素以及共病概况来提高糖尿病视网膜病变（DR）分期的准确性。我们实验了三种多模态融合方法，发现通过全连接层的融合是迄今为止最灵活的方法。此外，我们生成了对抗的、低质量的图像，并利用对比学习训练了一种分诊系统，指导模型识别出需要医生审查的样本。", "result": "研究中的方法通过维持在次优图像上的诊断准确性并整合关键健康数据，我们的系统可以提高早期检测率。这有助于减少医疗成本，提高早期检出率，并解决医疗服务获得的不平等问题，促进医疗服务公平。", "conclusion": "通过结合社会经济因素和共病概况，MultiRetNet是一种创新性工具，可以提高DR的检出率，并减少在未充分服务人群中的医疗差异。"}}
{"id": "2507.14900", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14900", "abs": "https://arxiv.org/abs/2507.14900", "authors": ["Chongxuan Huang", "Yongshi Ye", "Biao Fu", "Qifeng Su", "Xiaodong Shi"], "title": "From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable multilingual\ncapabilities, however, how to evaluate cross-lingual alignment remains\nunderexplored. Existing alignment benchmarks primarily focus on sentence\nembeddings, but prior research has shown that neural models tend to induce a\nnon-smooth representation space, which impact of semantic alignment evaluation\non low-resource languages. Inspired by neuroscientific findings that similar\ninformation activates overlapping neuronal regions, we propose a novel Neuron\nState-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a\nlignment capabilities of LLMs, which offers a more semantically grounded\napproach to assess cross-lingual alignment. We evaluate NeuronXA on several\nprominent multilingual LLMs (LLaMA, Qwen, Mistral, GLM, and OLMo) across two\ntransfer tasks and three multilingual benchmarks. The results demonstrate that\nwith only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation\nof 0.9556 with downstream tasks performance and 0.8514 with transferability.\nThese findings demonstrate NeuronXA's effectiveness in assessing both\ncross-lingual alignment and transferability, even with a small dataset. This\nhighlights its potential to advance cross-lingual alignment research and to\nimprove the semantic understanding of multilingual LLMs.", "AI": {"tldr": "提出了基于神经元状态的跨语言对齐评估方法NeuronXA，有效评估了几个多语言大模型的跨语言对齐能力，并展示了其在小数据集上的有效性。", "motivation": "为了克服现有跨语言对齐基准测试主要集中在句子嵌入上的局限性，这些测试受神经模型产生非平滑表示空间的影响，尤其是在低资源语言上，提出了基于神经元状态的新方法。", "method": "根据神经科学发现，提出基于神经元状态的跨语言对齐（NeuronXA）方法，用以评估语言模型跨语言的对齐能力。", "result": "在二项迁移任务和三项多语言基准测试中证明，NeuronXA在仅有100个并行句子对的情况下，与下游任务表现和迁移能力有很高的皮尔森相关性。", "conclusion": "NeuronXA方法可以有效评估大语言模型的跨语言对齐能力和迁移能力。这种方法显示出其在跨语言对齐研究和改善多语言大型语言模型的语义理解方面的潜力。"}}
{"id": "2507.14743", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14743", "abs": "https://arxiv.org/abs/2507.14743", "authors": ["Joseph Raj Vishal", "Rutuja Patil", "Manas Srinivas Gowda", "Katha Naik", "Yezhou Yang", "Bharatesh Chakravarthi"], "title": "InterAct-Video: Reasoning-Rich Video QA for Urban Traffic", "comment": null, "summary": "Traffic monitoring is crucial for urban mobility, road safety, and\nintelligent transportation systems (ITS). Deep learning has advanced\nvideo-based traffic monitoring through video question answering (VideoQA)\nmodels, enabling structured insight extraction from traffic videos. However,\nexisting VideoQA models struggle with the complexity of real-world traffic\nscenes, where multiple concurrent events unfold across spatiotemporal\ndimensions. To address these challenges, this paper introduces \\textbf{InterAct\nVideoQA}, a curated dataset designed to benchmark and enhance VideoQA models\nfor traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of\nreal-world traffic footage collected from diverse intersections, segmented into\n10-second video clips, with over 25,000 question-answer (QA) pairs covering\nspatiotemporal dynamics, vehicle interactions, incident detection, and other\ncritical traffic attributes. State-of-the-art VideoQA models are evaluated on\nInterAct VideoQA, exposing challenges in reasoning over fine-grained\nspatiotemporal dependencies within complex traffic scenarios. Additionally,\nfine-tuning these models on InterAct VideoQA yields notable performance\nimprovements, demonstrating the necessity of domain-specific datasets for\nVideoQA. InterAct VideoQA is publicly available as a benchmark dataset to\nfacilitate future research in real-world deployable VideoQA models for\nintelligent transportation systems. GitHub Repo:\nhttps://github.com/joe-rabbit/InterAct_VideoQA", "AI": {"tldr": "本文介绍了InterAct VideoQA数据集，用于交通监控任务，旨在改进VideoQA模型，特别是在处理复杂交通场景时的性能。", "motivation": "视频问答（VideoQA）模型在交通监控方面有显著进展，但现有的VideoQA模型在处理真实世界复杂交通场景时遇到了困难。该论文的动机是解决这些挑战。", "method": "本研究提出了InterAct VideoQA数据集，旨在为交通监控任务提供基准并改进VideoQA模型。该数据集包含了8小时的真实世界交通视频片段，每段10秒，总共有超过25000个问题回答对，涵盖时空动态、车辆互动、事故检测等关键交通属性。", "result": "对现有的最佳VideoQA模型进行了评估，并展示了在复杂交通场景中的细粒度时空依赖推理挑战。通过在InterAct VideoQA数据集上的微调，得到了显著的性能改进。", "conclusion": "实验表明，使用现有最佳VideoQA模型在InterAct VideoQA数据集上训练可以显著提高性能，强调了领域特定数据集对于VideoQA研究的重要性，该数据集公开提供以支持未来研究。"}}
{"id": "2507.14913", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14913", "abs": "https://arxiv.org/abs/2507.14913", "authors": ["Eliya Habba", "Noam Dahan", "Gili Lior", "Gabriel Stanovsky"], "title": "PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation", "comment": "Eliya Habba and Noam Dahan contributed equally to this work", "summary": "Evaluating LLMs with a single prompt has proven unreliable, with small\nchanges leading to significant performance differences. However, generating the\nprompt variations needed for a more robust multi-prompt evaluation is\nchallenging, limiting its adoption in practice. To address this, we introduce\nPromptSuite, a framework that enables the automatic generation of various\nprompts. PromptSuite is flexible - working out of the box on a wide range of\ntasks and benchmarks. It follows a modular prompt design, allowing controlled\nperturbations to each component, and is extensible, supporting the addition of\nnew components and perturbation types. Through a series of case studies, we\nshow that PromptSuite provides meaningful variations to support strong\nevaluation practices. It is available through both a Python API:\nhttps://github.com/eliyahabba/PromptSuite, and a user-friendly web interface:\nhttps://promptsuite.streamlit.app/", "AI": {"tldr": "介绍PromptSuite框架，解决自动生成多提示以支持强健评估的挑战。", "motivation": "单一提示评估大语言模型可靠性低，变化小也会导致性能差异大，多提示评估难以广泛采用。", "method": "通过引入PromptSuite框架解决多提示评估中的生成挑战，该框架能自动生成多种提示并支持模块化设计和可控的扰动方式。", "result": "通过一系列案例研究证明，PromptSuite能够提供有意义的提示变化以支持强健的评估实践。", "conclusion": "PromptSuite框架支持在各类任务和基准测试中实现强健的评估实践，并提供了Python API和用户友好的网络界面。"}}
