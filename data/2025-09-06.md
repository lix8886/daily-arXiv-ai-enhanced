<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 21]
- [cs.CV](#cs.CV) [Total: 24]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Speech-Based Cognitive Screening: A Systematic Evaluation of LLM Adaptation Strategies](https://arxiv.org/abs/2509.03525)
*Fatemeh Taherinezhad,Mohamad Javad Momeni Nezhad,Sepehr Karimi,Sina Rashidi,Ali Zolnour,Maryam Dadkhah,Yasaman Haghbin,Hossein AzadMaleki,Maryam Zolnoori*

Main category: cs.CL

> 研究针对DementiaBank语音语料库评估了九种单文本模型和三种多模态音频文本模型，以改进痴呆症检测。发现类中心示范和推理设计对于模型的优化至关重要。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于超过一半的美国老年人患有未被诊断的阿尔茨海默病和相关痴呆症，基于语言的筛查为可扩展的检测方法提供了潜在解决方案。

**Method:** 我们研究了使用DementiaBank语音语料库进行痴呆症检测时，大型语言模型的多种适应策略。这些策略包括上下文学习、不同示范选择策略、推理增强提示、参数高效微调和多模态集成。

**Result:** 研究结果显示，类中心示范在上下文学习中表现最佳，推理改善了较小模型的表现，而标记级微调通常产生最佳分数。增加分类头显著提高了表现不佳的模型。在多模态模型中，微调的音频-文本系统表现出色但未超过顶级文本模型。

**Conclusion:** 这些发现强调了模型适应策略在基于语言的痴呆症检测中的关键作用，这些策略包括示范选择和推理设计。合适适应后的开放式模型可以与商用系统媲美或超过其性能。

**Abstract:** Over half of US adults with Alzheimer disease and related dementias remain
undiagnosed, and speech-based screening offers a scalable detection approach.
We compared large language model adaptation strategies for dementia detection
using the DementiaBank speech corpus, evaluating nine text-only models and
three multimodal audio-text models on recordings from DementiaBank speech
corpus. Adaptations included in-context learning with different demonstration
selection policies, reasoning-augmented prompting, parameter-efficient
fine-tuning, and multimodal integration. Results showed that class-centroid
demonstrations achieved the highest in-context learning performance, reasoning
improved smaller models, and token-level fine-tuning generally produced the
best scores. Adding a classification head substantially improved
underperforming models. Among multimodal models, fine-tuned audio-text systems
performed well but did not surpass the top text-only models. These findings
highlight that model adaptation strategies, including demonstration selection,
reasoning design, and tuning method, critically influence speech-based dementia
detection, and that properly adapted open-weight models can match or exceed
commercial systems.

</details>


### [2] [Enhancing Speech Large Language Models through Reinforced Behavior Alignment](https://arxiv.org/abs/2509.03526)
*Yansong Liu,Jiateng Li,Yuan Liu*

Main category: cs.CL

> 针对基于语音的大型语言模型在指令遵循方面存在的性能差距，本文提出了一个名为强化行为对齐（RBA）的框架。RBA通过自我合成方法生成对齐数据并使用强化学习对齐模型行为，实验表明该方法提升了SpeechLMs的指令遵循性能，并扩展至多种任务中，达到了最新的性能水平。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型（LLMs）的最新进展引发了将它们的语言能力扩展到其他模态（如语音）的研究兴趣，但是由于模态间的差异，这些基于语音的LLMs（SpeechLMs）在指令遵循方面仍存在显著的性能差距，尤其是面对用户语音的动态变化时。

**Method:** 通过引入一个名为强化行为对齐（RBA）的框架来解决这一问题，该框架旨在提高SpeechLMs的语言生成能力。RBA不依赖于基于人类标注的监督微调，而是采用自我合成的方法，通过一个强大的教师LLM生成大量高保真的对齐数据，并使用基于强化学习的方法来对齐SpeechLMs的行为。

**Result:** 实验结果表明，该方法有效地提高了SpeechLMs的指令遵循能力，并超越了传统的蒸馏基线。此外，我们证明了RBA可以无缝扩展到包括口语问答和语音到文本翻译等任务，并在公开基准上取得了最先进的性能。

**Conclusion:** 通过自我生成数据，所提出的方法在开放基准上达到了最先进的性能，证明了RBA框架的有效性和适应性。

**Abstract:** The recent advancements of Large Language Models (LLMs) have spurred
considerable research interest in extending their linguistic capabilities
beyond text to other modalities, which leads to emergence of speech-based LLMs
(SpeechLMs) with capability of processing user request in either speech or
textual formats. However, owing to inter-modal discrepancies, these SpeechLMs
still exhibit a significant performance gap compared to their text-based LLM
counterparts in instruction-following, particularly when confronted with the
dynamic and variable nature of user speech. To address this challenge, this
paper introduces a framework termed Reinforced Behavior Alignment (RBA),
designed to bolster the language generation proficiency of SpeechLMs. Instead
of relying on supervised fine-tuning from human annotations, RBA employs a
self-synthesis methodology to generate extensive, high-fidelity alignment data
by a powerful teacher LLM. Then SpeechLMs is aligned its behavior with that of
a teacher using a reinforcement learning-based approach. Experimental results
demonstrate that this method effectively enhances the instruction-following
capabilities of SpeechLMs that outperform conventional distillation baselines.
Crucially, we demonstrate that RBA can be seamlessly extended to tasks such
including spoken question answering and speech-to-text translation, attaining
state-of-the-art performance on open benchmarks with only self-generated data.

</details>


### [3] [Multilevel Analysis of Cryptocurrency News using RAG Approach with Fine-Tuned Mistral Large Language Model](https://arxiv.org/abs/2509.03527)
*Bohdan M. Pavlyshenko*

Main category: cs.CL

> A fine-tuned Mistral 7B large language model with retrieval-augmented generation is used for multilevel analysis of cryptocurrency news, generating graph and text summaries which are combined into comprehensive reports. This approach reduces model hallucinations and provides high-quality, insightful analysis of the news.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to provide informative qualitative and quantitative analytics of cryptocurrency news using hierarchical analysis techniques. This approach aims to complement existing methods and provide a more comprehensive view of the news, while also aiming to reduce hallucinations typical of large language models through the use of knowledge graphs.

**Method:** The paper uses a fine-tuned Mistral 7B large language model with retrieval-augmented generation (RAG) for multilevel multitask analysis of cryptocurrency news. On the first level, the model generates graph and text summaries with sentiment scores and JSON representations. Higher levels consolidate these summaries into comprehensive reports. The model is fine-tuned with 4-bit quantization using the PEFT/LoRA approach.

**Result:** The results demonstrate that the fine-tuned Mistral 7B LLM models can provide insightful analysis of cryptocurrency news through multilevel summarization and consolidation techniques. The approach effectively combines graph and text-based insights to eliminate model hallucinations.

**Conclusion:** The conclusion is that the method proposed in the paper can effectively perform multilevel analysis of cryptocurrency news, providing both qualitative and quantitative insights. The model, fine-tuned with 4-bit quantization and using knowledge graph representations, reduces hallucinations and enhances the accuracy of the news analysis.

**Abstract:** In the paper, we consider multilevel multitask analysis of cryptocurrency
news using a fine-tuned Mistral 7B large language model with
retrieval-augmented generation (RAG).
  On the first level of analytics, the fine-tuned model generates graph and
text summaries with sentiment scores as well as JSON representations of
summaries. Higher levels perform hierarchical stacking that consolidates sets
of graph-based and text-based summaries as well as summaries of summaries into
comprehensive reports. The combination of graph and text summaries provides
complementary views of cryptocurrency news. The model is fine-tuned with 4-bit
quantization using the PEFT/LoRA approach. The representation of cryptocurrency
news as knowledge graph can essentially eliminate problems with large language
model hallucinations.
  The obtained results demonstrate that the use of fine-tuned Mistral 7B LLM
models for multilevel cryptocurrency news analysis can conduct informative
qualitative and quantitative analytics, providing important insights.

</details>


### [4] [The ProLiFIC dataset: Leveraging LLMs to Unveil the Italian Lawmaking Process](https://arxiv.org/abs/2509.03528)
*Matilde Contestabile,Chiara Ferrara,Alberto Giovannetti,Giovanni Parrillo,Andrea Vandin*

Main category: cs.CL

> 我们提出ProLiFIC，这是一个根据Normattiva门户网站的非结构化数据构建的、从1987年到2022年意大利立法过程的全面事件日志，通过使用大型语言模型，旨在作为法律过程挖掘的基准。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于过程挖掘（PM）在法律领域的效用受到数据集的可访问性和质量的限制，我们引入一个全面的意大利立法过程事件日志，以证明PM在法律领域的潜在价值。

**Method:** 介绍ProLiFIC（意大利立法流程的程序法制定流程），这是一个全面的意大利立法过程事件日志，时间跨度为1987年至2022年。该日志由Normattiva门户网站的非结构化数据创建，并使用大型语言模型（LLMs）进行结构调整，体现了将过程挖掘（PM）与LLMs结合的最新努力。

**Result:** 作为初步分析的例子，提出ProLiFIC作为法律过程挖掘的基准，以推动新的进展。

**Conclusion:** 通过初步分析实例，ProLiFIC被提出作为法律过程挖掘的基准，旨在推动该领域的新发展。

**Abstract:** Process Mining (PM), initially developed for industrial and business
contexts, has recently been applied to social systems, including legal ones.
However, PM's efficacy in the legal domain is limited by the accessibility and
quality of datasets. We introduce ProLiFIC (Procedural Lawmaking Flow in
Italian Chambers), a comprehensive event log of the Italian lawmaking process
from 1987 to 2022. Created from unstructured data from the Normattiva portal
and structured using large language models (LLMs), ProLiFIC aligns with recent
efforts in integrating PM with LLMs. We exemplify preliminary analyses and
propose ProLiFIC as a benchmark for legal PM, fostering new developments.

</details>


### [5] [Multimodal Proposal for an AI-Based Tool to Increase Cross-Assessment of Messages](https://arxiv.org/abs/2509.03529)
*Alejandro Álvarez Castro,Joaquín Ordieres-Meré*

Main category: cs.CL

> A novel multi-modal framework, using a two-stage transformer architecture, is introduced to create structurally aware and emotionally enriched embeddings of earnings call interactions, which generally improves the understanding of the discourse.

<details>
  <summary>Details</summary>

**Motivation:** While previous works on financial sentiment analysis incorporate multi-modal signals, they generally use flat models that fail to capture the hierarchical discourse structure of earnings calls.

**Method:** This paper proposes a two-stage transformer architecture to generate multi-modal discourse embeddings for earnings calls. The first stage encodes multi-modal content and structured metadata at the node level using contrastive learning, while the second synthesizes a global embedding for the entire conference.

**Result:** Experiments show that the generated embeddings are stable and semantically meaningful, capturing affective tone, structural logic, and thematic alignment.

**Conclusion:** The multi-modal discourse representation approach not only provides useful embeddings for financial reporting, but also has the potential to be applied to other domains, such as telemedicine, education, and political discourse.

**Abstract:** Earnings calls represent a uniquely rich and semi-structured source of
financial communication, blending scripted managerial commentary with
unscripted analyst dialogue. Although recent advances in financial sentiment
analysis have integrated multi-modal signals, such as textual content and vocal
tone, most systems rely on flat document-level or sentence-level models,
failing to capture the layered discourse structure of these interactions. This
paper introduces a novel multi-modal framework designed to generate
semantically rich and structurally aware embeddings of earnings calls, by
encoding them as hierarchical discourse trees. Each node, comprising either a
monologue or a question-answer pair, is enriched with emotional signals derived
from text, audio, and video, as well as structured metadata including coherence
scores, topic labels, and answer coverage assessments. A two-stage transformer
architecture is proposed: the first encodes multi-modal content and discourse
metadata at the node level using contrastive learning, while the second
synthesizes a global embedding for the entire conference. Experimental results
reveal that the resulting embeddings form stable, semantically meaningful
representations that reflect affective tone, structural logic, and thematic
alignment. Beyond financial reporting, the proposed system generalizes to other
high-stakes unscripted communicative domains such as tele-medicine, education,
and political discourse, offering a robust and explainable approach to
multi-modal discourse representation. This approach offers practical utility
for downstream tasks such as financial forecasting and discourse evaluation,
while also providing a generalizable method applicable to other domains
involving high-stakes communication.

</details>


### [6] [Reading Between the Signs: Predicting Future Suicidal Ideation from Adolescent Social Media Texts](https://arxiv.org/abs/2509.03530)
*Paul Blum,Enrico Liscio,Ruixuan Zhang,Caroline Figueroa,Pradeep K. Murukannaiah*

Main category: cs.CL

> 研究表明，利用在线论坛数据预测青少年的自杀行为是可能的，并提出了一种基于变压器的预测模型Early-SIB，该模型在测试中表现良好。

<details>
  <summary>Details</summary>

**Motivation:** 自杀是青少年（12-18岁）的主要死因之一，但由于缺乏接触心理健康服务的机会，许多情况下未被发现。鉴于年轻人经常在网上实时分享他们的想法和困扰，本研究希望通过在线论坛帖子预测青少年的自杀意念和行为。

**Method:** 本研究提出了一种基于变压器的Early-SIB模型，该模型可以依次处理用户撰写的帖子及其互动情况，以预测他们是否将发表涉及自杀意念和行为的帖子。

**Result:** Early-SIB模型在荷兰青少年论坛上的SIB预测中达到了0.73的平衡准确性，证明了此模型预测青少年未来SIB行为的有效性。

**Conclusion:** 研究显示，基于变压器的Early-SIB模型可以通过分析青少年在在线论坛上的帖子来预测未来的自杀意念和行为，这表明此类工具可以为传统方法提供有价值的补充。

**Abstract:** Suicide is a leading cause of death among adolescents (12-18), yet predicting
it remains a significant challenge. Many cases go undetected due to a lack of
contact with mental health services. Social media, however, offers a unique
opportunity, as young people often share their thoughts and struggles online in
real time. In this work, we propose a novel task and method to approach it:
predicting suicidal ideation and behavior (SIB) from forum posts before an
adolescent explicitly expresses suicidal ideation on an online forum. This
predictive framing, where no self-disclosure is used as input at any stage,
remains largely unexplored in the suicide prediction literature. To this end,
we introduce Early-SIB, a transformer-based model that sequentially processes
the posts a user writes and engages with to predict whether they will write a
SIB post. Our model achieves a balanced accuracy of 0.73 for predicting future
SIB on a Dutch youth forum, demonstrating that such tools can offer a
meaningful addition to traditional methods.

</details>


### [7] [Real-Time Detection of Hallucinated Entities in Long-Form Generation](https://arxiv.org/abs/2509.03531)
*Oscar Obeso,Andy Arditi,Javier Ferrando,Joshua Freeman,Cameron Holmes,Neel Nanda*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Large language models are now routinely used in high-stakes applications
where hallucinations can cause serious harm, such as medical consultations or
legal advice. Existing hallucination detection methods, however, are
impractical for real-world use, as they are either limited to short factual
queries or require costly external verification. We present a cheap, scalable
method for real-time identification of hallucinated tokens in long-form
generations, and scale it effectively to 70B parameter models. Our approach
targets \emph{entity-level hallucinations} -- e.g., fabricated names, dates,
citations -- rather than claim-level, thereby naturally mapping to token-level
labels and enabling streaming detection. We develop an annotation methodology
that leverages web search to annotate model responses with grounded labels
indicating which tokens correspond to fabricated entities. This dataset enables
us to train effective hallucination classifiers with simple and efficient
methods such as linear probes. Evaluating across four model families, our
classifiers consistently outperform baselines on long-form responses, including
more expensive methods such as semantic entropy (e.g., AUC 0.90 vs 0.71 for
Llama-3.3-70B), and are also an improvement in short-form question-answering
settings. Moreover, despite being trained only with entity-level labels, our
probes effectively detect incorrect answers in mathematical reasoning tasks,
indicating generalization beyond entities. While our annotation methodology is
expensive, we find that annotated responses from one model can be used to train
effective classifiers on other models; accordingly, we publicly release our
datasets to facilitate reuse. Overall, our work suggests a promising new
approach for scalable, real-world hallucination detection.

</details>


### [8] [Topic Identification in LLM Input-Output Pairs through the Lens of Information Bottleneck](https://arxiv.org/abs/2509.03533)
*Igor Halperin*

Main category: cs.CL

> 本文通过提出UDIB算法改进了大型语言模型中的语义发散检测框架，该框架能在处理提示和响应时生成更具信息量和简化的话题表示，从而增强检测编造内容的能力。

<details>
  <summary>Details</summary>

**Motivation:** 传统的语义发散度量(SDM)框架依赖于识别提示和响应之间的潜在主题，通常通过应用于句子嵌入的几何聚类来实现。这种方法侧重于空间接近性而非信息论分析的下游需求。

**Method:** 本研究提出了一种基于确定性信息瓶颈(DIB)的几何聚类方法，通过将DIB方法转化为高维数据的实用算法，用计算效率高的上界替换其不可计算的KL散度项，从而解决了现有SDM框架中话题优化的空间邻近性和信息论分析需求之间的不匹配问题。

**Result:** 研究中产生的一种新方法UDIB，可以看作是一种熵正则化和鲁棒化的K-means，能够自动生成数量精简且最能提供关于提示-响应关系信息的话题表示。这为现有的SDM框架提供了更好的基础，并提供了一种检测编造更敏感的新型工具。

**Conclusion:** 通过将DIB方法转化为针对高维数据的实用版UDIB，并应用于大型语言模型(LLM)提示和响应嵌入的联合聚类，创造出一种不仅空间上一致而且本质上能够最大化提示-响应关系信息的话题表示，从而改善了SDM框架，使之成为检测编造回答的一种更敏感的工具。

**Abstract:** Large Language Models (LLMs) are prone to critical failure modes, including
\textit{intrinsic faithfulness hallucinations} (also known as confabulations),
where a response deviates semantically from the provided context. Frameworks
designed to detect this, such as Semantic Divergence Metrics (SDM), rely on
identifying latent topics shared between prompts and responses, typically by
applying geometric clustering to their sentence embeddings. This creates a
disconnect, as the topics are optimized for spatial proximity, not for the
downstream information-theoretic analysis. In this paper, we bridge this gap by
developing a principled topic identification method grounded in the
Deterministic Information Bottleneck (DIB) for geometric clustering. Our key
contribution is to transform the DIB method into a practical algorithm for
high-dimensional data by substituting its intractable KL divergence term with a
computationally efficient upper bound. The resulting method, which we dub UDIB,
can be interpreted as an entropy-regularized and robustified version of K-means
that inherently favors a parsimonious number of informative clusters. By
applying UDIB to the joint clustering of LLM prompt and response embeddings, we
generate a shared topic representation that is not merely spatially coherent
but is fundamentally structured to be maximally informative about the
prompt-response relationship. This provides a superior foundation for the SDM
framework and offers a novel, more sensitive tool for detecting confabulations.

</details>


### [9] [QuesGenie: Intelligent Multimodal Question Generation](https://arxiv.org/abs/2509.03535)
*Ahmed Mubarak,Amna Ahmed,Amira Nasser,Aya Mohamed,Fares El-Sadek,Mohammed Ahmed,Ahmed Salah,Youssef Sobhy*

Main category: cs.CL

> 该项目开发了一个多模态问题生成系统，能从多种内容格式自动生成各种类型的问题，系统包含四个主要组件：多模态输入处理、问题生成、基于人类反馈的强化学习和一个端到端的交互界面，旨在实现自动化、可扩展且智能的问题生成，同时平衡资源效率、强大功能和用户友好性。

<details>
  <summary>Details</summary>

**Motivation:** 为了解决学习资源丰富但配套练习材料不足的问题，该项目开发了该多模态问题生成系统。

**Method:** 系统设计包含了多模态输入处理、问题生成模块、基于人类反馈的强化学习机制和一个整体的交互界面。

**Result:** 项目的成果是一个能够自动地、智能化地生成多种格式内容的问题生成系统，提高了资源的使用效率并提供了良好的用户体验。

**Conclusion:** 这个项目为自动、可扩展且智能的问题生成奠定了基础，能够有效解决当前学习资源与配套练习材料脱节的情况。

**Abstract:** In today's information-rich era, learners have access to abundant educational
resources, but the lack of practice materials tailored to these resources
presents a significant challenge. This project addresses that gap by developing
a multi-modal question generation system that can automatically generate
diverse question types from various content formats. The system features four
major components: multi-modal input handling, question generation,
reinforcement learning from human feedback (RLHF), and an end-to-end
interactive interface. This project lays the foundation for automated,
scalable, and intelligent question generation, carefully balancing resource
efficiency, robust functionality and a smooth user experience.

</details>


### [10] [AR$^2$: Adversarial Reinforcement Learning for Abstract Reasoning in Large Language Models](https://arxiv.org/abs/2509.03537)
*Cheng-Kai Yeh,Hsing-Wang Lee,Chung-Hung Kuo,Hen-Hsen Huang*

Main category: cs.CL

> AR$^2$ enhances the abstraction abilities of LLMs through adversarial reinforcement learning, improving their accuracy on challenging programming tasks.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this study is to improve abstraction, a crucial skill in computer science, which is not adequately addressed by the existing approaches focused on superficial pattern recognition in LLMs.

**Method:** AR$^2$ (Adversarial Reinforcement Learning for Abstract Reasoning) is introduced to enhance the abstraction abilities of LLMs.

**Result:** Experiments show that AR$^2$ significantly enhances the student model's performance on challenging programming tasks.

**Conclusion:** AR$^2$ demonstrates the importance of focusing on abstraction to improve the generalization ability of LLMs.

**Abstract:** Abstraction--the ability to recognize and distill essential computational
patterns from complex problem statements--is a foundational skill in computer
science, critical both for human problem-solvers and coding-oriented large
language models (LLMs). Despite recent advances in training LLMs for code
generation using reinforcement learning (RL), most existing approaches focus
primarily on superficial pattern recognition, overlooking explicit training for
abstraction. In this study, we propose AR$^2$ (Adversarial Reinforcement
Learning for Abstract Reasoning), a novel framework explicitly designed to
enhance the abstraction abilities of LLMs. AR$^2$ employs a teacher model to
transform kernel problems into narrative-rich, challenging descriptions without
changing their fundamental logic. Simultaneously, a student coding model is
trained to solve these complex narrative problems by extracting their
underlying computational kernels. Experimental results demonstrate that AR$^2$
substantially improves the student model's accuracy on previously unseen,
challenging programming tasks, underscoring abstraction as a key skill for
enhancing LLM generalization.

</details>


### [11] [Improving Factuality in LLMs via Inference-Time Knowledge Graph Construction](https://arxiv.org/abs/2509.03540)
*Shanglin Wu,Lihui Liu,Jinho D. Choi,Kai Shu*

Main category: cs.CL

> A new framework is proposed to dynamically build and expand knowledge graphs during inference, integrating internal and external knowledge for large language models (LLMs) to enhance factuality in responses.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to improve the factuality of large language model (LLM) responses by addressing their difficulty in generating factually consistent answers due to limitations in parametric memory, and the compositional reasoning and factual inconsistency issues posed by traditional retrieval-augmented generation methods.

**Method:** Our method starts by extracting a seed knowledge graph from the question using prompts, then iteratively expands the graph using the LLM's internal knowledge. It selectively refines the graph through external retrieval to enhance factual coverage and correct inaccuracies.

**Result:** The approach was evaluated on three factual question-answering benchmarks, and it showed consistent improvements in factual accuracy, answer precision, and interpretability compared to baseline prompting and static KG-augmented methods.

**Conclusion:** The dynamic construction and expansion of knowledge graphs during inference improve the factuality, precision, and interpretability of large language model answers, offering a structured, interpretable, and scalable enhancement to LLMs.

**Abstract:** Large Language Models (LLMs) often struggle with producing factually
consistent answers due to limitations in their parametric memory.
Retrieval-Augmented Generation (RAG) methods address this issue by
incorporating external knowledge from trusted sources at inference time.
However, such methods typically treat knowledge as unstructured text, which
limits their ability to support compositional reasoning and identify factual
inconsistencies. To overcome these limitations, we propose a novel framework
that dynamically constructs and expands knowledge graphs (KGs) during
inference, integrating both internal knowledge extracted from LLMs and external
information retrieved from external sources. Our method begins by extracting a
seed KG from the question via prompting, followed by iterative expansion using
the LLM's latent knowledge. The graph is then selectively refined through
external retrieval, enhancing factual coverage and correcting inaccuracies. We
evaluate our approach on three diverse factual QA benchmarks, demonstrating
consistent improvements in factual accuracy, answer precision, and
interpretability over baseline prompting and static KG-augmented methods. Our
findings suggest that inference-time KG construction is a promising direction
for enhancing LLM factuality in a structured, interpretable, and scalable
manner.

</details>


### [12] [ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference](https://arxiv.org/abs/2509.03565)
*Qi Chen,Jingxuan Wei,Zhuoya Yao,Haiguang Wang,Gaowei Wu,Bihui Yu,Siyuan Li,Cheng Tan*

Main category: cs.CL

> 本文提出了多文档科学推理任务，旨在从相关论文中提取并整合动机、方法与实验结果，以重建研究发展链。为此设计了ResearchPulse框架，包括三个代理：计划代理、动机-方法心智图代理和实验线图合成代理。通过ResearchPulse-Bench基准测试，研究展示了其系统在语义对齐、结构一致性和视觉保真性上优于GPT-4等方法。

<details>
  <summary>Details</summary>

**Motivation:** 理解科学思想如何演变不仅仅是总结单篇论文，还需要进行结构化、跨越文档的研究推理。

**Method:** 提出了ResearchPulse框架，包含三个代理来处理任务分解、动机-方法心智图构建以及实验线图合成。

**Result:** 实验表明，尽管使用的是7B规模的代理，系统在语义对齐、结构一致性和视觉保真性方面超越了强大的基线方法。

**Conclusion:** 本文引入了一个新的科学研究任务和用于完成该任务的框架，展示了其在相关任务中的优越性能，并提供了用于评估的基准数据集。

**Abstract:** Understanding how scientific ideas evolve requires more than summarizing
individual papers-it demands structured, cross-document reasoning over
thematically related research. In this work, we formalize multi-document
scientific inference, a new task that extracts and aligns motivation,
methodology, and experimental results across related papers to reconstruct
research development chains. This task introduces key challenges, including
temporally aligning loosely structured methods and standardizing heterogeneous
experimental tables. We present ResearchPulse, an agent-based framework that
integrates instruction planning, scientific content extraction, and structured
visualization. It consists of three coordinated agents: a Plan Agent for task
decomposition, a Mmap-Agent that constructs motivation-method mind maps, and a
Lchart-Agent that synthesizes experimental line charts. To support this task,
we introduce ResearchPulse-Bench, a citation-aware benchmark of annotated paper
clusters. Experiments show that our system, despite using 7B-scale agents,
consistently outperforms strong baselines like GPT-4o in semantic alignment,
structural consistency, and visual fidelity. The dataset are available in
https://huggingface.co/datasets/ResearchPulse/ResearchPulse-Bench.

</details>


### [13] [NoteBar: An AI-Assisted Note-Taking System for Personal Knowledge Management](https://arxiv.org/abs/2509.03610)
*Josh Wisoff,Yao Tang,Zhengyu Fang,Jordan Guzman,YuTang Wang,Alex Yu*

Main category: cs.CL

> NoteBar是一款结合Persona信息和高效大型语言模型进行自动笔记分类的AI辅助工具，配有支持研究与评估的数据集，部署成本低且效率高。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于大型语言模型成功的加速推动了AI辅助工具的发展，但现有解决方案在效率方面常常表现不佳，提出了NoteBar以期改善这一情况。

**Method:** NoteBar采用大型语言模型，并结合Persona信息来自动对笔记进行分类和组织，旨在提高工作效率。同时，引入了一个新的带有16种MBTI人设注解的数据集，该数据集含有3,173条笔记和8,494个标注概念，以此支持研究与评估。

**Result:** NoteBar能以实用且经济的方式部署，支持无需依赖大量基础设施的互动使用，显示出其作为AI辅助个人知识管理的基础功能。

**Conclusion:** NoteBar及其配套数据集为推进AI辅助个人知识管理工作提供了一个可扩展、可扩展的基础。

**Abstract:** Note-taking is a critical practice for capturing, organizing, and reflecting
on information in both academic and professional settings. The recent success
of large language models has accelerated the development of AI-assisted tools,
yet existing solutions often struggle with efficiency. We present NoteBar, an
AI-assisted note-taking tool that leverages persona information and efficient
language models to automatically organize notes into multiple categories and
better support user workflows. To support research and evaluation in this
space, we further introduce a novel persona-conditioned dataset of 3,173 notes
and 8,494 annotated concepts across 16 MBTI personas, offering both diversity
and semantic richness for downstream tasks. Finally, we demonstrate that
NoteBar can be deployed in a practical and cost-effective manner, enabling
interactive use without reliance on heavy infrastructure. Together, NoteBar and
its accompanying dataset provide a scalable and extensible foundation for
advancing AI-assisted personal knowledge management.

</details>


### [14] [E-ARMOR: Edge case Assessment and Review of Multilingual Optical Character Recognition](https://arxiv.org/abs/2509.03615)
*Aryan Gupta,Anupam Purwar*

Main category: cs.CL

> 研究发现，尽管大型视觉语言模型在某些指标上表现突出，但在资源受限的边缘部署场景中，传统OCR系统因其低计算需求、低延迟和高性价比，仍然是最优选择。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于评估大视觉语言模型在多语言、嘈杂和多样的现实世界图像中的OCR能力，并对比传统的OCR系统，尤其是在资源受限的边缘部署环境下的性能。

**Method:** 本文介绍了一种名为Sprinklr-Edge-OCR的新型OCR系统，该系统专门针对资源受限环境进行了优化。研究团队对五种先进的大视觉语言模型（LVLM）和两种传统的OCR系统进行了大规模的对比评估。评估基于一个专有的双手动注释数据集，该数据集包含了54种语言的图像。

**Result:** Qwen在精度上表现最佳（精度为0.54），而Sprinklr-Edge-OCR在整体F1得分上表现最好（0.46），并且在效率方面优于其他模型，图像处理速度加快35%，每处理1000张图像的成本仅为0.006美元，远低于LVLM。

**Conclusion:** 研究结论表明，即使在大型语言模型时代，传统的OCR系统仍是边缘部署最优化的选择，因其低计算资源需求、低延迟和高性价比。

**Abstract:** Optical Character Recognition (OCR) in multilingual, noisy, and diverse
real-world images remains a significant challenge for optical character
recognition systems. With the rise of Large Vision-Language Models (LVLMs),
there is growing interest in their ability to generalize and reason beyond
fixed OCR pipelines. In this work, we introduce Sprinklr-Edge-OCR, a novel OCR
system built specifically optimized for edge deployment in resource-constrained
environments. We present a large-scale comparative evaluation of five
state-of-the-art LVLMs (InternVL, Qwen, GOT OCR, LLaMA, MiniCPM) and two
traditional OCR systems (Sprinklr-Edge-OCR, SuryaOCR) on a proprietary, doubly
hand annotated dataset of multilingual (54 languages) images. Our benchmark
covers a broad range of metrics including accuracy, semantic consistency,
language coverage, computational efficiency (latency, memory, GPU usage), and
deployment cost. To better reflect real-world applicability, we also conducted
edge case deployment analysis, evaluating model performance on CPU only
environments. Among the results, Qwen achieved the highest precision (0.54),
while Sprinklr-Edge-OCR delivered the best overall F1 score (0.46) and
outperformed others in efficiency, processing images 35 faster (0.17 seconds
per image on average) and at less than 0.01 of the cost (0.006 USD per 1,000
images) compared to LVLM. Our findings demonstrate that the most optimal OCR
systems for edge deployment are the traditional ones even in the era of LLMs
due to their low compute requirements, low latency, and very high
affordability.

</details>


### [15] [Breaking the Mirror: Activation-Based Mitigation of Self-Preference in LLM Evaluators](https://arxiv.org/abs/2509.03647)
*Dani Roytburg,Matthew Bozoukov,Matthew Nguyen,Jou Barzdukas,Simon Fu,Narmeen Oozeer*

Main category: cs.CL

> 论文研究了如何通过使用引导向量减少大型语言模型在评估任务中的自我偏好偏差，结果显示其在减少不合理的自我偏好偏差方面非常有效，但对合理自我偏好和无偏见一致性的影响不稳定。

<details>
  <summary>Details</summary>

**Motivation:** 研究目的是探讨在推理阶段使用轻量级引导向量是否能够减少大型语言模型的自我偏好偏差，而无需重新训练模型。

**Method:** 我们引入了一个精选的数据集，将自我偏好偏差区分为合理的自我偏好示例和不合理的自我偏好示例，并使用两种方法构建了引导向量：对比激活增强（CAA）和基于优化的方法。

**Result:** 实验结果显示，引导向量可以将不合理的自我偏好偏差减少高达97%，远超过提示方法和直接偏好优化基线。然而，引导向量在合理的自我偏好和无偏见的一致性方面不稳定，这表明自我偏好跨越了多个或非线性方向。

**Conclusion:** 研究表明，引导向量作为保护措施在减少大型语言模型作为评估器时的自我偏好偏差方面显示出潜力和局限性，这推动了更加强大的干预措施的研究。

**Abstract:** Large language models (LLMs) increasingly serve as automated evaluators, yet
they suffer from "self-preference bias": a tendency to favor their own outputs
over those of other models. This bias undermines fairness and reliability in
evaluation pipelines, particularly for tasks like preference tuning and model
routing. We investigate whether lightweight steering vectors can mitigate this
problem at inference time without retraining. We introduce a curated dataset
that distinguishes self-preference bias into justified examples of
self-preference and unjustified examples of self-preference, and we construct
steering vectors using two methods: Contrastive Activation Addition (CAA) and
an optimization-based approach. Our results show that steering vectors can
reduce unjustified self-preference bias by up to 97\%, substantially
outperforming prompting and direct preference optimization baselines. Yet
steering vectors are unstable on legitimate self-preference and unbiased
agreement, implying self-preference spans multiple or nonlinear directions.
This underscores both their promise and limits as safeguards for LLM-as-judges
and motivates more robust interventions.

</details>


### [16] [Semantic Analysis of SNOMED CT Concept Co-occurrences in Clinical Documentation using MIMIC-IV](https://arxiv.org/abs/2509.03662)
*Ali Noori,Somya Mohanty,Prashanti Manda*

Main category: cs.CL

> 研究通过分析临床笔记中的SNOMED CT概念，利用NPMI和预训练嵌入，探讨了概念共现和语义相似性之间的关系，并展示了这种方法在改善临床记录完整性和支持决策方面的效用。

<details>
  <summary>Details</summary>

**Motivation:** 临床笔记包含丰富的临床叙述，但其非结构化格式给大规模分析带来了挑战。标准化术语，如SNOMED CT，提高了互操作性，但理解概念之间的共现关系和语义相似性仍然研究不够。本研究旨在解决这一问题。

**Method:** 本研究通过利用MIMIC-IV数据库，旨在研究SNOMED CT概念的共现模式和基于嵌入的语义相似性之间的关系。采用归一化点互信息(NPMI)和预训练嵌入（例如ClinicalBERT，BioBERT），研究了频率共现的概念是否在语义上接近，嵌入能否提供缺失的概念，以及这些关系随着时间的推移和在不同的专业领域中的变化。

**Result:** 研究结果表明，尽管共现和语义相似性之间存在弱相关性，但嵌入捕捉到的临床相关性不一定反映在文档频率中。基于嵌入的建议经常匹配后来记录的概念，支持其用于增强临床注释的效用。概念嵌入的聚类产生了连贯的临床主题（如症状、实验室结果、诊断和心血管状况），这些与患者的临床表型和护理模式相匹配。最后，共现模式与如死亡率和再入院等结果相关，证明了这种方法的实际效用。

**Conclusion:** 我们发现，共现统计和语义嵌入在提高文件完整性、揭示潜在临床关系以及为决策支持和表型应用提供信息方面具有互补价值。

**Abstract:** Clinical notes contain rich clinical narratives but their unstructured format
poses challenges for large-scale analysis. Standardized terminologies such as
SNOMED CT improve interoperability, yet understanding how concepts relate
through co-occurrence and semantic similarity remains underexplored. In this
study, we leverage the MIMIC-IV database to investigate the relationship
between SNOMED CT concept co-occurrence patterns and embedding-based semantic
similarity. Using Normalized Pointwise Mutual Information (NPMI) and pretrained
embeddings (e.g., ClinicalBERT, BioBERT), we examine whether frequently
co-occurring concepts are also semantically close, whether embeddings can
suggest missing concepts, and how these relationships evolve temporally and
across specialties. Our analyses reveal that while co-occurrence and semantic
similarity are weakly correlated, embeddings capture clinically meaningful
associations not always reflected in documentation frequency. Embedding-based
suggestions frequently matched concepts later documented, supporting their
utility for augmenting clinical annotations. Clustering of concept embeddings
yielded coherent clinical themes (symptoms, labs, diagnoses, cardiovascular
conditions) that map to patient phenotypes and care patterns. Finally,
co-occurrence patterns linked to outcomes such as mortality and readmission
demonstrate the practical utility of this approach. Collectively, our findings
highlight the complementary value of co-occurrence statistics and semantic
embeddings in improving documentation completeness, uncovering latent clinical
relationships, and informing decision support and phenotyping applications.

</details>


### [17] [MLSD: A Novel Few-Shot Learning Approach to Enhance Cross-Target and Cross-Domain Stance Detection](https://arxiv.org/abs/2509.03725)
*Parush Gera,Tempestt Neal*

Main category: cs.CL

> 本文提出并验证了一种跨领域和目标立场检测新方法MLSD，该方法有助于增强模型在多样环境下的表现。

<details>
  <summary>Details</summary>

**Motivation:** 为了实现更有效的跨领域和跨目标的立场检测，提升模型在新领域中迁移学习的能力，作者提出此方法以增强模型在不同环境下的表现。

**Method:** MLSD利用度量学习和三元损失技术，构建了判别性嵌入空间，这种方法有助于在新领域中获取有用的特征实例，提升模型适应跨领域和跨目标情境的能力。

**Result:** 本文提出了一种新的跨领域和目标立场检测方法Metric Learning-Based Few-Shot Learning for Cross-Target and Cross-Domain Stance Detection (MLSD)，利用三元损失的度量学习来捕捉立场目标之间的语义相似性和差异，增强了领域的适应性。通过建立判别嵌入空间，MLSD允许跨目标或跨领域的立场检测模型从新的目标领域中获取有用的例子。我们在两个数据集中的多种跨目标和跨领域场景中对MLSD进行了评估，结果显示，MMLSD在六种常用的立场检测模型上立场检测性能有统计学显著的提高。

**Conclusion:** 实验结果表明，相比于六种常用的立场检测模型，MLSD在多项跨目标和跨领域的场景中展示了显著的性能提升。这表明使用三元损失的度量学习方法能够有效提升立场检测系统的性能。

**Abstract:** We present the novel approach for stance detection across domains and
targets, Metric Learning-Based Few-Shot Learning for Cross-Target and
Cross-Domain Stance Detection (MLSD). MLSD utilizes metric learning with
triplet loss to capture semantic similarities and differences between stance
targets, enhancing domain adaptation. By constructing a discriminative
embedding space, MLSD allows a cross-target or cross-domain stance detection
model to acquire useful examples from new target domains. We evaluate MLSD in
multiple cross-target and cross-domain scenarios across two datasets, showing
statistically significant improvement in stance detection performance across
six widely used stance detection models.

</details>


### [18] [SiLVERScore: Semantically-Aware Embeddings for Sign Language Generation Evaluation](https://arxiv.org/abs/2509.03791)
*Saki Imai,Mert İnan,Anthony Sicilia,Malihe Alikhani*

Main category: cs.CL

> 本论文提出了一种新的手语生成评估方法SiLVERScore，针对传统评估方法中存在的多模态信息无法捕捉及评估误差来源不明确的问题，利用一个联合嵌入空间进行语义感知的评估。

<details>
  <summary>Details</summary>

**Motivation:** 现有的手语生成评估方法通过将生成的手语重新识别为文本并与参考文本进行比较，这种方法引入了二步评估流程的问题，难以准确反映手语的多模态性质，并且无法明确评估误差的来源是生成模型还是评价系统。

**Method:** 我们提出了一种名为SiLVERScore的新方法，它是一种基于嵌入的评价指标，能够在一个联合嵌入空间中评估手语生成的有效性。这一方法能够考虑语义信息，更加准确地评价手语生成的质量。

**Result:** 在PHOENIX-14T和CSL-Daily数据集上，SiLVERScore在正确与随机手语对之间的区分度上取得了接近完美的表现（ROC AUC = 0.99，重叠度小于7%），大大优于传统评估方法。

**Conclusion:** 通过对手语生成的质量进行语义感知的评价，SiLVERScore解决了传统评估方式的局限性，并且在多种数据集上展示了其可靠性和应用性能。

**Abstract:** Evaluating sign language generation is often done through back-translation,
where generated signs are first recognized back to text and then compared to a
reference using text-based metrics. However, this two-step evaluation pipeline
introduces ambiguity: it not only fails to capture the multimodal nature of
sign language-such as facial expressions, spatial grammar, and prosody-but also
makes it hard to pinpoint whether evaluation errors come from sign generation
model or the translation system used to assess it. In this work, we propose
SiLVERScore, a novel semantically-aware embedding-based evaluation metric that
assesses sign language generation in a joint embedding space. Our contributions
include: (1) identifying limitations of existing metrics, (2) introducing
SiLVERScore for semantically-aware evaluation, (3) demonstrating its robustness
to semantic and prosodic variations, and (4) exploring generalization
challenges across datasets. On PHOENIX-14T and CSL-Daily datasets, SiLVERScore
achieves near-perfect discrimination between correct and random pairs (ROC AUC
= 0.99, overlap < 7%), substantially outperforming traditional metrics.

</details>


### [19] [Measuring How (Not Just Whether) VLMs Build Common Ground](https://arxiv.org/abs/2509.03805)
*Saki Imai,Mert İnan,Anthony Sicilia,Malihe Alikhani*

Main category: cs.CL

> 针对视觉语言模型在交互式情境下的grounding能力评估，提出了一个新的评价体系，结果显示某些模型与人类的实际表现存在较大差距。这套新体系为今后的研究指明了方向。

<details>
  <summary>Details</summary>

**Motivation:** 当前的基准评估在单轮或问答设置中进行，然而，grounding是一个交互过程，在这个过程中，人们通过持续沟通逐步建立共同理解。

**Method:** 介绍了一个四指标套件（grounding效率、内容对齐、词汇适应性和类人性）来系统评估视觉语言模型（VLMs）在交互式grounding场景中的表现。通过部署这个套件，对150个自玩互动参照游戏会话中的三个专有VLMs进行评估，并将它们与人类二元组进行了比较。

**Result:** 所有三个模型在至少三个指标上偏离了人类模式，而GPT4o-mini总体上最为接近。发现（i）任务成功得分并不能表明成功的grounding；（ii）高图像-话语对齐并不一定预示任务成功。

**Conclusion:** 这套指标体系和发现为未来关于VLM ground的研究提供了一个框架。

**Abstract:** Large vision language models (VLMs) increasingly claim reasoning skills, yet
current benchmarks evaluate them in single-turn or question answering settings.
However, grounding is an interactive process in which people gradually develop
shared understanding through ongoing communication. We introduce a four-metric
suite (grounding efficiency, content alignment, lexical adaptation, and
human-likeness) to systematically evaluate VLM performance in interactive
grounding contexts. We deploy the suite on 150 self-play sessions of
interactive referential games between three proprietary VLMs and compare them
with human dyads. All three models diverge from human patterns on at least
three metrics, while GPT4o-mini is the closest overall. We find that (i) task
success scores do not indicate successful grounding and (ii) high
image-utterance alignment does not necessarily predict task success. Our metric
suite and findings offer a framework for future research on VLM grounding.

</details>


### [20] [Align-then-Slide: A complete evaluation framework for Ultra-Long Document-Level Machine Translation](https://arxiv.org/abs/2509.03809)
*Jiaxin Guo,Daimeng Wei,Yuanchang Luo,Xiaoyu Chen,Zhanglin Wu,Huan Yang,Hengchao Shang,Zongyao Li,Zhiqiang Rao,Jinlong Yang,Hao Yang*

Main category: cs.CL

> 提出了一种新框架Align-then-Slide用于超长文档级机器翻译评估，该框架包括自动句子级对齐和多粒度滑动评估两个阶段，能有效解决大型语言模型在doc-mt中的评估挑战，显示出良好的与人工评价一致性和翻译质量提升。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型促进了文档级机器翻译的发展，但挑战了假设句与句对齐的现有评估方法。这个问题促使研究者开发一种新的、更适合超长文本翻译评估的方法。

**Method:** 介绍了一种名为Align-then-Slide的评估框架，用于超长文档级机器翻译的评估。该框架包含两个阶段：对齐阶段，自动推断源语言和目标语言句子级别的对应关系，并调整目标语言句子数量，解决漏译和多对一或多对多的问题；n-Chunk滑动评估阶段，计算1-, 2-, 3-, 4-chunk的平均评估分数，以实现多粒度评估。

**Result:** 实验结果显示，该方法在WMT基准数据集上和专家MQM排名之间的皮尔逊相关系数为0.929。在新构建的真实世界测试集上，该方法也与人类判断高度一致。此外，Align-then-Slide生成的偏好数据可用于CPO训练以及作为GRPO的奖励模型，从而生成更受欢迎的翻译结果。

**Conclusion:** 该研究打造了一种准确、稳健且有助于采取行动的doc-mt系统评估工具。通过该框架，可以有效解决超长文本翻译中的对齐和评估问题。

**Abstract:** Large language models (LLMs) have ushered in a new era for document-level
machine translation (\textit{doc}-mt), yet their whole-document outputs
challenge existing evaluation methods that assume sentence-by-sentence
alignment. We introduce \textit{\textbf{Align-then-Slide}}, a complete
evaluation framework for ultra-long doc-mt. In the Align stage, we
automatically infer sentence-level source-target correspondences and rebuild
the target to match the source sentence number, resolving omissions and
many-to-one/one-to-many mappings. In the n-Chunk Sliding Evaluate stage, we
calculate averaged metric scores under 1-, 2-, 3- and 4-chunk for
multi-granularity assessment. Experiments on the WMT benchmark show a Pearson
correlation of 0.929 between our method with expert MQM rankings. On a newly
curated real-world test set, our method again aligns closely with human
judgments. Furthermore, preference data produced by Align-then-Slide enables
effective CPO training and its direct use as a reward model for GRPO, both
yielding translations preferred over a vanilla SFT baseline. The results
validate our framework as an accurate, robust, and actionable evaluation tool
for doc-mt systems.

</details>


### [21] [NE-PADD: Leveraging Named Entity Knowledge for Robust Partial Audio Deepfake Detection via Attention Aggregation](https://arxiv.org/abs/2509.03829)
*Huhong Xian,Rui Liu,Berrak Sisman,Haizhou Li*

Main category: cs.CL

> NE-PADD proposes a new approach for partial audio deepfake detection that uses two attention mechanisms to incorporate named entity knowledge, achieving superior performance on the PartialSpoof-NER dataset.

<details>
  <summary>Details</summary>

**Motivation:** To address the underexplored area of leveraging semantic information, particularly named entities, for frame-level positioning in partial audio deepfake detection.

**Method:** Our method, NE-PADD, comprises two parallel branches: SpeechNER and PADD, incorporating two attention mechanisms: AF and AT to leverage named entity knowledge for frame-level fake speech detection.

**Result:** Experiments on PartialSpoof-NER dataset show that NE-PADD outperforms existing baselines, demonstrating the effectiveness of integrating named entity knowledge.

**Conclusion:** The integration of named entity knowledge through the proposed NE-PADD method proves effective in enhancing the performance of partial audio deepfake detection.

**Abstract:** Different from traditional sentence-level audio deepfake detection (ADD),
partial audio deepfake detection (PADD) requires frame-level positioning of the
location of fake speech. While some progress has been made in this area,
leveraging semantic information from audio, especially named entities, remains
an underexplored aspect. To this end, we propose NE-PADD, a novel method for
Partial Audio Deepfake Detection (PADD) that leverages named entity knowledge
through two parallel branches: Speech Name Entity Recognition (SpeechNER) and
PADD. The approach incorporates two attention aggregation mechanisms: Attention
Fusion (AF) for combining attention weights and Attention Transfer (AT) for
guiding PADD with named entity semantics using an auxiliary loss. Built on the
PartialSpoof-NER dataset, experiments show our method outperforms existing
baselines, proving the effectiveness of integrating named entity knowledge in
PADD. The code is available at https://github.com/AI-S2-Lab/NE-PADD.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [22] [Towards Efficient General Feature Prediction in Masked Skeleton Modeling](https://arxiv.org/abs/2509.03609)
*Shengkai Sun,Zefan Zhang,Jianfeng Dong,Zhiyong Cheng,Xiaojun Chang,Meng Wang*

Main category: cs.CV

> 论文提出了GFP框架，通过高级特征预测方法改进了基于掩码自编码器的骨架动作识别模型，提高了计算效率和特征表示的质量，实现了在多个数据集上的最优性能。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在解决现有的基于掩码自编码器的方法存在计算冗余和语义表示有限的问题，通过提出新的框架来改进基于骨架的动作识别的自监督学习方法。

**Method:** 提出了一种名为GFP（通用特征预测框架）的新方法，该方法通过使用轻量级的目标生成网络来进行空间-时间分层的多样化监督信号生成，从而动态地取代传统的低级重建目标，生成从局部运动模式到全局语义表示的高级特征预测。这种方法通过约束优化来确保特征多样性并防止模型崩溃。

**Result:** 实验表明，在NTU RGB+D 60、NTU RGB+D 120和PKU-MMD数据集上，该方法不仅训练速度提高了6.2倍，而且在多种下游任务中都达到了最先进的性能。

**Conclusion:** 研究表明，使用高级特征预测替代低级重建目标能够显著提高计算效率和特征表示的质量，为基于骨架的动作识别任务提供了有效且高效的解决方案。

**Abstract:** Recent advances in the masked autoencoder (MAE) paradigm have significantly
propelled self-supervised skeleton-based action recognition. However, most
existing approaches limit reconstruction targets to raw joint coordinates or
their simple variants, resulting in computational redundancy and limited
semantic representation. To address this, we propose a novel General Feature
Prediction framework (GFP) for efficient mask skeleton modeling. Our key
innovation is replacing conventional low-level reconstruction with high-level
feature prediction that spans from local motion patterns to global semantic
representations. Specifically, we introduce a collaborative learning framework
where a lightweight target generation network dynamically produces diversified
supervision signals across spatial-temporal hierarchies, avoiding reliance on
pre-computed offline features. The framework incorporates constrained
optimization to ensure feature diversity while preventing model collapse.
Experiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits
of our approach: Computational efficiency (with 6.2$\times$ faster training
than standard masked skeleton modeling methods) and superior representation
quality, achieving state-of-the-art performance in various downstream tasks.

</details>


### [23] [Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge](https://arxiv.org/abs/2509.03614)
*Seungho Choe,Xiaoli Qin,Abubakr Shafique,Amanda Dy,Susan Done,Dimitrios Androutsos,April Khademi*

Main category: cs.CV

> 本文提出一种结合分割和分类的师生模型以应对有丝分裂检测中的领域偏移问题，实验证明其在初步测试集上的表现良好。

<details>
  <summary>Details</summary>

**Motivation:** 病理学家进行有丝分裂计数是耗时的，并且导致观察者之间的一致性问题。人工智能（AI）提出通过自动化检测有丝分裂，并保持决策一致性来解决这一问题。但是，AI工具容易受到领域偏移的影响，在训练集和测试集之间的性能会显著下降，这种差异包括器官、物种之间的形态多样性，以及染色方案的变化。此外，有丝分裂的数量远少于正常细胞核的数量，这引入了高度不平衡的数据检测任务。

**Method:** 本研究将有丝分裂检测定义为像素级别的分割问题，并提出了一种师生模型，该模型同时解决了有丝分裂检测（Track 1）和非典型有丝分裂分类（Track 2）的问题。方法基于UNet分割骨干网络，并整合了领域泛化模块，包括对比表征学习和领域对抗训练。通过师生策略生成像素级别的伪掩模，不仅用于标注的有丝分裂和难以区分的负样本，还有普通的细胞核，从而增强特征的区分性并改善领域偏移的鲁棒性。对于分类任务，采用了一个多尺度CNN分类器，利用分割模型中的特征图进行多任务学习。

**Result:** 在初步测试集上，算法在Track 1中的F1得分为0.7660，在Track 2中的平衡准确率为0.8414，展示了结合基于分割的检测和分类到统一框架中进行稳健有丝分裂分析的有效性。

**Conclusion:** 提出的方法通过整合领域泛化的模块和一个师生策略实现了一种稳健的有丝分裂分析系统，该系统在初步测试集上展示了良好的性能，为有丝分裂检测和分类任务提供了一个可靠的方法。

**Abstract:** Counting mitotic figures is time-intensive for pathologists and leads to
inter-observer variability. Artificial intelligence (AI) promises a solution by
automatically detecting mitotic figures while maintaining decision consistency.
However, AI tools are susceptible to domain shift, where a significant drop in
performance can occur due to differences in the training and testing sets,
including morphological diversity between organs, species, and variations in
staining protocols. Furthermore, the number of mitoses is much less than the
count of normal nuclei, which introduces severely imbalanced data for the
detection task. In this work, we formulate mitosis detection as a pixel-level
segmentation and propose a teacher-student model that simultaneously addresses
mitosis detection (Track 1) and atypical mitosis classification (Track 2). Our
method is based on a UNet segmentation backbone that integrates domain
generalization modules, namely contrastive representation learning and
domain-adversarial training. A teacher-student strategy is employed to generate
pixel-level pseudo-masks not only for annotated mitoses and hard negatives but
also for normal nuclei, thereby enhancing feature discrimination and improving
robustness against domain shift. For the classification task, we introduce a
multi-scale CNN classifier that leverages feature maps from the segmentation
model within a multi-task learning paradigm. On the preliminary test set, the
algorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of
0.8414 in Track 2, demonstrating the effectiveness of integrating
segmentation-based detection and classification into a unified framework for
robust mitosis analysis.

</details>


### [24] [Multi Attribute Bias Mitigation via Representation Learning](https://arxiv.org/abs/2509.03616)
*Rajeev Ranjan Dwivedi,Ankur Kumar,Vinod K Kurmi*

Main category: cs.CV

> GMBM 是一个双阶段框架，旨在解决视觉识别中的多重偏差问题，通过训练阶段识别偏差并在测试阶段消除它，从而提升模型在复杂偏差情境下的表现。

<details>
  <summary>Details</summary>

**Motivation:** 现代视觉模型在面对包括纹理、水印、化妆和场景对象搭配等多重重叠偏差时，其健壮性和公平性都会受到影响。单独缓解这些偏差是不够的，因为消除一个偏差可能允许或加剧其他偏差。

**Method:** GMBM 框架包括两个阶段：1) ABIL 阶段通过训练属性编码器并将其与主骨架网络集成来识别已知捷径的影响；2) 通过梯度抑制微调去掉与偏差相关联的梯度方向，得到一个简洁的忽略捷径的网络。

**Result:** 该方法在 FB CMNIST、CelebA 和 COCO 数据集上验证效果，提升了最差结果组的准确率，减少了多属性偏差放大，并在 SBA 评分中取得了新的低分。

**Conclusion:** GMBM 是第一个实际的端到端多重偏差解决方案，能够在偏差复杂性和分布变化增加的情况下提高视觉识别模型的性能。

**Abstract:** Real world images frequently exhibit multiple overlapping biases, including
textures, watermarks, gendered makeup, scene object pairings, etc. These biases
collectively impair the performance of modern vision models, undermining both
their robustness and fairness. Addressing these biases individually proves
inadequate, as mitigating one bias often permits or intensifies others. We
tackle this multi bias problem with Generalized Multi Bias Mitigation (GMBM), a
lean two stage framework that needs group labels only while training and
minimizes bias at test time. First, Adaptive Bias Integrated Learning (ABIL)
deliberately identifies the influence of known shortcuts by training encoders
for each attribute and integrating them with the main backbone, compelling the
classifier to explicitly recognize these biases. Then Gradient Suppression Fine
Tuning prunes those very bias directions from the backbone's gradients, leaving
a single compact network that ignores all the shortcuts it just learned to
recognize. Moreover we find that existing bias metrics break under subgroup
imbalance and train test distribution shifts, so we introduce Scaled Bias
Amplification (SBA): a test time measure that disentangles model induced bias
amplification from distributional differences. We validate GMBM on FB CMNIST,
CelebA, and COCO, where we boost worst group accuracy, halve multi attribute
bias amplification, and set a new low in SBA even as bias complexity and
distribution shifts intensify, making GMBM the first practical, end to end
multibias solution for visual recognition. Project page:
http://visdomlab.github.io/GMBM/

</details>


### [25] [Lightweight image segmentation for echocardiography](https://arxiv.org/abs/2509.03631)
*Anders Kjelsrud,Lasse Løvstakken,Erik Smistad,Håvard Dalen,Gilles Van De Vyver*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Accurate segmentation of the left ventricle in echocardiography can enable
fully automatic extraction of clinical measurements such as volumes and
ejection fraction. While models configured by nnU-Net perform well, they are
large and slow, thus limiting real-time use. We identified the most effective
components of nnU-Net for cardiac segmentation through an ablation study,
incrementally evaluating data augmentation schemes, architectural
modifications, loss functions, and post-processing techniques. Our analysis
revealed that simple affine augmentations and deep supervision drive
performance, while complex augmentations and large model capacity offer
diminishing returns. Based on these insights, we developed a lightweight U-Net
(2M vs 33M parameters) that achieves statistically equivalent performance to
nnU-Net on CAMUS (N=500) with Dice scores of 0.93/0.85/0.89 vs 0.93/0.86/0.89
for LV/MYO/LA ($p>0.05$), while being 16 times smaller and 4 times faster
(1.35ms vs 5.40ms per frame) than the default nnU-Net configuration.
Cross-dataset evaluation on an internal dataset (N=311) confirms comparable
generalization.

</details>


### [26] [treeX: Unsupervised Tree Instance Segmentation in Dense Forest Point Clouds](https://arxiv.org/abs/2509.03633)
*Josafat-Mattias Burmeister,Andreas Tockner,Stefan Reder,Markus Engel,Rico Richter,Jan-Peter Mund,Jürgen Döllner*

Main category: cs.CV

> 本文提出了一种改进的无监督树X算法，用更少的资源进行高效的3D点云数据处理和独立树提取，尤其适合地面和无人机激光扫描数据。

<details>
  <summary>Details</summary>

**Motivation:** 目前，虽然有研究引入了深度学习方法进行树木实例分割，但这些方法需要大规模标注数据集和大量的计算资源。而改进的treeX算法能够在减少资源消耗的同时，提高处理3D点云数据和提取独立树的效率。

**Method:** 一种改进的无监督算法treeX的修订版，该算法结合了基于聚类的树干检测与区域生长进行树冠划分。该方法提供了两组参数设置，一组适用于地面基于激光扫描数据（静态地面激光扫描TLS及个人激光扫描PLS），另一组适用于无人机搭载的激光扫描数据（ULS）.

**Result:** 经过对比测试，该改进算法在地面数据上的实例检测F1分数比原算法提高了0.11至0.49，对于无人机激光扫描数据，改进版获得了0.58的F1分数，而原算法无法正确分割实例。

**Conclusion:** 改进后的treeX算法在准确性和时间效率方面超过了原算法，适用于资源受限情况下的应用，或者作为深度学习模型的半自动标签生成工具。此外，提供了一个开源Python实现。

**Abstract:** Close-range laser scanning provides detailed 3D captures of forest stands but
requires efficient software for processing 3D point cloud data and extracting
individual trees. Although recent studies have introduced deep learning methods
for tree instance segmentation, these approaches require large annotated
datasets and substantial computational resources. As a resource-efficient
alternative, we present a revised version of the treeX algorithm, an
unsupervised method that combines clustering-based stem detection with region
growing for crown delineation. While the original treeX algorithm was developed
for personal laser scanning (PLS) data, we provide two parameter presets, one
for ground-based laser scanning (stationary terrestrial - TLS and PLS), and one
for UAV-borne laser scanning (ULS). We evaluated the method on six public
datasets (FOR-instance, ForestSemantic, LAUTx, NIBIO MLS, TreeLearn, Wytham
Woods) and compared it to six open-source methods (original treeX, treeiso,
RayCloudTools, ForAINet, SegmentAnyTree, TreeLearn). Compared to the original
treeX algorithm, our revision reduces runtime and improves accuracy, with
instance detection F$_1$-score gains of +0.11 to +0.49 for ground-based data.
For ULS data, our preset achieves an F$_1$-score of 0.58, whereas the original
algorithm fails to segment any correct instances. For TLS and PLS data, our
algorithm achieves accuracy similar to recent open-source methods, including
deep learning. Given its algorithmic design, we see two main applications for
our method: (1) as a resource-efficient alternative to deep learning approaches
in scenarios where the data characteristics align with the method design
(sufficient stem visibility and point density), and (2) for the semi-automatic
generation of labels for deep learning models. To enable broader adoption, we
provide an open-source Python implementation in the pointtree package.

</details>


### [27] [Reg3D: Reconstructive Geometry Instruction Tuning for 3D Scene Understanding](https://arxiv.org/abs/2509.03635)
*Hongpei Zheng,Lintao Xiang,Qijun Yang,Qian Lin,Hujun Yin*

Main category: cs.CV

> 本文介绍了Reg3D，一种用于3D空间理解的新重建几何指令调优框架，该框架解决了现有方法依赖于文本监督而无法提供几何约束的问题，通过在训练过程中直接引入几何感知监督来改进3D理解能力，并通过多个基准测试证明了其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 大型多模态模型（LMMs）在2D视觉理解方面取得了显著的进步，但将这些能力扩展到3D场景理解仍然是一项重大挑战。当前的方法主要依赖于文本监督，这无法提供学习鲁棒3D空间表示所需的几何约束。我们的动机是解决这一限制，以改进3D空间理解。

**Method:** 我们的方法名为Reg3D，这是一种重建几何指令调优框架，它通过在训练过程中直接引入几何感知监督，解决了现有方法主要依赖于仅使用文本监督的问题，这种方法无法提供学习鲁棒的3D空间表示所需的几何约束。与现有方法仅在输入级别注入3D信息不同，Reg3D采用双监督范式，既利用3D几何信息作为输入，也作为明确的学习目标。具体来说，我们设计了互补的对象级别和帧级别重建任务，在双编码器架构内鼓励几何一致性，以促进空间推理能力的发展。

**Result:** 在ScanQA、Scan2Cap、ScanRefer和SQA3D上的广泛实验表明，与现有方法相比，Reg3D实现了显著的性能提升，确立了一种新的具有空间意识的多模态模型训练范式。

**Conclusion:** 本文提出的Reg3D框架通过引入几何感知监督，证明了它可以有效改进3D场景理解的能力。实验证明了该框架的有效性和优越性，为具有空间意识的多模态模型训练树立了一个新的标准。

**Abstract:** The rapid development of Large Multimodal Models (LMMs) has led to remarkable
progress in 2D visual understanding; however, extending these capabilities to
3D scene understanding remains a significant challenge. Existing approaches
predominantly rely on text-only supervision, which fails to provide the
geometric constraints required for learning robust 3D spatial representations.
In this paper, we introduce Reg3D, a novel Reconstructive Geometry Instruction
Tuning framework that addresses this limitation by incorporating geometry-aware
supervision directly into the training process. Our key insight is that
effective 3D understanding necessitates reconstructing underlying geometric
structures rather than merely describing them. Unlike existing methods that
inject 3D information solely at the input level, Reg3D adopts a
dual-supervision paradigm that leverages 3D geometric information both as input
and as explicit learning targets. Specifically, we design complementary
object-level and frame-level reconstruction tasks within a dual-encoder
architecture, enforcing geometric consistency to encourage the development of
spatial reasoning capabilities. Extensive experiments on ScanQA, Scan2Cap,
ScanRefer, and SQA3D demonstrate that Reg3D delivers substantial performance
improvements, establishing a new training paradigm for spatially aware
multimodal models.

</details>


### [28] [QuantV2X: A Fully Quantized Multi-Agent System for Cooperative Perception](https://arxiv.org/abs/2509.03704)
*Seth Z. Zhao,Huizhi Zhang,Zhaowei Li,Juntong Peng,Anthony Chui,Zewei Zhou,Zonglin Meng,Hao Xiang,Zhiyu Huang,Fujia Wang,Ran Tian,Chenfeng Xu,Bolei Zhou,Jiaqi Ma*

Main category: cs.CV

> 提出了QuantV2X系统，一个用于V2X协同感知的全量化多代理系统，该系统在保持高精度的同时降低了计算负荷和传输带宽，提高了系统性能和扩展性，适合于实际部署。

<details>
  <summary>Details</summary>

**Motivation:** 过去的V2X研究主要关注于提高准确性指标，但忽视了基于系统级别的效率、延迟和实际部署关键考虑。大多数现有系统依赖于高精度模型，导致计算和传输成本高昂，使得其在资源有限的环境中实时运行变得不可行。

**Method:** 介绍了一种名为QuantV2X的全量化多代理系统，该系统设计用于V2X协同感知的高效和可扩展部署。QuantV2X引入了一种统一的端到端量化策略，同时应用于神经网络模型和传输的消息表示，以减少计算负载和传输带宽。

**Result:** 尽管在低比特约束条件下运行，QuantV2X仍能实现与全精度系统相当的精度。在部署导向的指标评估下，QuantV2X相比全精度基线系统大幅度减少了系统级延迟，并且在mAP30指标上提升了9.5。此外，QuantV2X能够更有效地扩展，使更大的、更强大的模型可以在严格的内存预算内运行。

**Conclusion:** 结果表明了全量化多代理中级融合系统在现实世界部署中的可行性。

**Abstract:** Cooperative perception through Vehicle-to-Everything (V2X) communication
offers significant potential for enhancing vehicle perception by mitigating
occlusions and expanding the field of view. However, past research has
predominantly focused on improving accuracy metrics without addressing the
crucial system-level considerations of efficiency, latency, and real-world
deployability. Noticeably, most existing systems rely on full-precision models,
which incur high computational and transmission costs, making them impractical
for real-time operation in resource-constrained environments. In this paper, we
introduce \textbf{QuantV2X}, the first fully quantized multi-agent system
designed specifically for efficient and scalable deployment of multi-modal,
multi-agent V2X cooperative perception. QuantV2X introduces a unified
end-to-end quantization strategy across both neural network models and
transmitted message representations that simultaneously reduces computational
load and transmission bandwidth. Remarkably, despite operating under low-bit
constraints, QuantV2X achieves accuracy comparable to full-precision systems.
More importantly, when evaluated under deployment-oriented metrics, QuantV2X
reduces system-level latency by 3.2$\times$ and achieves a +9.5 improvement in
mAP30 over full-precision baselines. Furthermore, QuantV2X scales more
effectively, enabling larger and more capable models to fit within strict
memory budgets. These results highlight the viability of a fully quantized
multi-agent intermediate fusion system for real-world deployment. The system
will be publicly released to promote research in this field:
https://github.com/ucla-mobility/QuantV2X.

</details>


### [29] [Transfer Learning-Based CNN Models for Plant Species Identification Using Leaf Venation Patterns](https://arxiv.org/abs/2509.03729)
*Bandita Bharadwaj,Ankur Mishra,Saurav Bharadwaj*

Main category: cs.CV

> 本文使用Swedish Leaf Dataset评估了三种深度学习模型在基于叶脉图案分类植物物种的应用，最终发现EfficientNetB0表现最优，凸显了其在植物分类中的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 植物物种分类基于叶脉图案是一个具有高度分类意义的重要形态特征，因此本文研究了三种深度学习模型在此领域的潜力。

**Method:** 本文评估了三种深度学习架构在基于叶脉图案的植物物种自动分类中的有效性：ResNet50、MobileNetV2和EfficientNetB0。

**Result:** 实验结果表明，EfficientNetB0的表现最好，测试准确率为94.67%，精度、召回率和F1分数均超过94.6%。

**Conclusion:** 研究结果突显了深度学习，尤其是EfficientNetB0，在利用叶脉特征开发可扩展且准确的植物分类工具方面的潜力。

**Abstract:** This study evaluates the efficacy of three deep learning architectures:
ResNet50, MobileNetV2, and EfficientNetB0 for automated plant species
classification based on leaf venation patterns, a critical morphological
feature with high taxonomic relevance. Using the Swedish Leaf Dataset
comprising images from 15 distinct species (75 images per species, totalling
1,125 images), the models were demonstrated using standard performance metrics
during training and testing phases. ResNet50 achieved a training accuracy of
94.11% but exhibited overfitting, reflected by a reduced testing accuracy of
88.45% and an F1 score of 87.82%. MobileNetV2 demonstrated better
generalization capabilities, attaining a testing accuracy of 93.34% and an F1
score of 93.23%, indicating its suitability for lightweight, real-time
applications. EfficientNetB0 outperformed both models, achieving a testing
accuracy of 94.67% with precision, recall, and F1 scores exceeding 94.6%,
highlighting its robustness in venation-based classification. The findings
underscore the potential of deep learning, particularly EfficientNetB0, in
developing scalable and accurate tools for automated plant taxonomy using
venation traits.

</details>


### [30] [LayoutGKN: Graph Similarity Learning of Floor Plans](https://arxiv.org/abs/2509.03737)
*Casper van Engelenburg,Jan van Gemert,Seyran Khademi*

Main category: cs.CV

> 本文提出了 LayoutGKN 方法，通过优化图匹配过程，实现了在保持或提高准确性的同时，大幅提升计算速度。

<details>
  <summary>Details</summary>

**Motivation:** 传统图匹配网络在比对图时需要昂贵的跨图节点级别交互，导致推理时间长。该方法旨在提高效率的同时保持或提高相似性计算的能力。

**Method:** LayoutGKN，通过将跨图节点级别的交互推迟到联合嵌入架构的最后，使用可微分图核作为最终学习到的节点级别嵌入的距离函数，来提高效率和速度。

**Result:** LayoutGKN 在计算相似性方面可以达到与图匹配网络相当或更好的结果，同时显著提高了速度。

**Conclusion:** 提出了 LayoutGKN 方法，它在保持或提高相似性计算质量的同时，显著提高了计算速度。

**Abstract:** Floor plans depict building layouts and are often represented as graphs to
capture the underlying spatial relationships. Comparison of these graphs is
critical for applications like search, clustering, and data visualization. The
most successful methods to compare graphs \ie, graph matching networks, rely on
costly intermediate cross-graph node-level interactions, therefore being slow
in inference time. We introduce \textbf{LayoutGKN}, a more efficient approach
that postpones the cross-graph node-level interactions to the end of the joint
embedding architecture. We do so by using a differentiable graph kernel as a
distance function on the final learned node-level embeddings. We show that
LayoutGKN computes similarity comparably or better than graph matching networks
while significantly increasing the speed.
\href{https://github.com/caspervanengelenburg/LayoutGKN}{Code and data} are
open.

</details>


### [31] [Singular Value Few-shot Adaptation of Vision-Language Models](https://arxiv.org/abs/2509.03740)
*Taha Koleilat,Hassan Rivaz,Yiming Xiao*

Main category: cs.CV

> 提出了基于SVD的CLIP-SVD，实现CLIP视觉-语言模型的高效领域适应，节省参数，改善泛化能力，在多个数据集中取得优秀效果。

<details>
  <summary>Details</summary>

**Motivation:** 解决零样本和小样本学习中，适应新细粒度领域的困难问题，尝试改进现有依赖于提示工程和完整模型精细调节的较低适应质量，不稳定性和损害预训练习得知识的现状。

**Method:** CLIP-SVD，一种多模态和参数高效适应技术，利用奇异值分解（SVD）修改CLIP的内部参数空间，仅微调CLIP参数矩阵的奇异值来调整基础向量，以实现领域适应同时保留预训练模型。仅需该模型总参数的0.04％，有效提升了适应性能，并保持其泛化能力。通过自然语言方法分析CLIP-SVD的适应效果和动态以便解释其性能提升原因。

**Result:** 在11个自然数据集和10个生物医学数据集中实现了分类的最先进结果，不仅在准确性上，还在小样本设置下的泛化能力上超越了以前的方法。

**Conclusion:** CLIP-SVD技术通过极少的额外参数调整实现高效且稳定地适应新领域的目标，同时保持了模型原有的丰富知识。此外，此方法还提供了可解释性分析，提升了理解和使用的便捷性。代码公开可以访问。

**Abstract:** Vision-language models (VLMs) like CLIP have shown impressive zero-shot and
few-shot learning capabilities across diverse applications. However, adapting
these models to new fine-grained domains remains difficult due to reliance on
prompt engineering and the high cost of full model fine-tuning. Existing
adaptation approaches rely on augmented components, such as prompt tokens and
adapter modules, which could limit adaptation quality, destabilize the model,
and compromise the rich knowledge learned during pretraining. In this work, we
present \textbf{CLIP-SVD}, a novel \textit{multi-modal} and
\textit{parameter-efficient} adaptation technique that leverages Singular Value
Decomposition (SVD) to modify the internal parameter space of CLIP without
injecting additional modules. Specifically, we fine-tune only the singular
values of the CLIP parameter matrices to rescale the basis vectors for domain
adaptation while retaining the pretrained model. This design enables enhanced
adaptation performance using only \textbf{0.04\%} of the model's total
parameters and better preservation of its generalization ability. CLIP-SVD
achieves state-of-the-art classification results on 11 natural and 10
biomedical datasets, outperforming previous methods in both accuracy and
generalization under few-shot settings. Additionally, we leverage a natural
language-based approach to analyze the effectiveness and dynamics of the CLIP
adaptation to allow interpretability of CLIP-SVD. The code is publicly
available at https://github.com/HealthX-Lab/CLIP-SVD.

</details>


### [32] [STA-Net: A Decoupled Shape and Texture Attention Network for Lightweight Plant Disease Classification](https://arxiv.org/abs/2509.03754)
*Zongsen Qiu*

Main category: cs.CV

> 研究提出了一种包含Shape-Texture Attention Module (STAM)的新模型，解决了在边缘设备上部署高精度病害诊断模型的难题。STA-Net模型在CCMT数据集上表现出优异的性能。

<details>
  <summary>Details</summary>

**Motivation:** 提高基于深度学习的植物病害诊断模型在边缘设备上的部署精度，因该模型能更好地捕捉到病害细微特征，如不规则病斑形状和复杂纹理。

**Method:** 使用培训免费的神经架构搜索方法(DeepMAD)创建有效的边缘设备网络骨架并引入形状-纹理注意力模块(STAM)来解决传统注意力机制的问题，STAM分为两个分支，一个分支使用可变形卷积来识别形状，另一个分支使用Gabor滤波器组来识别纹理。

**Result:** 在CCMT植物病害公开数据集上，其模型STA-Net（拥有401K参数和51.1MFLOPS）达到了89.00%的准确率和88.96%的F1分数，并且断层研究证实了STAM显著提升了基线和标准注意力模型的性能。

**Conclusion:** 通过将领域知识集成到分离注意力来进行边缘部署的精准农业AI展示了很有前景的方向。

**Abstract:** Responding to rising global food security needs, precision agriculture and
deep learning-based plant disease diagnosis have become crucial. Yet, deploying
high-precision models on edge devices is challenging. Most lightweight networks
use attention mechanisms designed for generic object recognition, which poorly
capture subtle pathological features like irregular lesion shapes and complex
textures. To overcome this, we propose a twofold solution: first, using a
training-free neural architecture search method (DeepMAD) to create an
efficient network backbone for edge devices; second, introducing the
Shape-Texture Attention Module (STAM). STAM splits attention into two branches
-- one using deformable convolutions (DCNv4) for shape awareness and the other
using a Gabor filter bank for texture awareness. On the public CCMT plant
disease dataset, our STA-Net model (with 401K parameters and 51.1M FLOPs)
reached 89.00% accuracy and an F1 score of 88.96%. Ablation studies confirm
STAM significantly improves performance over baseline and standard attention
models. Integrating domain knowledge via decoupled attention thus presents a
promising path for edge-deployed precision agriculture AI. The source code is
available at https://github.com/RzMY/STA-Net.

</details>


### [33] [SLENet: A Guidance-Enhanced Network for Underwater Camouflaged Object Detection](https://arxiv.org/abs/2509.03786)
*Xinxin Wang,Han Sun,Ningzhong Liu,Huiyu Zhou,Yinan Yao*

Main category: cs.CV

> 本文介绍了水下伪装对象检测（UCOD）任务，提出了一个基准数据集DeepCamo，并开发了一个名为SLENet的新框架，该框架能生成更精确的预测结果，并优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 水下伪装对象检测（UCOD）是为了在水下环境中识别与周围环境融为一体的对象，这对于海洋生态学至关重要。但由于光的扭曲、水体浑浊及海洋生物的复杂性，这一任务面临巨大挑战。

**Method:** 我们提出了SLENet框架，该框架结合了Gamma-Asymmetric Enhancement (GAE) 模块和定位引导分支（LGB），用来增强多尺度特征表示并生成富含全局语义信息的位置图。位置图引导多尺度监督解码器（MSSD）生成更准确的预测结果。

**Result:** 实验结果表明，SLENet在DeepCamo数据集以及三个基准COD数据集上的表现优于现有方法，证明了其在更广泛的COD任务中的高度通用性。

**Conclusion:** 通过提出SLENet框架，研究解决了水下环境中的对象检测难题，并证明该方法的性能优于现有的COD模型，在广泛的水下伪装对象检测任务中具有高级别通用性。

**Abstract:** Underwater Camouflaged Object Detection (UCOD) aims to identify objects that
blend seamlessly into underwater environments. This task is critically
important to marine ecology. However, it remains largely underexplored and
accurate identification is severely hindered by optical distortions, water
turbidity, and the complex traits of marine organisms. To address these
challenges, we introduce the UCOD task and present DeepCamo, a benchmark
dataset designed for this domain. We also propose Semantic Localization and
Enhancement Network (SLENet), a novel framework for UCOD. We first benchmark
state-of-the-art COD models on DeepCamo to reveal key issues, upon which SLENet
is built. In particular, we incorporate Gamma-Asymmetric Enhancement (GAE)
module and a Localization Guidance Branch (LGB) to enhance multi-scale feature
representation while generating a location map enriched with global semantic
information. This map guides the Multi-Scale Supervised Decoder (MSSD) to
produce more accurate predictions. Experiments on our DeepCamo dataset and
three benchmark COD datasets confirm SLENet's superior performance over SOTA
methods, and underscore its high generality for the broader COD task.

</details>


### [34] [Fitting Image Diffusion Models on Video Datasets](https://arxiv.org/abs/2509.03794)
*Juhun Lee,Simon S. Woo*

Main category: cs.CV

> 本文提出了一种通过利用视频中时间线索改进图像扩散模型训练的方法。该方法加速收敛速度并提高生成的多样性和质量。

<details>
  <summary>Details</summary>

**Motivation:** 现有的图像扩散模型训练仅依赖于独立采样的静态图像，这在捕捉时间变化时存在信息不足的问题，导致训练收敛慢、分布覆盖有限和泛化能力弱。本研究旨在解决这些问题。

**Method:** 提出了一种简单有效的训练策略，该策略利用连续视频帧中的时间归纳偏置来改进扩散模型训练。这种方法不需要对现有架构进行修改，可以无缝集成到现有的扩散模型训练流程中。

**Result:** 通过利用连续视频帧中的时间归纳偏置，本研究提出了一种简单有效的训练策略来改进扩散模型训练。该方法无需对架构进行修改，并且可以无缝集成到标准扩散训练流水线中。实验结果表明，该方法在HandCo数据集上能够加速2倍以上收敛速度，降低FID分数，并提高生成多样性。此外，优化分析表明，该正则化方法通过减少梯度方差有助于更快的收敛。

**Conclusion:** 实验证明，利用连续视频帧改进扩散模型训练的方法加速了2倍以上的收敛速度，降低了FID分数，并提高了生成多样性和捕捉时间变化的能力。优化分析显示该方法通过降低梯度方差贡献了更快的收敛速度。

**Abstract:** Image diffusion models are trained on independently sampled static images.
While this is the bedrock task protocol in generative modeling, capturing the
temporal world through the lens of static snapshots is information-deficient by
design. This limitation leads to slower convergence, limited distributional
coverage, and reduced generalization. In this work, we propose a simple and
effective training strategy that leverages the temporal inductive bias present
in continuous video frames to improve diffusion training. Notably, the proposed
method requires no architectural modification and can be seamlessly integrated
into standard diffusion training pipelines. We evaluate our method on the
HandCo dataset, where hand-object interactions exhibit dense temporal coherence
and subtle variations in finger articulation often result in semantically
distinct motions. Empirically, our method accelerates convergence by over
2$\text{x}$ faster and achieves lower FID on both training and validation
distributions. It also improves generative diversity by encouraging the model
to capture meaningful temporal variations. We further provide an optimization
analysis showing that our regularization reduces the gradient variance, which
contributes to faster convergence.

</details>


### [35] [MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting](https://arxiv.org/abs/2509.03800)
*Yuheng Li,Yenho Chen,Yuxiang Lai,Jike Zhong,Vanessa Wildman,Xiaofeng Yang*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Radiologic diagnostic errors-under-reading errors, inattentional blindness,
and communication failures-remain prevalent in clinical practice. These issues
often stem from missed localized abnormalities, limited global context, and
variability in report language. These challenges are amplified in 3D imaging,
where clinicians must examine hundreds of slices per scan. Addressing them
requires systems with precise localized detection, global volume-level
reasoning, and semantically consistent natural language reporting. However,
existing 3D vision-language models are unable to meet all three needs jointly,
lacking local-global understanding for spatial reasoning and struggling with
the variability and noise of uncurated radiology reports. We present
MedVista3D, a multi-scale semantic-enriched vision-language pretraining
framework for 3D CT analysis. To enable joint disease detection and holistic
interpretation, MedVista3D performs local and global image-text alignment for
fine-grained representation learning within full-volume context. To address
report variability, we apply language model rewrites and introduce a Radiology
Semantic Matching Bank for semantics-aware alignment. MedVista3D achieves
state-of-the-art performance on zero-shot disease classification, report
retrieval, and medical visual question answering, while transferring well to
organ segmentation and prognosis prediction. Code and datasets will be
released.

</details>


### [36] [Causality-guided Prompt Learning for Vision-language Models via Visual Granulation](https://arxiv.org/abs/2509.03803)
*Mengyu Gao,Qiulei Dong*

Main category: cs.CV

> 本文提出了一个通过视觉细化引导的因果提示学习方法CaPL，该方法在细粒度数据集上显著优于现有的提示学习方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大多数基于CLIP的提示学习方法在处理细粒度数据集时表现有限，因此提出了一个通过视觉细化引导的因果提示学习方法。

**Method:** CaPL方法包含两个模块：1. 使用布朗桥扩散模型将视觉特征分解为非个性化属性（被某些类别共享）和个人化属性（特定于单个类别）的属性解缠模块；2. 根据两种因果推理策略，通过集成上述属性来构建视觉颗粒的颗粒学习模块。

**Result:** 在15个数据集上的广泛实验结果显示，我们的CaPL方法显著优于最先进提示学习方法，尤其是在细粒度数据集上。

**Conclusion:** 通过学习到的视觉颗粒，CaPL方法能学习到更具有辨别性的文本提示，从而在细粒度识别任务上取得更好的效果。

**Abstract:** Prompt learning has recently attracted much attention for adapting
pre-trained vision-language models (e.g., CLIP) to downstream recognition
tasks. However, most of the existing CLIP-based prompt learning methods only
show a limited ability for handling fine-grained datasets. To address this
issue, we propose a causality-guided text prompt learning method via visual
granulation for CLIP, called CaPL, where the explored visual granulation
technique could construct sets of visual granules for the text prompt to
capture subtle discrepancies among different fine-grained classes through
casual inference. The CaPL method contains the following two modules: (1) An
attribute disentanglement module is proposed to decompose visual features into
non-individualized attributes (shared by some classes) and individualized
attributes (specific to single classes) using a Brownian Bridge Diffusion
Model; (2) A granule learning module is proposed to construct visual granules
by integrating the aforementioned attributes for recognition under two causal
inference strategies. Thanks to the learned visual granules, more
discriminative text prompt is expected to be learned. Extensive experimental
results on 15 datasets demonstrate that our CaPL method significantly
outperforms the state-of-the-art prompt learning methods, especially on
fine-grained datasets.

</details>


### [37] [EGTM: Event-guided Efficient Turbulence Mitigation](https://arxiv.org/abs/2509.03808)
*Huanan Li,Rui Fan,Juntao Guan,Weidong Hao,Lai Rui,Tong Wu,Yikai Wang,Lin Gu*

Main category: cs.CV

> The paper presents a novel EGTM framework for turbulence mitigation leveraging event cameras, achieving significant improvements in efficiency and restoration quality over current state-of-the-art methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind the research is to improve the computational and storage efficiency of turbulence mitigation (TM) techniques, which currently rely on high-capacity networks to handle coarse-grained turbulence dynamics.

**Method:** The paper introduces the 'event-lucky insight' and proposes the EGTM framework to extract reliable turbulence-free guidance using event cameras, addressing the limitations of existing TM methods that require high-capacity networks for 'lucky fusion'.

**Result:** The proposed approach outperforms existing state-of-the-art methods by 710 times in model size, 214 times in inference latency, and 224 times in model complexity, while achieving state-of-the-art restoration quality (+0.94 PSNR and +0.08 SSIM).

**Conclusion:** The research introduces the first real-world event-driven TM dataset, built through a custom turbulence data acquisition system. The results demonstrate significant efficiency and quality improvements in TM when using event cameras.

**Abstract:** Turbulence mitigation (TM) aims to remove the stochastic distortions and
blurs introduced by atmospheric turbulence into frame cameras. Existing
state-of-the-art deep-learning TM methods extract turbulence cues from multiple
degraded frames to find the so-called "lucky'', not distorted patch, for "lucky
fusion''. However, it requires high-capacity network to learn from
coarse-grained turbulence dynamics between synchronous frames with limited
frame-rate, thus fall short in computational and storage efficiency. Event
cameras, with microsecond-level temporal resolution, have the potential to
fundamentally address this bottleneck with efficient sparse and asynchronous
imaging mechanism. In light of this, we (i) present the fundamental
\textbf{``event-lucky insight''} to reveal the correlation between turbulence
distortions and inverse spatiotemporal distribution of event streams. Then,
build upon this insight, we (ii) propose a novel EGTM framework that extracts
pixel-level reliable turbulence-free guidance from the explicit but noisy
turbulent events for temporal lucky fusion. Moreover, we (iii) build the first
turbulence data acquisition system to contribute the first real-world
event-driven TM dataset. Extensive experimental results demonstrate that our
approach significantly surpass the existing SOTA TM method by 710 times, 214
times and 224 times in model size, inference latency and model complexity
respectively, while achieving the state-of-the-art in restoration quality
(+0.94 PSNR and +0.08 SSIM) on our real-world EGTM dataset. This demonstrating
the great efficiency merit of introducing event modality into TM task. Demo
code and data have been uploaded in supplementary material and will be released
once accepted.

</details>


### [38] [Focus Through Motion: RGB-Event Collaborative Token Sparsification for Efficient Object Detection](https://arxiv.org/abs/2509.03872)
*Nan Yang,Yang Wang,Zhanwen Liu,Yuchao Dai,Yang Liu,Xiangmo Zhao*

Main category: cs.CV

> FocusMamba通过自适应稀疏化处理和交叉模态融合，提升了RGB-Event检测的准确性和效率。

<details>
  <summary>Details</summary>

**Motivation:** 当前的RGB事件检测方法在特征提取和融合过程中对低信息区域进行统一处理，这导致了较高的计算成本和次优的性能表现。为了解决计算冗余问题，研究者提出了针对图像和事件模态的token稀疏化方法，但是这些方法使用固定的阈值或数量选择token，无法保留不同复杂度样本的有用token信息。为此，他们在准确性与效率之间寻求更好的平衡。

**Method:** 研究者提出了FocusMamba方法，该方法通过自适应协作稀疏化多模态特征并在稀疏化的结果基础上融合图像和事件两种模态的互补信息，从而有效地捕获信息。具体地说，他们设计了一种基于事件相机感知的场景内容变化来识别并自适应地丢弃低信息区域的策略（EGMS），并通过交叉模态融合模块（CMFF）来提升检测性能。

**Result:** 实验结果证明，所提出的方法在DSEC-Det和PKU-DAVIS-SOD数据集上，在准确性和效率方面均优于现有方法。代码将在https://github.com/Zizzzzzzz/FocusMamba上提供。

**Conclusion:** 在DSEC-Det和PKU-DAVIS-SOD数据集上的实验表明，提出的方法在准确性与效率方面优于现有方法。

**Abstract:** Existing RGB-Event detection methods process the low-information regions of
both modalities (background in images and non-event regions in event data)
uniformly during feature extraction and fusion, resulting in high computational
costs and suboptimal performance. To mitigate the computational redundancy
during feature extraction, researchers have respectively proposed token
sparsification methods for the image and event modalities. However, these
methods employ a fixed number or threshold for token selection, hindering the
retention of informative tokens for samples with varying complexity. To achieve
a better balance between accuracy and efficiency, we propose FocusMamba, which
performs adaptive collaborative sparsification of multimodal features and
efficiently integrates complementary information. Specifically, an Event-Guided
Multimodal Sparsification (EGMS) strategy is designed to identify and
adaptively discard low-information regions within each modality by leveraging
scene content changes perceived by the event camera. Based on the
sparsification results, a Cross-Modality Focus Fusion (CMFF) module is proposed
to effectively capture and integrate complementary features from both
modalities. Experiments on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate
that the proposed method achieves superior performance in both accuracy and
efficiency compared to existing methods. The code will be available at
https://github.com/Zizzzzzzz/FocusMamba.

</details>


### [39] [SalientFusion: Context-Aware Compositional Zero-Shot Food Recognition](https://arxiv.org/abs/2509.03873)
*Jiajun Song,Xiaoou Liu*

Main category: cs.CV

> 文章提出了一种新的组合零样本食物识别方法——SalientFusion，它解决了食物识别中背景冗余、角色混淆和语义偏差的问题，表现优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法难以识别未见过的食物类别，因此提出了零样本食物识别（ZSFL）以及组合零样本食物识别（CZSFR）任务，旨在识别包含新组合食物类别。

**Method:** 研究提出了SalientFusion方法，包括两个组件：SalientFormer用于移除冗余背景并使用深度特征解决主食和配菜角色混淆问题；DebiasAT通过调整提示和视觉特征的对齐来减少语义偏差。

**Result:** 在作者提出的两个基准数据集CZSFood-90和CZSFood-164以及通用组合零样本学习最受欢迎的数据集上，通过使用SalientFusion，达到了最先进的结果。

**Conclusion:** SalientFusion是一种语境感知的组合零样本食物识别方法，能有效解决食物识别中的背景冗余、角色混淆和语义偏差等挑战。

**Abstract:** Food recognition has gained significant attention, but the rapid emergence of
new dishes requires methods for recognizing unseen food categories, motivating
Zero-Shot Food Learning (ZSFL). We propose the task of Compositional Zero-Shot
Food Recognition (CZSFR), where cuisines and ingredients naturally align with
attributes and objects in Compositional Zero-Shot learning (CZSL). However,
CZSFR faces three challenges: (1) Redundant background information distracts
models from learning meaningful food features, (2) Role confusion between
staple and side dishes leads to misclassification, and (3) Semantic bias in a
single attribute can lead to confusion of understanding. Therefore, we propose
SalientFusion, a context-aware CZSFR method with two components: SalientFormer,
which removes background redundancy and uses depth features to resolve role
confusion; DebiasAT, which reduces the semantic bias by aligning prompts with
visual features. Using our proposed benchmarks, CZSFood-90 and CZSFood-164, we
show that SalientFusion achieves state-of-the-art results on these benchmarks
and the most popular general datasets for the general CZSL. The code is
avaliable at https://github.com/Jiajun-RUC/SalientFusion.

</details>


### [40] [Human Motion Video Generation: A Survey](https://arxiv.org/abs/2509.03883)
*Haiwei Xue,Xiangyang Luo,Zhanghao Hu,Xin Zhang,Xunzhi Xiang,Yuqin Dai,Jianzhuang Liu,Zhensong Zhang,Minglei Li,Jian Yang,Fei Ma,Zhiyong Wu,Changpeng Yang,Zonghong Dai,Fei Richard Yu*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Human motion video generation has garnered significant research interest due
to its broad applications, enabling innovations such as photorealistic singing
heads or dynamic avatars that seamlessly dance to music. However, existing
surveys in this field focus on individual methods, lacking a comprehensive
overview of the entire generative process. This paper addresses this gap by
providing an in-depth survey of human motion video generation, encompassing
over ten sub-tasks, and detailing the five key phases of the generation
process: input, motion planning, motion video generation, refinement, and
output. Notably, this is the first survey that discusses the potential of large
language models in enhancing human motion video generation. Our survey reviews
the latest developments and technological trends in human motion video
generation across three primary modalities: vision, text, and audio. By
covering over two hundred papers, we offer a thorough overview of the field and
highlight milestone works that have driven significant technological
breakthroughs. Our goal for this survey is to unveil the prospects of human
motion video generation and serve as a valuable resource for advancing the
comprehensive applications of digital humans. A complete list of the models
examined in this survey is available in Our Repository
https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.

</details>


### [41] [OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction](https://arxiv.org/abs/2509.03887)
*Bu Jin,Songen Gu,Xiaotao Hu,Yupeng Zheng,Xiaoyang Guo,Qian Zhang,Xiaoxiao Long,Wei Yin*

Main category: cs.CV

> 研究提出了OccTENS，一种能够进行高效、高保真长期占用生成的生成占用世界模型。通过重新定义为TENS任务和引入TensFormer，解决了现有模型的时间衰减及缺乏可控性的问题。

<details>
  <summary>Details</summary>

**Motivation:** 由于基于自回归（AR）的方法在长期生成时效率低下、随时间衰减并且缺乏可控性，因此提出了OccTENS，以在保持计算效率的同时实现可控和高质量的长期占用生成。

**Method:** OccTENS通过将占用世界模型重新定义为时空下一尺度预测（TENS）任务来解决现有问题，该任务将时间序列建模问题分解为空间尺度生成和时间场景预测。使用TensFormer，可以有效地管理和建模占用序列的时间因果关系和空间关系。为了增强姿态可控性，提出了一个整体的姿态聚集策略，该策略为占用和自我运动提供统一的序列建模。

**Result:** 实验结果表明，与现有最先进方法相比，OccTENS不仅具有更高的占用质量，而且推理速度更快。

**Conclusion:** OccTENS通过将问题分解为更易于处理的子任务以及使用TensFormer来管理时间和空间关系，成功解决了长期占用生成中的效率低下、衰减和缺乏可控性的问题，实验结果表明它具有更高的质量以及更快的推理速度。

**Abstract:** In this paper, we propose OccTENS, a generative occupancy world model that
enables controllable, high-fidelity long-term occupancy generation while
maintaining computational efficiency. Different from visual generation, the
occupancy world model must capture the fine-grained 3D geometry and dynamic
evolution of the 3D scenes, posing great challenges for the generative models.
Recent approaches based on autoregression (AR) have demonstrated the potential
to predict vehicle movement and future occupancy scenes simultaneously from
historical observations, but they typically suffer from \textbf{inefficiency},
\textbf{temporal degradation} in long-term generation and \textbf{lack of
controllability}. To holistically address these issues, we reformulate the
occupancy world model as a temporal next-scale prediction (TENS) task, which
decomposes the temporal sequence modeling problem into the modeling of spatial
scale-by-scale generation and temporal scene-by-scene prediction. With a
\textbf{TensFormer}, OccTENS can effectively manage the temporal causality and
spatial relationships of occupancy sequences in a flexible and scalable way. To
enhance the pose controllability, we further propose a holistic pose
aggregation strategy, which features a unified sequence modeling for occupancy
and ego-motion. Experiments show that OccTENS outperforms the state-of-the-art
method with both higher occupancy quality and faster inference time.

</details>


### [42] [Weakly-Supervised Learning of Dense Functional Correspondences](https://arxiv.org/abs/2509.03893)
*Stefan Stojanov,Linan Zhao,Yunzhi Zhang,Daniel L. K. Yamins,Jiajun Wu*

Main category: cs.CV

> 本文提出了一种弱监督学习方法，使用视觉语言模型来伪标签化多视角图像的功能部件，结合密集对比学习来建立密集的功能对应关系。

<details>
  <summary>Details</summary>

**Motivation:** 在不同类别间匹配图像时，通过对象的功能来指导对应关系的建立，因为用于特定功能的对象部件在形状和外观上往往具有相似性。

**Method:** 基于视觉语言模型的弱监督学习方法，通过伪标签多视角图像来识别功能部件，并结合密集对比学习来提炼功能和空间知识，以建立密集的功能对应关系。

**Result:** 实验结果表明，该方法优于基于现成的自监督图像表示和基于视觉语言模型的基线解决方案。

**Conclusion:** 本文提出的方法在合成和真实数据集上均表现出色，证明其能有效建立不同类别物体间的密集对应关系。

**Abstract:** Establishing dense correspondences across image pairs is essential for tasks
such as shape reconstruction and robot manipulation. In the challenging setting
of matching across different categories, the function of an object, i.e., the
effect that an object can cause on other objects, can guide how correspondences
should be established. This is because object parts that enable specific
functions often share similarities in shape and appearance. We derive the
definition of dense functional correspondence based on this observation and
propose a weakly-supervised learning paradigm to tackle the prediction task.
The main insight behind our approach is that we can leverage vision-language
models to pseudo-label multi-view images to obtain functional parts. We then
integrate this with dense contrastive learning from pixel correspondences to
distill both functional and spatial knowledge into a new model that can
establish dense functional correspondence. Further, we curate synthetic and
real evaluation datasets as task benchmarks. Our results demonstrate the
advantages of our approach over baseline solutions consisting of off-the-shelf
self-supervised image representations and grounded vision language models.

</details>


### [43] [Attn-Adapter: Attention Is All You Need for Online Few-shot Learner of Vision-Language Model](https://arxiv.org/abs/2509.03895)
*Phuoc-Nguyen Bui,Khanh-Binh Nguyen,Hyunseung Choo*

Main category: cs.CV

> 提出Attn-Adapter框架，通过双注意力机制解决视觉-语言模型在少样本场景中的问题，提升泛化性能。

<details>
  <summary>Details</summary>

**Motivation:** 解决对比视觉-语言模型在少样本场景中的计算密集型离线微调风险和过拟合问题。

**Method:** 提出了Attn-Adapter框架，该框架通过双注意力机制来增强CLIP的适应性，包含两个组件：Memory Attn-Adapter和Local-Global Attn-Adapter。

**Result:** Attn-Adapter在跨类别和跨数据集泛化方面超越了最先进的方法，同时保持高效推断并在CLIP骨干网络中扩展。

**Conclusion:** 通过提出的Attn-Adapter框架，成功克服了少样本场景中的这些问题，实现了更好的跨类别和跨数据集泛化性能。

**Abstract:** Contrastive vision-language models excel in zero-shot image recognition but
face challenges in few-shot scenarios due to computationally intensive offline
fine-tuning using prompt learning, which risks overfitting. To overcome these
limitations, we propose Attn-Adapter, a novel online few-shot learning
framework that enhances CLIP's adaptability via a dual attention mechanism. Our
design incorporates dataset-specific information through two components: the
Memory Attn-Adapter, which refines category embeddings using support examples,
and the Local-Global Attn-Adapter, which enriches image embeddings by
integrating local and global features. This architecture enables dynamic
adaptation from a few labeled samples without retraining the base model.
Attn-Adapter outperforms state-of-the-art methods in cross-category and
cross-dataset generalization, maintaining efficient inference and scaling
across CLIP backbones.

</details>


### [44] [SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation](https://arxiv.org/abs/2509.03897)
*Xiaofu Chen,Israfel Salazar,Yova Kementchedjhieva*

Main category: cs.CV

> 本论文介绍了一种新的评估方法SPECS，用于长图像描述任务，其优点在于效率高且能有效地评估描述的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 随着生成长且详细的图像描述的需求增加，标准的评估指标变得越来越不可靠。虽然基于大型语言模型的指标与人类判断有很强的相关性，但其成本很高，不利于模型开发过程中的迭代使用。因此，本研究旨在寻找一个与人类判断相关性强且计算成本低的解决方案。

**Method:** 本研究提出了SPECS（具体性增强的CLIPScore），这是一种无参考的相似性度量方法，专门用于长图像描述任务。SPECS通过新目标修改了CLIP模型，强调了对正确细节的奖励和对不正确细节的惩罚。

**Result:** 实验结果表明，SPECS在与基于大型语言模型的开源评估指标的人类判断相关性上达到了相似的性能，但计算效率更高。

**Conclusion:** SPECS是用于图像描述模型开发过程中迭代检查的有效替代方案，因为它既保持了与人类判断的相关性，又提高了计算效率。

**Abstract:** As interest grows in generating long, detailed image captions, standard
evaluation metrics become increasingly unreliable. N-gram-based metrics though
efficient, fail to capture semantic correctness. Representational Similarity
(RS) metrics, designed to address this, initially saw limited use due to high
computational costs, while today, despite advances in hardware, they remain
unpopular due to low correlation to human judgments. Meanwhile, metrics based
on large language models (LLMs) show strong correlation with human judgments,
but remain too expensive for iterative use during model development.
  We introduce SPECS (Specificity-Enhanced CLIPScore), a reference-free RS
metric tailored to long image captioning. SPECS modifies CLIP with a new
objective that emphasizes specificity: rewarding correct details and penalizing
incorrect ones. We show that SPECS matches the performance of open-source
LLM-based metrics in correlation to human judgments, while being far more
efficient. This makes it a practical alternative for iterative checkpoint
evaluation during image captioning model development.Our code can be found at
https://github.com/mbzuai-nlp/SPECS.

</details>


### [45] [A Generative Foundation Model for Chest Radiography](https://arxiv.org/abs/2509.03903)
*Yuanfeng Ji,Dan Lin,Xiyue Wang,Lu Zhang,Wenhui Zhou,Chongjian Ge,Ruihang Chu,Xiaoli Yang,Junhan Zhao,Junsong Chen,Xiangde Luo,Sen Yang,Jin Fang,Ping Luo,Ruijiang Li*

Main category: cs.CV

> 本文提出 ChexGen，这是一种用于生成胸部X光片的生成式视觉-语言基础模型。通过结合文本、掩码和边界框信息，ChexGen 能合成高质量的胸部X光片，并通过少量数据实现多个医疗任务的性能改进。

<details>
  <summary>Details</summary>

**Motivation:** 医疗图像的多样性和高质量标注稀缺问题是开发可靠医疗AI模型的关键障碍。本文解决了这个问题，通过创建一个能够生成高质量胸部X光图像的模型，以克服图像数据缺乏的问题。

**Method:** 开发了名为 ChexGen 的生成式视觉语言基础模型，该模型引入了用于文本引导、掩码引导和边界框引导的胸部X光片合成的统一框架。ChexGen 基于潜在扩散变压器架构，并在迄今最大的整理胸部X光数据集上进行了预训练，该数据集包含960,000张X光片-报告配对。

**Result:** ChexGen 通过专家评估和定量指标实现了精确的图像合成，并通过少量训练数据实现了疾病分类、检测和分割任务的性能提升。此外，它还能够创建多样化的患者队列，通过检测和减轻人口统计学偏差来增强模型的公平性。

**Conclusion:** 研究表明，生成式基础模型在构建更准确、数据高效且公平的医学AI系统方面具有变革作用。

**Abstract:** The scarcity of well-annotated diverse medical images is a major hurdle for
developing reliable AI models in healthcare. Substantial technical advances
have been made in generative foundation models for natural images. Here we
develop `ChexGen', a generative vision-language foundation model that
introduces a unified framework for text-, mask-, and bounding box-guided
synthesis of chest radiographs. Built upon the latent diffusion transformer
architecture, ChexGen was pretrained on the largest curated chest X-ray dataset
to date, consisting of 960,000 radiograph-report pairs. ChexGen achieves
accurate synthesis of radiographs through expert evaluations and quantitative
metrics. We demonstrate the utility of ChexGen for training data augmentation
and supervised pretraining, which led to performance improvements across
disease classification, detection, and segmentation tasks using a small
fraction of training data. Further, our model enables the creation of diverse
patient cohorts that enhance model fairness by detecting and mitigating
demographic biases. Our study supports the transformative role of generative
foundation models in building more accurate, data-efficient, and equitable
medical AI systems.

</details>
