<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 3]
- [cs.CV](#cs.CV) [Total: 8]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation](https://arxiv.org/abs/2506.19952)
*Deepon Halder,Thanmay Jayakumar,Raj Dabre*

Main category: cs.CL

> CycleDistill is an iterative approach that creates synthetic parallel corpora from monolingual data to improve machine translation quality, especially useful for low-resource languages.

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of LLMs in few-shot machine translation and the scarcity of parallel corpora for low-resource languages.

**Method:** CycleDistill, a bootstrapping method that uses LLMs and few-shot translation to create synthetic parallel corpora from monolingual data for fine-tuning.

**Result:** In experiments with three Indian languages, CycleDistill improved upon a few-shot baseline model by over 20-30 chrF points in the first iteration without needing extensive parallel corpora.

**Conclusion:** CycleDistill can generate high-quality MT systems using only monolingual corpora and a few initial examples, making it effective for low-resource languages.

**Abstract:** Large language models (LLMs), despite their ability to perform few-shot
machine translation (MT), often lag behind dedicated MT systems trained on
parallel corpora, which are crucial for high quality machine translation (MT).
However, parallel corpora are often scarce or non-existent for low-resource
languages. In this paper, we propose CycleDistill, a bootstrapping approach
leveraging LLMs and few-shot translation to obtain high-quality MT systems.
CycleDistill involves iteratively generating synthetic parallel corpora from
monolingual corpora via zero- or few-shot MT, which is then used to fine-tune
the model that was used for generating said data for MT. CycleDistill does not
need parallel corpora beyond 1 to 4 few-shot examples, and in our experiments
focusing on three Indian languages, by relying solely on monolingual corpora,
it can achieve high-quality machine translation, improving upon a few-shot
baseline model by over 20-30 chrF points on average in the first iteration. We
also study the effect of leveraging softmax activations during the distillation
process and observe mild improvements in translation quality.

</details>


### [2] [Inference Scaled GraphRAG: Improving Multi Hop Question Answering on Knowledge Graphs](https://arxiv.org/abs/2506.19967)
*Travis Thompson,Seung-Hwan Lim,Paul Liu,Ruoying He,Dongkuan Xu*

Main category: cs.CL

> 我们提出了一种名为Inference-Scaled GraphRAG的新框架，通过在推理时进行计算扩展来增强基于大规模语言模型（LLMs）的图推理能力。

<details>
  <summary>Details</summary>

**Motivation:** 本论文旨在解决当前大规模语言模型（LLMs）在知识密集型推理任务上的不足，特别是LLMs在利用结构化上下文和多跳信息时的局限性。

**Method:** 我们的方法结合了推理时的计算扩展，包括顺序扩展与深入的链式思考图遍历，以及并行扩展与投票机制来处理采样的轨迹。这些工作是在一个交替的推理-执行循环中进行的。

**Result:** 实验结果显示，相较于传统的GraphRAG和先前的图遍历基线方法，我们提出的方法在多跳问题回答任务上显著提升了性能。

**Conclusion:** 本研究的发现表明，推理时的计算扩展是一种有效且与架构无关的解决方案，适用于基于LLMs的结构化知识推理任务。

**Abstract:** Large Language Models (LLMs) have achieved impressive capabilities in
language understanding and generation, yet they continue to underperform on
knowledge-intensive reasoning tasks due to limited access to structured context
and multi-hop information. Retrieval-Augmented Generation (RAG) partially
mitigates this by grounding generation in retrieved context, but conventional
RAG and GraphRAG methods often fail to capture relational structure across
nodes in knowledge graphs. We introduce Inference-Scaled GraphRAG, a novel
framework that enhances LLM-based graph reasoning by applying inference-time
compute scaling. Our method combines sequential scaling with deep
chain-of-thought graph traversal, and parallel scaling with majority voting
over sampled trajectories within an interleaved reasoning-execution loop.
Experiments on the GRBench benchmark demonstrate that our approach
significantly improves multi-hop question answering performance, achieving
substantial gains over both traditional GraphRAG and prior graph traversal
baselines. These findings suggest that inference-time scaling is a practical
and architecture-agnostic solution for structured knowledge reasoning with LLMs

</details>


### [3] [Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation](https://arxiv.org/abs/2506.19998)
*Xinyi Ni,Haonan Jian,Qiuyang Wang,Vedanshi Chetan Shah,Pengyu Hong*

Main category: cs.CL

> 我们提出了Doc2Agent这一方案，它可以解决从非结构化API文档构建代理所遇到的挑战，显著提高了效率并降低了成本。

<details>
  <summary>Details</summary>

**Motivation:** 当前，大多数基于API的代理依赖于经过策划和统一的工具集，这些工具集不能反映现实世界API的复杂性，因此很难构建能够适应任意领域的工具使用代理。

**Method:** 我们提出了Doc2Agent，这是一个可扩展的流水线，用于从API文档生成可用于Python工具的代理，并通过代码代理迭代地优化这些工具。

**Result:** 相对于直接调用API的方法，我们的方法在WebArena基准测试中达到了55％的相对性能提升，且成本减少了90％。此外，一种专门针对糖材料科学领域的代理也展示了该流水线对复杂、知识密集任务的适应性。

**Conclusion:** Doc2Agent提供了一个通用的解决方案，适用于从非结构化API文档构建代理，解决复杂任务的能力证明了它的实用性和广泛适用性。

**Abstract:** REST APIs play important roles in enriching the action space of web agents,
yet most API-based agents rely on curated and uniform toolsets that do not
reflect the complexity of real-world APIs. Building tool-using agents for
arbitrary domains remains a major challenge, as it requires reading
unstructured API documentation, testing APIs and inferring correct parameters.
We propose Doc2Agent, a scalable pipeline to build agents that can call
Python-based tools generated from API documentation. Doc2Agent generates
executable tools from API documentations and iteratively refines them using a
code agent. We evaluate our approach on real-world APIs, WebArena APIs, and
research APIs, producing validated tools. We achieved a 55\% relative
performance improvement with 90\% lower cost compared to direct API calling on
WebArena benchmark. A domain-specific agent built for glycomaterial science
further demonstrates the pipeline's adaptability to complex, knowledge-rich
tasks. Doc2Agent offers a generalizable solution for building tool agents from
unstructured API documentation at scale.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [Computer Vision based Automated Quantification of Agricultural Sprayers Boom Displacement](https://arxiv.org/abs/2506.19939)
*Aryan Singh Dalal,Sidharth Rai,Rahul Singh,Treman Singh Kaloya,Rahul Harsha Cheppally,Ajay Sharda*

Main category: cs.CV

> 本研究开发了一种计算机视觉系统来量化农业喷洒机喷洒臂的移动，以高精度检测和估算目标位置，并可用于改进喷洒臂的设计，提高应用精度。

<details>
  <summary>Details</summary>

**Motivation:** 存在使用自走式农业喷洒机进行农业作业时喷洒臂不稳定的现象，没有定量知识来制定解决喷洒臂移动问题的策略，因此本研究旨在开发一种量化喷洒臂移动的计算机视觉系统。

**Method:** 开发了一套自动化的计算机视觉系统来实时追踪喷洒器边缘的目标，使用了YOLO V7、V8 和 V11 神经网络模型来在田间操作中量化垂直和横向的有效位移，并利用安装在喷洒臂上的倾角传感器来捕捉喷洒臂的角度并验证神经网络模型的输出结果。

**Result:** 结果表明模型可以以超过90%的准确率识别目标，并且对喷洒臂上目标的距离估算值与倾角传感器数据相差0.026米以内。

**Conclusion:** 该系统能够量化目前喷洒器上的喷洒臂移动，并且在进行少量的修改后，可能适用于任何其他喷洒器。该系统收集的数据可以帮助进行设计改进以提高喷洒器的稳定性，并实现更高的应用精度。

**Abstract:** Application rate errors when using self-propelled agricultural sprayers for
agricultural production remain a concern. Among other factors, spray boom
instability is one of the major contributors to application errors. Spray
booms' width of 38m, combined with 30 kph driving speeds, varying terrain, and
machine dynamics when maneuvering complex field boundaries, make controls of
these booms very complex. However, there is no quantitative knowledge on the
extent of boom movement to systematically develop a solution that might include
boom designs and responsive boom control systems. Therefore, this study was
conducted to develop an automated computer vision system to quantify the boom
movement of various agricultural sprayers. A computer vision system was
developed to track a target on the edge of the sprayer boom in real time. YOLO
V7, V8, and V11 neural network models were trained to track the boom's
movements in field operations to quantify effective displacement in the
vertical and transverse directions. An inclinometer sensor was mounted on the
boom to capture boom angles and validate the neural network model output. The
results showed that the model could detect the target with more than 90 percent
accuracy, and distance estimates of the target on the boom were within 0.026 m
of the inclinometer sensor data. This system can quantify the boom movement on
the current sprayer and potentially on any other sprayer with minor
modifications. The data can be used to make design improvements to make sprayer
booms more stable and achieve greater application accuracy.

</details>


### [5] [EBC-ZIP: Improving Blockwise Crowd Counting with Zero-Inflated Poisson Regression](https://arxiv.org/abs/2506.19955)
*Yiming Ma,Victor Sanchez,Tanaya Guha*

Main category: cs.CV

> 本论文提出了一种名为EBC-ZIP的群计数框架，该框架利用零膨胀泊松（ZIP）回归来处理计数数据的空间分布，从而更好地应对密集和稀疏区域的不平衡问题，并实现了更优的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的群计数方法往往忽视了真实世界场景中人群分布的高度不平衡问题，导致模型在稀疏区域表现不佳。同时，多数方法依赖于假设高斯分布的MSE损失函数，这并不适合离散、非负的计数数据。

**Method:** 论文提出的方法EBC-ZIP替代了传统的回归损失函数，采用了ZIP分布的负对数似然函数作为损失函数，以更好地处理包含大量零值的数据。方法构建在增强块分类（EBC）框架之上，不仅能更好地保持目标的离散性并且确保了训练的稳定性。

**Result:** 实验结果显示EBC-ZIP在四种群计数基准数据集上表现优于EBC，并达到了当前最优结果。

**Conclusion:** 通过采用ZIP回归模型，EBC-ZIP成功地处理了不平衡和离散问题，在群计数任务中表现出了优越的性能。

**Abstract:** Density map estimation has become the mainstream paradigm in crowd counting.
However, most existing methods overlook the extreme sparsity of ground-truth
density maps. In real-world crowd scenes, the vast majority of spatial regions
(often over 95%) contain no people, leading to heavily imbalanced count
distributions. Ignoring this imbalance can bias models toward overestimating
dense regions and underperforming in sparse areas. Furthermore, most loss
functions used in density estimation are majorly based on MSE and implicitly
assume Gaussian distributions, which are ill-suited for modeling discrete,
non-negative count data. In this paper, we propose EBC-ZIP, a crowd counting
framework that models the spatial distribution of counts using a Zero-Inflated
Poisson (ZIP) regression formulation. Our approach replaces the traditional
regression loss with the negative log-likelihood of the ZIP distribution,
enabling better handling of zero-heavy distributions while preserving count
accuracy. Built upon the recently proposed Enhanced Block Classification (EBC)
framework, EBC-ZIP inherits EBC's advantages in preserving the discreteness of
targets and ensuring training stability, while further improving performance
through a more principled probabilistic loss. We also evaluate EBC-ZIP with
backbones of varying computational complexity to assess its scalability.
Extensive experiments on four crowd counting benchmarks demonstrate that
EBC-ZIP consistently outperforms EBC and achieves state-of-the-art results.

</details>


### [6] [ToSA: Token Merging with Spatial Awareness](https://arxiv.org/abs/2506.20066)
*Hsiang-Wei Huang,Wenhao Chai,Kuang-Ming Chen,Cheng-Yen Yang,Jenq-Neng Hwang*

Main category: cs.CV

> 本文介绍了新型令牌合并方法ToSA，该方法通过引入深度图像生成伪空间令牌来提高令牌合并的有效性，并在多个基准测试中表现优异，显著减少ViT的运行时间。

<details>
  <summary>Details</summary>

**Motivation:** 尽管现有的令牌合并方法依赖视觉令牌的特征相似性来降低计算成本，但这些方法忽视了整合空间信息在早期层中作为令牌合并可靠标准的潜力。

**Method:** 通过引入深度图像作为输入生成伪空间令牌，ToSA结合了空间意识与语义意识来指导令牌合并过程。

**Result:** 本文提出了一种结合语义和空间意识的新颖令牌合并方法ToSA，该方法使用深度图像作为输入生成伪空间令牌，从而辅助视觉令牌合并过程，保留关键场景结构，提高合并策略的有效性。实验结果显示，ToSA在多个视觉和体感问题解答基准测试中优于先前的令牌合并方法，并显著减少了ViT的运行时间。

**Conclusion:** ToSA在提高合并策略的有效性同时，保留了关键的场景结构，并在多个视觉和体感问题解答基准测试中优于先前的方法，证明了其作为一种ViT加速的高效解决方案的潜力。

**Abstract:** Token merging has emerged as an effective strategy to accelerate Vision
Transformers (ViT) by reducing computational costs. However, existing methods
primarily rely on the visual token's feature similarity for token merging,
overlooking the potential of integrating spatial information, which can serve
as a reliable criterion for token merging in the early layers of ViT, where the
visual tokens only possess weak visual information. In this paper, we propose
ToSA, a novel token merging method that combines both semantic and spatial
awareness to guide the token merging process. ToSA leverages the depth image as
input to generate pseudo spatial tokens, which serve as auxiliary spatial
information for the visual token merging process. With the introduced spatial
awareness, ToSA achieves a more informed merging strategy that better preserves
critical scene structure. Experimental results demonstrate that ToSA
outperforms previous token merging methods across multiple benchmarks on visual
and embodied question answering while largely reducing the runtime of the ViT,
making it an efficient solution for ViT acceleration. The code will be
available at: https://github.com/hsiangwei0903/ToSA

</details>


### [7] [BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos](https://arxiv.org/abs/2506.20103)
*Jiahao Lin,Weixuan Peng,Bojia Zi,Yifeng Gao,Xianbiao Qi,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

> 论文介绍了一个新的名为BrokenVideos的数据集，用于提高对AI生成视频中视觉缺陷定位的检测性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的数据集仅限于视频或帧级别的检测，或者缺乏评估定位方法所需的精细的空间注释。该数据集的引入是为了填补在人工智能生成视频中缺陷定位基准评价的空白。

**Method:** 该论文引入了一个名为BrokenVideos的新基准数据集，包含3,254个人工智能生成的视频，并且每个视频都有细致的人工审核标注的像素级遮罩，用于高精度的评价视觉缺陷定位方法。

**Result:** 实验表明，在BrokenVideos数据集上训练先进的人工智能缺陷检测模型和多模态大型语言模型，能够显著提高它们在定位受损区域方面的性能。

**Conclusion:** BrokenVideos数据集为评估和改进生成视频模型中的缺陷定位研究提供了一个关键的基础。

**Abstract:** Recent advances in deep generative models have led to significant progress in
video generation, yet the fidelity of AI-generated videos remains limited.
Synthesized content often exhibits visual artifacts such as temporally
inconsistent motion, physically implausible trajectories, unnatural object
deformations, and local blurring that undermine realism and user trust.
Accurate detection and spatial localization of these artifacts are crucial for
both automated quality control and for guiding the development of improved
generative models. However, the research community currently lacks a
comprehensive benchmark specifically designed for artifact localization in AI
generated videos. Existing datasets either restrict themselves to video or
frame level detection or lack the fine-grained spatial annotations necessary
for evaluating localization methods. To address this gap, we introduce
BrokenVideos, a benchmark dataset of 3,254 AI-generated videos with
meticulously annotated, pixel-level masks highlighting regions of visual
corruption. Each annotation is validated through detailed human inspection to
ensure high quality ground truth. Our experiments show that training state of
the art artifact detection models and multi modal large language models (MLLMs)
on BrokenVideos significantly improves their ability to localize corrupted
regions. Through extensive evaluation, we demonstrate that BrokenVideos
establishes a critical foundation for benchmarking and advancing research on
artifact localization in generative video models. The dataset is available at:
https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/.

</details>


### [8] [From 2D to 3D Cognition: A Brief Survey of General World Models](https://arxiv.org/abs/2506.20134)
*Ningwei Xie,Zizi Tian,Lei Yang,Xiao-Ping Zhang,Meng Guo,Jie Li*

Main category: cs.CV

> 本文综述了从2D向3D世界模型转变，并分析了3D表示和世界知识在支持更强大的认知能力中的作用。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在填补领域内对新兴技术的系统分析空白，明确这些技术在推进3D认知世界模型发展中的作用。

**Method:** 本文采用了概念框架的方法，对从2D感知到3D认知的世界模型转变进行了有结构和前瞻性的回顾。

**Result:** 通过概念框架，本文辨别并分析了两大技术推动因素以及三大核心认知能力，并探讨了这些能力在实际应用中的情况。

**Conclusion:** 本文指出3D表示法的进步和世界知识的融合是关键的技术推动力，并剖析了支撑3D世界建模的三大核心认知能力：3D物理场景生成、3D空间推理和3D空间交互。另外，还探讨了这些能力在实际应用中的部署，并指出了在数据、建模和部署方面的挑战，以及未来提高更强大和通用的3D世界模型的方向。

**Abstract:** World models have garnered increasing attention in the development of
artificial general intelligence (AGI), serving as computational frameworks for
learning representations of the external world and forecasting future states.
While early efforts focused on 2D visual perception and simulation, recent
3D-aware generative world models have demonstrated the ability to synthesize
geometrically consistent, interactive 3D environments, marking a shift toward
3D spatial cognition. Despite rapid progress, the field lacks systematic
analysis to categorize emerging techniques and clarify their roles in advancing
3D cognitive world models. This survey addresses this need by introducing a
conceptual framework, providing a structured and forward-looking review of
world models transitioning from 2D perception to 3D cognition. Within this
framework, we highlight two key technological drivers, particularly advances in
3D representations and the incorporation of world knowledge, as fundamental
pillars. Building on these, we dissect three core cognitive capabilities that
underpin 3D world modeling: 3D physical scene generation, 3D spatial reasoning,
and 3D spatial interaction. We further examine the deployment of these
capabilities in real-world applications, including embodied AI, autonomous
driving, digital twin, and gaming/VR. Finally, we identify challenges across
data, modeling, and deployment, and outline future directions for advancing
more robust and generalizable 3D world models.

</details>


### [9] [EAR: Erasing Concepts from Unified Autoregressive Models](https://arxiv.org/abs/2506.20151)
*Haipeng Fan,Shiyuan Zhang,Baohunesitu,Zihang Guo,Huaiwen Zhang*

Main category: cs.CV

> 提出EAR方法用于有效且保留性能的概念擦除，引入WGA和TLM策略来对齐解码与擦除目标并保护无关内容。同时开发ECGVF基准用于概念擦除的评估。实验结果证明其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管自回归(AR)模型在视觉理解和图像生成任务中取得了统一和强大的表现，但如何在保持生成质量的同时从AR模型中移除不需要的概念仍然是一个开放的挑战。

**Method:** 我们提出了Erasure Autoregressive Model (EAR)，一种针对概念擦除的有效微调方法。特别地，我们引入了分窗梯度累积(windowed gradient accumulation, WGA)策略来对齐补丁级别解码和擦除目标，以及阈值损失掩码(thresholded loss masking, TLM)策略来保护微调期间与目标概念无关的内容。

**Result:** 在AR模型Janus-Pro上开展的ECGVF基准测试表明，EAR在擦除效果和模型实用性保持方面都取得了显著的改进。

**Conclusion:** 实验结果证明了我们提出的方法Erase Autoregressive Model (EAR)及其相应的评估基准Erase Concept Generator and Visual Filter (ECGVF)的有效性，能够在保持模型原有性能的同时有效擦除不需要的概念。

**Abstract:** Autoregressive (AR) models have achieved unified and strong performance
across both visual understanding and image generation tasks. However, removing
undesired concepts from AR models while maintaining overall generation quality
remains an open challenge. In this paper, we propose Erasure Autoregressive
Model (EAR), a fine-tuning method for effective and utility-preserving concept
erasure in AR models. Specifically, we introduce Windowed Gradient Accumulation
(WGA) strategy to align patch-level decoding with erasure objectives, and
Thresholded Loss Masking (TLM) strategy to protect content unrelated to the
target concept during fine-tuning. Furthermore, we propose a novel benchmark,
Erase Concept Generator and Visual Filter (ECGVF), aim at provide a more
rigorous and comprehensive foundation for evaluating concept erasure in AR
models. Specifically, we first employ structured templates across diverse large
language models (LLMs) to pre-generate a large-scale corpus of
target-replacement concept prompt pairs. Subsequently, we generate images from
these prompts and subject them to rigorous filtering via a visual classifier to
ensure concept fidelity and alignment. Extensive experimental results conducted
on the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR
achieves marked improvements in both erasure effectiveness and model utility
preservation. Code is available at: https://github.com/immc-lab/ear/

</details>


### [10] [Loss-Aware Automatic Selection of Structured Pruning Criteria for Deep Neural Network Acceleration](https://arxiv.org/abs/2506.20152)
*Deepak Ghimire,Kilho Lee,Seong-heum Kim*

Main category: cs.CV

> 本文提出了一种新的结构化修剪方法LAASP，可以自动选择最优修剪标准并在训练过程中进行修剪，减少了修剪导致的准确率下降，并且在CIFAR-10和ImageNet数据集上实验结果表明，该方法能够有效提升模型的准确率并减少计算量。

<details>
  <summary>Details</summary>

**Motivation:** 结构化修剪是一种知名的压缩神经网络的技术，使其适用于在资源有限的边缘设备部署。本研究的目标是通过提出一种新型的自适应修剪技术，提高现有方法在模型压缩和加速方面的效果。

**Method:** 本文提出了一种损失感知的自动选择结构化修剪标准（LAASP）技术，用于加速和简化深度神经网络。与传统的修剪方法（训练-修剪-微调三个阶段）不同，LAASP技术在训练过程中进行修剪，将修剪和微调结合在一个循环中。自动选择修剪标准基于网络在一小部分训练数据上的整体损失，同时在网络修剪一定数量的浮点操作后进行短期重新训练，以减少由于修剪造成的准确性下降。

**Result:** 实验显示，LAASP方法在CIFAR-10和ImageNet数据集上测试的VGGNet和ResNet模型效果显著。特别是对于CIFAR-10数据集上的ResNet56和ResNet110，其top-1准确率相比最先进的方法有明显提升，并减少了52%的网络浮点操作。在ImageNet数据集上，ResNet50模型的浮点操作减少了超过42%，而top-5准确率仅下降了0.33%。

**Conclusion:** 本研究通过自动选择最优的修剪标准并对修剪后模型进行短期重新训练，实现了资源高效的模型压缩和加速，实验结果显示该方法在多个基准数据集上达到甚至超过了现有技术水平的准确率并减少了计算量，证明了此方法的有效性。

**Abstract:** Structured pruning is a well-established technique for compressing neural
networks, making it suitable for deployment in resource-limited edge devices.
This paper presents an efficient Loss-Aware Automatic Selection of Structured
Pruning Criteria (LAASP) for slimming and accelerating deep neural networks.
The majority of pruning methodologies employ a sequential process consisting of
three stages: 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed
pruning technique adopts a pruning-while-training approach that eliminates the
first stage and integrates the second and third stages into a single cycle. The
automatic selection of magnitude or similarity-based filter pruning criteria
from a specified pool of criteria and the specific pruning layer at each
pruning iteration is guided by the network's overall loss on a small subset of
the training data. To mitigate the abrupt accuracy drop due to pruning, the
network is retrained briefly after each reduction of a predefined number of
floating-point operations (FLOPs). The optimal pruning rates for each layer in
the network are automatically determined, eliminating the need for manual
allocation of fixed or variable pruning rates for each layer. Experiments on
the VGGNet and ResNet models on the CIFAR-10 and ImageNet benchmark datasets
demonstrate the effectiveness of the proposed method. In particular, the
ResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the
top-1 accuracy compared to state-of-the-art methods while reducing the network
FLOPs by 52\%. Furthermore, the ResNet50 model on the ImageNet dataset reduces
FLOPs by more than 42\% with a negligible 0.33\% drop in top-5 accuracy. The
source code of this paper is publicly available online -
https://github.com/ghimiredhikura/laasp.

</details>


### [11] [Towards Efficient Exemplar Based Image Editing with Multimodal VLMs](https://arxiv.org/abs/2506.20155)
*Avadhoot Jadhav,Ashutosh Srivastava,Abhinav Java,Silky Singh,Tarun Ram Menta,Surgan Jandial,Balaji Krishnamurthy*

Main category: cs.CV

> 该研究提出了一个基于示例的图像编辑方法，利用文本到图像扩散模型和多模态模型，实现高效且高质量的图像编辑。

<details>
  <summary>Details</summary>

**Motivation:** 文本描述所有类型的图像编辑可能具有挑战性。某些图像编辑的模糊性更适合通过示例对的形式表达。

**Method:** 本研究通过利用预训练的文本到图像扩散模型和多模态视觉语言模型，解决了基于示例的图像编辑问题，该问题旨在将示例对中的编辑转移到内容图像上。

**Result:** 尽管该研究提出的端到端流程是无优化的，但实验证明其在多种类型的编辑上超越了基线方法，且速度提高了约4倍。

**Conclusion:** 本研究表明，结合文本到图像扩散模型和多模态视觉语言模型可以有效地进行基于示例的图像编辑，提高编辑质量的同时显著减少处理时间。

**Abstract:** Text-to-Image Diffusion models have enabled a wide array of image editing
applications. However, capturing all types of edits through text alone can be
challenging and cumbersome. The ambiguous nature of certain image edits is
better expressed through an exemplar pair, i.e., a pair of images depicting an
image before and after an edit respectively. In this work, we tackle
exemplar-based image editing -- the task of transferring an edit from an
exemplar pair to a content image(s), by leveraging pretrained text-to-image
diffusion models and multimodal VLMs. Even though our end-to-end pipeline is
optimization-free, our experiments demonstrate that it still outperforms
baselines on multiple types of edits while being ~4x faster.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior](https://arxiv.org/abs/2506.19999)
*Francesco Ignazio Re,Andreas Opedal,Glib Manaiev,Mario Giulianelli,Ryan Cotterell*

Main category: cs.LG

> 本文提出了一种新的模型，用于更好地理解阅读过程中的人的行为，特别是他们的目光固定和快速移动（扫视）模式，发现惊喜理论不足以解释细粒度的眼动行为。

<details>
  <summary>Details</summary>

**Motivation:** 动机是提供一个更通用的阅读行为概率模型，而不是基于标准方法中假设强烈的聚合眼动测量方式，该模型能够捕捉阅读过程中在时间和空间中发生的更复杂的动力学行为。

**Method:** 提出了一种基于标记时空点过程的阅读行为概率模型，该模型不仅捕捉了注视持续时间，还捕捉了注视在空间和时间上如何进行。使用霍克斯过程对扫视进行建模，霍克斯过程捕捉了每次注视如何激发附近时间点和空间点新的注视发生的概率。注视事件的持续时间建模为注视特异性预测随时间卷积的结果，因此捕捉了溢出效应。

**Result:** 实证研究表明，他们所提出的霍克斯过程模型对人类扫视的拟合优于基线模型。至于注视持续时间，将上下文惊喜作为一个预测因素，却只带来了模型预测准确性的细微改善，这表明惊喜理论难以解释控制粒度的眼动行为。

**Conclusion:** 研究表明，惊喜理论难以完全解释细粒度的眼动行为，新的模型在对阅读过程中人的扫视行为进行描述时表现较好。

**Abstract:** Reading is a process that unfolds across space and time, alternating between
fixations where a reader focuses on a specific point in space, and saccades
where a reader rapidly shifts their focus to a new point. An ansatz of
psycholinguistics is that modeling a reader's fixations and saccades yields
insight into their online sentence processing. However, standard approaches to
such modeling rely on aggregated eye-tracking measurements and models that
impose strong assumptions, ignoring much of the spatio-temporal dynamics that
occur during reading. In this paper, we propose a more general probabilistic
model of reading behavior, based on a marked spatio-temporal point process,
that captures not only how long fixations last, but also where they land in
space and when they take place in time. The saccades are modeled using a Hawkes
process, which captures how each fixation excites the probability of a new
fixation occurring near it in time and space. The duration time of fixation
events is modeled as a function of fixation-specific predictors convolved
across time, thus capturing spillover effects. Empirically, our Hawkes process
model exhibits a better fit to human saccades than baselines. With respect to
fixation durations, we observe that incorporating contextual surprisal as a
predictor results in only a marginal improvement in the model's predictive
accuracy. This finding suggests that surprisal theory struggles to explain
fine-grained eye movements.

</details>
