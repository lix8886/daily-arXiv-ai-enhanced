<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 32]
- [cs.CV](#cs.CV) [Total: 35]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Social Bias in Multilingual Language Models: A Survey](https://arxiv.org/abs/2508.20201)
*Lance Calvin Lim Gamboa,Yue Feng,Mark Lee*

Main category: cs.CL

> 研究回顾了多语言和非英语环境下的偏见评估与缓解方法，指出了一些主要方法设计选择上的不足，提出了未来研究可以强化多语言偏见文献的包容性、跨文化适宜性和与最先进的NLP进展的对齐方向。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于分析多语言模型是否存在与英语文本处理模型同样的社会偏见问题。

**Method:** 此研究通过系统回顾延伸至多语种和非英语环境下的偏见评估与缓解方法的相关研究，从语言多样性、文化意识以及评估指标和缓解技术的选择等方面来审视这些研究。

**Result:** 研究表明存在对某些语言的偏好，多语言缓解实验稀缺等问题，并总结了跨语言跨文化适应偏见基准时面临的常见问题及解决方法。

**Conclusion:** 研究指出了多语种偏见研究领域的一些不足，并为未来研究提供了方向，以增强多语言偏见研究的包容性和跨文化适宜性。

**Abstract:** Pretrained multilingual models exhibit the same social bias as models
processing English texts. This systematic review analyzes emerging research
that extends bias evaluation and mitigation approaches into multilingual and
non-English contexts. We examine these studies with respect to linguistic
diversity, cultural awareness, and their choice of evaluation metrics and
mitigation techniques. Our survey illuminates gaps in the field's dominant
methodological design choices (e.g., preference for certain languages, scarcity
of multilingual mitigation experiments) while cataloging common issues
encountered and solutions implemented in adapting bias benchmarks across
languages and cultures. Drawing from the implications of our findings, we chart
directions for future research that can reinforce the multilingual bias
literature's inclusivity, cross-cultural appropriateness, and alignment with
state-of-the-art NLP advancements.

</details>


### [2] [Prompting Strategies for Language Model-Based Item Generation in K-12 Education: Bridging the Gap Between Small and Large Language Models](https://arxiv.org/abs/2508.20217)
*Mohammad Amini,Babak Ahmadi,Xiaomeng Xiong,Yilin Zhang,Christopher Qiao*

Main category: cs.CL

> 该研究通过微调中型模型和多种结构化提示策略生成多选题，证明了中型模型在自动试题生成方面的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是探索使用语言模型自动生成多选题以进行形态学评估，旨在减少手动试题开发的成本和不一致性。

**Method:** 该研究采用两种方法：首先，比较了一个微调的中等规模模型（Gemma，20亿参数）和一个未调优的大模型（GPT-3.5，1750亿参数）；其次，评估了七种结构化提示策略，包括零样本、少样本、链式思考、基于角色、顺序提示及其组合。

**Result:** 研究表明，结构化提示策略，特别是链式思考和顺序设计的结合，显著提升了Gemma模型的表现。Gemma模型生成的项目比GPT-3.5的零样本响应更符合构造标准且更具教学意义，提示设计对中型模型的表现起到了关键作用。

**Conclusion:** 该研究证明，在数据有限的情况下，结构化提示和有效的微调可以增强中型模型的自动试题生成能力。指出自动指标、专家判断和大模型模拟相结合的价值，以确保与评估目标对齐。该工作流提供了一种实用且可扩展的方式来开发和验证K-12语言评估项目。

**Abstract:** This study explores automatic generation (AIG) using language models to
create multiple choice questions (MCQs) for morphological assessment, aiming to
reduce the cost and inconsistency of manual test development. The study used a
two-fold approach. First, we compared a fine-tuned medium model (Gemma, 2B)
with a larger untuned one (GPT-3.5, 175B). Second, we evaluated seven
structured prompting strategies, including zero-shot, few-shot,
chain-of-thought, role-based, sequential, and combinations. Generated items
were assessed using automated metrics and expert scoring across five
dimensions. We also used GPT-4.1, trained on expert-rated samples, to simulate
human scoring at scale. Results show that structured prompting, especially
strategies combining chain-of-thought and sequential design, significantly
improved Gemma's outputs. Gemma generally produced more construct-aligned and
instructionally appropriate items than GPT-3.5's zero-shot responses, with
prompt design playing a key role in mid-size model performance. This study
demonstrates that structured prompting and efficient fine-tuning can enhance
midsized models for AIG under limited data conditions. We highlight the value
of combining automated metrics, expert judgment, and large-model simulation to
ensure alignment with assessment goals. The proposed workflow offers a
practical and scalable way to develop and validate language assessment items
for K-12.

</details>


### [3] [Integrating SystemC TLM into FMI 3.0 Co-Simulations with an Open-Source Approach](https://arxiv.org/abs/2508.20223)
*Andrei Mihai Albu,Giovanni Pollo,Alessio Burrello,Daniele Jahier Pagliari,Cristian Tesconi,Alessandra Neri,Dario Soldi,Fabio Autieri,Sara Vinco*

Main category: cs.CL

> 本文介绍了一种开源方法，用于将SystemC TLM模型集成到FMI 3.0的协同仿真环境中，从而解决网络物理系统集成挑战并实现跨异构环境的标准化。

<details>
  <summary>Details</summary>

**Motivation:** 由于网络物理系统在汽车应用中的复杂性日益增加，因此迫切需要高效建模和跨域协同仿真技术。尽管系统C事务级建模技术能够支持有效硬软协同设计，但其与其他工程领域模型的有限互操作性导致集成挑战。

**Method:** 本文提出了一种将SystemC事务级建模（TLM）模型集成到基于功能模拟接口（FMI）的协同仿真工作流中的完全开源方法。通过将SystemC TLM组件封装为FMI 3.0协同模拟功能模拟单元（FMUs），该方法能够实现跨异构模拟环境的无缝和标准化集成。

**Result:** 提出了一个轻量级的开源工具链，并解决了如时间同步和数据交换等关键技术挑战，并通过具有代表性的案例研究展示了集成方案的可行性和有效性。

**Conclusion:** 通过将SystemC TLM模型集成到FMI 3.0协同模拟环境中，本文方法能够有效解决网络物理系统在跨域协同仿真中的互操作性问题，实现异构模拟环境的标准化集成。

**Abstract:** The growing complexity of cyber-physical systems, particularly in automotive
applications, has increased the demand for efficient modeling and cross-domain
co-simulation techniques. While SystemC Transaction-Level Modeling (TLM)
enables effective hardware/software co-design, its limited interoperability
with models from other engineering domains poses integration challenges. This
paper presents a fully open-source methodology for integrating SystemC TLM
models into Functional Mock-up Interface (FMI)-based co-simulation workflows.
By encapsulating SystemC TLM components as FMI 3.0 Co Simulation Functional
Mock-up Units (FMUs), the proposed approach facilitates seamless, standardized
integration across heterogeneous simulation environments. We introduce a
lightweight open-source toolchain, address key technical challenges such as
time synchronization and data exchange, and demonstrate the feasibility and
effectiveness of the integration through representative case studies.

</details>


### [4] [Can Compact Language Models Search Like Agents? Distillation-Guided Policy Optimization for Preserving Agentic RAG Capabilities](https://arxiv.org/abs/2508.20324)
*Rikuto Kotoge,Mai Nishimura,Jiaxin Ma*

Main category: cs.CL

> 为解决小型语言模型在强化学习中的挑战，文章提出了DGPO方法，并证实了它在资源受限环境下实现代理搜索行为的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 小型语言模型（例如0.5B参数）在强化学习中表现出推理能力差，导致奖励稀疏和训练不稳定。为解决这些问题，本研究动机在于利用DGPO方法提升小型语言模型的代理行为能力。

**Method:** 我们提出了Distillation-Guided Policy Optimization (DGPO)，通过冷启动初始化和策略优化过程中的持续指导来解决小型语言模型推理能力差的问题。

**Result:** 全面的实验表明，DGPO使小型模型能够实现复杂的代理搜索行为，在某些情况下甚至优于较大的教师模型。

**Conclusion:** DGPO方法使得代理RAG在计算资源受限的环境中成为可能，并且能够实现复杂的搜索行为。

**Abstract:** Reinforcement Learning has emerged as a post-training approach to elicit
agentic RAG behaviors such as search and planning from language models.
However, compact language models (e.g., 0.5B parameters) struggle due to poor
reasoning ability, resulting in sparse rewards and unstable training. To
overcome these difficulties, we propose Distillation-Guided Policy Optimization
(DGPO), which addresses the challenges through cold-start initialization from
teacher demonstrations and continuous teacher guidance during policy
optimization. To systematically evaluate our approach, we introduce Agentic RAG
Capabilities (ARC), a fine-grained metric analyzing reasoning, search
coordination, and response synthesis. Comprehensive experiments demonstrate
that DGPO enables compact models to achieve sophisticated agentic search
behaviors, even outperforming the larger teacher model in some cases. DGPO
makes agentic RAG feasible in computing resource-constrained environments.

</details>


### [5] [GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs](https://arxiv.org/abs/2508.20325)
*Haibo Jin,Ruoxi Chen,Peiyan Zhang,Andy Zhou,Yang Zhang,Haohan Wang*

Main category: cs.CL

> 本文介绍了一种名为GUARD的测试方法，用于检验大型语言模型是否符合政府发布的道德准则。GUARD不仅生成违反准则的问题来直接测试模型的反应，还结合了‘越狱’概念创建情境来识别可能规避内置安全机制的情况，并最终生成一个合规报告。该方法已在多个语言模型上进行了实证验证。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型在各个领域的广泛应用，其产生的有害回应引起了社会和监管机构的广泛关注。政府发布的道德准则对开发者提出了高标准要求，但缺乏将这些准则转化为具体测试问题的指导。

**Method:** GUARD方法通过自动化的准则违反问题生成来实现对语言模型的测试，同时引入了“越狱”诊断概念，以识别可能会绕过内置安全机制的情境。

**Result:** 该方法在七种不同的语言模型上进行了验证，这些模型包括Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4, GPT-4o, 和Claude-3.7，测试了它们在三个政府道德准则下的合规性，并进行了越狱诊断。

**Conclusion:** GUARD方法能够有效促进可靠语言模型应用的开发，通过生成合规报告来评估语言模型的准则遵循程度。

**Abstract:** As Large Language Models become increasingly integral to various domains,
their potential to generate harmful responses has prompted significant societal
and regulatory concerns. In response, governments have issued ethics guidelines
to promote the development of trustworthy AI. However, these guidelines are
typically high-level demands for developers and testers, leaving a gap in
translating them into actionable testing questions to verify LLM compliance.
  To address this challenge, we introduce GUARD (\textbf{G}uideline
\textbf{U}pholding Test through \textbf{A}daptive \textbf{R}ole-play and
Jailbreak \textbf{D}iagnostics), a testing method designed to operationalize
guidelines into specific guideline-violating questions that assess LLM
adherence. To implement this, GUARD uses automated generation of
guideline-violating questions based on government-issued guidelines, thereby
testing whether responses comply with these guidelines. When responses directly
violate guidelines, GUARD reports inconsistencies. Furthermore, for responses
that do not directly violate guidelines, GUARD integrates the concept of
``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that
provoke unethical or guideline-violating responses, effectively identifying
potential scenarios that could bypass built-in safety mechanisms. Our method
finally culminates in a compliance report, delineating the extent of adherence
and highlighting any violations.
  We have empirically validated the effectiveness of GUARD on seven LLMs,
including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4,
GPT-4o, and Claude-3.7, by testing compliance under three government-issued
guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can
transfer jailbreak diagnostics to vision-language models, demonstrating its
usage in promoting reliable LLM-based applications.

</details>


### [6] [Joint Enhancement of Relational Reasoning for Long-Context LLMs](https://arxiv.org/abs/2508.20351)
*Zhirui Chen,Wei Shen,Jiashui Huang,Ling Shao*

Main category: cs.CL

> 提出了JERR框架来解决大语言模型在处理长文本语境任务时的能力问题，并通过实验验证了其高效性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管取得了显著进展，大型语言模型（LLMs）仍然面临内存限制和处理复杂长语境任务的能力不足。此外，LLMs通常缺乏透明度，易于产生幻觉。为此，提出了JERR框架来解决这些挑战。

**Method:** JERR, 一个通过基于图的推理来增强长文语境理解的框架。JERR集成了三个关键组件：摘要提取、图构建和关系推理。首先，通过策略性地分割文本来提取摘要，使模型能够更高效地总结和理解信息。其次，构建有向无环图（DAG）来解决冗余问题，确保逻辑一致性和清晰性。最后，通过引入蒙特卡洛树搜索（MCTS），帮助模型导航复杂的推理路径，确保输出更加准确和可解释。

**Result:** 实验结果表明，在ROUGE和F1指标上，JERR一直优于所有基线，并在LLM-Rater评估中取得最高分。

**Conclusion:** JERR框架提供了解决大型语言模型在处理长文语境和复杂推理任务时的可靠性和透明性的有效方案。实验证明其在各项指标上表现出色。

**Abstract:** Despite significant progress, large language models (LLMs) still struggle
with long contexts due to memory limitations and their inability to tackle
complex and long-context tasks. Additionally, LLMs often suffer from a lack of
transparency and are prone to producing hallucinations. To address these
challenges, we propose \textbf{JERR}, a novel framework designed to enhance
long-context comprehension via graph-based reasoning in LLMs. JERR integrates
three key components: synopsis extraction, graph construction, and relational
reasoning. First, synopsis is extracted by chunking text strategically,
allowing the model to summarize and understand information more efficiently.
Second, we build a directed acyclic graph (DAG) to resolve redundancy, ensuring
logical consistency and clarity. Finally, we incorporate Monte Carlo Tree
Search (MCTS) to help the model navigate complex reasoning paths, ensuring more
accurate and interpretable outputs. This framework provides a novel solution
that enables LLMs to handle extended contexts and complex reasoning tasks with
improved reliability and transparency. Experimental results show that JERR
consistently outperforms all baselines on the ROUGE and F1 metrics, achieving
the highest scores on the LLM-Rater evaluation.

</details>


### [7] [Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems](https://arxiv.org/abs/2508.20373)
*Yuyao Wang,Bowen Liu,Jianheng Tang,Nuo Chen,Yuhan Li,Qifan Zhang,Jia Li*

Main category: cs.CL

> 本研究探索了一种可扩展性的方法，利用NP难图问题来提升大语言模型的长期推理能力，并提出了一种两阶段的后训练框架，有效提高了推理的深度和效率。

<details>
  <summary>Details</summary>

**Motivation:** RLLMs虽然在复杂推理任务上取得了显著进展，但需要依赖高质量数据集的后训练才能实现Long CoT能力，而这些数据集通常是成本高昂的人工标注。本研究旨在通过NP难图问题作为合成训练语料，探索一种可扩展性的训练替代方案。

**Method:** 本研究开发了一种两阶段的后训练框架，包括：(i) 基于抽取采样的NP难图问题实例进行Long CoT监督微调，显著提高了推理深度；(ii) 设计细粒度奖励机制的强化学习，增强了推理效率。

**Result:** 研究中的旗舰模型Graph-R1-7B在数学、编码、STEM和逻辑方面表现出强大的泛化能力，并在NP难图问题的准确性和推理效率方面超过QwQ-32B。

**Conclusion:** 研究成果表明，NP难图问题是提高LLM长链推理能力的有效且可扩展的资源，为LLM的后训练打开了一扇新的大门。

**Abstract:** Reasoning Large Language Models (RLLMs) have recently achieved remarkable
progress on complex reasoning tasks, largely enabled by their long
chain-of-thought (Long CoT) capabilities. However, developing these Long CoT
behaviors relies heavily on post-training with high-quality datasets, which are
typically costly and human-curated (e.g., mathematics and code), leaving
scalable alternatives unexplored. In this work, we introduce NP-hard (NPH)
graph problems as a novel synthetic training corpus, as they inherently require
deep reasoning, extensive exploration, and reflective strategies, which are
core characteristics of Long CoT reasoning. Building on this insight, we
develop a two-stage post-training framework: (i) Long CoT Supervised
Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially
enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a
fine-grained reward design, which sharpens reasoning efficiency. Our flagship
model, Graph-R1-7B, demonstrates strong generalization across mathematics,
coding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both
accuracy and reasoning efficiency. These results position NPH graph problems as
an effective and scalable resource for advancing Long CoT reasoning in LLMs,
opening a new frontier for LLM post-training. Our implementation is available
at https://github.com/Graph-Reasoner/Graph-R1, with models and datasets hosted
in our Hugging Face collection HKUST-DSAIL/Graph-R1.

</details>


### [8] [CAPE: Context-Aware Personality Evaluation Framework for Large Language Models](https://arxiv.org/abs/2508.20385)
*Jivnesh Sandhan,Fei Cheng,Tushar Sandhan,Yugo Murawaki*

Main category: cs.CL

> 提出了首个结合对话历史的大型语言模型人格评估框架 (CAPE), 实验证明对话历史可以增强模型应答一致性但也会引发人格转变，不同模型对问题顺序和对话历史的敏感度不同，同时公开了代码和数据集。

<details>
  <summary>Details</summary>

**Motivation:** 现有评估方法忽略了对话历史对模型回答的影响，提出了全新的CAPE框架来评估带有对话历史影响的LLM人格。

**Method:** 提出了Context-Aware Personality Evaluation (CAPE)框架，使用新的度量标准来量化LLM的回答一致性，并在7个LLM上进行实验。

**Result:** 实验表明，对话历史能增强回答一致性，但也会导致人格转变，GPT模型较稳定，而Gemini-1.5-Flash和Llama-8B则对对话历史和问题顺序极为敏感。

**Conclusion:** 引入CAPE框架使我们能够更准确地理解LLM的行为特性及其与人类行为的一致性，对角色扮演代理（RPAs）应用表明，带有人格变化的应答更加接近人类判断。

**Abstract:** Psychometric tests, traditionally used to assess humans, are now being
applied to Large Language Models (LLMs) to evaluate their behavioral traits.
However, existing studies follow a context-free approach, answering each
question in isolation to avoid contextual influence. We term this the Disney
World test, an artificial setting that ignores real-world applications, where
conversational history shapes responses. To bridge this gap, we propose the
first Context-Aware Personality Evaluation (CAPE) framework for LLMs,
incorporating prior conversational interactions. To thoroughly analyze the
influence of context, we introduce novel metrics to quantify the consistency of
LLM responses, a fundamental trait in human behavior.
  Our exhaustive experiments on 7 LLMs reveal that conversational history
enhances response consistency via in-context learning but also induces
personality shifts, with GPT-3.5-Turbo and GPT-4-Turbo exhibiting extreme
deviations. While GPT models are robust to question ordering, Gemini-1.5-Flash
and Llama-8B display significant sensitivity. Moreover, GPT models response
stem from their intrinsic personality traits as well as prior interactions,
whereas Gemini-1.5-Flash and Llama--8B heavily depend on prior interactions.
Finally, applying our framework to Role Playing Agents (RPAs) shows
context-dependent personality shifts improve response consistency and better
align with human judgments. Our code and datasets are publicly available at:
https://github.com/jivnesh/CAPE

</details>


### [9] [Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction](https://arxiv.org/abs/2508.20395)
*Xu Guo*

Main category: cs.CL

> 本研究展示了预测生成过程中的推理步骤实用性，并通过条件熵分析得出，减少的条件熵与正确答案强相关，而较长的路径不一定导致更好的结果。

<details>
  <summary>Details</summary>

**Motivation:** 现有LLMs在提高准确性方面往往依赖于生成中间推理步骤，但尚未探讨这些推理步骤的实用性如何影响最终答案的正确性。研究旨在解决这一问题，特别是在生成过程中预测哪些推理步骤有用以提高最终答案的正确率。

**Method:** 本研究通过在MATH数据集上使用Qwen2.5-32B和GPT-4o生成推理链，并用Qwen3-8B模型来量化这些链对最终准确性的贡献，来探索推理链的实用性。通过逐步扩展的上下文，使用条件熵（预期负对数似然度）测量模型在每个推理步骤上对答案跨度Y的不确定性。

**Result:** 研究结果表明，条件熵随推理步骤减少与正确的答案强相关，而条件熵保持不变或增加通常导致错误答案。此外，还验证了错误的推理路径往往比正确的路径更长，暗示更长的推理不会必然导致更好的结果。

**Conclusion:** 这些发现为设计高效的推理流水线奠定基础，帮助未来研究更早地检测和避免无成效的推理。

**Abstract:** Recent advancements in large language models (LLMs) often rely on generating
intermediate reasoning steps to enhance accuracy. However, little work has
examined how reasoning utility contributes to the final answer's correctness.
Due to the stochastic nature of autoregressive generation, generating more
context does not guarantee increased confidence in the answer. If we could
predict, during generation, whether a reasoning step will be useful, we could
stop early or prune ineffective steps, avoiding distractions in the final
decision.
  We present an oracle study on MATH dataset, using Qwen2.5-32B and GPT-4o to
generate reasoning chains, and then employing a separate model (Qwen3-8B) to
quantify the utility of these chains for final accuracy. Specifically, we
measure the model's uncertainty on the answer span Y at each reasoning step
using conditional entropy (expected negative log-likelihood over the
vocabulary) with context expanding step by step. Our results show a clear
pattern: conditional entropy that decreases over steps is strongly associated
with correct answers, whereas flat or increasing entropy often results in wrong
answers. We also corroborate that incorrect reasoning paths tend to be longer
than correct ones, suggesting that longer reasoning does not necessarily yield
better outcomes. These findings serve as a foundation to inspire future work on
designing efficient reasoning pipelines that detect and avoid unproductive
reasoning early.

</details>


### [10] [UI-Bench: A Benchmark for Evaluating Design Capabilities of AI Text-to-App Tools](https://arxiv.org/abs/2508.20410)
*Sam Jung,Agustin Garcinuno,Spencer Mateega*

Main category: cs.CL

> 本文介绍了一个新的基准测试UI-Bench，用于评估AI生成网站的视觉质量，包括10个工具，30个提示，300个生成网站和超过4000个专家评审。

<details>
  <summary>Details</summary>

**Motivation:** 尽管AI文本到应用程序工具承诺能在几分钟内生成高质量的应用程序和网站，但目前没有公开的基准测试严格验证这些说法。因此，研究者们创建了一个新的基准测试以填补这一空白。

**Method:** 本文介绍了一个名为UI-Bench的大型基准测试，用于通过专家两两比较评估10种AI文本到应用程序工具在网站设计中的视觉卓越性。

**Result:** UI-Bench使用专家给出的两两比较数据与TrueSkill模型结合，对各种AI工具生成的网站进行了排名，并公布了完整的提示集、开源评估框架和公共排行榜。

**Conclusion:** UI-Bench为推进AI驱动的网页设计提供了一个可重复的标准，并促进了相关领域的进步。

**Abstract:** AI text-to-app tools promise high quality applications and websites in
minutes, yet no public benchmark rigorously verifies those claims. We introduce
UI-Bench, the first large-scale benchmark that evaluates visual excellence
across competing AI text-to-app tools through expert pairwise comparison.
Spanning 10 tools, 30 prompts, 300 generated sites, and \textit{4000+} expert
judgments, UI-Bench ranks systems with a TrueSkill-derived model that yields
calibrated confidence intervals. UI-Bench establishes a reproducible standard
for advancing AI-driven web design. We release (i) the complete prompt set,
(ii) an open-source evaluation framework, and (iii) a public leaderboard. The
generated sites rated by participants will be released soon. View the UI-Bench
leaderboard at https://uibench.ai/leaderboard.

</details>


### [11] [DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual Dentistry Understanding](https://arxiv.org/abs/2508.20416)
*Hengchuan Zhu,Yihuan Xu,Yichen Li,Zijie Meng,Zuozhu Liu*

Main category: cs.CL

> 本研究开发了DentalBench，一个用于评估和改进大型语言模型在牙科领域性能的首个双语基准测试。评估结果表明应用领域适应性可以显著提升模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 虽然大型语言模型（LLMs）在医学领域已显示出强大的性能，但在需要深层次专业知识的特定医学领域，如牙科，其能力尚未得到充分探索。主要原因在于缺乏针对性的评估资源。

**Method:** 本研究提出了DentalBench，这是一个专为牙科领域设计的首个全面双语基准测试。它包括两个主要部分：DentalQA，一个包含36,597个问题的英汉问答测试集，涵盖4项任务和16个牙科子领域；DentalCorpus，一个包含3.37亿个标记的大型高质量语料库，支持监督微调和检索增强生成。

**Result:** 研究评估了14个不同的大型语言模型，涵盖专有、开源和特定于医疗的模型，发现任务类型和语言之间存在显著的性能差距。通过使用Qwen-2.5-3B模型进行的进一步实验表明，领域适应性改善了模型的性能，尤其是在知识密集型和术语聚焦的任务上。

**Conclusion:** 本研究强调了专业领域基准测试对于开发医疗健康应用领域可信且有效的LLMs的重要性，并指出领域适应对于提升模型性能至关重要。

**Abstract:** Recent advances in large language models (LLMs) and medical LLMs (Med-LLMs)
have demonstrated strong performance on general medical benchmarks. However,
their capabilities in specialized medical fields, such as dentistry which
require deeper domain-specific knowledge, remain underexplored due to the lack
of targeted evaluation resources. In this paper, we introduce DentalBench, the
first comprehensive bilingual benchmark designed to evaluate and advance LLMs
in the dental domain. DentalBench consists of two main components: DentalQA, an
English-Chinese question-answering (QA) benchmark with 36,597 questions
spanning 4 tasks and 16 dental subfields; and DentalCorpus, a large-scale,
high-quality corpus with 337.35 million tokens curated for dental domain
adaptation, supporting both supervised fine-tuning (SFT) and
retrieval-augmented generation (RAG). We evaluate 14 LLMs, covering
proprietary, open-source, and medical-specific models, and reveal significant
performance gaps across task types and languages. Further experiments with
Qwen-2.5-3B demonstrate that domain adaptation substantially improves model
performance, particularly on knowledge-intensive and terminology-focused tasks,
and highlight the importance of domain-specific benchmarks for developing
trustworthy and effective LLMs tailored to healthcare applications.

</details>


### [12] [KG-CQR: Leveraging Structured Relation Representations in Knowledge Graphs for Contextual Query Retrieval](https://arxiv.org/abs/2508.20417)
*Chi Minh Bui,Ngoc Mai Thieu,Van Vinh Nguyen,Json J. Jung,Khac-Hoai Nam Bui*

Main category: cs.CL

> A novel retrieval framework called KG-CQR is proposed to improve retrieval-augmented generation systems by enriching queries with structured relation representations.

<details>
  <summary>Details</summary>

**Motivation:** Improving the retrieval phase of retrieval-augmented generation systems by using a novel Contextual Query Retrieval approach with enhanced query contextual representation.

**Method:** KG-CQR framework is proposed, focusing on query enrichment through structured relation representations in a corpus-centric knowledge graph.

**Result:** Experimental results on RAGBench and MultiHop-RAG showed a 4-6% improvement in mAP and 2-3% in Recall@25 compared to strong baseline models.

**Conclusion:** KG-CQR consistently outperforms existing baseline models in terms of retrieval effectiveness and ensures scalability across different sized LLMs without additional training.

**Abstract:** The integration of knowledge graphs (KGs) with large language models (LLMs)
offers significant potential to improve the retrieval phase of
retrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR,
a novel framework for Contextual Query Retrieval (CQR) that enhances the
retrieval phase by enriching the contextual representation of complex input
queries using a corpus-centric KG. Unlike existing methods that primarily
address corpus-level context loss, KG-CQR focuses on query enrichment through
structured relation representations, extracting and completing relevant KG
subgraphs to generate semantically rich query contexts. Comprising subgraph
extraction, completion, and contextual generation modules, KG-CQR operates as a
model-agnostic pipeline, ensuring scalability across LLMs of varying sizes
without additional training. Experimental results on RAGBench and MultiHop-RAG
datasets demonstrate KG-CQR's superior performance, achieving a 4-6%
improvement in mAP and a 2-3% improvement in Recall@25 over strong baseline
models. Furthermore, evaluations on challenging RAG tasks such as multi-hop
question answering show that, by incorporating KG-CQR, the performance
consistently outperforms the existing baseline in terms of retrieval
effectiveness

</details>


### [13] [CAMB: A comprehensive industrial LLM benchmark on civil aviation maintenance](https://arxiv.org/abs/2508.20420)
*Feng Zhang,Chengjie Pang,Yuehan Zhang,Chenyu Luo*

Main category: cs.CL

> 本文提出了一个专门针对民用航空维护领域的工业级基准测试工具，此工具可用于评估大语言模型在该领域的性能，并指出其在知识和复杂推理方面的不足之处，从而促进相关研究和开发。

<details>
  <summary>Details</summary>

**Motivation:** 虽然大语言模型在解决数学和编程推理任务方面取得了一定的成功，但在专注于民用航空维护的评估工具方面，仍然存在显著的空白。鉴于此，本研究旨在填补这一空白，并促进相关研究和开发。

**Method:** 为了解决大语言模型在民用航空维护领域的评估工具缺乏问题，本文提出了一个专门针对这一领域的工业级基准。该基准旨在衡量语言模型在民用航空维护中的能力，识别知识缺口和复杂推理能力的不足。通过指出这些缺点，为有针对性的改进工作（如领域特定的微调、RAG 优化或专业提示工程）奠定基础，从而促进更智能的解决方案的开发。同时，鉴于检索增强生成(RAG)系统在实际应用中占据主导地位，本文利用该基准评估现有流行的向量嵌入模型和语言模型在民用航空维护场景中的表现。

**Result:** 实验探索和分析表明，我们提出的基准能够有效地评估模型在该领域的性能。

**Conclusion:** 该基准测试工具为评估语言模型在民用航空维护领域的性能提供了一个标准化的方法，同时开源了此评估基准和代码，以促进进一步的研究和开发。

**Abstract:** Civil aviation maintenance is a domain characterized by stringent industry
standards. Within this field, maintenance procedures and troubleshooting
represent critical, knowledge-intensive tasks that require sophisticated
reasoning. To address the lack of specialized evaluation tools for large
language models (LLMs) in this vertical, we propose and develop an
industrial-grade benchmark specifically designed for civil aviation
maintenance. This benchmark serves a dual purpose: It provides a standardized
tool to measure LLM capabilities within civil aviation maintenance, identifying
specific gaps in domain knowledge and complex reasoning. By pinpointing these
deficiencies, the benchmark establishes a foundation for targeted improvement
efforts (e.g., domain-specific fine-tuning, RAG optimization, or specialized
prompt engineering), ultimately facilitating progress toward more intelligent
solutions within civil aviation maintenance. Our work addresses a significant
gap in the current LLM evaluation, which primarily focuses on mathematical and
coding reasoning tasks. In addition, given that Retrieval-Augmented Generation
(RAG) systems are currently the dominant solutions in practical applications ,
we leverage this benchmark to evaluate existing well-known vector embedding
models and LLMs for civil aviation maintenance scenarios. Through experimental
exploration and analysis, we demonstrate the effectiveness of our benchmark in
assessing model performance within this domain, and we open-source this
evaluation benchmark and code to foster further research and
development:https://github.com/CamBenchmark/cambenchmark

</details>


### [14] [Searching the Title of Practical Work of the Informatics Engineering Bachelor Program with the Case Base Reasoning Method](https://arxiv.org/abs/2508.20442)
*Agung Sukrisna Jaya,Osvari Arsalan,Danny Matthew Saputra*

Main category: cs.CL

> 使用CBR、TF-IDF和余弦相似度技术有效搜索实际工作标题。测试表现出良好的检索性能。

<details>
  <summary>Details</summary>

**Motivation:** 为了利用基于先前发生的案例的经验来进行案例解决技术，特别是在搜索实际工作标题方面。

**Method:** 使用TF-IDF处理每个实际工作的标题词的向量化，并使用余弦相似度计算相似度值。系统可以以标题或关键词的形式进行搜索。

**Result:** 在两个阶段的测试中，使用705个实际工作标题进行了测试，结果显示在第二阶段找到的标题数量相同且平均匹配分数最高。

**Conclusion:** 该系统能够有效地通过标题或关键词搜索实际工作标题，并提供标题及其匹配值。

**Abstract:** Case Base Reasoning (CBR) is a case solving technique based on experience in
cases that have occurred before with the highest similarity. CBR is used to
search for practical work titles. TF-IDF is applied to process the
vectorization of each practical work title word and Cosine Similarity for the
calculation of similarity values. This system can search either in the form of
titles or keywords. The output of the system is the title of practical work and
the match value of each title. Based on the test results using 705 practical
work titles, testing was carried out with five titles and carried out in two
stages. The first stage searches with existing titles and the second stage
randomizes the title from the first stage. And the results obtained in the
second stage are the same number of titles found and the highest average match
score.

</details>


### [15] [MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers](https://arxiv.org/abs/2508.20453)
*Zhenting Wang,Qi Chang,Hemani Patel,Shashank Biju,Cheng-En Wu,Quan Liu,Aolin Ding,Alireza Rezazadeh,Ankit Shah,Yujia Bao,Eugene Siow*

Main category: cs.CL

> 研究提出了MCP-Bench，一种测试大型语言模型在复杂多步骤任务上表现的新基准，展示了现有模型在该任务上的不足。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于，以前基于API的基准测试无法充分评估模型检索工具、规划执行路径、在中间输出基础上生成响应以及跨域工作流程的能力。

**Method:** 该研究引入了MCP-Bench，这是一个用于评估大型语言模型（LLMs）在多步骤任务上表现的基准，这些任务需要工具使用、跨工具协调、精确参数控制和规划/推理能力来解决问题。

**Result:** 实验结果揭示了在MCP-Bench上对20种先进LLMs进行测试所面临的持久挑战。

**Conclusion:** 通过MCP-Bench框架，研究发现现有LLMs在处理复杂多步任务和跨领域的工作流程上存在显著的挑战，而这些能力是解决问题和执行复杂任务所必需的。

**Abstract:** We introduce MCP-Bench, a benchmark for evaluating large language models
(LLMs) on realistic, multi-step tasks that demand tool use, cross-tool
coordination, precise parameter control, and planning/reasoning for solving
tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28
representative live MCP servers spanning 250 tools across domains such as
finance, traveling, scientific computing, and academic search. Unlike prior
API-based benchmarks, each MCP server provides a set of complementary tools
designed to work together, enabling the construction of authentic, multi-step
tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability
to retrieve relevant tools from fuzzy instructions without explicit tool names,
plan multi-hop execution trajectories for complex objectives, ground responses
in intermediate tool outputs, and orchestrate cross-domain workflows -
capabilities not adequately evaluated by existing benchmarks that rely on
explicit tool specifications, shallow few-step workflows, and isolated domain
operations. We propose a multi-faceted evaluation framework covering tool-level
schema understanding and usage, trajectory-level planning, and task completion.
Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code
and data: https://github.com/Accenture/mcp-bench.

</details>


### [16] [Prediction of mortality and resource utilization in critical care: a deep learning approach using multimodal electronic health records with natural language processing techniques](https://arxiv.org/abs/2508.20460)
*Yucheng Ruan,Xiang Lan,Daniel J. Tan,Hairil Rizal Abdullah,Mengling Feng*

Main category: cs.CL

> TLDR A deep learning framework using NLP techniques was developed to integrate multimodal EHRs for predicting mortality and resource utilization. The model outperformed existing methods and showed robustness to data corruption.

<details>
  <summary>Details</summary>

**Motivation:** Motivation The study aimed to develop and evaluate a deep learning framework using NLP to integrate multimodal EHRs, addressing the shortcomings of existing methods that often ignore free-text notes and the potential of textual information within structured data, to predict mortality and resource utilization in ICU.

**Method:** Methods Utilizing two real-world EHR datasets, the study developed and evaluated a model on three clinical tasks, including an ablation study on the key components: medical prompts, free-texts, and pre-trained sentence encoder. The model's robustness against structured EHR data corruption was also assessed.

**Result:** Results The model improved performance metrics by 1.6%/0.8% on BACC/AUROC for mortality prediction, 0.5%/2.2% on RMSE/MAE for LOS prediction, and 10.9%/11.0% on RMSE/MAE for surgical duration estimation, outperforming existing methods consistently across tasks and at various corruption levels.

**Conclusion:** Conclusions The proposed framework is an effective and accurate approach for predicting mortality and resource utilization in critical care, with strong resilience to data corruption within structured data and the success of prompt learning with a transformer encoder for analyzing multimodal EHRs.

**Abstract:** Background Predicting mortality and resource utilization from electronic
health records (EHRs) is challenging yet crucial for optimizing patient
outcomes and managing costs in intensive care unit (ICU). Existing approaches
predominantly focus on structured EHRs, often ignoring the valuable clinical
insights in free-text notes. Additionally, the potential of textual information
within structured data is not fully leveraged. This study aimed to introduce
and assess a deep learning framework using natural language processing
techniques that integrates multimodal EHRs to predict mortality and resource
utilization in critical care settings. Methods Utilizing two real-world EHR
datasets, we developed and evaluated our model on three clinical tasks with
leading existing methods. We also performed an ablation study on three key
components in our framework: medical prompts, free-texts, and pre-trained
sentence encoder. Furthermore, we assessed the model's robustness against the
corruption in structured EHRs. Results Our experiments on two real-world
datasets across three clinical tasks showed that our proposed model improved
performance metrics by 1.6\%/0.8\% on BACC/AUROC for mortality prediction,
0.5%/2.2% on RMSE/MAE for LOS prediction, 10.9%/11.0% on RMSE/MAE for surgical
duration estimation compared to the best existing methods. It consistently
demonstrated superior performance compared to other baselines across three
tasks at different corruption rates. Conclusions The proposed framework is an
effective and accurate deep learning approach for predicting mortality and
resource utilization in critical care. The study also highlights the success of
using prompt learning with a transformer encoder in analyzing multimodal EHRs.
Importantly, the model showed strong resilience to data corruption within
structured data, especially at high corruption levels.

</details>


### [17] [ConspirED: A Dataset for Cognitive Traits of Conspiracy Theories and Large Language Model Safety](https://arxiv.org/abs/2508.20468)
*Luke Bates,Max Glockner,Preslav Nakov,Iryna Gurevych*

Main category: cs.CL

> 本文提出ConspirED数据集，捕捉网络阴谋论文章中的认知特征，发展计算模型识别共谋特征，评估大型语言或推理模型在共谋信息下的鲁棒性，发现这些模型容易被共谋内容误导。

<details>
  <summary>Details</summary>

**Motivation:** 阴谋论破坏公众对科学和机构的信任，并通过进化和吸收反证来抵抗揭穿。随着AI生成的错误信息变得越来越复杂，理解阴谋论内容的修辞模式对于开发针对性干预措施和评估AI的脆弱性非常重要。

**Method:** 通过引入ConspirED（CONSPIR评估数据集），该数据集捕捉多句段落（80-120字）中网络阴谋论文章的共谋思维的认知特征，使用CONSPIR认知框架进行注释，以寻找识别共谋特征的方法和确定文本摘录中主导特征的计算模型。另外，评估大型语言或推理模型（LLM/LRM）在共谋输入下的鲁棒性。

**Result:** 研究发现两者对共谋内容的理解出现了偏差，生成的输出反映了输入的推理模式，即使成功规避了可比的经过事实核查的错误信息。

**Conclusion:** 该数据集和计算模型的发展对于识别阴谋论特征具有重要意义，可以用来开发针对阴谋论的干预措施，同时评估人工智能在面对共谋信息时的脆弱性。

**Abstract:** Conspiracy theories erode public trust in science and institutions while
resisting debunking by evolving and absorbing counter-evidence. As AI-generated
misinformation becomes increasingly sophisticated, understanding rhetorical
patterns in conspiratorial content is important for developing interventions
such as targeted prebunking and assessing AI vulnerabilities. We introduce
ConspirED (CONSPIR Evaluation Dataset), which captures the cognitive traits of
conspiratorial ideation in multi-sentence excerpts (80--120 words) from online
conspiracy articles, annotated using the CONSPIR cognitive framework
(Lewandowsky and Cook, 2020). ConspirED is the first dataset of conspiratorial
content annotated for general cognitive traits. Using ConspirED, we (i) develop
computational models that identify conspiratorial traits and determine dominant
traits in text excerpts, and (ii) evaluate large language/reasoning model
(LLM/LRM) robustness to conspiratorial inputs. We find that both are misaligned
by conspiratorial content, producing output that mirrors input reasoning
patterns, even when successfully deflecting comparable fact-checked
misinformation.

</details>


### [18] [Languages Still Left Behind: Toward a Better Multilingual Machine Translation Benchmark](https://arxiv.org/abs/2508.20511)
*Chihiro Taguchi,Seng Mai,Keita Kurabe,Yusuke Sakai,Georgina Agyei,Soudabeh Eslami,David Chiang*

Main category: cs.CL

> 本研究揭示了FLORES+基准测试的一些关键缺陷，建议未来多语言MT基准应使用领域通用且文化中立的源文本，少依赖命名实体，以更好地反映现实生活中的翻译挑战。

<details>
  <summary>Details</summary>

**Motivation:** FLORES+多语言翻译基准测试广泛用于评估现代MT系统的性能，但本研究却发现其在真正多语言评估中的适用性存在严重缺陷。

**Method:** 本研究通过人类评估和实验来揭示FLORES+基准测试的问题，其中包含对四个语言（阿散蒂梯威语、日语、京语和南阿塞拜疆语）的翻译数据的调查。研究指出了源文本领域特定性和文化偏向性等关键问题，并展示了简单启发式方法能获得不低的BLEU分数，说明评估协议存在漏洞。

**Result:** 研究发现许多翻译都未达到声称的90%的质量标准，MT模型在高质量的自然数据上训练的性能在FLORES+上表现不佳，而在领域相关的评估集上则有显著提升。

**Conclusion:** 研究证明了改进多语言MT基准测试的必要性，提出应使用领域通用、文化中立的源文本，减少对命名实体的依赖，以更好地反映现实世界中的翻译挑战。

**Abstract:** Multilingual machine translation (MT) benchmarks play a central role in
evaluating the capabilities of modern MT systems. Among them, the FLORES+
benchmark is widely used, offering English-to-many translation data for over
200 languages, curated with strict quality control protocols. However, we study
data in four languages (Asante Twi, Japanese, Jinghpaw, and South Azerbaijani)
and uncover critical shortcomings in the benchmark's suitability for truly
multilingual evaluation. Human assessments reveal that many translations fall
below the claimed 90% quality standard, and the annotators report that source
sentences are often too domain-specific and culturally biased toward the
English-speaking world. We further demonstrate that simple heuristics, such as
copying named entities, can yield non-trivial BLEU scores, suggesting
vulnerabilities in the evaluation protocol. Notably, we show that MT models
trained on high-quality, naturalistic data perform poorly on FLORES+ while
achieving significant gains on our domain-relevant evaluation set. Based on
these findings, we advocate for multilingual MT benchmarks that use
domain-general and culturally neutral source texts rely less on named entities,
in order to better reflect real-world translation challenges.

</details>


### [19] [SciTopic: Enhancing Topic Discovery in Scientific Literature through Advanced LLM](https://arxiv.org/abs/2508.20514)
*Pengjiang Li,Zaitian Wang,Xinhao Zhang,Ran Zhang,Lu Jiang,Pengfei Wang,Yuanchun Zhou*

Main category: cs.CL

> SciTopic, a new method for scientific topic discovery, leverages large language models (LLMs) to better understand and capture complex relationships within scientific publications, demonstrating enhanced performance compared to existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind the research is to improve topic discovery in scientific literature by addressing the limitations of existing methods which rely on word embedding and struggle with complex text relationships. It explores the exceptional comprehension abilities of LLMs to enhance topic identification.

**Method:** The proposed method, SciTopic, uses a textual encoder to capture content from scientific publications, integrates entropy-based sampling and triplet tasks guided by large language models (LLMs) for space optimization, and fine-tunes the encoder using contrastive loss to better distinguish between topics.

**Result:** Experiments on three real-world datasets of scientific publications show that SciTopic outperforms current state-of-the-art methods, indicating a significant improvement in topic discovery and providing researchers with deeper and faster insights.

**Conclusion:** The conclusion drawn from the research is that the proposed SciTopic method, enhanced by LLMs, provides an effective way to discover topics in scientific literature, outperforming state-of-the-art methods and offering significant value for researchers in identifying trends and exploring new research areas.

**Abstract:** Topic discovery in scientific literature provides valuable insights for
researchers to identify emerging trends and explore new avenues for
investigation, facilitating easier scientific information retrieval. Many
machine learning methods, particularly deep embedding techniques, have been
applied to discover research topics. However, most existing topic discovery
methods rely on word embedding to capture the semantics and lack a
comprehensive understanding of scientific publications, struggling with
complex, high-dimensional text relationships. Inspired by the exceptional
comprehension of textual information by large language models (LLMs), we
propose an advanced topic discovery method enhanced by LLMs to improve
scientific topic identification, namely SciTopic. Specifically, we first build
a textual encoder to capture the content from scientific publications,
including metadata, title, and abstract. Next, we construct a space
optimization module that integrates entropy-based sampling and triplet tasks
guided by LLMs, enhancing the focus on thematic relevance and contextual
intricacies between ambiguous instances. Then, we propose to fine-tune the
textual encoder based on the guidance from the LLMs by optimizing the
contrastive loss of the triplets, forcing the text encoder to better
discriminate instances of different topics. Finally, extensive experiments
conducted on three real-world datasets of scientific publications demonstrate
that SciTopic outperforms the state-of-the-art (SOTA) scientific topic
discovery methods, enabling researchers to gain deeper and faster insights.

</details>


### [20] [Overview of BioASQ 2024: The twelfth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20532)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Salvador Lima-López,Eulàlia Farré-Maduell,Martin Krallinger,Natalia Loukachevitch,Vera Davydova,Elena Tutubalina,Georgios Paliouras*

Main category: cs.CL

> This abstract summarizes the twelfth edition of the BioASQ challenge within CLEF 2024, highlighting the inclusion of new tasks, the participation of 37 teams with over 700 submissions, and the advancements in the field.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to promote advancements in large-scale biomedical semantic indexing and question answering by challenging participants in various tasks that cover different facets of biomedical data processing.

**Method:** This year's BioASQ challenge included two established tasks, b and Synergy, and introduced two new tasks: MultiCardioNER for clinical entity detection in the cardiology domain across multiple languages, and BIONNE for nested entity recognition in Russian and English.

**Result:** In this edition, 37 teams participated with over 700 distinct submissions, demonstrating competitive performance across the four tasks.

**Conclusion:** The results indicate a continuous advancement in the state-of-the-art of biomedical semantic indexing and question answering tasks, as most of the participating systems achieved competitive performance.

**Abstract:** This is an overview of the twelfth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2024. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks b and Synergy, and two
new tasks: a) MultiCardioNER on the adaptation of clinical entity detection to
the cardiology domain in a multilingual setting, and b) BIONNE on nested NER in
Russian and English. In this edition of BioASQ, 37 competing teams participated
with more than 700 distinct submissions in total for the four different shared
tasks of the challenge. Similarly to previous editions, most of the
participating systems achieved competitive performance, suggesting the
continuous advancement of the state-of-the-art in the field.

</details>


### [21] [Overview of BioASQ 2025: The Thirteenth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20554)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Martin Krallinger,Miguel Rodríguez-Ortega,Eduard Rodriguez-López,Natalia Loukachevitch,Andrey Sakhovskiy,Elena Tutubalina,Dimitris Dimitriadis,Grigorios Tsoumakas,George Giannakoulas,Alexandra Bekiaridou,Athanasios Samaras,Giorgio Maria Di Nunzio,Nicola Ferro,Stefano Marchesin,Marco Martinelli,Gianmaria Silvello,Georgios Paliouras*

Main category: cs.CL

> BioASQ挑战赛第十三届成功举办，包括六项任务，83支队伍参赛，显示出生物医学信息处理技术的不断进步。

<details>
  <summary>Details</summary>

**Motivation:** BioASQ挑战赛旨在推动生物医学领域的信息处理技术发展，特别是大规模语义索引和问答系统的进步。

**Method:** 该论文主要描述了BioASQ挑战赛第十三届的比赛情况，包括两项常规任务b和Synergy，以及四项新任务：多语言临床总结、俄英双语嵌套命名实体链接、心脏病学临床编码及肠脑互动信息提取。

**Result:** 本次比赛中，共有83支队伍参赛，提交了1000多个不同的参赛作品，展示出了领域内的技术不断进步。

**Conclusion:** BioASQ挑战赛作为促进大规模生物医学语义索引和问答系统发展的国际比赛，其成果展示了生物医学信息处理技术的不断进步。

**Abstract:** This is an overview of the thirteenth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2025. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks, b and Synergy, and four
new tasks: a) Task MultiClinSum on multilingual clinical summarization. b) Task
BioNNE-L on nested named entity linking in Russian and English. c) Task
ELCardioCC on clinical coding in cardiology. d) Task GutBrainIE on gut-brain
interplay information extraction. In this edition of BioASQ, 83 competing teams
participated with more than 1000 distinct submissions in total for the six
different shared tasks of the challenge. Similar to previous editions, several
participating systems achieved competitive performance, indicating the
continuous advancement of the state-of-the-art in the field.

</details>


### [22] [Adaptive Federated Distillation for Multi-Domain Non-IID Textual Data](https://arxiv.org/abs/2508.20557)
*Jiahao Xiao,Jiangming Liu*

Main category: cs.CL

> 本文提出了一种统一的基准测试框架和自适应联邦蒸馏方法，以解决联邦学习中多领域非独立同分布数据的挑战，并展示了优于现有工作的效果。

<details>
  <summary>Details</summary>

**Motivation:** 现有的隐私保护联邦蒸馏方法主要关注标签的多样性，忽略了输入的语言领域多样性，这对自然语言处理至关重要。因此，我们引入了一组全面的多领域非独立同分布应用场景，并提出了一种包括多样化数据的统一基准测试框架。

**Method:** 我们提出了自适应联邦蒸馏（AdaFD）框架，旨在解决多领域非独立同分布（non-IID）数据的挑战，适用于同质和异质设置。

**Result:** 实验结果表明，我们的模型能够捕捉到本地客户端数据的多样性，并在现有的工作基础上实现了更好的性能。

**Conclusion:** 自适应联邦蒸馏（AdaFD）框架展示了在处理非独立同分布数据时能够适应多领域数据的能力，并在同质和异质设置下表现出色。

**Abstract:** The widespread success of pre-trained language models has established a new
training paradigm, where a global PLM is fine-tuned using task-specific data
from local clients. The local data are highly different from each other and can
not capture the global distribution of the whole data in real world. To address
the challenges of non-IID data in real environments, privacy-preserving
federated distillation has been proposed and highly investigated. However,
previous experimental non-IID scenarios are primarily identified with the label
(output) diversity, without considering the diversity of language domains
(input) that is crucial in natural language processing. In this paper, we
introduce a comprehensive set of multi-domain non-IID scenarios and propose a
unified benchmarking framework that includes diverse data. The benchmark can be
used to evaluate the federated learning framework in a real environment. To
this end, we propose an Adaptive Federated Distillation (AdaFD) framework
designed to address multi-domain non-IID challenges in both homogeneous and
heterogeneous settings. Experimental results demonstrate that our models
capture the diversity of local clients and achieve better performance compared
to the existing works. The code for this paper is available at:
https://github.com/jiahaoxiao1228/AdaFD.

</details>


### [23] [Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search](https://arxiv.org/abs/2508.20559)
*Zeyu Xiong,Yixuan Nan,Li Gao,Hengzhu Tang,Shuaiqiang Wang,Junfeng Wang,Dawei Yin*

Main category: cs.CL

> A new framework for real-time QDTS using generative models surpasses traditional extractive models, achieving state-of-the-art performance with high efficiency and lower resource requirements.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to improve upon traditional extractive summarization models, which are prevalent in industrial applications, by addressing their limitations in cumulative information loss and insufficient semantic understanding of queries and documents.

**Method:** This study proposes a novel framework that applies generative models to real-time Query-Driven Text Summarization (QDTS) in web search. The framework includes four critical components: large model distillation, supervised fine-tuning, direct preference optimization, and lookahead decoding to create a highly efficient and effective summarization model.

**Result:** The proposed model not only outperforms the production baseline but also achieves state-of-the-art performance in industrial-relevant metrics. Additionally, it showcases high deployment efficiency, handling approximately 50,000 queries per second on 334 NVIDIA L20 GPUs with an average latency of 55ms per query.

**Conclusion:** The integration of generative models through the proposed framework significantly advances the capability of real-time QDTS, offering enhanced semantic understanding and efficiency with fewer resources compared to traditional models and the existing baseline.

**Abstract:** In the dynamic landscape of large-scale web search, Query-Driven Text
Summarization (QDTS) aims to generate concise and informative summaries from
textual documents based on a given query, which is essential for improving user
engagement and facilitating rapid decision-making. Traditional extractive
summarization models, based primarily on ranking candidate summary segments,
have been the dominant approach in industrial applications. However, these
approaches suffer from two key limitations: 1) The multi-stage pipeline often
introduces cumulative information loss and architectural bottlenecks due to its
weakest component; 2) Traditional models lack sufficient semantic understanding
of both user queries and documents, particularly when dealing with complex
search intents. In this study, we propose a novel framework to pioneer the
application of generative models to address real-time QDTS in industrial web
search. Our approach integrates large model distillation, supervised
fine-tuning, direct preference optimization, and lookahead decoding to
transform a lightweight model with only 0.1B parameters into a
domain-specialized QDTS expert. Evaluated on multiple industry-relevant
metrics, our model outperforms the production baseline and achieves a new state
of the art. Furthermore, it demonstrates excellent deployment efficiency,
requiring only 334 NVIDIA L20 GPUs to handle \textasciitilde50,000 queries per
second under 55~ms average latency per query.

</details>


### [24] [KCS: Diversify Multi-hop Question Generation with Knowledge Composition Sampling](https://arxiv.org/abs/2508.20567)
*Yangfan Wang,Jie Liu,Chen Tang,Lian Yan,Jingchi Jiang*

Main category: cs.CL

> 提出了一种新的框架KCS，用于提高多跳问答生成问题的多样性，通过在给定上下文中抽样不同的知识组合，取得了高于基线3.9%的改进，并已在HotpotQA和2WikiMultihopQA数据集上验证了其有效性。代码已开源。

<details>
  <summary>Details</summary>

**Motivation:** 多跳问答面临数据稀疏性的挑战，这增加了语言模型学习虚假模式的可能性。以往研究集中在通过内容规划和不同表达方式来多样化问题生成，但这些方法往往侧重于生成简单的问题，并忽视了必要知识的整合，如文档中的相关句子。

**Method:** 介绍了一种创新框架Knowledge Composition Sampling (KCS)，该框架旨在通过在给定上下文中抽样不同知识组合来扩展生成的多跳问题的多样性。KCS将知识组合的选择建模为句子级别的条件预测任务，并使用概率对比损失来预测下一个最相关知识片段。在推理阶段，使用随机解码策略来有效平衡精度和多样性。

**Result:** 与竞争基线相比，该KCS提高了3.9%的知识组合选择的总体准确性，并且其用于数据增强时，在HotpotQA和2WikiMultihopQA数据集上都有改进。

**Conclusion:** KCS是用于生成多样化多跳问题的有效工具，能够在保持准确性的同时增加多样性。其源代码已开源。

**Abstract:** Multi-hop question answering faces substantial challenges due to data
sparsity, which increases the likelihood of language models learning spurious
patterns. To address this issue, prior research has focused on diversifying
question generation through content planning and varied expression. However,
these approaches often emphasize generating simple questions and neglect the
integration of essential knowledge, such as relevant sentences within
documents. This paper introduces the Knowledge Composition Sampling (KCS), an
innovative framework designed to expand the diversity of generated multi-hop
questions by sampling varied knowledge compositions within a given context. KCS
models the knowledge composition selection as a sentence-level conditional
prediction task and utilizes a probabilistic contrastive loss to predict the
next most relevant piece of knowledge. During inference, we employ a stochastic
decoding strategy to effectively balance accuracy and diversity. Compared to
competitive baselines, our KCS improves the overall accuracy of knowledge
composition selection by 3.9%, and its application for data augmentation yields
improvements on HotpotQA and 2WikiMultihopQA datasets. Our code is available
at: https://github.com/yangfanww/kcs.

</details>


### [25] [A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models](https://arxiv.org/abs/2508.20583)
*Soham Petkar,Hari Aakash K,Anirudh Vempati,Akshit Sinha,Ponnurangam Kumarauguru,Chirag Agarwal*

Main category: cs.CL

> 分析指出，当前的GLMs评估标准不足以评估多模态推理能力，因此需要新的评估方法，如CLEGR基准测试。发现LLM基线方法与GLMs性能相当，表明GNN主干并非必要。

<details>
  <summary>Details</summary>

**Motivation:** 发现当前GLMs的评估基准，主要是重新利用节点级别的分类数据集，不足以评估多模态推理能力。现有基准测试中的强表现是使用单一模态信息即可实现的，意味着它们不强制要求图-语言的融合。

**Method:** 通过引入CLEGR（组合语言-图推理）基准测试来评估多模态推理能力，该基准测试使用合成图生成管道配合需要联合推理结构和文本语义的问题来设计。

**Result:** 发现使用软提示的LLM基线方法与包含完整GNN主干的GLM具有相当的性能水平；并且在需要结构化推理的任务中，GLMs的表现明显下降。

**Conclusion:** 这些发现揭示了现有GLMs在图形推理方面的能力限制，并为社区迈向明确的多模态推理，涉及图结构和语言提供了一个基础。

**Abstract:** Developments in Graph-Language Models (GLMs) aim to integrate the structural
reasoning capabilities of Graph Neural Networks (GNNs) with the semantic
understanding of Large Language Models (LLMs). However, we demonstrate that
current evaluation benchmarks for GLMs, which are primarily repurposed
node-level classification datasets, are insufficient to assess multimodal
reasoning. Our analysis reveals that strong performance on these benchmarks is
achievable using unimodal information alone, suggesting that they do not
necessitate graph-language integration. To address this evaluation gap, we
introduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designed
to evaluate multimodal reasoning at various complexity levels. Our benchmark
employs a synthetic graph generation pipeline paired with questions that
require joint reasoning over structure and textual semantics. We perform a
thorough evaluation of representative GLM architectures and find that
soft-prompted LLM baselines perform on par with GLMs that incorporate a full
GNN backbone. This result calls into question the architectural necessity of
incorporating graph structure into LLMs. We further show that GLMs exhibit
significant performance degradation in tasks that require structural reasoning.
These findings highlight limitations in the graph reasoning capabilities of
current GLMs and provide a foundation for advancing the community toward
explicit multimodal reasoning involving graph structure and language.

</details>


### [26] [Generative Annotation for ASR Named Entity Correction](https://arxiv.org/abs/2508.20700)
*Yuanchang Luo,Daimeng Wei,Shaojun Li,Hengchao Shang,Jiaxin Guo,Zongyao Li,Zhanglin Wu,Xiaoyu Chen,Zhiqiang Rao,Jinlong Yang,Hao Yang*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** End-to-end automatic speech recognition systems often fail to transcribe
domain-specific named entities, causing catastrophic failures in downstream
tasks. Numerous fast and lightweight named entity correction (NEC) models have
been proposed in recent years. These models, mainly leveraging phonetic-level
edit distance algorithms, have shown impressive performances. However, when the
forms of the wrongly-transcribed words(s) and the ground-truth entity are
significantly different, these methods often fail to locate the wrongly
transcribed words in hypothesis, thus limiting their usage. We propose a novel
NEC method that utilizes speech sound features to retrieve candidate entities.
With speech sound features and candidate entities, we inovatively design a
generative method to annotate entity errors in ASR transcripts and replace the
text with correct entities. This method is effective in scenarios of word form
difference. We test our method using open-source and self-constructed test
sets. The results demonstrate that our NEC method can bring significant
improvement to entity accuracy. We will open source our self-constructed test
set and training data.

</details>


### [27] [Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label Hierarchical Learning](https://arxiv.org/abs/2508.20712)
*Nelson Filipe Costa,Leila Kosseim*

Main category: cs.CL

> 论文介绍了一种名为HArch的新模型，它是一种多语言、多标签的隐式语篇关系分类模型，采用层次化依赖关系进行预测，实验结果表明在多个语言环境下表现优异。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在开发一种能够在多语言环境下工作的多标签IDRR模型，并在最近发布的DiscoGeM 2.0语料库上进行评估，对比了不同的预训练编码器和GPT-4o、Llama-4-Maverick等模型的性能。

**Method:** 该论文介绍了一种多语言、多标签的隐式语篇关系识别（IDRR）模型——HArch。该模型基于层次化的语篇意义依赖关系，预测PDTB 3.0框架下的三种意义层次的概率分布。

**Result:** 实验结果显示，在英语环境中，RoBERTa-HArch表现最佳；而在多语言环境中，XLM-RoBERTa-HArch表现最优。此外，在所有语言配置下，使用少量示例提示对比展示了微调模型在IDRR任务上优于LLMs的性能。

**Conclusion:** HArch模型在DiscoGeM 1.0语料库上取得SOTA结果，进一步验证了层次化方法的有效性。

**Abstract:** This paper introduces the first multi-lingual and multi-label classification
model for implicit discourse relation recognition (IDRR). Our model, HArch, is
evaluated on the recently released DiscoGeM 2.0 corpus and leverages
hierarchical dependencies between discourse senses to predict probability
distributions across all three sense levels in the PDTB 3.0 framework. We
compare several pre-trained encoder backbones and find that RoBERTa-HArch
achieves the best performance in English, while XLM-RoBERTa-HArch performs best
in the multi-lingual setting. In addition, we compare our fine-tuned models
against GPT-4o and Llama-4-Maverick using few-shot prompting across all
language configurations. Our results show that our fine-tuned models
consistently outperform these LLMs, highlighting the advantages of
task-specific fine-tuning over prompting in IDRR. Finally, we report SOTA
results on the DiscoGeM 1.0 corpus, further validating the effectiveness of our
hierarchical approach.

</details>


### [28] [Addressing Tokenization Inconsistency in Steganography and Watermarking Based on Large Language Models](https://arxiv.org/abs/2508.20718)
*Ruiyi Yan,Yugo Murawaki*

Main category: cs.CL

> The paper addresses tokenization inconsistency that affects text-based steganography and watermarking, proposing methods to resolve it and improving robustness and quality of both techniques.

<details>
  <summary>Details</summary>

**Motivation:** The study aims to address tokenization inconsistency between Alice and Bob which undermines the robustness of steganography and watermarking.

**Method:** The paper proposes two solutions for tokenization inconsistency (TI): a stepwise verification method for steganography and a post-hoc rollback method for watermarking.

**Result:** Experiments show that addressing TI directly improves the quality and robustness of both steganography and watermarking techniques.

**Conclusion:** The proposed methods significantly improve text-based steganography and watermarking by addressing TI.

**Abstract:** Large language models have significantly enhanced the capacities and
efficiency of text generation. On the one hand, they have improved the quality
of text-based steganography. On the other hand, they have also underscored the
importance of watermarking as a safeguard against malicious misuse. In this
study, we focus on tokenization inconsistency (TI) between Alice and Bob in
steganography and watermarking, where TI can undermine robustness. Our
investigation reveals that the problematic tokens responsible for TI exhibit
two key characteristics: infrequency and temporariness. Based on these
findings, we propose two tailored solutions for TI elimination: a stepwise
verification method for steganography and a post-hoc rollback method for
watermarking. Experiments show that (1) compared to traditional disambiguation
methods in steganography, directly addressing TI leads to improvements in
fluency, imperceptibility, and anti-steganalysis capacity; (2) for
watermarking, addressing TI enhances detectability and robustness against
attacks.

</details>


### [29] [rStar2-Agent: Agentic Reasoning Technical Report](https://arxiv.org/abs/2508.20722)
*Ning Shang,Yifei Liu,Yi Zhu,Li Lyna Zhang,Weijiang Xu,Xinyu Guan,Buze Zhang,Bingcheng Dong,Xudong Zhou,Bowen Zhang,Ying Xin,Ziming Miao,Scarlett Li,Fan Yang,Mao Yang*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** We introduce rStar2-Agent, a 14B math reasoning model trained with agentic
reinforcement learning to achieve frontier-level performance. Beyond current
long CoT, the model demonstrates advanced cognitive behaviors, such as thinking
carefully before using Python coding tools and reflecting on code execution
feedback to autonomously explore, verify, and refine intermediate steps in
complex problem-solving. This capability is enabled through three key
innovations that makes agentic RL effective at scale: (i) an efficient RL
infrastructure with a reliable Python code environment that supports
high-throughput execution and mitigates the high rollout costs, enabling
training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic
RL algorithm with a Resample-on-Correct rollout strategy that addresses the
inherent environment noises from coding tools, allowing the model to reason
more effectively in a code environment; (iii) An efficient agent training
recipe that starts with non-reasoning SFT and progresses through multi-RL
stages, yielding advanced cognitive abilities with minimal compute cost. To
this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in
only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on
AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly
shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates
strong generalization to alignment, scientific reasoning, and agentic tool-use
tasks. Code and training recipes are available at
https://github.com/microsoft/rStar.

</details>


### [30] [Leveraging Semantic Triples for Private Document Generation with Local Differential Privacy Guarantees](https://arxiv.org/abs/2508.20736)
*Stephen Meisenbacher,Maulik Chevli,Florian Matthes*

Main category: cs.CL

> DP-ST introduces a method to generate coherent, privatized documents under local differential privacy that works well at lower ε values, improving upon existing methods which struggle to balance privacy and utility without requiring very high ε.

<details>
  <summary>Details</summary>

**Motivation:** The goal is to effectively privatize texts under local differential privacy while maintaining text coherence, addressing the issue of requiring very high ε values for reasonable privacy transformations.

**Method:** DP-ST, a method utilizing semantic triples for neighborhood-aware private document generation under local differential privacy.

**Result:** Evaluation shows that by limiting the differential privacy concept to a privatization neighborhood, and combining with LLM post-processing, the method produces coherent text even at lower ε values, demonstrating a balance between privacy and utility.

**Conclusion:** Coherence is crucial in achieving effective text privatization at reasonable privacy parameters, indicating a promising direction for future research in balancing privacy and text utility within differential privacy frameworks.

**Abstract:** Many works at the intersection of Differential Privacy (DP) in Natural
Language Processing aim to protect privacy by transforming texts under DP
guarantees. This can be performed in a variety of ways, from word perturbations
to full document rewriting, and most often under local DP. Here, an input text
must be made indistinguishable from any other potential text, within some bound
governed by the privacy parameter $\varepsilon$. Such a guarantee is quite
demanding, and recent works show that privatizing texts under local DP can only
be done reasonably under very high $\varepsilon$ values. Addressing this
challenge, we introduce DP-ST, which leverages semantic triples for
neighborhood-aware private document generation under local DP guarantees.
Through the evaluation of our method, we demonstrate the effectiveness of the
divide-and-conquer paradigm, particularly when limiting the DP notion (and
privacy guarantees) to that of a privatization neighborhood. When combined with
LLM post-processing, our method allows for coherent text generation even at
lower $\varepsilon$ values, while still balancing privacy and utility. These
findings highlight the importance of coherence in achieving balanced
privatization outputs at reasonable $\varepsilon$ levels.

</details>


### [31] [Specializing General-purpose LLM Embeddings for Implicit Hate Speech Detection across Datasets](https://arxiv.org/abs/2508.20750)
*Vassiliy Cheremetiev,Quang Long Ho Ngo,Chau Ying Kot,Alina Elena Baia,Andrea Cavallaro*

Main category: cs.CL

> 研究显示，通过微调大型语言模型的嵌入模型，可以在检测隐含仇恨言论上取得显著性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 隐含仇恨言论由于其表达方式隐秘，通常通过细微暗示、讽刺或暗语传达，使其难以被检测。

**Method:** 通过仅微调基于大型语言模型（LLMs）的通用嵌入模型，如Stella、Jasper、NV-Embed和E5，来检测隐含仇恨言论(IHS)。

**Result:** 在多个IHS数据集上的实验显示，使用这些模型在数据集内F1宏平均得分提高了1.10个百分点，跨数据集评估则提高了20.35个百分点。

**Conclusion:** 实验结果证明，无须额外增强，仅通过微调现有的通用嵌入模型即可达到行业领先水平，且在跨数据集的评估中表现尤为突出。

**Abstract:** Implicit hate speech (IHS) is indirect language that conveys prejudice or
hatred through subtle cues, sarcasm or coded terminology. IHS is challenging to
detect as it does not include explicit derogatory or inflammatory words. To
address this challenge, task-specific pipelines can be complemented with
external knowledge or additional information such as context, emotions and
sentiment data. In this paper, we show that, by solely fine-tuning recent
general-purpose embedding models based on large language models (LLMs), such as
Stella, Jasper, NV-Embed and E5, we achieve state-of-the-art performance.
Experiments on multiple IHS datasets show up to 1.10 percentage points
improvements for in-dataset, and up to 20.35 percentage points improvements in
cross-dataset evaluation, in terms of F1-macro score.

</details>


### [32] [GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and Efficient Open-Ended Text Generation](https://arxiv.org/abs/2508.20757)
*Yuanhao Ding,Esteban Garces Arias,Meimingwei Li,Julian Rodemann,Matthias Aßenmacher,Danlu Chen,Gaojuan Fan,Christian Heumann,Chongsheng Zhang*

Main category: cs.CL

> The paper introduces GUARD, an efficient, self-adaptive decoding method for LLM that balances the generation of diverse yet coherent text, outperforming existing methods in both quality and speed.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of balancing coherence with diversity in LLM outputs, which existing contrastive search-based decoding strategies struggle with.

**Method:** GUARD, a self-adaptive decoding method that combines global entropy estimates with local entropy deviations.

**Result:** GUARD effectively balances text diversity and coherence, and shows significant improvements in generation speed, validated by human and LLM evaluators.

**Conclusion:** The proposed GUARD method is a successful approach that mitigates abrupt variations in uncertainty, offering theoretical guarantees of unbiasedness and consistency, and reducing computational overhead.

**Abstract:** Open-ended text generation faces a critical challenge: balancing coherence
with diversity in LLM outputs. While contrastive search-based decoding
strategies have emerged to address this trade-off, their practical utility is
often limited by hyperparameter dependence and high computational costs. We
introduce GUARD, a self-adaptive decoding method that effectively balances
these competing objectives through a novel "Glocal" uncertainty-driven
framework. GUARD combines global entropy estimates with local entropy
deviations to integrate both long-term and short-term uncertainty signals. We
demonstrate that our proposed global entropy formulation effectively mitigates
abrupt variations in uncertainty, such as sudden overconfidence or high entropy
spikes, and provides theoretical guarantees of unbiasedness and consistency. To
reduce computational overhead, we incorporate a simple yet effective
token-count-based penalty into GUARD. Experimental results demonstrate that
GUARD achieves a good balance between text diversity and coherence, while
exhibiting substantial improvements in generation speed. In a more nuanced
comparison study across different dimensions of text quality, both human and
LLM evaluators validated its remarkable performance. Our code is available at
https://github.com/YecanLee/GUARD.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [33] [Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization](https://arxiv.org/abs/2508.20181)
*Alberto Compagnoni,Davide Caffagni,Nicholas Moratelli,Lorenzo Baraldi,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

> 本文提出了一种新的方法（CHAIR-DPO），解决了多模态大语言模型生成回答时的幻觉问题，该方法利用CHAIR指标对模型进行微调。

<details>
  <summary>Details</summary>

**Motivation:** 解决多模态大语言模型在生成回答时倾向于产生幻觉的问题，即生成的答案与视觉输入不一致。

**Method:** 通过利用已有的CHAIR指标，作者规避了生成合成偏好数据的复杂流程，直接对多模态大语言模型进行了偏好优化微调（DPO），以减少幻觉现象。

**Result:** 通过CHAIR-DPO方法，模型在若干幻觉基准测试上的表现得到了显著改善，幻觉产生的程度减少。

**Conclusion:** 利用CHAIR作为奖励基础，直接偏好优化微调多模态大语言模型，有效减轻了幻觉问题。

**Abstract:** Multimodal Large Language Models (MLLMs) emerge as a unified interface to
address a multitude of tasks, ranging from NLP to computer vision. Despite
showcasing state-of-the-art results in many benchmarks, a long-standing issue
is the tendency of MLLMs to hallucinate, that is to generate answers to the
user's query that are not reflected in the visual input. In this paper, we
address the problem of hallucinations as an alignment problem, seeking to steer
the MLLM so that it prefers generating content without hallucinations. In
contrast to recent approaches that require complicated pipelines to build
synthetic preference data for alignment training, often relying on proprietary
models, we capitalize on the well-known CHAIR metric, originally proposed to
gauge the degree of hallucinations in image captioning. Given a pair of
generated answers, we leverage CHAIR to distinguish winner and loser options
(i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf
MLLMs via Direct Preference Optimization (DPO). The resulting method, which we
refer to as CHAIR-DPO, effectively diminishes the amount of hallucinated
answers on several hallucination benchmarks, demonstrating the effectiveness of
fine-tuning the MLLM with a CHAIR-based reward. Source code and trained models
are publicly available at https://github.com/aimagelab/CHAIR-DPO.

</details>


### [34] [SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization](https://arxiv.org/abs/2508.20182)
*Yang Su,Shunquan Tan,Jiwu Huang*

Main category: cs.CV

> 提出了一种新的图像取证框架，利用Stable DiffusionV3的多模态处理能力进行伪造图像定位，显著提升了定位准确性和效率。

<details>
  <summary>Details</summary>

**Motivation:** 随着新型多模态大模型的驱动下，图像操纵技术快速发展，对图像取证构成了重大挑战。现有的伪造图像定位方法依赖于耗时耗力的人工标注数据，难以跟上新兴的图像操纵技术。为了应对这些挑战，我们提出了新的解决方案，使用了Stable Diffusion的技术以提升效率和准确性。

**Method:** 我们首次将Stable Diffusion（SD）的图像生成能力和强大的感知能力整合到一个图像取证框架中，以实现更高效和准确的伪造定位。理论分析表明，SD的多模态架构可以基于伪造相关信息进行条件化，从而模型可以自然地输出伪造定位结果。在此基础上，我们利用Stable DiffusionV3（SD3）的多模态框架增强伪造定位性能。通过在潜在空间中处理作为明确模态的图像伪造残差（使用特定高通滤波器提取的高频信号）来提升伪造定位性能。我们的方法完全保留了SD3提取的潜在特征，从而保留了输入图像中的丰富语义信息。

**Result:** 实验结果显示我们的框架在广泛的基准数据集上实现了高达12%的性能提升，并在涉及真实文档及自然场景下的伪造图像取证任务上表现出极佳的性能。即使是未曾于训练中见过的数据，仍保持了良好的表现。

**Conclusion:** 实验证明，我们的框架在广泛使用的基准测试数据集上比现有的最先进的图像伪造定位模型性能提高了12%。该模型在涉及真实世界文档伪造图像和自然场景的伪造图像的取证任务中表现良好，即使在训练过程中没有看到这种数据。此研究为未来图像取证技术的发展提供了新的方向。

**Abstract:** Driven by the new generation of multi-modal large models, such as Stable
Diffusion (SD), image manipulation technologies have advanced rapidly, posing
significant challenges to image forensics. However, existing image forgery
localization methods, which heavily rely on labor-intensive and costly
annotated data, are struggling to keep pace with these emerging image
manipulation technologies. To address these challenges, we are the first to
integrate both image generation and powerful perceptual capabilities of SD into
an image forensic framework, enabling more efficient and accurate forgery
localization. First, we theoretically show that the multi-modal architecture of
SD can be conditioned on forgery-related information, enabling the model to
inherently output forgery localization results. Then, building on this
foundation, we specifically leverage the multimodal framework of Stable
DiffusionV3 (SD3) to enhance forgery localization performance.We leverage the
multi-modal processing capabilities of SD3 in the latent space by treating
image forgery residuals -- high-frequency signals extracted using specific
highpass filters -- as an explicit modality. This modality is fused into the
latent space during training to enhance forgery localization performance.
Notably, our method fully preserves the latent features extracted by SD3,
thereby retaining the rich semantic information of the input image.
Experimental results show that our framework achieves up to 12% improvements in
performance on widely used benchmarking datasets compared to current
state-of-the-art image forgery localization models. Encouragingly, the model
demonstrates strong performance on forensic tasks involving real-world document
forgery images and natural scene forging images, even when such data were
entirely unseen during training.

</details>


### [35] [Grounding Multimodal Large Language Models with Quantitative Skin Attributes: A Retrieval Study](https://arxiv.org/abs/2508.20188)
*Max Torop,Masih Eskandar,Nicholas Kurtansky,Jinyang Liu,Jochen Weber,Octavia Camps,Veronica Rotemberg,Jennifer Dy,Kivanc Kose*

Main category: cs.CV

> 本研究通过多模态大规模语言模型（MLLM）结合定量属性，尝试提高皮肤病AI诊断模型的可解释性，并利用SLICE-3D数据集进行验证。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在提高AI诊断皮肤病模型的可解释性，以便这些模型能够在临床实践中应用。

**Method:** 本研究探索了多模态大规模语言模型（MLLM）与定量属性使用相结合的方法，以提高人工智能模型在诊断皮肤病（包括癌症）时的可解释性。

**Result:** 研究通过在MLLM嵌入空间中预测定量属性（如病灶面积）的值，证明了其在提高可解释性方面的潜力。

**Conclusion:** 结果表明，通过针对图像的微调，MLLM的嵌入空间可以与皮肤病相关定量属性实现有效关联，这为提高AI诊断的可解释性提供了新的途径。

**Abstract:** Artificial Intelligence models have demonstrated significant success in
diagnosing skin diseases, including cancer, showing the potential to assist
clinicians in their analysis. However, the interpretability of model
predictions must be significantly improved before they can be used in practice.
To this end, we explore the combination of two promising approaches: Multimodal
Large Language Models (MLLMs) and quantitative attribute usage. MLLMs offer a
potential avenue for increased interpretability, providing reasoning for
diagnosis in natural language through an interactive format. Separately, a
number of quantitative attributes that are related to lesion appearance (e.g.,
lesion area) have recently been found predictive of malignancy with high
accuracy. Predictions grounded as a function of such concepts have the
potential for improved interpretability. We provide evidence that MLLM
embedding spaces can be grounded in such attributes, through fine-tuning to
predict their values from images. Concretely, we evaluate this grounding in the
embedding space through an attribute-specific content-based image retrieval
case study using the SLICE-3D dataset.

</details>


### [36] [Enhancing Automatic Modulation Recognition With a Reconstruction-Driven Vision Transformer Under Limited Labels](https://arxiv.org/abs/2508.20193)
*Hossein Ahmadi,Banafsheh Saffari*

Main category: cs.CV

> 我们提出了一种新的统一视觉变压器框架，该框架结合了监督学习、自监督学习和重建技术，能够有效处理自动调制识别任务，展示出在低标签数据情况下优秀的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的自动调制识别（AMR）解决方案通常依赖于大量标注的数据集或多阶段训练管道，这在实践中限制了可扩展性和泛化能力。因此，我们提出了一个更简洁、泛化能力和标签效率更高的解决方案。

**Method:** 我们提出了一个统一的视觉变压器（ViT）框架，该框架结合了监督学习、自监督学习和重建目标。模型包括一个ViT编码器、一个轻量级的卷积解码器和一个线性分类器。重建分支将增强的信号映射回到原始信号，使编码器更加关注I/Q结构的细微之处。这一策略在预训练过程中促进了鲁棒性特征学习，而在微调过程中部分标签监督允许模型在有限的标签下有效分类。

**Result:** 在RML2018.01A数据集上，提出的框架表现优于监督CNN和ViT基线模型在低标签情况下的表现，仅需15-20%的标注数据就能达到ResNet级别的准确性，并且在不同的信噪比水平下都保持了强大的性能。

**Conclusion:** 总的来看，提出的框架为AMR提供了一个简单、泛化能力强和标签效率较高的解决方案。

**Abstract:** Automatic modulation recognition (AMR) is critical for cognitive radio,
spectrum monitoring, and secure wireless communication. However, existing
solutions often rely on large labeled datasets or multi-stage training
pipelines, which limit scalability and generalization in practice. We propose a
unified Vision Transformer (ViT) framework that integrates supervised,
self-supervised, and reconstruction objectives. The model combines a ViT
encoder, a lightweight convolutional decoder, and a linear classifier; the
reconstruction branch maps augmented signals back to their originals, anchoring
the encoder to fine-grained I/Q structure. This strategy promotes robust,
discriminative feature learning during pretraining, while partial label
supervision in fine-tuning enables effective classification with limited
labels. On the RML2018.01A dataset, our approach outperforms supervised CNN and
ViT baselines in low-label regimes, approaches ResNet-level accuracy with only
15-20% labeled data, and maintains strong performance across varying SNR
levels. Overall, the framework provides a simple, generalizable, and
label-efficient solution for AMR.

</details>


### [37] [InfinityHuman: Towards Long-Term Audio-Driven Human](https://arxiv.org/abs/2508.20210)
*Xiaodi Li,Pan Xie,Yi Ren,Qijun Gan,Chen Zhang,Fangyuan Kong,Xiang Yin,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

> This paper introduces InfinityHuman, a framework for generating high-quality audio-driven human animations with improved video, hand accuracy, and lip synchronization using a pose-guided refiner and a hand-specific reward mechanism.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the challenges of generating high-resolution, long-duration videos with consistent appearance and natural hand motions from audio, which are not sufficiently handled by existing methods.

**Method:** The paper proposes InfinityHuman, a coarse-to-fine framework that first generates audio-synchronized representations and then progressively refines them into high-resolution, long-duration videos using a pose-guided refiner. The system also introduces a hand-specific reward mechanism trained with high-quality hand motion data to enhance gesture realism.

**Result:** Experiments on the EMTD and HDTF datasets show that InfinityHuman achieves state-of-the-art performance in video quality, identity preservation, hand accuracy, and lip-sync.

**Conclusion:** The paper concludes that the proposed InfinityHuman system effectively resolves issues with identity drift, scene instability, and poor hand motion representation, leading to improved audio-driven human animation.

**Abstract:** Audio-driven human animation has attracted wide attention thanks to its
practical applications. However, critical challenges remain in generating
high-resolution, long-duration videos with consistent appearance and natural
hand motions. Existing methods extend videos using overlapping motion frames
but suffer from error accumulation, leading to identity drift, color shifts,
and scene instability. Additionally, hand movements are poorly modeled,
resulting in noticeable distortions and misalignment with the audio. In this
work, we propose InfinityHuman, a coarse-to-fine framework that first generates
audio-synchronized representations, then progressively refines them into
high-resolution, long-duration videos using a pose-guided refiner. Since pose
sequences are decoupled from appearance and resist temporal degradation, our
pose-guided refiner employs stable poses and the initial frame as a visual
anchor to reduce drift and improve lip synchronization. Moreover, to enhance
semantic accuracy and gesture realism, we introduce a hand-specific reward
mechanism trained with high-quality hand motion data. Experiments on the EMTD
and HDTF datasets show that InfinityHuman achieves state-of-the-art performance
in video quality, identity preservation, hand accuracy, and lip-sync. Ablation
studies further confirm the effectiveness of each module. Code will be made
public.

</details>


### [38] [Spherical Vision Transformers for Audio-Visual Saliency Prediction in 360-Degree Videos](https://arxiv.org/abs/2508.20221)
*Mert Cokelek,Halit Ozsoy,Nevrez Imamoglu,Cagri Ozcinar,Inci Ayhan,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

> 本研究探讨了如何利用视听线索有效地预测360度视频中的视觉显著性，并提出了两个显著性预测模型：SalViT360和SalViT360-AV。结果显示，这两种模型在预测观者注意力方面显著优于现有的方法。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在扩展对全向视觉在360度环境中的显著性预测，解决球形失真和空间音频集成的问题。基于现有的360度视听显著性预测数据集的不足，本研究构建了包含81个ODVs观测视频的新数据集YT360-EyeTracking。

**Method:** 本研究提出了两种新颖的预测模型：SalViT360，一种适用于全景视频的视觉-变换器框架，配备了球面几何感知的空间-时间注意力层，以及SalViT360-AV，该模型进一步整合了基于音频输入的变换器适配器。

**Result:** 实验在包括研究者构建的数据集YT360-EyeTracking在内的多个基准数据集上进行，结果表明新的模型方案SalViT360和SalViT360-AV在预测观者在360度场景中的注意力分布方面显著表现优于现有方法。

**Conclusion:** 实验结果表明，将空间音频线索整合入模型架构对于准确预测全景视频中的显著性至关重要，新的模型方案SalViT360和SalViT360-AV显著优于现有的方法。

**Abstract:** Omnidirectional videos (ODVs) are redefining viewer experiences in virtual
reality (VR) by offering an unprecedented full field-of-view (FOV). This study
extends the domain of saliency prediction to 360-degree environments,
addressing the complexities of spherical distortion and the integration of
spatial audio. Contextually, ODVs have transformed user experience by adding a
spatial audio dimension that aligns sound direction with the viewer's
perspective in spherical scenes. Motivated by the lack of comprehensive
datasets for 360-degree audio-visual saliency prediction, our study curates
YT360-EyeTracking, a new dataset of 81 ODVs, each observed under varying
audio-visual conditions. Our goal is to explore how to utilize audio-visual
cues to effectively predict visual saliency in 360-degree videos. Towards this
aim, we propose two novel saliency prediction models: SalViT360, a
vision-transformer-based framework for ODVs equipped with spherical
geometry-aware spatio-temporal attention layers, and SalViT360-AV, which
further incorporates transformer adapters conditioned on audio input. Our
results on a number of benchmark datasets, including our YT360-EyeTracking,
demonstrate that SalViT360 and SalViT360-AV significantly outperform existing
methods in predicting viewer attention in 360-degree scenes. Interpreting these
results, we suggest that integrating spatial audio cues in the model
architecture is crucial for accurate saliency prediction in omnidirectional
videos. Code and dataset will be available at
https://cyberiada.github.io/SalViT360.

</details>


### [39] [A Novel Framework for Automated Explain Vision Model Using Vision-Language Models](https://arxiv.org/abs/2508.20227)
*Phu-Vinh Nguyen,Tan-Hanh Pham,Chris Ngo,Truong Son Hy*

Main category: cs.CV

> This paper introduces a pipeline for explaining the behavior of vision models, utilizing Vision-Language Models to offer insights at both sample and dataset levels, thereby enhancing model explainability and mitigating potential biases.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the issue of explainability in vision models, which has often been overlooked. It aims to provide a meaningful explanation of the models' behavior, both at the sample level and at the dataset level, which can help prevent biased judgments and identify trends and patterns.

**Method:** This paper proposes a pipeline to explain vision models at both the sample and dataset levels, leveraging Vision-Language Models to understand the models' behavior.

**Result:** The proposed pipeline can discover failure cases and provide insights into vision models with minimal effort, making it easier to integrate xAI analysis into image analysis tasks.

**Conclusion:** The approach advances the integration of xAI analysis into vision model development, providing a deeper understanding of model behavior and helping to mitigate bias.

**Abstract:** The development of many vision models mainly focuses on improving their
performance using metrics such as accuracy, IoU, and mAP, with less attention
to explainability due to the complexity of applying xAI methods to provide a
meaningful explanation of trained models. Although many existing xAI methods
aim to explain vision models sample-by-sample, methods explaining the general
behavior of vision models, which can only be captured after running on a large
dataset, are still underexplored. Furthermore, understanding the behavior of
vision models on general images can be very important to prevent biased
judgments and help identify the model's trends and patterns. With the
application of Vision-Language Models, this paper proposes a pipeline to
explain vision models at both the sample and dataset levels. The proposed
pipeline can be used to discover failure cases and gain insights into vision
models with minimal effort, thereby integrating vision model development with
xAI analysis to advance image analysis.

</details>


### [40] [ATMS-KD: Adaptive Temperature and Mixed Sample Knowledge Distillation for a Lightweight Residual CNN in Agricultural Embedded Systems](https://arxiv.org/abs/2508.20232)
*Mohamed Ohamouddou,Said Ohamouddou,Abdellatif El Afia,Rafik Lasri*

Main category: cs.CV

> ATMS-KD is a framework for training lightweight CNN models for resource-limited agricultural environments, using MobileNetV3 as a teacher model, outperforming other methods with near 97.11% accuracy and low latency on a dataset of Rosa damascena images.

<details>
  <summary>Details</summary>

**Motivation:** To develop lightweight CNN models that are suitable for resource-constrained agricultural environments and to improve upon direct training methods for agricultural computer vision applications.

**Method:** The framework ATMS-KD combines adaptive temperature scheduling with mixed-sample augmentation to transfer knowledge from a MobileNetV3 Large teacher model to lightweight residual CNN student models under diverse agricultural environmental conditions using a dataset of Rosa damascena images.

**Result:** Student models trained using ATMS-KD achieved validation accuracies exceeding 96.7%, with the compact model reaching 97.11% accuracy, surpassing direct training and eleven established methods.

**Conclusion:** ATMS-KD effectively improves model accuracy and knowledge transfer across different student capacities in agricultural scenarios, achieving higher accuracy and lower inference latency than existing methods.

**Abstract:** This study proposes ATMS-KD (Adaptive Temperature and Mixed-Sample Knowledge
Distillation), a novel framework for developing lightweight CNN models suitable
for resource-constrained agricultural environments. The framework combines
adaptive temperature scheduling with mixed-sample augmentation to transfer
knowledge from a MobileNetV3 Large teacher model (5.7\,M parameters) to
lightweight residual CNN students. Three student configurations were evaluated:
Compact (1.3\,M parameters), Standard (2.4\,M parameters), and Enhanced (3.8\,M
parameters). The dataset used in this study consists of images of \textit{Rosa
damascena} (Damask rose) collected from agricultural fields in the Dades Oasis,
southeastern Morocco, providing a realistic benchmark for agricultural computer
vision applications under diverse environmental conditions. Experimental
evaluation on the Damascena rose maturity classification dataset demonstrated
significant improvements over direct training methods. All student models
achieved validation accuracies exceeding 96.7\% with ATMS-KD compared to
95--96\% with direct training. The framework outperformed eleven established
knowledge distillation methods, achieving 97.11\% accuracy with the compact
model -- a 1.60 percentage point improvement over the second-best approach
while maintaining the lowest inference latency of 72.19\,ms. Knowledge
retention rates exceeded 99\% for all configurations, demonstrating effective
knowledge transfer regardless of student model capacity.

</details>


### [41] [Linking heterogeneous microstructure informatics with expert characterization knowledge through customized and hybrid vision-language representations for industrial qualification](https://arxiv.org/abs/2508.20243)
*Mutahar Safdar,Gentry Wood,Max Zimmermann,Guy Lamouche,Priti Wanjara,Yaoyao Fiona Zhao*

Main category: cs.CV

> 本文提出了一种新的混合视觉语言框架，它可以有效地对增材制造的材料进行微结构表征，通过与专家知识的无缝结合，促进材料质量资格的可追溯性和解释性，增加工程信息学中的可扩展性和领域适应性。

<details>
  <summary>Details</summary>

**Motivation:** 先进的材料快速且可靠的质量资格认定仍然是工业制造中的瓶颈，特别是在使用非传统的增材制造过程生产异质结构时。

**Method:** 本研究提出了一种新的框架，该框架将微结构信息学与定制和混合视觉语言表示（VLRs）的专家表征知识相联系。通过将深度语义分割与预训练的多模态模型（CLIP和FLAVA）相结合，我们能够将视觉微结构数据和文本专家评估编码为共享表示。为了克服通用嵌入的限制，我们开发了一种定制的基于相似性的表征，该表征结合了专家标注图像及其相关文本描述的正负参考。这使我们能够通过网络相似性评分方法对先前未见过的微结构进行零样本分类。

**Result:** 在增材制造的金属基复合材料数据集上的验证表明，该框架能够根据各种表征标准区分合格和不合格样本。比较分析表明，FLAVA模型提供了更高的视觉敏感性，而CLIP模型则确保了与文本标准的一致性。Z-score标准化基于局部数据集驱动的分布调整了原始单模态和跨模态相似性得分，这在混合视觉-语言框架中使得对齐和分类更为有效。

**Conclusion:** 本研究的工作通过推动原始数据和专家知识之间的语义互操作性，增强了可追溯性和解释性，为工程信息学提供了一种可扩展和领域适应性强的资格策略。

**Abstract:** Rapid and reliable qualification of advanced materials remains a bottleneck
in industrial manufacturing, particularly for heterogeneous structures produced
via non-conventional additive manufacturing processes. This study introduces a
novel framework that links microstructure informatics with a range of expert
characterization knowledge using customized and hybrid vision-language
representations (VLRs). By integrating deep semantic segmentation with
pre-trained multi-modal models (CLIP and FLAVA), we encode both visual
microstructural data and textual expert assessments into shared
representations. To overcome limitations in general-purpose embeddings, we
develop a customized similarity-based representation that incorporates both
positive and negative references from expert-annotated images and their
associated textual descriptions. This allows zero-shot classification of
previously unseen microstructures through a net similarity scoring approach.
Validation on an additively manufactured metal matrix composite dataset
demonstrates the framework's ability to distinguish between acceptable and
defective samples across a range of characterization criteria. Comparative
analysis reveals that FLAVA model offers higher visual sensitivity, while the
CLIP model provides consistent alignment with the textual criteria. Z-score
normalization adjusts raw unimodal and cross-modal similarity scores based on
their local dataset-driven distributions, enabling more effective alignment and
classification in the hybrid vision-language framework. The proposed method
enhances traceability and interpretability in qualification pipelines by
enabling human-in-the-loop decision-making without task-specific model
retraining. By advancing semantic interoperability between raw data and expert
knowledge, this work contributes toward scalable and domain-adaptable
qualification strategies in engineering informatics.

</details>


### [42] [MedNet-PVS: A MedNeXt-Based Deep Learning Model for Automated Segmentation of Perivascular Spaces](https://arxiv.org/abs/2508.20256)
*Zhen Xuen Brandon Low,Rory Zhang,Hang Min,William Pham,Lucy Vivash,Jasmine Moses,Miranda Lynch,Karina Dorfman,Cassandra Marotta,Shaun Koh,Jacob Bunyamin,Ella Rowsthorn,Alex Jarema,Himashi Peiris,Zhaolin Chen,Sandy R. Shultz,David K. Wright,Dexiao Kong,Sharon L. Naismith,Terence J. O'Brien,Ying Xia,Meng Law,Benjamin Sinclair*

Main category: cs.CV

> 使用MedNeXt-L-k5为自动分割脑内扩大的周围血管空间（PVS）提供了一种有效的方法，尤其在T2加权MRI数据集上显示出比手动方法还高的精度，但在T1加权MRI上的性能有所下降。

<details>
  <summary>Details</summary>

**Motivation:** 寻求一个高效的方法用于自动分割脑部的扩大的周围血管空间（PVS），该过程手动方法费时且存在评分者间可靠性较低的问题，并且现有的自动化模型性能中等，且通常无法在多种临床和研究的MRI数据集中泛化。

**Method:** 使用了MedNeXt-L-k5模型，这是一种受Transformer启发的3D编码-解码卷积网络，用于自动分割扩大的周围血管空间(PVS)。两个模型分别使用来自Human Connectome Project-Aging (HCP-Aging)数据集的200个T2加权(MRI扫描)和七个研究中的六个扫描仪横跨40个非同质T1加权(MRI卷)进行训练。

**Result:** 在HCP-Aging T2w数据集上训练的MedNeXt-L-k5模型实现了类似于报道的评分者间可靠性，并且是文献中迄今报告的最高记录（在白质区域达到了0.88+/-0.06的高Dice系数）。而在HCP-Aging T1w数据集上训练的模型Dice分数低得多，为0.58+/-0.09（在白质）。LOSOCV条件下，模型的体素级Dice分数为0.38+/-0.16（白质）和0.35+/-0.12（背景），聚类级 Dice 分数为0.61+/-0.19（白质）和0.62+/-0.21（背景）。

**Conclusion:** MedNeXt-L-k5 提供了一个在多种T1w和T2w MRI数据集中高效自动分割扩大的周围血管空间（PVS）的解决方案。不过，MedNeXt-L-k5并没有超越nnU-Net，这表明在PVS分割中，尽管基于注意机制的全球环境可以在transformer启发的模型中提供，但对于高精度来说可能是不必要的。

**Abstract:** Enlarged perivascular spaces (PVS) are increasingly recognized as biomarkers
of cerebral small vessel disease, Alzheimer's disease, stroke, and
aging-related neurodegeneration. However, manual segmentation of PVS is
time-consuming and subject to moderate inter-rater reliability, while existing
automated deep learning models have moderate performance and typically fail to
generalize across diverse clinical and research MRI datasets. We adapted
MedNeXt-L-k5, a Transformer-inspired 3D encoder-decoder convolutional network,
for automated PVS segmentation. Two models were trained: one using a
homogeneous dataset of 200 T2-weighted (T2w) MRI scans from the Human
Connectome Project-Aging (HCP-Aging) dataset and another using 40 heterogeneous
T1-weighted (T1w) MRI volumes from seven studies across six scanners. Model
performance was evaluated using internal 5-fold cross validation (5FCV) and
leave-one-site-out cross validation (LOSOCV). MedNeXt-L-k5 models trained on
the T2w images of the HCP-Aging dataset achieved voxel-level Dice scores of
0.88+/-0.06 (white matter, WM), comparable to the reported inter-rater
reliability of that dataset, and the highest yet reported in the literature.
The same models trained on the T1w images of the HCP-Aging dataset achieved a
substantially lower Dice score of 0.58+/-0.09 (WM). Under LOSOCV, the model had
voxel-level Dice scores of 0.38+/-0.16 (WM) and 0.35+/-0.12 (BG), and
cluster-level Dice scores of 0.61+/-0.19 (WM) and 0.62+/-0.21 (BG).
MedNeXt-L-k5 provides an efficient solution for automated PVS segmentation
across diverse T1w and T2w MRI datasets. MedNeXt-L-k5 did not outperform the
nnU-Net, indicating that the attention-based mechanisms present in
transformer-inspired models to provide global context are not required for high
accuracy in PVS segmentation.

</details>


### [43] [Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation](https://arxiv.org/abs/2508.20265)
*Zhixiang Chi,Yanan Wu,Li Gu,Huan Liu,Ziqiang Wang,Yang Zhang,Yang Wang,Konstantinos N. Plataniotis*

Main category: cs.CV

> A feedback-driven framework is proposed to enhance CLIP's localization and semantic consistency by adapting output-based patch-level correspondences to intermediate attention, resulting in improved performance in open-vocabulary segmentation tasks.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitation of CLIP in open-vocabulary segmentation due to poor localization and lack of interaction between intermediate attention and text representations.

**Method:** Our approach involves a training-free, feedback-driven framework that enhances semantic consistency by adapting output-based patch-level correspondences back to intermediate attention. We design key modules such as attention isolation, confidence-based pruning, and adaptation ensemble to effectively integrate output coherence cues.

**Result:** The proposed framework is validated across multiple attention types and integrates seamlessly into four state-of-the-art approaches with three different backbones. It consistently improves performance across eight benchmarks.

**Conclusion:** This work introduces a novel framework that leverages output coherence to improve the localization and semantic consistency of CLIP, enhancing its performance in open-vocabulary segmentation tasks.

**Abstract:** CLIP exhibits strong visual-textual alignment but struggle with
open-vocabulary segmentation due to poor localization. Prior methods enhance
spatial coherence by modifying intermediate attention. But, this coherence
isn't consistently propagated to the final output due to subsequent operations
such as projections. Additionally, intermediate attention lacks direct
interaction with text representations, such semantic discrepancy limits the
full potential of CLIP.
  In this work, we propose a training-free, feedback-driven self-adaptive
framework that adapts output-based patch-level correspondences back to the
intermediate attention. The output predictions, being the culmination of the
model's processing, encapsulate the most comprehensive visual and textual
semantics about each patch. Our approach enhances semantic consistency between
internal representations and final predictions by leveraging the model's
outputs as a stronger spatial coherence prior. We design key modules, including
attention isolation, confidence-based pruning for sparse adaptation, and
adaptation ensemble, to effectively feedback the output coherence cues. Our
method functions as a plug-in module, seamlessly integrating into four
state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H). We
further validate our framework across multiple attention types (Q-K, self-self,
and Proxy augmented with MAE, SAM, and DINO). Our approach consistently
improves their performance across eight benchmarks.

</details>


### [44] [How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding](https://arxiv.org/abs/2508.20279)
*Zhuoran Yu,Yong Jae Lee*

Main category: cs.CV

> 该研究提供了一个探针框架，用于分析多模态大型语言模型（MLLMs）的内部处理动态，揭示了早期层执行视觉定位、中层支持词汇整合和语义推理、最终层准备输出的阶段性结构，并发现这种结构在不同模型中显示出一致性的存在。

<details>
  <summary>Details</summary>

**Motivation:** 尽管多模态大型语言模型在视觉-语言任务上表现出色，但其内部处理过程仍少有研究。因此，通过引入探针框架，系统地分析模型如何跨层处理视觉和文本信息，有助于深入理解这些模型的工作机制，提供关于多模态表示动态的轻量级、模型无关分析方法。

**Method:** 引入了一个探针框架，系统地分析多模态大型语言模型（MLLMs）如何跨层处理视觉和文本输入。训练线性分类器来预测每个层提取的标记嵌入中的细粒度视觉类别（如狗的品种），使用标准化的锚问题。为了揭示不同层的功能角色，评估这些探针在三种类型的受控提示变化下表现：(1) 词汇变体，测试对表面级别变化的敏感度；(2) 语义否定变体，通过在提示中修改视觉概念来反转预期答案；(3) 输出格式变体，保持推理不变但改变答案格式。

**Result:** 通过对LLaVA-1.5、LLaVA-Next-LLaMA-3和Qwen2-VL应用该框架，发现了一致的阶段性结构，在这种结构中，早期层执行视觉定位，中层支持词汇整合和语义推理，最终层准备任务特定输出。进一步表明，虽然总体阶段结构在视觉标记化、指令微调数据和预训练语料库的变化下保持稳定，但现在阶段特定层在基本LLM架构变化时显著移动。

**Conclusion:** 研究结果提供了一种关于多模态大型语言模型层结构的统一视角，并提供了一种用于分析多模态表示动态的轻量级模型无关方法。虽然总体阶段结构在某些变化下保持稳定，但阶段特定层分配在基本LLM架构变化时有显著移动。

**Abstract:** Multimodal Large Language Models (MLLMs) have demonstrated strong performance
across a wide range of vision-language tasks, yet their internal processing
dynamics remain underexplored. In this work, we introduce a probing framework
to systematically analyze how MLLMs process visual and textual inputs across
layers. We train linear classifiers to predict fine-grained visual categories
(e.g., dog breeds) from token embeddings extracted at each layer, using a
standardized anchor question. To uncover the functional roles of different
layers, we evaluate these probes under three types of controlled prompt
variations: (1) lexical variants that test sensitivity to surface-level
changes, (2) semantic negation variants that flip the expected answer by
modifying the visual concept in the prompt, and (3) output format variants that
preserve reasoning but alter the answer format. Applying our framework to
LLaVA-1.5, LLaVA-Next-LLaMA-3, and Qwen2-VL, we identify a consistent
stage-wise structure in which early layers perform visual grounding, middle
layers support lexical integration and semantic reasoning, and final layers
prepare task-specific outputs. We further show that while the overall
stage-wise structure remains stable across variations in visual tokenization,
instruction tuning data, and pretraining corpus, the specific layer allocation
to each stage shifts notably with changes in the base LLM architecture. Our
findings provide a unified perspective on the layer-wise organization of MLLMs
and offer a lightweight, model-agnostic approach for analyzing multimodal
representation dynamics.

</details>


### [45] [Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)](https://arxiv.org/abs/2508.20322)
*Zhi Li,Hau Phan,Matthew Emigh,Austin J. Brockmeier*

Main category: cs.CV

> 本文提出一种监督字典学习方法，通过稀疏非负组合字典原子来分解嵌入空间，这种方法被称为稀疏线性概念子空间（SLiCS），应用于SLiCS的图像检索能提高精度。

<details>
  <summary>Details</summary>

**Motivation:** 我们假设嵌入空间可以分解，以分离复杂场景的内容信息。这种方法能够提供更精确的概念过滤图像检索。

**Method:** 我们提出了一种监督字典学习方法，通过稀疏、非负的字典原子组合来估计合成模型。这种方法可以分解嵌入空间，将其拆分为多个与特定概念相关的子空间组件向量。

**Result:** 实验结果显示，通过我们提出的方法---稀疏线性概念子空间（SLiCS）得到的分离嵌入，能够在所有嵌入中提高概念过滤图像检索的精度。

**Conclusion:** 该研究表明，利用SLiCS方法分离后的嵌入，能在概念过滤图像检索中提供更加精确的结果。

**Abstract:** Vision-language co-embedding networks, such as CLIP, provide a latent
embedding space with semantic information that is useful for downstream tasks.
We hypothesize that the embedding space can be disentangled to separate the
information on the content of complex scenes by decomposing the embedding into
multiple concept-specific component vectors that lie in different subspaces. We
propose a supervised dictionary learning approach to estimate a linear
synthesis model consisting of sparse, non-negative combinations of groups of
vectors in the dictionary (atoms), whose group-wise activity matches the
multi-label information. Each concept-specific component is a non-negative
combination of atoms associated to a label. The group-structured dictionary is
optimized through a novel alternating optimization with guaranteed convergence.
Exploiting the text co-embeddings, we detail how semantically meaningful
descriptions can be found based on text embeddings of words best approximated
by a concept's group of atoms, and unsupervised dictionary learning can exploit
zero-shot classification of training set images using the text embeddings of
concept labels to provide instance-wise multi-labels. We show that the
disentangled embeddings provided by our sparse linear concept subspaces (SLiCS)
enable concept-filtered image retrieval (and conditional generation using
image-to-prompt) that is more precise. We also apply SLiCS to highly-compressed
autoencoder embeddings from TiTok and the latent embedding from self-supervised
DINOv2. Quantitative and qualitative results highlight the improved precision
of the concept-filtered image retrieval for all embeddings.

</details>


### [46] [MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models](https://arxiv.org/abs/2508.20345)
*Xiao Li,Yanfan Zhu,Ruining Deng,Wei-Qi Wei,Yu Wang,Shilin Zhao,Yaohong Wang,Haichun Yang,Yuankai Huo*

Main category: cs.CV

> 这篇论文介绍了一个名为MedFoundationHub的工具包，它使得医生无需编程技能即可使用医疗VLM，并支持工程师高效部署这些模型。通过评估五种模型的性能，发现它们在临床应用中存在一些限制。

<details>
  <summary>Details</summary>

**Motivation:** 开发MedFoundationHub的动机在于解决医疗视觉语言模型在临床应用中可能带来的安全和隐私风险，同时提高其在学术研究实验室中的可访问性和安全性。

**Method:** 这是通过开发一个名为MedFoundationHub的图形用户界面工具包来解决安全和隐私问题的方法，该工具包允许医生在不需要编程专业知识的情况下手动选择和使用不同的模型，并支持工程师高效部署医疗视觉语言模型（VLM），并通过Docker容器进行隐私保护推理。

**Result:** 结果展示了MedFoundationHub可以在本地工作站在单个NVIDIA A6000显卡的支持下运行，通过评估由五种顶尖模型生成的1015个临床评分事件，发现这些模型在特定病例中存在一定的局限性。

**Conclusion:** 结论是MedFoundationHub提供了保护隐私的推理能力，同时使得医生和工程师能够有效地使用和部署医疗视觉语言模型，但它也揭示了模型仍存在一些限制，如偏离目标答案、模糊推理和病理术语不一致等问题。

**Abstract:** Recent advances in medical vision-language models (VLMs) open up remarkable
opportunities for clinical applications such as automated report generation,
copilots for physicians, and uncertainty quantification. However, despite their
promise, medical VLMs introduce serious security concerns, most notably risks
of Protected Health Information (PHI) exposure, data leakage, and vulnerability
to cyberthreats - which are especially critical in hospital environments. Even
when adopted for research or non-clinical purposes, healthcare organizations
must exercise caution and implement safeguards. To address these challenges, we
present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)
enables physicians to manually select and use different models without
programming expertise, (2) supports engineers in efficiently deploying medical
VLMs in a plug-and-play fashion, with seamless integration of Hugging Face
open-source models, and (3) ensures privacy-preserving inference through
Docker-orchestrated, operating system agnostic deployment. MedFoundationHub
requires only an offline local workstation equipped with a single NVIDIA A6000
GPU, making it both secure and accessible within the typical resources of
academic research labs. To evaluate current capabilities, we engaged
board-certified pathologists to deploy and assess five state-of-the-art VLMs
(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and
LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,
yielding 1015 clinician-model scoring events. These assessments revealed
recurring limitations, including off-target answers, vague reasoning, and
inconsistent pathology terminology.

</details>


### [47] [Enhancing Mamba Decoder with Bidirectional Interaction in Multi-Task Dense Prediction](https://arxiv.org/abs/2508.20376)
*Mang Cao,Sanping Zhou,Yizhe Li,Ye Deng,Wenli Huang,Le Wang*

Main category: cs.CV

> The paper presents BIM, which uses BI-Scan and MS-Scan to improve multi-task dense prediction while balancing interaction completeness and computational efficiency.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to achieve sufficient interaction between tasks in multi-task dense prediction without sacrificing computational efficiency.

**Method:** This paper introduces Bidirectional Interaction Mamba (BIM) for multi-task dense prediction. It features two mechanisms: Bidirectional Interaction Scan (BI-Scan) for task-specific representation construction and Multi-Scale Scan (MS-Scan) for multi-granularity scene modeling.

**Result:** Experiments on the NYUD-V2 and PASCAL-Context benchmarks demonstrate that BIM outperforms state-of-the-art methods.

**Conclusion:** BIM effectively addresses the trade-off between interaction completeness and computational efficiency in multi-task dense prediction, offering superior performance.

**Abstract:** Sufficient cross-task interaction is crucial for success in multi-task dense
prediction. However, sufficient interaction often results in high computational
complexity, forcing existing methods to face the trade-off between interaction
completeness and computational efficiency. To address this limitation, this
work proposes a Bidirectional Interaction Mamba (BIM), which incorporates novel
scanning mechanisms to adapt the Mamba modeling approach for multi-task dense
prediction. On the one hand, we introduce a novel Bidirectional Interaction
Scan (BI-Scan) mechanism, which constructs task-specific representations as
bidirectional sequences during interaction. By integrating task-first and
position-first scanning modes within a unified linear complexity architecture,
BI-Scan efficiently preserves critical cross-task information. On the other
hand, we employ a Multi-Scale Scan~(MS-Scan) mechanism to achieve
multi-granularity scene modeling. This design not only meets the diverse
granularity requirements of various tasks but also enhances nuanced cross-task
feature interactions. Extensive experiments on two challenging benchmarks,
\emph{i.e.}, NYUD-V2 and PASCAL-Context, show the superiority of our BIM vs its
state-of-the-art competitors.

</details>


### [48] [Audio-Guided Visual Editing with Complex Multi-Modal Prompts](https://arxiv.org/abs/2508.20379)
*Hyeonyu Kim,Seokhoon Jeong,Seonghee Han,Chanhyuk Choi,Taehwan Kim*

Main category: cs.CV

> 本文提出了一种新的音频引导视觉编辑框架，解决了复杂视觉编辑任务的挑战，成功地将多种音频和文本提示融合到编辑任务中，无需额外训练，并展示了它在处理复杂场景时的优越性。

<details>
  <summary>Details</summary>

**Motivation:** 我们注意到，视觉编辑中的扩散模型在复杂场景下，仅靠文本提示无法充分描述需求，因此需要额外的非文本编辑提示。同时，现有的音频引导视觉编辑方法往往需要特定数据集上的训练来对齐音频与文本，这限制了它们在现实世界中的泛化能力。

**Method:** 我们的方法提出了一种基于音频引导的视觉编辑框架，无需额外训练即可处理复杂编辑任务，并结合了多种文本和音频提示。通过缓解音频编码器空间与扩散模型提示编码器空间之间的差异，我们集成了多样性的音频到视觉编辑任务中。另外，我们提出了一种新颖的方法来处理复杂场景，通过分离的噪声分支和自适应的补丁选择处理多模态编辑提示。

**Result:** 我们通过多样化的编辑任务实验，证明我们的框架在复杂编辑场景中表现优异，成功利用了丰富的音频信息，而相比之下纯文本方法则无法胜任。

**Conclusion:** 总之，我们的音频引导视觉编辑框架通过整合音频信息，显著提升了视觉编辑模型在复杂场景下的性能，证明了其在现实世界中应用的潜力。

**Abstract:** Visual editing with diffusion models has made significant progress but often
struggles with complex scenarios that textual guidance alone could not
adequately describe, highlighting the need for additional non-text editing
prompts. In this work, we introduce a novel audio-guided visual editing
framework that can handle complex editing tasks with multiple text and audio
prompts without requiring additional training. Existing audio-guided visual
editing methods often necessitate training on specific datasets to align audio
with text, limiting their generalization to real-world situations. We leverage
a pre-trained multi-modal encoder with strong zero-shot capabilities and
integrate diverse audio into visual editing tasks, by alleviating the
discrepancy between the audio encoder space and the diffusion model's prompt
encoder space. Additionally, we propose a novel approach to handle complex
scenarios with multiple and multi-modal editing prompts through our separate
noise branching and adaptive patch selection. Our comprehensive experiments on
diverse editing tasks demonstrate that our framework excels in handling
complicated editing scenarios by incorporating rich information from audio,
where text-only approaches fail.

</details>


### [49] [More Reliable Pseudo-labels, Better Performance: A Generalized Approach to Single Positive Multi-label Learning](https://arxiv.org/abs/2508.20381)
*Luong Tran,Thieu Vo,Anh Nguyen,Sang Dinh,Van Nguyen*

Main category: cs.CV

> 本文提出了一个新的损失函数GPR Loss和DAMP技术，共同形成了AEVLP框架，用于解决从部分注释数据中进行多标签学习的问题，并在多个基准数据集上取得了先进结果。

<details>
  <summary>Details</summary>

**Motivation:** 由于大规模数据集的完全注释代价高昂且耗费时间和人力，我们研究了如何从部分注释的数据中学习。特别是在单正多标签学习（SPML）极端情况下，每张图像只有一个正标签。传统方法容易产生不准确性和误报，伪标签策略也会引入额外噪音。

**Method:** 我们提出了一个名为GPR Loss的新损失函数，可以有效利用各种伪标签同时减少噪声。此外，我们还介绍了一种简单而有效的动态增强多焦点伪标签技术（DAMP）。这些贡献共同形成了一个自适应和高效的视觉语言伪标签框架（AEVLP）。

**Result:** 在四个基准数据集上的广泛实验表明，我们的框架显著提升了多标签分类效果，并达到了最先进的结果。

**Conclusion:** 我们研究的AEVLP框架在从部分注释数据进行多标签分类任务上，提高了准确率，减少了噪声影响，展示了优越的性能。

**Abstract:** Multi-label learning is a challenging computer vision task that requires
assigning multiple categories to each image. However, fully annotating
large-scale datasets is often impractical due to high costs and effort,
motivating the study of learning from partially annotated data. In the extreme
case of Single Positive Multi-Label Learning (SPML), each image is provided
with only one positive label, while all other labels remain unannotated.
Traditional SPML methods that treat missing labels as unknown or negative tend
to yield inaccuracies and false negatives, and integrating various
pseudo-labeling strategies can introduce additional noise. To address these
challenges, we propose the Generalized Pseudo-Label Robust Loss (GPR Loss), a
novel loss function that effectively learns from diverse pseudo-labels while
mitigating noise. Complementing this, we introduce a simple yet effective
Dynamic Augmented Multi-focus Pseudo-labeling (DAMP) technique. Together, these
contributions form the Adaptive and Efficient Vision-Language Pseudo-Labeling
(AEVLP) framework. Extensive experiments on four benchmark datasets demonstrate
that our framework significantly advances multi-label classification, achieving
state-of-the-art results.

</details>


### [50] [Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection](https://arxiv.org/abs/2508.20392)
*Chengjun Zhang,Yuhao Zhang,Jie Yang,Mohamad Sawan*

Main category: cs.CV

> A novel delay-spike approach and a temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for Spiking Neural Networks (SNNs) are proposed to improve visual detection performance by mitigating residual membrane potential and enabling more precise feature representation at ultra-low latency.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the underperformance of current ANN-SNN conversion methods in visual detection tasks by proposing a new approach and neuron architecture that can handle temporal properties of spikes better.

**Method:** The paper proposes a delay-spike approach to alleviate the problem of residual membrane potential due to heterogeneous spiking patterns and introduces a temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs that adjusts accumulation and firing based on the temporal sequence of steps.

**Result:** The tdIF method surpasses current ANN-SNN conversion approaches with state-of-the-art performance and ultra-low latency (within 5 time-steps) in visual detection tasks, demonstrated through extensive evaluations on object detection and lane line detection tasks.

**Conclusion:** The novel delay-spike approach and tdIF neuron architecture achieve higher performance and lower latency in SNN applications to visual detection tasks, while maintaining energy efficiency comparable to traditional IF neuron models.

**Abstract:** Spiking Neural Networks (SNNs), inspired by the brain, are characterized by
minimal power consumption and swift inference capabilities on neuromorphic
hardware, and have been widely applied to various visual perception tasks.
Current ANN-SNN conversion methods have achieved excellent results in
classification tasks with ultra-low time-steps, but their performance in visual
detection tasks remains suboptimal. In this paper, we propose a delay-spike
approach to mitigate the issue of residual membrane potential caused by
heterogeneous spiking patterns. Furthermore, we propose a novel
temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This
enables Integrate-and-fire (IF) neurons to dynamically adjust their
accumulation and firing behaviors based on the temporal order of time-steps.
Our method enables spikes to exhibit distinct temporal properties, rather than
relying solely on frequency-based representations. Moreover, the tdIF neuron
maintains energy consumption on par with traditional IF neuron. We demonstrate
that our method achieves more precise feature representation with lower
time-steps, enabling high performance and ultra-low latency in visual detection
tasks. In this study, we conduct extensive evaluation of the tdIF method across
two critical vision tasks: object detection and lane line detection. The
results demonstrate that the proposed method surpasses current ANN-SNN
conversion approaches, achieving state-of-the-art performance with ultra-low
latency (within 5 time-steps).

</details>


### [51] [Graph-Based Uncertainty Modeling and Multimodal Fusion for Salient Object Detection](https://arxiv.org/abs/2508.20415)
*Yuqi Xiong,Wuzhen Shi,Yang Wen,Ruhan Liu*

Main category: cs.CV

> 本文提出了动态不确定性传播和多模态协作推理网络（DUP-MCRNet），解决了现有显著性目标检测方法在复杂场景下容易丢失细节、模糊边缘和单模态信息融合不足的问题。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有显著性目标检测方法在复杂场景下容易丢失结构细节、模糊边缘和单模态信息融合不足，本文旨在提高复杂场景下的检测精度。

**Method:** 本文设计了动态不确定性图卷积模块(DUGC)和多模态协作融合策略(MCF)，结合空间语义距离构建的稀疏图，以及RGB、深度和边缘特征的注意图，来提高小结构和边缘区域的检测精度。

**Result:** 实验结果表明DUP-MCRNet在多个公共基准数据集上超越了现有的SOD方法，并且在边缘清晰度和复杂背景下鲁棒性方面表现尤为突出。

**Conclusion:** 该研究证明了动态不确定性传播和多模态协作推理网络DUP-MCRNet在复杂场景下的显著目标检测中具有更好的效果。

**Abstract:** In view of the problems that existing salient object detection (SOD) methods
are prone to losing details, blurring edges, and insufficient fusion of
single-modal information in complex scenes, this paper proposes a dynamic
uncertainty propagation and multimodal collaborative reasoning network
(DUP-MCRNet). Firstly, a dynamic uncertainty graph convolution module (DUGC) is
designed to propagate uncertainty between layers through a sparse graph
constructed based on spatial semantic distance, and combined with channel
adaptive interaction, it effectively improves the detection accuracy of small
structures and edge regions. Secondly, a multimodal collaborative fusion
strategy (MCF) is proposed, which uses learnable modality gating weights to
weightedly fuse the attention maps of RGB, depth, and edge features. It can
dynamically adjust the importance of each modality according to different
scenes, effectively suppress redundant or interfering information, and
strengthen the semantic complementarity and consistency between
cross-modalities, thereby improving the ability to identify salient regions
under occlusion, weak texture or background interference. Finally, the
detection performance at the pixel level and region level is optimized through
multi-scale BCE and IoU loss, cross-scale consistency constraints, and
uncertainty-guided supervision mechanisms. Extensive experiments show that
DUP-MCRNet outperforms various SOD methods on most common benchmark datasets,
especially in terms of edge clarity and robustness to complex backgrounds. Our
code is publicly available at https://github.com/YukiBear426/DUP-MCRNet.

</details>


### [52] [MSMVD: Exploiting Multi-scale Image Features via Multi-scale BEV Features for Multi-view Pedestrian Detection](https://arxiv.org/abs/2508.20447)
*Taiga Yamane,Satoshi Suzuki,Ryo Masumura,Shota Orihashi,Tomohiro Tanaka,Mana Ihori,Naoki Makishima,Naotaka Kawata*

Main category: cs.CV

> 本研究提出了MSMVD方法，利用多尺度BEV特征来提高行人检测性能，显著优于现有方法的检测精度。

<details>
  <summary>Details</summary>

**Motivation:** 该论文针对现有方法在检测多视角图像中小尺度或大尺度行人时效果不佳的问题，提出了MSMVD方法。

**Method:** MSMVD方法通过多尺度图像特征的投影来生成多尺度BEV特征，解决了行人检测中小尺度或大尺度目标检测不一致的问题。

**Result:** 实验结果表明，与之前的最高成绩相比，MSMVD在GMVD数据集上将MODA值提高了4.5个点。

**Conclusion:** 多尺度图像特征通过多尺度BEV特征可以极大地提高行人检测性能。

**Abstract:** Multi-View Pedestrian Detection (MVPD) aims to detect pedestrians in the form
of a bird's eye view (BEV) from multi-view images. In MVPD, end-to-end
trainable deep learning methods have progressed greatly. However, they often
struggle to detect pedestrians with consistently small or large scales in views
or with vastly different scales between views. This is because they do not
exploit multi-scale image features to generate the BEV feature and detect
pedestrians. To overcome this problem, we propose a novel MVPD method, called
Multi-Scale Multi-View Detection (MSMVD). MSMVD generates multi-scale BEV
features by projecting multi-scale image features extracted from individual
views into the BEV space, scale-by-scale. Each of these BEV features inherits
the properties of its corresponding scale image features from multiple views.
Therefore, these BEV features help the precise detection of pedestrians with
consistently small or large scales in views. Then, MSMVD combines information
at different scales of multiple views by processing the multi-scale BEV
features using a feature pyramid network. This improves the detection of
pedestrians with vastly different scales between views. Extensive experiments
demonstrate that exploiting multi-scale image features via multi-scale BEV
features greatly improves the detection performance, and MSMVD outperforms the
previous highest MODA by $4.5$ points on the GMVD dataset.

</details>


### [53] [A Spatial-Frequency Aware Multi-Scale Fusion Network for Real-Time Deepfake Detection](https://arxiv.org/abs/2508.20449)
*Libo Lv,Tianyi Wang,Mengxiao Huang,Ruixia Liu,Yinglong Wang*

Main category: cs.CV

> 本文提出了一种轻量级的实时深度造假检测架构Spatial-Frequency Aware Multi-Scale Fusion Network (SFMFNet)，在多个基准数据集上实现了准确率和效率的良好平衡。

<details>
  <summary>Details</summary>

**Motivation:** 针对现有深度造假检测技术计算成本高的问题，提出了一个轻量且有效的解决方案，以实现实时应用。

**Method:** 我们提出了一种名为Spatial-Frequency Aware Multi-Scale Fusion Network (SFMFNet) 的轻量级深度造假检测架构，设计了结合空间纹理和频率特征的模块，并引入了选择性交叉注意力机制和改进的模糊池化结构。

**Result:** 实验结果显示，SFMFNet在多个基准数据集上实现了准确率和效率的良好平衡，具有较强的泛化能力和实际应用价值。

**Conclusion:** SFMFNet作为一个轻量级模型，通过其特有的结构设计能够在保证检测精度的同时降低了计算成本，适用于实时应用场景。

**Abstract:** With the rapid advancement of real-time deepfake generation techniques,
forged content is becoming increasingly realistic and widespread across
applications like video conferencing and social media. Although
state-of-the-art detectors achieve high accuracy on standard benchmarks, their
heavy computational cost hinders real-time deployment in practical
applications. To address this, we propose the Spatial-Frequency Aware
Multi-Scale Fusion Network (SFMFNet), a lightweight yet effective architecture
for real-time deepfake detection. We design a spatial-frequency hybrid aware
module that jointly leverages spatial textures and frequency artifacts through
a gated mechanism, enhancing sensitivity to subtle manipulations. A
token-selective cross attention mechanism enables efficient multi-level feature
interaction, while a residual-enhanced blur pooling structure helps retain key
semantic cues during downsampling. Experiments on several benchmark datasets
show that SFMFNet achieves a favorable balance between accuracy and efficiency,
with strong generalization and practical value for real-time applications.

</details>


### [54] [Dual-Model Weight Selection and Self-Knowledge Distillation for Medical Image Classification](https://arxiv.org/abs/2508.20461)
*Ayaka Tsutsumi,Guang Li,Ren Togo,Takahiro Ogawa,Satoshi Kondo,Miki Haseyama*

Main category: cs.CV

> A novel method for medical image classification integrating dual-model weight selection and self-knowledge distillation is proposed, improving the efficiency and performance of models suitable for real-world medical settings with resource limitations.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to overcome the resource constraints faced when deploying large-scale models in medical settings, thus making it practical by developing an efficient framework that can achieve comparable performance with large models while being lightweight.

**Method:** In this paper, a medical image classification method is proposed that combines dual-model weight selection with self-knowledge distillation (SKD). It starts by initializing two lightweight models with weights from a large pretrained model to enable knowledge transfer. SKD is then applied to these models, supporting a variety of weight configurations without increasing computational cost, followed by fine-tuning for the target classification task.

**Result:** Experiments on chest X-ray images, lung computed tomography scans, and brain MRI scans demonstrated the superior performance and robustness of the approach compared with existing methods.

**Conclusion:** The research concludes that the proposed method, which combines dual-model weight selection and self-knowledge distillation, effectively addresses the challenge of retaining critical information in compact models. The approach is shown to be more robust and performant compared to existing methods across various medical image datasets.

**Abstract:** We propose a novel medical image classification method that integrates
dual-model weight selection with self-knowledge distillation (SKD). In
real-world medical settings, deploying large-scale models is often limited by
computational resource constraints, which pose significant challenges for their
practical implementation. Thus, developing lightweight models that achieve
comparable performance to large-scale models while maintaining computational
efficiency is crucial. To address this, we employ a dual-model weight selection
strategy that initializes two lightweight models with weights derived from a
large pretrained model, enabling effective knowledge transfer. Next, SKD is
applied to these selected models, allowing the use of a broad range of initial
weight configurations without imposing additional excessive computational cost,
followed by fine-tuning for the target classification tasks. By combining
dual-model weight selection with self-knowledge distillation, our method
overcomes the limitations of conventional approaches, which often fail to
retain critical information in compact models. Extensive experiments on
publicly available datasets-chest X-ray images, lung computed tomography scans,
and brain magnetic resonance imaging scans-demonstrate the superior performance
and robustness of our approach compared to existing methods.

</details>


### [55] [Re-Densification Meets Cross-Scale Propagation: Real-Time Compression of LiDAR Point Clouds](https://arxiv.org/abs/2508.20466)
*Pengpeng Yu,Haoran Li,Dingquan Li,Runqing Jiang,Jing Wang,Liang Lin,Yulan Guo*

Main category: cs.CV

> We propose a method for enhancing the efficiency of LiDAR point cloud compression through the integration of two modules: the Geometry Re-Densification Module and the Cross-scale Feature Propagation Module. Our approach provides state-of-the-art compression ratios and real-time performance.

<details>
  <summary>Details</summary>

**Motivation:** High-precision LiDAR scans, despite their utility, can be cumbersome due to their large storage and transmission requirements. Existing methods, which encode the unordered points into hierarchical structures as a first step for predictive coding, struggle with the efficient context modeling of sparsely distributed geometric details. This limitation affects their compression performance and speed, motivating the introduction of a new method for generating compact features that supports more efficient predictive coding.

**Method:** Our method consists of two main modules for improving the efficiency of predictive coding on sparse LiDAR point clouds. The Geometry Re-Densification Module (GRM) re-densifies sparse encoded geometry, extracts features at a denser scale, and then re-sparsifies these features for coding. This avoids the computational cost of processing highly sparse features. The Cross-scale Feature Propagation Module (CFPM) uses occupancy cues from multiple resolutions to guide feature propagation across scales, reducing redundancy and enhancing feature representation for the GRM.

**Result:** Experiments on the KITTI dataset show that our method achieves state-of-the-art compression ratios and is capable of real-time performance. Specifically, both encoding and decoding speeds reach 26 FPS at a 12-bit quantization level.

**Conclusion:** The use of our proposed framework, combining the Geometry Re-Densification Module and the Cross-scale Feature Propagation Module, leads to a significant improvement in the efficiency of predicting and coding LiDAR point clouds, achieving both high compression ratios and real-time performance. The compact feature representation produced supports more efficient context modeling while accelerating the coding process.

**Abstract:** LiDAR point clouds are fundamental to various applications, yet
high-precision scans incur substantial storage and transmission overhead.
Existing methods typically convert unordered points into hierarchical octree or
voxel structures for dense-to-sparse predictive coding. However, the extreme
sparsity of geometric details hinders efficient context modeling, thereby
limiting their compression performance and speed. To address this challenge, we
propose to generate compact features for efficient predictive coding. Our
framework comprises two lightweight modules. First, the Geometry
Re-Densification Module re-densifies encoded sparse geometry, extracts features
at denser scale, and then re-sparsifies the features for predictive coding.
This module avoids costly computation on highly sparse details while
maintaining a lightweight prediction head. Second, the Cross-scale Feature
Propagation Module leverages occupancy cues from multiple resolution levels to
guide hierarchical feature propagation. This design facilitates information
sharing across scales, thereby reducing redundant feature extraction and
providing enriched features for the Geometry Re-Densification Module. By
integrating these two modules, our method yields a compact feature
representation that provides efficient context modeling and accelerates the
coding process. Experiments on the KITTI dataset demonstrate state-of-the-art
compression ratios and real-time performance, achieving 26 FPS for both
encoding and decoding at 12-bit quantization. Code is available at
https://github.com/pengpeng-yu/FastPCC.

</details>


### [56] [Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation](https://arxiv.org/abs/2508.20470)
*Xiaochuan Li,Guoguang Du,Runze Zhang,Liang Jin,Qi Jia,Lihua Lu,Zhenhua Guo,Yaqian Zhao,Haiyang Liu,Tianqi Wang,Changsheng Li,Xiaoli Gong,Rengang Li,Baoyu Fan*

Main category: cs.CV

> 本研究利用视频数据生成3D内容，创建了Droplet3D-4M数据集及Droplet3D模型，验证了方法的有效性并支持场景级扩展。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机源于现有的3D数据资源相对稀缺，无法满足大规模训练的需求。而视频作为替代的监督信号，能有效缓解由于3D数据不足造成的泛化瓶颈。

**Method:** 本研究提出了一种利用视频模态进行3D资产生成的方法。通过创建Droplet3D-4M这一大规模视频数据集，该数据集具有多视角注释，能够提供空间和语义上的先验知识。此外，研究还开发了Droplet3D生成模型，可以支持图像和密集文本输入。

**Result:** 实验结果证明了该方法的有效性，生成的3D内容在空间一致性及语义准确性上表现良好。而且，该方法相较于现有的3D解决方案，有潜力扩展到场景级应用。

**Conclusion:** 研究表明，视频所蕴含的常识先验知识在提升3D生成效果方面有着重要价值，并且开源了包括数据集、代码、技术框架和模型权重在内的所有资源，加强了研究的可重复性和扩展性。

**Abstract:** Scaling laws have validated the success and promise of large-data-trained
models in creative generation across text, image, and video domains. However,
this paradigm faces data scarcity in the 3D domain, as there is far less of it
available on the internet compared to the aforementioned modalities.
Fortunately, there exist adequate videos that inherently contain commonsense
priors, offering an alternative supervisory signal to mitigate the
generalization bottleneck caused by limited native 3D data. On the one hand,
videos capturing multiple views of an object or scene provide a spatial
consistency prior for 3D generation. On the other hand, the rich semantic
information contained within the videos enables the generated content to be
more faithful to the text prompts and semantically plausible. This paper
explores how to apply the video modality in 3D asset generation, spanning
datasets to models. We introduce Droplet3D-4M, the first large-scale video
dataset with multi-view level annotations, and train Droplet3D, a generative
model supporting both image and dense text input. Extensive experiments
validate the effectiveness of our approach, demonstrating its ability to
produce spatially consistent and semantically plausible content. Moreover, in
contrast to the prevailing 3D solutions, our approach exhibits the potential
for extension to scene-level applications. This indicates that the commonsense
priors from the videos significantly facilitate 3D creation. We have
open-sourced all resources including the dataset, code, technical framework,
and model weights: https://dropletx.github.io/.

</details>


### [57] [Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation](https://arxiv.org/abs/2508.20471)
*Jiusi Li,Jackson Jiang,Jinyu Miao,Miao Long,Tuopu Wen,Peijin Jia,Shengxiang Liu,Chunlei Yu,Maolin Liu,Yuzhan Cai,Kun Jiang,Mengmeng Yang,Diange Yang*

Main category: cs.CV

> G^2Editor是一种用于在驾驶视频中进行逼真的对象编辑框架，解决了当前方法存在的视觉保真度低和姿态控制不精确的问题。

<details>
  <summary>Details</summary>

**Motivation:** 目前产生的驾驶场景生成方法通常面临视觉保真度低或姿态控制不精确的问题。为了解决这些问题，该论文提出了G^2Editor。

**Method:** 我们的方法提出了一种名为G^2Editor的框架，用于在驾驶视频中进行逼真的对象编辑。该方法利用3D高斯表示作为密集先验，注入去噪过程中以确保精确的姿态控制和空间一致性。同时，采用场景级3D边界框布局来重建被遮挡区域，并在生成过程中加入分层细粒度特征作为额外条件来指导编辑对象的外观细节。

**Result:** 实验表明，G^2Editor支持在同一框架中进行对象的重新定位、插入和删除，比现有的方法在姿态可控性和视觉质量上表现更好，也提升了下游数据驱动任务的效果。

**Conclusion:** 该方法不仅可以进行逼真的对象编辑，而且对姿态有一定的控制能力，这对于自动驾驶系统来说具有相关价值。

**Abstract:** Corner cases are crucial for training and validating autonomous driving
systems, yet collecting them from the real world is often costly and hazardous.
Editing objects within captured sensor data offers an effective alternative for
generating diverse scenarios, commonly achieved through 3D Gaussian Splatting
or image generative models. However, these approaches often suffer from limited
visual fidelity or imprecise pose control. To address these issues, we propose
G^2Editor, a framework designed for photorealistic and precise object editing
in driving videos. Our method leverages a 3D Gaussian representation of the
edited object as a dense prior, injected into the denoising process to ensure
accurate pose control and spatial consistency. A scene-level 3D bounding box
layout is employed to reconstruct occluded areas of non-target objects.
Furthermore, to guide the appearance details of the edited object, we
incorporate hierarchical fine-grained features as additional conditions during
generation. Experiments on the Waymo Open Dataset demonstrate that G^2Editor
effectively supports object repositioning, insertion, and deletion within a
unified framework, outperforming existing methods in both pose controllability
and visual quality, while also benefiting downstream data-driven tasks.

</details>


### [58] [Enhancing Corpus Callosum Segmentation in Fetal MRI via Pathology-Informed Domain Randomization](https://arxiv.org/abs/2508.20475)
*Marina Grifell i Plana,Vladyslav Zalevskyi,Léa Schmidt,Yvan Gomez,Thomas Sanchez,Vincent Dunet,Mériam Koob,Vanessa Siffredi,Meritxell Bach Cuadra*

Main category: cs.CV

> The paper presents a domain randomization strategy for improving fetal brain segmentation in cases of corpus callosum dysgenesis (CCD), addressing data scarcity and achieving more accurate biomarker extraction.

<details>
  <summary>Details</summary>

**Motivation:** Accurate segmentation of fetal brains is critical for assessing neurodevelopment and diagnosing conditions such as CCD. However, scarce annotated data limit the effectiveness of deep learning models in these cases.

**Method:** A pathology-informed domain randomization approach is proposed to embed prior knowledge of CCD into a synthetic data generation process, enabling robust segmentation without pathological annotations.

**Result:** The method demonstrates improved segmentation performance on CCD cases, with reduced LCC estimation errors and enhanced topological consistency compared to ground truth data. It also performs well on healthy brains and other pathologies.

**Conclusion:** Incorporating anatomical priors into synthetic data generation effectively combats data scarcity and improves the analysis of rare clinical conditions like CCD, offering more reliable biomarkers and segmentations.

**Abstract:** Accurate fetal brain segmentation is crucial for extracting biomarkers and
assessing neurodevelopment, especially in conditions such as corpus callosum
dysgenesis (CCD), which can induce drastic anatomical changes. However, the
rarity of CCD severely limits annotated data, hindering the generalization of
deep learning models. To address this, we propose a pathology-informed domain
randomization strategy that embeds prior knowledge of CCD manifestations into a
synthetic data generation pipeline. By simulating diverse brain alterations
from healthy data alone, our approach enables robust segmentation without
requiring pathological annotations.
  We validate our method on a cohort comprising 248 healthy fetuses, 26 with
CCD, and 47 with other brain pathologies, achieving substantial improvements on
CCD cases while maintaining performance on both healthy fetuses and those with
other pathologies. From the predicted segmentations, we derive clinically
relevant biomarkers, such as corpus callosum length (LCC) and volume, and show
their utility in distinguishing CCD subtypes. Our pathology-informed
augmentation reduces the LCC estimation error from 1.89 mm to 0.80 mm in
healthy cases and from 10.9 mm to 0.7 mm in CCD cases. Beyond these
quantitative gains, our approach yields segmentations with improved topological
consistency relative to available ground truth, enabling more reliable
shape-based analyses. Overall, this work demonstrates that incorporating
domain-specific anatomical priors into synthetic data pipelines can effectively
mitigate data scarcity and enhance analysis of rare but clinically significant
malformations.

</details>


### [59] [Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding](https://arxiv.org/abs/2508.20476)
*Jeong Hun Yeo,Hyeongseop Rha,Sungjune Park,Junil Won,Yong Man Ro*

Main category: cs.CV

> 本文介绍了一个能够处理多种沟通方式（手语、唇读和音频）的统一框架，它旨在设计一个通用的架构来处理异构输入，并探索这些模态之间的协同作用，实现在多个任务上优于现有模型的性能。

<details>
  <summary>Details</summary>

**Motivation:** 尽管自动语音识别系统取得了成功，但这些系统对于聋哑人或听力受损的人来说仍然不够方便。这是我们尝试构建一个融合手语、唇读和音频等多种沟通方式的统一框架的主要动力。

**Method:** 介绍了一种能够处理包括手语、唇部动作和音频在内的多种多样的输入的统一框架。该框架旨在设计一个通用的、不依赖特定模态的架构，能够有效处理异构输入，探索模态之间的协同作用，特别是将唇部动作作为手语理解中的非手动线索，以及实现与单任务专用模型相匹敌或更优的性能。

**Result:** 基于该框架，我们在SLT、VSR、ASR和AVSR任务上达到了与或优于特定任务的最先进的模型的性能。此外，我们的分析表明，将唇部动作作为一个单独的模态进行显式建模能够显著提高手语翻译性能。

**Conclusion:** 研究表明，结合多种沟通方式并集成在一个统一的框架中，能够显著提升在语言文本生成任务上的性能。特别是，将唇部动作作为一个单独的模态进行处理，对于提高手语翻译性能有显著效果。

**Abstract:** Audio is the primary modality for human communication and has driven the
success of Automatic Speech Recognition (ASR) technologies. However, such
systems remain inherently inaccessible to individuals who are deaf or hard of
hearing. Visual alternatives such as sign language and lip reading offer
effective substitutes, and recent advances in Sign Language Translation (SLT)
and Visual Speech Recognition (VSR) have improved audio-less communication.
Yet, these modalities have largely been studied in isolation, and their
integration within a unified framework remains underexplored. In this paper, we
introduce the first unified framework capable of handling diverse combinations
of sign language, lip movements, and audio for spoken-language text generation.
We focus on three main objectives: (i) designing a unified, modality-agnostic
architecture capable of effectively processing heterogeneous inputs; (ii)
exploring the underexamined synergy among modalities, particularly the role of
lip movements as non-manual cues in sign language comprehension; and (iii)
achieving performance on par with or superior to state-of-the-art models
specialized for individual tasks. Building on this framework, we achieve
performance on par with or better than task-specific state-of-the-art models
across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that
explicitly modeling lip movements as a separate modality significantly improves
SLT performance.

</details>


### [60] [Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding](https://arxiv.org/abs/2508.20478)
*Yuan Xie,Tianshui Chen,Zheng Ge,Lionel Ni*

Main category: cs.CV

> 本文提出了一种新的视频理解方法Video-MTR，通过多轮的推理和逐步选择关键视频片段来提高长视频理解的准确性和效率。

<details>
  <summary>Details</summary>

**Motivation:** 现有的视频理解方法存在复杂性和性能次优的问题，尤其是在缺乏端到端训练的情况下。本论文旨在解决长视频理解的长时依赖性和多事件的问题。

**Method:** Video-MTR是一种强化的多轮推理框架，设计用于逐步选择关键视频片段并理解问题。与传统的单轮视频推理不同，Video-MTR在多个回合进行推理，基于对先前处理片段的理解和当前问题选择视频片段。为了确保中间推理过程，引入了一种新颖的门控双层奖励系统，结合基于答案正确性的轨迹级奖励和强调帧-查询相关性的回合级奖励。该系统优化了视频片段选择和问题理解，去除了对外部视觉语言模型的依赖，并允许端到端训练。

**Result:** 在VideoMME、MLVU和EgoSchema等基准测试上的广泛实验表明，Video-MTR在准确性和效率上优于现有方法，推进了长视频理解领域的最先进技术。

**Conclusion:** 引入的Video-MTR方法优化了视频片段选择和问题理解，推进了长视频理解领域的最先进技术，通过端到端的训练提高了准确性和效率。

**Abstract:** Long-form video understanding, characterized by long-range temporal
dependencies and multiple events, remains a challenge. Existing methods often
rely on static reasoning or external visual-language models (VLMs), which face
issues like complexity and sub-optimal performance due to the lack of
end-to-end training. In this paper, we propose Video-MTR, a reinforced
multi-turn reasoning framework designed to enable iterative key video segment
selection and question comprehension. Unlike traditional video reasoning
pipeline, which generate predictions in a single turn, Video-MTR performs
reasoning in multiple turns, selecting video segments progressively based on
the evolving understanding of previously processed segments and the current
question. This iterative process allows for a more refined and contextually
aware analysis of the video. To ensure intermediate reasoning process, we
introduce a novel gated bi-level reward system, combining trajectory-level
rewards based on answer correctness and turn-level rewards emphasizing
frame-query relevance. This system optimizes both video segment selection and
question comprehension, eliminating the need for external VLMs and allowing
end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU,
and EgoSchema demonstrate that Video-MTR outperforms existing methods in both
accuracy and efficiency, advancing the state-of-the-art in long video
understanding.

</details>


### [61] [Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts](https://arxiv.org/abs/2508.20488)
*Zixuan Hu,Dongxiao Li,Xinzhu Ma,Shixiang Tang,Xiaotong Li,Wenhan Yang,Ling-Yu Duan*

Main category: cs.CV

> 提出一种名为DUO的双不确定性优化框架以改进单目3D物体检测的可靠性，特别是在环境和传感器变化导致的领域差异情况下的可靠性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的测试时间自适应方法虽然认识到低不确定性与高泛化能力之间的正相关性，但并未解决单目3D物体检测中的双重不确定性问题，包括语义不确定性和几何不确定性。DUO旨在填补这一空白，提升单目3D物体检测的可靠性。

**Method:** 我们提出了Dual Uncertainty Optimization (DUO)框架，该框架首次针对单目3D物体检测中的语义不确定性和几何不确定性进行联合优化。我们通过凸优化的视角，引入了焦损函数的创新凸结构，并进一步推导出一种无监督版本，实现无标签的不确定性加权。此外，我们设计了一种语义感知的规范场约束，在具有明确语义线索的区域保持几何一致性，降低由于不稳定3D表示而引起的不确定性。

**Result:** 实验结果表明，与现有方法相比，DUO在各种数据集和不同类型的域转移上均表现出优越性。

**Conclusion:** DUO框架通过联合最小化语义不确定性和几何不确定性，实现在单目3D物体检测任务上的显著改善，尤其在存在域转移的情况下。该框架通过双分支机制形成互补循环，进一步增强了空间感知和语义分类性能。

**Abstract:** Accurate monocular 3D object detection (M3OD) is pivotal for safety-critical
applications like autonomous driving, yet its reliability deteriorates
significantly under real-world domain shifts caused by environmental or sensor
variations. To address these shifts, Test-Time Adaptation (TTA) methods have
emerged, enabling models to adapt to target distributions during inference.
While prior TTA approaches recognize the positive correlation between low
uncertainty and high generalization ability, they fail to address the dual
uncertainty inherent to M3OD: semantic uncertainty (ambiguous class
predictions) and geometric uncertainty (unstable spatial localization). To
bridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTA
framework designed to jointly minimize both uncertainties for robust M3OD.
Through a convex optimization lens, we introduce an innovative convex structure
of the focal loss and further derive a novel unsupervised version, enabling
label-agnostic uncertainty weighting and balanced learning for high-uncertainty
objects. In parallel, we design a semantic-aware normal field constraint that
preserves geometric coherence in regions with clear semantic cues, reducing
uncertainty from the unstable 3D representation. This dual-branch mechanism
forms a complementary loop: enhanced spatial perception improves semantic
classification, and robust semantic predictions further refine spatial
understanding. Extensive experiments demonstrate the superiority of DUO over
existing methods across various datasets and domain shift types.

</details>


### [62] [CaddieSet: A Golf Swing Dataset with Human Joint Features and Ball Information](https://arxiv.org/abs/2508.20491)
*Seunghyeon Jung,Seoyoung Hong,Jiwoo Jeong,Seungwon Jeong,Jaerim Choi,Hoki Kim,Woojin Lee*

Main category: cs.CV

> 论文提出CaddieSet数据集，通过计算机视觉方法提取挥杆视频关键帧的关节信息，用于分析高尔夫挥杆与球轨迹的关系。

<details>
  <summary>Details</summary>

**Motivation:** 尽管深度学习领域的进展促进了提高高尔夫球手击球精度的研究，但这些研究未能定量建立挥杆姿势与球轨迹之间的关系，限制了提供给高尔夫球手的挥杆改进的见解。

**Method:** 本研究提出了一种新的数据集CaddieSet，该数据集通过基于计算机视觉的方法将单个挥杆视频分割成八个挥杆阶段，提取了关节信息和各种球信息。此外，根据专家的高尔夫领域知识，定义了15个关键指标来影响高尔夫挥杆结果，这些指标可以解释挥杆结果。

**Result:** 通过实验，验证了CaddieSet在预测球轨迹基准测试中的可行性，特别是使用了可解释的模型来验证我们提取的关节特征与已建立的领域知识在定量上的相符性。

**Conclusion:** 本研究预期为高尔夫挥杆分析提供了新的见解，不仅对学术界，而且对体育行业也有重要贡献。

**Abstract:** Recent advances in deep learning have led to more studies to enhance golfers'
shot precision. However, these existing studies have not quantitatively
established the relationship between swing posture and ball trajectory,
limiting their ability to provide golfers with the necessary insights for swing
improvement. In this paper, we propose a new dataset called CaddieSet, which
includes joint information and various ball information from a single shot.
CaddieSet extracts joint information from a single swing video by segmenting it
into eight swing phases using a computer vision-based approach. Furthermore,
based on expert golf domain knowledge, we define 15 key metrics that influence
a golf swing, enabling the interpretation of swing outcomes through
swing-related features. Through experiments, we demonstrated the feasibility of
CaddieSet for predicting ball trajectories using various benchmarks. In
particular, we focus on interpretable models among several benchmarks and
verify that swing feedback using our joint features is quantitatively
consistent with established domain knowledge. This work is expected to offer
new insight into golf swing analysis for both academia and the sports industry.

</details>


### [63] [IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection](https://arxiv.org/abs/2508.20492)
*Xuanming Cao,Chengyu Tao,Yifeng Cheng,Juan Du*

Main category: cs.CV

> 提出新的表面异常检测方法IAENet，通过重要性感知融合模块结合2D与3D模型的优势，实现了更低的误报率和更高的检测精度。

<details>
  <summary>Details</summary>

**Motivation:** 尽管基于3D点云的检测方法在工业制造产品检测中具有丰富的几何信息优势，但因为缺乏类似2D图像中强大的预训练基础模型而发展不足。本研究旨在通过结合2D和3D模型的优势来解决这一问题。

**Method:** 提出了一种名为重要性感知集成网络（IAENet）的框架，该框架结合了2D预训练模型与3D模型，并包含一个重要性感知融合（IAF）模块，动态评估每个模型的贡献并重新加权异常得分，同时设计了关键损失函数优化IAF。

**Result:** 实验在MVTec 3D-AD数据集上，IAENet达到了新的state-of-the-art，具有显著降低的误报率。

**Conclusion:** 本研究所提出的方法，不仅结合了多模态的信息，还通过重要性感知融合模块提高了异常检测的整体性能，显示了其在工业部署中的实用性。

**Abstract:** Surface anomaly detection is pivotal for ensuring product quality in
industrial manufacturing. While 2D image-based methods have achieved remarkable
success, 3D point cloud-based detection remains underexplored despite its
richer geometric cues. We argue that the key bottleneck is the absence of
powerful pretrained foundation backbones in 3D comparable to those in 2D. To
bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an
ensemble framework that synergizes 2D pretrained expert with 3D expert models.
However, naively fusing predictions from disparate sources is non-trivial:
existing strategies can be affected by a poorly performing modality and thus
degrade overall accuracy. To address this challenge, We introduce an novel
Importance-Aware Fusion (IAF) module that dynamically assesses the contribution
of each source and reweights their anomaly scores. Furthermore, we devise
critical loss functions that explicitly guide the optimization of IAF, enabling
it to combine the collective knowledge of the source experts but also preserve
their unique strengths, thereby enhancing the overall performance of anomaly
detection. Extensive experiments on MVTec 3D-AD demonstrate that our IAENet
achieves a new state-of-the-art with a markedly lower false positive rate,
underscoring its practical value for industrial deployment.

</details>


### [64] [Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent](https://arxiv.org/abs/2508.20505)
*En Ci,Shanyan Guan,Yanhao Ge,Yilin Zhang,Wei Li,Zhenyu Zhang,Jian Yang,Ying Tai*

Main category: cs.CV

> 研究提出了一种新的图像编辑框架DescriptiveEdit，通过整合参考图像信息和描述性提示以生成高质量的编辑结果。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决文本到图像生成中语义图像编辑的挑战，尤其是逆向算法不可避免地引入重建误差，而基于指令的模型主要受到数据集质量和规模的限制。

**Method:** 提出了一种名为DescriptiveEdit的基于描述性提示的编辑框架，其核心思想是将“基于指令的图像编辑”重新定义为“基于参考图像的文本到图像生成”，从而在不修改架构或进行逆向操作的情况下保留了训练良好的文本到图像模型的生成能力。具体来说，通过引入具有新的注意力桥接的交叉注意UNet，可以将参考图像特征注入到从提示生成编辑图像的过程中。

**Result:** 实验表明，与Emu Edit基准相比，DescriptiveEdit在编辑准确性方面有所提升，并具有更高的一致性。

**Conclusion:** DescriptiveEdit克服了指令数据集质量的限制，能够无缝与ControlNet、IP-Adapter等扩展集成，并且具有更好的可扩展性。

**Abstract:** Despite the progress in text-to-image generation, semantic image editing
remains a challenge. Inversion-based algorithms unavoidably introduce
reconstruction errors, while instruction-based models mainly suffer from
limited dataset quality and scale. To address these problems, we propose a
descriptive-prompt-based editing framework, named DescriptiveEdit. The core
idea is to re-frame `instruction-based image editing' as `reference-image-based
text-to-image generation', which preserves the generative power of well-trained
Text-to-Image models without architectural modifications or inversion.
Specifically, taking the reference image and a prompt as input, we introduce a
Cross-Attentive UNet, which newly adds attention bridges to inject reference
image features into the prompt-to-edit-image generation process. Owing to its
text-to-image nature, DescriptiveEdit overcomes limitations in instruction
dataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other
extensions, and is more scalable. Experiments on the Emu Edit benchmark show it
improves editing accuracy and consistency.

</details>


### [65] [DCFS: Continual Test-Time Adaptation via Dual Consistency of Feature and Sample](https://arxiv.org/abs/2508.20516)
*Wenting Yin,Han Sun,Xinru Meng,Ningzhong Liu,Huiyu Zhou*

Main category: cs.CV

> Proposes a CTTA framework named DCFS that uses dual-path feature consistency and confidence-aware sample learning to reduce noise in pseudo-labels and continuously adapt a pre-trained model at test time.

<details>
  <summary>Details</summary>

**Motivation:** To solve the challenges of confusion and learning biases introduced by solely focusing on target data features and the issues with pseudo-labels' quality and error accumulation.

**Method:** DCFS, a CTTA framework that introduces dual-path feature consistency and confidence-aware sample learning.

**Result:** The method is validated through extensive experiments demonstrating consistent performance across various datasets including CIFAR10-C, CIFAR100-C, and ImageNet-C.

**Conclusion:** DCFS effectively addresses the challenges in continual test-time adaptation scenarios through its unique approach of learning distinct feature representations and adaptive confidence-aware sample learning.

**Abstract:** Continual test-time adaptation aims to continuously adapt a pre-trained model
to a stream of target domain data without accessing source data. Without access
to source domain data, the model focuses solely on the feature characteristics
of the target data. Relying exclusively on these features can lead to confusion
and introduce learning biases. Currently, many existing methods generate
pseudo-labels via model predictions. However, the quality of pseudo-labels
cannot be guaranteed and the problem of error accumulation must be solved. To
address these challenges, we propose DCFS, a novel CTTA framework that
introduces dual-path feature consistency and confidence-aware sample learning.
This framework disentangles the whole feature representation of the target data
into semantic-related feature and domain-related feature using dual classifiers
to learn distinct feature representations. By maintaining consistency between
the sub-features and the whole feature, the model can comprehensively capture
data features from multiple perspectives. Additionally, to ensure that the
whole feature information of the target domain samples is not overlooked, we
set a adaptive threshold and calculate a confidence score for each sample to
carry out loss weighted self-supervised learning, effectively reducing the
noise of pseudo-labels and alleviating the problem of error accumulation. The
efficacy of our proposed method is validated through extensive experimentation
across various datasets, including CIFAR10-C, CIFAR100-C, and ImageNet-C,
demonstrating consistent performance in continual test-time adaptation
scenarios.

</details>


### [66] [Adam SLAM - the last mile of camera calibration with 3DGS](https://arxiv.org/abs/2508.20526)
*Matthieu Gendrin,Stéphane Pateux,Xiaoran Jiang,Théo Ladune,Luce Morin*

Main category: cs.CV

> The paper presents a method to improve camera calibration using a 3D generative shape model, resulting in a notable enhancement in the PSNR of the reconstructed images.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to enhance the camera calibration process, as it significantly affects the quality of novel view synthesis, and there is no ground truth for real scene calibrations.

**Method:** The paper suggests utilizing a 3DGS model to refine calibration through backpropagation of the color loss from novel view synthesis with respect to camera parameters.

**Result:** The proposed calibration method improves the PSNR by an average of 0.4 dB on the reference dataset used for 3DGS.

**Conclusion:** While the fine-tuning process may be time-consuming, it is worthwhile for calibrating critical reference scenes, such as those used in Mip-NeRF 360, where the quality of novel views is paramount.

**Abstract:** The quality of the camera calibration is of major importance for evaluating
progresses in novel view synthesis, as a 1-pixel error on the calibration has a
significant impact on the reconstruction quality. While there is no ground
truth for real scenes, the quality of the calibration is assessed by the
quality of the novel view synthesis. This paper proposes to use a 3DGS model to
fine tune calibration by backpropagation of novel view color loss with respect
to the cameras parameters. The new calibration alone brings an average
improvement of 0.4 dB PSNR on the dataset used as reference by 3DGS. The fine
tuning may be long and its suitability depends on the criticity of training
time, but for calibration of reference scenes, such as Mip-NeRF 360, the stake
of novel view quality is the most important.

</details>


### [67] [Learning What is Worth Learning: Active and Sequential Domain Adaptation for Multi-modal Gross Tumor Volume Segmentation](https://arxiv.org/abs/2508.20528)
*Jingyun Yang,Guoqing Zhang,Jingge Wang,Yang Li*

Main category: cs.CV

> 本文提出了一种用于动态多模态样本选择的主动和顺序领域适应框架，展示了良好的分割性能。

<details>
  <summary>Details</summary>

**Motivation:** 准确的肿瘤体积分割对于鼻咽癌和胶质母细胞瘤的放射治疗规划至关重要。虽然深度神经网络在医学图像分割方面取得了进展，但标注医学图像仍然耗时且劳动密集。因此提出了解决方案，以减少注释成本，并通过选择最具信息量的样本进行标注，用尽可能少的标注样本适应高性能模型。

**Method:** 我们提出了一种用于动态多模态样本选择的主动和顺序领域适应框架。我们设计了一个查询策略，优先对最有价值的样本进行标记和训练，这些样本基于其信息量和代表性进行选择。

**Result:** 在各种肿瘤体积分割任务上的实验证明，我们方法的分割性能显著优于现有的主动领域适应方法。

**Conclusion:** 实验表明，所提出的框架能够有效地进行多模态医学图像分割，并显著优于现有的主动领域适应方法。

**Abstract:** Accurate gross tumor volume segmentation on multi-modal medical data is
critical for radiotherapy planning in nasopharyngeal carcinoma and
glioblastoma. Recent advances in deep neural networks have brought promising
results in medical image segmentation, leading to an increasing demand for
labeled data. Since labeling medical images is time-consuming and
labor-intensive, active learning has emerged as a solution to reduce annotation
costs by selecting the most informative samples to label and adapting
high-performance models with as few labeled samples as possible. Previous
active domain adaptation (ADA) methods seek to minimize sample redundancy by
selecting samples that are farthest from the source domain. However, such
one-off selection can easily cause negative transfer, and access to source
medical data is often limited. Moreover, the query strategy for multi-modal
medical data remains unexplored. In this work, we propose an active and
sequential domain adaptation framework for dynamic multi-modal sample selection
in ADA. We derive a query strategy to prioritize labeling and training on the
most valuable samples based on their informativeness and representativeness.
Empirical validation on diverse gross tumor volume segmentation tasks
demonstrates that our method achieves favorable segmentation performance,
significantly outperforming state-of-the-art ADA methods. Code is available at
the git repository: \href{https://github.com/Hiyoochan/mmActS}{mmActS}.

</details>
