{"id": "2507.08865", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08865", "abs": "https://arxiv.org/abs/2507.08865", "authors": ["Javis AI Team", "Amrendra Singh", "Maulik Shah", "Dharshan Sampath"], "title": "Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale", "comment": null, "summary": "Extracting tables and key-value pairs from financial documents is essential\nfor business workflows such as auditing, data analytics, and automated invoice\nprocessing. In this work, we introduce Spatial ModernBERT-a transformer-based\nmodel augmented with spatial embeddings-to accurately detect and extract\ntabular data and key-value fields from complex financial documents. We cast the\nextraction task as token classification across three heads: (1) Label Head,\nclassifying each token as a label (e.g., PO Number, PO Date, Item Description,\nQuantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;\n(3) Row Head, distinguishing the start of item rows and header rows. The model\nis pretrained on the PubTables-1M dataset, then fine-tuned on a financial\ndocument dataset, achieving robust performance through cross-entropy loss on\neach classification head. We propose a post-processing method to merge tokens\nusing B-I-IB tagging, reconstruct the tabular layout, and extract key-value\npairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages\nboth textual and spatial cues, facilitating highly accurate table and key-value\nextraction in real-world financial documents.", "AI": {"tldr": "本研究提出了Spatial ModernBERT模型，一种结合了空间嵌入的变压器模型，用于从复杂财务文件中准确检测和提取表格数据及键值对。模型通过多头分类和后处理方法提高了准确性，在真实世界文档中表现出色。", "motivation": "从财务文档中提取表格和键值对对于诸如审计、数据分析和自动发票处理等商业工作流程至关重要。本研究旨在介绍能够从复杂财务文档中准确检测和提取表格数据及键值对的技术。", "method": "Spatial ModernBERT模型结合了空间嵌入和变压器模型，从复杂的财务文件中准确检测和提取表格数据及键值对。该模型将任务分为三个头进行标记分类：(1) 标签头，将每个标记分类为某种标签；(2) 列头，预测列索引；(3) 行头，区分项目行和表头行。模型首先在PubTables-1M数据集上进行预训练，然后在财务文档数据集上进行微调。此外，还提出了一种后处理方法，使用B-I-IB标签合并标记，重构表格布局并提取键值对。", "result": "实验表明，Spatial ModernBERT模型能够有效地利用文本和空间线索，实现对真实世界财务文档中表格和键值对的高度准确提取。", "conclusion": "Spatial ModernBERT能够在复杂财务文档中利用文本和空间信息进行有效且准确的表格及键值对提取，展示出其在真实世界应用中的潜力。"}}
{"id": "2507.08898", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08898", "abs": "https://arxiv.org/abs/2507.08898", "authors": ["Wenliang Shan", "Michael Fu", "Rui Yang", "Chakkrit", "Tantithamthavorn"], "title": "SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems", "comment": "Under Review at Information and Software Technology", "summary": "Safety alignment is critical for LLM-powered systems. While recent\nLLM-powered guardrail approaches such as LlamaGuard achieve high detection\naccuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),\nthey struggle with multilingual unsafe inputs. This limitation leaves LLM\nsystems vulnerable to unsafe and jailbreak prompts written in low-resource\nlanguages such as those in Southeast Asia. This paper introduces SEALGuard, a\nmultilingual guardrail designed to improve the safety alignment across diverse\nlanguages. It aims to address the multilingual safety alignment gap of existing\nguardrails and ensure effective filtering of unsafe and jailbreak prompts in\nLLM-powered systems. We adapt a general-purpose multilingual language model\ninto a multilingual guardrail using low-rank adaptation (LoRA). We construct\nSEALSBench, a large-scale multilingual safety alignment dataset containing over\n260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.\nWe evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on\nthis benchmark. Our findings show that multilingual unsafe and jailbreak\nprompts substantially degrade the performance of the state-of-the-art\nLlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and\n18%, respectively, compared to its performance on English-only prompts. In\ncontrast, SEALGuard outperforms existing guardrails in detecting multilingual\nunsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and\nachieving the best DSR, precision, and F1-score. Our ablation study further\nreveals the contributions of adaptation strategies and model size to the\noverall performance of SEALGuard. SEALGuard advances the safety alignment of\nLLM systems by introducing an effective multilingual guardrail.", "AI": {"tldr": "本文提出了SEALGuard来解决LLM系统多语言安全对齐的问题，通过低秩自适应技术在多语种提示中提升了防护性能。实验结果显示SEALGuard检测多语言不安全提示的效果优于现有方法。", "motivation": "虽然LLM驱动的防护措施（如LlamaGuard）在英语中的不安全输入检测方面表现出高精度，但是它们对于多语言不安全输入的处理效果较差，这使得LLM系统容易受到低资源语言（如东南亚语言）中不安全和破解提示的攻击。该研究旨在填补现有防护措施中的多语言安全对齐空白，解决低资源语言中的安全问题，提升语言模型系统的安全性。", "method": "介绍了一种叫做SEALGuard的多语言防护措施，用于改进语言模型系统中的多语言安全对齐。该研究采用低秩自适应（LoRA）技术将通用多语言语言模型适应成一种多语言防护措施。研究同时构建了一个大规模多语言安全对齐数据集SEALSBench，包含超过260,000个提示，涵盖了10种语言的安全、不安全和破解提示。该方法通过对比SEALGuard和最先进防护措施在SEALSBench上的表现来评估性能。实验结果表明，SEALGuard在检测多语言不安全和破解提示方面比现有防护措施表现出色，提升了48%的防御成功率（DSR），并且在最佳DSR、精确率和F1分数方面表现最佳。研究还通过消融研究揭示了适应策略和模型规模对SEALGuard整体性能的贡献。", "result": "实验表明，对于多语言不安全和破解提示，最先进防护措施LlamaGuard的防御成功率分别下降了9%和18%。而SEALGuard在检测多语言不安全和破解提示方面表现优于现有防护措施，提升了48%的防御成功率，并且在最佳防御成功率DSR、精确率和F1分数方面表现出色。", "conclusion": "本文介绍的SEALGuard通过引入高效的多语言防护措施，推动了LLM系统的安全对齐。适应策略和模型规模的影响进一步揭示了模型的性能贡献，对于提升LLM系统在多语言环境下的安全性具有重要影响。"}}
{"id": "2507.08916", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08916", "abs": "https://arxiv.org/abs/2507.08916", "authors": ["Mahmoud Alwakeel", "Aditya Nagori", "Vijay Krishnamoorthy", "Rishikesan Kamaleswaran"], "title": "Evaluating LLMs in Medicine: A Call for Rigor, Transparency", "comment": null, "summary": "Objectives: To evaluate the current limitations of large language models\n(LLMs) in medical question answering, focusing on the quality of datasets used\nfor their evaluation. Materials and Methods: Widely-used benchmark datasets,\nincluding MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,\ntransparency, and relevance to clinical scenarios. Alternatives, such as\nchallenge questions in medical journals, were also analyzed to identify their\npotential as unbiased evaluation tools. Results: Most existing datasets lack\nclinical realism, transparency, and robust validation processes. Publicly\navailable challenge questions offer some benefits but are limited by their\nsmall size, narrow scope, and exposure to LLM training. These gaps highlight\nthe need for secure, comprehensive, and representative datasets. Conclusion: A\nstandardized framework is critical for evaluating LLMs in medicine.\nCollaborative efforts among institutions and policymakers are needed to ensure\ndatasets and methodologies are rigorous, unbiased, and reflective of clinical\ncomplexities.", "AI": {"tldr": "研究评估了LLMs在医疗问答中的限制，指出现有数据集缺乏临床现实性等问题，强调了需要标准化框架来评估医学领域的LLMs。", "motivation": "研究旨在评估大型语言模型在医疗领域中的表现和数据集的优劣，识别当前存在的问题，并提出可能的解决方案。", "method": "评估大型语言模型（LLMs）在医疗问答上的当前限制，重点在于评估所用数据集的质量。研究了包括MedQA、MedMCQA、PubMedQA和MMLU在内的广泛使用的基准数据集，审查了它们的严格性、透明度和与临床场景的相关性。同时分析了医学期刊中的挑战性问题，以识别其作为无偏评估工具的潜力。", "result": "大多数现有的数据集在临床现实性、透明度和稳健的验证过程方面存在不足。公开的挑战性问题具有一些优势，但也受限于规模小、范围狭窄和LLM训练中的暴露。这些差距突显了需要安全、全面且具有代表性的数据集。", "conclusion": "为了评估医疗领域的大型语言模型，制定标准化框架至关重要。需要机构和政策制定者共同努力，确保数据集和方法论严格、无偏且能反映出临床复杂性。"}}
{"id": "2507.08924", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08924", "abs": "https://arxiv.org/abs/2507.08924", "authors": ["Seokhee Hong", "Sunkyoung Kim", "Guijin Son", "Soyeon Kim", "Yeonjung Hong", "Jinsik Lee"], "title": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation", "comment": null, "summary": "The development of Large Language Models (LLMs) requires robust benchmarks\nthat encompass not only academic domains but also industrial fields to\neffectively evaluate their applicability in real-world scenarios. In this\npaper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,\nreconstructed from the existing KMMLU, consists of questions from the Korean\nNational Technical Qualification exams, with critical errors removed to enhance\nreliability. KMMLU-Pro is based on Korean National Professional Licensure exams\nto reflect professional knowledge in Korea. Our experiments demonstrate that\nthese benchmarks comprehensively represent industrial knowledge in Korea. We\nrelease our dataset publicly available.", "AI": {"tldr": "本文介绍了两个朝鲜语专家级别的基准测试KMMLU-Redux和KMMLU-Pro，以更好地评估大型语言模型在工业领域应用的适用性。", "motivation": "开发大型语言模型需要稳健的基准测试，不仅要涵盖学术领域，还要包括工业领域，以有效地评估其在实际场景中的适用性。", "method": "KMMLU-Redux基于现有的KMMLU，从韩国国家技术资格考试中抽取问题，并删除了关键错误以提高可靠性。KMMLU-Pro基于韩国国家专业执照考试，反映了韩国的专业知识。", "result": "实验表明，这些基准测试全面代表了韩国的工业知识。", "conclusion": "研究团队公开发布了他们的数据集。"}}
{"id": "2507.08831", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08831", "abs": "https://arxiv.org/abs/2507.08831", "authors": ["Josh Qixuan Sun", "Xiaoying Xing", "Huaiyuan Weng", "Chul Min Yeum", "Mark Crowley"], "title": "View Invariant Learning for Vision-Language Navigation in Continuous Environments", "comment": "Under review", "summary": "Vision-Language Navigation in Continuous Environments (VLNCE), where an agent\nfollows instructions and moves freely to reach a destination, is a key research\nproblem in embodied AI. However, most navigation policies are sensitive to\nviewpoint changes, i.e., variations in camera height and viewing angle that\nalter the agent's observation. In this paper, we introduce a generalized\nscenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View\nInvariant Learning), a view-invariant post-training strategy that enhances the\nrobustness of existing navigation policies to changes in camera viewpoint. VIL\nemploys a contrastive learning framework to learn sparse and view-invariant\nfeatures. Additionally, we introduce a teacher-student framework for the\nWaypoint Predictor Module, a core component of most VLNCE baselines, where a\nview-dependent teacher model distills knowledge into a view-invariant student\nmodel. We employ an end-to-end training paradigm to jointly optimize these\ncomponents, thus eliminating the cost for individual module training. Empirical\nresults show that our method outperforms state-of-the-art approaches on\nV2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets\nR2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE\nsetting and find that, despite being trained for varied viewpoints, it often\nstill improves performance. On the more challenging RxR-CE dataset, our method\nalso achieved state-of-the-art performance across all metrics when compared to\nother map-free methods. This suggests that adding VIL does not diminish the\nstandard viewpoint performance and can serve as a plug-and-play post-training\nmethod.", "AI": {"tldr": "本文提出了V2-VLNCE（带有变化视角的VLNCE）场景，并提出了一种视点不变的学习策略VIL，增强了现有导航策略对相机视点变化的鲁棒性，在实验中VIL的表现优于最先进的方法。", "motivation": "大多数导航策略对视点变化敏感，而视点变化又会影响代理的观察。", "method": "该论文采用了一种对比学习框架学习稀疏视点不变特征，并引入了一个教师-学生框架用于Waypoint预测器模块，通过一个视点相关的教师模型将知识传授给视点不变的学生模型，最终采用端到端训练模式进行联合优化。", "result": "实验结果显示，该方法在V2-VLNCE场景下，对于两个标准的数据集R2R-CE和RxR-CE，其成功率超过了最先进的方法8-15%。", "conclusion": "该论文提出的方法不仅在变化视点的场景下表现出色，甚至在标准VLNCE设置中也能提升性能，可以说是‘即插即用’的后训练策略。"}}
{"id": "2507.08967", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08967", "abs": "https://arxiv.org/abs/2507.08967", "authors": ["Rongyi Zhu", "Yuhui Wang", "Tanqiu Jiang", "Jiacheng Liang", "Ting Wang"], "title": "Self-Improving Model Steering", "comment": "16 pages, 9 figures", "summary": "Model steering represents a powerful technique that dynamically aligns large\nlanguage models (LLMs) with human preferences during inference. However,\nconventional model-steering methods rely heavily on externally annotated data,\nnot only limiting their adaptability to varying contexts but also tethering\ntheir effectiveness to annotation quality. In this paper, we present SIMS, the\nfirst self-improving model-steering framework that operates without relying on\nexternal supervision. At its core, SIMS autonomously generates and refines\ncontrastive samples through iterative self-improvement cycles, enabling\nadaptive, context-specific steering. Additionally, SIMS employs novel\nstrategies, including prompt ranking and contrast sampling, to further enhance\nsteering efficacy. Extensive evaluation across diverse LLMs and benchmarks\ndemonstrates that SIMS substantially outperforms existing methods in steering\neffectiveness and adaptability, highlighting self-improving model steering as a\npromising direction for future research on inference-time LLM alignment.", "AI": {"tldr": "This paper introduces SIMS, a self-improving model-steering framework that does not rely on external data and demonstrates superior performance over existing methods in steering large language models effectively and adaptively.", "motivation": "To overcome the limitations of conventional model steering methods that rely on externally annotated data, which limit adaptability and depend on annotation quality.", "method": "SIMS, a self-improving model-steering framework that autonomously generates and refines contrastive samples without relying on external supervision, incorporating novel strategies such as prompt ranking and contrast sampling.", "result": "SIMS outperformed existing methods in terms of steering effectiveness and adaptability across diverse LLMs and benchmarks.", "conclusion": "Self-improving model steering, like SIMS, presents a promising approach for aligning LLMs with human preferences at inference time."}}
{"id": "2507.08917", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08917", "abs": "https://arxiv.org/abs/2507.08917", "authors": ["Justin D. Norman", "Hany Farid"], "title": "Detecting Deepfake Talking Heads from Facial Biometric Anomalies", "comment": "10 pages, 3 figures, 3 tables", "summary": "The combination of highly realistic voice cloning, along with visually\ncompelling avatar, face-swap, or lip-sync deepfake video generation, makes it\nrelatively easy to create a video of anyone saying anything. Today, such\ndeepfake impersonations are often used to power frauds, scams, and political\ndisinformation. We propose a novel forensic machine learning technique for the\ndetection of deepfake video impersonations that leverages unnatural patterns in\nfacial biometrics. We evaluate this technique across a large dataset of\ndeepfake techniques and impersonations, as well as assess its reliability to\nvideo laundering and its generalization to previously unseen video deepfake\ngenerators.", "AI": {"tldr": "本文提出了一种基于机器学习的新方法来检测深度伪造视频里的冒充行为。", "motivation": "鉴于高度逼真的声音克隆和视觉吸引的头像、面部替换或唇同步深度伪造视频生成，使得用任何人说任何内容的视频变得相对容易制造，这样的深度伪造形象经常被用来驱动欺诈、骗局和政治误导。", "method": "我们提出了一种新颖的基于面部生物特征中不自然模式的机器学习取证技术，用于检测深度伪造视频冒充。", "result": "我们对该技术进行了广泛的评估，涵盖了众多的深度伪造技术和冒充案例，并且评估了该技术在视频洗白中的可靠性以及它对以前未见过的视频深度伪造生成器的泛化能力。", "conclusion": "我们的技术可以通过检测面部生物特征中的异常模式来有效地识别深度伪造视频中的冒充行为。"}}
{"id": "2507.08969", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08969", "abs": "https://arxiv.org/abs/2507.08969", "authors": ["Drew Walker", "Jennifer Love", "Swati Rajwal", "Isabel C Walker", "Hannah LF Cooper", "Abeed Sarker", "Melvin Livingston III"], "title": "Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR", "comment": "3 Tables", "summary": "Introduction: Electronic health records (EHR) are a critical medium through\nwhich patient stigmatization is perpetuated among healthcare teams. Methods: We\nidentified linguistic features of doubt markers and stigmatizing labels in\nMIMIC-III EHR via expanded lexicon matching and supervised learning\nclassifiers. Predictors of rates of linguistic features were assessed using\nPoisson regression models. Results: We found higher rates of stigmatizing\nlabels per chart among patients who were Black or African American (RR: 1.16),\npatients with Medicare/Medicaid or government-run insurance (RR: 2.46),\nself-pay (RR: 2.12), and patients with a variety of stigmatizing disease and\nmental health conditions. Patterns among doubt markers were similar, though\nmale patients had higher rates of doubt markers (RR: 1.25). We found increased\nstigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),\nwith similar patterns of doubt markers. Discussion: Stigmatizing language\noccurred at higher rates among historically stigmatized patients, perpetuated\nby multiple provider types.", "AI": {"tldr": "研究发现，在电子健康记录中，某些患者的标签和怀疑用语使用率较高，包括非洲裔美国人、使用政府保险或自费支付的患者以及患有某些疾病或精神疾病的患者。护理人员和社会工作者使用贬损语言的频率也较高。", "motivation": "该研究旨在通过分析电子健康记录中的语言特征，识别和量化患者污名化的语言标记，以及怀疑标记。", "method": "使用扩大的词汇匹配和监督学习分类器，在MIMIC-III电子健康记录中识别怀疑标记和污名化标签的语言特征；通过Poisson回归模型评估这些语言特征的变化预测因素。", "result": "研究发现，非洲裔美国人患者、使用联邦医疗保险/医疗补助或政府运营保险的患者、自费患者以及患有多种污名化疾病和精神疾病的患者的污名化标签使用率较高。护理人员和社会工作者使用的污名化标签的频率也较高。", "conclusion": "历史性污名化的患者中，污名化语言的使用率较高，并且由多种医疗服务提供者类型所推动。"}}
{"id": "2507.08979", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08979", "abs": "https://arxiv.org/abs/2507.08979", "authors": ["Mahdiyar Molahasani", "Azadeh Motamedi", "Michael Greenspan", "Il-Min Kim", "Ali Etemad"], "title": "PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection", "comment": "Accepted to ICCV 2025", "summary": "We introduce Projection-based Reduction of Implicit Spurious bias in\nvision-language Models (PRISM), a new data-free and task-agnostic solution for\nbias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in\ntheir training data, leading to skewed predictions. PRISM is designed to debias\nVLMs without relying on predefined bias categories or additional external data.\nIt operates in two stages: first, an LLM is prompted with simple class prompts\nto generate scene descriptions that contain spurious correlations. Next, PRISM\nuses our novel contrastive-style debiasing loss to learn a projection that maps\nthe embeddings onto a latent space that minimizes spurious correlations while\npreserving the alignment between image and text embeddings.Extensive\nexperiments demonstrate that PRISM outperforms current debiasing methods on the\ncommonly used Waterbirds and CelebA datasets We make our code public at:\nhttps://github.com/MahdiyarMM/PRISM.", "AI": {"tldr": "PRISM是一种数据无关、任务无关的方案，旨在通过新颖的对比去偏技术，减轻视觉-语言模型的偏差，且不依赖于预定义的偏差类别或额外的外部数据。", "motivation": "视觉-语言模型（如CLIP）继承并放大了训练数据中的偏差，导致预测偏斜，PRISM为解决这个问题而设计。", "method": "PRISM采用两阶段方法来减轻视觉-语言模型中的偏差，首先使用LLM生成带有偶然关联的场景描述，其次使用创新的对比去偏损失学习映射嵌入到最小化偶然关联并保持图像和文本嵌入对齐的潜在空间的投影。", "result": "实验表明，PRISM在Waterbirds和CelebA数据集上优于现有的去偏差方法。", "conclusion": "PRISM提供了一种有效且无需额外数据的方法来减轻视觉-语言模型中的偏差，提高了模型的公平性和准确性。"}}
{"id": "2507.09011", "categories": ["cs.CL", "q-bio.NC", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.09011", "abs": "https://arxiv.org/abs/2507.09011", "authors": ["Ana Chkhaidze", "Reshanne R. Reeder", "Connor Gag", "Anastasia Kiyonaga", "Seana Coulson"], "title": "Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery", "comment": null, "summary": "A rapidly alternating red and black display known as Ganzflicker induces\nvisual hallucinations that reflect the generative capacity of the visual\nsystem. Recent proposals regarding the imagery spectrum, that is, differences\nin the visual system of individuals with absent imagery, typical imagery, and\nvivid imagery, suggest these differences should impact the complexity of other\ninternally generated visual experiences. Here, we used tools from natural\nlanguage processing to analyze free-text descriptions of hallucinations from\nover 4,000 participants, asking whether people with different imagery\nphenotypes see different things in their mind's eye during Ganzflicker-induced\nhallucinations. Strong imagers described complex, naturalistic content, while\nweak imagers reported simple geometric patterns. Embeddings from vision\nlanguage models better captured these differences than text-only language\nmodels, and participants with stronger imagery used language with richer\nsensorimotor associations. These findings may reflect individual variation in\ncoordination between early visual areas and higher-order regions relevant for\nthe imagery spectrum.", "AI": {"tldr": "本研究使用自然语言处理工具分析了超过4000名参与者在Ganzflicker诱导的视觉幻觉体验，发现拥有不同视觉想象能力的人看到的内容有明显差异，这可能反映了他们在视觉区域间的协调性存在个体差异。", "motivation": "最近关于图像频谱的提议提出，个体之间的视觉系统差异会影响其他内部生成的视觉体验的复杂性。本研究旨在探讨具有不同类型视觉想象能力的人在Ganzflicker诱导的幻觉中看到的内容是否存在差异。", "method": "使用自然语言处理工具对超过4000名参与者在Ganzflicker诱导的幻觉期间自发描述的幻觉进行了分析。", "result": "强想象者描述了复杂的、自然的内容，而弱想象者报告了简单的几何图案。视觉语言模型的嵌入比仅使用文本的语言模型更好地捕捉了这些差异，并且具有更强想象能力的参与者使用了具有更丰富感觉运动关联的语言。", "conclusion": "这些发现可能反映了早期视觉区域和与图像频谱相关的较高层区域之间的个体协调差异。"}}
{"id": "2507.08981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08981", "abs": "https://arxiv.org/abs/2507.08981", "authors": ["Hanbyel Cho", "Jaesung Ahn", "Yooshin Cho", "Junmo Kim"], "title": "Video Inference for Human Mesh Recovery with Vision Transformer", "comment": "Accepted to IEEE FG 2023", "summary": "Human Mesh Recovery (HMR) from an image is a challenging problem because of\nthe inherent ambiguity of the task. Existing HMR methods utilized either\ntemporal information or kinematic relationships to achieve higher accuracy, but\nthere is no method using both. Hence, we propose \"Video Inference for Human\nMesh Recovery with Vision Transformer (HMR-ViT)\" that can take into account\nboth temporal and kinematic information. In HMR-ViT, a Temporal-kinematic\nFeature Image is constructed using feature vectors obtained from video frames\nby an image encoder. When generating the feature image, we use a Channel\nRearranging Matrix (CRM) so that similar kinematic features could be located\nspatially close together. The feature image is then further encoded using\nVision Transformer, and the SMPL pose and shape parameters are finally inferred\nusing a regression network. Extensive evaluation on the 3DPW and Human3.6M\ndatasets indicates that our method achieves a competitive performance in HMR.", "AI": {"tldr": "提出了HMR-ViT，结合时空信息进行人体网格恢复，利用CRM和视觉转换器技术，在3DPW和Human3.6M数据集上取得了有竞争力的性能。", "motivation": "现有的人体网格恢复方法要么利用时间信息，要么利用运动学关系，但没有同时使用这两种信息。因此，我们提出了一种结合两者的方法。", "method": "HMR-ViT通过使用视频帧中的特征向量构建时空特征图像，利用通道重排矩阵(CRM)使相似的运动学特征在空间上靠近，并通过视觉转换器进一步编码该特征图像，最后使用回归网络推断SMPL姿态和形状参数。", "result": "在3DPW和Human3.6M数据集上进行了广泛的评估，表明HMR-ViT在此任务中取得了有竞争力的性能。", "conclusion": "HMR-ViT展示了在人体网格恢复任务中结合时间和运动学信息的有效性，使用视觉转换器提高了网格恢复的性能。"}}
{"id": "2507.09025", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09025", "abs": "https://arxiv.org/abs/2507.09025", "authors": ["Chien Van Nguyen", "Ruiyi Zhang", "Hanieh Deilamsalehy", "Puneet Mathur", "Viet Dac Lai", "Haoliang Wang", "Jayakumar Subramanian", "Ryan A. Rossi", "Trung Bui", "Nikos Vlassis", "Franck Dernoncourt", "Thien Huu Nguyen"], "title": "Lizard: An Efficient Linearization Framework for Large Language Models", "comment": "15 pages", "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.", "AI": {"tldr": "Lizard proposes a new method for making Transformer-based Large Language Models more efficient for long-context tasks by introducing subquadratic attention and a gating mechanism, leading to improved performance in language modeling benchmarks.", "motivation": "To overcome the quadratic complexity challenges and memory bottlenecks encountered by Transformer-based LLMs as context lengths increase, especially in infinite-context generation scenarios.", "method": "Lizard, a linearization framework which transforms pretrained Transformer-based Large Language Models (LLMs) into flexible, subquadratic architectures, introduces a subquadratic attention mechanism and a gating module to enable adaptive memory control and support constant-memory inference.", "result": "The experiments show that Lizard achieves near-lossless recovery of the teacher model's performance on standard language modeling tasks and outperforms previous linearization methods, demonstrating a 18 points improvement on the 5-shot MMLU benchmark and significant improvements in associative recall tasks.", "conclusion": "Lizard offers a promising approach to addressing the memory and computational overhead in large language models through the innovative integration of subquadratic attention and gating mechanisms, providing significant performance and flexibility improvements."}}
{"id": "2507.09005", "categories": ["cs.CV", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2507.09005", "abs": "https://arxiv.org/abs/2507.09005", "authors": ["Cheng-Hsi Hsiao", "Krishna Kumar"], "title": "From images to properties: a NeRF-driven framework for granular material parameter inversion", "comment": null, "summary": "We introduce a novel framework that integrates Neural Radiance Fields (NeRF)\nwith Material Point Method (MPM) simulation to infer granular material\nproperties from visual observations. Our approach begins by generating\nsynthetic experimental data, simulating an plow interacting with sand. The\nexperiment is rendered into realistic images as the photographic observations.\nThese observations include multi-view images of the experiment's initial state\nand time-sequenced images from two fixed cameras. Using NeRF, we reconstruct\nthe 3D geometry from the initial multi-view images, leveraging its capability\nto synthesize novel viewpoints and capture intricate surface details. The\nreconstructed geometry is then used to initialize material point positions for\nthe MPM simulation, where the friction angle remains unknown. We render images\nof the simulation under the same camera setup and compare them to the observed\nimages. By employing Bayesian optimization, we minimize the image loss to\nestimate the best-fitting friction angle. Our results demonstrate that friction\nangle can be estimated with an error within 2 degrees, highlighting the\neffectiveness of inverse analysis through purely visual observations. This\napproach offers a promising solution for characterizing granular materials in\nreal-world scenarios where direct measurement is impractical or impossible.", "AI": {"tldr": "本文提出了一种结合NeRF和MPM的新框架，通过视觉观测估计颗粒材料的摩擦角，实现误差在2度之内的高精度估计。", "motivation": "该研究旨在解决现实场景中直接测量颗粒材料特性不切实际或不可能的问题，提出了一种从视觉观测数据中推断材料特性的创新方法。", "method": "本文提出了一种将Neural Radiance Fields (NeRF)与Material Point Method (MPM)模拟相结合的新框架，用于从视觉观测中推断出颗粒材料的特性。具体方法是从合成实验数据开始，模拟犁与沙子的相互作用，生成多视角图像，并利用NeRF重构初始状态的3D几何，重建的几何用于MPM仿真中材料点位置的初始化。通过贝叶斯优化最小化图像损失，估计最佳拟合摩擦角。", "result": "通过对比仿真图像与观测图像，本文成功地将摩擦角估计误差限定在2度以内，验证了所提方法的有效性。", "conclusion": "研究表明，从视觉观察中估计颗粒材料摩擦角的方法是有效的，这种方法为无法直接测量材料特性的现实场景提供了一种解决方案。"}}
{"id": "2507.09037", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09037", "abs": "https://arxiv.org/abs/2507.09037", "authors": ["Bharadwaj Ravichandran", "David Joy", "Paul Elliott", "Brian Hu", "Jadie Adams", "Christopher Funk", "Emily Veenhuis", "Anthony Hoogs", "Arslan Basharat"], "title": "ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making", "comment": "10 pages total (including appendix), ICML 2025 Workshop on Reliable\n  and Responsible Foundation Models", "summary": "Large language models (LLMs) are increasingly being used as decision aids.\nHowever, users have diverse values and preferences that can affect their\ndecision-making, which requires novel methods for LLM alignment and\npersonalization. Existing LLM comparison tools largely focus on benchmarking\ntasks, such as knowledge-based question answering. In contrast, our proposed\nALIGN system focuses on dynamic personalization of LLM-based decision-makers\nthrough prompt-based alignment to a set of fine-grained attributes. Key\nfeatures of our system include robust configuration management, structured\noutput generation with reasoning, and several algorithm implementations with\nswappable LLM backbones, enabling different types of analyses. Our user\ninterface enables a qualitative, side-by-side comparison of LLMs and their\nalignment to various attributes, with a modular backend for easy algorithm\nintegration. Additionally, we perform a quantitative analysis comparing\nalignment approaches in two different domains: demographic alignment for public\nopinion surveys and value alignment for medical triage decision-making. The\nentire ALIGN framework is open source and will enable new research on reliable,\nresponsible, and personalized LLM-based decision-makers.", "AI": {"tldr": "The paper introduces the ALIGN system for dynamic personalization of LLM-based decision-makers using prompt-based alignment to a set of fine-grained attributes, enabling comparative analyses in different domains.", "motivation": "To address the diverse values and preferences of users when using LLMs as decision aids, requiring novel methods for alignment and personalization beyond existing benchmarking-focused tools.", "method": "Developed the ALIGN system with robust configuration management, structured output generation with reasoning, and swappable LLM backends. Also created a user interface for qualitative comparisons and performed a quantitative analysis in two domains.", "result": "The ALIGN framework enables a modular, qualitative, and quantitative comparison of LLMs and their alignment to various attributes across domains like demographic alignment and value alignment.", "conclusion": "The ALIGN system opens new research opportunities for reliable, responsible, and personalized LLM-based decision-makers by providing an open-source framework for diverse analyses and comparisons."}}
{"id": "2507.09008", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09008", "abs": "https://arxiv.org/abs/2507.09008", "authors": ["Xiwei Xuan", "Xiaoqi Wang", "Wenbin He", "Jorge Piazentin Ono", "Liang Gou", "Kwan-Liu Ma", "Liu Ren"], "title": "VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels", "comment": "IEEE Transactions on Visualization and Computer Graphics (2025)", "summary": "The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA)\nhave facilitated the auto-labeling of large-scale datasets, enhancing model\nperformance in challenging downstream tasks such as open-vocabulary object\ndetection and segmentation. However, the quality of FM-generated labels is less\nstudied as existing approaches focus more on data quantity over quality. This\nis because validating large volumes of data without ground truth presents a\nconsiderable challenge in practice. Existing methods typically rely on limited\nmetrics to identify problematic data, lacking a comprehensive perspective, or\napply human validation to only a small data fraction, failing to address the\nfull spectrum of potential issues. To overcome these challenges, we introduce\nVISTA, a visual analytics framework that improves data quality to enhance the\nperformance of multi-modal models. Targeting the complex and demanding domain\nof open-vocabulary image segmentation, VISTA integrates multi-phased data\nvalidation strategies with human expertise, enabling humans to identify,\nunderstand, and correct hidden issues within FM-generated labels. Through\ndetailed use cases on two benchmark datasets and expert reviews, we demonstrate\nVISTA's effectiveness from both quantitative and qualitative perspectives.", "AI": {"tldr": "本文提出了VISTA框架，以改进多模态模型生成标签的质量，通过多阶段数据验证策略与人类专业知识相结合的方式，提高了模型在复杂下游任务中的表现。", "motivation": "尽管多模态基础模型的进展促进了大规模数据集的自动生成标签，提升了模型在开放词汇目标检测和分割等棘手下游任务中的表现，但现有方法大多关注数据量而非数据质量，且在缺乏真实标签的情况下，验证大量数据极具挑战。现有方法通常依赖有限的指标来识别问题数据，或是仅对一小部分数据进行人工验证，无法全面解决问题。", "method": "提出了一种名为VISTA的视觉分析框架，以改善数据质量，进而提高多模态模型的性能。VISTA针对开放词汇图像分割这一复杂领域，集成了多阶段数据验证策略与人类专业知识，使人类能够识别、理解和纠正多模态基础模型生成标签中的隐藏问题。", "result": "通过两个基准数据集的详细用例和专家评审，从定量和定性两个角度证明了VISTA的有效性。", "conclusion": "VISTA通过结合多阶段数据验证策略和人类专业知识，能够有效改善多模态基础模型生成标签的质量，提升模型在开放词汇图像分割等任务中的性能。"}}
{"id": "2507.09075", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09075", "abs": "https://arxiv.org/abs/2507.09075", "authors": ["Wasi Uddin Ahmad", "Somshubra Majumdar", "Aleksander Ficek", "Sean Narenthiran", "Mehrzad Samadi", "Jocelyn Huang", "Siddhartha Jain", "Vahid Noroozi", "Boris Ginsburg"], "title": "OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique", "comment": "work in progress", "summary": "Recent advancements in reasoning-based Large Language Models (LLMs),\nparticularly their potential through test-time scaling, have created\nsignificant opportunities for distillation in code generation and critique.\nHowever, progress in both areas fundamentally depends on large-scale,\nhigh-quality datasets. In this work, we introduce OpenCodeReasoning-II, a\ndataset consists of 2.5M question-solution-critique triples (approx. 35K unique\nprogramming questions), making it nearly twice the size of the previous largest\npublicly available code reasoning dataset. In this work, we employ a two-stage\nsupervised fine-tuning strategy. The first stage focuses on fine-tuning for\ncode generation, while the second stage involves the joint training of models\nfor both code generation and critique. Our resulting finetuned Qwen2.5-Instruct\nmodels achieve performance in code generation that either exceeds or equals the\nbest prior open-weight distilled models. Notably, the integration of our code\ngeneration and critique models leads to significant improvements in competitive\ncoding performance. Furthermore, we present an extension of the LiveCodeBench\nbenchmark to specifically support the C++ programming language, thereby\nfacilitating more comprehensive LLM evaluation using this benchmark.", "AI": {"tldr": "本研究提出了一个包含2.5M问题-解决方案-评论三元组的数据集OpenCodeReasoning-II，并通过两阶段微调策略提高了Qwen2.5-Instruct模型在代码生成和代码审查上的性能。", "motivation": "代码生成和代码审查的进步在很大程度上依赖于大规模高质量的数据集。本研究旨在通过引入更大规模的数据集和改进的微调策略，提高代码生成和代码审查的性能，从而推动这两个领域的发展。", "method": "本研究采用两阶段监督微调策略。第一阶段专注于代码生成的微调，第二阶段则共同训练代码生成和代码审查模型。此外，引入了OpenCodeReasoning-II数据集，包含大约35K个独特的编程问题，总计2.5M个问题-解决方案-评论三元组，使其成为目前最大规模公开的代码推理数据集之一。研究还扩展了LiveCodeBench基准测试，支持C++编程语言，以更全面地评估LLMs。", "result": "研究表明，采用两阶段微调策略后的Qwen2.5-Instruct模型在代码生成方面的性能超过或等于现有最好的公开模型。同时，集成代码生成和代码审查模型的方法，在竞争性编程中的表现有了显著提升。", "conclusion": "通过引入大规模数据集OpenCodeReasoning-II和采取两阶段微调策略，成功提升了代码生成和代码审查模型的性能。此外，扩展LiveCodeBench基准测试支持C++有助于更全面地评估LLMs在编程任务中的表现。"}}
{"id": "2507.09036", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09036", "abs": "https://arxiv.org/abs/2507.09036", "authors": ["Florian Kofler", "Marcel Rosier", "Mehdi Astaraki", "Hendrik Möller", "Ilhem Isra Mekki", "Josef A. Buchner", "Anton Schmick", "Arianna Pfiffer", "Eva Oswald", "Lucas Zimmer", "Ezequiel de la Rosa", "Sarthak Pati", "Julian Canisius", "Arianna Piffer", "Ujjwal Baid", "Mahyar Valizadeh", "Akis Linardos", "Jan C. Peeken", "Surprosanna Shit", "Felix Steinbauer", "Daniel Rueckert", "Rolf Heckemann", "Spyridon Bakas", "Jan Kirschke", "Constantin von See", "Ivan Ezhov", "Marie Piraud", "Benedikt Wiestler", "Bjoern Menze"], "title": "BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis", "comment": "16p, 3f", "summary": "BrainLesion Suite is a versatile toolkit for building modular brain lesion\nimage analysis pipelines in Python. Following Pythonic principles, BrainLesion\nSuite is designed to provide a 'brainless' development experience, minimizing\ncognitive effort and streamlining the creation of complex workflows for\nclinical and scientific practice. At its core is an adaptable preprocessing\nmodule that performs co-registration, atlas registration, and optional\nskull-stripping and defacing on arbitrary multi-modal input images. BrainLesion\nSuite leverages algorithms from the BraTS challenge to synthesize missing\nmodalities, inpaint lesions, and generate pathology-specific tumor\nsegmentations. BrainLesion Suite also enables quantifying segmentation model\nperformance, with tools such as panoptica to compute lesion-wise metrics.\nAlthough BrainLesion Suite was originally developed for image analysis\npipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,\nit can be adapted for other biomedical image analysis applications. The\nindividual BrainLesion Suite packages and tutorials are accessible on GitHub.", "AI": {"tldr": "BrainLesion Suite 是用于脑部病变图像分析的 Python 工具集，它提供了一个高效的工作流程以及量化分析工具，可适应多种生物医学成像应用。", "motivation": "开发 BrainLesion Suite 的动机是为临床和科学研究提供一个高效、灵活的脑部病变图像分析工具集，最初的目标是针对胶质瘤、转移瘤和多发性硬化症等脑部病变的图像分析，但也可以调整用于其他生物医学图像分析的应用。", "method": "BrainLesion Suite利用Python编程语言，构建了一个用于脑部病变图像分析的模块化工具集。该工具集遵循Pythonic原则，旨在提供一个减少认知负担、简化复杂工作流创建的开发体验。核心是一个可适应的预处理模块，它执行共配准、大脑 atlas 注册，以及可选的去颅骨和图像脱敏处理，适用于任意多模态输入图像。", "result": "BrainLesion Suite 使用 BraTS 挑战中的算法来合成缺失的模态图像，填充病变区域，生成特定病理的肿瘤分割。此外，它还提供了量化分割模型性能的工具，例如 panoptica 计算病灶级别的度量。", "conclusion": "因此，BrainLesion Suite 不仅提供了一个强大的脑部病变图像分析解决方案，而且通过其模块化设计展示了高度的适应性和扩展性，有能力为其他生物医学图像分析需求提供服务。"}}
{"id": "2507.09076", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; H.5.2"], "pdf": "https://arxiv.org/pdf/2507.09076", "abs": "https://arxiv.org/abs/2507.09076", "authors": ["Jialong Mai", "Xiaofen Xing", "Yawei Li", "Zhipeng Li", "Jingyuan Xing", "Xiangmin Xu"], "title": "Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation", "comment": "submitted to EMNLP 2025", "summary": "Recent research has focused on applying speech large language model (SLLM) to\nimprove speech emotion recognition (SER). However, the inherently high frame\nrate in speech modality severely limits the signal processing and understanding\ncapabilities of SLLM. For example, a SLLM with a 4K context window can only\nprocess 80 seconds of audio at 50Hz feature sampling rate before reaching its\ncapacity limit. Input token compression methods used in SLLM overlook the\ncontinuity and inertia of emotions across multiple conversation turns. This\npaper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual\nsemantics and sentence-level emotion encoding, enabling processing of\nunlimited-length audio with limited context windows in SLLM. Specifically, DPM\nprogressively encodes sentence-level information and emotions into a temporary\nLoRA module during inference to effectively \"memorize\" the contextual\ninformation. We trained an emotion SLLM as a backbone and incorporated our DPM\ninto inference for emotion recognition in conversation (ERC). Experimental\nresults on the IEMOCAP dataset show that DPM significantly improves the emotion\nrecognition capabilities of SLLM when processing long audio sequences,\nachieving state-of-the-art performance.", "AI": {"tldr": "Introduces DPM for unlimited-length audio processing with limited SLLM context windows, enhancing SER outcomes.", "motivation": "To address the limitations of speech large language models (SLLMs) in processing high frame rate audio and capturing emotional continuity across multiple conversation turns for speech emotion recognition.", "method": "Dynamic Parameter Memory (DPM) mechanism with contextual semantics and sentence-level emotion encoding is proposed to enable the processing of unlimited-length audio with limited context windows in SLLM for speech emotion recognition.", "result": "Experimental results on the IEMOCAP dataset indicate significant improvements in emotion recognition capabilities of SLLM when processing long audio sequences, achieving state-of-the-art performance.", "conclusion": "The Dynamic Parameter Memory (DPM) mechanism improves long audio sequence emotion recognition capabilities in speech emotion recognition tasks, marking a new state-of-the-art performance."}}
{"id": "2507.09052", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09052", "abs": "https://arxiv.org/abs/2507.09052", "authors": ["Fang Chen", "Alex Villa", "Gongbo Liang", "Xiaoyi Lu", "Meng Tang"], "title": "Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?", "comment": "20 pages, 11 figures", "summary": "Training data for class-conditional image synthesis often exhibit a\nlong-tailed distribution with limited images for tail classes. Such an\nimbalance causes mode collapse and reduces the diversity of synthesized images\nfor tail classes. For class-conditional diffusion models trained on imbalanced\ndata, we aim to improve the diversity of tail class images without compromising\nthe fidelity and diversity of head class images. We achieve this by introducing\ntwo deceptively simple but highly effective contrastive loss functions.\nFirstly, we employ an unsupervised InfoNCE loss utilizing negative samples to\nincrease the distance/dissimilarity among synthetic images, particularly for\ntail classes. To further enhance the diversity of tail classes, our second loss\nis an MSE loss that contrasts class-conditional generation with unconditional\ngeneration at large timesteps. This second loss makes the denoising process\ninsensitive to class conditions for the initial steps, which enriches tail\nclasses through knowledge sharing from head classes. Conditional-unconditional\nalignment has been shown to enhance the performance of long-tailed GAN. We are\nthe first to adapt such alignment to diffusion models. We successfully\nleveraged contrastive learning for class-imbalanced diffusion models. Our\ncontrastive learning framework is easy to implement and outperforms standard\nDDPM and alternative methods for class-imbalanced diffusion models across\nvarious datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and\nImageNetLT.", "AI": {"tldr": "引入两种对比损失函数来提高类不平衡扩散模型中尾部类别图像的多样性，同时保持头部类别图像的质量和多样性，实验结果在多个数据集上优于标准 DDPM 和其他方法。", "motivation": "解决训练数据在类条件图像合成中存在的长尾分布问题，避免模式崩塌，提高尾部类别图像的多样性。", "method": "使用无监督的 InfoNCE 损失和 MSE 损失两种对比损失函数，增加合成图像之间的距离和多样性，特别是对于尾部类别。", "result": "对比损失框架易于实现，并在多个数据集上超过了标准 DDPM 和其他方法，包括 CIFAR10/100-LT, PlacesLT, TinyImageNetLT 和 ImageNetLT。", "conclusion": "成功地将对比学习应用到类不平衡的扩散模型中，提高了尾部类别图像的多样性，同时保持了头部类别图像的质量和多样性。"}}
{"id": "2507.09104", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09104", "abs": "https://arxiv.org/abs/2507.09104", "authors": ["Taolin Zhang", "Maosong Cao", "Alexander Lam", "Songyang Zhang", "Kai Chen"], "title": "CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards", "comment": null, "summary": "Recently, the role of LLM-as-judge in evaluating large language models has\ngained prominence. However, current judge models suffer from narrow\nspecialization and limited robustness, undermining their capacity for\ncomprehensive evaluations. In this work, we present CompassJudger-2, a novel\ngeneralist judge model that overcomes these limitations via a task-driven,\nmulti-domain data curation strategy. Central to our approach is supervising\njudgment tasks with verifiable rewards, guiding intrinsic critical reasoning\nthrough rejection sampling to foster robust, generalizable judgment\ncapabilities. We introduce a refined learning objective with margin policy\ngradient loss to enhance performance. Empirically, CompassJudger-2 achieves\nsuperior results across multiple judge and reward benchmarks, and our 7B model\ndemonstrates competitive judgment accuracy with significantly larger models\nlike DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a\ncomprehensive benchmark evaluating cross-domain judgment accuracy and rank\nconsistency to standardize judge model evaluation. These contributions advance\nrobust, scalable LLM judgment and establish new performance and evaluation\nstandards.", "AI": {"tldr": "This paper introduces CompassJudger-2, a generalist judge model designed to overcome the narrow specialization and limited robustness of current judge models by using a task-driven, multi-domain data curation and refined learning objective.", "motivation": "Current LLM-as-judge models lack broad specialization and robustness, which limits their ability to conduct comprehensive evaluations. This paper aims to address these issues.", "method": "Central to our approach is supervising judgment tasks with verifiable rewards and using a task-driven, multi-domain data curation strategy. We also introduced a refined learning objective with margin policy gradient loss.", "result": "CompassJudger-2 achieves superior results across multiple judge and reward benchmarks, demonstrating competitive judgment accuracy comparable to significantly larger models.", "conclusion": "The contributions of this work advance robust, scalable LLM judgment capabilities and establish new performance and evaluation standards, offering a significant step in enhancing the robustness and generalizability of judge models."}}
{"id": "2507.09068", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.09068", "abs": "https://arxiv.org/abs/2507.09068", "authors": ["Dell Zhang", "Xiangyu Chen", "Jixiang Luo", "Mengxi Jia", "Changzhi Sun", "Ruilong Ren", "Jingren Liu", "Hao Sun", "Xuelong Li"], "title": "Infinite Video Understanding", "comment": null, "summary": "The rapid advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have ushered in remarkable progress in video understanding.\nHowever, a fundamental challenge persists: effectively processing and\ncomprehending video content that extends beyond minutes or hours. While recent\nefforts like Video-XL-2 have demonstrated novel architectural solutions for\nextreme efficiency, and advancements in positional encoding such as HoPE and\nVideoRoPE++ aim to improve spatio-temporal understanding over extensive\ncontexts, current state-of-the-art models still encounter significant\ncomputational and memory constraints when faced with the sheer volume of visual\ntokens from lengthy sequences. Furthermore, maintaining temporal coherence,\ntracking complex events, and preserving fine-grained details over extended\nperiods remain formidable hurdles, despite progress in agentic reasoning\nsystems like Deep Video Discovery. This position paper posits that a logical,\nalbeit ambitious, next frontier for multimedia research is Infinite Video\nUnderstanding -- the capability for models to continuously process, understand,\nand reason about video data of arbitrary, potentially never-ending duration. We\nargue that framing Infinite Video Understanding as a blue-sky research\nobjective provides a vital north star for the multimedia, and the wider AI,\nresearch communities, driving innovation in areas such as streaming\narchitectures, persistent memory mechanisms, hierarchical and adaptive\nrepresentations, event-centric reasoning, and novel evaluation paradigms.\nDrawing inspiration from recent work on long/ultra-long video understanding and\nseveral closely related fields, we outline the core challenges and key research\ndirections towards achieving this transformative capability.", "AI": {"tldr": "本文提出视频理解的一个未来研究方向：无界视频理解。此方向将推动AI研究在多个领域的发展。", "motivation": "由于大型语言模型和多模态扩展在视频理解中的快速进展，但面对时长较长的视频，现有模型在计算和内存方面仍受限。本文旨在探讨无界视频理解作为未来多媒体和AI研究领域的长远目标。", "method": "本文并未详细描述具体的方法，而是讨论了目前视频理解领域的挑战，并提出了无界视频理解这一未来研究方向。", "result": "本文并未展示具体结果，而是详细分析了当前技术的局限性。", "conclusion": "文章呼吁多媒体和AI研究社区将无界视频理解视为研究标杆，推动在流媒体架构、持久记忆机制、层次化自适应表示、以事件为中心的推理等方面的创新。"}}
{"id": "2507.09155", "categories": ["cs.CL", "cs.AI", "68T50, 68T07"], "pdf": "https://arxiv.org/pdf/2507.09155", "abs": "https://arxiv.org/abs/2507.09155", "authors": ["Ali Vosoughi", "Ayoub Shahnazari", "Yufeng Xi", "Zeliang Zhang", "Griffin Hess", "Chenliang Xu", "Niaz Abdolrahim"], "title": "OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering", "comment": "10 pages, 6 figures, 5 tables. Code and dataset available at\n  https://github.com/niaz60/OpenXRD. Project webpage:\n  https://niaz60.github.io/OpenXRD/", "summary": "This work presents OPENXRD, an open-book pipeline designed for\ncrystallography question answering, which integrates textual prompts with\nconcise supporting content generated by GPT-4.5. Instead of using scanned\ntextbooks, which may lead to copyright issues, OPENXRD generates compact,\ndomain-specific references that help smaller models understand key concepts in\nX-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217\nexpert-level XRD questions by comparing different vision-language models,\nincluding GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,\nunder both closed-book (without supporting material) and open-book (with\nsupporting material) conditions. Our experimental results show significant\naccuracy improvements in models that use the GPT-4.5-generated summaries,\nparticularly those with limited prior training in crystallography. OPENXRD uses\nknowledge from larger models to fill knowledge gaps in crystallography and\nshows that AI-generated texts can help smaller models reason more effectively\nin scientific tasks. While the current version of OPENXRD focuses on text-based\ninputs, we also explore future extensions such as adding real crystal diagrams\nor diffraction patterns to improve interpretation in specialized materials\nscience contexts. Overall, OPENXRD shows that specialized open-book systems can\nbe useful in materials science and provides a foundation for broader natural\nlanguage processing (NLP) tools in critical scientific fields.", "AI": {"tldr": "OPENXRD是为晶体学问答任务设计的开放图书管道，通过使用AI生成的辅助材料显著提高了模型的理解能力和准确性，为科学领域的NLP工具奠定了基础。", "motivation": "使用扫描教科书可能会导致版权问题，因此，本研究开发了OPENXRD，用于提升较小语言模型在晶体学领域中的理解能力。", "method": "本研究提出了OPENXRD，一个开放图书管道，用于晶体学问答任务。该管道结合了文本提示和由GPT-4.5生成的简洁辅助内容。为了规避版权问题，OPENXRD生成紧凑的领域相关的参考内容，帮助较小的语言模型理解X射线衍射的关键概念。", "result": "实验结果表明，使用GPT-4.5生成的总结内容显著提升了语言模型在专家级别X射线衍射问题上的准确率，特别是在前期训练较少的模型中。", "conclusion": "OPENXRD证明了使用AI生成文本可以帮助较小模型更有效地进行科学任务推理，并为未来更广泛的自然语言处理工具提供了基础。"}}
{"id": "2507.09071", "categories": ["cs.CV", "I.2.10"], "pdf": "https://arxiv.org/pdf/2507.09071", "abs": "https://arxiv.org/abs/2507.09071", "authors": ["Tharun Adithya Srikrishnan", "Deval Shah", "Steven K. Reinhardt"], "title": "BlindSight: Harnessing Sparsity for Efficient VLMs", "comment": null, "summary": "Large vision-language models (VLMs) enable the joint processing of text and\nimages. However, the inclusion of vision data significantly expands the prompt\nlength. Along with the quadratic complexity of the attention computation, this\nresults in a longer prefill duration. An approach to mitigate this bottleneck\nis to leverage the inherent sparsity in the attention computation. In our\nanalysis of attention patterns in VLMs, we observe that a substantial portion\nof layers exhibit minimal cross-image attention, except through attention-sink\ntokens per image. These sparse attention patterns fall into distinct\ncategories: sink-only, document mask and a hybrid document-sink mask. Based on\nthis, we propose BlindSight: a training-free approach to optimize VLM inference\nusing a input template-aware attention sparsity mask. We utilize samples from a\ndataset to derive a prompt-agnostic sparsity categorization for every attention\nhead. We evaluate the proposed technique using VLMs such as Qwen2-VL,\nQwen2.5-VL and Gemma-3. BlindSight results in a 32%-41% reduction in FLOPs on\naverage with -2%-+2% accuracy compared to the original model in most evaluated\nmulti-image understanding benchmarks.", "AI": {"tldr": "研究提出BlindSight，一种优化VLM推理的方法，无需训练，利用输入模板感知注意力稀疏性掩码，减少了计算量，并保持了模型准确度。", "motivation": "大型视觉语言模型（VLM）在处理文本和图像时面临预填充时间长的问题，主要是由于视觉数据的加入导致提示长度增加。提出的方法旨在通过利用注意力计算中的固有稀疏性来缓解这一瓶颈。", "method": "通过分析VLM中的注意力模式，发现在大多数层中存在极少的跨图像注意力，仅通过每个图像的注意力汇点。基于此，提出BlindSight：一种无需训练的输入模板感知注意力稀疏性掩码，以优化VLM推理过程。", "result": "BlindSight在大多数评估的多图像理解基准上实现了平均32%-41%的计算量（FLOPs）减少，与原始模型相比准确度在-2%-+2%之间。", "conclusion": "BlindSight展示了在不牺牲模型性能的情况下显著减少VLM推理计算量的潜力，为未来的视觉语言模型设计提供了重要的方法和思路。"}}
{"id": "2507.09157", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09157", "abs": "https://arxiv.org/abs/2507.09157", "authors": ["Bhavinkumar Vinodbhai Kuwar", "Bikrant Bikram Pratap Maurya", "Priyanshu Gupta", "Nitin Choudhury"], "title": "PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning", "comment": null, "summary": "Detecting deception in strategic dialogues is a complex and high-stakes task\ndue to the subtlety of language and extreme class imbalance between deceptive\nand truthful communications. In this work, we revisit deception detection in\nthe Diplomacy dataset, where less than 5% of messages are labeled deceptive. We\nintroduce a lightweight yet effective model combining frozen BERT embeddings,\ninterpretable linguistic and game-specific features, and a Positive-Unlabeled\n(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is\ntailored for situations where only a small portion of deceptive messages are\nlabeled, and the majority are unlabeled. Our model achieves a new best macro F1\nof 0.60 while reducing trainable parameters by over 650x. Through comprehensive\nevaluations and ablation studies across seven models, we demonstrate the value\nof PU learning, linguistic interpretability, and speaker-aware representations.\nNotably, we emphasize that in this problem setting, accurately detecting\ndeception is more critical than identifying truthful messages. This priority\nguides our choice of PU learning, which explicitly models the rare but vital\ndeceptive class.", "AI": {"tldr": "本文提出了一种新的欺诈检测模型PU-Lie，在Diplomacy数据集中取得了0.60的新基准宏观F1值，同时将可训练参数减少了650多倍。", "motivation": "面对语言的微妙性和欺骗性与真实交流之间极端的类别不平衡问题，本文重新审视了Diplomacy数据集中欺诈检测的问题，其中标注的欺骗消息少于5%。", "method": "本文提出了一种轻量且有效的模型，该模型结合了冻结的BERT嵌入、可解释的语言特征和与游戏相关的特征，以及正样本-未标注样本（PU）学习目标。PU-Lie模型专门为只有少量欺骗性信息被标注的情况设计，大多数数据为未标注状态。", "result": "该模型在七个模型的综合评估和消融研究中，取得了新基准的0.60宏观F1值。", "conclusion": "研究强调在这种问题设置中准确检测欺骗比识别真实信息更重要。PU-Lie模型有效地降低了可训练参数，实现了欺诈检测的目标。"}}
{"id": "2507.09081", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09081", "abs": "https://arxiv.org/abs/2507.09081", "authors": ["Zhenyu Yu", "Mohd Yamani Idna Idris", "Hua Wang", "Pei Wang", "Junyi Chen", "Kun Wang"], "title": "From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion", "comment": null, "summary": "Quantitative remote sensing inversion aims to estimate continuous surface\nvariables-such as biomass, vegetation indices, and evapotranspiration-from\nsatellite observations, supporting applications in ecosystem monitoring, carbon\naccounting, and land management. With the evolution of remote sensing systems\nand artificial intelligence, traditional physics-based paradigms are giving way\nto data-driven and foundation model (FM)-based approaches. This paper\nsystematically reviews the methodological evolution of inversion techniques,\nfrom physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods\n(e.g., deep learning, multimodal fusion), and further to foundation models\n(e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application\nscenarios, and limitations of each paradigm, with emphasis on recent FM\nadvances in self-supervised pretraining, multi-modal integration, and\ncross-task adaptation. We also highlight persistent challenges in physical\ninterpretability, domain generalization, limited supervision, and uncertainty\nquantification. Finally, we envision the development of next-generation\nfoundation models for remote sensing inversion, emphasizing unified modeling\ncapacity, cross-domain generalization, and physical interpretability.", "AI": {"tldr": "本文回顾了遥感反演技术从物理模型向机器学习再向基础模型的演变，讨论了各方法的优缺点及其面临的挑战，并对未来发展提出了展望。", "motivation": "随着遥感系统和人工智能的发展，传统的基于物理的方法正在被数据驱动和基于基础模型的方法所取代。本文旨在系统回顾这些演变，并重点分析近期基础模型在自监督预训练、多模态整合和跨任务适应方面的进展。", "method": "论文系统地回顾了反演技术方法的演变过程，从物理模型（如PROSPECT, SCOPE, DART）到机器学习方法（如深度学习，多模态融合），再到基础模型（如SatMAE, GFM, mmEarth）。", "result": "文章比较了各个范式在建模假设、应用场景和限制方面的特点，并且凸显了在物理可解释性、领域泛化、有限监督和不确定性量化方面的持续挑战。", "conclusion": "论文展望了下一代遥感反演的基础模型的发展，强调统一建模能力、跨领域泛化和物理可解释性的重要性。"}}
{"id": "2507.09174", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09174", "abs": "https://arxiv.org/abs/2507.09174", "authors": ["Shuo Yang", "Zijian Yu", "Zhenzhe Ying", "Yuqin Dai", "Guoqing Wang", "Jun Lan", "Jinfeng Xu", "Jinze Li", "Edith C. H. Ngai"], "title": "RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking", "comment": null, "summary": "The rapid proliferation of multimodal misinformation presents significant\nchallenges for automated fact-checking systems, especially when claims are\nambiguous or lack sufficient context. We introduce RAMA, a novel\nretrieval-augmented multi-agent framework designed for verifying multimedia\nmisinformation. RAMA incorporates three core innovations: (1) strategic query\nformulation that transforms multimodal claims into precise web search queries;\n(2) cross-verification evidence aggregation from diverse, authoritative\nsources; and (3) a multi-agent ensemble architecture that leverages the\ncomplementary strengths of multiple multimodal large language models and prompt\nvariants. Extensive experiments demonstrate that RAMA achieves superior\nperformance on benchmark datasets, particularly excelling in resolving\nambiguous or improbable claims by grounding verification in retrieved factual\nevidence. Our findings underscore the necessity of integrating web-based\nevidence and multi-agent reasoning for trustworthy multimedia verification,\npaving the way for more reliable and scalable fact-checking solutions. RAMA\nwill be publicly available at https://github.com/kalendsyang/RAMA.git.", "AI": {"tldr": "RAMA是一个新的检索增强的多代理框架，用于核查多媒体虚假信息，实验显示，它在含糊或不可能的声明验证上表现出色。", "motivation": "多模式虚假信息的迅速蔓延给自动化事实核查系统带来了挑战，尤其是在声明含糊或缺乏足够上下文的情况下。因此，作者提出了RAMA，旨在解决这一问题。", "method": "RAMA是一个结合了三项核心创新的检索增强型多代理框架，用于验证多媒体虚假信息：(1) 将多模式声明转化为精确的网络搜索查询；(2) 从多样化的权威来源汇总交叉验证证据；(3) 一个多代理集成架构，利用多个多模态语言模型和提示变体的互补优势。", "result": "实验结果表明，RAMA在基准数据集上表现出色，特别擅长解决含糊或不可能的声明，通过检索的事实证据来验证。", "conclusion": "研究结果强调，为了实现可信的多媒体验证，需要整合基于网络的证据和多代理推理，为更可靠和可扩展的事实核查解决方案开辟道路。"}}
{"id": "2507.09082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09082", "abs": "https://arxiv.org/abs/2507.09082", "authors": ["Seungwoo Kim", "Khai Loong Aw", "Klemen Kotar", "Cristobal Eyzaguirre", "Wanhee Lee", "Yunong Liu", "Jared Watrous", "Stefan Stojanov", "Juan Carlos Niebles", "Jiajun Wu", "Daniel L. K. Yamins"], "title": "Taming generative video models for zero-shot optical flow extraction", "comment": "Project webpage: https://neuroailab.github.io/projects/kl_tracing", "summary": "Extracting optical flow from videos remains a core computer vision problem.\nMotivated by the success of large general-purpose models, we ask whether frozen\nself-supervised video models trained only for future frame prediction can be\nprompted, without fine-tuning, to output flow. Prior work reading out depth or\nillumination from video generators required fine-tuning, which is impractical\nfor flow where labels are scarce and synthetic datasets suffer from a\nsim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm,\nwhich can obtain point-wise correspondences by injecting a small tracer\nperturbation into a next-frame predictor and tracking its propagation, we\nextend this idea to generative video models. We explore several popular\narchitectures and find that successful zero-shot flow extraction in this manner\nis aided by three model properties: (1) distributional prediction of future\nframes (avoiding blurry or noisy outputs); (2) factorized latents that treat\neach spatio-temporal patch independently; and (3) random-access decoding that\ncan condition on any subset of future pixels. These properties are uniquely\npresent in the recent Local Random Access Sequence (LRAS) architecture.\nBuilding on LRAS, we propose KL-tracing: a novel test-time procedure that\ninjects a localized perturbation into the first frame, rolls out the model one\nstep, and computes the Kullback-Leibler divergence between perturbed and\nunperturbed predictive distributions. Without any flow-specific fine-tuning,\nour method outperforms state-of-the-art models on real-world TAP-Vid DAVIS\ndataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid\nKubric (4.7% relative improvement). Our results indicate that counterfactual\nprompting of controllable generative video models is a scalable and effective\nalternative to supervised or photometric-loss approaches for high-quality flow.", "AI": {"tldr": "研究通过冻结自监督未来帧预测视频模型，并提出KL追踪技术，来实现无需微调的零样本光流提取，结果在多个数据集上优于现有方法。", "motivation": "研究动机在于探索现有的大规模预训练模型能否在不进行特定调整的情况下直接用于提取光流。现有提取深度或光照特征的方法需要针对视频生成模型进行微调，但这在光流提取任务中是不可实现的，因为标注数据稀缺，且合成数据集存在从虚拟到现实的差距。", "method": "提出的方法基于Local Random Access Sequence (LRAS)架构，通过KL追踪技术，在测试时向第一帧注入局部扰动，然后计算扰动前后的预测分布的Kullback-Leibler散度。这种方法取代了繁琐的微调过程，表现出优越性。", "result": "本研究探讨了通过冻结自监督视频模型来提取无监督光流的可能性。该方法受到反事实世界模型（CWM）的启发，主要通过在下一帧预测器中注入少量追踪扰动并追踪其传播，来获得点到点的对应关系。研究发现，成功实现零样本光流提取的模型需要具备三种特性：分布预测未来帧的能力（避免模糊或噪声输出）、独立处理每个时空补丁的因素隐变量、以及能够以任意子集未来像素为条件的随机访问解码。研究提出的KL追踪方法，在现实世界和合成数据集上均取得了良好的效果。研究结果表明，通过反事实提示的可控制视频生成模型，可以作为监督学习或光度损失方法的替代方案，用于高质量光流提取。", "conclusion": "研究结论强调了该方法的有效性和相比已有的光流提取方法的优点。在无需针对光流进行特定调整的情况下，本方法在多个数据集上均取得了显著优于现有模型的表现。"}}
{"id": "2507.09185", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09185", "abs": "https://arxiv.org/abs/2507.09185", "authors": ["Ameen Ali", "Shahar Katz", "Lior Wolf", "Ivan Titov"], "title": "Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models", "comment": null, "summary": "Large language models (LLMs) often develop learned mechanisms specialized to\nspecific datasets, such as reliance on domain-specific correlations, which\nyield high-confidence predictions without generalizable reasoning. While\nbeneficial in one setting, these dataset-specific mechanisms typically degrade\nperformance when models encounter novel tasks or distributions. In this work,\nwe introduce a fine-tuning approach designed to enhance generalization by\nidentifying and pruning neurons associated with dataset-specific mechanisms in\ntransformer-based LLMs. Our method employs Integrated Gradients to quantify\neach neuron's influence on high-confidence predictions, pinpointing those that\ndisproportionately contribute to dataset-specific performance without\nsupporting robust, transferable reasoning. Selectively pruning these neurons\ncompels the model to depend on generalizable representations. Evaluated across\nmultiple-choice benchmarks, our pruning-based fine-tuning significantly\nenhances performance, surpassing prior (non-pruning) adaptation methods.", "AI": {"tldr": "本文提出了一种通过识别和修剪与特定数据集相关的神经元来改进大型语言模型的泛化能力的微调方法。实验结果显示，该方法在多项选择基准测试中的表现优于之前的适应方法。", "motivation": "大型语言模型倾向于依赖特定数据集的特定机制，这虽然在特定情况下是有益的，但在遇到新任务或数据分布时会降低性能。本文旨在提高模型的泛化能力。", "method": "Structure", "result": "{\"tldr\": \"本文提出了一种通过识别和修剪与特定数据集相关的神经元来改进大型语言模型的泛化能力的微调方法。实验结果显示，该方法在多项选择基准测试中的表现优于之前的适应方法。\", \"motivation\": \"大型语言模型倾向于依赖特定数据集的特定机制，这虽然在特定情况下是有益的，但在遇到新任务或数据分布时会降低性能。本文旨在提高模型的泛化能力。\", \"method\": \"本文方法使用Integrated Gradients来量化每个神经元对高置信度预测的影响，识别那些促进特定数据集性能但不支持稳健、可转移推理的神经元，并选择性地修剪它们。\", \"result\": \"实验结果显示，修剪神经元后模型在多项选择基准测试中的性能显著提高，并优于之前的非修剪适应方法。\", \"conclusion\": \"通过该方法，可以使模型更依赖于通用表达，从而提高其泛化能力。\"}", "conclusion": "通过该方法，可以使模型更依赖于通用表达，从而提高其泛化能力。"}}
{"id": "2507.09092", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09092", "abs": "https://arxiv.org/abs/2507.09092", "authors": ["Ram S Iyer", "Narayan S Iyer", "Rugmini Ammal P"], "title": "MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks", "comment": "12 pages, 10 figures", "summary": "With the intervention of machine vision in our crucial day to day necessities\nincluding healthcare and automated power plants, attention has been drawn to\nthe internal mechanisms of convolutional neural networks, and the reason why\nthe network provides specific inferences. This paper proposes a novel post-hoc\nvisual explanation method called MI CAM based on activation mapping. Differing\nfrom previous class activation mapping based approaches, MI CAM produces\nsaliency visualizations by weighing each feature map through its mutual\ninformation with the input image and the final result is generated by a linear\ncombination of weights and activation maps. It also adheres to producing causal\ninterpretations as validated with the help of counterfactual analysis. We aim\nto exhibit the visual performance and unbiased justifications for the model\ninferencing procedure achieved by MI CAM. Our approach works at par with all\nstate-of-the-art methods but particularly outperforms some in terms of\nqualitative and quantitative measures. The implementation of proposed method\ncan be found on https://anonymous.4open.science/r/MI-CAM-4D27", "AI": {"tldr": "本文提出了一种基于互信息和激活映射的新后验视觉解释方法MI CAM，它能可视化说明卷积神经网络的推理过程，并且在多个标准上超越了现有的方法。", "motivation": "随着机器视觉在医疗保健和自动化工厂等日常必需品中的应用，人们对卷积神经网络的内部机制以及网络提供特定推理的原因产生了兴趣。", "method": "MI CAM基于激活映射提出了一种新的后验视觉解释方法。与之前的基于类别激活映射的方法不同，MI CAM通过特征图与输入图像的互信息进行加权，最终结果由权重和激活图的线性组合生成。此外，它还能通过反事实分析来验证其因果解释。", "result": "所提出的方法在视觉性能和模型推理过程的无偏解释方面展示了表现。该方法在定性和定量测量上与所有最先进的方法相当，尤其在某些方面超过了它们。", "conclusion": "MI CAM方法提供了一种新的视角来解释卷积神经网络的推理过程，并在定性和定量评估标准上表现出优异性能。"}}
{"id": "2507.09205", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09205", "abs": "https://arxiv.org/abs/2507.09205", "authors": ["Leiyu Pan", "Bojian Xiong", "Lei Yang", "Renren Jin", "Shaowei Zhang", "Yue Chen", "Ling Shi", "Jiang Zhou", "Junru Wu", "Zhen Wang", "Jianxiang Peng", "Juesi Xiao", "Tianyu Dong", "Zhuowen Han", "Zhuo Chen", "Sangjee Dondrub", "Caizang Tai", "Haixing Zhao", "Huaque Cairang", "Suonan Cairang", "Rou Te", "Lengben Zhaxi", "Gazang Zhaxi", "Zhonglin Ye", "Yuhui Zheng", "Chunyan Peng", "Secha Jia", "Pema Tashi", "Cizhen Jiacuo", "Pema Dorjee", "Hongkai Liu", "Pema Yanggon", "Tsehang Dorjee", "Jiaxin Han", "Qiongying Hu", "Jilin Man", "Huanke You", "Yuqi Ren", "Duo La", "Deyi Xiong"], "title": "Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training", "comment": null, "summary": "Large language models have achieved remarkable progress across many\nlanguages. However, Tibetan, as a representative low-resource language, is\nparticularly underrepresented in existing models due to the scarcity of\nhigh-quality training corpora. To address this gap, we curate the largest\nTibetan pre-training corpus to date, aggregating data from diverse sources and\napplying a dedicated data cleaning and processing pipeline tailored for\nTibetan. With the curated data, we continue pre/post-training a multilingual\nbase model into Banzhida, a multilingual large language model that advances\ngenerative AI for Tibetan. To evaluate the Tibetan capabilities of the model,\nwe create new high-quality Tibetan benchmarks, and complement them with\nexisting public benchmarks. Experimental results demonstrate that Banzhida\nconsistently and significantly outperforms both open-source models of similar\nscale and Tibetan-tailored models across a wide range of tasks.", "AI": {"tldr": "Researchers developed Banzhida, a Tibetan-tailored multilingual large language model using the largest Tibetan pre-training corpus. It outperforms other similar models in various tasks.", "motivation": "To bridge the gap in performance for Tibetan, a low-resource language, given the scarcity of high-quality training corpora for existing language models.", "method": "The researchers created the largest Tibetan pre-training corpus to date by aggregating data from various sources and applying a dedicated data cleaning and processing pipeline for Tibetan. They then pre/post-trained a multilingual base model to develop Banzhida, a multilingual large language model tailored for Tibetan.", "result": "Banzhida demonstrates significant performance improvement over both open-source models of similar scale and Tibetan-tailored models across various tasks.", "conclusion": "The creation of the Tibetan pre-training corpus and the development of Banzhida contribute to advancing generative AI for Tibetan, showing promising results in enhancing the performance of models for low-resource languages."}}
{"id": "2507.09097", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09097", "abs": "https://arxiv.org/abs/2507.09097", "authors": ["Yunsoo Kim", "Jinge Wu", "Honghan Wu"], "title": "RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated promising performance\nin chest X-ray (CXR) analysis. To enhance human-computer interaction, several\nstudies have incorporated radiologists' eye gaze, typically through heatmaps or\ntextual prompts. However, these methods often overlook the sequential order of\neye movements, which could provide valuable insights by highlighting both the\nareas of interest and the order in which they are examined. In this work, we\npropose a novel approach called RadEyeVideo that integrates radiologists'\neye-fixation data as a video sequence, capturing both the temporal and spatial\ndynamics of their gaze. We evaluate this method in CXR report generation and\ndisease diagnosis using three general-domain, open-source LVLMs with video\ninput capabilities. When prompted with eye-gaze videos, model performance\nimproves by up to 24.6% in the report generation task and on average 15.2% for\nboth tasks using scaled evaluation metrics. Notably, RadEyeVideo enhanced an\nopen-domain LVLM model, LLaVA-OneVision, to surpass task-specific medical LVLMs\nsuch as MAIRA-2 and CheXagent, trained on large Chest X-ray data. This work\nhighlights that domain expert's knowledge (eye-gaze information in this case),\nwhen effectively integrated with LVLMs, can significantly enhance\ngeneral-domain models' capabilities in clinical tasks. RadEyeVideo is a step\ntoward a scalable human-centered approach of utilizing LVLMs in medical image\nanalytics.", "AI": {"tldr": "该研究提出了RadEyeVideo方法，将放射科医生的眼动数据作为视频序列整合，用以指导视觉-语言模型（LVLMs）进行胸腔X光（CXR）分析和疾病诊断，提升了任务表现。", "motivation": "现有的研究在胸部X光片分析中使用视觉-语言模型时，仅通过热图或文本提示结合放射科医生的眼动数据，忽略了眼动的顺序信息，而这些顺序信息也能提供重要见解。因此，本研究旨在通过将眼动数据以视频序列形式加入，提升模型在生成报告和疾病诊断的具体任务表现。", "method": "研究使用将放射科医生的眼动数据作为视频序列的方法，称为RadEyeVideo，并在三个通用LVLMs上进行胸腔X光报告生成和疾病诊断任务的评估。", "result": "在使用这种新的方法后，模型在报告生成任务上的表现提高了最多24.6%，在两个任务上平均提升了15.2%。该方法甚至使通用领域的LVLM模型的性能超过了专用于医学领域的模型。", "conclusion": "该工作展示了当领域专家的特定知识（在这里是眼动信息）有效整合到视觉-语言模型中时，可以显著提升模型在临床任务中的能力。RadEyeVideo这一方法是对一种可扩展的、以人为中心的医疗影像分析应用视觉语言模型方法的推进。"}}
{"id": "2507.09225", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.09225", "abs": "https://arxiv.org/abs/2507.09225", "authors": ["Biagio Scalingi", "Chiara Barattieri di San Pietro", "Paolo Canal", "Valentina Bambini"], "title": "MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis", "comment": "27 pages, 5 figures", "summary": "Visual metaphors of climate change (e.g., melting glaciers depicted as a\nmelting ice grenade) are regarded as valuable tools for addressing the\ncomplexity of environmental challenges. However, few studies have examined\ntheir impact on communication, also due to scattered availability of material.\nHere, we present a novel database of Metaphors of Climate Change in Images\n(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal\nimages and enriched with human ratings. For each image, we collected values of\ndifficulty, efficacy, artistic quality, and emotional arousal from human\nrating, as well as number of tags generated by participants to summarize the\nmessage. Semantic and emotion variables were further derived from the tags via\nNatural Language Processing. Visual metaphors were rated as more difficult to\nunderstand, yet more aesthetically pleasant than literal images, but did not\ndiffer in efficacy and arousal. The latter for visual metaphors, however, was\nhigher in participants with higher Need For Cognition. Furthermore, visual\nmetaphors received more tags, often referring to entities not depicted in the\nimage, and elicited words with more positive valence and greater dominance than\nliteral images. These results evidence the greater cognitive load of visual\nmetaphors, which nevertheless might induce positive effects such as deeper\ncognitive elaboration and abstraction compared to literal stimuli. Furthermore,\nwhile they are not deemed as more effective and arousing, visual metaphors seem\nto generate superior aesthetic appreciation and a more positively valenced\nexperience. Overall, this study contributes to understanding the impact of\nvisual metaphors of climate change both by offering a database for future\nresearch and by elucidating a cost-benefit trade-off to take into account when\nshaping environmental communication.", "AI": {"tldr": "研究开发了一个气候变迁视觉隐喻数据库，表明相较于字面图像，视觉隐喻在理解和认知上带来更大压力，但其审美愉悦性和激发积极情感体验的优势作为回报。", "motivation": "尽管气候变迁的视觉隐喻被视为应对环境挑战的有效工具，但很少有研究探讨它们对沟通的影响，部分原因是材料分散。本研究旨在填补这一空白。", "method": "本研究创建了一个名为MetaClimage的气候变迁视觉隐喻数据库，并将其与字面图像配对，添加了人类评分。对于每张图像，收集了难度、有效性、艺术质量、情感唤醒度等方面的人类评分，以及参与者生成的用于总结信息的标签数量。通过自然语言处理从标签中衍生出了语义和情感变量。", "result": "视觉隐喻在理解难度上较高，但在审美愉悦性上也更高，而在有效性和唤醒度方面与字面图像无异。参与者的认知需求较高时，视觉隐喻的唤醒度较高。视觉隐喻更常收到更多的标签，往往涉及到图像中未描绘的实体，并引发了更多具有积极情感和支配力的词语。这些结果表明视觉隐喻的认知压力更大，但可能会引发更深的思考和抽象思考。尽管它们不被认为更有效或更具唤醒度，但视觉隐喻似乎能产生更好的审美欣赏和积极体验。", "conclusion": "本研究通过对气候变迁的视觉隐喻的影响进行探讨，为未来研究提供了一个数据库，并揭示了在塑造环境沟通时需要考虑的利弊权衡。"}}
{"id": "2507.09102", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09102", "abs": "https://arxiv.org/abs/2507.09102", "authors": ["Yiyang Chen", "Shanshan Zhao", "Lunhao Duan", "Changxing Ding", "Dacheng Tao"], "title": "Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning", "comment": "Accepted by ICCV 2025", "summary": "Diffusion-based models, widely used in text-to-image generation, have proven\neffective in 2D representation learning. Recently, this framework has been\nextended to 3D self-supervised learning by constructing a conditional point\ngenerator for enhancing 3D representations. However, its performance remains\nconstrained by the 3D diffusion model, which is trained on the available 3D\ndatasets with limited size. We hypothesize that the robust capabilities of\ntext-to-image diffusion models, particularly Stable Diffusion (SD), which is\ntrained on large-scale datasets, can help overcome these limitations. To\ninvestigate this hypothesis, we propose PointSD, a framework that leverages the\nSD model for 3D self-supervised learning. By replacing the SD model's text\nencoder with a 3D encoder, we train a point-to-image diffusion model that\nallows point clouds to guide the denoising of rendered noisy images. With the\ntrained point-to-image diffusion model, we use noise-free images as the input\nand point clouds as the condition to extract SD features. Next, we train a 3D\nbackbone by aligning its features with these SD features, thereby facilitating\ndirect semantic learning. Comprehensive experiments on downstream point cloud\ntasks and ablation studies demonstrate that the SD model can enhance point\ncloud self-supervised learning. Code is publicly available at\nhttps://github.com/wdttt/PointSD.", "AI": {"tldr": "本文提出PointSD框架，旨在利用经过大规模数据集训练的Stable Diffusion模型来改善3D点云的自监督学习效果。", "motivation": "假设强大的文本到图像扩散模型，特别是经过大规模数据集训练的Stable Diffusion (SD)模型，能够帮助克服3D扩散模型在学习点云表示上的局限性。", "method": "通过将SD模型的文本编码器替换为3D编码器，训练了一个由点云引导渲染的噪声图像去噪的点云到图像扩散模型。接着，使用无噪声图像作为输入，点云作为条件，提取SD特征，并通过使3D骨架的特征与这些SD特征对齐来进行直接语义学习。", "result": "在下游点云任务和消融研究中的全面实验表明，SD模型能够增强点云的无监督学习。", "conclusion": "PointSD框架通过使用SD模型来改进3D自监督学习，并证明了该模型的有效性。"}}
{"id": "2507.09245", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09245", "abs": "https://arxiv.org/abs/2507.09245", "authors": ["Deshan Sumanathilaka", "Sameera Perera", "Sachithya Dharmasiri", "Maneesha Athukorala", "Anuja Dilrukshi Herath", "Rukshan Dias", "Pasindu Gamage", "Ruvan Weerasinghe", "Y. H. P. P. Priyadarshana"], "title": "Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources", "comment": "13 pages, 3 Tables, 3 figures", "summary": "The Swa-bhasha Resource Hub provides a comprehensive collection of data\nresources and algorithms developed for Romanized Sinhala to Sinhala\ntransliteration between 2020 and 2025. These resources have played a\nsignificant role in advancing research in Sinhala Natural Language Processing\n(NLP), particularly in training transliteration models and developing\napplications involving Romanized Sinhala. The current openly accessible data\nsets and corresponding tools are made publicly available through this hub. This\npaper presents a detailed overview of the resources contributed by the authors\nand includes a comparative analysis of existing transliteration applications in\nthe domain.", "AI": {"tldr": "本文介绍了Swa-bhasha Resource Hub，这是一个提供罗马化僧伽罗语到僧伽罗语转写资源的综合平台，对僧伽罗语自然语言处理研究有显著贡献。", "motivation": "研究的动机是提供一个公开且易于访问的平台，以便研究人员和开发者能够利用现有的数据集和工具进行僧伽罗语的自然语言处理研究，特别是转写模型的培训和应用开发。", "method": "本文主要介绍了Swa-bhasha Resource Hub，这是2020至2025年间为罗马化僧伽罗语到僧伽罗语的转写开发的数据资源和算法的综合集合。这些资源对于推动僧伽罗语自然语言处理的研究起到了很大作用，尤其是训练转写模型和开发涉及罗马化僧伽罗语的应用。", "result": "本文提供了作者贡献资源的详细介绍，以及领域内现有转写应用的比较分析。", "conclusion": "Swa-bhasha Resource Hub为罗马化僧伽罗语到僧伽罗语的转写研究提供了宝贵的资源，促进了该领域的发展。"}}
{"id": "2507.09105", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09105", "abs": "https://arxiv.org/abs/2507.09105", "authors": ["Maoxiao Ye", "Xinfeng Ye", "Mano Manoharan"], "title": "Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production", "comment": null, "summary": "Earlier Sign Language Production (SLP) models typically relied on\nautoregressive methods that generate output tokens one by one, which inherently\nprovide temporal alignment. Although techniques like Teacher Forcing can\nprevent model collapse during training, they still cannot solve the problem of\nerror accumulation during inference, since ground truth is unavailable at that\nstage. In contrast, more recent approaches based on diffusion models leverage\nstep-by-step denoising to enable high-quality generation. However, the\niterative nature of these models and the requirement to denoise entire\nsequences limit their applicability in real-time tasks like SLP. To address it,\nwe apply a hybrid approach combining autoregressive and diffusion models to SLP\nfor the first time, leveraging the strengths of both models in sequential\ndependency modeling and output refinement. To capture fine-grained body\nmovements, we design a Multi-Scale Pose Representation module that separately\nextracts detailed features from distinct articulators and integrates them via a\nMulti-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal\nAttention mechanism that utilizes joint-level confidence scores to dynamically\nguide the pose generation process, improving accuracy and robustness. Extensive\nexperiments on the PHOENIX14T and How2Sign datasets demonstrate the\neffectiveness of our method in both generation quality and real-time streaming\nefficiency.", "AI": {"tldr": "We introduce a hybrid autoregressive and diffusion model approach for SLP, which includes modules for multi-scale pose representation and confidence-aware causal attention to enhance the accuracy and real-time performance of the model.", "motivation": "The motivation comes from the need to improve Sign Language Production models which face issues such as error accumulation during inference and the inability to provide real-time output.", "method": "Our method involves a hybrid approach combining autoregressive and diffusion models for Sign Language Production (SLP). It includes a Multi-Scale Pose Representation module for capturing fine-grained body movements, as well as a Confidence-Aware Causal Attention mechanism to improve accuracy.", "result": "Experiments on the PHOENIX14T and How2Sign datasets show that our method performs well in terms of generation quality and real-time streaming efficiency.", "conclusion": "We conclude that the hybrid framework is effective in addressing the limitations of previous models by integrating the temporal modeling ability of autoregressive models with the refinement capabilities of diffusion models."}}
{"id": "2507.09259", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09259", "abs": "https://arxiv.org/abs/2507.09259", "authors": ["Yuchen Su", "Yonghua Zhu", "Yang Chen", "Diana Benavides-Prado", "Michael Witbrock"], "title": "Psychology-Driven Enhancement of Humour Translation", "comment": null, "summary": "Humour translation plays a vital role as a bridge between different cultures,\nfostering understanding and communication. Although most existing Large\nLanguage Models (LLMs) are capable of general translation tasks, these models\nstill struggle with humour translation, which is especially reflected through\nlinguistic interference and lacking humour in translated text. In this paper,\nwe propose a psychology-inspired Humour Decomposition Mechanism (HDM) that\nutilises Chain-of-Thought (CoT) to imitate the ability of the human thought\nprocess, stimulating LLMs to optimise the readability of translated humorous\ntexts. Moreover, we integrate humour theory in HDM to further enhance the\nhumorous elements in the translated text. Our automatic evaluation experiments\non open-source humour datasets demonstrate that our method significantly\nimproves the quality of humour translation, yielding average gains of 7.75\\% in\nhumour, 2.81\\% in fluency, and 6.13\\% in coherence of the generated text.", "AI": {"tldr": "提出了一种心理学启发的幽默分解机制，显著提升了幽默翻译的质量。", "motivation": "尽管现有的大型语言模型（LLMs）能够完成一般翻译任务，但在幽默翻译方面仍存在语言干扰和缺乏幽默性的问题。", "method": "采用心理学启发的幽默分解机制（HDM），利用CoT模仿人类思维过程，以优化翻译文本的可读性，并整合幽默理论进一步增强翻译文本中的幽默元素。", "result": "在开源幽默数据集上的自动评估实验表明，该方法显著提高了幽默翻译的质量，平均提高了7.75%的幽默性，2.81%的流畅度和6.13%的一致性。", "conclusion": "该研究证明了使用幽默分解机制和结合幽默理论可以有效提高幽默文本翻译的质量和效果。"}}
{"id": "2507.09111", "categories": ["cs.CV", "cs.HC", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.09111", "abs": "https://arxiv.org/abs/2507.09111", "authors": ["Di Wen", "Kunyu Peng", "Kailun Yang", "Yufan Chen", "Ruiping Liu", "Junwei Zheng", "Alina Roitberg", "Rainer Stiefelhagen"], "title": "RoHOI: Robustness Benchmark for Human-Object Interaction Detection", "comment": "Benchmarks, datasets, and code will be made publicly available at\n  https://github.com/Kratos-Wen/RoHOI", "summary": "Human-Object Interaction (HOI) detection is crucial for robot-human\nassistance, enabling context-aware support. However, models trained on clean\ndatasets degrade in real-world conditions due to unforeseen corruptions,\nleading to inaccurate prediction. To address this, we introduce the first\nrobustness benchmark for HOI detection, evaluating model resilience under\ndiverse challenges. Despite advances, current models struggle with\nenvironmental variability, occlusion, and noise. Our benchmark, RoHOI, includes\n20 corruption types based on HICO-DET and V-COCO datasets and a new\nrobustness-focused metric. We systematically analyze existing models in the\nrelated field, revealing significant performance drops under corruptions. To\nimprove robustness, we propose a Semantic-Aware Masking-based Progressive\nLearning (SAMPL) strategy to guide the model to be optimized based on holistic\nand partial cues, dynamically adjusting the model's optimization to enhance\nrobust feature learning. Extensive experiments show our approach outperforms\nstate-of-the-art methods, setting a new standard for robust HOI detection.\nBenchmarks, datasets, and code will be made publicly available at\nhttps://github.com/Kratos-Wen/RoHOI.", "AI": {"tldr": "Introduces RoHOI, a robustness benchmark for HOI detection, and SAMPL, a strategy that enhances the robustness of models to handle diverse corruptions effectively.", "motivation": "To create a robust benchmark for Human-Object Interaction (HOI) detection under various corruptive conditions, as existing models perform poorly in real-world scenarios.", "method": "Semantic-Aware Masking-based Progressive Learning (SAMPL) strategy to optimize models based on holistic and partial cues.", "result": "The proposed SAMPL strategy significantly outperforms state-of-the-art methods in HOI detection robustness.", "conclusion": "The new robustness-focused metric in the RoHOI benchmark highlights performance drops under corruptions, demonstrating the need for more robust strategies like SAMPL in HOI detection models."}}
{"id": "2507.09282", "categories": ["cs.CL", "cs.CR", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09282", "abs": "https://arxiv.org/abs/2507.09282", "authors": ["Dominika Woszczyk", "Ranya Aloufi", "Soteris Demetriou"], "title": "ClaritySpeech: Dementia Obfuscation in Speech", "comment": "Accepted at Interspeech 2025", "summary": "Dementia, a neurodegenerative disease, alters speech patterns, creating\ncommunication barriers and raising privacy concerns. Current speech\ntechnologies, such as automatic speech transcription (ASR), struggle with\ndementia and atypical speech, further challenging accessibility. This paper\npresents a novel dementia obfuscation in speech framework, ClaritySpeech,\nintegrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to\ncorrect dementia-affected speech while preserving speaker identity in low-data\nenvironments without fine-tuning. Results show a 16% and 10% drop in mean F1\nscore across various adversarial settings and modalities (audio, text, fusion)\nfor ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We\nalso find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15\nfor ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and\naccessibility.", "AI": {"tldr": "研究提出了一种新型的痴呆症言语加密框架，可以校正受痴呆症影响的言语，同时在低数据环境下无需微调并保持说话者身份，有效提高了隐私性和访问性。", "motivation": "痴呆症改变了言语模式，造成沟通障碍并引起隐私问题，现有的语音技术难以处理这一问题，本研究旨在解决这些问题，提高隐私性和访问性。", "method": "提出了一种名为ClaritySpeech的框架，整合了自动语音识别（ASR）、文本加密和零样本文本转语音（TTS）技术，以在无需微调的低数据环境下校正受痴呆症影响的语音并保持说话者身份。", "result": "该系统在ADReSS和ADReSSo数据集中分别将WER从0.73降低到0.08和0.15，增强了语音质量，并在不同的对抗设置和模式下使F1评分下降了16%和10%。", "conclusion": "该研究证明了ClaritySpeech框架的有效性，该框架在处理患有痴呆症人群的言语障碍方面具有潜在的应用价值，有助于改善其沟通隐私和无障碍访问。"}}
{"id": "2507.09118", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09118", "abs": "https://arxiv.org/abs/2507.09118", "authors": ["Linlan Huang", "Xusheng Cao", "Haori Lu", "Yifan Meng", "Fei Yang", "Xialei Liu"], "title": "Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning", "comment": "Accepted at ICCV 2025", "summary": "Continual learning aims to enable models to learn sequentially from\ncontinuously incoming data while retaining performance on previously learned\ntasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting\nstrong capabilities across various downstream tasks, there has been growing\ninterest in leveraging CLIP for continual learning in such scenarios. Most\nexisting works overlook the inherent modality gap in CLIP, a key factor in its\ngeneralization and adaptability. In this paper, we analyze the variations in\nthe modality gap during the fine-tuning of vision-language pre-trained models.\nOur observations reveal that the modality gap effectively reflects the extent\nto which pre-trained knowledge is preserved. Based on these insights, we\npropose a simple yet effective method, MG-CLIP, that improves CLIP's\nperformance in class-incremental learning. Our approach leverages modality gap\npreservation to mitigate forgetting and modality gap compensation to enhance\nthe capacity for new data, introducing a novel modality-gap-based perspective\nfor continual learning. Extensive experiments on multiple benchmarks\ndemonstrate that our method outperforms existing approaches without requiring\nadditional replay data. Our code is available at\nhttps://github.com/linlany/MindtheGap.", "AI": {"tldr": "该论文提出了一种名为MG-CLIP的新方法，旨在解决连续学习中视觉-语言模型的模态差距问题。通过维护模态差距和补偿新的数据，该方法能够有效减少遗忘并提高连续学习的表现。该方法在多个基准测试中表现出色，且不需要额外的回放数据。", "motivation": "论文的动力源于利用CLIP进行连续学习时，忽视了模态差距这一关键因素，该因素影响着模型的泛化和适应能力。", "method": "论文中的方法是分析视觉-语言预训练模型在微调过程中的模态差距变化，并提出MG-CLIP，它通过保持模态差距和补偿新的数据来减少遗忘并增强新的数据容量。", "result": "实验结果表明，MG-CLIP在多个基准测试中的表现优于现有方法，且无需额外的回放数据。", "conclusion": "该结论表明，作者提出的MG-CLIP通过维持模态差距来减少遗忘，并通过补偿新的数据来增强连续学习的性能，为连续学习提供了一种新颖的模态差距视角。"}}
{"id": "2507.09424", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09424", "abs": "https://arxiv.org/abs/2507.09424", "authors": ["Cathy Jiao", "Yijun Pan", "Emily Xiao", "Daisy Sheng", "Niket Jain", "Hanzhang Zhao", "Ishita Dasgupta", "Jiaqi W. Ma", "Chenyan Xiong"], "title": "DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models", "comment": null, "summary": "Data attribution methods quantify the influence of training data on model\noutputs and are becoming increasingly relevant for a wide range of LLM research\nand applications, including dataset curation, model interpretability, data\nvaluation. However, there remain critical gaps in systematic LLM-centric\nevaluation of data attribution methods. To this end, we introduce DATE-LM (Data\nAttribution Evaluation in Language Models), a unified benchmark for evaluating\ndata attribution methods through real-world LLM applications. DATE-LM measures\nattribution quality through three key tasks -- training data selection,\ntoxicity/bias filtering, and factual attribution. Our benchmark is designed for\nease of use, enabling researchers to configure and run large-scale evaluations\nacross diverse tasks and LLM architectures. Furthermore, we use DATE-LM to\nconduct a large-scale evaluation of existing data attribution methods. Our\nfindings show that no single method dominates across all tasks, data\nattribution methods have trade-offs with simpler baselines, and method\nperformance is sensitive to task-specific evaluation design. Finally, we\nrelease a public leaderboard for quick comparison of methods and to facilitate\ncommunity engagement. We hope DATE-LM serves as a foundation for future data\nattribution research in LLMs.", "AI": {"tldr": "DATE-LM是用于评估语言模型中数据归因方法的基准平台，通过该平台，研究者对现有方法进行了评估，发现各个方法各有优劣。", "motivation": "该研究的动机是系统性地评估语言模型中的数据归因方法，填补在这一领域中的关键空白。", "method": "内容介绍了一种名为DATE-LM的数据归因评估基准，它用于评估语言模型中的数据归因方法。DATE-LM通过三个关键任务——训练数据选择、毒性/偏见过滤和事实归因来衡量归因质量。研究者还利用DATE-LM对现有的数据归因方法进行了大规模评估。", "result": "研究结果显示没有单一的方法能在所有任务中占主导地位，数据归因方法相对于简单的基线方法有其优缺点，并且其性能对任务特定的评估设计敏感。", "conclusion": "希望DATE-LM能够成为未来语言模型中数据归因研究的基础。"}}
{"id": "2507.09122", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09122", "abs": "https://arxiv.org/abs/2507.09122", "authors": ["Chuan Guo", "Inwoo Hwang", "Jian Wang", "Bing Zhou"], "title": "SnapMoGen: Human Motion Generation from Expressive Texts", "comment": "Project Webpage: https://snap-research.github.io/SnapMoGen/", "summary": "Text-to-motion generation has experienced remarkable progress in recent\nyears. However, current approaches remain limited to synthesizing motion from\nshort or general text prompts, primarily due to dataset constraints. This\nlimitation undermines fine-grained controllability and generalization to unseen\nprompts. In this paper, we introduce SnapMoGen, a new text-motion dataset\nfeaturing high-quality motion capture data paired with accurate, expressive\ntextual annotations. The dataset comprises 20K motion clips totaling 44 hours,\naccompanied by 122K detailed textual descriptions averaging 48 words per\ndescription (vs. 12 words of HumanML3D). Importantly, these motion clips\npreserve original temporal continuity as they were in long sequences,\nfacilitating research in long-term motion generation and blending. We also\nimprove upon previous generative masked modeling approaches. Our model,\nMoMask++, transforms motion into multi-scale token sequences that better\nexploit the token capacity, and learns to generate all tokens using a single\ngenerative masked transformer. MoMask++ achieves state-of-the-art performance\non both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the\nability to process casual user prompts by employing an LLM to reformat inputs\nto align with the expressivity and narration style of SnapMoGen. Project\nwebpage: https://snap-research.github.io/SnapMoGen/", "AI": {"tldr": "Introduce SnapMoGen, a new dataset for text-to-motion generation, and MoMask++, a generative transformer model, which together improve the quality and controllability of synthesized motion.", "motivation": "The primary motivation is to address the limitations of current text-to-motion models and to enable more detailed control and better generalization through a new, richly annotated dataset and improved model architecture.", "method": "SnapMoGen, a dataset of text-motion featuring 20K motion clips and 122K detailed textual descriptions, is introduced. MoMask++, a generative masked transformer model, converts motion into multi-scale token sequences for enhanced performance.", "result": "MoMask++ achieves state-of-the-art performance on both the HumanML3D and SnapMoGen benchmarks, demonstrating improved fine-grained controllability and long-term motion synthesis capabilities.", "conclusion": "The introduction of SnapMoGen and the MoMask++ model represents a significant step towards more sophisticated and controllable text-to-motion synthesis, facilitating research into detailed motion and long-term sequence generation."}}
{"id": "2507.09470", "categories": ["cs.CL", "cs.AI", "68T07"], "pdf": "https://arxiv.org/pdf/2507.09470", "abs": "https://arxiv.org/abs/2507.09470", "authors": ["Mingchuan Yang", "Ziyuan Huang"], "title": "Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models", "comment": "29 pages, 5 tables", "summary": "This study explores the optimization of the DRAGON Longformer base model for\nclinical text classification, specifically targeting the binary classification\nof medical case descriptions. A dataset of 500 clinical cases containing\nstructured medical observations was used, with 400 cases for training and 100\nfor validation. Enhancements to the pre-trained\njoeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter\ntuning, domain-specific preprocessing, and architectural adjustments. Key\nmodifications involved increasing sequence length from 512 to 1024 tokens,\nadjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5\nto 8, and incorporating specialized medical terminology. The optimized model\nachieved notable performance gains: accuracy improved from 72.0% to 85.2%,\nprecision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from\n71.0% to 85.2%. Statistical analysis confirmed the significance of these\nimprovements (p < .001). The model demonstrated enhanced capability in\ninterpreting medical terminology, anatomical measurements, and clinical\nobservations. These findings contribute to domain-specific language model\nresearch and offer practical implications for clinical natural language\nprocessing applications. The optimized model's strong performance across\ndiverse medical conditions underscores its potential for broad use in\nhealthcare settings.", "AI": {"tldr": "通过调整超参数、预处理和模型架构，优化了joeranbosma/dragon-longformer-base-mixed-domain模型，用于临床文本的二元分类，模型在准确率、精确度、召回率以及F1-score上都有显著提升。", "motivation": "研究通过对DRAGON Longformer基线模型进行优化，特别是针对包含结构化医疗观察的临床案例描述的二元分类任务，以提高模型在临床应用中的效果。", "method": "使用包含500个临床案例的数据集进行研究，分为400个训练样本和100个验证样本。优化策略包括增加序列长度至1024个token，调整学习率，延长训练周期，以及加入专用的医学术语。", "result": "优化后的模型在准确率、精确度、召回率和F1-score上都有显著提升，分别为85.2%，84.1%，86.3%以及85.2%。这些改进经统计分析证明具有显著性(p < .001)。", "conclusion": "研究结果对特定领域的语言模型研究作出了贡献，并为临床自然语言处理的实际应用提供了有益见解。优化后的模型因其强大的表现，可望广泛应用于医疗环境。"}}
{"id": "2507.09139", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09139", "abs": "https://arxiv.org/abs/2507.09139", "authors": ["Dewen Zhang", "Tahir Hussain", "Wangpeng An", "Hayaru Shouno"], "title": "PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP Alignment", "comment": "Preprint", "summary": "Human pose estimation traditionally relies on architectures that encode\nkeypoint priors, limiting their generalization to novel poses or unseen\nkeypoints. Recent language-guided approaches like LocLLM reformulate keypoint\nlocalization as a vision-language task, enabling zero-shot generalization\nthrough textual descriptions. However, LocLLM's linear projector fails to\ncapture complex spatial-textual interactions critical for high-precision\nlocalization. To address this, we propose PoseLLM, the first Large Language\nModel (LLM)-based pose estimation framework that replaces the linear projector\nwith a nonlinear MLP vision-language connector. This lightweight two-layer MLP\nwith GELU activation enables hierarchical cross-modal feature transformation,\nenhancing the fusion of visual patches and textual keypoint descriptions.\nTrained exclusively on COCO data, PoseLLM achieves 77.8 AP on the COCO\nvalidation set, outperforming LocLLM by +0.4 AP, while maintaining strong\nzero-shot generalization on Human-Art and MPII. Our work demonstrates that a\nsimple yet powerful nonlinear connector significantly boosts localization\naccuracy without sacrificing generalization, advancing the state-of-the-art in\nlanguage-guided pose estimation. Code is available at\nhttps://github.com/Ody-trek/PoseLLM.", "AI": {"tldr": "PoseLLM, 一种大语言模型驱动的全新姿势估计方法，采用非线性MLP连接视觉和语言特征，提升了定位精度，并保持了出色的零样本泛化能力，超越了现有的LocLLM方法。", "motivation": "改进传统的依赖编码关键点先验知识的姿势估计架构，而这些架构限制了对新颖姿势或未见过关键点的泛化能力。为此引入了语言指导的方法，并通过改进的视觉-语言转换进一步提高了精度。", "method": "PoseLLM, 一种基于大语言模型的姿势估计框架，通过用非线性MLP连接视觉和语言特征来取代线性投影器，改进了复杂的空间-文本交互问题，提升定位精度。", "result": "在COCO验证集上实现了77.8 AP，比LocLLM高出+0.4 AP，并在Human-Art和MPII上保持了强大的零样本泛化能力。", "conclusion": "简单而强大的非线性连接显著提升了定位精度，同时没有牺牲其泛化能力，推进了语言引导姿势估计的最新技术。"}}
{"id": "2507.09474", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09474", "abs": "https://arxiv.org/abs/2507.09474", "authors": ["Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault"], "title": "The CoNLL-2013 Shared Task on Grammatical Error Correction", "comment": "12 pages", "summary": "The CoNLL-2013 shared task was devoted to grammatical error correction. In\nthis paper, we give the task definition, present the data sets, and describe\nthe evaluation metric and scorer used in the shared task. We also give an\noverview of the various approaches adopted by the participating teams, and\npresent the evaluation results.", "AI": {"tldr": "本文概述了CoNLL-2013共享任务，该任务聚焦语法错误纠正。", "motivation": "该研究的动机是定义语法规错任务、提供数据集、评估指标和评分程序，并总结参与者的方法。", "method": "论文没有详细描述具体的研究方法，而是介绍了共享任务中各参与团队采用的各种方法。", "result": "论文提出了共享任务的评估结果，但没有具体的结果数据。", "conclusion": "论文总结了CoNLL-2013共享任务中的各种方法及其评估结果，该任务侧重于语法错误纠正。"}}
{"id": "2507.09144", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09144", "abs": "https://arxiv.org/abs/2507.09144", "authors": ["Zhimin Liao", "Ping Wei", "Ruijie Zhang", "Shuaijia Chen", "Haoxuan Wang", "Ziyang Ren"], "title": "$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting", "comment": null, "summary": "Forecasting the evolution of 3D scenes and generating unseen scenarios via\noccupancy-based world models offers substantial potential for addressing corner\ncases in autonomous driving systems. While tokenization has revolutionized\nimage and video generation, efficiently tokenizing complex 3D scenes remains a\ncritical challenge for 3D world models. To address this, we propose\n$I^{2}$-World, an efficient framework for 4D occupancy forecasting. Our method\ndecouples scene tokenization into intra-scene and inter-scene tokenizers. The\nintra-scene tokenizer employs a multi-scale residual quantization strategy to\nhierarchically compress 3D scenes while preserving spatial details. The\ninter-scene tokenizer residually aggregates temporal dependencies across\ntimesteps. This dual design preserves the compactness of 3D tokenizers while\nretaining the dynamic expressiveness of 4D tokenizers. Unlike decoder-only\nGPT-style autoregressive models, $I^{2}$-World adopts an encoder-decoder\narchitecture. The encoder aggregates spatial context from the current scene and\npredicts a transformation matrix to enable high-level control over scene\ngeneration. The decoder, conditioned on this matrix and historical tokens,\nensures temporal consistency during generation. Experiments demonstrate that\n$I^{2}$-World achieves state-of-the-art performance, outperforming existing\nmethods by 25.1\\% in mIoU and 36.9\\% in IoU for 4D occupancy forecasting while\nexhibiting exceptional computational efficiency: it requires merely 2.9 GB of\ntraining memory and achieves real-time inference at 37.0 FPS. Our code is\navailable on https://github.com/lzzzzzm/II-World.", "AI": {"tldr": "提出了$I^{2}$-World框架，通过将场景分隔符分为场景内和场景间分隔符来优化4D占用预测，从而有效处理自动驾驶中超复杂3D场景的问题，并在性能和效率上取得显著成就。", "motivation": "当前高效地对复杂3D场景进行分词化仍是一个挑战，该问题阻碍了3D世界模型的发展，特别是对于自动驾驶中罕见情况的处理。", "method": "利用多尺度残差量化策略进行3D场景的层次化压缩，同时保留空间细节。同时通过残差时间聚合跨时间步长的依赖关系。此外，采用了编码器-解码器架构，以确保生成时的时空一致性。", "result": "在4D占用预测中，与其他方法相比，$I^{2}$-World在mIoU和IoU上分别提高了25.1%和36.9%。实验还表明，该框架在计算效率方面表现出色，只需2.9GB的训练内存和37.0 FPS的实时推理。", "conclusion": "研究证实了$I^{2}$-World在自动驾驶场景中解决复杂3D场景预测问题上的出色性能与效率。"}}
{"id": "2507.09477", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09477", "abs": "https://arxiv.org/abs/2507.09477", "authors": ["Yangning Li", "Weizhi Zhang", "Yuyao Yang", "Wei-Chieh Huang", "Yaozu Wu", "Junyu Luo", "Yuanchen Bei", "Henry Peng Zou", "Xiao Luo", "Yusheng Zhao", "Chunkit Chan", "Yankai Chen", "Zhongfen Deng", "Yinghui Li", "Hai-Tao Zheng", "Dongyuan Li", "Renhe Jiang", "Ming Zhang", "Yangqiu Song", "Philip S. Yu"], "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs", "comment": "submitted to ARR May", "summary": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.", "AI": {"tldr": "本文综述了推理增强的RAG方法和RAG增强推理方法，提出了协同RAG-推理框架，进一步提升知识密集型任务的性能。", "motivation": "解决现有的检索增强生成模型在需要多步推理的问题上表现不佳的问题，同时避免纯推理方法可能出现的事实性错误。通过综合这两种方法，提出一个统一的推理-检索视角。", "method": "通过将推理增强注入检索增强生成(RAG)的各个阶段来改进事实生成模型，并探讨不同类型检索知识如何为复杂推理提供前提和扩展上下文。同时，提出了协同RAG-推理框架，其中代理式大型语言模型迭代地交织搜索和推理以实现知识密集型基准的最先进性能。", "result": "论文分类了方法、数据集和开放挑战，以及提出的研究方向，以实现更有效、更具多模态适应性、值得信赖且以人类为中心的深度RAG-推理系统。", "conclusion": "RAG和推理方法的结合为未来的研究提供了新的方向和挑战，旨在开发更有效的知识密集型任务系统。"}}
{"id": "2507.09168", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09168", "abs": "https://arxiv.org/abs/2507.09168", "authors": ["Haiming Zhu", "Yangyang Xu", "Chenshu Xu", "Tingrui Shen", "Wenxi Liu", "Yong Du", "Jun Yu", "Shengfeng He"], "title": "Stable Score Distillation", "comment": null, "summary": "Text-guided image and 3D editing have advanced with diffusion-based models,\nyet methods like Delta Denoising Score often struggle with stability, spatial\ncontrol, and editing strength. These limitations stem from reliance on complex\nauxiliary structures, which introduce conflicting optimization signals and\nrestrict precise, localized edits. We introduce Stable Score Distillation\n(SSD), a streamlined framework that enhances stability and alignment in the\nediting process by anchoring a single classifier to the source prompt.\nSpecifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves\ncross-prompt alignment, and introduces a constant term null-text branch to\nstabilize the optimization process. This approach preserves the original\ncontent's structure and ensures that editing trajectories are closely aligned\nwith the source prompt, enabling smooth, prompt-specific modifications while\nmaintaining coherence in surrounding regions. Additionally, SSD incorporates a\nprompt enhancement branch to boost editing strength, particularly for style\ntransformations. Our method achieves state-of-the-art results in 2D and 3D\nediting tasks, including NeRF and text-driven style edits, with faster\nconvergence and reduced complexity, providing a robust and efficient solution\nfor text-guided editing.", "AI": {"tldr": "SSD 是一种用于文本引导图像和3D编辑的高效框架，通过简化结构和使用无分类器指引（CFG）来解决稳定性、空间控制和编辑强度的问题，同时保持编辑与原始内容的一致性。", "motivation": "解决现有方法如Delta Denoising Score在图像和3D编辑中稳定性、空间控制和编辑强度方面的问题。", "method": "SSD利用无分类器指引（CFG）方程实现跨提示的对齐，并引入常量无文本分支以稳定优化过程，同时有一个增强编辑分支来提高编辑强度。", "result": "SSD在2D和3D编辑任务，如NeRF和文本驱动风格编辑中达到了最先进的结果，具有更快的收敛性和较低的复杂度。", "conclusion": "SSD提供了一种用于文本引导编辑的稳健和高效解决方案，通过简化结构和提高一致性，解决了现有编辑方法的局限性。"}}
{"id": "2507.09482", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.09482", "abs": "https://arxiv.org/abs/2507.09482", "authors": ["Changli Wang", "Rui Wu", "Fang Yin"], "title": "ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning", "comment": null, "summary": "Human emotions are complex, with sarcasm being a subtle and distinctive form.\nDespite progress in sarcasm research, sarcasm generation remains underexplored,\nprimarily due to the overreliance on textual modalities and the neglect of\nvisual cues, as well as the mismatch between image content and sarcastic intent\nin existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm\ngeneration dataset with 4,970 samples, each containing an image, a sarcastic\ntext, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation\nframework that integrates Proximal Policy Optimization (PPO) and contrastive\nlearning. PPO utilizes reward scores from DIP to steer the generation of\nsarcastic texts, while contrastive learning encourages the model to favor\noutputs with higher reward scores. These strategies improve overall generation\nquality and produce texts with more pronounced sarcastic intent. We evaluate\nViSP across five metric sets and find it surpasses all baselines, including\nlarge language models, underscoring their limitations in sarcasm generation.\nFurthermore, we analyze the distributions of Sarcasm Scores and Factual\nIncongruity for both M2SaG and the texts generated by ViSP. The generated texts\nexhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity\n(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic\ncontent than the original dataset. % The dataset and code will be publicly\navailable. Our dataset and code will be released at\n\\textit{https://github.com/wclapply/ViSP}.", "AI": {"tldr": "M2SaG数据集和ViSP生成框架提升了讽刺文本的生成质量，尤其在提高讽刺意图表达方面。", "motivation": "鉴于现有的讽刺生成研究大多依赖于文本模态，忽略了视觉线索，以及现有数据集中图像内容与讽刺意图之间的不匹配，本研究旨在开发一个综合图像和文本的讽刺生成数据集和相应的生成模型。", "method": "本文引入了M2SaG数据集，该数据集包含4,970个样本，每个样本包括一张图片、一段讽刺文本和讽刺对象。为了基准测试M2SaG，本文提出了ViSP生成框架，该框架结合了近似策略优化（PPO）和对比学习。PPO利用DIP中的奖励分数来引导讽刺文本的生成，而对比学习鼓励模型更多生成具有更高奖励分数的输出。这些策略提高了总体生成质量和讽刺意图的表达。", "result": "ViSP在五个指标集上超越了所有基线模型，生成的文本具有更高的讽刺程度和事实不一致度。具体来说，生成的文本的平均讽刺分数为0.898，高于原始数据集的0.770；事实不一致度为0.768，高于原始的0.739。", "conclusion": "本文利用M2SaG数据集和ViSP生成框架成功增强了讽刺文本的生成质量，并展示了在讽刺意图表达方面的显著改进，揭示了大型语言模型在讽刺生成任务上的局限性。"}}
{"id": "2507.09180", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09180", "abs": "https://arxiv.org/abs/2507.09180", "authors": ["Zichun Xu", "Yuntao Li", "Zhaomin Wang", "Lei Zhuang", "Guocai Yang", "Jingdong Zhao"], "title": "Learning and Transferring Better with Depth Information in Visual Reinforcement Learning", "comment": null, "summary": "Depth information is robust to scene appearance variations and inherently\ncarries 3D spatial details. In this paper, a visual backbone based on the\nvision transformer is proposed to fuse RGB and depth modalities for enhancing\ngeneralization. Different modalities are first processed by separate CNN stems,\nand the combined convolutional features are delivered to the scalable vision\ntransformer to obtain visual representations. Moreover, a contrastive\nunsupervised learning scheme is designed with masked and unmasked tokens to\naccelerate the sample efficiency during the reinforcement learning progress.\nFor sim2real transfer, a flexible curriculum learning schedule is developed to\ndeploy domain randomization over training processes.", "AI": {"tldr": "本文提出了一种融合RGB和深度信息的视觉模型，用于增强模型泛化能力，并提出了加速学习过程的方法。", "motivation": "深度信息对于场景外观变化具有鲁棒性，并且本质上携带了3D空间细节。这种方法试图通过融合RGB和深度模态来提高模型的泛化能力。", "method": "本文提出了一种基于视觉转换器的视觉骨干网络，用于融合RGB和深度模态以增强泛化能力。不同的模态首先通过单独的CNN主干进行处理，然后将结合的卷积特征传递给可扩展的视觉转换器以获得视觉表示。此外，设计了一种对比无监督学习方案，使用掩码和未掩码标记来加速强化学习过程中的样本效率。对于sim2real迁移，开发了一种灵活的课程学习计划，在训练过程中部署领域随机化。", "result": "虽然具体的实验结果未在摘要中详细说明，但可以推测，这种融合方法在某些3D视觉任务的泛化能力上表现更好，特别是在从仿真到现实世界的迁移学习中。", "conclusion": "通过融合RGB和深度信息以及课程学习和领域随机化的应用，该方法在增强模型泛化能力和加速学习效率上显示出潜力。"}}
{"id": "2507.09485", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09485", "abs": "https://arxiv.org/abs/2507.09485", "authors": ["Junjie Liu", "Yuanhe Tian", "Yan Song"], "title": "Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis", "comment": null, "summary": "Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in\nsocial media scenarios to identify the sentiment polarity of specific aspect\nterms in a sentence. Although many existing studies leverage large language\nmodels (LLMs) to perform ABSA due to their strong context understanding\ncapabilities, they still face challenges to learn the context information in\nthe running text because of the short text, as well as the small and unbalanced\nlabeled training data, where most data are labeled with positive sentiment.\nData augmentation (DA) is a feasible strategy for providing richer contextual\ninformation, especially when using LLMs to create synthetic training data, but\nfaces challenges in ensuring a high quality of the augmented data.In this\npaper, we propose an LLM-based ABSA approach with training data\naugmentation.Specifically, an LLM is prompted to generate augmented training\ndata based on the original training data, so as to construct a new training\ndata with larger size and balanced label distributions to better train an ABSA\nmodel. Meanwhile, in order to improve the quality of the augmented data, we\npropose a reinforcement learning approach to optimize the data augmentation.\nLLM.Experiment results and further analyses on English benchmark datasets for\nABSA demonstrate the effectiveness of our approach, where superior performance\nis observed over strong baselines and most existing studies.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.09183", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09183", "abs": "https://arxiv.org/abs/2507.09183", "authors": ["Yongwei Jiang", "Yixiong Zou", "Yuhua Li", "Ruixuan Li"], "title": "Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning", "comment": "Accepted to ICCV 2025, 11 pages", "summary": "Few-Shot Class-Incremental Learning (FSCIL) faces dual challenges of data\nscarcity and incremental learning in real-world scenarios. While pool-based\nprompting methods have demonstrated success in traditional incremental\nlearning, their effectiveness in FSCIL settings remains unexplored. This paper\npresents the first study of current prompt pool methods in FSCIL tasks,\nrevealing an unanticipated performance degradation in incremental sessions.\nThrough comprehensive analysis, we identify that this phenomenon stems from\ntoken-dimension saturation: with limited data, excessive prompts compete for\ntask-relevant information, leading to model overfitting. Based on this finding,\nwe propose LGSP-Prompt (Local-Global Spatial Prompting), which innovatively\nshifts pool-based prompt learning from the token dimension to the spatial\ndimension. LGSP-Prompt generates spatial prompts by synergistically combining\nlocal spatial features and global frequency-domain representations to highlight\nkey patterns in input images. We construct two spatial prompt pools enabling\ndynamic prompt selection to maintain acquired knowledge while effectively\nlearning novel sessions. Extensive experiments demonstrate that our approach\nachieves state-of-the-art performance across multiple FSCIL benchmarks, showing\nsignificant advantages in both base knowledge preservation and incremental\nlearning. Our implementation is available at\nhttps://github.com/Jywsuperman/LGSP.", "AI": {"tldr": "本文提出了针对FSCIL问题的解决方案LGSP-Prompt。该方法克服了传统基于池的提示方法在FSCIL中遇到的性能下降问题，通过空间维度生成和选择提示，增强了模型的增量学习性能。", "motivation": "虽然基于池的提示方法在传统增量学习中已经取得了成功，但它们在FSCIL(少量样本增量学习)设置中的有效性仍有待探索。本文首次研究了当前提示池方法在FSCIL任务中的应用，发现了在增量会话中意外的性能下降。通过对这种现象的详细分析，研究人员发现，由于数据量有限，过多的提示会争夺任务相关信息，导致模型过拟合。", "method": "本文提出了一种名为LGSP-Prompt（局部-全局空间提示）的方法，该方法创造性地将基于池的提示学习从标记维度转移到空间维度。LGSP-Prompt通过协同结合局部空间特征和全局频域表示来生成空间提示，以突出输入图像中的关键模式。研究构建了两个空间提示池，以实现动态提示选择，既保持了已获得的知识，又有效地学习了新的会话。", "result": "广泛实验表明，该方法在多个FSCIL基准测试中达到了最先进的性能水平，在保持基础知识和增量学习上均展示出显著的优势。", "conclusion": "实验结果表明，LGSP-Prompt在多个FSCIL基准上的表现达到了最先进的水平，不仅在保持基础知识方面，而且在增量学习方面都显示出了显著的优势。"}}
{"id": "2507.09497", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09497", "abs": "https://arxiv.org/abs/2507.09497", "authors": ["Siyi Wu", "Zeyu Wang", "Xinyuan Song", "Zhengpeng Zhou", "Lifan Sun", "Tianyu Shi"], "title": "GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities", "comment": null, "summary": "Modern enterprise environments demand intelligent systems capable of handling\ncomplex, dynamic, and multi-faceted tasks with high levels of autonomy and\nadaptability. However, traditional single-purpose AI systems often lack\nsufficient coordination, memory reuse, and task decomposition capabilities,\nlimiting their scalability in realistic settings. To address these challenges,\nwe present \\textbf{GoalfyMax}, a protocol-driven framework for end-to-end\nmulti-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent\n(A2A) communication layer built on the Model Context Protocol (MCP), allowing\nindependent agents to coordinate through asynchronous, protocol-compliant\ninteractions. It incorporates the Experience Pack (XP) architecture, a layered\nmemory system that preserves both task rationales and execution traces,\nenabling structured knowledge retention and continual learning. Moreover, our\nsystem integrates advanced features including multi-turn contextual dialogue,\nlong-short term memory modules, and dynamic safety validation, supporting\nrobust, real-time strategy adaptation. Empirical results on complex task\norchestration benchmarks and case study demonstrate that GoalfyMax achieves\nsuperior adaptability, coordination, and experience reuse compared to baseline\nframeworks. These findings highlight its potential as a scalable, future-ready\nfoundation for multi-agent intelligent systems.", "AI": {"tldr": "研究提出了GoalfyMax框架，以新的通信方式和记忆系统提高多智能体之间的协调和适应能力，其在实验中展示了优于基线框架的表现。", "motivation": "为了应对传统单一用途AI系统在现实环境中扩展性受限的问题，该研究提出了GoalfyMax框架，以解决智能体之间缺乏协调、记忆复用和任务分解能力的问题。", "method": "GoalfyMax采用基于Model Context Protocol (MCP)的Agent-to-Agent (A2A)通信层，实现智能体之间的异步协调。此外，它采用Experience Pack (XP)架构，这是一种多层记忆系统，用来保留任务理性和执行轨迹，支持结构化知识保存和持续学习。系统还能支援多轮上下文对话、长短时记忆模块及动态安全性验证，支持健壮、实时的战略适应。", "result": "实验证明，GoalfyMax在复杂任务编排基准和案例研究中，相较于基线框架，具有更高的适应性、协调性和经验重用度。", "conclusion": "这些发现表明GoalfyMax作为一个可伸缩、具备未来技术能力的多智能体智能系统基础框架有潜在的实用价值。"}}
{"id": "2507.09184", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09184", "abs": "https://arxiv.org/abs/2507.09184", "authors": ["Qiyan Zhao", "Xiaofeng Zhang", "Yiheng Li", "Yun Xing", "Xiaosong Yuan", "Feilong Tang", "Sinan Fan", "Xuhang Chen", "Xuyao Zhang", "Dahan Wang"], "title": "MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models", "comment": "Accepted in ACM MM 2025", "summary": "Hallucinations pose a significant challenge in Large Vision Language Models\n(LVLMs), with misalignment between multimodal features identified as a key\ncontributing factor. This paper reveals the negative impact of the long-term\ndecay in Rotary Position Encoding (RoPE), used for positional modeling in\nLVLMs, on multimodal alignment. Concretely, under long-term decay, instruction\ntokens exhibit uneven perception of image tokens located at different positions\nwithin the two-dimensional space: prioritizing image tokens from the\nbottom-right region since in the one-dimensional sequence, these tokens are\npositionally closer to the instruction tokens. This biased perception leads to\ninsufficient image-instruction interaction and suboptimal multimodal alignment.\nWe refer to this phenomenon as image alignment bias. To enhance instruction's\nperception of image tokens at different spatial locations, we propose\nMCA-LLaVA, based on Manhattan distance, which extends the long-term decay to a\ntwo-dimensional, multi-directional spatial decay. MCA-LLaVA integrates the\none-dimensional sequence order and two-dimensional spatial position of image\ntokens for positional modeling, mitigating hallucinations by alleviating image\nalignment bias. Experimental results of MCA-LLaVA across various hallucination\nand general benchmarks demonstrate its effectiveness and generality. The code\ncan be accessed in https://github.com/ErikZ719/MCA-LLaVA.", "AI": {"tldr": "本文提出MCA-LLaVA方法来缓解大型视觉语言模型中的幻觉问题，通过将长距离衰减扩展到二维多方向衰减，并整合图像标记的一维序列顺序和二维空间位置。", "motivation": "解决大型视觉语言模型（LVLMs）中存在的幻觉问题，特别是由于Rotary Position Encoding（RoPE）长期衰减导致的模态对齐负面影响。", "method": "提出MCA-LLaVA方法，该方法基于曼哈顿距离，将长期衰减扩展到二维多方向空间衰减，整合了一维序列顺序和二维空间位置的图像标记，以改善指令对不同空间位置的图像标记的感知，减轻图像对齐偏差，从而缓解幻觉问题。", "result": "实验结果显示，MCA-LLaVA在不同的幻觉和通用基准测试中表现出色，展示了其有效性和通用性。", "conclusion": "MCA-LLaVA通过缓解图像对齐偏差显著提高了模型性能，减少了幻觉现象，其代码可在指定GitHub仓库获取。"}}
{"id": "2507.09506", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09506", "abs": "https://arxiv.org/abs/2507.09506", "authors": ["Junjie Wu", "Gefei Gu", "Yanan Zheng", "Dit-Yan Yeung", "Arman Cohan"], "title": "Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models", "comment": "ACL 2025 Main Conference. First 2 authors contributed equally", "summary": "Long-context language models (LCLMs) have exhibited impressive capabilities\nin long-context understanding tasks. Among these, long-context referencing -- a\ncrucial task that requires LCLMs to attribute items of interest to specific\nparts of long-context data -- remains underexplored. To bridge this gap, this\npaper proposes Referencing Evaluation for Long-context Language Models\n(Ref-Long), a novel benchmark designed to assess the long-context referencing\ncapability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the\nindexes of documents that reference a specific key, emphasizing contextual\nrelationships between the key and the documents over simple retrieval. Based on\nthe task design, we construct three subsets ranging from synthetic to realistic\nscenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs\nreveal significant shortcomings in long-context referencing, even among\nadvanced models like GPT-4o. To further investigate these challenges, we\nconduct comprehensive analyses, including human evaluations, task format\nadjustments, fine-tuning experiments, and error analyses, leading to several\nkey insights. Our data and code can be found in https://github.\ncom/wujunjie1998/Ref-Long.", "AI": {"tldr": "该论文提出了Ref-Long，一个评估LCLMs在长上下文引用中表现的新基准，实验和分析揭示了当前模型的不足。", "motivation": "由于长上下文引用是长上下文理解中的重要任务，而这一领域尚未得到充分的探索，该论文的目的在于填补这一研究空白。", "method": "该论文设计了一个新的基准测试Ref-Long，用于评估长上下文语言模型（LCLMs）的长上下文引用能力，包括合成到现实场景的三个子集。", "result": "实验结果显示，即使是像GPT-4o这样的先进模型在长上下文引用方面也存在显著不足。", "conclusion": "通过全面的分析，包括人类评估、任务格式调整、微调实验和错误分析，作者得出了几个关键见解。"}}
{"id": "2507.09200", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09200", "abs": "https://arxiv.org/abs/2507.09200", "authors": ["Trong-Thuan Nguyen", "Pha Nguyen", "Jackson Cothren", "Alper Yilmaz", "Minh-Triet Tran", "Khoa Luu"], "title": "THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage", "comment": null, "summary": "The rapid proliferation of video in applications such as autonomous driving,\nsurveillance, and sports analytics necessitates robust methods for dynamic\nscene understanding. Despite advances in static scene graph generation and\nearly attempts at video scene graph generation, previous methods often suffer\nfrom fragmented representations, failing to capture fine-grained spatial\ndetails and long-range temporal dependencies simultaneously. To address these\nlimitations, we introduce the Temporal Hierarchical Cyclic Scene Graph (THYME)\napproach, which synergistically integrates hierarchical feature aggregation\nwith cyclic temporal refinement to address these limitations. In particular,\nTHYME effectively models multi-scale spatial context and enforces temporal\nconsistency across frames, yielding more accurate and coherent scene graphs. In\naddition, we present AeroEye-v1.0, a novel aerial video dataset enriched with\nfive types of interactivity that overcome the constraints of existing datasets\nand provide a comprehensive benchmark for dynamic scene graph generation.\nEmpirically, extensive experiments on ASPIRe and AeroEye-v1.0 demonstrate that\nthe proposed THYME approach outperforms state-of-the-art methods, offering\nimproved scene understanding in ground-view and aerial scenarios.", "AI": {"tldr": "THYME方法结合分层特征聚合和循环时间细化，解决了多尺度空间上下文建模及帧间时间一致性问题，大幅提升了场景理解的准确性。", "motivation": "现有的场景图生成方法通常无法同时捕获精细的空间细节和长期的时间依赖性。THYME旨在解决这些问题，在多种场景中提升动态场景理解。", "method": "我们提出了Temporal Hierarchical Cyclic Scene Graph (THYME) 方法，该方法通过分层特征聚合和循环时间细化相结合的方式，来解决多尺度空间上下文建模及帧间时间一致性问题。", "result": "在ASPIRe和AeroEye-v1.0数据集上的广泛实验表明，提出的THYME方法大幅超越现有方法，提供更准确和连贯的场景图，并改进了地平视角和航拍场景的场景理解能力。", "conclusion": "THYME为动态场景理解引入了一种有效的方法，可以在多个尺度上建模空间上下文，并在帧间维护时间一致性，提高了场景理解的准确性。"}}
{"id": "2507.09509", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09509", "abs": "https://arxiv.org/abs/2507.09509", "authors": ["Patrícia Schmidtová", "Niyati Bafna", "Seth Aycock", "Gianluca Vico", "Wiktor Kamzela", "Katharina Hämmerl", "Vilém Zouhar"], "title": "How Important is `Perfect' English for Machine Translation Prompts?", "comment": null, "summary": "Large language models (LLMs) have achieved top results in recent machine\ntranslation evaluations, but they are also known to be sensitive to errors and\nperturbations in their prompts. We systematically evaluate how both humanly\nplausible and synthetic errors in user prompts affect LLMs' performance on two\nrelated tasks: Machine translation and machine translation evaluation. We\nprovide both a quantitative analysis and qualitative insights into how the\nmodels respond to increasing noise in the user prompt.\n  The prompt quality strongly affects the translation performance: With many\nerrors, even a good prompt can underperform a minimal or poor prompt without\nerrors. However, different noise types impact translation quality differently,\nwith character-level and combined noisers degrading performance more than\nphrasal perturbations. Qualitative analysis reveals that lower prompt quality\nlargely leads to poorer instruction following, rather than directly affecting\ntranslation quality itself. Further, LLMs can still translate in scenarios with\noverwhelming random noise that would make the prompt illegible to humans.", "AI": {"tldr": "研究发现提示词的质量对大规模语言模型的机器翻译性能有显著影响，且不同类型的噪声对翻译质量的影响也不同。", "motivation": "该研究旨在评估提示错误对大规模语言模型（LLMs）机器翻译性能的影响，理解不同类型的错误如何影响翻译质量，并提供有关模型如何响应用户提示中不同噪声水平的定性和定量见解。", "method": "通过系统地研究人类理解和合成错误对大规模语言模型（LLMs）在机器翻译和机器翻译评估任务上性能的影响，采用定量分析和定性分析方法来评估不同类型的噪声对翻译性能的影响。", "result": "该研究系统地评估了人类可理解误差和合成误差对大规模语言模型（LLMs）在机器翻译和机器翻译评估任务上性能的影响。研究表明，提示词的质量强烈影响翻译性能：当误差多时，即使是一个好的提示可能比没有误差的最小或较差的提示表现更差。然而，不同类型的噪声对翻译质量的影响不同，字符级别的和组合的噪声比语义结构的扰动更严重地影响性能。定性分析表明，提示词质量下降主要是由于模型较差的指令遵循，而不是直接影响翻译质量。此外，LLMs 甚至能在一个人类难以理解的提示场景中进行翻译。", "conclusion": "提示词的质量对翻译性能有强烈的影响，不同的噪声类型对性能的影响也各异。质量差的提示主要影响的是指令遵循能力而非翻译质量本身。即便在过于随机的噪声场景下，大规模语言模型仍能进行翻译。"}}
{"id": "2507.09207", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09207", "abs": "https://arxiv.org/abs/2507.09207", "authors": ["Alexander C. Ogren", "Berthy T. Feng", "Jihoon Ahn", "Katherine L. Bouman", "Chiara Daraio"], "title": "Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves", "comment": "ICCV 2025", "summary": "Wave propagation on the surface of a material contains information about\nphysical properties beneath its surface. We propose a method for inferring the\nthickness and stiffness of a structure from just a video of waves on its\nsurface. Our method works by extracting a dispersion relation from the video\nand then solving a physics-based optimization problem to find the best-fitting\nthickness and stiffness parameters. We validate our method on both simulated\nand real data, in both cases showing strong agreement with ground-truth\nmeasurements. Our technique provides a proof-of-concept for at-home health\nmonitoring of medically-informative tissue properties, and it is further\napplicable to fields such as human-computer interaction.", "AI": {"tldr": "通过视频分析表面波动，本研究提出了一种非侵入式测量结构厚度和刚度的方法，验证结果与真实值匹配良好，展示了潜在的健康监测应用价值。", "motivation": "此研究的动机在于通过分析材料表面波动来获取其下方物理特性信息，进而实现无需侵入即可测量如组织属性等信息。", "method": "我们提出了一种方法，该方法通过从视频中提取色散关系，然后解决一个基于物理的优化问题来推断结构的厚度和刚度。", "result": "该方法在模拟和实际数据上都得到了验证，显示出与真实测量值有很强的一致性。", "conclusion": "这项技术为家庭健康监测提供了概念验证，并且可以应用于诸如人机交互等领域。"}}
{"id": "2507.09536", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09536", "abs": "https://arxiv.org/abs/2507.09536", "authors": ["Daniela Kazakouskaya", "Timothee Mickus", "Janine Siewert"], "title": "Adapting Definition Modeling for New Languages: A Case Study on Belarusian", "comment": "To appear at SlavicNLP 2025", "summary": "Definition modeling, the task of generating new definitions for words in\ncontext, holds great prospect as a means to assist the work of lexicographers\nin documenting a broader variety of lects and languages, yet much remains to be\ndone in order to assess how we can leverage pre-existing models for as-of-yet\nunsupported languages. In this work, we focus on adapting existing models to\nBelarusian, for which we propose a novel dataset of 43,150 definitions. Our\nexperiments demonstrate that adapting a definition modeling systems requires\nminimal amounts of data, but that there currently are gaps in what automatic\nmetrics do capture.", "AI": {"tldr": "研究提出了一种新的定义生成数据集，并将其应用于白俄罗斯语的模型适配中。", "motivation": "定义模型生成任务在为语言学家记录不同方言和语言提供了巨大前景，但需要进一步研究如何将现有模型应用于尚未支持的语言中。", "method": "本研究提出了一个包含43,150个定义的新数据集，专注于将现有的定义生成模型适应到白俄罗斯语上。", "result": "实验表明，将定义生成系统适应到新语言需要相对较少的数据，但当前的自动评估指标仍存在一些不足。", "conclusion": "适应现有模型到白俄罗斯语中的研究表明，该过程需要的数据量相对较小，但自动评估指标尚不完善。"}}
{"id": "2507.09209", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09209", "abs": "https://arxiv.org/abs/2507.09209", "authors": ["Xiao Liang", "Di Wang", "Zhicheng Jiao", "Ronghan Li", "Pengfei Yang", "Quan Wang", "Tat-Seng Chua"], "title": "Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models", "comment": null, "summary": "The rapid advancements in Vision Language Models (VLMs) have prompted the\ndevelopment of multi-modal medical assistant systems. Despite this progress,\ncurrent models still have inherent probabilistic uncertainties, often producing\nerroneous or unverified responses-an issue with serious implications in medical\napplications. Existing methods aim to enhance the performance of Medical Vision\nLanguage Model (MedVLM) by adjusting model structure, fine-tuning with\nhigh-quality data, or through preference fine-tuning. However, these\ntraining-dependent strategies are costly and still lack sufficient alignment\nwith clinical expertise. To address these issues, we propose an\nexpert-in-the-loop framework named Expert-Controlled Classifier-Free Guidance\n(Expert-CFG) to align MedVLM with clinical expertise without additional\ntraining. This framework introduces an uncertainty estimation strategy to\nidentify unreliable outputs. It then retrieves relevant references to assist\nexperts in highlighting key terms and applies classifier-free guidance to\nrefine the token embeddings of MedVLM, ensuring that the adjusted outputs are\ncorrect and align with expert highlights. Evaluations across three medical\nvisual question answering benchmarks demonstrate that the proposed Expert-CFG,\nwith 4.2B parameters and limited expert annotations, outperforms\nstate-of-the-art models with 13B parameters. The results demonstrate the\nfeasibility of deploying such a system in resource-limited settings for\nclinical use.", "AI": {"tldr": "提出了一种无需额外训练的专家参与框架来提升医学视觉语言模型的性能和可靠性，并证明该方法在资源受限的临床环境中具有可行性。", "motivation": "医学视觉语言模型在医学应用中有产生错误或未经验证响应的问题，而现有的通过微调或模型结构调整来提升性能的方法成本高昂且缺乏与临床专业知识的充分对齐。", "method": "通过提出名为专家控制的无分类器自由引导（Expert-CFG）的专家参与框架，解决现有的医学视觉语言模型（MedVLM）无法充分与临床专业知识对齐的问题。该框架包括不确定性估计策略，用于识别不可靠的输出，并检索相关信息，辅助专家突出关键术语，应用无分类器自由引导方法来优化MedVLM的词块编码，确保调整后的输出与专家的突出内容一致。", "result": "在三个医疗视觉问答基准上的评估显示，与拥有13B参数的最先进的模型相比，拥有4.2B参数且仅使用有限的专家标注数据的Expert-CFG表现更优。", "conclusion": "评估结果表明了在资源受限的临床环境中部署该系统的可行性，同时也证明了专家参与方法在提升模型输出准确性方面的潜力。"}}
{"id": "2507.09601", "categories": ["cs.CL", "cs.AI", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2507.09601", "abs": "https://arxiv.org/abs/2507.09601", "authors": ["Hanwool Lee", "Sara Yu", "Yewon Hwang", "Jonghyun Choi", "Heejae Ahn", "Sungbum Jung", "Youngjae Yu"], "title": "NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance", "comment": "Under Review", "summary": "General-purpose sentence embedding models often struggle to capture\nspecialized financial semantics, especially in low-resource languages like\nKorean, due to domain-specific jargon, temporal meaning shifts, and misaligned\nbilingual vocabularies. To address these gaps, we introduce NMIXX (Neural\neMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual\nembedding models fine-tuned with 18.8K high-confidence triplets that pair\nin-domain paraphrases, hard negatives derived from a semantic-shift typology,\nand exact Korean-English translations. Concurrently, we release KorFinSTS, a\n1,921-pair Korean financial STS benchmark spanning news, disclosures, research\nreports, and regulations, designed to expose nuances that general benchmarks\nmiss.\n  When evaluated against seven open-license baselines, NMIXX's multilingual\nbge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and\n+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing\nother models by the largest margin, while revealing a modest trade-off in\ngeneral STS performance. Our analysis further shows that models with richer\nKorean token coverage adapt more effectively, underscoring the importance of\ntokenizer design in low-resource, cross-lingual settings. By making both models\nand the benchmark publicly available, we provide the community with robust\ntools for domain-adapted, multilingual representation learning in finance.", "AI": {"tldr": "本文介绍了NMIXX，一种用于跨语言探索金融领域的神经嵌入模型，并发布了KorFinSTS基准测试。结果显示，在金融STS任务上，NMIXX显著优于其他开源模型。这强调了分词器设计在金融领域和低资源语言中的重要性。", "motivation": "由于领域特定的行话、时间意义的转变以及双语词汇表的对齐问题，通用句子嵌入模型很难捕捉到特定于金融领域的语义，尤其是在韩语这样的低资源语言中。通过引入NMIXX，旨在填补这一空白。", "method": "介绍了一种名为NMIXX（Neural eMbeddings for Cross-lingual eXploration of Finance）的方法，这是一种跨语言嵌入模型，通过使用18.8K对高置信度的三元组进行微调，这些三元组包括领域内的近义词对、基于语义变化类型学的硬反例以及韩英精确翻译。同时发布了KorFinSTS，这是一个包含1,921对韩语金融STS基准测试，涵盖了新闻、披露、研究报告和法规，旨在揭示一般基准测试可能遗漏的细微差别。", "result": "在对七种开源基线进行评估时，NMIXX的多语言bge-m3变体在English FinSTS上获得了+0.10的Spearman's rho得分提升，在KorFinSTS上获得了+0.22的Spearman's rho得分提升，超过其他模型并且比未预适应的检查点表现更好。", "conclusion": "研究表明，具有更丰富的韩语标记覆盖范围的模型能更有效地适应，突显了在低资源跨语言情境中分词器设计的重要性。通过公开提供模型和基准，为金融领域的领域自适应和多语言表征学习提供了有力工具。"}}
{"id": "2507.09214", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09214", "abs": "https://arxiv.org/abs/2507.09214", "authors": ["Shiyi Mu", "Zichong Gu", "Hanqi Lyu", "Yilin Gao", "Shugong Xu"], "title": "Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline", "comment": "under review", "summary": "3D detection technology is widely used in the field of autonomous driving,\nwith its application scenarios gradually expanding from enclosed highways to\nopen conventional roads. For rare anomaly categories that appear on the road,\n3D detection models trained on closed sets often misdetect or fail to detect\nanomaly objects. To address this risk, it is necessary to enhance the\ngeneralization ability of 3D detection models for targets of arbitrary shapes\nand to possess the capability to filter out anomalies. The generalization of 3D\ndetection is limited by two factors: the coupled training of 2D and 3D, and the\ninsufficient diversity in the scale distribution of training samples. This\npaper proposes a Stereo-based 3D Anomaly object Detection (S3AD) algorithm,\nwhich decouples the training strategy of 3D and 2D to release the\ngeneralization ability for arbitrary 3D foreground detection, and proposes an\nanomaly scoring algorithm based on foreground confidence prediction, achieving\ntarget-level anomaly scoring. In order to further verify and enhance the\ngeneralization of anomaly detection, we use a 3D rendering method to synthesize\ntwo augmented reality binocular stereo 3D detection datasets which named\nKITTI-AR. KITTI-AR extends upon KITTI by adding 97 new categories, totaling 6k\npairs of stereo images. The KITTI-AR-ExD subset includes 39 common categories\nas extra training data to address the sparse sample distribution issue.\nAdditionally, 58 rare categories form the KITTI-AR-OoD subset, which are not\nused in training to simulate zero-shot scenarios in real-world settings, solely\nfor evaluating 3D anomaly detection. Finally, the performance of the algorithm\nand the dataset is verified in the experiments. (Code and dataset can be\nobtained at https://github.com/xxxx/xxx).", "AI": {"tldr": "本文提出了一种基于立体视觉的3D异常对象检测算法(S3AD)，解决了3D检测模型在开放道路环境下对罕见异常类别物体检测不准的问题，通过解耦3D和2D训练策略和提出异常评分算法，增强了算法的泛化能力。", "motivation": "为了提高3D检测模型对于任意形状目标的泛化能力，并具备过滤异常物体的能力，特别是处理开放道路上罕见异常类别物体的检测问题，而提出的新算法。", "method": "提出了基于立体视觉的3D异常对象检测算法(S3AD)，该算法将3D和2D的训练策略解耦，以释放任意3D前景检测的泛化能力，并提出基于前景置信度预测的异常评分算法，实现目标级异常评分。此外，为了进一步验证和增强异常检测的泛化能力，使用3D渲染方法合成了包含97个新类别的两个增强现实的双目立体3D检测数据集（命名为KITTI-AR）。", "result": "通过在合成的增强现实数据集KITTI-AR上进行实验，验证了所提出的算法和数据集在3D异常对象检测中的有效性，尤其是在罕见类别目标检测方面的优越性。", "conclusion": "实验验证了算法和数据集的性能。在增强现实数据集KITTI-AR上进行了广泛测试，显示了S3AD算法在处理罕见类别3D异常对象检测方面的优越性能。"}}
{"id": "2507.09628", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09628", "abs": "https://arxiv.org/abs/2507.09628", "authors": ["Salvatore Citraro", "Edith Haim", "Alessandra Carini", "Cynthia S. Q. Siew", "Giulio Rossetti", "Massimo Stella"], "title": "SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks", "comment": null, "summary": "We introduce SpreadPy as a Python library for simulating spreading activation\nin cognitive single-layer and multiplex networks. Our tool is designed to\nperform numerical simulations testing structure-function relationships in\ncognitive processes. By comparing simulation results with grounded theories in\nknowledge modelling, SpreadPy enables systematic investigations of how\nactivation dynamics reflect cognitive, psychological and clinical phenomena. We\ndemonstrate the library's utility through three case studies: (1) Spreading\nactivation on associative knowledge networks distinguishes students with high\nversus low math anxiety, revealing anxiety-related structural differences in\nconceptual organization; (2) Simulations of a creativity task show that\nactivation trajectories vary with task difficulty, exposing how cognitive load\nmodulates lexical access; (3) In individuals with aphasia, simulated activation\npatterns on lexical networks correlate with empirical error types (semantic vs.\nphonological) during picture-naming tasks, linking network structure to\nclinical impairments. SpreadPy's flexible framework allows researchers to model\nthese processes using empirically derived or theoretical networks, providing\nmechanistic insights into individual differences and cognitive impairments. The\nlibrary is openly available, supporting reproducible research in psychology,\nneuroscience, and education research.", "AI": {"tldr": "介绍SpreadPy，一个用于模拟认知过程扩散激活的Python库。通过三个案例研究表明其在理解认知、心理和临床现象中的作用。", "motivation": "研发SpreadPy的动机是为认知、心理和临床现象提供一个数值模拟工具，以分析激活动力学背后的机制。", "method": "我们介绍了SpreadPy，这是一个用于模拟认知单层和多层网络中扩散激活的Python库。工具旨在执行数值模拟以测试认知过程中的结构-功能关系。通过将模拟结果与知识建模中的理论进行比较，SpreadPy使系统地调查激活动力学如何反映认知、心理和临床现象成为可能。", "result": "通过三个案例研究展示了该库的实用性：(1) 高数学焦虑与低数学焦虑学生的关联知识网络上的扩散激活模式不同，揭示了焦虑相关的结构差异。(2) 创造性任务的模拟显示，激活轨迹随任务难度变化，揭示了认知负荷如何调节词汇访问。(3) 在失语症个体中，词汇网络上的模拟激活模式与命名任务中的实证错误类型（语义与语音学）相关联，将网络结构与临床损伤联系起来。", "conclusion": "SpreadPy是一款灵活的框架，研究人员可以使用经验性或理论网络来模拟这些过程，为个体差异和认知损伤提供机制性见解。该库公开提供，支持心理学、神经科学和教育研究的可复制研究。"}}
{"id": "2507.09216", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09216", "abs": "https://arxiv.org/abs/2507.09216", "authors": ["Jingguo Liu", "Han Yu", "Shigang Li", "Jianfeng Li"], "title": "360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models", "comment": "This paper is accecpted by ICMEW 2025", "summary": "Due to the current lack of large-scale datasets at the million-scale level,\ntasks involving panoramic images predominantly rely on existing two-dimensional\npre-trained image benchmark models as backbone networks. However, these\nnetworks are not equipped to recognize the distortions and discontinuities\ninherent in panoramic images, which adversely affects their performance in such\ntasks. In this paper, we introduce a novel spherical sampling method for\npanoramic images that enables the direct utilization of existing pre-trained\nmodels developed for two-dimensional images. Our method employs spherical\ndiscrete sampling based on the weights of the pre-trained models, effectively\nmitigating distortions while achieving favorable initial training values.\nAdditionally, we apply the proposed sampling method to panoramic image\nsegmentation, utilizing features obtained from the spherical model as masks for\nspecific channel attentions, which yields commendable results on commonly used\nindoor datasets, Stanford2D3D.", "AI": {"tldr": "本研究表明，采用提出的球面采样技术，可以在全景图像任务中直接利用现有的二维图像预训练模型，并获得了室内数据集上出色的分割性能。", "motivation": "由于缺乏百万规模的全景图像数据集，当前的任务主要依赖于二维预训练模型作为骨干网络，然而这些模型在处理全景图像时表现不佳，其原因在于全景图像中存在的失真和不连续性问题。", "method": "提出了一种球面采样方法，能够在全景图像中直接使用预训练的二维图像模型，同时通过球面离散采样和基于模型权重的方法，减轻图像失真，提高模型训练效果。", "result": "这篇论文提出了一种用于全景图像的球面采样方法，该方法使得现有的二维图像预训练模型可以直接应用于全景图像任务。通过基于预训练模型权重的球面离散采样，该方法能够有效缓解全景图像中的失真问题，并且在室内数据集Stanford2D3D上实现了显著的分割效果。", "conclusion": "实验结果表明，使用上述方法可以显著提高在全景图像上的分割性能，尤其是斯坦福2D3D数据集上的分割效果。"}}
{"id": "2507.09629", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09629", "abs": "https://arxiv.org/abs/2507.09629", "authors": ["Basel Mousi", "Nadir Durrani", "Fahim Dalvi"], "title": "An Exploration of Knowledge Editing for Arabic", "comment": null, "summary": "While Knowledge Editing (KE) has been widely explored in English, its\nbehavior in morphologically rich languages like Arabic remains underexamined.\nIn this work, we present the first study of Arabic KE. We evaluate four methods\n(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact\nbenchmarks, analyzing both multilingual and cross-lingual settings. Our\nexperiments on Llama-2-7B-chat show show that parameter-based methods struggle\nwith cross-lingual generalization, while instruction-tuned methods perform more\nrobustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show\nthat joint Arabic-English training improves both editability and transfer. We\nrelease Arabic KE benchmarks and multilingual training for LTE data to support\nfuture research.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.09217", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09217", "abs": "https://arxiv.org/abs/2507.09217", "authors": ["Görkay Aydemir"], "title": "Online Long-term Point Tracking in the Foundation Model Era", "comment": "arXiv admin note: substantial text overlap with arXiv:2501.18487", "summary": "Point tracking aims to identify the same physical point across video frames\nand serves as a geometry-aware representation of motion. This representation\nsupports a wide range of applications, from robotics to augmented reality, by\nenabling accurate modeling of dynamic environments. Most existing long-term\ntracking approaches operate in an offline setting, where future frames are\navailable to refine predictions and recover from occlusions. However,\nreal-world scenarios often demand online predictions: the model must operate\ncausally, using only current and past frames. This constraint is critical in\nstreaming video and embodied AI, where decisions must be made immediately based\non past observations. Under such constraints, viewpoint invariance becomes\nessential. Visual foundation models, trained on diverse large-scale datasets,\noffer the potential for robust geometric representations. While they lack\ntemporal reasoning on their own, they can be integrated into tracking pipelines\nto enrich spatial features. In this thesis, we address the problem of long-term\npoint tracking in an online setting, where frames are processed sequentially\nwithout access to future information or sliding windows. We begin by evaluating\nthe suitability of visual foundation models for this task and find that they\ncan serve as useful initializations and be integrated into tracking pipelines.\nHowever, to enable long-term tracking in an online setting, a dedicated design\nis still required. In particular, maintaining coherence over time in this\ncausal regime requires memory to propagate appearance and context across\nframes. To address this, we introduce Track-On, a transformer-based model that\ntreats each tracked point as a query and processes video frames one at a time.\nTrack-On sets a new state of the art across seven public benchmarks,\ndemonstrating the feasibility of long-term tracking without future access.", "AI": {"tldr": "本文介绍了一种名为Track-On的Transformer模型，该模型能实现长期在线点跟踪，不依赖未来帧信息。该模型利用视觉基础模型提供初始几何表示，并通过记忆机制传递外观和上下文信息来维持长时一致性。实验结果表明，该模型在七个公开基准上达到了新的技术水平。", "motivation": "本文旨在解决长期点跟踪问题，特别是在只能根据当前和过去帧进行预测的在线设置中。该问题具有重要意义，因为在诸如直播视频和具身AI的应用中，决策需要立即基于过去的数据做出。", "method": "Structure", "result": "{\n  \"tldr\": \"本文介绍了一种名为Track-On的Transformer模型，该模型能实现长期在线点跟踪，不依赖未来帧信息。该模型利用视觉基础模型提供初始几何表示，并通过记忆机制传递外观和上下文信息来维持长时一致性。实验结果表明，该模型在七个公开基准上达到了新的技术水平。\",\n  \"motivation\": \"本文旨在解决长期点跟踪问题，特别是在只能根据当前和过去帧进行预测的在线设置中。该问题具有重要意义，因为在诸如直播视频和具身AI的应用中，决策需要立即基于过去的数据做出。\",\n  \"method\": \"Track-On模型将每个跟踪点视为查询，逐帧处理视频。模型利用视觉基础模型的初始几何表示，并通过一种记忆机制来传播外观和上下文信息，以维持长时一致性。\",\n  \"result\": \"Track-On模型在没有未来帧信息的情况下达到长期跟踪的新水平，并在七个公开基准上展示了卓越的性能。\",\n  \"conclusion\": \"本文提出了有效的解决方案以满足实时决策场景下的长期点跟踪需求，展示了一个新的Transformer架构，它可以在线环境下维持长时间的动作一致性。\")", "conclusion": "本文提出了有效的解决方案以满足实时决策场景下的长期点跟踪需求，展示了一个新的Transformer架构，它可以在线环境下维持长时间的动作一致性。"}}
{"id": "2507.09638", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09638", "abs": "https://arxiv.org/abs/2507.09638", "authors": ["Pawitsapak Akarajaradwong", "Chompakorn Chaksangchaichot", "Pirat Pothavorn", "Attapol Thamrongrattanarit-Rutherford", "Ekapol Chuangsuwanich", "Sarana Nutanong"], "title": "Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?", "comment": null, "summary": "The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal\nquestion answering is still limited, especially for questions requiring\nextensive, complex legal reasoning. To address these limitations, we introduce\nan approach aligning LLMs toward improved law citation accuracy and better\nresponse quality using Group-Relative Policy Optimization (GRPO). Our approach\nleverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,\nsignificantly reducing computational expenses up to 2.5x compared to large\nlanguage model judges. Experiments on the NitiBench benchmark demonstrate\nsubstantial improvements: GRPO achieves up to 90% citation-F1 gains from the\nbase model and a 31% increase in joint quality metrics over instruction tuning.\nCrucially, our method shows enhanced robustness on complex legal reasoning\ntasks compared to instruction tuning, providing an effective and\nresource-efficient solution for enhancing Thai legal LLMs.", "AI": {"tldr": "采用GRPO方法并利用BGE-M3嵌入式作为奖励机制，显著提高泰国法律问答中引用准确性和回答质量，且成本效益高。", "motivation": "针对泰国法律问答中RETRIEVAL-AUGMENTED GENERATION (RAG)系统在需要复杂法律推理的问题上的表现不佳，提出改进方法。", "method": "采用Group-Relative Policy Optimization (GRPO)方法，并利用BGE-M3嵌入式作为成本效益高的语义相似度奖励，来提高泰国法律问题回答中法律引用的准确性及回答质量。", "result": "在NitiBench基准测试中，相较于基础模型，GRPO实现了高达90%的引用F1值提升，并在联合质量指标上比指令调优高出31%。", "conclusion": "该方法相较于指令调优，在复杂的法律推理任务上表现出更强的鲁棒性，为提升泰国法律LLMs提供了有效的资源高效解决方案。"}}
{"id": "2507.09222", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09222", "abs": "https://arxiv.org/abs/2507.09222", "authors": ["Behraj Khan", "Tahir Syed"], "title": "Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift", "comment": null, "summary": "Foundation models like CLIP and SAM have transformed computer vision and\nmedical imaging via low-shot transfer learning. However, deployment of these\nmodels hindered by two key challenges: \\textit{distribution shift} between\ntraining and test data, and \\textit{confidence misalignment} that leads to\noverconfident incorrect predictions. These issues manifest differently in\nvision-language classification and medical segmentation tasks, yet existing\nsolutions remain domain-specific. We propose \\textit{StaRFM}, a unified\nframework addressing both challenges. It introduces a Fisher information\npenalty (FIP), extended to 3D medical data via patch-wise regularization, to\nreduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence\nmisalignment penalty (CMP), reformulated for voxel-level predictions,\ncalibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes\nbounds showing FIP controls generalization via the Fisher-Rao norm, while CMP\nminimizes calibration error through Brier score optimization. StaRFM shows\nconsistent performance like \\texttt{+}3.5\\% accuracy and 28\\% lower ECE on 19\nvision datasets (e.g., ImageNet, Office-Home), 84.7\\% DSC and 4.8mm HD95 in\nmedical segmentation (e.g., BraTS, ATLAS), and 40\\% lower cross-domain\nperformance gap compared to prior benchmarking methods. The framework is\nplug-and-play, requiring minimal architectural changes for seamless integration\nwith foundation models. Code and models will be released at\nhttps://anonymous.4open.science/r/StaRFM-C0CD/README.md", "AI": {"tldr": "本研究提出了StaRFM框架来解决基础模型在视觉和医学图像分析中的部署挑战，实现了显著的性能改进和跨域性能差距减少。", "motivation": "由于分布转移和信心不匹配问题是基础模型在计算机视觉和医学成像领域部署中的关键挑战，当前的解决方案尚局限在特定领域，尚未有综合性的解决方案。", "method": "本论文提出了一个名为StaRFM的统一框架，以解决分布在训练和测试数据中的转移问题以及信心不匹配导致的过度自信的错误预测问题。该框架通过引入Fisher信息惩罚（FIP）以及扩展到3D医疗数据的片区域正则化来减少CLIP和SAM嵌入中的协变量移位。此外，提出了置信度不匹配惩罚（CMP），用于重新评估体素级别的预测结果，以调整分割任务中的不确定性。", "result": "实验结果显示，使用StaRFM后，在19个视觉数据集中平均提高了3.5%的准确性和降低了28%的预期校准误差（ECE）。在医学分割任务中，例如BraTS和ATLAS等数据集，DSC提高了84.7%，HD95降低了到4.8mm。相比之下，跨域性能差距与之前的基准方法相比减少了40%。", "conclusion": "StaRFM框架实现了广泛的性能改进，提升了3.5%的准确性和降低了28%的预期校准误差（ECE）在19个视觉数据集上，实现了84.7%的Dice相似度系数（DSC）和4.8mm的平均Hausdorff距离（HD95）在8个医学分割数据集上，同时与先前的基准方法相比，跨域性能差距减少了40%。"}}
{"id": "2507.09701", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09701", "abs": "https://arxiv.org/abs/2507.09701", "authors": ["Shulin Huang", "Linyi Yang", "Yue Zhang"], "title": "MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs", "comment": null, "summary": "Large language models exhibit cultural biases and limited cross-cultural\nunderstanding capabilities, particularly when serving diverse global user\npopulations. We propose MCEval, a novel multilingual evaluation framework that\nemploys dynamic cultural question construction and enables causal analysis\nthrough Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive\nevaluation spans 13 cultures and 13 languages, systematically assessing both\ncultural awareness and cultural bias across different linguistic scenarios. The\nframework provides 39,897 cultural awareness instances and 17,940 cultural bias\ninstances. Experimental results reveal performance disparities across different\nlinguistic scenarios, demonstrating that optimal cultural performance is not\nonly linked to training data distribution, but also is related to\nlanguage-culture alignment. The evaluation results also expose the fairness\nissue, where approaches appearing successful in the English scenario create\nsubstantial disadvantages. MCEval represents the first comprehensive\nmultilingual cultural evaluation framework that provides deeper insights into\nLLMs' cultural understanding.", "AI": {"tldr": "MCEval是一个新的多语言评估框架，评估13种文化和13种语言下的文化意识和偏见，揭示了最佳文化表现与语言分布和语言-文化一致性相关，并暴露了英语场景中方法在其他语言下的不公平性。", "motivation": "大型语言模型表现出文化偏见和跨文化理解能力有限，在服务多样化的全球用户群体时尤为明显。为了改进这一点，需要一个评估框架来衡量语言模型的文化理解和偏见。", "method": "提出MCEval，这是一个新型的多语言评估框架，利用动态文化问题构建，并通过反事实重述和混淆因素重述实现因果分析。该框架覆盖了13种文化和13种语言，系统地评估不同语言背景下的文化意识和文化偏见。", "result": "实验结果显示，在不同的语言背景下，性能存在差异。最佳的文化表现不仅与训练数据的分布有关，还与语言文化的一致性有关。该评估还表明，在英语场景中看似成功的方法在其他语言中可能会有严重的劣势。", "conclusion": "MCEval是第一个全面的多语言文化评估框架，提供了更深入的洞察，以理解语言模型的文化理解能力。"}}
{"id": "2507.09230", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09230", "abs": "https://arxiv.org/abs/2507.09230", "authors": ["G. Kutay Türkoglu", "Julian Tanke", "Iheb Belgacem", "Lev Markhasin"], "title": "EgoAnimate: Generating Human Animations from Egocentric top-down Views", "comment": "10 pages, 5 figures", "summary": "An ideal digital telepresence experience requires accurate replication of a\nperson's body, clothing, and movements. To capture and transfer these movements\ninto virtual reality, the egocentric (first-person) perspective can be adopted,\nwhich enables the use of a portable and cost-effective device without\nfront-view cameras. However, this viewpoint introduces challenges such as\nocclusions and distorted body proportions.\n  There are few works reconstructing human appearance from egocentric views,\nand none use a generative prior-based approach. Some methods create avatars\nfrom a single egocentric image during inference, but still rely on multi-view\ndatasets during training. To our knowledge, this is the first study using a\ngenerative backbone to reconstruct animatable avatars from egocentric inputs.\nBased on Stable Diffusion, our method reduces training burden and improves\ngeneralizability.\n  Inspired by methods such as SiTH and MagicMan, which perform 360-degree\nreconstruction from a frontal image, we introduce a pipeline that generates\nrealistic frontal views from occluded top-down images using ControlNet and a\nStable Diffusion backbone.\n  Our goal is to convert a single top-down egocentric image into a realistic\nfrontal representation and feed it into an image-to-motion model. This enables\ngeneration of avatar motions from minimal input, paving the way for more\naccessible and generalizable telepresence systems.", "AI": {"tldr": "采用基于Stable Diffusion的生成模型解决顶向下视角图像的遮挡问题，生成前视图像并转化为可动画的虚拟形象，提升了远程呈现的体验。", "motivation": "研究动机在于克服使用第一人称视角（即顶向下视角）捕捉人体和动作时遇到的遮挡和身体比例失真问题，通过生成式方法实现更便携且成本效益更高的远程呈现体验。", "method": "该研究采用了基于Stable Diffusion的生成式模型，并结合ControlNet来解决从顶向下视角的单一图像生成真实的前视图问题，从而能够将这种图像转换为动画形象，适用于虚拟现实中的远程呈现系统。", "result": "该研究首次展示了如何利用生成式背骨模型从单个顶向下图像中重建可操控的虚拟形象，并将这一技术推进至运动生成的过渡阶段，具有较高的泛化能力。", "conclusion": "该研究通过对顶向下视角图像进行处理，生成可应用于动作生成模型的前视图，从而简化了输入，增强了系统的可访问性和泛化能力，为远程呈现体验铺平了道路。"}}
{"id": "2507.09709", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09709", "abs": "https://arxiv.org/abs/2507.09709", "authors": ["Baturay Saglam", "Paul Kassianik", "Blaine Nelson", "Sajana Weerawardhena", "Yaron Singer", "Amin Karbasi"], "title": "Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces", "comment": null, "summary": "Understanding the latent space geometry of large language models (LLMs) is\nkey to interpreting their behavior and improving alignment. \\baturay{However,\nit remains unclear to what extent LLMs internally organize representations\nrelated to semantic understanding. To investigate this, we conduct a\nlarge-scale empirical study of hidden states in transformer-based LLMs,\nanalyzing 11 decoder-only models across 6 scientific topics and 12 layers each.\nWe find that high-level semantic information consistently lies in\nlow-dimensional subspaces that form linearly separable representations across\ndistinct domains. This separability becomes more pronounced in deeper layers\nand under prompts that trigger structured reasoning or alignment\nbehaviors$\\unicode{x2013}$even when surface content is unchanged. This geometry\nenables simple yet effective causal interventions in hidden space; for example,\nreasoning patterns like chain-of-thought can be captured by a single vector\ndirection. Together, these findings support the development of geometry-aware\ntools that operate directly on latent representations to detect and mitigate\nharmful or adversarial content, using methods such as transport-based defenses\nthat leverage this separability. As a proof of concept, we demonstrate this\npotential by training a simple MLP classifier as a lightweight latent-space\nguardrail, which detects adversarial and malicious prompts with high precision.", "AI": {"tldr": "研究发现大规模语言模型的潜在空间几何结构在语义理解中起到重要作用，支持开发新的工具检测并减轻潜在的有害内容。", "motivation": "研究大规模语言模型（LLMs）如何在内部组织与语义理解相关的表示，以解释其行为并提高对齐性。", "method": "通过大规模实证研究，分析了11个基于transformer的仅解码器模型，在6个科学主题和每个主题的12个层次上的隐藏状态。", "result": "发现高层次的语义信息一致存在于低维子空间中，并在不同领域中形成线性可分表示。这种可分性在更深的层和需要结构化推理或对齐行为的提示下变得更加明显。这种几何结构使隐藏空间中的简单因果干预成为可能，例如，可以通过一个向量方向捕捉到链式思维等推理模式。", "conclusion": "这些发现支持开发几何感知工具，直接对潜在表示进行操作，以检测和减轻有害或恶意内容，例如使用基于传输的防御方法。作为概念验证，训练了一个简单的MLP分类器作为轻量级的潜在空间守门员，能够高精度地检测对抗性和恶意提示。"}}
{"id": "2507.09242", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09242", "abs": "https://arxiv.org/abs/2507.09242", "authors": ["Shiqi Jiang", "Xinpeng Li", "Xi Mao", "Changbo Wang", "Chenhui Li"], "title": "PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process", "comment": "ACM International Conference on Multimedia 2025", "summary": "Artistic image assessment has become a prominent research area in computer\nvision. In recent years, the field has witnessed a proliferation of datasets\nand methods designed to evaluate the aesthetic quality of paintings. However,\nmost existing approaches focus solely on static final images, overlooking the\ndynamic and multi-stage nature of the artistic painting process. To address\nthis gap, we propose a novel framework for human-aligned assessment of painting\nprocesses. Specifically, we introduce the Painting Process Assessment Dataset\n(PPAD), the first large-scale dataset comprising real and synthetic painting\nprocess images, annotated by domain experts across eight detailed attributes.\nFurthermore, we present PPJudge (Painting Process Judge), a Transformer-based\nmodel enhanced with temporally-aware positional encoding and a heterogeneous\nmixture-of-experts architecture, enabling effective assessment of the painting\nprocess. Experimental results demonstrate that our method outperforms existing\nbaselines in accuracy, robustness, and alignment with human judgment, offering\nnew insights into computational creativity and art education.", "AI": {"tldr": "该研究提出了一种用于评估绘画过程的新框架，引入了Painting Process Assessment Dataset (PPAD) 和 PPJudge 模型，它是一个基于Transformer的模型，用于有效评估绘画过程，并在多个方面超过了现有的评估方法。", "motivation": "尽管近年来该领域看到了大量用于评估绘画审美质量的数据集和方法的涌现，但大多数现有方法仅关注静态的最终图像，忽略了艺术绘画过程的动态和多阶段性质。为了解决这一差距，我们提出了一个新颖的框架用于人类对齐的艺术绘画过程评估。", "method": "我们提出了一种新的框架用于人类对齐的艺术绘画过程评估。我们引入了Painting Process Assessment Dataset (PPAD)，这是第一个包含真实和合成绘画过程图像的大规模数据集，并由领域专家根据八个详细的属性进行了标注。此外，我们提出了PPJudge（Painting Process Judge），这是一个基于Transformer的模型，通过增强的时间感知位置编码和异构专家混合架构，实现了有效的绘画过程评估。", "result": "实验结果表明，我们的方法在准确性、鲁棒性和与人类判断的对齐方面超过了现有的基线方法。", "conclusion": "我们的方法为计算创造力和艺术教育提供了新的视角。"}}
{"id": "2507.09758", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.09758", "abs": "https://arxiv.org/abs/2507.09758", "authors": ["Qi Feng", "Yihong Liu", "Hinrich Schütze"], "title": "Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding", "comment": "18 pages, 23 figures. To appear in ACL 2025 Student Research Workshop\n  (SRW)", "summary": "Curriculum learning is a widely adopted training strategy in natural language\nprocessing (NLP), where models are exposed to examples organized by increasing\ndifficulty to enhance learning efficiency and performance. However, most\nexisting approaches rely on manually defined difficulty metrics -- such as text\nlength -- which may not accurately reflect the model's own perspective. To\novercome this limitation, we present a self-adaptive curriculum learning\nparadigm that prioritizes fine-tuning examples based on difficulty scores\npredicted by pre-trained language models (PLMs) themselves. Building on these\nscores, we explore various training strategies that differ in the ordering of\nexamples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed\nsampling. We evaluate our method on four natural language understanding (NLU)\ndatasets covering both binary and multi-class classification tasks.\nExperimental results show that our approach leads to faster convergence and\nimproved performance compared to standard random sampling.", "AI": {"tldr": "论文介绍了基于PLMs预测难度分数的自适应课程学习范式，探索了多种训练策略，验证了该方法在自然语言理解任务中的有效性。", "motivation": "大多数现有的课程学习方法依赖于手动定义的难度指标，这可能无法准确反映模型自身的视角。", "method": "该论文提出了一种自适应课程学习范式，根据预训练语言模型（PLMs）自己预测的难度分数对微调示例进行优先级排序。", "result": "实验证明，该方法相比标准的随机抽样方法，能够更快地收敛并改善性能。", "conclusion": "实验结果表明，自适应课程学习范式可以提高学习效率和模型性能。"}}
{"id": "2507.09248", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09248", "abs": "https://arxiv.org/abs/2507.09248", "authors": ["Varsha Devi", "Amine Bohi", "Pardeep Kumar"], "title": "AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition", "comment": "13 Pages, 4 figures, 2 tables ICIAP 2025", "summary": "Context-aware emotion recognition (CAER) enhances affective computing in\nreal-world scenarios, but traditional methods often suffer from context\nbias-spurious correlation between background context and emotion labels (e.g.\nassociating ``garden'' with ``happy''). In this paper, we propose\n\\textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces\n\\textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the\nConvNeXt backbone by integrating Spatial Transformer Network and\nSqueeze-and-Excitation layers for enhanced feature recalibration. At the core\nof AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM),\nwhich applies causal theory, perturbs context features, isolates spurious\ncorrelations, and performs an attention-driven correction guided by face\nfeatures to mitigate context bias. Experimental results on the CAER-S dataset\ndemonstrate the effectiveness of AGCD-Net, achieving state-of-the-art\nperformance and highlighting the importance of causal debiasing for robust\nemotion recognition in complex settings.", "AI": {"tldr": "论文提出了AGCD-Net，一种注意力引导的因果去偏网络，通过增强特征的重校准减少上下文偏差，实验证明其在情绪识别上表现优异。", "motivation": "传统的上下文感知情绪识别方法容易受到背景与情绪标签之间偶然关联的影响。因此，动机是为了构建一个能够减少这种偏差、提高在真实场景中情绪识别准确性的系统。", "method": "提出了AGCD-Net，利用Hybrid ConvNeXt增强特征的重校准，并通过AG-CIM基于因果理论扰动上下文特征，隔离偶然关联，利用面部特征进行注意力引导的修正以减少上下文偏置。", "result": "AGCD-Net模型通过引入Hybrid ConvNeXt（一种结合了空间变换网络和挤压激励层的卷积编码器）来增强特征重校准，提出了一种注意力引导的因果干预模块AG-CIM，以减少背景情境对情绪识别的偏差。实验结果在CAER-S数据集上验证了该模型的有效性，达到了最先进的性能，证明了因果去偏对于复杂环境下情绪识别的重要性。", "conclusion": "AGCD-Net通过去偏处理显著提升了情绪识别的准确性，特别是在存在上下文偏差的情况下，表现出色。实验验证了AGCD-Net的性能优于现有方法，并指出了因果去偏在复杂情绪识别任务中的重要性。"}}
{"id": "2507.09777", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.09777", "abs": "https://arxiv.org/abs/2507.09777", "authors": ["Gabriel Mordecki", "Guillermo Moncecchi", "Javier Couto"], "title": "Te Ahorré Un Click: A Revised Definition of Clickbait and Detection in Spanish News", "comment": null, "summary": "We revise the definition of clickbait, which lacks current consensus, and\nargue that the creation of a curiosity gap is the key concept that\ndistinguishes clickbait from other related phenomena such as sensationalism and\nheadlines that do not deliver what they promise or diverge from the article.\nTherefore, we propose a new definition: clickbait is a technique for generating\nheadlines and teasers that deliberately omit part of the information with the\ngoal of raising the readers' curiosity, capturing their attention and enticing\nthem to click. We introduce a new approach to clickbait detection datasets\ncreation, by refining the concept limits and annotations criteria, minimizing\nthe subjectivity in the decision as much as possible. Following it, we created\nand release TA1C (for Te Ahorr\\'e Un Click, Spanish for Saved You A Click), the\nfirst open source dataset for clickbait detection in Spanish. It consists of\n3,500 tweets coming from 18 well known media sources, manually annotated and\nreaching a 0.825 Fleiss' K inter annotator agreement. We implement strong\nbaselines that achieve 0.84 in F1-score.", "AI": {"tldr": "The paper revises the definition of clickbait, proposes a new definition focusing on the creation of a curiosity gap, and develops a new dataset for Spanish clickbait detection.", "motivation": "To revise the definition of clickbait and argue that the creation of a curiosity gap is the key concept that distinguishes clickbait from other related phenomena.", "method": "We introduce a new approach to clickbait detection datasets creation, by refining the concept limits and annotations criteria, minimizing the subjectivity in the decision as much as possible.", "result": "They created TA1C, the first open source dataset for clickbait detection in Spanish, consisting of 3,500 tweets from 18 media sources, manually annotated and with a 0.825 Fleiss' K inter annotator agreement.", "conclusion": "The study introduces a new open source dataset for clickbait detection in Spanish and implements strong baselines achieving 0.84 in F1-score."}}
{"id": "2507.09256", "categories": ["cs.CV", "cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.09256", "abs": "https://arxiv.org/abs/2507.09256", "authors": ["Junyu Chen", "Yihua Gao", "Mingyuan Ge", "Mingyong Li"], "title": "Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching", "comment": "Accepted by the Knowledge-Based Systems(KBS), 2025", "summary": "Image-text matching is crucial for bridging the semantic gap between computer\nvision and natural language processing. However, existing methods still face\nchallenges in handling high-order associations and semantic ambiguities among\nsimilar instances. These ambiguities arise from subtle differences between soft\npositive samples (semantically similar but incorrectly labeled) and soft\nnegative samples (locally matched but globally inconsistent), creating matching\nuncertainties. Furthermore, current methods fail to fully utilize the\nneighborhood relationships among semantically similar instances within training\nbatches, limiting the model's ability to learn high-order shared knowledge.\nThis paper proposes the Ambiguity-Aware and High-order Relation learning\nframework (AAHR) to address these issues. AAHR constructs a unified\nrepresentation space through dynamic clustering prototype contrastive learning,\neffectively mitigating the soft positive sample problem. The framework\nintroduces global and local feature extraction mechanisms and an adaptive\naggregation network, significantly enhancing full-grained semantic\nunderstanding capabilities. Additionally, AAHR employs intra-modal and\ninter-modal correlation matrices to investigate neighborhood relationships\namong sample instances thoroughly. It incorporates GNN to enhance semantic\ninteractions between instances. Furthermore, AAHR integrates momentum\ncontrastive learning to expand the negative sample set. These combined\nstrategies significantly improve the model's ability to discriminate between\nfeatures. Experimental results demonstrate that AAHR outperforms existing\nstate-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,\nconsiderably improving the accuracy and efficiency of image-text matching. The\ncode and model checkpoints for this research are available at\nhttps://github.com/Image-Text-Matching/AAHR .", "AI": {"tldr": "The paper proposes AAHR to tackle ambiguities in image-text matching by using dynamic clustering, adaptive aggregation, GNN, and momentum contrastive learning.", "motivation": "To improve image-text matching by better handling semantic ambiguities and leveraging neighborhood relationships.", "method": "AAHR uses dynamic clustering prototype contrastive learning, global and local feature extraction, GNN, and momentum contrastive learning to enhance semantic understanding.", "result": "Experimental results show AAHR outperforms state-of-the-art methods on several image-text datasets.", "conclusion": "AAHR is an effective framework that significantly improves the accuracy and efficiency of image-text matching by addressing the semantic ambiguities in training data."}}
{"id": "2507.09875", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09875", "abs": "https://arxiv.org/abs/2507.09875", "authors": ["Qinyuan Ye", "Robin Jia", "Xiang Ren"], "title": "Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition", "comment": "Code: https://github.com/INK-USC/function-induction", "summary": "Large language models demonstrate the intriguing ability to perform unseen\ntasks via in-context learning. However, it remains unclear what mechanisms\ninside the model drive such task-level generalization. In this work, we\napproach this question through the lens of off-by-one addition (i.e., 1+1=3,\n2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function\nas a second step. Leveraging circuit-style interpretability techniques such as\npath patching, we analyze the models' internal computations behind their\nnotable performance and present three key findings. First, we uncover a\nfunction induction mechanism that explains the model's generalization from\nstandard addition to off-by-one addition. This mechanism resembles the\nstructure of the induction head mechanism found in prior work and elevates it\nto a higher level of abstraction. Second, we show that the induction of the +1\nfunction is governed by multiple attention heads in parallel, each of which\nemits a distinct piece of the +1 function. Finally, we find that this function\ninduction mechanism is reused in a broader range of tasks, including synthetic\ntasks such as shifted multiple-choice QA and algorithmic tasks such as base-8\naddition. Overall, our findings offer deeper insights into how reusable and\ncomposable structures within language models enable task-level generalization.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.09266", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09266", "abs": "https://arxiv.org/abs/2507.09266", "authors": ["JianHe Low", "Ozge Mercanoglu Sincan", "Richard Bowden"], "title": "SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation", "comment": "Accepted in International Conference on Computer Vision (ICCV)\n  Workshops", "summary": "Gloss-free Sign Language Translation (SLT) has advanced rapidly, achieving\nstrong performances without relying on gloss annotations. However, these gains\nhave often come with increased model complexity and high computational demands,\nraising concerns about scalability, especially as large-scale sign language\ndatasets become more common. We propose a segment-aware visual tokenization\nframework that leverages sign segmentation to convert continuous video into\ndiscrete, sign-informed visual tokens. This reduces input sequence length by up\nto 50% compared to prior methods, resulting in up to 2.67x lower memory usage\nand better scalability on larger datasets. To bridge the visual and linguistic\nmodalities, we introduce a token-to-token contrastive alignment objective,\nalong with a dual-level supervision that aligns both language embeddings and\nintermediate hidden states. This improves fine-grained cross-modal alignment\nwithout relying on gloss-level supervision. Our approach notably exceeds the\nperformance of state-of-the-art methods on the PHOENIX14T benchmark, while\nsignificantly reducing sequence length. Further experiments also demonstrate\nour improved performance over prior work under comparable sequence-lengths,\nvalidating the potential of our tokenization and alignment strategies.", "AI": {"tldr": "新的SLT方法通过改进的标记化和对齐策略提高了性能，缩短了输入序列长度，降低了计算需求。", "motivation": "解决现有SLT模型复杂度高、计算成本高且不便于规模化处理的问题。", "method": "提出段落感知的视觉标记化框架，利用手势分段将连续视频转化为视觉标记，引入标记对标记的对比对齐目标和双层次监督机制以改善跨模态对齐。", "result": "该研究提出了一个基于手势分段的视觉标记化框架，显著缩短了输入序列长度，降低了内存使用。通过引入标记到标记的对比对齐目标和双层次监督机制，改善了跨模态对齐的精细度。在PHOENIX14T基准测试中，该方法显著超越了现有的最佳方法。", "conclusion": "提出的段落感知视觉标记化框架及其对齐策略在性能和效率上都优于现有方法，验证了所提出策略的有效性。"}}
